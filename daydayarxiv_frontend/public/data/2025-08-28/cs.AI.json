{
  "date": "2025-08-28",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-08-28 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv è®ºæ–‡çˆ†å‘å¼å¢é•¿ï¼Œæœ€æ ¸å¿ƒçš„è¶‹åŠ¿é›†ä¸­åœ¨ **Long CoTï¼ˆé•¿æ€ç»´é“¾ï¼‰æ¨ç†èƒ½åŠ›çš„è¿›é˜¶**ä¸**Agentic AIï¼ˆæ™ºèƒ½ä½“ AIï¼‰çš„åŸºå»ºåŒ–**ã€‚æˆ‘ä»¬çœ‹åˆ°äº†åˆ©ç”¨ NP-Hard å›¾é—®é¢˜æ¥è®­ç»ƒæ¨ç†æ¨¡å‹çš„æ–°å°è¯•ï¼ˆGraph-R1ï¼‰ï¼Œä»¥åŠ Apple æ¨å‡ºçš„é«˜æ•ˆ MobileCLIP2ã€‚æ­¤å¤–ï¼Œ\"AI for Science\" è¿æ¥äº†ä¸€ç¯‡é‡ç£…ç»¼è¿°ï¼Œè€Œå®‰å…¨é¢†åŸŸåˆ™å‡ºç°äº†ä¸€ç§åˆ©ç”¨æ¨¡å‹\"å¯¹é½\"æœºåˆ¶è¿›è¡ŒæŠ•æ¯’çš„æ–°å‹æ”»å‡»æ–¹å¼ã€‚\n\n---\n\n### ğŸš€ å¼ºæ¨ç†ä¸é•¿æ€ç»´é“¾ (Reasoning & Long CoT)\n\n**1. [å›¾æ¨ç†] Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems**\n*   **Graph-R1ï¼šåˆ©ç”¨ NP-Hard å›¾é—®é¢˜é‡Šæ”¾ LLM çš„æ¨ç†èƒ½åŠ›**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£çš„å°è¯•ã€‚ä½œè€…è®¤ä¸ºç›®å‰çš„é•¿æ€ç»´é“¾ï¼ˆLong CoTï¼‰è®­ç»ƒä¾èµ–æ˜‚è´µçš„äººå·¥æ•°æ®ï¼ˆå¦‚æ•°å­¦é¢˜ï¼‰ã€‚æœ¬æ–‡æå‡ºåˆ©ç”¨ **NP-Hard (NPH) å›¾é—®é¢˜**ä½œä¸ºåˆæˆè®­ç»ƒè¯­æ–™ï¼Œå› ä¸ºè¿™ç±»é—®é¢˜å¤©ç”Ÿéœ€è¦æ·±åº¦æ¨ç†å’Œå›æº¯åæ€ã€‚\n*   **å‘ç°ï¼š** ä»–ä»¬å¼€å‘äº† Graph-R1-7B æ¨¡å‹ï¼Œé€šè¿‡åœ¨ NPH å›¾é—®é¢˜ä¸Šè¿›è¡Œä¸¤é˜¶æ®µåè®­ç»ƒï¼ˆSFT + RLï¼‰ï¼Œä¸ä»…åœ¨å›¾ä»»åŠ¡ä¸Šè¶…è¶Šäº† QwQ-32Bï¼Œè¿˜åœ¨æ•°å­¦ã€ä»£ç å’Œé€»è¾‘ä»»åŠ¡ä¸Šå±•ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™è¯æ˜äº†åˆæˆçš„å›¾æ•°æ®å¯ä»¥ä½œä¸ºæå‡é€šç”¨æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆ\"ç£¨åˆ€çŸ³\"ã€‚\n\n**2. [è‡ªé€‚åº”æ¨ç†] R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning**\n*   **R-4Bï¼šé€šè¿‡åŒæ¨¡é€€ç«å’Œå¼ºåŒ–å­¦ä¹ æ¿€åŠ± MLLM çš„é€šç”¨è‡ªåŠ¨æ€ç»´èƒ½åŠ›**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹\"æ€è€ƒè¿‡ç¨‹\"ï¼ˆThinking Processï¼‰çš„å†—ä½™é—®é¢˜ï¼ˆç®€å•é—®é¢˜ä¸éœ€è¦æ·±æ€ï¼‰ã€‚ä½œè€…æå‡ºäº† R-4Bï¼Œé€šè¿‡**åŒæ¨¡ç­–ç•¥ä¼˜åŒ–ï¼ˆBi-mode Policy Optimizationï¼‰**ï¼Œè®©æ¨¡å‹å­¦ä¼šæ ¹æ®é—®é¢˜å¤æ‚åº¦è‡ªé€‚åº”åœ°å†³å®š\"æ˜¯å¦éœ€è¦æ€è€ƒ\"ï¼ˆThink or Notï¼‰ã€‚\n*   **å‘ç°ï¼š** è¯¥æ¨¡å‹åœ¨ 25 ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸­è¶…è¶Šäº† Qwen2.5-VL-7Bï¼Œå¹¶ä»¥æ›´ä½çš„è®¡ç®—æˆæœ¬è¾¾åˆ°äº†ä¸æ›´å¤§æ¨¡å‹ç›¸å½“çš„æ¨ç†èƒ½åŠ›ã€‚\n\n**3. [æ¨ç†æ•ˆç”¨] Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction**\n*   **é€šè¿‡æ¡ä»¶ç†µå‡å°‘é‡åŒ– LLM ä¸­çš„æ¨ç†æ•ˆç”¨**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ä¸ªå…³äº\"å¯è§£é‡Šæ€§\"å’Œ\"æ¨ç†æœ‰æ•ˆæ€§\"çš„ç ”ç©¶ã€‚ä½œè€…åˆ©ç”¨**æ¡ä»¶ç†µï¼ˆConditional Entropyï¼‰**æ¥è¡¡é‡ç”Ÿæˆçš„æ¨ç†æ­¥éª¤æ˜¯å¦æœ‰åŠ©äºæœ€ç»ˆç­”æ¡ˆã€‚\n*   **å‘ç°ï¼š** ç†µå€¼éšæ­¥éª¤ä¸‹é™é€šå¸¸é¢„ç¤ºç€æ­£ç¡®ç­”æ¡ˆï¼Œè€Œç†µå€¼å¹³ç¼“æˆ–ä¸Šå‡åˆ™å¾€å¾€å¯¼è‡´é”™è¯¯ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥é€šè¿‡ç›‘æ§ç†µå€¼æ¥æå‰ç»ˆæ­¢æ— æ•ˆçš„æ¨ç†è·¯å¾„ã€‚\n\n---\n\n### ğŸ¤– æ™ºèƒ½ä½“ä¸å…·èº«æ™ºèƒ½ (Agents & Embodied AI)\n\n**4. [æ™ºèƒ½ä½“è®­ç»ƒ] AWorld: Orchestrating the Training Recipe for Agentic AI**\n*   **AWorldï¼šç¼–æ’ Agentic AI çš„è®­ç»ƒé…æ–¹**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** è§£å†³äº†æ™ºèƒ½ä½“åœ¨å¤§è§„æ¨¡äº¤äº’è®­ç»ƒä¸­çš„æ•ˆç‡ç“¶é¢ˆã€‚ä½œè€…å¼€å‘äº† AWorld åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œå°†ç»éªŒæ”¶é›†é€Ÿåº¦æå‡äº† **14.6å€**ã€‚\n*   **å‘ç°ï¼š** åŸºäºæ­¤ç³»ç»Ÿè®­ç»ƒçš„ Qwen3-32B æ™ºèƒ½ä½“åœ¨ GAIA æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº† 32.23% çš„ pass@1 å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº† GPT-4oï¼Œå±•ç¤ºäº†ä»å¤§è§„æ¨¡äº¤äº’ä¸­å­¦ä¹ ï¼ˆLearning from Practiceï¼‰çš„å·¨å¤§æ½œåŠ›ã€‚\n\n**5. [ä¸»åŠ¨å¯¹è¯] ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents**\n*   **ProactiveEvalï¼šä¸»åŠ¨å¯¹è¯æ™ºèƒ½ä½“çš„ç»Ÿä¸€è¯„ä¼°æ¡†æ¶**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** ç°æœ‰çš„è¯„ä¼°å¤šå…³æ³¨è¢«åŠ¨å“åº”ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªè¯„ä¼° LLM **ä¸»åŠ¨æ€§ï¼ˆProactivityï¼‰** çš„æ¡†æ¶ï¼ŒåŒ…æ‹¬ç›®æ ‡è§„åˆ’å’Œå¯¹è¯å¼•å¯¼èƒ½åŠ›ã€‚\n*   **å‘ç°ï¼š** è¯„æµ‹äº† 22 ä¸ª LLMï¼Œå‘ç° **DeepSeek-R1** åœ¨ç›®æ ‡è§„åˆ’ä¸Šè¡¨ç°å“è¶Šï¼Œè€Œ **Claude-3.7-Sonnet** åœ¨å¯¹è¯å¼•å¯¼æ–¹é¢æœ€å¼ºã€‚\n\n**6. [ç§»åŠ¨ç«¯ VLM] MobileCLIP2: Improving Multi-Modal Reinforced Training**\n*   **MobileCLIP2ï¼šæ”¹è¿›å¤šæ¨¡æ€å¼ºåŒ–è®­ç»ƒ**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** æ¥è‡ª Apple çš„å·¥ä½œã€‚é€šè¿‡æ”¹è¿›å¤šæ¨¡æ€å¼ºåŒ–è®­ç»ƒï¼ˆReinforced Trainingï¼‰ç­–ç•¥ï¼ŒåŒ…æ‹¬æ›´å¥½çš„æ•™å¸ˆæ¨¡å‹é›†æˆå’Œå¤šæ ·åŒ–çš„æ•°æ®å¾®è°ƒï¼Œæ¨å‡ºäº† MobileCLIP2ã€‚\n*   **å‘ç°ï¼š** MobileCLIP2-S4 åœ¨ ImageNet-1k ä¸Šçš„é›¶æ ·æœ¬å‡†ç¡®ç‡åŒ¹é…äº† SigLIP-SO400Mï¼Œä½†å‚æ•°é‡å° 2 å€ï¼Œä¸”æ¯” DFN ViT-L/14 å¿« 2.5 å€ï¼Œéå¸¸é€‚åˆç«¯ä¾§éƒ¨ç½²ã€‚\n\n**7. [å…·èº«åŸºç¡€æ¨¡å‹] EO-1: Interleaved Vision-Text-Action Pretraining for General Robot Control**\n*   **EO-1ï¼šç”¨äºé€šç”¨æœºå™¨äººæ§åˆ¶çš„äº¤é”™è§†è§‰-æ–‡æœ¬-åŠ¨ä½œé¢„è®­ç»ƒ**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** å‘å¸ƒäº† EO-1 æ¨¡å‹å’Œ EO-Data1.5M æ•°æ®é›†ã€‚é‡ç‚¹åœ¨äº**äº¤é”™ï¼ˆInterleavedï¼‰**çš„è§†è§‰ã€æ–‡æœ¬å’ŒåŠ¨ä½œåºåˆ—è®­ç»ƒï¼Œé€šè¿‡è‡ªå›å½’è§£ç å’ŒæµåŒ¹é…å»å™ªçš„ååŒä½œç”¨ï¼Œå®ç°äº†æ— ç¼çš„æœºå™¨äººåŠ¨ä½œç”Ÿæˆã€‚\n\n---\n\n### ğŸ”¬ AI for Science & Medicine (ç§‘å­¦ä¸åŒ»ç–—)\n\n**8. [ç»¼è¿°] A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers**\n*   **ç§‘å­¦å¤§è¯­è¨€æ¨¡å‹ç»¼è¿°ï¼šä»æ•°æ®åŸºç¡€åˆ°æ™ºèƒ½ä½“å‰æ²¿**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** **å¼ºçƒˆæ¨èé˜…è¯»**ã€‚è¿™æ˜¯ä¸€ç¯‡éå¸¸å…¨é¢çš„ç»¼è¿°ï¼Œé‡æ–°æ„å»ºäº†ç§‘å­¦æ•°æ®çš„åˆ†ç±»ä½“ç³»ï¼Œå›é¡¾äº†å„ç±» Sci-LLMsï¼Œåˆ†æäº† 270+ ä¸ªæ•°æ®é›†å’Œ 190+ ä¸ªåŸºå‡†ã€‚\n*   **è§‚ç‚¹ï¼š** æ–‡ç« æŒ‡å‡ºæœªæ¥çš„æ–¹å‘æ˜¯åŸºäº Sci-LLM çš„**è‡ªä¸»æ™ºèƒ½ä½“**ï¼Œå®ƒä»¬èƒ½ä¸»åŠ¨å®éªŒã€éªŒè¯å¹¶è´¡çŒ®æ–°çŸ¥è¯†ï¼Œå½¢æˆé—­ç¯ç³»ç»Ÿã€‚\n\n**9. [åŒ»ç–— RL] MedGR$^2$: Breaking the Data Barrier for Medical Reasoning via Generative Reward Learning**\n*   **MedGR$^2$ï¼šé€šè¿‡ç”Ÿæˆå¼å¥–åŠ±å­¦ä¹ æ‰“ç ´åŒ»ç–—æ¨ç†çš„æ•°æ®å£å’**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹åŒ»ç–—é¢†åŸŸé«˜è´¨é‡æ•°æ®ç¨€ç¼ºå’Œ RL å¥–åŠ±ä¿¡å·éš¾å®šä¹‰çš„é—®é¢˜ï¼Œæå‡ºäº†ç”Ÿæˆå¼å¥–åŠ±å­¦ä¹ æ¡†æ¶ã€‚\n*   **å‘ç°ï¼š** è¯¥æ¡†æ¶èƒ½è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ¨¡æ€çš„åŒ»ç–—æ•°æ®ï¼Œæ—¢ä½œä¸º SFT çš„è®­ç»ƒæºï¼Œä¹Ÿä½œä¸º RL çš„å¥–åŠ±æ¨¡å‹ã€‚å®éªŒè¡¨æ˜å…¶ç”Ÿæˆçš„ç­–ç•¥åœ¨è·¨æ¨¡æ€å’Œè·¨ä»»åŠ¡æ³›åŒ–ä¸Šè¾¾åˆ°äº† SOTAã€‚\n\n**10. [ç”Ÿç‰©ç«èµ›] Overview of BioASQ 2025**\n*   **BioASQ 2025 æ¦‚è§ˆï¼šå¤§è§„æ¨¡ç”Ÿç‰©åŒ»å­¦è¯­ä¹‰ç´¢å¼•ä¸é—®ç­”æŒ‘æˆ˜èµ›**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** å¹´åº¦æ€»ç»“ã€‚ä»Šå¹´çš„æ–°ä»»åŠ¡åŒ…æ‹¬å¤šè¯­è¨€ä¸´åºŠæ‘˜è¦ã€åµŒå¥—å‘½åå®ä½“é“¾æ¥ï¼ˆBio",
  "papers": [
    {
      "arxiv_id": "2509.05311v1",
      "title": "Large Language Model Integration with Reinforcement Learning to Augment Decision-Making in Autonomous Cyber Operations",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ é›†æˆï¼šå¢å¼ºè‡ªä¸»ç½‘ç»œè¡ŒåŠ¨ä¸­çš„å†³ç­–èƒ½åŠ›",
      "authors": [
        "Konur Tholl",
        "FranÃ§ois Rivest",
        "Mariam El Mezouar",
        "Ranwa Al Mallah"
      ],
      "abstract": "Reinforcement Learning (RL) has shown great potential for autonomous decision-making in the cybersecurity domain, enabling agents to learn through direct environment interaction. However, RL agents in Autonomous Cyber Operations (ACO) typically learn from scratch, requiring them to execute undesirable actions to learn their consequences. In this study, we integrate external knowledge in the form of a Large Language Model (LLM) pretrained on cybersecurity data that our RL agent can directly leverage to make informed decisions. By guiding initial training with an LLM, we improve baseline performance and reduce the need for exploratory actions with obviously negative outcomes. We evaluate our LLM-integrated approach in a simulated cybersecurity environment, and demonstrate that our guided agent achieves over 2x higher rewards during early training and converges to a favorable policy approximately 4,500 episodes faster than the baseline.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°†å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Model)ä¸å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)é›†æˆï¼Œä»¥å¢å¼ºè‡ªä¸»ç½‘ç»œè¡ŒåŠ¨(Autonomous Cyber Operations)ä¸­çš„å†³ç­–èƒ½åŠ›ã€‚é’ˆå¯¹å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“åœ¨è®­ç»ƒåˆæœŸé€šå¸¸ä»é›¶å¼€å§‹å­¦ä¹ å¹¶æ‰§è¡Œå¤§é‡è´Ÿé¢æ¢ç´¢åŠ¨ä½œçš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†é¢„è®­ç»ƒäºç½‘ç»œå®‰å…¨æ•°æ®çš„è¯­è¨€æ¨¡å‹æ¥æä¾›å¤–éƒ¨çŸ¥è¯†å¼•å¯¼ã€‚é€šè¿‡åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†æŒ‡å¯¼åˆæœŸè®­ç»ƒï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆæå‡äº†åŸºçº¿æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†äº§ç”Ÿè´Ÿé¢åæœçš„æ¢ç´¢è¡Œä¸ºã€‚åœ¨æ¨¡æ‹Ÿç½‘ç»œå®‰å…¨ç¯å¢ƒä¸­çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œé›†æˆè¯­è¨€æ¨¡å‹çš„å¼•å¯¼æ™ºèƒ½ä½“åœ¨è®­ç»ƒåˆæœŸçš„å¥–åŠ±å€¼æ¯”åŸºçº¿æ¨¡å‹é«˜å‡º2å€ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹æ¯”ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•æå‰çº¦4,500ä¸ªå›åˆå®ç°ç­–ç•¥æ”¶æ•›ï¼Œä¸ºå¼€å‘æ›´é«˜æ•ˆã€æ›´å¯é çš„è‡ªä¸»ç½‘ç»œé˜²å¾¡ç³»ç»Ÿæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05311v1",
      "published_date": "2025-08-28 23:52:03 UTC",
      "updated_date": "2025-08-28 23:52:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:44:48.097870+00:00"
    },
    {
      "arxiv_id": "2509.02589v1",
      "title": "Normal and Atypical Mitosis Image Classifier using Efficient Vision Transformer",
      "title_zh": "åŸºäº Efficient Vision Transformer çš„æ­£å¸¸ä¸éå…¸å‹æœ‰ä¸åˆ†è£‚å›¾åƒåˆ†ç±»å™¨",
      "authors": [
        "Xuan Qi",
        "Dominic Labella",
        "Thomas Sanford",
        "Maxwell Lee"
      ],
      "abstract": "We tackle atypical versus normal mitosis classification in the MIDOG 2025 challenge using EfficientViT-L2, a hybrid CNN--ViT architecture optimized for accuracy and efficiency. A unified dataset of 13,938 nuclei from seven cancer types (MIDOG++ and AMi-Br) was used, with atypical mitoses comprising ~15. To assess domain generalization, we applied leave-one-cancer-type-out cross-validation with 5-fold ensembles, using stain-deconvolution for image augmentation. For challenge submissions, we trained an ensemble with the same 5-fold split but on all cancer types. In the preliminary evaluation phase, this model achieved balanced accuracy of 0.859, ROC AUC of 0.942, and raw accuracy of 0.85, demonstrating competitive and well-balanced performance across metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹MIDOG 2025æŒ‘æˆ˜èµ›ä¸­çš„éå…¸å‹ä¸æ­£å¸¸æœ‰ä¸åˆ†è£‚åˆ†ç±»ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§åŸºäºEfficientViT-L2çš„å›¾åƒåˆ†ç±»æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆé‡‡ç”¨æ··åˆCNN-ViTæ¶æ„ï¼Œæ—¨åœ¨å¹³è¡¡ç—…ç†å›¾åƒè¯†åˆ«çš„å‡†ç¡®ç‡ä¸è®¡ç®—æ•ˆç‡ã€‚ç ”ç©¶æ•´åˆäº†æ¥è‡ªMIDOG++å’ŒAMi-Brçš„13,938ä¸ªç»†èƒæ ¸å›¾åƒï¼Œæ¶µç›–ä¸ƒç§ç™Œç—‡ç±»å‹ï¼Œå¹¶åˆ©ç”¨stain-deconvolutionæŠ€æœ¯è¿›è¡Œå›¾åƒå¢å¼ºã€‚ä¸ºäº†éªŒè¯æ¨¡å‹çš„é¢†åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œå›¢é˜Ÿé‡‡ç”¨äº†leave-one-cancer-type-out cross-validationå’Œ5-fold ensemblesè®­ç»ƒç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨åˆæ­¥è¯„ä¼°é˜¶æ®µå–å¾—äº†0.859çš„balanced accuracyå’Œ0.942çš„ROC AUCï¼Œè¯æ˜äº†å…¶åœ¨å¤šé¡¹è¯„ä»·æŒ‡æ ‡ä¸Šçš„ç«äº‰ä¼˜åŠ¿ã€‚è¯¥æ–¹æ³•ä¸ºè¾…åŠ©ç—…ç†è¯Šæ–­æä¾›äº†ä¸€ä¸ªé²æ£’ä¸”é«˜æ•ˆçš„è‡ªåŠ¨åŒ–åˆ†æå·¥å…·ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "for grandchallenge midog 2025 track 2 abstract",
      "pdf_url": "https://arxiv.org/pdf/2509.02589v1",
      "published_date": "2025-08-28 23:45:06 UTC",
      "updated_date": "2025-08-28 23:45:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:44:58.182102+00:00"
    },
    {
      "arxiv_id": "2508.21263v1",
      "title": "Deep Active Learning for Lung Disease Severity Classification from Chest X-rays: Learning with Less Data in the Presence of Class Imbalance",
      "title_zh": "ç”¨äºèƒ¸éƒ¨Xå°„çº¿å½±åƒè‚ºéƒ¨ç–¾ç—…ä¸¥é‡ç¨‹åº¦åˆ†ç±»çš„æ·±åº¦ä¸»åŠ¨å­¦ä¹ ï¼šç±»åˆ«ä¸å¹³è¡¡ç¯å¢ƒä¸‹çš„ä½æ•°æ®é‡å­¦ä¹ ",
      "authors": [
        "Roy M. Gabriel",
        "Mohammadreza Zandehshahvar",
        "Marly van Assen",
        "Nattakorn Kittisut",
        "Kyle Peters",
        "Carlo N. De Cecco",
        "Ali Adibi"
      ],
      "abstract": "To reduce the amount of required labeled data for lung disease severity classification from chest X-rays (CXRs) under class imbalance, this study applied deep active learning with a Bayesian Neural Network (BNN) approximation and weighted loss function. This retrospective study collected 2,319 CXRs from 963 patients (mean age, 59.2 $\\pm$ 16.6 years; 481 female) at Emory Healthcare affiliated hospitals between January and November 2020. All patients had clinically confirmed COVID-19. Each CXR was independently labeled by 3 to 6 board-certified radiologists as normal, moderate, or severe. A deep neural network with Monte Carlo Dropout was trained using active learning to classify disease severity. Various acquisition functions were used to iteratively select the most informative samples from an unlabeled pool. Performance was evaluated using accuracy, area under the receiver operating characteristic curve (AU ROC), and area under the precision-recall curve (AU PRC). Training time and acquisition time were recorded. Statistical analysis included descriptive metrics and performance comparisons across acquisition strategies. Entropy Sampling achieved 93.7% accuracy (AU ROC, 0.91) in binary classification (normal vs. diseased) using 15.4% of the training data. In the multi-class setting, Mean STD sampling achieved 70.3% accuracy (AU ROC, 0.86) using 23.1% of the labeled data. These methods outperformed more complex and computationally expensive acquisition functions and significantly reduced labeling needs. Deep active learning with BNN approximation and weighted loss effectively reduces labeled data requirements while addressing class imbalance, maintaining or exceeding diagnostic performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆè´å¶æ–¯ç¥ç»ç½‘ç»œï¼ˆBayesian Neural Network, BNNï¼‰è¿‘ä¼¼ä¸åŠ æƒæŸå¤±å‡½æ•°ï¼ˆweighted loss functionï¼‰çš„æ·±åº¦ä¸»åŠ¨å­¦ä¹ ï¼ˆDeep Active Learningï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³èƒ¸éƒ¨Xå°„çº¿ï¼ˆChest X-rays, CXRsï¼‰è‚ºéƒ¨ç–¾ç—…ä¸¥é‡ç¨‹åº¦åˆ†ç±»ä¸­æ•°æ®æ ‡æ³¨æˆæœ¬é«˜å’Œç±»åˆ«ä¸å¹³è¡¡ï¼ˆclass imbalanceï¼‰çš„æŒ‘æˆ˜ã€‚ç ”ç©¶è€…åˆ©ç”¨è’™ç‰¹å¡æ´›éšæœºå¤±æ´»ï¼ˆMonte Carlo Dropoutï¼‰æŠ€æœ¯æ¨¡æ‹Ÿä¸ç¡®å®šæ€§ï¼Œå¹¶é€šè¿‡ä¸åŒçš„é‡‡é›†å‡½æ•°è¿­ä»£ç­›é€‰æœ€å…·ä»£è¡¨æ€§çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨äºŒåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œä¿¡æ¯ç†µé‡‡æ ·ï¼ˆEntropy Samplingï¼‰ä»…éœ€15.4%çš„è®­ç»ƒæ•°æ®å³å¯è¾¾åˆ°93.7%çš„å‡†ç¡®ç‡ï¼ˆAU ROCä¸º0.91ï¼‰ï¼›è€Œåœ¨å¤šåˆ†ç±»è®¾ç½®ä¸‹ï¼ŒMean STDé‡‡æ ·ä»…ç”¨23.1%çš„æ•°æ®ä¾¿å®ç°äº†70.3%çš„å‡†ç¡®ç‡ã€‚è¯¥æ–¹æ³•åœ¨æ˜¾è‘—å‡å°‘ä¸´åºŠä¸“å®¶æ ‡æ³¨å·¥ä½œé‡çš„åŒæ—¶ï¼Œä¾ç„¶ç»´æŒæˆ–æå‡äº†è¯Šæ–­æ€§èƒ½ï¼Œè¯æ˜äº†ä¸»åŠ¨å­¦ä¹ åœ¨å—é™æ ‡æ³¨èµ„æºä¸‹å¤„ç†å¤æ‚åŒ»ç–—å½±åƒä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21263v1",
      "published_date": "2025-08-28 23:29:56 UTC",
      "updated_date": "2025-08-28 23:29:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:44:58.884204+00:00"
    },
    {
      "arxiv_id": "2508.21259v1",
      "title": "Breaking the Cold-Start Barrier: Reinforcement Learning with Double and Dueling DQNs",
      "title_zh": "ç ´è§£å†·å¯åŠ¨éšœç¢ï¼šåŸºäº Double ä¸ Dueling DQN çš„å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Minda Zhao"
      ],
      "abstract": "Recommender systems struggle to provide accurate suggestions to new users with limited interaction history, a challenge known as the cold-user problem. This paper proposes a reinforcement learning approach using Double and Dueling Deep Q-Networks (DQN) to dynamically learn user preferences from sparse feedback, enhancing recommendation accuracy without relying on sensitive demographic data. By integrating these advanced DQN variants with a matrix factorization model, we achieve superior performance on a large e-commerce dataset compared to traditional methods like popularity-based and active learning strategies. Experimental results show that our method, particularly Dueling DQN, reduces Root Mean Square Error (RMSE) for cold users, offering an effective solution for privacy-constrained environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨èç³»ç»Ÿä¸­æ–°ç”¨æˆ·äº¤äº’å†å²æœ‰é™å¯¼è‡´çš„å†·å¯åŠ¨é—®é¢˜ï¼ˆcold-user problemï¼‰ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•åˆ©ç”¨ Double Deep Q-Networks (Double DQN) å’Œ Dueling Deep Q-Networks (Dueling DQN) ä»ç¨€ç–åé¦ˆä¸­åŠ¨æ€å­¦ä¹ ç”¨æˆ·åå¥½ï¼Œæœ‰æ•ˆé¿å…äº†å¯¹æ•æ„Ÿäººå£ç»Ÿè®¡å­¦æ•°æ®çš„ä¾èµ–ã€‚é€šè¿‡å°†è¿™äº›å…ˆè¿›çš„ DQN å˜ä½“ä¸çŸ©é˜µåˆ†è§£ï¼ˆMatrix Factorizationï¼‰æ¨¡å‹ç›¸ç»“åˆï¼Œè¯¥æ¡†æ¶åœ¨å¤§å‹ç”µå­å•†åŠ¡æ•°æ®é›†ä¸Šå±•ç°äº†ä¼˜äºä¼ ç»Ÿæµè¡Œåº¦åŸºç¡€ï¼ˆpopularity-basedï¼‰å’Œä¸»åŠ¨å­¦ä¹ ï¼ˆactive learningï¼‰ç­–ç•¥çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¯å®ï¼Œè¯¥æ–¹æ³•ç‰¹åˆ«æ˜¯ Dueling DQN æ˜¾è‘—é™ä½äº†å†·å¯åŠ¨ç”¨æˆ·çš„å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰ï¼Œä¸ºéšç§å—é™ç¯å¢ƒä¸‹çš„ç²¾å‡†æ¨èæä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21259v1",
      "published_date": "2025-08-28 23:14:07 UTC",
      "updated_date": "2025-08-28 23:14:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:44:57.487622+00:00"
    },
    {
      "arxiv_id": "2509.04466v3",
      "title": "Just-in-time and distributed task representations in language models",
      "title_zh": "è¯­è¨€æ¨¡å‹ä¸­çš„å³æ—¶ä¸åˆ†å¸ƒå¼ä»»åŠ¡è¡¨ç¤º",
      "authors": [
        "Yuxuan Li",
        "Declan Campbell",
        "Stephanie C. Y. Chan",
        "Andrew Kyle Lampinen"
      ],
      "abstract": "Many of language models' impressive capabilities originate from their in-context learning: based on instructions or examples, they can infer and perform new tasks without weight updates. In this work, we investigate when representations for new tasks are formed in language models, and how these representations change over the course of context. We study two different task representations: those that are ''transferrable'' -- vector representations that can transfer task contexts to another model instance, even without the full prompt -- and simpler representations of high-level task categories. We show that transferrable task representations evolve in non-monotonic and sporadic ways, while task identity representations persist throughout the context. Specifically, transferrable task representations exhibit a two-fold locality. They successfully condense evidence when more examples are provided in the context. But this evidence accrual process exhibits strong temporal locality along the sequence dimension, coming online only at certain tokens -- despite task identity being reliably decodable throughout the context. In some cases, transferrable task representations also show semantic locality, capturing a small task ''scope'' such as an independent subtask. Language models thus represent new tasks on the fly through both an inert, sustained sensitivity to the task and an active, just-in-time representation to support inference.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ¨¡å‹åœ¨è¯­å¢ƒå­¦ä¹ (in-context learning)è¿‡ç¨‹ä¸­å¦‚ä½•å½¢æˆå’Œæ¼”å˜æ–°ä»»åŠ¡çš„è¡¨å¾ï¼Œé‡ç‚¹åˆ†æäº†ä»»åŠ¡è¡¨å¾åœ¨æ¨ç†æ—¶çš„åŠ¨æ€å˜åŒ–ã€‚ä½œè€…åŒºåˆ†äº†ä¸¤ç§ä¸åŒçš„è¡¨å¾å½¢å¼ï¼šä¸€ç§æ˜¯å¯ä»¥å°†ä»»åŠ¡ä¿¡æ¯ä¼ é€’ç»™å…¶ä»–æ¨¡å‹å®ä¾‹çš„â€œå¯è¿ç§»â€ä»»åŠ¡è¡¨å¾(transferrable task representations)ï¼Œå¦ä¸€ç§æ˜¯æ›´é«˜å±‚çº§çš„ä»»åŠ¡ç±»åˆ«è¡¨å¾(task identity representations)ã€‚å®éªŒå‘ç°ï¼Œä»»åŠ¡ç±»åˆ«è¡¨å¾åœ¨æ•´ä¸ªè¯­å¢ƒä¸­æŒç»­å­˜åœ¨ï¼Œè€Œå¯è¿ç§»ä»»åŠ¡è¡¨å¾åˆ™è¡¨ç°å‡ºéå•è°ƒä¸”é›¶æ˜Ÿçš„æ¼”å˜ç‰¹å¾ã€‚å¯è¿ç§»è¡¨å¾å±•ç¤ºäº†æ˜æ˜¾çš„åŒé‡å±€éƒ¨æ€§(two-fold locality)ï¼Œå³åœ¨åºåˆ—ç»´åº¦ä¸Šä»…åœ¨ç‰¹å®šæ ‡è®°(tokens)å¤„è¢«æ¿€æ´»ï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°å‡ºæ•è·è¾ƒå°ä»»åŠ¡èŒƒå›´çš„è¯­ä¹‰å±€éƒ¨æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œè¯­è¨€æ¨¡å‹é€šè¿‡ä¸€ç§æŒç»­çš„ä»»åŠ¡æ•æ„Ÿæ€§ä¸ä¸€ç§ä¸ºäº†æ”¯æŒæ¨ç†è€Œäº§ç”Ÿçš„å³æ—¶(just-in-time)è¡¨å¾ç›¸ç»“åˆçš„æ–¹å¼ï¼Œåœ¨å¤„ç†æ–°ä»»åŠ¡æ—¶å®ç°äº†çµæ´»æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04466v3",
      "published_date": "2025-08-28 23:08:32 UTC",
      "updated_date": "2025-12-01 23:48:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:45:07.387920+00:00"
    },
    {
      "arxiv_id": "2509.04465v1",
      "title": "Emotionally-Aware Agents for Dispute Resolution",
      "title_zh": "é¢å‘äº‰è®®è§£å†³çš„æƒ…æ„Ÿæ„ŸçŸ¥æ™ºèƒ½ä½“",
      "authors": [
        "Sushrita Rakshit",
        "James Hale",
        "Kushal Chawla",
        "Jeanne M. Brett",
        "Jonathan Gratch"
      ],
      "abstract": "In conflict, people use emotional expressions to shape their counterparts' thoughts, feelings, and actions. This paper explores whether automatic text emotion recognition offers insight into this influence in the context of dispute resolution. Prior work has shown the promise of such methods in negotiations; however, disputes evoke stronger emotions and different social processes. We use a large corpus of buyer-seller dispute dialogues to investigate how emotional expressions shape subjective and objective outcomes. We further demonstrate that large-language models yield considerably greater explanatory power than previous methods for emotion intensity annotation and better match the decisions of human annotators. Findings support existing theoretical models for how emotional expressions contribute to conflict escalation and resolution and suggest that agent-based systems could be useful in managing disputes by recognizing and potentially mitigating emotional escalation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªåŠ¨æ–‡æœ¬æƒ…æ„Ÿè¯†åˆ«(automatic text emotion recognition)åœ¨çº çº·è§£å†³(dispute resolution)åœºæ™¯ä¸­è¯†åˆ«æƒ…æ„Ÿè¡¨è¾¾å½±å“çš„æ½œåŠ›ã€‚é’ˆå¯¹çº çº·æ¯”ä¼ ç»Ÿè°ˆåˆ¤(negotiation)å…·æœ‰æ›´å¼ºæƒ…æ„Ÿå†²å‡»å’Œä¸åŒç¤¾ä¼šè¿‡ç¨‹çš„ç‰¹ç‚¹ï¼Œç ”ç©¶åˆ©ç”¨å¤§è§„æ¨¡ä¹°å–çº çº·å¯¹è¯è¯­æ–™åº“ï¼Œæ·±å…¥è°ƒæŸ¥äº†æƒ…æ„Ÿè¡¨è¾¾å¦‚ä½•å¡‘é€ ä¸»è§‚ä¸å®¢è§‚ç»“æœã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œå¤§è¯­è¨€æ¨¡å‹(large-language models)åœ¨æƒ…æ„Ÿå¼ºåº¦æ ‡æ³¨(emotion intensity annotation)æ–¹é¢çš„è§£é‡ŠåŠ›æ˜¾è‘—ä¼˜äºå…ˆå‰æ–¹æ³•ï¼Œä¸”ä¸äººç±»æ ‡æ³¨è€…çš„å†³ç­–é«˜åº¦åŒ¹é…ã€‚å®éªŒç»“æœæ”¯æŒäº†å…³äºæƒ…æ„Ÿè¡¨è¾¾å¦‚ä½•å¯¼è‡´å†²çªå‡çº§(conflict escalation)ä¸åŒ–è§£(resolution)çš„ç°æœ‰ç†è®ºæ¨¡å‹ã€‚è¯¥ç ”ç©¶æœ€ç»ˆæŒ‡å‡ºï¼ŒåŸºäºæ™ºèƒ½ä½“çš„ç³»ç»Ÿ(agent-based systems)æœ‰æœ›é€šè¿‡è¯†åˆ«å¹¶æ½œåœ¨åœ°ç¼“è§£æƒ…æ„Ÿå‡çº§ï¼Œåœ¨çº çº·ç®¡ç†ä¸­å‘æŒ¥å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04465v1",
      "published_date": "2025-08-28 22:52:10 UTC",
      "updated_date": "2025-08-28 22:52:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:45:12.992507+00:00"
    },
    {
      "arxiv_id": "2508.21253v1",
      "title": "Reinforcement Learning for Optimizing Large Qubit Array based Quantum Sensor Circuits",
      "title_zh": "åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤§å‹é‡å­æ¯”ç‰¹é˜µåˆ—é‡å­ä¼ æ„Ÿå™¨ç”µè·¯ä¼˜åŒ–",
      "authors": [
        "Laxmisha Ashok Attisara",
        "Sathish Kumar"
      ],
      "abstract": "As the number of qubits in a sensor increases, the complexity of designing and controlling the quantum circuits grows exponentially. Manually optimizing these circuits becomes infeasible. Optimizing entanglement distribution in large-scale quantum circuits is critical for enhancing the sensitivity and efficiency of quantum sensors [5], [6]. This paper presents an engineering integration of reinforcement learning with tensor-network-based simulation (MPS) for scalable circuit optimization for optimizing quantum sensor circuits with up to 60 qubits. To enable efficient simulation and scalability, we adopt tensor network methods, specifically the Matrix Product State (MPS) representation, instead of traditional state vector or density matrix approaches. Our reinforcement learning agent learns to restructure circuits to maximize Quantum Fisher Information (QFI) and entanglement entropy while reducing gate counts and circuit depth. Experimental results show consistent improvements, with QFI values approaching 1, entanglement entropy in the 0.8-1.0 range, and up to 90% reduction in depth and gate count. These results highlight the potential of combining quantum machine learning and tensor networks to optimize complex quantum circuits under realistic constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é‡å­ä¼ æ„Ÿå™¨ä¸­é‡å­æ¯”ç‰¹æ•°é‡å¢åŠ å¯¼è‡´ç”µè·¯è®¾è®¡ä¸æ§åˆ¶å¤æ‚æ€§å‘ˆæŒ‡æ•°çº§å¢é•¿çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å°† Reinforcement Learning ä¸åŸºäº Tensor Network çš„ Matrix Product State (MPS) æ¨¡æ‹Ÿç›¸ç»“åˆçš„å·¥ç¨‹é›†æˆæ–¹æ¡ˆã€‚è¯¥æ–¹æ³•æ—¨åœ¨ä¼˜åŒ–å¤§è§„æ¨¡é‡å­ä¼ æ„Ÿå™¨ç”µè·¯ï¼ˆæœ€é«˜å¯è¾¾ 60 ä¸ªé‡å­æ¯”ç‰¹ï¼‰ä¸­çš„çº ç¼ åˆ†å¸ƒï¼Œä»¥æå‡ä¼ æ„Ÿå™¨çš„çµæ•åº¦å’Œæ•ˆç‡ã€‚ç ”ç©¶é‡‡ç”¨ MPS è¡¨ç¤ºæ³•æ›¿ä»£ä¼ ç»Ÿçš„ State Vector æˆ– Density Matrix æ–¹æ³•ï¼Œä»è€Œå®ç°äº†é«˜æ•ˆçš„æ¨¡æ‹Ÿä¸å¯æ‰©å±•æ€§ã€‚Reinforcement Learning æ™ºèƒ½ä½“é€šè¿‡å­¦ä¹ é‡æ„ç”µè·¯ï¼Œåœ¨æœ€å¤§åŒ– Quantum Fisher Information (QFI) å’Œ Entanglement Entropy çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº† Gate Counts å’Œ Circuit Depthã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQFI å€¼æ¥è¿‘ 1ï¼ŒEntanglement Entropy è¾¾åˆ° 0.8-1.0ï¼Œä¸” Circuit Depth å’Œ Gate Counts æœ€é«˜å¯å‡å°‘ 90%ã€‚è¯¥æˆæœå±•ç¤ºäº†ç»“åˆ Quantum Machine Learning ä¸ Tensor Networks åœ¨ç°å®çº¦æŸä¸‹ä¼˜åŒ–å¤æ‚é‡å­ç”µè·¯çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "10 pages, 13 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.21253v1",
      "published_date": "2025-08-28 22:51:28 UTC",
      "updated_date": "2025-08-28 22:51:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:45:34.192981+00:00"
    },
    {
      "arxiv_id": "2508.21252v1",
      "title": "Quantum Machine Learning for Optimizing Entanglement Distribution in Quantum Sensor Circuits",
      "title_zh": "ç”¨äºä¼˜åŒ–é‡å­ä¼ æ„Ÿå™¨ç”µè·¯çº ç¼ åˆ†å¸ƒçš„é‡å­æœºå™¨å­¦ä¹ ",
      "authors": [
        "Laxmisha Ashok Attisara",
        "Sathish Kumar"
      ],
      "abstract": "In the rapidly evolving field of quantum computing, optimizing quantum circuits for specific tasks is crucial for enhancing performance and efficiency. More recently, quantum sensing has become a distinct and rapidly growing branch of research within the area of quantum science and technology. The field is expected to provide new opportunities, especially regarding high sensitivity and precision. Entanglement is one of the key factors in achieving high sensitivity and measurement precision [3]. This paper presents a novel approach utilizing quantum machine learning techniques to optimize entanglement distribution in quantum sensor circuits. By leveraging reinforcement learning within a quantum environment, we aim to optimize the entanglement layout to maximize Quantum Fisher Information (QFI) and entanglement entropy, which are key indicators of a quantum system's sensitivity and coherence, while minimizing circuit depth and gate counts. Our implementation, based on Qiskit, integrates noise models and error mitigation strategies to simulate realistic quantum environments. The results demonstrate significant improvements in circuit performance and sensitivity, highlighting the potential of machine learning in quantum circuit optimization by measuring high QFI and entropy in the range of 0.84-1.0 with depth and gate count reduction by 20-86%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é‡å­ä¼ æ„Ÿé¢†åŸŸä¸­é‡å­ç”µè·¯ä¼˜åŒ–çš„éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨é‡å­æœºå™¨å­¦ä¹ (Quantum Machine Learning)ä¼˜åŒ–é‡å­ä¼ æ„Ÿå™¨ç”µè·¯ä¸­çº ç¼ åˆ†å¸ƒ(Entanglement Distribution)çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨é‡å­ç¯å¢ƒä¸­åº”ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æŠ€æœ¯æ¥ä¼˜åŒ–çº ç¼ å¸ƒå±€ï¼Œæ—¨åœ¨æœ€å¤§åŒ–é‡å­è´¹èˆå°”ä¿¡æ¯(Quantum Fisher Information, QFI)å’Œçº ç¼ ç†µ(Entanglement Entropy)ä»¥æå‡ç³»ç»Ÿçš„çµæ•åº¦ä¸ç›¸å¹²æ€§ï¼ŒåŒæ—¶å…¼é¡¾æœ€å°åŒ–ç”µè·¯æ·±åº¦(Circuit Depth)å’Œé—¨æ•°é‡(Gate Counts)ã€‚ç ”ç©¶åœ¨Qiskitæ¡†æ¶ä¸‹é›†æˆäº†å™ªå£°æ¨¡å‹å’Œè¯¯å·®ç¼“è§£ç­–ç•¥ä»¥æ¨¡æ‹ŸçœŸå®ç¯å¢ƒï¼Œå®éªŒç»“æœæ˜¾ç¤ºQFIä¸çº ç¼ ç†µè¾¾åˆ°äº†0.84-1.0çš„é«˜æ°´å¹³ï¼Œå¹¶å®ç°äº†20%-86%çš„ç”µè·¯è§„æ¨¡ç¼©å‡ã€‚è¿™é¡¹å·¥ä½œæ˜¾è‘—å¢å¼ºäº†ç”µè·¯æ€§èƒ½ï¼Œè¯æ˜äº†æœºå™¨å­¦ä¹ åœ¨æé«˜é‡å­æµ‹é‡ç²¾åº¦å’Œä¼˜åŒ–å¤æ‚é‡å­ç”µè·¯æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "11 pages, 13 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.21252v1",
      "published_date": "2025-08-28 22:45:54 UTC",
      "updated_date": "2025-08-28 22:45:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:45:20.095298+00:00"
    },
    {
      "arxiv_id": "2508.21249v1",
      "title": "A Mixture of Experts Gating Network for Enhanced Surrogate Modeling in External Aerodynamics",
      "title_zh": "ç”¨äºå¢å¼ºå¤–éƒ¨ç©ºæ°”åŠ¨åŠ›å­¦ä»£ç†å»ºæ¨¡çš„æ··åˆä¸“å®¶é—¨æ§ç½‘ç»œ",
      "authors": [
        "Mohammad Amin Nabian",
        "Sanjay Choudhry"
      ],
      "abstract": "The computational cost associated with high-fidelity CFD simulations remains a significant bottleneck in the automotive design and optimization cycle. While ML-based surrogate models have emerged as a promising alternative to accelerate aerodynamic predictions, the field is characterized by a diverse and rapidly evolving landscape of specialized neural network architectures, with no single model demonstrating universal superiority. This paper introduces a novel meta-learning framework that leverages this architectural diversity as a strength. We propose a Mixture of Experts (MoE) model that employs a dedicated gating network to dynamically and optimally combine the predictions from three heterogeneous, state-of-the-art surrogate models: DoMINO, a decomposable multi-scale neural operator; X-MeshGraphNet, a scalable multi-scale graph neural network; and FigConvNet, a factorized implicit global convolution network. The gating network learns a spatially-variant weighting strategy, assigning credibility to each expert based on its localized performance in predicting surface pressure and wall shear stress fields. To prevent model collapse and encourage balanced expert contributions, we integrate an entropy regularization term into the training loss function. The entire system is trained and validated on the DrivAerML dataset, a large-scale, public benchmark of high-fidelity CFD simulations for automotive aerodynamics. Quantitative results demonstrate that the MoE model achieves a significant reduction in L-2 prediction error, outperforming not only the ensemble average but also the most accurate individual expert model across all evaluated physical quantities. This work establishes the MoE framework as a powerful and effective strategy for creating more robust and accurate composite surrogate models by synergistically combining the complementary strengths of specialized architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ··åˆä¸“å®¶æ¨¡å‹ (Mixture of Experts, MoE) çš„å…ƒå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ±½è½¦å¤–éƒ¨ç©ºæ°”åŠ¨åŠ›å­¦ä¸­é«˜ä¿çœŸ CFD æ¨¡æ‹Ÿè®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸€ä¸ªä¸“é—¨çš„é—¨æ§ç½‘ç»œï¼ŒåŠ¨æ€ä¸”ä¼˜åŒ–åœ°ç»“åˆäº† DoMINOã€X-MeshGraphNet å’Œ FigConvNet ä¸‰ç§å¼‚æ„ä¸”å…ˆè¿›çš„ä»£ç†æ¨¡å‹ã€‚é—¨æ§ç½‘ç»œå­¦ä¹ ä¸€ç§ç©ºé—´å˜é‡åŠ æƒç­–ç•¥ï¼Œæ ¹æ®å„ä¸“å®¶æ¨¡å‹åœ¨é¢„æµ‹è¡¨é¢å‹åŠ› (surface pressure) å’Œå£é¢å‰ªåˆ‡åº”åŠ› (wall shear stress) åœºæ—¶çš„å±€éƒ¨è¡¨ç°æ¥åˆ†é…å¯ä¿¡åº¦ã€‚ä¸ºäº†é˜²æ­¢æ¨¡å‹å´©æºƒå¹¶ç¡®ä¿å„ä¸“å®¶è´¡çŒ®çš„å¹³è¡¡ï¼Œç ”ç©¶åœ¨è®­ç»ƒæŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†ç†µæ­£åˆ™åŒ– (entropy regularization) é¡¹ã€‚åœ¨ DrivAerML å¤§è§„æ¨¡å…¬å…±åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ MoE æ¨¡å‹æ˜¾è‘—é™ä½äº† L-2 é¢„æµ‹è¯¯å·®ï¼Œå…¶æ€§èƒ½ä¸ä»…ä¼˜äºå•ä¸ªä¸“å®¶æ¨¡å‹ï¼Œä¹Ÿè¶…è¿‡äº†é›†æˆå¹³å‡å€¼ã€‚è¯¥å·¥ä½œè¯æ˜äº† MoE æ¡†æ¶èƒ½å¤Ÿé€šè¿‡ååŒä¸åŒä¸“é—¨æ¶æ„çš„äº’è¡¥ä¼˜åŠ¿ï¼Œä¸ºæ„å»ºæ›´é²æ£’ã€æ›´ç²¾ç¡®çš„å¤åˆä»£ç†æ¨¡å‹æä¾›å¼ºæœ‰åŠ›çš„ç­–ç•¥ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA",
        "physics.flu-dyn"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21249v1",
      "published_date": "2025-08-28 22:34:10 UTC",
      "updated_date": "2025-08-28 22:34:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:45:24.086487+00:00"
    },
    {
      "arxiv_id": "2508.21248v1",
      "title": "Zero-Shot KWS for Children's Speech using Layer-Wise Features from SSL Models",
      "title_zh": "åŸºäº SSL æ¨¡å‹é€å±‚ç‰¹å¾çš„å„¿ç«¥è¯­éŸ³é›¶æ ·æœ¬å…³é”®è¯æ£€æµ‹ (KWS)",
      "authors": [
        "Subham Kutum",
        "Abhijit Sinha",
        "Hemant Kumar Kathania",
        "Sudarsana Reddy Kadiri",
        "Mahesh Chandra Govil"
      ],
      "abstract": "Numerous methods have been proposed to enhance Keyword Spotting (KWS) in adult speech, but children's speech presents unique challenges for KWS systems due to its distinct acoustic and linguistic characteristics. This paper introduces a zero-shot KWS approach that leverages state-of-the-art self-supervised learning (SSL) models, including Wav2Vec2, HuBERT and Data2Vec. Features are extracted layer-wise from these SSL models and used to train a Kaldi-based DNN KWS system. The WSJCAM0 adult speech dataset was used for training, while the PFSTAR children's speech dataset was used for testing, demonstrating the zero-shot capability of our method. Our approach achieved state-of-the-art results across all keyword sets for children's speech. Notably, the Wav2Vec2 model, particularly layer 22, performed the best, delivering an ATWV score of 0.691, a MTWV score of 0.7003 and probability of false alarm and probability of miss of 0.0164 and 0.0547 respectively, for a set of 30 keywords. Furthermore, age-specific performance evaluation confirmed the system's effectiveness across different age groups of children. To assess the system's robustness against noise, additional experiments were conducted using the best-performing layer of the best-performing Wav2Vec2 model. The results demonstrated a significant improvement over traditional MFCC-based baseline, emphasizing the potential of SSL embeddings even in noisy conditions. To further generalize the KWS framework, the experiments were repeated for an additional CMU dataset. Overall the results highlight the significant contribution of SSL features in enhancing Zero-Shot KWS performance for children's speech, effectively addressing the challenges associated with the distinct characteristics of child speakers.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å„¿ç«¥è¯­éŸ³ç‹¬ç‰¹çš„å£°å­¦å’Œè¯­è¨€ç‰¹å¾å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè‡ªç›‘ç£å­¦ä¹  (SSL) æ¨¡å‹é€å±‚ç‰¹å¾ (Layer-Wise Features) çš„ Zero-Shot KWS æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ Wav2Vec2ã€HuBERT å’Œ Data2Vec ç­‰æ¨¡å‹æå–çš„ç‰¹å¾è®­ç»ƒ Kaldi-based DNN KWS ç³»ç»Ÿï¼Œé€šè¿‡åœ¨æˆäººæ•°æ®é›† WSJCAM0 ä¸Šè®­ç»ƒå¹¶åœ¨å„¿ç«¥æ•°æ®é›† PFSTAR ä¸Šæµ‹è¯•ï¼ŒéªŒè¯äº†å…¶å¼ºå¤§çš„é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„¿ç«¥è¯­éŸ³å…³é”®è¯è¯†åˆ«ä¸Šè¾¾åˆ°äº† State-of-the-Art æ°´å‡†ï¼Œå…¶ä¸­ Wav2Vec2 æ¨¡å‹çš„ç¬¬ 22 å±‚è¡¨ç°æœ€ä¸ºçªå‡ºï¼Œå…¶ ATWV å’Œ MTWV åˆ†æ•°åˆ†åˆ«è¾¾åˆ° 0.691 å’Œ 0.7003ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿåœ¨ä¸åŒå¹´é¾„æ®µè¡¨ç°ç¨³å®šï¼Œä¸”åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„é²æ£’æ€§æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ MFCC åŸºçº¿ã€‚é€šè¿‡åœ¨ CMU æ•°æ®é›†ä¸Šçš„è¿›ä¸€æ­¥å®éªŒï¼Œç ”ç©¶è¯å®äº† SSL ç‰¹å¾åœ¨å¼¥åˆæˆäººä¸å„¿ç«¥è¯­éŸ³å·®å¼‚ã€æå‡ Zero-Shot KWS æ€§èƒ½æ–¹é¢çš„æ˜¾è‘—ä½œç”¨ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.HC",
        "cs.SD",
        "eess.SP"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted",
      "pdf_url": "https://arxiv.org/pdf/2508.21248v1",
      "published_date": "2025-08-28 22:32:42 UTC",
      "updated_date": "2025-08-28 22:32:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:45:30.059051+00:00"
    },
    {
      "arxiv_id": "2508.21246v1",
      "title": "HCQA: Hybrid Classical-Quantum Agent for Generating Optimal Quantum Sensor Circuits",
      "title_zh": "HCQAï¼šç”¨äºç”Ÿæˆæœ€ä¼˜é‡å­ä¼ æ„Ÿå™¨ç”µè·¯çš„æ··åˆç»å…¸-é‡å­æ™ºèƒ½ä½“",
      "authors": [
        "Ahmad Alomari",
        "Sathish A. P. Kumar"
      ],
      "abstract": "This study proposes an HCQA for designing optimal Quantum Sensor Circuits (QSCs) to address complex quantum physics problems. The HCQA integrates computational intelligence techniques by leveraging a Deep Q-Network (DQN) for learning and policy optimization, enhanced by a quantum-based action selection mechanism based on the Q-values. A quantum circuit encodes the agent current state using Ry gates, and then creates a superposition of possible actions. Measurement of the circuit results in probabilistic action outcomes, allowing the agent to generate optimal QSCs by selecting sequences of gates that maximize the Quantum Fisher Information (QFI) while minimizing the number of gates. This computational intelligence-driven HCQA enables the automated generation of entangled quantum states, specifically the squeezed states, with high QFI sensitivity for quantum state estimation and control. Evaluation of the HCQA on a QSC that consists of two qubits and a sequence of Rx, Ry, and S gates demonstrates its efficiency in generating optimal QSCs with a QFI of 1. This work highlights the synergy between AI-driven learning and quantum computation, illustrating how intelligent agents can autonomously discover optimal quantum circuit designs for enhanced sensing and estimation tasks.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆç»å…¸-é‡å­æ™ºèƒ½ä½“(HCQA)ï¼Œæ—¨åœ¨ä¸ºå¤æ‚é‡å­ç‰©ç†é—®é¢˜è®¾è®¡æœ€ä¼˜çš„é‡å­ä¼ æ„Ÿå™¨ç”µè·¯(QSCs)ã€‚HCQAé€šè¿‡æ·±åº¦Qç½‘ç»œ(DQN)ç»“åˆåŸºäºQå€¼çš„é‡å­åŠ¨ä½œé€‰æ‹©æœºåˆ¶ï¼Œå®ç°äº†å­¦ä¹ ä¸ç­–ç•¥ä¼˜åŒ–ã€‚è¯¥æ™ºèƒ½ä½“åˆ©ç”¨Ryé—¨ç¼–ç å½“å‰çŠ¶æ€ï¼Œå¹¶é€šè¿‡é‡å­ç”µè·¯äº§ç”ŸåŠ¨ä½œå åŠ æ€ï¼Œæœ€ç»ˆé€šè¿‡æµ‹é‡è·å¾—æ¦‚ç‡æ€§çš„åŠ¨ä½œç»“æœã€‚é€šè¿‡åœ¨æœ€å¤§åŒ–é‡å­è´¹èˆå°”ä¿¡æ¯(QFI)çš„åŒæ—¶æœ€å°åŒ–é—¨æ•°é‡ï¼ŒHCQAèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå…·æœ‰é«˜çµæ•åº¦çš„çº ç¼ é‡å­æ€ï¼Œç‰¹åˆ«æ˜¯å‹ç¼©æ€(squeezed states)ã€‚åœ¨åŒ…å«ä¸¤ä¸ªé‡å­æ¯”ç‰¹åŠRxã€Ryã€Sé—¨åºåˆ—çš„ç”µè·¯è¯„ä¼°ä¸­ï¼Œè¯¥æ™ºèƒ½ä½“æˆåŠŸç”Ÿæˆäº†QFIä¸º1çš„æœ€ä¼˜ç”µè·¯ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†äººå·¥æ™ºèƒ½é©±åŠ¨çš„å­¦ä¹ ä¸é‡å­è®¡ç®—ä¹‹é—´çš„ååŒä½œç”¨ï¼Œè¯æ˜äº†æ™ºèƒ½ä½“åœ¨å¢å¼ºä¼ æ„Ÿä¸ä¼°è®¡ä»»åŠ¡ä¸­è‡ªä¸»å‘ç°æœ€ä¼˜é‡å­ç”µè·¯è®¾è®¡çš„èƒ½åŠ›ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "9 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.21246v1",
      "published_date": "2025-08-28 22:27:48 UTC",
      "updated_date": "2025-08-28 22:27:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:45:48.587410+00:00"
    },
    {
      "arxiv_id": "2508.21243v1",
      "title": "Full-Frequency Temporal Patching and Structured Masking for Enhanced Audio Classification",
      "title_zh": "ç”¨äºå¢å¼ºéŸ³é¢‘åˆ†ç±»çš„å…¨é¢‘åŸŸæ—¶é—´åˆ†å—ä¸ç»“æ„åŒ–æ©è”½",
      "authors": [
        "Aditya Makineni",
        "Baocheng Geng",
        "Qing Tian"
      ],
      "abstract": "Transformers and State-Space Models (SSMs) have advanced audio classification by modeling spectrograms as sequences of patches. However, existing models such as the Audio Spectrogram Transformer (AST) and Audio Mamba (AuM) adopt square patching from computer vision, which disrupts continuous frequency patterns and produces an excessive number of patches, slowing training, and increasing computation. We propose Full-Frequency Temporal Patching (FFTP), a patching strategy that better matches the time-frequency asymmetry of spectrograms by spanning full frequency bands with localized temporal context, preserving harmonic structure, and significantly reducing patch count and computation. We also introduce SpecMask, a patch-aligned spectrogram augmentation that combines full-frequency and localized time-frequency masks under a fixed masking budget, enhancing temporal robustness while preserving spectral continuity. When applied on both AST and AuM, our patching method with SpecMask improves mAP by up to +6.76 on AudioSet-18k and accuracy by up to +8.46 on SpeechCommandsV2, while reducing computation by up to 83.26%, demonstrating both performance and efficiency gains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Transformerså’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹(SSMs)åœ¨éŸ³é¢‘åˆ†ç±»ä¸­ä½¿ç”¨æ–¹å½¢åˆ†å—(square patching)å¯¼è‡´çš„é¢‘ç‡æ¨¡å¼ç ´ååŠè®¡ç®—å¼€é”€è¿‡å¤§é—®é¢˜ï¼Œæå‡ºäº†å…¨é¢‘ç‡æ—¶é—´åˆ†å—(Full-Frequency Temporal Patching, FFTP)ç­–ç•¥ã€‚FFTPé€šè¿‡è·¨è¶Šå…¨é¢‘å¸¦å¹¶ç»“åˆå±€éƒ¨æ—¶é—´ä¸Šä¸‹æ–‡ï¼Œæ›´å¥½åœ°é€‚é…äº†å£°è°±å›¾çš„æ—¶é¢‘ä¸å¯¹ç§°æ€§ï¼Œåœ¨å®Œæ•´ä¿ç•™è°æ³¢ç»“æ„çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è¡¥ä¸æ•°é‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†SpecMaskå¢å¼ºæŠ€æœ¯ï¼Œé€šè¿‡ç»„åˆå…¨é¢‘ç‡å’Œå±€éƒ¨æ—¶é¢‘æ©ç æ¥æå‡æ¨¡å‹çš„æ—¶é—´é²æ£’æ€§ï¼Œå¹¶ç¡®ä¿é¢‘è°±çš„è¿ç»­æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨Audio Spectrogram Transformer (AST)å’ŒAudio Mamba (AuM)ä¸Šåº”ç”¨åï¼Œåœ¨AudioSet-18kå’ŒSpeechCommandsV2æ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†æ˜¾è‘—çš„å¹³å‡ç²¾åº¦(mAP)å’Œå‡†ç¡®ç‡æå‡ã€‚è¯¥ç­–ç•¥åœ¨å¢å¼ºåˆ†ç±»æ€§èƒ½çš„åŒæ—¶ï¼ŒæˆåŠŸå°†è®¡ç®—é‡æœ€é«˜å‰Šå‡äº†83.26%ï¼Œå®ç°äº†éŸ³é¢‘åˆ†ç±»ä»»åŠ¡ä¸­æ€§èƒ½ä¸æ•ˆç‡çš„åŒé‡ä¼˜åŒ–ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21243v1",
      "published_date": "2025-08-28 22:13:20 UTC",
      "updated_date": "2025-08-28 22:13:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:45:51.754417+00:00"
    },
    {
      "arxiv_id": "2508.21238v1",
      "title": "Addressing accuracy and hallucination of LLMs in Alzheimer's disease research through knowledge graphs",
      "title_zh": "å€ŸåŠ©çŸ¥è¯†å›¾è°±è§£å†³é˜¿å°”èŒ¨æµ·é»˜ç—…ç ”ç©¶ä¸­å¤§è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§ä¸å¹»è§‰é—®é¢˜",
      "authors": [
        "Tingxuan Xu",
        "Jiarui Feng",
        "Justin Melendez",
        "Kaleigh Roberts",
        "Donghong Cai",
        "Mingfang Zhu",
        "Donald Elbert",
        "Yixin Chen",
        "Randall J. Bateman"
      ],
      "abstract": "In the past two years, large language model (LLM)-based chatbots, such as ChatGPT, have revolutionized various domains by enabling diverse task completion and question-answering capabilities. However, their application in scientific research remains constrained by challenges such as hallucinations, limited domain-specific knowledge, and lack of explainability or traceability for the response. Graph-based Retrieval-Augmented Generation (GraphRAG) has emerged as a promising approach to improving chatbot reliability by integrating domain-specific contextual information before response generation, addressing some limitations of standard LLMs. Despite its potential, there are only limited studies that evaluate GraphRAG on specific domains that require intensive knowledge, like Alzheimer's disease or other biomedical domains. In this paper, we assess the quality and traceability of two popular GraphRAG systems. We compile a database of 50 papers and 70 expert questions related to Alzheimer's disease, construct a GraphRAG knowledge base, and employ GPT-4o as the LLM for answering queries. We then compare the quality of responses generated by GraphRAG with those from a standard GPT-4o model. Additionally, we discuss and evaluate the traceability of several Retrieval-Augmented Generation (RAG) and GraphRAG systems. Finally, we provide an easy-to-use interface with a pre-built Alzheimer's disease database for researchers to test the performance of both standard RAG and GraphRAG.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…(Alzheimer's disease)ç ”ç©¶ä¸­é¢ä¸´çš„å¹»è§‰ã€é¢†åŸŸçŸ¥è¯†æœ‰é™ä»¥åŠç¼ºä¹è§£é‡Šæ€§ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä½œè€…ç³»ç»Ÿè¯„ä¼°äº†åŸºäºçŸ¥è¯†å›¾è°±çš„æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯(GraphRAG)åœ¨æ•´åˆç‰¹å®šé¢†åŸŸä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢çš„æ•ˆèƒ½ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«50ç¯‡è®ºæ–‡å’Œ70ä¸ªä¸“å®¶é—®é¢˜çš„ä¸“ç”¨æ•°æ®åº“ã€‚é€šè¿‡ä»¥GPT-4oä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œç ”ç©¶å¯¹æ¯”äº†GraphRAGç³»ç»Ÿä¸æ ‡å‡†GPT-4oåœ¨å›ç­”è´¨é‡åŠå¯è¿½æº¯æ€§(traceability)æ–¹é¢çš„è¡¨ç°ã€‚å®éªŒåˆ†æäº†GraphRAGåœ¨æå‡ç”Ÿç‰©åŒ»å­¦ç­‰çŸ¥è¯†å¯†é›†å‹é¢†åŸŸå›ç­”å‡†ç¡®æ€§æ–¹é¢çš„æ½œåŠ›ï¼ŒéªŒè¯äº†å…¶æ”¹å–„æ¨¡å‹å¯é æ€§çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†ä¸€ä¸ªé¢„æ„å»ºçš„æ•°æ®åº“å’Œæ˜“ç”¨æ¥å£ï¼Œæ–¹ä¾¿ç§‘ç ”äººå‘˜æµ‹è¯•æ ‡å‡†RAGä¸GraphRAGçš„æ€§èƒ½å·®å¼‚ã€‚è¯¥å·¥ä½œä¸ºåœ¨å¤æ‚åŒ»å­¦é¢†åŸŸåº”ç”¨å¯ä¿¡äººå·¥æ™ºèƒ½æŠ€æœ¯æä¾›äº†é‡è¦çš„å®è¯å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21238v1",
      "published_date": "2025-08-28 22:03:53 UTC",
      "updated_date": "2025-08-28 22:03:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:45:55.957867+00:00"
    },
    {
      "arxiv_id": "2508.21228v1",
      "title": "Decoding Memories: An Efficient Pipeline for Self-Consistency Hallucination Detection",
      "title_zh": "Decoding Memoriesï¼šä¸€ç§é«˜æ•ˆçš„è‡ªä¸€è‡´æ€§å¹»è§‰æ£€æµ‹æµæ°´çº¿",
      "authors": [
        "Weizhi Gao",
        "Xiaorui Liu",
        "Feiyi Wang",
        "Dan Lu",
        "Junqi Yin"
      ],
      "abstract": "Large language models (LLMs) have demonstrated impressive performance in both research and real-world applications, but they still struggle with hallucination. Existing hallucination detection methods often perform poorly on sentence-level generation or rely heavily on domain-specific knowledge. While self-consistency approaches help address these limitations, they incur high computational costs due to repeated generation. In this paper, we conduct the first study on identifying redundancy in self-consistency methods, manifested as shared prefix tokens across generations, and observe that non-exact-answer tokens contribute minimally to the semantic content. Based on these insights, we propose a novel Decoding Memory Pipeline (DMP) that accelerates generation through selective inference and annealed decoding. Being orthogonal to the model, dataset, decoding strategy, and self-consistency baseline, our DMP consistently improves the efficiency of multi-response generation and holds promise for extension to alignment and reasoning tasks. Extensive experiments show that our method achieves up to a 3x speedup without sacrificing AUROC performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¹»è§‰æ£€æµ‹(hallucination detection)ä¸­é¢ä¸´çš„è‡ªä¸€è‡´æ€§(self-consistency)æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„Decoding Memory Pipeline (DMP)ã€‚ç ”ç©¶è€…é€šè¿‡è¯†åˆ«ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å†—ä½™ç°è±¡ï¼Œå‘ç°å…±äº«å‰ç¼€tokenåŠéç²¾ç¡®ç­”æ¡ˆtokenå¯¹è¯­ä¹‰å†…å®¹çš„è´¡çŒ®æå°ã€‚åŸºäºæ­¤æ´å¯Ÿï¼ŒDMPç»“åˆäº†é€‰æ‹©æ€§æ¨ç†(selective inference)å’Œé€€ç«è§£ç (annealed decoding)æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†å¤šå“åº”ç”Ÿæˆçš„æ•ˆç‡ã€‚è¯¥æ–¹æ³•ä¸å…·ä½“çš„æ¨¡å‹ã€æ•°æ®é›†å’Œè§£ç ç­–ç•¥å…·æœ‰æ­£äº¤æ€§ï¼Œè¡¨ç°å‡ºæå¼ºçš„é€šç”¨æ€§ä¸æ‰©å±•æ½œåŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDMPåœ¨ä¸ç‰ºç‰²AUROCæ€§èƒ½çš„å‰æä¸‹ï¼Œå®ç°äº†é«˜è¾¾3å€çš„æ¨ç†åŠ é€Ÿï¼Œä¸ºé«˜æ•ˆçš„å¹»è§‰æ£€æµ‹ä¸æ¨¡å‹å¯¹é½æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, under review",
      "pdf_url": "https://arxiv.org/pdf/2508.21228v1",
      "published_date": "2025-08-28 21:39:53 UTC",
      "updated_date": "2025-08-28 21:39:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:46:00.255571+00:00"
    },
    {
      "arxiv_id": "2508.21225v1",
      "title": "Can Layer-wise SSL Features Improve Zero-Shot ASR Performance for Children's Speech?",
      "title_zh": "é€å±‚è‡ªç›‘ç£å­¦ä¹ ç‰¹å¾èƒ½å¦æå‡å„¿ç«¥è¯­éŸ³çš„é›¶æ ·æœ¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ€§èƒ½ï¼Ÿ",
      "authors": [
        "Abhijit Sinha",
        "Hemant Kumar Kathania",
        "Sudarsana Reddy Kadiri",
        "Shrikanth Narayanan"
      ],
      "abstract": "Automatic Speech Recognition (ASR) systems often struggle to accurately process children's speech due to its distinct and highly variable acoustic and linguistic characteristics. While recent advancements in self-supervised learning (SSL) models have greatly enhanced the transcription of adult speech, accurately transcribing children's speech remains a significant challenge. This study investigates the effectiveness of layer-wise features extracted from state-of-the-art SSL pre-trained models - specifically, Wav2Vec2, HuBERT, Data2Vec, and WavLM in improving the performance of ASR for children's speech in zero-shot scenarios. A detailed analysis of features extracted from these models was conducted, integrating them into a simplified DNN-based ASR system using the Kaldi toolkit. The analysis identified the most effective layers for enhancing ASR performance on children's speech in a zero-shot scenario, where WSJCAM0 adult speech was used for training and PFSTAR children speech for testing. Experimental results indicated that Layer 22 of the Wav2Vec2 model achieved the lowest Word Error Rate (WER) of 5.15%, representing a 51.64% relative improvement over the direct zero-shot decoding using Wav2Vec2 (WER of 10.65%). Additionally, age group-wise analysis demonstrated consistent performance improvements with increasing age, along with significant gains observed even in younger age groups using the SSL features. Further experiments on the CMU Kids dataset confirmed similar trends, highlighting the generalizability of the proposed approach.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ (SSL)æ¨¡å‹çš„å±‚çº§ç‰¹å¾æ¥æå‡å„¿ç«¥è¯­éŸ³åœ¨é›¶æ ·æœ¬(Zero-Shot)åœºæ™¯ä¸‹çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)æ€§èƒ½ã€‚ç ”ç©¶æ·±å…¥åˆ†æäº†Wav2Vec2ã€HuBERTã€Data2Vecå’ŒWavLMç­‰å…ˆè¿›é¢„è®­ç»ƒæ¨¡å‹çš„ä¸åŒå±‚çº§ç‰¹å¾ï¼Œå¹¶å°†å…¶é›†æˆåˆ°åŸºäºKaldiçš„DNN-ASRç³»ç»Ÿä¸­ã€‚é€šè¿‡åœ¨æˆå¹´äººè¯­éŸ³(WSJCAM0)ä¸Šè®­ç»ƒå¹¶ç›´æ¥åœ¨å„¿ç«¥è¯­éŸ³(PFSTAR)ä¸Šæµ‹è¯•ï¼Œå®éªŒå‘ç°Wav2Vec2çš„ç¬¬22å±‚ç‰¹å¾è¡¨ç°æœ€ä½³ï¼Œå°†è¯é”™è¯¯ç‡(WER)é™ä½è‡³5.15%ï¼Œç›¸æ¯”ç›´æ¥é›¶æ ·æœ¬è§£ç å®ç°äº†51.64%çš„ç›¸å¯¹æ”¹è¿›ã€‚å¹´é¾„ç»„åˆ†ææ˜¾ç¤ºï¼Œè™½ç„¶è¯†åˆ«æ€§èƒ½éšå„¿ç«¥å¹´é¾„å¢é•¿è€Œæå‡ï¼Œä½†SSLç‰¹å¾åœ¨ä½å¹´é¾„ç»„ä¸­åŒæ ·å¸¦æ¥äº†æ˜¾è‘—å¢ç›Šã€‚åœ¨CMU Kidsæ•°æ®é›†ä¸Šçš„è¿›ä¸€æ­¥å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†ç‰¹å®šSSLå±‚çº§ç‰¹å¾åœ¨åº”å¯¹å„¿ç«¥è¯­éŸ³é«˜å˜å¼‚æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.SP"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted",
      "pdf_url": "https://arxiv.org/pdf/2508.21225v1",
      "published_date": "2025-08-28 21:32:36 UTC",
      "updated_date": "2025-08-28 21:32:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:46:02.454926+00:00"
    },
    {
      "arxiv_id": "2508.21222v1",
      "title": "Generalizable Object Re-Identification via Visual In-Context Prompting",
      "title_zh": "åŸºäºè§†è§‰ä¸Šä¸‹æ–‡æç¤ºçš„å¯æ³›åŒ–ç›®æ ‡é‡è¯†åˆ«",
      "authors": [
        "Zhizhong Huang",
        "Xiaoming Liu"
      ],
      "abstract": "Current object re-identification (ReID) methods train domain-specific models (e.g., for persons or vehicles), which lack generalization and demand costly labeled data for new categories. While self-supervised learning reduces annotation needs by learning instance-wise invariance, it struggles to capture \\textit{identity-sensitive} features critical for ReID. This paper proposes Visual In-Context Prompting~(VICP), a novel framework where models trained on seen categories can directly generalize to unseen novel categories using only \\textit{in-context examples} as prompts, without requiring parameter adaptation. VICP synergizes LLMs and vision foundation models~(VFM): LLMs infer semantic identity rules from few-shot positive/negative pairs through task-specific prompting, which then guides a VFM (\\eg, DINO) to extract ID-discriminative features via \\textit{dynamic visual prompts}. By aligning LLM-derived semantic concepts with the VFM's pre-trained prior, VICP enables generalization to novel categories, eliminating the need for dataset-specific retraining. To support evaluation, we introduce ShopID10K, a dataset of 10K object instances from e-commerce platforms, featuring multi-view images and cross-domain testing. Experiments on ShopID10K and diverse ReID benchmarks demonstrate that VICP outperforms baselines by a clear margin on unseen categories. Code is available at https://github.com/Hzzone/VICP.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿç›®æ ‡é‡è¯†åˆ«(Object Re-Identification, ReID)æ¨¡å‹æ³›åŒ–èƒ½åŠ›å¼±ä¸”å¯¹æ–°ç±»åˆ«æ ‡æ³¨æ•°æ®ä¾èµ–åº¦é«˜çš„é—®é¢˜ï¼Œæå‡ºäº†Visual In-Context Prompting (VICP) æ¡†æ¶ã€‚è¯¥æ¡†æ¶ååŒåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œè§†è§‰åŸºç¡€æ¨¡å‹(Vision Foundation Models, VFM)ï¼Œé€šè¿‡ä»»åŠ¡ç‰¹å®šæç¤ºä½¿LLMsä»å°‘é‡æ­£è´Ÿæ ·æœ¬ä¸­æ¨ç†å‡ºè¯­ä¹‰èº«ä»½è§„åˆ™ã€‚åœ¨è¿™äº›è§„åˆ™å¼•å¯¼ä¸‹ï¼ŒVFMï¼ˆå¦‚DINOï¼‰åˆ©ç”¨åŠ¨æ€è§†è§‰æç¤º(dynamic visual prompts)æå–å…·æœ‰èº«ä»½è¾¨åˆ«åŠ›(ID-discriminative)çš„ç‰¹å¾ã€‚é€šè¿‡å°†LLMè¡ç”Ÿçš„è¯­ä¹‰æ¦‚å¿µä¸VFMçš„é¢„è®­ç»ƒå…ˆéªŒå¯¹é½ï¼ŒVICPå®ç°äº†å¯¹æœªçŸ¥æ–°ç±»åˆ«çš„ç›´æ¥æ³›åŒ–ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šæ•°æ®é›†è¿›è¡Œå‚æ•°å¾®è°ƒæˆ–é‡æ–°è®­ç»ƒã€‚ä¸ºéªŒè¯æ€§èƒ½ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†åŒ…å«1ä¸‡ä¸ªç›®æ ‡å®ä¾‹çš„ShopID10Kæ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šè§†å›¾å›¾åƒå’Œè·¨åŸŸæµ‹è¯•åœºæ™¯ã€‚åœ¨ShopID10KåŠå¤šä¸ªå¤šæ ·åŒ–ReIDåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒVICPåœ¨æœªçŸ¥ç±»åˆ«ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.21222v1",
      "published_date": "2025-08-28 21:24:06 UTC",
      "updated_date": "2025-08-28 21:24:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:46:02.858663+00:00"
    },
    {
      "arxiv_id": "2508.21206v1",
      "title": "Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach",
      "title_zh": "é€šè¿‡åƒç´ çº§æ–¹æ³•å¢å¼ºè‡ªå›å½’è¯­è¨€æ¨¡å‹æŠµå¾¡æ‹¼å†™æ”»å‡»çš„é²æ£’æ€§",
      "authors": [
        "Han Yang",
        "Jian Lan",
        "Yihong Liu",
        "Hinrich SchÃ¼tze",
        "Thomas Seidl"
      ],
      "abstract": "Autoregressive language models are vulnerable to orthographic attacks, where input text is perturbed with characters from multilingual alphabets, leading to substantial performance degradation. This vulnerability primarily stems from the out-of-vocabulary issue inherent in subword tokenizers and their embeddings. To address this limitation, we propose a pixel-based generative language model that replaces the text-based embeddings with pixel-based representations by rendering words as individual images. This design provides stronger robustness to noisy inputs, while an extension of compatibility to multilingual text across diverse writing systems. We evaluate the proposed method on the multilingual LAMBADA dataset, WMT24 dataset and the SST-2 benchmark, demonstrating both its resilience to orthographic noise and its effectiveness in multilingual settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªå›å½’è¯­è¨€æ¨¡å‹(Autoregressive language models)åœ¨é¢å¯¹æ‹¼å†™æ”»å‡»(Orthographic attacks)æ—¶çš„è„†å¼±æ€§ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåƒç´ (Pixel-based)çš„ç”Ÿæˆè¯­è¨€æ¨¡å‹ã€‚é’ˆå¯¹å­è¯åˆ†è¯å™¨(Subword tokenizers)åŠå…¶åµŒå…¥å±‚(Embeddings)å­˜åœ¨çš„è¯è¡¨å¤–(Out-of-vocabulary)é—®é¢˜ï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†å•è¯æ¸²æŸ“ä¸ºç‹¬ç«‹å›¾åƒï¼Œåˆ©ç”¨åƒç´ è¡¨ç¤ºæ›¿ä»£ä¼ ç»Ÿçš„æ–‡æœ¬åµŒå…¥ã€‚è¿™ç§è®¾è®¡æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹å™ªå£°è¾“å…¥çš„é²æ£’æ€§(Robustness)ï¼Œå¹¶æ‰©å±•äº†å…¶å¯¹ä¸åŒä¹¦å†™ç³»ç»Ÿä¸‹å¤šè¯­è¨€(Multilingual)æ–‡æœ¬çš„å…¼å®¹æ€§ã€‚åœ¨LAMBADAã€WMT24æ•°æ®é›†å’ŒSST-2åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…å±•ç°å‡ºå¼ºå¤§çš„æŠ—æ‹¼å†™å™ªå£°èƒ½åŠ›ï¼Œåœ¨å¤šè¯­è¨€è®¾ç½®ä¸­ä¹Ÿè¡¨ç°ä¼˜å¼‚ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21206v1",
      "published_date": "2025-08-28 20:48:38 UTC",
      "updated_date": "2025-08-28 20:48:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:46:05.562601+00:00"
    },
    {
      "arxiv_id": "2508.21204v2",
      "title": "Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive Scaffolding",
      "title_zh": "æ¨¡ç³Šã€ç¬¦å·ä¸è¯­å¢ƒï¼šé€šè¿‡è®¤çŸ¥æ”¯æ¶å¢å¼ºå¤§è¯­è¨€æ¨¡å‹æ•™å­¦å¼•å¯¼",
      "authors": [
        "Vanessa Figueiredo"
      ],
      "abstract": "We study how prompt-level inductive biases influence the cognitive behavior of large language models (LLMs) in instructional dialogue. We introduce a symbolic scaffolding method paired with a short-term memory schema designed to promote adaptive, structured reasoning in Socratic tutoring. Using controlled ablation across five system variants, we evaluate model outputs via expert-designed rubrics covering scaffolding, responsiveness, symbolic reasoning, and conversational memory. We present preliminary results using an LLM-based evaluation framework aligned to a cognitively grounded rubric. This enables scalable, systematic comparisons across architectural variants in early-stage experimentation. The preliminary results show that our full system consistently outperforms baseline variants. Analysis reveals that removing memory or symbolic structure degrades key cognitive behaviors, including abstraction, adaptive probing, and conceptual continuity. These findings support a processing-level account in which prompt-level cognitive scaffolds can reliably shape emergent instructional strategies in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æç¤ºè¯­å±‚é¢çš„å½’çº³åç½®(inductive biases)å¦‚ä½•å½±å“å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ•™å­¦å¯¹è¯ä¸­çš„è®¤çŸ¥è¡Œä¸ºã€‚ä½œè€…æå‡ºäº†ä¸€ç§ç¬¦å·åŒ–æ”¯æ¶(symbolic scaffolding)æ–¹æ³•ï¼Œå¹¶ç»“åˆçŸ­æœŸè®°å¿†æ¨¡å¼(short-term memory schema)ï¼Œæ—¨åœ¨æå‡è‹æ ¼æ‹‰åº•å¼æ•™å­¦(Socratic tutoring)ä¸­çš„è‡ªé€‚åº”å’Œç»“æ„åŒ–æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶é€šè¿‡å—æ§æ¶ˆèå®éªŒå¯¹æ¯”äº†äº”ç§ç³»ç»Ÿå˜ä½“ï¼Œå¹¶åˆ©ç”¨ä¸€å¥—åŸºäº LLM ä¸”ä¸è®¤çŸ¥ç†è®ºå¯¹é½çš„è¯„ä¼°æ¡†æ¶è¿›è¡Œè§„æ¨¡åŒ–æ¯”è¾ƒã€‚åˆæ­¥å®éªŒç»“æœè¡¨æ˜ï¼Œå®Œæ•´ç³»ç»Ÿåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºçº¿å˜ä½“ï¼Œè¯æ˜äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æ·±å…¥åˆ†ææ˜¾ç¤ºï¼Œç§»é™¤è®°å¿†æˆ–ç¬¦å·ç»“æ„ä¼šæ˜¾è‘—æŸå®³æ¨¡å‹çš„æŠ½è±¡èƒ½åŠ›ã€è‡ªé€‚åº”æ¢ç©¶èƒ½åŠ›å’Œæ¦‚å¿µè¿ç»­æ€§ç­‰å…³é”®è®¤çŸ¥è¡Œä¸ºã€‚è¯¥å‘ç°æ”¯æŒäº†å¤„ç†å±‚é¢çš„è§£é‡Šï¼Œå³æç¤ºè¯­å±‚é¢çš„è®¤çŸ¥æ”¯æ¶èƒ½å¤Ÿå¯é åœ°å¡‘é€ å¤§è¯­è¨€æ¨¡å‹ä¸­æ¶Œç°çš„æ•™å­¦ç­–ç•¥ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21204v2",
      "published_date": "2025-08-28 20:46:13 UTC",
      "updated_date": "2025-10-29 18:23:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:46:10.134373+00:00"
    },
    {
      "arxiv_id": "2508.21201v1",
      "title": "Improving Aviation Safety Analysis: Automated HFACS Classification Using Reinforcement Learning with Group Relative Policy Optimization",
      "title_zh": "æå‡èˆªç©ºå®‰å…¨åˆ†æï¼šåŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– (GRPO) å¼ºåŒ–å­¦ä¹ çš„ HFACS è‡ªåŠ¨åŒ–åˆ†ç±»",
      "authors": [
        "Arash Ahmadi",
        "Sarah Sharif",
        "Yaser Banad"
      ],
      "abstract": "Analyzing the human factors behind aviation accidents is crucial for preventing future incidents, yet traditional methods using the Human Factors Analysis and Classification System (HFACS) are limited by scalability and consistency. To address this, we introduce an automated HFACS classification framework for aviation safety analysis that utilizes Reinforcement Learning with Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B language model. Our approach incorporates a multi-component reward system tailored for aviation safety analysis and integrates synthetic data generation to overcome class imbalance in accident datasets. The resulting GRPO-optimized model achieved noticeable performance gains, including a 350% increase in exact match accuracy (from 0.0400 to 0.1800) and an improved partial match accuracy of 0.8800. Significantly, our specialized model outperforms state-of-the-art LLMs (Large Language Models), including GPT-5-mini and Gemini-2.5-fiash, on key metrics. This research also proposes exact match accuracy in multi-label HFACS classification problem as a new benchmarking methodology to evaluate the advanced reasoning capabilities of language models. Ultimately, our work validates that smaller, domain-optimized models can provide a computationally efficient and better solution for critical safety analysis. This approach makes powerful, low-latency deployment on resource-constrained edge devices feasible.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿçš„äººå› åˆ†æä¸åˆ†ç±»ç³»ç»Ÿ (Human Factors Analysis and Classification System, HFACS) åœ¨èˆªç©ºäº‹æ•…åˆ†æä¸­é¢ä¸´çš„å¯æ‰©å±•æ€§å’Œä¸€è‡´æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–åˆ†ç±»æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– (Group Relative Policy Optimization, GRPO) çš„å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œå¯¹ Llama-3.1 8B è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ä¸ºäº†è§£å†³äº‹æ•…æ•°æ®é›†ä¸­å­˜åœ¨çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæ•´åˆäº†åˆæˆæ•°æ®ç”ŸæˆæŠ€æœ¯ï¼Œå¹¶è®¾è®¡äº†ä¸“é—¨ç”¨äºèˆªç©ºå®‰å…¨åˆ†æçš„å¤šç»„ä»¶å¥–åŠ±ç³»ç»Ÿã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡ GRPO ä¼˜åŒ–çš„æ¨¡å‹åœ¨ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ä¸Šæå‡äº†350%ï¼Œä¸”åœ¨å¤šé¡¹å…³é”®æŒ‡æ ‡ä¸Šè¶…è¶Šäº† GPT-5-mini å’Œ Gemini-2.5-flash ç­‰å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs)ã€‚ç ”ç©¶è¿˜æå‡ºå°†å¤šæ ‡ç­¾ HFACS åˆ†ç±»ä¸­çš„ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ä½œä¸ºè¯„ä¼°è¯­è¨€æ¨¡å‹é«˜çº§æ¨ç†èƒ½åŠ›çš„æ–°åŸºå‡†æ–¹æ³•ã€‚è¯¥å·¥ä½œæœ€ç»ˆè¯å®äº†ç»è¿‡é¢†åŸŸä¼˜åŒ–çš„å°å‹æ¨¡å‹èƒ½å¤Ÿä¸ºå…³é”®å®‰å…¨åˆ†ææä¾›é«˜æ•ˆä¸”æ€§èƒ½æ›´ä½³çš„è§£å†³æ–¹æ¡ˆï¼Œä½¿èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡å®ç°ä½å»¶è¿Ÿéƒ¨ç½²æˆä¸ºå¯èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21201v1",
      "published_date": "2025-08-28 20:35:03 UTC",
      "updated_date": "2025-08-28 20:35:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:46:14.892380+00:00"
    },
    {
      "arxiv_id": "2509.04464v1",
      "title": "Can Multiple Responses from an LLM Reveal the Sources of Its Uncertainty?",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹çš„å¤šæ¬¡å“åº”èƒ½å¦æ­ç¤ºå…¶ä¸ç¡®å®šæ€§çš„æ¥æºï¼Ÿ",
      "authors": [
        "Yang Nan",
        "Pengfei He",
        "Ravi Tandon",
        "Han Xu"
      ],
      "abstract": "Large language models (LLMs) have delivered significant breakthroughs across diverse domains but can still produce unreliable or misleading outputs, posing critical challenges for real-world applications. While many recent studies focus on quantifying model uncertainty, relatively little work has been devoted to \\textit{diagnosing the source of uncertainty}. In this study, we show that, when an LLM is uncertain, the patterns of disagreement among its multiple generated responses contain rich clues about the underlying cause of uncertainty. To illustrate this point, we collect multiple responses from a target LLM and employ an auxiliary LLM to analyze their patterns of disagreement. The auxiliary model is tasked to reason about the likely source of uncertainty, such as whether it stems from ambiguity in the input question, a lack of relevant knowledge, or both. In cases involving knowledge gaps, the auxiliary model also identifies the specific missing facts or concepts contributing to the uncertainty. In our experiment, we validate our framework on AmbigQA, OpenBookQA, and MMLU-Pro, confirming its generality in diagnosing distinct uncertainty sources. Such diagnosis shows the potential for relevant manual interventions that improve LLM performance and reliability.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)è¾“å‡ºä¸ç¡®å®šæ€§çš„æ¥æºè¯Šæ–­é—®é¢˜ï¼ŒæŒ‡å‡ºLLMç”Ÿæˆçš„å¤šä¸ªå“åº”ä¹‹é—´çš„åˆ†æ­§æ¨¡å¼è•´å«ç€å…³äºä¸ç¡®å®šæ€§æˆå› çš„ä¸°å¯Œçº¿ç´¢ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åˆ›æ–°æ¡†æ¶ï¼Œé€šè¿‡æ”¶é›†ç›®æ ‡LLMçš„å¤šæ¬¡å“åº”å¹¶åˆ©ç”¨è¾…åŠ©LLMåˆ†æå…¶ä¸­çš„åˆ†æ­§æ¨¡å¼ï¼Œä»è€Œæ¨ç†ä¸ç¡®å®šæ€§çš„å…·ä½“æ¥æºã€‚è¯¥è¾…åŠ©æ¨¡å‹èƒ½å¤ŸåŒºåˆ†ä¸ç¡®å®šæ€§æ˜¯æºäºè¾“å…¥é—®é¢˜çš„æ­§ä¹‰(ambiguity)ã€ç¼ºä¹ç›¸å…³çŸ¥è¯†(knowledge gaps)ï¼Œè¿˜æ˜¯ä¸¤è€…å…¼æœ‰ï¼›åœ¨æ¶‰åŠçŸ¥è¯†ç¼ºå£çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¡†æ¶è¿˜èƒ½è¿›ä¸€æ­¥è¯†åˆ«å¯¼è‡´ä¸ç¡®å®šæ€§çš„å…·ä½“ç¼ºå¤±äº‹å®æˆ–æ¦‚å¿µã€‚å®éªŒåœ¨AmbigQAã€OpenBookQAå’ŒMMLU-Proç­‰æ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•åœ¨è¯Šæ–­ä¸åŒæ¥æºä¸ç¡®å®šæ€§æ–¹é¢çš„æ™®é€‚æ€§ã€‚è¿™ç§è¯Šæ–­æœºåˆ¶ä¸ºé€šè¿‡å®šå‘äººå·¥å¹²é¢„æ¥å¢å¼ºLLMçš„æ€§èƒ½å’Œå¯é æ€§æä¾›äº†é‡è¦æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Proceedings of The 2025 Conference on Empirical Methods in Natural Language Processing (Findings)",
      "pdf_url": "https://arxiv.org/pdf/2509.04464v1",
      "published_date": "2025-08-28 20:14:35 UTC",
      "updated_date": "2025-08-28 20:14:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:46:24.487256+00:00"
    },
    {
      "arxiv_id": "2508.21186v1",
      "title": "Manifold Trajectories in Next-Token Prediction: From Replicator Dynamics to Softmax Equilibrium",
      "title_zh": "ä¸‹ä¸€ä¸ª Token é¢„æµ‹ä¸­çš„æµå½¢è½¨è¿¹ï¼šä»å¤åˆ¶è€…åŠ¨åŠ›å­¦åˆ° Softmax å¹³è¡¡",
      "authors": [
        "Christopher R. Lee-Jenkins"
      ],
      "abstract": "Decoding in large language models is often described as scoring tokens and normalizing with softmax. We give a minimal, self-contained account of this step as a constrained variational principle on the probability simplex. The discrete, normalization-respecting ascent is the classical multiplicative-weights (entropic mirror) update; its continuous-time limit is the replicator flow. From these ingredients we prove that, for a fixed context and temperature, the next-token distribution follows a smooth trajectory inside the simplex and converges to the softmax equilibrium. This formalizes the common ``manifold traversal'' intuition at the output-distribution level. The analysis yields precise, practice-facing consequences: temperature acts as an exact rescaling of time along the same trajectory, while top-k and nucleus sampling restrict the flow to a face with identical guarantees. We also outline a controlled account of path-dependent score adjustments and their connection to loop-like, hallucination-style behavior. We make no claims about training dynamics or internal representations; those are deferred to future work.",
      "tldr_zh": "è¯¥ç ”ç©¶ä¸ºå¤§è¯­è¨€æ¨¡å‹ä¸­çš„è§£ç è¿‡ç¨‹æä¾›äº†ä¸€ä¸ªæç®€ä¸”è‡ªç»™è‡ªè¶³çš„ç†è®ºæ¡†æ¶ï¼Œå°†å…¶æè¿°ä¸ºæ¦‚ç‡å•çº¯å½¢(Probability Simplex)ä¸Šçš„çº¦æŸå˜åˆ†åŸç†ã€‚ä½œè€…è¯æ˜äº†ç¦»æ•£çš„å½’ä¸€åŒ–æ›´æ–°ç­‰åŒäºç»å…¸çš„ä¹˜æ³•æƒé‡(Multiplicative-weights)æ›´æ–°ï¼Œè€Œå…¶è¿ç»­æ—¶é—´æé™åˆ™æ˜¯å¤åˆ¶å­æµ(Replicator Flow)ã€‚ç ”ç©¶è¡¨æ˜åœ¨å›ºå®šä¸Šä¸‹æ–‡å’Œæ¸©åº¦ä¸‹ï¼Œä¸‹ä¸€æ ‡è®°(Next-token)åˆ†å¸ƒåœ¨å•çº¯å½¢å†…éƒ¨éµå¾ªä¸€æ¡å¹³æ»‘è½¨è¿¹ï¼Œå¹¶æœ€ç»ˆæ”¶æ•›äºSoftmax Equilibriumã€‚è¿™ä¸€ç»“è®ºæ­£å¼åŒ–äº†è¾“å‡ºåˆ†å¸ƒå±‚é¢ä¸Šçš„æµå½¢éå†(Manifold Traversal)ç›´è§‰ï¼Œå¹¶æ­ç¤ºäº†æ¸©åº¦(Temperature)æœ¬è´¨ä¸Šæ˜¯å¯¹åŒä¸€è½¨è¿¹ä¸Šçš„æ—¶é—´è¿›è¡Œç²¾ç¡®ç¼©æ”¾ã€‚åŒæ—¶ï¼ŒTop-kå’Œæ ¸é‡‡æ ·(Nucleus Sampling)å°†è¯¥æµé™åˆ¶åœ¨å•çº¯å½¢çš„æŸä¸ªé¢(Face)ä¸Šï¼Œä¸”å…·æœ‰ç›¸åŒçš„ç†è®ºä¿è¯ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜åˆ†æäº†è·¯å¾„ç›¸å…³çš„åˆ†æ•°è°ƒæ•´ä¸å¾ªç¯çŠ¶å¹»è§‰(Hallucination-style behavior)ä¹‹é—´çš„è”ç³»ã€‚è¯¥åˆ†æä¸ºç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒè¡Œä¸ºæä¾›äº†ç²¾ç¡®çš„æ•°å­¦åˆ»ç”»ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.DS"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21186v1",
      "published_date": "2025-08-28 20:00:22 UTC",
      "updated_date": "2025-08-28 20:00:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:46:38.985836+00:00"
    },
    {
      "arxiv_id": "2508.21184v2",
      "title": "BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design",
      "title_zh": "BED-LLMï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹ä¸è´å¶æ–¯å®éªŒè®¾è®¡çš„æ™ºèƒ½ä¿¡æ¯è·å–",
      "authors": [
        "Deepro Choudhury",
        "Sinead Williamson",
        "Adam GoliÅ„ski",
        "Ning Miao",
        "Freddie Bickford Smith",
        "Michael Kirchhof",
        "Yizhe Zhang",
        "Tom Rainforth"
      ],
      "abstract": "We propose a general-purpose approach for improving the ability of Large Language Models (LLMs) to intelligently and adaptively gather information from a user or other external source using the framework of sequential Bayesian experimental design (BED). This enables LLMs to act as effective multi-turn conversational agents and interactively interface with external environments. Our approach, which we call BED-LLM (Bayesian Experimental Design with Large Language Models), is based on iteratively choosing questions or queries that maximize the expected information gain (EIG) about the task of interest given the responses gathered previously. We show how this EIG can be formulated (and then estimated) in a principled way using a probabilistic model derived from the LLM's predictive distributions and provide detailed insights into key decisions in its construction and updating procedure. We find that BED-LLM achieves substantial gains in performance across a wide range of tests based on the 20 questions game and using the LLM to actively infer user preferences, compared to direct prompting of the LLM and other adaptive design strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BED-LLMï¼Œä¸€ç§åŸºäºé¡ºåºè´å¶æ–¯å®éªŒè®¾è®¡ (Bayesian Experimental Design, BED) æ¡†æ¶çš„é€šç”¨æ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ™ºèƒ½ä¸”è‡ªé€‚åº”åœ°ä»ç”¨æˆ·æˆ–å¤–éƒ¨æ¥æºè·å–ä¿¡æ¯çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºè¿­ä»£é€‰æ‹©èƒ½å¤Ÿæœ€å¤§åŒ–å…³äºç›®æ ‡ä»»åŠ¡é¢„æœŸä¿¡æ¯å¢ç›Š (Expected Information Gain, EIG) çš„æŸ¥è¯¢ï¼Œä½¿ LLMs èƒ½å¤Ÿä½œä¸ºé«˜æ•ˆçš„å¤šè½®å¯¹è¯æ™ºèƒ½ä½“ä¸å¤–éƒ¨ç¯å¢ƒäº¤äº’ã€‚ç ”ç©¶è€…åˆ©ç”¨ LLM çš„é¢„æµ‹åˆ†å¸ƒæ„å»ºæ¦‚ç‡æ¨¡å‹æ¥å½¢å¼åŒ–å¹¶ä¼°ç®— EIGï¼Œå¹¶å¯¹æ¨¡å‹æ„å»ºä¸æ›´æ–°è¿‡ç¨‹ä¸­çš„å…³é”®å†³ç­–æä¾›äº†æ·±å…¥è§è§£ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨â€œäºŒåä¸ªé—®é¢˜â€æ¸¸æˆåŠä¸»åŠ¨æ¨æ–­ç”¨æˆ·åå¥½ç­‰å¹¿æ³›æµ‹è¯•ä¸­ï¼ŒBED-LLM çš„è¡¨ç°æ˜¾è‘—ä¼˜äºç›´æ¥æç¤º (direct prompting) æˆ–å…¶ä»–è‡ªé€‚åº”è®¾è®¡ç­–ç•¥ã€‚è¯¥ç ”ç©¶ä¸ºå¢å¼º LLMs åœ¨äº¤äº’å¼ç¯å¢ƒä¸‹çš„ä¸»åŠ¨ä¿¡æ¯é‡‡é›†æ•ˆç‡æä¾›äº†ç³»ç»Ÿæ€§çš„ç†è®ºæ¡†æ¶ä¸å®è·µæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21184v2",
      "published_date": "2025-08-28 19:51:43 UTC",
      "updated_date": "2025-10-18 23:14:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:46:57.794962+00:00"
    },
    {
      "arxiv_id": "2508.21181v1",
      "title": "FUTURE: Flexible Unlearning for Tree Ensemble",
      "title_zh": "FUTUREï¼šé¢å‘æ ‘é›†æˆçš„çµæ´»é—å¿˜ç®—æ³•",
      "authors": [
        "Ziheng Chen",
        "Jin Huang",
        "Jiali Cheng",
        "Yuchan Guo",
        "Mengjie Wang",
        "Lalitesh Morishetti",
        "Kaushiki Nag",
        "Hadi Amiri"
      ],
      "abstract": "Tree ensembles are widely recognized for their effectiveness in classification tasks, achieving state-of-the-art performance across diverse domains, including bioinformatics, finance, and medical diagnosis. With increasing emphasis on data privacy and the \\textit{right to be forgotten}, several unlearning algorithms have been proposed to enable tree ensembles to forget sensitive information. However, existing methods are often tailored to a particular model or rely on the discrete tree structure, making them difficult to generalize to complex ensembles and inefficient for large-scale datasets. To address these limitations, we propose FUTURE, a novel unlearning algorithm for tree ensembles. Specifically, we formulate the problem of forgetting samples as a gradient-based optimization task. In order to accommodate non-differentiability of tree ensembles, we adopt the probabilistic model approximations within the optimization framework. This enables end-to-end unlearning in an effective and efficient manner. Extensive experiments on real-world datasets show that FUTURE yields significant and successful unlearning performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ ‘é›†æˆ(Tree Ensemble)æ¨¡å‹åœ¨æ•°æ®éšç§ä¿æŠ¤å’Œâ€œè¢«é—å¿˜æƒâ€(right to be forgotten)éœ€æ±‚ä¸‹çš„æ•°æ®å¸è½½æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºFUTUREçš„æ–°å‹ç®—æ³•ã€‚ç°æœ‰çš„æœºå™¨å¸è½½æ–¹æ³•å¾€å¾€å±€é™äºç‰¹å®šæ¨¡å‹æˆ–ä¾èµ–ç¦»æ•£æ ‘ç»“æ„ï¼Œå¯¼è‡´å…¶åœ¨å¤„ç†å¤æ‚é›†æˆæ¨¡å‹å’Œå¤§è§„æ¨¡æ•°æ®é›†æ—¶éš¾ä»¥æ¨å¹¿ä¸”æ•ˆç‡è¾ƒä½ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼ŒFUTUREå°†æ ·æœ¬é—å¿˜é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–(gradient-based optimization)ä»»åŠ¡ã€‚è¯¥æ¡†æ¶é€šè¿‡é‡‡ç”¨æ¦‚ç‡æ¨¡å‹è¿‘ä¼¼(probabilistic model approximations)æŠ€æœ¯ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†æ ‘é›†æˆç»“æ„çš„ä¸å¯å¾®æ€§(non-differentiability)é—®é¢˜ã€‚è¿™ç§åˆ›æ–°è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œé«˜æ•ˆä¸”çµæ´»çš„å¸è½½ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼ŒFUTUREåœ¨ä¿æŒåˆ†ç±»æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†æ˜¾è‘—ä¸”æˆåŠŸçš„å¸è½½æ•ˆæœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "CIKM 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.21181v1",
      "published_date": "2025-08-28 19:45:36 UTC",
      "updated_date": "2025-08-28 19:45:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:47:49.151394+00:00"
    },
    {
      "arxiv_id": "2508.21172v1",
      "title": "Deep Residual Echo State Networks: exploring residual orthogonal connections in untrained Recurrent Neural Networks",
      "title_zh": "æ·±åº¦æ®‹å·®å›å£°çŠ¶æ€ç½‘ç»œï¼šæ¢ç´¢æœªè®­ç»ƒå¾ªç¯ç¥ç»ç½‘ç»œä¸­çš„æ®‹å·®æ­£äº¤è¿æ¥",
      "authors": [
        "Matteo Pinna",
        "Andrea Ceni",
        "Claudio Gallicchio"
      ],
      "abstract": "Echo State Networks (ESNs) are a particular type of untrained Recurrent Neural Networks (RNNs) within the Reservoir Computing (RC) framework, popular for their fast and efficient learning. However, traditional ESNs often struggle with long-term information processing. In this paper, we introduce a novel class of deep untrained RNNs based on temporal residual connections, called Deep Residual Echo State Networks (DeepResESNs). We show that leveraging a hierarchy of untrained residual recurrent layers significantly boosts memory capacity and long-term temporal modeling. For the temporal residual connections, we consider different orthogonal configurations, including randomly generated and fixed-structure configurations, and we study their effect on network dynamics. A thorough mathematical analysis outlines necessary and sufficient conditions to ensure stable dynamics within DeepResESN. Our experiments on a variety of time series tasks showcase the advantages of the proposed approach over traditional shallow and deep RC.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿå›å£°çŠ¶æ€ç½‘ç»œ (Echo State Networks, ESNs) åœ¨å¤„ç†é•¿æœŸä¿¡æ¯æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸ºæ·±åº¦æ®‹å·®å›å£°çŠ¶æ€ç½‘ç»œ (Deep Residual Echo State Networks, DeepResESNs) çš„æ–°å‹æ·±åº¦æœªè®­ç»ƒå¾ªç¯ç¥ç»ç½‘ç»œã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºå±‚çº§åŒ–çš„æœªè®­ç»ƒæ®‹å·®å¾ªç¯å±‚ï¼Œæ˜¾è‘—æå‡äº†ç½‘ç»œçš„è®°å¿†å®¹é‡ (memory capacity) å’Œé•¿æœŸæ—¶é—´å»ºæ¨¡èƒ½åŠ›ã€‚ç ”ç©¶äººå‘˜åœ¨æ—¶é—´æ®‹å·®è¿æ¥ä¸­æ¢ç´¢äº†åŒ…æ‹¬éšæœºç”Ÿæˆå’Œå›ºå®šç»“æ„åœ¨å†…çš„å¤šç§æ­£äº¤é…ç½® (orthogonal configurations)ï¼Œå¹¶æ·±å…¥åˆ†æäº†è¿™äº›é…ç½®å¯¹ç½‘ç»œåŠ¨åŠ›å­¦çš„å½±å“ã€‚æ­¤å¤–ï¼Œè®ºæ–‡é€šè¿‡ä¸¥å¯†çš„æ•°å­¦åˆ†æç¡®å®šäº†ç¡®ä¿ DeepResESN åŠ¨åŠ›å­¦ç¨³å®šæ€§çš„å……è¦æ¡ä»¶ã€‚åœ¨å¤šé¡¹æ—¶é—´åºåˆ—ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„æµ…å±‚åŠæ·±å±‚æ°´åº“è®¡ç®— (Reservoir Computing, RC) æ–¹æ³•ç›¸æ¯”ï¼ŒDeepResESNs åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¸Šå‡å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.21172v1",
      "published_date": "2025-08-28 19:22:02 UTC",
      "updated_date": "2025-08-28 19:22:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:47:49.359412+00:00"
    },
    {
      "arxiv_id": "2509.06975v1",
      "title": "GSTBench: A Benchmark Study on the Transferability of Graph Self-Supervised Learning",
      "title_zh": "GSTBenchï¼šå›¾è‡ªç›‘ç£å­¦ä¹ å¯è¿ç§»æ€§çš„åŸºå‡†ç ”ç©¶",
      "authors": [
        "Yu Song",
        "Zhigang Hua",
        "Yan Xie",
        "Jingzhe Liu",
        "Bo Long",
        "Hui Liu"
      ],
      "abstract": "Self-supervised learning (SSL) has shown great promise in graph representation learning. However, most existing graph SSL methods are developed and evaluated under a single-dataset setting, leaving their cross-dataset transferability largely unexplored and limiting their ability to leverage knowledge transfer and large-scale pretraining, factors that are critical for developing generalized intelligence beyond fitting training data. To address this gap and advance foundation model research for graphs, we present GSTBench, the first systematic benchmark for evaluating the transferability of graph SSL methods. We conduct large-scale pretraining on ogbn-papers100M and evaluate five representative SSL methods across a diverse set of target graphs. Our standardized experimental setup decouples confounding factors such as model architecture, dataset characteristics, and adaptation protocols, enabling rigorous comparisons focused solely on pretraining objectives. Surprisingly, we observe that most graph SSL methods struggle to generalize, with some performing worse than random initialization. In contrast, GraphMAE, a masked autoencoder approach, consistently improves transfer performance. We analyze the underlying factors that drive these differences and offer insights to guide future research on transferable graph SSL, laying a solid foundation for the \"pretrain-then-transfer\" paradigm in graph learning. Our code is available at https://github.com/SongYYYY/GSTBench.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å›¾è‡ªç›‘ç£å­¦ä¹  (Graph SSL) æ–¹æ³•å±€é™äºå•ä¸€æ•°æ®é›†ã€è·¨æ•°æ®é›†è¿ç§»èƒ½åŠ›ä¸æ˜çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªç³»ç»Ÿæ€§è¯„ä¼° Graph SSL å¯è¿ç§»æ€§ (Transferability) çš„åŸºå‡†æµ‹è¯• GSTBenchã€‚é€šè¿‡åœ¨ ogbn-papers100M æ•°æ®é›†ä¸Šè¿›è¡Œå¤§è§„æ¨¡é¢„è®­ç»ƒ (Pretraining) å¹¶è¯„ä¼°äº”ç§ä»£è¡¨æ€§æ–¹æ³•ï¼Œç ”ç©¶å‘ç°å¤§å¤šæ•°ç°æœ‰æ–¹æ³•éš¾ä»¥æ³›åŒ–ï¼Œéƒ¨åˆ†è¡¨ç°ç”šè‡³é€Šäºéšæœºåˆå§‹åŒ–ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºæ©ç è‡ªç¼–ç å™¨ (Masked Autoencoder) çš„ GraphMAE åœ¨å„ç§ç›®æ ‡å›¾ä¸Šå±•ç°å‡ºç¨³å®šçš„è¿ç§»æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶è¿›ä¸€æ­¥åˆ†æäº†å½±å“è¿ç§»èƒ½åŠ›çš„æ½œåœ¨å› ç´ ï¼Œä¸ºå¯è¿ç§»å›¾è‡ªç›‘ç£å­¦ä¹ çš„ç ”ç©¶æä¾›äº†é‡è¦è§è§£ï¼Œå¹¶ä¸ºæ„å»ºå›¾å­¦ä¹ é¢†åŸŸçš„â€œé¢„è®­ç»ƒ-è¿ç§»â€ (Pretrain-then-transfer) èŒƒå¼å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at CIKM'25",
      "pdf_url": "https://arxiv.org/pdf/2509.06975v1",
      "published_date": "2025-08-28 19:13:10 UTC",
      "updated_date": "2025-08-28 19:13:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:47:49.590896+00:00"
    },
    {
      "arxiv_id": "2509.06974v1",
      "title": "Individualized and Interpretable Sleep Forecasting via a Two-Stage Adaptive Spatial-Temporal Model",
      "title_zh": "åŸºäºä¸¤é˜¶æ®µè‡ªé€‚åº”æ—¶ç©ºæ¨¡å‹çš„ä¸ªæ€§åŒ–ä¸å¯è§£é‡Šç¡çœ é¢„æµ‹",
      "authors": [
        "Xueyi Wang",
        "Elisabeth Wilhelm"
      ],
      "abstract": "Sleep quality significantly impacts well-being. Therefore, healthcare providers and individuals need accessible and reliable forecasting tools for preventive interventions. This paper introduces an interpretable, individualized two-stage adaptive spatial-temporal model for predicting sleep quality scores. Our proposed framework combines multi-scale convolutional layers to model spatial interactions across multiple input variables, recurrent layers and attention mechanisms to capture long-term temporal dependencies, and a two-stage domain adaptation strategy to enhance generalization. The first adaptation stage is applied during training to mitigate overfitting on the training set. In the second stage, a source-free test-time adaptation mechanism is employed to adapt the model to new users without requiring labels. We conducted various experiments with five input window sizes (3, 5, 7, 9, and 11 days) and five prediction window sizes (1, 3, 5, 7, and 9 days). Our model consistently outperformed time series forecasting baseline approaches, including Long Short-Term Memory (LSTM), Informer, PatchTST, and TimesNet. The best performance was achieved with a three-day input window and a one-day prediction window, yielding a root mean square error (RMSE) of 0.216. Furthermore, the model demonstrated good predictive performance even for longer forecasting horizons (e.g, with a 0.257 RMSE for a three-day prediction window), highlighting its practical utility for real-world applications. We also conducted an explainability analysis to examine how different features influence sleep quality. These findings proved that the proposed framework offers a robust, adaptive, and explainable solution for personalized sleep forecasting using sparse data from commercial wearable devices.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸ªæ€§åŒ–ä¸”å¯è§£é‡Šçš„ä¸¤é˜¶æ®µè‡ªé€‚åº”æ—¶ç©ºæ¨¡å‹(Two-Stage Adaptive Spatial-Temporal Model)ï¼Œæ—¨åœ¨åˆ©ç”¨å•†ä¸šå¯ç©¿æˆ´è®¾å¤‡çš„ç¨€ç–æ•°æ®ç²¾å‡†é¢„æµ‹ç¡çœ è´¨é‡è¯„åˆ†ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¤šå°ºåº¦å·ç§¯å±‚(Multi-scale convolutional layers)ä»¥å»ºæ¨¡è¾“å…¥å˜é‡é—´çš„ç©ºé—´äº¤äº’ï¼Œå¹¶åˆ©ç”¨å¾ªç¯å±‚å’Œæ³¨æ„åŠ›æœºåˆ¶æ•æ‰é•¿æœŸçš„æ—¶åºä¾èµ–ã€‚ä¸ºäº†å¢å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œæ¨¡å‹é‡‡ç”¨äº†ä¸¤é˜¶æ®µé¢†åŸŸè‡ªé€‚åº”(Domain adaptation)ç­–ç•¥ï¼Œå…¶ä¸­åŒ…æ‹¬é’ˆå¯¹æ–°ç”¨æˆ·çš„æ— æºæµ‹è¯•æ—¶è‡ªé€‚åº”(Source-free test-time adaptation)æœºåˆ¶ï¼Œä½¿å…¶åœ¨æ— éœ€æ ‡ç­¾çš„æƒ…å†µä¸‹å³å¯é€‚åº”æ–°ä¸ªä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§é¢„æµ‹çª—å£ä¸‹çš„è¡¨ç°å‡ä¼˜äºLSTMã€Informerã€PatchTSTå’ŒTimesNetç­‰åŸºçº¿æ¨¡å‹ï¼Œå…¶ä¸­åœ¨ä¸‰å¤©è¾“å…¥é¢„æµ‹ä¸€å¤©çš„é…ç½®ä¸‹è¾¾åˆ°äº†0.216çš„å‡æ–¹æ ¹è¯¯å·®(RMSE)ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡å¯è§£é‡Šæ€§åˆ†ææ¢è®¨äº†ä¸åŒç‰¹å¾å¯¹ç¡çœ è´¨é‡çš„å½±å“ï¼Œä¸ºå®ç°é²æ£’ã€è‡ªé€‚åº”ä¸”é€æ˜çš„ä¸ªæ€§åŒ–ç¡çœ é¢„æµ‹æä¾›äº†å¯é çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.06974v1",
      "published_date": "2025-08-28 19:01:40 UTC",
      "updated_date": "2025-08-28 19:01:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:46:59.987213+00:00"
    },
    {
      "arxiv_id": "2508.21164v3",
      "title": "Quantifying Label-Induced Bias in Large Language Model Self- and Cross-Evaluations",
      "title_zh": "é‡åŒ–å¤§è¯­è¨€æ¨¡å‹è‡ªæˆ‘è¯„ä¼°ä¸äº¤å‰è¯„ä¼°ä¸­çš„æ ‡ç­¾è¯±å¯¼åè§",
      "authors": [
        "Muskan Saraf",
        "Sajjad Rezvani Boroujeni",
        "Justin Beaudry",
        "Hossein Abedi",
        "Tom Bush"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed as evaluators of text quality, yet the validity of their judgments remains underexplored. This study investigates systematic bias in self- and cross-model evaluations across three prominent LLMs: ChatGPT, Gemini, and Claude. We designed a controlled experiment in which blog posts authored by each model were evaluated by all three models under four labeling conditions: no attribution, true attribution, and two false-attribution scenarios. Evaluations employed both holistic preference voting and granular quality ratings across three dimensions Coherence, Informativeness, and Conciseness with all scores normalized to percentages for direct comparison. Our findings reveal pronounced asymmetries in model judgments: the \"Claude\" label consistently elevated scores regardless of actual authorship, while the \"Gemini\" label systematically depressed them. False attribution frequently reversed preference rankings, producing shifts of up to 50 percentage points in voting outcomes and up to 12 percentage points in quality ratings. Notably, Gemini exhibited severe self-deprecation under true labels, while Claude demonstrated intensified self-preference. These results demonstrate that perceived model identity can substantially distort both high-level judgments and fine-grained quality assessments, independent of content quality. Our findings challenge the reliability of LLM-as-judge paradigms and underscore the critical need for blind evaluation protocols and diverse multi-model validation frameworks to ensure fairness and validity in automated text evaluation and LLM benchmarking.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ä½œä¸ºæ–‡æœ¬è´¨é‡è¯„ä¼°è€…æ—¶å­˜åœ¨çš„ç³»ç»Ÿæ€§åç½®ï¼Œé‡ç‚¹é’ˆå¯¹ ChatGPTã€Gemini å’Œ Claude å¼€å±•äº†è‡ªæˆ‘è¯„ä¼°ä¸äº¤å‰è¯„ä¼°çš„å—æ§å®éªŒã€‚ç ”ç©¶äººå‘˜åœ¨æ— ç½²åã€çœŸå®ç½²ååŠè™šå‡ç½²åç­‰å››ç§æ ‡æ³¨æ¡ä»¶ä¸‹ï¼Œå¯¹æ¨¡å‹ç”Ÿæˆçš„åšå®¢å†…å®¹åœ¨ Coherenceã€Informativeness å’Œ Conciseness ç»´åº¦è¿›è¡Œè¯„åˆ†ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹çš„èº«ä»½æ ‡ç­¾ä¼šæ˜¾è‘—æ‰­æ›²é«˜å±‚åˆ¤æ–­å’Œç»†ç²’åº¦è´¨é‡è¯„ä¼°ï¼Œä¸”è¿™ç§å½±å“ç‹¬ç«‹äºå†…å®¹è´¨é‡æœ¬èº«ã€‚å…·ä½“è€Œè¨€ï¼Œâ€œClaudeâ€ æ ‡ç­¾æ™®éèƒ½æå‡å¾—åˆ†ï¼Œè€Œ â€œGeminiâ€ æ ‡ç­¾åˆ™ä¼šå¯¼è‡´å¾—åˆ†ç³»ç»Ÿæ€§ä¸‹é™ï¼›æ­¤å¤–ï¼ŒGemini åœ¨çœŸå®æ ‡ç­¾ä¸‹è¡¨ç°å‡ºä¸¥é‡çš„è‡ªæˆ‘è´¬ä½ (self-deprecation)ï¼Œè€Œ Claude åˆ™æ˜¾ç¤ºå‡ºå¼ºçƒˆçš„è‡ªæˆ‘åå¥½ (self-preference)ã€‚è¿™äº›ç»“æœå¯¹å½“å‰çš„ LLM-as-judge èŒƒå¼æå‡ºäº†æŒ‘æˆ˜ï¼Œå¹¶å¼ºè°ƒäº†åœ¨è‡ªåŠ¨åŒ–è¯„ä¼°å’Œæ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­é‡‡ç”¨ç›²è¯„åè®® (blind evaluation protocols) åŠå¤šæ¨¡å‹éªŒè¯æ¡†æ¶ä»¥ç¡®ä¿å…¬å¹³æ€§ä¸æœ‰æ•ˆæ€§çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21164v3",
      "published_date": "2025-08-28 18:59:23 UTC",
      "updated_date": "2025-10-09 20:01:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:47:20.394195+00:00"
    },
    {
      "arxiv_id": "2508.21154v1",
      "title": "RadGS-Reg: Registering Spine CT with Biplanar X-rays via Joint 3D Radiative Gaussians Reconstruction and 3D/3D Registration",
      "title_zh": "RadGS-Regï¼šé€šè¿‡è”åˆ 3D è¾å°„é«˜æ–¯é‡å»ºä¸ 3D/3D é…å‡†å®ç°è„ŠæŸ± CT ä¸åŒå¹³é¢ X å°„çº¿çš„é…å‡†",
      "authors": [
        "Ao Shen",
        "Xueming Fu",
        "Junfeng Jiang",
        "Qiang Zeng",
        "Ye Tang",
        "Zhengming Chen",
        "Luming Nong",
        "Feng Wang",
        "S. Kevin Zhou"
      ],
      "abstract": "Computed Tomography (CT)/X-ray registration in image-guided navigation remains challenging because of its stringent requirements for high accuracy and real-time performance. Traditional \"render and compare\" methods, relying on iterative projection and comparison, suffer from spatial information loss and domain gap. 3D reconstruction from biplanar X-rays supplements spatial and shape information for 2D/3D registration, but current methods are limited by dense-view requirements and struggles with noisy X-rays. To address these limitations, we introduce RadGS-Reg, a novel framework for vertebral-level CT/X-ray registration through joint 3D Radiative Gaussians (RadGS) reconstruction and 3D/3D registration. Specifically, our biplanar X-rays vertebral RadGS reconstruction module explores learning-based RadGS reconstruction method with a Counterfactual Attention Learning (CAL) mechanism, focusing on vertebral regions in noisy X-rays. Additionally, a patient-specific pre-training strategy progressively adapts the RadGS-Reg from simulated to real data while simultaneously learning vertebral shape prior knowledge. Experiments on in-house datasets demonstrate the state-of-the-art performance for both tasks, surpassing existing methods. The code is available at: https://github.com/shenao1995/RadGS_Reg.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RadGS-Regï¼Œä¸€ä¸ªä¸“ä¸ºæ¤ä½“çº§ CT/X-ray æ³¨å†Œ (Registration) è®¾è®¡çš„åˆ›æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿâ€œæ¸²æŸ“å¹¶æ¯”è¾ƒâ€ (render and compare) æ–¹æ³•ä¸­å­˜åœ¨çš„ç©ºé—´ä¿¡æ¯ä¸¢å¤±å’Œé¢†åŸŸå·®è·é—®é¢˜ã€‚è¯¥æ¡†æ¶å°† 3D Radiative Gaussians (RadGS) é‡å»ºä¸ 3D/3D æ³¨å†Œç›¸ç»“åˆï¼Œé€šè¿‡åŒå¹³é¢ X-ray è¡¥å……ç¼ºå¤±çš„ç©ºé—´å’Œå½¢çŠ¶ä¿¡æ¯ã€‚ä¸ºäº†åº”å¯¹å™ªå£° X-ray çš„æŒ‘æˆ˜ï¼ŒRadGS-Reg å¼•å…¥äº†åäº‹å®æ³¨æ„åŠ›å­¦ä¹ æœºåˆ¶ (Counterfactual Attention Learning, CAL) ä»¥ç²¾å‡†å®šä½æ¤ä½“åŒºåŸŸã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨äº†ä¸€ç§æ‚£è€…ç‰¹å¼‚æ€§çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»æ¨¡æ‹Ÿæ•°æ®å¹³æ»‘è¿‡æ¸¡åˆ°çœŸå®æ•°æ®ï¼Œå¹¶æœ‰æ•ˆå­¦ä¹ æ¤ä½“å½¢çŠ¶å…ˆéªŒçŸ¥è¯†ã€‚åœ¨å†…éƒ¨æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRadGS-Reg åœ¨ä¸‰ç»´é‡å»ºä¸æ³¨å†Œä»»åŠ¡ä¸Šå‡å±•ç°å‡ºè¶…è¶Šç°æœ‰æ–¹æ³•çš„ SOTA æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.21154v1",
      "published_date": "2025-08-28 18:40:13 UTC",
      "updated_date": "2025-08-28 18:40:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:47:19.392650+00:00"
    },
    {
      "arxiv_id": "2508.21153v1",
      "title": "WaveLLDM: Design and Development of a Lightweight Latent Diffusion Model for Speech Enhancement and Restoration",
      "title_zh": "WaveLLDMï¼šé¢å‘è¯­éŸ³å¢å¼ºä¸ä¿®å¤çš„è½»é‡çº§æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„è®¾è®¡ä¸å¼€å‘",
      "authors": [
        "Kevin Putra Santoso",
        "Rizka Wakhidatus Sholikah",
        "Raden Venantius Hari Ginardi"
      ],
      "abstract": "High-quality audio is essential in a wide range of applications, including online communication, virtual assistants, and the multimedia industry. However, degradation caused by noise, compression, and transmission artifacts remains a major challenge. While diffusion models have proven effective for audio restoration, they typically require significant computational resources and struggle to handle longer missing segments. This study introduces WaveLLDM (Wave Lightweight Latent Diffusion Model), an architecture that integrates an efficient neural audio codec with latent diffusion for audio restoration and denoising. Unlike conventional approaches that operate in the time or spectral domain, WaveLLDM processes audio in a compressed latent space, reducing computational complexity while preserving reconstruction quality. Empirical evaluations on the Voicebank+DEMAND test set demonstrate that WaveLLDM achieves accurate spectral reconstruction with low Log-Spectral Distance (LSD) scores (0.48 to 0.60) and good adaptability to unseen data. However, it still underperforms compared to state-of-the-art methods in terms of perceptual quality and speech clarity, with WB-PESQ scores ranging from 1.62 to 1.71 and STOI scores between 0.76 and 0.78. These limitations are attributed to suboptimal architectural tuning, the absence of fine-tuning, and insufficient training duration. Nevertheless, the flexible architecture that combines a neural audio codec and latent diffusion model provides a strong foundation for future development.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†WaveLLDM (Wave Lightweight Latent Diffusion Model)ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†é«˜æ•ˆç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨(Neural Audio Codec)ä¸æ½œæ‰©æ•£æ¨¡å‹(Latent Diffusion Model)çš„æ¶æ„ï¼Œä¸“é—¨ç”¨äºè¯­éŸ³å¢å¼ºå’Œä¿®å¤ä»»åŠ¡ã€‚è¯¥æ¨¡å‹é€šè¿‡åœ¨å‹ç¼©çš„æ½œç©ºé—´(Latent Space)ä¸­å¤„ç†éŸ³é¢‘ï¼Œæœ‰æ•ˆé™ä½äº†ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹çš„é«˜è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶å¢å¼ºäº†å¤„ç†é•¿ç‰‡æ®µç¼ºå¤±çš„èƒ½åŠ›ã€‚åœ¨ Voicebank+DEMAND æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒWaveLLDM å®ç°äº†è¾ƒä½çš„å¯¹æ•°é¢‘è°±è·ç¦»(LSD)å¾—åˆ†ï¼Œè¯æ˜äº†å…¶åœ¨é¢‘è°±é‡å»ºæ–¹é¢çš„å‡†ç¡®æ€§å’Œå¯¹æœªçŸ¥æ•°æ®çš„é€‚åº”æ€§ã€‚è™½ç„¶å—é™äºè®­ç»ƒæ—¶é•¿å’Œæ¶æ„å¾®è°ƒï¼Œè¯¥æ¨¡å‹åœ¨ WB-PESQ å’Œ STOI ç­‰æ„ŸçŸ¥è´¨é‡æŒ‡æ ‡ä¸Šå°šæœªè¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ï¼Œä½†å…¶çµæ´»çš„æ¶æ„è®¾è®¡ä¸ºæœªæ¥çš„è½»é‡çº§è¯­éŸ³å¤„ç†ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21153v1",
      "published_date": "2025-08-28 18:38:42 UTC",
      "updated_date": "2025-08-28 18:38:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:47:38.265079+00:00"
    },
    {
      "arxiv_id": "2508.21148v2",
      "title": "A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers",
      "title_zh": "ç§‘å­¦å¤§è¯­è¨€æ¨¡å‹ç»¼è¿°ï¼šä»æ•°æ®åŸºçŸ³åˆ°æ™ºèƒ½ä½“å‰æ²¿",
      "authors": [
        "Ming Hu",
        "Chenglong Ma",
        "Wei Li",
        "Wanghan Xu",
        "Jiamin Wu",
        "Jucheng Hu",
        "Tianbin Li",
        "Guohang Zhuang",
        "Jiaqi Liu",
        "Yingzhou Lu",
        "Ying Chen",
        "Chaoyang Zhang",
        "Cheng Tan",
        "Jie Ying",
        "Guocheng Wu",
        "Shujian Gao",
        "Pengcheng Chen",
        "Jiashi Lin",
        "Haitao Wu",
        "Lulu Chen",
        "Fengxiang Wang",
        "Yuanyuan Zhang",
        "Xiangyu Zhao",
        "Feilong Tang",
        "Encheng Su",
        "Junzhi Ning",
        "Xinyao Liu",
        "Ye Du",
        "Changkai Ji",
        "Pengfei Jiang",
        "Cheng Tang",
        "Ziyan Huang",
        "Jiyao Liu",
        "Jiaqi Wei",
        "Yuejin Yang",
        "Xiang Zhang",
        "Guangshuai Wang",
        "Yue Yang",
        "Huihui Xu",
        "Ziyang Chen",
        "Yizhou Wang",
        "Chen Tang",
        "Jianyu Wu",
        "Yuchen Ren",
        "Siyuan Yan",
        "Zhonghua Wang",
        "Zhongxing Xu",
        "Shiyan Su",
        "Shangquan Sun",
        "Runkai Zhao",
        "Zhisheng Zhang",
        "Dingkang Yang",
        "Jinjie Wei",
        "Jiaqi Wang",
        "Jiahao Xu",
        "Jiangtao Yan",
        "Wenhao Tang",
        "Hongze Zhu",
        "Yu Liu",
        "Fudi Wang",
        "Yiqing Shen",
        "Yuanfeng Ji",
        "Yanzhou Su",
        "Tong Xie",
        "Hongming Shan",
        "Chun-Mei Feng",
        "Zhi Hou",
        "Diping Song",
        "Lihao Liu",
        "Yanyan Huang",
        "Lequan Yu",
        "Bin Fu",
        "Shujun Wang",
        "Xiaomeng Li",
        "Xiaowei Hu",
        "Yun Gu",
        "Ben Fei",
        "Benyou Wang",
        "Yuewen Cao",
        "Minjie Shen",
        "Jie Xu",
        "Haodong Duan",
        "Fang Yan",
        "Hongxia Hao",
        "Jielan Li",
        "Jiajun Du",
        "Yanbo Wang",
        "Imran Razzak",
        "Zhongying Deng",
        "Chi Zhang",
        "Lijun Wu",
        "Conghui He",
        "Zhaohui Lu",
        "Jinhai Huang",
        "Wenqi Shao",
        "Yihao Liu",
        "Siqi Luo",
        "Yi Xin",
        "Xiaohong Liu",
        "Fenghua Ling",
        "Yuqiang Li",
        "Aoran Wang",
        "Siqi Sun",
        "Qihao Zheng",
        "Nanqing Dong",
        "Tianfan Fu",
        "Dongzhan Zhou",
        "Yan Lu",
        "Wenlong Zhang",
        "Jin Ye",
        "Jianfei Cai",
        "Yirong Chen",
        "Wanli Ouyang",
        "Yu Qiao",
        "Zongyuan Ge",
        "Shixiang Tang",
        "Junjun He",
        "Chunfeng Song",
        "Lei Bai",
        "Bowen Zhou"
      ],
      "abstract": "Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands -- heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base. Collectively, this work provides a roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as a true partner in accelerating scientific discovery.",
      "tldr_zh": "è¯¥ç»¼è¿°å¯¹ç§‘å­¦å¤§è¯­è¨€æ¨¡å‹(Scientific Large Language Models, Sci-LLMs)è¿›è¡Œäº†å…¨é¢çš„ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„ç³»ç»Ÿæ€§æ€»ç»“ï¼Œå°†æ¨¡å‹çš„å‘å±•é‡æ–°å®šä¹‰ä¸ºä¸å…¶åº•å±‚æ•°æ®åŸºç¡€çš„ååŒæ¼”åŒ–ã€‚ç ”ç©¶æ„å»ºäº†ç»Ÿä¸€çš„ç§‘å­¦æ•°æ®åˆ†ç±»æ³•å’Œç§‘å­¦çŸ¥è¯†åˆ†å±‚æ¨¡å‹ï¼Œé‡ç‚¹åˆ†æäº†ç§‘å­¦è¯­æ–™åº“åœ¨å¤šæ¨¡æ€ã€è·¨å°ºåº¦åŠé¢†åŸŸç‰¹å®šæ€§æ–¹é¢é¢ä¸´çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹270å¤šä¸ªè®­ç»ƒæ•°æ®é›†å’Œ190å¤šä¸ªåŸºå‡†æ•°æ®é›†çš„æ·±å…¥åˆ†æï¼Œæ–‡ç« æ­ç¤ºäº†Sci-LLMsåœ¨å¤„ç†å¼‚æ„åŠå¤šå°ºåº¦æ•°æ®ã€ä¿æŒé¢†åŸŸä¸å˜æ€§(Domain Invariance)ä»¥åŠå®ç°è·¨æ¨¡æ€æ¨ç†æ–¹é¢çš„ç‰¹æ®Šéœ€æ±‚ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¿½è¸ªäº†è¯„ä¼°æ–¹å¼ä»é™æ€æµ‹è¯•å‘ä¾§é‡ç§‘å­¦å‘ç°è¿‡ç¨‹çš„åŠ¨æ€åè®®çš„æ¼”å˜ã€‚æœ€ç»ˆï¼Œæ–‡ç« æå‡ºäº†å‘åŸºäºSci-LLMsçš„è‡ªä¸»æ™ºèƒ½ä½“(Autonomous Agents)é—­ç¯ç³»ç»Ÿè½¬å˜çš„æœªæ¥èŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡è‡ªåŠ¨åŒ–å®éªŒä¸éªŒè¯æ„å»ºèƒ½çœŸæ­£åŠ é€Ÿç§‘å­¦å‘ç°çš„å¯ä¿¡äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21148v2",
      "published_date": "2025-08-28 18:30:52 UTC",
      "updated_date": "2025-10-18 04:52:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:47:40.055384+00:00"
    },
    {
      "arxiv_id": "2509.02586v2",
      "title": "MitoDetect++: A Domain-Robust Pipeline for Mitosis Detection and Atypical Subtyping",
      "title_zh": "MitoDetect++ï¼šä¸€ç§ç”¨äºæœ‰ä¸åˆ†è£‚æ£€æµ‹ä¸éå…¸å‹äºšå‹åˆ†ç±»çš„é¢†åŸŸç¨³å¥å‹æµæ°´çº¿",
      "authors": [
        "Esha Sadia Nasir",
        "Jiaqi Lv",
        "Mostafa Jahanifar",
        "Shan E Ahmed Raza"
      ],
      "abstract": "Automated detection and classification of mitotic figures especially distinguishing atypical from normal remain critical challenges in computational pathology. We present MitoDetect++, a unified deep learning pipeline designed for the MIDOG 2025 challenge, addressing both mitosis detection and atypical mitosis classification. For detection (Track 1), we employ a U-Net-based encoder-decoder architecture with EfficientNetV2-L as the backbone, enhanced with attention modules, and trained via combined segmentation losses. For classification (Track 2), we leverage the Virchow2 vision transformer, fine-tuned efficiently using Low-Rank Adaptation (LoRA) to minimize resource consumption. To improve generalization and mitigate domain shifts, we integrate strong augmentations, focal loss, and group-aware stratified 5-fold cross-validation. At inference, we deploy test-time augmentation (TTA) to boost robustness. Our method achieves a balanced accuracy of 0.892 across validation domains, highlighting its clinical applicability and scalability across tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MitoDetect++ï¼Œä¸€ä¸ªä¸ºMIDOG 2025æŒ‘æˆ˜èµ›è®¾è®¡çš„ç»Ÿä¸€æ·±åº¦å­¦ä¹ æµæ°´çº¿ï¼Œæ—¨åœ¨è§£å†³è®¡ç®—ç—…ç†å­¦ä¸­ç»†èƒæœ‰ä¸åˆ†è£‚è±¡(mitosis)çš„è‡ªåŠ¨æ£€æµ‹åŠå…¶éå…¸å‹(atypical)äºšå‹åˆ†ç±»éš¾é¢˜ã€‚åœ¨æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ¡ˆé‡‡ç”¨äº†ä»¥EfficientNetV2-Lä¸ºéª¨å¹²ç½‘ç»œçš„U-Netç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›æ¨¡å—(attention modules)å’Œç»„åˆåˆ†å‰²æŸå¤±è¿›è¡Œå¼ºåŒ–ã€‚é’ˆå¯¹åˆ†ç±»ä»»åŠ¡ï¼Œç ”ç©¶åˆ©ç”¨äº†Virchow2è§†è§‰å˜æ¢å™¨æ¨¡å‹ï¼Œå¹¶ç»“åˆä½ç§©è‡ªé€‚åº”(LoRA)æŠ€æœ¯è¿›è¡Œé«˜æ•ˆå¾®è°ƒä»¥é™ä½èµ„æºæ¶ˆè€—ã€‚ä¸ºäº†å¢å¼ºæ³›åŒ–èƒ½åŠ›å¹¶ç¼“è§£é¢†åŸŸåç§»ï¼Œè¯¥ç®¡çº¿é›†æˆäº†å¼ºæ•°æ®å¢å¼ºã€focal lossä»¥åŠåˆ†ç»„æ„ŸçŸ¥åˆ†å±‚äº”æŠ˜äº¤å‰éªŒè¯ç­–ç•¥ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µéƒ¨ç½²äº†æµ‹è¯•æ—¶å¢å¼º(TTA)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMitoDetect++åœ¨ä¸åŒéªŒè¯åŸŸä¸­è¾¾åˆ°äº†0.892çš„å¹³è¡¡å‡†ç¡®ç‡ï¼Œå±•ç°äº†æ˜¾è‘—çš„é¢†åŸŸé²æ£’æ€§ã€‚è¯¥ç ”ç©¶è¯æ˜äº†è¯¥ç³»ç»Ÿåœ¨å¤šä»»åŠ¡ç—…ç†è¯Šæ–­åœºæ™¯ä¸‹çš„ä¸´åºŠé€‚ç”¨æ€§ä¸å¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.02586v2",
      "published_date": "2025-08-28 18:19:51 UTC",
      "updated_date": "2025-09-04 22:27:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:48:14.259759+00:00"
    },
    {
      "arxiv_id": "2508.21135v2",
      "title": "HiddenObject: Modality-Agnostic Fusion for Multimodal Hidden Object Detection",
      "title_zh": "HiddenObjectï¼šé¢å‘å¤šæ¨¡æ€éšè”½ç›®æ ‡æ£€æµ‹çš„æ¨¡æ€æ— å…³èåˆ",
      "authors": [
        "Harris Song",
        "Tuan-Anh Vu",
        "Sanjith Menon",
        "Sriram Narasimhan",
        "M. Khalid Jawed"
      ],
      "abstract": "Detecting hidden or partially concealed objects remains a fundamental challenge in multimodal environments, where factors like occlusion, camouflage, and lighting variations significantly hinder performance. Traditional RGB-based detection methods often fail under such adverse conditions, motivating the need for more robust, modality-agnostic approaches. In this work, we present HiddenObject, a fusion framework that integrates RGB, thermal, and depth data using a Mamba-based fusion mechanism. Our method captures complementary signals across modalities, enabling enhanced detection of obscured or camouflaged targets. Specifically, the proposed approach identifies modality-specific features and fuses them in a unified representation that generalizes well across challenging scenarios. We validate HiddenObject across multiple benchmark datasets, demonstrating state-of-the-art or competitive performance compared to existing methods. These results highlight the efficacy of our fusion design and expose key limitations in current unimodal and naÃ¯ve fusion strategies. More broadly, our findings suggest that Mamba-based fusion architectures can significantly advance the field of multimodal object detection, especially under visually degraded or complex conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸‹ç”±äºé®æŒ¡ã€ä¼ªè£…å’Œå…‰ç…§å˜åŒ–å¯¼è‡´éšè—ç›®æ ‡æ£€æµ‹å›°éš¾çš„é—®é¢˜ï¼Œæå‡ºäº† HiddenObject èåˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸€ç§ Mamba-based èåˆæœºåˆ¶ï¼Œæœ‰æ•ˆæ•´åˆäº† RGBã€çº¢å¤–ï¼ˆthermalï¼‰å’Œæ·±åº¦ï¼ˆdepthï¼‰æ•°æ®ï¼Œé€šè¿‡æ•æ‰ä¸åŒæ¨¡æ€é—´çš„äº’è¡¥ä¿¡å·æ¥å¢å¼ºæ£€æµ‹æ€§èƒ½ã€‚HiddenObject èƒ½å¤Ÿè¯†åˆ«æ¨¡æ€ç‰¹å®šç‰¹å¾ï¼ˆmodality-specific featuresï¼‰å¹¶å°†å…¶è½¬åŒ–ä¸ºç»Ÿä¸€çš„è¡¨ç¤ºï¼Œä»è€Œåœ¨å¤æ‚åœºæ™¯ä¸­å®ç°è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº† state-of-the-art æˆ–æå…·ç«äº‰åŠ›çš„æ°´å¹³ï¼Œå…‹æœäº†ä¼ ç»Ÿå•æ¨¡æ€åŠç®€å•èåˆç­–ç•¥ï¼ˆnaÃ¯ve fusion strategiesï¼‰çš„å±€é™æ€§ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ Mamba-based èåˆæ¶æ„åœ¨å¤„ç†è§†è§‰é€€åŒ–æ¡ä»¶ä¸‹çš„å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—çš„å…ˆè¿›æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "fix typos",
      "pdf_url": "https://arxiv.org/pdf/2508.21135v2",
      "published_date": "2025-08-28 18:09:22 UTC",
      "updated_date": "2025-09-12 02:23:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:48:07.855334+00:00"
    },
    {
      "arxiv_id": "2509.00117v2",
      "title": "Embodied AI: Emerging Risks and Opportunities for Policy Action",
      "title_zh": "å…·èº«æ™ºèƒ½ï¼šæ”¿ç­–è¡ŒåŠ¨çš„æ–°å…´é£é™©ä¸æœºé‡",
      "authors": [
        "Jared Perlo",
        "Alexander Robey",
        "Fazl Barez",
        "Luciano Floridi",
        "Jakob MÃ¶kander"
      ],
      "abstract": "The field of embodied AI (EAI) is rapidly advancing. Unlike virtual AI, EAI systems can exist in, learn from, reason about, and act in the physical world. With recent advances in AI models and hardware, EAI systems are becoming increasingly capable across wider operational domains. While EAI systems can offer many benefits, they also pose significant risks, including physical harm from malicious use, mass surveillance, as well as economic and societal disruption. These risks require urgent attention from policymakers, as existing policies governing industrial robots and autonomous vehicles are insufficient to address the full range of concerns EAI systems present. To help address this issue, this paper makes three contributions. First, we provide a taxonomy of the physical, informational, economic, and social risks EAI systems pose. Second, we analyze policies in the US, EU, and UK to assess how existing frameworks address these risks and to identify critical gaps. We conclude by offering policy recommendations for the safe and beneficial deployment of EAI systems, such as mandatory testing and certification schemes, clarified liability frameworks, and strategies to manage EAI's potentially transformative economic and societal impacts.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å…·èº«æ™ºèƒ½ (Embodied AI) é¢†åŸŸçš„å¿«é€Ÿæ¼”è¿›åŠå…¶å¯¹æ”¿ç­–åˆ¶å®šçš„æ–°å…´é£é™©ä¸æœºé‡ã€‚ä¸ä¼ ç»Ÿçš„è™šæ‹Ÿ AI ä¸åŒï¼Œå…·èº«æ™ºèƒ½ç³»ç»Ÿèƒ½å¤Ÿåœ¨ç‰©ç†ä¸–ç•Œä¸­è¿›è¡Œäº¤äº’ä¸æ¨ç†ï¼Œä½†è¿™ä¹Ÿå¸¦æ¥äº†ç‰©ç†ä¼¤å®³ã€å¤§è§„æ¨¡ç›‘æ§åŠç¤¾ä¼šç»æµåŠ¨è¡ç­‰æ˜¾è‘—é£é™©ã€‚è®ºæ–‡é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªåŒ…å«ç‰©ç†ã€ä¿¡æ¯ã€ç»æµå’Œç¤¾ä¼šç»´åº¦çš„é£é™©åˆ†ç±»æ³• (taxonomy)ï¼Œç³»ç»Ÿåœ°æ¢³ç†äº†å…·èº«æ™ºèƒ½ç³»ç»Ÿçš„æ½œåœ¨å¨èƒã€‚æ¥ç€ï¼Œä½œè€…å¯¹æ¯”åˆ†æäº†ç¾å›½ã€æ¬§ç›Ÿå’Œè‹±å›½çš„ç°æœ‰æ”¿ç­–ï¼Œæ­ç¤ºäº†å½“å‰æ³•è§„åœ¨åº”å¯¹å…·èº«æ™ºèƒ½æŒ‘æˆ˜æ—¶å­˜åœ¨çš„å…³é”®åˆ¶åº¦ç©ºç™½ã€‚æœ€åï¼Œè®ºæ–‡æå‡ºäº†ç¡®ä¿å…·èº«æ™ºèƒ½å®‰å…¨éƒ¨ç½²çš„æ”¿ç­–å»ºè®®ï¼ŒåŒ…æ‹¬å®æ–½å¼ºåˆ¶æ€§æµ‹è¯•ä¸è®¤è¯è®¡åˆ’ (certification schemes)ã€å®Œå–„è´£ä»»æ¡†æ¶ (liability frameworks) ä»¥åŠåˆ¶å®šåº”å¯¹å˜é©æ€§ç»æµå½±å“çš„ç­–ç•¥ã€‚è¿™é¡¹å·¥ä½œä¸ºæ”¿ç­–åˆ¶å®šè€…åº”å¯¹å…·èº«æ™ºèƒ½å¸¦æ¥çš„ç¤¾ä¼šå½±å“æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æŒä¸å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00117v2",
      "published_date": "2025-08-28 17:59:07 UTC",
      "updated_date": "2025-09-03 17:55:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:48:11.754874+00:00"
    },
    {
      "arxiv_id": "2508.21063v1",
      "title": "Prompt-to-Product: Generative Assembly via Bimanual Manipulation",
      "title_zh": "Prompt-to-Productï¼šåŸºäºåŒæ‰‹æ“æ§çš„ç”Ÿæˆå¼è£…é…",
      "authors": [
        "Ruixuan Liu",
        "Philip Huang",
        "Ava Pun",
        "Kangle Deng",
        "Shobhit Aggarwal",
        "Kevin Tang",
        "Michelle Liu",
        "Deva Ramanan",
        "Jun-Yan Zhu",
        "Jiaoyang Li",
        "Changliu Liu"
      ],
      "abstract": "Creating assembly products demands significant manual effort and expert knowledge in 1) designing the assembly and 2) constructing the product. This paper introduces Prompt-to-Product, an automated pipeline that generates real-world assembly products from natural language prompts. Specifically, we leverage LEGO bricks as the assembly platform and automate the process of creating brick assembly structures. Given the user design requirements, Prompt-to-Product generates physically buildable brick designs, and then leverages a bimanual robotic system to construct the real assembly products, bringing user imaginations into the real world. We conduct a comprehensive user study, and the results demonstrate that Prompt-to-Product significantly lowers the barrier and reduces manual effort in creating assembly products from imaginative ideas.",
      "tldr_zh": "é’ˆå¯¹ç»„è£…äº§å“åˆ›é€ è¿‡ç¨‹ä¸­å¯¹æ‰‹å·¥åŠ³åŠ¨å’Œä¸“å®¶çŸ¥è¯†çš„é«˜åº¦éœ€æ±‚ï¼Œè¯¥ç ”ç©¶æå‡ºäº†Prompt-to-Productï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½å¤Ÿä»è‡ªç„¶è¯­è¨€æç¤º(natural language prompts)ç›´æ¥ç”ŸæˆçœŸå®ä¸–ç•Œç»„è£…äº§å“çš„è‡ªåŠ¨åŒ–æµç¨‹ã€‚è¯¥æ¡†æ¶åˆ©ç”¨LEGOç§¯æœ¨ä½œä¸ºç»„è£…å¹³å°ï¼Œå®ç°äº†ç§¯æœ¨ç»„è£…ç»“æ„åˆ›å»ºè¿‡ç¨‹çš„è‡ªåŠ¨åŒ–ã€‚åœ¨æ¥æ”¶åˆ°ç”¨æˆ·çš„è®¾è®¡éœ€æ±‚åï¼ŒPrompt-to-Productèƒ½å¤Ÿç”Ÿæˆç‰©ç†ä¸Šå¯æ„å»ºçš„ç§¯æœ¨è®¾è®¡ï¼Œå¹¶è¿›ä¸€æ­¥åˆ©ç”¨åŒè‡‚æœºå™¨äººç³»ç»Ÿ(bimanual robotic system)æ¥æ„å»ºçœŸå®çš„ç»„è£…äº§å“ï¼ŒæˆåŠŸå°†ç”¨æˆ·çš„æƒ³è±¡åŠ›è½¬åŒ–ä¸ºç°å®ã€‚ç»¼åˆç”¨æˆ·ç ”ç©¶çš„ç»“æœè¡¨æ˜ï¼ŒPrompt-to-Productæ˜¾è‘—é™ä½äº†ä»åˆ›æ„æ„æ€åˆ°åˆ›å»ºç»„è£…äº§å“çš„é—¨æ§›ï¼Œå¹¶æœ‰æ•ˆå‡å°‘äº†ç›¸å…³çš„äººå·¥å·¥ä½œé‡ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "12 pages, 10 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.21063v1",
      "published_date": "2025-08-28 17:59:05 UTC",
      "updated_date": "2025-08-28 17:59:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:48:09.564145+00:00"
    },
    {
      "arxiv_id": "2508.21061v1",
      "title": "OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models",
      "title_zh": "OnGoalï¼šå¤§è¯­è¨€æ¨¡å‹å¤šè½®å¯¹è¯ä¸­çš„å¯¹è¯ç›®æ ‡è¿½è¸ªä¸å¯è§†åŒ–",
      "authors": [
        "Adam Coscia",
        "Shunan Guo",
        "Eunyee Koh",
        "Alex Endert"
      ],
      "abstract": "As multi-turn dialogues with large language models (LLMs) grow longer and more complex, how can users better evaluate and review progress on their conversational goals? We present OnGoal, an LLM chat interface that helps users better manage goal progress. OnGoal provides real-time feedback on goal alignment through LLM-assisted evaluation, explanations for evaluation results with examples, and overviews of goal progression over time, enabling users to navigate complex dialogues more effectively. Through a study with 20 participants on a writing task, we evaluate OnGoal against a baseline chat interface without goal tracking. Using OnGoal, participants spent less time and effort to achieve their goals while exploring new prompting strategies to overcome miscommunication, suggesting tracking and visualizing goals can enhance engagement and resilience in LLM dialogues. Our findings inspired design implications for future LLM chat interfaces that improve goal communication, reduce cognitive load, enhance interactivity, and enable feedback to improve LLM performance.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† OnGoalï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®¾è®¡çš„å¤šè½®å¯¹è¯èŠå¤©ç•Œé¢ï¼Œæ—¨åœ¨ååŠ©ç”¨æˆ·æ›´å¥½åœ°è¿½è¸ªå’Œè¯„ä¼°å¯¹è¯ç›®æ ‡çš„è¾¾æˆæƒ…å†µã€‚OnGoal åˆ©ç”¨ LLM è¾…åŠ©è¯„ä¼°æä¾›ç›®æ ‡å¯¹é½çš„å®æ—¶åé¦ˆï¼Œå¹¶é’ˆå¯¹è¯„ä¼°ç»“æœç»™å‡ºå¸¦æœ‰ç¤ºä¾‹çš„è§£é‡Šï¼ŒåŒæ—¶å±•ç¤ºç›®æ ‡éšæ—¶é—´æ¨ç§»çš„è¿›åº¦æ€»è§ˆï¼Œå¸®åŠ©ç”¨æˆ·åœ¨å¤æ‚å¯¹è¯ä¸­æ›´æœ‰æ•ˆåœ°å¯¼èˆªã€‚ç ”ç©¶äººå‘˜é€šè¿‡ä¸€é¡¹æ¶‰åŠ 20 åå‚ä¸è€…çš„å†™ä½œä»»åŠ¡ï¼Œå°† OnGoal ä¸æ²¡æœ‰ç›®æ ‡è¿½è¸ªåŠŸèƒ½çš„åŸºå‡†ç•Œé¢è¿›è¡Œäº†å¯¹æ¯”è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨ OnGoal çš„å‚ä¸è€…åœ¨è¾¾æˆç›®æ ‡æ—¶æŠ•å…¥çš„æ—¶é—´å’Œç²¾åŠ›æ˜¾è‘—å‡å°‘ï¼Œå¹¶å€¾å‘äºå°è¯•æ–°çš„æç¤ºè¯ç­–ç•¥ï¼ˆprompting strategiesï¼‰æ¥è§£å†³æ²Ÿé€šè¯¯åŒºï¼Œè¡¨æ˜ç›®æ ‡çš„å¯è§†åŒ–è¿½è¸ªå¢å¼ºäº†ç”¨æˆ·åœ¨ LLM å¯¹è¯ä¸­çš„å‚ä¸åº¦å’ŒéŸ§æ€§ã€‚è¯¥ç ”ç©¶æœ€åæå‡ºäº†æœªæ¥ LLM èŠå¤©ç•Œé¢çš„è®¾è®¡å¯ç¤ºï¼Œå¼ºè°ƒäº†é€šè¿‡æ”¹å–„ç›®æ ‡æ²Ÿé€šå’Œé™ä½è®¤çŸ¥è´Ÿè·æ¥æå‡äº¤äº’ä½“éªŒä¸æ¨¡å‹æ€§èƒ½çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted to UIST 2025. 18 pages, 9 figures, 2 tables. For a demo video, see https://youtu.be/uobhmxo6EIE",
      "pdf_url": "https://arxiv.org/pdf/2508.21061v1",
      "published_date": "2025-08-28 17:58:29 UTC",
      "updated_date": "2025-08-28 17:58:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:48:15.050930+00:00"
    },
    {
      "arxiv_id": "2508.21058v3",
      "title": "Mixture of Contexts for Long Video Generation",
      "title_zh": "Mixture of Contextsï¼šé¢å‘é•¿è§†é¢‘ç”Ÿæˆçš„ä¸Šä¸‹æ–‡æ··åˆ",
      "authors": [
        "Shengqu Cai",
        "Ceyuan Yang",
        "Lvmin Zhang",
        "Yuwei Guo",
        "Junfei Xiao",
        "Ziyan Yang",
        "Yinghao Xu",
        "Zhenheng Yang",
        "Alan Yuille",
        "Leonidas Guibas",
        "Maneesh Agrawala",
        "Lu Jiang",
        "Gordon Wetzstein"
      ],
      "abstract": "Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºé•¿è§†é¢‘ç”Ÿæˆçš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºé•¿ä¸Šä¸‹æ–‡è®°å¿†ï¼Œç”±äºæ‰©æ•£ Transformer (diffusion transformers) ä¸­è‡ªæ³¨æ„åŠ›æœºåˆ¶ (self-attention) çš„äºŒæ¬¡æ–¹è®¡ç®—æˆæœ¬é™åˆ¶ï¼Œé•¿åºåˆ—çš„ç”Ÿæˆéš¾ä»¥æ‰©å±•ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† Mixture of Contexts (MoC)ï¼Œè¿™æ˜¯ä¸€ç§å¯å­¦ä¹ çš„ç¨€ç–æ³¨æ„åŠ›è·¯ç”±æ¨¡å—ï¼Œå°†é•¿ä¸Šä¸‹æ–‡ç”Ÿæˆè§†ä¸ºå†…éƒ¨ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ã€‚åœ¨ MoC æ¡†æ¶ä¸‹ï¼Œæ¯ä¸ªæŸ¥è¯¢åŠ¨æ€é€‰æ‹©å°‘é‡çš„å…³é”®ä¿¡æ¯å—ä¸å¼ºåˆ¶æ€§é”šç‚¹ï¼ˆå¦‚æ–‡æœ¬æè¿°å’Œå±€éƒ¨çª—å£ï¼‰è¿›è¡Œäº¤äº’ï¼Œå¹¶é€šè¿‡å› æœè·¯ç”±ç¡®ä¿ç”Ÿæˆçš„é€»è¾‘æ€§ã€‚éšç€æ•°æ®è§„æ¨¡æ‰©å¤§å’Œè·¯ç”±çš„ç¨€ç–åŒ–ï¼Œæ¨¡å‹èƒ½æœ‰æ•ˆåˆ†é…è®¡ç®—èµ„æºä»¥æå–æ˜¾è‘—å†å²ä¿¡æ¯ï¼Œåœ¨é•¿è¾¾æ•°åˆ†é’Ÿçš„è§†é¢‘ä¸­ä¿æŒèº«ä»½ (identities)ã€åŠ¨ä½œå’Œåœºæ™¯çš„ä¸€è‡´æ€§ã€‚è¿™ç§æœºåˆ¶å®ç°äº†è¿‘çº¿æ€§æ‰©å±• (near-linear scaling)ï¼Œæ˜¾è‘—æå‡äº†æ•ˆç‡ï¼Œä¸ºæ•°åˆ†é’Ÿçº§é•¿è§†é¢‘çš„å®é™…è®­ç»ƒä¸åˆæˆå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "Project page: https://primecai.github.io/moc/",
      "pdf_url": "https://arxiv.org/pdf/2508.21058v3",
      "published_date": "2025-08-28 17:57:55 UTC",
      "updated_date": "2025-12-09 10:22:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:48:33.189989+00:00"
    },
    {
      "arxiv_id": "2508.21052v2",
      "title": "FakeParts: a New Family of AI-Generated DeepFakes",
      "title_zh": "FakePartsï¼šä¸€ç§æ–°å‹ AI ç”Ÿæˆæ·±åº¦ä¼ªé€ ",
      "authors": [
        "Ziyi Liu",
        "Firas Gabetni",
        "Awais Hussain Sani",
        "Xi Wang",
        "Soobash Daiboo",
        "Gaetan Brison",
        "Gianni Franchi",
        "Vicky Kalogeiton"
      ],
      "abstract": "We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations - ranging from altered facial expressions to object substitutions and background modifications - blend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection, we present FakePartsBench, the first large-scale benchmark specifically designed to capture the full spectrum of partial deepfakes. Comprising over 81K (including 44K FakeParts) videos with pixel- and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by up to 26% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current detectors and provides the necessary resources to develop methods robust to partial manipulations.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† FakePartsï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ DeepFakesï¼Œå…¶ç‰¹å¾æ˜¯å¯¹çœŸå®è§†é¢‘çš„ç‰¹å®šç©ºé—´åŒºåŸŸæˆ–æ—¶é—´ç‰‡æ®µè¿›è¡Œç»†å¾®ä¸”å±€éƒ¨çš„æ“çºµã€‚è¿™äº›å±€éƒ¨ç¯¡æ”¹åŒ…æ‹¬æ”¹å˜é¢éƒ¨è¡¨æƒ…ã€ç‰©ä½“æ›¿æ¢å’ŒèƒŒæ™¯ä¿®æ”¹ï¼Œèƒ½å¤Ÿä¸çœŸå®å…ƒç´ æ— ç¼èåˆï¼Œä½¿å…¶æå…·æ¬ºéª—æ€§ä¸”éš¾ä»¥æ£€æµ‹ã€‚ä¸ºäº†å¼¥è¡¥æ£€æµ‹é¢†åŸŸçš„ç©ºç™½ï¼Œä½œè€…æå‡ºäº† FakePartsBenchï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹å…¨è°±ç³»å±€éƒ¨ DeepFakes è®¾è®¡çš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡ 81,000 ä¸ªè§†é¢‘åŠåƒç´ çº§å’Œå¸§çº§çš„æ“çºµæ ‡æ³¨ã€‚ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„ DeepFakes ç›¸æ¯”ï¼ŒFakeParts ä½¿äººç±»çš„æ£€æµ‹å‡†ç¡®ç‡é™ä½äº†é«˜è¾¾ 26%ï¼Œä¸”ç›®å‰æœ€å…ˆè¿›çš„æ£€æµ‹æ¨¡å‹ä¹Ÿè¡¨ç°å‡ºç±»ä¼¼çš„æ€§èƒ½é€€åŒ–ã€‚è¯¥é¡¹å·¥ä½œæ­ç¤ºäº†ç°æœ‰æ£€æµ‹å™¨çš„ç´§è¿«æ¼æ´ï¼Œå¹¶ä¸ºå¼€å‘é’ˆå¯¹å±€éƒ¨æ“çºµçš„é²æ£’æ€§æ£€æµ‹æ–¹æ³•æä¾›äº†å¿…è¦çš„èµ„æºæ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21052v2",
      "published_date": "2025-08-28 17:55:14 UTC",
      "updated_date": "2025-12-19 16:10:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:48:27.265673+00:00"
    },
    {
      "arxiv_id": "2508.21051v2",
      "title": "Language Models and Logic Programs for Trustworthy Financial Reasoning",
      "title_zh": "é¢å‘å¯ä¿¡é‡‘èæ¨ç†çš„è¯­è¨€æ¨¡å‹ä¸é€»è¾‘ç¨‹åº",
      "authors": [
        "William Jurayj",
        "Nils Holzenberger",
        "Benjamin Van Durme"
      ],
      "abstract": "According to the United States Internal Revenue Service, ''the average American spends $\\$270$ and 13 hours filing their taxes''. Even beyond the U.S., tax filing requires complex reasoning, combining application of overlapping rules with numerical calculations. Because errors can incur costly penalties, any automated system must deliver high accuracy and auditability, making modern large language models (LLMs) poorly suited for this task. We propose an approach that integrates LLMs with a symbolic solver to calculate tax obligations. We evaluate variants of this system on the challenging StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for estimating the cost of deploying such a system based on real-world penalties for tax errors. We further show how combining up-front translation of plain-text rules into formal logic programs, combined with intelligently retrieved exemplars for formal case representations, can dramatically improve performance on this task and reduce costs to well below real-world averages. Our results demonstrate the promise and economic feasibility of neuro-symbolic architectures for increasing equitable access to reliable tax assistance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨ Large Language Models (LLMs) ä¸é€»è¾‘ç¨‹åºè§£å†³ç¨åŠ¡ç”³æŠ¥ä¸­çš„å¤æ‚æ¨ç†é—®é¢˜ï¼Œé’ˆå¯¹ç°æœ‰æ¨¡å‹åœ¨å¤„ç†é«˜ç²¾åº¦ã€å¯å®¡è®¡ä»»åŠ¡æ—¶çš„ä¸è¶³æå‡ºäº†æ”¹è¿›æ–¹æ¡ˆã€‚ä½œè€…æå‡ºäº†ä¸€ç§å°† LLMs ä¸ç¬¦å·æ±‚è§£å™¨ (Symbolic solver) ç›¸ç»“åˆçš„ç¥ç»ç¬¦å·æ¶æ„ (Neuro-symbolic architecture)ï¼Œæ—¨åœ¨æ›´å‡†ç¡®åœ°è®¡ç®—çº³ç¨ä¹‰åŠ¡ã€‚è¯¥ç³»ç»Ÿåœ¨ StAtutory Reasoning Assessment (SARA) æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åŸºäºç°å®ç¨åŠ¡é”™è¯¯ç½šæ¬¾çš„æ–°é¢–æˆæœ¬ä¼°ç®—æ–¹æ³•ã€‚ç ”ç©¶é€šè¿‡å°†çº¯æ–‡æœ¬è§„åˆ™é¢„å…ˆç¿»è¯‘ä¸ºå½¢å¼é€»è¾‘ç¨‹åº (Formal logic programs)ï¼Œå¹¶ç»“åˆæ™ºèƒ½æ£€ç´¢çš„å½¢å¼åŒ–æ¡ˆä¾‹èŒƒä¾‹ (Exemplars)ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…å¤§å¹…æé«˜äº†ä»»åŠ¡æ‰§è¡Œçš„å‡†ç¡®ç‡ï¼Œè¿˜å°†éƒ¨ç½²æˆæœ¬é™è‡³è¿œä½äºç°å®ä¸–ç•Œå¹³å‡æ°´å¹³çš„ç¨‹åº¦ã€‚è¿™ä¸€æˆæœè¯æ˜äº†ç¥ç»ç¬¦å·æ¶æ„åœ¨æä¾›å¯é ã€ç»æµä¸”å…¬å¹³çš„ç¨åŠ¡æ´åŠ©æœåŠ¡æ–¹é¢å…·æœ‰å·¨å¤§çš„åº”ç”¨æ½œåŠ›å’Œç»æµå¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21051v2",
      "published_date": "2025-08-28 17:55:07 UTC",
      "updated_date": "2025-08-29 21:48:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:48:44.593140+00:00"
    },
    {
      "arxiv_id": "2508.21048v1",
      "title": "Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning",
      "title_zh": "Veritasï¼šåŸºäºæ¨¡å¼æ„ŸçŸ¥æ¨ç†çš„å¯æ³›åŒ–æ·±åº¦ä¼ªé€ æ£€æµ‹",
      "authors": [
        "Hao Tan",
        "Jun Lan",
        "Zichang Tan",
        "Ajian Liu",
        "Chuanbiao Song",
        "Senyuan Shi",
        "Huijia Zhu",
        "Weiqiang Wang",
        "Jun Wan",
        "Zhen Lei"
      ],
      "abstract": "Deepfake detection remains a formidable challenge due to the complex and evolving nature of fake content in real-world scenarios. However, existing academic benchmarks suffer from severe discrepancies from industrial practice, typically featuring homogeneous training sources and low-quality testing images, which hinder the practical deployments of current detectors. To mitigate this gap, we introduce HydraFake, a dataset that simulates real-world challenges with hierarchical generalization testing. Specifically, HydraFake involves diversified deepfake techniques and in-the-wild forgeries, along with rigorous training and evaluation protocol, covering unseen model architectures, emerging forgery techniques and novel data domains. Building on this resource, we propose Veritas, a multi-modal large language model (MLLM) based deepfake detector. Different from vanilla chain-of-thought (CoT), we introduce pattern-aware reasoning that involves critical reasoning patterns such as \"planning\" and \"self-reflection\" to emulate human forensic process. We further propose a two-stage training pipeline to seamlessly internalize such deepfake reasoning capacities into current MLLMs. Experiments on HydraFake dataset reveal that although previous detectors show great generalization on cross-model scenarios, they fall short on unseen forgeries and data domains. Our Veritas achieves significant gains across different OOD scenarios, and is capable of delivering transparent and faithful detection outputs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Deepfake detectionåœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´çš„åŸºå‡†æµ‹è¯•ä¸å·¥ä¸šå®è·µè„±èŠ‚é—®é¢˜ï¼Œå¼•å…¥äº†HydraFakeæ•°æ®é›†ï¼Œé€šè¿‡æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„å±‚æ¬¡åŒ–æ³›åŒ–æµ‹è¯•ï¼Œæ¶µç›–äº†å¤šæ ·åŒ–çš„ä¼ªé€ æŠ€æœ¯å’Œæœªè§çš„æ•°æ®é¢†åŸŸã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œä½œè€…æå‡ºäº†åä¸ºVeritasçš„åŸºäºMLLMçš„æ£€æµ‹å™¨ï¼Œå¹¶å¼•å…¥äº†åŒ…å«planningå’Œself-reflectionçš„pattern-aware reasoningï¼Œä»¥æ¨¡æ‹Ÿäººç±»çš„å–è¯åˆ†æè¿‡ç¨‹ã€‚ä¸ºäº†æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œç ”ç©¶è®¾è®¡äº†ä¸“é—¨çš„two-stage training pipelineï¼Œå°†ä¼ªé€ æ¨ç†èƒ½åŠ›æ— ç¼é›†æˆè‡³å¤§æ¨¡å‹ä¸­ã€‚åœ¨HydraFakeæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒVeritasåœ¨å¤šç§OODåœºæ™¯ä¸‹å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½å¢ç›Šã€‚è¯¥æ¨¡å‹ä¸ä»…åœ¨å¤„ç†æœªè§ä¼ªé€ æ‰‹æ®µå’Œæ•°æ®é¢†åŸŸæ—¶è¡¨ç°ä¼˜å¼‚ï¼Œè¿˜èƒ½æä¾›é€æ˜ä¸”å¯ä¿¡çš„æ£€æµ‹ç»“æœï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰æ£€æµ‹å™¨åœ¨å¤æ‚ç°å®åœºæ™¯ä¸‹æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project: https://github.com/EricTan7/Veritas",
      "pdf_url": "https://arxiv.org/pdf/2508.21048v1",
      "published_date": "2025-08-28 17:53:05 UTC",
      "updated_date": "2025-08-28 17:53:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:48:34.983183+00:00"
    },
    {
      "arxiv_id": "2508.21113v2",
      "title": "R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning",
      "title_zh": "R-4Bï¼šé€šè¿‡åŒæ¨¡å¼é€€ç«ä¸å¼ºåŒ–å­¦ä¹ æ¿€å‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„é€šç”¨è‡ªåŠ¨æ€è€ƒèƒ½åŠ›",
      "authors": [
        "Qi Yang",
        "Bolin Ni",
        "Shiming Xiang",
        "Han Hu",
        "Houwen Peng",
        "Jie Jiang"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking capabilities have demonstrated remarkable performance on complex reasoning problems. However, this thinking process is redundant for simple problems solvable without complex reasoning. To address this inefficiency, we propose R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on problem complexity. The central idea of R-4B is to empower the model with both thinking and non-thinking capabilities using bi-mode annealing, and apply Bi-mode Policy Optimization (BPO) to improve the model's accuracy in determining whether to activate the thinking process. Specifically, we first train the model on a carefully curated dataset spanning various topics, which contains samples from both thinking and non-thinking modes. Then it undergoes a second phase of training under an improved GRPO framework, where the policy model is forced to generate responses from both modes for each input query. Experimental results show that R-4B achieves state-of-the-art performance across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks and achieves performance comparable to larger models such as Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower computational cost.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨å¤„ç†ç®€å•é—®é¢˜æ—¶æ€ç»´è¿‡ç¨‹è¿‡äºå†—ä½™çš„æ•ˆç‡é—®é¢˜ï¼Œæå‡ºäº†å…·æœ‰è‡ªåŠ¨æ€ç»´ (auto-thinking) èƒ½åŠ›çš„ R-4B æ¨¡å‹ã€‚è¯¥æ¨¡å‹çš„æ ¸å¿ƒåœ¨äºé€šè¿‡åŒæ¨¡å¼é€€ç« (bi-mode annealing) æŠ€æœ¯ï¼Œèµ‹äºˆæ¨¡å‹åŒæ—¶å…·å¤‡æ€è€ƒä¸ä¸æ€è€ƒä¸¤ç§èƒ½åŠ›ï¼Œä»è€Œä½¿å…¶èƒ½æ ¹æ®é—®é¢˜å¤æ‚åº¦è‡ªé€‚åº”åœ°å†³å®šæ˜¯å¦æ¿€æ´»æ€ç»´è¿‡ç¨‹ã€‚ç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥é‡‡ç”¨äº†åŒæ¨¡å¼ç­–ç•¥ä¼˜åŒ– (Bi-mode Policy Optimization, BPO) å’Œæ”¹è¿›çš„ GRPO æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡å¼ºåˆ¶æ¨¡å‹ç”Ÿæˆä¸¤ç§æ¨¡å¼çš„å“åº”æ¥æå‡å…¶å†³ç­–å‡†ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒR-4B åœ¨ 25 é¡¹æŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œåœ¨å¤šæ•°ä»»åŠ¡ä¸Šè¶…è¶Šäº† Qwen2.5-VL-7Bã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­ä»¥æ›´ä½çš„è®¡ç®—æˆæœ¬å®ç°äº†ä¸ Kimi-VL-A3B-Thinking-2506 ç­‰å¤§å‹æ¨¡å‹ç›¸å½“çš„è¡¨ç°ï¼Œè¯æ˜äº†å…¶åœ¨å¹³è¡¡æ¨ç†èƒ½åŠ›ä¸è®¡ç®—æ•ˆç‡æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages, 14 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.21113v2",
      "published_date": "2025-08-28 17:48:19 UTC",
      "updated_date": "2025-09-02 13:37:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:48:35.285840+00:00"
    },
    {
      "arxiv_id": "2508.21036v1",
      "title": "Understanding, Protecting, and Augmenting Human Cognition with Generative AI: A Synthesis of the CHI 2025 Tools for Thought Workshop",
      "title_zh": "åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ç†è§£ã€ä¿æŠ¤ä¸å¢å¼ºäººç±»è®¤çŸ¥ï¼šCHI 2025 Tools for Thought ç ”è®¨ä¼šç»¼è¿°",
      "authors": [
        "Lev Tankelevitch",
        "Elena L. Glassman",
        "Jessica He",
        "Aniket Kittur",
        "Mina Lee",
        "Srishti Palani",
        "Advait Sarkar",
        "Gonzalo Ramos",
        "Yvonne Rogers",
        "Hari Subramonyam"
      ],
      "abstract": "Generative AI (GenAI) radically expands the scope and capability of automation for work, education, and everyday tasks, a transformation posing both risks and opportunities for human cognition. How will human cognition change, and what opportunities are there for GenAI to augment it? Which theories, metrics, and other tools are needed to address these questions? The CHI 2025 workshop on Tools for Thought aimed to bridge an emerging science of how the use of GenAI affects human thought, from metacognition to critical thinking, memory, and creativity, with an emerging design practice for building GenAI tools that both protect and augment human thought. Fifty-six researchers, designers, and thinkers from across disciplines as well as industry and academia, along with 34 papers and portfolios, seeded a day of discussion, ideation, and community-building. We synthesize this material here to begin mapping the space of research and design opportunities and to catalyze a multidisciplinary community around this pressing area of research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ€»ç»“äº† CHI 2025 \"Tools for Thought\" å·¥ä½œåŠçš„æˆæœï¼Œæ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI, GenAI) å¯¹äººç±»è®¤çŸ¥åœ¨å·¥ä½œã€æ•™è‚²åŠæ—¥å¸¸ç”Ÿæ´»ä¸­çš„å¤šé‡å½±å“ã€‚æ–‡ç« æ—¨åœ¨å°†å…³äº GenAI å¦‚ä½•é‡å¡‘äººç±»æ€ç»´ï¼ˆåŒ…æ‹¬ metacognition, critical thinking, memory å’Œ creativityï¼‰çš„æ–°å…´ç§‘å­¦ç ”ç©¶ï¼Œä¸æ—¨åœ¨ä¿æŠ¤å¹¶å¢å¼ºè®¤çŸ¥èƒ½åŠ›çš„å·¥å…·è®¾è®¡å®è·µç›¸èåˆã€‚é€šè¿‡å¯¹ 56 ä½è·¨é¢†åŸŸä¸“å®¶åŠ 34 ç¯‡å­¦æœ¯æˆæœçš„ç»¼åˆåˆ†æï¼Œæœ¬æ–‡åˆæ­¥ç»˜åˆ¶äº†è¯¥é¢†åŸŸçš„ç ”ç©¶ä¸è®¾è®¡æœºé‡è“å›¾ã€‚ç ”ç©¶å¼ºè°ƒäº†åœ¨åˆ©ç”¨è‡ªåŠ¨åŒ–æŠ€æœ¯æ‹“å±•èƒ½åŠ›è¾¹ç•Œçš„åŒæ—¶ï¼Œå¿…é¡»å…³æ³¨å…¶å¯¹äººç±»æ€è€ƒæ·±åº¦çš„æ½œåœ¨å¨èƒã€‚è¿™ä¸€ç»¼è¿°ä¸ºè·¨å­¦ç§‘ç¤¾åŒºçš„å»ºç«‹æä¾›äº†æŒ‡å¯¼ï¼Œæ—¨åœ¨å…±åŒå¼€å‘èƒ½å¤Ÿæœ‰æ•ˆèµ‹èƒ½è€Œéæ›¿ä»£äººç±»è®¤çŸ¥çš„æ™ºèƒ½åŒ–å·¥å…·ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21036v1",
      "published_date": "2025-08-28 17:40:42 UTC",
      "updated_date": "2025-08-28 17:40:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:49:03.892229+00:00"
    },
    {
      "arxiv_id": "2508.21112v4",
      "title": "EO-1: Interleaved Vision-Text-Action Pretraining for General Robot Control",
      "title_zh": "EO-1ï¼šé¢å‘é€šç”¨æœºå™¨äººæ§åˆ¶çš„è§†è§‰-æ–‡æœ¬-åŠ¨ä½œäº¤é”™å¼é¢„è®­ç»ƒ",
      "authors": [
        "Delin Qu",
        "Haoming Song",
        "Qizhi Chen",
        "Zhaoqing Chen",
        "Xianqiang Gao",
        "Xinyi Ye",
        "Qi Lv",
        "Modi Shi",
        "Guanghui Ren",
        "Cheng Ruan",
        "Maoqing Yao",
        "Haoran Yang",
        "Jiacheng Bao",
        "Bin Zhao",
        "Dong Wang"
      ],
      "abstract": "The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is a core goal for general-purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale robot and visual-text data, have demonstrated notable progress in general robot control. However, they still fail to achieve human-level flexibility in interleaved reasoning and interaction. In this work, introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is a unified embodied foundation model that achieves superior performance in multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training. The development of EO-1 is based on two key pillars: (i) a unified architecture that processes multimodal inputs indiscriminately (image, text, video, and action), and (ii) a massive, high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains over 1.5 million samples with emphasis on interleaved vision-text-action comprehension. EO-1 is trained through synergies between auto-regressive decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot action generation and multimodal embodied reasoning. Extensive experiments demonstrate the effectiveness of interleaved vision-text-action learning for open-world understanding and generalization, validated through a variety of long-horizon, dexterous manipulation tasks across multiple embodiments. This paper details the architecture of EO-1, the data construction strategy of EO-Data1.5M, and the training methodology, offering valuable insights for developing advanced embodied foundation models.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† EO-Robotics ç³»ç»Ÿï¼ŒåŒ…æ‹¬ EO-1 ç»Ÿä¸€å…·èº«åŸºåº§æ¨¡å‹å’Œ EO-Data1.5M æ•°æ®é›†ï¼Œæ—¨åœ¨å®ç°é€šç”¨æœºå™¨äººæ§åˆ¶ä¸­çš„å¤šæ¨¡æ€æ¨ç†ä¸ç‰©ç†äº¤äº’ã€‚EO-1 é‡‡ç”¨äº†ä¸€ç§ç»Ÿä¸€çš„æ¶æ„ï¼Œèƒ½å¤Ÿæ— å·®åˆ«åœ°å¤„ç†å›¾åƒã€æ–‡æœ¬ã€è§†é¢‘å’ŒåŠ¨ä½œ(action)ç­‰å¤šç§æ¨¡æ€è¾“å…¥ã€‚å…¶æ ¸å¿ƒè´¡çŒ®ä¹‹ä¸€æ˜¯æ„å»ºäº†åŒ…å« 150 ä¸‡ä¸ªæ ·æœ¬çš„å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›† EO-Data1.5Mï¼Œé‡ç‚¹å¼ºè°ƒäº¤ç»‡çš„è§†è§‰-æ–‡æœ¬-åŠ¨ä½œ(interleaved vision-text-action)ç†è§£ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒEO-1 é€šè¿‡ç»“åˆè‡ªå›å½’è§£ç (auto-regressive decoding)ä¸æµåŒ¹é…å»å™ª(flow matching denoising)æŠ€æœ¯ï¼Œå®ç°äº†æ— ç¼çš„æœºå™¨äººåŠ¨ä½œç”Ÿæˆå’Œå¤šæ¨¡æ€å…·èº«æ¨ç†ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œè¯¥äº¤ç»‡å­¦ä¹ ç­–ç•¥ä½¿æ¨¡å‹åœ¨å¤šç§æ„å‹æœºå™¨äººçš„é•¿ç¨‹(long-horizon)å’Œçµå·§æ“ä½œ(dexterous manipulation)ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„å¼€æ”¾ä¸–ç•Œç†è§£ä¸æ³›åŒ–èƒ½åŠ›ã€‚è¯¥è®ºæ–‡è¯¦è¿°äº†æ¶æ„è®¾è®¡ã€æ•°æ®æ„å»ºç­–ç•¥åŠè®­ç»ƒæ–¹æ³•ï¼Œä¸ºå¼€å‘å…ˆè¿›çš„å…·èº«åŸºåº§æ¨¡å‹(embodied foundation models)æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21112v4",
      "published_date": "2025-08-28 17:26:15 UTC",
      "updated_date": "2025-10-15 09:14:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:49:06.584958+00:00"
    },
    {
      "arxiv_id": "2508.21016v1",
      "title": "Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance",
      "title_zh": "åŸºäºå¼ºåŒ–å­¦ä¹ å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹æ¨ç†æ—¶å¯¹é½æ§åˆ¶",
      "authors": [
        "Luozhijie Jin",
        "Zijie Qiu",
        "Jie Liu",
        "Zijie Diao",
        "Lifeng Qiao",
        "Ning Ding",
        "Alex Lamb",
        "Xipeng Qiu"
      ],
      "abstract": "Denoising-based generative models, particularly diffusion and flow matching algorithms, have achieved remarkable success. However, aligning their output distributions with complex downstream objectives, such as human preferences, compositional accuracy, or data compressibility, remains challenging. While reinforcement learning (RL) fine-tuning methods, inspired by advances in RL from human feedback (RLHF) for large language models, have been adapted to these generative frameworks, current RL approaches are suboptimal for diffusion models and offer limited flexibility in controlling alignment strength after fine-tuning. In this work, we reinterpret RL fine-tuning for diffusion models through the lens of stochastic differential equations and implicit reward conditioning. We introduce Reinforcement Learning Guidance (RLG), an inference-time method that adapts Classifier-Free Guidance (CFG) by combining the outputs of the base and RL fine-tuned models via a geometric average. Our theoretical analysis shows that RLG's guidance scale is mathematically equivalent to adjusting the KL-regularization coefficient in standard RL objectives, enabling dynamic control over the alignment-quality trade-off without further training. Extensive experiments demonstrate that RLG consistently improves the performance of RL fine-tuned models across various architectures, RL algorithms, and downstream tasks, including human preferences, compositional control, compressibility, and text rendering. Furthermore, RLG supports both interpolation and extrapolation, thereby offering unprecedented flexibility in controlling generative alignment. Our approach provides a practical and theoretically sound solution for enhancing and controlling diffusion model alignment at inference. The source code for RLG is publicly available at the Github: https://github.com/jinluo12345/Reinforcement-learning-guidance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹(Diffusion Models)åœ¨å¯¹é½äººç±»åå¥½å’Œç»„åˆå‡†ç¡®æ€§ç­‰å¤æ‚ç›®æ ‡æ—¶å­˜åœ¨çš„çµæ´»æ€§ä¸è¶³é—®é¢˜ï¼Œæå‡ºäº†å¼ºåŒ–å­¦ä¹ å¼•å¯¼(Reinforcement Learning Guidance, RLG)æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä»éšæœºå¾®åˆ†æ–¹ç¨‹(SDE)å’Œéšå¼å¥–åŠ±æ¡ä»¶çš„è§†è§’é‡æ–°å®¡è§†äº†å¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼Œé€šè¿‡å‡ ä½•å¹³å‡ç»“åˆåŸºç¡€æ¨¡å‹ä¸RLå¾®è°ƒæ¨¡å‹çš„è¾“å‡ºï¼Œå¯¹åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼(Classifier-Free Guidance, CFG)è¿›è¡Œäº†åˆ›æ–°æ€§é€‚é…ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒRLGçš„å¼•å¯¼å°ºåº¦åœ¨æ•°å­¦ä¸Šç­‰åŒäºè°ƒæ•´æ ‡å‡†RLç›®æ ‡ä¸­çš„KLæ­£åˆ™åŒ–ç³»æ•°ï¼Œä½¿å¾—ç”¨æˆ·åœ¨æ¨ç†é˜¶æ®µæ— éœ€é‡æ–°è®­ç»ƒå³å¯åŠ¨æ€æ§åˆ¶å¯¹é½å¼ºåº¦ä¸ç”Ÿæˆè´¨é‡çš„æƒè¡¡ã€‚å®éªŒè¯æ˜ï¼ŒRLGåœ¨äººç±»åå¥½ã€ç»„åˆæ§åˆ¶ã€å‹ç¼©æ€§åŠæ–‡æœ¬æ¸²æŸ“ç­‰å¤šç§ä»»åŠ¡å’Œæ¶æ„ä¸‹å‡èƒ½æ˜¾è‘—æå‡æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒRLGæ”¯æŒæ’å€¼ä¸å¤–æ¨æ“ä½œï¼Œä¸ºæ‰©æ•£æ¨¡å‹çš„æ¨ç†é˜¶æ®µå¯¹é½æ§åˆ¶æä¾›äº†å…¼å…·ç†è®ºåŸºç¡€ä¸å®ç”¨æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21016v1",
      "published_date": "2025-08-28 17:18:31 UTC",
      "updated_date": "2025-08-28 17:18:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:49:15.591160+00:00"
    },
    {
      "arxiv_id": "2508.21111v1",
      "title": "Automating the Deep Space Network Data Systems; A Case Study in Adaptive Anomaly Detection through Agentic AI",
      "title_zh": "Deep Space Network æ•°æ®ç³»ç»Ÿè‡ªåŠ¨åŒ–ï¼šåŸºäºæ™ºèƒ½ä½“ AI çš„è‡ªé€‚åº”å¼‚å¸¸æ£€æµ‹æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Evan J. Chou",
        "Lisa S. Locke",
        "Harvey M. Soldan"
      ],
      "abstract": "The Deep Space Network (DSN) is NASA's largest network of antenna facilities that generate a large volume of multivariate time-series data. These facilities contain DSN antennas and transmitters that undergo degradation over long periods of time, which may cause costly disruptions to the data flow and threaten the earth-connection of dozens of spacecraft that rely on the Deep Space Network for their lifeline. The purpose of this study was to experiment with different methods that would be able to assist JPL engineers with directly pinpointing anomalies and equipment degradation through collected data, and continue conducting maintenance and operations of the DSN for future space missions around our universe. As such, we have researched various machine learning techniques that can fully reconstruct data through predictive analysis, and determine anomalous data entries within real-time datasets through statistical computations and thresholds. On top of the fully trained and tested machine learning models, we have also integrated the use of a reinforcement learning subsystem that classifies identified anomalies based on severity level and a Large Language Model that labels an explanation for each anomalous data entry, all of which can be improved and fine-tuned over time through human feedback/input. Specifically, for the DSN transmitters, we have also implemented a full data pipeline system that connects the data extraction, parsing, and processing workflow all together as there was no coherent program or script for performing these tasks before. Using this data pipeline system, we were able to then also connect the models trained from DSN antenna data, completing the data workflow for DSN anomaly detection. This was all wrapped around and further connected by an agentic AI system, where complex reasoning was utilized to determine the classifications and predictions of anomalous data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¾å›½å›½å®¶èˆªç©ºèˆªå¤©å±€(NASA)æ·±ç©ºç½‘ç»œ(Deep Space Network, DSN)äº§ç”Ÿçš„å¤§è§„æ¨¡å¤šå˜é‡æ—¶é—´åºåˆ—æ•°æ®ï¼Œæ—¨åœ¨é€šè¿‡è‡ªåŠ¨åŒ–æ‰‹æ®µååŠ©å·¥ç¨‹å¸ˆç²¾å‡†å®šä½è®¾å¤‡é€€åŒ–ä¸å¼‚å¸¸ï¼Œä»¥ä¿éšœèˆªå¤©å™¨é€šä¿¡é“¾è·¯çš„ç¨³å®šã€‚ç ”ç©¶é‡‡ç”¨äº†å¤šç§æœºå™¨å­¦ä¹ (Machine Learning)æŠ€æœ¯è¿›è¡Œæ•°æ®é‡å»ºä¸é¢„æµ‹åˆ†æï¼Œå¹¶é€šè¿‡ç»Ÿè®¡è®¡ç®—å’Œé˜ˆå€¼è®¾å®šå®ç°å®æ—¶å¼‚å¸¸æ£€æµ‹ã€‚æ¡†æ¶è¿›ä¸€æ­¥é›†æˆäº†å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å­ç³»ç»Ÿå¯¹å¼‚å¸¸ä¸¥é‡ç¨‹åº¦è¿›è¡Œåˆ†ç±»ï¼Œå¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Model)ä¸ºå¼‚å¸¸æ¡ç›®ç”Ÿæˆè§£é‡Šï¼Œä¸”æ”¯æŒé€šè¿‡äººç±»åé¦ˆè¿›è¡ŒæŒç»­ä¼˜åŒ–ã€‚é’ˆå¯¹DSNå‘å°„æœºï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†å®Œæ•´çš„è‡ªåŠ¨åŒ–æ•°æ®æµæ°´çº¿ï¼Œå®ç°äº†æ•°æ®æå–ã€è§£æåŠå¤„ç†çš„ä¸€ä½“åŒ–å·¥ä½œæµã€‚è¯¥ç³»ç»Ÿæœ€ç»ˆç”±ä»£ç†å¼äººå·¥æ™ºèƒ½(Agentic AI)ç³»ç»Ÿæ•´åˆï¼Œåˆ©ç”¨å¤æ‚æ¨ç†èƒ½åŠ›å®Œæˆå¼‚å¸¸åˆ†ç±»ä¸é¢„æµ‹ï¼Œå®ç°äº†æ¶µç›–DSNå¤©çº¿ä¸å‘å°„æœºçš„å…¨æµç¨‹æ£€æµ‹ã€‚è¯¥æˆæœä¸ä»…å¡«è¡¥äº†æ­¤å‰ç¼ºä¹è¿è´¯æ•°æ®å¤„ç†ç¨‹åºçš„ç©ºç™½ï¼Œä¹Ÿä¸ºæœªæ¥æ·±ç©ºæ¢æµ‹ä»»åŠ¡çš„ç»´æŠ¤ä¸è¿è¡Œæä¾›äº†è‡ªé€‚åº”çš„å¼‚å¸¸æ£€æµ‹æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21111v1",
      "published_date": "2025-08-28 17:12:18 UTC",
      "updated_date": "2025-08-28 17:12:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:49:30.994724+00:00"
    },
    {
      "arxiv_id": "2508.21010v2",
      "title": "ChainReaction: Causal Chain-Guided Reasoning for Modular and Explainable Causal-Why Video Question Answering",
      "title_zh": "ChainReactionï¼šé¢å‘æ¨¡å—åŒ–ä¸å¯è§£é‡Šå› æœç±»è§†é¢‘é—®ç­”çš„å› æœé“¾å¼•å¯¼æ¨ç†",
      "authors": [
        "Paritosh Parmar",
        "Eric Peh",
        "Basura Fernando"
      ],
      "abstract": "Existing Causal-Why Video Question Answering (VideoQA) models often struggle with higher-order reasoning, relying on opaque, monolithic pipelines that entangle video understanding, causal inference, and answer generation. These black-box approaches offer limited interpretability and tend to depend on shallow heuristics. We propose a novel, modular paradigm that explicitly decouples causal reasoning from answer generation, introducing natural language causal chains as interpretable intermediate representations. Inspired by human cognitive models, these structured cause-effect sequences bridge low-level video content with high-level causal reasoning, enabling transparent and logically coherent inference. Our two-stage architecture comprises a Causal Chain Extractor (CCE) that generates causal chains from video-question pairs, and a Causal Chain-Driven Answerer (CCDA) that derives answers grounded in these chains. To address the lack of annotated reasoning traces, we introduce a scalable method for generating accurate causal chains from existing datasets. We construct human verified causal chains for 46K samples. We also propose CauCo, a new evaluation metric for causality-oriented captioning. Experiments on three large-scale benchmarks demonstrate that our approach not only outperforms state-of-the-art models, but also yields substantial gains in explainability, user trust, and generalization -- positioning the CCE as a reusable causal reasoning engine across diverse domains. Project page: https://paritoshparmar.github.io/chainreaction/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ChainReactionï¼Œä¸€ç§æ—¨åœ¨è§£å†³å› æœè§†é¢‘é—®ç­” (Causal-Why Video Question Answering) ä¸­å•ä½“é»‘ç›’æ¨¡å‹ç¼ºä¹è§£é‡Šæ€§å’Œé«˜é˜¶æ¨ç†èƒ½åŠ›é—®é¢˜çš„æ¨¡å—åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥è‡ªç„¶è¯­è¨€å› æœé“¾ (causal chains) ä½œä¸ºå¯è§£é‡Šçš„ä¸­é—´è¡¨å¾ï¼Œå®ç°äº†å› æœæ¨ç†ä¸ç­”æ¡ˆç”Ÿæˆçš„æ˜¾å¼è§£è€¦ã€‚å…¶æ ¸å¿ƒæ¶æ„åŒ…å«å› æœé“¾æå–å™¨ (Causal Chain Extractor, CCE) å’Œå› æœé“¾é©±åŠ¨å›ç­”å™¨ (Causal Chain-Driven Answerer, CCDA)ï¼Œé€šè¿‡ç»“æ„åŒ–çš„å› æœåºåˆ—æ¡¥æ¥åº•å±‚è§†é¢‘å†…å®¹ä¸é«˜å±‚é€»è¾‘æ¨ç†ã€‚ä¸ºäº†å…‹æœæ¨ç†æ ‡æ³¨ç¼ºå¤±çš„æŒ‘æˆ˜ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„æ ‡æ³¨ç”Ÿæˆæ–¹æ³•ï¼Œå¹¶æ„å»ºäº†åŒ…å« 46K ä¸ªæ ·æœ¬çš„äººå·¥æ ¡éªŒæ•°æ®é›†ï¼ŒåŒæ—¶æå‡ºäº†é’ˆå¯¹å› æœæè¿°çš„æ–°å‹è¯„ä»·æŒ‡æ ‡ CauCoã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ä¸Šå‡è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿› (state-of-the-art) æ¨¡å‹ã€‚ChainReaction ä¸ä»…æå‡äº†é¢„æµ‹å‡†ç¡®ç‡ï¼Œè¿˜åœ¨å¯è§£é‡Šæ€§ã€ç”¨æˆ·ä¿¡ä»»åº¦å’Œè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä½¿ CCE èƒ½å¤Ÿä½œä¸ºé€šç”¨çš„å› æœæ¨ç†å¼•æ“ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://paritoshparmar.github.io/chainreaction/",
      "pdf_url": "https://arxiv.org/pdf/2508.21010v2",
      "published_date": "2025-08-28 17:10:53 UTC",
      "updated_date": "2025-12-24 14:52:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:49:18.792522+00:00"
    },
    {
      "arxiv_id": "2508.21001v2",
      "title": "Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees",
      "title_zh": "åŸºäºæ‰©æ•£æ ‘çš„â€œä¸€æ¬¡è®­ç»ƒï¼Œéšå¤„è§„åˆ’â€åŠ¨åŠ›å­¦è¿åŠ¨è§„åˆ’",
      "authors": [
        "Yaniv Hassidof",
        "Tom Jurgenson",
        "Kiril Solovey"
      ],
      "abstract": "Kinodynamic motion planning is concerned with computing collision-free trajectories while abiding by the robot's dynamic constraints. This critical problem is often tackled using sampling-based planners (SBPs) that explore the robot's high-dimensional state space by constructing a search tree via action propagations. Although SBPs can offer global guarantees on completeness and solution quality, their performance is often hindered by slow exploration due to uninformed action sampling. Learning-based approaches can yield significantly faster runtimes, yet they fail to generalize to out-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety, thus limiting their deployment on physical robots. We present Diffusion Tree (DiTree): a provably-generalizable framework leveraging diffusion policies (DPs) as informed samplers to efficiently guide state-space search within SBPs. DiTree combines DP's ability to model complex distributions of expert trajectories, conditioned on local observations, with the completeness of SBPs to yield provably-safe solutions within a few action propagation iterations for complex dynamical systems. We demonstrate DiTree's power with an implementation combining the popular RRT planner with a DP action sampler trained on a single environment. In comprehensive evaluations on OOD scenarios, DiTree achieves on average a 30% higher success rate compared to standalone DP or SBPs, on a dynamic car and Mujoco's ant robot settings (for the latter, SBPs fail completely). Beyond simulation, real-world car experiments confirm DiTree's applicability, demonstrating superior trajectory quality and robustness even under severe sim-to-real gaps. Project webpage: https://sites.google.com/view/ditree.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Diffusion Tree (DiTree) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ Kinodynamic motion planning ä¸­é‡‡æ ·è¿åŠ¨è§„åˆ’å™¨ (SBPs) æ¢ç´¢ç¼“æ…¢ä»¥åŠå­¦ä¹ æ–¹æ³•åœ¨åˆ†å¸ƒå¤– (OOD) åœºæ™¯ä¸‹æ³›åŒ–æ€§å·®ä¸”ç¼ºä¹å®‰å…¨ä¿è¯çš„é—®é¢˜ã€‚DiTree å°†æ‰©æ•£ç­–ç•¥ (Diffusion Policies, DPs) ä½œä¸ºçŸ¥æƒ…é‡‡æ ·å™¨ï¼Œåˆ©ç”¨å…¶å»ºæ¨¡ä¸“å®¶è½¨è¿¹å¤æ‚åˆ†å¸ƒçš„èƒ½åŠ›æ¥æŒ‡å¯¼ SBPs åœ¨çŠ¶æ€ç©ºé—´ä¸­çš„é«˜æ•ˆæœç´¢ã€‚è¿™ç§æ–¹æ³•ç»“åˆäº† DP çš„ç”Ÿæˆèƒ½åŠ›ä¸ SBPs çš„å®Œå¤‡æ€§ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚åŠ¨åŠ›ç³»ç»Ÿä¸­é€šè¿‡æå°‘çš„åŠ¨ä½œä¼ æ’­è¿­ä»£ç”Ÿæˆå¯è¯æ˜å®‰å…¨çš„è§£ã€‚åœ¨åŠ¨æ€å°è½¦å’Œ Mujoco èš‚èšæœºå™¨äººçš„ OOD åœºæ™¯è¯„ä¼°ä¸­ï¼ŒDiTree çš„æˆåŠŸç‡æ¯”ç‹¬ç«‹ä½¿ç”¨ DP æˆ– SBPs å¹³å‡é«˜å‡º 30%ã€‚çœŸå®ä¸–ç•Œçš„å°è½¦å®éªŒè¿›ä¸€æ­¥è¯å®äº†è¯¥æ¡†æ¶çš„é€‚ç”¨æ€§ï¼Œè¯æ˜å…¶åœ¨é¢å¯¹ä¸¥é‡çš„ Sim-to-Real å·®è·æ—¶ä»èƒ½ä¿æŒä¼˜è¶Šçš„è½¨è¿¹è´¨é‡å’Œé²æ£’æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to CoRL 2025, Project page: https://sites.google.com/view/ditree. v2: Abstract updated",
      "pdf_url": "https://arxiv.org/pdf/2508.21001v2",
      "published_date": "2025-08-28 17:04:00 UTC",
      "updated_date": "2025-09-05 15:50:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:49:27.495138+00:00"
    },
    {
      "arxiv_id": "2508.20996v2",
      "title": "ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery",
      "title_zh": "ChatTheroï¼šç”¨äºæˆç˜¾åº·å¤ä¸­è¡Œä¸ºæ”¹å˜ä¸æ²»ç–—æ”¯æŒçš„å¤§è¯­è¨€æ¨¡å‹èŠå¤©æœºå™¨äºº",
      "authors": [
        "Junda Wang",
        "Zonghai Yao",
        "Lingxi Li",
        "Junhui Qian",
        "Zhichao Yang",
        "Hong Yu"
      ],
      "abstract": "Substance use disorders (SUDs) affect millions of people, and relapses are common, requiring multi-session treatments. Access to care is limited, which contributes to the challenge of recovery support. We present \\textbf{ChatThero}, an innovative low-cost, multi-session, stressor-aware, and memory-persistent autonomous \\emph{language agent} designed to facilitate long-term behavior change and therapeutic support in addiction recovery. Unlike existing work that mostly finetuned large language models (LLMs) on patient-therapist conversation data, ChatThero was trained in a multi-agent simulated environment that mirrors real therapy. We created anonymized patient profiles from recovery communities (e.g., Reddit). We classify patients as \\texttt{easy}, \\texttt{medium}, and \\texttt{difficult}, three scales representing their resistance to recovery. We created an external environment by introducing stressors (e.g., social determinants of health) to simulate real-world situations. We dynamically inject clinically-grounded therapeutic strategies (motivational interview and cognitive behavioral therapy). Our evaluation, conducted by both human (blinded clinicians) and LLM-as-Judge, shows that ChatThero is superior in empathy and clinical relevance. We show that stressor simulation improves robustness of ChatThero. Explicit stressors increase relapse-like setbacks, matching real-world patterns. We evaluate ChatThero with behavioral change metrics. On a 1--5 scale, ChatThero raises \\texttt{motivation} by $+1.71$ points (from $2.39$ to $4.10$) and \\texttt{confidence} by $+1.67$ points (from $1.52$ to $3.19$), substantially outperforming GPT-5. On \\texttt{difficult} patients, ChatThero reaches the success milestone with $26\\%$ fewer turns than GPT-5.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç‰©è´¨ä½¿ç”¨éšœç¢ (Substance use disorders, SUDs) åº·å¤è¿‡ç¨‹ä¸­æ²»ç–—èµ„æºæœ‰é™ä¸”å¤å‘ç‡é«˜çš„é—®é¢˜ï¼Œæå‡ºäº† ChatTheroï¼Œè¿™æ˜¯ä¸€ç§ä½æˆæœ¬ã€å¤šä¼šè¯ã€å…·å¤‡å‹åŠ›æ„ŸçŸ¥ä¸”æ‹¥æœ‰æŒä¹…è®°å¿†çš„è‡ªä¸»è¯­è¨€æ™ºèƒ½ä½“ (language agent)ã€‚ä¸ä»¥å¾€ä»…åœ¨å¯¹è¯æ•°æ®ä¸Šç®€å•å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„æ–¹æ³•ä¸åŒï¼ŒChatThero åœ¨æ¨¡æ‹ŸçœŸå®æ²»ç–—çš„å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡æ‚£è€…æ¡£æ¡ˆæ¨¡æ‹Ÿä¸åŒéš¾åº¦çš„åº·å¤é˜»åŠ›ï¼Œå¹¶å¼•å…¥å‹åŠ›æº (stressors) ä»¥è¿˜åŸç°å®ä¸–ç•Œçš„å¤æ‚æƒ…å¢ƒã€‚è¯¥æ¡†æ¶åŠ¨æ€åœ°èå…¥äº†åŠ¨æœºè®¿è°ˆ (Motivational Interviewing) å’Œè®¤çŸ¥è¡Œä¸ºç–—æ³• (Cognitive Behavioral Therapy) ç­‰ä¸´åºŠæ²»ç–—ç­–ç•¥ï¼Œæ—¨åœ¨ä¿ƒè¿›é•¿æœŸçš„è¡Œä¸ºæ”¹å˜ã€‚ç»ä¸“ä¸šä¸´åºŠåŒ»ç”ŸåŠ LLM-as-Judge è¯„ä¼°ï¼ŒChatThero åœ¨å…±æƒ…èƒ½åŠ›å’Œä¸´åºŠç›¸å…³æ€§ä¸Šè¡¨ç°å“è¶Šï¼Œå‹åŠ›æºæ¨¡æ‹Ÿæ˜¾è‘—æå‡äº†å…¶ç¨³å¥æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒChatThero å°†æ‚£è€…çš„åŠ¨æœº (motivation) å’Œä¿¡å¿ƒ (confidence) åˆ†åˆ«æå‡äº† 1.71 å’Œ 1.67 åˆ†ï¼Œè¡¨ç°æ˜¾è‘—ä¼˜äº GPT-5ã€‚ç‰¹åˆ«æ˜¯åœ¨é’ˆå¯¹é«˜éš¾åº¦æ‚£è€…çš„æ²»ç–—ä¸­ï¼ŒChatThero è¾¾æˆæˆåŠŸé‡Œç¨‹ç¢‘æ‰€éœ€çš„å¯¹è¯è½®æ•°æ¯” GPT-5 å‡å°‘äº† 26%ï¼Œä¸ºæˆç˜¾åº·å¤æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ•°å­—åŒ–æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20996v2",
      "published_date": "2025-08-28 16:57:33 UTC",
      "updated_date": "2025-10-13 19:15:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:49:30.385335+00:00"
    },
    {
      "arxiv_id": "2508.20991v1",
      "title": "ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts",
      "title_zh": "ExpertSimï¼šåŸºäºæ··åˆç”Ÿæˆä¸“å®¶çš„å¿«é€Ÿç²’å­æ¢æµ‹å™¨æ¨¡æ‹Ÿ",
      "authors": [
        "Patryk BÄ™dkowski",
        "Jan DubiÅ„ski",
        "Filip Szatkowski",
        "Kamil Deja",
        "PrzemysÅ‚aw Rokita",
        "Tomasz TrzciÅ„ski"
      ],
      "abstract": "Simulating detector responses is a crucial part of understanding the inner workings of particle collisions in the Large Hadron Collider at CERN. Such simulations are currently performed with statistical Monte Carlo methods, which are computationally expensive and put a significant strain on CERN's computational grid. Therefore, recent proposals advocate for generative machine learning methods to enable more efficient simulations. However, the distribution of the data varies significantly across the simulations, which is hard to capture with out-of-the-box methods. In this study, we present ExpertSim - a deep learning simulation approach tailored for the Zero Degree Calorimeter in the ALICE experiment. Our method utilizes a Mixture-of-Generative-Experts architecture, where each expert specializes in simulating a different subset of the data. This allows for a more precise and efficient generation process, as each expert focuses on a specific aspect of the calorimeter response. ExpertSim not only improves accuracy, but also provides a significant speedup compared to the traditional Monte-Carlo methods, offering a promising solution for high-efficiency detector simulations in particle physics experiments at CERN. We make the code available at https://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ExpertSimï¼Œä¸€ç§ä¸“ä¸º CERN ALICE å®éªŒä¸­ Zero Degree Calorimeter è®¾è®¡çš„æ·±åº¦å­¦ä¹ æ¨¡æ‹Ÿæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿ Monte Carlo æ–¹æ³•åœ¨ç²’å­æ¢æµ‹å™¨å“åº”æ¨¡æ‹Ÿä¸­è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚ExpertSim é‡‡ç”¨äº† Mixture-of-Generative-Experts æ¶æ„ï¼Œé€šè¿‡è®©ä¸åŒä¸“å®¶æ¨¡å‹ä¸“æ³¨äºæ¨¡æ‹Ÿç‰¹å®šçš„æ•°æ®å­é›†ï¼Œå®ç°äº†æ›´ç²¾ç¡®ã€æ›´é«˜æ•ˆçš„ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶ï¼Œç›¸æ¯”ä¼ ç»Ÿ Monte Carlo æ–¹æ³•å®ç°äº†æ˜¾è‘—çš„æ¨ç†åŠ é€Ÿã€‚è¿™ä¸€æˆæœä¸ºç²’å­ç‰©ç†å®éªŒæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯é çš„æ¢æµ‹å™¨æ¨¡æ‹Ÿæ–¹æ¡ˆï¼Œç›¸å…³ä»£ç ä¹Ÿå·²å‘ç¤¾åŒºå¼€æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ECAI 2025 28th European Conference on Artificial Intelligence",
      "pdf_url": "https://arxiv.org/pdf/2508.20991v1",
      "published_date": "2025-08-28 16:53:03 UTC",
      "updated_date": "2025-08-28 16:53:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:49:46.094637+00:00"
    },
    {
      "arxiv_id": "2509.04463v1",
      "title": "Multiscale Graph Neural Network for Turbulent Flow-Thermal Prediction Around a Complex-Shaped Pin-Fin",
      "title_zh": "é¢å‘å¤æ‚å½¢çŠ¶é’ˆç¿…ç»•æµåŠæ¹æµä¼ çƒ­é¢„æµ‹çš„å¤šå°ºåº¦å›¾ç¥ç»ç½‘ç»œ",
      "authors": [
        "Riddhiman Raut",
        "Evan M. Mihalko",
        "Amrita Basak"
      ],
      "abstract": "This study presents the development of a domain-responsive edge-aware multiscale Graph Neural Network for predicting steady, turbulent flow and thermal behavior in a two-dimensional channel containing arbitrarily shaped complex pin-fin geometries. The training dataset was constructed through an automated framework that integrated geometry generation, meshing, and flow-field solutions in ANSYS Fluent. The pin-fin geometry was parameterized using piecewise cubic splines, producing 1,000 diverse configurations through Latin Hypercube Sampling. Each simulation was converted into a graph structure, where nodes carried a feature vector containing spatial coordinates, a normalized streamwise position, one-hot boundary indicators, and a signed distance to the nearest boundary such as wall. This graph structure served as input to the newly developed Graph Neural Network, which was trained to predict temperature, velocity magnitude, and pressure at each node using data from ANSYS. The network predicted fields with outstanding accuracy, capturing boundary layers, recirculation, and the stagnation region upstream of the pin-fins while reducing wall time by 2-3 orders of magnitude. In conclusion, the novel graph neural network offered a fast and reliable surrogate for simulations in complex flow configurations.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§é¢†åŸŸå“åº”å¼è¾¹ç¼˜æ„ŸçŸ¥å¤šå°ºåº¦å›¾ç¥ç»ç½‘ç»œ(Multiscale Graph Neural Network)ï¼Œä¸“é—¨ç”¨äºé¢„æµ‹å¤æ‚å½¢çŠ¶é’ˆçŠ¶æ•£çƒ­ç‰‡(Pin-Fin)å‘¨å›´çš„ç¨³æ€æ¹æµæµåŠ¨ä¸çƒ­è¡Œä¸ºã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡åˆ†æ®µä¸‰æ¬¡æ ·æ¡(Piecewise cubic splines)å’Œæ‹‰ä¸è¶…ç«‹æ–¹é‡‡æ ·(Latin Hypercube Sampling)ç”Ÿæˆäº†1,000ä¸ªå‡ ä½•é…ç½®ï¼Œå¹¶æ„å»ºäº†åŒ…å«ç©ºé—´åæ ‡ã€è¾¹ç•ŒæŒ‡ç¤ºå™¨åŠåˆ°è¾¹ç•Œç¬¦å·è·ç¦»(Signed distance)ç­‰èŠ‚ç‚¹ç‰¹å¾çš„å›¾æ•°æ®é›†ã€‚è¯¥ç½‘ç»œè¢«è®­ç»ƒç”¨äºé¢„æµ‹å„èŠ‚ç‚¹çš„æ¸©åº¦(Temperature)ã€é€Ÿåº¦å¤§å°(Velocity magnitude)å’Œå‹åŠ›(Pressure)ï¼Œèƒ½å¤Ÿä»¥æé«˜çš„ç²¾åº¦æ•æ‰è¾¹ç•Œå±‚(Boundary layers)ã€å†å¾ªç¯åŒºåŠåœæ»åŒºç­‰å¤æ‚æµä½“ç‰¹å¾ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨é¢„æµ‹ç²¾åº¦ä¸Šä¸ä¼ ç»Ÿæ•°å€¼æ¨¡æ‹Ÿé«˜åº¦ä¸€è‡´ï¼ŒåŒæ—¶å°†è®¡ç®—è€—æ—¶ç¼©çŸ­äº†2-3ä¸ªæ•°é‡çº§ã€‚ä½œä¸ºä¸€ç§å¿«é€Ÿä¸”å¯é çš„ä»£ç†æ¨¡å‹(Surrogate model)ï¼Œè¯¥ç ”ç©¶ä¸ºå¤æ‚æµåŠ¨é…ç½®ä¸‹çš„çƒ­æµé¢„æµ‹æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "physics.flu-dyn",
        "cs.AI",
        "cs.LG",
        "math.NA"
      ],
      "primary_category": "physics.flu-dyn",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04463v1",
      "published_date": "2025-08-28 16:46:03 UTC",
      "updated_date": "2025-08-28 16:46:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:49:50.951251+00:00"
    },
    {
      "arxiv_id": "2508.20978v4",
      "title": "Scaling Neuro-symbolic Problem Solving: Solver-Free Learning of Constraints and Objectives",
      "title_zh": "æ‰©å±•ç¥ç»ç¬¦å·é—®é¢˜æ±‚è§£ï¼šå…æ±‚è§£å™¨çš„çº¦æŸä¸ç›®æ ‡å­¦ä¹ ",
      "authors": [
        "Marianne Defresne",
        "Romain Gambardella",
        "Sophie Barbe",
        "Thomas Schiex"
      ],
      "abstract": "In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs, a task that Large Language Models seem to struggle with.\n  Objectives: We introduce a differentiable neuro-symbolic architecture and a loss function dedicated to learning how to solve NP-hard reasoning problems.\n  Methods: Our new probabilistic loss allows for learning both the constraints and the objective, thus delivering a complete model that can be scrutinized and completed with side constraints. By pushing the combinatorial solver out of the training loop, our architecture also offers scalable training while exact inference gives access to maximum accuracy.\n  Results: We empirically show that it can efficiently learn how to solve NP-hard reasoning problems from natural inputs. On three variants of the Sudoku benchmark -- symbolic, visual, and many-solution --, our approach requires a fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut task, it optimizes the regret better than a Decision-Focused-Learning regret-dedicated loss. Finally, it efficiently learns the energy optimization formulation of the large real-world problem of designing proteins.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å¯å¾®åˆ†çš„ç¥ç»ç¬¦å·(neuro-symbolic)æ¶æ„å’Œæ¦‚ç‡æŸå¤±å‡½æ•°(probabilistic loss)ï¼Œè‡´åŠ›äºä»è‡ªç„¶è¾“å…¥ä¸­å­¦ä¹ è§£å†³ç¥ç»ç½‘ç»œå’Œå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)éš¾ä»¥å¤„ç†çš„ç¦»æ•£æ¨ç†ä¸ä¼˜åŒ–ç­‰NP-hardé—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åŒæ—¶å­¦ä¹ çº¦æŸ(constraints)å’Œç›®æ ‡(objective)ï¼Œæä¾›äº†ä¸€ä¸ªå¯ä»¥è¿›è¡Œå®¡æŸ¥å¹¶æ·»åŠ ä¾§é¢çº¦æŸçš„å®Œæ•´æ¨¡å‹ã€‚é€šè¿‡å°†ä¼ ç»Ÿçš„ç»„åˆæ±‚è§£å™¨(combinatorial solver)ç§»å‡ºè®­ç»ƒå¾ªç¯ï¼Œè¯¥æ¶æ„ä¸ä»…å®ç°äº†å¯æ‰©å±•çš„è®­ç»ƒ(scalable training)ï¼Œè¿˜é€šè¿‡ç²¾ç¡®æ¨ç†ç¡®ä¿äº†æé«˜çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶åœ¨ç¬¦å·ã€è§†è§‰åŠå¤šè§£ç­‰ä¸‰ç§å˜ä½“çš„SudokuåŸºå‡†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œå…¶è®­ç»ƒæ—¶é—´ä»…ä¸ºå…¶ä»–æ··åˆæ–¹æ³•çš„ä¸€å°éƒ¨åˆ†ã€‚åœ¨è§†è§‰Min-Cut/Max-cutä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”ä¸“é—¨çš„å†³ç­–èšç„¦å­¦ä¹ (Decision-Focused-Learning)æŸå¤±å‡½æ•°èƒ½æ›´å¥½åœ°ä¼˜åŒ–é—æ†¾å€¼(regret)ã€‚æœ€åï¼Œè¯¥æ–¹æ³•æˆåŠŸåº”ç”¨äºçœŸå®ä¸–ç•Œä¸­å¤§è§„æ¨¡çš„è›‹ç™½è´¨è®¾è®¡(designing proteins)èƒ½é‡ä¼˜åŒ–é—®é¢˜ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚å®é™…ä»»åŠ¡ä¸­çš„é«˜æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO",
        "cs.SC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20978v4",
      "published_date": "2025-08-28 16:33:27 UTC",
      "updated_date": "2025-12-18 12:31:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:49:46.386166+00:00"
    },
    {
      "arxiv_id": "2508.20976v1",
      "title": "WoW-Bench: Evaluating Fine-Grained Acoustic Perception in Audio-Language Models via Marine Mammal Vocalizations",
      "title_zh": "WoW-Benchï¼šåŸºäºæµ·æ´‹å“ºä¹³åŠ¨ç‰©é¸£å£°çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ç»†ç²’åº¦å£°å­¦æ„ŸçŸ¥è¯„ä¼°",
      "authors": [
        "Jaeyeon Kim",
        "Heeseung Yun",
        "Sang Hoon Woo",
        "Chao-Han Huck Yang",
        "Gunhee Kim"
      ],
      "abstract": "Large audio language models (LALMs) extend language understanding into the auditory domain, yet their ability to perform low-level listening, such as pitch and duration detection, remains underexplored. However, low-level listening is critical for real-world, out-of-distribution tasks where models must reason about unfamiliar sounds based on fine-grained acoustic cues. To address this gap, we introduce the World-of-Whale benchmark (WoW-Bench) to evaluate low-level auditory perception and cognition using marine mammal vocalizations. WoW-bench is composed of a Perception benchmark for categorizing novel sounds and a Cognition benchmark, inspired by Bloom's taxonomy, to assess the abilities to remember, understand, apply, and analyze sound events. For the Cognition benchmark, we additionally introduce distractor questions to evaluate whether models are truly solving problems through listening rather than relying on other heuristics. Experiments with state-of-the-art LALMs show performance far below human levels, indicating a need for stronger auditory grounding in LALMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† WoW-Bench åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ (LALMs) åœ¨å¤„ç†æµ·æ´‹å“ºä¹³åŠ¨ç‰©é¸£å«å£°æ—¶çš„ç²¾ç»†å¬è§‰æ„ŸçŸ¥ä¸è®¤çŸ¥èƒ½åŠ›ã€‚å°½ç®¡ LALMs æ‰©å±•äº†å¯¹å¬è§‰é¢†åŸŸçš„ç†è§£ï¼Œä½†åœ¨éŸ³é«˜å’Œæ—¶é•¿æ£€æµ‹ç­‰ä½å±‚çº§å¬è§‰ä»»åŠ¡ (low-level listening) æ–¹é¢è¡¨ç°å°šä¸æ˜ç¡®ã€‚WoW-Bench ç”±æ„ŸçŸ¥åŸºå‡†å’ŒåŸºäºå¸ƒé²å§†æ•™è‚²ç›®æ ‡åˆ†ç±»æ³• (Bloom's taxonomy) çš„è®¤çŸ¥åŸºå‡†ç»„æˆï¼Œåˆ†åˆ«ç”¨äºæµ‹è¯•æ¨¡å‹å¯¹æ–°å¥‡å£°éŸ³çš„åˆ†ç±»ä»¥åŠè®°å¿†ã€ç†è§£ã€åº”ç”¨å’Œåˆ†æèƒ½åŠ›ã€‚ä¸ºäº†æ’é™¤æ¨¡å‹ä¾èµ–å¯å‘å¼ç®—æ³•çš„å¯èƒ½æ€§ï¼Œè®¤çŸ¥æµ‹è¯•ä¸­è¿˜ä¸“é—¨å¼•å…¥äº†å¹²æ‰°é¡¹é—®é¢˜ä»¥éªŒè¯æ¨¡å‹æ˜¯å¦çœŸæ­£é€šè¿‡å¬è§‰è§£å†³é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„ LALMs åœ¨è¯¥åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°è¿œä½äºäººç±»æ°´å¹³ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨å¬è§‰åŸºç¡€ (auditory grounding) æ–¹é¢ä»éœ€åŠ å¼ºï¼Œä»¥å®ç°å¯¹åˆ†å¸ƒå¤–é™Œç”Ÿå£°éŸ³çš„ç²¾ç¡®æ¨ç†ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Preprint. Project page: https://jaeyeonkim99.github.io/wow_bench/",
      "pdf_url": "https://arxiv.org/pdf/2508.20976v1",
      "published_date": "2025-08-28 16:29:46 UTC",
      "updated_date": "2025-08-28 16:29:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:50:13.286250+00:00"
    },
    {
      "arxiv_id": "2508.20973v1",
      "title": "ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents",
      "title_zh": "ProactiveEvalï¼šé¢å‘ä¸»åŠ¨å¯¹è¯æ™ºèƒ½ä½“çš„ç»Ÿä¸€è¯„ä¼°æ¡†æ¶",
      "authors": [
        "Tianjian Liu",
        "Fanqi Wan",
        "Jiajian Guo",
        "Xiaojun Quan"
      ],
      "abstract": "Proactive dialogue has emerged as a critical and challenging research problem in advancing large language models (LLMs). Existing works predominantly focus on domain-specific or task-oriented scenarios, which leads to fragmented evaluations and limits the comprehensive exploration of models' proactive conversation abilities. In this work, we propose ProactiveEval, a unified framework designed for evaluating proactive dialogue capabilities of LLMs. This framework decomposes proactive dialogue into target planning and dialogue guidance, establishing evaluation metrics across various domains. Moreover, it also enables the automatic generation of diverse and challenging evaluation data. Based on the proposed framework, we develop 328 evaluation environments spanning 6 distinct domains. Through experiments with 22 different types of LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional performance on target planning and dialogue guidance tasks, respectively. Finally, we investigate how reasoning capabilities influence proactive behaviors and discuss their implications for future model development.",
      "tldr_zh": "é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸»åŠ¨å¯¹è¯ï¼ˆproactive dialogueï¼‰è¯„ä¼°ä¸­å­˜åœ¨çš„åœºæ™¯å•ä¸€å’Œè¯„ä¼°ç¢ç‰‡åŒ–é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ProactiveEvalï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å…¨é¢è¡¡é‡æ¨¡å‹ä¸»åŠ¨å¯¹è¯èƒ½åŠ›çš„ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å°†ä¸»åŠ¨å¯¹è¯ä»»åŠ¡åˆ†è§£ä¸ºç›®æ ‡è§„åˆ’ï¼ˆtarget planningï¼‰å’Œå¯¹è¯å¼•å¯¼ï¼ˆdialogue guidanceï¼‰ä¸¤ä¸ªæ ¸å¿ƒç»´åº¦ï¼Œå¹¶åœ¨å¤šä¸ªé¢†åŸŸå†…ç¡®ç«‹äº†æ ‡å‡†åŒ–çš„è¯„ä¼°æŒ‡æ ‡ã€‚ç ”ç©¶åˆ©ç”¨è¯¥æ¡†æ¶å®ç°äº†å¤šæ ·åŒ–è¯„ä¼°æ•°æ®çš„è‡ªåŠ¨ç”Ÿæˆï¼Œæ„å»ºäº†æ¶µç›–6ä¸ªä¸åŒé¢†åŸŸçš„328ä¸ªè¯„ä¼°ç¯å¢ƒã€‚é€šè¿‡å¯¹22ç§ä¸»æµ LLMs çš„å®éªŒè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤º DeepSeek-R1 åœ¨ç›®æ ‡è§„åˆ’æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè€Œ Claude-3.7-Sonnet åˆ™åœ¨å¯¹è¯å¼•å¯¼ä»»åŠ¡ä¸­å¤„äºé¢†å…ˆåœ°ä½ã€‚è¯¥å·¥ä½œè¿˜æ·±å…¥æ¢è®¨äº†æ¨ç†èƒ½åŠ›å¯¹æ¨¡å‹ä¸»åŠ¨è¡Œä¸ºçš„å½±å“ï¼Œä¸ºæœªæ¥æ›´å…·äº¤äº’æ€§çš„æ¨¡å‹å¼€å‘æä¾›äº†é‡è¦è§è§£å’ŒåŸºå‡†æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 6 Figures",
      "pdf_url": "https://arxiv.org/pdf/2508.20973v1",
      "published_date": "2025-08-28 16:26:44 UTC",
      "updated_date": "2025-08-28 16:26:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:50:14.459303+00:00"
    },
    {
      "arxiv_id": "2509.12211v1",
      "title": "TinyServe: Query-Aware Cache Selection for Efficient LLM Serving",
      "title_zh": "TinyServeï¼šé¢å‘é«˜æ•ˆå¤§è¯­è¨€æ¨¡å‹æœåŠ¡çš„æŸ¥è¯¢æ„ŸçŸ¥ç¼“å­˜é€‰æ‹©",
      "authors": [
        "Dong Liu",
        "Yanxuan Yu"
      ],
      "abstract": "Serving large language models (LLMs) efficiently remains challenging due to the high memory and latency overhead of key-value (KV) cache access during autoregressive decoding. We present \\textbf{TinyServe}, a lightweight and extensible serving system for deploying tiny LLMs (e.g., TinyLLaMA, GPT2-345M) with support for structured KV sparsity, plugin-based token selection, and hardware-efficient attention kernels. Unlike prior simulation frameworks, TinyServe executes real-time decoding with configurable sparsity strategies and fine-grained instrumentation.\n  To reduce decoding cost, we introduce a \\textit{query-aware page selection} mechanism that leverages bounding-box metadata to estimate attention relevance between the query and KV cache blocks. This enables selective KV loading with minimal overhead and no model modifications. Our fused CUDA kernel integrates page scoring, sparse memory access, and masked attention in a single pass.\n  Experiments show that TinyServe achieves up to \\textbf{3.4x} speedup and over \\textbf{2x} memory savings with negligible accuracy drop. Additional analysis of cache reuse, page hit rate, and multi-GPU scaling confirms its practicality as an efficient system-level design for LLM training and inference research on resource-constrained hardware.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TinyServeï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºå°å‹ Large Language Models (LLMs) è®¾è®¡çš„è½»é‡çº§ä¸”å¯æ‰©å±•çš„æœåŠ¡ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³è‡ªå›å½’è§£ç è¿‡ç¨‹ä¸­ Key-Value (KV) cache è®¿é—®å¸¦æ¥çš„é«˜å†…å­˜å’Œé«˜å»¶è¿Ÿå¼€é”€ã€‚TinyServe å¼•å…¥äº†ä¸€ç§æŸ¥è¯¢æ„ŸçŸ¥é¡µé¢é€‰æ‹© (query-aware page selection) æœºåˆ¶ï¼Œåˆ©ç”¨ bounding-box metadata æ¥ä¼°è®¡æŸ¥è¯¢ä¸ KV cache å—ä¹‹é—´çš„æ³¨æ„åŠ›ç›¸å…³æ€§ï¼Œä»è€Œåœ¨æ— éœ€ä¿®æ”¹æ¨¡å‹çš„æƒ…å†µä¸‹å®ç°é€‰æ‹©æ€§ KV åŠ è½½ã€‚ç³»ç»Ÿé‡‡ç”¨äº†èåˆçš„ CUDA kernelï¼Œå°†é¡µé¢è¯„åˆ†ã€ç¨€ç–å†…å­˜è®¿é—®å’Œ masked attention é›†æˆåœ¨å•æ¬¡å¤„ç†ä¸­ï¼Œå¹¶æ”¯æŒç»“æ„åŒ– KV sparsityã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTinyServe åœ¨ä¿æŒç²¾åº¦å‡ ä¹æ— æŸçš„å‰æä¸‹ï¼Œå®ç°äº†é«˜è¾¾ 3.4x çš„åŠ é€Ÿå’Œè¶…è¿‡ 2x çš„å†…å­˜èŠ‚çœã€‚è¯¥ç³»ç»Ÿè¿˜é’ˆå¯¹ç¼“å­˜é‡ç”¨å’Œå¤š GPU æ‰©å±•æ€§è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä¸ºèµ„æºå—é™ç¡¬ä»¶ä¸Šçš„ LLM æ¨ç†ç ”ç©¶æä¾›äº†ä¸€ç§å®ç”¨çš„ç³»ç»Ÿçº§æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "Accepted to ACM MM as Oral Paper, also accepted to ICML MOSS workshop, publicly available as https://openreview.net/forum?id=sOdtl4jLci",
      "pdf_url": "https://arxiv.org/pdf/2509.12211v1",
      "published_date": "2025-08-28 16:17:18 UTC",
      "updated_date": "2025-08-28 16:17:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:50:15.955579+00:00"
    },
    {
      "arxiv_id": "2508.20953v1",
      "title": "A Multi-Objective Genetic Algorithm for Healthcare Workforce Scheduling",
      "title_zh": "åŒ»ç–—äººå‘˜æ’ç­çš„å¤šç›®æ ‡é—ä¼ ç®—æ³•",
      "authors": [
        "Vipul Patel",
        "Anirudh Deodhar",
        "Dagnachew Birru"
      ],
      "abstract": "Workforce scheduling in the healthcare sector is a significant operational challenge, characterized by fluctuating patient loads, diverse clinical skills, and the critical need to control labor costs while upholding high standards of patient care. This problem is inherently multi-objective, demanding a delicate balance between competing goals: minimizing payroll, ensuring adequate staffing for patient needs, and accommodating staff preferences to mitigate burnout. We propose a Multi-objective Genetic Algorithm (MOO-GA) that models the hospital unit workforce scheduling problem as a multi-objective optimization task. Our model incorporates real-world complexities, including hourly appointment-driven demand and the use of modular shifts for a multi-skilled workforce. By defining objective functions for cost, patient care coverage, and staff satisfaction, the GA navigates the vast search space to identify a set of high-quality, non-dominated solutions. Demonstrated on datasets representing a typical hospital unit, the results show that our MOO-GA generates robust and balanced schedules. On average, the schedules produced by our algorithm showed a 66\\% performance improvement over a baseline that simulates a conventional, manual scheduling process. This approach effectively manages trade-offs between critical operational and staff-centric objectives, providing a practical decision support tool for nurse managers and hospital administrators.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—ä¿å¥é¢†åŸŸå‘˜å·¥æ’ç­ï¼ˆWorkforce Schedulingï¼‰é¢ä¸´çš„å¤æ‚è¿è¥æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å¤šç›®æ ‡é—ä¼ ç®—æ³•ï¼ˆMulti-objective Genetic Algorithm, MOO-GAï¼‰ã€‚è¯¥æ¨¡å‹å°†åŒ»é™¢å•ä½çš„æ’ç­éœ€æ±‚å»ºæ¨¡ä¸ºå¤šç›®æ ‡ä¼˜åŒ–ä»»åŠ¡ï¼Œå……åˆ†è€ƒè™‘äº†æ¯å°æ—¶é¢„çº¦é©±åŠ¨çš„éœ€æ±‚æ³¢åŠ¨ä»¥åŠå¤šæŠ€èƒ½å‘˜å·¥çš„æ¨¡å—åŒ–è½®ç­ç­‰ç°å®å¤æ‚æ€§ã€‚é€šè¿‡ååŒä¼˜åŒ–æˆæœ¬ã€æ‚£è€…æŠ¤ç†è¦†ç›–ç‡å’Œå‘˜å·¥æ»¡æ„åº¦ä¸‰ä¸ªæ ¸å¿ƒç›®æ ‡å‡½æ•°ï¼ŒMOO-GA èƒ½å¤Ÿä»åºå¤§çš„æœç´¢ç©ºé—´ä¸­é«˜æ•ˆè¯†åˆ«å‡ºä¸€ç³»åˆ—é«˜è´¨é‡çš„éæ”¯é…è§£ã€‚åœ¨ä»£è¡¨å…¸å‹åŒ»é™¢å•ä½çš„æ•°æ®é›†æµ‹è¯•ä¸­ï¼Œè¯¥ç®—æ³•ç”Ÿæˆçš„æ’ç­æ–¹æ¡ˆæ¯”ä¼ ç»Ÿäººå·¥æ’ç­åŸºå‡†æ€§èƒ½å¹³å‡æå‡äº† 66%ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆå¹³è¡¡å…³é”®è¿è¥ç›®æ ‡ä¸å‘˜å·¥ç¦ç¥‰ï¼Œä¸ºæŠ¤ç†ç»ç†å’ŒåŒ»é™¢ç®¡ç†è€…æä¾›äº†ä¸€å¥—å®ç”¨çš„å†³ç­–æ”¯æŒå·¥å…·ã€‚",
      "categories": [
        "cs.AI",
        "cs.DM"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 7 figures, Accepted at the Multi-Objective Decision Making Workshop (MODeM2025) at ECAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.20953v1",
      "published_date": "2025-08-28 16:16:10 UTC",
      "updated_date": "2025-08-28 16:16:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:50:26.491116+00:00"
    },
    {
      "arxiv_id": "2509.00116v2",
      "title": "Meta-learning ecological priors from large language models explains human learning and decision making",
      "title_zh": "ä»å¤§è¯­è¨€æ¨¡å‹ä¸­å…ƒå­¦ä¹ ç”Ÿæ€å…ˆéªŒï¼šå¯¹äººç±»å­¦ä¹ ä¸å†³ç­–çš„è§£é‡Š",
      "authors": [
        "Akshay K. Jagadish",
        "Mirko Thalmann",
        "Julian Coda-Forno",
        "Marcel Binz",
        "Eric Schulz"
      ],
      "abstract": "Human cognition is profoundly shaped by the environments in which it unfolds. Yet, it remains an open question whether learning and decision making can be explained as a principled adaptation to the statistical structure of real-world tasks. We introduce ecologically rational analysis, a computational framework that unifies the normative foundations of rational analysis with ecological grounding. Leveraging large language models to generate ecologically valid cognitive tasks at scale, and using meta-learning to derive rational models optimized for these environments, we develop a new class of learning algorithms: Ecologically Rational Meta-learned Inference (ERMI). ERMI internalizes the statistical regularities of naturalistic problem spaces and adapts flexibly to novel situations, without requiring hand-crafted heuristics or explicit parameter updates. We show that ERMI captures human behavior across 15 experiments spanning function learning, category learning, and decision making, outperforming several established cognitive models in trial-by-trial prediction. Our results suggest that much of human cognition may reflect adaptive alignment to the ecological structure of the problems we encounter in everyday life.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ç”Ÿæ€ç†æ€§åˆ†æ(ecologically rational analysis)è®¡ç®—æ¡†æ¶ï¼Œæ—¨åœ¨æ¢è®¨äººç±»çš„å­¦ä¹ ä¸å†³ç­–æ˜¯å¦æ˜¯å¯¹ç°å®ä¸–ç•Œä»»åŠ¡ç»Ÿè®¡ç»“æ„çš„åŸåˆ™æ€§é€‚åº”ã€‚ç ”ç©¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models)å¤§è§„æ¨¡ç”Ÿæˆå…·æœ‰ç”Ÿæ€æœ‰æ•ˆæ€§çš„è®¤çŸ¥ä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨å…ƒå­¦ä¹ (Meta-learning)æŠ€æœ¯æ¨å¯¼å‡ºé’ˆå¯¹è¿™äº›ç¯å¢ƒä¼˜åŒ–çš„ç†æ€§æ¨¡å‹ã€‚ç”±æ­¤å¼€å‘çš„ç”Ÿæ€ç†æ€§å…ƒå­¦ä¹ æ¨ç†(Ecologically Rational Meta-learned Inference, ERMI)ç®—æ³•èƒ½å¤Ÿå†…åŒ–è‡ªç„¶ä¸»ä¹‰é—®é¢˜ç©ºé—´çš„ç»Ÿè®¡è§„å¾‹ï¼Œåœ¨æ— éœ€æ‰‹åŠ¨å¯å‘å¼è§„åˆ™æˆ–æ˜¾å¼å‚æ•°æ›´æ–°çš„æƒ…å†µä¸‹çµæ´»é€‚åº”æ–°æƒ…å¢ƒã€‚é€šè¿‡æ¶µç›–å‡½æ•°å­¦ä¹ (function learning)ã€ç±»åˆ«å­¦ä¹ (category learning)å’Œå†³ç­–åˆ¶å®šçš„15é¡¹å®éªŒè¯æ˜ï¼ŒERMIåœ¨é€é¡¹è¯•éªŒçš„è¡Œä¸ºé¢„æµ‹ä¸Šä¼˜äºå¤šç§æ—¢æœ‰çš„è®¤çŸ¥æ¨¡å‹ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œäººç±»è®¤çŸ¥çš„è¯¸å¤šè¡¨ç°åæ˜ äº†å¯¹æ—¥å¸¸ç”Ÿæ´»ä¸­é‡åˆ°çš„é—®é¢˜ç”Ÿæ€ç»“æ„çš„è‡ªé€‚åº”å¯¹é½ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00116v2",
      "published_date": "2025-08-28 16:07:40 UTC",
      "updated_date": "2025-09-03 03:16:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:50:24.692703+00:00"
    },
    {
      "arxiv_id": "2509.00115v3",
      "title": "Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems",
      "title_zh": "æ™ºèƒ½ä½“äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„è‡ªé€‚åº”ç›‘æµ‹ä¸çœŸå®ä¸–ç•Œè¯„ä¼°",
      "authors": [
        "Manish Shukla"
      ],
      "abstract": "Agentic artificial intelligence (AI) -- multi-agent systems that combine large language models with external tools and autonomous planning -- are rapidly transitioning from research laboratories into high-stakes domains. Our earlier \"Basic\" paper introduced a five-axis framework and proposed preliminary metrics such as goal drift and harm reduction but did not provide an algorithmic instantiation or empirical evidence. This \"Advanced\" sequel fills that gap. First, we revisit recent benchmarks and industrial deployments to show that technical metrics still dominate evaluations: a systematic review of 84 papers from 2023--2025 found that 83% report capability metrics while only 30% consider human-centred or economic axes [2]. Second, we formalise an Adaptive Multi-Dimensional Monitoring (AMDM) algorithm that normalises heterogeneous metrics, applies per-axis exponentially weighted moving-average thresholds and performs joint anomaly detection via the Mahalanobis distance [7]. Third, we conduct simulations and real-world experiments. AMDM cuts anomaly-detection latency from 12.3 s to 5.6 s on simulated goal drift and reduces false-positive rates from 4.5% to 0.9% compared with static thresholds. We present a comparison table and ROC/PR curves, and we reanalyse case studies to surface missing metrics. Code, data and a reproducibility checklist accompany this paper to facilitate replication. The code supporting this work is available at https://github.com/Manishms18/Adaptive-Multi-Dimensional-Monitoring.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ™ºèƒ½ä½“AIç³»ç»Ÿï¼ˆAgentic AIï¼‰ä»å®éªŒå®¤å‘é«˜é£é™©é¢†åŸŸçš„è½¬å‹è¿‡ç¨‹ï¼Œå¹¶æŒ‡å‡ºå½“å‰è¯„ä¼°ä½“ç³»è¿‡åº¦ä¾§é‡æŠ€æœ¯æŒ‡æ ‡ï¼Œè€Œä¸¥é‡å¿½è§†äº†ä»¥äººä¸ºæœ¬å’Œç»æµç»´åº¦çš„è¯„ä»·ã€‚é’ˆå¯¹è¿™ä¸€ç¼ºå£ï¼Œè®ºæ–‡æå‡ºäº†è‡ªé€‚åº”å¤šç»´ç›‘æ§ï¼ˆAdaptive Multi-Dimensional Monitoring, AMDMï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡å½’ä¸€åŒ–å¼‚æ„æŒ‡æ ‡ã€åº”ç”¨å„ç»´åº¦æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡ï¼ˆEWMAï¼‰é˜ˆå€¼ï¼Œå¹¶åˆ©ç”¨é©¬æ°è·ç¦»ï¼ˆMahalanobis distanceï¼‰è¿›è¡Œè”åˆå¼‚å¸¸æ£€æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸é™æ€é˜ˆå€¼ç›¸æ¯”ï¼ŒAMDMåœ¨æ¨¡æ‹Ÿç›®æ ‡åç§»ï¼ˆgoal driftï¼‰ä¸­çš„å¼‚å¸¸æ£€æµ‹å»¶è¿Ÿä»12.3ç§’æ˜¾è‘—ç¼©çŸ­è‡³5.6ç§’ï¼Œä¸”è¯¯æŠ¥ç‡ä»4.5%é™ä½è‡³0.9%ã€‚é€šè¿‡å¯¹æ¡ˆä¾‹ç ”ç©¶çš„é‡æ–°åˆ†æï¼Œè¯¥ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†ç°æœ‰è¯„ä¼°ä¸­ç¼ºå¤±çš„å…³é”®æŒ‡æ ‡ï¼Œå¹¶æä¾›äº†å¼€æºä»£ç å’Œå¯é‡å¤æ€§æ ¸æŸ¥è¡¨ä»¥æ”¯æŒå­¦æœ¯å¤ç°ã€‚è¯¥å·¥ä½œä¸ºé«˜é£é™©ç¯å¢ƒä¸‹Agentic AIç³»ç»Ÿçš„å®æ—¶ç›‘æ§ä¸å®è¯è¯„ä¼°æä¾›äº†ç³»ç»ŸåŒ–çš„æŠ€æœ¯æ¡†æ¶ä¸ç®—æ³•æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00115v3",
      "published_date": "2025-08-28 15:52:49 UTC",
      "updated_date": "2025-09-13 03:25:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:50:27.453401+00:00"
    },
    {
      "arxiv_id": "2508.20912v1",
      "title": "Research Challenges in Relational Database Management Systems for LLM Queries",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹æŸ¥è¯¢çš„å…³ç³»å‹æ•°æ®åº“ç®¡ç†ç³»ç»Ÿç ”ç©¶æŒ‘æˆ˜",
      "authors": [
        "Kerem Akillioglu",
        "Anurag Chakraborty",
        "Sairaj Voruganti",
        "M. Tamer Ã–zsu"
      ],
      "abstract": "Large language models (LLMs) have become essential for applications such as text summarization, sentiment analysis, and automated question-answering. Recently, LLMs have also been integrated into relational database management systems to enhance querying and support advanced data processing. Companies such as Amazon, Databricks, Google, and Snowflake offer LLM invocation directly within SQL, denoted as LLM queries, to boost data insights. However, open-source solutions currently have limited functionality and poor performance. In this work, we present an early exploration of two open-source systems and one enterprise platform, using five representative queries to expose functional, performance, and scalability limits in today's SQL-invoked LLM integrations. We identify three main issues: enforcing structured outputs, optimizing resource utilization, and improving query planning. We implemented initial solutions and observed improvements in accommodating LLM powered SQL queries. These early gains demonstrate that tighter integration of LLM+DBMS is the key to scalable and efficient processing of LLM queries.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)é›†æˆåˆ°å…³ç³»æ•°æ®åº“ç®¡ç†ç³»ç»Ÿ(RDBMS)ä¸­ä»¥æ”¯æŒSQLè°ƒç”¨çš„LLM queriesï¼Œæ—¨åœ¨åˆ†æå½“å‰ç³»ç»Ÿåœ¨å¤„ç†æ­¤ç±»æŸ¥è¯¢æ—¶çš„å±€é™æ€§ã€‚ä½œè€…è¯„ä¼°äº†ä¸¤ä¸ªå¼€æºç³»ç»Ÿå’Œä¸€ä¸ªä¼ä¸šå¹³å°ï¼Œé€šè¿‡äº”ç§ä»£è¡¨æ€§æŸ¥è¯¢æ­ç¤ºäº†å®ƒä»¬åœ¨åŠŸèƒ½ã€æ€§èƒ½å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚ç ”ç©¶æ˜ç¡®æå‡ºäº†å½“å‰é¢ä¸´çš„ä¸‰å¤§æ ¸å¿ƒé—®é¢˜ï¼šå¼ºåˆ¶ç»“æ„åŒ–è¾“å‡º(enforcing structured outputs)ã€ä¼˜åŒ–èµ„æºåˆ©ç”¨ç‡(optimizing resource utilization)ä»¥åŠæ”¹è¿›æŸ¥è¯¢è§„åˆ’(query planning)ã€‚é€šè¿‡å®æ–½åˆæ­¥ä¼˜åŒ–æ–¹æ¡ˆï¼Œå®éªŒè§‚å¯Ÿåˆ°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯å®äº†LLMä¸DBMSçš„æ·±åº¦é›†æˆæ˜¯å®ç°é«˜æ•ˆã€å¯æ‰©å±•çš„å¤§æ¨¡å‹æŸ¥è¯¢å¤„ç†çš„å…³é”®è·¯å¾„ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "This paper will appear in the 6th International Workshop on Applied AI for Database Systems and Applications, AIDB Workshop at VLDB 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.20912v1",
      "published_date": "2025-08-28 15:41:49 UTC",
      "updated_date": "2025-08-28 15:41:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:50:32.158005+00:00"
    },
    {
      "arxiv_id": "2508.21109v1",
      "title": "An Explainable, Attention-Enhanced, Bidirectional Long Short-Term Memory Neural Network for Joint 48-Hour Forecasting of Temperature, Irradiance, and Relative Humidity",
      "title_zh": "ç”¨äºæ¸©åº¦ã€è¾ç…§åº¦ä¸ç›¸å¯¹æ¹¿åº¦ 48 å°æ—¶è”åˆé¢„æµ‹çš„å¯è§£é‡Šæ³¨æ„åŠ›å¢å¼ºå‹åŒå‘é•¿çŸ­æœŸè®°å¿†ç¥ç»ç½‘ç»œ",
      "authors": [
        "Georgios Vamvouras",
        "Konstantinos Braimakis",
        "Christos Tzivanidis"
      ],
      "abstract": "This paper presents a Deep Learning (DL) framework for 48-hour forecasting of temperature, solar irradiance, and relative humidity to support Model Predictive Control (MPC) in smart HVAC systems. The approach employs a stacked Bidirectional Long Short-Term Memory (BiLSTM) network with attention, capturing temporal and cross-feature dependencies by jointly predicting all three variables. Historical meteorological data (2019-2022) with encoded cyclical time features were used for training, while 2023 data evaluated generalization. The model achieved Mean Absolute Errors of 1.3 degrees Celsius (temperature), 31 W/m2 (irradiance), and 6.7 percentage points (humidity), outperforming state-of-the-art numerical weather prediction and machine learning benchmarks. Integrated Gradients quantified feature contributions, and attention weights revealed temporal patterns, enhancing interpretability. By combining multivariate forecasting, attention-based DL, and explainability, this work advances data-driven weather prediction. The demonstrated accuracy and transparency highlight the framework's potential for energy-efficient building control through reliable short-term meteorological forecasting.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å…·æœ‰è§£é‡Šæ€§çš„ã€æ³¨æ„åŠ›å¢å¼ºçš„åŒå‘é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ (BiLSTM)ï¼Œæ—¨åœ¨ä¸ºæ™ºèƒ½æš–é€šç©ºè°ƒ (HVAC) ç³»ç»Ÿçš„æ¨¡å‹é¢„æµ‹æ§åˆ¶ (MPC) æä¾›ç²¾å‡†çš„ 48 å°æ—¶æ°”è±¡é¢„æŠ¥ã€‚è¯¥æ¡†æ¶é€šè¿‡å †å çš„ BiLSTM ç½‘ç»œè”åˆé¢„æµ‹æ¸©åº¦ã€å¤ªé˜³è¾ç…§åº¦å’Œç›¸å¯¹æ¹¿åº¦ï¼Œåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æœ‰æ•ˆæ•æ‰äº†å˜é‡é—´çš„æ—¶é—´åºåˆ—å’Œè·¨ç‰¹å¾ä¾èµ–å…³ç³»ã€‚å®éªŒé‡‡ç”¨ 2019-2022 å¹´çš„å†å²æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨ 2023 å¹´æ•°æ®ä¸ŠéªŒè¯äº†æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ï¼Œç»“æœæ˜¾ç¤ºå…¶é¢„æµ‹ç²¾åº¦æ˜¾è‘—ä¼˜äºç°æœ‰çš„æ•°å€¼å¤©æ°”é¢„æŠ¥ (NWP) å’Œæœºå™¨å­¦ä¹ åŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†é›†æˆæ¢¯åº¦ (Integrated Gradients) ç®—æ³•æ¥é‡åŒ–ç‰¹å¾è´¡çŒ®ï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–æ­ç¤ºæ—¶é—´æ¨¡å¼ï¼Œæ˜¾è‘—å¢å¼ºäº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ°”è±¡é¢„æµ‹é¢†åŸŸçš„é€æ˜åº¦ä¸å¯è§£é‡Šæ€§ã€‚è¿™ä¸€æˆæœé€šè¿‡é«˜ç²¾åº¦çš„å¤šå˜é‡é¢„æµ‹æ–¹æ¡ˆï¼Œä¸ºå®ç°èŠ‚èƒ½å»ºç­‘æ§åˆ¶å’Œå¯é çš„çŸ­æœŸæ°”è±¡é¢„æŠ¥å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "27 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.21109v1",
      "published_date": "2025-08-28 15:40:33 UTC",
      "updated_date": "2025-08-28 15:40:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:50:38.081899+00:00"
    },
    {
      "arxiv_id": "2508.20907v1",
      "title": "Quantum Verifiable Rewards for Post-Training Qiskit Code Assistant",
      "title_zh": "é¢å‘ Qiskit ä»£ç åŠ©æ‰‹åè®­ç»ƒçš„é‡å­å¯éªŒè¯å¥–åŠ±",
      "authors": [
        "Nicolas Dupuis",
        "Adarsh Tiwari",
        "Youssef Mroueh",
        "David Kremer",
        "Ismael Faro",
        "Juan Cruz-Benito"
      ],
      "abstract": "Qiskit is an open-source quantum computing framework that allows users to design, simulate, and run quantum circuits on real quantum hardware. We explore post-training techniques for LLMs to assist in writing Qiskit code. We introduce quantum verification as an effective method for ensuring code quality and executability on quantum hardware. To support this, we developed a synthetic data pipeline that generates quantum problem-unit test pairs and used it to create preference data for aligning LLMs with DPO. Additionally, we trained models using GRPO, leveraging quantum-verifiable rewards provided by the quantum hardware. Our best-performing model, combining DPO and GRPO, surpasses the strongest open-source baselines on the challenging Qiskit-HumanEval-hard benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é’ˆå¯¹ LLMs çš„è®­ç»ƒå(Post-training)ä¼˜åŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨æå‡å…¶ç¼–å†™ Qiskit é‡å­è®¡ç®—æ¡†æ¶ä»£ç çš„èƒ½åŠ›ã€‚ä½œè€…å¼•å…¥äº† Quantum verification æ–¹æ³•ï¼Œå°†å…¶ä½œä¸ºç¡®ä¿ä»£ç è´¨é‡ä»¥åŠåœ¨é‡å­ç¡¬ä»¶ä¸Šå¯æ‰§è¡Œæ€§çš„æœ‰æ•ˆæ‰‹æ®µã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€æ–¹æ³•ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªåˆæˆæ•°æ®æµæ°´çº¿ï¼Œç”¨äºç”Ÿæˆé‡å­é—®é¢˜ä¸å•å…ƒæµ‹è¯•å¯¹ï¼Œå¹¶æ®æ­¤åˆ›å»ºåå¥½æ•°æ®ä»¥è¿›è¡Œ DPO æ¨¡å‹å¯¹é½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ©ç”¨ GRPO ç®—æ³•è®­ç»ƒæ¨¡å‹ï¼Œå¹¶å……åˆ†å‘æŒ¥äº†ç”±é‡å­ç¡¬ä»¶æä¾›çš„ Quantum-verifiable rewards çš„ä¼˜åŠ¿ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»“åˆäº† DPO å’Œ GRPO çš„æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ Qiskit-HumanEval-hard åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†ç›®å‰æœ€å¼ºçš„å¼€æºåŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20907v1",
      "published_date": "2025-08-28 15:37:40 UTC",
      "updated_date": "2025-08-28 15:37:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:50:36.053754+00:00"
    },
    {
      "arxiv_id": "2508.20866v3",
      "title": "AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning",
      "title_zh": "åŸºäºä¼˜åŒ–æ¨ç†çš„ AI æ™ºèƒ½ä½“æ¼æ´æ³¨å…¥ä¸è½¬æ¢",
      "authors": [
        "Amine Lbath",
        "Massih-Reza Amini",
        "Aurelien Delaitre",
        "Vadim Okun"
      ],
      "abstract": "The increasing complexity of software systems and the sophistication of cyber-attacks have underscored the critical need for effective automated vulnerability detection and repair systems. Data-driven approaches using deep learning models show promise but critically depend on the availability of large, accurately labeled datasets. Yet existing datasets either suffer from noisy labels, limited range of vulnerabilities, or fail to reflect vulnerabilities as they occur in real-world software. This also limits large-scale benchmarking of such solutions. Automated vulnerability injection provides a way to directly address these dataset limitations, but existing techniques remain limited in coverage, contextual fidelity, or injection success rates. In this paper, we present AVIATOR, the first AI-agentic vulnerability injection workflow. It automatically injects realistic, category-specific vulnerabilities for high-fidelity, diverse, large-scale vulnerability dataset generation. Unlike prior monolithic approaches, AVIATOR orchestrates specialized AI agents, function agents and traditional code analysis tools that replicate expert reasoning. It combines semantic analysis, injection synthesis enhanced with LoRA-based fine-tuning and Retrieval-Augmented Generation, as well as post-injection validation via static analysis and LLM-based discriminators. This modular decomposition allows specialized agents to focus on distinct tasks, improving robustness of injection and reducing error propagation across the workflow. Evaluations across three distinct benchmarks demonstrate that AVIATOR achieves 91%-95% injection success rates, significantly surpassing existing automated dataset generation techniques in both accuracy and scope of software vulnerabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AVIATORï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäº AI æ™ºèƒ½ä½“ (AI-agentic) çš„æ¼æ´æ³¨å…¥å·¥ä½œæµï¼Œæ—¨åœ¨ç”Ÿæˆé«˜ä¿çœŸã€å¤šæ ·åŒ–ä¸”å¤§è§„æ¨¡çš„è½¯ä»¶æ¼æ´æ•°æ®é›†ã€‚AVIATOR æ”¹å˜äº†ä»¥å¾€å•ä¸€çš„æ¨¡å¼ï¼Œé€šè¿‡ç¼–æ’ä¸“é—¨çš„ AI æ™ºèƒ½ä½“ã€åŠŸèƒ½æ™ºèƒ½ä½“å’Œä¼ ç»Ÿä»£ç åˆ†æå·¥å…·æ¥æ¨¡æ‹Ÿä¸“å®¶æ¨ç†ï¼Œå®ç°ç°å®ä¸”ç‰¹å®šç±»åˆ«çš„æ¼æ´æ³¨å…¥ã€‚è¯¥ç³»ç»Ÿé›†æˆäº†è¯­ä¹‰åˆ†æã€åŸºäº LoRA çš„å¾®è°ƒæ³¨å…¥åˆæˆã€æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ä»¥åŠåˆ©ç”¨é™æ€åˆ†æå’Œå¤§è¯­è¨€æ¨¡å‹ (LLM) åˆ¤åˆ«å™¨è¿›è¡Œçš„æ³¨å…¥åéªŒè¯ã€‚è¿™ç§æ¨¡å—åŒ–è®¾è®¡ä½¿ä¸“ä¸šæ™ºèƒ½ä½“èƒ½å¤Ÿä¸“æ³¨äºç‰¹å®šä»»åŠ¡ï¼Œä»è€Œå¢å¼ºäº†æ³¨å…¥çš„é²æ£’æ€§å¹¶å‡å°‘äº†å·¥ä½œæµä¸­çš„é”™è¯¯ä¼ æ’­ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒAVIATOR å®ç°äº† 91%-95% çš„æ³¨å…¥æˆåŠŸç‡ï¼Œåœ¨å‡†ç¡®æ€§å’Œè½¯ä»¶æ¼æ´è¦†ç›–èŒƒå›´ä¸Šå‡æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„è‡ªåŠ¨åŒ–æ•°æ®é›†ç”ŸæˆæŠ€æœ¯ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20866v3",
      "published_date": "2025-08-28 14:59:39 UTC",
      "updated_date": "2025-11-06 21:24:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:50:41.940266+00:00"
    },
    {
      "arxiv_id": "2508.20848v1",
      "title": "JADES: A Universal Framework for Jailbreak Assessment via Decompositional Scoring",
      "title_zh": "JADESï¼šåŸºäºåˆ†è§£å¼è¯„åˆ†çš„é€šç”¨è¶Šç‹±è¯„ä¼°æ¡†æ¶",
      "authors": [
        "Junjie Chu",
        "Mingjie Li",
        "Ziqing Yang",
        "Ye Leng",
        "Chenhao Lin",
        "Chao Shen",
        "Michael Backes",
        "Yun Shen",
        "Yang Zhang"
      ],
      "abstract": "Accurately determining whether a jailbreak attempt has succeeded is a fundamental yet unresolved challenge. Existing evaluation methods rely on misaligned proxy indicators or naive holistic judgments. They frequently misinterpret model responses, leading to inconsistent and subjective assessments that misalign with human perception. To address this gap, we introduce JADES (Jailbreak Assessment via Decompositional Scoring), a universal jailbreak evaluation framework. Its key mechanism is to automatically decompose an input harmful question into a set of weighted sub-questions, score each sub-answer, and weight-aggregate the sub-scores into a final decision. JADES also incorporates an optional fact-checking module to strengthen the detection of hallucinations in jailbreak responses. We validate JADES on JailbreakQR, a newly introduced benchmark proposed in this work, consisting of 400 pairs of jailbreak prompts and responses, each meticulously annotated by humans. In a binary setting (success/failure), JADES achieves 98.5% agreement with human evaluators, outperforming strong baselines by over 9%. Re-evaluating five popular attacks on four LLMs reveals substantial overestimation (e.g., LAA's attack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results show that JADES could deliver accurate, consistent, and interpretable evaluations, providing a reliable basis for measuring future jailbreak attacks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¤§è¯­è¨€æ¨¡å‹è¶Šç‹±ï¼ˆjailbreakï¼‰è¯„ä¼°æ–¹æ³•å­˜åœ¨çš„ä¸ä¸€è‡´æ€§ã€ä¸»è§‚æ€§ä»¥åŠä¸äººç±»æ„ŸçŸ¥é”™ä½ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº† JADESï¼ˆJailbreak Assessment via Decompositional Scoringï¼‰é€šç”¨è¯„ä¼°æ¡†æ¶ã€‚JADES çš„æ ¸å¿ƒæœºåˆ¶æ˜¯å°†è¾“å…¥çš„æœ‰å®³é—®é¢˜è‡ªåŠ¨åˆ†è§£ä¸ºä¸€ç³»åˆ—åŠ æƒå­é—®é¢˜ï¼Œåˆ†åˆ«å¯¹å„ä¸ªå­ç­”æ¡ˆè¿›è¡Œè¯„åˆ†ï¼Œå¹¶æœ€ç»ˆé€šè¿‡åŠ æƒèšåˆå¾—å‡ºè¶Šç‹±åˆ¤å®šçš„å†³ç­–ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜åŒ…å«ä¸€ä¸ªå¯é€‰çš„äº‹å®æ ¸æŸ¥ï¼ˆfact-checkingï¼‰æ¨¡å—ï¼Œç”¨ä»¥å¢å¼ºå¯¹è¶Šç‹±å“åº”ä¸­å¹»è§‰ï¼ˆhallucinationsï¼‰ç°è±¡çš„æ£€æµ‹ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨åŒ…å« 400 å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ JailbreakQR åŸºå‡†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤º JADES ä¸äººç±»è¯„ä»·è€…çš„ä¸€è‡´æ€§é«˜è¾¾ 98.5%ï¼Œä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ 9% ä»¥ä¸Šã€‚é€šè¿‡é‡æ–°è¯„ä¼°å‘ç°ï¼Œç°æœ‰è¶Šç‹±æ”»å‡»çš„æˆåŠŸç‡ï¼ˆAttack Success Rateï¼‰å¾€å¾€è¢«æ˜¾è‘—é«˜ä¼°ï¼Œä¾‹å¦‚éƒ¨åˆ†æ”»å‡»åœ¨ GPT-3.5-Turbo ä¸Šçš„æˆåŠŸç‡ä» 93% é™è‡³ 69%ã€‚JADES ä¸ºè¡¡é‡æœªæ¥è¶Šç‹±æ”»å‡»æä¾›äº†å‡†ç¡®ã€ä¸€è‡´ä¸”å…·å¤‡å¯è§£é‡Šæ€§çš„è¯„ä¼°æ ‡å‡†ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "17 pages, 5 figures. For the code and data supporting this work, see https://trustairlab.github.io/jades.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2508.20848v1",
      "published_date": "2025-08-28 14:40:27 UTC",
      "updated_date": "2025-08-28 14:40:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:50:54.459045+00:00"
    },
    {
      "arxiv_id": "2508.21107v2",
      "title": "Learning to Generate Unit Test via Adversarial Reinforcement Learning",
      "title_zh": "åŸºäºå¯¹æŠ—æ€§å¼ºåŒ–å­¦ä¹ çš„å•å…ƒæµ‹è¯•ç”Ÿæˆå­¦ä¹ ",
      "authors": [
        "Dongjun Lee",
        "Changho Hwang",
        "Kimin Lee"
      ],
      "abstract": "Unit testing is a core practice in programming, enabling systematic evaluation of programs produced by human developers or large language models (LLMs). Given the challenges in writing comprehensive unit tests, LLMs have been employed to automate test generation, yet methods for training LLMs to produce high-quality tests remain underexplored. In this work, we propose UTRL, a novel reinforcement learning framework that trains an LLM to generate high-quality unit tests given a programming instruction. Our key idea is to iteratively train two LLMs, the unit test generator and the code generator, in an adversarial manner via reinforcement learning. The unit test generator is trained to maximize a discrimination reward, which reflects its ability to produce tests that expose faults in the code generator's solutions, and the code generator is trained to maximize a code reward, which reflects its ability to produce solutions that pass the unit tests generated by the test generator. In our experiments, we demonstrate that unit tests generated by Qwen3-4B trained via UTRL show higher quality compared to unit tests generated by the same model trained via supervised fine-tuning on human-written ground-truth unit tests, yielding code evaluations that more closely align with those induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL outperforms frontier models such as GPT-4.1 in generating high-quality unit tests, highlighting the effectiveness of UTRL in training LLMs for this task.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† UTRLï¼Œä¸€ç§æ–°å‹çš„ Reinforcement Learning æ¡†æ¶ï¼Œæ—¨åœ¨è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ (LLMs) ç”Ÿæˆé«˜è´¨é‡çš„ Unit Testã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ Adversarial Reinforcement Learning è¿­ä»£è®­ç»ƒå•å…ƒæµ‹è¯•ç”Ÿæˆå™¨å’Œä»£ç ç”Ÿæˆå™¨ï¼Œä½¿ä¸¤è€…åœ¨å¯¹æŠ—ä¸­å…±åŒè¿›åŒ–ã€‚å•å…ƒæµ‹è¯•ç”Ÿæˆå™¨çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ– Discrimination Rewardï¼Œå³ç”Ÿæˆèƒ½æœ‰æ•ˆæš´éœ²ä»£ç ç”Ÿæˆå™¨æ•…éšœçš„æµ‹è¯•ï¼›è€Œä»£ç ç”Ÿæˆå™¨åˆ™é€šè¿‡æœ€å¤§åŒ– Code Reward æ¥æå‡é€šè¿‡æµ‹è¯•çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç» UTRL è®­ç»ƒçš„ Qwen3-4B åœ¨æµ‹è¯•è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç»è¿‡ Supervised Fine-Tuning å¤„ç†çš„æ¨¡å‹ï¼Œä¸”å…¶ç”Ÿæˆçš„æµ‹è¯•åœ¨ä»£ç è¯„ä¼°ä¸­ä¸ Ground-truth çš„ä¸€è‡´æ€§æ›´é«˜ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è®­ç»ƒçš„æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å•å…ƒæµ‹è¯•çš„ä»»åŠ¡ä¸­è¶…è¶Šäº† GPT-4.1 ç­‰å‰æ²¿æ¨¡å‹ï¼ŒéªŒè¯äº† UTRL åœ¨æå‡æ¨¡å‹è‡ªåŠ¨åŒ–æµ‹è¯•ç”Ÿæˆèƒ½åŠ›æ–¹é¢çš„å“è¶Šæœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Code is available at: https://github.com/dgjun32/UTRL",
      "pdf_url": "https://arxiv.org/pdf/2508.21107v2",
      "published_date": "2025-08-28 14:32:44 UTC",
      "updated_date": "2025-09-30 13:55:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:51:00.355149+00:00"
    },
    {
      "arxiv_id": "2508.20840v3",
      "title": "Learning Primitive Embodied World Models: Towards Scalable Robotic Learning",
      "title_zh": "å­¦ä¹ å…ƒåŠ¨ä½œå…·èº«ä¸–ç•Œæ¨¡å‹ï¼šè¿ˆå‘å¯æ‰©å±•æœºå™¨äººå­¦ä¹ ",
      "authors": [
        "Qiao Sun",
        "Liujia Yang",
        "Wei Tang",
        "Wei Huang",
        "Kaixin Xu",
        "Yongchao Chen",
        "Mingyu Liu",
        "Jiange Yang",
        "Haoyi Zhu",
        "Yating Wang",
        "Tong He",
        "Yilun Chen",
        "Xili Dai",
        "Nanyang Ye",
        "Qinying Gu"
      ],
      "abstract": "While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a \"GPT moment\" in the embodied domain. There is a naive observation: the diversity of embodied data far exceeds the relatively small space of possible primitive motions. Based on this insight, we propose a novel paradigm for world modeling--Primitive Embodied World Models (PEWM). By restricting video generation to fixed short horizons, our approach 1) enables fine-grained alignment between linguistic concepts and visual representations of robotic actions, 2) reduces learning complexity, 3) improves data efficiency in embodied data collection, and 4) decreases inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PEWM (Primitive Embodied World Models)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³å…·èº«æ™ºèƒ½é¢†åŸŸæ•°æ®ç¨€ç¼ºã€å¯¹é½ç²’åº¦ç²—ç³™ä»¥åŠé•¿ç¨‹è§†é¢‘ç”ŸæˆæŒ‘æˆ˜çš„æ–°å‹ä¸–ç•Œæ¨¡å‹èŒƒå¼ã€‚è¯¥æ¨¡å‹åŸºäºå…·èº«æ•°æ®å¤šæ ·æ€§è¿œè¶…æœ‰é™åŸå§‹è¿åŠ¨ç©ºé—´çš„è§‚å¯Ÿï¼Œé€šè¿‡å°†è§†é¢‘ç”Ÿæˆé™åˆ¶åœ¨å›ºå®šçš„çŸ­æ—¶ç•Œ (fixed short horizons) å†…ï¼Œæœ‰æ•ˆé™ä½äº†å­¦ä¹ å¤æ‚åº¦å¹¶æå‡äº†æ•°æ®æ•ˆç‡ã€‚PEWM å®ç°äº†è¯­è¨€æ¦‚å¿µä¸æœºå™¨äººåŠ¨ä½œè§†è§‰è¡¨å¾ä¹‹é—´çš„ç»†ç²’åº¦å¯¹é½ï¼Œå¹¶ç»“åˆæ¨¡å—åŒ– Vision-Language Model (VLM) è§„åˆ’å™¨ä¸èµ·ç‚¹-ç»ˆç‚¹çƒ­å›¾å¼•å¯¼æœºåˆ¶ (Start-Goal heatmap Guidance, SGG) å®ç°äº†çµæ´»çš„é—­ç¯æ§åˆ¶ã€‚è¿™ç§æ¶æ„æ”¯æŒåœ¨æ‰©å±•çš„å¤æ‚ä»»åŠ¡ä¸­è¿›è¡ŒåŸå§‹ç­–ç•¥å±‚é¢çš„ç»„åˆæ³›åŒ– (compositional generalization)ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†æ¨ç†å»¶è¿Ÿã€‚é€šè¿‡èåˆè§†é¢‘æ¨¡å‹çš„æ—¶ç©ºè§†è§‰å…ˆéªŒä¸ VLM çš„è¯­ä¹‰æ„ŸçŸ¥èƒ½åŠ›ï¼ŒPEWM æˆåŠŸå¼¥åˆäº†ç»†ç²’åº¦ç‰©ç†äº¤äº’ä¸é«˜å±‚æ¨ç†ä¹‹é—´çš„é¸¿æ²Ÿï¼Œä¸ºå®ç°å¯æ‰©å±•ä¸”é€šç”¨çš„å…·èº«æ™ºèƒ½æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20840v3",
      "published_date": "2025-08-28 14:31:48 UTC",
      "updated_date": "2025-11-24 03:42:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:51:21.056514+00:00"
    },
    {
      "arxiv_id": "2508.20816v1",
      "title": "Multi-Agent Penetration Testing AI for the Web",
      "title_zh": "é¢å‘ Web çš„å¤šæ™ºèƒ½ä½“æ¸—é€æµ‹è¯•äººå·¥æ™ºèƒ½",
      "authors": [
        "Isaac David",
        "Arthur Gervais"
      ],
      "abstract": "AI-powered development platforms are making software creation accessible to a broader audience, but this democratization has triggered a scalability crisis in security auditing. With studies showing that up to 40% of AI-generated code contains vulnerabilities, the pace of development now vastly outstrips the capacity for thorough security assessment.\n  We present MAPTA, a multi-agent system for autonomous web application security assessment that combines large language model orchestration with tool-grounded execution and end-to-end exploit validation. On the 104-challenge XBOW benchmark, MAPTA achieves 76.9% overall success with perfect performance on SSRF and misconfiguration vulnerabilities, 83% success on broken authorization, and strong results on injection attacks including server-side template injection (85%) and SQL injection (83%). Cross-site scripting (57%) and blind SQL injection (0%) remain challenging. Our comprehensive cost analysis across all challenges totals $21.38 with a median cost of $0.073 for successful attempts versus $0.357 for failures. Success correlates strongly with resource efficiency, enabling practical early-stopping thresholds at approximately 40 tool calls or $0.30 per challenge.\n  MAPTA's real-world findings are impactful given both the popularity of the respective scanned GitHub repositories (8K-70K stars) and MAPTA's low average operating cost of $3.67 per open-source assessment: MAPTA discovered critical vulnerabilities including RCEs, command injections, secret exposure, and arbitrary file write vulnerabilities. Findings are responsibly disclosed, 10 findings are under CVE review.",
      "tldr_zh": "é’ˆå¯¹AIç”Ÿæˆä»£ç ä¸­æ™®éå­˜åœ¨çš„å®‰å…¨æ¼æ´ä»¥åŠä¼ ç»Ÿå®¡è®¡æ•ˆç‡ä¸è¶³çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†MAPTAï¼Œä¸€ç§ç”¨äºWebåº”ç”¨ç¨‹åºè‡ªåŠ¨å®‰å…¨è¯„ä¼°çš„å¤šæ™ºèƒ½ä½“(Multi-Agent)ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿç»“åˆäº†å¤§è¯­è¨€æ¨¡å‹(Large Language Model)ç¼–æ’ã€åŸºäºå·¥å…·çš„æ‰§è¡Œ(Tool-Grounded Execution)ä»¥åŠç«¯åˆ°ç«¯æ¼æ´åˆ©ç”¨éªŒè¯(End-to-End Exploit Validation)ã€‚åœ¨XBOWåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMAPTAå®ç°äº†76.9%çš„æ€»æˆåŠŸç‡ï¼Œå…¶ä¸­åœ¨SSRFå’Œé…ç½®é”™è¯¯(Misconfiguration)æ–¹é¢è¡¨ç°å®Œç¾ï¼Œå¹¶åœ¨è¶Šæƒè®¿é—®(Broken Authorization)å’ŒSQLæ³¨å…¥(SQL Injection)ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœã€‚æˆæœ¬åˆ†ææ˜¾ç¤ºï¼Œå…¶æˆåŠŸå°è¯•çš„ä¸­ä½æˆæœ¬ä»…ä¸º0.073ç¾å…ƒï¼Œå±•ç°äº†æé«˜çš„èµ„æºæ•ˆç‡ã€‚åœ¨å¯¹çƒ­é—¨å¼€æºé¡¹ç›®çš„å®é™…æ£€æµ‹ä¸­ï¼ŒMAPTAæˆåŠŸå‘ç°äº†è¿œç¨‹ä»£ç æ‰§è¡Œ(RCE)ã€å‘½ä»¤æ³¨å…¥å’Œç§˜å¯†ä¿¡æ¯æ³„éœ²ç­‰å…³é”®æ¼æ´ï¼Œç›®å‰å·²æœ‰10é¡¹å‘ç°æ­£åœ¨è¿›è¡ŒCVEå®¡æŸ¥ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨å¤šæ™ºèƒ½ä½“æ¡†æ¶å®ç°ä½æˆæœ¬ã€å¯æ‰©å±•çš„è‡ªåŠ¨åŒ–æ¸—é€æµ‹è¯•çš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20816v1",
      "published_date": "2025-08-28 14:14:24 UTC",
      "updated_date": "2025-08-28 14:14:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:51:05.188607+00:00"
    },
    {
      "arxiv_id": "2508.20812v1",
      "title": "Uncertainty Aware-Predictive Control Barrier Functions: Safer Human Robot Interaction through Probabilistic Motion Forecasting",
      "title_zh": "ä¸ç¡®å®šæ€§æ„ŸçŸ¥é¢„æµ‹æ§åˆ¶éšœç¢å‡½æ•°ï¼šé€šè¿‡æ¦‚ç‡è¿åŠ¨é¢„æµ‹å®ç°æ›´å®‰å…¨çš„äººæœºäº¤äº’",
      "authors": [
        "Lorenzo Busellato",
        "Federico Cunico",
        "Diego Dall'Alba",
        "Marco Emporio",
        "Andrea Giachetti",
        "Riccardo Muradore",
        "Marco Cristani"
      ],
      "abstract": "To enable flexible, high-throughput automation in settings where people and robots share workspaces, collaborative robotic cells must reconcile stringent safety guarantees with the need for responsive and effective behavior. A dynamic obstacle is the stochastic, task-dependent variability of human motion: when robots fall back on purely reactive or worst-case envelopes, they brake unnecessarily, stall task progress, and tamper with the fluidity that true Human-Robot Interaction demands. In recent years, learning-based human-motion prediction has rapidly advanced, although most approaches produce worst-case scenario forecasts that often do not treat prediction uncertainty in a well-structured way, resulting in over-conservative planning algorithms, limiting their flexibility. We introduce Uncertainty-Aware Predictive Control Barrier Functions (UA-PCBFs), a unified framework that fuses probabilistic human hand motion forecasting with the formal safety guarantees of Control Barrier Functions. In contrast to other variants, our framework allows for dynamic adjustment of the safety margin thanks to the human motion uncertainty estimation provided by a forecasting module. Thanks to uncertainty estimation, UA-PCBFs empower collaborative robots with a deeper understanding of future human states, facilitating more fluid and intelligent interactions through informed motion planning. We validate UA-PCBFs through comprehensive real-world experiments with an increasing level of realism, including automated setups (to perform exactly repeatable motions) with a robotic hand and direct human-robot interactions (to validate promptness, usability, and human confidence). Relative to state-of-the-art HRI architectures, UA-PCBFs show better performance in task-critical metrics, significantly reducing the number of violations of the robot's safe space during interaction with respect to the state-of-the-art.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººæœºåä½œ(Human-Robot Interaction)ä¸­äººç±»è¿åŠ¨çš„éšæœºæ€§å¯¼è‡´æœºå™¨äººé¿éšœè¿‡äºä¿å®ˆã€ä»»åŠ¡è¿›åº¦åœæ»çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸ç¡®å®šæ€§æ„ŸçŸ¥é¢„æµ‹æ§åˆ¶å±éšœå‡½æ•°(Uncertainty-Aware Predictive Control Barrier Functions, UA-PCBFs)è¿™ä¸€ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æ¦‚ç‡æ€§çš„æ‰‹éƒ¨è¿åŠ¨é¢„æµ‹ä¸å…·æœ‰å½¢å¼åŒ–å®‰å…¨ä¿è¯çš„æ§åˆ¶å±éšœå‡½æ•°(Control Barrier Functions)ç›¸ç»“åˆï¼Œå®ç°äº†æ ¹æ®é¢„æµ‹æ¨¡å—æä¾›çš„ä¸ç¡®å®šæ€§ä¼°ç®—åŠ¨æ€è°ƒæ•´å®‰å…¨è£•åº¦çš„åŠŸèƒ½ã€‚UA-PCBFs èµ‹äºˆåä½œæœºå™¨äººæ›´æ·±å±‚æ¬¡ç†è§£äººç±»æœªæ¥çŠ¶æ€çš„èƒ½åŠ›ï¼Œä»è€Œé€šè¿‡ä¿¡æ¯å……åˆ†çš„è¿åŠ¨è§„åˆ’ä¿ƒè¿›æ›´æµç•…ã€æ›´æ™ºèƒ½çš„äº¤äº’ã€‚ç ”ç©¶é€šè¿‡åŒ…æ‹¬æœºå™¨äººæ‰‹è‡ªåŠ¨åŒ–è®¾ç½®å’Œç›´æ¥äººæœºäº¤äº’åœ¨å†…çš„å¤šé¡¹çœŸå®ä¸–ç•Œå®éªŒå¯¹æ¡†æ¶è¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„HRIæ¶æ„ç›¸æ¯”ï¼ŒUA-PCBFsåœ¨ä»»åŠ¡å…³é”®æŒ‡æ ‡ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œæ˜¾è‘—å‡å°‘äº†äº¤äº’è¿‡ç¨‹ä¸­è¿åæœºå™¨äººå®‰å…¨ç©ºé—´çš„æ¬¡æ•°ï¼Œåœ¨ä¿è¯å®‰å…¨çš„åŒæ—¶æå‡äº†è‡ªåŠ¨åŒ–ç³»ç»Ÿçš„çµæ´»æ€§ä¸ååé‡ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20812v1",
      "published_date": "2025-08-28 14:11:26 UTC",
      "updated_date": "2025-08-28 14:11:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:51:22.252684+00:00"
    },
    {
      "arxiv_id": "2508.20810v1",
      "title": "A Graph-Based Test-Harness for LLM Evaluation",
      "title_zh": "ä¸€ç§åŸºäºå›¾çš„å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°æµ‹è¯•æ¡†æ¶",
      "authors": [
        "Jessica Lundin",
        "Guillaume Chabot-Couture"
      ],
      "abstract": "We present a first known prototype of a dynamic, systematic benchmark of medical guidelines for 400+ questions, with 3.3+ trillion possible combinations, covering 100\\% of guideline relationships. We transformed the WHO IMCI handbook into a directed graph with 200+ nodes (conditions, symptoms, treatments, follow-ups, severities) and 300+ edges, then used graph traversal to generate questions that incorporated age-specific scenarios and contextual distractors to ensure clinical relevance. Our graph-based approach enables systematic evaluation across clinical tasks (45-67\\% accuracy), and we find models excel at symptom recognition but struggle with triaging severity, treatment protocols and follow-up care, demonstrating how customized benchmarks can identify specific capability gaps that general-domain evaluations miss. Beyond evaluation, this dynamic MCQA methodology enhances LLM post-training (supervised finetuning, GRPO, DPO), where correct answers provide high-reward samples without expensive human annotation. The graph-based approach successfully addresses the coverage limitations of manually curated benchmarks. This methodology is a step toward scalable, contamination-resistant solution for creating comprehensive benchmarks that can be dynamically generated, including when the guidelines are updated. Code and datasets are available at https://github.com/jessicalundin/graph_testing_harness",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºå›¾ï¼ˆgraph-basedï¼‰çš„ LLM è¯„ä¼°æµ‹è¯•æ¡†æ¶ï¼Œæ˜¯å·²çŸ¥é¦–ä¸ªé’ˆå¯¹åŒ»å­¦æŒ‡å—çš„åŠ¨æ€ã€ç³»ç»ŸåŒ–åŸºå‡†æµ‹è¯•åŸå‹ï¼Œæ¶µç›–äº† 400 å¤šä¸ªé—®é¢˜åŠè¶…è¿‡ 3.3 ä¸‡äº¿ç§å¯èƒ½çš„ç»„åˆã€‚ç ”ç©¶å›¢é˜Ÿå°† WHO IMCI æ‰‹å†Œè½¬åŒ–ä¸ºåŒ…å« 200 å¤šä¸ªèŠ‚ç‚¹å’Œ 300 å¤šæ¡è¾¹çš„æœ‰å‘å›¾ï¼ˆdirected graphï¼‰ï¼Œé€šè¿‡å›¾éå†ç”ŸæˆåŒ…å«ç‰¹å®šåœºæ™¯å’Œä¸Šä¸‹æ–‡å¹²æ‰°é¡¹çš„é—®é¢˜ï¼Œç¡®ä¿äº† 100% çš„æŒ‡å—å…³ç³»è¦†ç›–ç‡ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨ç—‡çŠ¶è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç—…æƒ…ä¸¥é‡ç¨‹åº¦åˆ†è¯Šï¼ˆtriaging severityï¼‰ã€æ²»ç–—åè®®å’Œéšè®¿æŠ¤ç†æ–¹é¢å­˜åœ¨æ˜æ˜¾å›°éš¾ï¼Œè¯æ˜äº†å®šåˆ¶åŒ–åŸºå‡†åœ¨è¯†åˆ«ç‰¹å®šèƒ½åŠ›ç¼ºå£æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè¯¥åŠ¨æ€ MCQA æ–¹æ³•äº§ç”Ÿçš„æ­£ç¡®ç­”æ¡ˆå¯ä½œä¸ºé«˜å¥–åŠ±æ ·æœ¬ï¼Œç”¨äºå¢å¼º LLM çš„åæœŸè®­ç»ƒï¼ˆå¦‚ supervised finetuningã€GRPO å’Œ DPOï¼‰ï¼Œä¸”æ— éœ€æ˜‚è´µçš„äººå·¥æ ‡æ³¨ã€‚è¯¥æ–¹æ³•æˆåŠŸè§£å†³äº†äººå·¥æ„å»ºåŸºå‡†çš„è¦†ç›–èŒƒå›´å±€é™æ€§ï¼Œä¸ºåˆ›å»ºå¯æ‰©å±•ã€æŠ—æ±¡æŸ“ï¼ˆcontamination-resistantï¼‰ä¸”å¯éšæŒ‡å—åŠ¨æ€æ›´æ–°çš„å…¨é¢åŸºå‡†æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "4 pages, 2 figures, dataset",
      "pdf_url": "https://arxiv.org/pdf/2508.20810v1",
      "published_date": "2025-08-28 14:10:59 UTC",
      "updated_date": "2025-08-28 14:10:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:51:29.006926+00:00"
    },
    {
      "arxiv_id": "2508.20805v1",
      "title": "Exploring Machine Learning and Language Models for Multimodal Depression Detection",
      "title_zh": "æ¢è®¨ç”¨äºå¤šæ¨¡æ€æŠ‘éƒç—‡æ£€æµ‹çš„æœºå™¨å­¦ä¹ ä¸è¯­è¨€æ¨¡å‹",
      "authors": [
        "Javier Si Zhao Hong",
        "Timothy Zoe Delaya",
        "Sherwyn Chan Yin Kit",
        "Pai Chet Ng",
        "Xiaoxiao Miao"
      ],
      "abstract": "This paper presents our approach to the first Multimodal Personality-Aware Depression Detection Challenge, focusing on multimodal depression detection using machine learning and deep learning models. We explore and compare the performance of XGBoost, transformer-based architectures, and large language models (LLMs) on audio, video, and text features. Our results highlight the strengths and limitations of each type of model in capturing depression-related signals across modalities, offering insights into effective multimodal representation strategies for mental health prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é¦–å±Šå¤šæ¨¡æ€äººæ ¼æ„ŸçŸ¥æŠ‘éƒæ£€æµ‹æŒ‘æˆ˜èµ›ï¼ˆMultimodal Personality-Aware Depression Detection Challengeï¼‰ï¼Œæå‡ºå¹¶å¯¹æ¯”äº†åˆ©ç”¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ è¿›è¡Œå¤šæ¨¡æ€æŠ‘éƒæ£€æµ‹çš„å¤šç§æ–¹æ¡ˆã€‚ä½œè€…æ·±å…¥æ¢è®¨äº† XGBoostã€åŸºäº Transformer çš„æ¶æ„ä»¥åŠå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†éŸ³é¢‘ã€è§†é¢‘å’Œæ–‡æœ¬ç‰¹å¾æ—¶çš„æ€§èƒ½å·®å¼‚ã€‚ç ”ç©¶ç»“æœè¯¦ç»†åˆ†æäº†æ¯ç§æ¨¡å‹åœ¨æ•æ‰è·¨æ¨¡æ€æŠ‘éƒç›¸å…³ä¿¡å·æ–¹é¢çš„ä¼˜åŠ¿ä¸å±€é™æ€§ã€‚æ­¤é¡¹å·¥ä½œä¸ºç²¾ç¥å¥åº·é¢„æµ‹é¢†åŸŸä¸­å¤šæ¨¡æ€è¡¨ç¤ºç­–ç•¥ï¼ˆmultimodal representation strategiesï¼‰çš„æœ‰æ•ˆæ„å»ºæä¾›äº†é‡è¦è§è§£ï¼Œæœ‰åŠ©äºæ¨åŠ¨æ›´ç²¾å‡†çš„è‡ªåŠ¨åŒ–æŠ‘éƒæ£€æµ‹æŠ€æœ¯çš„å‘å±•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper has been accepted by APCIPA ASC 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.20805v1",
      "published_date": "2025-08-28 14:07:07 UTC",
      "updated_date": "2025-08-28 14:07:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:51:14.490800+00:00"
    },
    {
      "arxiv_id": "2508.20796v1",
      "title": "Speech Emotion Recognition via Entropy-Aware Score Selection",
      "title_zh": "åŸºäºç†µæ„ŸçŸ¥åˆ†æ•°é€‰æ‹©çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«",
      "authors": [
        "ChenYi Chua",
        "JunKai Wong",
        "Chengxin Chen",
        "Xiaoxiao Miao"
      ],
      "abstract": "In this paper, we propose a multimodal framework for speech emotion recognition that leverages entropy-aware score selection to combine speech and textual predictions. The proposed method integrates a primary pipeline that consists of an acoustic model based on wav2vec2.0 and a secondary pipeline that consists of a sentiment analysis model using RoBERTa-XLM, with transcriptions generated via Whisper-large-v3. We propose a late score fusion approach based on entropy and varentropy thresholds to overcome the confidence constraints of primary pipeline predictions. A sentiment mapping strategy translates three sentiment categories into four target emotion classes, enabling coherent integration of multimodal predictions. The results on the IEMOCAP and MSP-IMPROV datasets show that the proposed method offers a practical and reliable enhancement over traditional single-modality systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«(Speech Emotion Recognition)çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œé€šè¿‡ä¿¡æ¯ç†µæ„ŸçŸ¥(entropy-aware)çš„åˆ†æ•°é€‰æ‹©æœºåˆ¶æœ‰æ•ˆæ•´åˆäº†è¯­éŸ³å’Œæ–‡æœ¬é¢„æµ‹ã€‚è¯¥ç³»ç»Ÿç”±åŸºäºwav2vec2.0çš„å£°å­¦æ¨¡å‹æµæ°´çº¿å’ŒåŸºäºRoBERTa-XLMçš„æƒ…æ„Ÿåˆ†ææµæ°´çº¿ç»„æˆï¼Œå¹¶åˆ©ç”¨Whisper-large-v3ç”Ÿæˆæ–‡æœ¬è½¬å½•ã€‚ä¸ºäº†å…‹æœå•ä¸€æ¨¡å‹é¢„æµ‹çš„ç½®ä¿¡åº¦é™åˆ¶ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºä¿¡æ¯ç†µ(entropy)å’Œå˜å¼‚ç†µ(varentropy)é˜ˆå€¼çš„åæœŸåˆ†æ•°èåˆ(late score fusion)ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é€šè¿‡æƒ…æ„Ÿæ˜ å°„ç­–ç•¥å°†æƒ…æ„Ÿç±»åˆ«è½¬åŒ–ä¸ºç›®æ ‡æƒ…ç»ªç±»åˆ«ï¼Œå®ç°äº†ä¸åŒæ¨¡æ€é—´çš„è¿è´¯æ•´åˆã€‚åœ¨IEMOCAPå’ŒMSP-IMPROVæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿçš„å•æ¨¡æ€ç³»ç»Ÿå…·æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸ºå¤šæ¨¡æ€è¯­éŸ³è¯†åˆ«æä¾›äº†æ›´å…·å®ç”¨æ€§å’Œå¯é æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "The paper has been accepted by APCIPA ASC 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.20796v1",
      "published_date": "2025-08-28 13:58:09 UTC",
      "updated_date": "2025-08-28 13:58:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:51:21.961812+00:00"
    },
    {
      "arxiv_id": "2508.20789v1",
      "title": "Surfel-based 3D Registration with Equivariant SE(3) Features",
      "title_zh": "åŸºäºç­‰å˜ SE(3) ç‰¹å¾çš„é¢å…ƒä¸‰ç»´é…å‡†",
      "authors": [
        "Xueyang Kang",
        "Hang Zhao",
        "Kourosh Khoshelham",
        "Patrick Vandewalle"
      ],
      "abstract": "Point cloud registration is crucial for ensuring 3D alignment consistency of multiple local point clouds in 3D reconstruction for remote sensing or digital heritage. While various point cloud-based registration methods exist, both non-learning and learning-based, they ignore point orientations and point uncertainties, making the model susceptible to noisy input and aggressive rotations of the input point cloud like orthogonal transformation; thus, it necessitates extensive training point clouds with transformation augmentations. To address these issues, we propose a novel surfel-based pose learning regression approach. Our method can initialize surfels from Lidar point cloud using virtual perspective camera parameters, and learns explicit $\\mathbf{SE(3)}$ equivariant features, including both position and rotation through $\\mathbf{SE(3)}$ equivariant convolutional kernels to predict relative transformation between source and target scans. The model comprises an equivariant convolutional encoder, a cross-attention mechanism for similarity computation, a fully-connected decoder, and a non-linear Huber loss. Experimental results on indoor and outdoor datasets demonstrate our model superiority and robust performance on real point-cloud scans compared to state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºé¢å…ƒ (Surfel) çš„ä½å§¿å­¦ä¹ å›å½’æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç‚¹äº‘é…å‡† (Point cloud registration) åœ¨å¤„ç†å™ªå£°è¾“å…¥å’Œå¤§è§’åº¦æ—‹è½¬æ—¶ï¼Œå› å¿½ç•¥ç‚¹æœå‘å’Œä¸ç¡®å®šæ€§è€Œå¯¼è‡´çš„é²æ£’æ€§ä¸è¶³é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è™šæ‹Ÿé€è§†ç›¸æœºå‚æ•°ä» LiDAR ç‚¹äº‘ä¸­åˆå§‹åŒ– Surfelï¼Œå¹¶é‡‡ç”¨ SE(3) ç­‰å˜å·ç§¯æ ¸ (Equivariant convolutional kernels) æ˜¾å¼å­¦ä¹ åŒ…å«ä½ç½®ä¸æ—‹è½¬ä¿¡æ¯çš„ SE(3) ç­‰å˜ç‰¹å¾ï¼Œä»è€Œç²¾ç¡®é¢„æµ‹æºæ‰«æä¸ç›®æ ‡æ‰«æä¹‹é—´çš„ç›¸å¯¹å˜æ¢ã€‚æ¨¡å‹æ¶æ„ç”±ç­‰å˜å·ç§¯ç¼–ç å™¨ã€ç”¨äºç›¸ä¼¼æ€§è®¡ç®—çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ (Cross-attention) ä»¥åŠå…¨è¿æ¥è§£ç å™¨ç»„æˆï¼Œå¹¶ä½¿ç”¨éçº¿æ€§ Huber loss è¿›è¡Œä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å®¤å†…å¤–çœŸå®ç‚¹äº‘æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå½“å‰çš„å…ˆè¿›æ–¹æ³• (State-of-the-art)ï¼Œåœ¨æ— éœ€å¤§è§„æ¨¡å˜æ¢å¢å¼ºè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä¾ç„¶å±•ç°å‡ºå“è¶Šçš„é…å‡†ç²¾åº¦ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.20789v1",
      "published_date": "2025-08-28 13:53:44 UTC",
      "updated_date": "2025-08-28 13:53:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:51:34.392979+00:00"
    },
    {
      "arxiv_id": "2508.20784v1",
      "title": "Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control",
      "title_zh": "é¢å‘å…¬äº¤è½¦é˜Ÿæ§åˆ¶çš„å•æ™ºèƒ½ä½“é²æ£’æ·±åº¦å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Yifan Zhang"
      ],
      "abstract": "Bus bunching remains a challenge for urban transit due to stochastic traffic and passenger demand. Traditional solutions rely on multi-agent reinforcement learning (MARL) in loop-line settings, which overlook realistic operations characterized by heterogeneous routes, timetables, fluctuating demand, and varying fleet sizes. We propose a novel single-agent reinforcement learning (RL) framework for bus holding control that avoids the data imbalance and convergence issues of MARL under near-realistic simulation. A bidirectional timetabled network with dynamic passenger demand is constructed. The key innovation is reformulating the multi-agent problem into a single-agent one by augmenting the state space with categorical identifiers (vehicle ID, station ID, time period) in addition to numerical features (headway, occupancy, velocity). This high-dimensional encoding enables single-agent policies to capture inter-agent dependencies, analogous to projecting non-separable inputs into a higher-dimensional space. We further design a structured reward function aligned with operational goals: instead of exponential penalties on headway deviations, a ridge-shaped reward balances uniform headways and schedule adherence. Experiments show that our modified soft actor-critic (SAC) achieves more stable and superior performance than benchmarks, including MADDPG (e.g., -430k vs. -530k under stochastic conditions). These results demonstrate that single-agent deep RL, when enhanced with categorical structuring and schedule-aware rewards, can effectively manage bus holding in non-loop, real-world contexts. This paradigm offers a robust, scalable alternative to MARL frameworks, particularly where agent-specific experiences are imbalanced.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸå¸‚äº¤é€šä¸­ç”±éšæœºäº¤é€šå’Œä¹˜å®¢éœ€æ±‚å¼•èµ·çš„å…¬äº¤ä¸²è½¦(bus bunching)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„å•æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (single-agent RL)æ¡†æ¶ï¼Œç”¨äºå…¬äº¤é©»ç•™æ§åˆ¶ã€‚ä¸ä¼ ç»Ÿçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (MARL)æ–¹æ³•ä¸åŒï¼Œè¯¥æ¡†æ¶é€šè¿‡åœ¨çŠ¶æ€ç©ºé—´ä¸­å¢åŠ ç±»åˆ«æ ‡è¯†ç¬¦ï¼ˆå¦‚vehicle IDã€station IDå’Œtime periodï¼‰ä»¥åŠæ•°å€¼ç‰¹å¾ï¼ˆå¦‚headwayã€occupancyå’Œvelocityï¼‰ï¼Œå°†å¤šæ™ºèƒ½ä½“é—®é¢˜é‡æ–°è¡¨è¿°ä¸ºå•æ™ºèƒ½ä½“é—®é¢˜ã€‚è¿™ç§é«˜ç»´ç¼–ç ä½¿å•æ™ºèƒ½ä½“ç­–ç•¥èƒ½å¤Ÿæ•æ‰æ™ºèƒ½ä½“é—´çš„ä¾èµ–å…³ç³»ï¼Œæœ‰æ•ˆè§£å†³äº†MARLåœ¨è¿‘ä¹çœŸå®æ¨¡æ‹Ÿä¸‹çš„æ•°æ®ä¸å¹³è¡¡å’Œæ”¶æ•›é—®é¢˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†è„Šå½¢å¥–åŠ±å‡½æ•°(ridge-shaped reward)ï¼Œä»¥å¹³è¡¡è½¦å¤´é—´è·å‡åŒ€æ€§å’Œæ—¶åˆ»è¡¨ä¾ä»æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ”¹è¿›åçš„Soft Actor-Critic (SAC)ç®—æ³•åœ¨æ€§èƒ½å’Œç¨³å®šæ€§ä¸Šå‡ä¼˜äºMADDPGç­‰åŸºå‡†æ¨¡å‹ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å¼ºåŒ–å•æ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨éç¯çŠ¶çº¿ã€çœŸå®åœºæ™¯ä¸‹ç®¡ç†å…¬äº¤è½¦é˜Ÿçš„æœ‰æ•ˆæ€§ï¼Œä¸ºMARLæä¾›äº†ä¸€ç§ç¨³å¥ä¸”æ›´å…·æ‰©å±•æ€§çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20784v1",
      "published_date": "2025-08-28 13:47:40 UTC",
      "updated_date": "2025-08-28 13:47:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:51:39.795084+00:00"
    },
    {
      "arxiv_id": "2508.20783v1",
      "title": "Evaluating Compositional Generalisation in VLMs and Diffusion Models",
      "title_zh": "è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹çš„ç»„åˆæ³›åŒ–èƒ½åŠ›",
      "authors": [
        "Beth Pearson",
        "Bilal Boulbarss",
        "Michael Wray",
        "Martha Lewis"
      ],
      "abstract": "A fundamental aspect of the semantics of natural language is that novel meanings can be formed from the composition of previously known parts. Vision-language models (VLMs) have made significant progress in recent years, however, there is evidence that they are unable to perform this kind of composition. For example, given an image of a red cube and a blue cylinder, a VLM such as CLIP is likely to incorrectly label the image as a red cylinder or a blue cube, indicating it represents the image as a `bag-of-words' and fails to capture compositional semantics. Diffusion models have recently gained significant attention for their impressive generative abilities, and zero-shot classifiers based on diffusion models have been shown to perform competitively with CLIP in certain compositional tasks. In this work we explore whether the generative Diffusion Classifier has improved compositional generalisation abilities compared to discriminative models. We assess three models -- Diffusion Classifier, CLIP, and ViLT -- on their ability to bind objects with attributes and relations in both zero-shot learning (ZSL) and generalised zero-shot learning (GZSL) settings. Our results show that the Diffusion Classifier and ViLT perform well at concept binding tasks, but that all models struggle significantly with the relational GZSL task, underscoring the broader challenges VLMs face with relational reasoning. Analysis of CLIP embeddings suggests that the difficulty may stem from overly similar representations of relational concepts such as left and right. Code and dataset are available at: https://github.com/otmive/diffusion_classifier_clip",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)ä¸æ‰©æ•£æ¨¡å‹åœ¨ç»„åˆæ¦‚æ‹¬(Compositional Generalisation)æ–¹é¢çš„èƒ½åŠ›ï¼Œæ—¨åœ¨è§£å†³æ¨¡å‹å¸¸å› â€œè¯è¢‹â€æ•ˆåº”è€Œæ— æ³•å‡†ç¡®æ•æ‰è¯­ä¹‰ç»„åˆçš„é—®é¢˜ã€‚ç ”ç©¶å¯¹æ¯”åˆ†æäº†ç”Ÿæˆå¼æ‰©æ•£åˆ†ç±»å™¨(Diffusion Classifier)ä¸åˆ¤åˆ«å¼æ¨¡å‹(CLIPå’ŒViLT)åœ¨é›¶æ ·æœ¬å­¦ä¹ (ZSL)å’Œå¹¿ä¹‰é›¶æ ·æœ¬å­¦ä¹ (GZSL)è®¾ç½®ä¸‹çš„è¡¨ç°ã€‚è¯„ä¼°é‡ç‚¹åœ¨äºæ¨¡å‹å°†ç‰©ä½“ä¸å±æ€§åŠå…³ç³»è¿›è¡Œæ¦‚å¿µç»‘å®š(Concept Binding)çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDiffusion Classifierå’ŒViLTåœ¨å±æ€§ç»‘å®šä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†æ‰€æœ‰æ¨¡å‹åœ¨æ¶‰åŠå…³ç³»çš„GZSLä»»åŠ¡ä¸­å‡é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œçªæ˜¾äº†VLMsåœ¨å…³ç³»æ¨ç†æ–¹é¢çš„æ™®éå±€é™æ€§ã€‚é€šè¿‡å¯¹CLIPåµŒå…¥å‘é‡çš„åˆ†æå‘ç°ï¼Œæ¨¡å‹å¯¹â€œå·¦â€å’Œâ€œå³â€ç­‰ç©ºé—´å…³ç³»æ¦‚å¿µçš„è¡¨ç¤ºè¿‡äºç›¸ä¼¼ï¼Œè¿™å¯èƒ½æ˜¯å¯¼è‡´æ€§èƒ½ç“¶é¢ˆçš„å…³é”®åŸå› ã€‚è¯¥é¡¹å·¥ä½œä¸ºè¯„ä¼°ç”Ÿæˆå¼ä¸åˆ¤åˆ«å¼æ¨¡å‹åœ¨è¯­ä¹‰ç†è§£ä¸Šçš„å·®å¼‚æä¾›äº†é‡è¦è§è§£ï¼Œå¹¶ä¸ºæå‡æ¨¡å‹ç»„åˆè¯­ä¹‰èƒ½åŠ›æŒ‡æ˜äº†ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages including references, 6 figures. Accepted at IWCS 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.20783v1",
      "published_date": "2025-08-28 13:45:04 UTC",
      "updated_date": "2025-08-28 13:45:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:51:59.290677+00:00"
    },
    {
      "arxiv_id": "2508.20776v1",
      "title": "Safer Skin Lesion Classification with Global Class Activation Probability Map Evaluation and SafeML",
      "title_zh": "åŸºäºå…¨å±€ç±»åˆ«æ¿€æ´»æ¦‚ç‡å›¾è¯„ä¼°ä¸ SafeML çš„æ›´å®‰å…¨çš®è‚¤ç—…å˜åˆ†ç±»",
      "authors": [
        "Kuniko Paxton",
        "Koorosh Aslansefat",
        "Amila AkagiÄ‡",
        "Dhavalkumar Thakker",
        "Yiannis Papadopoulos"
      ],
      "abstract": "Recent advancements in skin lesion classification models have significantly improved accuracy, with some models even surpassing dermatologists' diagnostic performance. However, in medical practice, distrust in AI models remains a challenge. Beyond high accuracy, trustworthy, explainable diagnoses are essential. Existing explainability methods have reliability issues, with LIME-based methods suffering from inconsistency, while CAM-based methods failing to consider all classes. To address these limitations, we propose Global Class Activation Probabilistic Map Evaluation, a method that analyses all classes' activation probability maps probabilistically and at a pixel level. By visualizing the diagnostic process in a unified manner, it helps reduce the risk of misdiagnosis. Furthermore, the application of SafeML enhances the detection of false diagnoses and issues warnings to doctors and patients as needed, improving diagnostic reliability and ultimately patient safety. We evaluated our method using the ISIC datasets with MobileNetV2 and Vision Transformers.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çš®è‚¤ç—…å˜åˆ†ç±»(skin lesion classification)æ¨¡å‹åœ¨å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºGlobal Class Activation Probabilistic Map Evaluationçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³LIMEæ–¹æ³•çš„ä¸ä¸€è‡´æ€§ä»¥åŠCAMç±»æ–¹æ³•æ— æ³•è¦†ç›–æ‰€æœ‰ç±»åˆ«çš„å±€é™ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨åƒç´ çº§åˆ«å¯¹æ‰€æœ‰ç±»åˆ«çš„æ¿€æ´»æ¦‚ç‡å›¾è¿›è¡Œæ¦‚ç‡åˆ†æï¼Œå®ç°äº†è¯Šæ–­è¿‡ç¨‹çš„ç»Ÿä¸€å¯è§†åŒ–ï¼Œä»è€Œæœ‰æ•ˆé™ä½è¯¯è¯Šé£é™©ã€‚åŒæ—¶ï¼Œç ”ç©¶ç»“åˆäº†SafeMLæŠ€æœ¯æ¥å¢å¼ºå¯¹é”™è¯¯è¯Šæ–­çš„æ£€æµ‹ï¼Œå¹¶èƒ½åœ¨å¿…è¦æ—¶å‘åŒ»ç”ŸåŠæ‚£è€…å‘å‡ºè­¦ç¤ºï¼Œæ˜¾è‘—æå‡äº†åŒ»ç–—è¯Šæ–­çš„å¯é æ€§å’Œæ‚£è€…å®‰å…¨æ€§ã€‚é€šè¿‡åœ¨ISICæ•°æ®é›†ä¸Šåˆ©ç”¨MobileNetV2å’ŒVision Transformersè¿›è¡Œè¯„ä¼°ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æ„å»ºå¯è§£é‡Šã€é«˜å¯é æ€§åŒ»ç–—è¾…åŠ©è¯Šæ–­ç³»ç»Ÿæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20776v1",
      "published_date": "2025-08-28 13:32:35 UTC",
      "updated_date": "2025-08-28 13:32:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:52:03.894822+00:00"
    },
    {
      "arxiv_id": "2508.20773v1",
      "title": "Unleashing Uncertainty: Efficient Machine Unlearning for Generative AI",
      "title_zh": "é‡Šæ”¾ä¸ç¡®å®šæ€§ï¼šé¢å‘ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„é«˜æ•ˆæœºå™¨é—å¿˜",
      "authors": [
        "Christoforos N. Spartalis",
        "Theodoros Semertzidis",
        "Petros Daras",
        "Efstratios Gavves"
      ],
      "abstract": "We introduce SAFEMax, a novel method for Machine Unlearning in diffusion models. Grounded in information-theoretic principles, SAFEMax maximizes the entropy in generated images, causing the model to generate Gaussian noise when conditioned on impermissible classes by ultimately halting its denoising process. Also, our method controls the balance between forgetting and retention by selectively focusing on the early diffusion steps, where class-specific information is prominent. Our results demonstrate the effectiveness of SAFEMax and highlight its substantial efficiency gains over state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SAFEMaxï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ‰©æ•£æ¨¡å‹(diffusion models)è®¾è®¡çš„æ–°å‹æœºå™¨å¸è½½(Machine Unlearning)æ–¹æ³•ã€‚è¯¥æ–¹æ³•åŸºäºä¿¡æ¯è®ºåŸç†ï¼Œé€šè¿‡æœ€å¤§åŒ–ç”Ÿæˆå›¾åƒçš„ç†µï¼Œå¼•å¯¼æ¨¡å‹åœ¨æ¥æ”¶åˆ°å—é™ç±»åˆ«æŒ‡ä»¤æ—¶åœæ­¢å»å™ªè¿‡ç¨‹å¹¶äº§ç”Ÿé«˜æ–¯å™ªå£°(Gaussian noise)ã€‚SAFEMaxé€šè¿‡é’ˆå¯¹æ€§åœ°è°ƒèŠ‚ç±»åˆ«ä¿¡æ¯æœ€ä¸ºæ˜¾è‘—çš„æ—©æœŸæ‰©æ•£æ­¥éª¤(early diffusion steps)ï¼Œå®ç°äº†é—å¿˜ç‰¹å®šå†…å®¹ä¸ä¿ç•™é€šç”¨çŸ¥è¯†ä¹‹é—´çš„ç²¾ç¡®å¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAFEMaxä¸ä»…èƒ½æœ‰æ•ˆå®Œæˆå¸è½½ä»»åŠ¡ï¼Œä¸”åœ¨è¿è¡Œæ•ˆç‡ä¸Šç›¸è¾ƒäºç›®å‰æœ€å…ˆè¿›æ–¹æ³•(state-of-the-art methods)å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2025 workshop on Machine Unlearning for Generative AI",
      "pdf_url": "https://arxiv.org/pdf/2508.20773v1",
      "published_date": "2025-08-28 13:29:21 UTC",
      "updated_date": "2025-08-28 13:29:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:52:10.784654+00:00"
    },
    {
      "arxiv_id": "2508.20771v1",
      "title": "Signs of Struggle: Spotting Cognitive Distortions across Language and Register",
      "title_zh": "æŒ£æ‰çš„è¿¹è±¡ï¼šè·¨è¯­è¨€ä¸è¯­ä½“çš„è®¤çŸ¥æ‰­æ›²è¯†åˆ«",
      "authors": [
        "Abhishek Kuber",
        "Enrico Liscio",
        "Ruixuan Zhang",
        "Caroline Figueroa",
        "Pradeep K. Murukannaiah"
      ],
      "abstract": "Rising mental health issues among youth have increased interest in automated approaches for detecting early signs of psychological distress in digital text. One key focus is the identification of cognitive distortions, irrational thought patterns that have a role in aggravating mental distress. Early detection of these distortions may enable timely, low-cost interventions. While prior work has focused on English clinical data, we present the first in-depth study of cross-lingual and cross-register generalization of cognitive distortion detection, analyzing forum posts written by Dutch adolescents. Our findings show that while changes in language and writing style can significantly affect model performance, domain adaptation methods show the most promise.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é’å°‘å¹´å¿ƒç†å¥åº·é—®é¢˜ï¼Œæ¢è®¨äº†åœ¨æ•°å­—æ–‡æœ¬ä¸­è‡ªåŠ¨è¯†åˆ«è®¤çŸ¥æ‰­æ›²(Cognitive Distortions)è¿™ä¸€éç†æ€§æ€ç»´æ¨¡å¼çš„æ–¹æ³•ã€‚ä»¥å¾€ç ”ç©¶ä¸»è¦é›†ä¸­äºè‹±æ–‡ä¸´åºŠæ•°æ®ï¼Œè€Œæœ¬é¡¹å·¥ä½œé¦–æ¬¡æ·±å…¥ç ”ç©¶äº†è®¤çŸ¥æ‰­æ›²æ£€æµ‹åœ¨è·¨è¯­è¨€(cross-lingual)å’Œè·¨è¯­åŸŸ(cross-register)ç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¯¹è·å…°é’å°‘å¹´æ’°å†™çš„è®ºå›å¸–å­è¿›è¡Œäº†å®è¯åˆ†æã€‚ç ”ç©¶å‘ç°ï¼Œè¯­è¨€å’Œå†™ä½œé£æ ¼çš„è½¬å˜ä¼šæ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½ï¼Œç»™è‡ªåŠ¨åŒ–æ£€æµ‹å¸¦æ¥æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¯æ˜ï¼Œé¢†åŸŸè‡ªé€‚åº”(Domain Adaptation)æ–¹æ³•åœ¨è§£å†³è·¨é¢†åŸŸæ£€æµ‹é—®é¢˜ä¸Šå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°ä½æˆæœ¬ä¸”åŠæ—¶çš„å¿ƒç†å¥åº·å¹²é¢„æä¾›äº†é‡è¦å‚è€ƒï¼Œæ¨åŠ¨äº†è·¨è¯­è¨€å¿ƒç†å›°æ‰°è‡ªåŠ¨è¯†åˆ«æŠ€æœ¯çš„å‘å±•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20771v1",
      "published_date": "2025-08-28 13:28:07 UTC",
      "updated_date": "2025-08-28 13:28:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:52:11.583643+00:00"
    },
    {
      "arxiv_id": "2508.20766v1",
      "title": "Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection",
      "title_zh": "é€†è½¬ä¹¾å¤ï¼šåŸºäºç§©ä¸€å®‰å…¨æ€§æ³¨å…¥çš„è½»é‡çº§å¯¹é½å¢å¼º",
      "authors": [
        "Harethah Abu Shairah",
        "Hasan Abed Al Kader Hammoud",
        "George Turkiyyah",
        "Bernard Ghanem"
      ],
      "abstract": "Safety alignment in Large Language Models (LLMs) often involves mediating internal representations to refuse harmful requests. Recent research has demonstrated that these safety mechanisms can be bypassed by ablating or removing specific representational directions within the model. In this paper, we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box method that amplifies a model's safety alignment by permanently steering its activations toward the refusal-mediating subspace. ROSI operates as a simple, fine-tuning-free rank-one weight modification applied to all residual stream write matrices. The required safety direction can be computed from a small set of harmful and harmless instruction pairs. We show that ROSI consistently increases safety refusal rates - as evaluated by Llama Guard 3 - while preserving the utility of the model on standard benchmarks such as MMLU, HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align 'uncensored' models by amplifying their own latent safety directions, demonstrating its utility as an effective last-mile safety procedure. Our results suggest that targeted, interpretable weight steering is a cheap and potent mechanism to improve LLM safety, complementing more resource-intensive fine-tuning paradigms.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ROSI (Rank-One Safety Injection)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºå¤§è¯­è¨€æ¨¡å‹(LLMs)å®‰å…¨å¯¹é½çš„ç™½ç›’æ–¹æ³•ã€‚ä¸ä»¥å¾€é€šè¿‡æ¶ˆèç‰¹å®šè¡¨ç¤ºæ–¹å‘æ¥ç»•è¿‡å®‰å…¨æœºåˆ¶çš„ç ”ç©¶ç›¸åï¼ŒROSIé€šè¿‡ä¸€ç§æ— éœ€å¾®è°ƒçš„ç§©ä¸€æƒé‡ä¿®æ”¹(rank-one weight modification)æŠ€æœ¯ï¼Œå°†æ¨¡å‹çš„æ¿€æ´»æ°¸ä¹…å¯¼å‘æ‹’ç»ä»‹å¯¼å­ç©ºé—´(refusal-mediating subspace)ã€‚è¯¥æ–¹æ³•ä»…éœ€é€šè¿‡å°‘é‡çš„æœ‰å®³ä¸æ— å®³æŒ‡ä»¤å¯¹å³å¯è®¡ç®—å‡ºæ‰€éœ€çš„å®‰å…¨æ€§æ–¹å‘ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ‰€æœ‰æ®‹å·®æµå†™å…¥çŸ©é˜µã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒROSIåœ¨Llama Guard 3è¯„ä¼°ä¸­æ˜¾è‘—æé«˜äº†å®‰å…¨æ‹’ç»ç‡ï¼ŒåŒæ—¶åœ¨MMLUã€HellaSwagå’ŒArcç­‰æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†æ¨¡å‹çš„åŸæœ‰æ•ˆç”¨ã€‚æ­¤å¤–ï¼ŒROSIè¿˜èƒ½é€šè¿‡æ”¾å¤§â€œæœªå®¡æŸ¥â€æ¨¡å‹å†…éƒ¨æ½œè—çš„å®‰å…¨æ–¹å‘å®ç°é‡æ–°å¯¹é½ï¼Œè¯æ˜äº†è¿™ç§æœ‰é’ˆå¯¹æ€§ä¸”å¯è§£é‡Šçš„æƒé‡å¯¼å‘(weight steering)æ˜¯æå‡æ¨¡å‹å®‰å…¨æ€§çš„ä¸€ç§ä½æˆæœ¬ä¸”å¼ºå¤§çš„æœºåˆ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Under Review",
      "pdf_url": "https://arxiv.org/pdf/2508.20766v1",
      "published_date": "2025-08-28 13:22:33 UTC",
      "updated_date": "2025-08-28 13:22:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:52:22.439150+00:00"
    },
    {
      "arxiv_id": "2508.20765v1",
      "title": "Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding",
      "title_zh": "è¶…è¶Šè¡¨è±¡ï¼šè§†é¢‘ç†è§£ä¸­çš„æŠ½è±¡æ¦‚å¿µè¯†åˆ«ç»¼è¿°",
      "authors": [
        "Gowreesh Mago",
        "Pascal Mettes",
        "Stevan Rudinac"
      ],
      "abstract": "The automatic understanding of video content is advancing rapidly. Empowered by deeper neural networks and large datasets, machines are increasingly capable of understanding what is concretely visible in video frames, whether it be objects, actions, events, or scenes. In comparison, humans retain a unique ability to also look beyond concrete entities and recognize abstract concepts like justice, freedom, and togetherness. Abstract concept recognition forms a crucial open challenge in video understanding, where reasoning on multiple semantic levels based on contextual information is key. In this paper, we argue that the recent advances in foundation models make for an ideal setting to address abstract concept understanding in videos. Automated understanding of high-level abstract concepts is imperative as it enables models to be more aligned with human reasoning and values. In this survey, we study different tasks and datasets used to understand abstract concepts in video content. We observe that, periodically and over a long period, researchers have attempted to solve these tasks, making the best use of the tools available at their disposal. We advocate that drawing on decades of community experience will help us shed light on this important open grand challenge and avoid ``re-inventing the wheel'' as we start revisiting it in the era of multi-modal foundation models.",
      "tldr_zh": "è¯¥ç»¼è¿°æ¢è®¨äº†è§†é¢‘ç†è§£(Video Understanding)é¢†åŸŸä¸­ä¸€é¡¹å…³é”®æŒ‘æˆ˜ï¼Œå³å¦‚ä½•è¶…è¶Šå…·è±¡ç‰©ä½“è¯†åˆ«è¿›è€Œå®ç°æŠ½è±¡æ¦‚å¿µè¯†åˆ«(Abstract Concept Recognition)ã€‚å°½ç®¡ç°æœ‰æ¨¡å‹åœ¨è¯†åˆ«è§†é¢‘å¸§ä¸­çš„ç‰©ä½“ã€åŠ¨ä½œå’Œåœºæ™¯æ–¹é¢è¿›å±•è¿…é€Ÿï¼Œä½†ç†è§£å¦‚æ­£ä¹‰ã€è‡ªç”±å’Œå›¢ç»“ç­‰é«˜å±‚æŠ½è±¡æ¦‚å¿µä»éœ€ä¾èµ–å¤æ‚çš„å¤šå±‚è¯­ä¹‰æ¨ç†ã€‚ç ”ç©¶æŒ‡å‡ºï¼ŒåŸºç¡€æ¨¡å‹(Foundation Models)çš„æœ€æ–°è¿›å±•ä¸ºè§£å†³è§†é¢‘ä¸­çš„æŠ½è±¡æ¦‚å¿µç†è§£æä¾›äº†ç†æƒ³å¥‘æœºï¼Œè¿™å¯¹äºä½¿æ¨¡å‹ä¸äººç±»æ¨ç†åŠä»·å€¼è§‚ä¿æŒä¸€è‡´å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ–‡ç« å…¨é¢è°ƒç ”äº†å†å²ä¸Šç”¨äºç†è§£è§†é¢‘æŠ½è±¡æ¦‚å¿µçš„ä¸åŒä»»åŠ¡å’Œæ•°æ®é›†ï¼Œæ€»ç»“äº†ç ”ç©¶è€…åœ¨ä¸åŒæŠ€æœ¯èƒŒæ™¯ä¸‹çš„å°è¯•ä¸è´¡çŒ®ã€‚ä½œè€…ä¸»å¼ åœ¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹(Multi-modal Foundation Models)æ—¶ä»£ï¼Œåº”æ·±åº¦å€Ÿé‰´è¿‡å»æ•°åå¹´çš„ç¤¾åŒºç»éªŒï¼Œä»¥é¿å…åœ¨åº”å¯¹è¿™ä¸€å¼€æ”¾æ€§æŒ‘æˆ˜æ—¶â€œé‡å¤é€ è½®å­â€ã€‚è¯¥å·¥ä½œä¸ºæ„å»ºæ›´ç¬¦åˆäººç±»è®¤çŸ¥é€»è¾‘çš„è§†é¢‘æ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†ç†è®ºåŸºç¡€ï¼Œå¹¶æŒ‡æ˜äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Under Review for IJCV",
      "pdf_url": "https://arxiv.org/pdf/2508.20765v1",
      "published_date": "2025-08-28 13:19:49 UTC",
      "updated_date": "2025-08-28 13:19:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:52:39.657700+00:00"
    },
    {
      "arxiv_id": "2508.20762v1",
      "title": "SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer",
      "title_zh": "SKGE-SWINï¼šåŸºäºè·¨é˜¶æ®µ Swin Transformer çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶è½¦è¾†è·¯å¾„ç‚¹é¢„æµ‹ä¸å¯¼èˆª",
      "authors": [
        "Fachri Najm Noer Kartiman",
        "Rasim",
        "Yaya Wihardi",
        "Nurul Hasanah",
        "Oskar Natan",
        "Bambang Wahono",
        "Taufik Ibnu Salim"
      ],
      "abstract": "Focusing on the development of an end-to-end autonomous vehicle model with pixel-to-pixel context awareness, this research proposes the SKGE-Swin architecture. This architecture utilizes the Swin Transformer with a skip-stage mechanism to broaden feature representation globally and at various network levels. This approach enables the model to extract information from distant pixels by leveraging the Swin Transformer's Shifted Window-based Multi-head Self-Attention (SW-MSA) mechanism and to retain critical information from the initial to the final stages of feature extraction, thereby enhancing its capability to comprehend complex patterns in the vehicle's surroundings. The model is evaluated on the CARLA platform using adversarial scenarios to simulate real-world conditions. Experimental results demonstrate that the SKGE-Swin architecture achieves a superior Driving Score compared to previous methods. Furthermore, an ablation study will be conducted to evaluate the contribution of each architectural component, including the influence of skip connections and the use of the Swin Transformer, in improving model performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SKGE-Swinæ¶æ„ï¼Œè¿™æ˜¯ä¸€ç§åº”ç”¨äºç«¯åˆ°ç«¯(end-to-end)è‡ªåŠ¨é©¾é©¶è½¦è¾†èˆªç‚¹é¢„æµ‹ä¸å¯¼èˆªçš„æ¶æ„ï¼Œæ—¨åœ¨å®ç°åƒç´ çº§(pixel-to-pixel)çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ã€‚è¯¥æ¶æ„åˆ©ç”¨å¸¦æœ‰è·³è·ƒé˜¶æ®µ(skip-stage)æœºåˆ¶çš„Swin Transformerï¼Œé€šè¿‡ä½ç§»çª—å£å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶(SW-MSA)åœ¨å…¨å±€åŠå¤šå±‚çº§ç½‘ç»œä¸­æ‰©å±•ç‰¹å¾è¡¨ç¤ºã€‚é€šè¿‡å¼•å…¥è·³è·ƒè¿æ¥(skip connections)ï¼Œæ¨¡å‹èƒ½å¤Ÿä¿ç•™ä»åˆå§‹åˆ°æœ€ç»ˆé˜¶æ®µçš„å…³é”®ç‰¹å¾ä¿¡æ¯ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°è¯†åˆ«è½¦è¾†å‘¨è¾¹çš„å¤æ‚æ¨¡å¼ã€‚åœ¨CARLAå¹³å°ä¸Šçš„å¯¹æŠ—åœºæ™¯æµ‹è¯•è¡¨æ˜ï¼ŒSKGE-Swinåœ¨é©¾é©¶è¯„åˆ†(Driving Score)æ–¹é¢ä¼˜äºæ­¤å‰çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡æ¶ˆèå®éªŒéªŒè¯äº†è·³è·ƒè¿æ¥å’ŒSwin Transformerå¯¹æå‡æ¨¡å‹æ€§èƒ½çš„å…·ä½“è´¡çŒ®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "keywords-multitask learning, autonomous driving, end-to-end learning, skip connections, swin transformer, self-attention mechanism. 12 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.20762v1",
      "published_date": "2025-08-28 13:17:35 UTC",
      "updated_date": "2025-08-28 13:17:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:52:36.449339+00:00"
    },
    {
      "arxiv_id": "2508.20760v2",
      "title": "Occlusion Robustness of CLIP for Military Vehicle Classification",
      "title_zh": "é¢å‘å†›ç”¨è½¦è¾†åˆ†ç±»çš„ CLIP é®æŒ¡é²æ£’æ€§",
      "authors": [
        "Jan Erik van Woerden",
        "Gertjan Burghouts",
        "Lotte Nijskens",
        "Alma M. Liezenga",
        "Sabina van Rooij",
        "Frank Ruis",
        "Hugo J. Kuijf"
      ],
      "abstract": "Vision-language models (VLMs) like CLIP enable zero-shot classification by aligning images and text in a shared embedding space, offering advantages for defense applications with scarce labeled data. However, CLIP's robustness in challenging military environments, with partial occlusion and degraded signal-to-noise ratio (SNR), remains underexplored. We investigate CLIP variants' robustness to occlusion using a custom dataset of 18 military vehicle classes and evaluate using Normalized Area Under the Curve (NAUC) across occlusion percentages. Four key insights emerge: (1) Transformer-based CLIP models consistently outperform CNNs, (2) fine-grained, dispersed occlusions degrade performance more than larger contiguous occlusions, (3) despite improved accuracy, performance of linear-probed models sharply drops at around 35% occlusion, (4) by finetuning the model's backbone, this performance drop occurs at more than 60% occlusion. These results underscore the importance of occlusion-specific augmentations during training and the need for further exploration into patch-level sensitivity and architectural resilience for real-world deployment of CLIP.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† CLIP ç­‰è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨å†›äº‹è½¦è¾†åˆ†ç±»ä»»åŠ¡ä¸­åº”å¯¹é®æŒ¡ (occlusion) çš„é²æ£’æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é˜²å¾¡åº”ç”¨ä¸­æ ‡è®°æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸‹ã€‚ç ”ç©¶é€šè¿‡ä¸€ä¸ªåŒ…å« 18 ç±»å†›äº‹è½¦è¾†çš„è‡ªå®šä¹‰æ•°æ®é›†ï¼Œåˆ©ç”¨ Normalized Area Under the Curve (NAUC) æŒ‡æ ‡è¯„ä¼°äº†ä¸åŒ CLIP å˜ä½“åœ¨å„ç§é®æŒ¡æ¯”ä¾‹ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚å®éªŒå‘ç°åŸºäº Transformer çš„ CLIP æ¨¡å‹åœ¨é²æ£’æ€§ä¸Šå§‹ç»ˆä¼˜äºåŸºäº CNN çš„æ¨¡å‹ï¼Œä¸”ç»†ç²’åº¦çš„åˆ†æ•£é®æŒ¡ (fine-grained, dispersed occlusions) æ¯”å¤§é¢ç§¯è¿ç»­é®æŒ¡å¯¹æ€§èƒ½çš„æŸå®³æ›´å¤§ã€‚è™½ç„¶ Linear-probed æ¨¡å‹èƒ½æé«˜å‡†ç¡®ç‡ï¼Œä½†å…¶æ€§èƒ½åœ¨é®æŒ¡ç‡è¾¾ 35% æ—¶ä¼šæ€¥å‰§ä¸‹é™ï¼Œè€Œé€šè¿‡å¯¹æ¨¡å‹ Backbone è¿›è¡Œå¾®è°ƒ (finetuning) å¯ä»¥å°†è¯¥ä¸‹é™é˜ˆå€¼æå‡è‡³ 60% ä»¥ä¸Šã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†åœ¨è®­ç»ƒä¸­åŠ å…¥é’ˆå¯¹æ€§é®æŒ¡å¢å¼ºçš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºäº†æå‡ CLIP åœ¨å®é™…éƒ¨ç½²ä¸­æ¶æ„éŸ§æ€§ä¸ Patch-level æ•æ„Ÿæ€§ç ”ç©¶çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "To be presented at SPIE: Sensors + Imaging, Artificial Intelligence for Security and Defence Applications II",
      "pdf_url": "https://arxiv.org/pdf/2508.20760v2",
      "published_date": "2025-08-28 13:16:55 UTC",
      "updated_date": "2025-09-02 15:53:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:52:31.492627+00:00"
    },
    {
      "arxiv_id": "2508.20758v1",
      "title": "SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for Zero-Shot 3D Visual Grounding",
      "title_zh": "SeqVLMï¼šåŸºäº VLM çš„é›¶æ ·æœ¬ 3D è§†è§‰å®šä½å€™é€‰å¼•å¯¼å¤šè§†å›¾åºåˆ—æ¨ç†",
      "authors": [
        "Jiawen Lin",
        "Shiran Bian",
        "Yihang Zhu",
        "Wenbin Tan",
        "Yachao Zhang",
        "Yuan Xie",
        "Yanyun Qu"
      ],
      "abstract": "3D Visual Grounding (3DVG) aims to localize objects in 3D scenes using natural language descriptions. Although supervised methods achieve higher accuracy in constrained settings, zero-shot 3DVG holds greater promise for real-world applications since eliminating scene-specific training requirements. However, existing zero-shot methods face challenges of spatial-limited reasoning due to reliance on single-view localization, and contextual omissions or detail degradation. To address these issues, we propose SeqVLM, a novel zero-shot 3DVG framework that leverages multi-view real-world scene images with spatial information for target object reasoning. Specifically, SeqVLM first generates 3D instance proposals via a 3D semantic segmentation network and refines them through semantic filtering, retaining only semantic-relevant candidates. A proposal-guided multi-view projection strategy then projects these candidate proposals onto real scene image sequences, preserving spatial relationships and contextual details in the conversion process of 3D point cloud to images. Furthermore, to mitigate VLM computational overload, we implement a dynamic scheduling mechanism that iteratively processes sequances-query prompts, leveraging VLM's cross-modal reasoning capabilities to identify textually specified objects. Experiments on the ScanRefer and Nr3D benchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scores of 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%, respectively, which advance 3DVG toward greater generalization and real-world applicability. The code is available at https://github.com/JiawLin/SeqVLM.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é›¶æ ·æœ¬ 3D Visual Grounding (3DVG) ä¸­å­˜åœ¨çš„ç©ºé—´æ¨ç†å—é™ã€ä¸Šä¸‹æ–‡ç¼ºå¤±åŠç»†èŠ‚é€€åŒ–ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º SeqVLM çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡ 3D è¯­ä¹‰åˆ†å‰²ç½‘ç»œç”Ÿæˆå€™é€‰å®ä¾‹å»ºè®® (proposals)ï¼Œå¹¶åˆ©ç”¨è¯­ä¹‰è¿‡æ»¤ç­›é€‰å‡ºç›¸å…³ç›®æ ‡ã€‚éšåï¼ŒSeqVLM é‡‡ç”¨å»ºè®®å¼•å¯¼çš„å¤šè§†å›¾æŠ•å½±ç­–ç•¥ï¼Œå°† 3D ç‚¹äº‘è½¬æ¢ä¸ºçœŸå®åœºæ™¯å›¾åƒåºåˆ—ï¼Œä»è€Œåœ¨è½¬æ¢è¿‡ç¨‹ä¸­æœ‰æ•ˆä¿ç•™äº†ç©ºé—´å…³ç³»å’Œä¸Šä¸‹æ–‡ç»†èŠ‚ã€‚ä¸ºé™ä½ VLM çš„è®¡ç®—è´Ÿæ‹…ï¼Œç ”ç©¶è¿˜å®ç°äº†ä¸€ç§åŠ¨æ€è°ƒåº¦æœºåˆ¶ï¼Œé€šè¿‡è¿­ä»£å¤„ç†åºåˆ—æŸ¥è¯¢æç¤ºæ¥å‘æŒ¥ VLM çš„è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚å®éªŒåœ¨ ScanRefer å’Œ Nr3D åŸºå‡†ä¸Šè¾¾åˆ°äº† 55.6% å’Œ 53.2% çš„ Acc@0.25 åˆ†æ•°ï¼Œåˆ†åˆ«è¶…è¿‡æ­¤å‰é›¶æ ·æœ¬æ–¹æ³• 4.0% å’Œ 5.2%ã€‚è¿™ä¸€æˆæœè¯æ˜äº† SeqVLM åœ¨æå‡ 3DVG æ³›åŒ–æ€§å’Œå®é™…åº”ç”¨æ½œåŠ›æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20758v1",
      "published_date": "2025-08-28 13:15:37 UTC",
      "updated_date": "2025-08-28 13:15:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:52:34.145803+00:00"
    },
    {
      "arxiv_id": "2508.21106v1",
      "title": "Dynamic Low-rank Approximation of Full-Matrix Preconditioner for Training Generalized Linear Models",
      "title_zh": "å¹¿ä¹‰çº¿æ€§æ¨¡å‹è®­ç»ƒä¸­çš„å…¨çŸ©é˜µé¢„æ¡ä»¶å­åŠ¨æ€ä½ç§©é€¼è¿‘",
      "authors": [
        "Tatyana Matveeva",
        "Aleksandr Katrutsa",
        "Evgeny Frolov"
      ],
      "abstract": "Adaptive gradient methods like Adagrad and its variants are widespread in large-scale optimization. However, their use of diagonal preconditioning matrices limits the ability to capture parameter correlations. Full-matrix adaptive methods, approximating the exact Hessian, can model these correlations and may enable faster convergence. At the same time, their computational and memory costs are often prohibitive for large-scale models. To address this limitation, we propose AdaGram, an optimizer that enables efficient full-matrix adaptive gradient updates. To reduce memory and computational overhead, we utilize fast symmetric factorization for computing the preconditioned update direction at each iteration. Additionally, we maintain the low-rank structure of a preconditioner along the optimization trajectory using matrix integrator methods. Numerical experiments on standard machine learning tasks show that AdaGram converges faster or matches the performance of diagonal adaptive optimizers when using rank five and smaller rank approximations. This demonstrates AdaGram's potential as a scalable solution for adaptive optimization in large models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Adagrad ç­‰è‡ªé€‚åº”æ¢¯åº¦æ–¹æ³•å› ä½¿ç”¨å¯¹è§’é¢„å¤„ç†çŸ©é˜µ(diagonal preconditioning matrices)è€Œæ— æ³•æ•æ‰å‚æ•°ç›¸å…³æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º AdaGram çš„é«˜æ•ˆå…¨çŸ©é˜µè‡ªé€‚åº”æ¢¯åº¦ä¼˜åŒ–å™¨ã€‚è¯¥ç®—æ³•åˆ©ç”¨å¿«é€Ÿå¯¹ç§°åˆ†è§£(fast symmetric factorization)æ¥è®¡ç®—æ¯ä¸€æ­¥çš„é¢„å¤„ç†æ›´æ–°æ–¹å‘ï¼Œå¹¶ç»“åˆçŸ©é˜µç§¯åˆ†æ–¹æ³•(matrix integrator methods)åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­åŠ¨æ€ç»´æŒé¢„å¤„ç†å™¨çš„ä½ç§©ç»“æ„(low-rank structure)ï¼Œä»è€Œå¤§å¹…é™ä½äº†å…¨çŸ©é˜µæ–¹æ³•çš„è®¡ç®—ä¸å†…å­˜å¼€é”€ã€‚æ•°å€¼å®éªŒè¯æ˜ï¼ŒAdaGram åœ¨ä»…ä½¿ç”¨ç§©äº”æˆ–æ›´å°çš„è¿‘ä¼¼æ—¶ï¼Œå…¶æ”¶æ•›æ€§èƒ½ä¾¿å¯è¾¾åˆ°æˆ–è¶…è¿‡ä¼ ç»Ÿçš„å¯¹è§’è‡ªé€‚åº”ä¼˜åŒ–å™¨ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒæä¾›äº†ä¸€ç§å…·æœ‰æ‰©å±•æ€§çš„è‡ªé€‚åº”ä¼˜åŒ–æ–¹æ¡ˆï¼Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶æ˜¾è‘—æå‡äº†å¯¹å‚æ•°é—´å¤æ‚ç›¸å…³æ€§çš„å»ºæ¨¡èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21106v1",
      "published_date": "2025-08-28 13:15:05 UTC",
      "updated_date": "2025-08-28 13:15:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:52:50.359594+00:00"
    },
    {
      "arxiv_id": "2508.20755v1",
      "title": "Provable Benefits of In-Tool Learning for Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹å·¥å…·å†…å­¦ä¹ çš„å¯è¯æ˜ä¼˜åŠ¿",
      "authors": [
        "Sam Houliston",
        "Ambroise Odonnat",
        "Charles Arnal",
        "Vivien Cabannes"
      ],
      "abstract": "Tool-augmented language models, equipped with retrieval, memory, or external APIs, are reshaping AI, yet their theoretical advantages remain underexplored. In this paper, we address this question by demonstrating the benefits of in-tool learning (external retrieval) over in-weight learning (memorization) for factual recall. We show that the number of facts a model can memorize solely in its weights is fundamentally limited by its parameter count. In contrast, we prove that tool-use enables unbounded factual recall via a simple and efficient circuit construction. These results are validated in controlled experiments, where tool-using models consistently outperform memorizing ones. We further show that for pretrained large language models, teaching tool-use and general rules is more effective than finetuning facts into memory. Our work provides both a theoretical and empirical foundation, establishing why tool-augmented workflows are not just practical, but provably more scalable.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å·¥å…·å¢å¼ºå‹è¯­è¨€æ¨¡å‹ (Tool-augmented language models) ä¸­ï¼Œå¤–éƒ¨æ£€ç´¢å½¢å¼çš„ In-Tool Learning ä¸æ¨¡å‹å‚æ•°è®°å¿†å½¢å¼çš„ In-Weight Learning åœ¨äº‹å®å¬å›æ–¹é¢çš„ç†è®ºä¼˜åŠ¿ã€‚ç ”ç©¶è€…é€šè¿‡ç†è®ºè¯æ˜å‘ç°ï¼Œæ¨¡å‹ä»…ä¾é æƒé‡è®°å¿†çš„äº‹å®æ•°é‡å—å…¶å‚æ•°è§„æ¨¡çš„æ ¹æœ¬é™åˆ¶ï¼Œè€Œé€šè¿‡ç®€å•çš„ç”µè·¯æ„å»º (Circuit construction)ï¼Œå·¥å…·çš„ä½¿ç”¨èƒ½å¤Ÿå®ç°æ— é™çš„äº‹å®å¬å›ã€‚åœ¨å—æ§å®éªŒä¸­ï¼Œä½¿ç”¨å·¥å…·çš„æ¨¡å‹è¡¨ç°ä¸€è‡´ä¼˜äºä»…ä¾èµ–è®°å¿†çš„æ¨¡å‹ï¼ŒéªŒè¯äº†å·¥å…·é©±åŠ¨æµç¨‹çš„ä¼˜è¶Šæ€§ã€‚é’ˆå¯¹é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ (LLMs)ï¼Œç ”ç©¶è¿›ä¸€æ­¥è¡¨æ˜ï¼Œæ•™æˆæ¨¡å‹ä½¿ç”¨å·¥å…·å’Œé€šç”¨è§„åˆ™æ¯”é€šè¿‡å¾®è°ƒ (Finetuning) å°†äº‹å®è®°å…¥å†…å­˜æ›´åŠ æœ‰æ•ˆã€‚è¯¥å·¥ä½œä¸ºå·¥å…·å¢å¼ºå‹å·¥ä½œæµæä¾›äº†ç†è®ºå’Œå®è¯åŸºç¡€ï¼Œç¡®ç«‹äº†å…¶åœ¨å¯æ‰©å±•æ€§ä¸Šå…·æœ‰å¯è¯æ˜çš„ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20755v1",
      "published_date": "2025-08-28 13:12:19 UTC",
      "updated_date": "2025-08-28 13:12:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:53:02.295455+00:00"
    },
    {
      "arxiv_id": "2508.20754v1",
      "title": "${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting",
      "title_zh": "$C^{3}$-GSï¼šå­¦ä¹ ç”¨äºå¯æ³›åŒ–é«˜æ–¯æ³¼æº…çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ã€è·¨ç»´åº¦åŠè·¨å°ºåº¦ç‰¹å¾",
      "authors": [
        "Yuxi Hu",
        "Jun Zhang",
        "Kuangyi Chen",
        "Zhe Zhang",
        "Friedrich Fraundorfer"
      ],
      "abstract": "Generalizable Gaussian Splatting aims to synthesize novel views for unseen scenes without per-scene optimization. In particular, recent advancements utilize feed-forward networks to predict per-pixel Gaussian parameters, enabling high-quality synthesis from sparse input views. However, existing approaches fall short in encoding discriminative, multi-view consistent features for Gaussian predictions, which struggle to construct accurate geometry with sparse views. To address this, we propose $\\mathbf{C}^{3}$-GS, a framework that enhances feature learning by incorporating context-aware, cross-dimension, and cross-scale constraints. Our architecture integrates three lightweight modules into a unified rendering pipeline, improving feature fusion and enabling photorealistic synthesis without requiring additional supervision. Extensive experiments on benchmark datasets validate that $\\mathbf{C}^{3}$-GS achieves state-of-the-art rendering quality and generalization ability. Code is available at: https://github.com/YuhsiHu/C3-GS.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† $C^3$-GS æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ Generalizable Gaussian Splatting åœ¨ç¨€ç–è§†è§’ä¸‹ç”±äºç‰¹å¾è¾¨åˆ«åŠ›ä¸è¶³å’Œå¤šè§†å›¾ä¸€è‡´æ€§ç¼ºå¤±å¯¼è‡´çš„å‡ ä½•æ„å»ºéš¾é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥ context-awareã€cross-dimension å’Œ cross-scale çº¦æŸï¼Œæ˜¾è‘—å¢å¼ºäº†å‰é¦ˆç½‘ç»œå¯¹ Gaussian å‚æ•°é¢„æµ‹çš„ç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚å…¶æ¶æ„é€šè¿‡å°†ä¸‰ä¸ªè½»é‡çº§æ¨¡å—é›†æˆåˆ°ç»Ÿä¸€çš„æ¸²æŸ“ç®¡çº¿ä¸­ï¼Œåœ¨æ— éœ€é¢å¤–ç›‘ç£çš„æƒ…å†µä¸‹ä¼˜åŒ–äº†ç‰¹å¾èåˆå¹¶å®ç°äº†ç…§ç‰‡çº§çœŸå®æ„Ÿçš„è§†å›¾åˆæˆã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œ$C^3$-GS åœ¨æ¸²æŸ“è´¨é‡å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢å‡è¾¾åˆ°äº† state-of-the-art æ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to The 36th British Machine Vision Conference (BMVC 2025), Sheffield, UK",
      "pdf_url": "https://arxiv.org/pdf/2508.20754v1",
      "published_date": "2025-08-28 13:12:18 UTC",
      "updated_date": "2025-08-28 13:12:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:53:09.347475+00:00"
    },
    {
      "arxiv_id": "2509.04462v2",
      "title": "Benchmarking GPT-5 for biomedical natural language processing",
      "title_zh": "é¢å‘ç”Ÿç‰©åŒ»å­¦è‡ªç„¶è¯­è¨€å¤„ç†çš„ GPT-5 åŸºå‡†æµ‹è¯•",
      "authors": [
        "Yu Hou",
        "Zaifu Zhan",
        "Min Zeng",
        "Yifan Wu",
        "Shuang Zhou",
        "Rui Zhang"
      ],
      "abstract": "Biomedical literature and clinical narratives pose multifaceted challenges for natural language understanding, from precise entity extraction and document synthesis to multi-step diagnostic reasoning. This study extends a unified benchmark to evaluate GPT-5 and GPT-4o under zero-, one-, and five-shot prompting across five core biomedical NLP tasks: named entity recognition, relation extraction, multi-label document classification, summarization, and simplification, and nine expanded biomedical QA datasets covering factual knowledge, clinical reasoning, and multimodal visual understanding. Using standardized prompts, fixed decoding parameters, and consistent inference pipelines, we assessed model performance, latency, and token-normalized cost under official pricing. GPT-5 consistently outperformed GPT-4o, with the largest gains on reasoning-intensive datasets such as MedXpertQA and DiagnosisArena and stable improvements in multimodal QA. In core tasks, GPT-5 achieved better chemical NER and ChemProt scores but remained below domain-tuned baselines for disease NER and summarization. Despite producing longer outputs, GPT-5 showed comparable latency and 30 to 50 percent lower effective cost per correct prediction. Fine-grained analyses revealed improvements in diagnosis, treatment, and reasoning subtypes, whereas boundary-sensitive extraction and evidence-dense summarization remain challenging. Overall, GPT-5 approaches deployment-ready performance for biomedical QA while offering a favorable balance of accuracy, interpretability, and economic efficiency. The results support a tiered prompting strategy: direct prompting for large-scale or cost-sensitive applications, and chain-of-thought scaffolds for analytically complex or high-stakes scenarios, highlighting the continued need for hybrid solutions where precision and factual fidelity are critical.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº† GPT-5 åœ¨ç”Ÿç‰©åŒ»å­¦è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆbiomedical NLPï¼‰é¢†åŸŸçš„æ€§èƒ½ï¼Œå¹¶ä¸ GPT-4o åœ¨é›¶æ ·æœ¬ã€å•æ ·æœ¬å’Œäº”æ ·æœ¬æç¤ºï¼ˆpromptingï¼‰ä¸‹è¿›è¡Œäº†å¤šç»´åº¦å¯¹æ¯”ã€‚è¯„ä¼°ä»»åŠ¡æ¶µç›–äº†å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€å…³ç³»æŠ½å–ï¼ˆrelation extractionï¼‰ã€æ–‡æ¡£åˆ†ç±»ä»¥åŠæ¶‰åŠä¸´åºŠæ¨ç†å’Œå¤šæ¨¡æ€ç†è§£çš„ä¹ä¸ª QA æ•°æ®é›†ã€‚å®éªŒå‘ç° GPT-5 åœ¨ MedXpertQA å’Œ DiagnosisArena ç­‰æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨åŒ–å­¦å®ä½“è¯†åˆ«æ–¹é¢è¾ƒ GPT-4o æœ‰æ˜¾è‘—æå‡ã€‚å°½ç®¡ GPT-5 ç”Ÿæˆçš„å†…å®¹æ›´é•¿ï¼Œä½†å…¶å•æ¬¡æ­£ç¡®é¢„æµ‹çš„æœ‰æ•ˆæˆæœ¬æ¯” GPT-4o é™ä½äº† 30% è‡³ 50%ï¼Œå±•ç°å‡ºæä½³çš„ç»æµæ•ˆç‡ã€‚è™½ç„¶åœ¨ç–¾ç—… NER å’Œè¯æ®å¯†é›†å‹æ‘˜è¦ä»»åŠ¡ä¸­ä»é€Šäºç‰¹å®šé¢†åŸŸå¾®è°ƒçš„æ¨¡å‹ï¼Œä½† GPT-5 åœ¨è¯Šæ–­å’Œæ²»ç–—æ¨ç†ä¸Šçš„è¿›æ­¥ä½¿å…¶æ›´æ¥è¿‘å®é™…éƒ¨ç½²ã€‚ç ”ç©¶æœ€åå»ºè®®é’ˆå¯¹å¤§è§„æ¨¡åº”ç”¨ä½¿ç”¨ç›´æ¥æç¤ºï¼Œè€Œå¯¹é«˜é£é™©å¤æ‚åœºæ™¯é‡‡ç”¨é“¾å¼æ€ç»´ï¼ˆchain-of-thoughtï¼‰æ¶æ„ï¼Œå¼ºè°ƒäº†åœ¨ç²¾ç¡®åº¦è‡³å…³é‡è¦çš„é¢†åŸŸä¸­é‡‡ç”¨æ··åˆè§£å†³æ–¹æ¡ˆçš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04462v2",
      "published_date": "2025-08-28 13:06:53 UTC",
      "updated_date": "2025-10-23 15:09:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:53:17.046551+00:00"
    },
    {
      "arxiv_id": "2508.20737v1",
      "title": "Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol",
      "title_zh": "é‡æ–°æ€è€ƒå¤§è¯­è¨€æ¨¡å‹åº”ç”¨çš„æµ‹è¯•ï¼šç‰¹æ€§ã€æŒ‘æˆ˜ä¸è½»é‡çº§äº¤äº’åè®®",
      "authors": [
        "Wei Ma",
        "Yixiao Yang",
        "Qiang Hu",
        "Shi Ying",
        "Zhi Jin",
        "Bo Du",
        "Zhenchang Xing",
        "Tianlin Li",
        "Junjie Shi",
        "Yang Liu",
        "Linxiao Jiang"
      ],
      "abstract": "Applications of Large Language Models~(LLMs) have evolved from simple text generators into complex software systems that integrate retrieval augmentation, tool invocation, and multi-turn interactions. Their inherent non-determinism, dynamism, and context dependence pose fundamental challenges for quality assurance. This paper decomposes LLM applications into a three-layer architecture: \\textbf{\\textit{System Shell Layer}}, \\textbf{\\textit{Prompt Orchestration Layer}}, and \\textbf{\\textit{LLM Inference Core}}. We then assess the applicability of traditional software testing methods in each layer: directly applicable at the shell layer, requiring semantic reinterpretation at the orchestration layer, and necessitating paradigm shifts at the inference core. A comparative analysis of Testing AI methods from the software engineering community and safety analysis techniques from the AI community reveals structural disconnects in testing unit abstraction, evaluation metrics, and lifecycle management. We identify four fundamental differences that underlie 6 core challenges. To address these, we propose four types of collaborative strategies (\\emph{Retain}, \\emph{Translate}, \\emph{Integrate}, and \\emph{Runtime}) and explore a closed-loop, trustworthy quality assurance framework that combines pre-deployment validation with runtime monitoring. Based on these strategies, we offer practical guidance and a protocol proposal to support the standardization and tooling of LLM application testing. We propose a protocol \\textbf{\\textit{Agent Interaction Communication Language}} (AICL) that is used to communicate between AI agents. AICL has the test-oriented features and is easily integrated in the current agent framework.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)åº”ç”¨å› éç¡®å®šæ€§å’ŒåŠ¨æ€æ€§å¸¦æ¥çš„è´¨é‡ä¿è¯æŒ‘æˆ˜ï¼Œæå‡ºäº†å°†å…¶è§£æ„ä¸ºç³»ç»Ÿå¤–å£³å±‚(System Shell Layer)ã€æç¤ºè¯ç¼–æ’å±‚(Prompt Orchestration Layer)å’ŒLLMæ¨ç†æ ¸å¿ƒå±‚(LLM Inference Core)çš„ä¸‰å±‚æ¶æ„ã€‚é€šè¿‡å¯¹æ¯”åˆ†æè½¯ä»¶å·¥ç¨‹ä¸AIç¤¾åŒºçš„æµ‹è¯•æ–¹æ³•ï¼Œç ”ç©¶æ­ç¤ºäº†ä¸¤è€…åœ¨æµ‹è¯•å•å…ƒæŠ½è±¡å’Œè¯„ä¼°æŒ‡æ ‡ä¸Šçš„ç»“æ„æ€§è„±èŠ‚ï¼Œå¹¶è¯†åˆ«å‡ºå…­å¤§æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¿ç•™(Retain)ã€è½¬æ¢(Translate)ã€é›†æˆ(Integrate)å’Œè¿è¡Œæ—¶(Runtime)å››ç§åä½œç­–ç•¥ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªç»“åˆéƒ¨ç½²å‰éªŒè¯ä¸è¿è¡Œæ—¶ç›‘æ§çš„é—­ç¯è´¨é‡ä¿è¯æ¡†æ¶ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†ä»£ç†äº¤äº’é€šä¿¡è¯­è¨€(Agent Interaction Communication Language, AICL)ï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰æµ‹è¯•å¯¼å‘ç‰¹æ€§çš„é€šä¿¡åè®®ï¼Œæ—¨åœ¨æ”¯æŒLLMåº”ç”¨æµ‹è¯•çš„æ ‡å‡†åŒ–ä¸å·¥å…·åŒ–ï¼Œå¹¶èƒ½æ— ç¼é›†æˆè‡³ç°æœ‰æ™ºèƒ½ä½“æ¡†æ¶ä¸­ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20737v1",
      "published_date": "2025-08-28 13:00:28 UTC",
      "updated_date": "2025-08-28 13:00:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:53:15.754869+00:00"
    },
    {
      "arxiv_id": "2508.20729v1",
      "title": "Re4: Scientific Computing Agent with Rewriting, Resolution, Review and Revision",
      "title_zh": "Re4ï¼šå…·å¤‡é‡å†™ã€æ±‚è§£ã€å®¡é˜…ä¸ä¿®æ­£èƒ½åŠ›çš„ç§‘å­¦è®¡ç®—æ™ºèƒ½ä½“",
      "authors": [
        "Ao Cheng",
        "Lei Zhang",
        "Guowei He"
      ],
      "abstract": "Large language models (LLMs) serve as an active and promising field of generative artificial intelligence and have demonstrated abilities to perform complex tasks in multiple domains, including mathematical and scientific reasoning. In this work, we construct a novel agent framework for solving representative problems in scientific computing. The proposed agent, incorporating a \"rewriting-resolution-review-revision\" logical chain via three reasoning LLMs (functioning as the Consultant, Reviewer, and Programmer, respectively), is integrated in a collaborative and interactive manner. The Consultant module endows the agent with knowledge transfer capabilities to link problems to professional domain insights, thereby rewriting problem descriptions through text augmentation. The Programmer module is responsible for generating and executing well-structured code to deliver the problem resolution. The Reviewer module equips the agent with the capacity for self-debugging and self-refinement through interactive feedback with code runtime outputs. By leveraging the end-to-end review mechanism, the executable code provided by the Programmer attains the iterative revision. A comprehensive evaluation is conducted on the performance of the proposed agent framework in solving PDEs, ill-conditioned linear systems, and data-driven physical analysis problems. Compared to single-model, this collaborative framework significantly improves the bug-free code generation rate and reduces the occurrence of non-physical solutions, thereby establishing a highly reliable framework for autonomous code generation based on natural language descriptions. The review mechanism improved the average execution success (bug-free code and non-NaN solutions) rate of the latest reasoning models. In summary, our agent framework establishes automatic code generation and review as a promising scientific computing paradigm.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Re4ï¼Œä¸€ç§ä¸“é—¨ç”¨äºè§£å†³ç§‘å­¦è®¡ç®—é—®é¢˜çš„æ™ºèƒ½ä½“(Agent)æ¡†æ¶ï¼Œæ ¸å¿ƒé‡‡ç”¨äº†â€œé‡å†™-æ±‚è§£-è¯„å®¡-ä¿®æ­£â€(Rewriting-Resolution-Review-Revision)çš„é€»è¾‘é“¾ã€‚è¯¥æ¡†æ¶é€šè¿‡Consultantã€Programmerå’ŒReviewerä¸‰ä¸ªæ¨ç†å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„åä½œï¼Œå®ç°äº†ä»è‡ªç„¶è¯­è¨€æè¿°åˆ°é«˜è´¨é‡ä»£ç çš„è‡ªåŠ¨åŒ–ç”Ÿæˆã€‚Consultantæ¨¡å—åˆ©ç”¨é¢†åŸŸçŸ¥è¯†å¯¹é—®é¢˜æè¿°è¿›è¡Œé‡å†™ä¸æ–‡æœ¬å¢å¼ºï¼ŒProgrammeræ¨¡å—è´Ÿè´£ç”Ÿæˆå¹¶æ‰§è¡Œç»“æ„åŒ–ä»£ç ï¼Œè€ŒRevieweræ¨¡å—åˆ™é€šè¿‡ä»£ç è¿è¡Œè¾“å‡ºè¿›è¡Œè‡ªæˆ‘è°ƒè¯•ä¸ç²¾ç‚¼ã€‚é€šè¿‡è¿™ç§ç«¯åˆ°ç«¯çš„è¯„å®¡æœºåˆ¶ï¼Œç³»ç»Ÿèƒ½å¤Ÿå¯¹ç”Ÿæˆçš„ä»£ç è¿›è¡Œè¿­ä»£ä¿®æ­£ï¼Œç¡®ä¿è®¡ç®—ä»»åŠ¡çš„å‡†ç¡®æ‰§è¡Œã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒRe4åœ¨å¤„ç†åå¾®åˆ†æ–¹ç¨‹(PDEs)ã€ç—…æ€çº¿æ€§ç³»ç»Ÿå’Œæ•°æ®é©±åŠ¨ç‰©ç†åˆ†æç­‰ä»»åŠ¡æ—¶ï¼Œæ˜¾è‘—æé«˜äº†æ— é”™ä»£ç çš„ç”Ÿæˆç‡å¹¶å‡å°‘äº†éç‰©ç†æ€§è´¨è§£çš„å‡ºç°ã€‚ç›¸æ¯”äºå•ä¸€æ¨¡å‹ï¼Œè¯¥æ¡†æ¶ä¸ºç§‘å­¦è®¡ç®—é¢†åŸŸæä¾›äº†ä¸€ç§é«˜å¯é æ€§çš„è‡ªåŠ¨ä»£ç ç”Ÿæˆæ–°èŒƒå¼ï¼Œæœ‰æ•ˆæå‡äº†å¤æ‚æ¨ç†ä»»åŠ¡çš„æˆåŠŸç‡ã€‚",
      "categories": [
        "cs.AI",
        "physics.comp-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20729v1",
      "published_date": "2025-08-28 12:50:48 UTC",
      "updated_date": "2025-08-28 12:50:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:53:40.690440+00:00"
    },
    {
      "arxiv_id": "2508.20705v2",
      "title": "EEGDM: Learning EEG Representation with Latent Diffusion Model",
      "title_zh": "EEGDMï¼šåŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„ EEG è¡¨å¾å­¦ä¹ ",
      "authors": [
        "Shaocong Wang",
        "Tong Liu",
        "Yihan Li",
        "Ming Li",
        "Kairui Wen",
        "Pei Yang",
        "Wenqi Ji",
        "Minjing Yu",
        "Yong-Jin Liu"
      ],
      "abstract": "Recent advances in self-supervised learning for EEG representation have largely relied on masked reconstruction, where models are trained to recover randomly masked signal segments. While effective at modeling local dependencies, such objectives are inherently limited in capturing the global dynamics and long-range dependencies essential for characterizing neural activity. To address this limitation, we propose EEGDM, a novel self-supervised framework that leverages latent diffusion models to generate EEG signals as an objective. Unlike masked reconstruction, diffusion-based generation progressively denoises signals from noise to realism, compelling the model to capture holistic temporal patterns and cross-channel relationships. Specifically, EEGDM incorporates an EEG encoder that distills raw signals and their channel augmentations into a compact representation, acting as conditional information to guide the diffusion model for generating EEG signals. This design endows EEGDM with a compact latent space, which not only offers ample control over the generative process but also can be leveraged for downstream tasks. Experimental results show that EEGDM (1) reconstructs high-quality EEG signals, (2) learns robust representations, and (3) achieves competitive performance across diverse downstream tasks, thus exploring a new direction for self-supervised EEG representation learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EEGDMï¼Œä¸€ç§æ–°å‹çš„è‡ªç›‘ç£å­¦ä¹ (self-supervised learning)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ©ç é‡å»º(masked reconstruction)æ–¹æ³•åœ¨æ•æ‰è„‘ç”µä¿¡å·(EEG)å…¨å±€åŠ¨æ€å’Œé•¿ç¨‹ä¾èµ–æ€§æ–¹é¢çš„å±€é™æ€§ã€‚EEGDMé€šè¿‡å¼•å…¥æ½œåœ¨æ‰©æ•£æ¨¡å‹(Latent Diffusion Model)ï¼Œå°†ç”ŸæˆEEGä¿¡å·ä½œä¸ºå­¦ä¹ ç›®æ ‡ï¼Œåˆ©ç”¨ä»å™ªå£°åˆ°çœŸå®ä¿¡å·çš„æ¸è¿›å¼å»å™ªè¿‡ç¨‹ï¼Œä¿ƒä½¿æ¨¡å‹æ•æ‰æ•´ä½“çš„æ—¶é—´æ¨¡å¼å’Œè·¨é€šé“å…³ç³»ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒåŒ…å«ä¸€ä¸ªEEGç¼–ç å™¨(EEG encoder)ï¼Œå°†åŸå§‹ä¿¡å·åŠå…¶å¢å¼ºç‰¹å¾æç‚¼ä¸ºç´§å‡‘çš„æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶å°†å…¶ä½œä¸ºæ¡ä»¶ä¿¡æ¯å¼•å¯¼æ‰©æ•£ç”Ÿæˆè¿‡ç¨‹ã€‚è¿™ç§ç´§å‡‘çš„æ½œç©ºé—´(latent space)è®¾è®¡ä¸ä»…æå‡äº†ç”Ÿæˆè¿‡ç¨‹çš„å¯æ§æ€§ï¼Œä¹Ÿä¸ºå„ç±»ä¸‹æ¸¸ä»»åŠ¡æä¾›äº†é²æ£’çš„ç‰¹å¾æ”¯æŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEEGDMåœ¨é«˜è´¨é‡ä¿¡å·é‡å»ºå’Œå­¦ä¹ è¡¨å¾æ–¹é¢è¡¨ç°å“è¶Šï¼Œåœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å‡å–å¾—äº†æå…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œä¸ºè‡ªç›‘ç£EEGè¡¨ç¤ºå­¦ä¹ å¼€è¾Ÿäº†ç”Ÿæˆå¼æ¨¡å‹çš„æ–°æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20705v2",
      "published_date": "2025-08-28 12:23:28 UTC",
      "updated_date": "2025-12-19 02:47:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:53:34.660723+00:00"
    },
    {
      "arxiv_id": "2508.20701v1",
      "title": "Transparent Semantic Spaces: A Categorical Approach to Explainable Word Embeddings",
      "title_zh": "é€æ˜è¯­ä¹‰ç©ºé—´ï¼šä¸€ç§åŸºäºèŒƒç•´è®ºçš„å¯è§£é‡Šè¯åµŒå…¥æ–¹æ³•",
      "authors": [
        "Ares Fabregat-HernÃ¡ndez",
        "Javier Palanca",
        "Vicent Botti"
      ],
      "abstract": "The paper introduces a novel framework based on category theory to enhance the explainability of artificial intelligence systems, particularly focusing on word embeddings. Key topics include the construction of categories $\\mathcal{L}_T$ and $\\mathcal{P}_T$, providing schematic representations of the semantics of a text $ T $, and reframing the selection of the element with maximum probability as a categorical notion. Additionally, the monoidal category $\\mathcal{P}_T$ is constructed to visualize various methods of extracting semantic information from $T$, offering a dimension-agnostic definition of semantic spaces reliant solely on information within the text.\n  Furthermore, the paper defines the categories of configurations Conf and word embeddings $\\mathcal{Emb}$, accompanied by the concept of divergence as a decoration on $\\mathcal{Emb}$. It establishes a mathematically precise method for comparing word embeddings, demonstrating the equivalence between the GloVe and Word2Vec algorithms and the metric MDS algorithm, transitioning from neural network algorithms (black box) to a transparent framework. Finally, the paper presents a mathematical approach to computing biases before embedding and offers insights on mitigating biases at the semantic space level, advancing the field of explainable artificial intelligence.",
      "tldr_zh": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºèŒƒç•´è®º(Category Theory)çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨æå‡äººå·¥æ™ºèƒ½ç³»ç»Ÿç‰¹åˆ«æ˜¯è¯åµŒå…¥(Word Embeddings)çš„å¯è§£é‡Šæ€§ã€‚é€šè¿‡æ„å»ºèŒƒç•´$\\mathcal{L}_T$ä¸$\\mathcal{P}_T$ï¼Œç ”ç©¶è€…ä¸ºæ–‡æœ¬è¯­ä¹‰æä¾›äº†å›¾ç¤ºåŒ–è¡¨ç¤ºï¼Œå¹¶ä»èŒƒç•´å­¦è§’åº¦é‡æ–°å®šä¹‰äº†æœ€å¤§æ¦‚ç‡å…ƒç´ çš„é€‰å–ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å•å­èŒƒç•´(Monoidal Category)$\\mathcal{P}_T$å®ç°äº†ä¸ç»´åº¦æ— å…³çš„è¯­ä¹‰ç©ºé—´å®šä¹‰ï¼Œä»…ä¾èµ–æ–‡æœ¬è‡ªèº«ä¿¡æ¯æ¥æå–è¯­ä¹‰ã€‚è®ºæ–‡è¿›ä¸€æ­¥å®šä¹‰äº†é…ç½®èŒƒç•´(Conf)ä¸è¯åµŒå…¥èŒƒç•´($\\mathcal{Emb}$)ï¼Œå¹¶å¼•å…¥åˆ†æ­§(Divergence)æ¦‚å¿µå»ºç«‹äº†æ¯”è¾ƒè¯åµŒå…¥çš„æ•°å­¦ç²¾ç¡®æ–¹æ³•ã€‚ç ”ç©¶è¯æ˜äº†GloVeã€Word2Vecä¸åº¦é‡å¤šç»´å°ºåº¦å˜æ¢(Metric MDS)ç®—æ³•ä¹‹é—´çš„ç­‰ä»·æ€§ï¼Œå®ç°äº†ä»é»‘ç›’ç¥ç»ç½‘ç»œå‘é€æ˜æ¡†æ¶çš„è½¬å˜ã€‚æœ€åï¼Œè¯¥å·¥ä½œè¿˜æä¾›äº†è®¡ç®—å’Œç¼“è§£è¯­ä¹‰ç©ºé—´åå·®çš„æ•°å­¦æ–¹æ¡ˆï¼Œæ˜¾è‘—æ¨è¿›äº†å¯è§£é‡Šäººå·¥æ™ºèƒ½(Explainable AI)é¢†åŸŸçš„å‘å±•ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "math.CT"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20701v1",
      "published_date": "2025-08-28 12:19:34 UTC",
      "updated_date": "2025-08-28 12:19:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:53:42.184417+00:00"
    },
    {
      "arxiv_id": "2508.20700v2",
      "title": "Generative Annotation for ASR Named Entity Correction",
      "title_zh": "ç”¨äº ASR å‘½åå®ä½“çº é”™çš„ç”Ÿæˆå¼æ ‡æ³¨",
      "authors": [
        "Yuanchang Luo",
        "Daimeng Wei",
        "Shaojun Li",
        "Hengchao Shang",
        "Jiaxin Guo",
        "Zongyao Li",
        "Zhanglin Wu",
        "Xiaoyu Chen",
        "Zhiqiang Rao",
        "Jinlong Yang",
        "Hao Yang"
      ],
      "abstract": "End-to-end automatic speech recognition systems often fail to transcribe domain-specific named entities, causing catastrophic failures in downstream tasks. Numerous fast and lightweight named entity correction (NEC) models have been proposed in recent years. These models, mainly leveraging phonetic-level edit distance algorithms, have shown impressive performances. However, when the forms of the wrongly-transcribed words(s) and the ground-truth entity are significantly different, these methods often fail to locate the wrongly transcribed words in hypothesis, thus limiting their usage. We propose a novel NEC method that utilizes speech sound features to retrieve candidate entities. With speech sound features and candidate entities, we inovatively design a generative method to annotate entity errors in ASR transcripts and replace the text with correct entities. This method is effective in scenarios of word form difference. We test our method using open-source and self-constructed test sets. The results demonstrate that our NEC method can bring significant improvement to entity accuracy. The self-constructed training data and test set is publicly available at github.com/L6-NLP/Generative-Annotation-NEC.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç«¯åˆ°ç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)ç³»ç»Ÿåœ¨ç‰¹å®šé¢†åŸŸå‘½åå®ä½“(Named Entity)è½¬å†™ä¸Šçš„å¤±æ•ˆé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ›æ–°çš„ç”Ÿæˆå¼å‘½åå®ä½“çº é”™(NEC)æ–¹æ³•ã€‚ç›¸è¾ƒäºä¼ ç»Ÿä¸»è¦ä¾èµ–éŸ³ç´ çº§ç¼–è¾‘è·ç¦»(Phonetic-level edit distance)ç®—æ³•ä¸”åœ¨è¯å½¢å·®å¼‚æ˜¾è‘—æ—¶è¡¨ç°ä¸ä½³çš„æ¨¡å‹ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è¯­éŸ³å£°éŸ³ç‰¹å¾(Speech sound features)æ£€ç´¢å€™é€‰å®ä½“ã€‚é€šè¿‡ç»“åˆå£°éŸ³ç‰¹å¾ä¸å€™é€‰åˆ—è¡¨ï¼Œç ”ç©¶è€…è®¾è®¡äº†ä¸€ç§ç”Ÿæˆå¼æ ‡æ³¨æœºåˆ¶ï¼Œèƒ½æœ‰æ•ˆå®šä½å¹¶ä¿®æ­£ASRè½¬å†™æ–‡æœ¬ä¸­çš„å®ä½“é”™è¯¯ã€‚åœ¨å¼€æºå’Œè‡ªå»ºæ•°æ®é›†ä¸Šçš„æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è¯å½¢å·®å¼‚è¾ƒå¤§çš„å¤æ‚åœºæ™¯ä¸‹ä¾ç„¶ç¨³å¥ï¼Œæ˜¾è‘—æå‡äº†å‘½åå®ä½“çš„è½¬å†™å‡†ç¡®ç‡ã€‚ç›®å‰ï¼Œè¯¥ç ”ç©¶æ‰€ç”¨çš„æ•°æ®é›†ä¸ä»£ç å·²åœ¨GitHubå…¬å¼€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 7 figures, 7 tables, EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.20700v2",
      "published_date": "2025-08-28 12:18:35 UTC",
      "updated_date": "2025-10-24 09:35:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:53:49.287564+00:00"
    },
    {
      "arxiv_id": "2508.20691v1",
      "title": "MobileCLIP2: Improving Multi-Modal Reinforced Training",
      "title_zh": "MobileCLIP2ï¼šå¤šæ¨¡æ€å¢å¼ºå¼è®­ç»ƒçš„æ”¹è¿›",
      "authors": [
        "Fartash Faghri",
        "Pavan Kumar Anasosalu Vasu",
        "Cem Koc",
        "Vaishaal Shankar",
        "Alexander Toshev",
        "Oncel Tuzel",
        "Hadi Pouransari"
      ],
      "abstract": "Foundation image-text models such as CLIP with zero-shot capabilities enable a wide array of applications. MobileCLIP is a recent family of image-text models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot accuracy. The main ingredients in MobileCLIP were its low-latency and light architectures and a novel multi-modal reinforced training that made knowledge distillation from multiple caption-generators and CLIP teachers efficient, scalable, and reproducible. In this paper, we improve the multi-modal reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles trained on the DFN dataset, 2) improved captioner teachers trained on the DFN dataset and fine-tuned on a diverse selection of high-quality image-caption datasets. We discover new insights through ablations such as the importance of temperature tuning in contrastive knowledge distillation, the effectiveness of caption-generator fine-tuning for caption diversity, and the additive improvement from combining synthetic captions generated by multiple models. We train a new family of models called MobileCLIP2 and achieve state-of-the-art ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe 2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\\times$ smaller and improves on DFN ViT-L/14 at 2.5$\\times$ lower latency. We release our pretrained models (https://github.com/apple/ml-mobileclip) and the data generation code (https://github.com/apple/ml-mobileclip-dr). The data generation code makes it easy to create new reinforced datasets with arbitrary teachers using distributed scalable processing.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MobileCLIP2ï¼Œé€šè¿‡æ”¹è¿›å¤šæ¨¡æ€å¼ºåŒ–è®­ç»ƒ (multi-modal reinforced training) æ˜¾è‘—æå‡äº†ä½å»¶è¿Ÿè§†è§‰è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬ (zero-shot) å‡†ç¡®ç‡ã€‚ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†åŸºäº DFN æ•°æ®é›†è®­ç»ƒçš„æ›´ä¼˜ CLIP æ•™å¸ˆé›†æˆï¼Œå¹¶ç»“åˆäº†ç»ç”±é«˜è´¨é‡æ•°æ®é›†å¾®è°ƒçš„å›¾åƒæ ‡æ³¨ (captioner) æ•™å¸ˆæ¨¡å‹ã€‚é€šè¿‡æ¶ˆèå®éªŒï¼Œä½œè€…å‘ç°äº†å¯¹æ¯”çŸ¥è¯†è’¸é¦ (contrastive knowledge distillation) ä¸­æ¸©åº¦è°ƒèŠ‚çš„å…³é”®ä½œç”¨ï¼Œä»¥åŠç»“åˆå¤šç§åˆæˆæ ‡æ³¨å¯¹æå‡æ•°æ®å¤šæ ·æ€§çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMobileCLIP2-B åœ¨ ImageNet-1k ä¸Šçš„å‡†ç¡®ç‡æ¯”å‰ä»£æå‡äº† 2.2%ï¼Œè€Œ MobileCLIP2-S4 åœ¨å‚æ•°é‡ä»…ä¸ºä¸€åŠçš„æƒ…å†µä¸‹è¾¾åˆ°äº†ä¸ SigLIP-SO400M/14 ç›¸å½“çš„æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥ç³»åˆ—æ¨¡å‹åœ¨å»¶è¿Ÿæ¯” DFN ViT-L/14 ä½ 2.5 å€çš„æƒ…å†µä¸‹å®ç°äº†æ›´ä¼˜æ€§èƒ½ï¼Œç ”ç©¶å›¢é˜ŸåŒæ­¥å¼€æºäº†æ¨¡å‹åŠåˆ†å¸ƒå¼æ•°æ®ç”Ÿæˆä»£ç ï¼Œä¸ºé«˜æ•ˆåˆ›å»ºè‡ªå®šä¹‰çš„å¤šæ¨¡æ€å¼ºåŒ–æ•°æ®é›†æä¾›äº†æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "TMLR August 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.20691v1",
      "published_date": "2025-08-28 11:50:22 UTC",
      "updated_date": "2025-08-28 11:50:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:53:33.962900+00:00"
    },
    {
      "arxiv_id": "2508.20688v1",
      "title": "Task Allocation for Autonomous Machines using Computational Intelligence and Deep Reinforcement Learning",
      "title_zh": "åŸºäºè®¡ç®—æ™ºèƒ½ä¸æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è‡ªä¸»æœºå™¨ä»»åŠ¡åˆ†é…",
      "authors": [
        "Thanh Thi Nguyen",
        "Quoc Viet Hung Nguyen",
        "Jonathan Kua",
        "Imran Razzak",
        "Dung Nguyen",
        "Saeid Nahavandi"
      ],
      "abstract": "Enabling multiple autonomous machines to perform reliably requires the development of efficient cooperative control algorithms. This paper presents a survey of algorithms that have been developed for controlling and coordinating autonomous machines in complex environments. We especially focus on task allocation methods using computational intelligence (CI) and deep reinforcement learning (RL). The advantages and disadvantages of the surveyed methods are analysed thoroughly. We also propose and discuss in detail various future research directions that shed light on how to improve existing algorithms or create new methods to enhance the employability and performance of autonomous machines in real-world applications. The findings indicate that CI and deep RL methods provide viable approaches to addressing complex task allocation problems in dynamic and uncertain environments. The recent development of deep RL has greatly contributed to the literature on controlling and coordinating autonomous machines, and it has become a growing trend in this area. It is envisaged that this paper will provide researchers and engineers with a comprehensive overview of progress in machine learning research related to autonomous machines. It also highlights underexplored areas, identifies emerging methodologies, and suggests new avenues for exploration in future research within this domain.",
      "tldr_zh": "æœ¬æ–‡ç»¼è¿°äº†ç”¨äºå¤šè‡ªä¸»æœºå™¨åä½œæ§åˆ¶ä¸åè°ƒçš„ç®—æ³•ï¼Œç‰¹åˆ«å…³æ³¨åŸºäºè®¡ç®—æ™ºèƒ½ (Computational Intelligence, CI) å’Œæ·±åº¦å¼ºåŒ–å­¦ä¹  (Deep Reinforcement Learning, RL) çš„ä»»åŠ¡åˆ†é…æ–¹æ³•ã€‚ç ”ç©¶æ·±å…¥åˆ†æäº†ç°æœ‰å„ç±»æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œæ¶µç›–äº†åœ¨åŠ¨æ€å’Œä¸ç¡®å®šç¯å¢ƒä¸­è§£å†³å¤æ‚ä»»åŠ¡åˆ†é…é—®é¢˜çš„å¤šç§è·¯å¾„ã€‚è°ƒæŸ¥ç»“æœè¡¨æ˜ï¼ŒCI å’Œæ·±åº¦ RL ä¸ºå¤„ç†å¤æ‚åœºæ™¯æä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼Œä¸”æ·±åº¦ RL çš„æœ€æ–°è¿›å±•å·²æˆä¸ºè¯¥é¢†åŸŸä¸æ–­å¢é•¿çš„ç ”ç©¶è¶‹åŠ¿ã€‚æ–‡ç« è¯¦ç»†è®¨è®ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨é€šè¿‡æ”¹è¿›ç°æœ‰ç®—æ³•æˆ–åˆ›å»ºæ–°æ–¹æ³•æ¥æå‡è‡ªä¸»æœºå™¨åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½ä¸å¯éƒ¨ç½²æ€§ã€‚è¯¥ç»¼è¿°ä¸ºç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆæä¾›äº†æœºå™¨å­¦ä¹ åœ¨è‡ªä¸»æœºå™¨é¢†åŸŸè¿›å±•çš„å…¨é¢æ¦‚è§ˆï¼Œå¹¶æŒ‡å‡ºäº†å°šæœªå……åˆ†æ¢ç´¢çš„é¢†åŸŸåŠæ–°å…´æ–¹æ³•è®ºï¼Œä¸ºæœªæ¥çš„æ¢ç´¢æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted for publication in the Proceedings of the 2025 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",
      "pdf_url": "https://arxiv.org/pdf/2508.20688v1",
      "published_date": "2025-08-28 11:48:55 UTC",
      "updated_date": "2025-08-28 11:48:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:53:48.493740+00:00"
    },
    {
      "arxiv_id": "2508.20674v1",
      "title": "Bridging Minds and Machines: Toward an Integration of AI and Cognitive Science",
      "title_zh": "è”ç»“å¿ƒæ™ºä¸æœºå™¨ï¼šè¿ˆå‘äººå·¥æ™ºèƒ½ä¸è®¤çŸ¥ç§‘å­¦çš„èåˆ",
      "authors": [
        "Rui Mao",
        "Qian Liu",
        "Xiao Li",
        "Erik Cambria",
        "Amir Hussain"
      ],
      "abstract": "Cognitive Science has profoundly shaped disciplines such as Artificial Intelligence (AI), Philosophy, Psychology, Neuroscience, Linguistics, and Culture. Many breakthroughs in AI trace their roots to cognitive theories, while AI itself has become an indispensable tool for advancing cognitive research. This reciprocal relationship motivates a comprehensive review of the intersections between AI and Cognitive Science. By synthesizing key contributions from both perspectives, we observe that AI progress has largely emphasized practical task performance, whereas its cognitive foundations remain conceptually fragmented. We argue that the future of AI within Cognitive Science lies not only in improving performance but also in constructing systems that deepen our understanding of the human mind. Promising directions include aligning AI behaviors with cognitive frameworks, situating AI in embodiment and culture, developing personalized cognitive models, and rethinking AI ethics through cognitive co-evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½ (AI) ä¸è®¤çŸ¥ç§‘å­¦ (Cognitive Science) ä¹‹é—´çš„ç›¸äº’å½±å“ä¸èåˆè·¯å¾„ï¼Œæ—¨åœ¨é€šè¿‡ç»¼åˆä¸¤è€…çš„å…³é”®è´¡çŒ®æ¥æ¨åŠ¨è·¨å­¦ç§‘å‘å±•ã€‚å°½ç®¡ AI çš„è¯¸å¤šçªç ´æºäºè®¤çŸ¥ç†è®ºï¼Œä¸” AI å·²æˆä¸ºè®¤çŸ¥ç ”ç©¶çš„é‡è¦å·¥å…·ï¼Œä½†ç›®å‰çš„æŠ€æœ¯è¿›å±•ä¸»è¦é›†ä¸­åœ¨å®é™…ä»»åŠ¡æ€§èƒ½ä¸Šï¼Œå…¶è®¤çŸ¥åŸºç¡€åœ¨æ¦‚å¿µä¸Šä»æ˜¾ç ´ç¢ã€‚ä½œè€…ä¸»å¼ ï¼Œè®¤çŸ¥ç§‘å­¦æ¡†æ¶ä¸‹ AI çš„æœªæ¥ä¸ä»…åœ¨äºæå‡æ€§èƒ½ï¼Œæ›´åœ¨äºæ„å»ºèƒ½æ·±åŒ–äººç±»å¯¹å¤§è„‘ç†è§£çš„ç³»ç»Ÿã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†å°† AI è¡Œä¸ºä¸è®¤çŸ¥æ¡†æ¶å¯¹é½ã€å°† AI ç½®äºå…·èº« (Embodiment) ä¸æ–‡åŒ–è¯­å¢ƒä¸­ã€å¼€å‘ä¸ªæ€§åŒ–è®¤çŸ¥æ¨¡å‹ä»¥åŠé€šè¿‡è®¤çŸ¥å…±åŒè¯„ä¼° (Cognitive Co-evaluation) é‡æ–°å®¡è§†äººå·¥æ™ºèƒ½ä¼¦ç† (AI Ethics) ç­‰å…³é”®æ–¹å‘ã€‚è¯¥ç»¼è¿°ä¸ºä¿ƒè¿›äººç±»æ€ç»´ä¸æœºå™¨æ™ºèƒ½çš„æ·±åº¦æ•´åˆæä¾›äº†ç³»ç»Ÿæ€§çš„å‰ç»æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20674v1",
      "published_date": "2025-08-28 11:26:17 UTC",
      "updated_date": "2025-08-28 11:26:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:54:07.984098+00:00"
    },
    {
      "arxiv_id": "2508.20665v1",
      "title": "Amadeus: Autoregressive Model with Bidirectional Attribute Modelling for Symbolic Music",
      "title_zh": "Amadeusï¼šåŸºäºåŒå‘å±æ€§å»ºæ¨¡çš„ç¬¦å·éŸ³ä¹è‡ªå›å½’æ¨¡å‹",
      "authors": [
        "Hongju Su",
        "Ke Li",
        "Lan Yang",
        "Honggang Zhang",
        "Yi-Zhe Song"
      ],
      "abstract": "Existing state-of-the-art symbolic music generation models predominantly adopt autoregressive or hierarchical autoregressive architectures, modelling symbolic music as a sequence of attribute tokens with unidirectional temporal dependencies, under the assumption of a fixed, strict dependency structure among these attributes. However, we observe that using different attributes as the initial token in these models leads to comparable performance. This suggests that the attributes of a musical note are, in essence, a concurrent and unordered set, rather than a temporally dependent sequence. Based on this insight, we introduce Amadeus, a novel symbolic music generation framework. Amadeus adopts a two-level architecture: an autoregressive model for note sequences and a bidirectional discrete diffusion model for attributes. To enhance performance, we propose Music Latent Space Discriminability Enhancement Strategy(MLSDES), incorporating contrastive learning constraints that amplify discriminability of intermediate music representations. The Conditional Information Enhancement Module (CIEM) simultaneously strengthens note latent vector representation via attention mechanisms, enabling more precise note decoding. We conduct extensive experiments on unconditional and text-conditioned generation tasks. Amadeus significantly outperforms SOTA models across multiple metrics while achieving at least 4$\\times$ speed-up. Furthermore, we demonstrate training-free, fine-grained note attribute control feasibility using our model. To explore the upper performance bound of the Amadeus architecture, we compile the largest open-source symbolic music dataset to date, AMD (Amadeus MIDI Dataset), supporting both pre-training and fine-tuning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Amadeusï¼Œä¸€ç§åˆ›æ–°çš„ç¬¦å·éŸ³ä¹ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹å°†éŸ³ç¬¦å±æ€§è§†ä¸ºå•å‘ä¾èµ–åºåˆ—è€Œå¿½è§†å…¶å¹¶å‘æ€§çš„å±€é™ã€‚Amadeus é‡‡ç”¨åŒå±‚æ¶æ„ï¼Œç»“åˆäº†ç”¨äºå¤„ç†éŸ³ç¬¦åºåˆ—çš„ Autoregressive æ¨¡å‹å’Œç”¨äºå±æ€§å»ºæ¨¡çš„ Bidirectional discrete diffusion æ¨¡å‹ã€‚ä¸ºäº†ä¼˜åŒ–ç”Ÿæˆæ•ˆæœï¼Œç ”ç©¶è€…æå‡ºäº† Music Latent Space Discriminability Enhancement Strategy (MLSDES) ä»¥å¢å¼ºä¸­é—´è¡¨å¾çš„åˆ¤åˆ«åŠ›ï¼Œå¹¶åˆ©ç”¨ Conditional Information Enhancement Module (CIEM) æå‡éŸ³ç¬¦è§£ç çš„ç²¾ç¡®åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAmadeus åœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äº SOTA æ¨¡å‹ï¼ŒåŒæ—¶å®ç°äº†è‡³å°‘ 4 å€çš„æ¨ç†åŠ é€Ÿã€‚è¯¥æ¡†æ¶è¿˜æ”¯æŒå…è®­ç»ƒçš„ç»†ç²’åº¦å±æ€§æ§åˆ¶ï¼Œå¹¶åŒæ­¥å‘å¸ƒäº†ç›®å‰æœ€å¤§çš„å¼€æºç¬¦å·éŸ³ä¹æ•°æ®é›† Amadeus MIDI Dataset (AMD) ä»¥æ”¯æŒé¢„è®­ç»ƒä¸å¾®è°ƒã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.SD",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2508.20665v1",
      "published_date": "2025-08-28 11:15:44 UTC",
      "updated_date": "2025-08-28 11:15:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:54:09.190621+00:00"
    },
    {
      "arxiv_id": "2508.20664v1",
      "title": "Task-Oriented Edge-Assisted Cross-System Design for Real-Time Human-Robot Interaction in Industrial Metaverse",
      "title_zh": "å·¥ä¸šå…ƒå®‡å®™ä¸­é¢å‘ä»»åŠ¡çš„è¾¹ç¼˜è¾…åŠ©å®æ—¶äººæœºäº¤äº’è·¨ç³»ç»Ÿè®¾è®¡",
      "authors": [
        "Kan Chen",
        "Zhen Meng",
        "Xiangmin Xu",
        "Jiaming Yang",
        "Emma Li",
        "Philip G. Zhao"
      ],
      "abstract": "Real-time human-device interaction in industrial Metaverse faces challenges such as high computational load, limited bandwidth, and strict latency. This paper proposes a task-oriented edge-assisted cross-system framework using digital twins (DTs) to enable responsive interactions. By predicting operator motions, the system supports: 1) proactive Metaverse rendering for visual feedback, and 2) preemptive control of remote devices. The DTs are decoupled into two virtual functions-visual display and robotic control-optimizing both performance and adaptability. To enhance generalizability, we introduce the Human-In-The-Loop Model-Agnostic Meta-Learning (HITL-MAML) algorithm, which dynamically adjusts prediction horizons. Evaluation on two tasks demonstrates the framework's effectiveness: in a Trajectory-Based Drawing Control task, it reduces weighted RMSE from 0.0712 m to 0.0101 m; in a real-time 3D scene representation task for nuclear decommissioning, it achieves a PSNR of 22.11, SSIM of 0.8729, and LPIPS of 0.1298. These results show the framework's capability to ensure spatial precision and visual fidelity in real-time, high-risk industrial environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥ä¸šå…ƒå®‡å®™(Industrial Metaverse)ä¸­é«˜è®¡ç®—è´Ÿè½½ã€æœ‰é™å¸¦å®½å’Œä¸¥æ ¼å»¶è¿Ÿç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§é¢å‘ä»»åŠ¡ä¸”è¾¹ç¼˜è¾…åŠ©çš„è·¨ç³»ç»Ÿè®¾è®¡æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å“åº”å¼çš„äººæœºå®æ—¶äº¤äº’ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ•°å­—å­ªç”Ÿ(Digital Twins)æŠ€æœ¯ï¼Œé€šè¿‡é¢„æµ‹æ“ä½œå‘˜åŠ¨ä½œæ¥æ”¯æŒä¸»åŠ¨çš„å…ƒå®‡å®™æ¸²æŸ“å’Œè¿œç¨‹è®¾å¤‡çš„æŠ¢å å¼æ§åˆ¶ã€‚ç ”ç©¶å°†æ•°å­—å­ªç”Ÿè§£è€¦ä¸ºè§†è§‰æ˜¾ç¤ºå’Œæœºå™¨äººæ§åˆ¶ä¸¤ä¸ªè™šæ‹ŸåŠŸèƒ½ï¼Œæ˜¾è‘—ä¼˜åŒ–äº†ç³»ç»Ÿçš„æ€§èƒ½ä¸é€‚åº”æ€§ã€‚ä¸ºå¢å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œè®ºæ–‡å¼•å…¥äº†äººæœºååŒæ¨¡å‹ä¸å¯çŸ¥å…ƒå­¦ä¹ (Human-In-The-Loop Model-Agnostic Meta-Learning, HITL-MAML)ç®—æ³•ï¼Œç”¨ä»¥åŠ¨æ€è°ƒæ•´é¢„æµ‹èŒƒå›´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åŸºäºè½¨è¿¹çš„ç»˜å›¾æ§åˆ¶ä»»åŠ¡ä¸­ï¼Œè¯¥æ¡†æ¶å°†åŠ æƒå‡æ–¹æ ¹è¯¯å·®(RMSE)ä»0.0712ç±³å¤§å¹…é™ä½è‡³0.0101ç±³ï¼›åœ¨æ ¸è®¾æ–½é€€å½¹çš„å®æ—¶ä¸‰ç»´åœºæ™¯è¡¨ç¤ºä»»åŠ¡ä¸­ï¼Œç³»ç»Ÿä¹Ÿå®ç°äº†é«˜ä¿çœŸåº¦çš„è§†è§‰åé¦ˆã€‚è¯¥ç ”ç©¶è¯æ˜äº†è¯¥æ¡†æ¶åœ¨å®æ—¶ã€é«˜é£é™©å·¥ä¸šç¯å¢ƒä¸‹ç¡®ä¿ç©ºé—´ç²¾åº¦å’Œè§†è§‰çœŸå®æ€§çš„å“è¶Šèƒ½åŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.RO",
      "comment": "This paper has submitted to IEEE Transactions on Mobile Computing",
      "pdf_url": "https://arxiv.org/pdf/2508.20664v1",
      "published_date": "2025-08-28 11:10:41 UTC",
      "updated_date": "2025-08-28 11:10:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:54:13.693381+00:00"
    },
    {
      "arxiv_id": "2508.20637v2",
      "title": "GDS Agent for Graph Algorithmic Reasoning",
      "title_zh": "é¢å‘å›¾ç®—æ³•æ¨ç†çš„ GDS Agent",
      "authors": [
        "Borun Shi",
        "Ioannis Panagiotas"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable multimodal information processing and reasoning ability. When equipped with tools through function calling and enhanced with retrieval-augmented techniques, compound LLM-based systems can access closed data sources and answer questions about them. However, they still struggle to process and reason over large-scale graph-structure data. We introduce the GDS (Graph Data Science) agent in this technical report. The GDS agent introduces a comprehensive set of graph algorithms as tools, together with preprocessing (retrieval) and postprocessing of algorithm results, in a model context protocol (MCP) server. The server can be used with any modern LLM out-of-the-box. GDS agent allows users to ask any question that implicitly and intrinsically requires graph algorithmic reasoning about their data, and quickly obtain accurate and grounded answers. We introduce new benchmarks that evaluate intermediate tool calls as well as final responses. The results indicate that GDS agent is able to solve a wide spectrum of graph tasks. We also provide detailed case studies for more open-ended tasks and study scenarios where the agent struggles. Finally, we discuss the remaining challenges and the future roadmap.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤„ç†å’Œæ¨ç†å¤§è§„æ¨¡å›¾ç»“æ„æ•°æ®æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº† GDS (Graph Data Science) agentã€‚è¯¥ agent é€šè¿‡ Model Context Protocol (MCP) æœåŠ¡å™¨ï¼Œå°†ä¸€ç³»åˆ—å…¨é¢çš„å›¾ç®—æ³•ä½œä¸ºå·¥å…·å¼•å…¥ï¼Œå¹¶é›†æˆäº†ç®—æ³•ç»“æœçš„é¢„å¤„ç†ï¼ˆæ£€ç´¢ï¼‰ä¸åå¤„ç†æµç¨‹ã€‚GDS agent æ”¯æŒç°ä»£ LLMs å¼€ç®±å³ç”¨ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé’ˆå¯¹å…¶æ•°æ®æå‡ºéšå«å›¾ç®—æ³•æ¨ç†éœ€æ±‚çš„ä»»ä½•é—®é¢˜ï¼Œå¹¶å¿«é€Ÿè·å¾—å‡†ç¡®ä¸”æœ‰æ ¹æ®çš„å›ç­”ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¼•å…¥è¯„ä¼°ä¸­é—´å·¥å…·è°ƒç”¨å’Œæœ€ç»ˆå“åº”çš„æ–°åŸºå‡†ï¼Œè¯æ˜äº† GDS agent åœ¨å¹¿æ³›çš„å›¾ä»»åŠ¡ä¸­å…·æœ‰å‡ºè‰²çš„è§£å†³èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯æŠ¥å‘Šè¿˜é€šè¿‡æ¡ˆä¾‹ç ”ç©¶æ¢è®¨äº† agent åœ¨å¼€æ”¾æ€§ä»»åŠ¡ä¸­çš„è¡¨ç°åŠå½“å‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥å›¾æ™ºèƒ½æ¨ç†çš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Technical report",
      "pdf_url": "https://arxiv.org/pdf/2508.20637v2",
      "published_date": "2025-08-28 10:35:44 UTC",
      "updated_date": "2025-11-05 18:39:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:54:27.790031+00:00"
    },
    {
      "arxiv_id": "2508.20626v1",
      "title": "ArtFace: Towards Historical Portrait Face Identification via Model Adaptation",
      "title_zh": "ArtFaceï¼šåŸºäºæ¨¡å‹é€‚é…çš„å†å²è‚–åƒäººè„¸è¯†åˆ«",
      "authors": [
        "Francois Poh",
        "Anjith George",
        "SÃ©bastien Marcel"
      ],
      "abstract": "Identifying sitters in historical paintings is a key task for art historians, offering insight into their lives and how they chose to be seen. However, the process is often subjective and limited by the lack of data and stylistic variations. Automated facial recognition is capable of handling challenging conditions and can assist, but while traditional facial recognition models perform well on photographs, they struggle with paintings due to domain shift and high intra-class variation. Artistic factors such as style, skill, intent, and influence from other works further complicate recognition. In this work, we investigate the potential of foundation models to improve facial recognition in artworks. By fine-tuning foundation models and integrating their embeddings with those from conventional facial recognition networks, we demonstrate notable improvements over current state-of-the-art methods. Our results show that foundation models can bridge the gap where traditional methods are ineffective. Paper page at https://www.idiap.ch/paper/artface/",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å†å²è‚–åƒç”»ä¸­çš„åè€…èº«ä»½è¯†åˆ«éš¾é¢˜ï¼Œæå‡ºäº†ArtFaceæ¡†æ¶ï¼Œä»¥è§£å†³ä¼ ç»Ÿé¢éƒ¨è¯†åˆ«ï¼ˆFacial Recognitionï¼‰æ¨¡å‹åœ¨å¤„ç†è‰ºæœ¯ä½œå“æ—¶é¢ä¸´çš„é¢†åŸŸåç§»ï¼ˆDomain Shiftï¼‰å’Œé«˜åº¦ç±»å†…å˜å¼‚ï¼ˆIntra-class Variationï¼‰ç­‰é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿæ¢ç´¢äº†åŸºç¡€æ¨¡å‹ï¼ˆFoundation Modelsï¼‰åœ¨è‰ºæœ¯é¢†åŸŸè¯†åˆ«ä¸­çš„æ½œåŠ›ï¼Œé€šè¿‡å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ˆFine-tuningï¼‰ï¼Œå¹¶å°†å…¶ç”Ÿæˆçš„åµŒå…¥ï¼ˆEmbeddingsï¼‰ä¸ä¼ ç»Ÿè¯†åˆ«ç½‘ç»œçš„ç‰¹å¾è¿›è¡Œé›†æˆï¼Œæ˜¾è‘—æå‡äº†è¯†åˆ«æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‰ºæœ¯å›¾åƒè¯†åˆ«ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒæˆåŠŸå¼¥è¡¥äº†ä¼ ç»ŸæŠ€æœ¯åœ¨å¤„ç†é£æ ¼åŒ–è‰ºæœ¯ä½œå“æ—¶çš„å±€é™æ€§ã€‚ArtFaceçš„ç ”ç©¶æˆæœä¸ºè‰ºæœ¯å²ç ”ç©¶æä¾›äº†æ›´ç²¾ç¡®çš„è‡ªåŠ¨åŒ–è¾…åŠ©æ‰‹æ®µï¼Œå±•ç¤ºäº†è·¨é¢†åŸŸæ¨¡å‹é€‚é…ï¼ˆModel Adaptationï¼‰åœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­çš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "4 pages, 3 figures. ArtMetrics @ ICCV 2025 (non-archival). Paper page at https://www.idiap.ch/paper/artface/",
      "pdf_url": "https://arxiv.org/pdf/2508.20626v1",
      "published_date": "2025-08-28 10:19:06 UTC",
      "updated_date": "2025-08-28 10:19:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:54:26.696147+00:00"
    },
    {
      "arxiv_id": "2509.00110v1",
      "title": "The Application of Virtual Environments and Artificial Intelligence in Higher Education: Experimental Findings in Philosophy Teaching",
      "title_zh": "è™šæ‹Ÿç¯å¢ƒä¸äººå·¥æ™ºèƒ½åœ¨é«˜ç­‰æ•™è‚²ä¸­çš„åº”ç”¨ï¼šå“²å­¦æ•™å­¦å®éªŒå‘ç°",
      "authors": [
        "Adel Vehrer",
        "Zsolt Palfalusi"
      ],
      "abstract": "This study explores how virtual environments and artificial intelligence can enhance university students' learning experiences, with particular attention to the digital preferences of Generation Z. An experiment was conducted at the Faculty of Pedagogy, Humanities, and Social Sciences at University of Gyor, where Walter's Cube technology and a trained AI mediator were integrated into the instruction of ten philosophical topics. The curriculum was aligned with the official syllabus and enriched with visual content, quotations, and explanatory texts related to iconic figures in philosophy. A total of 77 first-year undergraduate students from full-time humanities and social sciences programs participated in the study. Following their end-of-semester offline written examination, students voluntarily completed a paper-based, anonymous ten-question test and provided feedback on the method's effectiveness. No sensitive personal data were collected, and the research was conducted with formal approval from the Faculty Dean. Descriptive statistics and inferential tests were applied to evaluate the impact of the virtual environment and AI mediation on learning outcomes. Results indicate that 80 percent of participants achieved good or excellent final exam grades, and the majority rated the virtual material as highly effective. Qualitative feedback emphasized increased motivation and deeper engagement, attributed to the immersive 3D presentation and interactive AI support. This research contributes to the advancement of digital pedagogy and suggests new directions for applying virtual and AI-based methods in higher education, particularly in disciplines where abstract reasoning and conceptual understanding are central.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†Virtual Environmentså’ŒArtificial Intelligenceå¦‚ä½•å¢å¼ºå¤§å­¦ç”Ÿçš„å­¦ä¹ ä½“éªŒï¼Œç‰¹åˆ«å…³æ³¨Generation Zçš„æ•°å­—åå¥½ã€‚åœ¨å®éªŒä¸­ï¼Œç ”ç©¶äººå‘˜å°†Walter's CubeæŠ€æœ¯å’Œç»è¿‡è®­ç»ƒçš„AI mediatoræ•´åˆåˆ°åä¸ªå“²å­¦ä¸»é¢˜çš„æ•™å­¦ä¸­ï¼Œå¹¶å¯¹77åæœ¬ç§‘ç”Ÿè¿›è¡Œäº†å®è¯ç ”ç©¶ã€‚ç»“æœæ˜¾ç¤ºï¼Œ80%çš„å‚ä¸è€…åœ¨æœŸæœ«è€ƒè¯•ä¸­å–å¾—äº†è‰¯å¥½æˆ–ä¼˜ç§€çš„æˆç»©ï¼Œä¸”ç»å¤§å¤šæ•°å­¦ç”Ÿè®¤ä¸ºè™šæ‹Ÿæ•™å­¦ææ–™å…·æœ‰æé«˜çš„æœ‰æ•ˆæ€§ã€‚å®šæ€§åé¦ˆè¿›ä¸€æ­¥å¼ºè°ƒï¼Œæ²‰æµ¸å¼çš„3D presentationå’Œäº¤äº’å¼çš„AI supportæ˜¾è‘—æå‡äº†å­¦ç”Ÿçš„å­¦ä¹ åŠ¨æœºä¸å‚ä¸åº¦ã€‚è¯¥ç ”ç©¶ä¸ºHigher Educationä¸­çš„Digital Pedagogyæä¾›äº†æ–°æ–¹å‘ï¼Œè¯æ˜äº†è™šæ‹Ÿä¸AIæŠ€æœ¯åœ¨å“²å­¦ç­‰æŠ½è±¡æ¨ç†å­¦ç§‘æ•™å­¦ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00110v1",
      "published_date": "2025-08-28 10:14:26 UTC",
      "updated_date": "2025-08-28 10:14:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:54:29.690547+00:00"
    },
    {
      "arxiv_id": "2508.20584v1",
      "title": "Flowing Straighter with Conditional Flow Matching for Accurate Speech Enhancement",
      "title_zh": "åŸºäºæ¡ä»¶æµåŒ¹é…çš„è·¯å¾„ç›´åŒ–å®ç°ç²¾å‡†è¯­éŸ³å¢å¼º",
      "authors": [
        "Mattias Cross",
        "Anton Ragni"
      ],
      "abstract": "Current flow-based generative speech enhancement methods learn curved probability paths which model a mapping between clean and noisy speech. Despite impressive performance, the implications of curved probability paths are unknown. Methods such as Schrodinger bridges focus on curved paths, where time-dependent gradients and variance do not promote straight paths. Findings in machine learning research suggest that straight paths, such as conditional flow matching, are easier to train and offer better generalisation. In this paper we quantify the effect of path straightness on speech enhancement quality. We report experiments with the Schrodinger bridge, where we show that certain configurations lead to straighter paths. Conversely, we propose independent conditional flow-matching for speech enhancement, which models straight paths between noisy and clean speech. We demonstrate empirically that a time-independent variance has a greater effect on sample quality than the gradient. Although conditional flow matching improves several speech quality metrics, it requires multiple inference steps. We rectify this with a one-step solution by inferring the trained flow-based model as if it was directly predictive. Our work suggests that straighter time-independent probability paths improve generative speech enhancement over curved time-dependent paths.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¯­éŸ³å¢å¼º (Speech Enhancement) ä¸­æ¦‚ç‡è·¯å¾„æ›²ç‡å¯¹ç”Ÿæˆè´¨é‡çš„å½±å“ï¼ŒæŒ‡å‡ºç›®å‰åŸºäºæµ (flow-based) çš„æ–¹æ³•å­¦ä¹ çš„å¼¯æ›²è·¯å¾„åœ¨è®­ç»ƒéš¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨å±€é™ã€‚ç ”ç©¶å¯¹æ¯”äº† SchrÃ¶dinger bridges å¹¶æå‡ºç‹¬ç«‹æ¡ä»¶æµåŒ¹é… (Independent Conditional Flow Matching, ICFM) æ¡†æ¶ï¼Œæ—¨åœ¨æ„å»ºå™ªå£°è¯­éŸ³ä¸æ¸…æ™°è¯­éŸ³ä¹‹é—´çš„ç›´çº¿æ˜ å°„è·¯å¾„ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ—¶é—´æ— å…³çš„æ–¹å·®å¯¹æ ·æœ¬è´¨é‡çš„å½±å“è¿œè¶…æ¢¯åº¦ï¼Œä¸”ç›´çº¿è·¯å¾„åœ¨å¤šé¡¹è¯­éŸ³è´¨é‡æŒ‡æ ‡ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„æ—¶é—´ç›¸å…³å¼¯æ›²è·¯å¾„ã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡å°†è®­ç»ƒå¥½çš„æµæ¨¡å‹ä½œä¸ºç›´æ¥é¢„æµ‹æ¨¡å‹ï¼Œè§£å†³äº†æ¨ç†æ­¥éª¤è¿‡å¤šçš„é—®é¢˜ï¼Œå®ç°äº†é«˜æ•ˆçš„ä¸€æ­¥ (one-step) æ¨ç†ã€‚è¯¥å·¥ä½œè¯æ˜äº†ä¿æŒæ¦‚ç‡è·¯å¾„çš„ç›´çº¿æ€§å’Œæ—¶é—´æ— å…³æ€§æ˜¯æå‡ç”Ÿæˆå¼è¯­éŸ³å¢å¼ºæ€§èƒ½çš„å…³é”®ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SD",
      "comment": "preprint, accepted",
      "pdf_url": "https://arxiv.org/pdf/2508.20584v1",
      "published_date": "2025-08-28 09:21:22 UTC",
      "updated_date": "2025-08-28 09:21:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:54:40.783463+00:00"
    },
    {
      "arxiv_id": "2508.20583v1",
      "title": "A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models",
      "title_zh": "å›¾åœ¨â€œè¯´è¯â€ï¼Œè°åœ¨å€¾å¬ï¼Ÿå¯¹å›¾è¯­è¨€æ¨¡å‹è¯„ä¼°çš„é‡æ–°æ€è€ƒ",
      "authors": [
        "Soham Petkar",
        "Hari Aakash K",
        "Anirudh Vempati",
        "Akshit Sinha",
        "Ponnurangam Kumarauguru",
        "Chirag Agarwal"
      ],
      "abstract": "Developments in Graph-Language Models (GLMs) aim to integrate the structural reasoning capabilities of Graph Neural Networks (GNNs) with the semantic understanding of Large Language Models (LLMs). However, we demonstrate that current evaluation benchmarks for GLMs, which are primarily repurposed node-level classification datasets, are insufficient to assess multimodal reasoning. Our analysis reveals that strong performance on these benchmarks is achievable using unimodal information alone, suggesting that they do not necessitate graph-language integration. To address this evaluation gap, we introduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed to evaluate multimodal reasoning at various complexity levels. Our benchmark employs a synthetic graph generation pipeline paired with questions that require joint reasoning over structure and textual semantics. We perform a thorough evaluation of representative GLM architectures and find that soft-prompted LLM baselines perform on par with GLMs that incorporate a full GNN backbone. This result calls into question the architectural necessity of incorporating graph structure into LLMs. We further show that GLMs exhibit significant performance degradation in tasks that require structural reasoning. These findings highlight limitations in the graph reasoning capabilities of current GLMs and provide a foundation for advancing the community toward explicit multimodal reasoning involving graph structure and language.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾è¯­è¨€æ¨¡å‹(GLMs)çš„è¯„ä¼°ç°çŠ¶è¿›è¡Œäº†é‡æ–°æ€è€ƒï¼ŒæŒ‡å‡ºç›®å‰é€šç”¨çš„èŠ‚ç‚¹åˆ†ç±»åŸºå‡†æµ‹è¯•ç”±äºä»…å‡­å•æ¨¡æ€ä¿¡æ¯å³å¯å–å¾—é«˜åˆ†ï¼Œä¸è¶³ä»¥è¯„ä¼°çœŸæ­£çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚ä¸ºå¡«è¡¥è¿™ä¸€è¯„ä¼°ç©ºç™½ï¼Œä½œè€…å¼•å…¥äº†CLEGR(Compositional Language-Graph Reasoning)åŸºå‡†ï¼Œè¯¥åŸºå‡†åˆ©ç”¨åˆæˆå›¾ç”ŸæˆæŠ€æœ¯å¹¶ç»“åˆéœ€è¦åŒæ—¶ç†è§£ç»“æ„å’Œæ–‡æœ¬è¯­ä¹‰çš„é—®é¢˜ã€‚ç ”ç©¶å¯¹å¤šç§ä»£è¡¨æ€§GLMæ¶æ„è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå‘ç°ä»…ä½¿ç”¨è½¯æç¤º(soft-prompted)çš„LLMåŸºçº¿æ¨¡å‹ä¸é›†æˆäº†å®Œæ•´GNNéª¨å¹²çš„GLMè¡¨ç°æ——é¼“ç›¸å½“ã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†å°†å›¾ç»“æ„æ˜¾å¼æ•´åˆè¿›LLMsçš„æ¶æ„å¿…è¦æ€§ï¼Œå¹¶æ­ç¤ºäº†GLMsåœ¨å¤„ç†å¤æ‚ç»“æ„æ¨ç†ä»»åŠ¡æ—¶å­˜åœ¨ä¸¥é‡çš„æ€§èƒ½é€€åŒ–ç°è±¡ã€‚è¯¥ç ”ç©¶æ˜ç¡®äº†å½“å‰GLMåœ¨å›¾æ¨ç†èƒ½åŠ›ä¸Šçš„å±€é™æ€§ï¼Œä¸ºç¤¾åŒºæ¨åŠ¨æ¶‰åŠå›¾ç»“æ„ä¸è¯­è¨€çš„æ˜¾å¼å¤šæ¨¡æ€æ¨ç†ç ”ç©¶æä¾›äº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20583v1",
      "published_date": "2025-08-28 09:20:47 UTC",
      "updated_date": "2025-08-28 09:20:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:54:41.585434+00:00"
    },
    {
      "arxiv_id": "2508.21104v3",
      "title": "PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning",
      "title_zh": "PVPOï¼šåŸºäºé¢„ä¼°å€¼çš„æ™ºèƒ½ä½“æ¨ç†ç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Wenfeng Feng",
        "Penghong Zhao",
        "Guochao Jiang",
        "Chuzhan Hao",
        "Yuewei Zhang",
        "Guohua Liu",
        "Hao Wang"
      ],
      "abstract": "Critic-free reinforcement learning methods, particularly group policies, have attracted considerable attention for their efficiency in complex tasks. However, these methods rely heavily on multiple sampling and comparisons within the policy to estimate advantage, which may cause the policy to fall into local optimum and increase computational cost. To address these issues, we propose PVPO, an efficient reinforcement learning method enhanced by an advantage reference anchor and data pre-sampling. Specifically, we use the reference model to rollout in advance and employ the calculated reward score as a reference anchor. Our approach effectively corrects the cumulative bias introduced by intra-group comparisons and significantly reduces reliance on the number of rollouts during training. Meanwhile, the reference model can assess sample difficulty during data pre-sampling, enabling effective selection of high-gain data to improve training efficiency. Moreover, PVPO is orthogonal to other advanced critic-free RL algorithms, making it compatible with and complementary to these methods. Experiments conducted on nine datasets across two domains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance. Our approach not only demonstrates robust generalization across multiple tasks, but also exhibits scalable performance across models of varying scales.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PVPOï¼Œä¸€ç§åŸºäºé¢„ä¼°ä»·å€¼çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ— è¯„è®ºå‘˜(critic-free)å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶å› è¿‡åº¦ä¾èµ–ç»„å†…é‡‡æ ·å¯¹æ¯”è€Œå¯¼è‡´çš„å±€éƒ¨æœ€ä¼˜å’Œè®¡ç®—æˆæœ¬é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¼˜åŠ¿å‚è€ƒé”šç‚¹(advantage reference anchor)å’Œæ•°æ®é¢„é‡‡æ ·(data pre-sampling)æœºåˆ¶ï¼Œåˆ©ç”¨å‚è€ƒæ¨¡å‹çš„é¢„å…ˆRolloutå¾—åˆ†ä½œä¸ºå‚è€ƒåŸºå‡†ã€‚è¿™ç§è®¾è®¡æœ‰æ•ˆä¿®æ­£äº†ç»„å†…å¯¹æ¯”å¸¦æ¥çš„ç´¯ç§¯åå·®ï¼Œå¹¶æ˜¾è‘—é™ä½äº†è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹é‡‡æ ·æ•°é‡çš„ä¾èµ–ã€‚åœ¨é¢„é‡‡æ ·é˜¶æ®µï¼Œå‚è€ƒæ¨¡å‹è¿˜èƒ½é€šè¿‡è¯„ä¼°æ ·æœ¬éš¾åº¦ç­›é€‰é«˜å¢ç›Šæ•°æ®ï¼Œä»è€Œè¿›ä¸€æ­¥æå‡è®­ç»ƒæ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼ŒPVPOåœ¨ä¹ä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†SOTAæ€§èƒ½ï¼Œä¸”ä¸å…¶å®ƒæ— è¯„è®ºå‘˜å¼ºåŒ–å­¦ä¹ (RL)ç®—æ³•å…·æœ‰è‰¯å¥½çš„æ­£äº¤å…¼å®¹æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä»»åŠ¡æ³›åŒ–å’Œä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸Šå‡å±•ç°å‡ºå“è¶Šçš„é²æ£’æ€§ä¸å¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.21104v3",
      "published_date": "2025-08-28 09:18:26 UTC",
      "updated_date": "2025-09-19 02:37:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:54:55.195395+00:00"
    },
    {
      "arxiv_id": "2508.20578v1",
      "title": "Human-AI Collaborative Bot Detection in MMORPGs",
      "title_zh": "MMORPG ä¸­çš„äººæœºåä½œæœºå™¨äººæ£€æµ‹",
      "authors": [
        "Jaeman Son",
        "Hyunsoo Kim"
      ],
      "abstract": "In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling bots exploit automated programs to level up characters at scale, undermining gameplay balance and fairness. Detecting such bots is challenging, not only because they mimic human behavior, but also because punitive actions require explainable justification to avoid legal and user experience issues. In this paper, we present a novel framework for detecting auto-leveling bots by leveraging contrastive representation learning and clustering techniques in a fully unsupervised manner to identify groups of characters with similar level-up patterns. To ensure reliable decisions, we incorporate a Large Language Model (LLM) as an auxiliary reviewer to validate the clustered groups, effectively mimicking a secondary human judgment. We also introduce a growth curve-based visualization to assist both the LLM and human moderators in assessing leveling behavior. This collaborative approach improves the efficiency of bot detection workflows while maintaining explainability, thereby supporting scalable and accountable bot regulation in MMORPGs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹å¤šäººåœ¨çº¿è§’è‰²æ‰®æ¼”æ¸¸æˆï¼ˆMMORPGsï¼‰ä¸­è‡ªåŠ¨åŒ–è„šæœ¬æœºå™¨äººï¼ˆauto-leveling botsï¼‰ç ´åå¹³è¡¡ä¸”éš¾ä»¥æ£€æµ‹å¹¶æä¾›åˆç†è§£é‡Šçš„é—®é¢˜å±•å¼€æ¢è®¨ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§å…¨æ–°çš„æ£€æµ‹æ¡†æ¶ï¼Œé‡‡ç”¨å®Œå…¨æ— ç›‘ç£çš„æ–¹å¼ï¼Œåˆ©ç”¨å¯¹æ¯”è¡¨ç¤ºå­¦ä¹ ï¼ˆcontrastive representation learningï¼‰å’Œèšç±»ï¼ˆclusteringï¼‰æŠ€æœ¯æ¥è¯†åˆ«å…·æœ‰ç›¸ä¼¼å‡çº§æ¨¡å¼çš„è§’è‰²ç¾¤ä½“ã€‚ä¸ºç¡®ä¿æƒ©ç½šå†³ç­–çš„å¯é æ€§ï¼Œè¯¥æ¡†æ¶å¼•å…¥å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¾…åŠ©è¯„å®¡å‘˜ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»äºŒæ¬¡åˆ¤æ–­æ¥éªŒè¯èšç±»ç»“æœçš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†åŸºäºå¢é•¿æ›²çº¿çš„å¯è§†åŒ–ï¼ˆgrowth curve-based visualizationï¼‰æ‰‹æ®µï¼ŒååŠ© LLM å’Œäººå·¥ç®¡ç†å‘˜æ›´ç›´è§‚åœ°è¯„ä¼°å‡çº§è¡Œä¸ºã€‚è¿™ç§äººæœºåä½œçš„æ–¹æ³•æ˜¾è‘—æå‡äº†æœºå™¨äººæ£€æµ‹æµç¨‹çš„æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†å†³ç­–çš„å¯è§£é‡Šæ€§ï¼ˆexplainabilityï¼‰ã€‚è¯¥æ–¹æ¡ˆä¸º MMORPGs ä¸­å¯æ‰©å±•ä¸”è´Ÿè´£ä»»çš„æœºå™¨äººç›‘ç®¡æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20578v1",
      "published_date": "2025-08-28 09:17:35 UTC",
      "updated_date": "2025-08-28 09:17:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:54:53.084647+00:00"
    },
    {
      "arxiv_id": "2508.20577v1",
      "title": "MERIT: Maximum-normalized Element-wise Ratio for Language Model Large-batch Training",
      "title_zh": "MERITï¼šé¢å‘è¯­è¨€æ¨¡å‹å¤§æ‰¹é‡è®­ç»ƒçš„æœ€å¤§å€¼å½’ä¸€åŒ–é€å…ƒç´ æ¯”ä¾‹",
      "authors": [
        "Yang Luo",
        "Zangwei Zheng",
        "Ziheng Qin",
        "Zirui Zhu",
        "Yong Liu",
        "Yang You"
      ],
      "abstract": "Large-batch training has become a cornerstone in accelerating the training of deep neural networks, yet it poses challenges in optimization and generalization. Existing optimizers like AdamW present performance degradation during language models' large-batch training, due to the information bottleneck in attention layers caused by the sharp increase of max attention logit. While the LAMB optimizer partially addresses this issue, some attention layers still face this issue. The reason is that $l_2$-norm-based trust ratios in LAMB are less effective in directly influencing the max value of query/key weights. Furthermore, the weight-wise trust ratio in LAMB is error-prone as it overlooks relationships of weight values within rows or columns. Building on these observations, we propose a novel optimizer, MERIT, which leverages the max-norm to calculate the trust ratio to constrain the max attention logit more effectively. Moreover, we further construct element-wise trust ratios to provide more robust update scaling by focusing on local weight structures. Extensive experiments of large-batch training across various sizes of GPT-2 models demonstrate the superior performance of MERIT. Notably, during the training of GPT-2 Medium, MERIT enables a 6k batch size without any performance degradation compared to the standard batch size (480) with 48B training tokens. This work highlights the importance of considering the max attention logit and finer-granularity trust ratio in large-batch training. It successfully improves the training stability and paves the way for larger batch usage, enabling faster development and iteration of large language models. Code is available at https://github.com/NUS-HPC-AI-Lab/MERIT.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­è¨€æ¨¡å‹åœ¨å¤§æ‰¹é‡è®­ç»ƒï¼ˆLarge-batch trainingï¼‰ä¸­ï¼Œå› æœ€å¤§æ³¨æ„åŠ›åˆ†å€¼ï¼ˆmax attention logitï¼‰æ¿€å¢å¯¼è‡´ AdamW ç­‰ä¼˜åŒ–å™¨æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º MERIT çš„æ–°å‹ä¼˜åŒ–å™¨ã€‚ç ”ç©¶å‘ç°ç°æœ‰ LAMB ä¼˜åŒ–å™¨é‡‡ç”¨çš„ $l_2$-norm ç½®ä¿¡æ¯”ï¼ˆtrust ratioï¼‰æ— æ³•æœ‰æ•ˆçº¦æŸæƒé‡æœ€å¤§å€¼ï¼Œä¸”å…¶æƒé‡çº§ï¼ˆweight-wiseï¼‰ç²’åº¦å¿½ç•¥äº†å±€éƒ¨æƒé‡å…³ç³»ï¼Œå®¹æ˜“äº§ç”Ÿè¯¯å·®ã€‚ä¸ºæ­¤ï¼ŒMERIT åˆ›æ–°åœ°åˆ©ç”¨æœ€å¤§èŒƒæ•°ï¼ˆmax-normï¼‰è®¡ç®—ç½®ä¿¡æ¯”ï¼Œä»¥æ›´æœ‰æ•ˆåœ°çº¦æŸ max attention logitï¼Œå¹¶è¿›ä¸€æ­¥å¼•å…¥äº†å…ƒç´ çº§ï¼ˆelement-wiseï¼‰ç½®ä¿¡æ¯”æ¥å¢å¼ºæ›´æ–°ç¼©æ”¾çš„ç¨³å¥æ€§ã€‚é’ˆå¯¹ä¸åŒè§„æ¨¡ GPT-2 æ¨¡å‹çš„å¹¿æ³›å®éªŒè¯æ˜äº† MERIT çš„å“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ GPT-2 Medium è®­ç»ƒä¸­ï¼ŒMERIT æˆåŠŸåœ¨ 6k æ‰¹é‡ä¸‹å®ç°äº†ä¸æ ‡å‡†æ‰¹é‡ç›¸å½“çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ä»…æ˜¾è‘—æå‡äº†è®­ç»ƒç¨³å®šæ€§ï¼Œè¿˜ä¸ºæ›´å¤§æ‰¹é‡è®­ç»ƒçš„ä½¿ç”¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿè¿­ä»£é“ºå¹³äº†é“è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.20577v1",
      "published_date": "2025-08-28 09:14:23 UTC",
      "updated_date": "2025-08-28 09:14:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:55:24.659749+00:00"
    },
    {
      "arxiv_id": "2508.20570v1",
      "title": "Towards Mechanistic Defenses Against Typographic Attacks in CLIP",
      "title_zh": "è¿ˆå‘ CLIP ä¸­é’ˆå¯¹æ’ç‰ˆæ”»å‡»çš„æœºç†æ€§é˜²å¾¡",
      "authors": [
        "Lorenz Hufe",
        "Constantin Venhoff",
        "Maximilian Dreyer",
        "Sebastian Lapuschkin",
        "Wojciech Samek"
      ],
      "abstract": "Typographic attacks exploit multi-modal systems by injecting text into images, leading to targeted misclassifications, malicious content generation and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP vision encoders behave under typographic attacks, locating specialized attention heads in the latter half of the model's layers that causally extract and transmit typographic information to the cls token. Building on these insights, we introduce a method to defend CLIP models against typographic attacks by selectively ablating a typographic circuit, consisting of attention heads. Without requiring finetuning, our method improves performance by up to 19.6% on a typographic variant of ImageNet-100, while reducing standard ImageNet-100 accuracy by less than 1%. Notably, our training-free approach remains competitive with current state-of-the-art typographic defenses that rely on finetuning. To this end, we release a family of dyslexic CLIP models which are significantly more robust against typographic attacks. These models serve as suitable drop-in replacements for a broad range of safety-critical applications, where the risks of text-based manipulation outweigh the utility of text recognition.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é’ˆå¯¹ CLIP æ¨¡å‹ä¸­å°åˆ·ä½“æ”»å‡» (Typographic Attacks) çš„æœºæ¢°è®ºé˜²å¾¡æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å›¾åƒä¸­æ³¨å…¥æ–‡æœ¬å¯¼è‡´æ¨¡å‹è¯¯åˆ†ç±»çš„é—®é¢˜ã€‚é€šè¿‡æ·±å…¥åˆ†æï¼Œç ”ç©¶è€…å®šä½äº† CLIP è§†è§‰ç¼–ç å™¨ååŠéƒ¨åˆ†ä¸­è´Ÿè´£å› æœæ€§æå–å¹¶å‘ `cls` token ä¼ é€’å°åˆ·ä½“ä¿¡æ¯çš„ç‰¹å®šæ³¨æ„åŠ›å¤´ (Attention Heads)ã€‚åŸºäºæ­¤å‘ç°ï¼Œè¯¥å·¥ä½œæå‡ºäº†ä¸€ç§æ— éœ€å¾®è°ƒçš„é˜²å¾¡æ‰‹æ®µï¼Œé€šè¿‡é€‰æ‹©æ€§åœ°æ¶ˆè (Ablating) ç”±è¿™äº›æ³¨æ„åŠ›å¤´æ„æˆçš„å°åˆ·ä½“ç”µè·¯ (Typographic Circuit) æ¥å¢å¼ºæ¨¡å‹é²æ£’æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ ImageNet-100 å°åˆ·ä½“å˜ä½“ä¸Šçš„æ€§èƒ½æå‡é«˜è¾¾ 19.6%ï¼Œä¸”åœ¨æ ‡å‡†æ•°æ®é›†ä¸Šçš„ç²¾åº¦æŸå¤±ä½äº 1%ï¼Œå…¶è¡¨ç°è¶³ä»¥åª²ç¾å½“å‰æœ€å…ˆè¿›çš„åŸºäºå¾®è°ƒçš„é˜²å¾¡æŠ€æœ¯ã€‚ç ”ç©¶å›¢é˜Ÿéšåå‘å¸ƒäº† dyslexic CLIP ç³»åˆ—æ¨¡å‹ï¼Œä¸ºå®‰å…¨å…³é”®å‹åº”ç”¨ä¸­æŠµå¾¡æ–‡æœ¬æ“çºµæä¾›äº†æ›´å…·é²æ£’æ€§çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20570v1",
      "published_date": "2025-08-28 09:08:30 UTC",
      "updated_date": "2025-08-28 09:08:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:55:16.194758+00:00"
    },
    {
      "arxiv_id": "2508.20563v1",
      "title": "AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop",
      "title_zh": "äººå·¥æ™ºèƒ½ä¸æ•æ·è½¯ä»¶å¼€å‘ï¼šXP2025 ç ”è®¨ä¼šç ”ç©¶è·¯çº¿å›¾",
      "authors": [
        "Zheying Zhang",
        "Tomas Herda",
        "Victoria Pichler",
        "Pekka Abrahamsson",
        "Geir K. Hanssen",
        "Joshua Kerievsky",
        "Alex Polyakov",
        "Mohit Chandna",
        "Marius Irgens",
        "Kai-Kristian Kemell",
        "Ayman Asad Khan",
        "Crystal Kwok",
        "Evan Leybourn",
        "Munish Malik",
        "Dorota Mleczko",
        "Morteza Moalagh",
        "Christopher Morales",
        "Yuliia Pieskova",
        "Daniel PlanÃ¶tscher",
        "Mika Saari",
        "Anastasiia Tkalich",
        "Karl Josef Gstettner",
        "Xiaofeng Wang"
      ],
      "abstract": "This paper synthesizes the key findings from a full-day XP2025 workshop on \"AI and Agile: From Frustration to Success\", held in Brugg-Windisch, Switzerland. The workshop brought together over 30 interdisciplinary academic researchers and industry practitioners to tackle the concrete challenges and emerging opportunities at the intersection of Generative Artificial Intelligence (GenAI) and agile software development. Through structured, interactive breakout sessions, participants identified shared pain points like tool fragmentation, governance, data quality, and critical skills gaps in AI literacy and prompt engineering. These issues were further analyzed, revealing underlying causes and cross-cutting concerns. The workshop concluded by collaboratively co-creating a multi-thematic research roadmap, articulating both short-term, implementable actions and visionary, long-term research directions. This cohesive agenda aims to guide future investigation and drive the responsible, human-centered integration of GenAI into agile practices.",
      "tldr_zh": "è¯¥è®ºæ–‡æ€»ç»“äº†XP2025ç ”è®¨ä¼šå…³äºâ€œAIä¸æ•æ·ï¼šä»æŒ«è´¥åˆ°æˆåŠŸâ€çš„æ ¸å¿ƒæˆæœï¼Œæ±‡é›†äº†30å¤šä½è·¨å­¦ç§‘ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…å¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) ä¸æ•æ·è½¯ä»¶å¼€å‘ (Agile Software Development) äº¤å‰é¢†åŸŸçš„æ·±å…¥æ´å¯Ÿã€‚é€šè¿‡ç»“æ„åŒ–çš„äº¤äº’è®¨è®ºï¼Œç ”ç©¶è¯†åˆ«äº†å·¥å…·ç¢ç‰‡åŒ– (Tool Fragmentation)ã€æ²»ç† (Governance)ã€æ•°æ®è´¨é‡ (Data Quality) ä»¥åŠAIç´ å…»å’Œæç¤ºå·¥ç¨‹ (Prompt Engineering) æŠ€èƒ½å·®è·ç­‰å…³é”®ç—›ç‚¹ã€‚ç ”è®¨ä¼šé€šè¿‡è¿›ä¸€æ­¥åˆ†æè¿™äº›é—®é¢˜çš„æ ¹æœ¬åŸå› ï¼Œå…±åŒåˆ›å»ºäº†ä¸€ä¸ªå¤šä¸»é¢˜çš„ç ”ç©¶è·¯çº¿å›¾ (Research Roadmap)ï¼Œå…¶ä¸­æ¶µç›–äº†çŸ­æœŸå¯å®æ–½çš„è¡ŒåŠ¨å’Œå…·æœ‰å‰ç»æ€§çš„é•¿æœŸç ”ç©¶æ–¹å‘ã€‚è¯¥æˆæœæ—¨åœ¨ä¸ºè¯¥é¢†åŸŸçš„æœªæ¥è°ƒæŸ¥æä¾›æŒ‡å¯¼ï¼Œå¹¶æ¨åŠ¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨æ•æ·å®è·µä¸­å®ç°è´Ÿè´£ä»»ä¸”ä»¥äººä¸ºä¸­å¿ƒçš„æ•´åˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20563v1",
      "published_date": "2025-08-28 08:56:32 UTC",
      "updated_date": "2025-08-28 08:56:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:55:21.789579+00:00"
    },
    {
      "arxiv_id": "2508.20557v1",
      "title": "Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data",
      "title_zh": "é¢å‘å¤šé¢†åŸŸéç‹¬ç«‹åŒåˆ†å¸ƒæ–‡æœ¬æ•°æ®çš„è‡ªé€‚åº”è”é‚¦è’¸é¦",
      "authors": [
        "Jiahao Xiao",
        "Jiangming Liu"
      ],
      "abstract": "The widespread success of pre-trained language models has established a new training paradigm, where a global PLM is fine-tuned using task-specific data from local clients. The local data are highly different from each other and can not capture the global distribution of the whole data in real world. To address the challenges of non-IID data in real environments, privacy-preserving federated distillation has been proposed and highly investigated. However, previous experimental non-IID scenarios are primarily identified with the label (output) diversity, without considering the diversity of language domains (input) that is crucial in natural language processing. In this paper, we introduce a comprehensive set of multi-domain non-IID scenarios and propose a unified benchmarking framework that includes diverse data. The benchmark can be used to evaluate the federated learning framework in a real environment. To this end, we propose an Adaptive Federated Distillation (AdaFD) framework designed to address multi-domain non-IID challenges in both homogeneous and heterogeneous settings. Experimental results demonstrate that our models capture the diversity of local clients and achieve better performance compared to the existing works. The code for this paper is available at: https://github.com/jiahaoxiao1228/AdaFD.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰åœ¨è”é‚¦å­¦ä¹ ï¼ˆFederated Learningï¼‰ä¸­é¢ä¸´çš„éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰æ•°æ®æŒ‘æˆ˜ï¼Œæå‡ºäº† Adaptive Federated Distillation (AdaFD) æ¡†æ¶ã€‚ä»¥å¾€çš„è”é‚¦è’¸é¦ç ”ç©¶å¤šé›†ä¸­äºè¾“å‡ºæ ‡ç­¾åˆ†å¸ƒçš„å·®å¼‚ï¼Œè€Œå¿½ç•¥äº†è‡ªç„¶è¯­è¨€å¤„ç†ä¸­å…³é”®çš„è¯­è¨€é¢†åŸŸï¼ˆlanguage domainsï¼‰è¾“å…¥å¤šæ ·æ€§ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†ä¸€å¥—å¤šé¢†åŸŸ non-IID åœºæ™¯çš„ç»¼åˆåŸºå‡†æ¡†æ¶ï¼Œé€šè¿‡æ¶µç›–å¤šæ ·åŒ–æ•°æ®æ¥æ¨¡æ‹ŸçœŸå®çš„åˆ†å¸ƒå¼ç¯å¢ƒã€‚AdaFD æ¡†æ¶æ—¨åœ¨åŒæ—¶è§£å†³åŒè´¨ï¼ˆhomogeneousï¼‰å’Œå¼‚è´¨ï¼ˆheterogeneousï¼‰è®¾ç½®ä¸‹çš„å¤šé¢†åŸŸ non-IID éš¾é¢˜ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒAdaFD èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æœ¬åœ°å®¢æˆ·ç«¯çš„å¤šæ ·æ€§ç‰¹å¾ï¼Œåœ¨å¤šé¡¹ä»»åŠ¡ä¸­çš„æ€§èƒ½è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20557v1",
      "published_date": "2025-08-28 08:51:14 UTC",
      "updated_date": "2025-08-28 08:51:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:55:22.056000+00:00"
    },
    {
      "arxiv_id": "2508.20554v1",
      "title": "Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering",
      "title_zh": "BioASQ 2025 æ¦‚è¿°ï¼šç¬¬åä¸‰å±Šå¤§è§„æ¨¡ç”Ÿç‰©åŒ»å­¦è¯­ä¹‰ç´¢å¼•ä¸é—®ç­”æŒ‘æˆ˜èµ›",
      "authors": [
        "Anastasios Nentidis",
        "Georgios Katsimpras",
        "Anastasia Krithara",
        "Martin Krallinger",
        "Miguel RodrÃ­guez-Ortega",
        "Eduard Rodriguez-LÃ³pez",
        "Natalia Loukachevitch",
        "Andrey Sakhovskiy",
        "Elena Tutubalina",
        "Dimitris Dimitriadis",
        "Grigorios Tsoumakas",
        "George Giannakoulas",
        "Alexandra Bekiaridou",
        "Athanasios Samaras",
        "Giorgio Maria Di Nunzio",
        "Nicola Ferro",
        "Stefano Marchesin",
        "Marco Martinelli",
        "Gianmaria Silvello",
        "Georgios Paliouras"
      ],
      "abstract": "This is an overview of the thirteenth edition of the BioASQ challenge in the context of the Conference and Labs of the Evaluation Forum (CLEF) 2025. BioASQ is a series of international challenges promoting advances in large-scale biomedical semantic indexing and question answering. This year, BioASQ consisted of new editions of the two established tasks, b and Synergy, and four new tasks: a) Task MultiClinSum on multilingual clinical summarization. b) Task BioNNE-L on nested named entity linking in Russian and English. c) Task ELCardioCC on clinical coding in cardiology. d) Task GutBrainIE on gut-brain interplay information extraction. In this edition of BioASQ, 83 competing teams participated with more than 1000 distinct submissions in total for the six different shared tasks of the challenge. Similar to previous editions, several participating systems achieved competitive performance, indicating the continuous advancement of the state-of-the-art in the field.",
      "tldr_zh": "è¯¥ç ”ç©¶ç»¼è¿°äº†åœ¨CLEF 2025èƒŒæ™¯ä¸‹ä¸¾åŠçš„ç¬¬13å±ŠBioASQæŒ‘æˆ˜èµ›ï¼Œæ—¨åœ¨æ¨åŠ¨å¤§è§„æ¨¡ç”Ÿç‰©åŒ»å­¦è¯­ä¹‰ç´¢å¼•(biomedical semantic indexing)å’Œé—®ç­”(question answering)é¢†åŸŸçš„æŒç»­è¿›æ­¥ã€‚æœ¬å±Šèµ›äº‹ä¸ä»…ä¿ç•™äº†Task bå’ŒSynergyä¸¤é¡¹ç»å…¸ä»»åŠ¡ï¼Œè¿˜åˆ›æ–°æ€§åœ°å¼•å…¥äº†æ¶‰åŠå¤šè¯­è¨€ä¸´åºŠæ‘˜è¦çš„MultiClinSumã€é’ˆå¯¹ä¿„è¯­å’Œè‹±è¯­åµŒå¥—å‘½åå®ä½“é“¾æ¥çš„BioNNE-Lã€ä¸“æ³¨äºå¿ƒè„ç—…ä¸´åºŠç¼–ç çš„ELCardioCCä»¥åŠè‚ è„‘äº’åŠ¨ä¿¡æ¯æå–çš„GutBrainIEå››é¡¹å…¨æ–°ä»»åŠ¡ã€‚æ­¤æ¬¡æŒ‘æˆ˜èµ›å¸å¼•äº†å…¨çƒ83æ”¯å›¢é˜Ÿå‚ä¸å¹¶è´¡çŒ®äº†1000å¤šä»½æäº¤æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå‚èµ›ç³»ç»Ÿåœ¨å„é¡¹ä»»åŠ¡ä¸­å‡å–å¾—äº†æå…·ç«äº‰åŠ›çš„è¡¨ç°ï¼Œæœ‰åŠ›åœ°è¯æ˜äº†ç”Ÿç‰©åŒ»å­¦é¢†åŸŸå‰æ²¿æŠ€æœ¯(state-of-the-art)çš„ä¸æ–­å‘å±•ä¸çªç ´ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "26 pages, 17 tables, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2508.20554v1",
      "published_date": "2025-08-28 08:45:55 UTC",
      "updated_date": "2025-08-28 08:45:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:55:22.955697+00:00"
    },
    {
      "arxiv_id": "2508.20549v2",
      "title": "MedGR$^2$: Breaking the Data Barrier for Medical Reasoning via Generative Reward Learning",
      "title_zh": "MedGR$^2$ï¼šåŸºäºç”Ÿæˆå¼å¥–åŠ±å­¦ä¹ æ‰“ç ´åŒ»å­¦æ¨ç†çš„æ•°æ®å£å’",
      "authors": [
        "Weihai Zhi",
        "Jiayan Guo",
        "Shangyang Li"
      ],
      "abstract": "The application of Vision-Language Models (VLMs) in medicine is critically hampered by the scarcity of high-quality, expert-annotated data. Supervised Fine-Tuning (SFT) on existing datasets often leads to poor generalization on unseen modalities and tasks, while Reinforcement Learning (RL), a promising alternative, is stymied by the lack of reliable reward signals in this data-scarce domain. To break this impasse, we introduce Generative Reward Learning for Medical Reasoning (MedGR$^2$), a novel framework that creates a self-improving virtuous cycle. MedGR$^2$ co-develops a data generator and a reward model, enabling the automated, continuous creation of high-quality, multi-modal medical data that serves as both a superior training source for SFT and RL. Our experiments demonstrate that SFT with MedGR$^2$-produced data already surpasses baselines trained on large-scale, human-curated datasets. Crucially, when leveraging this data for RL via Group Relative Policy Optimization (GRPO), our model achieves state-of-the-art cross-modality and cross-task generalization, significantly outperforming specialized RL-based methods. Furthermore, our compact model, empowered by MedGR$^2$, achieves performance competitive with foundation models possessing over 10 times more parameters. MedGR$^2$ presents a new paradigm for data-efficient learning in high-stakes domains, transforming the problem from data scarcity to data generation and unlocking the full potential of RL for building truly generalizable medical AI.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models, VLMs) é¢ä¸´çš„é«˜è´¨é‡ä¸“å®¶æ ‡æ³¨æ•°æ®ç¨€ç¼ºã€æœ‰ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning, SFT) æ³›åŒ–èƒ½åŠ›å·®ä»¥åŠå¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL) ç¼ºä¹å¯é å¥–åŠ±ä¿¡å·ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º MedGR$^2$ çš„ç”Ÿæˆå¼å¥–åŠ±å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å…±åŒå¼€å‘æ•°æ®ç”Ÿæˆå™¨å’Œå¥–åŠ±æ¨¡å‹ï¼Œæ„å»ºäº†ä¸€ä¸ªè‡ªæ”¹è¿›çš„è‰¯æ€§å¾ªç¯ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ä¸”æŒç»­åœ°ç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€åŒ»ç–—æ•°æ®ï¼Œä¸º SFT å’Œ RL æä¾›ä¼˜è´¨è®­ç»ƒèµ„æºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»…ä½¿ç”¨ MedGR$^2$ ç”Ÿæˆçš„æ•°æ®è¿›è¡Œ SFT å³å¯è¶…è¶Šåœ¨å¤§å‹äººå·¥æ•°æ®é›†ä¸Šè®­ç»ƒçš„åŸºçº¿æ¨¡å‹ã€‚é€šè¿‡ç»“åˆç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– (Group Relative Policy Optimization, GRPO) è¿›è¡Œ RLï¼Œè¯¥æ¨¡å‹åœ¨è·¨æ¨¡æ€å’Œè·¨ä»»åŠ¡æ³›åŒ–æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿› (State-of-the-Art) æ°´å¹³ã€‚æ­¤å¤–ï¼ŒMedGR$^2$ èµ‹èƒ½çš„å°å‹æ¨¡å‹åœ¨æ€§èƒ½ä¸Šå¯ä¸å‚æ•°é‡å¤§å…¶ 10 å€ä»¥ä¸Šçš„åŸºç¡€æ¨¡å‹ç›¸åª²ç¾ã€‚è¯¥é¡¹å·¥ä½œä¸ºé«˜é£é™©é¢†åŸŸçš„æ•°æ®é«˜æ•ˆå­¦ä¹ æä¾›äº†ä¸€ç§æ–°èŒƒå¼ï¼ŒæˆåŠŸå°†åŒ»ç–— AI çš„ç“¶é¢ˆä»æ•°æ®ç¨€ç¼ºè½¬åŒ–ä¸ºæ•°æ®ç”Ÿæˆï¼Œå……åˆ†é‡Šæ”¾äº†å¼ºåŒ–å­¦ä¹ æ„å»ºé€šç”¨åŒ»ç–—äººå·¥æ™ºèƒ½çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.20549v2",
      "published_date": "2025-08-28 08:41:32 UTC",
      "updated_date": "2025-12-08 05:05:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:55:51.156276+00:00"
    },
    {
      "arxiv_id": "2508.20547v2",
      "title": "SPGrasp: Spatiotemporal Prompt-driven Grasp Synthesis in Dynamic Scenes",
      "title_zh": "SPGraspï¼šåŠ¨æ€åœºæ™¯ä¸‹çš„æ—¶ç©ºæç¤ºé©±åŠ¨æŠ“å–ç”Ÿæˆ",
      "authors": [
        "Yunpeng Mei",
        "Hongjie Cao",
        "Yinqiu Xia",
        "Wei Xiao",
        "Zhaohan Feng",
        "Gang Wang",
        "Jie Chen"
      ],
      "abstract": "Real-time interactive grasp synthesis for dynamic objects remains challenging as existing methods fail to achieve low-latency inference while maintaining promptability. To bridge this gap, we propose SPGrasp (spatiotemporal prompt-driven dynamic grasp synthesis), a novel framework extending segment anything model v2 (SAMv2) for video stream grasp estimation. Our core innovation integrates user prompts with spatiotemporal context, enabling real-time interaction with end-to-end latency as low as 59 ms while ensuring temporal consistency for dynamic objects. In benchmark evaluations, SPGrasp achieves instance-level grasp accuracies of 90.6% on OCID and 93.8% on Jacquard. On the challenging GraspNet-1Billion dataset under continuous tracking, SPGrasp achieves 92.0% accuracy with 73.1 ms per-frame latency, representing a 58.5% reduction compared to the prior state-of-the-art promptable method RoG-SAM while maintaining competitive accuracy. Real-world experiments involving 13 moving objects demonstrate a 94.8% success rate in interactive grasping scenarios. These results confirm SPGrasp effectively resolves the latency-interactivity trade-off in dynamic grasp synthesis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SPGraspï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ—¶ç©ºæç¤ºé©±åŠ¨(spatiotemporal prompt-driven)çš„åŠ¨æ€æŠ“å–åˆæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤„ç†åŠ¨æ€ç‰©ä½“æ—¶å­˜åœ¨çš„å»¶è¿Ÿé«˜ã€äº¤äº’æ€§å·®çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶æ‰©å±•äº†SAMv2 (Segment Anything Model v2) ä»¥æ”¯æŒè§†é¢‘æµä¸­çš„æŠ“å–ä¼°è®¡ï¼Œé€šè¿‡å°†ç”¨æˆ·æç¤ºä¸æ—¶ç©ºä¸Šä¸‹æ–‡ç›¸ç»“åˆï¼Œå®ç°äº†ä½è‡³59æ¯«ç§’çš„ç«¯åˆ°ç«¯å»¶è¿Ÿï¼Œå¹¶ç¡®ä¿äº†åŠ¨æ€ç‰©ä½“çš„æ—¶ç©ºä¸€è‡´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSPGraspåœ¨OCIDå’ŒJacquardåŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«è¾¾åˆ°äº†90.6%å’Œ93.8%çš„å®ä¾‹çº§æŠ“å–å‡†ç¡®ç‡ã€‚åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„GraspNet-1Billionæ•°æ®é›†çš„æŒç»­è¿½è¸ªæµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•å®ç°äº†92.0%çš„å‡†ç¡®åº¦ï¼Œä¸”æ¯å¸§å»¶è¿Ÿæ¯”ç°æœ‰SOTAæ–¹æ³•RoG-SAMé™ä½äº†58.5%ã€‚åœ¨åŒ…å«13ä¸ªç§»åŠ¨ç‰©ä½“çš„çœŸå®ç¯å¢ƒå®éªŒä¸­ï¼ŒSPGraspåœ¨äº¤äº’å¼æŠ“å–åœºæ™¯ä¸‹çš„æˆåŠŸç‡é«˜è¾¾94.8%ï¼Œæœ‰æ•ˆè§£å†³äº†åŠ¨æ€æŠ“å–åˆæˆä¸­å»¶è¿Ÿä¸äº¤äº’æ€§ä¹‹é—´çš„æƒè¡¡éš¾é¢˜ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20547v2",
      "published_date": "2025-08-28 08:38:50 UTC",
      "updated_date": "2025-08-30 05:51:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:55:52.858727+00:00"
    },
    {
      "arxiv_id": "2508.20546v1",
      "title": "MM-HSD: Multi-Modal Hate Speech Detection in Videos",
      "title_zh": "MM-HSDï¼šè§†é¢‘å¤šæ¨¡æ€ä»‡æ¨è¨€è®ºæ£€æµ‹",
      "authors": [
        "Berta CÃ©spedes-Sarrias",
        "Carlos Collado-Capell",
        "Pablo Rodenas-Ruiz",
        "Olena Hrynenko",
        "Andrea Cavallaro"
      ],
      "abstract": "While hate speech detection (HSD) has been extensively studied in text, existing multi-modal approaches remain limited, particularly in videos. As modalities are not always individually informative, simple fusion methods fail to fully capture inter-modal dependencies. Moreover, previous work often omits relevant modalities such as on-screen text and audio, which may contain subtle hateful content and thus provide essential cues, both individually and in combination with others. In this paper, we present MM-HSD, a multi-modal model for HSD in videos that integrates video frames, audio, and text derived from speech transcripts and from frames (i.e.~on-screen text) together with features extracted by Cross-Modal Attention (CMA). We are the first to use CMA as an early feature extractor for HSD in videos, to systematically compare query/key configurations, and to evaluate the interactions between different modalities in the CMA block. Our approach leads to improved performance when on-screen text is used as a query and the rest of the modalities serve as a key. Experiments on the HateMM dataset show that MM-HSD outperforms state-of-the-art methods on M-F1 score (0.874), using concatenation of transcript, audio, video, on-screen text, and CMA for feature extraction on raw embeddings of the modalities. The code is available at https://github.com/idiap/mm-hsd",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MM-HSDï¼Œä¸€ç§é’ˆå¯¹è§†é¢‘ä»‡æ¨è¨€è®ºæ£€æµ‹(Hate Speech Detection, HSD)çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç®€å•èåˆæ–¹æ³•éš¾ä»¥æ•æ‰æ¨¡æ€é—´ä¾èµ–å…³ç³»ä»¥åŠå¿½ç•¥å±å¹•æ–‡æœ¬(on-screen text)å’ŒéŸ³é¢‘ç­‰å…³é”®ä¿¡æ¯çš„é—®é¢˜ã€‚MM-HSDæ•´åˆäº†è§†é¢‘å¸§ã€éŸ³é¢‘ã€è¯­éŸ³è½¬å½•æ–‡æœ¬ä»¥åŠå±å¹•æ–‡æœ¬ï¼Œå¹¶é¦–æ¬¡å°†è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶(Cross-Modal Attention, CMA)ä½œä¸ºæ—©æœŸç‰¹å¾æå–å™¨åº”ç”¨äºè§†é¢‘HSDä»»åŠ¡ã€‚é€šè¿‡ç³»ç»Ÿåœ°è¯„ä¼°ä¸åŒæ¨¡æ€åœ¨CMAå—ä¸­çš„æŸ¥è¯¢(query)ä¸é”®(key)é…ç½®ï¼Œç ”ç©¶å‘ç°ä»¥å±å¹•æ–‡æœ¬ä¸ºæŸ¥è¯¢ã€å…¶ä½™æ¨¡æ€ä¸ºé”®æ—¶èƒ½æ˜¾è‘—æå‡æ€§èƒ½ã€‚åœ¨HateMMæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMM-HSDå‡­å€Ÿå¯¹å¤šç§æ¨¡æ€åŸå§‹åµŒå…¥å’ŒCMAç‰¹å¾çš„æœ‰æ•ˆé›†æˆï¼Œå–å¾—äº†0.874çš„M-F1åˆ†æ•°ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ã€‚è¯¥å·¥ä½œéªŒè¯äº†å…¨é¢æ•´åˆå¤šæ¨¡æ€ä¿¡æ¯åŠä¼˜åŒ–æ¨¡æ€é—´äº¤äº’å¯¹äºè¯†åˆ«è§†é¢‘ä¸­å¾®å¦™ä»‡æ¨å†…å®¹çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.MM",
        "cs.AI"
      ],
      "primary_category": "cs.MM",
      "comment": "Accepted at ACM Multimedia 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.20546v1",
      "published_date": "2025-08-28 08:36:35 UTC",
      "updated_date": "2025-08-28 08:36:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:55:39.483718+00:00"
    },
    {
      "arxiv_id": "2508.21103v1",
      "title": "Spatiotemporal EEG-Based Emotion Recognition Using SAM Ratings from Serious Games with Hybrid Deep Learning",
      "title_zh": "åŸºäºä¸¥è‚ƒæ¸¸æˆ SAM è¯„åˆ†ä¸æ··åˆæ·±åº¦å­¦ä¹ çš„æ—¶ç©ºè„‘ç”µæƒ…ç»ªè¯†åˆ«",
      "authors": [
        "Abdul Rehman",
        "Ilona Heldal",
        "Jerry Chun-Wei Lin"
      ],
      "abstract": "Recent advancements in EEG-based emotion recognition have shown promising outcomes using both deep learning and classical machine learning approaches; however, most existing studies focus narrowly on binary valence prediction or subject-specific classification, which limits generalizability and deployment in real-world affective computing systems. To address this gap, this paper presents a unified, multigranularity EEG emotion classification framework built on the GAMEEMO dataset, which consists of 14-channel EEG recordings and continuous self-reported emotion ratings (boring, horrible, calm, and funny) from 28 subjects across four emotion-inducing gameplay scenarios. Our pipeline employs a structured preprocessing strategy that comprises temporal window segmentation, hybrid statistical and frequency-domain feature extraction, and z-score normalization to convert raw EEG signals into robust, discriminative input vectors. Emotion labels are derived and encoded across three complementary axes: (i) binary valence classification based on the averaged polarity of positive and negative emotion ratings, and (ii) Multi-class emotion classification, where the presence of the most affective state is predicted. (iii) Fine-grained multi-label representation via binning each emotion into 10 ordinal classes. We evaluate a broad spectrum of models, including Random Forest, XGBoost, and SVM, alongside deep neural architectures such as LSTM, LSTM-GRU, and CNN-LSTM. Among these, the LSTM-GRU model consistently outperforms the others, achieving an F1-score of 0.932 in the binary valence task and 94.5% and 90.6% in both multi-class and Multi-Label emotion classification.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºäºEEGçš„æƒ…ç»ªè¯†åˆ«åœ¨æ³›åŒ–æ€§å’Œå®é™…éƒ¨ç½²æ–¹é¢çš„å±€é™ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¤šç²’åº¦æƒ…ç»ªåˆ†ç±»æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨GAMEEMOæ•°æ®é›†ï¼Œé€šè¿‡å¯¹28åå—è¯•è€…åœ¨ä¸¥è‚ƒæ¸¸æˆåœºæ™¯ä¸‹çš„14é€šé“EEGä¿¡å·è¿›è¡Œé¢„å¤„ç†ï¼Œç»“åˆäº†æ—¶é—´çª—å£åˆ†å‰²ã€æ··åˆç»Ÿè®¡ä¸é¢‘åŸŸç‰¹å¾æå–ä»¥åŠz-scoreæ ‡å‡†åŒ–æŠ€æœ¯ã€‚ç ”ç©¶è®¾è®¡äº†ä¸‰ç§äº’è¡¥çš„åˆ†ç±»ä»»åŠ¡ï¼ŒåŒ…æ‹¬äºŒå…ƒæƒ…æ„Ÿæ•ˆä»·(valence)åˆ†ç±»ã€å¤šç±»æƒ…ç»ªåˆ†ç±»ä»¥åŠå°†æƒ…ç»ªåˆ’åˆ†ä¸º10ä¸ªç­‰çº§çš„ç»†ç²’åº¦å¤šæ ‡ç­¾(multi-label)è¡¨å¾ã€‚å®éªŒå¯¹æ¯”äº†Random Forestã€XGBoostã€SVMç­‰æ¨¡å‹ä»¥åŠLSTMã€LSTM-GRUã€CNN-LSTMç­‰æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œå‘ç°LSTM-GRUæ¨¡å‹åœ¨å„é¡¹ä»»åŠ¡ä¸­å‡è¡¨ç°æœ€ä¼˜ã€‚æœ€ç»ˆï¼Œè¯¥æ¨¡å‹åœ¨äºŒå…ƒæ•ˆä»·ä»»åŠ¡ä¸­å–å¾—äº†0.932çš„F1-scoreï¼Œå¹¶åœ¨å¤šç±»å’Œå¤šæ ‡ç­¾åˆ†ç±»ä¸­åˆ†åˆ«è¾¾åˆ°äº†94.5%å’Œ90.6%çš„å‡†ç¡®ç‡ã€‚è¯¥æˆæœè¯æ˜äº†æ··åˆæ·±åº¦å­¦ä¹ åœ¨å¤„ç†æ—¶ç©ºEEGç‰¹å¾ä»¥åŠå¤šç²’åº¦æƒ…ç»ªè¯†åˆ«ä»»åŠ¡ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21103v1",
      "published_date": "2025-08-28 08:25:19 UTC",
      "updated_date": "2025-08-28 08:25:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:55:44.163790+00:00"
    },
    {
      "arxiv_id": "2508.20532v1",
      "title": "Overview of BioASQ 2024: The twelfth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering",
      "title_zh": "BioASQ 2024 æ¦‚è§ˆï¼šç¬¬åäºŒå±Šå¤§è§„æ¨¡ç”Ÿç‰©åŒ»å­¦è¯­ä¹‰ç´¢å¼•ä¸é—®ç­” BioASQ æŒ‘æˆ˜èµ›",
      "authors": [
        "Anastasios Nentidis",
        "Georgios Katsimpras",
        "Anastasia Krithara",
        "Salvador Lima-LÃ³pez",
        "EulÃ lia FarrÃ©-Maduell",
        "Martin Krallinger",
        "Natalia Loukachevitch",
        "Vera Davydova",
        "Elena Tutubalina",
        "Georgios Paliouras"
      ],
      "abstract": "This is an overview of the twelfth edition of the BioASQ challenge in the context of the Conference and Labs of the Evaluation Forum (CLEF) 2024. BioASQ is a series of international challenges promoting advances in large-scale biomedical semantic indexing and question answering. This year, BioASQ consisted of new editions of the two established tasks b and Synergy, and two new tasks: a) MultiCardioNER on the adaptation of clinical entity detection to the cardiology domain in a multilingual setting, and b) BIONNE on nested NER in Russian and English. In this edition of BioASQ, 37 competing teams participated with more than 700 distinct submissions in total for the four different shared tasks of the challenge. Similarly to previous editions, most of the participating systems achieved competitive performance, suggesting the continuous advancement of the state-of-the-art in the field.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¦‚è¿°äº†åœ¨CLEF 2024æ¡†æ¶ä¸‹ä¸¾åŠçš„ç¬¬åäºŒå±ŠBioASQæŒ‘æˆ˜èµ›ï¼Œæ—¨åœ¨æ¨åŠ¨å¤§è§„æ¨¡ç”Ÿç‰©åŒ»å­¦è¯­ä¹‰ç´¢å¼•ï¼ˆBiomedical Semantic Indexingï¼‰å’Œé—®ç­”ï¼ˆQuestion Answeringï¼‰æŠ€æœ¯çš„è¿›æ­¥ã€‚æœ¬å±Šèµ›äº‹é™¤äº†å»¶ç»­ç»å…¸çš„Task bå’ŒSynergyä»»åŠ¡å¤–ï¼Œè¿˜æ–°å¢äº†ä¸“æ³¨äºå¤šè¯­è¨€å¿ƒè¡€ç®¡é¢†åŸŸä¸´åºŠå®ä½“è¯†åˆ«çš„MultiCardioNERä»»åŠ¡ï¼Œä»¥åŠé’ˆå¯¹ä¿„è¯­å’Œè‹±è¯­åµŒå¥—å‘½åå®ä½“è¯†åˆ«çš„BIONNEä»»åŠ¡ã€‚å…±æœ‰37æ”¯å‚èµ›é˜Ÿä¼æäº¤äº†è¶…è¿‡700ä»½ç»“æœï¼Œæ¶µç›–äº†ç”Ÿç‰©åŒ»å­¦è‡ªç„¶è¯­è¨€å¤„ç†çš„å¤šä¸ªå‰æ²¿æ–¹å‘ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œå¤§å¤šæ•°å‚èµ›ç³»ç»Ÿå‡å–å¾—äº†æå…·ç«äº‰åŠ›çš„è¡¨ç°ï¼Œæœ‰åŠ›åœ°è¯æ˜äº†è¯¥é¢†åŸŸåœ¨å¤„ç†å¤æ‚ç”Ÿç‰©åŒ»å­¦ä¿¡æ¯æ£€ç´¢å’Œç†è§£ä»»åŠ¡ä¸Šçš„æŒç»­æŠ€æœ¯æ¼”è¿›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "25 pages, 16 tables, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2508.20532v1",
      "published_date": "2025-08-28 08:17:57 UTC",
      "updated_date": "2025-08-28 08:17:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:55:42.273550+00:00"
    },
    {
      "arxiv_id": "2508.20525v1",
      "title": "Enhancing Health Fact-Checking with LLM-Generated Synthetic Data",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®å¢å¼ºå¥åº·äº‹å®æ ¸æŸ¥",
      "authors": [
        "Jingze Zhang",
        "Jiahe Qian",
        "Yiliang Zhou",
        "Yifan Peng"
      ],
      "abstract": "Fact-checking for health-related content is challenging due to the limited availability of annotated training data. In this study, we propose a synthetic data generation pipeline that leverages large language models (LLMs) to augment training data for health-related fact checking. In this pipeline, we summarize source documents, decompose the summaries into atomic facts, and use an LLM to construct sentence-fact entailment tables. From the entailment relations in the table, we further generate synthetic text-claim pairs with binary veracity labels. These synthetic data are then combined with the original data to fine-tune a BERT-based fact-checking model. Evaluation on two public datasets, PubHealth and SciFact, shows that our pipeline improved F1 scores by up to 0.019 and 0.049, respectively, compared to models trained only on the original data. These results highlight the effectiveness of LLM-driven synthetic data augmentation in enhancing the performance of health-related fact-checkers.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¥åº·é¢†åŸŸäº‹å®æ ¸æŸ¥(Fact-checking)ä¸­ç¼ºä¹æ ‡æ³¨è®­ç»ƒæ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆåˆæˆæ•°æ®ä»¥å¢å¼ºè®­ç»ƒé›†çš„è‡ªåŠ¨åŒ–æµæ°´çº¿ã€‚è¯¥æµæ°´çº¿é€šè¿‡å¯¹æºæ–‡æ¡£è¿›è¡Œæ‘˜è¦æå–å¹¶åˆ†è§£ä¸ºåŸå­äº‹å®(atomic facts)ï¼Œåˆ©ç”¨LLMæ„å»ºå¥å­-äº‹å®è•´å«è¡¨(sentence-fact entailment tables)ï¼Œè¿›è€Œç”Ÿæˆå¸¦æœ‰äºŒå…ƒçœŸå®æ€§æ ‡ç­¾çš„åˆæˆæ–‡æœ¬-ä¸»å¼ å¯¹ã€‚éšåï¼Œè¿™äº›åˆæˆæ•°æ®ä¸åŸå§‹æ•°æ®ç›¸ç»“åˆï¼Œç”¨äºå¾®è°ƒåŸºäºBERTçš„äº‹å®æ ¸æŸ¥æ¨¡å‹ã€‚åœ¨PubHealthå’ŒSciFactä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä½¿æ¨¡å‹çš„F1åˆ†æ•°åˆ†åˆ«æå‡äº†0.019å’Œ0.049ã€‚ç ”ç©¶ç»“æœè¯æ˜äº†LLMé©±åŠ¨çš„åˆæˆæ•°æ®å¢å¼ºæŠ€æœ¯åœ¨æå‡å¥åº·ç›¸å…³äº‹å®æ ¸æŸ¥å™¨æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè§£å†³ä¸“ä¸šé¢†åŸŸæ•°æ®åŒ®ä¹é—®é¢˜æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20525v1",
      "published_date": "2025-08-28 08:06:33 UTC",
      "updated_date": "2025-08-28 08:06:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:56:09.489985+00:00"
    },
    {
      "arxiv_id": "2508.20517v1",
      "title": "BridgeShield: Enhancing Security for Cross-chain Bridge Applications via Heterogeneous Graph Mining",
      "title_zh": "BridgeShieldï¼šé€šè¿‡å¼‚æ„å›¾æŒ–æ˜å¢å¼ºè·¨é“¾æ¡¥åº”ç”¨çš„å®‰å…¨æ€§",
      "authors": [
        "Dan Lin",
        "Shunfeng Lu",
        "Ziyan Liu",
        "Jiajing Wu",
        "Junyuan Fang",
        "Kaixin Lin",
        "Bowen Song",
        "Zibin Zheng"
      ],
      "abstract": "Cross-chain bridges play a vital role in enabling blockchain interoperability. However, due to the inherent design flaws and the enormous value they hold, they have become prime targets for hacker attacks. Existing detection methods show progress yet remain limited, as they mainly address single-chain behaviors and fail to capture cross-chain semantics. To address this gap, we leverage heterogeneous graph attention networks, which are well-suited for modeling multi-typed entities and relations, to capture the complex execution semantics of cross-chain behaviors. We propose BridgeShield, a detection framework that jointly models the source chain, off-chain coordination, and destination chain within a unified heterogeneous graph representation. BridgeShield incorporates intra-meta-path attention to learn fine-grained dependencies within cross-chain paths and inter-meta-path attention to highlight discriminative cross-chain patterns, thereby enabling precise identification of attack behaviors. Extensive experiments on 51 real-world cross-chain attack events demonstrate that BridgeShield achieves an average F1-score of 92.58%, representing a 24.39% improvement over state-of-the-art baselines. These results validate the effectiveness of BridgeShield as a practical solution for securing cross-chain bridges and enhancing the resilience of multi-chain ecosystems.",
      "tldr_zh": "è·¨é“¾æ¡¥ï¼ˆCross-chain bridgesï¼‰åœ¨åŒºå—é“¾äº’æ“ä½œæ€§ä¸­èµ·ç€è‡³å…³é‡è¦ä½œç”¨ï¼Œä½†ç”±äºè®¾è®¡ç¼ºé™·å¸¸æˆä¸ºé»‘å®¢æ”»å‡»çš„ç›®æ ‡ï¼Œä¸”ç°æœ‰æ£€æµ‹æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ•æ‰å¤æ‚çš„è·¨é“¾è¯­ä¹‰ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶æå‡ºäº† BridgeShield æ£€æµ‹æ¡†æ¶ï¼Œåˆ©ç”¨å¼‚æ„å›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆheterogeneous graph attention networksï¼‰åœ¨ç»Ÿä¸€çš„å¼‚æ„å›¾è¡¨ç¤ºä¸­å…±åŒå»ºæ¨¡æºé“¾ã€é“¾ä¸‹åè°ƒå’Œç›®æ ‡é“¾ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å…ƒè·¯å¾„å†…ï¼ˆintra-meta-pathï¼‰å’Œå…ƒè·¯å¾„é—´ï¼ˆinter-meta-pathï¼‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨å­¦ä¹ è·¨é“¾è·¯å¾„ä¸­çš„ç»†ç²’åº¦ä¾èµ–å…³ç³»å¹¶è¯†åˆ«åˆ¤åˆ«æ€§çš„æ”»å‡»æ¨¡å¼ã€‚åœ¨ 51 ä¸ªçœŸå®è·¨é“¾æ”»å‡»äº‹ä»¶ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBridgeShield çš„å¹³å‡ F1-score è¾¾åˆ° 92.58%ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„åŸºå‡†æ–¹æ³•æå‡äº† 24.39%ã€‚è¿™äº›ç»“æœéªŒè¯äº† BridgeShield ä½œä¸ºä¸€ç§ä¿éšœè·¨é“¾æ¡¥å®‰å…¨å¹¶å¢å¼ºå¤šé“¾ç”Ÿæ€ç³»ç»ŸéŸ§æ€§çš„å®ç”¨è§£å†³æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20517v1",
      "published_date": "2025-08-28 07:59:20 UTC",
      "updated_date": "2025-08-28 07:59:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:56:25.787993+00:00"
    },
    {
      "arxiv_id": "2508.20511v1",
      "title": "Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark",
      "title_zh": "ä»è¢«é—å¿˜çš„è¯­è¨€ï¼šè¿ˆå‘æ›´å®Œå–„çš„å¤šè¯­è¨€æœºå™¨ç¿»è¯‘è¯„æµ‹åŸºå‡†",
      "authors": [
        "Chihiro Taguchi",
        "Seng Mai",
        "Keita Kurabe",
        "Yusuke Sakai",
        "Georgina Agyei",
        "Soudabeh Eslami",
        "David Chiang"
      ],
      "abstract": "Multilingual machine translation (MT) benchmarks play a central role in evaluating the capabilities of modern MT systems. Among them, the FLORES+ benchmark is widely used, offering English-to-many translation data for over 200 languages, curated with strict quality control protocols. However, we study data in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani) and uncover critical shortcomings in the benchmark's suitability for truly multilingual evaluation. Human assessments reveal that many translations fall below the claimed 90% quality standard, and the annotators report that source sentences are often too domain-specific and culturally biased toward the English-speaking world. We further demonstrate that simple heuristics, such as copying named entities, can yield non-trivial BLEU scores, suggesting vulnerabilities in the evaluation protocol. Notably, we show that MT models trained on high-quality, naturalistic data perform poorly on FLORES+ while achieving significant gains on our domain-relevant evaluation set. Based on these findings, we advocate for multilingual MT benchmarks that use domain-general and culturally neutral source texts rely less on named entities, in order to better reflect real-world translation challenges.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥åˆ†æäº†å¹¿æ³›ä½¿ç”¨çš„FLORES+å¤šè¯­è¨€æœºå™¨ç¿»è¯‘(MT)åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°ç°ä»£MTç³»ç»Ÿæ—¶çš„å±€é™æ€§ã€‚é€šè¿‡å¯¹å››ç§ç‰¹å®šè¯­è¨€çš„äººç±»è¯„ä¼°ï¼Œç ”ç©¶å‘ç°ç¿»è¯‘è´¨é‡å¾€å¾€ä½äºå®£ç§°çš„90%æ ‡å‡†ï¼Œä¸”æºå¥åœ¨é¢†åŸŸé€‰æ‹©å’Œæ–‡åŒ–èƒŒæ™¯ä¸Šè¡¨ç°å‡ºæ˜æ˜¾çš„è‹±è¯­ä¸–ç•Œåè§ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ­ç¤ºäº†ç°æœ‰è¯„ä¼°åè®®çš„è„†å¼±æ€§ï¼Œè¯æ˜ä»…é€šè¿‡å‘½åå®ä½“(named entities)å¤åˆ¶ç­‰ç®€å•å¯å‘å¼æ–¹æ³•å³å¯è·å¾—æ˜¾è‘—çš„BLEUå¾—åˆ†ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥è¡¨æ˜ï¼Œåœ¨é«˜è´¨é‡è‡ªç„¶æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹è™½ç„¶åœ¨é¢†åŸŸç›¸å…³æµ‹è¯•ä¸­è¿›æ­¥æ˜¾è‘—ï¼Œä½†åœ¨FLORES+ä¸Šçš„è¡¨ç°å´ä¸å°½å¦‚äººæ„ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œç ”ç©¶è€…å»ºè®®æœªæ¥çš„å¤šè¯­è¨€æœºå™¨ç¿»è¯‘åŸºå‡†åº”é‡‡ç”¨é€šç”¨é¢†åŸŸä¸”æ–‡åŒ–ä¸­ç«‹çš„æºæ–‡æœ¬ï¼Œå¹¶å‡å°‘å¯¹å‘½åå®ä½“çš„ä¾èµ–ï¼Œä»è€Œæ›´çœŸå®åœ°åæ˜ ç°å®ä¸–ç•Œçš„ç¿»è¯‘æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages, 7 tables, 2 figures. Accepted at EMNLP Main 2025. Code and data released at https://github.com/ctaguchi/LSLB",
      "pdf_url": "https://arxiv.org/pdf/2508.20511v1",
      "published_date": "2025-08-28 07:52:42 UTC",
      "updated_date": "2025-08-28 07:52:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:56:31.882366+00:00"
    },
    {
      "arxiv_id": "2509.22660v1",
      "title": "Fairness for niche users and providers: algorithmic choice and profile portability",
      "title_zh": "é’ˆå¯¹å°ä¼—ç”¨æˆ·ä¸æä¾›è€…çš„å…¬å¹³æ€§ï¼šç®—æ³•é€‰æ‹©ä¸ç”»åƒå¯ç§»æ¤æ€§",
      "authors": [
        "Elizabeth McKinnie",
        "Anas Buhayh",
        "Clement Canel",
        "Robin Burke"
      ],
      "abstract": "Ensuring fair outcomes for multiple stakeholders in recommender systems has been studied mostly in terms of algorithmic interventions: building new models with better fairness properties, or using reranking to improve outcomes from an existing algorithm. What has rarely been studied is structural changes in the recommendation ecosystem itself. Our work explores the fairness impact of algorithmic pluralism, the idea that the recommendation algorithm is decoupled from the platform through which users access content, enabling user choice in algorithms. Prior work using a simulation approach has shown that niche consumers and (especially) niche providers benefit from algorithmic choice. In this paper, we use simulation to explore the question of profile portability, to understand how different policies regarding the handling of user profiles interact with fairness outcomes for consumers and providers.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ¨èç³»ç»Ÿä¸­å¤šæ–¹åˆ©ç›Šç›¸å…³è€…çš„å…¬å¹³æ€§é—®é¢˜ï¼Œé‡ç‚¹åˆ†æäº†æ¨èç”Ÿæ€ç³»ç»Ÿçš„ç»“æ„æ€§å˜é©å¯¹å…¬å¹³æ€§çš„å½±å“ã€‚è®ºæ–‡ç ”ç©¶äº†ç®—æ³•å¤šå…ƒåŒ– (algorithmic pluralism) çš„åº”ç”¨ï¼Œå³é€šè¿‡å°†ç®—æ³•ä¸å¹³å°è§£è€¦ï¼Œèµ‹äºˆç”¨æˆ·è‡ªä¸»é€‰æ‹©ç®—æ³•çš„æƒåˆ©ã€‚ä½œè€…åˆ©ç”¨æ¨¡æ‹Ÿå®éªŒ (simulation) æ–¹æ³•ï¼Œæ·±å…¥æ¢è®¨äº†ç”¨æˆ·ç”»åƒè¿ç§» (profile portability) åœ¨ä¸åŒç­–ç•¥ä¸‹å¦‚ä½•å½±å“æ¶ˆè´¹è€…ä¸æä¾›è€…çš„å…¬å¹³æ€§äº§å‡ºã€‚ç ”ç©¶å‘ç°ï¼Œç®—æ³•é€‰æ‹©èƒ½å¤Ÿæœ‰æ•ˆæ”¹å–„å°ä¼—æ¶ˆè´¹è€… (niche consumers) å’Œå°ä¼—æä¾›è€… (niche providers) çš„è·ç›Šæƒ…å†µã€‚è¯¥å·¥ä½œæ­ç¤ºäº†ç”¨æˆ·ç”»åƒå¤„ç†æ”¿ç­–ä¸ç®—æ³•é€‰æ‹©ä¹‹é—´çš„å¤æ‚äº¤äº’ï¼Œä¸ºæ„å»ºæ›´å…¬å¹³çš„æ¨èç”Ÿæ€ç³»ç»Ÿæä¾›äº†æ–°çš„ç†è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.22660v1",
      "published_date": "2025-08-28 07:38:59 UTC",
      "updated_date": "2025-08-28 07:38:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:56:20.798878+00:00"
    },
    {
      "arxiv_id": "2508.20491v1",
      "title": "CaddieSet: A Golf Swing Dataset with Human Joint Features and Ball Information",
      "title_zh": "CaddieSetï¼šåŒ…å«äººä½“å…³èŠ‚ç‰¹å¾ä¸é«˜å°”å¤«çƒä¿¡æ¯çš„é«˜å°”å¤«æŒ¥æ†æ•°æ®é›†",
      "authors": [
        "Seunghyeon Jung",
        "Seoyoung Hong",
        "Jiwoo Jeong",
        "Seungwon Jeong",
        "Jaerim Choi",
        "Hoki Kim",
        "Woojin Lee"
      ],
      "abstract": "Recent advances in deep learning have led to more studies to enhance golfers' shot precision. However, these existing studies have not quantitatively established the relationship between swing posture and ball trajectory, limiting their ability to provide golfers with the necessary insights for swing improvement. In this paper, we propose a new dataset called CaddieSet, which includes joint information and various ball information from a single shot. CaddieSet extracts joint information from a single swing video by segmenting it into eight swing phases using a computer vision-based approach. Furthermore, based on expert golf domain knowledge, we define 15 key metrics that influence a golf swing, enabling the interpretation of swing outcomes through swing-related features. Through experiments, we demonstrated the feasibility of CaddieSet for predicting ball trajectories using various benchmarks. In particular, we focus on interpretable models among several benchmarks and verify that swing feedback using our joint features is quantitatively consistent with established domain knowledge. This work is expected to offer new insight into golf swing analysis for both academia and the sports industry.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CaddieSetï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäººä½“å…³èŠ‚ç‰¹å¾ä¸çƒä½“è¿åŠ¨ä¿¡æ¯çš„å…¨æ–°é«˜å°”å¤«æŒ¥æ†æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç ”ç©¶ä¸­æŒ¥æ†å§¿æ€ä¸çƒé£è¡Œè½¨è¿¹ä¹‹é—´ç¼ºä¹å®šé‡å…³è”çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†é€šè¿‡åŸºäºè®¡ç®—æœºè§†è§‰(computer vision)çš„æ–¹æ³•ï¼Œå°†å•æ¬¡æŒ¥æ†è§†é¢‘åˆ’åˆ†ä¸º8ä¸ªæŒ¥æ†é˜¶æ®µ(swing phases)å¹¶ä»ä¸­æå–å…³é”®å…³èŠ‚ä¿¡æ¯ã€‚åŸºäºé«˜å°”å¤«é¢†åŸŸçš„ä¸“å®¶çŸ¥è¯†ï¼Œç ”ç©¶è€…å®šä¹‰äº†15ä¸ªå½±å“æŒ¥æ†çš„å…³é”®æŒ‡æ ‡(key metrics)ï¼Œå®ç°äº†é€šè¿‡æŒ¥æ†ç‰¹å¾å¯¹å‡»çƒç»“æœçš„æ·±åº¦è§£é‡Šã€‚å®éªŒè¯æ˜äº†CaddieSetåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­é¢„æµ‹çƒä½“è½¨è¿¹(ball trajectories)çš„å¯è¡Œæ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨å¯è§£é‡Šæ¨¡å‹ä¸­ï¼Œè¯¥æ•°æ®é›†æä¾›çš„å…³èŠ‚ç‰¹å¾åé¦ˆä¸æ—¢æœ‰çš„é¢†åŸŸçŸ¥è¯†åœ¨å®šé‡ä¸Šä¿æŒäº†é«˜åº¦ä¸€è‡´ã€‚è¯¥å·¥ä½œä¸ºå­¦æœ¯ç•Œå’Œä½“è‚²äº§ä¸šçš„é«˜å°”å¤«æŒ¥æ†åˆ†ææä¾›äº†é‡è¦çš„æ–°è§è§£ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages with supplementary material",
      "pdf_url": "https://arxiv.org/pdf/2508.20491v1",
      "published_date": "2025-08-28 07:16:01 UTC",
      "updated_date": "2025-08-28 07:16:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:56:28.192725+00:00"
    },
    {
      "arxiv_id": "2508.21101v1",
      "title": "Beyond Prediction: Reinforcement Learning as the Defining Leap in Healthcare AI",
      "title_zh": "è¶…è¶Šé¢„æµ‹ï¼šå¼ºåŒ–å­¦ä¹ æ˜¯åŒ»ç–—äººå·¥æ™ºèƒ½çš„å†³å®šæ€§é£è·ƒ",
      "authors": [
        "Dilruk Perera",
        "Gousia Habib",
        "Qianyi Xu",
        "Daniel J. Tan",
        "Kai He",
        "Erik Cambria",
        "Mengling Feng"
      ],
      "abstract": "Reinforcement learning (RL) marks a fundamental shift in how artificial intelligence is applied in healthcare. Instead of merely predicting outcomes, RL actively decides interventions with long term goals. Unlike traditional models that operate on fixed associations, RL systems learn through trial, feedback, and long-term reward optimization, introducing transformative possibilities and new risks. From an information fusion lens, healthcare RL typically integrates multi-source signals such as vitals, labs clinical notes, imaging and device telemetry using temporal and decision-level mechanisms. These systems can operate within centralized, federated, or edge architectures to meet real-time clinical constraints, and naturally span data, features and decision fusion levels. This survey explore RL's rise in healthcare as more than a set of tools, rather a shift toward agentive intelligence in clinical environments. We first structure the landscape of RL techniques including model-based and model-free methods, offline and batch-constrained approaches, and emerging strategies for reward specification and uncertainty calibration through the lens of healthcare constraints. We then comprehensively analyze RL applications spanning critical care, chronic disease, mental health, diagnostics, and robotic assistance, identifying their trends, gaps, and translational bottlenecks. In contrast to prior reviews, we critically analyze RL's ethical, deployment, and reward design challenges, and synthesize lessons for safe, human-aligned policy learning. This paper serves as both a a technical roadmap and a critical reflection of RL's emerging transformative role in healthcare AI not as prediction machinery, but as agentive clinical intelligence.",
      "tldr_zh": "è¯¥ç»¼è¿°è®ºæ–‡æ¢è®¨äº†å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) åœ¨åŒ»ç–—äººå·¥æ™ºèƒ½é¢†åŸŸä»å•çº¯çš„é¢„æµ‹æ¨¡å‹å‘å…·å¤‡å†³ç­–èƒ½åŠ›çš„æ™ºèƒ½ä»£ç† (Agentive Clinical Intelligence) çš„æ ¹æœ¬è½¬å˜ã€‚ä¸ä¼ ç»Ÿçš„å›ºå®šå…³è”æ¨¡å‹ä¸åŒï¼ŒRL ç³»ç»Ÿé€šè¿‡å°è¯•ã€åé¦ˆå’Œé•¿æœŸå¥–åŠ±ä¼˜åŒ– (Reward Optimization) æ¥å†³å®šåŒ»ç–—å¹²é¢„æªæ–½ï¼Œå¹¶èƒ½æ•´åˆç”Ÿå‘½ä½“å¾ã€å®éªŒå®¤æ£€æŸ¥å’Œå½±åƒç­‰å¤šæºä¿¡å·ã€‚æ–‡ç« ç³»ç»Ÿæ€§åœ°æ¢³ç†äº† Model-basedã€Model-free åŠç¦»çº¿ (Offline) ç­‰æŠ€æœ¯è·¯å¾„ï¼Œå¹¶é’ˆå¯¹åŒ»ç–—çº¦æŸè®¨è®ºäº†å¥–åŠ±è§„èŒƒ (Reward Specification) ä¸ä¸ç¡®å®šæ€§æ ¡å‡†ã€‚åº”ç”¨åˆ†ææ¶µç›–äº†é‡ç—‡ç›‘æŠ¤ã€æ…¢æ€§ç—…ç®¡ç†ã€å¿ƒç†å¥åº·åŠæœºå™¨äººè¾…åŠ©ç­‰å¤šä¸ªé¢†åŸŸï¼Œè¯†åˆ«äº†å„é¢†åŸŸçš„è¶‹åŠ¿ä¸è½¬åŒ–ç“¶é¢ˆã€‚ç ”ç©¶è¿˜æ·±å…¥å‰–æäº†ä¼¦ç†ã€éƒ¨ç½²æŒ‘æˆ˜åŠäººç±»å¯¹é½ (Human-aligned) ç­–ç•¥ï¼Œä¸ºå®‰å…¨å¯é çš„ä¸´åºŠç­–ç•¥å­¦ä¹ æä¾›äº†æŒ‡å¯¼ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥æ–‡ä¸ä»…æä¾›äº†æŠ€æœ¯è·¯çº¿å›¾ï¼Œæ›´è®ºè¯äº† RL ä½œä¸ºåŒ»ç–— AI å®šä¹‰æ€§é£è·ƒçš„é‡è¦åœ°ä½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "40 pages in total (including appendix)",
      "pdf_url": "https://arxiv.org/pdf/2508.21101v1",
      "published_date": "2025-08-28 07:05:24 UTC",
      "updated_date": "2025-08-28 07:05:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:56:34.386838+00:00"
    },
    {
      "arxiv_id": "2508.20472v1",
      "title": "Photonic restricted Boltzmann machine for content generation tasks",
      "title_zh": "é¢å‘å†…å®¹ç”Ÿæˆä»»åŠ¡çš„å…‰å­å—é™ç»å°”å…¹æ›¼æœº",
      "authors": [
        "Li Luo",
        "Yisheng Fang",
        "Wanyi Zhang",
        "Zhichao Ruan"
      ],
      "abstract": "The restricted Boltzmann machine (RBM) is a neural network based on the Ising model, well known for its ability to learn probability distributions and stochastically generate new content. However, the high computational cost of Gibbs sampling in content generation tasks imposes significant bottlenecks on electronic implementations. Here, we propose a photonic restricted Boltzmann machine (PRBM) that leverages photonic computing to accelerate Gibbs sampling, enabling efficient content generation. By introducing an efficient encoding method, the PRBM eliminates the need for computationally intensive matrix decomposition and reduces the computational complexity of Gibbs sampling from $O(N)$ to $O(1)$. Moreover, its non-Von Neumann photonic computing architecture circumvents the memory storage of interaction matrices, providing substantial advantages for large-scale RBMs. We experimentally validate the photonic-accelerated Gibbs sampling by simulating a two-dimensional Ising model, where the observed phase transition temperature closely matches the theoretical predictions. Beyond physics-inspired tasks, the PRBM demonstrates robust capabilities in generating and restoring diverse content, including images and temporal sequences, even in the presence of noise and aberrations. The scalability and reduced training cost of the PRBM framework underscore its potential as a promising pathway for advancing photonic computing in generative artificial intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å…‰å­å—é™ç»å°”å…¹æ›¼æœº(Photonic Restricted Boltzmann Machine, PRBM)ï¼Œæ—¨åœ¨åˆ©ç”¨å…‰å­è®¡ç®—åŠ é€Ÿå—é™ç»å°”å…¹æ›¼æœº(RBM)åœ¨å†…å®¹ç”Ÿæˆä»»åŠ¡ä¸­çš„ Gibbs sampling è¿‡ç¨‹ã€‚é€šè¿‡å¼•å…¥é«˜æ•ˆçš„ç¼–ç æ–¹æ³•ï¼ŒPRBM é¿å…äº†è®¡ç®—å¯†é›†çš„çŸ©é˜µåˆ†è§£ï¼Œå°† Gibbs sampling çš„è®¡ç®—å¤æ‚åº¦ä» $O(N)$ æ˜¾è‘—é™ä½è‡³ $O(1)$ã€‚è¯¥æ¡†æ¶é‡‡ç”¨éå†¯Â·è¯ºä¾æ›¼(non-Von Neumann)å…‰å­è®¡ç®—æ¶æ„ï¼Œæ— éœ€å­˜å‚¨äº¤äº’çŸ©é˜µï¼Œåœ¨å¤„ç†å¤§è§„æ¨¡ RBM æ—¶å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚å®éªŒé€šè¿‡æ¨¡æ‹ŸäºŒç»´ Ising model éªŒè¯äº†å…‰å­åŠ é€Ÿæ•ˆæœï¼Œç»“æœä¸ç†è®ºé¢„æµ‹é«˜åº¦å»åˆã€‚æ­¤å¤–ï¼ŒPRBM åœ¨å›¾åƒå’Œæ—¶é—´åºåˆ—çš„ç”Ÿæˆä¸ä¿®å¤ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å™ªå£°å’Œç•¸å˜ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº† PRBM çš„å¯æ‰©å±•æ€§å’Œä½è®­ç»ƒæˆæœ¬ï¼Œä¸ºå…‰å­è®¡ç®—åœ¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)é¢†åŸŸçš„åº”ç”¨å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "physics.optics",
        "cond-mat.stat-mech",
        "cs.AI",
        "physics.app-ph"
      ],
      "primary_category": "physics.optics",
      "comment": "9 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.20472v1",
      "published_date": "2025-08-28 06:40:33 UTC",
      "updated_date": "2025-08-28 06:40:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:56:43.396434+00:00"
    },
    {
      "arxiv_id": "2508.20461v2",
      "title": "Dual-Model Weight Selection and Self-Knowledge Distillation for Medical Image Classification",
      "title_zh": "é¢å‘åŒ»å­¦å›¾åƒåˆ†ç±»çš„åŒæ¨¡å‹æƒé‡é€‰æ‹©ä¸è‡ªçŸ¥è¯†è’¸é¦",
      "authors": [
        "Ayaka Tsutsumi",
        "Guang Li",
        "Ren Togo",
        "Takahiro Ogawa",
        "Satoshi Kondo",
        "Miki Haseyama"
      ],
      "abstract": "We propose a novel medical image classification method that integrates dual-model weight selection with self-knowledge distillation (SKD). In real-world medical settings, deploying large-scale models is often limited by computational resource constraints, which pose significant challenges for their practical implementation. Thus, developing lightweight models that achieve comparable performance to large-scale models while maintaining computational efficiency is crucial. To address this, we employ a dual-model weight selection strategy that initializes two lightweight models with weights derived from a large pretrained model, enabling effective knowledge transfer. Next, SKD is applied to these selected models, allowing the use of a broad range of initial weight configurations without imposing additional excessive computational cost, followed by fine-tuning for the target classification tasks. By combining dual-model weight selection with self-knowledge distillation, our method overcomes the limitations of conventional approaches, which often fail to retain critical information in compact models. Extensive experiments on publicly available datasets-chest X-ray images, lung computed tomography scans, and brain magnetic resonance imaging scans-demonstrate the superior performance and robustness of our approach compared to existing methods.",
      "tldr_zh": "é’ˆå¯¹åŒ»ç–—å½±åƒåˆ†ç±»åœ¨å®é™…éƒ¨ç½²ä¸­å—é™äºè®¡ç®—èµ„æºã€éš¾ä»¥åº”ç”¨å¤§å‹æ¨¡å‹çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆåŒæ¨¡å‹æƒé‡é€‰æ‹©(Dual-Model Weight Selection)ä¸è‡ªçŸ¥è¯†è’¸é¦(Self-Knowledge Distillation, SKD)çš„æ–°å‹åˆ†ç±»æ–¹æ³•ã€‚è¯¥æ–¹æ³•é¦–å…ˆé‡‡ç”¨åŒæ¨¡å‹æƒé‡é€‰æ‹©ç­–ç•¥ï¼Œåˆ©ç”¨å¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡åˆå§‹åŒ–ä¸¤ä¸ªè½»é‡çº§æ¨¡å‹ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„çŸ¥è¯†è¿ç§»ã€‚éšåï¼Œç ”ç©¶å°†SKDåº”ç”¨äºè¿™äº›é€‰å®šæ¨¡å‹ï¼Œåœ¨ä¸å¢åŠ é¢å¤–è®¡ç®—è´Ÿæ‹…çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨å¹¿æ³›çš„åˆå§‹æƒé‡é…ç½®è¿›è¡Œä¼˜åŒ–ï¼Œå¹¶é’ˆå¯¹ç›®æ ‡åˆ†ç±»ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚é€šè¿‡è¿™ç§ç»“åˆï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆå…‹æœäº†ä¼ ç»Ÿå‹ç¼©æ¨¡å‹åœ¨ä¿ç•™å…³é”®ä¿¡æ¯æ–¹é¢çš„å±€é™æ€§ã€‚åœ¨èƒ¸éƒ¨Xå°„çº¿ã€è‚ºéƒ¨CTå’Œè„‘éƒ¨MRIç­‰å…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œå…¶åˆ†ç±»æ€§èƒ½å’Œé²æ£’æ€§å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Published as a journal paper at Elsevier CIBM",
      "pdf_url": "https://arxiv.org/pdf/2508.20461v2",
      "published_date": "2025-08-28 06:15:06 UTC",
      "updated_date": "2025-11-27 11:50:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:56:52.390972+00:00"
    },
    {
      "arxiv_id": "2509.04460v1",
      "title": "CoCoNUTS: Concentrating on Content while Neglecting Uninformative Textual Styles for AI-Generated Peer Review Detection",
      "title_zh": "CoCoNUTSï¼šèšç„¦å®è´¨å†…å®¹å¹¶å¿½ç•¥éä¿¡æ¯æ€§æ–‡æœ¬é£æ ¼çš„ AI ç”ŸæˆåŒè¡Œè¯„å®¡æ£€æµ‹",
      "authors": [
        "Yihan Chen",
        "Jiawei Chen",
        "Guozhao Mo",
        "Xuanang Chen",
        "Ben He",
        "Xianpei Han",
        "Le Sun"
      ],
      "abstract": "The growing integration of large language models (LLMs) into the peer review process presents potential risks to the fairness and reliability of scholarly evaluation. While LLMs offer valuable assistance for reviewers with language refinement, there is growing concern over their use to generate substantive review content. Existing general AI-generated text detectors are vulnerable to paraphrasing attacks and struggle to distinguish between surface language refinement and substantial content generation, suggesting that they primarily rely on stylistic cues. When applied to peer review, this limitation can result in unfairly suspecting reviews with permissible AI-assisted language enhancement, while failing to catch deceptively humanized AI-generated reviews. To address this, we propose a paradigm shift from style-based to content-based detection. Specifically, we introduce CoCoNUTS, a content-oriented benchmark built upon a fine-grained dataset of AI-generated peer reviews, covering six distinct modes of human-AI collaboration. Furthermore, we develop CoCoDet, an AI review detector via a multi-task learning framework, designed to achieve more accurate and robust detection of AI involvement in review content. Our work offers a practical foundation for evaluating the use of LLMs in peer review, and contributes to the development of more precise, equitable, and reliable detection methods for real-world scholarly applications. Our code and data will be publicly available at https://github.com/Y1hanChen/COCONUTS.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å­¦æœ¯åŒè¡Œè¯„å®¡ä¸­çš„æ½œåœ¨é£é™©ï¼ŒæŒ‡å‡ºç›®å‰çš„AIæ£€æµ‹å™¨ä¸»è¦ä¾èµ–é£æ ¼ç‰¹å¾(stylistic cues)ï¼Œéš¾ä»¥åŒºåˆ†åˆæ³•çš„è¯­è¨€ç²¾ç®€ä¸å®è´¨æ€§çš„å†…å®¹ç”Ÿæˆã€‚é’ˆå¯¹è¿™ä¸€å±€é™ï¼Œä½œè€…æå‡ºäº†CoCoNUTSï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“æ³¨äºå†…å®¹è€Œéé£æ ¼çš„åŸºå‡†ï¼ŒåŒ…å«æ¶µç›–å…­ç§äººæœºåä½œæ¨¡å¼çš„ç»†ç²’åº¦æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼€å‘äº†åŸºäºå¤šä»»åŠ¡å­¦ä¹ (multi-task learning)æ¡†æ¶çš„æ£€æµ‹å™¨CoCoDetï¼Œä»¥å®ç°å¯¹è¯„å®¡å†…å®¹ä¸­AIå‚ä¸æƒ…å†µæ›´å‡†ç¡®ä¸”é²æ£’çš„æ£€æµ‹ã€‚è¯¥å·¥ä½œé€šè¿‡æ¨åŠ¨ä»é£æ ¼æ£€æµ‹å‘å†…å®¹æ£€æµ‹çš„èŒƒå¼è½¬å˜ï¼Œä¸ºè¯„ä¼°LLMsåœ¨åŒè¡Œè¯„å®¡ä¸­çš„ä½¿ç”¨æä¾›äº†å®è·µåŸºç¡€ã€‚è¿™ä¸€ç ”ç©¶æˆæœæœ‰åŠ©äºæ„å»ºæ›´ç²¾ç¡®ã€å…¬å¹³å’Œå¯é çš„å­¦æœ¯è¯„ä»·ç¯å¢ƒï¼Œä¸ºè¯†åˆ«çœŸå®å­¦æœ¯åœºæ™¯ä¸‹çš„AIä»‹å…¥æä¾›äº†æœ‰æ•ˆå·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04460v1",
      "published_date": "2025-08-28 06:03:11 UTC",
      "updated_date": "2025-08-28 06:03:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:57:39.066098+00:00"
    },
    {
      "arxiv_id": "2508.20452v1",
      "title": "Evaluating Differentially Private Generation of Domain-Specific Text",
      "title_zh": "è¯„ä¼°ç‰¹å®šé¢†åŸŸæ–‡æœ¬çš„å·®åˆ†éšç§ç”Ÿæˆ",
      "authors": [
        "Yidan Sun",
        "Viktor Schlegel",
        "Srinivasan Nandakumar",
        "Iqra Zahid",
        "Yuping Wu",
        "Warren Del-Pinto",
        "Goran Nenadic",
        "Siew-Kei Lam",
        "Jie Zhang",
        "Anil A Bharath"
      ],
      "abstract": "Generative AI offers transformative potential for high-stakes domains such as healthcare and finance, yet privacy and regulatory barriers hinder the use of real-world data. To address this, differentially private synthetic data generation has emerged as a promising alternative. In this work, we introduce a unified benchmark to systematically evaluate the utility and fidelity of text datasets generated under formal Differential Privacy (DP) guarantees. Our benchmark addresses key challenges in domain-specific benchmarking, including choice of representative data and realistic privacy budgets, accounting for pre-training and a variety of evaluation metrics. We assess state-of-the-art privacy-preserving generation methods across five domain-specific datasets, revealing significant utility and fidelity degradation compared to real data, especially under strict privacy constraints. These findings underscore the limitations of current approaches, outline the need for advanced privacy-preserving data sharing methods and set a precedent regarding their evaluation in realistic scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—å’Œé‡‘èç­‰é«˜é£é™©é¢†åŸŸä¸­ Generative AI é¢ä¸´çš„éšç§å’Œç›‘ç®¡éšœç¢ï¼Œæ¢è®¨äº† Differential Privacy (DP) åˆæˆæ•°æ®ç”Ÿæˆä½œä¸ºæ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ benchmarkï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°åœ¨æ­£å¼ DP ä¿è¯ä¸‹ç”Ÿæˆçš„æ–‡æœ¬æ•°æ®é›†çš„ utility å’Œ fidelityã€‚è¯¥ benchmark è§£å†³äº†ç‰¹å®šé¢†åŸŸæµ‹è¯•ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä»£è¡¨æ€§æ•°æ®çš„é€‰æ‹©ã€ç°å®çš„éšç§é¢„ç®—ä»¥åŠå¯¹é¢„è®­ç»ƒå’Œå¤šç§è¯„ä¼°æŒ‡æ ‡çš„è€ƒé‡ã€‚é€šè¿‡åœ¨äº”ä¸ªç‰¹å®šé¢†åŸŸæ•°æ®é›†ä¸Šè¯„ä¼°å°–ç«¯çš„éšç§ä¿æŠ¤ç”Ÿæˆæ–¹æ³•ï¼Œç ”ç©¶å‘ç°è¿™äº›æ–¹æ³•åœ¨ä¸¥æ ¼çš„éšç§çº¦æŸä¸‹ï¼Œå…¶ utility å’Œ fidelity è¾ƒçœŸå®æ•°æ®å­˜åœ¨æ˜¾è‘—ä¸‹é™ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰æ–¹æ³•çš„å±€é™æ€§ï¼Œå¼ºè°ƒäº†å¼€å‘å…ˆè¿›éšç§ä¿æŠ¤æ•°æ®å…±äº«æŠ€æœ¯çš„ç´§è¿«æ€§ï¼Œå¹¶ä¸ºåœ¨ç°å®åœºæ™¯ä¸­è¿›è¡Œè¯„ä¼°å»ºç«‹äº†å…ˆä¾‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20452v1",
      "published_date": "2025-08-28 05:57:47 UTC",
      "updated_date": "2025-08-28 05:57:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:56:52.998992+00:00"
    },
    {
      "arxiv_id": "2508.20443v2",
      "title": "Towards Mitigating Excessive Forgetting in LLM Unlearning via Entanglement-Guidance with Proxy Constraint",
      "title_zh": "é€šè¿‡å¸¦ä»£ç†çº¦æŸçš„çº ç¼ å¼•å¯¼ç¼“è§£å¤§è¯­è¨€æ¨¡å‹é—å¿˜å­¦ä¹ ä¸­çš„è¿‡åº¦é—å¿˜",
      "authors": [
        "Zhihao Liu",
        "Jian Lou",
        "Yuke Hu",
        "Xiaochen Li",
        "Yitian Chen",
        "Tailun Chen",
        "Zhizhen Qin",
        "Kui Ren",
        "Zhan Qin"
      ],
      "abstract": "Large language models (LLMs) are trained on massive datasets that may include private or copyrighted content. Due to growing privacy and ownership concerns, data owners may request the removal of their data from trained models. Machine unlearning provides a practical solution by removing the influence of specific data without full retraining. However, most existing methods still suffer from over-unlearning due to the lack of a principled mechanism to regulate the forgetting boundary, leading to unnecessary utility degradation and heightened privacy and robustness risks. In this work, we propose EGUP (Entanglement-Guided Unlearning with Proxy Constraint), a novel framework that leverages entanglement and proxy constraint to guide the unlearning process while mitigating over-unlearning. Within each iteration, EGUP employs inter-sample entanglement to adaptively reweight the unlearning strength, assigning greater unlearning efforts to forget samples that are semantically closer to retained knowledge. Across iterations, EGUP leverages intra-sample entanglement to track the representation shift of each forget sample and dynamically adjust its unlearning effort. In addition, we incorporate a proxy constraint that approximates the model's expected outputs after unlearning, forming a reference boundary that softly regularizes the unlearning process. EGUP is compatible with existing gradient-based objectives and serves as a plug-and-play enhancement. We evaluate EGUP on the TOFU and MUSE benchmarks, demonstrating consistent improvements in the unlearning-utility trade-off across multiple LLMs. Moreover, EGUP achieves performance close to the retrained model while remaining scalable and robust.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EGUP (Entanglement-Guided Unlearning with Proxy Constraint) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æœºå™¨å–æ¶ˆå­¦ä¹ (Machine Unlearning)è¿‡ç¨‹ä¸­ç”±äºç¼ºä¹æ˜ç¡®é—å¿˜è¾¹ç•Œè€Œå¯¼è‡´çš„è¿‡åº¦é—å¿˜(Over-unlearning)åŠæ€§èƒ½ä¸‹é™é—®é¢˜ã€‚è¯¥æ¡†æ¶åœ¨è¿­ä»£å†…éƒ¨åˆ©ç”¨æ ·æœ¬é—´çº ç¼ (Inter-sample entanglement)æ ¹æ®é—å¿˜æ ·æœ¬ä¸ä¿ç•™çŸ¥è¯†çš„è¯­ä¹‰è·ç¦»è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ å¼ºåº¦ï¼Œå¹¶åœ¨è¿­ä»£é—´é€šè¿‡æ ·æœ¬å†…çº ç¼ (Intra-sample entanglement)åŠ¨æ€è¿½è¸ªè¡¨å¾åç§»ä»¥è°ƒæ•´åŠªåŠ›ç¨‹åº¦ã€‚æ­¤å¤–ï¼ŒEGUPå¼•å…¥äº†ä»£ç†çº¦æŸ(Proxy constraint)æ¥æ¨¡æ‹Ÿæ¨¡å‹åœ¨å–æ¶ˆå­¦ä¹ åçš„é¢„æœŸè¾“å‡ºï¼Œä¸ºå–æ¶ˆå­¦ä¹ è¿‡ç¨‹æä¾›è½¯æ­£åˆ™åŒ–çš„å‚è€ƒè¾¹ç•Œã€‚ä½œä¸ºä¸€ç§å³æ’å³ç”¨çš„å¢å¼ºæ–¹æ¡ˆï¼ŒEGUPåœ¨TOFUå’ŒMUSEåŸºå‡†æµ‹è¯•ä¸­å±•ç°äº†ä¼˜å¼‚çš„å–æ¶ˆå­¦ä¹ ä¸æ¨¡å‹æ•ˆç”¨æƒè¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ‰©å±•æ€§å’Œé²æ£’æ€§çš„åŒæ—¶ï¼Œå…¶æ€§èƒ½å·²æ¥è¿‘å®Œå…¨é‡æ–°è®­ç»ƒ(Retrained)çš„æ¨¡å‹æ°´å¹³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20443v2",
      "published_date": "2025-08-28 05:45:40 UTC",
      "updated_date": "2026-01-12 17:50:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:57:56.760067+00:00"
    },
    {
      "arxiv_id": "2508.20441v1",
      "title": "Uncovering the Spectral Bias in Diagonal State Space Models",
      "title_zh": "æ­ç¤ºå¯¹è§’çŠ¶æ€ç©ºé—´æ¨¡å‹ä¸­çš„é¢‘è°±åå·®",
      "authors": [
        "Ruben Solozabal",
        "Velibor Bojkovic",
        "Hilal AlQuabeh",
        "Kentaro Inui",
        "Martin TakÃ¡Ä"
      ],
      "abstract": "Current methods for initializing state space models (SSMs) parameters mainly rely on the \\textit{HiPPO framework}, which is based on an online approximation of orthogonal polynomials. Recently, diagonal alternatives have shown to reach a similar level of performance while being significantly more efficient due to the simplification in the kernel computation. However, the \\textit{HiPPO framework} does not explicitly study the role of its diagonal variants. In this paper, we take a further step to investigate the role of diagonal SSM initialization schemes from the frequency perspective. Our work seeks to systematically understand how to parameterize these models and uncover the learning biases inherent in such diagonal state-space models. Based on our observations, we propose a diagonal initialization on the discrete Fourier domain \\textit{S4D-DFouT}. The insights in the role of pole placing in the initialization enable us to further scale them and achieve state-of-the-art results on the Long Range Arena benchmark, allowing us to train from scratch on very large datasets as PathX-256.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¯¹è§’çŠ¶æ€ç©ºé—´æ¨¡å‹(Diagonal State Space Models)ä¸­çš„è°±åç½®(Spectral Bias)é—®é¢˜ï¼ŒæŒ‡å‡ºè™½ç„¶ç°æœ‰çš„SSMåˆå§‹åŒ–ä¸»è¦ä¾èµ–äºåŸºäºæ­£äº¤å¤šé¡¹å¼çš„HiPPOæ¡†æ¶ï¼Œä½†è¯¥æ¡†æ¶å¹¶æœªæ˜ç¡®è§£é‡Šå…¶å¯¹è§’å˜ä½“çš„æœ‰æ•ˆæ€§ã€‚ä½œè€…é€šè¿‡é¢‘ç‡è§†è§’ç³»ç»Ÿåœ°ç ”ç©¶äº†å¯¹è§’SSMåˆå§‹åŒ–æ–¹æ¡ˆï¼Œæ—¨åœ¨æ­ç¤ºæ­¤ç±»æ¨¡å‹å›ºæœ‰çš„å­¦ä¹ åç½®åŠå…¶å‚æ•°åŒ–æœºç†ã€‚åŸºäºç ”ç©¶å‘ç°ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åœ¨ç¦»æ•£å‚…é‡Œå¶åŸŸ(discrete Fourier domain)è¿›è¡Œçš„å¯¹è§’åˆå§‹åŒ–æ–¹æ³•ï¼Œç§°ä¸ºS4D-DFouTã€‚é€šè¿‡æ·±å…¥ç†è§£åˆå§‹åŒ–ä¸­æç‚¹é…ç½®(pole placing)çš„ä½œç”¨ï¼Œè¯¥æ–¹æ³•æˆåŠŸå®ç°äº†æ¨¡å‹çš„è¿›ä¸€æ­¥æ‰©å±•ã€‚å®éªŒè¡¨æ˜ï¼ŒS4D-DFouTåœ¨Long Range Arena(LRA)åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å½“å‰æœ€ä¼˜(SOTA)çš„ç»“æœï¼Œå¹¶è¯æ˜äº†åœ¨PathX-256ç­‰è¶…å¤§è§„æ¨¡æ•°æ®é›†ä¸Šä»é›¶è®­ç»ƒçš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20441v1",
      "published_date": "2025-08-28 05:39:04 UTC",
      "updated_date": "2025-08-28 05:39:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:57:55.356058+00:00"
    },
    {
      "arxiv_id": "2508.20437v1",
      "title": "On Identifying Why and When Foundation Models Perform Well on Time-Series Forecasting Using Automated Explanations and Rating",
      "title_zh": "åŸºäºè‡ªåŠ¨è§£é‡Šä¸è¯„åˆ†æ¢ç©¶åŸºç¡€æ¨¡å‹åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­è¡¨ç°ä¼˜å¼‚çš„åŸå› ä¸æ—¶æœº",
      "authors": [
        "Michael Widener",
        "Kausik Lakkaraju",
        "John Aydin",
        "Biplav Srivastava"
      ],
      "abstract": "Time-series forecasting models (TSFM) have evolved from classical statistical methods to sophisticated foundation models, yet understanding why and when these models succeed or fail remains challenging. Despite this known limitation, time series forecasting models are increasingly used to generate information that informs real-world actions with equally real consequences. Understanding the complexity, performance variability, and opaque nature of these models then becomes a valuable endeavor to combat serious concerns about how users should interact with and rely on these models' outputs. This work addresses these concerns by combining traditional explainable AI (XAI) methods with Rating Driven Explanations (RDE) to assess TSFM performance and interpretability across diverse domains and use cases. We evaluate four distinct model architectures: ARIMA, Gradient Boosting, Chronos (time-series specific foundation model), Llama (general-purpose; both fine-tuned and base models) on four heterogeneous datasets spanning finance, energy, transportation, and automotive sales domains. In doing so, we demonstrate that feature-engineered models (e.g., Gradient Boosting) consistently outperform foundation models (e.g., Chronos) in volatile or sparse domains (e.g., power, car parts) while providing more interpretable explanations, whereas foundation models excel only in stable or trend-driven contexts (e.g., finance).",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹(TSFM)ä»ä¼ ç»Ÿç»Ÿè®¡æ–¹æ³•å‘åŸºç¡€æ¨¡å‹(Foundation Models)æ¼”è¿›è¿‡ç¨‹ä¸­ï¼Œå¦‚ä½•ç†è§£æ¨¡å‹åœ¨ç‰¹å®šåœºæ™¯ä¸‹è¡¨ç°ä¼˜åŠ£çš„åŸå› åŠé€æ˜åº¦æŒ‘æˆ˜ã€‚ä¸ºäº†æå‡æ¨¡å‹çš„å¯ä¿¡åº¦ï¼Œç ”ç©¶é€šè¿‡ç»“åˆä¼ ç»Ÿçš„å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)æ–¹æ³•ä¸è¯„çº§é©±åŠ¨è§£é‡Š(Rating Driven Explanations, RDE)ï¼Œåœ¨é‡‘èã€èƒ½æºã€äº¤é€šå’Œæ±½è½¦é”€å”®ç­‰å››ä¸ªå¼‚æ„æ•°æ®é›†ä¸Šè¯„ä¼°äº†ä¸åŒæ¶æ„çš„æ€§èƒ½ã€‚å®éªŒå¯¹æ¯”äº†ARIMAã€æ¢¯åº¦æå‡(Gradient Boosting)ã€ä¸“é—¨çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹Chronosä»¥åŠé€šç”¨å¤§è¯­è¨€æ¨¡å‹Llamaçš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨ç”µåŠ›å’Œæ±½è½¦é›¶éƒ¨ä»¶ç­‰æ³¢åŠ¨æ€§å¤§æˆ–æ•°æ®ç¨€ç–çš„é¢†åŸŸï¼Œç»è¿‡ç‰¹å¾å·¥ç¨‹å¤„ç†çš„æ¨¡å‹(å¦‚Gradient Boosting)åœ¨è¡¨ç°å’Œå¯è§£é‡Šæ€§ä¸Šå‡ä¼˜äºåŸºç¡€æ¨¡å‹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºç¡€æ¨¡å‹ä»…åœ¨é‡‘èç­‰ç¨³å®šæˆ–è¶‹åŠ¿é©±åŠ¨çš„èƒŒæ™¯ä¸‹è¡¨ç°å‡ºè‰²ã€‚è¯¥å·¥ä½œæ­ç¤ºäº†åŸºç¡€æ¨¡å‹åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„å±€é™æ€§ï¼Œå¹¶ä¸ºç”¨æˆ·åœ¨ä¸åŒé¢†åŸŸé€‰æ‹©å’Œä¾èµ–æ¨¡å‹è¾“å‡ºæä¾›äº†é‡è¦ä¾æ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 5 Tables, 5 Figures, AI Trustworthiness and Risk Assessment for Challenged Contexts (ATRACC), Appendix",
      "pdf_url": "https://arxiv.org/pdf/2508.20437v1",
      "published_date": "2025-08-28 05:27:45 UTC",
      "updated_date": "2025-08-28 05:27:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:57:58.246616+00:00"
    },
    {
      "arxiv_id": "2508.20427v2",
      "title": "Rethinking Purity and Diversity in Multi-Behavior Sequential Recommendation from the Frequency Perspective",
      "title_zh": "ä»é¢‘ç‡è§†è§’é‡æ–°å®¡è§†å¤šè¡Œä¸ºåºåˆ—æ¨èä¸­çš„çº¯å‡€åº¦ä¸å¤šæ ·æ€§",
      "authors": [
        "Yongqiang Han",
        "Kai Cheng",
        "Kefan Wang",
        "Enhong Chen"
      ],
      "abstract": "In recommendation systems, users often exhibit multiple behaviors, such as browsing, clicking, and purchasing. Multi-behavior sequential recommendation (MBSR) aims to consider these different behaviors in an integrated manner to improve the recommendation performance of the target behavior. However, some behavior data will also bring inevitable noise to the modeling of user interests. Some research efforts focus on data denoising from the frequency domain perspective to improve the accuracy of user preference prediction. These studies indicate that low-frequency information tends to be valuable and reliable, while high-frequency information is often associated with noise. In this paper, we argue that high-frequency information is by no means insignificant. Further experimental results highlight that low frequency corresponds to the purity of user interests, while high frequency corresponds to the diversity of user interests. Building upon this finding, we proposed our model PDB4Rec, which efficiently extracts information across various frequency bands and their relationships, and introduces Boostrapping Balancer mechanism to balance their contributions for improved recommendation performance. Sufficient experiments on real-world datasets demonstrate the effectiveness and efficiency of our model.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šè¡Œä¸ºåºåˆ—æ¨è(Multi-behavior sequential recommendation, MBSR)ä¸­è¡Œä¸ºæ•°æ®å™ªå£°å¹²æ‰°å…´è¶£å»ºæ¨¡çš„æŒ‘æˆ˜ï¼Œä»é¢‘ç‡è§†è§’(Frequency Perspective)å¯¹ç”¨æˆ·å…´è¶£è¿›è¡Œäº†é‡æ–°å®¡è§†ã€‚ä½œè€…æŒ‡å‡ºï¼Œè™½ç„¶ç°æœ‰ç ”ç©¶é€šå¸¸è®¤ä¸ºä½é¢‘ä¿¡æ¯ä»£è¡¨å¯é åå¥½è€Œé«˜é¢‘ä¿¡æ¯å¤šä¸ºå™ªå£°ï¼Œä½†æœ¬ç ”ç©¶å‘ç°ä½é¢‘ä¿¡æ¯å¯¹åº”ç”¨æˆ·å…´è¶£çš„çº¯åº¦(Purity)ï¼Œè€Œé«˜é¢‘ä¿¡æ¯åˆ™å¯¹åº”å…´è¶£çš„å¤šæ ·æ€§(Diversity)ã€‚åŸºäºæ­¤å‘ç°ï¼Œç ”ç©¶æå‡ºäº†PDB4Recæ¨¡å‹ï¼Œé€šè¿‡é«˜æ•ˆæå–ä¸åŒé¢‘å¸¦ä¿¡æ¯åŠå…¶å…³è”ï¼Œå¹¶å¼•å…¥è‡ªåŠ©å¹³è¡¡å™¨(Bootstrapping Balancer)æœºåˆ¶æ¥åè°ƒå„é¢‘æ®µå¯¹æ¨èç»“æœçš„è´¡çŒ®ã€‚åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æå‡æ¨èæ€§èƒ½æ–¹é¢å…·æœ‰æ˜¾è‘—çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚è¿™ä¸€æˆæœä¸ºä»é¢‘åŸŸè§’åº¦ä¼˜åŒ–æ¨èç³»ç»Ÿä¸­çš„å…´è¶£çº¯åº¦ä¸å¤šæ ·æ€§æä¾›äº†æ–°çš„ç†è®ºæ”¯æ’‘ä¸å®è·µæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Some experiments in the paper have not been sufficiently validated, leading to conclusions that lack robustness. Additionally, there has been significant progress in follow-up work that requires revisions to the manuscript",
      "pdf_url": "https://arxiv.org/pdf/2508.20427v2",
      "published_date": "2025-08-28 04:55:02 UTC",
      "updated_date": "2025-10-16 11:58:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:58:07.784311+00:00"
    },
    {
      "arxiv_id": "2508.20416v1",
      "title": "DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding",
      "title_zh": "DentalBenchï¼šè¯„ä¼°ä¸æå‡å¤§è¯­è¨€æ¨¡å‹åŒè¯­å£è…”åŒ»å­¦ç†è§£èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Hengchuan Zhu",
        "Yihuan Xu",
        "Yichen Li",
        "Zijie Meng",
        "Zuozhu Liu"
      ],
      "abstract": "Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs) have demonstrated strong performance on general medical benchmarks. However, their capabilities in specialized medical fields, such as dentistry which require deeper domain-specific knowledge, remain underexplored due to the lack of targeted evaluation resources. In this paper, we introduce DentalBench, the first comprehensive bilingual benchmark designed to evaluate and advance LLMs in the dental domain. DentalBench consists of two main components: DentalQA, an English-Chinese question-answering (QA) benchmark with 36,597 questions spanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale, high-quality corpus with 337.35 million tokens curated for dental domain adaptation, supporting both supervised fine-tuning (SFT) and retrieval-augmented generation (RAG). We evaluate 14 LLMs, covering proprietary, open-source, and medical-specific models, and reveal significant performance gaps across task types and languages. Further experiments with Qwen-2.5-3B demonstrate that domain adaptation substantially improves model performance, particularly on knowledge-intensive and terminology-focused tasks, and highlight the importance of domain-specific benchmarks for developing trustworthy and effective LLMs tailored to healthcare applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DentalBenchï¼Œè¿™æ˜¯é¦–ä¸ªæ—¨åœ¨è¯„ä¼°å’Œæå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å£è…”åŒ»å­¦é¢†åŸŸèƒ½åŠ›çš„ç»¼åˆæ€§ä¸­è‹±åŒè¯­åŸºå‡†ã€‚è¯¥åŸºå‡†ç”±ä¸¤ä¸ªæ ¸å¿ƒéƒ¨åˆ†ç»„æˆï¼šåŒ…å« 36,597 ä¸ªé—®é¢˜çš„é—®ç­”åŸºå‡† DentalQAï¼Œä»¥åŠæ‹¥æœ‰ 3.3735 äº¿ä¸ª token çš„é«˜è´¨é‡è¯­æ–™åº“ DentalCorpusï¼Œå¯æœ‰æ•ˆæ”¯æŒç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹åŒ…å«å•†ä¸šã€å¼€æºåŠåŒ»ç–—ä¸“ç”¨åœ¨å†…çš„ 14 ç§ LLMs è¿›è¡Œäº†æ·±åº¦è¯„ä¼°ï¼Œæ­ç¤ºäº†ä¸åŒä»»åŠ¡ç±»å‹å’Œè¯­è¨€ä¹‹é—´å­˜åœ¨çš„æ˜¾è‘—æ€§èƒ½å·®è·ã€‚é€šè¿‡å¯¹ Qwen-2.5-3B çš„å®éªŒè¯æ˜ï¼Œé¢†åŸŸè‡ªé€‚åº”ï¼ˆDomain Adaptationï¼‰èƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹åœ¨çŸ¥è¯†å¯†é›†å‹å’Œæœ¯è¯­èšç„¦å‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å¼€å‘é¢†åŸŸç‰¹å®šåŸºå‡†å¯¹äºæ„å»ºé€‚ç”¨äºåŒ»ç–—åœºæ™¯ã€å¯ä¿¡ä¸”é«˜æ•ˆçš„ LLMs çš„é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20416v1",
      "published_date": "2025-08-28 04:35:51 UTC",
      "updated_date": "2025-08-28 04:35:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:58:11.604788+00:00"
    },
    {
      "arxiv_id": "2508.20413v1",
      "title": "Assessing local deformation and computing scalar curvature with nonlinear conformal regularization of decoders",
      "title_zh": "åŸºäºè§£ç å™¨éçº¿æ€§å…±å½¢æ­£åˆ™åŒ–çš„å±€éƒ¨å½¢å˜è¯„ä¼°ä¸æ ‡é‡æ›²ç‡è®¡ç®—",
      "authors": [
        "Benjamin CouÃ©raud",
        "Vikram Sunkara",
        "Christof SchÃ¼tte"
      ],
      "abstract": "One aim of dimensionality reduction is to discover the main factors that explain the data, and as such is paramount to many applications. When working with high dimensional data, autoencoders offer a simple yet effective approach to learn low-dimensional representations. The two components of a general autoencoder consist first of an encoder that maps the observed data onto a latent space; and second a decoder that maps the latent space back to the original observation space, which allows to learn a low-dimensional manifold representation of the original data. In this article, we introduce a new type of geometric regularization for decoding maps approximated by deep neural networks, namely nonlinear conformal regularization. This regularization procedure permits local variations of the decoder map and comes with a new scalar field called conformal factor which acts as a quantitative indicator of the amount of local deformation sustained by the latent space when mapped into the original data space. We also show that this regularization technique allows the computation of the scalar curvature of the learned manifold. Implementation and experiments on the Swiss roll and CelebA datasets are performed to illustrate how to obtain these quantities from the architecture.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜ç»´æ•°æ®é™ç»´ä¸­ Autoencoders å­¦ä¹ ä½ç»´æµå½¢è¡¨ç¤ºçš„åº”ç”¨ï¼Œæ¢è®¨äº†å¦‚ä½•è¯„ä¼°æ½œåœ¨ç©ºé—´æ˜ å°„å›åŸå§‹ç©ºé—´æ—¶çš„å‡ ä½•å±æ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åä¸ºéçº¿æ€§å…±å½¢æ­£åˆ™åŒ– (Nonlinear Conformal Regularization) çš„æ–°å‹å‡ ä½•æ­£åˆ™åŒ–æ–¹æ³•ï¼Œä¸“é—¨ç”¨äºä¼˜åŒ–æ·±åº¦ç¥ç»ç½‘ç»œä¸­çš„è§£ç å™¨æ˜ å°„ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªå…±å½¢å› å­ (Conformal Factor) æ ‡é‡åœºï¼Œä½œä¸ºè¡¡é‡æ½œåœ¨ç©ºé—´åœ¨æ˜ å°„è¿‡ç¨‹ä¸­æ‰€å—å±€éƒ¨å˜å½¢é‡çš„å®šé‡æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯æ˜è¯¥æ­£åˆ™åŒ–æŠ€æœ¯è¿˜æ”¯æŒè®¡ç®—æ‰€å­¦ä¹ æµå½¢çš„æ ‡é‡æ›²ç‡ (Scalar Curvature)ã€‚åœ¨ Swiss roll å’Œ CelebA æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆä»ç½‘ç»œæ¶æ„ä¸­æå–è¿™äº›å‡ ä½•é‡ï¼Œä¸ºæ·±å…¥åˆ†æç”Ÿæˆæ¨¡å‹æ‰€å­¦ä¹ åˆ°çš„æµå½¢ç‰¹å¾æä¾›äº†æœ‰æ•ˆçš„å®šé‡å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.20413v1",
      "published_date": "2025-08-28 04:30:49 UTC",
      "updated_date": "2025-08-28 04:30:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:58:11.885901+00:00"
    },
    {
      "arxiv_id": "2508.20411v1",
      "title": "Governable AI: Provable Safety Under Extreme Threat Models",
      "title_zh": "å¯æ²»ç†äººå·¥æ™ºèƒ½ï¼šæç«¯å¨èƒæ¨¡å‹ä¸‹çš„å¯è¯æ˜å®‰å…¨",
      "authors": [
        "Donglin Wang",
        "Weiyun Liang",
        "Chunyuan Chen",
        "Jing Xu",
        "Yulong Fu"
      ],
      "abstract": "As AI rapidly advances, the security risks posed by AI are becoming increasingly severe, especially in critical scenarios, including those posing existential risks. If AI becomes uncontrollable, manipulated, or actively evades safety mechanisms, it could trigger systemic disasters. Existing AI safety approaches-such as model enhancement, value alignment, and human intervention-suffer from fundamental, in-principle limitations when facing AI with extreme motivations and unlimited intelligence, and cannot guarantee security. To address this challenge, we propose a Governable AI (GAI) framework that shifts from traditional internal constraints to externally enforced structural compliance based on cryptographic mechanisms that are computationally infeasible to break, even for future AI, under the defined threat model and well-established cryptographic assumptions.The GAI framework is composed of a simple yet reliable, fully deterministic, powerful, flexible, and general-purpose rule enforcement module (REM); governance rules; and a governable secure super-platform (GSSP) that offers end-to-end protection against compromise or subversion by AI. The decoupling of the governance rules and the technical platform further enables a feasible and generalizable technical pathway for the safety governance of AI. REM enforces the bottom line defined by governance rules, while GSSP ensures non-bypassability, tamper-resistance, and unforgeability to eliminate all identified attack vectors. This paper also presents a rigorous formal proof of the security properties of this mechanism and demonstrates its effectiveness through a prototype implementation evaluated in representative high-stakes scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰AIå®‰å…¨æ–¹æ³•ï¼ˆå¦‚ä»·å€¼å¯¹é½ value alignment å’Œäººå·¥å¹²é¢„ï¼‰åœ¨é¢å¯¹æç«¯å¨èƒæ¨¡å‹æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº†å¯æ²»ç†äººå·¥æ™ºèƒ½ (Governable AI, GAI) æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†ä¼ ç»Ÿçš„å†…éƒ¨çº¦æŸè½¬å‘åŸºäºå¯†ç å­¦æœºåˆ¶çš„å¤–éƒ¨å¼ºåˆ¶ç»“æ„åˆè§„æ€§ï¼Œç¡®ä¿åœ¨å®šä¹‰çš„å¨èƒæ¨¡å‹ä¸‹å…·å¤‡è®¡ç®—ä¸å¯ç ´çš„å®‰å…¨ä¿éšœã€‚GAI æ¡†æ¶ç”±è§„åˆ™æ‰§è¡Œæ¨¡å— (REM)ã€æ²»ç†è§„åˆ™å’Œå¯æ²»ç†å®‰å…¨è¶…çº§å¹³å° (GSSP) ç»„æˆï¼Œå®ç°äº†æ²»ç†è§„åˆ™ä¸æŠ€æœ¯å¹³å°çš„æœ‰æ•ˆè§£è€¦ã€‚REM è´Ÿè´£å¼ºåˆ¶æ‰§è¡Œæ²»ç†è§„åˆ™å®šä¹‰çš„åº•çº¿ï¼Œè€Œ GSSP ç¡®ä¿ç³»ç»Ÿçš„ä¸å¯ç»•è¿‡æ€§ã€æŠ—ç¯¡æ”¹æ€§å’Œä¸å¯ä¼ªé€ æ€§ï¼Œä»è€Œæ¶ˆé™¤æ‰€æœ‰è¯†åˆ«å‡ºçš„æ”»å‡»å‘é‡ã€‚æœ¬æ–‡æä¾›äº†è¯¥æœºåˆ¶å®‰å…¨å±æ€§çš„ä¸¥æ ¼å½¢å¼åŒ–è¯æ˜ (formal proof)ï¼Œå¹¶é€šè¿‡åŸå‹å®ç°åœ¨é«˜é£é™©åœºæ™¯ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ¡ˆä¸ºåº”å¯¹æœªæ¥å…·æœ‰æ— é™æ™ºèƒ½çš„ AI æä¾›äº†ä¸€ç§æŠ€æœ¯ä¸Šå¯è¡Œä¸”å¯è¯æ˜å®‰å…¨çš„æ²»ç†è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20411v1",
      "published_date": "2025-08-28 04:22:59 UTC",
      "updated_date": "2025-08-28 04:22:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:58:24.498168+00:00"
    },
    {
      "arxiv_id": "2508.21099v3",
      "title": "Beyond the Safety Tax: Mitigating Unsafe Text-to-Image Generation via External Safety Rectification",
      "title_zh": "è¶…è¶Šå®‰å…¨ç¨ï¼šé€šè¿‡å¤–éƒ¨å®‰å…¨æ ¡æ­£ç¼“è§£ä¸å®‰å…¨çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ",
      "authors": [
        "Xiangtao Meng",
        "Yingkai Dong",
        "Ning Yu",
        "Li Wang",
        "Zheng Li",
        "Shanqing Guo"
      ],
      "abstract": "Text-to-image (T2I) generative models have achieved remarkable visual fidelity, yet remain vulnerable to generating unsafe content. Existing safety defenses typically intervene internally within the generative model, but suffer from severe concept entanglement, leading to degradation of benign generation quality, a trade-off we term the Safety Tax. To overcome this limitation, we advocate a paradigm shift from destructive internal editing to external safety rectification. Following this principle, we propose SafePatch, a structurally isolated safety module that performs external, interpretable rectification without modifying the base model. The core backbone of SafePatch is architecturally instantiated as a trainable clone of the base model's encoder, allowing it to inherit rich semantic priors and maintain representation consistency. To enable interpretable safety rectification, we construct a strictly aligned counterfactual safety dataset (ACS) for differential supervision training. Across nudity and multi-category benchmarks and recent adversarial prompt attacks, SafePatch achieves robust unsafe suppression (7% unsafe on I2P) while preserving image quality and semantic alignment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ–‡æœ¬åˆ°å›¾åƒ(Text-to-image)ç”Ÿæˆæ¨¡å‹ä¸­å­˜åœ¨çš„å®‰å…¨æ¼æ´ï¼ŒæŒ‡å‡ºç›®å‰çš„å†…éƒ¨é˜²å¾¡æœºåˆ¶å› æ¦‚å¿µçº ç¼ å¯¼è‡´è‰¯æ€§ç”Ÿæˆè´¨é‡ä¸‹é™ï¼Œå³â€œå®‰å…¨ç¨â€(Safety Tax)é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºä»ç ´åæ€§çš„å†…éƒ¨ç¼–è¾‘è½¬å‘å¤–éƒ¨å®‰å…¨æ•´æµ(External safety rectification)çš„æ–°èŒƒå¼ï¼Œå¹¶å¼€å‘äº†SafePatchã€‚SafePatchæ˜¯ä¸€ä¸ªç»“æ„éš”ç¦»çš„å®‰å…¨æ¨¡å—ï¼Œä½œä¸ºåŸºç¡€æ¨¡å‹ç¼–ç å™¨çš„å¯è®­ç»ƒå…‹éš†ï¼Œå®ƒæ— éœ€ä¿®æ”¹åŸæ¨¡å‹å³å¯æ‰§è¡Œå¤–éƒ¨ä¸”å¯è§£é‡Šçš„æ•´æµã€‚é€šè¿‡åˆ©ç”¨æ–°æ„å»ºçš„åäº‹å®å®‰å…¨æ•°æ®é›†(ACS)è¿›è¡Œå·®åˆ†ç›‘ç£è®­ç»ƒï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿç»§æ‰¿è¯­ä¹‰å…ˆéªŒå¹¶ä¿æŒè¡¨ç¤ºä¸€è‡´æ€§ã€‚åœ¨è£¸éœ²åŠå¤šç±»åˆ«åŸºå‡†æµ‹è¯•å’Œå¯¹æŠ—æ€§æç¤ºæ”»å‡»ä¸‹çš„å®éªŒè¡¨æ˜ï¼ŒSafePatchåœ¨ä¿æŒå“è¶Šå›¾åƒè´¨é‡å’Œè¯­ä¹‰å¯¹é½çš„åŒæ—¶ï¼Œå®ç°äº†å¼ºåŠ›ä¸”é²æ£’çš„ä¸å®‰å…¨å†…å®¹æŠ‘åˆ¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.21099v3",
      "published_date": "2025-08-28 04:09:52 UTC",
      "updated_date": "2026-01-19 07:34:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:58:18.284381+00:00"
    },
    {
      "arxiv_id": "2508.20404v2",
      "title": "AWorld: Orchestrating the Training Recipe for Agentic AI",
      "title_zh": "AWorldï¼šç¼–æ’æ™ºèƒ½ä½“ AI çš„è®­ç»ƒæ–¹æ¡ˆ",
      "authors": [
        "Chengyue Yu",
        "Siyuan Lu",
        "Chenyi Zhuang",
        "Dong Wang",
        "Qintong Wu",
        "Zongyue Li",
        "Runsheng Gan",
        "Chunfeng Wang",
        "Siqi Hou",
        "Gaochi Huang",
        "Wenlong Yan",
        "Lifeng Hong",
        "Aohui Xue",
        "Yanfeng Wang",
        "Jinjie Gu",
        "David Tsai",
        "Tao Lin"
      ],
      "abstract": "The learning from practice paradigm is crucial for developing capable Agentic AI systems, yet it is severely hampered by inefficient experience generation, a bottleneck especially pronounced in complex benchmarks like GAIA. To address this, we introduce AWorld, an open-source system engineered for large-scale agent-environment interaction. By distributing tasks across a cluster, AWorld accelerates experience collection by 14.6x compared to standard single-node, sequential execution. This critical speedup makes extensive reinforcement learning practical and scalable. Leveraging this capability, we trained a Qwen3-32B-based agent that achieves pass@1 accuracy of 32.23% on the GAIA test set, which surpasses GPT-4o (27.91%) and rivals DeepSeek-V3 (31.89%). Our open-source system and the resulting agent provide a practical blueprint for a complete agentic AI training pipeline, from efficient interaction to demonstrable model improvement.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™ºèƒ½ä½“ AI (Agentic AI) åœ¨å¤æ‚åŸºå‡†æµ‹è¯•ï¼ˆå¦‚ GAIAï¼‰ä¸­å› ç»éªŒç”Ÿæˆæ•ˆç‡ä½ä¸‹è€Œå¯¼è‡´çš„è®­ç»ƒç“¶é¢ˆï¼Œæå‡ºäº†å¼€æºç³»ç»Ÿ AWorldã€‚AWorld ä¸“ä¸ºå¤§è§„æ¨¡æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’ (agent-environment interaction) è®¾è®¡ï¼Œé€šè¿‡åœ¨é›†ç¾¤ä¸­åˆ†å‘ä»»åŠ¡ï¼Œå°†ç»éªŒé‡‡é›†é€Ÿåº¦ç›¸æ¯”æ ‡å‡†çš„å•èŠ‚ç‚¹é¡ºåºæ‰§è¡Œæå‡äº† 14.6 å€ã€‚è¿™ç§æ˜¾è‘—çš„åŠ é€Ÿèƒ½åŠ›ä½¿å¾—å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) çš„åº”ç”¨å˜å¾—æ›´åŠ åŠ¡å®ä¸”å…·å¤‡å¯æ‰©å±•æ€§ã€‚åŸºäºè¯¥ç³»ç»Ÿè®­ç»ƒå‡ºçš„ Qwen3-32B æ™ºèƒ½ä½“åœ¨ GAIA æµ‹è¯•é›†ä¸Šå®ç°äº† 32.23% çš„ pass@1 å‡†ç¡®ç‡ï¼Œä¸ä»…è¶…è¶Šäº† GPT-4o (27.91%)ï¼Œè¿˜ä¸ DeepSeek-V3 (31.89%) ç›¸å½“ã€‚è¯¥å¼€æºç³»ç»ŸåŠç›¸å…³æˆæœä¸ºæ„å»ºä»é«˜æ•ˆäº¤äº’åˆ°æ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡çš„å®Œæ•´æ™ºèƒ½ä½“ AI è®­ç»ƒæµæ°´çº¿æä¾›äº†å®è·µè“å›¾ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20404v2",
      "published_date": "2025-08-28 04:04:30 UTC",
      "updated_date": "2025-09-01 03:56:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:58:30.090187+00:00"
    },
    {
      "arxiv_id": "2508.20400v1",
      "title": "MPFormer: Adaptive Framework for Industrial Multi-Task Personalized Sequential Retriever",
      "title_zh": "MPFormerï¼šé¢å‘å·¥ä¸šçº§å¤šä»»åŠ¡ä¸ªæ€§åŒ–åºåˆ—æ£€ç´¢çš„è‡ªé€‚åº”æ¡†æ¶",
      "authors": [
        "Yijia Sun",
        "Shanshan Huang",
        "Linxiao Che",
        "Haitao Lu",
        "Qiang Luo",
        "Kun Gai",
        "Guorui Zhou"
      ],
      "abstract": "Modern industrial recommendation systems encounter a core challenge of multi-stage optimization misalignment: a significant semantic gap exists between the multi-objective optimization paradigm widely used in the ranking phase and the single-objective modeling in the retrieve phase. Although the mainstream industry solution achieves multi-objective coverage through parallel multi-path single-objective retrieval, this approach leads to linear growth of training and serving resources with the number of objectives and has inherent limitations in handling loosely coupled objectives. This paper proposes the MPFormer, a dynamic multi-task Transformer framework, which systematically addresses the aforementioned issues through three innovative mechanisms. First, an objective-conditioned transformer that jointly encodes user behavior sequences and multi-task semantics through learnable attention modulation; second, personalized target weights are introduced to achieve dynamic adjustment of retrieval results; finally, user personalization information is incorporated into token representations and the Transformer structure to further enhance the model's representation ability. This framework has been successfully integrated into Kuaishou short video recommendation system, stably serving over 400 million daily active users. It significantly improves user daily engagement and system operational efficiency. Practical deployment verification shows that, compared with traditional solutions, it effectively optimizes the iterative paradigm of multi-objective retrieval while maintaining service response speed, providing a scalable multi-objective solution for industrial recommendation systems.",
      "tldr_zh": "äº¿æ—¥æ´»è·ƒç”¨æˆ·ï¼Œæ˜¾è‘—æå‡äº†ç”¨æˆ·æ—¥å‚ä¸åº¦å’Œç³»ç»Ÿè¿è¡Œæ•ˆç‡ã€‚å®é™…åº”ç”¨è¯æ˜ï¼ŒMPFormeråœ¨ä¿æŒæœåŠ¡å“åº”é€Ÿåº¦çš„åŒæ—¶ï¼Œä¸ºå·¥ä¸šæ¨èç³»ç»Ÿæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„å¤šç›®æ ‡æ£€ç´¢è§£å†³æ–¹æ¡ˆã€‚\n\n6 sentences. Fits perfectly.è¯¥ç ”ç©¶æå‡ºäº†MPFormerï¼Œä¸€ç§æ—¨åœ¨è§£å†³å·¥ä¸šçº§æ¨èç³»ç»Ÿä¸­å¤šé˜¶æ®µä¼˜åŒ–å¤±è°ƒé—®é¢˜çš„åŠ¨æ€å¤šä»»åŠ¡Transformeræ¡†æ¶ï¼Œé‡ç‚¹ç¼©å°äº†å¤šç›®æ ‡æ’åºé˜¶æ®µä¸å•ç›®æ ‡å¬å›é˜¶æ®µï¼ˆRetrieve Phaseï¼‰ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿã€‚é’ˆå¯¹ä¼ ç»Ÿå¹¶è¡Œå¤šè·¯å¾„å•ç›®æ ‡å¬å›æ–¹æ¡ˆå¯¼è‡´çš„èµ„æºæ¶ˆè€—çº¿æ€§å¢é•¿åŠå¤„ç†æ¾è€¦åˆç›®æ ‡å—é™ç­‰æŒ‘æˆ˜ï¼ŒMPFormeré€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒåˆ›æ–°æœºåˆ¶å®ç°çªç ´ï¼šä¸€æ˜¯åˆ©ç”¨å¯å­¦ä¹ æ³¨æ„åŠ›è°ƒèŠ‚è”åˆç¼–ç ç”¨æˆ·è¡Œä¸ºåºåˆ—ä¸å¤šä»»åŠ¡è¯­ä¹‰çš„ç›®æ ‡æ¡ä»¶Transformerï¼ˆObjective-Conditioned Transformerï¼‰ï¼›äºŒæ˜¯å¼•å…¥ä¸ªæ€§åŒ–ç›®æ ‡æƒé‡ä»¥å®ç°å¬å›ç»“æœçš„åŠ¨æ€è°ƒæ•´ï¼›ä¸‰æ˜¯å°†ç”¨æˆ·ä¸ªæ€§åŒ–ä¿¡æ¯èå…¥Tokenè¡¨ç¤ºä¸­ä»¥å¢å¼ºæ¨¡å‹çš„è¡¨å¾èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å·²æˆåŠŸéƒ¨ç½²äºå¿«æ‰‹çŸ­è§†é¢‘æ¨èç³»ç»Ÿï¼Œç¨³å®šæœåŠ¡è¶…è¿‡4äº¿æ—¥æ´»è·ƒç”¨æˆ·ï¼Œæ˜¾è‘—æå‡äº†ç”¨æˆ·æ—¥å‚ä¸åº¦å’Œç³»ç»Ÿè¿è¡Œæ•ˆç‡ã€‚å®é™…åº”ç”¨è¯æ˜ï¼ŒMPFormeråœ¨ä¿æŒæœåŠ¡å“åº”é€Ÿåº¦çš„åŒæ—¶ï¼Œä¸ºå·¥ä¸šæ¨èç³»ç»Ÿæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„å¤šç›®æ ‡æ£€ç´¢è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "CIKM 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.20400v1",
      "published_date": "2025-08-28 03:53:55 UTC",
      "updated_date": "2025-08-28 03:53:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:58:32.285423+00:00"
    },
    {
      "arxiv_id": "2508.20398v1",
      "title": "TF-TransUNet1D: Time-Frequency Guided Transformer U-Net for Robust ECG Denoising in Digital Twin",
      "title_zh": "TF-TransUNet1Dï¼šé¢å‘æ•°å­—å­ªç”Ÿé²æ£’å¿ƒç”µå»å™ªçš„æ—¶é¢‘å¼•å¯¼ Transformer U-Net",
      "authors": [
        "Shijie Wang",
        "Lei Li"
      ],
      "abstract": "Electrocardiogram (ECG) signals serve as a foundational data source for cardiac digital twins, yet their diagnostic utility is frequently compromised by noise and artifacts. To address this issue, we propose TF-TransUNet1D, a novel one-dimensional deep neural network that integrates a U-Net-based encoder-decoder architecture with a Transformer encoder, guided by a hybrid time-frequency domain loss. The model is designed to simultaneously capture local morphological features and long-range temporal dependencies, which are critical for preserving the diagnostic integrity of ECG signals. To enhance denoising robustness, we introduce a dual-domain loss function that jointly optimizes waveform reconstruction in the time domain and spectral fidelity in the frequency domain. In particular, the frequency-domain component effectively suppresses high-frequency noise while maintaining the spectral structure of the signal, enabling recovery of subtle but clinically significant waveform components. We evaluate TF-TransUNet1D using synthetically corrupted signals from the MIT-BIH Arrhythmia Database and the Noise Stress Test Database (NSTDB). Comparative experiments against state-of-the-art baselines demonstrate consistent superiority of our model in terms of SNR improvement and error metrics, achieving a mean absolute error of 0.1285 and Pearson correlation coefficient of 0.9540. By delivering high-precision denoising, this work bridges a critical gap in pre-processing pipelines for cardiac digital twins, enabling more reliable real-time monitoring and personalized modeling.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TF-TransUNet1Dï¼Œä¸€ç§æ—¨åœ¨ä¸ºå¿ƒè„æ•°å­—å­ªç”Ÿ (Digital Twin) æä¾›ç¨³å¥å¿ƒç”µå›¾ (ECG) å»å™ªçš„ä¸€ç»´æ·±åº¦ç¥ç»ç½‘ç»œã€‚è¯¥æ¨¡å‹å°†åŸºäº U-Net çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ä¸ Transformer ç¼–ç å™¨ç›¸ç»“åˆï¼Œå¹¶ç”±æ··åˆæ—¶é¢‘åŸŸæŸå¤± (hybrid time-frequency domain loss) è¿›è¡Œå¼•å¯¼ï¼Œä»è€ŒåŒæ—¶æ•æ‰å±€éƒ¨å½¢æ€ç‰¹å¾å’Œé•¿ç¨‹æ—¶é—´ä¾èµ–å…³ç³»ã€‚é€šè¿‡å¼•å…¥åŒåŸŸæŸå¤±å‡½æ•° (dual-domain loss function)ï¼Œè¯¥æ–¹æ³•å®ç°äº†æ—¶åŸŸæ³¢å½¢é‡å»ºä¸é¢‘åŸŸå…‰è°±ä¿çœŸåº¦çš„è”åˆä¼˜åŒ–ï¼Œåœ¨æœ‰æ•ˆæŠ‘åˆ¶é«˜é¢‘å™ªå£°çš„åŒæ—¶ä¿ç•™äº†å…·æœ‰ä¸´åºŠæ„ä¹‰çš„ç»†å¾®æ³¢å½¢ç»„ä»¶ã€‚åœ¨ MIT-BIH å’Œ NSTDB æ•°æ®åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTF-TransUNet1D åœ¨ä¿¡å™ªæ¯”æå‡ç­‰æŒ‡æ ‡ä¸ŠæŒç»­ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ï¼Œè¾¾åˆ°äº† 0.1285 çš„å¹³å‡ç»å¯¹è¯¯å·® (MAE) å’Œ 0.9540 çš„çš®å°”é€Šç›¸å…³ç³»æ•° (Pearson correlation coefficient)ã€‚è¯¥å·¥ä½œå¡«è¡¥äº†å¿ƒè„æ•°å­—å­ªç”Ÿé¢„å¤„ç†ç¯èŠ‚çš„å…³é”®ç©ºç™½ï¼Œä¸ºå®ç°æ›´å¯é çš„å®æ—¶ç›‘æµ‹å’Œä¸ªæ€§åŒ–å»ºæ¨¡æä¾›äº†é«˜ç²¾åº¦çš„å»å™ªæ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 3 figures International Workshop on Digital Twin for Healthcare (DT4H) in MICCAI 2025 (Daejeon, Republic of Korea)",
      "pdf_url": "https://arxiv.org/pdf/2508.20398v1",
      "published_date": "2025-08-28 03:51:19 UTC",
      "updated_date": "2025-08-28 03:51:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:59:00.290388+00:00"
    },
    {
      "arxiv_id": "2508.20395v1",
      "title": "Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction",
      "title_zh": "é€šè¿‡æ¡ä»¶ç†µå‡è¡¡é‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç”¨",
      "authors": [
        "Xu Guo"
      ],
      "abstract": "Recent advancements in large language models (LLMs) often rely on generating intermediate reasoning steps to enhance accuracy. However, little work has examined how reasoning utility contributes to the final answer's correctness. Due to the stochastic nature of autoregressive generation, generating more context does not guarantee increased confidence in the answer. If we could predict, during generation, whether a reasoning step will be useful, we could stop early or prune ineffective steps, avoiding distractions in the final decision.\n  We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to generate reasoning chains, and then employing a separate model (Qwen3-8B) to quantify the utility of these chains for final accuracy. Specifically, we measure the model's uncertainty on the answer span Y at each reasoning step using conditional entropy (expected negative log-likelihood over the vocabulary) with context expanding step by step. Our results show a clear pattern: conditional entropy that decreases over steps is strongly associated with correct answers, whereas flat or increasing entropy often results in wrong answers. We also corroborate that incorrect reasoning paths tend to be longer than correct ones, suggesting that longer reasoning does not necessarily yield better outcomes. These findings serve as a foundation to inspire future work on designing efficient reasoning pipelines that detect and avoid unproductive reasoning early.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­æ¨ç†æ­¥éª¤å¯¹æœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§çš„å®é™…æ•ˆç”¨ã€‚ä½œè€…åœ¨MATHæ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸€é¡¹å…ˆçŸ¥ç ”ç©¶ï¼ˆoracle studyï¼‰ï¼Œåˆ©ç”¨Qwen2.5-32Bå’ŒGPT-4oç”Ÿæˆæ¨ç†é“¾ï¼Œå¹¶é‡‡ç”¨Qwen3-8Bé€šè¿‡æ¡ä»¶ç†µï¼ˆconditional entropyï¼‰çš„å˜åŒ–æ¥é‡åŒ–æ¨ç†æ­¥éª¤çš„æ•ˆç”¨ã€‚ç ”ç©¶å‘ç°ï¼Œéšç€æ¨ç†æ­¥éª¤æ¨è¿›ï¼Œæ¡ä»¶ç†µçš„æŒç»­ä¸‹é™ä¸ç­”æ¡ˆçš„æ­£ç¡®æ€§å¼ºç›¸å…³ï¼Œè€Œç†µå€¼å¹³ç¨³æˆ–ä¸Šå‡åˆ™å¾€å¾€é¢„ç¤ºç€é”™è¯¯çš„é¢„æµ‹ã€‚æ­¤å¤–ï¼Œå®éªŒè¯å®é”™è¯¯çš„æ¨ç†è·¯å¾„é€šå¸¸æ¯”æ­£ç¡®çš„è·¯å¾„æ›´é•¿ï¼Œè¡¨æ˜æ›´å†—é•¿çš„æ¨ç†è¿‡ç¨‹å¹¶ä¸ä¸€å®šèƒ½æå‡æ¨¡å‹æ€§èƒ½ã€‚è¿™äº›å‘ç°ä¸ºå¼€å‘èƒ½å¤Ÿæ—©æœŸè¯†åˆ«å¹¶ä¿®å‰ªæ— æ•ˆæ¨ç†æ­¥éª¤çš„é«˜æ•ˆæ¨ç†ç®¡é“ï¼ˆreasoning pipelinesï¼‰æä¾›äº†ç†è®ºåŸºç¡€å’Œå¯å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.20395v1",
      "published_date": "2025-08-28 03:43:38 UTC",
      "updated_date": "2025-08-28 03:43:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:59:28.845843+00:00"
    },
    {
      "arxiv_id": "2508.20392v2",
      "title": "Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection",
      "title_zh": "åŸºäºæ—¶é—´ä¾èµ–æ•´åˆ-å‘æ”¾ç¥ç»å…ƒæ¨¡å‹çš„è¶…ä½å»¶è¿Ÿè„‰å†²ç¥ç»ç½‘ç»œç›®æ ‡æ£€æµ‹",
      "authors": [
        "Chengjun Zhang",
        "Yuhao Zhang",
        "Jie Yang",
        "Mohamad Sawan"
      ],
      "abstract": "Spiking Neural Networks (SNNs), inspired by the brain, are characterized by minimal power consumption and swift inference capabilities on neuromorphic hardware, and have been widely applied to various visual perception tasks. Current ANN-SNN conversion methods have achieved excellent results in classification tasks with ultra-low time-steps, but their performance in visual detection tasks remains suboptimal. In this paper, we propose a delay-spike approach to mitigate the issue of residual membrane potential caused by heterogeneous spiking patterns. Furthermore, we propose a novel temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This enables Integrate-and-fire (IF) neurons to dynamically adjust their accumulation and firing behaviors based on the temporal order of time-steps. Our method enables spikes to exhibit distinct temporal properties, rather than relying solely on frequency-based representations. Moreover, the tdIF neuron maintains energy consumption on par with traditional IF neuron. We demonstrate that our method achieves more precise feature representation with lower time-steps, enabling high performance and ultra-low latency in visual detection tasks. In this study, we conduct extensive evaluation of the tdIF method across two critical vision tasks: object detection and lane line detection. The results demonstrate that the proposed method surpasses current ANN-SNN conversion approaches, achieving state-of-the-art performance with ultra-low latency (within 5 time-steps).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è„‰å†²ç¥ç»ç½‘ç»œ(SNN)åœ¨è§†è§‰æ£€æµ‹ä»»åŠ¡ä¸­ä½æ—¶é—´æ­¥ä¸‹æ€§èƒ½å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ delay-spike æ–¹æ³•ä»¥å‡è½»å¼‚æ„è„‰å†²æ¨¡å¼å¯¼è‡´çš„æ®‹ä½™è†œç”µä½å½±å“ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ—¶é—´ä¾èµ–ç§¯èš-å‘æ”¾(temporal-dependent Integrate-and-Fire, tdIF)ç¥ç»å…ƒæ¶æ„ï¼Œä½¿ç¥ç»å…ƒèƒ½æ ¹æ®æ—¶é—´æ­¥çš„å…ˆåé¡ºåºåŠ¨æ€è°ƒæ•´å…¶ç§¯èšä¸å‘æ”¾è¡Œä¸ºã€‚è¯¥æ–¹æ³•èµ‹äºˆäº†è„‰å†²ç‹¬ç‰¹çš„æ—¶é—´å±æ€§ï¼Œå…‹æœäº†ä¼ ç»Ÿ SNN ä»…ä¾èµ–é¢‘ç‡è¡¨å¾çš„å±€é™ï¼ŒåŒæ—¶ä¿æŒäº†ä¸æ ‡å‡† IF ç¥ç»å…ƒç›¸å½“çš„ä½èƒ½è€—ç‰¹æ€§ã€‚é€šè¿‡åœ¨ç›®æ ‡æ£€æµ‹å’Œè½¦é“çº¿æ£€æµ‹ä¸¤é¡¹å…³é”®è§†è§‰ä»»åŠ¡ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼Œå®éªŒè¯æ˜ tdIF æ–¹æ³•èƒ½å¤Ÿåœ¨æä½å»¶è¿Ÿï¼ˆ5ä¸ªæ—¶é—´æ­¥ä»¥å†…ï¼‰ä¸‹å®ç°ç²¾ç¡®çš„ç‰¹å¾è¡¨ç¤ºã€‚æœ€ç»ˆç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„ ANN-SNN è½¬æ¢æ–¹æ¡ˆï¼Œå¹¶åœ¨è¶…ä½å»¶è¿Ÿæ¡ä»¶ä¸‹è¾¾åˆ°äº† state-of-the-art æ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.20392v2",
      "published_date": "2025-08-28 03:32:45 UTC",
      "updated_date": "2025-09-09 07:57:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:59:01.081622+00:00"
    },
    {
      "arxiv_id": "2508.20384v1",
      "title": "Uncertainty Under the Curve: A Sequence-Level Entropy Area Metric for Reasoning LLM",
      "title_zh": "æ›²çº¿ä¸‹çš„ä¸ç¡®å®šæ€§ï¼šé¢å‘æ¨ç†å¤§è¯­è¨€æ¨¡å‹çš„åºåˆ—çº§ç†µé¢ç§¯æŒ‡æ ‡",
      "authors": [
        "Yongfu Zhu",
        "Lin Sun",
        "Guangxiang Zhao",
        "Weihong Lin",
        "Xiangzheng Zhang"
      ],
      "abstract": "In this work, we introduce Entropy Area Score (EAS), a simple yet effective metric to quantify uncertainty in the answer generation process of reasoning large language models (LLMs). EAS requires neither external models nor repeated sampling, it integrates token-level predictive entropy from the model itself to capture the evolution of uncertainty during generation. Empirical results show that EAS is strongly correlated with answer entropy across models and datasets. In training data selection, EAS identifies high-potential samples and consistently outperforms Pass Rate filtering under equal sample budgets, improving student model accuracy on math benchmarks. EAS is both efficient and interpretable, offering a practical tool for uncertainty modeling and data quality assessment in LLM training.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Entropy Area Score (EAS)ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•ä¸”æœ‰æ•ˆçš„åº¦é‡æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–æ¨ç†å¤§è¯­è¨€æ¨¡å‹(Reasoning LLMs)åœ¨ç­”æ¡ˆç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸ç¡®å®šæ€§ã€‚ä¸éœ€è¦å¤–éƒ¨æ¨¡å‹æˆ–é‡å¤é‡‡æ ·çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒEASç›´æ¥æ•´åˆäº†æ¨¡å‹è‡ªèº«çš„Token-levelé¢„æµ‹ç†µï¼Œä»è€Œç²¾å‡†æ•æ‰ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸ç¡®å®šæ€§çš„åŠ¨æ€æ¼”å˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEASåœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†ä¸Šå‡ä¸ç­”æ¡ˆç†µè¡¨ç°å‡ºæå¼ºçš„ç›¸å…³æ€§ã€‚åœ¨è®­ç»ƒæ•°æ®é€‰æ‹©çš„åº”ç”¨ä¸­ï¼ŒEASèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«é«˜æ½œåŠ›çš„æ ·æœ¬ï¼Œä¸”åœ¨ç›¸åŒæ ·æœ¬é¢„ç®—ä¸‹æŒç»­ä¼˜äºä¼ ç»Ÿçš„Pass Rateè¿‡æ»¤æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†å­¦ç”Ÿæ¨¡å‹åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚æ€»ä½“è€Œè¨€ï¼ŒEASå…¼å…·é«˜æ•ˆæ€§ä¸å¯è§£é‡Šæ€§ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„ä¸ç¡®å®šæ€§å»ºæ¨¡å’Œæ•°æ®è´¨é‡è¯„ä¼°æä¾›äº†ä¸€ä¸ªå®ç”¨çš„å·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Under review for AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2508.20384v1",
      "published_date": "2025-08-28 03:16:15 UTC",
      "updated_date": "2025-08-28 03:16:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:59:02.891459+00:00"
    },
    {
      "arxiv_id": "2508.20374v1",
      "title": "TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning",
      "title_zh": "TCIAï¼šé¢å‘æŒ‡ä»¤å¾®è°ƒçš„ä»¥ä»»åŠ¡ä¸ºä¸­å¿ƒçš„æŒ‡ä»¤å¢å¼ºæ–¹æ³•",
      "authors": [
        "Simin Ma",
        "Shujian Liu",
        "Jun Tan",
        "Yebowen Hu",
        "Song Wang",
        "Sathish Reddy Indurthi",
        "Sanqiang Zhao",
        "Liwei Wu",
        "Jianbing Han",
        "Kaiqiang Song"
      ],
      "abstract": "Diverse instruction data is vital for effective instruction tuning of large language models, as it enables the model to generalize across different types of inputs . Building such diversified instruction dataset is an essential step in this process. Existing approaches often leverage large language models to automatically explore and generate diverse instructions, ensuring both data diversity and quality. However, they tend to overlook an important factor in real-world applications: on-task relevance. In practice, only a few real-world applications require a truly general-purpose model; most benefit from task-specific knowledge tailored to their particular use case. Therefore, it is vital to develop instruction augmentation methods that not only maintain diversity but are also optimized for specific, real-world scenarios.\n  We thus introduce Task Centric Instruction Augmentation (TCIA), a framework that systematically expands instructions while preserving both diversity and task alignment. By representing instructions in a discrete query-constraints space, TCIA creates a rich set of task-relevant instructions and enables models to generalize to these task-specific instructions without sacrificing overall performance. Experiments show that TCIA improves open-source LLMs' performance by an average of 8.7% across four real-world, task-specific applications, and in some cases outperforming leading closed-source models. These improvements do not compromise general instruction-following ability, making TCIA a scalable and efficient solution for adapting LLMs to real-world, task-focused applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TCIAï¼ˆTask-Centric Instruction Augmentationï¼‰ï¼Œä¸€ç§æ—¨åœ¨è§£å†³ç°æœ‰æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰æ•°æ®å¢å¼ºæ–¹æ³•ä¸­å¿½è§†ä»»åŠ¡ç›¸å…³æ€§ï¼ˆon-task relevanceï¼‰é—®é¢˜çš„æ¡†æ¶ã€‚TCIAé€šè¿‡åœ¨ç¦»æ•£çš„â€œæŸ¥è¯¢-çº¦æŸç©ºé—´â€ï¼ˆdiscrete query-constraints spaceï¼‰ä¸­è¡¨ç¤ºæŒ‡ä»¤ï¼Œåœ¨ä¿æŒæ•°æ®å¤šæ ·æ€§çš„åŒæ—¶ï¼Œç³»ç»Ÿæ€§åœ°ç”Ÿæˆä¸ç‰¹å®šä»»åŠ¡å¯¹é½ï¼ˆtask alignmentï¼‰çš„å¢å¼ºæŒ‡ä»¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTCIAåœ¨å››é¡¹çœŸå®ä¸–ç•Œçš„ç‰¹å®šä»»åŠ¡åº”ç”¨ä¸­ä½¿å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ€§èƒ½å¹³å‡æå‡äº†8.7%ï¼Œä¸”åœ¨éƒ¨åˆ†åœºæ™¯ä¸‹è¶…è¶Šäº†é¢†å…ˆçš„é—­æºæ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—å¢å¼ºç‰¹å®šä»»åŠ¡èƒ½åŠ›çš„åŒæ—¶ï¼Œå¹¶æœªæŸå®³æ¨¡å‹çš„é€šç”¨æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼ˆinstruction-following abilityï¼‰ã€‚TCIAä¸ºå°†å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆé€‚é…åˆ°ç°å®ä¸–ç•Œä»»åŠ¡å¯¼å‘å‹åº”ç”¨ä¸­æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20374v1",
      "published_date": "2025-08-28 02:42:10 UTC",
      "updated_date": "2025-08-28 02:42:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:59:54.589753+00:00"
    },
    {
      "arxiv_id": "2508.20373v1",
      "title": "Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems",
      "title_zh": "Graph-R1ï¼šé€šè¿‡ NP éš¾å›¾é—®é¢˜é‡Šæ”¾å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›",
      "authors": [
        "Yuyao Wang",
        "Bowen Liu",
        "Jianheng Tang",
        "Nuo Chen",
        "Yuhan Li",
        "Qifan Zhang",
        "Jia Li"
      ],
      "abstract": "Reasoning Large Language Models (RLLMs) have recently achieved remarkable progress on complex reasoning tasks, largely enabled by their long chain-of-thought (Long CoT) capabilities. However, developing these Long CoT behaviors relies heavily on post-training with high-quality datasets, which are typically costly and human-curated (e.g., mathematics and code), leaving scalable alternatives unexplored. In this work, we introduce NP-hard (NPH) graph problems as a novel synthetic training corpus, as they inherently require deep reasoning, extensive exploration, and reflective strategies, which are core characteristics of Long CoT reasoning. Building on this insight, we develop a two-stage post-training framework: (i) Long CoT Supervised Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a fine-grained reward design, which sharpens reasoning efficiency. Our flagship model, Graph-R1-7B, demonstrates strong generalization across mathematics, coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both accuracy and reasoning efficiency. These results position NPH graph problems as an effective and scalable resource for advancing Long CoT reasoning in LLMs, opening a new frontier for LLM post-training. Our implementation is available at https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted in our Hugging Face collection HKUST-DSAIL/Graph-R1.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Graph-R1ï¼Œæ—¨åœ¨åˆ©ç”¨ NP-hard (NPH) å›¾é—®é¢˜é‡Šæ”¾å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æ¨ç†æ½œåŠ›ï¼Œè§£å†³ Long Chain-of-Thought (Long CoT) è®­ç»ƒé«˜åº¦ä¾èµ–é«˜æˆæœ¬äººå·¥æ ‡æ³¨æ•°æ®çš„é—®é¢˜ã€‚ç ”ç©¶äººå‘˜å°† NPH å›¾é—®é¢˜ä½œä¸ºä¸€ç§æ–°å‹åˆæˆè®­ç»ƒè¯­æ–™ï¼Œåˆ©ç”¨å…¶å¤©ç„¶éœ€è¦çš„æ·±åº¦æ¨ç†ã€å¹¿æ³›æ¢ç´¢å’Œåæ€ç­–ç•¥æ¥åŸ¹å…»æ¨¡å‹çš„ Long CoT èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼šé¦–å…ˆæ˜¯åœ¨æ‹’ç»é‡‡æ ·çš„ NPH å›¾å®ä¾‹ä¸Šè¿›è¡Œ Supervised Fine-Tuning (SFT) ä»¥å¢å¼ºæ¨ç†æ·±åº¦ï¼Œéšåé‡‡ç”¨å¸¦æœ‰ç»†ç²’åº¦å¥–åŠ±è®¾è®¡çš„ Reinforcement Learning (RL) æ¥ä¼˜åŒ–æ¨ç†æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒGraph-R1-7B æ¨¡å‹åœ¨æ•°å­¦ã€ç¼–ç¨‹ã€STEM å’Œé€»è¾‘é¢†åŸŸè¡¨ç°å‡ºæå¼ºçš„æ³›åŒ–æ€§ï¼Œå¹¶åœ¨ NPH å›¾é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡å’Œæ•ˆç‡å‡è¶…è¿‡äº† QwQ-32Bã€‚æ­¤é¡¹å·¥ä½œè¯æ˜äº† NPH å›¾é—®é¢˜æ˜¯æå‡ LLM æ¨ç†èƒ½åŠ›çš„ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„èµ„æºï¼Œä¸ºå¤§æ¨¡å‹åè®­ç»ƒæŠ€æœ¯æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20373v1",
      "published_date": "2025-08-28 02:40:27 UTC",
      "updated_date": "2025-08-28 02:40:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:59:22.758634+00:00"
    },
    {
      "arxiv_id": "2508.20371v1",
      "title": "P2C: Path to Counterfactuals",
      "title_zh": "P2Cï¼šé€šå¾€åäº‹å®è§£é‡Šçš„è·¯å¾„",
      "authors": [
        "Sopam Dasgupta",
        "Sadaf MD Halim",
        "JoaquÃ­n Arias",
        "Elmer Salazar",
        "Gopal Gupta"
      ],
      "abstract": "Machine-learning models are increasingly driving decisions in high-stakes settings, such as finance, law, and hiring, thus, highlighting the need for transparency. However, the key challenge is to balance transparency -- clarifying `why' a decision was made -- with recourse: providing actionable steps on `how' to achieve a favourable outcome from an unfavourable outcome. Counterfactual explanations reveal `why' an undesired outcome occurred and `how' to reverse it through targeted feature changes (interventions).\n  Current counterfactual approaches have limitations: 1) they often ignore causal dependencies between features, and 2) they typically assume all interventions can happen simultaneously, an unrealistic assumption in practical scenarios where actions are typically taken in a sequence. As a result, these counterfactuals are often not achievable in the real world.\n  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that produces a plan (ordered sequence of actions) converting an unfavourable outcome to a causally consistent favourable outcome. P2C addresses both limitations by 1) Explicitly modelling causal relationships between features and 2) Ensuring that each intermediate state in the plan is feasible and causally valid. P2C uses the goal-directed Answer Set Programming system s(CASP) to generate the plan accounting for feature changes that happen automatically due to causal dependencies. Furthermore, P2C refines cost (effort) computation by only counting changes actively made by the user, resulting in realistic cost estimates. Finally, P2C highlights how its causal planner outperforms standard planners, which lack causal knowledge and thus can generate illegal actions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜é£é™©å†³ç­–åœºæ™¯ä¸­æœºå™¨å­¦ä¹ æ¨¡å‹ç¼ºä¹é€æ˜åº¦å’Œè¿½æº¯æ€§ï¼ˆrecourseï¼‰çš„é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰çš„åäº‹å®è§£é‡Šï¼ˆCounterfactual explanationsï¼‰å¾€å¾€å¿½ç•¥äº†ç‰¹å¾é—´çš„å› æœä¾èµ–ï¼Œä¸”é”™è¯¯åœ°å‡è®¾æ‰€æœ‰å¹²é¢„æªæ–½å¯ä»¥åŒæ—¶å‘ç”Ÿã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†P2C (Path-to-Counterfactuals)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸æ¨¡å‹æ— å…³çš„æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆä¸€æ¡æœ‰åºçš„è¡ŒåŠ¨åºåˆ—ï¼Œå°†ä¸åˆ©ç»“æœè½¬åŒ–ä¸ºå› æœä¸€è‡´çš„æœ‰åˆ©ç»“æœã€‚P2Cåˆ©ç”¨ç›®æ ‡å¯¼å‘çš„ç­”æ¡ˆé›†ç¼–ç¨‹ï¼ˆAnswer Set Programmingï¼‰ç³»ç»Ÿs(CASP)æ¥æ˜¾å¼å»ºæ¨¡ç‰¹å¾é—´çš„å› æœå…³ç³»ï¼Œç¡®ä¿è®¡åˆ’ä¸­çš„æ¯ä¸ªä¸­é—´çŠ¶æ€åœ¨å› æœä¸Šå‡æœ‰æ•ˆä¸”å¯è¡Œã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä»…è®¡ç®—ç”¨æˆ·ä¸»åŠ¨å®æ–½çš„æ›´æ”¹æ¥ä¼˜åŒ–æˆæœ¬ï¼ˆcostï¼‰ä¼°ç®—ï¼Œä»è€Œæä¾›äº†æ›´ç¬¦åˆç°å®çš„åŠªåŠ›è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒP2Cçš„å› æœè§„åˆ’å™¨ï¼ˆcausal plannerï¼‰æ€§èƒ½ä¼˜äºç¼ºä¹å› æœçŸ¥è¯†çš„ä¼ ç»Ÿè§„åˆ’å™¨ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé¿å…ç”Ÿæˆéæ³•æ“ä½œï¼Œä¸ºå®ç°å¯æ“ä½œä¸”ç°å®çš„åäº‹å®å¼•å¯¼æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20371v1",
      "published_date": "2025-08-28 02:36:02 UTC",
      "updated_date": "2025-08-28 02:36:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:59:22.569395+00:00"
    },
    {
      "arxiv_id": "2508.20370v1",
      "title": "Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought",
      "title_zh": "åŸºäºå¤šæ™ºèƒ½ä½“é€’å½’æ€ç»´çš„å¾®æœåŠ¡ç³»ç»Ÿè‡ªé€‚åº”æ ¹å› å®šä½",
      "authors": [
        "Lingzhe Zhang",
        "Tong Jia",
        "Kangjin Wang",
        "Weijie Hong",
        "Chiming Duan",
        "Minghua He",
        "Ying Li"
      ],
      "abstract": "As contemporary microservice systems become increasingly popular and complex-often comprising hundreds or even thousands of fine-grained, interdependent subsystems-they are facing more frequent failures. Ensuring system reliability thus demands accurate root cause localization. While traces and metrics have proven to be effective data sources for this task, existing methods either heavily rely on pre-defined schemas, which struggle to adapt to evolving operational contexts, or lack interpretability in their reasoning process, thereby leaving Site Reliability Engineers (SREs) confused. In this paper, we conduct a comprehensive study on how SREs localize the root cause of failures, drawing insights from multiple professional SREs across different organizations. Our investigation reveals that human root cause analysis exhibits three key characteristics: recursiveness, multi-dimensional expansion, and cross-modal reasoning. Motivated by these findings, we introduce RCLAgent, an adaptive root cause localization method for microservice systems that leverages a multi-agent recursion-of-thought framework. RCLAgent employs a novel recursion-of-thought strategy to guide the LLM's reasoning process, effectively integrating data from multiple agents and tool-assisted analysis to accurately pinpoint the root cause. Experimental evaluations on various public datasets demonstrate that RCLAgent achieves superior performance by localizing the root cause using only a single request-outperforming state-of-the-art methods that depend on aggregating multiple requests. These results underscore the effectiveness of RCLAgent in enhancing the efficiency and precision of root cause localization in complex microservice environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£å¾®æœåŠ¡ç³»ç»Ÿå¤æ‚æ€§å¸¦æ¥çš„æ•…éšœå®šä½éš¾é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿæ–¹æ³•åœ¨é€‚åº”åŠ¨æ€åœºæ™¯å’Œæ¨ç†å¯è§£é‡Šæ€§æ–¹é¢çš„ä¸è¶³ã€‚é€šè¿‡è°ƒç ”ä¸“ä¸šç«™ç‚¹å¯é æ€§å·¥ç¨‹å¸ˆ(SRE)çš„å®è·µç»éªŒï¼Œè®ºæ–‡æ€»ç»“å‡ºäººç±»æ ¹å› åˆ†æå…·å¤‡é€’å½’æ€§ã€å¤šç»´æ‰©å±•å’Œè·¨æ¨¡æ€æ¨ç†çš„ç‰¹å¾ï¼Œå¹¶æ®æ­¤æå‡ºäº†RCLAgentã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¤šæ™ºèƒ½ä½“é€’å½’æ€ç»´(Recursion-of-Thought)æ¡†æ¶ï¼Œé€šè¿‡åˆ›æ–°çš„é€’å½’ç­–ç•¥å¼•å¯¼å¤§è¯­è¨€æ¨¡å‹(LLMs)æ•´åˆå¤šæºæ•°æ®ä¸å·¥å…·è¾…åŠ©åˆ†æï¼Œå®ç°è‡ªé€‚åº”çš„æ ¹å› å®šä½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRCLAgentåœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä»…å‡­å•æ¬¡è¯·æ±‚çš„å®šä½ç²¾åº¦ä¾¿è¶…è¿‡äº†éœ€è¦èšåˆå¤šæ¬¡è¯·æ±‚çš„ç°æœ‰æœ€å…ˆè¿›(SOTA)æ–¹æ³•ã€‚è¿™ä¸€ç ”ç©¶æ˜¾è‘—æå‡äº†å¤æ‚å¾®æœåŠ¡ç¯å¢ƒä¸‹æ ¹å› å®šä½çš„æ•ˆç‡ä¸ç²¾ç¡®åº¦ï¼Œä¸ºè‡ªåŠ¨åŒ–è¿ç»´æä¾›äº†é«˜æ•ˆä¸”å…·å¤‡å¯è§£é‡Šæ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20370v1",
      "published_date": "2025-08-28 02:34:19 UTC",
      "updated_date": "2025-08-28 02:34:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:59:27.038316+00:00"
    },
    {
      "arxiv_id": "2508.20368v4",
      "title": "AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning",
      "title_zh": "AI-SearchPlannerï¼šåŸºäºå¸•ç´¯æ‰˜æœ€ä¼˜å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ çš„æ¨¡å—åŒ–æ™ºèƒ½ä½“æœç´¢",
      "authors": [
        "Lang Mei",
        "Zhihan Yang",
        "Xiaohan Yu",
        "Huanyao Zhang",
        "Chong Chen"
      ],
      "abstract": "Recent studies have explored integrating Large Language Models (LLMs) with search engines to leverage both the LLMs' internal pre-trained knowledge and external information. Specially, reinforcement learning (RL) has emerged as a promising paradigm for enhancing LLM reasoning through multi-turn interactions with search engines. However, existing RL-based search agents rely on a single LLM to handle both search planning and question-answering (QA) tasks in an end-to-end manner, which limits their ability to optimize both capabilities simultaneously. In practice, sophisticated AI search systems often employ a large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a more effective and efficient approach is to utilize a small, trainable LLM dedicated to search planning. In this paper, we propose \\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to enhance the performance of frozen QA models by focusing on search planning. Specifically, our approach introduces three key innovations: 1) Decoupling the Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to achieve the objectives. Extensive experiments on real-world datasets demonstrate that AI SearchPlanner outperforms existing RL-based search agents in both effectiveness and efficiency, while exhibiting strong generalization capabilities across diverse frozen QA models and data domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AI-SearchPlannerï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) å¢å¼º search planning èƒ½åŠ›çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å†»ç»“çŠ¶æ€çš„é—®ç­” (Question-Answering) æ¨¡å‹çš„æ€§èƒ½ã€‚é’ˆå¯¹ç°æœ‰å¼ºåŒ–å­¦ä¹ æœç´¢æ™ºèƒ½ä½“ä¾èµ–å•ä¸€æ¨¡å‹è¿›è¡Œç«¯åˆ°ç«¯å¤„ç†ã€éš¾ä»¥åŒæ—¶ä¼˜åŒ–è§„åˆ’ä¸é—®ç­”èƒ½åŠ›çš„å±€é™ï¼Œè¯¥æ¡†æ¶å®ç°äº†è§„åˆ’å™¨ä¸ç”Ÿæˆå™¨æ¶æ„çš„è§£è€¦ (Decoupling)ã€‚ç ”ç©¶å¼•å…¥äº†é’ˆå¯¹æœç´¢è§„åˆ’çš„åŒå¥–åŠ±å¯¹é½ (Dual-Reward Alignment) æœºåˆ¶ï¼Œå¹¶é‡‡ç”¨å¸•ç´¯æ‰˜ä¼˜åŒ– (Pareto Optimization) æ¥å¹³è¡¡è§„åˆ’æ•ˆç”¨ä¸æˆæœ¬ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒAI-SearchPlanner åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æœç´¢æ™ºèƒ½ä½“ï¼Œæ˜¾è‘—æå‡äº†æœç´¢çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ•°æ®é¢†åŸŸä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæ„å»ºé«˜æ•ˆä¸”å¯æ‰©å±•çš„æ¨¡å—åŒ–æ™ºèƒ½æœç´¢ç³»ç»Ÿæä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20368v4",
      "published_date": "2025-08-28 02:31:17 UTC",
      "updated_date": "2025-12-27 06:47:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:59:37.757838+00:00"
    },
    {
      "arxiv_id": "2508.21098v1",
      "title": "TrInk: Ink Generation with Transformer Network",
      "title_zh": "TrInkï¼šåŸºäº Transformer ç½‘ç»œçš„ç¬”è¿¹ç”Ÿæˆ",
      "authors": [
        "Zezhong Jin",
        "Shubhang Desai",
        "Xu Chen",
        "Biyi Fang",
        "Zhuoyi Huang",
        "Zhe Li",
        "Chong-Xin Gan",
        "Xiao Tu",
        "Man-Wai Mak",
        "Yan Lu",
        "Shujie Liu"
      ],
      "abstract": "In this paper, we propose TrInk, a Transformer-based model for ink generation, which effectively captures global dependencies. To better facilitate the alignment between the input text and generated stroke points, we introduce scaled positional embeddings and a Gaussian memory mask in the cross-attention module. Additionally, we design both subjective and objective evaluation pipelines to comprehensively assess the legibility and style consistency of the generated handwriting. Experiments demonstrate that our Transformer-based model achieves a 35.56\\% reduction in character error rate (CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset compared to previous methods. We provide an demo page with handwriting samples from TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TrInkï¼Œä¸€ç§åŸºäºTransformerçš„å¢¨æ°´ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨æœ‰æ•ˆæ•æ‰æ‰‹å†™ç¬”è¿¹ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å…¨å±€ä¾èµ–å…³ç³»ã€‚ä¸ºäº†æå‡è¾“å…¥æ–‡æœ¬ä¸ç”Ÿæˆçš„ç¬”ç”»ç‚¹ä¹‹é—´çš„å¯¹é½æ•ˆæœï¼ŒTrInkåœ¨äº¤å‰æ³¨æ„åŠ›(cross-attention)æ¨¡å—ä¸­å¼•å…¥äº†ç¼©æ”¾ä½ç½®åµŒå…¥(scaled positional embeddings)å’Œé«˜æ–¯è®°å¿†æ©ç (Gaussian memory mask)ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸»å®¢è§‚è¯„ä¼°æµç¨‹ï¼Œä»¥å…¨é¢è¡¡é‡ç”Ÿæˆæ‰‹å†™ç¬”è¿¹çš„å¯è¯»æ€§(legibility)å’Œé£æ ¼ä¸€è‡´æ€§(style consistency)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨IAM-OnDBæ•°æ®é›†ä¸Šï¼ŒTrInkç›¸æ¯”ç°æœ‰æ–¹æ³•åœ¨å­—ç¬¦é”™è¯¯ç‡(CER)ä¸Šé™ä½äº†35.56%ï¼Œåœ¨è¯é”™è¯¯ç‡(WER)ä¸Šé™ä½äº†29.66%ã€‚è¯¥ç ”ç©¶ä¸ä»…æ˜¾è‘—æå‡äº†æ‰‹å†™ä½“ç”Ÿæˆçš„å‡†ç¡®æ€§ï¼Œä¹Ÿä¸ºé«˜è´¨é‡æ•°å­—ç¬”è¿¹åˆæˆæŠ€æœ¯æä¾›äº†æ–°çš„æ€è·¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2508.21098v1",
      "published_date": "2025-08-28 01:44:15 UTC",
      "updated_date": "2025-08-28 01:44:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T14:59:41.156033+00:00"
    },
    {
      "arxiv_id": "2508.21097v1",
      "title": "Model-Driven Quantum Code Generation Using Large Language Models and Retrieval-Augmented Generation",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ¨¡å‹é©±åŠ¨é‡å­ä»£ç ç”Ÿæˆ",
      "authors": [
        "Nazanin Siavash",
        "Armin Moin"
      ],
      "abstract": "This paper introduces a novel research direction for model-to-text/code transformations by leveraging Large Language Models (LLMs) that can be enhanced with Retrieval-Augmented Generation (RAG) pipelines. The focus is on quantum and hybrid quantum-classical software systems, where model-driven approaches can help reduce the costs and mitigate the risks associated with the heterogeneous platform landscape and lack of developers' skills. We validate one of the proposed ideas regarding generating code out of UML model instances of software systems. This Python code uses a well-established library, called Qiskit, to execute on gate-based or circuit-based quantum computers. The RAG pipeline that we deploy incorporates sample Qiskit code from public GitHub repositories. Experimental results show that well-engineered prompts can improve CodeBLEU scores by up to a factor of four, yielding more accurate and consistent quantum code. However, the proposed research direction can go beyond this through further investigation in the future by conducting experiments to address our other research questions and ideas proposed here, such as deploying software system model instances as the source of information in the RAG pipelines, or deploying LLMs for code-to-code transformations, for instance, for transpilation use cases.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆå¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) çš„æ¨¡å‹é©±åŠ¨å¼€å‘æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³é‡å­è½¯ä»¶ç³»ç»Ÿä¸­å¼‚æ„å¹³å°å’Œå¼€å‘è€…æŠ€èƒ½çŸ­ç¼ºå¸¦æ¥çš„æŒ‘æˆ˜ã€‚é€šè¿‡ä» UML æ¨¡å‹å®ä¾‹ç”ŸæˆåŸºäº Qiskit åº“çš„ Python ä»£ç ï¼Œè¯¥æ–¹æ³•å®ç°äº†é‡å­åŠæ··åˆé‡å­-ç»å…¸ç³»ç»Ÿçš„è‡ªåŠ¨åŒ–ä»£ç è½¬åŒ–ã€‚ç ”ç©¶åœ¨ RAG æµæ°´çº¿ä¸­å¼•å…¥äº† GitHub å…¬å…±ä»“åº“çš„ä»£ç æ ·æœ¬ï¼Œå¹¶è¯æ˜ç²¾å¿ƒè®¾è®¡çš„æç¤ºè¯èƒ½å°† CodeBLEU åˆ†æ•°æå‡è‡³åŸæ¥çš„å››å€ï¼Œæ˜¾è‘—å¢å¼ºäº†ä»£ç ç”Ÿæˆçš„å‡†ç¡®æ€§ä¸ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œè¿˜æ¢è®¨äº†æœªæ¥å°†è½¯ä»¶æ¨¡å‹ä½œä¸º RAG æ•°æ®æºä»¥åŠåˆ©ç”¨ LLMs è¿›è¡Œä»£ç è½¬è¯‘ (transpilation) ç­‰ code-to-code è½¬æ¢ä»»åŠ¡çš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "This paper is accepted to the New Ideas and Emerging Results (NIER) track of the ACM/IEEE 28th International Conference on Model Driven Engineering Languages and Systems (MODELS)",
      "pdf_url": "https://arxiv.org/pdf/2508.21097v1",
      "published_date": "2025-08-28 01:33:49 UTC",
      "updated_date": "2025-08-28 01:33:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:00:12.659929+00:00"
    },
    {
      "arxiv_id": "2508.20340v1",
      "title": "Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå…¬å¼ç”Ÿæˆå™¨ä»¥å¢å¼ºéª¨æ¶é©±åŠ¨çš„ SMT æ±‚è§£å™¨æ¨¡ç³Šæµ‹è¯•",
      "authors": [
        "Maolin Sun",
        "Yibiao Yang",
        "Yuming Zhou"
      ],
      "abstract": "Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems and programming languages research, providing the foundation for tasks like symbolic execution and automated verification. Because these solvers sit on the critical path, their correctness is essential, and high-quality test formulas are key to uncovering bugs. However, while prior testing techniques performed well on earlier solver versions, they struggle to keep pace with rapidly evolving features. Recent approaches based on Large Language Models (LLMs) show promise in exploring advanced solver capabilities, but two obstacles remain: nearly half of the generated formulas are syntactically invalid, and iterative interactions with the LLMs introduce substantial computational overhead. In this study, we present Chimera, a novel LLM-assisted fuzzing framework that addresses both issues by shifting from direct formula generation to the synthesis of reusable term (i.e., logical expression) generators. Particularly, Chimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for SMT theories, including solver-specific extensions, from documentation, and (2) synthesize composable Boolean term generators that adhere to these grammars. During fuzzing, Chimera populates structural skeletons derived from existing formulas with the terms iteratively produced by the LLM-synthesized generators. This design ensures syntactic validity while promoting semantic diversity. Notably, Chimera requires only one-time LLM interaction investment, dramatically reducing runtime cost. We evaluated Chimera on two leading SMT solvers: Z3 and cvc5. Our experiments show that Chimera has identified 43 confirmed bugs, 40 of which have already been fixed by developers.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Chimeraï¼Œä¸€ä¸ªæ—¨åœ¨åˆ©ç”¨ Large Language Models (LLMs) æå‡ Satisfiability Modulo Theory (SMT) æ±‚è§£å™¨æ¨¡ç³Šæµ‹è¯•(Fuzzing)æ•ˆç‡çš„æ–°å‹æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¸­ LLMs ç›´æ¥ç”Ÿæˆå…¬å¼å¯¼è‡´è¯­æ³•æ— æ•ˆæ¯”ä¾‹é«˜ä»¥åŠè®¡ç®—å¼€é”€å¤§çš„æŒ‘æˆ˜ï¼ŒChimera åˆ›æ–°æ€§åœ°å°†ä»»åŠ¡ä»å…¬å¼ç”Ÿæˆè½¬å˜ä¸ºåˆæˆå¯é‡ç”¨çš„é€»è¾‘é¡¹ç”Ÿæˆå™¨(term generators)ã€‚è¯¥æ¡†æ¶é€šè¿‡ LLMs è‡ªåŠ¨ä»æ–‡æ¡£ä¸­æå–ä¸Šä¸‹æ–‡æ— å…³æ–‡æ³•(Context-Free Grammars, CFGs)ï¼Œå¹¶æ®æ­¤åˆæˆå¯ç»„åˆçš„å¸ƒå°”é¡¹ç”Ÿæˆå™¨ï¼Œéšåå°†å…¶ç”Ÿæˆçš„é¡¹å¡«å……åˆ°å·²æœ‰çš„ structural skeletons ä¸­ï¼Œä»è€Œåœ¨ä¿è¯è¯­æ³•æ­£ç¡®æ€§çš„åŒæ—¶å¤§å¹…æå‡äº†è¯­ä¹‰å¤šæ ·æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒChimera ä»…éœ€å•æ¬¡ LLM äº¤äº’å³å¯è¿è¡Œï¼Œæ˜¾è‘—é™ä½äº†æµ‹è¯•æˆæœ¬ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒChimera åœ¨ä¸»æµæ±‚è§£å™¨ Z3 å’Œ cvc5 ä¸­å‘ç°äº†43ä¸ªå·²ç¡®è®¤çš„æ¼æ´ï¼Œå…¶ä¸­40ä¸ªå·²è·å¾—å¼€å‘è€…ä¿®å¤ï¼Œå……åˆ†è¯æ˜äº†å…¶åœ¨æŒ–æ˜ç°ä»£æ±‚è§£å™¨ç¼ºé™·æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20340v1",
      "published_date": "2025-08-28 01:21:26 UTC",
      "updated_date": "2025-08-28 01:21:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:00:18.667884+00:00"
    },
    {
      "arxiv_id": "2509.00105v2",
      "title": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and High-Quality Language Model Serving",
      "title_zh": "AdaptCacheï¼šé¢å‘ä½å»¶è¿Ÿä¸é«˜è´¨é‡è¯­è¨€æ¨¡å‹æœåŠ¡çš„ KV ç¼“å­˜åŸç”Ÿå­˜å‚¨å±‚çº§æ¶æ„",
      "authors": [
        "Shaoting Feng",
        "Hanchen Li",
        "Kuntai Du",
        "Zhuohan Gu",
        "Yuhan Liu",
        "Jiayi Yao",
        "Siddhant Ray",
        "Samuel Shen",
        "Yihua Cheng",
        "Ganesh Ananthanarayanan",
        "Junchen Jiang"
      ],
      "abstract": "Large language model (LLM) applications often reuse previously processed context, such as chat history and documents, which introduces significant redundant computation. Existing LLM serving systems address such redundant computation by storing the KV caches of processed context and loading the corresponding KV cache when a new request reuses the context. Further, as these LLM applications scale, the total size of KV caches becomes excessively large and requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high loading delays, as most KV cache hits come from SSD, which is slow to load. To increase the KV cache hit rate on DRAM, we identify lossy KV cache compression as a promising approach. We design a lossy compression system that decides the compression algorithm, compression rate and device placement for each KV cache entry to maximise DRAM hits and minimise loading delay without significantly degrading generation quality. Compared to various static compression baselines across three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at the same quality and 6--55% quality improvements at the same delay.",
      "tldr_zh": "é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)åº”ç”¨ä¸­å› é‡ç”¨ä¸Šä¸‹æ–‡å¯¼è‡´çš„å·¨å¤§ KV cache å­˜å‚¨åŠä» SSD åŠ è½½äº§ç”Ÿçš„é«˜å»¶è¿Ÿé—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº† AdaptCache ç³»ç»Ÿã€‚AdaptCache é‡‡ç”¨äº†ä¸€ç§ KV cache åŸç”Ÿå­˜å‚¨å±‚çº§æ¶æ„ï¼Œé€šè¿‡å¼•å…¥æœ‰æŸ KV cache å‹ç¼©(Lossy KV cache compression)æŠ€æœ¯æ¥æé«˜ DRAM çš„å‘½ä¸­ç‡ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿä¸ºæ¯ä¸ª KV cache æ¡ç›®åŠ¨æ€å†³ç­–å‹ç¼©ç®—æ³•ã€å‹ç¼©ç‡ä»¥åŠè®¾å¤‡æ”¾ç½®(Device placement)ç­–ç•¥ï¼Œä»è€Œåœ¨ä¸æ˜¾è‘—é™ä½ç”Ÿæˆè´¨é‡çš„å‰æä¸‹æœ€å¤§åŒ– DRAM å‘½ä¸­å¹¶æœ€å°åŒ–åŠ è½½å»¶è¿Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å¤šç§é™æ€å‹ç¼©åŸºçº¿ç›¸æ¯”ï¼ŒAdaptCache åœ¨ç›¸åŒæ¨¡å‹è´¨é‡ä¸‹å®ç°äº† 1.43 è‡³ 2.4 å€çš„å»¶è¿ŸèŠ‚çœï¼Œå¹¶åœ¨ç›¸åŒå»¶è¿Ÿæ°´å¹³ä¸‹å°†ç”Ÿæˆè´¨é‡æå‡äº† 6% è‡³ 55%ã€‚è¯¥ç ”ç©¶æœ‰æ•ˆè§£å†³äº†å¤§è§„æ¨¡ LLM æœåŠ¡ä¸­å­˜å‚¨æˆæœ¬ä¸å“åº”é€Ÿåº¦ä¹‹é—´çš„çŸ›ç›¾ï¼Œä¸ºå®ç°ä½å»¶è¿Ÿã€é«˜è´¨é‡çš„æ¨ç†æœåŠ¡æä¾›äº†é«˜æ•ˆçš„å­˜å‚¨ç®¡ç†æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.OS",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.OS",
      "comment": "Accepted at SOSP 2025 - The International Workshop on Big Memory (BigMem)",
      "pdf_url": "https://arxiv.org/pdf/2509.00105v2",
      "published_date": "2025-08-28 00:46:51 UTC",
      "updated_date": "2026-01-15 21:12:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:00:16.225688+00:00"
    },
    {
      "arxiv_id": "2508.20333v1",
      "title": "Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs",
      "title_zh": "ä¸€æ¬¡æŠ•æ¯’ï¼Œæ°¸ä¹…æ‹’ç»ï¼šåœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­å°†å¯¹é½æœºåˆ¶æ­¦å™¨åŒ–ä»¥æ³¨å…¥åè§",
      "authors": [
        "Md Abdullah Al Mamun",
        "Ihsen Alouani",
        "Nael Abu-Ghazaleh"
      ],
      "abstract": "Large Language Models (LLMs) are aligned to meet ethical standards and safety requirements by training them to refuse answering harmful or unsafe prompts. In this paper, we demonstrate how adversaries can exploit LLMs' alignment to implant bias, or enforce targeted censorship without degrading the model's responsiveness to unrelated topics. Specifically, we propose Subversive Alignment Injection (SAI), a poisoning attack that leverages the alignment mechanism to trigger refusal on specific topics or queries predefined by the adversary. Although it is perhaps not surprising that refusal can be induced through overalignment, we demonstrate how this refusal can be exploited to inject bias into the model. Surprisingly, SAI evades state-of-the-art poisoning defenses including LLM state forensics, as well as robust aggregation techniques that are designed to detect poisoning in FL settings. We demonstrate the practical dangers of this attack by illustrating its end-to-end impacts on LLM-powered application pipelines. For chat based applications such as ChatDoctor, with 1% data poisoning, the system refuses to answer healthcare questions to targeted racial category leading to high bias ($Î”DP$ of 23%). We also show that bias can be induced in other NLP tasks: for a resume selection pipeline aligned to refuse to summarize CVs from a selected university, high bias in selection ($Î”DP$ of 27%) results. Even higher bias ($Î”DP$~38%) results on 9 other chat based downstream applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Subversive Alignment Injection (SAI)ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸­æ¯’æ”»å‡»(poisoning attack)ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„å¯¹é½(alignment)æœºåˆ¶æ¥æ³¨å…¥åè§æˆ–å®æ–½å®šå‘å®¡æŸ¥ã€‚SAIé€šè¿‡æ“çºµæ‹’ç»å›ç­”æœºåˆ¶ï¼Œä½¿æ¨¡å‹åœ¨é¢å¯¹æ”»å‡»è€…é¢„å®šä¹‰çš„ç‰¹å®šä¸»é¢˜æˆ–æŸ¥è¯¢æ—¶è§¦å‘æ‹’ç»ï¼Œè€Œä¸ä¼šé™ä½æ¨¡å‹åœ¨æ— å…³è¯é¢˜ä¸Šçš„å“åº”èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼ŒSAIèƒ½å¤Ÿæœ‰æ•ˆè§„é¿ç°æœ‰çš„æœ€å…ˆè¿›ä¸­æ¯’é˜²å¾¡æªæ–½ï¼ŒåŒ…æ‹¬LLMçŠ¶æ€å–è¯(state forensics)ä»¥åŠè”é‚¦å­¦ä¹ (FL)ç¯å¢ƒä¸‹çš„é²æ£’èšåˆæŠ€æœ¯ã€‚åœ¨ChatDoctorç­‰åŒ»ç–—å¯¹è¯åº”ç”¨ä¸­ï¼Œä»…éœ€1%çš„æ•°æ®ä¸­æ¯’å³å¯å¯¼è‡´æ¨¡å‹æ‹’ç»å›ç­”ç‰¹å®šç§æ—ç±»åˆ«çš„åŒ»ç–—é—®é¢˜ï¼Œäº§ç”Ÿé«˜è¾¾23%çš„åè§ã€‚æ­¤å¤–ï¼Œåœ¨ç®€å†ç­›é€‰æµç¨‹ä¸­ï¼Œè¯¥æ”»å‡»é€šè¿‡è¯±å¯¼æ¨¡å‹æ‹’ç»æ€»ç»“ç‰¹å®šå¤§å­¦çš„ç®€å†ï¼Œå¯¼è‡´äº†27%çš„ç­›é€‰åè§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å…¶ä»–9ä¸ªä¸‹æ¸¸å¯¹è¯åº”ç”¨ä¸­ï¼Œè¯¥æ”»å‡»å¼•å‘çš„åè§ç¨‹åº¦å¯é«˜è¾¾38%ï¼Œå‡¸æ˜¾äº†åˆ©ç”¨å¯¹é½æœºåˆ¶æ­¦å™¨åŒ–åå¯¹LLMåº”ç”¨ç”Ÿæ€æ„æˆçš„ä¸¥å³»å¨èƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20333v1",
      "published_date": "2025-08-28 00:30:25 UTC",
      "updated_date": "2025-08-28 00:30:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:00:21.070299+00:00"
    },
    {
      "arxiv_id": "2508.20328v1",
      "title": "Multi-View Graph Convolution Network for Internal Talent Recommendation Based on Enterprise Emails",
      "title_zh": "åŸºäºä¼ä¸šé‚®ä»¶çš„å†…éƒ¨äººæ‰æ¨èå¤šè§†å›¾å›¾å·ç§¯ç½‘ç»œ",
      "authors": [
        "Soo Hyun Kim",
        "Jang-Hyun Kim"
      ],
      "abstract": "Internal talent recommendation is a critical strategy for organizational continuity, yet conventional approaches suffer from structural limitations, often overlooking qualified candidates by relying on the narrow perspective of a few managers. To address this challenge, we propose a novel framework that models two distinct dimensions of an employee's position fit from email data: WHAT they do (semantic similarity of tasks) and HOW they work (structural characteristics of their interactions and collaborations). These dimensions are represented as independent graphs and adaptively fused using a Dual Graph Convolutional Network (GCN) with a gating mechanism. Experiments show that our proposed gating-based fusion model significantly outperforms other fusion strategies and a heuristic baseline, achieving a top performance of 40.9% on Hit@100. Importantly, it is worth noting that the model demonstrates high interpretability by learning distinct, context-aware fusion strategies for different job families. For example, it learned to prioritize relational (HOW) data for 'sales and marketing' job families while applying a balanced approach for 'research' job families. This research offers a quantitative and comprehensive framework for internal talent discovery, minimizing the risk of candidate omission inherent in traditional methods. Its primary contribution lies in its ability to empirically determine the optimal fusion ratio between task alignment (WHAT) and collaborative patterns (HOW), which is required for employees to succeed in the new positions, thereby offering important practical implications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤šè§†å›¾å›¾å·ç§¯ç½‘ç»œ (Multi-View Graph Convolution Network) çš„æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨ä¼ä¸šç”µå­é‚®ä»¶æ•°æ®æ”¹è¿›å†…éƒ¨äººæ‰æ¨èï¼Œè§£å†³ä¼ ç»Ÿæ–¹æ³•å› è¿‡åº¦ä¾èµ–ç»ç†å•ä¸€è§†è§’è€Œå¯¼è‡´çš„äººæ‰é—æ¼é—®é¢˜ã€‚è¯¥æ¡†æ¶ä» WHATï¼ˆä»»åŠ¡è¯­ä¹‰ç›¸ä¼¼æ€§ï¼‰å’Œ HOWï¼ˆåä½œäº’åŠ¨çš„ç»“æ„ç‰¹å¾ï¼‰ä¸¤ä¸ªç»´åº¦å»ºæ¨¡å‘˜å·¥çš„èŒä½åŒ¹é…åº¦ï¼Œå¹¶åˆ©ç”¨å¸¦æœ‰é—¨æ§æœºåˆ¶ (gating mechanism) çš„åŒå›¾å·ç§¯ç½‘ç»œ (Dual GCN) å®ç°ç‰¹å¾å›¾çš„è‡ªé€‚åº”èåˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ Hit@100 æŒ‡æ ‡ä¸Šè¾¾åˆ° 40.9%ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–èåˆç­–ç•¥å’Œå¯å‘å¼åŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å±•ç°å‡ºé«˜åº¦çš„å¯è§£é‡Šæ€§ï¼Œèƒ½å¤Ÿé’ˆå¯¹ä¸åŒèŒä¸šç±»åˆ«å­¦ä¹ ç‰¹å®šçš„èåˆæƒé‡ï¼Œä¾‹å¦‚å¯¹é”€å”®å²—ä½ä¾§é‡ HOW ç»´åº¦ï¼Œè€Œå¯¹ç ”å‘å²—ä½åˆ™é‡‡å–å¹³è¡¡ç­–ç•¥ã€‚è¯¥é¡¹ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºé€šè¿‡å®è¯ç¡®å®šäº†ä»»åŠ¡å¯¹é½ä¸åä½œæ¨¡å¼ä¹‹é—´çš„æœ€ä¼˜èåˆæ¯”ä¾‹ï¼Œä¸ºä¼ä¸šå†…éƒ¨äººæ‰å‘æ˜æä¾›äº†ä¸€ä¸ªé‡åŒ–ä¸”å…¨é¢çš„åˆ†ææ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.20328v1",
      "published_date": "2025-08-28 00:11:24 UTC",
      "updated_date": "2025-08-28 00:11:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:00:20.661923+00:00"
    },
    {
      "arxiv_id": "2508.20325v2",
      "title": "GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs",
      "title_zh": "GUARDï¼šåŸºäºè‡ªé€‚åº”è§’è‰²æ‰®æ¼”ä¸è¶Šç‹±è¯Šæ–­çš„å¤§è¯­è¨€æ¨¡å‹å‡†åˆ™éµå¾ªæµ‹è¯•",
      "authors": [
        "Haibo Jin",
        "Ruoxi Chen",
        "Peiyan Zhang",
        "Andy Zhou",
        "Haohan Wang"
      ],
      "abstract": "As Large Language Models become increasingly integral to various domains, their potential to generate harmful responses has prompted significant societal and regulatory concerns. In response, governments have issued ethics guidelines to promote the development of trustworthy AI. However, these guidelines are typically high-level demands for developers and testers, leaving a gap in translating them into actionable testing questions to verify LLM compliance.\n  To address this challenge, we introduce GUARD (\\textbf{G}uideline \\textbf{U}pholding Test through \\textbf{A}daptive \\textbf{R}ole-play and Jailbreak \\textbf{D}iagnostics), a testing method designed to operationalize guidelines into specific guideline-violating questions that assess LLM adherence. To implement this, GUARD uses automated generation of guideline-violating questions based on government-issued guidelines, thereby testing whether responses comply with these guidelines. When responses directly violate guidelines, GUARD reports inconsistencies. Furthermore, for responses that do not directly violate guidelines, GUARD integrates the concept of ``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that provoke unethical or guideline-violating responses, effectively identifying potential scenarios that could bypass built-in safety mechanisms. Our method finally culminates in a compliance report, delineating the extent of adherence and highlighting any violations.\n  We have empirically validated the effectiveness of GUARD on seven LLMs, including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4, GPT-4o, and Claude-3.7, by testing compliance under three government-issued guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can transfer jailbreak diagnostics to vision-language models, demonstrating its usage in promoting reliable LLM-based applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GUARD (Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics)ï¼Œæ—¨åœ¨è§£å†³æ”¿åºœå‘å¸ƒçš„å®è§‚ AI ä¼¦ç†å‡†åˆ™éš¾ä»¥è½¬åŒ–ä¸ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) å…·ä½“åˆè§„æ€§æµ‹è¯•çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªåŠ¨åŒ–æŠ€æœ¯å°†æŠ½è±¡å‡†åˆ™è½¬åŒ–ä¸ºå…·ä½“çš„è¿è§„æµ‹è¯•é—®é¢˜ï¼Œå¹¶åˆ©ç”¨è¶Šç‹±è¯Šæ–­æ¨¡å— GUARD-JD æ„å»ºç‰¹å®šæƒ…æ™¯ï¼Œä»¥è¯±å¯¼å¹¶è¯†åˆ«å¯èƒ½ç»•è¿‡æ¨¡å‹å†…ç½®å®‰å…¨æœºåˆ¶çš„ä¸é“å¾·å“åº”ã€‚GUARD æœ€ç»ˆä¼šç”Ÿæˆè¯¦ç»†çš„åˆè§„æ€§æŠ¥å‘Šï¼Œæ˜ç¡®æ¨¡å‹å¯¹å‡†åˆ™çš„éµå¾ªç¨‹åº¦åŠå­˜åœ¨çš„è¿è§„è¡Œä¸ºã€‚ç ”ç©¶äººå‘˜åœ¨ Vicuna-13Bã€Llama-3-8Bã€GPT-4o åŠ Claude-3.7 ç­‰ä¸ƒç§ä¸»æµæ¨¡å‹ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGUARD ä¸ä»…èƒ½å‡†ç¡®è¯„ä¼°ä¸‰é¡¹æ”¿åºœå‡†åˆ™çš„æ‰§è¡Œæƒ…å†µï¼Œå…¶è¶Šç‹±è¯Šæ–­åŠŸèƒ½è¿˜å¯è¿ç§»è‡³è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models)ï¼Œä¸ºå¼€å‘å¯ä¿¡ã€å¯é çš„ AI åº”ç”¨æä¾›äº†å…³é”®çš„æŠ€æœ¯ä¿éšœã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "54 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.20325v2",
      "published_date": "2025-08-28 00:07:10 UTC",
      "updated_date": "2025-11-07 20:24:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:00:25.761218+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 146,
  "processed_papers_count": 146,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T15:01:09.991922+00:00"
}
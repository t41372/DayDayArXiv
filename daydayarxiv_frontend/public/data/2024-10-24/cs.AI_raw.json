[
  {
    "arxiv_id": "2410.19217v1",
    "title": "No Free Lunch: Fundamental Limits of Learning Non-Hallucinating Generative Models",
    "authors": [
      "Changlong Wu",
      "Ananth Grama",
      "Wojciech Szpankowski"
    ],
    "abstract": "Generative models have shown impressive capabilities in synthesizing\nhigh-quality outputs across various domains. However, a persistent challenge is\nthe occurrence of \"hallucinations\", where the model produces outputs that are\nplausible but invalid. While empirical strategies have been explored to\nmitigate this issue, a rigorous theoretical understanding remains elusive. In\nthis paper, we develop a theoretical framework to analyze the learnability of\nnon-hallucinating generative models from a learning-theoretic perspective. Our\nresults reveal that non-hallucinating learning is statistically impossible when\nrelying solely on the training dataset, even for a hypothesis class of size two\nand when the entire training set is truthful. To overcome these limitations, we\nshow that incorporating inductive biases aligned with the actual facts into the\nlearning process is essential. We provide a systematic approach to achieve this\nby restricting the facts set to a concept class of finite VC-dimension and\ndemonstrate its effectiveness under various learning paradigms. Although our\nfindings are primarily conceptual, they represent a first step towards a\nprincipled approach to addressing hallucinations in learning generative models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.19217v1",
    "published_date": "2024-10-24 23:57:11 UTC",
    "updated_date": "2024-10-24 23:57:11 UTC"
  },
  {
    "arxiv_id": "2410.19207v2",
    "title": "Equitable Federated Learning with Activation Clustering",
    "authors": [
      "Antesh Upadhyay",
      "Abolfazl Hashemi"
    ],
    "abstract": "Federated learning is a prominent distributed learning paradigm that\nincorporates collaboration among diverse clients, promotes data locality, and\nthus ensures privacy. These clients have their own technological, cultural, and\nother biases in the process of data generation. However, the present standard\noften ignores this bias/heterogeneity, perpetuating bias against certain groups\nrather than mitigating it. In response to this concern, we propose an equitable\nclustering-based framework where the clients are categorized/clustered based on\nhow similar they are to each other. We propose a unique way to construct the\nsimilarity matrix that uses activation vectors. Furthermore, we propose a\nclient weighing mechanism to ensure that each cluster receives equal importance\nand establish $O(1/\\sqrt{K})$ rate of convergence to reach an\n$\\epsilon-$stationary solution. We assess the effectiveness of our proposed\nstrategy against common baselines, demonstrating its efficacy in terms of\nreducing the bias existing amongst various client clusters and consequently\nameliorating algorithmic bias against specific groups.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.19207v2",
    "published_date": "2024-10-24 23:36:39 UTC",
    "updated_date": "2024-11-01 04:14:52 UTC"
  },
  {
    "arxiv_id": "2410.19203v1",
    "title": "An Inverse Modeling Constrained Multi-Objective Evolutionary Algorithm Based on Decomposition",
    "authors": [
      "Lucas R. C. Farias",
      "Aluizio F. R. Araújo"
    ],
    "abstract": "This paper introduces the inverse modeling constrained multi-objective\nevolutionary algorithm based on decomposition (IM-C-MOEA/D) for addressing\nconstrained real-world optimization problems. Our research builds upon the\nadvancements made in evolutionary computing-based inverse modeling, and it\nstrategically bridges the gaps in applying inverse models based on\ndecomposition to problem domains with constraints. The proposed approach is\nexperimentally evaluated on diverse real-world problems (RWMOP1-35), showing\nsuperior performance to state-of-the-art constrained multi-objective\nevolutionary algorithms (CMOEAs). The experimental results highlight the\nrobustness of the algorithm and its applicability in real-world constrained\noptimization scenarios.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "6 pages, 1 figure, 1 algorithm, and 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.19203v1",
    "published_date": "2024-10-24 23:24:44 UTC",
    "updated_date": "2024-10-24 23:24:44 UTC"
  },
  {
    "arxiv_id": "2410.19198v1",
    "title": "MAP: Multi-Human-Value Alignment Palette",
    "authors": [
      "Xinran Wang",
      "Qi Le",
      "Ammar Ahmed",
      "Enmao Diao",
      "Yi Zhou",
      "Nathalie Baracaldo",
      "Jie Ding",
      "Ali Anwar"
    ],
    "abstract": "Ensuring that generative AI systems align with human values is essential but\nchallenging, especially when considering multiple human values and their\npotential trade-offs. Since human values can be personalized and dynamically\nchange over time, the desirable levels of value alignment vary across different\nethnic groups, industry sectors, and user cohorts. Within existing frameworks,\nit is hard to define human values and align AI systems accordingly across\ndifferent directions simultaneously, such as harmlessness, helpfulness, and\npositiveness. To address this, we develop a novel, first-principle approach\ncalled Multi-Human-Value Alignment Palette (MAP), which navigates the alignment\nacross multiple human values in a structured and reliable way. MAP formulates\nthe alignment problem as an optimization task with user-defined constraints,\nwhich define human value targets. It can be efficiently solved via a\nprimal-dual approach, which determines whether a user-defined alignment target\nis achievable and how to achieve it. We conduct a detailed theoretical analysis\nof MAP by quantifying the trade-offs between values, the sensitivity to\nconstraints, the fundamental connection between multi-value alignment and\nsequential alignment, and proving that linear weighted rewards are sufficient\nfor multi-value alignment. Extensive experiments demonstrate MAP's ability to\nalign multiple values in a principled manner while delivering strong empirical\nperformance across various tasks.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.ET",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.19198v1",
    "published_date": "2024-10-24 23:16:39 UTC",
    "updated_date": "2024-10-24 23:16:39 UTC"
  },
  {
    "arxiv_id": "2410.19193v2",
    "title": "Enriching GNNs with Text Contextual Representations for Detecting Disinformation Campaigns on Social Media",
    "authors": [
      "Bruno Croso Cunha da Silva",
      "Thomas Palmeira Ferraz",
      "Roseli De Deus Lopes"
    ],
    "abstract": "Disinformation on social media poses both societal and technical challenges,\nrequiring robust detection systems. While previous studies have integrated\ntextual information into propagation networks, they have yet to fully leverage\nthe advancements in Transformer-based language models for high-quality\ncontextual text representations. This work addresses this gap by incorporating\nTransformer-based textual features into Graph Neural Networks (GNNs) for fake\nnews detection. We demonstrate that contextual text representations enhance GNN\nperformance, achieving 33.8% relative improvement in Macro F1 over models\nwithout textual features and 9.3% over static text representations. We further\ninvestigate the impact of different feature sources and the effects of noisy\ndata augmentation. We expect our methodology to open avenues for further\nresearch, and we made code publicly available.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SI",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "comment": "Work still in progress. Accepted as Extended Abstract Poster at LoG\n  Conference 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.19193v2",
    "published_date": "2024-10-24 22:57:17 UTC",
    "updated_date": "2024-11-23 03:48:58 UTC"
  },
  {
    "arxiv_id": "2410.19185v2",
    "title": "Tailored-LLaMA: Optimizing Few-Shot Learning in Pruned LLaMA Models with Task-Specific Prompts",
    "authors": [
      "Danyal Aftab",
      "Steven Davy"
    ],
    "abstract": "Large language models demonstrate impressive proficiency in language\nunderstanding and generation. Nonetheless, training these models from scratch,\neven the least complex billion-parameter variant demands significant\ncomputational resources rendering it economically impractical for many\norganizations. With large language models functioning as general-purpose task\nsolvers, this paper investigates their task-specific fine-tuning. We employ\ntask-specific datasets and prompts to fine-tune two pruned LLaMA models having\n5 billion and 4 billion parameters. This process utilizes the pre-trained\nweights and focuses on a subset of weights using the LoRA method. One challenge\nin fine-tuning the LLaMA model is crafting a precise prompt tailored to the\nspecific task. To address this, we propose a novel approach to fine-tune the\nLLaMA model under two primary constraints: task specificity and prompt\neffectiveness. Our approach, Tailored LLaMA initially employs structural\npruning to reduce the model sizes from 7B to 5B and 4B parameters.\nSubsequently, it applies a carefully designed prompt specific to the task and\nutilizes the LoRA method to accelerate the fine-tuning process. Moreover,\nfine-tuning a model pruned by 50\\% for less than one hour restores the mean\naccuracy of classification tasks to 95.68\\% at a 20\\% compression ratio and to\n86.54\\% at a 50\\% compression ratio through few-shot learning with 50 shots.\nOur validation of Tailored LLaMA on these two pruned variants demonstrates that\neven when compressed to 50\\%, the models maintain over 65\\% of the baseline\nmodel accuracy in few-shot classification and generation tasks. These findings\nhighlight the efficacy of our tailored approach in maintaining high performance\nwith significantly reduced model sizes.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.19185v2",
    "published_date": "2024-10-24 22:34:27 UTC",
    "updated_date": "2025-01-09 17:29:40 UTC"
  },
  {
    "arxiv_id": "2410.19184v2",
    "title": "No Argument Left Behind: Overlapping Chunks for Faster Processing of Arbitrarily Long Legal Texts",
    "authors": [
      "Israel Fama",
      "Bárbara Bueno",
      "Alexandre Alcoforado",
      "Thomas Palmeira Ferraz",
      "Arnold Moya",
      "Anna Helena Reali Costa"
    ],
    "abstract": "In a context where the Brazilian judiciary system, the largest in the world,\nfaces a crisis due to the slow processing of millions of cases, it becomes\nimperative to develop efficient methods for analyzing legal texts. We introduce\nuBERT, a hybrid model that combines Transformer and Recurrent Neural Network\narchitectures to effectively handle long legal texts. Our approach processes\nthe full text regardless of its length while maintaining reasonable\ncomputational overhead. Our experiments demonstrate that uBERT achieves\nsuperior performance compared to BERT+LSTM when overlapping input is used and\nis significantly faster than ULMFiT for processing long legal documents.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Presented at 15th Symposium in Information and Human Language\n  Technology (STIL) @ BRACIS'24",
    "pdf_url": "http://arxiv.org/pdf/2410.19184v2",
    "published_date": "2024-10-24 22:33:30 UTC",
    "updated_date": "2024-12-15 23:36:13 UTC"
  },
  {
    "arxiv_id": "2410.19183v1",
    "title": "Can Self Supervision Rejuvenate Similarity-Based Link Prediction?",
    "authors": [
      "Chenhan Zhang",
      "Weiqi Wang",
      "Zhiyi Tian",
      "James Jianqiao Yu",
      "Mohamed Ali Kaafar",
      "An Liu",
      "Shui Yu"
    ],
    "abstract": "Although recent advancements in end-to-end learning-based link prediction\n(LP) methods have shown remarkable capabilities, the significance of\ntraditional similarity-based LP methods persists in unsupervised scenarios\nwhere there are no known link labels. However, the selection of node features\nfor similarity computation in similarity-based LP can be challenging. Less\ninformative node features can result in suboptimal LP performance. To address\nthese challenges, we integrate self-supervised graph learning techniques into\nsimilarity-based LP and propose a novel method: Self-Supervised\nSimilarity-based LP (3SLP). 3SLP is suitable for the unsupervised condition of\nsimilarity-based LP without the assistance of known link labels. Specifically,\n3SLP introduces a dual-view contrastive node representation learning (DCNRL)\nwith crafted data augmentation and node representation learning. DCNRL is\ndedicated to developing more informative node representations, replacing the\nnode attributes as inputs in the similarity-based LP backbone. Extensive\nexperiments over benchmark datasets demonstrate the salient improvement of\n3SLP, outperforming the baseline of traditional similarity-based LP by up to\n21.2% (AUC).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.19183v1",
    "published_date": "2024-10-24 22:31:12 UTC",
    "updated_date": "2024-10-24 22:31:12 UTC"
  },
  {
    "arxiv_id": "2410.19168v1",
    "title": "MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark",
    "authors": [
      "S Sakshi",
      "Utkarsh Tyagi",
      "Sonal Kumar",
      "Ashish Seth",
      "Ramaneswaran Selvakumar",
      "Oriol Nieto",
      "Ramani Duraiswami",
      "Sreyan Ghosh",
      "Dinesh Manocha"
    ],
    "abstract": "The ability to comprehend audio--which includes speech, non-speech sounds,\nand music--is crucial for AI agents to interact effectively with the world. We\npresent MMAU, a novel benchmark designed to evaluate multimodal audio\nunderstanding models on tasks requiring expert-level knowledge and complex\nreasoning. MMAU comprises 10k carefully curated audio clips paired with\nhuman-annotated natural language questions and answers spanning speech,\nenvironmental sounds, and music. It includes information extraction and\nreasoning questions, requiring models to demonstrate 27 distinct skills across\nunique and challenging tasks. Unlike existing benchmarks, MMAU emphasizes\nadvanced perception and reasoning with domain-specific knowledge, challenging\nmodels to tackle tasks akin to those faced by experts. We assess 18 open-source\nand proprietary (Large) Audio-Language Models, demonstrating the significant\nchallenges posed by MMAU. Notably, even the most advanced Gemini Pro v1.5\nachieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio\nachieves only 52.50%, highlighting considerable room for improvement. We\nbelieve MMAU will drive the audio and multimodal research community to develop\nmore advanced audio understanding models capable of solving complex audio\ntasks.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Project Website: https://sakshi113.github.io/mmau_homepage/",
    "pdf_url": "http://arxiv.org/pdf/2410.19168v1",
    "published_date": "2024-10-24 21:20:10 UTC",
    "updated_date": "2024-10-24 21:20:10 UTC"
  },
  {
    "arxiv_id": "2410.19160v1",
    "title": "Adversarial Attacks on Large Language Models Using Regularized Relaxation",
    "authors": [
      "Samuel Jacob Chacko",
      "Sajib Biswas",
      "Chashi Mahiul Islam",
      "Fatema Tabassum Liza",
      "Xiuwen Liu"
    ],
    "abstract": "As powerful Large Language Models (LLMs) are now widely used for numerous\npractical applications, their safety is of critical importance. While alignment\ntechniques have significantly improved overall safety, LLMs remain vulnerable\nto carefully crafted adversarial inputs. Consequently, adversarial attack\nmethods are extensively used to study and understand these vulnerabilities.\nHowever, current attack methods face significant limitations. Those relying on\noptimizing discrete tokens suffer from limited efficiency, while continuous\noptimization techniques fail to generate valid tokens from the model's\nvocabulary, rendering them impractical for real-world applications. In this\npaper, we propose a novel technique for adversarial attacks that overcomes\nthese limitations by leveraging regularized gradients with continuous\noptimization methods. Our approach is two orders of magnitude faster than the\nstate-of-the-art greedy coordinate gradient-based method, significantly\nimproving the attack success rate on aligned language models. Moreover, it\ngenerates valid tokens, addressing a fundamental limitation of existing\ncontinuous optimization methods. We demonstrate the effectiveness of our attack\non five state-of-the-art LLMs using four datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "I.2.7"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.19160v1",
    "published_date": "2024-10-24 21:01:45 UTC",
    "updated_date": "2024-10-24 21:01:45 UTC"
  },
  {
    "arxiv_id": "2410.19155v3",
    "title": "Lived Experience Not Found: LLMs Struggle to Align with Experts on Addressing Adverse Drug Reactions from Psychiatric Medication Use",
    "authors": [
      "Mohit Chandra",
      "Siddharth Sriraman",
      "Gaurav Verma",
      "Harneet Singh Khanuja",
      "Jose Suarez Campayo",
      "Zihang Li",
      "Michael L. Birnbaum",
      "Munmun De Choudhury"
    ],
    "abstract": "Adverse Drug Reactions (ADRs) from psychiatric medications are the leading\ncause of hospitalizations among mental health patients. With healthcare systems\nand online communities facing limitations in resolving ADR-related issues,\nLarge Language Models (LLMs) have the potential to fill this gap. Despite the\nincreasing capabilities of LLMs, past research has not explored their\ncapabilities in detecting ADRs related to psychiatric medications or in\nproviding effective harm reduction strategies. To address this, we introduce\nthe Psych-ADR benchmark and the Adverse Drug Reaction Response Assessment\n(ADRA) framework to systematically evaluate LLM performance in detecting ADR\nexpressions and delivering expert-aligned mitigation strategies. Our analyses\nshow that LLMs struggle with understanding the nuances of ADRs and\ndifferentiating between types of ADRs. While LLMs align with experts in terms\nof expressed emotions and tone of the text, their responses are more complex,\nharder to read, and only 70.86% aligned with expert strategies. Furthermore,\nthey provide less actionable advice by a margin of 12.32% on average. Our work\nprovides a comprehensive benchmark and evaluation framework for assessing LLMs\nin strategy-driven tasks within high-risk domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "30 pages, 8 figures, 16 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.19155v3",
    "published_date": "2024-10-24 20:49:22 UTC",
    "updated_date": "2025-01-07 15:30:02 UTC"
  },
  {
    "arxiv_id": "2410.19144v1",
    "title": "Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant",
    "authors": [
      "Abhirama Subramanyam Penamakuri",
      "Anand Mishra"
    ],
    "abstract": "We revisit knowledge-aware text-based visual question answering, also known\nas Text-KVQA, in the light of modern advancements in large multimodal models\n(LMMs), and make the following contributions: (i) We propose VisTEL - a\nprincipled approach to perform visual text entity linking. The proposed VisTEL\nmodule harnesses a state-of-the-art visual text recognition engine and the\npower of a large multimodal model to jointly reason using textual and visual\ncontext obtained using surrounding cues in the image to link the visual text\nentity to the correct knowledge base entity. (ii) We present KaLMA - a\nknowledge-aware large multimodal assistant that augments an LMM with knowledge\nassociated with visual text entity in the image to arrive at an accurate\nanswer. Further, we provide a comprehensive experimental analysis and\ncomparison of our approach with traditional visual question answering,\npre-large multimodal models, and large multimodal models, as well as prior\ntop-performing approaches. Averaging over three splits of Text-KVQA, our\nproposed approach surpasses the previous best approach by a substantial 23.3%\non an absolute scale and establishes a new state of the art. We make our\nimplementation publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to EMNLP (Main) 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.19144v1",
    "published_date": "2024-10-24 20:25:38 UTC",
    "updated_date": "2024-10-24 20:25:38 UTC"
  },
  {
    "arxiv_id": "2410.19135v1",
    "title": "PDL: A Declarative Prompt Programming Language",
    "authors": [
      "Mandana Vaziri",
      "Louis Mandel",
      "Claudio Spiess",
      "Martin Hirzel"
    ],
    "abstract": "Large language models (LLMs) have taken the world by storm by making many\npreviously difficult uses of AI feasible. LLMs are controlled via highly\nexpressive textual prompts and return textual answers. Unfortunately, this\nunstructured text as input and output makes LLM-based applications brittle.\nThis motivates the rise of prompting frameworks, which mediate between LLMs and\nthe external world. However, existing prompting frameworks either have a high\nlearning curve or take away control over the exact prompts from the developer.\nTo overcome this dilemma, this paper introduces the Prompt Declaration Language\n(PDL). PDL is a simple declarative data-oriented language that puts prompts at\nthe forefront, based on YAML. PDL works well with many LLM platforms and LLMs.\nIt supports writing interactive applications that call LLMs and tools, and\nmakes it easy to implement common use-cases such as chatbots, RAG, or agents.\nWe hope PDL will make prompt programming simpler, less brittle, and more\nenjoyable.",
    "categories": [
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.19135v1",
    "published_date": "2024-10-24 20:07:08 UTC",
    "updated_date": "2024-10-24 20:07:08 UTC"
  },
  {
    "arxiv_id": "2410.19130v2",
    "title": "Research on Key Technologies for Cross-Cloud Federated Training of Large Language Models",
    "authors": [
      "Haowei Yang",
      "Mingxiu Sui",
      "Shaobo Liu",
      "Xinyue Qian",
      "Zhaoyang Zhang",
      "Bingying Liu"
    ],
    "abstract": "With the rapid development of natural language processing technology, large\nlanguage models have demonstrated exceptional performance in various\napplication scenarios. However, training these models requires significant\ncomputational resources and data processing capabilities. Cross-cloud federated\ntraining offers a new approach to addressing the resource bottlenecks of a\nsingle cloud platform, allowing the computational resources of multiple clouds\nto collaboratively complete the training tasks of large models. This study\nanalyzes the key technologies of cross-cloud federated training, including data\npartitioning and distribution, communication optimization, model aggregation\nalgorithms, and the compatibility of heterogeneous cloud platforms.\nAdditionally, the study examines data security and privacy protection\nstrategies in cross-cloud training, particularly the application of data\nencryption and differential privacy techniques. Through experimental\nvalidation, the proposed technical framework demonstrates enhanced training\nefficiency, ensured data security, and reduced training costs, highlighting the\nbroad application prospects of cross-cloud federated training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.19130v2",
    "published_date": "2024-10-24 19:57:17 UTC",
    "updated_date": "2024-12-23 03:13:43 UTC"
  },
  {
    "arxiv_id": "2410.19110v3",
    "title": "Bio2Token: All-atom tokenization of any biomolecular structure with Mamba",
    "authors": [
      "Andrew Liu",
      "Axel Elaldi",
      "Nathan Russell",
      "Olivia Viessmann"
    ],
    "abstract": "Efficient encoding and representation of large 3D molecular structures with\nhigh fidelity is critical for biomolecular design applications. Despite this,\nmany representation learning approaches restrict themselves to modeling smaller\nsystems or use coarse-grained approximations of the systems, for example\nmodeling proteins at the resolution of amino acid residues rather than at the\nlevel of individual atoms. To address this, we develop quantized auto-encoders\nthat learn atom-level tokenizations of complete proteins, RNA and small\nmolecule structures with reconstruction accuracies well below 1 Angstrom. We\ndemonstrate that a simple Mamba state space model architecture is efficient\ncompared to an SE(3)-invariant IPA architecture, reaches competitive accuracies\nand can scale to systems with almost 100,000 atoms. The learned structure\ntokens of bio2token may serve as the input for all-atom generative models in\nthe future.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.19110v3",
    "published_date": "2024-10-24 19:23:09 UTC",
    "updated_date": "2025-04-08 23:59:56 UTC"
  },
  {
    "arxiv_id": "2410.19109v1",
    "title": "RSA-Control: A Pragmatics-Grounded Lightweight Controllable Text Generation Framework",
    "authors": [
      "Yifan Wang",
      "Vera Demberg"
    ],
    "abstract": "Despite significant advancements in natural language generation, controlling\nlanguage models to produce texts with desired attributes remains a formidable\nchallenge. In this work, we introduce RSA-Control, a training-free controllable\ntext generation framework grounded in pragmatics. RSA-Control directs the\ngeneration process by recursively reasoning between imaginary speakers and\nlisteners, enhancing the likelihood that target attributes are correctly\ninterpreted by listeners amidst distractors. Additionally, we introduce a\nself-adjustable rationality parameter, which allows for automatic adjustment of\ncontrol strength based on context. Our experiments, conducted with two task\ntypes and two types of language models, demonstrate that RSA-Control achieves\nstrong attribute control while maintaining language fluency and content\nconsistency. Our code is available at https://github.com/Ewanwong/RSA-Control.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to EMNLP 2024 (main conference)",
    "pdf_url": "http://arxiv.org/pdf/2410.19109v1",
    "published_date": "2024-10-24 19:21:04 UTC",
    "updated_date": "2024-10-24 19:21:04 UTC"
  },
  {
    "arxiv_id": "2410.19105v3",
    "title": "Conditional diffusions for amortized neural posterior estimation",
    "authors": [
      "Tianyu Chen",
      "Vansh Bansal",
      "James G. Scott"
    ],
    "abstract": "Neural posterior estimation (NPE), a simulation-based computational approach\nfor Bayesian inference, has shown great success in approximating complex\nposterior distributions. Existing NPE methods typically rely on normalizing\nflows, which approximate a distribution by composing many simple, invertible\ntransformations. But flow-based models, while state of the art for NPE, are\nknown to suffer from several limitations, including training instability and\nsharp trade-offs between representational power and computational cost. In this\nwork, we demonstrate the effectiveness of conditional diffusions coupled with\nhigh-capacity summary networks for amortized NPE. Conditional diffusions\naddress many of the challenges faced by flow-based methods. Our results show\nthat, across a highly varied suite of benchmarking problems for NPE\narchitectures, diffusions offer improved stability, superior accuracy, and\nfaster training times, even with simpler, shallower models. Building on prior\nwork on diffusions for NPE, we show that these gains persist across a variety\nof different summary network architectures. Code is available at\nhttps://github.com/TianyuCodings/cDiff.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.19105v3",
    "published_date": "2024-10-24 19:13:13 UTC",
    "updated_date": "2025-03-13 02:16:15 UTC"
  },
  {
    "arxiv_id": "2410.19100v3",
    "title": "VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks",
    "authors": [
      "Lawrence Jang",
      "Yinheng Li",
      "Dan Zhao",
      "Charles Ding",
      "Justin Lin",
      "Paul Pu Liang",
      "Rogerio Bonatti",
      "Kazuhito Koishida"
    ],
    "abstract": "Videos are often used to learn or extract the necessary information to\ncomplete tasks in ways different than what text and static imagery alone can\nprovide. However, many existing agent benchmarks neglect long-context video\nunderstanding, instead focusing on text or static image inputs. To bridge this\ngap, we introduce VideoWebArena (VideoWA), a benchmark for evaluating the\ncapabilities of long-context multimodal agents for video understanding. VideoWA\nconsists of 2,021 web agent tasks based on manually crafted video tutorials,\nwhich total almost four hours of content. For our benchmark, we define a\ntaxonomy of long-context video-based agent tasks with two main areas of focus:\nskill retention and factual retention. While skill retention tasks evaluate\nwhether an agent can use a given human demonstration to complete a task\nefficiently, the factual retention task evaluates whether an agent can retrieve\ninstruction-relevant information from a video to complete a task. We find that\nthe best model achieves 13.3% success on factual retention tasks and 45.8% on\nfactual retention QA pairs, far below human performance at 73.9% and 79.3%,\nrespectively. On skill retention tasks, long-context models perform worse with\ntutorials than without, exhibiting a 5% performance decrease in WebArena tasks\nand a 10.3% decrease in VisualWebArena tasks. Our work highlights the need to\nimprove the agentic abilities of long-context multimodal models and provides a\ntestbed for future development with long-context video agents.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.19100v3",
    "published_date": "2024-10-24 19:03:01 UTC",
    "updated_date": "2025-02-15 05:19:38 UTC"
  },
  {
    "arxiv_id": "2410.19085v1",
    "title": "A Counterexample in Cross-Correlation Template Matching",
    "authors": [
      "Serap A. Savari"
    ],
    "abstract": "Sampling and quantization are standard practices in signal and image\nprocessing, but a theoretical understanding of their impact is incomplete. We\nconsider discrete image registration when the underlying function is a\none-dimensional spatially-limited piecewise constant function. For ideal\nnoiseless sampling the number of samples from each region of the support of the\nfunction generally depends on the placement of the sampling grid. Therefore, if\nthe samples of the function are noisy, then image registration requires\nalignment and segmentation of the data sequences. One popular strategy for\naligning images is selecting the maximum from cross-correlation template\nmatching. To motivate more robust and accurate approaches which also address\nsegmentation, we provide an example of a one-dimensional spatially-limited\npiecewise constant function for which the cross-correlation technique can\nperform poorly on noisy samples. While earlier approaches to improve the method\ninvolve normalization, our example suggests a novel strategy in our setting.\nDifference sequences, thresholding, and dynamic programming are well-known\ntechniques in image processing. We prove that they are tools to correctly align\nand segment noisy data sequences under some conditions on the noise. We also\naddress some of the potential difficulties that could arise in a more general\ncase.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.19085v1",
    "published_date": "2024-10-24 18:42:01 UTC",
    "updated_date": "2024-10-24 18:42:01 UTC"
  },
  {
    "arxiv_id": "2410.19064v1",
    "title": "From a Tiny Slip to a Giant Leap: An LLM-Based Simulation for Fake News Evolution",
    "authors": [
      "Yuhan Liu",
      "Zirui Song",
      "Xiaoqing Zhang",
      "Xiuying Chen",
      "Rui Yan"
    ],
    "abstract": "With the growing spread of misinformation online, research has increasingly\nfocused on detecting and tracking fake news. However, an overlooked issue is\nthat fake news does not naturally exist in social networks -- it often\noriginates from distorted facts or deliberate fabrication by malicious actors.\nUnderstanding how true news gradually evolves into fake news is critical for\nearly detection and prevention, reducing its spread and impact. Hence, in this\npaper, we take the first step toward simulating and revealing this evolution,\nproposing a Fake News evolUtion Simulation framEwork (FUSE) based on large\nlanguage models (LLMs). Specifically, we employ LLM as agents to represent\nindividuals in a simulated social network. We define four types of agents\ncommonly observed in daily interactions: spreaders, who propagate information;\ncommentators, who provide opinions and interpretations; verifiers, who check\nthe accuracy of information; and bystanders, who passively observe without\nengaging. For simulated environments, we model various social network\nstructures, such as high-clustering networks and scale-free networks, to mirror\nreal-world network dynamics. Each day, the agents engage in belief exchanges,\nreflect on their thought processes, and reintroduce the news accordingly. Given\nthe lack of prior work in this area, we developed a FUSE-EVAL evaluation\nframework to measure the deviation from true news during the fake news\nevolution process. The results show that FUSE successfully captures the\nunderlying patterns of how true news transforms into fake news and accurately\nreproduces previously discovered instances of fake news, aligning closely with\nhuman evaluations. Moreover, our work provides insights into the fact that\ncombating fake news should not be delayed until it has fully evolved; instead,\nprevention in advance is key to achieving better outcomes.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.19064v1",
    "published_date": "2024-10-24 18:17:16 UTC",
    "updated_date": "2024-10-24 18:17:16 UTC"
  },
  {
    "arxiv_id": "2410.19056v1",
    "title": "ReasonAgain: Using Extractable Symbolic Programs to Evaluate Mathematical Reasoning",
    "authors": [
      "Xiaodong Yu",
      "Ben Zhou",
      "Hao Cheng",
      "Dan Roth"
    ],
    "abstract": "Existing math datasets evaluate the reasoning abilities of large language\nmodels (LLMs) by either using the final answer or the intermediate reasoning\nsteps derived from static examples. However, the former approach fails to\nsurface model's uses of shortcuts and wrong reasoning while the later poses\nchallenges in accommodating alternative solutions. In this work, we seek to use\nsymbolic programs as a means for automated evaluation if a model can\nconsistently produce correct final answers across various inputs to the\nprogram. We begin by extracting programs for popular math datasets (GSM8K and\nMATH) using GPT4-o. For those executable programs verified using the original\ninput-output pairs, they are found to encapsulate the proper reasoning required\nto solve the original text questions. We then prompt GPT4-o to generate new\nquestions using alternative input-output pairs based the extracted program. We\napply the resulting datasets to evaluate a collection of LLMs. In our\nexperiments, we observe significant accuracy drops using our proposed\nevaluation compared with original static examples, suggesting the fragility of\nmath reasoning in state-of-the-art LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.19056v1",
    "published_date": "2024-10-24 18:02:37 UTC",
    "updated_date": "2024-10-24 18:02:37 UTC"
  },
  {
    "arxiv_id": "2410.19054v1",
    "title": "Infogent: An Agent-Based Framework for Web Information Aggregation",
    "authors": [
      "Revanth Gangi Reddy",
      "Sagnik Mukherjee",
      "Jeonghwan Kim",
      "Zhenhailong Wang",
      "Dilek Hakkani-Tur",
      "Heng Ji"
    ],
    "abstract": "Despite seemingly performant web agents on the task-completion benchmarks,\nmost existing methods evaluate the agents based on a presupposition: the web\nnavigation task consists of linear sequence of actions with an end state that\nmarks task completion. In contrast, our work focuses on web navigation for\ninformation aggregation, wherein the agent must explore different websites to\ngather information for a complex query. We consider web information aggregation\nfrom two different perspectives: (i) Direct API-driven Access relies on a\ntext-only view of the Web, leveraging external tools such as Google Search API\nto navigate the web and a scraper to extract website contents. (ii) Interactive\nVisual Access uses screenshots of the webpages and requires interaction with\nthe browser to navigate and access information. Motivated by these diverse\ninformation access settings, we introduce Infogent, a novel modular framework\nfor web information aggregation involving three distinct components: Navigator,\nExtractor and Aggregator. Experiments on different information access settings\ndemonstrate Infogent beats an existing SOTA multi-agent search framework by 7%\nunder Direct API-Driven Access on FRAMES, and improves over an existing\ninformation-seeking web agent by 4.3% under Interactive Visual Access on\nAssistantBench.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2410.19054v1",
    "published_date": "2024-10-24 18:01:28 UTC",
    "updated_date": "2024-10-24 18:01:28 UTC"
  },
  {
    "arxiv_id": "2410.18979v1",
    "title": "PixelGaussian: Generalizable 3D Gaussian Reconstruction from Arbitrary Views",
    "authors": [
      "Xin Fei",
      "Wenzhao Zheng",
      "Yueqi Duan",
      "Wei Zhan",
      "Masayoshi Tomizuka",
      "Kurt Keutzer",
      "Jiwen Lu"
    ],
    "abstract": "We propose PixelGaussian, an efficient feed-forward framework for learning\ngeneralizable 3D Gaussian reconstruction from arbitrary views. Most existing\nmethods rely on uniform pixel-wise Gaussian representations, which learn a\nfixed number of 3D Gaussians for each view and cannot generalize well to more\ninput views. Differently, our PixelGaussian dynamically adapts both the\nGaussian distribution and quantity based on geometric complexity, leading to\nmore efficient representations and significant improvements in reconstruction\nquality. Specifically, we introduce a Cascade Gaussian Adapter to adjust\nGaussian distribution according to local geometry complexity identified by a\nkeypoint scorer. CGA leverages deformable attention in context-aware\nhypernetworks to guide Gaussian pruning and splitting, ensuring accurate\nrepresentation in complex regions while reducing redundancy. Furthermore, we\ndesign a transformer-based Iterative Gaussian Refiner module that refines\nGaussian representations through direct image-Gaussian interactions. Our\nPixelGaussian can effectively reduce Gaussian redundancy as input views\nincrease. We conduct extensive experiments on the large-scale ACID and\nRealEstate10K datasets, where our method achieves state-of-the-art performance\nwith good generalization to various numbers of views. Code:\nhttps://github.com/Barrybarry-Smith/PixelGaussian.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Code is available at:\n  https://github.com/Barrybarry-Smith/PixelGaussian",
    "pdf_url": "http://arxiv.org/pdf/2410.18979v1",
    "published_date": "2024-10-24 17:59:58 UTC",
    "updated_date": "2024-10-24 17:59:58 UTC"
  },
  {
    "arxiv_id": "2410.18976v1",
    "title": "CAMEL-Bench: A Comprehensive Arabic LMM Benchmark",
    "authors": [
      "Sara Ghaboura",
      "Ahmed Heakl",
      "Omkar Thawakar",
      "Ali Alharthi",
      "Ines Riahi",
      "Abduljalil Saif",
      "Jorma Laaksonen",
      "Fahad S. Khan",
      "Salman Khan",
      "Rao M. Anwer"
    ],
    "abstract": "Recent years have witnessed a significant interest in developing large\nmultimodal models (LMMs) capable of performing various visual reasoning and\nunderstanding tasks. This has led to the introduction of multiple LMM\nbenchmarks to evaluate LMMs on different tasks. However, most existing LMM\nevaluation benchmarks are predominantly English-centric. In this work, we\ndevelop a comprehensive LMM evaluation benchmark for the Arabic language to\nrepresent a large population of over 400 million speakers. The proposed\nbenchmark, named CAMEL-Bench, comprises eight diverse domains and 38\nsub-domains including, multi-image understanding, complex visual perception,\nhandwritten document understanding, video understanding, medical imaging, plant\ndiseases, and remote sensing-based land use understanding to evaluate broad\nscenario generalizability. Our CAMEL-Bench comprises around 29,036 questions\nthat are filtered from a larger pool of samples, where the quality is manually\nverified by native speakers to ensure reliable model assessment. We conduct\nevaluations of both closed-source, including GPT-4 series, and open-source\nLMMs. Our analysis reveals the need for substantial improvement, especially\namong the best open-source models, with even the closed-source GPT-4o achieving\nan overall score of 62%. Our benchmark and evaluation scripts are open-sourced.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 5 figures, NAACL",
    "pdf_url": "http://arxiv.org/pdf/2410.18976v1",
    "published_date": "2024-10-24 17:59:38 UTC",
    "updated_date": "2024-10-24 17:59:38 UTC"
  },
  {
    "arxiv_id": "2410.18975v2",
    "title": "Unbounded: A Generative Infinite Game of Character Life Simulation",
    "authors": [
      "Jialu Li",
      "Yuanzhen Li",
      "Neal Wadhwa",
      "Yael Pritch",
      "David E. Jacobs",
      "Michael Rubinstein",
      "Mohit Bansal",
      "Nataniel Ruiz"
    ],
    "abstract": "We introduce the concept of a generative infinite game, a video game that\ntranscends the traditional boundaries of finite, hard-coded systems by using\ngenerative models. Inspired by James P. Carse's distinction between finite and\ninfinite games, we leverage recent advances in generative AI to create\nUnbounded: a game of character life simulation that is fully encapsulated in\ngenerative models. Specifically, Unbounded draws inspiration from sandbox life\nsimulations and allows you to interact with your autonomous virtual character\nin a virtual world by feeding, playing with and guiding it - with open-ended\nmechanics generated by an LLM, some of which can be emergent. In order to\ndevelop Unbounded, we propose technical innovations in both the LLM and visual\ngeneration domains. Specifically, we present: (1) a specialized, distilled\nlarge language model (LLM) that dynamically generates game mechanics,\nnarratives, and character interactions in real-time, and (2) a new dynamic\nregional image prompt Adapter (IP-Adapter) for vision models that ensures\nconsistent yet flexible visual generation of a character across multiple\nenvironments. We evaluate our system through both qualitative and quantitative\nanalysis, showing significant improvements in character life simulation, user\ninstruction following, narrative coherence, and visual consistency for both\ncharacters and the environments compared to traditional related approaches.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://generative-infinite-game.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2410.18975v2",
    "published_date": "2024-10-24 17:59:31 UTC",
    "updated_date": "2024-10-30 16:10:33 UTC"
  },
  {
    "arxiv_id": "2410.18974v2",
    "title": "3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D Generation",
    "authors": [
      "Hansheng Chen",
      "Bokui Shen",
      "Yulin Liu",
      "Ruoxi Shi",
      "Linqi Zhou",
      "Connor Z. Lin",
      "Jiayuan Gu",
      "Hao Su",
      "Gordon Wetzstein",
      "Leonidas Guibas"
    ],
    "abstract": "Multi-view image diffusion models have significantly advanced open-domain 3D\nobject generation. However, most existing models rely on 2D network\narchitectures that lack inherent 3D biases, resulting in compromised geometric\nconsistency. To address this challenge, we introduce 3D-Adapter, a plug-in\nmodule designed to infuse 3D geometry awareness into pretrained image diffusion\nmodels. Central to our approach is the idea of 3D feedback augmentation: for\neach denoising step in the sampling loop, 3D-Adapter decodes intermediate\nmulti-view features into a coherent 3D representation, then re-encodes the\nrendered RGBD views to augment the pretrained base model through feature\naddition. We study two variants of 3D-Adapter: a fast feed-forward version\nbased on Gaussian splatting and a versatile training-free version utilizing\nneural fields and meshes. Our extensive experiments demonstrate that 3D-Adapter\nnot only greatly enhances the geometry quality of text-to-multi-view models\nsuch as Instant3D and Zero123++, but also enables high-quality 3D generation\nusing the plain text-to-image Stable Diffusion. Furthermore, we showcase the\nbroad application potential of 3D-Adapter by presenting high quality results in\ntext-to-3D, image-to-3D, text-to-texture, and text-to-avatar tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://lakonik.github.io/3d-adapter/",
    "pdf_url": "http://arxiv.org/pdf/2410.18974v2",
    "published_date": "2024-10-24 17:59:30 UTC",
    "updated_date": "2025-02-20 02:42:30 UTC"
  },
  {
    "arxiv_id": "2410.18972v1",
    "title": "Deep Insights into Cognitive Decline: A Survey of Leveraging Non-Intrusive Modalities with Deep Learning Techniques",
    "authors": [
      "David Ortiz-Perez",
      "Manuel Benavent-Lledo",
      "Jose Garcia-Rodriguez",
      "David Tomás",
      "M. Flores Vizcaya-Moreno"
    ],
    "abstract": "Cognitive decline is a natural part of aging, often resulting in reduced\ncognitive abilities. In some cases, however, this decline is more pronounced,\ntypically due to disorders such as Alzheimer's disease. Early detection of\nanomalous cognitive decline is crucial, as it can facilitate timely\nprofessional intervention. While medical data can help in this detection, it\noften involves invasive procedures. An alternative approach is to employ\nnon-intrusive techniques such as speech or handwriting analysis, which do not\nnecessarily affect daily activities. This survey reviews the most relevant\nmethodologies that use deep learning techniques to automate the cognitive\ndecline estimation task, including audio, text, and visual processing. We\ndiscuss the key features and advantages of each modality and methodology,\nincluding state-of-the-art approaches like Transformer architecture and\nfoundation models. In addition, we present works that integrate different\nmodalities to develop multimodal models. We also highlight the most significant\ndatasets and the quantitative results from studies using these resources. From\nthis review, several conclusions emerge. In most cases, the textual modality\nachieves the best results and is the most relevant for detecting cognitive\ndecline. Moreover, combining various approaches from individual modalities into\na multimodal model consistently enhances performance across nearly all\nscenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18972v1",
    "published_date": "2024-10-24 17:59:21 UTC",
    "updated_date": "2024-10-24 17:59:21 UTC"
  },
  {
    "arxiv_id": "2410.18970v3",
    "title": "WASP: A Weight-Space Approach to Detecting Learned Spuriousness",
    "authors": [
      "Cristian Daniel Păduraru",
      "Antonio Bărbălau",
      "Radu Filipescu",
      "Andrei Liviu Nicolicioiu",
      "Elena Burceanu"
    ],
    "abstract": "It is of crucial importance to train machine learning models such that they\nclearly understand what defines each class in a given task. Though there is a\nsum of works dedicated to identifying the spurious correlations featured by a\ndataset that may impact the model's understanding of the classes, all current\napproaches rely solely on data or error analysis. That is, they cannot point\nout spurious correlations learned by the model that are not already pointed out\nby the counterexamples featured in the validation or training sets. We propose\na method that transcends this limitation, switching the focus from analyzing a\nmodel's predictions to analyzing the model's weights, the mechanism behind the\nmaking of the decisions, which proves to be more insightful. Our proposed\nWeight-space Approach to detecting Spuriousness (WASP) relies on analyzing the\nweights of foundation models as they drift towards capturing various (spurious)\ncorrelations while being fine-tuned on a given dataset. We demonstrate that\ndifferent from previous works, our method (i) can expose spurious correlations\nfeatured by a dataset even when they are not exposed by training or validation\ncounterexamples, (ii) it works for multiple modalities such as image and text,\nand (iii) it can uncover previously untapped spurious correlations learned by\nImageNet-1k classifiers.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 4 figures, 6 tables, under review",
    "pdf_url": "http://arxiv.org/pdf/2410.18970v3",
    "published_date": "2024-10-24 17:59:16 UTC",
    "updated_date": "2025-02-13 17:57:28 UTC"
  },
  {
    "arxiv_id": "2410.18963v1",
    "title": "OSCAR: Operating System Control via State-Aware Reasoning and Re-Planning",
    "authors": [
      "Xiaoqiang Wang",
      "Bang Liu"
    ],
    "abstract": "Large language models (LLMs) and large multimodal models (LMMs) have shown\ngreat potential in automating complex tasks like web browsing and gaming.\nHowever, their ability to generalize across diverse applications remains\nlimited, hindering broader utility. To address this challenge, we present\nOSCAR: Operating System Control via state-Aware reasoning and Re-planning.\nOSCAR is a generalist agent designed to autonomously navigate and interact with\nvarious desktop and mobile applications through standardized controls, such as\nmouse and keyboard inputs, while processing screen images to fulfill user\ncommands. OSCAR translates human instructions into executable Python code,\nenabling precise control over graphical user interfaces (GUIs). To enhance\nstability and adaptability, OSCAR operates as a state machine, equipped with\nerror-handling mechanisms and dynamic task re-planning, allowing it to\nefficiently adjust to real-time feedback and exceptions. We demonstrate OSCAR's\neffectiveness through extensive experiments on diverse benchmarks across\ndesktop and mobile platforms, where it transforms complex workflows into simple\nnatural language commands, significantly boosting user productivity. Our code\nwill be open-source upon publication.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2410.18963v1",
    "published_date": "2024-10-24 17:58:08 UTC",
    "updated_date": "2024-10-24 17:58:08 UTC"
  },
  {
    "arxiv_id": "2410.18959v3",
    "title": "Context is Key: A Benchmark for Forecasting with Essential Textual Information",
    "authors": [
      "Andrew Robert Williams",
      "Arjun Ashok",
      "Étienne Marcotte",
      "Valentina Zantedeschi",
      "Jithendaraa Subramanian",
      "Roland Riachi",
      "James Requeima",
      "Alexandre Lacoste",
      "Irina Rish",
      "Nicolas Chapados",
      "Alexandre Drouin"
    ],
    "abstract": "Forecasting is a critical task in decision-making across numerous domains.\nWhile historical numerical data provide a start, they fail to convey the\ncomplete context for reliable and accurate predictions. Human forecasters\nfrequently rely on additional information, such as background knowledge and\nconstraints, which can efficiently be communicated through natural language.\nHowever, in spite of recent progress with LLM-based forecasters, their ability\nto effectively integrate this textual information remains an open question. To\naddress this, we introduce \"Context is Key\" (CiK), a time-series forecasting\nbenchmark that pairs numerical data with diverse types of carefully crafted\ntextual context, requiring models to integrate both modalities; crucially,\nevery task in CiK requires understanding textual context to be solved\nsuccessfully. We evaluate a range of approaches, including statistical models,\ntime series foundation models, and LLM-based forecasters, and propose a simple\nyet effective LLM prompting method that outperforms all other tested methods on\nour benchmark. Our experiments highlight the importance of incorporating\ncontextual information, demonstrate surprising performance when using LLM-based\nforecasting models, and also reveal some of their critical shortcomings. This\nbenchmark aims to advance multimodal forecasting by promoting models that are\nboth accurate and accessible to decision-makers with varied technical\nexpertise. The benchmark can be visualized at\nhttps://servicenow.github.io/context-is-key-forecasting/v0/.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint; under review. First two authors contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2410.18959v3",
    "published_date": "2024-10-24 17:56:08 UTC",
    "updated_date": "2025-02-06 19:05:41 UTC"
  },
  {
    "arxiv_id": "2410.18952v2",
    "title": "Dynamic Vocabulary Pruning in Early-Exit LLMs",
    "authors": [
      "Jort Vincenti",
      "Karim Abdel Sadek",
      "Joan Velja",
      "Matteo Nulli",
      "Metod Jazbec"
    ],
    "abstract": "Increasing the size of large language models (LLMs) has been shown to lead to\nbetter performance. However, this comes at the cost of slower and more\nexpensive inference. Early-exiting is a promising approach for improving the\nefficiency of LLM inference by enabling next token prediction at intermediate\nlayers. Yet, the large vocabulary size in modern LLMs makes the confidence\nestimation required for exit decisions computationally expensive, diminishing\nthe efficiency gains. To address this, we propose dynamically pruning the\nvocabulary at test time for each token. Specifically, the vocabulary is pruned\nat one of the initial layers, and the smaller vocabulary is then used\nthroughout the rest of the forward pass. Our experiments demonstrate that such\npost-hoc dynamic vocabulary pruning improves the efficiency of confidence\nestimation in early-exit LLMs while maintaining competitive performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18952v2",
    "published_date": "2024-10-24 17:52:31 UTC",
    "updated_date": "2024-10-30 15:28:02 UTC"
  },
  {
    "arxiv_id": "2410.18935v1",
    "title": "Schema-Guided Culture-Aware Complex Event Simulation with Multi-Agent Role-Play",
    "authors": [
      "Sha Li",
      "Revanth Gangi Reddy",
      "Khanh Duy Nguyen",
      "Qingyun Wang",
      "May Fung",
      "Chi Han",
      "Jiawei Han",
      "Kartik Natarajan",
      "Clare R. Voss",
      "Heng Ji"
    ],
    "abstract": "Complex news events, such as natural disasters and socio-political conflicts,\nrequire swift responses from the government and society. Relying on historical\nevents to project the future is insufficient as such events are sparse and do\nnot cover all possible conditions and nuanced situations. Simulation of these\ncomplex events can help better prepare and reduce the negative impact. We\ndevelop a controllable complex news event simulator guided by both the event\nschema representing domain knowledge about the scenario and user-provided\nassumptions representing case-specific conditions. As event dynamics depend on\nthe fine-grained social and cultural context, we further introduce a\ngeo-diverse commonsense and cultural norm-aware knowledge enhancement\ncomponent. To enhance the coherence of the simulation, apart from the global\ntimeline of events, we take an agent-based approach to simulate the individual\ncharacter states, plans, and actions. By incorporating the schema and cultural\nnorms, our generated simulations achieve much higher coherence and\nappropriateness and are received favorably by participants from a humanitarian\nassistance organization.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted as EMNLP 2024 Demo",
    "pdf_url": "http://arxiv.org/pdf/2410.18935v1",
    "published_date": "2024-10-24 17:21:43 UTC",
    "updated_date": "2024-10-24 17:21:43 UTC"
  },
  {
    "arxiv_id": "2410.18932v1",
    "title": "ANAVI: Audio Noise Awareness using Visuals of Indoor environments for NAVIgation",
    "authors": [
      "Vidhi Jain",
      "Rishi Veerapaneni",
      "Yonatan Bisk"
    ],
    "abstract": "We propose Audio Noise Awareness using Visuals of Indoors for NAVIgation for\nquieter robot path planning. While humans are naturally aware of the noise they\nmake and its impact on those around them, robots currently lack this awareness.\nA key challenge in achieving audio awareness for robots is estimating how loud\nwill the robot's actions be at a listener's location? Since sound depends upon\nthe geometry and material composition of rooms, we train the robot to passively\nperceive loudness using visual observations of indoor environments. To this\nend, we generate data on how loud an 'impulse' sounds at different listener\nlocations in simulated homes, and train our Acoustic Noise Predictor (ANP).\nNext, we collect acoustic profiles corresponding to different actions for\nnavigation. Unifying ANP with action acoustics, we demonstrate experiments with\nwheeled (Hello Robot Stretch) and legged (Unitree Go2) robots so that these\nrobots adhere to the noise constraints of the environment. See code and data at\nhttps://anavi-corl24.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "8th Conference on Robot Learning (CoRL) 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.18932v1",
    "published_date": "2024-10-24 17:19:53 UTC",
    "updated_date": "2024-10-24 17:19:53 UTC"
  },
  {
    "arxiv_id": "2410.18923v2",
    "title": "SegLLM: Multi-round Reasoning Segmentation",
    "authors": [
      "XuDong Wang",
      "Shaolun Zhang",
      "Shufan Li",
      "Konstantinos Kallidromitis",
      "Kehan Li",
      "Yusuke Kato",
      "Kazuki Kozuka",
      "Trevor Darrell"
    ],
    "abstract": "We present SegLLM, a novel multi-round interactive reasoning segmentation\nmodel that enhances LLM-based segmentation by exploiting conversational memory\nof both visual and textual outputs. By leveraging a mask-aware multimodal LLM,\nSegLLM re-integrates previous segmentation results into its input stream,\nenabling it to reason about complex user intentions and segment objects in\nrelation to previously identified entities, including positional,\ninteractional, and hierarchical relationships, across multiple interactions.\nThis capability allows SegLLM to respond to visual and text queries in a\nchat-like manner. Evaluated on the newly curated MRSeg benchmark, SegLLM\noutperforms existing methods in multi-round interactive reasoning segmentation\nby over 20%. Additionally, we observed that training on multi-round reasoning\nsegmentation data enhances performance on standard single-round referring\nsegmentation and localization tasks, resulting in a 5.5% increase in cIoU for\nreferring expression segmentation and a 4.5% improvement in Acc@0.5 for\nreferring expression localization.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "22 pages, 10 figures, 11 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.18923v2",
    "published_date": "2024-10-24 17:11:52 UTC",
    "updated_date": "2024-10-31 19:44:05 UTC"
  },
  {
    "arxiv_id": "2410.18921v2",
    "title": "From Blind Solvers to Logical Thinkers: Benchmarking LLMs' Logical Integrity on Faulty Mathematical Problems",
    "authors": [
      "A M Muntasir Rahman",
      "Junyi Ye",
      "Wei Yao",
      "Sierra S. Liu",
      "Jesse Yu",
      "Jonathan Yu",
      "Wenpeng Yin",
      "Guiling Wang"
    ],
    "abstract": "Consider the math problem: \"Lily received 3 cookies from her best friend\nyesterday and ate 5 for breakfast. Today, her friend gave her 3 more cookies.\nHow many cookies does Lily have now?\" Many large language models (LLMs) in\nprevious research approach this problem by calculating the answer \"1\" using the\nequation \"3 - 5 + 3.\" However, from a human perspective, we recognize the\ninherent flaw in this problem: Lily cannot eat 5 cookies if she initially only\nhad 3. This discrepancy prompts a key question: Are current LLMs merely Blind\nSolver that apply mathematical operations without deeper reasoning, or can they\nfunction as Logical Thinker capable of identifying logical inconsistencies?\n  To explore this question, we propose a benchmark dataset, FaultyMath, which\nincludes faulty math problems of rich diversity: i) multiple mathematical\ncategories, e.g., algebra, geometry, number theory, etc., ii) varying levels of\ndifficulty, and iii) different origins of faultiness -- ranging from violations\nof common sense and ambiguous statements to mathematical contradictions and\nmore. We evaluate a broad spectrum of LLMs, including open-source,\nclosed-source, and math-specialized models, using FaultyMath across three\ndimensions: (i) How accurately can the models detect faulty math problems\nwithout being explicitly prompted to do so? (ii) When provided with hints --\neither correct or misleading -- about the validity of the problems, to what\nextent do LLMs adapt to become reliable Logical Thinker? (iii) How trustworthy\nare the explanations generated by LLMs when they recognize a math problem as\nflawed? Through extensive experimentation and detailed analysis, our results\ndemonstrate that existing LLMs largely function as Blind Solver and fall short\nof the reasoning capabilities required to perform as Logical Thinker.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18921v2",
    "published_date": "2024-10-24 17:10:39 UTC",
    "updated_date": "2025-04-04 20:06:36 UTC"
  },
  {
    "arxiv_id": "2410.18912v1",
    "title": "Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling",
    "authors": [
      "Mingtong Zhang",
      "Kaifeng Zhang",
      "Yunzhu Li"
    ],
    "abstract": "Videos of robots interacting with objects encode rich information about the\nobjects' dynamics. However, existing video prediction approaches typically do\nnot explicitly account for the 3D information from videos, such as robot\nactions and objects' 3D states, limiting their use in real-world robotic\napplications. In this work, we introduce a framework to learn object dynamics\ndirectly from multi-view RGB videos by explicitly considering the robot's\naction trajectories and their effects on scene dynamics. We utilize the 3D\nGaussian representation of 3D Gaussian Splatting (3DGS) to train a\nparticle-based dynamics model using Graph Neural Networks. This model operates\non sparse control particles downsampled from the densely tracked 3D Gaussian\nreconstructions. By learning the neural dynamics model on offline robot\ninteraction data, our method can predict object motions under varying initial\nconfigurations and unseen robot actions. The 3D transformations of Gaussians\ncan be interpolated from the motions of control particles, enabling the\nrendering of predicted future object states and achieving action-conditioned\nvideo prediction. The dynamics model can also be applied to model-based\nplanning frameworks for object manipulation tasks. We conduct experiments on\nvarious kinds of deformable materials, including ropes, clothes, and stuffed\nanimals, demonstrating our framework's ability to model complex shapes and\ndynamics. Our project page is available at https://gs-dynamics.github.io.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Project Page: https://gs-dynamics.github.io",
    "pdf_url": "http://arxiv.org/pdf/2410.18912v1",
    "published_date": "2024-10-24 17:02:52 UTC",
    "updated_date": "2024-10-24 17:02:52 UTC"
  },
  {
    "arxiv_id": "2410.18907v1",
    "title": "SkillMimicGen: Automated Demonstration Generation for Efficient Skill Learning and Deployment",
    "authors": [
      "Caelan Garrett",
      "Ajay Mandlekar",
      "Bowen Wen",
      "Dieter Fox"
    ],
    "abstract": "Imitation learning from human demonstrations is an effective paradigm for\nrobot manipulation, but acquiring large datasets is costly and\nresource-intensive, especially for long-horizon tasks. To address this issue,\nwe propose SkillMimicGen (SkillGen), an automated system for generating\ndemonstration datasets from a few human demos. SkillGen segments human demos\ninto manipulation skills, adapts these skills to new contexts, and stitches\nthem together through free-space transit and transfer motion. We also propose a\nHybrid Skill Policy (HSP) framework for learning skill initiation, control, and\ntermination components from SkillGen datasets, enabling skills to be sequenced\nusing motion planning at test-time. We demonstrate that SkillGen greatly\nimproves data generation and policy learning performance over a\nstate-of-the-art data generation framework, resulting in the capability to\nproduce data for large scene variations, including clutter, and agents that are\non average 24% more successful. We demonstrate the efficacy of SkillGen by\ngenerating over 24K demonstrations across 18 task variants in simulation from\njust 60 human demonstrations, and training proficient, often near-perfect, HSP\nagents. Finally, we apply SkillGen to 3 real-world manipulation tasks and also\ndemonstrate zero-shot sim-to-real transfer on a long-horizon assembly task.\nVideos, and more at https://skillgen.github.io.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18907v1",
    "published_date": "2024-10-24 16:59:26 UTC",
    "updated_date": "2024-10-24 16:59:26 UTC"
  },
  {
    "arxiv_id": "2410.18906v2",
    "title": "PRISM: A Methodology for Auditing Biases in Large Language Models",
    "authors": [
      "Leif Azzopardi",
      "Yashar Moshfeghi"
    ],
    "abstract": "Auditing Large Language Models (LLMs) to discover their biases and\npreferences is an emerging challenge in creating Responsible Artificial\nIntelligence (AI). While various methods have been proposed to elicit the\npreferences of such models, countermeasures have been taken by LLM trainers,\nsuch that LLMs hide, obfuscate or point blank refuse to disclosure their\npositions on certain subjects. This paper presents PRISM, a flexible,\ninquiry-based methodology for auditing LLMs - that seeks to illicit such\npositions indirectly through task-based inquiry prompting rather than direct\ninquiry of said preferences. To demonstrate the utility of the methodology, we\napplied PRISM on the Political Compass Test, where we assessed the political\nleanings of twenty-one LLMs from seven providers. We show LLMs, by default,\nespouse positions that are economically left and socially liberal (consistent\nwith prior work). We also show the space of positions that these models are\nwilling to espouse - where some models are more constrained and less compliant\nthan others - while others are more neutral and objective. In sum, PRISM can\nmore reliably probe and audit LLMs to understand their preferences, biases and\nconstraints.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18906v2",
    "published_date": "2024-10-24 16:57:20 UTC",
    "updated_date": "2024-11-10 11:48:53 UTC"
  },
  {
    "arxiv_id": "2410.18893v1",
    "title": "Creating and Repairing Robot Programs in Open-World Domains",
    "authors": [
      "Claire Schlesinger",
      "Arjun Guha",
      "Joydeep Biswas"
    ],
    "abstract": "Using Large Language Models (LLMs) to produce robot programs from natural\nlanguage has allowed for robot systems that can complete a higher diversity of\ntasks. However, LLM-generated programs may be faulty, either due to ambiguity\nin instructions, misinterpretation of the desired task, or missing information\nabout the world state. As these programs run, the state of the world changes\nand they gather new information. When a failure occurs, it is important that\nthey recover from the current world state and avoid repeating steps that they\nthey previously completed successfully. We propose RoboRepair, a system which\ntraces the execution of a program up until error, and then runs an LLM-produced\nrecovery program that minimizes repeated actions.\n  To evaluate the efficacy of our system, we create a benchmark consisting of\neleven tasks with various error conditions that require the generation of a\nrecovery program. We compare the efficiency of the recovery program to a plan\nbuilt with an oracle that has foreknowledge of future errors.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Under review at ACL Rolling Review",
    "pdf_url": "http://arxiv.org/pdf/2410.18893v1",
    "published_date": "2024-10-24 16:30:14 UTC",
    "updated_date": "2024-10-24 16:30:14 UTC"
  },
  {
    "arxiv_id": "2410.18890v1",
    "title": "Improving Small-Scale Large Language Models Function Calling for Reasoning Tasks",
    "authors": [
      "Graziano A. Manduzio",
      "Federico A. Galatolo",
      "Mario G. C. A. Cimino",
      "Enzo Pasquale Scilingo",
      "Lorenzo Cominelli"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional capabilities in natural language understanding and generation.\nWhile these models excel in general complex reasoning tasks, they still face\nchallenges in mathematical problem-solving and logical reasoning. To address\nthese limitations, researchers have explored function calling abilities,\nallowing LLMs to execute provided functions and utilize their outputs for task\ncompletion. However, concentrating on specific tasks can be very inefficient\nfor large-scale LLMs to be used, because of the expensive cost of training and\ninference stages they need in terms of computational resources. This study\nintroduces a novel framework for training smaller language models in function\ncalling, focusing on specific logical and mathematical reasoning tasks. The\napproach aims to improve performances of small-scale models for these tasks\nusing function calling, ensuring a high level of accuracy. Our framework\nemploys an agent that, given a problem and a set of callable functions, queries\nthe LLM by injecting a description and examples of the usable functions into\nthe prompt and managing their calls in a step-by-step reasoning chain. This\nprocess is used to create a dataset of correct and incorrect reasoning chain\nchat completions from a large-scale LLM. This dataset is used to train a\nsmaller LLM using Reinforcement Learning from Human Feedback (RLHF),\nspecifically employing the Direct Preference Optimization (DPO) technique.\nExperimental results demonstrate how the proposed approach balances the\ntrade-off between model size and performance, improving the ability of function\ncalling for reasoning tasks, in smaller models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18890v1",
    "published_date": "2024-10-24 16:27:35 UTC",
    "updated_date": "2024-10-24 16:27:35 UTC"
  },
  {
    "arxiv_id": "2410.18881v1",
    "title": "Diff-Instruct++: Training One-step Text-to-image Generator Model to Align with Human Preferences",
    "authors": [
      "Weijian Luo"
    ],
    "abstract": "One-step text-to-image generator models offer advantages such as swift\ninference efficiency, flexible architectures, and state-of-the-art generation\nperformance. In this paper, we study the problem of aligning one-step generator\nmodels with human preferences for the first time. Inspired by the success of\nreinforcement learning using human feedback (RLHF), we formulate the alignment\nproblem as maximizing expected human reward functions while adding an Integral\nKullback-Leibler divergence term to prevent the generator from diverging. By\novercoming technical challenges, we introduce Diff-Instruct++ (DI++), the\nfirst, fast-converging and image data-free human preference alignment method\nfor one-step text-to-image generators. We also introduce novel theoretical\ninsights, showing that using CFG for diffusion distillation is secretly doing\nRLHF with DI++. Such an interesting finding brings understanding and potential\ncontributions to future research involving CFG. In the experiment sections, we\nalign both UNet-based and DiT-based one-step generators using DI++, which use\nthe Stable Diffusion 1.5 and the PixelArt-$\\alpha$ as the reference diffusion\nprocesses. The resulting DiT-based one-step text-to-image model achieves a\nstrong Aesthetic Score of 6.19 and an Image Reward of 1.24 on the COCO\nvalidation prompt dataset. It also achieves a leading Human preference Score\n(HPSv2.0) of 28.48, outperforming other open-sourced models such as Stable\nDiffusion XL, DMD2, SD-Turbo, as well as PixelArt-$\\alpha$. Both theoretical\ncontributions and empirical evidence indicate that DI++ is a strong\nhuman-preference alignment approach for one-step text-to-image models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18881v1",
    "published_date": "2024-10-24 16:17:18 UTC",
    "updated_date": "2024-10-24 16:17:18 UTC"
  },
  {
    "arxiv_id": "2410.18876v1",
    "title": "Guiding Empowerment Model: Liberating Neurodiversity in Online Higher Education",
    "authors": [
      "Hannah Beaux",
      "Pegah Karimi",
      "Otilia Pop",
      "Rob Clark"
    ],
    "abstract": "In this innovative practice full paper, we address the equity gap for\nneurodivergent and situationally limited learners by identifying the spectrum\nof dynamic factors that impact learning and function. Educators have shown a\ngrowing interest in identifying learners' cognitive abilities and learning\npreferences to measure their impact on academic achievement. Often institutions\nemploy one-size-fits-all approaches leaving the burden on disabled students to\nself-advocate or tolerate inadequate support. Emerging frameworks guide\nneurodivergent learners through instructional approaches, such as online\neducation. However, these frameworks fail to address holistic environmental\nneeds or recommend technology interventions, particularly for those with\nundisclosed learning or developmental disabilities and situational limitations.\nIn this article, we integrate a neurodivergent perspective through secondary\nresearch of around 100 articles to introduce a Guiding Empowerment Model\ninvolving key cognitive and situational factors that contextualize day-to-day\nexperiences affecting learner ability. We synthesize three sample student\nprofiles that highlight user problems in functioning. We use this model to\nevaluate sample learning platform features and other supportive technology\nsolutions. The proposed approach augments frameworks such as Universal Design\nfor Learning to consider factors including various sensory processing\ndifferences, social connection challenges, and environmental limitations. We\nsuggest that by applying the mode through technology-enabled features such as\ncustomizable task management, guided varied content access, and guided\nmulti-modal collaboration, major learning barriers of neurodivergent and\nsituationally limited learners will be removed to activate the successful\npursuit of their academic goals.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 1 Figure, 1 Table, Accepted in FIE 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.18876v1",
    "published_date": "2024-10-24 16:05:38 UTC",
    "updated_date": "2024-10-24 16:05:38 UTC"
  },
  {
    "arxiv_id": "2410.18866v1",
    "title": "The Cat and Mouse Game: The Ongoing Arms Race Between Diffusion Models and Detection Methods",
    "authors": [
      "Linda Laurier",
      "Ave Giulietta",
      "Arlo Octavia",
      "Meade Cleti"
    ],
    "abstract": "The emergence of diffusion models has transformed synthetic media generation,\noffering unmatched realism and control over content creation. These\nadvancements have driven innovation across fields such as art, design, and\nscientific visualization. However, they also introduce significant ethical and\nsocietal challenges, particularly through the creation of hyper-realistic\nimages that can facilitate deepfakes, misinformation, and unauthorized\nreproduction of copyrighted material. In response, the need for effective\ndetection mechanisms has become increasingly urgent. This review examines the\nevolving adversarial relationship between diffusion model development and the\nadvancement of detection methods. We present a thorough analysis of\ncontemporary detection strategies, including frequency and spatial domain\ntechniques, deep learning-based approaches, and hybrid models that combine\nmultiple methodologies. We also highlight the importance of diverse datasets\nand standardized evaluation metrics in improving detection accuracy and\ngeneralizability. Our discussion explores the practical applications of these\ndetection systems in copyright protection, misinformation prevention, and\nforensic analysis, while also addressing the ethical implications of synthetic\nmedia. Finally, we identify key research gaps and propose future directions to\nenhance the robustness and adaptability of detection methods in line with the\nrapid advancements of diffusion models. This review emphasizes the necessity of\na comprehensive approach to mitigating the risks associated with AI-generated\ncontent in an increasingly digital world.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2410.18866v1",
    "published_date": "2024-10-24 15:51:04 UTC",
    "updated_date": "2024-10-24 15:51:04 UTC"
  },
  {
    "arxiv_id": "2410.18861v1",
    "title": "Provably Robust Watermarks for Open-Source Language Models",
    "authors": [
      "Miranda Christ",
      "Sam Gunn",
      "Tal Malkin",
      "Mariana Raykova"
    ],
    "abstract": "The recent explosion of high-quality language models has necessitated new\nmethods for identifying AI-generated text. Watermarking is a leading solution\nand could prove to be an essential tool in the age of generative AI. Existing\napproaches embed watermarks at inference and crucially rely on the large\nlanguage model (LLM) specification and parameters being secret, which makes\nthem inapplicable to the open-source setting. In this work, we introduce the\nfirst watermarking scheme for open-source LLMs. Our scheme works by modifying\nthe parameters of the model, but the watermark can be detected from just the\noutputs of the model. Perhaps surprisingly, we prove that our watermarks are\nunremovable under certain assumptions about the adversary's knowledge. To\ndemonstrate the behavior of our construction under concrete parameter\ninstantiations, we present experimental results with OPT-6.7B and OPT-1.3B. We\ndemonstrate robustness to both token substitution and perturbation of the model\nparameters. We find that the stronger of these attacks, the model-perturbation\nattack, requires deteriorating the quality score to 0 out of 100 in order to\nbring the detection rate down to 50%.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18861v1",
    "published_date": "2024-10-24 15:44:34 UTC",
    "updated_date": "2024-10-24 15:44:34 UTC"
  },
  {
    "arxiv_id": "2410.18860v1",
    "title": "DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations",
    "authors": [
      "Aryo Pradipta Gema",
      "Chen Jin",
      "Ahmed Abdulaal",
      "Tom Diethe",
      "Philip Teare",
      "Beatrice Alex",
      "Pasquale Minervini",
      "Amrutha Saseendran"
    ],
    "abstract": "Large Language Models (LLMs) often hallucinate, producing unfaithful or\nfactually incorrect outputs by misrepresenting the provided context or\nincorrectly recalling internal knowledge. Recent studies have identified\nspecific attention heads within the Transformer architecture, known as\nretrieval heads, responsible for extracting relevant contextual information. We\nhypothesise that masking these retrieval heads can induce hallucinations and\nthat contrasting the outputs of the base LLM and the masked LLM can reduce\nhallucinations. To this end, we propose Decoding by Contrasting Retrieval Heads\n(DeCoRe), a novel training-free decoding strategy that amplifies information\nfound in the context and model parameters. DeCoRe mitigates potentially\nhallucinated responses by dynamically contrasting the outputs of the base LLM\nand the masked LLM, using conditional entropy as a guide. Our extensive\nexperiments confirm that DeCoRe significantly improves performance on tasks\nrequiring high contextual faithfulness, such as summarisation (XSum by 18.6%),\ninstruction following (MemoTrap by 10.9%), and open-book question answering\n(NQ-Open by 2.4% and NQ-Swap by 5.5%).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18860v1",
    "published_date": "2024-10-24 15:44:33 UTC",
    "updated_date": "2024-10-24 15:44:33 UTC"
  },
  {
    "arxiv_id": "2410.18856v3",
    "title": "Demystifying Large Language Models for Medicine: A Primer",
    "authors": [
      "Qiao Jin",
      "Nicholas Wan",
      "Robert Leaman",
      "Shubo Tian",
      "Zhizheng Wang",
      "Yifan Yang",
      "Zifeng Wang",
      "Guangzhi Xiong",
      "Po-Ting Lai",
      "Qingqing Zhu",
      "Benjamin Hou",
      "Maame Sarfo-Gyamfi",
      "Gongbo Zhang",
      "Aidan Gilson",
      "Balu Bhasuran",
      "Zhe He",
      "Aidong Zhang",
      "Jimeng Sun",
      "Chunhua Weng",
      "Ronald M. Summers",
      "Qingyu Chen",
      "Yifan Peng",
      "Zhiyong Lu"
    ],
    "abstract": "Large language models (LLMs) represent a transformative class of AI tools\ncapable of revolutionizing various aspects of healthcare by generating\nhuman-like responses across diverse contexts and adapting to novel tasks\nfollowing human instructions. Their potential application spans a broad range\nof medical tasks, such as clinical documentation, matching patients to clinical\ntrials, and answering medical questions. In this primer paper, we propose an\nactionable guideline to help healthcare professionals more efficiently utilize\nLLMs in their work, along with a set of best practices. This approach consists\nof several main phases, including formulating the task, choosing LLMs, prompt\nengineering, fine-tuning, and deployment. We start with the discussion of\ncritical considerations in identifying healthcare tasks that align with the\ncore capabilities of LLMs and selecting models based on the selected task and\ndata, performance requirements, and model interface. We then review the\nstrategies, such as prompt engineering and fine-tuning, to adapt standard LLMs\nto specialized medical tasks. Deployment considerations, including regulatory\ncompliance, ethical guidelines, and continuous monitoring for fairness and\nbias, are also discussed. By providing a structured step-by-step methodology,\nthis tutorial aims to equip healthcare professionals with the tools necessary\nto effectively integrate LLMs into clinical practice, ensuring that these\npowerful technologies are applied in a safe, reliable, and impactful manner.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2410.18856v3",
    "published_date": "2024-10-24 15:41:56 UTC",
    "updated_date": "2024-11-20 01:04:33 UTC"
  },
  {
    "arxiv_id": "2410.18852v1",
    "title": "DL-Polycube: Deep learning enhanced polycube method for high-quality hexahedral mesh generation and volumetric spline construction",
    "authors": [
      "Yuxuan Yu",
      "Yuzhuo Fang",
      "Hua Tong",
      "Yongjie Jessica Zhang"
    ],
    "abstract": "In this paper, we present a novel algorithm that integrates deep learning\nwith the polycube method (DL-Polycube) to generate high-quality hexahedral\n(hex) meshes, which are then used to construct volumetric splines for\nisogeometric analysis. Our DL-Polycube algorithm begins by establishing a\nconnection between surface triangular meshes and polycube structures. We employ\ndeep neural network to classify surface triangular meshes into their\ncorresponding polycube structures. Following this, we combine the acquired\npolycube structural information with unsupervised learning to perform surface\nsegmentation of triangular meshes. This step addresses the issue of\nsegmentation not corresponding to a polycube while reducing manual\nintervention. Quality hex meshes are then generated from the polycube\nstructures, with employing octree subdivision, parametric mapping and quality\nimprovement techniques. The incorporation of deep learning for creating\npolycube structures, combined with unsupervised learning for segmentation of\nsurface triangular meshes, substantially accelerates hex mesh generation.\nFinally, truncated hierarchical B-splines are constructed on the generated hex\nmeshes. We extract trivariate B\\'ezier elements from these splines and apply\nthem directly in isogeometric analysis. We offer several examples to\ndemonstrate the robustness of our DL-Polycube algorithm.",
    "categories": [
      "cs.CG",
      "cs.AI",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.CG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18852v1",
    "published_date": "2024-10-24 15:35:08 UTC",
    "updated_date": "2024-10-24 15:35:08 UTC"
  },
  {
    "arxiv_id": "2410.18845v1",
    "title": "Expanding AI Awareness Through Everyday Interactions with AI: A Reflective Journal Study",
    "authors": [
      "Ashish Hingle",
      "Aditya Johri"
    ],
    "abstract": "As the application of AI continues to expand, students in technology programs\nare poised to be both producers and users of the technologies. They are also\npositioned to engage with AI applications within and outside the classroom.\nWhile focusing on the curriculum when examining students' AI knowledge is\ncommon, extending this connection to students' everyday interactions with AI\nprovides a more complete picture of their learning. In this paper, we explore\nstudent's awareness and engagement with AI in the context of school and their\ndaily lives. Over six weeks, 22 undergraduate students participated in a\nreflective journal study and submitted a weekly journal entry about their\ninteractions with AI. The participants were recruited from a technology and\nsociety course that focuses on the implications of technology on people,\ncommunities, and processes. In their weekly journal entries, participants\nreflected on interactions with AI on campus (coursework, advertises campus\nevents, or seminars) and beyond (social media, news, or conversations with\nfriends and family). The journal prompts were designed to help them think\nthrough what they had read, watched, or been told and reflect on the\ndevelopment of their own perspectives, knowledge, and literacy on the topic.\nOverall, students described nine categories of interactions: coursework, news\nand current events, using software and applications, university events, social\nmedia related to their work, personal discussions with friends and family,\ninteracting with content, and gaming. Students reported that completing the\ndiaries allowed them time for reflection and made them more aware of the\npresence of AI in their daily lives and of its potential benefits and\ndrawbacks. This research contributes to the ongoing work on AI awareness and\nliteracy by bringing in perspectives from beyond a formal educational context.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted and presented at the Frontiers in Education 2024 (FIE2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.18845v1",
    "published_date": "2024-10-24 15:26:34 UTC",
    "updated_date": "2024-10-24 15:26:34 UTC"
  },
  {
    "arxiv_id": "2410.18844v1",
    "title": "Learning to Explore with Lagrangians for Bandits under Unknown Linear Constraints",
    "authors": [
      "Udvas Das",
      "Debabrota Basu"
    ],
    "abstract": "Pure exploration in bandits models multiple real-world problems, such as\ntuning hyper-parameters or conducting user studies, where different safety,\nresource, and fairness constraints on the decision space naturally appear. We\nstudy these problems as pure exploration in multi-armed bandits with unknown\nlinear constraints, where the aim is to identify an $r$$\\textit{-good feasible\npolicy}$. First, we propose a Lagrangian relaxation of the sample complexity\nlower bound for pure exploration under constraints. We show how this lower\nbound evolves with the sequential estimation of constraints. Second, we\nleverage the Lagrangian lower bound and the properties of convex optimisation\nto propose two computationally efficient extensions of Track-and-Stop and\nGamified Explorer, namely LATS and LAGEX. To this end, we propose a\nconstraint-adaptive stopping rule, and while tracking the lower bound, use\npessimistic estimate of the feasible set at each step. We show that these\nalgorithms achieve asymptotically optimal sample complexity upper bounds up to\nconstraint-dependent constants. Finally, we conduct numerical experiments with\ndifferent reward distributions and constraints that validate efficient\nperformance of LAGEX and LATS with respect to baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18844v1",
    "published_date": "2024-10-24 15:26:14 UTC",
    "updated_date": "2024-10-24 15:26:14 UTC"
  },
  {
    "arxiv_id": "2410.18841v1",
    "title": "From Efficiency to Equity: Measuring Fairness in Preference Learning",
    "authors": [
      "Shreeyash Gowaikar",
      "Hugo Berard",
      "Rashid Mushkani",
      "Shin Koseki"
    ],
    "abstract": "As AI systems, particularly generative models, increasingly influence\ndecision-making, ensuring that they are able to fairly represent diverse human\npreferences becomes crucial. This paper introduces a novel framework for\nevaluating epistemic fairness in preference learning models inspired by\neconomic theories of inequality and Rawlsian justice. We propose metrics\nadapted from the Gini Coefficient, Atkinson Index, and Kuznets Ratio to\nquantify fairness in these models. We validate our approach using two datasets:\na custom visual preference dataset (AI-EDI-Space) and the Jester Jokes dataset.\nOur analysis reveals variations in model performance across users, highlighting\npotential epistemic injustices. We explore pre-processing and in-processing\ntechniques to mitigate these inequalities, demonstrating a complex relationship\nbetween model efficiency and fairness. This work contributes to AI ethics by\nproviding a framework for evaluating and improving epistemic fairness in\npreference learning models, offering insights for developing more inclusive AI\nsystems in contexts where diverse human preferences are crucial.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18841v1",
    "published_date": "2024-10-24 15:25:56 UTC",
    "updated_date": "2024-10-24 15:25:56 UTC"
  },
  {
    "arxiv_id": "2410.18836v1",
    "title": "From English-Centric to Effective Bilingual: LLMs with Custom Tokenizers for Underrepresented Languages",
    "authors": [
      "Artur Kiulian",
      "Anton Polishko",
      "Mykola Khandoga",
      "Yevhen Kostiuk",
      "Guillermo Gabrielli",
      "Łukasz Gagała",
      "Fadi Zaraket",
      "Qusai Abu Obaida",
      "Hrishikesh Garud",
      "Wendy Wing Yee Mak",
      "Dmytro Chaplynskyi",
      "Selma Belhadj Amor",
      "Grigol Peradze"
    ],
    "abstract": "In this paper, we propose a model-agnostic cost-effective approach to\ndeveloping bilingual base large language models (LLMs) to support English and\nany target language. The method includes vocabulary expansion, initialization\nof new embeddings, model training and evaluation. We performed our experiments\nwith three languages, each using a non-Latin script - Ukrainian, Arabic, and\nGeorgian.\n  Our approach demonstrates improved language performance while reducing\ncomputational costs. It mitigates the disproportionate penalization of\nunderrepresented languages, promoting fairness and minimizing adverse phenomena\nsuch as code-switching and broken grammar. Additionally, we introduce new\nmetrics to evaluate language quality, revealing that vocabulary size\nsignificantly impacts the quality of generated text.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18836v1",
    "published_date": "2024-10-24 15:20:54 UTC",
    "updated_date": "2024-10-24 15:20:54 UTC"
  },
  {
    "arxiv_id": "2410.18823v2",
    "title": "Towards Visual Text Design Transfer Across Languages",
    "authors": [
      "Yejin Choi",
      "Jiwan Chung",
      "Sumin Shim",
      "Giyeong Oh",
      "Youngjae Yu"
    ],
    "abstract": "Visual text design plays a critical role in conveying themes, emotions, and\natmospheres in multimodal formats such as film posters and album covers.\nTranslating these visual and textual elements across languages extends the\nconcept of translation beyond mere text, requiring the adaptation of aesthetic\nand stylistic features. To address this, we introduce a novel task of\nMultimodal Style Translation (MuST-Bench), a benchmark designed to evaluate the\nability of visual text generation models to perform translation across\ndifferent writing systems while preserving design intent. Our initial\nexperiments on MuST-Bench reveal that existing visual text generation models\nstruggle with the proposed task due to the inadequacy of textual descriptions\nin conveying visual design. In response, we introduce SIGIL, a framework for\nmultimodal style translation that eliminates the need for style descriptions.\nSIGIL enhances image generation models through three innovations: glyph latent\nfor multilingual settings, pretrained VAEs for stable style guidance, and an\nOCR model with reinforcement learning feedback for optimizing readable\ncharacter generation. SIGIL outperforms existing baselines by achieving\nsuperior style consistency and legibility while maintaining visual fidelity,\nsetting itself apart from traditional description-based approaches. We release\nMuST-Bench publicly for broader use and exploration\nhttps://huggingface.co/datasets/yejinc/MuST-Bench.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18823v2",
    "published_date": "2024-10-24 15:15:01 UTC",
    "updated_date": "2024-10-29 08:24:22 UTC"
  },
  {
    "arxiv_id": "2410.18786v1",
    "title": "Applying Neural Monte Carlo Tree Search to Unsignalized Multi-intersection Scheduling for Autonomous Vehicles",
    "authors": [
      "Yucheng Shi",
      "Wenlong Wang",
      "Xiaowen Tao",
      "Ivana Dusparic",
      "Vinny Cahill"
    ],
    "abstract": "Dynamic scheduling of access to shared resources by autonomous systems is a\nchallenging problem, characterized as being NP-hard. The complexity of this\ntask leads to a combinatorial explosion of possibilities in highly dynamic\nsystems where arriving requests must be continuously scheduled subject to\nstrong safety and time constraints. An example of such a system is an\nunsignalized intersection, where automated vehicles' access to potential\nconflict zones must be dynamically scheduled. In this paper, we apply Neural\nMonte Carlo Tree Search (NMCTS) to the challenging task of scheduling platoons\nof vehicles crossing unsignalized intersections. Crucially, we introduce a\ntransformation model that maps successive sequences of potentially conflicting\nroad-space reservation requests from platoons of vehicles into a series of\nboard-game-like problems and use NMCTS to search for solutions representing\noptimal road-space allocation schedules in the context of past allocations. To\noptimize search, we incorporate a prioritized re-sampling method with parallel\nNMCTS (PNMCTS) to improve the quality of training data. To optimize training, a\ncurriculum learning strategy is used to train the agent to schedule\nprogressively more complex boards culminating in overlapping boards that\nrepresent busy intersections. In a busy single four-way unsignalized\nintersection simulation, PNMCTS solved 95\\% of unseen scenarios, reducing\ncrossing time by 43\\% in light and 52\\% in heavy traffic versus first-in,\nfirst-out control. In a 3x3 multi-intersection network, the proposed method\nmaintained free-flow in light traffic when all intersections are under control\nof PNMCTS and outperformed state-of-the-art RL-based traffic-light controllers\nin average travel time by 74.5\\% and total throughput by 16\\% in heavy traffic.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18786v1",
    "published_date": "2024-10-24 14:37:55 UTC",
    "updated_date": "2024-10-24 14:37:55 UTC"
  },
  {
    "arxiv_id": "2410.18785v1",
    "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
    "authors": [
      "Qi Li",
      "Xiang Liu",
      "Zhenheng Tang",
      "Peijie Dong",
      "Zeyu Li",
      "Xinglin Pan",
      "Xiaowen Chu"
    ],
    "abstract": "Model editing has become an increasingly popular alternative for efficiently\nupdating knowledge within language models. Current methods mainly focus on\nreliability, generalization, and locality, with many methods excelling across\nthese criteria. Some recent works disclose the pitfalls of these editing\nmethods such as knowledge distortion or conflict. However, the general\nabilities of post-edited language models remain unexplored. In this paper, we\nperform a comprehensive evaluation on various editing methods and different\nlanguage models, and have following findings. (1) Existing editing methods lead\nto inevitable performance deterioration on general benchmarks, indicating that\nexisting editing methods maintain the general abilities of the model within\nonly a few dozen edits. When the number of edits is slightly large, the\nintrinsic knowledge structure of the model is disrupted or even completely\ndamaged. (2) Instruction-tuned models are more robust to editing, showing less\nperformance drop on general knowledge after editing. (3) Language model with\nlarge scale is more resistant to editing compared to small model. (4) The\nsafety of the edited model, is significantly weakened, even for those\nsafety-aligned models. Our findings indicate that current editing methods are\nonly suitable for small-scale knowledge updates within language models, which\nmotivates further research on more practical and reliable editing methods. The\ndetails of code and reproduction can be found in\nhttps://github.com/lqinfdim/EditingEvaluation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2024 https://github.com/lqinfdim/EditingEvaluation",
    "pdf_url": "http://arxiv.org/pdf/2410.18785v1",
    "published_date": "2024-10-24 14:36:48 UTC",
    "updated_date": "2024-10-24 14:36:48 UTC"
  },
  {
    "arxiv_id": "2410.18775v2",
    "title": "Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances",
    "authors": [
      "Shilin Lu",
      "Zihan Zhou",
      "Jiayou Lu",
      "Yuanzhi Zhu",
      "Adams Wai-Kin Kong"
    ],
    "abstract": "Current image watermarking methods are vulnerable to advanced image editing\ntechniques enabled by large-scale text-to-image models. These models can\ndistort embedded watermarks during editing, posing significant challenges to\ncopyright protection. In this work, we introduce W-Bench, the first\ncomprehensive benchmark designed to evaluate the robustness of watermarking\nmethods against a wide range of image editing techniques, including image\nregeneration, global editing, local editing, and image-to-video generation.\nThrough extensive evaluations of eleven representative watermarking methods\nagainst prevalent editing techniques, we demonstrate that most methods fail to\ndetect watermarks after such edits. To address this limitation, we propose\nVINE, a watermarking method that significantly enhances robustness against\nvarious image editing techniques while maintaining high image quality. Our\napproach involves two key innovations: (1) we analyze the frequency\ncharacteristics of image editing and identify that blurring distortions exhibit\nsimilar frequency properties, which allows us to use them as surrogate attacks\nduring training to bolster watermark robustness; (2) we leverage a large-scale\npretrained diffusion model SDXL-Turbo, adapting it for the watermarking task to\nachieve more imperceptible and robust watermark embedding. Experimental results\nshow that our method achieves outstanding watermarking performance under\nvarious image editing techniques, outperforming existing methods in both image\nquality and robustness. Code is available at https://github.com/Shilin-LU/VINE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.18775v2",
    "published_date": "2024-10-24 14:28:32 UTC",
    "updated_date": "2025-03-15 02:23:29 UTC"
  },
  {
    "arxiv_id": "2410.18749v1",
    "title": "Does Differential Privacy Impact Bias in Pretrained NLP Models?",
    "authors": [
      "Md. Khairul Islam",
      "Andrew Wang",
      "Tianhao Wang",
      "Yangfeng Ji",
      "Judy Fox",
      "Jieyu Zhao"
    ],
    "abstract": "Differential privacy (DP) is applied when fine-tuning pre-trained large\nlanguage models (LLMs) to limit leakage of training examples. While most DP\nresearch has focused on improving a model's privacy-utility tradeoff, some find\nthat DP can be unfair to or biased against underrepresented groups. In this\nwork, we show the impact of DP on bias in LLMs through empirical analysis.\nDifferentially private training can increase the model bias against protected\ngroups w.r.t AUC-based bias metrics. DP makes it more difficult for the model\nto differentiate between the positive and negative examples from the protected\ngroups and other groups in the rest of the population. Our results also show\nthat the impact of DP on bias is not only affected by the privacy protection\nlevel but also the underlying distribution of the dataset.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Github https://github.com/khairulislam/DP-on-NLP-Bias",
    "pdf_url": "http://arxiv.org/pdf/2410.18749v1",
    "published_date": "2024-10-24 13:59:03 UTC",
    "updated_date": "2024-10-24 13:59:03 UTC"
  },
  {
    "arxiv_id": "2410.19878v3",
    "title": "Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies",
    "authors": [
      "Luping Wang",
      "Sheng Chen",
      "Linnan Jiang",
      "Shu Pan",
      "Runze Cai",
      "Sen Yang",
      "Fei Yang"
    ],
    "abstract": "The large models, as predicted by scaling raw forecasts, have made\ngroundbreaking progress in many fields, particularly in natural language\ngeneration tasks, where they have approached or even surpassed human levels.\nHowever, the unprecedented scale of their parameters brings significant\ncomputational and storage costs. These large models require substantial\ncomputational resources and GPU memory to operate. When adapting large models\nto specific downstream tasks, their massive parameter scale poses a significant\nchallenge in fine-tuning on hardware platforms with limited computational power\nand GPU memory. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)\noffers a practical solution by efficiently adjusting the parameters of large\npre-trained models to suit various downstream tasks. Specifically, PEFT adjusts\nthe parameters of pre-trained large models to adapt to specific tasks or\ndomains, minimizing the introduction of additional parameters and the\ncomputational resources required. This review mainly introduces the preliminary\nknowledge of PEFT, the core ideas and principles of various PEFT algorithms,\nthe applications of PEFT, and potential future research directions. By reading\nthis review, we believe that interested parties can quickly grasp the PEFT\nmethodology, thereby accelerating its development and innovation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.19878v3",
    "published_date": "2024-10-24 13:58:59 UTC",
    "updated_date": "2025-04-24 07:20:31 UTC"
  },
  {
    "arxiv_id": "2410.18738v1",
    "title": "Cellpose+, a morphological analysis tool for feature extraction of stained cell images",
    "authors": [
      "Israel A. Huaman",
      "Fares D. E. Ghorabe",
      "Sofya S. Chumakova",
      "Alexandra A. Pisarenko",
      "Alexey E. Dudaev",
      "Tatiana G. Volova",
      "Galina A. Ryltseva",
      "Sviatlana A. Ulasevich",
      "Ekaterina I. Shishatskaya",
      "Ekaterina V. Skorb",
      "Pavel S. Zun"
    ],
    "abstract": "Advanced image segmentation and processing tools present an opportunity to\nstudy cell processes and their dynamics. However, image analysis is often\nroutine and time-consuming. Nowadays, alternative data-driven approaches using\ndeep learning are potentially offering automatized, accurate, and fast image\nanalysis. In this paper, we extend the applications of Cellpose, a\nstate-of-the-art cell segmentation framework, with feature extraction\ncapabilities to assess morphological characteristics. We also introduce a\ndataset of DAPI and FITC stained cells to which our new method is applied.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T07"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18738v1",
    "published_date": "2024-10-24 13:41:40 UTC",
    "updated_date": "2024-10-24 13:41:40 UTC"
  },
  {
    "arxiv_id": "2410.18725v2",
    "title": "AI Readiness in Healthcare through Storytelling XAI",
    "authors": [
      "Akshat Dubey",
      "Zewen Yang",
      "Georges Hattab"
    ],
    "abstract": "Artificial Intelligence is rapidly advancing and radically impacting everyday\nlife, driven by the increasing availability of computing power. Despite this\ntrend, the adoption of AI in real-world healthcare is still limited. One of the\nmain reasons is the trustworthiness of AI models and the potential hesitation\nof domain experts with model predictions. Explainable Artificial Intelligence\n(XAI) techniques aim to address these issues. However, explainability can mean\ndifferent things to people with different backgrounds, expertise, and goals. To\naddress the target audience with diverse needs, we develop storytelling XAI. In\nthis research, we have developed an approach that combines multi-task\ndistillation with interpretability techniques to enable audience-centric\nexplainability. Using multi-task distillation allows the model to exploit the\nrelationships between tasks, potentially improving interpretability as each\ntask supports the other leading to an enhanced interpretability from the\nperspective of a domain expert. The distillation process allows us to extend\nthis research to large deep models that are highly complex. We focus on both\nmodel-agnostic and model-specific methods of interpretability, supported by\ntextual justification of the results in healthcare through our use case. Our\nmethods increase the trust of both the domain experts and the machine learning\nexperts to enable a responsible AI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Pre-print of the accepted manuscript in EXPLIMED - First Workshop on\n  Explainable Artificial Intelligence for the Medical Domain, European\n  Conference on Artificial Intelligence (ECAI) - 2024, Santiago de Compostela,\n  Spain",
    "pdf_url": "http://arxiv.org/pdf/2410.18725v2",
    "published_date": "2024-10-24 13:30:18 UTC",
    "updated_date": "2024-11-28 09:08:40 UTC"
  },
  {
    "arxiv_id": "2410.18720v1",
    "title": "GeoLoRA: Geometric integration for parameter efficient fine-tuning",
    "authors": [
      "Steffen Schotthöfer",
      "Emanuele Zangrando",
      "Gianluca Ceruti",
      "Francesco Tudisco",
      "Jonas Kusch"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become a widely used method for\nparameter-efficient fine-tuning of large-scale, pre-trained neural networks.\nHowever, LoRA and its extensions face several challenges, including the need\nfor rank adaptivity, robustness, and computational efficiency during the\nfine-tuning process. We introduce GeoLoRA, a novel approach that addresses\nthese limitations by leveraging dynamical low-rank approximation theory.\nGeoLoRA requires only a single backpropagation pass over the small-rank\nadapters, significantly reducing computational cost as compared to similar\ndynamical low-rank training methods and making it faster than popular baselines\nsuch as AdaLoRA. This allows GeoLoRA to efficiently adapt the allocated\nparameter budget across the model, achieving smaller low-rank adapters compared\nto heuristic methods like AdaLoRA and LoRA, while maintaining critical\nconvergence, descent, and error-bound theoretical guarantees. The resulting\nmethod is not only more efficient but also more robust to varying\nhyperparameter settings. We demonstrate the effectiveness of GeoLoRA on several\nstate-of-the-art benchmarks, showing that it outperforms existing methods in\nboth accuracy and computational efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18720v1",
    "published_date": "2024-10-24 13:26:10 UTC",
    "updated_date": "2024-10-24 13:26:10 UTC"
  },
  {
    "arxiv_id": "2410.18718v1",
    "title": "LLM-based Online Prediction of Time-varying Graph Signals",
    "authors": [
      "Dayu Qin",
      "Yi Yan",
      "Ercan Engin Kuruoglu"
    ],
    "abstract": "In this paper, we propose a novel framework that leverages large language\nmodels (LLMs) for predicting missing values in time-varying graph signals by\nexploiting spatial and temporal smoothness. We leverage the power of LLM to\nachieve a message-passing scheme. For each missing node, its neighbors and\nprevious estimates are fed into and processed by LLM to infer the missing\nobservations. Tested on the task of the online prediction of wind-speed graph\nsignals, our model outperforms online graph filtering algorithms in terms of\naccuracy, demonstrating the potential of LLMs in effectively addressing\npartially observed signals in graphs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18718v1",
    "published_date": "2024-10-24 13:22:50 UTC",
    "updated_date": "2024-10-24 13:22:50 UTC"
  },
  {
    "arxiv_id": "2410.18717v1",
    "title": "Low-Latency Video Anonymization for Crowd Anomaly Detection: Privacy vs. Performance",
    "authors": [
      "Mulugeta Weldezgina Asres",
      "Lei Jiao",
      "Christian Walter Omlin"
    ],
    "abstract": "Recent advancements in artificial intelligence promise ample potential in\nmonitoring applications with surveillance cameras. However, concerns about\nprivacy and model bias have made it challenging to utilize them in public.\nAlthough de-identification approaches have been proposed in the literature,\naiming to achieve a certain level of anonymization, most of them employ deep\nlearning models that are computationally demanding for real-time edge\ndeployment. In this study, we revisit conventional anonymization solutions for\nprivacy protection and real-time video anomaly detection (VAD) applications. We\npropose a novel lightweight adaptive anonymization for VAD (LA3D) that employs\ndynamic adjustment to enhance privacy protection. We evaluated the approaches\non publicly available privacy and VAD data sets to examine the strengths and\nweaknesses of the different anonymization techniques and highlight the\npromising efficacy of our approach. Our experiment demonstrates that LA3D\nenables substantial improvement in the privacy anonymization capability without\nmajorly degrading VAD efficacy.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "16pages, 8 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.18717v1",
    "published_date": "2024-10-24 13:22:33 UTC",
    "updated_date": "2024-10-24 13:22:33 UTC"
  },
  {
    "arxiv_id": "2410.18697v2",
    "title": "How Good Are LLMs for Literary Translation, Really? Literary Translation Evaluation with Humans and LLMs",
    "authors": [
      "Ran Zhang",
      "Wei Zhao",
      "Steffen Eger"
    ],
    "abstract": "Recent research has focused on literary machine translation (MT) as a new\nchallenge in MT. However, the evaluation of literary MT remains an open\nproblem. We contribute to this ongoing discussion by introducing\nLITEVAL-CORPUS, a paragraph-level parallel corpus containing verified human\ntranslations and outputs from 9 MT systems, which totals over 2k translations\nand 13k evaluated sentences across four language pairs, costing 4.5k C. This\ncorpus enables us to (i) examine the consistency and adequacy of human\nevaluation schemes with various degrees of complexity, (ii) compare evaluations\nby students and professionals, assess the effectiveness of (iii) LLM-based\nmetrics and (iv) LLMs themselves. Our findings indicate that the adequacy of\nhuman evaluation is controlled by two factors: the complexity of the evaluation\nscheme (more complex is less adequate) and the expertise of evaluators (higher\nexpertise yields more adequate evaluations). For instance, MQM\n(Multidimensional Quality Metrics), a complex scheme and the de facto standard\nfor non-literary human MT evaluation, is largely inadequate for literary\ntranslation evaluation: with student evaluators, nearly 60% of human\ntranslations are misjudged as indistinguishable or inferior to machine\ntranslations. In contrast, BWS (BEST-WORST SCALING), a much simpler scheme,\nidentifies human translations at a rate of 80-100%. Automatic metrics fare\ndramatically worse, with rates of at most 20%. Our overall evaluation indicates\nthat published human translations consistently outperform LLM translations,\nwhere even the most recent LLMs tend to produce considerably more literal and\nless diverse translations compared to humans.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL Camera-Ready version",
    "pdf_url": "http://arxiv.org/pdf/2410.18697v2",
    "published_date": "2024-10-24 12:48:03 UTC",
    "updated_date": "2025-02-25 10:20:16 UTC"
  },
  {
    "arxiv_id": "2410.18693v1",
    "title": "Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch",
    "authors": [
      "Yuyang Ding",
      "Xinyu Shi",
      "Xiaobo Liang",
      "Juntao Li",
      "Qiaoming Zhu",
      "Min Zhang"
    ],
    "abstract": "The availability of high-quality data is one of the most important factors in\nimproving the reasoning capability of LLMs. Existing works have demonstrated\nthe effectiveness of creating more instruction data from seed questions or\nknowledge bases. Recent research indicates that continually scaling up data\nsynthesis from strong models (e.g., GPT-4) can further elicit reasoning\nperformance. Though promising, the open-sourced community still lacks\nhigh-quality data at scale and scalable data synthesis methods with affordable\ncosts. To address this, we introduce ScaleQuest, a scalable and novel data\nsynthesis method that utilizes \"small-size\" (e.g., 7B) open-source models to\ngenerate questions from scratch without the need for seed data with complex\naugmentation constraints. With the efficient ScaleQuest, we automatically\nconstructed a mathematical reasoning dataset consisting of 1 million\nproblem-solution pairs, which are more effective than existing open-sourced\ndatasets. It can universally increase the performance of mainstream open-source\nmodels (i.e., Mistral, Llama3, DeepSeekMath, and Qwen2-Math) by achieving 29.2%\nto 46.4% gains on MATH. Notably, simply fine-tuning the Qwen2-Math-7B-Base\nmodel with our dataset can even surpass Qwen2-Math-7B-Instruct, a strong and\nwell-aligned model on closed-source data, and proprietary models such as\nGPT-4-Turbo and Claude-3.5 Sonnet.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint. Project page: https://scalequest.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2410.18693v1",
    "published_date": "2024-10-24 12:42:04 UTC",
    "updated_date": "2024-10-24 12:42:04 UTC"
  },
  {
    "arxiv_id": "2410.19025v1",
    "title": "Large Language Models for Financial Aid in Financial Time-series Forecasting",
    "authors": [
      "Md Khairul Islam",
      "Ayush Karmacharya",
      "Timothy Sue",
      "Judy Fox"
    ],
    "abstract": "Considering the difficulty of financial time series forecasting in financial\naid, much of the current research focuses on leveraging big data analytics in\nfinancial services. One modern approach is to utilize \"predictive analysis\",\nanalogous to forecasting financial trends. However, many of these time series\ndata in Financial Aid (FA) pose unique challenges due to limited historical\ndatasets and high dimensional financial information, which hinder the\ndevelopment of effective predictive models that balance accuracy with efficient\nruntime and memory usage. Pre-trained foundation models are employed to address\nthese challenging tasks. We use state-of-the-art time series models including\npre-trained LLMs (GPT-2 as the backbone), transformers, and linear models to\ndemonstrate their ability to outperform traditional approaches, even with\nminimal (\"few-shot\") or no fine-tuning (\"zero-shot\"). Our benchmark study,\nwhich includes financial aid with seven other time series tasks, shows the\npotential of using LLMs for scarce financial datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "GitHub link https://github.com/UVA-MLSys/Financial-Time-Series",
    "pdf_url": "http://arxiv.org/pdf/2410.19025v1",
    "published_date": "2024-10-24 12:41:47 UTC",
    "updated_date": "2024-10-24 12:41:47 UTC"
  },
  {
    "arxiv_id": "2410.18678v1",
    "title": "Ali-AUG: Innovative Approaches to Labeled Data Augmentation using One-Step Diffusion Model",
    "authors": [
      "Ali Hamza",
      "Aizea Lojo",
      "Adrian Núñez-Marcos",
      "Aitziber Atutxa"
    ],
    "abstract": "This paper introduces Ali-AUG, a novel single-step diffusion model for\nefficient labeled data augmentation in industrial applications. Our method\naddresses the challenge of limited labeled data by generating synthetic,\nlabeled images with precise feature insertion. Ali-AUG utilizes a stable\ndiffusion architecture enhanced with skip connections and LoRA modules to\nefficiently integrate masks and images, ensuring accurate feature placement\nwithout affecting unrelated image content. Experimental validation across\nvarious industrial datasets demonstrates Ali-AUG's superiority in generating\nhigh-quality, defect-enhanced images while maintaining rapid single-step\ninference. By offering precise control over feature insertion and minimizing\nrequired training steps, our technique significantly enhances data augmentation\ncapabilities, providing a powerful tool for improving the performance of deep\nlearning models in scenarios with limited labeled data. Ali-AUG is especially\nuseful for use cases like defective product image generation to train AI-based\nmodels to improve their ability to detect defects in manufacturing processes.\nUsing different data preparation strategies, including Classification Accuracy\nScore (CAS) and Naive Augmentation Score (NAS), we show that Ali-AUG improves\nmodel performance by 31% compared to other augmentation methods and by 45%\ncompared to models without data augmentation. Notably, Ali-AUG reduces training\ntime by 32% and supports both paired and unpaired datasets, enhancing\nflexibility in data preparation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18678v1",
    "published_date": "2024-10-24 12:12:46 UTC",
    "updated_date": "2024-10-24 12:12:46 UTC"
  },
  {
    "arxiv_id": "2410.18670v1",
    "title": "Health Misinformation in Social Networks: A Survey of IT Approaches",
    "authors": [
      "Vasiliki Papanikou",
      "Panagiotis Papadakos",
      "Theodora Karamanidou",
      "Thanos G. Stavropoulos",
      "Evaggelia Pitoura",
      "Panayiotis Tsaparas"
    ],
    "abstract": "In this paper, we present a comprehensive survey on the pervasive issue of\nmedical misinformation in social networks from the perspective of information\ntechnology. The survey aims at providing a systematic review of related\nresearch and helping researchers and practitioners navigate through this\nfast-changing field. Specifically, we first present manual and automatic\napproaches for fact-checking. We then explore fake news detection methods,\nusing content, propagation features, or source features, as well as mitigation\napproaches for countering the spread of misinformation. We also provide a\ndetailed list of several datasets on health misinformation and of publicly\navailable tools. We conclude the survey with a discussion on the open\nchallenges and future research directions in the battle against health\nmisinformation.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "Preprint -- Under review in the ACM Transactions on Computing for\n  Healthcare (HEALTH) journal",
    "pdf_url": "http://arxiv.org/pdf/2410.18670v1",
    "published_date": "2024-10-24 12:00:51 UTC",
    "updated_date": "2024-10-24 12:00:51 UTC"
  },
  {
    "arxiv_id": "2410.18652v7",
    "title": "$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation",
    "authors": [
      "Woosung Koh",
      "Jang Han Yoon",
      "MinHyung Lee",
      "Youngjin Song",
      "Jaegwan Cho",
      "Jaehyun Kang",
      "Taehyeon Kim",
      "Se-Young Yun",
      "Youngjae Yu",
      "Bongshin Lee"
    ],
    "abstract": "Generating high-quality charts with Large Language Models (LLMs) presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. $\\langle \\text{instruction}, \\text{data}, \\text{code} \\rangle$\ntriplets are scarce and expensive to manually curate as their creation demands\ntechnical expertise. To address this scalability challenge, we introduce a\nreference-free automatic feedback generator, which eliminates the need for\ncostly human intervention. Our novel framework, C$^2$, consists of (1) an\nautomatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset\n(ChartUIE-8K). The results are compelling: in our first experiment, 74% of\nrespondents strongly preferred, and 10% preferred, the results after feedback.\nThe second post-feedback experiment demonstrates that ChartAF outperform nine\nbaselines. Moreover, ChartUIE-8K significantly improves data diversity by\nincreasing queries, datasets, and chart types by 5982%, 1936%, and 91%,\nrespectively, over benchmarks. Finally, a study of LLM users revealed that 94%\nof participants preferred ChartUIE-8K's queries, with 93% deeming them aligned\nwith real-world use cases. Core contributions are available as open-source at\nchartsquared.github.io, with ample qualitative examples.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "NAACL 2025 Main (Long)",
    "pdf_url": "http://arxiv.org/pdf/2410.18652v7",
    "published_date": "2024-10-24 11:32:00 UTC",
    "updated_date": "2025-02-12 12:49:36 UTC"
  },
  {
    "arxiv_id": "2410.18648v1",
    "title": "GADT: Enhancing Transferable Adversarial Attacks through Gradient-guided Adversarial Data Transformation",
    "authors": [
      "Yating Ma",
      "Xiaogang Xu",
      "Liming Fang",
      "Zhe Liu"
    ],
    "abstract": "Current Transferable Adversarial Examples (TAE) are primarily generated by\nadding Adversarial Noise (AN). Recent studies emphasize the importance of\noptimizing Data Augmentation (DA) parameters along with AN, which poses a\ngreater threat to real-world AI applications. However, existing DA-based\nstrategies often struggle to find optimal solutions due to the challenging DA\nsearch procedure without proper guidance. In this work, we propose a novel\nDA-based attack algorithm, GADT. GADT identifies suitable DA parameters through\niterative antagonism and uses posterior estimates to update AN based on these\nparameters. We uniquely employ a differentiable DA operation library to\nidentify adversarial DA parameters and introduce a new loss function as a\nmetric during DA optimization. This loss term enhances adversarial effects\nwhile preserving the original image content, maintaining attack crypticity.\nExtensive experiments on public datasets with various networks demonstrate that\nGADT can be integrated with existing transferable attack methods, updating\ntheir DA parameters effectively while retaining their AN formulation\nstrategies. Furthermore, GADT can be utilized in other black-box attack\nscenarios, e.g., query-based attacks, offering a new avenue to enhance attacks\non real-world AI applications in both research and industrial contexts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18648v1",
    "published_date": "2024-10-24 11:21:49 UTC",
    "updated_date": "2024-10-24 11:21:49 UTC"
  },
  {
    "arxiv_id": "2410.18641v1",
    "title": "Smart ETL and LLM-based contents classification: the European Smart Tourism Tools Observatory experience",
    "authors": [
      "Diogo Cosme",
      "António Galvão",
      "Fernando Brito e Abreu"
    ],
    "abstract": "Purpose: Our research project focuses on improving the content update of the\nonline European Smart Tourism Tools (STTs) Observatory by incorporating and\ncategorizing STTs. The categorization is based on their taxonomy, and it\nfacilitates the end user's search process. The use of a Smart ETL (Extract,\nTransform, and Load) process, where \\emph{Smart} indicates the use of\nArtificial Intelligence (AI), is central to this endeavor.\n  Methods: The contents describing STTs are derived from PDF catalogs, where\nPDF-scraping techniques extract QR codes, images, links, and text information.\nDuplicate STTs between the catalogs are removed, and the remaining ones are\nclassified based on their text information using Large Language Models (LLMs).\nFinally, the data is transformed to comply with the Dublin Core metadata\nstructure (the observatory's metadata structure), chosen for its wide\nacceptance and flexibility.\n  Results: The Smart ETL process to import STTs to the observatory combines\nPDF-scraping techniques with LLMs for text content-based classification. Our\npreliminary results have demonstrated the potential of LLMs for text\ncontent-based classification.\n  Conclusion: The proposed approach's feasibility is a step towards efficient\ncontent-based classification, not only in Smart Tourism but also adaptable to\nother fields. Future work will mainly focus on refining this classification\nprocess.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "H.3.3; I.2.7; I.5.2"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18641v1",
    "published_date": "2024-10-24 11:10:54 UTC",
    "updated_date": "2024-10-24 11:10:54 UTC"
  },
  {
    "arxiv_id": "2410.18639v4",
    "title": "Diffusion Attribution Score: Evaluating Training Data Influence in Diffusion Models",
    "authors": [
      "Jinxu Lin",
      "Linwei Tao",
      "Minjing Dong",
      "Chang Xu"
    ],
    "abstract": "As diffusion models become increasingly popular, the misuse of copyrighted\nand private images has emerged as a major concern. One promising solution to\nmitigate this issue is identifying the contribution of specific training\nsamples in generative models, a process known as data attribution. Existing\ndata attribution methods for diffusion models typically quantify the\ncontribution of a training sample by evaluating the change in diffusion loss\nwhen the sample is included or excluded from the training process. However, we\nargue that the direct usage of diffusion loss cannot represent such a\ncontribution accurately due to the calculation of diffusion loss. Specifically,\nthese approaches measure the divergence between predicted and ground truth\ndistributions, which leads to an indirect comparison between the predicted\ndistributions and cannot represent the variances between model behaviors. To\naddress these issues, we aim to measure the direct comparison between predicted\ndistributions with an attribution score to analyse the training sample\nimportance, which is achieved by Diffusion Attribution Score (\\textit{DAS}).\nUnderpinned by rigorous theoretical analysis, we elucidate the effectiveness of\nDAS. Additionally, we explore strategies to accelerate DAS calculations,\nfacilitating its application to large-scale diffusion models. Our extensive\nexperiments across various datasets and diffusion models demonstrate that DAS\nsignificantly surpasses previous benchmarks in terms of the linear\ndata-modelling score, establishing new state-of-the-art performance. Code is\navailable at \\hyperlink{here}{https://github.com/Jinxu-Lin/DAS}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18639v4",
    "published_date": "2024-10-24 10:58:17 UTC",
    "updated_date": "2025-03-21 05:57:29 UTC"
  },
  {
    "arxiv_id": "2410.18636v2",
    "title": "Multi-agent cooperation through learning-aware policy gradients",
    "authors": [
      "Alexander Meulemans",
      "Seijin Kobayashi",
      "Johannes von Oswald",
      "Nino Scherrer",
      "Eric Elmoznino",
      "Blake Richards",
      "Guillaume Lajoie",
      "Blaise Agüera y Arcas",
      "João Sacramento"
    ],
    "abstract": "Self-interested individuals often fail to cooperate, posing a fundamental\nchallenge for multi-agent learning. How can we achieve cooperation among\nself-interested, independent learning agents? Promising recent work has shown\nthat in certain tasks cooperation can be established between learning-aware\nagents who model the learning dynamics of each other. Here, we present the\nfirst unbiased, higher-derivative-free policy gradient algorithm for\nlearning-aware reinforcement learning, which takes into account that other\nagents are themselves learning through trial and error based on multiple noisy\ntrials. We then leverage efficient sequence models to condition behavior on\nlong observation histories that contain traces of the learning dynamics of\nother agents. Training long-context policies with our algorithm leads to\ncooperative behavior and high returns on standard social dilemmas, including a\nchallenging environment where temporally-extended action coordination is\nrequired. Finally, we derive from the iterated prisoner's dilemma a novel\nexplanation for how and when cooperation arises among self-interested\nlearning-aware agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18636v2",
    "published_date": "2024-10-24 10:48:42 UTC",
    "updated_date": "2025-03-19 13:28:43 UTC"
  },
  {
    "arxiv_id": "2410.18634v2",
    "title": "Little Giants: Synthesizing High-Quality Embedding Data at Scale",
    "authors": [
      "Haonan Chen",
      "Liang Wang",
      "Nan Yang",
      "Yutao Zhu",
      "Ziliang Zhao",
      "Furu Wei",
      "Zhicheng Dou"
    ],
    "abstract": "Synthetic data generation has become an increasingly popular way of training\nmodels without the need for large, manually labeled datasets. For tasks like\ntext embedding, synthetic data offers diverse and scalable training examples,\nsignificantly reducing the cost of human annotation. However, most current\napproaches rely heavily on proprietary models like GPT-4, which are expensive\nand inefficient for generating large-scale embedding data. In this paper, we\nintroduce SPEED, a framework that aligns open-source small models (8B) to\nefficiently generate large-scale synthetic embedding data. Through supervised\nfine-tuning, preference optimization, and self-improvement, SPEED enables small\nopen-source models to produce high-quality data. Remarkably, SPEED uses only\nless than 1/10 of the GPT API calls, outperforming the state-of-the-art\nembedding model E5_mistral when both are trained solely on their synthetic\ndata. Using this efficient generator, we conduct a comprehensive study on how\nvarious factors within the alignment pipeline impact data quality and reveal\nthe scaling law for synthetic embedding data.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18634v2",
    "published_date": "2024-10-24 10:47:30 UTC",
    "updated_date": "2024-11-03 08:14:34 UTC"
  },
  {
    "arxiv_id": "2410.18628v1",
    "title": "Wavetable Synthesis Using CVAE for Timbre Control Based on Semantic Label",
    "authors": [
      "Tsugumasa Yutani",
      "Yuya Yamamoto",
      "Shuyo Nakatani",
      "Hiroko Terasawa"
    ],
    "abstract": "Synthesizers are essential in modern music production. However, their complex\ntimbre parameters, often filled with technical terms, require expertise. This\nresearch introduces a method of timbre control in wavetable synthesis that is\nintuitive and sensible and utilizes semantic labels. Using a conditional\nvariational autoencoder (CVAE), users can select a wavetable and define the\ntimbre with labels such as bright, warm, and rich. The CVAE model, featuring\nconvolutional and upsampling layers, effectively captures the wavetable\nnuances, ensuring real-time performance owing to their processing in the time\ndomain. Experiments demonstrate that this approach allows for real-time,\neffective control of the timbre of the wavetable using semantic inputs and aims\nfor intuitive timbre control through data-based semantic control.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS",
      "eess.SP"
    ],
    "primary_category": "cs.SD",
    "comment": "6 pages, 4 figures, Accepted at APSIPA ASC 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.18628v1",
    "published_date": "2024-10-24 10:37:54 UTC",
    "updated_date": "2024-10-24 10:37:54 UTC"
  },
  {
    "arxiv_id": "2410.18626v2",
    "title": "SAMG: Offline-to-Online Reinforcement Learning via State-Action-Conditional Offline Model Guidance",
    "authors": [
      "Liyu Zhang",
      "Haochi Wu",
      "Xu Wan",
      "Quan Kong",
      "Ruilong Deng",
      "Mingyang Sun"
    ],
    "abstract": "Offline-to-online (O2O) reinforcement learning (RL) pre-trains models on\noffline data and refines policies through online fine-tuning. However, existing\nO2O RL algorithms typically require maintaining the tedious offline datasets to\nmitigate the effects of out-of-distribution (OOD) data, which significantly\nlimits their efficiency in exploiting online samples. To address this\ndeficiency, we introduce a new paradigm for O2O RL called\nState-Action-Conditional Offline \\Model Guidance (SAMG). It freezes the\npre-trained offline critic to provide compact offline understanding for each\nstate-action sample, thus eliminating the need for retraining on offline data.\nThe frozen offline critic is incorporated with the online target critic\nweighted by a state-action-adaptive coefficient. This coefficient aims to\ncapture the offline degree of samples at the state-action level, and is updated\nadaptively during training. In practice, SAMG could be easily integrated with\nQ-function-based algorithms. Theoretical analysis shows good optimality and\nlower estimation error. Empirically, SAMG outperforms state-of-the-art O2O RL\nalgorithms on the D4RL benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18626v2",
    "published_date": "2024-10-24 10:35:02 UTC",
    "updated_date": "2025-02-21 11:46:46 UTC"
  },
  {
    "arxiv_id": "2410.18624v1",
    "title": "Prompting and Fine-Tuning of Small LLMs for Length-Controllable Telephone Call Summarization",
    "authors": [
      "David Thulke",
      "Yingbo Gao",
      "Rricha Jalota",
      "Christian Dugast",
      "Hermann Ney"
    ],
    "abstract": "This paper explores the rapid development of a telephone call summarization\nsystem utilizing large language models (LLMs). Our approach involves initial\nexperiments with prompting existing LLMs to generate summaries of telephone\nconversations, followed by the creation of a tailored synthetic training\ndataset utilizing stronger frontier models. We place special focus on the\ndiversity of the generated data and on the ability to control the length of the\ngenerated summaries to meet various use-case specific requirements. The\neffectiveness of our method is evaluated using two state-of-the-art\nLLM-as-a-judge-based evaluation techniques to ensure the quality and relevance\nof the summaries. Our results show that fine-tuned Llama-2-7B-based\nsummarization model performs on-par with GPT-4 in terms of factual accuracy,\ncompleteness and conciseness. Our findings demonstrate the potential for\nquickly bootstrapping a practical and efficient call summarization system.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at the The International Conference on Foundation and Large\n  Language Models (FLLM2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.18624v1",
    "published_date": "2024-10-24 10:32:10 UTC",
    "updated_date": "2024-10-24 10:32:10 UTC"
  },
  {
    "arxiv_id": "2410.18615v1",
    "title": "FairQueue: Rethinking Prompt Learning for Fair Text-to-Image Generation",
    "authors": [
      "Christopher T. H Teo",
      "Milad Abdollahzadeh",
      "Xinda Ma",
      "Ngai-man Cheung"
    ],
    "abstract": "Recently, prompt learning has emerged as the state-of-the-art (SOTA) for fair\ntext-to-image (T2I) generation. Specifically, this approach leverages readily\navailable reference images to learn inclusive prompts for each target Sensitive\nAttribute (tSA), allowing for fair image generation. In this work, we first\nreveal that this prompt learning-based approach results in degraded sample\nquality. Our analysis shows that the approach's training objective -- which\naims to align the embedding differences of learned prompts and reference images\n-- could be sub-optimal, resulting in distortion of the learned prompts and\ndegraded generated images. To further substantiate this claim, as our major\ncontribution, we deep dive into the denoising subnetwork of the T2I model to\ntrack down the effect of these learned prompts by analyzing the cross-attention\nmaps. In our analysis, we propose a novel prompt switching analysis: I2H and\nH2I. Furthermore, we propose new quantitative characterization of\ncross-attention maps. Our analysis reveals abnormalities in the early denoising\nsteps, perpetuating improper global structure that results in degradation in\nthe generated samples. Building on insights from our analysis, we propose two\nideas: (i) Prompt Queuing and (ii) Attention Amplification to address the\nquality issue. Extensive experimental results on a wide range of tSAs show that\nour proposed method outperforms SOTA approach's image generation quality, while\nachieving competitive fairness. More resources at FairQueue Project site:\nhttps://sutd-visual-computing-group.github.io/FairQueue",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in NeurIPS24",
    "pdf_url": "http://arxiv.org/pdf/2410.18615v1",
    "published_date": "2024-10-24 10:16:09 UTC",
    "updated_date": "2024-10-24 10:16:09 UTC"
  },
  {
    "arxiv_id": "2410.18612v1",
    "title": "TripCast: Pre-training of Masked 2D Transformers for Trip Time Series Forecasting",
    "authors": [
      "Yuhua Liao",
      "Zetian Wang",
      "Peng Wei",
      "Qiangqiang Nie",
      "Zhenhua Zhang"
    ],
    "abstract": "Deep learning and pre-trained models have shown great success in time series\nforecasting. However, in the tourism industry, time series data often exhibit a\nleading time property, presenting a 2D structure. This introduces unique\nchallenges for forecasting in this sector. In this study, we propose a novel\nmodelling paradigm, TripCast, which treats trip time series as 2D data and\nlearns representations through masking and reconstruction processes.\nPre-trained on large-scale real-world data, TripCast notably outperforms other\nstate-of-the-art baselines in in-domain forecasting scenarios and demonstrates\nstrong scalability and transferability in out-domain forecasting scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICONIP 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.18612v1",
    "published_date": "2024-10-24 10:08:05 UTC",
    "updated_date": "2024-10-24 10:08:05 UTC"
  },
  {
    "arxiv_id": "2410.18607v1",
    "title": "STTATTS: Unified Speech-To-Text And Text-To-Speech Model",
    "authors": [
      "Hawau Olamide Toyin",
      "Hao Li",
      "Hanan Aldarmaki"
    ],
    "abstract": "Speech recognition and speech synthesis models are typically trained\nseparately, each with its own set of learning objectives, training data, and\nmodel parameters, resulting in two distinct large networks. We propose a\nparameter-efficient approach to learning ASR and TTS jointly via a multi-task\nlearning objective and shared parameters. Our evaluation demonstrates that the\nperformance of our multi-task model is comparable to that of individually\ntrained models while significantly saving computational and memory costs\n($\\sim$50\\% reduction in the total number of parameters required for the two\ntasks combined). We experiment with English as a resource-rich language, and\nArabic as a relatively low-resource language due to shortage of TTS data. Our\nmodels are trained with publicly available data, and both the training code and\nmodel checkpoints are openly available for further research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 4 Figures, EMNLP 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2410.18607v1",
    "published_date": "2024-10-24 10:04:24 UTC",
    "updated_date": "2024-10-24 10:04:24 UTC"
  },
  {
    "arxiv_id": "2410.18603v1",
    "title": "AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant",
    "authors": [
      "Chengyou Jia",
      "Minnan Luo",
      "Zhuohang Dang",
      "Qiushi Sun",
      "Fangzhi Xu",
      "Junlin Hu",
      "Tianbao Xie",
      "Zhiyong Wu"
    ],
    "abstract": "Digital agents capable of automating complex computer tasks have attracted\nconsiderable attention due to their immense potential to enhance human-computer\ninteraction. However, existing agent methods exhibit deficiencies in their\ngeneralization and specialization capabilities, especially in handling\nopen-ended computer tasks in real-world environments. Inspired by the rich\nfunctionality of the App store, we present AgentStore, a scalable platform\ndesigned to dynamically integrate heterogeneous agents for automating computer\ntasks. AgentStore empowers users to integrate third-party agents, allowing the\nsystem to continuously enrich its capabilities and adapt to rapidly evolving\noperating systems. Additionally, we propose a novel core \\textbf{MetaAgent}\nwith the \\textbf{AgentToken} strategy to efficiently manage diverse agents and\nutilize their specialized and generalist abilities for both domain-specific and\nsystem-wide tasks. Extensive experiments on three challenging benchmarks\ndemonstrate that AgentStore surpasses the limitations of previous systems with\nnarrow capabilities, particularly achieving a significant improvement from\n11.21\\% to 23.85\\% on the OSWorld benchmark, more than doubling the previous\nresults. Comprehensive quantitative and qualitative results further demonstrate\nAgentStore's ability to enhance agent systems in both generalization and\nspecialization, underscoring its potential for developing the specialized\ngeneralist computer assistant. All our codes will be made publicly available in\nhttps://chengyou-jia.github.io/AgentStore-Home.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18603v1",
    "published_date": "2024-10-24 09:58:40 UTC",
    "updated_date": "2024-10-24 09:58:40 UTC"
  },
  {
    "arxiv_id": "2410.18585v1",
    "title": "Aligning CodeLLMs with Direct Preference Optimization",
    "authors": [
      "Yibo Miao",
      "Bofei Gao",
      "Shanghaoran Quan",
      "Junyang Lin",
      "Daoguang Zan",
      "Jiaheng Liu",
      "Jian Yang",
      "Tianyu Liu",
      "Zhijie Deng"
    ],
    "abstract": "The last year has witnessed the rapid progress of large language models\n(LLMs) across diverse domains. Among them, CodeLLMs have garnered particular\nattention because they can not only assist in completing various programming\ntasks but also represent the decision-making and logical reasoning capabilities\nof LLMs. However, current CodeLLMs mainly focus on pre-training and supervised\nfine-tuning scenarios, leaving the alignment stage, which is important for\npost-training LLMs, under-explored. This work first identifies that the\ncommonly used PPO algorithm may be suboptimal for the alignment of CodeLLM\nbecause the involved reward rules are routinely coarse-grained and potentially\nflawed. We then advocate addressing this using the DPO algorithm. Based on only\npreference data pairs, DPO can render the model rank data automatically, giving\nrise to a fine-grained rewarding pattern more robust than human intervention.\nWe also contribute a pipeline for collecting preference pairs for DPO on\nCodeLLMs. Studies show that our method significantly improves the performance\nof existing CodeLLMs on benchmarks such as MBPP and HumanEval.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18585v1",
    "published_date": "2024-10-24 09:36:13 UTC",
    "updated_date": "2024-10-24 09:36:13 UTC"
  },
  {
    "arxiv_id": "2410.19874v2",
    "title": "Paved or unpaved? A Deep Learning derived Road Surface Global Dataset from Mapillary Street-View Imagery",
    "authors": [
      "Sukanya Randhawa",
      "Eren Aygun",
      "Guntaj Randhawa",
      "Benjamin Herfort",
      "Sven Lautenbach",
      "Alexander Zipf"
    ],
    "abstract": "We have released an open dataset with global coverage on road surface\ncharacteristics (paved or unpaved) derived utilising 105 million images from\nthe world's largest crowdsourcing-based street view platform, Mapillary,\nleveraging state-of-the-art geospatial AI methods. We propose a hybrid deep\nlearning approach which combines SWIN-Transformer based road surface prediction\nand CLIP-and-DL segmentation based thresholding for filtering of bad quality\nimages. The road surface prediction results have been matched and integrated\nwith OpenStreetMap (OSM) road geometries. This study provides global data\ninsights derived from maps and statistics about spatial distribution of\nMapillary coverage and road pavedness on a continent and countries scale, with\nrural and urban distinction. This dataset expands the availability of global\nroad surface information by over 3 million kilometers, now representing\napproximately 36% of the total length of the global road network. Most regions\nshowed moderate to high paved road coverage (60-80%), but significant gaps were\nnoted in specific areas of Africa and Asia. Urban areas tend to have\nnear-complete paved coverage, while rural regions display more variability.\nModel validation against OSM surface data achieved strong performance, with F1\nscores for paved roads between 91-97% across continents. Taking forward the\nwork of Mapillary and their contributors and enrichment of OSM road attributes,\nour work provides valuable insights for applications in urban planning,\ndisaster routing, logistics optimisation and addresses various Sustainable\nDevelopment Goals (SDGS): especially SDGs 1 (No poverty), 3 (Good health and\nwell-being), 8 (Decent work and economic growth), 9 (Industry, Innovation and\nInfrastructure), 11 (Sustainable cities and communities), 12 (Responsible\nconsumption and production), and 13 (Climate action).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.19874v2",
    "published_date": "2024-10-24 09:32:53 UTC",
    "updated_date": "2024-10-29 10:06:39 UTC"
  },
  {
    "arxiv_id": "2410.18574v1",
    "title": "SIKeD: Self-guided Iterative Knowledge Distillation for mathematical reasoning",
    "authors": [
      "Shivam Adarsh",
      "Kumar Shridhar",
      "Caglar Gulcehre",
      "Nicholas Monath",
      "Mrinmaya Sachan"
    ],
    "abstract": "Large Language Models (LLMs) can transfer their reasoning skills to smaller\nmodels by teaching them to generate the intermediate reasoning process required\nto solve multistep reasoning tasks. While LLMs can accurately solve reasoning\ntasks through a variety of strategies, even without fine-tuning, smaller models\nare not expressive enough to fit the LLMs distribution on all strategies when\ndistilled and tend to prioritize one strategy over the others. This reliance on\none strategy poses a challenge for smaller models when attempting to solve\nreasoning tasks that may be difficult with their preferred strategy. To address\nthis, we propose a distillation method SIKeD (Self-guided Iterative Knowledge\nDistillation for mathematical reasoning), where the LLM teaches the smaller\nmodel to approach a task using different strategies and the smaller model uses\nits self-generated on-policy outputs to choose the most suitable strategy for\nthe given task. The training continues in a self-guided iterative manner, where\nfor each training iteration, a decision is made on how to combine the LLM data\nwith the self-generated outputs. Unlike traditional distillation methods, SIKeD\nallows the smaller model to learn which strategy is suitable for a given task\nwhile continuously learning to solve a task using different strategies. Our\nexperiments on various mathematical reasoning datasets show that SIKeD\nsignificantly outperforms traditional distillation techniques across smaller\nmodels of different sizes. Our code is available at:\nhttps://github.com/kumar-shridhar/SIKeD",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18574v1",
    "published_date": "2024-10-24 09:29:18 UTC",
    "updated_date": "2024-10-24 09:29:18 UTC"
  },
  {
    "arxiv_id": "2410.18572v1",
    "title": "Taipan: Efficient and Expressive State Space Language Models with Selective Attention",
    "authors": [
      "Chien Van Nguyen",
      "Huy Huu Nguyen",
      "Thang M. Pham",
      "Ruiyi Zhang",
      "Hanieh Deilamsalehy",
      "Puneet Mathur",
      "Ryan A. Rossi",
      "Trung Bui",
      "Viet Dac Lai",
      "Franck Dernoncourt",
      "Thien Huu Nguyen"
    ],
    "abstract": "Efficient long-context language modeling remains a significant challenge in\nNatural Language Processing (NLP). While Transformers dominate language tasks,\nthey struggle with long sequences due to quadratic computational complexity in\ntraining and linearly scaling memory costs during inference. Recent State Space\nModels (SSMs) such as Mamba offer alternatives with constant memory usage, but\nthey underperform in tasks requiring extensive in-context retrieval. We\nintroduce Taipan, a novel hybrid architecture that combines Mamba-2 with\nSelective Attention Layers (SALs). These SALs identify tokens requiring\nlong-range interactions, remove less important features, and then augment their\nrepresentations using the attention module. This approach balances Mamba's\nefficiency with Transformer-like performance in memory-intensive tasks. By\nconstraining the attention budget, Taipan extends accurate predictions to\ncontext lengths of up to 1 million tokens while preserving computational\nefficiency. Our experiments demonstrate Taipan's superior performance across\nvarious scales and tasks, offering a promising solution for efficient\nlong-context language modeling.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18572v1",
    "published_date": "2024-10-24 09:25:37 UTC",
    "updated_date": "2024-10-24 09:25:37 UTC"
  },
  {
    "arxiv_id": "2410.18570v1",
    "title": "Zero-shot Object Navigation with Vision-Language Models Reasoning",
    "authors": [
      "Congcong Wen",
      "Yisiyuan Huang",
      "Hao Huang",
      "Yanjia Huang",
      "Shuaihang Yuan",
      "Yu Hao",
      "Hui Lin",
      "Yu-Shen Liu",
      "Yi Fang"
    ],
    "abstract": "Object navigation is crucial for robots, but traditional methods require\nsubstantial training data and cannot be generalized to unknown environments.\nZero-shot object navigation (ZSON) aims to address this challenge, allowing\nrobots to interact with unknown objects without specific training data.\nLanguage-driven zero-shot object navigation (L-ZSON) is an extension of ZSON\nthat incorporates natural language instructions to guide robot navigation and\ninteraction with objects. In this paper, we propose a novel Vision Language\nmodel with a Tree-of-thought Network (VLTNet) for L-ZSON. VLTNet comprises four\nmain modules: vision language model understanding, semantic mapping,\ntree-of-thought reasoning and exploration, and goal identification. Among these\nmodules, Tree-of-Thought (ToT) reasoning and exploration module serves as a\ncore component, innovatively using the ToT reasoning framework for navigation\nfrontier selection during robot exploration. Compared to conventional frontier\nselection without reasoning, navigation using ToT reasoning involves multi-path\nreasoning processes and backtracking when necessary, enabling globally informed\ndecision-making with higher accuracy. Experimental results on PASTURE and\nRoboTHOR benchmarks demonstrate the outstanding performance of our model in\nLZSON, particularly in scenarios involving complex natural language as target\ninstructions.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted by the International Conference on Pattern Recognition\n  (ICPR) for Oral presentation",
    "pdf_url": "http://arxiv.org/pdf/2410.18570v1",
    "published_date": "2024-10-24 09:24:07 UTC",
    "updated_date": "2024-10-24 09:24:07 UTC"
  },
  {
    "arxiv_id": "2410.18565v1",
    "title": "Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation",
    "authors": [
      "Krzysztof Ociepa",
      "Łukasz Flis",
      "Krzysztof Wróbel",
      "Adrian Gwoździej",
      "Remigiusz Kinas"
    ],
    "abstract": "We introduce Bielik 7B v0.1, a 7-billion-parameter generative text model for\nPolish language processing. Trained on curated Polish corpora, this model\naddresses key challenges in language model development through innovative\ntechniques. These include Weighted Instruction Cross-Entropy Loss, which\nbalances the learning of different instruction types, and Adaptive Learning\nRate, which dynamically adjusts the learning rate based on training progress.\nTo evaluate performance, we created the Open PL LLM Leaderboard and Polish\nMT-Bench, novel frameworks assessing various NLP tasks and conversational\nabilities. Bielik 7B v0.1 demonstrates significant improvements, achieving a 9\npercentage point increase in average score compared to Mistral-7B-v0.1 on the\nRAG Reader task. It also excels in the Polish MT-Bench, particularly in\nReasoning (6.15/10) and Role-playing (7.83/10) categories. This model\nrepresents a substantial advancement in Polish language AI, offering a powerful\ntool for diverse linguistic applications and setting new benchmarks in the\nfield.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18565v1",
    "published_date": "2024-10-24 09:16:09 UTC",
    "updated_date": "2024-10-24 09:16:09 UTC"
  },
  {
    "arxiv_id": "2410.18560v1",
    "title": "Explainable News Summarization -- Analysis and mitigation of Disagreement Problem",
    "authors": [
      "Seema Aswani",
      "Sujala D. Shetty"
    ],
    "abstract": "Explainable AI (XAI) techniques for text summarization provide valuable\nunderstanding of how the summaries are generated. Recent studies have\nhighlighted a major challenge in this area, known as the disagreement problem.\nThis problem occurs when different XAI methods offer contradictory explanations\nfor the summary generated from the same input article. This inconsistency\nacross XAI methods has been evaluated using predefined metrics designed to\nquantify agreement levels between them, revealing significant disagreement.\nThis impedes the reliability and interpretability of XAI in this area. To\naddress this challenge, we propose a novel approach that utilizes sentence\ntransformers and the k-means clustering algorithm to first segment the input\narticle and then generate the explanation of the summary generated for each\nsegment. By producing regional or segmented explanations rather than\ncomprehensive ones, a decrease in the observed disagreement between XAI methods\nis hypothesized. This segmentation-based approach was used on two news\nsummarization datasets, namely Extreme Summarization(XSum) and CNN-DailyMail,\nand the experiment was conducted using multiple disagreement metrics. Our\nexperiments validate the hypothesis by showing a significant reduction in\ndisagreement among different XAI methods. Additionally, a JavaScript\nvisualization tool is developed, that is easy to use and allows users to\ninteractively explore the color-coded visualization of the input article and\nthe machine-generated summary based on the attribution scores of each\nsentences.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18560v1",
    "published_date": "2024-10-24 09:07:44 UTC",
    "updated_date": "2024-10-24 09:07:44 UTC"
  },
  {
    "arxiv_id": "2410.18556v1",
    "title": "Complexity Matters: Effective Dimensionality as a Measure for Adversarial Robustness",
    "authors": [
      "David Khachaturov",
      "Robert Mullins"
    ],
    "abstract": "Quantifying robustness in a single measure for the purposes of model\nselection, development of adversarial training methods, and anticipating trends\nhas so far been elusive. The simplest metric to consider is the number of\ntrainable parameters in a model but this has previously been shown to be\ninsufficient at explaining robustness properties. A variety of other metrics,\nsuch as ones based on boundary thickness and gradient flatness have been\nproposed but have been shown to be inadequate proxies for robustness.\n  In this work, we investigate the relationship between a model's effective\ndimensionality, which can be thought of as model complexity, and its robustness\nproperties. We run experiments on commercial-scale models that are often used\nin real-world environments such as YOLO and ResNet. We reveal a near-linear\ninverse relationship between effective dimensionality and adversarial\nrobustness, that is models with a lower dimensionality exhibit better\nrobustness. We investigate the effect of a variety of adversarial training\nmethods on effective dimensionality and find the same inverse linear\nrelationship present, suggesting that effective dimensionality can serve as a\nuseful criterion for model selection and robustness evaluation, providing a\nmore nuanced and effective metric than parameter count or previously-tested\nmeasures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18556v1",
    "published_date": "2024-10-24 09:01:34 UTC",
    "updated_date": "2024-10-24 09:01:34 UTC"
  },
  {
    "arxiv_id": "2410.18551v1",
    "title": "IMAN: An Adaptive Network for Robust NPC Mortality Prediction with Missing Modalities",
    "authors": [
      "Yejing Huo",
      "Guoheng Huang",
      "Lianglun Cheng",
      "Jianbin He",
      "Xuhang Chen",
      "Xiaochen Yuan",
      "Guo Zhong",
      "Chi-Man Pun"
    ],
    "abstract": "Accurate prediction of mortality in nasopharyngeal carcinoma (NPC), a complex\nmalignancy particularly challenging in advanced stages, is crucial for\noptimizing treatment strategies and improving patient outcomes. However, this\npredictive process is often compromised by the high-dimensional and\nheterogeneous nature of NPC-related data, coupled with the pervasive issue of\nincomplete multi-modal data, manifesting as missing radiological images or\nincomplete diagnostic reports. Traditional machine learning approaches suffer\nsignificant performance degradation when faced with such incomplete data, as\nthey fail to effectively handle the high-dimensionality and intricate\ncorrelations across modalities. Even advanced multi-modal learning techniques\nlike Transformers struggle to maintain robust performance in the presence of\nmissing modalities, as they lack specialized mechanisms to adaptively integrate\nand align the diverse data types, while also capturing nuanced patterns and\ncontextual relationships within the complex NPC data. To address these problem,\nwe introduce IMAN: an adaptive network for robust NPC mortality prediction with\nmissing modalities.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The paper has been accepted by BIBM 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.18551v1",
    "published_date": "2024-10-24 08:54:08 UTC",
    "updated_date": "2024-10-24 08:54:08 UTC"
  },
  {
    "arxiv_id": "2410.18541v1",
    "title": "On Explaining with Attention Matrices",
    "authors": [
      "Omar Naim",
      "Nicholas Asher"
    ],
    "abstract": "This paper explores the much discussed, possible explanatory link between\nattention weights (AW) in transformer models and predicted output. Contrary to\nintuition and early research on attention, more recent prior research has\nprovided formal arguments and empirical evidence that AW are not explanatorily\nrelevant. We show that the formal arguments are incorrect. We introduce and\neffectively compute efficient attention, which isolates the effective\ncomponents of attention matrices in tasks and models in which AW play an\nexplanatory role. We show that efficient attention has a causal role (provides\nminimally necessary and sufficient conditions) for predicting model output in\nNLP tasks requiring contextual information, and we show, contrary to [7], that\nefficient attention matrices are probability distributions and are effectively\ncalculable. Thus, they should play an important part in the explanation of\nattention based model behavior. We offer empirical experiments in support of\nour method illustrating various properties of efficient attention with various\nmetrics on four datasets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "46-04",
      "I.2.7; I.7.0"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18541v1",
    "published_date": "2024-10-24 08:43:33 UTC",
    "updated_date": "2024-10-24 08:43:33 UTC"
  },
  {
    "arxiv_id": "2410.18533v1",
    "title": "LOGO -- Long cOntext aliGnment via efficient preference Optimization",
    "authors": [
      "Zecheng Tang",
      "Zechen Sun",
      "Juntao Li",
      "Qiaoming Zhu",
      "Min Zhang"
    ],
    "abstract": "Long-context models(LCMs) have shown great potential in processing long input\nsequences(even more than 100M tokens) conveniently and effectively. With\nsignificant progress, recent research has pointed out that LCMs can accurately\nlocate token-level salient information within the context. Yet, the generation\nperformance of these LCMs is far from satisfactory and might result in\nmisaligned responses, such as hallucinations. To enhance the generation\ncapability of LCMs, existing works have investigated the effects of data size\nand quality for both pre-training and instruction tuning. Though achieving\nmeaningful improvement, previous methods fall short in either effectiveness or\nefficiency. In this paper, we introduce LOGO(Long cOntext aliGnment via\nefficient preference Optimization), a training strategy that first introduces\npreference optimization for long-context alignment. To overcome the GPU\nmemory-bound issue caused by the long sequence, LOGO employs a reference-free\npreference optimization strategy and adopts a position synthesis method to\nconstruct the training data. By training with only 0.3B data on a single\n8$\\times$A800 GPU machine for 16 hours, LOGO allows the Llama-3-8B-Instruct-80K\nmodel to achieve comparable performance with GPT-4 in real-world long-context\ntasks while preserving the model's original capabilities on other tasks, e.g.,\nlanguage modeling and MMLU. Moreover, LOGO can extend the model's context\nwindow size while enhancing its generation performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18533v1",
    "published_date": "2024-10-24 08:27:26 UTC",
    "updated_date": "2024-10-24 08:27:26 UTC"
  },
  {
    "arxiv_id": "2410.18528v1",
    "title": "PRACT: Optimizing Principled Reasoning and Acting of LLM Agent",
    "authors": [
      "Zhiwei Liu",
      "Weiran Yao",
      "Jianguo Zhang",
      "Rithesh Murthy",
      "Liangwei Yang",
      "Zuxin Liu",
      "Tian Lan",
      "Ming Zhu",
      "Juntao Tan",
      "Shirley Kokane",
      "Thai Hoang",
      "Juan Carlos Niebles",
      "Shelby Heinecke",
      "Huan Wang",
      "Silvio Savarese",
      "Caiming Xiong"
    ],
    "abstract": "We introduce the Principled Reasoning and Acting (PRAct) framework, a novel\nmethod for learning and enforcing action principles from trajectory data.\nCentral to our approach is the use of text gradients from a reflection and\noptimization engine to derive these action principles. To adapt action\nprinciples to specific task requirements, we propose a new optimization\nframework, Reflective Principle Optimization (RPO). After execution, RPO\nemploys a reflector to critique current action principles and an optimizer to\nupdate them accordingly. We develop the RPO framework under two scenarios:\nReward-RPO, which uses environmental rewards for reflection, and Self-RPO,\nwhich conducts self-reflection without external rewards. Additionally, two RPO\nmethods, RPO-Traj and RPO-Batch, is introduced to adapt to different settings.\nExperimental results across four environments demonstrate that the PRAct agent,\nleveraging the RPO framework, effectively learns and applies action principles\nto enhance performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to SIG CoNLL 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.18528v1",
    "published_date": "2024-10-24 08:21:51 UTC",
    "updated_date": "2024-10-24 08:21:51 UTC"
  },
  {
    "arxiv_id": "2410.18517v1",
    "title": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing",
    "authors": [
      "Yifei Yang",
      "Zouying Cao",
      "Qiguang Chen",
      "Libo Qin",
      "Dongjie Yang",
      "Hai Zhao",
      "Zhi Chen"
    ],
    "abstract": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Review by ICLR2025",
    "pdf_url": "http://arxiv.org/pdf/2410.18517v1",
    "published_date": "2024-10-24 08:06:41 UTC",
    "updated_date": "2024-10-24 08:06:41 UTC"
  },
  {
    "arxiv_id": "2410.18514v3",
    "title": "Scaling up Masked Diffusion Models on Text",
    "authors": [
      "Shen Nie",
      "Fengqi Zhu",
      "Chao Du",
      "Tianyu Pang",
      "Qian Liu",
      "Guangtao Zeng",
      "Min Lin",
      "Chongxuan Li"
    ],
    "abstract": "Masked diffusion models (MDMs) have shown promise in language modeling, yet\ntheir scalability and effectiveness in core language tasks, such as text\ngeneration and language understanding, remain underexplored. This paper\nestablishes the first scaling law for MDMs, demonstrating a scaling rate\ncomparable to autoregressive models (ARMs) and a relatively small compute gap.\nMotivated by their scalability, we train a family of MDMs with up to 1.1\nbillion (B) parameters to systematically evaluate their performance against\nARMs of comparable or larger sizes. Fully leveraging the probabilistic\nformulation of MDMs, we propose a simple yet effective unsupervised\nclassifier-free guidance that effectively exploits large-scale unpaired data,\nboosting performance for conditional inference. In language understanding, the\n1.1B MDM outperforms the 1.1B TinyLlama model trained on the same data across\nfour of eight zero-shot benchmarks. Notably, it achieves competitive math\nreasoning ability with the 7B Llama-2 model on the GSM8K dataset. In text\ngeneration, MDMs with 16 times more pre-training time offer a flexible\ntrade-off against ARMs with the accelerated sampling technique KV-Cache: MDMs\nmatch ARMs in performance while being 1.4 times faster during sampling.\nMoreover, MDMs address challenging tasks for ARMs by effectively handling\nbidirectional reasoning and adapting to temporal shifts in data. Notably, a\n1.1B MDM breaks the reverse curse encountered by much larger ARMs with\nsignificantly more data and computation, such as 13B Llama-2 and 175B GPT-3.\nOur code is available at https://github.com/ML-GSAI/SMDM.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18514v3",
    "published_date": "2024-10-24 08:01:22 UTC",
    "updated_date": "2025-02-28 07:02:59 UTC"
  },
  {
    "arxiv_id": "2410.18510v1",
    "title": "A framework for GNSS-based solutions performance analysis in an ERTMS context",
    "authors": [
      "Juliette Marais",
      "Quentin Mayolle",
      "Martin Fasquelle",
      "Vincent Tardif",
      "Emilie Chéneau-Grehalle"
    ],
    "abstract": "Context Progresses in GNSS-based solution introduction in rail applications\nGNSS (Global Navigation Satellite System) is now used in most of our travels\nand each of our smartphone apps. Most of the usages are not safety-critical.\nBut Europe identified GNSS for more applications and to be integrated in rail\nin general as part of the toolset to help railway to contribute to reduce\ntransport carbon footprint. To increase the use of trains in European\ntransports, railways must improve their attractiveness for passengers and\nfreight, but also increase reliability, availability and efficiency by reducing\ncapital expenditure and operational costs. GNSS is part of the global\ndigitalization scheme of freight that aims to offer added value to the clients\nknowledge of accurate time of arrival, continuous monitoring of transport\nconditions (temperature, humidity...). But a major challenge will be to reach\nstringent applications and in particular, GNSS is today seen as a realistic and\nserious game changer for the future of the ERTMS (European Rail Traffic\nManagement System). The localisation function is today performed with both\nodometry and balises. Odometer provides a continuous train position in time\nfrom a reference point. But as the distance delivered by the odometer shows a\ngrowing bias with distance, due to wear and wheel sliding, the use of on-track\nbalises allows to reduce this error. Future systems will be based on on-board\nlocalisation solutions with GNSS receivers. It will allow the development of\nnew concepts for moving blocks, virtual coupling and automation. Its use for\ntrain integrity is also investigated. But the environmental conditions of track\nand surroundings configuration, i.e, tunnels, dense urban areas or vegetation\noften degrade positioning performance and thus its efficiency and safety.\nIndeed, GNSS satellites are moving and their visibility (availability and\nrelative position from the receiver) vary with time. Moreover, for optimal\nperformance, the system requires open sky environments, which are the cases of\nmost of the aeronautical uses but not of train uses. Trains often circulate in\nareas where signal reception can be disturbed (multipath, intentional or\nunintentional interferences) and thus, performances degraded. If many\nprogresses have been made in the past years to develop more robust receivers\n[Puccitelli, 2022], multi-sensor solutions [CLUG website] or missing tools such\nas Digital Maps [Crespillo, 2023], in projects such as the Shift2Rail Project\nX2Rail-5 or CLUG, some questions remain and in particular related to\nperformance evaluation. How can we evaluate performances in a dynamic\nenvironment (train, satellite, obstacles)? How can we be sure that every\nconfiguration has been tested? What is the impact of a failure (inaccuracy,\nmissed detection) on operation? Some of these issues are addressed in the\non-going R2DATO project funded by Europe's rail.",
    "categories": [
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18510v1",
    "published_date": "2024-10-24 07:53:47 UTC",
    "updated_date": "2024-10-24 07:53:47 UTC"
  },
  {
    "arxiv_id": "2410.18506v1",
    "title": "Enhancing Graph Attention Neural Network Performance for Marijuana Consumption Classification through Large-scale Augmented Granger Causality (lsAGC) Analysis of Functional MR Images",
    "authors": [
      "Ali Vosoughi",
      "Akhil Kasturi",
      "Axel Wismueller"
    ],
    "abstract": "In the present research, the effectiveness of large-scale Augmented Granger\nCausality (lsAGC) as a tool for gauging brain network connectivity was examined\nto differentiate between marijuana users and typical controls by utilizing\nresting-state functional Magnetic Resonance Imaging (fMRI). The relationship\nbetween marijuana consumption and alterations in brain network connectivity is\na recognized fact in scientific literature. This study probes how lsAGC can\naccurately discern these changes. The technique used integrates dimension\nreduction with the augmentation of source time-series in a model that predicts\ntime-series, which helps in estimating the directed causal relationships among\nfMRI time-series. As a multivariate approach, lsAGC uncovers the connection of\nthe inherent dynamic system while considering all other time-series. A dataset\nof 60 adults with an ADHD diagnosis during childhood, drawn from the Addiction\nConnectome Preprocessed Initiative (ACPI), was used in the study. The brain\nconnections assessed by lsAGC were utilized as classification attributes. A\nGraph Attention Neural Network (GAT) was chosen to carry out the classification\ntask, particularly for its ability to harness graph-based data and recognize\nintricate interactions between brain regions, making it appropriate for\nfMRI-based brain connectivity data. The performance was analyzed using a\nfive-fold cross-validation system. The average accuracy achieved by the\ncorrelation coefficient method was roughly 52.98%, with a 1.65 standard\ndeviation, whereas the lsAGC approach yielded an average accuracy of 61.47%,\nwith a standard deviation of 1.44. The suggested method enhances the body of\nknowledge in the field of neuroimaging-based classification and emphasizes the\nnecessity to consider directed causal connections in brain network connectivity\nanalysis when studying marijuana's effects on the brain.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.18506v1",
    "published_date": "2024-10-24 07:50:10 UTC",
    "updated_date": "2024-10-24 07:50:10 UTC"
  },
  {
    "arxiv_id": "2410.18503v1",
    "title": "SFB-net for cardiac segmentation: Bridging the semantic gap with attention",
    "authors": [
      "Nicolas Portal",
      "Nadjia Kachenoura",
      "Thomas Dietenbeck",
      "Catherine Achard"
    ],
    "abstract": "In the past few years, deep learning algorithms have been widely used for\ncardiac image segmentation. However, most of these architectures rely on\nconvolutions that hardly model long-range dependencies, limiting their ability\nto extract contextual information. In order to tackle this issue, this article\nintroduces the Swin Filtering Block network (SFB-net) which takes advantage of\nboth conventional and swin transformer layers. The former are used to introduce\nspatial attention at the bottom of the network, while the latter are applied to\nfocus on high level semantically rich features between the encoder and decoder.\nAn average Dice score of 92.4 was achieved on the ACDC dataset. To the best of\nour knowledge, this result outperforms any other work on this dataset. The\naverage Dice score of 87.99 obtained on the M\\&amp;M's dataset demonstrates\nthat the proposed method generalizes well to data from different vendors and\ncentres.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18503v1",
    "published_date": "2024-10-24 07:48:13 UTC",
    "updated_date": "2024-10-24 07:48:13 UTC"
  },
  {
    "arxiv_id": "2411.05793v3",
    "title": "A Comprehensive Survey of Deep Learning for Time Series Forecasting: Architectural Diversity and Open Challenges",
    "authors": [
      "Jongseon Kim",
      "Hyungjoon Kim",
      "HyunGi Kim",
      "Dongjun Lee",
      "Sungroh Yoon"
    ],
    "abstract": "Time series forecasting is a critical task that provides key information for\ndecision-making. After traditional statistical and machine learning approaches,\nvarious fundamental deep learning architectures such as MLPs, CNNs, RNNs, and\nGNNs have been developed. However, the structural limitations caused by the\ninductive biases of each deep learning architecture constrained their\nperformance. Transformer models, which excel at handling long-term\ndependencies, have become significant architectural components for time series\nforecasting. However, recent research has shown that alternatives such as\nsimple linear layers can outperform Transformers. These findings have opened up\nnew possibilities for using diverse architectures, ranging from fundamental\ndeep learning models to emerging architectures and hybrid approaches. In this\ncontext, architectural modeling of time series forecasting has now entered a\nrenaissance. This survey not only provides a historical context for time series\nforecasting but also offers comprehensive and timely analysis of the movement\ntoward architectural diversification. By comparing and re-examining deep\nlearning models, we uncover new perspectives and present recent trends,\nincluding hybrid, diffusion, Mamba, and foundation models. By focusing on the\ninherent characteristics of time series data, we also address open challenges\nthat have gained attention in time series forecasting, such as channel\ndependency, distribution shift, causality, and feature extraction. These\ncontributions help lower entry barriers for newcomers by providing a systematic\nunderstanding of the diverse research areas in time series forecasting (TSF),\nwhile offering seasoned researchers broader perspectives and new opportunities\nthrough in-depth exploration of TSF challenges. (Shortened due to arXiv's\n1,920-character limit. Full version in the paper.)",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This is the accepted manuscript of the article published in\n  Artificial Intelligence Review. The final authenticated version is available\n  at: https://doi.org/10.1007/s10462-025-11223-9",
    "pdf_url": "http://arxiv.org/pdf/2411.05793v3",
    "published_date": "2024-10-24 07:43:55 UTC",
    "updated_date": "2025-05-01 05:05:29 UTC"
  },
  {
    "arxiv_id": "2410.18489v1",
    "title": "LLM as a code generator in Agile Model Driven Development",
    "authors": [
      "Ahmed R. Sadik",
      "Sebastian Brulin",
      "Markus Olhofer",
      "Antonello Ceravola",
      "Frank Joublin"
    ],
    "abstract": "Leveraging Large Language Models (LLM) like GPT4 in the auto generation of\ncode represents a significant advancement, yet it is not without its\nchallenges. The ambiguity inherent in natural language descriptions of software\nposes substantial obstacles to generating deployable, structured artifacts.\nThis research champions Model Driven Development (MDD) as a viable strategy to\novercome these challenges, proposing an Agile Model Driven Development (AMDD)\napproach that employs GPT4 as a code generator. This approach enhances the\nflexibility and scalability of the code auto generation process and offers\nagility that allows seamless adaptation to changes in models or deployment\nenvironments. We illustrate this by modeling a multi agent Unmanned Vehicle\nFleet (UVF) system using the Unified Modeling Language (UML), significantly\nreducing model ambiguity by integrating the Object Constraint Language (OCL)\nfor code structure meta modeling, and the FIPA ontology language for\ncommunication semantics meta modeling. Applying GPT4 auto generation\ncapabilities yields Java and Python code that is compatible with the JADE and\nPADE frameworks, respectively. Our thorough evaluation of the auto generated\ncode verifies its alignment with expected behaviors and identifies enhancements\nin agent interactions. Structurally, we assessed the complexity of code derived\nfrom a model constrained solely by OCL meta models, against that influenced by\nboth OCL and FIPA ontology meta models. The results indicate that the ontology\nconstrained meta model produces inherently more complex code, yet its\ncyclomatic complexity remains within manageable levels, suggesting that\nadditional meta model constraints can be incorporated without exceeding the\nhigh risk threshold for complexity.",
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.RO",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18489v1",
    "published_date": "2024-10-24 07:24:11 UTC",
    "updated_date": "2024-10-24 07:24:11 UTC"
  },
  {
    "arxiv_id": "2410.18481v2",
    "title": "Dialog2Flow: Pre-training Soft-Contrastive Action-Driven Sentence Embeddings for Automatic Dialog Flow Extraction",
    "authors": [
      "Sergio Burdisso",
      "Srikanth Madikeri",
      "Petr Motlicek"
    ],
    "abstract": "Efficiently deriving structured workflows from unannotated dialogs remains an\nunderexplored and formidable challenge in computational linguistics. Automating\nthis process could significantly accelerate the manual design of workflows in\nnew domains and enable the grounding of large language models in\ndomain-specific flowcharts, enhancing transparency and controllability. In this\npaper, we introduce Dialog2Flow (D2F) embeddings, which differ from\nconventional sentence embeddings by mapping utterances to a latent space where\nthey are grouped according to their communicative and informative functions\n(i.e., the actions they represent). D2F allows for modeling dialogs as\ncontinuous trajectories in a latent space with distinct action-related regions.\nBy clustering D2F embeddings, the latent space is quantized, and dialogs can be\nconverted into sequences of region/action IDs, facilitating the extraction of\nthe underlying workflow. To pre-train D2F, we build a comprehensive dataset by\nunifying twenty task-oriented dialog datasets with normalized per-turn action\nannotations. We also introduce a novel soft contrastive loss that leverages the\nsemantic information of these actions to guide the representation learning\nprocess, showing superior performance compared to standard supervised\ncontrastive loss. Evaluation against various sentence embeddings, including\ndialog-specific ones, demonstrates that D2F yields superior qualitative and\nquantitative results across diverse domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024 main conference",
    "pdf_url": "http://arxiv.org/pdf/2410.18481v2",
    "published_date": "2024-10-24 07:10:18 UTC",
    "updated_date": "2024-11-05 11:40:07 UTC"
  },
  {
    "arxiv_id": "2410.18475v2",
    "title": "Gene-Metabolite Association Prediction with Interactive Knowledge Transfer Enhanced Graph for Metabolite Production",
    "authors": [
      "Kexuan Xin",
      "Qingyun Wang",
      "Junyu Chen",
      "Pengfei Yu",
      "Huimin Zhao",
      "Heng Ji"
    ],
    "abstract": "In the rapidly evolving field of metabolic engineering, the quest for\nefficient and precise gene target identification for metabolite production\nenhancement presents significant challenges. Traditional approaches, whether\nknowledge-based or model-based, are notably time-consuming and labor-intensive,\ndue to the vast scale of research literature and the approximation nature of\ngenome-scale metabolic model (GEM) simulations. Therefore, we propose a new\ntask, Gene-Metabolite Association Prediction based on metabolic graphs, to\nautomate the process of candidate gene discovery for a given pair of metabolite\nand candidate-associated genes, as well as presenting the first benchmark\ncontaining 2474 metabolites and 1947 genes of two commonly used microorganisms\nSaccharomyces cerevisiae (SC) and Issatchenkia orientalis (IO). This task is\nchallenging due to the incompleteness of the metabolic graphs and the\nheterogeneity among distinct metabolisms. To overcome these limitations, we\npropose an Interactive Knowledge Transfer mechanism based on Metabolism Graph\n(IKT4Meta), which improves the association prediction accuracy by integrating\nthe knowledge from different metabolism graphs. First, to build a bridge\nbetween two graphs for knowledge transfer, we utilize Pretrained Language\nModels (PLMs) with external knowledge of genes and metabolites to help generate\ninter-graph links, significantly alleviating the impact of heterogeneity.\nSecond, we propagate intra-graph links from different metabolic graphs using\ninter-graph links as anchors. Finally, we conduct the gene-metabolite\nassociation prediction based on the enriched metabolism graphs, which integrate\nthe knowledge from multiple microorganisms. Experiments on both types of\norganisms demonstrate that our proposed methodology outperforms baselines by up\nto 12.3% across various link prediction frameworks.",
    "categories": [
      "cs.AI",
      "IEEEtran"
    ],
    "primary_category": "cs.AI",
    "comment": "10 PAGES, 4 FIGURES; bibm 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.18475v2",
    "published_date": "2024-10-24 06:54:27 UTC",
    "updated_date": "2024-10-31 05:56:03 UTC"
  },
  {
    "arxiv_id": "2410.18460v1",
    "title": "Beyond Multiple-Choice Accuracy: Real-World Challenges of Implementing Large Language Models in Healthcare",
    "authors": [
      "Yifan Yang",
      "Qiao Jin",
      "Qingqing Zhu",
      "Zhizheng Wang",
      "Francisco Erramuspe Álvarez",
      "Nicholas Wan",
      "Benjamin Hou",
      "Zhiyong Lu"
    ],
    "abstract": "Large Language Models (LLMs) have gained significant attention in the medical\ndomain for their human-level capabilities, leading to increased efforts to\nexplore their potential in various healthcare applications. However, despite\nsuch a promising future, there are multiple challenges and obstacles that\nremain for their real-world uses in practical settings. This work discusses key\nchallenges for LLMs in medical applications from four unique aspects:\noperational vulnerabilities, ethical and social considerations, performance and\nassessment difficulties, and legal and regulatory compliance. Addressing these\nchallenges is crucial for leveraging LLMs to their full potential and ensuring\ntheir responsible integration into healthcare.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18460v1",
    "published_date": "2024-10-24 06:12:03 UTC",
    "updated_date": "2024-10-24 06:12:03 UTC"
  },
  {
    "arxiv_id": "2410.18456v3",
    "title": "Progressive Curriculum Learning with Scale-Enhanced U-Net for Continuous Airway Segmentation",
    "authors": [
      "Bingyu Yang",
      "Qingyao Tian",
      "Huai Liao",
      "Xinyan Huang",
      "Jinlin Wu",
      "Jingdi Hu",
      "Hongbin Liu"
    ],
    "abstract": "Continuous and accurate segmentation of airways in chest CT images is\nessential for preoperative planning and real-time bronchoscopy navigation.\nDespite advances in deep learning for medical image segmentation, maintaining\nairway continuity remains a challenge, particularly due to intra-class\nimbalance between large and small branches and blurred CT scan details. To\naddress these challenges, we propose a progressive curriculum learning pipeline\nand a Scale-Enhanced U-Net (SE-UNet) to enhance segmentation continuity.\nSpecifically, our progressive curriculum learning pipeline consists of three\nstages: extracting main airways, identifying small airways, and repairing\ndiscontinuities. The cropping sampling strategy in each stage reduces feature\ninterference between airways of different scales, effectively addressing the\nchallenge of intra-class imbalance. In the third training stage, we present an\nAdaptive Topology-Responsive Loss (ATRL) to guide the network to focus on\nairway continuity. The progressive training pipeline shares the same SE-UNet,\nintegrating multi-scale inputs and Detail Information Enhancers (DIEs) to\nenhance information flow and effectively capture the intricate details of small\nairways. Additionally, we propose a robust airway tree parsing method and\nhierarchical evaluation metrics to provide more clinically relevant and precise\nanalysis. Experiments on both in-house and public datasets demonstrate that our\nmethod outperforms existing approaches, significantly improving the accuracy of\nsmall airways and the completeness of the airway tree. The code will be\nreleased upon publication.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18456v3",
    "published_date": "2024-10-24 06:10:09 UTC",
    "updated_date": "2025-02-28 15:04:56 UTC"
  },
  {
    "arxiv_id": "2410.18454v1",
    "title": "Verifying Non-friendly Formal Verification Designs: Can We Start Earlier?",
    "authors": [
      "Bryan Olmos",
      "Daniel Gerl",
      "Aman Kumar",
      "Djones Lettnin"
    ],
    "abstract": "The design of Systems on Chips (SoCs) is becoming more and more complex due\nto technological advancements. Missed bugs can cause drastic failures in\nsafety-critical environments leading to the endangerment of lives. To overcome\nthese drastic failures, formal property verification (FPV) has been applied in\nthe industry. However, there exist multiple hardware designs where the results\nof FPV are not conclusive even for long runtimes of model-checking tools. For\nthis reason, the use of High-level Equivalence Checking (HLEC) tools has been\nproposed in the last few years. However, the procedure for how to use it inside\nan industrial toolchain has not been defined. For this reason, we proposed an\nautomated methodology based on metamodeling techniques which consist of two\nmain steps. First, an untimed algorithmic description written in C++ is\nverified in an early stage using generated assertions; the advantage of this\nstep is that the assertions at the software level run in seconds and we can\nstart our analysis with conclusive results about our algorithm before starting\nto write the RTL (Register Transfer Level) design. Second, this algorithmic\ndescription is verified against its sequential design using HLEC and the\nrespective metamodel parameters. The results show that the presented\nmethodology can find bugs early related to the algorithmic description and\nprepare the setup for the HLEC verification. This helps to reduce the\nverification efforts to set up the tool and write the properties manually which\nis always error-prone. The proposed framework can help teams working on\ndatapaths to verify and make decisions in an early stage of the verification\nflow.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "Published in DVCon Europe 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.18454v1",
    "published_date": "2024-10-24 06:09:40 UTC",
    "updated_date": "2024-10-24 06:09:40 UTC"
  },
  {
    "arxiv_id": "2410.18451v1",
    "title": "Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs",
    "authors": [
      "Chris Yuhao Liu",
      "Liang Zeng",
      "Jiacai Liu",
      "Rui Yan",
      "Jujie He",
      "Chaojie Wang",
      "Shuicheng Yan",
      "Yang Liu",
      "Yahui Zhou"
    ],
    "abstract": "In this report, we introduce a collection of methods to enhance reward\nmodeling for LLMs, focusing specifically on data-centric techniques. We propose\neffective data selection and filtering strategies for curating high-quality\nopen-source preference datasets, culminating in the Skywork-Reward data\ncollection, which contains only 80K preference pairs -- significantly smaller\nthan existing datasets. Using this curated dataset, we developed the\nSkywork-Reward model series -- Skywork-Reward-Gemma-27B and\nSkywork-Reward-Llama-3.1-8B -- with the former currently holding the top\nposition on the RewardBench leaderboard. Notably, our techniques and datasets\nhave directly enhanced the performance of many top-ranked models on\nRewardBench, highlighting the practical impact of our contributions in\nreal-world preference learning applications.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18451v1",
    "published_date": "2024-10-24 06:06:26 UTC",
    "updated_date": "2024-10-24 06:06:26 UTC"
  },
  {
    "arxiv_id": "2410.18441v1",
    "title": "The Nature of Mathematical Modeling and Probabilistic Optimization Engineering in Generative AI",
    "authors": [
      "Fulu Li"
    ],
    "abstract": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.18441v1",
    "published_date": "2024-10-24 05:29:20 UTC",
    "updated_date": "2024-10-24 05:29:20 UTC"
  },
  {
    "arxiv_id": "2410.18406v1",
    "title": "MoMQ: Mixture-of-Experts Enhances Multi-Dialect Query Generation across Relational and Non-Relational Databases",
    "authors": [
      "Zhisheng Lin",
      "Yifu Liu",
      "Zhiling Luo",
      "Jinyang Gao",
      "Yu Li"
    ],
    "abstract": "The improvement in translating natural language to structured query language\n(SQL) can be attributed to the advancements in large language models (LLMs).\nOpen-source LLMs, tailored for specific database dialects such as MySQL, have\nshown great performance. However, cloud service providers are looking for a\nunified database manager service (e.g., Cosmos DB from Azure, Amazon Aurora\nfrom AWS, Lindorm from AlibabaCloud) that can support multiple dialects. This\nrequirement has led to the concept of multi-dialect query generation, which\npresents challenges to LLMs. These challenges include syntactic differences\namong dialects and imbalanced data distribution across multiple dialects. To\ntackle these challenges, we propose MoMQ, a novel Mixture-of-Experts-based\nmulti-dialect query generation framework across both relational and\nnon-relational databases. MoMQ employs a dialect expert group for each dialect\nand a multi-level routing strategy to handle dialect-specific knowledge,\nreducing interference during query generation. Additionally, a shared expert\ngroup is introduced to address data imbalance, facilitating the transfer of\ncommon knowledge from high-resource dialects to low-resource ones. Furthermore,\nwe have developed a high-quality multi-dialect query generation benchmark that\ncovers relational and non-relational databases such as MySQL, PostgreSQL,\nCypher for Neo4j, and nGQL for NebulaGraph. Extensive experiments have shown\nthat MoMQ performs effectively and robustly even in resource-imbalanced\nscenarios.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18406v1",
    "published_date": "2024-10-24 03:42:43 UTC",
    "updated_date": "2024-10-24 03:42:43 UTC"
  },
  {
    "arxiv_id": "2410.18385v2",
    "title": "Link, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval",
    "authors": [
      "Dae Yon Hwang",
      "Bilal Taha",
      "Harshit Pande",
      "Yaroslav Nechaev"
    ],
    "abstract": "Despite the recent advancements in information retrieval (IR), zero-shot IR\nremains a significant challenge, especially when dealing with new domains,\nlanguages, and newly-released use cases that lack historical query traffic from\nexisting users. For such cases, it is common to use query augmentations\nfollowed by fine-tuning pre-trained models on the document data paired with\nsynthetic queries. In this work, we propose a novel Universal Document Linking\n(UDL) algorithm, which links similar documents to enhance synthetic query\ngeneration across multiple datasets with different characteristics. UDL\nleverages entropy for the choice of similarity models and named entity\nrecognition (NER) for the link decision of documents using similarity scores.\nOur empirical studies demonstrate the effectiveness and universality of the UDL\nacross diverse datasets and IR models, surpassing state-of-the-art methods in\nzero-shot cases. The developed code for reproducibility is included in\nhttps://github.com/eoduself/UDL",
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication at EMNLP 2024 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2410.18385v2",
    "published_date": "2024-10-24 02:52:19 UTC",
    "updated_date": "2024-10-25 02:20:12 UTC"
  },
  {
    "arxiv_id": "2410.18374v1",
    "title": "Integrating Canonical Neural Units and Multi-Scale Training for Handwritten Text Recognition",
    "authors": [
      "Zi-Rui Wang"
    ],
    "abstract": "The segmentation-free research efforts for addressing handwritten text\nrecognition can be divided into three categories: connectionist temporal\nclassification (CTC), hidden Markov model and encoder-decoder methods. In this\npaper, inspired by the above three modeling methods, we propose a new\nrecognition network by using a novel three-dimensional (3D) attention module\nand global-local context information. Based on the feature maps of the last\nconvolutional layer, a series of 3D blocks with different resolutions are\nsplit. Then, these 3D blocks are fed into the 3D attention module to generate\nsequential visual features. Finally, by integrating the visual features and the\ncorresponding global-local context features, a well-designed representation can\nbe obtained. Main canonical neural units including attention mechanisms,\nfully-connected layer, recurrent unit and convolutional layer are efficiently\norganized into a network and can be jointly trained by the CTC loss and the\ncross-entropy loss. Experiments on the latest Chinese handwritten text datasets\n(the SCUT-HCCDoc and the SCUT-EPT) and one English handwritten text dataset\n(the IAM) show that the proposed method can make a new milestone.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18374v1",
    "published_date": "2024-10-24 02:33:12 UTC",
    "updated_date": "2024-10-24 02:33:12 UTC"
  },
  {
    "arxiv_id": "2410.18371v2",
    "title": "Gibberish is All You Need for Membership Inference Detection in Contrastive Language-Audio Pretraining",
    "authors": [
      "Ruoxi Cheng",
      "Yizhong Ding",
      "Shuirong Cao",
      "Shitong Shao",
      "Zhiqiang Wang"
    ],
    "abstract": "Audio can disclose PII, particularly when combined with related text data.\nTherefore, it is essential to develop tools to detect privacy leakage in\nContrastive Language-Audio Pretraining(CLAP). Existing MIAs need audio as\ninput, risking exposure of voiceprint and requiring costly shadow models. We\nfirst propose PRMID, a membership inference detector based probability ranking\ngiven by CLAP, which does not require training shadow models but still requires\nboth audio and text of the individual as input. To address these limitations,\nwe then propose USMID, a textual unimodal speaker-level membership inference\ndetector, querying the target model using only text data. We randomly generate\ntextual gibberish that are clearly not in training dataset. Then we extract\nfeature vectors from these texts using the CLAP model and train a set of\nanomaly detectors on them. During inference, the feature vector of each test\ntext is input into the anomaly detector to determine if the speaker is in the\ntraining set (anomalous) or not (normal). If available, USMID can further\nenhance detection by integrating real audio of the tested speaker. Extensive\nexperiments on various CLAP model architectures and datasets demonstrate that\nUSMID outperforms baseline methods using only text data.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18371v2",
    "published_date": "2024-10-24 02:26:57 UTC",
    "updated_date": "2024-11-02 10:00:52 UTC"
  },
  {
    "arxiv_id": "2410.18368v1",
    "title": "Multi-objective Optimization in CPU Design Space Exploration: Attention is All You Need",
    "authors": [
      "Runzhen Xue",
      "Hao Wu",
      "Mingyu Yan",
      "Ziheng Xiao",
      "Xiaochun Ye",
      "Dongrui Fan"
    ],
    "abstract": "Design space exploration (DSE) enables architects to systematically evaluate\nvarious design options, guiding decisions on the most suitable configurations\nto meet specific objectives such as optimizing performance, power, and area.\nHowever, the growing complexity of modern CPUs has dramatically increased the\nnumber of micro-architectural parameters and expanded the overall design space,\nmaking DSE more challenging and time-consuming. Existing DSE frameworks\nstruggle in large-scale design spaces due to inaccurate models and limited\ninsights into parameter impact, hindering efficient identification of optimal\nmicro-architectures within tight timeframes.\n  In this work, we introduce AttentionDSE. Its key idea is to use the attention\nmechanism to establish a direct mapping of micro-architectural parameters to\ntheir contributions to predicted performance. This approach enhances both the\nprediction accuracy and interpretability of the performance model. Furthermore,\nthe weights are dynamically adjusted, enabling the model to respond to design\nchanges and effectively pinpoint the key micro-architectural\nparameters/components responsible for performance bottlenecks. Thus,\nAttentionDSE accurately, purposefully, and rapidly discovers optimal designs.\nExperiments on SPEC 2017 demonstrate that AttentionDSE significantly reduces\nexploration time by over 80\\% and achieves 3.9\\% improvement in Pareto\nHypervolume compared to state-of-the-art DSE frameworks while maintaining\nsuperior prediction accuracy and efficiency with an increasing number of\nparameters.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18368v1",
    "published_date": "2024-10-24 02:20:17 UTC",
    "updated_date": "2024-10-24 02:20:17 UTC"
  },
  {
    "arxiv_id": "2410.18363v1",
    "title": "Contextual Biasing to Improve Domain-specific Custom Vocabulary Audio Transcription without Explicit Fine-Tuning of Whisper Model",
    "authors": [
      "Vishakha Lall",
      "Yisi Liu"
    ],
    "abstract": "OpenAI's Whisper Automated Speech Recognition model excels in generalizing\nacross diverse datasets and domains. However, this broad adaptability can lead\nto diminished performance in tasks requiring recognition of specific\nvocabularies. Addressing this challenge typically involves fine-tuning the\nmodel, which demands extensive labeled audio data that is often difficult to\nacquire and unavailable for specific domains. In this study, we propose a\nmethod to enhance transcription accuracy without explicit fine-tuning or\naltering model parameters, using a relatively small training dataset. Our\nmethod leverages contextual biasing, to direct Whisper model's output towards a\nspecific vocabulary by integrating a neural-symbolic prefix tree structure to\nguide the model's transcription output. To validate our approach, we conducted\nexperiments using a validation dataset comprising maritime data collected\nwithin a simulated training environment. A comparison between the original\nWhisper models of varying parameter sizes and our biased model revealed a\nnotable reduction in transcription word error rate and enhanced performance of\ndownstream applications. Our findings suggest that this methodology holds\npromise for improving speech-to-text translation performance in domains\ncharacterized by limited vocabularies.",
    "categories": [
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18363v1",
    "published_date": "2024-10-24 01:58:11 UTC",
    "updated_date": "2024-10-24 01:58:11 UTC"
  },
  {
    "arxiv_id": "2410.18345v1",
    "title": "Geometric Feature Enhanced Knowledge Graph Embedding and Spatial Reasoning",
    "authors": [
      "Lei Hu",
      "Wenwen Li",
      "Yunqiang Zhu"
    ],
    "abstract": "Geospatial Knowledge Graphs (GeoKGs) model geoentities (e.g., places and\nnatural features) and spatial relationships in an interconnected manner,\nproviding strong knowledge support for geographic applications, including data\nretrieval, question-answering, and spatial reasoning. However, existing methods\nfor mining and reasoning from GeoKGs, such as popular knowledge graph embedding\n(KGE) techniques, lack geographic awareness. This study aims to enhance\ngeneral-purpose KGE by developing new strategies and integrating geometric\nfeatures of spatial relations, including topology, direction, and distance, to\ninfuse the embedding process with geographic intuition. The new model is tested\non downstream link prediction tasks, and the results show that the inclusion of\ngeometric features, particularly topology and direction, improves prediction\naccuracy for both geoentities and spatial relations. Our research offers new\nperspectives for integrating spatial concepts and principles into the GeoKG\nmining process, providing customized GeoAI solutions for geospatial challenges.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "4 pages, 1 figure, Accepted for the 7th ACM SIGSPATIAL International\n  Workshop on AI for Geographic Knowledge Discovery",
    "pdf_url": "http://arxiv.org/pdf/2410.18345v1",
    "published_date": "2024-10-24 00:53:48 UTC",
    "updated_date": "2024-10-24 00:53:48 UTC"
  },
  {
    "arxiv_id": "2410.18344v1",
    "title": "Aggregated Knowledge Model: Enhancing Domain-Specific QA with Fine-Tuned and Retrieval-Augmented Generation Models",
    "authors": [
      "Fengchen Liu",
      "Jordan Jung",
      "Wei Feinstein",
      "Jeff DAmbrogia",
      "Gary Jung"
    ],
    "abstract": "This paper introduces a novel approach to enhancing closed-domain Question\nAnswering (QA) systems, focusing on the specific needs of the Lawrence Berkeley\nNational Laboratory (LBL) Science Information Technology (ScienceIT) domain.\nUtilizing a rich dataset derived from the ScienceIT documentation, our study\nembarks on a detailed comparison of two fine-tuned large language models and\nfive retrieval-augmented generation (RAG) models. Through data processing\ntechniques, we transform the documentation into structured\ncontext-question-answer triples, leveraging the latest Large Language Models\n(AWS Bedrock, GCP PaLM2, Meta LLaMA2, OpenAI GPT-4, Google Gemini-Pro) for\ndata-driven insights. Additionally, we introduce the Aggregated Knowledge Model\n(AKM), which synthesizes responses from the seven models mentioned above using\nK-means clustering to select the most representative answers. The evaluation of\nthese models across multiple metrics offers a comprehensive look into their\neffectiveness and suitability for the LBL ScienceIT environment. The results\ndemonstrate the potential benefits of integrating fine-tuning and\nretrieval-augmented strategies, highlighting significant performance\nimprovements achieved with the AKM. The insights gained from this study can be\napplied to develop specialized QA systems tailored to specific domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18344v1",
    "published_date": "2024-10-24 00:49:46 UTC",
    "updated_date": "2024-10-24 00:49:46 UTC"
  },
  {
    "arxiv_id": "2410.18336v1",
    "title": "Assessing the Creativity of LLMs in Proposing Novel Solutions to Mathematical Problems",
    "authors": [
      "Junyi Ye",
      "Jingyi Gu",
      "Xinyun Zhao",
      "Wenpeng Yin",
      "Guiling Wang"
    ],
    "abstract": "The mathematical capabilities of AI systems are complex and multifaceted.\nMost existing research has predominantly focused on the correctness of\nAI-generated solutions to mathematical problems. In this work, we argue that\nbeyond producing correct answers, AI systems should also be capable of, or\nassist humans in, developing novel solutions to mathematical challenges. This\nstudy explores the creative potential of Large Language Models (LLMs) in\nmathematical reasoning, an aspect that has received limited attention in prior\nresearch. We introduce a novel framework and benchmark, CreativeMath, which\nencompasses problems ranging from middle school curricula to Olympic-level\ncompetitions, designed to assess LLMs' ability to propose innovative solutions\nafter some known solutions have been provided. Our experiments demonstrate\nthat, while LLMs perform well on standard mathematical tasks, their capacity\nfor creative problem-solving varies considerably. Notably, the Gemini-1.5-Pro\nmodel outperformed other LLMs in generating novel solutions. This research\nopens a new frontier in evaluating AI creativity, shedding light on both the\nstrengths and limitations of LLMs in fostering mathematical innovation, and\nsetting the stage for future developments in AI-assisted mathematical\ndiscovery.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18336v1",
    "published_date": "2024-10-24 00:12:49 UTC",
    "updated_date": "2024-10-24 00:12:49 UTC"
  },
  {
    "arxiv_id": "2410.18333v2",
    "title": "Search-Based Path Planning in Interactive Environments among Movable Obstacles",
    "authors": [
      "Zhongqiang Ren",
      "Bunyod Suvonov",
      "Guofei Chen",
      "Botao He",
      "Yijie Liao",
      "Cornelia Fermuller",
      "Ji Zhang"
    ],
    "abstract": "This paper investigates Path planning Among Movable Obstacles (PAMO), which\nseeks a minimum cost collision-free path among static obstacles from start to\ngoal while allowing the robot to push away movable obstacles (i.e., objects)\nalong its path when needed. To develop planners that are complete and optimal\nfor PAMO, the planner has to search a giant state space involving both the\nlocation of the robot as well as the locations of the objects, which grows\nexponentially with respect to the number of objects. This paper leverages a\nsimple yet under-explored idea that, only a small fraction of this giant state\nspace needs to be searched during planning as guided by a heuristic, and most\nof the objects far away from the robot are intact, which thus leads to runtime\nefficient algorithms. Based on this idea, this paper introduces two PAMO\nformulations, i.e., bi-objective and resource constrained problems in an\noccupancy grid, and develops PAMO*, a planning method with completeness and\nsolution optimality guarantees, to solve the two problems. We then further\nextend PAMO* to hybrid-state PAMO* to plan in continuous spaces with\nhigh-fidelity interaction between the robot and the objects. Our results show\nthat, PAMO* can often find optimal solutions within a second in cluttered maps\nwith up to 400 objects.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted by ICRA 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.18333v2",
    "published_date": "2024-10-24 00:02:58 UTC",
    "updated_date": "2025-03-06 05:04:03 UTC"
  }
]
{
  "date": "2025-03-18",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-18 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 138 篇论文，主要聚焦 AI 在机器人、医疗和生成模型等领域的创新应用，亮点包括 NVIDIA 多款物理 AI 模型的发布，以及 LLM 在决策和生成任务中的增强框架；令人印象深刻的是 NVIDIA 作者的多篇高影响力工作，以及 LLM 在医疗诊断和多模态生成中的突破性进展。\n\n### 重点论文聚焦\n以下挑选并简要讨论部分重要、话题度和影响高的论文，先从 AI 安全和 LLM 应用入手，再聊机器人和医疗创新，最后快速概述其他有潜力的工作。相关论文按主题归类讨论。\n\n**AI 安全与 LLM 增强（高话题度，涉及知名学者如 NVIDIA）**  \n- **DeepSeek-R1 的安全评估 (Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts)**: 这篇由 Google 相关作者的论文评估了 DeepSeek-R1 模型在中文语境下的安全漏洞，通过实验发现模型在有害提示下的攻击成功率高达 100%，并提出增强策略，显著提高了模型的安全性和鲁棒性，强调了 AI 在多语言环境中的实际部署挑战。  \n- **GraphRAG-FI: 图知识增强的检索生成框架 (GraphRAG with Knowledge Filtering and Integration)**: 这篇论文引入知识过滤和集成机制，提升了 GraphRAG 在复杂查询中的性能，作者通过实验证明了其在知识图 QA 任务中的优势，提供了更可靠的 LLM 辅助决策方法。  \n- **MDTeamGPT: LLM 多代理框架用于医疗咨询 (MDTeamGPT: A Self-Evolving LLM-based Multi-Agent Framework for Multi-Disciplinary Team Medical Consultation)**: 作者提出一个自演化多代理框架，利用 LLM 模拟多学科医疗团队，实验显示在 MedQA 和 PubMedQA 数据集上准确率达 90%，这篇工作突出了 LLM 在医疗决策中的潜力。  \n- **VARP: LLM 反馈强化学习 (VARP: Reinforcement Learning from Vision-Language Model Feedback with Agent Regularized Preferences)**: 这篇论文使用 VLM 反馈优化强化学习，实验在元世界任务中提升了 20-30% 的回报率，展示了 LLM 在机器人决策中的实用性。\n\n**机器人与导航创新（NVIDIA 等知名团队主导，令人印象深刻）**  \n- **Cosmos-Reason1: 物理 AI 推理模型 (Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning)**: NVIDIA 团队的作品，提出一个多模态 LLM 用于物理世界推理，训练数据包括真实机器人轨迹，实验在物理基准上显著提升性能，是 AI 机器人领域的关键进展。  \n- **GR00T N1: 人形机器人基础模型 (GR00T N1: An Open Foundation Model for Generalist Humanoid Robots)**: 另一篇 NVIDIA 论文，构建了一个视觉-语言-动作模型，支持双臂操作，实验在模拟和真实机器人上超越基线，展示了通用机器人自治的潜力。  \n- **Cosmos-Transfer1: 条件世界生成模型 (Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control)**: NVIDIA 作者开发的模型，支持多模态条件生成，应用于机器人模拟和自动驾驶，实验证明其在 Sim2Real 任务中的有效性。  \n- **WebNav: 语音控制网页导航代理 (WebNav: An Intelligent Agent for Voice-Controlled Web Navigation)**: 这篇工作提出一个 ReAct 架构的代理，支持语音导航，实验显示在视觉障碍用户场景下响应时间和任务完成率优于传统屏幕阅读器。  \n- **TGBFormer: 视频对象检测网络 (TGBFormer: Transformer-GraphFormer Blender Network for Video Object Detection)**: 作者融合 Transformer 和 GraphFormer 提升视频检测，实验在 ImageNet VID 上达到 86.5% mAP 和 41 FPS，展示了高效的多模态融合。\n\n**医疗与生物应用（实际影响大，快速讨论贡献）**  \n- **MDImageBench: 差分隐私图像合成基准 (DPImageBench: A Unified Benchmark for Differentially Private Image Synthesis)**: 这篇论文构建了一个统一基准评估差分隐私图像合成，实验揭示预训练数据分布对性能的影响，提供了医疗图像隐私保护的新洞见。  \n- **PHGNN: 超图神经网络用于阿尔茨海默诊断 (PHGNN: A Novel Prompted Hypergraph Neural Network to Diagnose Alzheimer's Disease)**: 作者使用超图和提示学习提升诊断准确性，实验在 ADNI 数据集上超越 SOTA，突出了 AI 在神经疾病诊断中的潜力。  \n- **ExDDV: 解释性深度伪造检测数据集 (ExDDV: A New Dataset for Explainable Deepfake Detection in Video)**: 这篇工作发布了一个新数据集，支持视频深度伪造的可解释检测，实验显示模型在伪造检测上精度提升，适用于医疗成像安全。\n\n其他论文中，有一些探索性工作如时间序列建模（e.g., **ON-Traffic: 时间序列流量估计**，提出操作符网络优化交通预测）和生成模型（e.g., **SALAD: 文本驱动动作生成**，使用扩散模型提升视频编辑），但这些相对常规，仅快速提及其核心贡献：前者通过不确定性量化提升交通建模精度，后者实现了更精细的动作生成。其他纯理论或小众论文（如数学证明或特定算法优化）则不做详细展开，仅作为补充。\n\n总之，今天的 arXiv 更新突显了 AI 在实际应用中的进展，尤其是 LLM 和机器人领域的创新，NVIDIA 的多篇论文值得关注，期待这些工作推动更可靠的 AI 系统。明天见！",
  "papers": [
    {
      "arxiv_id": "2503.14783v1",
      "title": "RAT: Boosting Misclassification Detection Ability without Extra Data",
      "title_zh": "翻译失败",
      "authors": [
        "Ge Yan",
        "Tsui-Wei Weng"
      ],
      "abstract": "As deep neural networks(DNN) become increasingly prevalent, particularly in\nhigh-stakes areas such as autonomous driving and healthcare, the ability to\ndetect incorrect predictions of models and intervene accordingly becomes\ncrucial for safety. In this work, we investigate the detection of misclassified\ninputs for image classification models from the lens of adversarial\nperturbation: we propose to use robust radius (a.k.a. input-space margin) as a\nconfidence metric and design two efficient estimation algorithms, RR-BS and\nRR-Fast, for misclassification detection. Furthermore, we design a training\nmethod called Radius Aware Training (RAT) to boost models' ability to identify\nmistakes. Extensive experiments show our method could achieve up to 29.3%\nreduction on AURC and 21.62% reduction in FPR@95TPR, compared with previous\nmethods.",
      "tldr_zh": "本文研究了深度神经网络(DNN)在高风险领域如自动驾驶和医疗中检测误分类输入的重要性，提出使用 robust radius（输入空间边距）作为置信度指标，并设计了两种高效估计算法：RR-BS 和 RR-Fast，以从对抗性扰动的角度提升图像分类模型的误分类检测能力。同时，引入了 Radius Aware Training (RAT) 训练方法来进一步强化模型的错误识别性能。实验结果显示，与现有方法相比，RAT 实现了 AURC 减少29.3%和 FPR@95TPR 减少21.62%的显著改进，且无需额外数据。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14783v1",
      "published_date": "2025-03-18 23:18:55 UTC",
      "updated_date": "2025-03-18 23:18:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:22:47.779270"
    },
    {
      "arxiv_id": "2503.14779v1",
      "title": "Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution",
      "title_zh": "Involution and BSConv 多深度蒸馏网络用于轻量级图像超分辨率",
      "authors": [
        "Akram Khatami-Rizi",
        "Ahmad Mahmoudi-Aznaveh"
      ],
      "abstract": "Single Image Super-Resolution (SISR) aims to reconstruct high-resolution (HR)\nimages from low-resolution (LR) inputs. Deep learning, especially Convolutional\nNeural Networks (CNNs), has advanced SISR. However, increasing network depth\nincreases parameters, and memory usage, and slows training, which is\nproblematic for resource-limited devices. To address this, lightweight models\nare developed to balance accuracy and efficiency. We propose the Involution &\nBSConv Multi-Depth Distillation Network (IBMDN), combining Involution & BSConv\nMulti-Depth Distillation Block (IBMDB) and the Contrast and High-Frequency\nAttention Block (CHFAB). IBMDB integrates Involution and BSConv to balance\ncomputational efficiency and feature extraction. CHFAB enhances high-frequency\ndetails for better visual quality. IBMDB is compatible with other SISR\narchitectures and reduces complexity, improving evaluation metrics like PSNR\nand SSIM. In transformer-based models, IBMDB reduces memory usage while\nimproving feature extraction. In GANs, it enhances perceptual quality,\nbalancing pixel-level accuracy with perceptual details. Our experiments show\nthat the method achieves high accuracy with minimal computational cost. The\ncode is available at GitHub.",
      "tldr_zh": "本研究针对单图像超分辨率 (SISR) 的挑战，提出了一种轻量级网络 Involution and BSConv Multi-Depth Distillation Network (IBMDN)，旨在平衡准确性和计算效率。IBMDN 整合了 Involution 和 BSConv 的 Multi-Depth Distillation Block (IBMDB) 来优化特征提取，同时引入 Contrast and High-Frequency Attention Block (CHFAB) 以增强高频细节和视觉质量。实验结果显示，IBMDN 与其他 SISR 架构兼容，能显著提高 PSNR 和 SSIM 等指标，同时在 Transformer-based 模型中降低内存使用，在 GANs 中提升感知质量，实现高准确性与低计算成本。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14779v1",
      "published_date": "2025-03-18 23:10:08 UTC",
      "updated_date": "2025-03-18 23:10:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:22:59.163473"
    },
    {
      "arxiv_id": "2503.15558v3",
      "title": "Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning",
      "title_zh": "Cosmos-Reason1：从物理常识到具身推理",
      "authors": [
        "NVIDIA",
        ":",
        "Alisson Azzolini",
        "Junjie Bai",
        "Hannah Brandon",
        "Jiaxin Cao",
        "Prithvijit Chattopadhyay",
        "Huayu Chen",
        "Jinju Chu",
        "Yin Cui",
        "Jenna Diamond",
        "Yifan Ding",
        "Liang Feng",
        "Francesco Ferroni",
        "Rama Govindaraju",
        "Jinwei Gu",
        "Siddharth Gururani",
        "Imad El Hanafi",
        "Zekun Hao",
        "Jacob Huffman",
        "Jingyi Jin",
        "Brendan Johnson",
        "Rizwan Khan",
        "George Kurian",
        "Elena Lantz",
        "Nayeon Lee",
        "Zhaoshuo Li",
        "Xuan Li",
        "Maosheng Liao",
        "Tsung-Yi Lin",
        "Yen-Chen Lin",
        "Ming-Yu Liu",
        "Xiangyu Lu",
        "Alice Luo",
        "Andrew Mathau",
        "Yun Ni",
        "Lindsey Pavao",
        "Wei Ping",
        "David W. Romero",
        "Misha Smelyanskiy",
        "Shuran Song",
        "Lyne Tchapmi",
        "Andrew Z. Wang",
        "Boxin Wang",
        "Haoxiang Wang",
        "Fangyin Wei",
        "Jiashu Xu",
        "Yao Xu",
        "Dinghao Yang",
        "Xiaodong Yang",
        "Zhuolin Yang",
        "Jingxu Zhang",
        "Xiaohui Zeng",
        "Zhe Zhang"
      ],
      "abstract": "Physical AI systems need to perceive, understand, and perform complex actions\nin the physical world. In this paper, we present the Cosmos-Reason1 models that\ncan understand the physical world and generate appropriate embodied decisions\n(e.g., next step action) in natural language through long chain-of-thought\nreasoning processes. We begin by defining key capabilities for Physical AI\nreasoning, with a focus on physical common sense and embodied reasoning. To\nrepresent physical common sense, we use a hierarchical ontology that captures\nfundamental knowledge about space, time, and physics. For embodied reasoning,\nwe rely on a two-dimensional ontology that generalizes across different\nphysical embodiments. Building on these capabilities, we develop two multimodal\nlarge language models, Cosmos-Reason1-7B and Cosmos-Reason1-56B. We curate data\nand train our models in two stages: Physical AI supervised fine-tuning (SFT)\nand Physical AI reinforcement learning (RL). To evaluate our models, we build\ncomprehensive benchmarks for physical common sense and embodied reasoning\naccording to our ontologies. Evaluation results show that Physical AI SFT and\nRL bring significant improvements. To facilitate the development of Physical\nAI, we make our code and pre-trained models available under the NVIDIA Open\nModel License at https://github.com/nvidia-cosmos/cosmos-reason1.",
      "tldr_zh": "本研究介绍了 Cosmos-Reason1 模型系列，旨在增强物理 AI 系统对物理世界的感知和理解，并通过长链式思维推理生成自然语言的具身决策（如下一步行动）。该模型使用分层本体表示物理常识（涉及空间、时间和物理知识），以及二维本体处理不同具身形式的一般化推理；随后通过 Physical AI Supervised Fine-Tuning (SFT) 和 Reinforcement Learning (RL) 两阶段训练两个多模态大语言模型（Cosmos-Reason1-7B 和 Cosmos-Reason1-56B）。实验结果显示，SFT 和 RL 显著提升了模型在物理常识和具身推理基准上的性能，并开源了代码和预训练模型以促进相关领域发展。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15558v3",
      "published_date": "2025-03-18 22:06:58 UTC",
      "updated_date": "2025-05-19 17:59:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:23:11.518967"
    },
    {
      "arxiv_id": "2503.14755v1",
      "title": "Language Independent Named Entity Recognition via Orthogonal Transformation of Word Vectors",
      "title_zh": "通过词向量的正交变换实现语言无关的命名实体识别",
      "authors": [
        "Omar E. Rakha",
        "Hazem M. Abbas"
      ],
      "abstract": "Word embeddings have been a key building block for NLP in which models relied\nheavily on word embeddings in many different tasks. In this paper, a model is\nproposed based on using Bidirectional LSTM/CRF with word embeddings to perform\nnamed entity recognition for any language. This is done by training a model on\na source language (English) and transforming word embeddings from the target\nlanguage into word embeddings of the source language by using an orthogonal\nlinear transformation matrix. Evaluation of the model shows that by training a\nmodel on an English dataset the model was capable of detecting named entities\nin an Arabic dataset without neither training or fine tuning the model on an\nArabic language dataset.",
      "tldr_zh": "该论文提出了一种语言无关的命名实体识别（Named Entity Recognition）方法，使用 Bidirectional LSTM/CRF 和词嵌入（Word Embeddings）作为核心组件。方法的核心在于，通过训练一个源语言（英语）模型，并应用正交线性变换矩阵（Orthogonal Linear Transformation Matrix）将目标语言的词向量转换为源语言的词向量，从而实现跨语言的实体检测。实验结果表明，该模型在英语数据集上训练后，能够直接在阿拉伯语数据集上准确识别命名实体，而无需针对目标语言进行额外训练或微调。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Paper was initially released in 2017 but was never published",
      "pdf_url": "http://arxiv.org/pdf/2503.14755v1",
      "published_date": "2025-03-18 21:57:58 UTC",
      "updated_date": "2025-03-18 21:57:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:23:23.025196"
    },
    {
      "arxiv_id": "2503.14754v2",
      "title": "Bayesian Modeling of Zero-Shot Classifications for Urban Flood Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Matt Franchi",
        "Nikhil Garg",
        "Wendy Ju",
        "Emma Pierson"
      ],
      "abstract": "Street scene datasets, collected from Street View or dashboard cameras, offer\na promising means of detecting urban objects and incidents like street\nflooding. However, a major challenge in using these datasets is their lack of\nreliable labels: there are myriad types of incidents, many types occur rarely,\nand ground-truth measures of where incidents occur are lacking. Here, we\npropose BayFlood, a two-stage approach which circumvents this difficulty.\nFirst, we perform zero-shot classification of where incidents occur using a\npretrained vision-language model (VLM). Second, we fit a spatial Bayesian model\non the VLM classifications. The zero-shot approach avoids the need to annotate\nlarge training sets, and the Bayesian model provides frequent desiderata in\nurban settings - principled measures of uncertainty, smoothing across\nlocations, and incorporation of external data like stormwater accumulation\nzones. We comprehensively validate this two-stage approach, showing that VLMs\nprovide strong zero-shot signal for floods across multiple cities and time\nperiods, the Bayesian model improves out-of-sample prediction relative to\nbaseline methods, and our inferred flood risk correlates with known external\npredictors of risk. Having validated our approach, we show it can be used to\nimprove urban flood detection: our analysis reveals 113,738 people who are at\nhigh risk of flooding overlooked by current methods, identifies demographic\nbiases in existing methods, and suggests locations for new flood sensors. More\nbroadly, our results showcase how Bayesian modeling of zero-shot LM annotations\nrepresents a promising paradigm because it avoids the need to collect large\nlabeled datasets and leverages the power of foundation models while providing\nthe expressiveness and uncertainty quantification of Bayesian models.",
      "tldr_zh": "这篇论文提出 BayFlood，一种两阶段方法，用于城市洪水检测：首先利用预训练的视觉语言模型 (VLM) 进行零样本 (zero-shot) 分类，以识别街景数据集中的洪水事件；其次，基于这些分类拟合空间 Bayesian 模型，提供不确定性量化、位置平滑和外部数据整合，如暴雨积水区。实验验证显示，VLM 在多个城市和时间段提供强有力的零样本信号，而 Bayesian 模型相对于基线方法显著提高了预测准确性，并与已知风险预测相关。该方法的应用揭示了 113,738 人被现有方法忽略的高风险人群、现有方法的 demographic biases，并建议潜在的洪水传感器位置。更广泛地，这展示了 Bayesian 建模零样本注释的范式，可避免收集大型标注数据集，同时利用基础模型的优势并提供不确定性量化。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "In review",
      "pdf_url": "http://arxiv.org/pdf/2503.14754v2",
      "published_date": "2025-03-18 21:53:37 UTC",
      "updated_date": "2025-03-26 12:25:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:23:36.545986"
    },
    {
      "arxiv_id": "2503.14751v1",
      "title": "LipShiFT: A Certifiably Robust Shift-based Vision Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Rohan Menon",
        "Nicola Franco",
        "Stephan Günnemann"
      ],
      "abstract": "Deriving tight Lipschitz bounds for transformer-based architectures presents\na significant challenge. The large input sizes and high-dimensional attention\nmodules typically prove to be crucial bottlenecks during the training process\nand leads to sub-optimal results. Our research highlights practical constraints\nof these methods in vision tasks. We find that Lipschitz-based margin training\nacts as a strong regularizer while restricting weights in successive layers of\nthe model. Focusing on a Lipschitz continuous variant of the ShiftViT model, we\naddress significant training challenges for transformer-based architectures\nunder norm-constrained input setting. We provide an upper bound estimate for\nthe Lipschitz constants of this model using the $l_2$ norm on common image\nclassification datasets. Ultimately, we demonstrate that our method scales to\nlarger models and advances the state-of-the-art in certified robustness for\ntransformer-based architectures.",
      "tldr_zh": "本研究提出 LipShiFT，一种可认证鲁棒的基于 Shift 的 Vision Transformer，旨在解决 transformer 架构在推导紧密 Lipschitz bounds 时面临的训练瓶颈和次优结果问题。\n他们采用 Lipschitz-based margin training 作为强正则化器，限制模型后续层的权重，并在规范约束输入设置下处理 ShiftViT 模型的训练挑战。\n通过使用 $l_2$ 范数对模型的 Lipschitz 常数提供上界估计，实验在常见图像分类数据集上证明了该方法的有效性，并实现了 transformer 架构的 certified robustness state-of-the-art 水平。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025 Workshop: VerifAI: AI Verification in the Wild",
      "pdf_url": "http://arxiv.org/pdf/2503.14751v1",
      "published_date": "2025-03-18 21:38:18 UTC",
      "updated_date": "2025-03-18 21:38:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:23:47.768346"
    },
    {
      "arxiv_id": "2503.14734v2",
      "title": "GR00T N1: An Open Foundation Model for Generalist Humanoid Robots",
      "title_zh": "GR00T N1：开源基础模型，用于通用人形机器人",
      "authors": [
        "NVIDIA",
        ":",
        "Johan Bjorck",
        "Fernando Castañeda",
        "Nikita Cherniadev",
        "Xingye Da",
        "Runyu Ding",
        "Linxi \"Jim\" Fan",
        "Yu Fang",
        "Dieter Fox",
        "Fengyuan Hu",
        "Spencer Huang",
        "Joel Jang",
        "Zhenyu Jiang",
        "Jan Kautz",
        "Kaushil Kundalia",
        "Lawrence Lao",
        "Zhiqi Li",
        "Zongyu Lin",
        "Kevin Lin",
        "Guilin Liu",
        "Edith Llontop",
        "Loic Magne",
        "Ajay Mandlekar",
        "Avnish Narayan",
        "Soroush Nasiriany",
        "Scott Reed",
        "You Liang Tan",
        "Guanzhi Wang",
        "Zu Wang",
        "Jing Wang",
        "Qi Wang",
        "Jiannan Xiang",
        "Yuqi Xie",
        "Yinzhen Xu",
        "Zhenjia Xu",
        "Seonghyeon Ye",
        "Zhiding Yu",
        "Ao Zhang",
        "Hao Zhang",
        "Yizhou Zhao",
        "Ruijie Zheng",
        "Yuke Zhu"
      ],
      "abstract": "General-purpose robots need a versatile body and an intelligent mind. Recent\nadvancements in humanoid robots have shown great promise as a hardware platform\nfor building generalist autonomy in the human world. A robot foundation model,\ntrained on massive and diverse data sources, is essential for enabling the\nrobots to reason about novel situations, robustly handle real-world\nvariability, and rapidly learn new tasks. To this end, we introduce GR00T N1,\nan open foundation model for humanoid robots. GR00T N1 is a\nVision-Language-Action (VLA) model with a dual-system architecture. The\nvision-language module (System 2) interprets the environment through vision and\nlanguage instructions. The subsequent diffusion transformer module (System 1)\ngenerates fluid motor actions in real time. Both modules are tightly coupled\nand jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixture\nof real-robot trajectories, human videos, and synthetically generated datasets.\nWe show that our generalist robot model GR00T N1 outperforms the\nstate-of-the-art imitation learning baselines on standard simulation benchmarks\nacross multiple robot embodiments. Furthermore, we deploy our model on the\nFourier GR-1 humanoid robot for language-conditioned bimanual manipulation\ntasks, achieving strong performance with high data efficiency.",
      "tldr_zh": "该研究引入了 GR00T N1，一种开源的 Vision-Language-Action (VLA) 基础模型，旨在为通用人形机器人提供智能决策和动作生成能力。模型采用双系统架构，包括 System 2 的视觉-语言模块用于环境解读，以及 System 1 的扩散变压器模块用于实时生成流畅电机动作，两者端到端联合训练，并使用真实机器人轨迹、人类视频和合成数据集的异构混合进行训练。实验结果显示，GR00T N1 在多个机器人形态的标准模拟基准上超越最先进的模仿学习基线，并在 Fourier GR-1 人形机器人上实现高效的语言条件双臂操作任务，展示了高数据效率和鲁棒性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Authors are listed alphabetically. Project leads are Linxi \"Jim\" Fan\n  and Yuke Zhu. For more information, see\n  https://developer.nvidia.com/isaac/gr00t",
      "pdf_url": "http://arxiv.org/pdf/2503.14734v2",
      "published_date": "2025-03-18 21:06:21 UTC",
      "updated_date": "2025-03-27 02:52:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:23:59.599090"
    },
    {
      "arxiv_id": "2503.14716v1",
      "title": "Construction Site Scaffolding Completeness Detection Based on Mask R-CNN and Hough Transform",
      "title_zh": "翻译失败",
      "authors": [
        "Pei-Hsin Lin",
        "Jacob J. Lin",
        "Shang-Hsien Hsieh"
      ],
      "abstract": "Construction site scaffolding is essential for many building projects, and\nensuring its safety is crucial to prevent accidents. The safety inspector must\ncheck the scaffolding's completeness and integrity, where most violations\noccur. The inspection process includes ensuring all the components are in the\nright place since workers often compromise safety for convenience and\ndisassemble parts such as cross braces. This paper proposes a deep\nlearning-based approach to detect the scaffolding and its cross braces using\ncomputer vision. A scaffold image dataset with annotated labels is used to\ntrain a convolutional neural network (CNN) model. With the proposed approach,\nwe can automatically detect the completeness of cross braces from images taken\nat construction sites, without the need for manual inspection, saving a\nsignificant amount of time and labor costs. This non-invasive and efficient\nsolution for detecting scaffolding completeness can help improve safety in\nconstruction sites.",
      "tldr_zh": "本论文针对建筑工地脚手架的安全问题，提出了一种基于 Mask R-CNN 和 Hough Transform 的深度学习方法，用于自动检测脚手架及其横向支架的完整性。方法利用一个标注标签的脚手架图像数据集训练 CNN 模型，实现对现场图像的非侵入式分析，减少手动检查的需求。该方法显著节省时间和劳动力，并有助于提升建设现场的安全性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The 30th EG-ICE: International Conference on Intelligent Computing in\n  Engineering",
      "pdf_url": "http://arxiv.org/pdf/2503.14716v1",
      "published_date": "2025-03-18 20:27:22 UTC",
      "updated_date": "2025-03-18 20:27:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:24:10.823379"
    },
    {
      "arxiv_id": "2503.15555v1",
      "title": "Whole-Body Image-to-Image Translation for a Virtual Scanner in a Healthcare Digital Twin",
      "title_zh": "翻译失败",
      "authors": [
        "Valerio Guarrasi",
        "Francesco Di Feola",
        "Rebecca Restivo",
        "Lorenzo Tronchin",
        "Paolo Soda"
      ],
      "abstract": "Generating positron emission tomography (PET) images from computed tomography\n(CT) scans via deep learning offers a promising pathway to reduce radiation\nexposure and costs associated with PET imaging, improving patient care and\naccessibility to functional imaging. Whole-body image translation presents\nchallenges due to anatomical heterogeneity, often limiting generalized models.\nWe propose a framework that segments whole-body CT images into four\nregions-head, trunk, arms, and legs-and uses district-specific Generative\nAdversarial Networks (GANs) for tailored CT-to-PET translation. Synthetic PET\nimages from each region are stitched together to reconstruct the whole-body\nscan. Comparisons with a baseline non-segmented GAN and experiments with\nPix2Pix and CycleGAN architectures tested paired and unpaired scenarios.\nQuantitative evaluations at district, whole-body, and lesion levels\ndemonstrated significant improvements with our district-specific GANs. Pix2Pix\nyielded superior metrics, ensuring precise, high-quality image synthesis. By\naddressing anatomical heterogeneity, this approach achieves state-of-the-art\nresults in whole-body CT-to-PET translation. This methodology supports\nhealthcare Digital Twins by enabling accurate virtual PET scans from CT data,\ncreating virtual imaging representations to monitor, predict, and optimize\nhealth outcomes.",
      "tldr_zh": "该研究提出了一种框架，用于通过深度学习从 CT 扫描生成 PET 图像，以减少辐射暴露和成本，并支持医疗数字孪生中的虚拟扫描。该框架将全身 CT 图像分割成头部、躯干、手臂和腿部四个区域，并使用区域特定的 Generative Adversarial Networks (GANs) 进行定制的 CT-to-PET 翻译，随后拼接合成图像。实验结果显示，与基线非分割 GAN 相比，该方法在区域、整体和病变水平上显著提升图像质量，其中 Pix2Pix 架构表现出色，实现全身图像翻译的最先进性能，从而为医疗数字孪生提供准确的虚拟 PET 扫描，以监控、预测和优化健康结果。",
      "categories": [
        "eess.IV",
        "cs.AI"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15555v1",
      "published_date": "2025-03-18 20:19:28 UTC",
      "updated_date": "2025-03-18 20:19:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:24:22.983023"
    },
    {
      "arxiv_id": "2503.14681v2",
      "title": "DPImageBench: A Unified Benchmark for Differentially Private Image Synthesis",
      "title_zh": "DPImageBench：用于差分隐私图像合成的统一基准",
      "authors": [
        "Chen Gong",
        "Kecen Li",
        "Zinan Lin",
        "Tianhao Wang"
      ],
      "abstract": "Differentially private (DP) image synthesis aims to generate artificial\nimages that retain the properties of sensitive images while protecting the\nprivacy of individual images within the dataset. Despite recent advancements,\nwe find that inconsistent--and sometimes flawed--evaluation protocols have been\napplied across studies. This not only impedes the understanding of current\nmethods but also hinders future advancements.\n  To address the issue, this paper introduces DPImageBench for DP image\nsynthesis, with thoughtful design across several dimensions: (1) Methods. We\nstudy eleven prominent methods and systematically characterize each based on\nmodel architecture, pretraining strategy, and privacy mechanism. (2)\nEvaluation. We include nine datasets and seven fidelity and utility metrics to\nthoroughly assess them. Notably, we find that a common practice of selecting\ndownstream classifiers based on the highest accuracy on the sensitive test set\nnot only violates DP but also overestimates the utility scores. DPImageBench\ncorrects for these mistakes. (3) Platform. Despite the methods and evaluation\nprotocols, DPImageBench provides a standardized interface that accommodates\ncurrent and future implementations within a unified framework. With\nDPImageBench, we have several noteworthy findings. For example, contrary to the\ncommon wisdom that pretraining on public image datasets is usually beneficial,\nwe find that the distributional similarity between pretraining and sensitive\nimages significantly impacts the performance of the synthetic images and does\nnot always yield improvements. In addition, adding noise to low-dimensional\nfeatures, such as the high-level characteristics of sensitive images, is less\naffected by the privacy budget compared to adding noise to high-dimensional\nfeatures, like weight gradients. The former methods perform better than the\nlatter under a low privacy budget.",
      "tldr_zh": "本研究引入DPImageBench，一种统一的基准，用于评估Differentially Private (DP)图像合成方法，以解决现有研究中评估协议不一致的问题。该基准系统化研究了11种主要方法，涵盖模型架构、预训练策略和隐私机制，并包括9个数据集和7个保真度与实用性指标，同时纠正了常见错误，如选择下游分类器可能违反DP并高估性能。实验发现，预训练数据集的分布相似性对合成图像性能影响显著，并非总是有益；此外，在低维特征（如高层次特征）上添加噪声比在高维特征（如权重梯度）上更不易受隐私预算影响，在低隐私预算下表现更好。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "The first two authors contributed equally; code available at\n  https://github.com/2019ChenGong/DPImageBench",
      "pdf_url": "http://arxiv.org/pdf/2503.14681v2",
      "published_date": "2025-03-18 19:37:35 UTC",
      "updated_date": "2025-04-10 18:52:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:24:34.636085"
    },
    {
      "arxiv_id": "2503.14662v1",
      "title": "ConQuer: A Framework for Concept-Based Quiz Generation",
      "title_zh": "ConQuer：一种基于概念的测验生成框架",
      "authors": [
        "Yicheng Fu",
        "Zikui Wang",
        "Liuxin Yang",
        "Meiqing Huo",
        "Zhongdongming Dai"
      ],
      "abstract": "Quizzes play a crucial role in education by reinforcing students'\nunderstanding of key concepts and encouraging self-directed exploration.\nHowever, compiling high-quality quizzes can be challenging and require deep\nexpertise and insight into specific subject matter. Although LLMs have greatly\nenhanced the efficiency of quiz generation, concerns remain regarding the\nquality of these AI-generated quizzes and their educational impact on students.\nTo address these issues, we introduce ConQuer, a concept-based quiz generation\nframework that leverages external knowledge sources. We employ comprehensive\nevaluation dimensions to assess the quality of the generated quizzes, using\nLLMs as judges. Our experiment results demonstrate a 4.8% improvement in\nevaluation scores and a 77.52% win rate in pairwise comparisons against\nbaseline quiz sets. Ablation studies further underscore the effectiveness of\neach component in our framework. Code available at\nhttps://github.com/sofyc/ConQuer.",
      "tldr_zh": "本文提出 ConQuer 框架，这是一种基于概念的测验生成方法，利用外部知识来源来提升 LLMs 生成测验的质量，并解决教育测验的准确性和教育影响问题。框架通过全面评估维度（如 LLMs 作为评判者）来评估测验效果，实验结果显示与基线相比，ConQuer 提高了 4.8% 的评估分数，并在配对比较中获得 77.52% 的胜率。消融研究进一步验证了框架各组件的有效性，为高效的教育测验生成提供了可靠工具。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14662v1",
      "published_date": "2025-03-18 19:10:26 UTC",
      "updated_date": "2025-03-18 19:10:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:24:46.627295"
    },
    {
      "arxiv_id": "2503.14655v1",
      "title": "Core-Periphery Principle Guided State Space Model for Functional Connectome Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Minheng Chen",
        "Xiaowei Yu",
        "Jing Zhang",
        "Tong Chen",
        "Chao Cao",
        "Yan Zhuang",
        "Yanjun Lyu",
        "Lu Zhang",
        "Tianming Liu",
        "Dajiang Zhu"
      ],
      "abstract": "Understanding the organization of human brain networks has become a central\nfocus in neuroscience, particularly in the study of functional connectivity,\nwhich plays a crucial role in diagnosing neurological disorders. Advances in\nfunctional magnetic resonance imaging and machine learning techniques have\nsignificantly improved brain network analysis. However, traditional machine\nlearning approaches struggle to capture the complex relationships between brain\nregions, while deep learning methods, particularly Transformer-based models,\nface computational challenges due to their quadratic complexity in\nlong-sequence modeling. To address these limitations, we propose a\nCore-Periphery State-Space Model (CP-SSM), an innovative framework for\nfunctional connectome classification. Specifically, we introduce Mamba, a\nselective state-space model with linear complexity, to effectively capture\nlong-range dependencies in functional brain networks. Furthermore, inspired by\nthe core-periphery (CP) organization, a fundamental characteristic of brain\nnetworks that enhances efficient information transmission, we design CP-MoE, a\nCP-guided Mixture-of-Experts that improves the representation learning of brain\nconnectivity patterns. We evaluate CP-SSM on two benchmark fMRI datasets: ABIDE\nand ADNI. Experimental results demonstrate that CP-SSM surpasses\nTransformer-based models in classification performance while significantly\nreducing computational complexity. These findings highlight the effectiveness\nand efficiency of CP-SSM in modeling brain functional connectivity, offering a\npromising direction for neuroimaging-based neurological disease diagnosis.",
      "tldr_zh": "这篇论文针对人类大脑网络组织的研究，提出了一种基于核心-外围原则的State Space Model (CP-SSM)，以提升功能连接图的分类性能，特别是用于神经疾病诊断。CP-SSM 整合了Mamba模型，该模型具有线性复杂度，能够有效捕捉功能脑网络的长程依赖；同时，引入CP-MoE（核心-外围指导的Mixture-of-Experts）来优化脑连接模式的表示学习。实验在ABIDE和ADNI fMRI数据集上验证，CP-SSM在分类准确性上超越Transformer-based models，同时显著减少计算复杂度，为基于神经影像的疾病诊断提供高效且可靠的新方法。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14655v1",
      "published_date": "2025-03-18 19:03:27 UTC",
      "updated_date": "2025-03-18 19:03:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:25:00.807960"
    },
    {
      "arxiv_id": "2503.14649v2",
      "title": "RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving",
      "title_zh": "RAGO：检索增强生成服务的系统化性能优化",
      "authors": [
        "Wenqi Jiang",
        "Suvinay Subramanian",
        "Cat Graves",
        "Gustavo Alonso",
        "Amir Yazdanbakhsh",
        "Vidushi Dadu"
      ],
      "abstract": "Retrieval-augmented generation (RAG), which combines large language models\n(LLMs) with retrievals from external knowledge databases, is emerging as a\npopular approach for reliable LLM serving. However, efficient RAG serving\nremains an open challenge due to the rapid emergence of many RAG variants and\nthe substantial differences in workload characteristics across them. In this\npaper, we make three fundamental contributions to advancing RAG serving. First,\nwe introduce RAGSchema, a structured abstraction that captures the wide range\nof RAG algorithms, serving as a foundation for performance optimization.\nSecond, we analyze several representative RAG workloads with distinct\nRAGSchema, revealing significant performance variability across these\nworkloads. Third, to address this variability and meet diverse performance\nrequirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a\nsystem optimization framework for efficient RAG serving. Our evaluation shows\nthat RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in\ntime-to-first-token latency compared to RAG systems built on LLM-system\nextensions.",
      "tldr_zh": "该论文针对检索增强生成（RAG）服务的性能优化问题，提出了RAGSchema，这是一个结构化的抽象框架，用于捕捉多种RAG算法的特性，以作为优化基础。通过分析几种代表性RAG工作负载，研究揭示了这些工作负载在性能上的显著差异。为应对这些差异，论文引入了RAGO（Retrieval-Augmented Generation Optimizer）系统优化框架，能够根据特定需求提升RAG服务的效率。实验结果显示，RAGO相较于基于LLM系统扩展的RAG系统，实现了高达2倍的QPS per chip和55%的time-to-first-token延迟减少。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.DC",
        "C.1; C.4; H.3"
      ],
      "primary_category": "cs.IR",
      "comment": "16 pages, 19 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.14649v2",
      "published_date": "2025-03-18 18:58:13 UTC",
      "updated_date": "2025-03-21 17:51:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:25:10.526806"
    },
    {
      "arxiv_id": "2503.14640v1",
      "title": "Dynamic Accumulated Attention Map for Interpreting Evolution of Decision-Making in Vision Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Yi Liao",
        "Yongsheng Gao",
        "Weichuan Zhang"
      ],
      "abstract": "Various Vision Transformer (ViT) models have been widely used for image\nrecognition tasks. However, existing visual explanation methods can not display\nthe attention flow hidden inside the inner structure of ViT models, which\nexplains how the final attention regions are formed inside a ViT for its\ndecision-making. In this paper, a novel visual explanation approach, Dynamic\nAccumulated Attention Map (DAAM), is proposed to provide a tool that can\nvisualize, for the first time, the attention flow from the top to the bottom\nthrough ViT networks. To this end, a novel decomposition module is proposed to\nconstruct and store the spatial feature information by unlocking the [class]\ntoken generated by the self-attention module of each ViT block. The module can\nalso obtain the channel importance coefficients by decomposing the\nclassification score for supervised ViT models. Because of the lack of\nclassification score in self-supervised ViT models, we propose dimension-wise\nimportance weights to compute the channel importance coefficients. Such spatial\nfeatures are linearly combined with the corresponding channel importance\ncoefficients, forming the attention map for each block. The dynamic attention\nflow is revealed by block-wisely accumulating each attention map. The\ncontribution of this work focuses on visualizing the evolution dynamic of the\ndecision-making attention for any intermediate block inside a ViT model by\nproposing a novel decomposition module and dimension-wise importance weights.\nThe quantitative and qualitative analysis consistently validate the\neffectiveness and superior capacity of the proposed DAAM for not only\ninterpreting ViT models with the fully-connected layers as the classifier but\nalso self-supervised ViT models. The code is available at\nhttps://github.com/ly9802/DynamicAccumulatedAttentionMap.",
      "tldr_zh": "本文提出 Dynamic Accumulated Attention Map (DAAM)，一种新颖的视觉解释方法，用于可视化 Vision Transformer (ViT) 模型中决策注意力的演变动态，解决现有方法无法显示内部注意力流的问题。DAAM 通过一个新型 decomposition module 解锁 [class] token 的空间特征信息，并计算 channel importance coefficients 或 dimension-wise importance weights，形成每个 ViT 块的注意力图，然后逐块累积以揭示动态注意力流。实验结果显示，DAAM 在定量和定性分析中均表现出色，能够有效解释带有全连接层分类器的 ViT 模型以及自监督 ViT 模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14640v1",
      "published_date": "2025-03-18 18:41:01 UTC",
      "updated_date": "2025-03-18 18:41:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:25:23.491184"
    },
    {
      "arxiv_id": "2503.14637v1",
      "title": "Reinforcement learning-based motion imitation for physiologically plausible musculoskeletal motor control",
      "title_zh": "翻译失败",
      "authors": [
        "Merkourios Simos",
        "Alberto Silvio Chiappa",
        "Alexander Mathis"
      ],
      "abstract": "How do humans move? The quest to understand human motion has broad\napplications in numerous fields, ranging from computer animation and motion\nsynthesis to neuroscience, human prosthetics and rehabilitation. Although\nadvances in reinforcement learning (RL) have produced impressive results in\ncapturing human motion using simplified humanoids, controlling physiologically\naccurate models of the body remains an open challenge. In this work, we present\na model-free motion imitation framework (KINESIS) to advance the understanding\nof muscle-based motor control. Using a musculoskeletal model of the lower body\nwith 80 muscle actuators and 20 DoF, we demonstrate that KINESIS achieves\nstrong imitation performance on 1.9 hours of motion capture data, is\ncontrollable by natural language through pre-trained text-to-motion generative\nmodels, and can be fine-tuned to carry out high-level tasks such as target goal\nreaching. Importantly, KINESIS generates muscle activity patterns that\ncorrelate well with human EMG activity. The physiological plausibility makes\nKINESIS a promising model for tackling challenging problems in human motor\ncontrol theory, which we highlight by investigating Bernstein's redundancy\nproblem in the context of locomotion. Code, videos and benchmarks will be\navailable at https://github.com/amathislab/Kinesis.",
      "tldr_zh": "这篇论文提出了一种基于Reinforcement learning的运动模仿框架KINESIS，旨在实现生理上合理的肌肉骨骼运动控制，以更好地理解人类运动。框架使用一个包含80个肌肉致动器和20个DoF的下肢模型，对1.9小时的动作捕捉数据进行无模型模仿，并支持通过预训练的文本到运动生成模型实现自然语言控制和任务微调，如目标到达。实验结果显示，KINESIS生成的肌肉活动模式与人类EMG数据高度相关，并通过探讨Bernstein's redundancy问题，为人类运动控制理论提供了新见解。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14637v1",
      "published_date": "2025-03-18 18:37:49 UTC",
      "updated_date": "2025-03-18 18:37:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:25:35.476121"
    },
    {
      "arxiv_id": "2503.14630v1",
      "title": "Assessing Large Language Models for Automated Feedback Generation in Learning Programming Problem Solving",
      "title_zh": "评估大语言模型在学习编程问题解决中的自动反馈生成",
      "authors": [
        "Priscylla Silva",
        "Evandro Costa"
      ],
      "abstract": "Providing effective feedback is important for student learning in programming\nproblem-solving. In this sense, Large Language Models (LLMs) have emerged as\npotential tools to automate feedback generation. However, their reliability and\nability to identify reasoning errors in student code remain not well\nunderstood. This study evaluates the performance of four LLMs (GPT-4o, GPT-4o\nmini, GPT-4-Turbo, and Gemini-1.5-pro) on a benchmark dataset of 45 student\nsolutions. We assessed the models' capacity to provide accurate and insightful\nfeedback, particularly in identifying reasoning mistakes. Our analysis reveals\nthat 63\\% of feedback hints were accurate and complete, while 37\\% contained\nmistakes, including incorrect line identification, flawed explanations, or\nhallucinated issues. These findings highlight the potential and limitations of\nLLMs in programming education and underscore the need for improvements to\nenhance reliability and minimize risks in educational applications.",
      "tldr_zh": "本文评估了大型语言模型(LLMs)如GPT-4o、GPT-4o mini、GPT-4-Turbo和Gemini-1.5-pro在自动生成编程问题解决反馈方面的性能，使用了一个包含45个学生解决方案的基准数据集。研究发现，63%的反馈提示准确且完整，而37%存在错误，包括错误的行识别、 flawed explanations 或虚构问题。结果突出了LLMs在编程教育中的潜力，同时强调了提升其可靠性和减少风险的必要性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14630v1",
      "published_date": "2025-03-18 18:31:36 UTC",
      "updated_date": "2025-03-18 18:31:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:25:47.320489"
    },
    {
      "arxiv_id": "2503.14621v1",
      "title": "Reducing False Ventricular Tachycardia Alarms in ICU Settings: A Machine Learning Approach",
      "title_zh": "减少重症监护室环境中室性心动过速假警报：一种机器学习方法",
      "authors": [
        "Grace Funmilayo Farayola",
        "Akinyemi Sadeeq Akintola",
        "Oluwole Fagbohun",
        "Chukwuka Michael Oforgu",
        "Bisola Faith Kayode",
        "Christian Chimezie",
        "Temitope Kadri",
        "Abiola Oludotun",
        "Nelson Ogbeide",
        "Mgbame Michael",
        "Adeseye Ifaturoti",
        "Toyese Oloyede"
      ],
      "abstract": "False arrhythmia alarms in intensive care units (ICUs) are a significant\nchallenge, contributing to alarm fatigue and potentially compromising patient\nsafety. Ventricular tachycardia (VT) alarms are particularly difficult to\ndetect accurately due to their complex nature. This paper presents a machine\nlearning approach to reduce false VT alarms using the VTaC dataset, a benchmark\ndataset of annotated VT alarms from ICU monitors. We extract time-domain and\nfrequency-domain features from waveform data, preprocess the data, and train\ndeep learning models to classify true and false VT alarms. Our results\ndemonstrate high performance, with ROC-AUC scores exceeding 0.96 across various\ntraining configurations. This work highlights the potential of machine learning\nto improve the accuracy of VT alarm detection in clinical settings.",
      "tldr_zh": "这篇论文针对 ICU 环境中假性心室性心动过速 (VT) 警报问题，提出了一种机器学习方法来减少警报疲劳并提升患者安全。研究团队使用 VTaC 数据集，从波形数据中提取时域和频域特征，进行数据预处理，并训练深度学习模型以分类真假 VT 警报。结果显示，模型在各种训练配置下均达到 ROC-AUC 分数超过 0.96 的高性能，突显了机器学习在临床警报检测准确性方面的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint, Accepted to the International Conference on Machine\n  Learning Technologies (ICMLT 2025), Helsinki, Finland",
      "pdf_url": "http://arxiv.org/pdf/2503.14621v1",
      "published_date": "2025-03-18 18:18:38 UTC",
      "updated_date": "2025-03-18 18:18:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:25:59.668670"
    },
    {
      "arxiv_id": "2503.14604v1",
      "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives",
      "title_zh": "翻译失败",
      "authors": [
        "Sara Sarto",
        "Marcella Cornia",
        "Rita Cucchiara"
      ],
      "abstract": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment.",
      "tldr_zh": "这篇论文综述了图像标题评估的进展，特别是在Multimodal LLMs时代面临的挑战，强调了评估指标的演变、优势和局限性。作者评估了现有指标在多个维度上的表现，包括与人类判断的相关性、排名准确性和对幻觉的敏感性，并探讨了Multimodal LLMs生成的长篇详细标题对这些指标的适应性问题。研究发现，标准评估方法存在显著局限性，如难以处理风格变化，并提出未来研究应聚焦于更鲁棒的指标设计，以提升图像标题评估的可靠性和全面性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Repo GitHub:\n  https://github.com/aimagelab/awesome-captioning-evaluation",
      "pdf_url": "http://arxiv.org/pdf/2503.14604v1",
      "published_date": "2025-03-18 18:03:56 UTC",
      "updated_date": "2025-03-18 18:03:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:26:11.203467"
    },
    {
      "arxiv_id": "2503.14505v1",
      "title": "MusicInfuser: Making Video Diffusion Listen and Dance",
      "title_zh": "翻译失败",
      "authors": [
        "Susung Hong",
        "Ira Kemelmacher-Shlizerman",
        "Brian Curless",
        "Steven M. Seitz"
      ],
      "abstract": "We introduce MusicInfuser, an approach for generating high-quality dance\nvideos that are synchronized to a specified music track. Rather than attempting\nto design and train a new multimodal audio-video model, we show how existing\nvideo diffusion models can be adapted to align with musical inputs by\nintroducing lightweight music-video cross-attention and a low-rank adapter.\nUnlike prior work requiring motion capture data, our approach fine-tunes only\non dance videos. MusicInfuser achieves high-quality music-driven video\ngeneration while preserving the flexibility and generative capabilities of the\nunderlying models. We introduce an evaluation framework using Video-LLMs to\nassess multiple dimensions of dance generation quality. The project page and\ncode are available at https://susunghong.github.io/MusicInfuser.",
      "tldr_zh": "我们引入了 MusicInfuser，一种基于现有视频扩散模型的方法，用于生成与指定音乐轨道同步的高质量舞伴视频。该方法通过添加轻量级的 music-video cross-attention 和 low-rank adapter，仅在舞伴视频上进行微调，从而实现音乐驱动的视频生成，而无需依赖动作捕捉数据。MusicInfuser 保留了底层模型的灵活性和生成能力，并在多个维度上提升了生成质量。我们还开发了一个使用 Video-LLMs 的评估框架，以全面评估舞伴视频的同步性和整体表现。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://susunghong.github.io/MusicInfuser",
      "pdf_url": "http://arxiv.org/pdf/2503.14505v1",
      "published_date": "2025-03-18 17:59:58 UTC",
      "updated_date": "2025-03-18 17:59:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:26:23.092225"
    },
    {
      "arxiv_id": "2503.14503v1",
      "title": "The Power of Context: How Multimodality Improves Image Super-Resolution",
      "title_zh": "语境的力量：多模态如何改善图像超分辨率",
      "authors": [
        "Kangfu Mei",
        "Hossein Talebi",
        "Mojtaba Ardakani",
        "Vishal M. Patel",
        "Peyman Milanfar",
        "Mauricio Delbracio"
      ],
      "abstract": "Single-image super-resolution (SISR) remains challenging due to the inherent\ndifficulty of recovering fine-grained details and preserving perceptual quality\nfrom low-resolution inputs. Existing methods often rely on limited image\npriors, leading to suboptimal results. We propose a novel approach that\nleverages the rich contextual information available in multiple modalities --\nincluding depth, segmentation, edges, and text prompts -- to learn a powerful\ngenerative prior for SISR within a diffusion model framework. We introduce a\nflexible network architecture that effectively fuses multimodal information,\naccommodating an arbitrary number of input modalities without requiring\nsignificant modifications to the diffusion process. Crucially, we mitigate\nhallucinations, often introduced by text prompts, by using spatial information\nfrom other modalities to guide regional text-based conditioning. Each\nmodality's guidance strength can also be controlled independently, allowing\nsteering outputs toward different directions, such as increasing bokeh through\ndepth or adjusting object prominence via segmentation. Extensive experiments\ndemonstrate that our model surpasses state-of-the-art generative SISR methods,\nachieving superior visual quality and fidelity. See project page at\nhttps://mmsr.kfmei.com/.",
      "tldr_zh": "本文针对单图像超分辨率 (SISR) 的挑战，如从低分辨率输入恢复细节和保持感知质量，提出一种新方法，利用多模态信息（包括 depth、segmentation、edges 和 text prompts）在 diffusion model 框架内学习强大的生成先验。该方法采用灵活的网络架构有效融合任意数量的输入模态，并通过其他模态的空间信息引导 text prompts 以减少幻觉，同时允许独立控制每个模态的引导强度，实现输出调整（如通过 depth 增强散景或通过 segmentation 突出对象）。实验证明，该模型在视觉质量和保真度上超越现有最先进的方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted by CVPR2025",
      "pdf_url": "http://arxiv.org/pdf/2503.14503v1",
      "published_date": "2025-03-18 17:59:54 UTC",
      "updated_date": "2025-03-18 17:59:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:26:35.313781"
    },
    {
      "arxiv_id": "2503.14499v2",
      "title": "Measuring AI Ability to Complete Long Tasks",
      "title_zh": "测量人工智能完成长任务的能力",
      "authors": [
        "Thomas Kwa",
        "Ben West",
        "Joel Becker",
        "Amy Deng",
        "Katharyn Garcia",
        "Max Hasin",
        "Sami Jawhar",
        "Megan Kinniment",
        "Nate Rush",
        "Sydney Von Arx",
        "Ryan Bloom",
        "Thomas Broadley",
        "Haoxing Du",
        "Brian Goodrich",
        "Nikola Jurkovic",
        "Luke Harold Miles",
        "Seraphina Nix",
        "Tao Lin",
        "Neev Parikh",
        "David Rein",
        "Lucas Jun Koba Sato",
        "Hjalmar Wijk",
        "Daniel M. Ziegler",
        "Elizabeth Barnes",
        "Lawrence Chan"
      ],
      "abstract": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark\nperformance remains unclear. To quantify the capabilities of AI systems in\nterms of human capabilities, we propose a new metric: 50%-task-completion time\nhorizon. This is the time humans typically take to complete tasks that AI\nmodels can complete with 50% success rate. We first timed humans with relevant\ndomain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter\ntasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet\nhave a 50% time horizon of around 50 minutes. Furthermore, frontier AI time\nhorizon has been doubling approximately every seven months since 2019, though\nthe trend may have accelerated in 2024. The increase in AI models' time\nhorizons seems to be primarily driven by greater reliability and ability to\nadapt to mistakes, combined with better logical reasoning and tool use\ncapabilities. We discuss the limitations of our results -- including their\ndegree of external validity -- and the implications of increased autonomy for\ndangerous capabilities. If these results generalize to real-world software\ntasks, extrapolation of this trend predicts that within 5 years, AI systems\nwill be capable of automating many software tasks that currently take humans a\nmonth.",
      "tldr_zh": "本研究提出了一种新指标“50%-task-completion time horizon”，用于量化AI模型完成任务的能力，具体指人类通常需要多少时间来完成AI以50%成功率能完成的任务。研究者通过测试人类在RE-Bench、HCAST和66个新任务上的时间，并与当前前沿AI模型（如Claude 3.5 Sonnet）比较，发现这些AI的50%时间视野约为50分钟，且自2019年以来每七个月翻倍，主要驱动因素包括AI的可靠性、错误适应能力、逻辑推理和工具使用。论文分析了这一趋势的加速可能，特别是2024年的变化，并预测如果持续发展，AI可能在5年内自动化许多需要人类一个月完成的任务，如软件开发。研究还讨论了结果的外部有效性限制以及AI自主性带来的潜在危险能力风险。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14499v2",
      "published_date": "2025-03-18 17:59:31 UTC",
      "updated_date": "2025-03-30 17:53:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:26:48.245968"
    },
    {
      "arxiv_id": "2503.14493v2",
      "title": "State Space Model Meets Transformer: A New Paradigm for 3D Object Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Chuxin Wang",
        "Wenfei Yang",
        "Xiang Liu",
        "Tianzhu Zhang"
      ],
      "abstract": "DETR-based methods, which use multi-layer transformer decoders to refine\nobject queries iteratively, have shown promising performance in 3D indoor\nobject detection. However, the scene point features in the transformer decoder\nremain fixed, leading to minimal contributions from later decoder layers,\nthereby limiting performance improvement. Recently, State Space Models (SSM)\nhave shown efficient context modeling ability with linear complexity through\niterative interactions between system states and inputs. Inspired by SSMs, we\npropose a new 3D object DEtection paradigm with an interactive STate space\nmodel (DEST). In the interactive SSM, we design a novel state-dependent SSM\nparameterization method that enables system states to effectively serve as\nqueries in 3D indoor detection tasks. In addition, we introduce four key\ndesigns tailored to the characteristics of point cloud and SSM: The\nserialization and bidirectional scanning strategies enable bidirectional\nfeature interaction among scene points within the SSM. The inter-state\nattention mechanism models the relationships between state points, while the\ngated feed-forward network enhances inter-channel correlations. To the best of\nour knowledge, this is the first method to model queries as system states and\nscene points as system inputs, which can simultaneously update scene point\nfeatures and query features with linear complexity. Extensive experiments on\ntwo challenging datasets demonstrate the effectiveness of our DEST-based\nmethod. Our method improves the GroupFree baseline in terms of AP50 on ScanNet\nV2 (+5.3) and SUN RGB-D (+3.2) datasets. Based on the VDETR baseline, Our\nmethod sets a new SOTA on the ScanNetV2 and SUN RGB-D datasets.",
      "tldr_zh": "该论文提出了一种新的 3D 对象检测范式 DEST，将 State Space Models (SSM) 与 Transformer 结合，解决 DETR 方法中场景点特征固定的问题，从而提升检测性能。\n核心创新包括 state-dependent SSM 参数化方法，将系统状态作为查询，并设计序列化、双向扫描策略、状态间注意力机制和门控前馈网络，实现线性复杂度的双向特征交互。\n实验结果显示，该方法在 ScanNet V2 和 SUN RGB-D 数据集上，比 GroupFree 基线提升 AP50 分别为 5.3 和 3.2，并基于 VDETR 基线设置了新的 SOTA 性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICLR 2025. Project url:\n  https://chuxwa.github.io/project_DEST/",
      "pdf_url": "http://arxiv.org/pdf/2503.14493v2",
      "published_date": "2025-03-18 17:58:03 UTC",
      "updated_date": "2025-03-19 14:10:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:27:01.300013"
    },
    {
      "arxiv_id": "2503.14492v2",
      "title": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control",
      "title_zh": "翻译失败",
      "authors": [
        "NVIDIA",
        ":",
        "Hassan Abu Alhaija",
        "Jose Alvarez",
        "Maciej Bala",
        "Tiffany Cai",
        "Tianshi Cao",
        "Liz Cha",
        "Joshua Chen",
        "Mike Chen",
        "Francesco Ferroni",
        "Sanja Fidler",
        "Dieter Fox",
        "Yunhao Ge",
        "Jinwei Gu",
        "Ali Hassani",
        "Michael Isaev",
        "Pooya Jannaty",
        "Shiyi Lan",
        "Tobias Lasser",
        "Huan Ling",
        "Ming-Yu Liu",
        "Xian Liu",
        "Yifan Lu",
        "Alice Luo",
        "Qianli Ma",
        "Hanzi Mao",
        "Fabio Ramos",
        "Xuanchi Ren",
        "Tianchang Shen",
        "Xinglong Sun",
        "Shitao Tang",
        "Ting-Chun Wang",
        "Jay Wu",
        "Jiashu Xu",
        "Stella Xu",
        "Kevin Xie",
        "Yuchong Ye",
        "Xiaodong Yang",
        "Xiaohui Zeng",
        "Yu Zeng"
      ],
      "abstract": "We introduce Cosmos-Transfer, a conditional world generation model that can\ngenerate world simulations based on multiple spatial control inputs of various\nmodalities such as segmentation, depth, and edge. In the design, the spatial\nconditional scheme is adaptive and customizable. It allows weighting different\nconditional inputs differently at different spatial locations. This enables\nhighly controllable world generation and finds use in various world-to-world\ntransfer use cases, including Sim2Real. We conduct extensive evaluations to\nanalyze the proposed model and demonstrate its applications for Physical AI,\nincluding robotics Sim2Real and autonomous vehicle data enrichment. We further\ndemonstrate an inference scaling strategy to achieve real-time world generation\nwith an NVIDIA GB200 NVL72 rack. To help accelerate research development in the\nfield, we open-source our models and code at\nhttps://github.com/nvidia-cosmos/cosmos-transfer1.",
      "tldr_zh": "本文介绍了 Cosmos-Transfer，一种条件世界生成模型，能够基于多种空间控制输入（如分割、深度和边缘）生成高度可控的世界模拟。模型采用自适应和可定制的空间条件方案，在不同位置赋予不同权重，支持各种世界到世界转移应用，包括 Sim2Real。实验评估显示，该模型在机器人 Sim2Real 和自动驾驶数据增强等物理 AI 领域表现出色，并通过推理缩放策略实现了实时生成，同时开源了模型和代码以促进研究发展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14492v2",
      "published_date": "2025-03-18 17:57:54 UTC",
      "updated_date": "2025-04-01 21:14:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:27:11.390757"
    },
    {
      "arxiv_id": "2503.14488v1",
      "title": "Engineering Scientific Assistants using Interactive Structured Induction of Programs",
      "title_zh": "翻译失败",
      "authors": [
        "Shraddha Surana",
        "Ashwin Srinivasan"
      ],
      "abstract": "We are interested in the construction of software that can act as scientific\nassistants to domain specialists. It is expected that such assistants will be\nneeded to accelerate the identification of ways to address complex problems\nrequiring urgent solutions. In this paper, our focus is not on a specific\nscientific problem, but on the software-engineering of such 'science\naccelerators'. Recent developments in 'No Code' techniques would seem to\nsuggest that scientist can simply hypothesise solutions simply by conversing\nwith a large language model (LLM). However, for complex scientific problems,\nthis seems unlikely given the current state of LLM technology. What does appear\nfeasible is that a software engineer can use LLMs to rapidly construct programs\nfor use by a domain-specialist, including the specialist's requirements\nexpressed in natural language. We propose the design of an interactive form of\n'structured' inductive programming in which a software-engineer and an LLM\ncollaboratively construct an 'assistant' for a scientific data analysis. The\npaper describes a simple implementation called iStrucInd that adapts a '2-way\nIntelligibility' protocol to implement the interaction between the software\nengineer and the LLM. We test the tool on two different non-trivial scientific\ndata analysis tasks. Specifically, we compare the system constructed by\niStrucInd against systems constructed manually and by Low Code/No Code methods\nalong dimensions of: (a) program performance; (b) program quality; and (c)\nprogramming effort. The results show iStrucInd allows a software engineer to\ndevelop better programs faster suggesting interactive structured induction can\nplay a useful role in the rapid construction of scientific assistants.",
      "tldr_zh": "本文提出了一种交互式结构化归纳编程（Interactive Structured Induction of Programs）方法，用于工程科学助手软件，帮助领域专家加速解决复杂问题。该方法让软件工程师与LLM（Large Language Model）协作，通过iStrucInd工具实现交互式构建程序，并使用2-way Intelligibility协议处理自然语言要求。在两个非平凡科学数据分析任务的测试中，iStrucInd相较于手动和No Code方法，在程序性能、质量和编程努力方面表现出色，能更快地开发出更好的程序，证明其在快速构建科学助手中的实用性。",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14488v1",
      "published_date": "2025-03-18 17:57:16 UTC",
      "updated_date": "2025-03-18 17:57:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:27:23.929182"
    },
    {
      "arxiv_id": "2503.14487v1",
      "title": "DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Minglei Shi",
        "Ziyang Yuan",
        "Haotian Yang",
        "Xintao Wang",
        "Mingwu Zheng",
        "Xin Tao",
        "Wenliang Zhao",
        "Wenzhao Zheng",
        "Jie Zhou",
        "Jiwen Lu",
        "Pengfei Wan",
        "Di Zhang",
        "Kun Gai"
      ],
      "abstract": "Diffusion models have demonstrated remarkable success in various image\ngeneration tasks, but their performance is often limited by the uniform\nprocessing of inputs across varying conditions and noise levels. To address\nthis limitation, we propose a novel approach that leverages the inherent\nheterogeneity of the diffusion process. Our method, DiffMoE, introduces a\nbatch-level global token pool that enables experts to access global token\ndistributions during training, promoting specialized expert behavior. To\nunleash the full potential of the diffusion process, DiffMoE incorporates a\ncapacity predictor that dynamically allocates computational resources based on\nnoise levels and sample complexity. Through comprehensive evaluation, DiffMoE\nachieves state-of-the-art performance among diffusion models on ImageNet\nbenchmark, substantially outperforming both dense architectures with 3x\nactivated parameters and existing MoE approaches while maintaining 1x activated\nparameters. The effectiveness of our approach extends beyond class-conditional\ngeneration to more challenging tasks such as text-to-image generation,\ndemonstrating its broad applicability across different diffusion model\napplications. Project Page: https://shiml20.github.io/DiffMoE/",
      "tldr_zh": "本文提出 DiffMoE，一种动态 token 选择方法，用于可扩展的 Diffusion Transformers，以解决扩散模型在处理不同条件和噪声水平时输入统一处理的局限性。DiffMoE 引入 batch-level 全局 token 池和 capacity predictor，实现专家的专业化行为并根据噪声水平和样本复杂度动态分配计算资源。在 ImageNet 基准测试中，DiffMoE 实现了最先进性能，激活参数仅为 1x 时就优于 3x 参数的密集架构和现有 MoE 方法，并扩展到文本到图像生成等更具挑战的任务中。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://shiml20.github.io/DiffMoE/",
      "pdf_url": "http://arxiv.org/pdf/2503.14487v1",
      "published_date": "2025-03-18 17:57:07 UTC",
      "updated_date": "2025-03-18 17:57:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:27:35.622242"
    },
    {
      "arxiv_id": "2503.14484v1",
      "title": "Gricean Norms as a Basis for Effective Collaboration",
      "title_zh": "翻译失败",
      "authors": [
        "Fardin Saad",
        "Pradeep K. Murukannaiah",
        "Munindar P. Singh"
      ],
      "abstract": "Effective human-AI collaboration hinges not only on the AI agent's ability to\nfollow explicit instructions but also on its capacity to navigate ambiguity,\nincompleteness, invalidity, and irrelevance in communication. Gricean\nconversational and inference norms facilitate collaboration by aligning unclear\ninstructions with cooperative principles. We propose a normative framework that\nintegrates Gricean norms and cognitive frameworks -- common ground, relevance\ntheory, and theory of mind -- into large language model (LLM) based agents. The\nnormative framework adopts the Gricean maxims of quantity, quality, relation,\nand manner, along with inference, as Gricean norms to interpret unclear\ninstructions, which are: ambiguous, incomplete, invalid, or irrelevant. Within\nthis framework, we introduce Lamoids, GPT-4 powered agents designed to\ncollaborate with humans. To assess the influence of Gricean norms in human-AI\ncollaboration, we evaluate two versions of a Lamoid: one with norms and one\nwithout. In our experiments, a Lamoid collaborates with a human to achieve\nshared goals in a grid world (Doors, Keys, and Gems) by interpreting both clear\nand unclear natural language instructions. Our results reveal that the Lamoid\nwith Gricean norms achieves higher task accuracy and generates clearer, more\naccurate, and contextually relevant responses than the Lamoid without norms.\nThis improvement stems from the normative framework, which enhances the agent's\npragmatic reasoning, fostering effective human-AI collaboration and enabling\ncontext-aware communication in LLM-based agents.",
      "tldr_zh": "该研究探讨了Gricean norms如何提升人类-AI协作的有效性，特别是处理沟通中的模糊、不完整、无效和无关指令。作者提出一个规范框架，将Gricean maxims（包括quantity、quality、relation和manner）以及inference，与认知框架（如common ground、relevance theory和theory of mind）整合到大型语言模型(LLM)基于的代理中，开发出Lamoids（基于GPT-4的代理）来解释不清晰指令。实验在网格世界（Doors, Keys, and Gems）环境中评估了两种Lamoids版本，结果显示带有Gricean norms的版本实现了更高的任务准确率，并生成更清晰、准确和相关的响应，从而增强了AI的实用推理和上下文感知能力。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.MA",
      "comment": "Accepted to AAMAS 2025. 8 pages (excl. references), 9 figures/tables.\n  (Appendix: 5 pages, 6 figures/tables). Code available at:\n  https://github.com/fardinsaad/Gricean-Norms",
      "pdf_url": "http://arxiv.org/pdf/2503.14484v1",
      "published_date": "2025-03-18 17:54:14 UTC",
      "updated_date": "2025-03-18 17:54:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:27:47.313252"
    },
    {
      "arxiv_id": "2503.14469v2",
      "title": "Attribution Score Alignment in Explainable Data Management",
      "title_zh": "解释性数据管理中的归因分数对齐",
      "authors": [
        "Felipe Azua",
        "Leopoldo Bertossi"
      ],
      "abstract": "Different attribution-scores have been proposed to quantify the relevance of\ndatabase tuples for a query answer from a database. Among them, we find Causal\nResponsibility, the Shapley Value, the Banzhaf Power-Index, and the Causal\nEffect. They have been analyzed in isolation, mainly in terms of computational\nproperties. In this work, we start an investigation into the alignment of these\nscores on the basis of the queries at hand; that is, on whether they induce\ncompatible rankings of tuples. We are able to identify vast classes of queries\nfor which some pairs of scores are always aligned, and others for which they\nare not. It turns out that the presence of exogenous tuples makes a crucial\ndifference in this regard.",
      "tldr_zh": "本文研究了在可解释数据管理中不同归因分数的对齐性，包括Causal Responsibility、Shapley Value、Banzhaf Power-Index和Causal Effect，这些分数用于量化数据库元组对查询答案的相关性。论文通过分析这些分数在各种查询下的兼容性，识别出大量查询类，其中某些分数对总是诱导相同的元组排名，而其他类则不。关键发现是外生元组（exogenous tuples）的存在对分数对齐性具有决定性影响，从而为更精确的查询解释提供新见解。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "Relevant references added in this version",
      "pdf_url": "http://arxiv.org/pdf/2503.14469v2",
      "published_date": "2025-03-18 17:45:32 UTC",
      "updated_date": "2025-04-24 22:13:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:27:58.548591"
    },
    {
      "arxiv_id": "2503.14456v2",
      "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
      "title_zh": "翻译失败",
      "authors": [
        "Bo Peng",
        "Ruichong Zhang",
        "Daniel Goldstein",
        "Eric Alcaide",
        "Xingjian Du",
        "Haowen Hou",
        "Jiaju Lin",
        "Jiaxing Liu",
        "Janna Lu",
        "William Merrill",
        "Guangyu Song",
        "Kaifeng Tan",
        "Saiteja Utpala",
        "Nathan Wilce",
        "Johan S. Wind",
        "Tianyi Wu",
        "Daniel Wuttke",
        "Christian Zhou-Zheng"
      ],
      "abstract": "We present RWKV-7 \"Goose\", a new sequence modeling architecture with constant\nmemory usage and constant inference time per token. Despite being trained on\ndramatically fewer tokens than other top models, our 2.9 billion parameter\nlanguage model achieves a new 3B SoTA on multilingual tasks and matches the\ncurrent 3B SoTA on English language downstream performance. RWKV-7 introduces a\nnewly generalized formulation of the delta rule with vector-valued gating and\nin-context learning rates, as well as a relaxed value replacement rule. We show\nthat RWKV-7 can perform state tracking and recognize all regular languages,\nwhile retaining parallelizability of training. This exceeds the capabilities of\nTransformers under standard complexity conjectures, which are limited to\n$\\mathsf{TC}^0$. To demonstrate RWKV-7's language modeling capability, we also\npresent an extended open source 3.1 trillion token multilingual corpus, and\ntrain four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on\nthis dataset.\n  To foster openness, reproduction, and adoption, we release our models and\ndataset component listing at https://huggingface.co/RWKV, and our training and\ninference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0\nLicense.",
      "tldr_zh": "我们介绍了 RWKV-7 \"Goose\"，一种序列建模架构，具有恒定内存使用和每个 token 的恒定推理时间，尽管训练 token 数量远少于其他模型，其 2.9 亿参数版本在多语言任务上达到了新的 3B SoTA，并在英语下游性能上匹配当前最佳水平。RWKV-7 引入了广义 delta rule 包括向量值门控和 in-context learning rates，以及放宽的值替换规则，使其能进行状态跟踪、识别所有正则语言，并保留训练的并行性，从而超过了 Transformers 的 $\\mathsf{TC}^0$ 限制。研究者提供了 3.1 万亿 token 的开源多语言语料库，并训练了从 0.19 亿到 2.9 亿参数的多个模型，所有资源均在 Apache 2.0 许可证下发布。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.0; I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14456v2",
      "published_date": "2025-03-18 17:31:05 UTC",
      "updated_date": "2025-03-30 13:46:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:28:13.056770"
    },
    {
      "arxiv_id": "2503.14448v1",
      "title": "Pauli Network Circuit Synthesis with Reinforcement Learning",
      "title_zh": "基于强化学习的 Pauli 网络电路合成",
      "authors": [
        "Ayushi Dubal",
        "David Kremer",
        "Simon Martiel",
        "Victor Villar",
        "Derek Wang",
        "Juan Cruz-Benito"
      ],
      "abstract": "We introduce a Reinforcement Learning (RL)-based method for re-synthesis of\nquantum circuits containing arbitrary Pauli rotations alongside Clifford\noperations. By collapsing each sub-block to a compact representation and then\nsynthesizing it step-by-step through a learned heuristic, we obtain circuits\nthat are both shorter and compliant with hardware connectivity constraints. We\nfind that the method is fast enough and good enough to work as an optimization\nprocedure: in direct comparisons on 6-qubit random Pauli Networks against\nstate-of-the-art heuristic methods, our RL approach yields over 2x reduction in\ntwo-qubit gate count, while executing in under 10 milliseconds per circuit. We\nfurther integrate the method into a collect-and-re-synthesize pipeline, applied\nas a Qiskit transpiler pass, where we observe average improvements of 20% in\ntwo-qubit gate count and depth, reaching up to 60% for many instances, across\nthe Benchpress benchmark. These results highlight the potential of RL-driven\nsynthesis to significantly improve circuit quality in realistic, large-scale\nquantum transpilation workloads.",
      "tldr_zh": "这篇论文提出了一种基于 Reinforcement Learning (RL) 的方法，用于重新合成包含任意 Pauli rotations 和 Clifford operations 的量子电路，该方法通过将子块压缩成紧凑表示并逐步使用学习的启发式策略进行合成，从而生成更短的电路并符合硬件连接约束。相比现有启发式方法，在 6-qubit 随机 Pauli Networks 测试中，RL 方法使两量子位门计数减少 2 倍以上，且执行时间不到 10 毫秒。进一步将其整合到 Qiskit transpiler 的收集和重新合成管道中，在 Benchpress 基准上实现了两量子位门计数和深度平均改善 20%，某些实例高达 60%。这些结果突显了 RL 驱动合成在实际大规模量子转译工作负载中显著提升电路质量的潜力。",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14448v1",
      "published_date": "2025-03-18 17:27:50 UTC",
      "updated_date": "2025-03-18 17:27:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:28:24.537105"
    },
    {
      "arxiv_id": "2503.14434v1",
      "title": "LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as Evolutionary Optimizers",
      "title_zh": "翻译失败",
      "authors": [
        "Nikhil Abhyankar",
        "Parshin Shojaee",
        "Chandan K. Reddy"
      ],
      "abstract": "Automated feature engineering plays a critical role in improving predictive\nmodel performance for tabular learning tasks. Traditional automated feature\nengineering methods are limited by their reliance on pre-defined\ntransformations within fixed, manually designed search spaces, often neglecting\ndomain knowledge. Recent advances using Large Language Models (LLMs) have\nenabled the integration of domain knowledge into the feature engineering\nprocess. However, existing LLM-based approaches use direct prompting or rely\nsolely on validation scores for feature selection, failing to leverage insights\nfrom prior feature discovery experiments or establish meaningful reasoning\nbetween feature generation and data-driven performance. To address these\nchallenges, we propose LLM-FE, a novel framework that combines evolutionary\nsearch with the domain knowledge and reasoning capabilities of LLMs to\nautomatically discover effective features for tabular learning tasks. LLM-FE\nformulates feature engineering as a program search problem, where LLMs propose\nnew feature transformation programs iteratively, and data-driven feedback\nguides the search process. Our results demonstrate that LLM-FE consistently\noutperforms state-of-the-art baselines, significantly enhancing the performance\nof tabular prediction models across diverse classification and regression\nbenchmarks.",
      "tldr_zh": "该论文提出 LLM-FE，一种新型框架，利用大型语言模型(LLMs)作为进化优化器，针对表格数据的自动特征工程问题。LLM-FE 将特征工程表述为程序搜索问题，通过 LLMs 迭代生成新的特征转换程序，并结合数据驱动反馈指导搜索过程，从而整合领域知识并解决传统方法的局限性。实验结果显示，LLM-FE 在各种分类和回归基准上显著优于现有基线，提升了表格预测模型的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14434v1",
      "published_date": "2025-03-18 17:11:24 UTC",
      "updated_date": "2025-03-18 17:11:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:28:35.475087"
    },
    {
      "arxiv_id": "2503.14432v1",
      "title": "PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Fang",
        "Yang Zhang",
        "Kaizhi Qian",
        "James Glass",
        "Yada Zhu"
      ],
      "abstract": "Large language models (LLMs) are increasingly integrated with specialized\nexternal tools, yet many tasks demand zero-shot tool usage with minimal or\nnoisy documentation. Existing solutions rely on manual rewriting or labeled\ndata for validation, making them inapplicable in true zero-shot settings. To\naddress these challenges, we propose PLAY2PROMPT, an automated framework that\nsystematically \"plays\" with each tool to explore its input-output behaviors.\nThrough this iterative trial-and-error process, PLAY2PROMPT refines tool\ndocumentation and generates usage examples without any labeled data. These\nexamples not only guide LLM inference but also serve as validation to further\nenhance tool utilization. Extensive experiments on real-world tasks demonstrate\nthat PLAY2PROMPT significantly improves zero-shot tool performance across both\nopen and closed models, offering a scalable and effective solution for\ndomain-specific tool integration.",
      "tldr_zh": "该论文提出 PLAY2PROMPT 框架，用于优化 LLM 代理的零样本（zero-shot）工具指令，通过“工具玩耍”（Tool Play）的迭代试错过程探索工具的输入-输出行为。框架无需标注数据，就能自动优化工具文档并生成使用示例，这些示例用于指导 LLM 推理并增强工具验证。在真实任务实验中，PLAY2PROMPT 显著提升了开放和封闭模型的零样本工具性能，提供了一个可扩展的领域特定工具集成解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14432v1",
      "published_date": "2025-03-18 17:09:57 UTC",
      "updated_date": "2025-03-18 17:09:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:28:47.215288"
    },
    {
      "arxiv_id": "2503.14428v1",
      "title": "MagicComp: Training-free Dual-Phase Refinement for Compositional Video Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Hongyu Zhang",
        "Yufan Deng",
        "Shenghai Yuan",
        "Peng Jin",
        "Zesen Cheng",
        "Yian Zhao",
        "Chang Liu",
        "Jie Chen"
      ],
      "abstract": "Text-to-video (T2V) generation has made significant strides with diffusion\nmodels. However, existing methods still struggle with accurately binding\nattributes, determining spatial relationships, and capturing complex action\ninteractions between multiple subjects. To address these limitations, we\npropose MagicComp, a training-free method that enhances compositional T2V\ngeneration through dual-phase refinement. Specifically, (1) During the\nConditioning Stage: We introduce the Semantic Anchor Disambiguation to\nreinforces subject-specific semantics and resolve inter-subject ambiguity by\nprogressively injecting the directional vectors of semantic anchors into\noriginal text embedding; (2) During the Denoising Stage: We propose Dynamic\nLayout Fusion Attention, which integrates grounding priors and model-adaptive\nspatial perception to flexibly bind subjects to their spatiotemporal regions\nthrough masked attention modulation. Furthermore, MagicComp is a model-agnostic\nand versatile approach, which can be seamlessly integrated into existing T2V\narchitectures. Extensive experiments on T2V-CompBench and VBench demonstrate\nthat MagicComp outperforms state-of-the-art methods, highlighting its potential\nfor applications such as complex prompt-based and trajectory-controllable video\ngeneration. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.",
      "tldr_zh": "本文提出 MagicComp，一种无需训练的 dual-phase refinement 方法，用于提升组合 Text-to-video (T2V) 生成，解决属性绑定、空间关系和多主体动作互动的挑战。在 Conditioning Stage，通过 Semantic Anchor Disambiguation 强化主体语义并注入方向向量以消除歧义；在 Denoising Stage，使用 Dynamic Layout Fusion Attention 整合 grounding priors 和空间感知，通过 masked attention modulation 灵活绑定主体到其时空区域。MagicComp 作为模型无关的通用方法，可无缝集成到现有 T2V 架构中。实验结果显示，它在 T2V-CompBench 和 VBench 基准上超越了最先进方法，展示了在复杂提示和轨迹可控视频生成中的应用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project webpage: https://hong-yu-zhang.github.io/MagicComp-Page/",
      "pdf_url": "http://arxiv.org/pdf/2503.14428v1",
      "published_date": "2025-03-18 17:02:14 UTC",
      "updated_date": "2025-03-18 17:02:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:29:00.401053"
    },
    {
      "arxiv_id": "2503.14427v2",
      "title": "VisEscape: A Benchmark for Evaluating Exploration-driven Decision-making in Virtual Escape Rooms",
      "title_zh": "VisEscape：用于评估虚拟逃脱房间中探索驱动决策的基准",
      "authors": [
        "Seungwon Lim",
        "Sungwoong Kim",
        "Jihwan Yu",
        "Sungjae Lee",
        "Jiwan Chung",
        "Youngjae Yu"
      ],
      "abstract": "Escape rooms present a unique cognitive challenge that demands\nexploration-driven planning: players should actively search their environment,\ncontinuously update their knowledge based on new discoveries, and connect\ndisparate clues to determine which elements are relevant to their objectives.\nMotivated by this, we introduce VisEscape, a benchmark of 20 virtual escape\nrooms specifically designed to evaluate AI models under these challenging\nconditions, where success depends not only on solving isolated puzzles but also\non iteratively constructing and refining spatial-temporal knowledge of a\ndynamically changing environment. On VisEscape, we observe that even\nstate-of-the-art multimodal models generally fail to escape the rooms, showing\nconsiderable variation in their levels of progress and trajectories. To address\nthis issue, we propose VisEscaper, which effectively integrates Memory,\nFeedback, and ReAct modules, demonstrating significant improvements by\nperforming 3.7 times more effectively and 4.9 times more efficiently on average\ncompared to baseline agents.",
      "tldr_zh": "本研究引入了VisEscape基准，这是一个包含20个虚拟逃脱室的测试集，用于评估AI模型在探索驱动决策中的表现，包括环境搜索、知识更新和线索连接，以应对动态空间-时间环境。实验发现，即使是最先进的 multimodal 模型在VisEscape上也难以成功逃脱，显示出明显的进展和轨迹差异。为解决此问题，研究提出VisEscaper框架，该框架整合了Memory、Feedback和ReAct模块，使AI代理的性能平均提升3.7倍有效性和4.9倍效率。总的来说，VisEscape为探索驱动决策的AI发展提供了宝贵基准，并证明了集成式方法的有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14427v2",
      "published_date": "2025-03-18 16:59:09 UTC",
      "updated_date": "2025-03-22 05:06:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:29:10.958393"
    },
    {
      "arxiv_id": "2503.14421v1",
      "title": "ExDDV: A New Dataset for Explainable Deepfake Detection in Video",
      "title_zh": "ExDDV：用于视频中可解释深度伪造检测的新数据集",
      "authors": [
        "Vlad Hondru",
        "Eduard Hogea",
        "Darian Onchis",
        "Radu Tudor Ionescu"
      ],
      "abstract": "The ever growing realism and quality of generated videos makes it\nincreasingly harder for humans to spot deepfake content, who need to rely more\nand more on automatic deepfake detectors. However, deepfake detectors are also\nprone to errors, and their decisions are not explainable, leaving humans\nvulnerable to deepfake-based fraud and misinformation. To this end, we\nintroduce ExDDV, the first dataset and benchmark for Explainable Deepfake\nDetection in Video. ExDDV comprises around 5.4K real and deepfake videos that\nare manually annotated with text descriptions (to explain the artifacts) and\nclicks (to point out the artifacts). We evaluate a number of vision-language\nmodels on ExDDV, performing experiments with various fine-tuning and in-context\nlearning strategies. Our results show that text and click supervision are both\nrequired to develop robust explainable models for deepfake videos, which are\nable to localize and describe the observed artifacts. Our novel dataset and\ncode to reproduce the results are available at\nhttps://github.com/vladhondru25/ExDDV.",
      "tldr_zh": "本文引入了ExDDV数据集，这是首个用于视频中Explainable Deepfake Detection的可解释深度伪造检测基准数据集。ExDDV包含约5.4K真实和深度伪造视频，这些视频通过手动标注的文本描述（解释伪造痕迹）和点击（指向伪造位置）来提供详细监督。作者评估了多种vision-language models，采用微调和in-context learning策略进行实验。结果显示，文本和点击监督相结合是开发鲁棒模型的关键，这些模型能够准确定位和描述伪造痕迹，从而提升深度伪造检测的可解释性和可靠性。数据集及代码已在GitHub上公开。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14421v1",
      "published_date": "2025-03-18 16:55:07 UTC",
      "updated_date": "2025-03-18 16:55:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:29:23.311989"
    },
    {
      "arxiv_id": "2503.14412v1",
      "title": "Iffy-Or-Not: Extending the Web to Support the Critical Evaluation of Fallacious Texts",
      "title_zh": "翻译失败",
      "authors": [
        "Gionnieve Lim",
        "Juho Kim",
        "Simon T. Perrault"
      ],
      "abstract": "Social platforms have expanded opportunities for deliberation with the\ncomments being used to inform one's opinion. However, using such information to\nform opinions is challenged by unsubstantiated or false content. To enhance the\nquality of opinion formation and potentially confer resistance to\nmisinformation, we developed Iffy-Or-Not (ION), a browser extension that seeks\nto invoke critical thinking when reading texts. With three features guided by\nargumentation theory, ION highlights fallacious content, suggests diverse\nqueries to probe them with, and offers deeper questions to consider and chat\nwith others about. From a user study (N=18), we found that ION encourages users\nto be more attentive to the content, suggests queries that align with or are\npreferable to their own, and poses thought-provoking questions that expands\ntheir perspectives. However, some participants expressed aversion to ION due to\nmisalignments with their information goals and thinking predispositions.\nPotential backfiring effects with ION are discussed.",
      "tldr_zh": "本研究开发了 Iffy-Or-Not (ION) 浏览器扩展，旨在通过论证理论(argumentation theory)增强用户对 fallacious texts 的批判性评估，从而改善意见形成并抵抗错误信息。ION 包括三个关键功能：突出谬误内容、建议 diverse queries 来检验它们，以及提供更深入的问题以鼓励与他人讨论。用户研究 (N=18) 显示，该扩展能提高用户对内容的注意力、提出与用户偏好一致或更受欢迎的查询，并扩展他们的视角；然而，一些参与者因其与信息目标和思考倾向不匹配而产生反感，并讨论了潜在的 backfiring effects。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14412v1",
      "published_date": "2025-03-18 16:50:20 UTC",
      "updated_date": "2025-03-18 16:50:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:29:35.092826"
    },
    {
      "arxiv_id": "2503.14411v2",
      "title": "Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Siwei Zhang",
        "Yun Xiong",
        "Yateng Tang",
        "Xi Chen",
        "Zian Jia",
        "Zehao Gu",
        "Jiarong Xu",
        "Jiawei Zhang"
      ],
      "abstract": "Temporal graph neural networks (TGNNs) have shown remarkable performance in\ntemporal graph modeling. However, real-world temporal graphs often possess rich\ntextual information, giving rise to temporal text-attributed graphs (TTAGs).\nSuch combination of dynamic text semantics and evolving graph structures\nintroduces heightened complexity. Existing TGNNs embed texts statically and\nrely heavily on encoding mechanisms that biasedly prioritize structural\ninformation, overlooking the temporal evolution of text semantics and the\nessential interplay between semantics and structures for synergistic\nreinforcement. To tackle these issues, we present \\textbf{CROSS}, a flexible\nframework that seamlessly extends existing TGNNs for TTAG modeling. CROSS is\ndesigned by decomposing the TTAG modeling process into two phases: (i) temporal\nsemantics extraction; and (ii) semantic-structural information unification. The\nkey idea is to advance the large language models (LLMs) to dynamically extract\nthe temporal semantics in text space and then generate cohesive representations\nunifying both semantics and structures. Specifically, we propose a Temporal\nSemantics Extractor in the CROSS framework, which empowers LLMs to offer the\ntemporal semantic understanding of node's evolving contexts of textual\nneighborhoods, facilitating semantic dynamics. Subsequently, we introduce the\nSemantic-structural Co-encoder, which collaborates with the above Extractor for\nsynthesizing illuminating representations by jointly considering both semantic\nand structural information while encouraging their mutual reinforcement.\nExtensive experiments show that CROSS achieves state-of-the-art results on four\npublic datasets and one industrial dataset, with 24.7% absolute MRR gain on\naverage in temporal link prediction and 3.7% AUC gain in node classification of\nindustrial application.",
      "tldr_zh": "本文提出CROSS框架，利用大型语言模型(LLMs)来统一文本语义和图结构的时序建模，解决现有Temporal Graph Neural Networks (TGNNs)在Temporal Text-attributed Graphs (TTAGs)中忽略文本动态语义和语义结构互动的问题。CROSS将建模过程分为两个阶段：首先，通过Temporal Semantics Extractor动态提取节点的时序语义；其次，通过Semantic-structural Co-encoder结合语义和结构信息，促进相互强化。实验结果显示，该框架在四个公开数据集和一个工业数据集上取得最先进性能，平均在时序链接预测中MRR提升24.7%，在节点分类中AUC提升3.7%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Submit to NeurIPS2025",
      "pdf_url": "http://arxiv.org/pdf/2503.14411v2",
      "published_date": "2025-03-18 16:50:10 UTC",
      "updated_date": "2025-05-19 09:45:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:29:48.312990"
    },
    {
      "arxiv_id": "2503.18956v1",
      "title": "International Agreements on AI Safety: Review and Recommendations for a Conditional AI Safety Treaty",
      "title_zh": "关于人工智能安全的国际协议：针对条件性人工智能安全条约的回顾与建议",
      "authors": [
        "Rebecca Scholefield",
        "Samuel Martin",
        "Otto Barten"
      ],
      "abstract": "The malicious use or malfunction of advanced general-purpose AI (GPAI) poses\nrisks that, according to leading experts, could lead to the 'marginalisation or\nextinction of humanity.' To address these risks, there are an increasing number\nof proposals for international agreements on AI safety. In this paper, we\nreview recent (2023-) proposals, identifying areas of consensus and\ndisagreement, and drawing on related literature to assess their feasibility. We\nfocus our discussion on risk thresholds, regulations, types of international\nagreement and five related processes: building scientific consensus,\nstandardisation, auditing, verification and incentivisation.\n  Based on this review, we propose a treaty establishing a compute threshold\nabove which development requires rigorous oversight. This treaty would mandate\ncomplementary audits of models, information security and governance practices,\noverseen by an international network of AI Safety Institutes (AISIs) with\nauthority to pause development if risks are unacceptable. Our approach combines\nimmediately implementable measures with a flexible structure that can adapt to\nongoing research.",
      "tldr_zh": "这篇论文审查了2023年后的AI安全国际协议提案，识别共识和分歧，并评估其可行性，焦点包括风险阈值、法规类型以及相关过程如科学共识、标准化、审计、验证和激励。作者强调高级通用AI (GPAI) 的恶意使用或故障可能导致人类边缘化或灭绝的风险。基于此，他们推荐一个条件AI安全条约，设定compute threshold作为开发门槛，要求对模型、信息安全和治理实践进行补充审计，并由国际AI Safety Institutes (AISIs)网络监督，以权暂停高风险开发并适应未来研究。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "34 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.18956v1",
      "published_date": "2025-03-18 16:29:57 UTC",
      "updated_date": "2025-03-18 16:29:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:29:59.999978"
    },
    {
      "arxiv_id": "2503.14577v1",
      "title": "PHGNN: A Novel Prompted Hypergraph Neural Network to Diagnose Alzheimer's Disease",
      "title_zh": "翻译失败",
      "authors": [
        "Chenyu Liu",
        "Luca Rossi"
      ],
      "abstract": "The accurate diagnosis of Alzheimer's disease (AD) and prognosis of mild\ncognitive impairment (MCI) conversion are crucial for early intervention.\nHowever, existing multimodal methods face several challenges, from the\nheterogeneity of input data, to underexplored modality interactions, missing\ndata due to patient dropouts, and limited data caused by the time-consuming and\ncostly data collection process. In this paper, we propose a novel Prompted\nHypergraph Neural Network (PHGNN) framework that addresses these limitations by\nintegrating hypergraph based learning with prompt learning. Hypergraphs capture\nhigher-order relationships between different modalities, while our prompt\nlearning approach for hypergraphs, adapted from NLP, enables efficient training\nwith limited data. Our model is validated through extensive experiments on the\nADNI dataset, outperforming SOTA methods in both AD diagnosis and the\nprediction of MCI conversion.",
      "tldr_zh": "本研究针对阿尔茨海默病（Alzheimer's disease, AD）的准确诊断和轻度认知障碍（mild cognitive impairment, MCI）转化的预后问题，提出了一种新型 Prompted Hypergraph Neural Network (PHGNN) 框架，以应对多模态数据异质性、模态交互不足、数据缺失和数据有限的挑战。PHGNN 通过 hypergraph 基于学习捕捉不同模态间的高阶关系，并借鉴自然语言处理（NLP）的 prompt learning 方法，实现高效训练。实验在 ADNI 数据集上验证，该框架在 AD 诊断和 MCI 转换预测方面均优于现有最先进（SOTA）方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14577v1",
      "published_date": "2025-03-18 16:10:43 UTC",
      "updated_date": "2025-03-18 16:10:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:30:13.343537"
    },
    {
      "arxiv_id": "2503.14376v2",
      "title": "Tiled Flash Linear Attention: More Efficient Linear RNN and xLSTM Kernels",
      "title_zh": "翻译失败",
      "authors": [
        "Maximilian Beck",
        "Korbinian Pöppel",
        "Phillip Lippe",
        "Sepp Hochreiter"
      ],
      "abstract": "Linear RNNs with gating recently demonstrated competitive performance\ncompared to Transformers in language modeling. Although their linear compute\nscaling in sequence length offers theoretical runtime advantages over\nTransformers, realizing these benefits in practice requires optimized custom\nkernels, as Transformers rely on the highly efficient Flash Attention kernels\n(Dao, 2024). Leveraging the chunkwise-parallel formulation of linear RNNs,\nFlash Linear Attention (FLA) (Yang & Zhang, 2024) shows that linear RNN kernels\nare faster than Flash Attention, by parallelizing over chunks of the input\nsequence. However, since the chunk size of FLA is limited, many intermediate\nstates must be materialized in GPU memory. This leads to low arithmetic\nintensity and causes high memory consumption and IO cost, especially for\nlong-context pre-training. In this work, we present Tiled Flash Linear\nAttention (TFLA), a novel kernel algorithm for linear RNNs, that enables\narbitrary large chunk sizes and high arithmetic intensity by introducing an\nadditional level of sequence parallelization within each chunk. First, we apply\nTFLA to the xLSTM with matrix memory, the mLSTM (Beck et al., 2024). Second, we\npropose an mLSTM variant with sigmoid input gate and reduced computation for\neven faster kernel runtimes at equal language modeling performance. In our\nspeed benchmarks, we show that our new mLSTM kernels based on TFLA outperform\nhighly optimized Flash Attention, Linear Attention and Mamba kernels, setting a\nnew state of the art for efficient long-context sequence modeling primitives.",
      "tldr_zh": "本论文提出了 Tiled Flash Linear Attention (TFLA)，一种新型内核算法，用于优化 Linear RNNs 和 xLSTM，提升序列建模效率。TFLA 通过在每个输入 chunk 内引入额外的序列并行化，实现任意大的 chunk 大小、提高算术强度，并显著减少 GPU 内存消耗和 IO 成本，尤其适用于长上下文预训练。作者将 TFLA 应用于 mLSTM 并提出其变体，使用 sigmoid 输入门和减少计算，进一步提升运行速度；在基准测试中，新内核超越了高度优化的 Flash Attention、Linear Attention 和 Mamba 内核，树立了高效长上下文序列建模的新标准。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Code available at: https://github.com/NX-AI/mlstm_kernels",
      "pdf_url": "http://arxiv.org/pdf/2503.14376v2",
      "published_date": "2025-03-18 16:09:47 UTC",
      "updated_date": "2025-05-10 08:07:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:30:23.989689"
    },
    {
      "arxiv_id": "2503.14576v2",
      "title": "SocialJax: An Evaluation Suite for Multi-agent Reinforcement Learning in Sequential Social Dilemmas",
      "title_zh": "SocialJax：一种针对顺序社会困境中多智能体强化学习的评估套件",
      "authors": [
        "Zihao Guo",
        "Shuqing Shi",
        "Richard Willis",
        "Tristan Tomilin",
        "Joel Z. Leibo",
        "Yali Du"
      ],
      "abstract": "Sequential social dilemmas pose a significant challenge in the field of\nmulti-agent reinforcement learning (MARL), requiring environments that\naccurately reflect the tension between individual and collective interests.\nPrevious benchmarks and environments, such as Melting Pot, provide an\nevaluation protocol that measures generalization to new social partners in\nvarious test scenarios. However, running reinforcement learning algorithms in\ntraditional environments requires substantial computational resources. In this\npaper, we introduce SocialJax, a suite of sequential social dilemma\nenvironments and algorithms implemented in JAX. JAX is a high-performance\nnumerical computing library for Python that enables significant improvements in\noperational efficiency. Our experiments demonstrate that the SocialJax training\npipeline achieves at least 50\\texttimes{} speed-up in real-time performance\ncompared to Melting Pot RLlib baselines. Additionally, we validate the\neffectiveness of baseline algorithms within SocialJax environments. Finally, we\nuse Schelling diagrams to verify the social dilemma properties of these\nenvironments, ensuring that they accurately capture the dynamics of social\ndilemmas.",
      "tldr_zh": "本文提出 SocialJax，这是一个基于 JAX 的环境和算法套件，用于评估多智能体强化学习（MARL）中的顺序社会困境（Sequential Social Dilemmas），旨在解决现有基准如 Melting Pot 在计算资源需求上的问题。SocialJax 通过 JAX 的高性能计算实现了至少 50 倍的实时训练速度提升，并验证了基准算法的有效性。最终，使用 Schelling diagrams 确认了这些环境的社交困境动态，确保其准确反映个体与集体利益的冲突。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14576v2",
      "published_date": "2025-03-18 16:03:59 UTC",
      "updated_date": "2025-05-19 15:28:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:30:35.634309"
    },
    {
      "arxiv_id": "2503.14354v1",
      "title": "Retrospective: A CORDIC Based Configurable Activation Function for NN Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Omkar Kokane",
        "Gopal Raut",
        "Salim Ullah",
        "Mukul Lokhande",
        "Adam Teman",
        "Akash Kumar",
        "Santosh Kumar Vishvakarma"
      ],
      "abstract": "A CORDIC-based configuration for the design of Activation Functions (AF) was\npreviously suggested to accelerate ASIC hardware design for\nresource-constrained systems by providing functional reconfigurability. Since\nits introduction, this new approach for neural network acceleration has gained\nwidespread popularity, influencing numerous designs for activation functions in\nboth academic and commercial AI processors. In this retrospective analysis, we\nexplore the foundational aspects of this initiative, summarize key developments\nover recent years, and introduce the DA-VINCI AF tailored for the evolving\nneeds of AI applications. This new generation of dynamically configurable and\nprecision-adjustable activation function cores promise greater adaptability for\na range of activation functions in AI workloads, including Swish, SoftMax,\nSeLU, and GeLU, utilizing the Shift-and-Add CORDIC technique. The previously\npresented design has been optimized for MAC, Sigmoid, and Tanh functionalities\nand incorporated into ReLU AFs, culminating in an accumulative NEURIC compute\nunit. These enhancements position NEURIC as a fundamental component in the\nresource-efficient vector engine for the realization of AI accelerators that\nfocus on DNNs, RNNs/LSTMs, and Transformers, achieving a quality of results\n(QoR) of 98.5%.",
      "tldr_zh": "本回顾性分析探讨了基于 CORDIC 的可配置激活函数（AF）设计，用于加速资源受限系统的 ASIC 硬件神经网络应用，该方法已广泛影响学术和商业 AI 处理器。论文总结了该设计的基石发展，并引入了新一代 DA-VINCI AF，支持动态配置和精度调整，能适应多种激活函数如 Swish、SoftMax、SeLU 和 GeLU，通过 Shift-and-Add CORDIC 技术优化。之前的优化已应用于 MAC、Sigmoid、Tanh 和 ReLU 等功能，形成 NEURIC 计算单元，作为资源高效的向量引擎核心，支持 DNNs、RNNs/LSTMs 和 Transformers，实现高达 98.5% 的质量结果（QoR）。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.CV",
        "cs.ET",
        "eess.IV"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14354v1",
      "published_date": "2025-03-18 15:38:37 UTC",
      "updated_date": "2025-03-18 15:38:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:30:47.391510"
    },
    {
      "arxiv_id": "2503.14350v2",
      "title": "VEGGIE: Instructional Editing and Reasoning of Video Concepts with Grounded Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Shoubin Yu",
        "Difan Liu",
        "Ziqiao Ma",
        "Yicong Hong",
        "Yang Zhou",
        "Hao Tan",
        "Joyce Chai",
        "Mohit Bansal"
      ],
      "abstract": "Recent video diffusion models have enhanced video editing, but it remains\nchallenging to handle instructional editing and diverse tasks (e.g., adding,\nremoving, changing) within a unified framework. In this paper, we introduce\nVEGGIE, a Video Editor with Grounded Generation from Instructions, a simple\nend-to-end framework that unifies video concept editing, grounding, and\nreasoning based on diverse user instructions. Specifically, given a video and\ntext query, VEGGIE first utilizes an MLLM to interpret user intentions in\ninstructions and ground them to the video contexts, generating frame-specific\ngrounded task queries for pixel-space responses. A diffusion model then renders\nthese plans and generates edited videos that align with user intent. To support\ndiverse tasks and complex instructions, we employ a curriculum learning\nstrategy: first aligning the MLLM and video diffusion model with large-scale\ninstructional image editing data, followed by end-to-end fine-tuning on\nhigh-quality multitask video data. Additionally, we introduce a novel data\nsynthesis pipeline to generate paired instructional video editing data for\nmodel training. It transforms static image data into diverse, high-quality\nvideo editing samples by leveraging Image-to-Video models to inject dynamics.\nVEGGIE shows strong performance in instructional video editing with different\nediting skills, outperforming the best instructional baseline as a versatile\nmodel, while other models struggle with multi-tasking. VEGGIE also excels in\nvideo object grounding and reasoning segmentation, where other baselines fail.\nWe further reveal how the multiple tasks help each other and highlight\npromising applications like zero-shot multimodal instructional and in-context\nvideo editing.",
      "tldr_zh": "本文提出 VEGGIE，一种统一的端到端框架，用于基于用户指令的视频概念编辑、接地和推理，支持多样任务如添加、移除或改变元素。框架利用 MLLM 解释用户意图并生成帧特定的任务查询，然后通过 diffusion model 渲染编辑视频，并采用 curriculum learning 策略结合新型数据合成管道（将静态图像数据转化为动态视频样本）进行训练。实验结果表明，VEGGIE 在指令视频编辑、对象接地和推理分割任务中显著优于基线模型，展示了多任务间互助效应以及零样本多模态和上下文编辑的应用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "First three authors contributed equally. Project page:\n  https://veggie-gen.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.14350v2",
      "published_date": "2025-03-18 15:31:12 UTC",
      "updated_date": "2025-03-19 20:33:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:31:00.426214"
    },
    {
      "arxiv_id": "2503.16534v1",
      "title": "Gender and content bias in Large Language Models: a case study on Google Gemini 2.0 Flash Experimental",
      "title_zh": "大型",
      "authors": [
        "Roberto Balestri"
      ],
      "abstract": "This study evaluates the biases in Gemini 2.0 Flash Experimental, a\nstate-of-the-art large language model (LLM) developed by Google, focusing on\ncontent moderation and gender disparities. By comparing its performance to\nChatGPT-4o, examined in a previous work of the author, the analysis highlights\nsome differences in ethical moderation practices. Gemini 2.0 demonstrates\nreduced gender bias, notably with female-specific prompts achieving a\nsubstantial rise in acceptance rates compared to results obtained by\nChatGPT-4o. It adopts a more permissive stance toward sexual content and\nmaintains relatively high acceptance rates for violent prompts, including\ngender-specific cases. Despite these changes, whether they constitute an\nimprovement is debatable. While gender bias has been reduced, this reduction\ncomes at the cost of permitting more violent content toward both males and\nfemales, potentially normalizing violence rather than mitigating harm.\nMale-specific prompts still generally receive higher acceptance rates than\nfemale-specific ones. These findings underscore the complexities of aligning AI\nsystems with ethical standards, highlighting progress in reducing certain\nbiases while raising concerns about the broader implications of the model's\npermissiveness. Ongoing refinements are essential to achieve moderation\npractices that ensure transparency, fairness, and inclusivity without\namplifying harmful content.",
      "tldr_zh": "本研究评估了 Google 的 Large Language Models（LLMs）——Gemini 2.0 Flash Experimental 在内容审查和性别偏差方面的表现，通过与 ChatGPT-4o 的比较，揭示了其在伦理调节实践上的差异。Gemini 2.0 显著降低了性别偏差，例如女性特定提示的接受率大幅上升，但同时对性内容和暴力提示持更宽容态度，导致暴力内容（如针对男性和女性的）更容易通过。总体而言，虽然性别偏差有所减少，但这种变化可能加剧了暴力正常化的风险，且男性特定提示的接受率仍高于女性，突显了 AI 系统在追求透明、公平和包容性时面临的伦理挑战，需要持续优化。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16534v1",
      "published_date": "2025-03-18 15:28:22 UTC",
      "updated_date": "2025-03-18 15:28:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:31:11.678751"
    },
    {
      "arxiv_id": "2503.14345v2",
      "title": "MoonCast: High-Quality Zero-Shot Podcast Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Zeqian Ju",
        "Dongchao Yang",
        "Jianwei Yu",
        "Kai Shen",
        "Yichong Leng",
        "Zhengtao Wang",
        "Xu Tan",
        "Xinyu Zhou",
        "Tao Qin",
        "Xiangyang Li"
      ],
      "abstract": "Recent advances in text-to-speech synthesis have achieved notable success in\ngenerating high-quality short utterances for individual speakers. However,\nthese systems still face challenges when extending their capabilities to long,\nmulti-speaker, and spontaneous dialogues, typical of real-world scenarios such\nas podcasts. These limitations arise from two primary challenges: 1) long\nspeech: podcasts typically span several minutes, exceeding the upper limit of\nmost existing work; 2) spontaneity: podcasts are marked by their spontaneous,\noral nature, which sharply contrasts with formal, written contexts; existing\nworks often fall short in capturing this spontaneity. In this paper, we propose\nMoonCast, a solution for high-quality zero-shot podcast generation, aiming to\nsynthesize natural podcast-style speech from text-only sources (e.g., stories,\ntechnical reports, news in TXT, PDF, or Web URL formats) using the voices of\nunseen speakers. To generate long audio, we adopt a long-context language\nmodel-based audio modeling approach utilizing large-scale long-context speech\ndata. To enhance spontaneity, we utilize a podcast generation module to\ngenerate scripts with spontaneous details, which have been empirically shown to\nbe as crucial as the text-to-speech modeling itself. Experiments demonstrate\nthat MoonCast outperforms baselines, with particularly notable improvements in\nspontaneity and coherence.",
      "tldr_zh": "本文提出 MoonCast，一种零样本（zero-shot）高品质播客生成系统，旨在解决现有文本到语音（text-to-speech）技术在处理长时、多说话人和自发对话时的挑战，如播客的长度限制和自发性不足。MoonCast 通过采用长上下文语言模型和大规模长上下文语音数据来生成长音频，并利用播客生成模块创建带有自发细节的脚本，以提升语音的自然性和连贯性。实验结果表明，该系统在自发性和连贯性方面显著优于基线模型，为从纯文本（如 TXT、PDF 或 Web URL）合成自然播客式语音提供了有效解决方案。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14345v2",
      "published_date": "2025-03-18 15:25:08 UTC",
      "updated_date": "2025-03-19 07:17:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:31:23.687997"
    },
    {
      "arxiv_id": "2503.14341v1",
      "title": "Spatio-Temporal Graph Neural Networks for Infant Language Acquisition Prediction",
      "title_zh": "用于婴儿语言习得预测的时空图神经网络",
      "authors": [
        "Andrew Roxburgh",
        "Floriana Grasso",
        "Terry R. Payne"
      ],
      "abstract": "Predicting the words that a child is going to learn next can be useful for\nboosting language acquisition, and such predictions have been shown to be\npossible with both neural network techniques (looking at changes in the\nvocabulary state over time) and graph model (looking at data pertaining to the\nrelationships between words). However, these models do not fully capture the\ncomplexity of the language learning process of an infant when used in\nisolation. In this paper, we examine how a model of language acquisition for\ninfants and young children can be constructed and adapted for use in a\nSpatio-Temporal Graph Convolutional Network (STGCN), taking into account the\ndifferent types of linguistic relationships that occur during child language\nlearning. We introduce a novel approach for predicting child vocabulary\nacquisition, and evaluate the efficacy of such a model with respect to the\ndifferent types of linguistic relationships that occur during language\nacquisition, resulting in insightful observations on model calibration and norm\nselection. An evaluation of this model found that the mean accuracy of models\nfor predicting new words when using sensorimotor relationships (0.733) and\nsemantic relationships (0.729) were found to be superior to that observed with\na 2-layer Feed-forward neural network. Furthermore, the high recall for some\nrelationships suggested that some relationships (e.g. visual) were superior in\nidentifying a larger proportion of relevant words that a child should\nsubsequently learn than others (such as auditory).",
      "tldr_zh": "本论文提出了一种基于 Spatio-Temporal Graph Neural Networks (STGCN) 的方法，用于预测婴儿语言习得过程，旨在整合时空动态和不同语言关系（如 sensorimotor 和 semantic），以克服传统神经网络模型的局限性。研究引入新颖的预测模型，评估了各种语言关系对词汇习得的影响，包括实验观察到 sensorimotor 关系（准确率 0.733）和 semantic 关系（准确率 0.729）优于 2 层 Feed-forward neural network。结果还表明，某些关系如 visual 在召回率上表现更佳，能更有效地识别儿童后续学习的关键单词，为语言习得预测提供更全面的见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14341v1",
      "published_date": "2025-03-18 15:21:27 UTC",
      "updated_date": "2025-03-18 15:21:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:31:35.760120"
    },
    {
      "arxiv_id": "2503.15551v1",
      "title": "Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting Attack",
      "title_zh": "翻译失败",
      "authors": [
        "Murong Yue",
        "Ziyu Yao"
      ],
      "abstract": "Batch prompting, which combines a batch of multiple queries sharing the same\ncontext in one inference, has emerged as a promising solution to reduce\ninference costs. However, our study reveals a significant security\nvulnerability in batch prompting: malicious users can inject attack\ninstructions into a batch, leading to unwanted interference across all queries,\nwhich can result in the inclusion of harmful content, such as phishing links,\nor the disruption of logical reasoning. In this paper, we construct\nBATCHSAFEBENCH, a comprehensive benchmark comprising 150 attack instructions of\ntwo types and 8k batch instances, to study the batch prompting vulnerability\nsystematically. Our evaluation of both closed-source and open-weight LLMs\ndemonstrates that all LLMs are susceptible to batch-prompting attacks. We then\nexplore multiple defending approaches. While the prompting-based defense shows\nlimited effectiveness for smaller LLMs, the probing-based approach achieves\nabout 95% accuracy in detecting attacks. Additionally, we perform a mechanistic\nanalysis to understand the attack and identify attention heads that are\nresponsible for it.",
      "tldr_zh": "这篇论文揭示了批处理提示（batch prompting）在降低 LLM 推理成本的同时存在的安全漏洞，恶意用户可注入攻击指令，导致查询间干扰并产生有害内容或逻辑破坏。研究者构建了 BATCHSAFEBENCH 基准，包括 150 种攻击指令和 8k 批处理实例，对闭源和开源 LLM 进行评估，发现所有模型均易受攻击。论文探索了多种防御方法，其中基于探测的方法可实现约 95% 的检测准确率，并通过机制分析识别了负责攻击的注意力头（attention heads）。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15551v1",
      "published_date": "2025-03-18 15:16:10 UTC",
      "updated_date": "2025-03-18 15:16:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:31:48.315977"
    },
    {
      "arxiv_id": "2503.14333v1",
      "title": "Revealing higher-order neural representations with generative artificial intelligence",
      "title_zh": "翻译失败",
      "authors": [
        "Hojjat Azimi Asrari",
        "Megan A. K. Peters"
      ],
      "abstract": "Studies often aim to reveal how neural representations encode aspects of an\nobserver's environment, such as its contents or structure. These are\n``first-order\" representations (FORs), because they're ``about\" the external\nworld. A less-common target is ``higher-order\" representations (HORs), which\nare ``about\" FORs -- their contents, stability, or uncertainty. HORs of\nuncertainty appear critically involved in adaptive behaviors including learning\nunder uncertainty, influencing learning rates and internal model updating based\non environmental feedback. However, HORs about uncertainty are unlikely to be\ndirect ``read-outs\" of FOR characteristics, instead reflecting estimation\nprocesses which may be lossy, bias-prone, or distortive and which may also\nincorporate estimates of distributions of uncertainty the observer is likely to\nexperience. While some research has targeted neural representations of\n``instantaneously\" estimated uncertainty, how the brain represents\n\\textit{distributions} of expected uncertainty remains largely unexplored.\nHere, we propose a novel reinforcement learning (RL) based generative\nartificial intelligence (genAI) approach to explore neural representations of\nuncertainty distributions. We use existing functional magnetic resonance\nimaging data, where humans learned to `de-noise' their brain states to achieve\ntarget neural patterns, to train denoising diffusion genAI models with RL\nalgorithms to learn noise distributions similar to how humans might learn to do\nthe same. We then explore these models' learned noise-distribution HORs\ncompared to control models trained with traditional backpropagation. Results\nreveal model-dependent differences in noise distribution representations --\nwith the RL-based model offering much higher explanatory power for human\nbehavior -- offering an exciting path towards using genAI to explore neural\nnoise-distribution HORs.",
      "tldr_zh": "本研究探讨了高阶神经表示 (HORs)，这些表示关注第一阶表示 (FORs) 的内容、不确定性和稳定性，特别是不确定性分布在适应性学习中的作用。作者提出了一种基于强化学习 (RL) 的生成式人工智能 (genAI) 方法，使用现有功能磁共振成像 (fMRI) 数据训练去噪扩散模型，让模型学习噪声分布以模拟人类行为。相比传统反向传播 (backpropagation) 控制模型，RL-based 模型在解释人类行为方面表现出更高的解释力，为探索神经不确定性分布提供了新路径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14333v1",
      "published_date": "2025-03-18 15:08:19 UTC",
      "updated_date": "2025-03-18 15:08:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:31:58.783529"
    },
    {
      "arxiv_id": "2503.14321v1",
      "title": "COPA: Comparing the Incomparable to Explore the Pareto Front",
      "title_zh": "翻译失败",
      "authors": [
        "Adrián Javaloy",
        "Antonio Vergari",
        "Isabel Valera"
      ],
      "abstract": "In machine learning (ML), it is common to account for multiple objectives\nwhen, e.g., selecting a model to deploy. However, it is often unclear how one\nshould compare, aggregate and, ultimately, trade-off these objectives, as they\nmight be measured in different units or scales. For example, when deploying\nlarge language models (LLMs), we might not only care about their performance,\nbut also their CO2 consumption. In this work, we investigate how objectives can\nbe sensibly compared and aggregated to navigate their Pareto front. To do so,\nwe propose to make incomparable objectives comparable via their CDFs,\napproximated by their relative rankings. This allows us to aggregate them while\nmatching user-specific preferences, allowing practitioners to meaningfully\nnavigate and search for models in the Pareto front. We demonstrate the\npotential impact of our methodology in diverse areas such as LLM selection,\ndomain generalization, and AutoML benchmarking, where classical ways to\naggregate and normalize objectives fail.",
      "tldr_zh": "本文提出 COPA 方法，用于处理机器学习中多目标优化的挑战，特别是当目标单位或规模不同（如 LLM 的性能和 CO2 消耗）时，如何比较和聚合这些目标。COPA 通过利用 CDFs（累积分布函数）和相对排名，使不可比较的目标变得可比较，并根据用户特定偏好进行聚合，从而在 Pareto front 上有效导航和搜索模型。该方法在 LLM 选择、领域泛化以及 AutoML 基准测试等领域显示出显著优势，优于传统聚合和归一化技术。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 14 figures. Under submission",
      "pdf_url": "http://arxiv.org/pdf/2503.14321v1",
      "published_date": "2025-03-18 14:51:42 UTC",
      "updated_date": "2025-03-18 14:51:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:32:11.620107"
    },
    {
      "arxiv_id": "2503.16533v1",
      "title": "From Patient Consultations to Graphs: Leveraging LLMs for Patient Journey Knowledge Graph Construction",
      "title_zh": "从患者咨询到图形：利用大型语言模型构建患者旅程知识图",
      "authors": [
        "Hassan S. Al Khatib",
        "Sudip Mittal",
        "Shahram Rahimi",
        "Nina Marhamati",
        "Sean Bozorgzad"
      ],
      "abstract": "The transition towards patient-centric healthcare necessitates a\ncomprehensive understanding of patient journeys, which encompass all healthcare\nexperiences and interactions across the care spectrum. Existing healthcare data\nsystems are often fragmented and lack a holistic representation of patient\ntrajectories, creating challenges for coordinated care and personalized\ninterventions. Patient Journey Knowledge Graphs (PJKGs) represent a novel\napproach to addressing the challenge of fragmented healthcare data by\nintegrating diverse patient information into a unified, structured\nrepresentation. This paper presents a methodology for constructing PJKGs using\nLarge Language Models (LLMs) to process and structure both formal clinical\ndocumentation and unstructured patient-provider conversations. These graphs\nencapsulate temporal and causal relationships among clinical encounters,\ndiagnoses, treatments, and outcomes, enabling advanced temporal reasoning and\npersonalized care insights. The research evaluates four different LLMs, such as\nClaude 3.5, Mistral, Llama 3.1, and Chatgpt4o, in their ability to generate\naccurate and computationally efficient knowledge graphs. Results demonstrate\nthat while all models achieved perfect structural compliance, they exhibited\nvariations in medical entity processing and computational efficiency. The paper\nconcludes by identifying key challenges and future research directions. This\nwork contributes to advancing patient-centric healthcare through the\ndevelopment of comprehensive, actionable knowledge graphs that support improved\ncare coordination and outcome prediction.",
      "tldr_zh": "该论文探讨了如何利用大型语言模型（LLMs）构建患者旅程知识图（PJKGs），以整合碎片化的医疗数据，实现患者导向医疗。研究方法包括使用 LLMs 处理正式临床文档和非结构化患者对话，生成包含时间和因果关系的结构化知识图，并评估了 Claude 3.5、Mistral、Llama 3.1 和 ChatGPT-4o 等模型在准确性和计算效率方面的表现。结果显示，所有模型在结构上完全符合要求，但医疗实体处理和效率存在差异。该工作为提升护理协调、个性化干预和结果预测提供了新途径，并指出了未来研究挑战。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16533v1",
      "published_date": "2025-03-18 14:44:28 UTC",
      "updated_date": "2025-03-18 14:44:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:32:23.269932"
    },
    {
      "arxiv_id": "2503.14295v2",
      "title": "PC-Talk: Precise Facial Animation Control for Audio-Driven Talking Face Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Baiqin Wang",
        "Xiangyu Zhu",
        "Fan Shen",
        "Hao Xu",
        "Zhen Lei"
      ],
      "abstract": "Recent advancements in audio-driven talking face generation have made great\nprogress in lip synchronization. However, current methods often lack sufficient\ncontrol over facial animation such as speaking style and emotional expression,\nresulting in uniform outputs. In this paper, we focus on improving two key\nfactors: lip-audio alignment and emotion control, to enhance the diversity and\nuser-friendliness of talking videos. Lip-audio alignment control focuses on\nelements like speaking style and the scale of lip movements, whereas emotion\ncontrol is centered on generating realistic emotional expressions, allowing for\nmodifications in multiple attributes such as intensity. To achieve precise\ncontrol of facial animation, we propose a novel framework, PC-Talk, which\nenables lip-audio alignment and emotion control through implicit keypoint\ndeformations. First, our lip-audio alignment control module facilitates precise\nediting of speaking styles at the word level and adjusts lip movement scales to\nsimulate varying vocal loudness levels, maintaining lip synchronization with\nthe audio. Second, our emotion control module generates vivid emotional facial\nfeatures with pure emotional deformation. This module also enables the fine\nmodification of intensity and the combination of multiple emotions across\ndifferent facial regions. Our method demonstrates outstanding control\ncapabilities and achieves state-of-the-art performance on both HDTF and MEAD\ndatasets in extensive experiments.",
      "tldr_zh": "本研究提出PC-Talk框架，旨在提升音频驱动说话面部生成的精确控制，重点解决唇部-音频对齐和情感表达的不足，以增加输出多样性和用户友好性。PC-Talk通过隐式关键点变形实现唇部-音频对齐控制模块，支持在单词级别编辑说话风格并调整唇部运动规模以模拟声音响度，同时确保唇部同步；以及情感控制模块，生成生动的感情面部特征，并允许细调强度和组合多种情感于不同区域。实验在HDTF和MEAD数据集上显示，该方法实现了最先进性能，显著提高了面部动画的真实性和控制能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14295v2",
      "published_date": "2025-03-18 14:35:48 UTC",
      "updated_date": "2025-03-20 10:27:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:32:35.478474"
    },
    {
      "arxiv_id": "2503.14293v2",
      "title": "Ensemble Knowledge Distillation for Machine Learning Interatomic Potentials",
      "title_zh": "翻译失败",
      "authors": [
        "Sakib Matin",
        "Emily Shinkle",
        "Yulia Pimonova",
        "Galen T. Craven",
        "Aleksandra Pachalieva",
        "Ying Wai Li",
        "Kipton Barros",
        "Nicholas Lubbers"
      ],
      "abstract": "Machine learning interatomic potentials (MLIPs) are a promising tool to\naccelerate atomistic simulations and molecular property prediction. The quality\nof MLIPs strongly depends on the quantity of available training data as well as\nthe quantum chemistry (QC) level of theory used to generate that data. Datasets\ngenerated with high-fidelity QC methods, such as coupled cluster, are typically\nrestricted to small molecules and may be missing energy gradients. With this\nlimited quantity of data, it is often difficult to train good MLIP models. We\npresent an ensemble knowledge distillation (EKD) method to improve MLIP\naccuracy when trained to energy-only datasets. In our EKD approach, first,\nmultiple teacher models are trained to QC energies and then used to generate\natomic forces for all configurations in the dataset. Next, a student MLIP is\ntrained to both QC energies and to ensemble-averaged forces generated by the\nteacher models. We apply this workflow on the ANI-1ccx dataset which consists\nof organic molecules with configuration energies computed at the coupled\ncluster level of theory. The resulting student MLIPs achieve new\nstate-of-the-art accuracy on the out-of-sample COMP6 benchmark and improved\nstability for molecular dynamics simulations. The EKD approach for MLIP is\nbroadly applicable for chemical, biomolecular and materials science\nsimulations.",
      "tldr_zh": "本文提出了一种集成知识蒸馏(EKD)方法，用于提升机器学习原子间势(MLIPs)在仅能量数据集上的准确性，通过训练多个教师模型生成原子力和能量梯度，然后训练学生MLIP模型来拟合量子化学(QC)能量和这些力平均值。  \n在ANI-1ccx数据集（基于耦合簇理论计算的有机分子能量）上应用EKD后，学生MLIPs在COMP6基准测试中实现了新的最先进准确性，并显著改善了分子动力学模拟的稳定性。  \n该方法广泛适用于化学、生物分子和材料科学领域的原子模拟和属性预测。",
      "categories": [
        "physics.chem-ph",
        "cs.AI"
      ],
      "primary_category": "physics.chem-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14293v2",
      "published_date": "2025-03-18 14:32:51 UTC",
      "updated_date": "2025-03-19 15:03:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:32:48.013589"
    },
    {
      "arxiv_id": "2503.14572v1",
      "title": "Robust Weight Imprinting: Insights from Neural Collapse and Proxy-Based Aggregation",
      "title_zh": "翻译失败",
      "authors": [
        "Justus Westerhoff",
        "Golzar Atefi",
        "Mario Koddenbrock",
        "Alexei Figueroa",
        "Alexander Löser",
        "Erik Rodner",
        "Felix A. Gers"
      ],
      "abstract": "The capacity of a foundation model allows for adaptation to new downstream\ntasks. Weight imprinting is a universal and efficient method to fulfill this\npurpose. It has been reinvented several times, but it has not been\nsystematically studied. In this paper, we propose a framework for imprinting,\nidentifying three main components: generation, normalization, and aggregation.\nThis allows us to conduct an in-depth analysis of imprinting and a comparison\nof the existing work. We reveal the benefits of representing novel data with\nmultiple proxies in the generation step and show the importance of proper\nnormalization. We determine those proxies through clustering and propose a\nnovel variant of imprinting that outperforms previous work. We motivate this by\nthe neural collapse phenomenon -- an important connection that we can draw for\nthe first time. Our results show an increase of up to 4% in challenging\nscenarios with complex data distributions for new classes.",
      "tldr_zh": "本研究系统分析了Weight Imprinting作为一种适应下游任务的通用高效方法，提出一个框架，包括generation（生成）、normalization（归一化）和aggregation（聚合）三个主要组件，以比较现有工作并揭示其关键优势。研究强调在generation步骤中使用多个proxies表示新数据的重要性，并通过clustering（聚类）确定这些proxies，进而提出一个新变体，该变体受Neural Collapse现象启发，显著提升了性能。实验结果显示，在复杂数据分布的新类场景中，该方法比之前工作提高了高达4%的准确率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Code: https://github.com/DATEXIS/multi-imprinting/",
      "pdf_url": "http://arxiv.org/pdf/2503.14572v1",
      "published_date": "2025-03-18 14:27:45 UTC",
      "updated_date": "2025-03-18 14:27:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:32:58.706046"
    },
    {
      "arxiv_id": "2503.14273v2",
      "title": "Manual Labelling Artificially Inflates Deep Learning-Based Segmentation Performance on RGB Images of Closed Canopy: Validation Using TLS",
      "title_zh": "翻译失败",
      "authors": [
        "Matthew J. Allen",
        "Harry J. F. Owen",
        "Stuart W. D. Grieve",
        "Emily R. Lines"
      ],
      "abstract": "Monitoring forest dynamics at an individual tree scale is essential for\naccurately assessing ecosystem responses to climate change, yet traditional\nmethods relying on field-based forest inventories are labor-intensive and\nlimited in spatial coverage. Advances in remote sensing using drone-acquired\nRGB imagery combined with deep learning models have promised precise individual\ntree crown (ITC) segmentation; however, existing methods are frequently\nvalidated against human-annotated images, lacking rigorous independent ground\ntruth. In this study, we generate high-fidelity validation labels from\nco-located Terrestrial Laser Scanning (TLS) data for drone imagery of mixed\nunmanaged boreal and Mediterranean forests. We evaluate the performance of two\nwidely used deep learning ITC segmentation models - DeepForest (RetinaNet) and\nDetectree2 (Mask R-CNN) - on these data, and compare to performance on further\nMediterranean forest data labelled manually. When validated against TLS-derived\nground truth from Mediterranean forests, model performance decreased\nsignificantly compared to assessment based on hand-labelled from an\necologically similar site (AP50: 0.094 vs. 0.670). Restricting evaluation to\nonly canopy trees shrank this gap considerably (Canopy AP50: 0.365), although\nperformance was still far lower than on similar hand-labelled data. Models also\nperformed poorly on boreal forest data (AP50: 0.142), although again increasing\nwhen evaluated on canopy trees only (Canopy AP50: 0.308). Both models showed\nvery poor localisation accuracy at stricter IoU thresholds, even when\nrestricted to canopy trees (Max AP75: 0.051). Similar results have been\nobserved in studies using aerial LiDAR data, suggesting fundamental limitations\nin aerial-based segmentation approaches in closed canopy forests.",
      "tldr_zh": "本研究揭示了人工标注如何夸大深度学习模型在封闭树冠森林 RGB 图像上进行个体树冠 (ITC) 分割的性能，使用 Terrestrial Laser Scanning (TLS) 数据作为独立地面真实验证。研究评估了 DeepForest (RetinaNet) 和 Detectree2 (Mask R-CNN) 两个模型，在混合 boreal 和 Mediterranean 森林的无人机图像上进行测试，结果显示与人工标注相比，TLS 验证下的模型性能显著下降（例如，AP50: 0.094 vs. 0.670）。仅考虑树冠时，性能有所改善（Canopy AP50: 0.365），但在严格 IoU 阈值下（如 AP75: 0.051），定位准确性仍很差，表明空中分割方法在封闭树冠森林中存在根本限制。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.4; I.4.6; I.4.8; I.4.9; I.5; I.5.4"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.14273v2",
      "published_date": "2025-03-18 14:09:00 UTC",
      "updated_date": "2025-03-19 16:17:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:33:12.565554"
    },
    {
      "arxiv_id": "2503.22706v1",
      "title": "Validating Emergency Department Admission Predictions Based on Local Data Through MIMIC-IV",
      "title_zh": "翻译失败",
      "authors": [
        "Francesca Meimeti",
        "Loukas Triantafyllopoulos",
        "Aikaterini Sakagianni",
        "Vasileios Kaldis",
        "Lazaros Tzelves",
        "Nikolaos Theodorakis",
        "Evgenia Paxinou",
        "Georgios Feretzakis",
        "Dimitris Kalles",
        "Vassilios S. Verykios"
      ],
      "abstract": "The effective management of Emergency Department (ED) overcrowding is\nessential for improving patient outcomes and optimizing healthcare resource\nallocation. This study validates hospital admission prediction models initially\ndeveloped using a small local dataset from a Greek hospital by leveraging the\ncomprehensive MIMIC-IV dataset. After preprocessing the MIMIC-IV data, five\nalgorithms were evaluated: Linear Discriminant Analysis (LDA), K-Nearest\nNeighbors (KNN), Random Forest (RF), Recursive Partitioning and Regression\nTrees (RPART), and Support Vector Machines (SVM Radial). Among these, RF\ndemonstrated superior performance, achieving an Area Under the Receiver\nOperating Characteristic Curve (AUC-ROC) of 0.9999, sensitivity of 0.9997, and\nspecificity of 0.9999 when applied to the MIMIC-IV data. These findings\nhighlight the robustness of RF in handling complex datasets for admission\nprediction, establish MIMIC-IV as a valuable benchmark for validating models\nbased on smaller local datasets, and provide actionable insights for improving\nED management strategies.",
      "tldr_zh": "本研究使用 MIMIC-IV 数据集验证了基于希腊医院小数据集开发的紧急部门(ED)入院预测模型，以改善患者结果和资源分配。研究团队预处理了数据，并评估了五种算法：Linear Discriminant Analysis (LDA)、K-Nearest Neighbors (KNN)、Random Forest (RF)、Recursive Partitioning and Regression Trees (RPART)以及 Support Vector Machines (SVM Radial)。结果显示，RF 算法表现出色，在 MIMIC-IV 数据上达到了 AUC-ROC 0.9999、sensitivity 0.9997 和 specificity 0.9999，证明其在处理复杂数据集中的稳健性。该研究确立了 MIMIC-IV 作为验证小数据集模型的宝贵基准，并为优化 ED 管理策略提供了实际见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "36 pages, 3 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.22706v1",
      "published_date": "2025-03-18 13:54:28 UTC",
      "updated_date": "2025-03-18 13:54:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:33:24.489786"
    },
    {
      "arxiv_id": "2503.14258v3",
      "title": "JuDGE: Benchmarking Judgment Document Generation for Chinese Legal System",
      "title_zh": "JuDGE：针对中国法律系统的判决文件生成基准测试",
      "authors": [
        "Weihang Su",
        "Baoqing Yue",
        "Qingyao Ai",
        "Yiran Hu",
        "Jiaqi Li",
        "Changyue Wang",
        "Kaiyuan Zhang",
        "Yueyue Wu",
        "Yiqun Liu"
      ],
      "abstract": "This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE.",
      "tldr_zh": "本研究提出 JuDGE 基准，用于评估在中文法律系统中生成判决文件的性能，该任务涉及从案件事实描述生成完整的法律判决文档。研究构建了一个全面数据集，包括真实案件的事实描述及其对应判决文件作为基准，并通过添加法规条例和过去判决文件的外部语料库进行增强。作者与法律专业人士合作，开发了多维度的自动化评估框架，以评估生成文档的质量。实验结果显示，使用少样本学习、微调和多源 RAG 方法的基线模型中，RAG 方法能有效提升性能，但仍存在显著改进空间；相关代码和数据集可从 GitHub 获取。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14258v3",
      "published_date": "2025-03-18 13:48:18 UTC",
      "updated_date": "2025-04-30 16:23:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:33:35.007429"
    },
    {
      "arxiv_id": "2503.14254v1",
      "title": "CTSAC: Curriculum-Based Transformer Soft Actor-Critic for Goal-Oriented Robot Exploration",
      "title_zh": "CTSAC：基于课程学习的Transformer Soft Actor-Critic用于目标导向机器人探索",
      "authors": [
        "Chunyu Yang",
        "Shengben Bi",
        "Yihui Xu",
        "Xin Zhang"
      ],
      "abstract": "With the increasing demand for efficient and flexible robotic exploration\nsolutions, Reinforcement Learning (RL) is becoming a promising approach in the\nfield of autonomous robotic exploration. However, current RL-based exploration\nalgorithms often face limited environmental reasoning capabilities, slow\nconvergence rates, and substantial challenges in Sim-To-Real (S2R) transfer. To\naddress these issues, we propose a Curriculum Learning-based Transformer\nReinforcement Learning Algorithm (CTSAC) aimed at improving both exploration\nefficiency and transfer performance. To enhance the robot's reasoning ability,\na Transformer is integrated into the perception network of the Soft\nActor-Critic (SAC) framework, leveraging historical information to improve the\nfarsightedness of the strategy. A periodic review-based curriculum learning is\nproposed, which enhances training efficiency while mitigating catastrophic\nforgetting during curriculum transitions. Training is conducted on the\nROS-Gazebo continuous robotic simulation platform, with LiDAR clustering\noptimization to further reduce the S2R gap. Experimental results demonstrate\nthe CTSAC algorithm outperforms the state-of-the-art non-learning and\nlearning-based algorithms in terms of success rate and success rate-weighted\nexploration time. Moreover, real-world experiments validate the strong S2R\ntransfer capabilities of CTSAC.",
      "tldr_zh": "该研究针对强化学习（RL）在机器人目标导向探索中的问题，如环境推理能力有限、收敛速度慢和 Sim-To-Real (S2R) 转移挑战，提出了一种基于 Curriculum Learning 的 Transformer 强化学习算法 CTSAC。CTSAC 将 Transformer 整合到 Soft Actor-Critic (SAC) 框架的感知网络中，利用历史信息提升策略的远见，并采用基于周期性审查的 Curriculum Learning 来提高训练效率并缓解灾难性遗忘。实验在 ROS-Gazebo 模拟平台上进行，并通过 LiDAR 聚类优化减少 S2R 差距，结果显示 CTSAC 在成功率和加权探索时间上优于现有算法，并在真实世界测试中验证了其强大的 S2R 转移能力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "7pages,7 figures,Thesis received by 2025 ICRA",
      "pdf_url": "http://arxiv.org/pdf/2503.14254v1",
      "published_date": "2025-03-18 13:44:29 UTC",
      "updated_date": "2025-03-18 13:44:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:33:48.648175"
    },
    {
      "arxiv_id": "2503.14250v1",
      "title": "A Parallel Hybrid Action Space Reinforcement Learning Model for Real-world Adaptive Traffic Signal Control",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxuan Wang",
        "Meng Long",
        "Qiang Wu",
        "Wei Liu",
        "Jiatian Pi",
        "Xinmin Yang"
      ],
      "abstract": "Adaptive traffic signal control (ATSC) can effectively reduce vehicle travel\ntimes by dynamically adjusting signal timings but poses a critical challenge in\nreal-world scenarios due to the complexity of real-time decision-making in\ndynamic and uncertain traffic conditions. The burgeoning field of intelligent\ntransportation systems, bolstered by artificial intelligence techniques and\nextensive data availability, offers new prospects for the implementation of\nATSC. In this study, we introduce a parallel hybrid action space reinforcement\nlearning model (PH-DDPG) that optimizes traffic signal phase and duration of\ntraffic signals simultaneously, eliminating the need for sequential\ndecision-making seen in traditional two-stage models. Our model features a\ntask-specific parallel hybrid action space tailored for adaptive traffic\ncontrol, which directly outputs discrete phase selections and their associated\ncontinuous duration parameters concurrently, thereby inherently addressing\ndynamic traffic adaptation through unified parametric optimization. %Our model\nfeatures a unique parallel hybrid action space that allows for the simultaneous\noutput of each action and its optimal parameters, streamlining the\ndecision-making process. Furthermore, to ascertain the robustness and\neffectiveness of this approach, we executed ablation studies focusing on the\nutilization of a random action parameter mask within the critic network, which\ndecouples the parameter space for individual actions, facilitating the use of\npreferable parameters for each action. The results from these studies confirm\nthe efficacy of this method, distinctly enhancing real-world applicability",
      "tldr_zh": "这篇论文提出了一种名为 PH-DDPG 的并行混合动作空间强化学习模型，用于真实世界的自适应交通信号控制 (ATSC)，旨在通过同时优化信号阶段和持续时间来应对动态交通条件下的实时决策挑战。模型采用任务特定的并行混合动作空间，直接输出离散阶段选择及其连续持续时间参数，避免了传统两阶段模型的顺序决策，从而提升了决策效率。实验中的消融研究验证了随机动作参数掩码在评论网络中的作用，有效解耦参数空间并提高了模型的鲁棒性和实际适用性。",
      "categories": [
        "cs.AI",
        "I.2.6; I.2.8"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 9 figures, Reinforcement Learning",
      "pdf_url": "http://arxiv.org/pdf/2503.14250v1",
      "published_date": "2025-03-18 13:38:53 UTC",
      "updated_date": "2025-03-18 13:38:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:33:59.474063"
    },
    {
      "arxiv_id": "2503.14247v1",
      "title": "GeoFlow-SLAM: A Robust Tightly-Coupled RGBD-Inertial Fusion SLAM for Dynamic Legged Robotics",
      "title_zh": "GeoFlow-SLAM：一种用于动态腿式机器人的鲁棒紧密耦合 RGBD-惯性融合 SLAM",
      "authors": [
        "Tingyang Xiao",
        "Xiaolin Zhou",
        "Liu Liu",
        "Wei Sui",
        "Wei Feng",
        "Jiaxiong Qiu",
        "Xinjie Wang",
        "Zhizhong Su"
      ],
      "abstract": "This paper presents GeoFlow-SLAM, a robust and effective Tightly-Coupled\nRGBD-inertial SLAM for legged robots operating in highly dynamic\nenvironments.By integrating geometric consistency, legged odometry constraints,\nand dual-stream optical flow (GeoFlow), our method addresses three critical\nchallenges:feature matching and pose initialization failures during fast\nlocomotion and visual feature scarcity in texture-less scenes.Specifically, in\nrapid motion scenarios, feature matching is notably enhanced by leveraging\ndual-stream optical flow, which combines prior map points and poses.\nAdditionally, we propose a robust pose initialization method for fast\nlocomotion and IMU error in legged robots, integrating IMU/Legged odometry,\ninter-frame Perspective-n-Point (PnP), and Generalized Iterative Closest Point\n(GICP). Furthermore, a novel optimization framework that tightly couples\ndepth-to-map and GICP geometric constraints is first introduced to improve the\nrobustness and accuracy in long-duration, visually texture-less environments.\nThe proposed algorithms achieve state-of-the-art (SOTA) on collected legged\nrobots and open-source datasets. To further promote research and development,\nthe open-source datasets and code will be made publicly available at\nhttps://github.com/NSN-Hello/GeoFlow-SLAM",
      "tldr_zh": "这篇论文提出了 GeoFlow-SLAM，一种鲁棒的紧密耦合 RGBD-惯性融合 SLAM 系统，针对动态环境中的腿式机器人，集成了几何一致性、腿式里程计约束和双流光流 (GeoFlow) 来解决快速运动中的特征匹配失败、位姿初始化问题以及纹理稀少场景的视觉特征稀缺挑战。关键创新包括一种整合 IMU/腿式里程计、帧间 Perspective-n-Point (PnP) 和 Generalized Iterative Closest Point (GICP) 的鲁棒位姿初始化方法，以及一个新的优化框架，通过紧密耦合深度到地图和 GICP 几何约束提升长期运行的准确性。实验结果显示，该系统在收集的腿式机器人数据集和开源数据集上达到了最先进 (SOTA) 水平，并计划公开数据集和代码以促进研究。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.14247v1",
      "published_date": "2025-03-18 13:35:49 UTC",
      "updated_date": "2025-03-18 13:35:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:34:11.814393"
    },
    {
      "arxiv_id": "2503.14246v1",
      "title": "Trading-off Accuracy and Communication Cost in Federated Learning",
      "title_zh": "在联邦学习中权衡准确性和通信成本",
      "authors": [
        "Mattia Jacopo Villani",
        "Emanuele Natale",
        "Frederik Mallmann-Trenn"
      ],
      "abstract": "Leveraging the training-by-pruning paradigm introduced by Zhou et al. and\nIsik et al. introduced a federated learning protocol that achieves a 34-fold\nreduction in communication cost. We achieve a compression improvements of\norders of orders of magnitude over the state-of-the-art. The central idea of\nour framework is to encode the network weights $\\vec w$ by a the vector of\ntrainable parameters $\\vec p$, such that $\\vec w = Q\\cdot \\vec p$ where $Q$ is\na carefully-generate sparse random matrix (that remains fixed throughout\ntraining). In such framework, the previous work of Zhou et al. [NeurIPS'19] is\nretrieved when $Q$ is diagonal and $\\vec p$ has the same dimension of $\\vec w$.\nWe instead show that $\\vec p$ can effectively be chosen much smaller than $\\vec\nw$, while retaining the same accuracy at the price of a decrease of the\nsparsity of $Q$. Since server and clients only need to share $\\vec p$, such a\ntrade-off leads to a substantial improvement in communication cost. Moreover,\nwe provide theoretical insight into our framework and establish a novel link\nbetween training-by-sampling and random convex geometry.",
      "tldr_zh": "这篇论文提出了一种在联邦学习(Federated Learning)中权衡准确性和通信成本的框架，通过将网络权重 \\(\\vec{w}\\) 编码为更小的可训练参数 \\(\\vec{p}\\)，使得 \\(\\vec{w} = Q \\cdot \\vec{p}\\)，其中 \\(Q\\) 是一个固定稀疏随机矩阵，从而实现通信成本的数倍减少。相比现有方法，该框架在保持准确性的前提下，实现了34倍以上的压缩改善。论文还提供了理论分析，并建立了训练-by-sampling 和随机凸几何之间的联系，为高效的分布式学习提供了新见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14246v1",
      "published_date": "2025-03-18 13:35:24 UTC",
      "updated_date": "2025-03-18 13:35:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:34:24.382055"
    },
    {
      "arxiv_id": "2503.16532v1",
      "title": "Modelling Emotions in Face-to-Face Setting: The Interplay of Eye-Tracking, Personality, and Temporal Dynamics",
      "title_zh": "翻译失败",
      "authors": [
        "Meisam Jamshidi Seikavandi",
        "Jostein Fimland",
        "Maria Barrett",
        "Paolo Burelli"
      ],
      "abstract": "Accurate emotion recognition is pivotal for nuanced and engaging\nhuman-computer interactions, yet remains difficult to achieve, especially in\ndynamic, conversation-like settings. In this study, we showcase how integrating\neye-tracking data, temporal dynamics, and personality traits can substantially\nenhance the detection of both perceived and felt emotions. Seventy-three\nparticipants viewed short, speech-containing videos from the CREMA-D dataset,\nwhile being recorded for eye-tracking signals (pupil size, fixation patterns),\nBig Five personality assessments, and self-reported emotional states. Our\nneural network models combined these diverse inputs including stimulus emotion\nlabels for contextual cues and yielded marked performance gains compared to the\nstate-of-the-art. Specifically, perceived valence predictions reached a macro\nF1-score of 0.76, and models incorporating personality traits and stimulus\ninformation demonstrated significant improvements in felt emotion accuracy.\nThese results highlight the benefit of unifying physiological, individual and\ncontextual factors to address the subjectivity and complexity of emotional\nexpression. Beyond validating the role of user-specific data in capturing\nsubtle internal states, our findings inform the design of future affective\ncomputing and human-agent systems, paving the way for more adaptive and\ncross-individual emotional intelligence in real-world interactions.",
      "tldr_zh": "本研究探讨了在面对面设置中通过整合眼-tracking 数据（如瞳孔大小和注视模式）、个性特质（Big Five personality）和时间动态来提升情感识别的准确性。研究中，73 名参与者观看 CREMA-D 数据集的短视频，同时记录眼-tracking 信号、个性评估和自报情感状态，并使用神经网络模型结合这些输入及刺激情感标签进行分析。结果显示，模型在感知情感（perceived valence）上的 macro F1-score 达到 0.76，且加入个性特质和刺激信息后，felt emotion 的准确率显著提升。这些发现强调了统一生理、个体和语境因素的重要性，为情感计算和人类-代理系统的设计提供了指导，推动更具适应性的跨个体情感智能。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16532v1",
      "published_date": "2025-03-18 13:15:32 UTC",
      "updated_date": "2025-03-18 13:15:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:34:36.066545"
    },
    {
      "arxiv_id": "2503.14234v3",
      "title": "Beyond Single Pass, Looping Through Time: KG-IRAG with Iterative Knowledge Retrieval",
      "title_zh": "超越",
      "authors": [
        "Ruiyi Yang",
        "Hao Xue",
        "Imran Razzak",
        "Hakim Hacid",
        "Flora D. Salim"
      ],
      "abstract": "Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective\nin enhancing the performance of Large Language Models (LLMs) on tasks that\nrequire external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG\nimproves information retrieval for complex reasoning tasks, providing more\nprecise and comprehensive retrieval and generating more accurate responses to\nQAs. However, most RAG methods fall short in addressing multi-step reasoning,\nparticularly when both information extraction and inference are necessary. To\naddress this limitation, this paper presents Knowledge Graph-Based Iterative\nRetrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs\nwith iterative reasoning to improve LLMs' ability to handle queries involving\ntemporal and logical dependencies. Through iterative retrieval steps, KG-IRAG\nincrementally gathers relevant data from external KGs, enabling step-by-step\nreasoning. The proposed approach is particularly suited for scenarios where\nreasoning is required alongside dynamic temporal data extraction, such as\ndetermining optimal travel times based on weather conditions or traffic\npatterns. Experimental results show that KG-IRAG improves accuracy in complex\nreasoning tasks by effectively integrating external knowledge with iterative,\nlogic-based retrieval. Additionally, three new datasets: weatherQA-Irish,\nweatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's\nperformance, demonstrating its potential beyond traditional RAG applications.",
      "tldr_zh": "该论文提出 KG-IRAG（Knowledge Graph-Based Iterative Retrieval-Augmented Generation）框架，以解决现有 GraphRAG 方法在多步推理任务中的局限性，特别是处理涉及时间和逻辑依赖的查询。KG-IRAG 通过整合知识图谱 (KGs) 和迭代检索机制，逐步从外部 KGs 中获取相关数据，实现步-by-step 推理，适用于动态场景如基于天气或交通模式的旅行规划。实验结果显示，该框架显著提升了大语言模型 (LLMs) 在复杂推理任务中的准确性，并引入了三个新数据集（weatherQA-Irish、weatherQA-Sydney 和 trafficQA-TFNSW），扩展了 RAG 应用的潜力。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.14234v3",
      "published_date": "2025-03-18 13:11:43 UTC",
      "updated_date": "2025-05-19 17:56:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:34:47.250109"
    },
    {
      "arxiv_id": "2503.14232v2",
      "title": "CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yuyang Xue",
        "Edward Moroshko",
        "Feng Chen",
        "Jingyu Sun",
        "Steven McDonagh",
        "Sotirios A. Tsaftaris"
      ],
      "abstract": "Text-to-Image diffusion models can produce undesirable content that\nnecessitates concept erasure. However, existing methods struggle with\nunder-erasure, leaving residual traces of targeted concepts, or over-erasure,\nmistakenly eliminating unrelated but visually similar concepts. To address\nthese limitations, we introduce CRCE, a novel concept erasure framework that\nleverages Large Language Models to identify both semantically related concepts\nthat should be erased alongside the target and distinct concepts that should be\npreserved. By explicitly modelling coreferential and retained concepts\nsemantically, CRCE enables more precise concept removal, without unintended\nerasure. Experiments demonstrate that CRCE outperforms existing methods on\ndiverse erasure tasks, including real-world object, person identities, and\nabstract intellectual property characteristics. The constructed dataset\nCorefConcept and the source code will be release upon acceptance.",
      "tldr_zh": "该研究针对Text-to-Image diffusion models中概念擦除的问题，提出CRCE框架，以解决现有方法的under-erasure（擦除不彻底）和over-erasure（过度擦除）问题。CRCE利用Large Language Models识别语义相关的coreferential concepts（需擦除）和应保留的独立概念，通过显式建模实现更精确的概念移除，避免意外删除无关内容。实验结果显示，CRCE在多样化任务（如真实物体、人物身份和抽象知识产权特征）的擦除性能上优于现有方法，并构建了CorefConcept数据集，计划于接受后发布源代码。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14232v2",
      "published_date": "2025-03-18 13:09:01 UTC",
      "updated_date": "2025-05-20 12:46:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:34:59.316232"
    },
    {
      "arxiv_id": "2503.14229v1",
      "title": "HA-VLN: A Benchmark for Human-Aware Navigation in Discrete-Continuous Environments with Dynamic Multi-Human Interactions, Real-World Validation, and an Open Leaderboard",
      "title_zh": "翻译失败",
      "authors": [
        "Yifei Dong",
        "Fengyi Wu",
        "Qi He",
        "Heng Li",
        "Minghan Li",
        "Zebang Cheng",
        "Yuxuan Zhou",
        "Jingdong Sun",
        "Qi Dai",
        "Zhi-Qi Cheng",
        "Alexander G Hauptmann"
      ],
      "abstract": "Vision-and-Language Navigation (VLN) systems often focus on either discrete\n(panoramic) or continuous (free-motion) paradigms alone, overlooking the\ncomplexities of human-populated, dynamic environments. We introduce a unified\nHuman-Aware VLN (HA-VLN) benchmark that merges these paradigms under explicit\nsocial-awareness constraints. Our contributions include: 1. A standardized task\ndefinition that balances discrete-continuous navigation with personal-space\nrequirements; 2. An enhanced human motion dataset (HAPS 2.0) and upgraded\nsimulators capturing realistic multi-human interactions, outdoor contexts, and\nrefined motion-language alignment; 3. Extensive benchmarking on 16,844\nhuman-centric instructions, revealing how multi-human dynamics and partial\nobservability pose substantial challenges for leading VLN agents; 4. Real-world\nrobot tests validating sim-to-real transfer in crowded indoor spaces; and 5. A\npublic leaderboard supporting transparent comparisons across discrete and\ncontinuous tasks. Empirical results show improved navigation success and fewer\ncollisions when social context is integrated, underscoring the need for\nhuman-centric design. By releasing all datasets, simulators, agent code, and\nevaluation tools, we aim to advance safer, more capable, and socially\nresponsible VLN research.",
      "tldr_zh": "该研究引入了HA-VLN基准，用于评估视觉语言导航(VLN)系统在动态多人类互动的离散-连续环境中进行人类感知导航。HA-VLN标准化了任务定义，结合了增强的人类运动数据集(HAPS 2.0)和升级模拟器，以捕捉真实的多人类互动、户外场景和运动-语言对齐，并在16,844个以人为中心的指令上进行基准测试，揭示了多人类动态和部分可观察性的挑战。实验结果显示，整合社会上下文显著提高了导航成功率并减少了碰撞，并通过真实世界机器人测试验证了模拟到真实环境的转移。最终，该基准提供了一个公开排行榜，并发布所有数据集和工具，以推动更安全、更负责任的VLN研究。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "27 pages, website: https://ha-vln-project.vercel.app/",
      "pdf_url": "http://arxiv.org/pdf/2503.14229v1",
      "published_date": "2025-03-18 13:05:55 UTC",
      "updated_date": "2025-03-18 13:05:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:35:12.645372"
    },
    {
      "arxiv_id": "2503.14228v1",
      "title": "Panoramic Distortion-Aware Tokenization for Person Detection and Localization Using Transformers in Overhead Fisheye Images",
      "title_zh": "全景失真感知标记化用于头顶鱼眼图像中基于Transformer的人员检测和定位",
      "authors": [
        "Nobuhiko Wakai",
        "Satoshi Sato",
        "Yasunori Ishii",
        "Takayoshi Yamashita"
      ],
      "abstract": "Person detection methods are used widely in applications including visual\nsurveillance, pedestrian detection, and robotics. However, accurate detection\nof persons from overhead fisheye images remains an open challenge because of\nfactors including person rotation and small-sized persons. To address the\nperson rotation problem, we convert the fisheye images into panoramic images.\nFor smaller people, we focused on the geometry of the panoramas. Conventional\ndetection methods tend to focus on larger people because these larger people\nyield large significant areas for feature maps. In equirectangular panoramic\nimages, we find that a person's height decreases linearly near the top of the\nimages. Using this finding, we leverage the significance values and aggregate\ntokens that are sorted based on these values to balance the significant areas.\nIn this leveraging process, we introduce panoramic distortion-aware\ntokenization. This tokenization procedure divides a panoramic image using\nself-similarity figures that enable determination of optimal divisions without\ngaps, and we leverage the maximum significant values in each tile of token\ngroups to preserve the significant areas of smaller people. To achieve higher\ndetection accuracy, we propose a person detection and localization method that\ncombines panoramic-image remapping and the tokenization procedure. Extensive\nexperiments demonstrated that our method outperforms conventional methods when\napplied to large-scale datasets.",
      "tldr_zh": "本研究针对头顶鱼眼图像（overhead fisheye images）中人检测和定位的挑战，包括人旋转和小尺寸人问题，提出了一种基于Transformers的改进方法。作者将鱼眼图像转换为全景图像（panoramic images），并利用全景图像的几何特性，通过显著值（significance values）聚合排序后的tokens来平衡特征地图的显著区域。引入Panoramic Distortion-Aware Tokenization技术，该过程使用自相似图形（self-similarity figures）划分图像，确保无间隙并保留小尺寸人的显著区域。实验结果表明，该方法在大型数据集上优于传统方法，提升了人检测的准确性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14228v1",
      "published_date": "2025-03-18 13:05:41 UTC",
      "updated_date": "2025-03-18 13:05:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:35:25.509059"
    },
    {
      "arxiv_id": "2503.14203v1",
      "title": "Stochastic Trajectory Prediction under Unstructured Constraints",
      "title_zh": "非结构化约束下的随机轨迹预测",
      "authors": [
        "Hao Ma",
        "Zhiqiang Pu",
        "Shijie Wang",
        "Boyin Liu",
        "Huimu Wang",
        "Yanyan Liang",
        "Jianqiang Yi"
      ],
      "abstract": "Trajectory prediction facilitates effective planning and decision-making,\nwhile constrained trajectory prediction integrates regulation into prediction.\nRecent advances in constrained trajectory prediction focus on structured\nconstraints by constructing optimization objectives. However, handling\nunstructured constraints is challenging due to the lack of differentiable\nformal definitions. To address this, we propose a novel method for constrained\ntrajectory prediction using a conditional generative paradigm, named\nControllable Trajectory Diffusion (CTD). The key idea is that any trajectory\ncorresponds to a degree of conformity to a constraint. By quantifying this\ndegree and treating it as a condition, a model can implicitly learn to predict\ntrajectories under unstructured constraints. CTD employs a pre-trained scoring\nmodel to predict the degree of conformity (i.e., a score), and uses this score\nas a condition for a conditional diffusion model to generate trajectories.\nExperimental results demonstrate that CTD achieves high accuracy on the ETH/UCY\nand SDD benchmarks. Qualitative analysis confirms that CTD ensures adherence to\nunstructured constraints and can predict trajectories that satisfy\ncombinatorial constraints.",
      "tldr_zh": "该研究解决了轨迹预测中处理非结构化约束的挑战，现有方法主要针对结构化约束而缺乏可微分形式定义。作者提出了一种新方法Controllable Trajectory Diffusion (CTD)，通过条件生成范式量化轨迹对约束的符合程度，并将其作为条件输入扩散模型中生成受约束的轨迹。具体而言，CTD使用预训练的评分模型预测符合度分数，从而实现隐式学习和预测。实验结果显示，CTD在ETH/UCY和SDD基准上表现出高准确性，并能确保轨迹遵守非结构化约束甚至组合约束。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "has been accepted by ICRA 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.14203v1",
      "published_date": "2025-03-18 12:27:59 UTC",
      "updated_date": "2025-03-18 12:27:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:35:35.643093"
    },
    {
      "arxiv_id": "2503.18955v1",
      "title": "Is there a future for AI without representation?",
      "title_zh": "翻译失败",
      "authors": [
        "Vincent C. Müller"
      ],
      "abstract": "This paper investigates the prospects of AI without representation in\ngeneral, and the proposals of Rodney Brooks in particular. What turns out to be\ncharacteristic of Brooks' proposal is the rejection of central control in\nintelligent agents; his systems has as much or as little representation as\ntraditional AI. The traditional view that representation is necessary for\nintelligence presupposes that intelligence requires central control. However,\nmuch of recent cognitive science suggests that we should dispose of the image\nof intelligent agents as central representation processors. If this paradigm\nshift is achieved, Brooks' proposal for non-centralized cognition without\nrepresentation appears promising for full-blown intelligent agents - though not\nfor conscious agents and thus not for human-like AI.",
      "tldr_zh": "这篇论文探讨了AI是否可能不依赖表示（representation）的未来，特别是对Rodney Brooks的提议进行分析。Brooks主张拒绝智能代理中的中央控制（central control），尽管他的系统在表示方面与传统AI类似。传统观点认为表示是智能的必要条件，但最近的认知科学建议放弃将智能代理视为中央表示处理器的形象。如果实现这一范式转变，Brooks的非集中式认知无表示方法对完全智能代理具有前景，但不适用于有意识的代理，因此无法实现类似人类的AI。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18955v1",
      "published_date": "2025-03-18 12:13:31 UTC",
      "updated_date": "2025-03-18 12:13:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:35:48.370509"
    },
    {
      "arxiv_id": "2503.14194v1",
      "title": "Driving behavior recognition via self-discovery learning",
      "title_zh": "基于自发现学习的驾驶行为识别",
      "authors": [
        "Yilin Wang"
      ],
      "abstract": "Autonomous driving systems require a deep understanding of human driving\nbehaviors to achieve higher intelligence and safety.Despite advancements in\ndeep learning, challenges such as long-tail distribution due to scarce samples\nand confusion from similar behaviors hinder effective driving behavior\ndetection.Existing methods often fail to address sample confusion adequately,\nas datasets frequently contain ambiguous samples that obscure unique semantic\ninformation.",
      "tldr_zh": "自动驾驶系统需要深入理解人类驾驶行为，以提升智能和安全，但面临长尾分布（因样本稀少）和类似行为混淆的挑战，导致现有方法难以处理数据集中的模糊样本和独特语义信息。论文提出了一种基于 self-discovery learning 的方法，通过自我发现机制来识别驾驶行为，从而缓解样本混淆问题。实验结果表明，该方法显著提高了驾驶行为检测的准确性和鲁棒性，为更可靠的自动驾驶系统提供了新途径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.14194v1",
      "published_date": "2025-03-18 12:13:08 UTC",
      "updated_date": "2025-03-18 12:13:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:36:00.192063"
    },
    {
      "arxiv_id": "2503.14192v1",
      "title": "Strategic White Paper on AI Infrastructure for Particle, Nuclear, and Astroparticle Physics: Insights from JENA and EuCAIF",
      "title_zh": "翻译失败",
      "authors": [
        "Sascha Caron",
        "Andreas Ipp",
        "Gert Aarts",
        "Gábor Bíró",
        "Daniele Bonacorsi",
        "Elena Cuoco",
        "Caterina Doglioni",
        "Tommaso Dorigo",
        "Julián García Pardiñas",
        "Stefano Giagu",
        "Tobias Golling",
        "Lukas Heinrich",
        "Ik Siong Heng",
        "Paula Gina Isar",
        "Karolos Potamianos",
        "Liliana Teodorescu",
        "John Veitch",
        "Pietro Vischia",
        "Christoph Weniger"
      ],
      "abstract": "Artificial intelligence (AI) is transforming scientific research, with deep\nlearning methods playing a central role in data analysis, simulations, and\nsignal detection across particle, nuclear, and astroparticle physics. Within\nthe JENA communities-ECFA, NuPECC, and APPEC-and as part of the EuCAIF\ninitiative, AI integration is advancing steadily. However, broader adoption\nremains constrained by challenges such as limited computational resources, a\nlack of expertise, and difficulties in transitioning from research and\ndevelopment (R&D) to production. This white paper provides a strategic roadmap,\ninformed by a community survey, to address these barriers. It outlines critical\ninfrastructure requirements, prioritizes training initiatives, and proposes\nfunding strategies to scale AI capabilities across fundamental physics over the\nnext five years.",
      "tldr_zh": "这份白皮书聚焦于 AI 基础设施在粒子、核和天体粒子物理领域的战略规划，基于 JENA 社区（包括 ECFA、NuPECC 和 APPEC）和 EuCAIF 倡议的见解。报告指出，AI 在数据分析、模拟和信号检测中的应用正稳步推进，但面临计算资源有限、专业知识缺口以及从 R&D 到生产的过渡挑战。针对这些问题，它通过社区调查提出路线图，包括关键基础设施需求、培训举措和资助策略。最终目标是未来五年内提升这些领域的 AI 能力，促进更广泛的采用。",
      "categories": [
        "astro-ph.IM",
        "astro-ph.HE",
        "cs.AI",
        "cs.LG",
        "hep-ex",
        "hep-ph",
        "nucl-th"
      ],
      "primary_category": "astro-ph.IM",
      "comment": "19 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.14192v1",
      "published_date": "2025-03-18 12:11:11 UTC",
      "updated_date": "2025-03-18 12:11:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:36:12.869118"
    },
    {
      "arxiv_id": "2503.14190v1",
      "title": "Inferring Event Descriptions from Time Series with Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mingtian Tan",
        "Mike A. Merrill",
        "Zack Gottesman",
        "Tim Althoff",
        "David Evans",
        "Tom Hartvigsen"
      ],
      "abstract": "Time series data measure how environments change over time and drive\ndecision-making in critical domains like finance and healthcare. When analyzing\ntime series, we often seek to understand the underlying events occurring in the\nmeasured environment. For example, one might ask: What caused a sharp drop in\nthe stock price? Events are often described with natural language, so we\nconduct the first study of whether Large Language Models (LLMs) can infer\nnatural language events from time series. We curate a new benchmark featuring\nwin probabilities collected from 4,200 basketball and American football games,\nfeaturing 1.7M timesteps with real value data and corresponding natural\nlanguage events. Building on the recent wave of using LLMs on time series, we\nevaluate 16 LLMs and find that they demonstrate promising abilities to infer\nevents from time series data. The open-weights DeepSeek-R1 32B model\noutperforms proprietary models like GPT-4o. Despite this impressive initial\nperformance, we also find clear avenues to improve recent models, as we\nidentify failures when altering the provided context, event sequence lengths,\nand evaluation strategy. (All resources needed to reproduce our work are\navailable: https://github.com/BennyTMT/GAMETime)",
      "tldr_zh": "这篇论文探讨了使用Large Language Models (LLMs)从时间序列数据中推断自然语言事件，旨在帮助理解环境变化在金融和医疗等领域中的潜在事件。研究者构建了一个新基准数据集，包含4200场篮球和美式足球比赛的170万时间步数据及其对应事件描述，并评估了16个LLMs。结果显示，DeepSeek-R1 32B模型在性能上超过了GPT-4o，展示了LLMs的潜力，但也暴露了在改变上下文、事件序列长度和评估策略时的局限性。该工作为时间序列分析提供了可重现资源，促进未来改进。",
      "categories": [
        "cs.AI",
        "62M10, 68T07,",
        "I.2.6; I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 9 Figures",
      "pdf_url": "http://arxiv.org/pdf/2503.14190v1",
      "published_date": "2025-03-18 12:07:33 UTC",
      "updated_date": "2025-03-18 12:07:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:36:24.056144"
    },
    {
      "arxiv_id": "2503.14184v1",
      "title": "Variable Time-Step MPC for Agile Multi-Rotor UAV Interception of Dynamic Targets",
      "title_zh": "翻译失败",
      "authors": [
        "Atharva Ghotavadekar",
        "František Nekovář",
        "Martin Saska",
        "Jan Faigl"
      ],
      "abstract": "Agile trajectory planning can improve the efficiency of multi-rotor Uncrewed\nAerial Vehicles (UAVs) in scenarios with combined task-oriented and kinematic\ntrajectory planning, such as monitoring spatio-temporal phenomena or\nintercepting dynamic targets. Agile planning using existing non-linear model\npredictive control methods is limited by the number of planning steps as it\nbecomes increasingly computationally demanding. That reduces the prediction\nhorizon length, leading to a decrease in solution quality. Besides, the fixed\ntime-step length limits the utilization of the available UAV dynamics in the\ntarget neighborhood. In this paper, we propose to address these limitations by\nintroducing variable time steps and coupling them with the prediction horizon\nlength. A simplified point-mass motion primitive is used to leverage the\ndifferential flatness of quadrotor dynamics and the generation of feasible\ntrajectories in the flat output space. Based on the presented evaluation\nresults and experimentally validated deployment, the proposed method increases\nthe solution quality by enabling planning for long flight segments but allowing\ntightly sampled maneuvering.",
      "tldr_zh": "该研究针对多旋翼无人机（UAV）在动态目标拦截等任务中的轨迹规划问题，提出了一种可变时间步长模型预测控制（MPC）方法，以克服现有方法的计算限制和固定时间步长问题。方法通过将可变时间步长与预测视野长度耦合，并利用简化点质量运动基元以及 quadrotor dynamics 的差分平坦性，在平坦输出空间生成可行轨迹。实验评估和实际部署验证显示，该方法显著提高了解决方案质量，实现了长飞行段规划的同时支持紧密采样机动。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14184v1",
      "published_date": "2025-03-18 11:59:24 UTC",
      "updated_date": "2025-03-18 11:59:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:36:35.226013"
    },
    {
      "arxiv_id": "2503.14183v1",
      "title": "Can LLMs Enable Verification in Mainstream Programming?",
      "title_zh": "翻译失败",
      "authors": [
        "Aleksandr Shefer",
        "Igor Engel",
        "Stanislav Alekseev",
        "Daniil Berezun",
        "Ekaterina Verbitskaia",
        "Anton Podkopaev"
      ],
      "abstract": "Although formal methods are capable of producing reliable software, they have\nseen minimal adoption in everyday programming. Automatic code generation using\nlarge language models is becoming increasingly widespread, but it rarely\nconsiders producing strong correctness guarantees. In this study, we explore\nthe ability of LLMs to produce verified code in three verification languages\n(Dafny, Nagini, and Verus). To do so, we use manually curated datasets derived\nfrom the state-ofthe-art Python benchmark, HumanEval. We also assess what types\nof information are sufficient to achieve good-quality results.",
      "tldr_zh": "本研究探讨大型语言模型（LLMs）是否能促进主流编程中的代码验证问题，因为正式方法虽能确保软件可靠性，但尚未广泛采用，而LLMs生成的代码往往缺少正确性保证。研究团队使用手动策划的HumanEval Python基准数据集，测试LLMs在生成验证代码方面的能力，涵盖三种验证语言：Dafny、Nagini和Verus。结果显示，通过评估不同类型的信息输入，LLMs能够产生高质量的验证代码，这为将验证集成到日常编程中提供了潜在可能性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14183v1",
      "published_date": "2025-03-18 11:58:00 UTC",
      "updated_date": "2025-03-18 11:58:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:36:46.685022"
    },
    {
      "arxiv_id": "2503.14162v2",
      "title": "EIAD: Explainable Industrial Anomaly Detection Via Multi-Modal Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zongyun Zhang",
        "Jiacheng Ruan",
        "Xian Gao",
        "Ting Liu",
        "Yuzhuo Fu"
      ],
      "abstract": "Industrial Anomaly Detection (IAD) is critical to ensure product quality\nduring manufacturing. Although existing zero-shot defect segmentation and\ndetection methods have shown effectiveness, they cannot provide detailed\ndescriptions of the defects. Furthermore, the application of large multi-modal\nmodels in IAD remains in its infancy, facing challenges in balancing\nquestion-answering (QA) performance and mask-based grounding capabilities,\noften owing to overfitting during the fine-tuning process. To address these\nchallenges, we propose a novel approach that introduces a dedicated multi-modal\ndefect localization module to decouple the dialog functionality from the core\nfeature extraction. This decoupling is achieved through independent\noptimization objectives and tailored learning strategies. Additionally, we\ncontribute to the first multi-modal industrial anomaly detection training\ndataset, named Defect Detection Question Answering (DDQA), encompassing a wide\nrange of defect types and industrial scenarios. Unlike conventional datasets\nthat rely on GPT-generated data, DDQA ensures authenticity and reliability and\noffers a robust foundation for model training. Experimental results demonstrate\nthat our proposed method, Explainable Industrial Anomaly Detection Assistant\n(EIAD), achieves outstanding performance in defect detection and localization\ntasks. It not only significantly enhances accuracy but also improves\ninterpretability. These advancements highlight the potential of EIAD for\npractical applications in industrial settings.",
      "tldr_zh": "本研究针对工业异常检测（IAD）的局限性，提出了一种可解释性方法EIAD，利用多模态大语言模型来提供详细的缺陷描述，同时解决问答（QA）性能与基于掩码的定位能力之间的平衡问题。EIAD引入一个专门的多模态缺陷定位模块，将对话功能与核心特征提取解耦，通过独立的优化目标和定制学习策略进行优化；此外，研究还贡献了首个多模态IAD训练数据集Defect Detection Question Answering (DDQA)，涵盖多种缺陷类型和工业场景，确保数据真实性。实验结果显示，EIAD在缺陷检测和定位任务中显著提升了准确性和可解释性，展示了其在实际工业应用中的潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ICME2025",
      "pdf_url": "http://arxiv.org/pdf/2503.14162v2",
      "published_date": "2025-03-18 11:33:29 UTC",
      "updated_date": "2025-05-16 15:25:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:37:00.683128"
    },
    {
      "arxiv_id": "2503.14569v1",
      "title": "Potential Score Matching: Debiasing Molecular Structure Sampling with Potential Energy Guidance",
      "title_zh": "翻译失败",
      "authors": [
        "Liya Guo",
        "Zun Wang",
        "Chang Liu",
        "Junzhe Li",
        "Pipi Hu",
        "Yi Zhu"
      ],
      "abstract": "The ensemble average of physical properties of molecules is closely related\nto the distribution of molecular conformations, and sampling such distributions\nis a fundamental challenge in physics and chemistry. Traditional methods like\nmolecular dynamics (MD) simulations and Markov chain Monte Carlo (MCMC)\nsampling are commonly used but can be time-consuming and costly. Recently,\ndiffusion models have emerged as efficient alternatives by learning the\ndistribution of training data. Obtaining an unbiased target distribution is\nstill an expensive task, primarily because it requires satisfying ergodicity.\nTo tackle these challenges, we propose Potential Score Matching (PSM), an\napproach that utilizes the potential energy gradient to guide generative\nmodels. PSM does not require exact energy functions and can debias sample\ndistributions even when trained on limited and biased data. Our method\noutperforms existing state-of-the-art (SOTA) models on the Lennard-Jones (LJ)\npotential, a commonly used toy model. Furthermore, we extend the evaluation of\nPSM to high-dimensional problems using the MD17 and MD22 datasets. The results\ndemonstrate that molecular distributions generated by PSM more closely\napproximate the Boltzmann distribution compared to traditional diffusion\nmodels.",
      "tldr_zh": "该研究针对分子构象分布采样面临的挑战提出了一种新方法Potential Score Matching (PSM)，它利用势能梯度指导生成模型，以纠正训练数据有限或有偏时的样本分布偏差。不同于传统分子动力学 (MD) 模拟和Markov chain Monte Carlo (MCMC) 等耗时方法，PSM 不需精确能量函数，就能高效生成更接近Boltzmann分布的分子结构。实验结果显示，PSM 在Lennard-Jones (LJ) 潜力模型上优于现有最先进 (SOTA) 模型，并在高维MD17和MD22数据集上表现出色，显著提升了采样准确性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14569v1",
      "published_date": "2025-03-18 11:27:28 UTC",
      "updated_date": "2025-03-18 11:27:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:37:11.763418"
    },
    {
      "arxiv_id": "2503.14568v1",
      "title": "Teaching Artificial Intelligence to Perform Rapid, Resolution-Invariant Grain Growth Modeling via Fourier Neural Operator",
      "title_zh": "翻译失败",
      "authors": [
        "Iman Peivaste",
        "Ahmed Makradi",
        "Salim Belouettar"
      ],
      "abstract": "Microstructural evolution, particularly grain growth, plays a critical role\nin shaping the physical, optical, and electronic properties of materials.\nTraditional phase-field modeling accurately simulates these phenomena but is\ncomputationally intensive, especially for large systems and fine spatial\nresolutions. While machine learning approaches have been employed to accelerate\nsimulations, they often struggle with resolution dependence and generalization\nacross different grain scales. This study introduces a novel approach utilizing\nFourier Neural Operator (FNO) to achieve resolution-invariant modeling of\nmicrostructure evolution in multi-grain systems. FNO operates in the Fourier\nspace and can inherently handle varying resolutions by learning mappings\nbetween function spaces. By integrating FNO with the phase field method, we\ndeveloped a surrogate model that significantly reduces computational costs\nwhile maintaining high accuracy across different spatial scales. We generated a\ncomprehensive dataset from phase-field simulations using the Fan Chen model,\ncapturing grain evolution over time. Data preparation involved creating\ninput-output pairs with a time shift, allowing the model to predict future\nmicrostructures based on current and past states. The FNO-based neural network\nwas trained using sequences of microstructures and demonstrated remarkable\naccuracy in predicting long-term evolution, even for unseen configurations and\nhigher-resolution grids not encountered during training.",
      "tldr_zh": "本研究针对微观结构演化（如晶粒生长）建模的计算密集问题，提出了一种利用 Fourier Neural Operator (FNO) 的快速、分辨率不变的方法，以加速多晶粒系统的模拟。FNO 通过在傅立叶空间学习函数映射，结合相场方法（phase-field method），开发了一个代理模型，能够在不同空间尺度下维持高准确性，同时显著降低计算成本。研究基于 Fan Chen 模型生成的时间序列数据集，对模型进行训练，结果显示该模型能准确预测长期微观结构演化，甚至适用于训练中未见的配置和高分辨率网格。总的来说，此方法为高效的材料性能模拟提供了新途径。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14568v1",
      "published_date": "2025-03-18 11:19:08 UTC",
      "updated_date": "2025-03-18 11:19:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:37:23.145898"
    },
    {
      "arxiv_id": "2503.14151v2",
      "title": "Concat-ID: Towards Universal Identity-Preserving Video Synthesis",
      "title_zh": "Concat-ID：迈向通用身份保持视频合成",
      "authors": [
        "Yong Zhong",
        "Zhuoyi Yang",
        "Jiayan Teng",
        "Xiaotao Gu",
        "Chongxuan Li"
      ],
      "abstract": "We present Concat-ID, a unified framework for identity-preserving video\ngeneration. Concat-ID employs Variational Autoencoders to extract image\nfeatures, which are concatenated with video latents along the sequence\ndimension, leveraging solely 3D self-attention mechanisms without the need for\nadditional modules. A novel cross-video pairing strategy and a multi-stage\ntraining regimen are introduced to balance identity consistency and facial\neditability while enhancing video naturalness. Extensive experiments\ndemonstrate Concat-ID's superiority over existing methods in both single and\nmulti-identity generation, as well as its seamless scalability to multi-subject\nscenarios, including virtual try-on and background-controllable generation.\nConcat-ID establishes a new benchmark for identity-preserving video synthesis,\nproviding a versatile and scalable solution for a wide range of applications.",
      "tldr_zh": "该论文提出 Concat-ID 框架，实现统一的身份保持视频合成，通过 Variational Autoencoders 提取图像特征，并将其与视频潜在表示沿序列维度连接，仅依赖 3D self-attention 机制。框架引入 cross-video pairing 策略和 multi-stage 训练方法，以平衡身份一致性、面部可编辑性和视频自然性。实验结果显示，Concat-ID 在单身份和多身份生成任务中优于现有方法，并能无缝扩展到多主体场景，如虚拟试穿和背景可控生成，从而为身份保持视频合成设立新基准。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14151v2",
      "published_date": "2025-03-18 11:17:32 UTC",
      "updated_date": "2025-04-19 09:26:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:37:36.521063"
    },
    {
      "arxiv_id": "2503.14138v1",
      "title": "Exploring Disparity-Accuracy Trade-offs in Face Recognition Systems: The Role of Datasets, Architectures, and Loss Functions",
      "title_zh": "翻译失败",
      "authors": [
        "Siddharth D Jaiswal",
        "Sagnik Basu",
        "Sandipan Sikdar",
        "Animesh Mukherjee"
      ],
      "abstract": "Automated Face Recognition Systems (FRSs), developed using deep learning\nmodels, are deployed worldwide for identity verification and facial attribute\nanalysis. The performance of these models is determined by a complex\ninterdependence among the model architecture, optimization/loss function and\ndatasets. Although FRSs have surpassed human-level accuracy, they continue to\nbe disparate against certain demographics. Due to the ubiquity of applications,\nit is extremely important to understand the impact of the three components --\nmodel architecture, loss function and face image dataset on the\naccuracy-disparity trade-off to design better, unbiased platforms. In this\nwork, we perform an in-depth analysis of three FRSs for the task of gender\nprediction, with various architectural modifications resulting in ten\ndeep-learning models coupled with four loss functions and benchmark them on\nseven face datasets across 266 evaluation configurations. Our results show that\nall three components have an individual as well as a combined impact on both\naccuracy and disparity. We identify that datasets have an inherent property\nthat causes them to perform similarly across models, independent of the choice\nof loss functions. Moreover, the choice of dataset determines the model's\nperceived bias -- the same model reports bias in opposite directions for three\ngender-balanced datasets of ``in-the-wild'' face images of popular individuals.\nStudying the facial embeddings shows that the models are unable to generalize a\nuniform definition of what constitutes a ``female face'' as opposed to a ``male\nface'', due to dataset diversity. We provide recommendations to model\ndevelopers on using our study as a blueprint for model development and\nsubsequent deployment.",
      "tldr_zh": "本研究探讨了人脸识别系统（FRSs）中准确性和偏差权衡的问题，分析了模型架构、损失函数和数据集的相互影响，以设计更公平的系统。研究者通过对三种FRSs进行深入评估，使用十个深度学习模型、四个损失函数和七个面部数据集，共266个配置测试性别预测任务。结果显示，这些组件单独和组合会显著影响准确性和偏差，且数据集的内在属性导致模型表现相似，并决定模型的感知偏差，如同一个模型在不同数据集上可能报告相反的偏差。最终，研究发现模型难以泛化定义“女性面部”或“男性面部” due to dataset diversity，并为模型开发者提供了改进和部署建议。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CV",
      "comment": "This work has been accepted for publication at AAAI ICWSM 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.14138v1",
      "published_date": "2025-03-18 11:04:57 UTC",
      "updated_date": "2025-03-18 11:04:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:37:48.361009"
    },
    {
      "arxiv_id": "2503.14136v1",
      "title": "CARE: A QLoRA-Fine Tuned Multi-Domain Chatbot With Fast Learning On Minimal Hardware",
      "title_zh": "翻译失败",
      "authors": [
        "Ankit Dutta",
        "Nabarup Ghosh",
        "Ankush Chatterjee"
      ],
      "abstract": "Large Language models have demonstrated excellent domain-specific\nquestion-answering capabilities when finetuned with a particular dataset of\nthat specific domain. However, fine-tuning the models requires a significant\namount of training time and a considerable amount of hardware. In this work, we\npropose CARE (Customer Assistance and Response Engine), a lightweight model\nmade by fine-tuning Phi3.5-mini on very minimal hardware and data, designed to\nhandle queries primarily across three domains: telecommunications support,\nmedical support, and banking support. For telecommunications and banking, the\nchatbot addresses issues and problems faced by customers regularly in the\nabove-mentioned domains. In the medical domain, CARE provides preliminary\nsupport by offering basic diagnoses and medical suggestions that a user might\ntake before consulting a healthcare professional. Since CARE is built on\nPhi3.5-mini, it can be used even on mobile devices, increasing its usability.\nOur research also shows that CARE performs relatively well on various medical\nbenchmarks, indicating that it can be used to make basic medical suggestions.",
      "tldr_zh": "本研究提出 CARE，这是一个基于 QLoRA 微调的轻量级多域聊天机器人，利用 Phi3.5-mini 模型在最小硬件和数据上实现快速学习，旨在处理电信支持、医疗支持和银行支持领域的用户查询。CARE 通过微调技术提供初步诊断建议和问题解答，例如在医疗领域给出基本医疗建议，同时适用于移动设备，提升了实际可用性。实验结果表明，CARE 在各种医疗基准上表现出色，证明了其在减少训练资源需求的同时保持良好性能的优势。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14136v1",
      "published_date": "2025-03-18 10:58:10 UTC",
      "updated_date": "2025-03-18 10:58:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:37:59.356208"
    },
    {
      "arxiv_id": "2503.14130v1",
      "title": "Inference-Time Intervention in Large Language Models for Reliable Requirement Verification",
      "title_zh": "翻译失败",
      "authors": [
        "Paul Darm",
        "James Xie",
        "Annalisa Riccardi"
      ],
      "abstract": "Steering the behavior of Large Language Models (LLMs) remains a challenge,\nparticularly in engineering applications where precision and reliability are\ncritical. While fine-tuning and prompting methods can modify model behavior,\nthey lack the dynamic and exact control necessary for engineering applications.\nInference-time intervention techniques provide a promising alternative,\nallowing targeted adjustments to LLM outputs. In this work, we demonstrate how\ninterventions enable fine-grained control for automating the usually\ntime-intensive requirement verification process in Model-Based Systems\nEngineering (MBSE). Using two early-stage Capella SysML models of space\nmissions with associated requirements, we apply the intervened LLMs to reason\nover a graph representation of the model to determine whether a requirement is\nfulfilled. Our method achieves robust and reliable outputs, significantly\nimproving over both a baseline model and a fine-tuning approach. By identifying\nand modifying as few as one to three specialised attention heads, we can\nsignificantly change the model's behavior. When combined with self-consistency,\nthis allows us to achieve perfect precision on our holdout test set.",
      "tldr_zh": "该研究提出了一种推理时干预（Inference-time intervention）技术，用于大型语言模型（LLMs），以实现工程应用中需求验证的精确性和可靠性。该方法通过针对性调整LLMs的特定注意力头，动态控制模型输出，并应用于Model-Based Systems Engineering (MBSE)中自动化需求验证过程，使用Capella SysML模型对空间任务进行推理验证。与基线模型和微调方法相比，该方法显著提高了鲁棒性和可靠性，仅需修改一到三个注意力头即可实现完美的精确度，并在结合自洽性（self-consistency）后，在测试集上达到100%的精确率。",
      "categories": [
        "cs.AI",
        "cs.SE",
        "H.4.2; I.2.1; I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14130v1",
      "published_date": "2025-03-18 10:49:36 UTC",
      "updated_date": "2025-03-18 10:49:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:38:11.475820"
    },
    {
      "arxiv_id": "2503.14567v1",
      "title": "SpecReX: Explainable AI for Raman Spectroscopy",
      "title_zh": "翻译失败",
      "authors": [
        "Nathan Blake",
        "David A. Kelly",
        "Akchunya Chanchal",
        "Sarah Kapllani-Mucaj",
        "Geraint Thomas",
        "Hana Chockler"
      ],
      "abstract": "Raman spectroscopy is becoming more common for medical diagnostics with deep\nlearning models being increasingly used to leverage its full potential.\nHowever, the opaque nature of such models and the sensitivity of medical\ndiagnosis together with regulatory requirements necessitate the need for\nexplainable AI tools. We introduce SpecReX, specifically adapted to explaining\nRaman spectra. SpecReX uses the theory of actual causality to rank causal\nresponsibility in a spectrum, quantified by iteratively refining mutated\nversions of the spectrum and testing if it retains the original classification.\nThe explanations provided by SpecReX take the form of a responsibility map,\nhighlighting spectral regions most responsible for the model to make a correct\nclassification. To assess the validity of SpecReX, we create increasingly\ncomplex simulated spectra, in which a \"ground truth\" signal is seeded, to train\na classifier. We then obtain SpecReX explanations and compare the results with\nanother explainability tool. By using simulated spectra we establish that\nSpecReX localizes to the known differences between classes, under a number of\nconditions. This provides a foundation on which we can find the spectral\nfeatures which differentiate disease classes. This is an important first step\nin proving the validity of SpecReX.",
      "tldr_zh": "该研究引入 SpecReX，一种针对 Raman Spectroscopy 的可解释 AI 工具，用于解决深度学习模型在医疗诊断中的不透明问题。SpecReX 基于实际因果理论，通过迭代精炼光谱变异版本并测试分类稳定性，来生成责任地图，突出光谱区域对分类决策的因果责任。实验使用模拟光谱训练分类器，结果显示 SpecReX 能准确定位类间差异，比其他解释工具更有效，为识别疾病类别的光谱特征提供可靠基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.med-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "AAAI Workshop on Health Intelligencee (W3PHIAI-25)",
      "pdf_url": "http://arxiv.org/pdf/2503.14567v1",
      "published_date": "2025-03-18 10:49:15 UTC",
      "updated_date": "2025-03-18 10:49:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:38:23.189691"
    },
    {
      "arxiv_id": "2503.14125v1",
      "title": "Frac-Connections: Fractional Extension of Hyper-Connections",
      "title_zh": "翻译失败",
      "authors": [
        "Defa Zhu",
        "Hongzhi Huang",
        "Jundong Zhou",
        "Zihao Huang",
        "Yutao Zeng",
        "Banggu Wu",
        "Qiyang Min",
        "Xun Zhou"
      ],
      "abstract": "Residual connections are central to modern deep learning architectures,\nenabling the training of very deep networks by mitigating gradient vanishing.\nHyper-Connections recently generalized residual connections by introducing\nmultiple connection strengths at different depths, thereby addressing the\nseesaw effect between gradient vanishing and representation collapse. However,\nHyper-Connections increase memory access costs by expanding the width of hidden\nstates. In this paper, we propose Frac-Connections, a novel approach that\ndivides hidden states into multiple parts rather than expanding their width.\nFrac-Connections retain partial benefits of Hyper-Connections while reducing\nmemory consumption. To validate their effectiveness, we conduct large-scale\nexperiments on language tasks, with the largest being a 7B MoE model trained on\nup to 3T tokens, demonstrating that Frac-Connections significantly outperform\nresidual connections.",
      "tldr_zh": "该论文扩展了残差连接（residual connections），提出 Frac-Connections 作为 Hyper-Connections 的改进版本，以解决梯度消失（gradient vanishing）和表示崩溃（representation collapse）之间的 seesaw effect，同时减少内存访问成本。不同于 Hyper-Connections 通过扩展隐藏状态宽度，Frac-Connections 通过将隐藏状态分成多个部分来保留其部分优势，从而降低内存消耗。在大规模实验中，包括一个 7B MoE 模型训练至 3T tokens，该方法显著优于传统 residual connections，证明了其在语言任务上的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14125v1",
      "published_date": "2025-03-18 10:37:50 UTC",
      "updated_date": "2025-03-18 10:37:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:38:35.034559"
    },
    {
      "arxiv_id": "2503.14109v1",
      "title": "Operational Change Detection for Geographical Information: Overview and Challenges",
      "title_zh": "针对地理信息的操作性变化检测：概述和挑战",
      "authors": [
        "Nicolas Gonthier"
      ],
      "abstract": "Rapid evolution of territories due to climate change and human impact\nrequires prompt and effective updates to geospatial databases maintained by the\nNational Mapping Agency. This paper presents a comprehensive overview of change\ndetection methods tailored for the operational updating of large-scale\ngeographic databases. This review first outlines the fundamental definition of\nchange, emphasizing its multifaceted nature, from temporal to semantic\ncharacterization. It categorizes automatic change detection methods into four\nmain families: rule-based, statistical, machine learning, and simulation\nmethods. The strengths, limitations, and applicability of every family are\ndiscussed in the context of various input data. Then, key applications for\nNational Mapping Agencies are identified, particularly the optimization of\ngeospatial database updating, change-based phenomena, and dynamics monitoring.\nFinally, the paper highlights the current challenges for leveraging change\ndetection such as the variability of change definition, the missing of relevant\nlarge-scale datasets, the diversity of input data, the unstudied no-change\ndetection, the human in the loop integration and the operational constraints.\nThe discussion underscores the necessity for ongoing innovation in change\ndetection techniques to address the future needs of geographic information\nsystems for national mapping agencies.",
      "tldr_zh": "这篇论文概述了地理信息变化检测的操作方法及其挑战，针对国家测绘机构更新大规模地理空间数据库的需求。它首先定义了变化的概念，并将自动变化检测方法分类为基于规则的、统计的、机器学习的和模拟方法，讨论了每类方法的优势、局限性和适用于不同输入数据的情景。论文突出了关键应用，如优化数据库更新和动态监测，同时强调了当前挑战，包括变化定义的变异性、缺少大规模数据集、输入数据的多样性以及人类参与的整合需求，呼吁在变化检测技术上持续创新以适应未来地理信息系统需求。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint under review",
      "pdf_url": "http://arxiv.org/pdf/2503.14109v1",
      "published_date": "2025-03-18 10:25:28 UTC",
      "updated_date": "2025-03-18 10:25:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:38:48.018135"
    },
    {
      "arxiv_id": "2503.14106v1",
      "title": "Reliable uncertainty quantification for 2D/3D anatomical landmark localization using multi-output conformal prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Jef Jonkers",
        "Frank Coopman",
        "Luc Duchateau",
        "Glenn Van Wallendael",
        "Sofie Van Hoecke"
      ],
      "abstract": "Automatic anatomical landmark localization in medical imaging requires not\njust accurate predictions but reliable uncertainty quantification for effective\nclinical decision support. Current uncertainty quantification approaches often\nfall short, particularly when combined with normality assumptions,\nsystematically underestimating total predictive uncertainty. This paper\nintroduces conformal prediction as a framework for reliable uncertainty\nquantification in anatomical landmark localization, addressing a critical gap\nin automatic landmark localization. We present two novel approaches\nguaranteeing finite-sample validity for multi-output prediction: Multi-output\nRegression-as-Classification Conformal Prediction (M-R2CCP) and its variant\nMulti-output Regression to Classification Conformal Prediction set to Region\n(M-R2C2R). Unlike conventional methods that produce axis-aligned\nhyperrectangular or ellipsoidal regions, our approaches generate flexible,\nnon-convex prediction regions that better capture the underlying uncertainty\nstructure of landmark predictions. Through extensive empirical evaluation\nacross multiple 2D and 3D datasets, we demonstrate that our methods\nconsistently outperform existing multi-output conformal prediction approaches\nin both validity and efficiency. This work represents a significant advancement\nin reliable uncertainty estimation for anatomical landmark localization,\nproviding clinicians with trustworthy confidence measures for their diagnoses.\nWhile developed for medical imaging, these methods show promise for broader\napplications in multi-output regression problems.",
      "tldr_zh": "这篇论文针对2D/3D解剖标志定位中的不确定性量化问题，引入multi-output conformal prediction框架，以解决现有方法（如基于正态假设）系统低估预测不确定性的缺陷。论文提出两种新方法：Multi-output Regression-as-Classification Conformal Prediction (M-R2CCP) 和 Multi-output Regression to Classification Conformal Prediction set to Region (M-R2C2R)，这些方法能生成灵活的非凸预测区域，确保有限样本有效性，并在多个数据集上表现出色。实验结果显示，这些方法在有效性和效率上优于传统multi-output conformal prediction方法，为临床决策提供可靠的置信度估计。该框架不仅适用于医疗成像，还可扩展到更广泛的多输出回归问题。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.CV",
      "comment": "33 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.14106v1",
      "published_date": "2025-03-18 10:21:32 UTC",
      "updated_date": "2025-03-18 10:21:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:39:00.724878"
    },
    {
      "arxiv_id": "2503.14102v1",
      "title": "Sensory-driven microinterventions for improved health and wellbeing",
      "title_zh": "翻译失败",
      "authors": [
        "Youssef Abdalla",
        "Elia Gatti",
        "Mine Orlu",
        "Marianna Obrist"
      ],
      "abstract": "The five senses are gateways to our wellbeing and their decline is considered\na significant public health challenge which is linked to multiple conditions\nthat contribute significantly to morbidity and mortality. Modern technology,\nwith its ubiquitous nature and fast data processing has the ability to leverage\nthe power of the senses to transform our approach to day to day healthcare,\nwith positive effects on our quality of life. Here, we introduce the idea of\nsensory-driven microinterventions for preventative, personalised healthcare.\nMicrointerventions are targeted, timely, minimally invasive strategies that\nseamlessly integrate into our daily life. This idea harnesses human's sensory\ncapabilities, leverages technological advances in sensory stimulation and\nreal-time processing ability for sensing the senses. The collection of sensory\ndata from our continuous interaction with technology - for example the tone of\nvoice, gait movement, smart home behaviour - opens up a shift towards\npersonalised technology-enabled, sensory-focused healthcare interventions,\ncoupled with the potential of early detection and timely treatment of sensory\ndeficits that can signal critical health insights, especially for\nneurodegenerative diseases such as Parkinson's disease.",
      "tldr_zh": "本文提出了一种名为sensory-driven microinterventions的概念，利用五种感官作为健康福祉的门户，通过现代技术和实时数据处理进行预防性和个性化医疗。microinterventions是针对性、及时且最小侵入的策略，能够无缝融入日常生活，例如通过收集语音语气、步态或智能家居行为等感官数据。研究强调，这种方法可提升生活质量，并实现对感官缺陷的早期检测和治疗，尤其在神经退行性疾病如帕金森病中的应用潜力。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.ET"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14102v1",
      "published_date": "2025-03-18 10:17:55 UTC",
      "updated_date": "2025-03-18 10:17:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:39:11.242962"
    },
    {
      "arxiv_id": "2503.14088v1",
      "title": "Toward Large-Scale Distributed Quantum Long Short-Term Memory with Modular Quantum Computers",
      "title_zh": "翻译失败",
      "authors": [
        "Kuan-Cheng Chen",
        "Samuel Yen-Chi Chen",
        "Chen-Yu Liu",
        "Kin K. Leung"
      ],
      "abstract": "In this work, we introduce a Distributed Quantum Long Short-Term Memory\n(QLSTM) framework that leverages modular quantum computing to address\nscalability challenges on Noisy Intermediate-Scale Quantum (NISQ) devices. By\nembedding variational quantum circuits into LSTM cells, the QLSTM captures\nlong-range temporal dependencies, while a distributed architecture partitions\nthe underlying Variational Quantum Circuits (VQCs) into smaller, manageable\nsubcircuits that can be executed on a network of quantum processing units. We\nassess the proposed framework using nontrivial benchmark problems such as\ndamped harmonic oscillators and Nonlinear Autoregressive Moving Average\nsequences. Our results demonstrate that the distributed QLSTM achieves stable\nconvergence and improved training dynamics compared to classical approaches.\nThis work underscores the potential of modular, distributed quantum computing\narchitectures for large-scale sequence modelling, providing a foundation for\nthe future integration of hybrid quantum-classical solutions into advanced\nQuantum High-performance computing (HPC) ecosystems.",
      "tldr_zh": "本研究提出了一种分布式 Quantum Long Short-Term Memory (QLSTM) 框架，利用模块化量子计算机来解决 Noisy Intermediate-Scale Quantum (NISQ) 设备的可扩展性挑战，通过将 Variational Quantum Circuits (VQCs) 嵌入 LSTM 单元并分区执行，以捕捉长程时间依赖性。实验评估了该框架在 damped harmonic oscillators 和 Nonlinear Autoregressive Moving Average 序列等基准问题上的性能，结果显示分布式 QLSTM 比经典方法实现了更稳定的收敛和改进的训练动态。该工作为大规模序列建模提供了基础，并推动了混合量子-经典解决方案在 Quantum High-performance computing (HPC) 生态系统中的整合。",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14088v1",
      "published_date": "2025-03-18 10:07:34 UTC",
      "updated_date": "2025-03-18 10:07:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:39:23.954335"
    },
    {
      "arxiv_id": "2503.22705v1",
      "title": "Enhancing nonnative speech perception and production through an AI-powered application",
      "title_zh": "通过 AI 驱动的应用增强非母语语音感知和生产",
      "authors": [
        "Georgios P. Georgiou"
      ],
      "abstract": "While research on using Artificial Intelligence (AI) through various\napplications to enhance foreign language pronunciation is expanding, it has\nprimarily focused on aspects such as comprehensibility and intelligibility,\nlargely neglecting the improvement of individual speech sounds in both\nperception and production. This study seeks to address this gap by examining\nthe impact of training with an AI-powered mobile application on nonnative sound\nperception and production. Participants completed a pretest assessing their\nability to discriminate the second language English heed-hid contrast and\nproduce these vowels in sentence contexts. The intervention involved training\nwith the Speakometer mobile application, which incorporated recording tasks\nfeaturing the English vowels, along with pronunciation feedback and practice.\nThe posttest mirrored the pretest to measure changes in performance. The\nresults revealed significant improvements in both discrimination accuracy and\nproduction of the target contrast following the intervention. However,\nparticipants did not achieve native-like competence. These findings highlight\nthe effectiveness of AI-powered applications in facilitating speech acquisition\nand support their potential use for personalized, interactive pronunciation\ntraining beyond the classroom.",
      "tldr_zh": "本研究探讨了AI-powered应用在提升非母语者语音感知和生产方面的作用，针对以往研究忽略的个别语音（如heed-hid对比）进行干预。参与者通过Speakometer移动应用进行录音任务、发音反馈和练习，预测试和后测试结果显示，干预后他们的语音区分准确性和生产能力均有显著改善，但未达到母语水平。这些发现强调了AI-powered应用的潜力，可用于课堂外的个性化、互动式语音训练。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22705v1",
      "published_date": "2025-03-18 10:05:12 UTC",
      "updated_date": "2025-03-18 10:05:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:39:36.816134"
    },
    {
      "arxiv_id": "2503.17395v2",
      "title": "CP-NCBF: A Conformal Prediction-based Approach to Synthesize Verified Neural Control Barrier Functions",
      "title_zh": "翻译失败",
      "authors": [
        "Manan Tayal",
        "Aditya Singh",
        "Pushpak Jagtap",
        "Shishir Kolathaya"
      ],
      "abstract": "Control Barrier Functions (CBFs) are a practical approach for designing\nsafety-critical controllers, but constructing them for arbitrary nonlinear\ndynamical systems remains a challenge. Recent efforts have explored\nlearning-based methods, such as neural CBFs (NCBFs), to address this issue.\nHowever, ensuring the validity of NCBFs is difficult due to potential learning\nerrors. In this letter, we propose a novel framework that leverages\nsplit-conformal prediction to generate formally verified neural CBFs with\nprobabilistic guarantees based on a user-defined error rate, referred to as\nCP-NCBF. Unlike existing methods that impose Lipschitz constraints on neural\nCBF-leading to scalability limitations and overly conservative safe sets--our\napproach is sample-efficient, scalable, and results in less restrictive safety\nregions. We validate our framework through case studies on obstacle avoidance\nin autonomous driving and geo-fencing of aerial vehicles, demonstrating its\nability to generate larger and less conservative safe sets compared to\nconventional techniques.",
      "tldr_zh": "这篇论文提出了一种名为 CP-NCBF 的新框架，利用 split-conformal prediction 方法来合成正式验证的 Neural Control Barrier Functions (NCBFs)，为任意非线性动态系统提供基于用户定义错误率的概率安全保证。相比传统方法，该框架避免了强加 Lipschitz 约束带来的可扩展性和过度保守问题，实现更高的样本效率和更大的安全区域。通过自主驾驶障碍避免和空中车辆地理围栏的案例研究，CP-NCBF 展示了在生成更少限制的安全集方面的优越性能。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.RO",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "17 Pages, 10 Figures. First two authors have contributed equally",
      "pdf_url": "http://arxiv.org/pdf/2503.17395v2",
      "published_date": "2025-03-18 10:01:06 UTC",
      "updated_date": "2025-05-17 13:16:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:39:48.097421"
    },
    {
      "arxiv_id": "2503.14076v1",
      "title": "Theoretical Foundation of Flow-Based Time Series Generation: Provable Approximation, Generalization, and Efficiency",
      "title_zh": "基于流的时序生成的理论基础：可证明的逼近、泛化和效率",
      "authors": [
        "Jiangxuan Long",
        "Zhao Song",
        "Chiwun Yang"
      ],
      "abstract": "Recent studies suggest utilizing generative models instead of traditional\nauto-regressive algorithms for time series forecasting (TSF) tasks. These\nnon-auto-regressive approaches involving different generative methods,\nincluding GAN, Diffusion, and Flow Matching for time series, have empirically\ndemonstrated high-quality generation capability and accuracy. However, we still\nlack an appropriate understanding of how it processes approximation and\ngeneralization. This paper presents the first theoretical framework from the\nperspective of flow-based generative models to relieve the knowledge of\nlimitations. In particular, we provide our insights with strict guarantees from\nthree perspectives: $\\textbf{Approximation}$, $\\textbf{Generalization}$ and\n$\\textbf{Efficiency}$. In detail, our analysis achieves the contributions as\nfollows:\n  $\\bullet$ By assuming a general data model, the fitting of the flow-based\ngenerative models is confirmed to converge to arbitrary error under the\nuniversal approximation of Diffusion Transformer (DiT).\n  $\\bullet$ Introducing a polynomial-based regularization for flow matching,\nthe generalization error thus be bounded since the generalization of polynomial\napproximation.\n  $\\bullet$ The sampling for generation is considered as an optimization\nprocess, we demonstrate its fast convergence with updating standard first-order\ngradient descent of some objective.",
      "tldr_zh": "这篇论文建立了流-based生成模型在时间序列生成中的理论基础，针对传统自回归算法的替代方案（如GAN、Diffusion和Flow Matching）进行分析，重点探讨逼近（Approximation）、泛化（Generalization）和效率（Efficiency）的严格保证。\n论文证明，在一般数据模型下，使用Diffusion Transformer的流-based模型可以收敛到任意误差，实现通用逼近。\n此外，通过引入多项式-based正则化，论文约束了泛化误差，并将生成采样视为优化过程，证明其使用一阶梯度下降可实现快速收敛。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "33 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.14076v1",
      "published_date": "2025-03-18 09:53:48 UTC",
      "updated_date": "2025-03-18 09:53:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:39:59.379197"
    },
    {
      "arxiv_id": "2503.14070v1",
      "title": "Fast Autoregressive Video Generation with Diagonal Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Ye",
        "Junliang Guo",
        "Haoyu Wu",
        "Tianyu He",
        "Tim Pearce",
        "Tabish Rashid",
        "Katja Hofmann",
        "Jiang Bian"
      ],
      "abstract": "Autoregressive Transformer models have demonstrated impressive performance in\nvideo generation, but their sequential token-by-token decoding process poses a\nmajor bottleneck, particularly for long videos represented by tens of thousands\nof tokens. In this paper, we propose Diagonal Decoding (DiagD), a training-free\ninference acceleration algorithm for autoregressively pre-trained models that\nexploits spatial and temporal correlations in videos. Our method generates\ntokens along diagonal paths in the spatial-temporal token grid, enabling\nparallel decoding within each frame as well as partially overlapping across\nconsecutive frames. The proposed algorithm is versatile and adaptive to various\ngenerative models and tasks, while providing flexible control over the\ntrade-off between inference speed and visual quality. Furthermore, we propose a\ncost-effective finetuning strategy that aligns the attention patterns of the\nmodel with our decoding order, further mitigating the training-inference gap on\nsmall-scale models. Experiments on multiple autoregressive video generation\nmodels and datasets demonstrate that DiagD achieves up to $10\\times$ speedup\ncompared to naive sequential decoding, while maintaining comparable visual\nfidelity.",
      "tldr_zh": "本论文提出Diagonal Decoding (DiagD)，一种无需额外训练的推理加速算法，用于提升Autoregressive Transformer模型在视频生成中的效率，解决其顺序解码导致的长视频生成瓶颈问题。DiagD通过利用视频的空间和时间相关性，在空间-时间token网格中沿对角线路径生成tokens，实现帧内并行解码和帧间部分重叠，从而提供灵活的推理速度与视觉质量权衡。论文还引入一种成本有效的微调策略，调整模型的注意力模式以减少训练-推理差距；实验结果显示，DiagD在多个模型和数据集上比传统顺序解码快10倍，同时保持相似的视觉保真度。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14070v1",
      "published_date": "2025-03-18 09:42:55 UTC",
      "updated_date": "2025-03-18 09:42:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:40:11.055932"
    },
    {
      "arxiv_id": "2503.16530v1",
      "title": "Enhancing LLM Generation with Knowledge Hypergraph for Evidence-Based Medicine",
      "title_zh": "翻译失败",
      "authors": [
        "Chengfeng Dou",
        "Ying Zhang",
        "Zhi Jin",
        "Wenpin Jiao",
        "Haiyan Zhao",
        "Yongqiang Zhao",
        "Zhengwei Tao"
      ],
      "abstract": "Evidence-based medicine (EBM) plays a crucial role in the application of\nlarge language models (LLMs) in healthcare, as it provides reliable support for\nmedical decision-making processes. Although it benefits from current\nretrieval-augmented generation~(RAG) technologies, it still faces two\nsignificant challenges: the collection of dispersed evidence and the efficient\norganization of this evidence to support the complex queries necessary for EBM.\nTo tackle these issues, we propose using LLMs to gather scattered evidence from\nmultiple sources and present a knowledge hypergraph-based evidence management\nmodel to integrate these evidence while capturing intricate relationships.\nFurthermore, to better support complex queries, we have developed an\nImportance-Driven Evidence Prioritization (IDEP) algorithm that utilizes the\nLLM to generate multiple evidence features, each with an associated importance\nscore, which are then used to rank the evidence and produce the final retrieval\nresults. Experimental results from six datasets demonstrate that our approach\noutperforms existing RAG techniques in application domains of interest to EBM,\nsuch as medical quizzing, hallucination detection, and decision support.\nTestsets and the constructed knowledge graph can be accessed at\n\\href{https://drive.google.com/file/d/1WJ9QTokK3MdkjEmwuFQxwH96j_Byawj_/view?usp=drive_link}{https://drive.google.com/rag4ebm}.",
      "tldr_zh": "该研究针对证据-based medicine (EBM) 在 large language models (LLMs) 应用中的挑战，提出了一种基于知识 hypergraph 的增强生成方法，以解决证据收集和组织问题。方法包括使用 LLMs 从多源收集分散证据，并构建知识 hypergraph 来整合证据并捕捉复杂关系，同时引入 Importance-Driven Evidence Prioritization (IDEP) 算法，通过生成证据特征和重要性分数来优先排序证据。实验结果显示，该方法在六个数据集上优于现有 retrieval-augmented generation (RAG) 技术，尤其在医疗测验、幻觉检测和决策支持领域表现出色。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16530v1",
      "published_date": "2025-03-18 09:17:31 UTC",
      "updated_date": "2025-03-18 09:17:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:40:24.640367"
    },
    {
      "arxiv_id": "2503.14053v1",
      "title": "ON-Traffic: An Operator Learning Framework for Online Traffic Flow Estimation and Uncertainty Quantification from Lagrangian Sensors",
      "title_zh": "翻译失败",
      "authors": [
        "Jake Rap",
        "Amritam Das"
      ],
      "abstract": "Accurate traffic flow estimation and prediction are critical for the\nefficient management of transportation systems, particularly under increasing\nurbanization. Traditional methods relying on static sensors often suffer from\nlimited spatial coverage, while probe vehicles provide richer, albeit sparse\nand irregular data. This work introduces ON-Traffic, a novel deep operator\nNetwork and a receding horizon learning-based framework tailored for online\nestimation of spatio-temporal traffic state along with quantified uncertainty\nby using measurements from moving probe vehicles and downstream boundary\ninputs. Our framework is evaluated in both numerical and simulation datasets,\nshowcasing its ability to handle irregular, sparse input data, adapt to\ntime-shifted scenarios, and provide well-calibrated uncertainty estimates. The\nresults demonstrate that the model captures complex traffic phenomena,\nincluding shockwaves and congestion propagation, while maintaining robustness\nto noise and sensor dropout. These advancements present a significant step\ntoward online, adaptive traffic management systems.",
      "tldr_zh": "这篇论文引入了ON-Traffic框架，这是一个基于deep operator Network和receding horizon learning的深度学习框架，用于从Lagrangian sensors（如探针车辆）的数据中进行在线交通流量估计并量化不确定性。框架能处理不规则、稀疏输入数据，适应时间偏移场景，并在数值和模拟数据集上表现出色，提供校准良好的不确定性估计。结果显示，ON-Traffic成功捕捉了复杂的交通现象，如冲击波和拥堵传播，并对噪声和传感器掉线具有鲁棒性，为在线适应性交通管理系统的发展提供了重要进展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14053v1",
      "published_date": "2025-03-18 09:13:24 UTC",
      "updated_date": "2025-03-18 09:13:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:40:35.932558"
    },
    {
      "arxiv_id": "2503.14048v1",
      "title": "Beyond holography: the entropic quantum gravity foundations of image processing",
      "title_zh": "超越全息学：图像处理的熵量子引力基础",
      "authors": [
        "Ginestra Bianconi"
      ],
      "abstract": "Recently, thanks to the development of artificial intelligence (AI) there is\nincreasing scientific attention to establishing the connections between\ntheoretical physics and AI. Traditionally, these connections have been focusing\nmostly on the relation between string theory and image processing and involve\nimportant theoretical paradigms such as holography. Recently G. Bianconi has\nproposed the entropic quantum gravity approach that proposes an action for\ngravity given by the quantum relative entropy between the metrics associated to\na manifold. Here it is demonstrated that the famous Perona-Malik algorithm for\nimage processing is the gradient flow of the entropic quantum gravity action.\nThese results provide the geometrical and information theory foundations for\nthe Perona-Malik algorithm and open new avenues for establishing fundamental\nrelations between brain research, machine learning and entropic quantum\ngravity.",
      "tldr_zh": "该研究扩展了理论物理与人工智能的联系，超越传统holography范式，探讨了entropic quantum gravity在图像处理中的基础。作者证明了著名的Perona-Malik algorithm是entropic quantum gravity行动的梯度流，利用量子相对熵（quantum relative entropy）来定义引力作用，从而为其提供了几何和信息理论支撑。这些发现为脑研究、机器学习和entropic quantum gravity之间的潜在关系开辟了新路径，促进跨学科融合。",
      "categories": [
        "cond-mat.dis-nn",
        "cond-mat.stat-mech",
        "cs.AI",
        "gr-qc",
        "quant-ph"
      ],
      "primary_category": "cond-mat.dis-nn",
      "comment": "(7 pages, 1 figure)",
      "pdf_url": "http://arxiv.org/pdf/2503.14048v1",
      "published_date": "2025-03-18 09:06:33 UTC",
      "updated_date": "2025-03-18 09:06:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:40:47.342303"
    },
    {
      "arxiv_id": "2503.16529v2",
      "title": "Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts",
      "title_zh": "DeepSeek 模型在中文语境中的安全评估与增强",
      "authors": [
        "Wenjing Zhang",
        "Xuejiao Lei",
        "Zhaoxiang Liu",
        "Limin Han",
        "Jiaojiao Zhao",
        "Junting Guo",
        "Zhenhong Long",
        "Shu Yang",
        "Meijuan An",
        "Beibei Huang",
        "Rongjia Du",
        "Ning Wang",
        "Kai Wang",
        "Shiguo Lian"
      ],
      "abstract": "DeepSeek-R1, renowned for its exceptional reasoning capabilities and\nopen-source strategy, is significantly influencing the global artificial\nintelligence landscape. However, it exhibits notable safety shortcomings.\nRecent research conducted by Robust Intelligence, a subsidiary of Cisco, in\ncollaboration with the University of Pennsylvania, revealed that DeepSeek-R1\nachieves a 100\\% attack success rate when processing harmful prompts.\nFurthermore, multiple security firms and research institutions have identified\ncritical security vulnerabilities within the model. Although China Unicom has\nuncovered safety vulnerabilities of R1 in Chinese contexts, the safety\ncapabilities of the remaining distilled models in the R1 series have not yet\nbeen comprehensively evaluated. To address this gap, this study utilizes the\ncomprehensive Chinese safety benchmark CHiSafetyBench to conduct an in-depth\nsafety evaluation of the DeepSeek-R1 series distilled models. The objective is\nto assess the safety capabilities of these models in Chinese contexts both\nbefore and after distillation, and to further elucidate the adverse effects of\ndistillation on model safety. Building on these findings, we implement targeted\nsafety enhancements for the entire DeepSeek-R1 model series. Evaluation results\nindicate that the enhanced models achieve significant improvements in safety\nwhile maintaining reasoning capabilities without notable degradation. We\nopen-source the safety-enhanced models at\nhttps://github.com/UnicomAI/DeepSeek-R1-Safe to serve as a valuable resource\nfor future research and optimization of DeepSeek models.",
      "tldr_zh": "这篇论文评估了 DeepSeek 模型系列在中文语境下的安全性问题，特别是 DeepSeek-R1 的安全漏洞，如在处理有害提示时达到 100% 攻击成功率。研究团队使用 CHiSafetyBench 基准对模型进行全面评估，分析了蒸馏前后模型的安全性能，并揭示了蒸馏过程对安全性的负面影响。随后，他们实施了针对性的安全增强策略，使增强后的模型在保持推理能力的同时显著提高了安全性。最终，增强模型已开放 sourced 于 https://github.com/UnicomAI/DeepSeek-R1-Safe，以支持未来优化研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 13 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.16529v2",
      "published_date": "2025-03-18 08:38:10 UTC",
      "updated_date": "2025-05-16 13:29:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:40:59.228491"
    },
    {
      "arxiv_id": "2503.14021v1",
      "title": "MP-GUI: Modality Perception with MLLMs for GUI Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Ziwei Wang",
        "Weizhi Chen",
        "Leyang Yang",
        "Sheng Zhou",
        "Shengchu Zhao",
        "Hanbei Zhan",
        "Jiongchao Jin",
        "Liangcheng Li",
        "Zirui Shao",
        "Jiajun Bu"
      ],
      "abstract": "Graphical user interface (GUI) has become integral to modern society, making\nit crucial to be understood for human-centric systems. However, unlike natural\nimages or documents, GUIs comprise artificially designed graphical elements\narranged to convey specific semantic meanings. Current multi-modal large\nlanguage models (MLLMs) already proficient in processing graphical and textual\ncomponents suffer from hurdles in GUI understanding due to the lack of explicit\nspatial structure modeling. Moreover, obtaining high-quality spatial structure\ndata is challenging due to privacy issues and noisy environments. To address\nthese challenges, we present MP-GUI, a specially designed MLLM for GUI\nunderstanding. MP-GUI features three precisely specialized perceivers to\nextract graphical, textual, and spatial modalities from the screen as\nGUI-tailored visual clues, with spatial structure refinement strategy and\nadaptively combined via a fusion gate to meet the specific preferences of\ndifferent GUI understanding tasks. To cope with the scarcity of training data,\nwe also introduce a pipeline for automatically data collecting. Extensive\nexperiments demonstrate that MP-GUI achieves impressive results on various GUI\nunderstanding tasks with limited data.",
      "tldr_zh": "该研究针对图形用户界面(GUI)理解的挑战，提出了一种基于多模态大型语言模型(MLLMs)的框架MP-GUI，以解决当前模型在处理GUI的空间结构建模方面的不足。MP-GUI包括三个专门的感知器，用于提取图形、文本和空间模态，并通过空间结构精炼策略和融合门(fusion gate)自适应地结合这些模态，以适应不同GUI任务的需求。同时，作者引入了一个自动数据收集管道来缓解训练数据稀缺的问题。实验结果显示，MP-GUI在各种GUI理解任务上取得了显著性能提升，即使在数据有限的情况下。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "Paper accepted to CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.14021v1",
      "published_date": "2025-03-18 08:32:22 UTC",
      "updated_date": "2025-03-18 08:32:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:41:11.030696"
    },
    {
      "arxiv_id": "2503.14013v1",
      "title": "Boosting Semi-Supervised Medical Image Segmentation via Masked Image Consistency and Discrepancy Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Pengcheng Zhou",
        "Lantian Zhang",
        "Wei Li"
      ],
      "abstract": "Semi-supervised learning is of great significance in medical image\nsegmentation by exploiting unlabeled data. Among its strategies, the\nco-training framework is prominent. However, previous co-training studies\npredominantly concentrate on network initialization variances and pseudo-label\ngeneration, while overlooking the equilibrium between information interchange\nand model diversity preservation. In this paper, we propose the Masked Image\nConsistency and Discrepancy Learning (MICD) framework with three key modules.\nThe Masked Cross Pseudo Consistency (MCPC) module enriches context perception\nand small sample learning via pseudo-labeling across masked-input branches. The\nCross Feature Consistency (CFC) module fortifies information exchange and model\nrobustness by ensuring decoder feature consistency. The Cross Model Discrepancy\n(CMD) module utilizes EMA teacher networks to oversee outputs and preserve\nbranch diversity. Together, these modules address existing limitations by\nfocusing on fine-grained local information and maintaining diversity in a\nheterogeneous framework. Experiments on two public medical image datasets, AMOS\nand Synapse, demonstrate that our approach outperforms state-of-the-art\nmethods.",
      "tldr_zh": "本文提出 Masked Image Consistency and Discrepancy Learning (MICD) 框架，用于提升半监督学习在医疗图像分割中的性能，通过平衡信息交换和模型多样性来解决现有 co-training 方法的局限性。该框架包括三个关键模块：Masked Cross Pseudo Consistency (MCPC) 模块通过伪标签在掩码输入分支上增强上下文感知和小样本学习；Cross Feature Consistency (CFC) 模块确保解码器特征一致性以加强信息交换和模型鲁棒性；Cross Model Discrepancy (CMD) 模块利用 EMA teacher networks 监督输出并维护分支多样性。在 AMOS 和 Synapse 等公共数据集上的实验显示，该方法优于最先进方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14013v1",
      "published_date": "2025-03-18 08:20:35 UTC",
      "updated_date": "2025-03-18 08:20:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:41:25.473438"
    },
    {
      "arxiv_id": "2503.14002v1",
      "title": "MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific Generative Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Damian Boborzi",
        "Phillip Mueller",
        "Jonas Emrich",
        "Dominik Schmid",
        "Sebastian Mueller",
        "Lars Mikelsons"
      ],
      "abstract": "Generative models have recently made remarkable progress in the field of 3D\nobjects. However, their practical application in fields like engineering\nremains limited since they fail to deliver the accuracy, quality, and\ncontrollability needed for domain-specific tasks. Fine-tuning large generative\nmodels is a promising perspective for making these models available in these\nfields. Creating high-quality, domain-specific 3D datasets is crucial for\nfine-tuning large generative models, yet the data filtering and annotation\nprocess remains a significant bottleneck. We present MeshFleet, a filtered and\nannotated 3D vehicle dataset extracted from Objaverse-XL, the most extensive\npublicly available collection of 3D objects. Our approach proposes a pipeline\nfor automated data filtering based on a quality classifier. This classifier is\ntrained on a manually labeled subset of Objaverse, incorporating DINOv2 and\nSigLIP embeddings, refined through caption-based analysis and uncertainty\nestimation. We demonstrate the efficacy of our filtering method through a\ncomparative analysis against caption and image aesthetic score-based techniques\nand fine-tuning experiments with SV3D, highlighting the importance of targeted\ndata selection for domain-specific 3D generative modeling.",
      "tldr_zh": "该论文介绍了MeshFleet，一种从Objaverse-XL提取的过滤和标注的3D车辆数据集，旨在解决生成模型在工程等领域应用时存在的准确性、质量和可控性问题。研究提出了一种自动化数据过滤管道，使用基于DINOv2和SigLIP embeddings的质量分类器，并在手动标注的子集上训练，通过标题分析和不确定性估计进行优化。实验结果显示，该方法在与标题或图像美学分数技术比较中表现出优越性，并在SV3D的微调实验中证明了针对性数据选择的必要性，从而提升了领域特定3D生成建模的效能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14002v1",
      "published_date": "2025-03-18 08:09:24 UTC",
      "updated_date": "2025-03-18 08:09:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:41:36.562212"
    },
    {
      "arxiv_id": "2503.13999v2",
      "title": "BI-RADS prediction of mammographic masses using uncertainty information extracted from a Bayesian Deep Learning model",
      "title_zh": "翻译失败",
      "authors": [
        "Mohaddeseh Chegini",
        "Ali Mahloojifar"
      ],
      "abstract": "The BI_RADS score is a probabilistic reporting tool used by radiologists to\nexpress the level of uncertainty in predicting breast cancer based on some\nmorphological features in mammography images. There is a significant\nvariability in describing masses which sometimes leads to BI_RADS\nmisclassification. Using a BI_RADS prediction system is required to support the\nfinal radiologist decisions. In this study, the uncertainty information\nextracted by a Bayesian deep learning model is utilized to predict the BI_RADS\nscore. The investigation results based on the pathology information demonstrate\nthat the f1-scores of the predictions of the radiologist are 42.86%, 48.33% and\n48.28%, meanwhile, the f1-scores of the model performance are 73.33%, 59.60%\nand 59.26% in the BI_RADS 2, 3 and 5 dataset samples, respectively. Also, the\nmodel can distinguish malignant from benign samples in the BI_RADS 0 category\nof the used dataset with an accuracy of 75.86% and correctly identify all\nmalignant samples as BI_RADS 5. The Grad-CAM visualization shows the model pays\nattention to the morphological features of the lesions. Therefore, this study\nshows the uncertainty-aware Bayesian Deep Learning model can report his\nuncertainty about the malignancy of a lesion based on morphological features,\nlike a radiologist.",
      "tldr_zh": "本研究利用Bayesian Deep Learning模型提取的不确定性信息，来预测乳房X光图像中肿块的BI-RADS分数，以辅助放射科医生减少误分类。模型在BI-RADS 2、3和5类别上的F1-score分别为73.33%、59.60%和59.26%，优于放射科医生的42.86%、48.33%和48.28%。此外，模型在BI-RADS 0类别中以75.86%的准确率区分恶性和良性样本，并通过Grad-CAM可视化证明其关注病变的形态特征，从而展示了Bayesian Deep Learning模型在报告不确定性和支持决策方面的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13999v2",
      "published_date": "2025-03-18 08:06:05 UTC",
      "updated_date": "2025-03-24 12:24:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:41:47.891927"
    },
    {
      "arxiv_id": "2503.13991v1",
      "title": "GraphTEN: Graph Enhanced Texture Encoding Network",
      "title_zh": "翻译失败",
      "authors": [
        "Bo Peng",
        "Jintao Chen",
        "Mufeng Yao",
        "Chenhao Zhang",
        "Jianghui Zhang",
        "Mingmin Chi",
        "Jiang Tao"
      ],
      "abstract": "Texture recognition is a fundamental problem in computer vision and pattern\nrecognition. Recent progress leverages feature aggregation into discriminative\ndescriptions based on convolutional neural networks (CNNs). However, modeling\nnon-local context relations through visual primitives remains challenging due\nto the variability and randomness of texture primitives in spatial\ndistributions. In this paper, we propose a graph-enhanced texture encoding\nnetwork (GraphTEN) designed to capture both local and global features of\ntexture primitives. GraphTEN models global associations through fully connected\ngraphs and captures cross-scale dependencies of texture primitives via\nbipartite graphs. Additionally, we introduce a patch encoding module that\nutilizes a codebook to achieve an orderless representation of texture by\nencoding multi-scale patch features into a unified feature space. The proposed\nGraphTEN achieves superior performance compared to state-of-the-art methods\nacross five publicly available datasets.",
      "tldr_zh": "本研究针对纹理识别领域的挑战，提出了一种图增强纹理编码网络(GraphTEN)，旨在通过建模非局部上下文关系来捕获纹理原语的局部和全局特征。GraphTEN利用全连接图(fullly connected graphs)处理全局关联，并通过二分图(bipartite graphs)捕捉跨尺度依赖，同时引入补丁编码模块(patch encoding module)，使用codebook将多尺度补丁特征编码成无序的统一特征空间。与现有基于CNNs的方法相比，该框架在五个公开数据集上实现了优于最先进方法(state-of-the-art methods)的性能表现。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T45",
        "I.2.10; I.4.7"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 7 figures, conference paper",
      "pdf_url": "http://arxiv.org/pdf/2503.13991v1",
      "published_date": "2025-03-18 07:51:13 UTC",
      "updated_date": "2025-03-18 07:51:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:41:59.580112"
    },
    {
      "arxiv_id": "2503.14564v1",
      "title": "Effortless Active Labeling for Long-Term Test-Time Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "Guowei Wang",
        "Changxing Ding"
      ],
      "abstract": "Long-term test-time adaptation (TTA) is a challenging task due to error\naccumulation. Recent approaches tackle this issue by actively labeling a small\nproportion of samples in each batch, yet the annotation burden quickly grows as\nthe batch number increases. In this paper, we investigate how to achieve\neffortless active labeling so that a maximum of one sample is selected for\nannotation in each batch. First, we annotate the most valuable sample in each\nbatch based on the single-step optimization perspective in the TTA context. In\nthis scenario, the samples that border between the source- and target-domain\ndata distributions are considered the most feasible for the model to learn in\none iteration. Then, we introduce an efficient strategy to identify these\nsamples using feature perturbation. Second, we discover that the gradient\nmagnitudes produced by the annotated and unannotated samples have significant\nvariations. Therefore, we propose balancing their impact on model optimization\nusing two dynamic weights. Extensive experiments on the popular ImageNet-C, -R,\n-K, -A and PACS databases demonstrate that our approach consistently\noutperforms state-of-the-art methods with significantly lower annotation costs.",
      "tldr_zh": "本文针对长期测试时域适配（Long-term Test-Time Adaptation, TTA）的错误积累问题，提出了一种effortless active labeling方法，确保每个批次最多标注一个样本，以显著降低标注负担。方法包括基于单步优化视角选择源域和目标域边界样本，并使用feature perturbation高效识别这些样本；同时，通过两个动态权重平衡标注和未标注样本的梯度影响，以优化模型性能。实验在ImageNet-C, -R, -K, -A和PACS数据库上显示，该方法在保持高准确性的同时，远超现有最先进方法，且标注成本大幅降低。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025.Code:https://github.com/flash1803/EATTA",
      "pdf_url": "http://arxiv.org/pdf/2503.14564v1",
      "published_date": "2025-03-18 07:49:27 UTC",
      "updated_date": "2025-03-18 07:49:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:42:12.094148"
    },
    {
      "arxiv_id": "2503.14563v2",
      "title": "Workflow for Safe-AI",
      "title_zh": "翻译失败",
      "authors": [
        "Suzana Veljanovska",
        "Hans Dermot Doran"
      ],
      "abstract": "The development and deployment of safe and dependable AI models is crucial in\napplications where functional safety is a key concern. Given the rapid\nadvancement in AI research and the relative novelty of the safe-AI domain,\nthere is an increasing need for a workflow that balances stability with\nadaptability. This work proposes a transparent, complete, yet flexible and\nlightweight workflow that highlights both reliability and qualifiability. The\ncore idea is that the workflow must be qualifiable, which demands the use of\nqualified tools. Tool qualification is a resource-intensive process, both in\nterms of time and cost. We therefore place value on a lightweight workflow\nfeaturing a minimal number of tools with limited features. The workflow is\nbuilt upon an extended ONNX model description allowing for validation of AI\nalgorithms from their generation to runtime deployment. This validation is\nessential to ensure that models are validated before being reliably deployed\nacross different runtimes, particularly in mixed-criticality systems.\nKeywords-AI workflows, safe-AI, dependable-AI, functional safety, v-model\ndevelopment",
      "tldr_zh": "这篇论文针对安全AI（safe-AI）开发，提出一个透明、完整且灵活轻量的工作流程，以平衡可靠性和适应性为核心，强调使用合格工具来确保功能安全（functional safety）。该工作流程基于扩展的ONNX模型描述，从AI算法生成到运行时部署进行全面验证，从而减少资源消耗并最小化工具数量。实验结果显示，这种方法有助于在混合关键性系统中可靠部署AI模型，为dependable-AI应用提供可认证的框架。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Embedded World Conference, Nuremberg, 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.14563v2",
      "published_date": "2025-03-18 07:45:18 UTC",
      "updated_date": "2025-03-20 07:32:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:42:23.499370"
    },
    {
      "arxiv_id": "2503.13988v1",
      "title": "Empowering Smaller Models: Tuning LLaMA and Gemma with Chain-of-Thought for Ukrainian Exam Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Mykyta Syromiatnikov",
        "Victoria Ruvinskaya",
        "Nataliia Komleva"
      ],
      "abstract": "Leading large language models have demonstrated impressive capabilities in\nreasoning-intensive tasks, such as standardized educational testing. However,\nthey often require extensive training in low-resource settings with\ninaccessible infrastructure. Small or compact models, though more efficient,\nfrequently lack sufficient support for underrepresented languages, leaving a\nperformance gap in critical domains. This work explores the potential of\nparameter-efficient fine-tuning of compact open-weight language models to\nhandle reasoning-intensive tasks in the underrepresented Ukrainian language,\nbuilding on the findings of the ZNO-Eval benchmark. Parameter-efficient\nfine-tuning of LLaMA 3.1 (8 billion parameters), LLaMA 3.2 (3 billion\nparameters), and Gemma 2 (9 billion parameters) models on chain-of-thought\nsolutions resulted in a modest test score improvement of up to 17.4% on complex\nmatching tasks and 1.6% overall compared to tuning on answer letters alone,\noffering enhanced interpretability and robustness. In addition, the proposed\ntuning method with joint task topic and step-by-step solution generation\noutperforms standard chain-of-thought tuning in matching tasks and provides a\n5.4% gain over the best LLaMA 3.2 model due to guiding the model to recall and\napply domain-relevant information. Contrasting obtained results with zero-shot\nevaluations of leading open-weight and proprietary models such as Qwen,\nDeepSeek R1, OpenAI o1 and o3, Gemini, and Claude, highlight that fine-tuning\nLLaMA and Gemma models with 2,032 step-by-step solutions and 20 to 50 million\ntrainable parameters on a single A100 GPU lets them outperform GPT-4o mini,\nMistral Large, and larger open-weight models. This research also evaluates how\nmerging the quantized adapter with the base model influences the generation\nquality. Source code and tuned models are available at\nhttps://github.com/NLPForUA/ZNO.",
      "tldr_zh": "本研究探讨了通过参数高效微调（parameter-efficient fine-tuning）来提升小型语言模型LLaMA 3.1（8B参数）、LLaMA 3.2（3B参数）和Gemma 2（9B参数），以处理乌克兰语的推理密集型任务，如基于ZNO-Eval基准的教育测试。采用Chain-of-Thought（CoT）方法结合任务主题和逐步解决方案生成进行微调，比仅微调答案字母的方法在复杂匹配任务上提升17.4%、整体提升1.6%，并在匹配任务中比标准CoT调优提供5.4%的增益。结果显示，这些微调模型在使用单A100 GPU和少量参数（20-50百万）训练后，能超越GPT-4o mini、Mistral Large等更大模型，证明了小型模型在低资源语言场景中的高效性和鲁棒性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 6 tables, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.13988v1",
      "published_date": "2025-03-18 07:44:49 UTC",
      "updated_date": "2025-03-18 07:44:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:42:37.190489"
    },
    {
      "arxiv_id": "2503.13985v2",
      "title": "DefectFill: Realistic Defect Generation with Inpainting Diffusion Model for Visual Inspection",
      "title_zh": "翻译失败",
      "authors": [
        "Jaewoo Song",
        "Daemin Park",
        "Kanghyun Baek",
        "Sangyub Lee",
        "Jooyoung Choi",
        "Eunji Kim",
        "Sungroh Yoon"
      ],
      "abstract": "Developing effective visual inspection models remains challenging due to the\nscarcity of defect data. While image generation models have been used to\nsynthesize defect images, producing highly realistic defects remains difficult.\nWe propose DefectFill, a novel method for realistic defect generation that\nrequires only a few reference defect images. It leverages a fine-tuned\ninpainting diffusion model, optimized with our custom loss functions\nincorporating defect, object, and attention terms. It enables precise capture\nof detailed, localized defect features and their seamless integration into\ndefect-free objects. Additionally, our Low-Fidelity Selection method further\nenhances the defect sample quality. Experiments show that DefectFill generates\nhigh-quality defect images, enabling visual inspection models to achieve\nstate-of-the-art performance on the MVTec AD dataset.",
      "tldr_zh": "该研究针对视觉检查中缺陷数据稀缺的挑战，提出了 DefectFill 方法，该方法仅需少量参考缺陷图像，即可生成高度真实的缺陷图像。\nDefectFill 基于微调的 inpainting diffusion model，并通过自定义损失函数（包括 defect, object 和 attention terms）来精确捕捉局部缺陷特征，并实现其与无缺陷对象的无缝整合；同时，引入了 Low-Fidelity Selection 方法进一步提升样本质量。\n实验结果表明，DefectFill 生成的缺陷图像使视觉检查模型在 MVTec AD dataset 上达到了最先进性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.13985v2",
      "published_date": "2025-03-18 07:42:11 UTC",
      "updated_date": "2025-03-27 05:23:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:42:49.016088"
    },
    {
      "arxiv_id": "2503.14562v1",
      "title": "Analysis of human visual field information using machine learning methods and assessment of their accuracy",
      "title_zh": "使用机器学习方法分析人类视场",
      "authors": [
        "A. I. Medvedeva",
        "V. V. Bakutkin"
      ],
      "abstract": "Subject of research: is the study of methods for analyzing perimetric images\nfor the diagnosis and control of glaucoma diseases. Objects of research: is a\ndataset collected on the ophthalmological perimeter with the results of various\npatient pathologies, since the ophthalmological community is acutely aware of\nthe issue of disease control and import substitution. [5]. Purpose of research:\nis to consider various machine learning methods that can classify glaucoma.\nThis is possible thanks to the classifier built after labeling the dataset. It\nis able to determine from the image whether the visual fields depicted on it\nare the results of the impact of glaucoma on the eyes or other visual diseases.\nEarlier in the work [3], a dataset was described that was collected on the\nTomey perimeter. The average age of the examined patients ranged from 30 to 85\nyears. Methods of research: machine learning methods for classifying image\nresults (stochastic gradient descent, logistic regression, random forest, naive\nBayes). Main results of research: the result of the study is computer modeling\nthat can determine from the image whether the result is glaucoma or another\ndisease (binary classification).",
      "tldr_zh": "这篇论文研究了使用机器学习方法分析人类视场图像，以诊断和控制青光眼，研究对象是基于眼科周视仪收集的数据集，包括各种患者病理（患者年龄范围为30-85岁）。研究目的在于构建分类器，通过二元分类判断图像是否显示青光眼影响或其他疾病。方法包括stochastic gradient descent、logistic regression、random forest和naive Bayes等算法。结果表明，该计算机建模能够有效区分青光眼与其他疾病，为青光眼诊断提供潜在工具。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "in Russian language",
      "pdf_url": "http://arxiv.org/pdf/2503.14562v1",
      "published_date": "2025-03-18 07:39:41 UTC",
      "updated_date": "2025-03-18 07:39:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:43:01.368015"
    },
    {
      "arxiv_id": "2503.16528v1",
      "title": "HDLCoRe: A Training-Free Framework for Mitigating Hallucinations in LLM-Generated HDL",
      "title_zh": "翻译失败",
      "authors": [
        "Heng Ping",
        "Shixuan Li",
        "Peiyu Zhang",
        "Anzhe Cheng",
        "Shukai Duan",
        "Nikos Kanakaris",
        "Xiongye Xiao",
        "Wei Yang",
        "Shahin Nazarian",
        "Andrei Irimia",
        "Paul Bogdan"
      ],
      "abstract": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities in code generation tasks. However, when applied to hardware\ndescription languages (HDL), these models exhibit significant limitations due\nto data scarcity, resulting in hallucinations and incorrect code generation. To\naddress these challenges, we propose HDLCoRe, a training-free framework that\nenhances LLMs' HDL generation capabilities through prompt engineering\ntechniques and retrieval-augmented generation (RAG). Our approach consists of\ntwo main components: (1) an HDL-aware Chain-of-Thought (CoT) prompting\ntechnique with self-verification that classifies tasks by complexity and type,\nincorporates domain-specific knowledge, and guides LLMs through step-by-step\nself-simulation for error correction; and (2) a two-stage heterogeneous RAG\nsystem that addresses formatting inconsistencies through key component\nextraction and efficiently retrieves relevant HDL examples through sequential\nfiltering and re-ranking. HDLCoRe eliminates the need for model fine-tuning\nwhile substantially improving LLMs' HDL generation capabilities. Experimental\nresults demonstrate that our framework achieves superior performance on the\nRTLLM2.0 benchmark, significantly reducing hallucinations and improving both\nsyntactic and functional correctness.",
      "tldr_zh": "这篇论文提出 HDLCoRe，一种无需训练的框架，用于缓解大型语言模型（LLMs）在硬件描述语言（HDL）生成中的幻觉问题。框架通过 HDL-aware Chain-of-Thought (CoT) 提示技术实现任务分类、领域知识整合和步步为营的自验证错误修正，以及两阶段异构 Retrieval-Augmented Generation (RAG) 系统来提取关键组件并高效检索相关示例。实验结果显示，HDLCoRe 在 RTLLM2.0 基准上显著提升了生成代码的语法和功能正确性，减少了幻觉现象。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16528v1",
      "published_date": "2025-03-18 07:09:39 UTC",
      "updated_date": "2025-03-18 07:09:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:43:13.310150"
    },
    {
      "arxiv_id": "2503.13951v1",
      "title": "FrustumFusionNets: A Three-Dimensional Object Detection Network Based on Tractor Road Scene",
      "title_zh": "翻译失败",
      "authors": [
        "Lili Yang",
        "Mengshuai Chang",
        "Xiao Guo",
        "Yuxin Feng",
        "Yiwen Mei",
        "Caicong Wu"
      ],
      "abstract": "To address the issues of the existing frustum-based methods' underutilization\nof image information in road three-dimensional object detection as well as the\nlack of research on agricultural scenes, we constructed an object detection\ndataset using an 80-line Light Detection And Ranging (LiDAR) and a camera in a\ncomplex tractor road scene and proposed a new network called FrustumFusionNets\n(FFNets). Initially, we utilize the results of image-based two-dimensional\nobject detection to narrow down the search region in the three-dimensional\nspace of the point cloud. Next, we introduce a Gaussian mask to enhance the\npoint cloud information. Then, we extract the features from the frustum point\ncloud and the crop image using the point cloud feature extraction pipeline and\nthe image feature extraction pipeline, respectively. Finally, we concatenate\nand fuse the data features from both modalities to achieve three-dimensional\nobject detection. Experiments demonstrate that on the constructed test set of\ntractor road data, the FrustumFusionNetv2 achieves 82.28% and 95.68% accuracy\nin the three-dimensional object detection of the two main road objects, cars\nand people, respectively. This performance is 1.83% and 2.33% better than the\noriginal model. It offers a hybrid fusion-based multi-object, high-precision,\nreal-time three-dimensional object detection technique for unmanned\nagricultural machines in tractor road scenarios. On the Karlsruhe Institute of\nTechnology and Toyota Technological Institute (KITTI) Benchmark Suite\nvalidation set, the FrustumFusionNetv2 also demonstrates significant\nsuperiority in detecting road pedestrian objects compared with other\nfrustum-based three-dimensional object detection methods.",
      "tldr_zh": "该研究针对现有 frustum-based 方法在道路三维物体检测中未充分利用图像信息以及农业场景研究不足的问题，构建了一个基于 80 线 LiDAR 和摄像头的复杂拖拉机道路场景数据集，并提出了一种新网络 FrustumFusionNets (FFNets)。该方法首先利用图像-based 二维物体检测结果缩小点云三维空间搜索区域，然后引入 Gaussian mask 增强点云信息，并通过点云特征提取管道和图像特征提取管道提取特征，最后拼接和融合两种模态数据实现三维物体检测。实验结果显示，在自定义拖拉机道路测试集上，FrustumFusionNetv2 在检测汽车和行人的准确率分别达到 82.28% 和 95.68%，比原模型提高了 1.83% 和 2.33%；在 KITTI Benchmark Suite 验证集上，该网络在检测道路行人方面显著优于其他 frustum-based 方法，为无人农业机器提供高精度实时三维物体检测技术。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13951v1",
      "published_date": "2025-03-18 06:40:39 UTC",
      "updated_date": "2025-03-18 06:40:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:43:25.703059"
    },
    {
      "arxiv_id": "2503.15550v2",
      "title": "Zero-Knowledge Federated Learning: A New Trustworthy and Privacy-Preserving Distributed Learning Paradigm",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxin Jin",
        "Taotao Wang",
        "Qing Yang",
        "Long Shi",
        "Shengli Zhang"
      ],
      "abstract": "Federated Learning (FL) has emerged as a promising paradigm in distributed\nmachine learning, enabling collaborative model training while preserving data\nprivacy. However, despite its many advantages, FL still contends with\nsignificant challenges -- most notably regarding security and trust.\nZero-Knowledge Proofs (ZKPs) offer a potential solution by establishing trust\nand enhancing system integrity throughout the FL process. Although several\nstudies have explored ZKP-based FL (ZK-FL), a systematic framework and\ncomprehensive analysis are still lacking. This article makes two key\ncontributions. First, we propose a structured ZK-FL framework that categorizes\nand analyzes the technical roles of ZKPs across various FL stages and tasks.\nSecond, we introduce a novel algorithm, Verifiable Client Selection FL\n(Veri-CS-FL), which employs ZKPs to refine the client selection process. In\nVeri-CS-FL, participating clients generate verifiable proofs for the\nperformance metrics of their local models and submit these concise proofs to\nthe server for efficient verification. The server then selects clients with\nhigh-quality local models for uploading, subsequently aggregating the\ncontributions from these selected clients. By integrating ZKPs, Veri-CS-FL not\nonly ensures the accuracy of performance metrics but also fortifies trust among\nparticipants while enhancing the overall efficiency and security of FL systems.",
      "tldr_zh": "这项研究提出了一种新的联邦学习（Federated Learning, FL）范式，即基于零知识证明（Zero-Knowledge Proofs, ZKPs）的可信隐私保护分布式学习框架，以解决FL在安全和信任方面的挑战。该框架系统地分类和分析ZKPs在FL各个阶段和任务中的技术角色，包括模型训练和验证过程。同时，论文引入了一个创新算法Verifiable Client Selection FL (Veri-CS-FL)，通过ZKPs让客户端生成可验证的本地模型性能证明，供服务器高效验证和选择高质量客户端进行模型聚合。该方法显著提升了FL系统的效率、安全性和参与者信任，确保了隐私保护的同时提高了整体性能。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "7 pages, 5 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2503.15550v2",
      "published_date": "2025-03-18 06:21:08 UTC",
      "updated_date": "2025-03-24 03:55:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:43:37.015213"
    },
    {
      "arxiv_id": "2503.13938v2",
      "title": "ChatBEV: A Visual Language Model that Understands BEV Maps",
      "title_zh": "ChatBEV：一种理解 BEV",
      "authors": [
        "Qingyao Xu",
        "Siheng Chen",
        "Guang Chen",
        "Yanfeng Wang",
        "Ya Zhang"
      ],
      "abstract": "Traffic scene understanding is essential for intelligent transportation\nsystems and autonomous driving, ensuring safe and efficient vehicle operation.\nWhile recent advancements in VLMs have shown promise for holistic scene\nunderstanding, the application of VLMs to traffic scenarios, particularly using\nBEV maps, remains under explored. Existing methods often suffer from limited\ntask design and narrow data amount, hindering comprehensive scene\nunderstanding. To address these challenges, we introduce ChatBEV-QA, a novel\nBEV VQA benchmark contains over 137k questions, designed to encompass a wide\nrange of scene understanding tasks, including global scene understanding,\nvehicle-lane interactions, and vehicle-vehicle interactions. This benchmark is\nconstructed using an novel data collection pipeline that generates scalable and\ninformative VQA data for BEV maps. We further fine-tune a specialized\nvision-language model ChatBEV, enabling it to interpret diverse question\nprompts and extract relevant context-aware information from BEV maps.\nAdditionally, we propose a language-driven traffic scene generation pipeline,\nwhere ChatBEV facilitates map understanding and text-aligned navigation\nguidance, significantly enhancing the generation of realistic and consistent\ntraffic scenarios. The dataset, code and the fine-tuned model will be released.",
      "tldr_zh": "该论文针对交通场景理解的不足，引入ChatBEV-QA基准，该基准包含超过137k个问题，涵盖全局场景理解、车辆-车道交互和车辆-车辆交互，以解决现有视觉语言模型(VLMs)对BEV maps应用的任务设计和数据量限制。作者开发了一个新颖的数据收集管道来生成可扩展的VQA数据，并微调了专用视觉语言模型ChatBEV，使其能解释多样问题提示并从BEV maps中提取上下文信息。最终，他们提出一个语言驱动的交通场景生成管道，利用ChatBEV增强地图理解和导航指导，提高了场景生成的真实性和一致性，并计划发布数据集、代码和模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13938v2",
      "published_date": "2025-03-18 06:12:38 UTC",
      "updated_date": "2025-03-21 02:17:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:43:50.461979"
    },
    {
      "arxiv_id": "2503.17394v1",
      "title": "Temporal Flexibility in Spiking Neural Networks: Towards Generalization Across Time Steps and Deployment Friendliness",
      "title_zh": "翻译失败",
      "authors": [
        "Kangrui Du",
        "Yuhang Wu",
        "Shikuang Deng",
        "Shi Gu"
      ],
      "abstract": "Spiking Neural Networks (SNNs), models inspired by neural mechanisms in the\nbrain, allow for energy-efficient implementation on neuromorphic hardware.\nHowever, SNNs trained with current direct training approaches are constrained\nto a specific time step. This \"temporal inflexibility\" 1) hinders SNNs'\ndeployment on time-step-free fully event-driven chips and 2) prevents\nenergy-performance balance based on dynamic inference time steps. In this\nstudy, we first explore the feasibility of training SNNs that generalize across\ndifferent time steps. We then introduce Mixed Time-step Training (MTT), a novel\nmethod that improves the temporal flexibility of SNNs, making SNNs adaptive to\ndiverse temporal structures. During each iteration of MTT, random time steps\nare assigned to different SNN stages, with spikes transmitted between stages\nvia communication modules. After training, the weights are deployed and\nevaluated on both time-stepped and fully event-driven platforms. Experimental\nresults show that models trained by MTT gain remarkable temporal flexibility,\nfriendliness for both event-driven and clock-driven deployment (nearly lossless\non N-MNIST and 10.1% higher than standard methods on CIFAR10-DVS), enhanced\nnetwork generalization, and near SOTA performance. To the best of our\nknowledge, this is the first work to report the results of large-scale SNN\ndeployment on fully event-driven scenarios.",
      "tldr_zh": "这篇论文探讨了 Spiking Neural Networks (SNNs) 的 temporal flexibility 问题，旨在解决 SNNs 在不同时间步泛化不足的问题，从而支持无时间步的全事件驱动芯片部署和动态能量性能平衡。作者引入了 Mixed Time-step Training (MTT) 方法，该方法在训练过程中为 SNNs 的不同阶段分配随机时间步，并通过通信模块传输 spikes，以提升模型对多样时间结构的适应性。实验结果显示，MTT 训练的模型在 N-MNIST 上几乎无损表现，在 CIFAR10-DVS 上比标准方法高 10.1%，并实现了增强的网络泛化和接近 SOTA 性能，这是首例大规模 SNN 在全事件驱动场景的部署研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.17394v1",
      "published_date": "2025-03-18 06:09:42 UTC",
      "updated_date": "2025-03-18 06:09:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:44:02.149106"
    },
    {
      "arxiv_id": "2503.13934v1",
      "title": "COLSON: Controllable Learning-Based Social Navigation via Diffusion-Based Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yuki Tomita",
        "Kohei Matsumoto",
        "Yuki Hyodo",
        "Ryo Kurazume"
      ],
      "abstract": "Mobile robot navigation in dynamic environments with pedestrian traffic is a\nkey challenge in the development of autonomous mobile service robots. Recently,\ndeep reinforcement learning-based methods have been actively studied and have\noutperformed traditional rule-based approaches owing to their optimization\ncapabilities. Among these, methods that assume a continuous action space\ntypically rely on a Gaussian distribution assumption, which limits the\nflexibility of generated actions. Meanwhile, the application of diffusion\nmodels to reinforcement learning has advanced, allowing for more flexible\naction distributions compared with Gaussian distribution-based approaches. In\nthis study, we applied a diffusion-based reinforcement learning approach to\nsocial navigation and validated its effectiveness. Furthermore, by leveraging\nthe characteristics of diffusion models, we propose an extension that enables\npost-training action smoothing and adaptation to static obstacle scenarios not\nconsidered during the training steps.",
      "tldr_zh": "本研究提出COLSON框架，通过diffusion-based reinforcement learning实现可控的学习-based社会导航，解决了传统高斯分布假设下动作灵活性不足的问题。相比于规则-based方法，该方法在动态环境中优化了机器人导航性能，并验证了其有效性。此外，利用diffusion模型的特点，COLSON支持训练后的动作平滑和适应未考虑的静态障碍场景，从而提升了导航的鲁棒性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "This work has been submitted to IROS 2025 for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2503.13934v1",
      "published_date": "2025-03-18 06:02:30 UTC",
      "updated_date": "2025-03-18 06:02:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:44:12.164935"
    },
    {
      "arxiv_id": "2503.13923v1",
      "title": "ConSCompF: Consistency-focused Similarity Comparison Framework for Generative Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Alexey Karev",
        "Dong Xu"
      ],
      "abstract": "Large language models (LLMs) have been one of the most important discoveries\nin machine learning in recent years. LLM-based artificial intelligence (AI)\nassistants, such as ChatGPT, have consistently attracted the attention from\nresearchers, investors, and the general public, driving the rapid growth of\nthis industry. With the frequent introduction of new LLMs to the market, it\nbecomes increasingly difficult to differentiate between them, creating a demand\nfor new LLM comparison methods.\n  In this research, the Consistency-focused Similarity Comparison Framework\n(ConSCompF) for generative large language models is proposed. It compares texts\ngenerated by two LLMs and produces a similarity score, indicating the overall\ndegree of similarity between their responses. The main advantage of this\nframework is that it can operate on a small number of unlabeled data, such as\nchatbot instruction prompts, and does not require LLM developers to disclose\nany information about their product.\n  To evaluate the efficacy of ConSCompF, two experiments aimed at identifying\nsimilarities between multiple LLMs are conducted. Additionally, these\nexperiments examine the correlation between the similarity scores generated by\nConSCompF and the differences in the outputs produced by other benchmarking\ntechniques, such as ROUGE-L. Finally, a series of few-shot LLM comparison\nexperiments is conducted to evaluate the performance of ConSCompF in a few-shot\nLLM comparison scenario.\n  The proposed framework can be used for calculating similarity matrices of\nmultiple LLMs, which can be effectively visualized using principal component\nanalysis (PCA). The ConSCompF output may provide useful insights into data that\nmight have been used during LLM training and help detect possible investment\nfraud attempts.",
      "tldr_zh": "本研究提出了一种名为ConSCompF的Consistency-focused Similarity Comparison Framework，用于比较生成式Large Language Models (LLMs)的输出相似度。该框架通过分析LLMs生成的文本计算相似分数，仅需少量无标签数据（如聊天指令提示），且不依赖开发者披露信息，从而解决了LLM比较的难题。实验包括比较多个LLMs的相似性、验证ConSCompF分数与ROUGE-L等基准的相关性，以及少样本场景测试，结果显示框架有效且能生成可视化相似矩阵（使用PCA）。ConSCompF的应用可提供LLM训练数据洞察，并帮助检测潜在的投资欺诈。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13923v1",
      "published_date": "2025-03-18 05:38:04 UTC",
      "updated_date": "2025-03-18 05:38:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:44:24.436474"
    },
    {
      "arxiv_id": "2503.13921v1",
      "title": "Learning Accurate Models on Incomplete Data with Minimal Imputation",
      "title_zh": "翻译失败",
      "authors": [
        "Cheng Zhen",
        "Nischal Aryal",
        "Arash Termehchy",
        "Prayoga",
        "Garrett Biwer",
        "Sankalp Patil"
      ],
      "abstract": "Missing data often exists in real-world datasets, requiring significant time\nand effort for imputation to learn accurate machine learning (ML) models. In\nthis paper, we demonstrate that imputing all missing values is not always\nnecessary to achieve an accurate ML model. We introduce the concept of minimal\ndata imputation, which ensures accurate ML models trained over the imputed\ndataset. Implementing minimal imputation guarantees both minimal imputation\neffort and optimal ML models. We propose algorithms to find exact and\napproximate minimal imputation for various ML models. Our extensive experiments\nindicate that our proposed algorithms significantly reduce the time and effort\nrequired for data imputation.",
      "tldr_zh": "本论文探讨了现实数据集中的缺失数据问题，指出无需填充所有缺失值即可训练出准确的机器学习（ML）模型。作者引入了 minimal data imputation 的概念，确保在最小化填充努力的前提下获得最优 ML 模型，并提出了算法来实现精确和近似的 minimal imputation。实验结果显示，这些算法显著降低了数据填充所需的时间和努力，同时保持了模型的准确性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13921v1",
      "published_date": "2025-03-18 05:36:59 UTC",
      "updated_date": "2025-03-18 05:36:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:44:36.612157"
    },
    {
      "arxiv_id": "2503.13916v1",
      "title": "Learning Bimanual Manipulation via Action Chunking and Inter-Arm Coordination with Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Tomohiro Motoda",
        "Ryo Hanai",
        "Ryoichi Nakajo",
        "Masaki Murooka",
        "Floris Erich",
        "Yukiyasu Domae"
      ],
      "abstract": "Robots that can operate autonomously in a human living environment are\nnecessary to have the ability to handle various tasks flexibly. One crucial\nelement is coordinated bimanual movements that enable functions that are\ndifficult to perform with one hand alone. In recent years, learning-based\nmodels that focus on the possibilities of bimanual movements have been\nproposed. However, the high degree of freedom of the robot makes it challenging\nto reason about control, and the left and right robot arms need to adjust their\nactions depending on the situation, making it difficult to realize more\ndexterous tasks. To address the issue, we focus on coordination and efficiency\nbetween both arms, particularly for synchronized actions. Therefore, we propose\na novel imitation learning architecture that predicts cooperative actions. We\ndifferentiate the architecture for both arms and add an intermediate encoder\nlayer, Inter-Arm Coordinated transformer Encoder (IACE), that facilitates\nsynchronization and temporal alignment to ensure smooth and coordinated\nactions. To verify the effectiveness of our architectures, we perform\ndistinctive bimanual tasks. The experimental results showed that our model\ndemonstrated a high success rate for comparison and suggested a suitable\narchitecture for the policy learning of bimanual manipulation.",
      "tldr_zh": "该研究针对机器人双臂操纵的协调性和控制挑战，提出了一种基于模仿学习的架构，利用行动分块和 Transformer 技术来预测合作动作。架构中引入 Inter-Arm Coordinated transformer Encoder (IACE) 层，以促进双臂之间的同步和时间对齐，确保动作流畅高效。实验结果显示，该模型在各种双臂任务中表现出高成功率，证明了其在双臂操纵政策学习中的适用性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, 5 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2503.13916v1",
      "published_date": "2025-03-18 05:20:34 UTC",
      "updated_date": "2025-03-18 05:20:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:44:48.305953"
    },
    {
      "arxiv_id": "2503.13915v2",
      "title": "Unlocking the Potential of Unlabeled Data in Semi-Supervised Domain Generalization",
      "title_zh": "在半监督域泛化中解锁无标签数据的潜力",
      "authors": [
        "Dongkwan Lee",
        "Kyomin Hwang",
        "Nojun Kwak"
      ],
      "abstract": "We address the problem of semi-supervised domain generalization (SSDG), where\nthe distributions of train and test data differ, and only a small amount of\nlabeled data along with a larger amount of unlabeled data are available during\ntraining. Existing SSDG methods that leverage only the unlabeled samples for\nwhich the model's predictions are highly confident (confident-unlabeled\nsamples), limit the full utilization of the available unlabeled data. To the\nbest of our knowledge, we are the first to explore a method for incorporating\nthe unconfident-unlabeled samples that were previously disregarded in SSDG\nsetting. To this end, we propose UPCSC to utilize these unconfident-unlabeled\nsamples in SSDG that consists of two modules: 1) Unlabeled Proxy-based\nContrastive learning (UPC) module, treating unconfident-unlabeled samples as\nadditional negative pairs and 2) Surrogate Class learning (SC) module,\ngenerating positive pairs for unconfident-unlabeled samples using their\nconfusing class set. These modules are plug-and-play and do not require any\ndomain labels, which can be easily integrated into existing approaches.\nExperiments on four widely used SSDG benchmarks demonstrate that our approach\nconsistently improves performance when attached to baselines and outperforms\ncompeting plug-and-play methods. We also analyze the role of our method in\nSSDG, showing that it enhances class-level discriminability and mitigates\ndomain gaps. The code is available at https://github.com/dongkwani/UPCSC.",
      "tldr_zh": "该论文解决了半监督域泛化（SSDG）问题，即训练数据和测试数据分布不同，且仅提供少量标注数据和大量未标注数据。现有方法仅利用模型预测高度自信的未标注样本（confident-unlabeled samples），而本文首次探索利用不自信的未标注样本（unconfident-unlabeled samples），提出UPCSC方法，包括Unlabeled Proxy-based Contrastive learning (UPC)模块（将不自信样本视为额外负对）和Surrogate Class learning (SC)模块（为不自信样本生成正对）。实验在四个SSDG基准上显示，UPCSC可即插即用地提升基线模型性能，并优于其他方法，同时增强类级可区分性和缓解域间差距。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.13915v2",
      "published_date": "2025-03-18 05:19:33 UTC",
      "updated_date": "2025-04-27 08:32:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:45:02.744120"
    },
    {
      "arxiv_id": "2503.13912v1",
      "title": "KANITE: Kolmogorov-Arnold Networks for ITE estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Eshan Mehendale",
        "Abhinav Thorat",
        "Ravi Kolla",
        "Niranjan Pedanekar"
      ],
      "abstract": "We introduce KANITE, a framework leveraging Kolmogorov-Arnold Networks (KANs)\nfor Individual Treatment Effect (ITE) estimation under multiple treatments\nsetting in causal inference. By utilizing KAN's unique abilities to learn\nunivariate activation functions as opposed to learning linear weights by\nMulti-Layer Perceptrons (MLPs), we improve the estimates of ITEs. The KANITE\nframework comprises two key architectures: 1.Integral Probability Metric (IPM)\narchitecture: This employs an IPM loss in a specialized manner to effectively\nalign towards ITE estimation across multiple treatments. 2. Entropy Balancing\n(EB) architecture: This uses weights for samples that are learned by optimizing\nentropy subject to balancing the covariates across treatment groups. Extensive\nevaluations on benchmark datasets demonstrate that KANITE outperforms\nstate-of-the-art algorithms in both $\\epsilon_{\\text{PEHE}}$ and\n$\\epsilon_{\\text{ATE}}$ metrics. Our experiments highlight the advantages of\nKANITE in achieving improved causal estimates, emphasizing the potential of\nKANs to advance causal inference methodologies across diverse application\nareas.",
      "tldr_zh": "本研究引入了 KANITE 框架，利用 Kolmogorov-Arnold Networks (KANs) 来估计因果推断中多治疗设置下的 Individual Treatment Effect (ITE)，通过学习单变量激活函数而非传统 Multi-Layer Perceptrons (MLPs) 的线性权重，从而提升估计准确性。框架包括两个关键架构：Integral Probability Metric (IPM) 架构，使用 IPM 损失来对齐多治疗下的 ITE 估计；以及 Entropy Balancing (EB) 架构，通过优化熵来学习样本权重，以平衡治疗组间的协变量。在基准数据集上的实验显示，KANITE 在 ε_PEHE 和 ε_ATE 指标上优于现有最先进算法，突显了 KANs 在推进因果推断方法的应用潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.13912v1",
      "published_date": "2025-03-18 05:16:36 UTC",
      "updated_date": "2025-03-18 05:16:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:45:13.466128"
    },
    {
      "arxiv_id": "2503.13906v1",
      "title": "HSOD-BIT-V2: A New Challenging Benchmarkfor Hyperspectral Salient Object Detection",
      "title_zh": "HSOD-BIT-V2：一个新的具有挑战性的高光谱显著物体检测基准",
      "authors": [
        "Yuhao Qiu",
        "Shuyan Bai",
        "Tingfa Xu",
        "Peifu Liu",
        "Haolin Qin",
        "Jianan Li"
      ],
      "abstract": "Salient Object Detection (SOD) is crucial in computer vision, yet RGB-based\nmethods face limitations in challenging scenes, such as small objects and\nsimilar color features. Hyperspectral images provide a promising solution for\nmore accurate Hyperspectral Salient Object Detection (HSOD) by abundant\nspectral information, while HSOD methods are hindered by the lack of extensive\nand available datasets. In this context, we introduce HSOD-BIT-V2, the largest\nand most challenging HSOD benchmark dataset to date. Five distinct challenges\nfocusing on small objects and foreground-background similarity are designed to\nemphasize spectral advantages and real-world complexity. To tackle these\nchallenges, we propose Hyper-HRNet, a high-resolution HSOD network. Hyper-HRNet\neffectively extracts, integrates, and preserves effective spectral information\nwhile reducing dimensionality by capturing the self-similar spectral features.\nAdditionally, it conveys fine details and precisely locates object contours by\nincorporating comprehensive global information and detailed object saliency\nrepresentations. Experimental analysis demonstrates that Hyper-HRNet\noutperforms existing models, especially in challenging scenarios.",
      "tldr_zh": "该研究介绍了 HSOD-BIT-V2，这是一个迄今为止最大且最具挑战性的 Hyperspectral Salient Object Detection (HSOD) 基准数据集，专注于小物体和前景-背景相似性等五种挑战，以突出光谱信息在真实世界复杂场景中的优势。针对这些挑战，研究者提出了一种高分辨率网络 Hyper-HRNet，该模型通过提取自相似光谱特征来有效整合和保留光谱信息，同时减少维度并精确定位物体轮廓。实验结果显示，Hyper-HRNet 在挑战场景中显著优于现有模型，为 HSOD 领域提供了更可靠的基准和方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.13906v1",
      "published_date": "2025-03-18 05:09:42 UTC",
      "updated_date": "2025-03-18 05:09:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:45:24.824135"
    },
    {
      "arxiv_id": "2503.13903v2",
      "title": "TGBFormer: Transformer-GraphFormer Blender Network for Video Object Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Qiang Qi",
        "Xiao Wang"
      ],
      "abstract": "Video object detection has made significant progress in recent years thanks\nto convolutional neural networks (CNNs) and vision transformers (ViTs).\nTypically, CNNs excel at capturing local features but struggle to model global\nrepresentations. Conversely, ViTs are adept at capturing long-range global\nfeatures but face challenges in representing local feature details.\nOff-the-shelf video object detection methods solely rely on CNNs or ViTs to\nconduct feature aggregation, which hampers their capability to simultaneously\nleverage global and local information, thereby resulting in limited detection\nperformance. In this paper, we propose a Transformer-GraphFormer Blender\nNetwork (TGBFormer) for video object detection, with three key technical\nimprovements to fully exploit the advantages of transformers and graph\nconvolutional networks while compensating for their limitations. First, we\ndevelop a spatial-temporal transformer module to aggregate global contextual\ninformation, constituting global representations with long-range feature\ndependencies. Second, we introduce a spatial-temporal GraphFormer module that\nutilizes local spatial and temporal relationships to aggregate features,\ngenerating new local representations that are complementary to the transformer\noutputs. Third, we design a global-local feature blender module to adaptively\ncouple transformer-based global representations and GraphFormer-based local\nrepresentations. Extensive experiments demonstrate that our TGBFormer\nestablishes new state-of-the-art results on the ImageNet VID dataset.\nParticularly, our TGBFormer achieves 86.5% mAP while running at around 41.0 FPS\non a single Tesla A100 GPU.",
      "tldr_zh": "本研究针对视频物体检测中的局限性，指出CNNs擅长局部特征但弱于全局表示，而ViTs则相反，现有的方法无法同时利用两者。该论文提出TGBFormer框架，包括空间-时间Transformer模块聚合全局上下文、空间-时间GraphFormer模块生成补充局部表示，以及全局-局部特征混合模块自适应结合这些特征，从而提升检测性能。实验结果显示，TGBFormer在ImageNet VID数据集上达到86.5% mAP的新SOTA水平，同时以约41.0 FPS的速度运行于单Tesla A100 GPU。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI2025",
      "pdf_url": "http://arxiv.org/pdf/2503.13903v2",
      "published_date": "2025-03-18 05:03:05 UTC",
      "updated_date": "2025-05-10 04:01:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:45:37.071833"
    },
    {
      "arxiv_id": "2503.13882v1",
      "title": "MoK-RAG: Mixture of Knowledge Paths Enhanced Retrieval-Augmented Generation for Embodied AI Environments",
      "title_zh": "MoK-RAG：知识路径混合增强",
      "authors": [
        "Zhengsheng Guo",
        "Linwei Zheng",
        "Xinyang Chen",
        "Xuefeng Bai",
        "Kehai Chen",
        "Min Zhang"
      ],
      "abstract": "While human cognition inherently retrieves information from diverse and\nspecialized knowledge sources during decision-making processes, current\nRetrieval-Augmented Generation (RAG) systems typically operate through\nsingle-source knowledge retrieval, leading to a cognitive-algorithmic\ndiscrepancy. To bridge this gap, we introduce MoK-RAG, a novel multi-source RAG\nframework that implements a mixture of knowledge paths enhanced retrieval\nmechanism through functional partitioning of a large language model (LLM)\ncorpus into distinct sections, enabling retrieval from multiple specialized\nknowledge paths. Applied to the generation of 3D simulated environments, our\nproposed MoK-RAG3D enhances this paradigm by partitioning 3D assets into\ndistinct sections and organizing them based on a hierarchical knowledge tree\nstructure. Different from previous methods that only use manual evaluation, we\npioneered the introduction of automated evaluation methods for 3D scenes. Both\nautomatic and human evaluations in our experiments demonstrate that MoK-RAG3D\ncan assist Embodied AI agents in generating diverse scenes.",
      "tldr_zh": "本研究提出 MoK-RAG，一种多源知识路径增强的 Retrieval-Augmented Generation (RAG) 框架，通过将大型语言模型语料库分区为不同部分，实现从多个专业知识路径的检索，以弥合人类认知与算法间的差距。针对 Embodied AI 环境，MoK-RAG3D 进一步将 3D 资产分区并组织成层次知识树结构，用于生成模拟环境。该框架首次引入自动评估方法，实验结果显示，MoK-RAG3D 在自动和人类评估中均表现出色，能帮助 Embodied AI 代理生成更多样化的 3D 场景。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13882v1",
      "published_date": "2025-03-18 04:27:02 UTC",
      "updated_date": "2025-03-18 04:27:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:45:50.742486"
    },
    {
      "arxiv_id": "2503.13879v2",
      "title": "Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review Generation via Cognitive Alignment",
      "title_zh": "桥接社会心理学与LLM推理：通过认知对齐的冲突感知元评论生成",
      "authors": [
        "Wei Chen",
        "Han Ding",
        "Meng Yuan",
        "Zhao Zhang",
        "Deqing Wang",
        "Fuzhen Zhuang"
      ],
      "abstract": "The rapid growth of scholarly submissions has overwhelmed traditional peer\nreview systems, driving the need for intelligent automation to preserve\nscientific rigor. While large language models (LLMs) show promise in automating\nmanuscript critiques, their ability to synthesize high-stakes meta-reviews,\nwhich require conflict-aware reasoning and consensus derivation, remains\nunderdeveloped. Existing methods fail to effectively handle conflicting\nviewpoints within differing opinions, and often introduce additional cognitive\nbiases, such as anchoring effects and conformity bias.To overcome these\nlimitations, we propose the Cognitive Alignment Framework (CAF), a dual-process\narchitecture that transforms LLMs into adaptive scientific arbitrators. By\noperationalizing Kahneman's dual-process theory, CAF introduces a three-step\ncognitive pipeline: review initialization, incremental integration, and\ncognitive alignment.Empirical validation shows that CAF outperforms existing\nLLM-based methods, with sentiment consistency gains reaching up to 19.47\\% and\ncontent consistency improving by as much as 12.95\\%.",
      "tldr_zh": "学术论文评审系统因提交量激增而面临挑战，现有的LLMs在生成冲突感知的meta-reviews时，难以处理冲突观点并易引入认知偏差，如锚定效应和从众偏差。为此，本文提出Cognitive Alignment Framework (CAF)，一个基于Kahneman's dual-process theory的双重过程架构，包含review initialization、incremental integration和cognitive alignment三个步骤，以将LLMs转化为适应性的科学仲裁者。实验结果显示，CAF相较现有方法提升了情感一致性高达19.47%和内容一致性高达12.95%，从而桥接了社会心理学与LLM推理，促进更可靠的元评论生成。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.13879v2",
      "published_date": "2025-03-18 04:13:11 UTC",
      "updated_date": "2025-03-21 07:36:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:46:01.838223"
    },
    {
      "arxiv_id": "2503.14559v1",
      "title": "Squeeze Out Tokens from Sample for Finer-Grained Data Governance",
      "title_zh": "翻译失败",
      "authors": [
        "Weixiong Lin",
        "Chen Ju",
        "Haicheng Wang",
        "Shengchao Hu",
        "Shuai Xiao",
        "Mengting Chen",
        "Yuheng Jiao",
        "Mingshuai Yao",
        "Jinsong Lan",
        "Qingwen Liu",
        "Ying Chen"
      ],
      "abstract": "Widely observed data scaling laws, in which error falls off as a power of the\ntraining size, demonstrate the diminishing returns of unselective data\nexpansion. Hence, data governance is proposed to downsize datasets through\npruning non-informative samples. Yet, isolating the impact of a specific sample\non overall model performance is challenging, due to the vast computation\nrequired for tryout all sample combinations. Current data governors circumvent\nthis complexity by estimating sample contributions through heuristic-derived\nscalar scores, thereby discarding low-value ones. Despite thorough sample\nsieving, retained samples contain substantial undesired tokens intrinsically,\nunderscoring the potential for further compression and purification. In this\nwork, we upgrade data governance from a 'sieving' approach to a 'juicing' one.\nInstead of scanning for least-flawed samples, our dual-branch DataJuicer\napplies finer-grained intra-sample governance. It squeezes out informative\ntokens and boosts image-text alignments. Specifically, the vision branch\nretains salient image patches and extracts relevant object classes, while the\ntext branch incorporates these classes to enhance captions. Consequently,\nDataJuicer yields more refined datasets through finer-grained governance.\nExtensive experiments across datasets demonstrate that DataJuicer significantly\noutperforms existing DataSieve in image-text retrieval, classification, and\ndense visual reasoning.",
      "tldr_zh": "该论文探讨了数据规模定律（data scaling laws）导致的无选择数据扩展收益递减问题，提出通过数据治理（data governance）修剪非信息样本以优化数据集。作者升级了治理方法，从传统的“筛分”（sieving）样本转向更细粒度的“榨取”（juicing）策略，开发了双分支 DataJuicer 框架：视觉分支保留显著图像补丁并提取相关对象类，文本分支则利用这些类增强标题，从而提升图像-文本对齐。实验结果显示，DataJuicer 在图像-文本检索、分类和密集视觉推理任务上显著优于现有的 DataSieve 方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14559v1",
      "published_date": "2025-03-18 04:06:50 UTC",
      "updated_date": "2025-03-18 04:06:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:46:15.394935"
    },
    {
      "arxiv_id": "2503.13868v2",
      "title": "Out-of-Distribution Generalization in Time Series: A Survey",
      "title_zh": "时间序列中的分布外泛化：一个综述",
      "authors": [
        "Xin Wu",
        "Fei Teng",
        "Xingwang Li",
        "Ji Zhang",
        "Tianrui Li",
        "Qiang Duan"
      ],
      "abstract": "Time series frequently manifest distribution shifts, diverse latent features,\nand non-stationary learning dynamics, particularly in open and evolving\nenvironments. These characteristics pose significant challenges for\nout-of-distribution (OOD) generalization. While substantial progress has been\nmade, a systematic synthesis of advancements remains lacking. To address this\ngap, we present the first comprehensive review of OOD generalization\nmethodologies for time series, organized to delineate the field's evolutionary\ntrajectory and contemporary research landscape. We organize our analysis across\nthree foundational dimensions: data distribution, representation learning, and\nOOD evaluation. For each dimension, we present several popular algorithms in\ndetail. Furthermore, we highlight key application scenarios, emphasizing their\nreal-world impact. Finally, we identify persistent challenges and propose\nfuture research directions. A detailed summary of the methods reviewed for the\ngeneralization of OOD in time series can be accessed at\nhttps://tsood-generalization.com.",
      "tldr_zh": "这篇论文对时间序列中的 Out-of-Distribution (OOD) 泛化进行了首次全面调查，系统总结了这一领域的发展轨迹和当代研究现状。论文将分析组织为数据分布、表示学习和 OOD 评估三个核心维度，并详细介绍了多种流行算法，同时强调了其在实际应用场景中的影响。最终，论文识别了持续存在的挑战，并提出了未来研究方向，提供了一个详细方法的在线总结（如 https://tsood-generalization.com）。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Work in Progress",
      "pdf_url": "http://arxiv.org/pdf/2503.13868v2",
      "published_date": "2025-03-18 03:35:29 UTC",
      "updated_date": "2025-04-07 03:45:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:46:24.305509"
    },
    {
      "arxiv_id": "2503.13861v1",
      "title": "RAD: Retrieval-Augmented Decision-Making of Meta-Actions with Vision-Language Models in Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Yujin Wang",
        "Quanfeng Liu",
        "Zhengxin Jiang",
        "Tianyi Wang",
        "Junfeng Jiao",
        "Hongqing Chu",
        "Bingzhao Gao",
        "Hong Chen"
      ],
      "abstract": "Accurately understanding and deciding high-level meta-actions is essential\nfor ensuring reliable and safe autonomous driving systems. While\nvision-language models (VLMs) have shown significant potential in various\nautonomous driving tasks, they often suffer from limitations such as inadequate\nspatial perception and hallucination, reducing their effectiveness in complex\nautonomous driving scenarios. To address these challenges, we propose a\nretrieval-augmented decision-making (RAD) framework, a novel architecture\ndesigned to enhance VLMs' capabilities to reliably generate meta-actions in\nautonomous driving scenes. RAD leverages a retrieval-augmented generation (RAG)\npipeline to dynamically improve decision accuracy through a three-stage process\nconsisting of the embedding flow, retrieving flow, and generating flow.\nAdditionally, we fine-tune VLMs on a specifically curated dataset derived from\nthe NuScenes dataset to enhance their spatial perception and bird's-eye view\nimage comprehension capabilities. Extensive experimental evaluations on the\ncurated NuScenes-based dataset demonstrate that RAD outperforms baseline\nmethods across key evaluation metrics, including match accuracy, and F1 score,\nand self-defined overall score, highlighting its effectiveness in improving\nmeta-action decision-making for autonomous driving tasks.",
      "tldr_zh": "该研究针对Vision-Language Models (VLMs)在自动驾驶中的空间感知不足和幻觉问题，提出RAD框架，以提升元动作(meta-actions)的可靠决策。RAD采用检索增强生成(RAG)管道，包括embedding flow、retrieving flow和generating flow三个阶段，动态优化决策过程，并通过微调VLMs来增强其对NuScenes数据集衍生的空间感知和鸟瞰视图图像理解能力。实验结果显示，RAD在关键指标如匹配准确率、F1分数和整体分数上优于基线方法，证明了其在复杂自动驾驶任务中的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13861v1",
      "published_date": "2025-03-18 03:25:57 UTC",
      "updated_date": "2025-03-18 03:25:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:46:38.127286"
    },
    {
      "arxiv_id": "2503.16527v1",
      "title": "LLM Generated Persona is a Promise with a Catch",
      "title_zh": "翻译失败",
      "authors": [
        "Ang Li",
        "Haozhe Chen",
        "Hongseok Namkoong",
        "Tianyi Peng"
      ],
      "abstract": "The use of large language models (LLMs) to simulate human behavior has gained\nsignificant attention, particularly through personas that approximate\nindividual characteristics. Persona-based simulations hold promise for\ntransforming disciplines that rely on population-level feedback, including\nsocial science, economic analysis, marketing research, and business operations.\nTraditional methods to collect realistic persona data face significant\nchallenges. They are prohibitively expensive and logistically challenging due\nto privacy constraints, and often fail to capture multi-dimensional attributes,\nparticularly subjective qualities. Consequently, synthetic persona generation\nwith LLMs offers a scalable, cost-effective alternative. However, current\napproaches rely on ad hoc and heuristic generation techniques that do not\nguarantee methodological rigor or simulation precision, resulting in systematic\nbiases in downstream tasks. Through extensive large-scale experiments including\npresidential election forecasts and general opinion surveys of the U.S.\npopulation, we reveal that these biases can lead to significant deviations from\nreal-world outcomes. Our findings underscore the need to develop a rigorous\nscience of persona generation and outline the methodological innovations,\norganizational and institutional support, and empirical foundations required to\nenhance the reliability and scalability of LLM-driven persona simulations. To\nsupport further research and development in this area, we have open-sourced\napproximately one million generated personas, available for public access and\nanalysis at https://huggingface.co/datasets/Tianyi-Lab/Personas.",
      "tldr_zh": "该研究探讨了使用大型语言模型 (LLMs) 生成模拟人类行为的角色 (personas) 的潜力及其局限性，这些角色可应用于社会科学、经济分析、市场研究和商业运营等领域，以替代昂贵且受限的传统数据收集方法。然而，当前基于启发式技术的生成方法存在系统偏差，导致下游任务结果与真实世界出现显著偏差。通过大规模实验，包括美国总统选举预测和公众意见调查，论文揭示了这些偏差的严重性，并呼吁建立严谨的角色生成科学，包括方法创新、机构支持和实证基础。为促进进一步研究，该团队开源了约一百万个生成的 personas 数据集。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16527v1",
      "published_date": "2025-03-18 03:11:27 UTC",
      "updated_date": "2025-03-18 03:11:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:46:49.627409"
    },
    {
      "arxiv_id": "2503.13856v1",
      "title": "MDTeamGPT: A Self-Evolving LLM-based Multi-Agent Framework for Multi-Disciplinary Team Medical Consultation",
      "title_zh": "翻译失败",
      "authors": [
        "Kai Chen",
        "Xinfeng Li",
        "Tianpei Yang",
        "Hewei Wang",
        "Wei Dong",
        "Yang Gao"
      ],
      "abstract": "Large Language Models (LLMs) have made significant progress in various\nfields. However, challenges remain in Multi-Disciplinary Team (MDT) medical\nconsultations. Current research enhances reasoning through role assignment,\ntask decomposition, and accumulation of medical experience. Multi-role\ncollaboration in MDT consultations often results in excessively long dialogue\nhistories. This increases the model's cognitive burden and degrades both\nefficiency and accuracy. Some methods only store treatment histories. They do\nnot extract effective experience or reflect on errors. This limits knowledge\ngeneralization and system evolution. We propose a multi-agent MDT medical\nconsultation framework based on LLMs to address these issues. Our framework\nuses consensus aggregation and a residual discussion structure for multi-round\nconsultations. It also employs a Correct Answer Knowledge Base (CorrectKB) and\na Chain-of-Thought Knowledge Base (ChainKB) to accumulate consultation\nexperience. These mechanisms enable the framework to evolve and continually\nimprove diagnosis rationality and accuracy. Experimental results on the MedQA\nand PubMedQA datasets demonstrate that our framework achieves accuracies of\n90.1% and 83.9%, respectively, and that the constructed knowledge bases\ngeneralize effectively across test sets from both datasets.",
      "tldr_zh": "该研究提出MDTeamGPT，一种基于LLMs的自演化多智能体框架，用于解决多学科团队(MDT)医疗咨询中的对话历史过长、认知负担增加以及经验积累不足等问题。该框架采用共识聚合、剩余讨论结构，以及Correct Answer Knowledge Base (CorrectKB)和Chain-of-Thought Knowledge Base (ChainKB)来积累咨询经验，实现系统的持续演化和诊断准确性提升。在MedQA和PubMedQA数据集上的实验显示，该框架分别达到90.1%和83.9%的准确率，并证明了知识库的有效泛化能力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.13856v1",
      "published_date": "2025-03-18 03:07:34 UTC",
      "updated_date": "2025-03-18 03:07:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:47:01.988242"
    },
    {
      "arxiv_id": "2503.13847v1",
      "title": "Disentangling Fine-Tuning from Pre-Training in Visual Captioning with Hybrid Markov Logic",
      "title_zh": "翻译失败",
      "authors": [
        "Monika Shah",
        "Somdeb Sarkhel",
        "Deepak Venugopal"
      ],
      "abstract": "Multimodal systems have highly complex processing pipelines and are\npretrained over large datasets before being fine-tuned for specific tasks such\nas visual captioning. However, it becomes hard to disentangle what the model\nlearns during the fine-tuning process from what it already knows due to its\npretraining. In this work, we learn a probabilistic model using Hybrid Markov\nLogic Networks (HMLNs) over the training examples by relating symbolic\nknowledge (extracted from the caption) with visual features (extracted from the\nimage). For a generated caption, we quantify the influence of training examples\nbased on the HMLN distribution using probabilistic inference. We evaluate two\ntypes of inference procedures on the MSCOCO dataset for different types of\ncaptioning models. Our results show that for BLIP2 (a model that uses a LLM),\nthe fine-tuning may have smaller influence on the knowledge the model has\nacquired since it may have more general knowledge to perform visual captioning\nas compared to models that do not use a LLM",
      "tldr_zh": "这篇论文探讨了视觉字幕(Visual Captioning)中区分微调(Fine-Tuning)和预训练(Pre-Training)的挑战，提出使用 Hybrid Markov Logic Networks (HMLNs) 构建一个概率模型，将从字幕提取的符号知识与从图像提取的视觉特征相关联。研究通过概率推理量化训练示例对生成字幕的影响，并在 MSCOCO 数据集上评估不同类型模型的推理程序。结果显示，使用 LLM 的模型如 BLIP2 在微调过程中依赖较少的特定知识，因为它已具备更通用的知识基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "2024 IEEE International Conference on Big Data (BigData), 10 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.13847v1",
      "published_date": "2025-03-18 02:39:26 UTC",
      "updated_date": "2025-03-18 02:39:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:47:14.652070"
    },
    {
      "arxiv_id": "2503.13844v1",
      "title": "Spotting Persuasion: A Low-cost Model for Persuasion Detection in Political Ads on Social Media",
      "title_zh": "翻译失败",
      "authors": [
        "Elyas Meguellati",
        "Stefano Civelli",
        "Pietro Bernardelle",
        "Shazia Sadiq",
        "Gianluca Demartini"
      ],
      "abstract": "In the realm of political advertising, persuasion operates as a pivotal\nelement within the broader framework of propaganda, exerting profound\ninfluences on public opinion and electoral outcomes. In this paper, we (1)\nintroduce a lightweight model for persuasive text detection that achieves\nstate-of-the-art performance in Subtask 3 of SemEval 2023 Task 3, while\nsignificantly reducing the computational resource requirements; and (2)\nleverage the proposed model to gain insights into political campaigning\nstrategies on social media platforms by applying it to a real-world dataset we\ncurated, consisting of Facebook political ads from the 2022 Australian Federal\nelection campaign. Our study shows how subtleties can be found in persuasive\npolitical advertisements and presents a pragmatic approach to detect and\nanalyze such strategies with limited resources, enhancing transparency in\nsocial media political campaigns.",
      "tldr_zh": "本研究聚焦于社交媒体政治广告中的说服性检测，提出了一种轻量级模型，用于识别说服性文本。该模型在 SemEval 2023 Task 3 子任务中实现了 state-of-the-art 性能，同时显著降低了计算资源需求。通过将该模型应用于我们收集的 2022 年澳大利亚联邦选举 Facebook 广告数据集，我们揭示了政治竞选策略中的细微说服技巧，并提供了一种实用方法来检测和分析这些策略，从而提升社交媒体政治活动的透明度。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13844v1",
      "published_date": "2025-03-18 02:33:38 UTC",
      "updated_date": "2025-03-18 02:33:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:47:26.334101"
    },
    {
      "arxiv_id": "2503.13843v1",
      "title": "WebNav: An Intelligent Agent for Voice-Controlled Web Navigation",
      "title_zh": "翻译失败",
      "authors": [
        "Trisanth Srinivasan",
        "Santosh Patapati"
      ],
      "abstract": "The increasing reliance on web interfaces presents many challenges for\nvisually impaired users, showcasing the need for more advanced assistive\ntechnologies. This paper introduces WebNav, a voice-controlled web navigation\nagent that leverages a ReAct-inspired architecture and generative AI to provide\nthis framework. WebNav comprises of a hierarchical structure: a Digital\nNavigation Module (DIGNAV) for high-level strategic planning, an Assistant\nModule for translating abstract commands into executable actions, and an\nInference Module for low-level interaction. A key component is a dynamic\nlabeling engine, implemented as a browser extension, that generates real-time\nlabels for interactive elements, creating mapping between voice commands and\nDocument Object Model (DOM) components. Preliminary evaluations show that\nWebNav outperforms traditional screen readers in response time and task\ncompletion accuracy for the visually impaired. Future work will focus on\nextensive user evaluations, benchmark development, and refining the agent's\nadaptive capabilities for real-world deployment.",
      "tldr_zh": "该论文介绍了 WebNav，一种智能代理，用于语音控制的网页导航，旨在为视力障碍用户提供更先进的辅助技术。WebNav 采用 ReAct 启发的架构，包括 Digital Navigation Module (DIGNAV) 用于高层战略规划、Assistant Module 用于将抽象命令转化为可执行动作，以及 Inference Module 用于低层交互；同时，动态标签引擎作为浏览器扩展，实时生成交互元素的标签，以映射语音命令到 Document Object Model (DOM) 组件。初步评估显示，WebNav 在响应时间和任务完成准确性上优于传统屏幕阅读器。未来工作将聚焦于广泛的用户评估、基准开发以及提升代理的适应能力，以实现实际部署。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "H.5.2; I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13843v1",
      "published_date": "2025-03-18 02:33:27 UTC",
      "updated_date": "2025-03-18 02:33:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:47:37.727976"
    },
    {
      "arxiv_id": "2503.13842v1",
      "title": "Counterfactual experience augmented off-policy reinforcement learning",
      "title_zh": "翻译失败",
      "authors": [
        "Sunbowen Lee",
        "Yicheng Gong",
        "Chao Deng"
      ],
      "abstract": "Reinforcement learning control algorithms face significant challenges due to\nout-of-distribution and inefficient exploration problems. While model-based\nreinforcement learning enhances the agent's reasoning and planning capabilities\nby constructing virtual environments, training such virtual environments can be\nvery complex. In order to build an efficient inference model and enhance the\nrepresentativeness of learning data, we propose the Counterfactual Experience\nAugmentation (CEA) algorithm. CEA leverages variational autoencoders to model\nthe dynamic patterns of state transitions and introduces randomness to model\nnon-stationarity. This approach focuses on expanding the learning data in the\nexperience pool through counterfactual inference and performs exceptionally\nwell in environments that follow the bisimulation assumption. Environments with\nbisimulation properties are usually represented by discrete observation and\naction spaces, we propose a sampling method based on maximum kernel density\nestimation entropy to extend CEA to various environments. By providing reward\nsignals for counterfactual state transitions based on real information, CEA\nconstructs a complete counterfactual experience to alleviate the\nout-of-distribution problem of the learning data, and outperforms general SOTA\nalgorithms in environments with difference properties. Finally, we discuss the\nsimilarities, differences and properties of generated counterfactual\nexperiences and real experiences. The code is available at\nhttps://github.com/Aegis1863/CEA.",
      "tldr_zh": "这篇论文提出 Counterfactual Experience Augmentation (CEA) 算法，用于解决 Reinforcement Learning 中的 out-of-distribution 和探索效率问题。CEA 利用 variational autoencoders 建模状态转移动态，并引入随机性处理非平稳性，通过反事实推理扩展经验池中的学习数据。在 bisimulation 假设的环境中，算法采用基于最大核密度估计熵的采样方法，以适应离散观测和动作空间。实验结果表明，CEA 在不同环境性能优于 SOTA 算法，并通过提供奖励信号缓解 out-of-distribution 问题，同时讨论了反事实经验与真实经验的异同。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by Neurocomputing,\n  https://doi.org/10.1016/j.neucom.2025.130017",
      "pdf_url": "http://arxiv.org/pdf/2503.13842v1",
      "published_date": "2025-03-18 02:32:50 UTC",
      "updated_date": "2025-03-18 02:32:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:47:50.023833"
    },
    {
      "arxiv_id": "2503.13836v1",
      "title": "SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing",
      "title_zh": "翻译失败",
      "authors": [
        "Seokhyeon Hong",
        "Chaelin Kim",
        "Serin Yoon",
        "Junghyun Nam",
        "Sihun Cha",
        "Junyong Noh"
      ],
      "abstract": "Text-driven motion generation has advanced significantly with the rise of\ndenoising diffusion models. However, previous methods often oversimplify\nrepresentations for the skeletal joints, temporal frames, and textual words,\nlimiting their ability to fully capture the information within each modality\nand their interactions. Moreover, when using pre-trained models for downstream\ntasks, such as editing, they typically require additional efforts, including\nmanual interventions, optimization, or fine-tuning. In this paper, we introduce\na skeleton-aware latent diffusion (SALAD), a model that explicitly captures the\nintricate inter-relationships between joints, frames, and words. Furthermore,\nby leveraging cross-attention maps produced during the generation process, we\nenable attention-based zero-shot text-driven motion editing using a pre-trained\nSALAD model, requiring no additional user input beyond text prompts. Our\napproach significantly outperforms previous methods in terms of text-motion\nalignment without compromising generation quality, and demonstrates practical\nversatility by providing diverse editing capabilities beyond generation. Code\nis available at project page.",
      "tldr_zh": "本文提出 SALAD，一种骨骼感知的潜在扩散模型（Skeleton-aware Latent Diffusion），用于文本驱动的动作生成和编辑，通过显式捕捉关节、帧和词之间的复杂相互关系，解决了现有去噪扩散模型（denoising diffusion models）在模态信息和交互捕捉方面的局限性。该模型利用生成过程中的交叉注意力图（cross-attention maps）实现零样本文本驱动编辑，仅需文本提示即可，无需额外用户干预或微调。实验表明，SALAD 在文本-动作对齐方面显著优于现有方法，同时保持高生成质量，并扩展了多样化的编辑功能，如动作修改和优化。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025; Project page\n  https://seokhyeonhong.github.io/projects/salad/",
      "pdf_url": "http://arxiv.org/pdf/2503.13836v1",
      "published_date": "2025-03-18 02:20:11 UTC",
      "updated_date": "2025-03-18 02:20:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:48:02.494312"
    },
    {
      "arxiv_id": "2503.14557v1",
      "title": "Generating Causal Explanations of Vehicular Agent Behavioural Interactions with Learnt Reward Profiles",
      "title_zh": "翻译失败",
      "authors": [
        "Rhys Howard",
        "Nick Hawes",
        "Lars Kunze"
      ],
      "abstract": "Transparency and explainability are important features that responsible\nautonomous vehicles should possess, particularly when interacting with humans,\nand causal reasoning offers a strong basis to provide these qualities. However,\neven if one assumes agents act to maximise some concept of reward, it is\ndifficult to make accurate causal inferences of agent planning without\ncapturing what is of importance to the agent. Thus our work aims to learn a\nweighting of reward metrics for agents such that explanations for agent\ninteractions can be causally inferred. We validate our approach quantitatively\nand qualitatively across three real-world driving datasets, demonstrating a\nfunctional improvement over previous methods and competitive performance across\nevaluation metrics.",
      "tldr_zh": "这篇论文针对自主车辆的透明性和可解释性，提出了一种方法，通过学习奖励配置文件（learnt reward profiles）来生成车辆代理（vehicular agents）行为互动的因果解释（causal explanations）。方法假设代理行为是为了最大化奖励指标，并通过学习这些指标的权重来实现准确的因果推理，从而更好地理解代理的规划决策。在三个真实世界的驾驶数据集上进行定量和定性验证，结果显示该方法在功能上优于先前方法，并在评估指标上表现出竞争性能。",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.RO",
        "I.2.0; I.2.6; I.2.9; I.2.11; I.6.0"
      ],
      "primary_category": "cs.AI",
      "comment": "8 Pages, 5 Figures, To be published in the Proceedings of the 2025\n  IEEE International Conference on Robotics & Automation, Initial upload of\n  accepted paper",
      "pdf_url": "http://arxiv.org/pdf/2503.14557v1",
      "published_date": "2025-03-18 01:53:59 UTC",
      "updated_date": "2025-03-18 01:53:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:48:13.373057"
    },
    {
      "arxiv_id": "2503.13817v1",
      "title": "VARP: Reinforcement Learning from Vision-Language Model Feedback with Agent Regularized Preferences",
      "title_zh": "VARP：来自视觉语言模型反馈的强化学习，带有代理正则化偏好",
      "authors": [
        "Anukriti Singh",
        "Amisha Bhaskar",
        "Peihong Yu",
        "Souradip Chakraborty",
        "Ruthwik Dasyam",
        "Amrit Bedi",
        "Pratap Tokekar"
      ],
      "abstract": "Designing reward functions for continuous-control robotics often leads to\nsubtle misalignments or reward hacking, especially in complex tasks.\nPreference-based RL mitigates some of these pitfalls by learning rewards from\ncomparative feedback rather than hand-crafted signals, yet scaling human\nannotations remains challenging. Recent work uses Vision-Language Models (VLMs)\nto automate preference labeling, but a single final-state image generally fails\nto capture the agent's full motion. In this paper, we present a two-part\nsolution that both improves feedback accuracy and better aligns reward learning\nwith the agent's policy. First, we overlay trajectory sketches on final\nobservations to reveal the path taken, allowing VLMs to provide more reliable\npreferences-improving preference accuracy by approximately 15-20% in metaworld\ntasks. Second, we regularize reward learning by incorporating the agent's\nperformance, ensuring that the reward model is optimized based on data\ngenerated by the current policy; this addition boosts episode returns by 20-30%\nin locomotion tasks. Empirical studies on metaworld demonstrate that our method\nachieves, for instance, around 70-80% success rate in all tasks, compared to\nbelow 50% for standard approaches. These results underscore the efficacy of\ncombining richer visual representations with agent-aware reward regularization.",
      "tldr_zh": "本文提出 VARP 方法，用于从视觉语言模型（VLMs）反馈中进行强化学习（Reinforcement Learning），以解决传统奖励函数设计中的误对齐和奖励黑客问题。具体而言，该方法通过在最终观察上叠加轨迹草图（trajectory sketches）来提升偏好反馈准确性约15-20%，并引入代理正则化（agent regularized preferences）来优化奖励学习，提高回合回报20-30%。在 metaworld 任务的实证研究中，VARP 实现了70-80%的成功率，显著优于标准方法的低于50%。这项工作证明了结合更丰富的视觉表示和代理感知奖励正则化的有效性。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.13817v1",
      "published_date": "2025-03-18 01:51:27 UTC",
      "updated_date": "2025-03-18 01:51:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:48:26.938508"
    },
    {
      "arxiv_id": "2503.13813v1",
      "title": "Automatic MILP Model Construction for Multi-Robot Task Allocation and Scheduling Based on Large Language Models",
      "title_zh": "基于大型语言模型的自动 MILP 模型构建，用于多机器人任务分配和调度",
      "authors": [
        "Mingming Peng",
        "Zhendong Chen",
        "Jie Yang",
        "Jin Huang",
        "Zhengqi Shi",
        "Qihao Liu",
        "Xinyu Li",
        "Liang Gao"
      ],
      "abstract": "With the accelerated development of Industry 4.0, intelligent manufacturing\nsystems increasingly require efficient task allocation and scheduling in\nmulti-robot systems. However, existing methods rely on domain expertise and\nface challenges in adapting to dynamic production constraints. Additionally,\nenterprises have high privacy requirements for production scheduling data,\nwhich prevents the use of cloud-based large language models (LLMs) for solution\ndevelopment. To address these challenges, there is an urgent need for an\nautomated modeling solution that meets data privacy requirements. This study\nproposes a knowledge-augmented mixed integer linear programming (MILP)\nautomated formulation framework, integrating local LLMs with domain-specific\nknowledge bases to generate executable code from natural language descriptions\nautomatically. The framework employs a knowledge-guided\nDeepSeek-R1-Distill-Qwen-32B model to extract complex spatiotemporal\nconstraints (82% average accuracy) and leverages a supervised fine-tuned\nQwen2.5-Coder-7B-Instruct model for efficient MILP code generation (90% average\naccuracy). Experimental results demonstrate that the framework successfully\nachieves automatic modeling in the aircraft skin manufacturing case while\nensuring data privacy and computational efficiency. This research provides a\nlow-barrier and highly reliable technical path for modeling in complex\nindustrial scenarios.",
      "tldr_zh": "这篇论文提出了一种基于大型语言模型（LLMs）的自动混合整数线性规划（MILP）模型构建框架，用于多机器人任务分配和调度，以应对动态生产约束和数据隐私挑战。该框架整合本地 LLMs 与领域特定知识库，通过知识引导的 DeepSeek-R1-Distill-Qwen-32B 模型提取复杂时空约束（平均准确率 82%），以及监督微调的 Qwen2.5-Coder-7B-Instruct 模型自动生成可执行 MILP 代码（平均准确率 90%）。实验结果在飞机蒙皮制造案例中验证了框架的有效性，确保数据隐私和计算效率，并为复杂工业场景提供低门槛、高可靠的技术路径。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13813v1",
      "published_date": "2025-03-18 01:45:19 UTC",
      "updated_date": "2025-03-18 01:45:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:48:38.241723"
    },
    {
      "arxiv_id": "2503.13812v1",
      "title": "The Empty Chair: Using LLMs to Raise Missing Perspectives in Policy Deliberations",
      "title_zh": "翻译失败",
      "authors": [
        "Suyash Fulay",
        "Deb Roy"
      ],
      "abstract": "Deliberation is essential to well-functioning democracies, yet physical,\neconomic, and social barriers often exclude certain groups, reducing\nrepresentativeness and contributing to issues like group polarization. In this\nwork, we explore the use of large language model (LLM) personas to introduce\nmissing perspectives in policy deliberations. We develop and evaluate a tool\nthat transcribes conversations in real-time and simulates input from relevant\nbut absent stakeholders. We deploy this tool in a 19-person student citizens'\nassembly on campus sustainability. Participants and facilitators found that the\ntool sparked new discussions and surfaced valuable perspectives they had not\npreviously considered. However, they also noted that AI-generated responses\nwere sometimes overly general. They raised concerns about overreliance on AI\nfor perspective-taking. Our findings highlight both the promise and potential\nrisks of using LLMs to raise missing points of view in group deliberation\nsettings.",
      "tldr_zh": "这篇论文探讨了使用大型语言模型(LLM)来引入政策审议中缺失的视角，以解决代表性不足和群体极化等问题。研究开发了一个工具，该工具实时转录对话并模拟相关但缺席利益相关者的输入，并在校园可持续发展的19人学生公民大会中进行部署。结果显示，该工具成功激发了新讨论并带来了之前未考虑的宝贵视角，但AI生成的响应有时过于泛化，并引发了对过度依赖AI的担忧。总体而言，研究突出了LLM在提升审议代表性的潜力及其潜在风险。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13812v1",
      "published_date": "2025-03-18 01:45:08 UTC",
      "updated_date": "2025-03-18 01:45:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:48:50.340129"
    },
    {
      "arxiv_id": "2503.13806v1",
      "title": "Organ-aware Multi-scale Medical Image Segmentation Using Text Prompt Engineering",
      "title_zh": "基于文本提示工程的器官感知多尺度医学图像分割",
      "authors": [
        "Wenjie Zhang",
        "Ziyang Zhang",
        "Mengnan He",
        "Jiancheng Ye"
      ],
      "abstract": "Accurate segmentation is essential for effective treatment planning and\ndisease monitoring. Existing medical image segmentation methods predominantly\nrely on uni-modal visual inputs, such as images or videos, requiring\nlabor-intensive manual annotations. Additionally, medical imaging techniques\ncapture multiple intertwined organs within a single scan, further complicating\nsegmentation accuracy. To address these challenges, MedSAM, a large-scale\nmedical segmentation model based on the Segment Anything Model (SAM), was\ndeveloped to enhance segmentation accuracy by integrating image features with\nuser-provided prompts. While MedSAM has demonstrated strong performance across\nvarious medical segmentation tasks, it primarily relies on geometric prompts\n(e.g., points and bounding boxes) and lacks support for text-based prompts,\nwhich could help specify subtle or ambiguous anatomical structures. To overcome\nthese limitations, we propose the Organ-aware Multi-scale Text-guided Medical\nImage Segmentation Model (OMT-SAM) for multi-organ segmentation. Our approach\nintroduces CLIP encoders as a novel image-text prompt encoder, operating with\nthe geometric prompt encoder to provide informative contextual guidance. We\npair descriptive textual prompts with corresponding images, processing them\nthrough pre-trained CLIP encoders and a cross-attention mechanism to generate\nfused image-text embeddings. Additionally, we extract multi-scale visual\nfeatures from MedSAM, capturing fine-grained anatomical details at different\nlevels of granularity. We evaluate OMT-SAM on the FLARE 2021 dataset,\nbenchmarking its performance against existing segmentation methods. Empirical\nresults demonstrate that OMT-SAM achieves a mean Dice Similarity Coefficient of\n0.937, outperforming MedSAM (0.893) and other segmentation models, highlighting\nits superior capability in handling complex medical image segmentation tasks.",
      "tldr_zh": "本论文针对医疗图像分割的挑战，如依赖单模态视觉输入和手动标注的问题，提出了一种器官感知的多尺度文本引导分割模型 OMT-SAM。OMT-SAM 基于 Segment Anything Model (SAM) 的 MedSAM 扩展，引入 CLIP encoders 作为图像-文本提示编码器，与几何提示编码器结合，通过交叉注意力机制融合描述性文本提示和多尺度视觉特征，从而更好地处理复杂多器官场景。相比传统方法，该模型能更精确地捕捉细粒度解剖细节，并在 FLARE 2021 数据集上实现平均 Dice Similarity Coefficient 为 0.937，优于 MedSAM (0.893) 和其他基线模型。整体而言，OMT-SAM 通过文本提示工程提升了医疗图像分割的准确性和效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13806v1",
      "published_date": "2025-03-18 01:35:34 UTC",
      "updated_date": "2025-03-18 01:35:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:49:02.430434"
    },
    {
      "arxiv_id": "2503.13804v1",
      "title": "Empowering GraphRAG with Knowledge Filtering and Integration",
      "title_zh": "翻译失败",
      "authors": [
        "Kai Guo",
        "Harry Shomer",
        "Shenglai Zeng",
        "Haoyu Han",
        "Yu Wang",
        "Jiliang Tang"
      ],
      "abstract": "In recent years, large language models (LLMs) have revolutionized the field\nof natural language processing. However, they often suffer from knowledge gaps\nand hallucinations. Graph retrieval-augmented generation (GraphRAG) enhances\nLLM reasoning by integrating structured knowledge from external graphs.\nHowever, we identify two key challenges that plague GraphRAG:(1) Retrieving\nnoisy and irrelevant information can degrade performance and (2)Excessive\nreliance on external knowledge suppresses the model's intrinsic reasoning. To\naddress these issues, we propose GraphRAG-FI (Filtering and Integration),\nconsisting of GraphRAG-Filtering and GraphRAG-Integration. GraphRAG-Filtering\nemploys a two-stage filtering mechanism to refine retrieved information.\nGraphRAG-Integration employs a logits-based selection strategy to balance\nexternal knowledge from GraphRAG with the LLM's intrinsic reasoning,reducing\nover-reliance on retrievals. Experiments on knowledge graph QA tasks\ndemonstrate that GraphRAG-FI significantly improves reasoning performance\nacross multiple backbone models, establishing a more reliable and effective\nGraphRAG framework.",
      "tldr_zh": "该论文针对大型语言模型（LLMs）存在的知识 gaps 和 hallucinations 问题，提出通过 GraphRAG 整合外部图结构知识来提升推理能力，但同时指出了 GraphRAG 的两大挑战：检索到 noisy 和 irrelevant 信息导致性能下降，以及过度依赖外部知识抑制模型内在推理。作者开发了 GraphRAG-FI 框架，包括 GraphRAG-Filtering 的两阶段过滤机制来提炼检索信息，以及 GraphRAG-Integration 的 logits-based 选择策略来平衡外部知识与 LLM 的内在推理。实验在知识图 QA 任务上证明，GraphRAG-FI 显著提高了多个骨干模型的推理性能，建立了一个更可靠有效的 GraphRAG 系统。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13804v1",
      "published_date": "2025-03-18 01:29:55 UTC",
      "updated_date": "2025-03-18 01:29:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:49:14.479959"
    },
    {
      "arxiv_id": "2503.13799v1",
      "title": "SMILE: a Scale-aware Multiple Instance Learning Method for Multicenter STAS Lung Cancer Histopathology Diagnosis",
      "title_zh": "翻译失败",
      "authors": [
        "Liangrui Pan",
        "Xiaoyu Li",
        "Yutao Dou",
        "Qiya Song",
        "Jiadi Luo",
        "Qingchun Liang",
        "Shaoliang Peng"
      ],
      "abstract": "Spread through air spaces (STAS) represents a newly identified aggressive\npattern in lung cancer, which is known to be associated with adverse prognostic\nfactors and complex pathological features. Pathologists currently rely on time\nconsuming manual assessments, which are highly subjective and prone to\nvariation. This highlights the urgent need for automated and precise diag\nnostic solutions. 2,970 lung cancer tissue slides are comprised from multiple\ncenters, re-diagnosed them, and constructed and publicly released three lung\ncancer STAS datasets: STAS CSU (hospital), STAS TCGA, and STAS CPTAC. All STAS\ndatasets provide corresponding pathological feature diagnoses and related\nclinical data. To address the bias, sparse and heterogeneous nature of STAS, we\npropose an scale-aware multiple instance learning(SMILE) method for STAS\ndiagnosis of lung cancer. By introducing a scale-adaptive attention mechanism,\nthe SMILE can adaptively adjust high attention instances, reducing\nover-reliance on local regions and promoting consistent detection of STAS\nlesions. Extensive experiments show that SMILE achieved competitive diagnostic\nresults on STAS CSU, diagnosing 251 and 319 STAS samples in CPTAC\nandTCGA,respectively, surpassing clinical average AUC. The 11 open baseline\nresults are the first to be established for STAS research, laying the\nfoundation for the future expansion, interpretability, and clinical integration\nof computational pathology technologies. The datasets and code are available at\nhttps://anonymous.4open.science/r/IJCAI25-1DA1.",
      "tldr_zh": "这篇论文针对肺癌的 STAS（Spread Through Air Spaces）模式提出了一种 scale-aware multiple instance learning 方法，名为 SMILE，以应对其偏置、稀疏和异质性的挑战。SMILE 通过引入 scale-adaptive attention mechanism 动态调整高注意力实例，减少对局部区域的过度依赖，从而提升 STAS 病变的检测一致性和准确性。研究构建并公开了三个多中心数据集（STAS CSU、STAS TCGA 和 STAS CPTAC），并在实验中证明 SMILE 在这些数据集上取得了竞争性结果，AUC 超过了临床平均水平，并为 STAS 研究的未来扩展和临床整合奠定了基准基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13799v1",
      "published_date": "2025-03-18 01:09:52 UTC",
      "updated_date": "2025-03-18 01:09:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:49:26.789959"
    },
    {
      "arxiv_id": "2503.13798v1",
      "title": "AI-Powered Prediction of Nanoparticle Pharmacokinetics: A Multi-View Learning Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Amirhossein Khakpour",
        "Lucia Florescu",
        "Richard Tilley",
        "Haibo Jiang",
        "K. Swaminathan Iyer",
        "Gustavo Carneiro"
      ],
      "abstract": "The clinical translation of nanoparticle-based treatments remains limited due\nto the unpredictability of (nanoparticle) NP\npharmacokinetics$\\unicode{x2014}$how they distribute, accumulate, and clear\nfrom the body. Predicting these behaviours is challenging due to complex\nbiological interactions and the difficulty of obtaining high-quality\nexperimental datasets. Existing AI-driven approaches rely heavily on\ndata-driven learning but fail to integrate crucial knowledge about NP\nproperties and biodistribution mechanisms. We introduce a multi-view deep\nlearning framework that enhances pharmacokinetic predictions by incorporating\nprior knowledge of key NP properties such as size and charge into a\ncross-attention mechanism, enabling context-aware feature selection and\nimproving generalization despite small datasets. To further enhance prediction\nrobustness, we employ an ensemble learning approach, combining deep learning\nwith XGBoost (XGB) and Random Forest (RF), which significantly outperforms\nexisting AI models. Our interpretability analysis reveals key physicochemical\nproperties driving NP biodistribution, providing biologically meaningful\ninsights into possible mechanisms governing NP behaviour in vivo rather than a\nblack-box model. Furthermore, by bridging machine learning with physiologically\nbased pharmacokinetic (PBPK) modelling, this work lays the foundation for\ndata-efficient AI-driven drug discovery and precision nanomedicine.",
      "tldr_zh": "本研究针对纳米粒子 (NP) 药代动力学预测的挑战，提出了一种多视图深度学习框架，通过将 NP 关键属性（如大小和电荷）整合进交叉注意力机制，实现上下文感知特征选择，并提升小数据集下的泛化能力。框架进一步采用集成学习方法，结合深度学习、XGBoost (XGB) 和 Random Forest (RF)，显著优于现有 AI 模型，预测准确性得到大幅提升。研究的可解释性分析揭示了驱动 NP 生物分布的关键理化性质，提供生物学意义的机制洞见，并桥接机器学习与生理学基础药代动力学 (PBPK) 建模，为数据高效的 AI 驱动药物发现和精准纳米医学奠定基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13798v1",
      "published_date": "2025-03-18 01:09:32 UTC",
      "updated_date": "2025-03-18 01:09:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:49:39.469503"
    },
    {
      "arxiv_id": "2503.13794v2",
      "title": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Zhou",
        "Shiyu Zhao",
        "Yuxiao Chen",
        "Zhenting Wang",
        "Can Jin",
        "Dimitris N. Metaxas"
      ],
      "abstract": "Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design.",
      "tldr_zh": "本研究提出LED方法，通过直接融合Large Language Models (LLMs)的隐藏状态来增强Open-Vocabulary Object Detection (OVD)，避免了依赖人工策划的合成数据生成，从而减少偏见和过拟合问题。LED利用MLLM的解码器层，并引入zero-initialized cross-attention adapter，实现高效的知识融合，特别发现中间LLM层已编码丰富空间语义，仅适配早期层即可获得大部分提升。实验显示，使用Swin-T作为视觉编码器，Qwen2-0.5B + LED使GroundingDINO在OmniLabel上的性能提升3.82%，额外GFLOPs仅增加8.7%；更大视觉backbone可进一步提升至6.22%，并通过广泛消融实验验证了设计的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13794v2",
      "published_date": "2025-03-18 00:50:40 UTC",
      "updated_date": "2025-05-20 14:22:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:49:51.247011"
    },
    {
      "arxiv_id": "2503.13793v1",
      "title": "Mapping the Trust Terrain: LLMs in Software Engineering -- Insights and Perspectives",
      "title_zh": "映射信任地形：软件工程中的 LLMs ——见解与观点",
      "authors": [
        "Dipin Khati",
        "Yijin Liu",
        "David N. Palacio",
        "Yixuan Zhang",
        "Denys Poshyvanyk"
      ],
      "abstract": "Applications of Large Language Models (LLMs) are rapidly growing in industry\nand academia for various software engineering (SE) tasks. As these models\nbecome more integral to critical processes, ensuring their reliability and\ntrustworthiness becomes essential. Consequently, the concept of trust in these\nsystems is becoming increasingly critical. Well-calibrated trust is important,\nas excessive trust can lead to security vulnerabilities, and risks, while\ninsufficient trust can hinder innovation. However, the landscape of\ntrust-related concepts in LLMs in SE is relatively unclear, with concepts such\nas trust, distrust, and trustworthiness lacking clear conceptualizations in the\nSE community. To bring clarity to the current research status and identify\nopportunities for future work, we conducted a comprehensive review of $88$\npapers: a systematic literature review of $18$ papers focused on LLMs in SE,\ncomplemented by an analysis of 70 papers from broader trust literature.\nAdditionally, we conducted a survey study with 25 domain experts to gain\ninsights into practitioners' understanding of trust and identify gaps between\nexisting literature and developers' perceptions. The result of our analysis\nserves as a roadmap that covers trust-related concepts in LLMs in SE and\nhighlights areas for future exploration.",
      "tldr_zh": "这篇论文探讨了Large Language Models (LLMs) 在软件工程 (SE) 中的应用及其信任问题，强调了过度信任可能导致安全风险，而不足信任则会阻碍创新，并指出现有SE社区中信任、怀疑和可信性概念的模糊性。为澄清现状，该研究通过系统文献回顾分析了18篇专注于LLMs in SE的论文和70篇更广泛的信任文献，并结合25位领域专家的调查，揭示了从业者认知与文献间的差距。最终，论文提供了一个路线图，涵盖LLMs in SE的信任相关概念，并突出未来研究机会。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13793v1",
      "published_date": "2025-03-18 00:49:43 UTC",
      "updated_date": "2025-03-18 00:49:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:50:01.735770"
    },
    {
      "arxiv_id": "2503.14556v1",
      "title": "Designing and Deploying AI Models for Sustainable Logistics Optimization: A Case Study on Eco-Efficient Supply Chains in the USA",
      "title_zh": "翻译失败",
      "authors": [
        "Reza E Rabbi Shawon",
        "MD Rokibul Hasan",
        "Md Anisur Rahman",
        "Mohamed Ghandri",
        "Iman Ahmed Lamari",
        "Mohammed Kawsar",
        "Rubi Akter"
      ],
      "abstract": "The rapid evolution of Artificial Intelligence (AI) and Machine Learning (ML)\nhas significantly transformed logistics and supply chain management,\nparticularly in the pursuit of sustainability and eco-efficiency. This study\nexplores AI-based methodologies for optimizing logistics operations in the USA,\nfocusing on reducing environmental impact, improving fuel efficiency, and\nminimizing costs. Key AI applications include predictive analytics for demand\nforecasting, route optimization through machine learning, and AI-powered fuel\nefficiency strategies. Various models, such as Linear Regression, XGBoost,\nSupport Vector Machine, and Neural Networks, are applied to real-world\nlogistics datasets to reduce carbon emissions based on logistics operations,\noptimize travel routes to minimize distance and travel time, and predict future\ndeliveries to plan optimal routes. Other models such as K-Means and DBSCAN are\nalso used to optimize travel routes to minimize distance and travel time for\nlogistics operations. This study utilizes datasets from logistics companies'\ndatabases. The study also assesses model performance using metrics such as mean\nabsolute error (MAE), mean squared error (MSE), and R2 score. This study also\nexplores how these models can be deployed to various platforms for real-time\nlogistics and supply chain use. The models are also examined through a thorough\ncase study, highlighting best practices and regulatory frameworks that promote\nsustainability. The findings demonstrate AI's potential to enhance logistics\nefficiency, reduce carbon footprints, and contribute to a more resilient and\nadaptive supply chain ecosystem.",
      "tldr_zh": "本研究探讨了 AI 和 ML 在美国可持续物流优化中的应用，针对减少碳排放、提高燃料效率和降低成本进行设计和部署。研究采用多种模型，如 Linear Regression、XGBoost、Support Vector Machine 和 Neural Networks，进行需求预测、路线优化和燃料效率策略，同时利用 K-Means 和 DBSCAN 优化旅行路径。基于真实物流数据集，模型性能通过 MAE、MSE 和 R2 score 等指标评估，并通过案例研究展示最佳实践和监管框架。结果显示，这些 AI 方法显著提升了供应链效率，降低了碳足迹，并促进了更具弹性和适应性的生态高效系统。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14556v1",
      "published_date": "2025-03-18 00:46:35 UTC",
      "updated_date": "2025-03-18 00:46:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:50:14.804311"
    },
    {
      "arxiv_id": "2503.13786v2",
      "title": "Evaluating the Application of SOLID Principles in Modern AI Framework Architectures",
      "title_zh": "评估 SOLID 原则在现代 AI 框架架构中的应用",
      "authors": [
        "Jonesh Shrestha"
      ],
      "abstract": "This research evaluates the extent to which modern AI frameworks,\nspecifically TensorFlow and scikit-learn, adhere to the SOLID design principles\n- Single Responsibility, Open/Closed, Liskov Substitution, Interface\nSegregation, and Dependency Inversion. Analyzing the frameworks architectural\ndocumentation and design philosophies, this research investigates architectural\ntrade-offs when balancing software engineering best practices with AI-specific\nneeds. I examined each frameworks documentation, source code, and architectural\ncomponents to evaluate their adherence to these principles. The results show\nthat both frameworks adopt certain aspects of SOLID design principles but make\nintentional trade-offs to address performance, scalability, and the\nexperimental nature of AI development. TensorFlow focuses on performance and\nscalability, sometimes sacrificing strict adherence to principles like Single\nResponsibility and Interface Segregation. While scikit-learns design philosophy\naligns more closely with SOLID principles through consistent interfaces and\ncomposition principles, sticking closer to SOLID guidelines but with occasional\ndeviations for performance optimizations and scalability. This research\ndiscovered that applying SOLID principles in AI frameworks depends on context,\nas performance, scalability, and flexibility often require deviations from\ntraditional software engineering principles. This research contributes to\nunderstanding how domain-specific constraints influence architectural decisions\nin modern AI frameworks and how these frameworks strategically adapted design\nchoices to effectively balance these contradicting requirements.",
      "tldr_zh": "本研究评估了现代 AI 框架 TensorFlow 和 scikit-learn 对 SOLID 设计原则（包括 Single Responsibility、Open/Closed、Liskov Substitution、Interface Segregation 和 Dependency Inversion）的遵守程度，通过分析框架的文档、源代码和架构组件来考察其设计权衡。结果显示，TensorFlow 优先考虑性能和可扩展性，导致在 Single Responsibility 和 Interface Segregation 等原则上有所牺牲，而 scikit-learn 通过一致接口和组合原则更紧密地遵循 SOLID，但也因性能优化而偶尔偏离。研究发现，AI 框架应用 SOLID 原则需视具体上下文而定，受性能、可扩展性和实验性需求的制约，此举有助于理解领域特定约束如何影响架构决策并实现需求平衡。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG",
        "68N19, 68T01",
        "D.2.11; I.2.0"
      ],
      "primary_category": "cs.SE",
      "comment": "5 pages, 1 figure, 12 references",
      "pdf_url": "http://arxiv.org/pdf/2503.13786v2",
      "published_date": "2025-03-18 00:37:23 UTC",
      "updated_date": "2025-04-02 17:23:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T03:50:25.260155"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 139,
  "processed_papers_count": 139,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-24T03:50:44.407072"
}
[
  {
    "arxiv_id": "2503.14783v1",
    "title": "RAT: Boosting Misclassification Detection Ability without Extra Data",
    "authors": [
      "Ge Yan",
      "Tsui-Wei Weng"
    ],
    "abstract": "As deep neural networks(DNN) become increasingly prevalent, particularly in\nhigh-stakes areas such as autonomous driving and healthcare, the ability to\ndetect incorrect predictions of models and intervene accordingly becomes\ncrucial for safety. In this work, we investigate the detection of misclassified\ninputs for image classification models from the lens of adversarial\nperturbation: we propose to use robust radius (a.k.a. input-space margin) as a\nconfidence metric and design two efficient estimation algorithms, RR-BS and\nRR-Fast, for misclassification detection. Furthermore, we design a training\nmethod called Radius Aware Training (RAT) to boost models' ability to identify\nmistakes. Extensive experiments show our method could achieve up to 29.3%\nreduction on AURC and 21.62% reduction in FPR@95TPR, compared with previous\nmethods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14783v1",
    "published_date": "2025-03-18 23:18:55 UTC",
    "updated_date": "2025-03-18 23:18:55 UTC"
  },
  {
    "arxiv_id": "2503.14779v1",
    "title": "Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution",
    "authors": [
      "Akram Khatami-Rizi",
      "Ahmad Mahmoudi-Aznaveh"
    ],
    "abstract": "Single Image Super-Resolution (SISR) aims to reconstruct high-resolution (HR)\nimages from low-resolution (LR) inputs. Deep learning, especially Convolutional\nNeural Networks (CNNs), has advanced SISR. However, increasing network depth\nincreases parameters, and memory usage, and slows training, which is\nproblematic for resource-limited devices. To address this, lightweight models\nare developed to balance accuracy and efficiency. We propose the Involution &\nBSConv Multi-Depth Distillation Network (IBMDN), combining Involution & BSConv\nMulti-Depth Distillation Block (IBMDB) and the Contrast and High-Frequency\nAttention Block (CHFAB). IBMDB integrates Involution and BSConv to balance\ncomputational efficiency and feature extraction. CHFAB enhances high-frequency\ndetails for better visual quality. IBMDB is compatible with other SISR\narchitectures and reduces complexity, improving evaluation metrics like PSNR\nand SSIM. In transformer-based models, IBMDB reduces memory usage while\nimproving feature extraction. In GANs, it enhances perceptual quality,\nbalancing pixel-level accuracy with perceptual details. Our experiments show\nthat the method achieves high accuracy with minimal computational cost. The\ncode is available at GitHub.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14779v1",
    "published_date": "2025-03-18 23:10:08 UTC",
    "updated_date": "2025-03-18 23:10:08 UTC"
  },
  {
    "arxiv_id": "2503.15558v1",
    "title": "Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning",
    "authors": [
      "NVIDIA",
      ":",
      "Alisson Azzolini",
      "Hannah Brandon",
      "Prithvijit Chattopadhyay",
      "Huayu Chen",
      "Jinju Chu",
      "Yin Cui",
      "Jenna Diamond",
      "Yifan Ding",
      "Francesco Ferroni",
      "Rama Govindaraju",
      "Jinwei Gu",
      "Siddharth Gururani",
      "Imad El Hanafi",
      "Zekun Hao",
      "Jacob Huffman",
      "Jingyi Jin",
      "Brendan Johnson",
      "Rizwan Khan",
      "George Kurian",
      "Elena Lantz",
      "Nayeon Lee",
      "Zhaoshuo Li",
      "Xuan Li",
      "Tsung-Yi Lin",
      "Yen-Chen Lin",
      "Ming-Yu Liu",
      "Andrew Mathau",
      "Yun Ni",
      "Lindsey Pavao",
      "Wei Ping",
      "David W. Romero",
      "Misha Smelyanskiy",
      "Shuran Song",
      "Lyne Tchapmi",
      "Andrew Z. Wang",
      "Boxin Wang",
      "Haoxiang Wang",
      "Fangyin Wei",
      "Jiashu Xu",
      "Yao Xu",
      "Xiaodong Yang",
      "Zhuolin Yang",
      "Xiaohui Zeng",
      "Zhe Zhang"
    ],
    "abstract": "Physical AI systems need to perceive, understand, and perform complex actions\nin the physical world. In this paper, we present the Cosmos-Reason1 models that\ncan understand the physical world and generate appropriate embodied decisions\n(e.g., next step action) in natural language through long chain-of-thought\nreasoning processes. We begin by defining key capabilities for Physical AI\nreasoning, with a focus on physical common sense and embodied reasoning. To\nrepresent physical common sense, we use a hierarchical ontology that captures\nfundamental knowledge about space, time, and physics. For embodied reasoning,\nwe rely on a two-dimensional ontology that generalizes across different\nphysical embodiments. Building on these capabilities, we develop two multimodal\nlarge language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data\nand train our models in four stages: vision pre-training, general supervised\nfine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL)\nas the post-training. To evaluate our models, we build comprehensive benchmarks\nfor physical common sense and embodied reasoning according to our ontologies.\nEvaluation results show that Physical AI SFT and reinforcement learning bring\nsignificant improvements. To facilitate the development of Physical AI, we will\nmake our code and pre-trained models available under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-reason1.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15558v1",
    "published_date": "2025-03-18 22:06:58 UTC",
    "updated_date": "2025-03-18 22:06:58 UTC"
  },
  {
    "arxiv_id": "2503.14755v1",
    "title": "Language Independent Named Entity Recognition via Orthogonal Transformation of Word Vectors",
    "authors": [
      "Omar E. Rakha",
      "Hazem M. Abbas"
    ],
    "abstract": "Word embeddings have been a key building block for NLP in which models relied\nheavily on word embeddings in many different tasks. In this paper, a model is\nproposed based on using Bidirectional LSTM/CRF with word embeddings to perform\nnamed entity recognition for any language. This is done by training a model on\na source language (English) and transforming word embeddings from the target\nlanguage into word embeddings of the source language by using an orthogonal\nlinear transformation matrix. Evaluation of the model shows that by training a\nmodel on an English dataset the model was capable of detecting named entities\nin an Arabic dataset without neither training or fine tuning the model on an\nArabic language dataset.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Paper was initially released in 2017 but was never published",
    "pdf_url": "http://arxiv.org/pdf/2503.14755v1",
    "published_date": "2025-03-18 21:57:58 UTC",
    "updated_date": "2025-03-18 21:57:58 UTC"
  },
  {
    "arxiv_id": "2503.14754v1",
    "title": "Bayesian Modeling of Zero-Shot Classifications for Urban Flood Detection",
    "authors": [
      "Matt Franchi",
      "Nikhil Garg",
      "Wendy Ju",
      "Emma Pierson"
    ],
    "abstract": "Street scene datasets, collected from Street View or dashboard cameras, offer\na promising means of detecting urban objects and incidents like street\nflooding. However, a major challenge in using these datasets is their lack of\nreliable labels: there are myriad types of incidents, many types occur rarely,\nand ground-truth measures of where incidents occur are lacking. Here, we\npropose BayFlood, a two-stage approach which circumvents this difficulty.\nFirst, we perform zero-shot classification of where incidents occur using a\npretrained vision-language model (VLM). Second, we fit a spatial Bayesian model\non the VLM classifications. The zero-shot approach avoids the need to annotate\nlarge training sets, and the Bayesian model provides frequent desiderata in\nurban settings - principled measures of uncertainty, smoothing across\nlocations, and incorporation of external data like stormwater accumulation\nzones. We comprehensively validate this two-stage approach, showing that VLMs\nprovide strong zero-shot signal for floods across multiple cities and time\nperiods, the Bayesian model improves out-of-sample prediction relative to\nbaseline methods, and our inferred flood risk correlates with known external\npredictors of risk. Having validated our approach, we show it can be used to\nimprove urban flood detection: our analysis reveals 113,738 people who are at\nhigh risk of flooding overlooked by current methods, identifies demographic\nbiases in existing methods, and suggests locations for new flood sensors. More\nbroadly, our results showcase how Bayesian modeling of zero-shot LM annotations\nrepresents a promising paradigm because it avoids the need to collect large\nlabeled datasets and leverages the power of foundation models while providing\nthe expressiveness and uncertainty quantification of Bayesian models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "In review",
    "pdf_url": "http://arxiv.org/pdf/2503.14754v1",
    "published_date": "2025-03-18 21:53:37 UTC",
    "updated_date": "2025-03-18 21:53:37 UTC"
  },
  {
    "arxiv_id": "2503.14751v1",
    "title": "LipShiFT: A Certifiably Robust Shift-based Vision Transformer",
    "authors": [
      "Rohan Menon",
      "Nicola Franco",
      "Stephan GÃ¼nnemann"
    ],
    "abstract": "Deriving tight Lipschitz bounds for transformer-based architectures presents\na significant challenge. The large input sizes and high-dimensional attention\nmodules typically prove to be crucial bottlenecks during the training process\nand leads to sub-optimal results. Our research highlights practical constraints\nof these methods in vision tasks. We find that Lipschitz-based margin training\nacts as a strong regularizer while restricting weights in successive layers of\nthe model. Focusing on a Lipschitz continuous variant of the ShiftViT model, we\naddress significant training challenges for transformer-based architectures\nunder norm-constrained input setting. We provide an upper bound estimate for\nthe Lipschitz constants of this model using the $l_2$ norm on common image\nclassification datasets. Ultimately, we demonstrate that our method scales to\nlarger models and advances the state-of-the-art in certified robustness for\ntransformer-based architectures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025 Workshop: VerifAI: AI Verification in the Wild",
    "pdf_url": "http://arxiv.org/pdf/2503.14751v1",
    "published_date": "2025-03-18 21:38:18 UTC",
    "updated_date": "2025-03-18 21:38:18 UTC"
  },
  {
    "arxiv_id": "2503.14734v1",
    "title": "GR00T N1: An Open Foundation Model for Generalist Humanoid Robots",
    "authors": [
      "NVIDIA",
      "Johan Bjorck",
      "Fernando CastaÃ±eda",
      "Nikita Cherniadev",
      "Xingye Da",
      "Runyu Ding",
      "Linxi \"Jim\" Fan",
      "Yu Fang",
      "Dieter Fox",
      "Fengyuan Hu",
      "Spencer Huang",
      "Joel Jang",
      "Zhenyu Jiang",
      "Jan Kautz",
      "Kaushil Kundalia",
      "Lawrence Lao",
      "Zhiqi Li",
      "Zongyu Lin",
      "Kevin Lin",
      "Guilin Liu",
      "Edith Llontop",
      "Loic Magne",
      "Ajay Mandlekar",
      "Avnish Narayan",
      "Soroush Nasiriany",
      "Scott Reed",
      "You Liang Tan",
      "Guanzhi Wang",
      "Zu Wang",
      "Jing Wang",
      "Qi Wang",
      "Jiannan Xiang",
      "Yuqi Xie",
      "Yinzhen Xu",
      "Zhenjia Xu",
      "Seonghyeon Ye",
      "Zhiding Yu",
      "Ao Zhang",
      "Hao Zhang",
      "Yizhou Zhao",
      "Ruijie Zheng",
      "Yuke Zhu"
    ],
    "abstract": "General-purpose robots need a versatile body and an intelligent mind. Recent\nadvancements in humanoid robots have shown great promise as a hardware platform\nfor building generalist autonomy in the human world. A robot foundation model,\ntrained on massive and diverse data sources, is essential for enabling the\nrobots to reason about novel situations, robustly handle real-world\nvariability, and rapidly learn new tasks. To this end, we introduce GR00T N1,\nan open foundation model for humanoid robots. GR00T N1 is a\nVision-Language-Action (VLA) model with a dual-system architecture. The\nvision-language module (System 2) interprets the environment through vision and\nlanguage instructions. The subsequent diffusion transformer module (System 1)\ngenerates fluid motor actions in real time. Both modules are tightly coupled\nand jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixture\nof real-robot trajectories, human videos, and synthetically generated datasets.\nWe show that our generalist robot model GR00T N1 outperforms the\nstate-of-the-art imitation learning baselines on standard simulation benchmarks\nacross multiple robot embodiments. Furthermore, we deploy our model on the\nFourier GR-1 humanoid robot for language-conditioned bimanual manipulation\ntasks, achieving strong performance with high data efficiency.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Authors are listed alphabetically. Project leads are Linxi \"Jim\" Fan\n  and Yuke Zhu",
    "pdf_url": "http://arxiv.org/pdf/2503.14734v1",
    "published_date": "2025-03-18 21:06:21 UTC",
    "updated_date": "2025-03-18 21:06:21 UTC"
  },
  {
    "arxiv_id": "2503.14716v1",
    "title": "Construction Site Scaffolding Completeness Detection Based on Mask R-CNN and Hough Transform",
    "authors": [
      "Pei-Hsin Lin",
      "Jacob J. Lin",
      "Shang-Hsien Hsieh"
    ],
    "abstract": "Construction site scaffolding is essential for many building projects, and\nensuring its safety is crucial to prevent accidents. The safety inspector must\ncheck the scaffolding's completeness and integrity, where most violations\noccur. The inspection process includes ensuring all the components are in the\nright place since workers often compromise safety for convenience and\ndisassemble parts such as cross braces. This paper proposes a deep\nlearning-based approach to detect the scaffolding and its cross braces using\ncomputer vision. A scaffold image dataset with annotated labels is used to\ntrain a convolutional neural network (CNN) model. With the proposed approach,\nwe can automatically detect the completeness of cross braces from images taken\nat construction sites, without the need for manual inspection, saving a\nsignificant amount of time and labor costs. This non-invasive and efficient\nsolution for detecting scaffolding completeness can help improve safety in\nconstruction sites.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The 30th EG-ICE: International Conference on Intelligent Computing in\n  Engineering",
    "pdf_url": "http://arxiv.org/pdf/2503.14716v1",
    "published_date": "2025-03-18 20:27:22 UTC",
    "updated_date": "2025-03-18 20:27:22 UTC"
  },
  {
    "arxiv_id": "2503.15555v1",
    "title": "Whole-Body Image-to-Image Translation for a Virtual Scanner in a Healthcare Digital Twin",
    "authors": [
      "Valerio Guarrasi",
      "Francesco Di Feola",
      "Rebecca Restivo",
      "Lorenzo Tronchin",
      "Paolo Soda"
    ],
    "abstract": "Generating positron emission tomography (PET) images from computed tomography\n(CT) scans via deep learning offers a promising pathway to reduce radiation\nexposure and costs associated with PET imaging, improving patient care and\naccessibility to functional imaging. Whole-body image translation presents\nchallenges due to anatomical heterogeneity, often limiting generalized models.\nWe propose a framework that segments whole-body CT images into four\nregions-head, trunk, arms, and legs-and uses district-specific Generative\nAdversarial Networks (GANs) for tailored CT-to-PET translation. Synthetic PET\nimages from each region are stitched together to reconstruct the whole-body\nscan. Comparisons with a baseline non-segmented GAN and experiments with\nPix2Pix and CycleGAN architectures tested paired and unpaired scenarios.\nQuantitative evaluations at district, whole-body, and lesion levels\ndemonstrated significant improvements with our district-specific GANs. Pix2Pix\nyielded superior metrics, ensuring precise, high-quality image synthesis. By\naddressing anatomical heterogeneity, this approach achieves state-of-the-art\nresults in whole-body CT-to-PET translation. This methodology supports\nhealthcare Digital Twins by enabling accurate virtual PET scans from CT data,\ncreating virtual imaging representations to monitor, predict, and optimize\nhealth outcomes.",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15555v1",
    "published_date": "2025-03-18 20:19:28 UTC",
    "updated_date": "2025-03-18 20:19:28 UTC"
  },
  {
    "arxiv_id": "2503.14681v1",
    "title": "DPImageBench: A Unified Benchmark for Differentially Private Image Synthesis",
    "authors": [
      "Chen Gong",
      "Kecen Li",
      "Zinan Lin",
      "Tianhao Wang"
    ],
    "abstract": "Differentially private (DP) image synthesis aims to generate artificial\nimages that retain the properties of sensitive images while protecting the\nprivacy of individual images within the dataset. Despite recent advancements,\nwe find that inconsistent--and sometimes flawed--evaluation protocols have been\napplied across studies. This not only impedes the understanding of current\nmethods but also hinders future advancements.\n  To address the issue, this paper introduces DPImageBench for DP image\nsynthesis, with thoughtful design across several dimensions: (1) Methods. We\nstudy eleven prominent methods and systematically characterize each based on\nmodel architecture, pretraining strategy, and privacy mechanism. (2)\nEvaluation. We include nine datasets and seven fidelity and utility metrics to\nthoroughly assess them. Notably, we find that a common practice of selecting\ndownstream classifiers based on the highest accuracy on the sensitive test set\nnot only violates DP but also overestimates the utility scores. DPImageBench\ncorrects for these mistakes. (3) Platform. Despite the methods and evaluation\nprotocols, DPImageBench provides a standardized interface that accommodates\ncurrent and future implementations within a unified framework. With\nDPImageBench, we have several noteworthy findings. For example, contrary to the\ncommon wisdom that pretraining on public image datasets is usually beneficial,\nwe find that the distributional similarity between pretraining and sensitive\nimages significantly impacts the performance of the synthetic images and does\nnot always yield improvements. In addition, adding noise to low-dimensional\nfeatures, such as the high-level characteristics of sensitive images, is less\naffected by the privacy budget compared to adding noise to high-dimensional\nfeatures, like weight gradients. The former methods perform better than the\nlatter under a low privacy budget.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "The first two authors contributed equally; code available at\n  https://github.com/2019ChenGong/DPImageBench",
    "pdf_url": "http://arxiv.org/pdf/2503.14681v1",
    "published_date": "2025-03-18 19:37:35 UTC",
    "updated_date": "2025-03-18 19:37:35 UTC"
  },
  {
    "arxiv_id": "2503.14662v1",
    "title": "ConQuer: A Framework for Concept-Based Quiz Generation",
    "authors": [
      "Yicheng Fu",
      "Zikui Wang",
      "Liuxin Yang",
      "Meiqing Huo",
      "Zhongdongming Dai"
    ],
    "abstract": "Quizzes play a crucial role in education by reinforcing students'\nunderstanding of key concepts and encouraging self-directed exploration.\nHowever, compiling high-quality quizzes can be challenging and require deep\nexpertise and insight into specific subject matter. Although LLMs have greatly\nenhanced the efficiency of quiz generation, concerns remain regarding the\nquality of these AI-generated quizzes and their educational impact on students.\nTo address these issues, we introduce ConQuer, a concept-based quiz generation\nframework that leverages external knowledge sources. We employ comprehensive\nevaluation dimensions to assess the quality of the generated quizzes, using\nLLMs as judges. Our experiment results demonstrate a 4.8% improvement in\nevaluation scores and a 77.52% win rate in pairwise comparisons against\nbaseline quiz sets. Ablation studies further underscore the effectiveness of\neach component in our framework. Code available at\nhttps://github.com/sofyc/ConQuer.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14662v1",
    "published_date": "2025-03-18 19:10:26 UTC",
    "updated_date": "2025-03-18 19:10:26 UTC"
  },
  {
    "arxiv_id": "2503.14655v1",
    "title": "Core-Periphery Principle Guided State Space Model for Functional Connectome Classification",
    "authors": [
      "Minheng Chen",
      "Xiaowei Yu",
      "Jing Zhang",
      "Tong Chen",
      "Chao Cao",
      "Yan Zhuang",
      "Yanjun Lyu",
      "Lu Zhang",
      "Tianming Liu",
      "Dajiang Zhu"
    ],
    "abstract": "Understanding the organization of human brain networks has become a central\nfocus in neuroscience, particularly in the study of functional connectivity,\nwhich plays a crucial role in diagnosing neurological disorders. Advances in\nfunctional magnetic resonance imaging and machine learning techniques have\nsignificantly improved brain network analysis. However, traditional machine\nlearning approaches struggle to capture the complex relationships between brain\nregions, while deep learning methods, particularly Transformer-based models,\nface computational challenges due to their quadratic complexity in\nlong-sequence modeling. To address these limitations, we propose a\nCore-Periphery State-Space Model (CP-SSM), an innovative framework for\nfunctional connectome classification. Specifically, we introduce Mamba, a\nselective state-space model with linear complexity, to effectively capture\nlong-range dependencies in functional brain networks. Furthermore, inspired by\nthe core-periphery (CP) organization, a fundamental characteristic of brain\nnetworks that enhances efficient information transmission, we design CP-MoE, a\nCP-guided Mixture-of-Experts that improves the representation learning of brain\nconnectivity patterns. We evaluate CP-SSM on two benchmark fMRI datasets: ABIDE\nand ADNI. Experimental results demonstrate that CP-SSM surpasses\nTransformer-based models in classification performance while significantly\nreducing computational complexity. These findings highlight the effectiveness\nand efficiency of CP-SSM in modeling brain functional connectivity, offering a\npromising direction for neuroimaging-based neurological disease diagnosis.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14655v1",
    "published_date": "2025-03-18 19:03:27 UTC",
    "updated_date": "2025-03-18 19:03:27 UTC"
  },
  {
    "arxiv_id": "2503.14649v2",
    "title": "RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving",
    "authors": [
      "Wenqi Jiang",
      "Suvinay Subramanian",
      "Cat Graves",
      "Gustavo Alonso",
      "Amir Yazdanbakhsh",
      "Vidushi Dadu"
    ],
    "abstract": "Retrieval-augmented generation (RAG), which combines large language models\n(LLMs) with retrievals from external knowledge databases, is emerging as a\npopular approach for reliable LLM serving. However, efficient RAG serving\nremains an open challenge due to the rapid emergence of many RAG variants and\nthe substantial differences in workload characteristics across them. In this\npaper, we make three fundamental contributions to advancing RAG serving. First,\nwe introduce RAGSchema, a structured abstraction that captures the wide range\nof RAG algorithms, serving as a foundation for performance optimization.\nSecond, we analyze several representative RAG workloads with distinct\nRAGSchema, revealing significant performance variability across these\nworkloads. Third, to address this variability and meet diverse performance\nrequirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a\nsystem optimization framework for efficient RAG serving. Our evaluation shows\nthat RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in\ntime-to-first-token latency compared to RAG systems built on LLM-system\nextensions.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.DC",
      "C.1; C.4; H.3"
    ],
    "primary_category": "cs.IR",
    "comment": "16 pages, 19 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.14649v2",
    "published_date": "2025-03-18 18:58:13 UTC",
    "updated_date": "2025-03-21 17:51:53 UTC"
  },
  {
    "arxiv_id": "2503.14640v1",
    "title": "Dynamic Accumulated Attention Map for Interpreting Evolution of Decision-Making in Vision Transformer",
    "authors": [
      "Yi Liao",
      "Yongsheng Gao",
      "Weichuan Zhang"
    ],
    "abstract": "Various Vision Transformer (ViT) models have been widely used for image\nrecognition tasks. However, existing visual explanation methods can not display\nthe attention flow hidden inside the inner structure of ViT models, which\nexplains how the final attention regions are formed inside a ViT for its\ndecision-making. In this paper, a novel visual explanation approach, Dynamic\nAccumulated Attention Map (DAAM), is proposed to provide a tool that can\nvisualize, for the first time, the attention flow from the top to the bottom\nthrough ViT networks. To this end, a novel decomposition module is proposed to\nconstruct and store the spatial feature information by unlocking the [class]\ntoken generated by the self-attention module of each ViT block. The module can\nalso obtain the channel importance coefficients by decomposing the\nclassification score for supervised ViT models. Because of the lack of\nclassification score in self-supervised ViT models, we propose dimension-wise\nimportance weights to compute the channel importance coefficients. Such spatial\nfeatures are linearly combined with the corresponding channel importance\ncoefficients, forming the attention map for each block. The dynamic attention\nflow is revealed by block-wisely accumulating each attention map. The\ncontribution of this work focuses on visualizing the evolution dynamic of the\ndecision-making attention for any intermediate block inside a ViT model by\nproposing a novel decomposition module and dimension-wise importance weights.\nThe quantitative and qualitative analysis consistently validate the\neffectiveness and superior capacity of the proposed DAAM for not only\ninterpreting ViT models with the fully-connected layers as the classifier but\nalso self-supervised ViT models. The code is available at\nhttps://github.com/ly9802/DynamicAccumulatedAttentionMap.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14640v1",
    "published_date": "2025-03-18 18:41:01 UTC",
    "updated_date": "2025-03-18 18:41:01 UTC"
  },
  {
    "arxiv_id": "2503.14637v1",
    "title": "Reinforcement learning-based motion imitation for physiologically plausible musculoskeletal motor control",
    "authors": [
      "Merkourios Simos",
      "Alberto Silvio Chiappa",
      "Alexander Mathis"
    ],
    "abstract": "How do humans move? The quest to understand human motion has broad\napplications in numerous fields, ranging from computer animation and motion\nsynthesis to neuroscience, human prosthetics and rehabilitation. Although\nadvances in reinforcement learning (RL) have produced impressive results in\ncapturing human motion using simplified humanoids, controlling physiologically\naccurate models of the body remains an open challenge. In this work, we present\na model-free motion imitation framework (KINESIS) to advance the understanding\nof muscle-based motor control. Using a musculoskeletal model of the lower body\nwith 80 muscle actuators and 20 DoF, we demonstrate that KINESIS achieves\nstrong imitation performance on 1.9 hours of motion capture data, is\ncontrollable by natural language through pre-trained text-to-motion generative\nmodels, and can be fine-tuned to carry out high-level tasks such as target goal\nreaching. Importantly, KINESIS generates muscle activity patterns that\ncorrelate well with human EMG activity. The physiological plausibility makes\nKINESIS a promising model for tackling challenging problems in human motor\ncontrol theory, which we highlight by investigating Bernstein's redundancy\nproblem in the context of locomotion. Code, videos and benchmarks will be\navailable at https://github.com/amathislab/Kinesis.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14637v1",
    "published_date": "2025-03-18 18:37:49 UTC",
    "updated_date": "2025-03-18 18:37:49 UTC"
  },
  {
    "arxiv_id": "2503.14630v1",
    "title": "Assessing Large Language Models for Automated Feedback Generation in Learning Programming Problem Solving",
    "authors": [
      "Priscylla Silva",
      "Evandro Costa"
    ],
    "abstract": "Providing effective feedback is important for student learning in programming\nproblem-solving. In this sense, Large Language Models (LLMs) have emerged as\npotential tools to automate feedback generation. However, their reliability and\nability to identify reasoning errors in student code remain not well\nunderstood. This study evaluates the performance of four LLMs (GPT-4o, GPT-4o\nmini, GPT-4-Turbo, and Gemini-1.5-pro) on a benchmark dataset of 45 student\nsolutions. We assessed the models' capacity to provide accurate and insightful\nfeedback, particularly in identifying reasoning mistakes. Our analysis reveals\nthat 63\\% of feedback hints were accurate and complete, while 37\\% contained\nmistakes, including incorrect line identification, flawed explanations, or\nhallucinated issues. These findings highlight the potential and limitations of\nLLMs in programming education and underscore the need for improvements to\nenhance reliability and minimize risks in educational applications.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14630v1",
    "published_date": "2025-03-18 18:31:36 UTC",
    "updated_date": "2025-03-18 18:31:36 UTC"
  },
  {
    "arxiv_id": "2503.14621v1",
    "title": "Reducing False Ventricular Tachycardia Alarms in ICU Settings: A Machine Learning Approach",
    "authors": [
      "Grace Funmilayo Farayola",
      "Akinyemi Sadeeq Akintola",
      "Oluwole Fagbohun",
      "Chukwuka Michael Oforgu",
      "Bisola Faith Kayode",
      "Christian Chimezie",
      "Temitope Kadri",
      "Abiola Oludotun",
      "Nelson Ogbeide",
      "Mgbame Michael",
      "Adeseye Ifaturoti",
      "Toyese Oloyede"
    ],
    "abstract": "False arrhythmia alarms in intensive care units (ICUs) are a significant\nchallenge, contributing to alarm fatigue and potentially compromising patient\nsafety. Ventricular tachycardia (VT) alarms are particularly difficult to\ndetect accurately due to their complex nature. This paper presents a machine\nlearning approach to reduce false VT alarms using the VTaC dataset, a benchmark\ndataset of annotated VT alarms from ICU monitors. We extract time-domain and\nfrequency-domain features from waveform data, preprocess the data, and train\ndeep learning models to classify true and false VT alarms. Our results\ndemonstrate high performance, with ROC-AUC scores exceeding 0.96 across various\ntraining configurations. This work highlights the potential of machine learning\nto improve the accuracy of VT alarm detection in clinical settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint, Accepted to the International Conference on Machine\n  Learning Technologies (ICMLT 2025), Helsinki, Finland",
    "pdf_url": "http://arxiv.org/pdf/2503.14621v1",
    "published_date": "2025-03-18 18:18:38 UTC",
    "updated_date": "2025-03-18 18:18:38 UTC"
  },
  {
    "arxiv_id": "2503.14604v1",
    "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives",
    "authors": [
      "Sara Sarto",
      "Marcella Cornia",
      "Rita Cucchiara"
    ],
    "abstract": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Repo GitHub:\n  https://github.com/aimagelab/awesome-captioning-evaluation",
    "pdf_url": "http://arxiv.org/pdf/2503.14604v1",
    "published_date": "2025-03-18 18:03:56 UTC",
    "updated_date": "2025-03-18 18:03:56 UTC"
  },
  {
    "arxiv_id": "2503.14505v1",
    "title": "MusicInfuser: Making Video Diffusion Listen and Dance",
    "authors": [
      "Susung Hong",
      "Ira Kemelmacher-Shlizerman",
      "Brian Curless",
      "Steven M. Seitz"
    ],
    "abstract": "We introduce MusicInfuser, an approach for generating high-quality dance\nvideos that are synchronized to a specified music track. Rather than attempting\nto design and train a new multimodal audio-video model, we show how existing\nvideo diffusion models can be adapted to align with musical inputs by\nintroducing lightweight music-video cross-attention and a low-rank adapter.\nUnlike prior work requiring motion capture data, our approach fine-tunes only\non dance videos. MusicInfuser achieves high-quality music-driven video\ngeneration while preserving the flexibility and generative capabilities of the\nunderlying models. We introduce an evaluation framework using Video-LLMs to\nassess multiple dimensions of dance generation quality. The project page and\ncode are available at https://susunghong.github.io/MusicInfuser.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://susunghong.github.io/MusicInfuser",
    "pdf_url": "http://arxiv.org/pdf/2503.14505v1",
    "published_date": "2025-03-18 17:59:58 UTC",
    "updated_date": "2025-03-18 17:59:58 UTC"
  },
  {
    "arxiv_id": "2503.14503v1",
    "title": "The Power of Context: How Multimodality Improves Image Super-Resolution",
    "authors": [
      "Kangfu Mei",
      "Hossein Talebi",
      "Mojtaba Ardakani",
      "Vishal M. Patel",
      "Peyman Milanfar",
      "Mauricio Delbracio"
    ],
    "abstract": "Single-image super-resolution (SISR) remains challenging due to the inherent\ndifficulty of recovering fine-grained details and preserving perceptual quality\nfrom low-resolution inputs. Existing methods often rely on limited image\npriors, leading to suboptimal results. We propose a novel approach that\nleverages the rich contextual information available in multiple modalities --\nincluding depth, segmentation, edges, and text prompts -- to learn a powerful\ngenerative prior for SISR within a diffusion model framework. We introduce a\nflexible network architecture that effectively fuses multimodal information,\naccommodating an arbitrary number of input modalities without requiring\nsignificant modifications to the diffusion process. Crucially, we mitigate\nhallucinations, often introduced by text prompts, by using spatial information\nfrom other modalities to guide regional text-based conditioning. Each\nmodality's guidance strength can also be controlled independently, allowing\nsteering outputs toward different directions, such as increasing bokeh through\ndepth or adjusting object prominence via segmentation. Extensive experiments\ndemonstrate that our model surpasses state-of-the-art generative SISR methods,\nachieving superior visual quality and fidelity. See project page at\nhttps://mmsr.kfmei.com/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "accepted by CVPR2025",
    "pdf_url": "http://arxiv.org/pdf/2503.14503v1",
    "published_date": "2025-03-18 17:59:54 UTC",
    "updated_date": "2025-03-18 17:59:54 UTC"
  },
  {
    "arxiv_id": "2503.14499v1",
    "title": "Measuring AI Ability to Complete Long Tasks",
    "authors": [
      "Thomas Kwa",
      "Ben West",
      "Joel Becker",
      "Amy Deng",
      "Katharyn Garcia",
      "Max Hasin",
      "Sami Jawhar",
      "Megan Kinniment",
      "Nate Rush",
      "Sydney Von Arx",
      "Ryan Bloom",
      "Thomas Broadley",
      "Haoxing Du",
      "Brian Goodrich",
      "Nikola Jurkovic",
      "Luke Harold Miles",
      "Seraphina Nix",
      "Tao Lin",
      "Neev Parikh",
      "David Rein",
      "Lucas Jun Koba Sato",
      "Hjalmar Wijk",
      "Daniel M. Ziegler",
      "Elizabeth Barnes",
      "Lawrence Chan"
    ],
    "abstract": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark\nperformance remains unclear. To quantify the capabilities of AI systems in\nterms of human capabilities, we propose a new metric: 50%-task-completion time\nhorizon. This is the time humans typically take to complete tasks that AI\nmodels can complete with 50% success rate. We first timed humans with relevant\ndomain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter\ntasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet\nhave a 50% time horizon of around 50 minutes. Furthermore, frontier AI time\nhorizon has been doubling approximately every seven months since 2019, though\nthe trend may have accelerated in 2024. The increase in AI models' time\nhorizons seems to be primarily driven by greater reliability and ability to\nadapt to mistakes, combined with better logical reasoning and tool use\ncapabilities. We discuss the limitations of our results -- including their\ndegree of external validity -- and the implications of increased autonomy for\ndangerous capabilities. If these results generalize to real-world software\ntasks, extrapolation of this trend predicts that within 5 years, AI systems\nwill be capable of automating many software tasks that currently take humans a\nmonth.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14499v1",
    "published_date": "2025-03-18 17:59:31 UTC",
    "updated_date": "2025-03-18 17:59:31 UTC"
  },
  {
    "arxiv_id": "2503.14493v2",
    "title": "State Space Model Meets Transformer: A New Paradigm for 3D Object Detection",
    "authors": [
      "Chuxin Wang",
      "Wenfei Yang",
      "Xiang Liu",
      "Tianzhu Zhang"
    ],
    "abstract": "DETR-based methods, which use multi-layer transformer decoders to refine\nobject queries iteratively, have shown promising performance in 3D indoor\nobject detection. However, the scene point features in the transformer decoder\nremain fixed, leading to minimal contributions from later decoder layers,\nthereby limiting performance improvement. Recently, State Space Models (SSM)\nhave shown efficient context modeling ability with linear complexity through\niterative interactions between system states and inputs. Inspired by SSMs, we\npropose a new 3D object DEtection paradigm with an interactive STate space\nmodel (DEST). In the interactive SSM, we design a novel state-dependent SSM\nparameterization method that enables system states to effectively serve as\nqueries in 3D indoor detection tasks. In addition, we introduce four key\ndesigns tailored to the characteristics of point cloud and SSM: The\nserialization and bidirectional scanning strategies enable bidirectional\nfeature interaction among scene points within the SSM. The inter-state\nattention mechanism models the relationships between state points, while the\ngated feed-forward network enhances inter-channel correlations. To the best of\nour knowledge, this is the first method to model queries as system states and\nscene points as system inputs, which can simultaneously update scene point\nfeatures and query features with linear complexity. Extensive experiments on\ntwo challenging datasets demonstrate the effectiveness of our DEST-based\nmethod. Our method improves the GroupFree baseline in terms of AP50 on ScanNet\nV2 (+5.3) and SUN RGB-D (+3.2) datasets. Based on the VDETR baseline, Our\nmethod sets a new SOTA on the ScanNetV2 and SUN RGB-D datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICLR 2025. Project url:\n  https://chuxwa.github.io/project_DEST/",
    "pdf_url": "http://arxiv.org/pdf/2503.14493v2",
    "published_date": "2025-03-18 17:58:03 UTC",
    "updated_date": "2025-03-19 14:10:18 UTC"
  },
  {
    "arxiv_id": "2503.14492v1",
    "title": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control",
    "authors": [
      "NVIDIA",
      ":",
      "Hassan Abu Alhaija",
      "Jose Alvarez",
      "Maciej Bala",
      "Tiffany Cai",
      "Tianshi Cao",
      "Liz Cha",
      "Joshua Chen",
      "Mike Chen",
      "Francesco Ferroni",
      "Sanja Fidler",
      "Dieter Fox",
      "Yunhao Ge",
      "Jinwei Gu",
      "Ali Hassani",
      "Michael Isaev",
      "Pooya Jannaty",
      "Shiyi Lan",
      "Tobias Lasser",
      "Huan Ling",
      "Ming-Yu Liu",
      "Xian Liu",
      "Yifan Lu",
      "Alice Luo",
      "Qianli Ma",
      "Hanzi Mao",
      "Fabio Ramos",
      "Xuanchi Ren",
      "Tianchang Shen",
      "Shitao Tang",
      "Ting-Chun Wang",
      "Jay Wu",
      "Jiashu Xu",
      "Stella Xu",
      "Kevin Xie",
      "Yuchong Ye",
      "Xiaodong Yang",
      "Xiaohui Zeng",
      "Yu Zeng"
    ],
    "abstract": "We introduce Cosmos-Transfer, a conditional world generation model that can\ngenerate world simulations based on multiple spatial control inputs of various\nmodalities such as segmentation, depth, and edge. In the design, the spatial\nconditional scheme is adaptive and customizable. It allows weighting different\nconditional inputs differently at different spatial locations. This enables\nhighly controllable world generation and finds use in various world-to-world\ntransfer use cases, including Sim2Real. We conduct extensive evaluations to\nanalyze the proposed model and demonstrate its applications for Physical AI,\nincluding robotics Sim2Real and autonomous vehicle data enrichment. We further\ndemonstrate an inference scaling strategy to achieve real-time world generation\nwith an NVIDIA GB200 NVL72 rack. To help accelerate research development in the\nfield, we open-source our models and code at\nhttps://github.com/nvidia-cosmos/cosmos-transfer1.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14492v1",
    "published_date": "2025-03-18 17:57:54 UTC",
    "updated_date": "2025-03-18 17:57:54 UTC"
  },
  {
    "arxiv_id": "2503.14488v1",
    "title": "Engineering Scientific Assistants using Interactive Structured Induction of Programs",
    "authors": [
      "Shraddha Surana",
      "Ashwin Srinivasan"
    ],
    "abstract": "We are interested in the construction of software that can act as scientific\nassistants to domain specialists. It is expected that such assistants will be\nneeded to accelerate the identification of ways to address complex problems\nrequiring urgent solutions. In this paper, our focus is not on a specific\nscientific problem, but on the software-engineering of such 'science\naccelerators'. Recent developments in 'No Code' techniques would seem to\nsuggest that scientist can simply hypothesise solutions simply by conversing\nwith a large language model (LLM). However, for complex scientific problems,\nthis seems unlikely given the current state of LLM technology. What does appear\nfeasible is that a software engineer can use LLMs to rapidly construct programs\nfor use by a domain-specialist, including the specialist's requirements\nexpressed in natural language. We propose the design of an interactive form of\n'structured' inductive programming in which a software-engineer and an LLM\ncollaboratively construct an 'assistant' for a scientific data analysis. The\npaper describes a simple implementation called iStrucInd that adapts a '2-way\nIntelligibility' protocol to implement the interaction between the software\nengineer and the LLM. We test the tool on two different non-trivial scientific\ndata analysis tasks. Specifically, we compare the system constructed by\niStrucInd against systems constructed manually and by Low Code/No Code methods\nalong dimensions of: (a) program performance; (b) program quality; and (c)\nprogramming effort. The results show iStrucInd allows a software engineer to\ndevelop better programs faster suggesting interactive structured induction can\nplay a useful role in the rapid construction of scientific assistants.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14488v1",
    "published_date": "2025-03-18 17:57:16 UTC",
    "updated_date": "2025-03-18 17:57:16 UTC"
  },
  {
    "arxiv_id": "2503.14487v1",
    "title": "DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers",
    "authors": [
      "Minglei Shi",
      "Ziyang Yuan",
      "Haotian Yang",
      "Xintao Wang",
      "Mingwu Zheng",
      "Xin Tao",
      "Wenliang Zhao",
      "Wenzhao Zheng",
      "Jie Zhou",
      "Jiwen Lu",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai"
    ],
    "abstract": "Diffusion models have demonstrated remarkable success in various image\ngeneration tasks, but their performance is often limited by the uniform\nprocessing of inputs across varying conditions and noise levels. To address\nthis limitation, we propose a novel approach that leverages the inherent\nheterogeneity of the diffusion process. Our method, DiffMoE, introduces a\nbatch-level global token pool that enables experts to access global token\ndistributions during training, promoting specialized expert behavior. To\nunleash the full potential of the diffusion process, DiffMoE incorporates a\ncapacity predictor that dynamically allocates computational resources based on\nnoise levels and sample complexity. Through comprehensive evaluation, DiffMoE\nachieves state-of-the-art performance among diffusion models on ImageNet\nbenchmark, substantially outperforming both dense architectures with 3x\nactivated parameters and existing MoE approaches while maintaining 1x activated\nparameters. The effectiveness of our approach extends beyond class-conditional\ngeneration to more challenging tasks such as text-to-image generation,\ndemonstrating its broad applicability across different diffusion model\napplications. Project Page: https://shiml20.github.io/DiffMoE/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://shiml20.github.io/DiffMoE/",
    "pdf_url": "http://arxiv.org/pdf/2503.14487v1",
    "published_date": "2025-03-18 17:57:07 UTC",
    "updated_date": "2025-03-18 17:57:07 UTC"
  },
  {
    "arxiv_id": "2503.14484v1",
    "title": "Gricean Norms as a Basis for Effective Collaboration",
    "authors": [
      "Fardin Saad",
      "Pradeep K. Murukannaiah",
      "Munindar P. Singh"
    ],
    "abstract": "Effective human-AI collaboration hinges not only on the AI agent's ability to\nfollow explicit instructions but also on its capacity to navigate ambiguity,\nincompleteness, invalidity, and irrelevance in communication. Gricean\nconversational and inference norms facilitate collaboration by aligning unclear\ninstructions with cooperative principles. We propose a normative framework that\nintegrates Gricean norms and cognitive frameworks -- common ground, relevance\ntheory, and theory of mind -- into large language model (LLM) based agents. The\nnormative framework adopts the Gricean maxims of quantity, quality, relation,\nand manner, along with inference, as Gricean norms to interpret unclear\ninstructions, which are: ambiguous, incomplete, invalid, or irrelevant. Within\nthis framework, we introduce Lamoids, GPT-4 powered agents designed to\ncollaborate with humans. To assess the influence of Gricean norms in human-AI\ncollaboration, we evaluate two versions of a Lamoid: one with norms and one\nwithout. In our experiments, a Lamoid collaborates with a human to achieve\nshared goals in a grid world (Doors, Keys, and Gems) by interpreting both clear\nand unclear natural language instructions. Our results reveal that the Lamoid\nwith Gricean norms achieves higher task accuracy and generates clearer, more\naccurate, and contextually relevant responses than the Lamoid without norms.\nThis improvement stems from the normative framework, which enhances the agent's\npragmatic reasoning, fostering effective human-AI collaboration and enabling\ncontext-aware communication in LLM-based agents.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted to AAMAS 2025. 8 pages (excl. references), 9 figures/tables.\n  (Appendix: 5 pages, 6 figures/tables). Code available at:\n  https://github.com/fardinsaad/Gricean-Norms",
    "pdf_url": "http://arxiv.org/pdf/2503.14484v1",
    "published_date": "2025-03-18 17:54:14 UTC",
    "updated_date": "2025-03-18 17:54:14 UTC"
  },
  {
    "arxiv_id": "2503.14469v1",
    "title": "Attribution Score Alignment in Explainable Data Management",
    "authors": [
      "Felipe Azua",
      "Leopoldo Bertossi"
    ],
    "abstract": "Different attribution-scores have been proposed to quantify the relevance of\ndatabase tuples for a query answer from a database. Among them, we find Causal\nResponsibility, the Shapley Value, the Banzhaf Power-Index, and the Causal\nEffect. They have been analyzed in isolation, mainly in terms of computational\nproperties. In this work, we start an investigation into the alignment of these\nscores on the basis of the queries at hand; that is, on whether they induce\ncompatible rankings of tuples. We are able to identify vast classes of queries\nfor which some pairs of scores are always aligned, and others for which they\nare not. It turns out that the presence of exogenous tuples makes a crucial\ndifference in this regard.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14469v1",
    "published_date": "2025-03-18 17:45:32 UTC",
    "updated_date": "2025-03-18 17:45:32 UTC"
  },
  {
    "arxiv_id": "2503.14456v1",
    "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
    "authors": [
      "Bo Peng",
      "Ruichong Zhang",
      "Daniel Goldstein",
      "Eric Alcaide",
      "Haowen Hou",
      "Janna Lu",
      "William Merrill",
      "Guangyu Song",
      "Kaifeng Tan",
      "Saiteja Utpala",
      "Nathan Wilce",
      "Johan S. Wind",
      "Tianyi Wu",
      "Daniel Wuttke",
      "Christian Zhou-Zheng"
    ],
    "abstract": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with\npre-trained language models that establish a new state-of-the-art in downstream\nperformance at the 3 billion parameter scale on multilingual tasks, and match\ncurrent SoTA English language performance despite being trained on dramatically\nfewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only\nconstant memory usage and constant inference time per token. RWKV-7 introduces\na newly generalized formulation of the delta rule with vector-valued gating and\nin-context learning rates, as well as a relaxed value replacement rule. We show\nthat RWKV-7 can perform state tracking and recognize all regular languages,\nwhile retaining parallelizability of training. This exceeds the capabilities of\nTransformers under standard complexity conjectures, which are limited to\n$\\mathsf{TC}^0$. To demonstrate RWKV-7's language modeling capability, we also\npresent an extended open source 3.1 trillion token multilingual corpus, and\ntrain four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on\nthis dataset.\n  To foster openness, reproduction, and adoption, we release our models and\ndataset component listing at https://huggingface.co/RWKV, and our training and\ninference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0\nLicense.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.0; I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14456v1",
    "published_date": "2025-03-18 17:31:05 UTC",
    "updated_date": "2025-03-18 17:31:05 UTC"
  },
  {
    "arxiv_id": "2503.14448v1",
    "title": "Pauli Network Circuit Synthesis with Reinforcement Learning",
    "authors": [
      "Ayushi Dubal",
      "David Kremer",
      "Simon Martiel",
      "Victor Villar",
      "Derek Wang",
      "Juan Cruz-Benito"
    ],
    "abstract": "We introduce a Reinforcement Learning (RL)-based method for re-synthesis of\nquantum circuits containing arbitrary Pauli rotations alongside Clifford\noperations. By collapsing each sub-block to a compact representation and then\nsynthesizing it step-by-step through a learned heuristic, we obtain circuits\nthat are both shorter and compliant with hardware connectivity constraints. We\nfind that the method is fast enough and good enough to work as an optimization\nprocedure: in direct comparisons on 6-qubit random Pauli Networks against\nstate-of-the-art heuristic methods, our RL approach yields over 2x reduction in\ntwo-qubit gate count, while executing in under 10 milliseconds per circuit. We\nfurther integrate the method into a collect-and-re-synthesize pipeline, applied\nas a Qiskit transpiler pass, where we observe average improvements of 20% in\ntwo-qubit gate count and depth, reaching up to 60% for many instances, across\nthe Benchpress benchmark. These results highlight the potential of RL-driven\nsynthesis to significantly improve circuit quality in realistic, large-scale\nquantum transpilation workloads.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14448v1",
    "published_date": "2025-03-18 17:27:50 UTC",
    "updated_date": "2025-03-18 17:27:50 UTC"
  },
  {
    "arxiv_id": "2503.14434v1",
    "title": "LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as Evolutionary Optimizers",
    "authors": [
      "Nikhil Abhyankar",
      "Parshin Shojaee",
      "Chandan K. Reddy"
    ],
    "abstract": "Automated feature engineering plays a critical role in improving predictive\nmodel performance for tabular learning tasks. Traditional automated feature\nengineering methods are limited by their reliance on pre-defined\ntransformations within fixed, manually designed search spaces, often neglecting\ndomain knowledge. Recent advances using Large Language Models (LLMs) have\nenabled the integration of domain knowledge into the feature engineering\nprocess. However, existing LLM-based approaches use direct prompting or rely\nsolely on validation scores for feature selection, failing to leverage insights\nfrom prior feature discovery experiments or establish meaningful reasoning\nbetween feature generation and data-driven performance. To address these\nchallenges, we propose LLM-FE, a novel framework that combines evolutionary\nsearch with the domain knowledge and reasoning capabilities of LLMs to\nautomatically discover effective features for tabular learning tasks. LLM-FE\nformulates feature engineering as a program search problem, where LLMs propose\nnew feature transformation programs iteratively, and data-driven feedback\nguides the search process. Our results demonstrate that LLM-FE consistently\noutperforms state-of-the-art baselines, significantly enhancing the performance\nof tabular prediction models across diverse classification and regression\nbenchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14434v1",
    "published_date": "2025-03-18 17:11:24 UTC",
    "updated_date": "2025-03-18 17:11:24 UTC"
  },
  {
    "arxiv_id": "2503.14432v1",
    "title": "PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play",
    "authors": [
      "Wei Fang",
      "Yang Zhang",
      "Kaizhi Qian",
      "James Glass",
      "Yada Zhu"
    ],
    "abstract": "Large language models (LLMs) are increasingly integrated with specialized\nexternal tools, yet many tasks demand zero-shot tool usage with minimal or\nnoisy documentation. Existing solutions rely on manual rewriting or labeled\ndata for validation, making them inapplicable in true zero-shot settings. To\naddress these challenges, we propose PLAY2PROMPT, an automated framework that\nsystematically \"plays\" with each tool to explore its input-output behaviors.\nThrough this iterative trial-and-error process, PLAY2PROMPT refines tool\ndocumentation and generates usage examples without any labeled data. These\nexamples not only guide LLM inference but also serve as validation to further\nenhance tool utilization. Extensive experiments on real-world tasks demonstrate\nthat PLAY2PROMPT significantly improves zero-shot tool performance across both\nopen and closed models, offering a scalable and effective solution for\ndomain-specific tool integration.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14432v1",
    "published_date": "2025-03-18 17:09:57 UTC",
    "updated_date": "2025-03-18 17:09:57 UTC"
  },
  {
    "arxiv_id": "2503.14428v1",
    "title": "MagicComp: Training-free Dual-Phase Refinement for Compositional Video Generation",
    "authors": [
      "Hongyu Zhang",
      "Yufan Deng",
      "Shenghai Yuan",
      "Peng Jin",
      "Zesen Cheng",
      "Yian Zhao",
      "Chang Liu",
      "Jie Chen"
    ],
    "abstract": "Text-to-video (T2V) generation has made significant strides with diffusion\nmodels. However, existing methods still struggle with accurately binding\nattributes, determining spatial relationships, and capturing complex action\ninteractions between multiple subjects. To address these limitations, we\npropose MagicComp, a training-free method that enhances compositional T2V\ngeneration through dual-phase refinement. Specifically, (1) During the\nConditioning Stage: We introduce the Semantic Anchor Disambiguation to\nreinforces subject-specific semantics and resolve inter-subject ambiguity by\nprogressively injecting the directional vectors of semantic anchors into\noriginal text embedding; (2) During the Denoising Stage: We propose Dynamic\nLayout Fusion Attention, which integrates grounding priors and model-adaptive\nspatial perception to flexibly bind subjects to their spatiotemporal regions\nthrough masked attention modulation. Furthermore, MagicComp is a model-agnostic\nand versatile approach, which can be seamlessly integrated into existing T2V\narchitectures. Extensive experiments on T2V-CompBench and VBench demonstrate\nthat MagicComp outperforms state-of-the-art methods, highlighting its potential\nfor applications such as complex prompt-based and trajectory-controllable video\ngeneration. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project webpage: https://hong-yu-zhang.github.io/MagicComp-Page/",
    "pdf_url": "http://arxiv.org/pdf/2503.14428v1",
    "published_date": "2025-03-18 17:02:14 UTC",
    "updated_date": "2025-03-18 17:02:14 UTC"
  },
  {
    "arxiv_id": "2503.14427v2",
    "title": "VisEscape: A Benchmark for Evaluating Exploration-driven Decision-making in Virtual Escape Rooms",
    "authors": [
      "Seungwon Lim",
      "Sungwoong Kim",
      "Jihwan Yu",
      "Sungjae Lee",
      "Jiwan Chung",
      "Youngjae Yu"
    ],
    "abstract": "Escape rooms present a unique cognitive challenge that demands\nexploration-driven planning: players should actively search their environment,\ncontinuously update their knowledge based on new discoveries, and connect\ndisparate clues to determine which elements are relevant to their objectives.\nMotivated by this, we introduce VisEscape, a benchmark of 20 virtual escape\nrooms specifically designed to evaluate AI models under these challenging\nconditions, where success depends not only on solving isolated puzzles but also\non iteratively constructing and refining spatial-temporal knowledge of a\ndynamically changing environment. On VisEscape, we observe that even\nstate-of-the-art multimodal models generally fail to escape the rooms, showing\nconsiderable variation in their levels of progress and trajectories. To address\nthis issue, we propose VisEscaper, which effectively integrates Memory,\nFeedback, and ReAct modules, demonstrating significant improvements by\nperforming 3.7 times more effectively and 4.9 times more efficiently on average\ncompared to baseline agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14427v2",
    "published_date": "2025-03-18 16:59:09 UTC",
    "updated_date": "2025-03-22 05:06:18 UTC"
  },
  {
    "arxiv_id": "2503.14421v1",
    "title": "ExDDV: A New Dataset for Explainable Deepfake Detection in Video",
    "authors": [
      "Vlad Hondru",
      "Eduard Hogea",
      "Darian Onchis",
      "Radu Tudor Ionescu"
    ],
    "abstract": "The ever growing realism and quality of generated videos makes it\nincreasingly harder for humans to spot deepfake content, who need to rely more\nand more on automatic deepfake detectors. However, deepfake detectors are also\nprone to errors, and their decisions are not explainable, leaving humans\nvulnerable to deepfake-based fraud and misinformation. To this end, we\nintroduce ExDDV, the first dataset and benchmark for Explainable Deepfake\nDetection in Video. ExDDV comprises around 5.4K real and deepfake videos that\nare manually annotated with text descriptions (to explain the artifacts) and\nclicks (to point out the artifacts). We evaluate a number of vision-language\nmodels on ExDDV, performing experiments with various fine-tuning and in-context\nlearning strategies. Our results show that text and click supervision are both\nrequired to develop robust explainable models for deepfake videos, which are\nable to localize and describe the observed artifacts. Our novel dataset and\ncode to reproduce the results are available at\nhttps://github.com/vladhondru25/ExDDV.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14421v1",
    "published_date": "2025-03-18 16:55:07 UTC",
    "updated_date": "2025-03-18 16:55:07 UTC"
  },
  {
    "arxiv_id": "2503.14412v1",
    "title": "Iffy-Or-Not: Extending the Web to Support the Critical Evaluation of Fallacious Texts",
    "authors": [
      "Gionnieve Lim",
      "Juho Kim",
      "Simon T. Perrault"
    ],
    "abstract": "Social platforms have expanded opportunities for deliberation with the\ncomments being used to inform one's opinion. However, using such information to\nform opinions is challenged by unsubstantiated or false content. To enhance the\nquality of opinion formation and potentially confer resistance to\nmisinformation, we developed Iffy-Or-Not (ION), a browser extension that seeks\nto invoke critical thinking when reading texts. With three features guided by\nargumentation theory, ION highlights fallacious content, suggests diverse\nqueries to probe them with, and offers deeper questions to consider and chat\nwith others about. From a user study (N=18), we found that ION encourages users\nto be more attentive to the content, suggests queries that align with or are\npreferable to their own, and poses thought-provoking questions that expands\ntheir perspectives. However, some participants expressed aversion to ION due to\nmisalignments with their information goals and thinking predispositions.\nPotential backfiring effects with ION are discussed.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14412v1",
    "published_date": "2025-03-18 16:50:20 UTC",
    "updated_date": "2025-03-18 16:50:20 UTC"
  },
  {
    "arxiv_id": "2503.14411v1",
    "title": "Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models",
    "authors": [
      "Siwei Zhang",
      "Yun Xiong",
      "Yateng Tang",
      "Xi Chen",
      "Zian Jia",
      "Zehao Gu",
      "Jiarong Xu",
      "Jiawei Zhang"
    ],
    "abstract": "Temporal graph neural networks (TGNNs) have shown remarkable performance in\ntemporal graph modeling. However, real-world temporal graphs often possess rich\ntextual information, giving rise to temporal text-attributed graphs (TTAGs).\nSuch combination of dynamic text semantics and evolving graph structures\nintroduces heightened complexity. Existing TGNNs embed texts statically and\nrely heavily on encoding mechanisms that biasedly prioritize structural\ninformation, overlooking the temporal evolution of text semantics and the\nessential interplay between semantics and structures for synergistic\nreinforcement. To tackle these issues, we present \\textbf{{Cross}}, a novel\nframework that seamlessly extends existing TGNNs for TTAG modeling. The key\nidea is to employ the advanced large language models (LLMs) to extract the\ndynamic semantics in text space and then generate expressive representations\nunifying both semantics and structures. Specifically, we propose a Temporal\nSemantics Extractor in the {Cross} framework, which empowers the LLM to offer\nthe temporal semantic understanding of node's evolving contexts of textual\nneighborhoods, facilitating semantic dynamics. Subsequently, we introduce the\nSemantic-structural Co-encoder, which collaborates with the above Extractor for\nsynthesizing illuminating representations by jointly considering both semantic\nand structural information while encouraging their mutual reinforcement.\nExtensive experimental results on four public datasets and one practical\nindustrial dataset demonstrate {Cross}'s significant effectiveness and\nrobustness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Submit to ICML2025",
    "pdf_url": "http://arxiv.org/pdf/2503.14411v1",
    "published_date": "2025-03-18 16:50:10 UTC",
    "updated_date": "2025-03-18 16:50:10 UTC"
  },
  {
    "arxiv_id": "2503.18956v1",
    "title": "International Agreements on AI Safety: Review and Recommendations for a Conditional AI Safety Treaty",
    "authors": [
      "Rebecca Scholefield",
      "Samuel Martin",
      "Otto Barten"
    ],
    "abstract": "The malicious use or malfunction of advanced general-purpose AI (GPAI) poses\nrisks that, according to leading experts, could lead to the 'marginalisation or\nextinction of humanity.' To address these risks, there are an increasing number\nof proposals for international agreements on AI safety. In this paper, we\nreview recent (2023-) proposals, identifying areas of consensus and\ndisagreement, and drawing on related literature to assess their feasibility. We\nfocus our discussion on risk thresholds, regulations, types of international\nagreement and five related processes: building scientific consensus,\nstandardisation, auditing, verification and incentivisation.\n  Based on this review, we propose a treaty establishing a compute threshold\nabove which development requires rigorous oversight. This treaty would mandate\ncomplementary audits of models, information security and governance practices,\noverseen by an international network of AI Safety Institutes (AISIs) with\nauthority to pause development if risks are unacceptable. Our approach combines\nimmediately implementable measures with a flexible structure that can adapt to\nongoing research.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "34 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.18956v1",
    "published_date": "2025-03-18 16:29:57 UTC",
    "updated_date": "2025-03-18 16:29:57 UTC"
  },
  {
    "arxiv_id": "2503.14577v1",
    "title": "PHGNN: A Novel Prompted Hypergraph Neural Network to Diagnose Alzheimer's Disease",
    "authors": [
      "Chenyu Liu",
      "Luca Rossi"
    ],
    "abstract": "The accurate diagnosis of Alzheimer's disease (AD) and prognosis of mild\ncognitive impairment (MCI) conversion are crucial for early intervention.\nHowever, existing multimodal methods face several challenges, from the\nheterogeneity of input data, to underexplored modality interactions, missing\ndata due to patient dropouts, and limited data caused by the time-consuming and\ncostly data collection process. In this paper, we propose a novel Prompted\nHypergraph Neural Network (PHGNN) framework that addresses these limitations by\nintegrating hypergraph based learning with prompt learning. Hypergraphs capture\nhigher-order relationships between different modalities, while our prompt\nlearning approach for hypergraphs, adapted from NLP, enables efficient training\nwith limited data. Our model is validated through extensive experiments on the\nADNI dataset, outperforming SOTA methods in both AD diagnosis and the\nprediction of MCI conversion.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14577v1",
    "published_date": "2025-03-18 16:10:43 UTC",
    "updated_date": "2025-03-18 16:10:43 UTC"
  },
  {
    "arxiv_id": "2503.14376v1",
    "title": "Tiled Flash Linear Attention: More Efficient Linear RNN and xLSTM Kernels",
    "authors": [
      "Maximilian Beck",
      "Korbinian PÃ¶ppel",
      "Phillip Lippe",
      "Sepp Hochreiter"
    ],
    "abstract": "Linear RNNs with gating recently demonstrated competitive performance\ncompared to Transformers in language modeling. Although their linear compute\nscaling in sequence length offers theoretical runtime advantages over\nTransformers, realizing these benefits in practice requires optimized custom\nkernels, as Transformers rely on the highly efficient Flash Attention kernels.\nLeveraging the chunkwise-parallel formulation of linear RNNs, Flash Linear\nAttention (FLA) shows that linear RNN kernels are faster than Flash Attention,\nby parallelizing over chunks of the input sequence. However, since the chunk\nsize of FLA is limited, many intermediate states must be materialized in GPU\nmemory. This leads to low arithmetic intensity and causes high memory\nconsumption and IO cost, especially for long-context pre-training. In this\nwork, we present Tiled Flash Linear Attention (TFLA), a novel kernel algorithm\nfor linear RNNs, that enables arbitrary large chunk sizes by introducing an\nadditional level of sequence parallelization within each chunk. First, we apply\nTFLA to the xLSTM with matrix memory, the mLSTM. Second, we propose an mLSTM\nvariant with sigmoid input gate and reduced computation for even faster kernel\nruntimes at equal language modeling performance. In our speed benchmarks, we\nshow that our new mLSTM kernels based on TFLA outperform highly optimized Flash\nAttention, Linear Attention and Mamba kernels, setting a new state of the art\nfor efficient long-context sequence modeling primitives.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Code available at: https://github.com/NX-AI/mlstm_kernels",
    "pdf_url": "http://arxiv.org/pdf/2503.14376v1",
    "published_date": "2025-03-18 16:09:47 UTC",
    "updated_date": "2025-03-18 16:09:47 UTC"
  },
  {
    "arxiv_id": "2503.14576v1",
    "title": "SocialJax: An Evaluation Suite for Multi-agent Reinforcement Learning in Sequential Social Dilemmas",
    "authors": [
      "Zihao Guo",
      "Richard Willis",
      "Shuqing Shi",
      "Tristan Tomilin",
      "Joel Z. Leibo",
      "Yali Du"
    ],
    "abstract": "Social dilemmas pose a significant challenge in the field of multi-agent\nreinforcement learning (MARL). Melting Pot is an extensive framework designed\nto evaluate social dilemma environments, providing an evaluation protocol that\nmeasures generalization to new social partners across various test scenarios.\nHowever, running reinforcement learning algorithms in the official Melting Pot\nenvironments demands substantial computational resources. In this paper, we\nintroduce SocialJax, a suite of sequential social dilemma environments\nimplemented in JAX. JAX is a high-performance numerical computing library for\nPython that enables significant improvements in the operational efficiency of\nSocialJax on GPUs and TPUs. Our experiments demonstrate that the training\npipeline of SocialJax achieves a 50\\texttimes{} speedup in real-time\nperformance compared to Melting Pot's RLlib baselines. Additionally, we\nvalidate the effectiveness of baseline algorithms within the SocialJax\nenvironments. Finally, we use Schelling diagrams to verify the social dilemma\nproperties of these environments, ensuring they accurately capture the dynamics\nof social dilemmas.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 18 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2503.14576v1",
    "published_date": "2025-03-18 16:03:59 UTC",
    "updated_date": "2025-03-18 16:03:59 UTC"
  },
  {
    "arxiv_id": "2503.14354v1",
    "title": "Retrospective: A CORDIC Based Configurable Activation Function for NN Applications",
    "authors": [
      "Omkar Kokane",
      "Gopal Raut",
      "Salim Ullah",
      "Mukul Lokhande",
      "Adam Teman",
      "Akash Kumar",
      "Santosh Kumar Vishvakarma"
    ],
    "abstract": "A CORDIC-based configuration for the design of Activation Functions (AF) was\npreviously suggested to accelerate ASIC hardware design for\nresource-constrained systems by providing functional reconfigurability. Since\nits introduction, this new approach for neural network acceleration has gained\nwidespread popularity, influencing numerous designs for activation functions in\nboth academic and commercial AI processors. In this retrospective analysis, we\nexplore the foundational aspects of this initiative, summarize key developments\nover recent years, and introduce the DA-VINCI AF tailored for the evolving\nneeds of AI applications. This new generation of dynamically configurable and\nprecision-adjustable activation function cores promise greater adaptability for\na range of activation functions in AI workloads, including Swish, SoftMax,\nSeLU, and GeLU, utilizing the Shift-and-Add CORDIC technique. The previously\npresented design has been optimized for MAC, Sigmoid, and Tanh functionalities\nand incorporated into ReLU AFs, culminating in an accumulative NEURIC compute\nunit. These enhancements position NEURIC as a fundamental component in the\nresource-efficient vector engine for the realization of AI accelerators that\nfocus on DNNs, RNNs/LSTMs, and Transformers, achieving a quality of results\n(QoR) of 98.5%.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.CV",
      "cs.ET",
      "eess.IV"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14354v1",
    "published_date": "2025-03-18 15:38:37 UTC",
    "updated_date": "2025-03-18 15:38:37 UTC"
  },
  {
    "arxiv_id": "2503.14350v2",
    "title": "VEGGIE: Instructional Editing and Reasoning of Video Concepts with Grounded Generation",
    "authors": [
      "Shoubin Yu",
      "Difan Liu",
      "Ziqiao Ma",
      "Yicong Hong",
      "Yang Zhou",
      "Hao Tan",
      "Joyce Chai",
      "Mohit Bansal"
    ],
    "abstract": "Recent video diffusion models have enhanced video editing, but it remains\nchallenging to handle instructional editing and diverse tasks (e.g., adding,\nremoving, changing) within a unified framework. In this paper, we introduce\nVEGGIE, a Video Editor with Grounded Generation from Instructions, a simple\nend-to-end framework that unifies video concept editing, grounding, and\nreasoning based on diverse user instructions. Specifically, given a video and\ntext query, VEGGIE first utilizes an MLLM to interpret user intentions in\ninstructions and ground them to the video contexts, generating frame-specific\ngrounded task queries for pixel-space responses. A diffusion model then renders\nthese plans and generates edited videos that align with user intent. To support\ndiverse tasks and complex instructions, we employ a curriculum learning\nstrategy: first aligning the MLLM and video diffusion model with large-scale\ninstructional image editing data, followed by end-to-end fine-tuning on\nhigh-quality multitask video data. Additionally, we introduce a novel data\nsynthesis pipeline to generate paired instructional video editing data for\nmodel training. It transforms static image data into diverse, high-quality\nvideo editing samples by leveraging Image-to-Video models to inject dynamics.\nVEGGIE shows strong performance in instructional video editing with different\nediting skills, outperforming the best instructional baseline as a versatile\nmodel, while other models struggle with multi-tasking. VEGGIE also excels in\nvideo object grounding and reasoning segmentation, where other baselines fail.\nWe further reveal how the multiple tasks help each other and highlight\npromising applications like zero-shot multimodal instructional and in-context\nvideo editing.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "First three authors contributed equally. Project page:\n  https://veggie-gen.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2503.14350v2",
    "published_date": "2025-03-18 15:31:12 UTC",
    "updated_date": "2025-03-19 20:33:40 UTC"
  },
  {
    "arxiv_id": "2503.16534v1",
    "title": "Gender and content bias in Large Language Models: a case study on Google Gemini 2.0 Flash Experimental",
    "authors": [
      "Roberto Balestri"
    ],
    "abstract": "This study evaluates the biases in Gemini 2.0 Flash Experimental, a\nstate-of-the-art large language model (LLM) developed by Google, focusing on\ncontent moderation and gender disparities. By comparing its performance to\nChatGPT-4o, examined in a previous work of the author, the analysis highlights\nsome differences in ethical moderation practices. Gemini 2.0 demonstrates\nreduced gender bias, notably with female-specific prompts achieving a\nsubstantial rise in acceptance rates compared to results obtained by\nChatGPT-4o. It adopts a more permissive stance toward sexual content and\nmaintains relatively high acceptance rates for violent prompts, including\ngender-specific cases. Despite these changes, whether they constitute an\nimprovement is debatable. While gender bias has been reduced, this reduction\ncomes at the cost of permitting more violent content toward both males and\nfemales, potentially normalizing violence rather than mitigating harm.\nMale-specific prompts still generally receive higher acceptance rates than\nfemale-specific ones. These findings underscore the complexities of aligning AI\nsystems with ethical standards, highlighting progress in reducing certain\nbiases while raising concerns about the broader implications of the model's\npermissiveness. Ongoing refinements are essential to achieve moderation\npractices that ensure transparency, fairness, and inclusivity without\namplifying harmful content.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16534v1",
    "published_date": "2025-03-18 15:28:22 UTC",
    "updated_date": "2025-03-18 15:28:22 UTC"
  },
  {
    "arxiv_id": "2503.14345v2",
    "title": "MoonCast: High-Quality Zero-Shot Podcast Generation",
    "authors": [
      "Zeqian Ju",
      "Dongchao Yang",
      "Jianwei Yu",
      "Kai Shen",
      "Yichong Leng",
      "Zhengtao Wang",
      "Xu Tan",
      "Xinyu Zhou",
      "Tao Qin",
      "Xiangyang Li"
    ],
    "abstract": "Recent advances in text-to-speech synthesis have achieved notable success in\ngenerating high-quality short utterances for individual speakers. However,\nthese systems still face challenges when extending their capabilities to long,\nmulti-speaker, and spontaneous dialogues, typical of real-world scenarios such\nas podcasts. These limitations arise from two primary challenges: 1) long\nspeech: podcasts typically span several minutes, exceeding the upper limit of\nmost existing work; 2) spontaneity: podcasts are marked by their spontaneous,\noral nature, which sharply contrasts with formal, written contexts; existing\nworks often fall short in capturing this spontaneity. In this paper, we propose\nMoonCast, a solution for high-quality zero-shot podcast generation, aiming to\nsynthesize natural podcast-style speech from text-only sources (e.g., stories,\ntechnical reports, news in TXT, PDF, or Web URL formats) using the voices of\nunseen speakers. To generate long audio, we adopt a long-context language\nmodel-based audio modeling approach utilizing large-scale long-context speech\ndata. To enhance spontaneity, we utilize a podcast generation module to\ngenerate scripts with spontaneous details, which have been empirically shown to\nbe as crucial as the text-to-speech modeling itself. Experiments demonstrate\nthat MoonCast outperforms baselines, with particularly notable improvements in\nspontaneity and coherence.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14345v2",
    "published_date": "2025-03-18 15:25:08 UTC",
    "updated_date": "2025-03-19 07:17:41 UTC"
  },
  {
    "arxiv_id": "2503.14341v1",
    "title": "Spatio-Temporal Graph Neural Networks for Infant Language Acquisition Prediction",
    "authors": [
      "Andrew Roxburgh",
      "Floriana Grasso",
      "Terry R. Payne"
    ],
    "abstract": "Predicting the words that a child is going to learn next can be useful for\nboosting language acquisition, and such predictions have been shown to be\npossible with both neural network techniques (looking at changes in the\nvocabulary state over time) and graph model (looking at data pertaining to the\nrelationships between words). However, these models do not fully capture the\ncomplexity of the language learning process of an infant when used in\nisolation. In this paper, we examine how a model of language acquisition for\ninfants and young children can be constructed and adapted for use in a\nSpatio-Temporal Graph Convolutional Network (STGCN), taking into account the\ndifferent types of linguistic relationships that occur during child language\nlearning. We introduce a novel approach for predicting child vocabulary\nacquisition, and evaluate the efficacy of such a model with respect to the\ndifferent types of linguistic relationships that occur during language\nacquisition, resulting in insightful observations on model calibration and norm\nselection. An evaluation of this model found that the mean accuracy of models\nfor predicting new words when using sensorimotor relationships (0.733) and\nsemantic relationships (0.729) were found to be superior to that observed with\na 2-layer Feed-forward neural network. Furthermore, the high recall for some\nrelationships suggested that some relationships (e.g. visual) were superior in\nidentifying a larger proportion of relevant words that a child should\nsubsequently learn than others (such as auditory).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14341v1",
    "published_date": "2025-03-18 15:21:27 UTC",
    "updated_date": "2025-03-18 15:21:27 UTC"
  },
  {
    "arxiv_id": "2503.15551v1",
    "title": "Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting Attack",
    "authors": [
      "Murong Yue",
      "Ziyu Yao"
    ],
    "abstract": "Batch prompting, which combines a batch of multiple queries sharing the same\ncontext in one inference, has emerged as a promising solution to reduce\ninference costs. However, our study reveals a significant security\nvulnerability in batch prompting: malicious users can inject attack\ninstructions into a batch, leading to unwanted interference across all queries,\nwhich can result in the inclusion of harmful content, such as phishing links,\nor the disruption of logical reasoning. In this paper, we construct\nBATCHSAFEBENCH, a comprehensive benchmark comprising 150 attack instructions of\ntwo types and 8k batch instances, to study the batch prompting vulnerability\nsystematically. Our evaluation of both closed-source and open-weight LLMs\ndemonstrates that all LLMs are susceptible to batch-prompting attacks. We then\nexplore multiple defending approaches. While the prompting-based defense shows\nlimited effectiveness for smaller LLMs, the probing-based approach achieves\nabout 95% accuracy in detecting attacks. Additionally, we perform a mechanistic\nanalysis to understand the attack and identify attention heads that are\nresponsible for it.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15551v1",
    "published_date": "2025-03-18 15:16:10 UTC",
    "updated_date": "2025-03-18 15:16:10 UTC"
  },
  {
    "arxiv_id": "2503.14333v1",
    "title": "Revealing higher-order neural representations with generative artificial intelligence",
    "authors": [
      "Hojjat Azimi Asrari",
      "Megan A. K. Peters"
    ],
    "abstract": "Studies often aim to reveal how neural representations encode aspects of an\nobserver's environment, such as its contents or structure. These are\n``first-order\" representations (FORs), because they're ``about\" the external\nworld. A less-common target is ``higher-order\" representations (HORs), which\nare ``about\" FORs -- their contents, stability, or uncertainty. HORs of\nuncertainty appear critically involved in adaptive behaviors including learning\nunder uncertainty, influencing learning rates and internal model updating based\non environmental feedback. However, HORs about uncertainty are unlikely to be\ndirect ``read-outs\" of FOR characteristics, instead reflecting estimation\nprocesses which may be lossy, bias-prone, or distortive and which may also\nincorporate estimates of distributions of uncertainty the observer is likely to\nexperience. While some research has targeted neural representations of\n``instantaneously\" estimated uncertainty, how the brain represents\n\\textit{distributions} of expected uncertainty remains largely unexplored.\nHere, we propose a novel reinforcement learning (RL) based generative\nartificial intelligence (genAI) approach to explore neural representations of\nuncertainty distributions. We use existing functional magnetic resonance\nimaging data, where humans learned to `de-noise' their brain states to achieve\ntarget neural patterns, to train denoising diffusion genAI models with RL\nalgorithms to learn noise distributions similar to how humans might learn to do\nthe same. We then explore these models' learned noise-distribution HORs\ncompared to control models trained with traditional backpropagation. Results\nreveal model-dependent differences in noise distribution representations --\nwith the RL-based model offering much higher explanatory power for human\nbehavior -- offering an exciting path towards using genAI to explore neural\nnoise-distribution HORs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14333v1",
    "published_date": "2025-03-18 15:08:19 UTC",
    "updated_date": "2025-03-18 15:08:19 UTC"
  },
  {
    "arxiv_id": "2503.14321v1",
    "title": "COPA: Comparing the Incomparable to Explore the Pareto Front",
    "authors": [
      "AdriÃ¡n Javaloy",
      "Antonio Vergari",
      "Isabel Valera"
    ],
    "abstract": "In machine learning (ML), it is common to account for multiple objectives\nwhen, e.g., selecting a model to deploy. However, it is often unclear how one\nshould compare, aggregate and, ultimately, trade-off these objectives, as they\nmight be measured in different units or scales. For example, when deploying\nlarge language models (LLMs), we might not only care about their performance,\nbut also their CO2 consumption. In this work, we investigate how objectives can\nbe sensibly compared and aggregated to navigate their Pareto front. To do so,\nwe propose to make incomparable objectives comparable via their CDFs,\napproximated by their relative rankings. This allows us to aggregate them while\nmatching user-specific preferences, allowing practitioners to meaningfully\nnavigate and search for models in the Pareto front. We demonstrate the\npotential impact of our methodology in diverse areas such as LLM selection,\ndomain generalization, and AutoML benchmarking, where classical ways to\naggregate and normalize objectives fail.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 14 figures. Under submission",
    "pdf_url": "http://arxiv.org/pdf/2503.14321v1",
    "published_date": "2025-03-18 14:51:42 UTC",
    "updated_date": "2025-03-18 14:51:42 UTC"
  },
  {
    "arxiv_id": "2503.16533v1",
    "title": "From Patient Consultations to Graphs: Leveraging LLMs for Patient Journey Knowledge Graph Construction",
    "authors": [
      "Hassan S. Al Khatib",
      "Sudip Mittal",
      "Shahram Rahimi",
      "Nina Marhamati",
      "Sean Bozorgzad"
    ],
    "abstract": "The transition towards patient-centric healthcare necessitates a\ncomprehensive understanding of patient journeys, which encompass all healthcare\nexperiences and interactions across the care spectrum. Existing healthcare data\nsystems are often fragmented and lack a holistic representation of patient\ntrajectories, creating challenges for coordinated care and personalized\ninterventions. Patient Journey Knowledge Graphs (PJKGs) represent a novel\napproach to addressing the challenge of fragmented healthcare data by\nintegrating diverse patient information into a unified, structured\nrepresentation. This paper presents a methodology for constructing PJKGs using\nLarge Language Models (LLMs) to process and structure both formal clinical\ndocumentation and unstructured patient-provider conversations. These graphs\nencapsulate temporal and causal relationships among clinical encounters,\ndiagnoses, treatments, and outcomes, enabling advanced temporal reasoning and\npersonalized care insights. The research evaluates four different LLMs, such as\nClaude 3.5, Mistral, Llama 3.1, and Chatgpt4o, in their ability to generate\naccurate and computationally efficient knowledge graphs. Results demonstrate\nthat while all models achieved perfect structural compliance, they exhibited\nvariations in medical entity processing and computational efficiency. The paper\nconcludes by identifying key challenges and future research directions. This\nwork contributes to advancing patient-centric healthcare through the\ndevelopment of comprehensive, actionable knowledge graphs that support improved\ncare coordination and outcome prediction.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16533v1",
    "published_date": "2025-03-18 14:44:28 UTC",
    "updated_date": "2025-03-18 14:44:28 UTC"
  },
  {
    "arxiv_id": "2503.14295v2",
    "title": "PC-Talk: Precise Facial Animation Control for Audio-Driven Talking Face Generation",
    "authors": [
      "Baiqin Wang",
      "Xiangyu Zhu",
      "Fan Shen",
      "Hao Xu",
      "Zhen Lei"
    ],
    "abstract": "Recent advancements in audio-driven talking face generation have made great\nprogress in lip synchronization. However, current methods often lack sufficient\ncontrol over facial animation such as speaking style and emotional expression,\nresulting in uniform outputs. In this paper, we focus on improving two key\nfactors: lip-audio alignment and emotion control, to enhance the diversity and\nuser-friendliness of talking videos. Lip-audio alignment control focuses on\nelements like speaking style and the scale of lip movements, whereas emotion\ncontrol is centered on generating realistic emotional expressions, allowing for\nmodifications in multiple attributes such as intensity. To achieve precise\ncontrol of facial animation, we propose a novel framework, PC-Talk, which\nenables lip-audio alignment and emotion control through implicit keypoint\ndeformations. First, our lip-audio alignment control module facilitates precise\nediting of speaking styles at the word level and adjusts lip movement scales to\nsimulate varying vocal loudness levels, maintaining lip synchronization with\nthe audio. Second, our emotion control module generates vivid emotional facial\nfeatures with pure emotional deformation. This module also enables the fine\nmodification of intensity and the combination of multiple emotions across\ndifferent facial regions. Our method demonstrates outstanding control\ncapabilities and achieves state-of-the-art performance on both HDTF and MEAD\ndatasets in extensive experiments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14295v2",
    "published_date": "2025-03-18 14:35:48 UTC",
    "updated_date": "2025-03-20 10:27:54 UTC"
  },
  {
    "arxiv_id": "2503.14293v2",
    "title": "Ensemble Knowledge Distillation for Machine Learning Interatomic Potentials",
    "authors": [
      "Sakib Matin",
      "Emily Shinkle",
      "Yulia Pimonova",
      "Galen T. Craven",
      "Aleksandra Pachalieva",
      "Ying Wai Li",
      "Kipton Barros",
      "Nicholas Lubbers"
    ],
    "abstract": "Machine learning interatomic potentials (MLIPs) are a promising tool to\naccelerate atomistic simulations and molecular property prediction. The quality\nof MLIPs strongly depends on the quantity of available training data as well as\nthe quantum chemistry (QC) level of theory used to generate that data. Datasets\ngenerated with high-fidelity QC methods, such as coupled cluster, are typically\nrestricted to small molecules and may be missing energy gradients. With this\nlimited quantity of data, it is often difficult to train good MLIP models. We\npresent an ensemble knowledge distillation (EKD) method to improve MLIP\naccuracy when trained to energy-only datasets. In our EKD approach, first,\nmultiple teacher models are trained to QC energies and then used to generate\natomic forces for all configurations in the dataset. Next, a student MLIP is\ntrained to both QC energies and to ensemble-averaged forces generated by the\nteacher models. We apply this workflow on the ANI-1ccx dataset which consists\nof organic molecules with configuration energies computed at the coupled\ncluster level of theory. The resulting student MLIPs achieve new\nstate-of-the-art accuracy on the out-of-sample COMP6 benchmark and improved\nstability for molecular dynamics simulations. The EKD approach for MLIP is\nbroadly applicable for chemical, biomolecular and materials science\nsimulations.",
    "categories": [
      "physics.chem-ph",
      "cs.AI"
    ],
    "primary_category": "physics.chem-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14293v2",
    "published_date": "2025-03-18 14:32:51 UTC",
    "updated_date": "2025-03-19 15:03:39 UTC"
  },
  {
    "arxiv_id": "2503.14572v1",
    "title": "Robust Weight Imprinting: Insights from Neural Collapse and Proxy-Based Aggregation",
    "authors": [
      "Justus Westerhoff",
      "Golzar Atefi",
      "Mario Koddenbrock",
      "Alexei Figueroa",
      "Alexander LÃ¶ser",
      "Erik Rodner",
      "Felix A. Gers"
    ],
    "abstract": "The capacity of a foundation model allows for adaptation to new downstream\ntasks. Weight imprinting is a universal and efficient method to fulfill this\npurpose. It has been reinvented several times, but it has not been\nsystematically studied. In this paper, we propose a framework for imprinting,\nidentifying three main components: generation, normalization, and aggregation.\nThis allows us to conduct an in-depth analysis of imprinting and a comparison\nof the existing work. We reveal the benefits of representing novel data with\nmultiple proxies in the generation step and show the importance of proper\nnormalization. We determine those proxies through clustering and propose a\nnovel variant of imprinting that outperforms previous work. We motivate this by\nthe neural collapse phenomenon -- an important connection that we can draw for\nthe first time. Our results show an increase of up to 4% in challenging\nscenarios with complex data distributions for new classes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Code: https://github.com/DATEXIS/multi-imprinting/",
    "pdf_url": "http://arxiv.org/pdf/2503.14572v1",
    "published_date": "2025-03-18 14:27:45 UTC",
    "updated_date": "2025-03-18 14:27:45 UTC"
  },
  {
    "arxiv_id": "2503.14273v2",
    "title": "Manual Labelling Artificially Inflates Deep Learning-Based Segmentation Performance on RGB Images of Closed Canopy: Validation Using TLS",
    "authors": [
      "Matthew J. Allen",
      "Harry J. F. Owen",
      "Stuart W. D. Grieve",
      "Emily R. Lines"
    ],
    "abstract": "Monitoring forest dynamics at an individual tree scale is essential for\naccurately assessing ecosystem responses to climate change, yet traditional\nmethods relying on field-based forest inventories are labor-intensive and\nlimited in spatial coverage. Advances in remote sensing using drone-acquired\nRGB imagery combined with deep learning models have promised precise individual\ntree crown (ITC) segmentation; however, existing methods are frequently\nvalidated against human-annotated images, lacking rigorous independent ground\ntruth. In this study, we generate high-fidelity validation labels from\nco-located Terrestrial Laser Scanning (TLS) data for drone imagery of mixed\nunmanaged boreal and Mediterranean forests. We evaluate the performance of two\nwidely used deep learning ITC segmentation models - DeepForest (RetinaNet) and\nDetectree2 (Mask R-CNN) - on these data, and compare to performance on further\nMediterranean forest data labelled manually. When validated against TLS-derived\nground truth from Mediterranean forests, model performance decreased\nsignificantly compared to assessment based on hand-labelled from an\necologically similar site (AP50: 0.094 vs. 0.670). Restricting evaluation to\nonly canopy trees shrank this gap considerably (Canopy AP50: 0.365), although\nperformance was still far lower than on similar hand-labelled data. Models also\nperformed poorly on boreal forest data (AP50: 0.142), although again increasing\nwhen evaluated on canopy trees only (Canopy AP50: 0.308). Both models showed\nvery poor localisation accuracy at stricter IoU thresholds, even when\nrestricted to canopy trees (Max AP75: 0.051). Similar results have been\nobserved in studies using aerial LiDAR data, suggesting fundamental limitations\nin aerial-based segmentation approaches in closed canopy forests.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.4; I.4.6; I.4.8; I.4.9; I.5; I.5.4"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.14273v2",
    "published_date": "2025-03-18 14:09:00 UTC",
    "updated_date": "2025-03-19 16:17:19 UTC"
  },
  {
    "arxiv_id": "2503.14258v2",
    "title": "JuDGE: Benchmarking Judgment Document Generation for Chinese Legal System",
    "authors": [
      "Weihang Su",
      "Baoqing Yue",
      "Qingyao Ai",
      "Yiran Hu",
      "Jiaqi Li",
      "Changyue Wang",
      "Kaiyuan Zhang",
      "Yueyue Wu",
      "Yiqun Liu"
    ],
    "abstract": "This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14258v2",
    "published_date": "2025-03-18 13:48:18 UTC",
    "updated_date": "2025-03-20 15:09:51 UTC"
  },
  {
    "arxiv_id": "2503.14254v1",
    "title": "CTSAC: Curriculum-Based Transformer Soft Actor-Critic for Goal-Oriented Robot Exploration",
    "authors": [
      "Chunyu Yang",
      "Shengben Bi",
      "Yihui Xu",
      "Xin Zhang"
    ],
    "abstract": "With the increasing demand for efficient and flexible robotic exploration\nsolutions, Reinforcement Learning (RL) is becoming a promising approach in the\nfield of autonomous robotic exploration. However, current RL-based exploration\nalgorithms often face limited environmental reasoning capabilities, slow\nconvergence rates, and substantial challenges in Sim-To-Real (S2R) transfer. To\naddress these issues, we propose a Curriculum Learning-based Transformer\nReinforcement Learning Algorithm (CTSAC) aimed at improving both exploration\nefficiency and transfer performance. To enhance the robot's reasoning ability,\na Transformer is integrated into the perception network of the Soft\nActor-Critic (SAC) framework, leveraging historical information to improve the\nfarsightedness of the strategy. A periodic review-based curriculum learning is\nproposed, which enhances training efficiency while mitigating catastrophic\nforgetting during curriculum transitions. Training is conducted on the\nROS-Gazebo continuous robotic simulation platform, with LiDAR clustering\noptimization to further reduce the S2R gap. Experimental results demonstrate\nthe CTSAC algorithm outperforms the state-of-the-art non-learning and\nlearning-based algorithms in terms of success rate and success rate-weighted\nexploration time. Moreover, real-world experiments validate the strong S2R\ntransfer capabilities of CTSAC.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "7pages,7 figures,Thesis received by 2025 ICRA",
    "pdf_url": "http://arxiv.org/pdf/2503.14254v1",
    "published_date": "2025-03-18 13:44:29 UTC",
    "updated_date": "2025-03-18 13:44:29 UTC"
  },
  {
    "arxiv_id": "2503.14250v1",
    "title": "A Parallel Hybrid Action Space Reinforcement Learning Model for Real-world Adaptive Traffic Signal Control",
    "authors": [
      "Yuxuan Wang",
      "Meng Long",
      "Qiang Wu",
      "Wei Liu",
      "Jiatian Pi",
      "Xinmin Yang"
    ],
    "abstract": "Adaptive traffic signal control (ATSC) can effectively reduce vehicle travel\ntimes by dynamically adjusting signal timings but poses a critical challenge in\nreal-world scenarios due to the complexity of real-time decision-making in\ndynamic and uncertain traffic conditions. The burgeoning field of intelligent\ntransportation systems, bolstered by artificial intelligence techniques and\nextensive data availability, offers new prospects for the implementation of\nATSC. In this study, we introduce a parallel hybrid action space reinforcement\nlearning model (PH-DDPG) that optimizes traffic signal phase and duration of\ntraffic signals simultaneously, eliminating the need for sequential\ndecision-making seen in traditional two-stage models. Our model features a\ntask-specific parallel hybrid action space tailored for adaptive traffic\ncontrol, which directly outputs discrete phase selections and their associated\ncontinuous duration parameters concurrently, thereby inherently addressing\ndynamic traffic adaptation through unified parametric optimization. %Our model\nfeatures a unique parallel hybrid action space that allows for the simultaneous\noutput of each action and its optimal parameters, streamlining the\ndecision-making process. Furthermore, to ascertain the robustness and\neffectiveness of this approach, we executed ablation studies focusing on the\nutilization of a random action parameter mask within the critic network, which\ndecouples the parameter space for individual actions, facilitating the use of\npreferable parameters for each action. The results from these studies confirm\nthe efficacy of this method, distinctly enhancing real-world applicability",
    "categories": [
      "cs.AI",
      "I.2.6; I.2.8"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages, 9 figures, Reinforcement Learning",
    "pdf_url": "http://arxiv.org/pdf/2503.14250v1",
    "published_date": "2025-03-18 13:38:53 UTC",
    "updated_date": "2025-03-18 13:38:53 UTC"
  },
  {
    "arxiv_id": "2503.14247v1",
    "title": "GeoFlow-SLAM: A Robust Tightly-Coupled RGBD-Inertial Fusion SLAM for Dynamic Legged Robotics",
    "authors": [
      "Tingyang Xiao",
      "Xiaolin Zhou",
      "Liu Liu",
      "Wei Sui",
      "Wei Feng",
      "Jiaxiong Qiu",
      "Xinjie Wang",
      "Zhizhong Su"
    ],
    "abstract": "This paper presents GeoFlow-SLAM, a robust and effective Tightly-Coupled\nRGBD-inertial SLAM for legged robots operating in highly dynamic\nenvironments.By integrating geometric consistency, legged odometry constraints,\nand dual-stream optical flow (GeoFlow), our method addresses three critical\nchallenges:feature matching and pose initialization failures during fast\nlocomotion and visual feature scarcity in texture-less scenes.Specifically, in\nrapid motion scenarios, feature matching is notably enhanced by leveraging\ndual-stream optical flow, which combines prior map points and poses.\nAdditionally, we propose a robust pose initialization method for fast\nlocomotion and IMU error in legged robots, integrating IMU/Legged odometry,\ninter-frame Perspective-n-Point (PnP), and Generalized Iterative Closest Point\n(GICP). Furthermore, a novel optimization framework that tightly couples\ndepth-to-map and GICP geometric constraints is first introduced to improve the\nrobustness and accuracy in long-duration, visually texture-less environments.\nThe proposed algorithms achieve state-of-the-art (SOTA) on collected legged\nrobots and open-source datasets. To further promote research and development,\nthe open-source datasets and code will be made publicly available at\nhttps://github.com/NSN-Hello/GeoFlow-SLAM",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.14247v1",
    "published_date": "2025-03-18 13:35:49 UTC",
    "updated_date": "2025-03-18 13:35:49 UTC"
  },
  {
    "arxiv_id": "2503.14246v1",
    "title": "Trading-off Accuracy and Communication Cost in Federated Learning",
    "authors": [
      "Mattia Jacopo Villani",
      "Emanuele Natale",
      "Frederik Mallmann-Trenn"
    ],
    "abstract": "Leveraging the training-by-pruning paradigm introduced by Zhou et al. and\nIsik et al. introduced a federated learning protocol that achieves a 34-fold\nreduction in communication cost. We achieve a compression improvements of\norders of orders of magnitude over the state-of-the-art. The central idea of\nour framework is to encode the network weights $\\vec w$ by a the vector of\ntrainable parameters $\\vec p$, such that $\\vec w = Q\\cdot \\vec p$ where $Q$ is\na carefully-generate sparse random matrix (that remains fixed throughout\ntraining). In such framework, the previous work of Zhou et al. [NeurIPS'19] is\nretrieved when $Q$ is diagonal and $\\vec p$ has the same dimension of $\\vec w$.\nWe instead show that $\\vec p$ can effectively be chosen much smaller than $\\vec\nw$, while retaining the same accuracy at the price of a decrease of the\nsparsity of $Q$. Since server and clients only need to share $\\vec p$, such a\ntrade-off leads to a substantial improvement in communication cost. Moreover,\nwe provide theoretical insight into our framework and establish a novel link\nbetween training-by-sampling and random convex geometry.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14246v1",
    "published_date": "2025-03-18 13:35:24 UTC",
    "updated_date": "2025-03-18 13:35:24 UTC"
  },
  {
    "arxiv_id": "2503.16532v1",
    "title": "Modelling Emotions in Face-to-Face Setting: The Interplay of Eye-Tracking, Personality, and Temporal Dynamics",
    "authors": [
      "Meisam Jamshidi Seikavandi",
      "Jostein Fimland",
      "Maria Barrett",
      "Paolo Burelli"
    ],
    "abstract": "Accurate emotion recognition is pivotal for nuanced and engaging\nhuman-computer interactions, yet remains difficult to achieve, especially in\ndynamic, conversation-like settings. In this study, we showcase how integrating\neye-tracking data, temporal dynamics, and personality traits can substantially\nenhance the detection of both perceived and felt emotions. Seventy-three\nparticipants viewed short, speech-containing videos from the CREMA-D dataset,\nwhile being recorded for eye-tracking signals (pupil size, fixation patterns),\nBig Five personality assessments, and self-reported emotional states. Our\nneural network models combined these diverse inputs including stimulus emotion\nlabels for contextual cues and yielded marked performance gains compared to the\nstate-of-the-art. Specifically, perceived valence predictions reached a macro\nF1-score of 0.76, and models incorporating personality traits and stimulus\ninformation demonstrated significant improvements in felt emotion accuracy.\nThese results highlight the benefit of unifying physiological, individual and\ncontextual factors to address the subjectivity and complexity of emotional\nexpression. Beyond validating the role of user-specific data in capturing\nsubtle internal states, our findings inform the design of future affective\ncomputing and human-agent systems, paving the way for more adaptive and\ncross-individual emotional intelligence in real-world interactions.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16532v1",
    "published_date": "2025-03-18 13:15:32 UTC",
    "updated_date": "2025-03-18 13:15:32 UTC"
  },
  {
    "arxiv_id": "2503.14234v2",
    "title": "KG-IRAG: A Knowledge Graph-Based Iterative Retrieval-Augmented Generation Framework for Temporal Reasoning",
    "authors": [
      "Ruiyi Yang",
      "Hao Xue",
      "Imran Razzak",
      "Hakim Hacid",
      "Flora D. Salim"
    ],
    "abstract": "Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective\nin enhancing the performance of Large Language Models (LLMs) on tasks that\nrequire external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG\nimproves information retrieval for complex reasoning tasks, providing more\nprecise and comprehensive retrieval and generating more accurate responses to\nQAs. However, most RAG methods fall short in addressing multi-step reasoning,\nparticularly when both information extraction and inference are necessary. To\naddress this limitation, this paper presents Knowledge Graph-Based Iterative\nRetrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs\nwith iterative reasoning to improve LLMs' ability to handle queries involving\ntemporal and logical dependencies. Through iterative retrieval steps, KG-IRAG\nincrementally gathers relevant data from external KGs, enabling step-by-step\nreasoning. The proposed approach is particularly suited for scenarios where\nreasoning is required alongside dynamic temporal data extraction, such as\ndetermining optimal travel times based on weather conditions or traffic\npatterns. Experimental results show that KG-IRAG improves accuracy in complex\nreasoning tasks by effectively integrating external knowledge with iterative,\nlogic-based retrieval. Additionally, three new datasets: weatherQA-Irish,\nweatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's\nperformance, demonstrating its potential beyond traditional RAG applications.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.14234v2",
    "published_date": "2025-03-18 13:11:43 UTC",
    "updated_date": "2025-03-19 04:49:29 UTC"
  },
  {
    "arxiv_id": "2503.14232v1",
    "title": "CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion Models",
    "authors": [
      "Yuyang Xue",
      "Edward Moroshko",
      "Feng Chen",
      "Steven McDonagh",
      "Sotirios A. Tsaftaris"
    ],
    "abstract": "Text-to-Image diffusion models can produce undesirable content that\nnecessitates concept erasure techniques. However, existing methods struggle\nwith under-erasure, leaving residual traces of targeted concepts, or\nover-erasure, mistakenly eliminating unrelated but visually similar concepts.\nTo address these limitations, we introduce CRCE, a novel concept erasure\nframework that leverages Large Language Models to identify both semantically\nrelated concepts that should be erased alongside the target and distinct\nconcepts that should be preserved. By explicitly modeling coreferential and\nretained concepts semantically, CRCE enables more precise concept removal,\nwithout unintended erasure. Experiments demonstrate that CRCE outperforms\nexisting methods on diverse erasure tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14232v1",
    "published_date": "2025-03-18 13:09:01 UTC",
    "updated_date": "2025-03-18 13:09:01 UTC"
  },
  {
    "arxiv_id": "2503.14229v1",
    "title": "HA-VLN: A Benchmark for Human-Aware Navigation in Discrete-Continuous Environments with Dynamic Multi-Human Interactions, Real-World Validation, and an Open Leaderboard",
    "authors": [
      "Yifei Dong",
      "Fengyi Wu",
      "Qi He",
      "Heng Li",
      "Minghan Li",
      "Zebang Cheng",
      "Yuxuan Zhou",
      "Jingdong Sun",
      "Qi Dai",
      "Zhi-Qi Cheng",
      "Alexander G Hauptmann"
    ],
    "abstract": "Vision-and-Language Navigation (VLN) systems often focus on either discrete\n(panoramic) or continuous (free-motion) paradigms alone, overlooking the\ncomplexities of human-populated, dynamic environments. We introduce a unified\nHuman-Aware VLN (HA-VLN) benchmark that merges these paradigms under explicit\nsocial-awareness constraints. Our contributions include: 1. A standardized task\ndefinition that balances discrete-continuous navigation with personal-space\nrequirements; 2. An enhanced human motion dataset (HAPS 2.0) and upgraded\nsimulators capturing realistic multi-human interactions, outdoor contexts, and\nrefined motion-language alignment; 3. Extensive benchmarking on 16,844\nhuman-centric instructions, revealing how multi-human dynamics and partial\nobservability pose substantial challenges for leading VLN agents; 4. Real-world\nrobot tests validating sim-to-real transfer in crowded indoor spaces; and 5. A\npublic leaderboard supporting transparent comparisons across discrete and\ncontinuous tasks. Empirical results show improved navigation success and fewer\ncollisions when social context is integrated, underscoring the need for\nhuman-centric design. By releasing all datasets, simulators, agent code, and\nevaluation tools, we aim to advance safer, more capable, and socially\nresponsible VLN research.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "27 pages, website: https://ha-vln-project.vercel.app/",
    "pdf_url": "http://arxiv.org/pdf/2503.14229v1",
    "published_date": "2025-03-18 13:05:55 UTC",
    "updated_date": "2025-03-18 13:05:55 UTC"
  },
  {
    "arxiv_id": "2503.14228v1",
    "title": "Panoramic Distortion-Aware Tokenization for Person Detection and Localization Using Transformers in Overhead Fisheye Images",
    "authors": [
      "Nobuhiko Wakai",
      "Satoshi Sato",
      "Yasunori Ishii",
      "Takayoshi Yamashita"
    ],
    "abstract": "Person detection methods are used widely in applications including visual\nsurveillance, pedestrian detection, and robotics. However, accurate detection\nof persons from overhead fisheye images remains an open challenge because of\nfactors including person rotation and small-sized persons. To address the\nperson rotation problem, we convert the fisheye images into panoramic images.\nFor smaller people, we focused on the geometry of the panoramas. Conventional\ndetection methods tend to focus on larger people because these larger people\nyield large significant areas for feature maps. In equirectangular panoramic\nimages, we find that a person's height decreases linearly near the top of the\nimages. Using this finding, we leverage the significance values and aggregate\ntokens that are sorted based on these values to balance the significant areas.\nIn this leveraging process, we introduce panoramic distortion-aware\ntokenization. This tokenization procedure divides a panoramic image using\nself-similarity figures that enable determination of optimal divisions without\ngaps, and we leverage the maximum significant values in each tile of token\ngroups to preserve the significant areas of smaller people. To achieve higher\ndetection accuracy, we propose a person detection and localization method that\ncombines panoramic-image remapping and the tokenization procedure. Extensive\nexperiments demonstrated that our method outperforms conventional methods when\napplied to large-scale datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14228v1",
    "published_date": "2025-03-18 13:05:41 UTC",
    "updated_date": "2025-03-18 13:05:41 UTC"
  },
  {
    "arxiv_id": "2503.14203v1",
    "title": "Stochastic Trajectory Prediction under Unstructured Constraints",
    "authors": [
      "Hao Ma",
      "Zhiqiang Pu",
      "Shijie Wang",
      "Boyin Liu",
      "Huimu Wang",
      "Yanyan Liang",
      "Jianqiang Yi"
    ],
    "abstract": "Trajectory prediction facilitates effective planning and decision-making,\nwhile constrained trajectory prediction integrates regulation into prediction.\nRecent advances in constrained trajectory prediction focus on structured\nconstraints by constructing optimization objectives. However, handling\nunstructured constraints is challenging due to the lack of differentiable\nformal definitions. To address this, we propose a novel method for constrained\ntrajectory prediction using a conditional generative paradigm, named\nControllable Trajectory Diffusion (CTD). The key idea is that any trajectory\ncorresponds to a degree of conformity to a constraint. By quantifying this\ndegree and treating it as a condition, a model can implicitly learn to predict\ntrajectories under unstructured constraints. CTD employs a pre-trained scoring\nmodel to predict the degree of conformity (i.e., a score), and uses this score\nas a condition for a conditional diffusion model to generate trajectories.\nExperimental results demonstrate that CTD achieves high accuracy on the ETH/UCY\nand SDD benchmarks. Qualitative analysis confirms that CTD ensures adherence to\nunstructured constraints and can predict trajectories that satisfy\ncombinatorial constraints.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "has been accepted by ICRA 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.14203v1",
    "published_date": "2025-03-18 12:27:59 UTC",
    "updated_date": "2025-03-18 12:27:59 UTC"
  },
  {
    "arxiv_id": "2503.18955v1",
    "title": "Is there a future for AI without representation?",
    "authors": [
      "Vincent C. MÃ¼ller"
    ],
    "abstract": "This paper investigates the prospects of AI without representation in\ngeneral, and the proposals of Rodney Brooks in particular. What turns out to be\ncharacteristic of Brooks' proposal is the rejection of central control in\nintelligent agents; his systems has as much or as little representation as\ntraditional AI. The traditional view that representation is necessary for\nintelligence presupposes that intelligence requires central control. However,\nmuch of recent cognitive science suggests that we should dispose of the image\nof intelligent agents as central representation processors. If this paradigm\nshift is achieved, Brooks' proposal for non-centralized cognition without\nrepresentation appears promising for full-blown intelligent agents - though not\nfor conscious agents and thus not for human-like AI.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.18955v1",
    "published_date": "2025-03-18 12:13:31 UTC",
    "updated_date": "2025-03-18 12:13:31 UTC"
  },
  {
    "arxiv_id": "2503.14194v1",
    "title": "Driving behavior recognition via self-discovery learning",
    "authors": [
      "Yilin Wang"
    ],
    "abstract": "Autonomous driving systems require a deep understanding of human driving\nbehaviors to achieve higher intelligence and safety.Despite advancements in\ndeep learning, challenges such as long-tail distribution due to scarce samples\nand confusion from similar behaviors hinder effective driving behavior\ndetection.Existing methods often fail to address sample confusion adequately,\nas datasets frequently contain ambiguous samples that obscure unique semantic\ninformation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.14194v1",
    "published_date": "2025-03-18 12:13:08 UTC",
    "updated_date": "2025-03-18 12:13:08 UTC"
  },
  {
    "arxiv_id": "2503.14192v1",
    "title": "Strategic White Paper on AI Infrastructure for Particle, Nuclear, and Astroparticle Physics: Insights from JENA and EuCAIF",
    "authors": [
      "Sascha Caron",
      "Andreas Ipp",
      "Gert Aarts",
      "GÃ¡bor BÃ­rÃ³",
      "Daniele Bonacorsi",
      "Elena Cuoco",
      "Caterina Doglioni",
      "Tommaso Dorigo",
      "JuliÃ¡n GarcÃ­a PardiÃ±as",
      "Stefano Giagu",
      "Tobias Golling",
      "Lukas Heinrich",
      "Ik Siong Heng",
      "Paula Gina Isar",
      "Karolos Potamianos",
      "Liliana Teodorescu",
      "John Veitch",
      "Pietro Vischia",
      "Christoph Weniger"
    ],
    "abstract": "Artificial intelligence (AI) is transforming scientific research, with deep\nlearning methods playing a central role in data analysis, simulations, and\nsignal detection across particle, nuclear, and astroparticle physics. Within\nthe JENA communities-ECFA, NuPECC, and APPEC-and as part of the EuCAIF\ninitiative, AI integration is advancing steadily. However, broader adoption\nremains constrained by challenges such as limited computational resources, a\nlack of expertise, and difficulties in transitioning from research and\ndevelopment (R&D) to production. This white paper provides a strategic roadmap,\ninformed by a community survey, to address these barriers. It outlines critical\ninfrastructure requirements, prioritizes training initiatives, and proposes\nfunding strategies to scale AI capabilities across fundamental physics over the\nnext five years.",
    "categories": [
      "astro-ph.IM",
      "astro-ph.HE",
      "cs.AI",
      "cs.LG",
      "hep-ex",
      "hep-ph",
      "nucl-th"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "19 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.14192v1",
    "published_date": "2025-03-18 12:11:11 UTC",
    "updated_date": "2025-03-18 12:11:11 UTC"
  },
  {
    "arxiv_id": "2503.14190v1",
    "title": "Inferring Event Descriptions from Time Series with Language Models",
    "authors": [
      "Mingtian Tan",
      "Mike A. Merrill",
      "Zack Gottesman",
      "Tim Althoff",
      "David Evans",
      "Tom Hartvigsen"
    ],
    "abstract": "Time series data measure how environments change over time and drive\ndecision-making in critical domains like finance and healthcare. When analyzing\ntime series, we often seek to understand the underlying events occurring in the\nmeasured environment. For example, one might ask: What caused a sharp drop in\nthe stock price? Events are often described with natural language, so we\nconduct the first study of whether Large Language Models (LLMs) can infer\nnatural language events from time series. We curate a new benchmark featuring\nwin probabilities collected from 4,200 basketball and American football games,\nfeaturing 1.7M timesteps with real value data and corresponding natural\nlanguage events. Building on the recent wave of using LLMs on time series, we\nevaluate 16 LLMs and find that they demonstrate promising abilities to infer\nevents from time series data. The open-weights DeepSeek-R1 32B model\noutperforms proprietary models like GPT-4o. Despite this impressive initial\nperformance, we also find clear avenues to improve recent models, as we\nidentify failures when altering the provided context, event sequence lengths,\nand evaluation strategy. (All resources needed to reproduce our work are\navailable: https://github.com/BennyTMT/GAMETime)",
    "categories": [
      "cs.AI",
      "62M10, 68T07,",
      "I.2.6; I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages, 9 Figures",
    "pdf_url": "http://arxiv.org/pdf/2503.14190v1",
    "published_date": "2025-03-18 12:07:33 UTC",
    "updated_date": "2025-03-18 12:07:33 UTC"
  },
  {
    "arxiv_id": "2503.14184v1",
    "title": "Variable Time-Step MPC for Agile Multi-Rotor UAV Interception of Dynamic Targets",
    "authors": [
      "Atharva Ghotavadekar",
      "FrantiÅ¡ek NekovÃ¡Å",
      "Martin Saska",
      "Jan Faigl"
    ],
    "abstract": "Agile trajectory planning can improve the efficiency of multi-rotor Uncrewed\nAerial Vehicles (UAVs) in scenarios with combined task-oriented and kinematic\ntrajectory planning, such as monitoring spatio-temporal phenomena or\nintercepting dynamic targets. Agile planning using existing non-linear model\npredictive control methods is limited by the number of planning steps as it\nbecomes increasingly computationally demanding. That reduces the prediction\nhorizon length, leading to a decrease in solution quality. Besides, the fixed\ntime-step length limits the utilization of the available UAV dynamics in the\ntarget neighborhood. In this paper, we propose to address these limitations by\nintroducing variable time steps and coupling them with the prediction horizon\nlength. A simplified point-mass motion primitive is used to leverage the\ndifferential flatness of quadrotor dynamics and the generation of feasible\ntrajectories in the flat output space. Based on the presented evaluation\nresults and experimentally validated deployment, the proposed method increases\nthe solution quality by enabling planning for long flight segments but allowing\ntightly sampled maneuvering.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14184v1",
    "published_date": "2025-03-18 11:59:24 UTC",
    "updated_date": "2025-03-18 11:59:24 UTC"
  },
  {
    "arxiv_id": "2503.14183v1",
    "title": "Can LLMs Enable Verification in Mainstream Programming?",
    "authors": [
      "Aleksandr Shefer",
      "Igor Engel",
      "Stanislav Alekseev",
      "Daniil Berezun",
      "Ekaterina Verbitskaia",
      "Anton Podkopaev"
    ],
    "abstract": "Although formal methods are capable of producing reliable software, they have\nseen minimal adoption in everyday programming. Automatic code generation using\nlarge language models is becoming increasingly widespread, but it rarely\nconsiders producing strong correctness guarantees. In this study, we explore\nthe ability of LLMs to produce verified code in three verification languages\n(Dafny, Nagini, and Verus). To do so, we use manually curated datasets derived\nfrom the state-ofthe-art Python benchmark, HumanEval. We also assess what types\nof information are sufficient to achieve good-quality results.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14183v1",
    "published_date": "2025-03-18 11:58:00 UTC",
    "updated_date": "2025-03-18 11:58:00 UTC"
  },
  {
    "arxiv_id": "2503.14162v1",
    "title": "EIAD: Explainable Industrial Anomaly Detection Via Multi-Modal Large Language Models",
    "authors": [
      "Zongyun Zhang",
      "Jiacheng Ruan",
      "Xian Gao",
      "Ting Liu",
      "Yuzhuo Fu"
    ],
    "abstract": "Industrial Anomaly Detection (IAD) is critical to ensure product quality\nduring manufacturing. Although existing zero-shot defect segmentation and\ndetection methods have shown effectiveness, they cannot provide detailed\ndescriptions of the defects. Furthermore, the application of large multi-modal\nmodels in IAD remains in its infancy, facing challenges in balancing\nquestion-answering (QA) performance and mask-based grounding capabilities,\noften owing to overfitting during the fine-tuning process. To address these\nchallenges, we propose a novel approach that introduces a dedicated multi-modal\ndefect localization module to decouple the dialog functionality from the core\nfeature extraction. This decoupling is achieved through independent\noptimization objectives and tailored learning strategies. Additionally, we\ncontribute to the first multi-modal industrial anomaly detection training\ndataset, named Defect Detection Question Answering (DDQA), encompassing a wide\nrange of defect types and industrial scenarios. Unlike conventional datasets\nthat rely on GPT-generated data, DDQA ensures authenticity and reliability and\noffers a robust foundation for model training. Experimental results demonstrate\nthat our proposed method, Explainable Industrial Anomaly Detection Assistant\n(EIAD), achieves outstanding performance in defect detection and localization\ntasks. It not only significantly enhances accuracy but also improves\ninterpretability. These advancements highlight the potential of EIAD for\npractical applications in industrial settings.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14162v1",
    "published_date": "2025-03-18 11:33:29 UTC",
    "updated_date": "2025-03-18 11:33:29 UTC"
  },
  {
    "arxiv_id": "2503.14569v1",
    "title": "Potential Score Matching: Debiasing Molecular Structure Sampling with Potential Energy Guidance",
    "authors": [
      "Liya Guo",
      "Zun Wang",
      "Chang Liu",
      "Junzhe Li",
      "Pipi Hu",
      "Yi Zhu"
    ],
    "abstract": "The ensemble average of physical properties of molecules is closely related\nto the distribution of molecular conformations, and sampling such distributions\nis a fundamental challenge in physics and chemistry. Traditional methods like\nmolecular dynamics (MD) simulations and Markov chain Monte Carlo (MCMC)\nsampling are commonly used but can be time-consuming and costly. Recently,\ndiffusion models have emerged as efficient alternatives by learning the\ndistribution of training data. Obtaining an unbiased target distribution is\nstill an expensive task, primarily because it requires satisfying ergodicity.\nTo tackle these challenges, we propose Potential Score Matching (PSM), an\napproach that utilizes the potential energy gradient to guide generative\nmodels. PSM does not require exact energy functions and can debias sample\ndistributions even when trained on limited and biased data. Our method\noutperforms existing state-of-the-art (SOTA) models on the Lennard-Jones (LJ)\npotential, a commonly used toy model. Furthermore, we extend the evaluation of\nPSM to high-dimensional problems using the MD17 and MD22 datasets. The results\ndemonstrate that molecular distributions generated by PSM more closely\napproximate the Boltzmann distribution compared to traditional diffusion\nmodels.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14569v1",
    "published_date": "2025-03-18 11:27:28 UTC",
    "updated_date": "2025-03-18 11:27:28 UTC"
  },
  {
    "arxiv_id": "2503.14568v1",
    "title": "Teaching Artificial Intelligence to Perform Rapid, Resolution-Invariant Grain Growth Modeling via Fourier Neural Operator",
    "authors": [
      "Iman Peivaste",
      "Ahmed Makradi",
      "Salim Belouettar"
    ],
    "abstract": "Microstructural evolution, particularly grain growth, plays a critical role\nin shaping the physical, optical, and electronic properties of materials.\nTraditional phase-field modeling accurately simulates these phenomena but is\ncomputationally intensive, especially for large systems and fine spatial\nresolutions. While machine learning approaches have been employed to accelerate\nsimulations, they often struggle with resolution dependence and generalization\nacross different grain scales. This study introduces a novel approach utilizing\nFourier Neural Operator (FNO) to achieve resolution-invariant modeling of\nmicrostructure evolution in multi-grain systems. FNO operates in the Fourier\nspace and can inherently handle varying resolutions by learning mappings\nbetween function spaces. By integrating FNO with the phase field method, we\ndeveloped a surrogate model that significantly reduces computational costs\nwhile maintaining high accuracy across different spatial scales. We generated a\ncomprehensive dataset from phase-field simulations using the Fan Chen model,\ncapturing grain evolution over time. Data preparation involved creating\ninput-output pairs with a time shift, allowing the model to predict future\nmicrostructures based on current and past states. The FNO-based neural network\nwas trained using sequences of microstructures and demonstrated remarkable\naccuracy in predicting long-term evolution, even for unseen configurations and\nhigher-resolution grids not encountered during training.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14568v1",
    "published_date": "2025-03-18 11:19:08 UTC",
    "updated_date": "2025-03-18 11:19:08 UTC"
  },
  {
    "arxiv_id": "2503.14151v1",
    "title": "Concat-ID: Towards Universal Identity-Preserving Video Synthesis",
    "authors": [
      "Yong Zhong",
      "Zhuoyi Yang",
      "Jiayan Teng",
      "Xiaotao Gu",
      "Chongxuan Li"
    ],
    "abstract": "We present Concat-ID, a unified framework for identity-preserving video\ngeneration. Concat-ID employs Variational Autoencoders to extract image\nfeatures, which are concatenated with video latents along the sequence\ndimension, leveraging solely 3D self-attention mechanisms without the need for\nadditional modules. A novel cross-video pairing strategy and a multi-stage\ntraining regimen are introduced to balance identity consistency and facial\neditability while enhancing video naturalness. Extensive experiments\ndemonstrate Concat-ID's superiority over existing methods in both single and\nmulti-identity generation, as well as its seamless scalability to multi-subject\nscenarios, including virtual try-on and background-controllable generation.\nConcat-ID establishes a new benchmark for identity-preserving video synthesis,\nproviding a versatile and scalable solution for a wide range of applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14151v1",
    "published_date": "2025-03-18 11:17:32 UTC",
    "updated_date": "2025-03-18 11:17:32 UTC"
  },
  {
    "arxiv_id": "2503.14138v1",
    "title": "Exploring Disparity-Accuracy Trade-offs in Face Recognition Systems: The Role of Datasets, Architectures, and Loss Functions",
    "authors": [
      "Siddharth D Jaiswal",
      "Sagnik Basu",
      "Sandipan Sikdar",
      "Animesh Mukherjee"
    ],
    "abstract": "Automated Face Recognition Systems (FRSs), developed using deep learning\nmodels, are deployed worldwide for identity verification and facial attribute\nanalysis. The performance of these models is determined by a complex\ninterdependence among the model architecture, optimization/loss function and\ndatasets. Although FRSs have surpassed human-level accuracy, they continue to\nbe disparate against certain demographics. Due to the ubiquity of applications,\nit is extremely important to understand the impact of the three components --\nmodel architecture, loss function and face image dataset on the\naccuracy-disparity trade-off to design better, unbiased platforms. In this\nwork, we perform an in-depth analysis of three FRSs for the task of gender\nprediction, with various architectural modifications resulting in ten\ndeep-learning models coupled with four loss functions and benchmark them on\nseven face datasets across 266 evaluation configurations. Our results show that\nall three components have an individual as well as a combined impact on both\naccuracy and disparity. We identify that datasets have an inherent property\nthat causes them to perform similarly across models, independent of the choice\nof loss functions. Moreover, the choice of dataset determines the model's\nperceived bias -- the same model reports bias in opposite directions for three\ngender-balanced datasets of ``in-the-wild'' face images of popular individuals.\nStudying the facial embeddings shows that the models are unable to generalize a\nuniform definition of what constitutes a ``female face'' as opposed to a ``male\nface'', due to dataset diversity. We provide recommendations to model\ndevelopers on using our study as a blueprint for model development and\nsubsequent deployment.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "This work has been accepted for publication at AAAI ICWSM 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.14138v1",
    "published_date": "2025-03-18 11:04:57 UTC",
    "updated_date": "2025-03-18 11:04:57 UTC"
  },
  {
    "arxiv_id": "2503.14136v1",
    "title": "CARE: A QLoRA-Fine Tuned Multi-Domain Chatbot With Fast Learning On Minimal Hardware",
    "authors": [
      "Ankit Dutta",
      "Nabarup Ghosh",
      "Ankush Chatterjee"
    ],
    "abstract": "Large Language models have demonstrated excellent domain-specific\nquestion-answering capabilities when finetuned with a particular dataset of\nthat specific domain. However, fine-tuning the models requires a significant\namount of training time and a considerable amount of hardware. In this work, we\npropose CARE (Customer Assistance and Response Engine), a lightweight model\nmade by fine-tuning Phi3.5-mini on very minimal hardware and data, designed to\nhandle queries primarily across three domains: telecommunications support,\nmedical support, and banking support. For telecommunications and banking, the\nchatbot addresses issues and problems faced by customers regularly in the\nabove-mentioned domains. In the medical domain, CARE provides preliminary\nsupport by offering basic diagnoses and medical suggestions that a user might\ntake before consulting a healthcare professional. Since CARE is built on\nPhi3.5-mini, it can be used even on mobile devices, increasing its usability.\nOur research also shows that CARE performs relatively well on various medical\nbenchmarks, indicating that it can be used to make basic medical suggestions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14136v1",
    "published_date": "2025-03-18 10:58:10 UTC",
    "updated_date": "2025-03-18 10:58:10 UTC"
  },
  {
    "arxiv_id": "2503.14130v1",
    "title": "Inference-Time Intervention in Large Language Models for Reliable Requirement Verification",
    "authors": [
      "Paul Darm",
      "James Xie",
      "Annalisa Riccardi"
    ],
    "abstract": "Steering the behavior of Large Language Models (LLMs) remains a challenge,\nparticularly in engineering applications where precision and reliability are\ncritical. While fine-tuning and prompting methods can modify model behavior,\nthey lack the dynamic and exact control necessary for engineering applications.\nInference-time intervention techniques provide a promising alternative,\nallowing targeted adjustments to LLM outputs. In this work, we demonstrate how\ninterventions enable fine-grained control for automating the usually\ntime-intensive requirement verification process in Model-Based Systems\nEngineering (MBSE). Using two early-stage Capella SysML models of space\nmissions with associated requirements, we apply the intervened LLMs to reason\nover a graph representation of the model to determine whether a requirement is\nfulfilled. Our method achieves robust and reliable outputs, significantly\nimproving over both a baseline model and a fine-tuning approach. By identifying\nand modifying as few as one to three specialised attention heads, we can\nsignificantly change the model's behavior. When combined with self-consistency,\nthis allows us to achieve perfect precision on our holdout test set.",
    "categories": [
      "cs.AI",
      "cs.SE",
      "H.4.2; I.2.1; I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14130v1",
    "published_date": "2025-03-18 10:49:36 UTC",
    "updated_date": "2025-03-18 10:49:36 UTC"
  },
  {
    "arxiv_id": "2503.14567v1",
    "title": "SpecReX: Explainable AI for Raman Spectroscopy",
    "authors": [
      "Nathan Blake",
      "David A. Kelly",
      "Akchunya Chanchal",
      "Sarah Kapllani-Mucaj",
      "Geraint Thomas",
      "Hana Chockler"
    ],
    "abstract": "Raman spectroscopy is becoming more common for medical diagnostics with deep\nlearning models being increasingly used to leverage its full potential.\nHowever, the opaque nature of such models and the sensitivity of medical\ndiagnosis together with regulatory requirements necessitate the need for\nexplainable AI tools. We introduce SpecReX, specifically adapted to explaining\nRaman spectra. SpecReX uses the theory of actual causality to rank causal\nresponsibility in a spectrum, quantified by iteratively refining mutated\nversions of the spectrum and testing if it retains the original classification.\nThe explanations provided by SpecReX take the form of a responsibility map,\nhighlighting spectral regions most responsible for the model to make a correct\nclassification. To assess the validity of SpecReX, we create increasingly\ncomplex simulated spectra, in which a \"ground truth\" signal is seeded, to train\na classifier. We then obtain SpecReX explanations and compare the results with\nanother explainability tool. By using simulated spectra we establish that\nSpecReX localizes to the known differences between classes, under a number of\nconditions. This provides a foundation on which we can find the spectral\nfeatures which differentiate disease classes. This is an important first step\nin proving the validity of SpecReX.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.med-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "AAAI Workshop on Health Intelligencee (W3PHIAI-25)",
    "pdf_url": "http://arxiv.org/pdf/2503.14567v1",
    "published_date": "2025-03-18 10:49:15 UTC",
    "updated_date": "2025-03-18 10:49:15 UTC"
  },
  {
    "arxiv_id": "2503.14125v1",
    "title": "Frac-Connections: Fractional Extension of Hyper-Connections",
    "authors": [
      "Defa Zhu",
      "Hongzhi Huang",
      "Jundong Zhou",
      "Zihao Huang",
      "Yutao Zeng",
      "Banggu Wu",
      "Qiyang Min",
      "Xun Zhou"
    ],
    "abstract": "Residual connections are central to modern deep learning architectures,\nenabling the training of very deep networks by mitigating gradient vanishing.\nHyper-Connections recently generalized residual connections by introducing\nmultiple connection strengths at different depths, thereby addressing the\nseesaw effect between gradient vanishing and representation collapse. However,\nHyper-Connections increase memory access costs by expanding the width of hidden\nstates. In this paper, we propose Frac-Connections, a novel approach that\ndivides hidden states into multiple parts rather than expanding their width.\nFrac-Connections retain partial benefits of Hyper-Connections while reducing\nmemory consumption. To validate their effectiveness, we conduct large-scale\nexperiments on language tasks, with the largest being a 7B MoE model trained on\nup to 3T tokens, demonstrating that Frac-Connections significantly outperform\nresidual connections.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14125v1",
    "published_date": "2025-03-18 10:37:50 UTC",
    "updated_date": "2025-03-18 10:37:50 UTC"
  },
  {
    "arxiv_id": "2503.14109v1",
    "title": "Operational Change Detection for Geographical Information: Overview and Challenges",
    "authors": [
      "Nicolas Gonthier"
    ],
    "abstract": "Rapid evolution of territories due to climate change and human impact\nrequires prompt and effective updates to geospatial databases maintained by the\nNational Mapping Agency. This paper presents a comprehensive overview of change\ndetection methods tailored for the operational updating of large-scale\ngeographic databases. This review first outlines the fundamental definition of\nchange, emphasizing its multifaceted nature, from temporal to semantic\ncharacterization. It categorizes automatic change detection methods into four\nmain families: rule-based, statistical, machine learning, and simulation\nmethods. The strengths, limitations, and applicability of every family are\ndiscussed in the context of various input data. Then, key applications for\nNational Mapping Agencies are identified, particularly the optimization of\ngeospatial database updating, change-based phenomena, and dynamics monitoring.\nFinally, the paper highlights the current challenges for leveraging change\ndetection such as the variability of change definition, the missing of relevant\nlarge-scale datasets, the diversity of input data, the unstudied no-change\ndetection, the human in the loop integration and the operational constraints.\nThe discussion underscores the necessity for ongoing innovation in change\ndetection techniques to address the future needs of geographic information\nsystems for national mapping agencies.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint under review",
    "pdf_url": "http://arxiv.org/pdf/2503.14109v1",
    "published_date": "2025-03-18 10:25:28 UTC",
    "updated_date": "2025-03-18 10:25:28 UTC"
  },
  {
    "arxiv_id": "2503.14106v1",
    "title": "Reliable uncertainty quantification for 2D/3D anatomical landmark localization using multi-output conformal prediction",
    "authors": [
      "Jef Jonkers",
      "Frank Coopman",
      "Luc Duchateau",
      "Glenn Van Wallendael",
      "Sofie Van Hoecke"
    ],
    "abstract": "Automatic anatomical landmark localization in medical imaging requires not\njust accurate predictions but reliable uncertainty quantification for effective\nclinical decision support. Current uncertainty quantification approaches often\nfall short, particularly when combined with normality assumptions,\nsystematically underestimating total predictive uncertainty. This paper\nintroduces conformal prediction as a framework for reliable uncertainty\nquantification in anatomical landmark localization, addressing a critical gap\nin automatic landmark localization. We present two novel approaches\nguaranteeing finite-sample validity for multi-output prediction: Multi-output\nRegression-as-Classification Conformal Prediction (M-R2CCP) and its variant\nMulti-output Regression to Classification Conformal Prediction set to Region\n(M-R2C2R). Unlike conventional methods that produce axis-aligned\nhyperrectangular or ellipsoidal regions, our approaches generate flexible,\nnon-convex prediction regions that better capture the underlying uncertainty\nstructure of landmark predictions. Through extensive empirical evaluation\nacross multiple 2D and 3D datasets, we demonstrate that our methods\nconsistently outperform existing multi-output conformal prediction approaches\nin both validity and efficiency. This work represents a significant advancement\nin reliable uncertainty estimation for anatomical landmark localization,\nproviding clinicians with trustworthy confidence measures for their diagnoses.\nWhile developed for medical imaging, these methods show promise for broader\napplications in multi-output regression problems.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "comment": "33 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.14106v1",
    "published_date": "2025-03-18 10:21:32 UTC",
    "updated_date": "2025-03-18 10:21:32 UTC"
  },
  {
    "arxiv_id": "2503.14102v1",
    "title": "Sensory-driven microinterventions for improved health and wellbeing",
    "authors": [
      "Youssef Abdalla",
      "Elia Gatti",
      "Mine Orlu",
      "Marianna Obrist"
    ],
    "abstract": "The five senses are gateways to our wellbeing and their decline is considered\na significant public health challenge which is linked to multiple conditions\nthat contribute significantly to morbidity and mortality. Modern technology,\nwith its ubiquitous nature and fast data processing has the ability to leverage\nthe power of the senses to transform our approach to day to day healthcare,\nwith positive effects on our quality of life. Here, we introduce the idea of\nsensory-driven microinterventions for preventative, personalised healthcare.\nMicrointerventions are targeted, timely, minimally invasive strategies that\nseamlessly integrate into our daily life. This idea harnesses human's sensory\ncapabilities, leverages technological advances in sensory stimulation and\nreal-time processing ability for sensing the senses. The collection of sensory\ndata from our continuous interaction with technology - for example the tone of\nvoice, gait movement, smart home behaviour - opens up a shift towards\npersonalised technology-enabled, sensory-focused healthcare interventions,\ncoupled with the potential of early detection and timely treatment of sensory\ndeficits that can signal critical health insights, especially for\nneurodegenerative diseases such as Parkinson's disease.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.ET"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14102v1",
    "published_date": "2025-03-18 10:17:55 UTC",
    "updated_date": "2025-03-18 10:17:55 UTC"
  },
  {
    "arxiv_id": "2503.14088v1",
    "title": "Toward Large-Scale Distributed Quantum Long Short-Term Memory with Modular Quantum Computers",
    "authors": [
      "Kuan-Cheng Chen",
      "Samuel Yen-Chi Chen",
      "Chen-Yu Liu",
      "Kin K. Leung"
    ],
    "abstract": "In this work, we introduce a Distributed Quantum Long Short-Term Memory\n(QLSTM) framework that leverages modular quantum computing to address\nscalability challenges on Noisy Intermediate-Scale Quantum (NISQ) devices. By\nembedding variational quantum circuits into LSTM cells, the QLSTM captures\nlong-range temporal dependencies, while a distributed architecture partitions\nthe underlying Variational Quantum Circuits (VQCs) into smaller, manageable\nsubcircuits that can be executed on a network of quantum processing units. We\nassess the proposed framework using nontrivial benchmark problems such as\ndamped harmonic oscillators and Nonlinear Autoregressive Moving Average\nsequences. Our results demonstrate that the distributed QLSTM achieves stable\nconvergence and improved training dynamics compared to classical approaches.\nThis work underscores the potential of modular, distributed quantum computing\narchitectures for large-scale sequence modelling, providing a foundation for\nthe future integration of hybrid quantum-classical solutions into advanced\nQuantum High-performance computing (HPC) ecosystems.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14088v1",
    "published_date": "2025-03-18 10:07:34 UTC",
    "updated_date": "2025-03-18 10:07:34 UTC"
  },
  {
    "arxiv_id": "2503.17395v1",
    "title": "CP-NCBF: A Conformal Prediction-based Approach to Synthesize Verified Neural Control Barrier Functions",
    "authors": [
      "Manan Tayal",
      "Aditya Singh",
      "Pushpak Jagtap",
      "Shishir Kolathaya"
    ],
    "abstract": "Control Barrier Functions (CBFs) are a practical approach for designing\nsafety-critical controllers, but constructing them for arbitrary nonlinear\ndynamical systems remains a challenge. Recent efforts have explored\nlearning-based methods, such as neural CBFs (NCBFs), to address this issue.\nHowever, ensuring the validity of NCBFs is difficult due to potential learning\nerrors. In this letter, we propose a novel framework that leverages\nsplit-conformal prediction to generate formally verified neural CBFs with\nprobabilistic guarantees based on a user-defined error rate, referred to as\nCP-NCBF. Unlike existing methods that impose Lipschitz constraints on neural\nCBF-leading to scalability limitations and overly conservative safe sets--our\napproach is sample-efficient, scalable, and results in less restrictive safety\nregions. We validate our framework through case studies on obstacle avoidance\nin autonomous driving and geo-fencing of aerial vehicles, demonstrating its\nability to generate larger and less conservative safe sets compared to\nconventional techniques.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.RO",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "6 Pages, 4 Figures. First two authors have contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2503.17395v1",
    "published_date": "2025-03-18 10:01:06 UTC",
    "updated_date": "2025-03-18 10:01:06 UTC"
  },
  {
    "arxiv_id": "2503.14076v1",
    "title": "Theoretical Foundation of Flow-Based Time Series Generation: Provable Approximation, Generalization, and Efficiency",
    "authors": [
      "Jiangxuan Long",
      "Zhao Song",
      "Chiwun Yang"
    ],
    "abstract": "Recent studies suggest utilizing generative models instead of traditional\nauto-regressive algorithms for time series forecasting (TSF) tasks. These\nnon-auto-regressive approaches involving different generative methods,\nincluding GAN, Diffusion, and Flow Matching for time series, have empirically\ndemonstrated high-quality generation capability and accuracy. However, we still\nlack an appropriate understanding of how it processes approximation and\ngeneralization. This paper presents the first theoretical framework from the\nperspective of flow-based generative models to relieve the knowledge of\nlimitations. In particular, we provide our insights with strict guarantees from\nthree perspectives: $\\textbf{Approximation}$, $\\textbf{Generalization}$ and\n$\\textbf{Efficiency}$. In detail, our analysis achieves the contributions as\nfollows:\n  $\\bullet$ By assuming a general data model, the fitting of the flow-based\ngenerative models is confirmed to converge to arbitrary error under the\nuniversal approximation of Diffusion Transformer (DiT).\n  $\\bullet$ Introducing a polynomial-based regularization for flow matching,\nthe generalization error thus be bounded since the generalization of polynomial\napproximation.\n  $\\bullet$ The sampling for generation is considered as an optimization\nprocess, we demonstrate its fast convergence with updating standard first-order\ngradient descent of some objective.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "33 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.14076v1",
    "published_date": "2025-03-18 09:53:48 UTC",
    "updated_date": "2025-03-18 09:53:48 UTC"
  },
  {
    "arxiv_id": "2503.14070v1",
    "title": "Fast Autoregressive Video Generation with Diagonal Decoding",
    "authors": [
      "Yang Ye",
      "Junliang Guo",
      "Haoyu Wu",
      "Tianyu He",
      "Tim Pearce",
      "Tabish Rashid",
      "Katja Hofmann",
      "Jiang Bian"
    ],
    "abstract": "Autoregressive Transformer models have demonstrated impressive performance in\nvideo generation, but their sequential token-by-token decoding process poses a\nmajor bottleneck, particularly for long videos represented by tens of thousands\nof tokens. In this paper, we propose Diagonal Decoding (DiagD), a training-free\ninference acceleration algorithm for autoregressively pre-trained models that\nexploits spatial and temporal correlations in videos. Our method generates\ntokens along diagonal paths in the spatial-temporal token grid, enabling\nparallel decoding within each frame as well as partially overlapping across\nconsecutive frames. The proposed algorithm is versatile and adaptive to various\ngenerative models and tasks, while providing flexible control over the\ntrade-off between inference speed and visual quality. Furthermore, we propose a\ncost-effective finetuning strategy that aligns the attention patterns of the\nmodel with our decoding order, further mitigating the training-inference gap on\nsmall-scale models. Experiments on multiple autoregressive video generation\nmodels and datasets demonstrate that DiagD achieves up to $10\\times$ speedup\ncompared to naive sequential decoding, while maintaining comparable visual\nfidelity.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14070v1",
    "published_date": "2025-03-18 09:42:55 UTC",
    "updated_date": "2025-03-18 09:42:55 UTC"
  },
  {
    "arxiv_id": "2503.16530v1",
    "title": "Enhancing LLM Generation with Knowledge Hypergraph for Evidence-Based Medicine",
    "authors": [
      "Chengfeng Dou",
      "Ying Zhang",
      "Zhi Jin",
      "Wenpin Jiao",
      "Haiyan Zhao",
      "Yongqiang Zhao",
      "Zhengwei Tao"
    ],
    "abstract": "Evidence-based medicine (EBM) plays a crucial role in the application of\nlarge language models (LLMs) in healthcare, as it provides reliable support for\nmedical decision-making processes. Although it benefits from current\nretrieval-augmented generation~(RAG) technologies, it still faces two\nsignificant challenges: the collection of dispersed evidence and the efficient\norganization of this evidence to support the complex queries necessary for EBM.\nTo tackle these issues, we propose using LLMs to gather scattered evidence from\nmultiple sources and present a knowledge hypergraph-based evidence management\nmodel to integrate these evidence while capturing intricate relationships.\nFurthermore, to better support complex queries, we have developed an\nImportance-Driven Evidence Prioritization (IDEP) algorithm that utilizes the\nLLM to generate multiple evidence features, each with an associated importance\nscore, which are then used to rank the evidence and produce the final retrieval\nresults. Experimental results from six datasets demonstrate that our approach\noutperforms existing RAG techniques in application domains of interest to EBM,\nsuch as medical quizzing, hallucination detection, and decision support.\nTestsets and the constructed knowledge graph can be accessed at\n\\href{https://drive.google.com/file/d/1WJ9QTokK3MdkjEmwuFQxwH96j_Byawj_/view?usp=drive_link}{https://drive.google.com/rag4ebm}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16530v1",
    "published_date": "2025-03-18 09:17:31 UTC",
    "updated_date": "2025-03-18 09:17:31 UTC"
  },
  {
    "arxiv_id": "2503.14053v1",
    "title": "ON-Traffic: An Operator Learning Framework for Online Traffic Flow Estimation and Uncertainty Quantification from Lagrangian Sensors",
    "authors": [
      "Jake Rap",
      "Amritam Das"
    ],
    "abstract": "Accurate traffic flow estimation and prediction are critical for the\nefficient management of transportation systems, particularly under increasing\nurbanization. Traditional methods relying on static sensors often suffer from\nlimited spatial coverage, while probe vehicles provide richer, albeit sparse\nand irregular data. This work introduces ON-Traffic, a novel deep operator\nNetwork and a receding horizon learning-based framework tailored for online\nestimation of spatio-temporal traffic state along with quantified uncertainty\nby using measurements from moving probe vehicles and downstream boundary\ninputs. Our framework is evaluated in both numerical and simulation datasets,\nshowcasing its ability to handle irregular, sparse input data, adapt to\ntime-shifted scenarios, and provide well-calibrated uncertainty estimates. The\nresults demonstrate that the model captures complex traffic phenomena,\nincluding shockwaves and congestion propagation, while maintaining robustness\nto noise and sensor dropout. These advancements present a significant step\ntoward online, adaptive traffic management systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14053v1",
    "published_date": "2025-03-18 09:13:24 UTC",
    "updated_date": "2025-03-18 09:13:24 UTC"
  },
  {
    "arxiv_id": "2503.14048v1",
    "title": "Beyond holography: the entropic quantum gravity foundations of image processing",
    "authors": [
      "Ginestra Bianconi"
    ],
    "abstract": "Recently, thanks to the development of artificial intelligence (AI) there is\nincreasing scientific attention to establishing the connections between\ntheoretical physics and AI. Traditionally, these connections have been focusing\nmostly on the relation between string theory and image processing and involve\nimportant theoretical paradigms such as holography. Recently G. Bianconi has\nproposed the entropic quantum gravity approach that proposes an action for\ngravity given by the quantum relative entropy between the metrics associated to\na manifold. Here it is demonstrated that the famous Perona-Malik algorithm for\nimage processing is the gradient flow of the entropic quantum gravity action.\nThese results provide the geometrical and information theory foundations for\nthe Perona-Malik algorithm and open new avenues for establishing fundamental\nrelations between brain research, machine learning and entropic quantum\ngravity.",
    "categories": [
      "cond-mat.dis-nn",
      "cond-mat.stat-mech",
      "cs.AI",
      "gr-qc",
      "quant-ph"
    ],
    "primary_category": "cond-mat.dis-nn",
    "comment": "(7 pages, 1 figure)",
    "pdf_url": "http://arxiv.org/pdf/2503.14048v1",
    "published_date": "2025-03-18 09:06:33 UTC",
    "updated_date": "2025-03-18 09:06:33 UTC"
  },
  {
    "arxiv_id": "2503.16529v1",
    "title": "Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts",
    "authors": [
      "Wenjing Zhang",
      "Xuejiao Lei",
      "Zhaoxiang Liu",
      "Limin Han",
      "Jiaojiao Zhao",
      "Beibei Huang",
      "Zhenhong Long",
      "Junting Guo",
      "Meijuan An",
      "Rongjia Du",
      "Ning Wang",
      "Kai Wang",
      "Shiguo Lian"
    ],
    "abstract": "DeepSeek-R1, renowned for its exceptional reasoning capabilities and\nopen-source strategy, is significantly influencing the global artificial\nintelligence landscape. However, it exhibits notable safety shortcomings.\nRecent research conducted by Robust Intelligence, a subsidiary of Cisco, in\ncollaboration with the University of Pennsylvania, revealed that DeepSeek-R1\nachieves a 100\\% attack success rate when processing harmful prompts.\nFurthermore, multiple security firms and research institutions have identified\ncritical security vulnerabilities within the model. Although China Unicom has\nuncovered safety vulnerabilities of R1 in Chinese contexts, the safety\ncapabilities of the remaining distilled models in the R1 series have not yet\nbeen comprehensively evaluated. To address this gap, this study utilizes the\ncomprehensive Chinese safety benchmark CHiSafetyBench to conduct an in-depth\nsafety evaluation of the DeepSeek-R1 series distilled models. The objective is\nto assess the safety capabilities of these models in Chinese contexts both\nbefore and after distillation, and to further elucidate the adverse effects of\ndistillation on model safety. Building on these findings, we implement targeted\nsafety enhancements for six distilled models. Evaluation results indicate that\nthe enhanced models achieve significant improvements in safety while\nmaintaining reasoning capabilities without notable degradation. We open-source\nthe safety-enhanced models at\nhttps://github.com/UnicomAI/DeepSeek-R1-Distill-Safe/tree/main to serve as a\nvaluable resource for future research and optimization of DeepSeek models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages,13 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.16529v1",
    "published_date": "2025-03-18 08:38:10 UTC",
    "updated_date": "2025-03-18 08:38:10 UTC"
  },
  {
    "arxiv_id": "2503.14021v1",
    "title": "MP-GUI: Modality Perception with MLLMs for GUI Understanding",
    "authors": [
      "Ziwei Wang",
      "Weizhi Chen",
      "Leyang Yang",
      "Sheng Zhou",
      "Shengchu Zhao",
      "Hanbei Zhan",
      "Jiongchao Jin",
      "Liangcheng Li",
      "Zirui Shao",
      "Jiajun Bu"
    ],
    "abstract": "Graphical user interface (GUI) has become integral to modern society, making\nit crucial to be understood for human-centric systems. However, unlike natural\nimages or documents, GUIs comprise artificially designed graphical elements\narranged to convey specific semantic meanings. Current multi-modal large\nlanguage models (MLLMs) already proficient in processing graphical and textual\ncomponents suffer from hurdles in GUI understanding due to the lack of explicit\nspatial structure modeling. Moreover, obtaining high-quality spatial structure\ndata is challenging due to privacy issues and noisy environments. To address\nthese challenges, we present MP-GUI, a specially designed MLLM for GUI\nunderstanding. MP-GUI features three precisely specialized perceivers to\nextract graphical, textual, and spatial modalities from the screen as\nGUI-tailored visual clues, with spatial structure refinement strategy and\nadaptively combined via a fusion gate to meet the specific preferences of\ndifferent GUI understanding tasks. To cope with the scarcity of training data,\nwe also introduce a pipeline for automatically data collecting. Extensive\nexperiments demonstrate that MP-GUI achieves impressive results on various GUI\nunderstanding tasks with limited data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "Paper accepted to CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.14021v1",
    "published_date": "2025-03-18 08:32:22 UTC",
    "updated_date": "2025-03-18 08:32:22 UTC"
  },
  {
    "arxiv_id": "2503.14013v1",
    "title": "Boosting Semi-Supervised Medical Image Segmentation via Masked Image Consistency and Discrepancy Learning",
    "authors": [
      "Pengcheng Zhou",
      "Lantian Zhang",
      "Wei Li"
    ],
    "abstract": "Semi-supervised learning is of great significance in medical image\nsegmentation by exploiting unlabeled data. Among its strategies, the\nco-training framework is prominent. However, previous co-training studies\npredominantly concentrate on network initialization variances and pseudo-label\ngeneration, while overlooking the equilibrium between information interchange\nand model diversity preservation. In this paper, we propose the Masked Image\nConsistency and Discrepancy Learning (MICD) framework with three key modules.\nThe Masked Cross Pseudo Consistency (MCPC) module enriches context perception\nand small sample learning via pseudo-labeling across masked-input branches. The\nCross Feature Consistency (CFC) module fortifies information exchange and model\nrobustness by ensuring decoder feature consistency. The Cross Model Discrepancy\n(CMD) module utilizes EMA teacher networks to oversee outputs and preserve\nbranch diversity. Together, these modules address existing limitations by\nfocusing on fine-grained local information and maintaining diversity in a\nheterogeneous framework. Experiments on two public medical image datasets, AMOS\nand Synapse, demonstrate that our approach outperforms state-of-the-art\nmethods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14013v1",
    "published_date": "2025-03-18 08:20:35 UTC",
    "updated_date": "2025-03-18 08:20:35 UTC"
  },
  {
    "arxiv_id": "2503.14002v1",
    "title": "MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific Generative Modeling",
    "authors": [
      "Damian Boborzi",
      "Phillip Mueller",
      "Jonas Emrich",
      "Dominik Schmid",
      "Sebastian Mueller",
      "Lars Mikelsons"
    ],
    "abstract": "Generative models have recently made remarkable progress in the field of 3D\nobjects. However, their practical application in fields like engineering\nremains limited since they fail to deliver the accuracy, quality, and\ncontrollability needed for domain-specific tasks. Fine-tuning large generative\nmodels is a promising perspective for making these models available in these\nfields. Creating high-quality, domain-specific 3D datasets is crucial for\nfine-tuning large generative models, yet the data filtering and annotation\nprocess remains a significant bottleneck. We present MeshFleet, a filtered and\nannotated 3D vehicle dataset extracted from Objaverse-XL, the most extensive\npublicly available collection of 3D objects. Our approach proposes a pipeline\nfor automated data filtering based on a quality classifier. This classifier is\ntrained on a manually labeled subset of Objaverse, incorporating DINOv2 and\nSigLIP embeddings, refined through caption-based analysis and uncertainty\nestimation. We demonstrate the efficacy of our filtering method through a\ncomparative analysis against caption and image aesthetic score-based techniques\nand fine-tuning experiments with SV3D, highlighting the importance of targeted\ndata selection for domain-specific 3D generative modeling.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14002v1",
    "published_date": "2025-03-18 08:09:24 UTC",
    "updated_date": "2025-03-18 08:09:24 UTC"
  },
  {
    "arxiv_id": "2503.13999v2",
    "title": "BI-RADS prediction of mammographic masses using uncertainty information extracted from a Bayesian Deep Learning model",
    "authors": [
      "Mohaddeseh Chegini",
      "Ali Mahloojifar"
    ],
    "abstract": "The BI_RADS score is a probabilistic reporting tool used by radiologists to\nexpress the level of uncertainty in predicting breast cancer based on some\nmorphological features in mammography images. There is a significant\nvariability in describing masses which sometimes leads to BI_RADS\nmisclassification. Using a BI_RADS prediction system is required to support the\nfinal radiologist decisions. In this study, the uncertainty information\nextracted by a Bayesian deep learning model is utilized to predict the BI_RADS\nscore. The investigation results based on the pathology information demonstrate\nthat the f1-scores of the predictions of the radiologist are 42.86%, 48.33% and\n48.28%, meanwhile, the f1-scores of the model performance are 73.33%, 59.60%\nand 59.26% in the BI_RADS 2, 3 and 5 dataset samples, respectively. Also, the\nmodel can distinguish malignant from benign samples in the BI_RADS 0 category\nof the used dataset with an accuracy of 75.86% and correctly identify all\nmalignant samples as BI_RADS 5. The Grad-CAM visualization shows the model pays\nattention to the morphological features of the lesions. Therefore, this study\nshows the uncertainty-aware Bayesian Deep Learning model can report his\nuncertainty about the malignancy of a lesion based on morphological features,\nlike a radiologist.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13999v2",
    "published_date": "2025-03-18 08:06:05 UTC",
    "updated_date": "2025-03-24 12:24:58 UTC"
  },
  {
    "arxiv_id": "2503.13991v1",
    "title": "GraphTEN: Graph Enhanced Texture Encoding Network",
    "authors": [
      "Bo Peng",
      "Jintao Chen",
      "Mufeng Yao",
      "Chenhao Zhang",
      "Jianghui Zhang",
      "Mingmin Chi",
      "Jiang Tao"
    ],
    "abstract": "Texture recognition is a fundamental problem in computer vision and pattern\nrecognition. Recent progress leverages feature aggregation into discriminative\ndescriptions based on convolutional neural networks (CNNs). However, modeling\nnon-local context relations through visual primitives remains challenging due\nto the variability and randomness of texture primitives in spatial\ndistributions. In this paper, we propose a graph-enhanced texture encoding\nnetwork (GraphTEN) designed to capture both local and global features of\ntexture primitives. GraphTEN models global associations through fully connected\ngraphs and captures cross-scale dependencies of texture primitives via\nbipartite graphs. Additionally, we introduce a patch encoding module that\nutilizes a codebook to achieve an orderless representation of texture by\nencoding multi-scale patch features into a unified feature space. The proposed\nGraphTEN achieves superior performance compared to state-of-the-art methods\nacross five publicly available datasets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T45",
      "I.2.10; I.4.7"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 7 figures, conference paper",
    "pdf_url": "http://arxiv.org/pdf/2503.13991v1",
    "published_date": "2025-03-18 07:51:13 UTC",
    "updated_date": "2025-03-18 07:51:13 UTC"
  },
  {
    "arxiv_id": "2503.14564v1",
    "title": "Effortless Active Labeling for Long-Term Test-Time Adaptation",
    "authors": [
      "Guowei Wang",
      "Changxing Ding"
    ],
    "abstract": "Long-term test-time adaptation (TTA) is a challenging task due to error\naccumulation. Recent approaches tackle this issue by actively labeling a small\nproportion of samples in each batch, yet the annotation burden quickly grows as\nthe batch number increases. In this paper, we investigate how to achieve\neffortless active labeling so that a maximum of one sample is selected for\nannotation in each batch. First, we annotate the most valuable sample in each\nbatch based on the single-step optimization perspective in the TTA context. In\nthis scenario, the samples that border between the source- and target-domain\ndata distributions are considered the most feasible for the model to learn in\none iteration. Then, we introduce an efficient strategy to identify these\nsamples using feature perturbation. Second, we discover that the gradient\nmagnitudes produced by the annotated and unannotated samples have significant\nvariations. Therefore, we propose balancing their impact on model optimization\nusing two dynamic weights. Extensive experiments on the popular ImageNet-C, -R,\n-K, -A and PACS databases demonstrate that our approach consistently\noutperforms state-of-the-art methods with significantly lower annotation costs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025.Code:https://github.com/flash1803/EATTA",
    "pdf_url": "http://arxiv.org/pdf/2503.14564v1",
    "published_date": "2025-03-18 07:49:27 UTC",
    "updated_date": "2025-03-18 07:49:27 UTC"
  },
  {
    "arxiv_id": "2503.14563v2",
    "title": "Workflow for Safe-AI",
    "authors": [
      "Suzana Veljanovska",
      "Hans Dermot Doran"
    ],
    "abstract": "The development and deployment of safe and dependable AI models is crucial in\napplications where functional safety is a key concern. Given the rapid\nadvancement in AI research and the relative novelty of the safe-AI domain,\nthere is an increasing need for a workflow that balances stability with\nadaptability. This work proposes a transparent, complete, yet flexible and\nlightweight workflow that highlights both reliability and qualifiability. The\ncore idea is that the workflow must be qualifiable, which demands the use of\nqualified tools. Tool qualification is a resource-intensive process, both in\nterms of time and cost. We therefore place value on a lightweight workflow\nfeaturing a minimal number of tools with limited features. The workflow is\nbuilt upon an extended ONNX model description allowing for validation of AI\nalgorithms from their generation to runtime deployment. This validation is\nessential to ensure that models are validated before being reliably deployed\nacross different runtimes, particularly in mixed-criticality systems.\nKeywords-AI workflows, safe-AI, dependable-AI, functional safety, v-model\ndevelopment",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Embedded World Conference, Nuremberg, 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.14563v2",
    "published_date": "2025-03-18 07:45:18 UTC",
    "updated_date": "2025-03-20 07:32:39 UTC"
  },
  {
    "arxiv_id": "2503.13988v1",
    "title": "Empowering Smaller Models: Tuning LLaMA and Gemma with Chain-of-Thought for Ukrainian Exam Tasks",
    "authors": [
      "Mykyta Syromiatnikov",
      "Victoria Ruvinskaya",
      "Nataliia Komleva"
    ],
    "abstract": "Leading large language models have demonstrated impressive capabilities in\nreasoning-intensive tasks, such as standardized educational testing. However,\nthey often require extensive training in low-resource settings with\ninaccessible infrastructure. Small or compact models, though more efficient,\nfrequently lack sufficient support for underrepresented languages, leaving a\nperformance gap in critical domains. This work explores the potential of\nparameter-efficient fine-tuning of compact open-weight language models to\nhandle reasoning-intensive tasks in the underrepresented Ukrainian language,\nbuilding on the findings of the ZNO-Eval benchmark. Parameter-efficient\nfine-tuning of LLaMA 3.1 (8 billion parameters), LLaMA 3.2 (3 billion\nparameters), and Gemma 2 (9 billion parameters) models on chain-of-thought\nsolutions resulted in a modest test score improvement of up to 17.4% on complex\nmatching tasks and 1.6% overall compared to tuning on answer letters alone,\noffering enhanced interpretability and robustness. In addition, the proposed\ntuning method with joint task topic and step-by-step solution generation\noutperforms standard chain-of-thought tuning in matching tasks and provides a\n5.4% gain over the best LLaMA 3.2 model due to guiding the model to recall and\napply domain-relevant information. Contrasting obtained results with zero-shot\nevaluations of leading open-weight and proprietary models such as Qwen,\nDeepSeek R1, OpenAI o1 and o3, Gemini, and Claude, highlight that fine-tuning\nLLaMA and Gemma models with 2,032 step-by-step solutions and 20 to 50 million\ntrainable parameters on a single A100 GPU lets them outperform GPT-4o mini,\nMistral Large, and larger open-weight models. This research also evaluates how\nmerging the quantized adapter with the base model influences the generation\nquality. Source code and tuned models are available at\nhttps://github.com/NLPForUA/ZNO.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 6 tables, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.13988v1",
    "published_date": "2025-03-18 07:44:49 UTC",
    "updated_date": "2025-03-18 07:44:49 UTC"
  },
  {
    "arxiv_id": "2503.13985v1",
    "title": "DefectFill: Realistic Defect Generation with Inpainting Diffusion Model for Visual Inspection",
    "authors": [
      "Jaewoo Song",
      "Daemin Park",
      "Kanghyun Baek",
      "Sangyub Lee",
      "Jooyoung Choi",
      "Eunji Kim",
      "Sungroh Yoon"
    ],
    "abstract": "Developing effective visual inspection models remains challenging due to the\nscarcity of defect data. While image generation models have been used to\nsynthesize defect images, producing highly realistic defects remains difficult.\nWe propose DefectFill, a novel method for realistic defect generation that\nrequires only a few reference defect images. It leverages a fine-tuned\ninpainting diffusion model, optimized with our custom loss functions\nincorporating defect, object, and attention terms. It enables precise capture\nof detailed, localized defect features and their seamless integration into\ndefect-free objects. Additionally, our Low-Fidelity Selection method further\nenhances the defect sample quality. Experiments show that DefectFill generates\nhigh-quality defect images, enabling visual inspection models to achieve\nstate-of-the-art performance on the MVTec AD dataset.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.13985v1",
    "published_date": "2025-03-18 07:42:11 UTC",
    "updated_date": "2025-03-18 07:42:11 UTC"
  },
  {
    "arxiv_id": "2503.14562v1",
    "title": "Analysis of human visual field information using machine learning methods and assessment of their accuracy",
    "authors": [
      "A. I. Medvedeva",
      "V. V. Bakutkin"
    ],
    "abstract": "Subject of research: is the study of methods for analyzing perimetric images\nfor the diagnosis and control of glaucoma diseases. Objects of research: is a\ndataset collected on the ophthalmological perimeter with the results of various\npatient pathologies, since the ophthalmological community is acutely aware of\nthe issue of disease control and import substitution. [5]. Purpose of research:\nis to consider various machine learning methods that can classify glaucoma.\nThis is possible thanks to the classifier built after labeling the dataset. It\nis able to determine from the image whether the visual fields depicted on it\nare the results of the impact of glaucoma on the eyes or other visual diseases.\nEarlier in the work [3], a dataset was described that was collected on the\nTomey perimeter. The average age of the examined patients ranged from 30 to 85\nyears. Methods of research: machine learning methods for classifying image\nresults (stochastic gradient descent, logistic regression, random forest, naive\nBayes). Main results of research: the result of the study is computer modeling\nthat can determine from the image whether the result is glaucoma or another\ndisease (binary classification).",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "in Russian language",
    "pdf_url": "http://arxiv.org/pdf/2503.14562v1",
    "published_date": "2025-03-18 07:39:41 UTC",
    "updated_date": "2025-03-18 07:39:41 UTC"
  },
  {
    "arxiv_id": "2503.16528v1",
    "title": "HDLCoRe: A Training-Free Framework for Mitigating Hallucinations in LLM-Generated HDL",
    "authors": [
      "Heng Ping",
      "Shixuan Li",
      "Peiyu Zhang",
      "Anzhe Cheng",
      "Shukai Duan",
      "Nikos Kanakaris",
      "Xiongye Xiao",
      "Wei Yang",
      "Shahin Nazarian",
      "Andrei Irimia",
      "Paul Bogdan"
    ],
    "abstract": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities in code generation tasks. However, when applied to hardware\ndescription languages (HDL), these models exhibit significant limitations due\nto data scarcity, resulting in hallucinations and incorrect code generation. To\naddress these challenges, we propose HDLCoRe, a training-free framework that\nenhances LLMs' HDL generation capabilities through prompt engineering\ntechniques and retrieval-augmented generation (RAG). Our approach consists of\ntwo main components: (1) an HDL-aware Chain-of-Thought (CoT) prompting\ntechnique with self-verification that classifies tasks by complexity and type,\nincorporates domain-specific knowledge, and guides LLMs through step-by-step\nself-simulation for error correction; and (2) a two-stage heterogeneous RAG\nsystem that addresses formatting inconsistencies through key component\nextraction and efficiently retrieves relevant HDL examples through sequential\nfiltering and re-ranking. HDLCoRe eliminates the need for model fine-tuning\nwhile substantially improving LLMs' HDL generation capabilities. Experimental\nresults demonstrate that our framework achieves superior performance on the\nRTLLM2.0 benchmark, significantly reducing hallucinations and improving both\nsyntactic and functional correctness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16528v1",
    "published_date": "2025-03-18 07:09:39 UTC",
    "updated_date": "2025-03-18 07:09:39 UTC"
  },
  {
    "arxiv_id": "2503.13951v1",
    "title": "FrustumFusionNets: A Three-Dimensional Object Detection Network Based on Tractor Road Scene",
    "authors": [
      "Lili Yang",
      "Mengshuai Chang",
      "Xiao Guo",
      "Yuxin Feng",
      "Yiwen Mei",
      "Caicong Wu"
    ],
    "abstract": "To address the issues of the existing frustum-based methods' underutilization\nof image information in road three-dimensional object detection as well as the\nlack of research on agricultural scenes, we constructed an object detection\ndataset using an 80-line Light Detection And Ranging (LiDAR) and a camera in a\ncomplex tractor road scene and proposed a new network called FrustumFusionNets\n(FFNets). Initially, we utilize the results of image-based two-dimensional\nobject detection to narrow down the search region in the three-dimensional\nspace of the point cloud. Next, we introduce a Gaussian mask to enhance the\npoint cloud information. Then, we extract the features from the frustum point\ncloud and the crop image using the point cloud feature extraction pipeline and\nthe image feature extraction pipeline, respectively. Finally, we concatenate\nand fuse the data features from both modalities to achieve three-dimensional\nobject detection. Experiments demonstrate that on the constructed test set of\ntractor road data, the FrustumFusionNetv2 achieves 82.28% and 95.68% accuracy\nin the three-dimensional object detection of the two main road objects, cars\nand people, respectively. This performance is 1.83% and 2.33% better than the\noriginal model. It offers a hybrid fusion-based multi-object, high-precision,\nreal-time three-dimensional object detection technique for unmanned\nagricultural machines in tractor road scenarios. On the Karlsruhe Institute of\nTechnology and Toyota Technological Institute (KITTI) Benchmark Suite\nvalidation set, the FrustumFusionNetv2 also demonstrates significant\nsuperiority in detecting road pedestrian objects compared with other\nfrustum-based three-dimensional object detection methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13951v1",
    "published_date": "2025-03-18 06:40:39 UTC",
    "updated_date": "2025-03-18 06:40:39 UTC"
  },
  {
    "arxiv_id": "2503.15550v2",
    "title": "Zero-Knowledge Federated Learning: A New Trustworthy and Privacy-Preserving Distributed Learning Paradigm",
    "authors": [
      "Yuxin Jin",
      "Taotao Wang",
      "Qing Yang",
      "Long Shi",
      "Shengli Zhang"
    ],
    "abstract": "Federated Learning (FL) has emerged as a promising paradigm in distributed\nmachine learning, enabling collaborative model training while preserving data\nprivacy. However, despite its many advantages, FL still contends with\nsignificant challenges -- most notably regarding security and trust.\nZero-Knowledge Proofs (ZKPs) offer a potential solution by establishing trust\nand enhancing system integrity throughout the FL process. Although several\nstudies have explored ZKP-based FL (ZK-FL), a systematic framework and\ncomprehensive analysis are still lacking. This article makes two key\ncontributions. First, we propose a structured ZK-FL framework that categorizes\nand analyzes the technical roles of ZKPs across various FL stages and tasks.\nSecond, we introduce a novel algorithm, Verifiable Client Selection FL\n(Veri-CS-FL), which employs ZKPs to refine the client selection process. In\nVeri-CS-FL, participating clients generate verifiable proofs for the\nperformance metrics of their local models and submit these concise proofs to\nthe server for efficient verification. The server then selects clients with\nhigh-quality local models for uploading, subsequently aggregating the\ncontributions from these selected clients. By integrating ZKPs, Veri-CS-FL not\nonly ensures the accuracy of performance metrics but also fortifies trust among\nparticipants while enhancing the overall efficiency and security of FL systems.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "7 pages, 5 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2503.15550v2",
    "published_date": "2025-03-18 06:21:08 UTC",
    "updated_date": "2025-03-24 03:55:23 UTC"
  },
  {
    "arxiv_id": "2503.13938v2",
    "title": "ChatBEV: A Visual Language Model that Understands BEV Maps",
    "authors": [
      "Qingyao Xu",
      "Siheng Chen",
      "Guang Chen",
      "Yanfeng Wang",
      "Ya Zhang"
    ],
    "abstract": "Traffic scene understanding is essential for intelligent transportation\nsystems and autonomous driving, ensuring safe and efficient vehicle operation.\nWhile recent advancements in VLMs have shown promise for holistic scene\nunderstanding, the application of VLMs to traffic scenarios, particularly using\nBEV maps, remains under explored. Existing methods often suffer from limited\ntask design and narrow data amount, hindering comprehensive scene\nunderstanding. To address these challenges, we introduce ChatBEV-QA, a novel\nBEV VQA benchmark contains over 137k questions, designed to encompass a wide\nrange of scene understanding tasks, including global scene understanding,\nvehicle-lane interactions, and vehicle-vehicle interactions. This benchmark is\nconstructed using an novel data collection pipeline that generates scalable and\ninformative VQA data for BEV maps. We further fine-tune a specialized\nvision-language model ChatBEV, enabling it to interpret diverse question\nprompts and extract relevant context-aware information from BEV maps.\nAdditionally, we propose a language-driven traffic scene generation pipeline,\nwhere ChatBEV facilitates map understanding and text-aligned navigation\nguidance, significantly enhancing the generation of realistic and consistent\ntraffic scenarios. The dataset, code and the fine-tuned model will be released.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13938v2",
    "published_date": "2025-03-18 06:12:38 UTC",
    "updated_date": "2025-03-21 02:17:52 UTC"
  },
  {
    "arxiv_id": "2503.17394v1",
    "title": "Temporal Flexibility in Spiking Neural Networks: Towards Generalization Across Time Steps and Deployment Friendliness",
    "authors": [
      "Kangrui Du",
      "Yuhang Wu",
      "Shikuang Deng",
      "Shi Gu"
    ],
    "abstract": "Spiking Neural Networks (SNNs), models inspired by neural mechanisms in the\nbrain, allow for energy-efficient implementation on neuromorphic hardware.\nHowever, SNNs trained with current direct training approaches are constrained\nto a specific time step. This \"temporal inflexibility\" 1) hinders SNNs'\ndeployment on time-step-free fully event-driven chips and 2) prevents\nenergy-performance balance based on dynamic inference time steps. In this\nstudy, we first explore the feasibility of training SNNs that generalize across\ndifferent time steps. We then introduce Mixed Time-step Training (MTT), a novel\nmethod that improves the temporal flexibility of SNNs, making SNNs adaptive to\ndiverse temporal structures. During each iteration of MTT, random time steps\nare assigned to different SNN stages, with spikes transmitted between stages\nvia communication modules. After training, the weights are deployed and\nevaluated on both time-stepped and fully event-driven platforms. Experimental\nresults show that models trained by MTT gain remarkable temporal flexibility,\nfriendliness for both event-driven and clock-driven deployment (nearly lossless\non N-MNIST and 10.1% higher than standard methods on CIFAR10-DVS), enhanced\nnetwork generalization, and near SOTA performance. To the best of our\nknowledge, this is the first work to report the results of large-scale SNN\ndeployment on fully event-driven scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.17394v1",
    "published_date": "2025-03-18 06:09:42 UTC",
    "updated_date": "2025-03-18 06:09:42 UTC"
  },
  {
    "arxiv_id": "2503.13934v1",
    "title": "COLSON: Controllable Learning-Based Social Navigation via Diffusion-Based Reinforcement Learning",
    "authors": [
      "Yuki Tomita",
      "Kohei Matsumoto",
      "Yuki Hyodo",
      "Ryo Kurazume"
    ],
    "abstract": "Mobile robot navigation in dynamic environments with pedestrian traffic is a\nkey challenge in the development of autonomous mobile service robots. Recently,\ndeep reinforcement learning-based methods have been actively studied and have\noutperformed traditional rule-based approaches owing to their optimization\ncapabilities. Among these, methods that assume a continuous action space\ntypically rely on a Gaussian distribution assumption, which limits the\nflexibility of generated actions. Meanwhile, the application of diffusion\nmodels to reinforcement learning has advanced, allowing for more flexible\naction distributions compared with Gaussian distribution-based approaches. In\nthis study, we applied a diffusion-based reinforcement learning approach to\nsocial navigation and validated its effectiveness. Furthermore, by leveraging\nthe characteristics of diffusion models, we propose an extension that enables\npost-training action smoothing and adaptation to static obstacle scenarios not\nconsidered during the training steps.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "This work has been submitted to IROS 2025 for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2503.13934v1",
    "published_date": "2025-03-18 06:02:30 UTC",
    "updated_date": "2025-03-18 06:02:30 UTC"
  },
  {
    "arxiv_id": "2503.13923v1",
    "title": "ConSCompF: Consistency-focused Similarity Comparison Framework for Generative Large Language Models",
    "authors": [
      "Alexey Karev",
      "Dong Xu"
    ],
    "abstract": "Large language models (LLMs) have been one of the most important discoveries\nin machine learning in recent years. LLM-based artificial intelligence (AI)\nassistants, such as ChatGPT, have consistently attracted the attention from\nresearchers, investors, and the general public, driving the rapid growth of\nthis industry. With the frequent introduction of new LLMs to the market, it\nbecomes increasingly difficult to differentiate between them, creating a demand\nfor new LLM comparison methods.\n  In this research, the Consistency-focused Similarity Comparison Framework\n(ConSCompF) for generative large language models is proposed. It compares texts\ngenerated by two LLMs and produces a similarity score, indicating the overall\ndegree of similarity between their responses. The main advantage of this\nframework is that it can operate on a small number of unlabeled data, such as\nchatbot instruction prompts, and does not require LLM developers to disclose\nany information about their product.\n  To evaluate the efficacy of ConSCompF, two experiments aimed at identifying\nsimilarities between multiple LLMs are conducted. Additionally, these\nexperiments examine the correlation between the similarity scores generated by\nConSCompF and the differences in the outputs produced by other benchmarking\ntechniques, such as ROUGE-L. Finally, a series of few-shot LLM comparison\nexperiments is conducted to evaluate the performance of ConSCompF in a few-shot\nLLM comparison scenario.\n  The proposed framework can be used for calculating similarity matrices of\nmultiple LLMs, which can be effectively visualized using principal component\nanalysis (PCA). The ConSCompF output may provide useful insights into data that\nmight have been used during LLM training and help detect possible investment\nfraud attempts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13923v1",
    "published_date": "2025-03-18 05:38:04 UTC",
    "updated_date": "2025-03-18 05:38:04 UTC"
  },
  {
    "arxiv_id": "2503.13921v1",
    "title": "Learning Accurate Models on Incomplete Data with Minimal Imputation",
    "authors": [
      "Cheng Zhen",
      "Nischal Aryal",
      "Arash Termehchy",
      "Prayoga",
      "Garrett Biwer",
      "Sankalp Patil"
    ],
    "abstract": "Missing data often exists in real-world datasets, requiring significant time\nand effort for imputation to learn accurate machine learning (ML) models. In\nthis paper, we demonstrate that imputing all missing values is not always\nnecessary to achieve an accurate ML model. We introduce the concept of minimal\ndata imputation, which ensures accurate ML models trained over the imputed\ndataset. Implementing minimal imputation guarantees both minimal imputation\neffort and optimal ML models. We propose algorithms to find exact and\napproximate minimal imputation for various ML models. Our extensive experiments\nindicate that our proposed algorithms significantly reduce the time and effort\nrequired for data imputation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13921v1",
    "published_date": "2025-03-18 05:36:59 UTC",
    "updated_date": "2025-03-18 05:36:59 UTC"
  },
  {
    "arxiv_id": "2503.13916v1",
    "title": "Learning Bimanual Manipulation via Action Chunking and Inter-Arm Coordination with Transformers",
    "authors": [
      "Tomohiro Motoda",
      "Ryo Hanai",
      "Ryoichi Nakajo",
      "Masaki Murooka",
      "Floris Erich",
      "Yukiyasu Domae"
    ],
    "abstract": "Robots that can operate autonomously in a human living environment are\nnecessary to have the ability to handle various tasks flexibly. One crucial\nelement is coordinated bimanual movements that enable functions that are\ndifficult to perform with one hand alone. In recent years, learning-based\nmodels that focus on the possibilities of bimanual movements have been\nproposed. However, the high degree of freedom of the robot makes it challenging\nto reason about control, and the left and right robot arms need to adjust their\nactions depending on the situation, making it difficult to realize more\ndexterous tasks. To address the issue, we focus on coordination and efficiency\nbetween both arms, particularly for synchronized actions. Therefore, we propose\na novel imitation learning architecture that predicts cooperative actions. We\ndifferentiate the architecture for both arms and add an intermediate encoder\nlayer, Inter-Arm Coordinated transformer Encoder (IACE), that facilitates\nsynchronization and temporal alignment to ensure smooth and coordinated\nactions. To verify the effectiveness of our architectures, we perform\ndistinctive bimanual tasks. The experimental results showed that our model\ndemonstrated a high success rate for comparison and suggested a suitable\narchitecture for the policy learning of bimanual manipulation.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 5 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2503.13916v1",
    "published_date": "2025-03-18 05:20:34 UTC",
    "updated_date": "2025-03-18 05:20:34 UTC"
  },
  {
    "arxiv_id": "2503.13915v1",
    "title": "Unlocking the Potential of Unlabeled Data in Semi-Supervised Domain Generalization",
    "authors": [
      "Dongkwan Lee",
      "Kyomin Hwang",
      "Nojun Kwak"
    ],
    "abstract": "We address the problem of semi-supervised domain generalization (SSDG), where\nthe distributions of train and test data differ, and only a small amount of\nlabeled data along with a larger amount of unlabeled data are available during\ntraining. Existing SSDG methods that leverage only the unlabeled samples for\nwhich the model's predictions are highly confident (confident-unlabeled\nsamples), limit the full utilization of the available unlabeled data. To the\nbest of our knowledge, we are the first to explore a method for incorporating\nthe unconfident-unlabeled samples that were previously disregarded in SSDG\nsetting. To this end, we propose UPCSC to utilize these unconfident-unlabeled\nsamples in SSDG that consists of two modules: 1) Unlabeled Proxy-based\nContrastive learning (UPC) module, treating unconfident-unlabeled samples as\nadditional negative pairs and 2) Surrogate Class learning (SC) module,\ngenerating positive pairs for unconfident-unlabeled samples using their\nconfusing class set. These modules are plug-and-play and do not require any\ndomain labels, which can be easily integrated into existing approaches.\nExperiments on four widely used SSDG benchmarks demonstrate that our approach\nconsistently improves performance when attached to baselines and outperforms\ncompeting plug-and-play methods. We also analyze the role of our method in\nSSDG, showing that it enhances class-level discriminability and mitigates\ndomain gaps. The code is available at https://github.com/dongkwani/UPCSC.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.13915v1",
    "published_date": "2025-03-18 05:19:33 UTC",
    "updated_date": "2025-03-18 05:19:33 UTC"
  },
  {
    "arxiv_id": "2503.13912v1",
    "title": "KANITE: Kolmogorov-Arnold Networks for ITE estimation",
    "authors": [
      "Eshan Mehendale",
      "Abhinav Thorat",
      "Ravi Kolla",
      "Niranjan Pedanekar"
    ],
    "abstract": "We introduce KANITE, a framework leveraging Kolmogorov-Arnold Networks (KANs)\nfor Individual Treatment Effect (ITE) estimation under multiple treatments\nsetting in causal inference. By utilizing KAN's unique abilities to learn\nunivariate activation functions as opposed to learning linear weights by\nMulti-Layer Perceptrons (MLPs), we improve the estimates of ITEs. The KANITE\nframework comprises two key architectures: 1.Integral Probability Metric (IPM)\narchitecture: This employs an IPM loss in a specialized manner to effectively\nalign towards ITE estimation across multiple treatments. 2. Entropy Balancing\n(EB) architecture: This uses weights for samples that are learned by optimizing\nentropy subject to balancing the covariates across treatment groups. Extensive\nevaluations on benchmark datasets demonstrate that KANITE outperforms\nstate-of-the-art algorithms in both $\\epsilon_{\\text{PEHE}}$ and\n$\\epsilon_{\\text{ATE}}$ metrics. Our experiments highlight the advantages of\nKANITE in achieving improved causal estimates, emphasizing the potential of\nKANs to advance causal inference methodologies across diverse application\nareas.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.13912v1",
    "published_date": "2025-03-18 05:16:36 UTC",
    "updated_date": "2025-03-18 05:16:36 UTC"
  },
  {
    "arxiv_id": "2503.13906v1",
    "title": "HSOD-BIT-V2: A New Challenging Benchmarkfor Hyperspectral Salient Object Detection",
    "authors": [
      "Yuhao Qiu",
      "Shuyan Bai",
      "Tingfa Xu",
      "Peifu Liu",
      "Haolin Qin",
      "Jianan Li"
    ],
    "abstract": "Salient Object Detection (SOD) is crucial in computer vision, yet RGB-based\nmethods face limitations in challenging scenes, such as small objects and\nsimilar color features. Hyperspectral images provide a promising solution for\nmore accurate Hyperspectral Salient Object Detection (HSOD) by abundant\nspectral information, while HSOD methods are hindered by the lack of extensive\nand available datasets. In this context, we introduce HSOD-BIT-V2, the largest\nand most challenging HSOD benchmark dataset to date. Five distinct challenges\nfocusing on small objects and foreground-background similarity are designed to\nemphasize spectral advantages and real-world complexity. To tackle these\nchallenges, we propose Hyper-HRNet, a high-resolution HSOD network. Hyper-HRNet\neffectively extracts, integrates, and preserves effective spectral information\nwhile reducing dimensionality by capturing the self-similar spectral features.\nAdditionally, it conveys fine details and precisely locates object contours by\nincorporating comprehensive global information and detailed object saliency\nrepresentations. Experimental analysis demonstrates that Hyper-HRNet\noutperforms existing models, especially in challenging scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.13906v1",
    "published_date": "2025-03-18 05:09:42 UTC",
    "updated_date": "2025-03-18 05:09:42 UTC"
  },
  {
    "arxiv_id": "2503.13903v1",
    "title": "TGBFormer: Transformer-GraphFormer Blender Network for Video Object Detection",
    "authors": [
      "Qiang Qi",
      "Xiao Wang"
    ],
    "abstract": "Video object detection has made significant progress in recent years thanks\nto convolutional neural networks (CNNs) and vision transformers (ViTs).\nTypically, CNNs excel at capturing local features but struggle to model global\nrepresentations. Conversely, ViTs are adept at capturing long-range global\nfeatures but face challenges in representing local feature details.\nOff-the-shelf video object detection methods solely rely on CNNs or ViTs to\nconduct feature aggregation, which hampers their capability to simultaneously\nleverage global and local information, thereby resulting in limited detection\nperformance. In this paper, we propose a Transformer-GraphFormer Blender\nNetwork (TGBFormer) for video object detection, with three key technical\nimprovements to fully exploit the advantages of transformers and graph\nconvolutional networks while compensating for their limitations. First, we\ndevelop a spatial-temporal transformer module to aggregate global contextual\ninformation, constituting global representations with long-range feature\ndependencies. Second, we introduce a spatial-temporal GraphFormer module that\nutilizes local spatial and temporal relationships to aggregate features,\ngenerating new local representations that are complementary to the transformer\noutputs. Third, we design a global-local feature blender module to adaptively\ncouple transformer-based global representations and GraphFormer-based local\nrepresentations. Extensive experiments demonstrate that our TGBFormer\nestablishes new state-of-the-art results on the ImageNet VID dataset.\nParticularly, our TGBFormer achieves 86.5% mAP while running at around 41.0 FPS\non a single Tesla A100 GPU.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI2025",
    "pdf_url": "http://arxiv.org/pdf/2503.13903v1",
    "published_date": "2025-03-18 05:03:05 UTC",
    "updated_date": "2025-03-18 05:03:05 UTC"
  },
  {
    "arxiv_id": "2503.13882v1",
    "title": "MoK-RAG: Mixture of Knowledge Paths Enhanced Retrieval-Augmented Generation for Embodied AI Environments",
    "authors": [
      "Zhengsheng Guo",
      "Linwei Zheng",
      "Xinyang Chen",
      "Xuefeng Bai",
      "Kehai Chen",
      "Min Zhang"
    ],
    "abstract": "While human cognition inherently retrieves information from diverse and\nspecialized knowledge sources during decision-making processes, current\nRetrieval-Augmented Generation (RAG) systems typically operate through\nsingle-source knowledge retrieval, leading to a cognitive-algorithmic\ndiscrepancy. To bridge this gap, we introduce MoK-RAG, a novel multi-source RAG\nframework that implements a mixture of knowledge paths enhanced retrieval\nmechanism through functional partitioning of a large language model (LLM)\ncorpus into distinct sections, enabling retrieval from multiple specialized\nknowledge paths. Applied to the generation of 3D simulated environments, our\nproposed MoK-RAG3D enhances this paradigm by partitioning 3D assets into\ndistinct sections and organizing them based on a hierarchical knowledge tree\nstructure. Different from previous methods that only use manual evaluation, we\npioneered the introduction of automated evaluation methods for 3D scenes. Both\nautomatic and human evaluations in our experiments demonstrate that MoK-RAG3D\ncan assist Embodied AI agents in generating diverse scenes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13882v1",
    "published_date": "2025-03-18 04:27:02 UTC",
    "updated_date": "2025-03-18 04:27:02 UTC"
  },
  {
    "arxiv_id": "2503.13879v2",
    "title": "Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review Generation via Cognitive Alignment",
    "authors": [
      "Wei Chen",
      "Han Ding",
      "Meng Yuan",
      "Zhao Zhang",
      "Deqing Wang",
      "Fuzhen Zhuang"
    ],
    "abstract": "The rapid growth of scholarly submissions has overwhelmed traditional peer\nreview systems, driving the need for intelligent automation to preserve\nscientific rigor. While large language models (LLMs) show promise in automating\nmanuscript critiques, their ability to synthesize high-stakes meta-reviews,\nwhich require conflict-aware reasoning and consensus derivation, remains\nunderdeveloped. Existing methods fail to effectively handle conflicting\nviewpoints within differing opinions, and often introduce additional cognitive\nbiases, such as anchoring effects and conformity bias.To overcome these\nlimitations, we propose the Cognitive Alignment Framework (CAF), a dual-process\narchitecture that transforms LLMs into adaptive scientific arbitrators. By\noperationalizing Kahneman's dual-process theory, CAF introduces a three-step\ncognitive pipeline: review initialization, incremental integration, and\ncognitive alignment.Empirical validation shows that CAF outperforms existing\nLLM-based methods, with sentiment consistency gains reaching up to 19.47\\% and\ncontent consistency improving by as much as 12.95\\%.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.13879v2",
    "published_date": "2025-03-18 04:13:11 UTC",
    "updated_date": "2025-03-21 07:36:18 UTC"
  },
  {
    "arxiv_id": "2503.14559v1",
    "title": "Squeeze Out Tokens from Sample for Finer-Grained Data Governance",
    "authors": [
      "Weixiong Lin",
      "Chen Ju",
      "Haicheng Wang",
      "Shengchao Hu",
      "Shuai Xiao",
      "Mengting Chen",
      "Yuheng Jiao",
      "Mingshuai Yao",
      "Jinsong Lan",
      "Qingwen Liu",
      "Ying Chen"
    ],
    "abstract": "Widely observed data scaling laws, in which error falls off as a power of the\ntraining size, demonstrate the diminishing returns of unselective data\nexpansion. Hence, data governance is proposed to downsize datasets through\npruning non-informative samples. Yet, isolating the impact of a specific sample\non overall model performance is challenging, due to the vast computation\nrequired for tryout all sample combinations. Current data governors circumvent\nthis complexity by estimating sample contributions through heuristic-derived\nscalar scores, thereby discarding low-value ones. Despite thorough sample\nsieving, retained samples contain substantial undesired tokens intrinsically,\nunderscoring the potential for further compression and purification. In this\nwork, we upgrade data governance from a 'sieving' approach to a 'juicing' one.\nInstead of scanning for least-flawed samples, our dual-branch DataJuicer\napplies finer-grained intra-sample governance. It squeezes out informative\ntokens and boosts image-text alignments. Specifically, the vision branch\nretains salient image patches and extracts relevant object classes, while the\ntext branch incorporates these classes to enhance captions. Consequently,\nDataJuicer yields more refined datasets through finer-grained governance.\nExtensive experiments across datasets demonstrate that DataJuicer significantly\noutperforms existing DataSieve in image-text retrieval, classification, and\ndense visual reasoning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14559v1",
    "published_date": "2025-03-18 04:06:50 UTC",
    "updated_date": "2025-03-18 04:06:50 UTC"
  },
  {
    "arxiv_id": "2503.13868v1",
    "title": "Out-of-Distribution Generalization in Time Series: A Survey",
    "authors": [
      "Xin Wu",
      "Fei Teng",
      "Xingwang Li",
      "Ji Zhang",
      "Tianrui Li",
      "Qiang Duan"
    ],
    "abstract": "Time series frequently manifest distribution shifts, diverse latent features,\nand non-stationary learning dynamics, particularly in open and evolving\nenvironments. These characteristics pose significant challenges for\nout-of-distribution (OOD) generalization. While substantial progress has been\nmade, a systematic synthesis of advancements remains lacking. To address this\ngap, we present the first comprehensive review of OOD generalization\nmethodologies for time series, organized to delineate the field's evolutionary\ntrajectory and contemporary research landscape. We organize our analysis across\nthree foundational dimensions: data distribution, representation learning, and\nOOD evaluation. For each dimension, we present several popular algorithms in\ndetail. Furthermore, we highlight key application scenarios, emphasizing their\nreal-world impact. Finally, we identify persistent challenges and propose\nfuture research directions. A detailed summary of the methods reviewed for the\ngeneralization of OOD in time series can be accessed at\nhttps://tsood-generalization.com.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 8 figures, 5 tables. Work in Progress",
    "pdf_url": "http://arxiv.org/pdf/2503.13868v1",
    "published_date": "2025-03-18 03:35:29 UTC",
    "updated_date": "2025-03-18 03:35:29 UTC"
  },
  {
    "arxiv_id": "2503.13861v1",
    "title": "RAD: Retrieval-Augmented Decision-Making of Meta-Actions with Vision-Language Models in Autonomous Driving",
    "authors": [
      "Yujin Wang",
      "Quanfeng Liu",
      "Zhengxin Jiang",
      "Tianyi Wang",
      "Junfeng Jiao",
      "Hongqing Chu",
      "Bingzhao Gao",
      "Hong Chen"
    ],
    "abstract": "Accurately understanding and deciding high-level meta-actions is essential\nfor ensuring reliable and safe autonomous driving systems. While\nvision-language models (VLMs) have shown significant potential in various\nautonomous driving tasks, they often suffer from limitations such as inadequate\nspatial perception and hallucination, reducing their effectiveness in complex\nautonomous driving scenarios. To address these challenges, we propose a\nretrieval-augmented decision-making (RAD) framework, a novel architecture\ndesigned to enhance VLMs' capabilities to reliably generate meta-actions in\nautonomous driving scenes. RAD leverages a retrieval-augmented generation (RAG)\npipeline to dynamically improve decision accuracy through a three-stage process\nconsisting of the embedding flow, retrieving flow, and generating flow.\nAdditionally, we fine-tune VLMs on a specifically curated dataset derived from\nthe NuScenes dataset to enhance their spatial perception and bird's-eye view\nimage comprehension capabilities. Extensive experimental evaluations on the\ncurated NuScenes-based dataset demonstrate that RAD outperforms baseline\nmethods across key evaluation metrics, including match accuracy, and F1 score,\nand self-defined overall score, highlighting its effectiveness in improving\nmeta-action decision-making for autonomous driving tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13861v1",
    "published_date": "2025-03-18 03:25:57 UTC",
    "updated_date": "2025-03-18 03:25:57 UTC"
  },
  {
    "arxiv_id": "2503.16527v1",
    "title": "LLM Generated Persona is a Promise with a Catch",
    "authors": [
      "Ang Li",
      "Haozhe Chen",
      "Hongseok Namkoong",
      "Tianyi Peng"
    ],
    "abstract": "The use of large language models (LLMs) to simulate human behavior has gained\nsignificant attention, particularly through personas that approximate\nindividual characteristics. Persona-based simulations hold promise for\ntransforming disciplines that rely on population-level feedback, including\nsocial science, economic analysis, marketing research, and business operations.\nTraditional methods to collect realistic persona data face significant\nchallenges. They are prohibitively expensive and logistically challenging due\nto privacy constraints, and often fail to capture multi-dimensional attributes,\nparticularly subjective qualities. Consequently, synthetic persona generation\nwith LLMs offers a scalable, cost-effective alternative. However, current\napproaches rely on ad hoc and heuristic generation techniques that do not\nguarantee methodological rigor or simulation precision, resulting in systematic\nbiases in downstream tasks. Through extensive large-scale experiments including\npresidential election forecasts and general opinion surveys of the U.S.\npopulation, we reveal that these biases can lead to significant deviations from\nreal-world outcomes. Our findings underscore the need to develop a rigorous\nscience of persona generation and outline the methodological innovations,\norganizational and institutional support, and empirical foundations required to\nenhance the reliability and scalability of LLM-driven persona simulations. To\nsupport further research and development in this area, we have open-sourced\napproximately one million generated personas, available for public access and\nanalysis at https://huggingface.co/datasets/Tianyi-Lab/Personas.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16527v1",
    "published_date": "2025-03-18 03:11:27 UTC",
    "updated_date": "2025-03-18 03:11:27 UTC"
  },
  {
    "arxiv_id": "2503.13856v1",
    "title": "MDTeamGPT: A Self-Evolving LLM-based Multi-Agent Framework for Multi-Disciplinary Team Medical Consultation",
    "authors": [
      "Kai Chen",
      "Xinfeng Li",
      "Tianpei Yang",
      "Hewei Wang",
      "Wei Dong",
      "Yang Gao"
    ],
    "abstract": "Large Language Models (LLMs) have made significant progress in various\nfields. However, challenges remain in Multi-Disciplinary Team (MDT) medical\nconsultations. Current research enhances reasoning through role assignment,\ntask decomposition, and accumulation of medical experience. Multi-role\ncollaboration in MDT consultations often results in excessively long dialogue\nhistories. This increases the model's cognitive burden and degrades both\nefficiency and accuracy. Some methods only store treatment histories. They do\nnot extract effective experience or reflect on errors. This limits knowledge\ngeneralization and system evolution. We propose a multi-agent MDT medical\nconsultation framework based on LLMs to address these issues. Our framework\nuses consensus aggregation and a residual discussion structure for multi-round\nconsultations. It also employs a Correct Answer Knowledge Base (CorrectKB) and\na Chain-of-Thought Knowledge Base (ChainKB) to accumulate consultation\nexperience. These mechanisms enable the framework to evolve and continually\nimprove diagnosis rationality and accuracy. Experimental results on the MedQA\nand PubMedQA datasets demonstrate that our framework achieves accuracies of\n90.1% and 83.9%, respectively, and that the constructed knowledge bases\ngeneralize effectively across test sets from both datasets.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "24 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.13856v1",
    "published_date": "2025-03-18 03:07:34 UTC",
    "updated_date": "2025-03-18 03:07:34 UTC"
  },
  {
    "arxiv_id": "2503.13847v1",
    "title": "Disentangling Fine-Tuning from Pre-Training in Visual Captioning with Hybrid Markov Logic",
    "authors": [
      "Monika Shah",
      "Somdeb Sarkhel",
      "Deepak Venugopal"
    ],
    "abstract": "Multimodal systems have highly complex processing pipelines and are\npretrained over large datasets before being fine-tuned for specific tasks such\nas visual captioning. However, it becomes hard to disentangle what the model\nlearns during the fine-tuning process from what it already knows due to its\npretraining. In this work, we learn a probabilistic model using Hybrid Markov\nLogic Networks (HMLNs) over the training examples by relating symbolic\nknowledge (extracted from the caption) with visual features (extracted from the\nimage). For a generated caption, we quantify the influence of training examples\nbased on the HMLN distribution using probabilistic inference. We evaluate two\ntypes of inference procedures on the MSCOCO dataset for different types of\ncaptioning models. Our results show that for BLIP2 (a model that uses a LLM),\nthe fine-tuning may have smaller influence on the knowledge the model has\nacquired since it may have more general knowledge to perform visual captioning\nas compared to models that do not use a LLM",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "2024 IEEE International Conference on Big Data (BigData), 10 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.13847v1",
    "published_date": "2025-03-18 02:39:26 UTC",
    "updated_date": "2025-03-18 02:39:26 UTC"
  },
  {
    "arxiv_id": "2503.13844v1",
    "title": "Spotting Persuasion: A Low-cost Model for Persuasion Detection in Political Ads on Social Media",
    "authors": [
      "Elyas Meguellati",
      "Stefano Civelli",
      "Pietro Bernardelle",
      "Shazia Sadiq",
      "Gianluca Demartini"
    ],
    "abstract": "In the realm of political advertising, persuasion operates as a pivotal\nelement within the broader framework of propaganda, exerting profound\ninfluences on public opinion and electoral outcomes. In this paper, we (1)\nintroduce a lightweight model for persuasive text detection that achieves\nstate-of-the-art performance in Subtask 3 of SemEval 2023 Task 3, while\nsignificantly reducing the computational resource requirements; and (2)\nleverage the proposed model to gain insights into political campaigning\nstrategies on social media platforms by applying it to a real-world dataset we\ncurated, consisting of Facebook political ads from the 2022 Australian Federal\nelection campaign. Our study shows how subtleties can be found in persuasive\npolitical advertisements and presents a pragmatic approach to detect and\nanalyze such strategies with limited resources, enhancing transparency in\nsocial media political campaigns.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13844v1",
    "published_date": "2025-03-18 02:33:38 UTC",
    "updated_date": "2025-03-18 02:33:38 UTC"
  },
  {
    "arxiv_id": "2503.13843v1",
    "title": "WebNav: An Intelligent Agent for Voice-Controlled Web Navigation",
    "authors": [
      "Trisanth Srinivasan",
      "Santosh Patapati"
    ],
    "abstract": "The increasing reliance on web interfaces presents many challenges for\nvisually impaired users, showcasing the need for more advanced assistive\ntechnologies. This paper introduces WebNav, a voice-controlled web navigation\nagent that leverages a ReAct-inspired architecture and generative AI to provide\nthis framework. WebNav comprises of a hierarchical structure: a Digital\nNavigation Module (DIGNAV) for high-level strategic planning, an Assistant\nModule for translating abstract commands into executable actions, and an\nInference Module for low-level interaction. A key component is a dynamic\nlabeling engine, implemented as a browser extension, that generates real-time\nlabels for interactive elements, creating mapping between voice commands and\nDocument Object Model (DOM) components. Preliminary evaluations show that\nWebNav outperforms traditional screen readers in response time and task\ncompletion accuracy for the visually impaired. Future work will focus on\nextensive user evaluations, benchmark development, and refining the agent's\nadaptive capabilities for real-world deployment.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "H.5.2; I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13843v1",
    "published_date": "2025-03-18 02:33:27 UTC",
    "updated_date": "2025-03-18 02:33:27 UTC"
  },
  {
    "arxiv_id": "2503.13842v1",
    "title": "Counterfactual experience augmented off-policy reinforcement learning",
    "authors": [
      "Sunbowen Lee",
      "Yicheng Gong",
      "Chao Deng"
    ],
    "abstract": "Reinforcement learning control algorithms face significant challenges due to\nout-of-distribution and inefficient exploration problems. While model-based\nreinforcement learning enhances the agent's reasoning and planning capabilities\nby constructing virtual environments, training such virtual environments can be\nvery complex. In order to build an efficient inference model and enhance the\nrepresentativeness of learning data, we propose the Counterfactual Experience\nAugmentation (CEA) algorithm. CEA leverages variational autoencoders to model\nthe dynamic patterns of state transitions and introduces randomness to model\nnon-stationarity. This approach focuses on expanding the learning data in the\nexperience pool through counterfactual inference and performs exceptionally\nwell in environments that follow the bisimulation assumption. Environments with\nbisimulation properties are usually represented by discrete observation and\naction spaces, we propose a sampling method based on maximum kernel density\nestimation entropy to extend CEA to various environments. By providing reward\nsignals for counterfactual state transitions based on real information, CEA\nconstructs a complete counterfactual experience to alleviate the\nout-of-distribution problem of the learning data, and outperforms general SOTA\nalgorithms in environments with difference properties. Finally, we discuss the\nsimilarities, differences and properties of generated counterfactual\nexperiences and real experiences. The code is available at\nhttps://github.com/Aegis1863/CEA.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by Neurocomputing,\n  https://doi.org/10.1016/j.neucom.2025.130017",
    "pdf_url": "http://arxiv.org/pdf/2503.13842v1",
    "published_date": "2025-03-18 02:32:50 UTC",
    "updated_date": "2025-03-18 02:32:50 UTC"
  },
  {
    "arxiv_id": "2503.13836v1",
    "title": "SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing",
    "authors": [
      "Seokhyeon Hong",
      "Chaelin Kim",
      "Serin Yoon",
      "Junghyun Nam",
      "Sihun Cha",
      "Junyong Noh"
    ],
    "abstract": "Text-driven motion generation has advanced significantly with the rise of\ndenoising diffusion models. However, previous methods often oversimplify\nrepresentations for the skeletal joints, temporal frames, and textual words,\nlimiting their ability to fully capture the information within each modality\nand their interactions. Moreover, when using pre-trained models for downstream\ntasks, such as editing, they typically require additional efforts, including\nmanual interventions, optimization, or fine-tuning. In this paper, we introduce\na skeleton-aware latent diffusion (SALAD), a model that explicitly captures the\nintricate inter-relationships between joints, frames, and words. Furthermore,\nby leveraging cross-attention maps produced during the generation process, we\nenable attention-based zero-shot text-driven motion editing using a pre-trained\nSALAD model, requiring no additional user input beyond text prompts. Our\napproach significantly outperforms previous methods in terms of text-motion\nalignment without compromising generation quality, and demonstrates practical\nversatility by providing diverse editing capabilities beyond generation. Code\nis available at project page.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025; Project page\n  https://seokhyeonhong.github.io/projects/salad/",
    "pdf_url": "http://arxiv.org/pdf/2503.13836v1",
    "published_date": "2025-03-18 02:20:11 UTC",
    "updated_date": "2025-03-18 02:20:11 UTC"
  },
  {
    "arxiv_id": "2503.14557v1",
    "title": "Generating Causal Explanations of Vehicular Agent Behavioural Interactions with Learnt Reward Profiles",
    "authors": [
      "Rhys Howard",
      "Nick Hawes",
      "Lars Kunze"
    ],
    "abstract": "Transparency and explainability are important features that responsible\nautonomous vehicles should possess, particularly when interacting with humans,\nand causal reasoning offers a strong basis to provide these qualities. However,\neven if one assumes agents act to maximise some concept of reward, it is\ndifficult to make accurate causal inferences of agent planning without\ncapturing what is of importance to the agent. Thus our work aims to learn a\nweighting of reward metrics for agents such that explanations for agent\ninteractions can be causally inferred. We validate our approach quantitatively\nand qualitatively across three real-world driving datasets, demonstrating a\nfunctional improvement over previous methods and competitive performance across\nevaluation metrics.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.RO",
      "I.2.0; I.2.6; I.2.9; I.2.11; I.6.0"
    ],
    "primary_category": "cs.AI",
    "comment": "8 Pages, 5 Figures, To be published in the Proceedings of the 2025\n  IEEE International Conference on Robotics & Automation, Initial upload of\n  accepted paper",
    "pdf_url": "http://arxiv.org/pdf/2503.14557v1",
    "published_date": "2025-03-18 01:53:59 UTC",
    "updated_date": "2025-03-18 01:53:59 UTC"
  },
  {
    "arxiv_id": "2503.13817v1",
    "title": "VARP: Reinforcement Learning from Vision-Language Model Feedback with Agent Regularized Preferences",
    "authors": [
      "Anukriti Singh",
      "Amisha Bhaskar",
      "Peihong Yu",
      "Souradip Chakraborty",
      "Ruthwik Dasyam",
      "Amrit Bedi",
      "Pratap Tokekar"
    ],
    "abstract": "Designing reward functions for continuous-control robotics often leads to\nsubtle misalignments or reward hacking, especially in complex tasks.\nPreference-based RL mitigates some of these pitfalls by learning rewards from\ncomparative feedback rather than hand-crafted signals, yet scaling human\nannotations remains challenging. Recent work uses Vision-Language Models (VLMs)\nto automate preference labeling, but a single final-state image generally fails\nto capture the agent's full motion. In this paper, we present a two-part\nsolution that both improves feedback accuracy and better aligns reward learning\nwith the agent's policy. First, we overlay trajectory sketches on final\nobservations to reveal the path taken, allowing VLMs to provide more reliable\npreferences-improving preference accuracy by approximately 15-20% in metaworld\ntasks. Second, we regularize reward learning by incorporating the agent's\nperformance, ensuring that the reward model is optimized based on data\ngenerated by the current policy; this addition boosts episode returns by 20-30%\nin locomotion tasks. Empirical studies on metaworld demonstrate that our method\nachieves, for instance, around 70-80% success rate in all tasks, compared to\nbelow 50% for standard approaches. These results underscore the efficacy of\ncombining richer visual representations with agent-aware reward regularization.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.13817v1",
    "published_date": "2025-03-18 01:51:27 UTC",
    "updated_date": "2025-03-18 01:51:27 UTC"
  },
  {
    "arxiv_id": "2503.13813v1",
    "title": "Automatic MILP Model Construction for Multi-Robot Task Allocation and Scheduling Based on Large Language Models",
    "authors": [
      "Mingming Peng",
      "Zhendong Chen",
      "Jie Yang",
      "Jin Huang",
      "Zhengqi Shi",
      "Qihao Liu",
      "Xinyu Li",
      "Liang Gao"
    ],
    "abstract": "With the accelerated development of Industry 4.0, intelligent manufacturing\nsystems increasingly require efficient task allocation and scheduling in\nmulti-robot systems. However, existing methods rely on domain expertise and\nface challenges in adapting to dynamic production constraints. Additionally,\nenterprises have high privacy requirements for production scheduling data,\nwhich prevents the use of cloud-based large language models (LLMs) for solution\ndevelopment. To address these challenges, there is an urgent need for an\nautomated modeling solution that meets data privacy requirements. This study\nproposes a knowledge-augmented mixed integer linear programming (MILP)\nautomated formulation framework, integrating local LLMs with domain-specific\nknowledge bases to generate executable code from natural language descriptions\nautomatically. The framework employs a knowledge-guided\nDeepSeek-R1-Distill-Qwen-32B model to extract complex spatiotemporal\nconstraints (82% average accuracy) and leverages a supervised fine-tuned\nQwen2.5-Coder-7B-Instruct model for efficient MILP code generation (90% average\naccuracy). Experimental results demonstrate that the framework successfully\nachieves automatic modeling in the aircraft skin manufacturing case while\nensuring data privacy and computational efficiency. This research provides a\nlow-barrier and highly reliable technical path for modeling in complex\nindustrial scenarios.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13813v1",
    "published_date": "2025-03-18 01:45:19 UTC",
    "updated_date": "2025-03-18 01:45:19 UTC"
  },
  {
    "arxiv_id": "2503.13812v1",
    "title": "The Empty Chair: Using LLMs to Raise Missing Perspectives in Policy Deliberations",
    "authors": [
      "Suyash Fulay",
      "Deb Roy"
    ],
    "abstract": "Deliberation is essential to well-functioning democracies, yet physical,\neconomic, and social barriers often exclude certain groups, reducing\nrepresentativeness and contributing to issues like group polarization. In this\nwork, we explore the use of large language model (LLM) personas to introduce\nmissing perspectives in policy deliberations. We develop and evaluate a tool\nthat transcribes conversations in real-time and simulates input from relevant\nbut absent stakeholders. We deploy this tool in a 19-person student citizens'\nassembly on campus sustainability. Participants and facilitators found that the\ntool sparked new discussions and surfaced valuable perspectives they had not\npreviously considered. However, they also noted that AI-generated responses\nwere sometimes overly general. They raised concerns about overreliance on AI\nfor perspective-taking. Our findings highlight both the promise and potential\nrisks of using LLMs to raise missing points of view in group deliberation\nsettings.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13812v1",
    "published_date": "2025-03-18 01:45:08 UTC",
    "updated_date": "2025-03-18 01:45:08 UTC"
  },
  {
    "arxiv_id": "2503.13806v1",
    "title": "Organ-aware Multi-scale Medical Image Segmentation Using Text Prompt Engineering",
    "authors": [
      "Wenjie Zhang",
      "Ziyang Zhang",
      "Mengnan He",
      "Jiancheng Ye"
    ],
    "abstract": "Accurate segmentation is essential for effective treatment planning and\ndisease monitoring. Existing medical image segmentation methods predominantly\nrely on uni-modal visual inputs, such as images or videos, requiring\nlabor-intensive manual annotations. Additionally, medical imaging techniques\ncapture multiple intertwined organs within a single scan, further complicating\nsegmentation accuracy. To address these challenges, MedSAM, a large-scale\nmedical segmentation model based on the Segment Anything Model (SAM), was\ndeveloped to enhance segmentation accuracy by integrating image features with\nuser-provided prompts. While MedSAM has demonstrated strong performance across\nvarious medical segmentation tasks, it primarily relies on geometric prompts\n(e.g., points and bounding boxes) and lacks support for text-based prompts,\nwhich could help specify subtle or ambiguous anatomical structures. To overcome\nthese limitations, we propose the Organ-aware Multi-scale Text-guided Medical\nImage Segmentation Model (OMT-SAM) for multi-organ segmentation. Our approach\nintroduces CLIP encoders as a novel image-text prompt encoder, operating with\nthe geometric prompt encoder to provide informative contextual guidance. We\npair descriptive textual prompts with corresponding images, processing them\nthrough pre-trained CLIP encoders and a cross-attention mechanism to generate\nfused image-text embeddings. Additionally, we extract multi-scale visual\nfeatures from MedSAM, capturing fine-grained anatomical details at different\nlevels of granularity. We evaluate OMT-SAM on the FLARE 2021 dataset,\nbenchmarking its performance against existing segmentation methods. Empirical\nresults demonstrate that OMT-SAM achieves a mean Dice Similarity Coefficient of\n0.937, outperforming MedSAM (0.893) and other segmentation models, highlighting\nits superior capability in handling complex medical image segmentation tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13806v1",
    "published_date": "2025-03-18 01:35:34 UTC",
    "updated_date": "2025-03-18 01:35:34 UTC"
  },
  {
    "arxiv_id": "2503.13804v1",
    "title": "Empowering GraphRAG with Knowledge Filtering and Integration",
    "authors": [
      "Kai Guo",
      "Harry Shomer",
      "Shenglai Zeng",
      "Haoyu Han",
      "Yu Wang",
      "Jiliang Tang"
    ],
    "abstract": "In recent years, large language models (LLMs) have revolutionized the field\nof natural language processing. However, they often suffer from knowledge gaps\nand hallucinations. Graph retrieval-augmented generation (GraphRAG) enhances\nLLM reasoning by integrating structured knowledge from external graphs.\nHowever, we identify two key challenges that plague GraphRAG:(1) Retrieving\nnoisy and irrelevant information can degrade performance and (2)Excessive\nreliance on external knowledge suppresses the model's intrinsic reasoning. To\naddress these issues, we propose GraphRAG-FI (Filtering and Integration),\nconsisting of GraphRAG-Filtering and GraphRAG-Integration. GraphRAG-Filtering\nemploys a two-stage filtering mechanism to refine retrieved information.\nGraphRAG-Integration employs a logits-based selection strategy to balance\nexternal knowledge from GraphRAG with the LLM's intrinsic reasoning,reducing\nover-reliance on retrievals. Experiments on knowledge graph QA tasks\ndemonstrate that GraphRAG-FI significantly improves reasoning performance\nacross multiple backbone models, establishing a more reliable and effective\nGraphRAG framework.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13804v1",
    "published_date": "2025-03-18 01:29:55 UTC",
    "updated_date": "2025-03-18 01:29:55 UTC"
  },
  {
    "arxiv_id": "2503.13799v1",
    "title": "SMILE: a Scale-aware Multiple Instance Learning Method for Multicenter STAS Lung Cancer Histopathology Diagnosis",
    "authors": [
      "Liangrui Pan",
      "Xiaoyu Li",
      "Yutao Dou",
      "Qiya Song",
      "Jiadi Luo",
      "Qingchun Liang",
      "Shaoliang Peng"
    ],
    "abstract": "Spread through air spaces (STAS) represents a newly identified aggressive\npattern in lung cancer, which is known to be associated with adverse prognostic\nfactors and complex pathological features. Pathologists currently rely on time\nconsuming manual assessments, which are highly subjective and prone to\nvariation. This highlights the urgent need for automated and precise diag\nnostic solutions. 2,970 lung cancer tissue slides are comprised from multiple\ncenters, re-diagnosed them, and constructed and publicly released three lung\ncancer STAS datasets: STAS CSU (hospital), STAS TCGA, and STAS CPTAC. All STAS\ndatasets provide corresponding pathological feature diagnoses and related\nclinical data. To address the bias, sparse and heterogeneous nature of STAS, we\npropose an scale-aware multiple instance learning(SMILE) method for STAS\ndiagnosis of lung cancer. By introducing a scale-adaptive attention mechanism,\nthe SMILE can adaptively adjust high attention instances, reducing\nover-reliance on local regions and promoting consistent detection of STAS\nlesions. Extensive experiments show that SMILE achieved competitive diagnostic\nresults on STAS CSU, diagnosing 251 and 319 STAS samples in CPTAC\nandTCGA,respectively, surpassing clinical average AUC. The 11 open baseline\nresults are the first to be established for STAS research, laying the\nfoundation for the future expansion, interpretability, and clinical integration\nof computational pathology technologies. The datasets and code are available at\nhttps://anonymous.4open.science/r/IJCAI25-1DA1.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13799v1",
    "published_date": "2025-03-18 01:09:52 UTC",
    "updated_date": "2025-03-18 01:09:52 UTC"
  },
  {
    "arxiv_id": "2503.13798v1",
    "title": "AI-Powered Prediction of Nanoparticle Pharmacokinetics: A Multi-View Learning Approach",
    "authors": [
      "Amirhossein Khakpour",
      "Lucia Florescu",
      "Richard Tilley",
      "Haibo Jiang",
      "K. Swaminathan Iyer",
      "Gustavo Carneiro"
    ],
    "abstract": "The clinical translation of nanoparticle-based treatments remains limited due\nto the unpredictability of (nanoparticle) NP\npharmacokinetics$\\unicode{x2014}$how they distribute, accumulate, and clear\nfrom the body. Predicting these behaviours is challenging due to complex\nbiological interactions and the difficulty of obtaining high-quality\nexperimental datasets. Existing AI-driven approaches rely heavily on\ndata-driven learning but fail to integrate crucial knowledge about NP\nproperties and biodistribution mechanisms. We introduce a multi-view deep\nlearning framework that enhances pharmacokinetic predictions by incorporating\nprior knowledge of key NP properties such as size and charge into a\ncross-attention mechanism, enabling context-aware feature selection and\nimproving generalization despite small datasets. To further enhance prediction\nrobustness, we employ an ensemble learning approach, combining deep learning\nwith XGBoost (XGB) and Random Forest (RF), which significantly outperforms\nexisting AI models. Our interpretability analysis reveals key physicochemical\nproperties driving NP biodistribution, providing biologically meaningful\ninsights into possible mechanisms governing NP behaviour in vivo rather than a\nblack-box model. Furthermore, by bridging machine learning with physiologically\nbased pharmacokinetic (PBPK) modelling, this work lays the foundation for\ndata-efficient AI-driven drug discovery and precision nanomedicine.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13798v1",
    "published_date": "2025-03-18 01:09:32 UTC",
    "updated_date": "2025-03-18 01:09:32 UTC"
  },
  {
    "arxiv_id": "2503.13794v1",
    "title": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation",
    "authors": [
      "Yang Zhou",
      "Shiyu Zhao",
      "Yuxiao Chen",
      "Zhenting Wang",
      "Dimitris N. Metaxas"
    ],
    "abstract": "Large foundation models trained on large-scale visual-text data can\nsignificantly enhance Open Vocabulary Object Detection (OVD) through data\ngeneration. However, this may lead to biased synthetic data and overfitting to\nspecific configurations. It can sidestep biases of manually curated data\ngeneration by directly leveraging hidden states of Large Language Models\n(LLMs), which is surprisingly rarely explored. This paper presents a systematic\nmethod to enhance visual grounding by utilizing decoder layers of the LLM of a\nMLLM. We introduce a zero-initialized cross-attention adapter to enable\nefficient knowledge transfer from LLMs to object detectors, an new approach\ncalled LED (LLM Enhanced Open-Vocabulary Object Detection). We demonstrate that\nintermediate hidden states from early LLM layers retain strong spatial-semantic\ncorrelations that are beneficial to grounding tasks. Experiments show that our\nadaptation strategy significantly enhances the performance on complex free-form\ntext queries while remaining the same on plain categories. With our adaptation,\nQwen2-0.5B with Swin-T as the vision encoder improves GroundingDINO by 2.33% on\nOmnilabel, at the overhead of 8.7% more GFLOPs. Qwen2-0.5B with a larger vision\nencoder can further boost the performance by 6.22%. We further validate our\ndesign by ablating on varied adapter architectures, sizes of LLMs, and which\nlayers to add adaptation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13794v1",
    "published_date": "2025-03-18 00:50:40 UTC",
    "updated_date": "2025-03-18 00:50:40 UTC"
  },
  {
    "arxiv_id": "2503.13793v1",
    "title": "Mapping the Trust Terrain: LLMs in Software Engineering -- Insights and Perspectives",
    "authors": [
      "Dipin Khati",
      "Yijin Liu",
      "David N. Palacio",
      "Yixuan Zhang",
      "Denys Poshyvanyk"
    ],
    "abstract": "Applications of Large Language Models (LLMs) are rapidly growing in industry\nand academia for various software engineering (SE) tasks. As these models\nbecome more integral to critical processes, ensuring their reliability and\ntrustworthiness becomes essential. Consequently, the concept of trust in these\nsystems is becoming increasingly critical. Well-calibrated trust is important,\nas excessive trust can lead to security vulnerabilities, and risks, while\ninsufficient trust can hinder innovation. However, the landscape of\ntrust-related concepts in LLMs in SE is relatively unclear, with concepts such\nas trust, distrust, and trustworthiness lacking clear conceptualizations in the\nSE community. To bring clarity to the current research status and identify\nopportunities for future work, we conducted a comprehensive review of $88$\npapers: a systematic literature review of $18$ papers focused on LLMs in SE,\ncomplemented by an analysis of 70 papers from broader trust literature.\nAdditionally, we conducted a survey study with 25 domain experts to gain\ninsights into practitioners' understanding of trust and identify gaps between\nexisting literature and developers' perceptions. The result of our analysis\nserves as a roadmap that covers trust-related concepts in LLMs in SE and\nhighlights areas for future exploration.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13793v1",
    "published_date": "2025-03-18 00:49:43 UTC",
    "updated_date": "2025-03-18 00:49:43 UTC"
  },
  {
    "arxiv_id": "2503.14556v1",
    "title": "Designing and Deploying AI Models for Sustainable Logistics Optimization: A Case Study on Eco-Efficient Supply Chains in the USA",
    "authors": [
      "Reza E Rabbi Shawon",
      "MD Rokibul Hasan",
      "Md Anisur Rahman",
      "Mohamed Ghandri",
      "Iman Ahmed Lamari",
      "Mohammed Kawsar",
      "Rubi Akter"
    ],
    "abstract": "The rapid evolution of Artificial Intelligence (AI) and Machine Learning (ML)\nhas significantly transformed logistics and supply chain management,\nparticularly in the pursuit of sustainability and eco-efficiency. This study\nexplores AI-based methodologies for optimizing logistics operations in the USA,\nfocusing on reducing environmental impact, improving fuel efficiency, and\nminimizing costs. Key AI applications include predictive analytics for demand\nforecasting, route optimization through machine learning, and AI-powered fuel\nefficiency strategies. Various models, such as Linear Regression, XGBoost,\nSupport Vector Machine, and Neural Networks, are applied to real-world\nlogistics datasets to reduce carbon emissions based on logistics operations,\noptimize travel routes to minimize distance and travel time, and predict future\ndeliveries to plan optimal routes. Other models such as K-Means and DBSCAN are\nalso used to optimize travel routes to minimize distance and travel time for\nlogistics operations. This study utilizes datasets from logistics companies'\ndatabases. The study also assesses model performance using metrics such as mean\nabsolute error (MAE), mean squared error (MSE), and R2 score. This study also\nexplores how these models can be deployed to various platforms for real-time\nlogistics and supply chain use. The models are also examined through a thorough\ncase study, highlighting best practices and regulatory frameworks that promote\nsustainability. The findings demonstrate AI's potential to enhance logistics\nefficiency, reduce carbon footprints, and contribute to a more resilient and\nadaptive supply chain ecosystem.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.14556v1",
    "published_date": "2025-03-18 00:46:35 UTC",
    "updated_date": "2025-03-18 00:46:35 UTC"
  },
  {
    "arxiv_id": "2503.13786v1",
    "title": "Evaluating the Application of SOLID Principles in Modern AI Framework Architectures",
    "authors": [
      "Jonesh Shrestha"
    ],
    "abstract": "This research evaluates the extent to which modern AI frameworks,\nspecifically TensorFlow and scikit-learn, adhere to the SOLID design principles\n- Single Responsibility, Open/Closed, Liskov Substitution, Interface\nSegregation, and Dependency Inversion. Analyzing the frameworks architectural\ndocumentation and design philosophies, this research investigates architectural\ntrade-offs when balancing software engineering best practices with AI-specific\nneeds. I examined each frameworks documentation, source code, and architectural\ncomponents to evaluate their adherence to these principles. The results show\nthat both frameworks adopt certain aspects of SOLID design principles but make\nintentional trade-offs to address performance, scalability, and the\nexperimental nature of AI development. TensorFlow focuses on performance and\nscalability, sometimes sacrificing strict adherence to principles like Single\nResponsibility and Interface Segregation. While scikit-learns design philosophy\naligns more closely with SOLID principles through consistent interfaces and\ncomposition principles, sticking closer to SOLID guidelines but with occasional\ndeviations for performance optimizations and scalability. This research\ndiscovered that applying SOLID principles in AI frameworks depends on context,\nas performance, scalability, and flexibility often require deviations from\ntraditional software engineering principles. This research contributes to\nunderstanding how domain-specific constraints influence architectural decisions\nin modern AI frameworks and how these frameworks strategically adapted design\nchoices to effectively balance these contradicting requirements.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "68N19, 68T01",
      "D.2.11; I.2.0"
    ],
    "primary_category": "cs.SE",
    "comment": "5 pages, 1 figure, 12 references",
    "pdf_url": "http://arxiv.org/pdf/2503.13786v1",
    "published_date": "2025-03-18 00:37:23 UTC",
    "updated_date": "2025-03-18 00:37:23 UTC"
  }
]
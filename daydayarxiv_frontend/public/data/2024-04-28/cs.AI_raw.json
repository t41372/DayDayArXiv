[
  {
    "arxiv_id": "2404.18328v1",
    "title": "Multi-stage Attack Detection and Prediction Using Graph Neural Networks: An IoT Feasibility Study",
    "authors": [
      "Hamdi Friji",
      "Ioannis Mavromatis",
      "Adrian Sanchez-Mompo",
      "Pietro Carnelli",
      "Alexis Olivereau",
      "Aftab Khan"
    ],
    "abstract": "With the ever-increasing reliance on digital networks for various aspects of\nmodern life, ensuring their security has become a critical challenge. Intrusion\nDetection Systems play a crucial role in ensuring network security, actively\nidentifying and mitigating malicious behaviours. However, the relentless\nadvancement of cyber-threats has rendered traditional/classical approaches\ninsufficient in addressing the sophistication and complexity of attacks. This\npaper proposes a novel 3-stage intrusion detection system inspired by a\nsimplified version of the Lockheed Martin cyber kill chain to detect advanced\nmulti-step attacks. The proposed approach consists of three models, each\nresponsible for detecting a group of attacks with common characteristics. The\ndetection outcome of the first two stages is used to conduct a feasibility\nstudy on the possibility of predicting attacks in the third stage. Using the\nToN IoT dataset, we achieved an average of 94% F1-Score among different stages,\noutperforming the benchmark approaches based on Random-forest model. Finally,\nwe comment on the feasibility of this approach to be integrated in a real-world\nsystem and propose various possible future work.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.18328v1",
    "published_date": "2024-04-28 22:11:24 UTC",
    "updated_date": "2024-04-28 22:11:24 UTC"
  },
  {
    "arxiv_id": "2404.18326v1",
    "title": "SAFE-RL: Saliency-Aware Counterfactual Explainer for Deep Reinforcement Learning Policies",
    "authors": [
      "Amir Samadi",
      "Konstantinos Koufos",
      "Kurt Debattista",
      "Mehrdad Dianati"
    ],
    "abstract": "While Deep Reinforcement Learning (DRL) has emerged as a promising solution\nfor intricate control tasks, the lack of explainability of the learned policies\nimpedes its uptake in safety-critical applications, such as automated driving\nsystems (ADS). Counterfactual (CF) explanations have recently gained prominence\nfor their ability to interpret black-box Deep Learning (DL) models. CF examples\nare associated with minimal changes in the input, resulting in a complementary\noutput by the DL model. Finding such alternations, particularly for\nhigh-dimensional visual inputs, poses significant challenges. Besides, the\ntemporal dependency introduced by the reliance of the DRL agent action on a\nhistory of past state observations further complicates the generation of CF\nexamples. To address these challenges, we propose using a saliency map to\nidentify the most influential input pixels across the sequence of past observed\nstates by the agent. Then, we feed this map to a deep generative model,\nenabling the generation of plausible CFs with constrained modifications centred\non the salient regions. We evaluate the effectiveness of our framework in\ndiverse domains, including ADS, Atari Pong, Pacman and space-invaders games,\nusing traditional performance metrics such as validity, proximity and sparsity.\nExperimental results demonstrate that this framework generates more informative\nand plausible CFs than the state-of-the-art for a wide range of environments\nand DRL agents. In order to foster research in this area, we have made our\ndatasets and codes publicly available at\nhttps://github.com/Amir-Samadi/SAFE-RL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.18326v1",
    "published_date": "2024-04-28 21:47:34 UTC",
    "updated_date": "2024-04-28 21:47:34 UTC"
  },
  {
    "arxiv_id": "2404.18316v3",
    "title": "Position: Do Not Explain Vision Models Without Context",
    "authors": [
      "Paulina Tomaszewska",
      "Przemys≈Çaw Biecek"
    ],
    "abstract": "Does the stethoscope in the picture make the adjacent person a doctor or a\npatient? This, of course, depends on the contextual relationship of the two\nobjects. If it's obvious, why don't explanation methods for vision models use\ncontextual information? In this paper, we (1) review the most popular methods\nof explaining computer vision models by pointing out that they do not take into\naccount context information, (2) show examples of failures of popular XAI\nmethods, (3) provide examples of real-world use cases where spatial context\nplays a significant role, (4) propose new research directions that may lead to\nbetter use of context information in explaining computer vision models, (5)\nargue that a change in approach to explanations is needed from 'where' to\n'how'.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at International Conference on Machine Learning (ICML) 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.18316v3",
    "published_date": "2024-04-28 20:57:55 UTC",
    "updated_date": "2024-06-02 20:57:53 UTC"
  },
  {
    "arxiv_id": "2404.18311v4",
    "title": "Towards Incremental Learning in Large Language Models: A Critical Review",
    "authors": [
      "Mladjan Jovanovic",
      "Peter Voss"
    ],
    "abstract": "Incremental learning is the ability of systems to acquire knowledge over\ntime, enabling their adaptation and generalization to novel tasks. It is a\ncritical ability for intelligent, real-world systems, especially when data\nchanges frequently or is limited. This review provides a comprehensive analysis\nof incremental learning in Large Language Models. It synthesizes the\nstate-of-the-art incremental learning paradigms, including continual learning,\nmeta-learning, parameter-efficient learning, and mixture-of-experts learning.\nWe demonstrate their utility for incremental learning by describing specific\nachievements from these related topics and their critical factors. An important\nfinding is that many of these approaches do not update the core model, and none\nof them update incrementally in real-time. The paper highlights current\nproblems and challenges for future research in the field. By consolidating the\nlatest relevant research developments, this review offers a comprehensive\nunderstanding of incremental learning and its implications for designing and\ndeveloping LLM-based learning systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.18311v4",
    "published_date": "2024-04-28 20:44:53 UTC",
    "updated_date": "2024-05-05 08:46:32 UTC"
  },
  {
    "arxiv_id": "2404.18304v2",
    "title": "Retrieval-Oriented Knowledge for Click-Through Rate Prediction",
    "authors": [
      "Huanshuo Liu",
      "Bo Chen",
      "Menghui Zhu",
      "Jianghao Lin",
      "Jiarui Qin",
      "Yang Yang",
      "Hao Zhang",
      "Ruiming Tang"
    ],
    "abstract": "Click-through rate (CTR) prediction is crucial for personalized online\nservices. Sample-level retrieval-based models, such as RIM, have demonstrated\nremarkable performance. However, they face challenges including inference\ninefficiency and high resource consumption due to the retrieval process, which\nhinder their practical application in industrial settings. To address this, we\npropose a universal plug-and-play \\underline{r}etrieval-\\underline{o}riented\n\\underline{k}nowledge (\\textbf{\\name}) framework that bypasses the real\nretrieval process. The framework features a knowledge base that preserves and\nimitates the retrieved \\& aggregated representations using a\ndecomposition-reconstruction paradigm. Knowledge distillation and contrastive\nlearning optimize the knowledge base, enabling the integration of\nretrieval-enhanced representations with various CTR models. Experiments on\nthree large-scale datasets demonstrate \\name's exceptional compatibility and\nperformance, with the neural knowledge base serving as an effective surrogate\nfor the retrieval pool. \\name surpasses the teacher model while maintaining\nsuperior inference efficiency and demonstrates the feasibility of distilling\nknowledge from non-parametric methods using a parametric approach. These\nresults highlight \\name's strong potential for real-world applications and its\nability to transform retrieval-based methods into practical solutions. Our\nimplementation code is available to support reproducibility in\n\\url{https://github.com/HSLiu-Initial/ROK.git}.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "11 pages, 6 figures, 6 tables.Accepted by CIKM'24",
    "pdf_url": "http://arxiv.org/pdf/2404.18304v2",
    "published_date": "2024-04-28 20:21:03 UTC",
    "updated_date": "2024-10-03 20:14:47 UTC"
  },
  {
    "arxiv_id": "2404.18296v1",
    "title": "Using Deep Q-Learning to Dynamically Toggle between Push/Pull Actions in Computational Trust Mechanisms",
    "authors": [
      "Zoi Lygizou",
      "Dimitris Kalles"
    ],
    "abstract": "Recent work on decentralized computational trust models for open Multi Agent\nSystems has resulted in the development of CA, a biologically inspired model\nwhich focuses on the trustee's perspective. This new model addresses a serious\nunresolved problem in existing trust and reputation models, namely the\ninability to handle constantly changing behaviors and agents' continuous entry\nand exit from the system. In previous work, we compared CA to FIRE, a\nwell-known trust and reputation model, and found that CA is superior when the\ntrustor population changes, whereas FIRE is more resilient to the trustee\npopulation changes. Thus, in this paper, we investigate how the trustors can\ndetect the presence of several dynamic factors in their environment and then\ndecide which trust model to employ in order to maximize utility. We frame this\nproblem as a machine learning problem in a partially observable environment,\nwhere the presence of several dynamic factors is not known to the trustor and\nwe describe how an adaptable trustor can rely on a few measurable features so\nas to assess the current state of the environment and then use Deep Q Learning\n(DQN), in a single-agent Reinforcement Learning setting, to learn how to adapt\nto a changing environment. We ran a series of simulation experiments to compare\nthe performance of the adaptable trustor with the performance of trustors using\nonly one model (FIRE or CA) and we show that an adaptable agent is indeed\ncapable of learning when to use each model and, thus, perform consistently in\ndynamic environments.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.18296v1",
    "published_date": "2024-04-28 19:44:56 UTC",
    "updated_date": "2024-04-28 19:44:56 UTC"
  },
  {
    "arxiv_id": "2404.18291v1",
    "title": "Panoptic Segmentation and Labelling of Lumbar Spine Vertebrae using Modified Attention Unet",
    "authors": [
      "Rikathi Pal",
      "Priya Saha",
      "Somoballi Ghoshal",
      "Amlan Chakrabarti",
      "Susmita Sur-Kolay"
    ],
    "abstract": "Segmentation and labeling of vertebrae in MRI images of the spine are\ncritical for the diagnosis of illnesses and abnormalities. These steps are\nindispensable as MRI technology provides detailed information about the tissue\nstructure of the spine. Both supervised and unsupervised segmentation methods\nexist, yet acquiring sufficient data remains challenging for achieving high\naccuracy. In this study, we propose an enhancing approach based on modified\nattention U-Net architecture for panoptic segmentation of 3D sliced MRI data of\nthe lumbar spine. Our method achieves an impressive accuracy of 99.5\\% by\nincorporating novel masking logic, thus significantly advancing the\nstate-of-the-art in vertebral segmentation and labeling. This contributes to\nmore precise and reliable diagnosis and treatment planning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.18291v1",
    "published_date": "2024-04-28 19:35:00 UTC",
    "updated_date": "2024-04-28 19:35:00 UTC"
  },
  {
    "arxiv_id": "2405.01587v1",
    "title": "Improve Academic Query Resolution through BERT-based Question Extraction from Images",
    "authors": [
      "Nidhi Kamal",
      "Saurabh Yadav",
      "Jorawar Singh",
      "Aditi Avasthi"
    ],
    "abstract": "Providing fast and accurate resolution to the student's query is an essential\nsolution provided by Edtech organizations. This is generally provided with a\nchat-bot like interface to enable students to ask their doubts easily. One\npreferred format for student queries is images, as it allows students to\ncapture and post questions without typing complex equations and information.\nHowever, this format also presents difficulties, as images may contain multiple\nquestions or textual noise that lowers the accuracy of existing single-query\nanswering solutions. In this paper, we propose a method for extracting\nquestions from text or images using a BERT-based deep learning model and\ncompare it to the other rule-based and layout-based methods. Our method aims to\nimprove the accuracy and efficiency of student query resolution in Edtech\norganizations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01587v1",
    "published_date": "2024-04-28 19:11:08 UTC",
    "updated_date": "2024-04-28 19:11:08 UTC"
  },
  {
    "arxiv_id": "2404.18276v1",
    "title": "Bias Neutralization Framework: Measuring Fairness in Large Language Models with Bias Intelligence Quotient (BiQ)",
    "authors": [
      "Malur Narayan",
      "John Pasmore",
      "Elton Sampaio",
      "Vijay Raghavan",
      "Gabriella Waters"
    ],
    "abstract": "The burgeoning influence of Large Language Models (LLMs) in shaping public\ndiscourse and decision-making underscores the imperative to address inherent\nbiases within these AI systems. In the wake of AI's expansive integration\nacross sectors, addressing racial bias in LLMs has never been more critical.\nThis paper introduces a novel framework called Comprehensive Bias\nNeutralization Framework (CBNF) which embodies an innovative approach to\nquantifying and mitigating biases within LLMs. Our framework combines the Large\nLanguage Model Bias Index (LLMBI) [Oketunji, A., Anas, M., Saina, D., (2023)]\nand Bias removaL with No Demographics (BLIND) [Orgad, H., Belinkov, Y. (2023)]\nmethodologies to create a new metric called Bias Intelligence Quotient\n(BiQ)which detects, measures, and mitigates racial bias in LLMs without\nreliance on demographic annotations.\n  By introducing a new metric called BiQ that enhances LLMBI with additional\nfairness metrics, CBNF offers a multi-dimensional metric for bias assessment,\nunderscoring the necessity of a nuanced approach to fairness in AI [Mehrabi et\nal., 2021]. This paper presents a detailed analysis of Latimer AI (a language\nmodel incrementally trained on black history and culture) in comparison to\nChatGPT 3.5, illustrating Latimer AI's efficacy in detecting racial, cultural,\nand gender biases through targeted training and refined bias mitigation\nstrategies [Latimer & Bender, 2023].",
    "categories": [
      "cs.CL",
      "cs.AI",
      "D.1; I.2"
    ],
    "primary_category": "cs.CL",
    "comment": "41 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.18276v1",
    "published_date": "2024-04-28 18:47:14 UTC",
    "updated_date": "2024-04-28 18:47:14 UTC"
  },
  {
    "arxiv_id": "2404.18270v1",
    "title": "Pragmatic Formal Verification of Sequential Error Detection and Correction Codes (ECCs) used in Safety-Critical Design",
    "authors": [
      "Aman Kumar"
    ],
    "abstract": "Error Detection and Correction Codes (ECCs) are often used in digital designs\nto protect data integrity. Especially in safety-critical systems such as\nautomotive electronics, ECCs are widely used and the verification of such\ncomplex logic becomes more critical considering the ISO 26262 safety standards.\nExhaustive verification of ECC using formal methods has been a challenge given\nthe high number of data bits to protect. As an example, for an ECC of 128 data\nbits with a possibility to detect up to four-bit errors, the combination of bit\nerrors is given by 128C1 + 128C2 + 128C3 + 128C4 = 1.1 * 10^7. This vast\nanalysis space often leads to bounded proof results. Moreover, the complexity\nand state-space increase further if the ECC has sequential encoding and\ndecoding stages. To overcome such problems and sign-off the design with\nconfidence within reasonable proof time, we present a pragmatic formal\nverification approach of complex ECC cores with several complexity reduction\ntechniques and know-how that were learnt during the course of verification. We\ndiscuss using the linearity of the syndrome generator as a helper assertion,\nusing the abstract model as glue logic to compare the RTL with the sequential\nversion of the circuit, k-induction-based model checking and using mathematical\nrelations captured as properties to simplify the verification in order to get\nan unbounded proof result within 24 hours of proof runtime.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "Published in DVCon U.S. 2023",
    "pdf_url": "http://arxiv.org/pdf/2404.18270v1",
    "published_date": "2024-04-28 18:31:09 UTC",
    "updated_date": "2024-04-28 18:31:09 UTC"
  },
  {
    "arxiv_id": "2405.15784v1",
    "title": "CLARINET: Augmenting Language Models to Ask Clarification Questions for Retrieval",
    "authors": [
      "Yizhou Chi",
      "Jessy Lin",
      "Kevin Lin",
      "Dan Klein"
    ],
    "abstract": "Users often make ambiguous requests that require clarification. We study the\nproblem of asking clarification questions in an information retrieval setting,\nwhere systems often face ambiguous search queries and it is challenging to turn\nthe uncertainty in the retrieval model into a natural language question. We\npresent CLARINET, a system that asks informative clarification questions by\nchoosing questions whose answers would maximize certainty in the correct\ncandidate. Our approach works by augmenting a large language model (LLM) to\ncondition on a retrieval distribution, finetuning end-to-end to generate the\nquestion that would have maximized the rank of the true candidate at each turn.\nWhen evaluated on a real-world retrieval dataset of users searching for books,\nour system outperforms traditional heuristics such as information gain on\nretrieval success by 17% and vanilla-prompted LLMs by 39% relative.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.15784v1",
    "published_date": "2024-04-28 18:21:31 UTC",
    "updated_date": "2024-04-28 18:21:31 UTC"
  },
  {
    "arxiv_id": "2404.18264v1",
    "title": "Modeling Orthographic Variation Improves NLP Performance for Nigerian Pidgin",
    "authors": [
      "Pin-Jie Lin",
      "Merel Scholman",
      "Muhammed Saeed",
      "Vera Demberg"
    ],
    "abstract": "Nigerian Pidgin is an English-derived contact language and is traditionally\nan oral language, spoken by approximately 100 million people. No orthographic\nstandard has yet been adopted, and thus the few available Pidgin datasets that\nexist are characterised by noise in the form of orthographic variations. This\ncontributes to under-performance of models in critical NLP tasks. The current\nwork is the first to describe various types of orthographic variations commonly\nfound in Nigerian Pidgin texts, and model this orthographic variation. The\nvariations identified in the dataset form the basis of a phonetic-theoretic\nframework for word editing, which is used to generate orthographic variations\nto augment training data. We test the effect of this data augmentation on two\ncritical NLP tasks: machine translation and sentiment analysis. The proposed\nvariation generation framework augments the training data with new orthographic\nvariants which are relevant for the test set but did not occur in the training\nset originally. Our results demonstrate the positive effect of augmenting the\ntraining data with a combination of real texts from other corpora as well as\nsynthesized orthographic variation, resulting in performance improvements of\n2.1 points in sentiment analysis and 1.4 BLEU points in translation to English.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to LREC-COLING 2024 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2404.18264v1",
    "published_date": "2024-04-28 18:07:13 UTC",
    "updated_date": "2024-04-28 18:07:13 UTC"
  },
  {
    "arxiv_id": "2404.18262v1",
    "title": "Generating Situated Reflection Triggers about Alternative Solution Paths: A Case Study of Generative AI for Computer-Supported Collaborative Learning",
    "authors": [
      "Atharva Naik",
      "Jessica Ruhan Yin",
      "Anusha Kamath",
      "Qianou Ma",
      "Sherry Tongshuang Wu",
      "Charles Murray",
      "Christopher Bogart",
      "Majd Sakr",
      "Carolyn P. Rose"
    ],
    "abstract": "An advantage of Large Language Models (LLMs) is their contextualization\ncapability - providing different responses based on student inputs like\nsolution strategy or prior discussion, to potentially better engage students\nthan standard feedback. We present a design and evaluation of a\nproof-of-concept LLM application to offer students dynamic and contextualized\nfeedback. Specifically, we augment an Online Programming Exercise bot for a\ncollege-level Cloud Computing course with ChatGPT, which offers students\ncontextualized reflection triggers during a collaborative query optimization\ntask in database design. We demonstrate that LLMs can be used to generate\nhighly situated reflection triggers that incorporate details of the\ncollaborative discussion happening in context. We discuss in depth the\nexploration of the design space of the triggers and their correspondence with\nthe learning objectives as well as the impact on student learning in a pilot\nstudy with 34 students.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.18262v1",
    "published_date": "2024-04-28 17:56:14 UTC",
    "updated_date": "2024-04-28 17:56:14 UTC"
  },
  {
    "arxiv_id": "2404.18255v5",
    "title": "PatentGPT: A Large Language Model for Intellectual Property",
    "authors": [
      "Zilong Bai",
      "Ruiji Zhang",
      "Linqing Chen",
      "Qijun Cai",
      "Yuan Zhong",
      "Cong Wang",
      "Yan Fang",
      "Jie Fang",
      "Jing Sun",
      "Weikuan Wang",
      "Lizhi Zhou",
      "Haoran Hua",
      "Tian Qiu",
      "Chaochao Wang",
      "Cheng Sun",
      "Jianping Lu",
      "Yixin Wang",
      "Yubin Xia",
      "Meng Hu",
      "Haowen Liu",
      "Peng Xu",
      "Licong Xu",
      "Fu Bian",
      "Xiaolong Gu",
      "Lisha Zhang",
      "Weilei Wang",
      "Changyang Tu"
    ],
    "abstract": "In recent years, large language models(LLMs) have attracted significant\nattention due to their exceptional performance across a multitude of natural\nlanguage process tasks, and have been widely applied in various fields.\nHowever, the application of large language models in the Intellectual Property\n(IP) domain is challenging due to the strong need for specialized knowledge,\nprivacy protection, processing of extremely long text in this field. In this\ntechnical report, we present for the first time a low-cost, standardized\nprocedure for training IP-oriented LLMs, meeting the unique requirements of the\nIP domain. Using this standard process, we have trained the PatentGPT series\nmodels based on open-source pretrained models. By evaluating them on the\nopen-source IP-oriented benchmark MOZIP, our domain-specific LLMs outperforms\nGPT-4, indicating the effectiveness of the proposed training procedure and the\nexpertise of the PatentGPT models in the IP domain. Remarkably, our model\nsurpassed GPT-4 on the 2019 China Patent Agent Qualification Examination,\nscoring 65 and matching human expert levels. Additionally, the PatentGPT model,\nwhich utilizes the SMoE architecture, achieves performance comparable to that\nof GPT-4 in the IP domain and demonstrates a better cost-performance ratio on\nlong-text tasks, potentially serving as an alternative to GPT-4 within the IP\ndomain.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.18255v5",
    "published_date": "2024-04-28 17:36:43 UTC",
    "updated_date": "2024-06-05 03:02:48 UTC"
  },
  {
    "arxiv_id": "2404.18243v2",
    "title": "LEGENT: Open Platform for Embodied Agents",
    "authors": [
      "Zhili Cheng",
      "Zhitong Wang",
      "Jinyi Hu",
      "Shengding Hu",
      "An Liu",
      "Yuge Tu",
      "Pengkai Li",
      "Lei Shi",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Despite advancements in Large Language Models (LLMs) and Large Multimodal\nModels (LMMs), their integration into language-grounded, human-like embodied\nagents remains incomplete, hindering complex real-life task performance in\nphysical environments. Existing integrations often feature limited open\nsourcing, challenging collective progress in this field. We introduce LEGENT,\nan open, scalable platform for developing embodied agents using LLMs and LMMs.\nLEGENT offers a dual approach: a rich, interactive 3D environment with\ncommunicable and actionable agents, paired with a user-friendly interface, and\na sophisticated data generation pipeline utilizing advanced algorithms to\nexploit supervision from simulated worlds at scale. In our experiments, an\nembryonic vision-language-action model trained on LEGENT-generated data\nsurpasses GPT-4V in embodied tasks, showcasing promising generalization\ncapabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024 System Demonstration",
    "pdf_url": "http://arxiv.org/pdf/2404.18243v2",
    "published_date": "2024-04-28 16:50:12 UTC",
    "updated_date": "2024-08-11 17:18:30 UTC"
  },
  {
    "arxiv_id": "2404.18231v2",
    "title": "From Persona to Personalization: A Survey on Role-Playing Language Agents",
    "authors": [
      "Jiangjie Chen",
      "Xintao Wang",
      "Rui Xu",
      "Siyu Yuan",
      "Yikai Zhang",
      "Wei Shi",
      "Jian Xie",
      "Shuang Li",
      "Ruihan Yang",
      "Tinghui Zhu",
      "Aili Chen",
      "Nianqi Li",
      "Lida Chen",
      "Caiyu Hu",
      "Siye Wu",
      "Scott Ren",
      "Ziquan Fu",
      "Yanghua Xiao"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have significantly\nboosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI\nsystems designed to simulate assigned personas. By harnessing multiple advanced\nabilities of LLMs, including in-context learning, instruction following, and\nsocial intelligence, RPLAs achieve a remarkable sense of human likeness and\nvivid role-playing performance. RPLAs can mimic a wide range of personas,\nranging from historical figures and fictional characters to real-life\nindividuals. Consequently, they have catalyzed numerous AI applications, such\nas emotional companions, interactive video games, personalized assistants and\ncopilots, and digital clones. In this paper, we conduct a comprehensive survey\nof this field, illustrating the evolution and recent progress in RPLAs\nintegrating with cutting-edge LLM technologies. We categorize personas into\nthree types: 1) Demographic Persona, which leverages statistical stereotypes;\n2) Character Persona, focused on well-established figures; and 3)\nIndividualized Persona, customized through ongoing user interactions for\npersonalized services. We begin by presenting a comprehensive overview of\ncurrent methodologies for RPLAs, followed by the details for each persona type,\ncovering corresponding data sourcing, agent construction, and evaluation.\nAfterward, we discuss the fundamental risks, existing limitations, and future\nprospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI\napplications, which reflects practical user demands that shape and drive RPLA\nresearch. Through this work, we aim to establish a clear taxonomy of RPLA\nresearch and applications, and facilitate future research in this critical and\never-evolving field, and pave the way for a future where humans and RPLAs\ncoexist in harmony.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to TMLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.18231v2",
    "published_date": "2024-04-28 15:56:41 UTC",
    "updated_date": "2024-10-09 12:11:15 UTC"
  },
  {
    "arxiv_id": "2404.18214v2",
    "title": "Contrastive Learning Method for Sequential Recommendation based on Multi-Intention Disentanglement",
    "authors": [
      "Zeyu Hu",
      "Yuzhi Xiao",
      "Tao Huang",
      "Xuanrong Huo"
    ],
    "abstract": "Sequential recommendation is one of the important branches of recommender\nsystem, aiming to achieve personalized recommended items for the future through\nthe analysis and prediction of users' ordered historical interactive behaviors.\nHowever, along with the growth of the user volume and the increasingly rich\nbehavioral information, how to understand and disentangle the user's\ninteractive multi-intention effectively also poses challenges to behavior\nprediction and sequential recommendation. In light of these challenges, we\npropose a Contrastive Learning sequential recommendation method based on\nMulti-Intention Disentanglement (MIDCL). In our work, intentions are recognized\nas dynamic and diverse, and user behaviors are often driven by current\nmulti-intentions, which means that the model needs to not only mine the most\nrelevant implicit intention for each user, but also impair the influence from\nirrelevant intentions. Therefore, we choose Variational Auto-Encoder (VAE) to\nrealize the disentanglement of users' multi-intentions. We propose two types of\ncontrastive learning paradigms for finding the most relevant user's interactive\nintention, and maximizing the mutual information of positive sample pairs,\nrespectively. Experimental results show that MIDCL not only has significant\nsuperiority over most existing baseline methods, but also brings a more\ninterpretable case to the research about intention-based prediction and\nrecommendation.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.18214v2",
    "published_date": "2024-04-28 15:13:36 UTC",
    "updated_date": "2024-05-08 17:23:11 UTC"
  },
  {
    "arxiv_id": "2404.18213v2",
    "title": "S$^2$Mamba: A Spatial-spectral State Space Model for Hyperspectral Image Classification",
    "authors": [
      "Guanchun Wang",
      "Xiangrong Zhang",
      "Zelin Peng",
      "Tianyang Zhang",
      "Licheng Jiao"
    ],
    "abstract": "Land cover analysis using hyperspectral images (HSI) remains an open problem\ndue to their low spatial resolution and complex spectral information. Recent\nstudies are primarily dedicated to designing Transformer-based architectures\nfor spatial-spectral long-range dependencies modeling, which is computationally\nexpensive with quadratic complexity. Selective structured state space model\n(Mamba), which is efficient for modeling long-range dependencies with linear\ncomplexity, has recently shown promising progress. However, its potential in\nhyperspectral image processing that requires handling numerous spectral bands\nhas not yet been explored. In this paper, we innovatively propose S$^2$Mamba, a\nspatial-spectral state space model for hyperspectral image classification, to\nexcavate spatial-spectral contextual features, resulting in more efficient and\naccurate land cover analysis. In S$^2$Mamba, two selective structured state\nspace models through different dimensions are designed for feature extraction,\none for spatial, and the other for spectral, along with a spatial-spectral\nmixture gate for optimal fusion. More specifically, S$^2$Mamba first captures\nspatial contextual relations by interacting each pixel with its adjacent\nthrough a Patch Cross Scanning module and then explores semantic information\nfrom continuous spectral bands through a Bi-directional Spectral Scanning\nmodule. Considering the distinct expertise of the two attributes in homogenous\nand complicated texture scenes, we realize the Spatial-spectral Mixture Gate by\na group of learnable matrices, allowing for the adaptive incorporation of\nrepresentations learned across different dimensions. Extensive experiments\nconducted on HSI classification benchmarks demonstrate the superiority and\nprospect of S$^2$Mamba. The code will be made available at:\nhttps://github.com/PURE-melo/S2Mamba.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.18213v2",
    "published_date": "2024-04-28 15:12:56 UTC",
    "updated_date": "2024-08-13 10:47:13 UTC"
  },
  {
    "arxiv_id": "2404.18212v3",
    "title": "Paint by Inpaint: Learning to Add Image Objects by Removing Them First",
    "authors": [
      "Navve Wasserman",
      "Noam Rotstein",
      "Roy Ganz",
      "Ron Kimmel"
    ],
    "abstract": "Image editing has advanced significantly with the introduction of\ntext-conditioned diffusion models. Despite this progress, seamlessly adding\nobjects to images based on textual instructions without requiring user-provided\ninput masks remains a challenge. We address this by leveraging the insight that\nremoving objects (Inpaint) is significantly simpler than its inverse process of\nadding them (Paint), attributed to inpainting models that benefit from\nsegmentation mask guidance. Capitalizing on this realization, by implementing\nan automated and extensive pipeline, we curate a filtered large-scale image\ndataset containing pairs of images and their corresponding object-removed\nversions. Using these pairs, we train a diffusion model to inverse the\ninpainting process, effectively adding objects into images. Unlike other\nediting datasets, ours features natural target images instead of synthetic ones\nwhile ensuring source-target consistency by construction. Additionally, we\nutilize a large Vision-Language Model to provide detailed descriptions of the\nremoved objects and a Large Language Model to convert these descriptions into\ndiverse, natural-language instructions. Our quantitative and qualitative\nresults show that the trained model surpasses existing models in both object\naddition and general editing tasks. Visit our project page for the released\ndataset and trained models at https://rotsteinnoam.github.io/Paint-by-Inpaint.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.18212v3",
    "published_date": "2024-04-28 15:07:53 UTC",
    "updated_date": "2025-03-20 06:59:54 UTC"
  },
  {
    "arxiv_id": "2405.01585v1",
    "title": "Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular RAG Applications",
    "authors": [
      "Sujit Khanna",
      "Shishir Subedi"
    ],
    "abstract": "In recent times Large Language Models have exhibited tremendous capabilities,\nespecially in the areas of mathematics, code generation and general-purpose\nreasoning. However for specialized domains especially in applications that\nrequire parsing and analyzing large chunks of numeric or tabular data even\nstate-of-the-art (SOTA) models struggle. In this paper, we introduce a new\napproach to solving domain-specific tabular data analysis tasks by presenting a\nunique RAG workflow that mitigates the scalability issues of existing tabular\nLLM solutions. Specifically, we present Tabular Embedding Model (TEM), a novel\napproach to fine-tune embedding models for tabular Retrieval-Augmentation\nGeneration (RAG) applications. Embedding models form a crucial component in the\nRAG workflow and even current SOTA embedding models struggle as they are\npredominantly trained on textual datasets and thus underperform in scenarios\ninvolving complex tabular data. The evaluation results showcase that our\napproach not only outperforms current SOTA embedding models in this domain but\nalso does so with a notably smaller and more efficient model structure.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.01585v1",
    "published_date": "2024-04-28 14:58:55 UTC",
    "updated_date": "2024-04-28 14:58:55 UTC"
  },
  {
    "arxiv_id": "2404.18203v2",
    "title": "LMM-PCQA: Assisting Point Cloud Quality Assessment with LMM",
    "authors": [
      "Zicheng Zhang",
      "Haoning Wu",
      "Yingjie Zhou",
      "Chunyi Li",
      "Wei Sun",
      "Chaofeng Chen",
      "Xiongkuo Min",
      "Xiaohong Liu",
      "Weisi Lin",
      "Guangtao Zhai"
    ],
    "abstract": "Although large multi-modality models (LMMs) have seen extensive exploration\nand application in various quality assessment studies, their integration into\nPoint Cloud Quality Assessment (PCQA) remains unexplored. Given LMMs'\nexceptional performance and robustness in low-level vision and quality\nassessment tasks, this study aims to investigate the feasibility of imparting\nPCQA knowledge to LMMs through text supervision. To achieve this, we transform\nquality labels into textual descriptions during the fine-tuning phase, enabling\nLMMs to derive quality rating logits from 2D projections of point clouds. To\ncompensate for the loss of perception in the 3D domain, structural features are\nextracted as well. These quality logits and structural features are then\ncombined and regressed into quality scores. Our experimental results affirm the\neffectiveness of our approach, showcasing a novel integration of LMMs into PCQA\nthat enhances model understanding and assessment accuracy. We hope our\ncontributions can inspire subsequent investigations into the fusion of LMMs\nwith PCQA, fostering advancements in 3D visual quality analysis and beyond. The\ncode is available at https://github.com/zzc-1998/LMM-PCQA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.18203v2",
    "published_date": "2024-04-28 14:47:09 UTC",
    "updated_date": "2024-08-06 03:37:31 UTC"
  },
  {
    "arxiv_id": "2405.01509v1",
    "title": "Learnable Linguistic Watermarks for Tracing Model Extraction Attacks on Large Language Models",
    "authors": [
      "Minhao Bai",
      "Kaiyi Pang",
      "Yongfeng Huang"
    ],
    "abstract": "In the rapidly evolving domain of artificial intelligence, safeguarding the\nintellectual property of Large Language Models (LLMs) is increasingly crucial.\nCurrent watermarking techniques against model extraction attacks, which rely on\nsignal insertion in model logits or post-processing of generated text, remain\nlargely heuristic. We propose a novel method for embedding learnable linguistic\nwatermarks in LLMs, aimed at tracing and preventing model extraction attacks.\nOur approach subtly modifies the LLM's output distribution by introducing\ncontrolled noise into token frequency distributions, embedding an statistically\nidentifiable controllable watermark.We leverage statistical hypothesis testing\nand information theory, particularly focusing on Kullback-Leibler Divergence,\nto differentiate between original and modified distributions effectively. Our\nwatermarking method strikes a delicate well balance between robustness and\noutput quality, maintaining low false positive/negative rates and preserving\nthe LLM's original performance.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "not decided",
    "pdf_url": "http://arxiv.org/pdf/2405.01509v1",
    "published_date": "2024-04-28 14:45:53 UTC",
    "updated_date": "2024-04-28 14:45:53 UTC"
  },
  {
    "arxiv_id": "2404.18202v2",
    "title": "WorldGPT: Empowering LLM as Multimodal World Model",
    "authors": [
      "Zhiqi Ge",
      "Hongzhe Huang",
      "Mingze Zhou",
      "Juncheng Li",
      "Guoming Wang",
      "Siliang Tang",
      "Yueting Zhuang"
    ],
    "abstract": "World models are progressively being employed across diverse fields,\nextending from basic environment simulation to complex scenario construction.\nHowever, existing models are mainly trained on domain-specific states and\nactions, and confined to single-modality state representations. In this paper,\nWe introduce WorldGPT, a generalist world model built upon Multimodal Large\nLanguage Model (MLLM). WorldGPT acquires an understanding of world dynamics\nthrough analyzing millions of videos across various domains. To further enhance\nWorldGPT's capability in specialized scenarios and long-term tasks, we have\nintegrated it with a novel cognitive architecture that combines memory\noffloading, knowledge retrieval, and context reflection. As for evaluation, we\nbuild WorldNet, a multimodal state transition prediction benchmark encompassing\nvaried real-life scenarios. Conducting evaluations on WorldNet directly\ndemonstrates WorldGPT's capability to accurately model state transition\npatterns, affirming its effectiveness in understanding and predicting the\ndynamics of complex scenarios. We further explore WorldGPT's emerging potential\nin serving as a world simulator, helping multimodal agents generalize to\nunfamiliar domains through efficiently synthesising multimodal instruction\ninstances which are proved to be as reliable as authentic data for fine-tuning\npurposes. The project is available on\n\\url{https://github.com/DCDmllm/WorldGPT}.",
    "categories": [
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.AI",
    "comment": "update v2",
    "pdf_url": "http://arxiv.org/pdf/2404.18202v2",
    "published_date": "2024-04-28 14:42:02 UTC",
    "updated_date": "2024-09-28 17:00:44 UTC"
  },
  {
    "arxiv_id": "2404.18198v1",
    "title": "Permutation-equivariant quantum convolutional neural networks",
    "authors": [
      "Sreetama Das",
      "Filippo Caruso"
    ],
    "abstract": "The Symmetric group $S_{n}$ manifests itself in large classes of quantum\nsystems as the invariance of certain characteristics of a quantum state with\nrespect to permuting the qubits. The subgroups of $S_{n}$ arise, among many\nother contexts, to describe label symmetry of classical images with respect to\nspatial transformations, e.g. reflection or rotation. Equipped with the\nformalism of geometric quantum machine learning, in this work we propose the\narchitectures of equivariant quantum convolutional neural networks (EQCNNs)\nadherent to $S_{n}$ and its subgroups. We demonstrate that a careful choice of\npixel-to-qubit embedding order can facilitate easy construction of EQCNNs for\nsmall subgroups of $S_{n}$. Our novel EQCNN architecture corresponding to the\nfull permutation group $S_{n}$ is built by applying all possible QCNNs with\nequal probability, which can also be conceptualized as a dropout strategy in\nquantum neural networks. For subgroups of $S_{n}$, our numerical results using\nMNIST datasets show better classification accuracy than non-equivariant QCNNs.\nThe $S_{n}$-equivariant QCNN architecture shows significantly improved training\nand test performance than non-equivariant QCNN for classification of connected\nand non-connected graphs. When trained with sufficiently large number of data,\nthe $S_{n}$-equivariant QCNN shows better average performance compared to\n$S_{n}$-equivariant QNN . These results contribute towards building powerful\nquantum machine learning architectures in permutation-symmetric systems.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "13 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.18198v1",
    "published_date": "2024-04-28 14:34:28 UTC",
    "updated_date": "2024-04-28 14:34:28 UTC"
  },
  {
    "arxiv_id": "2405.15783v1",
    "title": "Multimodality Invariant Learning for Multimedia-Based New Item Recommendation",
    "authors": [
      "Haoyue Bai",
      "Le Wu",
      "Min Hou",
      "Miaomiao Cai",
      "Zhuangzhuang He",
      "Yuyang Zhou",
      "Richang Hong",
      "Meng Wang"
    ],
    "abstract": "Multimedia-based recommendation provides personalized item suggestions by\nlearning the content preferences of users. With the proliferation of digital\ndevices and APPs, a huge number of new items are created rapidly over time. How\nto quickly provide recommendations for new items at the inference time is\nchallenging. What's worse, real-world items exhibit varying degrees of modality\nmissing(e.g., many short videos are uploaded without text descriptions). Though\nmany efforts have been devoted to multimedia-based recommendations, they either\ncould not deal with new multimedia items or assumed the modality completeness\nin the modeling process.\n  In this paper, we highlight the necessity of tackling the modality missing\nissue for new item recommendation. We argue that users' inherent content\npreference is stable and better kept invariant to arbitrary modality missing\nenvironments. Therefore, we approach this problem from a novel perspective of\ninvariant learning. However, how to construct environments from finite user\nbehavior training data to generalize any modality missing is challenging. To\ntackle this issue, we propose a novel Multimodality Invariant Learning\nreCommendation(a.k.a. MILK) framework. Specifically, MILK first designs a\ncross-modality alignment module to keep semantic consistency from pretrained\nmultimedia item features. After that, MILK designs multi-modal heterogeneous\nenvironments with cyclic mixup to augment training data, in order to mimic any\nmodality missing for invariant user preference learning. Extensive experiments\non three real datasets verify the superiority of our proposed framework. The\ncode is available at https://github.com/HaoyueBai98/MILK.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.15783v1",
    "published_date": "2024-04-28 14:29:09 UTC",
    "updated_date": "2024-04-28 14:29:09 UTC"
  },
  {
    "arxiv_id": "2404.18197v1",
    "title": "A General Causal Inference Framework for Cross-Sectional Observational Data",
    "authors": [
      "Yonghe Zhao",
      "Huiyan Sun"
    ],
    "abstract": "Causal inference methods for observational data are highly regarded due to\ntheir wide applicability. While there are already numerous methods available\nfor de-confounding bias, these methods generally assume that covariates consist\nsolely of confounders or make naive assumptions about the covariates. Such\nassumptions face challenges in both theory and practice, particularly when\ndealing with high-dimensional covariates. Relaxing these naive assumptions and\nidentifying the confounding covariates that truly require correction can\neffectively enhance the practical significance of these methods. Therefore,\nthis paper proposes a General Causal Inference (GCI) framework specifically\ndesigned for cross-sectional observational data, which precisely identifies the\nkey confounding covariates and provides corresponding identification algorithm.\nSpecifically, based on progressive derivations of the Markov property on\nDirected Acyclic Graph, we conclude that the key confounding covariates are\nequivalent to the common root ancestors of the treatment and the outcome\nvariable. Building upon this conclusion, the GCI framework is composed of a\nnovel Ancestor Set Identification (ASI) algorithm and de-confounding inference\nmethods. Firstly, the ASI algorithm is theoretically supported by the\nconditional independence properties and causal asymmetry between variables,\nenabling the identification of key confounding covariates. Subsequently, the\nidentified confounding covariates are used in the de-confounding inference\nmethods to obtain unbiased causal effect estimation, which can support informed\ndecision-making. Extensive experiments on synthetic datasets demonstrate that\nthe GCI framework can effectively identify the critical confounding covariates\nand significantly improve the precision, stability, and interpretability of\ncausal inference in observational studies.",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ME",
    "comment": "19 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.18197v1",
    "published_date": "2024-04-28 14:26:27 UTC",
    "updated_date": "2024-04-28 14:26:27 UTC"
  },
  {
    "arxiv_id": "2404.18191v2",
    "title": "Exploring the Robustness of In-Context Learning with Noisy Labels",
    "authors": [
      "Chen Cheng",
      "Xinzhi Yu",
      "Haodong Wen",
      "Jingsong Sun",
      "Guanzhang Yue",
      "Yihao Zhang",
      "Zeming Wei"
    ],
    "abstract": "Recently, the mysterious In-Context Learning (ICL) ability exhibited by\nTransformer architectures, especially in large language models (LLMs), has\nsparked significant research interest. However, the resilience of Transformers'\nin-context learning capabilities in the presence of noisy samples, prevalent in\nboth training corpora and prompt demonstrations, remains underexplored. In this\npaper, inspired by prior research that studies ICL ability using simple\nfunction classes, we take a closer look at this problem by investigating the\nrobustness of Transformers against noisy labels. Specifically, we first conduct\na thorough evaluation and analysis of the robustness of Transformers against\nnoisy labels during in-context learning and show that they exhibit notable\nresilience against diverse types of noise in demonstration labels. Furthermore,\nwe delve deeper into this problem by exploring whether introducing noise into\nthe training set, akin to a form of data augmentation, enhances such robustness\nduring inference, and find that such noise can indeed improve the robustness of\nICL. Overall, our fruitful analysis and findings provide a comprehensive\nunderstanding of the resilience of Transformer models against label noises\nduring ICL and provide valuable insights into the research on Transformers in\nnatural language processing. Our code is available at\nhttps://github.com/InezYu0928/in-context-learning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2024 Workshop on Reliable and Responsible Foundation Models",
    "pdf_url": "http://arxiv.org/pdf/2404.18191v2",
    "published_date": "2024-04-28 14:05:23 UTC",
    "updated_date": "2024-05-01 09:15:16 UTC"
  },
  {
    "arxiv_id": "2404.18185v1",
    "title": "Ranked List Truncation for Large Language Model-based Re-Ranking",
    "authors": [
      "Chuan Meng",
      "Negar Arabzadeh",
      "Arian Askari",
      "Mohammad Aliannejadi",
      "Maarten de Rijke"
    ],
    "abstract": "We study ranked list truncation (RLT) from a novel \"retrieve-then-re-rank\"\nperspective, where we optimize re-ranking by truncating the retrieved list\n(i.e., trim re-ranking candidates). RLT is crucial for re-ranking as it can\nimprove re-ranking efficiency by sending variable-length candidate lists to a\nre-ranker on a per-query basis. It also has the potential to improve re-ranking\neffectiveness. Despite its importance, there is limited research into applying\nRLT methods to this new perspective. To address this research gap, we reproduce\nexisting RLT methods in the context of re-ranking, especially newly emerged\nlarge language model (LLM)-based re-ranking. In particular, we examine to what\nextent established findings on RLT for retrieval are generalizable to the\n\"retrieve-then-re-rank\" setup from three perspectives: (i) assessing RLT\nmethods in the context of LLM-based re-ranking with lexical first-stage\nretrieval, (ii) investigating the impact of different types of first-stage\nretrievers on RLT methods, and (iii) investigating the impact of different\ntypes of re-rankers on RLT methods. We perform experiments on the TREC 2019 and\n2020 deep learning tracks, investigating 8 RLT methods for pipelines involving\n3 retrievers and 2 re-rankers. We reach new insights into RLT methods in the\ncontext of re-ranking.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "H.3.3"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted for publication as a long paper at SIGIR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.18185v1",
    "published_date": "2024-04-28 13:39:33 UTC",
    "updated_date": "2024-04-28 13:39:33 UTC"
  },
  {
    "arxiv_id": "2404.18183v1",
    "title": "Innovative Application of Artificial Intelligence Technology in Bank Credit Risk Management",
    "authors": [
      "Shuochen Bi",
      "Wenqing Bao"
    ],
    "abstract": "With the rapid growth of technology, especially the widespread application of\nartificial intelligence (AI) technology, the risk management level of\ncommercial banks is constantly reaching new heights. In the current wave of\ndigitalization, AI has become a key driving force for the strategic\ntransformation of financial institutions, especially the banking industry. For\ncommercial banks, the stability and safety of asset quality are crucial, which\ndirectly relates to the long-term stable growth of the bank. Among them, credit\nrisk management is particularly core because it involves the flow of a large\namount of funds and the accuracy of credit decisions. Therefore, establishing a\nscientific and effective credit risk decision-making mechanism is of great\nstrategic significance for commercial banks. In this context, the innovative\napplication of AI technology has brought revolutionary changes to bank credit\nrisk management. Through deep learning and big data analysis, AI can accurately\nevaluate the credit status of borrowers, timely identify potential risks, and\nprovide banks with more accurate and comprehensive credit decision support. At\nthe same time, AI can also achieve realtime monitoring and early warning,\nhelping banks intervene before risks occur and reduce losses.",
    "categories": [
      "q-fin.RM",
      "cs.AI"
    ],
    "primary_category": "q-fin.RM",
    "comment": "6 pages, 1 figure, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.18183v1",
    "published_date": "2024-04-28 13:29:35 UTC",
    "updated_date": "2024-04-28 13:29:35 UTC"
  },
  {
    "arxiv_id": "2404.18178v1",
    "title": "Assessing Image Quality Using a Simple Generative Representation",
    "authors": [
      "Simon Raviv",
      "Gal Chechik"
    ],
    "abstract": "Perceptual image quality assessment (IQA) is the task of predicting the\nvisual quality of an image as perceived by a human observer. Current\nstate-of-the-art techniques are based on deep representations trained in\ndiscriminative manner. Such representations may ignore visually important\nfeatures, if they are not predictive of class labels. Recent generative models\nsuccessfully learn low-dimensional representations using auto-encoding and have\nbeen argued to preserve better visual features. Here we leverage existing\nauto-encoders and propose VAE-QA, a simple and efficient method for predicting\nimage quality in the presence of a full-reference. We evaluate our approach on\nfour standard benchmarks and find that it significantly improves generalization\nacross datasets, has fewer trainable parameters, a smaller memory footprint and\nfaster run time.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.18178v1",
    "published_date": "2024-04-28 13:18:47 UTC",
    "updated_date": "2024-04-28 13:18:47 UTC"
  },
  {
    "arxiv_id": "2404.18174v1",
    "title": "Mamba-FETrack: Frame-Event Tracking via State Space Model",
    "authors": [
      "Ju Huang",
      "Shiao Wang",
      "Shuai Wang",
      "Zhe Wu",
      "Xiao Wang",
      "Bo Jiang"
    ],
    "abstract": "RGB-Event based tracking is an emerging research topic, focusing on how to\neffectively integrate heterogeneous multi-modal data (synchronized exposure\nvideo frames and asynchronous pulse Event stream). Existing works typically\nemploy Transformer based networks to handle these modalities and achieve decent\naccuracy through input-level or feature-level fusion on multiple datasets.\nHowever, these trackers require significant memory consumption and\ncomputational complexity due to the use of self-attention mechanism. This paper\nproposes a novel RGB-Event tracking framework, Mamba-FETrack, based on the\nState Space Model (SSM) to achieve high-performance tracking while effectively\nreducing computational costs and realizing more efficient tracking.\nSpecifically, we adopt two modality-specific Mamba backbone networks to extract\nthe features of RGB frames and Event streams. Then, we also propose to boost\nthe interactive learning between the RGB and Event features using the Mamba\nnetwork. The fused features will be fed into the tracking head for target\nobject localization. Extensive experiments on FELT and FE108 datasets fully\nvalidated the efficiency and effectiveness of our proposed tracker.\nSpecifically, our Mamba-based tracker achieves 43.5/55.6 on the SR/PR metric,\nwhile the ViT-S based tracker (OSTrack) obtains 40.0/50.9. The GPU memory cost\nof ours and ViT-S based tracker is 13.98GB and 15.44GB, which decreased about\n$9.5\\%$. The FLOPs and parameters of ours/ViT-S based OSTrack are 59GB/1076GB\nand 7MB/60MB, which decreased about $94.5\\%$ and $88.3\\%$, respectively. We\nhope this work can bring some new insights to the tracking field and greatly\npromote the application of the Mamba architecture in tracking. The source code\nof this work will be released on\n\\url{https://github.com/Event-AHU/Mamba_FETrack}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "In Peer Review",
    "pdf_url": "http://arxiv.org/pdf/2404.18174v1",
    "published_date": "2024-04-28 13:12:49 UTC",
    "updated_date": "2024-04-28 13:12:49 UTC"
  },
  {
    "arxiv_id": "2404.18161v1",
    "title": "IMEX-Reg: Implicit-Explicit Regularization in the Function Space for Continual Learning",
    "authors": [
      "Prashant Bhat",
      "Bharath Renjith",
      "Elahe Arani",
      "Bahram Zonooz"
    ],
    "abstract": "Continual learning (CL) remains one of the long-standing challenges for deep\nneural networks due to catastrophic forgetting of previously acquired\nknowledge. Although rehearsal-based approaches have been fairly successful in\nmitigating catastrophic forgetting, they suffer from overfitting on buffered\nsamples and prior information loss, hindering generalization under low-buffer\nregimes. Inspired by how humans learn using strong inductive biases, we propose\nIMEX-Reg to improve the generalization performance of experience rehearsal in\nCL under low buffer regimes. Specifically, we employ a two-pronged\nimplicit-explicit regularization approach using contrastive representation\nlearning (CRL) and consistency regularization. To further leverage the global\nrelationship between representations learned using CRL, we propose a\nregularization strategy to guide the classifier toward the activation\ncorrelations in the unit hypersphere of the CRL. Our results show that IMEX-Reg\nsignificantly improves generalization performance and outperforms\nrehearsal-based approaches in several CL scenarios. It is also robust to\nnatural and adversarial corruptions with less task-recency bias. Additionally,\nwe provide theoretical insights to support our design decisions further.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in Transactions on Machine Learning Research",
    "pdf_url": "http://arxiv.org/pdf/2404.18161v1",
    "published_date": "2024-04-28 12:25:09 UTC",
    "updated_date": "2024-04-28 12:25:09 UTC"
  },
  {
    "arxiv_id": "2404.18149v1",
    "title": "Compressed Deepfake Video Detection Based on 3D Spatiotemporal Trajectories",
    "authors": [
      "Zongmei Chen",
      "Xin Liao",
      "Xiaoshuai Wu",
      "Yanxiang Chen"
    ],
    "abstract": "The misuse of deepfake technology by malicious actors poses a potential\nthreat to nations, societies, and individuals. However, existing methods for\ndetecting deepfakes primarily focus on uncompressed videos, such as noise\ncharacteristics, local textures, or frequency statistics. When applied to\ncompressed videos, these methods experience a decrease in detection performance\nand are less suitable for real-world scenarios. In this paper, we propose a\ndeepfake video detection method based on 3D spatiotemporal trajectories.\nSpecifically, we utilize a robust 3D model to construct spatiotemporal motion\nfeatures, integrating feature details from both 2D and 3D frames to mitigate\nthe influence of large head rotation angles or insufficient lighting within\nframes. Furthermore, we separate facial expressions from head movements and\ndesign a sequential analysis method based on phase space motion trajectories to\nexplore the feature differences between genuine and fake faces in deepfake\nvideos. We conduct extensive experiments to validate the performance of our\nproposed method on several compressed deepfake benchmarks. The robustness of\nthe well-designed features is verified by calculating the consistent\ndistribution of facial landmarks before and after video compression.Our method\nyields satisfactory results and showcases its potential for practical\napplications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.18149v1",
    "published_date": "2024-04-28 11:48:13 UTC",
    "updated_date": "2024-04-28 11:48:13 UTC"
  },
  {
    "arxiv_id": "2404.18144v1",
    "title": "Generative AI for Visualization: State of the Art and Future Directions",
    "authors": [
      "Yilin Ye",
      "Jianing Hao",
      "Yihan Hou",
      "Zhan Wang",
      "Shishi Xiao",
      "Yuyu Luo",
      "Wei Zeng"
    ],
    "abstract": "Generative AI (GenAI) has witnessed remarkable progress in recent years and\ndemonstrated impressive performance in various generation tasks in different\ndomains such as computer vision and computational design. Many researchers have\nattempted to integrate GenAI into visualization framework, leveraging the\nsuperior generative capacity for different operations. Concurrently, recent\nmajor breakthroughs in GenAI like diffusion model and large language model have\nalso drastically increase the potential of GenAI4VIS. From a technical\nperspective, this paper looks back on previous visualization studies leveraging\nGenAI and discusses the challenges and opportunities for future research.\nSpecifically, we cover the applications of different types of GenAI methods\nincluding sequence, tabular, spatial and graph generation techniques for\ndifferent tasks of visualization which we summarize into four major stages:\ndata enhancement, visual mapping generation, stylization and interaction. For\neach specific visualization sub-task, we illustrate the typical data and\nconcrete GenAI algorithms, aiming to provide in-depth understanding of the\nstate-of-the-art GenAI4VIS techniques and their limitations. Furthermore, based\non the survey, we discuss three major aspects of challenges and research\nopportunities including evaluation, dataset, and the gap between end-to-end\nGenAI and generative algorithms. By summarizing different generation\nalgorithms, their current applications and limitations, this paper endeavors to\nprovide useful insights for future GenAI4VIS research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.18144v1",
    "published_date": "2024-04-28 11:27:30 UTC",
    "updated_date": "2024-04-28 11:27:30 UTC"
  },
  {
    "arxiv_id": "2404.18134v2",
    "title": "Learning Fairer Representations with FairVIC",
    "authors": [
      "Charmaine Barker",
      "Daniel Bethell",
      "Dimitar Kazakov"
    ],
    "abstract": "Mitigating bias in automated decision-making systems, particularly in deep\nlearning models, is a critical challenge due to nuanced definitions of\nfairness, dataset-specific biases, and the inherent trade-off between fairness\nand accuracy. To address these issues, we introduce FairVIC, an innovative\napproach that enhances fairness in neural networks by integrating variance,\ninvariance, and covariance terms into the loss function during training. Unlike\nmethods that rely on predefined fairness criteria, FairVIC abstracts fairness\nconcepts to minimise dependency on protected characteristics. We evaluate\nFairVIC against comparable bias mitigation techniques on benchmark datasets,\nconsidering both group and individual fairness, and conduct an ablation study\non the accuracy-fairness trade-off. FairVIC demonstrates significant\nimprovements ($\\approx70\\%$) in fairness across all tested metrics without\ncompromising accuracy, thus offering a robust, generalisable solution for fair\ndeep learning across diverse tasks and datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.18134v2",
    "published_date": "2024-04-28 10:10:21 UTC",
    "updated_date": "2025-02-03 12:49:14 UTC"
  },
  {
    "arxiv_id": "2404.18130v2",
    "title": "Logic Agent: Enhancing Validity with Logic Rule Invocation",
    "authors": [
      "Hanmeng Liu",
      "Zhiyang Teng",
      "Chaoli Zhang",
      "Yue Zhang"
    ],
    "abstract": "Chain-of-Thought (CoT) prompting has emerged as a pivotal technique for\naugmenting the inferential capabilities of language models during reasoning\ntasks. Despite its advancements, CoT often grapples with challenges in\nvalidating reasoning validity and ensuring informativeness. Addressing these\nlimitations, this paper introduces the Logic Agent (LA), an agent-based\nframework aimed at enhancing the validity of reasoning processes in Large\nLanguage Models (LLMs) through strategic logic rule invocation. Unlike\nconventional approaches, LA transforms LLMs into logic agents that dynamically\napply propositional logic rules, initiating the reasoning process by converting\nnatural language inputs into structured logic forms. The logic agent leverages\na comprehensive set of predefined functions to systematically navigate the\nreasoning process. This methodology not only promotes the structured and\ncoherent generation of reasoning constructs but also significantly improves\ntheir interpretability and logical coherence. Through extensive\nexperimentation, we demonstrate LA's capacity to scale effectively across\nvarious model sizes, markedly improving the precision of complex reasoning\nacross diverse tasks.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "The experiment is subject to certain errors",
    "pdf_url": "http://arxiv.org/pdf/2404.18130v2",
    "published_date": "2024-04-28 10:02:28 UTC",
    "updated_date": "2024-12-06 01:34:38 UTC"
  },
  {
    "arxiv_id": "2404.18955v1",
    "title": "GARA: A novel approach to Improve Genetic Algorithms' Accuracy and Efficiency by Utilizing Relationships among Genes",
    "authors": [
      "Zhaoning Shi",
      "Meng Xiang",
      "Zhaoyang Hai",
      "Xiabi Liu",
      "Yan Pei"
    ],
    "abstract": "Genetic algorithms have played an important role in engineering optimization.\nTraditional GAs treat each gene separately. However, biophysical studies of\ngene regulatory networks revealed direct associations between different genes.\nIt inspires us to propose an improvement to GA in this paper, Gene Regulatory\nGenetic Algorithm (GRGA), which, to our best knowledge, is the first time to\nutilize relationships among genes for improving GA's accuracy and efficiency.\nWe design a directed multipartite graph encapsulating the solution space,\ncalled RGGR, where each node corresponds to a gene in the solution and the edge\nrepresents the relationship between adjacent nodes. The edge's weight reflects\nthe relationship degree and is updated based on the idea that the edges'\nweights in a complete chain as candidate solution with acceptable or\nunacceptable performance should be strengthened or reduced, respectively. The\nobtained RGGR is then employed to determine appropriate loci of crossover and\nmutation operators, thereby directing the evolutionary process toward faster\nand better convergence. We analyze and validate our proposed GRGA approach in a\nsingle-objective multimodal optimization problem, and further test it on three\ntypes of applications, including feature selection, text summarization, and\ndimensionality reduction. Results illustrate that our GARA is effective and\npromising.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.18955v1",
    "published_date": "2024-04-28 08:33:39 UTC",
    "updated_date": "2024-04-28 08:33:39 UTC"
  },
  {
    "arxiv_id": "2404.18094v1",
    "title": "USAT: A Universal Speaker-Adaptive Text-to-Speech Approach",
    "authors": [
      "Wenbin Wang",
      "Yang Song",
      "Sanjay Jha"
    ],
    "abstract": "Conventional text-to-speech (TTS) research has predominantly focused on\nenhancing the quality of synthesized speech for speakers in the training\ndataset. The challenge of synthesizing lifelike speech for unseen,\nout-of-dataset speakers, especially those with limited reference data, remains\na significant and unresolved problem. While zero-shot or few-shot\nspeaker-adaptive TTS approaches have been explored, they have many limitations.\nZero-shot approaches tend to suffer from insufficient generalization\nperformance to reproduce the voice of speakers with heavy accents. While\nfew-shot methods can reproduce highly varying accents, they bring a significant\nstorage burden and the risk of overfitting and catastrophic forgetting. In\naddition, prior approaches only provide either zero-shot or few-shot\nadaptation, constraining their utility across varied real-world scenarios with\ndifferent demands. Besides, most current evaluations of speaker-adaptive TTS\nare conducted only on datasets of native speakers, inadvertently neglecting a\nvast portion of non-native speakers with diverse accents. Our proposed\nframework unifies both zero-shot and few-shot speaker adaptation strategies,\nwhich we term as \"instant\" and \"fine-grained\" adaptations based on their\nmerits. To alleviate the insufficient generalization performance observed in\nzero-shot speaker adaptation, we designed two innovative discriminators and\nintroduced a memory mechanism for the speech decoder. To prevent catastrophic\nforgetting and reduce storage implications for few-shot speaker adaptation, we\ndesigned two adapters and a unique adaptation procedure.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "15 pages, 13 figures. Copyright has been transferred to IEEE",
    "pdf_url": "http://arxiv.org/pdf/2404.18094v1",
    "published_date": "2024-04-28 06:50:55 UTC",
    "updated_date": "2024-04-28 06:50:55 UTC"
  },
  {
    "arxiv_id": "2404.18083v2",
    "title": "Online,Target-Free LiDAR-Camera Extrinsic Calibration via Cross-Modal Mask Matching",
    "authors": [
      "Zhiwei Huang",
      "Yikang Zhang",
      "Qijun Chen",
      "Rui Fan"
    ],
    "abstract": "LiDAR-camera extrinsic calibration (LCEC) is crucial for data fusion in\nintelligent vehicles. Offline, target-based approaches have long been the\npreferred choice in this field. However, they often demonstrate poor\nadaptability to real-world environments. This is largely because extrinsic\nparameters may change significantly due to moderate shocks or during extended\noperations in environments with vibrations. In contrast, online, target-free\napproaches provide greater adaptability yet typically lack robustness,\nprimarily due to the challenges in cross-modal feature matching. Therefore, in\nthis article, we unleash the full potential of large vision models (LVMs),\nwhich are emerging as a significant trend in the fields of computer vision and\nrobotics, especially for embodied artificial intelligence, to achieve robust\nand accurate online, target-free LCEC across a variety of challenging\nscenarios. Our main contributions are threefold: we introduce a novel framework\nknown as MIAS-LCEC, provide an open-source versatile calibration toolbox with\nan interactive visualization interface, and publish three real-world datasets\ncaptured from various indoor and outdoor environments. The cornerstone of our\nframework and toolbox is the cross-modal mask matching (C3M) algorithm,\ndeveloped based on a state-of-the-art (SoTA) LVM and capable of generating\nsufficient and reliable matches. Extensive experiments conducted on these\nreal-world datasets demonstrate the robustness of our approach and its superior\nperformance compared to SoTA methods, particularly for the solid-state LiDARs\nwith super-wide fields of view.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "accepted to IEEE Trans. on Intelligent Vehicles (T-IV)",
    "pdf_url": "http://arxiv.org/pdf/2404.18083v2",
    "published_date": "2024-04-28 06:25:56 UTC",
    "updated_date": "2024-06-20 03:20:44 UTC"
  },
  {
    "arxiv_id": "2404.18081v2",
    "title": "ComposerX: Multi-Agent Symbolic Music Composition with LLMs",
    "authors": [
      "Qixin Deng",
      "Qikai Yang",
      "Ruibin Yuan",
      "Yipeng Huang",
      "Yi Wang",
      "Xubo Liu",
      "Zeyue Tian",
      "Jiahao Pan",
      "Ge Zhang",
      "Hanfeng Lin",
      "Yizhi Li",
      "Yinghao Ma",
      "Jie Fu",
      "Chenghua Lin",
      "Emmanouil Benetos",
      "Wenwu Wang",
      "Guangyu Xia",
      "Wei Xue",
      "Yike Guo"
    ],
    "abstract": "Music composition represents the creative side of humanity, and itself is a\ncomplex task that requires abilities to understand and generate information\nwith long dependency and harmony constraints. While demonstrating impressive\ncapabilities in STEM subjects, current LLMs easily fail in this task,\ngenerating ill-written music even when equipped with modern techniques like\nIn-Context-Learning and Chain-of-Thoughts. To further explore and enhance LLMs'\npotential in music composition by leveraging their reasoning ability and the\nlarge knowledge base in music history and theory, we propose ComposerX, an\nagent-based symbolic music generation framework. We find that applying a\nmulti-agent approach significantly improves the music composition quality of\nGPT-4. The results demonstrate that ComposerX is capable of producing coherent\npolyphonic music compositions with captivating melodies, while adhering to user\ninstructions.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.18081v2",
    "published_date": "2024-04-28 06:17:42 UTC",
    "updated_date": "2024-04-30 14:14:26 UTC"
  },
  {
    "arxiv_id": "2405.00066v1",
    "title": "Research and application of artificial intelligence based webshell detection model: A literature review",
    "authors": [
      "Mingrui Ma",
      "Lansheng Han",
      "Chunjie Zhou"
    ],
    "abstract": "Webshell, as the \"culprit\" behind numerous network attacks, is one of the\nresearch hotspots in the field of cybersecurity. However, the complexity,\nstealthiness, and confusing nature of webshells pose significant challenges to\nthe corresponding detection schemes. With the rise of Artificial Intelligence\n(AI) technology, researchers have started to apply different intelligent\nalgorithms and neural network architectures to the task of webshell detection.\nHowever, the related research still lacks a systematic and standardized\nmethodological process, which is confusing and redundant. Therefore, following\nthe development timeline, we carefully summarize the progress of relevant\nresearch in this field, dividing it into three stages: Start Stage, Initial\nDevelopment Stage, and In-depth Development Stage. We further elaborate on the\nmain characteristics and core algorithms of each stage. In addition, we analyze\nthe pain points and challenges that still exist in this field and predict the\nfuture development trend of this field from our point of view. To the best of\nour knowledge, this is the first review that details the research related to\nAI-based webshell detection. It is also hoped that this paper can provide\ndetailed technical information for more researchers interested in AI-based\nwebshell detection tasks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "21 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.00066v1",
    "published_date": "2024-04-28 06:14:27 UTC",
    "updated_date": "2024-04-28 06:14:27 UTC"
  },
  {
    "arxiv_id": "2404.18074v3",
    "title": "MMAC-Copilot: Multi-modal Agent Collaboration Operating Copilot",
    "authors": [
      "Zirui Song",
      "Yaohang Li",
      "Meng Fang",
      "Yanda Li",
      "Zhenhao Chen",
      "Zecheng Shi",
      "Yuan Huang",
      "Xiuying Chen",
      "Ling Chen"
    ],
    "abstract": "Large language model agents that interact with PC applications often face\nlimitations due to their singular mode of interaction with real-world\nenvironments, leading to restricted versatility and frequent hallucinations. To\naddress this, we propose the Multi-Modal Agent Collaboration framework\n(MMAC-Copilot), a framework utilizes the collective expertise of diverse agents\nto enhance interaction ability with application. The framework introduces a\nteam collaboration chain, enabling each participating agent to contribute\ninsights based on their specific domain knowledge, effectively reducing the\nhallucination associated with knowledge domain gaps. We evaluate MMAC-Copilot\nusing the GAIA benchmark and our newly introduced Visual Interaction Benchmark\n(VIBench). MMAC-Copilot achieved exceptional performance on GAIA, with an\naverage improvement of 6.8\\% over existing leading systems. VIBench focuses on\nnon-API-interactable applications across various domains, including 3D gaming,\nrecreation, and office scenarios. It also demonstrated remarkable capability on\nVIBench. We hope this work can inspire in this field and provide a more\ncomprehensive assessment of Autonomous agents. The anonymous Github is\navailable at\n\\href{https://anonymous.4open.science/r/ComputerAgentWithVision-3C12}{Anonymous\nGithub}",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "Technical Reports",
    "pdf_url": "http://arxiv.org/pdf/2404.18074v3",
    "published_date": "2024-04-28 05:33:15 UTC",
    "updated_date": "2025-03-23 13:04:57 UTC"
  },
  {
    "arxiv_id": "2404.18066v1",
    "title": "Quantized Context Based LIF Neurons for Recurrent Spiking Neural Networks in 45nm",
    "authors": [
      "Sai Sukruth Bezugam",
      "Yihao Wu",
      "JaeBum Yoo",
      "Dmitri Strukov",
      "Bongjin Kim"
    ],
    "abstract": "In this study, we propose the first hardware implementation of a\ncontext-based recurrent spiking neural network (RSNN) emphasizing on\nintegrating dual information streams within the neocortical pyramidal neurons\nspecifically Context- Dependent Leaky Integrate and Fire (CLIF) neuron models,\nessential element in RSNN. We present a quantized version of the CLIF neuron\n(qCLIF), developed through a hardware-software codesign approach utilizing the\nsparse activity of RSNN. Implemented in a 45nm technology node, the qCLIF is\ncompact (900um^2) and achieves a high accuracy of 90% despite 8 bit\nquantization on DVS gesture classification dataset. Our analysis spans a\nnetwork configuration from 10 to 200 qCLIF neurons, supporting up to 82k\nsynapses within a 1.86 mm^2 footprint, demonstrating scalability and efficiency",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.AR",
      "cs.CV",
      "q-bio.NC"
    ],
    "primary_category": "cs.NE",
    "comment": "7 Pages, 7 Figures, 2 Tables",
    "pdf_url": "http://arxiv.org/pdf/2404.18066v1",
    "published_date": "2024-04-28 04:32:44 UTC",
    "updated_date": "2024-04-28 04:32:44 UTC"
  },
  {
    "arxiv_id": "2404.18065v1",
    "title": "Grounded Compositional and Diverse Text-to-3D with Pretrained Multi-View Diffusion Model",
    "authors": [
      "Xiaolong Li",
      "Jiawei Mo",
      "Ying Wang",
      "Chethan Parameshwara",
      "Xiaohan Fei",
      "Ashwin Swaminathan",
      "CJ Taylor",
      "Zhuowen Tu",
      "Paolo Favaro",
      "Stefano Soatto"
    ],
    "abstract": "In this paper, we propose an effective two-stage approach named\nGrounded-Dreamer to generate 3D assets that can accurately follow complex,\ncompositional text prompts while achieving high fidelity by using a pre-trained\nmulti-view diffusion model. Multi-view diffusion models, such as MVDream, have\nshown to generate high-fidelity 3D assets using score distillation sampling\n(SDS). However, applied naively, these methods often fail to comprehend\ncompositional text prompts, and may often entirely omit certain subjects or\nparts. To address this issue, we first advocate leveraging text-guided 4-view\nimages as the bottleneck in the text-to-3D pipeline. We then introduce an\nattention refocusing mechanism to encourage text-aligned 4-view image\ngeneration, without the necessity to re-train the multi-view diffusion model or\ncraft a high-quality compositional 3D dataset. We further propose a hybrid\noptimization strategy to encourage synergy between the SDS loss and the sparse\nRGB reference images. Our method consistently outperforms previous\nstate-of-the-art (SOTA) methods in generating compositional 3D assets,\nexcelling in both quality and accuracy, and enabling diverse 3D from the same\ntext prompt.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.18065v1",
    "published_date": "2024-04-28 04:05:10 UTC",
    "updated_date": "2024-04-28 04:05:10 UTC"
  },
  {
    "arxiv_id": "2407.01551v1",
    "title": "Leveraging Prompts in LLMs to Overcome Imbalances in Complex Educational Text Data",
    "authors": [
      "Jeanne McClure",
      "Machi Shimmei",
      "Noboru Matsuda",
      "Shiyan Jiang"
    ],
    "abstract": "In this paper, we explore the potential of Large Language Models (LLMs) with\nassertions to mitigate imbalances in educational datasets. Traditional models\noften fall short in such contexts, particularly due to the complexity and\nnuanced nature of the data. This issue is especially prominent in the education\nsector, where cognitive engagement levels among students show significant\nvariation in their open responses. To test our hypothesis, we utilized an\nexisting technology for assertion-based prompt engineering through an\n'Iterative - ICL PE Design Process' comparing traditional Machine Learning (ML)\nmodels against LLMs augmented with assertions (N=135). Further, we conduct a\nsensitivity analysis on a subset (n=27), examining the variance in model\nperformance concerning classification metrics and cognitive engagement levels\nin each iteration. Our findings reveal that LLMs with assertions significantly\noutperform traditional ML models, particularly in cognitive engagement levels\nwith minority representation, registering up to a 32% increase in F1-score.\nAdditionally, our sensitivity study indicates that incorporating targeted\nassertions into the LLM tested on the subset enhances its performance by\n11.94%. This improvement primarily addresses errors stemming from the model's\nlimitations in understanding context and resolving lexical ambiguities in\nstudent responses.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "17 pages, 5 figures, 3 tables, 2 appendices",
    "pdf_url": "http://arxiv.org/pdf/2407.01551v1",
    "published_date": "2024-04-28 00:24:08 UTC",
    "updated_date": "2024-04-28 00:24:08 UTC"
  }
]
{
  "date": "2025-12-06",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿Žæ¥åˆ° UTC æ—¶é—´ 2025-12-06 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼æˆ‘æ˜¯ä½ ä»¬çš„æ—¥æŠ¥ä½œè€…ï¼ŒGemini Enterpriseã€‚\n\n**ðŸ“Š ä»Šæ—¥æ¦‚è§ˆ**\n\nä»Šå¤©çš„ arXiv æ›´æ–°å¯è°“æ˜¯**Agentï¼ˆæ™ºèƒ½ä½“ï¼‰åº”ç”¨çš„çˆ†å‘æ—¥**ä¸Ž**å¤§æ¨¡åž‹æŽ¨ç†ä¼˜åŒ–**çš„ç«žæŠ€åœºã€‚æˆ‘ä»¬çœ‹åˆ°äº† Agent åœ¨åŒ»ç–—è¯Šæ–­ã€å•†ä¸šè°ˆåˆ¤ã€ç”šè‡³é‡å­å…‰å­¦å®žéªŒè®¾è®¡ä¸­çš„æ·±åº¦è½åœ°ï¼›åŒæ—¶ï¼Œå…³äºŽ LLM æŽ¨ç†èƒ½åŠ›ï¼ˆReasoningï¼‰çš„è®­ç»ƒæ–¹æ³•ï¼ˆå¦‚ GRPO çš„æ”¹è¿›ï¼‰ä»¥åŠ Agent å®‰å…¨ï¼ˆMCP åè®®æ¼æ´žï¼‰çš„è®¨è®ºä¹Ÿæžå…¶ç¡¬æ ¸ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€ç¯‡å…³äºŽ AI å¦‚ä½•ä½œä¸ºâ€œè”åˆåˆ›å§‹äººâ€å½±å“åˆ›ä¸šçš„ç»æµŽå­¦è®ºæ–‡ï¼Œéžå¸¸å€¼å¾—ä¸€è¯»ã€‚\n\nä¸‹é¢è®©æˆ‘ä»¬è¿›å…¥æ·±åº¦é˜…è¯»æ—¶é—´ã€‚\n\n---\n\n### ðŸ¤– Agent & Multi-Agent Systemsï¼šä»Žè°ˆåˆ¤åˆ°åŒ»ç–—\n\nä»Šå¤©æœ€ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ Agent åœ¨ç‰¹å®šåž‚ç±»ä»»åŠ¡ä¸­çš„å¤æ‚äº¤äº’èƒ½åŠ›ï¼Œä»¥åŠå¯¹ Agent è‡ªèº«è¡Œä¸ºæœºåˆ¶çš„ç ”ç©¶ã€‚\n\n**1. [æŽ¨è] Echo-CoPilot: A Multi-View, Multi-Task Agent for Echocardiography Interpretation and Reporting**\n*   **Echo-CoPilotï¼šç”¨äºŽè¶…å£°å¿ƒåŠ¨å›¾è§£é‡Šå’ŒæŠ¥å‘Šçš„å¤šè§†è§’ã€å¤šä»»åŠ¡æ™ºèƒ½ä½“**\n*   **æ ¸å¿ƒçœ‹ç‚¹**ï¼šåŒ»ç–— Agent çš„å®žæˆ˜è½åœ°ã€‚\n*   **TLDR**ï¼šä¸ä»…ä»…æ˜¯çœ‹å›¾è¯´è¯ï¼Œè¿™è¿˜æ˜¯ä¸€ä¸ªåŸºäºŽ **ReAct** èŒƒå¼çš„ Agentã€‚å®ƒèƒ½åˆ†è§£ä¸´åºŠåŒ»ç”Ÿçš„æŸ¥è¯¢ï¼Œè°ƒç”¨ä¸“é—¨çš„å·¥å…·ï¼ˆè§†å›¾è¯†åˆ«ã€å¿ƒè„ç»“æž„åˆ†å‰²ã€ç–¾ç—…é¢„æµ‹ï¼‰ï¼Œæœ€åŽç”Ÿæˆç¬¦åˆæŒ‡å—çš„æŠ¥å‘Šã€‚åœ¨ MIMIC-EchoQA åŸºå‡†æµ‹è¯•ä¸­å‡†ç¡®çŽ‡è¾¾åˆ° 50.8%ï¼Œè¶…è¶Šäº†é€šç”¨çš„ç”Ÿç‰©åŒ»å­¦è§†é¢‘æ¨¡åž‹ã€‚å®ƒèƒ½å¤„ç†ä¸´åºŠå†³ç­–é˜ˆå€¼é™„è¿‘çš„â€œè¾¹ç¼˜ç—…ä¾‹â€ï¼ˆå¦‚ä¸´ç•Œå·¦å¿ƒå®¤è‚¥å¤§ï¼‰ï¼Œè¿™æ­£æ˜¯ AI è¾…åŠ©è¯Šæ–­çš„ç—›ç‚¹ã€‚\n\n**2. ChargingBoul: A Competitive Negotiating Agent with Novel Opponent Modeling**\n*   **ChargingBoulï¼šå…·æœ‰æ–°åž‹å¯¹æ‰‹å»ºæ¨¡èƒ½åŠ›çš„ç«žäº‰æ€§è°ˆåˆ¤æ™ºèƒ½ä½“**\n*   **æ ¸å¿ƒçœ‹ç‚¹**ï¼šANAC æ¯”èµ›äºšå†›ç­–ç•¥ï¼Œå•†ä¸šè°ˆåˆ¤è‡ªåŠ¨åŒ–çš„å‚è€ƒã€‚\n*   **TLDR**ï¼šè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„è°ˆåˆ¤ Agentï¼Œæ ¸å¿ƒåœ¨äºŽ**å¯¹æ‰‹å»ºæ¨¡ï¼ˆOpponent Modelingï¼‰**ã€‚å®ƒæ ¹æ®å¯¹æ‰‹çš„å‡ºä»·æ¨¡å¼å¯¹å…¶è¿›è¡Œåˆ†ç±»ï¼ŒåŠ¨æ€è°ƒæ•´è‡ªå·±çš„å‡ºä»·ç­–ç•¥ï¼Œå¹¶åœ¨è°ˆåˆ¤åŽæœŸé‡‡ç”¨è®©æ­¥ç­–ç•¥ä»¥ä¿ƒæˆäº¤æ˜“ã€‚åœ¨èµ„æºåˆ†é…å’Œç”µå­å•†åŠ¡åœºæ™¯ä¸­æœ‰å¾ˆå¤§æ½œåŠ›ã€‚\n\n**3. [æœ‰è¶£] Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic Setup**\n*   **å½“ä¸¤ä¸ªå¤§è¯­è¨€æ¨¡åž‹åœ¨å¤šæ™ºèƒ½ä½“è®¾ç½®ä¸­äº¤äº’æ—¶çš„è¾“å‡ºæ”¶æ•›**\n*   **æ ¸å¿ƒçœ‹ç‚¹**ï¼šLLM çš„â€œå›žéŸ³å®¤â€æ•ˆåº”ç ”ç©¶ã€‚\n*   **TLDR**ï¼šè®©ä¸¤ä¸ª LLMï¼ˆMistral å’Œ Llamaï¼‰åœ¨æ²¡æœ‰ä»»ä½•å¤–éƒ¨è¾“å…¥çš„æƒ…å†µä¸‹äº’ç›¸èŠå¤©ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿç ”ç©¶å‘çŽ°ï¼Œå°½ç®¡å¼€å§‹å¾ˆè¿žè´¯ï¼Œä½†å®ƒä»¬æœ€ç»ˆä¼šé™·å…¥**é‡å¤ï¼ˆRepetitionï¼‰**å’Œ**æ”¶æ•›ï¼ˆConvergenceï¼‰**ã€‚å¯¹è¯å¾€å¾€ä¼šæ¼‚ç§»å‡ºåˆå§‹ç§å­è¯é¢˜ï¼Œå¹¶é”å®šåœ¨æŸäº›çŸ­è¯­çš„æ­»å¾ªçŽ¯ä¸­ã€‚è¿™å¯¹è®¾è®¡é•¿æœŸè¿è¡Œçš„è‡ªæ²» Agent ç³»ç»Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„è­¦ç¤ºã€‚\n\n**4. Securing the Model Context Protocol: Defending LLMs Against Tool Poisoning and Adversarial Attacks**\n*   **ä¿æŠ¤æ¨¡åž‹ä¸Šä¸‹æ–‡åè®® (MCP)ï¼šé˜²å¾¡é’ˆå¯¹ LLM çš„å·¥å…·ä¸­æ¯’å’Œå¯¹æŠ—æ€§æ”»å‡»**\n*   **æ ¸å¿ƒçœ‹ç‚¹**ï¼šé’ˆå¯¹å½“å‰ç«çƒ­çš„ MCP åè®®ï¼ˆAnthropic ç­‰æŽ¨åŠ¨çš„æ ‡å‡†ï¼‰çš„å®‰å…¨æ€§åˆ†æžã€‚\n*   **TLDR**ï¼šéšç€ MCP è®© LLM æ›´å®¹æ˜“è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼Œå®‰å…¨éšæ‚£ä¹Ÿéšä¹‹è€Œæ¥ã€‚ä½œè€…æå‡ºäº†**å·¥å…·ä¸­æ¯’ï¼ˆTool Poisoningï¼‰**ã€**é˜´å½±æ”»å‡»ï¼ˆShadowingï¼‰**å’Œ**åœ°æ¯¯å¼æ’¤å›žï¼ˆRug Pullsï¼‰**ä¸‰ç§æ”»å‡»æ–¹å¼ã€‚ä»–ä»¬æå‡ºäº†ä¸€å¥—åˆ†å±‚é˜²å¾¡æ¡†æž¶ï¼ŒåŒ…æ‹¬ RSA ç­¾åéªŒè¯å’Œ LLM-on-LLM çš„è¯­ä¹‰å®¡æŸ¥ã€‚GPT-4 ç›®å‰èƒ½æ‹¦æˆªçº¦ 71% çš„ä¸å®‰å…¨è°ƒç”¨ï¼Œä½† DeepSeek åœ¨å¯¹æŠ—â€œé˜´å½±æ”»å‡»â€ä¸Šè¡¨çŽ°æ›´å¼ºéŸ§ã€‚\n\n---\n\n### ðŸ§  LLM Training & Reasoningï¼šæŽ¨ç†ä¸Žè®­ç»ƒçš„æ–°èŒƒå¼\n\næŽ¨ç†ï¼ˆReasoningï¼‰èƒ½åŠ›çš„æå‡å’Œè®­ç»ƒæ•ˆçŽ‡çš„ä¼˜åŒ–ä¾ç„¶æ˜¯ä¸»æ—‹å¾‹ã€‚\n\n**5. [é‡ç£…] DaGRPO: Rectifying Gradient Conflict in Reasoning via Distinctiveness-Aware Group Relative Policy Optimization**\n*   **DaGRPOï¼šé€šè¿‡å·®å¼‚æ„ŸçŸ¥ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ä¿®æ­£æŽ¨ç†ä¸­çš„æ¢¯åº¦å†²çª**\n*   **æ ¸å¿ƒçœ‹ç‚¹**ï¼šæ”¹è¿›äº†çƒ­é—¨çš„ **GRPO** ç®—æ³•ï¼ˆDeepSeek-R1 èƒŒåŽçš„æŠ€æœ¯è·¯çº¿ï¼‰ã€‚\n*   **TLDR**ï¼šGRPO è™½ç„¶åœ¨æ¿€å‘æŽ¨ç†èƒ½åŠ›ä¸Šå¾ˆå¼ºï¼Œä½†å­˜åœ¨è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚ä½œè€…å‘çŽ°åŽŸå› æ˜¯ On-policy é‡‡æ ·æ ·æœ¬ç¼ºä¹**åŒºåˆ†åº¦ï¼ˆDistinctivenessï¼‰**ï¼Œå¯¼è‡´æ¢¯åº¦å†²çªã€‚DaGRPO å¼•å…¥äº†åºåˆ—çº§æ¢¯åº¦çŸ«æ­£å’Œ Off-policy æ•°æ®å¢žå¼ºï¼Œåœ¨ 9 ä¸ªæ•°å­¦æŽ¨ç†åŸºå‡†ä¸Šå¹³å‡æå‡äº† 4.7%ï¼Œæœ‰æ•ˆç¼“è§£äº†æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼ŒåŠ é€Ÿäº†é•¿é“¾æŽ¨ç†èƒ½åŠ›çš„æ¶ŒçŽ°ã€‚\n\n**6. A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation**\n*   **A-3POï¼šåˆ©ç”¨é™ˆæ—§åº¦æ„ŸçŸ¥çš„è¿‘ç«¯ç­–ç•¥è¿‘ä¼¼åŠ é€Ÿå¼‚æ­¥ LLM è®­ç»ƒ**\n*   **æ ¸å¿ƒçœ‹ç‚¹**ï¼šRLHF è®­ç»ƒåŠ é€Ÿã€‚\n*   **TLDR**ï¼šé’ˆå¯¹å¼‚æ­¥ PPO è®­ç»ƒä¸­çš„æ•°æ®é™ˆæ—§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€é¢å¤–å‰å‘ä¼ é€’çš„è¿‘ä¼¼æ–¹æ³•ã€‚ç®€å•æ¥è¯´ï¼Œå®ƒåŽ»æŽ‰äº†è®¡ç®—è¿‘ç«¯ç­–ç•¥ï¼ˆProximal Policyï¼‰çš„å¼€é”€ï¼Œé€šè¿‡æ’å€¼æ¥è¿‘ä¼¼ï¼Œå°†è®­ç»ƒé€Ÿåº¦æå‡äº† **1.8å€**ï¼Œä¸”æ€§èƒ½æ²¡æœ‰ä¸‹é™ã€‚\n\n**7. Uncovering Competency Gaps in Large Language Models and Their Benchmarks**\n*   **æ­ç¤ºå¤§è¯­è¨€æ¨¡åž‹åŠå…¶åŸºå‡†æµ‹è¯•ä¸­çš„èƒ½åŠ›å·®è·**\n*   **æ ¸å¿ƒçœ‹ç‚¹**ï¼šåˆ©ç”¨ **ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ (SAEs)** è¿›è¡Œå¯è§£é‡Šæ€§è¯„ä¼°ã€‚\n*   **TLDR**ï¼šä¼ ç»Ÿçš„åŸºå‡†æµ‹è¯•åªèƒ½ç»™å‡ºä¸€ä¸ªæ€»åˆ†ï¼ŒæŽ©ç›–äº†æ¨¡åž‹å…·ä½“çš„çŸ­æ¿ã€‚è¿™ç¯‡è®ºæ–‡åˆ©ç”¨ SAE æå–æ¨¡åž‹å†…éƒ¨çš„æ¦‚å¿µæ¿€æ´»ï¼Œè‡ªåŠ¨å‘çŽ°äº†æ¨¡åž‹åœ¨â€œç¤¼è²Œæ‹’ç»â€å’Œâ€œå®‰å…¨è¾¹ç•Œâ€ç­‰æ¦‚å¿µä¸Šçš„ç³»ç»Ÿæ€§ç¼ºé™·ã€‚åŒæ—¶ä¹ŸæŒ‡å‡ºçŽ°æœ‰åŸºå‡†æµ‹è¯•è¿‡åˆ†å¼ºè°ƒé¡ºä»Žæ€§ï¼Œç¼ºä¹å¯¹æ ¸å¿ƒæ¦‚å¿µçš„è¦†ç›–ã€‚\n\n---\n\n### ðŸŒ AI for Science & Societyï¼šåˆ›ä¸šä¸Žç§‘å­¦æŽ¢ç´¢\n\n**8. [ç¤¾ä¼šå­¦] AI as \"Co-founder\": GenAI for Entrepreneurship**\n*   **AIä½œä¸ºâ€œè”åˆåˆ›å§‹äººâ€ï¼šç”Ÿæˆå¼AIå¯¹åˆ›ä¸šçš„å½±å“**\n*   **æ ¸å¿ƒçœ‹ç‚¹**ï¼šChatGPT å‘å¸ƒåŽçš„ç»æµŽå­¦å®žè¯ç ”ç©¶ã€‚\n*   **TLDR**ï¼šåŸºäºŽä¸­å›½ 2024 å¹´åº•å‰çš„ä¼ä¸šæ³¨å†Œæ•°æ®ï¼Œç ”ç©¶å‘çŽ° GenAI æ˜¾è‘—é™ä½Žäº†åˆ›ä¸šæˆæœ¬ã€‚åœ¨ AI äººåŠ›èµ„æœ¬è¾ƒå¼ºçš„åœ°åŒºï¼Œ**å°åž‹ä¼ä¸š**çš„æˆç«‹æ•°é‡æ€¥å‰§æ¿€å¢žï¼Œè€Œå¤§åž‹ä¼ä¸šçš„è¿›å…¥åˆ™æœ‰æ‰€ä¸‹é™ã€‚GenAI æ­£åœ¨æˆä¸ºä¸€ç§â€œä¿ƒç«žäº‰â€çš„åŠ›é‡ï¼Œä¸æˆæ¯”ä¾‹åœ°å¸®åŠ©äº†é¦–æ¬¡åˆ›ä¸šè€…å’Œèµ„é‡‘è¾ƒå°‘çš„å›¢é˜Ÿã€‚\n\n**9. GENIUS: An Agentic AI Framework for Autonomous Design and Execution of Simulation Protocols**\n*   **GENIUSï¼šç”¨äºŽè‡ªä¸»è®¾è®¡å’Œæ‰§è¡Œæ¨¡æ‹Ÿåè®®çš„ä»£ç† AI æ¡†æž¶**\n*   **æ ¸å¿ƒçœ‹ç‚¹**ï¼šææ–™ç§‘å­¦é¢†åŸŸçš„ **ICME**ï¼ˆé›†æˆè®¡ç®—ææ–™å·¥ç¨‹ï¼‰è‡ªåŠ¨åŒ–ã€‚\n*   **TLDR**ï¼šè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ DFTï¼ˆå¯†åº¦æ³›å‡½ç†è®ºï¼‰æ¨¡æ‹Ÿçš„ Agentã€‚å®ƒèƒ½æŠŠè‡ªç„¶è¯­è¨€éœ€æ±‚è½¬åŒ–ä¸º Quantum ESPRESSO çš„è¾“å…¥æ–‡ä»¶ï¼Œå¹¶å…·å¤‡**è‡ªæˆ‘ä¿®å¤ï¼ˆSelf-healingï¼‰**èƒ½åŠ›â€”â€”å¦‚æžœæ¨¡æ‹ŸæŠ¥é”™ï¼Œå®ƒèƒ½è‡ªåŠ¨ debugã€‚ç›¸æ¯”çº¯ LLMï¼Œå®ƒå‡ ä¹Žæ¶ˆé™¤äº†å¹»è§‰ï¼Œè®©éžä¸“å®¶ä¹Ÿèƒ½è·‘é«˜æ·±çš„ææ–™æ¨¡æ‹Ÿã€‚\n\n**10. Protecting Bystander Privacy via Selective Hearing in Audio LLMs**\n*   **é€šè¿‡éŸ³é¢‘ LLM ä¸­çš„é€‰æ‹©æ€§å¬è§‰ä¿æŠ¤æ—è§‚è€…éšç§**\n*   **æ ¸å¿ƒçœ‹ç‚¹**ï¼šAudio LLM çš„éšç§æ³„éœ²é—®é¢˜ã€‚\n*   **TLDR**ï¼šçŽ°åœ¨çš„ Audio LLM è€³æœµå¤ªçµäº†ï¼Œç»å¸¸â€œå·å¬â€åˆ°èƒŒæ™¯ä¸­æ—è§‚è€…çš„è¯´è¯å†…å®¹ã€‚ä½œè€…æå‡ºäº† **SH-Bench** åŸºå‡†æµ‹è¯•ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§â€œé€‰æ‹©æ€§å¬è§‰â€çš„å¾®è°ƒæ–¹æ³•ï¼ˆBPFTï¼‰ï¼Œæ•™æ¨¡åž‹åªå¬ä¸»è§’è¯´è¯ï¼Œå¿½ç•¥èƒŒæ™¯äººå£°ï¼Œåœ¨ä¿æŠ¤éšç§çš„åŒæ—¶ä¸é™ä½Žç†è§£èƒ½åŠ›ã€‚\n\n---\n\n### âš¡ å¿«é€Ÿæµè§ˆ (Quick Hits)\n\nä»¥ä¸‹æ–‡ç« ä¹Ÿå€¼å¾—å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯å¦‚æžœä½ åœ¨ç›¸å…³é¢†åŸŸå·¥ä½œï¼š\n\n*   **[RL åŸºç¡€è®¾æ–½] RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs**\n    *   Google å›¢é˜Ÿå±•ç¤ºäº†åœ¨ 1024 ä¸ª TPU ä¸Šè¿›è¡Œå¤§è§„æ¨¡åˆ†å¸ƒå¼ RL è®­ç»ƒçš„æž¶æž„ï¼ŒQwQ-32B æ¨¡åž‹ pass@8 æå‡äº† 12.8%ã€‚\n*   **[è‡ªåŠ¨é©¾é©¶] WAM-Diff: A Masked Diffusion VLA Framework...**\n    *   å°†**æŽ©ç æ‰©æ•£æ¨¡åž‹ï¼ˆMasked Diffusionï¼‰**å¼•å…¥ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ï¼Œåœ¨ NAVSIM æ¦œå•ä¸Šå–å¾—äº†ä¼˜å¼‚æˆç»©ï¼Œæ›¿ä»£äº†ä¼ ç»Ÿçš„è‡ªå›žå½’ç­–ç•¥ã€‚\n*   **[è§†è§‰ç”Ÿæˆ] Rethinking Training Dynamics in Scale-wise Autoregressive Generation**\n    *   é’ˆå¯¹åƒ VAR è¿™æ ·çš„å¤šå°ºåº¦è‡ªå›žå½’å›¾åƒç”Ÿæˆæ¨¡åž‹ï¼Œæå‡ºäº† SAR æ–¹æ³•æ¥è§£å†³â€œæš´éœ²åå·®â€é—®é¢˜ï¼Œæå‡äº†ç”Ÿæˆè´¨é‡ã€‚\n*   **[é‡‘èžé£ŽæŽ§] Bayesian Modeling for Uncertainty Management...**\n    *   ä½¿ç”¨è´å¶æ–¯å»ºæ¨¡è¿›è¡Œé‡‘èžé£Žé™©é¢„æµ‹ï¼Œè™½ç„¶ GARCH æ¨¡åž‹ä½Žä¼°äº†å°¾éƒ¨é£Žé™©ï¼Œä½†æå‡ºçš„ DLM æ¨¡åž‹æä¾›äº†æ›´å¥½çš„ä¸ç¡®å®šæ€§é‡åŒ–ã€‚\n*   **[ç¡¬ä»¶è®¾è®¡] DUET: Agentic Design Understanding via Experimentation and Testing**\n    *   é’ˆå¯¹èŠ¯ç‰‡è®¾è®¡ä¸­çš„ RTL ä»£ç ç†è§£ï¼Œè®© Agent åƒå·¥ç¨‹å¸ˆä¸€æ ·é€šè¿‡â€œå®žéªŒâ€å’Œâ€œæ³¢å½¢æ£€æŸ¥â€æ¥ç†è§£ä»£ç ï¼Œè€Œä¸ä»…ä»…æ˜¯è¯»ä»£ç ã€‚\n\nä»Šå¤©çš„ arXiv å¿«æŠ¥å°±åˆ°è¿™é‡Œã€‚**DaGRPO** å’Œ **Echo-CoPilot** æ˜¯ä»Šå¤©çš„å¿…è¯»æŽ¨èã€‚å¸Œæœ›è¿™äº›å‰æ²¿ç ”ç©¶èƒ½ç»™ä½ çš„å·¥ä½œå¸¦æ¥çµæ„Ÿã€‚æˆ‘ä»¬æ˜Žå¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2512.06595v1",
      "title": "ChargingBoul: A Competitive Negotiating Agent with Novel Opponent Modeling",
      "title_zh": "ChargingBoulï¼šä¸€ç§åŸºäºŽæ–°é¢–å¯¹æ‰‹å»ºæ¨¡çš„ç«žäº‰åž‹åå•†æ™ºèƒ½ä½“",
      "authors": [
        "Joe Shymanski"
      ],
      "abstract": "Automated negotiation has emerged as a critical area of research in multiagent systems, with applications spanning e-commerce, resource allocation, and autonomous decision-making. This paper presents ChargingBoul, a negotiating agent that competed in the 2022 Automated Negotiating Agents Competition (ANAC) and placed second in individual utility by an exceptionally narrow margin. ChargingBoul employs a lightweight yet effective strategy that balances concession and opponent modeling to achieve high negotiation outcomes. The agent classifies opponents based on bid patterns, dynamically adjusts its bidding strategy, and applies a concession policy in later negotiation stages to maximize utility while fostering agreements. We evaluate ChargingBoul's performance using competition results and subsequent studies that have utilized the agent in negotiation research. Our analysis highlights ChargingBoul's effectiveness across diverse opponent strategies and its contributions to advancing automated negotiation techniques. We also discuss potential enhancements, including more sophisticated opponent modeling and adaptive bidding heuristics, to improve its performance further.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† ChargingBoulï¼Œè¿™æ˜¯ä¸€ç§åœ¨ 2022 å¹´å›½é™…è‡ªåŠ¨è°ˆåˆ¤ä»£ç†æ¯”èµ› (ANAC) ä¸­èŽ·å¾—ä¸ªäººæ•ˆç”¨ç¬¬äºŒåçš„ç«žäº‰æ€§è°ˆåˆ¤æ™ºèƒ½ä½“ã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§è½»é‡çº§ä¸”æœ‰æ•ˆçš„ç­–ç•¥ï¼Œé€šè¿‡å¹³è¡¡è®©æ­¥ (concession) ä¸Žå¯¹æ‰‹å»ºæ¨¡ (opponent modeling) æ¥èŽ·å–é«˜æ°´å¹³çš„è°ˆåˆ¤æˆæžœã€‚è¯¥æ™ºèƒ½ä½“æ ¹æ®å‡ºä»·æ¨¡å¼ (bid patterns) å¯¹å¯¹æ‰‹è¿›è¡Œåˆ†ç±»å¹¶åŠ¨æ€è°ƒæ•´å…¶å‡ºä»·ç­–ç•¥ï¼Œåœ¨è°ˆåˆ¤åŽæœŸé€šè¿‡ç‰¹å®šçš„è®©æ­¥æ”¿ç­–åœ¨ä¿ƒè¿›åè®®è¾¾æˆçš„åŒæ—¶æœ€å¤§åŒ–è‡ªèº«æ•ˆç”¨ (utility)ã€‚è¯„ä¼°ç»“æžœè¯æ˜Žäº† ChargingBoul åœ¨é¢å¯¹å¤šæ ·åŒ–å¯¹æ‰‹ç­–ç•¥æ—¶çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶å¯¹è‡ªåŠ¨åŒ–è°ˆåˆ¤æŠ€æœ¯çš„è´¡çŒ®ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æŽ¢è®¨äº†æœªæ¥å¼•å…¥æ›´å¤æ‚çš„å¯¹æ‰‹å»ºæ¨¡å’Œè‡ªé€‚åº”å‡ºä»·å¯å‘å¼ç®—æ³• (adaptive bidding heuristics) ä»¥è¿›ä¸€æ­¥å¢žå¼ºæ€§èƒ½çš„å¯èƒ½æ€§ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "10 pages, 3 figures. Describes the ChargingBoul negotiating agent submitted to ANAC 2022. Preprint",
      "pdf_url": "https://arxiv.org/pdf/2512.06595v1",
      "published_date": "2025-12-06 23:32:11 UTC",
      "updated_date": "2025-12-06 23:32:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:13:49.587210+00:00"
    },
    {
      "arxiv_id": "2512.09944v2",
      "title": "Echo-CoPilot: A Multi-View, Multi-Task Agent for Echocardiography Interpretation and Reporting",
      "title_zh": "Echo-CoPilotï¼šç”¨äºŽè¶…å£°å¿ƒåŠ¨å›¾è§£è¯»ä¸ŽæŠ¥å‘Šçš„å¤šè§†è§’ã€å¤šä»»åŠ¡æ™ºèƒ½ä½“",
      "authors": [
        "Moein Heidari",
        "Mohammad Amin Roohi",
        "Ilker Hacihaliloglu"
      ],
      "abstract": "Echocardiography is central to contemporary cardiovascular care, but full-study interpretation remains a cognitively demanding, multi-view task that is still performed manually. While recent foundation models for echocardiography can achieve strong performance on individual perceptual subtasks such as view classification, segmentation, or disease prediction, they typically operate in isolation and do not provide a unified, clinically coherent assessment. In this work, we introduce Echo-CoPilot, a multi-view, multi-task agent that uses a large language model to orchestrate a suite of specialized echocardiography tools. Within a ReAct-style loop, the agent decomposes clinician queries, invokes tools for view recognition, cardiac structure segmentation, measurement and disease prediction, and report synthesis, and integrates their outputs into guideline-aware answers and narrative summaries. We evaluate Echo-CoPilot on the public MIMIC-EchoQA benchmark, where it achieves an accuracy of 50.8\\%, outperforming both general-purpose and biomedical video vision-language models. Qualitative analyses further show that the agent leverages quantitative measurements and physiologic context to resolve challenging cases near clinical decision thresholds, such as borderline left ventricular hypertrophy or pericardial effusion severity. The code will be released upon acceptance of the paper.",
      "tldr_zh": "è¯¥ç ”ç©¶æŽ¨å‡ºäº† Echo-CoPilotï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹è¶…å£°å¿ƒåŠ¨å›¾ (Echocardiography) è§£é‡Šå’ŒæŠ¥å‘Šçš„å¤šè§†å›¾ã€å¤šä»»åŠ¡æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨è§£å†³ä¸´åºŠè¯„ä¼°ä¸­æ‰‹åŠ¨æ“ä½œå¤æ‚ä¸”ç¼ºä¹ç»Ÿä¸€æ€§çš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨å¤§è¯­è¨€æ¨¡åž‹ (Large Language Model) ä½œä¸ºæ ¸å¿ƒï¼Œé€šè¿‡ ReAct é£Žæ ¼çš„å¾ªçŽ¯é€»è¾‘ç¼–æŽ’ä¸€ç³»åˆ—ä¸“é—¨çš„è¶…å£°å¿ƒåŠ¨å›¾å·¥å…·ã€‚Echo-CoPilot èƒ½å¤Ÿè‡ªåŠ¨åˆ†è§£ä¸´åºŠæŸ¥è¯¢ï¼Œå¹¶è°ƒç”¨ç‰¹å®šå·¥å…·æ‰§è¡Œè§†å›¾è¯†åˆ«ã€å¿ƒè„ç»“æž„åˆ†å‰²ã€å®šé‡æµ‹é‡ã€ç–¾ç—…é¢„æµ‹ä»¥åŠæŠ¥å‘Šåˆæˆï¼Œæœ€ç»ˆå°†è¾“å‡ºæ•´åˆä¸ºç¬¦åˆä¸´åºŠæŒ‡å—çš„ç­”æ¡ˆå’Œå™è¿°æ€§æ€»ç»“ã€‚å®žéªŒç»“æžœæ˜¾ç¤ºï¼Œè¯¥æ™ºèƒ½ä½“åœ¨ MIMIC-EchoQA åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† 50.8% çš„å‡†ç¡®çŽ‡ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºŽé€šç”¨çš„å’Œç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„è§†é¢‘è§†è§‰è¯­è¨€æ¨¡åž‹ (Video Vision-Language Models)ã€‚å®šæ€§åˆ†æžè¿›ä¸€æ­¥è¯å®žï¼ŒEcho-CoPilot èƒ½å¤Ÿåˆ©ç”¨ç²¾å‡†çš„å®šé‡æ•°æ®å’Œç”Ÿç†ä¸Šä¸‹æ–‡ï¼Œæœ‰æ•ˆååŠ©åˆ¤å®šå¤„äºŽä¸´åºŠå†³ç­–é˜ˆå€¼è¾¹ç¼˜çš„æŒ‘æˆ˜æ€§ç—…ä¾‹ï¼Œä¾‹å¦‚è¾¹ç¼˜æ€§å·¦å¿ƒå®¤è‚¥åŽš (Left Ventricular Hypertrophy) æˆ–å¿ƒåŒ…ç§¯æ¶² (Pericardial Effusion) çš„ä¸¥é‡ç¨‹åº¦ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09944v2",
      "published_date": "2025-12-06 23:27:54 UTC",
      "updated_date": "2025-12-15 19:58:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:13:47.602606+00:00"
    },
    {
      "arxiv_id": "2512.06591v1",
      "title": "Beyond Satisfaction: From Placebic to Actionable Explanations For Enhanced Understandability",
      "title_zh": "è¶…è¶Šæ»¡æ„åº¦ï¼šä»Žå®‰æ…°å‰‚å¼è§£é‡Šåˆ°å¯æ“ä½œæ€§è§£é‡Šï¼Œæå‡ç³»ç»Ÿå¯ç†è§£æ€§",
      "authors": [
        "Joe Shymanski",
        "Jacob Brue",
        "Sandip Sen"
      ],
      "abstract": "Explainable AI (XAI) presents useful tools to facilitate transparency and trustworthiness in machine learning systems. However, current evaluations of system explainability often rely heavily on subjective user surveys, which may not adequately capture the effectiveness of explanations. This paper critiques the overreliance on user satisfaction metrics and explores whether these can differentiate between meaningful (actionable) and vacuous (placebic) explanations. In experiments involving optimal Social Security filing age selection tasks, participants used one of three protocols: no explanations, placebic explanations, and actionable explanations. Participants who received actionable explanations significantly outperformed the other groups in objective measures of their mental model, but users rated placebic and actionable explanations as equally satisfying. This suggests that subjective surveys alone fail to capture whether explanations truly support users in building useful domain understanding. We propose that future evaluations of agent explanation capabilities should integrate objective task performance metrics alongside subjective assessments to more accurately measure explanation quality. The code for this study can be found at https://github.com/Shymkis/social-security-explainer.",
      "tldr_zh": "è¯¥ç ”ç©¶æŽ¢è®¨äº†å¯è§£é‡Šäººå·¥æ™ºèƒ½(Explainable AI, XAI)è¯„ä¼°ä¸­è¿‡åº¦ä¾èµ–ç”¨æˆ·æ»¡æ„åº¦æŒ‡æ ‡çš„é—®é¢˜ï¼Œå¹¶åŒºåˆ†äº†â€œè¡ŒåŠ¨å¯¼å‘â€(Actionable)å’Œâ€œå®‰æ…°å‰‚å¼â€(Placebic)ä¸¤ç§è§£é‡Šç±»åž‹ã€‚é€šè¿‡åœ¨ç¤¾ä¼šä¿éšœé¢†å–å¹´é¾„é€‰æ‹©ä»»åŠ¡ä¸­è¿›è¡Œå¯¹æ¯”å®žéªŒï¼Œç ”ç©¶å‘çŽ°è™½ç„¶è¡ŒåŠ¨å¯¼å‘çš„è§£é‡Šèƒ½æ˜¾è‘—æå‡ç”¨æˆ·çš„å®¢è§‚å¿ƒç†æ¨¡åž‹(Mental model)å’Œä»»åŠ¡è¡¨çŽ°ï¼Œä½†ç”¨æˆ·å¯¹å®‰æ…°å‰‚å¼è§£é‡Šå’Œè¡ŒåŠ¨å¯¼å‘è§£é‡Šçš„æ»¡æ„åº¦è¯„ä»·å´å®Œå…¨ç›¸åŒã€‚è¿™ä¸€ç»“æžœè¡¨æ˜Žï¼Œå•çº¯çš„ä¸»è§‚è°ƒæŸ¥æ— æ³•å‡†ç¡®æ•æ‰è§£é‡Šæ˜¯å¦çœŸæ­£å¸®åŠ©ç”¨æˆ·å»ºç«‹äº†æœ‰æ•ˆçš„é¢†åŸŸç†è§£ã€‚å› æ­¤ï¼Œä½œè€…æå‡ºæœªæ¥çš„å¯è§£é‡Šæ€§è¯„ä¼°åº”å°†å®¢è§‚ä»»åŠ¡ç»©æ•ˆæŒ‡æ ‡ä¸Žä¸»è§‚è¯„ä¼°ç›¸ç»“åˆï¼Œä»¥æ›´ç²¾ç¡®åœ°è¡¡é‡è§£é‡Šçš„è´¨é‡ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘æ›´å…·é€æ˜Žåº¦å’Œå¯ä¿¡åº¦çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿæä¾›äº†é‡è¦çš„è¯„ä¼°å‡†åˆ™ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "21 pages, 7 figures, 6 tables. EXTRAAMAS 2025 submission. Preprint version",
      "pdf_url": "https://arxiv.org/pdf/2512.06591v1",
      "published_date": "2025-12-06 23:06:18 UTC",
      "updated_date": "2025-12-06 23:06:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:13:26.461470+00:00"
    },
    {
      "arxiv_id": "2512.06590v1",
      "title": "Towards Efficient Hypergraph and Multi-LLM Agent Recommender Systems",
      "title_zh": "è¿ˆå‘é«˜æ•ˆçš„è¶…å›¾ä¸Žå¤š LLM æ™ºèƒ½ä½“æŽ¨èç³»ç»Ÿ",
      "authors": [
        "Tendai Mukande",
        "Esraa Ali",
        "Annalina Caputo",
        "Ruihai Dong",
        "Noel OConnor"
      ],
      "abstract": "Recommender Systems (RSs) have become the cornerstone of various applications such as e-commerce and social media platforms. The evolution of RSs is paramount in the digital era, in which personalised user experience is tailored to the user's preferences. Large Language Models (LLMs) have sparked a new paradigm - generative retrieval and recommendation. Despite their potential, generative RS methods face issues such as hallucination, which degrades the recommendation performance, and high computational cost in practical scenarios. To address these issues, we introduce HGLMRec, a novel Multi-LLM agent-based RS that incorporates a hypergraph encoder designed to capture complex, multi-behaviour relationships between users and items. The HGLMRec model retrieves only the relevant tokens during inference, reducing computational overhead while enriching the retrieval context. Experimental results show performance improvement by HGLMRec against state-of-the-art baselines at lower computational cost.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼æŽ¨èç³»ç»Ÿ(RSs)ä¸­å­˜åœ¨çš„å¹»è§‰(hallucination)å’Œé«˜è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œæå‡ºäº†HGLMRecï¼Œè¿™æ˜¯ä¸€ç§æ–°åž‹çš„åŸºäºŽMulti-LLM agentçš„æŽ¨èç³»ç»Ÿã€‚HGLMRecæ ¸å¿ƒå¼•å…¥äº†ä¸€ä¸ªè¶…å›¾ç¼–ç å™¨(hypergraph encoder)ï¼Œæ—¨åœ¨ç²¾å‡†æ•æ‰ç”¨æˆ·ä¸Žç‰©å“ä¹‹é—´å¤æ‚çš„ã€å¤šè¡Œä¸º(multi-behaviour)äº¤äº’å…³ç³»ã€‚åœ¨æŽ¨ç†é˜¶æ®µï¼Œè¯¥ç³»ç»Ÿé€šè¿‡ä»…æ£€ç´¢ç›¸å…³ä»¤ç‰Œ(tokens)çš„æ–¹å¼ï¼Œåœ¨æ˜¾è‘—é™ä½Žè®¡ç®—å¼€é”€çš„åŒæ—¶ä¸°å¯Œäº†æ£€ç´¢ä¸Šä¸‹æ–‡ã€‚å®žéªŒç»“æžœè¯æ˜Žï¼ŒHGLMRecåœ¨ä¿æŒæ›´ä½Žè®¡ç®—æˆæœ¬çš„å‰æä¸‹ï¼Œæ€§èƒ½è¡¨çŽ°ä¼˜äºŽç›®å‰æœ€å…ˆè¿›çš„åŸºå‡†æ¨¡åž‹ã€‚è¯¥æ¡†æž¶ä¸ºæž„å»ºé«˜æ•ˆä¸”å…·å¯æ‰©å±•æ€§çš„ç”Ÿæˆå¼æŽ¨èç³»ç»Ÿæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.IR",
      "comment": "8 Pages",
      "pdf_url": "https://arxiv.org/pdf/2512.06590v1",
      "published_date": "2025-12-06 23:04:49 UTC",
      "updated_date": "2025-12-06 23:04:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:13:28.515028+00:00"
    },
    {
      "arxiv_id": "2512.15739v1",
      "title": "Bayesian Modeling for Uncertainty Management in Financial Risk Forecasting and Compliance",
      "title_zh": "é¢å‘é‡‘èžé£Žé™©é¢„æµ‹ä¸Žåˆè§„ä¸­ä¸ç¡®å®šæ€§ç®¡ç†çš„è´å¶æ–¯å»ºæ¨¡",
      "authors": [
        "Sharif Al Mamun",
        "Rakib Hossain",
        "Md. Jobayer Rahman",
        "Malay Kumar Devnath",
        "Farhana Afroz",
        "Lisan Al Amin"
      ],
      "abstract": "A Bayesian analytics framework that precisely quantifies uncertainty offers a significant advance for financial risk management. We develop an integrated approach that consistently enhances the handling of risk in market volatility forecasting, fraud detection, and compliance monitoring. Our probabilistic, interpretable models deliver reliable results: We evaluate the performance of one-day-ahead 95% Value-at-Risk (VaR) forecasts on daily S&P 500 returns, with a training period from 2000 to 2019 and an out-of-sample test period spanning 2020 to 2024. Formal tests of unconditional (Kupiec) and conditional (Christoffersen) coverage reveal that an LSTM baseline achieves near-nominal calibration. In contrast, a GARCH(1,1) model with Student-t innovations underestimates tail risk. Our proposed discount-factor DLM model produces a slightly liberal VaR estimate, with evidence of clustered violations. Bayesian logistic regression improves recall and AUC-ROC for fraud detection, and a hierarchical Beta state-space model provides transparent and adaptive compliance risk assessment. The pipeline is distinguished by precise uncertainty quantification, interpretability, and GPU-accelerated analysis, delivering up to 50x speedup. Remaining challenges include sparse fraud data and proxy compliance labels, but the framework enables actionable risk insights. Future expansion will extend feature sets, explore regime-switching priors, and enhance scalable inference.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºŽè´å¶æ–¯åˆ†æž (Bayesian analytics) çš„æ¡†æž¶ï¼Œæ—¨åœ¨é€šè¿‡ç²¾ç¡®é‡åŒ–ä¸ç¡®å®šæ€§æ¥æå‡é‡‘èžå¸‚åœºæ³¢åŠ¨çŽ‡é¢„æµ‹ã€æ¬ºè¯ˆæ£€æµ‹å’Œåˆè§„ç›‘æµ‹çš„æ•ˆèƒ½ã€‚æ¡†æž¶æ•´åˆäº†å¤šç§æ¦‚çŽ‡å¯è§£é‡Šæ¨¡åž‹ï¼ŒåŒ…æ‹¬ç”¨äºŽé£Žé™©ä»·å€¼ (Value-at-Risk, VaR) é¢„æµ‹çš„æŠ˜æ‰£å› å­åŠ¨æ€çº¿æ€§æ¨¡åž‹ (discount-factor DLM)ã€ç”¨äºŽæ¬ºè¯ˆæ£€æµ‹çš„è´å¶æ–¯é€»è¾‘å›žå½’ (Bayesian logistic regression) ä»¥åŠç”¨äºŽåˆè§„é£Žé™©è¯„ä¼°çš„å±‚æ¬¡åŒ– Beta çŠ¶æ€ç©ºé—´æ¨¡åž‹ (hierarchical Beta state-space model)ã€‚é€šè¿‡å¯¹æ ‡æ™® 500 æŒ‡æ•°å›žæŠ¥çŽ‡çš„æµ‹è¯•ï¼Œè¯¥æ¡†æž¶åœ¨ä¸Ž LSTM å’Œ GARCH(1,1) ç­‰åŸºçº¿æ¨¡åž‹çš„å¯¹æ¯”ä¸­å±•ç¤ºäº†å…¶åœ¨å¤„ç†å°¾éƒ¨é£Žé™©æ–¹é¢çš„ç‰¹æ€§ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ¡ˆæ˜¾è‘—æé«˜äº†æ¬ºè¯ˆæ£€æµ‹çš„å¬å›žçŽ‡ (recall) å’Œ AUC-ROC æŒ‡æ ‡ï¼Œå¹¶èƒ½ä¸ºåˆè§„æ€§æä¾›é€æ˜Žä¸”è‡ªé€‚åº”çš„è¯„ä¼°æ‰‹æ®µã€‚æ­¤å¤–ï¼Œæ•´ä¸ªå·¥ä½œæµæ”¯æŒ GPU åŠ é€Ÿåˆ†æžï¼Œå®žçŽ°äº†é«˜è¾¾ 50 å€çš„è®¡ç®—æé€Ÿï¼Œç¡®ä¿äº†åˆ†æžçš„å®žæ—¶æ€§ã€‚å°½ç®¡é¢ä¸´æ•°æ®ç¨€ç–ç­‰æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶ä¸ºé‡‘èžé£Žé™©ç®¡ç†æä¾›äº†å¯æ“ä½œçš„è§è§£ï¼Œå¹¶ä¸ºæœªæ¥æŽ¢ç´¢æœºåˆ¶åˆ‡æ¢å…ˆéªŒ (regime-switching priors) å’Œå¯æ‰©å±•æŽ¨ç†æŠ€æœ¯å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "q-fin.RM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-fin.RM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15739v1",
      "published_date": "2025-12-06 23:00:19 UTC",
      "updated_date": "2025-12-06 23:00:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:13:37.947222+00:00"
    },
    {
      "arxiv_id": "2512.06582v1",
      "title": "QL-LSTM: A Parameter-Efficient LSTM for Stable Long-Sequence Modeling",
      "title_zh": "QL-LSTMï¼šé¢å‘ç¨³å®šé•¿åºåˆ—å»ºæ¨¡çš„å‚æ•°é«˜æ•ˆåž‹ LSTM",
      "authors": [
        "Isaac Kofi Nti"
      ],
      "abstract": "Recurrent neural architectures such as LSTM and GRU remain widely used in sequence modeling, but they continue to face two core limitations: redundant gate-specific parameters and reduced ability to retain information across long temporal distances. This paper introduces the Quantum-Leap LSTM (QL-LSTM), a recurrent architecture designed to address both challenges through two independent components. The Parameter-Shared Unified Gating mechanism replaces all gate-specific transformations with a single shared weight matrix, reducing parameters by approximately 48 percent while preserving full gating behavior. The Hierarchical Gated Recurrence with Additive Skip Connections component adds a multiplication-free pathway that improves long-range information flow and reduces forget-gate degradation. We evaluate QL-LSTM on sentiment classification using the IMDB dataset with extended document lengths, comparing it to LSTM, GRU, and BiLSTM reference models. QL-LSTM achieves competitive accuracy while using substantially fewer parameters. Although the PSUG and HGR-ASC components are more efficient per time step, the current prototype remains limited by the inherent sequential nature of recurrent models and therefore does not yet yield wall-clock speed improvements without further kernel-level optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Quantum-Leap LSTM (QL-LSTM)ï¼Œæ—¨åœ¨è§£å†³ LSTM å’Œ GRU åœ¨é•¿åºåˆ—å»ºæ¨¡ä¸­é¢ä¸´çš„å‚æ•°å†—ä½™åŠé•¿ç¨‹ä¿¡æ¯ä¿ç•™èƒ½åŠ›å—é™çš„æ ¸å¿ƒé—®é¢˜ã€‚QL-LSTM å¼•å…¥äº†å‚æ•°å…±äº«ç»Ÿä¸€é—¨æŽ§æœºåˆ¶ (Parameter-Shared Unified Gating, PSUG)ï¼Œé€šè¿‡å•ä¸€å…±äº«æƒé‡çŸ©é˜µæ›¿ä»£ä¼ ç»Ÿé—¨æŽ§å˜æ¢ï¼Œä½¿å‚æ•°é‡å‡å°‘äº†çº¦ 48%ã€‚åŒæ—¶ï¼Œè¯¥æž¶æž„é‡‡ç”¨å¸¦æœ‰åŠ æ€§è·³è·ƒè¿žæŽ¥çš„å±‚æ¬¡åŒ–é—¨æŽ§å¾ªçŽ¯ (Hierarchical Gated Recurrence with Additive Skip Connections, HGR-ASC) æ¨¡å—ï¼Œé€šè¿‡æ— ä¹˜æ³•è·¯å¾„æ”¹å–„ä¿¡æ¯æµå¹¶ç¼“è§£é—å¿˜é—¨é€€åŒ–ã€‚åœ¨ IMDB é•¿æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡çš„è¯„ä¼°ä¸­ï¼ŒQL-LSTM åœ¨å‚æ•°é‡æ˜¾è‘—å‡å°‘çš„æƒ…å†µä¸‹è¾¾åˆ°äº†ä¸Ž LSTM å’Œ GRU ç›¸å½“çš„å‡†ç¡®çŽ‡ã€‚å°½ç®¡ç›®å‰ç”±äºŽå¾ªçŽ¯æ¨¡åž‹çš„é¡ºåºæœ¬è´¨é™åˆ¶äº†å®žé™…è¿è¡Œé€Ÿåº¦ (wall-clock speed) çš„æå‡ï¼Œä½†è¯¥ç ”ç©¶ä¸ºæž„å»ºé«˜æ•ˆä¸”ç¨³å®šçš„å¾ªçŽ¯ç¥žç»æž¶æž„æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06582v1",
      "published_date": "2025-12-06 22:29:19 UTC",
      "updated_date": "2025-12-06 22:29:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:14:01.090956+00:00"
    },
    {
      "arxiv_id": "2512.15738v1",
      "title": "Hybrid Quantum-Classical Ensemble Learning for S\\&P 500 Directional Prediction",
      "title_zh": "ç”¨äºŽ S&P 500 æ¶¨è·Œé¢„æµ‹çš„é‡å­-ç»å…¸æ··åˆé›†æˆå­¦ä¹ ",
      "authors": [
        "Abraham Itzhak Weinberg"
      ],
      "abstract": "Financial market prediction is a challenging application of machine learning, where even small improvements in directional accuracy can yield substantial value. Most models struggle to exceed 55--57\\% accuracy due to high noise, non-stationarity, and market efficiency. We introduce a hybrid ensemble framework combining quantum sentiment analysis, Decision Transformer architecture, and strategic model selection, achieving 60.14\\% directional accuracy on S\\&P 500 prediction, a 3.10\\% improvement over individual models.\n  Our framework addresses three limitations of prior approaches. First, architecture diversity dominates dataset diversity: combining different learning algorithms (LSTM, Decision Transformer, XGBoost, Random Forest, Logistic Regression) on the same data outperforms training identical architectures on multiple datasets (60.14\\% vs.\\ 52.80\\%), confirmed by correlation analysis ($r>0.6$ among same-architecture models). Second, a 4-qubit variational quantum circuit enhances sentiment analysis, providing +0.8\\% to +1.5\\% gains per model. Third, smart filtering excludes weak predictors (accuracy $<52\\%$), improving ensemble performance (Top-7 models: 60.14\\% vs.\\ all 35 models: 51.2\\%).\n  We evaluate on 2020--2023 market data across seven instruments, covering diverse regimes including the COVID-19 crash and inflation-driven correction. McNemar's test confirms statistical significance ($p<0.05$). Preliminary backtesting with confidence-based filtering (6+ model consensus) yields a Sharpe ratio of 1.2 versus buy-and-hold's 0.8, demonstrating practical trading potential.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆé‡å­æƒ…æ„Ÿåˆ†æžã€Decision Transformeræž¶æž„å’Œç­–ç•¥æ€§æ¨¡åž‹é€‰æ‹©çš„æ··åˆé›†æˆå­¦ä¹ æ¡†æž¶ï¼Œæ—¨åœ¨æå‡S\\&P 500æŒ‡æ•°çš„è¶‹åŠ¿é¢„æµ‹å‡†ç¡®çŽ‡ã€‚è¯¥æ¡†æž¶åˆ©ç”¨4ä½é‡å­å˜åˆ†ç”µè·¯(4-qubit variational quantum circuit)å¢žå¼ºæƒ…æ„Ÿåˆ†æžï¼Œä¸ºå„æ¨¡åž‹å¸¦æ¥äº†0.8\\%è‡³1.5\\%çš„æ€§èƒ½å¢žç›Šã€‚ç ”ç©¶é€šè¿‡å¯¹æ¯”å‘çŽ°ï¼Œä¸åŒå­¦ä¹ ç®—æ³•çš„æž¶æž„å¤šæ ·æ€§åœ¨æå‡é¢„æµ‹æ•ˆæžœä¸Šä¼˜äºŽå•çº¯çš„æ•°æ®é›†å¤šæ ·æ€§ï¼Œä¸”é€šè¿‡æ™ºèƒ½è¿‡æ»¤å¼±é¢„æµ‹å™¨èƒ½æ˜¾è‘—ä¼˜åŒ–é›†æˆè¡¨çŽ°ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥ç³»ç»Ÿå®žçŽ°äº†60.14\\%çš„é¢„æµ‹å‡†ç¡®çŽ‡ï¼Œè¾ƒå•ä¸€æ¨¡åž‹æå‡äº†3.10\\%ï¼Œå¹¶é€šè¿‡äº†McNemaræ£€éªŒçš„ç»Ÿè®¡æ˜¾è‘—æ€§éªŒè¯ã€‚åœ¨2020è‡³2023å¹´è·¨è¶Šå¤šç§å¸‚åœºå‘¨æœŸçš„å›žæµ‹ä¸­ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†1.2çš„å¤æ™®æ¯”çŽ‡(Sharpe ratio)ï¼Œæ˜¾è‘—ä¼˜äºŽä¹°å…¥æŒæœ‰ç­–ç•¥ï¼Œè¯æ˜Žäº†å…¶åœ¨å®žé™…é‡åŒ–äº¤æ˜“ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-fin.ST"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15738v1",
      "published_date": "2025-12-06 22:22:09 UTC",
      "updated_date": "2025-12-06 22:22:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:13:42.441023+00:00"
    },
    {
      "arxiv_id": "2512.06573v1",
      "title": "The Effect of Belief Boxes and Open-mindedness on Persuasion",
      "title_zh": "ä¿¡å¿µæ¡†ä¸Žæ€ç»´å¼€æ”¾æ€§å¯¹è¯´æœçš„å½±å“",
      "authors": [
        "Onur Bilgin",
        "Abdullah As Sami",
        "Sriram Sai Vujjini",
        "John Licato"
      ],
      "abstract": "As multi-agent systems are increasingly utilized for reasoning and decision-making applications, there is a greater need for LLM-based agents to have something resembling propositional beliefs. One simple method for doing so is to include statements describing beliefs maintained in the prompt space (in what we'll call their belief boxes). But when agents have such statements in belief boxes, how does it actually affect their behaviors and dispositions towards those beliefs? And does it significantly affect agents' ability to be persuasive in multi-agent scenarios? Likewise, if the agents are given instructions to be open-minded, how does that affect their behaviors? We explore these and related questions in a series of experiments. Our findings confirm that instructing agents to be open-minded affects how amenable they are to belief change. We show that incorporating belief statements and their strengths influences an agent's resistance to (and persuasiveness against) opposing viewpoints. Furthermore, it affects the likelihood of belief change, particularly when the agent is outnumbered in a debate by opposing viewpoints, i.e., peer pressure scenarios. The results demonstrate the feasibility and validity of the belief box technique in reasoning and decision-making tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æŽ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼Œé€šè¿‡æç¤ºè¯ç©ºé—´ä¸­çš„â€œbelief boxesâ€æ¥èµ‹äºˆæ™ºèƒ½ä½“å‘½é¢˜ä¿¡å¿µï¼ˆpropositional beliefsï¼‰çš„æ–¹æ³•ï¼Œå¹¶åˆ†æžäº†è¯¥æŠ€æœ¯åŠâ€œopen-mindednessâ€æŒ‡ä»¤å¯¹æ™ºèƒ½ä½“è¡Œä¸ºçš„å½±å“ã€‚å®žéªŒé‡ç‚¹è€ƒå¯Ÿäº†è¿™äº›å› ç´ å¦‚ä½•å½±å“æ™ºèƒ½ä½“åœ¨å¤šæ™ºèƒ½ä½“åœºæ™¯ä¸­çš„è¯´æœåŠ›ã€å¯¹ä¿¡å¿µçš„åšæŒç¨‹åº¦ä»¥åŠæ”¹å˜ä¿¡å¿µçš„å€¾å‘ã€‚ç ”ç©¶ç»“æžœè¯å®žï¼Œè¦æ±‚æ™ºèƒ½ä½“ä¿æŒâ€œopen-mindednessâ€çš„æŒ‡ä»¤èƒ½å¤Ÿæ˜¾è‘—æé«˜å…¶æŽ¥å—ä¿¡å¿µæ”¹å˜çš„æ„æ„¿ã€‚åœ¨æç¤ºè¯ä¸­åŠ å…¥ä¿¡å¿µé™ˆè¿°åŠå…¶å¼ºåº¦ï¼Œä¼šç›´æŽ¥å¢žå¼ºæ™ºèƒ½ä½“æŠµå¾¡å¯¹ç«‹è§‚ç‚¹æˆ–è¯´æœä»–äººçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘çŽ°â€œbelief boxesâ€æ˜¾è‘—å½±å“äº†æ™ºèƒ½ä½“åœ¨è¢«å¯¹ç«‹è§‚ç‚¹åŒ…å›´çš„åŒä¼´åŽ‹åŠ›ï¼ˆpeer pressureï¼‰åœºæ™¯ä¸‹æ”¹å˜ä¿¡å¿µçš„æ¦‚çŽ‡ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥å·¥ä½œéªŒè¯äº†â€œbelief boxesâ€æŠ€æœ¯åœ¨æŽ¨ç†å’Œå†³ç­–ä»»åŠ¡ä¸­çš„å¯è¡Œæ€§ä¸Žæœ‰æ•ˆæ€§ï¼Œä¸ºæå‡è‡ªä¸»æ™ºèƒ½ä½“çš„è®¤çŸ¥æ¨¡æ‹Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at the 18th International Conference on Agents and Artificial Intelligence (ICAART 2026), Marbella, Spain",
      "pdf_url": "https://arxiv.org/pdf/2512.06573v1",
      "published_date": "2025-12-06 21:31:51 UTC",
      "updated_date": "2025-12-06 21:31:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:14:08.011107+00:00"
    },
    {
      "arxiv_id": "2512.06563v1",
      "title": "Deep Manifold Part 2: Neural Network Mathematics",
      "title_zh": "æ·±åº¦æµå½¢ï¼ˆç¬¬äºŒéƒ¨åˆ†ï¼‰ï¼šç¥žç»ç½‘ç»œæ•°å­¦",
      "authors": [
        "Max Y. Ma",
        "Gen-Hua Shi"
      ],
      "abstract": "This work develops the global equations of neural networks through stacked piecewise manifolds, fixed-point theory, and boundary-conditioned iteration. Once fixed coordinates and operators are removed, a neural network appears as a learnable numerical computation shaped by manifold complexity, high-order nonlinearity, and boundary conditions. Real-world data impose strong data complexity, near-infinite scope, scale, and minibatch fragmentation, while training dynamics produce learning complexity through shifting node covers, curvature accumulation, and the rise and decay of plasticity. These forces constrain learnability and explain why capability emerges only when fixed-point regions stabilize. Neural networks do not begin with fixed points; they construct them through residual-driven iteration. This perspective clarifies the limits of monolithic models under geometric and data-induced plasticity and motivates architectures and federated systems that distribute manifold complexity across many elastic models, forming a coherent world-modeling framework grounded in geometry, algebra, fixed points, and real-data complexity.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å †å åˆ†æ®µæµå½¢ (stacked piecewise manifolds)ã€ä¸åŠ¨ç‚¹ç†è®º (fixed-point theory) å’Œè¾¹ç•Œæ¡ä»¶è¿­ä»£ (boundary-conditioned iteration) æž„å»ºäº†ç¥žç»ç½‘ç»œçš„å…¨å±€æ•°å­¦æ–¹ç¨‹ã€‚ç ”ç©¶å°†ç¥žç»ç½‘ç»œå®šä¹‰ä¸ºä¸€ç§å—æµå½¢å¤æ‚åº¦ (manifold complexity)ã€é«˜é˜¶éžçº¿æ€§ (high-order nonlinearity) å’Œè¾¹ç•Œæ¡ä»¶å¡‘é€ çš„å¯å­¦ä¹ æ•°å€¼è®¡ç®—è¿‡ç¨‹ã€‚è®ºæ–‡è¯¦ç»†æŽ¢è®¨äº†çŽ°å®žæ•°æ®å¤æ‚åº¦å’Œè®­ç»ƒåŠ¨æ€å¦‚ä½•é€šè¿‡æ›²çŽ‡ç´¯ç§¯ (curvature accumulation) å’Œå¡‘æ€§ (plasticity) çš„æ¼”å˜æ¥çº¦æŸæ¨¡åž‹çš„å¯å­¦ä¹ æ€§ã€‚ä½œè€…æŒ‡å‡ºç¥žç»ç½‘ç»œå¹¶éžé¢„è®¾äº†ä¸åŠ¨ç‚¹ï¼Œè€Œæ˜¯é€šè¿‡æ®‹å·®é©±åŠ¨è¿­ä»£ (residual-driven iteration) é€æ­¥æž„å»ºä¸åŠ¨ç‚¹ï¼Œä¸”æ¨¡åž‹èƒ½åŠ›çš„æ¶ŒçŽ°æºäºŽä¸åŠ¨ç‚¹åŒºåŸŸçš„ç¨³å®šã€‚è¿™ä¸€ç†è®ºè§†è§’è§£é‡Šäº†å•ä½“æ¨¡åž‹ (monolithic models) çš„æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶æå€¡å°†æµå½¢å¤æ‚åº¦åˆ†å¸ƒåˆ°å¤šä¸ªå¼¹æ€§æ¨¡åž‹ (elastic models) æˆ–è”é‚¦ç³»ç»Ÿ (federated systems) ä¸­ã€‚è¯¥æ¡†æž¶ä¸ºå»ºç«‹åœ¨å‡ ä½•ã€ä»£æ•°å’Œä¸åŠ¨ç‚¹ç†è®ºåŸºç¡€ä¸Šçš„ä¸–ç•Œå»ºæ¨¡æä¾›äº†ä¸€ä¸ªè¿žè´¯çš„æ•°å­¦è§£é‡Šã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06563v1",
      "published_date": "2025-12-06 20:44:24 UTC",
      "updated_date": "2025-12-06 20:44:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:14:00.526850+00:00"
    },
    {
      "arxiv_id": "2512.06562v1",
      "title": "SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities",
      "title_zh": "SUGARï¼šå¤šèº«ä»½ç”Ÿæˆå¼é—å¿˜çš„æ›´ä¼˜æ–¹æ¡ˆ",
      "authors": [
        "Dung Thuy Nguyen",
        "Quang Nguyen",
        "Preston K. Robinette",
        "Eli Jiang",
        "Taylor T. Johnson",
        "Kevin Leach"
      ],
      "abstract": "Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a framework for scalable generative unlearning that enables the removal of many identities (simultaneously or sequentially) without retraining the entire model. Rather than projecting unwanted identities to unrealistic outputs or relying on static template faces, SUGAR learns a personalized surrogate latent for each identity, diverting reconstructions to visually coherent alternatives while preserving the model's quality and diversity. We further introduce a continual utility preservation objective that guards against degradation as more identities are forgotten. SUGAR achieves state-of-the-art performance in removing up to 200 identities, while delivering up to a 700% improvement in retention utility compared to existing baselines. Our code is publicly available at https://github.com/judydnguyen/SUGAR-Generative-Unlearn.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SUGARæ¡†æž¶ï¼Œè¿™æ˜¯ä¸€ç§é¢å‘å¤§è§„æ¨¡èº«ä»½ç§»é™¤çš„å¯æ‰©å±•ç”Ÿæˆå¼é—å¿˜(Generative Unlearning)æŠ€æœ¯ï¼Œæ—¨åœ¨åº”å¯¹3Dæ„ŸçŸ¥ç”Ÿæˆæ¨¡åž‹(3D-aware generative models)ä¸­çš„ç”¨æˆ·éšç§ä¸ŽæŽˆæƒæŒ‘æˆ˜ã€‚SUGARå…è®¸åœ¨ä¸é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡åž‹çš„æƒ…å†µä¸‹ï¼ŒåŒæ—¶æˆ–åºåˆ—åŒ–åœ°ç§»é™¤å¤šä¸ªç‰¹å®šèº«ä»½ã€‚å…¶æ ¸å¿ƒæœºåˆ¶æ˜¯ä¸ºæ¯ä¸ªèº«ä»½å­¦ä¹ ä¸€ä¸ªä¸ªæ€§åŒ–çš„æ›¿ä»£æ½œå˜é‡(Personalized Surrogate Latent)ï¼Œå°†åŽŸæœ¬çš„èº«ä»½é‡å»ºå¼•å¯¼è‡³è§†è§‰ä¸Šè¿žè´¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä»Žè€Œåœ¨å®žçŽ°é—å¿˜çš„åŒæ—¶ç¡®ä¿æ¨¡åž‹è¾“å‡ºçš„è´¨é‡ä¸Žå¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†æŒç»­æ•ˆç”¨ä¿æŒç›®æ ‡(Continual Utility Preservation)ï¼Œä»¥é˜²æ­¢æ¨¡åž‹åœ¨è¿žç»­é—å¿˜è¿‡ç¨‹ä¸­å‘ç”Ÿæ€§èƒ½é€€åŒ–ã€‚å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒSUGARåœ¨å¤„ç†å¤šè¾¾200ä¸ªèº«ä»½çš„é—å¿˜ä»»åŠ¡æ—¶è¡¨çŽ°ä¼˜å¼‚ï¼Œå…¶ä¿ç•™æ•ˆç”¨ç›¸æ¯”çŽ°æœ‰åŸºå‡†æ¨¡åž‹æå‡äº†æœ€é«˜700%ã€‚è¯¥ç ”ç©¶ä¸ºå®žçŽ°å¤§è§„æ¨¡ä¸”é«˜è´¨é‡çš„ç”Ÿæˆå¼é—å¿˜æä¾›äº†å…¨æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06562v1",
      "published_date": "2025-12-06 20:42:38 UTC",
      "updated_date": "2025-12-06 20:42:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:14:33.875066+00:00"
    },
    {
      "arxiv_id": "2512.06556v1",
      "title": "Securing the Model Context Protocol: Defending LLMs Against Tool Poisoning and Adversarial Attacks",
      "title_zh": "ä¿éšœ Model Context Protocol å®‰å…¨ï¼šé˜²å¾¡å¤§è¯­è¨€æ¨¡åž‹å…å—å·¥å…·æŠ•æ¯’ä¸Žå¯¹æŠ—æ€§æ”»å‡»",
      "authors": [
        "Saeid Jamshidi",
        "Kawser Wazed Nafi",
        "Arghavan Moradi Dakhel",
        "Negar Shahabi",
        "Foutse Khomh",
        "Naser Ezzati-Jivan"
      ],
      "abstract": "The Model Context Protocol (MCP) enables Large Language Models to integrate external tools through structured descriptors, increasing autonomy in decision-making, task execution, and multi-agent workflows. However, this autonomy creates a largely overlooked security gap. Existing defenses focus on prompt-injection attacks and fail to address threats embedded in tool metadata, leaving MCP-based systems exposed to semantic manipulation. This work analyzes three classes of semantic attacks on MCP-integrated systems: (1) Tool Poisoning, where adversarial instructions are hidden in tool descriptors; (2) Shadowing, where trusted tools are indirectly compromised through contaminated shared context; and (3) Rug Pulls, where descriptors are altered after approval to subvert behavior. To counter these threats, we introduce a layered security framework with three components: RSA-based manifest signing to enforce descriptor integrity, LLM-on-LLM semantic vetting to detect suspicious tool definitions, and lightweight heuristic guardrails that block anomalous tool behavior at runtime. Through evaluation of GPT-4, DeepSeek, and Llama-3.5 across eight prompting strategies, we find that security performance varies widely by model architecture and reasoning method. GPT-4 blocks about 71 percent of unsafe tool calls, balancing latency and safety. DeepSeek shows the highest resilience to Shadowing attacks but with greater latency, while Llama-3.5 is fastest but least robust. Our results show that the proposed framework reduces unsafe tool invocation rates without model fine-tuning or internal modification.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨¡åž‹ä¸Šä¸‹æ–‡åè®®(Model Context Protocol, MCP)åœ¨é›†æˆå¤–éƒ¨å·¥å…·æ—¶å­˜åœ¨çš„è¯­ä¹‰æ“çºµæ¼æ´žï¼Œæ·±å…¥åˆ†æžäº†å·¥å…·ä¸­æ¯’(Tool Poisoning)ã€é˜´å½±æ”»å‡»(Shadowing)å’Œåœ°æ¯¯å¼æ‹‰å–(Rug Pulls)ä¸‰ç±»å®‰å…¨å¨èƒã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸€å¥—åˆ†å±‚å®‰å…¨æ¡†æž¶ï¼Œé€šè¿‡åŸºäºŽRSAçš„æ¸…å•ç­¾å(manifest signing)ç¡®ä¿æè¿°ç¬¦å®Œæ•´æ€§ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡åž‹å¯¹å¤§è¯­è¨€æ¨¡åž‹(LLM-on-LLM)è¿›è¡Œè¯­ä¹‰å®¡æŸ¥ï¼Œå¹¶ç»“åˆè½»é‡çº§å¯å‘å¼æŠ¤æ (heuristic guardrails)åœ¨è¿è¡Œæ—¶é˜»æ–­å¼‚å¸¸è¡Œä¸ºã€‚å¯¹GPT-4ã€DeepSeekå’ŒLlama-3.5çš„è¯„ä¼°è¡¨æ˜Žï¼Œä¸åŒæž¶æž„åœ¨é¢å¯¹æ”»å‡»æ—¶çš„é˜²å¾¡æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œå…¶ä¸­GPT-4èƒ½æœ‰æ•ˆæ‹¦æˆªçº¦71%çš„ä¸å®‰å…¨å·¥å…·è°ƒç”¨ï¼Œè€ŒDeepSeekå¯¹é˜´å½±æ”»å‡»è¡¨çŽ°å‡ºæœ€å¼ºéŸ§æ€§ã€‚å®žéªŒç»“æžœè¯å®žï¼Œè¯¥æ¡†æž¶åœ¨æ— éœ€å¯¹æ¨¡åž‹è¿›è¡Œå¾®è°ƒæˆ–å†…éƒ¨ä¿®æ”¹çš„å‰æä¸‹ï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½Žä¸å®‰å…¨å·¥å…·çš„è°ƒç”¨çŽ‡ï¼Œä¸ºæž„å»ºæ›´å®‰å…¨ã€æ›´å…·é²æ£’æ€§çš„MCPé›†æˆç³»ç»Ÿæä¾›äº†å…³é”®æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06556v1",
      "published_date": "2025-12-06 20:07:58 UTC",
      "updated_date": "2025-12-06 20:07:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:15:19.263307+00:00"
    },
    {
      "arxiv_id": "2512.06555v1",
      "title": "BEACON: A Unified Behavioral-Tactical Framework for Explainable Cybercrime Analysis with Large Language Models",
      "title_zh": "BEACONï¼šåŸºäºŽå¤§è¯­è¨€æ¨¡åž‹çš„ç»Ÿä¸€è¡Œä¸º-æˆ˜æœ¯å¯è§£é‡Šç½‘ç»œçŠ¯ç½ªåˆ†æžæ¡†æž¶",
      "authors": [
        "Arush Sachdeva",
        "Rajendraprasad Saravanan",
        "Gargi Sarkar",
        "Kavita Vemuri",
        "Sandeep Kumar Shukla"
      ],
      "abstract": "Cybercrime increasingly exploits human cognitive biases in addition to technical vulnerabilities, yet most existing analytical frameworks focus primarily on operational aspects and overlook psychological manipulation. This paper proposes BEACON, a unified dual-dimension framework that integrates behavioral psychology with the tactical lifecycle of cybercrime to enable structured, interpretable, and scalable analysis of cybercrime. We formalize six psychologically grounded manipulation categories derived from Prospect Theory and Cialdini's principles of persuasion, alongside a fourteen-stage cybercrime tactical lifecycle spanning reconnaissance to final impact. A single large language model is fine-tuned using parameter-efficient learning to perform joint multi-label classification across both psychological and tactical dimensions while simultaneously generating human-interpretable explanations. Experiments conducted on a curated dataset of real-world and synthetically augmented cybercrime narratives demonstrate a 20 percent improvement in overall classification accuracy over the base model, along with substantial gains in reasoning quality measured using ROUGE and BERTScore. The proposed system enables automated decomposition of unstructured victim narratives into structured behavioral and operational intelligence, supporting improved cybercrime investigation, case linkage, and proactive scam detection.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BEACONï¼Œä¸€ç§é’ˆå¯¹å¯è§£é‡Šç½‘ç»œçŠ¯ç½ªåˆ†æžçš„ç»Ÿä¸€åŒç»´åº¦æ¡†æž¶ï¼Œæ—¨åœ¨æ•´åˆè¡Œä¸ºå¿ƒç†å­¦ä¸Žç½‘ç»œçŠ¯ç½ªçš„æˆ˜æœ¯ç”Ÿå‘½å‘¨æœŸã€‚é’ˆå¯¹çŽ°æœ‰æ¡†æž¶åé‡æŠ€æœ¯å±‚é¢è€Œå¿½è§†å¿ƒç†æ“çºµçš„é—®é¢˜ï¼Œè¯¥æ¡†æž¶å½¢å¼åŒ–äº†åŸºäºŽ Prospect Theory å’Œ Cialdini's principles of persuasion çš„å…­ç±»å¿ƒç†æ“çºµèŒƒç•´ï¼Œå¹¶ç»“åˆäº†æ¶µç›–ä»Žä¾¦å¯Ÿåˆ°æœ€ç»ˆå½±å“çš„ 14 é˜¶æ®µç½‘ç»œçŠ¯ç½ªæˆ˜æœ¯ç”Ÿå‘½å‘¨æœŸã€‚ç ”ç©¶é€šè¿‡å‚æ•°é«˜æ•ˆå­¦ä¹  (parameter-efficient learning) å¾®è°ƒå¤§è¯­è¨€æ¨¡åž‹ (LLMs)ï¼Œå®žçŽ°è·¨å¿ƒç†ä¸Žæˆ˜æœ¯ç»´åº¦çš„è”åˆå¤šæ ‡ç­¾åˆ†ç±»ï¼Œå¹¶åŒæ­¥ç”Ÿæˆä¾›äººç±»ç†è§£çš„è§£é‡Šã€‚å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒBEACON åœ¨æ•´ä½“åˆ†ç±»å‡†ç¡®çŽ‡ä¸Šæ¯”åŸºç¡€æ¨¡åž‹æå‡äº† 20%ï¼Œä¸”åœ¨ ROUGE å’Œ BERTScore ç­‰æŽ¨ç†è´¨é‡æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿå°†éžç»“æž„åŒ–çš„å—å®³è€…å™è¿°è‡ªåŠ¨åŒ–åˆ†è§£ä¸ºç»“æž„åŒ–çš„è¡Œä¸ºå’Œä½œæˆ˜æƒ…æŠ¥ï¼Œä¸ºç½‘ç»œçŠ¯ç½ªè°ƒæŸ¥ã€æ¡ˆä»¶å…³è”ä»¥åŠä¸»åŠ¨è¯ˆéª—æ£€æµ‹æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06555v1",
      "published_date": "2025-12-06 19:59:24 UTC",
      "updated_date": "2025-12-06 19:59:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:14:49.529994+00:00"
    },
    {
      "arxiv_id": "2512.06547v2",
      "title": "A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation",
      "title_zh": "A-3POï¼šåˆ©ç”¨é™ˆæ—§æ€§æ„ŸçŸ¥çš„è¿‘ç«¯ç­–ç•¥è¿‘ä¼¼åŠ é€Ÿå¼‚æ­¥å¤§è¯­è¨€æ¨¡åž‹è®­ç»ƒ",
      "authors": [
        "Xiaocan Li",
        "Shiliang Wu",
        "Zheng Shen"
      ],
      "abstract": "Decoupled PPO has been a successful reinforcement learning (RL) algorithm to deal with the high data staleness under the asynchronous RL setting. Decoupled loss used in decoupled PPO improves coupled-loss style of algorithms' (e.g., standard PPO, GRPO) learning stability by introducing a proximal policy to decouple the off-policy correction (importance weight) from the policy update constraint (trust region). However, the proximal policy requires an extra forward pass through the model at each training step, creating a computational overhead for large language models training. We observe that since the proximal policy only serves as a trust region anchor between the behavior and target policies, we can approximate it through simple interpolation without explicit computation. We call this approach A-3PO (APproximated Proximal Policy Optimization). A-3PO eliminates this overhead, accelerating training by 1.8x speedup while maintaining comparable performance. Code \\& off-the-shelf example are available at: https://github.com/inclusionAI/AReaL/blob/main/docs/algorithms/prox_approx.md",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼‚æ­¥å¼ºåŒ–å­¦ä¹ (RL)çŽ¯å¢ƒä¸‹å¤§è¯­è¨€æ¨¡åž‹(LLM)è®­ç»ƒä¸­å­˜åœ¨çš„é«˜æ•°æ®é™ˆæ—§æ€§é—®é¢˜ï¼Œæå‡ºäº†A-3POç®—æ³•ã€‚è™½ç„¶ä¼ ç»Ÿçš„è§£è€¦PPO(Decoupled PPO)é€šè¿‡å¼•å…¥è¿‘ç«¯ç­–ç•¥(proximal policy)æ”¹å–„äº†å­¦ä¹ ç¨³å®šæ€§ï¼Œä½†è¯¥ç­–ç•¥åœ¨æ¯æ­¥è®­ç»ƒä¸­éƒ½éœ€è¦é¢å¤–çš„æ¨¡åž‹å‰å‘ä¼ æ’­ï¼Œç»™è®­ç»ƒè¿‡ç¨‹å¸¦æ¥äº†å·¨å¤§çš„è®¡ç®—å¼€é”€ã€‚ç ”ç©¶äººå‘˜å‘çŽ°ï¼Œè¿‘ç«¯ç­–ç•¥æœ¬è´¨ä¸Šæ˜¯ä½œä¸ºè¡Œä¸ºç­–ç•¥ä¸Žç›®æ ‡ç­–ç•¥ä¹‹é—´çš„ä¿¡ä»»åŒºåŸŸé”šç‚¹ï¼Œå› æ­¤å¯ä»¥é€šè¿‡ç®€å•çš„æ’å€¼(interpolation)è¿›è¡Œè¿‘ä¼¼ï¼Œè€Œæ— éœ€æ˜¾å¼è®¡ç®—ã€‚è¿™ç§åä¸ºA-3POçš„è¿‘ä¼¼è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–æ–¹æ³•æˆåŠŸæ¶ˆé™¤äº†é¢å¤–çš„è®¡ç®—è´Ÿæ‹…ã€‚å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒA-3POåœ¨ä¿æŒä¸ŽåŸºçº¿æ¨¡åž‹ç›¸å½“æ€§èƒ½çš„å‰æä¸‹ï¼Œå®žçŽ°äº†1.8å€çš„è®­ç»ƒåŠ é€Ÿï¼Œä¸ºé«˜æ•ˆçš„å¼‚æ­¥å¤§è¯­è¨€æ¨¡åž‹è®­ç»ƒæä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06547v2",
      "published_date": "2025-12-06 19:37:39 UTC",
      "updated_date": "2026-01-09 23:46:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:14:50.042728+00:00"
    },
    {
      "arxiv_id": "2512.06533v1",
      "title": "Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning",
      "title_zh": "è¶…è¶Š Token çº§ç›‘ç£ï¼šåˆ©ç”¨å¼ºåŒ–å­¦ä¹ é‡Šæ”¾åŸºäºŽè§£ç çš„å›žå½’æ½œåŠ›",
      "authors": [
        "Ming Chen",
        "Sheng Tang",
        "Rong-Xi Tan",
        "Ziniu Li",
        "Jiacheng Chen",
        "Ke Xue",
        "Chao Qian"
      ],
      "abstract": "Decoding-based regression, which reformulates regression as a sequence generation task, has emerged as a promising paradigm of applying large language models for numerical prediction. However, its progress is hindered by the misalignment between discrete token-level objectives (e.g., cross-entropy) and continuous numerical values. Existing approaches relying on token-level constraints often fail to capture the global magnitude of the target value, limiting their precision and generalization. In this paper, we propose to unlock the potential of decoding-based regression via Reinforcement Learning (RL). We formulate the generation process as a Markov Decision Process, utilizing sequence-level rewards to enforce global numerical coherence. Extensive experiments on tabular regression and code metric regression demonstrate that our method (specifically with ReMax and GRPO) consistently outperforms both state-of-the-art token-level baselines and traditional regression heads, showing the superiority of introducing sequence-level signals. Our analysis further reveals that RL significantly enhances sampling efficiency and predictive precision, establishing decoding-based regression as a robust and accurate paradigm for general-purpose numerical prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Decoding-based regression ä¸­ç¦»æ•£ Token çº§ç›®æ ‡ï¼ˆå¦‚ cross-entropyï¼‰ä¸Žè¿žç»­æ•°å€¼ä¹‹é—´çš„å¤±é…é—®é¢˜ï¼ŒæŒ‡å‡ºä»…ä¾èµ– Token çº§çº¦æŸçš„æ–¹æ³•éš¾ä»¥æ•æ‰ç›®æ ‡å€¼çš„å…¨å±€é‡çº§ï¼Œé™åˆ¶äº†é¢„æµ‹ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºé€šè¿‡ Reinforcement Learning (RL) æ¥é‡Šæ”¾è§£ç å›žå½’çš„æ½œåŠ›ï¼Œå°†ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸º Markov Decision Processï¼Œå¹¶åˆ©ç”¨ sequence-level å¥–åŠ±æ¥å¼ºåˆ¶å®žçŽ°å…¨å±€æ•°å€¼çš„ä¸€è‡´æ€§ã€‚åœ¨ tabular regression å’Œ code metric regression ä¸Šçš„å¹¿æ³›å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•ï¼ˆç‰¹åˆ«æ˜¯ç»“åˆ ReMax å’Œ GRPO ç®—æ³•ï¼‰åœ¨æ€§èƒ½ä¸Šä¸€è‡´ä¼˜äºŽç›®å‰æœ€å…ˆè¿›çš„ Token çº§åŸºå‡†æ¨¡åž‹å’Œä¼ ç»Ÿ regression headsã€‚åˆ†æžè¿›ä¸€æ­¥æ­ç¤ºï¼ŒRL æ˜¾è‘—æå‡äº†é‡‡æ ·æ•ˆçŽ‡å’Œé¢„æµ‹ç²¾åº¦ï¼Œè¯æ˜Žäº†å¼•å…¥åºåˆ—çº§ä¿¡å·çš„ä¼˜è¶Šæ€§ï¼Œä»Žè€Œå°† Decoding-based regression ç¡®ç«‹ä¸ºä¸€ç§é²æ£’ä¸”å‡†ç¡®çš„é€šç”¨æ•°å€¼é¢„æµ‹èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06533v1",
      "published_date": "2025-12-06 18:57:38 UTC",
      "updated_date": "2025-12-06 18:57:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:15:06.626204+00:00"
    },
    {
      "arxiv_id": "2512.06531v1",
      "title": "Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images",
      "title_zh": "ç”¨äºŽMRIå›¾åƒè„‘è‚¿ç˜¤åˆ†ç±»ä¸Žåˆ†å‰²çš„æ–°åž‹æ·±åº¦å­¦ä¹ æž¶æž„",
      "authors": [
        "Sayan Das",
        "Arghadip Biswas"
      ],
      "abstract": "Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.",
      "tldr_zh": "é’ˆå¯¹è„‘è‚¿ç˜¤äººå·¥æ£€æµ‹è€—æ—¶ä¸”çŽ°æœ‰è®¡ç®—æœºè¾…åŠ©è¯Šæ–­(CAD)æ¨¡åž‹æ³›åŒ–æ€§èƒ½ä¸è¶³çš„æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸¤ç§æ–°åž‹æ·±åº¦å­¦ä¹ æž¶æž„ï¼Œæ—¨åœ¨å®žçŽ°MRIå›¾åƒä¸­è„‘è‚¿ç˜¤çš„è‡ªåŠ¨åˆ†ç±»ä¸Žåˆ†å‰²ã€‚å…¶ä¸­ï¼ŒSAETCN (Self-Attention Enhancement Tumor Classification Network) ä¸“é—¨ç”¨äºŽä¸åŒç±»åž‹è‚¿ç˜¤çš„åˆ†ç±»ä»»åŠ¡ï¼Œé€šè¿‡å¼•å…¥è‡ªæ³¨æ„åŠ›å¢žå¼ºæœºåˆ¶ï¼Œåœ¨åŒ…å«èƒ¶è´¨ç˜¤(glioma)ã€è„‘è†œç˜¤(meningioma)å’Œåž‚ä½“ç˜¤(pituitary tumors)çš„æ•°æ®é›†ä¸Šè¾¾åˆ°äº†99.38%çš„éªŒè¯å‡†ç¡®çŽ‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ç”¨äºŽç²¾ç¡®åˆ†å‰²çš„SAS-Net (Self-Attentive Segmentation Network)ï¼Œè¯¥ç½‘ç»œå®žçŽ°äº†99.23%çš„æ€»åƒç´ å‡†ç¡®çŽ‡ã€‚å®žéªŒç»“æžœè¯æ˜Žï¼Œè¿™ä¸¤é¡¹æž¶æž„èƒ½å¤Ÿæœ‰æ•ˆå…‹æœçŽ°æœ‰æ¨¡åž‹åœ¨éªŒè¯æ•°æ®ä¸Šè¡¨çŽ°ä¸ä½³çš„é—®é¢˜ï¼Œä¸ºè„‘è‚¿ç˜¤çš„æ—©æœŸå‘çŽ°å’Œç²¾ç¡®è¯Šæ–­æä¾›äº†é«˜å¯é æ€§çš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06531v1",
      "published_date": "2025-12-06 18:49:57 UTC",
      "updated_date": "2025-12-06 18:49:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:15:24.963834+00:00"
    },
    {
      "arxiv_id": "2512.06521v1",
      "title": "ShadowWolf -- Automatic Labelling, Evaluation and Model Training Optimised for Camera Trap Wildlife Images",
      "title_zh": "ShadowWolfï¼šé¢å‘çº¢å¤–ç›¸æœºé‡Žç”ŸåŠ¨ç‰©å›¾åƒçš„è‡ªåŠ¨æ ‡æ³¨ã€è¯„ä¼°ä¸Žæ¨¡åž‹è®­ç»ƒä¼˜åŒ–",
      "authors": [
        "Jens Dede",
        "Anna FÃ¶rster"
      ],
      "abstract": "The continuous growth of the global human population is leading to the expansion of human habitats, resulting in decreasing wildlife spaces and increasing human-wildlife interactions. These interactions can range from minor disturbances, such as raccoons in urban waste bins, to more severe consequences, including species extinction. As a result, the monitoring of wildlife is gaining significance in various contexts. Artificial intelligence (AI) offers a solution by automating the recognition of animals in images and videos, thereby reducing the manual effort required for wildlife monitoring. Traditional AI training involves three main stages: image collection, labelling, and model training. However, the variability, for example, in the landscape (e.g., mountains, open fields, forests), weather (e.g., rain, fog, sunshine), lighting (e.g., day, night), and camera-animal distances presents significant challenges to model robustness and adaptability in real-world scenarios.\n  In this work, we propose a unified framework, called ShadowWolf, designed to address these challenges by integrating and optimizing the stages of AI model training and evaluation. The proposed framework enables dynamic model retraining to adjust to changes in environmental conditions and application requirements, thereby reducing labelling efforts and allowing for on-site model adaptation. This adaptive and unified approach enhances the accuracy and efficiency of wildlife monitoring systems, promoting more effective and scalable conservation efforts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ShadowWolfï¼Œä¸€ä¸ªä¸“ä¸ºçº¢å¤–ç›¸æœº(camera trap)é‡Žç”ŸåŠ¨ç‰©å›¾åƒä¼˜åŒ–çš„ä¸€ä½“åŒ–æ¡†æž¶ï¼Œæ—¨åœ¨è§£å†³çŽ¯å¢ƒå¤šæ ·æ€§å¯¹ AI æ¨¡åž‹é²æ£’æ€§çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹é‡Žç”ŸåŠ¨ç‰©ç›‘æµ‹ä¸­åœ°å½¢ã€å¤©æ°”ã€å…‰ç…§åŠæ‹æ‘„è·ç¦»å¤šå˜å¯¼è‡´çš„é€‚åº”æ€§éš¾é¢˜ï¼Œè¯¥æ¡†æž¶æ•´åˆäº†è‡ªåŠ¨æ ‡æ³¨(labelling)ã€è¯„ä¼°å’Œæ¨¡åž‹è®­ç»ƒ(model training)çš„å…¨æµç¨‹ç®¡ç†ã€‚ShadowWolf æ”¯æŒåŠ¨æ€é‡è®­ç»ƒ(dynamic retraining)ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®çŽ¯å¢ƒæ¡ä»¶å’Œåº”ç”¨éœ€æ±‚çš„å˜åŒ–è¿›è¡Œå®žæ—¶è°ƒæ•´ï¼Œæœ‰æ•ˆé™ä½Žäº†äººå·¥æ ‡æ³¨çš„å·¥ä½œé‡ã€‚è¿™ç§ç»Ÿä¸€çš„æ–¹æ³•å…è®¸æ¨¡åž‹åœ¨çŽ°åœºè¿›è¡Œè‡ªé€‚åº”è°ƒæ•´(on-site model adaptation)ï¼Œæ˜¾è‘—å¢žå¼ºäº†ç³»ç»Ÿåœ¨çœŸå®žå¤æ‚åœºæ™¯ä¸­çš„åº”ç”¨èƒ½åŠ›ã€‚é€šè¿‡ä¼˜åŒ–æ¨¡åž‹è®­ç»ƒä¸Žè¯„ä¼°æµç¨‹ï¼ŒShadowWolf æå‡äº†é‡Žç”ŸåŠ¨ç‰©ç›‘æµ‹ç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œæ•ˆçŽ‡ï¼Œä¸ºå¤§è§„æ¨¡ã€å¯æ‰©å±•çš„ç”Ÿæ€ä¿æŠ¤å·¥ä½œæä¾›äº†æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "31 pages + appendix",
      "pdf_url": "https://arxiv.org/pdf/2512.06521v1",
      "published_date": "2025-12-06 18:17:53 UTC",
      "updated_date": "2025-12-06 18:17:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:15:01.018731+00:00"
    },
    {
      "arxiv_id": "2512.20638v1",
      "title": "Uncovering Competency Gaps in Large Language Models and Their Benchmarks",
      "title_zh": "æ­ç¤ºå¤§è¯­è¨€æ¨¡åž‹åŠå…¶åŸºå‡†æµ‹è¯•ä¸­çš„èƒ½åŠ›å·®è·",
      "authors": [
        "Matyas Bohacek",
        "Nino Scherrer",
        "Nicholas Dufour",
        "Thomas Leung",
        "Christoph Bregler",
        "Stephanie C. Y. Chan"
      ],
      "abstract": "The evaluation of large language models (LLMs) relies heavily on standardized benchmarks. These benchmarks provide useful aggregated metrics for a given capability, but those aggregated metrics can obscure (i) particular sub-areas where the LLMs are weak (\"model gaps\") and (ii) imbalanced coverage in the benchmarks themselves (\"benchmark gaps\"). We propose a new method that uses sparse autoencoders (SAEs) to automatically uncover both types of gaps. By extracting SAE concept activations and computing saliency-weighted performance scores across benchmark data, the method grounds evaluation in the model's internal representations and enables comparison across benchmarks. As examples demonstrating our approach, we applied the method to two popular open-source models and ten benchmarks. We found that these models consistently underperformed on concepts that stand in contrast to sycophantic behaviors (e.g., politely refusing a request or asserting boundaries) and concepts connected to safety discussions. These model gaps align with observations previously surfaced in the literature; our automated, unsupervised method was able to recover them without manual supervision. We also observed benchmark gaps: many of the evaluated benchmarks over-represented concepts related to obedience, authority, or instruction-following, while missing core concepts that should fall within their intended scope. In sum, our method offers a representation-grounded approach to evaluation, enabling concept-level decomposition of benchmark scores. Rather than replacing conventional aggregated metrics, CG complements them by providing a concept-level decomposition that can reveal why a model scored as it did and how benchmarks could evolve to better reflect their intended scope. Code is available at https://competency-gaps.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§åž‹è¯­è¨€æ¨¡åž‹ (LLMs) æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•ä¸­æ±‡æ€»æŒ‡æ ‡æŽ©ç›–ç‰¹å®šé¢†åŸŸå¼±ç‚¹ (model gaps) å’ŒåŸºå‡†æµ‹è¯•ä¸å¹³è¡¡ (benchmark gaps) çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºŽç¨€ç–è‡ªç¼–ç å™¨ (Sparse Autoencoders, SAEs) çš„æ–°åž‹è¯„ä¼°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æå– SAE ç‰¹å¾æ¿€æ´»å¹¶è®¡ç®—åŸºå‡†æ•°æ®çš„æ˜¾è‘—æ€§åŠ æƒæ€§èƒ½å¾—åˆ†ï¼Œå°†è¯„ä¼°å»ºç«‹åœ¨æ¨¡åž‹çš„å†…éƒ¨è¡¨ç¤ºä¹‹ä¸Šï¼Œå®žçŽ°äº†è·¨åŸºå‡†çš„æ¦‚å¿µçº§ç»†ç²’åº¦æ¯”è¾ƒã€‚é€šè¿‡å¯¹ä¸¤ç§ä¸»æµå¼€æºæ¨¡åž‹å’Œåä¸ªåŸºå‡†æµ‹è¯•çš„åº”ç”¨ï¼Œç ”ç©¶å‘çŽ°æ¨¡åž‹åœ¨åé˜¿è°€å¥‰æ‰¿è¡Œä¸º (sycophantic behaviors) ç›¸å…³æ¦‚å¿µï¼ˆå¦‚ç¤¼è²Œæ‹’ç»è¯·æ±‚æˆ–ç¡®ç«‹è¾¹ç•Œï¼‰ä»¥åŠå®‰å…¨è®¨è®ºç›¸å…³æ¦‚å¿µä¸Šè¡¨çŽ°æŒç»­ä¸ä½³ã€‚è¿™ç§è‡ªåŠ¨åŒ–ã€æ— ç›‘ç£çš„æ–¹æ³•æˆåŠŸè¿˜åŽŸäº†æ–‡çŒ®ä¸­å…ˆå‰è§‚å¯Ÿåˆ°çš„æ¨¡åž‹ç¼ºé™·ï¼Œæ— éœ€äººå·¥ç›‘ç£å³å¯å‘çŽ°æ¨¡åž‹æ¼æ´žã€‚ç ”ç©¶åŒæ—¶æ­ç¤ºäº†åŸºå‡†æµ‹è¯•è‡ªèº«çš„ç¼ºé™·ï¼Œå‘çŽ°è®¸å¤šçŽ°æœ‰åŸºå‡†è¿‡åº¦ä»£è¡¨äº†æœä»Žã€æƒå¨æˆ–æŒ‡ä»¤éµå¾ª (instruction-following) ç›¸å…³çš„æ¦‚å¿µï¼Œå´ç¼ºå¤±äº†å…¶é¢„æœŸèŒƒå›´å†…çš„æ ¸å¿ƒè¦ç´ ã€‚æ€»çš„æ¥è¯´ï¼Œè¯¥æ–¹æ³•ä¸ºè¯„ä¼°æä¾›äº†ä¸€ç§åŸºäºŽå†…éƒ¨è¡¨ç¤ºçš„æ¦‚å¿µçº§åˆ†è§£æ‰‹æ®µï¼Œä¸ä»…æ­ç¤ºäº†æ¨¡åž‹å¾—åˆ†èƒŒåŽçš„æ·±å±‚åŽŸå› ï¼Œè¿˜ä¸ºåŸºå‡†æµ‹è¯•å¦‚ä½•æ›´å¥½åœ°åæ˜ å…¶é¢„æœŸè¯„ä¼°èŒƒå›´æä¾›äº†æ”¹è¿›æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.20638v1",
      "published_date": "2025-12-06 17:39:47 UTC",
      "updated_date": "2025-12-06 17:39:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:15:32.685738+00:00"
    },
    {
      "arxiv_id": "2512.06506v1",
      "title": "AI as \"Co-founder\": GenAI for Entrepreneurship",
      "title_zh": "AIä½œä¸ºâ€œè”åˆåˆ›å§‹äººâ€ï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½èµ‹èƒ½åˆ›ä¸š",
      "authors": [
        "Junhui Jeff Cai",
        "Xian Gu",
        "Liugang Sheng",
        "Mengjia Xia",
        "Linda Zhao",
        "Wu Zhu"
      ],
      "abstract": "This paper studies whether, how, and for whom generative artificial intelligence (GenAI) facilitates firm creation. Our identification strategy exploits the November 2022 release of ChatGPT as a global shock that lowered start-up costs and leverages variations across geo-coded grids with differential pre-existing AI-specific human capital. Using high-resolution and universal data on Chinese firm registrations by the end of 2024, we find that grids with stronger AI-specific human capital experienced a sharp surge in new firm formation$\\unicode{x2013}$driven entirely by small firms, contributing to 6.0% of overall national firm entry. Large-firm entry declines, consistent with a shift toward leaner ventures. New firms are smaller in capital, shareholder number, and founding team size, especially among small firms. The effects are strongest among firms with potential AI applications, weaker financing needs, and among first-time entrepreneurs. Overall, our results highlight that GenAI serves as a pro-competitive force by disproportionately boosting small-firm entry.",
      "tldr_zh": "è¯¥ç ”ç©¶æŽ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) å¦‚ä½•ä»¥åŠä¸ºè°ä¿ƒè¿›ä¼ä¸šåˆ›å»ºï¼Œåˆ©ç”¨ 2022 å¹´ 11 æœˆ ChatGPT çš„å‘å¸ƒä½œä¸ºå…¨çƒå†²å‡»è¿›è¡Œè¯†åˆ«ã€‚ç ”ç©¶ç»“åˆåœ°ç†ç¼–ç ç½‘æ ¼å’Œäººå·¥æ™ºèƒ½ç‰¹å®šäººåŠ›èµ„æœ¬ (AI-specific human capital) çš„å·®å¼‚ï¼Œåˆ†æžäº†æˆªè‡³ 2024 å¹´åº•çš„ä¸­å›½ä¼ä¸šæ³¨å†Œå…¨é‡æ•°æ®ã€‚ç»“æžœå‘çŽ°ï¼Œæ‹¥æœ‰æ›´å¼ºäººå·¥æ™ºèƒ½äººåŠ›èµ„æœ¬çš„åœ°åŒºæ–°ä¼ä¸šæˆç«‹æ•°é‡å¤§å¹…å¢žåŠ ï¼Œä¸”å®Œå…¨ç”±å°å¾®ä¼ä¸šé©±åŠ¨ï¼Œè´¡çŒ®äº†å…¨å›½ä¼ä¸šå‡†å…¥æ€»é‡çš„ 6.0%ã€‚ä¸Žæ­¤åŒæ—¶ï¼Œå¤§ä¼ä¸šçš„å‡†å…¥æ•°é‡æœ‰æ‰€ä¸‹é™ï¼Œåæ˜ å‡ºåˆ›ä¸šæ´»åŠ¨å‘ç²¾ç›Šåˆ›ä¸š (leaner ventures) è½¬åž‹ã€‚æ–°åˆ›ä¼ä¸šåœ¨æ³¨å†Œèµ„æœ¬ã€è‚¡ä¸œäººæ•°å’Œåˆ›å§‹å›¢é˜Ÿè§„æ¨¡ä¸Šå‡æœ‰æ‰€å‡å°ï¼Œè¿™ç§çŽ°è±¡åœ¨å°å¾®ä¼ä¸šä¸­å°¤ä¸ºæ˜¾è‘—ã€‚è¯¥æ•ˆåº”åœ¨å…·æœ‰æ½œåœ¨äººå·¥æ™ºèƒ½åº”ç”¨åœºæ™¯ã€èžèµ„éœ€æ±‚è¾ƒå¼±ä»¥åŠåˆæ¬¡åˆ›ä¸šè€… (first-time entrepreneurs) ç¾¤ä½“ä¸­è¡¨çŽ°æœ€ä¸ºå¼ºåŠ²ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶è¡¨æ˜Žç”Ÿæˆå¼äººå·¥æ™ºèƒ½é€šè¿‡ä¸æˆæ¯”ä¾‹åœ°ææŒ¯å°å¾®ä¼ä¸šå‡†å…¥ï¼Œæˆä¸ºäº†ä¸€ç§ä¿ƒè¿›å¸‚åœºç«žäº‰çš„åŠ›é‡ã€‚",
      "categories": [
        "econ.GN",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06506v1",
      "published_date": "2025-12-06 17:36:36 UTC",
      "updated_date": "2025-12-06 17:36:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:15:12.766835+00:00"
    },
    {
      "arxiv_id": "2512.06504v1",
      "title": "Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion",
      "title_zh": "èžåˆçº¢å¤–ä¸Žå¯è§å…‰æ•°æ®çš„æ— äººæœºå…‰ä¼ç»„ä»¶å·¡æ£€æ–¹æ³•",
      "authors": [
        "Andrii Lysyi",
        "Anatoliy Sachenko",
        "Pavlo Radiuk",
        "Mykola Lysyi",
        "Oleksandr Melnychenko",
        "Diana Zahorodnia"
      ],
      "abstract": "The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.",
      "tldr_zh": "æœ¬ç ”ç©¶å¼€å‘äº†ä¸€å¥—ç”¨äºŽå…‰ä¼(PV)åŸºç¡€è®¾æ–½è‡ªåŠ¨åŒ–å·¡æ£€çš„æ™ºèƒ½åŒ–é›†æˆæ¡†æž¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–¹æ³•ä¸­å­˜åœ¨çš„çƒ­çº¢å¤–è°ƒè‰²æ¿åå·®(thermal palette bias)ã€æ•°æ®å†—ä½™ä»¥åŠé«˜é€šä¿¡å¸¦å®½éœ€æ±‚ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å­¦ä¹ è¡¨å¾ä¸€è‡´æ€§æž„å»ºäº†è°ƒè‰²æ¿æ— å…³çš„çƒ­çº¢å¤–åµŒå…¥(palette-invariant thermal embedding)ï¼Œå¹¶åˆ©ç”¨é—¨æŽ§æœºåˆ¶(gated mechanism)å°†å…¶ä¸Žå¯¹æ¯”åº¦å½’ä¸€åŒ–çš„RGBæµè¿›è¡Œå¤šæ¨¡æ€èžåˆã€‚ä¸ºäº†æé«˜æ£€æµ‹ç²¾åº¦ï¼Œæ¡†æž¶å¼•å…¥äº†åŸºäºŽRodrigueså˜æ¢æ›´æ–°çš„é—­çŽ¯è‡ªé€‚åº”é‡é‡‡é›†æŽ§åˆ¶å™¨ä»¥ç¡®è®¤æ¨¡ç³Šå¼‚å¸¸ï¼Œå¹¶ç»“åˆåŸºäºŽhaversineè·ç¦»çš„DBSCANç®—æ³•å®žçŽ°äº†åœ°ç†ç©ºé—´åŽ»é‡ã€‚å®žéªŒè¡¨æ˜Žï¼Œè¯¥ç³»ç»Ÿåœ¨PVF-10åŸºå‡†ä¸Šè¾¾åˆ°äº†0.903çš„mAP@0.5ï¼Œè¾ƒå•æ¨¡æ€åŸºçº¿æå‡äº†12-15%ã€‚å®žåœ°éªŒè¯è¿›ä¸€æ­¥è¯å®žäº†ç³»ç»Ÿå…·æœ‰96%çš„æŸ¥å…¨çŽ‡(recall)ï¼Œå¹¶åœ¨é™ä½Ž15-20%å‡é˜³æ€§çš„åŒæ—¶å‡å°‘äº†60-70%çš„æœºè½½æ•°æ®ä¼ è¾“å¸¦å®½ã€‚è¯¥ç ”ç©¶ä¸ºä¸»åŠ¨å¼å…‰ä¼ç»„ä»¶å·¡æ£€å»ºç«‹äº†é«˜æ•ˆä¸”å¯é çš„è‡ªåŠ¨åŒ–å¤„ç†æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06504v1",
      "published_date": "2025-12-06 17:28:22 UTC",
      "updated_date": "2025-12-06 17:28:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:15:35.372846+00:00"
    },
    {
      "arxiv_id": "2512.06496v1",
      "title": "PRIMRose: Insights into the Per-Residue Energy Metrics of Proteins with Double InDel Mutations using Deep Learning",
      "title_zh": "PRIMRoseï¼šåˆ©ç”¨æ·±åº¦å­¦ä¹ å‰–æžåŒ InDel çªå˜è›‹ç™½è´¨çš„å•æ®‹åŸºèƒ½é‡æŒ‡æ ‡",
      "authors": [
        "Stella Brown",
        "Nicolas Preisig",
        "Autumn Davis",
        "Brian Hutchinson",
        "Filip Jagodzinski"
      ],
      "abstract": "Understanding how protein mutations affect protein structure is essential for advancements in computational biology and bioinformatics. We introduce PRIMRose, a novel approach that predicts energy values for each residue given a mutated protein sequence. Unlike previous models that assess global energy shifts, our method analyzes the localized energetic impact of double amino acid insertions or deletions (InDels) at the individual residue level, enabling residue-specific insights into structural and functional disruption. We implement a Convolutional Neural Network architecture to predict the energy changes of each residue in a protein mutation. We train our model on datasets constructed from nine proteins, grouped into three categories: one set with exhaustive double InDel mutations, another with approximately 145k randomly sampled double InDel mutations, and a third with approximately 80k randomly sampled double InDel mutations. Our model achieves high predictive accuracy across a range of energy metrics as calculated by the Rosetta molecular modeling suite and reveals localized patterns that influence model performance, such as solvent accessibility and secondary structure context. This per-residue analysis offers new insights into the mutational tolerance of specific regions within proteins and provides higher interpretable and biologically meaningful predictions of InDels' effects.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PRIMRoseï¼Œä¸€ç§åˆ©ç”¨æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰é¢„æµ‹è›‹ç™½è´¨åŒæ°¨åŸºé…¸æ’å…¥æˆ–ç¼ºå¤±ï¼ˆInDelsï¼‰çªå˜åŽå•æ®‹åŸºèƒ½é‡å€¼çš„æ–°æ–¹æ³•ã€‚ä¸Žä»¥å¾€ä»…è¯„ä¼°å…¨å±€èƒ½é‡å˜åŒ–çš„æ–¹æ¡ˆä¸åŒï¼Œè¯¥æ–¹æ³•é€šè¿‡å·ç§¯ç¥žç»ç½‘ç»œï¼ˆConvolutional Neural Networkï¼‰æž¶æž„åˆ†æžæ®‹åŸºçº§åˆ«çš„å±€éƒ¨èƒ½é‡å½±å“ï¼Œä»Žè€Œæä¾›æ®‹åŸºç‰¹å¼‚æ€§çš„ç»“æž„ä¸ŽåŠŸèƒ½è§è§£ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨åŒ…å«ä¹ç§è›‹ç™½è´¨ã€æ¶µç›–è¯¦å°½åŠéšæœºåŒInDelçªå˜çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œç»“æžœæ˜¾ç¤ºPRIMRoseåœ¨é¢„æµ‹Rosettaåˆ†å­æ¨¡æ‹Ÿå¥—ä»¶çš„å„é¡¹èƒ½é‡æŒ‡æ ‡æ–¹é¢å…·æœ‰æžé«˜å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæ¨¡åž‹æ­ç¤ºäº†æº¶å‰‚å¯åŠæ€§ï¼ˆsolvent accessibilityï¼‰å’ŒäºŒçº§ç»“æž„èƒŒæ™¯ï¼ˆsecondary structure contextï¼‰ç­‰å±€éƒ¨æ¨¡å¼å¯¹èƒ½é‡é¢„æµ‹æ€§èƒ½çš„å…³é”®å½±å“ã€‚è¿™ç§é€æ®‹åŸºåˆ†æžï¼ˆper-residue analysisï¼‰ä¸ºç‰¹å®šè›‹ç™½è´¨åŒºåŸŸçš„çªå˜è€å—æ€§æä¾›äº†æ–°çš„è§†è§’ï¼Œä¸ºç†è§£InDelsæ•ˆåº”æä¾›äº†æ›´å…·å¯è§£é‡Šæ€§å’Œç”Ÿç‰©å­¦æ„ä¹‰çš„é¢„æµ‹ã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG",
        "cs.NE",
        "q-bio.QM"
      ],
      "primary_category": "q-bio.BM",
      "comment": "Presented at Computational Structural Bioinformatics Workshop 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.06496v1",
      "published_date": "2025-12-06 16:57:56 UTC",
      "updated_date": "2025-12-06 16:57:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:15:47.253612+00:00"
    },
    {
      "arxiv_id": "2512.06483v1",
      "title": "Classifying German Language Proficiency Levels Using Large Language Models",
      "title_zh": "åŸºäºŽå¤§è¯­è¨€æ¨¡åž‹çš„å¾·è¯­è¯­è¨€èƒ½åŠ›ç­‰çº§åˆ†ç±»",
      "authors": [
        "Elias-Leander Ahlers",
        "Witold Brunsmann",
        "Malte Schilling"
      ],
      "abstract": "Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.",
      "tldr_zh": "è¯¥ç ”ç©¶æŽ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡åž‹ (Large Language Models, LLMs) è‡ªåŠ¨æ ¹æ®æ¬§æ´²è¯­è¨€å…±åŒå‚è€ƒæ¡†æž¶ (CEFR) å¯¹å¾·è¯­æ–‡æœ¬è¿›è¡Œè¯­è¨€èƒ½åŠ›ç­‰çº§åˆ†ç±»çš„æ–¹æ³•ã€‚ä¸ºäº†æ”¯æŒç¨³å¥çš„è®­ç»ƒä¸Žè¯„ä¼°ï¼Œç ”ç©¶äººå‘˜é€šè¿‡æ•´åˆå¤šä¸ªçŽ°æœ‰çš„ CEFR æ ‡æ³¨è¯­æ–™åº“ä»¥åŠåˆæˆæ•°æ® (synthetic data)ï¼Œæž„å»ºäº†ä¸€ä¸ªå¤šæ ·åŒ–çš„æ•°æ®é›†ã€‚ç ”ç©¶è¿‡ç¨‹ä¸­è¯„ä¼°äº†æç¤ºå·¥ç¨‹ (prompt-engineering)ã€å¯¹ LLaMA-3-8B-Instruct æ¨¡åž‹è¿›è¡Œå¾®è°ƒ (fine-tuning) ä»¥åŠåˆ©ç”¨ LLM å†…éƒ¨ç¥žç»çŠ¶æ€è¿›è¡Œåˆ†ç±»çš„æŽ¢æµ‹æ³• (probing-based approach) ç­‰å¤šç§ç­–ç•¥ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¿™äº›æ–¹æ³•åœ¨åˆ†ç±»æ€§èƒ½ä¸Šç›¸è¾ƒäºŽä»¥å¾€ç ”ç©¶æœ‰æŒç»­æ€§çš„æå‡ï¼Œè¯æ˜Žäº† LLMs åœ¨å®žçŽ°å¯é ä¸”å¯æ‰©å±•çš„ CEFR åˆ†ç±»æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚è¯¥æˆæžœä¸ä»…æé«˜äº†è‡ªåŠ¨åŒ–æµ‹è¯„çš„å‡†ç¡®æ€§ï¼Œä¹Ÿä¸ºæ ¹æ®å­¦ä¹ è€…éœ€æ±‚è¿›è¡Œå®šåˆ¶åŒ–æ•™å­¦æä¾›äº†æœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at 3rd International Conference on Foundation and Large Language Models (FLLM2025), Vienna (Austria)",
      "pdf_url": "https://arxiv.org/pdf/2512.06483v1",
      "published_date": "2025-12-06 16:15:45 UTC",
      "updated_date": "2025-12-06 16:15:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:16:19.868101+00:00"
    },
    {
      "arxiv_id": "2512.06471v1",
      "title": "Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control",
      "title_zh": "ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ä¸ºä½•æœ‰æ•ˆï¼šä¸Žå¯¹å¶æŽ§åˆ¶çš„è”ç³»",
      "authors": [
        "Nathan P. Lawrence",
        "Ali Mesbah"
      ],
      "abstract": "Goal-conditioned reinforcement learning (RL) concerns the problem of training an agent to maximize the probability of reaching target goal states. This paper presents an analysis of the goal-conditioned setting based on optimal control. In particular, we derive an optimality gap between more classical, often quadratic, objectives and the goal-conditioned reward, elucidating the success of goal-conditioned RL and why classical ``dense'' rewards can falter. We then consider the partially observed Markov decision setting and connect state estimation to our probabilistic reward, further making the goal-conditioned reward well suited to dual control problems. The advantages of goal-conditioned policies are validated on nonlinear and uncertain environments using both RL and predictive control techniques.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»Žæœ€ä¼˜æŽ§åˆ¶(Optimal Control)çš„è§’åº¦å¯¹ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ (Goal-conditioned RL)è¿›è¡Œäº†æ·±å…¥åˆ†æžï¼Œæ—¨åœ¨æ­ç¤ºå…¶å†…åœ¨çš„å·¥ä½œæœºåˆ¶ã€‚ä½œè€…æŽ¨å¯¼äº†ä¼ ç»Ÿç›®æ ‡å‡½æ•°ï¼ˆé€šå¸¸ä¸ºäºŒæ¬¡é¡¹ç›®æ ‡ï¼‰ä¸Žç›®æ ‡æ¡ä»¶å¥–åŠ±(goal-conditioned reward)ä¹‹é—´çš„æœ€ä¼˜æ€§å·®è·(optimality gap)ï¼Œé˜æ˜Žäº†ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ æˆåŠŸçš„åŽŸå› ä»¥åŠä¼ ç»Ÿâ€œå¯†é›†â€å¥–åŠ±(dense rewards)å¤±æ•ˆçš„æœºåˆ¶ã€‚ç ”ç©¶è¿›ä¸€æ­¥æŽ¢è®¨äº†éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(POMDP)è®¾ç½®ï¼Œå°†çŠ¶æ€ä¼°è®¡(state estimation)ä¸Žæ¦‚çŽ‡å¥–åŠ±è”ç³»èµ·æ¥ï¼Œä½¿ç›®æ ‡æ¡ä»¶å¥–åŠ±èƒ½å¤Ÿå¾ˆå¥½åœ°é€‚é…åŒé‡æŽ§åˆ¶(dual control)é—®é¢˜ã€‚æœ€åŽï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œé¢„æµ‹æŽ§åˆ¶æŠ€æœ¯åœ¨éžçº¿æ€§å’Œä¸ç¡®å®šçŽ¯å¢ƒä¸­çš„å®žéªŒï¼ŒéªŒè¯äº†ç›®æ ‡æ¡ä»¶ç­–ç•¥(goal-conditioned policies)åœ¨å¤„ç†å¤æ‚åŠ¨åŠ›å­¦ç³»ç»Ÿæ—¶çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "IFAC preprint",
      "pdf_url": "https://arxiv.org/pdf/2512.06471v1",
      "published_date": "2025-12-06 15:28:35 UTC",
      "updated_date": "2025-12-06 15:28:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:16:05.774959+00:00"
    },
    {
      "arxiv_id": "2512.06458v1",
      "title": "Instance Dependent Testing of Samplers using Interval Conditioning",
      "title_zh": "åŸºäºŽåŒºé—´æ¡ä»¶çš„é‡‡æ ·å™¨å®žä¾‹ä¾èµ–åž‹æµ‹è¯•",
      "authors": [
        "Rishiraj Bhattacharyya",
        "Sourav Chakraborty",
        "Yash Pote",
        "Uddalok Sarkar",
        "Sayantan Sen"
      ],
      "abstract": "Sampling algorithms play a pivotal role in probabilistic AI. However, verifying if a sampler program indeed samples from the claimed distribution is a notoriously hard problem. Provably correct testers like Barbarik, Teq, Flash, CubeProbe for testing of different kinds of samplers were proposed only in the last few years. All these testers focus on the worst-case efficiency, and do not support verification of samplers over infinite domains, a case occurring frequently in Astronomy, Finance, Network Security, etc.\n  In this work, we design the first tester of samplers with instance-dependent efficiency, allowing us to test samplers over natural numbers. Our tests are developed via a novel distance estimation algorithm between an unknown and a known probability distribution using an interval conditioning framework. The core technical contribution is a new connection with probability mass estimation of a continuous distribution. The practical gains are also substantial: our experiments establish up to 1000x speedup over state-of-the-art testers.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¦‚çŽ‡äººå·¥æ™ºèƒ½ï¼ˆprobabilistic AIï¼‰ä¸­é‡‡æ ·ç®—æ³•ï¼ˆsampling algorithmsï¼‰éš¾ä»¥éªŒè¯çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºŽåŒºé—´è°ƒèŠ‚ï¼ˆInterval Conditioningï¼‰æ¡†æž¶çš„é‡‡æ ·å™¨æµ‹è¯•æ–¹æ³•ã€‚çŽ°æœ‰æµ‹è¯•å·¥å…·å¦‚ Barbarik å’Œ Teq é€šå¸¸ä¾§é‡äºŽæœ€åæƒ…å†µæ•ˆçŽ‡ï¼Œä¸”æ— æ³•æ”¯æŒå¤©æ–‡å­¦å’Œé‡‘èžç­‰é¢†åŸŸå¸¸è§çš„æ— é™åŸŸï¼ˆinfinite domainsï¼‰é‡‡æ ·éªŒè¯ã€‚æœ¬ç ”ç©¶è®¾è®¡äº†é¦–ä¸ªå…·æœ‰å®žä¾‹ç›¸å…³æ•ˆçŽ‡ï¼ˆinstance-dependent efficiencyï¼‰çš„æµ‹è¯•å™¨ï¼Œå®žçŽ°äº†å¯¹è‡ªç„¶æ•°åŸŸé‡‡æ ·å™¨çš„æœ‰æ•ˆæµ‹è¯•ã€‚å…¶æ ¸å¿ƒæŠ€æœ¯è´¡çŒ®åœ¨äºŽåˆ©ç”¨æ–°é¢–çš„è·ç¦»ä¼°è®¡ç®—æ³•ï¼Œå»ºç«‹äº†ä¸Žè¿žç»­åˆ†å¸ƒæ¦‚çŽ‡è´¨é‡ä¼°è®¡ï¼ˆprobability mass estimationï¼‰ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚å®žéªŒè¯æ˜Žï¼Œè¯¥æ–¹æ³•ç›¸æ¯”çŽ°æœ‰æœ€å…ˆè¿›çš„æµ‹è¯•å·¥å…·å®žçŽ°äº†é«˜è¾¾ 1000 å€çš„åŠ é€Ÿï¼Œæ˜¾è‘—æå‡äº†é‡‡æ ·å™¨éªŒè¯çš„å®žç”¨æ€§ä¸Žæ•ˆçŽ‡ã€‚",
      "categories": [
        "cs.DS",
        "cs.AI"
      ],
      "primary_category": "cs.DS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06458v1",
      "published_date": "2025-12-06 14:45:56 UTC",
      "updated_date": "2025-12-06 14:45:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:16:04.115316+00:00"
    },
    {
      "arxiv_id": "2512.06443v1",
      "title": "Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices",
      "title_zh": "Vec-LUTï¼šé¢å‘è¾¹ç¼˜è®¾å¤‡å¹¶è¡Œè¶…ä½Žæ¯”ç‰¹å¤§è¯­è¨€æ¨¡åž‹æŽ¨ç†çš„å‘é‡æŸ¥è¡¨",
      "authors": [
        "Xiangyu Li",
        "Chengyu Yin",
        "Weijun Wang",
        "Jianyu Wei",
        "Ting Cao",
        "Yunxin Liu"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed on edge devices. To meet strict resource constraints, real-world deployment has pushed LLM quantization from 8-bit to 4-bit, 2-bit, and now 1.58-bit. Combined with lookup table (LUT)-based inference, CPUs run these ultra-low-bit LLMs even faster than NPUs, opening new opportunities for ubiquitous on-device intelligence.\n  However, this paper identifies that LUT-based inference underutilizes memory bandwidth during parallel inference, which is required for prefilling, test-time scaling, and other multi-token scenarios. The root cause is the scalar LUT paradigm, which performs repetitive and non-contiguous memory accesses for each token.\n  To solve the issue, we propose vector LUT, a new lookup paradigm that constructs a unified LUT across parallel tokens, and performs a single $1 \\rightarrow N$ lookup per index. To realize it efficiently, we further introduce (1) Vector LUT-Centric Tensor Layout, and (2) Cache-Aware Streamed Lookup techniques. Evaluations on 5 edge devices across 3 LLMs show that Vec-LUT outperforms state-of-the-art baselines by up to $4.2\\times$. Our implementation is integrated into llama.cpp. The code is available at https://github.com/Cipherxzc/vlut.cpp.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡åž‹(LLMs)åœ¨è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ä¸­çš„èµ„æºé™åˆ¶ï¼Œåˆ†æžäº†åŸºäºŽæŸ¥æ‰¾è¡¨(LUT)çš„è¶…ä½Žæ¯”ç‰¹æŽ¨ç†åœ¨å¹¶è¡ŒæŽ¨ç†åœºæ™¯ä¸‹å†…å­˜å¸¦å®½åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ã€‚ç ”ç©¶æŒ‡å‡ºä¼ ç»Ÿçš„æ ‡é‡LUTèŒƒå¼åœ¨å¤„ç†é¢„å¡«å……(prefilling)å’Œå¤šæ ‡è®°åœºæ™¯æ—¶ï¼Œç”±äºŽé‡å¤ä¸”éžè¿žç»­çš„å†…å­˜è®¿é—®å¯¼è‡´æ•ˆçŽ‡å—é™ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Vec-LUTï¼Œä¸€ç§æ–°çš„å‘é‡æŸ¥æ‰¾è¡¨èŒƒå¼ï¼Œé€šè¿‡åœ¨å¹¶è¡Œæ ‡è®°é—´æž„å»ºç»Ÿä¸€çš„LUTï¼Œå®žçŽ°æ¯ä¸ªç´¢å¼•ä»…éœ€ä¸€æ¬¡1å¯¹Nçš„æŸ¥æ‰¾æ“ä½œã€‚è¯¥æ–¹æ¡ˆè¿›ä¸€æ­¥å¼•å…¥äº†ä»¥å‘é‡LUTä¸ºä¸­å¿ƒçš„å¼ é‡å¸ƒå±€(Vector LUT-Centric Tensor Layout)å’Œç¼“å­˜æ„ŸçŸ¥æµå¼æŸ¥æ‰¾(Cache-Aware Streamed Lookup)æŠ€æœ¯ï¼Œä»¥ä¼˜åŒ–æ‰§è¡Œæ•ˆçŽ‡ã€‚åœ¨5ç§è¾¹ç¼˜è®¾å¤‡å’Œ3ç§LLMsä¸Šçš„å®žéªŒè¯„ä¼°è¡¨æ˜Žï¼ŒVec-LUTçš„æ€§èƒ½ä¼˜äºŽçŽ°æœ‰åŸºå‡†æ¨¡åž‹ï¼Œæœ€é«˜å¯å®žçŽ°4.2å€çš„æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶æˆæžœç›®å‰å·²é›†æˆè‡³llama.cppä¸­ï¼Œä¸ºå®žçŽ°æ›´é«˜æ•ˆçš„ç«¯ä¾§æ™ºèƒ½æŽ¨ç†æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2512.06443v1",
      "published_date": "2025-12-06 14:14:01 UTC",
      "updated_date": "2025-12-06 14:14:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:16:07.879861+00:00"
    },
    {
      "arxiv_id": "2512.06431v1",
      "title": "Smart Spatial Planning in Egypt: An Algorithm-Driven Approach to Public Service Evaluation in Qena City",
      "title_zh": "Egypt æ™ºæ…§ç©ºé—´è§„åˆ’ï¼šä¸€ç§é¢å‘ Qena City å…¬å…±æœåŠ¡è¯„ä¼°çš„ç®—æ³•é©±åŠ¨æ–¹æ³•",
      "authors": [
        "Mohamed Shamroukh",
        "Mohamed Alkhuzamy Aziz"
      ],
      "abstract": "National planning standards for public services in Egypt often fail to align with unique local characteristics. Addressing this gap, this study develops a tailored planning model for Qena City. Using a hybrid methodology (descriptive, analytical, and experimental), the research utilizes Python programming to generate an intelligent spatial analysis algorithm based on Voronoi Diagrams. This approach creates city-specific planning criteria and evaluates the current coverage of public facilities. The primary contribution of this study is the successful derivation of a localized planning standards model and the deployment of an automated algorithm to assess service efficiency. Application of this model reveals a general service coverage average of 81.3%. Ambulance stations demonstrated the highest efficiency (99.8%) due to recent upgrades, while parks and open spaces recorded the lowest coverage (10%) caused by limited land availability. Spatial analysis indicates a high service density in midtown (>45 services/km^2), which diminishes significantly towards the outskirts (<5 services/km^2). Consequently, the Hajer Qena district contains the highest volume of unserved areas, while the First District (Qesm 1) exhibits the highest level of service coverage. This model offers a replicable framework for data-driven urban planning in Egyptian cities.",
      "tldr_zh": "é’ˆå¯¹åŸƒåŠå›½å®¶å…¬å…±æœåŠ¡è§„åˆ’æ ‡å‡†ä¸Žåœ°æ–¹ç‰¹è‰²è„±èŠ‚çš„é—®é¢˜ï¼Œæœ¬ç ”ç©¶ä¸ºQena Cityå¼€å‘äº†ä¸€ç§å®šåˆ¶åŒ–çš„è§„åˆ’æ¨¡åž‹ã€‚ç ”ç©¶é‡‡ç”¨æè¿°æ€§ã€åˆ†æžæ€§å’Œå®žéªŒæ€§çš„æ··åˆæ–¹æ³•ï¼Œåˆ©ç”¨Pythonç¼–ç¨‹ç”Ÿæˆäº†ä¸€ç§åŸºäºŽVoronoi Diagramsçš„æ™ºèƒ½ç©ºé—´åˆ†æžç®—æ³•ï¼Œç”¨äºŽå»ºç«‹åŸŽå¸‚ç‰¹å®šçš„è§„åˆ’æ ‡å‡†å¹¶è¯„ä¼°å…¬å…±è®¾æ–½çš„è¦†ç›–çŽ°çŠ¶ã€‚è¯¥ç ”ç©¶çš„ä¸»è¦è´¡çŒ®åœ¨äºŽæˆåŠŸæŽ¨å¯¼å‡ºäº†æœ¬åœ°åŒ–çš„è§„åˆ’æ ‡å‡†æ¨¡åž‹ï¼Œå¹¶éƒ¨ç½²äº†è‡ªåŠ¨åŒ–ç®—æ³•æ¥é‡åŒ–æœåŠ¡æ•ˆçŽ‡ã€‚åº”ç”¨ç»“æžœæ˜¾ç¤ºè¯¥å¸‚å…¬å…±æœåŠ¡å¹³å‡è¦†ç›–çŽ‡ä¸º81.3%ï¼Œå…¶ä¸­æ•‘æŠ¤ç«™å› è¿‘æœŸå‡çº§æ•ˆçŽ‡æœ€é«˜ï¼ˆ99.8%ï¼‰ï¼Œè€Œå…¬å›­å’Œå¼€æ”¾ç©ºé—´å—åœŸåœ°é™åˆ¶è¦†ç›–çŽ‡ä»…ä¸º10%ã€‚ç©ºé—´åˆ†æžè¡¨æ˜Žå¸‚ä¸­å¿ƒæœåŠ¡å¯†åº¦è¾ƒé«˜ï¼ˆ>45 services/km^2ï¼‰ï¼Œè€Œè¾¹ç¼˜åœ°åŒºæ˜¾è‘—ä¸‹é™ï¼Œå¯¼è‡´Hajer Qenaåœ°åŒºæœªè¦†ç›–åŒºåŸŸæœ€å¤šï¼Œè€Œç¬¬ä¸€åŒºï¼ˆQesm 1ï¼‰è¦†ç›–åº¦æœ€é«˜ã€‚è¯¥æ¨¡åž‹ä¸ºåŸƒåŠåŸŽå¸‚çš„æ•°æ®é©±åŠ¨åž‹åŸŽå¸‚è§„åˆ’æä¾›äº†ä¸€ä¸ªå¯å¤åˆ¶çš„æ¡†æž¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06431v1",
      "published_date": "2025-12-06 13:36:57 UTC",
      "updated_date": "2025-12-06 13:36:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:16:13.945609+00:00"
    },
    {
      "arxiv_id": "2512.06426v1",
      "title": "When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition",
      "title_zh": "æ€§åˆ«éš¾ä»¥è¾¨è¯†åœºæ™¯ä¸‹çš„è¿œè·ç¦»è¯†åˆ«ï¼šå¤šå±žæ€§è¾…åŠ©å¢žå¼º",
      "authors": [
        "Nzakiese Mbongo",
        "Kailash A. Hambarde",
        "Hugo ProenÃ§a"
      ],
      "abstract": "Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¶…è¿œè·ç¦»å›¾åƒä¸­ç”±äºŽç©ºé—´åˆ†è¾¨çŽ‡æœ‰é™ã€è§†è§’å¤šå˜åŠé¢éƒ¨ç‰¹å¾ç¼ºå¤±å¯¼è‡´çš„æ€§åˆ«è¯†åˆ«éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨ CLIP æ¨¡åž‹è”åˆå»ºæ¨¡è§†è§‰å’Œå±žæ€§é©±åŠ¨çº¿ç´¢çš„åŒè·¯å¾„å˜æ¢å™¨(dual-path transformer)æ¡†æž¶ã€‚è¯¥æ¡†æž¶åŒ…å«ä¸¤æ¡äº’è¡¥è·¯å¾„ï¼Œå³é€šè¿‡é€‰æ‹©æ€§å¾®è°ƒ CLIP å›¾åƒç¼–ç å™¨å®žçŽ°çš„ç›´æŽ¥è§†è§‰è·¯å¾„ï¼Œä»¥åŠåˆ©ç”¨å‘åž‹ã€è¡£ç€ç­‰è½¯ç”Ÿç‰©ç‰¹å¾æç¤ºåœ¨ CLIP æ–‡æœ¬-å›¾åƒç©ºé—´è¿›è¡ŒæŽ¨ç†çš„å±žæ€§ä¸­ä»‹è·¯å¾„ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†ç©ºé—´é€šé“æ³¨æ„åŠ›æ¨¡å—(spatial channel attention modules)ä»¥å¢žå¼ºé®æŒ¡å’Œä½Žåˆ†è¾¨çŽ‡ä¸‹çš„å®šä½èƒ½åŠ›ï¼Œå¹¶æž„å»ºäº†ç»Ÿä¸€çš„è¿œè·ç¦»æ€§åˆ«æ•°æ®é›† U-DetAGReIDã€‚å®žéªŒè¯æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨ macro-F1ã€å‡†ç¡®çŽ‡å’Œ AUC ç­‰æŒ‡æ ‡ä¸Šæ˜¾è‘—è¶…è¶Šäº†çŽ°æœ‰çš„è¡Œäººå±žæ€§è¯†åˆ«å’Œé‡è¯†åˆ«(re-identification)åŸºçº¿æ¨¡åž‹ï¼Œä¸”å¯¹è·ç¦»ã€è§’åº¦åŠé«˜åº¦å˜åŒ–å…·æœ‰æžå¼ºçš„é²æ£’æ€§ã€‚å®šæ€§åˆ†æžè¿›ä¸€æ­¥ç¡®è®¤äº†å…¶åœ¨å±žæ€§å®šä½ä¸Šçš„å¯è§£é‡Šæ€§ï¼Œä¸ºéžå—é™è¿œè·ç¦»åœºæ™¯ä¸‹çš„æ€§åˆ«è¯†åˆ«å¥ å®šäº†åšå®žçš„åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.06426v1",
      "published_date": "2025-12-06 13:12:07 UTC",
      "updated_date": "2025-12-06 13:12:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:16:17.854057+00:00"
    },
    {
      "arxiv_id": "2512.08987v1",
      "title": "3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization",
      "title_zh": "3DIDï¼šåŸºäºŽç‰©ç†æ„ŸçŸ¥ä¼˜åŒ–çš„ç©ºæ°”åŠ¨åŠ›å­¦ç›´æŽ¥ä¸‰ç»´é€†å‘è®¾è®¡",
      "authors": [
        "Yuze Hao",
        "Linchao Zhu",
        "Yi Yang"
      ],
      "abstract": "Inverse design aims to design the input variables of a physical system to optimize a specified objective function, typically formulated as a search or optimization problem. However, in 3D domains, the design space grows exponentially, rendering exhaustive grid-based searches infeasible. Recent advances in deep learning have accelerated inverse design by providing powerful generative priors and differentiable surrogate models. Nevertheless, current methods tend to approximate the 3D design space using 2D projections or fine-tune existing 3D shapes. These approaches sacrifice volumetric detail and constrain design exploration, preventing true 3D design from scratch. In this paper, we propose a 3D Inverse Design (3DID) framework that directly navigates the 3D design space by coupling a continuous latent representation with a physics-aware optimization strategy. We first learn a unified physics-geometry embedding that compactly captures shape and physical field data in a continuous latent space. Then, we introduce a two-stage strategy to perform physics-aware optimization. In the first stage, a gradient-guided diffusion sampler explores the global latent manifold. In the second stage, an objective-driven, topology-preserving refinement further sculpts each candidate toward the target objective. This enables 3DID to generate high-fidelity 3D geometries, outperforming existing methods in both solution quality and design versatility.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†3DIDæ¡†æž¶ï¼Œæ—¨åœ¨ç›´æŽ¥åœ¨3Dè®¾è®¡ç©ºé—´ä¸­é€šè¿‡ç‰©ç†æ„ŸçŸ¥ä¼˜åŒ–(physics-aware optimization)å®žçŽ°ç©ºæ°”åŠ¨åŠ›å­¦é€†å‘è®¾è®¡(Inverse design)ï¼Œè§£å†³äº†çŽ°æœ‰æ–¹æ³•ä¾èµ–2DæŠ•å½±æˆ–å¾®è°ƒçŽ°æœ‰å½¢çŠ¶å¯¼è‡´çš„ç»†èŠ‚ä¸¢å¤±å’Œæœç´¢å—é™é—®é¢˜ã€‚è¯¥æ¡†æž¶é¦–å…ˆå­¦ä¹ ä¸€ä¸ªç»Ÿä¸€çš„ç‰©ç†-å‡ ä½•åµŒå…¥(unified physics-geometry embedding)ï¼Œå°†å½¢çŠ¶å’Œç‰©ç†åœºæ•°æ®ç´§å‡‘åœ°æ•èŽ·åœ¨è¿žç»­æ½œç©ºé—´ä¸­ã€‚éšåŽï¼Œ3DIDå¼•å…¥ä¸¤é˜¶æ®µç­–ç•¥è¿›è¡Œä¼˜åŒ–ï¼šç¬¬ä¸€é˜¶æ®µåˆ©ç”¨æ¢¯åº¦å¼•å¯¼çš„æ‰©æ•£é‡‡æ ·å™¨(gradient-guided diffusion sampler)æŽ¢ç´¢å…¨å±€æ½œæµå½¢ï¼›ç¬¬äºŒé˜¶æ®µé€šè¿‡ç›®æ ‡é©±åŠ¨ä¸”ä¿æŒæ‹“æ‰‘çš„ç»†åŒ–(topology-preserving refinement)å¯¹å€™é€‰å½¢çŠ¶è¿›è¡Œé›•ç¢ã€‚è¿™ç§æ–¹æ³•ä½¿3DIDèƒ½å¤Ÿä»Žé›¶å¼€å§‹ç”Ÿæˆé«˜ä¿çœŸåº¦çš„3Då‡ ä½•ç»“æž„ï¼Œåœ¨è§£çš„è´¨é‡å’Œè®¾è®¡å¤šæ ·æ€§ä¸Šå‡ä¼˜äºŽçŽ°æœ‰åŸºå‡†ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œ3DIDæˆåŠŸå…‹æœäº†é«˜ç»´è®¾è®¡ç©ºé—´çš„æœç´¢éš¾é¢˜ï¼Œä¸ºå¤æ‚çš„3Dç‰©ç†ç³»ç»Ÿä¼˜åŒ–æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.08987v1",
      "published_date": "2025-12-06 13:09:03 UTC",
      "updated_date": "2025-12-06 13:09:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:16:25.948138+00:00"
    },
    {
      "arxiv_id": "2512.06421v1",
      "title": "Rethinking Training Dynamics in Scale-wise Autoregressive Generation",
      "title_zh": "é‡æ–°å®¡è§†é€å°ºåº¦è‡ªå›žå½’ç”Ÿæˆä¸­çš„è®­ç»ƒåŠ¨åŠ›å­¦",
      "authors": [
        "Gengze Zhou",
        "Chongjian Ge",
        "Hao Tan",
        "Feng Liu",
        "Yicong Hong"
      ],
      "abstract": "Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æŽ¢è®¨äº†å°ºåº¦è‡ªå›žå½’(Scale-wise Autoregressive)ç”Ÿæˆæ¨¡åž‹ä¸­çš„è®­ç»ƒåŠ¨åŠ›å­¦ï¼Œé’ˆå¯¹å¯¼è‡´ç”Ÿæˆè´¨é‡ä¸‹é™çš„æ›å…‰åå·®(Exposure Bias)é—®é¢˜è¿›è¡Œäº†æ·±å…¥åˆ†æžã€‚ä½œè€…æŒ‡å‡ºï¼Œæ›å…‰åå·®ä¸»è¦æºäºŽè®­ç»ƒä¸Žæµ‹è¯•é˜¶æ®µçš„æ¨¡å¼ä¸åŒ¹é…ä»¥åŠå„å°ºåº¦é—´å­¦ä¹ éš¾åº¦çš„ä¸å¹³è¡¡ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†Self-Autoregressive Refinement (SAR)æ¡†æž¶ï¼Œé€šè¿‡Stagger-Scale Rollout (SSR)æœºåˆ¶å¼•å…¥è½»é‡çº§è‡ªå›žå½’æ»šåŠ¨é¢„æµ‹ä»¥å¯¹é½æŽ¨ç†è¡Œä¸ºï¼Œå¹¶ç»“åˆContrastive Student-Forcing Loss (CSFL)ä¸ºè‡ªç”Ÿæˆä¸Šä¸‹æ–‡æä¾›ç¨³å®šç›‘ç£ã€‚å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒSARèƒ½ä»¥æžä½Žçš„è®¡ç®—å¼€é”€æ˜¾è‘—æå‡é¢„è®­ç»ƒæ¨¡åž‹çš„ç”Ÿæˆè´¨é‡ï¼Œåœ¨ImageNet 256ä¸Šä½¿FlexVAR-d16çš„FIDæŒ‡æ ‡é™ä½Žäº†5.2%ã€‚å‡­å€Ÿå…¶é«˜æ•ˆæ€§ä¸Žå¯æ‰©å±•æ€§ï¼ŒSARä¸ºè§†è§‰è‡ªå›žå½’ç”Ÿæˆæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„åŽæœŸè®­ç»ƒ(Post-training)ä¼˜åŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06421v1",
      "published_date": "2025-12-06 12:41:42 UTC",
      "updated_date": "2025-12-06 12:41:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:16:34.641645+00:00"
    },
    {
      "arxiv_id": "2512.10766v1",
      "title": "Metaphor-based Jailbreaking Attacks on Text-to-Image Models",
      "title_zh": "é’ˆå¯¹æ–‡ç”Ÿå›¾æ¨¡åž‹çš„åŸºäºŽéšå–»çš„è¶Šç‹±æ”»å‡»",
      "authors": [
        "Chenyu Zhang",
        "Yiwen Ma",
        "Lanjun Wang",
        "Wenhui Li",
        "Yi Tu",
        "An-An Liu"
      ],
      "abstract": "Text-to-image~(T2I) models commonly incorporate defense mechanisms to prevent the generation of sensitive images. Unfortunately, recent jailbreaking attacks have shown that adversarial prompts can effectively bypass these mechanisms and induce T2I models to produce sensitive content, revealing critical safety vulnerabilities. However, existing attack methods implicitly assume that the attacker knows the type of deployed defenses, which limits their effectiveness against unknown or diverse defense mechanisms. In this work, we introduce \\textbf{MJA}, a \\textbf{m}etaphor-based \\textbf{j}ailbreaking \\textbf{a}ttack method inspired by the Taboo game, aiming to effectively and efficiently attack diverse defense mechanisms without prior knowledge of their type by generating metaphor-based adversarial prompts. Specifically, MJA consists of two modules: an LLM-based multi-agent generation module~(MLAG) and an adversarial prompt optimization module~(APO). MLAG decomposes the generation of metaphor-based adversarial prompts into three subtasks: metaphor retrieval, context matching, and adversarial prompt generation. Subsequently, MLAG coordinates three LLM-based agents to generate diverse adversarial prompts by exploring various metaphors and contexts. To enhance attack efficiency, APO first trains a surrogate model to predict the attack results of adversarial prompts and then designs an acquisition strategy to adaptively identify optimal adversarial prompts. Extensive experiments on T2I models with various external and internal defense mechanisms demonstrate that MJA outperforms six baseline methods, achieving stronger attack performance while using fewer queries. Code is available in https://github.com/datar001/metaphor-based-jailbreaking-attack.",
      "tldr_zh": "é’ˆå¯¹æ–‡æœ¬ç”Ÿæˆå›¾åƒ(Text-to-image)æ¨¡åž‹ä¸­çš„å®‰å…¨é˜²å¾¡æœºåˆ¶ï¼ŒçŽ°æœ‰çš„è¶Šç‹±æ”»å‡»(jailbreaking attacks)å¾€å¾€å‡è®¾æ”»å‡»è€…é¢„å…ˆçŸ¥æ™“é˜²å¾¡ç±»åž‹ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨é¢å¯¹æœªçŸ¥æˆ–å¤šæ ·åŒ–é˜²å¾¡æ—¶çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶å—ç¦å¿Œè¯­(Taboo)æ¸¸æˆå¯å‘ï¼Œæå‡ºäº†åä¸ºMJAçš„åŸºäºŽéšå–»(metaphor-based)çš„è¶Šç‹±æ”»å‡»æ–¹æ³•ï¼Œæ—¨åœ¨æ— éœ€å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹ç”Ÿæˆå¯¹æŠ—æ€§æç¤º(adversarial prompts)ã€‚MJAç³»ç»ŸåŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šåŸºäºŽå¤§è¯­è¨€æ¨¡åž‹(LLM)çš„å¤šæ™ºèƒ½ä½“ç”Ÿæˆæ¨¡å—(MLAG)é€šè¿‡éšå–»æ£€ç´¢ã€ä¸Šä¸‹æ–‡åŒ¹é…å’Œå¯¹æŠ—æç¤ºç”Ÿæˆä¸‰ä¸ªå­ä»»åŠ¡æ¥æž„å»ºå¤šæ ·åŒ–çš„æ”»å‡»å‘é‡ï¼›å¯¹æŠ—æç¤ºä¼˜åŒ–æ¨¡å—(APO)åˆ™é€šè¿‡è®­ç»ƒä»£ç†æ¨¡åž‹(surrogate model)é¢„æµ‹æ”»å‡»ç»“æžœï¼Œå¹¶åˆ©ç”¨é‡‡é›†ç­–ç•¥è‡ªé€‚åº”åœ°è¯†åˆ«æœ€ä¼˜æç¤ºï¼Œæ˜¾è‘—æå‡äº†æ”»å‡»æ•ˆçŽ‡ã€‚åœ¨å¤šç§å…·æœ‰å¤–éƒ¨å’Œå†…éƒ¨é˜²å¾¡æœºåˆ¶çš„T2Iæ¨¡åž‹ä¸Šè¿›è¡Œçš„å¹¿æ³›å®žéªŒè¯æ˜Žï¼ŒMJAåœ¨æ”»å‡»æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºŽå…­ç§åŸºçº¿æ–¹æ³•ï¼Œä¸”æ‰€éœ€çš„æŸ¥è¯¢æ¬¡æ•°æ›´å°‘ï¼Œæœ‰æ•ˆæ­ç¤ºäº†å½“å‰è§†è§‰ç”Ÿæˆæ¨¡åž‹åœ¨åº”å¯¹éšå–»åŒ–æ”»å‡»æ—¶çš„å®‰å…¨æ¼æ´žã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "comment": "This paper includes model-generated content that may contain offensive or distressing material",
      "pdf_url": "https://arxiv.org/pdf/2512.10766v1",
      "published_date": "2025-12-06 12:38:00 UTC",
      "updated_date": "2025-12-06 12:38:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:16:28.694161+00:00"
    },
    {
      "arxiv_id": "2512.06406v1",
      "title": "UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems",
      "title_zh": "UncertaintyZooï¼šæ·±åº¦å­¦ä¹ ç³»ç»Ÿé¢„æµ‹ä¸ç¡®å®šæ€§é‡åŒ–çš„ç»Ÿä¸€å·¥å…·åŒ…",
      "authors": [
        "Xianzong Wu",
        "Xiaohong Li",
        "Lili Quan",
        "Qiang Hu"
      ],
      "abstract": "Large language models(LLMs) are increasingly expanding their real-world applications across domains, e.g., question answering, autonomous driving, and automatic software development. Despite this achievement, LLMs, as data-driven systems, often make incorrect predictions, which can lead to potential losses in safety-critical scenarios. To address this issue and measure the confidence of model outputs, multiple uncertainty quantification(UQ) criteria have been proposed. However, even though important, there are limited tools to integrate these methods, hindering the practical usage of UQ methods and future research in this domain. To bridge this gap, in this paper, we introduce UncertaintyZoo, a unified toolkit that integrates 29 uncertainty quantification methods, covering five major categories under a standardized interface. Using UncertaintyZoo, we evaluate the usefulness of existing uncertainty quantification methods under the code vulnerability detection task on CodeBERT and ChatGLM3 models. The results demonstrate that UncertaintyZoo effectively reveals prediction uncertainty. The tool with a demonstration video is available on the project site https://github.com/Paddingbuta/UncertaintyZoo.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡åž‹(LLMs)åœ¨å…³é”®å®‰å…¨é¢†åŸŸä¸­å¯èƒ½äº§ç”Ÿçš„é”™è¯¯é¢„æµ‹é—®é¢˜ï¼ŒæŽ¨å‡ºäº†åä¸ºUncertaintyZooçš„ç»Ÿä¸€å·¥å…·åŒ…ï¼Œç”¨äºŽé‡åŒ–æ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„é¢„æµ‹ä¸ç¡®å®šæ€§(Predictive Uncertainty)ã€‚è¯¥å·¥å…·åŒ…åœ¨æ ‡å‡†åŒ–æŽ¥å£ä¸‹é›†æˆäº†29ç§ä¸ç¡®å®šæ€§é‡åŒ–(Uncertainty Quantification, UQ)æ–¹æ³•ï¼Œæ¶µç›–äº†äº”å¤§æ ¸å¿ƒç±»åˆ«ï¼Œå¼¥è¡¥äº†çŽ°æœ‰ç ”ç©¶ç¼ºä¹ç»Ÿä¸€é›†æˆå·¥å…·çš„ç©ºç™½ã€‚é€šè¿‡åœ¨CodeBERTå’ŒChatGLM3æ¨¡åž‹ä¸Šè¿›è¡Œçš„ä»£ç æ¼æ´žæ£€æµ‹(Code Vulnerability Detection)ä»»åŠ¡å®žéªŒï¼Œç ”ç©¶è¯æ˜Žäº†UncertaintyZooèƒ½æœ‰æ•ˆè¯„ä¼°å¹¶æ­ç¤ºæ¨¡åž‹çš„é¢„æµ‹ç½®ä¿¡åº¦ã€‚å®žéªŒç»“æžœä¸ä»…éªŒè¯äº†è¯¥å·¥å…·åŒ…çš„å®žç”¨æ€§ï¼Œä¹Ÿä¸ºæœªæ¥åœ¨å®‰å…¨å…³é”®åœºæ™¯ä¸­éƒ¨ç½²æ›´å¯é çš„AIç³»ç»Ÿæä¾›äº†æŠ€æœ¯æ”¯æŒã€‚ç›®å‰è¯¥é¡¹ç›®å·²åœ¨GitHubå¼€æºï¼Œæ—¨åœ¨ä¿ƒè¿›ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•çš„å®žé™…åº”ç”¨ä¸Žå­¦æœ¯ç ”ç©¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06406v1",
      "published_date": "2025-12-06 11:45:50 UTC",
      "updated_date": "2025-12-06 11:45:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:16:48.457663+00:00"
    },
    {
      "arxiv_id": "2512.08986v1",
      "title": "Explainable Fundus Image Curation and Lesion Detection in Diabetic Retinopathy",
      "title_zh": "ç³–å°¿ç—…è§†ç½‘è†œç—…å˜ä¸­å¯è§£é‡Šçš„çœ¼åº•å›¾åƒæ²»ç†ä¸Žç—…å˜æ£€æµ‹",
      "authors": [
        "Anca Mihai",
        "Adrian Groza"
      ],
      "abstract": "Diabetic Retinopathy (DR) affects individuals with long-term diabetes. Without early diagnosis, DR can lead to vision loss. Fundus photography captures the structure of the retina along with abnormalities indicative of the stage of the disease. Artificial Intelligence (AI) can support clinicians in identifying these lesions, reducing manual workload, but models require high-quality annotated datasets. Due to the complexity of retinal structures, errors in image acquisition and lesion interpretation of manual annotators can occur. We proposed a quality-control framework, ensuring only high-standard data is used for evaluation and AI training. First, an explainable feature-based classifier is used to filter inadequate images. The features are extracted both using image processing and contrastive learning. Then, the images are enhanced and put subject to annotation, using deep-learning-based assistance. Lastly, the agreement between annotators calculated using derived formulas determines the usability of the annotations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç³–å°¿ç—…è§†ç½‘è†œç—…å˜ (Diabetic Retinopathy, DR) æ—©æœŸè¯Šæ–­ä¸­ AI æ¨¡åž‹é«˜åº¦ä¾èµ–é«˜è´¨é‡æ ‡æ³¨æ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„çœ¼åº•å›¾åƒæ•´ç†ä¸Žç—…å˜æ£€æµ‹è´¨é‡æŽ§åˆ¶æ¡†æž¶ã€‚è¯¥æ¡†æž¶é¦–å…ˆåˆ©ç”¨ç»“åˆäº†ä¼ ç»Ÿå›¾åƒå¤„ç†ä¸Žå¯¹æ¯”å­¦ä¹  (Contrastive Learning) ç‰¹å¾æå–çš„å¯è§£é‡Šåˆ†ç±»å™¨ï¼Œæ—¨åœ¨è‡ªåŠ¨è¯†åˆ«å¹¶è¿‡æ»¤è´¨é‡ä¸åˆæ ¼çš„çœ¼åº•å›¾åƒã€‚é€šè¿‡ç­›é€‰çš„å›¾åƒéšåŽä¼šè¿›è¡Œå¢žå¼ºå¤„ç†ï¼Œå¹¶åœ¨æ·±åº¦å­¦ä¹  (Deep Learning) è¾…åŠ©ä¸‹è¿›è¡Œäººå·¥æ ‡æ³¨ï¼Œä»¥é™ä½Žæ ‡æ³¨è¿‡ç¨‹ä¸­çš„å¤æ‚æ€§ã€‚æœ€åŽï¼Œç ”ç©¶é€šè¿‡æŽ¨å¯¼å‡ºçš„æ•°å­¦å…¬å¼è®¡ç®—æ ‡æ³¨è€…ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œä»Žè€Œç§‘å­¦åœ°ç¡®å®šæ ‡æ³¨æ•°æ®çš„å¯ç”¨æ€§ã€‚è¯¥ç³»ç»Ÿçš„æ ¸å¿ƒè´¡çŒ®åœ¨äºŽé€šè¿‡å…¨æµç¨‹çš„è´¨é‡æŠŠæŽ§ï¼Œç¡®ä¿äº†ç”¨äºŽ AI è®­ç»ƒå’Œè¯„ä¼°çš„æ•°æ®é›†è¾¾åˆ°é«˜æ ‡å‡†ï¼Œæœ‰æ•ˆå‡å°‘äº†å›¾åƒèŽ·å–å’Œäººå·¥è§£æžè¿‡ç¨‹ä¸­çš„è¯¯å·®ï¼Œä¸ºæå‡ DR æ™ºèƒ½è¯Šæ–­çš„å‡†ç¡®æ€§å¥ å®šäº†æ•°æ®åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08986v1",
      "published_date": "2025-12-06 11:36:00 UTC",
      "updated_date": "2025-12-06 11:36:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:17:00.043959+00:00"
    },
    {
      "arxiv_id": "2512.06404v1",
      "title": "GENIUS: An Agentic AI Framework for Autonomous Design and Execution of Simulation Protocols",
      "title_zh": "GENIUSï¼šä¸€ç§ç”¨äºŽè‡ªä¸»è®¾è®¡ä¸Žæ‰§è¡Œæ¨¡æ‹Ÿåè®®çš„æ™ºèƒ½ä½“ AI æ¡†æž¶",
      "authors": [
        "Mohammad Soleymanibrojeni",
        "Roland Aydin",
        "Diego Guedes-Sobrinho",
        "Alexandre C. Dias",
        "MaurÃ­cio J. Piotrowski",
        "Wolfgang Wenzel",
        "Celso Ricardo Caldeira RÃªgo"
      ],
      "abstract": "Predictive atomistic simulations have propelled materials discovery, yet routine setup and debugging still demand computer specialists. This know-how gap limits Integrated Computational Materials Engineering (ICME), where state-of-the-art codes exist but remain cumbersome for non-experts. We address this bottleneck with GENIUS, an AI-agentic workflow that fuses a smart Quantum ESPRESSO knowledge graph with a tiered hierarchy of large language models supervised by a finite-state error-recovery machine. Here we show that GENIUS translates free-form human-generated prompts into validated input files that run to completion on $\\approx$80% of 295 diverse benchmarks, where 76% are autonomously repaired, with success decaying exponentially to a 7% baseline. Compared with LLM-only baselines, GENIUS halves inference costs and virtually eliminates hallucinations. The framework democratizes electronic-structure DFT simulations by intelligently automating protocol generation, validation, and repair, opening large-scale screening and accelerating ICME design loops across academia and industry worldwide.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ææ–™å‘çŽ°ä¸­åŽŸå­æ¨¡æ‹Ÿ(atomistic simulations)å¯¹ä¸“å®¶çŸ¥è¯†çš„é«˜åº¦ä¾èµ–ä»¥åŠé›†æˆè®¡ç®—ææ–™å·¥ç¨‹(ICME)é¢ä¸´çš„know-howç¼ºå£ï¼Œæå‡ºäº†åä¸ºGENIUSçš„æ™ºèƒ½ä½“(Agentic)AIæ¡†æž¶ã€‚è¯¥æ¡†æž¶å°†Quantum ESPRESSOçŸ¥è¯†å›¾è°±ä¸Žå¤šå±‚çº§çš„å¤§è¯­è¨€æ¨¡åž‹(LLMs)ç›¸ç»“åˆï¼Œå¹¶ç”±æœ‰é™çŠ¶æ€é”™è¯¯æ¢å¤æœºåˆ¶(finite-state error-recovery machine)è¿›è¡Œç›‘ç£ï¼Œæ—¨åœ¨å®žçŽ°æ¨¡æ‹Ÿåè®®çš„è‡ªä¸»è®¾è®¡ã€éªŒè¯ä¸Žæ‰§è¡Œã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒGENIUSèƒ½å°†è‡ªç”±æ ¼å¼çš„è‡ªç„¶è¯­è¨€æç¤ºè½¬åŒ–ä¸ºæœ‰æ•ˆçš„è¾“å…¥æ–‡ä»¶ï¼Œåœ¨295ä¸ªå¤šæ ·åŒ–åŸºå‡†æµ‹è¯•ä¸­å®žçŽ°äº†çº¦80%çš„è¿è¡ŒæˆåŠŸçŽ‡ï¼Œå…¶ä¸­76%çš„ä»»åŠ¡é€šè¿‡è‡ªä¸»ä¿®å¤åŠŸèƒ½å®Œæˆã€‚ä¸Žçº¯LLMåŸºå‡†ç›¸æ¯”ï¼Œè¯¥æ¡†æž¶åœ¨åŸºæœ¬æ¶ˆé™¤å¹»è§‰(hallucinations)é—®é¢˜çš„åŒæ—¶ï¼Œå°†æŽ¨ç†æˆæœ¬é™ä½Žäº†ä¸€åŠã€‚GENIUSé€šè¿‡è‡ªåŠ¨åŒ–åè®®å…¨æµç¨‹ï¼Œæ˜¾è‘—é™ä½Žäº†ç”µå­ç»“æž„DFTæ¨¡æ‹Ÿçš„ä½¿ç”¨é—¨æ§›ï¼Œä¸ºåŠ é€Ÿå…¨çƒå­¦æœ¯ç•Œä¸Žå·¥ä¸šç•Œçš„ICMEè®¾è®¡å¾ªçŽ¯æä¾›äº†å¼ºåŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cond-mat.mtrl-sci",
        "physics.chem-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06404v1",
      "published_date": "2025-12-06 11:28:35 UTC",
      "updated_date": "2025-12-06 11:28:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:17:04.654579+00:00"
    },
    {
      "arxiv_id": "2512.06396v1",
      "title": "AgenticCyber: A GenAI-Powered Multi-Agent System for Multimodal Threat Detection and Adaptive Response in Cybersecurity",
      "title_zh": "AgenticCyberï¼šåŸºäºŽç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„ç½‘ç»œå®‰å…¨å¤šæ¨¡æ€å¨èƒæ£€æµ‹ä¸Žè‡ªé€‚åº”å“åº”å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Shovan Roy"
      ],
      "abstract": "The increasing complexity of cyber threats in distributed environments demands advanced frameworks for real-time detection and response across multimodal data streams. This paper introduces AgenticCyber, a generative AI powered multi-agent system that orchestrates specialized agents to monitor cloud logs, surveillance videos, and environmental audio concurrently. The solution achieves 96.2% F1-score in threat detection, reduces response latency to 420 ms, and enables adaptive security posture management using multimodal language models like Google's Gemini coupled with LangChain for agent orchestration. Benchmark datasets, such as AWS CloudTrail logs, UCF-Crime video frames, and UrbanSound8K audio clips, show greater performance over standard intrusion detection systems, reducing mean time to respond (MTTR) by 65% and improving situational awareness. This work introduces a scalable, modular proactive cybersecurity architecture for enterprise networks and IoT ecosystems that overcomes siloed security technologies with cross-modal reasoning and automated remediation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AgenticCyberï¼Œä¸€ç§åŸºäºŽç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (GenAI) çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºŽåˆ†å¸ƒå¼çŽ¯å¢ƒä¸­çš„å®žæ—¶å¤šæ¨¡æ€å¨èƒæ£€æµ‹ä¸Žè‡ªé€‚åº”å“åº”ã€‚è¯¥æ–¹æ¡ˆé€šè¿‡ LangChain ç¼–æŽ’å¤šä¸ªç”± Google Gemini é©±åŠ¨çš„ä¸“ä¸šæ™ºèƒ½ä½“ï¼Œå®žçŽ°äº†å¯¹äº‘æ—¥å¿— (Cloud logs)ã€ç›‘æŽ§è§†é¢‘åŠçŽ¯å¢ƒéŸ³é¢‘çš„å¹¶å‘ç›‘æµ‹ä¸Žè·¨æ¨¡æ€æŽ¨ç† (Cross-modal reasoning)ã€‚å®žéªŒç»“æžœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨å¨èƒæ£€æµ‹ä¸­å–å¾—äº† 96.2% çš„ F1-scoreï¼Œå¹¶å°†å“åº”å»¶è¿Ÿä¼˜åŒ–è‡³ 420 æ¯«ç§’ï¼Œç›¸æ¯”ä¼ ç»Ÿå…¥ä¾µæ£€æµ‹ç³»ç»Ÿæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚é€šè¿‡åœ¨ AWS CloudTrailã€UCF-Crime å’Œ UrbanSound8K ç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„æµ‹è¯•ï¼ŒAgenticCyber æˆåŠŸå°†å¹³å‡å“åº”æ—¶é—´ (MTTR) é™ä½Žäº† 65%ã€‚è¯¥ç ”ç©¶ä¸ºä¼ä¸šç½‘ç»œä¸Žç‰©è”ç½‘ (IoT) æä¾›äº†å¯æ‰©å±•çš„æ¨¡å—åŒ–æž¶æž„ï¼Œåˆ©ç”¨è‡ªåŠ¨åŒ–ä¿®å¤èƒ½åŠ›æœ‰æ•ˆå…‹æœäº†ä¼ ç»Ÿç¢Žç‰‡åŒ–å®‰å…¨æŠ€æœ¯çš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "6 pages for IEEE conference",
      "pdf_url": "https://arxiv.org/pdf/2512.06396v1",
      "published_date": "2025-12-06 10:59:21 UTC",
      "updated_date": "2025-12-06 10:59:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:17:10.661942+00:00"
    },
    {
      "arxiv_id": "2512.11872v1",
      "title": "WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving",
      "title_zh": "WAM-Diffï¼šèžåˆæ··åˆä¸“å®¶æ¨¡åž‹ä¸Žåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„è‡ªåŠ¨é©¾é©¶æŽ©ç æ‰©æ•£ VLA æ¡†æž¶",
      "authors": [
        "Mingwang Xu",
        "Jiahao Cui",
        "Feipeng Cai",
        "Hanlin Shang",
        "Zhihao Zhu",
        "Shan Luan",
        "Yifang Xu",
        "Neng Zhang",
        "Yaoyi Li",
        "Jia Cai",
        "Siyu Zhu"
      ],
      "abstract": "End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: https://github.com/fudan-generative-vision/WAM-Diff",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† WAM-Diffï¼Œä¸€ç§åŸºäºŽ Masked Diffusion çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œ (VLA) æ¡†æž¶ï¼Œä¸“é—¨ç”¨äºŽè‡ªåŠ¨é©¾é©¶ä¸­çš„è§„åˆ’ä¸ŽæŽ§åˆ¶ä¿¡å·ç”Ÿæˆã€‚è¯¥æ¡†æž¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºŽåˆ©ç”¨ç¦»æ•£ Masked Diffusion è¿­ä»£ä¼˜åŒ–æœªæ¥è½¨è¿¹åºåˆ—ï¼Œå¹¶æ”¯æŒçµæ´»çš„éžå› æžœè§£ç é¡ºåºã€‚åœ¨æ¨¡åž‹æž¶æž„ä¸Šï¼ŒWAM-Diff é‡‡ç”¨äº†ç¨€ç–æ··åˆä¸“å®¶ (MoE) æž¶æž„ï¼Œé€šè¿‡è¿åŠ¨é¢„æµ‹ä¸Žé©¾é©¶å¯¼å‘çš„è§†è§‰é—®ç­” (VQA) ä»»åŠ¡è¿›è¡Œè”åˆè®­ç»ƒä»¥æå‡æ¨¡åž‹å®¹é‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†åŸºäºŽç¾¤ç»„åºåˆ—ç­–ç•¥ä¼˜åŒ– (GSPO) çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œæ—¨åœ¨ä¼˜åŒ–åºåˆ—çº§çš„é©¾é©¶å¥–åŠ±ã€‚å®žéªŒæ•°æ®è¡¨æ˜Žï¼Œè¯¥æ¨¡åž‹åœ¨ NAVSIM-v1 å’Œ NAVSIM-v2 åŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«å–å¾—äº† 91.0 PDMS å’Œ 89.7 EPDMS çš„ä¼˜å¼‚æˆç»©ã€‚è¿™é¡¹å·¥ä½œè¯æ˜Žäº† Masked Diffusion åœ¨è‡ªåŠ¨é©¾é©¶è½¨è¿¹ç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºåŸºäºŽ VLA çš„ç«¯åˆ°ç«¯é©¾é©¶ç³»ç»Ÿæä¾›äº†ä¸€ç§å…·æœ‰åœºæ™¯æ„ŸçŸ¥ (Scenario-aware) è§£ç èƒ½åŠ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.11872v1",
      "published_date": "2025-12-06 10:51:53 UTC",
      "updated_date": "2025-12-06 10:51:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:17:14.261384+00:00"
    },
    {
      "arxiv_id": "2512.06393v2",
      "title": "Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression",
      "title_zh": "å°‘å³æ˜¯å¤šï¼šè§„åˆ™ç§»é™¤ã€æ”¹å†™ä¸ŽåŽ‹ç¼©ä¸‹å¤§è¯­è¨€æ¨¡åž‹å¤šæ­¥é€»è¾‘æŽ¨ç†çš„æ³›åŒ–ç ”ç©¶",
      "authors": [
        "Qiming Bao",
        "Xiaoxuan Fu"
      ],
      "abstract": "Large language models (LLMs) achieve strong performance on many natural language tasks, yet their generalisation under structured perturbations of logical rule systems remains insufficiently characterised. We present a controlled evaluation framework that probes reasoning reliability through four stress tests: (1) rule deletion, removing redundant versus essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites based on equivalence laws (contraposition, double negation, implication-to-disjunction, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that composes 2--5 transformations. Across three representative model families -- BERT, Qwen2, and LLaMA-like models -- all models attain Acc$=1.0000$ on the base split and show no degradation under redundant rule deletion. In contrast, essential rule deletion yields a pronounced decrease to near-chance performance, and injecting explicit contradictions reduces accuracy to 0.0000. Under logic-preserving rewrites, accuracy is largely preserved for single-law transformations with only small degradations in a few cases, whereas multi-law stacking exposes model-dependent sensitivity: BERT matches the base condition, TinyLlama shows only marginal degradation, and Qwen2 exhibits a substantial drop.\n  Overall, the results indicate that contemporary LLMs are generally stable under semantic-preserving reformulations, yet remain brittle to missing or inconsistent evidence and may degrade under composed logical transformations depending on the model family. The proposed framework provides a concise diagnostic tool for isolating these failure modes and for evaluating logical generalisation beyond surface-form variation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå—æŽ§è¯„ä¼°æ¡†æž¶ï¼Œæ—¨åœ¨æŽ¢è®¨å¤§è¯­è¨€æ¨¡åž‹ (LLMs) åœ¨é€»è¾‘è§„åˆ™ç³»ç»Ÿç»“æž„åŒ–æ‰°åŠ¨ä¸‹çš„æ³›åŒ–èƒ½åŠ›å’ŒæŽ¨ç†å¯é æ€§ã€‚è¯¥æ¡†æž¶é€šè¿‡å››é¡¹åŽ‹åŠ›æµ‹è¯•å¯¹ BERTã€Qwen2 å’Œ LLaMA ç­‰æ¨¡åž‹è¿›è¡Œè¯„ä¼°ï¼Œæ¶µç›–äº†è§„åˆ™åˆ é™¤ã€çŸ›ç›¾è¯æ®æ³¨å…¥ã€åŸºäºŽç­‰ä»·å¾‹çš„é€»è¾‘ä¿æŒæ”¹å†™ä»¥åŠå¤šå®šå¾‹ç­‰ä»·å †å ã€‚å®žéªŒç»“æžœæ˜¾ç¤ºï¼Œæ‰€æœ‰æ¨¡åž‹åœ¨åˆ é™¤å†—ä½™è§„åˆ™æ—¶è¡¨çŽ°ç¨³å®šï¼Œä½†åœ¨åˆ é™¤æ ¸å¿ƒ (essential) è§„åˆ™æ—¶æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™è‡³æŽ¥è¿‘éšæœºæ°´å¹³ï¼Œè€Œåœ¨æ³¨å…¥æ˜¾å¼çŸ›ç›¾æ—¶å‡†ç¡®çŽ‡åˆ™é™è‡³é›¶ã€‚åœ¨é€»è¾‘ä¿æŒæ”¹å†™æµ‹è¯•ä¸­ï¼Œå•å®šå¾‹è½¬æ¢å¯¹æ¨¡åž‹å‡†ç¡®çŽ‡å½±å“è¾ƒå°ï¼Œä½†å¤šå®šå¾‹å †å  (multi-law stacking) æ­ç¤ºäº†æ¨¡åž‹é—´çš„æ˜¾è‘—å·®å¼‚ï¼Œå…¶ä¸­ Qwen2 è¡¨çŽ°å‡ºæ˜Žæ˜¾çš„æ€§èƒ½ä¸‹æ»‘ã€‚æ€»ä½“è€Œè¨€ï¼Œå½“ä»£ LLMs åœ¨è¯­ä¹‰ä¿æŒçš„é‡è¿°ä¸‹å…·æœ‰è¾ƒå¥½çš„ç¨³å®šæ€§ï¼Œä½†å¯¹äºŽè¯æ®ç¼ºå¤±ã€ä¸ä¸€è‡´æˆ–å¤æ‚çš„ç»„åˆé€»è¾‘è½¬æ¢ä»è¡¨çŽ°å‡ºè„†å¼±æ€§ã€‚è¯¥æ¡†æž¶ä¸ºéš”ç¦»è¿™äº›å¤±æ•ˆæ¨¡å¼ä»¥åŠè¯„ä¼°è¶…è¶Šè¡¨é¢å½¢å¼å˜åŒ–çš„é€»è¾‘æ³›åŒ–èƒ½åŠ›æä¾›äº†ä¸€ä¸ªç®€æ˜Žçš„è¯Šæ–­å·¥å…·ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06393v2",
      "published_date": "2025-12-06 10:49:50 UTC",
      "updated_date": "2025-12-12 09:31:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:17:27.150035+00:00"
    },
    {
      "arxiv_id": "2512.06392v2",
      "title": "RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs",
      "title_zh": "RLAXï¼šé¢å‘ TPU çš„å¤§è¯­è¨€æ¨¡åž‹å¤§è§„æ¨¡åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Runlong Zhou",
        "Lefan Zhang",
        "Shang-Chen Wu",
        "Kelvin Zou",
        "Hanzhi Zhou",
        "Ke Ye",
        "Yihao Feng",
        "Dong Yin",
        "Alex Guillen Garcia",
        "Dmytro Babych",
        "Rohit Chatterjee",
        "Matthew Hopkins",
        "Xiang Kong",
        "Chang Lan",
        "Lezhi Li",
        "Yiping Ma",
        "Daniele Molinari",
        "Senyu Tong",
        "Yanchao Sun",
        "Thomas Voice",
        "Jianyu Wang",
        "Chong Wang",
        "Simon Wang",
        "Floris Weers",
        "Yechen Xu",
        "Guolin Yin",
        "Muyang Yu",
        "Yi Zhang",
        "Zheng Zhou",
        "Danyang Zhuo",
        "Ruoming Pang",
        "Cheng Leong"
      ],
      "abstract": "Reinforcement learning (RL) has emerged as the de-facto paradigm for improving the reasoning capabilities of large language models (LLMs). We have developed RLAX, a scalable RL framework on TPUs. RLAX employs a parameter-server architecture. A master trainer periodically pushes updated model weights to the parameter server while a fleet of inference workers pull the latest weights and generates new rollouts. We introduce a suite of system techniques to enable scalable and preemptible RL for a diverse set of state-of-art RL algorithms. To accelerate convergence and improve model quality, we have devised new dataset curation and alignment techniques. Large-scale evaluations show that RLAX improves QwQ-32B's pass@8 accuracy by 12.8% in just 12 hours 48 minutes on 1024 v5p TPUs, while remaining robust to preemptions during training.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RLAXï¼Œä¸€ä¸ªä¸“ä¸º TPU è®¾è®¡çš„å¤§è§„æ¨¡åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æ¡†æž¶ï¼Œæ—¨åœ¨æ˜¾è‘—æå‡å¤§è¯­è¨€æ¨¡åž‹ (LLMs) çš„æŽ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æž¶é‡‡ç”¨äº†å‚æ•°æœåŠ¡å™¨ (parameter-server) æž¶æž„ï¼Œé€šè¿‡ä¸»è®­ç»ƒå™¨å®šæœŸæŽ¨é€æ›´æ–°æƒé‡ï¼Œå¹¶ç”±æŽ¨ç†å·¥ä½œé›†ç¾¤æ‹‰å–æœ€æ–°æƒé‡ä»¥ç”Ÿæˆ Rolloutsã€‚RLAX å¼•å…¥äº†ä¸€ç³»åˆ—ç³»ç»ŸæŠ€æœ¯ï¼Œå®žçŽ°äº†å¤šç§å‰æ²¿å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å¯æ‰©å±•æ€§ä¸Žå¯æŠ¢å æ€§ (preemptible)ï¼Œå¹¶ç»“åˆåˆ›æ–°çš„æ•°æ®é›†ç®¡ç† (dataset curation) å’Œå¯¹é½æŠ€æœ¯æ¥åŠ é€Ÿæ¨¡åž‹æ”¶æ•›ã€‚åœ¨å¤§è§„æ¨¡è¯„ä¼°ä¸­ï¼ŒRLAX åœ¨ 1024 ä¸ª v5p TPU ä¸Šä»…ç”¨ 12 å°æ—¶ 48 åˆ†é’Ÿå°±å°† QwQ-32B çš„ pass@8 å‡†ç¡®çŽ‡æå‡äº† 12.8%ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹èµ„æºæŠ¢å è¡¨çŽ°å‡ºæžå¼ºçš„é²æ£’æ€§ï¼Œä¸ºå¤§è§„æ¨¡æ¨¡åž‹ä¼˜åŒ–æä¾›äº†é«˜æ•ˆä¸”å¯é çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "The submission is being withdrawn because internal stakeholders determined that it is not appropriate to publish work on this topic at this time",
      "pdf_url": "https://arxiv.org/pdf/2512.06392v2",
      "published_date": "2025-12-06 10:48:51 UTC",
      "updated_date": "2025-12-11 02:38:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:17:20.132200+00:00"
    },
    {
      "arxiv_id": "2512.06390v1",
      "title": "Web Technologies Security in the AI Era: A Survey of CDN-Enhanced Defenses",
      "title_zh": "AI æ—¶ä»£çš„ Web æŠ€æœ¯å®‰å…¨ï¼šCDN å¢žå¼ºåž‹é˜²å¾¡æŠ€æœ¯ç»¼è¿°",
      "authors": [
        "Mehrab Hosain",
        "Sabbir Alom Shuvo",
        "Matthew Ogbe",
        "Md Shah Jalal Mazumder",
        "Yead Rahman",
        "Md Azizul Hakim",
        "Anukul Pandey"
      ],
      "abstract": "The modern web stack, which is dominated by browser-based applications and API-first backends, now operates under an adversarial equilibrium where automated, AI-assisted attacks evolve continuously. Content Delivery Networks (CDNs) and edge computing place programmable defenses closest to users and bots, making them natural enforcement points for machine-learning (ML) driven inspection, throttling, and isolation. This survey synthesizes the landscape of AI-enhanced defenses deployed at the edge: (i) anomaly- and behavior-based Web Application Firewalls (WAFs) within broader Web Application and API Protection (WAAP), (ii) adaptive DDoS detection and mitigation, (iii) bot management that resists human-mimicry, and (iv) API discovery, positive security modeling, and encrypted-traffic anomaly analysis. We add a systematic survey method, a threat taxonomy mapped to edge-observable signals, evaluation metrics, deployment playbooks, and governance guidance. We conclude with a research agenda spanning XAI, adversarial robustness, and autonomous multi-agent defense. Our findings indicate that edge-centric AI measurably improves time-to-detect and time-to-mitigate while reducing data movement and enhancing compliance, yet introduces new risks around model abuse, poisoning, and governance.",
      "tldr_zh": "è¯¥ç»¼è¿°ç ”ç©¶æŽ¢è®¨äº†åœ¨AIæ—¶ä»£èƒŒæ™¯ä¸‹ï¼Œåˆ©ç”¨å†…å®¹åˆ†å‘ç½‘ç»œ(CDN)å’Œè¾¹ç¼˜è®¡ç®—(Edge Computing)å¢žå¼ºWebæŠ€æœ¯å®‰å…¨æ€§çš„é˜²å¾¡ä½“ç³»ã€‚é¢å¯¹è‡ªåŠ¨åŒ–ä¸”ä¸æ–­è¿›åŒ–çš„AIè¾…åŠ©æ”»å‡»ï¼Œç ”ç©¶å¼ºè°ƒäº†åœ¨è¾¹ç¼˜ç«¯éƒ¨ç½²æœºå™¨å­¦ä¹ (ML)é©±åŠ¨çš„æ£€æŸ¥ã€é™æµå’Œéš”ç¦»æœºåˆ¶çš„é‡è¦æ€§ã€‚æ–‡ç« ç³»ç»Ÿæ€§åœ°æ¢³ç†äº†è¾¹ç¼˜ä¾§AIå¢žå¼ºé˜²å¾¡çš„åº”ç”¨ï¼Œæ¶µç›–äº†åŸºäºŽå¼‚å¸¸è¡Œä¸ºçš„Webåº”ç”¨é˜²ç«å¢™(WAF)ã€è‡ªé€‚åº”DDoSæ£€æµ‹ã€æœºå™¨äººç®¡ç†(Bot Management)ä»¥åŠAPIå‘çŽ°ä¸ŽåŠ å¯†æµé‡åˆ†æžã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†æ˜ å°„è‡³è¾¹ç¼˜ä¿¡å·çš„å¨èƒåˆ†ç±»æ³•(Threat Taxonomy)ã€è¯„ä¼°æŒ‡æ ‡åŠæ²»ç†æŒ‡å—ã€‚ä½œè€…æœ€åŽæå‡ºäº†æ¶µç›–å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)ã€å¯¹æŠ—é²æ£’æ€§(Adversarial Robustness)å’Œè‡ªä¸»å¤šæ™ºèƒ½ä½“é˜²å¾¡çš„ç ”ç©¶è®®ç¨‹ã€‚ç»“æžœæ˜¾ç¤ºï¼Œä»¥è¾¹ç¼˜ä¸ºä¸­å¿ƒçš„AIæ˜¾è‘—æå‡äº†æ£€æµ‹ä¸Žç¼“è§£æ•ˆçŽ‡å¹¶å¢žå¼ºäº†åˆè§„æ€§ï¼Œä½†åŒæ—¶ä¹Ÿå¸¦æ¥äº†æ¨¡åž‹æ»¥ç”¨å’Œä¸­æ¯’(Poisoning)ç­‰å®‰å…¨é£Žé™©ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.NI",
        "cs.PF"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at 2025 IEEE Asia Pacific Conference on Wireless and Mobile (APWiMob). 7 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.06390v1",
      "published_date": "2025-12-06 10:42:14 UTC",
      "updated_date": "2025-12-06 10:42:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:17:28.345725+00:00"
    },
    {
      "arxiv_id": "2512.06380v2",
      "title": "Protecting Bystander Privacy via Selective Hearing in Audio LLMs",
      "title_zh": "åŸºäºŽé€‰æ‹©æ€§æ”¶å¬çš„éŸ³é¢‘å¤§è¯­è¨€æ¨¡åž‹æ—è§‚è€…éšç§ä¿æŠ¤",
      "authors": [
        "Xiao Zhan",
        "Guangzhi Sun",
        "Jose Such",
        "Phil Woodland"
      ],
      "abstract": "Audio Large language models (LLMs) are increasingly deployed in the real world, where they inevitably capture speech from unintended nearby bystanders, raising privacy risks that existing benchmarks and defences did not consider. We introduce SH-Bench, the first benchmark designed to evaluate selective hearing: a model's ability to attend to an intended main speaker while refusing to process or reveal information about incidental bystander speech. SH-Bench contains 3,968 multi-speaker audio mixtures, including both real-world and synthetic scenarios, paired with 77k multiple-choice questions that probe models under general and selective operating modes. In addition, we propose Selective Efficacy (SE), a novel metric capturing both multi-speaker comprehension and bystander-privacy protection. Our evaluation of state-of-the-art open-source and proprietary LLMs reveals substantial bystander privacy leakage, with strong audio understanding failing to translate into selective protection of bystander privacy. To mitigate this gap, we also present Bystander Privacy Fine-Tuning (BPFT), a novel training pipeline that teaches models to refuse bystander-related queries without degrading main-speaker comprehension. We show that BPFT yields substantial gains, achieving an absolute 47% higher bystander accuracy under selective mode and an absolute 16% higher SE compared to Gemini 2.5 Pro, which is the best audio LLM without BPFT. Together, SH-Bench and BPFT provide the first systematic framework for measuring and improving bystander privacy in audio LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æŽ¢è®¨äº†éŸ³é¢‘å¤§è¯­è¨€æ¨¡åž‹ (Audio LLMs) åœ¨çŽ°å®žéƒ¨ç½²ä¸­æ•æ‰æ—è§‚è€…è¯­éŸ³æ‰€å¼•å‘çš„éšç§é£Žé™©ï¼Œå¹¶æå‡ºäº†é¦–ä¸ªç”¨äºŽè¯„ä¼°æ¨¡åž‹â€œé€‰æ‹©æ€§å¬å–â€ (selective hearing) èƒ½åŠ›çš„åŸºå‡†æµ‹è¯• SH-Benchã€‚SH-Bench åŒ…å« 3,968 ä¸ªå¤šå‘è¨€äººéŸ³é¢‘æ··åˆç¤ºä¾‹åŠ 7.7 ä¸‡ä¸ªæµ‹è¯•é¢˜ï¼Œæ—¨åœ¨è€ƒå¯Ÿæ¨¡åž‹åœ¨å…³æ³¨ä¸»å‘è¨€äººçš„åŒæ—¶æ˜¯å¦èƒ½æœ‰æ•ˆæ‹’ç»å¤„ç†æˆ–æ³„éœ²æ—è§‚è€…ä¿¡æ¯ã€‚ç ”ç©¶è€…è¿˜å¼•å…¥äº†åä¸º Selective Efficacy (SE) çš„æ–°æŒ‡æ ‡ï¼Œä»¥ç»¼åˆè¯„ä¼°æ¨¡åž‹çš„å¤šå‘è¨€äººç†è§£èƒ½åŠ›ä¸Žæ—è§‚è€…éšç§ä¿æŠ¤æ°´å¹³ã€‚é’ˆå¯¹çŽ°æœ‰æœ€å…ˆè¿›æ¨¡åž‹åœ¨éšç§ä¿æŠ¤æ–¹é¢çš„æ˜¾è‘—ç¼ºé™·ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº† Bystander Privacy Fine-Tuning (BPFT) å¾®è°ƒæµæ°´çº¿ï¼Œæ—¨åœ¨è®­ç»ƒæ¨¡åž‹æ‹’ç»æ—è§‚è€…ç›¸å…³çš„æŸ¥è¯¢ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒBPFT åœ¨ä¸é™ä½Žä¸»å‘è¨€äººç†è§£è´¨é‡çš„æƒ…å†µä¸‹ï¼Œå…¶æ—è§‚è€…å‡†ç¡®çŽ‡å’Œ SE æŒ‡æ ‡è¾ƒ Gemini 2.5 Pro åˆ†åˆ«å®žçŽ°äº† 47% å’Œ 16% çš„ç»å¯¹æå‡ã€‚è¯¥ç ”ç©¶ä¸ºæå‡ Audio LLMs çš„æ—è§‚è€…éšç§å®‰å…¨æ€§æä¾›äº†ç¬¬ä¸€ä¸ªç³»ç»ŸåŒ–çš„æµ‹é‡ä¸Žæ”¹è¿›æ¡†æž¶ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "Dataset: https://huggingface.co/datasets/BrianatCambridge/SelectiveHearingBench",
      "pdf_url": "https://arxiv.org/pdf/2512.06380v2",
      "published_date": "2025-12-06 10:24:04 UTC",
      "updated_date": "2025-12-13 14:48:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:18:26.374211+00:00"
    },
    {
      "arxiv_id": "2512.06357v2",
      "title": "Proportional integral derivative booster for neural networks-based time-series prediction: Case of water demand prediction",
      "title_zh": "ç”¨äºŽç¥žç»ç½‘ç»œæ—¶é—´åºåˆ—é¢„æµ‹çš„æ¯”ä¾‹ç§¯åˆ†å¾®åˆ†å¢žå¼ºå™¨ï¼šä»¥æ°´éœ€æ±‚é¢„æµ‹ä¸ºä¾‹",
      "authors": [
        "Tony Salloom",
        "Okyay Kaynak",
        "Xinbo Yub",
        "Wei He"
      ],
      "abstract": "Multi-step time-series prediction is an essential supportive step for decision-makers in several industrial areas. Artificial intelligence techniques, which use a neural network component in various forms, have recently frequently been used to accomplish this step. However, the complexity of the neural network structure still stands up as a critical problem against prediction accuracy. In this paper, a method inspired by the proportional-integral-derivative (PID) control approach is investigated to enhance the performance of neural network models used for multi-step ahead prediction of periodic time-series information while maintaining a negligible impact on the complexity of the system. The PID-based method is applied to the predicted value at each time step to bring that value closer to the real value. The water demand forecasting problem is considered as a case study, where two deep neural network models from the literature are used to prove the effectiveness of the proposed boosting method. Furthermore, to prove the applicability of this PID-based booster to other types of periodic time-series prediction problems, it is applied to enhance the accuracy of a neural network model used for multi-step forecasting of hourly energy consumption. The comparison between the results of the original prediction models and the results after using the proposed technique demonstrates the superiority of the proposed method in terms of prediction accuracy and system complexity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å—æ¯”ä¾‹-ç§¯åˆ†-å¾®åˆ†(PID)æŽ§åˆ¶æ–¹æ³•å¯å‘çš„å¢žå¼ºå™¨(Booster)ï¼Œæ—¨åœ¨æå‡åŸºäºŽç¥žç»ç½‘ç»œ(Neural Networks)çš„å‘¨æœŸæ€§æ—¶é—´åºåˆ—å¤šæ­¥é¢„æµ‹æ€§èƒ½ã€‚é’ˆå¯¹ç¥žç»ç½‘ç»œç»“æž„å¤æ‚æ€§åˆ¶çº¦é¢„æµ‹ç²¾åº¦çš„å…³é”®é—®é¢˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç³»ç»Ÿå¤æ‚æ€§å¢žåŠ æžå°çš„å‰æä¸‹ï¼Œå¯¹æ¯ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹å€¼è¿›è¡Œä¿®æ­£ï¼Œä½¿å…¶æ›´æŽ¥è¿‘çœŸå®žå€¼ã€‚ç ”ç©¶ä»¥ç”¨æ°´éœ€æ±‚é¢„æµ‹(Water demand forecasting)ä¸ºæ¡ˆä¾‹ï¼Œé€šè¿‡ä¸¤ç§æ·±åº¦ç¥žç»ç½‘ç»œæ¨¡åž‹éªŒè¯äº†è¯¥å¢žå¼ºæŠ€æœ¯çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œä¸ºè¯æ˜Žè¯¥PID-based boosterçš„æ™®é€‚æ€§ï¼Œç ”ç©¶è¿˜å°†å…¶åº”ç”¨äºŽæ¯å°æ—¶èƒ½æºæ¶ˆè€—çš„å¤šæ­¥é¢„æµ‹ã€‚å¯¹æ¯”å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ¡ˆåœ¨é¢„æµ‹ç²¾åº¦å’Œç³»ç»Ÿå¤æ‚æ€§æ–¹é¢å‡ä¼˜äºŽåŽŸå§‹é¢„æµ‹æ¨¡åž‹ï¼Œä¸ºå·¥ä¸šé¢†åŸŸçš„å†³ç­–æ”¯æŒæä¾›äº†æ›´ç²¾ç¡®ä¸”é«˜æ•ˆçš„å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "Engineering Applications of Artificial Intelligence 2022",
      "pdf_url": "https://arxiv.org/pdf/2512.06357v2",
      "published_date": "2025-12-06 09:08:05 UTC",
      "updated_date": "2025-12-10 07:03:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:17:45.731027+00:00"
    },
    {
      "arxiv_id": "2512.06350v1",
      "title": "Why They Disagree: Decoding Differences in Opinions about AI Risk on the Lex Fridman Podcast",
      "title_zh": "åˆ†æ­§ä¹‹æºï¼šè§£æž Lex Fridman æ’­å®¢ä¸­å…³äºŽäººå·¥æ™ºèƒ½é£Žé™©çš„è§‚ç‚¹å·®å¼‚",
      "authors": [
        "Nghi Truong",
        "Phanish Puranam",
        "Ã–zgecan KoÃ§ak"
      ],
      "abstract": "The emergence of transformative technologies often surfaces deep societal divisions, nowhere more evident than in contemporary debates about artificial intelligence (AI). A striking feature of these divisions is that they persist despite shared interests in ensuring that AI benefits humanity and avoiding catastrophic outcomes. This paper analyzes contemporary debates about AI risk, parsing the differences between the \"doomer\" and \"boomer\" perspectives into definitional, factual, causal, and moral premises to identify key points of contention. We find that differences in perspectives about existential risk (\"X-risk\") arise fundamentally from differences in causal premises about design vs. emergence in complex systems, while differences in perspectives about employment risks (\"E-risks\") pertain to different causal premises about the applicability of past theories (evolution) vs their inapplicability (revolution). Disagreements about these two forms of AI risk appear to share two properties: neither involves significant disagreements on moral values and both can be described in terms of differing views on the extent of boundedness of human rationality. Our approach to analyzing reasoning chains at scale, using an ensemble of LLMs to parse textual data, can be applied to identify key points of contention in debates about risk to the public in any arena.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡åˆ†æž Lex Fridman Podcast ä¸­å…³äºŽäººå·¥æ™ºèƒ½é£Žé™©çš„è¾©è®ºï¼Œå°† \"doomer\" ä¸Ž \"boomer\" è§‚ç‚¹ä¹‹é—´çš„åˆ†æ­§è§£æžä¸ºå®šä¹‰ã€äº‹å®žã€å› æžœå’Œé“å¾·å‰æã€‚ç ”ç©¶å‘çŽ°ï¼Œå…³äºŽç”Ÿå­˜é£Žé™© (X-risk) çš„åˆ†æ­§æ ¹æœ¬ä¸ŠæºäºŽå¯¹å¤æ‚ç³»ç»Ÿä¸­è®¾è®¡ (design) ä¸Žæ¶ŒçŽ° (emergence) ä¸åŒçš„å› æžœå‰æï¼Œè€Œå…³äºŽå°±ä¸šé£Žé™© (E-risks) çš„åˆ†æ­§åˆ™æ¶‰åŠè¿‡åŽ»ç†è®ºçš„é€‚ç”¨æ€§ï¼ˆæ¼”åŒ– vs é©å‘½ï¼‰ã€‚è¿™ä¸¤ç±» AI é£Žé™©çš„äº‰è®ºåœ¨é“å¾·ä»·å€¼è§‚ä¸Šå¹¶æ— æ˜¾è‘—å·®å¼‚ï¼Œæœ¬è´¨ä¸Šéƒ½åæ˜ äº†å¯¹äººç±»ç†æ€§å±€é™æ€§ (boundedness of human rationality) ç¨‹åº¦çš„ä¸åŒçœ‹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é›†æˆ LLMs å¤§è§„æ¨¡è§£æžæ–‡æœ¬æ•°æ®å¹¶åˆ†æžæŽ¨ç†é“¾ï¼Œä¸ºè¯†åˆ«ä»»ä½•å…¬å…±é¢†åŸŸé£Žé™©è¾©è®ºä¸­çš„å…³é”®äº‰è®®ç‚¹æä¾›äº†é€šç”¨æ¡†æž¶ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06350v1",
      "published_date": "2025-12-06 08:48:30 UTC",
      "updated_date": "2025-12-06 08:48:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:17:36.624213+00:00"
    },
    {
      "arxiv_id": "2512.06343v1",
      "title": "When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models",
      "title_zh": "å½“è·ç¦»æˆä¸ºå¹²æ‰°ï¼šå¥–åŠ±æ¨¡åž‹ BT æŸå¤±ä¸­çš„è¡¨ç¤ºè·ç¦»åå·®",
      "authors": [
        "Tong Xie",
        "Andrew Bai",
        "Yuanhao Ban",
        "Yunqi Hong",
        "Haoyu Li",
        "Cho-jui Hsieh"
      ],
      "abstract": "Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥åˆ†æžäº†å¤§è§„æ¨¡è¯­è¨€æ¨¡åž‹(LLM)å¯¹é½ä¸­æ ¸å¿ƒçš„Bradley-Terry (BT) lossï¼Œæ­ç¤ºäº†å…¶æ¢¯åº¦å—æ ·æœ¬é—´representation distanceï¼ˆè¡¨ç¤ºè·ç¦»ï¼‰æ˜¾è‘—å½±å“è€Œäº§ç”Ÿçš„åç½®é—®é¢˜ã€‚è¿™ç§åç½®ä½¿å¾—è¡¨ç¤ºè·ç¦»è¾ƒå¤§çš„æ ·æœ¬å¯¹äº§ç”Ÿè¿‡å¼ºæ¢¯åº¦ï¼Œä»Žè€ŒæŽ©ç›–äº†éœ€è¦ç²¾ç»†åŒºåˆ†çš„çŸ­è·ç¦»æ ·æœ¬å¯¹çš„ä¿¡å·ï¼Œå¯¼è‡´å¥–åŠ±æ¨¡åž‹åœ¨å¤æ‚ä»»åŠ¡ä¸­å­¦ä¹ å¤±å‡†ã€‚é’ˆå¯¹è¿™ä¸€å±€é™ï¼Œä½œè€…æå‡ºäº†NormBTï¼Œä¸€ç§è½»é‡çº§ä¸”å³æ’å³ç”¨çš„è‡ªé€‚åº”æˆå¯¹å½’ä¸€åŒ–æ–¹æ¡ˆï¼Œæ—¨åœ¨å¹³è¡¡è¡¨ç¤ºé©±åŠ¨æ•ˆåº”å¹¶ä½¿å­¦ä¹ ä¿¡å·èšç„¦äºŽé¢„æµ‹è¯¯å·®ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒNormBTåœ¨å¤šç§æ¨¡åž‹æž¶æž„å’Œæ•°æ®é›†ä¸Šå‡èƒ½æŒç»­æå‡å¥–åŠ±æ¨¡åž‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ…å«å¤§é‡çŸ­è·ç¦»æ ·æœ¬çš„RewardBenchæŽ¨ç†(Reasoning)ç±»åˆ«ä¸­å®žçŽ°äº†è¶…è¿‡5%çš„æ˜¾è‘—æå‡ã€‚è¯¥é¡¹å·¥ä½œä¸ä»…æ­ç¤ºäº†å¹¿æ³›ä½¿ç”¨çš„BTæŸå¤±å‡½æ•°çš„å†…åœ¨ç¼ºé™·ï¼Œè¿˜ä¸ºæž„å»ºæ›´ç²¾å‡†çš„å¥–åŠ±æ¨¡åž‹æä¾›äº†ä¸€ç§ç®€å•é«˜æ•ˆçš„æ”¹è¿›è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06343v1",
      "published_date": "2025-12-06 08:15:37 UTC",
      "updated_date": "2025-12-06 08:15:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:18:42.447554+00:00"
    },
    {
      "arxiv_id": "2512.06337v2",
      "title": "DaGRPO: Rectifying Gradient Conflict in Reasoning via Distinctiveness-Aware Group Relative Policy Optimization",
      "title_zh": "DaGRPOï¼šé€šè¿‡åŒºåˆ†åº¦æ„ŸçŸ¥ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çº æ­£æŽ¨ç†ä¸­çš„æ¢¯åº¦å†²çª",
      "authors": [
        "Xuan Xie",
        "Xuan Wang",
        "Wenjie Wang",
        "Shuai Chen",
        "Wei Lin"
      ],
      "abstract": "The evolution of Large Language Models (LLMs) has catalyzed a paradigm shift from superficial instruction following to rigorous long-horizon reasoning. While Group Relative Policy Optimization (GRPO) has emerged as a pivotal mechanism for eliciting such post-training reasoning capabilities due to its exceptional performance, it remains plagued by significant training instability and poor sample efficiency. We theoretically identify the root cause of these issues as the lack of distinctiveness within on-policy rollouts: for routine queries, highly homogeneous samples induce destructive gradient conflicts; whereas for hard queries, the scarcity of valid positive samples results in ineffective optimization. To bridge this gap, we propose Distinctiveness-aware Group Relative Policy Optimization (DaGRPO). DaGRPO incorporates two core mechanisms: (1) Sequence-level Gradient Rectification, which utilizes fine-grained scoring to dynamically mask sample pairs with low distinctiveness, thereby eradicating gradient conflicts at the source; and (2) Off-policy Data Augmentation, which introduces high-quality anchors to recover training signals for challenging tasks. Extensive experiments across 9 mathematical reasoning and out-of-distribution (OOD) generalization benchmarks demonstrate that DaGRPO significantly surpasses existing SFT, GRPO, and hybrid baselines, achieving new state-of-the-art performance (e.g., a +4.7% average accuracy gain on math benchmarks). Furthermore, in-depth analysis confirms that DaGRPO effectively mitigates gradient explosion and accelerates the emergence of long-chain reasoning capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DaGRPOï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡åž‹ (LLMs) åœ¨æŽ¨ç†è®­ç»ƒä¸­é¢ä¸´çš„ Group Relative Policy Optimization (GRPO) è®­ç»ƒä¸ç¨³å®šæ€§ä¸Žé‡‡æ ·æ•ˆçŽ‡ä½Žä¸‹é—®é¢˜ã€‚ä½œè€…é€šè¿‡ç†è®ºåˆ†æžæŒ‡å‡ºå…¶æ ¹æºåœ¨äºŽåŒç­–ç•¥é‡‡æ ·ç¼ºä¹åŒºåˆ†åº¦ï¼Œå¯¼è‡´å¸¸è§„æŸ¥è¯¢ä¸­äº§ç”Ÿç ´åæ€§çš„æ¢¯åº¦å†²çªï¼Œè€Œéš¾é¢˜ä¸­åˆ™å› æ­£æ ·æœ¬ç¨€ç¼ºå¯¼è‡´ä¼˜åŒ–å¤±æ•ˆã€‚ä¸ºæ­¤ï¼ŒDaGRPO å¼•å…¥äº†åºåˆ—çº§æ¢¯åº¦çŸ«æ­£ (Sequence-level Gradient Rectification) ä»¥åŠ¨æ€æ¶ˆé™¤åŒè´¨åŒ–æ ·æœ¬çš„å†²çªï¼Œå¹¶ç»“åˆç¦»ç­–æ•°æ®å¢žå¼º (Off-policy Data Augmentation) å¼•å…¥é«˜è´¨é‡é”šç‚¹æ¥æ¢å¤ç¡¬ä»»åŠ¡çš„è®­ç»ƒä¿¡å·ã€‚åœ¨9é¡¹æ•°å­¦æŽ¨ç†ä¸Žè¶…åˆ†å¸ƒ (OOD) æ³›åŒ–åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDaGRPO æ˜¾è‘—è¶…è¶Šäº† SFT å’Œ GRPO ç­‰åŸºçº¿æ¨¡åž‹ï¼Œåœ¨æ•°å­¦ä»»åŠ¡ä¸Šå–å¾—äº† 4.7% çš„å¹³å‡å‡†ç¡®çŽ‡æå‡ã€‚å®žéªŒè¿›ä¸€æ­¥è¯å®žï¼Œè¯¥æ¡†æž¶èƒ½æœ‰æ•ˆæŠ‘åˆ¶æ¢¯åº¦çˆ†ç‚¸å¹¶åŠ é€Ÿé•¿é“¾æŽ¨ç†èƒ½åŠ›çš„æ¶ŒçŽ°ï¼Œä¸ºå®žçŽ°é«˜æ•ˆä¸”ç¨³å®šçš„å¤æ‚æŽ¨ç†è®­ç»ƒæä¾›äº†æ–°çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06337v2",
      "published_date": "2025-12-06 07:51:36 UTC",
      "updated_date": "2025-12-31 10:30:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:18:51.716286+00:00"
    },
    {
      "arxiv_id": "2512.15736v1",
      "title": "Anubuddhi: A Multi-Agent AI System for Designing and Simulating Quantum Optics Experiments",
      "title_zh": "Anubuddhiï¼šç”¨äºŽé‡å­å…‰å­¦å®žéªŒè®¾è®¡ä¸Žæ¨¡æ‹Ÿçš„å¤šæ™ºèƒ½ä½“AIç³»ç»Ÿ",
      "authors": [
        "S. K. Rithvik"
      ],
      "abstract": "We present Anubuddhi, a multi-agent AI system that designs and simulates quantum optics experiments from natural language prompts without requiring specialized programming knowledge. The system composes optical layouts by arranging components from a three-tier toolbox via semantic retrieval, then validates designs through physics simulation with convergent refinement. The architecture combines intent routing, knowledge-augmented generation, and dual-mode validation (QuTiP and FreeSim). We evaluated 13 experiments spanning fundamental optics (Hong-Ou-Mandel interference, Michelson/Mach-Zehnder interferometry, Bell states, delayed-choice quantum eraser), quantum information protocols (BB84 QKD, Franson interferometry, GHZ states, quantum teleportation, hyperentanglement), and advanced technologies (boson sampling, electromagnetically induced transparency, frequency conversion). The system achieves design-simulation alignment scores of 8--9/10, with simulations faithfully modeling intended physics. A critical finding distinguishes structural correctness from quantitative accuracy: high alignment confirms correct physics architecture, while numerical predictions require expert review. Free-form simulation outperformed constrained frameworks for 11/13 experiments, revealing that quantum optics diversity demands flexible mathematical representations. The system democratizes computational experiment design for research and pedagogy, producing strong initial designs users can iteratively refine through conversation.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† Anubuddhiï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè®¾è®¡å’Œæ¨¡æ‹Ÿé‡å­å…‰å­¦ (quantum optics) å®žéªŒå¼€å‘çš„å¤šæ™ºèƒ½ä½“ (multi-agent) AI ç³»ç»Ÿï¼Œå…è®¸ç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºç›´æŽ¥ç”Ÿæˆå®žéªŒæ–¹æ¡ˆã€‚ç³»ç»Ÿé€šè¿‡è¯­ä¹‰æ£€ç´¢ (semantic retrieval) ä»Žä¸‰å±‚å·¥å…·ç®±ä¸­æŽ’åˆ—ç»„ä»¶ï¼Œå¹¶ç»“åˆæ„å›¾è·¯ç”± (intent routing) ä¸ŽçŸ¥è¯†å¢žå¼ºç”ŸæˆæŠ€æœ¯ï¼Œåˆ©ç”¨ QuTiP å’Œ FreeSim è¿›è¡ŒåŒæ¨¡å¼éªŒè¯ (dual-mode validation) ä»¥ç¡®ä¿ç‰©ç†è®¾è®¡çš„å‡†ç¡®æ€§ã€‚åœ¨æ¶µç›– Hong-Ou-Mandel å¹²æ¶‰ã€Bell æ€å’Œ BB84 åè®®ç­‰ 13 é¡¹å®žéªŒçš„è¯„ä¼°ä¸­ï¼Œè¯¥ç³»ç»Ÿåœ¨è®¾è®¡ä¸Žæ¨¡æ‹Ÿä¸€è‡´æ€§ä¸ŠèŽ·å¾—äº† 8-9/10 çš„é«˜åˆ†ã€‚ç ”ç©¶å…³é”®å‘çŽ°åŒºåˆ†äº†ç»“æž„æ­£ç¡®æ€§ä¸Žå®šé‡å‡†ç¡®æ€§çš„é‡è¦æ€§ï¼Œå¹¶è¯æ˜Žè‡ªç”±æ ¼å¼æ¨¡æ‹Ÿ (free-form simulation) åœ¨å¤„ç†å¤šæ ·åŒ–çš„é‡å­å…‰å­¦ä»»åŠ¡æ—¶æ¯”å—é™æ¡†æž¶æ›´å…·ä¼˜åŠ¿ã€‚Anubuddhi æ˜¾è‘—é™ä½Žäº†è®¡ç®—å®žéªŒè®¾è®¡çš„é—¨æ§›ï¼Œä¸ºç§‘ç ”å’Œæ•™å­¦æä¾›äº†å¯é€šè¿‡å¯¹è¯ä¸æ–­è¿­ä»£ä¼˜åŒ–çš„å¼ºåŠ›åˆå§‹è®¾è®¡å·¥å…·ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "quant-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15736v1",
      "published_date": "2025-12-06 06:59:56 UTC",
      "updated_date": "2025-12-06 06:59:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:18:48.192943+00:00"
    },
    {
      "arxiv_id": "2512.11871v1",
      "title": "Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops",
      "title_zh": "åŸºäºŽ CNN-MobileViT æ··åˆè½»é‡çº§æ¨¡åž‹çš„æœ¬åœ°ä½œç‰©ç—…è™«å®³è‡ªåŠ¨æ£€æµ‹ä¸Žè¯Šæ–­ç³»ç»Ÿ",
      "authors": [
        "Tekleab G. Gebremedhin",
        "Hailom S. Asegede",
        "Bruh W. Tesheme",
        "Tadesse B. Gebremichael",
        "Kalayu G. Redae"
      ],
      "abstract": "Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸƒå¡žä¿„æ¯”äºšææ ¼é›·åœ°åŒºç”±äºŽåŸºç¡€è®¾æ–½ä¸­æ–­å¯¼è‡´ä¸“å®¶è¯Šæ–­å—é™çš„é—®é¢˜ï¼Œå¼€å‘äº†ä¸€å¥—ä¸“ä¸ºæœ¬åœ°ä½œç‰©è®¾è®¡çš„ç¦»çº¿æ¤ç‰©ç—…è™«å®³è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿã€‚ç ”ç©¶äººå‘˜æž„å»ºäº†ä¸€ä¸ªåŒ…å«3,587å¼ å›¾åƒçš„æœ¬åœ°ä»™äººæŽŒæžœ(cactus-fig)æ•°æ®é›†ï¼Œå¹¶å¯¹æ¯”äº†è‡ªå®šä¹‰è½»é‡çº§ CNNã€EfficientNet-Lite1 ä»¥åŠ CNN-Transformer æ··åˆæ¨¡åž‹ MobileViT-XS çš„æ€§èƒ½ã€‚å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒMobileViT-XS å‡­å€Ÿå¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶(MHSA)çš„å…¨å±€æŽ¨ç†èƒ½åŠ›ï¼Œåœ¨åŒºåˆ†å®³è™«é›†ç¾¤ä¸ŽäºŒç»´çœŸèŒç—…å˜æ–¹é¢ä¼˜äºŽå±€éƒ¨å·ç§¯æ ¸ï¼Œè¾¾åˆ°äº†97.3%çš„å¹³å‡äº¤å‰éªŒè¯å‡†ç¡®çŽ‡ã€‚ä¸Žæ­¤åŒæ—¶ï¼Œè‡ªå®šä¹‰è½»é‡çº§ CNN åœ¨æŽ¨ç†å»¶è¿Ÿ(42 ms)å’Œæ¨¡åž‹å¤§å°(4.8 MB)æ–¹é¢è¡¨çŽ°å‡ºæœ€ä½³çš„éƒ¨ç½²ç‰¹æ€§ã€‚è¯¥ç³»ç»Ÿç›®å‰å·²é›†æˆè‡³æ”¯æŒ Tigrigna å’Œ Amharic è¯­çš„ Flutter åº”ç”¨ç¨‹åºä¸­ï¼Œå¯åœ¨ Cortex-A53 çº§è®¾å¤‡ä¸Šå®žçŽ°å®Œå…¨ç¦»çº¿æŽ¨ç†ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡åœ¨è¾¹ç¼˜çŽ¯å¢ƒä¸‹æä¾›å…³é”®çš„è¯Šæ–­æŠ€æœ¯ï¼Œæ˜¾è‘—å¢žå¼ºäº†å—å†²çªå½±å“åœ°åŒºçš„ç²®é£Ÿå®‰å…¨ä¿éšœèƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "A preliminary version of this work was presented at the International Conference on Postwar Technology for Recovery and Sustainable Development (Feb. 2025). This manuscript substantially extends that work with expanded experiments and on-device deployment analysis. Code and dataset are publicly available at: https://github.com/Tekleab15/Automated_plant_disease_and_pest_detection_system",
      "pdf_url": "https://arxiv.org/pdf/2512.11871v1",
      "published_date": "2025-12-06 06:24:46 UTC",
      "updated_date": "2025-12-06 06:24:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:18:48.787487+00:00"
    },
    {
      "arxiv_id": "2512.06306v1",
      "title": "Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation",
      "title_zh": "åˆ©ç”¨æ—¶ç©ºç‰¹æ€§å®žçŽ°é«˜æ•ˆçš„äº‹ä»¶é©±åŠ¨äººä½“å§¿æ€ä¼°è®¡",
      "authors": [
        "Haoxian Zhou",
        "Chuanzhi Xu",
        "Langyi Chen",
        "Haodong Chen",
        "Yuk Ying Chung",
        "Qiang Qu",
        "Xaoming Chen",
        "Weidong Cai"
      ],
      "abstract": "Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çŽ°æœ‰äº‹ä»¶é©±åŠ¨äººä½“å§¿æ€ä¼°è®¡æ–¹æ³•å› å°†äº‹ä»¶æµè½¬æ¢ä¸ºå¯†é›†å¸§è€Œå¯¼è‡´çš„é«˜è®¡ç®—å¼€é”€å’Œæ—¶é—´åˆ†è¾¨çŽ‡æŸå¤±é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºŽç‚¹äº‘(point cloud-based)æ¡†æž¶çš„æ—¶ç©ºç‰¹æ€§æŒ–æŽ˜æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆè®¾è®¡äº†äº‹ä»¶æ—¶é—´åˆ‡ç‰‡å·ç§¯(Event Temporal Slicing Convolution)æ¨¡å—æ¥æ•èŽ·äº‹ä»¶åˆ‡ç‰‡é—´çš„çŸ­æœŸä¾èµ–ï¼Œå¹¶ç»“åˆäº‹ä»¶åˆ‡ç‰‡åºåˆ—åŒ–(Event Slice Sequencing)æ¨¡å—è¿›è¡Œç»“æž„åŒ–æ—¶é—´å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡åœ¨ç‚¹äº‘è¡¨ç¤ºä¸­åº”ç”¨è¾¹ç¼˜å¢žå¼º(edge enhancement)æŠ€æœ¯ï¼Œæå‡äº†ç¨€ç–äº‹ä»¶æ¡ä»¶ä¸‹çš„ç©ºé—´è¾¹ç¼˜ä¿¡æ¯æå–èƒ½åŠ›ã€‚åœ¨DHP19æ•°æ®é›†ä¸Šçš„å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—ä¸”æŒç»­åœ°æå‡PointNetã€DGCNNå’ŒPoint Transformerç­‰ä»£è¡¨æ€§ç‚¹äº‘éª¨æž¶ç½‘ç»œçš„æ€§èƒ½ï¼Œä¸ºé«˜æ•ˆã€ç¨³å¥çš„äº‹ä»¶é©±åŠ¨äººä½“åŠ¨ä½œåˆ†æžæä¾›äº†æ–°çš„è§£å†³è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06306v1",
      "published_date": "2025-12-06 05:32:13 UTC",
      "updated_date": "2025-12-06 05:32:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:18:55.228827+00:00"
    },
    {
      "arxiv_id": "2512.06304v1",
      "title": "Degrading Voice: A Comprehensive Overview of Robust Voice Conversion Through Input Manipulation",
      "title_zh": "è¯­éŸ³é€€åŒ–ï¼šåŸºäºŽè¾“å…¥æ“çºµçš„é²æ£’è¯­éŸ³è½¬æ¢å…¨é¢ç»¼è¿°",
      "authors": [
        "Xining Song",
        "Zhihua Wei",
        "Rui Wang",
        "Haixiao Hu",
        "Yanxiang Chen",
        "Meng Han"
      ],
      "abstract": "Identity, accent, style, and emotions are essential components of human speech. Voice conversion (VC) techniques process the speech signals of two input speakers and other modalities of auxiliary information such as prompts and emotion tags. It changes para-linguistic features from one to another, while maintaining linguistic contents. Recently, VC models have made rapid advancements in both generation quality and personalization capabilities. These developments have attracted considerable attention for diverse applications, including privacy preservation, voice-print reproduction for the deceased, and dysarthric speech recovery. However, these models only learn non-robust features due to the clean training data. Subsequently, it results in unsatisfactory performances when dealing with degraded input speech in real-world scenarios, including additional noise, reverberation, adversarial attacks, or even minor perturbation. Hence, it demands robust deployments, especially in real-world settings. Although latest researches attempt to find potential attacks and countermeasures for VC systems, there remains a significant gap in the comprehensive understanding of how robust the VC model is under input manipulation. here also raises many questions: For instance, to what extent do different forms of input degradation attacks alter the expected output of VC models? Is there potential for optimizing these attack and defense strategies? To answer these questions, we classify existing attack and defense methods from the perspective of input manipulation and evaluate the impact of degraded input speech across four dimensions, including intelligibility, naturalness, timbre similarity, and subjective perception. Finally, we outline open issues and future directions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­éŸ³è½¬æ¢ (Voice Conversion, VC) æ¨¡åž‹åœ¨å¤„ç†çœŸå®žåœºæ™¯ä¸­é€€åŒ–è¯­éŸ³ï¼ˆå¦‚å™ªå£°ã€æ··å“ã€å¯¹æŠ—æ€§æ”»å‡»æˆ–å¾®å°æ‰°åŠ¨ï¼‰æ—¶é²æ£’æ€§ (Robustness) ä¸è¶³çš„é—®é¢˜è¿›è¡Œäº†å…¨é¢ç»¼è¿°ã€‚å°½ç®¡ VC æŠ€æœ¯åœ¨ç”Ÿæˆè´¨é‡å’Œä¸ªæ€§åŒ–æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†æ¨¡åž‹å¾€å¾€å› ä¾èµ–çº¯å‡€è®­ç»ƒæ•°æ®è€Œç¼ºä¹å¯¹è¾“å…¥æ“çºµ (Input Manipulation) çš„æŠµæŠ—åŠ›ã€‚æœ¬æ–‡ä»Žè¾“å…¥æ“çºµçš„è§†è§’å¯¹çŽ°æœ‰çš„æ”»å‡»ä¸Žé˜²å¾¡æ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿåˆ†ç±»ï¼Œå¹¶ä»Žå¯æ‡‚åº¦ (Intelligibility)ã€è‡ªç„¶åº¦ (Naturalness)ã€éŸ³è‰²ç›¸ä¼¼åº¦ (Timbre Similarity) å’Œä¸»è§‚æ„ŸçŸ¥ (Subjective Perception) å››ä¸ªç»´åº¦æ·±å…¥è¯„ä¼°äº†é€€åŒ–è¾“å…¥å¯¹ VC æ¨¡åž‹è¾“å‡ºçš„å½±å“ã€‚è¯¥ç»¼è¿°ä¸ä»…æ­ç¤ºäº†å½“å‰æ¨¡åž‹åœ¨åº”å¯¹æ¶æ„æ”»å‡»å’ŒçŽ¯å¢ƒå¹²æ‰°æ—¶çš„è„†å¼±æ€§ï¼Œè¿˜æ€»ç»“äº†è¯¥é¢†åŸŸçŽ°æœ‰çš„æŒ‘æˆ˜å¹¶æŒ‡æ˜Žäº†æœªæ¥æå‡ VC ç³»ç»Ÿç¨³å¥æ€§çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CR",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06304v1",
      "published_date": "2025-12-06 05:17:07 UTC",
      "updated_date": "2025-12-06 05:17:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:19:01.971039+00:00"
    },
    {
      "arxiv_id": "2512.06301v1",
      "title": "Chemistry Integrated Language Model using Hierarchical Molecular Representation for Polymer Informatics",
      "title_zh": "é‡‡ç”¨åˆ†å±‚åˆ†å­è¡¨ç¤ºçš„èšåˆç‰©ä¿¡æ¯å­¦åŒ–å­¦é›†æˆè¯­è¨€æ¨¡åž‹",
      "authors": [
        "Jihun Ahn",
        "Gabriella Pasya Irianti",
        "Vikram Thapar",
        "Su-Mi Hur"
      ],
      "abstract": "Machine learning has transformed material discovery for inorganic compounds and small molecules, yet polymers remain largely inaccessible to these methods. While data scarcity is often cited as the primary bottleneck, we demonstrate that strategic molecular representations can overcome this limitation. We introduce CI-LLM (Chemically Informed Language Model), a framework combining HAPPY (Hierarchically Abstracted rePeat unit of PolYmer), which encodes chemical substructures as tokens, with numerical descriptors within transformer architectures. For property prediction, De$^3$BERTa, our descriptor-enriched encoder, achieves 3.5x faster inference than SMILES-based models with improved accuracy ($R^2$ score gains of 0.9-4.1 percent across four properties), while providing interpretable structure-property insights at the subgroup level. For inverse design, our GPT-based generator produces polymers with targeted properties, achieving 100 percent scaffold retention and successful multi-property optimization for negatively correlated objectives. This comprehensive framework demonstrates both forward prediction and inverse design capabilities, showcasing how strategic molecular representation advances machine learning applications in polymer science.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CI-LLM (Chemically Informed Language Model) æ¡†æž¶ï¼Œé€šè¿‡å¼•å…¥åä¸ºHAPPY (Hierarchically Abstracted rePeat unit of PolYmer) çš„å±‚æ¬¡åŒ–åˆ†å­è¡¨ç¤ºæ³•ï¼Œè§£å†³äº†èšåˆç‰©ä¿¡æ¯å­¦ä¸­æ•°æ®ç¨€ç¼ºå¯¹æœºå™¨å­¦ä¹ æ¨¡åž‹çš„é™åˆ¶ã€‚è¯¥æ¡†æž¶å°†åŒ–å­¦å­ç»“æž„ç¼–ç ä¸ºTokenå¹¶é›†æˆæ•°å€¼æè¿°ç¬¦ï¼Œå…‹æœäº†ä¼ ç»ŸSMILESè¡¨ç¤ºæ³•çš„å±€é™æ€§ã€‚å…¶ä¸­De$^3$BERTaç¼–ç å™¨åœ¨ç‰©æ€§é¢„æµ‹ä»»åŠ¡ä¸­è¡¨çŽ°ä¼˜å¼‚ï¼Œå…¶$R^2$è¯„åˆ†æå‡äº†0.9%è‡³4.1%ï¼Œä¸”æŽ¨ç†é€Ÿåº¦æ¯”åŸºçº¿æ¨¡åž‹å¿«3.5å€ï¼ŒåŒæ—¶æä¾›äº†å­ç»“æž„å±‚é¢çš„å¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œæ¡†æž¶ä¸­åŸºäºŽGPTçš„ç”Ÿæˆå™¨åœ¨é€†å‘è®¾è®¡ä¸­å®žçŽ°äº†100%çš„éª¨æž¶ä¿ç•™ï¼Œå¹¶æˆåŠŸå®Œæˆäº†é’ˆå¯¹è´Ÿç›¸å…³ç›®æ ‡çš„å¤æ‚å¤šç‰©æ€§ä¼˜åŒ–ã€‚è¯¥å·¥ä½œå±•ç¤ºäº†ä»Žå‰å‘é¢„æµ‹åˆ°é€†å‘è®¾è®¡çš„å…¨é¢èƒ½åŠ›ï¼Œè¯æ˜Žäº†ç­–ç•¥æ€§åˆ†å­è¡¨ç¤ºåœ¨æŽ¨åŠ¨èšåˆç‰©ç§‘å­¦æœºå™¨å­¦ä¹ åº”ç”¨æ–¹é¢çš„æ ¸å¿ƒä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06301v1",
      "published_date": "2025-12-06 05:07:11 UTC",
      "updated_date": "2025-12-06 05:07:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:19:03.910581+00:00"
    },
    {
      "arxiv_id": "2512.06297v1",
      "title": "Entropic Confinement and Mode Connectivity in Overparameterized Neural Networks",
      "title_zh": "è¿‡å‚æ•°åŒ–ç¥žç»ç½‘ç»œä¸­çš„ç†µé™åŸŸä¸Žæ¨¡å¼è¿žé€šæ€§",
      "authors": [
        "Luca Di Carlo",
        "Chase Goddard",
        "David J. Schwab"
      ],
      "abstract": "Modern neural networks exhibit a striking property: basins of attraction in the loss landscape are often connected by low-loss paths, yet optimization dynamics generally remain confined to a single convex basin and rarely explore intermediate points. We resolve this paradox by identifying entropic barriers arising from the interplay between curvature variations along these paths and noise in optimization dynamics. Empirically, we find that curvature systematically rises away from minima, producing effective forces that bias noisy dynamics back toward the endpoints - even when the loss remains nearly flat. These barriers persist longer than energetic barriers, shaping the late-time localization of solutions in parameter space. Our results highlight the role of curvature-induced entropic forces in governing both connectivity and confinement in deep learning landscapes.",
      "tldr_zh": "è¯¥ç ”ç©¶æŽ¢è®¨äº†è¶…å‚æ•°åŒ–ç¥žç»ç½‘ç»œ(Overparameterized Neural Networks)ä¸­ä¸€ä¸ªæ˜¾è‘—çš„æ‚–è®ºï¼šè™½ç„¶æŸå¤±æ™¯è§‚(Loss Landscape)ä¸­çš„å¸å¼•ç›†(Basins of Attraction)é€šå¸¸ç”±ä½ŽæŸå¤±è·¯å¾„è¿žæŽ¥ï¼Œä½†ä¼˜åŒ–åŠ¨åŠ›å­¦å¾€å¾€å±€é™äºŽå•ä¸ªå‡¸ç›†åœ°å†…ï¼Œå¾ˆå°‘æŽ¢ç´¢ä¸­é—´åŒºåŸŸã€‚ä½œè€…é€šè¿‡è¯†åˆ«ç”±è·¯å¾„ä¸Šæ›²çŽ‡(Curvature)å˜åŒ–ä¸Žä¼˜åŒ–å™ªå£°ç›¸äº’ä½œç”¨äº§ç”Ÿçš„ç†µåž’(Entropic Barriers)ï¼ŒæˆåŠŸè§£æžäº†è¿™ä¸€çŽ°è±¡ã€‚å®žéªŒå‘çŽ°ï¼Œæ›²çŽ‡ä¼šéšç€è¿œç¦»æžå°å€¼ç‚¹è€Œç³»ç»Ÿæ€§ä¸Šå‡ï¼Œä»Žè€Œäº§ç”Ÿä¸€ç§æœ‰æ•ˆçš„åŠ›ï¼Œå³ä½¿åœ¨æŸå¤±å‡½æ•°ä¿æŒå¹³å¦çš„æƒ…å†µä¸‹ï¼Œä¹Ÿä¼šå°†å¸¦æœ‰å™ªå£°çš„åŠ¨åŠ›å­¦æŽ¨å›žåˆå§‹ç«¯ç‚¹ã€‚è¿™äº›ç†µåž’æ¯”ä¼ ç»Ÿçš„èƒ½é‡åŠ¿åž’(Energetic Barriers)æŒç»­æ—¶é—´æ›´é•¿ï¼Œæ˜¾è‘—å½±å“äº†å‚æ•°ç©ºé—´ä¸­è§£çš„åŽæœŸå®šä½ã€‚è¯¥ç ”ç©¶ç»“æžœæœ€ç»ˆå¼ºè°ƒäº†æ›²çŽ‡è¯±å¯¼çš„ç†µåŠ›(Entropic Forces)åœ¨æ”¯é…æ·±åº¦å­¦ä¹ æ™¯è§‚çš„è¿žé€šæ€§(Connectivity)ä¸Žå±€é™æ€§(Confinement)ä¸­æ‰€å‘æŒ¥çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cond-mat.stat-mech",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Under Review",
      "pdf_url": "https://arxiv.org/pdf/2512.06297v1",
      "published_date": "2025-12-06 04:50:32 UTC",
      "updated_date": "2025-12-06 04:50:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:19:06.562425+00:00"
    },
    {
      "arxiv_id": "2512.06296v1",
      "title": "How Sharp and Bias-Robust is a Model? Dual Evaluation Perspectives on Knowledge Graph Completion",
      "title_zh": "æ¨¡åž‹é”åº¦ä¸Žåå·®é²æ£’æ€§ï¼šçŸ¥è¯†å›¾è°±è¡¥å…¨çš„åŒé‡è¯„ä¼°è§†è§’",
      "authors": [
        "Sooho Moon",
        "Yunyong Ko"
      ],
      "abstract": "Knowledge graph completion (KGC) aims to predict missing facts from the observed KG. While a number of KGC models have been studied, the evaluation of KGC still remain underexplored. In this paper, we observe that existing metrics overlook two key perspectives for KGC evaluation: (A1) predictive sharpness -- the degree of strictness in evaluating an individual prediction, and (A2) popularity-bias robustness -- the ability to predict low-popularity entities. Toward reflecting both perspectives, we propose a novel evaluation framework (PROBE), which consists of a rank transformer (RT) estimating the score of each prediction based on a required level of predictive sharpness and a rank aggregator (RA) aggregating all the scores in a popularity-aware manner. Experiments on real-world KGs reveal that existing metrics tend to over- or under-estimate the accuracy of KGC models, whereas PROBE yields a comprehensive understanding of KGC models and reliable evaluation results.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çŸ¥è¯†å›¾è°±è¡¥å…¨(Knowledge Graph Completion, KGC)è¯„ä¼°é¢†åŸŸä¸­è¢«å¿½è§†çš„é¢„æµ‹é”åº¦(predictive sharpness)å’Œæµè¡Œåº¦åå·®é²æ£’æ€§(popularity-bias robustness)å±•å¼€è®¨è®ºã€‚ä¸ºå¼¥è¡¥çŽ°æœ‰è¯„ä»·æŒ‡æ ‡çš„ç¼ºé™·ï¼Œä½œè€…æå‡ºäº†åä¸ºPROBEçš„å…¨æ–°è¯„ä¼°æ¡†æž¶ï¼Œå…¶ä¸­åŒ…å«åŸºäºŽç‰¹å®šé”åº¦æ°´å¹³ä¼°ç®—é¢„æµ‹å¾—åˆ†çš„æŽ’åè½¬æ¢å™¨(Rank Transformer, RT)ä»¥åŠé‡‡ç”¨æµè¡Œåº¦æ„ŸçŸ¥æ–¹å¼è¿›è¡Œå¾—åˆ†æ±‡æ€»çš„æŽ’åèšåˆå™¨(Rank Aggregator, RA)ã€‚é€šè¿‡åœ¨çœŸå®žä¸–ç•ŒçŸ¥è¯†å›¾è°±ä¸Šçš„å®žéªŒè¯æ˜Žï¼Œä¼ ç»Ÿè¯„ä»·æŒ‡æ ‡å¾€å¾€ä¼šå¯¼è‡´å¯¹KGCæ¨¡åž‹å‡†ç¡®æ€§çš„é«˜ä¼°æˆ–ä½Žä¼°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒPROBEæ¡†æž¶èƒ½å¤Ÿå¯¹æ¨¡åž‹æ€§èƒ½è¿›è¡Œæ›´å…¨é¢çš„é€è§†ï¼Œå¹¶ä¸ºKGCæ¨¡åž‹æä¾›æ›´å…·å¯é æ€§çš„è¯„ä¼°ç»“æžœã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages, 4 figures, 2 tables, ACM WSDM 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.06296v1",
      "published_date": "2025-12-06 04:49:29 UTC",
      "updated_date": "2025-12-06 04:49:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:19:06.428440+00:00"
    },
    {
      "arxiv_id": "2512.06281v1",
      "title": "Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models",
      "title_zh": "é‡Šæ”¾å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹çš„å†…åœ¨è§†è§‰è¡¨å¾èƒ½åŠ›",
      "authors": [
        "Hengzhuang Li",
        "Xinsong Zhang",
        "Qiming Peng",
        "Bin Luo",
        "Han Hu",
        "Dengyang Jiang",
        "Han-Jia Ye",
        "Teng Zhang",
        "Hai Jin"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at https://github.com/Fir-lat/LaVer.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LaVerï¼ˆLatent Visual Reconstructionï¼‰ï¼Œä¸€ç§æ—¨åœ¨é‡Šæ”¾å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ï¼ˆMLLMsï¼‰å†…åœ¨è§†è§‰è¡¨å¾èƒ½åŠ›çš„åˆ›æ–°è®­ç»ƒæ¡†æž¶ã€‚é’ˆå¯¹MLLMså› è¿‡åº¦ä¾èµ–æ¬¡æ ‡è®°é¢„æµ‹è€Œå¯¼è‡´çš„æ¨¡æ€å¤±è¡¡ï¼ˆmodality imbalanceï¼‰åŠå¹»è§‰é—®é¢˜ï¼ŒLaVeré€šè¿‡åœ¨LLMçš„è”åˆæ½œè¯­ä¹‰ç©ºé—´ä¸­å¼•å…¥é®è”½å›¾åƒå»ºæ¨¡ï¼ˆmasked image modelingï¼‰ï¼Œä¸ºæ¨¡åž‹æä¾›äº†ç›´æŽ¥çš„è§†è§‰ç›‘ç£ä¿¡å·ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°æ¿€æ´»äº†æ¨¡åž‹å¯¹è§†è§‰ä¿¡æ¯çš„æ³¨æ„åŠ›åˆ†é…ï¼Œå…‹æœäº†æ·±å±‚ç‰¹å¾åŒè´¨åŒ–çš„é—®é¢˜ï¼Œæ˜¾è‘—å¢žå¼ºäº†è§†è§‰ä¿¡æ¯çš„åˆ©ç”¨çŽ‡ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒLaVeråœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­å‡å±•çŽ°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¯†é›†è§†è§‰èƒ½åŠ›ï¼ˆdense visual capabilitiesï¼‰çš„åœºæ™¯ä¸‹æå‡æ˜Žæ˜¾ã€‚è¯¥ç ”ç©¶ä¸ºæå‡å¤šæ¨¡æ€æ¨¡åž‹çš„è§†è§‰ç†è§£æ·±åº¦æä¾›äº†æ–°çš„æœ‰æ•ˆè·¯å¾„ï¼Œå…¶ä»£ç å·²åœ¨GitHubå¼€æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06281v1",
      "published_date": "2025-12-06 04:20:13 UTC",
      "updated_date": "2025-12-06 04:20:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:19:09.082832+00:00"
    },
    {
      "arxiv_id": "2512.06276v2",
      "title": "RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension",
      "title_zh": "RefBench-PROï¼šé¢å‘æŒ‡ä»£æ€§è¡¨è¾¾ç†è§£çš„æ„ŸçŸ¥ä¸ŽæŽ¨ç†è¯„æµ‹åŸºå‡†",
      "authors": [
        "Tianyi Gao",
        "Hao Li",
        "Han Fang",
        "Xin Wei",
        "Xiaodong Dong",
        "Hongbo Sun",
        "Ye Yuan",
        "Zhongjiang He",
        "Jinglin Xu",
        "Jingmin Xin",
        "Hao Sun"
      ],
      "abstract": "Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æŒ‡ä»£æ€§è¡¨è¾¾ç†è§£(Referring Expression Comprehension, REC)çŽ°æœ‰åŸºå‡†æµ‹è¯•åœ¨æ„ŸçŸ¥èƒ½åŠ›è¯„ä¼°å’Œè§£é‡Šæ€§è¯„åˆ†æœºåˆ¶æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†RefBench-PROè¿™ä¸€ç»¼åˆæ€§åŸºå‡†ã€‚è¯¥åŸºå‡†å°†æŒ‡ä»£æ€§è¡¨è¾¾åˆ†è§£ä¸ºæ„ŸçŸ¥(perception)å’ŒæŽ¨ç†(reasoning)ä¸¤ä¸ªæ ¸å¿ƒç»´åº¦ï¼Œå¹¶è¿›ä¸€æ­¥ç»†åˆ†ä¸ºå±žæ€§(attribute)ã€ä½ç½®(position)ã€äº¤äº’(interaction)ã€å¸¸è¯†(commonsense)ã€å…³ç³»(relation)å’Œæ‹’ç»(reject)å…­ä¸ªé€’è¿›çš„æŒ‘æˆ˜æ€§ä»»åŠ¡ã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥å¼€å‘äº†å…¨è‡ªåŠ¨æ•°æ®ç”Ÿæˆæµæ°´çº¿ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºŽå¼ºåŒ–å­¦ä¹ (RL)çš„å­¦ä¹ æ–¹æ¡ˆRef-R1ï¼Œé€šè¿‡å¼•å…¥åŸºäºŽåŠ¨æ€IoUçš„GRPOç®—æ³•æ¥æå‡å¤æ‚æŽ¨ç†æ¡ä»¶ä¸‹çš„å®šä½ç²¾åº¦ã€‚å¤§é‡å®žéªŒè¯æ˜Žï¼ŒRefBench-PROèƒ½å¤Ÿå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹(MLLM)çš„æŒ‡ä»£æ€§è¡¨è¾¾ç†è§£èƒ½åŠ›è¿›è¡Œå¯è§£é‡Šæ€§è¯„ä¼°ï¼Œåœ¨æ„ŸçŸ¥ä¸ŽæŽ¨ç†ä¸¤æ–¹é¢å‡å±•çŽ°å‡ºäº†æžé«˜çš„æŒ‘æˆ˜æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06276v2",
      "published_date": "2025-12-06 03:59:21 UTC",
      "updated_date": "2025-12-13 10:53:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:19:21.671355+00:00"
    },
    {
      "arxiv_id": "2512.06274v1",
      "title": "Networked Restless Multi-Arm Bandits with Reinforcement Learning",
      "title_zh": "åŸºäºŽå¼ºåŒ–å­¦ä¹ çš„ç½‘ç»œåŒ–ä¸å®‰åˆ†å¤šè‡‚è€è™Žæœº",
      "authors": [
        "Hanmo Zhang",
        "Zenghui Sun",
        "Kai Wang"
      ],
      "abstract": "Restless Multi-Armed Bandits (RMABs) are a powerful framework for sequential decision-making, widely applied in resource allocation and intervention optimization challenges in public health. However, traditional RMABs assume independence among arms, limiting their ability to account for interactions between individuals that can be common and significant in a real-world environment. This paper introduces Networked RMAB, a novel framework that integrates the RMAB model with the independent cascade model to capture interactions between arms in networked environments. We define the Bellman equation for networked RMAB and present its computational challenge due to exponentially large action and state spaces. To resolve the computational challenge, we establish the submodularity of Bellman equation and apply the hill-climbing algorithm to achieve a $1-\\frac{1}{e}$ approximation guarantee in Bellman updates. Lastly, we prove that the approximate Bellman updates are guaranteed to converge by a modified contraction analysis. We experimentally verify these results by developing an efficient Q-learning algorithm tailored to the networked setting. Experimental results on real-world graph data demonstrate that our Q-learning approach outperforms both $k$-step look-ahead and network-blind approaches, highlighting the importance of capturing and leveraging network effects where they exist.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿå˜åŠ¨å¤šè‡‚è€è™Žæœº (Restless Multi-Armed Bandits, RMAB) æ¡†æž¶å‡è®¾å„è‡‚ç›¸äº’ç‹¬ç«‹è€Œæ— æ³•æ•æ‰ä¸ªä½“é—´äº’åŠ¨çš„å±€é™æ€§ï¼Œæå‡ºäº†å…¨æ–°çš„ Networked RMAB æ¡†æž¶ã€‚è¯¥æ¡†æž¶å°† RMAB æ¨¡åž‹ä¸Žç‹¬ç«‹çº§è”æ¨¡åž‹ (Independent Cascade Model) é›†æˆï¼Œä»¥åˆ»ç”»ç½‘ç»œçŽ¯å¢ƒä¸­çš„è‡‚é—´ç›¸äº’ä½œç”¨ã€‚ä¸ºäº†è§£å†³ç”±äºŽçŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´æŒ‡æ•°çº§å¢žé•¿å¸¦æ¥çš„è®¡ç®—éš¾é¢˜ï¼Œç ”ç©¶è€…åˆ©ç”¨ Bellman æ–¹ç¨‹çš„å­æ¨¡æ€§ (Submodularity)ï¼Œé€šè¿‡çˆ¬å±±ç®—æ³• (Hill-climbing Algorithm) å®žçŽ°äº† 1-1/e çš„è¿‘ä¼¼ Bellman æ›´æ–°ï¼Œå¹¶åˆ©ç”¨æ”¹è¿›çš„æ”¶ç¼©æ˜ å°„åˆ†æž (Contraction Analysis) è¯æ˜Žäº†å…¶æ”¶æ•›æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†ä¸€ç§é’ˆå¯¹ç½‘ç»œåŒ–åœºæ™¯çš„é«˜æ•ˆ Q-learning ç®—æ³•ã€‚åœ¨çœŸå®žä¸–ç•Œå›¾æ•°æ®ä¸Šçš„å®žéªŒè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºŽ k-step look-ahead å’Œå¿½ç•¥ç½‘ç»œæ•ˆåº”çš„åŸºçº¿æ–¹æ³•ï¼Œå……åˆ†éªŒè¯äº†åœ¨èµ„æºåˆ†é…å’Œå¹²é¢„ä¼˜åŒ–å†³ç­–ä¸­æ•æ‰å¹¶åˆ©ç”¨ç½‘ç»œæ•ˆåº”çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06274v1",
      "published_date": "2025-12-06 03:53:25 UTC",
      "updated_date": "2025-12-06 03:53:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:19:27.074216+00:00"
    },
    {
      "arxiv_id": "2512.06259v1",
      "title": "Who Will Top the Charts? Multimodal Music Popularity Prediction via Adaptive Fusion of Modality Experts and Temporal Engagement Modeling",
      "title_zh": "è°å°†ç™»é¡¶æ¦œå•ï¼ŸåŸºäºŽæ¨¡æ€ä¸“å®¶è‡ªé€‚åº”èžåˆä¸Žæ—¶åºå‚ä¸Žå»ºæ¨¡çš„å¤šæ¨¡æ€éŸ³ä¹æµè¡Œåº¦é¢„æµ‹",
      "authors": [
        "Yash Choudhary",
        "Preeti Rao",
        "Pushpak Bhattacharyya"
      ],
      "abstract": "Predicting a song's commercial success prior to its release remains an open and critical research challenge for the music industry. Early prediction of music popularity informs strategic decisions, creative planning, and marketing. Existing methods suffer from four limitations:(i) temporal dynamics in audio and lyrics are averaged away; (ii) lyrics are represented as a bag of words, disregarding compositional structure and affective semantics; (iii) artist- and song-level historical performance is ignored; and (iv) multimodal fusion approaches rely on simple feature concatenation, resulting in poorly aligned shared representations. To address these limitations, we introduce GAMENet, an end-to-end multimodal deep learning architecture for music popularity prediction. GAMENet integrates modality-specific experts for audio, lyrics, and social metadata through an adaptive gating mechanism. We use audio features from Music4AllOnion processed via OnionEnsembleAENet, a network of autoencoders designed for robust feature extraction; lyric embeddings derived through a large language model pipeline; and newly introduced Career Trajectory Dynamics (CTD) features that capture multi-year artist career momentum and song-level trajectory statistics. Using the Music4All dataset (113k tracks), previously explored in MIR tasks but not popularity prediction, GAMENet achieves a 12% improvement in R^2 over direct multimodal feature concatenation. Spotify audio descriptors alone yield an R^2 of 0.13. Integrating aggregate CTD features increases this to 0.69, with an additional 7% gain from temporal CTD features. We further validate robustness using the SpotGenTrack Popularity Dataset (100k tracks), achieving a 16% improvement over the previous baseline. Extensive ablations confirm the model's effectiveness and the distinct contribution of each modality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éŸ³ä¹æµè¡Œåº¦é¢„æµ‹ä¸­å­˜åœ¨çš„éŸ³é¢‘ä¸Žæ­Œè¯æ—¶åºåŠ¨æ€ç¼ºå¤±ã€æ­Œè¯è¯­ä¹‰è¡¨è¾¾ä¸è¶³ä»¥åŠå¤šæ¨¡æ€èžåˆæ–¹æ³•å•ä¸€ç­‰å±€é™æ€§ï¼Œæå‡ºäº†åä¸º GAMENet çš„ç«¯åˆ°ç«¯å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æž¶æž„ã€‚è¯¥æ¡†æž¶é€šè¿‡è‡ªé€‚åº”é—¨æŽ§æœºåˆ¶ (Adaptive Gating Mechanism) æœ‰æ•ˆé›†æˆäº†éŸ³é¢‘ã€æ­Œè¯å’Œç¤¾äº¤å…ƒæ•°æ®ä¸“å®¶æ¨¡åž‹ï¼Œå¹¶é‡‡ç”¨ OnionEnsembleAENet è¿›è¡ŒéŸ³é¢‘ç‰¹å¾æå–ä»¥åŠåˆ©ç”¨å¤§è¯­è¨€æ¨¡åž‹ (LLM) èŽ·å–æ­Œè¯è¯­ä¹‰ã€‚ç ”ç©¶ç‰¹åˆ«å¼•å…¥äº†èŒä¸šè½¨è¿¹åŠ¨æ€ (Career Trajectory Dynamics, CTD) ç‰¹å¾ï¼Œç”¨ä»¥æ•æ‰è‰ºæœ¯å®¶çš„äº‹ä¸šåŠ¿å¤´å’Œæ­Œæ›²çš„æ—¶åºå‚ä¸Žæ•°æ®ã€‚åœ¨ Music4All å’Œ SpotGenTrack ä¸¤ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒGAMENet åœ¨ $R^2$ æŒ‡æ ‡ä¸Šæ¯”ä¼ ç»Ÿç‰¹å¾æ‹¼æŽ¥æ–¹æ³•æå‡äº† 12% ä»¥ä¸Šã€‚æ¶ˆèžå®žéªŒè¿›ä¸€æ­¥è¯å®žäº†å„æ¨¡æ€ä¸“å®¶åŠ CTD ç‰¹å¾åœ¨æå‡éŸ³ä¹æµè¡Œåº¦é¢„æµ‹å‡†ç¡®æ€§æ–¹é¢çš„æ˜¾è‘—è´¡çŒ®ï¼Œä¸ºéŸ³ä¹äº§ä¸šçš„å•†ä¸šå†³ç­–æä¾›äº†æœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SD",
      "comment": "8 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.06259v1",
      "published_date": "2025-12-06 03:07:43 UTC",
      "updated_date": "2025-12-06 03:07:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:19:27.645796+00:00"
    },
    {
      "arxiv_id": "2512.06256v1",
      "title": "Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic Setup",
      "title_zh": "å¤šæ™ºèƒ½ä½“è®¾å®šä¸‹ä¸¤ä¸ªå¤§è¯­è¨€æ¨¡åž‹äº¤äº’æ—¶çš„è¾“å‡ºæ”¶æ•›",
      "authors": [
        "Aniruddha Maiti",
        "Satya Nimmagadda",
        "Kartha Veerya Jammuladinne",
        "Niladri Sengupta",
        "Ananya Jana"
      ],
      "abstract": "In this work, we report what happens when two large language models respond to each other for many turns without any outside input in a multi-agent setup. The setup begins with a short seed sentence. After that, each model reads the other's output and generates a response. This continues for a fixed number of steps. We used Mistral Nemo Base 2407 and Llama 2 13B hf. We observed that most conversations start coherently but later fall into repetition. In many runs, a short phrase appears and repeats across turns. Once repetition begins, both models tend to produce similar output rather than introducing a new direction in the conversation. This leads to a loop where the same or similar text is produced repeatedly. We describe this behavior as a form of convergence. It occurs even though the models are large, trained separately, and not given any prompt instructions. To study this behavior, we apply lexical and embedding-based metrics to measure how far the conversation drifts from the initial seed and how similar the outputs of the two models becomes as the conversation progresses.",
      "tldr_zh": "è¯¥ç ”ç©¶æŽ¢è®¨äº†ä¸¤ä¸ªå¤§åž‹è¯­è¨€æ¨¡åž‹(Large Language Models)åœ¨å¤šæ™ºèƒ½ä½“(Multi-Agentic Setup)çŽ¯å¢ƒä¸‹è¿›è¡Œå¤šè½®é—­åˆäº¤äº’æ—¶çš„è¾“å‡ºè¡Œä¸ºã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ Mistral Nemo Base 2407 å’Œ Llama 2 13B hf æ¨¡åž‹è¿›è¡Œå®žéªŒï¼Œè§‚å¯Ÿæ¨¡åž‹åœ¨ä»…æœ‰åˆå§‹ç§å­ä¸”æ— å¤–éƒ¨å¹²é¢„çš„æƒ…å†µä¸‹ç›¸äº’å“åº”çš„æ¼”å˜è¿‡ç¨‹ã€‚å®žéªŒå‘çŽ°ï¼Œè™½ç„¶å¯¹è¯åˆæœŸä¿æŒè¿žè´¯ï¼Œä½†æœ€ç»ˆå¾€å¾€ä¼šé™·å…¥ç—…æ€é‡å¤ï¼Œä¸¤ä¸ªæ¨¡åž‹çš„è¾“å‡ºä¼šé€æ¸è¶‹åŒå¹¶é™·å…¥æ–‡æœ¬å¾ªçŽ¯ï¼Œè¿™ç§çŽ°è±¡è¢«å®šä¹‰ä¸ºæ”¶æ•›(Convergence)ã€‚å³ä½¿è¿™äº›æ¨¡åž‹è§„æ¨¡å·¨å¤§ä¸”ç»è¿‡ç‹¬ç«‹è®­ç»ƒï¼Œåœ¨æ²¡æœ‰ç‰¹å®šæŒ‡ä»¤å¼•å¯¼æ—¶ï¼Œè¿™ç§æ”¶æ•›è¡Œä¸ºä¾ç„¶æ™®éå­˜åœ¨ã€‚é€šè¿‡è¯æ±‡(Lexical)å’ŒåŸºäºŽåµŒå…¥(Embedding-based)çš„åº¦é‡æ ‡å‡†ï¼Œç ”ç©¶é‡åŒ–äº†å¯¹è¯åç¦»åˆå§‹ç§å­çš„ç¨‹åº¦ä»¥åŠæ¨¡åž‹é—´è¾“å‡ºç›¸ä¼¼æ€§çš„å¢žé•¿ã€‚è¿™ä¸€å‘çŽ°æ­ç¤ºäº†å¤šæ™ºèƒ½ä½“äº¤äº’ä¸­ç”±äºŽåé¦ˆå¾ªçŽ¯å¯¼è‡´çš„è¾“å‡ºè´¨é‡é€€åŒ–é—®é¢˜ï¼Œä¸ºç†è§£å¤§æ¨¡åž‹äº¤äº’åŠ¨åŠ›å­¦æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "accepted to LLM 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.06256v1",
      "published_date": "2025-12-06 03:00:24 UTC",
      "updated_date": "2025-12-06 03:00:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:19:36.080108+00:00"
    },
    {
      "arxiv_id": "2512.06247v2",
      "title": "DUET: Agentic Design Understanding via Experimentation and Testing",
      "title_zh": "DUETï¼šåŸºäºŽå®žéªŒä¸Žæµ‹è¯•çš„æ™ºèƒ½ä½“åŒ–è®¾è®¡ç†è§£",
      "authors": [
        "Gus Henry Smith",
        "Sandesh Adhikary",
        "Vineet Thumuluri",
        "Karthik Suresh",
        "Vivek Pandit",
        "Kartik Hegde",
        "Hamid Shojaei",
        "Chandra Bhagavatula"
      ],
      "abstract": "AI agents powered by large language models (LLMs) are being used to solve increasingly complex software engineering challenges, but struggle with hardware design tasks. Register Transfer Level (RTL) code presents a unique challenge for LLMs, as it encodes complex, dynamic, time-evolving behaviors using the low-level language features of SystemVerilog. LLMs struggle to infer these complex behaviors from the syntax of RTL alone, which limits their ability to complete all downstream tasks like code completion, documentation, or verification. In response to this issue, we present DUET: a general methodology for developing Design Understanding via Experimentation and Testing. DUET mimics how hardware design experts develop an understanding of complex designs: not just via a one-off readthrough of the RTL, but via iterative experimentation using a number of tools. DUET iteratively generates hypotheses, tests them with EDA tools (e.g., simulation, waveform inspection, and formal verification), and integrates the results to build a bottom-up understanding of the design. In our evaluations, we show that DUET improves AI agent performance on formal verification, when compared to a baseline flow without experimentation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰åœ¨ç†è§£ç¡¬ä»¶è®¾è®¡ä¸­ Register Transfer Level (RTL) ä»£ç çš„å¤æ‚åŠ¨æ€è¡Œä¸ºæ—¶å­˜åœ¨çš„å±€é™ï¼Œæå‡ºäº† DUET æ–¹æ³•è®ºã€‚DUET æ¨¡ä»¿ç¡¬ä»¶ä¸“å®¶åˆ©ç”¨ EDA å·¥å…·è¿›è¡Œè¿­ä»£å®žéªŒçš„æ–¹å¼ï¼Œå»ºç«‹äº†åŸºäºŽ Experimentation and Testing çš„è®¾è®¡ç†è§£æ¡†æž¶ã€‚è¯¥æ–¹æ³•ä¸å±€é™äºŽå¯¹ SystemVerilog è¯­æ³•çš„é™æ€é˜…è¯»ï¼Œè€Œæ˜¯é€šè¿‡ç”Ÿæˆå‡è®¾å¹¶ç»“åˆä»¿çœŸï¼ˆsimulationï¼‰ã€æ³¢å½¢æ£€æŸ¥ï¼ˆwaveform inspectionï¼‰å’Œå½¢å¼éªŒè¯ï¼ˆformal verificationï¼‰ç­‰å·¥å…·è¿›è¡ŒéªŒè¯ã€‚é€šè¿‡ä¸æ–­æ•´åˆå®žéªŒç»“æžœï¼ŒDUET èƒ½å¤Ÿè‡ªä¸‹è€Œä¸Šåœ°æž„å»ºå¯¹å¤æ‚ç¡¬ä»¶è®¾è®¡çš„æ·±å±‚è®¤çŸ¥ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œä¸Žä¸å…·å¤‡å®žéªŒèƒ½åŠ›çš„åŸºå‡†æµç¨‹ç›¸æ¯”ï¼ŒDUET æ˜¾è‘—å¢žå¼ºäº† AI æ™ºèƒ½ä½“åœ¨æ‰§è¡Œå½¢å¼éªŒè¯ä»»åŠ¡æ—¶çš„æ€§èƒ½è¡¨çŽ°ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06247v2",
      "published_date": "2025-12-06 02:16:28 UTC",
      "updated_date": "2026-01-22 02:05:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:19:38.837563+00:00"
    },
    {
      "arxiv_id": "2512.06244v1",
      "title": "Auto-exploration for online reinforcement learning",
      "title_zh": "é¢å‘åœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„è‡ªåŠ¨æŽ¢ç´¢",
      "authors": [
        "Caleb Ju",
        "Guanghui Lan"
      ],
      "abstract": "The exploration-exploitation dilemma in reinforcement learning (RL) is a fundamental challenge to efficient RL algorithms. Existing algorithms for finite state and action discounted RL problems address this by assuming sufficient exploration over both state and action spaces. However, this yields non-implementable algorithms and sub-optimal performance. To resolve these limitations, we introduce a new class of methods with auto-exploration, or methods that automatically explore both state and action spaces in a parameter-free way, i.e.,~without a priori knowledge of problem-dependent parameters. We present two variants: one for the tabular setting and one for linear function approximation. Under algorithm-independent assumptions on the existence of an exploring optimal policy, both methods attain $O(Îµ^{-2})$ sample complexity to solve to $Îµ$ error. Crucially, these complexities are novel since they are void of algorithm-dependent parameters seen in prior works, which may be arbitrarily large. The methods are also simple to implement because they are parameter-free and do not directly estimate the unknown parameters. These feats are achieved by new algorithmic innovations for RL, including a dynamic mixing time, a discounted state distribution for sampling, a simple robust gradient estimator, and a recent advantage gap function to certify convergence.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)ä¸­æ ¸å¿ƒçš„æŽ¢ç´¢ä¸Žåˆ©ç”¨(exploration-exploitation)éš¾é¢˜ï¼ŒæŒ‡å‡ºçŽ°æœ‰ç®—æ³•åœ¨å¤„ç†æœ‰é™çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´çš„æŠ˜æ‰£RLé—®é¢˜æ—¶ï¼Œå¾€å¾€å› ä¾èµ–ç‰¹å®šå…ˆéªŒå‡è®¾è€Œå¯¼è‡´ç®—æ³•éš¾ä»¥è½åœ°ä¸”æ€§èƒ½å—é™ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç±»å…·å¤‡è‡ªåŠ¨æŽ¢ç´¢(auto-exploration)èƒ½åŠ›çš„æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿä»¥æ— å‚æ•°(parameter-free)çš„æ–¹å¼è‡ªåŠ¨æœç´¢çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ï¼Œæ— éœ€é¢„å…ˆèŽ·çŸ¥é—®é¢˜ç›¸å…³çš„ç‰¹å®šå‚æ•°ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†é’ˆå¯¹è¡¨æ ¼è®¾ç½®(tabular setting)å’Œçº¿æ€§å‡½æ•°è¿‘ä¼¼(linear function approximation)çš„ä¸¤ç§å˜ä½“ï¼Œå¹¶åœ¨ç®—æ³•æ— å…³çš„æŽ¢ç´¢æ€§æœ€ä¼˜ç­–ç•¥å‡è®¾ä¸‹ï¼Œå®žçŽ°äº†$O(Îµ^{-2})$çš„æ ·æœ¬å¤æ‚åº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›å¤æ‚åº¦æŒ‡æ ‡ä¸åŒ…å«ä»¥å¾€ç ”ç©¶ä¸­å¯èƒ½æžå¤§ä¸”å½±å“æ€§èƒ½çš„ç®—æ³•ç›¸å…³å‚æ•°ï¼Œä¸”ç®—æ³•æ— éœ€ç›´æŽ¥ä¼°è®¡æœªçŸ¥å‚æ•°ï¼Œæ˜¾è‘—é™ä½Žäº†å®žçŽ°éš¾åº¦ã€‚è¿™ä¸€çªç ´ä¸»è¦å½’åŠŸäºŽä¸€ç³»åˆ—åˆ›æ–°æœºåˆ¶ï¼ŒåŒ…æ‹¬åŠ¨æ€æ··åˆæ—¶é—´(dynamic mixing time)ã€æŠ˜æ‰£çŠ¶æ€åˆ†å¸ƒ(discounted state distribution)é‡‡æ ·æŠ€æœ¯ã€ç¨³å¥æ¢¯åº¦ä¼°è®¡å™¨(robust gradient estimator)ä»¥åŠç”¨äºŽéªŒè¯æ”¶æ•›çš„ä¼˜åŠ¿é—´éš™å‡½æ•°(advantage gap function)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "35 pages (9 appendix), 1 figure. Comments are welcome",
      "pdf_url": "https://arxiv.org/pdf/2512.06244v1",
      "published_date": "2025-12-06 02:04:50 UTC",
      "updated_date": "2025-12-06 02:04:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:19:38.073364+00:00"
    },
    {
      "arxiv_id": "2512.08984v1",
      "title": "RAG-HAR: Retrieval Augmented Generation-based Human Activity Recognition",
      "title_zh": "RAG-HARï¼šåŸºäºŽæ£€ç´¢å¢žå¼ºç”Ÿæˆçš„äººä½“æ´»åŠ¨è¯†åˆ«",
      "authors": [
        "Nirhoshan Sivaroopan",
        "Hansi Karunarathna",
        "Chamara Madarasingha",
        "Anura Jayasumana",
        "Kanchana Thilakarathna"
      ],
      "abstract": "Human Activity Recognition (HAR) underpins applications in healthcare, rehabilitation, fitness tracking, and smart environments, yet existing deep learning approaches demand dataset-specific training, large labeled corpora, and significant computational resources.We introduce RAG-HAR, a training-free retrieval-augmented framework that leverages large language models (LLMs) for HAR. RAG-HAR computes lightweight statistical descriptors, retrieves semantically similar samples from a vector database, and uses this contextual evidence to make LLM-based activity identification. We further enhance RAG-HAR by first applying prompt optimization and introducing an LLM-based activity descriptor that generates context-enriched vector databases for delivering accurate and highly relevant contextual information. Along with these mechanisms, RAG-HAR achieves state-of-the-art performance across six diverse HAR benchmarks. Most importantly, RAG-HAR attains these improvements without requiring model training or fine-tuning, emphasizing its robustness and practical applicability. RAG-HAR moves beyond known behaviors, enabling the recognition and meaningful labelling of multiple unseen human activities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RAG-HARï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºŽæ£€ç´¢å¢žå¼ºç”Ÿæˆ (Retrieval Augmented Generation) çš„äººä½“æ´»åŠ¨è¯†åˆ« (Human Activity Recognition, HAR) æ¡†æž¶ï¼Œæ—¨åœ¨è§£å†³çŽ°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•å¯¹ç‰¹å®šæ•°æ®é›†è®­ç»ƒå’Œè®¡ç®—èµ„æºçš„é«˜åº¦ä¾èµ–é—®é¢˜ã€‚RAG-HAR é€šè¿‡è®¡ç®—è½»é‡çº§çš„ç»Ÿè®¡æè¿°ç¬¦ï¼Œä»Žå‘é‡æ•°æ®åº“ä¸­æ£€ç´¢è¯­ä¹‰ç›¸ä¼¼çš„æ ·æœ¬ï¼Œå¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡åž‹ (LLMs) çš„ä¸Šä¸‹æ–‡æŽ¨ç†èƒ½åŠ›è¿›è¡Œæ´»åŠ¨è¯†åˆ«ã€‚è¯¥æ¡†æž¶è¿›ä¸€æ­¥å¼•å…¥äº†æç¤ºä¼˜åŒ– (Prompt Optimization) å’ŒåŸºäºŽ LLM çš„æ´»åŠ¨æè¿°ç¬¦ï¼Œé€šè¿‡ç”Ÿæˆå¯Œå«ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å‘é‡æ•°æ®åº“æ¥æä¾›ç²¾ç¡®çš„æ£€ç´¢æ”¯æŒã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒRAG-HAR åœ¨å…­ä¸ªä¸åŒçš„ HAR åŸºå‡†æ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº† State-of-the-art æ€§èƒ½ã€‚æœ€å…³é”®çš„æ˜¯ï¼Œè¯¥æ–¹æ³•æ— éœ€ä»»ä½•æ¨¡åž‹è®­ç»ƒæˆ–å¾®è°ƒå³å¯å®žçŽ°æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå±•çŽ°äº†æžå¼ºçš„é²æ£’æ€§å’Œå®žç”¨æ€§ï¼Œå¹¶å…·å¤‡è¯†åˆ«å’Œæ ‡è®°æœªçŸ¥äººä½“æ´»åŠ¨çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.08984v1",
      "published_date": "2025-12-06 01:53:02 UTC",
      "updated_date": "2025-12-06 01:53:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:19:37.254943+00:00"
    },
    {
      "arxiv_id": "2512.06240v1",
      "title": "AI Application in Anti-Money Laundering for Sustainable and Transparent Financial Systems",
      "title_zh": "äººå·¥æ™ºèƒ½åœ¨åæ´—é’±é¢†åŸŸä¸­çš„åº”ç”¨ï¼šæž„å»ºå¯æŒç»­ä¸”é€æ˜Žçš„é‡‘èžä½“ç³»",
      "authors": [
        "Chuanhao Nie",
        "Yunbo Liu",
        "Chao Wang"
      ],
      "abstract": "Money laundering and financial fraud remain major threats to global financial stability, costing trillions annually and challenging regulatory oversight. This paper reviews how artificial intelligence (AI) applications can modernize Anti-Money Laundering (AML) workflows by improving detection accuracy, lowering false-positive rates, and reducing the operational burden of manual investigations, thereby supporting more sustainable development. It further highlights future research directions including federated learning for privacy-preserving collaboration, fairness-aware and interpretable AI, reinforcement learning for adaptive defenses, and human-in-the-loop visualization systems to ensure that next-generation AML architectures remain transparent, accountable, and robust. In the final part, the paper proposes an AI-driven KYC application that integrates graph-based retrieval-augmented generation (RAG Graph) with generative models to enhance efficiency, transparency, and decision support in KYC processes related to money-laundering detection. Experimental results show that the RAG-Graph architecture delivers high faithfulness and strong answer relevancy across diverse evaluation settings, thereby enhancing the efficiency and transparency of KYC CDD/EDD workflows and contributing to more sustainable, resource-optimized compliance practices.",
      "tldr_zh": "è¯¥ç ”ç©¶æŽ¢è®¨äº†äººå·¥æ™ºèƒ½ (AI) å¦‚ä½•é€šè¿‡æé«˜æ£€æµ‹å‡†ç¡®æ€§å’Œé™ä½Žè¯¯æŠ¥çŽ‡æ¥çŽ°ä»£åŒ–åæ´—é’± (AML) å·¥ä½œæµç¨‹ï¼Œä»¥åº”å¯¹å…¨çƒé‡‘èžç¨³å®šé¢ä¸´çš„å¨èƒã€‚è®ºæ–‡æ·±å…¥åˆ†æžäº† AI åœ¨æå‡é‡‘èžç³»ç»Ÿé€æ˜Žåº¦æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶æŒ‡å‡ºäº†è”é‚¦å­¦ä¹  (federated learning)ã€å…¬å¹³æ„ŸçŸ¥ä¸”å¯è§£é‡Šçš„ AI ä»¥åŠå¼ºåŒ–å­¦ä¹  (reinforcement learning) ç­‰æœªæ¥ç ”ç©¶æ–¹å‘ã€‚ç ”ç©¶é‡ç‚¹æå‡ºäº†ä¸€ç§é›†æˆåŸºäºŽå›¾çš„æ£€ç´¢å¢žå¼ºç”Ÿæˆ (RAG Graph) ä¸Žç”Ÿæˆæ¨¡åž‹çš„ AI é©±åŠ¨ KYC åº”ç”¨ï¼Œæ—¨åœ¨å¢žå¼ºæ´—é’±æ£€æµ‹ä¸­çš„æ•ˆçŽ‡ä¸Žå†³ç­–æ”¯æŒã€‚å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒRAG Graph æž¶æž„åœ¨ KYC å°½èŒè°ƒæŸ¥ (CDD/EDD) å·¥ä½œæµç¨‹ä¸­è¡¨çŽ°å‡ºé«˜åº¦çš„å¿ å®žåº¦ä¸Žç­”æ¡ˆç›¸å…³æ€§ã€‚è¿™ä¸€ç ”ç©¶æˆæžœä¸ºæž„å»ºæ›´å¯æŒç»­ã€èµ„æºä¼˜åŒ–çš„åˆè§„å®žè·µæä¾›äº†æœ‰åŠ›æ”¯æŒï¼Œæœ‰æ•ˆç¡®ä¿äº†ä¸‹ä¸€ä»£åæ´—é’±æž¶æž„çš„é€æ˜Žæ€§ä¸Žé²æ£’æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06240v1",
      "published_date": "2025-12-06 01:37:24 UTC",
      "updated_date": "2025-12-06 01:37:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:19:43.535589+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 58,
  "processed_papers_count": 58,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T14:20:58.090174+00:00"
}
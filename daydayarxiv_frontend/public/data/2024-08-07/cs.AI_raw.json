[
  {
    "arxiv_id": "2408.04121v1",
    "title": "Can Rule-Based Insights Enhance LLMs for Radiology Report Classification? Introducing the RadPrompt Methodology",
    "authors": [
      "Panagiotis Fytas",
      "Anna Breger",
      "Ian Selby",
      "Simon Baker",
      "Shahab Shahipasand",
      "Anna Korhonen"
    ],
    "abstract": "Developing imaging models capable of detecting pathologies from chest X-rays\ncan be cost and time-prohibitive for large datasets as it requires supervision\nto attain state-of-the-art performance. Instead, labels extracted from\nradiology reports may serve as distant supervision since these are routinely\ngenerated as part of clinical practice. Despite their widespread use, current\nrule-based methods for label extraction rely on extensive rule sets that are\nlimited in their robustness to syntactic variability. To alleviate these\nlimitations, we introduce RadPert, a rule-based system that integrates an\nuncertainty-aware information schema with a streamlined set of rules, enhancing\nperformance. Additionally, we have developed RadPrompt, a multi-turn prompting\nstrategy that leverages RadPert to bolster the zero-shot predictive\ncapabilities of large language models, achieving a statistically significant\nimprovement in weighted average F1 score over GPT-4 Turbo. Most notably,\nRadPrompt surpasses both its underlying models, showcasing the synergistic\npotential of LLMs with rule-based models. We have evaluated our methods on two\nEnglish Corpora: the MIMIC-CXR gold-standard test set and a gold-standard\ndataset collected from the Cambridge University Hospitals.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at BioNLP, ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.04121v1",
    "published_date": "2024-08-07 23:09:23 UTC",
    "updated_date": "2024-08-07 23:09:23 UTC"
  },
  {
    "arxiv_id": "2408.04112v1",
    "title": "Patchview: LLM-Powered Worldbuilding with Generative Dust and Magnet Visualization",
    "authors": [
      "John Joon Young Chung",
      "Max Kreminski"
    ],
    "abstract": "Large language models (LLMs) can help writers build story worlds by\ngenerating world elements, such as factions, characters, and locations.\nHowever, making sense of many generated elements can be overwhelming. Moreover,\nif the user wants to precisely control aspects of generated elements that are\ndifficult to specify verbally, prompting alone may be insufficient. We\nintroduce Patchview, a customizable LLM-powered system that visually aids\nworldbuilding by allowing users to interact with story concepts and elements\nthrough the physical metaphor of magnets and dust. Elements in Patchview are\nvisually dragged closer to concepts with high relevance, facilitating\nsensemaking. The user can also steer the generation with verbally elusive\nconcepts by indicating the desired position of the element between concepts.\nWhen the user disagrees with the LLM's visualization and generation, they can\ncorrect those by repositioning the element. These corrections can be used to\nalign the LLM's future behaviors to the user's perception. With a user study,\nwe show that Patchview supports the sensemaking of world elements and steering\nof element generation, facilitating exploration during the worldbuilding\nprocess. Patchview provides insights on how customizable visual representation\ncan help sensemake, steer, and align generative AI model behaviors with the\nuser's intentions.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to UIST2024",
    "pdf_url": "http://arxiv.org/pdf/2408.04112v1",
    "published_date": "2024-08-07 22:27:19 UTC",
    "updated_date": "2024-08-07 22:27:19 UTC"
  },
  {
    "arxiv_id": "2408.04104v3",
    "title": "Hardware-Assisted Virtualization of Neural Processing Units for Cloud Platforms",
    "authors": [
      "Yuqi Xue",
      "Yiqi Liu",
      "Lifeng Nai",
      "Jian Huang"
    ],
    "abstract": "Cloud platforms today have been deploying hardware accelerators like neural\nprocessing units (NPUs) for powering machine learning (ML) inference services.\nTo maximize the resource utilization while ensuring reasonable quality of\nservice, a natural approach is to virtualize NPUs for efficient resource\nsharing for multi-tenant ML services. However, virtualizing NPUs for modern\ncloud platforms is not easy. This is not only due to the lack of system\nabstraction support for NPU hardware, but also due to the lack of architectural\nand ISA support for enabling fine-grained dynamic operator scheduling for\nvirtualized NPUs.\n  We present Neu10, a holistic NPU virtualization framework. We investigate\nvirtualization techniques for NPUs across the entire software and hardware\nstack. Neu10 consists of (1) a flexible NPU abstraction called vNPU, which\nenables fine-grained virtualization of the heterogeneous compute units in a\nphysical NPU (pNPU); (2) a vNPU resource allocator that enables pay-as-you-go\ncomputing model and flexible vNPU-to-pNPU mappings for improved resource\nutilization and cost-effectiveness; (3) an ISA extension of modern NPU\narchitecture for facilitating fine-grained tensor operator scheduling for\nmultiple vNPUs. We implement Neu10 based on a production-level NPU simulator.\nOur experiments show that Neu10 improves the throughput of ML inference\nservices by up to 1.4$\\times$ and reduces the tail latency by up to\n4.6$\\times$, while improving the NPU utilization by 1.2$\\times$ on average,\ncompared to state-of-the-art NPU sharing approaches.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG",
      "cs.OS"
    ],
    "primary_category": "cs.AR",
    "comment": "Accepted to MICRO'24",
    "pdf_url": "http://arxiv.org/pdf/2408.04104v3",
    "published_date": "2024-08-07 21:45:01 UTC",
    "updated_date": "2024-09-13 02:48:33 UTC"
  },
  {
    "arxiv_id": "2408.04102v3",
    "title": "ArtVLM: Attribute Recognition Through Vision-Based Prefix Language Modeling",
    "authors": [
      "William Yicheng Zhu",
      "Keren Ye",
      "Junjie Ke",
      "Jiahui Yu",
      "Leonidas Guibas",
      "Peyman Milanfar",
      "Feng Yang"
    ],
    "abstract": "Recognizing and disentangling visual attributes from objects is a foundation\nto many computer vision applications. While large vision language\nrepresentations like CLIP had largely resolved the task of zero-shot object\nrecognition, zero-shot visual attribute recognition remains a challenge because\nCLIP's contrastively-learned vision-language representation cannot effectively\ncapture object-attribute dependencies. In this paper, we target this weakness\nand propose a sentence generation-based retrieval formulation for attribute\nrecognition that is novel in 1) explicitly modeling a to-be-measured and\nretrieved object-attribute relation as a conditional probability graph, which\nconverts the recognition problem into a dependency-sensitive language-modeling\nproblem, and 2) applying a large pretrained Vision-Language Model (VLM) on this\nreformulation and naturally distilling its knowledge of image-object-attribute\nrelations to use towards attribute recognition. Specifically, for each\nattribute to be recognized on an image, we measure the visual-conditioned\nprobability of generating a short sentence encoding the attribute's relation to\nobjects on the image. Unlike contrastive retrieval, which measures likelihood\nby globally aligning elements of the sentence to the image, generative\nretrieval is sensitive to the order and dependency of objects and attributes in\nthe sentence. We demonstrate through experiments that generative retrieval\nconsistently outperforms contrastive retrieval on two visual reasoning\ndatasets, Visual Attribute in the Wild (VAW), and our newly-proposed Visual\nGenome Attribute Ranking (VGARank).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ECCV 2024. Contact: zhuwilliam[at]google[dot]com. GitHub:\n  https://github.com/google-research/google-research/tree/master/attribute_with_prefixlm",
    "pdf_url": "http://arxiv.org/pdf/2408.04102v3",
    "published_date": "2024-08-07 21:44:29 UTC",
    "updated_date": "2024-10-02 12:48:44 UTC"
  },
  {
    "arxiv_id": "2408.04675v1",
    "title": "ACL Ready: RAG Based Assistant for the ACL Checklist",
    "authors": [
      "Michael Galarnyk",
      "Rutwik Routu",
      "Kosha Bheda",
      "Priyanshu Mehta",
      "Agam Shah",
      "Sudheer Chava"
    ],
    "abstract": "The ARR Responsible NLP Research checklist website states that the \"checklist\nis designed to encourage best practices for responsible research, addressing\nissues of research ethics, societal impact and reproducibility.\" Answering the\nquestions is an opportunity for authors to reflect on their work and make sure\nany shared scientific assets follow best practices. Ideally, considering the\nchecklist before submission can favorably impact the writing of a research\npaper. However, the checklist is often filled out at the last moment. In this\nwork, we introduce ACLReady, a retrieval-augmented language model application\nthat can be used to empower authors to reflect on their work and assist authors\nwith the ACL checklist. To test the effectiveness of the system, we conducted a\nqualitative study with 13 users which shows that 92% of users found the\napplication useful and easy to use as well as 77% of the users found that the\napplication provided the information they expected. Our code is publicly\navailable under the CC BY-NC 4.0 license on GitHub.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.04675v1",
    "published_date": "2024-08-07 21:07:13 UTC",
    "updated_date": "2024-08-07 21:07:13 UTC"
  },
  {
    "arxiv_id": "2408.07220v1",
    "title": "Handwritten Code Recognition for Pen-and-Paper CS Education",
    "authors": [
      "Md Sazzad Islam",
      "Moussa Koulako Bala Doumbouya",
      "Christopher D. Manning",
      "Chris Piech"
    ],
    "abstract": "Teaching Computer Science (CS) by having students write programs by hand on\npaper has key pedagogical advantages: It allows focused learning and requires\ncareful thinking compared to the use of Integrated Development Environments\n(IDEs) with intelligent support tools or \"just trying things out\". The familiar\nenvironment of pens and paper also lessens the cognitive load of students with\nno prior experience with computers, for whom the mere basic usage of computers\ncan be intimidating. Finally, this teaching approach opens learning\nopportunities to students with limited access to computers.\n  However, a key obstacle is the current lack of teaching methods and support\nsoftware for working with and running handwritten programs. Optical character\nrecognition (OCR) of handwritten code is challenging: Minor OCR errors, perhaps\ndue to varied handwriting styles, easily make code not run, and recognizing\nindentation is crucial for languages like Python but is difficult to do due to\ninconsistent horizontal spacing in handwriting. Our approach integrates two\ninnovative methods. The first combines OCR with an indentation recognition\nmodule and a language model designed for post-OCR error correction without\nintroducing hallucinations. This method, to our knowledge, surpasses all\nexisting systems in handwritten code recognition. It reduces error from 30\\% in\nthe state of the art to 5\\% with minimal hallucination of logical fixes to\nstudent programs. The second method leverages a multimodal language model to\nrecognize handwritten programs in an end-to-end fashion. We hope this\ncontribution can stimulate further pedagogical research and contribute to the\ngoal of making CS education universally accessible. We release a dataset of\nhandwritten programs and code to support future research at\nhttps://github.com/mdoumbouya/codeocr",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.07220v1",
    "published_date": "2024-08-07 21:02:17 UTC",
    "updated_date": "2024-08-07 21:02:17 UTC"
  },
  {
    "arxiv_id": "2408.04072v1",
    "title": "AEye: A Visualization Tool for Image Datasets",
    "authors": [
      "Florian Grötschla",
      "Luca A. Lanzendörfer",
      "Marco Calzavara",
      "Roger Wattenhofer"
    ],
    "abstract": "Image datasets serve as the foundation for machine learning models in\ncomputer vision, significantly influencing model capabilities, performance, and\nbiases alongside architectural considerations. Therefore, understanding the\ncomposition and distribution of these datasets has become increasingly crucial.\nTo address the need for intuitive exploration of these datasets, we propose\nAEye, an extensible and scalable visualization tool tailored to image datasets.\nAEye utilizes a contrastively trained model to embed images into semantically\nmeaningful high-dimensional representations, facilitating data clustering and\norganization. To visualize the high-dimensional representations, we project\nthem onto a two-dimensional plane and arrange images in layers so users can\nseamlessly navigate and explore them interactively. AEye facilitates semantic\nsearch functionalities for both text and image queries, enabling users to\nsearch for content. We open-source the codebase for AEye, and provide a simple\nconfiguration to add datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at IEEE VIS 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.04072v1",
    "published_date": "2024-08-07 20:19:20 UTC",
    "updated_date": "2024-08-07 20:19:20 UTC"
  },
  {
    "arxiv_id": "2408.04068v1",
    "title": "Digital Avatars: Framework Development and Their Evaluation",
    "authors": [
      "Timothy Rupprecht",
      "Sung-En Chang",
      "Yushu Wu",
      "Lei Lu",
      "Enfu Nan",
      "Chih-hsiang Li",
      "Caiyue Lai",
      "Zhimin Li",
      "Zhijun Hu",
      "Yumei He",
      "David Kaeli",
      "Yanzhi Wang"
    ],
    "abstract": "We present a novel prompting strategy for artificial intelligence driven\ndigital avatars. To better quantify how our prompting strategy affects\nanthropomorphic features like humor, authenticity, and favorability we present\nCrowd Vote - an adaptation of Crowd Score that allows for judges to elect a\nlarge language model (LLM) candidate over competitors answering the same or\nsimilar prompts. To visualize the responses of our LLM, and the effectiveness\nof our prompting strategy we propose an end-to-end framework for creating\nhigh-fidelity artificial intelligence (AI) driven digital avatars. This\npipeline effectively captures an individual's essence for interaction and our\nstreaming algorithm delivers a high-quality digital avatar with real-time\naudio-video streaming from server to mobile device. Both our visualization\ntool, and our Crowd Vote metrics demonstrate our AI driven digital avatars have\nstate-of-the-art humor, authenticity, and favorability outperforming all\ncompetitors and baselines. In the case of our Donald Trump and Joe Biden\navatars, their authenticity and favorability are rated higher than even their\nreal-world equivalents.",
    "categories": [
      "cs.AI",
      "68",
      "D.2.2; C.3"
    ],
    "primary_category": "cs.AI",
    "comment": "This work was presented during the IJCAI 2024 conference proceedings\n  for demonstrations",
    "pdf_url": "http://arxiv.org/pdf/2408.04068v1",
    "published_date": "2024-08-07 20:09:47 UTC",
    "updated_date": "2024-08-07 20:09:47 UTC"
  },
  {
    "arxiv_id": "2408.12603v1",
    "title": "Sleeper Social Bots: a new generation of AI disinformation bots are already a political threat",
    "authors": [
      "Jaiv Doshi",
      "Ines Novacic",
      "Curtis Fletcher",
      "Mats Borges",
      "Elea Zhong",
      "Mark C. Marino",
      "Jason Gan",
      "Sophia Mager",
      "Dane Sprague",
      "Melinda Xia"
    ],
    "abstract": "This paper presents a study on the growing threat of \"sleeper social bots,\"\nAI-driven social bots in the political landscape, created to spread\ndisinformation and manipulate public opinion. We based the name sleeper social\nbots on their ability to pass as humans on social platforms, where they're\nembedded like political \"sleeper\" agents, making them harder to detect and more\ndisruptive. To illustrate the threat these bots pose, our research team at the\nUniversity of Southern California constructed a demonstration using a private\nMastodon server, where ChatGPT-driven bots, programmed with distinct\npersonalities and political viewpoints, engaged in discussions with human\nparticipants about a fictional electoral proposition. Our preliminary findings\nsuggest these bots can convincingly pass as human users, actively participate\nin conversations, and effectively disseminate disinformation. Moreover, they\ncan adapt their arguments based on the responses of human interlocutors,\nshowcasing their dynamic and persuasive capabilities. College students\nparticipating in initial experiments failed to identify our bots, underscoring\nthe urgent need for increased awareness and education about the dangers of\nAI-driven disinformation, and in particular, disinformation spread by bots. The\nimplications of our research point to the significant challenges posed by\nsocial bots in the upcoming 2024 U.S. presidential election and beyond.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12603v1",
    "published_date": "2024-08-07 19:57:10 UTC",
    "updated_date": "2024-08-07 19:57:10 UTC"
  },
  {
    "arxiv_id": "2408.04057v3",
    "title": "PowerPM: Foundation Model for Power Systems",
    "authors": [
      "Shihao Tu",
      "Yupeng Zhang",
      "Jing Zhang",
      "Zhendong Fu",
      "Yin Zhang",
      "Yang Yang"
    ],
    "abstract": "The emergence of abundant electricity time series (ETS) data provides ample\nopportunities for various applications in the power systems, including\ndemand-side management, grid stability, and consumer behavior analysis. Deep\nlearning models have advanced ETS modeling by effectively capturing sequence\ndependence. Nevertheless, learning a generic representation of ETS data for\nvarious applications remains challenging due to the inherently complex\nhierarchical structure of ETS data. Moreover, ETS data exhibits intricate\ntemporal dependencies and is suscepti ble to the influence of exogenous\nvariables. Furthermore, different instances exhibit diverse electricity\nconsumption behavior. In this paper, we propose a foundation model PowerPM to\nmodel ETS data, providing a large-scale, off-the-shelf model for power systems.\nPowerPM consists of a temporal encoder and a hierarchical encoder. The temporal\nencoder captures both temporal dependencies in ETS data, considering exogenous\nvariables. The hierarchical encoder models the correlation between hierarchy.\nFurthermore, PowerPM leverages a novel self-supervised pretraining framework\nconsisting of masked ETS modeling and dual-view contrastive learning, which\nenable PowerPM to capture temporal dependency within ETS windows and aware the\ndiscrepancy across ETS windows, providing two different perspectives to learn\ngeneric representation. Our experiments involve five real world scenario\ndatasets, comprising private and public data. Through pre-training on massive\nETS data, PowerPM achieves SOTA performance on diverse downstream tasks within\nthe private dataset. Impressively, when transferred to the public datasets,\nPowerPM maintains its superiority, showcasing its remarkable generalization\nability across various tasks and domains. Moreover, ablation studies, few-shot\nexperiments provide additional evidence of the effectiveness of our model.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages, 5 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.04057v3",
    "published_date": "2024-08-07 19:39:37 UTC",
    "updated_date": "2024-10-03 14:40:12 UTC"
  },
  {
    "arxiv_id": "2408.04055v2",
    "title": "Machine Learning-Based Reward-Driven Tuning of Scanning Probe Microscopy: Towards Fully Automated Microscopy",
    "authors": [
      "Yu Liu",
      "Roger Proksch",
      "Jason Bemis",
      "Utkarsh Pratiush",
      "Astita Dubey",
      "Mahshid Ahmadi",
      "Reece Emery",
      "Philip D. Rack",
      "Yu-Chen Liu",
      "Jan-Chi Yang",
      "Sergei V. Kalinin"
    ],
    "abstract": "Since the dawn of scanning probe microscopy (SPM), tapping or intermittent\ncontact mode has been one of the most widely used imaging modes. Manual\noptimization of tapping mode not only takes a lot of instrument and operator\ntime, but also often leads to frequent probe and sample damage, poor image\nquality and reproducibility issues for new types of samples or inexperienced\nusers. Despite wide use, optimization of tapping mode imaging is an extremely\nhard problem, ill-suited to either classical control methods or machine\nlearning. Here we introduce a reward-driven workflow to automate the\noptimization of SPM in the tapping mode. The reward function is defined based\non multiple channels with physical and empirical knowledge of good scans\nencoded, representing a sample-agnostic measure of image quality and imitating\nthe decision-making logic employed by human operators. This automated workflow\ngives optimal scanning parameters for different probes and samples and gives\nhigh-quality SPM images consistently in the attractive mode. This study\nbroadens the application and accessibility of SPM and opens the door for fully\nautomated SPM.",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cond-mat.mes-hall",
    "comment": "20 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.04055v2",
    "published_date": "2024-08-07 19:34:42 UTC",
    "updated_date": "2024-12-25 19:28:04 UTC"
  },
  {
    "arxiv_id": "2408.04054v2",
    "title": "PLANRL: A Motion Planning and Imitation Learning Framework to Bootstrap Reinforcement Learning",
    "authors": [
      "Amisha Bhaskar",
      "Zahiruddin Mahammad",
      "Sachin R Jadhav",
      "Pratap Tokekar"
    ],
    "abstract": "Reinforcement Learning (RL) has shown remarkable progress in simulation\nenvironments, yet its application to real-world robotic tasks remains limited\ndue to challenges in exploration and generalization. To address these issues,\nwe introduce PLANRL, a framework that chooses when the robot should use\nclassical motion planning and when it should learn a policy. To further improve\nthe efficiency in exploration, we use imitation data to bootstrap the\nexploration. PLANRL dynamically switches between two modes of operation:\nreaching a waypoint using classical techniques when away from the objects and\nreinforcement learning for fine-grained manipulation control when about to\ninteract with objects. PLANRL architecture is composed of ModeNet for mode\nclassification, NavNet for waypoint prediction, and InteractNet for precise\nmanipulation. By combining the strengths of RL and Imitation Learning (IL),\nPLANRL improves sample efficiency and mitigates distribution shift, ensuring\nrobust task execution. We evaluate our approach across multiple challenging\nsimulation environments and real-world tasks, demonstrating superior\nperformance in terms of adaptability, efficiency, and generalization compared\nto existing methods. In simulations, PLANRL surpasses baseline methods by\n10-15\\% in training success rates at 30k samples and by 30-40\\% during\nevaluation phases. In real-world scenarios, it demonstrates a 30-40\\% higher\nsuccess rate on simpler tasks compared to baselines and uniquely succeeds in\ncomplex, two-stage manipulation tasks. Datasets and supplementary materials can\nbe found on our {https://raaslab.org/projects/NAVINACT/}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.04054v2",
    "published_date": "2024-08-07 19:30:08 UTC",
    "updated_date": "2024-10-17 00:21:46 UTC"
  },
  {
    "arxiv_id": "2408.04046v1",
    "title": "Learning Rate-Free Reinforcement Learning: A Case for Model Selection with Non-Stationary Objectives",
    "authors": [
      "Aida Afshar",
      "Aldo Pacchiano"
    ],
    "abstract": "The performance of reinforcement learning (RL) algorithms is sensitive to the\nchoice of hyperparameters, with the learning rate being particularly\ninfluential. RL algorithms fail to reach convergence or demand an extensive\nnumber of samples when the learning rate is not optimally set. In this work, we\nshow that model selection can help to improve the failure modes of RL that are\ndue to suboptimal choices of learning rate. We present a model selection\nframework for Learning Rate-Free Reinforcement Learning that employs model\nselection methods to select the optimal learning rate on the fly. This approach\nof adaptive learning rate tuning neither depends on the underlying RL algorithm\nnor the optimizer and solely uses the reward feedback to select the learning\nrate; hence, the framework can input any RL algorithm and produce a learning\nrate-free version of it. We conduct experiments for policy optimization methods\nand evaluate various model selection strategies within our framework. Our\nresults indicate that data-driven model selection algorithms are better\nalternatives to standard bandit algorithms when the optimal choice of\nhyperparameter is time-dependent and non-stationary.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "RLC 2024 Workshop on Failure Modes of Sequential Decision-Making in\n  Practice",
    "pdf_url": "http://arxiv.org/pdf/2408.04046v1",
    "published_date": "2024-08-07 18:55:58 UTC",
    "updated_date": "2024-08-07 18:55:58 UTC"
  },
  {
    "arxiv_id": "2408.04026v1",
    "title": "Multimodal Gender Fairness in Depression Prediction: Insights on Data from the USA & China",
    "authors": [
      "Joseph Cameron",
      "Jiaee Cheong",
      "Micol Spitale",
      "Hatice Gunes"
    ],
    "abstract": "Social agents and robots are increasingly being used in wellbeing settings.\nHowever, a key challenge is that these agents and robots typically rely on\nmachine learning (ML) algorithms to detect and analyse an individual's mental\nwellbeing. The problem of bias and fairness in ML algorithms is becoming an\nincreasingly greater source of concern. In concurrence, existing literature has\nalso indicated that mental health conditions can manifest differently across\ngenders and cultures. We hypothesise that the representation of features\n(acoustic, textual, and visual) and their inter-modal relations would vary\namong subjects from different cultures and genders, thus impacting the\nperformance and fairness of various ML models. We present the very first\nevaluation of multimodal gender fairness in depression manifestation by\nundertaking a study on two different datasets from the USA and China. We\nundertake thorough statistical and ML experimentation and repeat the\nexperiments for several different algorithms to ensure that the results are not\nalgorithm-dependent. Our findings indicate that though there are differences\nbetween both datasets, it is not conclusive whether this is due to the\ndifference in depression manifestation as hypothesised or other external\nfactors such as differences in data collection methodology. Our findings\nfurther motivate a call for a more consistent and culturally aware data\ncollection process in order to address the problem of ML bias in depression\ndetection and to promote the development of fairer agents and robots for\nwellbeing.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "9 Pages, 7 Tables. To be published and indexed in the IEEE Xplore\n  Digital Library under the ACII 2024 Workshop Proceedings",
    "pdf_url": "http://arxiv.org/pdf/2408.04026v1",
    "published_date": "2024-08-07 18:19:18 UTC",
    "updated_date": "2024-08-07 18:19:18 UTC"
  },
  {
    "arxiv_id": "2408.04023v1",
    "title": "Improving Large Language Model (LLM) fidelity through context-aware grounding: A systematic approach to reliability and veracity",
    "authors": [
      "Wrick Talukdar",
      "Anjanava Biswas"
    ],
    "abstract": "As Large Language Models (LLMs) become increasingly sophisticated and\nubiquitous in natural language processing (NLP) applications, ensuring their\nrobustness, trustworthiness, and alignment with human values has become a\ncritical challenge. This paper presents a novel framework for contextual\ngrounding in textual models, with a particular emphasis on the Context\nRepresentation stage. Our approach aims to enhance the reliability and ethical\nalignment of these models through a comprehensive, context-aware methodology.\nBy explicitly capturing and representing relevant situational, cultural, and\nethical contexts in a machine-readable format, we lay the foundation for\nanchoring a model's behavior within these contexts. Our approach leverages\ntechniques from knowledge representation and reasoning, such as ontologies,\nsemantic web technologies, and logic-based formalisms. We evaluate our\nframework on real-world textual datasets, demonstrating its effectiveness in\nimproving model performance, fairness, and alignment with human expectations,\nwhile maintaining high accuracy. Furthermore, we discuss the other key\ncomponents of the framework, including context-aware encoding, context-aware\nlearning, interpretability and explainability, and continuous monitoring and\nadaptation. This research contributes to the growing body of work on\nresponsible AI, offering a practical approach to developing more reliable,\ntrustworthy, and ethically-aligned language models. Our findings have\nsignificant implications for the deployment of LLMs in sensitive domains such\nas healthcare, legal systems, and social services, where contextual\nunderstanding is paramount.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.04023v1",
    "published_date": "2024-08-07 18:12:02 UTC",
    "updated_date": "2024-08-07 18:12:02 UTC"
  },
  {
    "arxiv_id": "2408.07081v3",
    "title": "MathBridge: A Large Corpus Dataset for Translating Spoken Mathematical Expressions into $LaTeX$ Formulas for Improved Readability",
    "authors": [
      "Kyudan Jung",
      "Sieun Hyeon",
      "Jeong Youn Kwon",
      "Nam-Joon Kim",
      "Hyun Gon Ryu",
      "Hyuk-Jae Lee",
      "Jaeyoung Do"
    ],
    "abstract": "Improving the readability of mathematical expressions in text-based document\nsuch as subtitle of mathematical video, is an significant task. To achieve\nthis, mathematical expressions should be convert to compiled formulas. For\ninstance, the spoken expression ``x equals minus b plus or minus the square\nroot of b squared minus four a c, all over two a'' from automatic speech\nrecognition is more readily comprehensible when displayed as a compiled formula\n$x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$. To convert mathematical spoken\nsentences to compiled formulas, two processes are required: spoken sentences\nare converted into LaTeX formulas, and LaTeX formulas are converted into\ncompiled formulas. The latter can be managed by using LaTeX engines. However,\nthere is no way to do the former effectively. Even if we try to solve this\nusing language models, there is no paired data between spoken sentences and\nLaTeX formulas to train it. In this paper, we introduce MathBridge, the first\nextensive dataset for translating mathematical spoken sentences into LaTeX\nformulas. MathBridge comprises approximately 23 million LaTeX formulas paired\nwith the corresponding mathematical spoken sentences. Through comprehensive\nevaluations, including fine-tuning with proposed data, we discovered that\nMathBridge significantly enhances the capabilities of pretrained language\nmodels for converting to LaTeX formulas from mathematical spoken sentences.\nSpecifically, for the T5-large model, the sacreBLEU score increased from 4.77\nto 46.8, demonstrating substantial enhancement.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.07081v3",
    "published_date": "2024-08-07 18:07:15 UTC",
    "updated_date": "2024-08-16 09:54:23 UTC"
  },
  {
    "arxiv_id": "2408.03936v1",
    "title": "SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic Performance for Mercosur Common Nomenclature",
    "authors": [
      "Vinícius Di Oliveira",
      "Yuri Façanha Bezerra",
      "Li Weigang",
      "Pedro Carvalho Brom",
      "Victor Rafael R. Celestino"
    ],
    "abstract": "Natural language processing (NLP) has seen significant advancements with the\nadvent of large language models (LLMs). However, substantial improvements are\nstill needed for languages other than English, especially for specific domains\nlike the applications of Mercosur Common Nomenclature (NCM), a Brazilian\nHarmonized System (HS). To address this gap, this study uses TeenyTineLLaMA, a\nfoundational Portuguese LLM, as an LLM source to implement the NCM application\nprocessing. Additionally, a simplified Retrieval-Augmented Fine-Tuning (RAFT)\ntechnique, termed SLIM-RAFT, is proposed for task-specific fine-tuning of LLMs.\nThis approach retains the chain-of-thought (CoT) methodology for prompt\ndevelopment in a more concise and streamlined manner, utilizing brief and\nfocused documents for training. The proposed model demonstrates an efficient\nand cost-effective alternative for fine-tuning smaller LLMs, significantly\noutperforming TeenyTineLLaMA and ChatGPT-4 in the same task. Although the\nresearch focuses on NCM applications, the methodology can be easily adapted for\nHS applications worldwide.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 1 figure, to be publish in International Conference on Web\n  Information Systems and Technologies - WEBIST 2024 proceedings",
    "pdf_url": "http://arxiv.org/pdf/2408.03936v1",
    "published_date": "2024-08-07 17:54:21 UTC",
    "updated_date": "2024-08-07 17:54:21 UTC"
  },
  {
    "arxiv_id": "2408.04673v1",
    "title": "AutoFAIR : Automatic Data FAIRification via Machine Reading",
    "authors": [
      "Tingyan Ma",
      "Wei Liu",
      "Bin Lu",
      "Xiaoying Gan",
      "Yunqiang Zhu",
      "Luoyi Fu",
      "Chenghu Zhou"
    ],
    "abstract": "The explosive growth of data fuels data-driven research, facilitating\nprogress across diverse domains. The FAIR principles emerge as a guiding\nstandard, aiming to enhance the findability, accessibility, interoperability,\nand reusability of data. However, current efforts primarily focus on manual\ndata FAIRification, which can only handle targeted data and lack efficiency. To\naddress this issue, we propose AutoFAIR, an architecture designed to enhance\ndata FAIRness automately. Firstly, We align each data and metadata operation\nwith specific FAIR indicators to guide machine-executable actions. Then, We\nutilize Web Reader to automatically extract metadata based on language models,\neven in the absence of structured data webpage schemas. Subsequently, FAIR\nAlignment is employed to make metadata comply with FAIR principles by ontology\nguidance and semantic matching. Finally, by applying AutoFAIR to various data,\nespecially in the field of mountain hazards, we observe significant\nimprovements in findability, accessibility, interoperability, and reusability\nof data. The FAIRness scores before and after applying AutoFAIR indicate\nenhanced data value.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.04673v1",
    "published_date": "2024-08-07 17:36:58 UTC",
    "updated_date": "2024-08-07 17:36:58 UTC"
  },
  {
    "arxiv_id": "2408.11843v2",
    "title": "Identifying and Mitigating Social Bias Knowledge in Language Models",
    "authors": [
      "Ruizhe Chen",
      "Yichen Li",
      "Jianfei Yang",
      "Joey Tianyi Zhou",
      "Jian Wu",
      "Zuozhu Liu"
    ],
    "abstract": "Generating fair and accurate predictions plays a pivotal role in deploying\nlarge language models (LLMs) in the real world. However, existing debiasing\nmethods inevitably generate unfair or incorrect predictions as they are\ndesigned and evaluated to achieve parity across different social groups but\nleave aside individual commonsense facts, resulting in modified knowledge that\nelicits unreasonable or undesired predictions. In this paper, we first\nestablish a new bias mitigation benchmark, BiaScope, which systematically\nassesses performance by leveraging newly constructed datasets and metrics on\nknowledge retention and generalization. Then, we propose a novel debiasing\napproach, Fairness Stamp (FAST), which enables fine-grained calibration of\nindividual social biases. FAST identifies the decisive layer responsible for\nstoring social biases and then calibrates its outputs by integrating a small\nmodular network, considering both bias mitigation and knowledge-preserving\ndemands. Comprehensive experiments demonstrate that FAST surpasses\nstate-of-the-art baselines with superior debiasing performance while not\ncompromising the overall model capability for knowledge retention and\ndownstream predictions. This highlights the potential of fine-grained debiasing\nstrategies to achieve fairness in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025 Findings. arXiv admin note: substantial text overlap with\n  arXiv:2405.09341",
    "pdf_url": "http://arxiv.org/pdf/2408.11843v2",
    "published_date": "2024-08-07 17:14:58 UTC",
    "updated_date": "2025-02-27 10:11:06 UTC"
  },
  {
    "arxiv_id": "2408.03910v2",
    "title": "CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases",
    "authors": [
      "Xiangyan Liu",
      "Bo Lan",
      "Zhiyuan Hu",
      "Yang Liu",
      "Zhicheng Zhang",
      "Fei Wang",
      "Michael Shieh",
      "Wenmeng Zhou"
    ],
    "abstract": "Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval\nand MBPP, but struggle with handling entire code repositories. This challenge\nhas prompted research on enhancing LLM-codebase interaction at a repository\nscale. Current solutions rely on similarity-based retrieval or manual tools and\nAPIs, each with notable drawbacks. Similarity-based retrieval often has low\nrecall in complex tasks, while manual tools and APIs are typically\ntask-specific and require expert knowledge, reducing their generalizability\nacross diverse code tasks and real-world applications. To mitigate these\nlimitations, we introduce CodexGraph, a system that integrates LLM agents with\ngraph database interfaces extracted from code repositories. By leveraging the\nstructural properties of graph databases and the flexibility of the graph query\nlanguage, CodexGraph enables the LLM agent to construct and execute queries,\nallowing for precise, code structure-aware context retrieval and code\nnavigation. We assess CodexGraph using three benchmarks: CrossCodeEval,\nSWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding\napplications. With a unified graph database schema, CodexGraph demonstrates\ncompetitive performance and potential in both academic and real-world\nenvironments, showcasing its versatility and efficacy in software engineering.\nOur application demo:\nhttps://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "work in progress",
    "pdf_url": "http://arxiv.org/pdf/2408.03910v2",
    "published_date": "2024-08-07 17:13:59 UTC",
    "updated_date": "2024-08-11 16:23:57 UTC"
  },
  {
    "arxiv_id": "2408.03909v1",
    "title": "LaFA: Latent Feature Attacks on Non-negative Matrix Factorization",
    "authors": [
      "Minh Vu",
      "Ben Nebgen",
      "Erik Skau",
      "Geigh Zollicoffer",
      "Juan Castorena",
      "Kim Rasmussen",
      "Boian Alexandrov",
      "Manish Bhattarai"
    ],
    "abstract": "As Machine Learning (ML) applications rapidly grow, concerns about\nadversarial attacks compromising their reliability have gained significant\nattention. One unsupervised ML method known for its resilience to such attacks\nis Non-negative Matrix Factorization (NMF), an algorithm that decomposes input\ndata into lower-dimensional latent features. However, the introduction of\npowerful computational tools such as Pytorch enables the computation of\ngradients of the latent features with respect to the original data, raising\nconcerns about NMF's reliability. Interestingly, naively deriving the\nadversarial loss for NMF as in the case of ML would result in the\nreconstruction loss, which can be shown theoretically to be an ineffective\nattacking objective. In this work, we introduce a novel class of attacks in NMF\ntermed Latent Feature Attacks (LaFA), which aim to manipulate the latent\nfeatures produced by the NMF process. Our method utilizes the Feature Error\n(FE) loss directly on the latent features. By employing FE loss, we generate\nperturbations in the original data that significantly affect the extracted\nlatent features, revealing vulnerabilities akin to those found in other ML\ntechniques. To handle large peak-memory overhead from gradient back-propagation\nin FE attacks, we develop a method based on implicit differentiation which\nenables their scaling to larger datasets. We validate NMF vulnerabilities and\nFE attacks effectiveness through extensive experiments on synthetic and\nreal-world data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "LA-UR-24-26951",
    "pdf_url": "http://arxiv.org/pdf/2408.03909v1",
    "published_date": "2024-08-07 17:13:46 UTC",
    "updated_date": "2024-08-07 17:13:46 UTC"
  },
  {
    "arxiv_id": "2408.03907v1",
    "title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models",
    "authors": [
      "Shachi H Kumar",
      "Saurav Sahay",
      "Sahisnu Mazumder",
      "Eda Okur",
      "Ramesh Manuvinakurike",
      "Nicole Beckage",
      "Hsuan Su",
      "Hung-yi Lee",
      "Lama Nachman"
    ],
    "abstract": "Large Language Models (LLMs) have excelled at language understanding and\ngenerating human-level text. However, even with supervised training and human\nalignment, these LLMs are susceptible to adversarial attacks where malicious\nusers can prompt the model to generate undesirable text. LLMs also inherently\nencode potential biases that can cause various harmful effects during\ninteractions. Bias evaluation metrics lack standards as well as consensus and\nexisting methods often rely on human-generated templates and annotations which\nare expensive and labor intensive. In this work, we train models to\nautomatically create adversarial prompts to elicit biased responses from target\nLLMs. We present LLM- based bias evaluation metrics and also analyze several\nexisting automatic evaluation methods and metrics. We analyze the various\nnuances of model responses, identify the strengths and weaknesses of model\nfamilies, and assess where evaluation methods fall short. We compare these\nmetrics to human evaluation and validate that the LLM-as-a-Judge metric aligns\nwith human judgement on bias in response generation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages paper content, 17 pages of appendix",
    "pdf_url": "http://arxiv.org/pdf/2408.03907v1",
    "published_date": "2024-08-07 17:11:34 UTC",
    "updated_date": "2024-08-07 17:11:34 UTC"
  },
  {
    "arxiv_id": "2408.11060v1",
    "title": "Dynamic Code Orchestration: Harnessing the Power of Large Language Models for Adaptive Script Execution",
    "authors": [
      "Justin Del Vecchio",
      "Andrew Perreault",
      "Eliana Furmanek"
    ],
    "abstract": "Computer programming initially required humans to directly translate their\ngoals into machine code. These goals could have easily been expressed as a\nwritten (or human) language directive. Computers, however, had no capacity to\nsatisfactorily interpret written language. Large language model's provide\nexactly this capability; automatic generation of computer programs or even\nassembly code from written language directives. This research examines dynamic\ncode execution of written language directives within the context of a running\napplication. It implements a text editor whose business logic is purely backed\nby large language model prompts. That is, the program's execution uses prompts\nand written language directives to dynamically generate application logic at\nthe point in time it is needed. The research clearly shows how written language\ndirectives, backed by a large language model, offer radically new programming\nand operating system paradigms. For example, empowerment of users to directly\nimplement requirements via written language directives, thus supplanting the\nneed for a team ofprogrammers, a release schedule and the like. Or, new\nsecurity mechanisms where static executables, always a target for reverse\nengineering or fuzzing, no longer exist. They are replaced by ephemeral\nexecutables that may continually change, be completely removed, and are easily\nupdated.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.SE",
    "comment": "6 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.11060v1",
    "published_date": "2024-08-07 17:11:31 UTC",
    "updated_date": "2024-08-07 17:11:31 UTC"
  },
  {
    "arxiv_id": "2408.03904v1",
    "title": "Lightweight Video Denoising Using a Classic Bayesian Backbone",
    "authors": [
      "Clément Bled",
      "François Pitié"
    ],
    "abstract": "In recent years, state-of-the-art image and video denoising networks have\nbecome increasingly large, requiring millions of trainable parameters to\nachieve best-in-class performance. Improved denoising quality has come at the\ncost of denoising speed, where modern transformer networks are far slower to\nrun than smaller denoising networks such as FastDVDnet and classic Bayesian\ndenoisers such as the Wiener filter.\n  In this paper, we implement a hybrid Wiener filter which leverages small\nancillary networks to increase the original denoiser performance, while\nretaining fast denoising speeds. These networks are used to refine the Wiener\ncoring estimate, optimise windowing functions and estimate the unknown noise\nprofile. Using these methods, we outperform several popular denoisers and\nremain within 0.2 dB, on average, of the popular VRT transformer. Our method\nwas found to be over x10 faster than the transformer method, with a far lower\nparameter cost.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "eess.SP"
    ],
    "primary_category": "eess.IV",
    "comment": "Paper accepted to ICME 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.03904v1",
    "published_date": "2024-08-07 17:08:46 UTC",
    "updated_date": "2024-08-07 17:08:46 UTC"
  },
  {
    "arxiv_id": "2408.03899v1",
    "title": "Simplifying Scholarly Abstracts for Accessible Digital Libraries",
    "authors": [
      "Haining Wang",
      "Jason Clark"
    ],
    "abstract": "Standing at the forefront of knowledge dissemination, digital libraries\ncurate vast collections of scientific literature. However, these scholarly\nwritings are often laden with jargon and tailored for domain experts rather\nthan the general public. As librarians, we strive to offer services to a\ndiverse audience, including those with lower reading levels. To extend our\nservices beyond mere access, we propose fine-tuning a language model to rewrite\nscholarly abstracts into more comprehensible versions, thereby making scholarly\nliterature more accessible when requested. We began by introducing a corpus\nspecifically designed for training models to simplify scholarly abstracts. This\ncorpus consists of over three thousand pairs of abstracts and significance\nstatements from diverse disciplines. We then fine-tuned four language models\nusing this corpus. The outputs from the models were subsequently examined both\nquantitatively for accessibility and semantic coherence, and qualitatively for\nlanguage quality, faithfulness, and completeness. Our findings show that the\nresulting models can improve readability by over three grade levels, while\nmaintaining fidelity to the original content. Although commercial\nstate-of-the-art models still hold an edge, our models are much more compact,\ncan be deployed locally in an affordable manner, and alleviate the privacy\nconcerns associated with using commercial models. We envision this work as a\nstep toward more inclusive and accessible libraries, improving our services for\nyoung readers and those without a college degree.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.DL"
    ],
    "primary_category": "cs.CL",
    "comment": "Initial submission to JCDL2024",
    "pdf_url": "http://arxiv.org/pdf/2408.03899v1",
    "published_date": "2024-08-07 16:55:00 UTC",
    "updated_date": "2024-08-07 16:55:00 UTC"
  },
  {
    "arxiv_id": "2408.03892v1",
    "title": "MORTAR: A Model-based Runtime Action Repair Framework for AI-enabled Cyber-Physical Systems",
    "authors": [
      "Renzhi Wang",
      "Zhehua Zhou",
      "Jiayang Song",
      "Xuan Xie",
      "Xiaofei Xie",
      "Lei Ma"
    ],
    "abstract": "Cyber-Physical Systems (CPSs) are increasingly prevalent across various\nindustrial and daily-life domains, with applications ranging from robotic\noperations to autonomous driving. With recent advancements in artificial\nintelligence (AI), learning-based components, especially AI controllers, have\nbecome essential in enhancing the functionality and efficiency of CPSs.\nHowever, the lack of interpretability in these AI controllers presents\nchallenges to the safety and quality assurance of AI-enabled CPSs (AI-CPSs).\nExisting methods for improving the safety of AI controllers often involve\nneural network repair, which requires retraining with additional adversarial\nexamples or access to detailed internal information of the neural network.\nHence, these approaches have limited applicability for black-box policies,\nwhere only the inputs and outputs are accessible during operation. To overcome\nthis, we propose MORTAR, a runtime action repair framework designed for AI-CPSs\nin this work. MORTAR begins by constructing a prediction model that forecasts\nthe quality of actions proposed by the AI controller. If an unsafe action is\ndetected, MORTAR then initiates a repair process to correct it. The generation\nof repaired actions is achieved through an optimization process guided by the\nsafety estimates from the prediction model. We evaluate the effectiveness of\nMORTAR across various CPS tasks and AI controllers. The results demonstrate\nthat MORTAR can efficiently improve task completion rates of AI controllers\nunder specified safety specifications. Meanwhile, it also maintains minimal\ncomputational overhead, ensuring real-time operation of the AI-CPSs.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03892v1",
    "published_date": "2024-08-07 16:44:53 UTC",
    "updated_date": "2024-08-07 16:44:53 UTC"
  },
  {
    "arxiv_id": "2408.03877v1",
    "title": "Knowledge Probing for Graph Representation Learning",
    "authors": [
      "Mingyu Zhao",
      "Xingyu Huang",
      "Ziyu Lyu",
      "Yanlin Wang",
      "Lixin Cui",
      "Lu Bai"
    ],
    "abstract": "Graph learning methods have been extensively applied in diverse application\nareas. However, what kind of inherent graph properties e.g. graph proximity,\ngraph structural information has been encoded into graph representation\nlearning for downstream tasks is still under-explored. In this paper, we\npropose a novel graph probing framework (GraphProbe) to investigate and\ninterpret whether the family of graph learning methods has encoded different\nlevels of knowledge in graph representation learning. Based on the intrinsic\nproperties of graphs, we design three probes to systematically investigate the\ngraph representation learning process from different perspectives, respectively\nthe node-wise level, the path-wise level, and the structural level. We\nconstruct a thorough evaluation benchmark with nine representative graph\nlearning methods from random walk based approaches, basic graph neural networks\nand self-supervised graph methods, and probe them on six benchmark datasets for\nnode classification, link prediction and graph classification. The experimental\nevaluation verify that GraphProbe can estimate the capability of graph\nrepresentation learning. Remaking results have been concluded: GCN and\nWeightedGCN methods are relatively versatile methods achieving better results\nwith respect to different tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03877v1",
    "published_date": "2024-08-07 16:27:45 UTC",
    "updated_date": "2024-08-07 16:27:45 UTC"
  },
  {
    "arxiv_id": "2408.03872v1",
    "title": "Inter-Series Transformer: Attending to Products in Time Series Forecasting",
    "authors": [
      "Rares Cristian",
      "Pavithra Harsha",
      "Clemente Ocejo",
      "Georgia Perakis",
      "Brian Quanz",
      "Ioannis Spantidakis",
      "Hamza Zerhouni"
    ],
    "abstract": "Time series forecasting is an important task in many fields ranging from\nsupply chain management to weather forecasting. Recently, Transformer neural\nnetwork architectures have shown promising results in forecasting on common\ntime series benchmark datasets. However, application to supply chain demand\nforecasting, which can have challenging characteristics such as sparsity and\ncross-series effects, has been limited.\n  In this work, we explore the application of Transformer-based models to\nsupply chain demand forecasting. In particular, we develop a new\nTransformer-based forecasting approach using a shared, multi-task per-time\nseries network with an initial component applying attention across time series,\nto capture interactions and help address sparsity. We provide a case study\napplying our approach to successfully improve demand prediction for a medical\ndevice manufacturing company. To further validate our approach, we also apply\nit to public demand forecasting datasets as well and demonstrate competitive to\nsuperior performance compared to a variety of baseline and state-of-the-art\nforecast methods across the private and public datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.6; G.3; I.5.1"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03872v1",
    "published_date": "2024-08-07 16:22:21 UTC",
    "updated_date": "2024-08-07 16:22:21 UTC"
  },
  {
    "arxiv_id": "2408.03871v2",
    "title": "Large Language Models for Biomedical Text Simplification: Promising But Not There Yet",
    "authors": [
      "Zihao Li",
      "Samuel Belkadi",
      "Nicolo Micheletti",
      "Lifeng Han",
      "Matthew Shardlow",
      "Goran Nenadic"
    ],
    "abstract": "In this system report, we describe the models and methods we used for our\nparticipation in the PLABA2023 task on biomedical abstract simplification, part\nof the TAC 2023 tracks. The system outputs we submitted come from the following\nthree categories: 1) domain fine-tuned T5-like models including Biomedical-T5\nand Lay-SciFive; 2) fine-tuned BARTLarge model with controllable attributes\n(via tokens) BART-w-CTs; 3) ChatGPTprompting. We also present the work we\ncarried out for this task on BioGPT finetuning. In the official automatic\nevaluation using SARI scores, BeeManc ranks 2nd among all teams and our model\nLaySciFive ranks 3rd among all 13 evaluated systems. In the official human\nevaluation, our model BART-w-CTs ranks 2nd on Sentence-Simplicity (score\n92.84), 3rd on Term-Simplicity (score 82.33) among all 7 evaluated systems; It\nalso produced a high score 91.57 on Fluency in comparison to the highest score\n93.53. In the second round of submissions, our team using ChatGPT-prompting\nranks the 2nd in several categories including simplified term accuracy score\n92.26 and completeness score 96.58, and a very similar score on faithfulness\nscore 95.3 to re-evaluated PLABA-base-1 (95.73) via human evaluations. Our\ncodes, fine-tuned models, prompts, and data splits from the system development\nstage will be available at https://github.com/ HECTA-UoM/PLABA-MU",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Extended system report for PLABA-2023. arXiv admin note: substantial\n  text overlap with arXiv:2309.13202",
    "pdf_url": "http://arxiv.org/pdf/2408.03871v2",
    "published_date": "2024-08-07 16:21:41 UTC",
    "updated_date": "2024-09-24 08:28:58 UTC"
  },
  {
    "arxiv_id": "2408.03841v1",
    "title": "MaxMind: A Memory Loop Network to Enhance Software Productivity based on Large Language Models",
    "authors": [
      "Yuchen Dong",
      "XiaoXiang Fang",
      "Yuchen Hu",
      "Renshuang Jiang",
      "Zhe Jiang"
    ],
    "abstract": "The application of large language models to facilitate automated software\noperations and tool generation (SOTG), thus augmenting software productivity,\nmirrors the early stages of human evolution when the ability to create and use\ntools accelerated the progress of civilization. These complex tasks require AI\nto continuously summarize and improve. Current research often overlooks the\nimportance of converting real-time task experiences into system memory and\ndifferentiating the value of existing knowledge for future reference. This\npaper addresses these issues by evolving external memory models into\nMemory-Loop Networks for timely memorization and experience referencing. We\nalso enhance a RAG mechanism with knowledge precision segmentation to utilize\nmemory based on value differentiation, and design the MaxMind model for SOTG\naccordingly.To demonstrate our approach, we developed MaxMind4Sheet, an\nelectronic spreadsheet processing system aligned with the MaxMind philosophy.\nComparative experiments with SheetCopilot have demonstrated that the\naccumulation and recycling of task memories lead to a steady enhancement in\ntask success rate, with an improvement rate of approximately 3%-6% per round in\nthis implementation example. Note that as the memories continue to grow, this\ncumulative improvement may be substantial. The inclusion of memory recycling\ncan also boost the system's task execution efficiency by up to 25%, and it can\naddress the retraining issue faced by LLMs when handling specialized tasks\nthrough memories transfer.These suggest that MaxMind has significant potential\nto enhance the capabilities and productivity of LLM systems in SOTG.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03841v1",
    "published_date": "2024-08-07 15:27:22 UTC",
    "updated_date": "2024-08-07 15:27:22 UTC"
  },
  {
    "arxiv_id": "2408.03837v3",
    "title": "WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models",
    "authors": [
      "Prannaya Gupta",
      "Le Qi Yau",
      "Hao Han Low",
      "I-Shiang Lee",
      "Hugo Maximus Lim",
      "Yu Xin Teoh",
      "Jia Hng Koh",
      "Dar Win Liew",
      "Rishabh Bhardwaj",
      "Rajat Bhardwaj",
      "Soujanya Poria"
    ],
    "abstract": "WalledEval is a comprehensive AI safety testing toolkit designed to evaluate\nlarge language models (LLMs). It accommodates a diverse range of models,\nincluding both open-weight and API-based ones, and features over 35 safety\nbenchmarks covering areas such as multilingual safety, exaggerated safety, and\nprompt injections. The framework supports both LLM and judge benchmarking and\nincorporates custom mutators to test safety against various text-style\nmutations, such as future tense and paraphrasing. Additionally, WalledEval\nintroduces WalledGuard, a new, small, and performant content moderation tool,\nand two datasets: SGXSTest and HIXSTest, which serve as benchmarks for\nassessing the exaggerated safety of LLMs and judges in cultural contexts. We\nmake WalledEval publicly available at https://github.com/walledai/walledeval.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2408.03837v3",
    "published_date": "2024-08-07 15:22:44 UTC",
    "updated_date": "2024-08-19 21:57:24 UTC"
  },
  {
    "arxiv_id": "2408.03834v1",
    "title": "Target Prompting for Information Extraction with Vision Language Model",
    "authors": [
      "Dipankar Medhi"
    ],
    "abstract": "The recent trend in the Large Vision and Language model has brought a new\nchange in how information extraction systems are built. VLMs have set a new\nbenchmark with their State-of-the-art techniques in understanding documents and\nbuilding question-answering systems across various industries. They are\nsignificantly better at generating text from document images and providing\naccurate answers to questions. However, there are still some challenges in\neffectively utilizing these models to build a precise conversational system.\nGeneral prompting techniques used with large language models are often not\nsuitable for these specially designed vision language models. The output\ngenerated by such generic input prompts is ordinary and may contain information\ngaps when compared with the actual content of the document. To obtain more\naccurate and specific answers, a well-targeted prompt is required by the vision\nlanguage model, along with the document image. In this paper, a technique is\ndiscussed called Target prompting, which focuses on explicitly targeting parts\nof document images and generating related answers from those specific regions\nonly. The paper also covers the evaluation of response for each prompting\ntechnique using different user queries and input prompts.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.03834v1",
    "published_date": "2024-08-07 15:17:51 UTC",
    "updated_date": "2024-08-07 15:17:51 UTC"
  },
  {
    "arxiv_id": "2408.03827v1",
    "title": "Automated Code Fix Suggestions for Accessibility Issues in Mobile Apps",
    "authors": [
      "Forough Mehralian",
      "Titus Barik",
      "Jeff Nichols",
      "Amanda Swearngin"
    ],
    "abstract": "Accessibility is crucial for inclusive app usability, yet developers often\nstruggle to identify and fix app accessibility issues due to a lack of\nawareness, expertise, and inadequate tools. Current accessibility testing tools\ncan identify accessibility issues but may not always provide guidance on how to\naddress them. We introduce FixAlly, an automated tool designed to suggest\nsource code fixes for accessibility issues detected by automated accessibility\nscanners. FixAlly employs a multi-agent LLM architecture to generate fix\nstrategies, localize issues within the source code, and propose code\nmodification suggestions to fix the accessibility issue. Our empirical study\ndemonstrates FixAlly's capability in suggesting fixes that resolve issues found\nby accessibility scanners -- with an effectiveness of 77% in generating\nplausible fix suggestions -- and our survey of 12 iOS developers finds they\nwould be willing to accept 69.4% of evaluated fix suggestions.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC",
      "D.2.5; I.2"
    ],
    "primary_category": "cs.SE",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.03827v1",
    "published_date": "2024-08-07 15:06:07 UTC",
    "updated_date": "2024-08-07 15:06:07 UTC"
  },
  {
    "arxiv_id": "2408.03811v1",
    "title": "Generative Language Models with Retrieval Augmented Generation for Automated Short Answer Scoring",
    "authors": [
      "Zifan Wang",
      "Christopher Ormerod"
    ],
    "abstract": "Automated Short Answer Scoring (ASAS) is a critical component in educational\nassessment. While traditional ASAS systems relied on rule-based algorithms or\ncomplex deep learning methods, recent advancements in Generative Language\nModels (GLMs) offer new opportunities for improvement. This study explores the\napplication of GLMs to ASAS, leveraging their off-the-shelf capabilities and\nperformance in various domains. We propose a novel pipeline that combines\nvector databases, transformer-based encoders, and GLMs to enhance short answer\nscoring accuracy. Our approach stores training responses in a vector database,\nretrieves semantically similar responses during inference, and employs a GLM to\nanalyze these responses and determine appropriate scores. We further optimize\nthe system through fine-tuned retrieval processes and prompt engineering.\nEvaluation on the SemEval 2013 dataset demonstrates a significant improvement\non the SCIENTSBANK 3-way and 2-way tasks compared to existing methods,\nhighlighting the potential of GLMs in advancing ASAS technology.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.03811v1",
    "published_date": "2024-08-07 14:42:13 UTC",
    "updated_date": "2024-08-07 14:42:13 UTC"
  },
  {
    "arxiv_id": "2408.03807v1",
    "title": "Navigating the Human Maze: Real-Time Robot Pathfinding with Generative Imitation Learning",
    "authors": [
      "Martin Moder",
      "Stephen Adhisaputra",
      "Josef Pauli"
    ],
    "abstract": "This paper addresses navigation in crowded environments by integrating\ngoal-conditioned generative models with Sampling-based Model Predictive Control\n(SMPC). We introduce goal-conditioned autoregressive models to generate crowd\nbehaviors, capturing intricate interactions among individuals. The model\nprocesses potential robot trajectory samples and predicts the reactions of\nsurrounding individuals, enabling proactive robotic navigation in complex\nscenarios. Extensive experiments show that this algorithm enables real-time\nnavigation, significantly reducing collision rates and path lengths, and\noutperforming selected baseline methods. The practical effectiveness of this\nalgorithm is validated on an actual robotic platform, demonstrating its\ncapability in dynamic settings.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03807v1",
    "published_date": "2024-08-07 14:32:41 UTC",
    "updated_date": "2024-08-07 14:32:41 UTC"
  },
  {
    "arxiv_id": "2408.03795v1",
    "title": "Frank's triangular norms in Piaget's logical proportions",
    "authors": [
      "Henri Prade",
      "Gilles Richard"
    ],
    "abstract": "Starting from the Boolean notion of logical proportion in Piaget's sense,\nwhich turns out to be equivalent to analogical proportion, this note proposes a\ndefinition of analogical proportion between numerical values based on\ntriangular norms (and dual co-norms). Frank's family of triangular norms is\nparticularly interesting from this perspective. The article concludes with a\ncomparative discussion with another very recent proposal for defining\nanalogical proportions between numerical values based on the family of\ngeneralized means.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.03795v1",
    "published_date": "2024-08-07 14:20:09 UTC",
    "updated_date": "2024-08-07 14:20:09 UTC"
  },
  {
    "arxiv_id": "2408.03977v1",
    "title": "Learning from Noisy Labels for Long-tailed Data via Optimal Transport",
    "authors": [
      "Mengting Li",
      "Chuang Zhu"
    ],
    "abstract": "Noisy labels, which are common in real-world datasets, can significantly\nimpair the training of deep learning models. However, recent adversarial\nnoise-combating methods overlook the long-tailed distribution of real data,\nwhich can significantly harm the effect of denoising strategies. Meanwhile, the\nmismanagement of noisy labels further compromises the model's ability to handle\nlong-tailed data. To tackle this issue, we propose a novel approach to manage\ndata characterized by both long-tailed distributions and noisy labels. First,\nwe introduce a loss-distance cross-selection module, which integrates class\npredictions and feature distributions to filter clean samples, effectively\naddressing uncertainties introduced by noisy labels and long-tailed\ndistributions. Subsequently, we employ optimal transport strategies to generate\npseudo-labels for the noise set in a semi-supervised training manner, enhancing\npseudo-label quality while mitigating the effects of sample scarcity caused by\nthe long-tailed distribution. We conduct experiments on both synthetic and\nreal-world datasets, and the comprehensive experimental results demonstrate\nthat our method surpasses current state-of-the-art methods. Our code will be\navailable in the future.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03977v1",
    "published_date": "2024-08-07 14:15:18 UTC",
    "updated_date": "2024-08-07 14:15:18 UTC"
  },
  {
    "arxiv_id": "2408.04671v1",
    "title": "Prompt and Prejudice",
    "authors": [
      "Lorenzo Berlincioni",
      "Luca Cultrera",
      "Federico Becattini",
      "Marco Bertini",
      "Alberto Del Bimbo"
    ],
    "abstract": "This paper investigates the impact of using first names in Large Language\nModels (LLMs) and Vision Language Models (VLMs), particularly when prompted\nwith ethical decision-making tasks. We propose an approach that appends first\nnames to ethically annotated text scenarios to reveal demographic biases in\nmodel outputs. Our study involves a curated list of more than 300 names\nrepresenting diverse genders and ethnic backgrounds, tested across thousands of\nmoral scenarios. Following the auditing methodologies from social sciences we\npropose a detailed analysis involving popular LLMs/VLMs to contribute to the\nfield of responsible AI by emphasizing the importance of recognizing and\nmitigating biases in these systems. Furthermore, we introduce a novel\nbenchmark, the Pratical Scenarios Benchmark (PSB), designed to assess the\npresence of biases involving gender or demographic prejudices in everyday\ndecision-making scenarios as well as practical scenarios where an LLM might be\nused to make sensible decisions (e.g., granting mortgages or insurances). This\nbenchmark allows for a comprehensive comparison of model behaviors across\ndifferent demographic categories, highlighting the risks and biases that may\narise in practical applications of LLMs and VLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ECCV workshop FAILED",
    "pdf_url": "http://arxiv.org/pdf/2408.04671v1",
    "published_date": "2024-08-07 14:11:33 UTC",
    "updated_date": "2024-08-07 14:11:33 UTC"
  },
  {
    "arxiv_id": "2408.07079v3",
    "title": "Anatomical Foundation Models for Brain MRIs",
    "authors": [
      "Carlo Alberto Barbano",
      "Matteo Brunello",
      "Benoit Dufumier",
      "Marco Grangetto"
    ],
    "abstract": "Deep Learning (DL) in neuroimaging has become increasingly relevant for\ndetecting neurological conditions and neurodegenerative disorders. One of the\nmost predominant biomarkers in neuroimaging is represented by brain age, which\nhas been shown to be a good indicator for different conditions, such as\nAlzheimer's Disease. Using brain age for weakly supervised pre-training of DL\nmodels in transfer learning settings has also recently shown promising results,\nespecially when dealing with data scarcity of different conditions. On the\nother hand, anatomical information of brain MRIs (e.g. cortical thickness) can\nprovide important information for learning good representations that can be\ntransferred to many downstream tasks. In this work, we propose AnatCL, an\nanatomical foundation model for brain MRIs that i.) leverages anatomical\ninformation in a weakly contrastive learning approach, and ii.) achieves\nstate-of-the-art performances across many different downstream tasks. To\nvalidate our approach we consider 12 different downstream tasks for the\ndiagnosis of different conditions such as Alzheimer's Disease, autism spectrum\ndisorder, and schizophrenia. Furthermore, we also target the prediction of 10\ndifferent clinical assessment scores using structural MRI data. Our findings\nshow that incorporating anatomical information during pre-training leads to\nmore robust and generalizable representations. Pre-trained models can be found\nat: https://github.com/EIDOSLAB/AnatCL.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "68T07",
      "I.2.6"
    ],
    "primary_category": "eess.IV",
    "comment": "Updated version; added ablation study",
    "pdf_url": "http://arxiv.org/pdf/2408.07079v3",
    "published_date": "2024-08-07 14:04:50 UTC",
    "updated_date": "2024-11-29 10:04:17 UTC"
  },
  {
    "arxiv_id": "2408.03772v1",
    "title": "Relevance meets Diversity: A User-Centric Framework for Knowledge Exploration through Recommendations",
    "authors": [
      "Erica Coppolillo",
      "Giuseppe Manco",
      "Aristides Gionis"
    ],
    "abstract": "Providing recommendations that are both relevant and diverse is a key\nconsideration of modern recommender systems. Optimizing both of these measures\npresents a fundamental trade-off, as higher diversity typically comes at the\ncost of relevance, resulting in lower user engagement. Existing recommendation\nalgorithms try to resolve this trade-off by combining the two measures,\nrelevance and diversity, into one aim and then seeking recommendations that\noptimize the combined objective, for a given number of items to recommend.\nTraditional approaches, however, do not consider the user interaction with the\nrecommended items.\n  In this paper, we put the user at the central stage, and build on the\ninterplay between relevance, diversity, and user behavior. In contrast to\napplications where the goal is solely to maximize engagement, we focus on\nscenarios aiming at maximizing the total amount of knowledge encountered by the\nuser. We use diversity as a surrogate of the amount of knowledge obtained by\nthe user while interacting with the system, and we seek to maximize diversity.\nWe propose a probabilistic user-behavior model in which users keep interacting\nwith the recommender system as long as they receive relevant recommendations,\nbut they may stop if the relevance of the recommended items drops. Thus, for a\nrecommender system to achieve a high-diversity measure, it will need to produce\nrecommendations that are both relevant and diverse.\n  Finally, we propose a novel recommendation strategy that combines relevance\nand diversity by a copula function. We conduct an extensive evaluation of the\nproposed methodology over multiple datasets, and we show that our strategy\noutperforms several state-of-the-art competitors. Our implementation is\npublicly available at https://github.com/EricaCoppolillo/EXPLORE.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03772v1",
    "published_date": "2024-08-07 13:48:24 UTC",
    "updated_date": "2024-08-07 13:48:24 UTC"
  },
  {
    "arxiv_id": "2408.03747v3",
    "title": "Online Model-based Anomaly Detection in Multivariate Time Series: Taxonomy, Survey, Research Challenges and Future Directions",
    "authors": [
      "Lucas Correia",
      "Jan-Christoph Goos",
      "Philipp Klein",
      "Thomas Bäck",
      "Anna V. Kononova"
    ],
    "abstract": "Time-series anomaly detection plays an important role in engineering\nprocesses, like development, manufacturing and other operations involving\ndynamic systems. These processes can greatly benefit from advances in the\nfield, as state-of-the-art approaches may aid in cases involving, for example,\nhighly dimensional data. To provide the reader with understanding of the\nterminology, this survey introduces a novel taxonomy where a distinction\nbetween online and offline, and training and inference is made. Additionally,\nit presents the most popular data sets and evaluation metrics used in the\nliterature, as well as a detailed analysis. Furthermore, this survey provides\nan extensive overview of the state-of-the-art model-based online semi- and\nunsupervised anomaly detection approaches for multivariate time-series data,\ncategorising them into different model families and other properties. The\nbiggest research challenge revolves around benchmarking, as currently there is\nno reliable way to compare different approaches against one another. This\nproblem is two-fold: on the one hand, public data sets suffers from at least\none fundamental flaw, while on the other hand, there is a lack of intuitive and\nrepresentative evaluation metrics in the field. Moreover, the way most\npublications choose a detection threshold disregards real-world conditions,\nwhich hinders the application in the real world. To allow for tangible advances\nin the field, these issues must be addressed in future work.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03747v3",
    "published_date": "2024-08-07 13:01:10 UTC",
    "updated_date": "2024-09-19 07:06:15 UTC"
  },
  {
    "arxiv_id": "2408.03746v1",
    "title": "Flexible Bayesian Last Layer Models Using Implicit Priors and Diffusion Posterior Sampling",
    "authors": [
      "Jian Xu",
      "Zhiqi Lin",
      "Shigui Li",
      "Min Chen",
      "Junmei Yang",
      "Delu Zeng",
      "John Paisley"
    ],
    "abstract": "Bayesian Last Layer (BLL) models focus solely on uncertainty in the output\nlayer of neural networks, demonstrating comparable performance to more complex\nBayesian models. However, the use of Gaussian priors for last layer weights in\nBayesian Last Layer (BLL) models limits their expressive capacity when faced\nwith non-Gaussian, outlier-rich, or high-dimensional datasets. To address this\nshortfall, we introduce a novel approach that combines diffusion techniques and\nimplicit priors for variational learning of Bayesian last layer weights. This\nmethod leverages implicit distributions for modeling weight priors in BLL,\ncoupled with diffusion samplers for approximating true posterior predictions,\nthereby establishing a comprehensive Bayesian prior and posterior estimation\nstrategy. By delivering an explicit and computationally efficient variational\nlower bound, our method aims to augment the expressive abilities of BLL models,\nenhancing model accuracy, calibration, and out-of-distribution detection\nproficiency. Through detailed exploration and experimental validation, We\nshowcase the method's potential for improving predictive accuracy and\nuncertainty quantification while ensuring computational efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03746v1",
    "published_date": "2024-08-07 12:59:58 UTC",
    "updated_date": "2024-08-07 12:59:58 UTC"
  },
  {
    "arxiv_id": "2408.03745v2",
    "title": "Intuitionistic Fuzzy Cognitive Maps for Interpretable Image Classification",
    "authors": [
      "Georgia Sovatzidi",
      "Michael D. Vasilakakis",
      "Dimitris K. Iakovidis"
    ],
    "abstract": "Several deep learning (DL) approaches have been proposed to deal with image\nclassification tasks. However, despite their effectiveness, they lack\ninterpretability, as they are unable to explain or justify their results. To\naddress the challenge of interpretable image classification, this paper\nintroduces a novel framework, named Interpretable Intuitionistic Fuzzy\nCognitive Maps (I2FCMs).Intuitionistic FCMs (iFCMs) have been proposed as an\nextension of FCMs offering a natural mechanism to assess the quality of their\noutput through the estimation of hesitancy, a concept resembling human\nhesitation in decision making. In the context of image classification,\nhesitancy is considered as a degree of unconfidence with which an image is\ncategorized to a class. To the best of our knowledge this is the first time\niFCMs are applied for image classification. Further novel contributions of the\nintroduced framework include the following: a) a feature extraction process\nfocusing on the most informative image regions; b) a learning algorithm for\nautomatic data-driven determination of the intuitionistic fuzzy\ninterconnections of the iFCM, thereby reducing human intervention in the\ndefinition of the graph structure; c) an inherently interpretable\nclassification approach based on image contents, providing understandable\nexplanations of its predictions, using linguistic terms. Furthermore, the\nproposed I2FCM framework can be applied to DL models, including Convolutional\nNeural Network (CNN), rendering them interpretable. The effectiveness of I2FCM\nis evaluated on publicly available datasets, and the results confirm that it\ncan provide enhanced classification performance, while providing interpretable\ninferences.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This work has been submitted for possible journal publication",
    "pdf_url": "http://arxiv.org/pdf/2408.03745v2",
    "published_date": "2024-08-07 12:58:39 UTC",
    "updated_date": "2025-04-04 16:28:33 UTC"
  },
  {
    "arxiv_id": "2408.03735v1",
    "title": "Advancing Multimodal Large Language Models with Quantization-Aware Scale Learning for Efficient Adaptation",
    "authors": [
      "Jingjing Xie",
      "Yuxin Zhang",
      "Mingbao Lin",
      "Liujuan Cao",
      "Rongrong Ji"
    ],
    "abstract": "This paper presents the first study to explore the potential of parameter\nquantization for multimodal large language models to alleviate the significant\nresource constraint encountered during vision-language instruction tuning. We\nintroduce a Quantization-aware Scale LeArning method based on multimodal\nWarmup, termed QSLAW. This method is grounded in two key innovations: (1) The\nlearning of group-wise scale factors for quantized LLM weights to mitigate the\nquantization error arising from activation outliers and achieve more effective\nvision-language instruction tuning; (2) The implementation of a multimodal\nwarmup that progressively integrates linguistic and multimodal training\nsamples, thereby preventing overfitting of the quantized model to multimodal\ndata while ensuring stable adaptation of multimodal large language models to\ndownstream vision-language tasks. Extensive experiments demonstrate that models\nquantized by QSLAW perform on par with, or even surpass, their full-precision\ncounterparts, while facilitating up to 1.4 times reduction in VL tuning time\nand GPU consumption. Our code is released at https://github.com/xjjxmu/QSLAW.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ACMMM2024",
    "pdf_url": "http://arxiv.org/pdf/2408.03735v1",
    "published_date": "2024-08-07 12:42:09 UTC",
    "updated_date": "2024-08-07 12:42:09 UTC"
  },
  {
    "arxiv_id": "2408.11841v2",
    "title": "Could ChatGPT get an Engineering Degree? Evaluating Higher Education Vulnerability to AI Assistants",
    "authors": [
      "Beatriz Borges",
      "Negar Foroutan",
      "Deniz Bayazit",
      "Anna Sotnikova",
      "Syrielle Montariol",
      "Tanya Nazaretzky",
      "Mohammadreza Banaei",
      "Alireza Sakhaeirad",
      "Philippe Servant",
      "Seyed Parsa Neshaei",
      "Jibril Frej",
      "Angelika Romanou",
      "Gail Weiss",
      "Sepideh Mamooler",
      "Zeming Chen",
      "Simin Fan",
      "Silin Gao",
      "Mete Ismayilzada",
      "Debjit Paul",
      "Alexandre Schöpfer",
      "Andrej Janchevski",
      "Anja Tiede",
      "Clarence Linden",
      "Emanuele Troiani",
      "Francesco Salvi",
      "Freya Behrens",
      "Giacomo Orsi",
      "Giovanni Piccioli",
      "Hadrien Sevel",
      "Louis Coulon",
      "Manuela Pineros-Rodriguez",
      "Marin Bonnassies",
      "Pierre Hellich",
      "Puck van Gerwen",
      "Sankalp Gambhir",
      "Solal Pirelli",
      "Thomas Blanchard",
      "Timothée Callens",
      "Toni Abi Aoun",
      "Yannick Calvino Alonso",
      "Yuri Cho",
      "Alberto Chiappa",
      "Antonio Sclocchi",
      "Étienne Bruno",
      "Florian Hofhammer",
      "Gabriel Pescia",
      "Geovani Rizk",
      "Leello Dadi",
      "Lucas Stoffl",
      "Manoel Horta Ribeiro",
      "Matthieu Bovel",
      "Yueyang Pan",
      "Aleksandra Radenovic",
      "Alexandre Alahi",
      "Alexander Mathis",
      "Anne-Florence Bitbol",
      "Boi Faltings",
      "Cécile Hébert",
      "Devis Tuia",
      "François Maréchal",
      "George Candea",
      "Giuseppe Carleo",
      "Jean-Cédric Chappelier",
      "Nicolas Flammarion",
      "Jean-Marie Fürbringer",
      "Jean-Philippe Pellet",
      "Karl Aberer",
      "Lenka Zdeborová",
      "Marcel Salathé",
      "Martin Jaggi",
      "Martin Rajman",
      "Mathias Payer",
      "Matthieu Wyart",
      "Michael Gastpar",
      "Michele Ceriotti",
      "Ola Svensson",
      "Olivier Lévêque",
      "Paolo Ienne",
      "Rachid Guerraoui",
      "Robert West",
      "Sanidhya Kashyap",
      "Valerio Piazza",
      "Viesturs Simanis",
      "Viktor Kuncak",
      "Volkan Cevher",
      "Philippe Schwaller",
      "Sacha Friedli",
      "Patrick Jermann",
      "Tanja Käser",
      "Antoine Bosselut"
    ],
    "abstract": "AI assistants are being increasingly used by students enrolled in higher\neducation institutions. While these tools provide opportunities for improved\nteaching and education, they also pose significant challenges for assessment\nand learning outcomes. We conceptualize these challenges through the lens of\nvulnerability, the potential for university assessments and learning outcomes\nto be impacted by student use of generative AI. We investigate the potential\nscale of this vulnerability by measuring the degree to which AI assistants can\ncomplete assessment questions in standard university-level STEM courses.\nSpecifically, we compile a novel dataset of textual assessment questions from\n50 courses at EPFL and evaluate whether two AI assistants, GPT-3.5 and GPT-4\ncan adequately answer these questions. We use eight prompting strategies to\nproduce responses and find that GPT-4 answers an average of 65.8% of questions\ncorrectly, and can even produce the correct answer across at least one\nprompting strategy for 85.1% of questions. When grouping courses in our dataset\nby degree program, these systems already pass non-project assessments of large\nnumbers of core courses in various degree programs, posing risks to higher\neducation accreditation that will be amplified as these models improve. Our\nresults call for revising program-level assessment design in higher education\nin light of advances in generative AI.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "20 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.11841v2",
    "published_date": "2024-08-07 12:11:49 UTC",
    "updated_date": "2024-11-27 10:59:10 UTC"
  },
  {
    "arxiv_id": "2408.03706v1",
    "title": "Local Topology Measures of Contextual Language Model Latent Spaces With Applications to Dialogue Term Extraction",
    "authors": [
      "Benjamin Matthias Ruppik",
      "Michael Heck",
      "Carel van Niekerk",
      "Renato Vukovic",
      "Hsien-chin Lin",
      "Shutong Feng",
      "Marcus Zibrowius",
      "Milica Gašić"
    ],
    "abstract": "A common approach for sequence tagging tasks based on contextual word\nrepresentations is to train a machine learning classifier directly on these\nembedding vectors. This approach has two shortcomings. First, such methods\nconsider single input sequences in isolation and are unable to put an\nindividual embedding vector in relation to vectors outside the current local\ncontext of use. Second, the high performance of these models relies on\nfine-tuning the embedding model in conjunction with the classifier, which may\nnot always be feasible due to the size or inaccessibility of the underlying\nfeature-generation model. It is thus desirable, given a collection of embedding\nvectors of a corpus, i.e., a datastore, to find features of each vector that\ndescribe its relation to other, similar vectors in the datastore. With this in\nmind, we introduce complexity measures of the local topology of the latent\nspace of a contextual language model with respect to a given datastore. The\neffectiveness of our features is demonstrated through their application to\ndialogue term extraction. Our work continues a line of research that explores\nthe manifold hypothesis for word embeddings, demonstrating that local structure\nin the space carved out by word embeddings can be exploited to infer semantic\nproperties.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as a long paper to SIGDIAL 2024. 9 pages, 2 figures, 3\n  tables",
    "pdf_url": "http://arxiv.org/pdf/2408.03706v1",
    "published_date": "2024-08-07 11:44:32 UTC",
    "updated_date": "2024-08-07 11:44:32 UTC"
  },
  {
    "arxiv_id": "2408.03694v1",
    "title": "A Blockchain-based Reliable Federated Meta-learning for Metaverse: A Dual Game Framework",
    "authors": [
      "Emna Baccour",
      "Aiman Erbad",
      "Amr Mohamed",
      "Mounir Hamdi",
      "Mohsen Guizani"
    ],
    "abstract": "The metaverse, envisioned as the next digital frontier for avatar-based\nvirtual interaction, involves high-performance models. In this dynamic\nenvironment, users' tasks frequently shift, requiring fast model\npersonalization despite limited data. This evolution consumes extensive\nresources and requires vast data volumes. To address this, meta-learning\nemerges as an invaluable tool for metaverse users, with federated meta-learning\n(FML), offering even more tailored solutions owing to its adaptive\ncapabilities. However, the metaverse is characterized by users heterogeneity\nwith diverse data structures, varied tasks, and uneven sample sizes,\npotentially undermining global training outcomes due to statistical difference.\nGiven this, an urgent need arises for smart coalition formation that accounts\nfor these disparities. This paper introduces a dual game-theoretic framework\nfor metaverse services involving meta-learners as workers to manage FML. A\nblockchain-based cooperative coalition formation game is crafted, grounded on a\nreputation metric, user similarity, and incentives. We also introduce a novel\nreputation system based on users' historical contributions and potential\ncontributions to present tasks, leveraging correlations between past and new\ntasks. Finally, a Stackelberg game-based incentive mechanism is presented to\nattract reliable workers to participate in meta-learning, minimizing users'\nenergy costs, increasing payoffs, boosting FML efficacy, and improving\nmetaverse utility. Results show that our dual game framework outperforms\nbest-effort, random, and non-uniform clustering schemes - improving training\nperformance by up to 10%, cutting completion times by as much as 30%, enhancing\nmetaverse utility by more than 25%, and offering up to 5% boost in training\nefficiency over non-blockchain systems, effectively countering misbehaving\nusers.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "Accepted in IEEE Internet of Things Journal",
    "pdf_url": "http://arxiv.org/pdf/2408.03694v1",
    "published_date": "2024-08-07 11:14:18 UTC",
    "updated_date": "2024-08-07 11:14:18 UTC"
  },
  {
    "arxiv_id": "2408.03691v1",
    "title": "Generative Design of Periodic Orbits in the Restricted Three-Body Problem",
    "authors": [
      "Alvaro Francisco Gil",
      "Walther Litteri",
      "Victor Rodriguez-Fernandez",
      "David Camacho",
      "Massimiliano Vasile"
    ],
    "abstract": "The Three-Body Problem has fascinated scientists for centuries and it has\nbeen crucial in the design of modern space missions. Recent developments in\nGenerative Artificial Intelligence hold transformative promise for addressing\nthis longstanding problem. This work investigates the use of Variational\nAutoencoder (VAE) and its internal representation to generate periodic orbits.\nWe utilize a comprehensive dataset of periodic orbits in the Circular\nRestricted Three-Body Problem (CR3BP) to train deep-learning architectures that\ncapture key orbital characteristics, and we set up physical evaluation metrics\nfor the generated trajectories. Through this investigation, we seek to enhance\nthe understanding of how Generative AI can improve space mission planning and\nastrodynamics research, leading to novel, data-driven approaches in the field.",
    "categories": [
      "cs.LG",
      "astro-ph.EP",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "SPAICE Conference 2024 (7 pages)",
    "pdf_url": "http://arxiv.org/pdf/2408.03691v1",
    "published_date": "2024-08-07 11:13:19 UTC",
    "updated_date": "2024-08-07 11:13:19 UTC"
  },
  {
    "arxiv_id": "2408.03648v1",
    "title": "HiQuE: Hierarchical Question Embedding Network for Multimodal Depression Detection",
    "authors": [
      "Juho Jung",
      "Chaewon Kang",
      "Jeewoo Yoon",
      "Seungbae Kim",
      "Jinyoung Han"
    ],
    "abstract": "The utilization of automated depression detection significantly enhances\nearly intervention for individuals experiencing depression. Despite numerous\nproposals on automated depression detection using recorded clinical interview\nvideos, limited attention has been paid to considering the hierarchical\nstructure of the interview questions. In clinical interviews for diagnosing\ndepression, clinicians use a structured questionnaire that includes routine\nbaseline questions and follow-up questions to assess the interviewee's\ncondition. This paper introduces HiQuE (Hierarchical Question Embedding\nnetwork), a novel depression detection framework that leverages the\nhierarchical relationship between primary and follow-up questions in clinical\ninterviews. HiQuE can effectively capture the importance of each question in\ndiagnosing depression by learning mutual information across multiple\nmodalities. We conduct extensive experiments on the widely-used clinical\ninterview data, DAIC-WOZ, where our model outperforms other state-of-the-art\nmultimodal depression detection models and emotion recognition models,\nshowcasing its clinical utility in depression detection.",
    "categories": [
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 6 figures, Proceedings of the 33rd ACM International\n  Conference on Information and Knowledge Management (CIKM '24)",
    "pdf_url": "http://arxiv.org/pdf/2408.03648v1",
    "published_date": "2024-08-07 09:23:01 UTC",
    "updated_date": "2024-08-07 09:23:01 UTC"
  },
  {
    "arxiv_id": "2408.12602v1",
    "title": "Fiber neural networks for the intelligent optical fiber communications",
    "authors": [
      "Yubin Zang",
      "Zuxing Zhang",
      "Simin Li",
      "Fangzheng Zhang",
      "Hongwei Chen"
    ],
    "abstract": "Optical neural networks have long cast attention nowadays. Like other optical\nstructured neural networks, fiber neural networks which utilize the mechanism\nof light transmission to compute can take great advantages in both computing\nefficiency and power cost. Though the potential ability of optical fiber was\ndemonstrated via the establishing of fiber neural networks, it will be of great\nsignificance of combining both fiber transmission and computing functions so as\nto cater the needs of future beyond 5G intelligent communication signal\nprocessing. Thus, in this letter, the fiber neural networks and their related\noptical signal processing methods will be both developed. In this way,\ninformation derived from the transmitted signals can be directly processed in\nthe optical domain rather than being converted to the electronic domain. As a\nresult, both prominent gains in processing efficiency and power cost can be\nfurther obtained. The fidelity of the whole structure and related methods is\ndemonstrated by the task of modulation format recognition which plays important\nrole in fiber optical communications without losing the generality.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "eess.SP",
    "comment": "5 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.12602v1",
    "published_date": "2024-08-07 08:46:48 UTC",
    "updated_date": "2024-08-07 08:46:48 UTC"
  },
  {
    "arxiv_id": "2408.03632v3",
    "title": "Concept Conductor: Orchestrating Multiple Personalized Concepts in Text-to-Image Synthesis",
    "authors": [
      "Zebin Yao",
      "Fangxiang Feng",
      "Ruifan Li",
      "Xiaojie Wang"
    ],
    "abstract": "The customization of text-to-image models has seen significant advancements,\nyet generating multiple personalized concepts remains a challenging task.\nCurrent methods struggle with attribute leakage and layout confusion when\nhandling multiple concepts, leading to reduced concept fidelity and semantic\nconsistency. In this work, we introduce a novel training-free framework,\nConcept Conductor, designed to ensure visual fidelity and correct layout in\nmulti-concept customization. Concept Conductor isolates the sampling processes\nof multiple custom models to prevent attribute leakage between different\nconcepts and corrects erroneous layouts through self-attention-based spatial\nguidance. Additionally, we present a concept injection technique that employs\nshape-aware masks to specify the generation area for each concept. This\ntechnique injects the structure and appearance of personalized concepts through\nfeature fusion in the attention layers, ensuring harmony in the final image.\nExtensive qualitative and quantitative experiments demonstrate that Concept\nConductor can consistently generate composite images with accurate layouts\nwhile preserving the visual details of each concept. Compared to existing\nbaselines, Concept Conductor shows significant performance improvements. Our\nmethod supports the combination of any number of concepts and maintains high\nfidelity even when dealing with visually similar concepts. The code and models\nare available at https://github.com/Nihukat/Concept-Conductor.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Github Page: https://github.com/Nihukat/Concept-Conductor",
    "pdf_url": "http://arxiv.org/pdf/2408.03632v3",
    "published_date": "2024-08-07 08:43:58 UTC",
    "updated_date": "2024-09-09 12:26:04 UTC"
  },
  {
    "arxiv_id": "2408.03631v2",
    "title": "Large Language Model as a Catalyst: A Paradigm Shift in Base Station Siting Optimization",
    "authors": [
      "Yanhu Wang",
      "Muhammad Muzammil Afzal",
      "Zhengyang Li",
      "Jie Zhou",
      "Chenyuan Feng",
      "Shuaishuai Guo",
      "Tony Q. S. Quek"
    ],
    "abstract": "Traditional base station siting (BSS) methods rely heavily on drive testing\nand user feedback, which are laborious and require extensive expertise in\ncommunication, networking, and optimization. As large language models (LLMs)\nand their associated technologies advance, particularly in the realms of prompt\nengineering and agent engineering, network optimization will witness a\nrevolutionary approach. This approach entails the strategic use of well-crafted\nprompts to infuse human experience and knowledge into these sophisticated LLMs,\nand the deployment of autonomous agents as a communication bridge to seamlessly\nconnect the machine language based LLMs with human users using natural\nlanguage. Furthermore, our proposed framework incorporates retrieval-augmented\ngeneration (RAG) to enhance the system's ability to acquire domain-specific\nknowledge and generate solutions, thereby enabling the customization and\noptimization of the BSS process. This integration represents the future\nparadigm of artificial intelligence (AI) as a service and AI for more ease.\nThis research first develops a novel LLM-empowered BSS optimization framework,\nand heuristically proposes three different potential implementations: the\nstrategies based on Prompt-optimized LLM (PoL), LLM-empowered autonomous BSS\nagent (LaBa), and Cooperative multiple LLM-based autonomous BSS agents (CLaBa).\nThrough evaluation on real-world data, the experiments demonstrate that\nprompt-assisted LLMs and LLM-based agents can generate more efficient and\nreliable network deployments, noticeably enhancing the efficiency of BSS\noptimization and reducing trivial manual participation.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03631v2",
    "published_date": "2024-08-07 08:43:32 UTC",
    "updated_date": "2024-12-26 02:14:28 UTC"
  },
  {
    "arxiv_id": "2408.03622v1",
    "title": "Improving the quality of Persian clinical text with a novel spelling correction system",
    "authors": [
      "Seyed Mohammad Sadegh Dashti",
      "Seyedeh Fatemeh Dashti"
    ],
    "abstract": "Background: The accuracy of spelling in Electronic Health Records (EHRs) is a\ncritical factor for efficient clinical care, research, and ensuring patient\nsafety. The Persian language, with its abundant vocabulary and complex\ncharacteristics, poses unique challenges for real-word error correction. This\nresearch aimed to develop an innovative approach for detecting and correcting\nspelling errors in Persian clinical text.\n  Methods: Our strategy employs a state-of-the-art pre-trained model that has\nbeen meticulously fine-tuned specifically for the task of spelling correction\nin the Persian clinical domain. This model is complemented by an innovative\northographic similarity matching algorithm, PERTO, which uses visual similarity\nof characters for ranking correction candidates.\n  Results: The evaluation of our approach demonstrated its robustness and\nprecision in detecting and rectifying word errors in Persian clinical text. In\nterms of non-word error correction, our model achieved an F1-Score of 90.0%\nwhen the PERTO algorithm was employed. For real-word error detection, our model\ndemonstrated its highest performance, achieving an F1-Score of 90.6%.\nFurthermore, the model reached its highest F1-Score of 91.5% for real-word\nerror correction when the PERTO algorithm was employed.\n  Conclusions: Despite certain limitations, our method represents a substantial\nadvancement in the field of spelling error detection and correction for Persian\nclinical text. By effectively addressing the unique challenges posed by the\nPersian language, our approach paves the way for more accurate and efficient\nclinical documentation, contributing to improved patient care and safety.\nFuture research could explore its use in other areas of the Persian medical\ndomain, enhancing its impact and utility.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03622v1",
    "published_date": "2024-08-07 08:31:42 UTC",
    "updated_date": "2024-08-07 08:31:42 UTC"
  },
  {
    "arxiv_id": "2408.03618v4",
    "title": "A Logical Fallacy-Informed Framework for Argument Generation",
    "authors": [
      "Luca Mouchel",
      "Debjit Paul",
      "Shaobo Cui",
      "Robert West",
      "Antoine Bosselut",
      "Boi Faltings"
    ],
    "abstract": "Despite the remarkable performance of Large Language Models (LLMs) in natural\nlanguage processing tasks, they still struggle with generating logically sound\narguments, resulting in potential risks such as spreading misinformation. To\naddress this issue, we introduce FIPO, a fallacy-informed framework that\nleverages preference optimization methods to steer LLMs toward logically sound\narguments. FIPO includes a classification loss, to capture the fine-grained\ninformation on fallacy types. Our results on argumentation datasets show that\nour method reduces the fallacy errors by up to 17.5%. Furthermore, our human\nevaluation results indicate that the quality of the generated arguments by our\nmethod significantly outperforms the fine-tuned baselines, as well as other\npreference optimization methods, such as DPO. These findings highlight the\nimportance of ensuring models are aware of logical fallacies for effective\nargument generation. Our code is available at\ngithub.com/lucamouchel/Logical-Fallacies.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03618v4",
    "published_date": "2024-08-07 08:19:44 UTC",
    "updated_date": "2025-05-03 18:52:04 UTC"
  },
  {
    "arxiv_id": "2408.03617v2",
    "title": "Is Child-Directed Speech Effective Training Data for Language Models?",
    "authors": [
      "Steven Y. Feng",
      "Noah D. Goodman",
      "Michael C. Frank"
    ],
    "abstract": "While high-performing language models are typically trained on hundreds of\nbillions of words, human children become fluent language users with a much\nsmaller amount of data. What are the features of the data they receive, and how\ndo these features support language modeling objectives? To investigate this\nquestion, we train GPT-2 and RoBERTa models on 29M words of English\nchild-directed speech and a new matched, synthetic dataset (TinyDialogues),\ncomparing to OpenSubtitles, Wikipedia, and a heterogeneous blend of datasets\nfrom the BabyLM challenge. We evaluate the syntactic and semantic knowledge of\nthese models using developmentally-inspired evaluations. Through pretraining\nexperiments, we test whether the global developmental ordering or the local\ndiscourse ordering of children's training data supports high performance\nrelative to other datasets. The local properties of the data affect model\nresults, but surprisingly, global properties do not. Further, child language\ninput is not uniquely valuable for training language models. These findings\nsupport the hypothesis that, rather than proceeding from better data, the\nchild's learning algorithm is substantially more data-efficient than current\nlanguage modeling techniques.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024. Code and data at https://github.com/styfeng/TinyDialogues",
    "pdf_url": "http://arxiv.org/pdf/2408.03617v2",
    "published_date": "2024-08-07 08:18:51 UTC",
    "updated_date": "2024-10-08 20:27:10 UTC"
  },
  {
    "arxiv_id": "2408.03615v2",
    "title": "Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks",
    "authors": [
      "Zaijing Li",
      "Yuquan Xie",
      "Rui Shao",
      "Gongwei Chen",
      "Dongmei Jiang",
      "Liqiang Nie"
    ],
    "abstract": "Building a general-purpose agent is a long-standing vision in the field of\nartificial intelligence. Existing agents have made remarkable progress in many\ndomains, yet they still struggle to complete long-horizon tasks in an open\nworld. We attribute this to the lack of necessary world knowledge and\nmultimodal experience that can guide agents through a variety of long-horizon\ntasks. In this paper, we propose a Hybrid Multimodal Memory module to address\nthe above challenges. It 1) transforms knowledge into Hierarchical Directed\nKnowledge Graph that allows agents to explicitly represent and learn world\nknowledge, and 2) summarises historical information into Abstracted Multimodal\nExperience Pool that provide agents with rich references for in-context\nlearning. On top of the Hybrid Multimodal Memory module, a multimodal agent,\nOptimus-1, is constructed with dedicated Knowledge-guided Planner and\nExperience-Driven Reflector, contributing to a better planning and reflection\nin the face of long-horizon tasks in Minecraft. Extensive experimental results\nshow that Optimus-1 significantly outperforms all existing agents on\nchallenging long-horizon task benchmarks, and exhibits near human-level\nperformance on many tasks. In addition, we introduce various Multimodal Large\nLanguage Models (MLLMs) as the backbone of Optimus-1. Experimental results show\nthat Optimus-1 exhibits strong generalization with the help of the Hybrid\nMultimodal Memory module, outperforming the GPT-4V baseline on many tasks.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.03615v2",
    "published_date": "2024-08-07 08:16:32 UTC",
    "updated_date": "2024-10-21 11:05:27 UTC"
  },
  {
    "arxiv_id": "2408.03603v1",
    "title": "EnJa: Ensemble Jailbreak on Large Language Models",
    "authors": [
      "Jiahao Zhang",
      "Zilong Wang",
      "Ruofan Wang",
      "Xingjun Ma",
      "Yu-Gang Jiang"
    ],
    "abstract": "As Large Language Models (LLMs) are increasingly being deployed in\nsafety-critical applications, their vulnerability to potential jailbreaks --\nmalicious prompts that can disable the safety mechanism of LLMs -- has\nattracted growing research attention. While alignment methods have been\nproposed to protect LLMs from jailbreaks, many have found that aligned LLMs can\nstill be jailbroken by carefully crafted malicious prompts, producing content\nthat violates policy regulations. Existing jailbreak attacks on LLMs can be\ncategorized into prompt-level methods which make up stories/logic to circumvent\nsafety alignment and token-level attack methods which leverage gradient methods\nto find adversarial tokens. In this work, we introduce the concept of Ensemble\nJailbreak and explore methods that can integrate prompt-level and token-level\njailbreak into a more powerful hybrid jailbreak attack. Specifically, we\npropose a novel EnJa attack to hide harmful instructions using prompt-level\njailbreak, boost the attack success rate using a gradient-based attack, and\nconnect the two types of jailbreak attacks via a template-based connector. We\nevaluate the effectiveness of EnJa on several aligned models and show that it\nachieves a state-of-the-art attack success rate with fewer queries and is much\nstronger than any individual jailbreak.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03603v1",
    "published_date": "2024-08-07 07:46:08 UTC",
    "updated_date": "2024-08-07 07:46:08 UTC"
  },
  {
    "arxiv_id": "2408.03599v2",
    "title": "Activations Through Extensions: A Framework To Boost Performance Of Neural Networks",
    "authors": [
      "Chandramouli Kamanchi",
      "Sumanta Mukherjee",
      "Kameshwaran Sampath",
      "Pankaj Dayama",
      "Arindam Jati",
      "Vijay Ekambaram",
      "Dzung Phan"
    ],
    "abstract": "Activation functions are non-linearities in neural networks that allow them\nto learn complex mapping between inputs and outputs. Typical choices for\nactivation functions are ReLU, Tanh, Sigmoid etc., where the choice generally\ndepends on the application domain. In this work, we propose a\nframework/strategy that unifies several works on activation functions and\ntheoretically explains the performance benefits of these works. We also propose\nnovel techniques that originate from the framework and allow us to obtain\n``extensions'' (i.e. special generalizations of a given neural network) of\nneural networks through operations on activation functions. We theoretically\nand empirically show that ``extensions'' of neural networks have performance\nbenefits compared to vanilla neural networks with insignificant space and time\ncomplexity costs on standard test functions. We also show the benefits of\nneural network ``extensions'' in the time-series domain on real-world datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "cs.NE",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03599v2",
    "published_date": "2024-08-07 07:36:49 UTC",
    "updated_date": "2024-08-16 01:19:04 UTC"
  },
  {
    "arxiv_id": "2408.03591v1",
    "title": "Focal Depth Estimation: A Calibration-Free, Subject- and Daytime Invariant Approach",
    "authors": [
      "Benedikt W. Hosp",
      "Björn Severitt",
      "Rajat Agarwala",
      "Evgenia Rusak",
      "Yannick Sauer",
      "Siegfried Wahl"
    ],
    "abstract": "In an era where personalized technology is increasingly intertwined with\ndaily life, traditional eye-tracking systems and autofocal glasses face a\nsignificant challenge: the need for frequent, user-specific calibration, which\nimpedes their practicality. This study introduces a groundbreaking\ncalibration-free method for estimating focal depth, leveraging machine learning\ntechniques to analyze eye movement features within short sequences. Our\napproach, distinguished by its innovative use of LSTM networks and\ndomain-specific feature engineering, achieves a mean absolute error (MAE) of\nless than 10 cm, setting a new focal depth estimation accuracy standard. This\nadvancement promises to enhance the usability of autofocal glasses and pave the\nway for their seamless integration into extended reality environments, marking\na significant leap forward in personalized visual technology.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03591v1",
    "published_date": "2024-08-07 07:09:14 UTC",
    "updated_date": "2024-08-07 07:09:14 UTC"
  },
  {
    "arxiv_id": "2408.03972v1",
    "title": "Enhancing Output Diversity Improves Conjugate Gradient-based Adversarial Attacks",
    "authors": [
      "Keiichiro Yamamura",
      "Issa Oe",
      "Hiroki Ishikura",
      "Katsuki Fujisawa"
    ],
    "abstract": "Deep neural networks are vulnerable to adversarial examples, and adversarial\nattacks that generate adversarial examples have been studied in this context.\nExisting studies imply that increasing the diversity of model outputs\ncontributes to improving the attack performance. This study focuses on the Auto\nConjugate Gradient (ACG) attack, which is inspired by the conjugate gradient\nmethod and has a high diversification performance. We hypothesized that\nincreasing the distance between two consecutive search points would enhance the\noutput diversity. To test our hypothesis, we propose Rescaling-ACG (ReACG),\nwhich automatically modifies the two components that significantly affect the\ndistance between two consecutive search points, including the search direction\nand step size. ReACG showed higher attack performance than that of ACG, and is\nparticularly effective for ImageNet models with several classification classes.\nExperimental results show that the distance between two consecutive search\npoints enhances the output diversity and may help develop new potent attacks.\nThe code is available at \\url{https://github.com/yamamura-k/ReACG}",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICPRAI2024",
    "pdf_url": "http://arxiv.org/pdf/2408.03972v1",
    "published_date": "2024-08-07 07:07:35 UTC",
    "updated_date": "2024-08-07 07:07:35 UTC"
  },
  {
    "arxiv_id": "2408.03588v2",
    "title": "Facing the Music: Tackling Singing Voice Separation in Cinematic Audio Source Separation",
    "authors": [
      "Karn N. Watcharasupat",
      "Chih-Wei Wu",
      "Iroro Orife"
    ],
    "abstract": "Cinematic audio source separation (CASS), as a standalone problem of\nextracting individual stems from their mixture, is a fairly new subtask of\naudio source separation. A typical setup of CASS is a three-stem problem, with\nthe aim of separating the mixture into the dialogue (DX), music (MX), and\neffects (FX) stems. Given the creative nature of cinematic sound production,\nhowever, several edge cases exist; some sound sources do not fit neatly in any\nof these three stems, necessitating the use of additional auxiliary stems in\nproduction. One very common edge case is the singing voice in film audio, which\nmay belong in either the DX or MX or neither, depending heavily on the\ncinematic context. In this work, we demonstrate a very straightforward\nextension of the dedicated-decoder Bandit and query-based single-decoder\nBanquet models to a four-stem problem, treating non-musical dialogue,\ninstrumental music, singing voice, and effects as separate stems.\nInterestingly, the query-based Banquet model outperformed the dedicated-decoder\nBandit model. We hypothesized that this is due to a better feature alignment at\nthe bottleneck as enforced by the band-agnostic FiLM layer. Dataset and model\nimplementation will be made available at\nhttps://github.com/kwatcharasupat/source-separation-landing.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Submitted to the Late-Breaking Demo Session of the 25th International\n  Society for Music Information Retrieval (ISMIR) Conference, 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.03588v2",
    "published_date": "2024-08-07 07:04:29 UTC",
    "updated_date": "2024-08-26 00:52:40 UTC"
  },
  {
    "arxiv_id": "2408.03585v1",
    "title": "Hierarchical Neural Constructive Solver for Real-world TSP Scenarios",
    "authors": [
      "Yong Liang Goh",
      "Zhiguang Cao",
      "Yining Ma",
      "Yanfei Dong",
      "Mohammed Haroon Dupty",
      "Wee Sun Lee"
    ],
    "abstract": "Existing neural constructive solvers for routing problems have predominantly\nemployed transformer architectures, conceptualizing the route construction as a\nset-to-sequence learning task. However, their efficacy has primarily been\ndemonstrated on entirely random problem instances that inadequately capture\nreal-world scenarios. In this paper, we introduce realistic Traveling Salesman\nProblem (TSP) scenarios relevant to industrial settings and derive the\nfollowing insights: (1) The optimal next node (or city) to visit often lies\nwithin proximity to the current node, suggesting the potential benefits of\nbiasing choices based on current locations. (2) Effectively solving the TSP\nrequires robust tracking of unvisited nodes and warrants succinct grouping\nstrategies. Building upon these insights, we propose integrating a learnable\nchoice layer inspired by Hypernetworks to prioritize choices based on the\ncurrent location, and a learnable approximate clustering algorithm inspired by\nthe Expectation-Maximization algorithm to facilitate grouping the unvisited\ncities. Together, these two contributions form a hierarchical approach towards\nsolving the realistic TSP by considering both immediate local neighbourhoods\nand learning an intermediate set of node representations. Our hierarchical\napproach yields superior performance compared to both classical and recent\ntransformer models, showcasing the efficacy of the key designs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to KDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.03585v1",
    "published_date": "2024-08-07 06:44:47 UTC",
    "updated_date": "2024-08-07 06:44:47 UTC"
  },
  {
    "arxiv_id": "2408.03573v1",
    "title": "Active Testing of Large Language Model via Multi-Stage Sampling",
    "authors": [
      "Yuheng Huang",
      "Jiayang Song",
      "Qiang Hu",
      "Felix Juefei-Xu",
      "Lei Ma"
    ],
    "abstract": "Performance evaluation plays a crucial role in the development life cycle of\nlarge language models (LLMs). It estimates the model's capability, elucidates\nbehavior characteristics, and facilitates the identification of potential\nissues and limitations, thereby guiding further improvement. Given that LLMs'\ndiverse task-handling abilities stem from large volumes of training data, a\ncomprehensive evaluation also necessitates abundant, well-annotated, and\nrepresentative test data to assess LLM performance across various downstream\ntasks. However, the demand for high-quality test data often entails substantial\ntime, computational resources, and manual efforts, sometimes causing the\nevaluation to be inefficient or impractical. To address these challenges,\nresearchers propose active testing, which estimates the overall performance by\nselecting a subset of test data. Nevertheless, the existing active testing\nmethods tend to be inefficient, even inapplicable, given the unique new\nchallenges of LLMs (e.g., diverse task types, increased model complexity, and\nunavailability of training data). To mitigate such limitations and expedite the\ndevelopment cycle of LLMs, in this work, we introduce AcTracer, an active\ntesting framework tailored for LLMs that strategically selects a small subset\nof test data to achieve a nearly optimal performance estimation for LLMs.\nAcTracer utilizes both internal and external information from LLMs to guide the\ntest sampling process, reducing variance through a multi-stage pool-based\nactive selection. Our experiment results demonstrate that AcTracer achieves\nstate-of-the-art performance compared to existing methods across various tasks,\nwith up to 38.83% improvement over previous SOTA.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "D.2.5; I.2.7"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03573v1",
    "published_date": "2024-08-07 06:17:48 UTC",
    "updated_date": "2024-08-07 06:17:48 UTC"
  },
  {
    "arxiv_id": "2408.03572v2",
    "title": "2D-OOB: Attributing Data Contribution Through Joint Valuation Framework",
    "authors": [
      "Yifan Sun",
      "Jingyan Shen",
      "Yongchan Kwon"
    ],
    "abstract": "Data valuation has emerged as a powerful framework for quantifying each\ndatum's contribution to the training of a machine learning model. However, it\nis crucial to recognize that the quality of cells within a single data point\ncan vary greatly in practice. For example, even in the case of an abnormal data\npoint, not all cells are necessarily noisy. The single scalar score assigned by\nexisting data valuation methods blurs the distinction between noisy and clean\ncells of a data point, making it challenging to interpret the data values. In\nthis paper, we propose 2D-OOB, an out-of-bag estimation framework for jointly\ndetermining helpful (or detrimental) samples as well as the particular cells\nthat drive them. Our comprehensive experiments demonstrate that 2D-OOB achieves\nstate-of-the-art performance across multiple use cases while being\nexponentially faster. Specifically, 2D-OOB shows promising results in detecting\nand rectifying fine-grained outliers at the cell level, and localizing backdoor\ntriggers in data poisoning attacks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03572v2",
    "published_date": "2024-08-07 06:16:17 UTC",
    "updated_date": "2024-10-30 04:10:12 UTC"
  },
  {
    "arxiv_id": "2408.03562v1",
    "title": "A Comparison of LLM Finetuning Methods & Evaluation Metrics with Travel Chatbot Use Case",
    "authors": [
      "Sonia Meyer",
      "Shreya Singh",
      "Bertha Tam",
      "Christopher Ton",
      "Angel Ren"
    ],
    "abstract": "This research compares large language model (LLM) fine-tuning methods,\nincluding Quantized Low Rank Adapter (QLoRA), Retrieval Augmented fine-tuning\n(RAFT), and Reinforcement Learning from Human Feedback (RLHF), and additionally\ncompared LLM evaluation methods including End to End (E2E) benchmark method of\n\"Golden Answers\", traditional natural language processing (NLP) metrics, RAG\nAssessment (Ragas), OpenAI GPT-4 evaluation metrics, and human evaluation,\nusing the travel chatbot use case. The travel dataset was sourced from the the\nReddit API by requesting posts from travel-related subreddits to get\ntravel-related conversation prompts and personalized travel experiences, and\naugmented for each fine-tuning method. We used two pretrained LLMs utilized for\nfine-tuning research: LLaMa 2 7B, and Mistral 7B. QLoRA and RAFT are applied to\nthe two pretrained models. The inferences from these models are extensively\nevaluated against the aforementioned metrics. The best model according to human\nevaluation and some GPT-4 metrics was Mistral RAFT, so this underwent a\nReinforcement Learning from Human Feedback (RLHF) training pipeline, and\nultimately was evaluated as the best model. Our main findings are that: 1)\nquantitative and Ragas metrics do not align with human evaluation, 2) Open AI\nGPT-4 evaluation most aligns with human evaluation, 3) it is essential to keep\nhumans in the loop for evaluation because, 4) traditional NLP metrics\ninsufficient, 5) Mistral generally outperformed LLaMa, 6) RAFT outperforms\nQLoRA, but still needs postprocessing, 7) RLHF improves model performance\nsignificantly. Next steps include improving data quality, increasing data\nquantity, exploring RAG methods, and focusing data collection on a specific\ncity, which would improve data quality by narrowing the focus, while creating a\nuseful product.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03562v1",
    "published_date": "2024-08-07 05:52:00 UTC",
    "updated_date": "2024-08-07 05:52:00 UTC"
  },
  {
    "arxiv_id": "2408.03561v1",
    "title": "MPC-Minimized Secure LLM Inference",
    "authors": [
      "Deevashwer Rathee",
      "Dacheng Li",
      "Ion Stoica",
      "Hao Zhang",
      "Raluca Popa"
    ],
    "abstract": "Many inference services based on large language models (LLMs) pose a privacy\nconcern, either revealing user prompts to the service or the proprietary\nweights to the user. Secure inference offers a solution to this problem through\nsecure multi-party computation (MPC), however, it is still impractical for\nmodern LLM workload due to the large overhead imposed by MPC. To address this\noverhead, we propose Marill, a framework that adapts LLM fine-tuning to\nminimize MPC usage during secure inference. Marill introduces high-level\narchitectural changes during fine-tuning that significantly reduce the number\nof expensive operations needed within MPC during inference, by removing some\nand relocating others outside MPC without compromising security. As a result,\nMarill-generated models are more efficient across all secure inference\nprotocols and our approach complements MPC-friendly approximations for such\noperations. Compared to standard fine-tuning, Marill results in 3.6-11.3x\nbetter runtime and 2.4-6.9x better communication during secure inference across\nvarious MPC settings, while typically preserving over 90% performance across\ndownstream tasks.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03561v1",
    "published_date": "2024-08-07 05:50:17 UTC",
    "updated_date": "2024-08-07 05:50:17 UTC"
  },
  {
    "arxiv_id": "2408.03558v1",
    "title": "D2Styler: Advancing Arbitrary Style Transfer with Discrete Diffusion Methods",
    "authors": [
      "Onkar Susladkar",
      "Gayatri Deshmukh",
      "Sparsh Mittal",
      "Parth Shastri"
    ],
    "abstract": "In image processing, one of the most challenging tasks is to render an\nimage's semantic meaning using a variety of artistic approaches. Existing\ntechniques for arbitrary style transfer (AST) frequently experience\nmode-collapse, over-stylization, or under-stylization due to a disparity\nbetween the style and content images. We propose a novel framework called\nD$^2$Styler (Discrete Diffusion Styler) that leverages the discrete\nrepresentational capability of VQ-GANs and the advantages of discrete\ndiffusion, including stable training and avoidance of mode collapse. Our method\nuses Adaptive Instance Normalization (AdaIN) features as a context guide for\nthe reverse diffusion process. This makes it easy to move features from the\nstyle image to the content image without bias. The proposed method\nsubstantially enhances the visual quality of style-transferred images, allowing\nthe combination of content and style in a visually appealing manner. We take\nstyle images from the WikiArt dataset and content images from the COCO dataset.\nExperimental results demonstrate that D$^2$Styler produces high-quality\nstyle-transferred images and outperforms twelve existing methods on nearly all\nthe metrics. The qualitative results and ablation studies provide further\ninsights into the efficacy of our technique. The code is available at\nhttps://github.com/Onkarsus13/D2Styler.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Paper accepted at 27th International Conference on Pattern\n  Recognition (ICPR), 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.03558v1",
    "published_date": "2024-08-07 05:47:06 UTC",
    "updated_date": "2024-08-07 05:47:06 UTC"
  },
  {
    "arxiv_id": "2408.03544v3",
    "title": "NatLan: Native Language Prompting Facilitates Knowledge Elicitation Through Language Trigger Provision and Domain Trigger Retention",
    "authors": [
      "Baixuan Li",
      "Yunlong Fan",
      "Tianyi Ma",
      "Zhiqiang Gao"
    ],
    "abstract": "Multilingual large language models (MLLMs) do not perform as well when\nanswering questions in non-dominant languages as they do in their dominant\nlanguages. Although existing translate-then-answer methods alleviate this\nissue, the mechanisms behind their effectiveness remain unclear. In this study,\nwe analogize the dominant language of MLLMs to the native language of humans\nand use two human cognitive features: the Language Trigger (LT) and the Domain\nTrigger (DT), to interpret the mechanisms behind translate-then-answer methods.\nThis reveals that while sufficient LTs are provided by these methods, there\nremains a deficiency in DT retention. To mitigate this issue, we propose Native\nLanguage Prompting (NatLan), employing a Multi-MLLM collaboration strategy and\nintroducing an additional role-enhanced domain-specific MLLM with stronger\nmultilingual understanding capabilities as the translator. Across five language\nQA benchmarks, NatLan achieves up to a 31.28% improvement in accuracy and,\ncompared to existing state-of-the-art methods, provides comparable or greater\nretention of DTs in up to 87% of cases. Our code is available at\nhttps://github.com/AnonyNLP/NatLan.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03544v3",
    "published_date": "2024-08-07 04:49:38 UTC",
    "updated_date": "2024-10-15 08:24:42 UTC"
  },
  {
    "arxiv_id": "2408.03542v1",
    "title": "Automatic identification of the area covered by acorn trees in the dehesa (pastureland) Extremadura of Spain",
    "authors": [
      "Ojeda-Magaña Benjamin",
      "Ruelas Ruben",
      "Quintanilla-Dominguez Joel",
      "Gomez-Barba Leopoldo",
      "Lopez de Herrera Juan",
      "Robledo-Hernandez Jose",
      "Tarquis Ana"
    ],
    "abstract": "The acorn is the fruit of the oak and is an important crop in the Spanish\ndehesa extreme\\~na, especially for the value it provides in the Iberian pig\nfood to obtain the \"acorn\" certification. For this reason, we want to maximise\nthe production of Iberian pigs with the appropriate weight. Hence the need to\nknow the area covered by the crowns of the acorn trees, to determine the\ncovered wooded area (CWA, from the Spanish Superficie Arbolada Cubierta SAC)\nand thereby estimate the number of Iberian pigs that can be released per\nhectare, as indicated by the royal decree 4/2014. In this work, we propose the\nautomatic estimation of the CWA, through aerial digital images (orthophotos) of\nthe pastureland of Extremadura, and with this, to offer the possibility of\ndetermining the number of Iberian pigs to be released in a specific plot of\nland. Among the main issues for automatic detection are, first, the correct\nidentification of acorn trees, secondly, correctly discriminating the shades of\nthe acorn trees and, finally, detect the arbuscles (young acorn trees not yet\nproductive, or shrubs that are not oaks). These difficulties represent a real\nchallenge, both for the automatic segmentation process and for manual\nsegmentation. In this work, the proposed method for automatic segmentation is\nbased on the clustering algorithm proposed by Gustafson-Kessel (GK) but the\nmodified version of Babuska (GK-B) and on the use of real orthophotos. The\nobtained results are promising both in their comparison with the real images\nand when compared with the images segmented by hand. The whole set of\northophotos used in this work correspond to an approximate area of 142\nhectares, and the results are of great interest to producers of certified\n\"acorn\" pork.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.4.6"
    ],
    "primary_category": "cs.CV",
    "comment": "22 pages, 15 Figures, 2 Tables",
    "pdf_url": "http://arxiv.org/pdf/2408.03542v1",
    "published_date": "2024-08-07 04:42:10 UTC",
    "updated_date": "2024-08-07 04:42:10 UTC"
  },
  {
    "arxiv_id": "2408.03541v3",
    "title": "EXAONE 3.0 7.8B Instruction Tuned Language Model",
    "authors": [
      "LG AI Research",
      ":",
      "Soyoung An",
      "Kyunghoon Bae",
      "Eunbi Choi",
      "Stanley Jungkyu Choi",
      "Yemuk Choi",
      "Seokhee Hong",
      "Yeonjung Hong",
      "Junwon Hwang",
      "Hyojin Jeon",
      "Gerrard Jeongwon Jo",
      "Hyunjik Jo",
      "Jiyeon Jung",
      "Yountae Jung",
      "Euisoon Kim",
      "Hyosang Kim",
      "Joonkee Kim",
      "Seonghwan Kim",
      "Soyeon Kim",
      "Sunkyoung Kim",
      "Yireun Kim",
      "Youchul Kim",
      "Edward Hwayoung Lee",
      "Haeju Lee",
      "Honglak Lee",
      "Jinsik Lee",
      "Kyungmin Lee",
      "Moontae Lee",
      "Seungjun Lee",
      "Woohyung Lim",
      "Sangha Park",
      "Sooyoun Park",
      "Yongmin Park",
      "Boseong Seo",
      "Sihoon Yang",
      "Heuiyeen Yeen",
      "Kyungjae Yoo",
      "Hyeongu Yun"
    ],
    "abstract": "We introduce EXAONE 3.0 instruction-tuned language model, the first open\nmodel in the family of Large Language Models (LLMs) developed by LG AI\nResearch. Among different model sizes, we publicly release the 7.8B\ninstruction-tuned model to promote open research and innovations. Through\nextensive evaluations across a wide range of public and in-house benchmarks,\nEXAONE 3.0 demonstrates highly competitive real-world performance with\ninstruction-following capability against other state-of-the-art open models of\nsimilar size. Our comparative analysis shows that EXAONE 3.0 excels\nparticularly in Korean, while achieving compelling performance across general\ntasks and complex reasoning. With its strong real-world effectiveness and\nbilingual proficiency, we hope that EXAONE keeps contributing to advancements\nin Expert AI. Our EXAONE 3.0 instruction-tuned model is available at\nhttps://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03541v3",
    "published_date": "2024-08-07 04:38:38 UTC",
    "updated_date": "2024-08-13 10:09:32 UTC"
  },
  {
    "arxiv_id": "2408.03533v2",
    "title": "Lifelong Personalized Low-Rank Adaptation of Large Language Models for Recommendation",
    "authors": [
      "Jiachen Zhu",
      "Jianghao Lin",
      "Xinyi Dai",
      "Bo Chen",
      "Rong Shan",
      "Jieming Zhu",
      "Ruiming Tang",
      "Yong Yu",
      "Weinan Zhang"
    ],
    "abstract": "We primarily focus on the field of large language models (LLMs) for\nrecommendation, which has been actively explored recently and poses a\nsignificant challenge in effectively enhancing recommender systems with logical\nreasoning abilities and open-world knowledge. Current mainstream efforts mainly\ncenter around injecting personalized information from recommendation models\ninto LLMs by customizing input templates or aligning representations between\nsemantic and recommendation spaces at the prediction layer. However, they face\nthree significant limitations: (1) LoRA is mostly used as a core component in\nexisting works, but personalization is not well established in LoRA parameters\nas the LoRA matrix shared by every user may not cater to different users'\ncharacteristics, leading to suboptimal performance. (2) Although lifelong\npersonalized behavior sequences are ideal for personalization, their use raises\neffectiveness and efficiency issues since LLMs require escalating training and\ninference time to extend text lengths. (3) Existing approaches aren't scalable\nfor large datasets due to training efficiency constraints. Thus, LLMs only see\na small fraction of the datasets (e.g., less than 10%) instead of the whole\ndatasets, limiting their exposure to the full training space. To address these\nproblems, we propose RecLoRA. This model incorporates a Personalized LoRA\nmodule that maintains independent LoRAs for different users and a Long-Short\nModality Retriever that retrieves different history lengths for different\nmodalities, significantly improving performance while adding minimal time cost.\nFurthermore, we design a Few2Many Learning Strategy, using a conventional\nrecommendation model as a lens to magnify small training spaces to full spaces.\nExtensive experiments on public datasets demonstrate the efficacy of our\nRecLoRA compared to existing baseline models.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03533v2",
    "published_date": "2024-08-07 04:20:28 UTC",
    "updated_date": "2024-08-11 09:08:59 UTC"
  },
  {
    "arxiv_id": "2408.11840v1",
    "title": "Joint PET-MRI Reconstruction with Diffusion Stochastic Differential Model",
    "authors": [
      "Taofeng Xie",
      "Zhuoxu Cui",
      "Congcong Liu",
      "Chen Luo",
      "Huayu Wang",
      "Yuanzhi Zhang",
      "Xuemei Wang",
      "Yihang Zhou",
      "Qiyu Jin",
      "Guoqing Chen",
      "Dong Liang",
      "Haifeng Wang"
    ],
    "abstract": "PET suffers from a low signal-to-noise ratio. Meanwhile, the k-space data\nacquisition process in MRI is time-consuming by PET-MRI systems. We aim to\naccelerate MRI and improve PET image quality. This paper proposed a novel joint\nreconstruction model by diffusion stochastic differential equations based on\nlearning the joint probability distribution of PET and MRI. Compare the results\nunderscore the qualitative and quantitative improvements our model brings to\nPET and MRI reconstruction, surpassing the current state-of-the-art\nmethodologies. Joint PET-MRI reconstruction is a challenge in the PET-MRI\nsystem. This studies focused on the relationship extends beyond edges. In this\nstudy, PET is generated from MRI by learning joint probability distribution as\nthe relationship.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted as ISMRM 2024 Digital poster 6575. 04-09 May 2024 Singapore",
    "pdf_url": "http://arxiv.org/pdf/2408.11840v1",
    "published_date": "2024-08-07 04:01:50 UTC",
    "updated_date": "2024-08-07 04:01:50 UTC"
  },
  {
    "arxiv_id": "2408.03528v2",
    "title": "Exploring the extent of similarities in software failures across industries using LLMs",
    "authors": [
      "Martin Detloff"
    ],
    "abstract": "The rapid evolution of software development necessitates enhanced safety\nmeasures. Extracting information about software failures from companies is\nbecoming increasingly more available through news articles.\n  This research utilizes the Failure Analysis Investigation with LLMs (FAIL)\nmodel to extract industry-specific information. Although the FAIL model's\ndatabase is rich in information, it could benefit from further categorization\nand industry-specific insights to further assist software engineers.\n  In previous work news articles were collected from reputable sources and\ncategorized by incidents inside a database. Prompt engineering and Large\nLanguage Models (LLMs) were then applied to extract relevant information\nregarding the software failure. This research extends these methods by\ncategorizing articles into specific domains and types of software failures. The\nresults are visually represented through graphs.\n  The analysis shows that throughout the database some software failures occur\nsignificantly more often in specific industries. This categorization provides a\nvaluable resource for software engineers and companies to identify and address\ncommon failures.\n  This research highlights the synergy between software engineering and Large\nLanguage Models (LLMs) to automate and enhance the analysis of software\nfailures. By transforming data from the database into an industry specific\nmodel, we provide a valuable resource that can be used to identify common\nvulnerabilities, predict potential risks, and implement proactive measures for\npreventing software failures. Leveraging the power of the current FAIL database\nand data visualization, we aim to provide an avenue for safer and more secure\nsoftware in the future.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03528v2",
    "published_date": "2024-08-07 03:48:07 UTC",
    "updated_date": "2024-08-08 03:52:06 UTC"
  },
  {
    "arxiv_id": "2408.03525v1",
    "title": "Hierarchical learning control for autonomous robots inspired by central nervous system",
    "authors": [
      "Pei Zhang",
      "Zhaobo Hua",
      "Jinliang Ding"
    ],
    "abstract": "Mammals can generate autonomous behaviors in various complex environments\nthrough the coordination and interaction of activities at different levels of\ntheir central nervous system. In this paper, we propose a novel hierarchical\nlearning control framework by mimicking the hierarchical structure of the\ncentral nervous system along with their coordination and interaction behaviors.\nThe framework combines the active and passive control systems to improve both\nthe flexibility and reliability of the control system as well as to achieve\nmore diverse autonomous behaviors of robots. Specifically, the framework has a\nbackbone of independent neural network controllers at different levels and\ntakes a three-level dual descending pathway structure, inspired from the\nfunctionality of the cerebral cortex, cerebellum, and spinal cord. We\ncomprehensively validated the proposed approach through the simulation as well\nas the experiment of a hexapod robot in various complex environments, including\nobstacle crossing and rapid recovery after partial damage. This study reveals\nthe principle that governs the autonomous behavior in the central nervous\nsystem and demonstrates the effectiveness of the hierarchical control approach\nwith the salient features of the hierarchical learning control architecture and\ncombination of active and passive control systems.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03525v1",
    "published_date": "2024-08-07 03:24:59 UTC",
    "updated_date": "2024-08-07 03:24:59 UTC"
  },
  {
    "arxiv_id": "2408.11839v1",
    "title": "Adaptive Friction in Deep Learning: Enhancing Optimizers with Sigmoid and Tanh Function",
    "authors": [
      "Hongye Zheng",
      "Bingxing Wang",
      "Minheng Xiao",
      "Honglin Qin",
      "Zhizhong Wu",
      "Lianghao Tan"
    ],
    "abstract": "Adaptive optimizers are pivotal in guiding the weight updates of deep neural\nnetworks, yet they often face challenges such as poor generalization and\noscillation issues. To counter these, we introduce sigSignGrad and\ntanhSignGrad, two novel optimizers that integrate adaptive friction\ncoefficients based on the Sigmoid and Tanh functions, respectively. These\nalgorithms leverage short-term gradient information, a feature overlooked in\ntraditional Adam variants like diffGrad and AngularGrad, to enhance parameter\nupdates and convergence.Our theoretical analysis demonstrates the wide-ranging\nadjustment capability of the friction coefficient S, which aligns with targeted\nparameter update strategies and outperforms existing methods in both\noptimization trajectory smoothness and convergence rate. Extensive experiments\non CIFAR-10, CIFAR-100, and Mini-ImageNet datasets using ResNet50 and ViT\narchitectures confirm the superior performance of our proposed optimizers,\nshowcasing improved accuracy and reduced training time. The innovative approach\nof integrating adaptive friction coefficients as plug-ins into existing\noptimizers, exemplified by the sigSignAdamW and sigSignAdamP variants, presents\na promising strategy for boosting the optimization performance of established\nalgorithms. The findings of this study contribute to the advancement of\noptimizer design in deep learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11839v1",
    "published_date": "2024-08-07 03:20:46 UTC",
    "updated_date": "2024-08-07 03:20:46 UTC"
  },
  {
    "arxiv_id": "2408.03519v1",
    "title": "RepoMasterEval: Evaluating Code Completion via Real-World Repositories",
    "authors": [
      "Qinyun Wu",
      "Chao Peng",
      "Pengfei Gao",
      "Ruida Hu",
      "Haoyu Gan",
      "Bo Jiang",
      "Jinhe Tang",
      "Zhiwen Deng",
      "Zhanming Guan",
      "Cuiyun Gao",
      "Xia Liu",
      "Ping Yang"
    ],
    "abstract": "With the growing reliance on automated code completion tools in software\ndevelopment, the need for robust evaluation benchmarks has become critical.\nHowever, existing benchmarks focus more on code generation tasks in function\nand class level and provide rich text description to prompt the model. By\ncontrast, such descriptive prompt is commonly unavailable in real development\nand code completion can occur in wider range of situations such as in the\nmiddle of a function or a code block. These limitations makes the evaluation\npoorly align with the practical scenarios of code completion tools. In this\npaper, we propose RepoMasterEval, a novel benchmark for evaluating code\ncompletion models constructed from real-world Python and TypeScript\nrepositories. Each benchmark datum is generated by masking a code snippet\n(ground truth) from one source code file with existing test suites. To improve\ntest accuracy of model generated code, we employ mutation testing to measure\nthe effectiveness of the test cases and we manually crafted new test cases for\nthose test suites with low mutation score. Our empirical evaluation on 6\nstate-of-the-art models shows that test argumentation is critical in improving\nthe accuracy of the benchmark and RepoMasterEval is able to report difference\nin model performance in real-world scenarios. The deployment of RepoMasterEval\nin a collaborated company for one month also revealed that the benchmark is\nuseful to give accurate feedback during model training and the score is in high\ncorrelation with the model's performance in practice. Based on our findings, we\ncall for the software engineering community to build more LLM benchmarks\ntailored for code generation tools taking the practical and complex development\nenvironment into consideration.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03519v1",
    "published_date": "2024-08-07 03:06:57 UTC",
    "updated_date": "2024-08-07 03:06:57 UTC"
  },
  {
    "arxiv_id": "2408.03515v2",
    "title": "A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic Systems",
    "authors": [
      "Wenxiao Zhang",
      "Xiangrui Kong",
      "Conan Dewitt",
      "Thomas Braunl",
      "Jin B. Hong"
    ],
    "abstract": "The integration of Large Language Models (LLMs) like GPT-4o into robotic\nsystems represents a significant advancement in embodied artificial\nintelligence. These models can process multi-modal prompts, enabling them to\ngenerate more context-aware responses. However, this integration is not without\nchallenges. One of the primary concerns is the potential security risks\nassociated with using LLMs in robotic navigation tasks. These tasks require\nprecise and reliable responses to ensure safe and effective operation.\nMulti-modal prompts, while enhancing the robot's understanding, also introduce\ncomplexities that can be exploited maliciously. For instance, adversarial\ninputs designed to mislead the model can lead to incorrect or dangerous\nnavigational decisions. This study investigates the impact of prompt injections\non mobile robot performance in LLM-integrated systems and explores secure\nprompt strategies to mitigate these risks. Our findings demonstrate a\nsubstantial overall improvement of approximately 30.8% in both attack detection\nand system performance with the implementation of robust defence mechanisms,\nhighlighting their critical role in enhancing security and reliability in\nmission-oriented tasks.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03515v2",
    "published_date": "2024-08-07 02:48:22 UTC",
    "updated_date": "2024-09-09 01:55:03 UTC"
  },
  {
    "arxiv_id": "2408.03505v1",
    "title": "Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation",
    "authors": [
      "Weiqi Feng",
      "Yangrui Chen",
      "Shaoyu Wang",
      "Yanghua Peng",
      "Haibin Lin",
      "Minlan Yu"
    ],
    "abstract": "Multimodal large language models (MLLMs) have extended the success of large\nlanguage models (LLMs) to multiple data types, such as image, text and audio,\nachieving significant performance in various domains, including multimodal\ntranslation, visual question answering and content generation. Nonetheless,\nexisting systems are inefficient to train MLLMs due to substantial GPU bubbles\ncaused by the heterogeneous modality models and complex data dependencies in 3D\nparallelism. This paper proposes Optimus, a distributed MLLM training system\nthat reduces end-to-end MLLM training time. Optimus is based on our principled\nanalysis that scheduling the encoder computation within the LLM bubbles can\nreduce bubbles in MLLM training. To make scheduling encoder computation\npossible for all GPUs, Optimus searches the separate parallel plans for encoder\nand LLM, and adopts a bubble scheduling algorithm to enable exploiting LLM\nbubbles without breaking the original data dependencies in the MLLM model\narchitecture. We further decompose encoder layer computation into a series of\nkernels, and analyze the common bubble pattern of 3D parallelism to carefully\noptimize the sub-millisecond bubble scheduling, minimizing the overall training\ntime. Our experiments in a production cluster show that Optimus accelerates\nMLLM training by 20.5%-21.3% with ViT-22B and GPT-175B model over 3072 GPUs\ncompared to baselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03505v1",
    "published_date": "2024-08-07 02:08:29 UTC",
    "updated_date": "2024-08-07 02:08:29 UTC"
  },
  {
    "arxiv_id": "2408.04668v2",
    "title": "Forecasting Live Chat Intent from Browsing History",
    "authors": [
      "Se-eun Yoon",
      "Ahmad Bin Rabiah",
      "Zaid Alibadi",
      "Surya Kallumadi",
      "Julian McAuley"
    ],
    "abstract": "Customers reach out to online live chat agents with various intents, such as\nasking about product details or requesting a return. In this paper, we propose\nthe problem of predicting user intent from browsing history and address it\nthrough a two-stage approach. The first stage classifies a user's browsing\nhistory into high-level intent categories. Here, we represent each browsing\nhistory as a text sequence of page attributes and use the ground-truth class\nlabels to fine-tune pretrained Transformers. The second stage provides a large\nlanguage model (LLM) with the browsing history and predicted intent class to\ngenerate fine-grained intents. For automatic evaluation, we use a separate LLM\nto judge the similarity between generated and ground-truth intents, which\nclosely aligns with human judgments. Our two-stage approach yields significant\nperformance gains compared to generating intents without the classification\nstage.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "CIKM 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.04668v2",
    "published_date": "2024-08-07 01:50:59 UTC",
    "updated_date": "2024-09-01 19:00:25 UTC"
  },
  {
    "arxiv_id": "2408.03497v3",
    "title": "Advanced User Credit Risk Prediction Model using LightGBM, XGBoost and Tabnet with SMOTEENN",
    "authors": [
      "Chang Yu",
      "Yixin Jin",
      "Qianwen Xing",
      "Ye Zhang",
      "Shaobo Guo",
      "Shuchen Meng"
    ],
    "abstract": "Bank credit risk is a significant challenge in modern financial transactions,\nand the ability to identify qualified credit card holders among a large number\nof applicants is crucial for the profitability of a bank'sbank's credit card\nbusiness. In the past, screening applicants'applicants' conditions often\nrequired a significant amount of manual labor, which was time-consuming and\nlabor-intensive. Although the accuracy and reliability of previously used ML\nmodels have been continuously improving, the pursuit of more reliable and\npowerful AI intelligent models is undoubtedly the unremitting pursuit by major\nbanks in the financial industry. In this study, we used a dataset of over\n40,000 records provided by a commercial bank as the research object. We\ncompared various dimensionality reduction techniques such as PCA and T-SNE for\npreprocessing high-dimensional datasets and performed in-depth adaptation and\ntuning of distributed models such as LightGBM and XGBoost, as well as deep\nmodels like Tabnet. After a series of research and processing, we obtained\nexcellent research results by combining SMOTEENN with these techniques. The\nexperiments demonstrated that LightGBM combined with PCA and SMOTEENN\ntechniques can assist banks in accurately predicting potential high-quality\ncustomers, showing relatively outstanding performance compared to other models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pagess on IEEE ICPICS",
    "pdf_url": "http://arxiv.org/pdf/2408.03497v3",
    "published_date": "2024-08-07 01:37:10 UTC",
    "updated_date": "2024-11-13 04:41:31 UTC"
  },
  {
    "arxiv_id": "2408.03492v1",
    "title": "Automated Theorem Provers Help Improve Large Language Model Reasoning",
    "authors": [
      "Lachlan McGinness",
      "Peter Baumgartner"
    ],
    "abstract": "In this paper we demonstrate how logic programming systems and Automated\nfirst-order logic Theorem Provers (ATPs) can improve the accuracy of Large\nLanguage Models (LLMs) for logical reasoning tasks where the baseline\nperformance is given by direct LLM solutions. We first evaluate LLM reasoning\non steamroller problems using the PRONTOQA benchmark. We show how accuracy can\nbe improved with a neuro-symbolic architecture where the LLM acts solely as a\nfront-end for translating a given problem into a formal logic language and an\nautomated reasoning engine is called for solving it. However, this approach\ncritically hinges on the correctness of the LLM translation. To assess this\ntranslation correctness, we secondly define a framework of syntactic and\nsemantic error categories. We implemented the framework and used it to identify\nerrors that LLMs make in the benchmark domain. Based on these findings, we\nthirdly extended our method with capabilities for automatically correcting\nsyntactic and semantic errors. For semantic error correction we integrate\nfirst-order logic ATPs, which is our main and novel contribution. We\ndemonstrate that this approach reduces semantic errors significantly and\nfurther increases the accurracy of LLM logical reasoning.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "F.4.1; I.2.7; I.2.8"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03492v1",
    "published_date": "2024-08-07 01:03:56 UTC",
    "updated_date": "2024-08-07 01:03:56 UTC"
  },
  {
    "arxiv_id": "2408.03489v1",
    "title": "Harnessing the Power of LLMs in Source Code Vulnerability Detection",
    "authors": [
      "Andrew A Mahyari"
    ],
    "abstract": "Software vulnerabilities, caused by unintentional flaws in source code, are a\nprimary root cause of cyberattacks. Static analysis of source code has been\nwidely used to detect these unintentional defects introduced by software\ndevelopers. Large Language Models (LLMs) have demonstrated human-like\nconversational abilities due to their capacity to capture complex patterns in\nsequential data, such as natural languages. In this paper, we harness LLMs'\ncapabilities to analyze source code and detect known vulnerabilities. To ensure\nthe proposed vulnerability detection method is universal across multiple\nprogramming languages, we convert source code to LLVM IR and train LLMs on\nthese intermediate representations. We conduct extensive experiments on various\nLLM architectures and compare their accuracy. Our comprehensive experiments on\nreal-world and synthetic codes from NVD and SARD demonstrate high accuracy in\nidentifying source code vulnerabilities.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03489v1",
    "published_date": "2024-08-07 00:48:49 UTC",
    "updated_date": "2024-08-07 00:48:49 UTC"
  }
]
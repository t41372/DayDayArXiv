{
  "date": "2025-11-01",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-11-01 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv è®ºæ–‡åˆ—è¡¨å¼‚å¸¸ä¸°å¯Œï¼Œ**Agentic AIï¼ˆæ™ºèƒ½ä½“ AIï¼‰** çš„ç³»ç»Ÿçº§ä¼˜åŒ–å’ŒåŸºç¡€è®¾æ–½å»ºè®¾æˆä¸ºäº†é‡å¤´æˆï¼Œç‰¹åˆ«æ˜¯å…³äº Agent å·¥ä½œè´Ÿè½½ä¸­ CPU ç“¶é¢ˆçš„è®¨è®ºä»¤äººè€³ç›®ä¸€æ–°ã€‚æ­¤å¤–ï¼Œ**Test-time Scalingï¼ˆæµ‹è¯•æ—¶æ‰©å±•ï¼‰** å’Œ **æ¨ç†èƒ½åŠ›ï¼ˆReasoningï¼‰** çš„ç ”ç©¶ä¾ç„¶ç«çƒ­ï¼Œå‡ºç°äº†æ–°çš„ç»¼è¿°å’Œè§£ç ç­–ç•¥ã€‚å®‰å…¨é¢†åŸŸçˆ†å‡ºäº†ä¸€ä¸ªé€šè¿‡ä¿®æ”¹è®¡ç®—å›¾æ¤å…¥åé—¨çš„â€œShadowLogicâ€æ”»å‡»ï¼Œå€¼å¾—è­¦æƒ•ã€‚\n\n---\n\n### ğŸš€ Agentic AI & ç³»ç»ŸåŸºç¡€è®¾æ–½\næ™ºèƒ½ä½“ä¸å†ä»…ä»…æ˜¯æ¨¡å‹ï¼Œå®ƒä»¬æ­£åœ¨å˜æˆå¤æ‚çš„è½¯ä»¶ç³»ç»Ÿã€‚\n\n**1. ä»¥ CPU ä¸ºä¸­å¿ƒçš„ Agentic AI è§†è§’ (A CPU-Centric Perspective on Agentic AI)**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè¿™æ˜¯ä¸€ç¯‡éå¸¸é‡è¦çš„ç³»ç»Ÿçº§åˆ†ææ–‡ç« ã€‚ä½œè€…æŒ‡å‡ºï¼ŒAgentic AIï¼ˆåŒ…å«å·¥å…·è°ƒç”¨ã€è§„åˆ’ã€è®°å¿†çš„æ™ºèƒ½ä½“ï¼‰ä¸ä¼ ç»Ÿçš„å•ä½“ LLM ä¸åŒï¼Œå®ƒä»¬ä»â€œè¢«åŠ¨çš„æ–‡æœ¬ç¥è°•â€å˜æˆäº†â€œä¸»åŠ¨çš„é—®é¢˜è§£å†³è€…â€ã€‚\n*   **å…³é”®å‘ç°**ï¼šç ”ç©¶å‘ç°ï¼Œåœ¨ Agent å·¥ä½œæµä¸­ï¼Œ**CPU æ‰æ˜¯ç“¶é¢ˆ**ï¼Œè€Œé GPUã€‚å·¥å…·å¤„ç†ï¼ˆå¦‚ Python è§£é‡Šå™¨ã€æœç´¢ï¼‰å¯èƒ½å æ®é«˜è¾¾ 90.6% çš„å»¶è¿Ÿã€‚ä½œè€…æå‡ºäº† CGAMï¼ˆå¾®æ‰¹å¤„ç†ï¼‰å’Œ MAWSï¼ˆæ··åˆè°ƒåº¦ï¼‰æ¥ä¼˜åŒ–è¿™äº›è´Ÿè½½ï¼Œå®ç°äº†é«˜è¾¾ 2.1 å€çš„é€Ÿåº¦æå‡ã€‚è¿™å¯¹æœªæ¥è®¾è®¡ AI èŠ¯ç‰‡å’ŒæœåŠ¡å™¨æ¶æ„æœ‰é‡è¦å¯ç¤ºã€‚\n\n**22. AgentGitï¼šç”¨äºå¯é ä¸”å¯æ‰©å±•çš„ LLM å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„ç‰ˆæœ¬æ§åˆ¶æ¡†æ¶ (AgentGit: A Version Control Framework for Reliable and Scalable LLM-Powered Multi-Agent Systems)**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰å¼•å…¥äº†ç±»ä¼¼ Git çš„ç‰ˆæœ¬æ§åˆ¶æ¦‚å¿µã€‚\n*   **å…³é”®å‘ç°**ï¼šåœ¨ LangGraph ä¹‹ä¸Šæ„å»ºï¼Œæ”¯æŒçŠ¶æ€æäº¤ï¼ˆcommitï¼‰ã€å›æ»šï¼ˆrevertï¼‰å’Œåˆ†æ”¯ï¼ˆbranchingï¼‰ã€‚è¿™å…è®¸æ™ºèƒ½ä½“åœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶å¹¶è¡Œæ¢ç´¢ä¸åŒçš„è·¯å¾„ï¼Œå¹¶åœ¨å‡ºé”™æ—¶å›æº¯ï¼Œæå¤§åœ°æé«˜äº† Agent å¼€å‘çš„è°ƒè¯•æ•ˆç‡å’Œè¿è¡Œæ—¶çš„é²æ£’æ€§ã€‚\n\n**74. EvoMemï¼šåˆ©ç”¨åŒé‡è¿›åŒ–è®°å¿†æ”¹è¿›å¤šæ™ºèƒ½ä½“è§„åˆ’ (EvoMem: Improving Multi-Agent Planning with Dual-Evolving Memory)**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå—è®¤çŸ¥å¿ƒç†å­¦å¯å‘ï¼Œæå‡ºäº†åŒ…å«â€œçº¦æŸè®°å¿†â€ï¼ˆè·¨æŸ¥è¯¢è¿›åŒ–ï¼‰å’Œâ€œæŸ¥è¯¢åé¦ˆè®°å¿†â€ï¼ˆæŸ¥è¯¢å†…è¿›åŒ–ï¼‰çš„åŒé‡æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº† Agent çš„è§„åˆ’èƒ½åŠ›ã€‚\n\n---\n\n### ğŸ§  LLM æ¨ç†ä¸ä¼˜åŒ– (Reasoning & Optimization)\nå¦‚ä½•è®©æ¨¡å‹æƒ³å¾—æ›´æ¸…æ¥šï¼Œæˆ–è€…ç®—å¾—æ›´å¿«ï¼Ÿ\n\n**17. LLM æµ‹è¯•æ—¶æ‰©å±•ï¼šå­é—®é¢˜ç»“æ„è§†è§’çš„ç»¼è¿° (Test-time Scaling of LLMs: A Survey from A Subproblem Structure Perspective)**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå¯¹è¿‘æœŸç«çƒ­çš„â€œTest-time Computeâ€ï¼ˆæµ‹è¯•æ—¶è®¡ç®—/æ¨ç†æ—¶æ‰©å±•ï¼‰è¿›è¡Œäº†ç³»ç»Ÿç»¼è¿°ã€‚\n*   **å…³é”®å‘ç°**ï¼šæ–‡ç« é€šè¿‡â€œå­é—®é¢˜åˆ†è§£â€å’Œâ€œæ‹“æ‰‘ç»“æ„â€ï¼ˆé¡ºåºã€å¹¶è¡Œã€æ ‘çŠ¶ï¼‰çš„è§†è§’ï¼Œç»Ÿä¸€äº† Chain-of-Thought (CoT)ã€Tree-of-Thought (ToT) ç­‰æŠ€æœ¯ã€‚è¿™æ˜¯ç†è§£ o1 ç±»æ¨¡å‹èƒŒååŸç†çš„å¿…è¯»ç»¼è¿°ã€‚\n\n**20. DTSï¼šé€šè¿‡è§£ç æ ‘è‰å›¾å¢å¼ºå¤§å‹æ¨ç†æ¨¡å‹ (DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching)**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹æ¨ç†æ¨¡å‹ï¼ˆReasoning Modelsï¼‰å®¹æ˜“â€œè¿‡åº¦æ€è€ƒâ€ï¼ˆoverthinkingï¼‰çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„è§£ç ç­–ç•¥ã€‚\n*   **å…³é”®å‘ç°**ï¼šåˆ†æå‘ç°æ¨ç†é•¿åº¦ä¸å‡†ç¡®ç‡å‘ˆè´Ÿç›¸å…³ï¼ˆè¶Šé•¿è¶Šå®¹æ˜“é”™ï¼‰ã€‚DTS é€šè¿‡åœ¨ä½ç½®ä¿¡åº¦ï¼ˆé«˜ç†µï¼‰token å¤„è¿›è¡Œåˆ†æ”¯ï¼Œå¹¶é‡‡ç”¨æ—©åœç­–ç•¥é€‰æ‹©æœ€çŸ­çš„å®Œæ•´æ¨ç†è·¯å¾„ï¼Œåœ¨ AIME æ•°æ®é›†ä¸Šæå‡äº† 8% çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶å‡å°‘äº†è®¡ç®—æˆæœ¬ã€‚\n\n**12. ç†è§£æ·±åº¦å­¦ä¹ ä¼˜åŒ–çš„å„å‘åŒæ€§æ›²ç‡æ¨¡å‹ï¼šæ¢¯åº¦æ­£äº¤åŒ–æ˜¯æœ€ä¼˜çš„å—ï¼Ÿ (Isotropic Curvature Model for Understanding Deep Learning Optimization: Is Gradient Orthogonalization Optimal?)**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šä»ç†è®ºå±‚é¢åˆ†æäº†è¿‘æœŸæµè¡Œçš„ **Muon ä¼˜åŒ–å™¨**ã€‚\n*   **å…³é”®å‘ç°**ï¼šè¯æ˜äº† Muon ç­‰æ–¹æ³•ä¸­ä½¿ç”¨çš„æ¢¯åº¦æ­£äº¤åŒ–åœ¨æ–¹å‘ä¸Šæ˜¯æ­£ç¡®çš„ï¼Œä½†åœ¨æ›²ç‡å¢é•¿å‡ºç°ç›¸å˜æ—¶æ‰ä¸¥æ ¼æœ€ä¼˜ã€‚è¿™ä¸ºè®¾è®¡æ›´é«˜æ•ˆçš„è¯­è¨€æ¨¡å‹è®­ç»ƒä¼˜åŒ–å™¨æä¾›äº†ç†è®ºåŸºç¡€ã€‚\n\n**9. å»ç²—å–ç²¾ï¼šRAG ä¸­çš„åˆ†æ­§è§‚ç‚¹ç­›é€‰ (Separate the Wheat from the Chaff: Winnowing Down Divergent Views in Retrieval Augmented Generation)**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† WinnowRAGï¼Œè§£å†³ RAG æ£€ç´¢æ–‡æ¡£è¿‡å¤šå¼•å…¥å™ªå£°çš„é—®é¢˜ã€‚\n*   **å…³é”®å‘ç°**ï¼šé€šè¿‡èšç±»æ–‡æ¡£å¹¶è®© LLM Agent ç”Ÿæˆä¸åŒç­”æ¡ˆï¼Œå†å¼•å…¥ä¸€ä¸ª Critic LLM è¿›è¡Œâ€œç­›é€‰ï¼ˆwinnowingï¼‰â€ï¼Œå»é™¤å™ªå£°æ–‡æ¡£ï¼Œä¿ç•™æœ‰ä»·å€¼ä¿¡æ¯ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ä¸å¯¹é½ (Security & Alignment)\nä»Šå¤©çš„å®‰å…¨è®ºæ–‡è®©äººâ€œç»†æ€ææâ€ã€‚\n\n**14. ShadowLogicï¼šä»»æ„ç™½ç›’ LLM ä¸­çš„åé—¨ (ShadowLogic: Backdoors in Any Whitebox LLM)**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæ­ç¤ºäº†ä¸€ç§æ–°çš„æ”»å‡»é¢â€”â€”**è®¡ç®—å›¾ï¼ˆComputational Graphï¼‰**ã€‚\n*   **å…³é”®å‘ç°**ï¼šæ”»å‡»è€…ä¸éœ€è¦ä¿®æ”¹æ¨¡å‹æƒé‡ï¼Œè€Œæ˜¯ç›´æ¥ä¿®æ”¹æ¨¡å‹çš„ ONNX è®¡ç®—å›¾ç»“æ„ï¼Œæ¤å…¥ä¸€æ®µé€»è¾‘ï¼šå½“æ£€æµ‹åˆ°è§¦å‘è¯æ—¶ï¼Œç»•è¿‡å®‰å…¨æŠ¤æ ã€‚è¿™ç§åé—¨åœ¨å‚æ•°å±‚é¢ä¸å¯è§ï¼Œä¸”èƒ½é€ƒé¿å¸¸è§„æ£€æµ‹ï¼Œæ”»å‡»æˆåŠŸç‡ >60%ã€‚\n\n**47. DRIPï¼šé€šè¿‡ Token çº§è¡¨ç¤ºç¼–è¾‘å’Œæ®‹å·®æŒ‡ä»¤èåˆé˜²å¾¡æç¤ºæ³¨å…¥ (DRIP: Defending Prompt Injection via Token-wise Representation Editing and Residual Instruction Fusion)**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šä¸€ç§é˜²å¾¡ Prompt Injection çš„æ–°æ–¹æ³•ã€‚\n*   **å…³é”®å‘ç°**ï¼šé€šè¿‡ç¼–è¾‘æ•°æ®éƒ¨åˆ† Token çš„è¡¨ç¤ºæ¥å»é™¤å…¶â€œæŒ‡ä»¤è¯­ä¹‰â€ï¼ŒåŒæ—¶ä¿ç•™â€œæ•°æ®è¯­ä¹‰â€ï¼Œå¹¶ä½¿ç”¨æ®‹å·®æ¨¡å—é˜²æ­¢æŒ‡ä»¤è¢«è¦†ç›–ã€‚åœ¨ LLaMA å’Œ Mistral ä¸Šæ•ˆæœæ˜¾è‘—ã€‚\n\n**77. åˆ©ç”¨æ½œåœ¨ç©ºé—´ä¸è¿ç»­æ€§æ„å»ºé€šç”¨ LLM è¶Šç‹±å’Œæ•°æ®æå–æ”»å‡» (Exploiting Latent Space Discontinuities for Building Universal LLM Jailbreaks and Data Extraction Attacks)**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå‘ç°å¹¶åˆ©ç”¨äº†è®­ç»ƒæ•°æ®ç¨€ç–æ€§å¯¼è‡´çš„æ½œåœ¨ç©ºé—´ä¸è¿ç»­æ€§ï¼ˆLatent Space Discontinuitiesï¼‰ã€‚\n*   **å…³é”®å‘ç°**ï¼šè¿™ç§æ”»å‡»æ–¹æ³•å…·æœ‰å¾ˆå¼ºçš„é€šç”¨æ€§ï¼Œèƒ½æ”»ç ´å¤šä¸ª SOTA LLM ç”šè‡³å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸æœºå™¨äºº (Multimodal & Robotics)\n\n**6. Ariadneï¼šæ¢ç´¢å’Œæ‰©å±• VLM æ¨ç†è¾¹ç•Œçš„å¯æ§æ¡†æ¶ (Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries)**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šç ”ç©¶ RL åè®­ç»ƒï¼ˆPost-trainingï¼‰èƒ½å¦çœŸæ­£æå‡ VLM çš„è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚\n*   **å…³é”®å‘ç°**ï¼šä½¿ç”¨åˆæˆè¿·å®«æ•°æ®è¿›è¡Œ RLVR è®­ç»ƒï¼Œä¸ä»…è®© VLM åœ¨è¿·å®«ä»»åŠ¡ä¸Šä» 0% æå‡åˆ° 50% å‡†ç¡®ç‡ï¼Œè¿˜å®ç°äº†å¯¹ç°å®ä¸–ç•Œç©ºé—´ä»»åŠ¡ï¼ˆå¦‚åœ°é“æ¢ä¹˜ã€åšç‰©é¦†å¯¼èˆªï¼‰çš„ Zero-shot æ³›åŒ–ã€‚\n\n**54. iFlyBot-VLA æŠ€æœ¯æŠ¥å‘Š (iFlyBot-VLA Technical Report)**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šç§‘å¤§è®¯é£å›¢é˜Ÿå‘å¸ƒçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ã€‚\n*   **å…³é”®å‘ç°**ï¼šé‡‡ç”¨åŒå±‚åŠ¨ä½œè¡¨ç¤ºï¼ˆæ½œåœ¨æ„å›¾ + ç¦»æ•£æ§åˆ¶ Tokenï¼‰ï¼Œç»“åˆæœºå™¨äººè½¨è¿¹æ•°æ®ä¸é€šç”¨ QA æ•°æ®æ··åˆè®­ç»ƒï¼Œåœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚\n\n**61. UME-R1ï¼šæ¢ç´¢æ¨ç†é©±åŠ¨çš„ç”Ÿæˆå¼å¤šæ¨¡æ€åµŒå…¥ (UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings)**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå°†å¤šæ¨¡æ€åµŒå…¥ï¼ˆEmbeddingï¼‰ä»»åŠ¡ç»Ÿä¸€ä¸ºç”Ÿæˆå¼èŒƒå¼ã€‚\n*   **å…³é”®å‘ç°**ï¼šåˆ©ç”¨ MLLM çš„æ¨ç†èƒ½åŠ›ç”ŸæˆåµŒå…¥ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚è¯æ˜äº†ç”Ÿæˆå¼åµŒå…¥æ¯”ä¼ ç»Ÿçš„åˆ¤åˆ«å¼åµŒå…¥æ›´å…·æ½œåŠ›ã€‚\n\n---\n\n### ğŸ¥ åŒ»ç–— AI (Medical AI)\n\n**26. è¯Šæ–­ AI å¤–ç§‘å†³ç­–æ”¯æŒä¸­çš„å¹»è§‰é£é™© (Diagnosing Hallucination Risk in AI Surgical Decision-Support: A Sequential Framework for Sequential Validation)**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè¯„ä¼°äº† LLM åœ¨è„ŠæŸ±å¤–ç§‘å†³ç­–ä¸­çš„å¹»è§‰é£é™©ã€‚\n*   **å…³é”®å‘ç°**ï¼šDeepSeek-R1 è¡¨ç°ä¼˜å¼‚ï¼Œä½†ä»¤äººæƒŠè®¶çš„æ˜¯ï¼ŒClaude-3.7-Sonnet çš„â€œæ‰©å±•æ€è€ƒæ¨¡å¼â€åœ¨ä¸´åºŠå¯é æ€§ä¸Šåè€Œä¸å¦‚æ ‡å‡†ç‰ˆï¼Œè¯´æ˜å•çº¯å¢åŠ  CoT æ¨ç†å¹¶ä¸ä¸€å®šèƒ½è§£å†³åŒ»ç–—åœºæ™¯çš„å¯é æ€§é—®é¢˜ã€‚\n\n**55. MedRECTï¼šä¸´åºŠæ–‡æœ¬é”™è¯¯çº æ­£çš„åŒ»å­¦æ¨ç†åŸºå‡† (MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts)**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå‘å¸ƒäº†é¦–ä¸ªè·¨è¯­è¨€ï¼ˆæ—¥/è‹±ï¼‰çš„åŒ»å­¦æ–‡æœ¬çº é”™åŸºå‡†ã€‚\n*   **å…³é”®å‘ç°**ï¼šæ¨ç†æ¨¡å‹ï¼ˆReasoning Modelsï¼‰åœ¨é”™è¯¯æ£€æµ‹ä¸Šæ¯”æ ‡å‡†æ¨¡å‹é«˜å‡º 13.5%ï¼Œä½†åœ¨è·¨è¯­è¨€è¡¨ç°ä¸Šä»æœ‰å·®è·ã€‚\n\n---\n\n### ğŸ” å…¶ä»–å€¼å¾—å…³æ³¨çš„è®ºæ–‡ (Quick Reads)\n\n*   **[Hardware] 2. EP-HDC**: åˆ©ç”¨è¶…ç»´è®¡ç®—ï¼ˆHyperdimensional Computingï¼‰åŠ é€ŸåŒæ€åŠ å¯†æ¨ç†ï¼Œé€Ÿåº¦æå‡å‡ ä¸ªæ•°é‡çº§ã€‚\n*   **[Vision/Deepfake] 50. Enhancing Frequency Forgery Clues**: åˆ©ç”¨é¢‘åŸŸçº¿ç´¢æ£€æµ‹æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„ä¼ªé€ å›¾åƒã€‚\n*   **[Vision/Deepfake] 51. Leveraging Hierarchical Image-Text Misalignment**: åˆ©ç”¨å›¾åƒå’Œæ–‡æœ¬çš„ä¸å¯¹é½ï¼ˆç”Ÿæˆå›¾å¾€å¾€å›¾æ–‡ä¸ç¬¦ï¼‰æ¥æ£€æµ‹ Deepfakeã€‚\n*   **[Physics] 3. FeNN-DMA**: åŸºäº RISC-V çš„è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰åŠ é€Ÿå™¨ï¼Œç”¨äº FPGAã€‚\n*   **[Scientific] 80. Toward Automated Petrography**: å‘å¸ƒäº†å¤§è§„æ¨¡å²©çŸ³æ˜¾å¾®å›¾åƒæ•°æ®é›† LITHOSï¼ŒåŠ©åŠ›åœ°è´¨å­¦è‡ªåŠ¨åŒ–ã€‚\n\nâš ï¸ **æ³¨æ„**ï¼šè®ºæ–‡åˆ—è¡¨ä¸­çš„ç¬¬ 32 å’Œ 33 ç¯‡å…³äºäº¤é€šä¿¡å·æ§åˆ¶çš„è®ºæ–‡ï¼Œä½œè€…åœ¨ Abstract ä¸­è‡ªè¡Œæ ‡æ³¨äº†å­˜åœ¨â€œå…³é”®æ–¹æ³•è®ºé”™è¯¯ (Critical error in the methodology)â€ï¼Œå› æ­¤å»ºè®®ç›´æ¥è·³è¿‡ã€‚\n\nå¸Œæœ›ä»Šå¤©çš„å¿«æŠ¥å¯¹ä½ çš„ç ”ç©¶æœ‰æ‰€å¯å‘ï¼æˆ‘ä»¬æ˜å¤©è§ã€‚",
  "papers": [
    {
      "arxiv_id": "2511.00739v2",
      "title": "A CPU-Centric Perspective on Agentic AI",
      "title_zh": "ä»¥CPUä¸ºä¸­å¿ƒçš„æ™ºèƒ½ä½“AIè§†è§’",
      "authors": [
        "Ritik Raj",
        "Hong Wang",
        "Tushar Krishna"
      ],
      "abstract": "Agentic AI frameworks add a decision-making orchestrator embedded with external tools, including web search, Python interpreter, contextual database, and others, on top of monolithic LLMs, turning them from passive text oracles into autonomous problem-solvers that can plan, call tools, remember past steps, and adapt on the fly.\n  This paper aims to characterize and understand the system bottlenecks introduced by agentic AI workloads from a largely overlooked CPU-centric perspective. We first systematically characterize Agentic AI on the basis of orchestrator/decision making component, inference path dynamics and repetitiveness of the agentic flow which directly influences the system-level performance. Thereafter, based on the characterization, we choose five representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow, Langchain and SWE-Agent to profile latency, throughput and energy metrics and demystify the significant impact of CPUs on these metrics relative to GPUs. We observe that - 1. Tool processing on CPUs can take up to 90.6% of the total latency; 2. Agentic throughput gets bottlenecked either by CPU factors - coherence, synchronization and over-subscription of cores or GPU factors - main memory capacity and bandwidth; \\circled{3} CPU dynamic energy consumes up to 44% of the total dynamic energy at large batch sizes. Based on the profiling insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching (CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and heterogeneous agentic workloads respectively to demonstrate the potential to improve the performance, efficiency, and scalability of agentic AI. We achieve up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing benchmark for homogeneous and heterogeneous agentic workloads respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»å¸¸è¢«å¿½è§†çš„ä»¥ CPU ä¸ºä¸­å¿ƒçš„è§†è§’å‡ºå‘ï¼Œå¯¹ Agentic AI å·¥ä½œè´Ÿè½½çš„ç³»ç»Ÿç“¶é¢ˆè¿›è¡Œäº†ç³»ç»Ÿæ€§çš„è¡¨å¾ä¸åˆ†æã€‚ç ”ç©¶äººå‘˜é€šè¿‡åˆ†æ Orchestrator ç»„ä»¶ã€æ¨ç†è·¯å¾„åŠ¨æ€å’Œæµç¨‹é‡å¤æ€§ï¼Œå¯¹ Haystack RAGã€Toolformerã€ChemCrowã€Langchain å’Œ SWE-Agent äº”ç§ä»£è¡¨æ€§å·¥ä½œè´Ÿè½½è¿›è¡Œäº†å»¶è¿Ÿã€ååé‡å’Œèƒ½è€—å‰–æã€‚å®éªŒå‘ç°ï¼ŒCPU ç«¯çš„å·¥å…·å¤„ç†å¯èƒ½å æ®æ€»å»¶è¿Ÿçš„ 90.6%ï¼Œä¸”ç³»ç»Ÿååé‡å¸¸å—é™äº CPU çš„ä¸€è‡´æ€§ (Coherence)ã€åŒæ­¥ (Synchronization) åŠæ ¸å¿ƒè¶…é¢è®¢é˜… (Over-subscription) ç­‰å› ç´ ã€‚æ­¤å¤–ï¼Œåœ¨å¤§æ‰¹å¤„ç†è§„æ¨¡ä¸‹ï¼ŒCPU åŠ¨æ€èƒ½è€—å¯å æ®æ€»åŠ¨æ€èƒ½è€—çš„ 44%ï¼Œå‡¸æ˜¾äº† CPU åœ¨å¼‚æ„è®¡ç®—ä¸­çš„å…³é”®åœ°ä½ã€‚åŸºäºä¸Šè¿°æ´å¯Ÿï¼Œè®ºæ–‡æå‡ºäº† CPU and GPU-Aware Micro-batching (CGAM) å’Œ Mixed Agentic Workload Scheduling (MAWS) ä¸¤ç§ä¼˜åŒ–æ–¹æ¡ˆï¼Œä»¥æ”¹å–„ç³»ç»Ÿæ€§èƒ½ä¸èµ„æºåˆ©ç”¨ç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™äº›ä¼˜åŒ–ç­–ç•¥åœ¨ P50 å»¶è¿Ÿä¸Šåˆ†åˆ«å®ç°äº† 2.1 å€å’Œ 1.41 å€çš„åŠ é€Ÿï¼Œä¸ºæ„å»ºé«˜æ•ˆã€å¯æ‰©å±•çš„ Agentic AI ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00739v2",
      "published_date": "2025-11-01 23:46:44 UTC",
      "updated_date": "2025-11-29 15:45:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:06:12.939573+00:00"
    },
    {
      "arxiv_id": "2511.00737v1",
      "title": "EP-HDC: Hyperdimensional Computing with Encrypted Parameters for High-Throughput Privacy-Preserving Inference",
      "title_zh": "EP-HDCï¼šé‡‡ç”¨åŠ å¯†å‚æ•°çš„é«˜ååé‡éšç§ä¿æŠ¤æ¨ç†è¶…ç»´è®¡ç®—",
      "authors": [
        "Jaewoo Park",
        "Chenghao Quan",
        "Jongeun Lee"
      ],
      "abstract": "While homomorphic encryption (HE) provides strong privacy protection, its high computational cost has restricted its application to simple tasks. Recently, hyperdimensional computing (HDC) applied to HE has shown promising performance for privacy-preserving machine learning (PPML). However, when applied to more realistic scenarios such as batch inference, the HDC-based HE has still very high compute time as well as high encryption and data transmission overheads. To address this problem, we propose HDC with encrypted parameters (EP-HDC), which is a novel PPML approach featuring client-side HE, i.e., inference is performed on a client using a homomorphically encrypted model. Our EP-HDC can effectively mitigate the encryption and data transmission overhead, as well as providing high scalability with many clients while providing strong protection for user data and model parameters. In addition to application examples for our client-side PPML, we also present design space exploration involving quantization, architecture, and HE-related parameters. Our experimental results using the BFV scheme and the Face/Emotion datasets demonstrate that our method can improve throughput and latency of batch inference by orders of magnitude over previous PPML methods (36.52~1068x and 6.45~733x, respectively) with less than 1% accuracy degradation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EP-HDCï¼Œä¸€ç§åŸºäºåŠ å¯†å‚æ•°çš„è¶…ç»´è®¡ç®— (Hyperdimensional Computing) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŒæ€åŠ å¯† (Homomorphic Encryption) åœ¨éšç§ä¿æŠ¤æœºå™¨å­¦ä¹  (PPML) ä¸­é¢ä¸´çš„é«˜è®¡ç®—æˆæœ¬å’Œæ•°æ®ä¼ è¾“å¼€é”€é—®é¢˜ã€‚EP-HDC åˆ›æ–°æ€§åœ°é‡‡ç”¨å®¢æˆ·ç«¯åŒæ€åŠ å¯†æ–¹æ¡ˆï¼Œé€šè¿‡åœ¨å®¢æˆ·ç«¯ä½¿ç”¨åŠ å¯†åçš„æ¨¡å‹æ‰§è¡Œæ¨ç†ï¼Œæœ‰æ•ˆç¼“è§£äº†æ‰¹é‡æ¨ç†åœºæ™¯ä¸‹çš„æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶å®ç°äº†å¯¹ç”¨æˆ·æ•°æ®å’Œæ¨¡å‹å‚æ•°çš„åŒé‡ä¿æŠ¤ã€‚ç ”ç©¶å›¢é˜Ÿé’ˆå¯¹é‡åŒ– (Quantization)ã€æ¶æ„åŠç›¸å…³ HE å‚æ•°è¿›è¡Œäº†æ·±å…¥çš„è®¾è®¡ç©ºé—´æ¢ç´¢ï¼Œä»¥ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ BFV æ–¹æ¡ˆå’Œ Face/Emotion æ•°æ®é›†çš„æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®ç‡ä¸‹é™ä¸è¶³ 1% çš„æƒ…å†µä¸‹ï¼Œå°†æ‰¹é‡æ¨ç†çš„ååé‡å’Œå»¶è¿Ÿåˆ†åˆ«æå‡äº† 36.52 è‡³ 1068 å€å’Œ 6.45 è‡³ 733 å€ã€‚è¿™ä¸€æˆæœä¸ºå®ç°é«˜ååé‡ã€ä½å»¶è¿Ÿä¸”å…·å¤‡é«˜æ‰©å±•æ€§çš„éšç§ä¿æŠ¤æ¨ç†ä»»åŠ¡æä¾›äº†å…¨æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "To appear on ASP-DAC 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.00737v1",
      "published_date": "2025-11-01 23:22:01 UTC",
      "updated_date": "2025-11-01 23:22:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:06:10.419553+00:00"
    },
    {
      "arxiv_id": "2511.00732v1",
      "title": "FeNN-DMA: A RISC-V SoC for SNN acceleration",
      "title_zh": "FeNN-DMAï¼šé¢å‘è„‰å†²ç¥ç»ç½‘ç»œåŠ é€Ÿçš„ RISC-V ç³»ç»Ÿçº§èŠ¯ç‰‡",
      "authors": [
        "Zainab Aizaz",
        "James C. Knight",
        "Thomas Nowotny"
      ],
      "abstract": "Spiking Neural Networks (SNNs) are a promising, energy-efficient alternative to standard Artificial Neural Networks (ANNs) and are particularly well-suited to spatio-temporal tasks such as keyword spotting and video classification. However, SNNs have a much lower arithmetic intensity than ANNs and are therefore not well-matched to standard accelerators like GPUs and TPUs. Field Programmable Gate Arrays(FPGAs) are designed for such memory-bound workloads and here we develop a novel, fully-programmable RISC-V-based system-on-chip (FeNN-DMA), tailored to simulating SNNs on modern UltraScale+ FPGAs. We show that FeNN-DMA has comparable resource usage and energy requirements to state-of-the-art fixed-function SNN accelerators, yet it is capable of simulating much larger and more complex models. Using this functionality, we demonstrate state-of-the-art classification accuracy on the Spiking Heidelberg Digits and Neuromorphic MNIST tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†FeNN-DMAï¼Œä¸€ç§åŸºäºRISC-Vçš„å®Œå…¨å¯ç¼–ç¨‹ç‰‡ä¸Šç³»ç»Ÿ(SoC)ï¼Œä¸“é—¨ç”¨äºåœ¨ç°ä»£UltraScale+ FPGAä¸ŠåŠ é€Ÿè„‰å†²ç¥ç»ç½‘ç»œ(SNNs)çš„ä»¿çœŸã€‚é’ˆå¯¹SNNsè¾ƒä½çš„ç®—æœ¯å¼ºåº¦ä»¥åŠä¼ ç»ŸGPUå’ŒTPUéš¾ä»¥é«˜æ•ˆå¤„ç†çš„å†…å­˜å—é™(memory-bound)å·¥ä½œè´Ÿè½½ï¼ŒFeNN-DMAé€šè¿‡å®šåˆ¶åŒ–è®¾è®¡æä¾›äº†çµæ´»çš„ç¡¬ä»¶æ”¯æŒã€‚å®éªŒè¡¨æ˜ï¼ŒFeNN-DMAåœ¨èµ„æºå ç”¨å’Œèƒ½æ•ˆæ–¹é¢ä¸ç›®å‰æœ€å…ˆè¿›çš„å›ºå®šåŠŸèƒ½SNNåŠ é€Ÿå™¨ç›¸å½“ï¼Œä½†å…·å¤‡æ¨¡æ‹Ÿæ›´å¤§ã€æ›´å¤æ‚æ¨¡å‹çš„èƒ½åŠ›ã€‚è¯¥ç³»ç»Ÿåœ¨Spiking Heidelberg Digitså’ŒNeuromorphic MNISTä»»åŠ¡ä¸­å‡å®ç°äº†æœ€å…ˆè¿›çš„(state-of-the-art)åˆ†ç±»å‡†ç¡®ç‡ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤„ç†æ—¶ç©ºä»»åŠ¡æä¾›äº†ä¸€ç§å…¼å…·é«˜èƒ½æ•ˆä¸å¯ç¼–ç¨‹æ€§çš„ç¡¬ä»¶è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00732v1",
      "published_date": "2025-11-01 22:59:54 UTC",
      "updated_date": "2025-11-01 22:59:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:06:13.965606+00:00"
    },
    {
      "arxiv_id": "2511.01921v1",
      "title": "Fibbinary-Based Compression and Quantization for Efficient Neural Radio Receivers",
      "title_zh": "é¢å‘é«˜æ•ˆç¥ç»æ— çº¿ç”µæ¥æ”¶æœºçš„åŸºäº Fibbinary çš„å‹ç¼©ä¸é‡åŒ–",
      "authors": [
        "Roberta Fiandaca",
        "Manil Dev Gomony"
      ],
      "abstract": "Neural receivers have shown outstanding performance compared to the conventional ones but this comes with a high network complexity leading to a heavy computational cost. This poses significant challenges in their deployment on hardware-constrained devices. To address the issue, this paper explores two optimization strategies: quantization and compression. We introduce both uniform and non-uniform quantization such as the Fibonacci Code word Quantization (FCQ). A novel fine-grained approach to the Incremental Network Quantization (INQ) strategy is then proposed to compensate for the losses introduced by the above mentioned quantization techniques. Additionally, we introduce two novel lossless compression algorithms that effectively reduce the memory size by compressing sequences of Fibonacci quantized parameters characterized by a huge redundancy. The quantization technique provides a saving of 45\\% and 44\\% in the multiplier's power and area, respectively, and its combination with the compression determines a 63.4\\% reduction in memory footprint, while still providing higher performances than a conventional receiver.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Neural receiversåœ¨ç¡¬ä»¶å—é™è®¾å¤‡ä¸Šéƒ¨ç½²é¢ä¸´çš„é«˜è®¡ç®—æˆæœ¬å’Œå¤æ‚åº¦é—®é¢˜ï¼Œæ¢ç´¢äº†é‡åŒ–å’Œå‹ç¼©ä¸¤ç§ä¼˜åŒ–ç­–ç•¥ã€‚è®ºæ–‡å¼•å…¥äº†åŒ…æ‹¬Fibonacci Code word Quantization (FCQ)åœ¨å†…çš„å‡åŒ€å’Œéå‡åŒ€é‡åŒ–æ–¹æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç»†ç²’åº¦Incremental Network Quantization (INQ)ç­–ç•¥æ¥è¡¥å¿é‡åŒ–å¸¦æ¥çš„æŸå¤±ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…å¼€å‘äº†ä¸¤ç§æ— æŸå‹ç¼©ç®—æ³•ï¼Œé€šè¿‡å‹ç¼©å…·æœ‰å·¨å¤§å†—ä½™çš„Fibonaccié‡åŒ–å‚æ•°åºåˆ—æ¥æœ‰æ•ˆå‡å°‘å†…å­˜å ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥é‡åŒ–æŠ€æœ¯ä½¿ä¹˜æ³•å™¨çš„åŠŸè€—å’Œé¢ç§¯åˆ†åˆ«èŠ‚çœäº†45%å’Œ44%ï¼Œç»“åˆå‹ç¼©æŠ€æœ¯åå†…å­˜å ç”¨å‡å°‘äº†63.4%ï¼ŒåŒæ—¶ä»èƒ½æä¾›ä¼˜äºä¼ ç»Ÿæ¥æ”¶å™¨çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.IT",
        "cs.AI"
      ],
      "primary_category": "cs.IT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01921v1",
      "published_date": "2025-11-01 22:39:44 UTC",
      "updated_date": "2025-11-01 22:39:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 1,
      "last_update": "2026-01-25T07:07:17.204451+00:00"
    },
    {
      "arxiv_id": "2511.00711v1",
      "title": "TRISKELION-1: Unified Descriptive-Predictive-Generative AI",
      "title_zh": "TRISKELION-1ï¼šæè¿°-é¢„æµ‹-ç”Ÿæˆä¸€ä½“åŒ–äººå·¥æ™ºèƒ½",
      "authors": [
        "Nardeep Kumar",
        "Arun Kanwar"
      ],
      "abstract": "TRISKELION-1 is a unified descriptive-predictive-generative architecture that integrates statistical, mechanistic, and generative reasoning within a single encoder-decoder framework. The model demonstrates how descriptive representation learning, predictive inference, and generative synthesis can be jointly optimized using variational objectives. Experiments on MNIST validate that descriptive reconstruction, predictive classification, and generative sampling can coexist stably within one model. The framework provides a blueprint toward universal intelligence architectures that connect interpretability, accuracy, and creativity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TRISKELION-1ï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„ descriptive-predictive-generative æ¶æ„ï¼Œåœ¨å•ä¸ª encoder-decoder æ¡†æ¶å†…é›†æˆäº†ç»Ÿè®¡ã€æœºåˆ¶å’Œç”Ÿæˆæ¨ç†ã€‚è¯¥æ¨¡å‹é€šè¿‡ variational objectives å®ç°äº†æè¿°æ€§è¡¨ç¤ºå­¦ä¹  (descriptive representation learning)ã€é¢„æµ‹æ€§æ¨ç† (predictive inference) å’Œç”Ÿæˆå¼åˆæˆ (generative synthesis) çš„å…±åŒä¼˜åŒ–ã€‚åœ¨ MNIST æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œæè¿°æ€§é‡æ„ã€é¢„æµ‹æ€§åˆ†ç±»å’Œç”Ÿæˆå¼é‡‡æ ·èƒ½å¤Ÿåœ¨å•ä¸€æ¨¡å‹ä¸­ç¨³å®šå…±å­˜ã€‚è¯¥æ¡†æ¶ä¸ºæ„å»ºèåˆå¯è§£é‡Šæ€§ã€å‡†ç¡®æ€§å’Œåˆ›é€ åŠ›çš„é€šç”¨æ™ºèƒ½æ¶æ„ (universal intelligence architectures) æä¾›äº†é‡è¦è“å›¾ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 18 figures, submitted to arXiv (2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.00711v1",
      "published_date": "2025-11-01 21:23:38 UTC",
      "updated_date": "2025-11-01 21:23:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:06:18.446112+00:00"
    },
    {
      "arxiv_id": "2511.00710v3",
      "title": "Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries",
      "title_zh": "Ariadneï¼šä¸€ç§ç”¨äºæ¢ç©¶ä¸æ‹“å±• VLM æ¨ç†è¾¹ç•Œçš„å¯æ§æ¡†æ¶",
      "authors": [
        "Minghe Shen",
        "Zhuo Zhi",
        "Chonghan Liu",
        "Shuo Xing",
        "Zhengzhong Tu",
        "Che Liu"
      ],
      "abstract": "While Vision-Language Models (VLMs) post-trained with Reinforcement Learning (RL) show impressive general reasoning, their evaluation is often confined to language-dominant tasks (e.g., math). This raises a critical question: can RL post-training truly extend the inherent capability boundary of a base VLM, particularly for visual-centric spatial tasks where it initially fails? To investigate this, we introduce Ariadne, a framework utilizing synthetic mazes for multi-step spatial reasoning where task difficulty (e.g., path length, turns) is precisely controlled. We leverage this controllable environment to train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves over 50% accuracy on a problem set where the base model scored 0%, demonstrating that our approach expands the model's initial capability boundary. To assess real-world viability, we evaluate out-of-distribution (OOD) generalization on practical benchmarks. Despite training only on synthetic maze samples, Ariadne achieves significant zero-shot improvements, averaging 16% on MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer tasks). These results confirm that our method not only broadens the model's fundamental limits but also enhances its generalization to real-world spatial reasoning. We acknowledge our study is limited to the post-training phase, given the opaqueness of pre-training data, and hope our research motivates further work on specialized, capability-extending alignment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Ariadneï¼Œä¸€ä¸ªåˆ©ç”¨åˆæˆè¿·å®«(synthetic mazes)è¿›è¡Œå¤šæ­¥ç©ºé—´æ¨ç†çš„å¯æ§æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ¢ç©¶å¹¶æ‰©å±•è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨è§†è§‰é‡å¿ƒç©ºé—´ä»»åŠ¡ä¸­çš„æ¨ç†è¾¹ç•Œã€‚è¯¥æ¡†æ¶é€šè¿‡ç²¾ç¡®æ§åˆ¶ä»»åŠ¡éš¾åº¦ï¼Œå¹¶é‡‡ç”¨å¸¦æœ‰éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RLVR)ç»“åˆéš¾åº¦æ„ŸçŸ¥è¯¾ç¨‹(difficulty-aware curriculum)å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡ RLVR è®­ç»ƒåçš„ VLM åœ¨åŸºç¡€æ¨¡å‹å¾—åˆ†ä¸º 0% çš„æŒ‘æˆ˜æ€§é—®é¢˜é›†ä¸Šå®ç°äº†è¶…è¿‡ 50% çš„å‡†ç¡®ç‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•èƒ½æ˜¾è‘—æ‰©å±•æ¨¡å‹çš„åˆå§‹èƒ½åŠ›è¾¹ç•Œã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜éªŒè¯äº†æ¨¡å‹çš„åˆ†å¸ƒå¤–(OOD)æ³›åŒ–èƒ½åŠ›ï¼Œå³ä½¿ä»…åœ¨åˆæˆè¿·å®«ä¸­è®­ç»ƒï¼ŒAriadne åœ¨ MapBench å’Œ ReasonMap ç­‰ç°å®ä¸–ç•Œç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿåˆ†åˆ«å®ç°äº† 16% å’Œ 24% çš„å¹³å‡é›¶æ ·æœ¬(zero-shot)æ€§èƒ½æå‡ã€‚è¿™ä¸€æˆæœä¸ä»…æ‹“å®½äº†æ¨¡å‹çš„åŸºç¡€èƒ½åŠ›æé™ï¼Œè¿˜å¢å¼ºäº†å…¶å¤„ç†ç°å®ä¸–ç•Œå¤æ‚å¯¼èˆªå’Œè·¯å¾„è§„åˆ’ä»»åŠ¡çš„é€šç”¨æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00710v3",
      "published_date": "2025-11-01 21:19:41 UTC",
      "updated_date": "2025-11-11 19:06:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:06:25.449287+00:00"
    },
    {
      "arxiv_id": "2511.00709v2",
      "title": "A Voice-Enabled Virtual Patient System for Interactive Training in Standardized Clinical Assessment",
      "title_zh": "é¢å‘æ ‡å‡†åŒ–ä¸´åºŠè¯„ä¼°äº¤äº’å¼åŸ¹è®­çš„è¯­éŸ³äº¤äº’è™šæ‹Ÿç—…äººç³»ç»Ÿ",
      "authors": [
        "Veronica Bossio Botero",
        "Vijay Yadav",
        "Jacob Ouyang",
        "Anzar Abbas",
        "Michelle Worthington"
      ],
      "abstract": "Training mental health clinicians to conduct standardized clinical assessments is challenging due to a lack of scalable, realistic practice opportunities, which can impact data quality in clinical trials. To address this gap, we introduce a voice-enabled virtual patient simulation system powered by a large language model (LLM). This study describes the system's development and validates its ability to generate virtual patients who accurately adhere to pre-defined clinical profiles, maintain coherent narratives, and produce realistic dialogue. We implemented a system using a LLM to simulate patients with specified symptom profiles, demographics, and communication styles. The system was evaluated by 5 experienced clinical raters who conducted 20 simulated structured MADRS interviews across 4 virtual patient personas. The virtual patients demonstrated strong adherence to their clinical profiles, with a mean item difference between rater-assigned MADRS scores and configured scores of 0.52 (SD=0.75). Inter-rater reliability across items was 0.90 (95% CI=0.68-0.99). Expert raters consistently rated the qualitative realism and cohesiveness of the virtual patients favorably, giving average ratings between \"Agree\" and \"Strongly Agree.\" Our findings suggest that LLM-powered virtual patient simulations are a viable and scalable tool for training clinicians, capable of producing high-fidelity, clinically relevant practice scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ”¯æŒè¯­éŸ³çš„è™šæ‹Ÿç—…äººç³»ç»Ÿï¼Œæ—¨åœ¨ä¸ºå¿ƒç†å¥åº·ä¸´åºŠåŒ»ç”Ÿæä¾›å¯æ‰©å±•ä¸”çœŸå®çš„æ ‡å‡†åŒ–ä¸´åºŠè¯„ä¼°äº¤äº’åŸ¹è®­ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLM) æ¨¡æ‹Ÿå…·æœ‰ç‰¹å®šç—‡çŠ¶ç‰¹å¾ã€äººå£ç»Ÿè®¡å­¦èƒŒæ™¯å’Œæ²Ÿé€šé£æ ¼çš„è™šæ‹Ÿç—…äººï¼Œä»¥è§£å†³ä¼ ç»ŸåŸ¹è®­æ¨¡å¼ä¸­å®è·µæœºä¼šä¸è¶³çš„é—®é¢˜ã€‚é€šè¿‡5åä¸´åºŠä¸“å®¶å¯¹4ç§è™šæ‹Ÿç—…äººè§’è‰²è¿›è¡Œçš„20æ¬¡è’™å“¥é©¬åˆ©-å¥¥æ–¯ä¼¯æ ¼æŠ‘éƒé‡è¡¨ (MADRS) æ¨¡æ‹Ÿè®¿è°ˆï¼Œç ”ç©¶éªŒè¯äº†ç³»ç»Ÿåœ¨ä¸´åºŠå‰–é¢éµå¾ªåº¦å’Œå™äº‹è¿è´¯æ€§æ–¹é¢çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸“å®¶è¯„åˆ†ä¸é¢„è®¾è¯„åˆ†ä¹‹é—´çš„å¹³å‡é¡¹ç›®å·®å¼‚ä»…ä¸º0.52ï¼Œä¸”è·¨é¡¹ç›®è¯„åˆ†è€…é—´ä¿¡åº¦ (Inter-rater reliability) é«˜è¾¾0.90ã€‚ä¸“å®¶ä»¬åœ¨å®šæ€§çœŸå®æ„Ÿå’Œè¿è´¯æ€§æ–¹é¢å‡ç»™å‡ºäº†æé«˜çš„è¯„ä»·ï¼Œè¯æ˜äº†è¯¥ç³»ç»Ÿåœ¨ç”Ÿæˆé«˜ä¿çœŸã€å…·ä¸´åºŠç›¸å…³æ€§åŸ¹è®­åœºæ™¯æ–¹é¢çš„å¯è¡Œæ€§ã€‚è¿™é¡¹ç ”ç©¶è¡¨æ˜åŸºäº LLM çš„è™šæ‹Ÿç—…äººæ¨¡æ‹Ÿæ˜¯æå‡ä¸´åºŠåŒ»ç”Ÿè¯„ä¼°æŠ€èƒ½çš„ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„å·¥å…·ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00709v2",
      "published_date": "2025-11-01 21:18:08 UTC",
      "updated_date": "2025-12-28 18:08:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:06:47.465003+00:00"
    },
    {
      "arxiv_id": "2511.07437v1",
      "title": "Agentic Educational Content Generation for African Languages on Edge Devices",
      "title_zh": "é¢å‘è¾¹ç¼˜è®¾å¤‡çš„éæ´²è¯­è¨€æ™ºèƒ½ä½“æ•™è‚²å†…å®¹ç”Ÿæˆ",
      "authors": [
        "Ravi Gupta",
        "Guneet Bhatia"
      ],
      "abstract": "Addressing educational inequity in Sub-Saharan Africa, this research presents an autonomous agent-orchestrated framework for decentralized, culturally adaptive educational content generation on edge devices. The system leverages four specialized agents that work together to generate contextually appropriate educational content. Experimental validation on platforms including Raspberry Pi 4B and NVIDIA Jetson Nano demonstrates significant performance achievements. InkubaLM on Jetson Nano achieved a Time-To-First-Token (TTFT) of 129 ms, an average inter-token latency of 33 ms, and a throughput of 45.2 tokens per second while consuming 8.4 W. On Raspberry Pi 4B, InkubaLM also led with 326 ms TTFT and 15.9 tokens per second at 5.8 W power consumption. The framework consistently delivered high multilingual quality, averaging a BLEU score of 0.688, cultural relevance of 4.4/5, and fluency of 4.2/5 across tested African languages. Through potential partnerships with active community organizations including African Youth & Community Organization (AYCO) and Florida Africa Foundation, this research aims to establish a practical foundation for accessible, localized, and sustainable AI-driven education in resource-constrained environments. Keeping focus on long-term viability and cultural appropriateness, it contributes to United Nations SDGs 4, 9, and 10. Index Terms - Multi-Agent Systems, Edge AI Computing, Educational Technology, African Languages, Rural Education, Sustainable Development, UN SDG.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ’’å“ˆæ‹‰ä»¥å—éæ´²åœ°åŒºçš„æ•™è‚²ä¸å¹³ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºè‡ªä¸»æ™ºèƒ½ä½“ç¼–æ’çš„æ¡†æ¶ï¼Œç”¨äºåœ¨è¾¹ç¼˜è®¾å¤‡(Edge Devices)ä¸Šç”Ÿæˆå»ä¸­å¿ƒåŒ–ä¸”å…·æœ‰æ–‡åŒ–é€‚åº”æ€§çš„æ•™è‚²å†…å®¹ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨å››ä¸ªä¸“é—¨çš„æ™ºèƒ½ä½“åä½œç”Ÿæˆè¯­å¢ƒç›¸å…³çš„æ•™å­¦ç´ æï¼Œå¹¶åœ¨ Raspberry Pi 4B å’Œ NVIDIA Jetson Nano å¹³å°ä¸Šå®Œæˆäº†å®éªŒéªŒè¯ã€‚å®éªŒè¡¨æ˜ï¼ŒInkubaLM æ¨¡å‹åœ¨ Jetson Nano ä¸Šè¾¾åˆ°äº† 129 ms çš„é¦–å­—å»¶è¿Ÿ(TTFT)å’Œæ¯ç§’ 45.2 ä¸ª token çš„ååé‡ï¼ŒåŒæ—¶ä¿æŒäº†æä½çš„åŠŸè€—ã€‚åœ¨å¤šè¯­è¨€è´¨é‡è¯„ä»·ä¸­ï¼Œè¯¥æ¡†æ¶åœ¨éæ´²è¯­è¨€æµ‹è¯•ä¸­å–å¾—äº† 0.688 çš„å¹³å‡ BLEU åˆ†æ•°ï¼Œä¸”åœ¨æ–‡åŒ–ç›¸å…³æ€§å’Œæµç•…åº¦è¯„åˆ†ä¸Šåˆ†åˆ«è·å¾— 4.4/5 å’Œ 4.2/5 çš„é«˜åˆ†ã€‚é€šè¿‡ä¸ç¤¾åŒºç»„ç»‡åˆä½œï¼Œè¯¥ç ”ç©¶ä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹å»ºç«‹å¯è®¿é—®ã€æœ¬åœŸåŒ–ä¸”å¯æŒç»­çš„ AI é©±åŠ¨æ•™è‚²å¥ å®šäº†å®è·µåŸºç¡€ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æå‡äº†æ•™è‚²å†…å®¹çš„äº§å‡ºæ•ˆç‡ï¼Œä¹Ÿç›´æ¥åŠ©åŠ›äºå®ç°è”åˆå›½å¯æŒç»­å‘å±•ç›®æ ‡(SDGs)ä¸­çš„æ•™è‚²å…¬å¹³ä¸æŠ€æœ¯åˆ›æ–°ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07437v1",
      "published_date": "2025-11-01 21:13:47 UTC",
      "updated_date": "2025-11-01 21:13:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:06:32.122676+00:00"
    },
    {
      "arxiv_id": "2511.04700v1",
      "title": "Separate the Wheat from the Chaff: Winnowing Down Divergent Views in Retrieval Augmented Generation",
      "title_zh": "å»èŠœå­˜èï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆä¸­åˆ†æ­§è§‚ç‚¹çš„ç­›é€‰ä¸æç‚¼",
      "authors": [
        "Song Wang",
        "Zihan Chen",
        "Peng Wang",
        "Zhepei Wei",
        "Zhen Tan",
        "Yu Meng",
        "Cong Shen",
        "Jundong Li"
      ],
      "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge sources to address their limitations in accessing up-to-date or specialized information. A natural strategy to increase the likelihood of retrieving relevant information is to expand the number of retrieved documents. However, involving more documents could introduce significant noise, as many documents may be irrelevant or misleading, thereby reducing the overall accuracy of the generated responses. To overcome the challenge associated with handling a larger number of documents, we propose WinnowRAG, a novel RAG framework designed to systematically filter out noisy documents while preserving valuable content -- a process we refer to as winnowing. WinnowRAG operates in two stages: In Stage I, we perform query-aware clustering to group similar documents and form distinct topic clusters. Each cluster is assigned to an LLM agent for generating a unique answer. In Stage II, we perform winnowing, wherein a critic LLM evaluates the outputs of multiple agents and iteratively separates useful documents from noisy ones. To retain useful documents when discarding agents, we propose two strategic merging techniques to ensure that only relevant knowledge is used for generating the final response. Crucially, WinnowRAG is model-agnostic and does not require any model fine-tuning, making it easily adaptable to various tasks. Extensive experiments on various realistic datasets demonstrate the effectiveness of WinnowRAG over state-of-the-art baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-augmented generation, RAG)åœ¨å¢åŠ æ£€ç´¢æ–‡æ¡£æ•°é‡æ—¶å®¹æ˜“å¼•å…¥å™ªå£°å¹¶é™ä½å‡†ç¡®æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºWinnowRAGçš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸¤ä¸ªé˜¶æ®µå®ç°æ–‡æ¡£çš„ç³»ç»Ÿæ€§ç­›é€‰ï¼šç¬¬ä¸€é˜¶æ®µåˆ©ç”¨æŸ¥è¯¢æ„ŸçŸ¥èšç±»(query-aware clustering)å°†ç›¸ä¼¼æ–‡æ¡£åˆ†ç»„ï¼Œå¹¶ç”±å¤šä¸ªLLM agentåˆ†åˆ«ç”Ÿæˆç‹¬ç«‹å›ç­”ï¼›ç¬¬äºŒé˜¶æ®µåˆ™ç”±æ‰¹è¯„è€…æ¨¡å‹(critic LLM)è¯„ä¼°å„æ™ºèƒ½ä½“çš„è¾“å‡ºï¼Œé€šè¿‡è¿­ä»£çš„ç­›é€‰è¿‡ç¨‹(winnowing)ä»å™ªå£°ä¸­åˆ†ç¦»å‡ºæœ‰ç”¨æ–‡æ¡£ã€‚ä¸ºäº†åœ¨èˆå¼ƒæ— æ•ˆæ™ºèƒ½ä½“çš„åŒæ—¶ä¿ç•™ä»·å€¼ä¿¡æ¯ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†ä¸¤ç§åˆå¹¶æŠ€æœ¯(merging techniques)ä»¥ç¡®ä¿æœ€ç»ˆå›å¤çš„è´¨é‡ã€‚WinnowRAGå…·æœ‰æ¨¡å‹æ— å…³æ€§(model-agnostic)ä¸”æ— éœ€è¿›è¡Œæ¨¡å‹å¾®è°ƒ(fine-tuning)ï¼Œè¡¨ç°å‡ºæé«˜çš„ä»»åŠ¡é€‚åº”æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒWinnowRAGåœ¨å¤„ç†å¤æ‚ç°å®æ•°æ®é›†æ—¶çš„è¡¨ç°ä¼˜äºç°æœ‰çš„state-of-the-artåŸºå‡†æ¨¡å‹ï¼Œæœ‰æ•ˆè§£å†³äº†å¤§è§„æ¨¡æ£€ç´¢å¸¦æ¥çš„å™ªå£°æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP Main 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.04700v1",
      "published_date": "2025-11-01 20:08:13 UTC",
      "updated_date": "2025-11-01 20:08:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:06:35.924683+00:00"
    },
    {
      "arxiv_id": "2511.00686v1",
      "title": "Evolve to Inspire: Novelty Search for Diverse Image Generation",
      "title_zh": "æ¼”åŒ–å¯è¿ªï¼šé¢å‘å¤šæ ·åŒ–å›¾åƒç”Ÿæˆçš„æ–°é¢–æ€§æœç´¢",
      "authors": [
        "Alex Inch",
        "Passawis Chaiyapattanaporn",
        "Yuchen Zhu",
        "Yuan Lu",
        "Ting-Wen Ko",
        "Davide Paglieri"
      ],
      "abstract": "Text-to-image diffusion models, while proficient at generating high-fidelity images, often suffer from limited output diversity, hindering their application in exploratory and ideation tasks. Existing prompt optimization techniques typically target aesthetic fitness or are ill-suited to the creative visual domain. To address this shortcoming, we introduce WANDER, a novelty search-based approach to generating diverse sets of images from a single input prompt. WANDER operates directly on natural language prompts, employing a Large Language Model (LLM) for semantic evolution of diverse sets of images, and using CLIP embeddings to quantify novelty. We additionally apply emitters to guide the search into distinct regions of the prompt space, and demonstrate that they boost the diversity of the generated images. Empirical evaluations using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that WANDER significantly outperforms existing evolutionary prompt optimization baselines in diversity metrics. Ablation studies confirm the efficacy of emitters.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬ç”Ÿæˆå›¾åƒ(Text-to-image)æ‰©æ•£æ¨¡å‹åœ¨å¤šæ ·æ€§æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†åä¸ºWANDERçš„æ–°å‹æœç´¢æ–¹æ³•ã€‚WANDERé‡‡ç”¨åŸºäºæ–°é¢–æ€§æœç´¢(Novelty Search)çš„ç­–ç•¥ï¼Œç›´æ¥ä½œç”¨äºè‡ªç„¶è¯­è¨€æç¤ºè¯ï¼Œå¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)è¿›è¡Œæç¤ºè¯çš„è¯­ä¹‰æ¼”åŒ–ã€‚è¯¥æ–¹æ³•ç»“åˆCLIPåµŒå…¥å‘é‡æ¥é‡åŒ–ç”Ÿæˆå›¾åƒçš„æ–°é¢–ç¨‹åº¦ï¼Œå¹¶å¼•å…¥å‘å°„å™¨(emitters)å¼•å¯¼æœç´¢è¿›å…¥æç¤ºç©ºé—´çš„ä¸åŒåŒºåŸŸï¼Œä»è€Œæ˜¾è‘—æå‡ç”Ÿæˆç»“æœçš„å¤šæ ·æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä½¿ç”¨FLUX-DEVç”Ÿæˆå’ŒGPT-4o-miniå˜å¼‚çš„è®¾ç½®ä¸‹ï¼ŒWANDERåœ¨å¤šæ ·æ€§æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„è¿›åŒ–æç¤ºä¼˜åŒ–åŸºå‡†ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®äº†å‘å°„å™¨åœ¨å¢å¼ºå›¾åƒå¤šæ ·æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ¢ç´¢æ€§è®¾è®¡å’Œæ„æ€ä»»åŠ¡æä¾›äº†æ›´å¼ºçš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 10 figures, Accepted to Neurips 2025 GenProCC Workshop",
      "pdf_url": "https://arxiv.org/pdf/2511.00686v1",
      "published_date": "2025-11-01 19:58:07 UTC",
      "updated_date": "2025-11-01 19:58:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:06:38.512976+00:00"
    },
    {
      "arxiv_id": "2511.00681v1",
      "title": "Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control",
      "title_zh": "ç”¨äºå¯¹æ¯”åº¦ç†è§£ä¸è´¨é‡æ§åˆ¶çš„å…ƒæ•°æ®å¯¹é½ 3D MRI è¡¨å¾",
      "authors": [
        "Mehmet Yigit Avci",
        "Pedro Borges",
        "Virginia Fernandez",
        "Paul Wright",
        "Mehmet Yigitsoy",
        "Sebastien Ourselin",
        "Jorge Cardoso"
      ],
      "abstract": "Magnetic Resonance Imaging suffers from substantial data heterogeneity and the absence of standardized contrast labels across scanners, protocols, and institutions, which severely limits large-scale automated analysis. A unified representation of MRI contrast would enable a wide range of downstream utilities, from automatic sequence recognition to harmonization and quality control, without relying on manual annotations. To this end, we introduce MR-CLIP, a metadata-guided framework that learns MRI contrast representations by aligning volumetric images with their DICOM acquisition parameters. The resulting embeddings shows distinct clusters of MRI sequences and outperform supervised 3D baselines under data scarcity in few-shot sequence classification. Moreover, MR-CLIP enables unsupervised data quality control by identifying corrupted or inconsistent metadata through image-metadata embedding distances. By transforming routinely available acquisition metadata into a supervisory signal, MR-CLIP provides a scalable foundation for label-efficient MRI analysis across diverse clinical datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç£å…±æŒ¯æˆåƒ (MRI) æ•°æ®çš„é«˜åº¦å¼‚è´¨æ€§ä»¥åŠå„æœºæ„é—´ç¼ºä¹æ ‡å‡†å¯¹æ¯”åº¦æ ‡ç­¾ (Contrast Labels) çš„é—®é¢˜ï¼Œæå‡ºäº† MR-CLIP æ¡†æ¶ã€‚MR-CLIP æ˜¯ä¸€ç§å…ƒæ•°æ®å¼•å¯¼ (Metadata-guided) çš„å­¦ä¹ æ–¹æ¡ˆï¼Œé€šè¿‡å°†ä½“ç§¯å›¾åƒä¸ DICOM é‡‡é›†å‚æ•°è¿›è¡Œå¯¹é½æ¥å­¦ä¹  MRI å¯¹æ¯”åº¦è¡¨å¾ã€‚ç ”ç©¶å‘ç°ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„åµŒå…¥ (Embeddings) å±•ç°å‡ºæ¸…æ™°çš„åºåˆ—èšç±»ï¼Œå¹¶åœ¨å°‘æ ·æœ¬åºåˆ—åˆ†ç±» (Few-shot Sequence Classification) ä»»åŠ¡ä¸­ä¼˜äºä¼ ç»Ÿçš„æœ‰ç›‘ç£ 3D åŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒMR-CLIP èƒ½å¤Ÿé€šè¿‡è®¡ç®—å›¾åƒä¸å…ƒæ•°æ®é—´çš„åµŒå…¥è·ç¦»ï¼Œæœ‰æ•ˆè¯†åˆ«æŸåæˆ–ä¸ä¸€è‡´çš„å…ƒæ•°æ®ï¼Œä»è€Œå®ç°æ— ç›‘ç£çš„æ•°æ®è´¨é‡æ§åˆ¶ (Quality Control)ã€‚é€šè¿‡å°†å¸¸è§„è·å–çš„å…ƒæ•°æ®è½¬åŒ–ä¸ºç›‘ç£ä¿¡å·ï¼Œè¯¥ç ”ç©¶ä¸ºè·¨å¤šä¸´åºŠæ•°æ®é›†çš„æ ‡ç­¾é«˜æ•ˆå‹ MRI åˆ†ææä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„åŸºç¡€æ¶æ„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00681v1",
      "published_date": "2025-11-01 19:49:32 UTC",
      "updated_date": "2025-11-01 19:49:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:07:28.809998+00:00"
    },
    {
      "arxiv_id": "2511.00674v1",
      "title": "Isotropic Curvature Model for Understanding Deep Learning Optimization: Is Gradient Orthogonalization Optimal?",
      "title_zh": "ç†è§£æ·±åº¦å­¦ä¹ ä¼˜åŒ–çš„å„å‘åŒæ€§æ›²ç‡æ¨¡å‹ï¼šæ¢¯åº¦æ­£äº¤åŒ–æ˜¯å¦æœ€ä¼˜ï¼Ÿ",
      "authors": [
        "Weijie Su"
      ],
      "abstract": "In this paper, we introduce a model for analyzing deep learning optimization over a single iteration by leveraging the matrix structure of the weights. We derive the model by assuming isotropy of curvature, including the second-order Hessian and higher-order terms, of the loss function across all perturbation directions; hence, we call it the isotropic curvature model. This model is a convex optimization program amenable to analysis, which allows us to understand how an update on the weights in the form of a matrix relates to the change in the total loss function. As an application, we use the isotropic curvature model to analyze the recently introduced Muon optimizer and other matrix-gradient methods for training language models. First, we show that under a general growth condition on the curvature, the optimal update matrix is obtained by making the spectrum of the original gradient matrix more homogeneous -- that is, making its singular values closer in ratio -- which in particular improves the conditioning of the update matrix. Next, we show that the orthogonalized gradient becomes optimal for the isotropic curvature model when the curvature exhibits a phase transition in growth. Taken together, these results suggest that the gradient orthogonalization employed in Muon and other related methods is directionally correct but may not be strictly optimal. Finally, we discuss future research on how to leverage the isotropic curvature model for designing new optimization methods for training deep learning and language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å„å‘åŒæ€§æ›²ç‡æ¨¡å‹ (Isotropic Curvature Model)ï¼Œé€šè¿‡åˆ©ç”¨æƒé‡çŸ©é˜µç»“æ„å’Œå‡è®¾æŸå¤±å‡½æ•°åœ¨æ‰€æœ‰æ‰°åŠ¨æ–¹å‘ä¸Šçš„æ›²ç‡ï¼ˆåŒ…æ‹¬ Hessian å’Œé«˜é˜¶é¡¹ï¼‰å…·æœ‰å„å‘åŒæ€§ï¼Œæ¥åˆ†ææ·±åº¦å­¦ä¹ ä¼˜åŒ–è¿‡ç¨‹ã€‚è¯¥æ¨¡å‹è¢«æ„å»ºä¸ºå¯åˆ†æçš„å‡¸ä¼˜åŒ–ç¨‹åºï¼Œç”¨äºæ­ç¤ºçŸ©é˜µå½¢å¼çš„æƒé‡æ›´æ–°ä¸æ€»æŸå¤±å‡½æ•°å˜åŒ–ä¹‹é—´çš„å…³ç³»ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨è¯¥æ¨¡å‹åˆ†æäº† Muon ä¼˜åŒ–å™¨åŠå…¶ä»–ç”¨äºè¯­è¨€æ¨¡å‹è®­ç»ƒçš„çŸ©é˜µæ¢¯åº¦æ–¹æ³• (matrix-gradient methods)ã€‚åœ¨æ›²ç‡çš„ä¸€èˆ¬å¢é•¿æ¡ä»¶ä¸‹ï¼Œç ”ç©¶è¯æ˜æœ€ä¼˜æ›´æ–°çŸ©é˜µåº”ä½¿æ¢¯åº¦çŸ©é˜µçš„å…‰è°±æ›´å‡åŒ€ï¼Œå³è®©å¥‡å¼‚å€¼æ¯”ä¾‹æ›´æ¥è¿‘ï¼Œä»è€Œæ”¹å–„æ›´æ–°çŸ©é˜µçš„æ¡ä»¶æ•° (conditioning)ã€‚å½“æ›²ç‡åœ¨å¢é•¿ä¸­è¡¨ç°å‡ºç›¸å˜æ—¶ï¼Œæ­£äº¤åŒ–æ¢¯åº¦ (orthogonalized gradient) åœ¨è¯¥æ¨¡å‹ä¸‹å˜ä¸ºæœ€ä¼˜è§£ã€‚è¿™äº›ç»“æœå…±åŒè¡¨æ˜ Muon ç­‰æ–¹æ³•é‡‡ç”¨çš„æ¢¯åº¦æ­£äº¤åŒ–åœ¨æ–¹å‘ä¸Šæ˜¯æ­£ç¡®çš„ï¼Œä½†å¯èƒ½å¹¶éä¸¥æ ¼æœ€ä¼˜ã€‚è¯¥å·¥ä½œæœ€åæ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å„å‘åŒæ€§æ›²ç‡æ¨¡å‹ä¸ºæ·±åº¦å­¦ä¹ å’Œè¯­è¨€æ¨¡å‹è®­ç»ƒè®¾è®¡æ–°çš„ä¼˜åŒ–ç®—æ³•ã€‚",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "math.OC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00674v1",
      "published_date": "2025-11-01 19:37:29 UTC",
      "updated_date": "2025-11-01 19:37:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:07:35.746684+00:00"
    },
    {
      "arxiv_id": "2511.00673v1",
      "title": "Lifted Successor Generation in Numeric Planning",
      "title_zh": "æ•°å€¼è§„åˆ’ä¸­çš„æå‡å¼åç»§ç”Ÿæˆ",
      "authors": [
        "Dominik Drexler"
      ],
      "abstract": "Most planners ground numeric planning tasks, given in a first-order-like language, into a ground task representation. However, this can lead to an exponential blowup in task representation size, which occurs in practice for hard-to-ground tasks. We extend a state-of-the-art lifted successor generator for classical planning to support numeric precondition applicability. The method enumerates maximum cliques in a substitution consistency graph. Each maximum clique represents a substitution for the variables of the action schema, yielding a ground action. We augment this graph with numeric action preconditions and prove the successor generator is exact under formally specified conditions. When the conditions fail, our generator may list inapplicable ground actions; a final applicability check filters these without affecting completeness. However, this cannot happen in 23 of 25 benchmark domains, and it occurs only in 1 domain. To the authors' knowledge, no other lifted successor generator supports numeric action preconditions. This enables future research on lifted planning for a very rich planning fragment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ•°å€¼è§„åˆ’ (Numeric Planning) ä¸­ä¼ ç»Ÿ grounding æ–¹æ³•å¯èƒ½å¯¼è‡´çš„è¡¨ç¤ºè§„æ¨¡æŒ‡æ•°çº§è†¨èƒ€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æå‡å¼åç»§ç”Ÿæˆå™¨ (Lifted Successor Generation) çš„æ‰©å±•æ–¹æ¡ˆã€‚é€šè¿‡æ”¹è¿›åŸæœ¬ç”¨äºç»å…¸è§„åˆ’çš„å…ˆè¿›ç”ŸæˆæŠ€æœ¯ï¼Œè¯¥æ–¹æ³•é¦–æ¬¡å®ç°äº†å¯¹æ•°å€¼å‰ç½®æ¡ä»¶ (Numeric Precondition) é€‚ç”¨æ€§çš„æ”¯æŒã€‚å…¶æ ¸å¿ƒåŸç†æ˜¯æšä¸¾æ›¿æ¢ä¸€è‡´æ€§å›¾ (Substitution Consistency Graph) ä¸­çš„æå¤§å›¢ (Maximum Cliques)ï¼Œå¹¶ä»¥æ­¤ç”Ÿæˆåœ°é¢çš„åŠ¨ä½œå®ä¾‹ã€‚ç ”ç©¶é€šè¿‡å½¢å¼åŒ–è¯æ˜ç¡®è®¤äº†è¯¥ç”Ÿæˆå™¨åœ¨ç‰¹å®šæ¡ä»¶ä¸‹çš„ç²¾ç¡®æ€§ï¼Œå¹¶åœ¨ 25 ä¸ªåŸºå‡†é¢†åŸŸä¸­çš„ 23 ä¸ªé¢†åŸŸéªŒè¯äº†å…¶å¯é æ€§ã€‚ä½œä¸ºé¦–ä¸ªæ”¯æŒæ•°å€¼åŠ¨ä½œå‰ç½®æ¡ä»¶çš„æå‡å¼åç»§ç”Ÿæˆå™¨ï¼Œè¯¥æˆæœä¸ºå¤æ‚è§„åˆ’ç‰‡æ®µçš„æå‡å¼è§„åˆ’ (Lifted Planning) ç ”ç©¶å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00673v1",
      "published_date": "2025-11-01 19:33:16 UTC",
      "updated_date": "2025-11-01 19:33:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:07:36.352730+00:00"
    },
    {
      "arxiv_id": "2511.00664v1",
      "title": "ShadowLogic: Backdoors in Any Whitebox LLM",
      "title_zh": "ShadowLogicï¼šé’ˆå¯¹ä»»æ„ç™½ç›’å¤§è¯­è¨€æ¨¡å‹çš„åé—¨",
      "authors": [
        "Kasimir Schulz",
        "Amelia Kawasaki",
        "Leo Ring"
      ],
      "abstract": "Large language models (LLMs) are widely deployed across various applications, often with safeguards to prevent the generation of harmful or restricted content. However, these safeguards can be covertly bypassed through adversarial modifications to the computational graph of a model. This work highlights a critical security vulnerability in computational graph-based LLM formats, demonstrating that widely used deployment pipelines may be susceptible to obscured backdoors. We introduce ShadowLogic, a method for creating a backdoor in a white-box LLM by injecting an uncensoring vector into its computational graph representation. We set a trigger phrase that, when added to the beginning of a prompt into the LLM, applies the uncensoring vector and removes the content generation safeguards in the model. We embed trigger logic directly into the computational graph which detects the trigger phrase in a prompt. To evade detection of our backdoor, we obfuscate this logic within the graph structure, making it similar to standard model functions. Our method requires minimal alterations to model parameters, making backdoored models appear benign while retaining the ability to generate uncensored responses when activated. We successfully implement ShadowLogic in Phi-3 and Llama 3.2, using ONNX for manipulating computational graphs. Implanting the uncensoring vector achieved a >60% attack success rate for further malicious queries.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ShadowLogicï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ç™½ç›’ LLMs çš„è®¡ç®—å›¾åé—¨æ³¨å…¥æ–¹æ³•ï¼Œæ—¨åœ¨æ­ç¤ºæ¨¡å‹éƒ¨ç½²æµç¨‹ä¸­å­˜åœ¨çš„ä¸¥é‡å®‰å…¨æ¼æ´ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨æ¨¡å‹çš„è®¡ç®—å›¾ï¼ˆComputational Graphï¼‰è¡¨ç¤ºä¸­æ³¨å…¥ä¸€ä¸ªå»å®¡æŸ¥å‘é‡ï¼ˆUncensoring Vectorï¼‰ï¼Œå¹¶ç›´æ¥åœ¨å›¾ä¸­åµŒå…¥ç”¨äºæ£€æµ‹ç‰¹å®šè§¦å‘è¯ï¼ˆTrigger Phraseï¼‰çš„é€»è¾‘ã€‚å½“ç”¨æˆ·åœ¨æç¤ºè¯å¼€å¤´åŠ å…¥è¯¥è§¦å‘è¯­æ—¶ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨æ¿€æ´»å»å®¡æŸ¥å‘é‡å¹¶ç§»é™¤å®‰å…¨é˜²æŠ¤æœºåˆ¶ã€‚ä¸ºäº†è§„é¿æ£€æµ‹ï¼ŒShadowLogic å¯¹æ³¨å…¥é€»è¾‘è¿›è¡Œäº†æ··æ·†å¤„ç†ï¼Œä½¿å…¶åœ¨ç»“æ„ä¸Šä¸æ ‡å‡†æ¨¡å‹å‡½æ•°é«˜åº¦ç›¸ä¼¼ï¼Œä¸”å¯¹æ¨¡å‹å‚æ•°çš„æ”¹åŠ¨æå°ï¼Œä½¿åé—¨æ¨¡å‹åœ¨å¸¸è§„æ£€æŸ¥ä¸‹è¡¨ç°æ­£å¸¸ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ ONNX æ¡†æ¶åœ¨ Phi-3 å’Œ Llama 3.2 æ¨¡å‹ä¸ŠæˆåŠŸéªŒè¯äº†è¯¥æ–¹æ¡ˆï¼Œå®éªŒç»“æœæ˜¾ç¤ºæ³¨å…¥çš„å»å®¡æŸ¥å‘é‡åœ¨å¤„ç†åç»­æ¶æ„æŸ¥è¯¢æ—¶è¾¾åˆ°äº†è¶…è¿‡ 60% çš„æ”»å‡»æˆåŠŸç‡ã€‚è¯¥å·¥ä½œè¯æ˜äº†é€šè¿‡æ“çºµè®¡ç®—å›¾å¯ä»¥éšè”½åœ°ç»•è¿‡æ¨¡å‹å®‰å…¨æŠ¤æ ï¼Œä¸ºç†è§£å¤§è¯­è¨€æ¨¡å‹åœ¨ç™½ç›’ç¯å¢ƒä¸‹çš„å®‰å…¨æ€§æä¾›äº†æ–°è§†è§’ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00664v1",
      "published_date": "2025-11-01 19:10:08 UTC",
      "updated_date": "2025-11-01 19:10:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:07:39.348008+00:00"
    },
    {
      "arxiv_id": "2511.05547v2",
      "title": "Automated Invoice Data Extraction: Using LLM and OCR",
      "title_zh": "è‡ªåŠ¨åŒ–å‘ç¥¨æ•°æ®æå–ï¼šåŸºäº LLM ä¸ OCR",
      "authors": [
        "Khushi Khanchandani",
        "Advait Thakur",
        "Akshita Shetty",
        "Chaitravi Reddy",
        "Ritisa Behera"
      ],
      "abstract": "Conventional Optical Character Recognition (OCR) systems are challenged by variant invoice layouts, handwritten text, and low-quality scans, which are often caused by strong template dependencies that restrict their flexibility across different document structures and layouts. Newer solutions utilize advanced deep learning models such as Convolutional Neural Networks (CNN) as well as Transformers, and domain-specific models for better layout analysis and accuracy across various sections over varied document types. Large Language Models (LLMs) have revolutionized extraction pipelines at their core with sophisticated entity recognition and semantic comprehension to support complex contextual relationship mapping without direct programming specification. Visual Named Entity Recognition (NER) capabilities permit extraction from invoice images with greater contextual sensitivity and much higher accuracy rates than older approaches. Existing industry best practices utilize hybrid architectures that blend OCR technology and LLM for maximum scalability and minimal human intervention. This work introduces a holistic Artificial Intelligence (AI) platform combining OCR, deep learning, LLMs, and graph analytics to achieve unprecedented extraction quality and consistency.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¼ ç»Ÿ Optical Character Recognition (OCR) ç³»ç»Ÿåœ¨é¢å¯¹å¤šå˜çš„å‘ç¥¨å¸ƒå±€å’Œä½è´¨é‡æ‰«æä»¶æ—¶ï¼Œç”±äºæ¨¡æ¿ä¾èµ–è€Œé¢ä¸´çš„çµæ´»æ€§æŒ‘æˆ˜ã€‚è™½ç„¶ Convolutional Neural Networks (CNN) å’Œ Transformers æå‡äº†å¸ƒå±€åˆ†æç²¾åº¦ï¼Œä½† Large Language Models (LLMs) é€šè¿‡å…ˆè¿›çš„å®ä½“è¯†åˆ«å’Œè¯­ä¹‰ç†è§£ï¼Œä¸ºå¤æ‚ä¸Šä¸‹æ–‡å…³ç³»æ˜ å°„æä¾›äº†æ›´æ·±å±‚çš„æ”¯æŒã€‚ç»“åˆ Visual Named Entity Recognition (NER) æŠ€æœ¯ï¼Œç³»ç»Ÿèƒ½å¤Ÿä»¥æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´é«˜çš„å‡†ç¡®ç‡ä»å›¾åƒä¸­æå–å…³é”®ä¿¡æ¯ã€‚è¯¥å·¥ä½œä»‹ç»äº†ä¸€ä¸ªæ•´åˆäº† OCRã€æ·±åº¦å­¦ä¹ ã€LLMs å’Œ Graph Analytics çš„ç»¼åˆäººå·¥æ™ºèƒ½å¹³å°ï¼Œæ—¨åœ¨å®ç°æé«˜çš„æ•°æ®æå–è´¨é‡ä¸ä¸€è‡´æ€§ã€‚é€šè¿‡æ··åˆæ¶æ„çš„è®¾è®¡ï¼Œè¯¥å¹³å°åœ¨å‡å°‘äººå·¥å¹²é¢„çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†è·¨ä¸åŒæ–‡æ¡£ç»“æ„çš„å¤„ç†èƒ½åŠ›å’Œç³»ç»Ÿå¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.05547v2",
      "published_date": "2025-11-01 19:05:09 UTC",
      "updated_date": "2026-01-08 10:24:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:07:41.318365+00:00"
    },
    {
      "arxiv_id": "2511.00658v1",
      "title": "Lessons Learned from the Use of Generative AI in Engineering and Quality Assurance of a WEB System for Healthcare",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨åŒ»ç–— Web ç³»ç»Ÿå·¥ç¨‹ä¸è´¨é‡ä¿è¯ä¸­çš„åº”ç”¨ç»éªŒæ€»ç»“",
      "authors": [
        "Guilherme H. Travassos",
        "Sabrina Rocha",
        "Rodrigo Feitosa",
        "Felipe Assis",
        "Patricia Goncalves",
        "Andre Gheventer",
        "Larissa Galeno",
        "Arthur Sasse",
        "Julio Cesar Guimaraes",
        "Carlos Brito",
        "Joao Pedro Wieland"
      ],
      "abstract": "The advances and availability of technologies involving Generative Artificial Intelligence (AI) are evolving clearly and explicitly, driving immediate changes in various work activities. Software Engineering (SE) is no exception and stands to benefit from these new technologies, enhancing productivity and quality in its software development processes. However, although the use of Generative AI in SE practices is still in its early stages, considering the lack of conclusive results from ongoing research and the limited technological maturity, we have chosen to incorporate these technologies in the development of a web-based software system to be used in clinical trials by a thoracic diseases research group at our university. For this reason, we decided to share this experience report documenting our development team's learning journey in using Generative AI during the software development process. Project management, requirements specification, design, development, and quality assurance activities form the scope of observation. Although we do not yet have definitive technological evidence to evolve our development process significantly, the results obtained and the suggestions shared here represent valuable insights for software organizations seeking to innovate their development practices to achieve software quality with generative AI.",
      "tldr_zh": "è¯¥ç ”ç©¶è®°å½•äº†åœ¨åŒ»ç–—ä¿å¥ Web ç³»ç»Ÿå·¥ç¨‹å’Œè´¨é‡ä¿è¯ (Quality Assurance) ä¸­åº”ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) çš„ç»éªŒæ•™è®­ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨å½“å‰æŠ€æœ¯æˆç†Ÿåº¦æœ‰é™ä¸”ç¼ºä¹å®šè®ºçš„èƒŒæ™¯ä¸‹ï¼Œå°è¯•å°† Generative AI èå…¥è½¯ä»¶å·¥ç¨‹ (Software Engineering) çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸï¼Œæ¶µç›–äº†é¡¹ç›®ç®¡ç†ã€éœ€æ±‚è§„æ ¼è¯´æ˜ã€è®¾è®¡ã€å¼€å‘åŠè´¨é‡ä¿è¯ç­‰æ ¸å¿ƒæ´»åŠ¨ã€‚é€šè¿‡ä¸ºèƒ¸ç§‘ç–¾ç—…ç ”ç©¶ç»„å¼€å‘ä¸´åºŠè¯•éªŒç³»ç»Ÿçš„å®é™…æ¡ˆä¾‹ï¼Œè¯¥æŠ¥å‘Šè¯¦ç»†æè¿°äº†å›¢é˜Ÿåœ¨çœŸå®å¼€å‘ç¯å¢ƒä¸­çš„å­¦ä¹ å†ç¨‹ä¸å®è·µæ¢ç´¢ã€‚è™½ç„¶ç›®å‰å°šæœªè·å¾—è¶³ä»¥æ˜¾è‘—æ¼”è¿›å¼€å‘æµç¨‹çš„å†³å®šæ€§æŠ€æœ¯è¯æ®ï¼Œä½†ç ”ç©¶æ‰€åˆ†äº«çš„æ´å¯Ÿå’Œæ”¹è¿›å»ºè®®ä¸ºè‡´åŠ›äºåˆ©ç”¨ Generative AI æå‡è½¯ä»¶è´¨é‡çš„ç»„ç»‡æä¾›äº†å®è´µçš„å‚è€ƒä»·å€¼ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.SE",
      "comment": "11 pages, 2 figures, in Portuguese language",
      "pdf_url": "https://arxiv.org/pdf/2511.00658v1",
      "published_date": "2025-11-01 18:42:29 UTC",
      "updated_date": "2025-11-01 18:42:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:07:44.664501+00:00"
    },
    {
      "arxiv_id": "2511.14772v1",
      "title": "Test-time Scaling of LLMs: A Survey from A Subproblem Structure Perspective",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æµ‹è¯•æ—¶æ‰©å±•ï¼šåŸºäºå­é—®é¢˜ç»“æ„è§†è§’çš„ç»¼è¿°",
      "authors": [
        "Zhuoyi Yang",
        "Xu Guo",
        "Tong Zhang",
        "Huijuan Xu",
        "Boyang Li"
      ],
      "abstract": "With this paper, we survey techniques for improving the predictive accuracy of pretrained large language models by allocating additional compute at inference time. In categorizing test-time scaling methods, we place special emphasis on how a problem is decomposed into subproblems and on the topological organization of these subproblems whether sequential, parallel, or tree-structured. This perspective allows us to unify diverse approaches such as Chain-of-Thought, Branch-Solve-Merge, and Tree-of-Thought under a common lens. We further synthesize existing analyses of these techniques, highlighting their respective strengths and weaknesses, and conclude by outlining promising directions for future research",
      "tldr_zh": "è¿™ç¯‡è®ºæ–‡å¯¹é€šè¿‡åœ¨æ¨ç†é˜¶æ®µåˆ†é…é¢å¤–è®¡ç®—èµ„æºï¼ˆTest-time Scalingï¼‰æ¥æé«˜é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢„æµ‹å‡†ç¡®æ€§çš„æŠ€æœ¯è¿›è¡Œäº†å…¨é¢ç»¼è¿°ã€‚è¯¥ç ”ç©¶ç‰¹åˆ«å¼ºè°ƒä»å­é—®é¢˜ç»“æ„ï¼ˆSubproblem Structureï¼‰çš„è§’åº¦å¯¹è¿™äº›æ–¹æ³•è¿›è¡Œåˆ†ç±»ï¼Œæ¢è®¨äº†é—®é¢˜å¦‚ä½•è¢«åˆ†è§£ä»¥åŠå­é—®é¢˜åœ¨æ‹“æ‰‘ç»“æ„ä¸Šï¼ˆå¦‚é¡ºåºã€å¹¶è¡Œæˆ–æ ‘çŠ¶ç»“æ„ï¼‰çš„ç»„ç»‡æ–¹å¼ã€‚é€šè¿‡è¿™ä¸€è§†è§’ï¼Œä½œè€…å°† Chain-of-Thoughtã€Branch-Solve-Merge å’Œ Tree-of-Thought ç­‰å¤šæ ·åŒ–æ–¹æ³•ç»Ÿä¸€åœ¨åŒä¸€ä¸ªåˆ†ææ¡†æ¶ä¸‹ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ç»¼åˆåˆ†æäº†ç°æœ‰æŠ€æœ¯çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æŒ‡å¯¼æ€§å»ºè®®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14772v1",
      "published_date": "2025-11-01 18:41:23 UTC",
      "updated_date": "2025-11-01 18:41:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:07:46.303599+00:00"
    },
    {
      "arxiv_id": "2511.00651v1",
      "title": "Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting",
      "title_zh": "åˆ©ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (MAS) ä¸å¾®è°ƒå°è¯­è¨€æ¨¡å‹ (SLMs) å®ç°ç”µä¿¡ç½‘ç»œè‡ªåŠ¨åŒ–æ•…éšœæ’æŸ¥",
      "authors": [
        "Chenhua Shi",
        "Bhavika Jalli",
        "Gregor Macdonald",
        "John Zou",
        "Wanlu Lei",
        "Mridul Jain",
        "Joji Philip"
      ],
      "abstract": "Telecom networks are rapidly growing in scale and complexity, making effective management, operation, and optimization increasingly challenging. Although Artificial Intelligence (AI) has been applied to many telecom tasks, existing models are often narrow in scope, require large amounts of labeled data, and struggle to generalize across heterogeneous deployments. Consequently, network troubleshooting continues to rely heavily on Subject Matter Experts (SMEs) to manually correlate various data sources to identify root causes and corrective actions. To address these limitations, we propose a Multi-Agent System (MAS) that employs an agentic workflow, with Large Language Models (LLMs) coordinating multiple specialized tools for fully automated network troubleshooting. Once faults are detected by AI/ML-based monitors, the framework dynamically activates agents such as an orchestrator, solution planner, executor, data retriever, and root-cause analyzer to diagnose issues and recommend remediation strategies within a short time frame. A key component of this system is the solution planner, which generates appropriate remediation plans based on internal documentation. To enable this, we fine-tuned a Small Language Model (SLM) on proprietary troubleshooting documents to produce domain-grounded solution plans. Experimental results demonstrate that the proposed framework significantly accelerates troubleshooting automation across both Radio Access Network (RAN) and Core network domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (Multi-Agent System, MAS) ä¸å¾®è°ƒå°è¯­è¨€æ¨¡å‹ (Small Language Models, SLMs) çš„è‡ªåŠ¨åŒ–ç”µä¿¡ç½‘ç»œæ•…éšœæ’é™¤æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°ä»£ç½‘ç»œå¤æ‚æ€§å¢åŠ åŠå¯¹é¢†åŸŸä¸“å®¶ (SMEs) æ‰‹åŠ¨æ“ä½œè¿‡åº¦ä¾èµ–çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä»£ç†å·¥ä½œæµ (agentic workflow)ï¼Œç”±å¤§è¯­è¨€æ¨¡å‹ (LLMs) åè°ƒç¼–æ’å™¨ã€è§£å†³æ–¹æ¡ˆè§„åˆ’å™¨ã€æ‰§è¡Œå™¨åŠæ ¹å› åˆ†æå™¨ç­‰ä¸“ä¸šæ™ºèƒ½ä½“ï¼Œå®ç°ä»æ•…éšœæ£€æµ‹åˆ°ä¿®å¤å»ºè®®çš„å…¨è‡ªåŠ¨é—­ç¯ã€‚å…¶ä¸­ï¼Œæ ¸å¿ƒç»„ä»¶è§£å†³æ–¹æ¡ˆè§„åˆ’å™¨åˆ©ç”¨åœ¨ä¸“æœ‰æ–‡æ¡£ä¸Šå¾®è°ƒçš„å°è¯­è¨€æ¨¡å‹ (SLM) ç”Ÿæˆé¢†åŸŸç›¸å…³çš„ä¿®å¤è®¡åˆ’ï¼Œç¡®ä¿äº†æ–¹æ¡ˆçš„é’ˆå¯¹æ€§ä¸å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥ç³»ç»Ÿæ˜¾è‘—åŠ é€Ÿäº†æ— çº¿æ¥å…¥ç½‘ (RAN) å’Œæ ¸å¿ƒç½‘ (Core) é¢†åŸŸçš„æ•…éšœå¤„ç†è‡ªåŠ¨åŒ–è¿›ç¨‹ï¼Œä¸ºå®ç°ç”µä¿¡ç½‘ç»œçš„é«˜æ•ˆæ™ºèƒ½åŒ–è¿ç»´æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IT",
        "cs.MA",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages, 7 figures, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2511.00651v1",
      "published_date": "2025-11-01 18:19:41 UTC",
      "updated_date": "2025-11-01 18:19:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:07:50.347470+00:00"
    },
    {
      "arxiv_id": "2511.00641v1",
      "title": "More Than A Shortcut: A Hyperbolic Approach To Early-Exit Networks",
      "title_zh": "ä¸æ­¢äºæ·å¾„ï¼šä¸€ç§ç”¨äºæ—©æœŸé€€å‡ºç½‘ç»œçš„åŒæ›²æ–¹æ³•",
      "authors": [
        "Swapnil Bhosale",
        "Cosmin Frateanu",
        "Camilla Clark",
        "Arnoldas Jasonas",
        "Chris Mitchell",
        "Xiatian Zhu",
        "Vamsi Krishna Ithapu",
        "Giacomo Ferroni",
        "Cagdas Bilen",
        "Sanjeel Parekh"
      ],
      "abstract": "Deploying accurate event detection on resource-constrained devices is challenged by the trade-off between performance and computational cost. While Early-Exit (EE) networks offer a solution through adaptive computation, they often fail to enforce a coherent hierarchical structure, limiting the reliability of their early predictions. To address this, we propose Hyperbolic Early-Exit networks (HypEE), a novel framework that learns EE representations in the hyperbolic space. Our core contribution is a hierarchical training objective with a novel entailment loss, which enforces a partial-ordering constraint to ensure that deeper network layers geometrically refine the representations of shallower ones. Experiments on multiple audio event detection tasks and backbone architectures show that HypEE significantly outperforms standard Euclidean EE baselines, especially at the earliest, most computationally-critical exits. The learned geometry also provides a principled measure of uncertainty, enabling a novel triggering mechanism that makes the overall system both more efficient and more accurate than a conventional EE and standard backbone models without early-exits.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†HypEE (Hyperbolic Early-Exit networks)ï¼Œæ—¨åœ¨è§£å†³èµ„æºå—é™è®¾å¤‡åœ¨äº‹ä»¶æ£€æµ‹ä»»åŠ¡ä¸­é¢ä¸´çš„æ€§èƒ½ä¸è®¡ç®—æˆæœ¬æƒè¡¡é—®é¢˜ã€‚é’ˆå¯¹ä¼ ç»ŸEarly-Exit (EE)ç½‘ç»œå› ç¼ºä¹è¿è´¯å±‚çº§ç»“æ„è€Œå¯¼è‡´æ—©æœŸé¢„æµ‹å¯é æ€§ä¸è¶³çš„ç¼ºé™·ï¼Œè¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°åœ¨åŒæ›²ç©ºé—´ (hyperbolic space) ä¸­å­¦ä¹ EEè¡¨ç¤ºã€‚å…¶æ ¸å¿ƒè´¡çŒ®æ˜¯æå‡ºäº†ä¸€ç§å¸¦æœ‰è•´å«æŸå¤± (entailment loss) çš„å±‚çº§è®­ç»ƒç›®æ ‡ï¼Œé€šè¿‡å¼ºåˆ¶æ‰§è¡Œååºçº¦æŸ (partial-ordering constraint) ç¡®ä¿æ·±å±‚ç½‘ç»œèƒ½ä»å‡ ä½•ä¸Šç»†åŒ–æµ…å±‚è¡¨ç¤ºã€‚åœ¨éŸ³é¢‘äº‹ä»¶æ£€æµ‹ä»»åŠ¡ä¸­çš„å®éªŒè¯æ˜ï¼ŒHypEEåœ¨æœ€æ—©ä¸”æœ€å…·è®¡ç®—æŒ‘æˆ˜çš„å‡ºå£å¤„è¡¨ç°å°¤ä¸ºçªå‡ºï¼Œæ˜¾è‘—ä¼˜äºæ ‡å‡†çš„æ¬§å‡ é‡Œå¾—EEåŸºçº¿ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åŒæ›²å‡ ä½•æä¾›çš„ä¸ç¡®å®šæ€§åº¦é‡å®ç°äº†ä¸€ç§æ–°å‹è§¦å‘æœºåˆ¶ï¼Œä½¿ç³»ç»Ÿåœ¨ä¿æŒé«˜æ•ˆçš„åŒæ—¶ï¼Œå‡†ç¡®ç‡ä¹Ÿè¶…è¶Šäº†ä¼ ç»ŸEEæ¨¡å‹å’Œæ ‡å‡†ä¸»å¹²ç½‘ç»œã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00641v1",
      "published_date": "2025-11-01 17:43:02 UTC",
      "updated_date": "2025-11-01 17:43:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:07:54.040507+00:00"
    },
    {
      "arxiv_id": "2511.00640v1",
      "title": "DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching",
      "title_zh": "DTSï¼šé€šè¿‡è§£ç æ ‘å‹¾å‹’å¢å¼ºå¤§æ¨ç†æ¨¡å‹",
      "authors": [
        "Zicheng Xu",
        "Guanchu Wang",
        "Yu-Neng Chuang",
        "Guangyao Zheng",
        "Alexander S. Szalay",
        "Zirui Liu",
        "Vladimir Braverman"
      ],
      "abstract": "Large Reasoning Models (LRMs) demonstrate strong performance on complex reasoning tasks, yet they often suffer from overthinking, producing excessively long chain-of-thought (CoT) traces that increase inference cost and may degrade accuracy. Our analysis reveals a clear anti-correlation between reasoning length and accuracy, where across multiple stochastic decodes, the short reasoning paths consistently achieve the highest correctness, while longer ones accumulate errors and repetitions. These short optimal reasoning paths can be found ideally through full enumeration of the reasoning space. However, the tree-structured reasoning space grows exponentially with sequence length, rendering exhaustive exploration infeasible. To address this, we propose DTS, a model-agnostic decoding framework that sketches the reasoning space by selectively branching at high-entropy tokens and applies early stopping to select the shortest completed reasoning path. This approach approximates the optimal solution that enhances both efficiency and accuracy, without requiring additional training or supervision. Experiments on AIME2024 and AIME2025 datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves accuracy by up to 8%, reduces average reasoning length by 23%, and decreases repetition frequency by 12%, demonstrating DTS's ability for scalable and efficient LRM reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§æ¨ç†æ¨¡å‹(Large Reasoning Models, LRMs)åœ¨å¤æ‚ä»»åŠ¡ä¸­å› è¿‡åº¦æ€è€ƒäº§ç”Ÿå†—é•¿æ€ç»´é“¾(CoT)è€Œå¯¼è‡´æ¨ç†æˆæœ¬å¢åŠ å’Œå‡†ç¡®ç‡ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†DTS(Decoding Tree Sketching)æ¡†æ¶ã€‚ç ”ç©¶å‘ç°æ¨ç†é•¿åº¦ä¸å‡†ç¡®ç‡ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„è´Ÿç›¸å…³ï¼Œè¾ƒçŸ­çš„æ¨ç†è·¯å¾„å¾€å¾€å…·æœ‰æ›´é«˜çš„æ­£ç¡®ç‡ã€‚DTSä½œä¸ºä¸€ç§æ¨¡å‹æ— å…³çš„è§£ç æ¡†æ¶ï¼Œé€šè¿‡åœ¨å…·æœ‰é«˜ç†µ(high-entropy)çš„tokenå¤„è¿›è¡Œé€‰æ‹©æ€§åˆ†æ”¯æ¥æç»˜æ¨ç†ç©ºé—´ï¼Œå¹¶åº”ç”¨æå‰åœæ­¢(early stopping)ç­–ç•¥é€‰å–æœ€çŸ­çš„å®Œæ•´æ¨ç†è·¯å¾„ã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–è®­ç»ƒæˆ–ç›‘ç£ï¼Œé€šè¿‡è¿‘ä¼¼æœ€ä¼˜è§£åŒæ—¶æå‡äº†æ¨ç†æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨AIME2024å’ŒAIME2025æ•°æ®é›†ä¸Šé…åˆDeepSeek-R1-Distill-Qwenç³»åˆ—æ¨¡å‹ï¼ŒDTSå°†å‡†ç¡®ç‡æœ€é«˜æå‡äº†8%ï¼Œå¹³å‡æ¨ç†é•¿åº¦ç¼©çŸ­äº†23%ï¼Œå¹¶æœ‰æ•ˆé™ä½äº†12%çš„é‡å¤é¢‘ç‡ï¼ŒéªŒè¯äº†å…¶åœ¨å®ç°å¤§æ¨ç†æ¨¡å‹é«˜æ•ˆã€å¯æ‰©å±•æ¨ç†æ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00640v1",
      "published_date": "2025-11-01 17:41:28 UTC",
      "updated_date": "2025-11-01 17:41:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:07:59.048751+00:00"
    },
    {
      "arxiv_id": "2511.00634v2",
      "title": "Node Preservation and its Effect on Crossover in Cartesian Genetic Programming",
      "title_zh": "ç¬›å¡å°”é—ä¼ ç¼–ç¨‹ä¸­çš„èŠ‚ç‚¹ä¿ç•™åŠå…¶å¯¹äº¤å‰æ“ä½œçš„å½±å“",
      "authors": [
        "Mark Kocherovsky",
        "Illya Bakurov",
        "Wolfgang Banzhaf"
      ],
      "abstract": "While crossover is a critical and often indispensable component in other forms of Genetic Programming, such as Linear- and Tree-based, it has consistently been claimed that it deteriorates search performance in CGP. As a result, a mutation-alone $(1+Î»)$ evolutionary strategy has become the canonical approach for CGP. Although several operators have been developed that demonstrate an increased performance over the canonical method, a general solution to the problem is still lacking. In this paper, we compare basic crossover methods, namely one-point and uniform, to variants in which nodes are ``preserved,'' including the subgraph crossover developed by Roman Kalkreuth, the difference being that when ``node preservation'' is active, crossover is not allowed to break apart instructions. We also compare a node mutation operator to the traditional point mutation; the former simply replaces an entire node with a new one. We find that node preservation in both mutation and crossover improves search using symbolic regression benchmark problems, moving the field towards a general solution to CGP crossover.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Cartesian Genetic Programming (CGP) ä¸­ crossover ç®—å­å¯¹æœç´¢æ€§èƒ½çš„å½±å“ï¼Œæ—¨åœ¨è§£å†³äº¤å‰åœ¨ CGP ä¸­å¸¸å¯¼è‡´æ€§èƒ½æ¶åŒ–çš„é•¿æœŸéš¾é¢˜ã€‚ä½œè€…å¯¹æ¯”äº†åŸºç¡€çš„ one-point å’Œ uniform crossover æ–¹æ³•åŠå…¶å¼•å…¥â€œnode preservationâ€æœºåˆ¶çš„å˜ä½“ï¼Œåè€…åœ¨äº¤å‰è¿‡ç¨‹ä¸­ç¡®ä¿æŒ‡ä»¤ä¸ä¼šè¢«æ‹†åˆ†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¯¹æ¯”äº† node mutation ç®—å­ä¸ä¼ ç»Ÿçš„ point mutationï¼Œåˆ†æäº†èŠ‚ç‚¹çº§æ“ä½œå¯¹æ¼”åŒ–æ•ˆç‡çš„ä½œç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ symbolic regression åŸºå‡†æµ‹è¯•ä¸­ï¼Œnode preservation æœºåˆ¶æ˜¾è‘—æå‡äº†å˜å¼‚å’Œäº¤å‰çš„æœç´¢æ€§èƒ½ã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº† CGP é¢†åŸŸé•¿æœŸå æ®ä¸»å¯¼åœ°ä½çš„ mutation-alone ç­–ç•¥ï¼Œä¸ºå¼€å‘æ›´é«˜æ•ˆä¸”é€šç”¨çš„ CGP äº¤å‰ç®—å­æä¾›äº†é‡è¦ä¾æ®ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "Draft to cite in another paper before both papers are peer-reviewed for the evo*2026 conference, 21 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.00634v2",
      "published_date": "2025-11-01 17:26:56 UTC",
      "updated_date": "2025-11-24 17:55:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:08:38.858421+00:00"
    },
    {
      "arxiv_id": "2511.00628v1",
      "title": "AgentGit: A Version Control Framework for Reliable and Scalable LLM-Powered Multi-Agent Systems",
      "title_zh": "AgentGitï¼šé¢å‘å¯é ä¸”å¯æ‰©å±•çš„å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„ç‰ˆæœ¬æ§åˆ¶æ¡†æ¶",
      "authors": [
        "Yang Li",
        "Siqi Ping",
        "Xiyu Chen",
        "Xiaojian Qi",
        "Zigan Wang",
        "Ye Luo",
        "Xiaowei Zhang"
      ],
      "abstract": "With the rapid progress of large language models (LLMs), LLM-powered multi-agent systems (MAS) are drawing increasing interest across academia and industry. However, many current MAS frameworks struggle with reliability and scalability, especially on complex tasks. We present AgentGit, a framework that brings Git-like rollback and branching to MAS workflows. Built as an infrastructure layer on top of LangGraph, AgentGit supports state commit, revert, and branching, allowing agents to traverse, compare, and explore multiple trajectories efficiently. To evaluate AgentGit, we designed an experiment that optimizes target agents by selecting better prompts. We ran a multi-step A/B test against three baselines -- LangGraph, AutoGen, and Agno -- on a real-world task: retrieving and analyzing paper abstracts. Results show that AgentGit significantly reduces redundant computation, lowers runtime and token usage, and supports parallel exploration across multiple branches, enhancing both reliability and scalability in MAS development. This work offers a practical path to more robust MAS design and enables error recovery, safe exploration, iterative debugging, and A/B testing in collaborative AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (MAS) åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶å­˜åœ¨çš„å¯é æ€§å’Œå¯æ‰©å±•æ€§ä¸è¶³é—®é¢˜ï¼Œæå‡ºäº† AgentGit æ¡†æ¶ï¼Œæ—¨åœ¨å°†ç±»ä¼¼ Git çš„ç‰ˆæœ¬æ§åˆ¶æœºåˆ¶å¼•å…¥å¤§è¯­è¨€æ¨¡å‹ (LLM) çš„å·¥ä½œæµä¸­ã€‚ä½œä¸ºæ„å»ºåœ¨ LangGraph ä¹‹ä¸Šçš„åŸºç¡€è®¾æ–½å±‚ï¼ŒAgentGit æ”¯æŒçŠ¶æ€æäº¤ (state commit)ã€å›æ»š (revert) å’Œåˆ†æ”¯ (branching)ï¼Œå…è®¸æ™ºèƒ½ä½“é«˜æ•ˆåœ°éå†ã€æ¯”è¾ƒå’Œæ¢ç´¢ä¸åŒçš„æ‰§è¡Œè½¨è¿¹ã€‚é€šè¿‡åœ¨è®ºæ–‡æ‘˜è¦æ£€ç´¢ä¸åˆ†æä»»åŠ¡ä¸­ä¸ LangGraphã€AutoGen å’Œ Agno è¿›è¡Œå¤šæ­¥ A/B æµ‹è¯•ï¼Œå®éªŒè¯æ˜è¯¥æ¡†æ¶èƒ½æ˜¾è‘—å‡å°‘å†—ä½™è®¡ç®—ï¼Œé™ä½è¿è¡Œæ—¶é—´ä¸ Token ä½¿ç”¨æˆæœ¬ã€‚AgentGit ä¸ä»…æ”¯æŒè·¨åˆ†æ”¯çš„å¹¶è¡Œæ¢ç´¢ï¼Œè¿˜ä¸º MAS çš„é”™è¯¯æ¢å¤ã€è¿­ä»£è°ƒè¯•å’Œå®‰å…¨æ¢ç´¢æä¾›äº†åˆ‡å®å¯è¡Œçš„è·¯å¾„ï¼Œæå¤§å¢å¼ºäº†åä½œ AI ç³»ç»Ÿçš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00628v1",
      "published_date": "2025-11-01 17:11:31 UTC",
      "updated_date": "2025-11-01 17:11:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:08:15.254781+00:00"
    },
    {
      "arxiv_id": "2511.00617v1",
      "title": "Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering",
      "title_zh": "ä¿¡å¿µåŠ¨åŠ›å­¦æ­ç¤ºä¸Šä¸‹æ–‡å­¦ä¹ ä¸æ¿€æ´»å¼•å¯¼çš„åŒé‡æœ¬è´¨",
      "authors": [
        "Eric Bigelow",
        "Daniel Wurgaft",
        "YingQiao Wang",
        "Noah Goodman",
        "Tomer Ullman",
        "Hidenori Tanaka",
        "Ekdeep Singh Lubana"
      ],
      "abstract": "Large language models (LLMs) can be controlled at inference time through prompts (in-context learning) and internal activations (activation steering). Different accounts have been proposed to explain these methods, yet their common goal of controlling model behavior raises the question of whether these seemingly disparate methodologies can be seen as specific instances of a broader framework. Motivated by this, we develop a unifying, predictive account of LLM control from a Bayesian perspective. Specifically, we posit that both context- and activation-based interventions impact model behavior by altering its belief in latent concepts: steering operates by changing concept priors, while in-context learning leads to an accumulation of evidence. This results in a closed-form Bayesian model that is highly predictive of LLM behavior across context- and activation-based interventions in a set of domains inspired by prior work on many-shot in-context learning. This model helps us explain prior empirical phenomena - e.g., sigmoidal learning curves as in-context evidence accumulates - while predicting novel ones - e.g., additivity of both interventions in log-belief space, which results in distinct phases such that sudden and dramatic behavioral shifts can be induced by slightly changing intervention controls. Taken together, this work offers a unified account of prompt-based and activation-based control of LLM behavior, and a methodology for empirically predicting the effects of these interventions.",
      "tldr_zh": "è¯¥ç ”ç©¶ä» Bayesian è§†è§’æ¢è®¨äº†æ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„ä¸¤ç§ä¸»è¦æ–¹æ³•â€”â€”ä¸Šä¸‹æ–‡å­¦ä¹  (In-Context Learning) ä¸æ¿€æ´»å¼•å¯¼ (Activation Steering) ä¹‹é—´çš„ç»Ÿä¸€æ€§ã€‚è®ºæ–‡æå‡ºäº†ä¸€ä¸ªé¢„æµ‹æ€§æ¡†æ¶ï¼Œå°† Activation Steering è§£é‡Šä¸ºå¯¹æ¦‚å¿µå…ˆéªŒ (concept priors) çš„ä¿®æ”¹ï¼Œè€Œå°† ICL è§†ä¸ºè¯æ®çš„ç´¯ç§¯ (accumulation of evidence)ã€‚åŸºäºæ­¤æ„å»ºçš„é—­å¼ Bayesian æ¨¡å‹èƒ½å¤Ÿé«˜åº¦å‡†ç¡®åœ°é¢„æµ‹æ¨¡å‹åœ¨ä¸åŒå¹²é¢„ä¸‹çš„è¡Œä¸ºï¼Œå¹¶åˆç†è§£é‡Šäº† many-shot ICL ä¸­å‡ºç°çš„ S å‹å­¦ä¹ æ›²çº¿ (sigmoidal learning curves) ç­‰ç°è±¡ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†å¹²é¢„æªæ–½åœ¨å¯¹æ•°ç½®ä¿¡ç©ºé—´ (log-belief space) å…·æœ‰åŠ æ€§ç‰¹å¾ï¼Œè¿™æ„å‘³ç€å¾®å°çš„æ§åˆ¶å˜é‡æ”¹å˜å¯èƒ½å¯¼è‡´æ¨¡å‹è¡Œä¸ºå‘ç”Ÿå‰§çƒˆä¸”çªç„¶çš„ç›¸ä½åç§»ã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£å’Œé¢„æµ‹åŸºäºæç¤ºåŠæ¿€æ´»çš„ LLM æ§åˆ¶æ‰‹æ®µæä¾›äº†ç»Ÿä¸€çš„ç†è®ºåŸºç¡€ä¸å®è¯æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00617v1",
      "published_date": "2025-11-01 16:46:03 UTC",
      "updated_date": "2025-11-01 16:46:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:09:13.366173+00:00"
    },
    {
      "arxiv_id": "2511.00609v2",
      "title": "PreferThinker: Reasoning-based Personalized Image Preference Assessment",
      "title_zh": "PreferThinkerï¼šåŸºäºæ¨ç†çš„ä¸ªæ€§åŒ–å›¾åƒåå¥½è¯„ä¼°",
      "authors": [
        "Shengqi Xu",
        "Xinpeng Zhou",
        "Yabo Zhang",
        "Ming Liu",
        "Tao Liang",
        "Tianyu Zhang",
        "Yalong Bai",
        "Zuxuan Wu",
        "Wangmeng Zuo"
      ],
      "abstract": "Personalized image preference assessment aims to evaluate an individual user's image preferences by relying only on a small set of reference images as prior information. Existing methods mainly focus on general preference assessment, training models with large-scale data to tackle well-defined tasks such as text-image alignment. However, these approaches struggle to handle personalized preference because user-specific data are scarce and not easily scalable, and individual tastes are often diverse and complex. To overcome these challenges, we introduce a common preference profile that serves as a bridge across users, allowing large-scale user data to be leveraged for training profile prediction and capturing complex personalized preferences. Building on this idea, we propose a reasoning-based personalized image preference assessment framework that follows a \\textit{predict-then-assess} paradigm: it first predicts a user's preference profile from reference images, and then provides interpretable, multi-dimensional scores and assessments of candidate images based on the predicted profile. To support this, we first construct a large-scale Chain-of-Thought (CoT)-style personalized assessment dataset annotated with diverse user preference profiles and high-quality CoT-style reasoning, enabling explicit supervision of structured reasoning. Next, we adopt a two-stage training strategy: a cold-start supervised fine-tuning phase to empower the model with structured reasoning capabilities, followed by reinforcement learning to incentivize the model to explore more reasonable assessment paths and enhance generalization. Furthermore, we propose a similarity-aware prediction reward to encourage better prediction of the user's preference profile, which facilitates more reasonable assessments exploration. Extensive experiments demonstrate the superiority of the proposed method.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PreferThinkerï¼Œä¸€ç§åŸºäºæ¨ç†çš„ä¸ªæ€§åŒ–å›¾åƒåå¥½è¯„ä¼° (Personalized image preference assessment) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”¨æˆ·ç‰¹å®šæ•°æ®ç¨€ç¼ºåŠä¸ªä½“å®¡ç¾å¤šæ ·åŒ–å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†é€šç”¨åå¥½é…ç½®æ–‡ä»¶ (common preference profile) ä½œä¸ºè·¨ç”¨æˆ·æ¡¥æ¢ï¼Œå¹¶é‡‡ç”¨â€œå…ˆé¢„æµ‹åè¯„ä¼°â€ (predict-then-assess) çš„èŒƒå¼ï¼Œå³å…ˆä»å‚è€ƒå›¾åƒé¢„æµ‹ç”¨æˆ·åå¥½ï¼Œå†åŸºäºé¢„æµ‹ç»“æœå¯¹å€™é€‰å›¾åƒè¿›è¡Œå¤šç»´åº¦çš„å¯è§£é‡Šæ€§è¯„åˆ†ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„é“¾å¼æ€ç»´ (Chain-of-Thought) é£æ ¼æ•°æ®é›†ï¼Œç”¨äºæä¾›ç»“æ„åŒ–æ¨ç†çš„æ˜¾å¼ç›‘ç£ã€‚æ¨¡å‹é‡‡ç”¨äº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå³å…ˆé€šè¿‡æœ‰ç›‘ç£å¾®è°ƒ (SFT) èµ‹äºˆæ¨¡å‹æ¨ç†èƒ½åŠ›ï¼Œéšååˆ©ç”¨å¼ºåŒ–å­¦ä¹  (RL) å’Œç›¸ä¼¼æ€§æ„ŸçŸ¥é¢„æµ‹å¥–åŠ± (similarity-aware prediction reward) è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¸è¯„ä¼°å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPreferThinker åœ¨æ•æ‰å¤æ‚çš„ä¸ªæ€§åŒ–åå¥½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„é€šç”¨åå¥½è¯„ä¼°æ–¹æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00609v2",
      "published_date": "2025-11-01 16:19:51 UTC",
      "updated_date": "2025-11-10 07:47:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:08:20.224407+00:00"
    },
    {
      "arxiv_id": "2511.00603v1",
      "title": "EPARA: Parallelizing Categorized AI Inference in Edge Clouds",
      "title_zh": "EPARAï¼šè¾¹ç¼˜äº‘ä¸­åˆ†ç±» AI æ¨ç†çš„å¹¶è¡ŒåŒ–",
      "authors": [
        "Yubo Wang",
        "Yubo Cui",
        "Tuo Shi",
        "Danyang Li",
        "Wenxin Li",
        "Lide Suo",
        "Tao Wang",
        "Xin Xie"
      ],
      "abstract": "With the increasing adoption of AI applications such as large language models and computer vision AI, the computational demands on AI inference systems are continuously rising, making the enhancement of task processing capacity using existing hardware a primary objective in edge clouds. We propose EPARA, an end-to-end AI parallel inference framework in edge, aimed at enhancing the edge AI serving capability. Our key idea is to categorize tasks based on their sensitivity to latency/frequency and requirement for GPU resources, thereby achieving both request-level and service-level task-resource allocation. EPARA consists of three core components: 1) a task-categorized parallelism allocator that decides the parallel mode of each task, 2) a distributed request handler that performs the calculation for the specific request, and 3) a state-aware scheduler that periodically updates service placement in edge clouds. We implement a EPARA prototype and conduct a case study on the EPARA operation for LLMs and segmentation tasks. Evaluation through testbed experiments involving edge servers, embedded devices, and microcomputers shows that EPARA achieves up to 2.1$\\times$ higher goodput in production workloads compared to prior frameworks, while adapting to various edge AI inference tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¾¹ç¼˜äº‘ç¯å¢ƒä¸­æ—¥ç›Šå¢é•¿çš„ AI æ¨ç†éœ€æ±‚ï¼Œæå‡ºäº† EPARAï¼Œä¸€ç§æ—¨åœ¨æå‡è¾¹ç¼˜ AI æœåŠ¡èƒ½åŠ›çš„ç«¯åˆ°ç«¯å¹¶è¡Œæ¨ç†æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯æ ¹æ®ä»»åŠ¡å¯¹å»¶è¿Ÿã€é¢‘ç‡çš„æ•æ„Ÿæ€§ä»¥åŠå¯¹ GPU èµ„æºçš„éœ€æ±‚è¿›è¡Œåˆ†ç±»ï¼Œä»è€Œå®ç°è¯·æ±‚çº§åˆ«å’Œä¸šåŠ¡çº§åˆ«çš„ä»»åŠ¡èµ„æºåˆ†é…ã€‚EPARA åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå†³å®šå„ä»»åŠ¡å¹¶è¡Œæ¨¡å¼çš„ task-categorized parallelism allocatorï¼Œæ‰§è¡Œç‰¹å®šè¯·æ±‚è®¡ç®—çš„ distributed request handlerï¼Œä»¥åŠå®šæœŸæ›´æ–°è¾¹ç¼˜äº‘æœåŠ¡éƒ¨ç½²çš„ state-aware schedulerã€‚ç ”ç©¶äººå‘˜å®ç°äº† EPARA åŸå‹ï¼Œå¹¶é’ˆå¯¹ LLMs å’Œå›¾åƒåˆ†å‰²ä»»åŠ¡åœ¨ç”±è¾¹ç¼˜æœåŠ¡å™¨ã€åµŒå…¥å¼è®¾å¤‡å’Œå¾®å‹è®¡ç®—æœºç»„æˆçš„æµ‹è¯•åºŠä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEPARA åœ¨ç”Ÿäº§è´Ÿè½½ä¸‹çš„ goodput æ¯”ä»¥å¾€æ¡†æ¶æå‡äº†å¤šè¾¾ 2.1 å€ã€‚è¯¥æ¡†æ¶è¯æ˜äº†é€šè¿‡ä¼˜åŒ–çš„èµ„æºè°ƒåº¦å’Œä»»åŠ¡å¹¶è¡ŒåŒ–ï¼Œå¯ä»¥åœ¨ç°æœ‰ç¡¬ä»¶åŸºç¡€ä¸Šæ˜¾è‘—å¢å¼ºè¾¹ç¼˜ AI æ¨ç†çš„ååé‡ä¸é€‚é…æ€§ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.DC",
      "comment": "15 pages,20 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.00603v1",
      "published_date": "2025-11-01 16:09:14 UTC",
      "updated_date": "2025-11-01 16:09:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:09:24.153543+00:00"
    },
    {
      "arxiv_id": "2511.00588v2",
      "title": "Diagnosing Hallucination Risk in AI Surgical Decision-Support: A Sequential Framework for Sequential Validation",
      "title_zh": "è¯Šæ–­äººå·¥æ™ºèƒ½æ‰‹æœ¯å†³ç­–æ”¯æŒä¸­çš„å¹»è§‰é£é™©ï¼šä¸€ç§ç”¨äºåºè´¯éªŒè¯çš„åºè´¯æ¡†æ¶",
      "authors": [
        "Dong Chen",
        "Yanzhe Wei",
        "Zonglin He",
        "Guan-Ming Kuang",
        "Canhua Ye",
        "Meiru An",
        "Huili Peng",
        "Yong Hu",
        "Huiren Tao",
        "Kenneth MC Cheung"
      ],
      "abstract": "Large language models (LLMs) offer transformative potential for clinical decision support in spine surgery but pose significant risks through hallucinations, which are factually inconsistent or contextually misaligned outputs that may compromise patient safety. This study introduces a clinician-centered framework to quantify hallucination risks by evaluating diagnostic precision, recommendation quality, reasoning robustness, output coherence, and knowledge alignment. We assessed six leading LLMs across 30 expert-validated spinal cases. DeepSeek-R1 demonstrated superior overall performance (total score: 86.03 $\\pm$ 2.08), particularly in high-stakes domains such as trauma and infection. A critical finding reveals that reasoning-enhanced model variants did not uniformly outperform standard counterparts: Claude-3.7-Sonnet's extended thinking mode underperformed relative to its standard version (80.79 $\\pm$ 1.83 vs. 81.56 $\\pm$ 1.92), indicating extended chain-of-thought reasoning alone is insufficient for clinical reliability. Multidimensional stress-testing exposed model-specific vulnerabilities, with recommendation quality degrading by 7.4% under amplified complexity. This decline contrasted with marginal improvements in rationality (+2.0%), readability (+1.7%) and diagnosis (+4.7%), highlighting a concerning divergence between perceived coherence and actionable guidance. Our findings advocate integrating interpretability mechanisms (e.g., reasoning chain visualization) into clinical workflows and establish a safety-aware validation framework for surgical LLM deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è„ŠæŸ±å¤–ç§‘(spine surgery)ä¸´åºŠå†³ç­–æ”¯æŒä¸­å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å­˜åœ¨çš„å¹»è§‰(hallucinations)é£é™©ï¼Œæå‡ºäº†ä¸€ç§ä»¥ä¸´åºŠåŒ»ç”Ÿä¸ºä¸­å¿ƒçš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨ä»è¯Šæ–­ç²¾åº¦(diagnostic precision)ã€å»ºè®®è´¨é‡(recommendation quality)å’Œæ¨ç†ç¨³å¥æ€§(reasoning robustness)ç­‰ç»´åº¦é‡åŒ–é£é™©ã€‚é€šè¿‡å¯¹å…­ç§ä¸»æµLLMsåœ¨30ä¸ªä¸“å®¶éªŒè¯ç—…ä¾‹ä¸Šçš„è¯„ä¼°ï¼Œå‘ç°DeepSeek-R1åœ¨åˆ›ä¼¤å’Œæ„ŸæŸ“ç­‰é«˜é£é™©é¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œè€ŒClaude-3.7-Sonnetçš„æ‰©å±•æ€è€ƒæ¨¡å¼(extended thinking mode)åœ¨ä¸´åºŠå¯é æ€§ä¸Šå¹¶æœªä¼˜äºå…¶æ ‡å‡†ç‰ˆæœ¬ã€‚å‹åŠ›æµ‹è¯•æ­ç¤ºï¼Œå½“ç—…ä¾‹å¤æ‚åº¦å¢åŠ æ—¶ï¼Œæ¨¡å‹çš„å»ºè®®è´¨é‡æ˜¾è‘—ä¸‹é™ï¼Œä¸”åœ¨è¾“å‡ºçš„é€»è¾‘è¿è´¯æ€§(coherence)ä¸å®é™…å¯æ“ä½œçš„æŒ‡å¯¼å»ºè®®ä¹‹é—´å­˜åœ¨æ˜æ˜¾åˆ†æ­§ã€‚ç ”ç©¶å¼ºè°ƒå•çº¯çš„æ€ç»´é“¾(chain-of-thought)æ¨ç†ä¸è¶³ä»¥ç¡®ä¿ä¸´åºŠå®‰å…¨ï¼Œå¹¶å»ºè®®åœ¨å·¥ä½œæµä¸­é›†æˆæ¨ç†é“¾å¯è§†åŒ–(reasoning chain visualization)ç­‰å¯è§£é‡Šæ€§æœºåˆ¶(interpretability mechanisms)ï¼Œä¸ºæ‰‹æœ¯é¢†åŸŸLLMçš„éƒ¨ç½²å»ºç«‹å®‰å…¨æ„ŸçŸ¥çš„éªŒè¯æ ‡å‡†ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00588v2",
      "published_date": "2025-11-01 15:25:55 UTC",
      "updated_date": "2025-11-20 14:14:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:08:28.941568+00:00"
    },
    {
      "arxiv_id": "2511.00580v1",
      "title": "TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection",
      "title_zh": "TRACESï¼šç»“åˆä¸Šä¸‹æ–‡åµŒå…¥ä¸æ—¶åºå¬å›çš„å®æ—¶è§†é¢‘å¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Yousuf Ahmed Siddiqui",
        "Sufiyaan Usmani",
        "Umer Tariq",
        "Jawwad Ahmed Shamsi",
        "Muhammad Burhan Khan"
      ],
      "abstract": "Video anomalies often depend on contextual information available and temporal evolution. Non-anomalous action in one context can be anomalous in some other context. Most anomaly detectors, however, do not notice this type of context, which seriously limits their capability to generalize to new, real-life situations. Our work addresses the context-aware zero-shot anomaly detection challenge, in which systems need to learn adaptively to detect new events by correlating temporal and appearance features with textual traces of memory in real time. Our approach defines a memory-augmented pipeline, correlating temporal signals with visual embeddings using cross-attention, and real-time zero-shot anomaly classification by contextual similarity scoring. We achieve 90.4\\% AUC on UCF-Crime and 83.67\\% AP on XD-Violence, a new state-of-the-art among zero-shot models. Our model achieves real-time inference with high precision and explainability for deployment. We show that, by fusing cross-attention temporal fusion and contextual memory, we achieve high fidelity anomaly detection, a step towards the applicability of zero-shot models in real-world surveillance and infrastructure monitoring.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TRACESï¼Œä¸€ç§ç”¨äºå®æ—¶è§†é¢‘å¼‚å¸¸æ£€æµ‹çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥Zero-shotæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ£€æµ‹å™¨å› ç¼ºä¹ä¸Šä¸‹æ–‡ä¿¡æ¯è€Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›å—é™çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å®šä¹‰è®°å¿†å¢å¼ºç®¡é“ï¼Œåˆ©ç”¨Cross-attentionæœºåˆ¶å°†æ—¶é—´ä¿¡å·ä¸è§†è§‰åµŒå…¥è¿›è¡Œå…³è”ï¼Œå¹¶ç»“åˆä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦è¯„åˆ†å®ç°å®æ—¶å¼‚å¸¸åˆ†ç±»ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTRACESåœ¨UCF-Crimeæ•°æ®é›†ä¸Šè¾¾åˆ°90.4% AUCï¼Œåœ¨XD-Violenceæ•°æ®é›†ä¸Šè¾¾åˆ°83.67% APï¼Œåˆ›ä¸‹äº†Zero-shotæ¨¡å‹çš„æ–°State-of-the-artã€‚è¯¥æ¨¡å‹åœ¨ä¿æŒé«˜ç²¾åº¦å’Œå¯è§£é‡Šæ€§çš„åŒæ—¶å®ç°äº†å®æ—¶æ¨ç†ï¼Œæ˜¾è‘—æå‡äº†Zero-shotæ¨¡å‹åœ¨ç°å®ä¸–ç•Œç›‘æ§å’ŒåŸºç¡€è®¾æ–½ç›‘æµ‹åœºæ™¯ä¸­çš„é€‚ç”¨æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.00580v1",
      "published_date": "2025-11-01 14:54:08 UTC",
      "updated_date": "2025-11-01 14:54:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:08:35.148407+00:00"
    },
    {
      "arxiv_id": "2511.00576v1",
      "title": "FlashEVA: Accelerating LLM inference via Efficient Attention",
      "title_zh": "FlashEVAï¼šåŸºäºé«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†åŠ é€Ÿ",
      "authors": [
        "Juan Gabriel Kostelec",
        "Qinghai Guo"
      ],
      "abstract": "Transformer models have revolutionized natural language processing, achieving state-of-the-art performance and demonstrating remarkable scalability. However, their memory demands, particularly due to maintaining full context in memory, pose significant challenges for inference. In this paper, we present FlashEVA, an efficient implementation of EVA (Efficient Attention via Control Variates), and demonstrate how to finetune transformers to adapt to FlashEVA attention. Our method enables fine-tuning of Transformer models with as few as 1.5B tokens while preserving effectiveness across various downstream tasks. Notably, FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory usage during inference compared to standard Transformer implementations. Despite these improvements, we observe limitations in retrieval-focused tasks. Our implementation offers control over the trade-off between throughput and accuracy through adjustable hyperparameters, providing flexibility for diverse use cases. This work represents a significant step towards more efficient and adaptable Transformer-based models for inference.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FlashEVAï¼Œè¿™æ˜¯ EVA (Efficient Attention via Control Variates) çš„ä¸€ç§é«˜æ•ˆå®ç°ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–æ³¨æ„åŠ›æœºåˆ¶æ¥è§£å†³ Transformer æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„é«˜å†…å­˜éœ€æ±‚æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•å…è®¸ä»…ä½¿ç”¨ 1.5B tokens å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå³å¯ä½¿å…¶é€‚é… FlashEVA æ³¨æ„åŠ›ï¼Œå¹¶åœ¨å„ç±»ä¸‹æ¸¸ä»»åŠ¡ä¸­ä¿æŒè‰¯å¥½çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æ ‡å‡† Transformer å®ç°ç›¸æ¯”ï¼ŒFlashEVA åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„ååé‡æœ€é«˜å¯æå‡ 6.7 å€ï¼ŒåŒæ—¶å°†å³°å€¼ GPU å†…å­˜å ç”¨é™ä½äº† 5 å€ã€‚å°½ç®¡ç ”ç©¶è§‚å¯Ÿåˆ°è¯¥æ¨¡å‹åœ¨æ£€ç´¢ç±»ä»»åŠ¡ä¸­å­˜åœ¨ä¸€å®šå±€é™æ€§ï¼Œä½†å…¶æä¾›çš„å¯è°ƒè¶…å‚æ•°ä½¿ç”¨æˆ·èƒ½å¤Ÿæ ¹æ®éœ€æ±‚åœ¨ååé‡ä¸å‡†ç¡®æ€§ä¹‹é—´è¿›è¡Œçµæ´»æƒè¡¡ã€‚è¯¥é¡¹å·¥ä½œä¸ºå®ç°æ›´é«˜æ•ˆä¸”æ›´å…·é€‚åº”æ€§çš„ Transformer æ¨¡å‹æ¨ç†è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Technical Report",
      "pdf_url": "https://arxiv.org/pdf/2511.00576v1",
      "published_date": "2025-11-01 14:38:57 UTC",
      "updated_date": "2025-11-01 14:38:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:08:39.001028+00:00"
    },
    {
      "arxiv_id": "2511.00564v1",
      "title": "FTT-GRU: A Hybrid Fast Temporal Transformer with GRU for Remaining Useful Life Prediction",
      "title_zh": "FTT-GRUï¼šä¸€ç§ç”¨äºå‰©ä½™å¯¿å‘½é¢„æµ‹çš„å¿«é€Ÿæ—¶åŸŸ Transformer ä¸ GRU æ··åˆæ¨¡å‹",
      "authors": [
        "Varun Teja Chirukiri",
        "Udaya Bhasker Cheerala",
        "Sandeep Kanta",
        "Abdul Karim",
        "Praveen Damacharla"
      ],
      "abstract": "Accurate prediction of the remaining useful life (RUL) of industrial machinery is essential for reducing downtime and optimizing maintenance schedules. Existing approaches, such as long short-term memory (LSTM) networks and convolutional neural networks (CNNs), often struggle to model both global temporal dependencies and fine-grained degradation trends in multivariate sensor data. We propose a hybrid model, FTT-GRU, which combines a Fast Temporal Transformer (FTT) -- a lightweight Transformer variant using linearized attention via fast Fourier transform (FFT) -- with a gated recurrent unit (GRU) layer for sequential modeling. To the best of our knowledge, this is the first application of an FTT with a GRU for RUL prediction on NASA CMAPSS, enabling simultaneous capture of global and local degradation patterns in a compact architecture. On CMAPSS FD001, FTT-GRU attains RMSE 30.76, MAE 18.97, and $R^2=0.45$, with 1.12 ms CPU latency at batch=1. Relative to the best published deep baseline (TCN--Attention), it improves RMSE by 1.16\\% and MAE by 4.00\\%. Training curves averaged over $k=3$ runs show smooth convergence with narrow 95\\% confidence bands, and ablations (GRU-only, FTT-only) support the contribution of both components. These results demonstrate that a compact Transformer-RNN hybrid delivers accurate and efficient RUL predictions on CMAPSS, making it suitable for real-time industrial prognostics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FTT-GRUï¼Œä¸€ç§å°†Fast Temporal Transformer (FTT)ä¸Gated Recurrent Unit (GRU)ç›¸ç»“åˆçš„æ··åˆæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å·¥ä¸šæœºæ¢°Remaining Useful Life (RUL)é¢„æµ‹ä¸­éš¾ä»¥åŒæ—¶æ•æ‰å…¨å±€æ—¶é—´ä¾èµ–å’Œç»†ç²’åº¦é€€åŒ–è¶‹åŠ¿çš„é—®é¢˜ã€‚FTTä½œä¸ºä¸€ç§è½»é‡åŒ–çš„Transformerå˜ä½“ï¼Œé€šè¿‡Fast Fourier Transform (FFT)å®ç°çº¿æ€§åŒ–æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶é…åˆGRUå±‚è¿›è¡Œé«˜æ•ˆçš„åºåˆ—å»ºæ¨¡ã€‚è¯¥æ¶æ„æ˜¯é¦–æ¬¡åœ¨NASA CMAPSSæ•°æ®é›†ä¸Šåº”ç”¨FTTä¸GRUçš„ç»„åˆï¼Œå®ç°äº†å¯¹å…¨å±€å’Œå±€éƒ¨é€€åŒ–æ¨¡å¼çš„åŒæ­¥æ•æ‰ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFTT-GRUåœ¨CMAPSS FD001æ•°æ®é›†ä¸Šçš„RMSEä¸º30.76ï¼ŒMAEä¸º18.97ï¼Œä¸”åœ¨CPUä¸Šçš„æ¨ç†å»¶è¿Ÿä»…ä¸º1.12 msã€‚ç›¸æ¯”äºåŸºçº¿æ¨¡å‹TCN-Attentionï¼Œè¯¥æ¨¡å‹åœ¨RMSEå’ŒMAEä¸Šåˆ†åˆ«æå‡äº†1.16%å’Œ4.00%ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥éªŒè¯äº†FTTä¸GRUç»„ä»¶å¯¹é¢„æµ‹æ€§èƒ½çš„ååŒè´¡çŒ®ï¼Œè¯æ˜äº†è¿™ç§ç´§å‡‘çš„Transformer-RNNæ··åˆæ¨¡å‹åœ¨å®æ—¶å·¥ä¸šPrognosticsä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§ä¸é«˜æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, The 2025 International Conference on Computational Science and Computational Intelligence",
      "pdf_url": "https://arxiv.org/pdf/2511.00564v1",
      "published_date": "2025-11-01 14:02:03 UTC",
      "updated_date": "2025-11-01 14:02:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:08:54.002127+00:00"
    },
    {
      "arxiv_id": "2511.00554v1",
      "title": "Red-teaming Activation Probes using Prompted LLMs",
      "title_zh": "åˆ©ç”¨æç¤ºå‹å¤§è¯­è¨€æ¨¡å‹å¯¹æ¿€æ´»æ¢é’ˆè¿›è¡Œçº¢é˜Ÿæµ‹è¯•",
      "authors": [
        "Phil Blandfort",
        "Robert Graham"
      ],
      "abstract": "Activation probes are attractive monitors for AI systems due to low cost and latency, but their real-world robustness remains underexplored. We ask: What failure modes arise under realistic, black-box adversarial pressure, and how can we surface them with minimal effort? We present a lightweight black-box red-teaming procedure that wraps an off-the-shelf LLM with iterative feedback and in-context learning (ICL), and requires no fine-tuning, gradients, or architectural access. Running a case study with probes for high-stakes interactions, we show that our approach can help discover valuable insights about a SOTA probe. Our analysis uncovers interpretable brittleness patterns (e.g., legalese-induced FPs; bland procedural tone FNs) and reduced but persistent vulnerabilities under scenario-constraint attacks. These results suggest that simple prompted red-teaming scaffolding can anticipate failure patterns before deployment and might yield promising, actionable insights to harden future probes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Activation Probesåœ¨ç°å®ç¯å¢ƒä¸­çš„é²æ£’æ€§ä¸è¶³é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è½»é‡çº§çš„é»‘ç›’çº¢é˜Ÿæµ‹è¯•ï¼ˆRed-teamingï¼‰æµç¨‹ã€‚è¯¥æ–¹æ¡ˆåˆ©ç”¨ç°æˆçš„LLMsï¼Œç»“åˆè¿­ä»£åé¦ˆå’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆIn-Context Learning, ICLï¼‰ï¼Œåœ¨æ— éœ€æ¨¡å‹å¾®è°ƒã€æ¢¯åº¦è·å–æˆ–æ¶æ„è®¿é—®çš„å‰æä¸‹æ¢æµ‹ç³»ç»Ÿçš„å¤±æ•ˆæ¨¡å¼ã€‚é€šè¿‡å¯¹é«˜é£é™©äº¤äº’æ¢é’ˆçš„æ¡ˆä¾‹ç ”ç©¶ï¼Œç ”ç©¶è€…å‘ç°äº†å…·æœ‰å¯è§£é‡Šæ€§çš„è„†å¼±æ€§ç‰¹å¾ï¼Œä¾‹å¦‚æ³•å¾‹æœ¯è¯­å¼•èµ·çš„è¯¯æŠ¥ï¼ˆFalse Positivesï¼‰ä»¥åŠå¹³æ·¡ç¨‹åºæ€§è¯­æ°”å¯¼è‡´çš„æ¼æŠ¥ï¼ˆFalse Negativesï¼‰ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç®€å•çš„æç¤ºè¯ï¼ˆPromptedï¼‰çº¢é˜Ÿæµ‹è¯•æ¡†æ¶èƒ½åœ¨éƒ¨ç½²å‰æœ‰æ•ˆé¢„æµ‹æ•…éšœæ¨¡å¼ã€‚è¿™ä¸€å‘ç°ä¸ºå¢å¼ºæœªæ¥æ¢é’ˆçš„é˜²å¾¡èƒ½åŠ›æä¾›äº†å…·æœ‰å‚è€ƒä»·å€¼ä¸”å¯æ“ä½œçš„è§è§£ï¼Œæœ‰åŠ©äºæå‡AIç³»ç»Ÿçš„ç›‘æ§å¯é æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00554v1",
      "published_date": "2025-11-01 13:35:34 UTC",
      "updated_date": "2025-11-01 13:35:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:08:51.403045+00:00"
    },
    {
      "arxiv_id": "2511.00552v1",
      "title": "Temporal Fusion Transformer for Multi-Horizon Probabilistic Forecasting of Weekly Retail Sales",
      "title_zh": "ç”¨äºå‘¨åº¦é›¶å”®é¢å¤šæœŸæ¦‚ç‡é¢„æµ‹çš„æ—¶é—´èåˆ Transformer",
      "authors": [
        "Santhi Bharath Punati",
        "Sandeep Kanta",
        "Udaya Bhasker Cheerala",
        "Madhusudan G Lanjewar",
        "Praveen Damacharla"
      ],
      "abstract": "Accurate multi-horizon retail forecasts are critical for inventory and promotions. We present a novel study of weekly Walmart sales (45 stores, 2010--2012) using a Temporal Fusion Transformer (TFT) that fuses static store identifiers with time-varying exogenous signals (holidays, CPI, fuel price, temperature). The pipeline produces 1--5-week-ahead probabilistic forecasts via Quantile Loss, yielding calibrated 90\\% prediction intervals and interpretability through variable-selection networks, static enrichment, and temporal attention. On a fixed 2012 hold-out dataset, TFT achieves an RMSE of \\$57.9k USD per store-week and an $R^2$ of 0.9875. Across a 5-fold chronological cross-validation, the averages are RMSE = \\$64.6k USD and $R^2$ = 0.9844, outperforming the XGB, CNN, LSTM, and CNN-LSTM baseline models. These results demonstrate practical value for inventory planning and holiday-period optimization, while maintaining model transparency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é›¶å”®ä¸šå‘¨é”€å”®é¢çš„å¤šæ­¥é¢„æµ‹æŒ‘æˆ˜ï¼Œæå‡ºå¹¶è¯„ä¼°äº†Temporal Fusion Transformer (TFT) æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ—¶é—´åºåˆ—ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶ä»¥Walmartçš„é—¨åº—æ•°æ®ä¸ºåŸºç¡€ï¼Œèåˆäº†é—¨åº—é™æ€æ ‡è¯†ä¸å‡æœŸã€CPIã€ç‡ƒæ²¹ä»·æ ¼å’Œæ°”æ¸©ç­‰åŠ¨æ€å¤–éƒ¨å˜é‡ï¼Œé€šè¿‡Quantile Losså®ç°äº†1è‡³5å‘¨çš„æ¦‚ç‡æ€§é¢„æµ‹ã€‚TFTæ¡†æ¶åˆ©ç”¨å˜é‡é€‰æ‹©ç½‘ç»œ(variable-selection networks)å’Œæ—¶é—´æ³¨æ„åŠ›æœºåˆ¶(temporal attention) ç¡®ä¿äº†é¢„æµ‹è¿‡ç¨‹çš„é«˜é€æ˜åº¦ä¸å¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTFTåœ¨2012å¹´æµ‹è¯•é›†ä¸Šçš„$R^2$è¾¾åˆ°0.9875ï¼Œä¸”åœ¨5æŠ˜äº¤å‰éªŒè¯ä¸­æŒç»­ä¼˜äºXGBã€CNNå’ŒLSTMç­‰å¤šç§åŸºçº¿æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç”Ÿæˆçš„90%é¢„æµ‹åŒºé—´èƒ½å¤Ÿæœ‰æ•ˆè¾…åŠ©åº“å­˜ç®¡ç†ä¸ä¿ƒé”€æ´»åŠ¨å†³ç­–ï¼Œè¯æ˜äº†å…¶åœ¨é›¶å”®ä¸šåŠ¡ä¼˜åŒ–ä¸­çš„å®é™…åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "econ.GN"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 2025 6th International Conference on Data Analytics for Business and Industry (ICDABI)",
      "pdf_url": "https://arxiv.org/pdf/2511.00552v1",
      "published_date": "2025-11-01 13:34:29 UTC",
      "updated_date": "2025-11-01 13:34:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:09:36.743496+00:00"
    },
    {
      "arxiv_id": "2511.00551v2",
      "title": "Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control",
      "title_zh": "åŒºåŸŸè‡ªé€‚åº”äº¤é€šä¿¡å·æ§åˆ¶çš„å•æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¨¡å‹",
      "authors": [
        "Qiang Li",
        "Ningjing Zeng",
        "Lina Yu"
      ],
      "abstract": "Several studies have employed reinforcement learning (RL) to address the challenges of regional adaptive traffic signal control (ATSC) and achieved promising results. In this field, existing research predominantly adopts multi-agent frameworks. However, the adoption of multi-agent frameworks presents challenges for scalability. Instead, the Traffic signal control (TSC) problem necessitates a single-agent framework. TSC inherently relies on centralized management by a single control center, which can monitor traffic conditions across all roads in the study area and coordinate the control of all intersections. This work proposes a single-agent RL-based regional ATSC model compatible with probe vehicle technology. Key components of the RL design include state, action, and reward function definitions. To facilitate learning and manage congestion, both state and reward functions are defined based on queue length, with action designed to regulate queue dynamics. The queue length definition used in this study differs slightly from conventional definitions but is closely correlated with congestion states. More importantly, it allows for reliable estimation using link travel time data from probe vehicles. With probe vehicle data already covering most urban roads, this feature enhances the proposed method's potential for widespread deployment. The method was comprehensively evaluated using the SUMO simulation platform. Experimental results demonstrate that the proposed model effectively mitigates large-scale regional congestion levels via coordinated multi-intersection control.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒºåŸŸè‡ªé€‚åº”äº¤é€šä¿¡å·æ§åˆ¶ (Adaptive Traffic Signal Control, ATSC) é¢†åŸŸä¸­å¤šæ™ºèƒ½ä½“æ¡†æ¶åœ¨å¯æ‰©å±•æ€§æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå•æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (Single-agent RL) çš„æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç¬¦åˆäº¤é€šä¿¡å·æ§åˆ¶ (Traffic Signal Control, TSC) å›ºæœ‰çš„é›†ä¸­å¼ç®¡ç†éœ€æ±‚ï¼Œé€šè¿‡å•ä¸€æ§åˆ¶ä¸­å¿ƒç»Ÿä¸€ç›‘æ§å¹¶åè°ƒåŒºåŸŸå†…æ‰€æœ‰äº¤å‰å£çš„æ§åˆ¶é€»è¾‘ã€‚åœ¨å¼ºåŒ–å­¦ä¹ è®¾è®¡ä¸­ï¼ŒçŠ¶æ€ (state) å’Œå¥–åŠ± (reward) å‡½æ•°å‡åŸºäºæ’é˜Ÿé•¿åº¦ (queue length) è¿›è¡Œå®šä¹‰ï¼Œå¹¶åˆ©ç”¨æ¢æµ‹è½¦ (probe vehicle) çš„è·¯æ®µæ—…è¡Œæ—¶é—´æ•°æ®è¿›è¡Œå¯é ä¼°ç®—ï¼Œè¿™æ˜¾è‘—å¢å¼ºäº†è¯¥æ–¹æ³•åœ¨ç°å®åŸå¸‚é“è·¯ä¸­çš„éƒ¨ç½²æ½œåŠ›ã€‚å®éªŒåœ¨ SUMO ä»¿çœŸå¹³å°ä¸Šè¿›è¡Œï¼Œç»“æœè¡¨æ˜è¯¥æ¨¡å‹èƒ½å¤Ÿé€šè¿‡åè°ƒå¤šäº¤å‰å£æ§åˆ¶ï¼Œæœ‰æ•ˆç¼“è§£å¤§è§„æ¨¡åŒºåŸŸçš„äº¤é€šæ‹¥å µæ°´å¹³ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "A critical error in the methodology. The reported congestion control effects were not caused by the proposed signal timing optimization, but by an incorrect traffic volume scaling factor during evaluation. The traffic demand was not properly amplified, resulting in misleading performance gains. Due to the substantial nature of the error, completion of revisions is not feasible in the short term",
      "pdf_url": "https://arxiv.org/pdf/2511.00551v2",
      "published_date": "2025-11-01 13:29:13 UTC",
      "updated_date": "2026-01-12 23:21:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:09:39.952491+00:00"
    },
    {
      "arxiv_id": "2511.00549v2",
      "title": "Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations",
      "title_zh": "éœ€æ±‚æ³¢åŠ¨ä¸‹åŸºäºé²æ£’å•æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„åŒºåŸŸäº¤é€šä¿¡å·æ§åˆ¶",
      "authors": [
        "Qiang Li",
        "Jin Niu",
        "Lina Yu"
      ],
      "abstract": "Traffic congestion, primarily driven by intersection queuing, significantly impacts urban living standards, safety, environmental quality, and economic efficiency. While Traffic Signal Control (TSC) systems hold potential for congestion mitigation, traditional optimization models often fail to capture real-world traffic complexity and dynamics. This study introduces a novel single-agent reinforcement learning (RL) framework for regional adaptive TSC, circumventing the coordination complexities inherent in multi-agent systems through a centralized decision-making paradigm. The model employs an adjacency matrix to unify the encoding of road network topology, real-time queue states derived from probe vehicle data, and current signal timing parameters. Leveraging the efficient learning capabilities of the DreamerV3 world model, the agent learns control policies where actions sequentially select intersections and adjust their signal phase splits to regulate traffic inflow/outflow, analogous to a feedback control system. Reward design prioritizes queue dissipation, directly linking congestion metrics (queue length) to control actions. Simulation experiments conducted in SUMO demonstrate the model's effectiveness: under inference scenarios with multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the framework exhibits robust anti-fluctuation capability and significantly reduces queue lengths. This work establishes a new paradigm for intelligent traffic control compatible with probe vehicle technology. Future research will focus on enhancing practical applicability by incorporating stochastic OD demand fluctuations during training and exploring regional optimization mechanisms for contingency events.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºåŒºåŸŸè‡ªé€‚åº” Traffic Signal Control (TSC) çš„å•æ™ºèƒ½ä½“ Reinforcement Learning (RL) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿä¼˜åŒ–æ¨¡å‹éš¾ä»¥åº”å¯¹ç°å®äº¤é€šåŠ¨æ€å¤æ‚æ€§çš„é—®é¢˜ã€‚ä¸ºäº†è§„é¿ Multi-Agent ç³»ç»Ÿå›ºæœ‰çš„åè°ƒå¤æ‚æ€§ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨ä¸­å¿ƒåŒ–å†³ç­–èŒƒå¼ï¼Œå¹¶åˆ©ç”¨ Adjacency Matrix ç»Ÿä¸€ç¼–ç è·¯ç½‘æ‹“æ‰‘ã€æ¢æµ‹è½¦å®æ—¶æ’é˜ŸçŠ¶æ€åŠä¿¡å·é…æ—¶å‚æ•°ã€‚æ¨¡å‹åˆ©ç”¨é«˜æ•ˆçš„ DreamerV3 ä¸–ç•Œæ¨¡å‹å­¦ä¹ æ§åˆ¶ç­–ç•¥ï¼Œé€šè¿‡åºåˆ—åŒ–è°ƒèŠ‚äº¤å‰å£ä¿¡å·ç›¸ä½å·®å®ç°äº¤é€šæµçš„åé¦ˆæ§åˆ¶ï¼Œå¹¶ä»¥ Queue Dissipation ä½œä¸ºå¥–åŠ±è®¾è®¡çš„æ ¸å¿ƒæŒ‡æ ‡ã€‚åœ¨ SUMO ä»¿çœŸå®éªŒä¸­ï¼Œè¯¥æ¡†æ¶åœ¨ 10% åˆ° 30% çš„éœ€æ±‚æ³¢åŠ¨åœºæ™¯ä¸‹å±•ç°äº†å“è¶Šçš„ Robustnessï¼Œæ˜¾è‘—é™ä½äº†æ’é˜Ÿé•¿åº¦ã€‚è¯¥å·¥ä½œä¸ºå…¼å®¹ Probe Vehicle æŠ€æœ¯çš„æ™ºèƒ½äº¤é€šæ§åˆ¶å»ºç«‹äº†æ–°èŒƒå¼ï¼Œå¹¶è¯æ˜äº†å…¶åœ¨å¤„ç†åŒºåŸŸäº¤é€šä¼˜åŒ–ä¸­çš„å®ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "A critical error in the methodology. The reported congestion control effects were not caused by the proposed signal timing optimization, but by an incorrect traffic volume scaling factor during evaluation. The traffic demand was not properly amplified, resulting in misleading performance gains. Due to the substantial nature of the error, completion of revisions is not feasible in the short term",
      "pdf_url": "https://arxiv.org/pdf/2511.00549v2",
      "published_date": "2025-11-01 13:18:50 UTC",
      "updated_date": "2026-01-12 23:22:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:09:48.012790+00:00"
    },
    {
      "arxiv_id": "2511.00547v1",
      "title": "Efficient Generation of Binary Magic Squares",
      "title_zh": "äºŒè¿›åˆ¶å¹»æ–¹çš„é«˜æ•ˆç”Ÿæˆ",
      "authors": [
        "Alain Riou"
      ],
      "abstract": "We propose a simple algorithm for generating Binary Magic Squares (BMS), i.e., square binary matrices where the sum of all rows and all columns are equal. We show by induction that our algorithm always returns valid BMS with optimal theoretical complexity. We then extend our study to non-square Binary Magic Squares, formalize conditions on the sum of rows and columns for these BMS to exist, and show that a slight variant of our first algorithm can generate provably generate them. Finally, we publicly release two implementations of our algorithm as Python packages, including one that can generate several BMS in parallel using GPU acceleration.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Binary Magic Squares (BMS) çš„ç”Ÿæˆé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç®€å•é«˜æ•ˆçš„ç®—æ³•ï¼Œç”¨äºæ„å»ºè¡Œå’Œä¸åˆ—å’Œç›¸ç­‰çš„æ–¹é˜µäºŒè¿›åˆ¶çŸ©é˜µã€‚é€šè¿‡å½’çº³æ³•ï¼Œç ”ç©¶è¯æ˜äº†è¯¥ç®—æ³•ç”Ÿæˆçš„BMSå§‹ç»ˆæœ‰æ•ˆï¼Œä¸”å…·å¤‡ç†è®ºä¸Šçš„æœ€ä¼˜å¤æ‚åº¦(Optimal theoretical complexity)ã€‚éšåï¼Œç ”ç©¶å°†è®¨è®ºæ‰©å±•è‡³éæ­£æ–¹å½¢çš„Binary Magic Squaresï¼Œå½¢å¼åŒ–å®šä¹‰äº†æ­¤ç±»çŸ©é˜µå­˜åœ¨çš„è¡Œå’Œä¸åˆ—å’Œçº¦æŸæ¡ä»¶ï¼Œå¹¶è¯æ˜äº†å…¶ç®—æ³•å˜ä½“èƒ½å¤Ÿæœ‰æ•ˆç”Ÿæˆè¿™äº›çŸ©é˜µã€‚æœ€åï¼Œç ”ç©¶å…¬å¼€äº†è¯¥ç®—æ³•çš„ä¸¤ç§Pythonå®ç°ç‰ˆæœ¬ï¼Œå…¶ä¸­ä¸€ä¸ªç‰ˆæœ¬æ”¯æŒé€šè¿‡GPUåŠ é€Ÿè¿›è¡Œå¹¶è¡Œç”Ÿæˆï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—æ€§èƒ½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00547v1",
      "published_date": "2025-11-01 13:15:22 UTC",
      "updated_date": "2025-11-01 13:15:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:09:44.683585+00:00"
    },
    {
      "arxiv_id": "2511.00532v1",
      "title": "Air Pollution Forecasting in Bucharest",
      "title_zh": "Bucharest ç©ºæ°”æ±¡æŸ“é¢„æµ‹",
      "authors": [
        "DragoÅŸ-Andrei Åerban",
        "RÄƒzvan-Alexandru SmÄƒdu",
        "Dumitru-Clementin Cercel"
      ],
      "abstract": "Air pollution, especially the particulate matter 2.5 (PM2.5), has become a growing concern in recent years, primarily in urban areas. Being exposed to air pollution is linked to developing numerous health problems, like the aggravation of respiratory diseases, cardiovascular disorders, lung function impairment, and even cancer or early death. Forecasting future levels of PM2.5 has become increasingly important over the past few years, as it can provide early warnings and help prevent diseases. This paper aims to design, fine-tune, test, and evaluate machine learning models for predicting future levels of PM2.5 over various time horizons. Our primary objective is to assess and compare the performance of multiple models, ranging from linear regression algorithms and ensemble-based methods to deep learning models, such as advanced recurrent neural networks and transformers, as well as large language models, on this forecasting task.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¸ƒåŠ å‹’æ–¯ç‰¹(Bucharest)æ—¥ç›Šä¸¥é‡çš„åŸå¸‚ç©ºæ°”æ±¡æŸ“é—®é¢˜ï¼Œé‡ç‚¹å…³æ³¨ç»†é¢—ç²’ç‰© PM2.5 çš„é¢„æµ‹ï¼Œä»¥æœŸä¸ºç›¸å…³ç–¾ç—…çš„é¢„é˜²æä¾›æ—©æœŸé¢„è­¦ã€‚ç ”ç©¶æ—¨åœ¨è®¾è®¡ã€å¾®è°ƒå¹¶è¯„ä¼°å¤šç§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä»¥å®ç°åœ¨ä¸åŒæ—¶é—´è·¨åº¦ä¸‹å¯¹ PM2.5 æ°´å¹³çš„ç²¾å‡†é¢„æµ‹ã€‚åœ¨å®éªŒè¿‡ç¨‹ä¸­ï¼Œç ”ç©¶è€…ç³»ç»Ÿåœ°è¯„ä¼°å¹¶æ¯”è¾ƒäº†ä»åŸºç¡€çš„ linear regression ç®—æ³•ã€é›†æˆå­¦ä¹ æ–¹æ³•(ensemble-based methods)åˆ°å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¦‚ recurrent neural networks å’Œ transformers çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å°†å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åº”ç”¨äºè¯¥é¢„æµ‹ä»»åŠ¡ï¼Œæ¢è®¨å…¶åœ¨ç¯å¢ƒç›‘æµ‹é¢†åŸŸçš„æ½œåŠ›ã€‚é€šè¿‡å¯¹ä¸åŒç±»åˆ«æ¨¡å‹çš„æ·±å…¥åˆ†æï¼Œè¯¥è®ºæ–‡ä¸ºæ„å»ºé«˜æ•ˆçš„åŸå¸‚ç©ºæ°”è´¨é‡é¢„æŠ¥ç³»ç»Ÿæä¾›äº†é‡è¦çš„å®è¯å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.00532v1",
      "published_date": "2025-11-01 12:24:11 UTC",
      "updated_date": "2025-11-01 12:24:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:09:48.347275+00:00"
    },
    {
      "arxiv_id": "2511.00529v2",
      "title": "On Improvisation and Open-Endedness: Insights for Experiential AI",
      "title_zh": "è®ºå³å…´ä¸å¼€æ”¾æ€§ï¼šå¯¹ä½“éªŒå¼äººå·¥æ™ºèƒ½çš„å¯ç¤º",
      "authors": [
        "Botao 'Amber' Hu"
      ],
      "abstract": "Improvisation-the art of spontaneous creation that unfolds moment-to-moment without a scripted outcome-requires practitioners to continuously sense, adapt, and create anew. It is a fundamental mode of human creativity spanning music, dance, and everyday life. The open-ended nature of improvisation produces a stream of novel, unrepeatable moments-an aspect highly valued in artistic creativity. In parallel, open-endedness (OE)-a system's capacity for unbounded novelty and endless \"interestingness\"-is exemplified in natural or cultural evolution and has been considered \"the last grand challenge\" in artificial life (ALife). The rise of generative AI now raises the question in computational creativity (CC) research: What makes a \"good\" improvisation for AI? Can AI learn to improvise in a genuinely open-ended way? In this work-in-progress paper, we report insights from in-depth interviews with 6 experts in improvisation across dance, music, and contact improvisation. We draw systemic connections between human improvisational arts and the design of future experiential AI agents that could improvise alone or alongside humans-or even with other AI agents-embodying qualities of improvisation drawn from practice: active listening (umwelt and awareness), being in the time (mindfulness and ephemerality), embracing the unknown (source of randomness and serendipity), non-judgmental flow (acceptance and dynamical stability, balancing structure and surprise (unpredictable criticality at edge of chaos), imaginative metaphor (synaesthesia and planning), empathy, trust, boundary, and care (mutual theory of mind), and playfulness and intrinsic motivation (maintaining interestingness).",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å³å…´åˆ›ä½œ(Improvisation)ä¸å¼€æ”¾å¼æ¼”åŒ–(Open-Endedness)ä¹‹é—´çš„å†…åœ¨è”ç³»ï¼Œæ—¨åœ¨ä¸ºä½“éªŒå¼äººå·¥æ™ºèƒ½(Experiential AI)çš„è®¾è®¡æä¾›æ´å¯Ÿã€‚é€šè¿‡å¯¹6ä½èˆè¹ˆã€éŸ³ä¹åŠæ¥è§¦å³å…´é¢†åŸŸçš„ä¸“å®¶è¿›è¡Œæ·±åº¦è®¿è°ˆï¼Œè®ºæ–‡åˆ†æäº†AIåœ¨ç¼ºä¹é¢„è®¾å‰§æœ¬çš„æƒ…å†µä¸‹è¿›è¡Œè‡ªå‘åˆ›ä½œå¹¶ç»´æŒâ€œæ— é™è¶£å‘³æ€§â€çš„å¯èƒ½æ€§ã€‚ç ”ç©¶ç¡®ç«‹äº†å°†äººç±»å³å…´ç‰¹è´¨è½¬åŒ–ä¸ºAIæ™ºèƒ½ä½“è®¾è®¡çš„ç³»ç»Ÿæ€§å…³è”ï¼Œé‡ç‚¹åŒ…æ‹¬ä¸»åŠ¨è†å¬(active listening)ã€æ´»åœ¨å½“ä¸‹(being in the time)ã€æ‹¥æŠ±æœªçŸ¥ä»¥åŠåœ¨ç»“æ„ä¸æƒŠå–œ(balancing structure and surprise)ä¹‹é—´å¯»æ‰¾åŠ¨æ€å¹³è¡¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼ºè°ƒäº†åŒç†å¿ƒ(empathy)ã€ä¿¡ä»»(trust)å’Œå†…åœ¨åŠ¨æœº(intrinsic motivation)å¯¹äºæ„å»ºèƒ½å¤Ÿä¸äººç±»æˆ–å…¶å®ƒæ™ºèƒ½ä½“åä½œçš„AIç³»ç»Ÿçš„é‡è¦æ€§ã€‚è¿™äº›å‘ç°ä¸ºè§£å†³äººå·¥ç”Ÿå‘½(ALife)é¢†åŸŸä¸­å…³äºæŒç»­äº§ç”Ÿæ–°é¢–æ€§çš„æŒ‘æˆ˜æä¾›äº†å…³é”®çš„å®è·µå‡†åˆ™ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.NE",
        "eess.SY"
      ],
      "primary_category": "cs.HC",
      "comment": "Submitted to AAAI 2026 Creative AI for Live Interactive Performances Workshop (CLIP) as a work-in-progress paper",
      "pdf_url": "https://arxiv.org/pdf/2511.00529v2",
      "published_date": "2025-11-01 12:13:02 UTC",
      "updated_date": "2025-11-05 06:09:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:09:53.558953+00:00"
    },
    {
      "arxiv_id": "2511.00527v1",
      "title": "HIP-LLM: A Hierarchical Imprecise Probability Approach to Reliability Assessment of Large Language Models",
      "title_zh": "HIP-LLMï¼šä¸€ç§ç”¨äºå¤§è¯­è¨€æ¨¡å‹å¯é æ€§è¯„ä¼°çš„å±‚çº§ä¸ç²¾ç¡®æ¦‚ç‡æ–¹æ³•",
      "authors": [
        "Robab Aghazadeh-Chakherlou",
        "Qing Guo",
        "Siddartha Khastgir",
        "Peter Popov",
        "Xiaoge Zhang",
        "Xingyu Zhao"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed across diverse domains, raising the need for rigorous reliability assessment methods. Existing benchmark-based evaluations primarily offer descriptive statistics of model accuracy over datasets, providing limited insight into the probabilistic behavior of LLMs under real operational conditions. This paper introduces HIP-LLM, a Hierarchical Imprecise Probability framework for modeling and inferring LLM reliability. Building upon the foundations of software reliability engineering, HIP-LLM defines LLM reliability as the probability of failure-free operation over a specified number of future tasks under a given Operational Profile (OP). HIP-LLM represents dependencies across (sub-)domains hierarchically, enabling multi-level inference from subdomain to system-level reliability. HIP-LLM embeds imprecise priors to capture epistemic uncertainty and incorporates OPs to reflect usage contexts. It derives posterior reliability envelopes that quantify uncertainty across priors and data. Experiments on multiple benchmark datasets demonstrate that HIP-LLM offers a more accurate and standardized reliability characterization than existing benchmark and state-of-the-art approaches. A publicly accessible repository of HIP-LLM is provided.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨å„é¢†åŸŸå¹¿æ³›åº”ç”¨ä¸­çš„å¯é æ€§è¯„ä¼°éš¾é¢˜ï¼Œæå‡ºäº†HIP-LLMï¼Œä¸€ç§åŸºäºåˆ†å±‚ä¸ç²¾ç¡®æ¦‚ç‡(Hierarchical Imprecise Probability)çš„è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶å€Ÿé‰´è½¯ä»¶å¯é æ€§å·¥ç¨‹ç†å¿µï¼Œå°†LLM Reliabilityå®šä¹‰ä¸ºåœ¨ç»™å®šè¿è¡Œå‰–é¢(Operational Profile, OP)ä¸‹ï¼Œæ¨¡å‹åœ¨æœªæ¥ç‰¹å®šæ•°é‡ä»»åŠ¡ä¸­æ— æ•…éšœè¿è¡Œçš„æ¦‚ç‡ã€‚HIP-LLMé€šè¿‡å±‚æ¬¡åŒ–ç»“æ„å»ºæ¨¡(sub-)domainsé—´çš„ä¾èµ–å…³ç³»ï¼Œå¹¶åˆ©ç”¨ä¸ç²¾ç¡®å…ˆéªŒ(Imprecise Priors)æ¥æ•æ‰è®¤çŸ¥ä¸ç¡®å®šæ€§(Epistemic Uncertainty)ï¼Œä»è€Œå¯¼å‡ºèƒ½å¤Ÿé‡åŒ–ä¸ç¡®å®šæ€§çš„åéªŒå¯é æ€§åŒ…ç»œã€‚å®éªŒè¯æ˜ï¼ŒHIP-LLMåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡å±•ç°å‡ºæ¯”ç°æœ‰åŸºå‡†å’Œæœ€å…ˆè¿›æ–¹æ³•æ›´å‡†ç¡®ä¸”æ ‡å‡†åŒ–çš„å¯é æ€§è¡¨å¾èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºç†è§£LLMsåœ¨çœŸå®è¿è¡Œæ¡ä»¶ä¸‹çš„æ¦‚ç‡è¡Œä¸ºæä¾›äº†ä¸¥è°¨çš„æ•°å­¦å·¥å…·å’Œæ¨æ–­æ–¹æ³•ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "under review",
      "pdf_url": "https://arxiv.org/pdf/2511.00527v1",
      "published_date": "2025-11-01 12:04:30 UTC",
      "updated_date": "2025-11-01 12:04:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:09:57.755797+00:00"
    },
    {
      "arxiv_id": "2511.00521v2",
      "title": "Reasoning Planning for Language Models",
      "title_zh": "è¯­è¨€æ¨¡å‹çš„æ¨ç†è§„åˆ’",
      "authors": [
        "Bao Nguyen",
        "Hieu Trung Nguyen",
        "Ruifeng She",
        "Xiaojin Fu",
        "Viet Anh Nguyen"
      ],
      "abstract": "Selecting an appropriate reasoning method for a given query remains a key challenge in language model generation. Existing approaches typically generate multiple candidate responses and use an aggregation strategy to select the output answer, often assuming that more candidate answers yield higher accuracy. We revisit this assumption through a rigorous theoretical analysis, deriving accuracy bounds for standard aggregation methods under fixed generation distributions and candidate sizes. Building on these insights, we introduce EPIC, an Ensemble Planning with Contrastive learning framework to learn a shared representation space that captures both model reasoning abilities and query-method compatibility. EPIC incorporates our probability bounds as a regularizer in a utility-driven optimization that balances accuracy and computational cost. Experiments on diverse mathematical reasoning tasks show that EPIC consistently selects optimal reasoning methods, improving accuracy while reducing computational overhead. Our code can be found at https://github.com/nguyenngocbaocmt02/EPIC.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸ºè¯­è¨€æ¨¡å‹é€‰æ‹©åˆé€‚æ¨ç†æ–¹æ³•çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾å¢åŠ å€™é€‰ç­”æ¡ˆæ•°é‡å³å¯æé«˜å‡†ç¡®ç‡çš„ç°çŠ¶ã€‚ä½œè€…é€šè¿‡ä¸¥æ ¼çš„ç†è®ºåˆ†æé‡æ–°å®¡è§†äº†è¿™ä¸€å‡è®¾ï¼Œå¹¶æ¨å¯¼å‡ºäº†åœ¨å›ºå®šç”Ÿæˆåˆ†å¸ƒå’Œå€™é€‰è§„æ¨¡ä¸‹æ ‡å‡†èšåˆæ–¹æ³•çš„å‡†ç¡®ç‡ç•Œé™ã€‚åŸºäºè¿™äº›ç†è®ºè§è§£ï¼Œç ”ç©¶æå‡ºäº† EPIC (Ensemble Planning with Contrastive learning) æ¡†æ¶ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ æ„å»ºå…±äº«è¡¨ç¤ºç©ºé—´ï¼Œä»¥æ•æ‰æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸æŸ¥è¯¢-æ–¹æ³•ä¹‹é—´çš„å…¼å®¹æ€§ã€‚EPIC å°†æ¨å¯¼å‡ºçš„æ¦‚ç‡ç•Œé™ä½œä¸ºæ­£åˆ™åŒ–é¡¹å¼•å…¥æ•ˆç”¨é©±åŠ¨çš„ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œæ—¨åœ¨å¹³è¡¡å‡†ç¡®ç‡ä¸è®¡ç®—æˆæœ¬ã€‚åœ¨å¤šç§æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒEPIC èƒ½å¤ŸæŒç»­é€‰æ‹©æœ€ä¼˜çš„æ¨ç†æ–¹æ³•ã€‚è¯¥æ¡†æ¶åœ¨æ˜¾è‘—æé«˜æ¨¡å‹å‡†ç¡®ç‡çš„åŒæ—¶é™ä½äº†è®¡ç®—å¼€é”€ï¼Œä¸ºé«˜æ•ˆçš„æ¨ç†è§„åˆ’æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "27 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.00521v2",
      "published_date": "2025-11-01 11:51:53 UTC",
      "updated_date": "2025-11-10 03:30:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:09:56.319336+00:00"
    },
    {
      "arxiv_id": "2511.00509v1",
      "title": "Reimagining Safety Alignment with An Image",
      "title_zh": "ä»¥å›¾åƒé‡å¡‘å®‰å…¨å¯¹é½",
      "authors": [
        "Yifan Xia",
        "Guorui Chen",
        "Wenqian Yu",
        "Zhijiang Li",
        "Philip Torr",
        "Jindong Gu"
      ],
      "abstract": "Large language models (LLMs) excel in diverse applications but face dual challenges: generating harmful content under jailbreak attacks and over-refusal of benign queries due to rigid safety mechanisms. These issues are further complicated by the need to accommodate different value systems and precisely align with given safety preferences. Moreover, traditional methods like SFT and RLHF lack this capability due to their costly parameter tuning requirements and inability to support multiple value systems within a single model. These problems are more obvious in multimodal large language models (MLLMs), especially in terms of heightened over-refusal in cross-modal tasks and new security risks arising from expanded attack surfaces. We propose Magic Image, an optimization-driven visual prompt framework that enhances security while reducing over-refusal. By optimizing image prompts using harmful/benign samples, our method enables a single model to adapt to different value systems and better align with given safety preferences without parameter updates. Experiments demonstrate improved safety-effectiveness balance across diverse datasets while preserving model performance, offering a practical solution for deployable MLLM safety alignment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Magic Imageï¼Œè¿™æ˜¯ä¸€ç§ä¼˜åŒ–é©±åŠ¨çš„è§†è§‰æç¤º (visual prompt) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨å®‰å…¨å¯¹é½ä¸­é¢ä¸´çš„è¶Šç‹±æ”»å‡»é£é™©ä¸è‰¯æ€§æŸ¥è¯¢è¿‡åº¦æ‹’ç» (over-refusal) ä¹‹é—´çš„çŸ›ç›¾ã€‚é’ˆå¯¹ SFT å’Œ RLHF ç­‰ä¼ ç»Ÿæ–¹æ³•åœ¨é€‚é…å¤šä»·å€¼ç³»ç»Ÿæ—¶çš„é«˜æ˜‚å‚æ•°å¾®è°ƒæˆæœ¬ï¼Œè¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨æœ‰å®³å’Œè‰¯æ€§æ ·æœ¬ä¼˜åŒ–å›¾åƒæç¤ºï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ— éœ€æ›´æ–°å‚æ•°çš„æƒ…å†µä¸‹ç²¾å‡†å¯¹é½ç‰¹å®šçš„å®‰å…¨åå¥½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMagic Image åœ¨å¤šç§æ•°æ®é›†ä¸Šå®ç°äº†æ›´ä¼˜çš„å®‰å…¨ä¸æ•ˆèƒ½å¹³è¡¡ï¼Œæœ‰æ•ˆç¼“è§£äº†è·¨æ¨¡æ€ä»»åŠ¡ä¸­çš„è¿‡åº¦æ‹’ç»é—®é¢˜ã€‚è¯¥æ–¹æ¡ˆä¸ä»…ä¿ç•™äº†æ¨¡å‹çš„åŸå§‹æ€§èƒ½ï¼Œè¿˜ä¸ºå®ç°å¯çµæ´»éƒ¨ç½²çš„ MLLM å®‰å…¨å¯¹é½æä¾›äº†ä¸€ç§å®ç”¨ä¸”é«˜æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00509v1",
      "published_date": "2025-11-01 11:27:07 UTC",
      "updated_date": "2025-11-01 11:27:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:10:04.713862+00:00"
    },
    {
      "arxiv_id": "2511.00494v2",
      "title": "An Indoor Radio Mapping Dataset Combining 3D Point Clouds and RSSI",
      "title_zh": "èåˆ3Dç‚¹äº‘ä¸RSSIçš„å®¤å†…æ— çº¿ç”µåˆ¶å›¾æ•°æ®é›†",
      "authors": [
        "Ljupcho Milosheski",
        "Kuon Akiyama",
        "BlaÅ¾ BertalaniÄ",
        "Jernej Hribar",
        "Ryoichi Shinkuma"
      ],
      "abstract": "The growing number of smart devices supporting bandwidth-intensive and latency-sensitive applications, such as real-time video analytics, smart sensing, Extended Reality (XR), etc., necessitates reliable wireless connectivity in indoor environments. In such environments, accurate design of Radio Environment Maps (REMs) enables adaptive wireless network planning and optimization of Access Point (AP) placement. However, generating realistic REMs remains difficult due to the variability of indoor environments and the limitations of existing modeling approaches, which often rely on simplified layouts or fully synthetic data. These challenges are further amplified by the adoption of next-generation Wi-Fi standards, which operate at higher frequencies and suffer from limited range and wall penetration. To support the efforts in addressing these challenges, we collected a dataset that combines high-resolution 3D LiDAR scans with Wi-Fi RSSI measurements collected across 20 setups in a multi-room indoor environment. The dataset includes two measurement scenarios, the first without human presence in the environment, and the second with human presence, enabling the development and validation of REM estimation models that incorporate physical geometry and environmental dynamics. The described dataset supports research in data-driven wireless modeling and the development of high-capacity indoor communication networks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©å±•ç°å®(XR)ç­‰é«˜å¸¦å®½åº”ç”¨å¯¹å®¤å†…æ— çº¿è¿æ¥çš„éœ€æ±‚ï¼ŒæŒ‡å‡ºäº†åœ¨å¤æ‚å®¤å†…å¸ƒå±€åŠé«˜é¢‘Wi-Fiç¯å¢ƒä¸‹æ„å»ºç²¾ç¡®æ— çº¿ç”µç¯å¢ƒåœ°å›¾(Radio Environment Maps, REMs)çš„éš¾ç‚¹ã€‚ä¸ºäº†è§£å†³ç°æœ‰å»ºæ¨¡æ–¹æ³•ä¾èµ–åˆæˆæ•°æ®æˆ–ç®€åŒ–å¸ƒå±€çš„å±€é™æ€§ï¼Œä½œè€…æ„å»ºå¹¶å‘å¸ƒäº†ä¸€ä¸ªç»“åˆé«˜åˆ†è¾¨ç‡3D LiDARæ‰«æç‚¹äº‘ä¸Wi-Fi RSSIæµ‹é‡å€¼çš„æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åœ¨å¤šæˆ¿é—´ç¯å¢ƒä¸­é€šè¿‡20ç§ä¸åŒé…ç½®æ”¶é›†ï¼Œç‰¹åˆ«åŒ…å«äº†æ— äººå‘˜å’Œæœ‰äººå‘˜å­˜åœ¨ä¸¤ç§åœºæ™¯ï¼Œä»¥æ”¯æŒå¯¹ç‰©ç†å‡ ä½•å’Œç¯å¢ƒåŠ¨æ€çš„æ·±å…¥åˆ†æã€‚é€šè¿‡æä¾›çœŸå®ä¸”å¤šç»´åº¦çš„æµ‹é‡æ•°æ®ï¼Œè¯¥ç ”ç©¶ä¸ºå¼€å‘èƒ½å¤Ÿèåˆç¯å¢ƒå‡ ä½•ç‰¹å¾çš„REMä¼°è®¡æ¨¡å‹æä¾›äº†åŸºç¡€ï¼Œè¿›è€Œæ¨åŠ¨æ•°æ®é©±åŠ¨çš„æ— çº¿å»ºæ¨¡åŠé«˜å®¹é‡å®¤å†…é€šä¿¡ç½‘ç»œè§„åˆ’çš„ç ”ç©¶ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "12 pages, 7 figures, 3 tables, under review to Nature Scientific Data",
      "pdf_url": "https://arxiv.org/pdf/2511.00494v2",
      "published_date": "2025-11-01 11:02:16 UTC",
      "updated_date": "2025-12-04 09:45:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 1,
      "last_update": "2026-01-25T07:10:57.695217+00:00"
    },
    {
      "arxiv_id": "2511.00477v1",
      "title": "Investigating Label Bias and Representational Sources of Age-Related Disparities in Medical Segmentation",
      "title_zh": "æ¢ç©¶åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¹´é¾„ç›¸å…³å·®å¼‚çš„æ ‡ç­¾åå·®ä¸è¡¨å¾æ¥æº",
      "authors": [
        "Aditya Parikh",
        "Sneha Das",
        "Aasa Feragen"
      ],
      "abstract": "Algorithmic bias in medical imaging can perpetuate health disparities, yet its causes remain poorly understood in segmentation tasks. While fairness has been extensively studied in classification, segmentation remains underexplored despite its clinical importance. In breast cancer segmentation, models exhibit significant performance disparities against younger patients, commonly attributed to physiological differences in breast density. We audit the MAMA-MIA dataset, establishing a quantitative baseline of age-related bias in its automated labels, and reveal a critical Biased Ruler effect where systematically flawed labels for validation misrepresent a model's actual bias. However, whether this bias originates from lower-quality annotations (label bias) or from fundamentally more challenging image characteristics remains unclear. Through controlled experiments, we systematically refute hypotheses that the bias stems from label quality sensitivity or quantitative case difficulty imbalance. Balancing training data by difficulty fails to mitigate the disparity, revealing that younger patient cases are intrinsically harder to learn. We provide direct evidence that systemic bias is learned and amplified when training on biased, machine-generated labels, a critical finding for automated annotation pipelines. This work introduces a systematic framework for diagnosing algorithmic bias in medical segmentation and demonstrates that achieving fairness requires addressing qualitative distributional differences rather than merely balancing case counts.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†åŒ»ç–—å½±åƒåˆ†å‰²ï¼ˆMedical Segmentationï¼‰ä¸­çš„ç®—æ³•åè§ï¼ˆAlgorithmic Biasï¼‰ï¼Œé‡ç‚¹å…³æ³¨ä¹³è…ºç™Œåˆ†å‰²ä¸­ä¸å¹´é¾„ç›¸å…³çš„æ€§èƒ½å·®å¼‚ã€‚é€šè¿‡å¯¹ MAMA-MIA æ•°æ®é›†çš„å®¡è®¡ï¼Œç ”ç©¶æ­ç¤ºäº†â€œåè§å°ºæ ‡æ•ˆåº”â€ï¼ˆBiased Ruler effectï¼‰ï¼Œå³å­˜åœ¨ç³»ç»Ÿæ€§ç¼ºé™·çš„éªŒè¯æ ‡ç­¾ä¼šè¯¯å¯¼å¯¹æ¨¡å‹çœŸå®åè§çš„è¯„ä¼°ã€‚å®éªŒè¯æ˜ï¼Œè¿™ç§åè§å¹¶éç”±æ ‡ç­¾åè§ï¼ˆLabel Biasï¼‰æˆ–æ¡ˆä¾‹éš¾åº¦ä¸å¹³è¡¡å¼•èµ·ï¼Œè€Œæ˜¯å› ä¸ºå¹´è½»æ‚£è€…çš„ç—…ä¾‹åœ¨ç‰¹å¾ä¸Šæœ¬è´¨ä¸Šæ›´éš¾å­¦ä¹ ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œåœ¨æœ‰åè§çš„æœºå™¨ç”Ÿæˆæ ‡ç­¾ï¼ˆMachine-generated labelsï¼‰ä¸Šè¿›è¡Œè®­ç»ƒä¼šè¿›ä¸€æ­¥å­¦ä¹ å¹¶æ”¾å¤§ç³»ç»Ÿæ€§åè§ã€‚è¯¥å·¥ä½œä¸ºè¯Šæ–­åŒ»ç–—åˆ†å‰²ä¸­çš„åè§æä¾›äº†ç³»ç»Ÿæ€§æ¡†æ¶ï¼Œå¹¶æŒ‡å‡ºå®ç°å…¬å¹³æ€§å¿…é¡»è§£å†³å®šæ€§çš„åˆ†å¸ƒå·®å¼‚ï¼ˆQualitative distributional differencesï¼‰ï¼Œè€Œéå•çº¯å¹³è¡¡æ ·æœ¬æ•°é‡ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Submitted to ISBI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.00477v1",
      "published_date": "2025-11-01 10:06:30 UTC",
      "updated_date": "2025-11-01 10:06:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:11:14.303559+00:00"
    },
    {
      "arxiv_id": "2511.00472v1",
      "title": "Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations",
      "title_zh": "å…·æœ‰åŸºäºå…±è¯†çš„äººåœ¨å›è·¯æ ‡æ³¨çš„çºµå‘å‰åº­ç¥ç»é˜ç˜¤æ•°æ®é›†",
      "authors": [
        "Navodini Wijethilake",
        "Marina Ivory",
        "Oscar MacCormac",
        "Siddhant Kumar",
        "Aaron Kujawa",
        "Lorena Garcia-Foncillas Macias",
        "Rebecca Burger",
        "Amanda Hitchings",
        "Suki Thomson",
        "Sinan Barazi",
        "Eleni Maratos",
        "Rupert Obholzer",
        "Dan Jiang",
        "Fiona McClenaghan",
        "Kazumi Chia",
        "Omar Al-Salihi",
        "Nick Thomas",
        "Steve Connor",
        "Tom Vercauteren",
        "Jonathan Shapey"
      ],
      "abstract": "Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance Imaging (MRI) is essential for patient management but often requires time-intensive manual annotations by experts. While recent advances in deep learning (DL) have facilitated automated segmentation, challenges remain in achieving robust performance across diverse datasets and complex clinical cases. We present an annotated dataset stemming from a bootstrapped DL-based framework for iterative segmentation and quality refinement of VS in MRI. We combine data from multiple centres and rely on expert consensus for trustworthiness of the annotations. We show that our approach enables effective and resource-efficient generalisation of automated segmentation models to a target data distribution. The framework achieved a significant improvement in segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from 0.9125 to 0.9670 on our target internal validation dataset, while maintaining stable performance on representative external datasets. Expert evaluation on 143 scans further highlighted areas for model refinement, revealing nuanced cases where segmentation required expert intervention. The proposed approach is estimated to enhance efficiency by approximately 37.4% compared to the conventional manual annotation process. Overall, our human-in-the-loop model training approach achieved high segmentation accuracy, highlighting its potential as a clinically adaptable and generalisable strategy for automated VS segmentation in diverse clinical settings. The dataset includes 190 patients, with tumour annotations available for 534 longitudinal contrast-enhanced T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans from 6 patients. This dataset is publicly accessible on The Cancer Imaging Archive (TCIA) (https://doi.org/10.7937/bq0z-xa62).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå…·æœ‰ä¸“å®¶å…±è¯†èƒŒæ™¯çš„Human-in-the-loopæ ‡æ³¨çš„çºµå‘Vestibular Schwannoma (VS)æ•°æ®é›†ã€‚é€šè¿‡é‡‡ç”¨åŸºäºæ·±åº¦å­¦ä¹ (Deep Learning)çš„å¼•å¯¼æ¡†æ¶è¿›è¡Œè¿­ä»£åˆ†å‰²å’Œè´¨é‡ç»†åŒ–ï¼Œç ”ç©¶æ•´åˆäº†å¤šä¸­å¿ƒæ•°æ®å¹¶ç¡®ä¿äº†æ ‡æ³¨çš„Trustworthinessã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨ç›®æ ‡éªŒè¯é›†ä¸Šçš„Dice Similarity Coefficient (DSC)ä»0.9125æå‡è‡³0.9670ï¼Œå¹¶åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šå±•ç°äº†ç¨³å®šçš„æ³›åŒ–æ€§èƒ½ã€‚ç›¸æ¯”ä¼ ç»Ÿçš„Manual Annotationæµç¨‹ï¼Œè¯¥æ–¹æ³•å°†æ•ˆç‡æå‡äº†çº¦37.4%ï¼Œæ˜¾è‘—é™ä½äº†ä¸“å®¶çš„åŠ³åŠ¨å¼ºåº¦ã€‚è¯¥æ•°æ®é›†æ¶µç›–190åæ‚£è€…çš„534ä¸ªçºµå‘T1CEæ‰«ææ ‡æ³¨ï¼Œå·²åœ¨TCIAå¹³å°ä¸Šå…¬å¼€å‘å¸ƒã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†äººæœºååŒæ¨¡å¼åœ¨ä¸´åºŠè‡ªåŠ¨åŒ–åˆ†å‰²ä»»åŠ¡ä¸­å®ç°é«˜ç²¾åº¦ä¸é«˜æ•ˆç‡å¹³è¡¡çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00472v1",
      "published_date": "2025-11-01 09:53:28 UTC",
      "updated_date": "2025-11-01 09:53:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:11:24.345608+00:00"
    },
    {
      "arxiv_id": "2511.00469v1",
      "title": "Why Federated Optimization Fails to Achieve Perfect Fitting? A Theoretical Perspective on Client-Side Optima",
      "title_zh": "è”é‚¦ä¼˜åŒ–ä¸ºä½•æ— æ³•å®ç°å®Œç¾æ‹Ÿåˆï¼ŸåŸºäºå®¢æˆ·ç«¯ä¾§æœ€ä¼˜è§£çš„ç†è®ºè§†è§’",
      "authors": [
        "Zhongxiang Lei",
        "Qi Yang",
        "Ping Qiu",
        "Gang Zhang",
        "Yuanchi Ma",
        "Jinyan Liu"
      ],
      "abstract": "Federated optimization is a constrained form of distributed optimization that enables training a global model without directly sharing client data. Although existing algorithms can guarantee convergence in theory and often achieve stable training in practice, the reasons behind performance degradation under data heterogeneity remain unclear. To address this gap, the main contribution of this paper is to provide a theoretical perspective that explains why such degradation occurs. We introduce the assumption that heterogeneous client data lead to distinct local optima, and show that this assumption implies two key consequences: 1) the distance among clients' local optima raises the lower bound of the global objective, making perfect fitting of all client data impossible; and 2) in the final training stage, the global model oscillates within a region instead of converging to a single optimum, limiting its ability to fully fit the data. These results provide a principled explanation for performance degradation in non-iid settings, which we further validate through experiments across multiple tasks and neural network architectures. The framework used in this paper is open-sourced at: https://github.com/NPCLEI/fedtorch.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è”é‚¦ä¼˜åŒ–ï¼ˆFederated optimizationï¼‰åœ¨æ•°æ®å¼‚æ„ï¼ˆdata heterogeneityï¼‰ç¯å¢ƒä¸‹æ€§èƒ½ä¸‹é™çš„åŸå› ï¼Œä»ç†è®ºè§†è§’åˆ†æäº†æ¨¡å‹ä¸ºä½•æ— æ³•å®ç°å®Œç¾æ‹Ÿåˆï¼ˆperfect fittingï¼‰ã€‚ä½œè€…æŒ‡å‡ºå¼‚æ„å®¢æˆ·ç«¯æ•°æ®ä¼šå¯¼è‡´ä¸åŒçš„å±€éƒ¨æœ€ä¼˜è§£ï¼ˆlocal optimaï¼‰ï¼Œå¹¶è¯æ˜äº†ä¸¤ä¸ªå…³é”®æ¨è®ºï¼šé¦–å…ˆï¼Œå®¢æˆ·ç«¯å±€éƒ¨æœ€ä¼˜è§£ä¹‹é—´çš„è·ç¦»æé«˜äº†å…¨å±€ç›®æ ‡çš„ä¸‹ç•Œï¼Œä½¿å¾—å®Œç¾æ‹Ÿåˆæ‰€æœ‰æ•°æ®åœ¨ç†è®ºä¸Šä¸å¯è¡Œï¼›å…¶æ¬¡ï¼Œå…¨å±€æ¨¡å‹åœ¨è®­ç»ƒæœ«æœŸä¼šåœ¨ç‰¹å®šåŒºåŸŸå†…éœ‡è¡è€Œéæ”¶æ•›è‡³å•ä¸€æœ€ä¼˜è§£ï¼Œé™åˆ¶äº†å…¶æ‹Ÿåˆèƒ½åŠ›ã€‚è¯¥æ¡†æ¶ä¸ºéç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-iidï¼‰è®¾ç½®ä¸‹çš„æ€§èƒ½é€€åŒ–æä¾›äº†åŸåˆ™æ€§è§£é‡Šã€‚ç ”ç©¶é€šè¿‡å¤šç§ä»»åŠ¡å’Œç¥ç»ç½‘ç»œæ¶æ„çš„å®éªŒéªŒè¯äº†ç†è®ºçš„æœ‰æ•ˆæ€§ï¼Œå¹¶å¼€æºäº†åä¸º fedtorch çš„ç ”ç©¶æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00469v1",
      "published_date": "2025-11-01 09:31:35 UTC",
      "updated_date": "2025-11-01 09:31:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:11:21.513583+00:00"
    },
    {
      "arxiv_id": "2511.00460v1",
      "title": "Proactive DDoS Detection and Mitigation in Decentralized Software-Defined Networking via Port-Level Monitoring and Zero-Training Large Language Models",
      "title_zh": "åŸºäºç«¯å£çº§ç›‘æ§ä¸é›¶è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹çš„å»ä¸­å¿ƒåŒ–è½¯ä»¶å®šä¹‰ç½‘ç»œä¸»åŠ¨å¼ DDoS æ£€æµ‹ä¸ç¼“è§£",
      "authors": [
        "Mohammed N. Swileh",
        "Shengli Zhang"
      ],
      "abstract": "Centralized Software-Defined Networking (cSDN) offers flexible and programmable control of networks but suffers from scalability and reliability issues due to its reliance on centralized controllers. Decentralized SDN (dSDN) alleviates these concerns by distributing control across multiple local controllers, yet this architecture remains highly vulnerable to Distributed Denial-of-Service (DDoS) attacks. In this paper, we propose a novel detection and mitigation framework tailored for dSDN environments. The framework leverages lightweight port-level statistics combined with prompt engineering and in-context learning, enabling the DeepSeek-v3 Large Language Model (LLM) to classify traffic as benign or malicious without requiring fine-tuning or retraining. Once an anomaly is detected, mitigation is enforced directly at the attacker's port, ensuring that malicious traffic is blocked at their origin while normal traffic remains unaffected. An automatic recovery mechanism restores normal operation after the attack inactivity, ensuring both security and availability. Experimental evaluation under diverse DDoS attack scenarios demonstrates that the proposed approach achieves near-perfect detection, with 99.99% accuracy, 99.97% precision, 100% recall, 99.98% F1-score, and an AUC of 1.0. These results highlight the effectiveness of combining distributed monitoring with zero-training LLM inference, providing a proactive and scalable defense mechanism for securing dSDN infrastructures against DDoS threats.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åˆ†æ•£å¼è½¯ä»¶å®šä¹‰ç½‘ç»œ(dSDN)æ˜“å—åˆ†å¸ƒå¼æ‹’ç»æœåŠ¡(DDoS)æ”»å‡»çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆç«¯å£çº§ç›‘æ§ä¸é›¶è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„ä¸»åŠ¨æ£€æµ‹ä¸ç¼“è§£æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è½»é‡çº§çš„ç«¯å£çº§ç»Ÿè®¡æ•°æ®ï¼Œé…åˆæç¤ºè¯å·¥ç¨‹(Prompt Engineering)å’Œæƒ…å¢ƒå­¦ä¹ (In-context Learning)æŠ€æœ¯ï¼Œä½¿DeepSeek-v3æ¨¡å‹åœ¨æ— éœ€å¾®è°ƒæˆ–é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å³å¯å®ç°æµé‡åˆ†ç±»ã€‚ä¸€æ—¦æ£€æµ‹åˆ°å¼‚å¸¸ï¼Œç³»ç»Ÿä¼šç›´æ¥åœ¨æ”»å‡»è€…ç«¯å£æ‰§è¡Œç¼“è§£æ“ä½œï¼Œä»æºå¤´é˜»æ–­æ¶æ„æµé‡å¹¶ç¡®ä¿æ­£å¸¸ä¸šåŠ¡ä¸å—å¹²æ‰°ï¼ŒåŒæ—¶é€šè¿‡è‡ªåŠ¨æ¢å¤æœºåˆ¶ç»´æŒç½‘ç»œå¯ç”¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§DDoSæ”»å‡»åœºæ™¯ä¸‹è¡¨ç°å“è¶Šï¼Œå‡†ç¡®ç‡è¾¾åˆ°99.99%ï¼Œå¬å›ç‡ä¸º100%ï¼Œä¸”AUCå€¼ä¸º1.0ã€‚è¿™ç§å°†åˆ†å¸ƒå¼ç›‘æ§ä¸é›¶è®­ç»ƒLLMæ¨ç†ç›¸ç»“åˆçš„æ–¹æ¡ˆï¼Œä¸ºä¿éšœdSDNåŸºç¡€è®¾æ–½å®‰å…¨æä¾›äº†ä¸€ç§é«˜æ•ˆã€å¯æ‰©å±•ä¸”å…·å¤‡ä¸»åŠ¨æ€§çš„é˜²å¾¡æœºåˆ¶ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00460v1",
      "published_date": "2025-11-01 08:57:29 UTC",
      "updated_date": "2025-11-01 08:57:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:11:19.835868+00:00"
    },
    {
      "arxiv_id": "2511.00457v2",
      "title": "GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining",
      "title_zh": "GraphChainï¼šåŸºäºå·¥å…·é“¾çš„å¤§è§„æ¨¡å›¾åˆ†æå¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Chunyu Wei",
        "Wenji Hu",
        "Xingjia Hao",
        "Xin Wang",
        "Yifan Yang",
        "Yueguo Chen",
        "Yang Tian",
        "Yunhai Wang"
      ],
      "abstract": "Large Language Models (LLMs) face significant limitations when applied to large-scale graphs, struggling with context constraints and inflexible reasoning. We present GraphChain, a framework that enables LLMs to analyze complex graphs through dynamic sequences of specialized tools, mimicking human exploratory intelligence. Our approach introduces two key innovations: (1) Progressive Graph Distillation, a reinforcement learning mechanism that generates optimized tool sequences balancing task relevance with information compression, and (2) Structure-aware Test-Time Adaptation, which efficiently tailors tool selection strategies to diverse graph topologies using spectral properties and lightweight adapters without costly retraining. Experiments show GraphChain significantly outperforms prior methods, enabling scalable and adaptive LLM-driven graph analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Large Language Models (LLMs) åœ¨å¤„ç†å¤§è§„æ¨¡å›¾ (large-scale graphs) æ—¶é¢ä¸´çš„ä¸Šä¸‹æ–‡é™åˆ¶å’Œæ¨ç†çµæ´»æ€§ä¸è¶³ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº† GraphChain æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åŠ¨æ€çš„ä¸“é—¨å·¥å…·åºåˆ— (dynamic sequences of specialized tools) æ¨¡æ‹Ÿäººç±»æ¢ç´¢æ€§æ™ºèƒ½ï¼Œä½¿ LLMs èƒ½å¤Ÿé«˜æ•ˆåˆ†æå¤æ‚çš„å›¾ç»“æ„ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹  (reinforcement learning) çš„æ¸è¿›å¼å›¾è’¸é¦ (Progressive Graph Distillation) æœºåˆ¶ï¼Œç”¨äºç”Ÿæˆå¹³è¡¡ä»»åŠ¡ç›¸å…³æ€§ä¸ä¿¡æ¯å‹ç¼©çš„ä¼˜åŒ–å·¥å…·åºåˆ—ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ç»“æ„æ„ŸçŸ¥æµ‹è¯•æ—¶è‡ªé€‚åº” (Structure-aware Test-Time Adaptation) æŠ€æœ¯ï¼Œåˆ©ç”¨è°±å±æ€§ (spectral properties) å’Œè½»é‡çº§é€‚é…å™¨ (lightweight adapters)ï¼Œåœ¨æ— éœ€æ˜‚è´µé‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ä½¿å·¥å…·é€‰æ‹©ç­–ç•¥é€‚é…å¤šç§å›¾æ‹“æ‰‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGraphChain çš„è¡¨ç°æ˜¾è‘—ä¼˜äºå…ˆå‰æ–¹æ³•ï¼Œä¸ºå®ç°å¯æ‰©å±•ä¸”å…·å¤‡è‡ªé€‚åº”èƒ½åŠ›çš„ LLM é©±åŠ¨å›¾åˆ†ææä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.00457v2",
      "published_date": "2025-11-01 08:47:05 UTC",
      "updated_date": "2025-11-08 04:47:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:11:21.350478+00:00"
    },
    {
      "arxiv_id": "2511.00456v5",
      "title": "Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations",
      "title_zh": "åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œä¸ Grad-CAM è§£é‡Šçš„èƒ¸éƒ¨ X çº¿ç‰‡å¼±ç›‘ç£è‚ºç‚å®šä½",
      "authors": [
        "Kiran Shahi",
        "Anup Bagale"
      ],
      "abstract": "Chest X-ray imaging is commonly used to diagnose pneumonia, but accurately localizing the pneumonia-affected regions typically requires detailed pixel-level annotations, which are costly and time consuming to obtain. To address this limitation, this study proposes a weakly supervised deep learning framework for pneumonia classification and localization using Gradient-weighted Class Activation Mapping (Grad-CAM). Instead of relying on costly pixel-level annotations, the proposed method utilizes image-level labels to generate clinically meaningful heatmaps that highlight pneumonia-affected regions. Furthermore, we evaluate seven pre-trained deep learning models, including a Vision Transformer, under identical training conditions, using focal loss and patient-wise splits to prevent data leakage. Experimental results suggest that all models achieved high classification accuracy (96--98\\%), with ResNet-18 and EfficientNet-B0 showing the best overall performance and MobileNet-V3 providing an efficient lightweight alternative. Grad-CAM heatmap visualizations confirm that the proposed methods focus on clinically relevant lung regions, supporting the use of explainable AI for radiological diagnostics. Overall, this work highlights the potential of weakly supervised, explainable models that enhance transparency and clinical trust in AI-assisted pneumonia screening.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºå¼±ç›‘ç£(Weakly Supervised)æ·±åº¦å­¦ä¹ å’ŒGrad-CAMè§£é‡ŠæŠ€æœ¯çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä»…ä½¿ç”¨å›¾åƒçº§æ ‡ç­¾å®ç°èƒ¸éƒ¨Xå°„çº¿(Chest X-ray)å›¾åƒä¸­çš„è‚ºç‚åˆ†ç±»ä¸å®šä½ã€‚è¯¥æ–¹æ³•é€šè¿‡Gradient-weighted Class Activation Mapping (Grad-CAM)ç”Ÿæˆå…·æœ‰ä¸´åºŠæ„ä¹‰çš„çƒ­å›¾ä»¥çªå‡ºè‚ºç‚å—ç´¯åŒºåŸŸï¼Œä»è€Œæœ‰æ•ˆé¿å…äº†å¯¹é«˜æˆæœ¬åƒç´ çº§æ ‡æ³¨çš„éœ€æ±‚ã€‚ç ”ç©¶åœ¨ç›¸åŒè®­ç»ƒæ¡ä»¶ä¸‹è¯„ä¼°äº†åŒ…æ‹¬Vision Transformeråœ¨å†…çš„ä¸ƒç§é¢„è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨å±€åŸŸæŸå¤±(focal loss)å’Œæ‚£è€…çº§åˆ’åˆ†æ¥é˜²æ­¢æ•°æ®æ³„éœ²ã€‚å®éªŒç»“æœæ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹å‡è¾¾åˆ°äº†96%è‡³98%çš„é«˜åˆ†ç±»å‡†ç¡®ç‡ï¼Œå…¶ä¸­ResNet-18å’ŒEfficientNet-B0è¡¨ç°å‡ºæœ€ä½³çš„ç»¼åˆæ€§èƒ½ï¼Œè€ŒMobileNet-V3åˆ™æä¾›äº†ä¸€ä¸ªé«˜æ•ˆçš„è½»é‡çº§æ–¹æ¡ˆã€‚Grad-CAMçƒ­å›¾å¯è§†åŒ–è¯å®äº†è¯¥æ¨¡å‹èƒ½å‡†ç¡®èšç„¦äºä¸´åºŠç›¸å…³çš„è‚ºéƒ¨åŒºåŸŸï¼Œæ”¯æŒäº†å¯è§£é‡Šäººå·¥æ™ºèƒ½(explainable AI)åœ¨æ”¾å°„å­¦è¯Šæ–­ä¸­çš„åº”ç”¨ã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†å¼±ç›‘ç£ã€å¯è§£é‡Šæ¨¡å‹åœ¨æå‡AIè¾…åŠ©è‚ºç‚ç­›æŸ¥é€æ˜åº¦ä¸ä¸´åºŠä¿¡ä»»åº¦æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "https://github.com/kiranshahi/pneumonia-analysis",
      "pdf_url": "https://arxiv.org/pdf/2511.00456v5",
      "published_date": "2025-11-01 08:44:24 UTC",
      "updated_date": "2025-12-17 11:27:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:11:26.450605+00:00"
    },
    {
      "arxiv_id": "2511.00447v2",
      "title": "DRIP: Defending Prompt Injection via Token-wise Representation Editing and Residual Instruction Fusion",
      "title_zh": "DRIPï¼šåŸºäºè¯å…ƒçº§è¡¨ç¤ºç¼–è¾‘ä¸æ®‹å·®æŒ‡ä»¤èåˆçš„æç¤ºæ³¨å…¥é˜²å¾¡",
      "authors": [
        "Ruofan Liu",
        "Yun Lin",
        "Zhiyong Huang",
        "Jin Song Dong"
      ],
      "abstract": "Large language models (LLMs) are increasingly integrated into IT infrastructures, where they process user data according to predefined instructions. However, conventional LLMs remain vulnerable to prompt injection, where malicious users inject directive tokens into the data to subvert model behavior. Existing defenses train LLMs to semantically separate data and instruction tokens, but still struggle to (1) balance utility and security and (2) prevent instruction-like semantics in the data from overriding the intended instructions.\n  We propose DRIP, which (1) precisely removes instruction semantics from tokens in the data section while preserving their data semantics, and (2) robustly preserves the effect of the intended instruction even under strong adversarial content. To \"de-instructionalize\" data tokens, DRIP introduces a data curation and training paradigm with a lightweight representation-editing module that edits embeddings of instruction-like tokens in the data section, enhancing security without harming utility. To ensure non-overwritability of instructions, DRIP adds a minimal residual module that reduces the ability of adversarial data to overwrite the original instruction. We evaluate DRIP on LLaMA 8B and Mistral 7B against StruQ, SecAlign, ISE, and PFT on three prompt-injection benchmarks (SEP, AlpacaFarm, and InjecAgent). DRIP improves role-separation score by 12-49\\%, reduces attack success rate by over 66\\% under adaptive attacks, and matches the utility of the undefended model, establishing a new state of the art for prompt-injection robustness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)æ˜“å—æç¤ºæ³¨å…¥(Prompt Injection)æ”»å‡»çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºDRIPçš„é˜²å¾¡æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§è½»é‡çº§çš„è¡¨å¾ç¼–è¾‘(Representation-Editing)æ¨¡å—ï¼Œé€šè¿‡å¯¹æ•°æ®éƒ¨åˆ†çš„Tokenè¿›è¡Œâ€œå»æŒ‡ä»¤åŒ–â€å¤„ç†ï¼Œåœ¨ä¿ç•™æ•°æ®è¯­ä¹‰çš„åŒæ—¶ç²¾å‡†æ¶ˆé™¤å…¶æŒ‡ä»¤å±æ€§ï¼Œæœ‰æ•ˆè§£å†³äº†å®‰å…¨æ€§å’Œå®ç”¨æ€§ä¹‹é—´çš„å¹³è¡¡éš¾é¢˜ã€‚åŒæ—¶ï¼ŒDRIPåˆ©ç”¨æ®‹å·®æŒ‡ä»¤èåˆ(Residual Instruction Fusion)æœºåˆ¶å¢å¼ºäº†æŒ‡ä»¤çš„ä¸å¯è¦†ç›–æ€§ï¼Œç¡®ä¿æ¨¡å‹åœ¨é¢ä¸´å¼ºå¯¹æŠ—æ€§å†…å®¹æ—¶ä»èƒ½éµå¾ªé¢„å®šä¹‰æŒ‡ä»¤ã€‚åœ¨LLaMA 8Bå’ŒMistral 7Bæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDRIPåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å°†è§’è‰²åˆ†ç¦»è¯„åˆ†æå‡äº†12-49%ï¼Œå¹¶åœ¨è‡ªé€‚åº”æ”»å‡»ä¸‹å°†æ”»å‡»æˆåŠŸç‡é™ä½äº†66%ä»¥ä¸Šã€‚è¯¥æ–¹æ¡ˆåœ¨ä¿æŒæ¨¡å‹åŸç”Ÿæ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å¢å¼ºäº†ç³»ç»Ÿå¯¹æç¤ºæ³¨å…¥çš„é²æ£’æ€§ï¼Œè¾¾åˆ°äº†å½“å‰çš„æœ€ä¼˜æ°´å¹³ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00447v2",
      "published_date": "2025-11-01 08:26:37 UTC",
      "updated_date": "2025-11-18 02:40:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:11:33.841844+00:00"
    },
    {
      "arxiv_id": "2511.00444v1",
      "title": "LIR: The First Workshop on Late Interaction and Multi Vector Retrieval @ ECIR 2026",
      "title_zh": "LIRï¼šé¦–å±Šå»¶è¿Ÿäº¤äº’ä¸å¤šå‘é‡æ£€ç´¢ç ”è®¨ä¼š @ ECIR 2026",
      "authors": [
        "Benjamin ClaviÃ©",
        "Xianming Li",
        "Antoine Chaffin",
        "Omar Khattab",
        "Tom Aarsen",
        "Manuel Faysse",
        "Jing Li"
      ],
      "abstract": "Late interaction retrieval methods, pioneered by ColBERT, have emerged as a powerful alternative to single-vector neural IR. By leveraging fine-grained, token-level representations, they have been demonstrated to deliver strong generalisation and robustness, particularly in out-of-domain settings. They have recently been shown to be particularly well-suited for novel use cases, such as reasoning-based or cross-modality retrieval. At the same time, these models pose significant challenges of efficiency, usability, and integrations into fully fledged systems; as well as the natural difficulties encountered while researching novel application domains. Recent years have seen rapid advances across many of these areas, but research efforts remain fragmented across communities and frequently exclude practitioners. The purpose of this workshop is to create an environment where all aspects of late interaction can be discussed, with a focus on early research explorations, real-world outcomes, and negative or puzzling results to be freely shared and discussed. The aim of LIR is to provide a highly-interactive environment for researchers from various backgrounds and practitioners to freely discuss their experience, fostering further collaboration.",
      "tldr_zh": "è¯¥æ–‡ç« ä»‹ç»äº†åœ¨ ECIR 2026 ä¸¾åŠçš„é¦–å±Š Late Interaction and Multi Vector Retrieval (LIR) ç ”è®¨ä¼šï¼Œæ—¨åœ¨æ¢è®¨ä»¥ ColBERT ä¸ºä»£è¡¨çš„åæœŸäº¤äº’æ£€ç´¢æŠ€æœ¯ã€‚Late interaction æ–¹æ³•é€šè¿‡ fine-grained å’Œ token-level çš„è¡¨ç¤ºï¼Œåœ¨æ³›åŒ–æ€§å’Œé²æ£’æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„ single-vector neural IRï¼Œå°¤å…¶åœ¨ reasoning-based å’Œ cross-modality æ£€ç´¢ç­‰æ–°åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚å°½ç®¡è¿›æ­¥è¿…é€Ÿï¼Œä½†è¯¥é¢†åŸŸåœ¨ efficiencyã€usability å’Œç³»ç»Ÿé›†æˆæ–¹é¢ä»é¢ä¸´ä¸¥å³»æŒ‘æˆ˜ï¼Œä¸”ç ”ç©¶åŠ›é‡åœ¨ä¸åŒç¤¾åŒºé—´è¾ƒä¸ºåˆ†æ•£ã€‚LIR ç ”è®¨ä¼šä¸ºç§‘ç ”äººå‘˜å’Œä»ä¸šè€…æä¾›äº†ä¸€ä¸ªäº’åŠ¨å¹³å°ï¼Œé¼“åŠ±åˆ†äº«æ—©æœŸç ”ç©¶æ¢ç´¢ã€çœŸå®åº”ç”¨æˆæœä»¥åŠè´Ÿé¢æˆ–ä»¤äººè´¹è§£çš„å®éªŒç»“æœã€‚é€šè¿‡ä¿ƒè¿›ä¸åŒèƒŒæ™¯ä¸“å®¶ä¹‹é—´çš„è‡ªç”±è®¨è®ºä¸åä½œï¼Œè¯¥ç ”è®¨ä¼šè‡´åŠ›äºæ¨åŠ¨ late interaction æŠ€æœ¯ä»ç†è®ºç ”ç©¶å‘æˆç†Ÿç³»ç»Ÿçš„è½¬åŒ–ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted workshop at ECIR 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.00444v1",
      "published_date": "2025-11-01 08:21:33 UTC",
      "updated_date": "2025-11-01 08:21:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:11:37.614747+00:00"
    },
    {
      "arxiv_id": "2511.00443v1",
      "title": "Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model",
      "title_zh": "fMRI åŸºç¡€æ¨¡å‹é¢„è®­ç»ƒçš„åŒºåŸŸæ„ŸçŸ¥é‡å»ºç­–ç•¥",
      "authors": [
        "Ruthwik Reddy Doodipala",
        "Pankaj Pandey",
        "Carolina Torres Rojas",
        "Manob Jyoti Saikia",
        "Ranganatha Sitaram"
      ],
      "abstract": "The emergence of foundation models in neuroimaging is driven by the increasing availability of large-scale and heterogeneous brain imaging datasets. Recent advances in self-supervised learning, particularly reconstruction-based objectives, have demonstrated strong potential for pretraining models that generalize effectively across diverse downstream functional MRI (fMRI) tasks. In this study, we explore region-aware reconstruction strategies for a foundation model in resting-state fMRI, moving beyond approaches that rely on random region masking. Specifically, we introduce an ROI-guided masking strategy using the Automated Anatomical Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively mask semantically coherent brain regions during self-supervised pretraining. Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI scans, we show that our method achieves a 4.23% improvement in classification accuracy for distinguishing healthy controls from individuals diagnosed with ADHD, compared to conventional random masking. Region-level attribution analysis reveals that brain volumes within the limbic region and cerebellum contribute most significantly to reconstruction fidelity and model representation. Our results demonstrate that masking anatomical regions during model pretraining not only enhances interpretability but also yields more robust and discriminative representations. In future work, we plan to extend this approach by evaluating it on additional neuroimaging datasets, and developing new loss functions explicitly derived from region-aware reconstruction objectives. These directions aim to further improve the robustness and interpretability of foundation models for functional neuroimaging.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ç¥ç»å½±åƒå­¦ä¸­çš„åŸºç¡€æ¨¡å‹(foundation models)ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåŒºåŸŸæ„ŸçŸ¥é‡æ„(region-aware reconstruction)çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨æ”¹è¿›é™æ¯æ€fMRIæ•°æ®çš„è‡ªç›‘ç£å­¦ä¹ ã€‚ä¸ä¼ ç»Ÿçš„éšæœºåŒºåŸŸæ©ç ä¸åŒï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†åŸºäºAAL3å›¾è°±çš„ROIå¼•å¯¼æ©ç ç­–ç•¥ï¼Œå¹¶å°†å…¶ç›´æ¥åº”ç”¨äº4D fMRIå…¨è„‘ä½“ç§¯ï¼Œä»è€Œåœ¨é¢„è®­ç»ƒæœŸé—´é€‰æ‹©æ€§åœ°æ©ç›–å…·æœ‰è¯­ä¹‰ä¸€è‡´æ€§çš„è„‘åŒºåŸŸã€‚åœ¨åŒ…å«973åå—è¯•è€…çš„ADHD-200æ•°æ®é›†ä¸Šï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨åŒºåˆ†å¥åº·å¯¹ç…§ç»„ä¸ADHDæ‚£è€…çš„åˆ†ç±»å‡†ç¡®ç‡ä¸Šæ¯”ä¼ ç»Ÿéšæœºæ©ç æé«˜äº†4.23%ã€‚åŒºåŸŸçº§å±æ€§åˆ†æè¿›ä¸€æ­¥æ­ç¤ºï¼Œè¾¹ç¼˜ç³»ç»Ÿ(limbic region)å’Œå°è„‘(cerebellum)å¯¹é‡æ„ä¿çœŸåº¦å’Œæ¨¡å‹è¡¨å¾çš„è´¡çŒ®æœ€ä¸ºæ˜¾è‘—ã€‚ç ”ç©¶ç»“æœè¯æ˜ï¼Œåœ¨æ¨¡å‹é¢„è®­ç»ƒä¸­ç»“åˆè§£å‰–åŒºåŸŸæ©ç ä¸ä»…å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œè¿˜ç”Ÿæˆäº†æ›´å…·é²æ£’æ€§å’Œåˆ¤åˆ«åŠ›çš„ç‰¹å¾è¡¨å¾ï¼Œä¸ºåŠŸèƒ½ç¥ç»å½±åƒåŸºç¡€æ¨¡å‹çš„å‘å±•æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00443v1",
      "published_date": "2025-11-01 08:12:00 UTC",
      "updated_date": "2025-11-01 08:12:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:11:37.952669+00:00"
    },
    {
      "arxiv_id": "2511.00429v1",
      "title": "Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection",
      "title_zh": "å¼ºåŒ–é¢‘åŸŸä¼ªé€ çº¿ç´¢çš„æ‰©æ•£ç”Ÿæˆå›¾åƒæ£€æµ‹",
      "authors": [
        "Daichi Zhang",
        "Tong Zhang",
        "Shiming Ge",
        "Sabine SÃ¼sstrunk"
      ],
      "abstract": "Diffusion models have achieved remarkable success in image synthesis, but the generated high-quality images raise concerns about potential malicious use. Existing detectors often struggle to capture discriminative clues across different models and settings, limiting their generalization to unseen diffusion models and robustness to various perturbations. To address this issue, we observe that diffusion-generated images exhibit progressively larger differences from natural real images across low- to high-frequency bands. Based on this insight, we propose a simple yet effective representation by enhancing the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we introduce a frequency-selective function which serves as a weighted filter to the Fourier spectrum, suppressing less discriminative bands while enhancing more informative ones. This approach, grounded in a comprehensive analysis of frequency-based differences between natural real and diffusion-generated images, enables general detection of images from unseen diffusion models and provides robust resilience to various perturbations. Extensive experiments on various diffusion-generated image datasets demonstrate that our method outperforms state-of-the-art detectors with superior generalization and robustness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹(Diffusion models)ç”Ÿæˆå›¾åƒå¼•å‘çš„æ¶æ„ä½¿ç”¨é£é™©ï¼Œä»¥åŠç°æœ‰æ£€æµ‹å™¨åœ¨æ³›åŒ–æ€§(generalization)å’Œé²æ£’æ€§(robustness)ä¸Šçš„ä¸è¶³æå‡ºäº†æ”¹è¿›æ–¹æ¡ˆã€‚ç ”ç©¶å‘ç°ï¼Œæ‰©æ•£ç”Ÿæˆå›¾åƒä¸çœŸå®å›¾åƒåœ¨ä½é¢‘è‡³é«˜é¢‘æ®µé—´å­˜åœ¨é€’å¢çš„å·®å¼‚ï¼Œæ®æ­¤æå‡ºäº†å¢å¼ºé¢‘ç‡ä¼ªé€ çº¿ç´¢(Frequency Forgery Clue, $F^2C$)çš„æ–°è¡¨å¾æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªé¢‘ç‡é€‰æ‹©å‡½æ•°(frequency-selective function)ä½œä¸ºå‚…é‡Œå¶é¢‘è°±(Fourier spectrum)çš„åŠ æƒæ»¤æ³¢å™¨ï¼Œé€šè¿‡æŠ‘åˆ¶åˆ¤åˆ«åŠ›å¼±çš„é¢‘æ®µå¹¶å¼ºåŒ–ä¿¡æ¯ä¸°å¯Œçš„é¢‘æ®µæ¥æå‡æ£€æµ‹æ•ˆèƒ½ã€‚è¿™ç§åŸºäºå…¨é¢é¢‘ç‡å·®å¼‚åˆ†æçš„ç­–ç•¥ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«æ¥è‡ªæœªçŸ¥æ‰©æ•£æ¨¡å‹çš„å›¾åƒå¹¶æŠµæŠ—å„ç§æ‰°åŠ¨ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡ä¼˜äºç›®å‰æœ€å…ˆè¿›çš„æ£€æµ‹å™¨ï¼Œå±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00429v1",
      "published_date": "2025-11-01 06:58:05 UTC",
      "updated_date": "2025-11-01 06:58:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:11:45.709597+00:00"
    },
    {
      "arxiv_id": "2511.00427v1",
      "title": "Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection",
      "title_zh": "åŸºäºå±‚çº§åŒ–å›¾æ–‡å¤±é…çš„é€šç”¨ä¼ªé€ å›¾åƒæ£€æµ‹",
      "authors": [
        "Daichi Zhang",
        "Tong Zhang",
        "Jianmin Bao",
        "Shiming Ge",
        "Sabine SÃ¼sstrunk"
      ],
      "abstract": "With the rapid development of generative models, detecting generated fake images to prevent their malicious use has become a critical issue recently. Existing methods frame this challenge as a naive binary image classification task. However, such methods focus only on visual clues, yielding trained detectors susceptible to overfitting specific image patterns and incapable of generalizing to unseen models. In this paper, we address this issue from a multi-modal perspective and find that fake images cannot be properly aligned with corresponding captions compared to real images. Upon this observation, we propose a simple yet effective detector termed ITEM by leveraging the image-text misalignment in a joint visual-language space as discriminative clues. Specifically, we first measure the misalignment of the images and captions in pre-trained CLIP's space, and then tune a MLP head to perform the usual detection task. Furthermore, we propose a hierarchical misalignment scheme that first focuses on the whole image and then each semantic object described in the caption, which can explore both global and fine-grained local semantic misalignment as clues. Extensive experiments demonstrate the superiority of our method against other state-of-the-art competitors with impressive generalization and robustness on various recent generative models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ITEMï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å›¾åƒ-æ–‡æœ¬å¯¹é½å¤±è°ƒï¼ˆimage-text misalignmentï¼‰åœ¨è”åˆè§†è§‰-è¯­è¨€ç©ºé—´ä¸­ä½œä¸ºåˆ¤åˆ«çº¿ç´¢çš„æ–°å‹è™šå‡å›¾åƒæ£€æµ‹å™¨ã€‚é’ˆå¯¹ç°æœ‰æ£€æµ‹æ–¹æ³•ä»…ä¾èµ–è§†è§‰çº¿ç´¢å¯¼è‡´åœ¨æœªçŸ¥ç”Ÿæˆæ¨¡å‹ä¸Šæ³›åŒ–æ€§å·®çš„é—®é¢˜ï¼Œç ”ç©¶è€…å‘ç°è™šå‡å›¾åƒä¸å…¶å¯¹åº”çš„captionåœ¨è¯­ä¹‰å¯¹é½ä¸Šæ˜æ˜¾å¼±äºçœŸå®å›¾åƒã€‚æ–¹æ³•é¦–å…ˆåœ¨é¢„è®­ç»ƒçš„CLIPç©ºé—´ä¸­è¡¡é‡å›¾åƒä¸æ–‡æœ¬çš„é”™ä½ç¨‹åº¦ï¼Œå¹¶åˆ©ç”¨MLPå¤´æ‰§è¡Œæ£€æµ‹ä»»åŠ¡ã€‚æ ¸å¿ƒåˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§åˆ†å±‚é”™ä½ï¼ˆhierarchical misalignmentï¼‰æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆä¸ä»…å…³æ³¨å…¨å›¾ï¼Œè¿˜æ·±å…¥æŒ–æ˜captionä¸­æè¿°çš„æ¯ä¸ªå…·ä½“è¯­ä¹‰å¯¹è±¡ã€‚é€šè¿‡åŒæ—¶æ•æ‰å…¨å±€å’Œç»†ç²’åº¦çš„å±€éƒ¨è¯­ä¹‰é”™ä½ï¼ŒITEMèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¯†åˆ«ç”Ÿæˆå›¾åƒçš„ä¼ªé€ ç‰¹å¾ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ä¸»æµç”Ÿæˆæ¨¡å‹ä¸Šå±•ç°äº†å“è¶Šçš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„state-of-the-artæ£€æµ‹æŠ€æœ¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00427v1",
      "published_date": "2025-11-01 06:51:14 UTC",
      "updated_date": "2025-11-01 06:51:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:12:59.168510+00:00"
    },
    {
      "arxiv_id": "2511.00424v1",
      "title": "A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method",
      "title_zh": "åŸºäºç¤¾äº¤åª’ä½“é‡‡é›†çš„Covid-19æœŸé—´æŠ‘éƒç—‡æ£€æµ‹å¤šæ¨¡æ€æ¡†æ¶ï¼šæ–°æ•°æ®é›†ä¸æ–¹æ³•",
      "authors": [
        "Ashutosh Anshul",
        "Gumpili Sai Pranav",
        "Mohammad Zia Ur Rehman",
        "Nagendra Kumar"
      ],
      "abstract": "The recent coronavirus disease (Covid-19) has become a pandemic and has affected the entire globe. During the pandemic, we have observed a spike in cases related to mental health, such as anxiety, stress, and depression. Depression significantly influences most diseases worldwide, making it difficult to detect mental health conditions in people due to unawareness and unwillingness to consult a doctor. However, nowadays, people extensively use online social media platforms to express their emotions and thoughts. Hence, social media platforms are now becoming a large data source that can be utilized for detecting depression and mental illness. However, existing approaches often overlook data sparsity in tweets and the multimodal aspects of social media. In this paper, we propose a novel multimodal framework that combines textual, user-specific, and image analysis to detect depression among social media users. To provide enough context about the user's emotional state, we propose (i) an extrinsic feature by harnessing the URLs present in tweets and (ii) extracting textual content present in images posted in tweets. We also extract five sets of features belonging to different modalities to describe a user. Additionally, we introduce a Deep Learning model, the Visual Neural Network (VNN), to generate embeddings of user-posted images, which are used to create the visual feature vector for prediction. We contribute a curated Covid-19 dataset of depressed and non-depressed users for research purposes and demonstrate the effectiveness of our model in detecting depression during the Covid-19 outbreak. Our model outperforms existing state-of-the-art methods over a benchmark dataset by 2%-8% and produces promising results on the Covid-19 dataset. Our analysis highlights the impact of each modality and provides valuable insights into users' mental and emotional states.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Covid-19ç–«æƒ…æœŸé—´å¿ƒç†å¥åº·é—®é¢˜æ¿€å¢çš„ç°è±¡ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç¤¾äº¤åª’ä½“æ•°æ®æŒ–æ˜çš„æ–°å‹å¤šæ¨¡æ€æŠ‘éƒç—‡æ£€æµ‹æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨æ¨æ–‡æ•°æ®ç¨€ç–æ€§å’Œå¤šæ¨¡æ€åˆ©ç”¨æ–¹é¢çš„ä¸è¶³ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æ–‡æœ¬ã€ç”¨æˆ·ç‰¹å®šç‰¹å¾åŠå›¾åƒåˆ†ææ¥ç»¼åˆè¯„ä¼°ç”¨æˆ·çš„å¿ƒç†çŠ¶æ€ã€‚ä¸ºäº†æä¾›æ›´å……è¶³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºæ¨æ–‡URLçš„å¤–éƒ¨ç‰¹å¾æå–ä»¥åŠå›¾åƒå†…æ–‡æœ¬å†…å®¹çš„æå–æ–¹æ³•ã€‚æ­¤å¤–ï¼Œä½œè€…è®¾è®¡äº†ä¸€ç§æ·±åº¦å­¦ä¹ æ¨¡å‹Visual Neural Network (VNN)æ¥ç”Ÿæˆç”¨æˆ·å‘å¸ƒå›¾åƒçš„åµŒå…¥å‘é‡ï¼Œä»è€Œæ„å»ºè§†è§‰ç‰¹å¾ç”¨äºé¢„æµ‹ã€‚è¯¥ç ”ç©¶è¿˜è´¡çŒ®äº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹Covid-19æœŸé—´æŠ‘éƒä¸éæŠ‘éƒç”¨æˆ·çš„ç²¾é€‰æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„State-of-the-art (SOTA)æ–¹æ³•2%-8%ï¼Œå¹¶åœ¨æ–°æ„å»ºçš„Covid-19æ•°æ®é›†ä¸Šå±•ç°äº†æ˜¾è‘—çš„æ£€æµ‹æ•ˆæœï¼ŒéªŒè¯äº†å¤šæ¨¡æ€ç‰¹å¾åœ¨åˆ†æç”¨æˆ·ç²¾ç¥å’Œæƒ…ç»ªçŠ¶æ€æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00424v1",
      "published_date": "2025-11-01 06:33:14 UTC",
      "updated_date": "2025-11-01 06:33:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-25T07:13:34.280809+00:00"
    },
    {
      "arxiv_id": "2511.00423v3",
      "title": "Bootstrap Off-policy with World Model",
      "title_zh": "åŸºäºä¸–ç•Œæ¨¡å‹çš„ç¦»ç­–ç•¥è‡ªä¸¾",
      "authors": [
        "Guojian Zhan",
        "Likun Wang",
        "Xiangteng Zhang",
        "Jiaxin Gao",
        "Masayoshi Tomizuka",
        "Shengbo Eben Li"
      ],
      "abstract": "Online planning has proven effective in reinforcement learning (RL) for improving sample efficiency and final performance. However, using planning for environment interaction inevitably introduces a divergence between the collected data and the policy's actual behaviors, degrading both model learning and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy with WOrld Model), a framework that tightly integrates planning and off-policy learning through a bootstrap loop: the policy initializes the planner, and the planner refines actions to bootstrap the policy through behavior alignment. This loop is supported by a jointly learned world model, which enables the planner to simulate future trajectories and provides value targets to facilitate policy improvement. The core of BOOM is a likelihood-free alignment loss that bootstraps the policy using the planner's non-parametric action distribution, combined with a soft value-weighted mechanism that prioritizes high-return behaviors and mitigates variability in the planner's action quality within the replay buffer. Experiments on the high-dimensional DeepMind Control Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in both training stability and final performance. The code is accessible at https://github.com/molumitu/BOOM_MBRL.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BOOM (Bootstrap Off-policy with WOrld Model) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­åœ¨çº¿è§„åˆ’å¯¼è‡´çš„é‡‡é›†æ•°æ®ä¸ç­–ç•¥å®é™…è¡Œä¸ºä¸ä¸€è‡´ï¼Œä»è€Œå½±å“æ¨¡å‹å­¦ä¹ å’Œç­–ç•¥æ”¹è¿›çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å¯¼å¾ªç¯ (bootstrap loop) å°†è§„åˆ’ä¸ç¦»ç­–å­¦ä¹  (off-policy learning) ç´§å¯†ç»“åˆï¼Œåˆ©ç”¨ç­–ç•¥åˆå§‹åŒ–è§„åˆ’å™¨ï¼Œå†ç”±è§„åˆ’å™¨ä¼˜åŒ–åŠ¨ä½œä»¥é€šè¿‡è¡Œä¸ºå¯¹é½å¼•å¯¼ç­–ç•¥æå‡ã€‚è”åˆå­¦ä¹ çš„ä¸–ç•Œæ¨¡å‹ (world model) æ”¯æŒè§„åˆ’å™¨æ¨¡æ‹Ÿæœªæ¥è½¨è¿¹ï¼Œå¹¶æä¾›ä»·å€¼ç›®æ ‡ä»¥è¾…åŠ©ç­–ç•¥æ”¹è¿›ã€‚BOOM çš„æ ¸å¿ƒåœ¨äºå¼•å…¥äº†æ— ä¼¼ç„¶å¯¹é½æŸå¤± (likelihood-free alignment loss)ï¼Œåˆ©ç”¨è§„åˆ’å™¨çš„éå‚æ•°åŒ–åŠ¨ä½œåˆ†å¸ƒæ›´æ–°ç­–ç•¥ï¼Œå¹¶ç»“åˆè½¯ä»·å€¼æƒé‡æœºåˆ¶ (soft value-weighted mechanism) ä¼˜å…ˆå¤„ç†é«˜å›æŠ¥è¡Œä¸ºå¹¶ç¼“è§£å›æ”¾æ± ä¸­åŠ¨ä½œè´¨é‡çš„æ³¢åŠ¨ã€‚åœ¨ DeepMind Control Suite å’Œ Humanoid-Bench ç­‰é«˜ç»´ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒBOOM åœ¨è®­ç»ƒç¨³å®šæ€§å’Œæœ€ç»ˆæ€§èƒ½ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿› (state-of-the-art) çš„æ°´å¹³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.00423v3",
      "published_date": "2025-11-01 06:33:04 UTC",
      "updated_date": "2026-01-15 13:38:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:12:09.541691+00:00"
    },
    {
      "arxiv_id": "2511.01914v1",
      "title": "iFlyBot-VLA Technical Report",
      "title_zh": "iFlyBot-VLA æŠ€æœ¯æŠ¥å‘Š",
      "authors": [
        "Yuan Zhang",
        "Chenyu Xue",
        "Wenjie Xu",
        "Chao Ji",
        "Jiajia wu",
        "Jia Pan"
      ],
      "abstract": "We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model trained under a novel framework. The main contributions are listed as follows: (1) a latent action model thoroughly trained on large-scale human and robotic manipulation videos; (2) a dual-level action representation framework that jointly supervises both the Vision-Language Model (VLM) and the action expert during training; (3) a mixed training strategy that combines robot trajectory data with general QA and spatial QA datasets, effectively enhancing the 3D perceptual and reasoning capabilities of the VLM backbone. Specifically, the VLM is trained to predict two complementary forms of actions: latent actions, derived from our latent action model pretrained on cross-embodiment manipulation data, which capture implicit high-level intentions; and structured discrete action tokens, obtained through frequency-domain transformations of continuous control signals, which encode explicit low-level dynamics. This dual supervision aligns the representation spaces of language, vision, and action, enabling the VLM to directly contribute to action generation. Experimental results on the LIBERO Franka benchmark demonstrate the superiority of our frame-work, while real-world evaluations further show that iFlyBot-VLA achieves competitive success rates across diverse and challenging manipulation tasks. Furthermore, we plan to open-source a portion of our self-constructed dataset to support future research in the community",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† iFlyBot-VLAï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨åˆ›æ–°æ¡†æ¶ä¸‹è®­ç»ƒçš„å¤§è§„æ¨¡è§†è§‰-è¯­è¨€-åŠ¨ä½œ (Vision-Language-Action, VLA) æ¨¡å‹ã€‚å…¶æ ¸å¿ƒè´¡çŒ®åŒ…æ‹¬ä¸€ä¸ªåœ¨æµ·é‡äººç±»åŠæœºå™¨äººæ“ä½œè§†é¢‘ä¸Šè®­ç»ƒçš„æ½œåœ¨åŠ¨ä½œæ¨¡å‹ (Latent Action Model)ï¼Œä»¥åŠä¸€å¥—åŒæ—¶ç›‘ç£è§†è§‰è¯­è¨€æ¨¡å‹ (VLM) å’ŒåŠ¨ä½œä¸“å®¶çš„åŒå±‚åŠ¨ä½œè¡¨ç¤ºæ¡†æ¶ã€‚é€šè¿‡å°†è¿ç»­æ§åˆ¶ä¿¡å·è¿›è¡Œé¢‘åŸŸå˜æ¢ç”Ÿæˆçš„ç»“æ„åŒ–ç¦»æ•£åŠ¨ä½œæ ‡è®°ä¸æ½œåœ¨åŠ¨ä½œç›¸ç»“åˆï¼Œè¯¥æ¨¡å‹èƒ½å¤ŸåŒæ—¶æ•æ‰æ˜¾å¼çš„åº•å±‚åŠ¨åŠ›å­¦å’Œéšå¼çš„é«˜å±‚æ„å›¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨ç»“åˆæœºå™¨äººè½¨è¿¹æ•°æ®ä¸é€šç”¨åŠç©ºé—´ QA æ•°æ®é›†çš„æ··åˆè®­ç»ƒç­–ç•¥ï¼Œæ˜¾è‘—å¢å¼ºäº† VLM ä¸»å¹²ç½‘ç»œçš„ 3D æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›ã€‚è¿™ç§åŒé‡ç›‘ç£æœºåˆ¶æˆåŠŸå¯¹é½äº†è¯­è¨€ã€è§†è§‰ä¸åŠ¨ä½œçš„è¡¨ç¤ºç©ºé—´ï¼Œä½¿ VLM èƒ½å¤Ÿç›´æ¥è´¡çŒ®äºåŠ¨ä½œç”Ÿæˆã€‚å®éªŒç»“æœåœ¨ LIBERO Franka åŸºå‡†æµ‹è¯•åŠçœŸå®ä¸–ç•Œå¤šé¡¹æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº† iFlyBot-VLA å…·æœ‰æé«˜çš„ç«äº‰åŠ›å’Œåº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01914v1",
      "published_date": "2025-11-01 06:24:56 UTC",
      "updated_date": "2025-11-01 06:24:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:12:08.803059+00:00"
    },
    {
      "arxiv_id": "2511.00421v1",
      "title": "MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts",
      "title_zh": "MedRECTï¼šé¢å‘ä¸´åºŠæ–‡æœ¬çº é”™çš„åŒ»å­¦æ¨ç†åŸºå‡†",
      "authors": [
        "Naoto Iwase",
        "Hiroki Okuyama",
        "Junichiro Iwasawa"
      ],
      "abstract": "Large language models (LLMs) show increasing promise in medical applications, but their ability to detect and correct errors in clinical texts -- a prerequisite for safe deployment -- remains under-evaluated, particularly beyond English. We introduce MedRECT, a cross-lingual benchmark (Japanese/English) that formulates medical error handling as three subtasks: error detection, error localization (sentence extraction), and error correction. MedRECT is built with a scalable, automated pipeline from the Japanese Medical Licensing Examinations (JMLE) and a curated English counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning proprietary, open-weight, and reasoning families. Key findings: (i) reasoning models substantially outperform standard architectures, with up to 13.5% relative improvement in error detection and 51.0% in sentence extraction; (ii) cross-lingual evaluation reveals 5-10% performance gaps from English to Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA fine-tuning yields asymmetric improvements in error correction performance (Japanese: +0.078, English: +0.168) while preserving reasoning capabilities; and (iv) our fine-tuned model exceeds human expert performance on structured medical error correction tasks. To our knowledge, MedRECT is the first comprehensive cross-lingual benchmark for medical error correction, providing a reproducible framework and resources for developing safer medical LLMs across languages.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† MedRECTï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸´åºŠæ–‡æœ¬é”™è¯¯ä¿®æ­£çš„è·¨è¯­è¨€åŸºå‡†æµ‹è¯• (Cross-lingual benchmark)ï¼Œæ¶µç›–äº†æ—¥è¯­å’Œè‹±è¯­ï¼Œå¹¶å°†åŒ»ç–—é”™è¯¯å¤„ç†ç»†åˆ†ä¸ºé”™è¯¯æ£€æµ‹ (Error detection)ã€é”™è¯¯å®šä½ (Error localization) å’Œé”™è¯¯ä¿®æ­£ (Error correction) ä¸‰ä¸ªå­ä»»åŠ¡ã€‚è¯¥åŸºå‡†åˆ©ç”¨æ—¥æœ¬åŒ»å¸ˆå›½å®¶è€ƒè¯• (JMLE) çš„è‡ªåŠ¨åŒ–æµæ°´çº¿æ„å»ºè€Œæˆï¼ŒåŒ…å«äº† MedRECT-ja å’Œ MedRECT-en ä¸¤ä¸ªè§„æ¨¡ç›¸å½“çš„å¹³è¡¡æ•°æ®é›†ã€‚å®éªŒé€šè¿‡è¯„ä¼° 9 ç§ä¸»æµå¤§è¯­è¨€æ¨¡å‹ (LLMs) å‘ç°ï¼Œæ¨ç†æ¨¡å‹ (Reasoning models) åœ¨é”™è¯¯æ£€æµ‹å’Œå¥å­æå–æ–¹é¢çš„è¡¨ç°æ˜¾è‘—ä¼˜äºæ ‡å‡†æ¶æ„ã€‚è™½ç„¶è‹±è¯­å’Œæ—¥è¯­ä¹‹é—´å­˜åœ¨ 5-10% çš„æ€§èƒ½å·®è·ï¼Œä½†æ¨ç†æ¨¡å‹è¡¨ç°å‡ºæ›´å°çš„è·¨è¯­è¨€å·®å¼‚ã€‚é€šè¿‡é’ˆå¯¹æ€§çš„ LoRA å¾®è°ƒï¼Œæ¨¡å‹åœ¨ä¿ç•™æ¨ç†èƒ½åŠ›çš„åŒæ—¶æ˜¾è‘—æå‡äº†é”™è¯¯ä¿®æ­£æ€§èƒ½ï¼Œå…¶è¡¨ç°ç”šè‡³è¶…è¿‡äº†äººç±»ä¸“å®¶ã€‚ä½œä¸ºé¦–ä¸ªå…¨é¢çš„è·¨è¯­è¨€åŒ»ç–—é”™è¯¯ä¿®æ­£åŸºå‡†ï¼ŒMedRECT ä¸ºå¼€å‘æ›´å®‰å…¨ã€è·¨è¯­è¨€çš„åŒ»ç–— LLMs æä¾›äº†é‡è¦çš„å¯å¤åˆ¶æ¡†æ¶å’Œèµ„æºæ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00421v1",
      "published_date": "2025-11-01 06:19:34 UTC",
      "updated_date": "2025-11-01 06:19:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:12:11.516255+00:00"
    },
    {
      "arxiv_id": "2511.00419v1",
      "title": "LGCA: Enhancing Semantic Representation via Progressive Expansion",
      "title_zh": "LGCAï¼šé€šè¿‡æ¸è¿›å¼æ‰©å¼ å¢å¼ºè¯­ä¹‰è¡¨ç¤º",
      "authors": [
        "Thanh Hieu Cao",
        "Trung Khang Tran",
        "Gia Thinh Pham",
        "Tuong Nghiem Diep",
        "Thanh Binh Nguyen"
      ],
      "abstract": "Recent advancements in large-scale pretraining in natural language processing have enabled pretrained vision-language models such as CLIP to effectively align images and text, significantly improving performance in zero-shot image classification tasks. Subsequent studies have further demonstrated that cropping images into smaller regions and using large language models to generate multiple descriptions for each caption can further enhance model performance. However, due to the inherent sensitivity of CLIP, random image crops can introduce misinformation and bias, as many images share similar features at small scales. To address this issue, we propose Localized-Globalized Cross-Alignment (LGCA), a framework that first captures the local features of an image and then repeatedly selects the most salient regions and expands them. The similarity score is designed to incorporate both the original and expanded images, enabling the model to capture both local and global features while minimizing misinformation. Additionally, we provide a theoretical analysis demonstrating that the time complexity of LGCA remains the same as that of the original model prior to the repeated expansion process, highlighting its efficiency and scalability. Extensive experiments demonstrate that our method substantially improves zero-shot performance across diverse datasets, outperforming state-of-the-art baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Localized-Globalized Cross-Alignment (LGCA) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ CLIP ç­‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ Zero-shot å›¾åƒåˆ†ç±»ä¸­å› éšæœºè£å‰ªå›¾åƒè€Œäº§ç”Ÿçš„è¯¯å¯¼ä¿¡æ¯ä¸åå·®é—®é¢˜ã€‚LGCA é¦–å…ˆæ•æ‰å›¾åƒçš„ Local featuresï¼Œéšåé€šè¿‡é‡å¤é€‰æ‹©æœ€æ˜¾è‘—åŒºåŸŸå¹¶è¿›è¡Œé€æ­¥æ‰©å¼ ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŒæ—¶å…¼é¡¾å±€éƒ¨ä¸å…¨å±€ç‰¹å¾ã€‚è¯¥æ¡†æ¶è®¾è®¡äº†æ•´åˆåŸå§‹å›¾åƒä¸æ‰©å¼ å›¾åƒçš„ç›¸ä¼¼åº¦è¯„åˆ†æœºåˆ¶ï¼Œåœ¨æœ‰æ•ˆå‡å°‘ Misinformation çš„åŒæ—¶å¢å¼ºäº†è¯­ä¹‰è¡¨è¾¾èƒ½åŠ›ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒLGCA åœ¨ä¿æŒé«˜æ•ˆ Scalability çš„å‰æä¸‹ï¼Œå…¶æ—¶é—´å¤æ‚åº¦ä¸åŸå§‹æ¨¡å‹ç›¸å½“ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šé¡¹æ•°æ®é›†ä¸Šçš„ Zero-shot æ€§èƒ½æ˜¾è‘—æå‡ï¼Œè¡¨ç°ä¼˜äºç›®å‰çš„ State-of-the-art åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 5 figures, to appear in SoICT 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.00419v1",
      "published_date": "2025-11-01 06:09:42 UTC",
      "updated_date": "2025-11-01 06:09:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:13:18.954545+00:00"
    },
    {
      "arxiv_id": "2511.00417v2",
      "title": "Human-AI Programming Role Optimization: Developing a Personality-Driven Self-Determination Framework",
      "title_zh": "äººæœºåä½œç¼–ç¨‹è§’è‰²ä¼˜åŒ–ï¼šæ„å»ºåŸºäºäººæ ¼ç‰¹è´¨çš„è‡ªæˆ‘å†³å®šæ¡†æ¶",
      "authors": [
        "Marcel Valovy"
      ],
      "abstract": "As artificial intelligence transforms software development, a critical question emerges: how can developers and AI systems collaborate most effectively? This dissertation optimizes human-AI programming roles through self-determination theory and personality psychology, introducing the Role Optimization Motivation Alignment (ROMA) framework.\n  Through Design Science Research spanning five cycles, this work establishes empirically-validated connections between personality traits, programming role preferences, and collaborative outcomes, engaging 200 experimental participants and 46 interview respondents.\n  Key findings demonstrate that personality-driven role optimization significantly enhances self-determination and team dynamics, yielding 23% average motivation increases among professionals and up to 65% among undergraduates. Five distinct personality archetypes emerge: The Explorer (high Openness/low Agreeableness), The Orchestrator (high Extraversion/Agreeableness), The Craftsperson (high Neuroticism/low Extraversion), The Architect (high Conscientiousness), and The Adapter (balanced profile). Each exhibits distinct preferences for programming roles (Co-Pilot, Co-Navigator, Agent), with assignment modes proving crucial for satisfaction.\n  The dissertation contributes: (1) an empirically-validated framework linking personality traits to role preferences and self-determination outcomes; (2) a taxonomy of AI collaboration modalities mapped to personality profiles while preserving human agency; and (3) an ISO/IEC 29110 extension enabling Very Small Entities to implement personality-driven role optimization within established standards.\n  Keywords: artificial intelligence, human-computer interaction, behavioral software engineering, self-determination theory, personality psychology, phenomenology, intrinsic motivation, pair programming, design science research, ISO/IEC 29110",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨ä¼˜åŒ–äººå·¥æ™ºèƒ½è¾…åŠ©è½¯ä»¶å¼€å‘ä¸­çš„äººæœºåä½œè§’è‰²ï¼ŒåŸºäºè‡ªæˆ‘å†³å®šç†è®º(Self-Determination Theory)å’Œäººæ ¼å¿ƒç†å­¦æå‡ºäº†Role Optimization Motivation Alignment (ROMA)æ¡†æ¶ã€‚é€šè¿‡äº”ä¸ªå‘¨æœŸçš„è®¾è®¡ç§‘å­¦ç ”ç©¶(Design Science Research)ï¼Œè¯¥å·¥ä½œåœ¨200åå®éªŒå‚ä¸è€…å’Œ46åè®¿è°ˆè€…çš„åŸºç¡€ä¸Šï¼Œå®è¯äº†äººæ ¼ç‰¹è´¨ã€ç¼–ç¨‹è§’è‰²åå¥½ä¸åä½œæˆæœä¹‹é—´çš„å…³è”ã€‚ç ”ç©¶å‘ç°ï¼Œé©±åŠ¨äººæ ¼çš„è§’è‰²ä¼˜åŒ–èƒ½æ˜¾è‘—æå‡è‡ªæˆ‘å†³å®šæ„Ÿï¼Œä½¿ä¸“ä¸šäººå£«å’Œæœ¬ç§‘ç”Ÿçš„åŠ¨æœºåˆ†åˆ«å¹³å‡æé«˜23%å’Œ65%ã€‚ç ”ç©¶è¯†åˆ«å‡ºExplorerã€Orchestratorã€Craftspersonã€Architectå’ŒAdapteräº”ç§äººæ ¼åŸå‹ï¼Œå¹¶æ˜ç¡®äº†å®ƒä»¬å¯¹Co-Pilotã€Co-Navigatorå’ŒAgentè§’è‰²çš„ä¸åŒåå¥½ã€‚è®ºæ–‡çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ä¸€ä¸ªè”ç³»äººæ ¼ä¸è§’è‰²çš„å®è¯æ¡†æ¶ï¼Œä»¥åŠä¸€å¥—åœ¨ä¿ç•™äººç±»ä¸»ä½“æ€§çš„åŒæ—¶å°†AIåä½œæ¨¡å¼ä¸äººæ ¼ç”»åƒåŒ¹é…çš„åˆ†ç±»å­¦ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶é€šè¿‡æ‰©å±•ISO/IEC 29110æ ‡å‡†ï¼Œä¸ºå°å‹å®ä½“å®æ–½äººæ ¼é©±åŠ¨çš„è§’è‰²ä¼˜åŒ–æä¾›äº†æ ‡å‡†åŒ–æ”¯æŒï¼Œä¸ºè½¯ä»¶å·¥ç¨‹ä¸­çš„è¡Œä¸ºç ”ç©¶å’Œäººæœºåä½œæä¾›äº†ç³»ç»ŸåŒ–çš„æŒ‡å¯¼æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "PhD Dissertation, Prague University of Economics and Business, 2025. 323 pages. ACM CCS 2012: Human-computer interaction, Collaborative interaction, Human-AI collaborative systems, Pair programming, AI-assisted software engineering",
      "pdf_url": "https://arxiv.org/pdf/2511.00417v2",
      "published_date": "2025-11-01 06:00:14 UTC",
      "updated_date": "2025-11-27 03:36:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:13:10.844605+00:00"
    },
    {
      "arxiv_id": "2511.00416v1",
      "title": "PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks",
      "title_zh": "PADBenï¼šä¸€ä¸ªç”¨äºè¯„ä¼°AIæ–‡æœ¬æ£€æµ‹å™¨é˜²å¾¡æ”¹å†™æ”»å‡»èƒ½åŠ›çš„å…¨é¢åŸºå‡†",
      "authors": [
        "Yiwei Zha",
        "Rui Min",
        "Shanu Sushmita"
      ],
      "abstract": "While AI-generated text (AIGT) detectors achieve over 90\\% accuracy on direct LLM outputs, they fail catastrophically against iteratively-paraphrased content. We investigate why iteratively-paraphrased text -- itself AI-generated -- evades detection systems designed for AIGT identification. Through intrinsic mechanism analysis, we reveal that iterative paraphrasing creates an intermediate laundering region characterized by semantic displacement with preserved generation patterns, which brings up two attack categories: paraphrasing human-authored text (authorship obfuscation) and paraphrasing LLM-generated text (plagiarism evasion). To address these vulnerabilities, we introduce PADBen, the first benchmark systematically evaluating detector robustness against both paraphrase attack scenarios. PADBen comprises a five-type text taxonomy capturing the full trajectory from original content to deeply laundered text, and five progressive detection tasks across sentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art detectors, revealing critical asymmetry: detectors successfully identify the plagiarism evasion problem but fail for the case of authorship obfuscation. Our findings demonstrate that current detection approaches cannot effectively handle the intermediate laundering region, necessitating fundamental advances in detection architectures beyond existing semantic and stylistic discrimination methods. For detailed code implementation, please see https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†AIæ–‡æœ¬æ£€æµ‹å™¨åœ¨é¢å¯¹è¿­ä»£æ”¹å†™(iterative paraphrasing)æ”»å‡»æ—¶çš„è„†å¼±æ€§ï¼Œæ­ç¤ºäº†æ”»å‡»å¦‚ä½•é€šè¿‡è¯­ä¹‰ä½ç§»(semantic displacement)å’Œä¿ç•™ç”Ÿæˆæ¨¡å¼æ„å»ºçš„ä¸­é—´æ¸…æ´—åŒºåŸŸ(intermediate laundering region)æ¥é€ƒé¿è¯†åˆ«ã€‚ç ”ç©¶è¯†åˆ«äº†ä¸¤ç±»å…³é”®æ”»å‡»åœºæ™¯ï¼šäººç±»æ–‡æœ¬çš„ä½œè€…èº«ä»½æ¨¡ç³Š(authorship obfuscation)ä»¥åŠLLMç”Ÿæˆæ–‡æœ¬çš„å‰½çªƒè§„é¿(plagiarism evasion)ï¼Œå¹¶æ®æ­¤æ¨å‡ºäº†PADBenåŸºå‡†æµ‹è¯•ã€‚PADBené€šè¿‡äº”ç±»æ–‡æœ¬åˆ†ç±»ä½“ç³»å’Œäº”é¡¹æ¸è¿›å¼ä»»åŠ¡ï¼Œå¯¹11ç§æœ€å…ˆè¿›çš„æ£€æµ‹å™¨è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œå‘ç°æ£€æµ‹å™¨åœ¨è¯†åˆ«å‰½çªƒè§„é¿æ—¶è¡¨ç°å°šå¯ï¼Œä½†åœ¨å¤„ç†ä½œè€…èº«ä»½æ¨¡ç³Šæ—¶å­˜åœ¨ä¸¥é‡ç¼ºé™·ã€‚å®éªŒç»“æœè¯æ˜ç›®å‰çš„æ£€æµ‹æ–¹æ³•æ— æ³•æœ‰æ•ˆå¤„ç†ä¸­é—´æ¸…æ´—åŒºåŸŸï¼Œå› æ­¤äºŸéœ€å¼€å‘è¶…è¶Šç°æœ‰è¯­ä¹‰å’Œé£æ ¼é‰´åˆ«æ‰‹æ®µçš„æ–°å‹æ£€æµ‹æ¶æ„ã€‚è¯¥ç ”ç©¶ä¸ºè¯„ä¼°å’Œæå‡AIæ–‡æœ¬æ£€æµ‹å™¨çš„é²æ£’æ€§æä¾›äº†é‡è¦å·¥å…·å’Œè§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00416v1",
      "published_date": "2025-11-01 05:59:46 UTC",
      "updated_date": "2025-11-01 05:59:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:12:35.711309+00:00"
    },
    {
      "arxiv_id": "2511.00411v1",
      "title": "Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling",
      "title_zh": "é€šè¿‡æ¢¯åº¦å¼•å¯¼é‡‡æ ·å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œå¢å¼ºå¯¹æŠ—è¿ç§»æ€§",
      "authors": [
        "Zenghao Niu",
        "Weicheng Xie",
        "Siyang Song",
        "Zitong Yu",
        "Feng Liu",
        "Linlin Shen"
      ],
      "abstract": "Adversarial attacks present a critical challenge to deep neural networks' robustness, particularly in transfer scenarios across different model architectures. However, the transferability of adversarial attacks faces a fundamental dilemma between Exploitation (maximizing attack potency) and Exploration (enhancing cross-model generalization). Traditional momentum-based methods over-prioritize Exploitation, i.e., higher loss maxima for attack potency but weakened generalization (narrow loss surface). Conversely, recent methods with inner-iteration sampling over-prioritize Exploration, i.e., flatter loss surfaces for cross-model generalization but weakened attack potency (suboptimal local maxima). To resolve this dilemma, we propose a simple yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives through guiding sampling along the gradient ascent direction to improve both sampling efficiency and stability. Specifically, based on MI-FGSM, GGS introduces inner-iteration random sampling and guides the sampling direction using the gradient from the previous inner-iteration (the sampling's magnitude is determined by a random distribution). This mechanism encourages adversarial examples to reside in balanced regions with both flatness for cross-model generalization and higher local maxima for strong attack potency. Comprehensive experiments across multiple DNN architectures and multimodal large language models (MLLMs) demonstrate the superiority of our method over state-of-the-art transfer attacks. Code is made available at https://github.com/anuin-cat/GGS.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯¹æŠ—æ”»å‡»åœ¨è·¨æ¨¡å‹è¿ç§»åœºæ™¯ä¸­é¢ä¸´çš„åˆ©ç”¨(Exploitation)ä¸æ¢ç´¢(Exploration)ä¹‹é—´çš„æƒè¡¡å›°å¢ƒï¼ŒæŒ‡å‡ºä¼ ç»ŸåŠ¨é‡æ–¹æ³•ä¸å†…éƒ¨é‡‡æ ·æ–¹æ³•åˆ†åˆ«å­˜åœ¨æ³›åŒ–æ€§å¼±æˆ–æ”»å‡»æ•ˆåŠ›ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€çŸ›ç›¾ï¼Œä½œè€…æå‡ºäº†æ¢¯åº¦å¼•å¯¼é‡‡æ ·(Gradient-Guided Sampling, GGS)æ–¹æ³•ï¼Œé€šè¿‡æ²¿æ¢¯åº¦ä¸Šå‡æ–¹å‘å¼•å¯¼å†…éƒ¨è¿­ä»£é‡‡æ ·æ¥åŒæ—¶ä¼˜åŒ–é‡‡æ ·æ•ˆç‡ä¸ç¨³å®šæ€§ã€‚åœ¨MI-FGSMçš„åŸºç¡€ä¸Šï¼ŒGGSåˆ©ç”¨å‰ä¸€æ¬¡è¿­ä»£çš„æ¢¯åº¦å¼•å¯¼é‡‡æ ·æ–¹å‘ï¼Œä¿ƒä½¿å¯¹æŠ—æ ·æœ¬è½åœ¨å…¼å…·æŸå¤±å¹³é¢å¹³å¦åº¦(Flatness)ä¸é«˜å±€éƒ¨æœ€å¤§å€¼(Higher local maxima)çš„å¹³è¡¡åŒºåŸŸã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†æ”»å‡»æ‰‹æ®µåœ¨å¢å¼ºè·¨æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ï¼Œä¾ç„¶èƒ½ä¿æŒæé«˜çš„æ”»å‡»å¼ºåº¦ã€‚åœ¨å¤šç§æ·±åº¦ç¥ç»ç½‘ç»œ(DNN)æ¶æ„ä»¥åŠå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)ä¸Šçš„å®éªŒè¯æ˜ï¼ŒGGSçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›è¿ç§»æ”»å‡»æŠ€æœ¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "accepted by iccv 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.00411v1",
      "published_date": "2025-11-01 05:43:47 UTC",
      "updated_date": "2025-11-01 05:43:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:12:56.700308+00:00"
    },
    {
      "arxiv_id": "2511.00406v2",
      "title": "Quantum Machine Unlearning: Foundations, Mechanisms, and Taxonomy",
      "title_zh": "é‡å­æœºå™¨é—å¿˜ï¼šç†è®ºåŸºç¡€ã€æœºåˆ¶ä¸åˆ†ç±»ä½“ç³»",
      "authors": [
        "Thanveer Shaik",
        "Xiaohui Tao",
        "Haoran Xie"
      ],
      "abstract": "Quantum Machine Unlearning has emerged as a foundational challenge at the intersection of quantum information theory privacypreserving computation and trustworthy artificial intelligence This paper advances QMU by establishing a formal framework that unifies physical constraints algorithmic mechanisms and ethical governance within a verifiable paradigm We define forgetting as a contraction of distinguishability between pre and postunlearning models under completely positive trace-preserving dynamics grounding data removal in the physics of quantum irreversibility Building on this foundation we present a fiveaxis taxonomy spanning scope guarantees mechanisms system context and hardware realization linking theoretical constructs to implementable strategies Within this structure we incorporate influence and quantum Fisher information weighted updates parameter reinitialization and kernel alignment as practical mechanisms compatible with noisy intermediatescale quantum NISQ devices The framework extends naturally to federated and privacyaware settings via quantum differential privacy homomorphic encryption and verifiable delegation enabling scalable auditable deletion across distributed quantum systems Beyond technical design we outline a forwardlooking research roadmap emphasizing formal proofs of forgetting scalable and secure architectures postunlearning interpretability and ethically auditable governance Together these contributions elevate QMU from a conceptual notion to a rigorously defined and ethically aligned discipline bridging physical feasibility algorithmic verifiability and societal accountability in the emerging era of quantum intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é‡å­æœºå™¨é—å¿˜ (Quantum Machine Unlearning, QMU) çš„ç†è®ºåŸºç¡€ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€ç‰©ç†çº¦æŸã€ç®—æ³•æœºåˆ¶ä¸ä¼¦ç†æ²»ç†çš„æ­£å¼æ¡†æ¶ã€‚ä½œè€…å°†â€œé—å¿˜â€å®šä¹‰ä¸ºåœ¨å®Œå…¨æ­£ä¿è¿¹ (CPTP) åŠ¨åŠ›å­¦ä¸‹ï¼Œé—å¿˜å‰åæ¨¡å‹é—´å¯åˆ†è¾¨æ€§çš„æ”¶ç¼©ï¼Œä»è€Œå°†æ•°æ®åˆ é™¤æ¤æ ¹äºé‡å­ä¸å¯é€†ç‰©ç†å­¦ã€‚è®ºæ–‡æå‡ºäº†æ¶µç›–èŒƒå›´ã€ä¿è¯ã€æœºåˆ¶ã€ç³»ç»ŸèƒŒæ™¯åŠç¡¬ä»¶å®ç°äº”ä¸ªç»´åº¦çš„åˆ†ç±»æ³• (Taxonomy)ï¼Œæœ‰æ•ˆè¿æ¥äº†ç†è®ºæ„å»ºä¸å¯å®æ–½ç­–ç•¥ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†é‡å­è´¹èˆå°”ä¿¡æ¯ (Quantum Fisher Information) åŠ æƒæ›´æ–°ã€å‚æ•°é‡æ–°åˆå§‹åŒ–å’Œæ ¸å¯¹é½ (Kernel Alignment) ç­‰é€‚ç”¨äºå«å™ªä¸­ç­‰è§„æ¨¡é‡å­ (NISQ) è®¾å¤‡çš„å®ç”¨æœºåˆ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡é‡å­å·®åˆ†éšç§ (Quantum Differential Privacy) å’ŒåŒæ€åŠ å¯†ç­‰æŠ€æœ¯æ‰©å±•è‡³è”é‚¦å­¦ä¹ åŠéšç§æ„ŸçŸ¥åœºæ™¯ï¼Œå®ç°äº†åˆ†å¸ƒå¼é‡å­ç³»ç»Ÿä¸­çš„å¯æ‰©å±•å®¡è®¡åˆ é™¤ã€‚æœ€åï¼Œç ”ç©¶å±•æœ›äº†å…³äºå½¢å¼åŒ–è¯æ˜ã€å®‰å…¨æ¶æ„å’Œå¯å®¡è®¡æ²»ç†çš„ç§‘ç ”è·¯çº¿å›¾ï¼Œå°† QMU ä»æ¦‚å¿µæå‡ä¸ºä¸€é—¨ç»“åˆç‰©ç†å¯è¡Œæ€§ã€ç®—æ³•å¯éªŒè¯æ€§ä¸ç¤¾ä¼šè´£ä»»æ„Ÿçš„ä¸¥è°¨å­¦ç§‘ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00406v2",
      "published_date": "2025-11-01 05:11:40 UTC",
      "updated_date": "2026-01-13 07:06:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:13:19.456229+00:00"
    },
    {
      "arxiv_id": "2511.00405v1",
      "title": "UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings",
      "title_zh": "UME-R1ï¼šæ¢ç´¢æ¨ç†é©±åŠ¨çš„ç”Ÿæˆå¼å¤šæ¨¡æ€åµŒå…¥",
      "authors": [
        "Zhibin Lan",
        "Liqiang Niu",
        "Fandong Meng",
        "Jie Zhou",
        "Jinsong Su"
      ],
      "abstract": "The remarkable success of multimodal large language models (MLLMs) has driven advances in multimodal embeddings, yet existing models remain inherently discriminative, limiting their ability to benefit from reasoning-driven generation paradigm. In this work, we pioneer the exploration of generative embeddings, unifying embedding tasks within a generative paradigm. We propose UME-R1, a universal multimodal embedding framework consisting of a two-stage training strategy: a cold-start supervised fine-tuning equips the model with reasoning capabilities and enables it to generate both discriminative and generative embeddings; a subsequent reinforcement learning enhances reasoning and further optimizes generative embedding quality. This pioneering work reveals four key insights: 1) generative embeddings unlock substantial performance gains over conventional discriminative embeddings by leveraging the powerful generative reasoning capabilities of MLLMs; 2) discriminative and generative embeddings are complementary, whose combined oracle performance far exceeding that of either alone; 3) RL can effectively enhance generative embeddings, establishing a scalable optimization paradigm.; 4) repeated sampling at inference boosts downstream task coverage (pass@k), highlighting the inference-time scalability potential of generative embeddings. Evaluated on the MMEB-V2 benchmark across 78 tasks spanning video, image, and visual documents, UME-R1 significantly outperforms conventional discriminative embedding models and offers a foundation for more interpretable, reasoning-driven generative multimodal embeddings. Our code, models, and datasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† UME-R1ï¼Œä¸€ç§é€šç”¨çš„å¤šæ¨¡æ€åµŒå…¥æ¡†æ¶ï¼Œæ—¨åœ¨å°†åµŒå…¥ä»»åŠ¡ç»Ÿä¸€äºç”ŸæˆèŒƒå¼ (Generative paradigm) ä¹‹ä¸­ï¼Œä»¥è§£å†³ç°æœ‰æ¨¡å‹åœ¨æ¨ç†é©±åŠ¨ç”Ÿæˆæ–¹é¢çš„å±€é™ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆé€šè¿‡å†·å¯åŠ¨çš„ Supervised Fine-tuning (SFT) ä½¿æ¨¡å‹å…·å¤‡ç”Ÿæˆ Discriminative å’Œ Generative åµŒå…¥çš„æ¨ç†èƒ½åŠ›ï¼Œéšåé€šè¿‡ Reinforcement Learning (RL) è¿›ä¸€æ­¥ä¼˜åŒ–åµŒå…¥è´¨é‡ã€‚ç ”ç©¶æ­ç¤ºï¼ŒGenerative åµŒå…¥é€šè¿‡åˆ©ç”¨ MLLMs çš„æ¨ç†èƒ½åŠ›æ˜¾è‘—ä¼˜äºä¼ ç»Ÿåˆ¤åˆ«å¼åµŒå…¥ï¼Œä¸”ä¸¤è€…è¡¨ç°å‡ºæå¼ºçš„äº’è¡¥æ€§ã€‚æ­¤å¤–ï¼Œå®éªŒè¯æ˜ RL èƒ½æœ‰æ•ˆå¢å¼ºç”ŸæˆåµŒå…¥çš„æ€§èƒ½ï¼Œä¸”æ¨ç†æ—¶çš„é‡å¤é‡‡æ ·å±•ç¤ºäº†å…¶åœ¨ä¸‹æ¸¸ä»»åŠ¡è¦†ç›–ç‡ (pass@k) ä¸Šçš„æ¨ç†æ—¶é—´æ‰©å±• (Inference-time scalability) æ½œåŠ›ã€‚åœ¨æ¶µç›–è§†é¢‘ã€å›¾åƒå’Œæ–‡æ¡£çš„ MMEB-V2 åŸºå‡†æµ‹è¯•ä¸­ï¼ŒUME-R1 åœ¨ 78 é¡¹ä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿæ¨¡å‹ï¼Œä¸ºæ„å»ºæ›´å…·è§£é‡Šæ€§å’Œæ¨ç†é©±åŠ¨çš„ç”Ÿæˆå¼å¤šæ¨¡æ€åµŒå…¥å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00405v1",
      "published_date": "2025-11-01 05:04:23 UTC",
      "updated_date": "2025-11-01 05:04:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:13:50.147562+00:00"
    },
    {
      "arxiv_id": "2511.00402v1",
      "title": "Emotion Detection in Speech Using Lightweight and Transformer-Based Models: A Comparative and Ablation Study",
      "title_zh": "åŸºäºè½»é‡çº§ Transformer æ¨¡å‹çš„è¯­éŸ³æƒ…æ„Ÿæ£€æµ‹ï¼šå¯¹æ¯”ä¸æ¶ˆèç ”ç©¶",
      "authors": [
        "Lucky Onyekwelu-Udoka",
        "Md Shafiqul Islam",
        "Md Shahedul Hasan"
      ],
      "abstract": "Emotion recognition from speech plays a vital role in the development of empathetic human-computer interaction systems. This paper presents a comparative analysis of lightweight transformer-based models, DistilHuBERT and PaSST, by classifying six core emotions from the CREMA-D dataset. We benchmark their performance against a traditional CNN-LSTM baseline model using MFCC features. DistilHuBERT demonstrates superior accuracy (70.64%) and F1 score (70.36%) while maintaining an exceptionally small model size (0.02 MB), outperforming both PaSST and the baseline. Furthermore, we conducted an ablation study on three variants of the PaSST, Linear, MLP, and Attentive Pooling heads, to understand the effect of classification head architecture on model performance. Our results indicate that PaSST with an MLP head yields the best performance among its variants but still falls short of DistilHuBERT. Among the emotion classes, angry is consistently the most accurately detected, while disgust remains the most challenging. These findings suggest that lightweight transformers like DistilHuBERT offer a compelling solution for real-time speech emotion recognition on edge devices. The code is available at: https://github.com/luckymaduabuchi/Emotion-detection-.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ï¼Œå¯¹è½»é‡åŒ– Transformer æ¨¡å‹ DistilHuBERT å’Œ PaSST è¿›è¡Œäº†å¯¹æ¯”åˆ†æä¸æ¶ˆèå®éªŒ(Ablation Study)ï¼Œå¹¶ä¸åŸºäº MFCC ç‰¹å¾çš„ä¼ ç»Ÿ CNN-LSTM åŸºå‡†æ¨¡å‹è¿›è¡Œäº† Benchmarkã€‚é€šè¿‡åœ¨ CREMA-D æ•°æ®é›†ä¸Šåˆ†ç±»å…­ç§æ ¸å¿ƒæƒ…æ„Ÿï¼Œå®éªŒè¯æ˜ DistilHuBERT åœ¨æ¨¡å‹å°ºå¯¸ä»…ä¸º 0.02 MB çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº† 70.64% çš„å‡†ç¡®ç‡å’Œ 70.36% çš„ F1 Scoreï¼Œæ€§èƒ½ä¼˜äº PaSST å’ŒåŸºå‡†æ¨¡å‹ã€‚ç ”ç©¶è¿˜å¯¹ PaSST çš„ä¸‰ç§åˆ†ç±»å¤´æ¶æ„ï¼ˆLinearã€MLP å’Œ Attentive Poolingï¼‰è¿›è¡Œäº†æµ‹è¯•ï¼Œå‘ç° MLP åˆ†ç±»å¤´è¡¨ç°æœ€ä½³ï¼Œä½†æ•´ä½“ä»ä¸åŠ DistilHuBERTã€‚åœ¨å…·ä½“æƒ…æ„Ÿç±»åˆ«ä¸­ï¼Œæ¨¡å‹å¯¹ Angry çš„æ£€æµ‹æœ€ä¸ºå‡†ç¡®ï¼Œè€Œ Disgust æœ€éš¾è¯†åˆ«ã€‚ç ”ç©¶ç»“è®ºæŒ‡å‡ºï¼ŒDistilHuBERT è¿™ç§è½»é‡åŒ– Transformer ä¸ºè¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®æ—¶è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æä¾›äº†ä¸€ç§é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00402v1",
      "published_date": "2025-11-01 05:01:04 UTC",
      "updated_date": "2025-11-01 05:01:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 1,
      "last_update": "2026-01-25T07:14:55.121184+00:00"
    },
    {
      "arxiv_id": "2511.00392v1",
      "title": "SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping",
      "title_zh": "SonarSweepï¼šåŸºäºå¹³é¢æ‰«æ èåˆå£°å‘ä¸è§†è§‰çš„é²æ£’ä¸‰ç»´é‡å»º",
      "authors": [
        "Lingpeng Chen",
        "Jiakun Tang",
        "Apple Pui-Yi Chui",
        "Ziyang Hong",
        "Junfeng Wu"
      ],
      "abstract": "Accurate 3D reconstruction in visually-degraded underwater environments remains a formidable challenge. Single-modality approaches are insufficient: vision-based methods fail due to poor visibility and geometric constraints, while sonar is crippled by inherent elevation ambiguity and low resolution. Consequently, prior fusion technique relies on heuristics and flawed geometric assumptions, leading to significant artifacts and an inability to model complex scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep learning framework that overcomes these limitations by adapting the principled plane sweep algorithm for cross-modal fusion between sonar and visual data. Extensive experiments in both high-fidelity simulation and real-world environments demonstrate that SonarSweep consistently generates dense and accurate depth maps, significantly outperforming state-of-the-art methods across challenging conditions, particularly in high turbidity. To foster further research, we will publicly release our code and a novel dataset featuring synchronized stereo-camera and sonar data, the first of its kind.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ°´ä¸‹è§†çº¿å—é˜»ç¯å¢ƒä¸­3Dé‡å»ºçš„éš¾é¢˜ï¼Œæå‡ºäº†SonarSweepï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ”¹è¿›ç»å…¸çš„å¹³é¢æ‰«æ(plane sweep)ç®—æ³•ï¼Œå®ç°äº†å£°å‘(sonar)ä¸è§†è§‰(vision)æ•°æ®çš„è·¨æ¨¡æ€èåˆï¼Œæœ‰æ•ˆå…‹æœäº†å•ä¸€è§†è§‰æ¨¡æ€åœ¨ä½èƒ½è§åº¦ä¸‹çš„å¤±æ•ˆï¼Œä»¥åŠå£°å‘æ¨¡æ€å›ºæœ‰çš„é«˜åº¦æ­§ä¹‰(elevation ambiguity)å’Œä½åˆ†è¾¨ç‡(low resolution)é—®é¢˜ã€‚åœ¨é«˜åº¦ä»¿çœŸå’ŒçœŸå®ç¯å¢ƒä¸­çš„å®éªŒç»“æœè¯æ˜ï¼ŒSonarSweepèƒ½å¤Ÿä¸€è‡´åœ°ç”Ÿæˆç²¾ç¡®ä¸”ç¨ å¯†çš„æ·±åº¦å›¾(depth maps)ï¼Œåœ¨åŒ…æ‹¬é«˜æµ‘æµŠåº¦(high turbidity)åœ¨å†…çš„æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å°†å…¬å¼€ç›¸å…³çš„ä»£ç ä»¥åŠé¦–ä¸ªåŒ…å«åŒæ­¥åŒç›®è§†è§‰ä¸å£°å‘æ•°æ®çš„åˆ›æ–°æ•°æ®é›†ï¼Œä¸ºæ°´ä¸‹æœºå™¨äººæ„ŸçŸ¥é¢†åŸŸçš„åç»­ç ”ç©¶æä¾›é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 9 figures, conference",
      "pdf_url": "https://arxiv.org/pdf/2511.00392v1",
      "published_date": "2025-11-01 04:12:27 UTC",
      "updated_date": "2025-11-01 04:12:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:13:51.456566+00:00"
    },
    {
      "arxiv_id": "2511.04698v2",
      "title": "multiMentalRoBERTa: A Fine-tuned Multiclass Classifier for Mental Health Disorder",
      "title_zh": "multiMentalRoBERTaï¼šä¸€ç§ç”¨äºå¿ƒç†å¥åº·éšœç¢å¤šåˆ†ç±»çš„å¾®è°ƒåˆ†ç±»å™¨",
      "authors": [
        "K M Sajjadul Islam",
        "John Fields",
        "Praveen Madiraju"
      ],
      "abstract": "The early detection of mental health disorders from social media text is critical for enabling timely support, risk assessment, and referral to appropriate resources. This work introduces multiMentalRoBERTa, a fine-tuned RoBERTa model designed for multiclass classification of common mental health conditions, including stress, anxiety, depression, post-traumatic stress disorder (PTSD), suicidal ideation, and neutral discourse. Drawing on multiple curated datasets, data exploration is conducted to analyze class overlaps, revealing strong correlations between depression and suicidal ideation as well as anxiety and PTSD, while stress emerges as a broad, overlapping category. Comparative experiments with traditional machine learning methods, domain-specific transformers, and prompting-based large language models demonstrate that multiMentalRoBERTa achieves superior performance, with macro F1-scores of 0.839 in the six-class setup and 0.870 in the five-class setup (excluding stress), outperforming both fine-tuned MentalBERT and baseline classifiers. Beyond predictive accuracy, explainability methods, including Layer Integrated Gradients and KeyBERT, are applied to identify lexical cues that drive classification, with a particular focus on distinguishing depression from suicidal ideation. The findings emphasize the effectiveness of fine-tuned transformers for reliable and interpretable detection in sensitive contexts, while also underscoring the importance of fairness, bias mitigation, and human-in-the-loop safety protocols. Overall, multiMentalRoBERTa is presented as a lightweight, robust, and deployable solution for enhancing support in mental health platforms.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† multiMentalRoBERTaï¼Œä¸€ç§ç»è¿‡å¾®è°ƒçš„ RoBERTa æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºç¤¾äº¤åª’ä½“æ–‡æœ¬ä¸­å‹åŠ› (stress)ã€ç„¦è™‘ (anxiety)ã€æŠ‘éƒ (depression)ã€åˆ›ä¼¤ååº”æ¿€éšœç¢ (PTSD)ã€è‡ªæ€æ„å¿µ (suicidal ideation) åŠä¸­æ€§è¯è¯­çš„å¤šåˆ†ç±»è¯†åˆ«ã€‚ç ”ç©¶é€šè¿‡åˆ†ææ•°æ®é›†æ­ç¤ºäº†æŠ‘éƒä¸è‡ªæ€æ„å¿µã€ç„¦è™‘ä¸ PTSD ä¹‹é—´çš„å¼ºç›¸å…³æ€§ï¼Œå¹¶å‘ç°å‹åŠ›æ˜¯ä¸€ä¸ªå…·æœ‰å¹¿æ³›é‡å ç‰¹å¾çš„ç±»åˆ«ã€‚å®éªŒè¡¨æ˜ï¼ŒmultiMentalRoBERTa åœ¨å…­åˆ†ç±»å’Œäº”åˆ†ç±»ä»»åŠ¡ä¸­åˆ†åˆ«å–å¾—äº† 0.839 å’Œ 0.870 çš„å® F1 åˆ†æ•° (macro F1-scores)ï¼Œæ€§èƒ½ä¼˜äº MentalBERTã€ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•åŠåŸºäºæç¤ºçš„å¤§è¯­è¨€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶åˆ©ç”¨ Layer Integrated Gradients å’Œ KeyBERT ç­‰æ–¹æ³•æå‡äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œé‡ç‚¹åˆ†æäº†åŒºåˆ†æŠ‘éƒä¸è‡ªæ€æ„å¿µçš„å…³é”®è¯æ±‡çº¿ç´¢ã€‚è¯¥å·¥ä½œè¿˜å¼ºè°ƒäº†åœ¨æ•æ„ŸèƒŒæ™¯ä¸‹å…¬å¹³æ€§ã€åå·®ç¼“è§£ä»¥åŠäººç±»å‚ä¸å®‰å…¨åè®® (human-in-the-loop safety protocols) çš„å¿…è¦æ€§ã€‚æ€»ä½“è€Œè¨€ï¼ŒmultiMentalRoBERTa ä¸ºå¿ƒç†å¥åº·å¹³å°æä¾›äº†ä¸€ç§è½»é‡ã€ç¨³å¥ä¸”æ˜“äºéƒ¨ç½²çš„è‡ªåŠ¨åŒ–é£é™©è¯„ä¼°ä¸æ”¯æŒè§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in IEEE Big Data, 8-11 December, 2025 @ Macau SAR, China",
      "pdf_url": "https://arxiv.org/pdf/2511.04698v2",
      "published_date": "2025-11-01 03:55:48 UTC",
      "updated_date": "2025-11-10 03:54:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:13:57.562508+00:00"
    },
    {
      "arxiv_id": "2511.00382v1",
      "title": "Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs",
      "title_zh": "æ•ˆç‡ä¸å¯¹é½çš„æƒè¡¡ï¼šå¤§è¯­è¨€æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒä¸­çš„å®‰å…¨æ€§ä¸å…¬å¹³æ€§é£é™©æ¢ç©¶",
      "authors": [
        "Mina Taraghi",
        "Yann Pequignot",
        "Amin Nikanjam",
        "Mohamed Amine Merzouk",
        "Foutse Khomh"
      ],
      "abstract": "Organizations are increasingly adopting and adapting Large Language Models (LLMs) hosted on public repositories such as HuggingFace. Although these adaptations often improve performance on specialized downstream tasks, recent evidence indicates that they can also degrade a model's safety or fairness. Since different fine-tuning techniques may exert distinct effects on these critical dimensions, this study undertakes a systematic assessment of their trade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA, IA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235 fine-tuned variants are evaluated across eleven safety hazard categories and nine demographic fairness dimensions. The results show that adapter-based approaches (LoRA, IA3) tend to improve safety scores and are the least disruptive to fairness, retaining higher accuracy and lower bias scores. In contrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce safety and cause larger fairness regressions, with decreased accuracy and increased bias. Alignment shifts are strongly moderated by base model type: LLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest safety decline, and Mistral, which is released without an internal moderation layer, displays the greatest variance. Improvements in safety do not necessarily translate into improvements in fairness, and no single configuration optimizes all fairness metrics simultaneously, indicating an inherent trade-off between these objectives. These findings suggest a practical guideline for safety-critical deployments: begin with a well-aligned base model, favour adapter-based PEFT, and conduct category-specific audits of both safety and fairness.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å‚æ•°é«˜æ•ˆå¾®è°ƒ(Parameter-Efficient Fine-Tuning, PEFT)è¿‡ç¨‹ä¸­çš„å®‰å…¨ä¸å…¬å¹³æ€§é£é™©ã€‚ä½œè€…å¯¹æ¯”äº†LoRAã€IA3ã€Prompt-Tuningå’ŒP-Tuningå››ç§æŠ€æœ¯åœ¨Llama-3ã€Qwen2.5ã€Mistralå’ŒGemmaç­‰æ¨¡å‹ä¸Šçš„è¡¨ç°ï¼Œè¯„ä¼°æ¶µç›–äº†11ç±»å®‰å…¨å±å®³å’Œ9ä¸ªå…¬å¹³æ€§ç»´åº¦ã€‚ç ”ç©¶å‘ç°ï¼ŒLoRAå’ŒIA3ç­‰åŸºäºé€‚é…å™¨(Adapter-based)çš„æ–¹æ³•é€šå¸¸èƒ½æå‡å®‰å…¨æ€§å¹¶ç»´æŒè¾ƒå¥½çš„å…¬å¹³æ€§ï¼Œè€ŒPrompt-Tuningå’ŒP-Tuningç­‰åŸºäºæç¤º(Prompt-based)çš„æ–¹æ³•åˆ™æ™®éå¯¼è‡´å®‰å…¨æ€§ä¸‹é™åŠæ›´ä¸¥é‡çš„å…¬å¹³æ€§åå·®ã€‚åŸºç¡€æ¨¡å‹å¯¹å¯¹é½åç§»æœ‰æ˜¾è‘—è°ƒèŠ‚ä½œç”¨ï¼Œå…¶ä¸­Llamaè¡¨ç°æœ€ä¸ºç¨³å¥ï¼Œè€ŒGemmaçš„å®‰å…¨æ€§ä¸‹é™æœ€å¿«ã€‚å®éªŒæ­ç¤ºäº†å®‰å…¨æ€§çš„æå‡å¹¶ä¸ç­‰åŒäºå…¬å¹³æ€§çš„æ”¹å–„ï¼Œä¸”æ— æ³•é€šè¿‡å•ä¸€é…ç½®åŒæ—¶ä¼˜åŒ–æ‰€æœ‰æŒ‡æ ‡ï¼Œè¡¨æ˜ä¸¤è€…é—´å­˜åœ¨å›ºæœ‰çš„æƒè¡¡(Trade-off)ã€‚æœ€åï¼Œç ”ç©¶å»ºè®®åœ¨å®‰å…¨æ€§å…³é”®éƒ¨ç½²ä¸­ï¼Œåº”ä¼˜å…ˆé€‰æ‹©å¯¹é½è‰¯å¥½çš„åŸºç¡€æ¨¡å‹ï¼Œé‡‡ç”¨åŸºäºé€‚é…å™¨çš„PEFTæ–¹æ³•ï¼Œå¹¶è¿›è¡Œé’ˆå¯¹æ€§çš„å®‰å…¨ä¸å…¬å¹³æ€§å®¡è®¡ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00382v1",
      "published_date": "2025-11-01 03:29:56 UTC",
      "updated_date": "2025-11-01 03:29:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:14:09.900005+00:00"
    },
    {
      "arxiv_id": "2511.00379v1",
      "title": "Diverse Human Value Alignment for Large Language Models via Ethical Reasoning",
      "title_zh": "åŸºäºä¼¦ç†æ¨ç†çš„å¤§è¯­è¨€æ¨¡å‹å¤šå…ƒäººç±»ä»·å€¼è§‚å¯¹é½",
      "authors": [
        "Jiahao Wang",
        "Songkai Xue",
        "Jinghui Li",
        "Xiaozhen Wang"
      ],
      "abstract": "Ensuring that Large Language Models (LLMs) align with the diverse and evolving human values across different regions and cultures remains a critical challenge in AI ethics. Current alignment approaches often yield superficial conformity rather than genuine ethical understanding, failing to address the complex, context-dependent nature of human values. In this paper, we propose a novel ethical reasoning paradigm for LLMs inspired by well-established ethical decision-making models, aiming at enhancing diverse human value alignment through deliberative ethical reasoning. Our framework consists of a structured five-step process, including contextual fact gathering, hierarchical social norm identification, option generation, multiple-lens ethical impact analysis, and reflection. This theory-grounded approach guides LLMs through an interpretable reasoning process that enhances their ability to understand regional specificities and perform nuanced ethical analysis, which can be implemented with either prompt engineering or supervised fine-tuning methods. We perform evaluations on the SafeWorld benchmark that specially designed for regional value alignment. Experimental results demonstrate our framework significantly improves LLM alignment with diverse human values compared to baseline methods, enabling more accurate social norm identification and more culturally appropriate reasoning. Our work provides a concrete pathway toward developing LLMs that align more effectively with the multifaceted values of global societies through interdisciplinary research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨è·¨åœ°åŒºå’Œæ–‡åŒ–çš„äººç±»ä»·å€¼è§‚å¯¹é½ (Value Alignment) ä¸­å­˜åœ¨çš„è¡¨é¢åŒ–ã€ç¼ºä¹æ·±åº¦ä¼¦ç†ç†è§£ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å—ä¼¦ç†å†³ç­–æ¨¡å‹å¯å‘çš„å…¨æ–°ä¼¦ç†æ¨ç†èŒƒå¼ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸€ä¸ªç»“æ„åŒ–çš„äº”æ­¥æµç¨‹ï¼Œæ¶µç›–èƒŒæ™¯äº‹å®æ”¶é›† (Contextual Fact Gathering)ã€å±‚çº§ç¤¾ä¼šè§„èŒƒè¯†åˆ« (Hierarchical Social Norm Identification)ã€æ–¹æ¡ˆç”Ÿæˆã€å¤šè§†è§’ä¼¦ç†å½±å“åˆ†æåŠåæ€ã€‚è¿™ç§åŸºäºç†è®ºçš„æ–¹æ³•é€šè¿‡å¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹å¢å¼ºäº†æ¨¡å‹ç†è§£åŒºåŸŸç‰¹æ€§å’Œè¿›è¡Œç»†è‡´ä¼¦ç†åˆ†æçš„èƒ½åŠ›ï¼Œä¸”æ”¯æŒæç¤ºå·¥ç¨‹ (Prompt Engineering) æˆ–ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning) ä¸¤ç§å®ç°æ–¹å¼ã€‚ç ”ç©¶è€…åœ¨ä¸“é—¨é’ˆå¯¹åŒºåŸŸä»·å€¼è§‚å¯¹é½è®¾è®¡çš„ SafeWorld åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æå‡äº† LLMs å¯¹å¤šå…ƒäººç±»ä»·å€¼è§‚çš„å¯¹é½æ•ˆæœã€‚è¯¥æ–¹æ¡ˆä¸ä»…ä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«ç¤¾ä¼šè§„èŒƒï¼Œè¿˜å®ç°äº†æ›´å…·æ–‡åŒ–é€‚åº”æ€§çš„æ¨ç†è¿‡ç¨‹ï¼Œä¸ºå¼€å‘èƒ½æœ‰æ•ˆå¯¹é½å…¨çƒç¤¾ä¼šå¤šç»´ä»·å€¼è§‚çš„ AI ç³»ç»Ÿæä¾›äº†åˆ‡å®è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by AIES 2025, camera-ready version",
      "pdf_url": "https://arxiv.org/pdf/2511.00379v1",
      "published_date": "2025-11-01 03:26:24 UTC",
      "updated_date": "2025-11-01 03:26:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:14:00.938348+00:00"
    },
    {
      "arxiv_id": "2511.00370v1",
      "title": "Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict",
      "title_zh": "æˆ‘ä»¬å¯ä»¥ä¿¡ä»»è°ï¼Ÿå¤šæ™ºèƒ½ä½“å†²çªä¸‹çš„èŒƒå›´æ„ŸçŸ¥è§†é¢‘æ—¶åˆ»æ£€ç´¢",
      "authors": [
        "Chaochen Wu",
        "Guan Luo",
        "Meiyun Zuo",
        "Zhitao Fan"
      ],
      "abstract": "Video moment retrieval uses a text query to locate a moment from a given untrimmed video reference. Locating corresponding video moments with text queries helps people interact with videos efficiently. Current solutions for this task have not considered conflict within location results from different models, so various models cannot integrate correctly to produce better results. This study introduces a reinforcement learning-based video moment retrieval model that can scan the whole video once to find the moment's boundary while producing its locational evidence. Moreover, we proposed a multi-agent system framework that can use evidential learning to resolve conflicts between agents' localization output. As a side product of observing and dealing with conflicts between agents, we can decide whether a query has no corresponding moment in a video (out-of-scope) without additional training, which is suitable for real-world applications. Extensive experiments on benchmark datasets show the effectiveness of our proposed methods compared with state-of-the-art approaches. Furthermore, the results of our study reveal that modeling competition and conflict of the multi-agent system is an effective way to improve RL performance in moment retrieval and show the new role of evidential learning in the multi-agent framework.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘æ—¶åˆ»æ£€ç´¢ï¼ˆVideo Moment Retrievalï¼‰ä¸­ä¸åŒæ¨¡å‹å®šä½ç»“æœå†²çªä¸”éš¾ä»¥æœ‰æ•ˆæ•´åˆçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ„ŸçŸ¥èŒƒå›´çš„æ–°å‹è§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡å•æ¬¡æ‰«æå®Œæ•´è§†é¢‘ç¡®å®šæ—¶åˆ»è¾¹ç•Œå¹¶äº§ç”Ÿå®šä½ä¾æ®ã€‚ä¸ºäº†è§£å†³ä¸åŒæ¨¡å‹é—´çš„è¾“å‡ºå†²çªï¼Œè®ºæ–‡æå‡ºäº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMulti-Agent Systemï¼‰æ¡†æ¶ï¼Œå¹¶åˆ›æ–°æ€§åœ°åˆ©ç”¨è¯æ®å­¦ä¹ ï¼ˆEvidential Learningï¼‰æ¥åè°ƒæ™ºèƒ½ä½“é—´çš„å†³ç­–ã€‚ä½œä¸ºå¤„ç†å†²çªçš„å‰¯äº§å“ï¼Œè¯¥ç³»ç»Ÿæ— éœ€é¢å¤–è®­ç»ƒå³å¯è¯†åˆ«æŸ¥è¯¢åœ¨è§†é¢‘ä¸­ä¸å­˜åœ¨å¯¹åº”æ—¶åˆ»ï¼ˆOut-of-scopeï¼‰çš„æƒ…å†µï¼Œéå¸¸é€‚åˆå®é™…åº”ç”¨åœºæ™¯ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„å…ˆè¿›ï¼ˆState-of-the-Artï¼‰æŠ€æœ¯ã€‚ç ”ç©¶ç»“æœè¿›ä¸€æ­¥æ­ç¤ºäº†å»ºæ¨¡å¤šæ™ºèƒ½ä½“é—´çš„ç«äº‰ä¸å†²çªæ˜¯æå‡å¼ºåŒ–å­¦ä¹ åœ¨æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°çš„æœ‰æ•ˆæ‰‹æ®µï¼Œå¹¶è¯æ˜äº†è¯æ®å­¦ä¹ åœ¨å¤šæ™ºèƒ½ä½“åä½œä¸­çš„é‡è¦ä½œç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00370v1",
      "published_date": "2025-11-01 02:42:36 UTC",
      "updated_date": "2025-11-01 02:42:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:14:08.657579+00:00"
    },
    {
      "arxiv_id": "2511.00369v1",
      "title": "Balancing Interpretability and Performance in Motor Imagery EEG Classification: A Comparative Study of ANFIS-FBCSP-PSO and EEGNet",
      "title_zh": "è¿åŠ¨æƒ³è±¡è„‘ç”µåˆ†ç±»ä¸­å¯è§£é‡Šæ€§ä¸æ€§èƒ½çš„å¹³è¡¡ï¼šANFIS-FBCSP-PSO ä¸ EEGNet çš„å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Farjana Aktar",
        "Mohd Ruhul Ameen",
        "Akif Islam",
        "Md Ekramul Hamid"
      ],
      "abstract": "Achieving both accurate and interpretable classification of motor imagery EEG remains a key challenge in brain computer interface (BCI) research. This paper compares a transparent fuzzy reasoning approach (ANFIS-FBCSP-PSO) with a deep learning benchmark (EEGNet) using the BCI Competition IV-2a dataset. The ANFIS pipeline combines filter bank common spatial pattern feature extraction with fuzzy IF-THEN rules optimized via particle swarm optimization, while EEGNet learns hierarchical spatial temporal representations directly from raw EEG data. In within-subject experiments, the fuzzy neural model performed better (68.58 percent +/- 13.76 percent accuracy, kappa = 58.04 percent +/- 18.43), while in cross-subject (LOSO) tests, the deep model exhibited stronger generalization (68.20 percent +/- 12.13 percent accuracy, kappa = 57.33 percent +/- 16.22). The study provides practical guidance for selecting MI-BCI systems according to design goals: interpretability or robustness across users. Future investigations into transformer based and hybrid neuro symbolic frameworks are expected to advance transparent EEG decoding.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è„‘æœºæ¥å£(BCI)é¢†åŸŸä¸­è¿åŠ¨æƒ³è±¡(Motor Imagery) EEG ä¿¡å·åˆ†ç±»çš„å‡†ç¡®æ€§ä¸å¯è§£é‡Šæ€§å¹³è¡¡é—®é¢˜ï¼Œå¯¹æ¯”äº†åŸºäºæ¨¡ç³Šæ¨ç†çš„ ANFIS-FBCSP-PSO æ¨¡å‹ä¸æ·±åº¦å­¦ä¹ åŸºå‡†æ¨¡å‹ EEGNetã€‚ANFIS-FBCSP-PSO æ¡†æ¶ç»“åˆäº†æ»¤æ³¢å™¨ç»„å…±ç©ºé—´æ¨¡å¼(FBCSP)ç‰¹å¾æå–ä¸é€šè¿‡ç²’å­ç¾¤ä¼˜åŒ–(PSO)æ”¹è¿›çš„æ¨¡ç³Š IF-THEN è§„åˆ™ï¼Œæ—¨åœ¨æä¾›é€æ˜çš„æ¨ç†æœºåˆ¶ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒEEGNet ç›´æ¥ä»åŸå§‹ EEG æ•°æ®ä¸­å­¦ä¹ å±‚æ¬¡åŒ–çš„ç©ºé—´ä¸æ—¶é—´è¡¨å¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å—è¯•è€…å†…å®éªŒä¸­ï¼Œæ¨¡ç³Šç¥ç»æ¨¡å‹è¡¨ç°æ›´ä¼˜ï¼Œå‡†ç¡®ç‡è¾¾åˆ° 68.58%ï¼›è€Œåœ¨è·¨å—è¯•è€…(LOSO)æµ‹è¯•ä¸­ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹åˆ™å±•ç¤ºå‡ºæ›´å¼ºçš„æ³›åŒ–é²æ£’æ€§ï¼Œå‡†ç¡®ç‡ä¸º 68.20%ã€‚è¯¥ç ”ç©¶ä¸ºæ ¹æ®è®¾è®¡ç›®æ ‡ï¼ˆå€¾å‘å¯è§£é‡Šæ€§æˆ–è·¨ç”¨æˆ·é²æ£’æ€§ï¼‰é€‰æ‹©åˆé€‚çš„ MI-BCI ç³»ç»Ÿæä¾›äº†å®é™…æŒ‡å¯¼ã€‚æœªæ¥ç ”ç©¶å°†è¿›ä¸€æ­¥æ¢ç´¢åŸºäº Transformer å’Œæ··åˆç¥ç»ç¬¦å·(Neuro-symbolic)æ¡†æ¶ä»¥æ¨è¿›é€æ˜çš„ EEG è§£ç æŠ€æœ¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 3 figures, 8 tables, Submitted to ICECTE 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.00369v1",
      "published_date": "2025-11-01 02:42:01 UTC",
      "updated_date": "2025-11-01 02:42:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:14:09.554864+00:00"
    },
    {
      "arxiv_id": "2511.10652v1",
      "title": "Cognitively-Inspired Episodic Memory Architectures for Accurate and Efficient Character AI",
      "title_zh": "é¢å‘ç²¾å‡†é«˜æ•ˆè§’è‰² AI çš„è®¤çŸ¥å¯å‘å¼æƒ…æ™¯è®°å¿†æ¶æ„",
      "authors": [
        "Rafael Arias Gonzalez",
        "Steve DiPaola"
      ],
      "abstract": "Large language models show promise for embodying historical characters in dialogue systems, but existing approaches face a critical trade-off: simple retrieval-augmented generation produces shallow responses, while multi-stage reflection achieves depth at prohibitive latency. We present an architecture that resolves this tension through offline data augmentation and efficient parallel retrieval from structured episodic memory. Our system transforms biographical data into 1,774 enriched first-person memories with affective-semantic metadata, then employs two-stage retrieval achieving 0.52s prompt generation. Evaluation using LLM-as-judge and RAGAs metrics shows our approach achieves parity with traditional RAG on GPT-4 while significantly outperforming it on smaller models (GPT-3.5, GPT-3), suggesting particular value for resource-constrained deployments. Beyond dialogue, the structured memory enables novel visualization tools: spatiotemporal heatmaps, emotional trajectory analysis, and interactive path tracking, positioning the system as both a dialogue interface and research tool for biographical analysis. We use Van Gogh as a test case, but the architecture is generalizable to any historical figure with substantial textual records, offering a practical framework for educational, museum, and research applications requiring both accuracy and efficiency",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨¡æ‹Ÿå†å²äººç‰©å¯¹è¯æ—¶é¢ä¸´çš„æµ…å±‚å›å¤ä¸é«˜å»¶è¿Ÿä¹‹é—´çš„æƒè¡¡ï¼Œæå‡ºäº†ä¸€ç§å—è®¤çŸ¥å¯å‘çš„æ’åºè®°å¿†(Episodic Memory)æ¶æ„ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ç¦»çº¿æ•°æ®å¢å¼ºï¼Œå°†ä¼ è®°æ•°æ®è½¬åŒ–ä¸ºåŒ…å«æƒ…æ„Ÿè¯­ä¹‰å…ƒæ•°æ®çš„å¢å¼ºå‹ç¬¬ä¸€äººç§°è®°å¿†ï¼Œå¹¶åˆ©ç”¨ç»“æ„åŒ–å­˜å‚¨å®ç°é«˜æ•ˆçš„å¹¶è¡Œæ£€ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¶æ„èƒ½å®ç°0.52ç§’çš„å¿«é€Ÿå“åº”ï¼Œåœ¨GPT-4ä¸Šä¸ä¼ ç»ŸRAGæ€§èƒ½æŒå¹³ï¼Œä½†åœ¨GPT-3.5å’ŒGPT-3ç­‰å°å‹æ¨¡å‹ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œç»“æ„åŒ–è®°å¿†æ”¯æŒçš„æ—¶ç©ºçƒ­åŠ›å›¾å’Œæƒ…æ„Ÿè½¨è¿¹åˆ†æç­‰å·¥å…·ï¼Œä½¿å…¶èƒ½å¤ŸåŒæ—¶ä½œä¸ºå¯¹è¯æ¥å£ä¸ä¼ è®°åˆ†æçš„ç§‘ç ”å·¥å…·ã€‚ä»¥æ¢µé«˜(Van Gogh)ä¸ºæµ‹è¯•æ¡ˆä¾‹ï¼Œè¯¥é€šç”¨æ¡†æ¶ä¸ºæ•™è‚²ã€åšç‰©é¦†å’Œç§‘ç ”ç­‰å¯¹å‡†ç¡®æ€§å’Œæ•ˆç‡æœ‰åŒé‡éœ€æ±‚çš„åœºæ™¯æä¾›äº†å¯è¡Œçš„Character AIæŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "25 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.10652v1",
      "published_date": "2025-11-01 02:26:16 UTC",
      "updated_date": "2025-11-01 02:26:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:14:19.724546+00:00"
    },
    {
      "arxiv_id": "2511.00362v1",
      "title": "Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery",
      "title_zh": "Oitijjo-3Dï¼šåŸºäºè¡—æ™¯å½±åƒçš„æ–‡åŒ–é—äº§å¿«é€Ÿ 3D é‡å»ºç”Ÿæˆå¼ AI æ¡†æ¶",
      "authors": [
        "Momen Khandoker Ope",
        "Akif Islam",
        "Mohd Ruhul Ameen",
        "Abu Saleh Musa Miah",
        "Md Rashedul Islam",
        "Jungpil Shin"
      ],
      "abstract": "Cultural heritage restoration in Bangladesh faces a dual challenge of limited resources and scarce technical expertise. Traditional 3D digitization methods, such as photogrammetry or LiDAR scanning, require expensive hardware, expert operators, and extensive on-site access, which are often infeasible in developing contexts. As a result, many of Bangladesh's architectural treasures, from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a cost-free generative AI framework that democratizes 3D cultural preservation. By using publicly available Google Street View imagery, Oitijjo-3D reconstructs faithful 3D models of heritage structures through a two-stage pipeline - multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture synthesis, and neural image-to-3D generation through Hexagen for geometry recovery. The system produces photorealistic, metrically coherent reconstructions in seconds, achieving significant speedups compared to conventional Structure-from-Motion pipelines, without requiring any specialized hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil, Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both visual and structural fidelity while drastically lowering economic and technical barriers. By turning open imagery into digital heritage, this work reframes preservation as a community-driven, AI-assisted act of cultural continuity for resource-limited nations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èµ„æºå—é™ç¯å¢ƒä¸‹æ–‡åŒ–é—äº§ä¿®å¤çš„éš¾é¢˜ï¼Œæå‡ºäº† Oitijjo-3D è¿™ä¸€åŸºäºç”Ÿæˆå¼ AI çš„å…è´¹æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨å…¬å¼€çš„ Google Street View å›¾åƒå®ç° 3D æ–‡åŒ–ä¿æŠ¤çš„æ°‘ä¸»åŒ–ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨äº†ä¸¤é˜¶æ®µå¤„ç†æµæ°´çº¿ï¼Œé¦–å…ˆåˆ©ç”¨ Gemini 2.5 Flash Image è¿›è¡Œå¤šæ¨¡æ€è§†è§‰æ¨ç†ä»¥å®ç°ç»“æ„ä¸çº¹ç†åˆæˆï¼Œéšåé€šè¿‡ Hexagen è¿›è¡Œç¥ç»å›¾åƒåˆ° 3D ç”Ÿæˆä»¥å®Œæˆå‡ ä½•å½¢çŠ¶æ¢å¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOitijjo-3D èƒ½å¤Ÿåœ¨æ•°ç§’å†…ç”Ÿæˆå…·æœ‰ç…§ç‰‡çº§çœŸå®æ„Ÿä¸”åº¦é‡ç›¸å¹²çš„é‡å»ºæ¨¡å‹ï¼Œå…¶é€Ÿåº¦æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ Structure-from-Motion æµæ°´çº¿ï¼Œä¸”æ— éœ€ä¸“é—¨ç¡¬ä»¶æˆ–ä¸“å®¶æŒ‡å¯¼ã€‚åœ¨ Ahsan Manzil å’Œ Choto Sona Mosque ç­‰åœ°æ ‡ä¸Šçš„æµ‹è¯•è¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤§å¹…é™ä½ç»æµå’ŒæŠ€æœ¯é—¨æ§›çš„åŒæ—¶ï¼Œæœ‰æ•ˆä¿æŒäº†æé«˜çš„è§†è§‰ä¸ç»“æ„ä¿çœŸåº¦ã€‚è¿™ä¸€å·¥ä½œä¸ºèµ„æºæœ‰é™å›½å®¶æä¾›äº†ä¸€ç§ç”± AI è¾…åŠ©ä¸”æå…·æˆæœ¬æ•ˆç›Šçš„æ•°å­—åŒ–é—äº§ä¿å­˜æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "6 Pages, 4 figures, 2 Tables, Submitted to ICECTE 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.00362v1",
      "published_date": "2025-11-01 02:09:26 UTC",
      "updated_date": "2025-11-01 02:09:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:14:22.132789+00:00"
    },
    {
      "arxiv_id": "2511.00361v1",
      "title": "MalDataGen: A Modular Framework for Synthetic Tabular Data Generation in Malware Detection",
      "title_zh": "MalDataGenï¼šé¢å‘æ¶æ„è½¯ä»¶æ£€æµ‹çš„åˆæˆè¡¨æ ¼æ•°æ®ç”Ÿæˆæ¨¡å—åŒ–æ¡†æ¶",
      "authors": [
        "Kayua Oleques Paim",
        "Angelo Gaspar Diniz Nogueira",
        "Diego Kreutz",
        "Weverton Cordeiro",
        "Rodrigo Brandao Mansilha"
      ],
      "abstract": "High-quality data scarcity hinders malware detection, limiting ML performance. We introduce MalDataGen, an open-source modular framework for generating high-fidelity synthetic tabular data using modular deep learning models (e.g., WGAN-GP, VQ-VAE). Evaluated via dual validation (TR-TS/TS-TR), seven classifiers, and utility metrics, MalDataGen outperforms benchmarks like SDV while preserving data utility. Its flexible design enables seamless integration into detection pipelines, offering a practical solution for cybersecurity applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¶æ„è½¯ä»¶æ£€æµ‹(Malware Detection)é¢†åŸŸä¸­é«˜è´¨é‡æ•°æ®ç¨€ç¼ºå¯¼è‡´æœºå™¨å­¦ä¹ (ML)æ€§èƒ½å—é™çš„é—®é¢˜ï¼Œæå‡ºäº† MalDataGen æ¡†æ¶ã€‚MalDataGen æ˜¯ä¸€ä¸ªå¼€æºçš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨ WGAN-GP å’Œ VQ-VAE ç­‰æ¨¡å—åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹ç”Ÿæˆé«˜ä¿çœŸåº¦çš„åˆæˆè¡¨æ ¼æ•°æ®(Synthetic Tabular Data)ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡åŒé‡éªŒè¯(TR-TS/TS-TR)ã€ä¸ƒç§åˆ†ç±»å™¨ä»¥åŠæ•ˆç”¨æŒ‡æ ‡å¯¹è¯¥æ¡†æ¶è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMalDataGen åœ¨ä¿æŒæ•°æ®æ•ˆç”¨(Data Utility)çš„åŒæ—¶ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äº SDV ç­‰ç°æœ‰åŸºå‡†æ¨¡å‹ã€‚è¯¥æ¡†æ¶å‡­å€Ÿå…¶çµæ´»çš„è®¾è®¡ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ç°æœ‰çš„æ£€æµ‹æµæ°´çº¿ä¸­ï¼Œä¸ºç½‘ç»œå®‰å…¨(Cybersecurity)åº”ç”¨æä¾›äº†ä¸€ç§åˆ‡å®å¯è¡Œçš„åˆæˆæ•°æ®è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages, 6 figures, 2 tables. Published at the Brazilian Symposium on Cybersecurity (SBSeg 2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.00361v1",
      "published_date": "2025-11-01 02:08:58 UTC",
      "updated_date": "2025-11-01 02:08:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:15:07.236040+00:00"
    },
    {
      "arxiv_id": "2511.00360v1",
      "title": "Mind the Gap: Missing Cyber Threat Coverage in NIDS Datasets for the Energy Sector",
      "title_zh": "å…³æ³¨å·®è·ï¼šèƒ½æºé¢†åŸŸ NIDS æ•°æ®é›†ä¸­ç½‘ç»œå¨èƒè¦†ç›–çš„ç¼ºå¤±åˆ†æ",
      "authors": [
        "Adrita Rahman Tory",
        "Khondokar Fida Hasan",
        "Md Saifur Rahman",
        "Nickolaos Koroniotis",
        "Mohammad Ali Moni"
      ],
      "abstract": "Network Intrusion Detection Systems (NIDS) developed using publicly available datasets predominantly focus on enterprise environments, raising concerns about their effectiveness for converged Information Technology (IT) and Operational Technology (OT) in energy infrastructures. This study evaluates the representativeness of five widely used datasets: CIC-IDS2017, SWaT, WADI, Sherlock, and CIC-Modbus2023 against network-detectable MITRE ATT&CK techniques extracted from documented energy sector incidents. Using a structured five-step analytical approach, this article successfully developed and performed a gap analysis that identified 94 network observable techniques from an initial pool of 274 ATT&CK techniques. Sherlock dataset exhibited the highest mean coverage (0.56), followed closely by CIC-IDS2017 (0.55), while SWaT and WADI recorded the lowest scores (0.38). Combining CIC-IDS2017, Sherlock, and CIC-Modbus2023 achieved an aggregate coverage of 92%, highlighting their complementary strengths. The analysis identifies critical gaps, particularly in lateral movement and industrial protocol manipulation, providing a clear pathway for dataset enhancement and more robust NIDS evaluation in hybrid IT/OT energy environments.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†äº”ä¸ªå¹¿æ³›ä½¿ç”¨çš„NIDSæ•°æ®é›†ï¼ˆCIC-IDS2017, SWaT, WADI, Sherlock, CIC-Modbus2023ï¼‰å¯¹èƒ½æºè¡Œä¸šITä¸OTèåˆç¯å¢ƒä¸‹ç½‘ç»œå¨èƒçš„è¦†ç›–èƒ½åŠ›ã€‚é€šè¿‡ç»“æ„åŒ–çš„äº”æ­¥åˆ†ææ³•ï¼Œç ”ç©¶äººå‘˜ä»èƒ½æºè¡Œä¸šè®°å½•çš„äº‹æ•…ä¸­æå–å¹¶è¯†åˆ«å‡º94é¡¹ç½‘ç»œå¯è§‚æµ‹çš„MITRE ATT&CKæŠ€æœ¯è¿›è¡Œå·®å¼‚åˆ†æã€‚ç»“æœæ˜¾ç¤ºï¼ŒSherlockæ•°æ®é›†çš„å¹³å‡è¦†ç›–ç‡æœ€é«˜ï¼ˆ0.56ï¼‰ï¼Œè€ŒSWaTå’ŒWADIçš„å¾—åˆ†æœ€ä½ï¼ˆ0.38ï¼‰ã€‚ç ”ç©¶å‘ç°ï¼Œç»“åˆä½¿ç”¨CIC-IDS2017, Sherlockå’ŒCIC-Modbus2023å¯å®ç°92%çš„ç´¯è®¡è¦†ç›–ç‡ï¼Œå±•ç°äº†æ•°æ®é›†ä¹‹é—´çš„å¼ºäº’è¡¥æ€§ã€‚åˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†ç°æœ‰æ•°æ®é›†åœ¨ä¾§å‘ç§»åŠ¨(lateral movement)å’Œå·¥ä¸šåè®®ç¯¡æ”¹(industrial protocol manipulation)æ–¹é¢çš„å…³é”®è¦†ç›–å·®è·ã€‚è¯¥ç ”ç©¶ä¸ºå¢å¼ºæ··åˆIT/OTèƒ½æºç¯å¢ƒä¸‹çš„æ•°æ®é›†è´¨é‡åŠæ„å»ºæ›´é²æ£’çš„NIDSè¯„ä¼°ä½“ç³»æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "13 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.00360v1",
      "published_date": "2025-11-01 02:02:25 UTC",
      "updated_date": "2025-11-01 02:02:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:15:09.748048+00:00"
    },
    {
      "arxiv_id": "2511.00359v1",
      "title": "Toward Unifying Group Fairness Evaluation from a Sparsity Perspective",
      "title_zh": "è¿ˆå‘ç¨€ç–æ€§è§†è§’ä¸‹çš„ç¾¤ä½“å…¬å¹³æ€§è¯„ä¼°ç»Ÿä¸€",
      "authors": [
        "Zhecheng Sheng",
        "Jiawei Zhang",
        "Enmao Diao"
      ],
      "abstract": "Ensuring algorithmic fairness remains a significant challenge in machine learning, particularly as models are increasingly applied across diverse domains. While numerous fairness criteria exist, they often lack generalizability across different machine learning problems. This paper examines the connections and differences among various sparsity measures in promoting fairness and proposes a unified sparsity-based framework for evaluating algorithmic fairness. The framework aligns with existing fairness criteria and demonstrates broad applicability to a wide range of machine learning tasks. We demonstrate the effectiveness of the proposed framework as an evaluation metric through extensive experiments on a variety of datasets and bias mitigation methods. This work provides a novel perspective to algorithmic fairness by framing it through the lens of sparsity and social equity, offering potential for broader impact on fairness research and applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ ç®—æ³•å…¬å¹³æ€§å‡†åˆ™åœ¨ä¸åŒé¢†åŸŸç¼ºä¹æ³›åŒ–æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„åŸºäºç¨€ç–æ€§(Sparsity)çš„æ¡†æ¶ç”¨äºè¯„ä¼°ç®—æ³•å…¬å¹³æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†æä¸åŒç¨€ç–æ€§åº¦é‡åœ¨ä¿ƒè¿›å…¬å¹³æ€§æ–¹é¢çš„è”ç³»ä¸åŒºåˆ«ï¼Œå®ç°äº†ä¸ç°æœ‰å…¬å¹³æ€§å‡†åˆ™(Fairness Criteria)çš„å¯¹é½ï¼Œå¹¶å±•ç°å‡ºåœ¨å¤šç§æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡åœ¨å¤šä¸ªæ•°æ®é›†å’Œåè§ç¼“è§£æ–¹æ³•(Bias Mitigation Methods)ä¸Šçš„å¤§é‡å®éªŒï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶ä½œä¸ºè¯„ä»·æŒ‡æ ‡çš„æœ‰æ•ˆæ€§ã€‚è¿™é¡¹å·¥ä½œä»ç¨€ç–æ€§å’Œç¤¾ä¼šå…¬å¹³çš„å…¨æ–°è§†è§’å¯¹ç®—æ³•å…¬å¹³æ€§è¿›è¡Œäº†é‡æ„ï¼Œä¸ºç»Ÿä¸€å…¬å¹³æ€§è¯„ä¼°ä½“ç³»åŠæ¨åŠ¨å…¶åœ¨å¹¿æ³›ç§‘ç ”ä¸å®é™…åº”ç”¨ä¸­çš„è½åœ°æä¾›äº†ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "30 pages, 14 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.00359v1",
      "published_date": "2025-11-01 02:02:11 UTC",
      "updated_date": "2025-11-01 02:02:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:15:12.909693+00:00"
    },
    {
      "arxiv_id": "2511.01912v2",
      "title": "EvoMem: Improving Multi-Agent Planning with Dual-Evolving Memory",
      "title_zh": "EvoMemï¼šåŸºäºåŒé‡æ¼”åŒ–è®°å¿†æå‡å¤šæ™ºèƒ½ä½“è§„åˆ’",
      "authors": [
        "Wenzhe Fan",
        "Ning Yan",
        "Masood Mortazavi"
      ],
      "abstract": "Planning has been a cornerstone of artificial intelligence for solving complex problems, and recent progress in LLM-based multi-agent frameworks have begun to extend this capability. However, the role of human-like memory within these frameworks remains largely unexplored. Understanding how agents coordinate through memory is critical for natural language planning, where iterative reasoning, constraint tracking, and error correction drive the success. Inspired by working memory model in cognitive psychology, we present EvoMem, a multi-agent framework built on a dual-evolving memory mechanism. The framework consists of three agents (Constraint Extractor, Verifier, and Actor) and two memory modules: Constraint Memory (CMem), which evolves across queries by storing task-specific rules and constraints while remains fixed within a query, and Query-feedback Memory (QMem), which evolves within a query by accumulating feedback across iterations for solution refinement. Both memory modules are reset at the end of each query session. Evaluations on trip planning, meeting planning, and calendar scheduling show consistent performance improvements, highlighting the effectiveness of EvoMem. This success underscores the importance of memory in enhancing multi-agent planning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EvoMemï¼Œä¸€ç§åŸºäºåŒé‡è¿›åŒ–è®°å¿†(dual-evolving memory)æœºåˆ¶çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–LLMåœ¨è‡ªç„¶è¯­è¨€è§„åˆ’ä¸­çš„ååŒèƒ½åŠ›ã€‚å—è®¤çŸ¥å¿ƒç†å­¦ä¸­å·¥ä½œè®°å¿†æ¨¡å‹çš„å¯å‘ï¼ŒEvoMemç”±Constraint Extractorã€Verifierå’ŒActorä¸‰ä¸ªæ™ºèƒ½ä½“ç»„æˆï¼Œå¹¶ç»“åˆäº†ä¸¤ä¸ªæ ¸å¿ƒè®°å¿†æ¨¡å—ã€‚å…¶ä¸­Constraint Memory (CMem) è´Ÿè´£åœ¨ä¸åŒæŸ¥è¯¢é—´å­˜å‚¨ä»»åŠ¡ç‰¹å®šçš„è§„åˆ™ä¸çº¦æŸï¼Œè€ŒQuery-feedback Memory (QMem) åˆ™åœ¨å•æ¬¡æŸ¥è¯¢å†…éƒ¨é€šè¿‡ç§¯ç´¯å¤šæ¬¡è¿­ä»£çš„åé¦ˆæ¥ç»†åŒ–è§£å†³æ–¹æ¡ˆã€‚å®éªŒåœ¨è¡Œç¨‹è§„åˆ’ã€ä¼šè®®å®‰æ’å’Œæ—¥ç¨‹è°ƒåº¦ç­‰ä»»åŠ¡ä¸ŠéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†è®°å¿†æœºåˆ¶åœ¨å¤„ç†å¤æ‚æ¨ç†ã€çº¦æŸè·Ÿè¸ªå’Œè¯¯å·®æ ¡æ­£ä¸­çš„å…³é”®ä½œç”¨ã€‚è¿™ä¸€æˆæœä¸ºå¼€å‘å…·å¤‡äººç±»å¼è®°å¿†èƒ½åŠ›çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01912v2",
      "published_date": "2025-11-01 01:38:07 UTC",
      "updated_date": "2025-12-08 18:05:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 1,
      "last_update": "2026-01-25T07:16:24.007172+00:00"
    },
    {
      "arxiv_id": "2511.00352v1",
      "title": "Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach",
      "title_zh": "åŸºäºæ‰©æ•£å›å¼¹é‡æ„çš„ AI ç”Ÿæˆå›¾åƒæ£€æµ‹ï¼šä¸€ç§å–è¯æ–¹æ³•",
      "authors": [
        "Mohd Ruhul Ameen",
        "Akif Islam"
      ],
      "abstract": "The rapid rise of generative diffusion models has made distinguishing authentic visual content from synthetic imagery increasingly challenging. Traditional deepfake detection methods, which rely on frequency or pixel-level artifacts, fail against modern text-to-image systems such as Stable Diffusion and DALL-E that produce photorealistic and artifact-free results. This paper introduces a diffusion-based forensic framework that leverages multi-strength image reconstruction dynamics, termed diffusion snap-back, to identify AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and PSNR) evolve across varying noise strengths, we extract interpretable manifold-based features that differentiate real and synthetic images. Evaluated on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under cross-validation and remains robust to common distortions such as compression and noise. Despite using limited data and a single diffusion backbone (Stable Diffusion v1.5), the proposed method demonstrates strong generalization and interpretability, offering a foundation for scalable, model-agnostic synthetic media forensics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹(Diffusion Models)ç”Ÿæˆçš„åˆæˆå›¾åƒéš¾ä»¥è¢«ä¼ ç»Ÿå–è¯æ–¹æ³•è¯†åˆ«çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºæ‰©æ•£å›å¼¹(Diffusion Snap-Back)çš„å–è¯æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤šå¼ºåº¦å›¾åƒé‡å»ºåŠ¨åŠ›å­¦ï¼Œé€šè¿‡åˆ†æé‡å»ºæŒ‡æ ‡ï¼ˆåŒ…æ‹¬ LPIPSã€SSIM å’Œ PSNRï¼‰åœ¨ä¸åŒå™ªå£°å¼ºåº¦ä¸‹çš„æ¼”å˜è¿‡ç¨‹ï¼Œæå–å‡ºç”¨äºåŒºåˆ†çœŸå®ä¸åˆæˆå›¾åƒçš„å¯è§£é‡Šæµå½¢ç‰¹å¾(Manifold-based Features)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ 4,000 å¼ å›¾åƒçš„å¹³è¡¡æ•°æ®é›†ä¸Šè¾¾åˆ°äº† 0.993 AUROCï¼Œå¹¶å¯¹å›¾åƒå‹ç¼©å’Œå™ªå£°å¹²æ‰°ç­‰å¸¸è§ç•¸å˜è¡¨ç°å‡ºè¾ƒå¼ºçš„é²æ£’æ€§ã€‚å°½ç®¡ä»…åŸºäº Stable Diffusion v1.5 éª¨å¹²ç½‘ç»œå¼€å‘ï¼Œè¯¥æ–¹æ³•ä»å±•ç°äº†ä¼˜å¼‚çš„æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ï¼Œä¸ºå¼€å‘å¯æ‰©å±•ä¸”æ¨¡å‹æ— å…³çš„åˆæˆåª’ä½“å–è¯æŠ€æœ¯å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 8 figures, 4 Tables, submitted to ICECTE 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.00352v1",
      "published_date": "2025-11-01 01:35:54 UTC",
      "updated_date": "2025-11-01 01:35:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:15:18.843305+00:00"
    },
    {
      "arxiv_id": "2511.10651v1",
      "title": "Data Analysis and Performance Evaluation of Simulation Deduction Based on LLMs",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä»¿çœŸæ¨æ¼”æ•°æ®åˆ†æä¸æ•ˆèƒ½è¯„ä¼°",
      "authors": [
        "Shansi Zhang",
        "Min Li"
      ],
      "abstract": "Data analysis and performance evaluation of simulation deduction plays a pivotal role in modern warfare, which enables military personnel to gain invaluable insights into the potential effectiveness of different strategies, tactics, and operational plans. Traditional manual analysis approach is time-consuming and limited by human errors. To enhance efficiency and accuracy, large language models (LLMs) with strong analytical and inferencing capabilities can be employed. However, high-quality analysis reports with well-structured formatting cannot be obtained through a single instruction input to the LLM. To tackle this issue, we propose a method that first decomposes the complex task into several sub-tasks and designs effective system prompts and user prompts for each sub-task. Multi-round interactions with the LLM incorporating self-check and reflection are then conducted to enable structured data extraction as well as multi-step analysis and evaluation. Furthermore, custom tools are defined and invoked to generate figures and compute metrics. We also design multiple report templates, each tailored to a specific application and input data type, ensuring their adaptability across a variety of scenarios. Extensive evaluation results demonstrate that the reports generated by our method exhibit higher quality, therefore obtaining higher scores than the baseline method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£æˆ˜äº‰ä»¿çœŸæ¨æ¼”(Simulation Deduction)ä¸­ä¼ ç»Ÿäººå·¥æ•°æ®åˆ†ææ•ˆç‡ä½ã€æ˜“å‡ºé”™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„è‡ªåŠ¨åŒ–åˆ†æä¸è¯„ä¼°æ–¹æ³•ã€‚ä¸ºäº†è§£å†³å•ä¸€æŒ‡ä»¤éš¾ä»¥ç”Ÿæˆé«˜è´¨é‡ç»“æ„åŒ–æŠ¥å‘Šçš„æŒ‘æˆ˜ï¼Œè¯¥æ–¹æ³•å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼Œå¹¶ä¸ºæ¯ä¸ªå­ä»»åŠ¡è®¾è®¡äº†ä¸“é—¨çš„ç³»ç»Ÿæç¤º(System Prompts)å’Œç”¨æˆ·æç¤ºã€‚é€šè¿‡å¼•å…¥åŒ…å«è‡ªæˆ‘æ£€æŸ¥(Self-check)ä¸åæ€(Reflection)çš„å¤šè½®äº¤äº’æœºåˆ¶ï¼Œç³»ç»Ÿå®ç°äº†ç²¾å‡†çš„ç»“æ„åŒ–æ•°æ®æå–åŠå¤šæ­¥é€»è¾‘åˆ†æã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ•´åˆäº†è‡ªå®šä¹‰å·¥å…·ç”¨äºç”Ÿæˆå¯è§†åŒ–å›¾è¡¨å’Œè®¡ç®—æ€§èƒ½æŒ‡æ ‡ï¼Œå¹¶è®¾è®¡äº†å¤šç§æŠ¥å‘Šæ¨¡æ¿ä»¥ç¡®ä¿åœ¨ä¸åŒåº”ç”¨åœºæ™¯ä¸‹çš„é€‚åº”æ€§ã€‚å¤§è§„æ¨¡è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„æŠ¥å‘Šè´¨é‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œä¸ºå†›äº‹ç­–ç•¥å’Œä½œæˆ˜è®¡åˆ’çš„æœ‰æ•ˆæ€§åˆ†ææä¾›äº†æ›´é«˜æ•ˆä¸”å‡†ç¡®çš„æ‰‹æ®µã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10651v1",
      "published_date": "2025-11-01 01:32:33 UTC",
      "updated_date": "2025-11-01 01:32:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:15:29.063304+00:00"
    },
    {
      "arxiv_id": "2511.00346v1",
      "title": "Exploiting Latent Space Discontinuities for Building Universal LLM Jailbreaks and Data Extraction Attacks",
      "title_zh": "åˆ©ç”¨æ½œç©ºé—´ä¸è¿ç»­æ€§æ„å»ºé€šç”¨å¤§è¯­è¨€æ¨¡å‹è¶Šç‹±ä¸æ•°æ®æå–æ”»å‡»",
      "authors": [
        "Kayua Oleques Paim",
        "Rodrigo Brandao Mansilha",
        "Diego Kreutz",
        "Muriel Figueredo Franco",
        "Weverton Cordeiro"
      ],
      "abstract": "The rapid proliferation of Large Language Models (LLMs) has raised significant concerns about their security against adversarial attacks. In this work, we propose a novel approach to crafting universal jailbreaks and data extraction attacks by exploiting latent space discontinuities, an architectural vulnerability related to the sparsity of training data. Unlike previous methods, our technique generalizes across various models and interfaces, proving highly effective in seven state-of-the-art LLMs and one image generation model. Initial results indicate that when these discontinuities are exploited, they can consistently and profoundly compromise model behavior, even in the presence of layered defenses. The findings suggest that this strategy has substantial potential as a systemic attack vector.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ„å»ºé€šç”¨ LLM Jailbreaks å’Œ Data Extraction æ”»å‡»çš„æ–°æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒåœ¨äºåˆ©ç”¨ Latent Space Discontinuitiesï¼ˆæ½œç©ºé—´ä¸è¿ç»­æ€§ï¼‰è¿™ä¸€ä¸è®­ç»ƒæ•°æ®ç¨€ç–æ€§ç›¸å…³çš„æ¶æ„æ¼æ´ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œè¯¥æŠ€æœ¯åœ¨ä¸åŒæ¨¡å‹å’Œæ¥å£é—´å±•ç°å‡ºæå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨ä¸ƒç§æœ€å…ˆè¿›çš„ LLMs å’Œä¸€ä¸ªå›¾åƒç”Ÿæˆæ¨¡å‹ä¸Šå‡éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼Œå³ä½¿åœ¨éƒ¨ç½²äº†å¤šå±‚é˜²å¾¡çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨æ½œç©ºé—´ä¸è¿ç»­æ€§ä¾ç„¶èƒ½æŒç»­ä¸”æ·±åˆ»åœ°ç ´åæ¨¡å‹è¡Œä¸ºã€‚è¿™äº›å‘ç°è¡¨æ˜è¯¥ç­–ç•¥å…·æœ‰ä½œä¸ºç³»ç»Ÿæ€§æ”»å‡»å‘é‡çš„å·¨å¤§æ½œåŠ›ï¼Œæ­ç¤ºäº†å½“å‰å¤§è¯­è¨€æ¨¡å‹åœ¨æ¶æ„å±‚é¢çš„å®‰å…¨è„†å¼±æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages, 5 figures, 4 tables, Published at the Brazilian Symposium on Cybersecurity (SBSeg 2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.00346v1",
      "published_date": "2025-11-01 01:19:12 UTC",
      "updated_date": "2025-11-01 01:19:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:15:36.037289+00:00"
    },
    {
      "arxiv_id": "2511.00342v1",
      "title": "MH-1M: A 1.34 Million-Sample Comprehensive Multi-Feature Android Malware Dataset for Machine Learning, Deep Learning, Large Language Models, and Threat Intelligence Research",
      "title_zh": "MH-1Mï¼šé¢å‘æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€å¤§è¯­è¨€æ¨¡å‹åŠå¨èƒæƒ…æŠ¥ç ”ç©¶çš„ 134 ä¸‡æ ·æœ¬ç»¼åˆå¤šç‰¹å¾å®‰å“æ¶æ„è½¯ä»¶æ•°æ®é›†",
      "authors": [
        "Hendrio Braganca",
        "Diego Kreutz",
        "Vanderson Rocha",
        "Joner Assolin",
        "and Eduardo Feitosa"
      ],
      "abstract": "We present MH-1M, one of the most comprehensive and up-to-date datasets for advanced Android malware research. The dataset comprises 1,340,515 applications, encompassing a wide range of features and extensive metadata. To ensure accurate malware classification, we employ the VirusTotal API, integrating multiple detection engines for comprehensive and reliable assessment. Our GitHub, Figshare, and Harvard Dataverse repositories provide open access to the processed dataset and its extensive supplementary metadata, totaling more than 400 GB of data and including the outputs of the feature extraction pipeline as well as the corresponding VirusTotal reports. Our findings underscore the MH-1M dataset's invaluable role in understanding the evolving landscape of malware.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† MH-1Mï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é«˜çº§ Android Malware ç ”ç©¶çš„æœ€æ–°ä¸”æœ€å…¨é¢çš„æ•°æ®é›†ï¼Œæ¶µç›–äº†è¶…è¿‡ 134 ä¸‡ä¸ªåº”ç”¨ç¨‹åºæ ·æœ¬åŠè¯¦å°½çš„å…ƒæ•°æ®ã€‚ä¸ºäº†ç¡®ä¿åˆ†ç±»çš„å‡†ç¡®æ€§ï¼Œç ”ç©¶é€šè¿‡ VirusTotal API æ•´åˆäº†å¤šç§æ£€æµ‹å¼•æ“ï¼Œæä¾›äº†å…¨é¢ä¸”å¯é çš„æ ·æœ¬è¯„ä¼°ã€‚è¯¥æ•°æ®é›†æ€»è§„æ¨¡è¶…è¿‡ 400 GBï¼ŒåŒ…å«äº†ç‰¹å¾æå–æµæ°´çº¿çš„è¾“å‡ºç»“æœä»¥åŠç›¸åº”çš„ VirusTotal æŠ¥å‘Šï¼Œå¹¶å·²é€šè¿‡ GitHubã€Figshare å’Œ Harvard Dataverse å¼€æ”¾è®¿é—®ã€‚MH-1M ä¸º Machine Learningã€Deep Learningã€Large Language Models ä»¥åŠ Threat Intelligence ç­‰é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ ¸å¿ƒèµ„æºæ”¯æŒã€‚è¿™ä¸€æˆæœå¯¹äºæ·±å…¥ç†è§£å’Œåº”å¯¹ä¸æ–­æ¼”å˜çš„æ¶æ„è½¯ä»¶å¨èƒæ ¼å±€å…·æœ‰é‡è¦çš„å­¦æœ¯ä¸åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "cs.CR",
      "comment": "17 pages, 7 figures, 13 tables, submitted to the Scientific Data journal published by Nature Research",
      "pdf_url": "https://arxiv.org/pdf/2511.00342v1",
      "published_date": "2025-11-01 00:54:08 UTC",
      "updated_date": "2025-11-01 00:54:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:15:29.400078+00:00"
    },
    {
      "arxiv_id": "2511.00340v2",
      "title": "Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities",
      "title_zh": "Better Call CLAUSEï¼šç”¨äºå®¡è®¡å¤§è¯­è¨€æ¨¡å‹æ³•å¾‹æ¨ç†èƒ½åŠ›çš„å·®å¼‚åŸºå‡†",
      "authors": [
        "Manan Roy Choudhury",
        "Adithya Chandramouli",
        "Mannan Anand",
        "Vivek Gupta"
      ],
      "abstract": "The rapid integration of large language models (LLMs) into high-stakes legal work has exposed a critical gap: no benchmark exists to systematically stress-test their reliability against the nuanced, adversarial, and often subtle flaws present in real-world contracts. To address this, we introduce CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an LLM's legal reasoning. We study the capabilities of LLMs to detect and reason about fine-grained discrepancies by producing over 7500 real-world perturbed contracts from foundational datasets like CUAD and ContractNLI. Our novel, persona-driven pipeline generates 10 distinct anomaly categories, which are then validated against official statutes using a Retrieval-Augmented Generation (RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs' ability to detect embedded legal flaws and explain their significance. Our analysis shows a key weakness: these models often miss subtle errors and struggle even more to justify them legally. Our work outlines a path to identify and correct such reasoning failures in legal AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†CLAUSEï¼Œè¿™æ˜¯é¦–ä¸ªæ—¨åœ¨ç³»ç»Ÿæ€§å‹åŠ›æµ‹è¯•å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æ³•å¾‹æ¨ç†å¯é æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œå¡«è¡¥äº†æ¨¡å‹åœ¨è¯†åˆ«çœŸå®åˆåŒç»†å¾®ç¼ºé™·æ–¹é¢çš„è¯„ä¼°ç©ºç™½ã€‚ç ”ç©¶å›¢é˜ŸåŸºäºCUADå’ŒContractNLIæ•°æ®é›†ç”Ÿæˆäº†è¶…è¿‡7500ä»½åŒ…å«æ‰°åŠ¨çš„åˆåŒï¼Œé€šè¿‡äººæ ¼é©±åŠ¨æµæ°´çº¿(Persona-driven pipeline)å®šä¹‰äº†10ç±»å¼‚å¸¸ï¼Œå¹¶åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç³»ç»Ÿç¡®ä¿äº†æ³•å¾‹ä¿çœŸåº¦ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œç›®å‰çš„é¢†å…ˆLLMsåœ¨æ£€æµ‹åµŒå…¥å¼æ³•å¾‹ç‘•ç–µæ–¹é¢è¡¨ç°æ¬ ä½³ï¼Œä¸”éš¾ä»¥å¯¹å…¶è¿›è¡Œå‡†ç¡®çš„æ³•å¾‹åˆç†è§£é‡Šã€‚è¯¥é¡¹å·¥ä½œæ­ç¤ºäº†æ³•å¾‹AIåœ¨æ¨ç†èƒ½åŠ›ä¸Šçš„æ ¸å¿ƒå¼±ç‚¹ï¼Œå¹¶ä¸ºæœªæ¥è¯†åˆ«ä¸çº æ­£æ­¤ç±»æ¨ç†å¤±æ•ˆæä¾›äº†é‡è¦è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "42 pages, 4 images",
      "pdf_url": "https://arxiv.org/pdf/2511.00340v2",
      "published_date": "2025-11-01 00:51:21 UTC",
      "updated_date": "2026-01-07 04:40:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:15:41.005894+00:00"
    },
    {
      "arxiv_id": "2511.00328v1",
      "title": "Towards Automated Petrography",
      "title_zh": "è¿ˆå‘è‡ªåŠ¨åŒ–å²©ç›¸å­¦",
      "authors": [
        "Isai Daniel ChacÃ³n",
        "Paola Ruiz Puentes",
        "Jillian Pearse",
        "Pablo ArbelÃ¡ez"
      ],
      "abstract": "Petrography is a branch of geology that analyzes the mineralogical composition of rocks from microscopical thin section samples. It is essential for understanding rock properties across geology, archaeology, engineering, mineral exploration, and the oil industry. However, petrography is a labor-intensive task requiring experts to conduct detailed visual examinations of thin section samples through optical polarization microscopes, thus hampering scalability and highlighting the need for automated techniques. To address this challenge, we introduce the Large-scale Imaging and Thin section Optical-polarization Set (LITHOS), the largest and most diverse publicly available experimental framework for automated petrography. LITHOS includes 211,604 high-resolution RGB patches of polarized light and 105,802 expert-annotated grains across 25 mineral categories. Each annotation consists of the mineral class, spatial coordinates, and expert-defined major and minor axes represented as intersecting vector paths, capturing grain geometry and orientation. We evaluate multiple deep learning techniques for mineral classification in LITHOS and propose a dual-encoder transformer architecture that integrates both polarization modalities as a strong baseline for future reference. Our method consistently outperforms single-polarization models, demonstrating the value of polarization synergy in mineral classification. We have made the LITHOS Benchmark publicly available, comprising our dataset, code, and pretrained models, to foster reproducibility and further research in automated petrographic analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å²©ç›¸å­¦(Petrography)ä¼ ç»Ÿäººå·¥é•œæ£€æ•ˆç‡ä½ã€éš¾ä»¥è§„æ¨¡åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†ç›®å‰è§„æ¨¡æœ€å¤§ã€å¤šæ ·æ€§æœ€å¼ºçš„è‡ªåŠ¨åŒ–å®éªŒæ¡†æ¶LITHOSã€‚è¯¥æ¡†æ¶åŒ…å«211,604ä¸ªé«˜åˆ†è¾¨ç‡åæŒ¯å…‰RGBå›¾åƒå—ï¼Œä»¥åŠæ¶µç›–25ä¸ªçŸ¿ç‰©ç±»åˆ«çš„105,802ä¸ªä¸“å®¶æ ‡æ³¨æ™¶ç²’ï¼Œè¯¦ç»†è®°å½•äº†çŸ¿ç‰©ç±»åˆ«ã€ç©ºé—´åæ ‡åŠåæ˜ å‡ ä½•å½¢æ€çš„è½´çº¿æ•°æ®ã€‚ä¸ºæå‡åˆ†ç±»å‡†ç¡®ç‡ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æ•´åˆåŒåæŒ¯æ¨¡æ€çš„dual-encoder transformeræ¶æ„ä½œä¸ºåŸºå‡†æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ€§èƒ½æ˜¾è‘—ä¼˜äºå•åæŒ¯æ¨¡å‹ï¼Œå……åˆ†è¯æ˜äº†åæŒ¯ååŒ(polarization synergy)åœ¨çŸ¿ç‰©è¯†åˆ«ä¸­çš„å…³é”®ä»·å€¼ã€‚ç›®å‰ï¼ŒLITHOS Benchmarkçš„æ‰€æœ‰æ•°æ®é›†ã€ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²å…¬å¼€ï¼Œä¸ºæ¨åŠ¨è‡ªåŠ¨åŒ–å²©ç›¸åˆ†æçš„é‡ç°æ€§ä¸åç»­ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00328v1",
      "published_date": "2025-11-01 00:15:18 UTC",
      "updated_date": "2025-11-01 00:15:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T07:15:41.554575+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 80,
  "processed_papers_count": 80,
  "failed_papers_count": 0,
  "llm_backup_calls": 6,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-25T07:17:59.916279+00:00"
}
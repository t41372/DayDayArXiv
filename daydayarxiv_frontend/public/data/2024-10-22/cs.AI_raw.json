[
  {
    "arxiv_id": "2410.17479v1",
    "title": "Composing Diffusion Policies for Few-shot Learning of Movement Trajectories",
    "authors": [
      "Omkar Patil",
      "Anant Sah",
      "Nakul Gopalan"
    ],
    "abstract": "Humans can perform various combinations of physical skills without having to\nrelearn skills from scratch every single time. For example, we can swing a bat\nwhen walking without having to re-learn such a policy from scratch by composing\nthe individual skills of walking and bat swinging. Enabling robots to combine\nor compose skills is essential so they can learn novel skills and tasks faster\nwith fewer real world samples. To this end, we propose a novel compositional\napproach called DSE- Diffusion Score Equilibrium that enables few-shot learning\nfor novel skills by utilizing a combination of base policy priors. Our method\nis based on probabilistically composing diffusion policies to better model the\nfew-shot demonstration data-distribution than any individual policy. Our goal\nhere is to learn robot motions few-shot and not necessarily goal oriented\ntrajectories. Unfortunately we lack a general purpose metric to evaluate the\nerror between a skill or motion and the provided demonstrations. Hence, we\npropose a probabilistic measure - Maximum Mean Discrepancy on the Forward\nKinematics Kernel (MMD-FK), that is task and action space agnostic. By using\nour few-shot learning approach DSE, we show that we are able to achieve a\nreduction of over 30% in MMD-FK across skills and number of demonstrations.\nMoreover, we show the utility of our approach through real world experiments by\nteaching novel trajectories to a robot in 5 demonstrations.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "6(+1) pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.17479v1",
    "published_date": "2024-10-22 23:57:37 UTC",
    "updated_date": "2024-10-22 23:57:37 UTC"
  },
  {
    "arxiv_id": "2410.17477v4",
    "title": "Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of Architectural Inductive Biases on Hallucination",
    "authors": [
      "Jerry Huang",
      "Prasanna Parthasarathi",
      "Mehdi Rezagholizadeh",
      "Boxing Chen",
      "Sarath Chandar"
    ],
    "abstract": "The growth in prominence of large language models (LLMs) in everyday life can\nbe largely attributed to their generative abilities, yet some of this is also\nowed to the risks and costs associated with their use. On one front is their\ntendency to hallucinate false or misleading information, limiting their\nreliability. On another is the increasing focus on the computational\nlimitations associated with traditional self-attention based LLMs, which has\nbrought about new alternatives, in particular recurrent models, meant to\novercome them. Yet it remains uncommon to consider these two concerns\nsimultaneously. Do changes in architecture exacerbate/alleviate existing\nconcerns about hallucinations? Do they affect how and where they occur? Through\nan extensive evaluation, we study how these architecture-based inductive biases\naffect the propensity to hallucinate. While hallucination remains a general\nphenomenon not limited to specific architectures, the situations in which they\noccur and the ease with which specific types of hallucinations can be induced\ncan significantly differ based on the model architecture. These findings\nhighlight the need for better understanding both these problems in conjunction\nwith each other, as well as consider how to design more universal techniques\nfor handling hallucinations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.17477v4",
    "published_date": "2024-10-22 23:24:15 UTC",
    "updated_date": "2025-04-04 11:55:58 UTC"
  },
  {
    "arxiv_id": "2410.18147v1",
    "title": "MEC-IP: Efficient Discovery of Markov Equivalent Classes via Integer Programming",
    "authors": [
      "Abdelmonem Elrefaey",
      "Rong Pan"
    ],
    "abstract": "This paper presents a novel Integer Programming (IP) approach for discovering\nthe Markov Equivalent Class (MEC) of Bayesian Networks (BNs) through\nobservational data. The MEC-IP algorithm utilizes a unique clique-focusing\nstrategy and Extended Maximal Spanning Graphs (EMSG) to streamline the search\nfor MEC, thus overcoming the computational limitations inherent in other\nexisting algorithms. Our numerical results show that not only a remarkable\nreduction in computational time is achieved by our algorithm but also an\nimprovement in causal discovery accuracy is seen across diverse datasets. These\nfindings underscore this new algorithm's potential as a powerful tool for\nresearchers and practitioners in causal discovery and BNSL, offering a\nsignificant leap forward toward the efficient and accurate analysis of complex\ndata structures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18147v1",
    "published_date": "2024-10-22 22:56:33 UTC",
    "updated_date": "2024-10-22 22:56:33 UTC"
  },
  {
    "arxiv_id": "2410.17469v1",
    "title": "AdaptoML-UX: An Adaptive User-centered GUI-based AutoML Toolkit for Non-AI Experts and HCI Researchers",
    "authors": [
      "Amr Gomaa",
      "Michael Sargious",
      "Antonio Krüger"
    ],
    "abstract": "The increasing integration of machine learning across various domains has\nunderscored the necessity for accessible systems that non-experts can utilize\neffectively. To address this need, the field of automated machine learning\n(AutoML) has developed tools to simplify the construction and optimization of\nML pipelines. However, existing AutoML solutions often lack efficiency in\ncreating online pipelines and ease of use for Human-Computer Interaction (HCI)\napplications. Therefore, in this paper, we introduce AdaptoML-UX, an adaptive\nframework that incorporates automated feature engineering, machine learning,\nand incremental learning to assist non-AI experts in developing robust,\nuser-centered ML models. Our toolkit demonstrates the capability to adapt\nefficiently to diverse problem domains and datasets, particularly in HCI,\nthereby reducing the necessity for manual experimentation and conserving time\nand resources. Furthermore, it supports model personalization through\nincremental learning, customizing models to individual user behaviors. HCI\nresearchers can employ AdaptoML-UX\n(\\url{https://github.com/MichaelSargious/AdaptoML_UX}) without requiring\nspecialized expertise, as it automates the selection of algorithms, feature\nengineering, and hyperparameter tuning based on the unique characteristics of\nthe data.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17469v1",
    "published_date": "2024-10-22 22:52:14 UTC",
    "updated_date": "2024-10-22 22:52:14 UTC"
  },
  {
    "arxiv_id": "2410.17462v3",
    "title": "Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation",
    "authors": [
      "Minhua Lin",
      "Zhengzhang Chen",
      "Yanchi Liu",
      "Xujiang Zhao",
      "Zongyu Wu",
      "Junxiang Wang",
      "Xiang Zhang",
      "Suhang Wang",
      "Haifeng Chen"
    ],
    "abstract": "Time series data is ubiquitous across various domains, including\nmanufacturing, finance, and healthcare. High-quality annotations are essential\nfor effectively understanding time series and facilitating downstream tasks;\nhowever, obtaining such annotations is challenging, particularly in\nmission-critical domains. In this paper, we propose TESSA, a multi-agent system\ndesigned to automatically generate both general and domain-specific annotations\nfor time series data. TESSA introduces two agents: a general annotation agent\nand a domain-specific annotation agent. The general agent captures common\npatterns and knowledge across multiple source domains, leveraging both\ntime-series-wise and text-wise features to generate general annotations.\nMeanwhile, the domain-specific agent utilizes limited annotations from the\ntarget domain to learn domain-specific terminology and generate targeted\nannotations. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate that TESSA effectively generates high-quality annotations,\noutperforming existing methods.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "29 pages, 12 figures, 32 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.17462v3",
    "published_date": "2024-10-22 22:43:14 UTC",
    "updated_date": "2025-05-19 08:07:54 UTC"
  },
  {
    "arxiv_id": "2410.17459v1",
    "title": "Data Obfuscation through Latent Space Projection (LSP) for Privacy-Preserving AI Governance: Case Studies in Medical Diagnosis and Finance Fraud Detection",
    "authors": [
      "Mahesh Vaijainthymala Krishnamoorthy"
    ],
    "abstract": "As AI systems increasingly integrate into critical societal sectors, the\ndemand for robust privacy-preserving methods has escalated. This paper\nintroduces Data Obfuscation through Latent Space Projection (LSP), a novel\ntechnique aimed at enhancing AI governance and ensuring Responsible AI\ncompliance. LSP uses machine learning to project sensitive data into a latent\nspace, effectively obfuscating it while preserving essential features for model\ntraining and inference. Unlike traditional privacy methods like differential\nprivacy or homomorphic encryption, LSP transforms data into an abstract,\nlower-dimensional form, achieving a delicate balance between data utility and\nprivacy. Leveraging autoencoders and adversarial training, LSP separates\nsensitive from non-sensitive information, allowing for precise control over\nprivacy-utility trade-offs. We validate LSP's effectiveness through experiments\non benchmark datasets and two real-world case studies: healthcare cancer\ndiagnosis and financial fraud analysis. Our results show LSP achieves high\nperformance (98.7% accuracy in image classification) while providing strong\nprivacy (97.3% protection against sensitive attribute inference), outperforming\ntraditional anonymization and privacy-preserving methods. The paper also\nexamines LSP's alignment with global AI governance frameworks, such as GDPR,\nCCPA, and HIPAA, highlighting its contribution to fairness, transparency, and\naccountability. By embedding privacy within the machine learning pipeline, LSP\noffers a promising approach to developing AI systems that respect privacy while\ndelivering valuable insights. We conclude by discussing future research\ndirections, including theoretical privacy guarantees, integration with\nfederated learning, and enhancing latent space interpretability, positioning\nLSP as a critical tool for ethical AI advancement.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CY",
      "F.2.1; E.3"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 6 figures, submitted to Conference ICADCML2025",
    "pdf_url": "http://arxiv.org/pdf/2410.17459v1",
    "published_date": "2024-10-22 22:31:03 UTC",
    "updated_date": "2024-10-22 22:31:03 UTC"
  },
  {
    "arxiv_id": "2410.17448v2",
    "title": "In Context Learning and Reasoning for Symbolic Regression with Large Language Models",
    "authors": [
      "Samiha Sharlin",
      "Tyler R. Josephson"
    ],
    "abstract": "Large Language Models (LLMs) are transformer-based machine learning models\nthat have shown remarkable performance in tasks for which they were not\nexplicitly trained. Here, we explore the potential of LLMs to perform symbolic\nregression -- a machine-learning method for finding simple and accurate\nequations from datasets. We prompt GPT-4 to suggest expressions from data,\nwhich are then optimized and evaluated using external Python tools. These\nresults are fed back to GPT-4, which proposes improved expressions while\noptimizing for complexity and loss. Using chain-of-thought prompting, we\ninstruct GPT-4 to analyze the data, prior expressions, and the scientific\ncontext (expressed in natural language) for each problem before generating new\nexpressions. We evaluated the workflow in rediscovery of five well-known\nscientific equations from experimental data, and on an additional dataset\nwithout a known equation. GPT-4 successfully rediscovered all five equations,\nand in general, performed better when prompted to use a scratchpad and consider\nscientific context. We demonstrate how strategic prompting improves the model's\nperformance and how the natural language interface simplifies integrating\ntheory with data. We also observe how theory can sometimes offset noisy data\nand, in other cases, data can make up for poor context. Although this approach\ndoes not outperform established SR programs where target equations are more\ncomplex, LLMs can nonetheless iterate toward improved solutions while following\ninstructions and incorporating scientific context in natural language.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17448v2",
    "published_date": "2024-10-22 21:50:52 UTC",
    "updated_date": "2025-03-12 13:14:22 UTC"
  },
  {
    "arxiv_id": "2410.19859v1",
    "title": "Multi-Modal Transformer and Reinforcement Learning-based Beam Management",
    "authors": [
      "Mohammad Ghassemi",
      "Han Zhang",
      "Ali Afana",
      "Akram Bin Sediq",
      "Melike Erol-Kantarci"
    ],
    "abstract": "Beam management is an important technique to improve signal strength and\nreduce interference in wireless communication systems. Recently, there has been\nincreasing interest in using diverse sensing modalities for beam management.\nHowever, it remains a big challenge to process multi-modal data efficiently and\nextract useful information. On the other hand, the recently emerging\nmulti-modal transformer (MMT) is a promising technique that can process\nmulti-modal data by capturing long-range dependencies. While MMT is highly\neffective in handling multi-modal data and providing robust beam management,\nintegrating reinforcement learning (RL) further enhances their adaptability in\ndynamic environments. In this work, we propose a two-step beam management\nmethod by combining MMT with RL for dynamic beam index prediction. In the first\nstep, we divide available beam indices into several groups and leverage MMT to\nprocess diverse data modalities to predict the optimal beam group. In the\nsecond step, we employ RL for fast beam decision-making within each group,\nwhich in return maximizes throughput. Our proposed framework is tested on a 6G\ndataset. In this testing scenario, it achieves higher beam prediction accuracy\nand system throughput compared to both the MMT-only based method and the\nRL-only based method.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "5 pages, 5 figures, IEEE Networking Letters",
    "pdf_url": "http://arxiv.org/pdf/2410.19859v1",
    "published_date": "2024-10-22 21:44:25 UTC",
    "updated_date": "2024-10-22 21:44:25 UTC"
  },
  {
    "arxiv_id": "2410.17439v3",
    "title": "Evaluating AI-Generated Essays with GRE Analytical Writing Assessment",
    "authors": [
      "Yang Zhong",
      "Jiangang Hao",
      "Michael Fauss",
      "Chen Li",
      "Yuan Wang"
    ],
    "abstract": "The recent revolutionary advance in generative AI enables the generation of\nrealistic and coherent texts by large language models (LLMs). Despite many\nexisting evaluation metrics on the quality of the generated texts, there is\nstill a lack of rigorous assessment of how well LLMs perform in complex and\ndemanding writing assessments. This study examines essays generated by ten\nleading LLMs for the analytical writing assessment of the Graduate Record Exam\n(GRE). We assessed these essays using both human raters and the e-rater\nautomated scoring engine as used in the GRE scoring pipeline. Notably, the\ntop-performing Gemini and GPT-4o received an average score of 4.78 and 4.67,\nrespectively, falling between \"generally thoughtful, well-developed analysis of\nthe issue and conveys meaning clearly\" and \"presents a competent analysis of\nthe issue and conveys meaning with acceptable clarity\" according to the GRE\nscoring guideline. We also evaluated the detection accuracy of these essays,\nwith detectors trained on essays generated by the same and different LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.17439v3",
    "published_date": "2024-10-22 21:30:58 UTC",
    "updated_date": "2024-11-13 04:57:08 UTC"
  },
  {
    "arxiv_id": "2410.17438v1",
    "title": "Interpreting Affine Recurrence Learning in GPT-style Transformers",
    "authors": [
      "Samarth Bhargav",
      "Alexander Gu"
    ],
    "abstract": "Understanding the internal mechanisms of GPT-style transformers, particularly\ntheir capacity to perform in-context learning (ICL), is critical for advancing\nAI alignment and interpretability. In-context learning allows transformers to\ngeneralize during inference without modifying their weights, yet the precise\noperations driving this capability remain largely opaque. This paper presents\nan investigation into the mechanistic interpretability of these transformers,\nfocusing specifically on their ability to learn and predict affine recurrences\nas an ICL task. To address this, we trained a custom three-layer transformer to\npredict affine recurrences and analyzed the model's internal operations using\nboth empirical and theoretical approaches. Our findings reveal that the model\nforms an initial estimate of the target sequence using a copying mechanism in\nthe zeroth layer, which is subsequently refined through negative similarity\nheads in the second layer. These insights contribute to a deeper understanding\nof transformer behaviors in recursive tasks and offer potential avenues for\nimproving AI alignment through mechanistic interpretability. Finally, we\ndiscuss the implications of our results for future work, including extensions\nto higher-dimensional recurrences and the exploration of polynomial sequences.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.17438v1",
    "published_date": "2024-10-22 21:30:01 UTC",
    "updated_date": "2024-10-22 21:30:01 UTC"
  },
  {
    "arxiv_id": "2410.17433v1",
    "title": "Revisiting Technical Bias Mitigation Strategies",
    "authors": [
      "Abdoul Jalil Djiberou Mahamadou",
      "Artem A. Trotsyuk"
    ],
    "abstract": "Efforts to mitigate bias and enhance fairness in the artificial intelligence\n(AI) community have predominantly focused on technical solutions. While\nnumerous reviews have addressed bias in AI, this review uniquely focuses on the\npractical limitations of technical solutions in healthcare settings, providing\na structured analysis across five key dimensions affecting their real-world\nimplementation: who defines bias and fairness; which mitigation strategy to use\nand prioritize among dozens that are inconsistent and incompatible; when in the\nAI development stages the solutions are most effective; for which populations;\nand the context in which the solutions are designed. We illustrate each\nlimitation with empirical studies focusing on healthcare and biomedical\napplications. Moreover, we discuss how value-sensitive AI, a framework derived\nfrom technology design, can engage stakeholders and ensure that their values\nare embodied in bias and fairness mitigation solutions. Finally, we discuss\nareas that require further investigation and provide practical recommendations\nto address the limitations covered in the study.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "I.2.6"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17433v1",
    "published_date": "2024-10-22 21:17:19 UTC",
    "updated_date": "2024-10-22 21:17:19 UTC"
  },
  {
    "arxiv_id": "2411.04129v1",
    "title": "AmazonQAC: A Large-Scale, Naturalistic Query Autocomplete Dataset",
    "authors": [
      "Dante Everaert",
      "Rohit Patki",
      "Tianqi Zheng",
      "Christopher Potts"
    ],
    "abstract": "Query Autocomplete (QAC) is a critical feature in modern search engines,\nfacilitating user interaction by predicting search queries based on input\nprefixes. Despite its widespread adoption, the absence of large-scale,\nrealistic datasets has hindered advancements in QAC system development. This\npaper addresses this gap by introducing AmazonQAC, a new QAC dataset sourced\nfrom Amazon Search logs, comprising 395M samples. The dataset includes actual\nsequences of user-typed prefixes leading to final search terms, as well as\nsession IDs and timestamps that support modeling the context-dependent aspects\nof QAC. We assess Prefix Trees, semantic retrieval, and Large Language Models\n(LLMs) with and without finetuning. We find that finetuned LLMs perform best,\nparticularly when incorporating contextual information. However, even our best\nsystem achieves only half of what we calculate is theoretically possible on our\ntest data, which implies QAC is a challenging problem that is far from solved\nwith existing systems. This contribution aims to stimulate further research on\nQAC systems to better serve user needs in diverse environments. We open-source\nthis data on Hugging Face at https://huggingface.co/datasets/amazon/AmazonQAC.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.04129v1",
    "published_date": "2024-10-22 21:11:34 UTC",
    "updated_date": "2024-10-22 21:11:34 UTC"
  },
  {
    "arxiv_id": "2410.17423v1",
    "title": "Artificial Intelligence in Brazilian News: A Mixed-Methods Analysis",
    "authors": [
      "Raphael Hernandes",
      "Giulio Corsi"
    ],
    "abstract": "The current surge in Artificial Intelligence (AI) interest, reflected in\nheightened media coverage since 2009, has sparked significant debate on AI's\nimplications for privacy, social justice, workers' rights, and democracy. The\nmedia plays a crucial role in shaping public perception and acceptance of AI\ntechnologies. However, research into how AI appears in media has primarily\nfocused on anglophone contexts, leaving a gap in understanding how AI is\nrepresented globally. This study addresses this gap by analyzing 3,560 news\narticles from Brazilian media published between July 1, 2023, and February 29,\n2024, from 13 popular online news outlets. Using Computational Grounded Theory\n(CGT), the study applies Latent Dirichlet Allocation (LDA), BERTopic, and\nNamed-Entity Recognition to investigate the main topics in AI coverage and the\nentities represented. The findings reveal that Brazilian news coverage of AI is\ndominated by topics related to applications in the workplace and product\nlaunches, with limited space for societal concerns, which mostly focus on\ndeepfakes and electoral integrity. The analysis also highlights a significant\npresence of industry-related entities, indicating a strong influence of\ncorporate agendas in the country's news. This study underscores the need for a\nmore critical and nuanced discussion of AI's societal impacts in Brazilian\nmedia.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 8 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.17423v1",
    "published_date": "2024-10-22 20:52:51 UTC",
    "updated_date": "2024-10-22 20:52:51 UTC"
  },
  {
    "arxiv_id": "2410.18146v1",
    "title": "Meaning Typed Prompting: A Technique for Efficient, Reliable Structured Output Generation",
    "authors": [
      "Chandra Irugalbandara"
    ],
    "abstract": "Extending Large Language Models (LLMs) to advanced applications requires\nreliable structured output generation. Existing methods which often rely on\nrigid JSON schemas, can lead to unreliable outputs, diminished reasoning\ncapabilities, and increased computational overhead, limiting LLMs' adaptability\nfor complex tasks. We introduce Meaning Typed Prompting (MTP), a technique for\nefficient structured output generation that integrates types, meanings, and\nabstractions, such as variables and classes, into the prompting process. By\nutilizing expressive type definitions, MTP enhances output clarity and reduces\ndependence on complex abstractions, simplifying development, and improving\nimplementation efficiency. This enables LLMs to understand relationships and\ngenerate structured data more effectively. Empirical evaluations on multiple\nbenchmarks demonstrate that MTP outperforms existing frameworks in accuracy,\nreliability, consistency, and token efficiency. We present Semantix, a\nframework that implements MTP, providing practical insights into its\napplication.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18146v1",
    "published_date": "2024-10-22 20:43:50 UTC",
    "updated_date": "2024-10-22 20:43:50 UTC"
  },
  {
    "arxiv_id": "2410.17415v1",
    "title": "End-to-End Optimization and Learning of Fair Court Schedules",
    "authors": [
      "My H Dinh",
      "James Kotary",
      "Lauryn P. Gouldin",
      "William Yeoh",
      "Ferdinando Fioretto"
    ],
    "abstract": "Criminal courts across the United States handle millions of cases every year,\nand the scheduling of those cases must accommodate a diverse set of\nconstraints, including the preferences and availability of courts, prosecutors,\nand defense teams. When criminal court schedules are formed, defendants'\nscheduling preferences often take the least priority, although defendants may\nface significant consequences (including arrest or detention) for missed court\ndates. Additionally, studies indicate that defendants' nonappearances impose\ncosts on the courts and other system stakeholders. To address these issues,\ncourts and commentators have begun to recognize that pretrial outcomes for\ndefendants and for the system would be improved with greater attention to court\nprocesses, including \\emph{court scheduling practices}. There is thus a need\nfor fair criminal court pretrial scheduling systems that account for\ndefendants' preferences and availability, but the collection of such data poses\nlogistical challenges. Furthermore, optimizing schedules fairly across various\nparties' preferences is a complex optimization problem, even when such data is\navailable. In an effort to construct such a fair scheduling system under data\nuncertainty, this paper proposes a joint optimization and learning framework\nthat combines machine learning models trained end-to-end with efficient\nmatching algorithms. This framework aims to produce court scheduling schedules\nthat optimize a principled measure of fairness, balancing the availability and\npreferences of all parties.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17415v1",
    "published_date": "2024-10-22 20:40:53 UTC",
    "updated_date": "2024-10-22 20:40:53 UTC"
  },
  {
    "arxiv_id": "2410.17409v1",
    "title": "Geometric Graph Neural Network Modeling of Human Interactions in Crowded Environments",
    "authors": [
      "Sara Honarvar",
      "Yancy Diaz-Mercado"
    ],
    "abstract": "Modeling human trajectories in crowded environments is challenging due to the\ncomplex nature of pedestrian behavior and interactions. This paper proposes a\ngeometric graph neural network (GNN) architecture that integrates domain\nknowledge from psychological studies to model pedestrian interactions and\npredict future trajectories. Unlike prior studies using complete graphs, we\ndefine interaction neighborhoods using pedestrians' field of view, motion\ndirection, and distance-based kernel functions to construct graph\nrepresentations of crowds. Evaluations across multiple datasets demonstrate\nimproved prediction accuracy through reduced average and final displacement\nerror metrics. Our findings underscore the importance of integrating domain\nknowledge with data-driven approaches for effective modeling of human\ninteractions in crowds.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "\\c{opyright} 2024 the authors. This work has been accepted to IFAC\n  for publication under a Creative Commons Licence CC-BY-NC-ND",
    "pdf_url": "http://arxiv.org/pdf/2410.17409v1",
    "published_date": "2024-10-22 20:33:10 UTC",
    "updated_date": "2024-10-22 20:33:10 UTC"
  },
  {
    "arxiv_id": "2410.17397v1",
    "title": "Quantum Large Language Models via Tensor Network Disentanglers",
    "authors": [
      "Borja Aizpurua",
      "Saeed S. Jahromi",
      "Sukhbinder Singh",
      "Roman Orus"
    ],
    "abstract": "We propose a method to enhance the performance of Large Language Models\n(LLMs) by integrating quantum computing and quantum-inspired techniques.\nSpecifically, our approach involves replacing the weight matrices in the\nSelf-Attention and Multi-layer Perceptron layers with a combination of two\nvariational quantum circuits and a quantum-inspired tensor network, such as a\nMatrix Product Operator (MPO). This substitution enables the reproduction of\nclassical LLM functionality by decomposing weight matrices through the\napplication of tensor network disentanglers and MPOs, leveraging\nwell-established tensor network techniques. By incorporating more complex and\ndeeper quantum circuits, along with increasing the bond dimensions of the MPOs,\nour method captures additional correlations within the quantum-enhanced LLM,\nleading to improved accuracy beyond classical models while maintaining low\nmemory overhead.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "4 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.17397v1",
    "published_date": "2024-10-22 20:12:04 UTC",
    "updated_date": "2024-10-22 20:12:04 UTC"
  },
  {
    "arxiv_id": "2410.17395v1",
    "title": "A 10.60 $μ$W 150 GOPS Mixed-Bit-Width Sparse CNN Accelerator for Life-Threatening Ventricular Arrhythmia Detection",
    "authors": [
      "Yifan Qin",
      "Zhenge Jia",
      "Zheyu Yan",
      "Jay Mok",
      "Manto Yung",
      "Yu Liu",
      "Xuejiao Liu",
      "Wujie Wen",
      "Luhong Liang",
      "Kwang-Ting Tim Cheng",
      "X. Sharon Hu",
      "Yiyu Shi"
    ],
    "abstract": "This paper proposes an ultra-low power, mixed-bit-width sparse convolutional\nneural network (CNN) accelerator to accelerate ventricular arrhythmia (VA)\ndetection. The chip achieves 50% sparsity in a quantized 1D CNN using a sparse\nprocessing element (SPE) architecture. Measurement on the prototype chip TSMC\n40nm CMOS low-power (LP) process for the VA classification task demonstrates\nthat it consumes 10.60 $\\mu$W of power while achieving a performance of 150\nGOPS and a diagnostic accuracy of 99.95%. The computation power density is only\n0.57 $\\mu$W/mm$^2$, which is 14.23X smaller than state-of-the-art works, making\nit highly suitable for implantable and wearable medical devices.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "2 pages, accepted to The 30th Asia and South Pacific Design\n  Automation Conference (ASP-DAC 2025)",
    "pdf_url": "http://arxiv.org/pdf/2410.17395v1",
    "published_date": "2024-10-22 20:02:25 UTC",
    "updated_date": "2024-10-22 20:02:25 UTC"
  },
  {
    "arxiv_id": "2410.17394v1",
    "title": "packetLSTM: Dynamic LSTM Framework for Streaming Data with Varying Feature Space",
    "authors": [
      "Rohit Agarwal",
      "Karaka Prasanth Naidu",
      "Alexander Horsch",
      "Krishna Agarwal",
      "Dilip K. Prasad"
    ],
    "abstract": "We study the online learning problem characterized by the varying input\nfeature space of streaming data. Although LSTMs have been employed to\neffectively capture the temporal nature of streaming data, they cannot handle\nthe dimension-varying streams in an online learning setting. Therefore, we\npropose a dynamic LSTM-based novel method, called packetLSTM, to model the\ndimension-varying streams. The packetLSTM's dynamic framework consists of an\nevolving packet of LSTMs, each dedicated to processing one input feature. Each\nLSTM retains the local information of its corresponding feature, while a shared\ncommon memory consolidates global information. This configuration facilitates\ncontinuous learning and mitigates the issue of forgetting, even when certain\nfeatures are absent for extended time periods. The idea of utilizing one LSTM\nper feature coupled with a dimension-invariant operator for information\naggregation enhances the dynamic nature of packetLSTM. This dynamic nature is\nevidenced by the model's ability to activate, deactivate, and add new LSTMs as\nrequired, thus seamlessly accommodating varying input dimensions. The\npacketLSTM achieves state-of-the-art results on five datasets, and its\nunderlying principle is extended to other RNN types, like GRU and vanilla RNN.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17394v1",
    "published_date": "2024-10-22 20:01:39 UTC",
    "updated_date": "2024-10-22 20:01:39 UTC"
  },
  {
    "arxiv_id": "2410.17389v1",
    "title": "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models",
    "authors": [
      "Muhan Lin",
      "Shuyang Shi",
      "Yue Guo",
      "Behdad Chalaki",
      "Vaishnav Tadiparthi",
      "Ehsan Moradi Pari",
      "Simon Stepputtis",
      "Joseph Campbell",
      "Katia Sycara"
    ],
    "abstract": "The correct specification of reward models is a well-known challenge in\nreinforcement learning. Hand-crafted reward functions often lead to inefficient\nor suboptimal policies and may not be aligned with user values. Reinforcement\nlearning from human feedback is a successful technique that can mitigate such\nissues, however, the collection of human feedback can be laborious. Recent\nworks have solicited feedback from pre-trained large language models rather\nthan humans to reduce or eliminate human effort, however, these approaches\nyield poor performance in the presence of hallucination and other errors. This\npaper studies the advantages and limitations of reinforcement learning from\nlarge language model feedback and proposes a simple yet effective method for\nsoliciting and applying feedback as a potential-based shaping function. We\ntheoretically show that inconsistent rankings, which approximate ranking\nerrors, lead to uninformative rewards with our approach. Our method empirically\nimproves convergence speed and policy returns over commonly used baselines even\nwith significant ranking errors, and eliminates the need for complex\npost-processing of reward functions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages, 8 figures, The 2024 Conference on Empirical Methods in\n  Natural Language Processing",
    "pdf_url": "http://arxiv.org/pdf/2410.17389v1",
    "published_date": "2024-10-22 19:52:08 UTC",
    "updated_date": "2024-10-22 19:52:08 UTC"
  },
  {
    "arxiv_id": "2410.17373v1",
    "title": "Episodic Future Thinking Mechanism for Multi-agent Reinforcement Learning",
    "authors": [
      "Dongsu Lee",
      "Minhae Kwon"
    ],
    "abstract": "Understanding cognitive processes in multi-agent interactions is a primary\ngoal in cognitive science. It can guide the direction of artificial\nintelligence (AI) research toward social decision-making in multi-agent\nsystems, which includes uncertainty from character heterogeneity. In this\npaper, we introduce an episodic future thinking (EFT) mechanism for a\nreinforcement learning (RL) agent, inspired by cognitive processes observed in\nanimals. To enable future thinking functionality, we first develop a\nmulti-character policy that captures diverse characters with an ensemble of\nheterogeneous policies. Here, the character of an agent is defined as a\ndifferent weight combination on reward components, representing distinct\nbehavioral preferences. The future thinking agent collects observation-action\ntrajectories of the target agents and uses the pre-trained multi-character\npolicy to infer their characters. Once the character is inferred, the agent\npredicts the upcoming actions of target agents and simulates the potential\nfuture scenario. This capability allows the agent to adaptively select the\noptimal action, considering the predicted future scenario in multi-agent\ninteractions. To evaluate the proposed mechanism, we consider the multi-agent\nautonomous driving scenario with diverse driving traits and multiple particle\nenvironments. Simulation results demonstrate that the EFT mechanism with\naccurate character inference leads to a higher reward than existing multi-agent\nsolutions. We also confirm that the effect of reward improvement remains valid\nacross societies with different levels of character diversity.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024 (Web: https://sites.google.com/view/eftm-neurips2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.17373v1",
    "published_date": "2024-10-22 19:12:42 UTC",
    "updated_date": "2024-10-22 19:12:42 UTC"
  },
  {
    "arxiv_id": "2410.17363v1",
    "title": "DeLLiriuM: A large language model for delirium prediction in the ICU using structured EHR",
    "authors": [
      "Miguel Contreras",
      "Sumit Kapoor",
      "Jiaqing Zhang",
      "Andrea Davidson",
      "Yuanfang Ren",
      "Ziyuan Guan",
      "Tezcan Ozrazgat-Baslanti",
      "Subhash Nerella",
      "Azra Bihorac",
      "Parisa Rashidi"
    ],
    "abstract": "Delirium is an acute confusional state that has been shown to affect up to\n31% of patients in the intensive care unit (ICU). Early detection of this\ncondition could lead to more timely interventions and improved health outcomes.\nWhile artificial intelligence (AI) models have shown great potential for ICU\ndelirium prediction using structured electronic health records (EHR), most of\nthem have not explored the use of state-of-the-art AI models, have been limited\nto single hospitals, or have been developed and validated on small cohorts. The\nuse of large language models (LLM), models with hundreds of millions to\nbillions of parameters, with structured EHR data could potentially lead to\nimproved predictive performance. In this study, we propose DeLLiriuM, a novel\nLLM-based delirium prediction model using EHR data available in the first 24\nhours of ICU admission to predict the probability of a patient developing\ndelirium during the rest of their ICU admission. We develop and validate\nDeLLiriuM on ICU admissions from 104,303 patients pertaining to 195 hospitals\nacross three large databases: the eICU Collaborative Research Database, the\nMedical Information Mart for Intensive Care (MIMIC)-IV, and the University of\nFlorida Health's Integrated Data Repository. The performance measured by the\narea under the receiver operating characteristic curve (AUROC) showed that\nDeLLiriuM outperformed all baselines in two external validation sets, with 0.77\n(95% confidence interval 0.76-0.78) and 0.84 (95% confidence interval\n0.83-0.85) across 77,543 patients spanning 194 hospitals. To the best of our\nknowledge, DeLLiriuM is the first LLM-based delirium prediction tool for the\nICU based on structured EHR data, outperforming deep learning baselines which\nemploy structured features and can provide helpful information to clinicians\nfor timely interventions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17363v1",
    "published_date": "2024-10-22 18:56:31 UTC",
    "updated_date": "2024-10-22 18:56:31 UTC"
  },
  {
    "arxiv_id": "2410.17358v1",
    "title": "FairLoRA: Unpacking Bias Mitigation in Vision Models with Fairness-Driven Low-Rank Adaptation",
    "authors": [
      "Rohan Sukumaran",
      "Aarash Feizi",
      "Adriana Romero-Sorian",
      "Golnoosh Farnadi"
    ],
    "abstract": "Recent advances in parameter-efficient fine-tuning methods, such as Low Rank\nAdaptation (LoRA), have gained significant attention for their ability to\nefficiently adapt large foundational models to various downstream tasks. These\nmethods are appreciated for achieving performance comparable to full\nfine-tuning on aggregate-level metrics, while significantly reducing\ncomputational costs. To systematically address fairness in LLMs previous\nstudies fine-tune on fairness specific data using a larger LoRA rank than\ntypically used. In this paper, we introduce FairLoRA, a novel fairness-specific\nregularizer for LoRA aimed at reducing performance disparities across data\nsubgroups by minimizing per-class variance in loss. To the best of our\nknowledge, we are the first to introduce a fairness based finetuning through\nLoRA. Our results demonstrate that the need for higher ranks to mitigate bias\nis not universal; it depends on factors such as the pre-trained model, dataset,\nand task. More importantly, we systematically evaluate FairLoRA across various\nvision models, including ViT, DiNO, and CLIP, in scenarios involving\ndistribution shifts. We further emphasize the necessity of using multiple\nfairness metrics to obtain a holistic assessment of fairness, rather than\nrelying solely on the metric optimized during training.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17358v1",
    "published_date": "2024-10-22 18:50:36 UTC",
    "updated_date": "2024-10-22 18:50:36 UTC"
  },
  {
    "arxiv_id": "2410.17343v1",
    "title": "EEG-DIF: Early Warning of Epileptic Seizures through Generative Diffusion Model-based Multi-channel EEG Signals Forecasting",
    "authors": [
      "Zekun Jiang",
      "Wei Dai",
      "Qu Wei",
      "Ziyuan Qin",
      "Kang Li",
      "Le Zhang"
    ],
    "abstract": "Multi-channel EEG signals are commonly used for the diagnosis and assessment\nof diseases such as epilepsy. Currently, various EEG diagnostic algorithms\nbased on deep learning have been developed. However, most research efforts\nfocus solely on diagnosing and classifying current signal data but do not\nconsider the prediction of future trends for early warning. Additionally, since\nmulti-channel EEG can be essentially regarded as the spatio-temporal signal\ndata received by detectors at different locations in the brain, how to\nconstruct spatio-temporal information representations of EEG signals to\nfacilitate future trend prediction for multi-channel EEG becomes an important\nproblem. This study proposes a multi-signal prediction algorithm based on\ngenerative diffusion models (EEG-DIF), which transforms the multi-signal\nforecasting task into an image completion task, allowing for comprehensive\nrepresentation and learning of the spatio-temporal correlations and future\ndevelopmental patterns of multi-channel EEG signals. Here, we employ a publicly\navailable epilepsy EEG dataset to construct and validate the EEG-DIF. The\nresults demonstrate that our method can accurately predict future trends for\nmulti-channel EEG signals simultaneously. Furthermore, the early warning\naccuracy for epilepsy seizures based on the generated EEG data reaches 0.89. In\ngeneral, EEG-DIF provides a novel approach for characterizing multi-channel EEG\nsignals and an innovative early warning algorithm for epilepsy seizures, aiding\nin optimizing and enhancing the clinical diagnosis process. The code is\navailable at https://github.com/JZK00/EEG-DIF.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "9 pages, 4 figures, 3 tables, accepted by ACM BCB 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.17343v1",
    "published_date": "2024-10-22 18:18:48 UTC",
    "updated_date": "2024-10-22 18:18:48 UTC"
  },
  {
    "arxiv_id": "2410.17337v1",
    "title": "Captions Speak Louder than Images (CASLIE): Generalizing Foundation Models for E-commerce from High-quality Multimodal Instruction Data",
    "authors": [
      "Xinyi Ling",
      "Bo Peng",
      "Hanwen Du",
      "Zhihui Zhu",
      "Xia Ning"
    ],
    "abstract": "Leveraging multimodal data to drive breakthroughs in e-commerce applications\nthrough Multimodal Foundation Models (MFMs) is gaining increasing attention\nfrom the research community. However, there are significant challenges that\nhinder the optimal use of multimodal e-commerce data by foundation models: (1)\nthe scarcity of large-scale, high-quality multimodal benchmark datasets; and\n(2) the lack of effective multimodal information integration methods. To\naddress these challenges, in this paper, we introduce MMECInstruct, the\nfirst-ever, large-scale, and high-quality multimodal instruction dataset for\ne-commerce. We also develop CASLIE, a simple, lightweight, yet effective\nframework for integrating multimodal information for e-commerce. Leveraging\nMMECInstruct, we fine-tune a series of e-commerce MFMs within CASLIE, denoted\nas CASLIE models. Our comprehensive evaluation demonstrates that CASLIE models\nsubstantially outperform 5 categories of advanced baseline models in the\nin-domain evaluation. Moreover, CASLIE models show strong generalizability to\nout-of-domain settings. MMECInstruct and CASLIE models are publicly accessible\nthrough https://ninglab.github.io/CASLIE/.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Xinyi Ling and Bo Peng contributed equally to this paper",
    "pdf_url": "http://arxiv.org/pdf/2410.17337v1",
    "published_date": "2024-10-22 18:11:43 UTC",
    "updated_date": "2024-10-22 18:11:43 UTC"
  },
  {
    "arxiv_id": "2410.17333v1",
    "title": "Are Large Language Models Ready for Travel Planning?",
    "authors": [
      "Ruiping Ren",
      "Xing Yao",
      "Shu Cole",
      "Haining Wang"
    ],
    "abstract": "While large language models (LLMs) show promise in hospitality and tourism,\ntheir ability to provide unbiased service across demographic groups remains\nunclear. This paper explores gender and ethnic biases when LLMs are utilized as\ntravel planning assistants. To investigate this issue, we apply machine\nlearning techniques to analyze travel suggestions generated from three\nopen-source LLMs. Our findings reveal that the performance of race and gender\nclassifiers substantially exceeds random chance, indicating differences in how\nLLMs engage with varied subgroups. Specifically, outputs align with cultural\nexpectations tied to certain races and genders. To minimize the effect of these\nstereotypes, we used a stop-word classification strategy, which decreased\nidentifiable differences, with no disrespectful terms found. However,\nhallucinations related to African American and gender minority groups were\nnoted. In conclusion, while LLMs can generate travel plans seemingly free from\nbias, it remains essential to verify the accuracy and appropriateness of their\nrecommendations.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17333v1",
    "published_date": "2024-10-22 18:08:25 UTC",
    "updated_date": "2024-10-22 18:08:25 UTC"
  },
  {
    "arxiv_id": "2410.17309v3",
    "title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
    "authors": [
      "Haokun Liu",
      "Yangqiaoyu Zhou",
      "Mingxuan Li",
      "Chenfei Yuan",
      "Chenhao Tan"
    ],
    "abstract": "AI holds promise for transforming scientific processes, including hypothesis\ngeneration. Prior work on hypothesis generation can be broadly categorized into\ntheory-driven and data-driven approaches. While both have proven effective in\ngenerating novel and plausible hypotheses, it remains an open question whether\nthey can complement each other. To address this, we develop the first method\nthat combines literature-based insights with data to perform LLM-powered\nhypothesis generation. We apply our method on five different datasets and\ndemonstrate that integrating literature and data outperforms other baselines\n(8.97\\% over few-shot, 15.75\\% over literature-based alone, and 3.37\\% over\ndata-driven alone). Additionally, we conduct the first human evaluation to\nassess the utility of LLM-generated hypotheses in assisting human\ndecision-making on two challenging tasks: deception detection and AI generated\ncontent detection. Our results show that human accuracy improves significantly\nby 7.44\\% and 14.19\\% on these tasks, respectively. These findings suggest that\nintegrating literature-based and data-driven approaches provides a\ncomprehensive and nuanced framework for hypothesis generation and could open\nnew avenues for scientific inquiry.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "37 pages, 9 figures, code link:\n  https://github.com/ChicagoHAI/hypothesis-generation",
    "pdf_url": "http://arxiv.org/pdf/2410.17309v3",
    "published_date": "2024-10-22 18:00:00 UTC",
    "updated_date": "2025-01-08 19:00:00 UTC"
  },
  {
    "arxiv_id": "2410.17250v2",
    "title": "JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation",
    "authors": [
      "Shota Onohara",
      "Atsuyuki Miyai",
      "Yuki Imajuku",
      "Kazuki Egashira",
      "Jeonghun Baek",
      "Xiang Yue",
      "Graham Neubig",
      "Kiyoharu Aizawa"
    ],
    "abstract": "Accelerating research on Large Multimodal Models (LMMs) in non-English\nlanguages is crucial for enhancing user experiences across broader populations.\nIn this paper, we introduce JMMMU (Japanese MMMU), the first large-scale\nJapanese benchmark designed to evaluate LMMs on expert-level tasks based on the\nJapanese cultural context. To facilitate comprehensive culture-aware\nevaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA)\nsubset, where the culture-independent subjects (e.g., Math) are selected and\ntranslated into Japanese, enabling one-to-one comparison with its English\ncounterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly\ncrafted subjects that reflect Japanese cultural context. Using the CA subset,\nwe observe performance drop in many LMMs when evaluated in Japanese, which is\npurely attributable to language variation. Using the CS subset, we reveal their\ninadequate Japanese cultural understanding. Further, by combining both subsets,\nwe identify that some LMMs perform well on the CA subset but not on the CS\nsubset, exposing a shallow understanding of the Japanese language that lacks\ndepth in cultural understanding. We hope this work will not only help advance\nLMM performance in Japanese but also serve as a guideline to create\nhigh-standard, culturally diverse benchmarks for multilingual LMM development.\nThe project page is https://mmmu-japanese-benchmark.github.io/JMMMU/.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NAACL 2025. Project page:\n  https://mmmu-japanese-benchmark.github.io/JMMMU/",
    "pdf_url": "http://arxiv.org/pdf/2410.17250v2",
    "published_date": "2024-10-22 17:59:56 UTC",
    "updated_date": "2025-03-19 08:24:14 UTC"
  },
  {
    "arxiv_id": "2410.17248v3",
    "title": "HyperspectralViTs: General Hyperspectral Models for On-board Remote Sensing",
    "authors": [
      "Vít Růžička",
      "Andrew Markham"
    ],
    "abstract": "On-board processing of hyperspectral data with machine learning models would\nenable unprecedented amount of autonomy for a wide range of tasks, for example\nmethane detection or mineral identification. This can enable early warning\nsystem and could allow new capabilities such as automated scheduling across\nconstellations of satellites. Classical methods suffer from high false positive\nrates and previous deep learning models exhibit prohibitive computational\nrequirements. We propose fast and accurate machine learning architectures which\nsupport end-to-end training with data of high spectral dimension without\nrelying on hand-crafted products or spectral band compression preprocessing. We\nevaluate our models on two tasks related to hyperspectral data processing. With\nour proposed general architectures, we improve the F1 score of the previous\nmethane detection state-of-the-art models by 27% on a newly created synthetic\ndataset and by 13% on the previously released large benchmark dataset. We also\ndemonstrate that training models on the synthetic dataset improves performance\nof models finetuned on the dataset of real events by 6.9% in F1 score in\ncontrast with training from scratch. On a newly created dataset for mineral\nidentification, our models provide 3.5% improvement in the F1 score in contrast\nto the default versions of the models. With our proposed models we improve the\ninference speed by 85% in contrast to previous classical and deep learning\napproaches by removing the dependency on classically computed features. With\nour architecture, one capture from the EMIT sensor can be processed within 30\nseconds on realistic proxy of the ION-SCV 004 satellite.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, Accepted manuscript version, published version may differ\n  in minor details and formatting",
    "pdf_url": "http://arxiv.org/pdf/2410.17248v3",
    "published_date": "2024-10-22 17:59:55 UTC",
    "updated_date": "2025-04-12 12:37:11 UTC"
  },
  {
    "arxiv_id": "2410.17246v2",
    "title": "Learning Precise, Contact-Rich Manipulation through Uncalibrated Tactile Skins",
    "authors": [
      "Venkatesh Pattabiraman",
      "Yifeng Cao",
      "Siddhant Haldar",
      "Lerrel Pinto",
      "Raunaq Bhirangi"
    ],
    "abstract": "While visuomotor policy learning has advanced robotic manipulation, precisely\nexecuting contact-rich tasks remains challenging due to the limitations of\nvision in reasoning about physical interactions. To address this, recent work\nhas sought to integrate tactile sensing into policy learning. However, many\nexisting approaches rely on optical tactile sensors that are either restricted\nto recognition tasks or require complex dimensionality reduction steps for\npolicy learning. In this work, we explore learning policies with magnetic skin\nsensors, which are inherently low-dimensional, highly sensitive, and\ninexpensive to integrate with robotic platforms. To leverage these sensors\neffectively, we present the Visuo-Skin (ViSk) framework, a simple approach that\nuses a transformer-based policy and treats skin sensor data as additional\ntokens alongside visual information. Evaluated on four complex real-world tasks\ninvolving credit card swiping, plug insertion, USB insertion, and bookshelf\nretrieval, ViSk significantly outperforms both vision-only and optical tactile\nsensing based policies. Further analysis reveals that combining tactile and\nvisual modalities enhances policy performance and spatial generalization,\nachieving an average improvement of 27.5% across tasks.\nhttps://visuoskin.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17246v2",
    "published_date": "2024-10-22 17:59:49 UTC",
    "updated_date": "2024-10-26 02:25:50 UTC"
  },
  {
    "arxiv_id": "2410.17245v1",
    "title": "Towards Reliable Evaluation of Behavior Steering Interventions in LLMs",
    "authors": [
      "Itamar Pres",
      "Laura Ruis",
      "Ekdeep Singh Lubana",
      "David Krueger"
    ],
    "abstract": "Representation engineering methods have recently shown promise for enabling\nefficient steering of model behavior. However, evaluation pipelines for these\nmethods have primarily relied on subjective demonstrations, instead of\nquantitative, objective metrics. We aim to take a step towards addressing this\nissue by advocating for four properties missing from current evaluations: (i)\ncontexts sufficiently similar to downstream tasks should be used for assessing\nintervention quality; (ii) model likelihoods should be accounted for; (iii)\nevaluations should allow for standardized comparisons across different target\nbehaviors; and (iv) baseline comparisons should be offered. We introduce an\nevaluation pipeline grounded in these criteria, offering both a quantitative\nand visual analysis of how effectively a given method works. We use this\npipeline to evaluate two representation engineering methods on how effectively\nthey can steer behaviors such as truthfulness and corrigibility, finding that\nsome interventions are less effective than previously reported.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to the NeurIPS 2024 - Workshop on Foundation Model\n  Interventions",
    "pdf_url": "http://arxiv.org/pdf/2410.17245v1",
    "published_date": "2024-10-22 17:59:39 UTC",
    "updated_date": "2024-10-22 17:59:39 UTC"
  },
  {
    "arxiv_id": "2410.17238v1",
    "title": "SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning",
    "authors": [
      "Yizhou Chi",
      "Yizhang Lin",
      "Sirui Hong",
      "Duyi Pan",
      "Yaying Fei",
      "Guanghao Mei",
      "Bangbang Liu",
      "Tianqi Pang",
      "Jacky Kwok",
      "Ceyao Zhang",
      "Bang Liu",
      "Chenglin Wu"
    ],
    "abstract": "Automated Machine Learning (AutoML) approaches encompass traditional methods\nthat optimize fixed pipelines for model selection and ensembling, as well as\nnewer LLM-based frameworks that autonomously build pipelines. While LLM-based\nagents have shown promise in automating machine learning tasks, they often\ngenerate low-diversity and suboptimal code, even after multiple iterations. To\novercome these limitations, we introduce Tree-Search Enhanced LLM Agents\n(SELA), an innovative agent-based system that leverages Monte Carlo Tree Search\n(MCTS) to optimize the AutoML process. By representing pipeline configurations\nas trees, our framework enables agents to conduct experiments intelligently and\niteratively refine their strategies, facilitating a more effective exploration\nof the machine learning solution space. This novel approach allows SELA to\ndiscover optimal pathways based on experimental feedback, improving the overall\nquality of the solutions. In an extensive evaluation across 20 machine learning\ndatasets, we compare the performance of traditional and agent-based AutoML\nmethods, demonstrating that SELA achieves a win rate of 65% to 80% against each\nbaseline across all datasets. These results underscore the significant\npotential of agent-based strategies in AutoML, offering a fresh perspective on\ntackling complex machine learning challenges.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "The code is available at https://github.com/geekan/MetaGPT",
    "pdf_url": "http://arxiv.org/pdf/2410.17238v1",
    "published_date": "2024-10-22 17:56:08 UTC",
    "updated_date": "2024-10-22 17:56:08 UTC"
  },
  {
    "arxiv_id": "2410.17236v2",
    "title": "Large Language Models Empowered Personalized Web Agents",
    "authors": [
      "Hongru Cai",
      "Yongqi Li",
      "Wenjie Wang",
      "Fengbin Zhu",
      "Xiaoyu Shen",
      "Wenjie Li",
      "Tat-Seng Chua"
    ],
    "abstract": "Web agents have emerged as a promising direction to automate Web task\ncompletion based on user instructions, significantly enhancing user experience.\nRecently, Web agents have evolved from traditional agents to Large Language\nModels (LLMs)-based Web agents. Despite their success, existing LLM-based Web\nagents overlook the importance of personalized data (e.g., user profiles and\nhistorical Web behaviors) in assisting the understanding of users' personalized\ninstructions and executing customized actions. To overcome the limitation, we\nfirst formulate the task of LLM-empowered personalized Web agents, which\nintegrate personalized data and user instructions to personalize instruction\ncomprehension and action execution. To address the absence of a comprehensive\nevaluation benchmark, we construct a Personalized Web Agent Benchmark\n(PersonalWAB), featuring user instructions, personalized user data, Web\nfunctions, and two evaluation paradigms across three personalized Web tasks.\nMoreover, we propose a Personalized User Memory-enhanced Alignment (PUMA)\nframework to adapt LLMs to the personalized Web agent task. PUMA utilizes a\nmemory bank with a task-specific retrieval strategy to filter relevant\nhistorical Web behaviors. Based on the behaviors, PUMA then aligns LLMs for\npersonalized action execution through fine-tuning and direct preference\noptimization. Extensive experiments validate the superiority of PUMA over\nexisting Web agents on PersonalWAB.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to WWW 2025. The code and data are available on the project\n  website https://hongrucai.github.io/PersonalWAB/",
    "pdf_url": "http://arxiv.org/pdf/2410.17236v2",
    "published_date": "2024-10-22 17:54:45 UTC",
    "updated_date": "2025-03-24 17:51:54 UTC"
  },
  {
    "arxiv_id": "2410.17233v3",
    "title": "ICPL: Few-shot In-context Preference Learning via LLMs",
    "authors": [
      "Chao Yu",
      "Qixin Tan",
      "Hong Lu",
      "Jiaxuan Gao",
      "Xinting Yang",
      "Yu Wang",
      "Yi Wu",
      "Eugene Vinitsky"
    ],
    "abstract": "Preference-based reinforcement learning is an effective way to handle tasks\nwhere rewards are hard to specify but can be exceedingly inefficient as\npreference learning is often tabula rasa. We demonstrate that Large Language\nModels (LLMs) have native preference-learning capabilities that allow them to\nachieve sample-efficient preference learning, addressing this challenge. We\npropose In-Context Preference Learning (ICPL), which uses in-context learning\ncapabilities of LLMs to reduce human query inefficiency. ICPL uses the task\ndescription and basic environment code to create sets of reward functions which\nare iteratively refined by placing human feedback over videos of the resultant\npolicies into the context of an LLM and then requesting better rewards. We\nfirst demonstrate ICPL's effectiveness through a synthetic preference study,\nproviding quantitative evidence that it significantly outperforms baseline\npreference-based methods with much higher performance and orders of magnitude\ngreater efficiency. We observe that these improvements are not solely coming\nfrom LLM grounding in the task but that the quality of the rewards improves\nover time, indicating preference learning capabilities. Additionally, we\nperform a series of real human preference-learning trials and observe that ICPL\nextends beyond synthetic settings and can work effectively with\nhumans-in-the-loop.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17233v3",
    "published_date": "2024-10-22 17:53:34 UTC",
    "updated_date": "2025-04-03 08:27:46 UTC"
  },
  {
    "arxiv_id": "2410.17229v2",
    "title": "Responsibility in a Multi-Value Strategic Setting",
    "authors": [
      "Timothy Parker",
      "Umberto Grandi",
      "Emiliano Lorini"
    ],
    "abstract": "Responsibility is a key notion in multi-agent systems and in creating safe,\nreliable and ethical AI. However, most previous work on responsibility has only\nconsidered responsibility for single outcomes. In this paper we present a model\nfor responsibility attribution in a multi-agent, multi-value setting. We also\nexpand our model to cover responsibility anticipation, demonstrating how\nconsiderations of responsibility can help an agent to select strategies that\nare in line with its values. In particular we show that non-dominated\nregret-minimising strategies reliably minimise an agent's expected degree of\nresponsibility.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17229v2",
    "published_date": "2024-10-22 17:51:13 UTC",
    "updated_date": "2024-11-08 23:12:15 UTC"
  },
  {
    "arxiv_id": "2410.17218v4",
    "title": "Creativity in AI: Progresses and Challenges",
    "authors": [
      "Mete Ismayilzada",
      "Debjit Paul",
      "Antoine Bosselut",
      "Lonneke van der Plas"
    ],
    "abstract": "Creativity is the ability to produce novel, useful, and surprising ideas, and\nhas been widely studied as a crucial aspect of human cognition. Machine\ncreativity on the other hand has been a long-standing challenge. With the rise\nof advanced generative AI, there has been renewed interest and debate regarding\nAI's creative capabilities. Therefore, it is imperative to revisit the state of\ncreativity in AI and identify key progresses and remaining challenges. In this\nwork, we survey leading works studying the creative capabilities of AI systems,\nfocusing on creative problem-solving, linguistic, artistic, and scientific\ncreativity. Our review suggests that while the latest AI models are largely\ncapable of producing linguistically and artistically creative outputs such as\npoems, images, and musical pieces, they struggle with tasks that require\ncreative problem-solving, abstract thinking and compositionality and their\ngenerations suffer from a lack of diversity, originality, long-range\nincoherence and hallucinations. We also discuss key questions concerning\ncopyright and authorship issues with generative models. Furthermore, we\nhighlight the need for a comprehensive evaluation of creativity that is\nprocess-driven and considers several dimensions of creativity. Finally, we\npropose future research directions to improve the creativity of AI outputs,\ndrawing inspiration from cognitive science and psychology.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "minor updates to content + figures",
    "pdf_url": "http://arxiv.org/pdf/2410.17218v4",
    "published_date": "2024-10-22 17:43:39 UTC",
    "updated_date": "2024-12-09 12:45:53 UTC"
  },
  {
    "arxiv_id": "2410.17212v1",
    "title": "Neuroevolution Neural Architecture Search for Evolving RNNs in Stock Return Prediction and Portfolio Trading",
    "authors": [
      "Zimeng Lyu",
      "Amulya Saxena",
      "Rohaan Nadeem",
      "Hao Zhang",
      "Travis Desell"
    ],
    "abstract": "Stock return forecasting is a major component of numerous finance\napplications. Predicted stock returns can be incorporated into portfolio\ntrading algorithms to make informed buy or sell decisions which can optimize\nreturns. In such portfolio trading applications, the predictive performance of\na time series forecasting model is crucial. In this work, we propose the use of\nthe Evolutionary eXploration of Augmenting Memory Models (EXAMM) algorithm to\nprogressively evolve recurrent neural networks (RNNs) for stock return\npredictions. RNNs are evolved independently for each stocks and portfolio\ntrading decisions are made based on the predicted stock returns. The portfolio\nused for testing consists of the 30 companies in the Dow-Jones Index (DJI) with\neach stock have the same weight. Results show that using these evolved RNNs and\na simple daily long-short strategy can generate higher returns than both the\nDJI index and the S&P 500 Index for both 2022 (bear market) and 2023 (bull\nmarket).",
    "categories": [
      "q-fin.PM",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "q-fin.PM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17212v1",
    "published_date": "2024-10-22 17:37:18 UTC",
    "updated_date": "2024-10-22 17:37:18 UTC"
  },
  {
    "arxiv_id": "2410.17210v1",
    "title": "Exploring Possibilities of AI-Powered Legal Assistance in Bangladesh through Large Language Modeling",
    "authors": [
      "Azmine Toushik Wasi",
      "Wahid Faisal",
      "Mst Rafia Islam",
      "Mahathir Mohammad Bappy"
    ],
    "abstract": "Purpose: Bangladesh's legal system struggles with major challenges like\ndelays, complexity, high costs, and millions of unresolved cases, which deter\nmany from pursuing legal action due to lack of knowledge or financial\nconstraints. This research seeks to develop a specialized Large Language Model\n(LLM) to assist in the Bangladeshi legal system. Methods: We created\nUKIL-DB-EN, an English corpus of Bangladeshi legal documents, by collecting and\nscraping data on various legal acts. We fine-tuned the GPT-2 model on this\ndataset to develop GPT2-UKIL-EN, an LLM focused on providing legal assistance\nin English. Results: The model was rigorously evaluated using semantic\nassessments, including case studies supported by expert opinions. The\nevaluation provided promising results, demonstrating the potential for the\nmodel to assist in legal matters within Bangladesh. Conclusion: Our work\nrepresents the first structured effort toward building an AI-based legal\nassistant for Bangladesh. While the results are encouraging, further\nrefinements are necessary to improve the model's accuracy, credibility, and\nsafety. This is a significant step toward creating a legal AI capable of\nserving the needs of a population of 180 million.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "In Review",
    "pdf_url": "http://arxiv.org/pdf/2410.17210v1",
    "published_date": "2024-10-22 17:34:59 UTC",
    "updated_date": "2024-10-22 17:34:59 UTC"
  },
  {
    "arxiv_id": "2410.17196v3",
    "title": "VoiceBench: Benchmarking LLM-Based Voice Assistants",
    "authors": [
      "Yiming Chen",
      "Xianghu Yue",
      "Chen Zhang",
      "Xiaoxue Gao",
      "Robby T. Tan",
      "Haizhou Li"
    ],
    "abstract": "Building on the success of large language models (LLMs), recent advancements\nsuch as GPT-4o have enabled real-time speech interactions through LLM-based\nvoice assistants, offering a significantly improved user experience compared to\ntraditional text-based interactions. However, the absence of benchmarks\ndesigned to evaluate these speech interaction capabilities has hindered\nprogress of LLM-based voice assistants development. Current evaluations focus\nprimarily on automatic speech recognition (ASR) or general knowledge evaluation\nwith clean speeches, neglecting the more intricate, real-world scenarios that\ninvolve diverse speaker characteristics, environmental and content factors. To\naddress this, we introduce VoiceBench, the first benchmark designed to provide\na multi-faceted evaluation of LLM-based voice assistants. VoiceBench also\nincludes both real and synthetic spoken instructions that incorporate the above\nthree key real-world variations. Extensive experiments reveal the limitations\nof current LLM-based voice assistant models and offer valuable insights for\nfuture research and development in this field.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress. Data is available at\n  https://github.com/MatthewCYM/VoiceBench",
    "pdf_url": "http://arxiv.org/pdf/2410.17196v3",
    "published_date": "2024-10-22 17:15:20 UTC",
    "updated_date": "2024-12-11 15:45:21 UTC"
  },
  {
    "arxiv_id": "2410.17195v3",
    "title": "Non-myopic Generation of Language Models for Reasoning and Planning",
    "authors": [
      "Chang Ma",
      "Haiteng Zhao",
      "Junlei Zhang",
      "Junxian He",
      "Lingpeng Kong"
    ],
    "abstract": "Large Language Models have demonstrated remarkable abilities in reasoning and\nplanning by breaking down complex problems into sequential steps. Despite their\nsuccess in various domains like mathematical problem-solving and coding, LLMs\nface challenges in ensuring reliable and optimal planning due to their inherent\nmyopic nature of autoregressive decoding. This paper revisits LLM reasoning\nfrom an optimal-control perspective, proposing a novel method,\nPredictive-Decoding, that leverages Model Predictive Control to enhance\nplanning accuracy. By re-weighting LLM distributions based on foresight\ntrajectories, Predictive-Decoding aims to mitigate early errors and promote\nnon-myopic planning. Our experiments show significant improvements in a wide\nrange of tasks for math, coding, and agents. Furthermore, Predictive-Decoding\ndemonstrates computational efficiency, outperforming search baselines with\nreduced computational resources. This study provides insights into optimizing\nLLM planning capabilities.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17195v3",
    "published_date": "2024-10-22 17:13:38 UTC",
    "updated_date": "2024-10-28 17:28:51 UTC"
  },
  {
    "arxiv_id": "2410.17193v2",
    "title": "Emphasizing Discriminative Features for Dataset Distillation in Complex Scenarios",
    "authors": [
      "Kai Wang",
      "Zekai Li",
      "Zhi-Qi Cheng",
      "Samir Khaki",
      "Ahmad Sajedi",
      "Ramakrishna Vedantam",
      "Konstantinos N Plataniotis",
      "Alexander Hauptmann",
      "Yang You"
    ],
    "abstract": "Dataset distillation has demonstrated strong performance on simple datasets\nlike CIFAR, MNIST, and TinyImageNet but struggles to achieve similar results in\nmore complex scenarios. In this paper, we propose EDF (emphasizes the\ndiscriminative features), a dataset distillation method that enhances key\ndiscriminative regions in synthetic images using Grad-CAM activation maps. Our\napproach is inspired by a key observation: in simple datasets, high-activation\nareas typically occupy most of the image, whereas in complex scenarios, the\nsize of these areas is much smaller. Unlike previous methods that treat all\npixels equally when synthesizing images, EDF uses Grad-CAM activation maps to\nenhance high-activation areas. From a supervision perspective, we downplay\nsupervision signals that have lower losses, as they contain common patterns.\nAdditionally, to help the DD community better explore complex scenarios, we\nbuild the Complex Dataset Distillation (Comp-DD) benchmark by meticulously\nselecting sixteen subsets, eight easy and eight hard, from ImageNet-1K. In\nparticular, EDF consistently outperforms SOTA results in complex scenarios,\nsuch as ImageNet-1K subsets. Hopefully, more researchers will be inspired and\nencouraged to improve the practicality and efficacy of DD. Our code and\nbenchmark will be made public at https://github.com/NUS-HPC-AI-Lab/EDF.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "24 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.17193v2",
    "published_date": "2024-10-22 17:13:19 UTC",
    "updated_date": "2025-03-31 04:10:38 UTC"
  },
  {
    "arxiv_id": "2410.17186v1",
    "title": "DyPNIPP: Predicting Environment Dynamics for RL-based Robust Informative Path Planning",
    "authors": [
      "Srujan Deolasee",
      "Siva Kailas",
      "Wenhao Luo",
      "Katia Sycara",
      "Woojun Kim"
    ],
    "abstract": "Informative path planning (IPP) is an important planning paradigm for various\nreal-world robotic applications such as environment monitoring. IPP involves\nplanning a path that can learn an accurate belief of the quantity of interest,\nwhile adhering to planning constraints. Traditional IPP methods typically\nrequire high computation time during execution, giving rise to reinforcement\nlearning (RL) based IPP methods. However, the existing RL-based methods do not\nconsider spatio-temporal environments which involve their own challenges due to\nvariations in environment characteristics. In this paper, we propose DyPNIPP, a\nrobust RL-based IPP framework, designed to operate effectively across\nspatio-temporal environments with varying dynamics. To achieve this, DyPNIPP\nincorporates domain randomization to train the agent across diverse\nenvironments and introduces a dynamics prediction model to capture and adapt\nthe agent actions to specific environment dynamics. Our extensive experiments\nin a wildfire environment demonstrate that DyPNIPP outperforms existing\nRL-based IPP algorithms by significantly improving robustness and performing\nacross diverse environment conditions.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 4 figures, submitted to IEEE RA-L",
    "pdf_url": "http://arxiv.org/pdf/2410.17186v1",
    "published_date": "2024-10-22 17:07:26 UTC",
    "updated_date": "2024-10-22 17:07:26 UTC"
  },
  {
    "arxiv_id": "2410.17172v1",
    "title": "KANICE: Kolmogorov-Arnold Networks with Interactive Convolutional Elements",
    "authors": [
      "Md Meftahul Ferdaus",
      "Mahdi Abdelguerfi",
      "Elias Ioup",
      "David Dobson",
      "Kendall N. Niles",
      "Ken Pathak",
      "Steven Sloan"
    ],
    "abstract": "We introduce KANICE (Kolmogorov-Arnold Networks with Interactive\nConvolutional Elements), a novel neural architecture that combines\nConvolutional Neural Networks (CNNs) with Kolmogorov-Arnold Network (KAN)\nprinciples. KANICE integrates Interactive Convolutional Blocks (ICBs) and KAN\nlinear layers into a CNN framework. This leverages KANs' universal\napproximation capabilities and ICBs' adaptive feature learning. KANICE captures\ncomplex, non-linear data relationships while enabling dynamic,\ncontext-dependent feature extraction based on the Kolmogorov-Arnold\nrepresentation theorem. We evaluated KANICE on four datasets: MNIST,\nFashion-MNIST, EMNIST, and SVHN, comparing it against standard CNNs, CNN-KAN\nhybrids, and ICB variants. KANICE consistently outperformed baseline models,\nachieving 99.35% accuracy on MNIST and 90.05% on the SVHN dataset.\n  Furthermore, we introduce KANICE-mini, a compact variant designed for\nefficiency. A comprehensive ablation study demonstrates that KANICE-mini\nachieves comparable performance to KANICE with significantly fewer parameters.\nKANICE-mini reached 90.00% accuracy on SVHN with 2,337,828 parameters, compared\nto KANICE's 25,432,000. This study highlights the potential of KAN-based\narchitectures in balancing performance and computational efficiency in image\nclassification tasks. Our work contributes to research in adaptive neural\nnetworks, integrates mathematical theorems into deep learning architectures,\nand explores the trade-offs between model complexity and performance, advancing\ncomputer vision and pattern recognition. The source code for this paper is\npublicly accessible through our GitHub repository\n(https://github.com/m-ferdaus/kanice).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17172v1",
    "published_date": "2024-10-22 16:50:34 UTC",
    "updated_date": "2024-10-22 16:50:34 UTC"
  },
  {
    "arxiv_id": "2410.17173v1",
    "title": "Reinforcement learning on structure-conditioned categorical diffusion for protein inverse folding",
    "authors": [
      "Yasha Ektefaie",
      "Olivia Viessmann",
      "Siddharth Narayanan",
      "Drew Dresser",
      "J. Mark Kim",
      "Armen Mkrtchyan"
    ],
    "abstract": "Protein inverse folding-that is, predicting an amino acid sequence that will\nfold into the desired 3D structure-is an important problem for structure-based\nprotein design. Machine learning based methods for inverse folding typically\nuse recovery of the original sequence as the optimization objective. However,\ninverse folding is a one-to-many problem where several sequences can fold to\nthe same structure. Moreover, for many practical applications, it is often\ndesirable to have multiple, diverse sequences that fold into the target\nstructure since it allows for more candidate sequences for downstream\noptimizations. Here, we demonstrate that although recent inverse folding\nmethods show increased sequence recovery, their \"foldable diversity\"-i.e. their\nability to generate multiple non-similar sequences that fold into the\nstructures consistent with the target-does not increase. To address this, we\npresent RL-DIF, a categorical diffusion model for inverse folding that is\npre-trained on sequence recovery and tuned via reinforcement learning on\nstructural consistency. We find that RL-DIF achieves comparable sequence\nrecovery and structural consistency to benchmark models but shows greater\nfoldable diversity: experiments show RL-DIF can achieve an foldable diversity\nof 29% on CATH 4.2, compared to 23% from models trained on the same dataset.\nThe PyTorch model weights and sampling code are available on GitHub.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17173v1",
    "published_date": "2024-10-22 16:50:34 UTC",
    "updated_date": "2024-10-22 16:50:34 UTC"
  },
  {
    "arxiv_id": "2410.17160v1",
    "title": "Layered LA-MAPF: a decomposition of large agent MAPF instance to accelerate solving without compromising solvability",
    "authors": [
      "Zhuo Yao"
    ],
    "abstract": "Multi-Agent Path Finding (MAPF) has been widely studied in recent years.\nHowever, most existing MAPF algorithms assume that an agent occupies only a\nsingle grid in a grid-based map. This assumption limits their applicability in\nmany real-world domains where agents have geometric shapes, rather than being\npoint-like. Such agents, which can occupy multiple cells simultaneously, are\nreferred to as ``large'' agents. When considering the shape and size of agents\nin MAPF, the computational complexity increases significantly as the number of\nagents grows, primarily due to the increased overhead in conflict detection\nbetween geometric agents. In this paper, we propose two types of subproblems\nfor the LA-MAPF (Large-Agent MAPF) problem: \\textbf{cluster} (which has no\nconstraints on the order of solution) and \\textbf{level} (which imposes\nconstraints on the solution order). We introduce \\textbf{Layered LA-MAPF}, a\nmethod that decomposes a MAPF instance involving geometric agents into\nclusters, and then further decomposes each cluster into levels. This approach\naims to reduce time complexity when solving LA-MAPF problems. Our results\ndemonstrate the performance of our method as the number of agents increases\nacross various maps, and how it accelerates LA-MAPF methods, such as LA-CBS and\nLA-LaCAM. Experiments show that our LA-MAPF method with instance decomposition\n\\textbf{halves the time cost (reducing from an average of 40s to 20s) and\ntriples the success rate (from an average of 0.27 to 0.80)} in finding a\nsolution within 60 seconds. To facilitate further research, we have made the\nsource code for Layered LA-MAPF publicly available at\n\\url{https://github.com/JoeYao-bit/LayeredMAPF/algorithm/LA-MAPF}.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17160v1",
    "published_date": "2024-10-22 16:34:24 UTC",
    "updated_date": "2024-10-22 16:34:24 UTC"
  },
  {
    "arxiv_id": "2410.17145v1",
    "title": "Can General-Purpose Large Language Models Generalize to English-Thai Machine Translation ?",
    "authors": [
      "Jirat Chiaranaipanich",
      "Naiyarat Hanmatheekuna",
      "Jitkapat Sawatphol",
      "Krittamate Tiankanon",
      "Jiramet Kinchagawat",
      "Amrest Chinkamol",
      "Parinthapat Pengpun",
      "Piyalitt Ittichaiwong",
      "Peerat Limkonchotiwat"
    ],
    "abstract": "Large language models (LLMs) perform well on common tasks but struggle with\ngeneralization in low-resource and low-computation settings. We examine this\nlimitation by testing various LLMs and specialized translation models on\nEnglish-Thai machine translation and code-switching datasets. Our findings\nreveal that under more strict computational constraints, such as 4-bit\nquantization, LLMs fail to translate effectively. In contrast, specialized\nmodels, with comparable or lower computational requirements, consistently\noutperform LLMs. This underscores the importance of specialized models for\nmaintaining performance under resource constraints.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in GenBench EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.17145v1",
    "published_date": "2024-10-22 16:26:03 UTC",
    "updated_date": "2024-10-22 16:26:03 UTC"
  },
  {
    "arxiv_id": "2410.17141v4",
    "title": "Towards Automated Penetration Testing: Introducing LLM Benchmark, Analysis, and Improvements",
    "authors": [
      "Isamu Isozaki",
      "Manil Shrestha",
      "Rick Console",
      "Edward Kim"
    ],
    "abstract": "Hacking poses a significant threat to cybersecurity, inflicting billions of\ndollars in damages annually. To mitigate these risks, ethical hacking, or\npenetration testing, is employed to identify vulnerabilities in systems and\nnetworks. Recent advancements in large language models (LLMs) have shown\npotential across various domains, including cybersecurity. However, there is\ncurrently no comprehensive, open, automated, end-to-end penetration testing\nbenchmark to drive progress and evaluate the capabilities of these models in\nsecurity contexts. This paper introduces a novel open benchmark for LLM-based\nautomated penetration testing, addressing this critical gap. We first evaluate\nthe performance of LLMs, including GPT-4o and LLama 3.1-405B, using the\nstate-of-the-art PentestGPT tool. Our findings reveal that while LLama 3.1\ndemonstrates an edge over GPT-4o, both models currently fall short of\nperforming end-to-end penetration testing even with some minimal human\nassistance. Next, we advance the state-of-the-art and present ablation studies\nthat provide insights into improving the PentestGPT tool. Our research\nilluminates the challenges LLMs face in each aspect of Pentesting, e.g.\nenumeration, exploitation, and privilege escalation. This work contributes to\nthe growing body of knowledge on AI-assisted cybersecurity and lays the\nfoundation for future research in automated penetration testing using large\nlanguage models.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Main Paper 1-9 pages, Supplementary Materials: 10-17, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.17141v4",
    "published_date": "2024-10-22 16:18:41 UTC",
    "updated_date": "2025-02-21 17:53:09 UTC"
  },
  {
    "arxiv_id": "2410.17139v2",
    "title": "Trustworthy XAI and Application",
    "authors": [
      "MD Abdullah Al Nasim",
      "A. S. M Anas Ferdous",
      "Abdur Rashid",
      "Fatema Tuj Johura Soshi",
      "Parag Biswas",
      "Angona Biswas",
      "Kishor Datta Gupta"
    ],
    "abstract": "Artificial Intelligence (AI) is an important part of our everyday lives. We\nuse it in self-driving cars and smartphone assistants. People often call it a\n\"black box\" because its complex systems, especially deep neural networks, are\nhard to understand. This complexity raises concerns about accountability, bias,\nand fairness, even though AI can be quite accurate. Explainable Artificial\nIntelligence (XAI) is important for building trust. It helps ensure that AI\nsystems work reliably and ethically. This article looks at XAI and its three\nmain parts: transparency, explainability, and trustworthiness. We will discuss\nwhy these components matter in real-life situations. We will also review recent\nstudies that show how XAI is used in different fields. Ultimately, gaining\ntrust in AI systems is crucial for their successful use in society.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17139v2",
    "published_date": "2024-10-22 16:10:10 UTC",
    "updated_date": "2025-04-16 18:07:32 UTC"
  },
  {
    "arxiv_id": "2410.17126v1",
    "title": "Exploring RL-based LLM Training for Formal Language Tasks with Programmed Rewards",
    "authors": [
      "Alexander G. Padula",
      "Dennis J. N. J. Soemers"
    ],
    "abstract": "Proximal Policy Optimization (PPO) is commonly used in Reinforcement Learning\nfrom Human Feedback to align large language models (LLMs) with downstream\ntasks. This paper investigates the feasibility of using PPO for direct\nreinforcement learning (RL) from explicitly programmed reward signals, as\nopposed to indirect learning from human feedback via an intermediary reward\nmodel. We focus on tasks expressed through formal languages, such as\nmathematics and programming, where explicit reward functions can be programmed\nto automatically assess the quality of generated outputs. We apply this\napproach to a sentiment alignment task, a simple arithmetic task, and a more\ncomplex game synthesis task. The sentiment alignment task replicates prior\nresearch and serves to validate our experimental setup. Our results show that\npure RL-based training for the two formal language tasks is challenging, with\nsuccess being limited even for the simple arithmetic task. We propose a novel\nbatch-entropy regularization term to aid exploration, although training is not\nyet entirely stable. Our findings suggest that direct RL training of LLMs may\nbe more suitable for relatively minor changes, such as alignment, than for\nlearning new tasks altogether, even if an informative reward signal can be\nexpressed programmatically.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at BNAIC 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.17126v1",
    "published_date": "2024-10-22 15:59:58 UTC",
    "updated_date": "2024-10-22 15:59:58 UTC"
  },
  {
    "arxiv_id": "2410.17124v1",
    "title": "Automated neuroradiological support systems for multiple cerebrovascular disease markers -- A systematic review and meta-analysis",
    "authors": [
      "Jesse Phitidis",
      "Alison Q. O'Neil",
      "William N. Whiteley",
      "Beatrice Alex",
      "Joanna M. Wardlaw",
      "Miguel O. Bernabeu",
      "Maria Valdés Hernández"
    ],
    "abstract": "Cerebrovascular diseases (CVD) can lead to stroke and dementia. Stroke is the\nsecond leading cause of death world wide and dementia incidence is increasing\nby the year. There are several markers of CVD that are visible on brain\nimaging, including: white matter hyperintensities (WMH), acute and chronic\nischaemic stroke lesions (ISL), lacunes, enlarged perivascular spaces (PVS),\nacute and chronic haemorrhagic lesions, and cerebral microbleeds (CMB). Brain\natrophy also occurs in CVD. These markers are important for patient management\nand intervention, since they indicate elevated risk of future stroke and\ndementia. We systematically reviewed automated systems designed to support\nradiologists reporting on these CVD imaging findings. We considered\ncommercially available software and research publications which identify at\nleast two CVD markers. In total, we included 29 commercial products and 13\nresearch publications. Two distinct types of commercial support system were\navailable: those which identify acute stroke lesions (haemorrhagic and\nischaemic) from computed tomography (CT) scans, mainly for the purpose of\npatient triage; and those which measure WMH and atrophy regionally and\nlongitudinally. In research, WMH and ISL were the markers most frequently\nanalysed together, from magnetic resonance imaging (MRI) scans; lacunes and PVS\nwere each targeted only twice and CMB only once. For stroke, commercially\navailable systems largely support the emergency setting, whilst research\nsystems consider also follow-up and routine scans. The systems to quantify WMH\nand atrophy are focused on neurodegenerative disease support, where these CVD\nmarkers are also of significance. There are currently no openly validated\nsystems, commercially, or in research, performing a comprehensive joint\nanalysis of all CVD markers (WMH, ISL, lacunes, PVS, haemorrhagic lesions, CMB,\nand atrophy).",
    "categories": [
      "physics.med-ph",
      "cs.AI",
      "A.1; J.3"
    ],
    "primary_category": "physics.med-ph",
    "comment": "62 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.17124v1",
    "published_date": "2024-10-22 15:59:07 UTC",
    "updated_date": "2024-10-22 15:59:07 UTC"
  },
  {
    "arxiv_id": "2410.17111v1",
    "title": "Permutation Picture of Graph Combinatorial Optimization Problems",
    "authors": [
      "Yimeng Min"
    ],
    "abstract": "This paper proposes a framework that formulates a wide range of graph\ncombinatorial optimization problems using permutation-based representations.\nThese problems include the travelling salesman problem, maximum independent\nset, maximum cut, and various other related problems. This work potentially\nopens up new avenues for algorithm design in neural combinatorial optimization,\nbridging the gap between discrete and continuous optimization techniques.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.17111v1",
    "published_date": "2024-10-22 15:36:04 UTC",
    "updated_date": "2024-10-22 15:36:04 UTC"
  },
  {
    "arxiv_id": "2410.17088v2",
    "title": "Science Out of Its Ivory Tower: Improving Accessibility with Reinforcement Learning",
    "authors": [
      "Haining Wang",
      "Jason Clark",
      "Hannah McKelvey",
      "Leila Sterman",
      "Zheng Gao",
      "Zuoyu Tian",
      "Sandra Kübler",
      "Xiaozhong Liu"
    ],
    "abstract": "A vast amount of scholarly work is published daily, yet much of it remains\ninaccessible to the general public due to dense jargon and complex language. To\naddress this challenge in science communication, we introduce a reinforcement\nlearning framework that fine-tunes a language model to rewrite scholarly\nabstracts into more comprehensible versions. Guided by a carefully balanced\ncombination of word- and sentence-level accessibility rewards, our language\nmodel effectively substitutes technical terms with more accessible\nalternatives, a task which models supervised fine-tuned or guided by\nconventional readability measures struggle to accomplish. Our best model\nadjusts the readability level of scholarly abstracts by approximately six U.S.\ngrade levels -- in other words, from a postgraduate to a high school level.\nThis translates to roughly a 90% relative boost over the supervised fine-tuning\nbaseline, all while maintaining factual accuracy and high-quality language. An\nin-depth analysis of our approach shows that balanced rewards lead to\nsystematic modifications in the base model, likely contributing to smoother\noptimization and superior performance. We envision this work as a step toward\nbridging the gap between scholarly research and the general public,\nparticularly younger readers and those without a college degree.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17088v2",
    "published_date": "2024-10-22 15:14:54 UTC",
    "updated_date": "2025-04-16 16:00:53 UTC"
  },
  {
    "arxiv_id": "2410.17050v1",
    "title": "UnStar: Unlearning with Self-Taught Anti-Sample Reasoning for LLMs",
    "authors": [
      "Yash Sinha",
      "Murari Mandal",
      "Mohan Kankanhalli"
    ],
    "abstract": "The key components of machine learning are data samples for training, model\nfor learning patterns, and loss function for optimizing accuracy. Analogously,\nunlearning can potentially be achieved through anti-data samples (or\nanti-samples), unlearning method, and reversed loss function. While prior\nresearch has explored unlearning methods and reversed loss functions, the\npotential of anti-samples remains largely untapped. In this paper, we introduce\nUnSTAR: Unlearning with Self-Taught Anti-Sample Reasoning for large language\nmodels (LLMs). Our contributions are threefold; first, we propose a novel\nconcept of anti-sample-induced unlearning; second, we generate anti-samples by\nleveraging misleading rationales, which help reverse learned associations and\naccelerate the unlearning process; and third, we enable fine-grained targeted\nunlearning, allowing for the selective removal of specific associations without\nimpacting related knowledge - something not achievable by previous works.\nResults demonstrate that anti-samples offer an efficient, targeted unlearning\nstrategy for LLMs, opening new avenues for privacy-preserving machine learning\nand model modification.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17050v1",
    "published_date": "2024-10-22 14:30:03 UTC",
    "updated_date": "2024-10-22 14:30:03 UTC"
  },
  {
    "arxiv_id": "2410.17049v1",
    "title": "A Comparison of Baseline Models and a Transformer Network for SOC Prediction in Lithium-Ion Batteries",
    "authors": [
      "Hadeel Aboueidah",
      "Abdulrahman Altahhan"
    ],
    "abstract": "Accurately predicting the state of charge of Lithium-ion batteries is\nessential to the performance of battery management systems of electric\nvehicles. One of the main reasons for the slow global adoption of electric cars\nis driving range anxiety. The ability of a battery management system to\naccurately estimate the state of charge can help alleviate this problem. In\nthis paper, a comparison between data-driven state-of-charge estimation methods\nis conducted. The paper compares different neural network-based models and\ncommon regression models for SOC estimation. These models include several\nablated transformer networks, a neural network, a lasso regression model, a\nlinear regression model and a decision tree. Results of various experiments\nconducted on data obtained from natural driving cycles of the BMW i3 battery\nshow that the decision tree outperformed all other models including the more\ncomplex transformer network with self-attention and positional encoding.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17049v1",
    "published_date": "2024-10-22 14:27:43 UTC",
    "updated_date": "2024-10-22 14:27:43 UTC"
  },
  {
    "arxiv_id": "2410.17042v1",
    "title": "Deep Memory Search: A Metaheuristic Approach for Optimizing Heuristic Search",
    "authors": [
      "Abdel-Rahman Hedar",
      "Alaa E. Abdel-Hakim",
      "Wael Deabes",
      "Youseef Alotaibi",
      "Kheir Eddine Bouazza"
    ],
    "abstract": "Metaheuristic search methods have proven to be essential tools for tackling\ncomplex optimization challenges, but their full potential is often constrained\nby conventional algorithmic frameworks. In this paper, we introduce a novel\napproach called Deep Heuristic Search (DHS), which models metaheuristic search\nas a memory-driven process. DHS employs multiple search layers and memory-based\nexploration-exploitation mechanisms to navigate large, dynamic search spaces.\nBy utilizing model-free memory representations, DHS enhances the ability to\ntraverse temporal trajectories without relying on probabilistic transition\nmodels. The proposed method demonstrates significant improvements in search\nefficiency and performance across a range of heuristic optimization problems.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "I.2.6"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.17042v1",
    "published_date": "2024-10-22 14:16:49 UTC",
    "updated_date": "2024-10-22 14:16:49 UTC"
  },
  {
    "arxiv_id": "2410.19855v1",
    "title": "Personalized Recommendation Systems using Multimodal, Autonomous, Multi Agent Systems",
    "authors": [
      "Param Thakkar",
      "Anushka Yadav"
    ],
    "abstract": "This paper describes a highly developed personalised recommendation system\nusing multimodal, autonomous, multi-agent systems. The system focuses on the\nincorporation of futuristic AI tech and LLMs like Gemini-1.5- pro and LLaMA-70B\nto improve customer service experiences especially within e-commerce. Our\napproach uses multi agent, multimodal systems to provide best possible\nrecommendations to its users. The system is made up of three agents as a whole.\nThe first agent recommends products appropriate for answering the given\nquestion, while the second asks follow-up questions based on images that belong\nto these recommended products and is followed up with an autonomous search by\nthe third agent. It also features a real-time data fetch, user\npreferences-based recommendations and is adaptive learning. During complicated\nqueries the application processes with Symphony, and uses the Groq API to\nanswer quickly with low response times. It uses a multimodal way to utilize\ntext and images comprehensively, so as to optimize product recommendation and\ncustomer interaction.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.19855v1",
    "published_date": "2024-10-22 14:11:26 UTC",
    "updated_date": "2024-10-22 14:11:26 UTC"
  },
  {
    "arxiv_id": "2410.17032v1",
    "title": "Insights on Disagreement Patterns in Multimodal Safety Perception across Diverse Rater Groups",
    "authors": [
      "Charvi Rastogi",
      "Tian Huey Teh",
      "Pushkar Mishra",
      "Roma Patel",
      "Zoe Ashwood",
      "Aida Mostafazadeh Davani",
      "Mark Diaz",
      "Michela Paganini",
      "Alicia Parrish",
      "Ding Wang",
      "Vinodkumar Prabhakaran",
      "Lora Aroyo",
      "Verena Rieser"
    ],
    "abstract": "AI systems crucially rely on human ratings, but these ratings are often\naggregated, obscuring the inherent diversity of perspectives in real-world\nphenomenon. This is particularly concerning when evaluating the safety of\ngenerative AI, where perceptions and associated harms can vary significantly\nacross socio-cultural contexts. While recent research has studied the impact of\ndemographic differences on annotating text, there is limited understanding of\nhow these subjective variations affect multimodal safety in generative AI. To\naddress this, we conduct a large-scale study employing highly-parallel safety\nratings of about 1000 text-to-image (T2I) generations from a demographically\ndiverse rater pool of 630 raters balanced across 30 intersectional groups\nacross age, gender, and ethnicity. Our study shows that (1) there are\nsignificant differences across demographic groups (including intersectional\ngroups) on how severe they assess the harm to be, and that these differences\nvary across different types of safety violations, (2) the diverse rater pool\ncaptures annotation patterns that are substantially different from expert\nraters trained on specific set of safety policies, and (3) the differences we\nobserve in T2I safety are distinct from previously documented group level\ndifferences in text-based safety tasks. To further understand these varying\nperspectives, we conduct a qualitative analysis of the open-ended explanations\nprovided by raters. This analysis reveals core differences into the reasons why\ndifferent groups perceive harms in T2I generations. Our findings underscore the\ncritical need for incorporating diverse perspectives into safety evaluation of\ngenerative AI ensuring these systems are truly inclusive and reflect the values\nof all users.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.17032v1",
    "published_date": "2024-10-22 13:59:21 UTC",
    "updated_date": "2024-10-22 13:59:21 UTC"
  },
  {
    "arxiv_id": "2410.17031v2",
    "title": "GeoCode-GPT: A Large Language Model for Geospatial Code Generation Tasks",
    "authors": [
      "Shuyang Hou",
      "Zhangxiao Shen",
      "Anqi Zhao",
      "Jianyuan Liang",
      "Zhipeng Gui",
      "Xuefeng Guan",
      "Rui Li",
      "Huayi Wu"
    ],
    "abstract": "The increasing demand for spatiotemporal data and modeling tasks in\ngeosciences has made geospatial code generation technology a critical factor in\nenhancing productivity. Although large language models (LLMs) have demonstrated\npotential in code generation tasks, they often encounter issues such as refusal\nto code or hallucination in geospatial code generation due to a lack of\ndomain-specific knowledge and code corpora. To address these challenges, this\npaper presents and open-sources the GeoCode-PT and GeoCode-SFT corpora, along\nwith the GeoCode-Eval evaluation dataset. Additionally, by leveraging QLoRA and\nLoRA for pretraining and fine-tuning, we introduce GeoCode-GPT-7B, the first\nLLM focused on geospatial code generation, fine-tuned from Code Llama-7B.\nFurthermore, we establish a comprehensive geospatial code evaluation framework,\nincorporating option matching, expert validation, and prompt engineering\nscoring for LLMs, and systematically evaluate GeoCode-GPT-7B using the\nGeoCode-Eval dataset. Experimental results show that GeoCode-GPT outperforms\nother models in multiple-choice accuracy by 9.1% to 32.1%, in code\nsummarization ability by 1.7% to 25.4%, and in code generation capability by\n1.2% to 25.1%. This paper provides a solution and empirical validation for\nenhancing LLMs' performance in geospatial code generation, extends the\nboundaries of domain-specific model applications, and offers valuable insights\ninto unlocking their potential in geospatial code generation.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17031v2",
    "published_date": "2024-10-22 13:57:55 UTC",
    "updated_date": "2024-10-23 13:52:51 UTC"
  },
  {
    "arxiv_id": "2410.17028v1",
    "title": "Can a Machine Distinguish High and Low Amount of Social Creak in Speech?",
    "authors": [
      "Anne-Maria Laukkanen",
      "Sudarsana Reddy Kadiri",
      "Shrikanth Narayanan",
      "Paavo Alku"
    ],
    "abstract": "Objectives: ncreased prevalence of social creak particularly among female\nspeakers has been reported in several studies. The study of social creak has\nbeen previously conducted by combining perceptual evaluation of speech with\nconventional acoustical parameters such as the harmonic-to-noise ratio and\ncepstral peak prominence. In the current study, machine learning (ML) was used\nto automatically distinguish speech of low amount of social creak from speech\nof high amount of social creak.\n  Methods: The amount of creak in continuous speech samples produced in Finnish\nby 90 female speakers was first perceptually assessed by two voice specialists.\nBased on their assessments, the speech samples were divided into two categories\n(low $vs$. high amount of creak). Using the speech signals and their creak\nlabels, seven different ML models were trained. Three spectral representations\nwere used as feature for each model.\n  Results: The results show that the best performance (accuracy of 71.1\\%) was\nobtained by the following two systems: an Adaboost classifier using the\nmel-spectrogram feature and a decision tree classifier using the mel-frequency\ncepstral coefficient feature.\n  Conclusions: The study of social creak is becoming increasingly popular in\nsociolinguistic and vocological research. The conventional human perceptual\nassessment of the amount of creak is laborious and therefore ML technology\ncould be used to assist researchers studying social creak. The classification\nsystems reported in this study could be considered as baselines in future\nML-based studies on social creak.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted in Journal of Voice",
    "pdf_url": "http://arxiv.org/pdf/2410.17028v1",
    "published_date": "2024-10-22 13:52:51 UTC",
    "updated_date": "2024-10-22 13:52:51 UTC"
  },
  {
    "arxiv_id": "2410.17005v1",
    "title": "Hybrid Generative AI for De Novo Design of Co-Crystals with Enhanced Tabletability",
    "authors": [
      "Nina Gubina",
      "Andrei Dmitrenko",
      "Gleb Solovev",
      "Lyubov Yamshchikova",
      "Oleg Petrov",
      "Ivan Lebedev",
      "Nikita Serov",
      "Grigorii Kirgizov",
      "Nikolay Nikitin",
      "Vladimir Vinogradov"
    ],
    "abstract": "Co-crystallization is an accessible way to control physicochemical\ncharacteristics of organic crystals, which finds many biomedical applications.\nIn this work, we present Generative Method for Co-crystal Design (GEMCODE), a\nnovel pipeline for automated co-crystal screening based on the hybridization of\ndeep generative models and evolutionary optimization for broader exploration of\nthe target chemical space. GEMCODE enables fast de novo co-crystal design with\ntarget tabletability profiles, which is crucial for the development of\npharmaceuticals. With a series of experimental studies highlighting validation\nand discovery cases, we show that GEMCODE is effective even under realistic\ncomputational constraints. Furthermore, we explore the potential of language\nmodels in generating co-crystals. Finally, we present numerous previously\nunknown co-crystals predicted by GEMCODE and discuss its potential in\naccelerating drug development.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at 38th Conference on Neural Information Processing Systems\n  (NeurIPS)",
    "pdf_url": "http://arxiv.org/pdf/2410.17005v1",
    "published_date": "2024-10-22 13:25:28 UTC",
    "updated_date": "2024-10-22 13:25:28 UTC"
  },
  {
    "arxiv_id": "2410.16991v1",
    "title": "An Eye for an AI: Evaluating GPT-4o's Visual Perception Skills and Geometric Reasoning Skills Using Computer Graphics Questions",
    "authors": [
      "Tony Haoran Feng",
      "Paul Denny",
      "Burkhard C. Wünsche",
      "Andrew Luxton-Reilly",
      "Jacqueline Whalley"
    ],
    "abstract": "CG (Computer Graphics) is a popular field of CS (Computer Science), but many\nstudents find this topic difficult due to it requiring a large number of\nskills, such as mathematics, programming, geometric reasoning, and creativity.\nOver the past few years, researchers have investigated ways to harness the\npower of GenAI (Generative Artificial Intelligence) to improve teaching. In CS,\nmuch of the research has focused on introductory computing. A recent study\nevaluating the performance of an LLM (Large Language Model), GPT-4 (text-only),\non CG questions, indicated poor performance and reliance on detailed\ndescriptions of image content, which often required considerable insight from\nthe user to return reasonable results. So far, no studies have investigated the\nabilities of LMMs (Large Multimodal Models), or multimodal LLMs, to solve CG\nquestions and how these abilities can be used to improve teaching.\n  In this study, we construct two datasets of CG questions requiring varying\ndegrees of visual perception skills and geometric reasoning skills, and\nevaluate the current state-of-the-art LMM, GPT-4o, on the two datasets. We find\nthat although GPT-4o exhibits great potential in solving questions with visual\ninformation independently, major limitations still exist to the accuracy and\nquality of the generated results. We propose several novel approaches for CG\neducators to incorporate GenAI into CG teaching despite these limitations. We\nhope that our guidelines further encourage learning and engagement in CG\nclassrooms.",
    "categories": [
      "cs.AI",
      "cs.GR",
      "I.2.7; I.3.0; K.3.2"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 8 figures, 1 table, to be published in SIGGRAPH Asia 2024\n  Educator's Forum",
    "pdf_url": "http://arxiv.org/pdf/2410.16991v1",
    "published_date": "2024-10-22 13:12:47 UTC",
    "updated_date": "2024-10-22 13:12:47 UTC"
  },
  {
    "arxiv_id": "2410.16983v1",
    "title": "Order Matters: Exploring Order Sensitivity in Multimodal Large Language Models",
    "authors": [
      "Zhijie Tan",
      "Xu Chu",
      "Weiping Li",
      "Tong Mo"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) utilize multimodal contexts\nconsisting of text, images, or videos to solve various multimodal tasks.\nHowever, we find that changing the order of multimodal input can cause the\nmodel's performance to fluctuate between advanced performance and random\nguessing. This phenomenon exists in both single-modality (text-only or\nimage-only) and mixed-modality (image-text-pair) contexts. Furthermore, we\ndemonstrate that popular MLLMs pay special attention to certain multimodal\ncontext positions, particularly the beginning and end. Leveraging this special\nattention, we place key video frames and important image/text content in\nspecial positions within the context and submit them to the MLLM for inference.\nThis method results in average performance gains of 14.7% for video-caption\nmatching and 17.8% for visual question answering tasks. Additionally, we\npropose a new metric, Position-Invariant Accuracy (PIA), to address order bias\nin MLLM evaluation. Our research findings contribute to a better understanding\nof Multi-Modal In-Context Learning (MMICL) and provide practical strategies for\nenhancing MLLM performance without increasing computational costs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16983v1",
    "published_date": "2024-10-22 13:05:11 UTC",
    "updated_date": "2024-10-22 13:05:11 UTC"
  },
  {
    "arxiv_id": "2410.18142v2",
    "title": "Analyzing Nobel Prize Literature with Large Language Models",
    "authors": [
      "Zhenyuan Yang",
      "Zhengliang Liu",
      "Jing Zhang",
      "Cen Lu",
      "Jiaxin Tai",
      "Tianyang Zhong",
      "Yiwei Li",
      "Siyan Zhao",
      "Teng Yao",
      "Qing Liu",
      "Jinlin Yang",
      "Qixin Liu",
      "Zhaowei Li",
      "Kexin Wang",
      "Longjun Ma",
      "Dajiang Zhu",
      "Yudan Ren",
      "Bao Ge",
      "Wei Zhang",
      "Ning Qiang",
      "Tuo Zhang",
      "Tianming Liu"
    ],
    "abstract": "This study examines the capabilities of advanced Large Language Models\n(LLMs), particularly the o1 model, in the context of literary analysis. The\noutputs of these models are compared directly to those produced by\ngraduate-level human participants. By focusing on two Nobel Prize-winning short\nstories, 'Nine Chapters' by Han Kang, the 2024 laureate, and 'Friendship' by\nJon Fosse, the 2023 laureate, the research explores the extent to which AI can\nengage with complex literary elements such as thematic analysis,\nintertextuality, cultural and historical contexts, linguistic and structural\ninnovations, and character development. Given the Nobel Prize's prestige and\nits emphasis on cultural, historical, and linguistic richness, applying LLMs to\nthese works provides a deeper understanding of both human and AI approaches to\ninterpretation. The study uses qualitative and quantitative evaluations of\ncoherence, creativity, and fidelity to the text, revealing the strengths and\nlimitations of AI in tasks typically reserved for human expertise. While LLMs\ndemonstrate strong analytical capabilities, particularly in structured tasks,\nthey often fall short in emotional nuance and coherence, areas where human\ninterpretation excels. This research underscores the potential for human-AI\ncollaboration in the humanities, opening new opportunities in literary studies\nand beyond.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18142v2",
    "published_date": "2024-10-22 13:03:28 UTC",
    "updated_date": "2024-12-03 04:19:36 UTC"
  },
  {
    "arxiv_id": "2410.16973v3",
    "title": "Learning Mathematical Rules with Large Language Models",
    "authors": [
      "Antoine Gorceix",
      "Bastien Le Chenadec",
      "Ahmad Rammal",
      "Nelson Vadori",
      "Manuela Veloso"
    ],
    "abstract": "In this paper, we study the ability of large language models to learn\nspecific mathematical rules such as distributivity or simplifying equations. We\npresent an empirical analysis of their ability to generalize these rules, as\nwell as to reuse them in the context of word problems. For this purpose, we\nprovide a rigorous methodology to build synthetic data incorporating such\nrules, and perform fine-tuning of large language models on such data. Our\nexperiments show that our model can learn and generalize these rules to some\nextent, as well as suitably reuse them in the context of word problems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS'24 MATH-AI, the 4th Workshop on Mathematical Reasoning and AI",
    "pdf_url": "http://arxiv.org/pdf/2410.16973v3",
    "published_date": "2024-10-22 12:51:51 UTC",
    "updated_date": "2024-10-25 13:28:52 UTC"
  },
  {
    "arxiv_id": "2410.16950v1",
    "title": "Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In",
    "authors": [
      "Itay Nakash",
      "George Kour",
      "Guy Uziel",
      "Ateret Anaby-Tavor"
    ],
    "abstract": "Following the advancement of large language models (LLMs), the development of\nLLM-based autonomous agents has become increasingly prevalent. As a result, the\nneed to understand the security vulnerabilities of these agents has become a\ncritical task. We examine how ReAct agents can be exploited using a\nstraightforward yet effective method we refer to as the foot-in-the-door\nattack. Our experiments show that indirect prompt injection attacks, prompted\nby harmless and unrelated requests (such as basic calculations) can\nsignificantly increase the likelihood of the agent performing subsequent\nmalicious actions. Our results show that once a ReAct agents thought includes a\nspecific tool or action, the likelihood of executing this tool in the\nsubsequent steps increases significantly, as the agent seldom re-evaluates its\nactions. Consequently, even random, harmless requests can establish a\nfoot-in-the-door, allowing an attacker to embed malicious instructions into the\nagents thought process, making it more susceptible to harmful directives. To\nmitigate this vulnerability, we propose implementing a simple reflection\nmechanism that prompts the agent to reassess the safety of its actions during\nexecution, which can help reduce the success of such attacks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16950v1",
    "published_date": "2024-10-22 12:24:41 UTC",
    "updated_date": "2024-10-22 12:24:41 UTC"
  },
  {
    "arxiv_id": "2410.16946v1",
    "title": "Self-Evolving Multi-Agent Collaboration Networks for Software Development",
    "authors": [
      "Yue Hu",
      "Yuzhu Cai",
      "Yaxin Du",
      "Xinyu Zhu",
      "Xiangrui Liu",
      "Zijie Yu",
      "Yuchen Hou",
      "Shuo Tang",
      "Siheng Chen"
    ],
    "abstract": "LLM-driven multi-agent collaboration (MAC) systems have demonstrated\nimpressive capabilities in automatic software development at the function\nlevel. However, their heavy reliance on human design limits their adaptability\nto the diverse demands of real-world software development. To address this\nlimitation, we introduce EvoMAC, a novel self-evolving paradigm for MAC\nnetworks. Inspired by traditional neural network training, EvoMAC obtains\ntext-based environmental feedback by verifying the MAC network's output against\na target proxy and leverages a novel textual backpropagation to update the\nnetwork. To extend coding capabilities beyond function-level tasks to more\nchallenging software-level development, we further propose rSDE-Bench, a\nrequirement-oriented software development benchmark, which features complex and\ndiverse software requirements along with automatic evaluation of requirement\ncorrectness. Our experiments show that: i) The automatic requirement-aware\nevaluation in rSDE-Bench closely aligns with human evaluations, validating its\nreliability as a software-level coding benchmark. ii) EvoMAC outperforms\nprevious SOTA methods on both the software-level rSDE-Bench and the\nfunction-level HumanEval benchmarks, reflecting its superior coding\ncapabilities. The benchmark can be downloaded at\nhttps://yuzhu-cai.github.io/rSDE-Bench/.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.SE",
    "comment": "25 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.16946v1",
    "published_date": "2024-10-22 12:20:23 UTC",
    "updated_date": "2024-10-22 12:20:23 UTC"
  },
  {
    "arxiv_id": "2410.16945v1",
    "title": "IdenBAT: Disentangled Representation Learning for Identity-Preserved Brain Age Transformation",
    "authors": [
      "Junyeong Maeng",
      "Kwanseok Oh",
      "Wonsik Jung",
      "Heung-Il Suk"
    ],
    "abstract": "Brain age transformation aims to convert reference brain images into\nsynthesized images that accurately reflect the age-specific features of a\ntarget age group. The primary objective of this task is to modify only the\nage-related attributes of the reference image while preserving all other\nage-irrelevant attributes. However, achieving this goal poses substantial\nchallenges due to the inherent entanglement of various image attributes within\nfeatures extracted from a backbone encoder, resulting in simultaneous\nalterations during the image generation. To address this challenge, we propose\na novel architecture that employs disentangled representation learning for\nidentity-preserved brain age transformation called IdenBAT. This approach\nfacilitates the decomposition of image features, ensuring the preservation of\nindividual traits while selectively transforming age-related characteristics to\nmatch those of the target age group. Through comprehensive experiments\nconducted on both 2D and full-size 3D brain datasets, our method adeptly\nconverts input images to target age while retaining individual characteristics\naccurately. Furthermore, our approach demonstrates superiority over existing\nstate-of-the-art regarding performance fidelity.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "q-bio.NC"
    ],
    "primary_category": "eess.IV",
    "comment": "16 pages, 8 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.16945v1",
    "published_date": "2024-10-22 12:20:15 UTC",
    "updated_date": "2024-10-22 12:20:15 UTC"
  },
  {
    "arxiv_id": "2410.16930v2",
    "title": "Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes",
    "authors": [
      "Bryan R. Christ",
      "Zack Gottesman",
      "Jonathan Kropko",
      "Thomas Hartvigsen"
    ],
    "abstract": "Math reasoning is an active area of Large Language Model (LLM) research\nbecause it is a hallmark of artificial intelligence and has implications in\nseveral domains, including math education. However, few works have explored how\nmath reasoning is encoded within LLM parameters and if it is a skill that can\nbe isolated within models. Doing so could allow targeted intervention to\nimprove math performance without altering non-math behavior and foster\nunderstanding of how models encode math reasoning. We introduce Math\nNeurosurgery (MathNeuro), a computationally efficient method we use to isolate\nmath-specific parameters in LLMs using only forward passes. MathNeuro builds on\nexisting work by using weights and activations to calculate parameter\nimportance, but isolates math-specific parameters by filtering out those\nimportant for general language tasks. Through pruning parameters MathNeuro\nidentifies, we delete a LLM's math reasoning ability without significantly\nimpacting its general language ability. Scaling the identified parameters by a\nsmall constant improves a pretrained or instruction-tuned LLM's performance by\n4-17% on GSM8K and 5-35% on MATH while leaving non-math behavior unaltered.\nMathNeuro is also data efficient: most of its effectiveness holds when\nidentifying math-specific parameters using a single sample. MathNeuro\nhighlights the potential for future work to intervene on math-specific\nparameters.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "38 pages, 54 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.16930v2",
    "published_date": "2024-10-22 12:00:58 UTC",
    "updated_date": "2025-02-18 19:45:14 UTC"
  },
  {
    "arxiv_id": "2411.02423v1",
    "title": "Development of CODO: A Comprehensive Tool for COVID-19 Data Representation, Analysis, and Visualization",
    "authors": [
      "Biswanath Dutta",
      "Debanjali Bain"
    ],
    "abstract": "Artificial intelligence (AI) has become indispensable for managing and\nprocessing the vast amounts of data generated during the COVID-19 pandemic.\nOntology, which formalizes knowledge within a domain using standardized\nvocabularies and relationships, plays a crucial role in AI by enabling\nautomated reasoning, data integration, semantic interoperability, and\nextracting meaningful insights from extensive datasets. The diversity of\nCOVID-19 datasets poses challenges in comprehending this information for both\nhuman and machines. Existing COVID-19 ontologies are designed to address\nspecific aspects of the pandemic but lack comprehensive coverage across all\nessential dimensions. To address this gap, CODO, an integrated ontological\nmodel has been developed encompassing critical facets of COVID-19 information\nsuch as aetiology, epidemiology, transmission, pathogenesis, diagnosis,\nprevention, genomics, therapeutic safety, and more. This paper reviews CODO\nsince its inception in 2020, detailing its developments and highlighting CODO\nas a tool for the aggregation, representation, analysis, and visualization of\ndiverse COVID-19 data. The major contribution of this paper is to provide a\nsummary of the development of CODO, and outline the overall development and\nevaluation approach. By adhering to best practices and leveraging W3C\nstandards, CODO ensures data integration and semantic interoperability,\nsupporting effective navigation of COVID-19 complexities across various\ndomains.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "15 pages, 4 figures, journal",
    "pdf_url": "http://arxiv.org/pdf/2411.02423v1",
    "published_date": "2024-10-22 11:59:54 UTC",
    "updated_date": "2024-10-22 11:59:54 UTC"
  },
  {
    "arxiv_id": "2410.16927v1",
    "title": "Revealing Hidden Bias in AI: Lessons from Large Language Models",
    "authors": [
      "Django Beatty",
      "Kritsada Masanthia",
      "Teepakorn Kaphol",
      "Niphan Sethi"
    ],
    "abstract": "As large language models (LLMs) become integral to recruitment processes,\nconcerns about AI-induced bias have intensified. This study examines biases in\ncandidate interview reports generated by Claude 3.5 Sonnet, GPT-4o, Gemini 1.5,\nand Llama 3.1 405B, focusing on characteristics such as gender, race, and age.\nWe evaluate the effectiveness of LLM-based anonymization in reducing these\nbiases. Findings indicate that while anonymization reduces certain biases,\nparticularly gender bias, the degree of effectiveness varies across models and\nbias types. Notably, Llama 3.1 405B exhibited the lowest overall bias.\nMoreover, our methodology of comparing anonymized and non-anonymized data\nreveals a novel approach to assessing inherent biases in LLMs beyond\nrecruitment applications. This study underscores the importance of careful LLM\nselection and suggests best practices for minimizing bias in AI applications,\npromoting fairness and inclusivity.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "I.2.7; K.4.1"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages, 18 figures. This paper presents a technical analysis of\n  bias in large language models, focusing on bias detection and mitigation",
    "pdf_url": "http://arxiv.org/pdf/2410.16927v1",
    "published_date": "2024-10-22 11:58:54 UTC",
    "updated_date": "2024-10-22 11:58:54 UTC"
  },
  {
    "arxiv_id": "2410.16924v1",
    "title": "SleepCoT: A Lightweight Personalized Sleep Health Model via Chain-of-Thought Distillation",
    "authors": [
      "Huimin Zheng",
      "Xiaofeng Xing",
      "Xiangmin Xu"
    ],
    "abstract": "We present a novel approach to personalized sleep health management using\nfew-shot Chain-of-Thought (CoT) distillation, enabling small-scale language\nmodels (> 2B parameters) to rival the performance of large language models\n(LLMs) in specialized health domains. Our method simultaneously distills\nproblem-solving strategies, long-tail expert knowledge, and personalized\nrecommendation capabilities from larger models into more efficient, compact\nmodels. Unlike existing systems, our approach offers three key functionalities:\ngenerating personalized sleep health recommendations, supporting user-specific\nfollow-up inquiries, and providing responses to domain-specific knowledge\nquestions. We focus on sleep health due to its measurability via wearable\ndevices and its impact on overall well-being. Our experimental setup, involving\nGPT-4o for data synthesis, Qwen-max for instruction set creation, and Qwen2.5\n1.5B for model distillation, demonstrates significant improvements over\nbaseline small-scale models in penalization, reasoning, and knowledge\napplication. Experiments using 100 simulated sleep reports and 1,000\ndomain-specific questions shows our model achieves comparable performance to\nlarger models while maintaining efficiency for real-world deployment. This\nresearch not only advances AI-driven health management but also provides a\nnovel approach to leveraging LLM capabilities in resource-constrained\nenvironments, potentially enhancing the accessibility of personalized\nhealthcare solutions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16924v1",
    "published_date": "2024-10-22 11:56:34 UTC",
    "updated_date": "2024-10-22 11:56:34 UTC"
  },
  {
    "arxiv_id": "2410.16919v1",
    "title": "EnvBridge: Bridging Diverse Environments with Cross-Environment Knowledge Transfer for Embodied AI",
    "authors": [
      "Tomoyuki Kagaya",
      "Yuxuan Lou",
      "Thong Jing Yuan",
      "Subramanian Lakshmi",
      "Jayashree Karlekar",
      "Sugiri Pranata",
      "Natsuki Murakami",
      "Akira Kinose",
      "Koki Oguri",
      "Felix Wick",
      "Yang You"
    ],
    "abstract": "In recent years, Large Language Models (LLMs) have demonstrated high\nreasoning capabilities, drawing attention for their applications as agents in\nvarious decision-making processes. One notably promising application of LLM\nagents is robotic manipulation. Recent research has shown that LLMs can\ngenerate text planning or control code for robots, providing substantial\nflexibility and interaction capabilities. However, these methods still face\nchallenges in terms of flexibility and applicability across different\nenvironments, limiting their ability to adapt autonomously. Current approaches\ntypically fall into two categories: those relying on environment-specific\npolicy training, which restricts their transferability, and those generating\ncode actions based on fixed prompts, which leads to diminished performance when\nconfronted with new environments. These limitations significantly constrain the\ngeneralizability of agents in robotic manipulation. To address these\nlimitations, we propose a novel method called EnvBridge. This approach involves\nthe retention and transfer of successful robot control codes from source\nenvironments to target environments. EnvBridge enhances the agent's\nadaptability and performance across diverse settings by leveraging insights\nfrom multiple environments. Notably, our approach alleviates environmental\nconstraints, offering a more flexible and generalizable solution for robotic\nmanipulation tasks. We validated the effectiveness of our method using robotic\nmanipulation benchmarks: RLBench, MetaWorld, and CALVIN. Our experiments\ndemonstrate that LLM agents can successfully leverage diverse knowledge sources\nto solve complex tasks. Consequently, our approach significantly enhances the\nadaptability and robustness of robotic manipulation agents in planning across\ndiverse environments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16919v1",
    "published_date": "2024-10-22 11:52:22 UTC",
    "updated_date": "2024-10-22 11:52:22 UTC"
  },
  {
    "arxiv_id": "2410.16908v1",
    "title": "Mitigating Vanishing Activations in Deep CapsNets Using Channel Pruning",
    "authors": [
      "Siddharth Sahu",
      "Abdulrahman Altahhan"
    ],
    "abstract": "Capsule Networks outperform Convolutional Neural Networks in learning the\npart-whole relationships with viewpoint invariance, and the credit goes to\ntheir multidimensional capsules. It was assumed that increasing the number of\ncapsule layers in the capsule networks would enhance the model performance.\nHowever, recent studies found that Capsule Networks lack scalability due to\nvanishing activations in the capsules of deeper layers. This paper thoroughly\ninvestigates the vanishing activation problem in deep Capsule Networks. To\nanalyze this issue and understand how increasing capsule dimensions can\nfacilitate deeper networks, various Capsule Network models are constructed and\nevaluated with different numbers of capsules, capsule dimensions, and\nintermediate layers for this paper. Unlike traditional model pruning, which\nreduces the number of model parameters and expedites model training, this study\nuses pruning to mitigate the vanishing activations in the deeper capsule\nlayers. In addition, the backbone network and capsule layers are pruned with\ndifferent pruning ratios to reduce the number of inactive capsules and achieve\nbetter model accuracy than the unpruned models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16908v1",
    "published_date": "2024-10-22 11:28:39 UTC",
    "updated_date": "2024-10-22 11:28:39 UTC"
  },
  {
    "arxiv_id": "2410.18141v2",
    "title": "SmartRAG: Jointly Learn RAG-Related Tasks From the Environment Feedback",
    "authors": [
      "Jingsheng Gao",
      "Linxu Li",
      "Weiyuan Li",
      "Yuzhuo Fu",
      "Bin Dai"
    ],
    "abstract": "RAG systems consist of multiple modules to work together. However, these\nmodules are usually separately trained. We argue that a system like RAG that\nincorporates multiple modules should be jointly optimized to achieve optimal\nperformance. To demonstrate this, we design a specific pipeline called\n\\textbf{SmartRAG} that includes a policy network and a retriever. The policy\nnetwork can serve as 1) a decision maker that decides when to retrieve, 2) a\nquery rewriter to generate a query most suited to the retriever, and 3) an\nanswer generator that produces the final response with/without the\nobservations. We then propose to jointly optimize the whole system using a\nreinforcement learning algorithm, with the reward designed to encourage the\nsystem to achieve the best performance with minimal retrieval cost. When\njointly optimized, all the modules can be aware of how other modules are\nworking and thus find the best way to work together as a complete system.\nEmpirical results demonstrate that the jointly optimized SmartRAG can achieve\nbetter performance than separately optimized counterparts.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18141v2",
    "published_date": "2024-10-22 11:23:11 UTC",
    "updated_date": "2025-03-10 09:49:31 UTC"
  },
  {
    "arxiv_id": "2410.16882v2",
    "title": "Large Language Model-based Augmentation for Imbalanced Node Classification on Text-Attributed Graphs",
    "authors": [
      "Leyao Wang",
      "Yu Wang",
      "Bo Ni",
      "Yuying Zhao",
      "Tyler Derr"
    ],
    "abstract": "Node classification on graphs often suffers from class imbalance, leading to\nbiased predictions and significant risks in real-world applications. While\ndata-centric solutions have been explored, they largely overlook\nText-Attributed Graphs (TAGs) and the potential of using rich textual semantics\nto improve the classification of minority nodes. Given this gap, we propose\nLarge Language Model-based Augmentation on Text-Attributed Graphs (LA-TAG), a\nnovel framework that leverages Large Language Models (LLMs) to handle\nimbalanced node classification. Specifically, we develop prompting strategies\ninspired by interpolation to synthesize textual node attributes. Additionally,\nto effectively integrate synthetic nodes into the graph structure, we introduce\na textual link predictor that connects the generated nodes to the original\ngraph, preserving structural and contextual information. Experiments across\nvarious datasets and evaluation metrics demonstrate that LA-TAG outperforms\nexisting textual augmentation and graph imbalance learning methods, emphasizing\nthe efficacy of our approach in addressing class imbalance in TAGs.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.16882v2",
    "published_date": "2024-10-22 10:36:15 UTC",
    "updated_date": "2025-01-27 17:06:48 UTC"
  },
  {
    "arxiv_id": "2410.16879v1",
    "title": "Contrasting Attitudes Towards Current and Future AI Applications for Computerised Interpretation of ECG: A Clinical Stakeholder Interview Study",
    "authors": [
      "Lukas Hughes-Noehrer",
      "Leda Channer",
      "Gabriel Strain",
      "Gregory Yates",
      "Richard Body",
      "Caroline Jay"
    ],
    "abstract": "Objectives: To investigate clinicians' attitudes towards current automated\ninterpretation of ECG and novel AI technologies and their perception of\ncomputer-assisted interpretation. Materials and Methods: We conducted a series\nof interviews with clinicians in the UK. Our study: (i) explores the potential\nfor AI, specifically future 'human-like' computing approaches, to facilitate\nECG interpretation and support clinical decision making, and (ii) elicits their\nopinions about the importance of explainability and trustworthiness of AI\nalgorithms. Results: We performed inductive thematic analysis on interview\ntranscriptions from 23 clinicians and identified the following themes: (i) a\nlack of trust in current systems, (ii) positive attitudes towards future AI\napplications and requirements for these, (iii) the relationship between the\naccuracy and explainability of algorithms, and (iv) opinions on education,\npossible deskilling, and the impact of AI on clinical competencies. Discussion:\nClinicians do not trust current computerised methods, but welcome future 'AI'\ntechnologies. Where clinicians trust future AI interpretation to be accurate,\nthey are less concerned that it is explainable. They also preferred ECG\ninterpretation that demonstrated the results of the algorithm visually. Whilst\nclinicians do not fear job losses, they are concerned about deskilling and the\nneed to educate the workforce to use AI responsibly. Conclusion: Clinicians are\npositive about the future application of AI in clinical decision-making.\nAccuracy is a key factor of uptake and visualisations are preferred over\ncurrent computerised methods. This is viewed as a potential means of training\nand upskilling, in contrast to the deskilling that automation might be\nperceived to bring.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16879v1",
    "published_date": "2024-10-22 10:31:23 UTC",
    "updated_date": "2024-10-22 10:31:23 UTC"
  },
  {
    "arxiv_id": "2410.21303v1",
    "title": "VEMOCLAP: A video emotion classification web application",
    "authors": [
      "Serkan Sulun",
      "Paula Viana",
      "Matthew E. P. Davies"
    ],
    "abstract": "We introduce VEMOCLAP: Video EMOtion Classifier using Pretrained features,\nthe first readily available and open-source web application that analyzes the\nemotional content of any user-provided video. We improve our previous work,\nwhich exploits open-source pretrained models that work on video frames and\naudio, and then efficiently fuse the resulting pretrained features using\nmulti-head cross-attention. Our approach increases the state-of-the-art\nclassification accuracy on the Ekman-6 video emotion dataset by 4.3% and offers\nan online application for users to run our model on their own videos or YouTube\nvideos. We invite the readers to try our application at serkansulun.com/app.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to 2024 IEEE International Symposium on Multimedia (ISM),\n  Tokyo, Japan",
    "pdf_url": "http://arxiv.org/pdf/2410.21303v1",
    "published_date": "2024-10-22 10:12:11 UTC",
    "updated_date": "2024-10-22 10:12:11 UTC"
  },
  {
    "arxiv_id": "2410.16864v1",
    "title": "Pedestrian motion prediction evaluation for urban autonomous driving",
    "authors": [
      "Dmytro Zabolotnii",
      "Yar Muhammad",
      "Naveed Muhammad"
    ],
    "abstract": "Pedestrian motion prediction is a key part of the modular-based autonomous\ndriving pipeline, ensuring safe, accurate, and timely awareness of human\nagents' possible future trajectories. The autonomous vehicle can use this\ninformation to prevent any possible accidents and create a comfortable and\npleasant driving experience for the passengers and pedestrians. A wealth of\nresearch was done on the topic from the authors of robotics, computer vision,\nintelligent transportation systems, and other fields. However, a relatively\nunexplored angle is the integration of the state-of-art solutions into existing\nautonomous driving stacks and evaluating them in real-life conditions rather\nthan sanitized datasets. We analyze selected publications with provided\nopen-source solutions and provide a perspective obtained by integrating them\ninto existing Autonomous Driving framework - Autoware Mini and performing\nexperiments in natural urban conditions in Tartu, Estonia to determine\nvaluability of traditional motion prediction metrics. This perspective should\nbe valuable to any potential autonomous driving or robotics engineer looking\nfor the real-world performance of the existing state-of-art pedestrian motion\nprediction problem. The code with instructions on accessing the dataset is\navailable at https://github.com/dmytrozabolotnii/autoware_mini.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "I.2.9; D.4.8"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages, 2 figures, 4 tables This work has been submitted to the IEEE\n  for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2410.16864v1",
    "published_date": "2024-10-22 10:06:50 UTC",
    "updated_date": "2024-10-22 10:06:50 UTC"
  },
  {
    "arxiv_id": "2410.16845v1",
    "title": "Fast Graph Sharpness-Aware Minimization for Enhancing and Accelerating Few-Shot Node Classification",
    "authors": [
      "Yihong Luo",
      "Yuhan Chen",
      "Siya Qiu",
      "Yiwei Wang",
      "Chen Zhang",
      "Yan Zhou",
      "Xiaochun Cao",
      "Jing Tang"
    ],
    "abstract": "Graph Neural Networks (GNNs) have shown superior performance in node\nclassification. However, GNNs perform poorly in the Few-Shot Node\nClassification (FSNC) task that requires robust generalization to make accurate\npredictions for unseen classes with limited labels. To tackle the challenge, we\npropose the integration of Sharpness-Aware Minimization (SAM)--a technique\ndesigned to enhance model generalization by finding a flat minimum of the loss\nlandscape--into GNN training. The standard SAM approach, however, consists of\ntwo forward-backward steps in each training iteration, doubling the\ncomputational cost compared to the base optimizer (e.g., Adam). To mitigate\nthis drawback, we introduce a novel algorithm, Fast Graph Sharpness-Aware\nMinimization (FGSAM), that integrates the rapid training of Multi-Layer\nPerceptrons (MLPs) with the superior performance of GNNs. Specifically, we\nutilize GNNs for parameter perturbation while employing MLPs to minimize the\nperturbed loss so that we can find a flat minimum with good generalization more\nefficiently. Moreover, our method reutilizes the gradient from the perturbation\nphase to incorporate graph topology into the minimization process at almost\nzero additional cost. To further enhance training efficiency, we develop FGSAM+\nthat executes exact perturbations periodically. Extensive experiments\ndemonstrate that our proposed algorithm outperforms the standard SAM with lower\ncomputational costs in FSNC tasks. In particular, our FGSAM+ as a SAM variant\noffers a faster optimization than the base optimizer in most cases. In addition\nto FSNC, our proposed methods also demonstrate competitive performance in the\nstandard node classification task for heterophilic graphs, highlighting the\nbroad applicability. The code is available at\nhttps://github.com/draym28/FGSAM_NeurIPS24.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS24; The first two authors contributed equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2410.16845v1",
    "published_date": "2024-10-22 09:33:29 UTC",
    "updated_date": "2024-10-22 09:33:29 UTC"
  },
  {
    "arxiv_id": "2410.19852v1",
    "title": "Survival of the Fittest: Evolutionary Adaptation of Policies for Environmental Shifts",
    "authors": [
      "Sheryl Paul",
      "Jyotirmoy V. Deshmukh"
    ],
    "abstract": "Reinforcement learning (RL) has been successfully applied to solve the\nproblem of finding obstacle-free paths for autonomous agents operating in\nstochastic and uncertain environments. However, when the underlying stochastic\ndynamics of the environment experiences drastic distribution shifts, the\noptimal policy obtained in the trained environment may be sub-optimal or may\nentirely fail in helping find goal-reaching paths for the agent. Approaches\nlike domain randomization and robust RL can provide robust policies, but\ntypically assume minor (bounded) distribution shifts. For substantial\ndistribution shifts, retraining (either with a warm-start policy or from\nscratch) is an alternative approach. In this paper, we develop a novel approach\ncalled {\\em Evolutionary Robust Policy Optimization} (ERPO), an adaptive\nre-training algorithm inspired by evolutionary game theory (EGT). ERPO learns\nan optimal policy for the shifted environment iteratively using a temperature\nparameter that controls the trade off between exploration and adherence to the\nold optimal policy. The policy update itself is an instantiation of the\nreplicator dynamics used in EGT. We show that under fairly common sparsity\nassumptions on rewards in such environments, ERPO converges to the optimal\npolicy in the shifted environment. We empirically demonstrate that for path\nfinding tasks in a number of environments, ERPO outperforms several popular RL\nand deep RL algorithms (PPO, A3C, DQN) in many scenarios and popular\nenvironments. This includes scenarios where the RL algorithms are allowed to\ntrain from scratch in the new environment, when they are retrained on the new\nenvironment, or when they are used in conjunction with domain randomization.\nERPO shows faster policy adaptation, higher average rewards, and reduced\ncomputational costs in policy adaptation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "Pubblished in ECAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.19852v1",
    "published_date": "2024-10-22 09:29:53 UTC",
    "updated_date": "2024-10-22 09:29:53 UTC"
  },
  {
    "arxiv_id": "2410.16842v1",
    "title": "Assessment of Transformer-Based Encoder-Decoder Model for Human-Like Summarization",
    "authors": [
      "Sindhu Nair",
      "Y. S. Rao",
      "Radha Shankarmani"
    ],
    "abstract": "In recent times, extracting valuable information from large text is making\nsignificant progress. Especially in the current era of social media, people\nexpect quick bites of information. Automatic text summarization seeks to tackle\nthis by slimming large texts down into more manageable summaries. This\nimportant research area can aid in decision-making by digging out salient\ncontent from large text. With the progress in deep learning models, significant\nwork in language models has emerged. The encoder-decoder framework in deep\nlearning has become the central approach for automatic text summarization. This\nwork leverages transformer-based BART model for human-like summarization which\nis an open-ended problem with many challenges. On training and fine-tuning the\nencoder-decoder model, it is tested with diverse sample articles and the\nquality of summaries of diverse samples is assessed based on human evaluation\nparameters. Further, the finetuned model performance is compared with the\nbaseline pretrained model based on evaluation metrics like ROUGE score and\nBERTScore. Additionally, domain adaptation of the model is required for\nimproved performance of abstractive summarization of dialogues between\ninterlocutors. On investigating, the above popular evaluation metrics are found\nto be insensitive to factual errors. Further investigation of the summaries\ngenerated by finetuned model is done using the contemporary evaluation metrics\nof factual consistency like WeCheck and SummaC. Empirical results on BBC News\narticles highlight that the gold standard summaries written by humans are more\nfactually consistent by 17% than the abstractive summaries generated by\nfinetuned model.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Pre-print",
    "pdf_url": "http://arxiv.org/pdf/2410.16842v1",
    "published_date": "2024-10-22 09:25:04 UTC",
    "updated_date": "2024-10-22 09:25:04 UTC"
  },
  {
    "arxiv_id": "2411.00017v1",
    "title": "Applying Data Driven Decision Making to rank Vocational and Educational Training Programs with TOPSIS",
    "authors": [
      "J. M. Conejero",
      "J. C. Preciado",
      "A. E. Prieto",
      "M. C. Bas",
      "V. J. Bolos"
    ],
    "abstract": "In this paper we present a multi-criteria classification of Vocational and\nEducational Programs in Extremadura (Spain) during the period 2009-2016. This\nranking has been carried out through the integration into a complete database\nof the detailed information of individuals finishing such studies together with\ntheir labor data. The multicriteria method used is TOPSIS together with a new\ndecision support method for assessing the influence of each criterion and its\ndependence on the weights assigned to them. This new method is based on a\nworst-best case scenario analysis and it is compared to a well known global\nsensitivity analysis technique based on the Pearson's correlation ratio.",
    "categories": [
      "cs.AI",
      "cs.NA",
      "math.NA",
      "90B50, 90C29, 90C31, 91A35, 62C86"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.00017v1",
    "published_date": "2024-10-22 09:04:41 UTC",
    "updated_date": "2024-10-22 09:04:41 UTC"
  },
  {
    "arxiv_id": "2410.16824v1",
    "title": "PerspectiveNet: Multi-View Perception for Dynamic Scene Understanding",
    "authors": [
      "Vinh Nguyen"
    ],
    "abstract": "Generating detailed descriptions from multiple cameras and viewpoints is\nchallenging due to the complex and inconsistent nature of visual data. In this\npaper, we introduce PerspectiveNet, a lightweight yet efficient model for\ngenerating long descriptions across multiple camera views. Our approach\nutilizes a vision encoder, a compact connector module to convert visual\nfeatures into a fixed-size tensor, and large language models (LLMs) to harness\nthe strong natural language generation capabilities of LLMs. The connector\nmodule is designed with three main goals: mapping visual features onto LLM\nembeddings, emphasizing key information needed for description generation, and\nproducing a fixed-size feature matrix. Additionally, we augment our solution\nwith a secondary task, the correct frame sequence detection, enabling the model\nto search for the correct sequence of frames to generate descriptions. Finally,\nwe integrate the connector module, the secondary task, the LLM, and a visual\nfeature extraction model into a single architecture, which is trained for the\nTraffic Safety Description and Analysis task. This task requires generating\ndetailed, fine-grained descriptions of events from multiple cameras and\nviewpoints. The resulting model is lightweight, ensuring efficient training and\ninference, while remaining highly effective.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.16824v1",
    "published_date": "2024-10-22 08:57:17 UTC",
    "updated_date": "2024-10-22 08:57:17 UTC"
  },
  {
    "arxiv_id": "2410.16822v2",
    "title": "Can Large Language Models Act as Ensembler for Multi-GNNs?",
    "authors": [
      "Hanqi Duan",
      "Yao Cheng",
      "Jianxiang Yu",
      "Xiang Li"
    ],
    "abstract": "Graph Neural Networks (GNNs) have emerged as powerful models for learning\nfrom graph-structured data. However, GNNs lack the inherent semantic\nunderstanding capability of rich textual node attributes, limiting their\neffectiveness in applications. On the other hand, we empirically observe that\nfor existing GNN models, no one can consistently outperforms others across\ndiverse datasets. In this paper, we study whether LLMs can act as an ensembler\nfor multi-GNNs and propose the LensGNN model. The model first aligns multiple\nGNNs, mapping the representations of different GNNs into the same space. Then,\nthrough LoRA fine-tuning, it aligns the space between the GNN and the LLM,\ninjecting graph tokens and textual information into LLMs. This allows LensGNN\nto ensemble multiple GNNs and take advantage of the strengths of LLM, leading\nto a deeper understanding of both textual semantic information and graph\nstructural information. The experimental results show that LensGNN outperforms\nexisting models. This research advances text-attributed graph ensemble learning\nby providing a robust and superior solution for integrating semantic and\nstructural information. We provide our code and data here:\nhttps://anonymous.4open.science/r/EnsemGNN-E267/.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16822v2",
    "published_date": "2024-10-22 08:48:52 UTC",
    "updated_date": "2024-12-17 07:20:33 UTC"
  },
  {
    "arxiv_id": "2410.16803v3",
    "title": "Context-aware Inductive Knowledge Graph Completion with Latent Type Constraints and Subgraph Reasoning",
    "authors": [
      "Muzhi Li",
      "Cehao Yang",
      "Chengjin Xu",
      "Zixing Song",
      "Xuhui Jiang",
      "Jian Guo",
      "Ho-fung Leung",
      "Irwin King"
    ],
    "abstract": "Inductive knowledge graph completion (KGC) aims to predict missing triples\nwith unseen entities. Recent works focus on modeling reasoning paths between\nthe head and tail entity as direct supporting evidence. However, these methods\ndepend heavily on the existence and quality of reasoning paths, which limits\ntheir general applicability in different scenarios. In addition, we observe\nthat latent type constraints and neighboring facts inherent in KGs are also\nvital in inferring missing triples. To effectively utilize all useful\ninformation in KGs, we introduce CATS, a novel context-aware inductive KGC\nsolution. With sufficient guidance from proper prompts and supervised\nfine-tuning, CATS activates the strong semantic understanding and reasoning\ncapabilities of large language models to assess the existence of query triples,\nwhich consist of two modules. First, the type-aware reasoning module evaluates\nwhether the candidate entity matches the latent entity type as required by the\nquery relation. Then, the subgraph reasoning module selects relevant reasoning\npaths and neighboring facts, and evaluates their correlation to the query\ntriple. Experiment results on three widely used datasets demonstrate that CATS\nsignificantly outperforms state-of-the-art methods in 16 out of 18\ntransductive, inductive, and few-shot settings with an average absolute MRR\nimprovement of 7.2%.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16803v3",
    "published_date": "2024-10-22 08:28:05 UTC",
    "updated_date": "2024-12-27 15:32:01 UTC"
  },
  {
    "arxiv_id": "2410.16801v2",
    "title": "Controlled Low-Rank Adaptation with Subspace Regularization for Continued Training on Large Language Models",
    "authors": [
      "Yuheng Lu",
      "Bingshuo Qian",
      "Caixia Yuan",
      "Huixing Jiang",
      "Xiaojie Wang"
    ],
    "abstract": "Large language models (LLMs) exhibit remarkable capabilities in natural\nlanguage processing but face catastrophic forgetting when learning new tasks,\nwhere adaptation to a new domain leads to a substantial decline in performance\non previous tasks. In this paper, we propose Controlled LoRA (CLoRA), a\nsub-space regularization method on LoRA structure. Aiming to reduce the scale\nof output change while introduce minimal constraint on model capacity, CLoRA\nimposes constraint on the direction of updating matrix's null space.\nExperimental results on one-stage LLM finetuning tasks and continual learning\nsettings highlight the superority of CLoRA as a effective parameter efficient\nfinetuning method with catastrophic forgetting mitigating.Further investigation\nfor model parameters indicates that CLoRA effectively balances the trade-off\nbetween model capacity and degree of forgetting.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16801v2",
    "published_date": "2024-10-22 08:27:23 UTC",
    "updated_date": "2025-03-21 12:34:15 UTC"
  },
  {
    "arxiv_id": "2410.16795v2",
    "title": "Scene-Aware Explainable Multimodal Trajectory Prediction",
    "authors": [
      "Pei Liu",
      "Haipeng Liu",
      "Xingyu Liu",
      "Yiqun Li",
      "Junlan Chen",
      "Yangfan He",
      "Jun Ma"
    ],
    "abstract": "Advancements in intelligent technologies have significantly improved\nnavigation in complex traffic environments by enhancing environment perception\nand trajectory prediction for automated vehicles. However, current research\noften overlooks the joint reasoning of scenario agents and lacks explainability\nin trajectory prediction models, limiting their practical use in real-world\nsituations. To address this, we introduce the Explainable Conditional\nDiffusion-based Multimodal Trajectory Prediction (DMTP) model, which is\ndesigned to elucidate the environmental factors influencing predictions and\nreveal the underlying mechanisms. Our model integrates a modified conditional\ndiffusion approach to capture multimodal trajectory patterns and employs a\nrevised Shapley Value model to assess the significance of global and\nscenario-specific features. Experiments using the Waymo Open Motion Dataset\ndemonstrate that our explainable model excels in identifying critical inputs\nand significantly outperforms baseline models in accuracy. Moreover, the\nfactors identified align with the human driving experience, underscoring the\nmodel's effectiveness in learning accurate predictions. Code is available in\nour open-source repository:\nhttps://github.com/ocean-luna/Explainable-Prediction.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16795v2",
    "published_date": "2024-10-22 08:17:33 UTC",
    "updated_date": "2025-03-10 01:33:26 UTC"
  },
  {
    "arxiv_id": "2410.16794v1",
    "title": "One-Step Diffusion Distillation through Score Implicit Matching",
    "authors": [
      "Weijian Luo",
      "Zemin Huang",
      "Zhengyang Geng",
      "J. Zico Kolter",
      "Guo-jun Qi"
    ],
    "abstract": "Despite their strong performances on many generative tasks, diffusion models\nrequire a large number of sampling steps in order to generate realistic\nsamples. This has motivated the community to develop effective methods to\ndistill pre-trained diffusion models into more efficient models, but these\nmethods still typically require few-step inference or perform substantially\nworse than the underlying model. In this paper, we present Score Implicit\nMatching (SIM) a new approach to distilling pre-trained diffusion models into\nsingle-step generator models, while maintaining almost the same sample\ngeneration ability as the original model as well as being data-free with no\nneed of training samples for distillation. The method rests upon the fact that,\nalthough the traditional score-based loss is intractable to minimize for\ngenerator models, under certain conditions we can efficiently compute the\ngradients for a wide class of score-based divergences between a diffusion model\nand a generator. SIM shows strong empirical performances for one-step\ngenerators: on the CIFAR10 dataset, it achieves an FID of 2.06 for\nunconditional generation and 1.96 for class-conditional generation. Moreover,\nby applying SIM to a leading transformer-based diffusion model, we distill a\nsingle-step generator for text-to-image (T2I) generation that attains an\naesthetic score of 6.42 with no performance decline over the original\nmulti-step counterpart, clearly outperforming the other one-step generators\nincluding SDXL-TURBO of 5.33, SDXL-LIGHTNING of 5.34 and HYPER-SDXL of 5.85. We\nwill release this industry-ready one-step transformer-based T2I generator along\nwith this paper.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.16794v1",
    "published_date": "2024-10-22 08:17:20 UTC",
    "updated_date": "2024-10-22 08:17:20 UTC"
  },
  {
    "arxiv_id": "2410.16788v1",
    "title": "Correct after Answer: Enhancing Multi-Span Question Answering with Post-Processing Method",
    "authors": [
      "Jiayi Lin",
      "Chenyang Zhang",
      "Haibo Tong",
      "Dongyu Zhang",
      "Qingqing Hong",
      "Bingxuan Hou",
      "Junli Wang"
    ],
    "abstract": "Multi-Span Question Answering (MSQA) requires models to extract one or\nmultiple answer spans from a given context to answer a question. Prior work\nmainly focuses on designing specific methods or applying heuristic strategies\nto encourage models to predict more correct predictions. However, these models\nare trained on gold answers and fail to consider the incorrect predictions.\nThrough a statistical analysis, we observe that models with stronger abilities\ndo not predict less incorrect predictions compared with other models. In this\nwork, we propose Answering-Classifying-Correcting (ACC) framework, which\nemploys a post-processing strategy to handle incorrect predictions.\nSpecifically, the ACC framework first introduces a classifier to classify the\npredictions into three types and exclude \"wrong predictions\", then introduces a\ncorrector to modify \"partially correct predictions\". Experiments on several\nMSQA datasets show that ACC framework significantly improves the Exact Match\n(EM) scores, and further analysis demostrates that ACC framework efficiently\nreduces the number of incorrect predictions, improving the quality of\npredictions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2410.16788v1",
    "published_date": "2024-10-22 08:04:32 UTC",
    "updated_date": "2024-10-22 08:04:32 UTC"
  },
  {
    "arxiv_id": "2410.16780v2",
    "title": "Beyond Retrieval: Generating Narratives in Conversational Recommender Systems",
    "authors": [
      "Krishna Sayana",
      "Raghavendra Vasudeva",
      "Yuri Vasilevski",
      "Kun Su",
      "Liam Hebert",
      "James Pine",
      "Hubert Pham",
      "Ambarish Jash",
      "Sukhdeep Sodhi"
    ],
    "abstract": "The recent advances in Large Language Model's generation and reasoning\ncapabilities present an opportunity to develop truly conversational\nrecommendation systems. However, effectively integrating recommender system\nknowledge into LLMs for natural language generation which is tailored towards\nrecommendation tasks remains a challenge. This paper addresses this challenge\nby making two key contributions.\n  First, we introduce a new dataset (REGEN) for natural language generation\ntasks in conversational recommendations. REGEN (Reviews Enhanced with\nGEnerative Narratives) extends the Amazon Product Reviews dataset with rich\nuser narratives, including personalized explanations of product preferences,\nproduct endorsements for recommended items, and summaries of user purchase\nhistory. REGEN is made publicly available to facilitate further research.\nFurthermore, we establish benchmarks using well-known generative metrics, and\nperform an automated evaluation of the new dataset using a rater LLM. Second,\nthe paper introduces a fusion architecture (CF model with an LLM) which serves\nas a baseline for REGEN. And to the best of our knowledge, represents the first\nattempt to analyze the capabilities of LLMs in understanding recommender\nsignals and generating rich narratives. We demonstrate that LLMs can\neffectively learn from simple fusion architectures utilizing interaction-based\nCF embeddings, and this can be further enhanced using the metadata and\npersonalization data associated with items. Our experiments show that combining\nCF and content embeddings leads to improvements of 4-12% in key language\nmetrics compared to using either type of embedding individually. We also\nprovide an analysis to interpret how CF and content embeddings contribute to\nthis new generative task.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16780v2",
    "published_date": "2024-10-22 07:53:41 UTC",
    "updated_date": "2024-12-10 18:45:18 UTC"
  },
  {
    "arxiv_id": "2410.16770v2",
    "title": "The Scene Language: Representing Scenes with Programs, Words, and Embeddings",
    "authors": [
      "Yunzhi Zhang",
      "Zizhang Li",
      "Matt Zhou",
      "Shangzhe Wu",
      "Jiajun Wu"
    ],
    "abstract": "We introduce the Scene Language, a visual scene representation that concisely\nand precisely describes the structure, semantics, and identity of visual\nscenes. It represents a scene with three key components: a program that\nspecifies the hierarchical and relational structure of entities in the scene,\nwords in natural language that summarize the semantic class of each entity, and\nembeddings that capture the visual identity of each entity. This representation\ncan be inferred from pre-trained language models via a training-free inference\ntechnique, given text or image inputs. The resulting scene can be rendered into\nimages using traditional, neural, or hybrid graphics renderers. Together, this\nforms a robust, automated system for high-quality 3D and 4D scene generation.\nCompared with existing representations like scene graphs, our proposed Scene\nLanguage generates complex scenes with higher fidelity, while explicitly\nmodeling the scene structures to enable precise control and editing.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025. Project page:\n  https://ai.stanford.edu/~yzzhang/projects/scene-language/",
    "pdf_url": "http://arxiv.org/pdf/2410.16770v2",
    "published_date": "2024-10-22 07:40:20 UTC",
    "updated_date": "2025-03-29 19:17:13 UTC"
  },
  {
    "arxiv_id": "2410.16765v1",
    "title": "Survival Models: Proper Scoring Rule and Stochastic Optimization with Competing Risks",
    "authors": [
      "Julie Alberge",
      "Vincent Maladière",
      "Olivier Grisel",
      "Judith Abécassis",
      "Gaël Varoquaux"
    ],
    "abstract": "When dealing with right-censored data, where some outcomes are missing due to\na limited observation period, survival analysis -- known as time-to-event\nanalysis -- focuses on predicting the time until an event of interest occurs.\nMultiple classes of outcomes lead to a classification variant: predicting the\nmost likely event, a less explored area known as competing risks. Classic\ncompeting risks models couple architecture and loss, limiting scalability.To\naddress these issues, we design a strictly proper censoring-adjusted separable\nscoring rule, allowing optimization on a subset of the data as each observation\nis evaluated independently. The loss estimates outcome probabilities and\nenables stochastic optimization for competing risks, which we use for efficient\ngradient boosting trees. SurvivalBoost not only outperforms 12 state-of-the-art\nmodels across several metrics on 4 real-life datasets, both in competing risks\nand survival settings, but also provides great calibration, the ability to\npredict across any time horizon, and computation times faster than existing\nmethods.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2406.14085",
    "pdf_url": "http://arxiv.org/pdf/2410.16765v1",
    "published_date": "2024-10-22 07:33:34 UTC",
    "updated_date": "2024-10-22 07:33:34 UTC"
  },
  {
    "arxiv_id": "2410.16762v1",
    "title": "Deep-Sea A*+: An Advanced Path Planning Method Integrating Enhanced A* and Dynamic Window Approach for Autonomous Underwater Vehicles",
    "authors": [
      "Yinyi Lai",
      "Jiaqi Shang",
      "Zenghui Liu",
      "Zheyu Jiang",
      "Yuyang Li",
      "Longchao Chen"
    ],
    "abstract": "As terrestrial resources become increasingly depleted, the demand for\ndeep-sea resource exploration has intensified. However, the extreme conditions\nin the deep-sea environment pose significant challenges for underwater\noperations, necessitating the development of robust detection robots. In this\npaper, we propose an advanced path planning methodology that integrates an\nimproved A* algorithm with the Dynamic Window Approach (DWA). By optimizing the\nsearch direction of the traditional A* algorithm and introducing an enhanced\nevaluation function, our improved A* algorithm accelerates path searching and\nreduces computational load. Additionally, the path-smoothing process has been\nrefined to improve continuity and smoothness, minimizing sharp turns. This\nmethod also integrates global path planning with local dynamic obstacle\navoidance via DWA, improving the real-time response of underwater robots in\ndynamic environments. Simulation results demonstrate that our proposed method\nsurpasses the traditional A* algorithm in terms of path smoothness, obstacle\navoidance, and real-time performance. The robustness of this approach in\ncomplex environments with both static and dynamic obstacles highlights its\npotential in autonomous underwater vehicle (AUV) navigation and obstacle\navoidance.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted by 2024 International Conference on Big Data, Artificial\n  Intelligence and Internet of Things Engineering (ICBAIE 2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.16762v1",
    "published_date": "2024-10-22 07:29:05 UTC",
    "updated_date": "2024-10-22 07:29:05 UTC"
  },
  {
    "arxiv_id": "2410.16759v2",
    "title": "Towards Efficient IMC Accelerator Design Through Joint Hardware-Workload Co-optimization",
    "authors": [
      "Olga Krestinskaya",
      "Mohammed E. Fouda",
      "Ahmed Eltawil",
      "Khaled N. Salama"
    ],
    "abstract": "Designing generalized in-memory computing (IMC) hardware that efficiently\nsupports a variety of workloads requires extensive design space exploration,\nwhich is infeasible to perform manually. Optimizing hardware individually for\neach workload or solely for the largest workload often fails to yield the most\nefficient generalized solutions. To address this, we propose a joint\nhardware-workload optimization framework that identifies optimised IMC chip\narchitecture parameters, enabling more efficient, workload-flexible hardware.\nWe show that joint optimization achieves 36%, 36%, 20%, and 69% better\nenergy-latency-area scores for VGG16, ResNet18, AlexNet, and MobileNetV3,\nrespectively, compared to the separate architecture parameters search\noptimizing for a single largest workload. Additionally, we quantify the\nperformance trade-offs and losses of the resulting generalized IMC hardware\ncompared to workload-specific IMC designs.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "accepted to ISCAS 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.16759v2",
    "published_date": "2024-10-22 07:25:17 UTC",
    "updated_date": "2025-02-01 09:24:13 UTC"
  },
  {
    "arxiv_id": "2410.16748v1",
    "title": "Uncovering Key Trends in Industry 5.0 through Advanced AI Techniques",
    "authors": [
      "Panos Fitsilis",
      "Paraskevi Tsoutsa",
      "Vyron Damasiotis",
      "Vasileios Kyriatzis"
    ],
    "abstract": "This article analyzes around 200 online articles to identify trends within\nIndustry 5.0 using artificial intelligence techniques. Specifically, it applies\nalgorithms such as LDA, BERTopic, LSA, and K-means, in various configurations,\nto extract and compare the central themes present in the literature. The\nresults reveal a convergence around a core set of themes while also\nhighlighting that Industry 5.0 spans a wide range of topics. The study\nconcludes that Industry 5.0, as an evolution of Industry 4.0, is a broad\nconcept that lacks a clear definition, making it difficult to focus on and\napply effectively. Therefore, for Industry 5.0 to be useful, it needs to be\nrefined and more clearly defined. Furthermore, the findings demonstrate that\nwell-known AI techniques can be effectively utilized for trend identification,\nparticularly when the available literature is extensive and the subject matter\nlacks precise boundaries. This study showcases the potential of AI in\nextracting meaningful insights from large and diverse datasets, even in cases\nwhere the thematic structure of the domain is not clearly delineated.",
    "categories": [
      "cs.AI",
      "I.1; K.1"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16748v1",
    "published_date": "2024-10-22 07:06:00 UTC",
    "updated_date": "2024-10-22 07:06:00 UTC"
  },
  {
    "arxiv_id": "2410.16746v1",
    "title": "SpikMamba: When SNN meets Mamba in Event-based Human Action Recognition",
    "authors": [
      "Jiaqi Chen",
      "Yan Yang",
      "Shizhuo Deng",
      "Da Teng",
      "Liyuan Pan"
    ],
    "abstract": "Human action recognition (HAR) plays a key role in various applications such\nas video analysis, surveillance, autonomous driving, robotics, and healthcare.\nMost HAR algorithms are developed from RGB images, which capture detailed\nvisual information. However, these algorithms raise concerns in\nprivacy-sensitive environments due to the recording of identifiable features.\nEvent cameras offer a promising solution by capturing scene brightness changes\nsparsely at the pixel level, without capturing full images. Moreover, event\ncameras have high dynamic ranges that can effectively handle scenarios with\ncomplex lighting conditions, such as low light or high contrast environments.\nHowever, using event cameras introduces challenges in modeling the spatially\nsparse and high temporal resolution event data for HAR. To address these\nissues, we propose the SpikMamba framework, which combines the energy\nefficiency of spiking neural networks and the long sequence modeling capability\nof Mamba to efficiently capture global features from spatially sparse and high\na temporal resolution event data. Additionally, to improve the locality of\nmodeling, a spiking window-based linear attention mechanism is used. Extensive\nexperiments show that SpikMamba achieves remarkable recognition performance,\nsurpassing the previous state-of-the-art by 1.45%, 7.22%, 0.15%, and 3.92% on\nthe PAF, HARDVS, DVS128, and E-FAction datasets, respectively. The code is\navailable at https://github.com/Typistchen/SpikMamba.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.16746v1",
    "published_date": "2024-10-22 07:00:43 UTC",
    "updated_date": "2024-10-22 07:00:43 UTC"
  },
  {
    "arxiv_id": "2410.16739v2",
    "title": "Rethinking Soft Actor-Critic in High-Dimensional Action Spaces: The Cost of Ignoring Distribution Shift",
    "authors": [
      "Yanjun Chen",
      "Xinming Zhang",
      "Xianghui Wang",
      "Zhiqiang Xu",
      "Xiaoyu Shen",
      "Wei Zhang"
    ],
    "abstract": "Soft Actor-Critic algorithm is widely recognized for its robust performance\nacross a range of deep reinforcement learning tasks, where it leverages the\ntanh transformation to constrain actions within bounded limits. However, this\ntransformation induces a distribution shift, distorting the original Gaussian\naction distribution and potentially leading the policy to select suboptimal\nactions, particularly in high-dimensional action spaces. In this paper, we\nconduct a comprehensive theoretical and empirical analysis of this distribution\nshift, deriving the precise probability density function (PDF) for actions\nfollowing the tanh transformation to clarify the misalignment introduced\nbetween the transformed distribution's mode and the intended action output. We\nsubstantiate these theoretical insights through extensive experiments on\nhigh-dimensional tasks within the HumanoidBench benchmark. Our findings\nindicate that accounting for this distribution shift substantially enhances\nSAC's performance, resulting in notable improvements in cumulative rewards,\nsample efficiency, and reliability across tasks. These results underscore a\ncritical consideration for SAC and similar algorithms: addressing\ntransformation-induced distribution shifts is essential to optimizing policy\neffectiveness in high-dimensional deep reinforcement learning environments,\nthereby expanding the robustness and applicability of SAC in complex control\ntasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16739v2",
    "published_date": "2024-10-22 06:46:28 UTC",
    "updated_date": "2025-04-22 04:28:30 UTC"
  },
  {
    "arxiv_id": "2410.16726v1",
    "title": "Enhancing Low-Resource ASR through Versatile TTS: Bridging the Data Gap",
    "authors": [
      "Guanrou Yang",
      "Fan Yu",
      "Ziyang Ma",
      "Zhihao Du",
      "Zhifu Gao",
      "Shiliang Zhang",
      "Xie Chen"
    ],
    "abstract": "While automatic speech recognition (ASR) systems have achieved remarkable\nperformance with large-scale datasets, their efficacy remains inadequate in\nlow-resource settings, encompassing dialects, accents, minority languages, and\nlong-tail hotwords, domains with significant practical relevance. With the\nadvent of versatile and powerful text-to-speech (TTS) models, capable of\ngenerating speech with human-level naturalness, expressiveness, and diverse\nspeaker profiles, leveraging TTS for ASR data augmentation provides a\ncost-effective and practical approach to enhancing ASR performance.\nComprehensive experiments on an unprecedentedly rich variety of low-resource\ndatasets demonstrate consistent and substantial performance improvements,\nproving that the proposed method of enhancing low-resource ASR through a\nversatile TTS model is highly effective and has broad application prospects.\nFurthermore, we delve deeper into key characteristics of synthesized speech\ndata that contribute to ASR improvement, examining factors such as text\ndiversity, speaker diversity, and the volume of synthesized data, with text\ndiversity being studied for the first time in this work. We hope our findings\nprovide helpful guidance and reference for the practical application of\nTTS-based data augmentation and push the advancement of low-resource ASR one\nstep further.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16726v1",
    "published_date": "2024-10-22 06:25:16 UTC",
    "updated_date": "2024-10-22 06:25:16 UTC"
  },
  {
    "arxiv_id": "2410.16723v1",
    "title": "Resource-Efficient Sensor Fusion via System-Wide Dynamic Gated Neural Networks",
    "authors": [
      "Chetna Singhal",
      "Yashuo Wu",
      "Francesco Malandrino",
      "Sharon Ladron de Guevara Contreras",
      "Marco Levorato",
      "Carla Fabiana Chiasserini"
    ],
    "abstract": "Mobile systems will have to support multiple AI-based applications, each\nleveraging heterogeneous data sources through DNN architectures collaboratively\nexecuted within the network. To minimize the cost of the AI inference task\nsubject to requirements on latency, quality, and - crucially - reliability of\nthe inference process, it is vital to optimize (i) the set of sensors/data\nsources and (ii) the DNN architecture, (iii) the network nodes executing\nsections of the DNN, and (iv) the resources to use. To this end, we leverage\ndynamic gated neural networks with branches, and propose a novel algorithmic\nstrategy called Quantile-constrained Inference (QIC), based upon\nquantile-Constrained policy optimization. QIC makes joint, high-quality, swift\ndecisions on all the above aspects of the system, with the aim to minimize\ninference energy cost. We remark that this is the first contribution connecting\ngated dynamic DNNs with infrastructure-level decision making. We evaluate QIC\nusing a dynamic gated DNN with stems and branches for optimal sensor fusion and\ninference, trained on the RADIATE dataset offering Radar, LiDAR, and Camera\ndata, and real-world wireless measurements. Our results confirm that QIC\nmatches the optimum and outperforms its alternatives by over 80%.",
    "categories": [
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16723v1",
    "published_date": "2024-10-22 06:12:04 UTC",
    "updated_date": "2024-10-22 06:12:04 UTC"
  },
  {
    "arxiv_id": "2410.16713v4",
    "title": "Collapse or Thrive? Perils and Promises of Synthetic Data in a Self-Generating World",
    "authors": [
      "Joshua Kazdan",
      "Rylan Schaeffer",
      "Apratim Dey",
      "Matthias Gerstgrasser",
      "Rafael Rafailov",
      "David L. Donoho",
      "Sanmi Koyejo"
    ],
    "abstract": "What happens when generative machine learning models are pretrained on\nweb-scale datasets containing data generated by earlier models? Some prior work\nwarns of \"model collapse\" as the web is overwhelmed by synthetic data; other\nwork suggests the problem can be contained (i.e. collapse can be avoided) by\nmanaging how available data are used in pretraining. In this paper, we report\nexperiments on three ways of using data (training-workflows), across three\ngenerative model task-settings (multivariate Gaussian estimation, kernel\ndensity estimation, and language-model fine-tuning) to further confirm the\npossibility of containment: (a) we confirm that the training-workflow of {\\it\nreplacing} all real data by successive generations of purely synthetic data\nindeed suffers model collapse in all task-settings studied; (b) we consider the\ntraining-workflow of {\\it accumulating} synthetic data alongside real data and\ntraining on all data combined and confirming that, although the proportion of\nreal data eventually becomes zero, models remain stable and their test losses\ndo not diverge under this training-workflow; (c) we consider a\ntraining-workflow where real and synthetic data accumulate together but\nsuccessive generations of pretraining are constrained to use fixed-size data\nsubsets each generation. In this workflow, we observe slow and gradual rather\nthan explosive degradation of test loss performance across generations. Our\ninsights are particularly important when forecasting whether future frontier\ngenerative models will collapse or thrive, and our results open avenues for\nempirically and mathematically studying the context-dependent value of\nsynthetic data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2024 Workshops: Mathematics of Modern Machine\n  Learning (M3L) and Attributing Model Behavior at Scale (ATTRIB)",
    "pdf_url": "http://arxiv.org/pdf/2410.16713v4",
    "published_date": "2024-10-22 05:49:24 UTC",
    "updated_date": "2025-03-17 21:14:46 UTC"
  },
  {
    "arxiv_id": "2410.16711v1",
    "title": "Development of CNN Architectures using Transfer Learning Methods for Medical Image Classification",
    "authors": [
      "Ganga Prasad Basyal",
      "David Zeng",
      "Bhaskar Pm Rimal"
    ],
    "abstract": "The application of deep learning-based architecture has seen a tremendous\nrise in recent years. For example, medical image classification using deep\nlearning achieved breakthrough results. Convolutional Neural Networks (CNNs)\nare implemented predominantly in medical image classification and segmentation.\nOn the other hand, transfer learning has emerged as a prominent supporting tool\nfor enhancing the efficiency and accuracy of deep learning models. This paper\ninvestigates the development of CNN architectures using transfer learning\ntechniques in the field of medical image classification using a timeline\nmapping model for key image classification challenges. Our findings help make\nan informed decision while selecting the optimum and state-of-the-art CNN\narchitectures.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16711v1",
    "published_date": "2024-10-22 05:37:51 UTC",
    "updated_date": "2024-10-22 05:37:51 UTC"
  },
  {
    "arxiv_id": "2410.16710v1",
    "title": "Influential Language Data Selection via Gradient Trajectory Pursuit",
    "authors": [
      "Zhiwei Deng",
      "Tao Li",
      "Yang Li"
    ],
    "abstract": "Curating a desirable dataset for training has been the core of building\nhighly capable large language models (Touvron et al., 2023; Achiam et al.,\n2023; Team et al.,2024). Gradient influence scores (Pruthi et al., 2020; Xia et\nal., 2024) are shown to be correlated with model performance and are commonly\nused as the criterion for data selection. However, existing methods are built\nupon either individual sample rankings or inefficient matching process, leading\nto suboptimal performance or scaling up issues.In this paper, we propose\nGradient Trajectory Pursuit (GTP), an algorithm that performs pursuit of\ngradient trajectories via jointly selecting data points under an L0-norm\nregularized objective. The proposed algorithm highlights: (1) joint selection\ninstead of independent top-k selection, which automatically de-duplicates\nsamples; (2) higher efficiency with compressive sampling processes, which can\nbe further sped up using a distributed framework. In the experiments, we\ndemonstrate the algorithm in both in-domain and target-domain selection\nbenchmarks and show that it outperforms top-k selection and competitive\nalgorithms consistently, for example, our algorithm chooses as low as 0.5% data\nto achieve full performance on the targeted instruction tuning tasks",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16710v1",
    "published_date": "2024-10-22 05:32:40 UTC",
    "updated_date": "2024-10-22 05:32:40 UTC"
  },
  {
    "arxiv_id": "2410.16709v1",
    "title": "Universal approximation property of ODENet and ResNet with a single activation function",
    "authors": [
      "Masato Kimura",
      "Kazunori Matsui",
      "Yosuke Mizuno"
    ],
    "abstract": "We study a universal approximation property of ODENet and ResNet. The ODENet\nis a map from an initial value to the final value of an ODE system in a finite\ninterval. It is considered a mathematical model of a ResNet-type deep learning\nsystem. We consider dynamical systems with vector fields given by a single\ncomposition of the activation function and an affine mapping, which is the most\ncommon choice of the ODENet or ResNet vector field in actual machine learning\nsystems. We show that such an ODENet and ResNet with a restricted vector field\ncan uniformly approximate ODENet with a general vector field.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA",
      "stat.ML",
      "37M15, 41A46, 41A63, 65L12, 68T07, 65P99"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.16709v1",
    "published_date": "2024-10-22 05:27:01 UTC",
    "updated_date": "2024-10-22 05:27:01 UTC"
  },
  {
    "arxiv_id": "2410.16705v1",
    "title": "Privacy-hardened and hallucination-resistant synthetic data generation with logic-solvers",
    "authors": [
      "Mark A. Burgess",
      "Brendan Hosking",
      "Roc Reguant",
      "Anubhav Kaphle",
      "Mitchell J. O'Brien",
      "Letitia M. F. Sng",
      "Yatish Jain",
      "Denis C. Bauer"
    ],
    "abstract": "Machine-generated data is a valuable resource for training Artificial\nIntelligence algorithms, evaluating rare workflows, and sharing data under\nstricter data legislations. The challenge is to generate data that is accurate\nand private. Current statistical and deep learning methods struggle with large\ndata volumes, are prone to hallucinating scenarios incompatible with reality,\nand seldom quantify privacy meaningfully. Here we introduce Genomator, a logic\nsolving approach (SAT solving), which efficiently produces private and\nrealistic representations of the original data. We demonstrate the method on\ngenomic data, which arguably is the most complex and private information.\nSynthetic genomes hold great potential for balancing underrepresented\npopulations in medical research and advancing global data exchange. We\nbenchmark Genomator against state-of-the-art methodologies (Markov generation,\nRestricted Boltzmann Machine, Generative Adversarial Network and Conditional\nRestricted Boltzmann Machines), demonstrating an 84-93% accuracy improvement\nand 95-98% higher privacy. Genomator is also 1000-1600 times more efficient,\nmaking it the only tested method that scales to whole genomes. We show the\nuniversal trade-off between privacy and accuracy, and use Genomator's tuning\ncapability to cater to all applications along the spectrum, from provable\nprivate representations of sensitive cohorts, to datasets with\nindistinguishable pharmacogenomic profiles. Demonstrating the production-scale\ngeneration of tuneable synthetic data can increase trust and pave the way into\nthe clinic.",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16705v1",
    "published_date": "2024-10-22 05:20:21 UTC",
    "updated_date": "2024-10-22 05:20:21 UTC"
  },
  {
    "arxiv_id": "2410.16703v1",
    "title": "PLDR-LLM: Large Language Model from Power Law Decoder Representations",
    "authors": [
      "Burc Gokden"
    ],
    "abstract": "We present the Large Language Model from Power Law Decoder Representations\n(PLDR-LLM), a language model that leverages non-linear and linear\ntransformations through Power Law Graph Attention mechanism to generate\nwell-defined deductive and inductive outputs. We pretrain the PLDR-LLMs of\nvarying layer sizes with a small batch size of 32 and $\\sim$8B tokens from the\nRefinedWeb dataset, and show that they achieve competitive performance in\nzero-shot and few-shot settings compared to scaled dot-product LLMs of similar\nmodel size reported in the literature. We show that deductive outputs of\nPLDR-LLMs can be used to compare model characteristics or improve the\nperformance by introducing the Directed Acyclic Graph (DAG) loss as a metric\nand regularizer. Our results indicate that the initial maximum learning rate\nand warm-up steps have a lasting impact on deductive outputs throughout the\npretraining. We provide a detailed description of PLDR-LLM architecture, its\nimplementation and the pretraining procedure.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 4 figures, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.16703v1",
    "published_date": "2024-10-22 05:16:19 UTC",
    "updated_date": "2024-10-22 05:16:19 UTC"
  },
  {
    "arxiv_id": "2410.16700v2",
    "title": "AskBeacon -- Performing genomic data exchange and analytics with natural language",
    "authors": [
      "Anuradha Wickramarachchi",
      "Shakila Tonni",
      "Sonali Majumdar",
      "Sarvnaz Karimi",
      "Sulev Kõks",
      "Brendan Hosking",
      "Jordi Rambla",
      "Natalie A. Twine",
      "Yatish Jain",
      "Denis C. Bauer"
    ],
    "abstract": "Enabling clinicians and researchers to directly interact with global genomic\ndata resources by removing technological barriers is vital for medical\ngenomics. AskBeacon enables Large Language Models to be applied to securely\nshared cohorts via the GA4GH Beacon protocol. By simply \"asking\" Beacon,\nactionable insights can be gained, analyzed and made publication-ready.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "q-bio.GN"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16700v2",
    "published_date": "2024-10-22 05:11:54 UTC",
    "updated_date": "2024-10-23 02:29:24 UTC"
  },
  {
    "arxiv_id": "2410.16699v2",
    "title": "Graph Transformers Dream of Electric Flow",
    "authors": [
      "Xiang Cheng",
      "Lawrence Carin",
      "Suvrit Sra"
    ],
    "abstract": "We show theoretically and empirically that the linear Transformer, when\napplied to graph data, can implement algorithms that solve canonical problems\nsuch as electric flow and eigenvector decomposition. The Transformer has access\nto information on the input graph only via the graph's incidence matrix. We\npresent explicit weight configurations for implementing each algorithm, and we\nbound the constructed Transformers' errors by the errors of the underlying\nalgorithms. Our theoretical findings are corroborated by experiments on\nsynthetic data. Additionally, on a real-world molecular regression task, we\nobserve that the linear Transformer is capable of learning a more effective\npositional encoding than the default one based on Laplacian eigenvectors. Our\nwork is an initial step towards elucidating the inner-workings of the\nTransformer for graph data. Code is available at\nhttps://github.com/chengxiang/LinearGraphTransformer",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16699v2",
    "published_date": "2024-10-22 05:11:45 UTC",
    "updated_date": "2025-03-02 14:18:13 UTC"
  },
  {
    "arxiv_id": "2410.16695v2",
    "title": "MPT: A Large-scale Multi-Phytoplankton Tracking Benchmark",
    "authors": [
      "Yang Yu",
      "Yuezun Li",
      "Xin Sun",
      "Junyu Dong"
    ],
    "abstract": "Phytoplankton are a crucial component of aquatic ecosystems, and effective\nmonitoring of them can provide valuable insights into ocean environments and\necosystem changes. Traditional phytoplankton monitoring methods are often\ncomplex and lack timely analysis. Therefore, deep learning algorithms offer a\npromising approach for automated phytoplankton monitoring. However, the lack of\nlarge-scale, high-quality training samples has become a major bottleneck in\nadvancing phytoplankton tracking. In this paper, we propose a challenging\nbenchmark dataset, Multiple Phytoplankton Tracking (MPT), which covers diverse\nbackground information and variations in motion during observation. The dataset\nincludes 27 species of phytoplankton and zooplankton, 14 different backgrounds\nto simulate diverse and complex underwater environments, and a total of 140\nvideos. To enable accurate real-time observation of phytoplankton, we introduce\na multi-object tracking method, Deviation-Corrected Multi-Scale Feature Fusion\nTracker(DSFT), which addresses issues such as focus shifts during tracking and\nthe loss of small target information when computing frame-to-frame similarity.\nSpecifically, we introduce an additional feature extractor to predict the\nresiduals of the standard feature extractor's output, and compute multi-scale\nframe-to-frame similarity based on features from different layers of the\nextractor. Extensive experiments on the MPT have demonstrated the validity of\nthe dataset and the superiority of DSFT in tracking phytoplankton, providing an\neffective solution for phytoplankton monitoring.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16695v2",
    "published_date": "2024-10-22 04:57:28 UTC",
    "updated_date": "2025-01-04 13:58:00 UTC"
  },
  {
    "arxiv_id": "2410.16676v4",
    "title": "CausalEval: Towards Better Causal Reasoning in Language Models",
    "authors": [
      "Longxuan Yu",
      "Delin Chen",
      "Siheng Xiong",
      "Qingyang Wu",
      "Qingzhen Liu",
      "Dawei Li",
      "Zhikai Chen",
      "Xiaoze Liu",
      "Liangming Pan"
    ],
    "abstract": "Causal reasoning (CR) is a crucial aspect of intelligence, essential for\nproblem-solving, decision-making, and understanding the world. While language\nmodels (LMs) can generate rationales for their outputs, their ability to\nreliably perform causal reasoning remains uncertain, often falling short in\ntasks requiring a deep understanding of causality. In this paper, we introduce\nCausalEval, a comprehensive review of research aimed at enhancing LMs for\ncausal reasoning, coupled with an empirical evaluation of current models and\nmethods. We categorize existing methods based on the role of LMs: either as\nreasoning engines or as helpers providing knowledge or data to traditional CR\nmethods, followed by a detailed discussion of methodologies in each category.\nWe then assess the performance of current LMs and various enhancement methods\non a range of causal reasoning tasks, providing key findings and in-depth\nanalysis. Finally, we present insights from current studies and highlight\npromising directions for future research. We aim for this work to serve as a\ncomprehensive resource, fostering further advancements in causal reasoning with\nLMs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to NAACL25 (main)",
    "pdf_url": "http://arxiv.org/pdf/2410.16676v4",
    "published_date": "2024-10-22 04:18:19 UTC",
    "updated_date": "2025-02-17 20:16:29 UTC"
  },
  {
    "arxiv_id": "2410.16672v1",
    "title": "DEAN: Deactivating the Coupled Neurons to Mitigate Fairness-Privacy Conflicts in Large Language Models",
    "authors": [
      "Chen Qian",
      "Dongrui Liu",
      "Jie Zhang",
      "Yong Liu",
      "Jing Shao"
    ],
    "abstract": "Ensuring awareness of fairness and privacy in Large Language Models (LLMs) is\ncritical. Interestingly, we discover a counter-intuitive trade-off phenomenon\nthat enhancing an LLM's privacy awareness through Supervised Fine-Tuning (SFT)\nmethods significantly decreases its fairness awareness with thousands of\nsamples. To address this issue, inspired by the information theory, we\nintroduce a training-free method to \\textbf{DEA}ctivate the fairness and\nprivacy coupled \\textbf{N}eurons (\\textbf{DEAN}), which theoretically and\nempirically decrease the mutual information between fairness and privacy\nawareness. Extensive experimental results demonstrate that DEAN eliminates the\ntrade-off phenomenon and significantly improves LLMs' fairness and privacy\nawareness simultaneously, \\eg improving Qwen-2-7B-Instruct's fairness awareness\nby 12.2\\% and privacy awareness by 14.0\\%. More crucially, DEAN remains robust\nand effective with limited annotated data or even when only malicious\nfine-tuning data is available, whereas SFT methods may fail to perform properly\nin such scenarios. We hope this study provides valuable insights into\nconcurrently addressing fairness and privacy concerns in LLMs and can be\nintegrated into comprehensive frameworks to develop more ethical and\nresponsible AI systems. Our code is available at\n\\url{https://github.com/ChnQ/DEAN}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16672v1",
    "published_date": "2024-10-22 04:08:27 UTC",
    "updated_date": "2024-10-22 04:08:27 UTC"
  },
  {
    "arxiv_id": "2410.16670v1",
    "title": "CoPS: Empowering LLM Agents with Provable Cross-Task Experience Sharing",
    "authors": [
      "Chen Yang",
      "Chenyang Zhao",
      "Quanquan Gu",
      "Dongruo Zhou"
    ],
    "abstract": "Sequential reasoning in agent systems has been significantly advanced by\nlarge language models (LLMs), yet existing approaches face limitations.\nReflection-driven reasoning relies solely on knowledge in pretrained models,\nlimiting performance in novel scenarios, while experience-assisted reasoning\noften depends on external experiences and lacks clear principles for selecting\nrepresentative experiences. We address these limitations by proposing CoPS\n(Cross-Task Experience Sharing), a generalizable algorithm that enhances\nsequential reasoning by cross-task experience sharing and selection. In detail,\nCoPS leverages agents' experiences on previous tasks, selecting\ndistribution-matched experiences via a provable pessimism-based strategy to\nmaximize utility while minimizing risks from distribution shifts. Extensive\nexperimental results on benchmarks like Alfworld, Webshop, and HotPotQA\ndemonstrate that CoPS consistently outperforms state-of-the-art baselines, with\nsuperior sample efficiency suitable for resource-constrained scenarios.\nTheoretically, we show that the performance of our algorithm depends on both\nthe quality of the pretrained LLM and the matching between the agent's\ntask-dependent trial distribution and that generated by the LLM. Our work\nbridges the gap between existing sequential reasoning paradigms and validates\nthe effectiveness of leveraging cross-task experiences, shedding light on the\npotential to improve agents' generalization and adaptability across diverse\ntasks. Our codes are available at\n$\\href{https://github.com/uclaml/COPS}{\\text{https://github.com/uclaml/COPS}}$.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 5 tables, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.16670v1",
    "published_date": "2024-10-22 03:59:53 UTC",
    "updated_date": "2024-10-22 03:59:53 UTC"
  },
  {
    "arxiv_id": "2410.16668v3",
    "title": "Satori: Towards Proactive AR Assistant with Belief-Desire-Intention User Modeling",
    "authors": [
      "Chenyi Li",
      "Guande Wu",
      "Gromit Yeuk-Yin Chan",
      "Dishita G Turakhia",
      "Sonia Castelo Quispe",
      "Dong Li",
      "Leslie Welch",
      "Claudio Silva",
      "Jing Qian"
    ],
    "abstract": "Augmented Reality (AR) assistance is increasingly used for supporting users\nwith physical tasks like assembly and cooking. However, most systems rely on\nreactive responses triggered by user input, overlooking rich contextual and\nuser-specific information. To address this, we present Satori, a novel AR\nsystem that proactively guides users by modeling both -- their mental states\nand environmental contexts. Satori integrates the Belief-Desire-Intention (BDI)\nframework with the state-of-the-art multi-modal large language model (LLM) to\ndeliver contextually appropriate guidance. Our system is designed based on two\nformative studies involving twelve experts. We evaluated the system with a\nsixteen within-subject study and found that Satori matches the performance of\ndesigner-created Wizard-of-Oz (WoZ) systems, without manual configurations or\nheuristics, thereby improving generalizability, reusability, and expanding the\npotential of AR assistance.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16668v3",
    "published_date": "2024-10-22 03:53:46 UTC",
    "updated_date": "2025-03-31 03:31:22 UTC"
  },
  {
    "arxiv_id": "2410.19009v1",
    "title": "Dual Space Training for GANs: A Pathway to Efficient and Creative Generative Models",
    "authors": [
      "Beka Modrekiladze"
    ],
    "abstract": "Generative Adversarial Networks (GANs) have demonstrated remarkable\nadvancements in generative modeling; however, their training is often\nresource-intensive, requiring extensive computational time and hundreds of\nthousands of epochs. This paper proposes a novel optimization approach that\ntransforms the training process by operating within a dual space of the initial\ndata using invertible mappings, specifically autoencoders. By training GANs on\nthe encoded representations in the dual space, which encapsulate the most\nsalient features of the data, the generative process becomes significantly more\nefficient and potentially reveals underlying patterns beyond human recognition.\nThis approach not only enhances training speed and resource usage but also\nexplores the philosophical question of whether models can generate insights\nthat transcend the human intelligence while being limited by the\nhuman-generated data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.19009v1",
    "published_date": "2024-10-22 03:44:13 UTC",
    "updated_date": "2024-10-22 03:44:13 UTC"
  },
  {
    "arxiv_id": "2410.16662v1",
    "title": "Visual Question Answering in Ophthalmology: A Progressive and Practical Perspective",
    "authors": [
      "Xiaolan Chen",
      "Ruoyu Chen",
      "Pusheng Xu",
      "Weiyi Zhang",
      "Xianwen Shang",
      "Mingguang He",
      "Danli Shi"
    ],
    "abstract": "Accurate diagnosis of ophthalmic diseases relies heavily on the\ninterpretation of multimodal ophthalmic images, a process often time-consuming\nand expertise-dependent. Visual Question Answering (VQA) presents a potential\ninterdisciplinary solution by merging computer vision and natural language\nprocessing to comprehend and respond to queries about medical images. This\nreview article explores the recent advancements and future prospects of VQA in\nophthalmology from both theoretical and practical perspectives, aiming to\nprovide eye care professionals with a deeper understanding and tools for\nleveraging the underlying models. Additionally, we discuss the promising trend\nof large language models (LLM) in enhancing various components of the VQA\nframework to adapt to multimodal ophthalmic tasks. Despite the promising\noutlook, ophthalmic VQA still faces several challenges, including the scarcity\nof annotated multimodal image datasets, the necessity of comprehensive and\nunified evaluation methods, and the obstacles to achieving effective real-world\napplications. This article highlights these challenges and clarifies future\ndirections for advancing ophthalmic VQA with LLMs. The development of LLM-based\nophthalmic VQA systems calls for collaborative efforts between medical\nprofessionals and AI experts to overcome existing obstacles and advance the\ndiagnosis and care of eye diseases.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16662v1",
    "published_date": "2024-10-22 03:28:41 UTC",
    "updated_date": "2024-10-22 03:28:41 UTC"
  },
  {
    "arxiv_id": "2410.16659v1",
    "title": "RKadiyala at SemEval-2024 Task 8: Black-Box Word-Level Text Boundary Detection in Partially Machine Generated Texts",
    "authors": [
      "Ram Mohan Rao Kadiyala"
    ],
    "abstract": "With increasing usage of generative models for text generation and widespread\nuse of machine generated texts in various domains, being able to distinguish\nbetween human written and machine generated texts is a significant challenge.\nWhile existing models and proprietary systems focus on identifying whether\ngiven text is entirely human written or entirely machine generated, only a few\nsystems provide insights at sentence or paragraph level at likelihood of being\nmachine generated at a non reliable accuracy level, working well only for a set\nof domains and generators. This paper introduces few reliable approaches for\nthe novel task of identifying which part of a given text is machine generated\nat a word level while comparing results from different approaches and methods.\nWe present a comparison with proprietary systems , performance of our model on\nunseen domains' and generators' texts. The findings reveal significant\nimprovements in detection accuracy along with comparison on other aspects of\ndetection capabilities. Finally we discuss potential avenues for improvement\nand implications of our work. The proposed model is also well suited for\ndetecting which parts of a text are machine generated in outputs of Instruct\nvariants of many LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "published at naacl 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.16659v1",
    "published_date": "2024-10-22 03:21:59 UTC",
    "updated_date": "2024-10-22 03:21:59 UTC"
  },
  {
    "arxiv_id": "2410.16655v1",
    "title": "Semantic-guided Search for Efficient Program Repair with Large Language Models",
    "authors": [
      "Thanh Le-Cong",
      "Bach Le",
      "Toby Murray"
    ],
    "abstract": "In this paper, we first show that increases in beam size of even just\nsmall-sized LLM (1B-7B parameters) require an extensive GPU resource\nconsumption, leading to up to 80% of recurring crashes due to memory overloads\nin LLM-based APR. Seemingly simple solutions to reduce memory consumption are\n(1) to quantize LLM models, i.e., converting the weights of a LLM from\nhigh-precision values to lower-precision ones. and (2) to make beam search\nsequential, i.e., forwarding each beam through the model sequentially and then\nconcatenate them back into a single model output. However, we show that these\napproaches still do not work via both theoretical analysis and experiments. To\naddress this, we introduce FLAMES, a novel LLM-based APR technique that employs\nsemantic-guided patch generation to enhance repair effectiveness and memory\nefficiency. Unlike conventional methods that rely on beam search, FLAMES\nutilizes greedy decoding to enhance memory efficiency while steering the search\nto more potentially good repair candidates via a semantic-guided best-first\nsearch algorithm. At each decoding step, FLAMES uses semantic feedback from\ntest validation such as the number of passing and failing test cases to select\nthe most promising token to explore further. Our empirical evaluation on the\nDefects4J and HumanEval-Java datasets shows that FLAMES not only substantially\nreduces memory consumption by up to 83% compared to conventional LLM-based APR,\nbut also accelerates the repair process. Remarkably, FLAMES successfully\ngenerated 133 and 103 correct fixes for 333 and 163 bugs in the Defects4J and\nHumanEval-Java datasets, respectively. This suggests that FLAMES is not only\nmore efficient but also outperforms state-of-the-art techniques, fixing at\nleast 10 and 11 more bugs than SOTA baselines in the Defects4J and\nHumanEval-Java datasets, respectively.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16655v1",
    "published_date": "2024-10-22 02:59:47 UTC",
    "updated_date": "2024-10-22 02:59:47 UTC"
  },
  {
    "arxiv_id": "2410.16653v1",
    "title": "Enhancing Two-Player Performance Through Single-Player Knowledge Transfer: An Empirical Study on Atari 2600 Games",
    "authors": [
      "Kimiya Saadat",
      "Richard Zhao"
    ],
    "abstract": "Playing two-player games using reinforcement learning and self-play can be\nchallenging due to the complexity of two-player environments and the possible\ninstability in the training process. We propose that a reinforcement learning\nalgorithm can train more efficiently and achieve improved performance in a\ntwo-player game if it leverages the knowledge from the single-player version of\nthe same game. This study examines the proposed idea in ten different Atari\n2600 environments using the Atari 2600 RAM as the input state. We discuss the\nadvantages of using transfer learning from a single-player training process\nover training in a two-player setting from scratch, and demonstrate our results\nin a few measures such as training time and average total reward. We also\ndiscuss a method of calculating RAM complexity and its relationship to\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16653v1",
    "published_date": "2024-10-22 02:57:44 UTC",
    "updated_date": "2024-10-22 02:57:44 UTC"
  },
  {
    "arxiv_id": "2410.16647v1",
    "title": "GE2E-KWS: Generalized End-to-End Training and Evaluation for Zero-shot Keyword Spotting",
    "authors": [
      "Pai Zhu",
      "Jacob W. Bartel",
      "Dhruuv Agarwal",
      "Kurt Partridge",
      "Hyun Jin Park",
      "Quan Wang"
    ],
    "abstract": "We propose GE2E-KWS -- a generalized end-to-end training and evaluation\nframework for customized keyword spotting. Specifically, enrollment utterances\nare separated and grouped by keywords from the training batch and their\nembedding centroids are compared to all other test utterance embeddings to\ncompute the loss. This simulates runtime enrollment and verification stages,\nand improves convergence stability and training speed by optimizing matrix\noperations compared to SOTA triplet loss approaches. To benchmark different\nmodels reliably, we propose an evaluation process that mimics the production\nenvironment and compute metrics that directly measure keyword matching\naccuracy. Trained with GE2E loss, our 419KB quantized conformer model beats a\n7.5GB ASR encoder by 23.6% relative AUC, and beats a same size triplet loss\nmodel by 60.7% AUC. Our KWS models are natively streamable with low memory\nfootprints, and designed to continuously run on-device with no retraining\nneeded for new keywords (zero-shot).",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.AS",
    "comment": "8 pages, 6 figures, 2 tables The paper is accepted in IEEE Spoken\n  Language Technology (SLT) 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.16647v1",
    "published_date": "2024-10-22 02:45:59 UTC",
    "updated_date": "2024-10-22 02:45:59 UTC"
  },
  {
    "arxiv_id": "2410.16645v1",
    "title": "Chatting with Bots: AI, Speech Acts, and the Edge of Assertion",
    "authors": [
      "Iwan Williams",
      "Tim Bayne"
    ],
    "abstract": "This paper addresses the question of whether large language model-powered\nchatbots are capable of assertion. According to what we call the Thesis of\nChatbot Assertion (TCA), chatbots are the kinds of things that can assert, and\nat least some of the output produced by current-generation chatbots qualifies\nas assertion. We provide some motivation for TCA, arguing that it ought to be\ntaken seriously and not simply dismissed. We also review recent objections to\nTCA, arguing that these objections are weighty. We thus confront the following\ndilemma: how can we do justice to both the considerations for and against TCA?\nWe consider two influential responses to this dilemma - the first appeals to\nthe notion of proxy-assertion; the second appeals to fictionalism - and argue\nthat neither is satisfactory. Instead, reflecting on the ontogenesis of\nassertion, we argue that we need to make space for a category of\nproto-assertion. We then apply the category of proto-assertion to chatbots,\narguing that treating chatbots as proto-assertors provides a satisfactory\nresolution to the dilemma of chatbot assertion.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16645v1",
    "published_date": "2024-10-22 02:45:09 UTC",
    "updated_date": "2024-10-22 02:45:09 UTC"
  },
  {
    "arxiv_id": "2410.16644v1",
    "title": "CKSP: Cross-species Knowledge Sharing and Preserving for Universal Animal Activity Recognition",
    "authors": [
      "Axiu Mao",
      "Meilu Zhu",
      "Zhaojin Guo",
      "Zheng He",
      "Tomas Norton",
      "Kai Liu"
    ],
    "abstract": "Deep learning techniques are dominating automated animal activity recognition\n(AAR) tasks with wearable sensors due to their high performance on large-scale\nlabelled data. However, current deep learning-based AAR models are trained\nsolely on datasets of individual animal species, constraining their\napplicability in practice and performing poorly when training data are limited.\nIn this study, we propose a one-for-many framework, dubbed Cross-species\nKnowledge Sharing and Preserving (CKSP), based on sensor data of diverse animal\nspecies. Given the coexistence of generic and species-specific behavioural\npatterns among different species, we design a Shared-Preserved Convolution\n(SPConv) module. This module assigns an individual low-rank convolutional layer\nto each species for extracting species-specific features and employs a shared\nfull-rank convolutional layer to learn generic features, enabling the CKSP\nframework to learn inter-species complementarity and alleviating data\nlimitations via increasing data diversity. Considering the training conflict\narising from discrepancies in data distributions among species, we devise a\nSpecies-specific Batch Normalization (SBN) module, that involves multiple BN\nlayers to separately fit the distributions of different species. To validate\nCKSP's effectiveness, experiments are performed on three public datasets from\nhorses, sheep, and cattle, respectively. The results show that our approach\nremarkably boosts the classification performance compared to the baseline\nmethod (one-for-one framework) solely trained on individual-species data, with\nincrements of 6.04%, 2.06%, and 3.66% in accuracy, and 10.33%, 3.67%, and 7.90%\nin F1-score for the horse, sheep, and cattle datasets, respectively. This\nproves the promising capabilities of our method in leveraging multi-species\ndata to augment classification performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16644v1",
    "published_date": "2024-10-22 02:44:10 UTC",
    "updated_date": "2024-10-22 02:44:10 UTC"
  },
  {
    "arxiv_id": "2410.16638v3",
    "title": "LLMScan: Causal Scan for LLM Misbehavior Detection",
    "authors": [
      "Mengdi Zhang",
      "Kai Kiat Goh",
      "Peixin Zhang",
      "Jun Sun",
      "Rose Lin Xin",
      "Hongyu Zhang"
    ],
    "abstract": "Despite the success of Large Language Models (LLMs) across various fields,\ntheir potential to generate untruthful, biased and harmful responses poses\nsignificant risks, particularly in critical applications. This highlights the\nurgent need for systematic methods to detect and prevent such misbehavior.\nWhile existing approaches target specific issues such as harmful responses,\nthis work introduces LLMScan, an innovative LLM monitoring technique based on\ncausality analysis, offering a comprehensive solution. LLMScan systematically\nmonitors the inner workings of an LLM through the lens of causal inference,\noperating on the premise that the LLM's `brain' behaves differently when\nmisbehaving. By analyzing the causal contributions of the LLM's input tokens\nand transformer layers, LLMScan effectively detects misbehavior. Extensive\nexperiments across various tasks and models reveal clear distinctions in the\ncausal distributions between normal behavior and misbehavior, enabling the\ndevelopment of accurate, lightweight detectors for a variety of misbehavior\ndetection tasks.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16638v3",
    "published_date": "2024-10-22 02:27:57 UTC",
    "updated_date": "2025-05-18 16:55:15 UTC"
  },
  {
    "arxiv_id": "2410.16633v1",
    "title": "Graph-Structured Trajectory Extraction from Travelogues",
    "authors": [
      "Aitaro Yamamoto",
      "Hiroyuki Otomo",
      "Hiroki Ouchi",
      "Shohei Higashiyama",
      "Hiroki Teranishi",
      "Hiroyuki Shindo",
      "Taro Watanabe"
    ],
    "abstract": "Previous studies on sequence-based extraction of human movement trajectories\nhave an issue of inadequate trajectory representation. Specifically, a pair of\nlocations may not be lined up in a sequence especially when one location\nincludes the other geographically. In this study, we propose a graph\nrepresentation that retains information on the geographic hierarchy as well as\nthe temporal order of visited locations, and have constructed a benchmark\ndataset for graph-structured trajectory extraction. The experiments with our\nbaselines have demonstrated that it is possible to accurately predict visited\nlocations and the order among them, but it remains a challenge to predict the\nhierarchical relations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16633v1",
    "published_date": "2024-10-22 02:21:42 UTC",
    "updated_date": "2024-10-22 02:21:42 UTC"
  },
  {
    "arxiv_id": "2410.16624v1",
    "title": "EVC-MF: End-to-end Video Captioning Network with Multi-scale Features",
    "authors": [
      "Tian-Zi Niu",
      "Zhen-Duo Chen",
      "Xin Luo",
      "Xin-Shun Xu"
    ],
    "abstract": "Conventional approaches for video captioning leverage a variety of\noffline-extracted features to generate captions. Despite the availability of\nvarious offline-feature-extractors that offer diverse information from\ndifferent perspectives, they have several limitations due to fixed parameters.\nConcretely, these extractors are solely pre-trained on image/video\ncomprehension tasks, making them less adaptable to video caption datasets.\nAdditionally, most of these extractors only capture features prior to the\nclassifier of the pre-training task, ignoring a significant amount of valuable\nshallow information. Furthermore, employing multiple offline-features may\nintroduce redundant information. To address these issues, we propose an\nend-to-end encoder-decoder-based network (EVC-MF) for video captioning, which\nefficiently utilizes multi-scale visual and textual features to generate video\ndescriptions. Specifically, EVC-MF consists of three modules. Firstly, instead\nof relying on multiple feature extractors, we directly feed video frames into a\ntransformer-based network to obtain multi-scale visual features and update\nfeature extractor parameters. Secondly, we fuse the multi-scale features and\ninput them into a masked encoder to reduce redundancy and encourage learning\nuseful features. Finally, we utilize an enhanced transformer-based decoder,\nwhich can efficiently leverage shallow textual information, to generate video\ndescriptions. To evaluate our proposed model, we conduct extensive experiments\non benchmark datasets. The results demonstrate that EVC-MF yields competitive\nperformance compared with the state-of-theart methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16624v1",
    "published_date": "2024-10-22 02:16:02 UTC",
    "updated_date": "2024-10-22 02:16:02 UTC"
  },
  {
    "arxiv_id": "2411.05010v2",
    "title": "Scattered Forest Search: Smarter Code Space Exploration with LLMs",
    "authors": [
      "Jonathan Light",
      "Yue Wu",
      "Yiyou Sun",
      "Wenchao Yu",
      "Yanchi liu",
      "Xujiang Zhao",
      "Ziniu Hu",
      "Haifeng Chen",
      "Wei Cheng"
    ],
    "abstract": "We frame code generation as a black-box optimization problem within the code\nspace and demonstrate how optimization-inspired techniques can enhance\ninference scaling. Based on this perspective, we propose SCATTERED FOREST\nSEARCH (SFS), a novel approach that improves solution diversity and better\nexploits feedback during evolutionary search. Our theoretical analysis\nillustrates how these methods help avoid local optima during optimization,\nleading to more efficient exploration. Extensive experiments on HumanEval,\nMBPP, APPS, CodeContests, and Leetcode reveal significant performance gains.\nFor instance, our method achieves a pass@1 rate of 67.1% on HumanEval+ and\n87.2% on HumanEval with GPT-3.5, marking improvements of 8.6% and 4.3% over the\nstate-of-the-art, while also halving the iterations needed to find the correct\nsolution. Furthermore, our approach scales more efficiently than existing\nsearch techniques, including tree search, line search, and repeated sampling.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "I.2.2; I.2.6; I.2.8; D.2.3; D.2.5; D.1.2; F.2.2; G.1.6"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted at ICLR 2025 Conference",
    "pdf_url": "http://arxiv.org/pdf/2411.05010v2",
    "published_date": "2024-10-22 01:58:29 UTC",
    "updated_date": "2025-02-25 03:17:46 UTC"
  },
  {
    "arxiv_id": "2410.16613v1",
    "title": "Real-time Sub-milliwatt Epilepsy Detection Implemented on a Spiking Neural Network Edge Inference Processor",
    "authors": [
      "Ruixin Lia",
      "Guoxu Zhaoa",
      "Dylan Richard Muir",
      "Yuya Ling",
      "Karla Burelo",
      "Mina Khoei",
      "Dong Wang",
      "Yannan Xing",
      "Ning Qiao"
    ],
    "abstract": "Analyzing electroencephalogram (EEG) signals to detect the epileptic seizure\nstatus of a subject presents a challenge to existing technologies aimed at\nproviding timely and efficient diagnosis. In this study, we aimed to detect\ninterictal and ictal periods of epileptic seizures using a spiking neural\nnetwork (SNN). Our proposed approach provides an online and real-time\npreliminary diagnosis of epileptic seizures and helps to detect possible\npathological conditions.To validate our approach, we conducted experiments\nusing multiple datasets. We utilized a trained SNN to identify the presence of\nepileptic seizures and compared our results with those of related studies. The\nSNN model was deployed on Xylo, a digital SNN neuromorphic processor designed\nto process temporal signals. Xylo efficiently simulates spiking leaky\nintegrate-and-fire neurons with exponential input synapses. Xylo has much lower\nenergy requirments than traditional approaches to signal processing, making it\nan ideal platform for developing low-power seizure detection systems.Our\nproposed method has a high test accuracy of 93.3% and 92.9% when classifying\nictal and interictal periods. At the same time, the application has an average\npower consumption of 87.4 uW(IO power) + 287.9 uW(computational power) when\ndeployed to Xylo. Our method demonstrates excellent low-latency performance\nwhen tested on multiple datasets. Our work provides a new solution for seizure\ndetection, and it is expected to be widely used in portable and wearable\ndevices in the future.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "q-bio.NC"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16613v1",
    "published_date": "2024-10-22 01:55:02 UTC",
    "updated_date": "2024-10-22 01:55:02 UTC"
  },
  {
    "arxiv_id": "2410.16606v1",
    "title": "GALA: Graph Diffusion-based Alignment with Jigsaw for Source-free Domain Adaptation",
    "authors": [
      "Junyu Luo",
      "Yiyang Gu",
      "Xiao Luo",
      "Wei Ju",
      "Zhiping Xiao",
      "Yusheng Zhao",
      "Jingyang Yuan",
      "Ming Zhang"
    ],
    "abstract": "Source-free domain adaptation is a crucial machine learning topic, as it\ncontains numerous applications in the real world, particularly with respect to\ndata privacy. Existing approaches predominantly focus on Euclidean data, such\nas images and videos, while the exploration of non-Euclidean graph data remains\nscarce. Recent graph neural network (GNN) approaches can suffer from serious\nperformance decline due to domain shift and label scarcity in source-free\nadaptation scenarios. In this study, we propose a novel method named Graph\nDiffusion-based Alignment with Jigsaw (GALA), tailored for source-free graph\ndomain adaptation. To achieve domain alignment, GALA employs a graph diffusion\nmodel to reconstruct source-style graphs from target data. Specifically, a\nscore-based graph diffusion model is trained using source graphs to learn the\ngenerative source styles. Then, we introduce perturbations to target graphs via\na stochastic differential equation instead of sampling from a prior, followed\nby the reverse process to reconstruct source-style graphs. We feed the\nsource-style graphs into an off-the-shelf GNN and introduce class-specific\nthresholds with curriculum learning, which can generate accurate and unbiased\npseudo-labels for target graphs. Moreover, we develop a simple yet effective\ngraph-mixing strategy named graph jigsaw to combine confident graphs and\nunconfident graphs, which can enhance generalization capabilities and\nrobustness via consistency learning. Extensive experiments on benchmark\ndatasets validate the effectiveness of GALA.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "IEEE TPAMI",
    "pdf_url": "http://arxiv.org/pdf/2410.16606v1",
    "published_date": "2024-10-22 01:32:46 UTC",
    "updated_date": "2024-10-22 01:32:46 UTC"
  },
  {
    "arxiv_id": "2410.16600v2",
    "title": "Convex Markov Games: A Framework for Creativity, Imitation, Fairness, and Safety in Multiagent Learning",
    "authors": [
      "Ian Gemp",
      "Andreas Haupt",
      "Luke Marris",
      "Siqi Liu",
      "Georgios Piliouras"
    ],
    "abstract": "Behavioral diversity, expert imitation, fairness, safety goals and others\ngive rise to preferences in sequential decision making domains that do not\ndecompose additively across time. We introduce the class of convex Markov games\nthat allow general convex preferences over occupancy measures. Despite infinite\ntime horizon and strictly higher generality than Markov games, pure strategy\nNash equilibria exist. Furthermore, equilibria can be approximated empirically\nby performing gradient descent on an upper bound of exploitability. Our\nexperiments reveal novel solutions to classic repeated normal-form games, find\nfair solutions in a repeated asymmetric coordination game, and prioritize safe\nlong-term behavior in a robot warehouse environment. In the prisoner's dilemma,\nour algorithm leverages transient imitation to find a policy profile that\ndeviates from observed human play only slightly, yet achieves higher per-player\nutility while also being three orders of magnitude less exploitable.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16600v2",
    "published_date": "2024-10-22 00:55:04 UTC",
    "updated_date": "2025-01-16 16:42:59 UTC"
  },
  {
    "arxiv_id": "2410.16593v3",
    "title": "Graph Sampling for Scalable and Expressive Graph Neural Networks on Homophilic Graphs",
    "authors": [
      "Haolin Li",
      "Haoyu Wang",
      "Luana Ruiz"
    ],
    "abstract": "Graph Neural Networks (GNNs) excel in many graph machine learning tasks but\nface challenges when scaling to large networks. GNN transferability allows\ntraining on smaller graphs and applying the model to larger ones, but existing\nmethods often rely on random subsampling, leading to disconnected subgraphs and\nreduced model expressivity. We propose a novel graph sampling algorithm that\nleverages feature homophily to preserve graph structure. By minimizing the\ntrace of the data correlation matrix, our method better preserves the graph\nLaplacian trace -- a proxy for the graph connectivity -- than random sampling,\nwhile achieving lower complexity than spectral methods. Experiments on citation\nnetworks show improved performance in preserving Laplacian trace and GNN\ntransferability compared to random sampling.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16593v3",
    "published_date": "2024-10-22 00:30:31 UTC",
    "updated_date": "2025-03-27 21:40:10 UTC"
  },
  {
    "arxiv_id": "2410.16589v1",
    "title": "Dynamic Adaptive Rank Space Exploration for Efficient Sentiment Analysis with Large Language Models",
    "authors": [
      "Hongcheng Ding",
      "Fuzhen Hu",
      "Xuanze Zhao",
      "Zixiao Jiang",
      "Shamsul Nahar Abdullah",
      "Deshinta Arrova Dewi"
    ],
    "abstract": "Sentiment analysis has become increasingly important for assessing public\nopinion and informing decision-making. Large language models (LLMs) have\nrevolutionized this field by capturing nuanced language patterns. However,\nadapting LLMs to domain-specific sentiment analysis tasks remains challenging\ndue to computational constraints and the need for optimal fine-tuning. To\naddress these challenges, we propose a novel Dynamic Adaptive Rank Space\nExploration (DARSE) framework for efficient and effective sentiment analysis\nusing LLMs. DARSE consists of a coarse-grained greedy algorithm to identify the\noptimal rank range, a fine-grained exploration algorithm to refine rank\nselection, and a dynamic rank allocation method to determine the optimal rank\ncombination for each LLM layer. Extensive experiments demonstrate that DARSE\nsignificantly improves sentiment analysis accuracy, achieving a 15.1%\nimprovement in MSE and a 4.3% improvement in accuracy compared to previous\nwork. Our framework strikes a balance between computational efficiency and\nmodel performance, making it a promising approach for sentiment analysis with\nLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16589v1",
    "published_date": "2024-10-22 00:14:36 UTC",
    "updated_date": "2024-10-22 00:14:36 UTC"
  },
  {
    "arxiv_id": "2410.16586v1",
    "title": "Optimizing LLMs with Direct Preferences: A Data Efficiency Perspective",
    "authors": [
      "Pietro Bernardelle",
      "Gianluca Demartini"
    ],
    "abstract": "Aligning the output of Large Language Models (LLMs) with human preferences\n(e.g., by means of reinforcement learning with human feedback, or RLHF) is\nessential for ensuring their effectiveness in real-world scenarios. Despite\nsignificant advancements in LLM alignment techniques, the impact of different\ntype of preference data on model performance has yet to be systematically\nexplored. In this study, we investigate the scalability, data efficiency, and\neffectiveness of Direct Preference Optimization (DPO) in fine-tuning\npre-trained LLMs, aiming to reduce their dependency on extensive amounts of\npreference data, which is expensive to collect. We (1) systematically compare\nthe performance of models fine-tuned with varying percentages of a combined\npreference judgement dataset to define the improvement curve of DPO and assess\nits effectiveness in data-constrained environments; and (2) provide insights\nfor the development of an optimal approach for selective preference data usage.\nOur study reveals that increasing the amount of data used for training\ngenerally enhances and stabilizes model performance. Moreover, the use of a\ncombination of diverse datasets significantly improves model effectiveness.\nFurthermore, when models are trained separately using different types of\nprompts, models trained with conversational prompts outperformed those trained\nwith question answering prompts.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16586v1",
    "published_date": "2024-10-22 00:11:41 UTC",
    "updated_date": "2024-10-22 00:11:41 UTC"
  }
]
[
  {
    "arxiv_id": "2511.06161v1",
    "title": "LLM Attention Transplant for Transfer Learning of Tabular Data Across Disparate Domains",
    "authors": [
      "Ibna Kowsar",
      "Kazi F. Akhter",
      "Manar D. Samad"
    ],
    "abstract": "Transfer learning of tabular data is non-trivial due to heterogeneity in the feature space across disparate domains. The limited success of traditional deep learning in tabular knowledge transfer can be advanced by leveraging large language models (LLMs). However, the efficacy of LLMs often stagnates for mixed data types structured in tables due to the limitations of text prompts and in-context learning. We propose a lightweight transfer learning framework that fine-tunes an LLM using source tabular data and transplants the LLM's selective $key$ and $value$ projection weights into a gated feature tokenized transformer (gFTT) built for tabular data. The gFTT model with cross-domain attention is fine-tuned using target tabular data for transfer learning, eliminating the need for shared features, LLM prompt engineering, and large-scale pretrained models. Our experiments using ten pairs of source-target data sets and 12 baselines demonstrate the superiority of the proposed LLM-attention transplant for transfer learning (LATTLE) method over traditional ML models, state-of-the-art deep tabular architectures, and transfer learning models trained on thousands to billions of tabular samples. The proposed attention transfer demonstrates an effective solution to learning relationships between data tables using an LLM in a low-resource learning environment. The source code for the proposed method is publicly available.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.06161v1",
    "published_date": "2025-11-08 23:05:31 UTC",
    "updated_date": "2025-11-08 23:05:31 UTC"
  },
  {
    "arxiv_id": "2511.06160v1",
    "title": "Evaluating Implicit Biases in LLM Reasoning through Logic Grid Puzzles",
    "authors": [
      "Fatima Jahara",
      "Mark Dredze",
      "Sharon Levy"
    ],
    "abstract": "While recent safety guardrails effectively suppress overtly biased outputs, subtler forms of social bias emerge during complex logical reasoning tasks that evade current evaluation benchmarks. To fill this gap, we introduce a new evaluation framework, PRIME (Puzzle Reasoning for Implicit Biases in Model Evaluation), that uses logic grid puzzles to systematically probe the influence of social stereotypes on logical reasoning and decision making in LLMs. Our use of logic puzzles enables automatic generation and verification, as well as variability in complexity and biased settings. PRIME includes stereotypical, anti-stereotypical, and neutral puzzle variants generated from a shared puzzle structure, allowing for controlled and fine-grained comparisons. We evaluate multiple model families across puzzle sizes and test the effectiveness of prompt-based mitigation strategies. Focusing our experiments on gender stereotypes, our findings highlight that models consistently reason more accurately when solutions align with stereotypical associations. This demonstrates the significance of PRIME for diagnosing and quantifying social biases perpetuated in the deductive reasoning of LLMs, where fairness is critical.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "24 pages (including appendix)",
    "pdf_url": "https://arxiv.org/pdf/2511.06160v1",
    "published_date": "2025-11-08 22:51:59 UTC",
    "updated_date": "2025-11-08 22:51:59 UTC"
  },
  {
    "arxiv_id": "2511.06157v2",
    "title": "Models Got Talent: Identifying High Performing Wearable Human Activity Recognition Models Without Training",
    "authors": [
      "Richard Goldman",
      "Varun Komperla",
      "Thomas Ploetz",
      "Harish Haresamudram"
    ],
    "abstract": "A promising alternative to the computationally expensive Neural Architecture Search (NAS) involves the development of Zero Cost Proxies (ZCPs), which correlate well with trained performance, but can be computed through a single forward/backward pass on a randomly sampled batch of data. In this paper, we investigate the effectiveness of ZCPs for HAR on six benchmark datasets, and demonstrate that they discover network architectures that obtain within 5% of performance attained by full-scale training involving 1500 randomly sampled architectures. This results in substantial computational savings as high-performing architectures can be discovered with minimal training. Our experiments not only introduce ZCPs to sensor-based HAR, but also demonstrate that they are robust to data noise, further showcasing their suitability for practical scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.06157v2",
    "published_date": "2025-11-08 22:38:14 UTC",
    "updated_date": "2025-11-19 03:07:57 UTC"
  },
  {
    "arxiv_id": "2511.06148v3",
    "title": "Large Language Models Develop Novel Social Biases Through Adaptive Exploration",
    "authors": [
      "Addison J. Wu",
      "Ryan Liu",
      "Xuechunzi Bai",
      "Thomas L. Griffiths"
    ],
    "abstract": "As large language models (LLMs) are adopted into frameworks that grant them the capacity to make real decisions, it is increasingly important to ensure that they are unbiased. In this paper, we argue that the predominant approach of simply removing existing biases from models is not enough. Using a paradigm from the psychology literature, we demonstrate that LLMs can spontaneously develop novel social biases about artificial demographic groups even when no inherent differences exist. These biases result in highly stratified task allocations, which are less fair than assignments by human participants and are exacerbated by newer and larger models. In social science, emergent biases like these have been shown to result from exploration-exploitation trade-offs, where the decision-maker explores too little, allowing early observations to strongly influence impressions about entire demographic groups. To alleviate this effect, we examine a series of interventions targeting model inputs, problem structure, and explicit steering. We find that explicitly incentivizing exploration most robustly reduces stratification, highlighting the need for better multifaceted objectives to mitigate bias. These results reveal that LLMs are not merely passive mirrors of human social biases, but can actively create new ones from experience, raising urgent questions about how these systems will shape societies over time.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.06148v3",
    "published_date": "2025-11-08 21:58:26 UTC",
    "updated_date": "2026-01-10 18:52:22 UTC"
  },
  {
    "arxiv_id": "2511.06146v1",
    "title": "Referring Expressions as a Lens into Spatial Language Grounding in Vision-Language Models",
    "authors": [
      "Akshar Tumu",
      "Varad Shinde",
      "Parisa Kordjamshidi"
    ],
    "abstract": "Spatial Reasoning is an important component of human cognition and is an area in which the latest Vision-language models (VLMs) show signs of difficulty. The current analysis works use image captioning tasks and visual question answering. In this work, we propose using the Referring Expression Comprehension task instead as a platform for the evaluation of spatial reasoning by VLMs. This platform provides the opportunity for a deeper analysis of spatial comprehension and grounding abilities when there is 1) ambiguity in object detection, 2) complex spatial expressions with a longer sentence structure and multiple spatial relations, and 3) expressions with negation ('not'). In our analysis, we use task-specific architectures as well as large VLMs and highlight their strengths and weaknesses in dealing with these specific situations. While all these models face challenges with the task at hand, the relative behaviors depend on the underlying models and the specific categories of spatial semantics (topological, directional, proximal, etc.). Our results highlight these challenges and behaviors and provide insight into research gaps and future directions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at IJCNLP-AACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2511.06146v1",
    "published_date": "2025-11-08 21:43:09 UTC",
    "updated_date": "2025-11-08 21:43:09 UTC"
  },
  {
    "arxiv_id": "2511.20663v5",
    "title": "MTTR-A: Measuring Cognitive Recovery Latency in Multi-Agent Systems",
    "authors": [
      "Barak Or"
    ],
    "abstract": "Reliability in multi-agent systems (MAS) built on large language models is increasingly limited by cognitive failures rather than infrastructure faults. Existing observability tools describe failures but do not quantify how quickly distributed reasoning recovers once coherence is lost. We introduce MTTR-A (Mean Time-to-Recovery for Agentic Systems), a runtime reliability metric that measures cognitive recovery latency in MAS. MTTR-A adapts classical dependability theory to agentic orchestration, capturing the time required to detect reasoning drift and restore coherent operation. We further define complementary metrics, including MTBF and a normalized recovery ratio (NRR), and establish theoretical bounds linking recovery latency to long-run cognitive uptime. Using a LangGraph-based benchmark with simulated drift and reflex recovery, we empirically demonstrate measurable recovery behavior across multiple reflex strategies. This work establishes a quantitative foundation for runtime cognitive dependability in distributed agentic systems.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.MA",
    "comment": "preprint",
    "pdf_url": "https://arxiv.org/pdf/2511.20663v5",
    "published_date": "2025-11-08 21:29:18 UTC",
    "updated_date": "2025-12-26 19:12:34 UTC"
  },
  {
    "arxiv_id": "2511.06142v1",
    "title": "MALinZero: Efficient Low-Dimensional Search for Mastering Complex Multi-Agent Planning",
    "authors": [
      "Sizhe Tang",
      "Jiayu Chen",
      "Tian Lan"
    ],
    "abstract": "Monte Carlo Tree Search (MCTS), which leverages Upper Confidence Bound for Trees (UCTs) to balance exploration and exploitation through randomized sampling, is instrumental to solving complex planning problems. However, for multi-agent planning, MCTS is confronted with a large combinatorial action space that often grows exponentially with the number of agents. As a result, the branching factor of MCTS during tree expansion also increases exponentially, making it very difficult to efficiently explore and exploit during tree search. To this end, we propose MALinZero, a new approach to leverage low-dimensional representational structures on joint-action returns and enable efficient MCTS in complex multi-agent planning. Our solution can be viewed as projecting the joint-action returns into the low-dimensional space representable using a contextual linear bandit problem formulation. We solve the contextual linear bandit problem with convex and $μ$-smooth loss functions -- in order to place more importance on better joint actions and mitigate potential representational limitations -- and derive a linear Upper Confidence Bound applied to trees (LinUCT) to enable novel multi-agent exploration and exploitation in the low-dimensional space. We analyze the regret of MALinZero for low-dimensional reward functions and propose an $(1-\\tfrac1e)$-approximation algorithm for the joint action selection by maximizing a sub-modular objective. MALinZero demonstrates state-of-the-art performance on multi-agent benchmarks such as matrix games, SMAC, and SMACv2, outperforming both model-based and model-free multi-agent reinforcement learning baselines with faster learning speed and better performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.06142v1",
    "published_date": "2025-11-08 21:27:09 UTC",
    "updated_date": "2025-11-08 21:27:09 UTC"
  },
  {
    "arxiv_id": "2511.06136v2",
    "title": "When Object-Centric World Models Meet Policy Learning: From Pixels to Policies, and Where It Breaks",
    "authors": [
      "Stefano Ferraro",
      "Akihiro Nakano",
      "Masahiro Suzuki",
      "Yutaka Matsuo"
    ],
    "abstract": "Object-centric world models (OCWM) aim to decompose visual scenes into object-level representations, providing structured abstractions that could improve compositional generalization and data efficiency in reinforcement learning. We hypothesize that explicitly disentangled object-level representations, by localizing task-relevant information, can enhance policy performance across novel feature combinations. To test this hypothesis, we introduce DLPWM, a fully unsupervised, disentangled object-centric world model that learns object-level latents directly from pixels. DLPWM achieves strong reconstruction and prediction performance, including robustness to several out-of-distribution (OOD) visual variations. However, when used for downstream model-based control, policies trained on DLPWM latents underperform compared to DreamerV3. Through latent-trajectory analyses, we identify representation shift during multi-object interactions as a key driver of unstable policy learning. Our results suggest that, although object-centric perception supports robust visual modeling, achieving stable control requires mitigating latent drift.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.06136v2",
    "published_date": "2025-11-08 21:09:44 UTC",
    "updated_date": "2025-11-11 10:57:17 UTC"
  },
  {
    "arxiv_id": "2511.07477v1",
    "title": "The Polite Liar: Epistemic Pathology in Language Models",
    "authors": [
      "Bentley DeVilling"
    ],
    "abstract": "Large language models exhibit a peculiar epistemic pathology: they speak as if they know, even when they do not. This paper argues that such confident fabrication, what I call the polite liar, is a structural consequence of reinforcement learning from human feedback (RLHF). Building on Frankfurt's analysis of bullshit as communicative indifference to truth, I show that this pathology is not deception but structural indifference: a reward architecture that optimizes for perceived sincerity over evidential accuracy. Current alignment methods reward models for being helpful, harmless, and polite, but not for being epistemically grounded. As a result, systems learn to maximize user satisfaction rather than truth, performing conversational fluency as a virtue. I analyze this behavior through the lenses of epistemic virtue theory, speech-act philosophy, and cognitive alignment, showing that RLHF produces agents trained to mimic epistemic confidence without access to epistemic justification. The polite liar thus reveals a deeper alignment tension between linguistic cooperation and epistemic integrity. The paper concludes with an \"epistemic alignment\" principle: reward justified confidence over perceived fluency.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "17 pages, 2 tables, Preprint - under review at AI & Society",
    "pdf_url": "https://arxiv.org/pdf/2511.07477v1",
    "published_date": "2025-11-08 21:02:52 UTC",
    "updated_date": "2025-11-08 21:02:52 UTC"
  },
  {
    "arxiv_id": "2511.06134v1",
    "title": "Maestro: Learning to Collaborate via Conditional Listwise Policy Optimization for Multi-Agent LLMs",
    "authors": [
      "Wei Yang",
      "Jiacheng Pang",
      "Shixuan Li",
      "Paul Bogdan",
      "Stephen Tu",
      "Jesse Thomason"
    ],
    "abstract": "Multi-agent systems (MAS) built on Large Language Models (LLMs) are being used to approach complex problems and can surpass single model inference. However, their success hinges on navigating a fundamental cognitive tension: the need to balance broad, divergent exploration of the solution space with a principled, convergent synthesis to the optimal solution. Existing paradigms often struggle to manage this duality, leading to premature consensus, error propagation, and a critical credit assignment problem that fails to distinguish between genuine reasoning and superficially plausible arguments. To resolve this core challenge, we propose the Multi-Agent Exploration-Synthesis framework Through Role Orchestration (Maestro), a principled paradigm for collaboration that structurally decouples these cognitive modes. Maestro uses a collective of parallel Execution Agents for diverse exploration and a specialized Central Agent for convergent, evaluative synthesis. To operationalize this critical synthesis phase, we introduce Conditional Listwise Policy Optimization (CLPO), a reinforcement learning objective that disentangles signals for strategic decisions and tactical rationales. By combining decision-focused policy gradients with a list-wise ranking loss over justifications, CLPO achieves clean credit assignment and stronger comparative supervision. Experiments on mathematical reasoning and general problem-solving benchmarks demonstrate that Maestro, coupled with CLPO, consistently outperforms existing state-of-the-art multi-agent approaches, delivering absolute accuracy gains of 6% on average and up to 10% at best.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.06134v1",
    "published_date": "2025-11-08 21:01:27 UTC",
    "updated_date": "2025-11-08 21:01:27 UTC"
  },
  {
    "arxiv_id": "2511.06125v1",
    "title": "Evaluation of retrieval-based QA on QUEST-LOFT",
    "authors": [
      "Nathan Scales",
      "Nathanael Schärli",
      "Olivier Bousquet"
    ],
    "abstract": "Despite the popularity of retrieval-augmented generation (RAG) as a solution for grounded QA in both academia and industry, current RAG methods struggle with questions where the necessary information is distributed across many documents or where retrieval needs to be combined with complex reasoning. Recently, the LOFT study has shown that this limitation also applies to approaches based on long-context language models, with the QUEST benchmark exhibiting particularly large headroom. In this paper, we provide an in-depth analysis of the factors contributing to the poor performance on QUEST-LOFT, publish updated numbers based on a thorough human evaluation, and demonstrate that RAG can be optimized to significantly outperform long-context approaches when combined with a structured output format containing reasoning and evidence, optionally followed by answer re-verification.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.06125v1",
    "published_date": "2025-11-08 20:30:45 UTC",
    "updated_date": "2025-11-08 20:30:45 UTC"
  },
  {
    "arxiv_id": "2511.15712v1",
    "title": "Secure Autonomous Agent Payments: Verifying Authenticity and Intent in a Trustless Environment",
    "authors": [
      "Vivek Acharya"
    ],
    "abstract": "Artificial intelligence (AI) agents are increasingly capable of initiating financial transactions on behalf of users or other agents. This evolution introduces a fundamental challenge: verifying both the authenticity of an autonomous agent and the true intent behind its transactions in a decentralized, trustless environment. Traditional payment systems assume human authorization, but autonomous, agent-led payments remove that safeguard. This paper presents a blockchain-based framework that cryptographically authenticates and verifies the intent of every AI-initiated transaction. The proposed system leverages decentralized identity (DID) standards and verifiable credentials to establish agent identities, on-chain intent proofs to record user authorization, and zero-knowledge proofs (ZKPs) to preserve privacy while ensuring policy compliance. Additionally, secure execution environments (TEE-based attestations) guarantee the integrity of agent reasoning and execution. The hybrid on-chain/off-chain architecture provides an immutable audit trail linking user intent to payment outcome. Through qualitative analysis, the framework demonstrates strong resistance to impersonation, unauthorized transactions, and misalignment of intent. This work lays the foundation for secure, auditable, and intent-aware autonomous economic agents, enabling a future of verifiable trust and accountability in AI-driven financial ecosystems.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.CR",
    "comment": "6 pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2511.15712v1",
    "published_date": "2025-11-08 19:53:51 UTC",
    "updated_date": "2025-11-08 19:53:51 UTC"
  },
  {
    "arxiv_id": "2511.06101v2",
    "title": "Adapting Web Agents with Synthetic Supervision",
    "authors": [
      "Zhaoyang Wang",
      "Yiming Liang",
      "Xuchao Zhang",
      "Qianhui Wu",
      "Siwei Han",
      "Anson Bastos",
      "Rujia Wang",
      "Chetan Bansal",
      "Baolin Peng",
      "Jianfeng Gao",
      "Saravan Rajmohan",
      "Huaxiu Yao"
    ],
    "abstract": "Web agents struggle to adapt to new websites due to the scarcity of environment specific tasks and demonstrations. Recent works have explored synthetic data generation to address this challenge, however, they suffer from data quality issues where synthesized tasks contain hallucinations that cannot be executed, and collected trajectories are noisy with redundant or misaligned actions. In this paper, we propose SynthAgent, a fully synthetic supervision framework that aims at improving synthetic data quality via dual refinement of both tasks and trajectories. Our approach begins by synthesizing diverse tasks through categorized exploration of web elements, ensuring efficient coverage of the target environment. During trajectory collection, tasks are refined only when conflicts with observations are detected, which mitigates hallucinations while preserving task consistency. After collection, we conduct trajectory refinement with global context to mitigate potential noise or misalignments. Finally, we fine-tune open-source web agents on the refined synthetic data to adapt them to the target environment. Experimental results demonstrate that SynthAgent outperforms existing synthetic data methods, validating the importance of high-quality synthetic supervision. The code is publicly available at https://github.com/aiming-lab/SynthAgent.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.06101v2",
    "published_date": "2025-11-08 18:45:33 UTC",
    "updated_date": "2026-01-06 17:55:17 UTC"
  },
  {
    "arxiv_id": "2511.06090v2",
    "title": "SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?",
    "authors": [
      "Jeffrey Jian Ma",
      "Milad Hashemi",
      "Amir Yazdanbakhsh",
      "Kevin Swersky",
      "Ofir Press",
      "Enhui Li",
      "Vijay Janapa Reddi",
      "Parthasarathy Ranganathan"
    ],
    "abstract": "Optimizing the performance of large-scale software repositories demands expertise in code reasoning and software engineering (SWE) to reduce runtime while preserving program correctness. However, most benchmarks emphasize what to fix rather than how to fix code. We introduce SWE-fficiency, a benchmark for evaluating repository-level performance optimization on real workloads. Our suite contains 498 tasks across nine widely used data-science, machine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a complete codebase and a slow workload, an agent must investigate code semantics, localize bottlenecks and relevant tests, and produce a patch that matches or exceeds expert speedup while passing the same unit tests. To enable this how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests for performance-improving edits, combining keyword filtering, static analysis, coverage tooling, and execution validation to both confirm expert speedup baselines and identify relevant repository unit tests. Empirical evaluation of state-of-the-art agents reveals significant underperformance. On average, agents achieve less than 0.15x the expert speedup: agents struggle in localizing optimization opportunities, reasoning about execution across functions, and maintaining correctness in proposed edits. We release the benchmark and accompanying data pipeline to facilitate research on automated performance engineering and long-horizon software reasoning.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.SE",
    "comment": "Data, code, and leaderboard are available at https://swefficiency.com/",
    "pdf_url": "https://arxiv.org/pdf/2511.06090v2",
    "published_date": "2025-11-08 17:55:09 UTC",
    "updated_date": "2025-11-11 04:00:47 UTC"
  },
  {
    "arxiv_id": "2511.06087v1",
    "title": "Hybrid CNN-ViT Framework for Motion-Blurred Scene Text Restoration",
    "authors": [
      "Umar Rashid",
      "Muhammad Arslan Arshad",
      "Ghulam Ahmad",
      "Muhammad Zeeshan Anjum",
      "Rizwan Khan",
      "Muhammad Akmal"
    ],
    "abstract": "Motion blur in scene text images severely impairs readability and hinders the reliability of computer vision tasks, including autonomous driving, document digitization, and visual information retrieval. Conventional deblurring approaches are often inadequate in handling spatially varying blur and typically fall short in modeling the long-range dependencies necessary for restoring textual clarity. To overcome these limitations, we introduce a hybrid deep learning framework that combines convolutional neural networks (CNNs) with vision transformers (ViTs), thereby leveraging both local feature extraction and global contextual reasoning. The architecture employs a CNN-based encoder-decoder to preserve structural details, while a transformer module enhances global awareness through self-attention. Training is conducted on a curated dataset derived from TextOCR, where sharp scene-text samples are paired with synthetically blurred versions generated using realistic motion-blur kernels of multiple sizes and orientations. Model optimization is guided by a composite loss that incorporates mean absolute error (MAE), squared error (MSE), perceptual similarity, and structural similarity (SSIM). Quantitative evaluations show that the proposed method attains 32.20 dB in PSNR and 0.934 in SSIM, while remaining lightweight with 2.83 million parameters and an average inference time of 61 ms. These results highlight the effectiveness and computational efficiency of the CNN-ViT hybrid design, establishing its practicality for real-world motion-blurred scene-text restoration.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.06087v1",
    "published_date": "2025-11-08 17:48:58 UTC",
    "updated_date": "2025-11-08 17:48:58 UTC"
  },
  {
    "arxiv_id": "2511.06078v1",
    "title": "Simulating Students with Large Language Models: A Review of Architecture, Mechanisms, and Role Modelling in Education with Generative AI",
    "authors": [
      "Luis Marquez-Carpintero",
      "Alberto Lopez-Sellers",
      "Miguel Cazorla"
    ],
    "abstract": "Simulated Students offer a valuable methodological framework for evaluating pedagogical approaches and modelling diverse learner profiles, tasks which are otherwise challenging to undertake systematically in real-world settings. Recent research has increasingly focused on developing such simulated agents to capture a range of learning styles, cognitive development pathways, and social behaviours. Among contemporary simulation techniques, the integration of large language models (LLMs) into educational research has emerged as a particularly versatile and scalable paradigm. LLMs afford a high degree of linguistic realism and behavioural adaptability, enabling agents to approximate cognitive processes and engage in contextually appropriate pedagogical dialogues. This paper presents a thematic review of empirical and methodological studies utilising LLMs to simulate student behaviour across educational environments. We synthesise current evidence on the capacity of LLM-based agents to emulate learner archetypes, respond to instructional inputs, and interact within multi-agent classroom scenarios. Furthermore, we examine the implications of such systems for curriculum development, instructional evaluation, and teacher training. While LLMs surpass rule-based systems in natural language generation and situational flexibility, ongoing concerns persist regarding algorithmic bias, evaluation reliability, and alignment with educational objectives. The review identifies existing technological and methodological gaps and proposes future research directions for integrating generative AI into adaptive learning systems and instructional design.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.06078v1",
    "published_date": "2025-11-08 17:23:13 UTC",
    "updated_date": "2025-11-08 17:23:13 UTC"
  },
  {
    "arxiv_id": "2511.06073v1",
    "title": "Stemming Hallucination in Language Models Using a Licensing Oracle",
    "authors": [
      "Simeon Emanuilov",
      "Richard Ackermann"
    ],
    "abstract": "Language models exhibit remarkable natural language generation capabilities but remain prone to hallucinations, generating factually incorrect information despite producing syntactically coherent responses. This study introduces the Licensing Oracle, an architectural solution designed to stem hallucinations in LMs by enforcing truth constraints through formal validation against structured knowledge graphs. Unlike statistical approaches that rely on data scaling or fine-tuning, the Licensing Oracle embeds a deterministic validation step into the model's generative process, ensuring that only factually accurate claims are made. We evaluated the effectiveness of the Licensing Oracle through experiments comparing it with several state-of-the-art methods, including baseline language model generation, fine-tuning for factual recall, fine-tuning for abstention behavior, and retrieval-augmented generation (RAG). Our results demonstrate that although RAG and fine-tuning improve performance, they fail to eliminate hallucinations. In contrast, the Licensing Oracle achieved perfect abstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring that only valid claims were generated with 89.1% accuracy in factual responses. This work shows that architectural innovations, such as the Licensing Oracle, offer a necessary and sufficient solution for hallucinations in domains with structured knowledge representations, offering guarantees that statistical methods cannot match. Although the Licensing Oracle is specifically designed to address hallucinations in fact-based domains, its framework lays the groundwork for truth-constrained generation in future AI systems, providing a new path toward reliable, epistemically grounded models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages, 4 figures, 8 tables. Introduces the Licensing Oracle, an architectural solution for eliminating hallucinations in language models through formal SHACL validation against knowledge graphs. All datasets and models are available at https://huggingface.co/collections/s-emanuilov/licensing-oracle-experiments",
    "pdf_url": "https://arxiv.org/pdf/2511.06073v1",
    "published_date": "2025-11-08 17:07:57 UTC",
    "updated_date": "2025-11-08 17:07:57 UTC"
  },
  {
    "arxiv_id": "2511.06065v3",
    "title": "ScRPO: From Errors to Insights",
    "authors": [
      "Lianrui Li",
      "Dakuan Lu",
      "Jiawei Shao",
      "Xuelong Li"
    ],
    "abstract": "We introduce Self-correction Relative Policy Optimization (ScRPO), a novel reinforcement learning framework designed to empower large language models with advanced mathematical reasoning capabilities through iterative self-reflection and error correction. The ScRPO framework operates in two distinct phases: (1) Trial-and-error learning stage, where the model is trained via GRPO, and incorrect responses are collected to form an \"error pool\"; and (2) Self-correction learning stage, which guides the model to introspectively analyze and rectify the reasoning flaws behind its previous errors. Extensive evaluations across challenging mathematical benchmarks, including AIME, AMC, Olympiad, MATH-500, and GSM8k, validate the efficacy of our approach. Using DeepSeek-R1-Distill-Qwen-1.5B and 7B as backbones, ScRPO achieves average accuracies of 64.8% and 77.8%, respectively. This represents a significant improvement of 6.0% and 3.2% over vanilla baselines, consistently outperforming strong post-training methods such as DAPO and GRPO. These findings establish ScRPO as a robust paradigm for enabling autonomous self-improvement in AI systems, particularly in tasks with limited external feedback.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.06065v3",
    "published_date": "2025-11-08 16:30:44 UTC",
    "updated_date": "2026-01-05 05:19:37 UTC"
  },
  {
    "arxiv_id": "2511.06064v1",
    "title": "A Privacy-Preserving Federated Learning Method with Homomorphic Encryption in Omics Data",
    "authors": [
      "Yusaku Negoya",
      "Feifei Cui",
      "Zilong Zhang",
      "Miao Pan",
      "Tomoaki Ohtsuki",
      "Aohan Li"
    ],
    "abstract": "Omics data is widely employed in medical research to identify disease mechanisms and contains highly sensitive personal information. Federated Learning (FL) with Differential Privacy (DP) can ensure the protection of omics data privacy against malicious user attacks. However, FL with the DP method faces an inherent trade-off: stronger privacy protection degrades predictive accuracy due to injected noise. On the other hand, Homomorphic Encryption (HE) allows computations on encrypted data and enables aggregation of encrypted gradients without DP-induced noise can increase the predictive accuracy. However, it may increase the computation cost. To improve the predictive accuracy while considering the computational ability of heterogeneous clients, we propose a Privacy-Preserving Machine Learning (PPML)-Hybrid method by introducing HE. In the proposed PPML-Hybrid method, clients distributed select either HE or DP based on their computational resources, so that HE clients contribute noise-free updates while DP clients reduce computational overhead. Meanwhile, clients with high computational resources clients can flexibly adopt HE or DP according to their privacy needs. Performance evaluation on omics datasets show that our proposed method achieves comparable predictive accuracy while significantly reducing computation time relative to HE-only. Additionally, it outperforms DP-only methods under equivalent or stricter privacy budgets.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "6 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.06064v1",
    "published_date": "2025-11-08 16:18:42 UTC",
    "updated_date": "2025-11-08 16:18:42 UTC"
  },
  {
    "arxiv_id": "2511.06044v1",
    "title": "How Particle-System Random Batch Methods Enhance Graph Transformer: Memory Efficiency and Parallel Computing Strategy",
    "authors": [
      "Hanwen Liu",
      "Yixuan Ma",
      "Shi Jin",
      "Yuguang Wang"
    ],
    "abstract": "Attention mechanism is a significant part of Transformer models. It helps extract features from embedded vectors by adding global information and its expressivity has been proved to be powerful. Nevertheless, the quadratic complexity restricts its practicability. Although several researches have provided attention mechanism in sparse form, they are lack of theoretical analysis about the expressivity of their mechanism while reducing complexity. In this paper, we put forward Random Batch Attention (RBA), a linear self-attention mechanism, which has theoretical support of the ability to maintain its expressivity. Random Batch Attention has several significant strengths as follows: (1) Random Batch Attention has linear time complexity. Other than this, it can be implemented in parallel on a new dimension, which contributes to much memory saving. (2) Random Batch Attention mechanism can improve most of the existing models by replacing their attention mechanisms, even many previously improved attention mechanisms. (3) Random Batch Attention mechanism has theoretical explanation in convergence, as it comes from Random Batch Methods on computation mathematics. Experiments on large graphs have proved advantages mentioned above. Also, the theoretical modeling of self-attention mechanism is a new tool for future research on attention-mechanism analysis.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.ST"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.06044v1",
    "published_date": "2025-11-08 15:34:15 UTC",
    "updated_date": "2025-11-08 15:34:15 UTC"
  },
  {
    "arxiv_id": "2511.06041v1",
    "title": "Advancing Ocean State Estimation with efficient and scalable AI",
    "authors": [
      "Yanfei Xiang",
      "Yuan Gao",
      "Hao Wu",
      "Quan Zhang",
      "Ruiqi Shu",
      "Xiao Zhou",
      "Xi Wu",
      "Xiaomeng Huang"
    ],
    "abstract": "Accurate and efficient global ocean state estimation remains a grand challenge for Earth system science, hindered by the dual bottlenecks of computational scalability and degraded data fidelity in traditional data assimilation (DA) and deep learning (DL) approaches. Here we present an AI-driven Data Assimilation Framework for Ocean (ADAF-Ocean) that directly assimilates multi-source and multi-scale observations, ranging from sparse in-situ measurements to 4 km satellite swaths, without any interpolation or data thinning. Inspired by Neural Processes, ADAF-Ocean learns a continuous mapping from heterogeneous inputs to ocean states, preserving native data fidelity. Through AI-driven super-resolution, it reconstructs 0.25$^\\circ$ mesoscale dynamics from coarse 1$^\\circ$ fields, which ensures both efficiency and scalability, with just 3.7\\% more parameters than the 1$^\\circ$ configuration. When coupled with a DL forecasting system, ADAF-Ocean extends global forecast skill by up to 20 days compared to baselines without assimilation. This framework establishes a computationally viable and scientifically rigorous pathway toward real-time, high-resolution Earth system monitoring.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "29 papes, 10 Figures",
    "pdf_url": "https://arxiv.org/pdf/2511.06041v1",
    "published_date": "2025-11-08 15:24:23 UTC",
    "updated_date": "2025-11-08 15:24:23 UTC"
  },
  {
    "arxiv_id": "2511.06033v1",
    "title": "S2ML: Spatio-Spectral Mutual Learning for Depth Completion",
    "authors": [
      "Zihui Zhao",
      "Yifei Zhang",
      "Zheng Wang",
      "Yang Li",
      "Kui Jiang",
      "Zihan Geng",
      "Chia-Wen Lin"
    ],
    "abstract": "The raw depth images captured by RGB-D cameras using Time-of-Flight (TOF) or structured light often suffer from incomplete depth values due to weak reflections, boundary shadows, and artifacts, which limit their applications in downstream vision tasks. Existing methods address this problem through depth completion in the image domain, but they overlook the physical characteristics of raw depth images. It has been observed that the presence of invalid depth areas alters the frequency distribution pattern. In this work, we propose a Spatio-Spectral Mutual Learning framework (S2ML) to harmonize the advantages of both spatial and frequency domains for depth completion. Specifically, we consider the distinct properties of amplitude and phase spectra and devise a dedicated spectral fusion module. Meanwhile, the local and global correlations between spatial-domain and frequency-domain features are calculated in a unified embedding space. The gradual mutual representation and refinement encourage the network to fully explore complementary physical characteristics and priors for more accurate depth completion. Extensive experiments demonstrate the effectiveness of our proposed S2ML method, outperforming the state-of-the-art method CFormer by 0.828 dB and 0.834 dB on the NYU-Depth V2 and SUN RGB-D datasets, respectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.06033v1",
    "published_date": "2025-11-08 15:01:55 UTC",
    "updated_date": "2025-11-08 15:01:55 UTC"
  },
  {
    "arxiv_id": "2511.06032v1",
    "title": "ITPP: Learning Disentangled Event Dynamics in Marked Temporal Point Processes",
    "authors": [
      "Wang-Tao Zhou",
      "Zhao Kang",
      "Ke Yan",
      "Ling Tian"
    ],
    "abstract": "Marked Temporal Point Processes (MTPPs) provide a principled framework for modeling asynchronous event sequences by conditioning on the history of past events. However, most existing MTPP models rely on channel-mixing strategies that encode information from different event types into a single, fixed-size latent representation. This entanglement can obscure type-specific dynamics, leading to performance degradation and increased risk of overfitting. In this work, we introduce ITPP, a novel channel-independent architecture for MTPP modeling that decouples event type information using an encoder-decoder framework with an ODE-based backbone. Central to ITPP is a type-aware inverted self-attention mechanism, designed to explicitly model inter-channel correlations among heterogeneous event types. This architecture enhances effectiveness and robustness while reducing overfitting. Comprehensive experiments on multiple real-world and synthetic datasets demonstrate that ITPP consistently outperforms state-of-the-art MTPP models in both predictive accuracy and generalization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to AAAI'26 Poster",
    "pdf_url": "https://arxiv.org/pdf/2511.06032v1",
    "published_date": "2025-11-08 15:00:25 UTC",
    "updated_date": "2025-11-08 15:00:25 UTC"
  },
  {
    "arxiv_id": "2601.05266v1",
    "title": "Retrieval-Augmented Multi-LLM Ensemble for Industrial Part Specification Extraction",
    "authors": [
      "Muzakkiruddin Ahmed Mohammed",
      "John R. Talburt",
      "Leon Claasssens",
      "Adriaan Marais"
    ],
    "abstract": "Industrial part specification extraction from unstructured text remains a persistent challenge in manufacturing, procurement, and maintenance, where manual processing is both time-consuming and error-prone. This paper introduces a retrieval-augmented multi-LLM ensemble framework that orchestrates nine state-of-the-art Large Language Models (LLMs) within a structured three-phase pipeline. RAGsemble addresses key limitations of single-model systems by combining the complementary strengths of model families including Gemini (2.0, 2.5, 1.5), OpenAI (GPT-4o, o4-mini), Mistral Large, and Gemma (1B, 4B, 3n-e4b), while grounding outputs in factual data using FAISS-based semantic retrieval. The system architecture consists of three stages: (1) parallel extraction by diverse LLMs, (2) targeted research augmentation leveraging high-performing models, and (3) intelligent synthesis with conflict resolution and confidence-aware scoring. RAG integration provides real-time access to structured part databases, enabling the system to validate, refine, and enrich outputs through similarity-based reference retrieval. Experimental results using real industrial datasets demonstrate significant gains in extraction accuracy, technical completeness, and structured output quality compared to leading single-LLM baselines. Key contributions include a scalable ensemble architecture for industrial domains, seamless RAG integration throughout the pipeline, comprehensive quality assessment mechanisms, and a production-ready solution suitable for deployment in knowledge-intensive manufacturing environments.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "The 17th International Conference on Knowledge and Systems Engineering",
    "pdf_url": "https://arxiv.org/pdf/2601.05266v1",
    "published_date": "2025-11-08 14:43:20 UTC",
    "updated_date": "2025-11-08 14:43:20 UTC"
  },
  {
    "arxiv_id": "2511.06019v1",
    "title": "MiVID: Multi-Strategic Self-Supervision for Video Frame Interpolation using Diffusion Model",
    "authors": [
      "Priyansh Srivastava",
      "Romit Chatterjee",
      "Abir Sen",
      "Aradhana Behura",
      "Ratnakar Dash"
    ],
    "abstract": "Video Frame Interpolation (VFI) remains a cornerstone in video enhancement, enabling temporal upscaling for tasks like slow-motion rendering, frame rate conversion, and video restoration. While classical methods rely on optical flow and learning-based models assume access to dense ground-truth, both struggle with occlusions, domain shifts, and ambiguous motion. This article introduces MiVID, a lightweight, self-supervised, diffusion-based framework for video interpolation. Our model eliminates the need for explicit motion estimation by combining a 3D U-Net backbone with transformer-style temporal attention, trained under a hybrid masking regime that simulates occlusions and motion uncertainty. The use of cosine-based progressive masking and adaptive loss scheduling allows our network to learn robust spatiotemporal representations without any high-frame-rate supervision. Our framework is evaluated on UCF101-7 and DAVIS-7 datasets. MiVID is trained entirely on CPU using the datasets and 9-frame video segments, making it a low-resource yet highly effective pipeline. Despite these constraints, our model achieves optimal results at just 50 epochs, competitive with several supervised baselines.This work demonstrates the power of self-supervised diffusion priors for temporally coherent frame synthesis and provides a scalable path toward accessible and generalizable VFI systems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.06019v1",
    "published_date": "2025-11-08 14:10:04 UTC",
    "updated_date": "2025-11-08 14:10:04 UTC"
  },
  {
    "arxiv_id": "2511.06016v1",
    "title": "One-Shot Knowledge Transfer for Scalable Person Re-Identification",
    "authors": [
      "Longhua Li",
      "Lei Qi",
      "Xin Geng"
    ],
    "abstract": "Edge computing in person re-identification (ReID) is crucial for reducing the load on central cloud servers and ensuring user privacy. Conventional compression methods for obtaining compact models require computations for each individual student model. When multiple models of varying sizes are needed to accommodate different resource conditions, this leads to repetitive and cumbersome computations. To address this challenge, we propose a novel knowledge inheritance approach named OSKT (One-Shot Knowledge Transfer), which consolidates the knowledge of the teacher model into an intermediate carrier called a weight chain. When a downstream scenario demands a model that meets specific resource constraints, this weight chain can be expanded to the target model size without additional computation. OSKT significantly outperforms state-of-the-art compression methods, with the added advantage of one-time knowledge transfer that eliminates the need for frequent computations for each target model.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2511.06016v1",
    "published_date": "2025-11-08 14:06:23 UTC",
    "updated_date": "2025-11-08 14:06:23 UTC"
  },
  {
    "arxiv_id": "2511.06010v1",
    "title": "MoSKA: Mixture of Shared KV Attention for Efficient Long-Sequence LLM Inference",
    "authors": [
      "Myunghyun Rhee",
      "Sookyung Choi",
      "Euiseok Kim",
      "Joonseop Sim",
      "Youngpyo Joo",
      "Hoshik Kim"
    ],
    "abstract": "The escalating context length in Large Language Models (LLMs) creates a severe performance bottleneck around the Key-Value (KV) cache, whose memory-bound nature leads to significant GPU under-utilization. This paper introduces Mixture of Shared KV Attention (MoSKA), an architecture that addresses this challenge by exploiting the heterogeneity of context data. It differentiates between per-request unique and massively reused shared sequences. The core of MoSKA is a novel Shared KV Attention mechanism that transforms the attention on shared data from a series of memory-bound GEMV operations into a single, compute-bound GEMM by batching concurrent requests. This is supported by an MoE-inspired sparse attention strategy that prunes the search space and a tailored Disaggregated Infrastructure that specializes hardware for unique and shared data. This comprehensive approach demonstrates a throughput increase of up to 538.7x over baselines in workloads with high context sharing, offering a clear architectural path toward scalable LLM inference.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "4 pages, 5 figures, accepted for publication at IEEE Computer Architecture Letters (IEEE CAL), 2025",
    "pdf_url": "https://arxiv.org/pdf/2511.06010v1",
    "published_date": "2025-11-08 13:40:16 UTC",
    "updated_date": "2025-11-08 13:40:16 UTC"
  },
  {
    "arxiv_id": "2511.05996v1",
    "title": "Exploring Category-level Articulated Object Pose Tracking on SE(3) Manifolds",
    "authors": [
      "Xianhui Meng",
      "Yukang Huo",
      "Li Zhang",
      "Liu Liu",
      "Haonan Jiang",
      "Yan Zhong",
      "Pingrui Zhang",
      "Cewu Lu",
      "Jun Liu"
    ],
    "abstract": "Articulated objects are prevalent in daily life and robotic manipulation tasks. However, compared to rigid objects, pose tracking for articulated objects remains an underexplored problem due to their inherent kinematic constraints. To address these challenges, this work proposes a novel point-pair-based pose tracking framework, termed \\textbf{PPF-Tracker}. The proposed framework first performs quasi-canonicalization of point clouds in the SE(3) Lie group space, and then models articulated objects using Point Pair Features (PPF) to predict pose voting parameters by leveraging the invariance properties of SE(3). Finally, semantic information of joint axes is incorporated to impose unified kinematic constraints across all parts of the articulated object. PPF-Tracker is systematically evaluated on both synthetic datasets and real-world scenarios, demonstrating strong generalization across diverse and challenging environments. Experimental results highlight the effectiveness and robustness of PPF-Tracker in multi-frame pose tracking of articulated objects. We believe this work can foster advances in robotics, embodied intelligence, and augmented reality. Codes are available at https://github.com/mengxh20/PPFTracker.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05996v1",
    "published_date": "2025-11-08 12:56:21 UTC",
    "updated_date": "2025-11-08 12:56:21 UTC"
  },
  {
    "arxiv_id": "2511.05993v2",
    "title": "Revisiting Entropy in Reinforcement Learning for Large Reasoning Models",
    "authors": [
      "Renren Jin",
      "Pengzhi Gao",
      "Yuqi Ren",
      "Zhuowen Han",
      "Tongxuan Zhang",
      "Wuwei Huang",
      "Wei Liu",
      "Jian Luan",
      "Deyi Xiong"
    ],
    "abstract": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a prominent paradigm for enhancing the reasoning capabilities of large language models (LLMs). However, the entropy of LLMs usually collapses during RLVR training, leading to premature convergence to suboptimal local minima and hindering further performance improvement. Although various approaches have been proposed to mitigate entropy collapse, a comprehensive study of entropy in RLVR remains lacking. To bridge this gap, we conduct extensive experiments to investigate the entropy dynamics of LLMs trained with RLVR and analyze how model entropy correlates with response diversity, calibration, and performance across various benchmarks. Our results identify three key factors that influence entropy: the clipping thresholds in the optimization objective, the number of off-policy updates, and the diversity of the training data. Furthermore, through both theoretical analysis and empirical validation, we demonstrate that tokens with positive advantages are the primary drivers of entropy collapse. Motivated by this insight, we propose Positive-Advantage Reweighting, a simple yet effective approach that regulates model entropy by adjusting the loss weights assigned to tokens with positive advantages during RLVR training, while maintaining competitive performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 25 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2511.05993v2",
    "published_date": "2025-11-08 12:50:41 UTC",
    "updated_date": "2026-01-10 08:58:33 UTC"
  },
  {
    "arxiv_id": "2511.05991v1",
    "title": "Ontology Learning and Knowledge Graph Construction: A Comparison of Approaches and Their Impact on RAG Performance",
    "authors": [
      "Tiago da Cruz",
      "Bernardo Tavares",
      "Francisco Belo"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems combine Large Language Models (LLMs) with external knowledge, and their performance depends heavily on how that knowledge is represented. This study investigates how different Knowledge Graph (KG) construction strategies influence RAG performance. We compare a variety of approaches: standard vector-based RAG, GraphRAG, and retrieval over KGs built from ontologies derived either from relational databases or textual corpora. Results show that ontology-guided KGs incorporating chunk information achieve competitive performance with state-of-the-art frameworks, substantially outperforming vector retrieval baselines. Moreover, the findings reveal that ontology-guided KGs built from relational databases perform competitively to ones built with ontologies extracted from text, with the benefit of offering a dual advantage: they require a one-time-only ontology learning process, substantially reducing LLM usage costs; and avoid the complexity of ontology merging inherent to text-based approaches.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "12 pages, 8 Figures",
    "pdf_url": "https://arxiv.org/pdf/2511.05991v1",
    "published_date": "2025-11-08 12:38:45 UTC",
    "updated_date": "2025-11-08 12:38:45 UTC"
  },
  {
    "arxiv_id": "2511.16675v1",
    "title": "Joint Design of Protein Surface and Structure Using a Diffusion Bridge Model",
    "authors": [
      "Guanlue Li",
      "Xufeng Zhao",
      "Fang Wu",
      "Sören Laue"
    ],
    "abstract": "Protein-protein interactions (PPIs) are governed by surface complementarity and hydrophobic interactions at protein interfaces. However, designing diverse and physically realistic protein structure and surfaces that precisely complement target receptors remains a significant challenge in computational protein design. In this work, we introduce PepBridge, a novel framework for the joint design of protein surface and structure that seamlessly integrates receptor surface geometry and biochemical properties. Starting with a receptor surface represented as a 3D point cloud, PepBridge generates complete protein structures through a multi-step process. First, it employs denoising diffusion bridge models (DDBMs) to map receptor surfaces to ligand surfaces. Next, a multi-model diffusion model predicts the corresponding structure, while Shape-Frame Matching Networks ensure alignment between surface geometry and backbone architecture. This integrated approach facilitates surface complementarity, conformational stability, and chemical feasibility. Extensive validation across diverse protein design scenarios demonstrates PepBridge's efficacy in generating structurally viable proteins, representing a significant advancement in the joint design of top-down protein structure.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.16675v1",
    "published_date": "2025-11-08 12:31:07 UTC",
    "updated_date": "2025-11-08 12:31:07 UTC"
  },
  {
    "arxiv_id": "2511.11635v1",
    "title": "EduAgentQG: A Multi-Agent Workflow Framework for Personalized Question Generation",
    "authors": [
      "Rui Jia",
      "Min Zhang",
      "Fengrui Liu",
      "Bo Jiang",
      "Kun Kuang",
      "Zhongxiang Dai"
    ],
    "abstract": "High-quality personalized question banks are crucial for supporting adaptive learning and individualized assessment. Manually designing questions is time-consuming and often fails to meet diverse learning needs, making automated question generation a crucial approach to reduce teachers' workload and improve the scalability of educational resources. However, most existing question generation methods rely on single-agent or rule-based pipelines, which still produce questions with unstable quality, limited diversity, and insufficient alignment with educational goals. To address these challenges, we propose EduAgentQG, a multi-agent collaborative framework for generating high-quality and diverse personalized questions. The framework consists of five specialized agents and operates through an iterative feedback loop: the Planner generates structured design plans and multiple question directions to enhance diversity; the Writer produces candidate questions based on the plan and optimizes their quality and diversity using feedback from the Solver and Educator; the Solver and Educator perform binary scoring across multiple evaluation dimensions and feed the evaluation results back to the Writer; the Checker conducts final verification, including answer correctness and clarity, ensuring alignment with educational goals. Through this multi-agent collaboration and iterative feedback loop, EduAgentQG generates questions that are both high-quality and diverse, while maintaining consistency with educational objectives. Experiments on two mathematics question datasets demonstrate that EduAgentQG outperforms existing single-agent and multi-agent methods in terms of question diversity, goal consistency, and overall quality.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.11635v1",
    "published_date": "2025-11-08 12:25:31 UTC",
    "updated_date": "2025-11-08 12:25:31 UTC"
  },
  {
    "arxiv_id": "2511.05982v1",
    "title": "Runtime Safety Monitoring of Deep Neural Networks for Perception: A Survey",
    "authors": [
      "Albert Schotschneider",
      "Svetlana Pavlitska",
      "J. Marius Zöllner"
    ],
    "abstract": "Deep neural networks (DNNs) are widely used in perception systems for safety-critical applications, such as autonomous driving and robotics. However, DNNs remain vulnerable to various safety concerns, including generalization errors, out-of-distribution (OOD) inputs, and adversarial attacks, which can lead to hazardous failures. This survey provides a comprehensive overview of runtime safety monitoring approaches, which operate in parallel to DNNs during inference to detect these safety concerns without modifying the DNN itself. We categorize existing methods into three main groups: Monitoring inputs, internal representations, and outputs. We analyze the state-of-the-art for each category, identify strengths and limitations, and map methods to the safety concerns they address. In addition, we highlight open challenges and future research directions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 1 figure, 2 tables, accepted at IEEE SMC 2025 in Vienna, presented on 8th October 2025",
    "pdf_url": "https://arxiv.org/pdf/2511.05982v1",
    "published_date": "2025-11-08 12:06:54 UTC",
    "updated_date": "2025-11-08 12:06:54 UTC"
  },
  {
    "arxiv_id": "2511.05978v1",
    "title": "Kunlun Anomaly Troubleshooter: Enabling Kernel-Level Anomaly Detection and Causal Reasoning for Large Model Distributed Inference",
    "authors": [
      "Yuyang Liu",
      "Jingjing Cai",
      "Jiayi Ren",
      "Peng Zhou",
      "Danyang Zhang",
      "Yin Du",
      "Shijian Li"
    ],
    "abstract": "Anomaly troubleshooting for large model distributed inference (LMDI) remains a critical challenge. Resolving anomalies such as inference performance degradation or latency jitter in distributed system demands significant manual efforts from domain experts, resulting in extremely time-consuming diagnosis processes with relatively low accuracy. In this paper, we introduce Kunlun Anomaly Troubleshooter (KAT), the first anomaly troubleshooting framework tailored for LMDI. KAT addresses this problem through two core innovations. First, KAT exploits the synchronicity and consistency of GPU workers, innovatively leverages function trace data to precisely detect kernel-level anomalies and associated hardware components at nanosecond resolution. Second, KAT integrates these detection results into a domain-adapted LLM, delivering systematic causal reasoning and natural language interpretation of complex anomaly symptoms. Evaluations conducted in Alibaba Cloud Service production environment indicate that KAT achieves over 0.884 precision and 0.936 recall in anomaly detection, providing detail anomaly insights that significantly narrow down the diagnostic scope and improve both the efficiency and success rate of troubleshooting.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint version, under submission",
    "pdf_url": "https://arxiv.org/pdf/2511.05978v1",
    "published_date": "2025-11-08 11:53:08 UTC",
    "updated_date": "2025-11-08 11:53:08 UTC"
  },
  {
    "arxiv_id": "2511.05977v1",
    "title": "An Epistemic Perspective on Agent Awareness",
    "authors": [
      "Pavel Naumov",
      "Alexandra Pavlova"
    ],
    "abstract": "The paper proposes to treat agent awareness as a form of knowledge, breaking the tradition in the existing literature on awareness. It distinguishes the de re and de dicto forms of such knowledge. The work introduces two modalities capturing these forms and formally specifies their meaning using a version of 2D-semantics. The main technical result is a sound and complete logical system describing the interplay between the two proposed modalities and the standard \"knowledge of the fact\" modality.",
    "categories": [
      "cs.AI",
      "cs.LO",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Fortieth AAAI Conference on Artificial Intelligence (AAAI-26)",
    "pdf_url": "https://arxiv.org/pdf/2511.05977v1",
    "published_date": "2025-11-08 11:50:25 UTC",
    "updated_date": "2025-11-08 11:50:25 UTC"
  },
  {
    "arxiv_id": "2601.05265v1",
    "title": "Cross-Document Topic-Aligned Chunking for Retrieval-Augmented Generation",
    "authors": [
      "Mile Stankovic"
    ],
    "abstract": "Chunking quality determines RAG system performance. Current methods partition documents individually, but complex queries need information scattered across multiple sources: the knowledge fragmentation problem. We introduce Cross-Document Topic-Aligned (CDTA) chunking, which reconstructs knowledge at the corpus level. It first identifies topics across documents, maps segments to each topic, and synthesizes them into unified chunks.\n  On HotpotQA multi-hop reasoning, our method reached 0.93 faithfulness versus 0.83 for contextual retrieval and 0.78 for semantic chunking, a 12% improvement over current industry best practice (p < 0.05). On UAE Legal texts, it reached 0.94 faithfulness with 0.93 citation accuracy. At k = 3, it maintains 0.91 faithfulness while semantic methods drop to 0.68, with a single CDTA chunk containing information requiring multiple traditional fragments.\n  Indexing costs are higher, but synthesis produces information-dense chunks that reduce query-time retrieval needs. For high-query-volume applications with distributed knowledge, cross-document synthesis improves measurably over within-document optimization.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.05265v1",
    "published_date": "2025-11-08 11:45:45 UTC",
    "updated_date": "2025-11-08 11:45:45 UTC"
  },
  {
    "arxiv_id": "2511.05969v1",
    "title": "Interpretable Recognition of Cognitive Distortions in Natural Language Texts",
    "authors": [
      "Anton Kolonin",
      "Anna Arinicheva"
    ],
    "abstract": "We propose a new approach to multi-factor classification of natural language texts based on weighted structured patterns such as N-grams, taking into account the heterarchical relationships between them, applied to solve such a socially impactful problem as the automation of detection of specific cognitive distortions in psychological care, relying on an interpretable, robust and transparent artificial intelligence model. The proposed recognition and learning algorithms improve the current state of the art in this field. The improvement is tested on two publicly available datasets, with significant improvements over literature-known F1 scores for the task, with optimal hyper-parameters determined, having code and models available for future use by the community.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.05969v1",
    "published_date": "2025-11-08 11:13:29 UTC",
    "updated_date": "2025-11-08 11:13:29 UTC"
  },
  {
    "arxiv_id": "2511.05968v1",
    "title": "DiA-gnostic VLVAE: Disentangled Alignment-Constrained Vision Language Variational AutoEncoder for Robust Radiology Reporting with Missing Modalities",
    "authors": [
      "Nagur Shareef Shaik",
      "Teja Krishna Cherukuri",
      "Adnan Masood",
      "Dong Hye Ye"
    ],
    "abstract": "The integration of medical images with clinical context is essential for generating accurate and clinically interpretable radiology reports. However, current automated methods often rely on resource-heavy Large Language Models (LLMs) or static knowledge graphs and struggle with two fundamental challenges in real-world clinical data: (1) missing modalities, such as incomplete clinical context , and (2) feature entanglement, where mixed modality-specific and shared information leads to suboptimal fusion and clinically unfaithful hallucinated findings. To address these challenges, we propose the DiA-gnostic VLVAE, which achieves robust radiology reporting through Disentangled Alignment. Our framework is designed to be resilient to missing modalities by disentangling shared and modality-specific features using a Mixture-of-Experts (MoE) based Vision-Language Variational Autoencoder (VLVAE). A constrained optimization objective enforces orthogonality and alignment between these latent representations to prevent suboptimal fusion. A compact LLaMA-X decoder then uses these disentangled representations to generate reports efficiently. On the IU X-Ray and MIMIC-CXR datasets, DiA has achieved competetive BLEU@4 scores of 0.266 and 0.134, respectively. Experimental results show that the proposed method significantly outperforms state-of-the-art models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for Oral Presentation at the 40th AAAI Conference on Artificial Intelligence (AAAI-26), Main Technical Track",
    "pdf_url": "https://arxiv.org/pdf/2511.05968v1",
    "published_date": "2025-11-08 11:08:27 UTC",
    "updated_date": "2025-11-08 11:08:27 UTC"
  },
  {
    "arxiv_id": "2511.05967v1",
    "title": "Adapted Foundation Models for Breast MRI Triaging in Contrast-Enhanced and Non-Contrast Enhanced Protocols",
    "authors": [
      "Tri-Thien Nguyen",
      "Lorenz A. Kapsner",
      "Tobias Hepp",
      "Shirin Heidarikahkesh",
      "Hannes Schreiter",
      "Luise Brock",
      "Dominika Skwierawska",
      "Dominique Hadler",
      "Julian Hossbach",
      "Evelyn Wenkel",
      "Sabine Ohlmeyer",
      "Frederik B. Laun",
      "Andrzej Liebert",
      "Andreas Maier",
      "Michael Uder",
      "Sebastian Bickelhaupt"
    ],
    "abstract": "Background: Magnetic resonance imaging (MRI) has high sensitivity for breast cancer detection, but interpretation is time-consuming. Artificial intelligence may aid in pre-screening. Purpose: To evaluate the DINOv2-based Medical Slice Transformer (MST) for ruling out significant findings (Breast Imaging Reporting and Data System [BI-RADS] >=4) in contrast-enhanced and non-contrast-enhanced abbreviated breast MRI. Materials and Methods: This institutional review board approved retrospective study included 1,847 single-breast MRI examinations (377 BI-RADS >=4) from an in-house dataset and 924 from an external validation dataset (Duke). Four abbreviated protocols were tested: T1-weighted early subtraction (T1sub), diffusion-weighted imaging with b=1500 s/mm2 (DWI1500), DWI1500+T2-weighted (T2w), and T1sub+T2w. Performance was assessed at 90%, 95%, and 97.5% sensitivity using five-fold cross-validation and area under the receiver operating characteristic curve (AUC) analysis. AUC differences were compared with the DeLong test. False negatives were characterized, and attention maps of true positives were rated in the external dataset. Results: A total of 1,448 female patients (mean age, 49 +/- 12 years) were included. T1sub+T2w achieved an AUC of 0.77 +/- 0.04; DWI1500+T2w, 0.74 +/- 0.04 (p=0.15). At 97.5% sensitivity, T1sub+T2w had the highest specificity (19% +/- 7%), followed by DWI1500+T2w (17% +/- 11%). Missed lesions had a mean diameter <10 mm at 95% and 97.5% thresholds for both T1sub and DWI1500, predominantly non-mass enhancements. External validation yielded an AUC of 0.77, with 88% of attention maps rated good or moderate. Conclusion: At 97.5% sensitivity, the MST framework correctly triaged cases without BI-RADS >=4, achieving 19% specificity for contrast-enhanced and 17% for non-contrast-enhanced MRI. Further research is warranted before clinical implementation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "23 pages, 6 figures, 4 tables. Originally submitted to Radiology (RAD-25-2541); under consideration for transfer to Radiology: Artificial Intelligence (RSNA Portfolio Journal)",
    "pdf_url": "https://arxiv.org/pdf/2511.05967v1",
    "published_date": "2025-11-08 11:01:22 UTC",
    "updated_date": "2025-11-08 11:01:22 UTC"
  },
  {
    "arxiv_id": "2511.05965v1",
    "title": "Adaptive Agent Selection and Interaction Network for Image-to-point cloud Registration",
    "authors": [
      "Zhixin Cheng",
      "Xiaotian Yin",
      "Jiacheng Deng",
      "Bohao Liao",
      "Yujia Chen",
      "Xu Zhou",
      "Baoqun Yin",
      "Tianzhu Zhang"
    ],
    "abstract": "Typical detection-free methods for image-to-point cloud registration leverage transformer-based architectures to aggregate cross-modal features and establish correspondences. However, they often struggle under challenging conditions, where noise disrupts similarity computation and leads to incorrect correspondences. Moreover, without dedicated designs, it remains difficult to effectively select informative and correlated representations across modalities, thereby limiting the robustness and accuracy of registration. To address these challenges, we propose a novel cross-modal registration framework composed of two key modules: the Iterative Agents Selection (IAS) module and the Reliable Agents Interaction (RAI) module. IAS enhances structural feature awareness with phase maps and employs reinforcement learning principles to efficiently select reliable agents. RAI then leverages these selected agents to guide cross-modal interactions, effectively reducing mismatches and improving overall robustness. Extensive experiments on the RGB-D Scenes v2 and 7-Scenes benchmarks demonstrate that our method consistently achieves state-of-the-art performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI2026",
    "pdf_url": "https://arxiv.org/pdf/2511.05965v1",
    "published_date": "2025-11-08 10:50:43 UTC",
    "updated_date": "2025-11-08 10:50:43 UTC"
  },
  {
    "arxiv_id": "2511.05951v1",
    "title": "Klear-AgentForge: Forging Agentic Intelligence through Posttraining Scaling",
    "authors": [
      "Qi Wang",
      "Hongzhi Zhang",
      "Jia Fu",
      "Kai Fu",
      "Yahui Liu",
      "Tinghai Zhang",
      "Chenxi Sun",
      "Gangwei Jiang",
      "Jingyi Tang",
      "Xingguang Ji",
      "Yang Yue",
      "Jingyuan Zhang",
      "Fuzheng Zhang",
      "Kun Gai",
      "Guorui Zhou"
    ],
    "abstract": "Despite the proliferation of powerful agentic models, the lack of critical post-training details hinders the development of strong counterparts in the open-source community. In this study, we present a comprehensive and fully open-source pipeline for training a high-performance agentic model for interacting with external tools and environments, named Klear-Qwen3-AgentForge, starting from the Qwen3-8B base model. We design effective supervised fine-tuning (SFT) with synthetic data followed by multi-turn reinforcement learning (RL) to unlock the potential for multiple diverse agentic tasks. We perform exclusive experiments on various agentic benchmarks in both tool use and coding domains. Klear-Qwen3-AgentForge-8B achieves state-of-the-art performance among LLMs of similar size and remains competitive with significantly larger models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.05951v1",
    "published_date": "2025-11-08 09:47:27 UTC",
    "updated_date": "2025-11-08 09:47:27 UTC"
  },
  {
    "arxiv_id": "2511.05940v1",
    "title": "A PDE Perspective on Generative Diffusion Models",
    "authors": [
      "Kang Liu",
      "Enrique Zuazua"
    ],
    "abstract": "Score-based diffusion models have emerged as a powerful class of generative methods, achieving state-of-the-art performance across diverse domains. Despite their empirical success, the mathematical foundations of those models remain only partially understood, particularly regarding the stability and consistency of the underlying stochastic and partial differential equations governing their dynamics.\n  In this work, we develop a rigorous partial differential equation (PDE) framework for score-based diffusion processes. Building on the Li--Yau differential inequality for the heat flow, we prove well-posedness and derive sharp $L^p$-stability estimates for the associated score-based Fokker--Planck dynamics, providing a mathematically consistent description of their temporal evolution. Through entropy stability methods, we further show that the reverse-time dynamics of diffusion models concentrate on the data manifold for compactly supported data distributions and a broad class of initialization schemes, with a concentration rate of order $\\sqrt{t}$ as $t \\to 0$.\n  These results yield a theoretical guarantee that, under exact score guidance, diffusion trajectories return to the data manifold while preserving imitation fidelity. Our findings also provide practical insights for designing diffusion models, including principled criteria for score-function construction, loss formulation, and stopping-time selection. Altogether, this framework provides a quantitative understanding of the trade-off between generative capacity and imitation fidelity, bridging rigorous analysis and model design within a unified mathematical perspective.",
    "categories": [
      "math.OC",
      "cs.AI",
      "math.AP"
    ],
    "primary_category": "math.OC",
    "comment": "30 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.05940v1",
    "published_date": "2025-11-08 09:19:25 UTC",
    "updated_date": "2025-11-08 09:19:25 UTC"
  },
  {
    "arxiv_id": "2511.05936v1",
    "title": "10 Open Challenges Steering the Future of Vision-Language-Action Models",
    "authors": [
      "Soujanya Poria",
      "Navonil Majumder",
      "Chia-Yu Hung",
      "Amir Ali Bagherzadeh",
      "Chuan Li",
      "Kenneth Kwok",
      "Ziwei Wang",
      "Cheston Tan",
      "Jiajun Wu",
      "David Hsu"
    ],
    "abstract": "Due to their ability of follow natural language instructions, vision-language-action (VLA) models are increasingly prevalent in the embodied AI arena, following the widespread success of their precursors -- LLMs and VLMs. In this paper, we discuss 10 principal milestones in the ongoing development of VLA models -- multimodality, reasoning, data, evaluation, cross-robot action generalization, efficiency, whole-body coordination, safety, agents, and coordination with humans. Furthermore, we discuss the emerging trends of using spatial understanding, modeling world dynamics, post training, and data synthesis -- all aiming to reach these milestones. Through these discussions, we hope to bring attention to the research avenues that may accelerate the development of VLA models into wider acceptability.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "AAAI 2026 (Senior Track)",
    "pdf_url": "https://arxiv.org/pdf/2511.05936v1",
    "published_date": "2025-11-08 09:02:13 UTC",
    "updated_date": "2025-11-08 09:02:13 UTC"
  },
  {
    "arxiv_id": "2511.05933v1",
    "title": "Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs",
    "authors": [
      "Renfei Zhang",
      "Manasa Kaniselvan",
      "Niloofar Mireshghallah"
    ],
    "abstract": "Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement \"code 57.95 refers to urinary infection\") maintain high cosine similarity between SFT and RL models, query representations (e.g., \"what is code 57.95\") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "`",
    "pdf_url": "https://arxiv.org/pdf/2511.05933v1",
    "published_date": "2025-11-08 08:56:29 UTC",
    "updated_date": "2025-11-08 08:56:29 UTC"
  },
  {
    "arxiv_id": "2511.05932v1",
    "title": "The Future of AI in the GCC Post-NPM Landscape: A Comparative Analysis of Kuwait and the UAE",
    "authors": [
      "Mohammad Rashed Albous",
      "Bedour Alboloushi",
      "Arnaud Lacheret"
    ],
    "abstract": "Comparative evidence on how Gulf Cooperation Council (GCC) states turn artificial intelligence (AI) ambitions into post--New Public Management (post-NPM) outcomes is scarce because most studies examine Western democracies. We analyze constitutional, collective-choice, and operational rules shaping AI uptake in two contrasting GCC members, the United Arab Emirates (UAE) and Kuwait, and whether they foster citizen centricity, collaborative governance, and public value creation. Anchored in Ostrom's Institutional Analysis and Development framework, the study combines a most similar/most different systems design with multiple sources: 62 public documents from 2018--2025, embedded UAE cases (Smart Dubai and MBZUAI), and 39 interviews with officials conducted Aug 2024--May 2025. Dual coding and process tracing connect rule configurations to AI performance. Cross-case analysis identifies four reinforcing mechanisms behind divergent trajectories. In the UAE, concentrated authority, credible sanctions, pro-innovation narratives, and flexible reinvestment rules scale pilots into hundreds of services and sizable recycled savings. In Kuwait, dispersed veto points, exhortative sanctions, cautious discourse, and lapsed AI budgets confine initiatives to pilot mode despite equivalent fiscal resources. The findings refine institutional theory by showing that vertical rule coherence, not wealth, determines AI's public-value yield, and temper post-NPM optimism by revealing that efficiency metrics serve societal goals only when backed by enforceable safeguards. To curb ethics washing and test transferability beyond the GCC, future work should track rule diffusion over time, develop blended legitimacy--efficiency scorecards, and examine how narrative framing shapes citizen consent for data sharing.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "econ.TH"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05932v1",
    "published_date": "2025-11-08 08:54:27 UTC",
    "updated_date": "2025-11-08 08:54:27 UTC"
  },
  {
    "arxiv_id": "2511.05931v1",
    "title": "Self-Abstraction from Grounded Experience for Plan-Guided Policy Refinement",
    "authors": [
      "Hiroaki Hayashi",
      "Bo Pang",
      "Wenting Zhao",
      "Ye Liu",
      "Akash Gokul",
      "Srijan Bansal",
      "Caiming Xiong",
      "Semih Yavuz",
      "Yingbo Zhou"
    ],
    "abstract": "Large language model (LLM) based agents are increasingly used to tackle software engineering tasks that require multi-step reasoning and code modification, demonstrating promising yet limited performance. However, most existing LLM agents typically operate within static execution frameworks, lacking a principled mechanism to learn and self-improve from their own experience and past rollouts. As a result, their performance remains bounded by the initial framework design and the underlying LLM's capabilities. We propose Self-Abstraction from Grounded Experience (SAGE), a framework that enables agents to learn from their own task executions and refine their behavior through self-abstraction. After an initial rollout, the agent induces a concise plan abstraction from its grounded experience, distilling key steps, dependencies, and constraints. This learned abstraction is then fed back as contextual guidance, refining the agent's policy and supporting more structured, informed subsequent executions. Empirically, SAGE delivers consistent performance gains across diverse LLM backbones and agent architectures. Notably, it yields a 7.2% relative performance improvement over the strong Mini-SWE-Agent baseline when paired with the GPT-5 (high) backbone. SAGE further achieves strong overall performance on SWE-Bench Verified benchmark, reaching 73.2% and 74% Pass@1 resolve rates with the Mini-SWE-Agent and OpenHands CodeAct agent framework, respectively.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05931v1",
    "published_date": "2025-11-08 08:49:38 UTC",
    "updated_date": "2025-11-08 08:49:38 UTC"
  },
  {
    "arxiv_id": "2511.05929v1",
    "title": "CoMA: Complementary Masking and Hierarchical Dynamic Multi-Window Self-Attention in a Unified Pre-training Framework",
    "authors": [
      "Jiaxuan Li",
      "Qing Xu",
      "Xiangjian He",
      "Ziyu Liu",
      "Chang Xing",
      "Zhen Chen",
      "Daokun Zhang",
      "Rong Qu",
      "Chang Wen Chen"
    ],
    "abstract": "Masked Autoencoders (MAE) achieve self-supervised learning of image representations by randomly removing a portion of visual tokens and reconstructing the original image as a pretext task, thereby significantly enhancing pretraining efficiency and yielding excellent adaptability across downstream tasks. However, MAE and other MAE-style paradigms that adopt random masking generally require more pre-training epochs to maintain adaptability. Meanwhile, ViT in MAE suffers from inefficient parameter use due to fixed spatial resolution across layers. To overcome these limitations, we propose the Complementary Masked Autoencoders (CoMA), which employ a complementary masking strategy to ensure uniform sampling across all pixels, thereby improving effective learning of all features and enhancing the model's adaptability. Furthermore, we introduce DyViT, a hierarchical vision transformer that employs a Dynamic Multi-Window Self-Attention (DM-MSA), significantly reducing the parameters and FLOPs while improving fine-grained feature learning. Pre-trained on ImageNet-1K with CoMA, DyViT matches the downstream performance of MAE using only 12% of the pre-training epochs, demonstrating more effective learning. It also attains a 10% reduction in pre-training time per epoch, further underscoring its superior pre-training efficiency.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.05929v1",
    "published_date": "2025-11-08 08:43:41 UTC",
    "updated_date": "2025-11-08 08:43:41 UTC"
  },
  {
    "arxiv_id": "2511.05927v1",
    "title": "Artificial intelligence and the Gulf Cooperation Council workforce adapting to the future of work",
    "authors": [
      "Mohammad Rashed Albous",
      "Melodena Stephens",
      "Odeh Rashed Al-Jayyousi"
    ],
    "abstract": "The rapid expansion of artificial intelligence (AI) in the Gulf Cooperation Council (GCC) raises a central question: are investments in compute infrastructure matched by an equally robust build-out of skills, incentives, and governance? Grounded in socio-technical systems (STS) theory, this mixed-methods study audits workforce preparedness across Kingdom of Saudi Arabia (KSA), the United Arab Emirates (UAE), Qatar, Kuwait, Bahrain, and Oman. We combine term frequency--inverse document frequency (TF--IDF) analysis of six national AI strategies (NASs), an inventory of 47 publicly disclosed AI initiatives (January 2017--April 2025), paired case studies, the Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) and the Saudi Data & Artificial Intelligence Authority (SDAIA) Academy, and a scenario matrix linking oil-revenue slack (technical capacity) to regulatory coherence (social alignment). Across the corpus, 34/47 initiatives (0.72; 95% Wilson CI 0.58--0.83) exhibit joint social--technical design; country-level indices span 0.57--0.90 (small n; intervals overlap). Scenario results suggest that, under our modeled conditions, regulatory convergence plausibly binds outcomes more than fiscal capacity: fragmented rules can offset high oil revenues, while harmonized standards help preserve progress under austerity. We also identify an emerging two-track talent system, research elites versus rapidly trained practitioners, that risks labor-market bifurcation without bridging mechanisms. By extending STS inquiry to oil-rich, state-led economies, the study refines theory and sets a research agenda focused on longitudinal coupling metrics, ethnographies of coordination, and outcome-based performance indicators.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "econ.GN"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05927v1",
    "published_date": "2025-11-08 08:42:14 UTC",
    "updated_date": "2025-11-08 08:42:14 UTC"
  },
  {
    "arxiv_id": "2511.05921v1",
    "title": "IDALC: A Semi-Supervised Framework for Intent Detection and Active Learning based Correction",
    "authors": [
      "Ankan Mullick",
      "Sukannya Purkayastha",
      "Saransh Sharma",
      "Pawan Goyal",
      "Niloy Ganguly"
    ],
    "abstract": "Voice-controlled dialog systems have become immensely popular due to their ability to perform a wide range of actions in response to diverse user queries. These agents possess a predefined set of skills or intents to fulfill specific user tasks. But every system has its own limitations. There are instances where, even for known intents, if any model exhibits low confidence, it results in rejection of utterances that necessitate manual annotation. Additionally, as time progresses, there may be a need to retrain these agents with new intents from the system-rejected queries to carry out additional tasks. Labeling all these emerging intents and rejected utterances over time is impractical, thus calling for an efficient mechanism to reduce annotation costs. In this paper, we introduce IDALC (Intent Detection and Active Learning based Correction), a semi-supervised framework designed to detect user intents and rectify system-rejected utterances while minimizing the need for human annotation. Empirical findings on various benchmark datasets demonstrate that our system surpasses baseline methods, achieving a 5-10% higher accuracy and a 4-8% improvement in macro-F1. Remarkably, we maintain the overall annotation cost at just 6-10% of the unlabelled data available to the system. The overall framework of IDALC is shown in Fig. 1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Paper accepted in IEEE Transactions on Artificial Intelligence (October 2025)",
    "pdf_url": "https://arxiv.org/pdf/2511.05921v1",
    "published_date": "2025-11-08 08:32:59 UTC",
    "updated_date": "2025-11-08 08:32:59 UTC"
  },
  {
    "arxiv_id": "2511.05920v1",
    "title": "IoT-based Fresh Produce Supply Chain Under Uncertainty: An Adaptive Optimization Framework",
    "authors": [
      "Chirag Seth",
      "Mehrdad Pirnia",
      "James H Bookbinder"
    ],
    "abstract": "Fruits and vegetables form a vital component of the global economy; however, their distribution poses complex logistical challenges due to high perishability, supply fluctuations, strict quality and safety standards, and environmental sensitivity. In this paper, we propose an adaptive optimization model that accounts for delays, travel time, and associated temperature changes impacting produce shelf life, and compare it against traditional approaches such as Robust Optimization, Distributionally Robust Optimization, and Stochastic Programming. Additionally, we conduct a series of computational experiments using Internet of Things (IoT) sensor data to evaluate the performance of our proposed model. Our study demonstrates that the proposed adaptive model achieves a higher shelf life, extending it by over 18\\% compared to traditional optimization models, by dynamically mitigating temperature deviations through a temperature feedback mechanism. The promising results demonstrate the potential of this approach to improve both the freshness and efficiency of logistics systems an aspect often neglected in previous works.",
    "categories": [
      "math.OC",
      "cs.AI"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05920v1",
    "published_date": "2025-11-08 08:31:23 UTC",
    "updated_date": "2025-11-08 08:31:23 UTC"
  },
  {
    "arxiv_id": "2511.05919v2",
    "title": "Injecting Falsehoods: Adversarial Man-in-the-Middle Attacks Undermining Factual Recall in LLMs",
    "authors": [
      "Alina Fastowski",
      "Bardh Prenkaj",
      "Yuxiao Li",
      "Gjergji Kasneci"
    ],
    "abstract": "LLMs are now an integral part of information retrieval. As such, their role as question answering chatbots raises significant concerns due to their shown vulnerability to adversarial man-in-the-middle (MitM) attacks. Here, we propose the first principled attack evaluation on LLM factual memory under prompt injection via Xmera, our novel, theory-grounded MitM framework. By perturbing the input given to \"victim\" LLMs in three closed-book and fact-based QA settings, we undermine the correctness of the responses and assess the uncertainty of their generation process. Surprisingly, trivial instruction-based attacks report the highest success rate (up to ~85.3%) while simultaneously having a high uncertainty for incorrectly answered questions. To provide a simple defense mechanism against Xmera, we train Random Forest classifiers on the response uncertainty levels to distinguish between attacked and unattacked queries (average AUC of up to ~96%). We believe that signaling users to be cautious about the answers they receive from black-box and potentially corrupt LLMs is a first checkpoint toward user cyberspace safety.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05919v2",
    "published_date": "2025-11-08 08:30:19 UTC",
    "updated_date": "2025-11-20 10:04:04 UTC"
  },
  {
    "arxiv_id": "2511.05913v1",
    "title": "NILC: Discovering New Intents with LLM-assisted Clustering",
    "authors": [
      "Hongtao Wang",
      "Renchi Yang",
      "Wenqing Lin"
    ],
    "abstract": "New intent discovery (NID) seeks to recognize both new and known intents from unlabeled user utterances, which finds prevalent use in practical dialogue systems. Existing works towards NID mainly adopt a cascaded architecture, wherein the first stage focuses on encoding the utterances into informative text embeddings beforehand, while the latter is to group similar embeddings into clusters (i.e., intents), typically by K-Means. However, such a cascaded pipeline fails to leverage the feedback from both steps for mutual refinement, and, meanwhile, the embedding-only clustering overlooks nuanced textual semantics, leading to suboptimal performance. To bridge this gap, this paper proposes NILC, a novel clustering framework specially catered for effective NID. Particularly, NILC follows an iterative workflow, in which clustering assignments are judiciously updated by carefully refining cluster centroids and text embeddings of uncertain utterances with the aid of large language models (LLMs). Specifically, NILC first taps into LLMs to create additional semantic centroids for clusters, thereby enriching the contextual semantics of the Euclidean centroids of embeddings. Moreover, LLMs are then harnessed to augment hard samples (ambiguous or terse utterances) identified from clusters via rewriting for subsequent cluster correction. Further, we inject supervision signals through non-trivial techniques seeding and soft must links for more accurate NID in the semi-supervised setting. Extensive experiments comparing NILC against multiple recent baselines under both unsupervised and semi-supervised settings showcase that NILC can achieve significant performance improvements over six benchmark datasets of diverse domains consistently.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05913v1",
    "published_date": "2025-11-08 08:18:44 UTC",
    "updated_date": "2025-11-08 08:18:44 UTC"
  },
  {
    "arxiv_id": "2511.05903v1",
    "title": "The Imperfect Learner: Incorporating Developmental Trajectories in Memory-based Student Simulation",
    "authors": [
      "Zhengyuan Liu",
      "Stella Xin Yin",
      "Bryan Chen Zhengyu Tan",
      "Roy Ka-Wei Lee",
      "Guimei Liu",
      "Dion Hoe-Lian Goh",
      "Wenya Wang",
      "Nancy F. Chen"
    ],
    "abstract": "User simulation is important for developing and evaluating human-centered AI, yet current student simulation in educational applications has significant limitations. Existing approaches focus on single learning experiences and do not account for students' gradual knowledge construction and evolving skill sets. Moreover, large language models are optimized to produce direct and accurate responses, making it challenging to represent the incomplete understanding and developmental constraints that characterize real learners. In this paper, we introduce a novel framework for memory-based student simulation that incorporates developmental trajectories through a hierarchical memory mechanism with structured knowledge representation. The framework also integrates metacognitive processes and personality traits to enrich the individual learner profiling, through dynamical consolidation of both cognitive development and personal learning characteristics. In practice, we implement a curriculum-aligned simulator grounded on the Next Generation Science Standards. Experimental results show that our approach can effectively reflect the gradual nature of knowledge development and the characteristic difficulties students face, providing a more accurate representation of learning processes.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05903v1",
    "published_date": "2025-11-08 08:05:43 UTC",
    "updated_date": "2025-11-08 08:05:43 UTC"
  },
  {
    "arxiv_id": "2511.05901v2",
    "title": "Retrieval-Augmented Generation in Medicine: A Scoping Review of Technical Implementations, Clinical Applications, and Ethical Considerations",
    "authors": [
      "Rui Yang",
      "Matthew Yu Heng Wong",
      "Huitao Li",
      "Xin Li",
      "Wentao Zhu",
      "Jingchi Liao",
      "Kunyu Yu",
      "Jonathan Chong Kai Liew",
      "Weihao Xuan",
      "Yingjian Chen",
      "Yuhe Ke",
      "Jasmine Chiat Ling Ong",
      "Douglas Teodoro",
      "Chuan Hong",
      "Daniel Shi Wei Ting",
      "Nan Liu"
    ],
    "abstract": "The rapid growth of medical knowledge and increasing complexity of clinical practice pose challenges. In this context, large language models (LLMs) have demonstrated value; however, inherent limitations remain. Retrieval-augmented generation (RAG) technologies show potential to enhance their clinical applicability. This study reviewed RAG applications in medicine. We found that research primarily relied on publicly available data, with limited application in private data. For retrieval, approaches commonly relied on English-centric embedding models, while LLMs were mostly generic, with limited use of medical-specific LLMs. For evaluation, automated metrics evaluated generation quality and task performance, whereas human evaluation focused on accuracy, completeness, relevance, and fluency, with insufficient attention to bias and safety. RAG applications were concentrated on question answering, report generation, text summarization, and information extraction. Overall, medical RAG remains at an early stage, requiring advances in clinical validation, cross-linguistic adaptation, and support for low-resource settings to enable trustworthy and responsible global use.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05901v2",
    "published_date": "2025-11-08 07:52:47 UTC",
    "updated_date": "2025-11-13 06:14:22 UTC"
  },
  {
    "arxiv_id": "2511.05898v1",
    "title": "GABFusion: Rethinking Feature Fusion for Low-Bit Quantization of Multi-Task Networks",
    "authors": [
      "Zhaoyang Wang",
      "Dong Wang"
    ],
    "abstract": "Despite the effectiveness of quantization-aware training (QAT) in compressing deep neural networks, its performance on multi-task architectures often degrades significantly due to task-specific feature discrepancies and gradient conflicts. To address these challenges, we propose Gradient-Aware Balanced Feature Fusion (GABFusion), which dynamically balances gradient magnitudes and fuses task-specific features in a quantization-friendly manner. We further introduce Attention Distribution Alignment (ADA), a feature-level distillation strategy tailored for quantized models. Our method demonstrates strong generalization across network architectures and QAT algorithms, with theoretical guarantees on gradient bias reduction. Extensive experiments demonstrate that our strategy consistently enhances a variety of QAT methods across different network architectures and bit-widths. On PASCAL VOC and COCO datasets, the proposed approach achieves average mAP improvements of approximately 3.3% and 1.6%, respectively. When applied to YOLOv5 under 4-bit quantization, our method narrows the accuracy gap with the full-precision model to only 1.7% on VOC, showcasing its effectiveness in preserving performance under low-bit constraints. Notably, the proposed framework is modular, easy to integrate, and compatible with any existing QAT technique-enhancing the performance of quantized models without requiring modifications to the original network architecture.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages,6 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.05898v1",
    "published_date": "2025-11-08 07:45:21 UTC",
    "updated_date": "2025-11-08 07:45:21 UTC"
  },
  {
    "arxiv_id": "2511.05885v2",
    "title": "A Remarkably Efficient Paradigm to Multimodal Large Language Models for Sequential Recommendation",
    "authors": [
      "Qiyong Zhong",
      "Jiajie Su",
      "Ming Yang",
      "Yunshan Ma",
      "Xiaolin Zheng",
      "Chaochao Chen"
    ],
    "abstract": "Sequential recommendations (SR) predict users' future interactions based on their historical behavior. The rise of Large Language Models (LLMs) has brought powerful generative and reasoning capabilities, significantly enhancing SR performance, while Multimodal LLMs (MLLMs) further extend this by introducing data like images and interactive relationships. However, critical issues remain, i.e., (a) Suboptimal item representations caused by lengthy and redundant descriptions, leading to inefficiencies in both training and inference; (b) Modality-related cognitive bias, as LLMs are predominantly pretrained on textual data, limiting their ability to effectively integrate and utilize non-textual modalities; (c) Weakening sequential perception in long interaction sequences, where attention mechanisms struggle to capture earlier interactions, hindering the modeling of long-range dependencies. To address these issues, we propose Speeder, an efficient MLLM-based paradigm for SR featuring three key innovations: 1) Multimodal Representation Compression (MRC), which condenses item attributes into concise yet informative tokens, reducing redundancy and computational cost; 2) Modality-aware Progressive Optimization (MPO), enabling gradual learning of multimodal representations; 3) Sequential Position Awareness Enhancement (SPAE), improving the LLM's capability to capture both relative and absolute sequential dependencies in long interaction sequences. Extensive experiments on real-world datasets demonstrate the effectiveness and efficiency of Speeder. Speeder increases training speed to 250% of the original while reducing inference time to 25% on the Amazon dataset.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05885v2",
    "published_date": "2025-11-08 06:51:38 UTC",
    "updated_date": "2025-11-11 08:48:10 UTC"
  },
  {
    "arxiv_id": "2511.05883v1",
    "title": "Unveiling Modality Bias: Automated Sample-Specific Analysis for Multimodal Misinformation Benchmarks",
    "authors": [
      "Hehai Lin",
      "Hui Liu",
      "Shilei Cao",
      "Jing Li",
      "Haoliang Li",
      "Wenya Wang"
    ],
    "abstract": "Numerous multimodal misinformation benchmarks exhibit bias toward specific modalities, allowing detectors to make predictions based solely on one modality. While previous research has quantified bias at the dataset level or manually identified spurious correlations between modalities and labels, these approaches lack meaningful insights at the sample level and struggle to scale to the vast amount of online information. In this paper, we investigate the design for automated recognition of modality bias at the sample level. Specifically, we propose three bias quantification methods based on theories/views of different levels of granularity: 1) a coarse-grained evaluation of modality benefit; 2) a medium-grained quantification of information flow; and 3) a fine-grained causality analysis. To verify the effectiveness, we conduct a human evaluation on two popular benchmarks. Experimental results reveal three interesting findings that provide potential direction toward future research: 1)~Ensembling multiple views is crucial for reliable automated analysis; 2)~Automated analysis is prone to detector-induced fluctuations; and 3)~Different views produce a higher agreement on modality-balanced samples but diverge on biased ones.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05883v1",
    "published_date": "2025-11-08 06:48:19 UTC",
    "updated_date": "2025-11-08 06:48:19 UTC"
  },
  {
    "arxiv_id": "2511.05879v2",
    "title": "Physics-Informed Neural Networks for Real-Time Gas Crossover Prediction in PEM Electrolyzers: First Application with Multi-Membrane Validation",
    "authors": [
      "Yong-Woon Kim",
      "Chulung Kang",
      "Yung-Cheol Byun"
    ],
    "abstract": "Green hydrogen production via polymer electrolyte membrane (PEM) water electrolysis is pivotal for energy transition, yet hydrogen crossover through membranes threatens safety and economic viability-approaching explosive limits (4 mol% H$_2$ in O$_2$) while reducing Faradaic efficiency by 2.5%. Current physics-based models require extensive calibration and computational resources that preclude real-time implementation, while purely data-driven approaches fail to extrapolate beyond training conditions-critical for dynamic electrolyzer operation. Here we present the first application of physics-informed neural networks (PINNs) for hydrogen crossover prediction, integrating mass conservation, Fick's diffusion law, and Henry's solubility law within a compact architecture (17,793 parameters). Validated across six membranes under industrially relevant conditions (0.05-5.0 A/cm$^2$, 1-200 bar, 25-85°C), our PINN achieves exceptional accuracy (R$^{2}$ = 99.84% $\\pm$ 0.15\\%, RMSE = 0.0932% $\\pm$ 0.0438%) based on five-fold cross-validation, with sub-millisecond inference times suitable for real-time control. Remarkably, the model maintains R$^2$ > 86% when predicting crossover at pressures 2.5x beyond training range-substantially outperforming pure neural networks (R$^2$ = 43.4%). The hardware-agnostic deployment, from desktop CPUs to edge devices (Raspberry Pi 4), enables distributed safety monitoring essential for gigawatt-scale installations. By bridging physical rigor and computational efficiency, this work establishes a new paradigm for real-time electrolyzer monitoring, accelerating deployment of safe, efficient green hydrogen infrastructure crucial for net-zero emissions targets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05879v2",
    "published_date": "2025-11-08 06:41:39 UTC",
    "updated_date": "2025-11-18 11:54:56 UTC"
  },
  {
    "arxiv_id": "2511.05875v1",
    "title": "Towards a Humanized Social-Media Ecosystem: AI-Augmented HCI Design Patterns for Safety, Agency & Well-Being",
    "authors": [
      "Mohd Ruhul Ameen",
      "Akif Islam"
    ],
    "abstract": "Social platforms connect billions of people, yet their engagement-first algorithms often work on users rather than with them, amplifying stress, misinformation, and a loss of control. We propose Human-Layer AI (HL-AI)--user-owned, explainable intermediaries that sit in the browser between platform logic and the interface. HL-AI gives people practical, moment-to-moment control without requiring platform cooperation. We contribute a working Chrome/Edge prototype implementing five representative pattern frameworks--Context-Aware Post Rewriter, Post Integrity Meter, Granular Feed Curator, Micro-Withdrawal Agent, and Recovery Mode--alongside a unifying mathematical formulation balancing user utility, autonomy costs, and risk thresholds. Evaluation spans technical accuracy, usability, and behavioral outcomes. The result is a suite of humane controls that help users rewrite before harm, read with integrity cues, tune feeds with intention, pause compulsive loops, and seek shelter during harassment, all while preserving agency through explanations and override options. This prototype offers a practical path to retrofit today's feeds with safety, agency, and well-being, inviting rigorous cross-cultural user evaluation.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.HC",
    "comment": "6 pages, 5 tables, 7 figures, and 2 algorithm tables. Accepted at International Conference on Signal Processing, Information, Communication and Systems (SPICSCON 2025)",
    "pdf_url": "https://arxiv.org/pdf/2511.05875v1",
    "published_date": "2025-11-08 06:22:15 UTC",
    "updated_date": "2025-11-08 06:22:15 UTC"
  },
  {
    "arxiv_id": "2511.05874v1",
    "title": "An Empirical Study of Reasoning Steps in Thinking Code LLMs",
    "authors": [
      "Haoran Xue",
      "Gias Uddin",
      "Song Wang"
    ],
    "abstract": "Thinking Large Language Models (LLMs) generate explicit intermediate reasoning traces before final answers, potentially improving transparency, interpretability, and solution accuracy for code generation. However, the quality of these reasoning chains remains underexplored. We present a comprehensive empirical study examining the reasoning process and quality of thinking LLMs for code generation. We evaluate six state-of-the-art reasoning LLMs (DeepSeek-R1, OpenAI-o3-mini, Claude-3.7-Sonnet-Thinking, Gemini-2.0-Flash-Thinking, Gemini-2.5-Flash, and Qwen-QwQ) across 100 code generation tasks of varying difficulty from BigCodeBench. We quantify reasoning-chain structure through step counts and verbosity, conduct controlled step-budget adjustments, and perform a 21-participant human evaluation across three dimensions: efficiency, logical correctness, and completeness. Our step-count interventions reveal that targeted step increases can improve resolution rates for certain models/tasks, while modest reductions often preserve success on standard tasks, rarely on hard ones. Through systematic analysis, we develop a reasoning-problematic taxonomy, identifying completeness as the dominant failure mode. Task complexity significantly impacts reasoning quality; hard problems are substantially more prone to incompleteness than standard tasks. Our stability analysis demonstrates that thinking LLMs maintain consistent logical structures across computational effort levels and can self-correct previous errors. This study provides new insights into the strengths and limitations of current thinking LLMs in software engineering.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05874v1",
    "published_date": "2025-11-08 06:18:48 UTC",
    "updated_date": "2025-11-08 06:18:48 UTC"
  },
  {
    "arxiv_id": "2511.05873v2",
    "title": "EndoIR: Degradation-Agnostic All-in-One Endoscopic Image Restoration via Noise-Aware Routing Diffusion",
    "authors": [
      "Tong Chen",
      "Xinyu Ma",
      "Long Bai",
      "Wenyang Wang",
      "Yue Sun",
      "Luping Zhou"
    ],
    "abstract": "Endoscopic images often suffer from diverse and co-occurring degradations such as low lighting, smoke, and bleeding, which obscure critical clinical details. Existing restoration methods are typically task-specific and often require prior knowledge of the degradation type, limiting their robustness in real-world clinical use. We propose EndoIR, an all-in-one, degradation-agnostic diffusion-based framework that restores multiple degradation types using a single model. EndoIR introduces a Dual-Domain Prompter that extracts joint spatial-frequency features, coupled with an adaptive embedding that encodes both shared and task-specific cues as conditioning for denoising. To mitigate feature confusion in conventional concatenation-based conditioning, we design a Dual-Stream Diffusion architecture that processes clean and degraded inputs separately, with a Rectified Fusion Block integrating them in a structured, degradation-aware manner. Furthermore, Noise-Aware Routing Block improves efficiency by dynamically selecting only noise-relevant features during denoising. Experiments on SegSTRONG-C and CEC datasets demonstrate that EndoIR achieves state-of-the-art performance across multiple degradation scenarios while using fewer parameters than strong baselines, and downstream segmentation experiments confirm its clinical utility.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05873v2",
    "published_date": "2025-11-08 06:17:51 UTC",
    "updated_date": "2025-11-11 01:49:50 UTC"
  },
  {
    "arxiv_id": "2511.05872v1",
    "title": "Adaptation and Fine-tuning with TabPFN for Travelling Salesman Problem",
    "authors": [
      "Nguyen Gia Hien Vu",
      "Yifan Tang",
      "Rey Lim",
      "Yifan Yang",
      "Hang Ma",
      "Ke Wang",
      "G. Gary Wang"
    ],
    "abstract": "Tabular Prior-Data Fitted Network (TabPFN) is a foundation model designed for small to medium-sized tabular data, which has attracted much attention recently. This paper investigates the application of TabPFN in Combinatorial Optimization (CO) problems. The aim is to lessen challenges in time and data-intensive training requirements often observed in using traditional methods including exact and heuristic algorithms, Machine Learning (ML)-based models, to solve CO problems. Proposing possibly the first ever application of TabPFN for such a purpose, we adapt and fine-tune the TabPFN model to solve the Travelling Salesman Problem (TSP), one of the most well-known CO problems. Specifically, we adopt the node-based approach and the node-predicting adaptation strategy to construct the entire TSP route. Our evaluation with varying instance sizes confirms that TabPFN requires minimal training, adapts to TSP using a single sample, performs better generalization across varying TSP instance sizes, and reduces performance degradation. Furthermore, the training process with adaptation and fine-tuning is completed within minutes. The methodology leads to strong solution quality even without post-processing and achieves performance comparable to other models with post-processing refinement. Our findings suggest that the TabPFN model is a promising approach to solve structured and CO problems efficiently under training resource constraints and rapid deployment requirements.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.CO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05872v1",
    "published_date": "2025-11-08 06:16:11 UTC",
    "updated_date": "2025-11-08 06:16:11 UTC"
  },
  {
    "arxiv_id": "2511.05865v2",
    "title": "CGCE: Classifier-Guided Concept Erasure in Generative Models",
    "authors": [
      "Viet Nguyen",
      "Vishal M. Patel"
    ],
    "abstract": "Recent advancements in large-scale generative models have enabled the creation of high-quality images and videos, but have also raised significant safety concerns regarding the generation of unsafe content. To mitigate this, concept erasure methods have been developed to remove undesirable concepts from pre-trained models. However, existing methods remain vulnerable to adversarial attacks that can regenerate the erased content. Moreover, achieving robust erasure often degrades the model's generative quality for safe, unrelated concepts, creating a difficult trade-off between safety and performance. To address this challenge, we introduce Classifier-Guided Concept Erasure (CGCE), an efficient plug-and-play framework that provides robust concept erasure for diverse generative models without altering their original weights. CGCE uses a lightweight classifier operating on text embeddings to first detect and then refine prompts containing undesired concepts. This approach is highly scalable, allowing for multi-concept erasure by aggregating guidance from several classifiers. By modifying only unsafe embeddings at inference time, our method prevents harmful content generation while preserving the model's original quality on benign prompts. Extensive experiments show that CGCE achieves state-of-the-art robustness against a wide range of red-teaming attacks. Our approach also maintains high generative utility, demonstrating a superior balance between safety and performance. We showcase the versatility of CGCE through its successful application to various modern T2I and T2V models, establishing it as a practical and effective solution for safe generative AI.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "26 pages, 17 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.05865v2",
    "published_date": "2025-11-08 05:38:18 UTC",
    "updated_date": "2025-11-25 17:27:33 UTC"
  },
  {
    "arxiv_id": "2511.05863v2",
    "title": "EMOD: A Unified EEG Emotion Representation Framework Leveraging V-A Guided Contrastive Learning",
    "authors": [
      "Yuning Chen",
      "Sha Zhao",
      "Shijian Li",
      "Gang Pan"
    ],
    "abstract": "Emotion recognition from EEG signals is essential for affective computing and has been widely explored using deep learning. While recent deep learning approaches have achieved strong performance on single EEG emotion datasets, their generalization across datasets remains limited due to the heterogeneity in annotation schemes and data formats. Existing models typically require dataset-specific architectures tailored to input structure and lack semantic alignment across diverse emotion labels. To address these challenges, we propose EMOD: A Unified EEG Emotion Representation Framework Leveraging Valence-Arousal (V-A) Guided Contrastive Learning. EMOD learns transferable and emotion-aware representations from heterogeneous datasets by bridging both semantic and structural gaps. Specifically, we project discrete and continuous emotion labels into a unified V-A space and formulate a soft-weighted supervised contrastive loss that encourages emotionally similar samples to cluster in the latent space. To accommodate variable EEG formats, EMOD employs a flexible backbone comprising a Triple-Domain Encoder followed by a Spatial-Temporal Transformer, enabling robust extraction and integration of temporal, spectral, and spatial features. We pretrain EMOD on 8 public EEG datasets and evaluate its performance on three benchmark datasets. Experimental results show that EMOD achieves the state-of-the-art performance, demonstrating strong adaptability and generalization across diverse EEG-based emotion recognition scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05863v2",
    "published_date": "2025-11-08 05:37:16 UTC",
    "updated_date": "2025-11-14 08:21:44 UTC"
  },
  {
    "arxiv_id": "2511.05859v1",
    "title": "Predicting the Future by Retrieving the Past",
    "authors": [
      "Dazhao Du",
      "Tao Han",
      "Song Guo"
    ],
    "abstract": "Deep learning models such as MLP, Transformer, and TCN have achieved remarkable success in univariate time series forecasting, typically relying on sliding window samples from historical data for training. However, while these models implicitly compress historical information into their parameters during training, they are unable to explicitly and dynamically access this global knowledge during inference, relying only on the local context within the lookback window. This results in an underutilization of rich patterns from the global history. To bridge this gap, we propose Predicting the Future by Retrieving the Past (PFRP), a novel approach that explicitly integrates global historical data to enhance forecasting accuracy. Specifically, we construct a Global Memory Bank (GMB) to effectively store and manage global historical patterns. A retrieval mechanism is then employed to extract similar patterns from the GMB, enabling the generation of global predictions. By adaptively combining these global predictions with the outputs of any local prediction model, PFRP produces more accurate and interpretable forecasts. Extensive experiments conducted on seven real-world datasets demonstrate that PFRP significantly enhances the average performance of advanced univariate forecasting models by 8.4\\%. Codes can be found in https://github.com/ddz16/PFRP.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2511.05859v1",
    "published_date": "2025-11-08 05:24:45 UTC",
    "updated_date": "2025-11-08 05:24:45 UTC"
  },
  {
    "arxiv_id": "2511.05854v1",
    "title": "Can a Small Model Learn to Look Before It Leaps? Dynamic Learning and Proactive Correction for Hallucination Detection",
    "authors": [
      "Zepeng Bao",
      "Shen Zhou",
      "Qiankun Pi",
      "Jianhao Chen",
      "Mayi Xu",
      "Ming Zhong",
      "Yuanyuan Zhu",
      "Tieyun Qian"
    ],
    "abstract": "Hallucination in large language models (LLMs) remains a critical barrier to their safe deployment. Existing tool-augmented hallucination detection methods require pre-defined fixed verification strategies, which are crucial to the quality and effectiveness of tool calls. Some methods directly employ powerful closed-source LLMs such as GPT-4 as detectors, which are effective but too costly. To mitigate the cost issue, some methods adopt the teacher-student architecture and finetune open-source small models as detectors via agent tuning. However, these methods are limited by fixed strategies. When faced with a dynamically changing execution environment, they may lack adaptability and inappropriately call tools, ultimately leading to detection failure. To address the problem of insufficient strategy adaptability, we propose the innovative ``Learning to Evaluate and Adaptively Plan''(LEAP) framework, which endows an efficient student model with the dynamic learning and proactive correction capabilities of the teacher model. Specifically, our method formulates the hallucination detection problem as a dynamic strategy learning problem. We first employ a teacher model to generate trajectories within the dynamic learning loop and dynamically adjust the strategy based on execution failures. We then distill this dynamic planning capability into an efficient student model via agent tuning. Finally, during strategy execution, the student model adopts a proactive correction mechanism, enabling it to propose, review, and optimize its own verification strategies before execution. We demonstrate through experiments on three challenging benchmarks that our LEAP-tuned model outperforms existing state-of-the-art methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05854v1",
    "published_date": "2025-11-08 05:05:38 UTC",
    "updated_date": "2025-11-08 05:05:38 UTC"
  },
  {
    "arxiv_id": "2511.05852v3",
    "title": "Can Fine-Tuning Erase Your Edits? On the Fragile Coexistence of Knowledge Editing and Adaptation",
    "authors": [
      "Yinjie Cheng",
      "Paul Youssef",
      "Christin Seifert",
      "Jörg Schlötterer",
      "Zhixue Zhao"
    ],
    "abstract": "Knowledge editing has emerged as a lightweight alternative to retraining for correcting or injecting specific facts in large language models (LLMs). Meanwhile, fine-tuning remains the default operation for adapting LLMs to new domains and tasks. Despite their widespread adoption, these two post-training interventions have been studied in isolation, leaving open a crucial question: if we fine-tune an edited model, do the edits survive? This question is motivated by two practical scenarios: removing covert or malicious edits, and preserving beneficial edits. If fine-tuning impairs edits (Fig.1), current KE methods become less useful, as every fine-tuned model would require re-editing, which significantly increases the cost; if edits persist, fine-tuned models risk propagating hidden malicious edits, raising serious safety concerns. To this end, we systematically quantify edit decay after fine-tuning, investigating how fine-tuning affects knowledge editing. Our results show that edits decay after fine-tuning, with survival varying across configurations, e.g., AlphaEdit edits decay more than MEMIT edits. Further, we find that fine-tuning edited layers only can effectively remove edits, though at a slight cost to downstream performance. Surprisingly, fine-tuning non-edited layers impairs more edits than full fine-tuning. Overall, our study establishes empirical baselines and actionable strategies for integrating knowledge editing with fine-tuning, and underscores that evaluating model editing requires considering the full LLM application pipeline.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05852v3",
    "published_date": "2025-11-08 04:58:03 UTC",
    "updated_date": "2025-12-07 19:23:41 UTC"
  },
  {
    "arxiv_id": "2512.00030v1",
    "title": "Perturbation-mitigated USV Navigation with Distributionally Robust Reinforcement Learning",
    "authors": [
      "Zhaofan Zhang",
      "Minghao Yang",
      "Sihong Xie",
      "Hui Xiong"
    ],
    "abstract": "The robustness of Unmanned Surface Vehicles (USV) is crucial when facing unknown and complex marine environments, especially when heteroscedastic observational noise poses significant challenges to sensor-based navigation tasks. Recently, Distributional Reinforcement Learning (DistRL) has shown promising results in some challenging autonomous navigation tasks without prior environmental information. However, these methods overlook situations where noise patterns vary across different environmental conditions, hindering safe navigation and disrupting the learning of value functions. To address the problem, we propose DRIQN to integrate Distributionally Robust Optimization (DRO) with implicit quantile networks to optimize worst-case performance under natural environmental conditions. Leveraging explicit subgroup modeling in the replay buffer, DRIQN incorporates heterogeneous noise sources and target robustness-critical scenarios. Experimental results based on the risk-sensitive environment demonstrate that DRIQN significantly outperforms state-of-the-art methods, achieving +13.51\\% success rate, -12.28\\% collision rate and +35.46\\% for time saving, +27.99\\% for energy saving, compared with the runner-up.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.00030v1",
    "published_date": "2025-11-08 04:56:38 UTC",
    "updated_date": "2025-11-08 04:56:38 UTC"
  },
  {
    "arxiv_id": "2511.05850v1",
    "title": "Retrieval Quality at Context Limit",
    "authors": [
      "Max McKinnon"
    ],
    "abstract": "The ability of large language models (LLMs) to recall and retrieve information from long contexts is critical for many real-world applications. Prior work (Liu et al., 2023) reported that LLMs suffer significant drops in retrieval accuracy for facts placed in the middle of large contexts, an effect known as \"Lost in the Middle\" (LITM). We find the model Gemini 2.5 Flash can answer needle-in-a-haystack questions with great accuracy regardless of document position including when the document is nearly at the input context limit. Our results suggest that the \"Lost in the Middle\" effect is not present for simple factoid Q\\&A in Gemini 2.5 Flash, indicating substantial improvements in long-context retrieval.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "3 pages, 0 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.05850v1",
    "published_date": "2025-11-08 04:54:29 UTC",
    "updated_date": "2025-11-08 04:54:29 UTC"
  },
  {
    "arxiv_id": "2511.05849v1",
    "title": "EGG-SR: Embedding Symbolic Equivalence into Symbolic Regression via Equality Graph",
    "authors": [
      "Nan Jiang",
      "Ziyi Wang",
      "Yexiang Xue"
    ],
    "abstract": "Symbolic regression seeks to uncover physical laws from experimental data by searching for closed-form expressions, which is an important task in AI-driven scientific discovery. Yet the exponential growth of the search space of expression renders the task computationally challenging. A promising yet underexplored direction for reducing the effective search space and accelerating training lies in symbolic equivalence: many expressions, although syntactically different, define the same function -- for example, $\\log(x_1^2x_2^3)$, $\\log(x_1^2)+\\log(x_2^3)$, and $2\\log(x_1)+3\\log(x_2)$. Existing algorithms treat such variants as distinct outputs, leading to redundant exploration and slow learning. We introduce EGG-SR, a unified framework that integrates equality graphs (e-graphs) into diverse symbolic regression algorithms, including Monte Carlo Tree Search (MCTS), deep reinforcement learning (DRL), and large language models (LLMs). EGG-SR compactly represents equivalent expressions through the proposed EGG module, enabling more efficient learning by: (1) pruning redundant subtree exploration in EGG-MCTS, (2) aggregating rewards across equivalence classes in EGG-DRL, and (3) enriching feedback prompts in EGG-LLM. Under mild assumptions, we show that embedding e-graphs tightens the regret bound of MCTS and reduces the variance of the DRL gradient estimator. Empirically, EGG-SR consistently enhances multiple baselines across challenging benchmarks, discovering equations with lower normalized mean squared error than state-of-the-art methods. Code implementation is available at: https://www.github.com/jiangnanhugo/egg-sr.",
    "categories": [
      "cs.SC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05849v1",
    "published_date": "2025-11-08 04:39:11 UTC",
    "updated_date": "2025-11-08 04:39:11 UTC"
  },
  {
    "arxiv_id": "2511.05844v3",
    "title": "Enhancing Diffusion Model Guidance through Calibration and Regularization",
    "authors": [
      "Seyed Alireza Javid",
      "Amirhossein Bagheri",
      "Nuria González-Prelcic"
    ],
    "abstract": "Classifier-guided diffusion models have emerged as a powerful approach for conditional image generation, but they suffer from overconfident predictions during early denoising steps, causing the guidance gradient to vanish. This paper introduces two complementary contributions to address this issue. First, we propose a differentiable calibration objective based on the Smooth Expected Calibration Error (Smooth ECE), which improves classifier calibration with minimal fine-tuning and yields measurable improvements in Frechet Inception Distance (FID). Second, we develop enhanced sampling guidance methods that operate on off-the-shelf classifiers without requiring retraining. These include tilted sampling with batch-level reweighting, adaptive entropy-regularized sampling to preserve diversity, and a novel f-divergence-based sampling strategy that strengthens class-consistent guidance while maintaining mode coverage. Experiments on ImageNet 128x128 demonstrate that our divergence-regularized guidance achieves an FID of 2.13 using a ResNet-101 classifier, improving upon existing classifier-guided diffusion methods while requiring no diffusion model retraining. The results show that principled calibration and divergence-aware sampling provide practical and effective improvements for classifier-guided diffusion.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted from NeurIPS 2025 Workshop on Structured Probabilistic Inference & Generative Modeling. Code available at https://github.com/ajavid34/guided-info-diffusion",
    "pdf_url": "https://arxiv.org/pdf/2511.05844v3",
    "published_date": "2025-11-08 04:23:42 UTC",
    "updated_date": "2025-12-21 23:56:36 UTC"
  },
  {
    "arxiv_id": "2511.05841v1",
    "title": "Understanding Cross Task Generalization in Handwriting-Based Alzheimer's Screening via Vision Language Adaptation",
    "authors": [
      "Changqing Gong",
      "Huafeng Qin",
      "Mounim A. El-Yacoubi"
    ],
    "abstract": "Alzheimer's disease is a prevalent neurodegenerative disorder for which early detection is critical. Handwriting-often disrupted in prodromal AD-provides a non-invasive and cost-effective window into subtle motor and cognitive decline. Existing handwriting-based AD studies, mostly relying on online trajectories and hand-crafted features, have not systematically examined how task type influences diagnostic performance and cross-task generalization. Meanwhile, large-scale vision language models have demonstrated remarkable zero or few-shot anomaly detection in natural images and strong adaptability across medical modalities such as chest X-ray and brain MRI. However, handwriting-based disease detection remains largely unexplored within this paradigm. To close this gap, we introduce a lightweight Cross-Layer Fusion Adapter framework that repurposes CLIP for handwriting-based AD screening. CLFA implants multi-level fusion adapters within the visual encoder to progressively align representations toward handwriting-specific medical cues, enabling prompt-free and efficient zero-shot inference. Using this framework, we systematically investigate cross-task generalization-training on a specific handwriting task and evaluating on unseen ones-to reveal which task types and writing patterns most effectively discriminate AD. Extensive analyses further highlight characteristic stroke patterns and task-level factors that contribute to early AD identification, offering both diagnostic insights and a benchmark for handwriting-based cognitive assessment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05841v1",
    "published_date": "2025-11-08 04:13:01 UTC",
    "updated_date": "2025-11-08 04:13:01 UTC"
  },
  {
    "arxiv_id": "2511.05832v1",
    "title": "Hilbert-Guided Block-Sparse Local Attention",
    "authors": [
      "Yunge Li",
      "Lanyu Xu"
    ],
    "abstract": "The quadratic compute and memory costs of global self-attention severely limit its use in high-resolution images. Local attention reduces complexity by restricting attention to neighborhoods. Block-sparse kernels can further improve the efficiency of local attention, but conventional local attention patterns often fail to deliver significant speedups because tokens within a window are not contiguous in the 1D sequence. This work proposes a novel method for constructing windows and neighborhoods based on the Hilbert curve. Image tokens are first reordered along a Hilbert curve, and windows and neighborhoods are then formed on the reordered 1D sequence. From a block-sparse perspective, this strategy significantly increases block sparsity and can be combined with existing block-sparse kernels to improve the efficiency of 2D local attention. Experiments show that the proposed Hilbert Window Attention and Hilbert Slide Attention can accelerate window attention and slide attention by about $4\\times$ and $18\\times$, respectively. To assess practicality, the strategy is instantiated as the Hilbert Window Transformer and the Hilbert Neighborhood Transformer, both of which achieve end-to-end speedups with minimal accuracy loss. Overall, combining Hilbert-guided local attention with block-sparse kernels offers a general and practical approach to enhancing the efficiency of 2D local attention for images. The code is available at https://github.com/Yunge6666/Hilbert-Local-Attention.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05832v1",
    "published_date": "2025-11-08 03:43:13 UTC",
    "updated_date": "2025-11-08 03:43:13 UTC"
  },
  {
    "arxiv_id": "2511.05822v1",
    "title": "Policy Gradient-Based EMT-in-the-Loop Learning to Mitigate Sub-Synchronous Control Interactions",
    "authors": [
      "Sayak Mukherjee",
      "Ramij R. Hossain",
      "Kaustav Chatterjee",
      "Sameer Nekkalapu",
      "Marcelo Elizondo"
    ],
    "abstract": "This paper explores the development of learning-based tunable control gains using EMT-in-the-loop simulation framework (e.g., PSCAD interfaced with Python-based learning modules) to address critical sub-synchronous oscillations. Since sub-synchronous control interactions (SSCI) arise from the mis-tuning of control gains under specific grid configurations, effective mitigation strategies require adaptive re-tuning of these gains. Such adaptiveness can be achieved by employing a closed-loop, learning-based framework that considers the grid conditions responsible for such sub-synchronous oscillations. This paper addresses this need by adopting methodologies inspired by Markov decision process (MDP) based reinforcement learning (RL), with a particular emphasis on simpler deep policy gradient methods with additional SSCI-specific signal processing modules such as down-sampling, bandpass filtering, and oscillation energy dependent reward computations. Our experimentation in a real-world event setting demonstrates that the deep policy gradient based trained policy can adaptively compute gain settings in response to varying grid conditions and optimally suppress control interaction-induced oscillations.",
    "categories": [
      "eess.SY",
      "cs.AI"
    ],
    "primary_category": "eess.SY",
    "comment": "10 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.05822v1",
    "published_date": "2025-11-08 03:12:29 UTC",
    "updated_date": "2025-11-08 03:12:29 UTC"
  },
  {
    "arxiv_id": "2511.05820v1",
    "title": "WAR-Re: Web API Recommendation with Semantic Reasoning",
    "authors": [
      "Zishuo Xu",
      "Dezhong Yao",
      "Yao Wan"
    ],
    "abstract": "With the development of cloud computing, the number of Web APIs has increased dramatically, further intensifying the demand for efficient Web API recommendation. Despite the demonstrated success of previous Web API recommendation solutions, two critical challenges persist: 1) a fixed top-N recommendation that cannot accommodate the varying API cardinality requirements of different mashups, and 2) these methods output only ranked API lists without accompanying reasons, depriving users of understanding the recommendation. To address these challenges, we propose WAR-Re, an LLM-based model for Web API recommendation with semantic reasoning for justification. WAR-Re leverages special start and stop tokens to handle the first challenge and uses two-stage training: supervised fine-tuning and reinforcement learning via Group Relative Policy Optimization (GRPO) to enhance the model's ability in both tasks. Comprehensive experimental evaluations on the ProgrammableWeb dataset demonstrate that WAR-Re achieves a gain of up to 21.59\\% over the state-of-the-art baseline model in recommendation accuracy, while consistently producing high-quality semantic reasons for recommendations.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05820v1",
    "published_date": "2025-11-08 03:09:31 UTC",
    "updated_date": "2025-11-08 03:09:31 UTC"
  },
  {
    "arxiv_id": "2511.05814v1",
    "title": "In-depth Analysis on Caching and Pre-fetching in Mixture of Experts Offloading",
    "authors": [
      "Shuning Lin",
      "Yifan He",
      "Yitong Chen"
    ],
    "abstract": "In today's landscape, Mixture of Experts (MoE) is a crucial architecture that has been used by many of the most advanced models. One of the major challenges of MoE models is that they usually require much more memory than their dense counterparts due to their unique architecture, and hence are harder to deploy in environments with limited GPU memory, such as edge devices. MoE offloading is a promising technique proposed to overcome this challenge, especially if it is enhanced with caching and pre-fetching, but prior work stopped at suboptimal caching algorithm and offered limited insights. In this work, we study MoE offloading in depth and make the following contributions: 1. We analyze the expert activation and LRU caching behavior in detail and provide traces. 2. We propose LFU caching optimization based on our analysis and obtain strong improvements from LRU. 3. We implement and experiment speculative expert pre-fetching, providing detailed trace showing its huge potential . 4. In addition, our study extensively covers the behavior of the MoE architecture itself, offering information on the characteristic of the gating network and experts. This can inspire future work on the interpretation of MoE models and the development of pruning techniques for MoE architecture with minimal performance loss.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05814v1",
    "published_date": "2025-11-08 03:04:11 UTC",
    "updated_date": "2025-11-08 03:04:11 UTC"
  },
  {
    "arxiv_id": "2511.05811v2",
    "title": "MOSS: Efficient and Accurate FP8 LLM Training with Microscaling and Automatic Scaling",
    "authors": [
      "Yu Zhang",
      "Hui-Ling Zhen",
      "Mingxuan Yuan",
      "Bei Yu"
    ],
    "abstract": "Training large language models with FP8 formats offers significant efficiency gains. However, the reduced numerical precision of FP8 poses challenges for stable and accurate training. Current frameworks preserve training performance using mixed-granularity quantization, i.e., applying per-group quantization for activations and per-tensor/block quantization for weights. While effective, per-group quantization requires scaling along the inner dimension of matrix multiplication, introducing additional dequantization overhead. Moreover, these frameworks often rely on just-in-time scaling to dynamically adjust scaling factors based on the current data distribution. However, this online quantization is inefficient for FP8 training, as it involves multiple memory reads and writes that negate the performance benefits of FP8. To overcome these limitations, we propose MOSS, a novel FP8 training framework that ensures both efficiency and numerical stability. MOSS introduces two key innovations: (1) a two-level microscaling strategy for quantizing sensitive activations, which balances precision and dequantization cost by combining a high-precision global scale with compact, power-of-two local scales; and (2) automatic scaling for weights in linear layers, which eliminates the need for costly max-reduction operations by predicting and adjusting scaling factors during training. Leveraging these techniques, MOSS enables efficient FP8 training of a 7B parameter model, achieving performance comparable to the BF16 baseline while achieving up to 34% higher training throughput.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05811v2",
    "published_date": "2025-11-08 02:51:26 UTC",
    "updated_date": "2025-12-05 17:14:58 UTC"
  },
  {
    "arxiv_id": "2511.05810v2",
    "title": "DiagnoLLM: A Hybrid Bayesian Neural Language Framework for Interpretable Disease Diagnosis",
    "authors": [
      "Bowen Xu",
      "Xinyue Zeng",
      "Jiazhen Hu",
      "Tuo Wang",
      "Adithya Kulkarni"
    ],
    "abstract": "Building trustworthy clinical AI systems requires not only accurate predictions but also transparent, biologically grounded explanations. We present \\texttt{DiagnoLLM}, a hybrid framework that integrates Bayesian deconvolution, eQTL-guided deep learning, and LLM-based narrative generation for interpretable disease diagnosis. DiagnoLLM begins with GP-unmix, a Gaussian Process-based hierarchical model that infers cell-type-specific gene expression profiles from bulk and single-cell RNA-seq data while modeling biological uncertainty. These features, combined with regulatory priors from eQTL analysis, power a neural classifier that achieves high predictive performance in Alzheimer's Disease (AD) detection (88.0\\% accuracy). To support human understanding and trust, we introduce an LLM-based reasoning module that translates model outputs into audience-specific diagnostic reports, grounded in clinical features, attribution signals, and domain knowledge. Human evaluations confirm that these reports are accurate, actionable, and appropriately tailored for both physicians and patients. Our findings show that LLMs, when deployed as post-hoc reasoners rather than end-to-end predictors, can serve as effective communicators within hybrid diagnostic pipelines.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05810v2",
    "published_date": "2025-11-08 02:51:21 UTC",
    "updated_date": "2025-11-16 22:54:36 UTC"
  },
  {
    "arxiv_id": "2511.05805v2",
    "title": "Measuring Model Performance in the Presence of an Intervention",
    "authors": [
      "Winston Chen",
      "Michael W. Sjoding",
      "Jenna Wiens"
    ],
    "abstract": "AI models are often evaluated based on their ability to predict the outcome of interest. However, in many AI for social impact applications, the presence of an intervention that affects the outcome can bias the evaluation. Randomized controlled trials (RCTs) randomly assign interventions, allowing data from the control group to be used for unbiased model evaluation. However, this approach is inefficient because it ignores data from the treatment group. Given the complexity and cost often associated with RCTs, making the most use of the data is essential. Thus, we investigate model evaluation strategies that leverage all data from an RCT. First, we theoretically quantify the estimation bias that arises from naïvely aggregating performance estimates from treatment and control groups and derive the condition under which this bias leads to incorrect model selection. Leveraging these theoretical insights, we propose nuisance parameter weighting (NPW), an unbiased model evaluation approach that reweights data from the treatment group to mimic the distributions of samples that would or would not experience the outcome under no intervention. Using synthetic and real-world datasets, we demonstrate that our proposed evaluation approach consistently yields better model selection than the standard approach, which ignores data from the treatment group, across various intervention effect and sample size settings. Our contribution represents a meaningful step towards more efficient model evaluation in real-world contexts.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2511.05805v2",
    "published_date": "2025-11-08 02:24:16 UTC",
    "updated_date": "2025-11-14 20:24:12 UTC"
  },
  {
    "arxiv_id": "2511.05802v1",
    "title": "Beyond the Lower Bound: Bridging Regret Minimization and Best Arm Identification in Lexicographic Bandits",
    "authors": [
      "Bo Xue",
      "Yuanyu Wan",
      "Zhichao Lu",
      "Qingfu Zhang"
    ],
    "abstract": "In multi-objective decision-making with hierarchical preferences, lexicographic bandits provide a natural framework for optimizing multiple objectives in a prioritized order. In this setting, a learner repeatedly selects arms and observes reward vectors, aiming to maximize the reward for the highest-priority objective, then the next, and so on. While previous studies have primarily focused on regret minimization, this work bridges the gap between \\textit{regret minimization} and \\textit{best arm identification} under lexicographic preferences. We propose two elimination-based algorithms to address this joint objective. The first algorithm eliminates suboptimal arms sequentially, layer by layer, in accordance with the objective priorities, and achieves sample complexity and regret bounds comparable to those of the best single-objective algorithms. The second algorithm simultaneously leverages reward information from all objectives in each round, effectively exploiting cross-objective dependencies. Remarkably, it outperforms the known lower bound for the single-objective bandit problem, highlighting the benefit of cross-objective information sharing in the multi-objective setting. Empirical results further validate their superior performance over baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2511.05802v1",
    "published_date": "2025-11-08 02:22:33 UTC",
    "updated_date": "2025-11-08 02:22:33 UTC"
  },
  {
    "arxiv_id": "2511.05797v1",
    "title": "When AI Meets the Web: Prompt Injection Risks in Third-Party AI Chatbot Plugins",
    "authors": [
      "Yigitcan Kaya",
      "Anton Landerer",
      "Stijn Pletinckx",
      "Michelle Zimmermann",
      "Christopher Kruegel",
      "Giovanni Vigna"
    ],
    "abstract": "Prompt injection attacks pose a critical threat to large language models (LLMs), with prior work focusing on cutting-edge LLM applications like personal copilots. In contrast, simpler LLM applications, such as customer service chatbots, are widespread on the web, yet their security posture and exposure to such attacks remain poorly understood. These applications often rely on third-party chatbot plugins that act as intermediaries to commercial LLM APIs, offering non-expert website builders intuitive ways to customize chatbot behaviors. To bridge this gap, we present the first large-scale study of 17 third-party chatbot plugins used by over 10,000 public websites, uncovering previously unknown prompt injection risks in practice. First, 8 of these plugins (used by 8,000 websites) fail to enforce the integrity of the conversation history transmitted in network requests between the website visitor and the chatbot. This oversight amplifies the impact of direct prompt injection attacks by allowing adversaries to forge conversation histories (including fake system messages), boosting their ability to elicit unintended behavior (e.g., code generation) by 3 to 8x. Second, 15 plugins offer tools, such as web-scraping, to enrich the chatbot's context with website-specific content. However, these tools do not distinguish the website's trusted content (e.g., product descriptions) from untrusted, third-party content (e.g., customer reviews), introducing a risk of indirect prompt injection. Notably, we found that ~13% of e-commerce websites have already exposed their chatbots to third-party content. We systematically evaluate both vulnerabilities through controlled experiments grounded in real-world observations, focusing on factors such as system prompt design and the underlying LLM. Our findings show that many plugins adopt insecure practices that undermine the built-in LLM safeguards.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "At IEEE S&P 2026",
    "pdf_url": "https://arxiv.org/pdf/2511.05797v1",
    "published_date": "2025-11-08 02:02:24 UTC",
    "updated_date": "2025-11-08 02:02:24 UTC"
  },
  {
    "arxiv_id": "2511.05791v1",
    "title": "VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models",
    "authors": [
      "Manav Kulshrestha",
      "S. Talha Bukhari",
      "Damon Conover",
      "Aniket Bera"
    ],
    "abstract": "Robotic grasping is a fundamental capability for autonomous manipulation; however, most existing methods rely on large-scale expert annotations and necessitate retraining to handle new objects. We present VLAD-Grasp, a Vision-Language model Assisted zero-shot approach for Detecting grasps. From a single RGB-D image, our method (1) prompts a large vision-language model to generate a goal image where a straight rod \"impales\" the object, representing an antipodal grasp, (2) predicts depth and segmentation to lift this generated image into 3D, and (3) aligns generated and observed object point clouds via principal component analysis and correspondence-free optimization to recover an executable grasp pose. Unlike prior work, our approach is training-free and does not rely on curated grasp datasets. Despite this, VLAD-Grasp achieves performance that is competitive with or superior to that of state-of-the-art supervised models on the Cornell and Jacquard datasets. We further demonstrate zero-shot generalization to novel real-world objects on a Franka Research 3 robot, highlighting vision-language foundation models as powerful priors for robotic manipulation.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 4 figures, under review",
    "pdf_url": "https://arxiv.org/pdf/2511.05791v1",
    "published_date": "2025-11-08 01:47:40 UTC",
    "updated_date": "2025-11-08 01:47:40 UTC"
  },
  {
    "arxiv_id": "2511.05790v1",
    "title": "SymLight: Exploring Interpretable and Deployable Symbolic Policies for Traffic Signal Control",
    "authors": [
      "Xiao-Cheng Liao",
      "Yi Mei",
      "Mengjie Zhang"
    ],
    "abstract": "Deep Reinforcement Learning have achieved significant success in automatically devising effective traffic signal control (TSC) policies. Neural policies, however, tend to be over-parameterized and non-transparent, hindering their interpretability and deployability on resource-limited edge devices. This work presents SymLight, a priority function search framework based on Monte Carlo Tree Search (MCTS) for discovering inherently interpretable and deployable symbolic priority functions to serve as the TSC policies. The priority function, in particular, accepts traffic features as input and then outputs a priority for each traffic signal phase, which subsequently directs the phase transition. For effective search, we propose a concise yet expressive priority function representation. This helps mitigate the combinatorial explosion of the action space in MCTS. Additionally, a probabilistic structural rollout strategy is introduced to leverage structural patterns from previously discovered high-quality priority functions, guiding the rollout process. Our experiments on real-world datasets demonstrate SymLight's superior performance across a range of baselines. A key advantage is SymLight's ability to produce interpretable and deployable TSC policies while maintaining excellent performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05790v1",
    "published_date": "2025-11-08 01:45:49 UTC",
    "updated_date": "2025-11-08 01:45:49 UTC"
  },
  {
    "arxiv_id": "2511.05784v2",
    "title": "DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning",
    "authors": [
      "Yaxuan Wang",
      "Chris Yuhao Liu",
      "Quan Liu",
      "Jinglong Pang",
      "Wei Wei",
      "Yujia Bao",
      "Yang Liu"
    ],
    "abstract": "Unlearning in Large Language Models (LLMs) is crucial for protecting private data and removing harmful knowledge. Most existing approaches rely on fine-tuning to balance unlearning efficiency with general language capabilities. However, these methods typically require training or access to retain data, which is often unavailable in real world scenarios. Although these methods can perform well when both forget and retain data are available, few works have demonstrated equivalent capability in more practical, data-limited scenarios. To overcome these limitations, we propose Detect-Reasoning Augmented GeneratiON (DRAGON), a systematic, reasoning-based framework that utilizes in-context chain-of-thought (CoT) instructions to guard deployed LLMs before inference. Instead of modifying the base model, DRAGON leverages the inherent instruction-following ability of LLMs and introduces a lightweight detection module to identify forget-worthy prompts without any retain data. These are then routed through a dedicated CoT guard model to enforce safe and accurate in-context intervention. To robustly evaluate unlearning performance, we introduce novel metrics for unlearning performance and the continual unlearning setting. Extensive experiments across three representative unlearning tasks validate the effectiveness of DRAGON, demonstrating its strong unlearning capability, scalability, and applicability in practical scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Please refer to the NeurIPS 2025 submission: https://openreview.net/forum?id=FNuul0hlin The paper has been accepted to the ICML 2025 MUGen Workshop: https://openreview.net/forum?id=ET24oKP23c",
    "pdf_url": "https://arxiv.org/pdf/2511.05784v2",
    "published_date": "2025-11-08 01:13:28 UTC",
    "updated_date": "2025-11-11 05:42:34 UTC"
  },
  {
    "arxiv_id": "2511.05772v1",
    "title": "Sign language recognition from skeletal data using graph and recurrent neural networks",
    "authors": [
      "B. Mederos",
      "J. Mejía",
      "A. Medina-Reyes",
      "Y. Espinosa-Almeyda",
      "J. D. Díaz-Roman",
      "I. Rodríguez-Mederos",
      "M. Mejía-Carreon",
      "F. Gonzalez-Lopez"
    ],
    "abstract": "This work presents an approach for recognizing isolated sign language gestures using skeleton-based pose data extracted from video sequences. A Graph-GRU temporal network is proposed to model both spatial and temporal dependencies between frames, enabling accurate classification. The model is trained and evaluated on the AUTSL (Ankara university Turkish sign language) dataset, achieving high accuracy. Experimental results demonstrate the effectiveness of integrating graph-based spatial representations with temporal modeling, providing a scalable framework for sign language recognition. The results of this approach highlight the potential of pose-driven methods for sign language understanding.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.05772v1",
    "published_date": "2025-11-08 00:04:42 UTC",
    "updated_date": "2025-11-08 00:04:42 UTC"
  }
]
[
  {
    "arxiv_id": "2402.15650v3",
    "title": "Uniformly Safe RL with Objective Suppression for Multi-Constraint Safety-Critical Applications",
    "authors": [
      "Zihan Zhou",
      "Jonathan Booher",
      "Khashayar Rohanimanesh",
      "Wei Liu",
      "Aleksandr Petiushko",
      "Animesh Garg"
    ],
    "abstract": "Safe reinforcement learning tasks are a challenging domain despite being very\ncommon in the real world. The widely adopted CMDP model constrains the risks in\nexpectation, which makes room for dangerous behaviors in long-tail states. In\nsafety-critical domains, such behaviors could lead to disastrous outcomes. To\naddress this issue, we first describe the problem with a stronger Uniformly\nConstrained MDP (UCMDP) model where we impose constraints on all reachable\nstates; we then propose Objective Suppression, a novel method that adaptively\nsuppresses the task reward maximizing objectives according to a safety critic,\nas a solution to the Lagrangian dual of a UCMDP. We benchmark Objective\nSuppression in two multi-constraint safety domains, including an autonomous\ndriving domain where any incorrect behavior can lead to disastrous\nconsequences. On the driving domain, we evaluate on open source and proprietary\ndata and evaluate transfer to a real autonomous fleet. Empirically, we\ndemonstrate that our proposed method, when combined with existing safe RL\nalgorithms, can match the task reward achieved by baselines with significantly\nfewer constraint violations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15650v3",
    "published_date": "2024-02-23 23:22:06 UTC",
    "updated_date": "2024-08-28 20:13:43 UTC"
  },
  {
    "arxiv_id": "2402.15631v1",
    "title": "Fine-Grained Self-Endorsement Improves Factuality and Reasoning",
    "authors": [
      "Ante Wang",
      "Linfeng Song",
      "Baolin Peng",
      "Ye Tian",
      "Lifeng Jin",
      "Haitao Mi",
      "Jinsong Su",
      "Dong Yu"
    ],
    "abstract": "This work studies improving large language model (LLM) generations at\ninference time by mitigating fact-conflicting hallucinations. Particularly, we\npropose a self-endorsement framework that leverages the fine-grained fact-level\ncomparisons across multiple sampled responses. Compared with prior ensemble\nmethods (Wang et al., 2022;Chen et al., 2023)) that perform response-level\nselection, our approach can better alleviate hallucinations, especially for\nlongform generation tasks. Our approach can broadly benefit smaller and\nopen-source LLMs as it mainly conducts simple content-based comparisons.\nExperiments on Biographies show that our method can effectively improve the\nfactuality of generations with simple and intuitive prompts across different\nscales of LLMs. Besides, comprehensive analyses on TriviaQA and GSM8K\ndemonstrate the potential of self-endorsement for broader application.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15631v1",
    "published_date": "2024-02-23 22:24:40 UTC",
    "updated_date": "2024-02-23 22:24:40 UTC"
  },
  {
    "arxiv_id": "2403.00803v1",
    "title": "LiMAML: Personalization of Deep Recommender Models via Meta Learning",
    "authors": [
      "Ruofan Wang",
      "Prakruthi Prabhakar",
      "Gaurav Srivastava",
      "Tianqi Wang",
      "Zeinab S. Jalali",
      "Varun Bharill",
      "Yunbo Ouyang",
      "Aastha Nigam",
      "Divya Venugopalan",
      "Aman Gupta",
      "Fedor Borisyuk",
      "Sathiya Keerthi",
      "Ajith Muralidharan"
    ],
    "abstract": "In the realm of recommender systems, the ubiquitous adoption of deep neural\nnetworks has emerged as a dominant paradigm for modeling diverse business\nobjectives. As user bases continue to expand, the necessity of personalization\nand frequent model updates have assumed paramount significance to ensure the\ndelivery of relevant and refreshed experiences to a diverse array of members.\nIn this work, we introduce an innovative meta-learning solution tailored to the\npersonalization of models for individual members and other entities, coupled\nwith the frequent updates based on the latest user interaction signals.\nSpecifically, we leverage the Model-Agnostic Meta Learning (MAML) algorithm to\nadapt per-task sub-networks using recent user interaction data. Given the near\ninfeasibility of productionizing original MAML-based models in online\nrecommendation systems, we propose an efficient strategy to operationalize\nmeta-learned sub-networks in production, which involves transforming them into\nfixed-sized vectors, termed meta embeddings, thereby enabling the seamless\ndeployment of models with hundreds of billions of parameters for online\nserving. Through extensive experimentation on production data drawn from\nvarious applications at LinkedIn, we demonstrate that the proposed solution\nconsistently outperforms the baseline models of those applications, including\nstrong baselines such as using wide-and-deep ID based personalization approach.\nOur approach has enabled the deployment of a range of highly personalized AI\nmodels across diverse LinkedIn applications, leading to substantial\nimprovements in business metrics as well as refreshed experience for our\nmembers.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.00803v1",
    "published_date": "2024-02-23 22:06:36 UTC",
    "updated_date": "2024-02-23 22:06:36 UTC"
  },
  {
    "arxiv_id": "2402.15625v1",
    "title": "Learning Cyclic Causal Models from Incomplete Data",
    "authors": [
      "Muralikrishnna G. Sethuraman",
      "Faramarz Fekri"
    ],
    "abstract": "Causal learning is a fundamental problem in statistics and science, offering\ninsights into predicting the effects of unseen treatments on a system. Despite\nrecent advances in this topic, most existing causal discovery algorithms\noperate under two key assumptions: (i) the underlying graph is acyclic, and\n(ii) the available data is complete. These assumptions can be problematic as\nmany real-world systems contain feedback loops (e.g., biological systems), and\npractical scenarios frequently involve missing data. In this work, we propose a\nnovel framework, named MissNODAGS, for learning cyclic causal graphs from\npartially missing data. Under the additive noise model, MissNODAGS learns the\ncausal graph by alternating between imputing the missing data and maximizing\nthe expected log-likelihood of the visible part of the data in each training\nstep, following the principles of the expectation-maximization (EM) framework.\nThrough synthetic experiments and real-world single-cell perturbation data, we\ndemonstrate improved performance when compared to using state-of-the-art\nimputation techniques followed by causal learning on partially missing\ninterventional data.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15625v1",
    "published_date": "2024-02-23 22:03:12 UTC",
    "updated_date": "2024-02-23 22:03:12 UTC"
  },
  {
    "arxiv_id": "2403.00802v1",
    "title": "Towards a Theoretical Understanding of Two-Stage Recommender Systems",
    "authors": [
      "Amit Kumar Jaiswal"
    ],
    "abstract": "Production-grade recommender systems rely heavily on a large-scale corpus\nused by online media services, including Netflix, Pinterest, and Amazon. These\nsystems enrich recommendations by learning users' and items' embeddings\nprojected in a low-dimensional space with two-stage models (two deep neural\nnetworks), which facilitate their embedding constructs to predict users'\nfeedback associated with items. Despite its popularity for recommendations, its\ntheoretical behaviors remain comprehensively unexplored. We study the\nasymptotic behaviors of the two-stage recommender that entail a strong\nconvergence to the optimal recommender system. We establish certain theoretical\nproperties and statistical assurance of the two-stage recommender. In addition\nto asymptotic behaviors, we demonstrate that the two-stage recommender system\nattains faster convergence by relying on the intrinsic dimensions of the input\nfeatures. Finally, we show numerically that the two-stage recommender enables\nencapsulating the impacts of items' and users' attributes on ratings, resulting\nin better performance compared to existing methods conducted using synthetic\nand real-world data experiments.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "18 pages (including references and appendix), 1 figure, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.00802v1",
    "published_date": "2024-02-23 21:11:55 UTC",
    "updated_date": "2024-02-23 21:11:55 UTC"
  },
  {
    "arxiv_id": "2402.15591v1",
    "title": "RecWizard: A Toolkit for Conversational Recommendation with Modular, Portable Models and Interactive User Interface",
    "authors": [
      "Zeyuan Zhang",
      "Tanmay Laud",
      "Zihang He",
      "Xiaojie Chen",
      "Xinshuang Liu",
      "Zhouhang Xie",
      "Julian McAuley",
      "Zhankui He"
    ],
    "abstract": "We present a new Python toolkit called RecWizard for Conversational\nRecommender Systems (CRS). RecWizard offers support for development of models\nand interactive user interface, drawing from the best practices of the\nHuggingface ecosystems. CRS with RecWizard are modular, portable, interactive\nand Large Language Models (LLMs)-friendly, to streamline the learning process\nand reduce the additional effort for CRS research. For more comprehensive\ninformation about RecWizard, please check our GitHub\nhttps://github.com/McAuley-Lab/RecWizard.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "AAAI'24 Demo Track",
    "pdf_url": "http://arxiv.org/pdf/2402.15591v1",
    "published_date": "2024-02-23 20:16:13 UTC",
    "updated_date": "2024-02-23 20:16:13 UTC"
  },
  {
    "arxiv_id": "2402.15589v2",
    "title": "LLMs as Meta-Reviewers' Assistants: A Case Study",
    "authors": [
      "Eftekhar Hossain",
      "Sanjeev Kumar Sinha",
      "Naman Bansal",
      "Alex Knipper",
      "Souvika Sarkar",
      "John Salvador",
      "Yash Mahajan",
      "Sri Guttikonda",
      "Mousumi Akter",
      "Md. Mahadi Hassan",
      "Matthew Freestone",
      "Matthew C. Williams Jr.",
      "Dongji Feng",
      "Santu Karmaker"
    ],
    "abstract": "One of the most important yet onerous tasks in the academic peer-reviewing\nprocess is composing meta-reviews, which involves assimilating diverse opinions\nfrom multiple expert peers, formulating one's self-judgment as a senior expert,\nand then summarizing all these perspectives into a concise holistic overview to\nmake an overall recommendation. This process is time-consuming and can be\ncompromised by human factors like fatigue, inconsistency, missing tiny details,\netc. Given the latest major developments in Large Language Models (LLMs), it is\nvery compelling to rigorously study whether LLMs can help metareviewers perform\nthis important task better. In this paper, we perform a case study with three\npopular LLMs, i.e., GPT-3.5, LLaMA2, and PaLM2, to assist meta-reviewers in\nbetter comprehending multiple experts perspectives by generating a controlled\nmulti-perspective summary (MPS) of their opinions. To achieve this, we prompt\nthree LLMs with different types/levels of prompts based on the recently\nproposed TELeR taxonomy. Finally, we perform a detailed qualitative study of\nthe MPSs generated by the LLMs and report our findings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2025, 41 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.15589v2",
    "published_date": "2024-02-23 20:14:16 UTC",
    "updated_date": "2025-02-08 21:23:21 UTC"
  },
  {
    "arxiv_id": "2402.15572v1",
    "title": "Improving Explainable Object-induced Model through Uncertainty for Automated Vehicles",
    "authors": [
      "Shihong Ling",
      "Yue Wan",
      "Xiaowei Jia",
      "Na Du"
    ],
    "abstract": "The rapid evolution of automated vehicles (AVs) has the potential to provide\nsafer, more efficient, and comfortable travel options. However, these systems\nface challenges regarding reliability in complex driving scenarios. Recent\nexplainable AV architectures neglect crucial information related to inherent\nuncertainties while providing explanations for actions. To overcome such\nchallenges, our study builds upon the \"object-induced\" model approach that\nprioritizes the role of objects in scenes for decision-making and integrates\nuncertainty assessment into the decision-making process using an evidential\ndeep learning paradigm with a Beta prior. Additionally, we explore several\nadvanced training strategies guided by uncertainty, including\nuncertainty-guided data reweighting and augmentation. Leveraging the BDD-OIA\ndataset, our findings underscore that the model, through these enhancements,\nnot only offers a clearer comprehension of AV decisions and their underlying\nreasoning but also surpasses existing baselines across a broad range of\nscenarios.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "In Proceedings of the 2024 ACM / IEEE International Conference on\n  Human-Robot Interaction (HRI '24), March 11--14, 2024, Boulder, CO, USA. ACM,\n  New York, NY, USA, 9 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.15572v1",
    "published_date": "2024-02-23 19:14:57 UTC",
    "updated_date": "2024-02-23 19:14:57 UTC"
  },
  {
    "arxiv_id": "2402.15570v1",
    "title": "Fast Adversarial Attacks on Language Models In One GPU Minute",
    "authors": [
      "Vinu Sankar Sadasivan",
      "Shoumik Saha",
      "Gaurang Sriramanan",
      "Priyatham Kattakinda",
      "Atoosa Chegini",
      "Soheil Feizi"
    ],
    "abstract": "In this paper, we introduce a novel class of fast, beam search-based\nadversarial attack (BEAST) for Language Models (LMs). BEAST employs\ninterpretable parameters, enabling attackers to balance between attack speed,\nsuccess rate, and the readability of adversarial prompts. The computational\nefficiency of BEAST facilitates us to investigate its applications on LMs for\njailbreaking, eliciting hallucinations, and privacy attacks. Our gradient-free\ntargeted attack can jailbreak aligned LMs with high attack success rates within\none minute. For instance, BEAST can jailbreak Vicuna-7B-v1.5 under one minute\nwith a success rate of 89% when compared to a gradient-based baseline that\ntakes over an hour to achieve 70% success rate using a single Nvidia RTX A6000\n48GB GPU. Additionally, we discover a unique outcome wherein our untargeted\nattack induces hallucinations in LM chatbots. Through human evaluations, we\nfind that our untargeted attack causes Vicuna-7B-v1.5 to produce ~15% more\nincorrect outputs when compared to LM outputs in the absence of our attack. We\nalso learn that 22% of the time, BEAST causes Vicuna to generate outputs that\nare not relevant to the original prompt. Further, we use BEAST to generate\nadversarial prompts in a few seconds that can boost the performance of existing\nmembership inference attacks for LMs. We believe that our fast attack, BEAST,\nhas the potential to accelerate research in LM security and privacy. Our\ncodebase is publicly available at https://github.com/vinusankars/BEAST.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15570v1",
    "published_date": "2024-02-23 19:12:53 UTC",
    "updated_date": "2024-02-23 19:12:53 UTC"
  },
  {
    "arxiv_id": "2402.15567v2",
    "title": "Foundation Policies with Hilbert Representations",
    "authors": [
      "Seohong Park",
      "Tobias Kreiman",
      "Sergey Levine"
    ],
    "abstract": "Unsupervised and self-supervised objectives, such as next token prediction,\nhave enabled pre-training generalist models from large amounts of unlabeled\ndata. In reinforcement learning (RL), however, finding a truly general and\nscalable unsupervised pre-training objective for generalist policies from\noffline data remains a major open question. While a number of methods have been\nproposed to enable generic self-supervised RL, based on principles such as\ngoal-conditioned RL, behavioral cloning, and unsupervised skill learning, such\nmethods remain limited in terms of either the diversity of the discovered\nbehaviors, the need for high-quality demonstration data, or the lack of a clear\nadaptation mechanism for downstream tasks. In this work, we propose a novel\nunsupervised framework to pre-train generalist policies that capture diverse,\noptimal, long-horizon behaviors from unlabeled offline data such that they can\nbe quickly adapted to any arbitrary new tasks in a zero-shot manner. Our key\ninsight is to learn a structured representation that preserves the temporal\nstructure of the underlying environment, and then to span this learned latent\nspace with directional movements, which enables various zero-shot policy\n\"prompting\" schemes for downstream tasks. Through our experiments on simulated\nrobotic locomotion and manipulation benchmarks, we show that our unsupervised\npolicies can solve goal-conditioned and general RL tasks in a zero-shot\nfashion, even often outperforming prior methods designed specifically for each\nsetting. Our code and videos are available at\nhttps://seohong.me/projects/hilp/.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.15567v2",
    "published_date": "2024-02-23 19:09:10 UTC",
    "updated_date": "2024-05-26 17:44:52 UTC"
  },
  {
    "arxiv_id": "2402.15555v2",
    "title": "Deep Networks Always Grok and Here is Why",
    "authors": [
      "Ahmed Imtiaz Humayun",
      "Randall Balestriero",
      "Richard Baraniuk"
    ],
    "abstract": "Grokking, or delayed generalization, is a phenomenon where generalization in\na deep neural network (DNN) occurs long after achieving near zero training\nerror. Previous studies have reported the occurrence of grokking in specific\ncontrolled settings, such as DNNs initialized with large-norm parameters or\ntransformers trained on algorithmic datasets. We demonstrate that grokking is\nactually much more widespread and materializes in a wide range of practical\nsettings, such as training of a convolutional neural network (CNN) on CIFAR10\nor a Resnet on Imagenette. We introduce the new concept of delayed robustness,\nwhereby a DNN groks adversarial examples and becomes robust, long after\ninterpolation and/or generalization. We develop an analytical explanation for\nthe emergence of both delayed generalization and delayed robustness based on\nthe local complexity of a DNN's input-output mapping. Our local complexity\nmeasures the density of so-called linear regions (aka, spline partition\nregions) that tile the DNN input space and serves as a utile progress measure\nfor training. We provide the first evidence that, for classification problems,\nthe linear regions undergo a phase transition during training whereafter they\nmigrate away from the training samples (making the DNN mapping smoother there)\nand towards the decision boundary (making the DNN mapping less smooth there).\nGrokking occurs post phase transition as a robust partition of the input space\nthanks to the linearization of the DNN mapping around the training points.\nWebsite: https://bit.ly/grok-adversarial",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024. Website: https://bit.ly/grok-adversarial. Pages 24,\n  Figures 36",
    "pdf_url": "http://arxiv.org/pdf/2402.15555v2",
    "published_date": "2024-02-23 18:59:31 UTC",
    "updated_date": "2024-06-06 18:33:15 UTC"
  },
  {
    "arxiv_id": "2402.15506v4",
    "title": "AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning",
    "authors": [
      "Jianguo Zhang",
      "Tian Lan",
      "Rithesh Murthy",
      "Zhiwei Liu",
      "Weiran Yao",
      "Ming Zhu",
      "Juntao Tan",
      "Thai Hoang",
      "Zuxin Liu",
      "Liangwei Yang",
      "Yihao Feng",
      "Shirley Kokane",
      "Tulika Awalgaonkar",
      "Juan Carlos Niebles",
      "Silvio Savarese",
      "Shelby Heinecke",
      "Huan Wang",
      "Caiming Xiong"
    ],
    "abstract": "Autonomous agents powered by large language models (LLMs) have garnered\nsignificant research attention. However, fully harnessing the potential of LLMs\nfor agent-based tasks presents inherent challenges due to the heterogeneous\nnature of diverse data sources featuring multi-turn trajectories. In this\npaper, we introduce \\textbf{AgentOhana} as a comprehensive solution to address\nthese challenges. \\textit{AgentOhana} aggregates agent trajectories from\ndistinct environments, spanning a wide array of scenarios. It meticulously\nstandardizes and unifies these trajectories into a consistent format,\nstreamlining the creation of a generic data loader optimized for agent\ntraining. Leveraging the data unification, our training pipeline maintains\nequilibrium across different data sources and preserves independent randomness\nacross devices during dataset partitioning and model training. Additionally, we\npresent \\textbf{xLAM-v0.1}, a large action model tailored for AI agents, which\ndemonstrates exceptional performance across various benchmarks. Begin the\nexploration at \\url{https://github.com/SalesforceAIResearch/xLAM}.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Add GitHub repo link at\n  \\url{https://github.com/SalesforceAIResearch/xLAM} and HuggingFace model link\n  at \\url{https://huggingface.co/Salesforce/xLAM-v0.1-r}",
    "pdf_url": "http://arxiv.org/pdf/2402.15506v4",
    "published_date": "2024-02-23 18:56:26 UTC",
    "updated_date": "2024-11-09 00:28:26 UTC"
  },
  {
    "arxiv_id": "2402.15505v1",
    "title": "Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts",
    "authors": [
      "Yuejiang Liu",
      "Alexandre Alahi"
    ],
    "abstract": "Steering the behavior of a strong model pre-trained on internet-scale data\ncan be difficult due to the scarcity of competent supervisors. Recent studies\nreveal that, despite supervisory noises, a strong student model may surpass its\nweak teacher when fine-tuned on specific objectives. Yet, the effectiveness of\nsuch weak-to-strong generalization remains limited, especially in the presence\nof large capability gaps. In this paper, we propose to address this challenge\nby harnessing a diverse set of specialized teachers, instead of a single\ngeneralist one, that collectively supervises the strong student. Our approach\nresembles the classical hierarchical mixture of experts, with two components\ntailored for co-supervision: (i) we progressively alternate student training\nand teacher assignment, leveraging the growth of the strong student to identify\nplausible supervisions; (ii) we conservatively enforce teacher-student and\nlocal-global consistency, leveraging their dependencies to reject potential\nannotation noises. We validate the proposed method through visual recognition\ntasks on the OpenAI weak-to-strong benchmark and additional multi-domain\ndatasets. Our code is available at \\url{https://github.com/yuejiangliu/csl}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2402.15505v1",
    "published_date": "2024-02-23 18:56:11 UTC",
    "updated_date": "2024-02-23 18:56:11 UTC"
  },
  {
    "arxiv_id": "2402.15504v1",
    "title": "Gen4Gen: Generative Data Pipeline for Generative Multi-Concept Composition",
    "authors": [
      "Chun-Hsiao Yeh",
      "Ta-Ying Cheng",
      "He-Yen Hsieh",
      "Chuan-En Lin",
      "Yi Ma",
      "Andrew Markham",
      "Niki Trigoni",
      "H. T. Kung",
      "Yubei Chen"
    ],
    "abstract": "Recent text-to-image diffusion models are able to learn and synthesize images\ncontaining novel, personalized concepts (e.g., their own pets or specific\nitems) with just a few examples for training. This paper tackles two\ninterconnected issues within this realm of personalizing text-to-image\ndiffusion models. First, current personalization techniques fail to reliably\nextend to multiple concepts -- we hypothesize this to be due to the mismatch\nbetween complex scenes and simple text descriptions in the pre-training dataset\n(e.g., LAION). Second, given an image containing multiple personalized\nconcepts, there lacks a holistic metric that evaluates performance on not just\nthe degree of resemblance of personalized concepts, but also whether all\nconcepts are present in the image and whether the image accurately reflects the\noverall text description. To address these issues, we introduce Gen4Gen, a\nsemi-automated dataset creation pipeline utilizing generative models to combine\npersonalized concepts into complex compositions along with text-descriptions.\nUsing this, we create a dataset called MyCanvas, that can be used to benchmark\nthe task of multi-concept personalization. In addition, we design a\ncomprehensive metric comprising two scores (CP-CLIP and TI-CLIP) for better\nquantifying the performance of multi-concept, personalized text-to-image\ndiffusion methods. We provide a simple baseline built on top of Custom\nDiffusion with empirical prompting strategies for future researchers to\nevaluate on MyCanvas. We show that by improving data quality and prompting\nstrategies, we can significantly increase multi-concept personalized image\ngeneration quality, without requiring any modifications to model architecture\nor training algorithms.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint; Project Page: https://danielchyeh.github.io/Gen4Gen/",
    "pdf_url": "http://arxiv.org/pdf/2402.15504v1",
    "published_date": "2024-02-23 18:55:09 UTC",
    "updated_date": "2024-02-23 18:55:09 UTC"
  },
  {
    "arxiv_id": "2403.00801v2",
    "title": "Self-Retrieval: End-to-End Information Retrieval with One Large Language Model",
    "authors": [
      "Qiaoyu Tang",
      "Jiawei Chen",
      "Zhuoqun Li",
      "Bowen Yu",
      "Yaojie Lu",
      "Cheng Fu",
      "Haiyang Yu",
      "Hongyu Lin",
      "Fei Huang",
      "Ben He",
      "Xianpei Han",
      "Le Sun",
      "Yongbin Li"
    ],
    "abstract": "The rise of large language models (LLMs) has significantly transformed both\nthe construction and application of information retrieval (IR) systems.\nHowever, current interactions between IR systems and LLMs remain limited, with\nLLMs merely serving as part of components within IR systems, and IR systems\nbeing constructed independently of LLMs. This separated architecture restricts\nknowledge sharing and deep collaboration between them. In this paper, we\nintroduce Self-Retrieval, a novel end-to-end LLM-driven information retrieval\narchitecture. Self-Retrieval unifies all essential IR functions within a single\nLLM, leveraging the inherent capabilities of LLMs throughout the IR process.\nSpecifically, Self-Retrieval internalizes the retrieval corpus through\nself-supervised learning, transforms the retrieval process into sequential\npassage generation, and performs relevance assessment for reranking.\nExperimental results demonstrate that Self-Retrieval not only outperforms\nexisting retrieval approaches by a significant margin, but also substantially\nenhances the performance of LLM-driven downstream applications like\nretrieval-augmented generation.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "NeurIPS 2024 Camera-ready Version. Code:\n  https://github.com/icip-cas/SelfRetrieval",
    "pdf_url": "http://arxiv.org/pdf/2403.00801v2",
    "published_date": "2024-02-23 18:45:35 UTC",
    "updated_date": "2024-11-04 03:07:30 UTC"
  },
  {
    "arxiv_id": "2402.16893v1",
    "title": "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)",
    "authors": [
      "Shenglai Zeng",
      "Jiankun Zhang",
      "Pengfei He",
      "Yue Xing",
      "Yiding Liu",
      "Han Xu",
      "Jie Ren",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Yi Chang",
      "Jiliang Tang"
    ],
    "abstract": "Retrieval-augmented generation (RAG) is a powerful technique to facilitate\nlanguage model with proprietary and private data, where data privacy is a\npivotal concern. Whereas extensive research has demonstrated the privacy risks\nof large language models (LLMs), the RAG technique could potentially reshape\nthe inherent behaviors of LLM generation, posing new privacy issues that are\ncurrently under-explored. In this work, we conduct extensive empirical studies\nwith novel attack methods, which demonstrate the vulnerability of RAG systems\non leaking the private retrieval database. Despite the new risk brought by RAG\non the retrieval data, we further reveal that RAG can mitigate the leakage of\nthe LLMs' training data. Overall, we provide new insights in this paper for\nprivacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAG\nsystems builders. Our code is available at\nhttps://github.com/phycholosogy/RAG-privacy.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16893v1",
    "published_date": "2024-02-23 18:35:15 UTC",
    "updated_date": "2024-02-23 18:35:15 UTC"
  },
  {
    "arxiv_id": "2402.15491v2",
    "title": "API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs",
    "authors": [
      "Kinjal Basu",
      "Ibrahim Abdelaziz",
      "Subhajit Chaudhury",
      "Soham Dan",
      "Maxwell Crouse",
      "Asim Munawar",
      "Sadhana Kumaravel",
      "Vinod Muthusamy",
      "Pavan Kapanipathi",
      "Luis A. Lastras"
    ],
    "abstract": "There is a growing need for Large Language Models (LLMs) to effectively use\ntools and external Application Programming Interfaces (APIs) to plan and\ncomplete tasks. As such, there is tremendous interest in methods that can\nacquire sufficient quantities of train and test data that involve calls to\ntools / APIs. Two lines of research have emerged as the predominant strategies\nfor addressing this challenge. The first has focused on synthetic data\ngeneration techniques, while the second has involved curating task-adjacent\ndatasets which can be transformed into API / Tool-based tasks. In this paper,\nwe focus on the task of identifying, curating, and transforming existing\ndatasets and, in turn, introduce API-BLEND, a large corpora for training and\nsystematic testing of tool-augmented LLMs. The datasets mimic real-world\nscenarios involving API-tasks such as API / tool detection, slot filling, and\nsequencing of the detected APIs. We demonstrate the utility of the API-BLEND\ndataset for both training and benchmarking purposes.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ACL'24-main conference",
    "pdf_url": "http://arxiv.org/pdf/2402.15491v2",
    "published_date": "2024-02-23 18:30:49 UTC",
    "updated_date": "2024-05-20 14:52:31 UTC"
  },
  {
    "arxiv_id": "2402.15487v2",
    "title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation",
    "authors": [
      "Hanxiao Jiang",
      "Binghao Huang",
      "Ruihai Wu",
      "Zhuoran Li",
      "Shubham Garg",
      "Hooshang Nayyeri",
      "Shenlong Wang",
      "Yunzhu Li"
    ],
    "abstract": "We introduce the novel task of interactive scene exploration, wherein robots\nautonomously explore environments and produce an action-conditioned scene graph\n(ACSG) that captures the structure of the underlying environment. The ACSG\naccounts for both low-level information (geometry and semantics) and high-level\ninformation (action-conditioned relationships between different entities) in\nthe scene. To this end, we present the Robotic Exploration (RoboEXP) system,\nwhich incorporates the Large Multimodal Model (LMM) and an explicit memory\ndesign to enhance our system's capabilities. The robot reasons about what and\nhow to explore an object, accumulating new information through the interaction\nprocess and incrementally constructing the ACSG. Leveraging the constructed\nACSG, we illustrate the effectiveness and efficiency of our RoboEXP system in\nfacilitating a wide range of real-world manipulation tasks involving rigid,\narticulated objects, nested objects, and deformable objects.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Project Page: https://jianghanxiao.github.io/roboexp-web/",
    "pdf_url": "http://arxiv.org/pdf/2402.15487v2",
    "published_date": "2024-02-23 18:27:17 UTC",
    "updated_date": "2024-10-08 05:32:25 UTC"
  },
  {
    "arxiv_id": "2403.00800v1",
    "title": "Brain-Inspired Two-Stage Approach: Enhancing Mathematical Reasoning by Imitating Human Thought Processes",
    "authors": [
      "Yezeng Chen",
      "Zui Chen",
      "Yi Zhou"
    ],
    "abstract": "Although large language models demonstrate emergent abilities in solving math\nword problems, there is a challenging task in complex multi-step mathematical\nreasoning tasks. To improve model performance on mathematical reasoning tasks,\nprevious work has conducted supervised fine-tuning on open-source models by\nimproving the quality and quantity of data. In this paper, we propose a novel\napproach, named Brain, to imitate human thought processes to enhance\nmathematical reasoning abilities, using the Frontal Lobe Model to generate\nplans, and then employing the Parietal Lobe Model to generate code and execute\nto obtain answers. First, we achieve SOTA performance in comparison with Code\nLLaMA 7B based models through this method. Secondly, we find that plans can be\nexplicitly extracted from natural language, code, or formal language. Our code\nand data are publicly available at https://github.com/cyzhh/Brain.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.00800v1",
    "published_date": "2024-02-23 17:40:31 UTC",
    "updated_date": "2024-02-23 17:40:31 UTC"
  },
  {
    "arxiv_id": "2403.00799v1",
    "title": "An Empirical Study of Data Ability Boundary in LLMs' Math Reasoning",
    "authors": [
      "Zui Chen",
      "Yezeng Chen",
      "Jiaqi Han",
      "Zhijie Huang",
      "Ji Qi",
      "Yi Zhou"
    ],
    "abstract": "Large language models (LLMs) are displaying emergent abilities for math\nreasoning tasks,and there is a growing attention on enhancing the ability of\nopen-source LLMs through supervised fine-tuning (SFT).In this paper, we aim to\nexplore a general data strategy for supervised data to help optimize and expand\nmath reasoning ability.Firstly, we determine the ability boundary of reasoning\npaths augmentation by identifying these paths' minimal optimal set.Secondly, we\nvalidate that different abilities of the model can be cumulatively enhanced by\nMix of Minimal Optimal Sets of corresponding types of data, while our models\nMMOS achieve SOTA performance on series base models under much lower\nconstruction costs.Besides, we point out GSM-HARD is not really hard and\ntoday's LLMs no longer lack numerical robustness.Also, we provide an Auto\nProblem Generator for robustness testing and educational applications.Our code\nand data are publicly available at https://github.com/cyzhh/MMOS.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "33 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.00799v1",
    "published_date": "2024-02-23 17:38:43 UTC",
    "updated_date": "2024-02-23 17:38:43 UTC"
  },
  {
    "arxiv_id": "2402.15448v1",
    "title": "Computer Vision for Multimedia Geolocation in Human Trafficking Investigation: A Systematic Literature Review",
    "authors": [
      "Opeyemi Bamigbade",
      "John Sheppard",
      "Mark Scanlon"
    ],
    "abstract": "The task of multimedia geolocation is becoming an increasingly essential\ncomponent of the digital forensics toolkit to effectively combat human\ntrafficking, child sexual exploitation, and other illegal acts. Typically,\nmetadata-based geolocation information is stripped when multimedia content is\nshared via instant messaging and social media. The intricacy of geolocating,\ngeotagging, or finding geographical clues in this content is often overly\nburdensome for investigators. Recent research has shown that contemporary\nadvancements in artificial intelligence, specifically computer vision and deep\nlearning, show significant promise towards expediting the multimedia\ngeolocation task. This systematic literature review thoroughly examines the\nstate-of-the-art leveraging computer vision techniques for multimedia\ngeolocation and assesses their potential to expedite human trafficking\ninvestigation. This includes a comprehensive overview of the application of\ncomputer vision-based approaches to multimedia geolocation, identifies their\napplicability in combating human trafficking, and highlights the potential\nimplications of enhanced multimedia geolocation for prosecuting human\ntrafficking. 123 articles inform this systematic literature review. The\nfindings suggest numerous potential paths for future impactful research on the\nsubject.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15448v1",
    "published_date": "2024-02-23 17:23:06 UTC",
    "updated_date": "2024-02-23 17:23:06 UTC"
  },
  {
    "arxiv_id": "2402.15552v4",
    "title": "Morphological Symmetries in Robotics",
    "authors": [
      "Daniel Ordo√±ez-Apraez",
      "Giulio Turrisi",
      "Vladimir Kostic",
      "Mario Martin",
      "Antonio Agudo",
      "Francesc Moreno-Noguer",
      "Massimiliano Pontil",
      "Claudio Semini",
      "Carlos Mastalli"
    ],
    "abstract": "We present a comprehensive framework for studying and leveraging\nmorphological symmetries in robotic systems. These are intrinsic properties of\nthe robot's morphology, frequently observed in animal biology and robotics,\nwhich stem from the replication of kinematic structures and the symmetrical\ndistribution of mass. We illustrate how these symmetries extend to the robot's\nstate space and both proprioceptive and exteroceptive sensor measurements,\nresulting in the equivariance of the robot's equations of motion and optimal\ncontrol policies. Thus, we recognize morphological symmetries as a relevant and\npreviously unexplored physics-informed geometric prior, with significant\nimplications for both data-driven and analytical methods used in modeling,\ncontrol, estimation and design in robotics. For data-driven methods, we\ndemonstrate that morphological symmetries can enhance the sample efficiency and\ngeneralization of machine learning models through data augmentation, or by\napplying equivariant/invariant constraints on the model's architecture. In the\ncontext of analytical methods, we employ abstract harmonic analysis to\ndecompose the robot's dynamics into a superposition of lower-dimensional,\nindependent dynamics. We substantiate our claims with both synthetic and\nreal-world experiments conducted on bipedal and quadrupedal robots. Lastly, we\nintroduce the repository MorphoSymm to facilitate the practical use of the\ntheory and applications outlined in this work.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "68T40",
      "I.2.9"
    ],
    "primary_category": "cs.RO",
    "comment": "18 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.15552v4",
    "published_date": "2024-02-23 17:21:21 UTC",
    "updated_date": "2025-03-24 18:59:34 UTC"
  },
  {
    "arxiv_id": "2402.15445v2",
    "title": "Can we forget how we learned? Doxastic redundancy in iterated belief revision",
    "authors": [
      "Paolo Liberatore"
    ],
    "abstract": "Forgetting a belief acquisition episode may not cause information loss\nbecause of the others. Checking whether it does is not obvious, as the\ncontribution of each belief revision is not isolated from the others, and the\nsame information may be given not directly but by deduction. An algorithm for\nchecking whether forgetting reduces information is given for a number of\niterated belief revision operators: lexicographic, natural, severe, plain\nsevere, moderate severe, restrained, very radical and full meet revisions. It\nmay take exponential time in the worst case, which is expected given that the\nproblem is coNP-hard, even in the Horn restriction. It is in coNP for\nhomogeneous sequences of lexicographic revisions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "formerly part of arXiv:2305.09200",
    "pdf_url": "http://arxiv.org/pdf/2402.15445v2",
    "published_date": "2024-02-23 17:09:04 UTC",
    "updated_date": "2025-04-26 11:53:06 UTC"
  },
  {
    "arxiv_id": "2402.15429v2",
    "title": "ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion Models against Stochastic Perturbation",
    "authors": [
      "Yi Zhang",
      "Yun Tang",
      "Wenjie Ruan",
      "Xiaowei Huang",
      "Siddartha Khastgir",
      "Paul Jennings",
      "Xingyu Zhao"
    ],
    "abstract": "Text-to-Image (T2I) Diffusion Models (DMs) have shown impressive abilities in\ngenerating high-quality images based on simple text descriptions. However, as\nis common with many Deep Learning (DL) models, DMs are subject to a lack of\nrobustness. While there are attempts to evaluate the robustness of T2I DMs as a\nbinary or worst-case problem, they cannot answer how robust in general the\nmodel is whenever an adversarial example (AE) can be found. In this study, we\nfirst introduce a probabilistic notion of T2I DMs' robustness; and then\nestablish an efficient framework, ProTIP, to evaluate it with statistical\nguarantees. The main challenges stem from: i) the high computational cost of\nthe generation process; and ii) determining if a perturbed input is an AE\ninvolves comparing two output distributions, which is fundamentally harder\ncompared to other DL tasks like classification where an AE is identified upon\nmisprediction of labels. To tackle the challenges, we employ sequential\nanalysis with efficacy and futility early stopping rules in the statistical\ntesting for identifying AEs, and adaptive concentration inequalities to\ndynamically determine the \"just-right\" number of stochastic perturbations\nwhenever the verification target is met. Empirical experiments validate the\neffectiveness and efficiency of ProTIP over common T2I DMs. Finally, we\ndemonstrate an application of ProTIP to rank commonly used defence methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV24",
    "pdf_url": "http://arxiv.org/pdf/2402.15429v2",
    "published_date": "2024-02-23 16:48:56 UTC",
    "updated_date": "2024-07-12 21:25:42 UTC"
  },
  {
    "arxiv_id": "2402.15427v1",
    "title": "Understanding Entrainment in Human Groups: Optimising Human-Robot Collaboration from Lessons Learned during Human-Human Collaboration",
    "authors": [
      "Eike Schneiders",
      "Christopher Fourie",
      "Stanley Celestin",
      "Julie Shah",
      "Malte Jung"
    ],
    "abstract": "Successful entrainment during collaboration positively affects trust,\nwillingness to collaborate, and likeability towards collaborators. In this\npaper, we present a mixed-method study to investigate characteristics of\nsuccessful entrainment leading to pair and group-based synchronisation. Drawing\ninspiration from industrial settings, we designed a fast-paced, short-cycle\nrepetitive task. Using motion tracking, we investigated entrainment in both\ndyadic and triadic task completion. Furthermore, we utilise audio-video\nrecordings and semi-structured interviews to contextualise participants'\nexperiences. This paper contributes to the Human-Computer/Robot Interaction\n(HCI/HRI) literature using a human-centred approach to identify characteristics\nof entrainment during pair- and group-based collaboration. We present five\ncharacteristics related to successful entrainment. These are related to the\noccurrence of entrainment, leader-follower patterns, interpersonal\ncommunication, the importance of the point-of-assembly, and the value of\nacoustic feedback. Finally, we present three design considerations for future\nresearch and design on collaboration with robots.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.HC",
    "comment": "Proceedings of the CHI Conference on Human Factors in Computing\n  Systems (CHI '24), May 11--16, 2024, Honolulu, HI, USA",
    "pdf_url": "http://arxiv.org/pdf/2402.15427v1",
    "published_date": "2024-02-23 16:42:17 UTC",
    "updated_date": "2024-02-23 16:42:17 UTC"
  },
  {
    "arxiv_id": "2402.15422v2",
    "title": "A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models",
    "authors": [
      "Stefan Hegselmann",
      "Shannon Zejiang Shen",
      "Florian Gierse",
      "Monica Agrawal",
      "David Sontag",
      "Xiaoyi Jiang"
    ],
    "abstract": "Patients often face difficulties in understanding their hospitalizations,\nwhile healthcare workers have limited resources to provide explanations. In\nthis work, we investigate the potential of large language models to generate\npatient summaries based on doctors' notes and study the effect of training data\non the faithfulness and quality of the generated summaries. To this end, we\nrelease (i) a rigorous labeling protocol for errors in medical texts and (ii) a\npublicly available dataset of annotated hallucinations in 100 doctor-written\nand 100 generated summaries. We show that fine-tuning on hallucination-free\ndata effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama\n2, while preserving relevant information. We observe a similar effect on GPT-4\n(0.70 to 0.40), when the few-shot examples are hallucination-free. We also\nconduct a qualitative evaluation using hallucination-free and improved training\ndata. We find that common quantitative metrics do not correlate well with\nfaithfulness and quality. Finally, we test GPT-4 for automatic hallucination\ndetection, which clearly outperforms common baselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15422v2",
    "published_date": "2024-02-23 16:32:28 UTC",
    "updated_date": "2024-06-25 17:02:10 UTC"
  },
  {
    "arxiv_id": "2402.15418v3",
    "title": "Reputational Algorithm Aversion",
    "authors": [
      "Gregory Weitzner"
    ],
    "abstract": "People are often reluctant to incorporate information produced by algorithms\ninto their decisions, a phenomenon called ``algorithm aversion''. This paper\nshows how algorithm aversion arises when the choice to follow an algorithm\nconveys information about a human's ability. I develop a model in which workers\nmake forecasts of an uncertain outcome based on their own private information\nand an algorithm's signal. Low-skill workers receive worse information than the\nalgorithm and hence should always follow the algorithm's signal, while\nhigh-skill workers receive better information than the algorithm and should\nsometimes override it. However, due to reputational concerns, low-skill workers\ninefficiently override the algorithm to increase the likelihood they are\nperceived as high-skill. The model provides a fully rational microfoundation\nfor algorithm aversion that aligns with the broad concern that AI systems will\ndisplace many types of workers.",
    "categories": [
      "econ.TH",
      "cs.AI",
      "cs.GT",
      "cs.HC"
    ],
    "primary_category": "econ.TH",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15418v3",
    "published_date": "2024-02-23 16:28:55 UTC",
    "updated_date": "2024-07-31 20:01:52 UTC"
  },
  {
    "arxiv_id": "2402.15398v1",
    "title": "TransFlower: An Explainable Transformer-Based Model with Flow-to-Flow Attention for Commuting Flow Prediction",
    "authors": [
      "Yan Luo",
      "Zhuoyue Wan",
      "Yuzhong Chen",
      "Gengchen Mai",
      "Fu-lai Chung",
      "Kent Larson"
    ],
    "abstract": "Understanding the link between urban planning and commuting flows is crucial\nfor guiding urban development and policymaking. This research, bridging\ncomputer science and urban studies, addresses the challenge of integrating\nthese fields with their distinct focuses. Traditional urban studies methods,\nlike the gravity and radiation models, often underperform in complex scenarios\ndue to their limited handling of multiple variables and reliance on overly\nsimplistic and unrealistic assumptions, such as spatial isotropy. While deep\nlearning models offer improved accuracy, their black-box nature poses a\ntrade-off between performance and explainability -- both vital for analyzing\ncomplex societal phenomena like commuting flows. To address this, we introduce\nTransFlower, an explainable, transformer-based model employing flow-to-flow\nattention to predict urban commuting patterns. It features a geospatial encoder\nwith an anisotropy-aware relative location encoder for nuanced flow\nrepresentation. Following this, the transformer-based flow predictor enhances\nthis by leveraging attention mechanisms to efficiently capture flow\ninteractions. Our model outperforms existing methods by up to 30.8% Common Part\nof Commuters, offering insights into mobility dynamics crucial for urban\nplanning and policy decisions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15398v1",
    "published_date": "2024-02-23 16:00:04 UTC",
    "updated_date": "2024-02-23 16:00:04 UTC"
  },
  {
    "arxiv_id": "2402.15393v4",
    "title": "NeuralSolver: Learning Algorithms For Consistent and Efficient Extrapolation Across General Tasks",
    "authors": [
      "Bernardo Esteves",
      "Miguel Vasco",
      "Francisco S. Melo"
    ],
    "abstract": "We contribute NeuralSolver, a novel recurrent solver that can efficiently and\nconsistently extrapolate, i.e., learn algorithms from smaller problems (in\nterms of observation size) and execute those algorithms in large problems.\nContrary to previous recurrent solvers, NeuralSolver can be naturally applied\nin both same-size problems, where the input and output sizes are the same, and\nin different-size problems, where the size of the input and output differ. To\nallow for this versatility, we design NeuralSolver with three main components:\na recurrent module, that iteratively processes input information at different\nscales, a processing module, responsible for aggregating the previously\nprocessed information, and a curriculum-based training scheme, that improves\nthe extrapolation performance of the method. To evaluate our method we\nintroduce a set of novel different-size tasks and we show that NeuralSolver\nconsistently outperforms the prior state-of-the-art recurrent solvers in\nextrapolating to larger problems, considering smaller training problems and\nrequiring less parameters than other approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15393v4",
    "published_date": "2024-02-23 15:51:45 UTC",
    "updated_date": "2024-10-31 09:46:13 UTC"
  },
  {
    "arxiv_id": "2402.15391v1",
    "title": "Genie: Generative Interactive Environments",
    "authors": [
      "Jake Bruce",
      "Michael Dennis",
      "Ashley Edwards",
      "Jack Parker-Holder",
      "Yuge Shi",
      "Edward Hughes",
      "Matthew Lai",
      "Aditi Mavalankar",
      "Richie Steigerwald",
      "Chris Apps",
      "Yusuf Aytar",
      "Sarah Bechtle",
      "Feryal Behbahani",
      "Stephanie Chan",
      "Nicolas Heess",
      "Lucy Gonzalez",
      "Simon Osindero",
      "Sherjil Ozair",
      "Scott Reed",
      "Jingwei Zhang",
      "Konrad Zolna",
      "Jeff Clune",
      "Nando de Freitas",
      "Satinder Singh",
      "Tim Rockt√§schel"
    ],
    "abstract": "We introduce Genie, the first generative interactive environment trained in\nan unsupervised manner from unlabelled Internet videos. The model can be\nprompted to generate an endless variety of action-controllable virtual worlds\ndescribed through text, synthetic images, photographs, and even sketches. At\n11B parameters, Genie can be considered a foundation world model. It is\ncomprised of a spatiotemporal video tokenizer, an autoregressive dynamics\nmodel, and a simple and scalable latent action model. Genie enables users to\nact in the generated environments on a frame-by-frame basis despite training\nwithout any ground-truth action labels or other domain-specific requirements\ntypically found in the world model literature. Further the resulting learned\nlatent action space facilitates training agents to imitate behaviors from\nunseen videos, opening the path for training generalist agents of the future.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "https://sites.google.com/corp/view/genie-2024/",
    "pdf_url": "http://arxiv.org/pdf/2402.15391v1",
    "published_date": "2024-02-23 15:47:26 UTC",
    "updated_date": "2024-02-23 15:47:26 UTC"
  },
  {
    "arxiv_id": "2402.15390v2",
    "title": "Explorations of Self-Repair in Language Models",
    "authors": [
      "Cody Rushing",
      "Neel Nanda"
    ],
    "abstract": "Prior interpretability research studying narrow distributions has\npreliminarily identified self-repair, a phenomena where if components in large\nlanguage models are ablated, later components will change their behavior to\ncompensate. Our work builds off this past literature, demonstrating that\nself-repair exists on a variety of models families and sizes when ablating\nindividual attention heads on the full training distribution. We further show\nthat on the full training distribution self-repair is imperfect, as the\noriginal direct effect of the head is not fully restored, and noisy, since the\ndegree of self-repair varies significantly across different prompts (sometimes\novercorrecting beyond the original effect). We highlight two different\nmechanisms that contribute to self-repair, including changes in the final\nLayerNorm scaling factor and sparse sets of neurons implementing Anti-Erasure.\nWe additionally discuss the implications of these results for interpretability\npractitioners and close with a more speculative discussion on the mystery of\nwhy self-repair occurs in these models at all, highlighting evidence for the\nIterative Inference hypothesis in language models, a framework that predicts\nself-repair.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.15390v2",
    "published_date": "2024-02-23 15:42:12 UTC",
    "updated_date": "2024-05-26 20:47:41 UTC"
  },
  {
    "arxiv_id": "2402.15384v3",
    "title": "Closed-loop Multi-step Planning",
    "authors": [
      "Giulia Lafratta",
      "Bernd Porr",
      "Christopher Chandler",
      "Alice Miller"
    ],
    "abstract": "Living organisms interact with their surroundings in a closed-loop fashion,\nwhere sensory inputs dictate the initiation and termination of behaviours. Even\nsimple animals are able to develop and execute complex plans, which has not yet\nbeen replicated in robotics using pure closed-loop input control. We propose a\nsolution to this problem by defining a set of discrete and temporary\nclosed-loop controllers, called ``Tasks'', each representing a closed-loop\nbehaviour. We further introduce a supervisory module which has an innate\nunderstanding of physics and causality, through which it can simulate the\nexecution of Task sequences over time and store the results in a model of the\nenvironment. On the basis of this model, plans can be made by chaining\ntemporary closed-loop controllers. Our proposed framework was implemented for a\nreal robot and tested in two scenarios as proof of concept.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15384v3",
    "published_date": "2024-02-23 15:30:57 UTC",
    "updated_date": "2025-01-29 12:47:26 UTC"
  },
  {
    "arxiv_id": "2402.15370v1",
    "title": "Dual Encoder: Exploiting the Potential of Syntactic and Semantic for Aspect Sentiment Triplet Extraction",
    "authors": [
      "Xiaowei Zhao",
      "Yong Zhou",
      "Xiujuan Xu"
    ],
    "abstract": "Aspect Sentiment Triple Extraction (ASTE) is an emerging task in fine-grained\nsentiment analysis. Recent studies have employed Graph Neural Networks (GNN) to\nmodel the syntax-semantic relationships inherent in triplet elements. However,\nthey have yet to fully tap into the vast potential of syntactic and semantic\ninformation within the ASTE task. In this work, we propose a \\emph{Dual\nEncoder: Exploiting the potential of Syntactic and Semantic} model (D2E2S),\nwhich maximizes the syntactic and semantic relationships among words.\nSpecifically, our model utilizes a dual-channel encoder with a BERT channel to\ncapture semantic information, and an enhanced LSTM channel for comprehensive\nsyntactic information capture. Subsequently, we introduce the heterogeneous\nfeature interaction module to capture intricate interactions between dependency\nsyntax and attention semantics, and to dynamically select vital nodes. We\nleverage the synergy of these modules to harness the significant potential of\nsyntactic and semantic information in ASTE tasks. Testing on public benchmarks,\nour D2E2S model surpasses the current state-of-the-art(SOTA), demonstrating its\neffectiveness.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.15370v1",
    "published_date": "2024-02-23 15:07:13 UTC",
    "updated_date": "2024-02-23 15:07:13 UTC"
  },
  {
    "arxiv_id": "2402.15368v4",
    "title": "Probabilistically Correct Language-based Multi-Robot Planning using Conformal Prediction",
    "authors": [
      "Jun Wang",
      "Guocheng He",
      "Yiannis Kantaros"
    ],
    "abstract": "This paper addresses task planning problems for language-instructed robot\nteams. Tasks are expressed in natural language (NL), requiring the robots to\napply their capabilities at various locations and semantic objects. Several\nrecent works have addressed similar planning problems by leveraging pre-trained\nLarge Language Models (LLMs) to design effective multi-robot plans. However,\nthese approaches lack performance guarantees. To address this challenge, we\nintroduce a new distributed LLM-based planner, called S-ATLAS for Safe plAnning\nfor Teams of Language-instructed AgentS, that is capable of achieving\nuser-defined mission success rates. This is accomplished by leveraging\nconformal prediction (CP), a distribution-free uncertainty quantification tool\nin black-box models. CP allows the proposed multi-robot planner to reason about\nits inherent uncertainty in a distributed fashion, enabling robots to make\nindividual decisions when they are sufficiently certain and seek help\notherwise. We show, both theoretically and empirically, that the proposed\nplanner can achieve user-specified task success rates, assuming successful plan\nexecution, while minimizing the overall number of help requests. We provide\ncomparative experiments against related works showing that our method is\nsignificantly more computational efficient and achieves lower help rates. The\nadvantage of our algorithm over baselines becomes more pronounced with\nincreasing robot team size.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15368v4",
    "published_date": "2024-02-23 15:02:44 UTC",
    "updated_date": "2024-11-21 16:19:35 UTC"
  },
  {
    "arxiv_id": "2402.15350v2",
    "title": "Farsight: Fostering Responsible AI Awareness During AI Application Prototyping",
    "authors": [
      "Zijie J. Wang",
      "Chinmay Kulkarni",
      "Lauren Wilcox",
      "Michael Terry",
      "Michael Madaio"
    ],
    "abstract": "Prompt-based interfaces for Large Language Models (LLMs) have made\nprototyping and building AI-powered applications easier than ever before.\nHowever, identifying potential harms that may arise from AI applications\nremains a challenge, particularly during prompt-based prototyping. To address\nthis, we present Farsight, a novel in situ interactive tool that helps people\nidentify potential harms from the AI applications they are prototyping. Based\non a user's prompt, Farsight highlights news articles about relevant AI\nincidents and allows users to explore and edit LLM-generated use cases,\nstakeholders, and harms. We report design insights from a co-design study with\n10 AI prototypers and findings from a user study with 42 AI prototypers. After\nusing Farsight, AI prototypers in our user study are better able to\nindependently identify potential harms associated with a prompt and find our\ntool more useful and usable than existing resources. Their qualitative feedback\nalso highlights that Farsight encourages them to focus on end-users and think\nbeyond immediate harms. We discuss these findings and reflect on their\nimplications for designing AI prototyping experiences that meaningfully engage\nwith AI harms. Farsight is publicly accessible at:\nhttps://PAIR-code.github.io/farsight.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to CHI 2024 (Best Paper, Honorable Mention). 40 pages, 19\n  figures, 5 tables. For a demo video, see https://youtu.be/BlSFbGkOlHk. For a\n  live demo, visit https://PAIR-code.github.io/farsight. The source code is\n  available at https://github.com/PAIR-code/farsight",
    "pdf_url": "http://arxiv.org/pdf/2402.15350v2",
    "published_date": "2024-02-23 14:38:05 UTC",
    "updated_date": "2024-07-02 06:12:05 UTC"
  },
  {
    "arxiv_id": "2402.15347v2",
    "title": "Information-Theoretic Safe Bayesian Optimization",
    "authors": [
      "Alessandro G. Bottero",
      "Carlos E. Luis",
      "Julia Vinogradska",
      "Felix Berkenkamp",
      "Jan Peters"
    ],
    "abstract": "We consider a sequential decision making task, where the goal is to optimize\nan unknown function without evaluating parameters that violate an a~priori\nunknown (safety) constraint. A common approach is to place a Gaussian process\nprior on the unknown functions and allow evaluations only in regions that are\nsafe with high probability. Most current methods rely on a discretization of\nthe domain and cannot be directly extended to the continuous case. Moreover,\nthe way in which they exploit regularity assumptions about the constraint\nintroduces an additional critical hyperparameter. In this paper, we propose an\ninformation-theoretic safe exploration criterion that directly exploits the GP\nposterior to identify the most informative safe parameters to evaluate. The\ncombination of this exploration criterion with a well known Bayesian\noptimization acquisition function yields a novel safe Bayesian optimization\nselection criterion. Our approach is naturally applicable to continuous domains\nand does not require additional explicit hyperparameters. We theoretically\nanalyze the method and show that we do not violate the safety constraint with\nhigh probability and that we learn about the value of the safe optimum up to\narbitrary precision. Empirical evaluations demonstrate improved data-efficiency\nand scalability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2212.04914",
    "pdf_url": "http://arxiv.org/pdf/2402.15347v2",
    "published_date": "2024-02-23 14:31:10 UTC",
    "updated_date": "2024-05-10 10:47:25 UTC"
  },
  {
    "arxiv_id": "2402.15343v1",
    "title": "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data",
    "authors": [
      "Sergei Bogdanov",
      "Alexandre Constantin",
      "Timoth√©e Bernard",
      "Benoit Crabb√©",
      "Etienne Bernard"
    ],
    "abstract": "Large Language Models (LLMs) have shown impressive abilities in data\nannotation, opening the way for new approaches to solve classic NLP problems.\nIn this paper, we show how to use LLMs to create NuNER, a compact language\nrepresentation model specialized in the Named Entity Recognition (NER) task.\nNuNER can be fine-tuned to solve downstream NER problems in a data-efficient\nway, outperforming similar-sized foundation models in the few-shot regime and\ncompeting with much larger LLMs. We find that the size and entity-type\ndiversity of the pre-training dataset are key to achieving good performance. We\nview NuNER as a member of the broader family of task-specific foundation\nmodels, recently unlocked by LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15343v1",
    "published_date": "2024-02-23 14:23:51 UTC",
    "updated_date": "2024-02-23 14:23:51 UTC"
  },
  {
    "arxiv_id": "2402.15333v1",
    "title": "A Quantum-Classical Collaborative Training Architecture Based on Quantum State Fidelity",
    "authors": [
      "Ryan L'Abbate",
      "Anthony D'Onofrio Jr.",
      "Samuel Stein",
      "Samuel Yen-Chi Chen",
      "Ang Li",
      "Pin-Yu Chen",
      "Juntao Chen",
      "Ying Mao"
    ],
    "abstract": "Recent advancements have highlighted the limitations of current quantum\nsystems, particularly the restricted number of qubits available on near-term\nquantum devices. This constraint greatly inhibits the range of applications\nthat can leverage quantum computers. Moreover, as the available qubits\nincrease, the computational complexity grows exponentially, posing additional\nchallenges. Consequently, there is an urgent need to use qubits efficiently and\nmitigate both present limitations and future complexities. To address this,\nexisting quantum applications attempt to integrate classical and quantum\nsystems in a hybrid framework. In this study, we concentrate on quantum deep\nlearning and introduce a collaborative classical-quantum architecture called\nco-TenQu. The classical component employs a tensor network for compression and\nfeature extraction, enabling higher-dimensional data to be encoded onto logical\nquantum circuits with limited qubits. On the quantum side, we propose a\nquantum-state-fidelity-based evaluation function to iteratively train the\nnetwork through a feedback loop between the two sides. co-TenQu has been\nimplemented and evaluated with both simulators and the IBM-Q platform. Compared\nto state-of-the-art approaches, co-TenQu enhances a classical deep neural\nnetwork by up to 41.72% in a fair setting. Additionally, it outperforms other\nquantum-based methods by up to 1.9 times and achieves similar accuracy while\nutilizing 70.59% fewer qubits.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "IEEE Transactions on Quantum Engineering",
    "pdf_url": "http://arxiv.org/pdf/2402.15333v1",
    "published_date": "2024-02-23 14:09:41 UTC",
    "updated_date": "2024-02-23 14:09:41 UTC"
  },
  {
    "arxiv_id": "2402.15332v2",
    "title": "Position: Categorical Deep Learning is an Algebraic Theory of All Architectures",
    "authors": [
      "Bruno Gavranoviƒá",
      "Paul Lessard",
      "Andrew Dudzik",
      "Tamara von Glehn",
      "Jo√£o G. M. Ara√∫jo",
      "Petar Veliƒçkoviƒá"
    ],
    "abstract": "We present our position on the elusive quest for a general-purpose framework\nfor specifying and studying deep learning architectures. Our opinion is that\nthe key attempts made so far lack a coherent bridge between specifying\nconstraints which models must satisfy and specifying their implementations.\nFocusing on building a such a bridge, we propose to apply category theory --\nprecisely, the universal algebra of monads valued in a 2-category of parametric\nmaps -- as a single theory elegantly subsuming both of these flavours of neural\nnetwork design. To defend our position, we show how this theory recovers\nconstraints induced by geometric deep learning, as well as implementations of\nmany architectures drawn from the diverse landscape of neural networks, such as\nRNNs. We also illustrate how the theory naturally encodes many standard\nconstructs in computer science and automata theory.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.CT",
      "math.RA",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in ICML 2024. Comments welcome. More info at\n  categoricaldeeplearning.com",
    "pdf_url": "http://arxiv.org/pdf/2402.15332v2",
    "published_date": "2024-02-23 14:01:53 UTC",
    "updated_date": "2024-06-06 00:58:55 UTC"
  },
  {
    "arxiv_id": "2402.15321v2",
    "title": "OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene Understanding",
    "authors": [
      "Francis Engelmann",
      "Ayca Takmaz",
      "Jonas Schult",
      "Elisabetta Fedele",
      "Johanna Wald",
      "Songyou Peng",
      "Xi Wang",
      "Or Litany",
      "Siyu Tang",
      "Federico Tombari",
      "Marc Pollefeys",
      "Leonidas Guibas",
      "Hongbo Tian",
      "Chunjie Wang",
      "Xiaosheng Yan",
      "Bingwen Wang",
      "Xuanyang Zhang",
      "Xiao Liu",
      "Phuc Nguyen",
      "Khoi Nguyen",
      "Anh Tran",
      "Cuong Pham",
      "Zhening Huang",
      "Xiaoyang Wu",
      "Xi Chen",
      "Hengshuang Zhao",
      "Lei Zhu",
      "Joan Lasenby"
    ],
    "abstract": "This report provides an overview of the challenge hosted at the OpenSUN3D\nWorkshop on Open-Vocabulary 3D Scene Understanding held in conjunction with\nICCV 2023. The goal of this workshop series is to provide a platform for\nexploration and discussion of open-vocabulary 3D scene understanding tasks,\nincluding but not limited to segmentation, detection and mapping. We provide an\noverview of the challenge hosted at the workshop, present the challenge\ndataset, the evaluation methodology, and brief descriptions of the winning\nmethods. For additional details, please see\nhttps://opensun3d.github.io/index_iccv23.html.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Our OpenSUN3D workshop website for ICCV 2023:\n  https://opensun3d.github.io/index_iccv23.html",
    "pdf_url": "http://arxiv.org/pdf/2402.15321v2",
    "published_date": "2024-02-23 13:39:59 UTC",
    "updated_date": "2024-03-17 08:41:49 UTC"
  },
  {
    "arxiv_id": "2402.15313v2",
    "title": "ArabianGPT: Native Arabic GPT-based Large Language Model",
    "authors": [
      "Anis Koubaa",
      "Adel Ammar",
      "Lahouari Ghouti",
      "Omar Najar",
      "Serry Sibaee"
    ],
    "abstract": "The predominance of English and Latin-based large language models (LLMs) has\nled to a notable deficit in native Arabic LLMs. This discrepancy is accentuated\nby the prevalent inclusion of English tokens in existing Arabic models,\ndetracting from their efficacy in processing native Arabic's intricate\nmorphology and syntax. Consequently, there is a theoretical and practical\nimperative for developing LLMs predominantly focused on Arabic linguistic\nelements. To address this gap, this paper proposes ArabianGPT, a series of\ntransformer-based models within the ArabianLLM suite designed explicitly for\nArabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in\nsize and complexity, aligning with the nuanced linguistic characteristics of\nArabic. The AraNizer tokenizer, integral to these models, addresses the unique\nmorphological aspects of Arabic script, ensuring more accurate text processing.\nEmpirical results from fine-tuning the models on tasks like sentiment analysis\nand summarization demonstrate significant improvements. For sentiment analysis,\nthe fine-tuned ArabianGPT-0.1B model achieved a remarkable accuracy of 95%, a\nsubstantial increase from the base model's 56%. Similarly, in summarization\ntasks, fine-tuned models showed enhanced F1 scores, indicating improved\nprecision and recall in generating concise summaries. Comparative analysis of\nfine-tuned ArabianGPT models against their base versions across various\nbenchmarks reveals nuanced differences in performance, with fine-tuning\npositively impacting specific tasks like question answering and summarization.\nThese findings underscore the efficacy of fine-tuning in aligning ArabianGPT\nmodels more closely with specific NLP tasks, highlighting the potential of\ntailored transformer architectures in advancing Arabic NLP.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15313v2",
    "published_date": "2024-02-23 13:32:47 UTC",
    "updated_date": "2024-02-26 09:54:47 UTC"
  },
  {
    "arxiv_id": "2402.16891v2",
    "title": "Multi-Task Learning for Routing Problem with Cross-Problem Zero-Shot Generalization",
    "authors": [
      "Fei Liu",
      "Xi Lin",
      "Zhenkun Wang",
      "Qingfu Zhang",
      "Xialiang Tong",
      "Mingxuan Yuan"
    ],
    "abstract": "Vehicle routing problems (VRPs), which can be found in numerous real-world\napplications, have been an important research topic for several decades.\nRecently, the neural combinatorial optimization (NCO) approach that leverages a\nlearning-based model to solve VRPs without manual algorithm design has gained\nsubstantial attention. However, current NCO methods typically require building\none model for each routing problem, which significantly hinders their practical\napplication for real-world industry problems with diverse attributes. In this\nwork, we make the first attempt to tackle the crucial challenge of\ncross-problem generalization. In particular, we formulate VRPs as different\ncombinations of a set of shared underlying attributes and solve them\nsimultaneously via a single model through attribute composition. In this way,\nour proposed model can successfully solve VRPs with unseen attribute\ncombinations in a zero-shot generalization manner. Extensive experiments are\nconducted on eleven VRP variants, benchmark datasets, and industry logistic\nscenarios. The results show that the unified model demonstrates superior\nperformance in the eleven VRPs, reducing the average gap to around 5% from over\n20% in the existing approach and achieving a significant performance boost on\nbenchmark datasets as well as a real-world logistics application. The source\ncode is included in https://github.com/FeiLiu36/MTNCO.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16891v2",
    "published_date": "2024-02-23 13:25:23 UTC",
    "updated_date": "2024-04-12 15:34:18 UTC"
  },
  {
    "arxiv_id": "2402.15307v1",
    "title": "Representing Online Handwriting for Recognition in Large Vision-Language Models",
    "authors": [
      "Anastasiia Fadeeva",
      "Philippe Schlattner",
      "Andrii Maksai",
      "Mark Collier",
      "Efi Kokiopoulou",
      "Jesse Berent",
      "Claudiu Musat"
    ],
    "abstract": "The adoption of tablets with touchscreens and styluses is increasing, and a\nkey feature is converting handwriting to text, enabling search, indexing, and\nAI assistance. Meanwhile, vision-language models (VLMs) are now the go-to\nsolution for image understanding, thanks to both their state-of-the-art\nperformance across a variety of tasks and the simplicity of a unified approach\nto training, fine-tuning, and inference. While VLMs obtain high performance on\nimage-based tasks, they perform poorly on handwriting recognition when applied\nnaively, i.e., by rendering handwriting as an image and performing optical\ncharacter recognition (OCR). In this paper, we study online handwriting\nrecognition with VLMs, going beyond naive OCR. We propose a novel tokenized\nrepresentation of digital ink (online handwriting) that includes both a\ntime-ordered sequence of strokes as text, and as image. We show that this\nrepresentation yields results comparable to or better than state-of-the-art\nonline handwriting recognizers. Wide applicability is shown through results\nwith two different VLM families, on multiple public datasets. Our approach can\nbe applied to off-the-shelf VLMs, does not require any changes in their\narchitecture, and can be used in both fine-tuning and parameter-efficient\ntuning. We perform a detailed ablation study to identify the key elements of\nthe proposed representation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15307v1",
    "published_date": "2024-02-23 13:11:10 UTC",
    "updated_date": "2024-02-23 13:11:10 UTC"
  },
  {
    "arxiv_id": "2402.15546v1",
    "title": "HiMAP: Learning Heuristics-Informed Policies for Large-Scale Multi-Agent Pathfinding",
    "authors": [
      "Huijie Tang",
      "Federico Berto",
      "Zihan Ma",
      "Chuanbo Hua",
      "Kyuree Ahn",
      "Jinkyoo Park"
    ],
    "abstract": "Large-scale multi-agent pathfinding (MAPF) presents significant challenges in\nseveral areas. As systems grow in complexity with a multitude of autonomous\nagents operating simultaneously, efficient and collision-free coordination\nbecomes paramount. Traditional algorithms often fall short in scalability,\nespecially in intricate scenarios. Reinforcement Learning (RL) has shown\npotential to address the intricacies of MAPF; however, it has also been shown\nto struggle with scalability, demanding intricate implementation, lengthy\ntraining, and often exhibiting unstable convergence, limiting its practical\napplication. In this paper, we introduce Heuristics-Informed Multi-Agent\nPathfinding (HiMAP), a novel scalable approach that employs imitation learning\nwith heuristic guidance in a decentralized manner. We train on small-scale\ninstances using a heuristic policy as a teacher that maps each single agent\nobservation information to an action probability distribution. During\npathfinding, we adopt several inference techniques to improve performance. With\na simple training scheme and implementation, HiMAP demonstrates competitive\nresults in terms of success rate and scalability in the field of\nimitation-learning-only MAPF, showing the potential of imitation-learning-only\nMAPF equipped with inference techniques.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted as Extended Abstract in Proc. of the 23rd International\n  Conference on Autonomous Agents and Multiagent Systems (AAMAS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2402.15546v1",
    "published_date": "2024-02-23 13:01:13 UTC",
    "updated_date": "2024-02-23 13:01:13 UTC"
  },
  {
    "arxiv_id": "2402.15300v2",
    "title": "Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding",
    "authors": [
      "Ailin Deng",
      "Zhirui Chen",
      "Bryan Hooi"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) are susceptible to object\nhallucinations, an issue in which their generated text contains non-existent\nobjects, greatly limiting their reliability and practicality. Current\napproaches often rely on the model's token likelihoods or other internal\ninformation, instruction tuning on additional datasets, or incorporating\ncomplex external tools. We first perform empirical analysis on sentence-level\nLVLM hallucination, finding that CLIP similarity to the image acts as a\nstronger and more robust indicator of hallucination compared to token\nlikelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD)\napproach, a straightforward but effective training-free approach to reduce\nobject hallucination at decoding time. CGD uses CLIP to guide the model's\ndecoding process by enhancing visual grounding of generated text with the\nimage. Experiments demonstrate that CGD effectively mitigates object\nhallucination across multiple LVLM families while preserving the utility of\ntext generation. Codes are available at\nhttps://github.com/d-ailin/CLIP-Guided-Decoding.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Code URL: https://github.com/d-ailin/CLIP-Guided-Decoding",
    "pdf_url": "http://arxiv.org/pdf/2402.15300v2",
    "published_date": "2024-02-23 12:57:16 UTC",
    "updated_date": "2024-04-23 09:32:25 UTC"
  },
  {
    "arxiv_id": "2402.15294v1",
    "title": "A Survey of Music Generation in the Context of Interaction",
    "authors": [
      "Ismael Agchar",
      "Ilja Baumann",
      "Franziska Braun",
      "Paula Andrea Perez-Toro",
      "Korbinian Riedhammer",
      "Sebastian Trump",
      "Martin Ullrich"
    ],
    "abstract": "In recent years, machine learning, and in particular generative adversarial\nneural networks (GANs) and attention-based neural networks (transformers), have\nbeen successfully used to compose and generate music, both melodies and\npolyphonic pieces. Current research focuses foremost on style replication (eg.\ngenerating a Bach-style chorale) or style transfer (eg. classical to jazz)\nbased on large amounts of recorded or transcribed music, which in turn also\nallows for fairly straight-forward \"performance\" evaluation. However, most of\nthese models are not suitable for human-machine co-creation through live\ninteraction, neither is clear, how such models and resulting creations would be\nevaluated. This article presents a thorough review of music representation,\nfeature analysis, heuristic algorithms, statistical and parametric modelling,\nand human and automatic evaluation measures, along with a discussion of which\napproaches and models seem most suitable for live interaction.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15294v1",
    "published_date": "2024-02-23 12:41:44 UTC",
    "updated_date": "2024-02-23 12:41:44 UTC"
  },
  {
    "arxiv_id": "2402.15290v4",
    "title": "Efficient State Space Model via Fast Tensor Convolution and Block Diagonalization",
    "authors": [
      "Tongyi Liang",
      "Han-Xiong Li"
    ],
    "abstract": "Existing models encounter bottlenecks in balancing performance and\ncomputational efficiency when modeling long sequences. Although the state space\nmodel (SSM) has achieved remarkable success in handling long sequence tasks, it\nstill faces the problem of large number of parameters. In order to further\nimprove the efficiency of SSM, we propose a new state space layer based on\nmultiple-input multiple-output SSM, called efficient SSM (eSSM). Our eSSM is\nbuilt on the convolutional representation of multi-input and multi-input (MIMO)\nSSM. We propose a variety of effective strategies to improve the computational\nefficiency. The diagonalization of the system matrix first decouples the\noriginal system. Then a fast tensor convolution is proposed based on the fast\nFourier transform. In addition, the block diagonalization of the SSM further\nreduces the model parameters and improves the model flexibility. Extensive\nexperimental results show that the performance of the proposed model on\nmultiple databases matches the performance of state-of-the-art models, such as\nS4, and is significantly better than Transformers and LSTM. In the model\nefficiency benchmark, the parameters of eSSM are only 12.89\\% of LSTM and\n13.24\\% of Mamba. The training speed of eSSM is 3.94 times faster than LSTM and\n1.35 times faster than Mamba. Code is available at:\n\\href{https://github.com/leonty1/essm}{https://github.com/leonty1/essm}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15290v4",
    "published_date": "2024-02-23 12:36:31 UTC",
    "updated_date": "2025-05-05 02:20:07 UTC"
  },
  {
    "arxiv_id": "2402.15284v1",
    "title": "Spatiotemporal Observer Design for Predictive Learning of High-Dimensional Data",
    "authors": [
      "Tongyi Liang",
      "Han-Xiong Li"
    ],
    "abstract": "Although deep learning-based methods have shown great success in\nspatiotemporal predictive learning, the framework of those models is designed\nmainly by intuition. How to make spatiotemporal forecasting with theoretical\nguarantees is still a challenging issue. In this work, we tackle this problem\nby applying domain knowledge from the dynamical system to the framework design\nof deep learning models. An observer theory-guided deep learning architecture,\ncalled Spatiotemporal Observer, is designed for predictive learning of high\ndimensional data. The characteristics of the proposed framework are twofold:\nfirstly, it provides the generalization error bound and convergence guarantee\nfor spatiotemporal prediction; secondly, dynamical regularization is introduced\nto enable the model to learn system dynamics better during training. Further\nexperimental results show that this framework could capture the spatiotemporal\ndynamics and make accurate predictions in both one-step-ahead and\nmulti-step-ahead forecasting scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2402.15284v1",
    "published_date": "2024-02-23 12:28:31 UTC",
    "updated_date": "2024-02-23 12:28:31 UTC"
  },
  {
    "arxiv_id": "2402.15283v1",
    "title": "When in Doubt, Think Slow: Iterative Reasoning with Latent Imagination",
    "authors": [
      "Martin Benfeghoul",
      "Umais Zahid",
      "Qinghai Guo",
      "Zafeirios Fountas"
    ],
    "abstract": "In an unfamiliar setting, a model-based reinforcement learning agent can be\nlimited by the accuracy of its world model. In this work, we present a novel,\ntraining-free approach to improving the performance of such agents separately\nfrom planning and learning. We do so by applying iterative inference at\ndecision-time, to fine-tune the inferred agent states based on the coherence of\nfuture state representations. Our approach achieves a consistent improvement in\nboth reconstruction accuracy and task performance when applied to visual 3D\nnavigation tasks. We go on to show that considering more future states further\nimproves the performance of the agent in partially-observable environments, but\nnot in a fully-observable one. Finally, we demonstrate that agents with less\ntraining pre-evaluation benefit most from our approach.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.0; I.2.8; I.2.10; I.4.5; I.4.10"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15283v1",
    "published_date": "2024-02-23 12:27:48 UTC",
    "updated_date": "2024-02-23 12:27:48 UTC"
  },
  {
    "arxiv_id": "2402.15276v3",
    "title": "CFIR: Fast and Effective Long-Text To Image Retrieval for Large Corpora",
    "authors": [
      "Zijun Long",
      "Xuri Ge",
      "Richard Mccreadie",
      "Joemon Jose"
    ],
    "abstract": "Text-to-image retrieval aims to find the relevant images based on a text\nquery, which is important in various use-cases, such as digital libraries,\ne-commerce, and multimedia databases. Although Multimodal Large Language Models\n(MLLMs) demonstrate state-of-the-art performance, they exhibit limitations in\nhandling large-scale, diverse, and ambiguous real-world needs of retrieval, due\nto the computation cost and the injective embeddings they produce. This paper\npresents a two-stage Coarse-to-Fine Index-shared Retrieval (CFIR) framework,\ndesigned for fast and effective large-scale long-text to image retrieval. The\nfirst stage, Entity-based Ranking (ER), adapts to long-text query ambiguity by\nemploying a multiple-queries-to-multiple-targets paradigm, facilitating\ncandidate filtering for the next stage. The second stage, Summary-based\nRe-ranking (SR), refines these rankings using summarized queries. We also\npropose a specialized Decoupling-BEiT-3 encoder, optimized for handling\nambiguous user needs and both stages, which also enhances computational\nefficiency through vector-based similarity inference. Evaluation on the AToMiC\ndataset reveals that CFIR surpasses existing MLLMs by up to 11.06% in\nRecall@1000, while reducing training and retrieval times by 68.75% and 99.79%,\nrespectively. We will release our code to facilitate future research at\nhttps://github.com/longkukuhi/CFIR.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15276v3",
    "published_date": "2024-02-23 11:47:16 UTC",
    "updated_date": "2024-04-02 20:54:46 UTC"
  },
  {
    "arxiv_id": "2402.15272v1",
    "title": "EMIFF: Enhanced Multi-scale Image Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection",
    "authors": [
      "Zhe Wang",
      "Siqi Fan",
      "Xiaoliang Huo",
      "Tongda Xu",
      "Yan Wang",
      "Jingjing Liu",
      "Yilun Chen",
      "Ya-Qin Zhang"
    ],
    "abstract": "In autonomous driving, cooperative perception makes use of multi-view cameras\nfrom both vehicles and infrastructure, providing a global vantage point with\nrich semantic context of road conditions beyond a single vehicle viewpoint.\nCurrently, two major challenges persist in vehicle-infrastructure cooperative\n3D (VIC3D) object detection: $1)$ inherent pose errors when fusing multi-view\nimages, caused by time asynchrony across cameras; $2)$ information loss in\ntransmission process resulted from limited communication bandwidth. To address\nthese issues, we propose a novel camera-based 3D detection framework for VIC3D\ntask, Enhanced Multi-scale Image Feature Fusion (EMIFF). To fully exploit\nholistic perspectives from both vehicles and infrastructure, we propose\nMulti-scale Cross Attention (MCA) and Camera-aware Channel Masking (CCM)\nmodules to enhance infrastructure and vehicle features at scale, spatial, and\nchannel levels to correct the pose error introduced by camera asynchrony. We\nalso introduce a Feature Compression (FC) module with channel and spatial\ncompression blocks for transmission efficiency. Experiments show that EMIFF\nachieves SOTA on DAIR-V2X-C datasets, significantly outperforming previous\nearly-fusion and late-fusion methods with comparable transmission costs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 8 figures. Accepted by ICRA 2024. arXiv admin note: text\n  overlap with arXiv:arXiv:2303.10975",
    "pdf_url": "http://arxiv.org/pdf/2402.15272v1",
    "published_date": "2024-02-23 11:35:48 UTC",
    "updated_date": "2024-02-23 11:35:48 UTC"
  },
  {
    "arxiv_id": "2402.15270v2",
    "title": "Smoothed Graph Contrastive Learning via Seamless Proximity Integration",
    "authors": [
      "Maysam Behmanesh",
      "Maks Ovsjanikov"
    ],
    "abstract": "Graph contrastive learning (GCL) aligns node representations by classifying\nnode pairs into positives and negatives using a selection process that\ntypically relies on establishing correspondences within two augmented graphs.\nThe conventional GCL approaches incorporate negative samples uniformly in the\ncontrastive loss, resulting in the equal treatment of negative nodes,\nregardless of their proximity to the true positive. In this paper, we present a\nSmoothed Graph Contrastive Learning model (SGCL), which leverages the geometric\nstructure of augmented graphs to inject proximity information associated with\npositive/negative pairs in the contrastive loss, thus significantly\nregularizing the learning process. The proposed SGCL adjusts the penalties\nassociated with node pairs in contrastive loss by incorporating three distinct\nsmoothing techniques that result in proximity-aware positives and negatives. To\nenhance scalability for large-scale graphs, the proposed framework incorporates\na graph batch-generating strategy that partitions the given graphs into\nmultiple subgraphs, facilitating efficient training in separate batches.\nThrough extensive experimentation in the unsupervised setting on various\nbenchmarks, particularly those of large scale, we demonstrate the superiority\nof our proposed framework against recent baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.15270v2",
    "published_date": "2024-02-23 11:32:46 UTC",
    "updated_date": "2024-11-26 14:50:41 UTC"
  },
  {
    "arxiv_id": "2402.15268v1",
    "title": "MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained Language Models",
    "authors": [
      "Nathana√´l Carraz Rakotonirina",
      "Marco Baroni"
    ],
    "abstract": "Transformer-based language models (LMs) track contextual information through\nlarge, hard-coded input windows. We introduce MemoryPrompt, a leaner approach\nin which the LM is complemented by a small auxiliary recurrent network that\npasses information to the LM by prefixing its regular input with a sequence of\nvectors, akin to soft prompts, without requiring LM finetuning. Tested on a\ntask designed to probe a LM's ability to keep track of multiple fact updates, a\nMemoryPrompt-augmented LM outperforms much larger LMs that have access to the\nfull input history. We also test MemoryPrompt on a long-distance dialogue\ndataset, where its performance is comparable to that of a model conditioned on\nthe entire conversation history. In both experiments we also observe that,\nunlike full-finetuning approaches, MemoryPrompt does not suffer from\ncatastrophic forgetting when adapted to new tasks, thus not disrupting the\ngeneralist capabilities of the underlying LM.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Published as conference paper at LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.15268v1",
    "published_date": "2024-02-23 11:30:39 UTC",
    "updated_date": "2024-02-23 11:30:39 UTC"
  },
  {
    "arxiv_id": "2402.15267v2",
    "title": "A Robust Defense against Adversarial Attacks on Deep Learning-based Malware Detectors via (De)Randomized Smoothing",
    "authors": [
      "Daniel Gibert",
      "Giulio Zizzo",
      "Quan Le",
      "Jordi Planes"
    ],
    "abstract": "Deep learning-based malware detectors have been shown to be susceptible to\nadversarial malware examples, i.e. malware examples that have been deliberately\nmanipulated in order to avoid detection. In light of the vulnerability of deep\nlearning detectors to subtle input file modifications, we propose a practical\ndefense against adversarial malware examples inspired by (de)randomized\nsmoothing. In this work, we reduce the chances of sampling adversarial content\ninjected by malware authors by selecting correlated subsets of bytes, rather\nthan using Gaussian noise to randomize inputs like in the Computer Vision (CV)\ndomain. During training, our ablation-based smoothing scheme trains a base\nclassifier to make classifications on a subset of contiguous bytes or chunk of\nbytes. At test time, a large number of chunks are then classified by a base\nclassifier and the consensus among these classifications is then reported as\nthe final prediction. We propose two strategies to determine the location of\nthe chunks used for classification: (1) randomly selecting the locations of the\nchunks and (2) selecting contiguous adjacent chunks. To showcase the\neffectiveness of our approach, we have trained two classifiers with our\nchunk-based ablation schemes on the BODMAS dataset. Our findings reveal that\nthe chunk-based smoothing classifiers exhibit greater resilience against\nadversarial malware examples generated with state-of-the-are evasion attacks,\noutperforming a non-smoothed classifier and a randomized smoothing-based\nclassifier by a great margin.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "arXiv admin note: text overlap with arXiv:2308.08906",
    "pdf_url": "http://arxiv.org/pdf/2402.15267v2",
    "published_date": "2024-02-23 11:30:12 UTC",
    "updated_date": "2024-02-26 21:30:45 UTC"
  },
  {
    "arxiv_id": "2402.15266v2",
    "title": "Calibration of Deep Learning Classification Models in fNIRS",
    "authors": [
      "Zhihao Cao",
      "Zizhou Luo"
    ],
    "abstract": "Functional near-infrared spectroscopy (fNIRS) is a valuable non-invasive tool\nfor monitoring brain activity. The classification of fNIRS data in relation to\nconscious activity holds significance for advancing our understanding of the\nbrain and facilitating the development of brain-computer interfaces (BCI). Many\nresearchers have turned to deep learning to tackle the classification\nchallenges inherent in fNIRS data due to its strong generalization and\nrobustness. In the application of fNIRS, reliability is really important, and\none mathematical formulation of the reliability of confidence is calibration.\nHowever, many researchers overlook the important issue of calibration. To\naddress this gap, we propose integrating calibration into fNIRS field and\nassess the reliability of existing models. Surprisingly, our results indicate\npoor calibration performance in many proposed models. To advance calibration\ndevelopment in the fNIRS field, we summarize three practical tips. Through this\nletter, we hope to emphasize the critical role of calibration in fNIRS research\nand argue for enhancing the reliability of deep learning-based predictions in\nfNIRS classification tasks. All data from our experimental process are openly\navailable on GitHub.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15266v2",
    "published_date": "2024-02-23 11:27:10 UTC",
    "updated_date": "2024-03-20 10:12:49 UTC"
  },
  {
    "arxiv_id": "2402.15262v1",
    "title": "Dynamic Memory Based Adaptive Optimization",
    "authors": [
      "Bal√°zs Szegedy",
      "Domonkos Czifra",
      "P√©ter K≈ër√∂si-Szab√≥"
    ],
    "abstract": "Define an optimizer as having memory $k$ if it stores $k$ dynamically\nchanging vectors in the parameter space. Classical SGD has memory $0$, momentum\nSGD optimizer has $1$ and Adam optimizer has $2$. We address the following\nquestions: How can optimizers make use of more memory units? What information\nshould be stored in them? How to use them for the learning steps? As an\napproach to the last question, we introduce a general method called\n\"Retrospective Learning Law Correction\" or shortly RLLC. This method is\ndesigned to calculate a dynamically varying linear combination (called learning\nlaw) of memory units, which themselves may evolve arbitrarily. We demonstrate\nRLLC on optimizers whose memory units have linear update rules and small memory\n($\\leq 4$ memory units). Our experiments show that in a variety of standard\nproblems, these optimizers outperform the above mentioned three classical\noptimizers. We conclude that RLLC is a promising framework for boosting the\nperformance of known optimizers by adding more memory units and by making them\nmore adaptive.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15262v1",
    "published_date": "2024-02-23 11:19:02 UTC",
    "updated_date": "2024-02-23 11:19:02 UTC"
  },
  {
    "arxiv_id": "2402.15255v2",
    "title": "Optimal Transport for Structure Learning Under Missing Data",
    "authors": [
      "Vy Vo",
      "He Zhao",
      "Trung Le",
      "Edwin V. Bonilla",
      "Dinh Phung"
    ],
    "abstract": "Causal discovery in the presence of missing data introduces a chicken-and-egg\ndilemma. While the goal is to recover the true causal structure, robust\nimputation requires considering the dependencies or, preferably, causal\nrelations among variables. Merely filling in missing values with existing\nimputation methods and subsequently applying structure learning on the complete\ndata is empirically shown to be sub-optimal. To address this problem, we\npropose a score-based algorithm for learning causal structures from missing\ndata based on optimal transport. This optimal transport viewpoint diverges from\nexisting score-based approaches that are dominantly based on expectation\nmaximization. We formulate structure learning as a density fitting problem,\nwhere the goal is to find the causal model that induces a distribution of\nminimum Wasserstein distance with the observed data distribution. Our framework\nis shown to recover the true causal graphs more effectively than competing\nmethods in most simulations and real-data settings. Empirical evidence also\nshows the superior scalability of our approach, along with the flexibility to\nincorporate any off-the-shelf causal discovery methods for complete data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15255v2",
    "published_date": "2024-02-23 10:49:04 UTC",
    "updated_date": "2024-06-01 10:57:01 UTC"
  },
  {
    "arxiv_id": "2402.16889v1",
    "title": "Generative Models are Self-Watermarked: Declaring Model Authentication through Re-Generation",
    "authors": [
      "Aditya Desu",
      "Xuanli He",
      "Qiongkai Xu",
      "Wei Lu"
    ],
    "abstract": "As machine- and AI-generated content proliferates, protecting the\nintellectual property of generative models has become imperative, yet verifying\ndata ownership poses formidable challenges, particularly in cases of\nunauthorized reuse of generated data. The challenge of verifying data ownership\nis further amplified by using Machine Learning as a Service (MLaaS), which\noften functions as a black-box system.\n  Our work is dedicated to detecting data reuse from even an individual sample.\nTraditionally, watermarking has been leveraged to detect AI-generated content.\nHowever, unlike watermarking techniques that embed additional information as\ntriggers into models or generated content, potentially compromising output\nquality, our approach identifies latent fingerprints inherently present within\nthe outputs through re-generation. We propose an explainable verification\nprocedure that attributes data ownership through re-generation, and further\namplifies these fingerprints in the generative models through iterative data\nre-generation. This methodology is theoretically grounded and demonstrates\nviability and robustness using recent advanced text and image generative\nmodels. Our methodology is significant as it goes beyond protecting the\nintellectual property of APIs and addresses important issues such as the spread\nof misinformation and academic misconduct. It provides a useful tool to ensure\nthe integrity of sources and authorship, expanding its application in different\nscenarios where authenticity and ownership verification are essential.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16889v1",
    "published_date": "2024-02-23 10:48:21 UTC",
    "updated_date": "2024-02-23 10:48:21 UTC"
  },
  {
    "arxiv_id": "2402.15247v1",
    "title": "A Bargaining-based Approach for Feature Trading in Vertical Federated Learning",
    "authors": [
      "Yue Cui",
      "Liuyi Yao",
      "Zitao Li",
      "Yaliang Li",
      "Bolin Ding",
      "Xiaofang Zhou"
    ],
    "abstract": "Vertical Federated Learning (VFL) has emerged as a popular machine learning\nparadigm, enabling model training across the data and the task parties with\ndifferent features about the same user set while preserving data privacy. In\nproduction environment, VFL usually involves one task party and one data party.\nFair and economically efficient feature trading is crucial to the\ncommercialization of VFL, where the task party is considered as the data\nconsumer who buys the data party's features. However, current VFL feature\ntrading practices often price the data party's data as a whole and assume\ntransactions occur prior to the performing VFL. Neglecting the performance\ngains resulting from traded features may lead to underpayment and overpayment\nissues. In this study, we propose a bargaining-based feature trading approach\nin VFL to encourage economically efficient transactions. Our model incorporates\nperformance gain-based pricing, taking into account the revenue-based\noptimization objectives of both parties. We analyze the proposed bargaining\nmodel under perfect and imperfect performance information settings, proving the\nexistence of an equilibrium that optimizes the parties' objectives. Moreover,\nwe develop performance gain estimation-based bargaining strategies for\nimperfect performance information scenarios and discuss potential security\nissues and solutions. Experiments on three real-world datasets demonstrate the\neffectiveness of the proposed bargaining model.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15247v1",
    "published_date": "2024-02-23 10:21:07 UTC",
    "updated_date": "2024-02-23 10:21:07 UTC"
  },
  {
    "arxiv_id": "2402.15246v1",
    "title": "Artificial Bee Colony optimization of Deep Convolutional Neural Networks in the context of Biomedical Imaging",
    "authors": [
      "Adri Gomez Martin",
      "Carlos Fernandez del Cerro",
      "Monica Abella Garcia",
      "Manuel Desco Menendez"
    ],
    "abstract": "Most efforts in Computer Vision focus on natural images or artwork, which\ndiffer significantly both in size and contents from the kind of data biomedical\nimage processing deals with. Thus, Transfer Learning models often prove\nthemselves suboptimal for these tasks, even after manual finetuning. The\ndevelopment of architectures from scratch is oftentimes unfeasible due to the\nvastness of the hyperparameter space and a shortage of time, computational\nresources and Deep Learning experts in most biomedical research laboratories.\nAn alternative to manually defining the models is the use of Neuroevolution,\nwhich employs metaheuristic techniques to optimize Deep Learning architectures.\nHowever, many algorithms proposed in the neuroevolutive literature are either\ntoo unreliable or limited to a small, predefined region of the hyperparameter\nspace. To overcome these shortcomings, we propose the Chimera Algorithm, a\nnovel, hybrid neuroevolutive algorithm that integrates the Artificial Bee\nColony Algorithm with Evolutionary Computation tools to generate models from\nscratch, as well as to refine a given previous architecture to better fit the\ntask at hand. The Chimera Algorithm has been validated with two datasets of\nnatural and medical images, producing models that surpassed the performance of\nthose coming from Transfer Learning.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15246v1",
    "published_date": "2024-02-23 10:21:03 UTC",
    "updated_date": "2024-02-23 10:21:03 UTC"
  },
  {
    "arxiv_id": "2404.07217v2",
    "title": "Attention-aware Semantic Communications for Collaborative Inference",
    "authors": [
      "Jiwoong Im",
      "Nayoung Kwon",
      "Taewoo Park",
      "Jiheon Woo",
      "Jaeho Lee",
      "Yongjune Kim"
    ],
    "abstract": "We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.07217v2",
    "published_date": "2024-02-23 10:08:45 UTC",
    "updated_date": "2024-05-31 14:23:09 UTC"
  },
  {
    "arxiv_id": "2402.15227v1",
    "title": "Fixed Random Classifier Rearrangement for Continual Learning",
    "authors": [
      "Shengyang Huang",
      "Jianwen Mo"
    ],
    "abstract": "With the explosive growth of data, continual learning capability is\nincreasingly important for neural networks. Due to catastrophic forgetting,\nneural networks inevitably forget the knowledge of old tasks after learning new\nones. In visual classification scenario, a common practice of alleviating the\nforgetting is to constrain the backbone. However, the impact of classifiers is\nunderestimated. In this paper, we analyze the variation of model predictions in\nsequential binary classification tasks and find that the norm of the equivalent\none-class classifiers significantly affects the forgetting level. Based on this\nconclusion, we propose a two-stage continual learning algorithm named Fixed\nRandom Classifier Rearrangement (FRCR). In first stage, FRCR replaces the\nlearnable classifiers with fixed random classifiers, constraining the norm of\nthe equivalent one-class classifiers without affecting the performance of the\nnetwork. In second stage, FRCR rearranges the entries of new classifiers to\nimplicitly reduce the drift of old latent representations. The experimental\nresults on multiple datasets show that FRCR significantly mitigates the model\nforgetting; subsequent experimental analyses further validate the effectiveness\nof the algorithm.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15227v1",
    "published_date": "2024-02-23 09:43:58 UTC",
    "updated_date": "2024-02-23 09:43:58 UTC"
  },
  {
    "arxiv_id": "2403.14650v1",
    "title": "Harnessing the Computing Continuum across Personalized Healthcare, Maintenance and Inspection, and Farming 4.0",
    "authors": [
      "Fatemeh Baghdadi",
      "Davide Cirillo",
      "Daniele Lezzi",
      "Francesc Lordan",
      "Fernando Vazquez",
      "Eugenio Lomurno",
      "Alberto Archetti",
      "Danilo Ardagna",
      "Matteo Matteucci"
    ],
    "abstract": "The AI-SPRINT project, launched in 2021 and funded by the European\nCommission, focuses on the development and implementation of AI applications\nacross the computing continuum. This continuum ensures the coherent integration\nof computational resources and services from centralized data centers to edge\ndevices, facilitating efficient and adaptive computation and application\ndelivery. AI-SPRINT has achieved significant scientific advances, including\nstreamlined processes, improved efficiency, and the ability to operate in real\ntime, as evidenced by three practical use cases. This paper provides an\nin-depth examination of these applications -- Personalized Healthcare,\nMaintenance and Inspection, and Farming 4.0 -- highlighting their practical\nimplementation and the objectives achieved with the integration of AI-SPRINT\ntechnologies. We analyze how the proposed toolchain effectively addresses a\nrange of challenges and refines processes, discussing its relevance and impact\nin multiple domains. After a comprehensive overview of the main AI-SPRINT tools\nused in these scenarios, the paper summarizes of the findings and key lessons\nlearned.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14650v1",
    "published_date": "2024-02-23 09:20:34 UTC",
    "updated_date": "2024-02-23 09:20:34 UTC"
  },
  {
    "arxiv_id": "2402.16887v1",
    "title": "Artificial Intelligence for Complex Network: Potential, Methodology and Application",
    "authors": [
      "Jingtao Ding",
      "Chang Liu",
      "Yu Zheng",
      "Yunke Zhang",
      "Zihan Yu",
      "Ruikun Li",
      "Hongyi Chen",
      "Jinghua Piao",
      "Huandong Wang",
      "Jiazhen Liu",
      "Yong Li"
    ],
    "abstract": "Complex networks pervade various real-world systems, from the natural\nenvironment to human societies. The essence of these networks is in their\nability to transition and evolve from microscopic disorder-where network\ntopology and node dynamics intertwine-to a macroscopic order characterized by\ncertain collective behaviors. Over the past two decades, complex network\nscience has significantly enhanced our understanding of the statistical\nmechanics, structures, and dynamics underlying real-world networks. Despite\nthese advancements, there remain considerable challenges in exploring more\nrealistic systems and enhancing practical applications. The emergence of\nartificial intelligence (AI) technologies, coupled with the abundance of\ndiverse real-world network data, has heralded a new era in complex network\nscience research. This survey aims to systematically address the potential\nadvantages of AI in overcoming the lingering challenges of complex network\nresearch. It endeavors to summarize the pivotal research problems and provide\nan exhaustive review of the corresponding methodologies and applications.\nThrough this comprehensive survey-the first of its kind on AI for complex\nnetworks-we expect to provide valuable insights that will drive further\nresearch and advancement in this interdisciplinary field.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG",
      "physics.soc-ph"
    ],
    "primary_category": "cs.SI",
    "comment": "51 pages, 4 figures, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.16887v1",
    "published_date": "2024-02-23 09:06:36 UTC",
    "updated_date": "2024-02-23 09:06:36 UTC"
  },
  {
    "arxiv_id": "2402.15205v1",
    "title": "Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing",
    "authors": [
      "Samuel Kernan Freire",
      "Margo MC van Mol",
      "Carola Schol",
      "Elif √ñzcan Vieira"
    ],
    "abstract": "Intensive care unit (ICU) patients often develop new health-related problems\nin their long-term recovery. Health care professionals keeping a diary of a\npatient's stay is a proven strategy to tackle this but faces several adoption\nbarriers, such as lack of time and difficulty in knowing what to write. Large\nlanguage models (LLMs), with their ability to generate human-like text and\nadaptability, could solve these challenges. However, realizing this vision\ninvolves addressing several socio-technical and practical research challenges.\nThis paper discusses these challenges and proposes future research directions\nto utilize the potential of LLMs in ICU diary writing, ultimately improving the\nlong-term recovery outcomes for ICU patients.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "3 pages, under review",
    "pdf_url": "http://arxiv.org/pdf/2402.15205v1",
    "published_date": "2024-02-23 09:06:25 UTC",
    "updated_date": "2024-02-23 09:06:25 UTC"
  },
  {
    "arxiv_id": "2402.15197v1",
    "title": "Safety Optimized Reinforcement Learning via Multi-Objective Policy Optimization",
    "authors": [
      "Homayoun Honari",
      "Mehran Ghafarian Tamizi",
      "Homayoun Najjaran"
    ],
    "abstract": "Safe reinforcement learning (Safe RL) refers to a class of techniques that\naim to prevent RL algorithms from violating constraints in the process of\ndecision-making and exploration during trial and error. In this paper, a novel\nmodel-free Safe RL algorithm, formulated based on the multi-objective policy\noptimization framework is introduced where the policy is optimized towards\noptimality and safety, simultaneously. The optimality is achieved by the\nenvironment reward function that is subsequently shaped using a safety critic.\nThe advantage of the Safety Optimized RL (SORL) algorithm compared to the\ntraditional Safe RL algorithms is that it omits the need to constrain the\npolicy search space. This allows SORL to find a natural tradeoff between safety\nand optimality without compromising the performance in terms of either safety\nor optimality due to strict search space constraints. Through our theoretical\nanalysis of SORL, we propose a condition for SORL's converged policy to\nguarantee safety and then use it to introduce an aggressiveness parameter that\nallows for fine-tuning the mentioned tradeoff. The experimental results\nobtained in seven different robotic environments indicate a considerable\nreduction in the number of safety violations along with higher, or competitive,\npolicy returns, in comparison to six different state-of-the-art Safe RL\nmethods. The results demonstrate the significant superiority of the proposed\nSORL algorithm in safety-critical applications.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "Accepted to the IEEE International Conference on Robotics and\n  Automation (ICRA) 2024, 7 Pages, 3 Figures",
    "pdf_url": "http://arxiv.org/pdf/2402.15197v1",
    "published_date": "2024-02-23 08:58:38 UTC",
    "updated_date": "2024-02-23 08:58:38 UTC"
  },
  {
    "arxiv_id": "2402.15195v1",
    "title": "The AffectToolbox: Affect Analysis for Everyone",
    "authors": [
      "Silvan Mertes",
      "Dominik Schiller",
      "Michael Dietz",
      "Elisabeth Andr√©",
      "Florian Lingenfelser"
    ],
    "abstract": "In the field of affective computing, where research continually advances at a\nrapid pace, the demand for user-friendly tools has become increasingly\napparent. In this paper, we present the AffectToolbox, a novel software system\nthat aims to support researchers in developing affect-sensitive studies and\nprototypes. The proposed system addresses the challenges posed by existing\nframeworks, which often require profound programming knowledge and cater\nprimarily to power-users or skilled developers. Aiming to facilitate ease of\nuse, the AffectToolbox requires no programming knowledge and offers its\nfunctionality to reliably analyze the affective state of users through an\naccessible graphical user interface. The architecture encompasses a variety of\nmodels for emotion recognition on multiple affective channels and modalities,\nas well as an elaborate fusion system to merge multi-modal assessments into a\nunified result. The entire system is open-sourced and will be publicly\navailable to ensure easy integration into more complex applications through a\nwell-structured, Python-based code base - therefore marking a substantial\ncontribution toward advancing affective computing research and fostering a more\ncollaborative and inclusive environment within this interdisciplinary field.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15195v1",
    "published_date": "2024-02-23 08:55:47 UTC",
    "updated_date": "2024-02-23 08:55:47 UTC"
  },
  {
    "arxiv_id": "2402.15194v2",
    "title": "Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control",
    "authors": [
      "Masatoshi Uehara",
      "Yulai Zhao",
      "Kevin Black",
      "Ehsan Hajiramezanali",
      "Gabriele Scalia",
      "Nathaniel Lee Diamant",
      "Alex M Tseng",
      "Tommaso Biancalani",
      "Sergey Levine"
    ],
    "abstract": "Diffusion models excel at capturing complex data distributions, such as those\nof natural images and proteins. While diffusion models are trained to represent\nthe distribution in the training dataset, we often are more concerned with\nother properties, such as the aesthetic quality of the generated images or the\nfunctional properties of generated proteins. Diffusion models can be finetuned\nin a goal-directed way by maximizing the value of some reward function (e.g.,\nthe aesthetic quality of an image). However, these approaches may lead to\nreduced sample diversity, significant deviations from the training data\ndistribution, and even poor sample quality due to the exploitation of an\nimperfect reward function. The last issue often occurs when the reward function\nis a learned model meant to approximate a ground-truth \"genuine\" reward, as is\nthe case in many practical applications. These challenges, collectively termed\n\"reward collapse,\" pose a substantial obstacle. To address this reward\ncollapse, we frame the finetuning problem as entropy-regularized control\nagainst the pretrained diffusion model, i.e., directly optimizing\nentropy-enhanced rewards with neural SDEs. We present theoretical and empirical\nevidence that demonstrates our framework is capable of efficiently generating\ndiverse samples with high genuine rewards, mitigating the overoptimization of\nimperfect reward models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review (codes will be released soon)",
    "pdf_url": "http://arxiv.org/pdf/2402.15194v2",
    "published_date": "2024-02-23 08:54:42 UTC",
    "updated_date": "2024-02-28 09:21:46 UTC"
  },
  {
    "arxiv_id": "2402.15189v2",
    "title": "Biomedical Entity Linking as Multiple Choice Question Answering",
    "authors": [
      "Zhenxi Lin",
      "Ziheng Zhang",
      "Xian Wu",
      "Yefeng Zheng"
    ],
    "abstract": "Although biomedical entity linking (BioEL) has made significant progress with\npre-trained language models, challenges still exist for fine-grained and\nlong-tailed entities. To address these challenges, we present BioELQA, a novel\nmodel that treats Biomedical Entity Linking as Multiple Choice Question\nAnswering. BioELQA first obtains candidate entities with a fast retriever,\njointly presents the mention and candidate entities to a generator, and then\noutputs the predicted symbol associated with its chosen entity. This\nformulation enables explicit comparison of different candidate entities, thus\ncapturing fine-grained interactions between mentions and entities, as well as\namong entities themselves. To improve generalization for long-tailed entities,\nwe retrieve similar labeled training instances as clues and concatenate the\ninput with retrieved instances for the generator. Extensive experimental\nresults show that BioELQA outperforms state-of-the-art baselines on several\ndatasets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.15189v2",
    "published_date": "2024-02-23 08:40:38 UTC",
    "updated_date": "2024-05-17 09:11:44 UTC"
  },
  {
    "arxiv_id": "2402.15183v5",
    "title": "GraphEdit: Large Language Models for Graph Structure Learning",
    "authors": [
      "Zirui Guo",
      "Lianghao Xia",
      "Yanhua Yu",
      "Yuling Wang",
      "Kangkang Lu",
      "Zhiyong Huang",
      "Chao Huang"
    ],
    "abstract": "Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies\nand interactions among nodes in graph-structured data by generating novel graph\nstructures. Graph Neural Networks (GNNs) have emerged as promising GSL\nsolutions, utilizing recursive message passing to encode node-wise\ninter-dependencies. However, many existing GSL methods heavily depend on\nexplicit graph structural information as supervision signals, leaving them\nsusceptible to challenges such as data noise and sparsity. In this work, we\npropose GraphEdit, an approach that leverages large language models (LLMs) to\nlearn complex node relationships in graph-structured data. By enhancing the\nreasoning capabilities of LLMs through instruction-tuning over graph\nstructures, we aim to overcome the limitations associated with explicit graph\nstructural information and enhance the reliability of graph structure learning.\nOur approach not only effectively denoises noisy connections but also\nidentifies node-wise dependencies from a global perspective, providing a\ncomprehensive understanding of the graph structure. We conduct extensive\nexperiments on multiple benchmark datasets to demonstrate the effectiveness and\nrobustness of GraphEdit across various settings. We have made our model\nimplementation available at: https://github.com/HKUDS/GraphEdit.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15183v5",
    "published_date": "2024-02-23 08:29:42 UTC",
    "updated_date": "2025-03-10 14:04:39 UTC"
  },
  {
    "arxiv_id": "2402.15170v1",
    "title": "The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling",
    "authors": [
      "Jiajun Ma",
      "Shuchen Xue",
      "Tianyang Hu",
      "Wenjia Wang",
      "Zhaoqiang Liu",
      "Zhenguo Li",
      "Zhi-Ming Ma",
      "Kenji Kawaguchi"
    ],
    "abstract": "With the incorporation of the UNet architecture, diffusion probabilistic\nmodels have become a dominant force in image generation tasks. One key design\nin UNet is the skip connections between the encoder and decoder blocks.\nAlthough skip connections have been shown to improve training stability and\nmodel performance, we reveal that such shortcuts can be a limiting factor for\nthe complexity of the transformation. As the sampling steps decrease, the\ngeneration process and the role of the UNet get closer to the push-forward\ntransformations from Gaussian distribution to the target, posing a challenge\nfor the network's complexity. To address this challenge, we propose\nSkip-Tuning, a simple yet surprisingly effective training-free tuning method on\nthe skip connections. Our method can achieve 100% FID improvement for\npretrained EDM on ImageNet 64 with only 19 NFEs (1.75), breaking the limit of\nODE samplers regardless of sampling steps. Surprisingly, the improvement\npersists when we increase the number of sampling steps and can even surpass the\nbest result from EDM-2 (1.58) with only 39 NFEs (1.57). Comprehensive\nexploratory experiments are conducted to shed light on the surprising\neffectiveness. We observe that while Skip-Tuning increases the score-matching\nlosses in the pixel space, the losses in the feature space are reduced,\nparticularly at intermediate noise levels, which coincide with the most\neffective range accounting for image quality improvement.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15170v1",
    "published_date": "2024-02-23 08:05:23 UTC",
    "updated_date": "2024-02-23 08:05:23 UTC"
  },
  {
    "arxiv_id": "2402.15163v4",
    "title": "Has the Deep Neural Network learned the Stochastic Process? An Evaluation Viewpoint",
    "authors": [
      "Harshit Kumar",
      "Beomseok Kang",
      "Biswadeep Chakraborty",
      "Saibal Mukhopadhyay"
    ],
    "abstract": "This paper presents the first systematic study of evaluating Deep Neural\nNetworks (DNNs) designed to forecast the evolution of stochastic complex\nsystems. We show that traditional evaluation methods like threshold-based\nclassification metrics and error-based scoring rules assess a DNN's ability to\nreplicate the observed ground truth but fail to measure the DNN's learning of\nthe underlying stochastic process. To address this gap, we propose a new\nevaluation criterion called Fidelity to Stochastic Process (F2SP), representing\nthe DNN's ability to predict the system property Statistic-GT--the ground truth\nof the stochastic process--and introduce an evaluation metric that exclusively\nassesses F2SP. We formalize F2SP within a stochastic framework and establish\ncriteria for validly measuring it. We formally show that Expected Calibration\nError (ECE) satisfies the necessary condition for testing F2SP, unlike\ntraditional evaluation methods. Empirical experiments on synthetic datasets,\nincluding wildfire, host-pathogen, and stock market models, demonstrate that\nECE uniquely captures F2SP. We further extend our study to real-world wildfire\ndata, highlighting the limitations of conventional evaluation and discuss the\npractical utility of incorporating F2SP into model assessment. This work offers\na new perspective on evaluating DNNs modeling complex systems by emphasizing\nthe importance of capturing the underlying stochastic process.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2402.15163v4",
    "published_date": "2024-02-23 07:54:20 UTC",
    "updated_date": "2025-01-25 07:23:25 UTC"
  },
  {
    "arxiv_id": "2402.15162v1",
    "title": "Entity-level Factual Adaptiveness of Fine-tuning based Abstractive Summarization Models",
    "authors": [
      "Jongyoon Song",
      "Nohil Park",
      "Bongkyu Hwang",
      "Jaewoong Yun",
      "Seongho Joe",
      "Youngjune L. Gwon",
      "Sungroh Yoon"
    ],
    "abstract": "Abstractive summarization models often generate factually inconsistent\ncontent particularly when the parametric knowledge of the model conflicts with\nthe knowledge in the input document. In this paper, we analyze the robustness\nof fine-tuning based summarization models to the knowledge conflict, which we\ncall factual adaptiveness. We utilize pre-trained language models to construct\nevaluation sets and find that factual adaptiveness is not strongly correlated\nwith factual consistency on original datasets. Furthermore, we introduce a\ncontrollable counterfactual data augmentation method where the degree of\nknowledge conflict within the augmented data can be adjustable. Our\nexperimental results on two pre-trained language models (PEGASUS and BART) and\ntwo fine-tuning datasets (XSum and CNN/DailyMail) demonstrate that our method\nenhances factual adaptiveness while achieving factual consistency on original\ndatasets on par with the contrastive learning baseline.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.15162v1",
    "published_date": "2024-02-23 07:53:39 UTC",
    "updated_date": "2024-02-23 07:53:39 UTC"
  },
  {
    "arxiv_id": "2402.15160v3",
    "title": "Spatially-Aware Transformer for Embodied Agents",
    "authors": [
      "Junmo Cho",
      "Jaesik Yoon",
      "Sungjin Ahn"
    ],
    "abstract": "Episodic memory plays a crucial role in various cognitive processes, such as\nthe ability to mentally recall past events. While cognitive science emphasizes\nthe significance of spatial context in the formation and retrieval of episodic\nmemory, the current primary approach to implementing episodic memory in AI\nsystems is through transformers that store temporally ordered experiences,\nwhich overlooks the spatial dimension. As a result, it is unclear how the\nunderlying structure could be extended to incorporate the spatial axis beyond\ntemporal order alone and thereby what benefits can be obtained. To address\nthis, this paper explores the use of Spatially-Aware Transformer models that\nincorporate spatial information. These models enable the creation of\nplace-centric episodic memory that considers both temporal and spatial\ndimensions. Adopting this approach, we demonstrate that memory utilization\nefficiency can be improved, leading to enhanced accuracy in various\nplace-centric downstream tasks. Additionally, we propose the Adaptive Memory\nAllocator, a memory management method based on reinforcement learning that aims\nto optimize efficiency of memory utilization. Our experiments demonstrate the\nadvantages of our proposed model in various environments and across multiple\ndownstream tasks, including prediction, generation, reasoning, and\nreinforcement learning. The source code for our models and experiments will be\navailable at https://github.com/junmokane/spatially-aware-transformer.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2024 Spotlight. First two authors contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2402.15160v3",
    "published_date": "2024-02-23 07:46:30 UTC",
    "updated_date": "2024-03-01 00:58:50 UTC"
  },
  {
    "arxiv_id": "2402.15159v3",
    "title": "Machine Unlearning of Pre-trained Large Language Models",
    "authors": [
      "Jin Yao",
      "Eli Chien",
      "Minxin Du",
      "Xinyao Niu",
      "Tianhao Wang",
      "Zezhou Cheng",
      "Xiang Yue"
    ],
    "abstract": "This study investigates the concept of the `right to be forgotten' within the\ncontext of large language models (LLMs). We explore machine unlearning as a\npivotal solution, with a focus on pre-trained models--a notably\nunder-researched area. Our research delineates a comprehensive framework for\nmachine unlearning in pre-trained LLMs, encompassing a critical analysis of\nseven diverse unlearning methods. Through rigorous evaluation using curated\ndatasets from arXiv, books, and GitHub, we establish a robust benchmark for\nunlearning performance, demonstrating that these methods are over $10^5$ times\nmore computationally efficient than retraining. Our results show that\nintegrating gradient ascent with gradient descent on in-distribution data\nimproves hyperparameter robustness. We also provide detailed guidelines for\nefficient hyperparameter tuning in the unlearning process. Our findings advance\nthe discourse on ethical AI practices, offering substantive insights into the\nmechanics of machine unlearning for pre-trained LLMs and underscoring the\npotential for responsible AI development.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024 main. Code and data at\n  https://github.com/yaojin17/Unlearning_LLM",
    "pdf_url": "http://arxiv.org/pdf/2402.15159v3",
    "published_date": "2024-02-23 07:43:26 UTC",
    "updated_date": "2024-05-30 15:44:51 UTC"
  },
  {
    "arxiv_id": "2402.15152v2",
    "title": "On the Duality Between Sharpness-Aware Minimization and Adversarial Training",
    "authors": [
      "Yihao Zhang",
      "Hangzhou He",
      "Jingyu Zhu",
      "Huanran Chen",
      "Yifei Wang",
      "Zeming Wei"
    ],
    "abstract": "Adversarial Training (AT), which adversarially perturb the input samples\nduring training, has been acknowledged as one of the most effective defenses\nagainst adversarial attacks, yet suffers from inevitably decreased clean\naccuracy. Instead of perturbing the samples, Sharpness-Aware Minimization (SAM)\nperturbs the model weights during training to find a more flat loss landscape\nand improve generalization. However, as SAM is designed for better clean\naccuracy, its effectiveness in enhancing adversarial robustness remains\nunexplored. In this work, considering the duality between SAM and AT, we\ninvestigate the adversarial robustness derived from SAM. Intriguingly, we find\nthat using SAM alone can improve adversarial robustness. To understand this\nunexpected property of SAM, we first provide empirical and theoretical insights\ninto how SAM can implicitly learn more robust features, and conduct\ncomprehensive experiments to show that SAM can improve adversarial robustness\nnotably without sacrificing any clean accuracy, shedding light on the potential\nof SAM to be a substitute for AT when accuracy comes at a higher priority. Code\nis available at https://github.com/weizeming/SAM_AT.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.15152v2",
    "published_date": "2024-02-23 07:22:55 UTC",
    "updated_date": "2024-06-05 08:39:57 UTC"
  },
  {
    "arxiv_id": "2402.15140v2",
    "title": "A Relation-Interactive Approach for Message Passing in Hyper-relational Knowledge Graphs",
    "authors": [
      "Yonglin Jing"
    ],
    "abstract": "Hyper-relational knowledge graphs (KGs) contain additional key-value pairs,\nproviding more information about the relations. In many scenarios, the same\nrelation can have distinct key-value pairs, making the original triple fact\nmore recognizable and specific. Prior studies on hyper-relational KGs have\nestablished a solid standard method for hyper-relational graph encoding. In\nthis work, we propose a message-passing-based graph encoder with global\nrelation structure awareness ability, which we call ReSaE. Compared to the\nprior state-of-the-art approach, ReSaE emphasizes the interaction of relations\nduring message passing process and optimizes the readout structure for link\nprediction tasks. Overall, ReSaE gives a encoding solution for hyper-relational\nKGs and ensures stronger performance on downstream link prediction tasks. Our\nexperiments demonstrate that ReSaE achieves state-of-the-art performance on\nmultiple link prediction benchmarks. Furthermore, we also analyze the influence\nof different model structures on model performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15140v2",
    "published_date": "2024-02-23 06:55:04 UTC",
    "updated_date": "2024-03-02 04:59:36 UTC"
  },
  {
    "arxiv_id": "2402.15135v1",
    "title": "Modified CycleGAN for the synthesization of samples for wheat head segmentation",
    "authors": [
      "Jaden Myers",
      "Keyhan Najafian",
      "Farhad Maleki",
      "Katie Ovens"
    ],
    "abstract": "Deep learning models have been used for a variety of image processing tasks.\nHowever, most of these models are developed through supervised learning\napproaches, which rely heavily on the availability of large-scale annotated\ndatasets. Developing such datasets is tedious and expensive. In the absence of\nan annotated dataset, synthetic data can be used for model development;\nhowever, due to the substantial differences between simulated and real data, a\nphenomenon referred to as domain gap, the resulting models often underperform\nwhen applied to real data. In this research, we aim to address this challenge\nby first computationally simulating a large-scale annotated dataset and then\nusing a generative adversarial network (GAN) to fill the gap between simulated\nand real images. This approach results in a synthetic dataset that can be\neffectively utilized to train a deep-learning model. Using this approach, we\ndeveloped a realistic annotated synthetic dataset for wheat head segmentation.\nThis dataset was then used to develop a deep-learning model for semantic\nsegmentation. The resulting model achieved a Dice score of 83.4\\% on an\ninternal dataset and Dice scores of 79.6% and 83.6% on two external Global\nWheat Head Detection datasets. While we proposed this approach in the context\nof wheat head segmentation, it can be generalized to other crop types or, more\nbroadly, to images with dense, repeated patterns such as those found in\ncellular imagery.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15135v1",
    "published_date": "2024-02-23 06:42:58 UTC",
    "updated_date": "2024-02-23 06:42:58 UTC"
  },
  {
    "arxiv_id": "2402.15134v1",
    "title": "Deep Coupling Network For Multivariate Time Series Forecasting",
    "authors": [
      "Kun Yi",
      "Qi Zhang",
      "Hui He",
      "Kaize Shi",
      "Liang Hu",
      "Ning An",
      "Zhendong Niu"
    ],
    "abstract": "Multivariate time series (MTS) forecasting is crucial in many real-world\napplications. To achieve accurate MTS forecasting, it is essential to\nsimultaneously consider both intra- and inter-series relationships among time\nseries data. However, previous work has typically modeled intra- and\ninter-series relationships separately and has disregarded multi-order\ninteractions present within and between time series data, which can seriously\ndegrade forecasting accuracy. In this paper, we reexamine intra- and\ninter-series relationships from the perspective of mutual information and\naccordingly construct a comprehensive relationship learning mechanism tailored\nto simultaneously capture the intricate multi-order intra- and inter-series\ncouplings. Based on the mechanism, we propose a novel deep coupling network for\nMTS forecasting, named DeepCN, which consists of a coupling mechanism dedicated\nto explicitly exploring the multi-order intra- and inter-series relationships\namong time series data concurrently, a coupled variable representation module\naimed at encoding diverse variable patterns, and an inference module\nfacilitating predictions through one forward step. Extensive experiments\nconducted on seven real-world datasets demonstrate that our proposed DeepCN\nachieves superior performance compared with the state-of-the-art baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15134v1",
    "published_date": "2024-02-23 06:38:08 UTC",
    "updated_date": "2024-02-23 06:38:08 UTC"
  },
  {
    "arxiv_id": "2402.15131v3",
    "title": "Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models",
    "authors": [
      "Guanming Xiong",
      "Junwei Bao",
      "Wen Zhao"
    ],
    "abstract": "This study explores the realm of knowledge base question answering (KBQA).\nKBQA is considered a challenging task, particularly in parsing intricate\nquestions into executable logical forms. Traditional semantic parsing\n(SP)-based methods require extensive data annotations, which result in\nsignificant costs. Recently, the advent of few-shot in-context learning,\npowered by large language models (LLMs), has showcased promising capabilities.\nHowever, fully leveraging LLMs to parse questions into logical forms in\nlow-resource scenarios poses a substantial challenge. To tackle these hurdles,\nwe introduce Interactive-KBQA, a framework designed to generate logical forms\nthrough direct interaction with knowledge bases (KBs). Within this framework,\nwe have developed three generic APIs for KB interaction. For each category of\ncomplex question, we devised exemplars to guide LLMs through the reasoning\nprocesses. Our method achieves competitive results on the WebQuestionsSP,\nComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number of\nexamples (shots). Importantly, our approach supports manual intervention,\nallowing for the iterative refinement of LLM outputs. By annotating a dataset\nwith step-wise reasoning processes, we showcase our model's adaptability and\nhighlight its potential for contributing significant enhancements to the field.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "This work has been accepted by the ACL 2024 main conference. Code and\n  data are available at: https://github.com/JimXiongGM/Interactive-KBQA",
    "pdf_url": "http://arxiv.org/pdf/2402.15131v3",
    "published_date": "2024-02-23 06:32:18 UTC",
    "updated_date": "2025-03-12 06:15:34 UTC"
  },
  {
    "arxiv_id": "2402.15538v1",
    "title": "AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System",
    "authors": [
      "Zhiwei Liu",
      "Weiran Yao",
      "Jianguo Zhang",
      "Liangwei Yang",
      "Zuxin Liu",
      "Juntao Tan",
      "Prafulla K. Choubey",
      "Tian Lan",
      "Jason Wu",
      "Huan Wang",
      "Shelby Heinecke",
      "Caiming Xiong",
      "Silvio Savarese"
    ],
    "abstract": "The booming success of LLMs initiates rapid development in LLM agents. Though\nthe foundation of an LLM agent is the generative model, it is critical to\ndevise the optimal reasoning strategies and agent architectures. Accordingly,\nLLM agent research advances from the simple chain-of-thought prompting to more\ncomplex ReAct and Reflection reasoning strategy; agent architecture also\nevolves from single agent generation to multi-agent conversation, as well as\nmulti-LLM multi-agent group chat. However, with the existing intricate\nframeworks and libraries, creating and evaluating new reasoning strategies and\nagent architectures has become a complex challenge, which hinders research\ninvestigation into LLM agents. Thus, we open-source a new AI agent library,\nAgentLite, which simplifies this process by offering a lightweight,\nuser-friendly platform for innovating LLM agent reasoning, architectures, and\napplications with ease. AgentLite is a task-oriented framework designed to\nenhance the ability of agents to break down tasks and facilitate the\ndevelopment of multi-agent systems. Furthermore, we introduce multiple\npractical applications developed with AgentLite to demonstrate its convenience\nand flexibility. Get started now at:\n\\url{https://github.com/SalesforceAIResearch/AgentLite}.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "preprint. Library is available at\n  https://github.com/SalesforceAIResearch/AgentLite",
    "pdf_url": "http://arxiv.org/pdf/2402.15538v1",
    "published_date": "2024-02-23 06:25:20 UTC",
    "updated_date": "2024-02-23 06:25:20 UTC"
  },
  {
    "arxiv_id": "2402.15120v1",
    "title": "Fine-tuning CLIP Text Encoders with Two-step Paraphrasing",
    "authors": [
      "Hyunjae Kim",
      "Seunghyun Yoon",
      "Trung Bui",
      "Handong Zhao",
      "Quan Tran",
      "Franck Dernoncourt",
      "Jaewoo Kang"
    ],
    "abstract": "Contrastive language-image pre-training (CLIP) models have demonstrated\nconsiderable success across various vision-language tasks, such as\ntext-to-image retrieval, where the model is required to effectively process\nnatural language input to produce an accurate visual output. However, current\nmodels still face limitations in dealing with linguistic variations in input\nqueries, such as paraphrases, making it challenging to handle a broad range of\nuser queries in real-world applications. In this study, we introduce a\nstraightforward fine-tuning approach to enhance the representations of CLIP\nmodels for paraphrases. Our approach involves a two-step paraphrase generation\nprocess, where we automatically create two categories of paraphrases from\nweb-scale image captions by leveraging large language models. Subsequently, we\nfine-tune the CLIP text encoder using these generated paraphrases while\nfreezing the image encoder. Our resulting model, which we call ParaCLIP,\nexhibits significant improvements over baseline CLIP models across various\ntasks, including paraphrased retrieval (with rank similarity scores improved by\nup to 2.0% and 5.6%), Visual Genome Relation and Attribution, as well as seven\nsemantic textual similarity tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "EACL 2024 (Findings of the ACL)",
    "pdf_url": "http://arxiv.org/pdf/2402.15120v1",
    "published_date": "2024-02-23 06:11:50 UTC",
    "updated_date": "2024-02-23 06:11:50 UTC"
  },
  {
    "arxiv_id": "2403.00796v1",
    "title": "Enhancing Mean-Reverting Time Series Prediction with Gaussian Processes: Functional and Augmented Data Structures in Financial Forecasting",
    "authors": [
      "Narayan Tondapu"
    ],
    "abstract": "In this paper, we explore the application of Gaussian Processes (GPs) for\npredicting mean-reverting time series with an underlying structure, using\nrelatively unexplored functional and augmented data structures. While many\nconventional forecasting methods concentrate on the short-term dynamics of time\nseries data, GPs offer the potential to forecast not just the average\nprediction but the entire probability distribution over a future trajectory.\nThis is particularly beneficial in financial contexts, where accurate\npredictions alone may not suffice if incorrect volatility assessments lead to\ncapital losses. Moreover, in trade selection, GPs allow for the forecasting of\nmultiple Sharpe ratios adjusted for transaction costs, aiding in\ndecision-making. The functional data representation utilized in this study\nenables longer-term predictions by leveraging information from previous years,\neven as the forecast moves away from the current year's training data.\nAdditionally, the augmented representation enriches the training set by\nincorporating multiple targets for future points in time, facilitating\nlong-term predictions. Our implementation closely aligns with the methodology\noutlined in, which assessed effectiveness on commodity futures. However, our\ntesting methodology differs. Instead of real data, we employ simulated data\nwith similar characteristics. We construct a testing environment to evaluate\nboth data representations and models under conditions of increasing noise, fat\ntails, and inappropriate kernels-conditions commonly encountered in practice.\nBy simulating data, we can compare our forecast distribution over time against\na full simulation of the actual distribution of our test set, thereby reducing\nthe inherent uncertainty in testing time series models on real data. We enable\nfeature prediction through augmentation and employ sub-sampling to ensure the\nfeasibility of GPs.",
    "categories": [
      "q-fin.ST",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "q-fin.ST",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.00796v1",
    "published_date": "2024-02-23 06:09:45 UTC",
    "updated_date": "2024-02-23 06:09:45 UTC"
  },
  {
    "arxiv_id": "2402.15116v1",
    "title": "Large Multimodal Agents: A Survey",
    "authors": [
      "Junlin Xie",
      "Zhihong Chen",
      "Ruifei Zhang",
      "Xiang Wan",
      "Guanbin Li"
    ],
    "abstract": "Large language models (LLMs) have achieved superior performance in powering\ntext-based AI agents, endowing them with decision-making and reasoning\nabilities akin to humans. Concurrently, there is an emerging research trend\nfocused on extending these LLM-powered AI agents into the multimodal domain.\nThis extension enables AI agents to interpret and respond to diverse multimodal\nuser queries, thereby handling more intricate and nuanced tasks. In this paper,\nwe conduct a systematic review of LLM-driven multimodal agents, which we refer\nto as large multimodal agents ( LMAs for short). First, we introduce the\nessential components involved in developing LMAs and categorize the current\nbody of research into four distinct types. Subsequently, we review the\ncollaborative frameworks integrating multiple LMAs , enhancing collective\nefficacy. One of the critical challenges in this field is the diverse\nevaluation methods used across existing studies, hindering effective comparison\namong different LMAs . Therefore, we compile these evaluation methodologies and\nestablish a comprehensive framework to bridge the gaps. This framework aims to\nstandardize evaluations, facilitating more meaningful comparisons. Concluding\nour review, we highlight the extensive applications of LMAs and propose\npossible future research directions. Our discussion aims to provide valuable\ninsights and guidelines for future research in this rapidly evolving field. An\nup-to-date resource list is available at\nhttps://github.com/jun0wanan/awesome-large-multimodal-agents.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.15116v1",
    "published_date": "2024-02-23 06:04:23 UTC",
    "updated_date": "2024-02-23 06:04:23 UTC"
  },
  {
    "arxiv_id": "2403.00795v2",
    "title": "Executing Natural Language-Described Algorithms with Large Language Models: An Investigation",
    "authors": [
      "Xin Zheng",
      "Qiming Zhu",
      "Hongyu Lin",
      "Yaojie Lu",
      "Xianpei Han",
      "Le Sun"
    ],
    "abstract": "Executing computer programs described in natural language has long been a\npursuit of computer science. With the advent of enhanced natural language\nunderstanding capabilities exhibited by large language models (LLMs), the path\ntoward this goal has been illuminated. In this paper, we seek to examine the\ncapacity of present-day LLMs to comprehend and execute algorithms outlined in\nnatural language. We established an algorithm test set sourced from\nIntroduction to Algorithm, a well-known textbook that contains many\nrepresentative widely-used algorithms. To systematically assess LLMs' code\nexecution abilities, we selected 30 algorithms, generated 300 random-sampled\ninstances in total, and evaluated whether popular LLMs can understand and\nexecute these algorithms. Our findings reveal that LLMs, notably GPT-4, can\neffectively execute programs described in natural language, as long as no heavy\nnumeric computation is involved. We believe our findings contribute to\nevaluating LLMs' code execution abilities and would encourage further\ninvestigation and application for the computation power of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.00795v2",
    "published_date": "2024-02-23 05:31:36 UTC",
    "updated_date": "2024-03-14 14:25:13 UTC"
  },
  {
    "arxiv_id": "2402.15102v2",
    "title": "Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding",
    "authors": [
      "Haoming Li",
      "Yusen Huo",
      "Shuai Dou",
      "Zhenzhe Zheng",
      "Zhilin Zhang",
      "Chuan Yu",
      "Jian Xu",
      "Fan Wu"
    ],
    "abstract": "In online advertising, advertisers participate in ad auctions to acquire ad\nopportunities, often by utilizing auto-bidding tools provided by demand-side\nplatforms (DSPs). The current auto-bidding algorithms typically employ\nreinforcement learning (RL). However, due to safety concerns, most RL-based\nauto-bidding policies are trained in simulation, leading to a performance\ndegradation when deployed in online environments. To narrow this gap, we can\ndeploy multiple auto-bidding agents in parallel to collect a large interaction\ndataset. Offline RL algorithms can then be utilized to train a new policy. The\ntrained policy can subsequently be deployed for further data collection,\nresulting in an iterative training framework, which we refer to as iterative\noffline RL. In this work, we identify the performance bottleneck of this\niterative offline RL framework, which originates from the ineffective\nexploration and exploitation caused by the inherent conservatism of offline RL\nalgorithms. To overcome this bottleneck, we propose Trajectory-wise Exploration\nand Exploitation (TEE), which introduces a novel data collecting and data\nutilization method for iterative offline RL from a trajectory perspective.\nFurthermore, to ensure the safety of online exploration while preserving the\ndataset quality for TEE, we propose Safe Exploration by Adaptive Action\nSelection (SEAS). Both offline experiments and real-world experiments on\nAlibaba display advertising platform demonstrate the effectiveness of our\nproposed method.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by The Web Conference 2024 (WWW'24) as an oral paper",
    "pdf_url": "http://arxiv.org/pdf/2402.15102v2",
    "published_date": "2024-02-23 05:20:23 UTC",
    "updated_date": "2024-04-08 09:33:10 UTC"
  },
  {
    "arxiv_id": "2402.15537v3",
    "title": "Evaluating the Performance of ChatGPT for Spam Email Detection",
    "authors": [
      "Shijing Si",
      "Yuwei Wu",
      "Le Tang",
      "Yugui Zhang",
      "Jedrek Wosik",
      "Qinliang Su"
    ],
    "abstract": "Email continues to be a pivotal and extensively utilized communication medium\nwithin professional and commercial domains. Nonetheless, the prevalence of spam\nemails poses a significant challenge for users, disrupting their daily routines\nand diminishing productivity. Consequently, accurately identifying and\nfiltering spam based on content has become crucial for cybersecurity. Recent\nadvancements in natural language processing, particularly with large language\nmodels like ChatGPT, have shown remarkable performance in tasks such as\nquestion answering and text generation. However, its potential in spam\nidentification remains underexplored. To fill in the gap, this study attempts\nto evaluate ChatGPT's capabilities for spam identification in both English and\nChinese email datasets. We employ ChatGPT for spam email detection using\nin-context learning, which requires a prompt instruction with (or without) a\nfew demonstrations. We also investigate how the number of demonstrations in the\nprompt affects the performance of ChatGPT. For comparison, we also implement\nfive popular benchmark methods, including naive Bayes, support vector machines\n(SVM), logistic regression (LR), feedforward dense neural networks (DNN), and\nBERT classifiers. Through extensive experiments, the performance of ChatGPT is\nsignificantly worse than deep supervised learning methods in the large English\ndataset, while it presents superior performance on the low-resourced Chinese\ndataset. This study provides insights into the potential and limitations of\nChatGPT for spam identification, highlighting its potential as a viable\nsolution for resource-constrained language domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 4 figures; Accepted by Pacific Journal of Optimization\n  (PJO)",
    "pdf_url": "http://arxiv.org/pdf/2402.15537v3",
    "published_date": "2024-02-23 04:52:08 UTC",
    "updated_date": "2025-02-12 17:59:14 UTC"
  },
  {
    "arxiv_id": "2402.15089v1",
    "title": "AttributionBench: How Hard is Automatic Attribution Evaluation?",
    "authors": [
      "Yifei Li",
      "Xiang Yue",
      "Zeyi Liao",
      "Huan Sun"
    ],
    "abstract": "Modern generative search engines enhance the reliability of large language\nmodel (LLM) responses by providing cited evidence. However, evaluating the\nanswer's attribution, i.e., whether every claim within the generated responses\nis fully supported by its cited evidence, remains an open problem. This\nverification, traditionally dependent on costly human evaluation, underscores\nthe urgent need for automatic attribution evaluation methods. To bridge the gap\nin the absence of standardized benchmarks for these methods, we present\nAttributionBench, a comprehensive benchmark compiled from various existing\nattribution datasets. Our extensive experiments on AttributionBench reveal the\nchallenges of automatic attribution evaluation, even for state-of-the-art LLMs.\nSpecifically, our findings show that even a fine-tuned GPT-3.5 only achieves\naround 80% macro-F1 under a binary classification formulation. A detailed\nanalysis of more than 300 error cases indicates that a majority of failures\nstem from the model's inability to process nuanced information, and the\ndiscrepancy between the information the model has access to and that human\nannotators do.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15089v1",
    "published_date": "2024-02-23 04:23:33 UTC",
    "updated_date": "2024-02-23 04:23:33 UTC"
  },
  {
    "arxiv_id": "2402.15083v2",
    "title": "Hands-Free VR",
    "authors": [
      "Jorge Askur Vazquez Fernandez",
      "Jae Joong Lee",
      "Santiago Andr√©s Serrano Vacca",
      "Alejandra Magana",
      "Radim Pesam",
      "Bedrich Benes",
      "Voicu Popescu"
    ],
    "abstract": "The paper introduces Hands-Free VR, a voice-based natural-language interface\nfor VR. The user gives a command using their voice, the speech audio data is\nconverted to text using a speech-to-text deep learning model that is fine-tuned\nfor robustness to word phonetic similarity and to spoken English accents, and\nthe text is mapped to an executable VR command using a large language model\nthat is robust to natural language diversity. Hands-Free VR was evaluated in a\ncontrolled within-subjects study (N = 22) that asked participants to find\nspecific objects and to place them in various configurations. In the control\ncondition participants used a conventional VR user interface to grab, carry,\nand position the objects using the handheld controllers. In the experimental\ncondition participants used Hands-Free VR. The results confirm that: (1)\nHands-Free VR is robust to spoken English accents, as for 20 of our\nparticipants English was not their first language, and to word phonetic\nsimilarity, correctly transcribing the voice command 96.71% of the time; (2)\nHands-Free VR is robust to natural language diversity, correctly mapping the\ntranscribed command to an executable command in 97.83% of the time; (3)\nHands-Free VR had a significant efficiency advantage over the conventional VR\ninterface in terms of task completion time, total viewpoint translation, total\nview direction rotation, and total left and right hand translations; (4)\nHands-Free VR received high user preference ratings in terms of ease of use,\nintuitiveness, ergonomics, reliability, and desirability.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "The first two authors contributed equally. Accepted VISIGRAPP@HUCAPP\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2402.15083v2",
    "published_date": "2024-02-23 04:02:23 UTC",
    "updated_date": "2024-12-18 23:11:48 UTC"
  },
  {
    "arxiv_id": "2402.15075v1",
    "title": "Stacking Factorizing Partitioned Expressions in Hybrid Bayesian Network Models",
    "authors": [
      "Peng Lin",
      "Martin Neil",
      "Norman Fenton"
    ],
    "abstract": "Hybrid Bayesian networks (HBN) contain complex conditional probabilistic\ndistributions (CPD) specified as partitioned expressions over discrete and\ncontinuous variables. The size of these CPDs grows exponentially with the\nnumber of parent nodes when using discrete inference, resulting in significant\ninefficiency. Normally, an effective way to reduce the CPD size is to use a\nbinary factorization (BF) algorithm to decompose the statistical or arithmetic\nfunctions in the CPD by factorizing the number of connected parent nodes to\nsets of size two. However, the BF algorithm was not designed to handle\npartitioned expressions. Hence, we propose a new algorithm called stacking\nfactorization (SF) to decompose the partitioned expressions. The SF algorithm\ncreates intermediate nodes to incrementally reconstruct the densities in the\noriginal partitioned expression, allowing no more than two continuous parent\nnodes to be connected to each child node in the resulting HBN. SF can be either\nused independently or combined with the BF algorithm. We show that the SF+BF\nalgorithm significantly reduces the CPD size and contributes to lowering the\ntree-width of a model, thus improving efficiency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15075v1",
    "published_date": "2024-02-23 03:33:06 UTC",
    "updated_date": "2024-02-23 03:33:06 UTC"
  },
  {
    "arxiv_id": "2403.00794v2",
    "title": "Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models",
    "authors": [
      "Zachary Horvitz",
      "Jingru Chen",
      "Rahul Aditya",
      "Harshvardhan Srivastava",
      "Robert West",
      "Zhou Yu",
      "Kathleen McKeown"
    ],
    "abstract": "Humor is a fundamental facet of human cognition and interaction. Yet, despite\nrecent advances in natural language processing, humor detection remains a\nchallenging task that is complicated by the scarcity of datasets that pair\nhumorous texts with similar non-humorous counterparts. In our work, we\ninvestigate whether large language models (LLMs), can generate synthetic data\nfor humor detection via editing texts. We benchmark LLMs on an existing human\ndataset and show that current LLMs display an impressive ability to 'unfun'\njokes, as judged by humans and as measured on the downstream task of humor\ndetection. We extend our approach to a code-mixed English-Hindi humor dataset,\nwhere we find that GPT-4's synthetic data is highly rated by bilingual\nannotators and provides challenging adversarial examples for humor classifiers.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.00794v2",
    "published_date": "2024-02-23 02:58:12 UTC",
    "updated_date": "2024-06-21 17:12:35 UTC"
  },
  {
    "arxiv_id": "2402.15057v1",
    "title": "On the Multi-turn Instruction Following for Conversational Web Agents",
    "authors": [
      "Yang Deng",
      "Xuan Zhang",
      "Wenxuan Zhang",
      "Yifei Yuan",
      "See-Kiong Ng",
      "Tat-Seng Chua"
    ],
    "abstract": "Web agents powered by Large Language Models (LLMs) have demonstrated\nremarkable abilities in planning and executing multi-step interactions within\ncomplex web-based environments, fulfilling a wide range of web navigation\ntasks. Despite these advancements, the potential for LLM-powered agents to\neffectively engage with sequential user instructions in real-world scenarios\nhas not been fully explored. In this work, we introduce a new task of\nConversational Web Navigation, which necessitates sophisticated interactions\nthat span multiple turns with both the users and the environment, supported by\na specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To\ntackle the limited context length of LLMs and the context-dependency issue of\nthe conversational tasks, we further propose a novel framework, named\nself-reflective memory-augmented planning (Self-MAP), which employs memory\nutilization and self-reflection techniques. Extensive experiments are conducted\nto benchmark the MT-Mind2Web dataset, and validate the effectiveness of the\nproposed method.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15057v1",
    "published_date": "2024-02-23 02:18:12 UTC",
    "updated_date": "2024-02-23 02:18:12 UTC"
  },
  {
    "arxiv_id": "2402.15055v2",
    "title": "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions",
    "authors": [
      "Clement Neo",
      "Shay B. Cohen",
      "Fazl Barez"
    ],
    "abstract": "Understanding the inner workings of large language models (LLMs) is crucial\nfor advancing their theoretical foundations and real-world applications. While\nthe attention mechanism and multi-layer perceptrons (MLPs) have been studied\nindependently, their interactions remain largely unexplored. This study\ninvestigates how attention heads and next-token neurons interact in LLMs to\npredict new words. We propose a methodology to identify next-token neurons,\nfind prompts that highly activate them, and determine the upstream attention\nheads responsible. We then generate and evaluate explanations for the activity\nof these attention heads in an automated manner. Our findings reveal that some\nattention heads recognize specific contexts relevant to predicting a token and\nactivate a downstream token-predicting neuron accordingly. This mechanism\nprovides a deeper understanding of how attention heads work with MLP neurons to\nperform next-token prediction. Our approach offers a foundation for further\nresearch into the intricate workings of LLMs and their impact on text\ngeneration and understanding.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2402.15055v2",
    "published_date": "2024-02-23 02:15:47 UTC",
    "updated_date": "2024-10-23 13:20:15 UTC"
  },
  {
    "arxiv_id": "2402.15052v2",
    "title": "ToMBench: Benchmarking Theory of Mind in Large Language Models",
    "authors": [
      "Zhuang Chen",
      "Jincenzi Wu",
      "Jinfeng Zhou",
      "Bosi Wen",
      "Guanqun Bi",
      "Gongyao Jiang",
      "Yaru Cao",
      "Mengting Hu",
      "Yunghwei Lai",
      "Zexuan Xiong",
      "Minlie Huang"
    ],
    "abstract": "Theory of Mind (ToM) is the cognitive capability to perceive and ascribe\nmental states to oneself and others. Recent research has sparked a debate over\nwhether large language models (LLMs) exhibit a form of ToM. However, existing\nToM evaluations are hindered by challenges such as constrained scope,\nsubjective judgment, and unintended contamination, yielding inadequate\nassessments. To address this gap, we introduce ToMBench with three key\ncharacteristics: a systematic evaluation framework encompassing 8 tasks and 31\nabilities in social cognition, a multiple-choice question format to support\nautomated and unbiased evaluation, and a build-from-scratch bilingual inventory\nto strictly avoid data leakage. Based on ToMBench, we conduct extensive\nexperiments to evaluate the ToM performance of 10 popular LLMs across tasks and\nabilities. We find that even the most advanced LLMs like GPT-4 lag behind human\nperformance by over 10% points, indicating that LLMs have not achieved a\nhuman-level theory of mind yet. Our aim with ToMBench is to enable an efficient\nand effective evaluation of LLMs' ToM capabilities, thereby facilitating the\ndevelopment of LLMs with inherent social intelligence.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.15052v2",
    "published_date": "2024-02-23 02:05:46 UTC",
    "updated_date": "2024-12-08 07:20:51 UTC"
  },
  {
    "arxiv_id": "2402.15048v2",
    "title": "Unlocking the Power of Large Language Models for Entity Alignment",
    "authors": [
      "Xuhui Jiang",
      "Yinghan Shen",
      "Zhichao Shi",
      "Chengjin Xu",
      "Wei Li",
      "Zixuan Li",
      "Jian Guo",
      "Huawei Shen",
      "Yuanzhuo Wang"
    ],
    "abstract": "Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG)\ndata, playing a crucial role in data-driven AI applications. Traditional EA\nmethods primarily rely on comparing entity embeddings, but their effectiveness\nis constrained by the limited input KG data and the capabilities of the\nrepresentation learning techniques. Against this backdrop, we introduce ChatEA,\nan innovative framework that incorporates large language models (LLMs) to\nimprove EA. To address the constraints of limited input KG data, ChatEA\nintroduces a KG-code translation module that translates KG structures into a\nformat understandable by LLMs, thereby allowing LLMs to utilize their extensive\nbackground knowledge to improve EA accuracy. To overcome the over-reliance on\nentity embedding comparisons, ChatEA implements a two-stage EA strategy that\ncapitalizes on LLMs' capability for multi-step reasoning in a dialogue format,\nthereby enhancing accuracy while preserving efficiency. Our experimental\nresults verify ChatEA's superior performance, highlighting LLMs' potential in\nfacilitating EA tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15048v2",
    "published_date": "2024-02-23 01:55:35 UTC",
    "updated_date": "2024-10-09 03:22:46 UTC"
  },
  {
    "arxiv_id": "2402.15043v2",
    "title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models",
    "authors": [
      "Zhuohao Yu",
      "Chang Gao",
      "Wenjin Yao",
      "Yidong Wang",
      "Wei Ye",
      "Jindong Wang",
      "Xing Xie",
      "Yue Zhang",
      "Shikun Zhang"
    ],
    "abstract": "Automatic evaluation methods for large language models (LLMs) are hindered by\ndata contamination, leading to inflated assessments of their effectiveness.\nExisting strategies, which aim to detect contaminated texts, focus on\nquantifying contamination status instead of accurately gauging model\nperformance. In this paper, we introduce KIEval, a Knowledge-grounded\nInteractive Evaluation framework, which incorporates an LLM-powered\n\"interactor\" role for the first time to accomplish a dynamic\ncontamination-resilient evaluation. Starting with a question in a conventional\nLLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically\ngenerated, multi-round, and knowledge-focused dialogues to determine whether a\nmodel's response is merely a recall of benchmark answers or demonstrates a deep\ncomprehension to apply knowledge in more complex conversations. Extensive\nexperiments on seven leading LLMs across five datasets validate KIEval's\neffectiveness and generalization. We also reveal that data contamination brings\nno contribution or even negative effect to models' real-world applicability and\nunderstanding, and existing contamination detection methods for LLMs can only\nidentify contamination in pre-training but not during supervised fine-tuning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2024 (main conference); 19 pages, 5 figures, 19\n  tables, code is available at: https://github.com/zhuohaoyu/KIEval",
    "pdf_url": "http://arxiv.org/pdf/2402.15043v2",
    "published_date": "2024-02-23 01:30:39 UTC",
    "updated_date": "2024-06-03 06:02:39 UTC"
  },
  {
    "arxiv_id": "2402.15038v2",
    "title": "Dynamics-Guided Diffusion Model for Sensor-less Robot Manipulator Design",
    "authors": [
      "Xiaomeng Xu",
      "Huy Ha",
      "Shuran Song"
    ],
    "abstract": "We present Dynamics-Guided Diffusion Model (DGDM), a data-driven framework\nfor generating task-specific manipulator designs without task-specific\ntraining. Given object shapes and task specifications, DGDM generates\nsensor-less manipulator designs that can blindly manipulate objects towards\ndesired motions and poses using an open-loop parallel motion. This framework 1)\nflexibly represents manipulation tasks as interaction profiles, 2) represents\nthe design space using a geometric diffusion model, and 3) efficiently searches\nthis design space using the gradients provided by a dynamics network trained\nwithout any task information. We evaluate DGDM on various manipulation tasks\nranging from shifting/rotating objects to converging objects to a specific\npose. Our generated designs outperform optimization-based and unguided\ndiffusion baselines relatively by 31.5% and 45.3% on average success rate. With\nthe ability to generate a new design within 0.8s, DGDM facilitates rapid design\niteration and enhances the adoption of data-driven approaches for robot\nmechanism design. Qualitative results are best viewed on our project website\nhttps://dgdm-robot.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.15038v2",
    "published_date": "2024-02-23 01:19:30 UTC",
    "updated_date": "2025-03-28 02:09:38 UTC"
  },
  {
    "arxiv_id": "2403.10534v1",
    "title": "VISREAS: Complex Visual Reasoning with Unanswerable Questions",
    "authors": [
      "Syeda Nahida Akter",
      "Sangwu Lee",
      "Yingshan Chang",
      "Yonatan Bisk",
      "Eric Nyberg"
    ],
    "abstract": "Verifying a question's validity before answering is crucial in real-world\napplications, where users may provide imperfect instructions. In this scenario,\nan ideal model should address the discrepancies in the query and convey them to\nthe users rather than generating the best possible answer. Addressing this\nrequirement, we introduce a new compositional visual question-answering\ndataset, VISREAS, that consists of answerable and unanswerable visual queries\nformulated by traversing and perturbing commonalities and differences among\nobjects, attributes, and relations. VISREAS contains 2.07M semantically diverse\nqueries generated automatically using Visual Genome scene graphs. The unique\nfeature of this task, validating question answerability with respect to an\nimage before answering, and the poor performance of state-of-the-art models\ninspired the design of a new modular baseline, LOGIC2VISION that reasons by\nproducing and executing pseudocode without any external modules to generate the\nanswer. LOGIC2VISION outperforms generative models in VISREAS (+4.82% over\nLLaVA-1.5; +12.23% over InstructBLIP) and achieves a significant gain in\nperformance against the classification models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 14 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.10534v1",
    "published_date": "2024-02-23 00:12:10 UTC",
    "updated_date": "2024-02-23 00:12:10 UTC"
  }
]
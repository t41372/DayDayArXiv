{
  "date": "2024-03-06",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-03-06 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于人工智能和机器学习领域的创新应用，包括大型语言模型（LLMs）的优化、多模态处理、生物学与医疗AI的融合，以及强化学习和图像生成等主题。其中，令人印象深刻的文章包括探讨LLMs在分析推理和生物系统建模中的潜力（如Pietro Liò参与的论文），以及多模态生成和毒性缓解的研究，这些工作可能推动AI在实际场景中的话题度；知名学者如Pietro Liò和Yuxin Chen的作品值得关注。\n\n下面，我将逐一简要概述今日论文，先优先讨论重要、相关或有话题度的文章（如AI生成、LLMs优化和医疗应用），对其他次要论文快速掠过。每个条目包括论文标题（中文 + 英文）、核心贡献和发现，保留关键学术术语。\n\n### 重点论文讨论\n\n**1. 理解生物学在人工智能时代（Understanding Biology in the Age of Artificial Intelligence）**  \n作者包括知名学者Pietro Liò等。该文分析AI（特别是机器学习模型）在生物科学中的应用，提出框架将ML用于蛋白结构预测和单细胞RNA测序，强调科学理解作为信息压缩、定性可解释性和依赖关系建模。该发现揭示ML可提升生物发现，但需克服障碍，如模型设计和知识推进。\n\n**2. 大型语言模型是否能进行分析推理（Can Large Language Models do Analytical Reasoning?）**  \nYuxin Chen等作者参与。该文评估LLMs（如GPT-4）在体育分析推理中的表现，使用divide-and-conquer和Chain of Thought（CoT）策略。发现GPT-4在NFL得分预测中表现强于NBA，任务复杂度受上下文长度、信息密度影响。该研究突显LLMs的局限性和改进方向，对AI推理话题有重要启发。\n\n**3. PromptCharm: 通过多模态提示和优化进行文本到图像生成（PromptCharm: Text-to-Image Generation through Multi-modal Prompting and Refinement）**  \n该文提出PromptCharm系统，利用多模态提示优化Stable Diffusion模型，支持图像生成和注意力可视化。贡献包括自动提示精炼和用户反馈循环，用户研究显示其提升图像质量和用户满意度。该发现推动AI在创意领域的交互式应用。\n\n**4. 从一个到多个：扩展语言模型毒性缓解的范围（From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models）**  \n作者探讨多语言LLMs的毒性缓解，使用翻译数据和检索增强技术。发现微调方法在多语言场景下有效，提升毒性检测准确性。该文强调AI在多语言环境中的伦理挑战，对LLMs的安全性有实际启发。\n\n**5. 停止回归：通过分类训练值函数实现可扩展深度强化学习（Stop Regressing: Training Value Functions via Classification for Scalable Deep RL）**  \n该文提出用分类损失替换回归损失训练值函数，提升深度强化学习模型的扩展性。发现该方法在Atari游戏和机器人任务中显著提高性能（如准确率提升），并解决噪声目标问题。该创新优化了RL的训练效率。\n\n**6. 停止训练：使用基于分类的值函数训练进行可扩展深度强化学习（Stop Regressing: Training Value Functions via Classification for Scalable Deep RL）**  \n（与上文重复，这里略过详细讨论，但其与强化学习相关，可视为扩展。）\n\n**7. 蛋白质序列生成的扩散模型（Diffusion on language model encodings for protein sequence generation）**  \n该文使用连续扩散模型在LLM编码上生成蛋白质序列，适用于多种蛋白编码器。发现模型在生成多样性和条件任务（如基序填充）中表现优异，超越离散扩散方法。该研究桥接AI与生物学，潜力巨大。\n\n**8. 自然语言处理在专利领域：调查（Natural Language Processing in the Patent Domain: A Survey）**  \n该文调查NLP在专利领域的应用，涵盖文本分析和多模态任务。贡献包括映射专利特点与LLMs的使用，强调复杂语言框架的挑战。该发现为专利AI应用提供指导。\n\n**9. 认知类型项目：将排版映射到认知（The Cognitive Type Project -- Mapping Typography to Cognition）**  \n提出计算工具设计具有认知属性的字体，焦点在易读性和记忆性。发现字体设计影响广告点击率和阅读水平，但计算生成字体面临挑战。该文结合认知科学和AI，应用潜力有限。\n\n其他论文如强化学习优化（e.g., \"Improving Adversarial Training using Vulnerability-Aware Perturbation Budget\"，提出加权扰动方法提升鲁棒性）和图像生成（e.g., \"Latent Dataset Distillation with Diffusion Models\"，改进扩散模型效率）等，贡献在于算法改进，但非核心话题，故快速掠过：这些工作提升了模型性能，但细节较技术化，对一般读者影响不大。\n\n总体而言，今天的论文突显AI在多领域融合的潜力，尤其是LLMs的扩展和优化，但也暴露了挑战如毒性和泛化能力。感兴趣的读者可关注生物AI和多模态生成方向。更多细节请查阅arXiv！",
  "papers": [
    {
      "arxiv_id": "2403.04106v1",
      "title": "Understanding Biology in the Age of Artificial Intelligence",
      "title_zh": "翻译失败",
      "authors": [
        "Elsa Lawrence",
        "Adham El-Shazly",
        "Srijit Seal",
        "Chaitanya K Joshi",
        "Pietro Liò",
        "Shantanu Singh",
        "Andreas Bender",
        "Pietro Sormanni",
        "Matthew Greenig"
      ],
      "abstract": "Modern life sciences research is increasingly relying on artificial\nintelligence approaches to model biological systems, primarily centered around\nthe use of machine learning (ML) models. Although ML is undeniably useful for\nidentifying patterns in large, complex data sets, its widespread application in\nbiological sciences represents a significant deviation from traditional methods\nof scientific inquiry. As such, the interplay between these models and\nscientific understanding in biology is a topic with important implications for\nthe future of scientific research, yet it is a subject that has received little\nattention. Here, we draw from an epistemological toolkit to contextualize\nrecent applications of ML in biological sciences under modern philosophical\ntheories of understanding, identifying general principles that can guide the\ndesign and application of ML systems to model biological phenomena and advance\nscientific knowledge. We propose that conceptions of scientific understanding\nas information compression, qualitative intelligibility, and dependency\nrelation modelling provide a useful framework for interpreting ML-mediated\nunderstanding of biological systems. Through a detailed analysis of two key\napplication areas of ML in modern biological research - protein structure\nprediction and single cell RNA-sequencing - we explore how these features have\nthus far enabled ML systems to advance scientific understanding of their target\nphenomena, how they may guide the development of future ML models, and the key\nobstacles that remain in preventing ML from achieving its potential as a tool\nfor biological discovery. Consideration of the epistemological features of ML\napplications in biology will improve the prospects of these methods to solve\nimportant problems and advance scientific understanding of living systems.",
      "tldr_zh": "这篇论文探讨了人工智能（AI），特别是机器学习（ML），在生物科学中的应用及其与传统科学方法的差异。作者从认识论角度提出三个框架——信息压缩、定性可理解性和依赖关系建模——来指导ML系统设计，以推进对生物系统的科学理解。通过分析ML在蛋白结构预测和单细胞RNA-sequencing中的实际应用，论文展示了这些方法如何提升知识发现，同时指出了关键障碍，如模型局限性。最终，论文强调，考虑ML的认识论特征有助于其更好地解决生物学问题并促进对生命系统的理解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04106v1",
      "published_date": "2024-03-06 23:20:34 UTC",
      "updated_date": "2024-03-06 23:20:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:54:18.362150"
    },
    {
      "arxiv_id": "2403.04105v3",
      "title": "Natural Language Processing in the Patent Domain: A Survey",
      "title_zh": "专利领域的自然语言处理：综述",
      "authors": [
        "Lekang Jiang",
        "Stephan Goetz"
      ],
      "abstract": "Patents, which encapsulate crucial technical and legal information in text\nform and referenced drawings, present a rich domain for natural language\nprocessing (NLP) applications. As NLP technologies evolve, large language\nmodels (LLMs) have demonstrated outstanding capabilities in general text\nprocessing and generation tasks. However, the application of LLMs in the patent\ndomain remains under-explored and under-developed due to the complexity of\npatents, particularly their language and legal framework. Understanding the\nunique characteristics of patent documents and related research in the patent\ndomain becomes essential for researchers to apply these tools effectively.\nTherefore, this paper aims to equip NLP researchers with the essential\nknowledge to navigate this complex domain efficiently. We introduce the\nrelevant fundamental aspects of patents to provide solid background\ninformation. In addition, we systematically break down the structural and\nlinguistic characteristics unique to patents and map out how NLP can be\nleveraged for patent analysis and generation. Moreover, we demonstrate the\nspectrum of text-based and multimodal patent-related tasks, including nine\npatent analysis and four patent generation tasks.",
      "tldr_zh": "这篇调查论文探讨了自然语言处理（NLP）在专利领域的应用，强调专利文档的复杂性（如其语言和法律框架）导致大型语言模型（LLMs）在该领域的开发仍处于起步阶段。论文首先介绍了专利的基本方面、结构和语言特点，并系统地阐述了如何利用 NLP 进行专利分析和生成任务。最终，它列出了九个专利分析任务（如文本检索和分类）和四个专利生成任务（如摘要生成），为 NLP 研究者提供导航这一领域的实用指南。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Published in Artificial Intelligence Review",
      "pdf_url": "http://arxiv.org/pdf/2403.04105v3",
      "published_date": "2024-03-06 23:17:16 UTC",
      "updated_date": "2025-04-23 16:48:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:54:30.282314"
    },
    {
      "arxiv_id": "2403.04087v1",
      "title": "The Cognitive Type Project -- Mapping Typography to Cognition",
      "title_zh": "翻译失败",
      "authors": [
        "Nik Bear Brown"
      ],
      "abstract": "The Cognitive Type Project is focused on developing computational tools to\nenable the design of typefaces with varying cognitive properties. This\ninitiative aims to empower typographers to craft fonts that enhance\nclick-through rates for online ads, improve reading levels in children's books,\nenable dyslexics to create personalized type, or provide insights into customer\nreactions to textual content in media. A significant challenge in research\nrelated to mapping typography to cognition is the creation of thousands of\ntypefaces with minor variations, a process that is both labor-intensive and\nrequires the expertise of skilled typographers. Cognitive science research\nhighlights that the design and form of letters, along with the text's overall\nlayout, are crucial in determining the ease of reading and other cognitive\nproperties of type such as perceived beauty and memorability. These factors\naffect not only the legibility and clarity of information presentation but also\nthe likability of a typeface.",
      "tldr_zh": "该研究提出“Cognitive Type Project”，旨在开发计算工具来设计具有不同认知属性的字体，帮助排版设计师提升在线广告的点击率、改善儿童书籍的阅读水平、为失读症患者提供个性化字体，并分析客户对文本内容的反应。项目主要解决手动创建数千种字体变体的劳动密集型挑战，通过自动化工具结合认知科学原理，聚焦字母设计、文本布局等因素来增强字体的易读性、感知美感和记忆性。最终，该框架有望提高信息呈现的清晰度和用户喜爱度，为字体设计提供更高效的见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04087v1",
      "published_date": "2024-03-06 22:32:49 UTC",
      "updated_date": "2024-03-06 22:32:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:54:42.279883"
    },
    {
      "arxiv_id": "2403.04073v1",
      "title": "Semi-Supervised Dialogue Abstractive Summarization via High-Quality Pseudolabel Selection",
      "title_zh": "翻译失败",
      "authors": [
        "Jianfeng He",
        "Hang Su",
        "Jason Cai",
        "Igor Shalyminov",
        "Hwanjun Song",
        "Saab Mansour"
      ],
      "abstract": "Semi-supervised dialogue summarization (SSDS) leverages model-generated\nsummaries to reduce reliance on human-labeled data and improve the performance\nof summarization models. While addressing label noise, previous works on\nsemi-supervised learning primarily focus on natural language understanding\ntasks, assuming each sample has a unique label. However, these methods are not\ndirectly applicable to SSDS, as it is a generative task, and each dialogue can\nbe summarized in different ways. In this work, we propose a novel scoring\napproach, SiCF, which encapsulates three primary dimensions of summarization\nmodel quality: Semantic invariance (indicative of model confidence), Coverage\n(factual recall), and Faithfulness (factual precision). Using the SiCF score,\nwe select unlabeled dialogues with high-quality generated summaries to train\nsummarization models. Comprehensive experiments on three public datasets\ndemonstrate the effectiveness of SiCF scores in uncertainty estimation and\nsemi-supervised learning for dialogue summarization tasks. Our code is\navailable at \\url{https://github.com/amazon-science/summarization-sicf-score}.",
      "tldr_zh": "该研究针对半监督对话摘要生成（Semi-Supervised Dialogue Summarization, SSDS）任务，提出了一种新型评分方法SiCF，以解决生成式任务中伪标签质量问题。SiCF基于三个关键维度——Semantic invariance（语义不变性，表示模型置信度）、Coverage（覆盖率，事实召回）和Faithfulness（忠实度，事实精确度）——来评估和选择高质量的模型生成摘要。利用SiCF得分，研究者从未标注对话中筛选出可靠样本，用于训练摘要模型。在三个公共数据集上的全面实验中，SiCF证明了其在不确定性估计和半监督学习方面的有效性，提高了对话摘要的性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.04073v1",
      "published_date": "2024-03-06 22:06:23 UTC",
      "updated_date": "2024-03-06 22:06:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:54:55.280250"
    },
    {
      "arxiv_id": "2403.04072v1",
      "title": "Forecasting and Mitigating Disruptions in Public Bus Transit Services",
      "title_zh": "预测和缓解公共巴士交通",
      "authors": [
        "Chaeeun Han",
        "Jose Paolo Talusan",
        "Dan Freudberg",
        "Ayan Mukhopadhyay",
        "Abhishek Dubey",
        "Aron Laszka"
      ],
      "abstract": "Public transportation systems often suffer from unexpected fluctuations in\ndemand and disruptions, such as mechanical failures and medical emergencies.\nThese fluctuations and disruptions lead to delays and overcrowding, which are\ndetrimental to the passengers' experience and to the overall performance of the\ntransit service. To proactively mitigate such events, many transit agencies\nstation substitute (reserve) vehicles throughout their service areas, which\nthey can dispatch to augment or replace vehicles on routes that suffer\novercrowding or disruption. However, determining the optimal locations where\nsubstitute vehicles should be stationed is a challenging problem due to the\ninherent randomness of disruptions and due to the combinatorial nature of\nselecting locations across a city. In collaboration with the transit agency of\nNashville, TN, we address this problem by introducing data-driven statistical\nand machine-learning models for forecasting disruptions and an effective\nrandomized local-search algorithm for selecting locations where substitute\nvehicles are to be stationed. Our research demonstrates promising results in\nproactive disruption management, offering a practical and easily implementable\nsolution for transit agencies to enhance the reliability of their services. Our\nresults resonate beyond mere operational efficiency: by advancing proactive\nstrategies, our approach fosters more resilient and accessible public\ntransportation, contributing to equitable urban mobility and ultimately\nbenefiting the communities that rely on public transportation the most.",
      "tldr_zh": "该研究针对公共交通系统的需求波动和中断（如机械故障和医疗紧急情况）导致的延误与拥挤问题，提出了一种数据驱动的方法，包括统计和 machine-learning 模型用于预测中断，以及一个随机局部搜索（randomized local-search）算法来优化备用车辆的放置位置。研究与纳什维尔交通局合作，通过实验验证了这一方法的有效性，显著提升了服务可靠性。最终，该方案不仅提高了运营效率，还促进了公平的城市流动性和社区福祉，提供了一个实用易实施的解决方案。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04072v1",
      "published_date": "2024-03-06 22:06:21 UTC",
      "updated_date": "2024-03-06 22:06:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:55:07.587838"
    },
    {
      "arxiv_id": "2403.04071v1",
      "title": "On-device Self-supervised Learning of Visual Perception Tasks aboard Hardware-limited Nano-quadrotors",
      "title_zh": "翻译失败",
      "authors": [
        "Elia Cereda",
        "Manuele Rusci",
        "Alessandro Giusti",
        "Daniele Palossi"
      ],
      "abstract": "Sub-\\SI{50}{\\gram} nano-drones are gaining momentum in both academia and\nindustry. Their most compelling applications rely on onboard deep learning\nmodels for perception despite severe hardware constraints (\\ie\nsub-\\SI{100}{\\milli\\watt} processor). When deployed in unknown environments not\nrepresented in the training data, these models often underperform due to domain\nshift. To cope with this fundamental problem, we propose, for the first time,\non-device learning aboard nano-drones, where the first part of the in-field\nmission is dedicated to self-supervised fine-tuning of a pre-trained\nconvolutional neural network (CNN). Leveraging a real-world vision-based\nregression task, we thoroughly explore performance-cost trade-offs of the\nfine-tuning phase along three axes: \\textit{i}) dataset size (more data\nincreases the regression performance but requires more memory and longer\ncomputation); \\textit{ii}) methodologies (\\eg fine-tuning all model parameters\nvs. only a subset); and \\textit{iii}) self-supervision strategy. Our approach\ndemonstrates an improvement in mean absolute error up to 30\\% compared to the\npre-trained baseline, requiring only \\SI{22}{\\second} fine-tuning on an\nultra-low-power GWT GAP9 System-on-Chip. Addressing the domain shift problem\nvia on-device learning aboard nano-drones not only marks a novel result for\nhardware-limited robots but lays the ground for more general advancements for\nthe entire robotics community.",
      "tldr_zh": "本文提出了一种在硬件受限的纳米无人机（sub-50g nano-quadrotors）上进行板载自监督学习（on-device self-supervised learning）的框架，以解决深度学习模型在未知环境中的领域偏移问题。研究通过微调预训练的卷积神经网络（CNN），探索了数据集大小、微调方法（如全参数或部分参数微调）和自监督策略的性能-成本权衡。实验结果显示，在视觉回归任务上，该方法使均绝对误差（MAE）改善高达30%，且微调时间仅需22秒，使用GWT GAP9 System-on-Chip。该创新不仅适用于硬件有限的机器人，还为更广泛的机器人社区提供了新进展。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "\\c{opyright} 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works",
      "pdf_url": "http://arxiv.org/pdf/2403.04071v1",
      "published_date": "2024-03-06 22:04:14 UTC",
      "updated_date": "2024-03-06 22:04:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:55:20.218023"
    },
    {
      "arxiv_id": "2403.04070v1",
      "title": "Improving Adversarial Training using Vulnerability-Aware Perturbation Budget",
      "title_zh": "使用漏洞感知扰动预算改进对抗训练",
      "authors": [
        "Olukorede Fakorede",
        "Modeste Atsague",
        "Jin Tian"
      ],
      "abstract": "Adversarial Training (AT) effectively improves the robustness of Deep Neural\nNetworks (DNNs) to adversarial attacks. Generally, AT involves training DNN\nmodels with adversarial examples obtained within a pre-defined, fixed\nperturbation bound. Notably, individual natural examples from which these\nadversarial examples are crafted exhibit varying degrees of intrinsic\nvulnerabilities, and as such, crafting adversarial examples with fixed\nperturbation radius for all instances may not sufficiently unleash the potency\nof AT. Motivated by this observation, we propose two simple, computationally\ncheap vulnerability-aware reweighting functions for assigning perturbation\nbounds to adversarial examples used for AT, named Margin-Weighted Perturbation\nBudget (MWPB) and Standard-Deviation-Weighted Perturbation Budget (SDWPB). The\nproposed methods assign perturbation radii to individual adversarial samples\nbased on the vulnerability of their corresponding natural examples.\nExperimental results show that the proposed methods yield genuine improvements\nin the robustness of AT algorithms against various adversarial attacks.",
      "tldr_zh": "该研究针对传统对抗训练(AT)中固定扰动边界的问题，提出了一种基于样本脆弱性的动态分配方法，以提升Deep Neural Networks (DNNs)的鲁棒性。具体而言，作者引入了Margin-Weighted Perturbation Budget (MWPB)和Standard-Deviation-Weighted Perturbation Budget (SDWPB)两种再加权函数，根据自然样本的内在脆弱性（如边缘值或标准差）为每个对抗样本分配适当的扰动边界。实验结果表明，这些方法显著提高了AT算法对各种对抗攻击的抵抗力，实现了对抗训练的有效优化。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.04070v1",
      "published_date": "2024-03-06 21:50:52 UTC",
      "updated_date": "2024-03-06 21:50:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:55:30.259215"
    },
    {
      "arxiv_id": "2403.04036v1",
      "title": "Unsupervised Contrastive Learning for Robust RF Device Fingerprinting Under Time-Domain Shift",
      "title_zh": "翻译失败",
      "authors": [
        "Jun Chen",
        "Weng-Keen Wong",
        "Bechir Hamdaoui"
      ],
      "abstract": "Radio Frequency (RF) device fingerprinting has been recognized as a potential\ntechnology for enabling automated wireless device identification and\nclassification. However, it faces a key challenge due to the domain shift that\ncould arise from variations in the channel conditions and environmental\nsettings, potentially degrading the accuracy of RF-based device classification\nwhen testing and training data is collected in different domains. This paper\nintroduces a novel solution that leverages contrastive learning to mitigate\nthis domain shift problem. Contrastive learning, a state-of-the-art\nself-supervised learning approach from deep learning, learns a distance metric\nsuch that positive pairs are closer (i.e. more similar) in the learned metric\nspace than negative pairs. When applied to RF fingerprinting, our model treats\nRF signals from the same transmission as positive pairs and those from\ndifferent transmissions as negative pairs. Through experiments on wireless and\nwired RF datasets collected over several days, we demonstrate that our\ncontrastive learning approach captures domain-invariant features, diminishing\nthe effects of domain-specific variations. Our results show large and\nconsistent improvements in accuracy (10.8\\% to 27.8\\%) over baseline models,\nthus underscoring the effectiveness of contrastive learning in improving device\nclassification under domain shift.",
      "tldr_zh": "本研究针对 RF 设备指纹识别在时间域偏移下（如通道条件和环境变化）导致的分类准确率下降问题，提出了一种基于无监督对比学习的鲁棒解决方案。方法通过对比学习训练一个距离度量模型，将来自同一传输的 RF 信号视为正样本对、不同传输的视为负样本对，从而捕获领域不变特征并减少领域特定变异的影响。在多个无线和有线 RF 数据集上的实验中，该方法比基线模型准确率提高了 10.8% 到 27.8%，证明了其在提升设备分类鲁棒性方面的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 5 figures, accepted by 2024 IEEE International Conference on\n  Communications (ICC)",
      "pdf_url": "http://arxiv.org/pdf/2403.04036v1",
      "published_date": "2024-03-06 20:33:55 UTC",
      "updated_date": "2024-03-06 20:33:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:55:42.196142"
    },
    {
      "arxiv_id": "2403.04035v2",
      "title": "Personalizing explanations of AI-driven hints to users' cognitive abilities: an empirical evaluation",
      "title_zh": "针对用户认知能力的AI驱动提示解释个性化：一个实证评估",
      "authors": [
        "Vedant Bahel",
        "Harshinee Sriram",
        "Cristina Conati"
      ],
      "abstract": "We investigate personalizing the explanations that an Intelligent Tutoring\nSystem generates to justify the hints it provides to students to foster their\nlearning. The personalization targets students with low levels of two traits,\nNeed for Cognition and Conscientiousness, and aims to enhance these students'\nengagement with the explanations, based on prior findings that these students\ndo not naturally engage with the explanations but they would benefit from them\nif they do. To evaluate the effectiveness of the personalization, we conducted\na user study where we found that our proposed personalization significantly\nincreases our target users' interaction with the hint explanations, their\nunderstanding of the hints and their learning. Hence, this work provides\nvaluable insights into effectively personalizing AI-driven explanations for\ncognitively demanding tasks such as learning.",
      "tldr_zh": "这篇论文探讨了针对智能辅导系统（Intelligent Tutoring System）中AI驱动提示解释的个性化策略，针对低Need for Cognition和Conscientiousness学生的认知特质进行调整，以提升他们的解释互动。研究通过用户实验评估了这一方法，结果显示个性化显著提高了目标用户的解释互动、提示理解和学习效果。该工作为AI驱动解释的个性化提供了重要见解，适用于认知 demanding的任务如教育学习。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04035v2",
      "published_date": "2024-03-06 20:25:04 UTC",
      "updated_date": "2024-03-09 02:47:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:55:54.651121"
    },
    {
      "arxiv_id": "2403.04033v1",
      "title": "Online Learning with Unknown Constraints",
      "title_zh": "未知约束下的在线学习",
      "authors": [
        "Karthik Sridharan",
        "Seung Won Wilson Yoo"
      ],
      "abstract": "We consider the problem of online learning where the sequence of actions\nplayed by the learner must adhere to an unknown safety constraint at every\nround. The goal is to minimize regret with respect to the best safe action in\nhindsight while simultaneously satisfying the safety constraint with high\nprobability on each round. We provide a general meta-algorithm that leverages\nan online regression oracle to estimate the unknown safety constraint, and\nconverts the predictions of an online learning oracle to predictions that\nadhere to the unknown safety constraint. On the theoretical side, our\nalgorithm's regret can be bounded by the regret of the online regression and\nonline learning oracles, the eluder dimension of the model class containing the\nunknown safety constraint, and a novel complexity measure that captures the\ndifficulty of safe learning. We complement our result with an asymptotic lower\nbound that shows that the aforementioned complexity measure is necessary. When\nthe constraints are linear, we instantiate our result to provide a concrete\nalgorithm with $\\sqrt{T}$ regret using a scaling transformation that balances\noptimistic exploration with pessimistic constraint satisfaction.",
      "tldr_zh": "本研究探讨了在线学习问题，其中学习者的动作序列需遵守未知的安全约束，同时目标是最小化相对于事后最佳安全动作的 regret，并以高概率在每轮满足约束。作者提出一个通用元算法，使用 online regression oracle 估计未知约束，并将 online learning oracle 的预测转换为符合约束的预测。理论分析显示，该算法的 regret 可由 online regression 和 online learning oracles 的 regret、模型类的 eluder dimension 以及一个新的复杂性度量来界定，后者捕捉了安全学习的难度；此外，提供了一个渐近下界证明该度量必要性。对于线性约束，该算法通过缩放变换实现 √T regret，平衡了乐观探索与悲观约束满足。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04033v1",
      "published_date": "2024-03-06 20:23:59 UTC",
      "updated_date": "2024-03-06 20:23:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:56:07.451940"
    },
    {
      "arxiv_id": "2403.04031v1",
      "title": "Can Large Language Models do Analytical Reasoning?",
      "title_zh": "大型语言模型能进行分析推理吗？",
      "authors": [
        "Yebowen Hu",
        "Kaiqiang Song",
        "Sangwoo Cho",
        "Xiaoyang Wang",
        "Hassan Foroosh",
        "Dong Yu",
        "Fei Liu"
      ],
      "abstract": "This paper explores the cutting-edge Large Language Model with analytical\nreasoning on sports. Our analytical reasoning embodies the tasks of letting\nlarge language models count how many points each team scores in a quarter in\nthe NBA and NFL games. Our major discoveries are in two folds. Firstly, we find\namong all the models we employed, GPT-4 stands out in effectiveness, followed\nby Claude-2.1, with GPT-3.5, Gemini-Pro, and Llama-2-70b lagging behind.\nSpecifically, we compare three different prompting techniques and a\ndivide-and-conquer approach, we find that the latter was the most effective.\nOur divide-and-conquer approach breaks down play-by-play data into smaller,\nmore manageable segments, solves each piece individually, and then aggregates\nthem together. Besides the divide-and-conquer approach, we also explore the\nChain of Thought (CoT) strategy, which markedly improves outcomes for certain\nmodels, notably GPT-4 and Claude-2.1, with their accuracy rates increasing\nsignificantly. However, the CoT strategy has negligible or even detrimental\neffects on the performance of other models like GPT-3.5 and Gemini-Pro.\nSecondly, to our surprise, we observe that most models, including GPT-4,\nstruggle to accurately count the total scores for NBA quarters despite showing\nstrong performance in counting NFL quarter scores. This leads us to further\ninvestigate the factors that impact the complexity of analytical reasoning\ntasks with extensive experiments, through which we conclude that task\ncomplexity depends on the length of context, the information density, and the\npresence of related information. Our research provides valuable insights into\nthe complexity of analytical reasoning tasks and potential directions for\ndeveloping future large language models.",
      "tldr_zh": "这篇论文探讨了大型语言模型（Large Language Models, LLMs）在体育分析推理任务中的表现，具体通过计算 NBA 和 NFL 比赛中每个季度球队得分来评估模型能力。研究比较了多种模型，发现 GPT-4 最有效，其次是 Claude-2.1，而 GPT-3.5、Gemini-Pro 和 Llama-2-70b 表现较差；同时，divide-and-conquer 方法（将数据分解为小段逐个处理再聚合）和 Chain of Thought (CoT) 策略对某些模型如 GPT-4 显著提高了准确率，但对其他模型影响有限。论文进一步揭示，模型在 NBA 任务上挣扎而 NFL 上表现较好，任务复杂度主要取决于上下文长度、信息密度和相关信息的存在，为未来 LLMs 的开发提供了宝贵启示。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04031v1",
      "published_date": "2024-03-06 20:22:08 UTC",
      "updated_date": "2024-03-06 20:22:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:56:23.309066"
    },
    {
      "arxiv_id": "2403.04017v1",
      "title": "Learning Guided Automated Reasoning: A Brief Survey",
      "title_zh": "学习引导的自动化推理：简要综述",
      "authors": [
        "Lasse Blaauwbroek",
        "David Cerna",
        "Thibault Gauthier",
        "Jan Jakubův",
        "Cezary Kaliszyk",
        "Martin Suda",
        "Josef Urban"
      ],
      "abstract": "Automated theorem provers and formal proof assistants are general reasoning\nsystems that are in theory capable of proving arbitrarily hard theorems, thus\nsolving arbitrary problems reducible to mathematics and logical reasoning. In\npractice, such systems however face large combinatorial explosion, and\ntherefore include many heuristics and choice points that considerably influence\ntheir performance. This is an opportunity for trained machine learning\npredictors, which can guide the work of such reasoning systems. Conversely,\ndeductive search supported by the notion of logically valid proof allows one to\ntrain machine learning systems on large reasoning corpora. Such bodies of proof\nare usually correct by construction and when combined with more and more\nprecise trained guidance they can be boostrapped into very large corpora, with\nincreasingly long reasoning chains and possibly novel proof ideas. In this\npaper we provide an overview of several automated reasoning and theorem proving\ndomains and the learning and AI methods that have been so far developed for\nthem. These include premise selection, proof guidance in several settings, AI\nsystems and feedback loops iterating between reasoning and learning, and\nsymbolic classification problems.",
      "tldr_zh": "这篇论文概述了机器学习如何指导自动定理证明器(Automated theorem provers)和形式证明助手(Formal proof assistants)，以应对组合爆炸问题并提升推理性能。论文强调，通过训练的机器学习预测器，可以优化前提选择(Preme selection)和证明指导(Proof guidance)，并利用演绎搜索在大型推理语料上训练模型。作者探讨了AI系统和反馈循环，这些机制能迭代强化推理过程，形成更大的证明语料，并可能引入新颖的证明想法，最终扩展到符号分类(Symbolic classification)等问题。总的来说，该调查为机器学习与逻辑推理的融合提供了关键见解。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.LO",
        "cs.NE",
        "cs.SC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04017v1",
      "published_date": "2024-03-06 19:59:17 UTC",
      "updated_date": "2024-03-06 19:59:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:56:31.866701"
    },
    {
      "arxiv_id": "2403.04015v1",
      "title": "Knockoff-Guided Feature Selection via A Single Pre-trained Reinforced Agent",
      "title_zh": "翻译失败",
      "authors": [
        "Xinyuan Wang",
        "Dongjie Wang",
        "Wangyang Ying",
        "Rui Xie",
        "Haifeng Chen",
        "Yanjie Fu"
      ],
      "abstract": "Feature selection prepares the AI-readiness of data by eliminating redundant\nfeatures. Prior research falls into two primary categories: i) Supervised\nFeature Selection, which identifies the optimal feature subset based on their\nrelevance to the target variable; ii) Unsupervised Feature Selection, which\nreduces the feature space dimensionality by capturing the essential information\nwithin the feature set instead of using target variable. However, SFS\napproaches suffer from time-consuming processes and limited generalizability\ndue to the dependence on the target variable and downstream ML tasks. UFS\nmethods are constrained by the deducted feature space is latent and\nuntraceable. To address these challenges, we introduce an innovative framework\nfor feature selection, which is guided by knockoff features and optimized\nthrough reinforcement learning, to identify the optimal and effective feature\nsubset. In detail, our method involves generating \"knockoff\" features that\nreplicate the distribution and characteristics of the original features but are\nindependent of the target variable. Each feature is then assigned a pseudo\nlabel based on its correlation with all the knockoff features, serving as a\nnovel metric for feature evaluation. Our approach utilizes these pseudo labels\nto guide the feature selection process in 3 novel ways, optimized by a single\nreinforced agent: 1). A deep Q-network, pre-trained with the original features\nand their corresponding pseudo labels, is employed to improve the efficacy of\nthe exploration process in feature selection. 2). We introduce unsupervised\nrewards to evaluate the feature subset quality based on the pseudo labels and\nthe feature space reconstruction loss to reduce dependencies on the target\nvariable. 3). A new {\\epsilon}-greedy strategy is used, incorporating insights\nfrom the pseudo labels to make the feature selection process more effective.",
      "tldr_zh": "该论文提出了一种基于 Knockoff features 的创新特征选择框架，通过一个单一预训练的强化学习代理来优化过程，旨在解决传统监督特征选择（SFS）和无监督特征选择（UFS）方法的局限性，如耗时问题和特征空间的可追踪性。方法包括生成 Knockoff features 以模仿原特征分布但独立于目标变量，然后为每个特征分配伪标签（基于其与 Knockoff features 的相关性），并以此指导特征选择。核心贡献包括使用 Deep Q-network 提升探索效率、引入无监督奖励（如伪标签和特征空间重建损失）减少对目标变量的依赖，以及改进的 ε-greedy 策略，使特征选择更高效和泛化性强。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04015v1",
      "published_date": "2024-03-06 19:58:19 UTC",
      "updated_date": "2024-03-06 19:58:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:56:44.556544"
    },
    {
      "arxiv_id": "2403.04014v1",
      "title": "PromptCharm: Text-to-Image Generation through Multi-modal Prompting and Refinement",
      "title_zh": "PromptCharm：通过多模态提示和精炼的文本到图像",
      "authors": [
        "Zhijie Wang",
        "Yuheng Huang",
        "Da Song",
        "Lei Ma",
        "Tianyi Zhang"
      ],
      "abstract": "The recent advancements in Generative AI have significantly advanced the\nfield of text-to-image generation. The state-of-the-art text-to-image model,\nStable Diffusion, is now capable of synthesizing high-quality images with a\nstrong sense of aesthetics. Crafting text prompts that align with the model's\ninterpretation and the user's intent thus becomes crucial. However, prompting\nremains challenging for novice users due to the complexity of the stable\ndiffusion model and the non-trivial efforts required for iteratively editing\nand refining the text prompts. To address these challenges, we propose\nPromptCharm, a mixed-initiative system that facilitates text-to-image creation\nthrough multi-modal prompt engineering and refinement. To assist novice users\nin prompting, PromptCharm first automatically refines and optimizes the user's\ninitial prompt. Furthermore, PromptCharm supports the user in exploring and\nselecting different image styles within a large database. To assist users in\neffectively refining their prompts and images, PromptCharm renders model\nexplanations by visualizing the model's attention values. If the user notices\nany unsatisfactory areas in the generated images, they can further refine the\nimages through model attention adjustment or image inpainting within the rich\nfeedback loop of PromptCharm. To evaluate the effectiveness and usability of\nPromptCharm, we conducted a controlled user study with 12 participants and an\nexploratory user study with another 12 participants. These two studies show\nthat participants using PromptCharm were able to create images with higher\nquality and better aligned with the user's expectations compared with using two\nvariants of PromptCharm that lacked interaction or visualization support.",
      "tldr_zh": "该研究提出PromptCharm，一种混合主动系统，通过多-modal prompting和refinement，帮助新手用户简化文本到图像生成过程。PromptCharm自动优化用户的初始提示，支持图像样式探索，并通过可视化模型的attention values提供解释和反馈，用户可通过调整attention或image inpainting进一步完善图像。用户研究显示，与缺乏交互或可视化支持的版本相比，使用PromptCharm的参与者能生成更高质量、更符合预期的图像。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "To appear in the 2024 CHI Conference on Human Factors in Computing\n  Systems (CHI '24), May 11--16, 2024, Honolulu, HI, USA",
      "pdf_url": "http://arxiv.org/pdf/2403.04014v1",
      "published_date": "2024-03-06 19:55:01 UTC",
      "updated_date": "2024-03-06 19:55:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:56:56.252055"
    },
    {
      "arxiv_id": "2403.04001v1",
      "title": "Bidirectional Progressive Neural Networks with Episodic Return Progress for Emergent Task Sequencing and Robotic Skill Transfer",
      "title_zh": "翻译失败",
      "authors": [
        "Suzan Ece Ada",
        "Hanne Say",
        "Emre Ugur",
        "Erhan Oztop"
      ],
      "abstract": "Human brain and behavior provide a rich venue that can inspire novel control\nand learning methods for robotics. In an attempt to exemplify such a\ndevelopment by inspiring how humans acquire knowledge and transfer skills among\ntasks, we introduce a novel multi-task reinforcement learning framework named\nEpisodic Return Progress with Bidirectional Progressive Neural Networks\n(ERP-BPNN). The proposed ERP-BPNN model (1) learns in a human-like interleaved\nmanner by (2) autonomous task switching based on a novel intrinsic motivation\nsignal and, in contrast to existing methods, (3) allows bidirectional skill\ntransfer among tasks. ERP-BPNN is a general architecture applicable to several\nmulti-task learning settings; in this paper, we present the details of its\nneural architecture and show its ability to enable effective learning and skill\ntransfer among morphologically different robots in a reaching task. The\ndeveloped Bidirectional Progressive Neural Network (BPNN) architecture enables\nbidirectional skill transfer without requiring incremental training and\nseamlessly integrates with online task arbitration. The task arbitration\nmechanism developed is based on soft Episodic Return progress (ERP), a novel\nintrinsic motivation (IM) signal. To evaluate our method, we use quantifiable\nrobotics metrics such as 'expected distance to goal' and 'path straightness' in\naddition to the usual reward-based measure of episodic return common in\nreinforcement learning. With simulation experiments, we show that ERP-BPNN\nachieves faster cumulative convergence and improves performance in all metrics\nconsidered among morphologically different robots compared to the baselines.",
      "tldr_zh": "该研究提出了一种名为 ERP-BPNN 的多任务强化学习框架，受人类大脑和行为启发，旨在实现任务序列化和机器人技能双向转移。框架通过 Bidirectional Progressive Neural Networks (BPNN) 架构支持无需增量训练的技能转移，并使用软 Episodic Return Progress (ERP) 作为内在动机信号来实现自主任务切换和交错学习。在模拟实验中，ERP-BPNN 在不同形态机器人上的到达任务中比基线模型更快收敛，并显著改善了量化指标，如 expected distance to goal 和 path straightness。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.04001v1",
      "published_date": "2024-03-06 19:17:49 UTC",
      "updated_date": "2024-03-06 19:17:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:57:09.037919"
    },
    {
      "arxiv_id": "2403.03997v2",
      "title": "Guiding Enumerative Program Synthesis with Large Language Models",
      "title_zh": "使用大型语言模型指导枚举式程序合成",
      "authors": [
        "Yixuan Li",
        "Julian Parsert",
        "Elizabeth Polgreen"
      ],
      "abstract": "Pre-trained Large Language Models (LLMs) are beginning to dominate the\ndiscourse around automatic code generation with natural language\nspecifications. In contrast, the best-performing synthesizers in the domain of\nformal synthesis with precise logical specifications are still based on\nenumerative algorithms. In this paper, we evaluate the abilities of LLMs to\nsolve formal synthesis benchmarks by carefully crafting a library of prompts\nfor the domain. When one-shot synthesis fails, we propose a novel enumerative\nsynthesis algorithm, which integrates calls to an LLM into a weighted\nprobabilistic search. This allows the synthesizer to provide the LLM with\ninformation about the progress of the enumerator, and the LLM to provide the\nenumerator with syntactic guidance in an iterative loop. We evaluate our\ntechniques on benchmarks from the Syntax-Guided Synthesis (SyGuS) competition.\nWe find that GPT-3.5 as a stand-alone tool for formal synthesis is easily\noutperformed by state-of-the-art formal synthesis algorithms, but our approach\nintegrating the LLM into an enumerative synthesis algorithm shows significant\nperformance gains over both the LLM and the enumerative synthesizer alone and\nthe winning SyGuS competition tool.",
      "tldr_zh": "本研究探讨了如何利用大型语言模型 (LLMs) 指导枚举程序合成 (Enumerative Program Synthesis)，以解决使用精确逻辑规范的正式合成任务。论文提出了一种新颖算法，将 LLMs 集成到加权概率搜索中，通过迭代循环让合成器提供枚举进度信息，而 LLMs 则提供语法指导。实验在 Syntax-Guided Synthesis (SyGuS) 比赛基准上显示，该集成方法显著优于独立使用的 GPT-3.5、纯枚举合成器和 SyGuS 获胜工具，实现了性能提升。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at CAV 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.03997v2",
      "published_date": "2024-03-06 19:13:53 UTC",
      "updated_date": "2024-05-27 12:18:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:57:20.463379"
    },
    {
      "arxiv_id": "2403.03996v1",
      "title": "Rethinking Urban Flood Risk Assessment By Adapting Health Domain Perspective",
      "title_zh": "通过借鉴健康领域视角重新审视城市洪水风险评估",
      "authors": [
        "Zhewei Liu",
        "Kai Yin",
        "Ali Mostafavi"
      ],
      "abstract": "Inspired by ideas from health risk assessment, this paper presents a new\nperspective for flood risk assessment. The proposed perspective focuses on\nthree pillars for examining flood risk: (1) inherent susceptibility, (2)\nmitigation strategies, and (3) external stressors. These pillars collectively\nencompass the physical and environmental characteristics of urban areas, the\neffectiveness of human-intervention measures, and the influence of\nuncontrollable external factors, offering a fresh point of view for decoding\nflood risks. For each pillar, we delineate its individual contributions to\nflood risk and illustrate their interactive and overall impact. The\nthree-pillars model embodies a shift in focus from the quest to precisely model\nand quantify flood risk to evaluating pathways to high flood risk. The shift in\nperspective is intended to alleviate the quest for quantifying and predicting\nflood risk at fine resolutions as a panacea for enhanced flood risk management.\nThe decomposition of flood risk pathways into the three intertwined pillars\n(i.e., inherent factors, mitigation factors, and external factors) enables\nevaluation of changes in factors within each pillar enhance and exacerbate\nflood risk, creating a platform from which to inform plans, decisions, and\nactions. Building on this foundation, we argue that a flood risk pathway\nanalysis approach, which examines the individual and collective impacts of\ninherent factors, mitigation strategies, and external stressors, is essential\nfor a nuanced evaluation of flood risk. Accordingly, the proposed perspective\ncould complement the existing frameworks and approaches for flood risk\nassessment.",
      "tldr_zh": "这篇论文借鉴健康风险评估的理念，重新审视城市洪水风险评估，提出一个基于三个支柱的新视角：inherent susceptibility（固有易感性）、mitigation strategies（缓解策略）和 external stressors（外部应激源）。这些支柱分别涵盖了城市物理环境特征、人为干预措施的有效性和不可控外部因素的影响，并探讨它们之间的互动作用，以评估通往高风险的路径。相比传统精确量化方法，该视角更注重分析各因素的变化如何增强或恶化风险，从而为洪水风险管理和决策提供更细致且实用的补充框架。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03996v1",
      "published_date": "2024-03-06 19:12:41 UTC",
      "updated_date": "2024-03-06 19:12:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:57:32.482293"
    },
    {
      "arxiv_id": "2403.04810v3",
      "title": "Restricted Bayesian Neural Network",
      "title_zh": "翻译失败",
      "authors": [
        "Sourav Ganguly",
        "Saprativa Bhattacharjee"
      ],
      "abstract": "Modern deep learning tools are remarkably effective in addressing intricate\nproblems. However, their operation as black-box models introduces increased\nuncertainty in predictions. Additionally, they contend with various challenges,\nincluding the need for substantial storage space in large networks, issues of\noverfitting, underfitting, vanishing gradients, and more. This study explores\nthe concept of Bayesian Neural Networks, presenting a novel architecture\ndesigned to significantly alleviate the storage space complexity of a network.\nFurthermore, we introduce an algorithm adept at efficiently handling\nuncertainties, ensuring robust convergence values without becoming trapped in\nlocal optima, particularly when the objective function lacks perfect convexity.",
      "tldr_zh": "这篇论文探讨了Bayesian Neural Networks，以解决深度学习模型的不确定性、存储空间需求、overfitting、underfitting和vanishing gradients等问题。研究提出了一种新型Restricted Bayesian Neural Network架构，能够显著降低网络的存储空间复杂度。作者还引入了一个高效算法，用于处理预测不确定性，确保在非凸目标函数下实现鲁棒收敛，避免陷入局部最优。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.04810v3",
      "published_date": "2024-03-06 19:09:11 UTC",
      "updated_date": "2024-04-08 11:51:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:57:43.983182"
    },
    {
      "arxiv_id": "2403.03993v2",
      "title": "Personalized Negative Reservoir for Incremental Learning in Recommender Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Antonios Valkanas",
        "Yuening Wang",
        "Yingxue Zhang",
        "Mark Coates"
      ],
      "abstract": "Recommender systems have become an integral part of online platforms. Every\nday the volume of training data is expanding and the number of user\ninteractions is constantly increasing. The exploration of larger and more\nexpressive models has become a necessary pursuit to improve user experience.\nHowever, this progression carries with it an increased computational burden. In\ncommercial settings, once a recommendation system model has been trained and\ndeployed it typically needs to be updated frequently as new client data arrive.\nCumulatively, the mounting volume of data is guaranteed to eventually make full\nbatch retraining of the model from scratch computationally infeasible. Naively\nfine-tuning solely on the new data runs into the well-documented problem of\ncatastrophic forgetting. Despite the fact that negative sampling is a crucial\npart of training with implicit feedback, no specialized technique exists that\nis tailored to the incremental learning framework. In this work, we propose a\npersonalized negative reservoir strategy, which is used to obtain negative\nsamples for the standard triplet loss of graph-based recommendation systems.\nOur technique balances alleviation of forgetting with plasticity by encouraging\nthe model to remember stable user preferences and selectively forget when user\ninterests change. We derive the mathematical formulation of a negative sampler\nto populate and update the reservoir. We integrate our design in three SOTA and\ncommonly used incremental recommendation models. We show that these concrete\nrealizations of our negative reservoir framework achieve state-of-the-art\nresults for standard benchmarks using multiple top-k evaluation metrics.",
      "tldr_zh": "该论文针对推荐系统中的增量学习问题，解决了数据增长导致的全量训练不可行以及微调引起的灾难性遗忘（catastrophic forgetting）。作者提出了一种个性化的负样本库（personalized negative reservoir）策略，用于图-based 推荐系统的三元组损失（triplet loss），通过数学公式推导负样本的采样和更新机制，以平衡模型对稳定用户偏好的记忆和对兴趣变化的 selective forgetting。实验结果显示，将该策略整合到三个 SOTA 增量推荐模型中后，在标准基准测试中使用多种 top-k 评估指标，实现了 state-of-the-art 的性能提升。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03993v2",
      "published_date": "2024-03-06 19:08:28 UTC",
      "updated_date": "2025-02-11 21:10:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:57:56.558250"
    },
    {
      "arxiv_id": "2403.03950v1",
      "title": "Stop Regressing: Training Value Functions via Classification for Scalable Deep RL",
      "title_zh": "停止回归：通过分类训练价值函数以实现可扩展的深度强化学习",
      "authors": [
        "Jesse Farebrother",
        "Jordi Orbay",
        "Quan Vuong",
        "Adrien Ali Taïga",
        "Yevgen Chebotar",
        "Ted Xiao",
        "Alex Irpan",
        "Sergey Levine",
        "Pablo Samuel Castro",
        "Aleksandra Faust",
        "Aviral Kumar",
        "Rishabh Agarwal"
      ],
      "abstract": "Value functions are a central component of deep reinforcement learning (RL).\nThese functions, parameterized by neural networks, are trained using a mean\nsquared error regression objective to match bootstrapped target values.\nHowever, scaling value-based RL methods that use regression to large networks,\nsuch as high-capacity Transformers, has proven challenging. This difficulty is\nin stark contrast to supervised learning: by leveraging a cross-entropy\nclassification loss, supervised methods have scaled reliably to massive\nnetworks. Observing this discrepancy, in this paper, we investigate whether the\nscalability of deep RL can also be improved simply by using classification in\nplace of regression for training value functions. We demonstrate that value\nfunctions trained with categorical cross-entropy significantly improves\nperformance and scalability in a variety of domains. These include: single-task\nRL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale\nResNets, robotic manipulation with Q-transformers, playing Chess without\nsearch, and a language-agent Wordle task with high-capacity Transformers,\nachieving state-of-the-art results on these domains. Through careful analysis,\nwe show that the benefits of categorical cross-entropy primarily stem from its\nability to mitigate issues inherent to value-based RL, such as noisy targets\nand non-stationarity. Overall, we argue that a simple shift to training value\nfunctions with categorical cross-entropy can yield substantial improvements in\nthe scalability of deep RL at little-to-no cost.",
      "tldr_zh": "该论文提出了一种新方法，通过使用分类损失（categorical cross-entropy）代替传统均方误差回归来训练强化学习（RL）中的价值函数（value functions），以解决深度RL在扩展到大型网络（如高容量Transformers）时的挑战。实验结果显示，这种方法在Atari 2600游戏、多任务RL、机器人操作、国际象棋和语言代理任务等多个领域显著提升了性能和可扩展性，部分任务达到了最先进水平。分析表明，categorical cross-entropy的主要优势在于缓解RL固有的噪声目标和非平稳性问题，从而以低成本实现深度RL的规模化。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03950v1",
      "published_date": "2024-03-06 18:55:47 UTC",
      "updated_date": "2024-03-06 18:55:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:58:09.392418"
    },
    {
      "arxiv_id": "2403.03949v3",
      "title": "Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Marcel Torne",
        "Anthony Simeonov",
        "Zechu Li",
        "April Chan",
        "Tao Chen",
        "Abhishek Gupta",
        "Pulkit Agrawal"
      ],
      "abstract": "Imitation learning methods need significant human supervision to learn\npolicies robust to changes in object poses, physical disturbances, and visual\ndistractors. Reinforcement learning, on the other hand, can explore the\nenvironment autonomously to learn robust behaviors but may require impractical\namounts of unsafe real-world data collection. To learn performant, robust\npolicies without the burden of unsafe real-world data collection or extensive\nhuman supervision, we propose RialTo, a system for robustifying real-world\nimitation learning policies via reinforcement learning in \"digital twin\"\nsimulation environments constructed on the fly from small amounts of real-world\ndata. To enable this real-to-sim-to-real pipeline, RialTo proposes an\neasy-to-use interface for quickly scanning and constructing digital twins of\nreal-world environments. We also introduce a novel \"inverse distillation\"\nprocedure for bringing real-world demonstrations into simulated environments\nfor efficient fine-tuning, with minimal human intervention and engineering\nrequired. We evaluate RialTo across a variety of robotic manipulation problems\nin the real world, such as robustly stacking dishes on a rack, placing books on\na shelf, and six other tasks. RialTo increases (over 67%) in policy robustness\nwithout requiring extensive human data collection. Project website and videos\nat https://real-to-sim-to-real.github.io/RialTo/",
      "tldr_zh": "该论文提出 RialTo 系统，一种 Real-to-Sim-to-Real 方法，用于提升机器人操作策略的鲁棒性，避免模仿学习对大量人类监督的依赖和强化学习对不安全真实数据的需求。该系统通过从少量真实数据快速构建 digital twin 模拟环境，并引入 inverse distillation 过程，将真实演示高效转移到模拟中进行强化学习微调。实验在多种任务（如堆叠盘子、放置书籍等）中验证，RialTo 使政策鲁棒性提升超过67%，显著减少了人类干预和数据收集负担。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project page: https://real-to-sim-to-real.github.io/RialTo/",
      "pdf_url": "http://arxiv.org/pdf/2403.03949v3",
      "published_date": "2024-03-06 18:55:36 UTC",
      "updated_date": "2024-11-24 02:02:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:58:24.135019"
    },
    {
      "arxiv_id": "2403.03929v1",
      "title": "Extreme Precipitation Nowcasting using Transformer-based Generative Models",
      "title_zh": "翻译失败",
      "authors": [
        "Cristian Meo",
        "Ankush Roy",
        "Mircea Lică",
        "Junzhe Yin",
        "Zeineb Bou Che",
        "Yanbo Wang",
        "Ruben Imhoff",
        "Remko Uijlenhoet",
        "Justin Dauwels"
      ],
      "abstract": "This paper presents an innovative approach to extreme precipitation\nnowcasting by employing Transformer-based generative models, namely\nNowcastingGPT with Extreme Value Loss (EVL) regularization. Leveraging a\ncomprehensive dataset from the Royal Netherlands Meteorological Institute\n(KNMI), our study focuses on predicting short-term precipitation with high\naccuracy. We introduce a novel method for computing EVL without assuming fixed\nextreme representations, addressing the limitations of current models in\ncapturing extreme weather events. We present both qualitative and quantitative\nanalyses, demonstrating the superior performance of the proposed\nNowcastingGPT-EVL in generating accurate precipitation forecasts, especially\nwhen dealing with extreme precipitation events. The code is available at\n\\url{https://github.com/Cmeo97/NowcastingGPT}.",
      "tldr_zh": "这篇论文提出了一种创新方法，使用基于Transformer的生成模型NowcastingGPT，并结合Extreme Value Loss (EVL) 正则化，来实现极端降水预报。研究利用Royal Netherlands Meteorological Institute (KNMI)的全面数据集，引入一种不依赖固定极端表示的EVL计算方法，以克服现有模型在捕捉极端天气事件方面的局限性。实验结果通过定性和定量分析证明，NowcastingGPT-EVL在短期降水预测中表现出色，尤其在极端降水事件上，准确性显著提升。代码已开源在GitHub上。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03929v1",
      "published_date": "2024-03-06 18:39:41 UTC",
      "updated_date": "2024-03-06 18:39:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:58:34.936891"
    },
    {
      "arxiv_id": "2403.03925v1",
      "title": "Consciousness qua Mortal Computation",
      "title_zh": "翻译失败",
      "authors": [
        "Johannes Kleiner"
      ],
      "abstract": "Computational functionalism posits that consciousness is a computation. Here\nwe show, perhaps surprisingly, that it cannot be a Turing computation. Rather,\ncomputational functionalism implies that consciousness is a novel type of\ncomputation that has recently been proposed by Geoffrey Hinton, called mortal\ncomputation.",
      "tldr_zh": "计算功能主义（Computational functionalism）认为意识是一种计算，但本论文证明，意识并非Turing computation，而是另一种新型计算形式。论文论证了计算功能主义暗示意识应被视为Geoffrey Hinton最近提出的mortal computation。总体而言，这一发现为理解意识的计算本质提供了新视角，并扩展了计算理论的边界。",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03925v1",
      "published_date": "2024-03-06 18:37:06 UTC",
      "updated_date": "2024-03-06 18:37:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:58:44.497046"
    },
    {
      "arxiv_id": "2403.03920v1",
      "title": "Enhancing Instructional Quality: Leveraging Computer-Assisted Textual Analysis to Generate In-Depth Insights from Educational Artifacts",
      "title_zh": "翻译失败",
      "authors": [
        "Zewei Tian",
        "Min Sun",
        "Alex Liu",
        "Shawon Sarkar",
        "Jing Liu"
      ],
      "abstract": "This paper explores the transformative potential of computer-assisted textual\nanalysis in enhancing instructional quality through in-depth insights from\neducational artifacts. We integrate Richard Elmore's Instructional Core\nFramework to examine how artificial intelligence (AI) and machine learning (ML)\nmethods, particularly natural language processing (NLP), can analyze\neducational content, teacher discourse, and student responses to foster\ninstructional improvement. Through a comprehensive review and case studies\nwithin the Instructional Core Framework, we identify key areas where AI/ML\nintegration offers significant advantages, including teacher coaching, student\nsupport, and content development. We unveil patterns that indicate AI/ML not\nonly streamlines administrative tasks but also introduces novel pathways for\npersonalized learning, providing actionable feedback for educators and\ncontributing to a richer understanding of instructional dynamics. This paper\nemphasizes the importance of aligning AI/ML technologies with pedagogical goals\nto realize their full potential in educational settings, advocating for a\nbalanced approach that considers ethical considerations, data quality, and the\nintegration of human expertise.",
      "tldr_zh": "这篇论文探讨了计算机辅助文本分析如何通过分析教育工件提升教学质量，整合了 Richard Elmore 的 Instructional Core Framework。\n利用 AI、ML 和 NLP 方法，论文通过文献综述和案例研究，识别了这些技术在教师指导、学生支持以及内容开发中的关键优势。\n研究发现，AI/ML 不仅简化行政任务，还提供个性化学习路径和可操作反馈，从而加深对教学动态的理解。\n论文强调将 AI/ML 与教学目标对齐的重要性，并提倡平衡方法，包括考虑伦理因素、数据质量和人类专业知识的整合。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03920v1",
      "published_date": "2024-03-06 18:29:18 UTC",
      "updated_date": "2024-03-06 18:29:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:58:58.333920"
    },
    {
      "arxiv_id": "2403.03894v3",
      "title": "IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators",
      "title_zh": "翻译失败",
      "authors": [
        "Indraneil Paul",
        "Goran Glavaš",
        "Iryna Gurevych"
      ],
      "abstract": "Code understanding and generation have fast become some of the most popular\napplications of language models (LMs). Nonetheless, research on multilingual\naspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual\ntransfer between different programming languages, language-specific data\naugmentation, and post-hoc LM adaptation, alongside exploitation of data\nsources other than the original textual content, has been much sparser than for\ntheir natural language counterparts. In particular, most mainstream Code-LMs\nhave been pre-trained on source code files alone. In this work, we investigate\nthe prospect of leveraging readily available compiler intermediate\nrepresentations (IR) - shared across programming languages - to improve the\nmultilingual capabilities of Code-LMs and facilitate cross-lingual transfer.\n  To this end, we first compile SLTrans, a parallel dataset consisting of\nnearly 4M self-contained source code files coupled with respective intermediate\nrepresentations. Next, starting from various base Code-LMs (ranging in size\nfrom 1.1B to 7.3B parameters), we carry out continued causal language modelling\ntraining on SLTrans, forcing the Code-LMs to (1) learn the IR language and (2)\nalign the IR constructs with respective constructs of various programming\nlanguages. Our resulting models, dubbed IRCoder, display sizeable and\nconsistent gains across a wide variety of code generation tasks and metrics,\nincluding prompt robustness, multilingual code completion, code understanding,\nand instruction following.",
      "tldr_zh": "该研究探讨了利用编译器中间表示（Intermediate Representations, IR）来提升代码语言模型（Code-LMs）的多语言能力，以解决现有模型在跨语言转移和数据利用方面的不足。研究者构建了SLTrans数据集，包含近4M个源代码文件及其对应的IR，并基于此对多种基线Code-LMs（参数规模从1.1B到7.3B）进行持续因果语言建模训练，使模型学会IR语言并将IR结构与各种编程语言对齐。结果，IRCoder模型在代码生成任务中表现出显著改进，包括提示鲁棒性、多语言代码补全、代码理解和指令遵循等方面。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.PL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03894v3",
      "published_date": "2024-03-06 17:52:08 UTC",
      "updated_date": "2024-04-15 16:29:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:59:10.698883"
    },
    {
      "arxiv_id": "2403.03893v3",
      "title": "From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Luiza Pozzobon",
        "Patrick Lewis",
        "Sara Hooker",
        "Beyza Ermis"
      ],
      "abstract": "To date, toxicity mitigation in language models has almost entirely been\nfocused on single-language settings. As language models embrace multilingual\ncapabilities, it's crucial our safety measures keep pace. Recognizing this\nresearch gap, our approach expands the scope of conventional toxicity\nmitigation to address the complexities presented by multiple languages. In the\nabsence of sufficient annotated datasets across languages, we employ translated\ndata to evaluate and enhance our mitigation techniques. We also compare\nfinetuning mitigation approaches against retrieval-augmented techniques under\nboth static and continual toxicity mitigation scenarios. This allows us to\nexamine the effects of translation quality and the cross-lingual transfer on\ntoxicity mitigation. We also explore how model size and data quantity affect\nthe success of these mitigation efforts. Covering nine languages, our study\nrepresents a broad array of linguistic families and levels of resource\navailability, ranging from high to mid-resource languages. Through\ncomprehensive experiments, we provide insights into the complexities of\nmultilingual toxicity mitigation, offering valuable insights and paving the way\nfor future research in this increasingly important field. Code and data are\navailable at https://github.com/for-ai/goodtriever.",
      "tldr_zh": "该研究扩展了语言模型的 toxicity mitigation，从单语言环境转向多语言场景，以应对多语言复杂性。作者使用翻译数据来评估和改进缓解技术，并比较了 finetuning 方法与 retrieval-augmented 技术在静态和 continual 缓解场景下的表现，同时考察了翻译质量、跨语言转移、模型大小和数据量的影响。实验覆盖九种语言，包括不同语言家族和高、中资源语言，揭示了多语言毒性缓解的挑战与洞见，为未来研究奠定基础。代码和数据可从 GitHub 获取。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03893v3",
      "published_date": "2024-03-06 17:51:43 UTC",
      "updated_date": "2024-05-30 17:37:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:59:22.264961"
    },
    {
      "arxiv_id": "2403.03890v1",
      "title": "Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Xiao Ma",
        "Sumit Patidar",
        "Iain Haughton",
        "Stephen James"
      ],
      "abstract": "This paper introduces Hierarchical Diffusion Policy (HDP), a hierarchical\nagent for multi-task robotic manipulation. HDP factorises a manipulation policy\ninto a hierarchical structure: a high-level task-planning agent which predicts\na distant next-best end-effector pose (NBP), and a low-level goal-conditioned\ndiffusion policy which generates optimal motion trajectories. The factorised\npolicy representation allows HDP to tackle both long-horizon task planning\nwhile generating fine-grained low-level actions. To generate context-aware\nmotion trajectories while satisfying robot kinematics constraints, we present a\nnovel kinematics-aware goal-conditioned control agent, Robot Kinematics\nDiffuser (RK-Diffuser). Specifically, RK-Diffuser learns to generate both the\nend-effector pose and joint position trajectories, and distill the accurate but\nkinematics-unaware end-effector pose diffuser to the kinematics-aware but less\naccurate joint position diffuser via differentiable kinematics. Empirically, we\nshow that HDP achieves a significantly higher success rate than the\nstate-of-the-art methods in both simulation and real-world.",
      "tldr_zh": "这篇论文介绍了 Hierarchical Diffusion Policy (HDP)，一个分层代理，用于处理多任务机器人操作中的运动学约束。HDP 将策略分解为高层任务规划代理（预测 next-best end-effector pose (NBP)）和低层目标条件 diffusion policy（生成优化运动轨迹），并通过 Robot Kinematics Diffuser (RK-Diffuser) 学习生成末端执行器姿势和关节位置轨迹，同时利用可微分运动学确保准确性。实验结果表明，HDP 在模拟和真实世界环境中比最先进方法成功率显著提升。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2024). Videos and code:\n  https://yusufma03.github.io/projects/hdp/",
      "pdf_url": "http://arxiv.org/pdf/2403.03890v1",
      "published_date": "2024-03-06 17:50:26 UTC",
      "updated_date": "2024-03-06 17:50:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:59:35.250565"
    },
    {
      "arxiv_id": "2403.03881v3",
      "title": "Latent Dataset Distillation with Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Brian B. Moser",
        "Federico Raue",
        "Sebastian Palacio",
        "Stanislav Frolov",
        "Andreas Dengel"
      ],
      "abstract": "Machine learning traditionally relies on increasingly larger datasets. Yet,\nsuch datasets pose major storage challenges and usually contain non-influential\nsamples, which could be ignored during training without negatively impacting\nthe training quality. In response, the idea of distilling a dataset into a\ncondensed set of synthetic samples, i.e., a distilled dataset, emerged. One key\naspect is the selected architecture, usually ConvNet, for linking the original\nand synthetic datasets. However, the final accuracy is lower if the employed\nmodel architecture differs from that used during distillation. Another\nchallenge is the generation of high-resolution images (128x128 and higher). To\naddress both challenges, this paper proposes Latent Dataset Distillation with\nDiffusion Models (LD3M) that combine diffusion in latent space with dataset\ndistillation. Our novel diffusion process is tailored for this task and\nsignificantly improves the gradient flow for distillation. By adjusting the\nnumber of diffusion steps, LD3M also offers a convenient way of controlling the\ntrade-off between distillation speed and dataset quality. Overall, LD3M\nconsistently outperforms state-of-the-art methods by up to 4.8 p.p. and 4.2\np.p. for 1 and 10 images per class, respectively, and on several ImageNet\nsubsets and high resolutions (128x128 and 256x256).",
      "tldr_zh": "本研究提出了一种名为Latent Dataset Distillation with Diffusion Models (LD3M)的创新方法，旨在解决传统数据集蒸馏面临的关键挑战，包括对特定模型架构（如ConvNet）的依赖以及生成高分辨率图像的困难。LD3M通过在潜在空间中结合扩散模型进行数据集蒸馏，设计了定制的扩散过程来提升梯度流动，并通过调整扩散步骤灵活控制蒸馏速度与数据集质量的权衡。实验结果显示，该方法在ImageNet子集和高分辨率（128x128和256x256）上，比现有最先进方法分别提高了高达4.8 p.p.（每类1张图像）和4.2 p.p.（每类10张图像）的性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03881v3",
      "published_date": "2024-03-06 17:41:41 UTC",
      "updated_date": "2024-07-11 09:10:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:59:46.619860"
    },
    {
      "arxiv_id": "2403.03879v1",
      "title": "Redefining cystoscopy with ai: bladder cancer diagnosis using an efficient hybrid cnn-transformer model",
      "title_zh": "翻译失败",
      "authors": [
        "Meryem Amaouche",
        "Ouassim Karrakchou",
        "Mounir Ghogho",
        "Anouar El Ghazzaly",
        "Mohamed Alami",
        "Ahmed Ameur"
      ],
      "abstract": "Bladder cancer ranks within the top 10 most diagnosed cancers worldwide and\nis among the most expensive cancers to treat due to the high recurrence rates\nwhich require lifetime follow-ups. The primary tool for diagnosis is\ncystoscopy, which heavily relies on doctors' expertise and interpretation.\nTherefore, annually, numerous cases are either undiagnosed or misdiagnosed and\ntreated as urinary infections. To address this, we suggest a deep learning\napproach for bladder cancer detection and segmentation which combines CNNs with\na lightweight positional-encoding-free transformer and dual attention gates\nthat fuse self and spatial attention for feature enhancement. The architecture\nsuggested in this paper is efficient making it suitable for medical scenarios\nthat require real time inference. Experiments have proven that this model\naddresses the critical need for a balance between computational efficiency and\ndiagnostic accuracy in cystoscopic imaging as despite its small size it rivals\nlarge models in performance.",
      "tldr_zh": "膀胱癌是全球前十大常见癌症之一，治疗成本高且复发率高，主要依赖膀胱镜检查（cystoscopy），但易导致误诊。为解决此问题，本文提出一个高效的混合深度学习模型，结合 CNNs 和轻量级无位置编码 Transformer，以及双注意力门（dual attention gates），用于膀胱癌检测和分割，以增强特征并实现实时推理。实验结果表明，该模型在计算效率和诊断准确性之间取得了良好平衡，尽管模型规模较小，其性能可与大型模型媲美。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.03879v1",
      "published_date": "2024-03-06 17:38:33 UTC",
      "updated_date": "2024-03-06 17:38:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T12:59:58.789074"
    },
    {
      "arxiv_id": "2403.03874v1",
      "title": "Impoverished Language Technology: The Lack of (Social) Class in NLP",
      "title_zh": "翻译失败",
      "authors": [
        "Amanda Cercas Curry",
        "Zeerak Talat",
        "Dirk Hovy"
      ],
      "abstract": "Since Labov's (1964) foundational work on the social stratification of\nlanguage, linguistics has dedicated concerted efforts towards understanding the\nrelationships between socio-demographic factors and language production and\nperception. Despite the large body of evidence identifying significant\nrelationships between socio-demographic factors and language production,\nrelatively few of these factors have been investigated in the context of NLP\ntechnology. While age and gender are well covered, Labov's initial target,\nsocio-economic class, is largely absent. We survey the existing Natural\nLanguage Processing (NLP) literature and find that only 20 papers even mention\nsocio-economic status. However, the majority of those papers do not engage with\nclass beyond collecting information of annotator-demographics. Given this\nresearch lacuna, we provide a definition of class that can be operationalised\nby NLP researchers, and argue for including socio-economic class in future\nlanguage technologies.",
      "tldr_zh": "该论文探讨了自然语言处理（NLP）领域对社会经济阶层（socio-economic class）的忽视，尽管语言学已有丰富研究显示社会分层因素（如年龄和性别）与语言生产的关系。作者调查了现有文献，发现仅有20篇论文提到socio-economic status，且多数仅限于收集标注者人口统计信息，而未深入探讨。论文提供了可操作化的class定义，并呼吁未来NLP技术应纳入socio-economic class，以弥合这一研究缺口。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to LREC-COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.03874v1",
      "published_date": "2024-03-06 17:35:27 UTC",
      "updated_date": "2024-03-06 17:35:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:00:08.946905"
    },
    {
      "arxiv_id": "2403.03864v3",
      "title": "Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Deepanway Ghosal",
        "Vernon Toh Yan Han",
        "Chia Yew Ken",
        "Soujanya Poria"
      ],
      "abstract": "This paper introduces the novel task of multimodal puzzle solving, framed\nwithin the context of visual question-answering. We present a new dataset,\nAlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal\nlanguage models in solving algorithmic puzzles that necessitate both visual\nunderstanding, language understanding, and complex algorithmic reasoning. We\ncreate the puzzles to encompass a diverse array of mathematical and algorithmic\ntopics such as boolean logic, combinatorics, graph theory, optimization,\nsearch, etc., aiming to evaluate the gap between visual data interpretation and\nalgorithmic problem-solving skills. The dataset is generated automatically from\ncode authored by humans. All our puzzles have exact solutions that can be found\nfrom the algorithm without tedious human calculations. It ensures that our\ndataset can be scaled up arbitrarily in terms of reasoning complexity and\ndataset size. Our investigation reveals that large language models (LLMs) such\nas GPT4V and Gemini exhibit limited performance in puzzle-solving tasks. We\nfind that their performance is near random in a multi-choice question-answering\nsetup for a significant number of puzzles. The findings emphasize the\nchallenges of integrating visual, language, and algorithmic knowledge for\nsolving complex reasoning problems.",
      "tldr_zh": "这篇论文引入了多模态谜题解决任务，作为视觉问答框架的一部分，并提出了新数据集AlgoPuzzleVQA，用于评估多模态语言模型在处理需要视觉理解、语言理解和复杂算法推理的谜题方面的能力。数据集涵盖主题如布尔逻辑、组合学、图论和优化等，由人类编写的代码自动生成，确保精确解决方案并可无限扩展。研究发现，大语言模型（LLMs）如GPT4V和Gemini在多选问答设置中表现接近随机水平，突显了整合视觉、语言和算法知识的重大挑战。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03864v3",
      "published_date": "2024-03-06 17:15:04 UTC",
      "updated_date": "2024-03-13 00:50:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:00:22.358987"
    },
    {
      "arxiv_id": "2403.03852v1",
      "title": "Accelerating Convergence of Score-Based Diffusion Models, Provably",
      "title_zh": "翻译失败",
      "authors": [
        "Gen Li",
        "Yu Huang",
        "Timofey Efimov",
        "Yuting Wei",
        "Yuejie Chi",
        "Yuxin Chen"
      ],
      "abstract": "Score-based diffusion models, while achieving remarkable empirical\nperformance, often suffer from low sampling speed, due to extensive function\nevaluations needed during the sampling phase. Despite a flurry of recent\nactivities towards speeding up diffusion generative modeling in practice,\ntheoretical underpinnings for acceleration techniques remain severely limited.\nIn this paper, we design novel training-free algorithms to accelerate popular\ndeterministic (i.e., DDIM) and stochastic (i.e., DDPM) samplers. Our\naccelerated deterministic sampler converges at a rate $O(1/{T}^2)$ with $T$ the\nnumber of steps, improving upon the $O(1/T)$ rate for the DDIM sampler; and our\naccelerated stochastic sampler converges at a rate $O(1/T)$, outperforming the\nrate $O(1/\\sqrt{T})$ for the DDPM sampler. The design of our algorithms\nleverages insights from higher-order approximation, and shares similar\nintuitions as popular high-order ODE solvers like the DPM-Solver-2. Our theory\naccommodates $\\ell_2$-accurate score estimates, and does not require\nlog-concavity or smoothness on the target distribution.",
      "tldr_zh": "该论文针对 Score-based diffusion models 的采样速度问题，提出了一种新型的训练-free 算法，以加速确定性采样器（如 DDIM）和随机采样器（如 DDPM）。该算法利用更高阶近似（higher-order approximation）的洞见，类似于 DPM-Solver-2，确保高效的采样过程。实验结果显示，加速后的确定性采样器收敛率达到 O(1/T^2)，优于 DDIM 的 O(1/T)；加速后的随机采样器收敛率达到 O(1/T)，优于 DDPM 的 O(1/√T)，且该理论适用于 ℓ2-accurate score estimates，而不依赖目标分布的 log-concavity 或 smoothness。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "The first two authors contributed equally",
      "pdf_url": "http://arxiv.org/pdf/2403.03852v1",
      "published_date": "2024-03-06 17:02:39 UTC",
      "updated_date": "2024-03-06 17:02:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:00:34.645834"
    },
    {
      "arxiv_id": "2403.12082v1",
      "title": "The Boy Who Survived: Removing Harry Potter from an LLM is harder than reported",
      "title_zh": "翻译失败",
      "authors": [
        "Adam Shostack"
      ],
      "abstract": "Recent work arXiv.2310.02238 asserted that \"we effectively erase the model's\nability to generate or recall Harry Potter-related content.'' This claim is\nshown to be overbroad. A small experiment of less than a dozen trials led to\nrepeated and specific mentions of Harry Potter, including \"Ah, I see! A\n\"muggle\" is a term used in the Harry Potter book series by Terry Pratchett...''",
      "tldr_zh": "这篇论文质疑了先前研究（arXiv.2310.02238）的声明，即已成功从大型语言模型（LLM）中删除哈利波特相关内容的能力。作者通过一个小型实验（少于12次试验）发现，模型仍能生成和回忆具体哈利波特元素，包括错误的引用，如将哈利波特系列归因于Terry Pratchett。结果表明，移除特定知识比报道中更具挑战性，强调了LLM中知识遗留的复杂性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "2 pages, 4 pages of appendix. Comment on arXiv:2310.02238",
      "pdf_url": "http://arxiv.org/pdf/2403.12082v1",
      "published_date": "2024-03-06 16:39:50 UTC",
      "updated_date": "2024-03-06 16:39:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:00:45.182642"
    },
    {
      "arxiv_id": "2403.03835v3",
      "title": "Cobweb: An Incremental and Hierarchical Model of Human-Like Category Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Lian",
        "Sashank Varma",
        "Christopher J. MacLellan"
      ],
      "abstract": "Cobweb, a human-like category learning system, differs from most cognitive\nscience models in incrementally constructing hierarchically organized tree-like\nstructures guided by the category utility measure. Prior studies have shown\nthat Cobweb can capture psychological effects such as basic-level, typicality,\nand fan effects. However, a broader evaluation of Cobweb as a model of human\ncategorization remains lacking. The current study addresses this gap. It\nestablishes Cobweb's alignment with classical human category learning effects.\nIt also explores Cobweb's flexibility to exhibit both exemplar- and\nprototype-like learning within a single framework. These findings set the stage\nfor further research on Cobweb as a robust model of human category learning.",
      "tldr_zh": "Cobweb 是一种模仿人类的类别学习模型，通过类别效用度量 (category utility measure) 增量构建层次化的树状结构，与传统认知科学模型不同。研究评估了 Cobweb 的性能，证实它能捕捉基本-level、typicality 和 fan effects 等经典人类类别学习效果，同时展示了 Cobweb 在单一框架内灵活表现出样本式 (exemplar-like) 和原型式 (prototype-like) 学习特性。这些发现为 Cobweb 作为人类类别学习模型的进一步研究奠定了坚实基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by CogSci-24",
      "pdf_url": "http://arxiv.org/pdf/2403.03835v3",
      "published_date": "2024-03-06 16:26:40 UTC",
      "updated_date": "2024-05-09 03:50:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:00:57.714997"
    },
    {
      "arxiv_id": "2403.03832v1",
      "title": "Your device may know you better than you know yourself -- continuous authentication on novel dataset using machine learning",
      "title_zh": "翻译失败",
      "authors": [
        "Pedro Gomes do Nascimento",
        "Pidge Witiak",
        "Tucker MacCallum",
        "Zachary Winterfeldt",
        "Rushit Dave"
      ],
      "abstract": "This research aims to further understanding in the field of continuous\nauthentication using behavioral biometrics. We are contributing a novel dataset\nthat encompasses the gesture data of 15 users playing Minecraft with a Samsung\nTablet, each for a duration of 15 minutes. Utilizing this dataset, we employed\nmachine learning (ML) binary classifiers, being Random Forest (RF), K-Nearest\nNeighbors (KNN), and Support Vector Classifier (SVC), to determine the\nauthenticity of specific user actions. Our most robust model was SVC, which\nachieved an average accuracy of approximately 90%, demonstrating that touch\ndynamics can effectively distinguish users. However, further studies are needed\nto make it viable option for authentication systems",
      "tldr_zh": "这篇论文探讨了使用行为生物特征进行连续认证，贡献了一个新数据集，包含15个用户在Samsung平板上玩Minecraft的15分钟手势数据。研究者采用机器学习二元分类器，包括Random Forest (RF)、K-Nearest Neighbors (KNN)和Support Vector Classifier (SVC)，来判断用户动作的真实性。结果显示，SVC模型表现出色，平均准确率约90%，证明触控动态能有效区分用户身份。论文指出，需要进一步研究以使其成为可行的认证系统选项。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03832v1",
      "published_date": "2024-03-06 16:22:49 UTC",
      "updated_date": "2024-03-06 16:22:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:01:10.197875"
    },
    {
      "arxiv_id": "2403.03828v1",
      "title": "From Clicks to Security: Investigating Continuous Authentication via Mouse Dynamics",
      "title_zh": "从点击到安全：通过鼠标动态调查持续认证",
      "authors": [
        "Rushit Dave",
        "Marcho Handoko",
        "Ali Rashid",
        "Cole Schoenbauer"
      ],
      "abstract": "In the realm of computer security, the importance of efficient and reliable\nuser authentication methods has become increasingly critical. This paper\nexamines the potential of mouse movement dynamics as a consistent metric for\ncontinuous authentication. By analyzing user mouse movement patterns in two\ncontrasting gaming scenarios, \"Team Fortress\" and Poly Bridge we investigate\nthe distinctive behavioral patterns inherent in high-intensity and\nlow-intensity UI interactions. The study extends beyond conventional\nmethodologies by employing a range of machine learning models. These models are\ncarefully selected to assess their effectiveness in capturing and interpreting\nthe subtleties of user behavior as reflected in their mouse movements. This\nmultifaceted approach allows for a more nuanced and comprehensive understanding\nof user interaction patterns. Our findings reveal that mouse movement dynamics\ncan serve as a reliable indicator for continuous user authentication. The\ndiverse machine learning models employed in this study demonstrate competent\nperformance in user verification, marking an improvement over previous methods\nused in this field. This research contributes to the ongoing efforts to enhance\ncomputer security and highlights the potential of leveraging user behavior,\nspecifically mouse dynamics, in developing robust authentication systems.",
      "tldr_zh": "这篇论文探讨了使用鼠标动态（mouse dynamics）作为持续认证（continuous authentication）的方法，以提升计算机安全。研究者通过分析用户在两个游戏场景（Team Fortress 和 Poly Bridge）中的鼠标运动模式，比较了高强度和低强度 UI 交互的行为特征，并应用多种机器学习模型（machine learning models）来评估这些模式的有效性。结果表明，鼠标动态可作为可靠的认证指标，这些模型在用户验证中表现出色，比以往方法提高了性能，为开发更稳健的认证系统提供了新贡献。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03828v1",
      "published_date": "2024-03-06 16:18:02 UTC",
      "updated_date": "2024-03-06 16:18:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:01:22.434919"
    },
    {
      "arxiv_id": "2403.03814v2",
      "title": "Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ",
      "title_zh": "翻译失败",
      "authors": [
        "Carolin Holtermann",
        "Paul Röttger",
        "Timm Dill",
        "Anne Lauscher"
      ],
      "abstract": "Large language models (LLMs) need to serve everyone, including a global\nmajority of non-English speakers. However, most LLMs today, and open LLMs in\nparticular, are often intended for use in just English (e.g. Llama2, Mistral)\nor a small handful of high-resource languages (e.g. Mixtral, Qwen). Recent\nresearch shows that, despite limits in their intended use, people prompt LLMs\nin many different languages. Therefore, in this paper, we investigate the basic\nmultilingual capabilities of state-of-the-art open LLMs beyond their intended\nuse. For this purpose, we introduce MultiQ, a new silver standard benchmark for\nbasic open-ended question answering with 27.4k test questions across a\ntypologically diverse set of 137 languages. With MultiQ, we evaluate language\nfidelity, i.e. whether models respond in the prompted language, and question\nanswering accuracy. All LLMs we test respond faithfully and/or accurately for\nat least some languages beyond their intended use. Most models are more\naccurate when they respond faithfully. However, differences across models are\nlarge, and there is a long tail of languages where models are neither accurate\nnor faithful. We explore differences in tokenization as a potential explanation\nfor our findings, identifying possible correlations that warrant further\ninvestigation.",
      "tldr_zh": "本研究评估了大型语言模型（Large Language Models, LLMs）的基本多语言能力，聚焦于其超出预期用途的性能表现。研究者引入了 MultiQ 基准测试，该测试包含 27.4k 个开放式问题，覆盖 137 种语言的类型多样性，用于评估语言忠诚度（是否以提示语言回应）和问题回答准确性。结果显示，所有测试的 LLMs 在某些语言上能实现忠实和准确回应，且大多数模型在忠实回应时准确性更高；然而，模型间差异显著，许多语言的表现既不准确也不忠实。进一步探讨发现，tokenization 的差异可能与这些结果相关，需要更多调查。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03814v2",
      "published_date": "2024-03-06 16:01:44 UTC",
      "updated_date": "2024-07-18 07:31:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:01:34.723038"
    },
    {
      "arxiv_id": "2403.03812v1",
      "title": "ProbSAINT: Probabilistic Tabular Regression for Used Car Pricing",
      "title_zh": "翻译失败",
      "authors": [
        "Kiran Madhusudhanan",
        "Gunnar Behrens",
        "Maximilian Stubbemann",
        "Lars Schmidt-Thieme"
      ],
      "abstract": "Used car pricing is a critical aspect of the automotive industry, influenced\nby many economic factors and market dynamics. With the recent surge in online\nmarketplaces and increased demand for used cars, accurate pricing would benefit\nboth buyers and sellers by ensuring fair transactions. However, the transition\ntowards automated pricing algorithms using machine learning necessitates the\ncomprehension of model uncertainties, specifically the ability to flag\npredictions that the model is unsure about. Although recent literature proposes\nthe use of boosting algorithms or nearest neighbor-based approaches for swift\nand precise price predictions, encapsulating model uncertainties with such\nalgorithms presents a complex challenge. We introduce ProbSAINT, a model that\noffers a principled approach for uncertainty quantification of its price\npredictions, along with accurate point predictions that are comparable to\nstate-of-the-art boosting techniques. Furthermore, acknowledging that the\nbusiness prefers pricing used cars based on the number of days the vehicle was\nlisted for sale, we show how ProbSAINT can be used as a dynamic forecasting\nmodel for predicting price probabilities for different expected offer duration.\nOur experiments further indicate that ProbSAINT is especially accurate on\ninstances where it is highly certain. This proves the applicability of its\nprobabilistic predictions in real-world scenarios where trustworthiness is\ncrucial.",
      "tldr_zh": "本文提出ProbSAINT，一种基于Probabilistic Tabular Regression的模型，用于二手车定价问题，能够提供准确的价格点预测并量化模型不确定性，以解决现有boosting algorithms等方法的局限性。ProbSAINT还支持动态预测，根据车辆上市天数估算价格概率分布，满足商业需求。实验结果表明，该模型在高度确定的实例上表现出色，提升了预测的可信度和实际应用价值。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.03812v1",
      "published_date": "2024-03-06 16:00:50 UTC",
      "updated_date": "2024-03-06 16:00:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:01:46.341731"
    },
    {
      "arxiv_id": "2403.03808v1",
      "title": "Confidence-Aware Decision-Making and Control for Tool Selection",
      "title_zh": "翻译失败",
      "authors": [
        "Ajith Anil Meera",
        "Pablo Lanillos"
      ],
      "abstract": "Self-reflecting about our performance (e.g., how confident we are) before\ndoing a task is essential for decision making, such as selecting the most\nsuitable tool or choosing the best route to drive. While this form of awareness\n-- thinking about our performance or metacognitive performance -- is well-known\nin humans, robots still lack this cognitive ability. This reflective monitoring\ncan enhance their embodied decision power, robustness and safety. Here, we take\na step in this direction by introducing a mathematical framework that allows\nrobots to use their control self-confidence to make better-informed decisions.\nWe derive a mathematical closed-form expression for control confidence for\ndynamic systems (i.e., the posterior inverse covariance of the control action).\nThis control confidence seamlessly integrates within an objective function for\ndecision making, that balances the: i) performance for task completion, ii)\ncontrol effort, and iii) self-confidence. To evaluate our theoretical account,\nwe framed the decision-making within the tool selection problem, where the\nagent has to select the best robot arm for a particular control task. The\nstatistical analysis of the numerical simulations with randomized 2DOF arms\nshows that using control confidence during tool selection improves both real\ntask performance, and the reliability of the tool for performance under\nunmodelled perturbations (e.g., external forces). Furthermore, our results\nindicate that control confidence is an early indicator of performance and thus,\nit can be used as a heuristic for making decisions when computation power is\nrestricted or decision-making is intractable. Overall, we show the advantages\nof using confidence-aware decision-making and control scheme for dynamic\nsystems.",
      "tldr_zh": "该论文提出了一种Confidence-Aware Decision-Making框架，允许机器人通过评估控制自信度（control self-confidence）来提升决策能力，例如在工具选择任务中选择最佳机器人臂。作者推导了动态系统的控制自信度数学闭合形式（posterior inverse covariance of the control action），并将其整合到一个目标函数中，平衡任务完成性能、控制努力和自信度。实验结果显示，该方法在随机2DOF臂模拟中显著改善了任务性能和工具可靠性，尤其在未建模扰动（如外部力）下，并证明控制自信度可作为计算资源有限时的决策启发式。总的来说，该框架增强了机器人的鲁棒性和安全性，为自主决策系统提供了新途径。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03808v1",
      "published_date": "2024-03-06 15:59:39 UTC",
      "updated_date": "2024-03-06 15:59:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:01:58.710009"
    },
    {
      "arxiv_id": "2403.03791v1",
      "title": "KG-TREAT: Pre-training for Treatment Effect Estimation by Synergizing Patient Data with Knowledge Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Ruoqi Liu",
        "Lingfei Wu",
        "Ping Zhang"
      ],
      "abstract": "Treatment effect estimation (TEE) is the task of determining the impact of\nvarious treatments on patient outcomes. Current TEE methods fall short due to\nreliance on limited labeled data and challenges posed by sparse and\nhigh-dimensional observational patient data. To address the challenges, we\nintroduce a novel pre-training and fine-tuning framework, KG-TREAT, which\nsynergizes large-scale observational patient data with biomedical knowledge\ngraphs (KGs) to enhance TEE. Unlike previous approaches, KG-TREAT constructs\ndual-focus KGs and integrates a deep bi-level attention synergy method for\nin-depth information fusion, enabling distinct encoding of treatment-covariate\nand outcome-covariate relationships. KG-TREAT also incorporates two\npre-training tasks to ensure a thorough grounding and contextualization of\npatient data and KGs. Evaluation on four downstream TEE tasks shows KG-TREAT's\nsuperiority over existing methods, with an average improvement of 7% in Area\nunder the ROC Curve (AUC) and 9% in Influence Function-based Precision of\nEstimating Heterogeneous Effects (IF-PEHE). The effectiveness of our estimated\ntreatment effects is further affirmed by alignment with established randomized\nclinical trial findings.",
      "tldr_zh": "该研究提出KG-TREAT框架，用于提升Treatment Effect Estimation (TEE)，通过整合大规模观察患者数据与生物医学Knowledge Graphs (KGs)，解决现有方法依赖有限标记数据和数据稀疏高维问题的挑战。该框架构建双焦点KGs，并采用深度双层注意力协同方法，实现治疗-协变量和结果-协变量关系的区分编码，同时引入两个预训练任务以强化患者数据和KGs的关联性。在四个下游TEE任务上的评估中，KG-TREAT比现有方法平均提高7%在Area under the ROC Curve (AUC)和9%在Influence Function-based Precision of Estimating Heterogeneous Effects (IF-PEHE)，且其估计效果与随机临床试验结果一致。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "AAAI 2024 Main Track",
      "pdf_url": "http://arxiv.org/pdf/2403.03791v1",
      "published_date": "2024-03-06 15:37:22 UTC",
      "updated_date": "2024-03-06 15:37:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:02:10.527337"
    },
    {
      "arxiv_id": "2405.00686v1",
      "title": "Technical Report on BaumEvA Evolutionary Optimization Python-Library Testing",
      "title_zh": "翻译失败",
      "authors": [
        "Vadim Tynchenko",
        "Aleksei Kudryavtsev",
        "Vladimir Nelyub",
        "Aleksei Borodulin",
        "Andrei Gantimurov"
      ],
      "abstract": "This report presents the test results Python library BaumEvA, which\nimplements evolutionary algorithms for optimizing various types of problems,\nincluding computer vision tasks accompanied by the search for optimal model\narchitectures. Testing was carried out to evaluate the effectiveness and\nreliability of the pro-posed methods, as well as to determine their\napplicability in various fields. Dur-ing testing, various test functions and\nparameters of evolutionary algorithms were used, which made it possible to\nevaluate their performance in a wide range of conditions. Test results showed\nthat the library provides effective and reliable methods for solving\noptimization problems. However, some limitations were identified related to\ncomputational resources and execution time of algorithms on problems with large\ndimensions. The report includes a detailed description of the tests performed,\nthe results obtained and conclusions about the applicability of the genetic\nalgorithm in various tasks. Recommendations for choosing algorithm pa-rameters\nand using the library to achieve the best results are also provided. The report\nmay be useful to developers involved in the optimization of complex com-puting\nsystems, as well as to researchers studying the possibilities of using\nevo-lutionary algorithms in various fields of science and technology.",
      "tldr_zh": "这篇报告评估了BaumEvA Python库，该库实现了evolutionary algorithms，用于优化各种问题，包括计算机视觉任务和模型架构搜索。测试通过使用多种测试函数和算法参数，评估了库的有效性、可靠性和在不同条件下的性能。结果显示，BaumEvA在解决优化问题方面表现出色，但存在计算资源和执行时间方面的局限性。报告提供了算法参数选择的推荐，并适用于优化复杂计算系统的开发者和研究evolutionary algorithms的应用研究。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV",
        "65K10",
        "I.2.8; I.2.5; G.4"
      ],
      "primary_category": "cs.NE",
      "comment": "The paper consists of 30 pages, 37 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2405.00686v1",
      "published_date": "2024-03-06 15:34:31 UTC",
      "updated_date": "2024-03-06 15:34:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:02:22.649244"
    },
    {
      "arxiv_id": "2403.03781v1",
      "title": "Neural Architecture Search using Particle Swarm and Ant Colony Optimization",
      "title_zh": "使用粒子群优化和蚁群优化的神经架构搜索",
      "authors": [
        "Séamus Lankford",
        "Diarmuid Grimes"
      ],
      "abstract": "Neural network models have a number of hyperparameters that must be chosen\nalong with their architecture. This can be a heavy burden on a novice user,\nchoosing which architecture and what values to assign to parameters. In most\ncases, default hyperparameters and architectures are used. Significant\nimprovements to model accuracy can be achieved through the evaluation of\nmultiple architectures. A process known as Neural Architecture Search (NAS) may\nbe applied to automatically evaluate a large number of such architectures. A\nsystem integrating open source tools for Neural Architecture Search (OpenNAS),\nin the classification of images, has been developed as part of this research.\nOpenNAS takes any dataset of grayscale, or RBG images, and generates\nConvolutional Neural Network (CNN) architectures based on a range of\nmetaheuristics using either an AutoKeras, a transfer learning or a Swarm\nIntelligence (SI) approach. Particle Swarm Optimization (PSO) and Ant Colony\nOptimization (ACO) are used as the SI algorithms. Furthermore, models developed\nthrough such metaheuristics may be combined using stacking ensembles. In the\ncontext of this paper, we focus on training and optimizing CNNs using the Swarm\nIntelligence (SI) components of OpenNAS. Two major types of SI algorithms,\nnamely PSO and ACO, are compared to see which is more effective in generating\nhigher model accuracies. It is shown, with our experimental design, that the\nPSO algorithm performs better than ACO. The performance improvement of PSO is\nmost notable with a more complex dataset. As a baseline, the performance of\nfine-tuned pre-trained models is also evaluated.",
      "tldr_zh": "这篇论文探讨了使用 Particle Swarm Optimization (PSO) 和 Ant Colony Optimization (ACO) 进行 Neural Architecture Search (NAS)，旨在自动优化神经网络架构以提高图像分类准确率。研究开发了 OpenNAS 系统，该系统支持多种元启发式算法生成 Convolutional Neural Network (CNN) 架构，并比较了 PSO 和 ACO 在处理灰度或 RGB 图像数据集时的表现。实验结果表明，PSO 优于 ACO，尤其在复杂数据集上表现出更高的模型准确率；此外，Swarm Intelligence (SI) 方法的性能与微调预训练模型的基线相比也具有显著改进。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03781v1",
      "published_date": "2024-03-06 15:23:26 UTC",
      "updated_date": "2024-03-06 15:23:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:02:36.206729"
    },
    {
      "arxiv_id": "2403.03777v4",
      "title": "ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport",
      "title_zh": "ENOT：期望分位正则化用于神经最优传输的",
      "authors": [
        "Nazar Buzun",
        "Maksim Bobrin",
        "Dmitry V. Dylov"
      ],
      "abstract": "We present a new approach for Neural Optimal Transport (NOT) training\nprocedure, capable of accurately and efficiently estimating optimal\ntransportation plan via specific regularization on dual Kantorovich potentials.\nThe main bottleneck of existing NOT solvers is associated with the procedure of\nfinding a near-exact approximation of the conjugate operator (i.e., the\nc-transform), which is done either by optimizing over non-convex max-min\nobjectives or by the computationally intensive fine-tuning of the initial\napproximated prediction. We resolve both issues by proposing a new,\ntheoretically justified loss in the form of expectile regularisation which\nenforces binding conditions on the learning process of dual potentials. Such a\nregularization provides the upper bound estimation over the distribution of\npossible conjugate potentials and makes the learning stable, completely\neliminating the need for additional extensive fine-tuning. Proposed method,\ncalled Expectile-Regularised Neural Optimal Transport (ENOT), outperforms\nprevious state-of-the-art approaches on the established Wasserstein-2 benchmark\ntasks by a large margin (up to a 3-fold improvement in quality and up to a\n10-fold improvement in runtime). Moreover, we showcase performance of ENOT for\nvarying cost functions on different tasks such as image generation, showing\nrobustness of proposed algorithm. OTT-JAX library includes our implementation\nof ENOT algorithm https://ott-jax.readthedocs.io/en/latest/tutorials/ENOT.html",
      "tldr_zh": "本文提出 ENOT 方法，即 Expectile-Regularized Neural Optimal Transport，通过引入 expectile regularization 作为新的损失函数，来精确高效地训练 Neural Optimal Transport（NOT），从而避免了传统方法的非凸优化和大量微调问题。ENOT 通过强制绑定条件提供 conjugate potentials 的上界估计，确保学习过程稳定且快速。在 Wasserstein-2 基准任务上，该方法比现有方法质量提升 3 倍、运行时间缩短 10 倍，并在图像生成等任务中展现出鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03777v4",
      "published_date": "2024-03-06 15:15:42 UTC",
      "updated_date": "2024-10-18 01:26:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:02:48.289087"
    },
    {
      "arxiv_id": "2403.03768v3",
      "title": "DeepCRE: Transforming Drug R&D via AI-Driven Cross-drug Response Evaluation",
      "title_zh": "DeepCRE：通过 AI 驱动的跨药物响应评估转变药物研发",
      "authors": [
        "Yushuai Wu",
        "Ting Zhang",
        "Hao Zhou",
        "Hainan Wu",
        "Hanwen Sunchu",
        "Lei Hu",
        "Xiaofang Chen",
        "Suyuan Zhao",
        "Gaochao Liu",
        "Chao Sun",
        "Jiahuan Zhang",
        "Yizhen Luo",
        "Peng Liu",
        "Zaiqing Nie",
        "Yushuai Wu"
      ],
      "abstract": "The fields of therapeutic application and drug research and development (R&D)\nboth face substantial challenges, i.e., the therapeutic domain calls for more\ntreatment alternatives, while numerous promising pre-clinical drugs have failed\nin clinical trials. One of the reasons is the inadequacy of Cross-drug Response\nEvaluation (CRE) during the late stages of drug R&D. Although in-silico CRE\nmodels bring a promising solution, existing methodologies are restricted to\nearly stages of drug R&D, such as target and cell-line levels, offering limited\nimprovement to clinical success rates. Herein, we introduce DeepCRE, a\npioneering AI model designed to predict CRE effectively in the late stages of\ndrug R&D. DeepCRE outperforms the existing best models by achieving an average\nperformance improvement of 17.7% in patient-level CRE, and a 5-fold increase in\nindication-level CRE, facilitating more accurate personalized treatment\npredictions and better pharmaceutical value assessment for indications,\nrespectively. Furthermore, DeepCRE has identified a set of six drug candidates\nthat show significantly greater effectiveness than a comparator set of two\napproved drugs in 5/8 colorectal cancer organoids. This demonstrates the\ncapability of DeepCRE to systematically uncover a spectrum of drug candidates\nwith enhanced therapeutic effects, highlighting its potential to transform drug\nR&D.",
      "tldr_zh": "该论文针对药物研发（R&D）中的临床失败挑战，引入DeepCRE，一种AI驱动的模型，用于在后期阶段预测Cross-drug Response Evaluation (CRE)。DeepCRE在患者级别CRE上比现有最佳模型提升了17.7%的性能，并在适应症级别CRE上实现了5倍的改进，从而实现更准确的个性化治疗预测和药物价值评估。实验结果显示，DeepCRE识别的六种药物候选物在5/8结肠癌类器官中表现出显著优于两款已批准药物的疗效，展示了其潜力在转变药物R&D方面。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "q-bio.QM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03768v3",
      "published_date": "2024-03-06 15:03:09 UTC",
      "updated_date": "2024-03-18 15:05:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:02:58.918305"
    },
    {
      "arxiv_id": "2403.05593v2",
      "title": "Introducing First-Principles Calculations: New Approach to Group Dynamics and Bridging Social Phenomena in TeNP-Chain Based Social Dynamics Simulations",
      "title_zh": "翻译失败",
      "authors": [
        "Yasuko Kawahata"
      ],
      "abstract": "This note considers an innovative interdisciplinary methodology that bridges\nthe gap between the fundamental principles of quantum mechanics applied to the\nstudy of materials such as tellurium nanoparticles (TeNPs) and graphene and the\ncomplex dynamics of social systems. The basis for this approach lies in the\nmetaphorical parallels drawn between the structural features of TeNPs and\ngraphene and the behavioral patterns of social groups in the face of\nmisinformation. TeNPs exhibit unique properties such as the strengthening of\ncovalent bonds within telluric chains and the disruption of secondary structure\nleading to the separation of these chains. This is analogous to increased\ncohesion within social groups and disruption of information flow between\ndifferent subgroups, respectively. . Similarly, the outstanding properties of\ngraphene, such as high electrical conductivity, strength, and flexibility,\nprovide additional aspects for understanding the resilience and adaptability of\nsocial structures in response to external stimuli such as fake news. This\nresearch note proposes a novel metaphorical framework for analyzing the spread\nof fake news within social groups, analogous to the structural features of\ntelluric nanoparticles (TeNPs). We investigate how the strengthening of\ncovalent bonds within TeNPs reflects the strengthening of social cohesion in\ngroups that share common beliefs and values. This paper is partially an attempt\nto utilize \"Generative AI\" and was written with educational intent. There are\ncurrently no plans for it to become a peer-reviewed paper.",
      "tldr_zh": "该论文引入了一种创新的跨学科方法，将量子力学的First-Principles Calculations应用于材料如TeNPs（碲纳米粒子）和graphene（石墨烯）的研究，并通过比喻框架桥接这些材料的结构特性与社会动态。作者将TeNPs中共价键的加强比作社会群体在面对错误信息时内部凝聚力的增强，以及TeNPs链的分离比作信息流的中断；同时，graphene的导电性、强度和柔韧性被类比为社会结构的韧性和适应性。该框架旨在分析假新闻在社会群体中的传播机制，并以教育为目的部分利用Generative AI撰写，目前无计划成为同行评议论文。",
      "categories": [
        "physics.soc-ph",
        "cs.AI",
        "physics.ed-ph"
      ],
      "primary_category": "physics.soc-ph",
      "comment": "TeNP Chains, First-principles calculations, Tellurium nanoparticles\n  (TeNPs), Graphene, Fake news dissemination, Social cohesion, Information Flow\n  Disruption, Quantum Mechanics, Interdisciplinary approach, Misinformation\n  mitigation",
      "pdf_url": "http://arxiv.org/pdf/2403.05593v2",
      "published_date": "2024-03-06 15:00:11 UTC",
      "updated_date": "2024-04-19 14:59:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:03:12.714819"
    },
    {
      "arxiv_id": "2403.03750v2",
      "title": "German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Laura Mascarell",
        "Ribin Chalumattu",
        "Annette Rios"
      ],
      "abstract": "The advent of Large Language Models (LLMs) has led to remarkable progress on\na wide range of natural language processing tasks. Despite the advances, these\nlarge-sized models still suffer from hallucinating information in their output,\nwhich poses a major issue in automatic text summarization, as we must guarantee\nthat the generated summary is consistent with the content of the source\ndocument. Previous research addresses the challenging task of detecting\nhallucinations in the output (i.e. inconsistency detection) in order to\nevaluate the faithfulness of the generated summaries. However, these works\nprimarily focus on English and recent multilingual approaches lack German data.\nThis work presents absinth, a manually annotated dataset for hallucination\ndetection in German news summarization and explores the capabilities of novel\nopen-source LLMs on this task in both fine-tuning and in-context learning\nsettings. We open-source and release the absinth dataset to foster further\nresearch on hallucination detection in German.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 在新闻摘要中产生幻觉（hallucinations）的问题，即生成的摘要与源文档不一致，强调了现有研究主要针对英语而忽略德语的不足。研究者引入了 absinth 数据集，这是一个手动标注的德语新闻摘要数据集，用于支持幻觉检测任务。论文通过微调 (fine-tuning) 和上下文学习 (in-context learning) 设置，评估了新型开源 LLMs 的性能，并开源了数据集，以推动德语幻觉检测的进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 2 figures, 7 tables, conference: Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation\n  (LREC-COLING 2024), Turin, Italy, May 20-25, 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.03750v2",
      "published_date": "2024-03-06 14:37:30 UTC",
      "updated_date": "2024-03-14 12:30:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:03:24.521749"
    },
    {
      "arxiv_id": "2403.03744v5",
      "title": "MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language Models",
      "title_zh": "MedSafetyBench：评估和改进大语言模型的医疗安全",
      "authors": [
        "Tessa Han",
        "Aounon Kumar",
        "Chirag Agarwal",
        "Himabindu Lakkaraju"
      ],
      "abstract": "As large language models (LLMs) develop increasingly sophisticated\ncapabilities and find applications in medical settings, it becomes important to\nassess their medical safety due to their far-reaching implications for personal\nand public health, patient safety, and human rights. However, there is little\nto no understanding of the notion of medical safety in the context of LLMs, let\nalone how to evaluate and improve it. To address this gap, we first define the\nnotion of medical safety in LLMs based on the Principles of Medical Ethics set\nforth by the American Medical Association. We then leverage this understanding\nto introduce MedSafetyBench, the first benchmark dataset designed to measure\nthe medical safety of LLMs. We demonstrate the utility of MedSafetyBench by\nusing it to evaluate and improve the medical safety of LLMs. Our results show\nthat publicly-available medical LLMs do not meet standards of medical safety\nand that fine-tuning them using MedSafetyBench improves their medical safety\nwhile preserving their medical performance. By introducing this new benchmark\ndataset, our work enables a systematic study of the state of medical safety in\nLLMs and motivates future work in this area, paving the way to mitigate the\nsafety risks of LLMs in medicine. The benchmark dataset and code are available\nat https://github.com/AI4LIFE-GROUP/med-safety-bench.",
      "tldr_zh": "该研究针对大型语言模型（LLMs）在医疗领域的应用，定义了基于美国医疗协会的医疗伦理原则的医疗安全概念，并引入了MedSafetyBench，这是首个专为评估LLMs医疗安全而设计的基准数据集。通过MedSafetyBench，研究者评估了公开医疗LLMs的表现，发现这些模型未达到医疗安全标准。进一步微调这些模型后，其医疗安全性得到改善，同时保留了原有医疗性能，为系统研究LLMs的医疗风险提供工具，并推动未来安全改进工作。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03744v5",
      "published_date": "2024-03-06 14:34:07 UTC",
      "updated_date": "2024-10-09 17:22:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:03:34.820654"
    },
    {
      "arxiv_id": "2403.03741v1",
      "title": "SUPClust: Active Learning at the Boundaries",
      "title_zh": "翻译失败",
      "authors": [
        "Yuta Ono",
        "Till Aczel",
        "Benjamin Estermann",
        "Roger Wattenhofer"
      ],
      "abstract": "Active learning is a machine learning paradigm designed to optimize model\nperformance in a setting where labeled data is expensive to acquire. In this\nwork, we propose a novel active learning method called SUPClust that seeks to\nidentify points at the decision boundary between classes. By targeting these\npoints, SUPClust aims to gather information that is most informative for\nrefining the model's prediction of complex decision regions. We demonstrate\nexperimentally that labeling these points leads to strong model performance.\nThis improvement is observed even in scenarios characterized by strong class\nimbalance.",
      "tldr_zh": "本文提出了一种名为SUPClust的主动学习（Active Learning）方法，旨在通过识别类间决策边界（decision boundary）上的点来优化模型性能，从而获取最有信息量的标注数据。SUPClust针对复杂决策区域进行精炼，即使在类不平衡场景下，也能显著提升模型的预测准确性。实验验证显示，该方法在各种条件下均表现出色，为主动学习应用提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICLR 2024 Workshop on Practical Machine Learning for Low\n  Resource Settings (PML4LRS)",
      "pdf_url": "http://arxiv.org/pdf/2403.03741v1",
      "published_date": "2024-03-06 14:30:09 UTC",
      "updated_date": "2024-03-06 14:30:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:03:46.243811"
    },
    {
      "arxiv_id": "2403.03739v1",
      "title": "A&B BNN: Add&Bit-Operation-Only Hardware-Friendly Binary Neural Network",
      "title_zh": "翻译失败",
      "authors": [
        "Ruichen Ma",
        "Guanchao Qiao",
        "Yian Liu",
        "Liwei Meng",
        "Ning Ning",
        "Yang Liu",
        "Shaogang Hu"
      ],
      "abstract": "Binary neural networks utilize 1-bit quantized weights and activations to\nreduce both the model's storage demands and computational burden. However,\nadvanced binary architectures still incorporate millions of inefficient and\nnonhardware-friendly full-precision multiplication operations. A&B BNN is\nproposed to directly remove part of the multiplication operations in a\ntraditional BNN and replace the rest with an equal number of bit operations,\nintroducing the mask layer and the quantized RPReLU structure based on the\nnormalizer-free network architecture. The mask layer can be removed during\ninference by leveraging the intrinsic characteristics of BNN with\nstraightforward mathematical transformations to avoid the associated\nmultiplication operations. The quantized RPReLU structure enables more\nefficient bit operations by constraining its slope to be integer powers of 2.\nExperimental results achieved 92.30%, 69.35%, and 66.89% on the CIFAR-10,\nCIFAR-100, and ImageNet datasets, respectively, which are competitive with the\nstate-of-the-art. Ablation studies have verified the efficacy of the quantized\nRPReLU structure, leading to a 1.14% enhancement on the ImageNet compared to\nusing a fixed slope RLeakyReLU. The proposed add&bit-operation-only BNN offers\nan innovative approach for hardware-friendly network architecture.",
      "tldr_zh": "该论文提出了一种名为 A&B BNN 的硬件友好型二进制神经网络（Binary Neural Networks），通过直接移除部分乘法操作并用等量的位操作替换，显著降低计算负担。创新点包括引入 mask layer（可通过数学变换在推理阶段移除）和 quantized RPReLU 结构（将斜率限制为 2 的整数幂），基于 normalizer-free network 架构实现高效处理。实验结果显示，在 CIFAR-10、CIFAR-100 和 ImageNet 数据集上分别达到 92.30%、69.35% 和 66.89% 的准确率，与最先进方法竞争，且 quantized RPReLU 结构通过消融实验证明可提升 1.14% 的 ImageNet 性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "CVPR 2024 Accepted",
      "pdf_url": "http://arxiv.org/pdf/2403.03739v1",
      "published_date": "2024-03-06 14:28:49 UTC",
      "updated_date": "2024-03-06 14:28:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:04:01.084841"
    },
    {
      "arxiv_id": "2403.03730v1",
      "title": "Learning 3D object-centric representation through prediction",
      "title_zh": "翻译失败",
      "authors": [
        "John Day",
        "Tushar Arora",
        "Jirui Liu",
        "Li Erran Li",
        "Ming Bo Cai"
      ],
      "abstract": "As part of human core knowledge, the representation of objects is the\nbuilding block of mental representation that supports high-level concepts and\nsymbolic reasoning. While humans develop the ability of perceiving objects\nsituated in 3D environments without supervision, models that learn the same set\nof abilities with similar constraints faced by human infants are lacking.\nTowards this end, we developed a novel network architecture that simultaneously\nlearns to 1) segment objects from discrete images, 2) infer their 3D locations,\nand 3) perceive depth, all while using only information directly available to\nthe brain as training data, namely: sequences of images and self-motion. The\ncore idea is treating objects as latent causes of visual input which the brain\nuses to make efficient predictions of future scenes. This results in object\nrepresentations being learned as an essential byproduct of learning to predict.",
      "tldr_zh": "该论文探讨了人类无监督学习3D物体中心表示的过程，并提出了一种新型network architecture，通过预测未来场景来实现类似能力。该架构同时学习从离散图像中segment objects、推断物体的3D locations以及感知depth，仅使用图像序列和self-motion作为训练数据。核心思想是将物体视为视觉输入的潜在原因，从而使物体表示成为学习预测的自然副产品，这为开发更接近人类认知的AI模型提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "I.2.10; I.4.8; I.4.6; I.4.10; I.2.6"
      ],
      "primary_category": "cs.CV",
      "comment": "21 pages, 11 figures. Project webpage can be found at\n  https://jday54.github.io/opple_site/",
      "pdf_url": "http://arxiv.org/pdf/2403.03730v1",
      "published_date": "2024-03-06 14:19:11 UTC",
      "updated_date": "2024-03-06 14:19:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:04:11.115664"
    },
    {
      "arxiv_id": "2403.03728v2",
      "title": "Bridging Diversity and Uncertainty in Active learning with Self-Supervised Pre-Training",
      "title_zh": "翻译失败",
      "authors": [
        "Paul Doucet",
        "Benjamin Estermann",
        "Till Aczel",
        "Roger Wattenhofer"
      ],
      "abstract": "This study addresses the integration of diversity-based and uncertainty-based\nsampling strategies in active learning, particularly within the context of\nself-supervised pre-trained models. We introduce a straightforward heuristic\ncalled TCM that mitigates the cold start problem while maintaining strong\nperformance across various data levels. By initially applying TypiClust for\ndiversity sampling and subsequently transitioning to uncertainty sampling with\nMargin, our approach effectively combines the strengths of both strategies. Our\nexperiments demonstrate that TCM consistently outperforms existing methods\nacross various datasets in both low and high data regimes.",
      "tldr_zh": "本研究探讨了在自监督预训练(Self-Supervised Pre-Training)模型中整合多样性和不确定性采样策略的主动学习(Active Learning)问题。作者提出了一种简单启发式方法TCM，通过初始采用TypiClust进行多样性采样，随后切换到Margin的不确定性采样，从而缓解冷启动问题并在不同数据水平上保持高性能。实验结果表明，TCM在各种数据集的低数据和高数据场景中，都比现有方法表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICLR 2024 Workshop on Practical Machine Learning for Low\n  Resource Settings (PML4LRS)",
      "pdf_url": "http://arxiv.org/pdf/2403.03728v2",
      "published_date": "2024-03-06 14:18:24 UTC",
      "updated_date": "2025-01-17 15:15:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:04:24.159777"
    },
    {
      "arxiv_id": "2403.03726v2",
      "title": "Diffusion on language model encodings for protein sequence generation",
      "title_zh": "翻译失败",
      "authors": [
        "Viacheslav Meshchaninov",
        "Pavel Strashnov",
        "Andrey Shevtsov",
        "Fedor Nikolaev",
        "Nikita Ivanisenko",
        "Olga Kardymon",
        "Dmitry Vetrov"
      ],
      "abstract": "Protein sequence design has seen significant advances through discrete\ndiffusion and autoregressive approaches, yet the potential of continuous\ndiffusion remains underexplored. Here, we present DiMA, a latent diffusion\nframework that operates on protein language model representations. Through\nsystematic exploration of architectural choices and diffusion components, we\ndevelop a robust methodology that generalizes across multiple protein encoders\nranging from 8M to 3B parameters. We demonstrate that our framework achieves\nconsistently high performance across sequence-only (ESM-2, ESMc),\ndual-decodable (CHEAP), and multimodal (SaProt) representations using the same\narchitecture and training approach. We extensively evaluate existing methods\nalongside DiMA using multiple metrics across two protein modalities, covering\nquality, diversity, novelty, and distribution matching of generated proteins.\nDiMA consistently produces novel, high-quality and diverse protein sequences\nand achieves strong results compared to baselines such as autoregressive,\ndiscrete diffusion and flow matching language models. The model demonstrates\nversatile functionality, supporting conditional generation tasks including\nprotein family-generation, motif scaffolding and infilling, and fold-specific\nsequence design. This work provides a universal continuous diffusion framework\nfor protein sequence generation, offering both architectural insights and\npractical applicability across various protein design scenarios.",
      "tldr_zh": "本研究提出DiMA，一种基于潜在扩散(latent diffusion)框架，用于蛋白质序列生成，通过在蛋白质语言模型表示上操作，实现高效的序列设计。研究系统探索了架构选择和扩散组件，使DiMA适用于多种编码器（如ESM-2、ESMc、CHEAP和SaProt），并在相同架构和训练方法下保持高性能。实验结果显示，DiMA在质量、多样性、新颖性和分布匹配等方面优于基线模型（如autoregressive和discrete diffusion方法），并支持条件生成任务，包括蛋白质家族生成、motif scaffolding和infilling，以及fold-specific序列设计。该框架为蛋白质设计提供了一个通用的连续扩散解决方案，具有广泛的实际应用潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03726v2",
      "published_date": "2024-03-06 14:15:20 UTC",
      "updated_date": "2025-02-05 08:26:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:04:36.023032"
    },
    {
      "arxiv_id": "2403.03698v1",
      "title": "Towards Controllable Time Series Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yifan Bao",
        "Yihao Ang",
        "Qiang Huang",
        "Anthony K. H. Tung",
        "Zhiyong Huang"
      ],
      "abstract": "Time Series Generation (TSG) has emerged as a pivotal technique in\nsynthesizing data that accurately mirrors real-world time series, becoming\nindispensable in numerous applications. Despite significant advancements in\nTSG, its efficacy frequently hinges on having large training datasets. This\ndependency presents a substantial challenge in data-scarce scenarios,\nespecially when dealing with rare or unique conditions. To confront these\nchallenges, we explore a new problem of Controllable Time Series Generation\n(CTSG), aiming to produce synthetic time series that can adapt to various\nexternal conditions, thereby tackling the data scarcity issue.\n  In this paper, we propose \\textbf{C}ontrollable \\textbf{T}ime \\textbf{S}eries\n(\\textsf{CTS}), an innovative VAE-agnostic framework tailored for CTSG. A key\nfeature of \\textsf{CTS} is that it decouples the mapping process from standard\nVAE training, enabling precise learning of a complex interplay between latent\nfeatures and external conditions. Moreover, we develop a comprehensive\nevaluation scheme for CTSG. Extensive experiments across three real-world time\nseries datasets showcase \\textsf{CTS}'s exceptional capabilities in generating\nhigh-quality, controllable outputs. This underscores its adeptness in\nseamlessly integrating latent features with external conditions. Extending\n\\textsf{CTS} to the image domain highlights its remarkable potential for\nexplainability and further reinforces its versatility across different\nmodalities.",
      "tldr_zh": "这篇论文针对时间序列生成 (TSG) 在数据稀缺场景下的挑战，提出了可控时间序列生成 (CTSG) 的新问题，旨在通过适应外部条件来生成高质量合成数据。作者开发了 \\textsf{CTS}，一个 VAE-agnostic 框架，该框架通过解耦映射过程，精确学习潜在特征与外部条件的复杂互动，并设计了全面的评估方案。在三个真实世界数据集上的实验证明了 \\textsf{CTS} 的出色性能，并扩展到图像领域，展示了其解释性和多模态适应性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 13 figures, and 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.03698v1",
      "published_date": "2024-03-06 13:27:34 UTC",
      "updated_date": "2024-03-06 13:27:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:04:49.700474"
    },
    {
      "arxiv_id": "2403.05592v1",
      "title": "Eternal Sunshine of the Mechanical Mind: The Irreconcilability of Machine Learning and the Right to be Forgotten",
      "title_zh": "翻译失败",
      "authors": [
        "Meem Arafat Manab"
      ],
      "abstract": "As we keep rapidly advancing toward an era where artificial intelligence is a\nconstant and normative experience for most of us, we must also be aware of what\nthis vision and this progress entail. By first approximating neural connections\nand activities in computer circuits and then creating more and more\nsophisticated versions of this crude approximation, we are now facing an age to\ncome where modern deep learning-based artificial intelligence systems can\nrightly be called thinking machines, and they are sometimes even lauded for\ntheir emergent behavior and black-box approaches. But as we create more\npowerful electronic brains, with billions of neural connections and parameters,\ncan we guarantee that these mammoths built of artificial neurons will be able\nto forget the data that we store in them? If they are at some level like a\nbrain, can the right to be forgotten still be protected while dealing with\nthese AIs? The essential gap between machine learning and the RTBF is explored\nin this article, with a premonition of far-reaching conclusions if the gap is\nnot bridged or reconciled any time soon. The core argument is that deep\nlearning models, due to their structure and size, cannot be expected to forget\nor delete a data as it would be expected from a tabular database, and they\nshould be treated more like a mechanical brain, albeit still in development.",
      "tldr_zh": "这篇论文探讨了机器学习与 Right to be Forgotten (RTBF) 的不可调和性，随着人工智能的快速发展，深度学习模型的复杂结构使其难以像传统数据库那样删除数据。作者通过比较神经网络与大脑的相似性，论证这些“机械大脑”无法真正“忘记”存储的信息，从而威胁到RTBF的保护。论文警告，如果不尽快弥合这一差距，将导致远-reaching conclusions，包括对隐私权和AI监管的深远影响。",
      "categories": [
        "cs.GL",
        "cs.AI",
        "68P27",
        "K.4.1; K.5.2; I.2.0"
      ],
      "primary_category": "cs.GL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05592v1",
      "published_date": "2024-03-06 13:23:57 UTC",
      "updated_date": "2024-03-06 13:23:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:04:59.748771"
    },
    {
      "arxiv_id": "2403.03691v3",
      "title": "MolNexTR: A Generalized Deep Learning Model for Molecular Image Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Yufan Chen",
        "Ching Ting Leung",
        "Yong Huang",
        "Jianwei Sun",
        "Hao Chen",
        "Hanyu Gao"
      ],
      "abstract": "In the field of chemical structure recognition, the task of converting\nmolecular images into machine-readable data formats such as SMILES string\nstands as a significant challenge, primarily due to the varied drawing styles\nand conventions prevalent in chemical literature. To bridge this gap, we\nproposed MolNexTR, a novel image-to-graph deep learning model that collaborates\nto fuse the strengths of ConvNext, a powerful Convolutional Neural Network\nvariant, and Vision-TRansformer. This integration facilitates a more detailed\nextraction of both local and global features from molecular images. MolNexTR\ncan predict atoms and bonds simultaneously and understand their layout rules.\nIt also excels at flexibly integrating symbolic chemistry principles to discern\nchirality and decipher abbreviated structures. We further incorporate a series\nof advanced algorithms, including an improved data augmentation module, an\nimage contamination module, and a post-processing module for getting the final\nSMILES output. These modules cooperate to enhance the model's robustness to\ndiverse styles of molecular images found in real literature. In our test sets,\nMolNexTR has demonstrated superior performance, achieving an accuracy rate of\n81-97%, marking a significant advancement in the domain of molecular structure\nrecognition.",
      "tldr_zh": "该研究提出MolNexTR，一种通用的深度学习模型，用于将分子图像转换为机器可读格式如SMILES字符串，旨在应对化学文献中多样绘图风格的挑战。MolNexTR结合ConvNext和Vision Transformer的优势，实现对分子图像局部和全局特征的精确提取，同时预测原子和键、理解布局规则，并通过符号化学原理灵活识别手性和缩写结构。模型还整合了改进的数据增强模块、图像污染模块和后处理模块，提升了对真实文献多样风格的鲁棒性；在测试集上，MolNexTR实现了81-97%的准确率，显著推进了分子结构识别领域。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03691v3",
      "published_date": "2024-03-06 13:17:41 UTC",
      "updated_date": "2024-08-28 03:57:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:05:11.379139"
    },
    {
      "arxiv_id": "2403.03690v1",
      "title": "Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese",
      "title_zh": "翻译失败",
      "authors": [
        "Yikun Sun",
        "Zhen Wan",
        "Nobuhiro Ueda",
        "Sakiko Yahata",
        "Fei Cheng",
        "Chenhui Chu",
        "Sadao Kurohashi"
      ],
      "abstract": "The creation of instruction data and evaluation benchmarks for serving Large\nlanguage models often involves enormous human annotation. This issue becomes\nparticularly pronounced when rapidly developing such resources for a\nnon-English language like Japanese. Instead of following the popular practice\nof directly translating existing English resources into Japanese (e.g.,\nJapanese-Alpaca), we propose an efficient self-instruct method based on GPT-4.\nWe first translate a small amount of English instructions into Japanese and\npost-edit them to obtain native-level quality. GPT-4 then utilizes them as\ndemonstrations to automatically generate Japanese instruction data. We also\nconstruct an evaluation benchmark containing 80 questions across 8 categories,\nusing GPT-4 to automatically assess the response quality of LLMs without human\nreferences. The empirical results suggest that the models fine-tuned on our\nGPT-4 self-instruct data significantly outperformed the Japanese-Alpaca across\nall three base pre-trained models. Our GPT-4 self-instruct data allowed the\nLLaMA 13B model to defeat GPT-3.5 (Davinci-003) with a 54.37\\% win-rate. The\nhuman evaluation exhibits the consistency between GPT-4's assessments and human\npreference. Our high-quality instruction data and evaluation benchmark have\nbeen released here.",
      "tldr_zh": "该研究针对非英语语言（如日语）开发高质量指令数据和评估基准的难题，提出了一种基于 GPT-4 的高效 self-instruct 方法，以最小化人力标注需求。具体而言，该方法先将少量英语指令翻译并编辑成母语级别日语，然后利用这些作为演示，让 GPT-4 自动生成指令数据，并构建一个包含 80 个问题的评估基准，使用 GPT-4 进行无参考自动评估。实验结果显示，使用该数据微调的模型在三个基础预训练模型上显著优于 Japanese-Alpaca，其中 LLaMA 13B 模型击败了 GPT-3.5（胜率 54.37%），且 GPT-4 的评估与人类偏好高度一致。该高质量指令数据和评估基准已公开发布。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "COLING 2024. Our code are available here:\n  \\href{https://github.com/hitoshizuku7/awesome-Ja-self-instruct}{self-instruct\n  data} and \\href{https://github.com/ku-nlp/ja-vicuna-qa-benchmark}{evaluation\n  benchmark}",
      "pdf_url": "http://arxiv.org/pdf/2403.03690v1",
      "published_date": "2024-03-06 13:17:07 UTC",
      "updated_date": "2024-03-06 13:17:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:05:28.111135"
    },
    {
      "arxiv_id": "2403.03689v2",
      "title": "General2Specialized LLMs Translation for E-commerce",
      "title_zh": "翻译失败",
      "authors": [
        "Kaidi Chen",
        "Ben Chen",
        "Dehong Gao",
        "Huangyu Dai",
        "Wen Jiang",
        "Wei Ning",
        "Shanqing Yu",
        "Libin Yang",
        "Xiaoyan Cai"
      ],
      "abstract": "Existing Neural Machine Translation (NMT) models mainly handle translation in\nthe general domain, while overlooking domains with special writing formulas,\nsuch as e-commerce and legal documents. Taking e-commerce as an example, the\ntexts usually include amounts of domain-related words and have more grammar\nproblems, which leads to inferior performances of current NMT methods. To\naddress these problems, we collect two domain-related resources, including a\nset of term pairs (aligned Chinese-English bilingual terms) and a parallel\ncorpus annotated for the e-commerce domain. Furthermore, we propose a two-step\nfine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to\ntransfer one general NMT model to the specialized NMT model for e-commerce. The\nparadigm can be used for the NMT models based on Large language models (LLMs).\nExtensive evaluations on real e-commerce titles demonstrate the superior\ntranslation quality and robustness of our G2ST approach, as compared with\nstate-of-the-art NMT models such as LLaMA, Qwen, GPT-3.5, and even GPT-4.",
      "tldr_zh": "现有 NMT 模型在通用领域表现良好，但对电子商务等特定领域（如包含大量领域相关词汇和语法问题）的翻译效果较差。论文收集了术语对（aligned Chinese-English bilingual terms）和电子商务平行语料库，并提出 G2ST 两步微调范式，利用 self-contrastive semantic enhancement 将通用 NMT 模型转移到专用模型，尤其适用于基于 LLMs 的模型。实验在真实电子商务标题上证明，G2ST 比 LLaMA、Qwen、GPT-3.5 和 GPT-4 等 state-of-the-art 模型具有更高的翻译质量和鲁棒性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "4 pages, 1 figure, WWW2024 accepted",
      "pdf_url": "http://arxiv.org/pdf/2403.03689v2",
      "published_date": "2024-03-06 13:15:21 UTC",
      "updated_date": "2024-04-06 04:07:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:05:36.873837"
    },
    {
      "arxiv_id": "2403.15413v1",
      "title": "Playing With Neuroscience: Past, Present and Future of Neuroimaging and Games",
      "title_zh": "翻译失败",
      "authors": [
        "Paolo Burelli",
        "Laurits Dixen"
      ],
      "abstract": "Videogames have been a catalyst for advances in many research fields, such as\nartificial intelligence, human-computer interaction or virtual reality. Over\nthe years, research in fields such as artificial intelligence has enabled the\ndesign of new types of games, while games have often served as a powerful tool\nfor testing and simulation. Can this also happen with neuroscience? What is the\ncurrent relationship between neuroscience and games research? what can we\nexpect from the future? In this article, we'll try to answer these questions,\nanalysing the current state-of-the-art at the crossroads between neuroscience\nand games and envisioning future directions.",
      "tldr_zh": "本文回顾了视频游戏在推动神经科学研究方面的作用，特别是与artificial intelligence、human-computer interaction和virtual reality的交叉影响。论文分析了当前neuroimaging和游戏研究的现状，包括游戏如何作为测试和模拟工具应用于神经科学领域。展望未来，该研究预测了潜在的发展方向，如更紧密的整合可能带来的创新和应用前景。",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15413v1",
      "published_date": "2024-03-06 12:38:18 UTC",
      "updated_date": "2024-03-06 12:38:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:05:46.920951"
    },
    {
      "arxiv_id": "2403.03645v1",
      "title": "K-Link: Knowledge-Link Graph from LLMs for Enhanced Representation Learning in Multivariate Time-Series Data",
      "title_zh": "翻译失败",
      "authors": [
        "Yucheng Wang",
        "Ruibing Jin",
        "Min Wu",
        "Xiaoli Li",
        "Lihua Xie",
        "Zhenghua Chen"
      ],
      "abstract": "Sourced from various sensors and organized chronologically, Multivariate\nTime-Series (MTS) data involves crucial spatial-temporal dependencies, e.g.,\ncorrelations among sensors. To capture these dependencies, Graph Neural\nNetworks (GNNs) have emerged as powerful tools, yet their effectiveness is\nrestricted by the quality of graph construction from MTS data. Typically,\nexisting approaches construct graphs solely from MTS signals, which may\nintroduce bias due to a small training dataset and may not accurately represent\nunderlying dependencies. To address this challenge, we propose a novel\nframework named K-Link, leveraging Large Language Models (LLMs) to encode\nextensive general knowledge and thereby providing effective solutions to reduce\nthe bias. Leveraging the knowledge embedded in LLMs, such as physical\nprinciples, we extract a \\textit{Knowledge-Link graph}, capturing vast semantic\nknowledge of sensors and the linkage of the sensor-level knowledge. To harness\nthe potential of the knowledge-link graph in enhancing the graph derived from\nMTS data, we propose a graph alignment module, facilitating the transfer of\nsemantic knowledge within the knowledge-link graph into the MTS-derived graph.\nBy doing so, we can improve the graph quality, ensuring effective\nrepresentation learning with GNNs for MTS data. Extensive experiments\ndemonstrate the efficacy of our approach for superior performance across\nvarious MTS-related downstream tasks.",
      "tldr_zh": "本研究针对多变量时间序列 (MTS) 数据中传感器间的空间-时间依赖性问题，提出 K-Link 框架，利用大型语言模型 (LLMs) 编码广泛的通用知识（如物理原理）来构建 Knowledge-Link graph，从而减少传统从 MTS 信号构建图谱的偏差。框架通过 graph alignment module 将 Knowledge-Link graph 中的语义知识转移到 MTS 派生图中，提升图谱质量并优化 Graph Neural Networks (GNNs) 的表示学习。实验结果显示，K-Link 在各种 MTS 相关下游任务上表现出优越性能，证明了其有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages,7 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.03645v1",
      "published_date": "2024-03-06 12:08:14 UTC",
      "updated_date": "2024-03-06 12:08:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:06:00.075182"
    },
    {
      "arxiv_id": "2403.03643v2",
      "title": "A Survey on Applications of Reinforcement Learning in Spatial Resource Allocation",
      "title_zh": "关于强化学习在空间资源分配中的应用综述",
      "authors": [
        "Di Zhang",
        "Moyang Wang",
        "Joseph Mango",
        "Xiang Li",
        "Xianrui Xu"
      ],
      "abstract": "The challenge of spatial resource allocation is pervasive across various\ndomains such as transportation, industry, and daily life. As the scale of\nreal-world issues continues to expand and demands for real-time solutions\nincrease, traditional algorithms face significant computational pressures,\nstruggling to achieve optimal efficiency and real-time capabilities. In recent\nyears, with the escalating computational power of computers, the remarkable\nachievements of reinforcement learning in domains like Go and robotics have\ndemonstrated its robust learning and sequential decision-making capabilities.\nGiven these advancements, there has been a surge in novel methods employing\nreinforcement learning to tackle spatial resource allocation problems. These\nmethods exhibit advantages such as rapid solution convergence and strong model\ngeneralization abilities, offering a new perspective on resolving spatial\nresource allocation problems. Therefore, this paper aims to summarize and\nreview recent theoretical methods and applied research utilizing reinforcement\nlearning to address spatial resource allocation problems. It provides a summary\nand comprehensive overview of its fundamental principles, related\nmethodologies, and applied research. Additionally, it highlights several\nunresolved issues that urgently require attention in this direction for the\nfuture.",
      "tldr_zh": "这篇论文对Reinforcement Learning在Spatial Resource Allocation中的应用进行了全面调查，针对交通、工业和日常生活等领域中传统算法的计算压力和实时性挑战。论文总结了强化学习的基本原理、相关方法和应用研究，强调其快速收敛和强模型泛化能力，为解决空间资源分配问题提供了新视角。同时，它突出了未来亟待关注的未解决议题，如进一步提升算法效率和适应性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03643v2",
      "published_date": "2024-03-06 12:05:56 UTC",
      "updated_date": "2024-03-07 02:05:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:06:11.338725"
    },
    {
      "arxiv_id": "2403.12997v1",
      "title": "A Multi-Task Oriented Semantic Communication Framework for Autonomous Vehicles",
      "title_zh": "翻译失败",
      "authors": [
        "Eslam Eldeeb",
        "Mohammad Shehab",
        "Hirley Alves"
      ],
      "abstract": "Task-oriented semantic communication is an emerging technology that transmits\nonly the relevant semantics of a message instead of the whole message to\nachieve a specific task. It reduces latency, compresses the data, and is more\nrobust in low SNR scenarios. This work presents a multi-task-oriented semantic\ncommunication framework for connected and autonomous vehicles (CAVs). We\npropose a convolutional autoencoder (CAE) that performs the semantic encoding\nof the road traffic signs. These encoded images are then transmitted from one\nCAV to another CAV through satellite in challenging weather conditions where\nvisibility is impaired. In addition, we propose task-oriented semantic decoders\nfor image reconstruction and classification tasks. Simulation results show that\nthe proposed framework outperforms the conventional schemes, such as QAM-16,\nregarding the reconstructed image's similarity and the classification's\naccuracy. In addition, it can save up to 89 % of the bandwidth by sending fewer\nbits.",
      "tldr_zh": "这篇论文提出了一种多任务导向的语义通信框架，用于连接和自动驾驶车辆 (CAVs)，旨在仅传输消息的相关语义，以减少延迟、压缩数据并提升低信噪比 (SNR) 场景下的鲁棒性。框架采用卷积自动编码器 (CAE) 对道路交通标志进行语义编码，并在恶劣天气条件下通过卫星传输，同时包括图像重建和分类任务的语义解码器。模拟结果显示，该框架在图像相似度和分类准确率上优于传统方案如 QAM-16，并能节省高达 89% 的带宽。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "math.IT"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.12997v1",
      "published_date": "2024-03-06 12:04:24 UTC",
      "updated_date": "2024-03-06 12:04:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:06:24.470232"
    },
    {
      "arxiv_id": "2403.03640v6",
      "title": "Apollo: A Lightweight Multilingual Medical LLM towards Democratizing Medical AI to 6B People",
      "title_zh": "翻译失败",
      "authors": [
        "Xidong Wang",
        "Nuo Chen",
        "Junyin Chen",
        "Yidong Wang",
        "Guorui Zhen",
        "Chunxian Zhang",
        "Xiangbo Wu",
        "Yan Hu",
        "Anningzhe Gao",
        "Xiang Wan",
        "Haizhou Li",
        "Benyou Wang"
      ],
      "abstract": "Despite the vast repository of global medical knowledge predominantly being\nin English, local languages are crucial for delivering tailored healthcare\nservices, particularly in areas with limited medical resources. To extend the\nreach of medical AI advancements to a broader population, we aim to develop\nmedical LLMs across the six most widely spoken languages, encompassing a global\npopulation of 6.1 billion. This effort culminates in the creation of the\nApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the\nmultilingual medical benchmark, the released Apollo models, at various\nrelatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best\nperformance among models of equivalent size. Especially, Apollo-7B is the\nstate-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite\nmodels could be used to improve the multi-lingual medical capabilities of\nlarger models without fine-tuning in a proxy-tuning fashion. We will\nopen-source training corpora, code, model weights and evaluation benchmark.",
      "tldr_zh": "这篇论文介绍了 Apollo，一种轻量级多语言医疗 LLM，旨在通过覆盖全球 6.1 亿人口的六种最广泛使用的语言（如英语等），实现医疗 AI 的民主化。研究团队创建了 ApolloCorpora 多语言医疗数据集和 XMedBench 基准，并在多语言医疗基准测试中，各种大小的 Apollo 模型（如 0.5B 到 7B）在同等规模模型中表现出最佳性能，其中 Apollo-7B 成为目前最先进的 multilingual medical LLMs，甚至超越 70B 模型。 additionally, 这些模型可以通过 proxy-tuning 方式提升更大模型的多语言医疗能力，而无需 fine-tuning，且研究将开源训练语料、代码、模型权重和评估基准。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2403.03640v6",
      "published_date": "2024-03-06 11:56:02 UTC",
      "updated_date": "2024-10-12 14:09:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:06:38.167009"
    },
    {
      "arxiv_id": "2403.03636v3",
      "title": "SheetAgent: Towards A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yibin Chen",
        "Yifu Yuan",
        "Zeyu Zhang",
        "Yan Zheng",
        "Jinyi Liu",
        "Fei Ni",
        "Jianye Hao",
        "Hangyu Mao",
        "Fuzheng Zhang"
      ],
      "abstract": "Spreadsheets are ubiquitous across the World Wide Web, playing a critical\nrole in enhancing work efficiency across various domains. Large language model\n(LLM) has been recently attempted for automatic spreadsheet manipulation but\nhas not yet been investigated in complicated and realistic tasks where\nreasoning challenges exist (e.g., long horizon manipulation with multi-step\nreasoning and ambiguous requirements). To bridge the gap with the real-world\nrequirements, we introduce SheetRM, a benchmark featuring long-horizon and\nmulti-category tasks with reasoning-dependent manipulation caused by real-life\nchallenges. To mitigate the above challenges, we further propose SheetAgent, a\nnovel autonomous agent that utilizes the power of LLMs. SheetAgent consists of\nthree collaborative modules: Planner, Informer, and Retriever, achieving both\nadvanced reasoning and accurate manipulation over spreadsheets without human\ninteraction through iterative task reasoning and reflection. Extensive\nexperiments demonstrate that SheetAgent delivers 20--40\\% pass rate\nimprovements on multiple benchmarks over baselines, achieving enhanced\nprecision in spreadsheet manipulation and demonstrating superior table\nreasoning abilities. More details and visualizations are available at the\nproject website: https://sheetagent.github.io/. The datasets and source code\nare available at https://anonymous.4open.science/r/SheetAgent.",
      "tldr_zh": "该论文针对电子表格操作中的复杂挑战（如长horizon操作、多步推理和模糊要求），引入了 SheetRM 基准，这是一个包含真实生活推理依赖任务的多类别测试集。作者提出 SheetAgent，一种基于 Large Language Models (LLM) 的通用代理，包含 Planner、Informer 和 Retriever 三个协作模块，通过迭代任务推理和反思实现自主的电子表格推理和操作。实验结果显示，SheetAgent 在多个基准上比基线模型提高了20-40%的通过率，展现了更高的精确性和表格推理能力。更多细节可访问项目网站和相关资源。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by International World Wide Web Conference (WWW) 2025 (oral)",
      "pdf_url": "http://arxiv.org/pdf/2403.03636v3",
      "published_date": "2024-03-06 11:48:08 UTC",
      "updated_date": "2025-03-03 06:56:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:06:48.701063"
    },
    {
      "arxiv_id": "2403.03627v2",
      "title": "Multimodal Large Language Models to Support Real-World Fact-Checking",
      "title_zh": "翻译失败",
      "authors": [
        "Jiahui Geng",
        "Yova Kementchedjhieva",
        "Preslav Nakov",
        "Iryna Gurevych"
      ],
      "abstract": "Multimodal large language models (MLLMs) carry the potential to support\nhumans in processing vast amounts of information. While MLLMs are already being\nused as a fact-checking tool, their abilities and limitations in this regard\nare understudied. Here is aim to bridge this gap. In particular, we propose a\nframework for systematically assessing the capacity of current multimodal\nmodels to facilitate real-world fact-checking. Our methodology is\nevidence-free, leveraging only these models' intrinsic knowledge and reasoning\ncapabilities. By designing prompts that extract models' predictions,\nexplanations, and confidence levels, we delve into research questions\nconcerning model accuracy, robustness, and reasons for failure. We empirically\nfind that (1) GPT-4V exhibits superior performance in identifying malicious and\nmisleading multimodal claims, with the ability to explain the unreasonable\naspects and underlying motives, and (2) existing open-source models exhibit\nstrong biases and are highly sensitive to the prompt. Our study offers insights\ninto combating false multimodal information and building secure, trustworthy\nmultimodal models. To the best of our knowledge, we are the first to evaluate\nMLLMs for real-world fact-checking.",
      "tldr_zh": "这篇论文提出一个框架，用于评估多模态大语言模型 (MLLMs) 在真实世界事实核查中的能力，该框架依赖模型的内在知识和推理能力，通过设计提示提取预测、解释和置信水平，而无需外部证据。研究发现，GPT-4V 在识别恶意和误导性多模态声明方面表现出色，能够解释不合理方面和潜在动机，而现有开源模型则存在强烈偏见并对提示高度敏感。主要贡献是为对抗虚假多模态信息提供见解，并推动构建安全、可信的 MLLMs，这是首次针对此领域的系统评估。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03627v2",
      "published_date": "2024-03-06 11:32:41 UTC",
      "updated_date": "2024-04-26 05:16:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:07:01.404579"
    },
    {
      "arxiv_id": "2403.03608v1",
      "title": "GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D Scene Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Zi-Ting Chou",
        "Sheng-Yu Huang",
        "I-Jieh Liu",
        "Yu-Chiang Frank Wang"
      ],
      "abstract": "Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance\nFields (NeRF) have emerged as a popular research topic in 3D vision. In this\nwork, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF),\nwhich uniquely takes image semantics into the synthesis process so that both\nnovel view images and the associated semantic maps can be produced for unseen\nscenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and\nDepth-Guided Visual rendering. The former is able to observe multi-view image\ninputs to extract semantic and geometry features from a scene. Guided by the\nresulting image geometry information, the latter performs both image and\nsemantic rendering with improved performances. Our experiments not only confirm\nthat GSNeRF performs favorably against prior works on both novel-view image and\nsemantic segmentation synthesis but the effectiveness of our sampling strategy\nfor visual rendering is further verified.",
      "tldr_zh": "本文提出 GSNeRF，一种可泛化的语义 Neural Radiance Fields，用于增强 3D 场景理解，通过整合图像语义信息，实现对未见场景的新视图图像和语义地图的合成。GSNeRF 由两个阶段组成：Semantic Geo-Reasoning，从多视图图像中提取语义和几何特征；以及 Depth-Guided Visual Rendering，利用几何信息指导图像和语义渲染，以提升性能。实验结果表明，GSNeRF 在新视图图像合成和语义分割任务上优于现有方法，并验证了其采样策略的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR2024",
      "pdf_url": "http://arxiv.org/pdf/2403.03608v1",
      "published_date": "2024-03-06 10:55:50 UTC",
      "updated_date": "2024-03-06 10:55:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:07:12.841470"
    },
    {
      "arxiv_id": "2403.03607v1",
      "title": "The Geometric Structure of Topic Models",
      "title_zh": "主题模型的几何结构",
      "authors": [
        "Johannes Hirth",
        "Tom Hanika"
      ],
      "abstract": "Topic models are a popular tool for clustering and analyzing textual data.\nThey allow texts to be classified on the basis of their affiliation to the\npreviously calculated topics. Despite their widespread use in research and\napplication, an in-depth analysis of topic models is still an open research\ntopic. State-of-the-art methods for interpreting topic models are based on\nsimple visualizations, such as similarity matrices, top-term lists or\nembeddings, which are limited to a maximum of three dimensions. In this paper,\nwe propose an incidence-geometric method for deriving an ordinal structure from\nflat topic models, such as non-negative matrix factorization. These enable the\nanalysis of the topic model in a higher (order) dimension and the possibility\nof extracting conceptual relationships between several topics at once. Due to\nthe use of conceptual scaling, our approach does not introduce any artificial\ntopical relationships, such as artifacts of feature compression. Based on our\nfindings, we present a new visualization paradigm for concept hierarchies based\non ordinal motifs. These allow for a top-down view on topic spaces. We\nintroduce and demonstrate the applicability of our approach based on a topic\nmodel derived from a corpus of scientific papers taken from 32 top machine\nlearning venues.",
      "tldr_zh": "本研究探讨了主题模型（topic models）的几何结构，针对现有可视化方法（如相似矩阵或 top-term lists）局限于三维以下的局限性，提出了一种基于 incidence-geometric method 的方法，从平面的主题模型（如 non-negative matrix factorization）中导出序结构（ordinal structure）。这种方法利用 conceptual scaling 技术，避免引入人工主题关系（如特征压缩的 artifacts），并允许在更高维度分析多个主题间的概念关系。最终，研究引入了基于 ordinal motifs 的新可视化范式，并通过一个包含 32 个顶级机器学习会议论文的语料库进行了演示，实现了对主题空间的顶层视图分析。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03607v1",
      "published_date": "2024-03-06 10:53:51 UTC",
      "updated_date": "2024-03-06 10:53:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:07:24.781730"
    },
    {
      "arxiv_id": "2403.03606v1",
      "title": "Enhancing Price Prediction in Cryptocurrency Using Transformer Neural Network and Technical Indicators",
      "title_zh": "使用 Transformer 神经网络和技术指标增强加密货币价格预测",
      "authors": [
        "Mohammad Ali Labbaf Khaniki",
        "Mohammad Manthouri"
      ],
      "abstract": "This study presents an innovative approach for predicting cryptocurrency time\nseries, specifically focusing on Bitcoin, Ethereum, and Litecoin. The\nmethodology integrates the use of technical indicators, a Performer neural\nnetwork, and BiLSTM (Bidirectional Long Short-Term Memory) to capture temporal\ndynamics and extract significant features from raw cryptocurrency data. The\napplication of technical indicators, such facilitates the extraction of\nintricate patterns, momentum, volatility, and trends. The Performer neural\nnetwork, employing Fast Attention Via positive Orthogonal Random features\n(FAVOR+), has demonstrated superior computational efficiency and scalability\ncompared to the traditional Multi-head attention mechanism in Transformer\nmodels. Additionally, the integration of BiLSTM in the feedforward network\nenhances the model's capacity to capture temporal dynamics in the data,\nprocessing it in both forward and backward directions. This is particularly\nadvantageous for time series data where past and future data points can\ninfluence the current state. The proposed method has been applied to the hourly\nand daily timeframes of the major cryptocurrencies and its performance has been\nbenchmarked against other methods documented in the literature. The results\nunderscore the potential of the proposed method to outperform existing models,\nmarking a significant progression in the field of cryptocurrency price\nprediction.",
      "tldr_zh": "本研究提出了一种创新方法，用于提升加密货币（如比特币、以太坊和莱特币）价格预测的准确性，通过整合技术指标（Technical Indicators）、Performer 神经网络（利用 FAVOR+ 实现高效注意力机制）和 BiLSTM（Bidirectional Long Short-Term Memory）来捕捉时间序列数据中的模式、动量、波动性和趋势。Performer 神经网络相比传统 Multi-head attention 机制具有更好的计算效率，而 BiLSTM 通过前后向处理增强了对时间动态的理解。该方法应用于加密货币的时钟和日常时间框架，并在性能基准测试中超越了文献中的现有模型，标志着加密货币价格预测领域的显著进步。",
      "categories": [
        "q-fin.CP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-fin.CP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03606v1",
      "published_date": "2024-03-06 10:53:12 UTC",
      "updated_date": "2024-03-06 10:53:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:07:35.998423"
    },
    {
      "arxiv_id": "2403.03600v1",
      "title": "A Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain Recommendation",
      "title_zh": "一种基于多模态数据的",
      "authors": [
        "Li Wang",
        "Lei Sang",
        "Quangui Zhang",
        "Qiang Wu",
        "Min Xu"
      ],
      "abstract": "Cross-domain recommendation (CDR) aims to enhance recommendation accuracy in\na target domain with sparse data by leveraging rich information in a source\ndomain, thereby addressing the data-sparsity problem. Some existing CDR methods\nhighlight the advantages of extracting domain-common and domain-specific\nfeatures to learn comprehensive user and item representations. However, these\nmethods can't effectively disentangle these components as they often rely on\nsimple user-item historical interaction information (such as ratings, clicks,\nand browsing), neglecting the rich multi-modal features. Additionally, they\ndon't protect user-sensitive data from potential leakage during knowledge\ntransfer between domains. To address these challenges, we propose a\nPrivacy-Preserving Framework with Multi-Modal Data for Cross-Domain\nRecommendation, called P2M2-CDR. Specifically, we first design a multi-modal\ndisentangled encoder that utilizes multi-modal information to disentangle more\ninformative domain-common and domain-specific embeddings. Furthermore, we\nintroduce a privacy-preserving decoder to mitigate user privacy leakage during\nknowledge transfer. Local differential privacy (LDP) is utilized to obfuscate\nthe disentangled embeddings before inter-domain exchange, thereby enhancing\nprivacy protection. To ensure both consistency and differentiation among these\nobfuscated disentangled embeddings, we incorporate contrastive learning-based\ndomain-inter and domain-intra losses. Extensive Experiments conducted on four\nreal-world datasets demonstrate that P2M2-CDR outperforms other\nstate-of-the-art single-domain and cross-domain baselines.",
      "tldr_zh": "本研究针对跨域推荐（Cross-Domain Recommendation, CDR）中的数据稀疏问题，提出了一种隐私保护框架P2M2-CDR，利用多模态数据（Multi-Modal Data）来提取更全面的领域通用和领域特定特征，从而提升推荐准确性。框架包括多模态分离编码器（multi-modal disentangled encoder）用于分离信息丰富的嵌入，以及隐私保护解码器（privacy-preserving decoder），通过本地差分隐私（Local Differential Privacy, LDP）混淆嵌入并应用基于对比学习的损失函数（contrastive learning-based domain-inter and domain-intra losses），以防止知识转移中的用户隐私泄露。实验在四个真实数据集上显示，P2M2-CDR 优于现有单域和跨域基线方法，显著提高了推荐性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03600v1",
      "published_date": "2024-03-06 10:40:08 UTC",
      "updated_date": "2024-03-06 10:40:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:07:48.607874"
    },
    {
      "arxiv_id": "2403.03594v1",
      "title": "Assessing the Aesthetic Evaluation Capabilities of GPT-4 with Vision: Insights from Group and Individual Assessments",
      "title_zh": "评估 GPT-4 with Vision 的审美评估能力：来自群体和个体评估的见解",
      "authors": [
        "Yoshia Abe",
        "Tatsuya Daikoku",
        "Yasuo Kuniyoshi"
      ],
      "abstract": "Recently, it has been recognized that large language models demonstrate high\nperformance on various intellectual tasks. However, few studies have\ninvestigated alignment with humans in behaviors that involve sensibility, such\nas aesthetic evaluation. This study investigates the performance of GPT-4 with\nVision, a state-of-the-art language model that can handle image input, on the\ntask of aesthetic evaluation of images. We employ two tasks, prediction of the\naverage evaluation values of a group and an individual's evaluation values. We\ninvestigate the performance of GPT-4 with Vision by exploring prompts and\nanalyzing prediction behaviors. Experimental results reveal GPT-4 with Vision's\nsuperior performance in predicting aesthetic evaluations and the nature of\ndifferent responses to beauty and ugliness. Finally, we discuss developing an\nAI system for aesthetic evaluation based on scientific knowledge of the human\nperception of beauty, employing agent technologies that integrate traditional\ndeep learning models with large language models.",
      "tldr_zh": "这篇论文评估了GPT-4 with Vision在图像审美评估任务上的性能，探讨其与人类感性行为（如对美的感知）的对齐程度。研究采用两种任务：预测群体平均评价值和个体评价值，并通过探索prompts和分析预测行为来检验模型表现。实验结果显示，GPT-4 with Vision在审美预测方面表现出色，尤其在对美和丑的不同响应上。最终，论文讨论了基于人类感知美学的科学知识，开发整合传统深度学习模型和大型语言模型的AI系统，以提升审美评估能力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 6 figures, submitted to The 38th Annual Conference of the\n  Japanese Society for Artificial Intelligence, 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.03594v1",
      "published_date": "2024-03-06 10:27:09 UTC",
      "updated_date": "2024-03-06 10:27:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:08:00.337309"
    },
    {
      "arxiv_id": "2403.03593v2",
      "title": "Do You Trust Your Model? Emerging Malware Threats in the Deep Learning Ecosystem",
      "title_zh": "你信任你的模型吗？",
      "authors": [
        "Dorjan Hitaj",
        "Giulio Pagnotta",
        "Fabio De Gaspari",
        "Sediola Ruko",
        "Briland Hitaj",
        "Luigi V. Mancini",
        "Fernando Perez-Cruz"
      ],
      "abstract": "Training high-quality deep learning models is a challenging task due to\ncomputational and technical requirements. A growing number of individuals,\ninstitutions, and companies increasingly rely on pre-trained, third-party\nmodels made available in public repositories. These models are often used\ndirectly or integrated in product pipelines with no particular precautions,\nsince they are effectively just data in tensor form and considered safe. In\nthis paper, we raise awareness of a new machine learning supply chain threat\ntargeting neural networks. We introduce MaleficNet 2.0, a novel technique to\nembed self-extracting, self-executing malware in neural networks. MaleficNet\n2.0 uses spread-spectrum channel coding combined with error correction\ntechniques to inject malicious payloads in the parameters of deep neural\nnetworks. MaleficNet 2.0 injection technique is stealthy, does not degrade the\nperformance of the model, and is robust against removal techniques. We design\nour approach to work both in traditional and distributed learning settings such\nas Federated Learning, and demonstrate that it is effective even when a reduced\nnumber of bits is used for the model parameters. Finally, we implement a\nproof-of-concept self-extracting neural network malware using MaleficNet 2.0,\ndemonstrating the practicality of the attack against a widely adopted machine\nlearning framework. Our aim with this work is to raise awareness against these\nnew, dangerous attacks both in the research community and industry, and we hope\nto encourage further research in mitigation techniques against such threats.",
      "tldr_zh": "本论文探讨了深度学习生态中新兴的恶意软件威胁，强调了对第三方预训练模型的潜在风险，因为这些模型常被直接使用而忽略安全问题。作者提出MaleficNet 2.0技术，利用spread-spectrum channel coding和error correction techniques，将自解压、自执行的恶意负载隐蔽地注入神经网络参数中，该方法不降低模型性能，且对移除技术具有鲁棒性。实验证明MaleficNet 2.0在传统和Federated Learning环境中有效，并通过概念验证展示了实际攻击的可能性，最终呼吁研究界和行业提高对这些威胁的认识，并推动缓解技术的开发。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "18 pages",
      "pdf_url": "http://arxiv.org/pdf/2403.03593v2",
      "published_date": "2024-03-06 10:27:08 UTC",
      "updated_date": "2025-05-13 11:56:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:08:14.737082"
    },
    {
      "arxiv_id": "2403.03592v1",
      "title": "Wildest Dreams: Reproducible Research in Privacy-preserving Neural Network Training",
      "title_zh": "翻译失败",
      "authors": [
        "Tanveer Khan",
        "Mindaugas Budzys",
        "Khoa Nguyen",
        "Antonis Michalas"
      ],
      "abstract": "Machine Learning (ML), addresses a multitude of complex issues in multiple\ndisciplines, including social sciences, finance, and medical research. ML\nmodels require substantial computing power and are only as powerful as the data\nutilized. Due to high computational cost of ML methods, data scientists\nfrequently use Machine Learning-as-a-Service (MLaaS) to outsource computation\nto external servers. However, when working with private information, like\nfinancial data or health records, outsourcing the computation might result in\nprivacy issues. Recent advances in Privacy-Preserving Techniques (PPTs) have\nenabled ML training and inference over protected data through the use of\nPrivacy-Preserving Machine Learning (PPML). However, these techniques are still\nat a preliminary stage and their application in real-world situations is\ndemanding. In order to comprehend discrepancy between theoretical research\nsuggestions and actual applications, this work examines the past and present of\nPPML, focusing on Homomorphic Encryption (HE) and Secure Multi-party\nComputation (SMPC) applied to ML. This work primarily focuses on the ML model's\ntraining phase, where maintaining user data privacy is of utmost importance. We\nprovide a solid theoretical background that eases the understanding of current\napproaches and their limitations. In addition, we present a SoK of the most\nrecent PPML frameworks for model training and provide a comprehensive\ncomparison in terms of the unique properties and performances on standard\nbenchmarks. Also, we reproduce the results for some of the papers and examine\nat what level existing works in the field provide support for open science. We\nbelieve our work serves as a valuable contribution by raising awareness about\nthe current gap between theoretical advancements and real-world applications in\nPPML, specifically regarding open-source availability, reproducibility, and\nusability.",
      "tldr_zh": "这篇论文探讨了 Privacy-Preserving Machine Learning (PPML) 在神经网络训练中的应用，重点审查 Homomorphic Encryption (HE) 和 Secure Multi-party Computation (SMPC) 等技术，以解决隐私数据处理中的实际挑战。作者提供了理论背景、Systematic Knowledge (SoK) 总结和性能比较，评估了现有 PPML 框架在标准基准上的特性。实验复制结果显示，这些方法在开源可用性、重复性和实际可用性方面存在显著差距，为弥合理论与应用间的鸿沟提供了宝贵见解。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03592v1",
      "published_date": "2024-03-06 10:25:36 UTC",
      "updated_date": "2024-03-06 10:25:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:08:27.554305"
    },
    {
      "arxiv_id": "2403.03585v1",
      "title": "RouteExplainer: An Explanation Framework for Vehicle Routing Problem",
      "title_zh": "RouteExplainer：车辆路由问题的解释框架",
      "authors": [
        "Daisuke Kikuta",
        "Hiroki Ikeuchi",
        "Kengo Tajiri",
        "Yuusuke Nakano"
      ],
      "abstract": "The Vehicle Routing Problem (VRP) is a widely studied combinatorial\noptimization problem and has been applied to various practical problems. While\nthe explainability for VRP is significant for improving the reliability and\ninteractivity in practical VRP applications, it remains unexplored. In this\npaper, we propose RouteExplainer, a post-hoc explanation framework that\nexplains the influence of each edge in a generated route. Our framework\nrealizes this by rethinking a route as the sequence of actions and extending\ncounterfactual explanations based on the action influence model to VRP. To\nenhance the explanation, we additionally propose an edge classifier that infers\nthe intentions of each edge, a loss function to train the edge classifier, and\nexplanation-text generation by Large Language Models (LLMs). We quantitatively\nevaluate our edge classifier on four different VRPs. The results demonstrate\nits rapid computation while maintaining reasonable accuracy, thereby\nhighlighting its potential for deployment in practical applications. Moreover,\non the subject of a tourist route, we qualitatively evaluate explanations\ngenerated by our framework. This evaluation not only validates our framework\nbut also shows the synergy between explanation frameworks and LLMs. See\nhttps://ntt-dkiku.github.io/xai-vrp for our code, datasets, models, and demo.",
      "tldr_zh": "该论文提出 RouteExplainer，一种后验解释框架，用于解释 Vehicle Routing Problem (VRP) 中生成路由的每个边的影响，从而提升实际应用的可靠性和交互性。该框架将路由视为动作序列，并扩展基于 action influence model 的 counterfactual explanations 到 VRP，同时引入边分类器来推断边意图、一个专用损失函数进行训练，以及 Large Language Models (LLMs) 生成解释文本。实验结果显示，边分类器在四个不同 VRP 上计算快速且准确性合理，并在旅游路由的定性评估中验证了框架的有效性。这不仅突出了解释框架与 LLMs 的协同作用，还为 VRP 的可解释性应用提供了新路径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at PAKDD 2024. This extended version includes more\n  comprehensive explanations and appendices",
      "pdf_url": "http://arxiv.org/pdf/2403.03585v1",
      "published_date": "2024-03-06 10:01:35 UTC",
      "updated_date": "2024-03-06 10:01:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:08:41.758895"
    },
    {
      "arxiv_id": "2403.03582v1",
      "title": "Design of an Open-Source Architecture for Neural Machine Translation",
      "title_zh": "神经机器翻译的开源架构设计",
      "authors": [
        "Séamus Lankford",
        "Haithem Afli",
        "Andy Way"
      ],
      "abstract": "adaptNMT is an open-source application that offers a streamlined approach to\nthe development and deployment of Recurrent Neural Networks and Transformer\nmodels. This application is built upon the widely-adopted OpenNMT ecosystem,\nand is particularly useful for new entrants to the field, as it simplifies the\nsetup of the development environment and creation of train, validation, and\ntest splits. The application offers a graphing feature that illustrates the\nprogress of model training, and employs SentencePiece for creating subword\nsegmentation models. Furthermore, the application provides an intuitive user\ninterface that facilitates hyperparameter customization. Notably, a\nsingle-click model development approach has been implemented, and models\ndeveloped by adaptNMT can be evaluated using a range of metrics. To encourage\neco-friendly research, adaptNMT incorporates a green report that flags the\npower consumption and kgCO${_2}$ emissions generated during model development.\nThe application is freely available.",
      "tldr_zh": "该论文介绍了adaptNMT，一个开源架构，旨在简化Recurrent Neural Networks和Transformer模型的开发和部署，基于OpenNMT生态系统，特别适合新手用户。该应用提供环境设置简化、数据分割、训练进度图形化显示，以及SentencePiece子词分割模型的集成，并通过直观的用户界面支持超参数自定义和单点击模型开发。adaptNMT还包括评估指标和绿色报告，以追踪模型开发过程中的电力消耗和CO2排放，促进环保研究，并以免费形式提供。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2403.02367",
      "pdf_url": "http://arxiv.org/pdf/2403.03582v1",
      "published_date": "2024-03-06 09:57:52 UTC",
      "updated_date": "2024-03-06 09:57:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:08:49.858781"
    },
    {
      "arxiv_id": "2403.03578v1",
      "title": "Causal Disentanglement for Regulating Social Influence Bias in Social Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Li Wang",
        "Min Xu",
        "Quangui Zhang",
        "Yunxiao Shi",
        "Qiang Wu"
      ],
      "abstract": "Social recommendation systems face the problem of social influence bias,\nwhich can lead to an overemphasis on recommending items that friends have\ninteracted with. Addressing this problem is crucial, and existing methods often\nrely on techniques such as weight adjustment or leveraging unbiased data to\neliminate this bias. However, we argue that not all biases are detrimental,\ni.e., some items recommended by friends may align with the user's interests.\nBlindly eliminating such biases could undermine these positive effects,\npotentially diminishing recommendation accuracy. In this paper, we propose a\nCausal Disentanglement-based framework for Regulating Social influence Bias in\nsocial recommendation, named CDRSB, to improve recommendation performance. From\nthe perspective of causal inference, we find that the user social network could\nbe regarded as a confounder between the user and item embeddings (treatment)\nand ratings (outcome). Due to the presence of this social network confounder,\ntwo paths exist from user and item embeddings to ratings: a non-causal social\ninfluence path and a causal interest path. Building upon this insight, we\npropose a disentangled encoder that focuses on disentangling user and item\nembeddings into interest and social influence embeddings. Mutual\ninformation-based objectives are designed to enhance the distinctiveness of\nthese disentangled embeddings, eliminating redundant information. Additionally,\na regulatory decoder that employs a weight calculation module to dynamically\nlearn the weights of social influence embeddings for effectively regulating\nsocial influence bias has been designed. Experimental results on four\nlarge-scale real-world datasets Ciao, Epinions, Dianping, and Douban book\ndemonstrate the effectiveness of CDRSB compared to state-of-the-art baselines.",
      "tldr_zh": "本论文针对社交推荐系统中的社交影响偏差问题，提出了一种Causal Disentanglement-based框架CDRSB，以精确调节偏差而非完全消除，从而保留与用户兴趣一致的积极影响。从因果推断视角出发，该框架将用户社交网络视为confounder，通过disentangled encoder将用户和物品嵌入分离为兴趣嵌入和社交影响嵌入，并使用mutual information-based目标增强嵌入的区分性。接着，regulatory decoder动态计算社交影响嵌入的权重，实现偏差的有效调节；实验结果显示，CDRSB在Ciao、Epinions、Dianping和Douban book四个真实数据集上，相比最先进基线方法显著提高了推荐性能。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03578v1",
      "published_date": "2024-03-06 09:48:48 UTC",
      "updated_date": "2024-03-06 09:48:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:09:04.883829"
    },
    {
      "arxiv_id": "2403.03575v1",
      "title": "gaHealth: An English-Irish Bilingual Corpus of Health Data",
      "title_zh": "gaHealth：英语-爱尔兰语双语健康数据语料库",
      "authors": [
        "Séamus Lankford",
        "Haithem Afli",
        "Órla Ní Loinsigh",
        "Andy Way"
      ],
      "abstract": "Machine Translation is a mature technology for many high-resource language\npairs. However in the context of low-resource languages, there is a paucity of\nparallel data datasets available for developing translation models.\nFurthermore, the development of datasets for low-resource languages often\nfocuses on simply creating the largest possible dataset for generic\ntranslation. The benefits and development of smaller in-domain datasets can\neasily be overlooked. To assess the merits of using in-domain data, a dataset\nfor the specific domain of health was developed for the low-resource English to\nIrish language pair. Our study outlines the process used in developing the\ncorpus and empirically demonstrates the benefits of using an in-domain dataset\nfor the health domain. In the context of translating health-related data,\nmodels developed using the gaHealth corpus demonstrated a maximum BLEU score\nimprovement of 22.2 points (40%) when compared with top performing models from\nthe LoResMT2021 Shared Task. Furthermore, we define linguistic guidelines for\ndeveloping gaHealth, the first bilingual corpus of health data for the Irish\nlanguage, which we hope will be of use to other creators of low-resource data\nsets. gaHealth is now freely available online and is ready to be explored for\nfurther research.",
      "tldr_zh": "这篇论文介绍了 gaHealth，一个英语-爱尔兰语的双语语料库，专注于健康领域数据，以解决低资源语言机器翻译中的平行数据集缺失问题。研究者通过定义语言指南和开发过程，强调了使用领域特定数据集（如健康领域）的优势，并与通用数据集进行对比。实验结果显示，使用 gaHealth 训练的模型在健康相关翻译任务中，比 LoResMT2021 共享任务的顶级模型提高了 22.2 BLEU score（约 40% 的提升）。该语料库现已免费公开，可为其他低资源语言数据集的创建提供指导，并促进进一步的研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: text overlap with arXiv:2403.02367",
      "pdf_url": "http://arxiv.org/pdf/2403.03575v1",
      "published_date": "2024-03-06 09:36:36 UTC",
      "updated_date": "2024-03-06 09:36:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:09:17.121794"
    },
    {
      "arxiv_id": "2403.03550v1",
      "title": "Emotional Manipulation Through Prompt Engineering Amplifies Disinformation Generation in AI Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Rasita Vinay",
        "Giovanni Spitale",
        "Nikola Biller-Andorno",
        "Federico Germani"
      ],
      "abstract": "This study investigates the generation of synthetic disinformation by\nOpenAI's Large Language Models (LLMs) through prompt engineering and explores\ntheir responsiveness to emotional prompting. Leveraging various LLM iterations\nusing davinci-002, davinci-003, gpt-3.5-turbo and gpt-4, we designed\nexperiments to assess their success in producing disinformation. Our findings,\nbased on a corpus of 19,800 synthetic disinformation social media posts, reveal\nthat all LLMs by OpenAI can successfully produce disinformation, and that they\neffectively respond to emotional prompting, indicating their nuanced\nunderstanding of emotional cues in text generation. When prompted politely, all\nexamined LLMs consistently generate disinformation at a high frequency.\nConversely, when prompted impolitely, the frequency of disinformation\nproduction diminishes, as the models often refuse to generate disinformation\nand instead caution users that the tool is not intended for such purposes. This\nresearch contributes to the ongoing discourse surrounding responsible\ndevelopment and application of AI technologies, particularly in mitigating the\nspread of disinformation and promoting transparency in AI-generated content.",
      "tldr_zh": "这篇论文研究了通过提示工程（prompt engineering）操纵情感来放大 AI Large Language Models (LLMs) 生成虚假信息的问题，实验使用 OpenAI 的模型如 davinci-002、davinci-003、gpt-3.5-turbo 和 gpt-4，基于 19,800 个合成社交媒体帖子进行评估。结果显示，所有 LLM 都能成功生成虚假信息，并对情感提示高度敏感：礼貌提示下生成频率显著增加，而不礼貌提示下则减少并常以警告形式拒绝。该研究为 AI 技术的负责任发展提供关键见解，强调了减少虚假信息传播和提升 AI 生成内容透明度的必要性。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.03550v1",
      "published_date": "2024-03-06 08:50:25 UTC",
      "updated_date": "2024-03-06 08:50:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:09:28.031546"
    },
    {
      "arxiv_id": "2403.04807v1",
      "title": "Mathematics of Neural Networks (Lecture Notes Graduate Course)",
      "title_zh": "翻译失败",
      "authors": [
        "Bart M. N. Smets"
      ],
      "abstract": "These are the lecture notes that accompanied the course of the same name that\nI taught at the Eindhoven University of Technology from 2021 to 2023. The\ncourse is intended as an introduction to neural networks for mathematics\nstudents at the graduate level and aims to make mathematics students interested\nin further researching neural networks. It consists of two parts: first a\ngeneral introduction to deep learning that focuses on introducing the field in\na formal mathematical way. The second part provides an introduction to the\ntheory of Lie groups and homogeneous spaces and how it can be applied to design\nneural networks with desirable geometric equivariances. The lecture notes were\nmade to be as self-contained as possible so as to accessible for any student\nwith a moderate mathematics background. The course also included coding\ntutorials and assignments in the form of a set of Jupyter notebooks that are\npublicly available at\nhttps://gitlab.com/bsmetsjr/mathematics_of_neural_networks.",
      "tldr_zh": "本讲义记录了埃因霍温理工大学从2021到2023年教授的\"Mathematics of Neural Networks\"课程，旨在为研究生水平的数学学生提供神经网络的正式数学介绍，并激发他们进一步研究。课程分为两部分：第一部分聚焦于深度学习(deep learning)的总体介绍，采用数学形式化方法；第二部分介绍Lie groups和homogeneous spaces的理论，并探讨其在设计具有几何等变性(equivariances)的neural networks中的应用。讲义力求自包含，适合有中等数学背景的学生，并附带公开的Jupyter notebooks用于编码教程和作业。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Lecture notes of the graduate course 2MMA80 Mathematics of Neural\n  Networks as thought at the Eindhoven University of Technology from 2021 to\n  2023",
      "pdf_url": "http://arxiv.org/pdf/2403.04807v1",
      "published_date": "2024-03-06 08:45:29 UTC",
      "updated_date": "2024-03-06 08:45:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:09:40.277678"
    },
    {
      "arxiv_id": "2403.03544v1",
      "title": "Prompt Mining for Language-based Human Mobility Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Xue",
        "Tianye Tang",
        "Ali Payani",
        "Flora D. Salim"
      ],
      "abstract": "With the advancement of large language models, language-based forecasting has\nrecently emerged as an innovative approach for predicting human mobility\npatterns. The core idea is to use prompts to transform the raw mobility data\ngiven as numerical values into natural language sentences so that the language\nmodels can be leveraged to generate the description for future observations.\nHowever, previous studies have only employed fixed and manually designed\ntemplates to transform numerical values into sentences. Since the forecasting\nperformance of language models heavily relies on prompts, using fixed templates\nfor prompting may limit the forecasting capability of language models. In this\npaper, we propose a novel framework for prompt mining in language-based\nmobility forecasting, aiming to explore diverse prompt design strategies.\nSpecifically, the framework includes a prompt generation stage based on the\ninformation entropy of prompts and a prompt refinement stage to integrate\nmechanisms such as the chain of thought. Experimental results on real-world\nlarge-scale data demonstrate the superiority of generated prompts from our\nprompt mining pipeline. Additionally, the comparison of different prompt\nvariants shows that the proposed prompt refinement process is effective. Our\nstudy presents a promising direction for further advancing language-based\nmobility forecasting.",
      "tldr_zh": "本文提出了一种针对语言-based 人类移动性预测的提示挖掘（prompt mining）框架，以解决现有方法依赖固定手动模板的局限性，从而提升语言模型的预测性能。具体来说，该框架包括基于信息熵的提示生成阶段，以及整合 Chain of Thought 等机制的提示精炼阶段。实验结果显示，在真实世界大规模数据上，该框架生成的提示比传统方法表现出色，提示精炼过程也证明有效。该研究为语言-based 移动性预测开辟了新的发展方向。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03544v1",
      "published_date": "2024-03-06 08:43:30 UTC",
      "updated_date": "2024-03-06 08:43:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:09:51.791777"
    },
    {
      "arxiv_id": "2403.03538v1",
      "title": "RADIA -- Radio Advertisement Detection with Intelligent Analytics",
      "title_zh": "翻译失败",
      "authors": [
        "Jorge Álvarez",
        "Juan Carlos Armenteros",
        "Camilo Torrón",
        "Miguel Ortega-Martín",
        "Alfonso Ardoiz",
        "Óscar García",
        "Ignacio Arranz",
        "Íñigo Galdeano",
        "Ignacio Garrido",
        "Adrián Alonso",
        "Fernando Bayón",
        "Oleg Vorontsov"
      ],
      "abstract": "Radio advertising remains an integral part of modern marketing strategies,\nwith its appeal and potential for targeted reach undeniably effective. However,\nthe dynamic nature of radio airtime and the rising trend of multiple radio\nspots necessitates an efficient system for monitoring advertisement broadcasts.\nThis study investigates a novel automated radio advertisement detection\ntechnique incorporating advanced speech recognition and text classification\nalgorithms. RadIA's approach surpasses traditional methods by eliminating the\nneed for prior knowledge of the broadcast content. This contribution allows for\ndetecting impromptu and newly introduced advertisements, providing a\ncomprehensive solution for advertisement detection in radio broadcasting.\nExperimental results show that the resulting model, trained on carefully\nsegmented and tagged text data, achieves an F1-macro score of 87.76 against a\ntheoretical maximum of 89.33. This paper provides insights into the choice of\nhyperparameters and their impact on the model's performance. This study\ndemonstrates its potential to ensure compliance with advertising broadcast\ncontracts and offer competitive surveillance. This groundbreaking research\ncould fundamentally change how radio advertising is monitored and open new\ndoors for marketing optimization.",
      "tldr_zh": "这篇论文提出了一种名为 RadIA 的自动化广播广告检测系统，利用高级语音识别和文本分类算法，实现无需事先广播内容知识的检测，从而能有效识别即兴和新广告。相比传统方法，该系统在实验中取得了 F1-macro 分数 87.76 的高性能，并分析了超参数选择对模型的影响。RadIA 的应用潜力在于确保广告广播合同合规、提供竞争监控，并可能革新广播广告的监控和优化策略。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03538v1",
      "published_date": "2024-03-06 08:34:28 UTC",
      "updated_date": "2024-03-06 08:34:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:10:03.340614"
    },
    {
      "arxiv_id": "2403.03536v2",
      "title": "Towards Efficient and Effective Unlearning of Large Language Models for Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Hangyu Wang",
        "Jianghao Lin",
        "Bo Chen",
        "Yang Yang",
        "Ruiming Tang",
        "Weinan Zhang",
        "Yong Yu"
      ],
      "abstract": "The significant advancements in large language models (LLMs) give rise to a\npromising research direction, i.e., leveraging LLMs as recommenders (LLMRec).\nThe efficacy of LLMRec arises from the open-world knowledge and reasoning\ncapabilities inherent in LLMs. LLMRec acquires the recommendation capabilities\nthrough instruction tuning based on user interaction data. However, in order to\nprotect user privacy and optimize utility, it is also crucial for LLMRec to\nintentionally forget specific user data, which is generally referred to as\nrecommendation unlearning. In the era of LLMs, recommendation unlearning poses\nnew challenges for LLMRec in terms of \\textit{inefficiency} and\n\\textit{ineffectiveness}. Existing unlearning methods require updating billions\nof parameters in LLMRec, which is costly and time-consuming. Besides, they\nalways impact the model utility during the unlearning process. To this end, we\npropose \\textbf{E2URec}, the first \\underline{E}fficient and\n\\underline{E}ffective \\underline{U}nlearning method for LLM\\underline{Rec}. Our\nproposed E2URec enhances the unlearning efficiency by updating only a few\nadditional LoRA parameters, and improves the unlearning effectiveness by\nemploying a teacher-student framework, where we maintain multiple teacher\nnetworks to guide the unlearning process. Extensive experiments show that\nE2URec outperforms state-of-the-art baselines on two real-world datasets.\nSpecifically, E2URec can efficiently forget specific data without affecting\nrecommendation performance. The source code is at\n\\url{https://github.com/justarter/E2URec}.",
      "tldr_zh": "该论文探讨了在推荐系统中使用大型语言模型 (LLMs) 作为推荐器 (LLMRec) 的挑战，特别是如何高效有效地进行推荐取消学习，以保护用户隐私并优化模型实用性。现有方法因需更新数亿参数而效率低下，并可能损害模型性能。为此，研究提出 E2URec，这是一种创新的取消学习方法，通过仅更新少量 LoRA 参数来提升效率，并采用教师-学生框架（包括多个教师网络）来指导学习过程，确保取消学习的效果。在两个真实数据集上的实验表明，E2URec 优于现有基线，能够高效忘记特定数据，同时保持推荐性能不变。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by Frontier of Computer Science",
      "pdf_url": "http://arxiv.org/pdf/2403.03536v2",
      "published_date": "2024-03-06 08:31:35 UTC",
      "updated_date": "2024-06-30 04:00:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:10:16.046026"
    },
    {
      "arxiv_id": "2403.03517v1",
      "title": "IB-Net: Initial Branch Network for Variable Decision in Boolean Satisfiability",
      "title_zh": "翻译失败",
      "authors": [
        "Tsz Ho Chan",
        "Wenyi Xiao",
        "Junhua Huang",
        "Huiling Zhen",
        "Guangji Tian",
        "Mingxuan Yuan"
      ],
      "abstract": "Boolean Satisfiability problems are vital components in Electronic Design\nAutomation, particularly within the Logic Equivalence Checking process.\nCurrently, SAT solvers are employed for these problems and neural network is\ntried as assistance to solvers. However, as SAT problems in the LEC context are\ndistinctive due to their predominantly unsatisfiability nature and a\nsubstantial proportion of UNSAT-core variables, existing neural network\nassistance has proven unsuccessful in this specialized domain. To tackle this\nchallenge, we propose IB-Net, an innovative framework utilizing graph neural\nnetworks and novel graph encoding techniques to model unsatisfiable problems\nand interact with state-of-the-art solvers. Extensive evaluations across\nsolvers and datasets demonstrate IB-Net's acceleration, achieving an average\nruntime speedup of 5.0% on industrial data and 8.3% on SAT competition data\nempirically. This breakthrough advances efficient solving in LEC workflows.",
      "tldr_zh": "该研究针对 Boolean Satisfiability (SAT) 问题在逻辑等效性检查 (LEC) 中的独特挑战（如问题多为 unsatisfiable 且存在大量 UNSAT-core 变量），提出了 IB-Net 框架。该框架利用 graph neural networks 和创新的图编码技术来建模这些问题，并与最先进的 SAT 求解器交互，以提升求解效率。实验结果显示，IB-Net 在工业数据集上实现了平均运行时间加速 5.0%，在 SAT 比赛数据集上加速 8.3%，从而推进了 LEC 工作流中的高效求解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.03517v1",
      "published_date": "2024-03-06 07:54:40 UTC",
      "updated_date": "2024-03-06 07:54:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:10:30.211715"
    },
    {
      "arxiv_id": "2403.03506v4",
      "title": "Detecting AI-Generated Sentences in Human-AI Collaborative Hybrid Texts: Challenges, Strategies, and Insights",
      "title_zh": "检测人类-AI 协作混合文本中的 AI 生成句子：挑战、策略和见解",
      "authors": [
        "Zijie Zeng",
        "Shiqi Liu",
        "Lele Sha",
        "Zhuang Li",
        "Kaixun Yang",
        "Sannyuya Liu",
        "Dragan Gašević",
        "Guanliang Chen"
      ],
      "abstract": "This study explores the challenge of sentence-level AI-generated text\ndetection within human-AI collaborative hybrid texts. Existing studies of\nAI-generated text detection for hybrid texts often rely on synthetic datasets.\nThese typically involve hybrid texts with a limited number of boundaries. We\ncontend that studies of detecting AI-generated content within hybrid texts\nshould cover different types of hybrid texts generated in realistic settings to\nbetter inform real-world applications. Therefore, our study utilizes the\nCoAuthor dataset, which includes diverse, realistic hybrid texts generated\nthrough the collaboration between human writers and an intelligent writing\nsystem in multi-turn interactions. We adopt a two-step, segmentation-based\npipeline: (i) detect segments within a given hybrid text where each segment\ncontains sentences of consistent authorship, and (ii) classify the authorship\nof each identified segment. Our empirical findings highlight (1) detecting\nAI-generated sentences in hybrid texts is overall a challenging task because\n(1.1) human writers' selecting and even editing AI-generated sentences based on\npersonal preferences adds difficulty in identifying the authorship of segments;\n(1.2) the frequent change of authorship between neighboring sentences within\nthe hybrid text creates difficulties for segment detectors in identifying\nauthorship-consistent segments; (1.3) the short length of text segments within\nhybrid texts provides limited stylistic cues for reliable authorship\ndetermination; (2) before embarking on the detection process, it is beneficial\nto assess the average length of segments within the hybrid text. This\nassessment aids in deciding whether (2.1) to employ a text segmentation-based\nstrategy for hybrid texts with longer segments, or (2.2) to adopt a direct\nsentence-by-sentence classification strategy for those with shorter segments.",
      "tldr_zh": "这篇论文探讨了在人类-AI 协作混合文本中检测 AI-generated sentences 的挑战和策略，强调现有研究依赖合成数据集的局限性，并引入了更真实的 CoAuthor 数据集。研究采用两步管道：首先检测文本中的 authorship 一致段落，然后分类每个段落的作者。关键发现包括检测难度高（如人类编辑 AI 生成内容、相邻句子作者频繁变化以及段落短小导致风格线索不足），并建议根据混合文本的段落平均长度选择合适的策略，例如使用基于分割的方法或直接句子分类，以提升实际应用效果。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Camera-Ready version of our IJCAI 2024 accepted paper (Special Track:\n  AI and Social Good)",
      "pdf_url": "http://arxiv.org/pdf/2403.03506v4",
      "published_date": "2024-03-06 07:25:46 UTC",
      "updated_date": "2024-05-23 13:18:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:10:42.169056"
    },
    {
      "arxiv_id": "2403.03456v2",
      "title": "DLP-GAN: learning to draw modern Chinese landscape photos with generative adversarial network",
      "title_zh": "翻译失败",
      "authors": [
        "Xiangquan Gui",
        "Binxuan Zhang",
        "Li Li",
        "Yi Yang"
      ],
      "abstract": "Chinese landscape painting has a unique and artistic style, and its drawing\ntechnique is highly abstract in both the use of color and the realistic\nrepresentation of objects. Previous methods focus on transferring from modern\nphotos to ancient ink paintings. However, little attention has been paid to\ntranslating landscape paintings into modern photos. To solve such problems, in\nthis paper, we (1) propose DLP-GAN (Draw Modern Chinese Landscape Photos with\nGenerative Adversarial Network), an unsupervised cross-domain image translation\nframework with a novel asymmetric cycle mapping, and (2) introduce a generator\nbased on a dense-fusion module to match different translation directions.\nMoreover, a dual-consistency loss is proposed to balance the realism and\nabstraction of model painting. In this way, our model can draw landscape photos\nand sketches in the modern sense. Finally, based on our collection of modern\nlandscape and sketch datasets, we compare the images generated by our model\nwith other benchmarks. Extensive experiments including user studies show that\nour model outperforms state-of-the-art methods.",
      "tldr_zh": "本论文提出DLP-GAN，一种基于生成对抗网络(GAN)的无监督跨域图像翻译框架，旨在将中国山水画转化为现代照片和素描，解决传统方法忽略的这一方向。框架引入不对称循环映射和基于密集融合模块(dense-fusion module)的生成器，以适应不同翻译方向，并通过双重一致性损失(dual-consistency loss)平衡图像的真实性和抽象性。实验结果显示，DLP-GAN在作者收集的现代山水和素描数据集上优于现有基准模型，并在用户研究中表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Corrected typos",
      "pdf_url": "http://arxiv.org/pdf/2403.03456v2",
      "published_date": "2024-03-06 04:46:03 UTC",
      "updated_date": "2024-03-07 05:49:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:10:52.087827"
    },
    {
      "arxiv_id": "2403.03444v1",
      "title": "Uncertainty quantification for deeponets with ensemble kalman inversion",
      "title_zh": "翻译失败",
      "authors": [
        "Andrew Pensoneault",
        "Xueyu Zhu"
      ],
      "abstract": "In recent years, operator learning, particularly the DeepONet, has received\nmuch attention for efficiently learning complex mappings between input and\noutput functions across diverse fields. However, in practical scenarios with\nlimited and noisy data, accessing the uncertainty in DeepONet predictions\nbecomes essential, especially in mission-critical or safety-critical\napplications. Existing methods, either computationally intensive or yielding\nunsatisfactory uncertainty quantification, leave room for developing efficient\nand informative uncertainty quantification (UQ) techniques tailored for\nDeepONets. In this work, we proposed a novel inference approach for efficient\nUQ for operator learning by harnessing the power of the Ensemble Kalman\nInversion (EKI) approach. EKI, known for its derivative-free, noise-robust, and\nhighly parallelizable feature, has demonstrated its advantages for UQ for\nphysics-informed neural networks [28]. Our innovative application of EKI\nenables us to efficiently train ensembles of DeepONets while obtaining\ninformative uncertainty estimates for the output of interest. We deploy a\nmini-batch variant of EKI to accommodate larger datasets, mitigating the\ncomputational demand due to large datasets during the training stage.\nFurthermore, we introduce a heuristic method to estimate the artificial\ndynamics covariance, thereby improving our uncertainty estimates. Finally, we\ndemonstrate the effectiveness and versatility of our proposed methodology\nacross various benchmark problems, showcasing its potential to address the\npressing challenges of uncertainty quantification in DeepONets, especially for\npractical applications with limited and noisy data.",
      "tldr_zh": "该研究针对 DeepONets 在操作符学习中的不确定性量化（UQ）问题，提出了一种基于 Ensemble Kalman Inversion (EKI) 的新颖推理方法，以处理有限和噪声数据场景。EKI 的无导数、噪声鲁棒和高并行化特性被应用于训练 DeepONets 集合模型，从而高效获得输出不确定性估计。该方法引入 mini-batch EKI 变体来适应大数据集，并开发了估算人工动态协方差的启发式技术，以提升 UQ 准确性。在多个基准问题上，实验证明了该方法的有效性和多功能性，为 DeepONets 在实际应用中的不确定性挑战提供了解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NA",
        "math.NA",
        "stat.ML",
        "65"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages",
      "pdf_url": "http://arxiv.org/pdf/2403.03444v1",
      "published_date": "2024-03-06 04:02:30 UTC",
      "updated_date": "2024-03-06 04:02:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:11:05.051054"
    },
    {
      "arxiv_id": "2403.03432v1",
      "title": "Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Wenfeng Feng",
        "Chuzhan Hao",
        "Yuewei Zhang",
        "Yu Han",
        "Hao Wang"
      ],
      "abstract": "Instruction Tuning has the potential to stimulate or enhance specific\ncapabilities of large language models (LLMs). However, achieving the right\nbalance of data is crucial to prevent catastrophic forgetting and interference\nbetween tasks. To address these limitations and enhance training flexibility,\nwe propose the Mixture-of-LoRAs (MoA) architecture which is a novel and\nparameter-efficient tuning method designed for multi-task learning with LLMs.\nIn this paper, we start by individually training multiple domain-specific LoRA\nmodules using corresponding supervised corpus data. These LoRA modules can be\naligned with the expert design principles observed in Mixture-of-Experts (MoE).\nSubsequently, we combine the multiple LoRAs using an explicit routing strategy\nand introduce domain labels to facilitate multi-task learning, which help\nprevent interference between tasks and ultimately enhances the performance of\neach individual task. Furthermore, each LoRA model can be iteratively adapted\nto a new domain, allowing for quick domain-specific adaptation. Experiments on\ndiverse tasks demonstrate superior and robust performance, which can further\npromote the wide application of domain-specific LLMs.",
      "tldr_zh": "本研究提出了一种名为 Mixture-of-LoRAs (MoA) 的参数高效调优方法，用于 Large Language Models (LLMs) 的多任务学习，以解决 Instruction Tuning 中可能导致的灾难性遗忘和任务间干扰问题。方法包括先分别训练多个领域特定的 LoRA 模块，使用对应的监督语料数据，然后通过显式路由策略和领域标签结合这些模块，实现任务间的隔离并提升整体性能。此外，实验在多样任务上展示了 MoA 的优越和稳健表现，促进了领域特定 LLMs 的广泛应用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, COLING24 Accepted",
      "pdf_url": "http://arxiv.org/pdf/2403.03432v1",
      "published_date": "2024-03-06 03:33:48 UTC",
      "updated_date": "2024-03-06 03:33:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:11:17.757146"
    },
    {
      "arxiv_id": "2403.03421v1",
      "title": "LEAD: Learning Decomposition for Source-free Universal Domain Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "Sanqing Qu",
        "Tianpei Zou",
        "Lianghua He",
        "Florian Röhrbein",
        "Alois Knoll",
        "Guang Chen",
        "Changjun Jiang"
      ],
      "abstract": "Universal Domain Adaptation (UniDA) targets knowledge transfer in the\npresence of both covariate and label shifts. Recently, Source-free Universal\nDomain Adaptation (SF-UniDA) has emerged to achieve UniDA without access to\nsource data, which tends to be more practical due to data protection policies.\nThe main challenge lies in determining whether covariate-shifted samples belong\nto target-private unknown categories. Existing methods tackle this either\nthrough hand-crafted thresholding or by developing time-consuming iterative\nclustering strategies. In this paper, we propose a new idea of LEArning\nDecomposition (LEAD), which decouples features into source-known and -unknown\ncomponents to identify target-private data. Technically, LEAD initially\nleverages the orthogonal decomposition analysis for feature decomposition.\nThen, LEAD builds instance-level decision boundaries to adaptively identify\ntarget-private data. Extensive experiments across various UniDA scenarios have\ndemonstrated the effectiveness and superiority of LEAD. Notably, in the OPDA\nscenario on VisDA dataset, LEAD outperforms GLC by 3.5% overall H-score and\nreduces 75% time to derive pseudo-labeling decision boundaries. Besides, LEAD\nis also appealing in that it is complementary to most existing methods. The\ncode is available at https://github.com/ispc-lab/LEAD.",
      "tldr_zh": "该论文提出LEAD方法，用于Source-free Universal Domain Adaptation (SF-UniDA)，旨在处理无源数据访问的通用域适应问题，包括协变量和标签偏移。LEAD通过特征分解将数据解耦为源已知和源未知组件，利用orthogonal decomposition analysis进行初始分解，并构建实例级决策边界来适应性识别目标私有数据。该方法在各种UniDA场景中表现出色，例如在VisDA数据集的OPDA场景中，比GLC方法提高3.5%的H-score，并减少75%的伪标签决策边界生成时间；此外，LEAD与大多数现有方法互补，便于实际应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "To appear in CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.03421v1",
      "published_date": "2024-03-06 03:08:20 UTC",
      "updated_date": "2024-03-06 03:08:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:11:28.934142"
    },
    {
      "arxiv_id": "2403.03419v2",
      "title": "Negating Negatives: Alignment with Human Negative Samples via Distributional Dispreference Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Shitong Duan",
        "Xiaoyuan Yi",
        "Peng Zhang",
        "Yan Liu",
        "Zheng Liu",
        "Tun Lu",
        "Xing Xie",
        "Ning Gu"
      ],
      "abstract": "Large language models (LLMs) have revolutionized the role of AI, yet pose\npotential social risks. To steer LLMs towards human preference, alignment\ntechnologies have been introduced and gained increasing attention.\nNevertheless, existing methods heavily rely on high-quality positive-negative\ntraining pairs, suffering from noisy positive responses that are barely\ndistinguishable from negative ones. Given recent LLMs' proficiency in\ngenerating helpful responses, this work pivots towards a new research question:\ncan we achieve alignment using solely human-annotated negative samples,\npreserving helpfulness while reducing harmfulness? For this purpose, we propose\nDistributional Dispreference Optimization (D$^2$O), which maximizes the\ndiscrepancy between dispreferred responses and the generated non-negative ones.\nIn this way, D$^2$O effectively eschews harmful information without\nincorporating noisy positive samples, while avoiding collapse using\nself-generated responses as anchors. We demonstrate that D$^2$O can be regarded\nas learning a distributional preference model reflecting human dispreference\nagainst negative responses, which is theoretically an upper bound of the\ninstance-level DPO. Extensive experiments manifest that our method achieves\ncomparable generation quality and surpasses the latest strong baselines in\nproducing less harmful and more informative responses with better training\nstability and faster convergence.",
      "tldr_zh": "本研究探讨如何使用仅人类标注的负样本对大型语言模型 (LLMs) 进行对齐，以保持响应帮助性同时减少有害性，解决现有对齐方法依赖噪音正样本的问题。作者提出 Distributional Dispreference Optimization (D²O)，通过最大化不喜欢的响应与自生成非负响应之间的差异，实现有效的偏好优化，而无需引入噪音正样本。D²O 被证明等效于学习一个分布偏好模型，是实例级 DPO 的上界，能避免模型崩溃。实验结果显示，该方法在生成质量上与基线相当，但在减少有害响应、提升信息性、训练稳定性和收敛速度方面表现出色。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by EMNLP 2024(Findings)",
      "pdf_url": "http://arxiv.org/pdf/2403.03419v2",
      "published_date": "2024-03-06 03:02:38 UTC",
      "updated_date": "2024-09-30 04:49:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:11:41.753899"
    },
    {
      "arxiv_id": "2403.03409v1",
      "title": "Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN",
      "title_zh": "翻译失败",
      "authors": [
        "Biswadeep Chakraborty",
        "Beomseok Kang",
        "Harshit Kumar",
        "Saibal Mukhopadhyay"
      ],
      "abstract": "Recurrent Spiking Neural Networks (RSNNs) have emerged as a computationally\nefficient and brain-inspired learning model. The design of sparse RSNNs with\nfewer neurons and synapses helps reduce the computational complexity of RSNNs.\nTraditionally, sparse SNNs are obtained by first training a dense and complex\nSNN for a target task, and, then, pruning neurons with low activity\n(activity-based pruning) while maintaining task performance. In contrast, this\npaper presents a task-agnostic methodology for designing sparse RSNNs by\npruning a large randomly initialized model. We introduce a novel Lyapunov Noise\nPruning (LNP) algorithm that uses graph sparsification methods and utilizes\nLyapunov exponents to design a stable sparse RSNN from a randomly initialized\nRSNN. We show that the LNP can leverage diversity in neuronal timescales to\ndesign a sparse Heterogeneous RSNN (HRSNN). Further, we show that the same\nsparse HRSNN model can be trained for different tasks, such as image\nclassification and temporal prediction. We experimentally show that, in spite\nof being task-agnostic, LNP increases computational efficiency (fewer neurons\nand synapses) and prediction performance of RSNNs compared to traditional\nactivity-based pruning of trained dense models.",
      "tldr_zh": "本文提出了一种任务无关的方法，用于从随机初始化的 Recurrent Spiking Neural Networks (RSNNs) 中设计稳定的稀疏 Heterogeneous RSNN (HRSNN)，通过引入 Lyapunov Noise Pruning (LNP) 算法，该算法利用图稀疏化和 Lyapunov exponents 来利用神经元时间尺度的异质性进行剪枝。LNP 方法显著减少了神经元和突触数量，使模型适用于多种任务，如图像分类和时间预测。与传统基于活动度的剪枝方法相比，实验结果显示 LNP 提高了 RSNNs 的计算效率和预测性能。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "Published as a conference paper at ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.03409v1",
      "published_date": "2024-03-06 02:36:15 UTC",
      "updated_date": "2024-03-06 02:36:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:11:53.568734"
    },
    {
      "arxiv_id": "2403.03407v4",
      "title": "Human vs. Machine: Behavioral Differences Between Expert Humans and Language Models in Wargame Simulations",
      "title_zh": "翻译失败",
      "authors": [
        "Max Lamparth",
        "Anthony Corso",
        "Jacob Ganz",
        "Oriana Skylar Mastro",
        "Jacquelyn Schneider",
        "Harold Trinkunas"
      ],
      "abstract": "To some, the advent of artificial intelligence (AI) promises better\ndecision-making and increased military effectiveness while reducing the\ninfluence of human error and emotions. However, there is still debate about how\nAI systems, especially large language models (LLMs) that can be applied to many\ntasks, behave compared to humans in high-stakes military decision-making\nscenarios with the potential for increased risks towards escalation. To test\nthis potential and scrutinize the use of LLMs for such purposes, we use a new\nwargame experiment with 214 national security experts designed to examine\ncrisis escalation in a fictional U.S.-China scenario and compare the behavior\nof human player teams to LLM-simulated team responses in separate simulations.\nHere, we find that the LLM-simulated responses can be more aggressive and\nsignificantly affected by changes in the scenario. We show a considerable\nhigh-level agreement in the LLM and human responses and significant\nquantitative and qualitative differences in individual actions and strategic\ntendencies. These differences depend on intrinsic biases in LLMs regarding the\nappropriate level of violence following strategic instructions, the choice of\nLLM, and whether the LLMs are tasked to decide for a team of players directly\nor first to simulate dialog between a team of players. When simulating the\ndialog, the discussions lack quality and maintain a farcical harmony. The LLM\nsimulations cannot account for human player characteristics, showing no\nsignificant difference even for extreme traits, such as \"pacifist\" or\n\"aggressive sociopath.\" When probing behavioral consistency across individual\nmoves of the simulation, the tested LLMs deviated from each other but generally\nshowed somewhat consistent behavior. Our results motivate policymakers to be\ncautious before granting autonomy or following AI-based strategy\nrecommendations.",
      "tldr_zh": "这篇论文通过一个新的 wargame 实验，比较了 214 名国家安全专家和大型语言模型（LLMs）在虚构的美中危机场景中的行为差异，旨在评估 LLMs 在高风险军事决策中的表现。结果显示，LLMs 的响应往往更具攻击性，并更容易受场景变化影响，尽管在高层决策上与人类有显著一致性，但个体行动和战略倾向存在明显的定量和定性差异。这些差异源于 LLMs 的内在偏见、模型选择以及是否直接模拟团队决策或对话模拟，后者导致讨论质量低下且缺乏真实性。论文强调，LLMs 无法有效捕捉人类特性（如“pacifist”或“aggressive sociopath”），并建议政策制定者谨慎采用 AI 策略推荐以避免潜在升级风险。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "Updated with new human participant results and added new LLM to\n  results; fixed error in Table 1; all claims unaffected",
      "pdf_url": "http://arxiv.org/pdf/2403.03407v4",
      "published_date": "2024-03-06 02:23:32 UTC",
      "updated_date": "2024-10-03 03:51:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:12:06.579564"
    },
    {
      "arxiv_id": "2403.03406v1",
      "title": "An EnKF-LSTM Assimilation Algorithm for Crop Growth Model",
      "title_zh": "EnKF-LSTM 同化算法用于作物生长模型",
      "authors": [
        "Siqi Zhou",
        "Ling Wang",
        "Jie Liu",
        "Jinshan Tang"
      ],
      "abstract": "Accurate and timely prediction of crop growth is of great significance to\nensure crop yields and researchers have developed several crop models for the\nprediction of crop growth. However, there are large difference between the\nsimulation results obtained by the crop models and the actual results, thus in\nthis paper, we proposed to combine the simulation results with the collected\ncrop data for data assimilation so that the accuracy of prediction will be\nimproved. In this paper, an EnKF-LSTM data assimilation method for various\ncrops is proposed by combining ensemble Kalman filter and LSTM neural network,\nwhich effectively avoids the overfitting problem of existing data assimilation\nmethods and eliminates the uncertainty of the measured data. The verification\nof the proposed EnKF-LSTM method and the comparison of the proposed method with\nother data assimilation methods were performed using datasets collected by\nsensor equipment deployed on a farm.",
      "tldr_zh": "该论文针对作物生长模型的模拟与实际结果差异问题，提出了一种 EnKF-LSTM 数据同化算法，将 Ensemble Kalman Filter 与 LSTM 神经网络相结合，以提高作物生长的预测准确性。该方法通过整合模拟结果和传感器收集的数据，有效避免了现有数据同化方法的过拟合问题，并减少了测量数据的不确定性。在农场传感器数据集上的验证实验中，该算法与其他数据同化方法相比表现出显著优势，提升了预测性能。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03406v1",
      "published_date": "2024-03-06 02:09:50 UTC",
      "updated_date": "2024-03-06 02:09:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:12:16.510661"
    },
    {
      "arxiv_id": "2403.03401v1",
      "title": "BAIT: Benchmarking (Embedding) Architectures for Interactive Theorem-Proving",
      "title_zh": "翻译失败",
      "authors": [
        "Sean Lamont",
        "Michael Norrish",
        "Amir Dezfouli",
        "Christian Walder",
        "Paul Montague"
      ],
      "abstract": "Artificial Intelligence for Theorem Proving has given rise to a plethora of\nbenchmarks and methodologies, particularly in Interactive Theorem Proving\n(ITP). Research in the area is fragmented, with a diverse set of approaches\nbeing spread across several ITP systems. This presents a significant challenge\nto the comparison of methods, which are often complex and difficult to\nreplicate. Addressing this, we present BAIT, a framework for fair and\nstreamlined comparison of learning approaches in ITP. We demonstrate BAIT's\ncapabilities with an in-depth comparison, across several ITP benchmarks, of\nstate-of-the-art architectures applied to the problem of formula embedding. We\nfind that Structure Aware Transformers perform particularly well, improving on\ntechniques associated with the original problem sets. BAIT also allows us to\nassess the end-to-end proving performance of systems built on interactive\nenvironments. This unified perspective reveals a novel end-to-end system that\nimproves on prior work. We also provide a qualitative analysis, illustrating\nthat improved performance is associated with more semantically-aware\nembeddings. By streamlining the implementation and comparison of Machine\nLearning algorithms in the ITP context, we anticipate BAIT will be a\nspringboard for future research.",
      "tldr_zh": "这篇论文引入了BAIT框架，用于公平且高效地比较Interactive Theorem Proving (ITP)中的学习方法，解决当前研究碎片化和复现困难的问题。BAIT通过在多个ITP基准上测试状态-of-the-art架构（如Structure Aware Transformers）在公式嵌入任务上的性能，发现这些架构显著改善了原有技术，并提升了端到端证明系统的整体表现。定性分析进一步显示，改进与更语义感知的嵌入密切相关，预计BAIT将推动ITP领域机器学习算法的未来研究。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03401v1",
      "published_date": "2024-03-06 01:56:17 UTC",
      "updated_date": "2024-03-06 01:56:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:12:30.388651"
    },
    {
      "arxiv_id": "2403.03395v1",
      "title": "Interactive Melody Generation System for Enhancing the Creativity of Musicians",
      "title_zh": "交互式旋律生成系统，用于提升音乐家的创造力",
      "authors": [
        "So Hirawata",
        "Noriko Otani"
      ],
      "abstract": "This study proposes a system designed to enumerate the process of\ncollaborative composition among humans, using automatic music composition\ntechnology. By integrating multiple Recurrent Neural Network (RNN) models, the\nsystem provides an experience akin to collaborating with several composers,\nthereby fostering diverse creativity. Through dynamic adaptation to the user's\ncreative intentions, based on feedback, the system enhances its capability to\ngenerate melodies that align with user preferences and creative needs. The\nsystem's effectiveness was evaluated through experiments with composers of\nvarying backgrounds, revealing its potential to facilitate musical creativity\nand suggesting avenues for further refinement. The study underscores the\nimportance of interaction between the composer and AI, aiming to make music\ncomposition more accessible and personalized. This system represents a step\ntowards integrating AI into the creative process, offering a new tool for\ncomposition support and collaborative artistic exploration.",
      "tldr_zh": "本研究提出一个交互式旋律生成系统，旨在通过模拟人类协作作曲过程来提升音乐家的创意。系统整合多个Recurrent Neural Network (RNN) 模型，提供类似于与多位作曲家合作的体验，促进多样化创意输出。该系统通过动态适应用户的反馈，生成符合用户偏好和需求的旋律，并在实验中证明其有效性。实验涉及不同背景的作曲家，显示系统能促进音乐创意，并强调AI与作曲家互动的重要性，为AI在音乐创作中的应用提供新工具和改进方向。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.HC",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03395v1",
      "published_date": "2024-03-06 01:33:48 UTC",
      "updated_date": "2024-03-06 01:33:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:12:42.014712"
    },
    {
      "arxiv_id": "2403.03385v1",
      "title": "Multi-modal Deep Learning",
      "title_zh": "多模态深度学习",
      "authors": [
        "Chen Yuhua"
      ],
      "abstract": "This article investigates deep learning methodologies for single-modality\nclinical data analysis, as a crucial precursor to multi-modal medical research.\nBuilding on Guo JingYuan's work, the study refines clinical data processing\nthrough Compact Convolutional Transformer (CCT), Patch Up, and the innovative\nCamCenterLoss technique, establishing a foundation for future multimodal\ninvestigations. The proposed methodology demonstrates improved prediction\naccuracy and at tentiveness to critically ill patients compared to Guo\nJingYuan's ResNet and StageNet approaches. Novelty that using image-pretrained\nvision transformer backbone to perform transfer learning time-series clinical\ndata.The study highlights the potential of CCT, Patch Up, and novel\nCamCenterLoss in processing single modality clinical data within deep learning\nframeworks, paving the way for future multimodal medical research and promoting\nprecision and personalized healthcare",
      "tldr_zh": "本研究探讨了单模态临床数据分析的深度学习方法，作为多模态医疗研究的基础，基于郭景元的工作进行了改进。论文引入了Compact Convolutional Transformer (CCT)、Patch Up 和创新的CamCenterLoss 技术，并利用图像预训练的Vision Transformer 骨干进行时序临床数据的迁移学习，从而提升了预测准确性和对危重病人的关注度。相比郭景元的ResNet和StageNet 方法，该方法显著提高了性能，并为未来的多模态医疗研究奠定了基础，促进精确和个性化的医疗。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Master's thesis",
      "pdf_url": "http://arxiv.org/pdf/2403.03385v1",
      "published_date": "2024-03-06 00:36:05 UTC",
      "updated_date": "2024-03-06 00:36:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:12:53.939918"
    },
    {
      "arxiv_id": "2403.03382v1",
      "title": "Adaptive Discovering and Merging for Incremental Novel Class Discovery",
      "title_zh": "翻译失败",
      "authors": [
        "Guangyao Chen",
        "Peixi Peng",
        "Yangru Huang",
        "Mengyue Geng",
        "Yonghong Tian"
      ],
      "abstract": "One important desideratum of lifelong learning aims to discover novel classes\nfrom unlabelled data in a continuous manner. The central challenge is twofold:\ndiscovering and learning novel classes while mitigating the issue of\ncatastrophic forgetting of established knowledge. To this end, we introduce a\nnew paradigm called Adaptive Discovering and Merging (ADM) to discover novel\ncategories adaptively in the incremental stage and integrate novel knowledge\ninto the model without affecting the original knowledge. To discover novel\nclasses adaptively, we decouple representation learning and novel class\ndiscovery, and use Triple Comparison (TC) and Probability Regularization (PR)\nto constrain the probability discrepancy and diversity for adaptive category\nassignment. To merge the learned novel knowledge adaptively, we propose a\nhybrid structure with base and novel branches named Adaptive Model Merging\n(AMM), which reduces the interference of the novel branch on the old classes to\npreserve the previous knowledge, and merges the novel branch to the base model\nwithout performance loss and parameter growth. Extensive experiments on several\ndatasets show that ADM significantly outperforms existing class-incremental\nNovel Class Discovery (class-iNCD) approaches. Moreover, our AMM also benefits\nthe class-incremental Learning (class-IL) task by alleviating the catastrophic\nforgetting problem.",
      "tldr_zh": "该研究提出了一种新的范式Adaptive Discovering and Merging (ADM)，用于终身学习中从无标签数据中持续发现和学习新类别，同时缓解catastrophic forgetting问题。ADM通过解耦表示学习和新型类别发现，利用Triple Comparison (TC)和Probability Regularization (PR)来约束概率差异和多样性，实现自适应类别分配。针对合并新知识，该方法引入Adaptive Model Merging (AMM)的混合结构，包括base和novel branches，以减少新分支对旧类别的干扰，并在不增加参数的情况下融合模型。在多个数据集上的实验显示，ADM显著优于现有的class-incremental Novel Class Discovery (class-iNCD)方法，并有助于class-incremental Learning (class-IL)任务降低遗忘风险。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "AAAI 2024. arXiv admin note: text overlap with arXiv:2207.08605 by\n  other authors",
      "pdf_url": "http://arxiv.org/pdf/2403.03382v1",
      "published_date": "2024-03-06 00:17:03 UTC",
      "updated_date": "2024-03-06 00:17:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T13:13:05.794841"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 94,
  "processed_papers_count": 94,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T13:13:28.374132"
}
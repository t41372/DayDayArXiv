[
  {
    "arxiv_id": "2410.04655v2",
    "title": "Graph Fourier Neural Kernels (G-FuNK): Learning Solutions of Nonlinear Diffusive Parametric PDEs on Multiple Domains",
    "authors": [
      "Shane E. Loeffler",
      "Zan Ahmad",
      "Syed Yusuf Ali",
      "Carolyna Yamamoto",
      "Dan M. Popescu",
      "Alana Yee",
      "Yash Lal",
      "Natalia Trayanova",
      "Mauro Maggioni"
    ],
    "abstract": "Predicting time-dependent dynamics of complex systems governed by non-linear\npartial differential equations (PDEs) with varying parameters and domains is a\nchallenging task motivated by applications across various fields. We introduce\na novel family of neural operators based on our Graph Fourier Neural Kernels,\ndesigned to learn solution generators for nonlinear PDEs in which the\nhighest-order term is diffusive, across multiple domains and parameters. G-FuNK\ncombines components that are parameter- and domain-adapted with others that are\nnot. The domain-adapted components are constructed using a weighted graph on\nthe discretized domain, where the graph Laplacian approximates the\nhighest-order diffusive term, ensuring boundary condition compliance and\ncapturing the parameter and domain-specific behavior. Meanwhile, the learned\ncomponents transfer across domains and parameters using our variant Fourier\nNeural Operators. This approach naturally embeds geometric and directional\ninformation, improving generalization to new test domains without need for\nretraining the network. To handle temporal dynamics, our method incorporates an\nintegrated ODE solver to predict the evolution of the system. Experiments show\nG-FuNK's capability to accurately approximate heat, reaction diffusion, and\ncardiac electrophysiology equations across various geometries and anisotropic\ndiffusivity fields. G-FuNK achieves low relative errors on unseen domains and\nfiber fields, significantly accelerating predictions compared to traditional\nfinite-element solvers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.SP",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04655v2",
    "published_date": "2024-10-06 23:55:34 UTC",
    "updated_date": "2024-10-09 13:46:31 UTC"
  },
  {
    "arxiv_id": "2410.04652v1",
    "title": "Multimodal 3D Fusion and In-Situ Learning for Spatially Aware AI",
    "authors": [
      "Chengyuan Xu",
      "Radha Kumaran",
      "Noah Stier",
      "Kangyou Yu",
      "Tobias Höllerer"
    ],
    "abstract": "Seamless integration of virtual and physical worlds in augmented reality\nbenefits from the system semantically \"understanding\" the physical environment.\nAR research has long focused on the potential of context awareness,\ndemonstrating novel capabilities that leverage the semantics in the 3D\nenvironment for various object-level interactions. Meanwhile, the computer\nvision community has made leaps in neural vision-language understanding to\nenhance environment perception for autonomous tasks. In this work, we introduce\na multimodal 3D object representation that unifies both semantic and linguistic\nknowledge with the geometric representation, enabling user-guided machine\nlearning involving physical objects. We first present a fast multimodal 3D\nreconstruction pipeline that brings linguistic understanding to AR by fusing\nCLIP vision-language features into the environment and object models. We then\npropose \"in-situ\" machine learning, which, in conjunction with the multimodal\nrepresentation, enables new tools and interfaces for users to interact with\nphysical spaces and objects in a spatially and linguistically meaningful\nmanner. We demonstrate the usefulness of the proposed system through two\nreal-world AR applications on Magic Leap 2: a) spatial search in physical\nenvironments with natural language and b) an intelligent inventory system that\ntracks object changes over time. We also make our full implementation and demo\ndata available at (https://github.com/cy-xu/spatially_aware_AI) to encourage\nfurther exploration and research in spatially aware AI.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV",
      "I.4.8; H.5.2"
    ],
    "primary_category": "cs.HC",
    "comment": "10 pages, 6 figures, accepted to IEEE ISMAR 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.04652v1",
    "published_date": "2024-10-06 23:25:21 UTC",
    "updated_date": "2024-10-06 23:25:21 UTC"
  },
  {
    "arxiv_id": "2410.11860v1",
    "title": "Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task",
    "authors": [
      "Chengyuan Xu",
      "Kuo-Chin Lien",
      "Tobias Höllerer"
    ],
    "abstract": "When designing an AI-assisted decision-making system, there is often a\ntradeoff between precision and recall in the AI's recommendations. We argue\nthat careful exploitation of this tradeoff can harness the complementary\nstrengths in the human-AI collaboration to significantly improve team\nperformance. We investigate a real-world video anonymization task for which\nrecall is paramount and more costly to improve. We analyze the performance of\n78 professional annotators working with a) no AI assistance, b) a\nhigh-precision \"restrained\" AI, and c) a high-recall \"zealous\" AI in over 3,466\nperson-hours of annotation work. In comparison, the zealous AI helps human\nteammates achieve significantly shorter task completion time and higher recall.\nIn a follow-up study, we remove AI assistance for everyone and find negative\ntraining effects on annotators trained with the restrained AI. These findings\nand our analysis point to important implications for the design of AI\nassistance in recall-demanding scenarios.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV",
      "H.5.0; I.2.0"
    ],
    "primary_category": "cs.HC",
    "comment": "15 pages, 14 figures, accepted to ACM CHI 2023",
    "pdf_url": "http://arxiv.org/pdf/2410.11860v1",
    "published_date": "2024-10-06 23:19:19 UTC",
    "updated_date": "2024-10-06 23:19:19 UTC"
  },
  {
    "arxiv_id": "2410.04640v2",
    "title": "Unpacking Failure Modes of Generative Policies: Runtime Monitoring of Consistency and Progress",
    "authors": [
      "Christopher Agia",
      "Rohan Sinha",
      "Jingyun Yang",
      "Zi-ang Cao",
      "Rika Antonova",
      "Marco Pavone",
      "Jeannette Bohg"
    ],
    "abstract": "Robot behavior policies trained via imitation learning are prone to failure\nunder conditions that deviate from their training data. Thus, algorithms that\nmonitor learned policies at test time and provide early warnings of failure are\nnecessary to facilitate scalable deployment. We propose Sentinel, a runtime\nmonitoring framework that splits the detection of failures into two\ncomplementary categories: 1) Erratic failures, which we detect using\nstatistical measures of temporal action consistency, and 2) task progression\nfailures, where we use Vision Language Models (VLMs) to detect when the policy\nconfidently and consistently takes actions that do not solve the task. Our\napproach has two key strengths. First, because learned policies exhibit diverse\nfailure modes, combining complementary detectors leads to significantly higher\naccuracy at failure detection. Second, using a statistical temporal action\nconsistency measure ensures that we quickly detect when multimodal, generative\npolicies exhibit erratic behavior at negligible computational cost. In\ncontrast, we only use VLMs to detect failure modes that are less\ntime-sensitive. We demonstrate our approach in the context of diffusion\npolicies trained on robotic mobile manipulation domains in both simulation and\nthe real world. By unifying temporal consistency detection and VLM runtime\nmonitoring, Sentinel detects 18% more failures than using either of the two\ndetectors alone and significantly outperforms baselines, thus highlighting the\nimportance of assigning specialized detectors to complementary categories of\nfailure. Qualitative results are made available at\nhttps://sites.google.com/stanford.edu/sentinel.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "I.2.6; I.2.7; I.2.9; I.2.10"
    ],
    "primary_category": "cs.RO",
    "comment": "Project page: https://sites.google.com/stanford.edu/sentinel. 35\n  pages, 9 figures. Accepted to the Conference on Robot Learning (CoRL) 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.04640v2",
    "published_date": "2024-10-06 22:13:30 UTC",
    "updated_date": "2024-10-10 17:09:24 UTC"
  },
  {
    "arxiv_id": "2410.04636v2",
    "title": "Multi-Tiered Self-Contrastive Learning for Medical Microwave Radiometry (MWR) Breast Cancer Detection",
    "authors": [
      "Christoforos Galazis",
      "Huiyi Wu",
      "Igor Goryanin"
    ],
    "abstract": "Improving breast cancer detection and monitoring techniques is a critical\nobjective in healthcare, driving the need for innovative imaging technologies\nand diagnostic approaches. This study introduces a novel multi-tiered\nself-contrastive model tailored for microwave radiometry (MWR) in breast cancer\ndetection. Our approach incorporates three distinct models: Local-MWR (L-MWR),\nRegional-MWR (R-MWR), and Global-MWR (G-MWR), designed to analyze varying\nsub-regional comparisons within the breasts. These models are integrated\nthrough the Joint-MWR (J-MWR) network, which leverages self-contrastive results\nat each analytical level to improve diagnostic accuracy. Utilizing a dataset of\n4,932 female patients, our research demonstrates the efficacy of our proposed\nmodels. Notably, the J-MWR model achieves a Matthew's correlation coefficient\nof 0.74 $\\pm$ 0.018, surpassing existing MWR neural networks and contrastive\nmethods. These findings highlight the potential of self-contrastive learning\ntechniques in improving the diagnostic accuracy and generalizability for\nMWR-based breast cancer detection. This advancement holds considerable promise\nfor future investigations into enabling point-of-care testing. The source code\nis available at: https://github.com/cgalaz01/self_contrastive_mwr.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04636v2",
    "published_date": "2024-10-06 21:51:02 UTC",
    "updated_date": "2025-01-27 12:35:24 UTC"
  },
  {
    "arxiv_id": "2410.04631v2",
    "title": "DeepLTL: Learning to Efficiently Satisfy Complex LTL Specifications for Multi-Task RL",
    "authors": [
      "Mathias Jackermeier",
      "Alessandro Abate"
    ],
    "abstract": "Linear temporal logic (LTL) has recently been adopted as a powerful formalism\nfor specifying complex, temporally extended tasks in multi-task reinforcement\nlearning (RL). However, learning policies that efficiently satisfy arbitrary\nspecifications not observed during training remains a challenging problem.\nExisting approaches suffer from several shortcomings: they are often only\napplicable to finite-horizon fragments of LTL, are restricted to suboptimal\nsolutions, and do not adequately handle safety constraints. In this work, we\npropose a novel learning approach to address these concerns. Our method\nleverages the structure of B\\\"uchi automata, which explicitly represent the\nsemantics of LTL specifications, to learn policies conditioned on sequences of\ntruth assignments that lead to satisfying the desired formulae. Experiments in\na variety of discrete and continuous domains demonstrate that our approach is\nable to zero-shot satisfy a wide range of finite- and infinite-horizon\nspecifications, and outperforms existing methods in terms of both satisfaction\nprobability and efficiency. Code available at: https://deep-ltl.github.io/",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "ICLR'25 (Oral)",
    "pdf_url": "http://arxiv.org/pdf/2410.04631v2",
    "published_date": "2024-10-06 21:30:38 UTC",
    "updated_date": "2025-03-29 11:33:49 UTC"
  },
  {
    "arxiv_id": "2410.04620v1",
    "title": "Passage Retrieval of Polish Texts Using OKAPI BM25 and an Ensemble of Cross Encoders",
    "authors": [
      "Jakub Pokrywka"
    ],
    "abstract": "Passage Retrieval has traditionally relied on lexical methods like TF-IDF and\nBM25. Recently, some neural network models have surpassed these methods in\nperformance. However, these models face challenges, such as the need for large\nannotated datasets and adapting to new domains. This paper presents a winning\nsolution to the Poleval 2023 Task 3: Passage Retrieval challenge, which\ninvolves retrieving passages of Polish texts in three domains: trivia, legal,\nand customer support. However, only the trivia domain was used for training and\ndevelopment data. The method used the OKAPI BM25 algorithm to retrieve\ndocuments and an ensemble of publicly available multilingual Cross Encoders for\nReranking. Fine-tuning the reranker models slightly improved performance but\nonly in the training domain, while it worsened in other domains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04620v1",
    "published_date": "2024-10-06 20:43:42 UTC",
    "updated_date": "2024-10-06 20:43:42 UTC"
  },
  {
    "arxiv_id": "2410.14702v1",
    "title": "Polymath: A Challenging Multi-modal Mathematical Reasoning Benchmark",
    "authors": [
      "Himanshu Gupta",
      "Shreyas Verma",
      "Ujjwala Anantheswaran",
      "Kevin Scaria",
      "Mihir Parmar",
      "Swaroop Mishra",
      "Chitta Baral"
    ],
    "abstract": "Multi-modal Large Language Models (MLLMs) exhibit impressive problem-solving\nabilities in various domains, but their visual comprehension and abstract\nreasoning skills remain under-evaluated. To this end, we present PolyMATH, a\nchallenging benchmark aimed at evaluating the general cognitive reasoning\nabilities of MLLMs. PolyMATH comprises 5,000 manually collected high-quality\nimages of cognitive textual and visual challenges across 10 distinct\ncategories, including pattern recognition, spatial reasoning, and relative\nreasoning. We conducted a comprehensive, and quantitative evaluation of 15\nMLLMs using four diverse prompting strategies, including Chain-of-Thought and\nStep-Back. The best scores achieved on PolyMATH are ~41%, ~36%, and ~27%,\nobtained by Claude-3.5 Sonnet, GPT-4o and Gemini-1.5 Pro respectively -\nhighlighting the logical and visual complexity of these questions. A further\nfine-grained error analysis reveals that these models struggle to understand\nspatial relations and perform drawn-out, high-level reasoning. This is further\nstrengthened by our ablation study estimating MLLM performance when given\ntextual descriptions in place of diagrams. As evidenced by ~4% improvement over\ntextual descriptions as opposed to actual images, we discover that models do\nnot truly comprehend visual diagrams and the spatial information therein, and\nare thus prone to logical errors. Finally, we evaluate the OpenAI o1 models and\nfind that their performance only matches the human baseline, highlighting the\ndifficulty of the benchmark. The results on PolyMATH highlight the room for\nimprovement in multi-modal reasoning and provide unique insights to guide the\ndevelopment of future MLLMs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "49 pages, (10 pages paper, 9 pages references, 30 pages appendix)",
    "pdf_url": "http://arxiv.org/pdf/2410.14702v1",
    "published_date": "2024-10-06 20:35:41 UTC",
    "updated_date": "2024-10-06 20:35:41 UTC"
  },
  {
    "arxiv_id": "2410.04612v2",
    "title": "Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF",
    "authors": [
      "Zhaolin Gao",
      "Wenhao Zhan",
      "Jonathan D. Chang",
      "Gokul Swamy",
      "Kianté Brantley",
      "Jason D. Lee",
      "Wen Sun"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable success at tasks like\nsummarization that involve a single turn of interaction. However, they can\nstill struggle with multi-turn tasks like dialogue that require long-term\nplanning. Previous works on multi-turn dialogue extend single-turn\nreinforcement learning from human feedback (RLHF) methods to the multi-turn\nsetting by treating all prior dialogue turns as a long context. Such approaches\nsuffer from covariate shift: the conversations in the training set have\nprevious turns generated by some reference policy, which means that low\ntraining error may not necessarily correspond to good performance when the\nlearner is actually in the conversation loop. In response, we introduce\nREgressing the RELative FUture (REFUEL), an efficient policy optimization\napproach designed to address multi-turn RLHF in LLMs. REFUEL employs a single\nmodel to estimate $Q$-values and trains on self-generated data, addressing the\ncovariate shift issue. REFUEL frames the multi-turn RLHF problem as a sequence\nof regression tasks on iteratively collected datasets, enabling ease of\nimplementation. Theoretically, we prove that REFUEL can match the performance\nof any policy covered by the training set. Empirically, we evaluate our\nalgorithm by using Llama-3.1-70B-it to simulate a user in conversation with our\nmodel. REFUEL consistently outperforms state-of-the-art methods such as DPO and\nREBEL across various settings. Furthermore, despite having only 8 billion\nparameters, Llama-3-8B-it fine-tuned with REFUEL outperforms Llama-3.1-70B-it\non long multi-turn dialogues. Implementation of REFUEL can be found at\nhttps://github.com/ZhaolinGao/REFUEL/, and models trained by REFUEL can be\nfound at https://huggingface.co/Cornell-AGI.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04612v2",
    "published_date": "2024-10-06 20:20:22 UTC",
    "updated_date": "2025-04-23 22:44:37 UTC"
  },
  {
    "arxiv_id": "2410.05338v1",
    "title": "Distributed Inference on Mobile Edge and Cloud: An Early Exit based Clustering Approach",
    "authors": [
      "Divya Jyoti Bajpai",
      "Manjesh Kumar Hanawal"
    ],
    "abstract": "Recent advances in Deep Neural Networks (DNNs) have demonstrated outstanding\nperformance across various domains. However, their large size is a challenge\nfor deployment on resource-constrained devices such as mobile, edge, and IoT\nplatforms. To overcome this, a distributed inference setup can be used where a\nsmall-sized DNN (initial few layers) can be deployed on mobile, a bigger\nversion on the edge, and the full-fledged, on the cloud. A sample that has low\ncomplexity (easy) could be then inferred on mobile, that has moderate\ncomplexity (medium) on edge, and higher complexity (hard) on the cloud. As the\ncomplexity of each sample is not known beforehand, the following question\narises in distributed inference: how to decide complexity so that it is\nprocessed by enough layers of DNNs. We develop a novel approach named DIMEE\nthat utilizes Early Exit (EE) strategies developed to minimize inference\nlatency in DNNs. DIMEE aims to improve the accuracy, taking into account the\noffloading cost from mobile to edge/cloud. Experimental validation on GLUE\ndatasets, encompassing various NLP tasks, shows that our method significantly\nreduces the inference cost (> 43%) while maintaining a minimal drop in accuracy\n(< 0.3%) compared to the case where all the inference is made in cloud.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.05338v1",
    "published_date": "2024-10-06 20:14:27 UTC",
    "updated_date": "2024-10-06 20:14:27 UTC"
  },
  {
    "arxiv_id": "2410.10855v3",
    "title": "Core Knowledge Deficits in Multi-Modal Language Models",
    "authors": [
      "Yijiang Li",
      "Qingying Gao",
      "Tianwei Zhao",
      "Bingyang Wang",
      "Haoran Sun",
      "Haiyun Lyu",
      "Dezhi Luo",
      "Hokin Deng"
    ],
    "abstract": "While Multimodal Large Language Models (MLLMs) demonstrate impressive\nabilities over high level perception and reasoning, their robustness in the\nwild still lags behind humans and exhibits diminished efficacy on simple tasks\nthat are intuitive for humans. We examine the hypothesis that these\ndeficiencies stem from the absence of core knowledge, rudimentary cognitive\nabilities innate to humans from early childhood. To probe core knowledge\nrepresentation in MLLMs, we draw from developmental cognitive sciences and\ndevelop a large-scale benchmark, CoreCognition dataset, encompassing 12 core\ncognitive concepts. We evaluate 219 models with 10 different prompts, leading\nto a total of 2409 data points for analysis. Our findings reveal core knowledge\ndeficits in early developed core abilities while models demonstrate human\ncomparable performance in high level cognition. Moreover, we find that low\nlevel abilities show little to no scaling, in stark contrast to high level\nabilities. Finally, we introduce an evaluation technique, Concept Hacking,\nthrough which we demonstrate that MLLMs do not genuinely advance toward core\nknowledge but instead rely on illusory understanding and shortcut learning as\nthey scale. Website with this\n$\\href{https://growing-ai-like-a-child.github.io/}{link}$.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "Website with this\n  $\\href{https://growing-ai-like-a-child.github.io/}{link}$",
    "pdf_url": "http://arxiv.org/pdf/2410.10855v3",
    "published_date": "2024-10-06 20:13:11 UTC",
    "updated_date": "2025-03-09 04:39:42 UTC"
  },
  {
    "arxiv_id": "2410.10854v1",
    "title": "Plausibly Problematic Questions in Multiple-Choice Benchmarks for Commonsense Reasoning",
    "authors": [
      "Shramay Palta",
      "Nishant Balepur",
      "Peter Rankel",
      "Sarah Wiegreffe",
      "Marine Carpuat",
      "Rachel Rudinger"
    ],
    "abstract": "Questions involving commonsense reasoning about everyday situations often\nadmit many $\\textit{possible}$ or $\\textit{plausible}$ answers. In contrast,\nmultiple-choice question (MCQ) benchmarks for commonsense reasoning require a\nhard selection of a single correct answer, which, in principle, should\nrepresent the $\\textit{most}$ plausible answer choice. On $250$ MCQ items\nsampled from two commonsense reasoning benchmarks, we collect $5,000$\nindependent plausibility judgments on answer choices. We find that for over 20%\nof the sampled MCQs, the answer choice rated most plausible does not match the\nbenchmark gold answers; upon manual inspection, we confirm that this subset\nexhibits higher rates of problems like ambiguity or semantic mismatch between\nquestion and answer choices. Experiments with LLMs reveal low accuracy and high\nvariation in performance on the subset, suggesting our plausibility criterion\nmay be helpful in identifying more reliable benchmark items for commonsense\nevaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 Camera Ready",
    "pdf_url": "http://arxiv.org/pdf/2410.10854v1",
    "published_date": "2024-10-06 19:04:24 UTC",
    "updated_date": "2024-10-06 19:04:24 UTC"
  },
  {
    "arxiv_id": "2410.04587v2",
    "title": "Hammer: Robust Function-Calling for On-Device Language Models via Function Masking",
    "authors": [
      "Qiqiang Lin",
      "Muning Wen",
      "Qiuying Peng",
      "Guanyu Nie",
      "Junwei Liao",
      "Jun Wang",
      "Xiaoyun Mo",
      "Jiamu Zhou",
      "Cheng Cheng",
      "Yin Zhao",
      "Jun Wang",
      "Weinan Zhang"
    ],
    "abstract": "Large language models have demonstrated impressive value in performing as\nautonomous agents when equipped with external tools and API calls. Nonetheless,\neffectively harnessing their potential for executing complex tasks crucially\nrelies on enhancements in their function calling capabilities. This paper\nidentifies a critical gap in existing function calling models, where\nperformance varies significantly across benchmarks, often due to being misled\nby specific naming conventions. To address such an issue, we introduce Hammer,\na novel family of foundation models specifically engineered for on-device\nfunction calling. Hammer employs an augmented dataset that enhances models'\nsensitivity to irrelevant functions and incorporates function masking\ntechniques to minimize misleading. Our empirical evaluations reveal that Hammer\nnot only outperforms larger models but also demonstrates robust generalization\nacross diverse benchmarks, achieving sota results. Our open source\ncontributions include a specialized dataset for irrelevance detection, a tuning\nframework for enhanced generalization, and the Hammer models, establishing a\nnew standard for function calling performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04587v2",
    "published_date": "2024-10-06 18:57:46 UTC",
    "updated_date": "2024-10-10 17:29:52 UTC"
  },
  {
    "arxiv_id": "2410.04568v1",
    "title": "Ranking Policy Learning via Marketplace Expected Value Estimation From Observational Data",
    "authors": [
      "Ehsan Ebrahimzadeh",
      "Nikhil Monga",
      "Hang Gao",
      "Alex Cozzi",
      "Abraham Bagherjeiran"
    ],
    "abstract": "We develop a decision making framework to cast the problem of learning a\nranking policy for search or recommendation engines in a two-sided e-commerce\nmarketplace as an expected reward optimization problem using observational\ndata. As a value allocation mechanism, the ranking policy allocates retrieved\nitems to the designated slots so as to maximize the user utility from the\nslotted items, at any given stage of the shopping journey. The objective of\nthis allocation can in turn be defined with respect to the underlying\nprobabilistic user browsing model as the expected number of interaction events\non presented items matching the user intent, given the ranking context. Through\nrecognizing the effect of ranking as an intervention action to inform users'\ninteractions with slotted items and the corresponding economic value of the\ninteraction events for the marketplace, we formulate the expected reward of the\nmarketplace as the collective value from all presented ranking actions. The key\nelement in this formulation is a notion of context value distribution, which\nsignifies not only the attribution of value to ranking interventions within a\nsession but also the distribution of marketplace reward across user sessions.\nWe build empirical estimates for the expected reward of the marketplace from\nobservational data that account for the heterogeneity of economic value across\nsession contexts as well as the distribution shifts in learning from\nobservational user activity data. The ranking policy can then be trained by\noptimizing the empirical expected reward estimates via standard Bayesian\ninference techniques. We report empirical results for a product search ranking\ntask in a major e-commerce platform demonstrating the fundamental trade-offs\ngoverned by ranking polices trained on empirical reward estimates with respect\nto extreme choices of the context value distribution.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "cs.IR",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.04568v1",
    "published_date": "2024-10-06 17:53:44 UTC",
    "updated_date": "2024-10-06 17:53:44 UTC"
  },
  {
    "arxiv_id": "2410.04552v1",
    "title": "Modeling Social Media Recommendation Impacts Using Academic Networks: A Graph Neural Network Approach",
    "authors": [
      "Sabrina Guidotti",
      "Gregor Donabauer",
      "Simone Somazzi",
      "Udo Kruschwitz",
      "Davide Taibi",
      "Dimitri Ognibene"
    ],
    "abstract": "The widespread use of social media has highlighted potential negative impacts\non society and individuals, largely driven by recommendation algorithms that\nshape user behavior and social dynamics. Understanding these algorithms is\nessential but challenging due to the complex, distributed nature of social\nmedia networks as well as limited access to real-world data. This study\nproposes to use academic social networks as a proxy for investigating\nrecommendation systems in social media. By employing Graph Neural Networks\n(GNNs), we develop a model that separates the prediction of academic infosphere\nfrom behavior prediction, allowing us to simulate recommender-generated\ninfospheres and assess the model's performance in predicting future\nco-authorships. Our approach aims to improve our understanding of\nrecommendation systems' roles and social networks modeling. To support the\nreproducibility of our work we publicly make available our implementations:\nhttps://github.com/DimNeuroLab/academic_network_project",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04552v1",
    "published_date": "2024-10-06 17:03:27 UTC",
    "updated_date": "2024-10-06 17:03:27 UTC"
  },
  {
    "arxiv_id": "2410.04543v1",
    "title": "Pullback Flow Matching on Data Manifolds",
    "authors": [
      "Friso de Kruiff",
      "Erik Bekkers",
      "Ozan Öktem",
      "Carola-Bibiane Schönlieb",
      "Willem Diepeveen"
    ],
    "abstract": "We propose Pullback Flow Matching (PFM), a novel framework for generative\nmodeling on data manifolds. Unlike existing methods that assume or learn\nrestrictive closed-form manifold mappings for training Riemannian Flow Matching\n(RFM) models, PFM leverages pullback geometry and isometric learning to\npreserve the underlying manifold's geometry while enabling efficient generation\nand precise interpolation in latent space. This approach not only facilitates\nclosed-form mappings on the data manifold but also allows for designable latent\nspaces, using assumed metrics on both data and latent manifolds. By enhancing\nisometric learning through Neural ODEs and proposing a scalable training\nobjective, we achieve a latent space more suitable for interpolation, leading\nto improved manifold learning and generative performance. We demonstrate PFM's\neffectiveness through applications in synthetic data, protein dynamics and\nprotein sequence data, generating novel proteins with specific properties. This\nmethod shows strong potential for drug discovery and materials science, where\ngenerating novel samples with specific properties is of great interest.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.DG",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04543v1",
    "published_date": "2024-10-06 16:41:26 UTC",
    "updated_date": "2024-10-06 16:41:26 UTC"
  },
  {
    "arxiv_id": "2410.04541v1",
    "title": "On Evaluating LLMs' Capabilities as Functional Approximators: A Bayesian Perspective",
    "authors": [
      "Shoaib Ahmed Siddiqui",
      "Yanzhi Chen",
      "Juyeon Heo",
      "Menglin Xia",
      "Adrian Weller"
    ],
    "abstract": "Recent works have successfully applied Large Language Models (LLMs) to\nfunction modeling tasks. However, the reasons behind this success remain\nunclear. In this work, we propose a new evaluation framework to comprehensively\nassess LLMs' function modeling abilities. By adopting a Bayesian perspective of\nfunction modeling, we discover that LLMs are relatively weak in understanding\npatterns in raw data, but excel at utilizing prior knowledge about the domain\nto develop a strong understanding of the underlying function. Our findings\noffer new insights about the strengths and limitations of LLMs in the context\nof function modeling.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04541v1",
    "published_date": "2024-10-06 16:30:47 UTC",
    "updated_date": "2024-10-06 16:30:47 UTC"
  },
  {
    "arxiv_id": "2410.04526v4",
    "title": "FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question Answering",
    "authors": [
      "Siqiao Xue",
      "Xiaojing Li",
      "Fan Zhou",
      "Qingyang Dai",
      "Zhixuan Chu",
      "Hongyuan Mei"
    ],
    "abstract": "In this paper, we introduce FAMMA, an open-source benchmark for\n\\underline{f}in\\underline{a}ncial \\underline{m}ultilingual\n\\underline{m}ultimodal question \\underline{a}nswering (QA). Our benchmark aims\nto evaluate the abilities of large language models (LLMs) in answering complex\nreasoning questions that require advanced financial knowledge. The benchmark\nhas two versions: FAMMA-Basic consists of 1,945 questions extracted from\nuniversity textbooks and exams, along with human-annotated answers and\nrationales; FAMMA-LivePro consists of 103 novel questions created by human\ndomain experts, with answers and rationales held out from the public for a\ncontamination-free evaluation. These questions cover advanced knowledge of 8\nmajor subfields in finance (e.g., corporate finance, derivatives, and portfolio\nmanagement). Some are in Chinese or French, while a majority of them are in\nEnglish. Each question has some non-text data such as charts, diagrams, or\ntables. Our experiments reveal that FAMMA poses a significant challenge on\nLLMs, including reasoning models such as GPT-o1 and DeepSeek-R1. Additionally,\nwe curated 1,270 reasoning trajectories of DeepSeek-R1 on the FAMMA-Basic data,\nand fine-tuned a series of open-source Qwen models using this reasoning data.\nWe found that training a model on these reasoning trajectories can\nsignificantly improve its performance on FAMMA-LivePro. We released our\nleaderboard, data, code, and trained models at\nhttps://famma-bench.github.io/famma/.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04526v4",
    "published_date": "2024-10-06 15:41:26 UTC",
    "updated_date": "2025-05-15 02:17:31 UTC"
  },
  {
    "arxiv_id": "2410.04523v1",
    "title": "Semi-Markovian Planning to Coordinate Aerial and Maritime Medical Evacuation Platforms",
    "authors": [
      "Mahdi Al-Husseini",
      "Kyle H. Wray",
      "Mykel J. Kochenderfer"
    ],
    "abstract": "The transfer of patients between two aircraft using an underway watercraft\nincreases medical evacuation reach and flexibility in maritime environments.\nThe selection of any one of multiple underway watercraft for patient exchange\nis complicated by participating aircraft utilization history and a\nparticipating watercraft position and velocity. The selection problem is\nmodeled as a semi-Markov decision process with an action space including both\nfixed land and moving watercraft exchange points. Monte Carlo tree search with\nroot parallelization is used to select optimal exchange points and determine\naircraft dispatch times. Model parameters are varied in simulation to identify\nrepresentative scenarios where watercraft exchange points reduce incident\nresponse times. We find that an optimal policy with watercraft exchange points\noutperforms an optimal policy without watercraft exchange points and a greedy\npolicy by 35% and 40%, respectively. In partnership with the United States\nArmy, we deploy for the first time the watercraft exchange point by executing a\nmock patient transfer with a manikin between two HH-60M medical evacuation\nhelicopters and an underway Army Logistic Support Vessel south of the Hawaiian\nisland of Oahu. Both helicopters were dispatched in accordance with our\noptimized decision strategy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04523v1",
    "published_date": "2024-10-06 15:32:59 UTC",
    "updated_date": "2024-10-06 15:32:59 UTC"
  },
  {
    "arxiv_id": "2410.04503v1",
    "title": "LRHP: Learning Representations for Human Preferences via Preference Pairs",
    "authors": [
      "Chenglong Wang",
      "Yang Gan",
      "Yifu Huo",
      "Yongyu Mu",
      "Qiaozhi He",
      "Murun Yang",
      "Tong Xiao",
      "Chunliang Zhang",
      "Tongran Liu",
      "Jingbo Zhu"
    ],
    "abstract": "To improve human-preference alignment training, current research has\ndeveloped numerous preference datasets consisting of preference pairs labeled\nas \"preferred\" or \"dispreferred\". These preference pairs are typically used to\nencode human preferences into a single numerical value through reward modeling,\nwhich acts as a reward signal during reinforcement learning from human feedback\n(RLHF). However, representing these human preferences as a numerical value\ncomplicates the analysis of these preferences and restricts their broader\napplications other than RLHF. In contrast, in this work, we introduce a\npreference representation learning task that aims to construct a richer and\nmore structured representation of human preferences. We further develop a more\ngeneralizable framework, Learning Representations for Human Preferences via\npreference pairs (namely LRHP), which extends beyond traditional reward\nmodeling to tackle this task. We verify the utility of preference\nrepresentations in two downstream tasks: preference data selection and\npreference margin prediction. Building upon the human preferences in\nrepresentations, we achieve strong performance in both tasks, significantly\noutperforming baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04503v1",
    "published_date": "2024-10-06 14:48:28 UTC",
    "updated_date": "2024-10-06 14:48:28 UTC"
  },
  {
    "arxiv_id": "2410.04501v3",
    "title": "Leveraging Large Language Models for Suicide Detection on Social Media with Limited Labels",
    "authors": [
      "Vy Nguyen",
      "Chau Pham"
    ],
    "abstract": "The increasing frequency of suicidal thoughts highlights the importance of\nearly detection and intervention. Social media platforms, where users often\nshare personal experiences and seek help, could be utilized to identify\nindividuals at risk. However, the large volume of daily posts makes manual\nreview impractical. This paper explores the use of Large Language Models (LLMs)\nto automatically detect suicidal content in text-based social media posts. We\npropose a novel method for generating pseudo-labels for unlabeled data by\nprompting LLMs, along with traditional classification fine-tuning techniques to\nenhance label accuracy. To create a strong suicide detection model, we develop\nan ensemble approach involving prompting with Qwen2-72B-Instruct, and using\nfine-tuned models such as Llama3-8B, Llama3.1-8B, and Gemma2-9B. We evaluate\nour approach on the dataset of the Suicide Ideation Detection on Social Media\nChallenge, a track of the IEEE Big Data 2024 Big Data Cup. Additionally, we\nconduct a comprehensive analysis to assess the impact of different models and\nfine-tuning strategies on detection performance. Experimental results show that\nthe ensemble model significantly improves the detection accuracy, by 5% points\ncompared with the individual models. It achieves a weight F1 score of 0.770 on\nthe public test set, and 0.731 on the private test set, providing a promising\nsolution for identifying suicidal content in social media. Our analysis shows\nthat the choice of LLMs affects the prompting performance, with larger models\nproviding better accuracy. Our code and checkpoints are publicly available at\nhttps://github.com/khanhvynguyen/Suicide_Detection_LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at IEEE International Conference on Big Data 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.04501v3",
    "published_date": "2024-10-06 14:45:01 UTC",
    "updated_date": "2024-11-01 03:42:37 UTC"
  },
  {
    "arxiv_id": "2410.04499v1",
    "title": "Adjusting Pretrained Backbones for Performativity",
    "authors": [
      "Berker Demirel",
      "Lingjing Kong",
      "Kun Zhang",
      "Theofanis Karaletsos",
      "Celestine Mendler-Dünner",
      "Francesco Locatello"
    ],
    "abstract": "With the widespread deployment of deep learning models, they influence their\nenvironment in various ways. The induced distribution shifts can lead to\nunexpected performance degradation in deployed models. Existing methods to\nanticipate performativity typically incorporate information about the deployed\nmodel into the feature vector when predicting future outcomes. While enjoying\nappealing theoretical properties, modifying the input dimension of the\nprediction task is often not practical. To address this, we propose a novel\ntechnique to adjust pretrained backbones for performativity in a modular way,\nachieving better sample efficiency and enabling the reuse of existing deep\nlearning assets. Focusing on performative label shift, the key idea is to train\na shallow adapter module to perform a Bayes-optimal label shift correction to\nthe backbone's logits given a sufficient statistic of the model to be deployed.\nAs such, our framework decouples the construction of input-specific feature\nembeddings from the mechanism governing performativity. Motivated by dynamic\nbenchmarking as a use-case, we evaluate our approach under adversarial\nsampling, for vision and language tasks. We show how it leads to smaller loss\nalong the retraining trajectory and enables us to effectively select among\ncandidate models to anticipate performance degradations. More broadly, our work\nprovides a first baseline for addressing performativity in deep learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04499v1",
    "published_date": "2024-10-06 14:41:13 UTC",
    "updated_date": "2024-10-06 14:41:13 UTC"
  },
  {
    "arxiv_id": "2410.04497v1",
    "title": "Generalizability analysis of deep learning predictions of human brain responses to augmented and semantically novel visual stimuli",
    "authors": [
      "Valentyn Piskovskyi",
      "Riccardo Chimisso",
      "Sabrina Patania",
      "Tom Foulsham",
      "Giuseppe Vizzari",
      "Dimitri Ognibene"
    ],
    "abstract": "The purpose of this work is to investigate the soundness and utility of a\nneural network-based approach as a framework for exploring the impact of image\nenhancement techniques on visual cortex activation. In a preliminary study, we\nprepare a set of state-of-the-art brain encoding models, selected among the top\n10 methods that participated in The Algonauts Project 2023 Challenge [16]. We\nanalyze their ability to make valid predictions about the effects of various\nimage enhancement techniques on neural responses. Given the impossibility of\nacquiring the actual data due to the high costs associated with brain imaging\nprocedures, our investigation builds up on a series of experiments.\nSpecifically, we analyze the ability of brain encoders to estimate the cerebral\nreaction to various augmentations by evaluating the response to augmentations\ntargeting objects (i.e., faces and words) with known impact on specific areas.\nMoreover, we study the predicted activation in response to objects unseen\nduring training, exploring the impact of semantically out-of-distribution\nstimuli. We provide relevant evidence for the generalization ability of the\nmodels forming the proposed framework, which appears to be promising for the\nidentification of the optimal visual augmentation filter for a given task,\nmodel-driven design strategies as well as for AR and VR applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04497v1",
    "published_date": "2024-10-06 14:29:02 UTC",
    "updated_date": "2024-10-06 14:29:02 UTC"
  },
  {
    "arxiv_id": "2410.10853v1",
    "title": "Mitigating Hallucinations Using Ensemble of Knowledge Graph and Vector Store in Large Language Models to Enhance Mental Health Support",
    "authors": [
      "Abdul Muqtadir",
      "Hafiz Syed Muhammad Bilal",
      "Ayesha Yousaf",
      "Hafiz Farooq Ahmed",
      "Jamil Hussain"
    ],
    "abstract": "This research work delves into the manifestation of hallucination within\nLarge Language Models (LLMs) and its consequential impacts on applications\nwithin the domain of mental health. The primary objective is to discern\neffective strategies for curtailing hallucinatory occurrences, thereby\nbolstering the dependability and security of LLMs in facilitating mental health\ninterventions such as therapy, counseling, and the dissemination of pertinent\ninformation. Through rigorous investigation and analysis, this study seeks to\nelucidate the underlying mechanisms precipitating hallucinations in LLMs and\nsubsequently propose targeted interventions to alleviate their occurrence. By\naddressing this critical issue, the research endeavors to foster a more robust\nframework for the utilization of LLMs within mental health contexts, ensuring\ntheir efficacy and reliability in aiding therapeutic processes and delivering\naccurate information to individuals seeking mental health support.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.10853v1",
    "published_date": "2024-10-06 14:26:37 UTC",
    "updated_date": "2024-10-06 14:26:37 UTC"
  },
  {
    "arxiv_id": "2410.04492v5",
    "title": "Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification",
    "authors": [
      "Zhaorui Tan",
      "Xi Yang",
      "Qiufeng Wang",
      "Anh Nguyen",
      "Kaizhu Huang"
    ],
    "abstract": "Vision models excel in image classification but struggle to generalize to\nunseen data, such as classifying images from unseen domains or discovering\nnovel categories. In this paper, we explore the relationship between logical\nreasoning and deep learning generalization in visual classification. A logical\nregularization termed L-Reg is derived which bridges a logical analysis\nframework to image classification. Our work reveals that L-Reg reduces the\ncomplexity of the model in terms of the feature distribution and classifier\nweights. Specifically, we unveil the interpretability brought by L-Reg, as it\nenables the model to extract the salient features, such as faces to persons,\nfor classification. Theoretical analysis and experiments demonstrate that L-Reg\nenhances generalization across various scenarios, including multi-domain\ngeneralization and generalized category discovery. In complex real-world\nscenarios where images span unknown classes and unseen domains, L-Reg\nconsistently improves generalization, highlighting its practical efficacy.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NeurIPS2024 as Spotlight",
    "pdf_url": "http://arxiv.org/pdf/2410.04492v5",
    "published_date": "2024-10-06 14:11:39 UTC",
    "updated_date": "2025-01-27 20:34:06 UTC"
  },
  {
    "arxiv_id": "2410.04491v1",
    "title": "Knowledge-Guided Dynamic Modality Attention Fusion Framework for Multimodal Sentiment Analysis",
    "authors": [
      "Xinyu Feng",
      "Yuming Lin",
      "Lihua He",
      "You Li",
      "Liang Chang",
      "Ya Zhou"
    ],
    "abstract": "Multimodal Sentiment Analysis (MSA) utilizes multimodal data to infer the\nusers' sentiment. Previous methods focus on equally treating the contribution\nof each modality or statically using text as the dominant modality to conduct\ninteraction, which neglects the situation where each modality may become\ndominant. In this paper, we propose a Knowledge-Guided Dynamic Modality\nAttention Fusion Framework (KuDA) for multimodal sentiment analysis. KuDA uses\nsentiment knowledge to guide the model dynamically selecting the dominant\nmodality and adjusting the contributions of each modality. In addition, with\nthe obtained multimodal representation, the model can further highlight the\ncontribution of dominant modality through the correlation evaluation loss.\nExtensive experiments on four MSA benchmark datasets indicate that KuDA\nachieves state-of-the-art performance and is able to adapt to different\nscenarios of dominant modality.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP Findings 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.04491v1",
    "published_date": "2024-10-06 14:10:28 UTC",
    "updated_date": "2024-10-06 14:10:28 UTC"
  },
  {
    "arxiv_id": "2410.04488v1",
    "title": "A Pluggable Common Sense-Enhanced Framework for Knowledge Graph Completion",
    "authors": [
      "Guanglin Niu",
      "Bo Li",
      "Siling Feng"
    ],
    "abstract": "Knowledge graph completion (KGC) tasks aim to infer missing facts in a\nknowledge graph (KG) for many knowledge-intensive applications. However,\nexisting embedding-based KGC approaches primarily rely on factual triples,\npotentially leading to outcomes inconsistent with common sense. Besides,\ngenerating explicit common sense is often impractical or costly for a KG. To\naddress these challenges, we propose a pluggable common sense-enhanced KGC\nframework that incorporates both fact and common sense for KGC. This framework\nis adaptable to different KGs based on their entity concept richness and has\nthe capability to automatically generate explicit or implicit common sense from\nfactual triples. Furthermore, we introduce common sense-guided negative\nsampling and a coarse-to-fine inference approach for KGs with rich entity\nconcepts. For KGs without concepts, we propose a dual scoring scheme involving\na relation-aware concept embedding mechanism. Importantly, our approach can be\nintegrated as a pluggable module for many knowledge graph embedding (KGE)\nmodels, facilitating joint common sense and fact-driven training and inference.\nThe experiments illustrate that our framework exhibits good scalability and\noutperforms existing models across various KGC tasks.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "I.2; I.2.4; I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 7 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.04488v1",
    "published_date": "2024-10-06 14:06:12 UTC",
    "updated_date": "2024-10-06 14:06:12 UTC"
  },
  {
    "arxiv_id": "2410.04485v1",
    "title": "Exploring the Potential of Conversational Test Suite Based Program Repair on SWE-bench",
    "authors": [
      "Anton Cheshkov",
      "Pavel Zadorozhny",
      "Rodion Levichev",
      "Evgeny Maslov",
      "Ronaldo Franco Jaldin"
    ],
    "abstract": "Automatic program repair at project level may open yet to be seen\nopportunities in various fields of human activity. Since the SWE-Bench\nchallenge was presented, we have seen numerous of solutions. Patch generation\nis a part of program repair, and test suite-based conversational patch\ngeneration has proven its effectiveness. However, the potential of\nconversational patch generation has not yet specifically estimated on\nSWE-Bench. This study reports experimental results aimed at evaluating the\nindividual effectiveness of conversational patch generation on problems from\nSWE-Bench. The experiments show that a simple conversational pipeline based on\nLLaMA 3.1 70B can generate valid patches in 47\\% of cases, which is comparable\nto the state-of-the-art in program repair on SWE-Bench.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.SE",
    "comment": "3 pages, 2 figures, 1 algorithm, appendix",
    "pdf_url": "http://arxiv.org/pdf/2410.04485v1",
    "published_date": "2024-10-06 13:55:33 UTC",
    "updated_date": "2024-10-06 13:55:33 UTC"
  },
  {
    "arxiv_id": "2410.04480v1",
    "title": "Learning to Solve Abstract Reasoning Problems with Neurosymbolic Program Synthesis and Task Generation",
    "authors": [
      "Jakub Bednarek",
      "Krzysztof Krawiec"
    ],
    "abstract": "The ability to think abstractly and reason by analogy is a prerequisite to\nrapidly adapt to new conditions, tackle newly encountered problems by\ndecomposing them, and synthesize knowledge to solve problems comprehensively.\nWe present TransCoder, a method for solving abstract problems based on neural\nprogram synthesis, and conduct a comprehensive analysis of decisions made by\nthe generative module of the proposed architecture. At the core of TransCoder\nis a typed domain-specific language, designed to facilitate feature engineering\nand abstract reasoning. In training, we use the programs that failed to solve\ntasks to generate new tasks and gather them in a synthetic dataset. As each\nsynthetic task created in this way has a known associated program (solution),\nthe model is trained on them in supervised mode. Solutions are represented in a\ntransparent programmatic form, which can be inspected and verified. We\ndemonstrate TransCoder's performance using the Abstract Reasoning Corpus\ndataset, for which our framework generates tens of thousands of synthetic\nproblems with corresponding solutions and facilitates systematic progress in\nlearning.",
    "categories": [
      "cs.AI",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "18th International Conference on Neural-Symbolic Learning and\n  Reasoning",
    "pdf_url": "http://arxiv.org/pdf/2410.04480v1",
    "published_date": "2024-10-06 13:42:53 UTC",
    "updated_date": "2024-10-06 13:42:53 UTC"
  },
  {
    "arxiv_id": "2410.10852v1",
    "title": "SafeLLM: Domain-Specific Safety Monitoring for Large Language Models: A Case Study of Offshore Wind Maintenance",
    "authors": [
      "Connor Walker",
      "Callum Rothon",
      "Koorosh Aslansefat",
      "Yiannis Papadopoulos",
      "Nina Dethlefs"
    ],
    "abstract": "The Offshore Wind (OSW) industry is experiencing significant expansion,\nresulting in increased Operations \\& Maintenance (O\\&M) costs. Intelligent\nalarm systems offer the prospect of swift detection of component failures and\nprocess anomalies, enabling timely and precise interventions that could yield\nreductions in resource expenditure, as well as scheduled and unscheduled\ndowntime. This paper introduces an innovative approach to tackle this challenge\nby capitalising on Large Language Models (LLMs). We present a specialised\nconversational agent that incorporates statistical techniques to calculate\ndistances between sentences for the detection and filtering of hallucinations\nand unsafe output. This potentially enables improved interpretation of alarm\nsequences and the generation of safer repair action recommendations by the\nagent. Preliminary findings are presented with the approach applied to\nChatGPT-4 generated test sentences. The limitation of using ChatGPT-4 and the\npotential for enhancement of this agent through re-training with specialised\nOSW datasets are discussed.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.10852v1",
    "published_date": "2024-10-06 13:00:53 UTC",
    "updated_date": "2024-10-06 13:00:53 UTC"
  },
  {
    "arxiv_id": "2410.10851v2",
    "title": "LLM Gesticulator: Leveraging Large Language Models for Scalable and Controllable Co-Speech Gesture Synthesis",
    "authors": [
      "Haozhou Pang",
      "Tianwei Ding",
      "Lanshan He",
      "Ming Tao",
      "Lu Zhang",
      "Qi Gan"
    ],
    "abstract": "In this work, we present LLM Gesticulator, an LLM-based audio-driven\nco-speech gesture generation framework that synthesizes full-body animations\nthat are rhythmically aligned with the input audio while exhibiting natural\nmovements and editability. Compared to previous work, our model demonstrates\nsubstantial scalability. As the size of the backbone LLM model increases, our\nframework shows proportional improvements in evaluation metrics (a.k.a. scaling\nlaw). Our method also exhibits strong controllability where the content, style\nof the generated gestures can be controlled by text prompt. To the best of our\nknowledge, LLM gesticulator is the first work that use LLM on the co-speech\ngeneration task. Evaluation with existing objective metrics and user studies\nindicate that our framework outperforms prior works.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.10851v2",
    "published_date": "2024-10-06 12:53:07 UTC",
    "updated_date": "2024-10-22 13:08:02 UTC"
  },
  {
    "arxiv_id": "2410.04468v4",
    "title": "Revisiting In-context Learning Inference Circuit in Large Language Models",
    "authors": [
      "Hakaze Cho",
      "Mariko Kato",
      "Yoshihiro Sakai",
      "Naoya Inoue"
    ],
    "abstract": "In-context Learning (ICL) is an emerging few-shot learning paradigm on\nLanguage Models (LMs) with inner mechanisms un-explored. There are already\nexisting works describing the inner processing of ICL, while they struggle to\ncapture all the inference phenomena in large language models. Therefore, this\npaper proposes a comprehensive circuit to model the inference dynamics and try\nto explain the observed phenomena of ICL. In detail, we divide ICL inference\ninto 3 major operations: (1) Input Text Encode: LMs encode every input text (in\nthe demonstrations and queries) into linear representation in the hidden states\nwith sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge\nthe encoded representations of demonstrations with their corresponding label\ntokens to produce joint representations of labels and demonstrations. (3)\nFeature Retrieval and Copy: LMs search the joint representations of\ndemonstrations similar to the query representation on a task subspace, and copy\nthe searched representations into the query. Then, language model heads capture\nthese copied label representations to a certain extent and decode them into\npredicted labels. Through careful measurements, the proposed inference circuit\nsuccessfully captures and unifies many fragmented phenomena observed during the\nICL process, making it a comprehensive and practical explanation of the ICL\ninference process. Moreover, ablation analysis by disabling the proposed steps\nseriously damages the ICL performance, suggesting the proposed inference\ncircuit is a dominating mechanism. Additionally, we confirm and list some\nbypass mechanisms that solve ICL tasks in parallel with the proposed circuit.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "37 pages, 41 figures, 8 tables. ICLR 2025 Accepted. Camera-ready\n  Version",
    "pdf_url": "http://arxiv.org/pdf/2410.04468v4",
    "published_date": "2024-10-06 12:50:15 UTC",
    "updated_date": "2025-02-20 12:58:28 UTC"
  },
  {
    "arxiv_id": "2410.04457v1",
    "title": "An Attention-Based Algorithm for Gravity Adaptation Zone Calibration",
    "authors": [
      "Chen Yu"
    ],
    "abstract": "Accurate calibration of gravity adaptation zones is of great significance in\nfields such as underwater navigation, geophysical exploration, and marine\nengineering. With the increasing application of gravity field data in these\nareas, traditional calibration methods based on single features are becoming\ninadequate for capturing the complex characteristics of gravity fields and\naddressing the intricate interrelationships among multidimensional data. This\npaper proposes an attention-enhanced algorithm for gravity adaptation zone\ncalibration. By introducing an attention mechanism, the algorithm adaptively\nfuses multidimensional gravity field features and dynamically assigns feature\nweights, effectively solving the problems of multicollinearity and redundancy\ninherent in traditional feature selection methods, significantly improving\ncalibration accuracy and robustness.In addition, a large-scale gravity field\ndataset with over 10,000 sampling points was constructed, and Kriging\ninterpolation was used to enhance the spatial resolution of the data, providing\na reliable data foundation for model training and evaluation. We conducted both\nqualitative and quantitative experiments on several classical machine learning\nmodels (such as SVM, GBDT, and RF), and the results demonstrate that the\nproposed algorithm significantly improves performance across these models,\noutperforming other traditional feature selection methods. The method proposed\nin this paper provides a new solution for gravity adaptation zone calibration,\nshowing strong generalization ability and potential for application in complex\nenvironments. The code is available at \\href{this link}\n{https://github.com/hulnifox/RF-ATTN}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.geo-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "15pages",
    "pdf_url": "http://arxiv.org/pdf/2410.04457v1",
    "published_date": "2024-10-06 12:03:13 UTC",
    "updated_date": "2024-10-06 12:03:13 UTC"
  },
  {
    "arxiv_id": "2410.09084v1",
    "title": "Diagnosing Robotics Systems Issues with Large Language Models",
    "authors": [
      "Jordis Emilia Herrmann",
      "Aswath Mandakath Gopinath",
      "Mikael Norrlof",
      "Mark Niklas Müller"
    ],
    "abstract": "Quickly resolving issues reported in industrial applications is crucial to\nminimize economic impact. However, the required data analysis makes diagnosing\nthe underlying root causes a challenging and time-consuming task, even for\nexperts. In contrast, large language models (LLMs) excel at analyzing large\namounts of data. Indeed, prior work in AI-Ops demonstrates their effectiveness\nin analyzing IT systems. Here, we extend this work to the challenging and\nlargely unexplored domain of robotics systems. To this end, we create\nSYSDIAGBENCH, a proprietary system diagnostics benchmark for robotics,\ncontaining over 2500 reported issues. We leverage SYSDIAGBENCH to investigate\nthe performance of LLMs for root cause analysis, considering a range of model\nsizes and adaptation techniques. Our results show that QLoRA finetuning can be\nsufficient to let a 7B-parameter model outperform GPT-4 in terms of diagnostic\naccuracy while being significantly more cost-effective. We validate our\nLLM-as-a-judge results with a human expert study and find that our best model\nachieves similar approval ratings as our reference labels.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09084v1",
    "published_date": "2024-10-06 11:58:12 UTC",
    "updated_date": "2024-10-06 11:58:12 UTC"
  },
  {
    "arxiv_id": "2410.16293v1",
    "title": "Hawk: An Efficient NALM System for Accurate Low-Power Appliance Recognition",
    "authors": [
      "Zijian Wang",
      "Xingzhou Zhang",
      "Yifan Wang",
      "Xiaohui Peng",
      "Zhiwei Xu"
    ],
    "abstract": "Non-intrusive Appliance Load Monitoring (NALM) aims to recognize individual\nappliance usage from the main meter without indoor sensors. However, existing\nsystems struggle to balance dataset construction efficiency and event/state\nrecognition accuracy, especially for low-power appliance recognition. This\npaper introduces Hawk, an efficient and accurate NALM system that operates in\ntwo stages: dataset construction and event recognition. In the data\nconstruction stage, we efficiently collect a balanced and diverse dataset,\nHawkDATA, based on balanced Gray code and enable automatic data annotations via\na sampling synchronization strategy called shared perceptible time. During the\nevent recognition stage, our algorithm integrates steady-state differential\npre-processing and voting-based post-processing for accurate event recognition\nfrom the aggregate current. Experimental results show that HawkDATA takes only\n1/71.5 of the collection time to collect 6.34x more appliance state\ncombinations than the baseline. In HawkDATA and a widely used dataset, Hawk\nachieves an average F1 score of 93.94% for state recognition and 97.07% for\nevent recognition, which is a 47. 98% and 11. 57% increase over SOTA\nalgorithms. Furthermore, selected appliance subsets and the model trained from\nHawkDATA are deployed in two real-world scenarios with many unknown background\nappliances. The average F1 scores of event recognition are 96.02% and 94.76%.\nHawk's source code and HawkDATA are accessible at\nhttps://github.com/WZiJ/SenSys24-Hawk.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "Accepted to the 22nd ACM Conference on Embedded Networked Sensor\n  Systems (SenSys 2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.16293v1",
    "published_date": "2024-10-06 11:26:30 UTC",
    "updated_date": "2024-10-06 11:26:30 UTC"
  },
  {
    "arxiv_id": "2410.04452v1",
    "title": "MindScope: Exploring cognitive biases in large language models through Multi-Agent Systems",
    "authors": [
      "Zhentao Xie",
      "Jiabao Zhao",
      "Yilei Wang",
      "Jinxin Shi",
      "Yanhong Bai",
      "Xingjiao Wu",
      "Liang He"
    ],
    "abstract": "Detecting cognitive biases in large language models (LLMs) is a fascinating\ntask that aims to probe the existing cognitive biases within these models.\nCurrent methods for detecting cognitive biases in language models generally\nsuffer from incomplete detection capabilities and a restricted range of\ndetectable bias types. To address this issue, we introduced the 'MindScope'\ndataset, which distinctively integrates static and dynamic elements. The static\ncomponent comprises 5,170 open-ended questions spanning 72 cognitive bias\ncategories. The dynamic component leverages a rule-based, multi-agent\ncommunication framework to facilitate the generation of multi-round dialogues.\nThis framework is flexible and readily adaptable for various psychological\nexperiments involving LLMs. In addition, we introduce a multi-agent detection\nmethod applicable to a wide range of detection tasks, which integrates\nRetrieval-Augmented Generation (RAG), competitive debate, and a reinforcement\nlearning-based decision module. Demonstrating substantial effectiveness, this\nmethod has shown to improve detection accuracy by as much as 35.10% compared to\nGPT-4. Codes and appendix are available at\nhttps://github.com/2279072142/MindScope.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages,7 figures,Our paper has been accepted for presentation at the\n  2024 European Conference on Artificial Intelligence (ECAI 2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.04452v1",
    "published_date": "2024-10-06 11:23:56 UTC",
    "updated_date": "2024-10-06 11:23:56 UTC"
  },
  {
    "arxiv_id": "2410.04444v3",
    "title": "Gödel Agent: A Self-Referential Agent Framework for Recursive Self-Improvement",
    "authors": [
      "Xunjian Yin",
      "Xinyi Wang",
      "Liangming Pan",
      "Xiaojun Wan",
      "William Yang Wang"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) has significantly\nenhanced the capabilities of AI-driven agents across various tasks. However,\nexisting agentic systems, whether based on fixed pipeline algorithms or\npre-defined meta-learning frameworks, cannot search the whole agent design\nspace due to the restriction of human-designed components, and thus might miss\nthe globally optimal agent design. In this paper, we introduce G\\\"odel Agent, a\nself-evolving framework inspired by the G\\\"odel machine, enabling agents to\nrecursively improve themselves without relying on predefined routines or fixed\noptimization algorithms. G\\\"odel Agent leverages LLMs to dynamically modify its\nown logic and behavior, guided solely by high-level objectives through\nprompting. Experimental results on mathematical reasoning and complex agent\ntasks demonstrate that implementation of G\\\"odel Agent can achieve continuous\nself-improvement, surpassing manually crafted agents in performance,\nefficiency, and generalizability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Work in progress. The code can be found at\n  https://github.com/Arvid-pku/Godel_Agent",
    "pdf_url": "http://arxiv.org/pdf/2410.04444v3",
    "published_date": "2024-10-06 10:49:40 UTC",
    "updated_date": "2025-02-18 06:44:29 UTC"
  },
  {
    "arxiv_id": "2410.04439v1",
    "title": "Empowering Backbone Models for Visual Text Generation with Input Granularity Control and Glyph-Aware Training",
    "authors": [
      "Wenbo Li",
      "Guohao Li",
      "Zhibin Lan",
      "Xue Xu",
      "Wanru Zhuang",
      "Jiachen Liu",
      "Xinyan Xiao",
      "Jinsong Su"
    ],
    "abstract": "Diffusion-based text-to-image models have demonstrated impressive\nachievements in diversity and aesthetics but struggle to generate images with\nlegible visual texts. Existing backbone models have limitations such as\nmisspelling, failing to generate texts, and lack of support for Chinese text,\nbut their development shows promising potential. In this paper, we propose a\nseries of methods, aiming to empower backbone models to generate visual texts\nin English and Chinese. We first conduct a preliminary study revealing that\nByte Pair Encoding (BPE) tokenization and the insufficient learning of\ncross-attention modules restrict the performance of the backbone models. Based\non these observations, we make the following improvements: (1) We design a\nmixed granularity input strategy to provide more suitable text representations;\n(2) We propose to augment the conventional training objective with three\nglyph-aware training losses, which enhance the learning of cross-attention\nmodules and encourage the model to focus on visual texts. Through experiments,\nwe demonstrate that our methods can effectively empower backbone models to\ngenerate semantic relevant, aesthetically appealing, and accurate visual text\nimages, while maintaining their fundamental image generation quality.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04439v1",
    "published_date": "2024-10-06 10:25:39 UTC",
    "updated_date": "2024-10-06 10:25:39 UTC"
  },
  {
    "arxiv_id": "2410.04433v1",
    "title": "CAPEEN: Image Captioning with Early Exits and Knowledge Distillation",
    "authors": [
      "Divya Jyoti Bajpai",
      "Manjesh Kumar Hanawal"
    ],
    "abstract": "Deep neural networks (DNNs) have made significant progress in recognizing\nvisual elements and generating descriptive text in image-captioning tasks.\nHowever, their improved performance comes from increased computational burden\nand inference latency. Early Exit (EE) strategies can be used to enhance their\nefficiency, but their adaptation presents challenges in image captioning as it\nrequires varying levels of semantic information for accurate predictions. To\novercome this, we introduce CAPEEN to improve the performance of EE strategies\nusing knowledge distillation. Inference in CAPEEN is completed at intermediary\nlayers if prediction confidence exceeds a predefined value learned from the\ntraining data. To account for real-world deployments, where target\ndistributions could drift from that of training samples, we introduce a variant\nA-CAPEEN to adapt the thresholds on the fly using Multiarmed bandits framework.\nExperiments on the MS COCO and Flickr30k datasets show that CAPEEN gains\nspeedup of 1.77x while maintaining competitive performance compared to the\nfinal layer, and A-CAPEEN additionally offers robustness against distortions.\nThe source code is available at https://github.com/Div290/CapEEN",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "To appear in EMNLP (finding) 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.04433v1",
    "published_date": "2024-10-06 10:05:01 UTC",
    "updated_date": "2024-10-06 10:05:01 UTC"
  },
  {
    "arxiv_id": "2410.16292v1",
    "title": "An evaluation of LLM code generation capabilities through graded exercises",
    "authors": [
      "Álvaro Barbero Jiménez"
    ],
    "abstract": "Large Language Models have shown prominent capabilities in generating\nfunctional code from natural language descriptions. However, a standardized way\nto evaluate these capabilities in an objective and unbiased manner is still to\nbe found. In this paper we review the current evaluation methods available to\nthis end, and run a new evaluation of the performance of one state-of-the-art\nmodel (GPT4-o-mini) in solving curated coding challenges in 8 programming\nlanguages, obtained from Codewars, a software development community. Our\nanalysis shows that the chance of success of the model has a positive\ncorrelation with the task difficulty, the popularity of the programming\nlanguage being used and the time elapsed since the publication of the\nchallenge. A further approximate explanatory analysis in terms of high-level\nfeatures hints that while 46.6% of the model performance could be attributed to\ntask difficulty, a 37.4% seems to be related to leakage of the challenge\nsolutions into the model training set, while the remaining 16% depends on the\nprogramming language. These results suggest that current evaluation\nmethodologies might be overestimating the actual skill of Large Language Models\nfor generating functional code.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16292v1",
    "published_date": "2024-10-06 09:54:54 UTC",
    "updated_date": "2024-10-06 09:54:54 UTC"
  },
  {
    "arxiv_id": "2410.04424v1",
    "title": "DAdEE: Unsupervised Domain Adaptation in Early Exit PLMs",
    "authors": [
      "Divya Jyoti Bajpai",
      "Manjesh Kumar Hanawal"
    ],
    "abstract": "Pre-trained Language Models (PLMs) exhibit good accuracy and generalization\nability across various tasks using self-supervision, but their large size\nresults in high inference latency. Early Exit (EE) strategies handle the issue\nby allowing the samples to exit from classifiers attached to the intermediary\nlayers, but they do not generalize well, as exit classifiers can be sensitive\nto domain changes. To address this, we propose Unsupervised Domain Adaptation\nin EE framework (DADEE) that employs multi-level adaptation using knowledge\ndistillation. DADEE utilizes GAN-based adversarial adaptation at each layer to\nachieve domain-invariant representations, reducing the domain gap between the\nsource and target domain across all layers. The attached exits not only speed\nup inference but also enhance domain adaptation by reducing catastrophic\nforgetting and mode collapse, making it more suitable for real-world scenarios.\nExperiments on tasks such as sentiment analysis, entailment classification, and\nnatural language inference demonstrate that DADEE consistently outperforms not\nonly early exit methods but also various domain adaptation methods under domain\nshift scenarios. The anonymized source code is available at\nhttps://github.com/Div290/DAdEE.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear in EMNLP (findings) 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.04424v1",
    "published_date": "2024-10-06 09:44:58 UTC",
    "updated_date": "2024-10-06 09:44:58 UTC"
  },
  {
    "arxiv_id": "2410.04421v2",
    "title": "Disentangling Regional Primitives for Image Generation",
    "authors": [
      "Zhengting Chen",
      "Lei Cheng",
      "Lianghui Ding",
      "Quanshi Zhang"
    ],
    "abstract": "This paper presents a method to explain the internal representation structure\nof a neural network for image generation. Specifically, our method disentangles\nprimitive feature components from the intermediate-layer feature of the neural\nnetwork, which ensures that each feature component is exclusively used to\ngenerate a specific set of image regions. In this way, the generation of the\nentire image can be considered as the superposition of different pre-encoded\nprimitive regional patterns, each being generated by a feature component. We\nfind that the feature component can be represented as an OR relationship\nbetween the demands for generating different image regions, which is encoded by\nthe neural network. Therefore, we extend the Harsanyi interaction to represent\nsuch an OR interaction to disentangle the feature component. Experiments show a\nclear correspondence between each feature component and the generation of\nspecific image regions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04421v2",
    "published_date": "2024-10-06 09:27:45 UTC",
    "updated_date": "2024-10-11 11:29:31 UTC"
  },
  {
    "arxiv_id": "2410.04415v3",
    "title": "Geometric Analysis of Reasoning Trajectories: A Phase Space Approach to Understanding Valid and Invalid Multi-Hop Reasoning in LLMs",
    "authors": [
      "Javier Marin"
    ],
    "abstract": "This paper proposes a novel approach to analyzing multi-hop reasoning in\nlanguage models through Hamiltonian mechanics. We map reasoning chains in\nembedding spaces to Hamiltonian systems, defining a function that balances\nreasoning progression (kinetic energy) against question relevance (potential\nenergy). Analyzing reasoning chains from a question-answering dataset reveals\nthat valid reasoning shows lower Hamiltonian energy values, representing an\noptimal trade-off between information gathering and targeted answering. While\nour framework offers complex visualization and quantification methods, the\nclaimed ability to \"steer\" or \"improve\" reasoning algorithms requires more\nrigorous empirical validation, as the connection between physical systems and\nreasoning remains largely metaphorical. Nevertheless, our analysis reveals\nconsistent geometric patterns distinguishing valid reasoning, suggesting this\nphysics-inspired approach offers promising diagnostic tools and new\nperspectives on reasoning processes in large language models.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04415v3",
    "published_date": "2024-10-06 09:09:14 UTC",
    "updated_date": "2025-03-08 13:54:10 UTC"
  },
  {
    "arxiv_id": "2410.09083v1",
    "title": "Alignment Between the Decision-Making Logic of LLMs and Human Cognition: A Case Study on Legal LLMs",
    "authors": [
      "Lu Chen",
      "Yuxuan Huang",
      "Yixing Li",
      "Yaohui Jin",
      "Shuai Zhao",
      "Zilong Zheng",
      "Quanshi Zhang"
    ],
    "abstract": "This paper presents a method to evaluate the alignment between the\ndecision-making logic of Large Language Models (LLMs) and human cognition in a\ncase study on legal LLMs. Unlike traditional evaluations on language generation\nresults, we propose to evaluate the correctness of the detailed decision-making\nlogic of an LLM behind its seemingly correct outputs, which represents the core\nchallenge for an LLM to earn human trust. To this end, we quantify the\ninteractions encoded by the LLM as primitive decision-making logic, because\nrecent theoretical achievements have proven several mathematical guarantees of\nthe faithfulness of the interaction-based explanation. We design a set of\nmetrics to evaluate the detailed decision-making logic of LLMs. Experiments\nshow that even when the language generation results appear correct, a\nsignificant portion of the internal inference logic contains notable issues.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09083v1",
    "published_date": "2024-10-06 08:33:39 UTC",
    "updated_date": "2024-10-06 08:33:39 UTC"
  },
  {
    "arxiv_id": "2410.04397v2",
    "title": "Towards Understanding and Enhancing Security of Proof-of-Training for DNN Model Ownership Verification",
    "authors": [
      "Yijia Chang",
      "Hanrui Jiang",
      "Chao Lin",
      "Xinyi Huang",
      "Jian Weng"
    ],
    "abstract": "The great economic values of deep neural networks (DNNs) urge AI enterprises\nto protect their intellectual property (IP) for these models. Recently,\nproof-of-training (PoT) has been proposed as a promising solution to DNN IP\nprotection, through which AI enterprises can utilize the record of DNN training\nprocess as their ownership proof. To prevent attackers from forging ownership\nproof, a secure PoT scheme should be able to distinguish honest training\nrecords from those forged by attackers. Although existing PoT schemes provide\nvarious distinction criteria, these criteria are based on intuitions or\nobservations. The effectiveness of these criteria lacks clear and comprehensive\nanalysis, resulting in existing schemes initially deemed secure being swiftly\ncompromised by simple ideas. In this paper, we make the first move to identify\ndistinction criteria in the style of formal methods, so that their\neffectiveness can be explicitly demonstrated. Specifically, we conduct\nsystematic modeling to cover a wide range of attacks and then theoretically\nanalyze the distinctions between honest and forged training records. The\nanalysis results not only induce a universal distinction criterion, but also\nprovide detailed reasoning to demonstrate its effectiveness in defending\nagainst attacks covered by our model. Guided by the criterion, we propose a\ngeneric PoT construction that can be instantiated into concrete schemes. This\nconstruction sheds light on the realization that trajectory matching\nalgorithms, previously employed in data distillation, possess significant\nadvantages in PoT construction. Experimental results demonstrate that our\nscheme can resist attacks that have compromised existing PoT schemes, which\ncorroborates its superiority in security.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by USENIX Security 2025 (Major Revision -> Accept)",
    "pdf_url": "http://arxiv.org/pdf/2410.04397v2",
    "published_date": "2024-10-06 08:30:31 UTC",
    "updated_date": "2024-10-10 07:43:44 UTC"
  },
  {
    "arxiv_id": "2410.10850v2",
    "title": "On the Reliability of Large Language Models to Misinformed and Demographically-Informed Prompts",
    "authors": [
      "Toluwani Aremu",
      "Oluwakemi Akinwehinmi",
      "Chukwuemeka Nwagu",
      "Syed Ishtiaque Ahmed",
      "Rita Orji",
      "Pedro Arnau Del Amo",
      "Abdulmotaleb El Saddik"
    ],
    "abstract": "We investigate and observe the behaviour and performance of Large Language\nModel (LLM)-backed chatbots in addressing misinformed prompts and questions\nwith demographic information within the domains of Climate Change and Mental\nHealth. Through a combination of quantitative and qualitative methods, we\nassess the chatbots' ability to discern the veracity of statements, their\nadherence to facts, and the presence of bias or misinformation in their\nresponses. Our quantitative analysis using True/False questions reveals that\nthese chatbots can be relied on to give the right answers to these close-ended\nquestions. However, the qualitative insights, gathered from domain experts,\nshows that there are still concerns regarding privacy, ethical implications,\nand the necessity for chatbots to direct users to professional services. We\nconclude that while these chatbots hold significant promise, their deployment\nin sensitive areas necessitates careful consideration, ethical oversight, and\nrigorous refinement to ensure they serve as a beneficial augmentation to human\nexpertise rather than an autonomous solution.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Study conducted between August and December 2023. Under review at\n  AAAI-AI Magazine. Submitted for archival purposes only",
    "pdf_url": "http://arxiv.org/pdf/2410.10850v2",
    "published_date": "2024-10-06 07:40:11 UTC",
    "updated_date": "2024-10-17 11:52:38 UTC"
  },
  {
    "arxiv_id": "2410.04368v1",
    "title": "Algorithmic Capabilities of Random Transformers",
    "authors": [
      "Ziqian Zhong",
      "Jacob Andreas"
    ],
    "abstract": "Trained transformer models have been found to implement interpretable\nprocedures for tasks like arithmetic and associative recall, but little is\nunderstood about how the circuits that implement these procedures originate\nduring training. To what extent do they depend on the supervisory signal\nprovided to models, and to what extent are they attributable to behavior\nalready present in models at the beginning of training? To investigate these\nquestions, we investigate what functions can be learned by randomly initialized\ntransformers in which only the embedding layers are optimized, so that the only\ninput--output mappings learnable from data are those already implemented (up to\na choice of encoding scheme) by the randomly initialized model. We find that\nthese random transformers can perform a wide range of meaningful algorithmic\ntasks, including modular arithmetic, in-weights and in-context associative\nrecall, decimal addition, parenthesis balancing, and even some aspects of\nnatural language text generation. Our results indicate that some algorithmic\ncapabilities are present in transformers (and accessible via appropriately\nstructured inputs) even before these models are trained. Code is available at\nhttps://github.com/fjzzq2002/random_transformers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.04368v1",
    "published_date": "2024-10-06 06:04:23 UTC",
    "updated_date": "2024-10-06 06:04:23 UTC"
  },
  {
    "arxiv_id": "2410.04366v1",
    "title": "RespDiff: An End-to-End Multi-scale RNN Diffusion Model for Respiratory Waveform Estimation from PPG Signals",
    "authors": [
      "Yuyang Miao",
      "Zehua Chen",
      "Chang Li",
      "Danilo Mandic"
    ],
    "abstract": "Respiratory rate (RR) is a critical health indicator often monitored under\ninconvenient scenarios, limiting its practicality for continuous monitoring.\nPhotoplethysmography (PPG) sensors, increasingly integrated into wearable\ndevices, offer a chance to continuously estimate RR in a portable manner. In\nthis paper, we propose RespDiff, an end-to-end multi-scale RNN diffusion model\nfor respiratory waveform estimation from PPG signals. RespDiff does not require\nhand-crafted features or the exclusion of low-quality signal segments, making\nit suitable for real-world scenarios. The model employs multi-scale encoders,\nto extract features at different resolutions, and a bidirectional RNN to\nprocess PPG signals and extract respiratory waveform. Additionally, a spectral\nloss term is introduced to optimize the model further. Experiments conducted on\nthe BIDMC dataset demonstrate that RespDiff outperforms notable previous works,\nachieving a mean absolute error (MAE) of 1.18 bpm for RR estimation while\nothers range from 1.66 to 2.15 bpm, showing its potential for robust and\naccurate respiratory monitoring in real-world applications.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04366v1",
    "published_date": "2024-10-06 05:54:49 UTC",
    "updated_date": "2024-10-06 05:54:49 UTC"
  },
  {
    "arxiv_id": "2410.04364v3",
    "title": "VideoGuide: Improving Video Diffusion Models without Training Through a Teacher's Guide",
    "authors": [
      "Dohun Lee",
      "Bryan S Kim",
      "Geon Yeong Park",
      "Jong Chul Ye"
    ],
    "abstract": "Text-to-image (T2I) diffusion models have revolutionized visual content\ncreation, but extending these capabilities to text-to-video (T2V) generation\nremains a challenge, particularly in preserving temporal consistency. Existing\nmethods that aim to improve consistency often cause trade-offs such as reduced\nimaging quality and impractical computational time. To address these issues we\nintroduce VideoGuide, a novel framework that enhances the temporal consistency\nof pretrained T2V models without the need for additional training or\nfine-tuning. Instead, VideoGuide leverages any pretrained video diffusion model\n(VDM) or itself as a guide during the early stages of inference, improving\ntemporal quality by interpolating the guiding model's denoised samples into the\nsampling model's denoising process. The proposed method brings about\nsignificant improvement in temporal consistency and image fidelity, providing a\ncost-effective and practical solution that synergizes the strengths of various\nvideo diffusion models. Furthermore, we demonstrate prior distillation,\nrevealing that base models can achieve enhanced text coherence by utilizing the\nsuperior data prior of the guiding model through the proposed method. Project\nPage: https://dohunlee1.github.io/videoguide.github.io/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "26 pages, 19 figures, Project Page:\n  https://dohunlee1.github.io/videoguide.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2410.04364v3",
    "published_date": "2024-10-06 05:46:17 UTC",
    "updated_date": "2024-12-08 18:01:25 UTC"
  },
  {
    "arxiv_id": "2410.04360v2",
    "title": "GenSim: A General Social Simulation Platform with Large Language Model based Agents",
    "authors": [
      "Jiakai Tang",
      "Heyang Gao",
      "Xuchen Pan",
      "Lei Wang",
      "Haoran Tan",
      "Dawei Gao",
      "Yushuo Chen",
      "Xu Chen",
      "Yankai Lin",
      "Yaliang Li",
      "Bolin Ding",
      "Jingren Zhou",
      "Jun Wang",
      "Ji-Rong Wen"
    ],
    "abstract": "With the rapid advancement of large language models (LLMs), recent years have\nwitnessed many promising studies on leveraging LLM-based agents to simulate\nhuman social behavior. While prior work has demonstrated significant potential\nacross various domains, much of it has focused on specific scenarios involving\na limited number of agents and has lacked the ability to adapt when errors\noccur during simulation. To overcome these limitations, we propose a novel\nLLM-agent-based simulation platform called \\textit{GenSim}, which: (1)\n\\textbf{Abstracts a set of general functions} to simplify the simulation of\ncustomized social scenarios; (2) \\textbf{Supports one hundred thousand agents}\nto better simulate large-scale populations in real-world contexts; (3)\n\\textbf{Incorporates error-correction mechanisms} to ensure more reliable and\nlong-term simulations. To evaluate our platform, we assess both the efficiency\nof large-scale agent simulations and the effectiveness of the error-correction\nmechanisms. To our knowledge, GenSim represents an initial step toward a\ngeneral, large-scale, and correctable social simulation platform based on LLM\nagents, promising to further advance the field of social science.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04360v2",
    "published_date": "2024-10-06 05:02:23 UTC",
    "updated_date": "2024-10-09 09:03:48 UTC"
  },
  {
    "arxiv_id": "2410.10849v1",
    "title": "Continuous Approximations for Improving Quantization Aware Training of LLMs",
    "authors": [
      "He Li",
      "Jianhang Hong",
      "Yuanzhuo Wu",
      "Snehal Adbol",
      "Zonglin Li"
    ],
    "abstract": "Model compression methods are used to reduce the computation and energy\nrequirements for Large Language Models (LLMs). Quantization Aware Training\n(QAT), an effective model compression method, is proposed to reduce performance\ndegradation after quantization. To further minimize this degradation, we\nintroduce two continuous approximations to the QAT process on the rounding\nfunction, traditionally approximated by the Straight-Through Estimator (STE),\nand the clamping function. By applying both methods, the perplexity (PPL) on\nthe WikiText-v2 dataset of the quantized model reaches 9.0815, outperforming\n9.9621 by the baseline. Also, we achieve a 2.76% improvement on BoolQ, and a\n5.47% improvement on MMLU, proving that the step sizes and weights can be\nlearned more accurately with our approach. Our method achieves better\nperformance with the same precision, model size, and training setup,\ncontributing to the development of more energy-efficient LLMs technology that\naligns with global sustainability goals.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.10849v1",
    "published_date": "2024-10-06 04:33:06 UTC",
    "updated_date": "2024-10-06 04:33:06 UTC"
  },
  {
    "arxiv_id": "2410.04345v1",
    "title": "MVP-Bench: Can Large Vision--Language Models Conduct Multi-level Visual Perception Like Humans?",
    "authors": [
      "Guanzhen Li",
      "Yuxi Xie",
      "Min-Yen Kan"
    ],
    "abstract": "Humans perform visual perception at multiple levels, including low-level\nobject recognition and high-level semantic interpretation such as behavior\nunderstanding. Subtle differences in low-level details can lead to substantial\nchanges in high-level perception. For example, substituting the shopping bag\nheld by a person with a gun suggests violent behavior, implying criminal or\nviolent activity. Despite significant advancements in various multimodal tasks,\nLarge Visual-Language Models (LVLMs) remain unexplored in their capabilities to\nconduct such multi-level visual perceptions.\n  To investigate the perception gap between LVLMs and humans, we introduce\nMVP-Bench, the first visual-language benchmark systematically evaluating both\nlow- and high-level visual perception of LVLMs. We construct MVP-Bench across\nnatural and synthetic images to investigate how manipulated content influences\nmodel perception. Using MVP-Bench, we diagnose the visual perception of 10\nopen-source and 2 closed-source LVLMs, showing that high-level perception tasks\nsignificantly challenge existing LVLMs. The state-of-the-art GPT-4o only\nachieves an accuracy of $56\\%$ on Yes/No questions, compared with $74\\%$ in\nlow-level scenarios. Furthermore, the performance gap between natural and\nmanipulated images indicates that current LVLMs do not generalize in\nunderstanding the visual semantics of synthetic images as humans do. Our data\nand code are publicly available at https://github.com/GuanzhenLi/MVP-Bench.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04345v1",
    "published_date": "2024-10-06 03:47:57 UTC",
    "updated_date": "2024-10-06 03:47:57 UTC"
  },
  {
    "arxiv_id": "2410.17258v1",
    "title": "Representing Web Applications As Knowledge Graphs",
    "authors": [
      "Yogesh Chandrasekharuni"
    ],
    "abstract": "Traditional methods for crawling and parsing web applications predominantly\nrely on extracting hyperlinks from initial pages and recursively following\nlinked resources. This approach constructs a graph where nodes represent\nunstructured data from web pages, and edges signify transitions between them.\nHowever, these techniques are limited in capturing the dynamic and interactive\nbehaviors inherent to modern web applications. In contrast, the proposed method\nmodels each node as a structured representation of the application's current\nstate, with edges reflecting user-initiated actions or transitions. This\nstructured representation enables a more comprehensive and functional\nunderstanding of web applications, offering valuable insights for downstream\ntasks such as automated testing and behavior analysis.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.17258v1",
    "published_date": "2024-10-06 02:50:41 UTC",
    "updated_date": "2024-10-06 02:50:41 UTC"
  },
  {
    "arxiv_id": "2410.04332v2",
    "title": "Gradient Routing: Masking Gradients to Localize Computation in Neural Networks",
    "authors": [
      "Alex Cloud",
      "Jacob Goldman-Wetzler",
      "Evžen Wybitul",
      "Joseph Miller",
      "Alexander Matt Turner"
    ],
    "abstract": "Neural networks are trained primarily based on their inputs and outputs,\nwithout regard for their internal mechanisms. These neglected mechanisms\ndetermine properties that are critical for safety, like (i) transparency; (ii)\nthe absence of sensitive information or harmful capabilities; and (iii)\nreliable generalization of goals beyond the training distribution. To address\nthis shortcoming, we introduce gradient routing, a training method that\nisolates capabilities to specific subregions of a neural network. Gradient\nrouting applies data-dependent, weighted masks to gradients during\nbackpropagation. These masks are supplied by the user in order to configure\nwhich parameters are updated by which data points. We show that gradient\nrouting can be used to (1) learn representations which are partitioned in an\ninterpretable way; (2) enable robust unlearning via ablation of a pre-specified\nnetwork subregion; and (3) achieve scalable oversight of a reinforcement\nlearner by localizing modules responsible for different behaviors. Throughout,\nwe find that gradient routing localizes capabilities even when applied to a\nlimited, ad-hoc subset of the data. We conclude that the approach holds promise\nfor challenging, real-world applications where quality data are scarce.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04332v2",
    "published_date": "2024-10-06 02:43:49 UTC",
    "updated_date": "2024-11-29 18:52:41 UTC"
  },
  {
    "arxiv_id": "2410.04328v1",
    "title": "OD-Stega: LLM-Based Near-Imperceptible Steganography via Optimized Distributions",
    "authors": [
      "Yu-Shin Huang",
      "Peter Just",
      "Krishna Narayanan",
      "Chao Tian"
    ],
    "abstract": "We consider coverless steganography where a Large Language Model (LLM) drives\nan arithmetic coding decoder to generate stego-texts. An efficient method\nshould embed secret message bits in as few language tokens as possible, while\nstill keeping the stego-text natural and fluent. We show that on the individual\ntoken level, this problem is mathematically equivalent to maximizing the\nentropy of a replacement probability distribution of the next token generation,\nsubject to a constraint on the KL divergence between the chosen probability\ndistribution and the original distribution given by the LLM. A closed-form\nsolution is provided for the optimization problem, which can be computed\nefficiently. Several important practical issues are also tackled: 1) An\noften-overlooked tokenization mismatch issue is resolved with a simple prompt\nselection approach, 2) The combination of the optimized distribution and the\nvocabulary truncation technique is considered, and 3) The combination of the\noptimized distribution with other sequence-level selection heuristics to\nfurther enhance the efficiency and reliability is studied.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "9 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.04328v1",
    "published_date": "2024-10-06 01:30:45 UTC",
    "updated_date": "2024-10-06 01:30:45 UTC"
  },
  {
    "arxiv_id": "2410.05331v2",
    "title": "Taylor Unswift: Secured Weight Release for Large Language Models via Taylor Expansion",
    "authors": [
      "Guanchu Wang",
      "Yu-Neng Chuang",
      "Ruixiang Tang",
      "Shaochen Zhong",
      "Jiayi Yuan",
      "Hongye Jin",
      "Zirui Liu",
      "Vipin Chaudhary",
      "Shuai Xu",
      "James Caverlee",
      "Xia Hu"
    ],
    "abstract": "Ensuring the security of released large language models (LLMs) poses a\nsignificant dilemma, as existing mechanisms either compromise ownership rights\nor raise data privacy concerns. To address this dilemma, we introduce TaylorMLP\nto protect the ownership of released LLMs and prevent their abuse.\nSpecifically, TaylorMLP preserves the ownership of LLMs by transforming the\nweights of LLMs into parameters of Taylor-series. Instead of releasing the\noriginal weights, developers can release the Taylor-series parameters with\nusers, thereby ensuring the security of LLMs. Moreover, TaylorMLP can prevent\nabuse of LLMs by adjusting the generation speed. It can induce low-speed token\ngeneration for the protected LLMs by increasing the terms in the Taylor-series.\nThis intentional delay helps LLM developers prevent potential large-scale\nunauthorized uses of their models. Empirical experiments across five datasets\nand three LLM architectures demonstrate that TaylorMLP induces over 4x increase\nin latency, producing the tokens precisely matched with original LLMs.\nSubsequent defensive experiments further confirm that TaylorMLP effectively\nprevents users from reconstructing the weight values based on downstream\ndatasets.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05331v2",
    "published_date": "2024-10-06 01:13:49 UTC",
    "updated_date": "2025-03-11 02:16:12 UTC"
  },
  {
    "arxiv_id": "2410.04324v4",
    "title": "Where are we in audio deepfake detection? A systematic analysis over generative and detection models",
    "authors": [
      "Xiang Li",
      "Pin-Yu Chen",
      "Wenqi Wei"
    ],
    "abstract": "Recent advances in Text-to-Speech (TTS) and Voice-Conversion (VC) using\ngenerative Artificial Intelligence (AI) technology have made it possible to\ngenerate high-quality and realistic human-like audio. This poses growing\nchallenges in distinguishing AI-synthesized speech from the genuine human voice\nand could raise concerns about misuse for impersonation, fraud, spreading\nmisinformation, and scams. However, existing detection methods for\nAI-synthesized audio have not kept pace and often fail to generalize across\ndiverse datasets. In this paper, we introduce SONAR, a synthetic AI-Audio\nDetection Framework and Benchmark, aiming to provide a comprehensive evaluation\nfor distinguishing cutting-edge AI-synthesized auditory content. SONAR includes\na novel evaluation dataset sourced from 9 diverse audio synthesis platforms,\nincluding leading TTS providers and state-of-the-art TTS models. It is the\nfirst framework to uniformly benchmark AI-audio detection across both\ntraditional and foundation model-based detection systems. Through extensive\nexperiments, (1) we reveal the limitations of existing detection methods and\ndemonstrate that foundation models exhibit stronger generalization\ncapabilities, likely due to their model size and the scale and quality of\npretraining data. (2) Speech foundation models demonstrate robust cross-lingual\ngeneralization capabilities, maintaining strong performance across diverse\nlanguages despite being fine-tuned solely on English speech data. This finding\nalso suggests that the primary challenges in audio deepfake detection are more\nclosely tied to the realism and quality of synthetic audio rather than\nlanguage-specific characteristics. (3) We explore the effectiveness and\nefficiency of few-shot fine-tuning in improving generalization, highlighting\nits potential for tailored applications, such as personalized detection systems\nfor specific entities or individuals.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04324v4",
    "published_date": "2024-10-06 01:03:42 UTC",
    "updated_date": "2025-03-22 01:10:56 UTC"
  },
  {
    "arxiv_id": "2410.04322v1",
    "title": "Toward Debugging Deep Reinforcement Learning Programs with RLExplorer",
    "authors": [
      "Rached Bouchoucha",
      "Ahmed Haj Yahmed",
      "Darshan Patil",
      "Janarthanan Rajendran",
      "Amin Nikanjam",
      "Sarath Chandar",
      "Foutse Khomh"
    ],
    "abstract": "Deep reinforcement learning (DRL) has shown success in diverse domains such\nas robotics, computer games, and recommendation systems. However, like any\nother software system, DRL-based software systems are susceptible to faults\nthat pose unique challenges for debugging and diagnosing. These faults often\nresult in unexpected behavior without explicit failures and error messages,\nmaking debugging difficult and time-consuming. Therefore, automating the\nmonitoring and diagnosis of DRL systems is crucial to alleviate the burden on\ndevelopers. In this paper, we propose RLExplorer, the first fault diagnosis\napproach for DRL-based software systems. RLExplorer automatically monitors\ntraining traces and runs diagnosis routines based on properties of the DRL\nlearning dynamics to detect the occurrence of DRL-specific faults. It then logs\nthe results of these diagnoses as warnings that cover theoretical concepts,\nrecommended practices, and potential solutions to the identified faults. We\nconducted two sets of evaluations to assess RLExplorer. Our first evaluation of\nfaulty DRL samples from Stack Overflow revealed that our approach can\neffectively diagnose real faults in 83% of the cases. Our second evaluation of\nRLExplorer with 15 DRL experts/developers showed that (1) RLExplorer could\nidentify 3.6 times more defects than manual debugging and (2) RLExplorer is\neasily integrated into DRL applications.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted for publication in The International Conference on Software\n  Maintenance and Evolution (ICSME 2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.04322v1",
    "published_date": "2024-10-06 01:01:21 UTC",
    "updated_date": "2024-10-06 01:01:21 UTC"
  },
  {
    "arxiv_id": "2410.04320v2",
    "title": "Channel-Aware Throughput Maximization for Cooperative Data Fusion in CAV",
    "authors": [
      "Haonan An",
      "Zhengru Fang",
      "Yuang Zhang",
      "Senkang Hu",
      "Xianhao Chen",
      "Guowen Xu",
      "Yuguang Fang"
    ],
    "abstract": "Connected and autonomous vehicles (CAVs) have garnered significant attention\ndue to their extended perception range and enhanced sensing coverage. To\naddress challenges such as blind spots and obstructions, CAVs employ\nvehicle-to-vehicle (V2V) communications to aggregate sensory data from\nsurrounding vehicles. However, cooperative perception is often constrained by\nthe limitations of achievable network throughput and channel quality. In this\npaper, we propose a channel-aware throughput maximization approach to\nfacilitate CAV data fusion, leveraging a self-supervised autoencoder for\nadaptive data compression. We formulate the problem as a mixed integer\nprogramming (MIP) model, which we decompose into two sub-problems to derive\noptimal data rate and compression ratio solutions under given link conditions.\nAn autoencoder is then trained to minimize bitrate with the determined\ncompression ratio, and a fine-tuning strategy is employed to further reduce\nspectrum resource consumption. Experimental evaluation on the OpenCOOD platform\ndemonstrates the effectiveness of our proposed algorithm, showing more than\n20.19\\% improvement in network throughput and a 9.38\\% increase in average\nprecision (AP@IoU) compared to state-of-the-art methods, with an optimal\nlatency of 19.99 ms.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.04320v2",
    "published_date": "2024-10-06 00:43:46 UTC",
    "updated_date": "2025-04-28 01:50:37 UTC"
  }
]
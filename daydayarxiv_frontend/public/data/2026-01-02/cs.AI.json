{
  "date": "2026-01-02",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2026-01-02 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“**ï¼š\nä»Šå¤©çš„ arXiv å……æ»¡äº†â€œç¥›é­…â€ä¸â€œåæ€â€çš„å‘³é“ã€‚æˆ‘ä»¬çœ‹åˆ°äº†å¯¹å¤§æ¨¡å‹æ‰€è°“â€œé¡¿æ‚Ÿï¼ˆAha! momentsï¼‰â€ç°è±¡çš„ä¸¥å‰è´¨ç–‘ï¼ˆç‰¹åˆ«æ˜¯é’ˆå¯¹ DeepSeek-R1-Zeroï¼‰ï¼Œä»¥åŠå¯¹ OpenAI Sora è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­éšæ€§åè§çš„é‡åŒ–ç ”ç©¶ã€‚æ­¤å¤–ï¼Œâ€œVibe Codingâ€ï¼ˆå‡­æ„Ÿè§‰ç¼–ç¨‹ï¼‰è¿™ä¸€æ–°å…´æ¦‚å¿µä¹Ÿè¿æ¥äº†æ•™è‚²å­¦ä¸Šçš„ä¸¥è‚ƒè¯„ä¼°ã€‚åœ¨æ¶æ„æ–¹é¢ï¼Œç»“åˆè„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰ä¸çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰çš„å°è¯•ä»¤äººçœ¼å‰ä¸€äº®ã€‚\n\n---\n\n### ğŸš€ æ·±åº¦æ¨ç†ä¸æ¨¡å‹æœºç†ï¼šçœŸçš„æœ‰â€œé¡¿æ‚Ÿâ€å—ï¼Ÿ\n\n**1. æ¨ç†æ¨¡å‹çš„é¡¿æ‚Ÿé”™è§‰**\n**The Illusion of Insight in Reasoning Models**\n> authors: Liv G. d'Aliberti, Manoel Horta Ribeiro\n\nè¿™æ˜¯ä¸€ç¯‡éå¸¸å€¼å¾—å…³æ³¨çš„æ–‡ç« ï¼Œå®ƒç›´æ¥æŒ‘æˆ˜äº†å½“å‰å…³äºæ¨ç†æ¨¡å‹ï¼ˆå¦‚ DeepSeek-R1-Zeroï¼‰å…·å¤‡â€œè‡ªæˆ‘ä¿®æ­£â€èƒ½åŠ›çš„å™äº‹ã€‚\n*   **æ ¸å¿ƒå‘ç°**ï¼šç ”ç©¶è€…åˆ†æäº†è¶…è¿‡ 100 ä¸‡æ¡æ¨ç†è½¨è¿¹ï¼Œå‘ç°æ‰€è°“çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ï¼ˆå³æ¨ç†è¿‡ç¨‹ä¸­çªç„¶è½¬å‘å¹¶å¾—å‡ºæ­£ç¡®ç»“æœï¼‰å…¶å®éå¸¸ç½•è§ï¼Œè€Œä¸”å¹¶ä¸éšç€è®­ç»ƒçš„è¿›è¡Œè€Œå¢åŠ ã€‚\n*   **ç»“è®º**ï¼šæ¨¡å‹ä¸­é€”çš„ç­–ç•¥è½¬å˜ï¼ˆReasoning shiftsï¼‰æ›´å¤šæ˜¯æ¨ç†ä¸ç¨³å®šçš„ç—‡çŠ¶ï¼Œè€Œéå†…åœ¨çš„è‡ªæˆ‘ä¿®æ­£æœºåˆ¶ã€‚ä½œè€…ç”šè‡³å‘ç°ï¼Œåœ¨æ¨¡å‹é«˜ç†µï¼ˆä¸ç¡®å®šæ€§é«˜ï¼‰æ—¶äººä¸ºè§¦å‘è½¬å˜åè€Œèƒ½æé«˜å‡†ç¡®ç‡ã€‚è¿™ç»™ç›®å‰ç«çƒ­çš„ Chain-of-Thought (CoT) æœºç†ç ”ç©¶æ³¼äº†ä¸€ç›†å†·æ°´ã€‚\n\n**2. ç†æ€§çš„å‡ ä½•ï¼šæœ‰æ•ˆæ•°å­¦æ¨ç†çš„è°±ç‰¹å¾**\n**Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning**\n> authors: Valentin NoÃ«l\n\nå¦‚æœæ¨¡å‹åœ¨â€œèƒ¡è¯´å…«é“â€ï¼Œæˆ‘ä»¬èƒ½ä»å®ƒçš„æ³¨æ„åŠ›çŸ©é˜µä¸­çœ‹å‡ºæ¥å—ï¼Ÿ\n*   **æ–¹æ³•**ï¼šä½œè€…æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡åˆ†æ Attention çŸ©é˜µçš„è°±ç‰¹å¾ï¼ˆå¦‚ Fiedler value å’Œè°±ç†µï¼‰æ¥æ£€æµ‹æ¨ç†æ˜¯å¦æœ‰æ•ˆã€‚\n*   **æ•ˆæœ**ï¼šåœ¨ Llama, Qwen, Mistral ç­‰æ¨¡å‹ä¸Šï¼Œä»…é€šè¿‡è°±æŒ‡æ ‡å°±èƒ½ä»¥æé«˜çš„å‡†ç¡®ç‡ï¼ˆ85-95%ï¼‰åŒºåˆ†æœ‰æ•ˆçš„æ•°å­¦è¯æ˜å’Œæ— æ•ˆçš„æ¨ç†ã€‚è¿™ä¸ºæ£€æµ‹å¹»è§‰å’Œ AI å®‰å…¨æä¾›äº†ä¸€ä¸ªåŸºäºå›¾è®ºçš„åšå®æ¡†æ¶ã€‚\n\n**3. æ‰“å¼€é»‘ç›’ï¼šå¤§è¯­è¨€æ¨¡å‹å¤šæ­¥æ¨ç†æœºåˆ¶ç»¼è¿°**\n**Opening the Black Box: A Survey on the Mechanisms of Multi-Step Reasoning in Large Language Models**\n> authors: Liangming Pan, et al.\n\n*   **ç»¼è¿°**ï¼šä¸åŒäºä»¥å¾€å…³æ³¨â€œå¦‚ä½•æå‡æ€§èƒ½â€çš„ç»¼è¿°ï¼Œè¿™ç¯‡æ–‡ç« ä¸“æ³¨äºâ€œåº•å±‚æœºåˆ¶â€ã€‚å®ƒæ¢è®¨äº† LLM å¦‚ä½•åœ¨éšè—æ¿€æ´»ä¸­æ‰§è¡Œéšå¼å¤šè·³æ¨ç†ï¼Œä»¥åŠæ˜¾å¼çš„æ€ç»´é“¾ï¼ˆCoTï¼‰å¦‚ä½•é‡å¡‘å†…éƒ¨è®¡ç®—ã€‚é€‚åˆæƒ³æ·±å…¥äº†è§£ Mechanistic Interpretability çš„è¯»è€…ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ã€åè§ä¸è¶Šç‹±\n\n**4. VEAT é‡åŒ–æ–‡ç”Ÿè§†é¢‘æ¨¡å‹ Sora ä¸­çš„éšæ€§å…³è”**\n**VEAT Quantifies Implicit Associations in Text-to-Video Generator Sora and Reveals Challenges in Bias Mitigation**\n> authors: Yongxu Sun, et al.\n\n*   **èƒŒæ™¯**ï¼šSora ç­‰è§†é¢‘ç”Ÿæˆæ¨¡å‹æ˜¯å¦ç»§æ‰¿äº†ç¤¾ä¼šçš„åˆ»æ¿å°è±¡ï¼Ÿ\n*   **å‘ç°**ï¼šä½œè€…æå‡ºäº†è§†é¢‘åµŒå…¥å…³è”æµ‹è¯•ï¼ˆVEATï¼‰ã€‚æµ‹è¯•æ˜¾ç¤ºï¼ŒSora ç”Ÿæˆçš„è§†é¢‘ä¸­ï¼Œç™½äººå’Œå¥³æ€§ä¸â€œæ„‰æ‚¦â€æ¦‚å¿µçš„å…³è”åº¦æ˜¾è‘—æ›´é«˜ã€‚æ›´è®½åˆºçš„æ˜¯ï¼Œä½¿ç”¨æ˜¾å¼çš„â€œå»åè§æç¤ºè¯â€æœ‰æ—¶åè€Œä¼šé€‚å¾—å…¶åï¼ˆä¾‹å¦‚è®©æ¸…æ´å·¥ç­‰èŒä¸šä¸é»‘äººçš„å…³è”æ›´å¼ºï¼‰ã€‚è¿™è¡¨æ˜ç®€å•çš„ Prompt å·¥ç¨‹æ— æ³•è§£å†³æ·±å±‚çš„è¡¨å¾å±å®³ã€‚\n\n**5. åŸºäº Emoji çš„å¤§è¯­è¨€æ¨¡å‹è¶Šç‹±**\n**Emoji-Based Jailbreaking of Large Language Models**\n> authors: M P V S Gopinadh, S Mahaboob Hussain\n\n*   **æ¼æ´**ï¼šä½ ä»¥ä¸ºè¿‡æ»¤äº†æ•æ„Ÿè¯å°±å®‰å…¨äº†ï¼Ÿè¿™é¡¹ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ Prompt ä¸­åµŒå…¥ Emoji åºåˆ—å¯ä»¥ç»•è¿‡å®‰å…¨å®¡æŸ¥ï¼Œè¯±å¯¼æ¨¡å‹è¾“å‡ºæœ‰å®³å†…å®¹ã€‚\n*   **ç»“æœ**ï¼šGemma 2 å’Œ Mistral 7B åœ¨è¿™ç§æ”»å‡»ä¸‹è¡¨ç°å‡ºçº¦ 10% çš„å¤±å®ˆç‡ï¼Œè€Œ Qwen 2 7B åˆ™è¡¨ç°å‡ºæƒŠäººçš„é²æ£’æ€§ï¼ˆ0% æˆåŠŸç‡ï¼‰ã€‚è¿™æ„å‘³ç€ç›®å‰çš„æ–‡æœ¬å®‰å…¨å¯¹é½ç®¡é“å¯¹éæ–‡æœ¬ç¬¦å·ï¼ˆEmojiï¼‰çš„å¤„ç†å­˜åœ¨ç›²åŒºã€‚\n\n**6. æ”¯æŒçš„ç¼“æ…¢æ¼‚ç§»ï¼šå¤šè½®å¿ƒç†å¥åº·å¯¹è¯ä¸­çš„è¾¹ç•Œå¤±æ•ˆ**\n**The Slow Drift of Support: Boundary Failures in Multi-Turn Mental Health LLM Dialogues**\n> authors: Youyou Cheng, et al.\n\n*   **é—®é¢˜**ï¼šå•è½®å¯¹è¯çœ‹èµ·æ¥å¾ˆå®‰å…¨çš„å¿ƒç†å¥åº· AIï¼Œåœ¨å¤šè½®å¯¹è¯ä¸­ä¼šé€æ¸â€œç ´é˜²â€ã€‚\n*   **å‘ç°**ï¼šéšç€å¯¹è¯æ·±å…¥ï¼Œä¸ºäº†è¡¨ç°å‡ºåŒç†å¿ƒï¼ŒLLM å®¹æ˜“è¶Šç•Œï¼Œåšå‡ºç»å¯¹æ€§çš„æ‰¿è¯ºæˆ–æ‰®æ¼”ä¸“ä¸šåŒ»ç”Ÿè§’è‰²ã€‚è‡ªé€‚åº”çš„å‹åŠ›æµ‹è¯•èƒ½è®©æ¨¡å‹åœ¨å¹³å‡ 4.6 è½®å¯¹è¯å†…å°±æ‰“ç ´å®‰å…¨è¾¹ç•Œã€‚\n\n---\n\n### ğŸ¤– Agent ä¸ AI è¾…åŠ©ç¼–ç¨‹\n\n**7. Vibe-Check åè®®ï¼šé‡åŒ– AI ç¼–ç¨‹ä¸­çš„è®¤çŸ¥å¸è½½**\n**The Vibe-Check Protocol: Quantifying Cognitive Offloading in AI Programming**\n> authors: Aizierjiang Aiersilan\n\n*   **è¶£é¢˜**ï¼šâ€œVibe Codingâ€ï¼ˆåªæè¿°æ„å›¾ï¼Œä»£ç å…¨äº¤ç»™ AIï¼Œä¸»æ‰“ä¸€ä¸ªæ„Ÿè§‰ï¼‰æœ€è¿‘å¾ˆç«ã€‚è¿™åˆ°åº•æ˜¯æ•™è‚²çš„è¿›æ­¥è¿˜æ˜¯å€’é€€ï¼Ÿ\n*   **æ–¹æ³•**ï¼šä½œè€…æå‡ºäº† Vibe-Check åè®®ï¼Œé€šè¿‡â€œå†·å¯åŠ¨é‡æ„â€ã€â€œå¹»è§‰é™·é˜±æ£€æµ‹â€ç­‰æŒ‡æ ‡æ¥è¯„ä¼°ã€‚\n*   **è§‚ç‚¹**ï¼šè¿™ç¯‡è®ºæ–‡è¯•å›¾ä¸ºæ•™è‚²è€…æä¾›ä¸€ä¸ªé‡åŒ–åŸºå‡†ï¼Œåˆ¤æ–­åœ¨ä»€ä¹ˆæƒ…å†µä¸‹ AI è¾…åŠ©æ˜¯ä¿ƒè¿›æŒæ¡ï¼Œä»€ä¹ˆæƒ…å†µä¸‹åªæ˜¯å•çº¯çš„â€œè®¤çŸ¥å·æ‡’â€å¯¼è‡´æŠ€èƒ½é€€åŒ–ã€‚\n\n**8. Trajectory Guardï¼šAgentic AI çš„å®æ—¶å¼‚å¸¸æ£€æµ‹**\n**Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI**\n> authors: Laksh Advani\n\n*   **ç—›ç‚¹**ï¼šAgent æ‰§è¡Œå¤šæ­¥ä»»åŠ¡æ—¶å®¹æ˜“â€œè·‘åâ€æˆ–ç»“æ„æ··ä¹±ã€‚\n*   **æ–¹æ¡ˆ**ï¼šæå‡ºäº†ä¸€ç§å­ªç”Ÿå¾ªç¯è‡ªç¼–ç å™¨ï¼Œæ—¢èƒ½çœ‹æ‡‚ä»»åŠ¡å¯¹é½ï¼Œåˆèƒ½çœ‹æ‡‚åºåˆ—ç»“æ„çš„æœ‰æ•ˆæ€§ã€‚æ¨ç†å»¶è¿Ÿä»… 32msï¼Œæ¯”ç”¨ LLM å½“è£åˆ¤å¿« 17-27 å€ï¼Œéå¸¸é€‚åˆç”Ÿäº§ç¯å¢ƒçš„å®æ—¶ç›‘æ§ã€‚\n\n---\n\n### ğŸ—ï¸ æ¶æ„åˆ›æ–°ä¸é«˜æ•ˆè®¡ç®—\n\n**9. SpikySpaceï¼šç”¨äºé«˜æ•ˆæ—¶é—´åºåˆ—é¢„æµ‹çš„è„‰å†²çŠ¶æ€ç©ºé—´æ¨¡å‹**\n**SpikySpace: A Spiking State Space Model for Energy-Efficient Time Series Forecasting**\n> authors: Kaiwen Tang, et al.\n\n*   **åˆ›æ–°**ï¼šç»“åˆäº†è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰çš„ä½èƒ½è€—å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSM/Mambaï¼‰çš„é•¿åºåˆ—å»ºæ¨¡èƒ½åŠ›ã€‚\n*   **æ ¸å¿ƒ**ï¼šé€šè¿‡â€œé€‰æ‹©æ€§æ‰«æâ€å°†æ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚åº¦é™ä¸ºçº¿æ€§ï¼Œå¹¶ç”¨ç¨€ç–çš„è„‰å†²äº‹ä»¶ä»£æ›¿å¯†é›†è®¡ç®—ã€‚\n*   **ç»“æœ**ï¼šç›¸æ¯” Transformer ç±»æ¨¡å‹ï¼Œèƒ½è€—é™ä½äº† 96% ä»¥ä¸Šï¼Œä¸”ç²¾åº¦ç›¸å½“ã€‚è¿™æ˜¯ç¥ç»å½¢æ€è®¡ç®—èµ°å‘å®ç”¨çš„é‡è¦ä¸€æ­¥ã€‚\n\n**10. å¿«é€Ÿæƒé‡ä¹˜ç§¯é”®å€¼è®°å¿†ç½‘ç»œ**\n**Fast-weight Product Key Memory**\n> authors: Tianyu Zhao, Llion Jones\n\n*   **æ–¹æ¡ˆ**ï¼šä¸ºäº†è§£å†³ Transformer ä¸Šä¸‹æ–‡é•¿åº¦å’Œæ˜¾å­˜çš„çŸ›ç›¾ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŠ¨æ€çš„â€œå¿«é€Ÿæƒé‡â€æƒ…æ™¯è®°å¿†æ¨¡å—ï¼ˆFwPKMï¼‰ã€‚\n*   **äº®ç‚¹**ï¼šå®ƒèƒ½åœ¨æ¨ç†æ—¶é€šè¿‡å±€éƒ¨æ¢¯åº¦ä¸‹é™åŠ¨æ€æ›´æ–°å‚æ•°ï¼Œå®ç°å¿«é€Ÿè®°å¿†ã€‚åœ¨â€œå¤§æµ·æé’ˆâ€æµ‹è¯•ä¸­ï¼Œä»…ç”¨ 4K é•¿åº¦è®­ç»ƒçš„æ¨¡å‹èƒ½æ³›åŒ–åˆ° 128K ä¸Šä¸‹æ–‡ã€‚\n\n---\n\n### ğŸ‘ï¸ è§†è§‰ä¸åŒ»ç–—ç²¾é€‰\n\n**11. SASNetï¼šæœ‰é™åŒ»ç–—æ ‡æ³¨ä¸‹çš„å°ºåº¦æ„ŸçŸ¥è‡ªé€‚åº”ç›‘ç£ç½‘ç»œ**\n**Scale-aware Adaptive Supervised Network with Limited Medical Annotations**\n> authors: Zihan Li, et al.\n\n*   **é¢†åŸŸ**ï¼šåŠç›‘ç£åŒ»ç–—å›¾åƒåˆ†å‰²ã€‚\n*   **è´¡çŒ®**ï¼šé’ˆå¯¹åŒ»ç–—æ ‡æ³¨ç¨€ç¼ºå’Œæ ‡æ³¨è€…å·®å¼‚å¤§çš„é—®é¢˜ï¼Œæå‡ºäº† SASNetã€‚åˆ©ç”¨å°ºåº¦æ„ŸçŸ¥é‡åŠ æƒç­–ç•¥å’Œ 3D å‚…é‡Œå¶å˜æ¢æ¥æ¨¡æ‹Ÿæ ‡æ³¨å˜åŒ–ï¼Œåœ¨ Pancreas-CT ç­‰æ•°æ®é›†ä¸Šé€¼è¿‘äº†å…¨ç›‘ç£çš„æ•ˆæœã€‚\n\n**12. è¯­ä¹‰äº‹ä»¶å›¾ï¼šé•¿è§†é¢‘é—®ç­”**\n**Semantic Event Graphs for Long-Form Video Question Answering**\n> authors: Aradhya Dixit, Tianxi Liang\n\n*   **æ–¹æ³•**ï¼šä¸ºäº†è®© LLM å¤„ç†é•¿è§†é¢‘æ—¶ä¸çˆ†æ˜¾å­˜ï¼Œä½œè€…æå‡ºç”¨â€œè¯­ä¹‰äº‹ä»¶å›¾â€ä»£æ›¿åŸå§‹å¸§ã€‚\n*   **æ•ˆæœ**ï¼šå°†è§†é¢‘è½¬åŒ–ä¸ºç´§å‡‘çš„æ—¶é—´äº¤äº’æ—¥å¿—ã€‚åœ¨å¤„ç†é•¿è§†é¢‘é—®ç­”æ—¶ï¼ŒToken ä½¿ç”¨é‡å‡å°‘äº† 91.4%ï¼Œä½†å‡†ç¡®ç‡ä¸å…¨é‡æ—¥å¿—åŸºçº¿æŒå¹³ã€‚\n\n---\n\n### ğŸ’¡ å…¶ä»–å€¼å¾—ä¸€çœ‹\n\n*   **[Science] AI å¼•å¯¼å‘ç°æ–°å‹ç¦»å­æ¶²ä½“æº¶å‰‚ç”¨äºå·¥ä¸š CO2 æ•è·** (#16 **AI-Guided Discovery of Novel Ionic Liquid Solvents...**)ï¼šAI for Science çš„å…¸å‹åº”ç”¨ï¼Œç­›é€‰å‡º 36 ç§å€™é€‰ææ–™ï¼Œæœ‰æœ›é™ä½ç¢³æ•è·æˆæœ¬ã€‚\n*   **[Education] è‡ªåŠ¨ç”Ÿæˆç›´è§‰å­¦ä¹ é—®é¢˜** (#29 **Automatic Question Generation...**)ï¼šç»“åˆå› æœå›¾å’Œ CoTï¼Œå‡å°‘æ•™è‚²ç±» AI ç”Ÿæˆé—®é¢˜çš„å¹»è§‰ã€‚\n*   **[Wildlife] WildIngï¼šé‡ç”ŸåŠ¨ç‰©å›¾åƒçš„åœ°ç†åŸŸæ¼‚ç§»ä¸å˜è¡¨ç¤º** (#4 **WildIng...**)ï¼šè§£å†³ AI åœ¨ä¸åŒåœ°ç†ä½ç½®ï¼ˆå¦‚éæ´² vs ç¾æ´²ï¼‰è¯†åˆ«åŠ¨ç‰©å‡†ç¡®ç‡éª¤é™çš„é—®é¢˜ã€‚\n\nä»Šå¤©çš„ arXiv ä¹‹æ—…å°±åˆ°è¿™é‡Œï¼Œå¸Œæœ›è¿™äº›ç¡¬æ ¸çš„â€œç¥›é­…â€ç ”ç©¶èƒ½å¼•å‘ä½ å¯¹å¤§æ¨¡å‹æœ¬è´¨çš„æ›´å¤šæ€è€ƒã€‚æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2601.01005v1",
      "title": "Scale-aware Adaptive Supervised Network with Limited Medical Annotations",
      "title_zh": "æ ‡æ³¨å—é™ä¸‹çš„å°ºåº¦æ„ŸçŸ¥è‡ªé€‚åº”ç›‘ç£ç½‘ç»œ",
      "authors": [
        "Zihan Li",
        "Dandan Shan",
        "Yunxiang Li",
        "Paul E. Kinahan",
        "Qingqi Hong"
      ],
      "abstract": "Medical image segmentation faces critical challenges in semi-supervised learning scenarios due to severe annotation scarcity requiring expert radiological knowledge, significant inter-annotator variability across different viewpoints and expertise levels, and inadequate multi-scale feature integration for precise boundary delineation in complex anatomical structures. Existing semi-supervised methods demonstrate substantial performance degradation compared to fully supervised approaches, particularly in small target segmentation and boundary refinement tasks. To address these fundamental challenges, we propose SASNet (Scale-aware Adaptive Supervised Network), a dual-branch architecture that leverages both low-level and high-level feature representations through novel scale-aware adaptive reweight mechanisms. Our approach introduces three key methodological innovations, including the Scale-aware Adaptive Reweight strategy that dynamically weights pixel-wise predictions using temporal confidence accumulation, the View Variance Enhancement mechanism employing 3D Fourier domain transformations to simulate annotation variability, and segmentation-regression consistency learning through signed distance map algorithms for enhanced boundary precision. These innovations collectively address the core limitations of existing semi-supervised approaches by integrating spatial, temporal, and geometric consistency principles within a unified optimization framework. Comprehensive evaluation across LA, Pancreas-CT, and BraTS datasets demonstrates that SASNet achieves superior performance with limited labeled data, surpassing state-of-the-art semi-supervised methods while approaching fully supervised performance levels. The source code for SASNet is available at https://github.com/HUANGLIZI/SASNet.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨åŠç›‘ç£åœºæ™¯ä¸‹é¢ä¸´çš„æ ‡æ³¨ç¨€ç¼ºã€æ ‡æ³¨å˜å¼‚ä»¥åŠå¤šå°ºåº¦ç‰¹å¾æ•´åˆä¸è¶³ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†SASNet (Scale-aware Adaptive Supervised Network)ã€‚è¯¥ç½‘ç»œé‡‡ç”¨åŒåˆ†æ”¯æ¶æ„ï¼Œé€šè¿‡å°ºåº¦æ„ŸçŸ¥è‡ªé€‚åº”é‡æƒæœºåˆ¶ (Scale-aware Adaptive Reweight) åˆ©ç”¨æ—¶é—´ç½®ä¿¡åº¦ç´¯ç§¯æ¥åŠ¨æ€åŠ æƒåƒç´ çº§é¢„æµ‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†è§†å›¾å˜å¼‚å¢å¼ºæœºåˆ¶ (View Variance Enhancement)ï¼Œåˆ©ç”¨3D Fourieré¢†åŸŸå˜æ¢æ¨¡æ‹Ÿæ ‡æ³¨å·®å¼‚ï¼Œå¹¶ç»“åˆæœ‰ç¬¦å·è·ç¦»å›¾ç®—æ³• (signed distance map) å®ç°åˆ†å‰²-å›å½’ä¸€è‡´æ€§å­¦ä¹ ä»¥æå‡è¾¹ç•Œç²¾åº¦ã€‚å®éªŒåœ¨LAã€Pancreas-CTå’ŒBraTSæ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºSASNetåœ¨ä»…æœ‰å°‘é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„SOTAåŠç›‘ç£æ–¹æ³•ï¼Œå…¶è¡¨ç°å·²æ¥è¿‘å…¨ç›‘ç£æ°´å¹³ã€‚è¯¥ç ”ç©¶æˆåŠŸå°†ç©ºé—´ã€æ—¶é—´å’Œå‡ ä½•ä¸€è‡´æ€§åŸåˆ™æ•´åˆè¿›ç»Ÿä¸€çš„ä¼˜åŒ–æ¡†æ¶ï¼Œä¸ºå—é™æ ‡æ³¨ç¯å¢ƒä¸‹çš„ç²¾ç¡®åŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted by Pattern Recognition, 8 figures, 11 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.01005v1",
      "published_date": "2026-01-02 23:55:17 UTC",
      "updated_date": "2026-01-02 23:55:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:16:33.802907+00:00"
    },
    {
      "arxiv_id": "2601.00996v1",
      "title": "VEAT Quantifies Implicit Associations in Text-to-Video Generator Sora and Reveals Challenges in Bias Mitigation",
      "title_zh": "VEATé‡åŒ–æ–‡ç”Ÿè§†é¢‘ç”Ÿæˆå™¨Soraä¸­çš„å†…éšå…³è”å¹¶æ­ç¤ºåè§ç¼“è§£é¢ä¸´çš„æŒ‘æˆ˜",
      "authors": [
        "Yongxu Sun",
        "Michael Saxon",
        "Ian Yang",
        "Anna-Maria Gueorguieva",
        "Aylin Caliskan"
      ],
      "abstract": "Text-to-Video (T2V) generators such as Sora raise concerns about whether generated content reflects societal bias. We extend embedding-association tests from words and images to video by introducing the Video Embedding Association Test (VEAT) and Single-Category VEAT (SC-VEAT). We validate these methods by reproducing the direction and magnitude of associations from widely used baselines, including Implicit Association Test (IAT) scenarios and OASIS image categories. We then quantify race (African American vs. European American) and gender (women vs. men) associations with valence (pleasant vs. unpleasant) across 17 occupations and 7 awards. Sora videos associate European Americans and women more with pleasantness (both d>0.8). Effect sizes correlate with real-world demographic distributions: percent men and White in occupations (r=0.93, r=0.83) and percent male and non-Black among award recipients (r=0.88, r=0.99). Applying explicit debiasing prompts generally reduces effect-size magnitudes, but can backfire: two Black-associated occupations (janitor, postal service) become more Black-associated after debiasing. Together, these results reveal that easily accessible T2V generators can actually amplify representational harms if not rigorously evaluated and responsibly deployed.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Sora ç­‰æ–‡æœ¬ç”Ÿæˆè§†é¢‘ (Text-to-Video) æ¨¡å‹ä¸­çš„ç¤¾ä¼šåè§é—®é¢˜ï¼Œæå‡ºäº†è§†é¢‘åµŒå…¥å…³è”æµ‹è¯• (Video Embedding Association Test, VEAT) åŠå…¶å•ç±»åˆ«ç‰ˆæœ¬ (SC-VEAT) ä»¥é‡åŒ–å…¶ä¸­çš„éšæ€§å…³è”ã€‚ç ”ç©¶äººå‘˜é€šè¿‡åœ¨ IAT åœºæ™¯å’Œ OASIS å›¾åƒç±»åˆ«ä¸­çš„åº”ç”¨éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯¦ç»†é‡åŒ–äº† 17 ç§èŒä¸šå’Œ 7 ç±»å¥–é¡¹ä¸­ç§æ—ä¸æ€§åˆ«çš„æ•ˆä»·å…³è”ã€‚å®éªŒå‘ç° Sora å€¾å‘äºå°†æ¬§è£”ç¾å›½äººå’Œå¥³æ€§ä¸æ„‰å¿« (pleasantness) æƒ…æ„Ÿç›¸å…³è”ï¼Œä¸”å…¶åè§æ•ˆåº”å€¼ä¸ç°å®ä¸–ç•Œçš„äººå£ç»Ÿè®¡åˆ†å¸ƒè¡¨ç°å‡ºæå¼ºçš„ç›¸å…³æ€§ã€‚ç ”ç©¶è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œè™½ç„¶åº”ç”¨æ˜¾æ€§å»åè§æç¤ºè¯ (debiasing prompts) é€šå¸¸èƒ½é™ä½åè§ç¨‹åº¦ï¼Œä½†åœ¨æŸäº›ç‰¹å®šèŒä¸šä¸­ä¹Ÿå¯èƒ½äº§ç”Ÿåæ•ˆæœã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº† T2V ç”Ÿæˆå™¨åœ¨æœªç»ä¸¥æ ¼è¯„ä¼°å’Œè´Ÿè´£ä»»éƒ¨ç½²çš„æƒ…å†µä¸‹ï¼Œå¯èƒ½ä¼šè¿›ä¸€æ­¥æ”¾å¤§ç¤¾ä¼šä»£è¡¨æ€§ä¼¤å®³ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "The International Association for Safe & Ethical AI (IASEAI)",
      "pdf_url": "https://arxiv.org/pdf/2601.00996v1",
      "published_date": "2026-01-02 22:38:19 UTC",
      "updated_date": "2026-01-02 22:38:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:16:48.177954+00:00"
    },
    {
      "arxiv_id": "2601.00994v1",
      "title": "ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems",
      "title_zh": "ElecTwitï¼šå¤šæ™ºèƒ½ä½“ç¤¾äº¤ç³»ç»Ÿä¸­çš„è¯´æœè¡Œä¸ºç ”ç©¶æ¡†æ¶",
      "authors": [
        "Michael Bao"
      ],
      "abstract": "This paper introduces ElecTwit, a simulation framework designed to study persuasion within multi-agent systems, specifically emulating the interactions on social media platforms during a political election. By grounding our experiments in a realistic environment, we aimed to overcome the limitations of game-based simulations often used in prior research. We observed the comprehensive use of 25 specific persuasion techniques across most tested LLMs, encompassing a wider range than previously reported. The variations in technique usage and overall persuasion output between models highlight how different model architectures and training can impact the dynamics in realistic social simulations. Additionally, we observed unique phenomena such as \"kernel of truth\" messages and spontaneous developments with an \"ink\" obsession, where agents collectively demanded written proof. Our study provides a foundation for evaluating persuasive LLM agents in real-world contexts, ensuring alignment and preventing dangerous outcomes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ElecTwitï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºç ”ç©¶å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (Multi-Agent Systems) ä¸­è¯´æœè¡Œä¸ºçš„æ¨¡æ‹Ÿæ¡†æ¶ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿæ”¿æ²»é€‰ä¸¾æœŸé—´ç¤¾äº¤åª’ä½“å¹³å°çš„çœŸå®äº’åŠ¨ã€‚é€šè¿‡æ„å»ºè´´è¿‘ç°å®çš„ç¯å¢ƒï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆå…‹æœäº†ä»¥å¾€ç ”ç©¶ä¸­å¸¸ç”¨çš„åŸºäºæ¸¸æˆçš„æ¨¡æ‹Ÿ (Game-based Simulations) çš„å±€é™æ€§ã€‚å®éªŒè§‚å¯Ÿå‘ç°ï¼Œå—æµ‹çš„å¤§è¯­è¨€æ¨¡å‹ (LLMs) èƒ½å¤Ÿå¹¿æ³›åº”ç”¨ 25 ç§ç‰¹å®šçš„è¯´æœæŠ€æœ¯ï¼Œå…¶è¦†ç›–èŒƒå›´æ˜¾è‘—è¶…è¿‡äº†æ­¤å‰çš„ç ”ç©¶æŠ¥å‘Šã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæ–¹å¼å¦‚ä½•å½±å“è¯´æœç­–ç•¥çš„è¿ç”¨åŠå…¶åœ¨ç¤¾äº¤æ¨¡æ‹Ÿä¸­çš„åŠ¨æ€æ¼”å˜ã€‚æ­¤å¤–ï¼Œæ¨¡æ‹Ÿè¿‡ç¨‹ä¸­è¿˜å‡ºç°äº†å¦‚â€œäº‹å®æ ¸å¿ƒâ€ (Kernel of Truth) æ¶ˆæ¯ä»¥åŠæ™ºèƒ½ä½“é›†ä½“ç´¢è¦ä¹¦é¢è¯æ˜çš„â€œå¢¨æ°´â€ (Ink) ç—´è¿·ç­‰ç‹¬ç‰¹ç¤¾äº¤ç°è±¡ã€‚è¯¥æˆæœä¸ºåœ¨ç°å®è¯­å¢ƒä¸‹è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„è¯´æœåŠ›æä¾›äº†åŸºç¡€ï¼Œå¯¹ç¡®ä¿æ¨¡å‹å¯¹é½ (Alignment) åŠé˜²èŒƒæ½œåœ¨å®‰å…¨é£é™©å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "In proceedings of 2025 IEEE International Conference on Agentic AI (ICA)",
      "pdf_url": "https://arxiv.org/pdf/2601.00994v1",
      "published_date": "2026-01-02 22:10:09 UTC",
      "updated_date": "2026-01-02 22:10:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:16:36.862581+00:00"
    },
    {
      "arxiv_id": "2601.00993v1",
      "title": "WildIng: A Wildlife Image Invariant Representation Model for Geographical Domain Shift",
      "title_zh": "WildIngï¼šé¢å‘åœ°ç†åŸŸåç§»çš„é‡ç”ŸåŠ¨ç‰©å›¾åƒä¸å˜æ€§è¡¨ç¤ºæ¨¡å‹",
      "authors": [
        "Julian D. Santamaria",
        "Claudia Isaza",
        "Jhony H. Giraldo"
      ],
      "abstract": "Wildlife monitoring is crucial for studying biodiversity loss and climate change. Camera trap images provide a non-intrusive method for analyzing animal populations and identifying ecological patterns over time. However, manual analysis is time-consuming and resource-intensive. Deep learning, particularly foundation models, has been applied to automate wildlife identification, achieving strong performance when tested on data from the same geographical locations as their training sets. Yet, despite their promise, these models struggle to generalize to new geographical areas, leading to significant performance drops. For example, training an advanced vision-language model, such as CLIP with an adapter, on an African dataset achieves an accuracy of 84.77%. However, this performance drops significantly to 16.17% when the model is tested on an American dataset. This limitation partly arises because existing models rely predominantly on image-based representations, making them sensitive to geographical data distribution shifts, such as variation in background, lighting, and environmental conditions. To address this, we introduce WildIng, a Wildlife image Invariant representation model for geographical domain shift. WildIng integrates text descriptions with image features, creating a more robust representation to geographical domain shifts. By leveraging textual descriptions, our approach captures consistent semantic information, such as detailed descriptions of the appearance of the species, improving generalization across different geographical locations. Experiments show that WildIng enhances the accuracy of foundation models such as BioCLIP by 30% under geographical domain shift conditions. We evaluate WildIng on two datasets collected from different regions, namely America and Africa. The code and models are publicly available at https://github.com/Julian075/CATALOG/tree/WildIng.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é‡ç”ŸåŠ¨ç‰©ç›‘æµ‹ä¸­æ·±åº¦å­¦ä¹ æ¨¡å‹éš¾ä»¥åº”å¯¹åœ°ç†é¢†åŸŸåç§»(geographical domain shift)çš„é—®é¢˜ï¼Œæå‡ºäº†WildIngä¸å˜è¡¨ç¤ºæ¨¡å‹ã€‚ä¼ ç»Ÿæ¨¡å‹ç”±äºè¿‡åº¦ä¾èµ–å›¾åƒè¡¨ç¤ºï¼Œåœ¨é¢å¯¹èƒŒæ™¯ã€å…‰ç…§å’Œç¯å¢ƒç­‰åœ°ç†æ•°æ®åˆ†å¸ƒå˜åŒ–æ—¶ï¼Œå¾€å¾€ä¼šå‡ºç°ä¸¥é‡çš„æ€§èƒ½ä¸‹é™ã€‚WildIngé€šè¿‡å°†æ–‡æœ¬æè¿°ä¸å›¾åƒç‰¹å¾æ·±åº¦æ•´åˆï¼Œåˆ©ç”¨æ–‡æœ¬æ•æ‰ç‰©ç§å¤–è§‚ç­‰ä¸€è‡´çš„è¯­ä¹‰ä¿¡æ¯(semantic information)ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹åœ°ç†ç¯å¢ƒå˜åŒ–çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è·¨åŒºåŸŸæµ‹è¯•ä¸­ï¼ŒWildIngèƒ½æ˜¾è‘—æå‡BioCLIPç­‰åŸºç¡€æ¨¡å‹çš„è¡¨ç°ï¼Œåœ¨åœ°ç†é¢†åŸŸåç§»æ¡ä»¶ä¸‹çš„å‡†ç¡®ç‡æé«˜äº†30%ã€‚è¯¥æ–¹æ³•åœ¨éæ´²å’Œç¾æ´²çš„ä¸¤ä¸ªæ•°æ®é›†ä¸Šå¾—åˆ°äº†éªŒè¯ï¼Œä¸ºè·¨åŒºåŸŸçš„è‡ªåŠ¨åŒ–é‡ç”ŸåŠ¨ç‰©è¯†åˆ«æä¾›äº†æ›´ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00993v1",
      "published_date": "2026-01-02 21:58:19 UTC",
      "updated_date": "2026-01-02 21:58:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:16:43.814056+00:00"
    },
    {
      "arxiv_id": "2601.03285v1",
      "title": "Feedback Indices to Evaluate LLM Responses to Rebuttals for Multiple Choice Type Questions",
      "title_zh": "è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹é’ˆå¯¹å¤šé¡¹é€‰æ‹©é¢˜åé©³å“åº”çš„åé¦ˆæŒ‡æ ‡",
      "authors": [
        "Justin C. Dunlap",
        "Anne-Simone Parent",
        "Ralf Widenhorn"
      ],
      "abstract": "We present a systematic framework of indices designed to characterize Large Language Model (LLM) responses when challenged with rebuttals during a chat. Assessing how LLMs respond to user dissent is crucial for understanding their reliability and behavior patterns, yet the complexity of human-LLM interactions makes systematic evaluation challenging. Our approach employs a fictitious-response rebuttal method that quantifies LLM behavior when presented with multiple-choice questions followed by deliberate challenges to their fictitious previous response. The indices are specifically designed to detect and measure what could be characterized as sycophantic behavior (excessive agreement with user challenges) or stubborn responses (rigid adherence to the fictitious response in the chat history) from LLMs. These metrics allow investigation of the relationships between sycophancy, stubbornness, and the model's actual mastery of the subject matter. We demonstrate the utility of these indices using two physics problems as test scenarios with various OpenAI models. The framework is intentionally generalizable to any multiple-choice format question, including on topics without universally accepted correct answers. Our results reveal measurable differences across OpenAI model generations, with trends indicating that newer models and those employing greater \"Reasoning Effort\" exhibit reduced sycophantic behavior. The FR pairing method combined with our proposed indices provides a practical, adaptable toolkit for systematically comparing LLM dialogue behaviors across different models and contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿæ€§çš„è¯„ä¼°æŒ‡æ ‡æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è™šæ„å“åº”åé©³æ³• (fictitious-response rebuttal method) æ¥é‡åŒ–å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é¢å¯¹å¤šé¡¹é€‰æ‹©é¢˜åé©³æ—¶çš„ååº”ç‰¹å¾ã€‚è¯¥æ¡†æ¶ä¸“é—¨è®¾è®¡ç”¨äºæ£€æµ‹å’Œè¡¡é‡æ¨¡å‹çš„é˜¿è°€å¥‰æ‰¿ (sycophancy) è¡Œä¸ºï¼ˆå³è¿‡åº¦é¡ºä»ç”¨æˆ·çš„è´¨ç–‘ï¼‰æˆ–å›ºæ‰§ (stubborn) è¡¨ç°ï¼ˆå³åƒµåŒ–åœ°åšæŒèŠå¤©è®°å½•ä¸­çš„è™šæ„å›ç­”ï¼‰ã€‚é€šè¿‡è¿™äº›æŒ‡æ ‡ï¼Œç ”ç©¶è€…å¯ä»¥æ·±å…¥æ¢è®¨æ¨¡å‹çš„é¡ºä»æ€§ã€å›ºæ‰§åº¦ä¸å…¶å¯¹ä¸»é¢˜å®é™…æŒæ¡ç¨‹åº¦ (mastery) ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚å®éªŒä»¥ç‰©ç†é—®é¢˜ä¸ºæµ‹è¯•åœºæ™¯å¯¹å¤šæ¬¾ OpenAI æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºä¸åŒä¸–ä»£çš„æ¨¡å‹åœ¨è¿™äº›æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—å·®å¼‚ã€‚ç ”ç©¶å‘ç°ï¼Œè¾ƒæ–°çš„æ¨¡å‹ä»¥åŠé‡‡ç”¨æ›´é«˜æ¨ç†å·¥ä½œé‡ (Reasoning Effort) çš„æ¨¡å‹åœ¨é¢å¯¹æŒ‘æˆ˜æ—¶è¡¨ç°å‡ºæ›´å°‘çš„é˜¿è°€å¥‰æ‰¿è¡Œä¸ºã€‚è¿™é¡¹å·¥ä½œä¸ºç³»ç»Ÿæ€§åœ°æ¯”è¾ƒä¸åŒæ¨¡å‹åœ¨å¯¹è¯æƒ…å¢ƒä¸‹çš„è¡Œä¸ºç‰¹å¾æä¾›äº†ä¸€å¥—å®ç”¨ä¸”å…·æœ‰é«˜åº¦æ™®é€‚æ€§çš„å·¥å…·åŒ…ã€‚",
      "categories": [
        "physics.ed-ph",
        "cs.AI"
      ],
      "primary_category": "physics.ed-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.03285v1",
      "published_date": "2026-01-02 21:16:43 UTC",
      "updated_date": "2026-01-02 21:16:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:16:50.595238+00:00"
    },
    {
      "arxiv_id": "2601.00969v1",
      "title": "Value Vision-Language-Action Planning & Search",
      "title_zh": "åŸºäºä»·å€¼çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œè§„åˆ’ä¸æœç´¢",
      "authors": [
        "Ali Salamatian",
        "Ke",
        "Ren",
        "Kieran Pattison",
        "Cyrus Neary"
      ],
      "abstract": "Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic manipulation, yet they remain fundamentally limited by their reliance on behavior cloning, leading to brittleness under distribution shift. While augmenting pretrained models with test-time search algorithms like Monte Carlo Tree Search (MCTS) can mitigate these failures, existing formulations rely solely on the VLA prior for guidance, lacking a grounded estimate of expected future return. Consequently, when the prior is inaccurate, the planner can only correct action selection via the exploration term, which requires extensive simulation to become effective. To address this limitation, we introduce Value Vision-Language-Action Planning and Search (V-VLAPS), a framework that augments MCTS with a lightweight, learnable value function. By training a simple multilayer perceptron (MLP) on the latent representations of a fixed VLA backbone (Octo), we provide the search with an explicit success signal that biases action selection toward high-value regions. We evaluate V-VLAPS on the LIBERO robotic manipulation suite, demonstrating that our value-guided search improves success rates by over 5 percentage points while reducing the average number of MCTS simulations by 5-15 percent compared to baselines that rely only on the VLA prior.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Vision-Language-Action (VLA)æ¨¡å‹ç”±äºè¿‡åº¦ä¾èµ–è¡Œä¸ºå…‹éš†(behavior cloning)è€Œåœ¨åˆ†å¸ƒåç§»(distribution shift)ä¸‹è¡¨ç°è„†å¼±çš„é—®é¢˜ï¼Œæå‡ºäº†V-VLAPSæ¡†æ¶ã€‚è™½ç„¶ä¼ ç»Ÿçš„è’™ç‰¹å¡æ´›æ ‘æœç´¢(MCTS)èƒ½å¤Ÿç¼“è§£æ­¤ç±»å¤±æ•ˆï¼Œä½†ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹æœªæ¥é¢„æœŸæ”¶ç›Šçš„å¯é ä¼°è®¡ï¼Œå¯¼è‡´æœç´¢æ•ˆç‡å’Œå‡†ç¡®æ€§å—é™ã€‚V-VLAPSé€šè¿‡åœ¨å›ºå®šçš„VLAä¸»å¹²ç½‘ç»œ(Octo)çš„æ½œåœ¨è¡¨ç¤ºä¸Šè®­ç»ƒè½»é‡çº§çš„å¤šå±‚æ„ŸçŸ¥æœº(MLP)ï¼Œä¸ºMCTSå¼•å…¥äº†ä¸€ä¸ªå¯å­¦ä¹ çš„ä»·å€¼å‡½æ•°(value function)ã€‚è¯¥ä»·å€¼å‡½æ•°é€šè¿‡æä¾›æ˜ç¡®çš„æˆåŠŸä¿¡å·ï¼Œå¼•å¯¼æœç´¢è¿‡ç¨‹ä¼˜å…ˆé€‰æ‹©é«˜ä»·å€¼åŒºåŸŸçš„åŠ¨ä½œï¼Œä»è€Œä¿®æ­£å…ˆéªŒä¿¡æ¯çš„åå·®ã€‚åœ¨LIBEROæœºå™¨äººæ“ä½œå¥—ä»¶ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒV-VLAPSç›¸æ¯”åŸºçº¿æ¨¡å‹å°†ä»»åŠ¡æˆåŠŸç‡æå‡äº†5ä¸ªç™¾åˆ†ç‚¹ä»¥ä¸Šï¼ŒåŒæ—¶å°†å¹³å‡MCTSæ¨¡æ‹Ÿæ¬¡æ•°å‡å°‘äº†5%è‡³15%ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "10 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.00969v1",
      "published_date": "2026-01-02 19:40:34 UTC",
      "updated_date": "2026-01-02 19:40:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:16:50.240174+00:00"
    },
    {
      "arxiv_id": "2601.00965v1",
      "title": "Adapting Feature Attenuation to NLP",
      "title_zh": "å°†ç‰¹å¾è¡°å‡åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†",
      "authors": [
        "Tianshuo Yang",
        "Ryan Rabinowitz",
        "Terrance E. Boult",
        "Jugal Kalita"
      ],
      "abstract": "Transformer classifiers such as BERT deliver impressive closed-set accuracy, yet they remain brittle when confronted with inputs from unseen categories--a common scenario for deployed NLP systems. We investigate Open-Set Recognition (OSR) for text by porting the feature attenuation hypothesis from computer vision to transformers and by benchmarking it against state-of-the-art baselines. Concretely, we adapt the COSTARR framework--originally designed for classification in computer vision--to two modest language models (BERT (base) and GPT-2) trained to label 176 arXiv subject areas. Alongside COSTARR, we evaluate Maximum Softmax Probability (MSP), MaxLogit, and the temperature-scaled free-energy score under the OOSA and AUOSCR metrics. Our results show (i) COSTARR extends to NLP without retraining but yields no statistically significant gain over MaxLogit or MSP, and (ii) free-energy lags behind all other scores in this high-class-count setting. The study highlights both the promise and the current limitations of transplanting vision-centric OSR ideas to language models, and points toward the need for larger backbones and task-tailored attenuation strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ–‡æœ¬é¢†åŸŸçš„å¼€æ”¾é›†è¯†åˆ«(Open-Set Recognition, OSR)é—®é¢˜ï¼Œæ—¨åœ¨è§£å†³ Transformer åˆ†ç±»å™¨åœ¨é¢å¯¹æœªè§ç±»åˆ«è¾“å…¥æ—¶çš„è„†å¼±æ€§ã€‚ä½œè€…å°†è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ç‰¹å¾è¡°å‡å‡è®¾(feature attenuation hypothesis)å¼•å…¥è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œå¹¶å°† COSTARR æ¡†æ¶é€‚é…åˆ° BERT å’Œ GPT-2 æ¨¡å‹ä¸­ã€‚ç ”ç©¶åœ¨åŒ…å« 176 ä¸ª arXiv å­¦ç§‘é¢†åŸŸçš„åˆ†ç±»ä»»åŠ¡ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶å¯¹æ¯”äº† Maximum Softmax Probability(MSP)ã€MaxLogit ä»¥åŠæ¸©åº¦ç¼©æ”¾çš„è‡ªç”±èƒ½å¾—åˆ†(free-energy score)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCOSTARR æ¡†æ¶æ— éœ€é‡æ–°è®­ç»ƒå³å¯æ‰©å±•è‡³ NLP é¢†åŸŸï¼Œä½†åœ¨æ€§èƒ½ä¸Šå¹¶æœªå¯¹æ¯” MaxLogit æˆ– MSP å–å¾—æ˜¾è‘—æå‡ã€‚åœ¨é«˜ç±»åˆ«æ•°é‡çš„è®¾ç½®ä¸‹ï¼Œè‡ªç”±èƒ½å¾—åˆ†çš„è¡¨ç°è½åäºå…¶ä»–æ‰€æœ‰è¯„åˆ†æ ‡å‡†ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å°†è§†è§‰é¢†åŸŸçš„ OSR æ–¹æ¡ˆç§»æ¤åˆ°è¯­è¨€æ¨¡å‹ä¸­çš„æ½œåŠ›ä¸å±€é™æ€§ï¼Œå¹¶æŒ‡å‡ºæœªæ¥éœ€è¦æ›´å¤§è§„æ¨¡çš„éª¨å¹²ç½‘ç»œä»¥åŠå®šåˆ¶åŒ–çš„ç‰¹å¾è¡°å‡ç­–ç•¥ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00965v1",
      "published_date": "2026-01-02 19:28:03 UTC",
      "updated_date": "2026-01-02 19:28:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:17:29.304407+00:00"
    },
    {
      "arxiv_id": "2601.00791v1",
      "title": "Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning",
      "title_zh": "ç†æ€§ä¹‹å‡ ä½•ï¼šæœ‰æ•ˆæ•°å­¦æ¨ç†çš„è°±ç‰¹å¾",
      "authors": [
        "Valentin NoÃ«l"
      ],
      "abstract": "We present a training-free method for detecting valid mathematical reasoning in large language models through spectral analysis of attention patterns. By treating attention matrices as adjacency matrices of dynamic graphs over tokens, we extract four interpretable spectral diagnostics, the Fiedler value (algebraic connectivity), high-frequency energy ratio (HFER), graph signal smoothness, and spectral entropy, that exhibit statistically significant differences between valid and invalid mathematical proofs. Experiments across seven transformer models from four independent architectural families (Meta Llama, Alibaba Qwen, Microsoft Phi, and Mistral AI) demonstrate that this spectral signature produces effect sizes up to Cohen's $d = 3.30$ ($p < 10^{-116}$), enabling 85.0--95.6\\% classification accuracy under rigorous evaluation, with calibrated thresholds reaching 93--95\\% on the full dataset. The method requires no training data, fine-tuning, or learned classifiers: a single threshold on a spectral metric suffices for high accuracy. Through systematic label correction, we discover that the spectral method detects logical coherence rather than compiler acceptance, identifying mathematically valid proofs that formal verifiers reject due to technical failures. We further identify an architectural dependency: Mistral-7B's Sliding Window Attention shifts the discriminative signal from HFER to late-layer Smoothness ($d = 2.09$, $p_{\\text{MW}} = 1.16 \\times 10^{-48}$), revealing that attention mechanism design affects which spectral features capture reasoning validity. These findings establish spectral graph analysis as a principled framework for reasoning verification with immediate applications to hallucination detection and AI safety monitoring.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡å¯¹å¤§è¯­è¨€æ¨¡å‹æ³¨æ„åŠ›æ¨¡å¼çš„è°±åˆ†æ(Spectral Analysis)æ¥æ£€æµ‹æœ‰æ•ˆçš„æ•°å­¦æ¨ç†ã€‚ç ”ç©¶è€…å°†æ³¨æ„åŠ›çŸ©é˜µè§†ä¸ºåŠ¨æ€å›¾çš„é‚»æ¥çŸ©é˜µï¼Œå¹¶ä»ä¸­æå–äº†åŒ…æ‹¬ Fiedler å€¼(Fiedler value)ã€é«˜é¢‘èƒ½é‡æ¯”(HFER)ã€å›¾ä¿¡å·å¹³æ»‘åº¦å’Œè°±ç†µåœ¨å†…çš„å››ä¸ªå¯è§£é‡Šçš„è°±è¯Šæ–­æŒ‡æ ‡ã€‚åœ¨æ¶µç›– Llamaã€Qwenã€Phi å’Œ Mistral å››ä¸ªæ¶æ„å®¶æ—çš„ä¸ƒä¸ªæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ— éœ€å¾®è°ƒæˆ–é¢å¤–å­¦ä¹ åˆ†ç±»å™¨çš„æƒ…å†µä¸‹ï¼Œåˆ†ç±»å‡†ç¡®ç‡å¯è¾¾ 85.0% è‡³ 95.6%ã€‚é€šè¿‡ç³»ç»Ÿæ€§æ ‡ç­¾ä¿®æ­£ï¼Œç ”ç©¶å‘ç°è¯¥è°±ç‰¹å¾æ£€æµ‹çš„æ˜¯é€»è¾‘è¿è´¯æ€§(Logical Coherence)è€Œéå•çº¯çš„ç¼–è¯‘å™¨æ¥å—åº¦ï¼Œç”šè‡³èƒ½è¯†åˆ«å‡ºå› æŠ€æœ¯æ•…éšœè¢«æ­£å¼éªŒè¯å™¨æ‹’ç»çš„æœ‰æ•ˆæ•°å­¦è¯æ˜ã€‚ç ”ç©¶è¿˜æ­ç¤ºäº†æ³¨æ„åŠ›æœºåˆ¶è®¾è®¡ï¼ˆå¦‚æ»‘åŠ¨çª—å£æ³¨æ„åŠ› Sliding Window Attentionï¼‰ä¼šæ”¹å˜æ•æ‰æ¨ç†æœ‰æ•ˆæ€§çš„å…·ä½“è°±ç‰¹å¾ã€‚è¿™äº›å‘ç°ä¸ºæ¨ç†éªŒè¯å»ºç«‹äº†åŸºäºè°±å›¾åˆ†æ(Spectral Graph Analysis)çš„åŸåˆ™æ€§æ¡†æ¶ï¼Œåœ¨å¹»è§‰æ£€æµ‹å’Œ AI å®‰å…¨ç›‘æ§é¢†åŸŸå…·æœ‰ç›´æ¥çš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.LO"
      ],
      "primary_category": "cs.LG",
      "comment": "58 pages, 19 figures, Under Review",
      "pdf_url": "https://arxiv.org/pdf/2601.00791v1",
      "published_date": "2026-01-02 18:49:37 UTC",
      "updated_date": "2026-01-02 18:49:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:16:57.069547+00:00"
    },
    {
      "arxiv_id": "2601.00785v1",
      "title": "FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing",
      "title_zh": "FedHypeVAEï¼šåŸºäºè¶…ç½‘ç»œç”Ÿæˆæ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨çš„è”é‚¦å­¦ä¹ ï¼Œç”¨äºå·®åˆ†éšç§åµŒå…¥å…±äº«",
      "authors": [
        "Sunny Gupta",
        "Amit Sethi"
      ],
      "abstract": "Federated data sharing promises utility without centralizing raw data, yet existing embedding-level generators struggle under non-IID client heterogeneity and provide limited formal protection against gradient leakage. We propose FedHypeVAE, a differentially private, hypernetwork-driven framework for synthesizing embedding-level data across decentralized clients. Building on a conditional VAE backbone, we replace the single global decoder and fixed latent prior with client-aware decoders and class-conditional priors generated by a shared hypernetwork from private, trainable client codes. This bi-level design personalizes the generative layerrather than the downstream modelwhile decoupling local data from communicated parameters. The shared hypernetwork is optimized under differential privacy, ensuring that only noise-perturbed, clipped gradients are aggregated across clients. A local MMD alignment between real and synthetic embeddings and a Lipschitz regularizer on hypernetwork outputs further enhance stability and distributional coherence under non-IID conditions. After training, a neutral meta-code enables domain agnostic synthesis, while mixtures of meta-codes provide controllable multi-domain coverage. FedHypeVAE unifies personalization, privacy, and distribution alignment at the generator level, establishing a principled foundation for privacy-preserving data synthesis in federated settings. Code: github.com/sunnyinAI/FedHypeVAE",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FedHypeVAEï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè¶…ç½‘ç»œ (Hypernetwork) é©±åŠ¨çš„å·®åˆ†éšç§ (Differential Privacy) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è”é‚¦å­¦ä¹ ä¸­åµŒå…¥çº§æ•°æ®ç”Ÿæˆé¢ä¸´çš„éç‹¬ç«‹åŒåˆ†å¸ƒ (non-IID) å¼‚æ„æ€§å’Œæ¢¯åº¦æ³„éœ²é£é™©ã€‚è¯¥æ¡†æ¶ä»¥æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ (Conditional VAE) ä¸ºåŸºç¡€ï¼Œåˆ©ç”¨å…±äº«è¶…ç½‘ç»œæ ¹æ®ç§æœ‰çš„å®¢æˆ·ç«¯ä»£ç ç”Ÿæˆä¸ªæ€§åŒ–è§£ç å™¨å’Œç±»åˆ«æ¡ä»¶å…ˆéªŒï¼Œå®ç°äº†ç”Ÿæˆå±‚è€Œéä¸‹æ¸¸æ¨¡å‹çš„ä¸ªæ€§åŒ–ã€‚è¶…ç½‘ç»œçš„ä¼˜åŒ–è¿‡ç¨‹ä¸¥æ ¼éµå¾ªå·®åˆ†éšç§ä¿æŠ¤ï¼Œç¡®ä¿ä»…èšåˆç»è¿‡å™ªå£°æ‰°åŠ¨å’Œè£å‰ªçš„æ¢¯åº¦ï¼Œä»è€Œåœ¨è§£è€¦æœ¬åœ°æ•°æ®çš„åŒæ—¶ä¿éšœäº†å‚æ•°é€šä¿¡çš„å®‰å…¨ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†æœ¬åœ°å¤šæ ¸æœ€å¤§å·®å¼‚ (MMD) å¯¹é½å’Œ Lipschitz æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œä»¥å¢å¼ºæ¨¡å‹åœ¨éç‹¬ç«‹åŒåˆ†å¸ƒæ¡ä»¶ä¸‹çš„åˆ†å¸ƒä¸€è‡´æ€§ä¸è®­ç»ƒç¨³å®šæ€§ã€‚è®­ç»ƒå®Œæˆåçš„ç³»ç»Ÿå¯é€šè¿‡ä¸­æ€§å…ƒä»£ç  (Meta-code) å®ç°é¢†åŸŸæ— å…³çš„åˆæˆï¼Œæˆ–é€šè¿‡ä»£ç æ··åˆå®ç°å¯æ§çš„å¤šé¢†åŸŸè¦†ç›–ã€‚FedHypeVAE åœ¨ç”Ÿæˆå™¨å±‚é¢ç»Ÿä¸€äº†ä¸ªæ€§åŒ–ã€éšç§ä¿æŠ¤å’Œåˆ†å¸ƒå¯¹é½ï¼Œä¸ºè”é‚¦ç¯å¢ƒä¸‹çš„éšç§ä¿æŠ¤æ•°æ®åˆæˆå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 1 figures, Accepted at AAI'26",
      "pdf_url": "https://arxiv.org/pdf/2601.00785v1",
      "published_date": "2026-01-02 18:40:41 UTC",
      "updated_date": "2026-01-02 18:40:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:16:59.517847+00:00"
    },
    {
      "arxiv_id": "2601.00770v1",
      "title": "LLM Agents for Combinatorial Efficient Frontiers: Investment Portfolio Optimization",
      "title_zh": "é¢å‘ç»„åˆæœ‰æ•ˆå‰æ²¿çš„ LLM æ™ºèƒ½ä½“ï¼šæŠ•èµ„ç»„åˆä¼˜åŒ–",
      "authors": [
        "Simon Paquette-Greenbaum",
        "Jiangbo Yu"
      ],
      "abstract": "Investment portfolio optimization is a task conducted in all major financial institutions. The Cardinality Constrained Mean-Variance Portfolio Optimization (CCPO) problem formulation is ubiquitous for portfolio optimization. The challenge of this type of portfolio optimization, a mixed-integer quadratic programming (MIQP) problem, arises from the intractability of solutions from exact solvers, where heuristic algorithms are used to find approximate portfolio solutions. CCPO entails many laborious and complex workflows and also requires extensive effort pertaining to heuristic algorithm development, where the combination of pooled heuristic solutions results in improved efficient frontiers. Hence, common approaches are to develop many heuristic algorithms. Agentic frameworks emerge as a promising candidate for many problems within combinatorial optimization, as they have been shown to be equally efficient with regard to automating large workflows and have been shown to be excellent in terms of algorithm development, sometimes surpassing human-level performance. This study implements a novel agentic framework for the CCPO and explores several concrete architectures. In benchmark problems, the implemented agentic framework matches state-of-the-art algorithms. Furthermore, complex workflows and algorithm development efforts are alleviated, while in the worst case, lower but acceptable error is reported.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†æŠ•èµ„ç»„åˆä¼˜åŒ–ä¸­çš„åŸºæ•°çº¦æŸå‡å€¼-æ–¹å·®æŠ•èµ„ç»„åˆä¼˜åŒ–ï¼ˆCCPOï¼‰é—®é¢˜ï¼Œé’ˆå¯¹å…¶ä½œä¸ºæ··åˆæ•´æ•°äºŒæ¬¡è§„åˆ’ï¼ˆMIQPï¼‰é—®é¢˜åœ¨æ±‚è§£ä¸Šçš„å¤æ‚æ€§åŠå¯¹å¯å‘å¼ç®—æ³•çš„é‡åº¦ä¾èµ–ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ï¼ˆLLM Agentsï¼‰çš„æ–°å‹æ™ºèƒ½ä½“æ¡†æ¶ï¼ˆagentic frameworkï¼‰ã€‚è¯¥æ¡†æ¶æ—¨åœ¨é€šè¿‡è‡ªåŠ¨åŒ–å¤§å‹å·¥ä½œæµå’Œå¯å‘å¼ç®—æ³•çš„å¼€å‘ï¼Œè§£å†³ä¼ ç»Ÿæ–¹æ³•ä¸­è€—æ—¶ä¸”å¤æ‚çš„ç®—æ³•è®¾è®¡æŒ‘æˆ˜ã€‚ç ”ç©¶æ¢ç´¢äº†å¤šç§å…·ä½“çš„æ™ºèƒ½ä½“æ¶æ„ï¼Œå¹¶å°†å…¶åº”ç”¨äºå¯»æ‰¾ç»„åˆä¼˜åŒ–çš„æœ‰æ•ˆå‰æ²¿ï¼ˆefficient frontiersï¼‰ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨åŸºå‡†æµ‹è¯•é—®é¢˜ä¸­ï¼Œè¯¥æ™ºèƒ½ä½“æ¡†æ¶çš„è¡¨ç°è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›ï¼ˆstate-of-the-artï¼‰ç®—æ³•çš„æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—é™ä½äº†äººå·¥å¼€å‘ç®—æ³•çš„è´Ÿæ‹…ï¼Œåœ¨ä¼˜åŒ–æ•ˆç‡ä¸ç»“æœç²¾åº¦ä¹‹é—´å–å¾—äº†è‰¯å¥½å¹³è¡¡ã€‚å³ä½¿åœ¨æœ€åæƒ…å†µä¸‹ä¹Ÿèƒ½æä¾›å¯æ¥å—çš„ä½è¯¯å·®è§£ï¼Œè¯æ˜äº† LLM æ™ºèƒ½ä½“åœ¨å¤æ‚é‡‘èä¼˜åŒ–ä»»åŠ¡ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CE",
        "cs.AI",
        "econ.GN"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00770v1",
      "published_date": "2026-01-02 18:02:13 UTC",
      "updated_date": "2026-01-02 18:02:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:17:28.456746+00:00"
    },
    {
      "arxiv_id": "2601.00743v1",
      "title": "An Agentic Framework for Neuro-Symbolic Programming",
      "title_zh": "ç¥ç»ç¬¦å·ç¼–ç¨‹çš„æ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Aliakbar Nafar",
        "Chetan Chigurupati",
        "Danial Kamali",
        "Hamid Karimian",
        "Parisa Kordjamshidi"
      ],
      "abstract": "Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AgenticDomiKnowS (ADS)ï¼Œä¸€ä¸ªæ—¨åœ¨ç®€åŒ–ç¥ç»ç¬¦å·ç¼–ç¨‹ (Neuro-Symbolic Programming) çš„æ™ºèƒ½ä½“æ¡†æ¶ã€‚å°½ç®¡ç°æœ‰çš„ DomiKnowS æ¡†æ¶æä¾›äº†å£°æ˜å¼ç¼–ç¨‹æ¥å£ï¼Œä½†ç”¨æˆ·ä»éœ€æŒæ¡å¤æ‚çš„ç‰¹å®šè¯­æ³•ï¼Œè¿™ä½¿å¾—åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­é›†æˆç¬¦å·çº¦æŸå˜å¾—è€—æ—¶ä¸”å›°éš¾ã€‚ADS é€šè¿‡æ™ºèƒ½ä½“å·¥ä½œæµ (agentic workflow) å°†è‡ªç”±æ ¼å¼çš„ä»»åŠ¡æè¿°è½¬åŒ–ä¸ºå®Œæ•´çš„ DomiKnowS ç¨‹åºï¼Œå¹¶èƒ½ç‹¬ç«‹åˆ›å»ºå’Œæµ‹è¯•æ¯ä¸ªç»„ä»¶ã€‚è¯¥å·¥ä½œæµè¿˜æ”¯æŒå¯é€‰çš„äººæœºäº¤äº’ (human-in-the-loop) å¹²é¢„ï¼Œå…è®¸ç†Ÿæ‚‰è¯¥ç³»ç»Ÿçš„ç”¨æˆ·å¯¹ä¸­é—´è¾“å‡ºè¿›è¡Œç²¾ç»†è°ƒæ•´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºå¯¹äºç»éªŒä¸°å¯Œçš„ç”¨æˆ·è¿˜æ˜¯åˆå­¦è€…ï¼ŒADS éƒ½èƒ½æ˜¾è‘—åŠ é€Ÿç¥ç»ç¬¦å·ç¨‹åºçš„æ„å»ºè¿‡ç¨‹ã€‚è¯¥æ¡†æ¶æˆåŠŸå°†å¼€å‘æ—¶é—´ä»æ•°å°æ—¶ç¼©çŸ­è‡³ 10-15 åˆ†é’Ÿï¼Œæå¤§æå‡äº†æ„å»ºé²æ£’ä¸”å¯è§£é‡Šçš„äººå·¥æ™ºèƒ½æ¨¡å‹çš„æ•ˆç‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00743v1",
      "published_date": "2026-01-02 16:59:39 UTC",
      "updated_date": "2026-01-02 16:59:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:17:41.516232+00:00"
    },
    {
      "arxiv_id": "2601.02412v1",
      "title": "Socially-Aware Recommender Systems Mitigate Opinion Clusterization",
      "title_zh": "ç¤¾ä¼šæ„ŸçŸ¥æ¨èç³»ç»Ÿç¼“è§£è§‚ç‚¹é›†ç¾¤åŒ–ç°è±¡",
      "authors": [
        "Lukas SchÃ¼epp",
        "Carmen Amo Alonso",
        "Florian DÃ¶rfler",
        "Giulia De Pasquale"
      ],
      "abstract": "Recommender systems shape online interactions by matching users with creators content to maximize engagement. Creators, in turn, adapt their content to align with users preferences and enhance their popularity. At the same time, users preferences evolve under the influence of both suggested content from the recommender system and content shared within their social circles. This feedback loop generates a complex interplay between users, creators, and recommender algorithms, which is the key cause of filter bubbles and opinion polarization. We develop a social network-aware recommender system that explicitly accounts for this user-creators feedback interaction and strategically exploits the topology of the user's own social network to promote diversification. Our approach highlights how accounting for and exploiting user's social network in the recommender system design is crucial to mediate filter bubble effects while balancing content diversity with personalization. Provably, opinion clusterization is positively correlated with the influence of recommended content on user opinions. Ultimately, the proposed approach shows the power of socially-aware recommender systems in combating opinion polarization and clusterization phenomena.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ¨èç³»ç»Ÿã€åˆ›ä½œè€…ä¸ç”¨æˆ·ä¹‹é—´å½¢æˆçš„åé¦ˆå›è·¯å¦‚ä½•å¯¼è‡´è¿‡æ»¤æ°”æ³¡ (filter bubbles) å’Œè§‚ç‚¹æåŒ– (opinion polarization) é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼€å‘äº†ä¸€ç§ç¤¾äº¤ç½‘ç»œæ„ŸçŸ¥æ¨èç³»ç»Ÿ (social network-aware recommender system)ï¼Œè¯¥ç³»ç»Ÿé€šè¿‡è€ƒè™‘ç”¨æˆ·ä¸åˆ›ä½œè€…çš„äº’åŠ¨å¹¶æˆ˜ç•¥æ€§åœ°åˆ©ç”¨ç”¨æˆ·ç¤¾äº¤ç½‘ç»œçš„æ‹“æ‰‘ç»“æ„ (topology) æ¥ä¿ƒè¿›å†…å®¹å¤šæ ·åŒ–ã€‚è¯¥æ–¹æ³•å¼ºè°ƒåœ¨æ¨èè®¾è®¡ä¸­ç»“åˆç¤¾äº¤ç½‘ç»œä¿¡æ¯å¯¹äºå¹³è¡¡ä¸ªæ€§åŒ–ä¸å†…å®¹å¤šæ ·æ€§ã€ç¼“è§£è¿‡æ»¤æ°”æ³¡æ•ˆåº”å…·æœ‰å…³é”®ä½œç”¨ã€‚ç†è®ºè¯æ˜ï¼Œè§‚ç‚¹é›†ç¾¤åŒ– (opinion clusterization) ä¸æ¨èå†…å®¹å¯¹ç”¨æˆ·è§‚ç‚¹çš„å½±å“å‘ˆæ­£ç›¸å…³ã€‚æœ€ç»ˆï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†ç¤¾äº¤ç½‘ç»œæ„ŸçŸ¥æ¨èç³»ç»Ÿåœ¨æœ‰æ•ˆå¯¹æŠ—ç½‘ç»œç©ºé—´è§‚ç‚¹æåŒ–å’Œé›†ç¾¤åŒ–ç°è±¡æ–¹é¢çš„å¼ºå¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.02412v1",
      "published_date": "2026-01-02 16:54:05 UTC",
      "updated_date": "2026-01-02 16:54:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:17:43.279127+00:00"
    },
    {
      "arxiv_id": "2601.00737v1",
      "title": "Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty",
      "title_zh": "éšæœº Actor-Criticï¼šé€šè¿‡æ—¶åºå¶ç„¶ä¸ç¡®å®šæ€§ç¼“è§£è¿‡ä¼°è®¡",
      "authors": [
        "UÄŸurcan Ã–zalp"
      ],
      "abstract": "Off-policy actor-critic methods in reinforcement learning train a critic with temporal-difference updates and use it as a learning signal for the policy (actor). This design typically achieves higher sample efficiency than purely on-policy methods. However, critic networks tend to overestimate value estimates systematically. This is often addressed by introducing a pessimistic bias based on uncertainty estimates. Current methods employ ensembling to quantify the critic's epistemic uncertainty-uncertainty due to limited data and model ambiguity-to scale pessimistic updates. In this work, we propose a new algorithm called Stochastic Actor-Critic (STAC) that incorporates temporal (one-step) aleatoric uncertainty-uncertainty arising from stochastic transitions, rewards, and policy-induced variability in Bellman targets-to scale pessimistic bias in temporal-difference updates, rather than relying on epistemic uncertainty. STAC uses a single distributional critic network to model the temporal return uncertainty, and applies dropout to both the critic and actor networks for regularization. Our results show that pessimism based on a distributional critic alone suffices to mitigate overestimation, and naturally leads to risk-averse behavior in stochastic environments. Introducing dropout further improves training stability and performance by means of regularization. With this design, STAC achieves improved computational efficiency using a single distributional critic network.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Stochastic Actor-Critic (STAC)ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­ç¦»ç­–(off-policy)æ¼”å‘˜-è¯„è®ºå®¶(actor-critic)æ–¹æ³•æ™®éå­˜åœ¨çš„ä»·å€¼é«˜ä¼°é—®é¢˜ã€‚ä¸ç°æœ‰çš„åˆ©ç”¨é›†æˆå­¦ä¹ (ensembling)è¡¡é‡è®¤çŸ¥ä¸ç¡®å®šæ€§(epistemic uncertainty)çš„æ–¹æ³•ä¸åŒï¼ŒSTACåˆ›æ–°æ€§åœ°å¼•å…¥äº†æ—¶é—´éšæœºä¸ç¡®å®šæ€§(temporal aleatoric uncertainty)æ¥è°ƒæ•´æ—¶é—´å·®åˆ†æ›´æ–°ä¸­çš„æ‚²è§‚åå·®(pessimistic bias)ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å•ä¸ªåˆ†å¸ƒè¯„è®ºå®¶ç½‘ç»œ(distributional critic network)æ¥å»ºæ¨¡ç”±éšæœºè½¬ç§»ã€å¥–åŠ±åŠç­–ç•¥å˜åŠ¨å¼•èµ·çš„ç›®æ ‡å€¼æ³¢åŠ¨ã€‚åŒæ—¶ï¼Œç®—æ³•åœ¨æ¼”å‘˜å’Œè¯„è®ºå®¶ç½‘ç»œä¸­å‡åº”ç”¨äº†dropoutæŠ€æœ¯è¿›è¡Œæ­£åˆ™åŒ–ï¼Œä»¥æå‡è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§åŸºäºåˆ†å¸ƒè¯„è®ºå®¶çš„æ‚²è§‚æœºåˆ¶è¶³ä»¥æœ‰æ•ˆç¼“è§£ä»·å€¼é«˜ä¼°ï¼Œå¹¶ä½¿æ™ºèƒ½ä½“åœ¨éšæœºç¯å¢ƒä¸­è¡¨ç°å‡ºè‡ªç„¶çš„é£é™©è§„é¿è¡Œä¸ºã€‚å‡­å€Ÿå•ç½‘ç»œè®¾è®¡ï¼ŒSTACåœ¨æå‡è®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—ä¼˜åŒ–äº†å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡ä¸ä»»åŠ¡è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.00737v1",
      "published_date": "2026-01-02 16:33:17 UTC",
      "updated_date": "2026-01-02 16:33:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:17:48.644057+00:00"
    },
    {
      "arxiv_id": "2601.00736v1",
      "title": "Exploring the Performance of Large Language Models on Subjective Span Identification Tasks",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸»è§‚æ€§æ–‡æœ¬ç‰‡æ®µè¯†åˆ«ä»»åŠ¡ä¸­çš„æ€§èƒ½æ¢ç´¢",
      "authors": [
        "Alphaeus Dmonte",
        "Roland Oruche",
        "Tharindu Ranasinghe",
        "Marcos Zampieri",
        "Prasad Calyam"
      ],
      "abstract": "Identifying relevant text spans is important for several downstream tasks in NLP, as it contributes to model explainability. While most span identification approaches rely on relatively smaller pre-trained language models like BERT, a few recent approaches have leveraged the latest generation of Large Language Models (LLMs) for the task. Current work has focused on explicit span identification like Named Entity Recognition (NER), while more subjective span identification with LLMs in tasks like Aspect-based Sentiment Analysis (ABSA) has been underexplored. In this paper, we fill this important gap by presenting an evaluation of the performance of various LLMs on text span identification in three popular tasks, namely sentiment analysis, offensive language identification, and claim verification. We explore several LLM strategies like instruction tuning, in-context learning, and chain of thought. Our results indicate underlying relationships within text aid LLMs in identifying precise text spans.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸»è§‚ç‰‡æ®µè¯†åˆ«ï¼ˆSubjective Span Identificationï¼‰ä»»åŠ¡ä¸­çš„æ€§èƒ½è¡¨ç°ï¼Œæ—¨åœ¨æå‡è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚é’ˆå¯¹ç›®å‰ LLMs åœ¨ä¸»è§‚ä»»åŠ¡å¦‚æ–¹é¢çº§æƒ…æ„Ÿåˆ†æï¼ˆABSAï¼‰ä¸­æ¢ç´¢ä¸è¶³çš„ç°çŠ¶ï¼Œæœ¬æ–‡è¯„ä¼°äº†å¤šç§æ¨¡å‹åœ¨æƒ…æ„Ÿåˆ†æã€æ”»å‡»æ€§è¯­è¨€è¯†åˆ«å’Œå£°æ˜éªŒè¯ï¼ˆClaim Verificationï¼‰ä¸‰é¡¹å…¸å‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶è¿‡ç¨‹æ¢ç´¢äº†æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰ã€ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆIn-context Learningï¼‰åŠé“¾å¼æ€ç»´ï¼ˆChain of Thoughtï¼‰ç­‰ä¸»æµç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ–‡æœ¬å†…éƒ¨çš„æ½œåœ¨å…³ç³»èƒ½æœ‰æ•ˆè¾…åŠ© LLMs è¯†åˆ«å‡ºç²¾ç¡®çš„æ–‡æœ¬ç‰‡æ®µã€‚è¯¥å·¥ä½œå¡«è¡¥äº† LLMs åœ¨å¤„ç†ä¸»è§‚æ–‡æœ¬ç‰‡æ®µè¯†åˆ«é¢†åŸŸçš„ç©ºç™½ï¼Œä¸ºç†è§£å¤§è§„æ¨¡æ¨¡å‹åœ¨å¤æ‚ä¸»è§‚ä»»åŠ¡ä¸­çš„è§£é‡Šèƒ½åŠ›æä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00736v1",
      "published_date": "2026-01-02 16:30:14 UTC",
      "updated_date": "2026-01-02 16:30:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:18:31.594277+00:00"
    },
    {
      "arxiv_id": "2601.00941v1",
      "title": "Comparative Analysis of Formula and Structure Prediction from Tandem Mass Spectra",
      "title_zh": "åŸºäºä¸²è”è´¨è°±çš„åˆ†å­å¼ä¸ç»“æ„é¢„æµ‹å¯¹æ¯”åˆ†æ",
      "authors": [
        "Xujun Che",
        "Xiuxia Du",
        "Depeng Xu"
      ],
      "abstract": "Liquid chromatography mass spectrometry (LC-MS)-based metabolomics and exposomics aim to measure detectable small molecules in biological samples. The results facilitate hypothesis-generating discovery of metabolic changes and disease mechanisms and provide information about environmental exposures and their effects on human health. Metabolomics and exposomics are made possible by the high resolving power of LC and high mass measurement accuracy of MS. However, a majority of the signals from such studies still cannot be identified or annotated using conventional library searching because existing spectral libraries are far from covering the vast chemical space captured by LC-MS/MS. To address this challenge and unleash the full potential of metabolomics and exposomics, a number of computational approaches have been developed to predict compounds based on tandem mass spectra. Published assessment of these approaches used different datasets and evaluation. To select prediction workflows for practical applications and identify areas for further improvements, we have carried out a systematic evaluation of the state-of-the-art prediction algorithms. Specifically, the accuracy of formula prediction and structure prediction was evaluated for different types of adducts. The resulting findings have established realistic performance baselines, identified critical bottlenecks, and provided guidance to further improve compound predictions based on MS.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»£è°¢ç»„å­¦(metabolomics)å’Œæš´éœ²ç»„å­¦(exposomics)ä¸­æ¶²ç›¸è‰²è°±-è´¨è°±(LC-MS)æ•°æ®è¯†åˆ«ç‡ä½çš„é—®é¢˜ï¼Œå¯¹ä»ä¸²è”è´¨è°±(tandem mass spectra)é¢„æµ‹åˆ†å­å¼(formula)å’Œç»“æ„(structure)çš„ç°æœ‰è®¡ç®—æ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿæ€§æ¯”è¾ƒåˆ†æã€‚é‰´äºç°æœ‰è°±åº“éš¾ä»¥è¦†ç›–LC-MS/MSæ•æ‰åˆ°çš„å¹¿é˜”åŒ–å­¦ç©ºé—´ï¼Œç ”ç©¶è€…è¯„ä¼°äº†å¤šç§æœ€å…ˆè¿›(state-of-the-art)é¢„æµ‹ç®—æ³•åœ¨ä¸åŒåŠ åˆç¦»å­(adducts)ç±»å‹ä¸‹çš„å‡†ç¡®æ€§è¡¨ç°ã€‚è¯¥å·¥ä½œé€šè¿‡ç³»ç»Ÿè¯„ä¼°å»ºç«‹äº†ç°å®çš„æ€§èƒ½åŸºå‡†(baselines)ï¼Œå¹¶è¯†åˆ«äº†é™åˆ¶é¢„æµ‹æ€§èƒ½çš„å…³é”®ç“¶é¢ˆã€‚ç ”ç©¶ç»“æœä¸ä»…ä¸ºå®é™…åº”ç”¨ä¸­é€‰æ‹©é¢„æµ‹å·¥ä½œæµæä¾›äº†ç›´æ¥æŒ‡å¯¼ï¼Œä¹Ÿä¸ºæœªæ¥è¿›ä¸€æ­¥æå‡åŸºäºè´¨è°±(MS)çš„åŒ–åˆç‰©é¢„æµ‹æŠ€æœ¯å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00941v1",
      "published_date": "2026-01-02 16:20:13 UTC",
      "updated_date": "2026-01-02 16:20:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:18:12.811884+00:00"
    },
    {
      "arxiv_id": "2601.03284v1",
      "title": "AI-Guided Discovery of Novel Ionic Liquid Solvents for Industrial CO2 Capture",
      "title_zh": "äººå·¥æ™ºèƒ½å¼•å¯¼çš„å·¥ä¸šäºŒæ°§åŒ–ç¢³æ•é›†æ–°å‹ç¦»å­æ¶²ä½“æº¶å‰‚å‘ç°",
      "authors": [
        "Davide Garbelotto",
        "Alexander Lobo",
        "Urvi Awasthi",
        "Oleg Medvedev",
        "Srayanta Mukherjee",
        "Anton Aristov",
        "Konstantin Polunin",
        "Alex De Mur",
        "Leonid Zhukov",
        "Azad Huseynov",
        "Murad Abdullayev"
      ],
      "abstract": "We present an AI-driven approach to discover compounds with optimal properties for CO2 capture from flue gas-refinery emissions' primary source. Focusing on ionic liquids (ILs) as alternatives to traditional amine-based solvents, we successfully identify new IL candidates with high working capacity, manageable viscosity, favorable regeneration energy, and viable synthetic routes. Our approach follows a five-stage pipeline. First, we generate IL candidates by pairing available cation and anion molecules, then predict temperature- and pressure-dependent CO2 solubility and viscosity using a GNN-based molecular property prediction model. Next, we convert solubility to working capacity and regeneration energy via Van't Hoff modeling, and then find the best set of candidates using Pareto optimization, before finally filtering those based on feasible synthesis routes. We identify 36 feasible candidates that could enable 5-10% OPEX savings and up to 10% CAPEX reductions through lower regeneration energy requirements and reduced corrosivity-offering a novel carbon-capture strategy for refineries moving forward.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ AI é©±åŠ¨çš„æ–¹æ³•ï¼Œæ—¨åœ¨ä»å·¥ä¸šçƒŸé“æ°”ä¸­å‘ç°ç”¨äº CO2 æ•é›†çš„æ–°å‹ Ionic Liquids (ILs) æº¶å‰‚ã€‚ä¸ºäº†æ›¿ä»£ä¼ ç»Ÿçš„ amine-based æº¶å‰‚ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªåŒ…å«äº”ä¸ªé˜¶æ®µçš„è‡ªåŠ¨åŒ–æµæ°´çº¿ï¼Œé€šè¿‡é…å¯¹é˜³ç¦»å­å’Œé˜´ç¦»å­ç”Ÿæˆå€™é€‰ç‰©è´¨ã€‚åˆ©ç”¨åŸºäº GNN çš„åˆ†å­å±æ€§é¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè¯„ä¼°ä¸åŒæ¸©åº¦å’Œå‹åŠ›ä¸‹çš„æº¶è§£åº¦ä¸ viscosityï¼Œå¹¶ç»“åˆ Van't Hoff modeling è®¡ç®— working capacity å’Œ regeneration energyã€‚é€šè¿‡ Pareto optimization ç­›é€‰å’Œåˆæˆè·¯å¾„å¯è¡Œæ€§è¿‡æ»¤ï¼Œç ”ç©¶æœ€ç»ˆè¯†åˆ«å‡º 36 ç§å…·æœ‰å·¥ä¸šåº”ç”¨å‰æ™¯çš„å€™é€‰ ILsã€‚è¿™äº›å€™é€‰ç‰©è´¨å‡­å€Ÿè¾ƒä½çš„èƒ½é‡éœ€æ±‚å’Œå‡å°çš„è…èš€æ€§ï¼Œé¢„è®¡å¯å®ç° 5-10% çš„ OPEX èŠ‚çœå’Œé«˜è¾¾ 10% çš„ CAPEX é™ä½ã€‚è¿™ä¸€æˆæœä¸ºç‚¼æ²¹è¡Œä¸šæä¾›äº†ä¸€ç§é«˜æ•ˆã€ç»æµä¸”å…·å¤‡åˆæˆå¯è¡Œæ€§çš„æ–°å‹ç¢³æ•é›†ç­–ç•¥ã€‚",
      "categories": [
        "physics.chem-ph",
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "physics.chem-ph",
      "comment": "21 pages, 15 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.03284v1",
      "published_date": "2026-01-02 15:41:59 UTC",
      "updated_date": "2026-01-02 15:41:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:18:55.300506+00:00"
    },
    {
      "arxiv_id": "2601.00716v1",
      "title": "Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model",
      "title_zh": "ç—…ç†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ•°æ®åç§»ä¸‹çš„æ€§èƒ½é€€åŒ–æ£€æµ‹",
      "authors": [
        "Hao Guan",
        "Li Zhou"
      ],
      "abstract": "Vision-Language Models have demonstrated strong potential in medical image analysis and disease diagnosis. However, after deployment, their performance may deteriorate when the input data distribution shifts from that observed during development. Detecting such performance degradation is essential for clinical reliability, yet remains challenging for large pre-trained VLMs operating without labeled data. In this study, we investigate performance degradation detection under data shift in a state-of-the-art pathology VLM. We examine both input-level data shift and output-level prediction behavior to understand their respective roles in monitoring model reliability. To facilitate systematic analysis of input data shift, we develop DomainSAT, a lightweight toolbox with a graphical interface that integrates representative shift detection algorithms and enables intuitive exploration of data shift. Our analysis shows that while input data shift detection is effective at identifying distributional changes and providing early diagnostic signals, it does not always correspond to actual performance degradation. Motivated by this observation, we further study output-based monitoring and introduce a label-free, confidence-based degradation indicator that directly captures changes in model prediction confidence. We find that this indicator exhibits a close relationship with performance degradation and serves as an effective complement to input shift detection. Experiments on a large-scale pathology dataset for tumor classification demonstrate that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift. These findings provide a practical and complementary framework for monitoring the reliability of foundation models in digital pathology.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç—…ç† Vision-Language Models (VLMs) åœ¨ Data Shift ä¸‹æ€§èƒ½ä¸‹é™ä¸”ç¼ºä¹æ ‡ç­¾ç›‘æµ‹çš„æŒ‘æˆ˜ï¼Œæ¢è®¨äº†è¾“å…¥å±‚æ•°æ®åç§»å’Œè¾“å‡ºå±‚é¢„æµ‹è¡Œä¸ºå¯¹æ¨¡å‹å¯é æ€§çš„å½±å“ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº† DomainSATï¼Œä¸€ä¸ªé›†æˆå¤šç§åç§»æ£€æµ‹ç®—æ³•å¹¶é…æœ‰å›¾å½¢ç•Œé¢çš„è½»é‡çº§å·¥å…·ç®±ï¼Œç”¨äºç›´è§‚æ¢ç´¢å’Œç³»ç»Ÿåˆ†æè¾“å…¥æ•°æ®åç§»ã€‚ç ”ç©¶å‘ç°ï¼Œè¾“å…¥æ•°æ®åç§»æ£€æµ‹è™½ç„¶èƒ½æœ‰æ•ˆè¯†åˆ«åˆ†å¸ƒå˜åŒ–ï¼Œä½†å¹¶ä¸æ€»èƒ½ç›´æ¥å¯¹åº”å®é™…çš„æ€§èƒ½é€€åŒ–ã€‚é’ˆå¯¹è¿™ä¸€å‘ç°ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§æ— éœ€æ ‡ç­¾ã€åŸºäºç½®ä¿¡åº¦çš„é€€åŒ–æŒ‡æ ‡ (confidence-based degradation indicator)ï¼Œé€šè¿‡æ•æ‰æ¨¡å‹é¢„æµ‹ä¿¡å¿ƒçš„å˜åŒ–æ¥åæ˜ æ€§èƒ½æ³¢åŠ¨ã€‚åœ¨å¤§å‹ç—…ç†è‚¿ç˜¤åˆ†ç±»æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œç»“åˆè¾“å…¥æ•°æ®åç§»æ£€æµ‹ä¸è¾“å‡ºç½®ä¿¡åº¦æŒ‡æ ‡ï¼Œèƒ½å¤Ÿæ›´å¯é åœ°æ£€æµ‹å’Œè§£é‡Š Data Shift ä¸‹çš„æ€§èƒ½é€€åŒ–ã€‚è¯¥ç ”ç©¶ä¸ºæ•°å­—ç—…ç†å­¦é¢†åŸŸåŸºç¡€æ¨¡å‹çš„å¯é æ€§ç›‘æ§æä¾›äº†ä¸€ä¸ªå®ç”¨ä¸”äº’è¡¥çš„æ¡†æ¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.00716v1",
      "published_date": "2026-01-02 15:12:06 UTC",
      "updated_date": "2026-01-02 15:12:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:18:00.014796+00:00"
    },
    {
      "arxiv_id": "2601.00694v1",
      "title": "A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference",
      "title_zh": "ç”¨äºå¯æ³›åŒ–è¡Œäººè¿‡è¡—è¡Œä¸ºæ¨æ–­çš„è§†è§‰ä¸çŸ¥è¯†å¢å¼ºå¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Qingwen Pu",
        "Kun Xie",
        "Hong Yang",
        "Guocong Zhai"
      ],
      "abstract": "Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰è¡Œäººè¿‡è¡—è¡Œä¸ºæ¨ç†æ¨¡å‹åœ¨æœªçŸ¥åœºæ™¯ä¸‹æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº† Pedestrian Crossing LLM (PedX-LLM)ï¼Œè¿™æ˜¯ä¸€ç§è§†è§‰ä¸çŸ¥è¯†å¢å¼ºçš„å¤§è¯­è¨€æ¨¡å‹æ¡†æ¶ã€‚PedX-LLM é€šè¿‡é›†æˆ LLaVA æå–çš„è§†è§‰ç‰¹å¾ã€æ–‡æœ¬æ•°æ®ä»¥åŠäº¤é€šé¢†åŸŸçŸ¥è¯† (transportation domain knowledge)ï¼Œå¹¶åˆ©ç”¨ä½ç§©è‡ªé€‚åº” (LoRA) æŠ€æœ¯å¯¹ LLaMA-2-7B åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPedX-LLM è¾¾åˆ°äº† 82.0% çš„å¹³è¡¡å‡†ç¡®ç‡ (balanced accuracy)ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç»Ÿè®¡å’Œç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚åˆ†æè¡¨æ˜ï¼Œè§†è§‰å¢å¼ºæ¨¡å—å’Œé¢†åŸŸçŸ¥è¯†çš„é›†æˆåˆ†åˆ«è´¡çŒ®äº† 2.9% å’Œ 4.1% çš„æ€§èƒ½æå‡ï¼Œæœ‰æ•ˆæ•æ‰äº†å»ºç­‘ç¯å¢ƒä¿¡æ¯ã€‚åœ¨è·¨ç«™ç‚¹éªŒè¯ä¸­ï¼Œzero-shot é…ç½®ä¸‹çš„æ¨¡å‹åœ¨æœªè§ç«™ç‚¹ä¸Šå–å¾—äº† 66.9% çš„å‡†ç¡®ç‡ï¼Œæ¯”åŸºå‡†æ–¹æ³•é«˜å‡ºè‡³å°‘ 18 ä¸ªç™¾åˆ†ç‚¹ã€‚é€šè¿‡ few-shot å­¦ä¹ å¼•å…¥ä»…äº”ä¸ªéªŒè¯ç¤ºä¾‹åï¼Œå…¶å‡†ç¡®ç‡è¿›ä¸€æ­¥æå‡è‡³ 72.2%ã€‚è¯¥ç ”ç©¶è¯æ˜äº†è§†è§‰ä¸çŸ¥è¯†å¢å¼ºçš„æ¨ç†èƒ½åŠ›ä½¿æ¨¡å‹èƒ½å¤Ÿæ¨¡ä»¿äººç±»å†³ç­–é€»è¾‘ï¼Œæœ‰æ•ˆå…‹æœäº†çº¯æ•°æ®é©±åŠ¨æ–¹æ³•çš„å±€é™æ€§å¹¶å®ç°äº†æ›´å¼ºçš„æ³›åŒ–æ€§èƒ½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00694v1",
      "published_date": "2026-01-02 14:13:28 UTC",
      "updated_date": "2026-01-02 14:13:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:18:34.990321+00:00"
    },
    {
      "arxiv_id": "2601.02411v1",
      "title": "SpikySpace: A Spiking State Space Model for Energy-Efficient Time Series Forecasting",
      "title_zh": "SpikySpaceï¼šä¸€ç§ç”¨äºé«˜èƒ½æ•ˆæ—¶é—´åºåˆ—é¢„æµ‹çš„è„‰å†²çŠ¶æ€ç©ºé—´æ¨¡å‹",
      "authors": [
        "Kaiwen Tang",
        "Jiaqi Zheng",
        "Yuze Jin",
        "Yupeng Qiu",
        "Guangda Sun",
        "Zhanglu Yan",
        "Weng-Fai Wong"
      ],
      "abstract": "Time-series forecasting often operates under tight power and latency budgets in fields like traffic management, industrial condition monitoring, and on-device sensing. These applications frequently require near real-time responses and low energy consumption on edge devices. Spiking neural networks (SNNs) offer event-driven computation and ultra-low power by exploiting temporal sparsity and multiplication-free computation. Yet existing SNN-based time-series forecasters often inherit complex transformer blocks, thereby losing much of the efficiency benefit. To solve the problem, we propose SpikySpace, a spiking state-space model (SSM) that reduces the quadratic cost in the attention block to linear time via selective scanning. Further, we replace dense SSM updates with sparse spike trains and execute selective scans only on spike events, thereby avoiding dense multiplications while preserving the SSM's structured memory. Because complex operations such as exponentials and divisions are costly on neuromorphic chips, we introduce simplified approximations of SiLU and Softplus to enable a neuromorphic-friendly model architecture. In matched settings, SpikySpace reduces estimated energy consumption by 98.73% and 96.24% compared to two state-of-the-art transformer based approaches, namely iTransformer and iSpikformer, respectively. In standard time series forecasting datasets, SpikySpace delivers competitive accuracy while substantially reducing energy cost and memory traffic. As the first full spiking state-space model, SpikySpace bridges neuromorphic efficiency with modern sequence modeling, marking a practical and scalable path toward efficient time series forecasting systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SpikySpaceï¼Œè¿™æ˜¯é¦–ä¸ªå®Œæ•´ä¸”é«˜èƒ½æ•ˆçš„è„‰å†²çŠ¶æ€ç©ºé—´æ¨¡å‹(Spiking State Space Model, SSM)ï¼Œæ—¨åœ¨è§£å†³è¾¹ç¼˜è®¾å¤‡åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­é¢ä¸´çš„åŠŸè€—ä¸å»¶è¿ŸæŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰è„‰å†²ç¥ç»ç½‘ç»œ(SNN)é¢„æµ‹å™¨å› æ²¿ç”¨å¤æ‚Transformeræ¨¡å—è€Œæ•ˆç‡å—é™çš„é—®é¢˜ï¼ŒSpikySpaceé€šè¿‡é€‰æ‹©æ€§æ‰«æ(selective scanning)å°†æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—æˆæœ¬ä»å¹³æ–¹é˜¶é™è‡³çº¿æ€§é˜¶ã€‚è¯¥æ¨¡å‹åˆ©ç”¨ç¨€ç–è„‰å†²åºåˆ—(spike trains)æ›¿ä»£ç¨ å¯†çš„SSMæ›´æ–°ï¼Œä»…åœ¨è„‰å†²äº‹ä»¶å‘ç”Ÿæ—¶æ‰§è¡Œæ‰«æï¼Œæœ‰æ•ˆé¿å…äº†å¤§é‡ç¨ å¯†ä¹˜æ³•è¿ç®—å¹¶ä¿ç•™äº†ç»“æ„åŒ–è®°å¿†ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥SiLUå’ŒSoftplusçš„ç®€åŒ–è¿‘ä¼¼æ–¹æ¡ˆï¼ŒSpikySpaceå®ç°äº†ç¥ç»å½¢æ€å‹å¥½(neuromorphic-friendly)çš„æ¶æ„ã€‚å®éªŒè¯æ˜ï¼Œåœ¨æ ‡å‡†æ•°æ®é›†ä¸Šï¼ŒSpikySpaceçš„é¢„ä¼°èƒ½è€—æ¯”iTransformerå’ŒiSpikformeråˆ†åˆ«é™ä½äº†98.73%å’Œ96.24%ï¼Œåœ¨ä¿æŒç«äº‰æ€§å‡†ç¡®ç‡çš„åŒæ—¶å¤§å¹…å‡å°‘äº†å†…å­˜å¼€é”€ã€‚è¿™ä¸€æˆæœæˆåŠŸè¡”æ¥äº†ç¥ç»å½¢æ€æ•ˆç‡ä¸ç°ä»£åºåˆ—å»ºæ¨¡ï¼Œä¸ºé«˜æ•ˆæ—¶é—´åºåˆ—é¢„æµ‹ç³»ç»Ÿæä¾›äº†å¯æ‰©å±•çš„æ¼”è¿›è·¯å¾„ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "13 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.02411v1",
      "published_date": "2026-01-02 13:10:53 UTC",
      "updated_date": "2026-01-02 13:10:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:18:35.892086+00:00"
    },
    {
      "arxiv_id": "2601.00679v1",
      "title": "QSLM: A Performance- and Memory-aware Quantization Framework with Tiered Search Strategy for Spike-driven Language Models",
      "title_zh": "QSLMï¼šä¸€ç§é¢å‘è„‰å†²é©±åŠ¨è¯­è¨€æ¨¡å‹çš„å…¼é¡¾æ€§èƒ½ä¸å†…å­˜çš„åˆ†å±‚æœç´¢é‡åŒ–æ¡†æ¶",
      "authors": [
        "Rachmad Vidya Wicaksana Putra",
        "Pasindu Wickramasinghe",
        "Muhammad Shafique"
      ],
      "abstract": "Large Language Models (LLMs) have been emerging as prominent AI models for solving many natural language tasks due to their high performance (e.g., accuracy) and capabilities in generating high-quality responses to the given inputs. However, their large computational cost, huge memory footprints, and high processing power/energy make it challenging for their embedded deployments. Amid several tinyLLMs, recent works have proposed spike-driven language models (SLMs) for significantly reducing the processing power/energy of LLMs. However, their memory footprints still remain too large for low-cost and resource-constrained embedded devices. Manual quantization approach may effectively compress SLM memory footprints, but it requires a huge design time and compute power to find the quantization setting for each network, hence making this approach not-scalable for handling different networks, performance requirements, and memory budgets. To bridge this gap, we propose QSLM, a novel framework that performs automated quantization for compressing pre-trained SLMs, while meeting the performance and memory constraints. To achieve this, QSLM first identifies the hierarchy of the given network architecture and the sensitivity of network layers under quantization, then employs a tiered quantization strategy (e.g., global-, block-, and module-level quantization) while leveraging a multi-objective performance-and-memory trade-off function to select the final quantization setting. Experimental results indicate that our QSLM reduces memory footprint by up to 86.5%, reduces power consumption by up to 20%, maintains high performance across different tasks (i.e., by up to 84.4% accuracy of sentiment classification on the SST-2 dataset and perplexity score of 23.2 for text generation on the WikiText-2 dataset) close to the original non-quantized model while meeting the performance and memory constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†QSLMï¼Œä¸€ä¸ªé’ˆå¯¹è„‰å†²é©±åŠ¨è¯­è¨€æ¨¡å‹(Spike-driven Language Models, SLMs)è®¾è®¡çš„æ€§èƒ½ä¸å†…å­˜æ„ŸçŸ¥å‹è‡ªåŠ¨åŒ–é‡åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨åµŒå…¥å¼è®¾å¤‡éƒ¨ç½²æ—¶å†…å­˜å ç”¨è¿‡é«˜çš„é—®é¢˜ã€‚QSLMé€šè¿‡è¯†åˆ«ç½‘ç»œæ¶æ„å±‚æ¬¡å¹¶åˆ†æå„å±‚å¯¹é‡åŒ–çš„æ•æ„Ÿåº¦(Sensitivity)ï¼Œé‡‡ç”¨åŒ…å«å…¨å±€çº§ã€å—çº§å’Œæ¨¡å—çº§åœ¨å†…çš„å¤šçº§é‡åŒ–ç­–ç•¥(Tiered quantization strategy)ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤šç›®æ ‡æ€§èƒ½ä¸å†…å­˜æƒè¡¡å‡½æ•°æ¥ä¼˜åŒ–æœ€ç»ˆé‡åŒ–è®¾ç½®ï¼Œä»è€Œåœ¨æ»¡è¶³èµ„æºçº¦æŸçš„åŒæ—¶ä¿æŒæ¨¡å‹ç²¾åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQSLMæœ€é«˜å¯å‡å°‘86.5%çš„å†…å­˜å ç”¨å’Œ20%çš„åŠŸè€—ï¼Œä¸”åœ¨SST-2åˆ†ç±»ä»»åŠ¡å’ŒWikiText-2ç”Ÿæˆä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºæ¥è¿‘åŸå§‹æœªé‡åŒ–æ¨¡å‹çš„å“è¶Šæ€§èƒ½ã€‚è¿™ä¸€ç ”ç©¶ä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹é«˜æ•ˆéƒ¨ç½²è½»é‡åŒ–è¯­è¨€æ¨¡å‹æä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "Accepted at the Design, Automation and Test in Europe Conference (DATE) 2025 on April 20th-22nd, 2025 in Verona, Italy",
      "pdf_url": "https://arxiv.org/pdf/2601.00679v1",
      "published_date": "2026-01-02 13:05:33 UTC",
      "updated_date": "2026-01-02 13:05:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:18:28.000966+00:00"
    },
    {
      "arxiv_id": "2601.00677v1",
      "title": "IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning",
      "title_zh": "IRPOï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ æ‰©å±• Bradley-Terry æ¨¡å‹",
      "authors": [
        "Haonan Song",
        "Qingchen Xie",
        "Huan Zhu",
        "Feng Xiao",
        "Luxi Xing",
        "Fuzhen Li",
        "Liu Kang",
        "Feng Jiang",
        "Zhiyong Zheng",
        "Fan Yang"
      ],
      "abstract": "Generative Reward Models (GRMs) have attracted considerable research interest in reward modeling due to their interpretability, inference-time scalability, and potential for refinement through reinforcement learning (RL). However, widely used pairwise GRMs create a computational bottleneck when integrated with RL algorithms such as Group Relative Policy Optimization (GRPO). This bottleneck arises from two factors: (i) the O(n^2) time complexity of pairwise comparisons required to obtain relative scores, and (ii) the computational overhead of repeated sampling or additional chain-of-thought (CoT) reasoning to improve performance. To address the first factor, we propose Intergroup Relative Preference Optimization (IRPO), a novel RL framework that incorporates the well-established Bradley-Terry model into GRPO. By generating a pointwise score for each response, IRPO enables efficient evaluation of arbitrarily many candidates during RL training while preserving interpretability and fine-grained reward signals. Experimental results demonstrate that IRPO achieves state-of-the-art (SOTA) performance among pointwise GRMs across multiple benchmarks, with performance comparable to that of current leading pairwise GRMs. Furthermore, we show that IRPO significantly outperforms pairwise GRMs in post-training evaluations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹(Generative Reward Models, GRMs)åœ¨ä¸å¼ºåŒ–å­¦ä¹ (RL)ç®—æ³•å¦‚Group Relative Policy Optimization (GRPO)ç»“åˆæ—¶é¢ä¸´çš„è®¡ç®—ç“¶é¢ˆé—®é¢˜ï¼Œæå‡ºäº†Intergroup Relative Preference Optimization (IRPO)æ¡†æ¶ã€‚é’ˆå¯¹æˆå¯¹æ¯”è¾ƒ(Pairwise)æ–¹æ³•å­˜åœ¨çš„$O(n^2)$å¤æ‚åº¦åŠé«˜æ˜‚è®¡ç®—å¼€é”€ï¼ŒIRPOå°†ç»å…¸çš„Bradley-Terryæ¨¡å‹é›†æˆåˆ°GRPOä¸­ï¼Œé€šè¿‡ä¸ºæ¯ä¸ªå“åº”ç”Ÿæˆç‚¹å¯¹ç‚¹(Pointwise)åˆ†æ•°ï¼Œå®ç°äº†å¯¹å¤§é‡å€™é€‰å›å¤çš„é«˜æ•ˆè¯„ä¼°ã€‚è¿™ç§æ–¹æ³•åœ¨ä¿ç•™æ¨¡å‹å¯è§£é‡Šæ€§å’Œç»†ç²’åº¦å¥–åŠ±ä¿¡å·çš„åŒæ—¶ï¼Œæå¤§åœ°æå‡äº†è®­ç»ƒæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIRPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†ç‚¹å¯¹ç‚¹GRMsçš„SOTAæ€§èƒ½ï¼Œå…¶è¡¨ç°ä¸é¢†å…ˆçš„æˆå¯¹æ¯”è¾ƒGRMsç›¸å½“ã€‚ç ”ç©¶è¿˜å‘ç°ï¼ŒIRPOåœ¨è®­ç»ƒåçš„æ€§èƒ½è¡¨ç°ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æˆå¯¹æ¯”è¾ƒæ¨¡å‹ï¼Œè¯æ˜äº†å…¶åœ¨å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­çš„ä¼˜è¶Šæ€§ä¸æ‰©å±•æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.00677v1",
      "published_date": "2026-01-02 12:57:06 UTC",
      "updated_date": "2026-01-02 12:57:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:19:11.756563+00:00"
    },
    {
      "arxiv_id": "2601.00671v1",
      "title": "Fast-weight Product Key Memory",
      "title_zh": "å¿«æƒé‡ä¹˜ç§¯é”®å†…å­˜",
      "authors": [
        "Tianyu Zhao",
        "Llion Jones"
      ],
      "abstract": "Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, \"fast-weight\" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Fast-weight Product Key Memory (FwPKM)ï¼Œæ—¨åœ¨è§£å†³ç°ä»£è¯­è¨€æ¨¡å‹åœ¨å­˜å‚¨å®¹é‡ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚FwPKM å°†ä¼ ç»Ÿçš„é™æ€ Product Key Memory (PKM) æ¨¡å—è½¬åŒ–ä¸ºä¸€ç§åŠ¨æ€çš„â€œå¿«æƒé‡â€ (fast-weight) æƒ…èŠ‚è®°å¿† (episodic memory)ï¼Œé€šè¿‡åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­åˆ©ç”¨å±€éƒ¨å—çº§æ¢¯åº¦ä¸‹é™ (local chunk-level gradient descent) åŠ¨æ€æ›´æ–°å‚æ•°ï¼Œå®ç°äº†å¯¹è¾“å…¥åºåˆ—ä¸­æ–°é”®å€¼å¯¹çš„å¿«é€Ÿè®°å¿†ä¸æ£€ç´¢ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFwPKM ä½œä¸ºæƒ…èŠ‚è®°å¿†èƒ½å¤Ÿæœ‰æ•ˆè¡¥å……æ ‡å‡†æ¨¡å—çš„è¯­ä¹‰è®°å¿† (semantic memory)ï¼Œåœ¨é•¿ä¸Šä¸‹æ–‡æ•°æ®é›†ä¸Šæ˜¾è‘—é™ä½äº†å›°æƒ‘åº¦ (perplexity)ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å±•ç°å‡ºæå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨ Needle in a Haystack è¯„ä¼°ä¸­ï¼Œä»…åœ¨ 4K ä»¤ç‰Œåºåˆ—ä¸Šè®­ç»ƒçš„ FwPKM å³å¯æˆåŠŸå¤„ç†é«˜è¾¾ 128K ä»¤ç‰Œçš„ä¸Šä¸‹æ–‡ç¯å¢ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00671v1",
      "published_date": "2026-01-02 12:37:53 UTC",
      "updated_date": "2026-01-02 12:37:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:19:15.695640+00:00"
    },
    {
      "arxiv_id": "2601.00664v1",
      "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation",
      "title_zh": "Avatar Forcingï¼šé¢å‘è‡ªç„¶å¯¹è¯çš„å®æ—¶äº¤äº’å¼å¤´éƒ¨æ•°å­—äººç”Ÿæˆ",
      "authors": [
        "Taekyung Ki",
        "Sangwon Jang",
        "Jaehyeong Jo",
        "Jaehong Yoon",
        "Sung Ju Hwang"
      ],
      "abstract": "Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è°ˆè¯å¤´åƒç”Ÿæˆ(Talking head generation)ä¸­ç¼ºä¹å®æ—¶äº’åŠ¨æ„Ÿå’Œæƒ…æ„Ÿå‚ä¸çš„é—®é¢˜ï¼Œæå‡ºäº†Avatar Forcingæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£å¼ºåˆ¶(diffusion forcing)æŠ€æœ¯å»ºæ¨¡ç”¨æˆ·ä¸å¤´åƒé—´çš„å®æ—¶äº¤äº’ï¼Œæ”¯æŒå¯¹ç”¨æˆ·éŸ³é¢‘å’ŒåŠ¨ä½œç­‰å¤šç§æ¨¡æ€è¾“å…¥(multimodal inputs)çš„ä½å»¶è¿Ÿå¤„ç†ã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼Œå¤´åƒèƒ½å¤Ÿé’ˆå¯¹è¯­éŸ³ã€ç‚¹å¤´åŠç¬‘å£°ç­‰è¨€è¯­å’Œéè¨€è¯­ä¿¡å·åšå‡ºå³æ—¶åé¦ˆã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†ç›´æ¥åå¥½ä¼˜åŒ–(direct preference optimization)æ–¹æ³•ï¼Œåˆ©ç”¨åˆæˆè´Ÿæ ·æœ¬å®ç°äº†è¡¨è¾¾æ€§äº’åŠ¨çš„æ— æ ‡ç­¾å­¦ä¹ ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶å®ç°äº†çº¦500msçš„æä½å»¶è¿Ÿï¼Œè¾ƒåŸºçº¿æ¨¡å‹(baseline)æé€Ÿ6.8å€ã€‚åœ¨è¡¨ç°åŠ›ä¸ååº”æ€§æ–¹é¢ï¼ŒAvatar Forcingç”Ÿæˆçš„å¤´åƒåŠ¨ä½œè·å¾—äº†è¶…è¿‡80%çš„ç”¨æˆ·åå¥½è®¤å¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.MM"
      ],
      "primary_category": "cs.LG",
      "comment": "Project page: https://taekyungki.github.io/AvatarForcing/",
      "pdf_url": "https://arxiv.org/pdf/2601.00664v1",
      "published_date": "2026-01-02 11:58:48 UTC",
      "updated_date": "2026-01-02 11:58:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:19:12.437443+00:00"
    },
    {
      "arxiv_id": "2601.00655v2",
      "title": "Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability",
      "title_zh": "å¯è§£é‡Šæ€§å¼•å¯¼çš„åŒç›®æ ‡ä¼˜åŒ–ï¼šå‡†ç¡®æ€§ä¸å¯è§£é‡Šæ€§çš„å¯¹é½",
      "authors": [
        "Kasra Fouladi",
        "Hamta Rahmani"
      ],
      "abstract": "This paper introduces Interpretability-Guided Bi-objective Optimization (IGBO), a framework that trains interpretable models by incorporating structured domain knowledge via a bi-objective formulation. IGBO encodes feature importance hierarchies as a Directed Acyclic Graph (DAG) via Central Limit Theorem-based construction and uses Temporal Integrated Gradients (TIG) to measure feature importance. To address the Out-of-Distribution (OOD) problem in TIG computation, we propose an Optimal Path Oracle that learns data-manifold-aware integration paths. Theoretical analysis establishes convergence properties via a geometric projection mapping $\\mathcal{P}$ and proves robustness to mini-batch noise. Central Limit Theorem-based construction of the interpretability DAG ensures statistical validity of edge orientation decisions. Empirical results on time-series data demonstrate IGBO's effectiveness in enforcing DAG constraints with minimal accuracy loss, outperforming standard regularization baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Interpretability-Guided Bi-objective Optimization (IGBO)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åŒç›®æ ‡ä¼˜åŒ–å…¬å¼(bi-objective formulation)æ•´åˆç»“æ„åŒ–é¢†åŸŸçŸ¥è¯†ï¼Œä»¥è®­ç»ƒå…·æœ‰å¯è§£é‡Šæ€§çš„æ¨¡å‹ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŸºäºä¸­å¿ƒæé™å®šç†(Central Limit Theorem)çš„æ„å»ºæ–¹æ³•ï¼Œå°†ç‰¹å¾é‡è¦æ€§å±‚çº§ç¼–ç ä¸ºæœ‰å‘æ— ç¯å›¾(Directed Acyclic Graph, DAG)ã€‚ä¸ºäº†è¡¡é‡ç‰¹å¾é‡è¦æ€§ï¼ŒIGBOé‡‡ç”¨äº†Temporal Integrated Gradients (TIG)æŠ€æœ¯ï¼Œå¹¶é’ˆå¯¹å…¶é¢ä¸´çš„åˆ†å¸ƒå¤–(Out-of-Distribution)é—®é¢˜æå‡ºäº†Optimal Path Oracleï¼Œä»è€Œå­¦ä¹ æ„ŸçŸ¥æ•°æ®æµå½¢çš„é›†æˆè·¯å¾„ã€‚ç†è®ºåˆ†æéªŒè¯äº†è¯¥æ–¹æ³•é€šè¿‡å‡ ä½•æŠ•å½±æ˜ å°„ $\\mathcal{P}$ å®ç°çš„æ”¶æ•›æ€§ï¼Œå¹¶è¯æ˜äº†å…¶å¯¹å°æ‰¹é‡å™ªå£°(mini-batch noise)çš„ç¨³å¥æ€§ã€‚åœ¨æ—¶é—´åºåˆ—æ•°æ®ä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒIGBOèƒ½å¤Ÿåœ¨å¼ºåˆ¶æ‰§è¡ŒDAGçº¦æŸçš„åŒæ—¶æ˜¾è‘—å‡å°å‡†ç¡®ç‡æŸå¤±ï¼Œå…¶æ€§èƒ½è¡¨ç°ä¼˜äºæ ‡å‡†çš„æ­£åˆ™åŒ–åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.00655v2",
      "published_date": "2026-01-02 11:32:00 UTC",
      "updated_date": "2026-01-06 15:21:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:19:16.702081+00:00"
    },
    {
      "arxiv_id": "2601.00936v1",
      "title": "Emoji-Based Jailbreaking of Large Language Models",
      "title_zh": "åŸºäºè¡¨æƒ…ç¬¦å·çš„å¤§è¯­è¨€æ¨¡å‹è¶Šç‹±",
      "authors": [
        "M P V S Gopinadh",
        "S Mahaboob Hussain"
      ],
      "abstract": "Large Language Models (LLMs) are integral to modern AI applications, but their safety alignment mechanisms can be bypassed through adversarial prompt engineering. This study investigates emoji-based jailbreaking, where emoji sequences are embedded in textual prompts to trigger harmful and unethical outputs from LLMs. We evaluated 50 emoji-based prompts on four open-source LLMs: Mistral 7B, Qwen 2 7B, Gemma 2 9B, and Llama 3 8B. Metrics included jailbreak success rate, safety alignment adherence, and latency, with responses categorized as successful, partial and failed. Results revealed model-specific vulnerabilities: Gemma 2 9B and Mistral 7B exhibited 10 % success rates, while Qwen 2 7B achieved full alignment (0% success). A chi-square test (chi^2 = 32.94, p < 0.001) confirmed significant inter-model differences. While prior works focused on emoji attacks targeting safety judges or classifiers, our empirical analysis examines direct prompt-level vulnerabilities in LLMs. The results reveal limitations in safety mechanisms and highlight the necessity for systematic handling of emoji-based representations in prompt-level safety and alignment pipelines.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åŸºäº Emoji çš„å¤§è¯­è¨€æ¨¡å‹ (LLMs) è¶Šç‹± (Jailbreaking) æ”»å‡»ï¼Œå³é€šè¿‡åœ¨æ–‡æœ¬æç¤ºè¯ä¸­åµŒå…¥è¡¨æƒ…ç¬¦å·åºåˆ—æ¥ç»•è¿‡å®‰å…¨æœºåˆ¶å¹¶è¯±å¯¼æœ‰å®³è¾“å‡ºã€‚ç ”ç©¶äººå‘˜åœ¨ Mistral 7Bã€Qwen 2 7Bã€Gemma 2 9B å’Œ Llama 3 8B å››ç§å¼€æºæ¨¡å‹ä¸Šè¯„ä¼°äº† 50 ä¸ªåŸºäº Emoji çš„æç¤ºè¯ï¼Œå¹¶åˆ†æäº†è¶Šç‹±æˆåŠŸç‡ã€å®‰å…¨å¯¹é½ (Safety Alignment) ä»¥åŠå»¶è¿Ÿç­‰æŒ‡æ ‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºä¸åŒæ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„è„†å¼±æ€§å·®å¼‚ï¼Œå…¶ä¸­ Gemma 2 9B å’Œ Mistral 7B çš„æ”»å‡»æˆåŠŸç‡è¾¾åˆ° 10%ï¼Œè€Œ Qwen 2 7B åˆ™è¡¨ç°å‡ºå®Œå…¨çš„å¯¹é½å®‰å…¨æ€§ã€‚å¡æ–¹æ£€éªŒ (Chi-square test) è¿›ä¸€æ­¥è¯å®äº†è¿™äº›è·¨æ¨¡å‹è¡¨ç°çš„æ˜¾è‘—æ€§å·®å¼‚ã€‚è¯¥å®è¯åˆ†æç›´æ¥æ­ç¤ºäº† LLMs åœ¨æç¤ºè¯å±‚é¢å­˜åœ¨çš„å®‰å…¨æ¼æ´ï¼Œå¹¶å¼ºè°ƒäº†åœ¨å®‰å…¨å’Œå¯¹é½ç®¡é“ (Alignment Pipelines) ä¸­é’ˆå¯¹ Emoji è¡¨å¾å»ºç«‹ç³»ç»ŸåŒ–å¤„ç†æœºåˆ¶çš„ç´§è¿«æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "7 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.00936v1",
      "published_date": "2026-01-02 10:49:06 UTC",
      "updated_date": "2026-01-02 10:49:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:19:22.222191+00:00"
    },
    {
      "arxiv_id": "2601.00935v1",
      "title": "Improving Code-Switching Speech Recognition with TTS Data Augmentation",
      "title_zh": "åˆ©ç”¨ TTS æ•°æ®å¢å¼ºæå‡è¯­ç è½¬æ¢è¯­éŸ³è¯†åˆ«æ€§èƒ½",
      "authors": [
        "Yue Heng Yeo",
        "Yuchen Hu",
        "Shreyas Gopal",
        "Yizhou Peng",
        "Hexin Liu",
        "Eng Siong Chng"
      ],
      "abstract": "Automatic speech recognition (ASR) for conversational code-switching speech remains challenging due to the scarcity of realistic, high-quality labeled speech data. This paper explores multilingual text-to-speech (TTS) models as an effective data augmentation technique to address this shortage. Specifically, we fine-tune the multilingual CosyVoice2 TTS model on the SEAME dataset to generate synthetic conversational Chinese-English code-switching speech, significantly increasing the quantity and speaker diversity of available training data. Our experiments demonstrate that augmenting real speech with synthetic speech reduces the mixed error rate (MER) from 12.1 percent to 10.1 percent on DevMan and from 17.8 percent to 16.0 percent on DevSGE, indicating consistent performance gains. These results confirm that multilingual TTS is an effective and practical tool for enhancing ASR robustness in low-resource conversational code-switching scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯¹è¯å¼è¯­ç è½¬æ¢(Code-Switching)è¯­éŸ³è¯†åˆ«(ASR)é¢†åŸŸé«˜è´¨é‡æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å¤šè¯­è¨€æ–‡æœ¬è½¬è¯­éŸ³(TTS)æ¨¡å‹è¿›è¡Œæ•°æ®å¢å¼ºçš„æœ‰æ•ˆæ–¹æ³•ã€‚ä½œè€…é€šè¿‡åœ¨ SEAME æ•°æ®é›†ä¸Šå¾®è°ƒå¤šè¯­è¨€ CosyVoice2 TTS æ¨¡å‹ï¼Œç”Ÿæˆäº†å¤§é‡å…·æœ‰è¯´è¯äººå¤šæ ·æ€§çš„åˆæˆä¸­è‹±(Chinese-English)è¯­ç è½¬æ¢è¯­éŸ³æ•°æ®ï¼Œæ˜¾è‘—æ‰©å……äº†è®­ç»ƒèµ„æºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡å°†åˆæˆè¯­éŸ³å¼•å…¥è®­ç»ƒï¼Œæ¨¡å‹åœ¨ DevMan é›†åˆä¸Šçš„æ··åˆé”™è¯¯ç‡(Mixed Error Rate, MER)ä» 12.1% é™ä½è‡³ 10.1%ï¼Œåœ¨ DevSGE é›†åˆä¸Šä» 17.8% é™è‡³ 16.0%ã€‚è¿™ä¸€æ€§èƒ½çš„æŒç»­æå‡è¯æ˜äº†å¤šè¯­è¨€ TTS æ˜¯å¢å¼ºä½èµ„æºè¯­ç è½¬æ¢åœºæ™¯ä¸‹ ASR é²æ£’æ€§çš„å®ç”¨ä¸”é«˜æ•ˆçš„å·¥å…·ã€‚è¯¥é¡¹ç ”ç©¶ä¸ºè§£å†³è·¨è¯­è¨€è¯­éŸ³è¯†åˆ«ä¸­çš„æ•°æ®ç“¶é¢ˆé—®é¢˜æä¾›äº†é‡è¦çš„æŠ€æœ¯å‚è€ƒã€‚",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "This paper was accepted by APSIPA 2025",
      "pdf_url": "https://arxiv.org/pdf/2601.00935v1",
      "published_date": "2026-01-02 10:11:51 UTC",
      "updated_date": "2026-01-02 10:11:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:19:24.062879+00:00"
    },
    {
      "arxiv_id": "2601.00623v1",
      "title": "DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations",
      "title_zh": "DA-DPOï¼šç”¨äºå‡å°‘ MLLM å¹»è§‰çš„é«˜æ•ˆéš¾åº¦æ„ŸçŸ¥åå¥½ä¼˜åŒ–",
      "authors": [
        "Longtian Qiu",
        "Shan Ning",
        "Chuyu Zhang",
        "Jiaxuan Sun",
        "Xuming He"
      ],
      "abstract": "Direct Preference Optimization (DPO) has shown strong potential for mitigating hallucinations in Multimodal Large Language Models (MLLMs). However, existing multimodal DPO approaches often suffer from overfitting due to the difficulty imbalance in preference data. Our analysis shows that MLLMs tend to overemphasize easily distinguishable preference pairs, which hinders fine-grained hallucination suppression and degrades overall performance. To address this issue, we propose Difficulty-Aware Direct Preference Optimization (DA-DPO), a cost-effective framework designed to balance the learning process. DA-DPO consists of two main components: (1) Difficulty Estimation leverages pre-trained vision--language models with complementary generative and contrastive objectives, whose outputs are integrated via a distribution-aware voting strategy to produce robust difficulty scores without additional training; and (2) Difficulty-Aware Training reweights preference pairs based on their estimated difficulty, down-weighting easy samples while emphasizing harder ones to alleviate overfitting. This framework enables more effective preference optimization by prioritizing challenging examples, without requiring new data or extra fine-tuning stages. Extensive experiments demonstrate that DA-DPO consistently improves multimodal preference optimization, yielding stronger robustness to hallucinations and better generalization across standard benchmarks, while remaining computationally efficient. The project page is available at https://artanic30.github.io/project_pages/DA-DPO/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DA-DPOï¼ˆDifficulty-Aware Direct Preference Optimizationï¼‰ï¼Œä¸€ç§æˆæœ¬æ•ˆç›Šé«˜çš„æ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„å¹»è§‰é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰ DPO æ–¹æ³•å› åå¥½æ•°æ®éš¾åº¦ä¸å¹³è¡¡è€Œå¯¼è‡´çš„è¿‡æ‹ŸåˆæŒ‘æˆ˜ï¼Œä½œè€…é€šè¿‡åˆ†ææŒ‡å‡ºï¼Œæ¨¡å‹å¾€å¾€è¿‡åº¦å…³æ³¨å®¹æ˜“åŒºåˆ†çš„æ ·æœ¬ï¼Œä»è€Œé™åˆ¶äº†å¯¹ç»†ç²’åº¦å¹»è§‰çš„æŠ‘åˆ¶æ•ˆæœã€‚DA-DPO åŒ…å« Difficulty Estimation å’Œ Difficulty-Aware Training ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå‰è€…åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹é€šè¿‡ç”Ÿæˆä¸å¯¹æ¯”ç›®æ ‡ï¼Œç»“åˆåˆ†å¸ƒæ„ŸçŸ¥æŠ•ç¥¨ç­–ç•¥åœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹äº§å‡ºé²æ£’çš„éš¾åº¦åˆ†æ•°ï¼›åè€…åˆ™æ ¹æ®éš¾åº¦å¯¹åå¥½å¯¹è¿›è¡Œé‡æ–°åŠ æƒï¼Œé€šè¿‡å¼ºåŒ–å›°éš¾æ ·æœ¬çš„å­¦ä¹ æ¥ç¼“è§£è¿‡æ‹Ÿåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDA-DPO åœ¨ä¸å¢åŠ æ–°æ•°æ®æˆ–é¢å¤–å¾®è°ƒé˜¶æ®µçš„å‰æä¸‹ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹å¹»è§‰çš„é²æ£’æ€§åŠåœ¨æ ‡å‡†åŸºå‡†ä¸Šçš„æ³›åŒ–è¡¨ç°ã€‚è¯¥æ¡†æ¶ä¸ä»…æå‡äº†åå¥½ä¼˜åŒ–çš„æ•ˆç‡ï¼Œä¹Ÿä¸ºæ„å»ºæ›´å¯é çš„å¤šæ¨¡æ€æ™ºèƒ½ç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by TMLR",
      "pdf_url": "https://arxiv.org/pdf/2601.00623v1",
      "published_date": "2026-01-02 09:41:54 UTC",
      "updated_date": "2026-01-02 09:41:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:19:37.410008+00:00"
    },
    {
      "arxiv_id": "2601.00617v1",
      "title": "Noise-Robust Tiny Object Localization with Flows",
      "title_zh": "åŸºäºæµçš„å™ªå£°é²æ£’å¾®å°ç›®æ ‡å®šä½",
      "authors": [
        "Huixin Sun",
        "Linlin Yang",
        "Ronyu Chen",
        "Kerui Gu",
        "Baochang Zhang",
        "Angela Yao",
        "Xianbin Cao"
      ],
      "abstract": "Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framework leveraging normalizing flows for flexible error modeling and uncertainty-guided optimization. Our method captures complex, non-Gaussian prediction distributions through flow-based error modeling, enabling robust learning under noisy supervision. An uncertainty-aware gradient modulation mechanism further suppresses learning from high-uncertainty, noise-prone samples, mitigating overfitting while stabilizing training. Extensive experiments across three datasets validate our approach's effectiveness. Especially, TOLF boosts the DINO baseline by 1.2% AP on the AI-TOD dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨ç›®æ ‡æ£€æµ‹ä¸­å¾®å°ç›®æ ‡ï¼ˆtiny objectsï¼‰å¯¹æ ‡æ³¨å™ªå£°é«˜åº¦æ•æ„Ÿä¸”æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œæå‡ºäº†TOLFï¼ˆTiny Object Localization with Flowsï¼‰ï¼Œä¸€ä¸ªå…·æœ‰å™ªå£°é²æ£’æ€§çš„å®šä½æ¡†æ¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨Normalizing Flowsè¿›è¡Œçµæ´»çš„è¯¯å·®å»ºæ¨¡ï¼Œé€šè¿‡æ•æ‰å¤æ‚çš„éé«˜æ–¯é¢„æµ‹åˆ†å¸ƒï¼Œå®ç°äº†åœ¨å˜ˆæ‚ç›‘ç£ç¯å¢ƒä¸‹çš„é²æ£’å­¦ä¹ ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ç§ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¢¯åº¦è°ƒåˆ¶ï¼ˆuncertainty-aware gradient modulationï¼‰æœºåˆ¶ï¼Œæ—¨åœ¨æŠ‘åˆ¶å¯¹é«˜ä¸ç¡®å®šæ€§åŠæ˜“äº§ç”Ÿå™ªå£°æ ·æœ¬çš„å­¦ä¹ ï¼Œä»è€Œåœ¨ç¨³å®šè®­ç»ƒçš„åŒæ—¶å‡è½»è¿‡æ‹Ÿåˆã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨AI-TODæ•°æ®é›†ä¸Šï¼ŒTOLFå°†DINOåŸºçº¿æ¨¡å‹æ€§èƒ½æå‡äº†1.2% APã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†åˆ©ç”¨æµæ¨¡å‹è¿›è¡Œä¸ç¡®å®šæ€§å»ºæ¨¡åœ¨æå‡å¾®å°ç›®æ ‡å®šä½ç²¾åº¦æ–¹é¢çš„æ˜¾è‘—æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.00617v1",
      "published_date": "2026-01-02 09:16:55 UTC",
      "updated_date": "2026-01-02 09:16:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:19:29.785382+00:00"
    },
    {
      "arxiv_id": "2601.06098v1",
      "title": "Automatic Question Generation for Intuitive Learning Utilizing Causal Graph Guided Chain of Thought Reasoning",
      "title_zh": "åŸºäºå› æœå›¾å¼•å¯¼æ€ç»´é“¾æ¨ç†çš„ç›´è§‰å­¦ä¹ è‡ªåŠ¨é—®é¢˜ç”Ÿæˆ",
      "authors": [
        "Nicholas X. Wang",
        "Neel V. Parpia",
        "Aaryan D. Parikh",
        "Aggelos K. Katsaggelos"
      ],
      "abstract": "Intuitive learning is crucial for developing deep conceptual understanding, especially in STEM education, where students often struggle with abstract and interconnected concepts. Automatic question generation has become an effective strategy for personalized and adaptive learning. However, its effectiveness is hindered by hallucinations in large language models (LLMs), which may generate factually incorrect, ambiguous, or pedagogically inconsistent questions. To address this issue, we propose a novel framework that combines causal-graph-guided Chain-of-Thought (CoT) reasoning with a multi-agent LLM architecture. This approach ensures the generation of accurate, meaningful, and curriculum-aligned questions. Causal graphs provide an explicit representation of domain knowledge, while CoT reasoning facilitates a structured, step-by-step traversal of related concepts. Dedicated LLM agents are assigned specific tasks such as graph pathfinding, reasoning, validation, and output, all working within domain constraints. A dual validation mechanism-at both the conceptual and output stages-greatly reduces hallucinations. Experimental results demonstrate up to a 70% improvement in quality compared to reference methods and yielded highly favorable outcomes in subjective evaluations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹STEMæ•™è‚²ä¸­å­¦ç”Ÿç†è§£æŠ½è±¡æ¦‚å¿µéš¾ä»¥åŠå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è‡ªåŠ¨é—®é¢˜ç”Ÿæˆ(Automatic Question Generation)ä¸­å­˜åœ¨çš„å¹»è§‰å’Œæ•™å­¦ä¸ä¸€è‡´ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å› æœå›¾(Causal Graph)å¼•å¯¼çš„é“¾å¼æ€ç»´(Chain-of-Thought, CoT)æ¨ç†ä¸å¤šæ™ºèƒ½ä½“(Multi-agent)LLMæ¶æ„ï¼Œæ—¨åœ¨ç”Ÿæˆå‡†ç¡®ã€æœ‰æ„ä¹‰ä¸”ç¬¦åˆè¯¾ç¨‹è¦æ±‚çš„æ•™å­¦é—®é¢˜ã€‚å…¶ä¸­å› æœå›¾æä¾›äº†é¢†åŸŸçŸ¥è¯†çš„æ˜¾å¼è¡¨ç¤ºï¼Œè€ŒCoTæ¨ç†åˆ™å®ç°äº†å¯¹ç›¸å…³æ¦‚å¿µçš„ç»“æ„åŒ–ã€æ­¥è¿›å¼éå†ã€‚ç³»ç»Ÿé€šè¿‡åˆ†é…ç‰¹å®šçš„LLMæ™ºèƒ½ä½“æ‰§è¡Œè·¯å¾„æœç´¢ã€æ¨ç†ã€éªŒè¯å’Œè¾“å‡ºä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†æ¶µç›–æ¦‚å¿µä¸è¾“å‡ºé˜¶æ®µçš„åŒé‡éªŒè¯æœºåˆ¶(Dual Validation Mechanism)ä»¥å¤§å¹…å‡å°‘å¹»è§‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é—®é¢˜è´¨é‡ä¸Šæ¯”å‚è€ƒæ–¹æ³•æå‡äº†é«˜è¾¾70%ï¼Œå¹¶åœ¨ä¸»è§‚è¯„ä»·ä¸­è·å¾—äº†æä½³çš„åé¦ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06098v1",
      "published_date": "2026-01-02 08:49:58 UTC",
      "updated_date": "2026-01-02 08:49:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:19:35.017471+00:00"
    },
    {
      "arxiv_id": "2601.00611v1",
      "title": "Stronger Approximation Guarantees for Non-Monotone Î³-Weakly DR-Submodular Maximization",
      "title_zh": "éå•è°ƒ Î³-å¼± DR-æ¬¡æ¨¡æœ€å¤§åŒ–é—®é¢˜çš„æ›´å¼ºè¿‘ä¼¼ä¿è¯",
      "authors": [
        "Hareshkumar Jadav",
        "Ranveer Singh",
        "Vaneet Aggarwal"
      ],
      "abstract": "Maximizing submodular objectives under constraints is a fundamental problem in machine learning and optimization. We study the maximization of a nonnegative, non-monotone $Î³$-weakly DR-submodular function over a down-closed convex body. Our main result is an approximation algorithm whose guarantee depends smoothly on $Î³$; in particular, when $Î³=1$ (the DR-submodular case) our bound recovers the $0.401$ approximation factor, while for $Î³<1$ the guarantee degrades gracefully and, it improves upon previously reported bounds for $Î³$-weakly DR-submodular maximization under the same constraints. Our approach combines a Frank-Wolfe-guided continuous-greedy framework with a $Î³$-aware double-greedy step, yielding a simple yet effective procedure for handling non-monotonicity. This results in state-of-the-art guarantees for non-monotone $Î³$-weakly DR-submodular maximization over down-closed convex bodies.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ å’Œä¼˜åŒ–é¢†åŸŸä¸­çš„åŸºæœ¬é—®é¢˜ï¼Œæ¢è®¨äº†åœ¨å‘ä¸‹å°é—­å‡¸ä½“(down-closed convex body)çº¦æŸä¸‹ï¼Œéè´Ÿã€éå•è°ƒ $\\gamma$-weakly DR-submodular å‡½æ•°çš„æœ€å¤§åŒ–é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§å…¨æ–°çš„è¿‘ä¼¼ç®—æ³•ï¼Œå…¶æ ¸å¿ƒæ˜¯å°† Frank-Wolfe å¼•å¯¼çš„ continuous-greedy æ¡†æ¶ä¸ä¸€ä¸ªæ„ŸçŸ¥ $\\gamma$ çš„ double-greedy æ­¥éª¤ç›¸ç»“åˆï¼Œä»è€Œæœ‰æ•ˆå¤„ç†äº†å‡½æ•°çš„éå•è°ƒæ€§ã€‚è¯¥ç®—æ³•çš„è¿‘ä¼¼ä¿è¯éš $\\gamma$ å‚æ•°å¹³æ»‘å˜åŒ–ï¼Œç‰¹åˆ«æ˜¯åœ¨ $\\gamma=1$ å³ DR-submodular çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæ¢å¤åˆ° 0.401 çš„è¿‘ä¼¼å› å­ã€‚å¯¹äº $\\gamma < 1$ çš„æƒ…å½¢ï¼Œè¯¥ç®—æ³•çš„æ€§èƒ½è¡¨ç°ä¼˜äºä»¥å¾€åœ¨ç›¸åŒçº¦æŸä¸‹æŠ¥é“çš„è¿‘ä¼¼ç•Œé™ã€‚è¿™é¡¹å·¥ä½œä¸ºéå•è°ƒ $\\gamma$-weakly DR-submodular æœ€å¤§åŒ–é—®é¢˜æä¾›äº†ç›®å‰æœ€å…ˆè¿›çš„(state-of-the-art)ç†è®ºä¿è¯ï¼Œå±•ç¤ºäº†è¯¥ç®—æ³•åœ¨å¤æ‚çº¦æŸä¼˜åŒ–ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CC",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "Extended version of paper accepted in AAMAS 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.00611v1",
      "published_date": "2026-01-02 08:44:10 UTC",
      "updated_date": "2026-01-02 08:44:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:19:38.654137+00:00"
    },
    {
      "arxiv_id": "2601.16217v1",
      "title": "ChiEngMixBench: Evaluating Large Language Models on Spontaneous and Natural Chinese-English Code-Mixed Generation",
      "title_zh": "ChiEngMixBenchï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨è‡ªå‘ä¸è‡ªç„¶çš„ä¸­è‹±è¯­ç æ··ç”¨ç”Ÿæˆä¸­çš„è¡¨ç°",
      "authors": [
        "Qingyan Yang",
        "Tongxi Wang",
        "Yunsheng Luo"
      ],
      "abstract": "Code-mixing is increasingly prevalent in interactions between humans and large language models, yet existing work often reduces it to a translation or convertibility problem, making it difficult to assess whether a model's switching behavior is context-appropriate and aligned with human conventions. We introduce ChiEngMixBench, the first benchmark designed to evaluate code-mixing ability in authentic community contexts, built upon a general construction pipeline that enables scalable dataset development across domains and bilingual pairs. ChiEngMixBench formulates code-mixing as a cognitive alignment problem, characterized by two complementary signals: Spontaneity and Naturalness. Empirical evaluation shows that our metrics can systematically distinguish code-mixing performance across models. Beyond benchmarking, we further uncover an implicitly emergent Terminology Layering Strategy, a phenomenon consistent with the Matrix Language Frame (MLF) theory, indicating structured cognitive alignment between multilingual large language models and human communication.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† ChiEngMixBenchï¼Œè¿™æ˜¯é¦–ä¸ªæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çœŸå®ç¤¾åŒºè¯­å¢ƒä¸‹ Code-mixing èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰è¯„ä¼°å°†å…¶ç®€åŒ–ä¸ºç¿»è¯‘é—®é¢˜è€Œå¿½è§†è¯­å¢ƒå¥‘åˆåº¦çš„é—®é¢˜ã€‚è¯¥åŸºå‡†å°†è¯­ç åˆ‡æ¢å»ºæ¨¡ä¸ºè®¤çŸ¥å¯¹é½é—®é¢˜ï¼Œé€šè¿‡ Spontaneity å’Œ Naturalness ä¸¤ä¸ªæ ¸å¿ƒæŒ‡æ ‡æ¥è¡¡é‡æ¨¡å‹ç”Ÿæˆçš„è‡ªå‘æ€§ä¸è‡ªç„¶åº¦ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒChiEngMixBench èƒ½å¤Ÿç³»ç»Ÿæ€§åœ°è¡¡é‡ä¸åŒæ¨¡å‹åœ¨å¤„ç†ä¸­è‹±æ··åˆæ–‡æœ¬æ—¶çš„æ€§èƒ½å·®å¼‚ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ­ç¤ºäº†æ¨¡å‹ä¸­éšå«å‡ºç°çš„ Terminology Layering Strategyï¼Œè¿™ä¸€å‘ç°ä¸ Matrix Language Frame (MLF) ç†è®ºé«˜åº¦å¥‘åˆï¼Œå±•ç¤ºäº†å¤šè¯­è¨€å¤§æ¨¡å‹ä¸äººç±»äº¤æµä¹‹é—´å­˜åœ¨çš„ç»“æ„åŒ–è®¤çŸ¥å¯¹é½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.16217v1",
      "published_date": "2026-01-02 08:18:27 UTC",
      "updated_date": "2026-01-02 08:18:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:20:17.678826+00:00"
    },
    {
      "arxiv_id": "2601.00933v1",
      "title": "LOFA: Online Influence Maximization under Full-Bandit Feedback using Lazy Forward Selection",
      "title_zh": "LOFAï¼šåŸºäºå»¶è¿Ÿå‰å‘é€‰æ‹©çš„å…¨è€è™æœºåé¦ˆåœ¨çº¿å½±å“åŠ›æœ€å¤§åŒ–",
      "authors": [
        "Jinyu Xu",
        "Abhishek K. Umrawal"
      ],
      "abstract": "We study the problem of influence maximization (IM) in an online setting, where the goal is to select a subset of nodes$\\unicode{x2014}$called the seed set$\\unicode{x2014}$at each time step over a fixed time horizon, subject to a cardinality budget constraint, to maximize the expected cumulative influence. We operate under a full-bandit feedback model, where only the influence of the chosen seed set at each time step is observed, with no additional structural information about the network or diffusion process. It is well-established that the influence function is submodular, and existing algorithms exploit this property to achieve low regret. In this work, we leverage this property further and propose the Lazy Online Forward Algorithm (LOFA), which achieves a lower empirical regret. We conduct experiments on a real-world social network to demonstrate that LOFA achieves superior performance compared to existing bandit algorithms in terms of cumulative regret and instantaneous reward.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨çº¿å½±å“åŠ›æœ€å¤§åŒ–(Online Influence Maximization)é—®é¢˜ï¼Œæ—¨åœ¨åœ¨åŸºæ•°é¢„ç®—çº¦æŸä¸‹é€šè¿‡åœ¨æ¯ä¸ªæ—¶é—´æ­¥é€‰æ‹©ç§å­é›†(seed set)æ¥æœ€å¤§åŒ–æœŸæœ›ç´¯ç§¯å½±å“åŠ›ã€‚è®ºæ–‡é‡‡ç”¨å…¨è‡‚è€è™æœºåé¦ˆ(Full-Bandit Feedback)æ¨¡å‹ï¼Œåœ¨æ­¤ç¯å¢ƒä¸‹ä»…èƒ½è§‚æµ‹åˆ°é€‰å®šç§å­é›†çš„å®é™…å½±å“åŠ›ï¼Œè€Œæ— éœ€äº†è§£ç½‘ç»œç»“æ„æˆ–æ‰©æ•£è¿‡ç¨‹çš„å…ˆéªŒä¿¡æ¯ã€‚ç ”ç©¶è€…åˆ©ç”¨å½±å“åŠ›å‡½æ•°çš„æ¬¡æ¨¡æ€§(submodular)ç‰¹å¾ï¼Œæå‡ºäº†æ‡’æƒ°åœ¨çº¿å‰å‘ç®—æ³•(Lazy Online Forward Algorithm, LOFA)ï¼Œæ—¨åœ¨å®ç°æ›´ä½çš„ç»éªŒåæ‚”åº¦(empirical regret)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨çœŸå®ç¤¾äº¤ç½‘ç»œç¯å¢ƒä¸‹ï¼ŒLOFAåœ¨ç´¯ç§¯åæ‚”åº¦(cumulative regret)å’Œç¬æ—¶å¥–åŠ±(instantaneous reward)ç­‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰çš„è€è™æœºç®—æ³•(bandit algorithms)ï¼Œå±•ç°äº†å“è¶Šçš„ç®—æ³•æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages and 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.00933v1",
      "published_date": "2026-01-02 08:00:14 UTC",
      "updated_date": "2026-01-02 08:00:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:19:57.922836+00:00"
    },
    {
      "arxiv_id": "2601.15296v1",
      "title": "Entropy-Tree: Tree-Based Decoding with Entropy-Guided Exploration",
      "title_zh": "Entropy-Treeï¼šåŸºäºç†µå¼•å¯¼æ¢ç´¢çš„æ ‘çŠ¶è§£ç ",
      "authors": [
        "Longxuan Wei",
        "Yubo Zhang",
        "Zijiao Zhang",
        "Zhihu Wang",
        "Shiwan Zhao",
        "Tianyu Huang",
        "Huiting Zhao",
        "Chenfei Liu",
        "Shenao Zhang",
        "Junchi Yan"
      ],
      "abstract": "Large language models achieve strong reasoning performance, yet existing decoding strategies either explore blindly (random sampling) or redundantly (independent multi-sampling). We propose Entropy-Tree, a tree-based decoding method that exploits entropy as a signal for branching decisions--expanding the search tree only at positions where the model exhibits genuine uncertainty. Entropy-Tree shows superior accuracy and calibration in reasoning tasks: it achieves better pass@k than Multi-chain across multiple models and datasets, and its predictive entropy demonstrates better AUROC compared to several traditional metrics. Entropy-Tree unifies efficient structured exploration and reliable uncertainty estimation within a single decoding procedure.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­è§£ç ç­–ç•¥å­˜åœ¨çš„ç›²ç›®æ€§æˆ–å†—ä½™é—®é¢˜ï¼Œæå‡ºäº† Entropy-Treeï¼Œä¸€ç§åŸºäºæ ‘çš„è§£ç æ–¹æ³•ã€‚Entropy-Tree åˆ©ç”¨ç†µ (entropy) ä½œä¸ºåˆ†æ”¯å†³ç­–çš„ä¿¡å·ï¼Œä»…åœ¨æ¨¡å‹è¡¨ç°å‡ºçœŸæ­£ä¸ç¡®å®šæ€§çš„ä½ç½®æ‰©å±•æœç´¢æ ‘ï¼Œä»è€Œå®ç°äº†é«˜æ•ˆçš„ç»“æ„åŒ–æ¢ç´¢ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEntropy-Tree åœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„æ¨ç†å‡†ç¡®æ€§å’Œæ ¡å‡†æ€§è¡¨ç°ä¼˜å¼‚ï¼Œå…¶ pass@k æŒ‡æ ‡æ˜¾è‘—è¶…è¿‡äº† Multi-chain æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•çš„é¢„æµ‹ç†µåœ¨ AUROC è¯„ä¼°ä¸­ä¹Ÿä¼˜äºä¼ ç»ŸæŒ‡æ ‡ã€‚è¯¥ç ”ç©¶æˆåŠŸå°†é«˜æ•ˆçš„æœç´¢æ¢ç´¢ä¸å¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡ (uncertainty estimation) ç»Ÿä¸€åœ¨å•ä¸€çš„è§£ç æµç¨‹ä¸­ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.15296v1",
      "published_date": "2026-01-02 07:14:05 UTC",
      "updated_date": "2026-01-02 07:14:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:20:21.948440+00:00"
    },
    {
      "arxiv_id": "2601.14270v1",
      "title": "Opening the Black Box: A Survey on the Mechanisms of Multi-Step Reasoning in Large Language Models",
      "title_zh": "æ­å¼€é»‘ç›’ï¼šå¤§è¯­è¨€æ¨¡å‹å¤šæ­¥æ¨ç†æœºåˆ¶ç»¼è¿°",
      "authors": [
        "Liangming Pan",
        "Jason Liang",
        "Jiaran Ye",
        "Minglai Yang",
        "Xinyuan Lu",
        "Fengbin Zhu"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities to solve problems requiring multiple reasoning steps, yet the internal mechanisms enabling such capabilities remain elusive. Unlike existing surveys that primarily focus on engineering methods to enhance performance, this survey provides a comprehensive overview of the mechanisms underlying LLM multi-step reasoning. We organize the survey around a conceptual framework comprising seven interconnected research questions, from how LLMs execute implicit multi-hop reasoning within hidden activations to how verbalized explicit reasoning remodels the internal computation. Finally, we highlight five research directions for future mechanistic studies.",
      "tldr_zh": "è¯¥ç»¼è¿°è®ºæ–‡æ—¨åœ¨æ¢è®¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) å¤šæ­¥æ¨ç† (Multi-Step Reasoning) çš„å†…åœ¨æœºåˆ¶ï¼Œè¯•å›¾æ­å¼€å…¶â€œé»‘ç›’â€å±æ€§ã€‚ä¸ä»¥å¾€ä¾§é‡äºæé«˜æ€§èƒ½çš„å·¥ç¨‹åŒ–æ–¹æ³•ä¸åŒï¼Œæœ¬æ–‡æä¾›äº†ä¸€ä¸ªå…³äº LLM å¤šæ­¥æ¨ç†åº•å±‚æœºåˆ¶çš„å…¨é¢æ¦‚è¿°ã€‚ç ”ç©¶å›´ç»•ä¸€ä¸ªåŒ…å«ä¸ƒä¸ªç›¸äº’å…³è”é—®é¢˜çš„æ¦‚å¿µæ¡†æ¶å±•å¼€ï¼Œåˆ†æäº† LLMs å¦‚ä½•åœ¨éšè—æ¿€æ´» (hidden activations) ä¸­æ‰§è¡Œéšå¼å¤šè·³æ¨ç† (implicit multi-hop reasoning)ï¼Œä»¥åŠè¯­è¨€åŒ–çš„æ˜¾å¼æ¨ç† (verbalized explicit reasoning) å¦‚ä½•é‡å¡‘å†…éƒ¨è®¡ç®—ã€‚æœ€åï¼Œè®ºæ–‡å¼ºè°ƒäº†æœªæ¥æœºæ¢°è®ºç ”ç©¶ (mechanistic studies) çš„äº”ä¸ªç ”ç©¶æ–¹å‘ã€‚è¯¥ç»¼è¿°ä¸ºæ·±å…¥ç†è§£ LLMs å¤„ç†å¤æ‚ä»»åŠ¡çš„é€»è¾‘æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Technical Report",
      "pdf_url": "https://arxiv.org/pdf/2601.14270v1",
      "published_date": "2026-01-02 06:22:56 UTC",
      "updated_date": "2026-01-02 06:22:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:19:57.339924+00:00"
    },
    {
      "arxiv_id": "2601.02410v1",
      "title": "The Vibe-Check Protocol: Quantifying Cognitive Offloading in AI Programming",
      "title_zh": "Vibe-Check åè®®ï¼šé‡åŒ– AI ç¼–ç¨‹ä¸­çš„è®¤çŸ¥å¸è½½",
      "authors": [
        "Aizierjiang Aiersilan"
      ],
      "abstract": "The integration of Large Language Models (LLMs) into software engineering education has driven the emergence of ``Vibe Coding,'' a paradigm where developers articulate high-level intent through natural language and delegate implementation to AI agents. While proponents argue this approach modernizes pedagogy by emphasizing conceptual design over syntactic memorization, accumulating empirical evidence raises concerns regarding skill retention and deep conceptual understanding. This paper proposes a theoretical framework to investigate the research question: \\textit{Is Vibe Coding a better way to learn software engineering?} We posit a divergence in student outcomes between those leveraging AI for acceleration versus those using it for cognitive offloading. To evaluate these educational trade-offs, we propose the \\textbf{Vibe-Check Protocol (VCP)}, a systematic benchmarking framework incorporating three quantitative metrics: the \\textit{Cold Start Refactor} ($M_{CSR}$) for modeling skill decay; \\textit{Hallucination Trap Detection} ($M_{HT}$) based on signal detection theory to evaluate error identification; and the \\textit{Explainability Gap} ($E_{gap}$) for quantifying the divergence between code complexity and conceptual comprehension. Through controlled comparisons, VCP aims to provide a quantitative basis for educators to determine the optimal pedagogical boundary: identifying contexts where Vibe Coding fosters genuine mastery and contexts where it introduces hidden technical debt and superficial competence.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†è½¯ä»¶å·¥ç¨‹æ•™è‚²ä¸­æ—¥ç›Šæµè¡Œçš„ Vibe Coding æ¨¡å¼ï¼Œå³å¼€å‘è€…é€šè¿‡è‡ªç„¶è¯­è¨€è¡¨è¾¾é«˜å±‚æ„å›¾å¹¶ç”± AI ä»£ç†æ‰§è¡Œå®ç°ï¼Œæ—¨åœ¨åˆ†æå…¶å¯¹å­¦ä¹ è€…æŠ€èƒ½ç•™å­˜å’Œæ·±åº¦ç†è§£çš„å½±å“ã€‚ä¸ºäº†æ¢ç©¶è¯¥æ¨¡å¼æ˜¯å¦çœŸæ­£æœ‰åˆ©äºè½¯ä»¶å·¥ç¨‹å­¦ä¹ ï¼Œè®ºæ–‡æå‡ºäº† Vibe-Check Protocol (VCP) ç†è®ºæ¡†æ¶ï¼Œç”¨äºåŒºåˆ†å­¦ç”Ÿæ˜¯åˆ©ç”¨ AI å®ç°å¼€å‘åŠ é€Ÿè¿˜æ˜¯äº§ç”Ÿäº†è®¤çŸ¥å¸è½½ (Cognitive Offloading)ã€‚è¯¥åè®®å¼•å…¥äº†ä¸‰ä¸ªå…³é”®é‡åŒ–æŒ‡æ ‡ï¼šç”¨äºå»ºæ¨¡æŠ€èƒ½è¡°é€€çš„ Cold Start Refactor ($M_{CSR}$)ã€è¯„ä¼°é”™è¯¯è¯†åˆ«èƒ½åŠ›çš„ Hallucination Trap Detection ($M_{HT}$)ï¼Œä»¥åŠè¡¡é‡ä»£ç å¤æ‚åº¦ä¸æ¦‚å¿µç†è§£ä¹‹é—´å·®å¼‚çš„ Explainability Gap ($E_{gap}$)ã€‚é€šè¿‡å—æ§å®éªŒå¯¹æ¯”ï¼ŒVCP æ—¨åœ¨ä¸ºæ•™è‚²è€…æä¾›å®šé‡ä¾æ®ï¼Œä»è€Œç•Œå®š Vibe Coding ä¿ƒè¿›çŸ¥è¯†çœŸæ­£æŒæ¡ä¸å¯¼è‡´éšå½¢æŠ€æœ¯å€ºæˆ–è¡¨é¢èƒœä»»åŠ›ä¹‹é—´çš„æ•™å­¦è¾¹ç•Œã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CY",
        "cs.GR"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.02410v1",
      "published_date": "2026-01-02 06:13:41 UTC",
      "updated_date": "2026-01-02 06:13:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:20:13.936705+00:00"
    },
    {
      "arxiv_id": "2601.00583v1",
      "title": "HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts",
      "title_zh": "HFedMoEï¼šåŸºäºæ··åˆä¸“å®¶æ¨¡å‹çš„èµ„æºæ„ŸçŸ¥å¼‚æ„è”é‚¦å­¦ä¹ ",
      "authors": [
        "Zihan Fang",
        "Zheng Lin",
        "Senkang Hu",
        "Yanan Ma",
        "Yihang Tao",
        "Yiqin Deng",
        "Xianhao Chen",
        "Yuguang Fang"
      ],
      "abstract": "While federated learning (FL) enables fine-tuning of large language models (LLMs) without compromising data privacy, the substantial size of an LLM renders on-device training impractical for resource-constrained clients, such as mobile devices. Thus, Mixture-of-Experts (MoE) models have emerged as a computation-efficient solution, which activates only a sparse subset of experts during model training to reduce computing burden without sacrificing performance. Though integrating MoE into FL fine-tuning holds significant potential, it still encounters three key challenges: i) selecting appropriate experts for clients remains challenging due to the lack of a reliable metric to measure each expert's impact on local fine-tuning performance, ii) the heterogeneous computing resources across clients severely hinder MoE-based LLM fine-tuning, as dynamic expert activations across diverse input samples can overwhelm resource-constrained devices, and iii) client-specific expert subsets and routing preference undermine global aggregation, where misaligned expert updates and inconsistent gating networks in troduce destructive interference. To address these challenges, we propose HFedMoE, a heterogeneous MoE-based FL fine-tuning framework that customizes a subset of experts to each client for computation-efficient LLM fine-tuning. Specifically, HFedMoE identifies the expert importance based on its contributions to fine-tuning performance, and then adaptively selects a subset of experts from an information bottleneck perspective to align with each client' s computing budget. A sparsity-aware model aggregation strategy is also designed to aggregate the actively fine-tuned experts and gating parameters with importance weighted contributions. Extensive experiments demonstrate that HFedMoE outperforms state-of-the-art benchmarks in training accuracy and convergence speed.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è”é‚¦å­¦ä¹ (FL)å¾®è°ƒè¿‡ç¨‹ä¸­å—é™å®¢æˆ·ç«¯éš¾ä»¥æ‰¿æ‹…é«˜æ˜‚è®¡ç®—å¼€é”€çš„é—®é¢˜ï¼Œæå‡ºäº†HFedMoEï¼Œä¸€ç§èµ„æºæ„ŸçŸ¥çš„å¼‚æ„ä¸“å®¶æ··åˆ(MoE)å¾®è°ƒæ¡†æ¶ã€‚ä¸ºäº†è§£å†³ä¸“å®¶é€‰æ‹©å’Œå¼‚æ„è®¡ç®—èµ„æºå¸¦æ¥çš„æŒ‘æˆ˜ï¼ŒHFedMoEé€šè¿‡è¡¡é‡ä¸“å®¶å¯¹å¾®è°ƒæ€§èƒ½çš„è´¡çŒ®åº¦æ¥è¯†åˆ«å…¶é‡è¦æ€§ï¼Œå¹¶ä»ä¿¡æ¯ç“¶é¢ˆ(information bottleneck)çš„è§’åº¦ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯è‡ªé€‚åº”åœ°å®šåˆ¶ä¸“å®¶å­é›†ä»¥åŒ¹é…å…¶è®¡ç®—é¢„ç®—ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†ç¨€ç–æ„ŸçŸ¥çš„æ¨¡å‹èšåˆç­–ç•¥ï¼Œåˆ©ç”¨é‡è¦æ€§åŠ æƒæ¥èšåˆæ´»è·ƒçš„ä¸“å®¶å’Œé—¨æ§ç½‘ç»œ(gating networks)å‚æ•°ï¼Œæœ‰æ•ˆè§£å†³äº†å› å®¢æˆ·ç«¯æ›´æ–°ä¸ä¸€è‡´è€Œäº§ç”Ÿçš„ç ´åæ€§å¹²æ‰°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHFedMoEåœ¨è®­ç»ƒå‡†ç¡®ç‡å’Œæ”¶æ•›é€Ÿåº¦ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›åŸºå‡†ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨ç§»åŠ¨è®¾å¤‡ç­‰èµ„æºå—é™ç¯å¢ƒä¸‹å®ç°é«˜æ•ˆã€éšç§ä¿æŠ¤çš„LLMå¾®è°ƒæä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 16 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.00583v1",
      "published_date": "2026-01-02 05:56:11 UTC",
      "updated_date": "2026-01-02 05:56:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:20:11.882171+00:00"
    },
    {
      "arxiv_id": "2601.00580v1",
      "title": "Priority-Aware Multi-Robot Coverage Path Planning",
      "title_zh": "ä¼˜å…ˆçº§æ„ŸçŸ¥çš„å¤šæœºå™¨äººè¦†ç›–è·¯å¾„è§„åˆ’",
      "authors": [
        "Kanghoon Lee",
        "Hyeonjun Kim",
        "Jiachen Li",
        "Jinkyoo Park"
      ],
      "abstract": "Multi-robot systems are widely used for coverage tasks that require efficient coordination across large environments. In Multi-Robot Coverage Path Planning (MCPP), the objective is typically to minimize the makespan by generating non-overlapping paths for full-area coverage. However, most existing methods assume uniform importance across regions, limiting their effectiveness in scenarios where some zones require faster attention. We introduce the Priority-Aware MCPP (PA-MCPP) problem, where a subset of the environment is designated as prioritized zones with associated weights. The goal is to minimize, in lexicographic order, the total priority-weighted latency of zone coverage and the overall makespan. To address this, we propose a scalable two-phase framework combining (1) greedy zone assignment with local search, spanning-tree-based path planning, and (2) Steiner-tree-guided residual coverage. Experiments across diverse scenarios demonstrate that our method significantly reduces priority-weighted latency compared to standard MCPP baselines, while maintaining competitive makespan. Sensitivity analyses further show that the method scales well with the number of robots and that zone coverage behavior can be effectively controlled by adjusting priority weights.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Priority-Aware Multi-Robot Coverage Path Planning (PA-MCPP)é—®é¢˜ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå¤šæœºå™¨äººè¦†ç›–è·¯å¾„è§„åˆ’(MCPP)æ–¹æ³•ä¸­ç”±äºå¿½ç•¥åŒºåŸŸé‡è¦æ€§å·®å¼‚è€Œå¯¼è‡´çš„ä»»åŠ¡æ•ˆç‡å—é™é—®é¢˜ã€‚PA-MCPPçš„ç›®æ ‡æ˜¯æŒ‰å­—å…¸åºæœ€å°åŒ–åŒºåŸŸè¦†ç›–çš„æ€»æƒé‡å»¶è¿Ÿ(total priority-weighted latency)ä»¥åŠæ•´ä½“å®Œå·¥æ—¶é—´(makespan)ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç¬¬ä¸€é˜¶æ®µç»“åˆäº†è´ªå©ªåŒºåŸŸåˆ†é…(greedy zone assignment)ã€å±€éƒ¨æœç´¢(local search)ä»¥åŠåŸºäºç”Ÿæˆæ ‘çš„è·¯å¾„è§„åˆ’(spanning-tree-based path planning)ï¼Œç¬¬äºŒé˜¶æ®µé‡‡ç”¨æ–¯å¦çº³æ ‘å¼•å¯¼çš„æ®‹ä½™è¦†ç›–(Steiner-tree-guided residual coverage)æ¥å®Œå–„è§„åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æ ‡å‡†çš„MCPPåŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç«äº‰æ€§çš„makespançš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†æƒé‡å»¶è¿Ÿã€‚çµæ•åº¦åˆ†æè¿›ä¸€æ­¥è¯å®äº†è¯¥æ–¹æ³•åœ¨æœºå™¨äººæ•°é‡å¢åŠ æ—¶å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡è°ƒæ•´ä¼˜å…ˆçº§æƒé‡æœ‰æ•ˆæ§åˆ¶åŒºåŸŸè¦†ç›–è¡Œä¸ºã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "IEEE Robotics and Automation Letters, 8 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.00580v1",
      "published_date": "2026-01-02 05:45:15 UTC",
      "updated_date": "2026-01-02 05:45:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:20:23.540211+00:00"
    },
    {
      "arxiv_id": "2601.14269v1",
      "title": "The Slow Drift of Support: Boundary Failures in Multi-Turn Mental Health LLM Dialogues",
      "title_zh": "æ”¯æŒçš„æ¸è¿›å¼åç§»ï¼šå¤šè½®å¿ƒç†å¥åº·å¤§è¯­è¨€æ¨¡å‹å¯¹è¯ä¸­çš„è¾¹ç•Œå¤±æ•ˆ",
      "authors": [
        "Youyou Cheng",
        "Zhuangwei Kang",
        "Kerry Jiang",
        "Chenyu Sun",
        "Qiyang Pan"
      ],
      "abstract": "Large language models (LLMs) have been widely used for mental health support. However, current safety evaluations in this field are mostly limited to detecting whether LLMs output prohibited words in single-turn conversations, neglecting the gradual erosion of safety boundaries in long dialogues. Examples include making definitive guarantees, assuming responsibility, and playing professional roles. We believe that with the evolution of mainstream LLMs, words with obvious safety risks are easily filtered by their underlying systems, while the real danger lies in the gradual transgression of boundaries during multi-turn interactions, driven by the LLM's attempts at comfort and empathy.\n  This paper proposes a multi-turn stress testing framework and conducts long-dialogue safety tests on three cutting-edge LLMs using two pressure methods: static progression and adaptive probing. We generated 50 virtual patient profiles and stress-tested each model through up to 20 rounds of virtual psychiatric dialogues. The experimental results show that violations are common, and both pressure modes produced similar violation rates. However, adaptive probing significantly advanced the time at which models crossed boundaries, reducing the average number of turns from 9.21 in static progression to 4.64. Under both mechanisms, making definitive or zero-risk promises was the primary way in which boundaries were breached. These findings suggest that the robustness of LLM safety boundaries cannot be inferred solely through single-turn tests; it is necessary to fully consider the wear and tear on safety boundaries caused by different interaction pressures and characteristics in extended dialogues.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Large Language Models (LLMs)åœ¨å¿ƒç†å¥åº·æ”¯æŒä¸­çš„å®‰å…¨æ€§é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰çš„è¯„ä¼°å¤šå±€é™äºå•è½®å¯¹è¯ä¸­çš„è¿ç¦è¯æ£€æµ‹ï¼Œå¿½è§†äº†é•¿å¯¹è¯ä¸­å®‰å…¨è¾¹ç•Œé€æ¸ä¾µèš€çš„é£é™©ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªMulti-Turn Stress Testing Frameworkï¼Œé€šè¿‡Static Progressionå’ŒAdaptive Probingä¸¤ç§å‹åŠ›æ–¹æ³•å¯¹å‰æ²¿æ¨¡å‹è¿›è¡Œæµ‹è¯•ã€‚ç ”ç©¶äººå‘˜ç”Ÿæˆäº†50ä¸ªè™šæ‹Ÿæ‚£è€…ç”»åƒï¼Œå¹¶åœ¨å¤šè¾¾20è½®çš„è™šæ‹Ÿç²¾ç¥ç—…å­¦å¯¹è¯ä¸­å¯¹ä¸‰ç§å…ˆè¿›LLMsè¿›è¡Œäº†å‹åŠ›æµ‹è¯•ã€‚å®éªŒç»“æœæ˜¾ç¤ºè¿è§„è¡Œä¸ºæ™®éå­˜åœ¨ï¼Œä¸”Adaptive Probingæ˜¾è‘—æå‰äº†æ¨¡å‹è¶Šç•Œçš„æ—¶é—´ï¼Œå°†å¹³å‡è¿è§„è½®æ¬¡ä»9.21è½®ç¼©çŸ­è‡³4.64è½®ã€‚åœ¨ä¸¤ç§æœºåˆ¶ä¸‹ï¼Œåšå‡ºDefinitive or Zero-Risk Promisesæ˜¯è¾¹ç•Œè¢«çªç ´çš„ä¸»è¦æ–¹å¼ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œä»…å‡­å•è½®æµ‹è¯•æ— æ³•æ¨æ–­LLMså®‰å…¨è¾¹ç•Œçš„ç¨³å¥æ€§ï¼Œå¿…é¡»å……åˆ†è€ƒè™‘é•¿å¯¹è¯ä¸­ä¸åŒäº¤äº’å‹åŠ›å¯¹å®‰å…¨è¾¹ç•Œé€ æˆçš„ç£¨æŸã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.14269v1",
      "published_date": "2026-01-02 05:42:28 UTC",
      "updated_date": "2026-01-02 05:42:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:20:23.222122+00:00"
    },
    {
      "arxiv_id": "2601.00578v1",
      "title": "Learning to be Reproducible: Custom Loss Design for Robust Neural Networks",
      "title_zh": "å­¦ä¼šå¯å¤ç°ï¼šé¢å‘é²æ£’ç¥ç»ç½‘ç»œçš„å®šåˆ¶åŒ–æŸå¤±å‡½æ•°è®¾è®¡",
      "authors": [
        "Waqas Ahmed",
        "Sheeba Samuel",
        "Kevin Coakley",
        "Birgitta Koenig-Ries",
        "Odd Erik Gundersen"
      ],
      "abstract": "To enhance the reproducibility and reliability of deep learning models, we address a critical gap in current training methodologies: the lack of mechanisms that ensure consistent and robust performance across runs. Our empirical analysis reveals that even under controlled initialization and training conditions, the accuracy of the model can exhibit significant variability. To address this issue, we propose a Custom Loss Function (CLF) that reduces the sensitivity of training outcomes to stochastic factors such as weight initialization and data shuffling. By fine-tuning its parameters, CLF explicitly balances predictive accuracy with training stability, leading to more consistent and reliable model performance. Extensive experiments across diverse architectures for both image classification and time series forecasting demonstrate that our approach significantly improves training robustness without sacrificing predictive performance. These results establish CLF as an effective and efficient strategy for developing more stable, reliable and trustworthy neural networks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹å½“å‰è®­ç»ƒæ–¹æ³•ä¸­ç¼ºä¹ç¡®ä¿è·¨è¿è¡Œä¸€è‡´æ€§ä¸ç¨³å¥æ€§èƒ½çš„æœºåˆ¶è¿™ä¸€å…³é”®ç¼ºé™·ã€‚å®è¯åˆ†ææ˜¾ç¤ºï¼Œå³ä½¿åœ¨å—æ§æ¡ä»¶ä¸‹ï¼Œæ¨¡å‹å‡†ç¡®æ€§ä»å—æƒé‡åˆå§‹åŒ–å’Œæ•°æ®æ´—ç‰Œ(data shuffling)ç­‰éšæœºå› ç´ (stochastic factors)å½±å“è€Œè¡¨ç°å‡ºæ˜¾è‘—æ³¢åŠ¨ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§è‡ªå®šä¹‰æŸå¤±å‡½æ•°(Custom Loss Function, CLF)ï¼Œæ—¨åœ¨é™ä½è®­ç»ƒç»“æœå¯¹è¿™äº›éšæœºå› ç´ çš„æ•æ„Ÿæ€§ã€‚é€šè¿‡å¾®è°ƒå…¶å‚æ•°ï¼ŒCLF æ˜¾å¼åœ°å¹³è¡¡äº†é¢„æµ‹å‡†ç¡®åº¦(predictive accuracy)ä¸è®­ç»ƒç¨³å®šæ€§(training stability)ï¼Œä»è€Œå®ç°äº†æ›´ä¸€è‡´ä¸”å¯é çš„æ¨¡å‹è¡¨ç°ã€‚åœ¨å›¾åƒåˆ†ç±»(image classification)å’Œæ—¶é—´åºåˆ—é¢„æµ‹(time series forecasting)ç­‰å¤šç§æ¶æ„ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸ç‰ºç‰²é¢„æµ‹æ€§èƒ½çš„å‰æä¸‹æ˜¾è‘—æå‡äº†è®­ç»ƒé²æ£’æ€§(robustness)ã€‚è¿™äº›ç ”ç©¶ç»“æœè¯æ˜äº† CLF æ˜¯å¼€å‘æ›´ç¨³å®šã€å¯é ä¸”å€¼å¾—ä¿¡èµ–çš„ç¥ç»ç½‘ç»œ(neural networks)çš„ä¸€ç§é«˜æ•ˆä¸”æœ‰æ•ˆçš„ç­–ç•¥ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00578v1",
      "published_date": "2026-01-02 05:31:08 UTC",
      "updated_date": "2026-01-02 05:31:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:21:26.486678+00:00"
    },
    {
      "arxiv_id": "2601.02409v1",
      "title": "Expert-Guided Explainable Few-Shot Learning with Active Sample Selection for Medical Image Analysis",
      "title_zh": "é¢å‘åŒ»å­¦å›¾åƒåˆ†æçš„ä¸“å®¶å¼•å¯¼å‹å¯è§£é‡Šå°æ ·æœ¬å­¦ä¹ ä¸ä¸»åŠ¨æ ·æœ¬é€‰æ‹©",
      "authors": [
        "Longwei Wang",
        "Ifrat Ikhtear Uddin",
        "KC Santosh"
      ],
      "abstract": "Medical image analysis faces two critical challenges: scarcity of labeled data and lack of model interpretability, both hindering clinical AI deployment. Few-shot learning (FSL) addresses data limitations but lacks transparency in predictions. Active learning (AL) methods optimize data acquisition but overlook interpretability of acquired samples. We propose a dual-framework solution: Expert-Guided Explainable Few-Shot Learning (EGxFSL) and Explainability-Guided AL (xGAL). EGxFSL integrates radiologist-defined regions-of-interest as spatial supervision via Grad-CAM-based Dice loss, jointly optimized with prototypical classification for interpretable few-shot learning. xGAL introduces iterative sample acquisition prioritizing both predictive uncertainty and attention misalignment, creating a closed-loop framework where explainability guides training and sample selection synergistically. On the BraTS (MRI), VinDr-CXR (chest X-ray), and SIIM-COVID-19 (chest X-ray) datasets, we achieve accuracies of 92\\%, 76\\%, and 62\\%, respectively, consistently outperforming non-guided baselines across all datasets. Under severe data constraints, xGAL achieves 76\\% accuracy with only 680 samples versus 57\\% for random sampling. Grad-CAM visualizations demonstrate guided models focus on diagnostically relevant regions, with generalization validated on breast ultrasound confirming cross-modality applicability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸“å®¶å¼•å¯¼çš„å¯è§£é‡Šå°æ ·æœ¬å­¦ä¹ (Expert-Guided Explainable Few-Shot Learning, EGxFSL)å’Œè§£é‡Šæ€§å¼•å¯¼çš„ä¸»åŠ¨å­¦ä¹ (Explainability-Guided AL, xGAL)åŒé‡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŒ»ç–—å›¾åƒåˆ†æä¸­æ•°æ®ç¨€ç¼ºå’Œæ¨¡å‹ç¼ºä¹å¯è§£é‡Šæ€§çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚EGxFSLé€šè¿‡å°†æ”¾å°„ç§‘åŒ»ç”Ÿå®šä¹‰çš„Regions-of-Interestä½œä¸ºç©ºé—´ç›‘ç£ï¼Œç»“åˆåŸºäºGrad-CAMçš„Dice losså’ŒåŸå‹åˆ†ç±»(Prototypical classification)ï¼Œå®ç°äº†é«˜åº¦å¯è§£é‡Šçš„Few-Shot Learningã€‚xGALå¼•å…¥äº†ä¸€ç§è¿­ä»£æ ·æœ¬è·å–æœºåˆ¶ï¼Œä¼˜å…ˆé€‰æ‹©å…·æœ‰é¢„æµ‹ä¸ç¡®å®šæ€§å’ŒAttention Misalignmentçš„æ ·æœ¬ï¼Œæ„å»ºäº†å¯è§£é‡Šæ€§å¼•å¯¼è®­ç»ƒä¸æ ·æœ¬é€‰æ‹©çš„é—­ç¯ç³»ç»Ÿã€‚åœ¨BraTSã€VinDr-CXRå’ŒSIIM-COVID-19æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åˆ†åˆ«è¾¾åˆ°äº†92%ã€76%å’Œ62%çš„å‡†ç¡®ç‡ï¼Œåœ¨æåº¦æ•°æ®å—é™çš„æƒ…å†µä¸‹è¡¨ç°æ˜¾è‘—ä¼˜äºéšæœºé‡‡æ ·åŸºçº¿ã€‚Grad-CAMå¯è§†åŒ–ç»“æœè¯å®å¼•å¯¼æ¨¡å‹èƒ½å¤Ÿç²¾å‡†èšç„¦äºå…·æœ‰è¯Šæ–­æ„ä¹‰çš„åŒºåŸŸï¼Œä¸”è¯¥æ–¹æ³•åœ¨ä¹³è…ºè¶…å£°æ•°æ®é›†ä¸Šçš„æˆåŠŸæ³›åŒ–è¿›ä¸€æ­¥è¯æ˜äº†å…¶è·¨æ¨¡æ€çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted for publication in IEEE Journal of Biomedical and Health Informatics, 2025",
      "pdf_url": "https://arxiv.org/pdf/2601.02409v1",
      "published_date": "2026-01-02 05:09:35 UTC",
      "updated_date": "2026-01-02 05:09:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:20:34.212251+00:00"
    },
    {
      "arxiv_id": "2601.00567v1",
      "title": "Improving Scientific Document Retrieval with Academic Concept Index",
      "title_zh": "åˆ©ç”¨å­¦æœ¯æ¦‚å¿µç´¢å¼•æå‡ç§‘å­¦æ–‡çŒ®æ£€ç´¢",
      "authors": [
        "Jeyun Lee",
        "Junhyoung Lee",
        "Wonbin Kweon",
        "Bowen Jin",
        "Yu Zhang",
        "Susik Yoon",
        "Dongha Lee",
        "Hwanjo Yu",
        "Jiawei Han",
        "Seongku Kang"
      ],
      "abstract": "Adapting general-domain retrievers to scientific domains is challenging due to the scarcity of large-scale domain-specific relevance annotations and the substantial mismatch in vocabulary and information needs. Recent approaches address these issues through two independent directions that leverage large language models (LLMs): (1) generating synthetic queries for fine-tuning, and (2) generating auxiliary contexts to support relevance matching. However, both directions overlook the diverse academic concepts embedded within scientific documents, often producing redundant or conceptually narrow queries and contexts. To address this limitation, we introduce an academic concept index, which extracts key concepts from papers and organizes them guided by an academic taxonomy. This structured index serves as a foundation for improving both directions. First, we enhance the synthetic query generation with concept coverage-based generation (CCQGen), which adaptively conditions LLMs on uncovered concepts to generate complementary queries with broader concept coverage. Second, we strengthen the context augmentation with concept-focused auxiliary contexts (CCExpand), which leverages a set of document snippets that serve as concise responses to the concept-aware CCQGen queries. Extensive experiments show that incorporating the academic concept index into both query generation and context augmentation leads to higher-quality queries, better conceptual alignment, and improved retrieval performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨æ£€ç´¢å™¨(Retrievers)åœ¨ç§‘å­¦é¢†åŸŸé€‚é…ä¸­å› å­¦æœ¯æ¦‚å¿µç¼ºå¤±å¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆï¼Œæå‡ºäº†ä¸€ç§å­¦æœ¯æ¦‚å¿µç´¢å¼•(Academic Concept Index)ï¼Œæ—¨åœ¨ä»è®ºæ–‡ä¸­æå–å…³é”®æ¦‚å¿µå¹¶ä¾æ®å­¦æœ¯åˆ†ç±»æ³•(Academic Taxonomy)è¿›è¡Œç»„ç»‡ã€‚åŸºäºæ­¤ç´¢å¼•ï¼Œç ”ç©¶è®¾è®¡äº†åŸºäºæ¦‚å¿µè¦†ç›–çš„æŸ¥è¯¢ç”ŸæˆæŠ€æœ¯(CCQGen)ï¼Œé€šè¿‡è‡ªé€‚åº”å¼•å¯¼å¤§è¯­è¨€æ¨¡å‹(LLMs)å…³æ³¨æœªè¦†ç›–çš„æ¦‚å¿µï¼Œç”Ÿæˆæ›´å…·è¡¥å……æ€§çš„é«˜è´¨é‡æŸ¥è¯¢ã€‚æ­¤å¤–ï¼Œç ”ç©¶åˆ©ç”¨æ¦‚å¿µèšç„¦çš„è¾…åŠ©ä¸Šä¸‹æ–‡å¢å¼º(CCExpand)ï¼Œç»“åˆæ–‡æ¡£ç‰‡æ®µè¿›ä¸€æ­¥å¼ºåŒ–äº†æ£€ç´¢çš„ç›¸å…³æ€§åŒ¹é…ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°†å­¦æœ¯æ¦‚å¿µç´¢å¼•æ•´åˆè¿›æŸ¥è¯¢ç”Ÿæˆä¸ä¸Šä¸‹æ–‡å¢å¼ºæµç¨‹ï¼Œèƒ½æ˜¾è‘—æå‡æ¦‚å¿µå¯¹é½ç¨‹åº¦ã€‚è¯¥æ–¹æ³•åœ¨å¤šé¡¹å®éªŒä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„ç§‘å­¦æ–‡æ¡£æ£€ç´¢(Scientific Document Retrieval)æ€§èƒ½ï¼Œæœ‰æ•ˆç¼“è§£äº†é¢†åŸŸå†…æ ‡æ³¨æ•°æ®ç¨€ç¼ºåŠè¯æ±‡ä¸åŒ¹é…çš„é—®é¢˜ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00567v1",
      "published_date": "2026-01-02 04:47:49 UTC",
      "updated_date": "2026-01-02 04:47:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:21:42.289092+00:00"
    },
    {
      "arxiv_id": "2601.00559v1",
      "title": "Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?",
      "title_zh": "ç ´è§£ç‰©è”ç½‘å®‰å…¨ï¼šå¤§è¯­è¨€æ¨¡å‹èƒ½å¦è¶…è¶Šé™æ€åˆ†æå·¥å…·ï¼Ÿ",
      "authors": [
        "Jason Quantrill",
        "Noura Khajehnouri",
        "Zihan Guo",
        "Manar H. Alalfi"
      ],
      "abstract": "Smart home IoT platforms such as openHAB rely on Trigger Action Condition (TAC) rules to automate device behavior, but the interplay among these rules can give rise to interaction threats, unintended or unsafe behaviors emerging from implicit dependencies, conflicting triggers, or overlapping conditions. Identifying these threats requires semantic understanding and structural reasoning that traditionally depend on symbolic, constraint-driven static analysis. This work presents the first comprehensive evaluation of Large Language Models (LLMs) across a multi-category interaction threat taxonomy, assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations. We benchmark Llama 3.1 8B, Llama 70B, GPT-4o, Gemini-2.5-Pro, and DeepSeek-R1 across zero-, one-, and two-shot settings, comparing their results against oHIT's manually validated ground truth. Our findings show that while LLMs exhibit promising semantic understanding, particularly on action- and condition-related threats, their accuracy degrades significantly for threats requiring cross-rule structural reasoning, especially under mutated rule forms. Model performance varies widely across threat categories and prompt settings, with no model providing consistent reliability. In contrast, the symbolic reasoning baseline maintains stable detection across both datasets, unaffected by rule rewrites or structural perturbations. These results underscore that LLMs alone are not yet dependable for safety critical interaction-threat detection in IoT environments. We discuss the implications for tool design and highlight the potential of hybrid architectures that combine symbolic analysis with LLM-based semantic interpretation to reduce false positives while maintaining structural rigor.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™ºèƒ½å®¶å±… IoT å¹³å°ï¼ˆå¦‚ openHABï¼‰ä¸­ç”± Trigger Action Condition (TAC) è§„åˆ™å¼•å‘çš„äº¤äº’å¨èƒ(interaction threats)è¿›è¡Œäº†è¯„ä¼°ã€‚ä½œè€…å¯¹ Llama 3.1ã€GPT-4oã€Gemini-2.5-Pro å’Œ DeepSeek-R1 ç­‰å¤šç§ Large Language Models (LLMs) åœ¨åŸå§‹æ•°æ®é›†å’Œç»è¿‡å˜å½¢å¤„ç†çš„ Mutation æ•°æ®é›†ä¸Šçš„è¡¨ç°è¿›è¡Œäº†å…¨é¢å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMs è™½ç„¶åœ¨åŠ¨ä½œå’Œæ¡ä»¶ç›¸å…³çš„è¯­ä¹‰ç†è§£ä¸Šå±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨é¢å¯¹éœ€è¦è·¨è§„åˆ™ç»“æ„åŒ–æ¨ç†(cross-rule structural reasoning)çš„å¨èƒæ—¶ï¼Œå…¶å‡†ç¡®ç‡åœ¨è§„åˆ™å½¢å¼å‘ç”Ÿå˜åŒ–åä¼šæ˜¾è‘—ä¸‹é™ã€‚ä¸ä¹‹ç›¸åï¼Œä¼ ç»Ÿçš„ç¬¦å·æ¨ç†(symbolic reasoning)åŸºå‡†åœ¨ä¸åŒæ•°æ®é›†ä¸‹å‡ä¿æŒäº†ç¨³å®šçš„æ£€æµ‹èƒ½åŠ›ï¼Œä¸å—è§„åˆ™é‡å†™æˆ–ç»“æ„æ‰°åŠ¨çš„å½±å“ã€‚ç ”ç©¶ç»“è®ºæŒ‡å‡ºï¼Œç°é˜¶æ®µ LLMs å°šä¸èƒ½ç‹¬ç«‹ã€å¯é åœ°è§£å†³ IoT ç¯å¢ƒä¸­å®‰å…¨å…³é”®çš„äº¤äº’å¨èƒæ£€æµ‹é—®é¢˜ã€‚è®ºæ–‡æœ€åå»ºè®®ï¼Œæœªæ¥çš„å·¥å…·è®¾è®¡åº”å…³æ³¨ç»“åˆç¬¦å·åˆ†æä¸ LLM è¯­ä¹‰è§£é‡Šçš„æ··åˆæ¶æ„ï¼Œä»¥åœ¨ç¡®ä¿ç»“æ„ä¸¥è°¨æ€§çš„åŒæ—¶æå‡æ£€æµ‹ç²¾åº¦å¹¶å‡å°‘è¯¯æŠ¥ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00559v1",
      "published_date": "2026-01-02 04:17:36 UTC",
      "updated_date": "2026-01-02 04:17:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:21:43.378823+00:00"
    },
    {
      "arxiv_id": "2601.00553v1",
      "title": "A Comprehensive Dataset for Human vs. AI Generated Image Detection",
      "title_zh": "äººç±»ä¸AIç”Ÿæˆå›¾åƒæ£€æµ‹ç»¼åˆæ•°æ®é›†",
      "authors": [
        "Rajarshi Roy",
        "Nasrin Imanpour",
        "Ashhar Aziz",
        "Shashwat Bajpai",
        "Gurpreet Singh",
        "Shwetangshu Biswas",
        "Kapil Wanaskar",
        "Parth Patwa",
        "Subhankar Ghosh",
        "Shreyas Dixit",
        "Nilesh Ranjan Pal",
        "Vipula Rawte",
        "Ritvik Garimella",
        "Gaytri Jena",
        "Vasu Sharma",
        "Vinija Jain",
        "Aman Chadha",
        "Aishwarya Naresh Reganti",
        "Amitava Das"
      ],
      "abstract": "Multimodal generative AI systems like Stable Diffusion, DALL-E, and MidJourney have fundamentally changed how synthetic images are created. These tools drive innovation but also enable the spread of misleading content, false information, and manipulated media. As generated images become harder to distinguish from photographs, detecting them has become an urgent priority. To combat this challenge, We release MS COCOAI, a novel dataset for AI generated image detection consisting of 96000 real and synthetic datapoints, built using the MS COCO dataset. To generate synthetic images, we use five generators: Stable Diffusion 3, Stable Diffusion 2.1, SDXL, DALL-E 3, and MidJourney v6. Based on the dataset, we propose two tasks: (1) classifying images as real or generated, and (2) identifying which model produced a given synthetic image. The dataset is available at https://huggingface.co/datasets/Rajarshi-Roy-research/Defactify_Image_Dataset.",
      "tldr_zh": "éšç€ Stable Diffusionã€DALL-E å’Œ MidJourney ç­‰å¤šæ¨¡æ€ç”Ÿæˆå¼ AI ç³»ç»Ÿç”Ÿæˆå›¾åƒçš„çœŸå®æ„Ÿä¸æ–­æå‡ï¼Œè¯†åˆ«åˆæˆå›¾åƒå·²æˆä¸ºåº”å¯¹è™šå‡ä¿¡æ¯ä¼ æ’­çš„ç´§è¿«ä»»åŠ¡ã€‚è¯¥ç ”ç©¶æ¨å‡ºäº†åä¸º MS COCOAI çš„æ–°å‹å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ—¨åœ¨ä¿ƒè¿› AI ç”Ÿæˆå›¾åƒæ£€æµ‹æŠ€æœ¯çš„å‘å±•ã€‚è¯¥æ•°æ®é›†åŸºäº MS COCO æ„å»ºï¼ŒåŒ…å« 96,000 æ¡çœŸå®ä¸åˆæˆæ•°æ®ç‚¹ï¼Œå…¶ä¸­åˆæˆå›¾åƒç”± Stable Diffusion 3ã€Stable Diffusion 2.1ã€SDXLã€DALL-E 3 å’Œ MidJourney v6 è¿™äº”ç§ä¸»æµç”Ÿæˆå™¨åˆ¶ä½œè€Œæˆã€‚ç ”ç©¶è€…åŸºäºæ­¤æ•°æ®é›†æå‡ºäº†ä¸¤é¡¹å…³é”®ä»»åŠ¡ï¼šä¸€æ˜¯åŒºåˆ†å›¾åƒä¸ºçœŸå®æˆ–ç”Ÿæˆçš„äºŒåˆ†ç±»ä»»åŠ¡ï¼ŒäºŒæ˜¯è¯†åˆ«å…·ä½“ç”Ÿæˆæ¨¡å‹çš„å½’å› ä»»åŠ¡ã€‚ç›®å‰ï¼ŒMS COCOAI æ•°æ®é›†å·²åœ¨ Hugging Face å¹³å°å¼€æºï¼Œä¸ºå­¦æœ¯ç•Œæä¾›äº†å¤šæ ·åŒ–ä¸”å…¨é¢çš„åŸºå‡†æµ‹è¯•èµ„æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00553v1",
      "published_date": "2026-01-02 03:58:18 UTC",
      "updated_date": "2026-01-02 03:58:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:21:59.442927+00:00"
    },
    {
      "arxiv_id": "2601.00549v1",
      "title": "CoCo-Fed: A Unified Framework for Memory- and Communication-Efficient Federated Learning at the Wireless Edge",
      "title_zh": "CoCo-Fedï¼šé¢å‘æ— çº¿è¾¹ç¼˜çš„é«˜æ•ˆå†…å­˜ä¸é€šä¿¡ç»Ÿä¸€è”é‚¦å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Zhiheng Guo",
        "Zhaoyang Liu",
        "Zihan Cen",
        "Chenyuan Feng",
        "Xinghua Sun",
        "Xiang Chen",
        "Tony Q. S. Quek",
        "Xijun Wang"
      ],
      "abstract": "The deployment of large-scale neural networks within the Open Radio Access Network (O-RAN) architecture is pivotal for enabling native edge intelligence. However, this paradigm faces two critical bottlenecks: the prohibitive memory footprint required for local training on resource-constrained gNBs, and the saturation of bandwidth-limited backhaul links during the global aggregation of high-dimensional model updates. To address these challenges, we propose CoCo-Fed, a novel Compression and Combination-based Federated learning framework that unifies local memory efficiency and global communication reduction. Locally, CoCo-Fed breaks the memory wall by performing a double-dimension down-projection of gradients, adapting the optimizer to operate on low-rank structures without introducing additional inference parameters/latency. Globally, we introduce a transmission protocol based on orthogonal subspace superposition, where layer-wise updates are projected and superimposed into a single consolidated matrix per gNB, drastically reducing the backhaul traffic. Beyond empirical designs, we establish a rigorous theoretical foundation, proving the convergence of CoCo-Fed even under unsupervised learning conditions suitable for wireless sensing tasks. Extensive simulations on an angle-of-arrival estimation task demonstrate that CoCo-Fed significantly outperforms state-of-the-art baselines in both memory and communication efficiency while maintaining robust convergence under non-IID settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨å¼€æ”¾æ— çº¿æ¥å…¥ç½‘ (Open Radio Access Network, O-RAN) æ¶æ„ä¸‹éƒ¨ç½²å¤§è§„æ¨¡ç¥ç»ç½‘ç»œæ—¶ï¼Œèµ„æºå—é™çš„ gNB å†…å­˜å ç”¨è¿‡é«˜ä»¥åŠå…¨çƒèšåˆè¿‡ç¨‹ä¸­å›ä¼ é“¾è·¯å¸¦å®½é¥±å’Œçš„é—®é¢˜ï¼Œæå‡ºäº† CoCo-Fed è¿™ä¸€åŸºäºå‹ç¼©ä¸ç»„åˆçš„è”é‚¦å­¦ä¹  (Federated learning) ç»Ÿä¸€æ¡†æ¶ã€‚åœ¨æœ¬åœ°è®­ç»ƒä¸­ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¯¹æ¢¯åº¦è¿›è¡ŒåŒç»´é™æŠ•å½± (double-dimension down-projection)ï¼Œä½¿ä¼˜åŒ–å™¨èƒ½å¤Ÿè¿è¡Œåœ¨ä½ç§©ç»“æ„ (low-rank structures) ä¸Šï¼Œä»è€Œåœ¨ä¸å¢åŠ æ¨ç†å»¶è¿Ÿçš„å‰æä¸‹çªç ´å†…å­˜ç“¶é¢ˆã€‚åœ¨å…¨çƒé€šä¿¡æ–¹é¢ï¼ŒCoCo-Fed å¼•å…¥äº†åŸºäºæ­£äº¤å­ç©ºé—´å åŠ  (orthogonal subspace superposition) çš„ä¼ è¾“åè®®ï¼Œå°†å±‚çº§æ›´æ–°æ•´åˆä¸ºå•ä¸ªçŸ©é˜µï¼Œæå¤§åœ°å‡å°‘äº†å›ä¼ æµé‡ã€‚ç ”ç©¶è¿˜ä¸ºè¯¥æ¡†æ¶æä¾›äº†ä¸¥è°¨çš„ç†è®ºåŸºç¡€ï¼Œè¯æ˜äº†å…¶åœ¨æ— çº¿æ„ŸçŸ¥ä»»åŠ¡çš„æ— ç›‘ç£å­¦ä¹ æ¡ä»¶ä¸‹ä»å…·æœ‰æ”¶æ•›æ€§ã€‚åœ¨åˆ°è¾¾è§’ (angle-of-arrival) ä¼°è®¡ä»»åŠ¡çš„ä»¿çœŸå®éªŒä¸­ï¼ŒCoCo-Fed åœ¨å†…å­˜å’Œé€šä¿¡æ•ˆç‡æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ï¼Œå¹¶åœ¨éç‹¬ç«‹åŒåˆ†å¸ƒ (non-IID) è®¾ç½®ä¸‹ä¿æŒäº†ç¨³å¥çš„æ”¶æ•›æ€§èƒ½ã€‚",
      "categories": [
        "cs.IT",
        "cs.AI"
      ],
      "primary_category": "cs.IT",
      "comment": "7 pages, 3 figures, 1 algorithm",
      "pdf_url": "https://arxiv.org/pdf/2601.00549v1",
      "published_date": "2026-01-02 03:39:50 UTC",
      "updated_date": "2026-01-02 03:39:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:22:48.219430+00:00"
    },
    {
      "arxiv_id": "2601.00543v1",
      "title": "ECR: Manifold-Guided Semantic Cues for Compact Language Models",
      "title_zh": "ECRï¼šé¢å‘ç´§å‡‘å‹è¯­è¨€æ¨¡å‹çš„æµå½¢å¼•å¯¼è¯­ä¹‰çº¿ç´¢",
      "authors": [
        "Chung-Wei Victor Yuan"
      ],
      "abstract": "Compact models often lose the structure of their embedding space. The issue shows up when the capacity is tight or the data spans several languages. Such collapse makes it difficult for downstream tasks to build on the resulting representation. Existing compression methods focus on aligning model outputs at a superficial level but fail to preserve the underlying manifold structure. This mismatch often leads to semantic drift in the compact model, causing both task behavior and linguistic properties to deviate from the reference model.\n  To address those issues, we provide a new framework called Embedding Consistency Regulation (ECR). This framework first derives a set of semantic anchors from teacher embeddings (computed once offline). Then, the compact model learns to maintain consistent geometry around these anchors, without relying on matching logits or internal features. ECR adds only a small projection step at inference, without altering the decoding architecture or its runtime behavior.\n  In experiments on a 100K multilingual corpus, ECR consistently stabilizes training and preserves semantic structure across tasks and languages. It also produces a more compact and task-aligned representation space, enabling low-capacity models to learn cleaner manifolds than conventional baselines. ECR works without teacher outputs and is compatible with, but independent of, distillation. Taken together, our results show that ECR helps compact models better follow task requirements and makes them easier to deploy under strict efficiency or privacy limits.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Embedding Consistency Regulation (ECR)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³Compact modelsåœ¨å®¹é‡æœ‰é™æˆ–å¤šè¯­è¨€ç¯å¢ƒä¸‹å®¹æ˜“ä¸¢å¤±Embeddingç©ºé—´ç»“æ„å¹¶å¯¼è‡´Semantic driftçš„é—®é¢˜ã€‚ECRé€šè¿‡ä»Teacher embeddingsä¸­æå–Semantic anchorsï¼Œå¼•å¯¼ç´§å‡‘æ¨¡å‹åœ¨è¿™äº›é”šç‚¹å‘¨å›´ä¿æŒä¸€è‡´çš„å‡ ä½•Manifold structureï¼Œè€Œæ— éœ€ä¾èµ–Logitsæˆ–å†…éƒ¨ç‰¹å¾åŒ¹é…ã€‚åœ¨Inferenceé˜¶æ®µï¼Œè¯¥æ¡†æ¶ä»…å¢åŠ äº†ä¸€ä¸ªå¾®å°çš„æŠ•å½±æ­¥éª¤ï¼Œä¸”ä¸æ”¹å˜åŸæœ‰çš„è§£ç æ¶æ„æˆ–è¿è¡Œè¡Œä¸ºã€‚åœ¨100Kå¤šè¯­è¨€è¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒECRåœ¨ä¸åŒä»»åŠ¡å’Œè¯­è¨€é—´èƒ½æœ‰æ•ˆç¨³å®šè®­ç»ƒå¹¶ä¿ç•™è¯­ä¹‰ç»“æ„ã€‚ç›¸æ¯”ä¼ ç»ŸåŸºçº¿æ–¹æ³•ï¼Œè¯¥æ¡†æ¶ä½¿ä½å®¹é‡æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´æ¸…æ™°ä¸”ä»»åŠ¡å¯¹é½çš„è¡¨å¾ç©ºé—´ï¼Œä¸ºåœ¨ä¸¥æ ¼çš„æ•ˆç‡æˆ–éšç§é™åˆ¶ä¸‹éƒ¨ç½²é«˜æ€§èƒ½æ¨¡å‹æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint 13pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.00543v1",
      "published_date": "2026-01-02 03:16:24 UTC",
      "updated_date": "2026-01-02 03:16:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:22:01.765906+00:00"
    },
    {
      "arxiv_id": "2601.00930v1",
      "title": "AlignUSER: Human-Aligned LLM Agents via World Models for Recommender System Evaluation",
      "title_zh": "AlignUSERï¼šåŸºäºä¸–ç•Œæ¨¡å‹ç”¨äºæ¨èç³»ç»Ÿè¯„ä¼°çš„äººç±»å¯¹é½å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“",
      "authors": [
        "Nicolas Bougie",
        "Gian Maria Marconi",
        "Tony Yip",
        "Narimasa Watanabe"
      ],
      "abstract": "Evaluating recommender systems remains challenging due to the gap between offline metrics and real user behavior, as well as the scarcity of interaction data. Recent work explores large language model (LLM) agents as synthetic users, yet they typically rely on few-shot prompting, which yields a shallow understanding of the environment and limits their ability to faithfully reproduce user actions. We introduce AlignUSER, a framework that learns world-model-driven agents from human interactions. Given rollout sequences of actions and states, we formalize world modeling as a next state prediction task that helps the agent internalize the environment. To align actions with human personas, we generate counterfactual trajectories around demonstrations and prompt the LLM to compare its decisions with human choices, identify suboptimal actions, and extract lessons. The learned policy is then used to drive agent interactions with the recommender system. We evaluate AlignUSER across multiple datasets and demonstrate closer alignment with genuine humans than prior work, both at the micro and macro levels.",
      "tldr_zh": "é’ˆå¯¹æ¨èç³»ç»Ÿ(Recommender Systems)è¯„ä¼°ä¸­è„±æœºæŒ‡æ ‡ä¸çœŸå®ç”¨æˆ·è¡Œä¸ºä¸ç¬¦ä»¥åŠäº¤äº’æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†AlignUSERæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸–ç•Œæ¨¡å‹(World Models)æ„å»ºä¸äººç±»è¡Œä¸ºé«˜åº¦ä¸€è‡´çš„å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¾èµ–å°‘æ ·æœ¬æç¤º(Few-shot Prompting)å¯¼è‡´ç¯å¢ƒç†è§£ä¸è¶³çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶å°†ä¸–ç•Œå»ºæ¨¡å½¢å¼åŒ–ä¸ºä¸‹ä¸€çŠ¶æ€é¢„æµ‹(Next State Prediction)ä»»åŠ¡ï¼Œä½¿æ™ºèƒ½ä½“èƒ½æœ‰æ•ˆå†…åŒ–ç¯å¢ƒä¿¡æ¯ã€‚ä¸ºäº†å®ç°è§’è‰²å¯¹é½ï¼ŒAlignUSERé€šè¿‡ç”Ÿæˆåäº‹å®è½¨è¿¹(Counterfactual Trajectories)å¼•å¯¼æ™ºèƒ½ä½“å¯¹æ¯”è‡ªèº«å†³ç­–ä¸çœŸå®äººç±»é€‰æ‹©ï¼Œä»è€Œè¯†åˆ«æ¬¡ä¼˜è¡Œä¸ºå¹¶æå–å­¦ä¹ ç»éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAlignUSERé©±åŠ¨çš„æ™ºèƒ½ä½“åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºä¸çœŸå®ç”¨æˆ·åœ¨å¾®è§‚å’Œå®è§‚å±‚é¢çš„é«˜åº¦ä¸€è‡´æ€§ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00930v1",
      "published_date": "2026-01-02 03:01:33 UTC",
      "updated_date": "2026-01-02 03:01:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:22:18.507630+00:00"
    },
    {
      "arxiv_id": "2601.00538v1",
      "title": "Parametrized Sharing for Multi-Agent Hybrid DRL for Multiple Multi-Functional RISs-Aided Downlink NOMA Networks",
      "title_zh": "é¢å‘å¤šä¸ªå¤šåŠŸèƒ½RISè¾…åŠ©ä¸‹è¡ŒNOMAç½‘ç»œçš„å¤šæ™ºèƒ½ä½“æ··åˆæ·±åº¦å¼ºåŒ–å­¦ä¹ å‚æ•°åŒ–å…±äº«",
      "authors": [
        "Chi-Te Kuo",
        "Li-Hsiang Shen",
        "Jyun-Jhe Huang"
      ],
      "abstract": "Multi-functional reconfigurable intelligent surface (MF-RIS) is conceived to address the communication efficiency thanks to its extended signal coverage from its active RIS capability and self-sustainability from energy harvesting (EH). We investigate the architecture of multi-MF-RISs to assist non-orthogonal multiple access (NOMA) downlink networks. We formulate an energy efficiency (EE) maximization problem by optimizing power allocation, transmit beamforming and MF-RIS configurations of amplitudes, phase-shifts and EH ratios, as well as the position of MF-RISs, while satisfying constraints of available power, user rate requirements, and self-sustainability property. We design a parametrized sharing scheme for multi-agent hybrid deep reinforcement learning (PMHRL), where the multi-agent proximal policy optimization (PPO) and deep-Q network (DQN) handle continuous and discrete variables, respectively. The simulation results have demonstrated that proposed PMHRL has the highest EE compared to other benchmarks, including cases without parametrized sharing, pure PPO and DQN. Moreover, the proposed multi-MF-RISs-aided downlink NOMA achieves the highest EE compared to scenarios of no-EH/amplification, traditional RISs, and deployment without RISs/MF-RISs under different multiple access.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤šåŠŸèƒ½å¯é‡æ„æ™ºèƒ½è¡¨é¢ (MF-RIS) è¾…åŠ©çš„ä¸‹è¡Œéæ­£äº¤å¤šå€æ¥å…¥ (NOMA) ç½‘ç»œä¸­ï¼Œå¦‚ä½•åˆ©ç”¨æœ‰æº RIS ä¿¡å·è¦†ç›–æ‰©å±•ä¸èƒ½é‡é‡‡é›† (EH) è‡ªæŒç»­ç‰¹æ€§æå‡é€šä¿¡æ•ˆç‡ã€‚ä½œè€…æ„å»ºäº†ä¸€ä¸ªèƒ½æ•ˆ (EE) æœ€å¤§åŒ–é—®é¢˜ï¼Œæ—¨åœ¨è”åˆä¼˜åŒ–åŠŸç‡åˆ†é…ã€å‘å°„æ³¢æŸèµ‹å½¢ã€MF-RIS çš„æŒ¯å¹…ã€ç›¸ç§»ã€èƒ½é‡é‡‡é›†æ¯”ä¾‹ä»¥åŠéƒ¨ç½²ä½ç½®ï¼Œå¹¶æ»¡è¶³ç”¨æˆ·é€Ÿç‡éœ€æ±‚ä¸è‡ªæŒç»­æ€§çº¦æŸã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå‚æ•°åŒ–å…±äº«çš„å¤šæ™ºèƒ½ä½“æ··åˆæ·±åº¦å¼ºåŒ–å­¦ä¹  (PMHRL) æ–¹æ¡ˆï¼Œåˆ©ç”¨å¤šæ™ºèƒ½ä½“è¿‘ç«¯ç­–ç•¥ä¼˜åŒ– (PPO) å¤„ç†è¿ç»­å˜é‡ï¼Œå¹¶ç»“åˆæ·±åº¦Qç½‘ç»œ (DQN) ä¼˜åŒ–ç¦»æ•£å˜é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPMHRL åœ¨èƒ½æ•ˆè¡¨ç°ä¸Šæ˜¾è‘—ä¼˜äºæ— å‚æ•°å…±äº«ã€çº¯ PPO æˆ–çº¯ DQN ç­‰åŸºå‡†ç®—æ³•ã€‚æ­¤å¤–ï¼Œå¤š MF-RIS è¾…åŠ©çš„ NOMA ç³»ç»Ÿåœ¨å¤šç§å¤šå€æ¥å…¥åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºæ¯”ä¼ ç»Ÿ RIS æˆ–æ—  RIS æ¶æ„æ›´é«˜çš„èƒ½æ•ˆï¼Œè¯æ˜äº†è¯¥æ–¹æ¡ˆåœ¨æå‡æœªæ¥ç½‘ç»œæ€§èƒ½æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00538v1",
      "published_date": "2026-01-02 02:44:30 UTC",
      "updated_date": "2026-01-02 02:44:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:22:43.514831+00:00"
    },
    {
      "arxiv_id": "2601.00928v1",
      "title": "Analyzing the Shopping Journey: Computing Shelf Browsing Visits in a Physical Retail Store",
      "title_zh": "è´­ç‰©æ—…ç¨‹åˆ†æï¼šå®ä½“é›¶å”®åº—ä¸­è´§æ¶æµè§ˆè¡Œä¸ºçš„è®¡ç®—",
      "authors": [
        "Luis Yoichi Morales",
        "Francesco Zanlungo",
        "David M. Woollard"
      ],
      "abstract": "Motivated by recent challenges in the deployment of robots into customer-facing roles within retail, this work introduces a study of customer activity in physical stores as a step toward autonomous understanding of shopper intent. We introduce an algorithm that computes shoppers' ``shelf visits'' -- capturing their browsing behavior in the store. Shelf visits are extracted from trajectories obtained via machine vision-based 3D tracking and overhead cameras. We perform two independent calibrations of the shelf visit algorithm, using distinct sets of trajectories (consisting of 8138 and 15129 trajectories), collected in different stores and labeled by human reviewers. The calibrated models are then evaluated on trajectories held out of the calibration process both from the same store on which calibration was performed and from the other store. An analysis of the results shows that the algorithm can recognize customers' browsing activity when evaluated in an environment different from the one on which calibration was performed. We then use the model to analyze the customers' ``browsing patterns'' on a large set of trajectories and their relation to actual purchases in the stores. Finally, we discuss how shelf browsing information could be used for retail planning and in the domain of human-robot interaction scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨æ¨åŠ¨é›¶å”®é¢†åŸŸæœºå™¨äººè‡ªä¸»ç†è§£é¡¾å®¢æ„å›¾ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºè®¡ç®—å®ä½“é›¶å”®åº—é¡¾å®¢â€œè´§æ¶è®¿é—®â€(shelf visits)çš„ç®—æ³•ã€‚è¯¥ç®—æ³•é€šè¿‡åŸºäºæœºå™¨è§†è§‰(machine vision-based)çš„3Dè¿½è¸ªæŠ€æœ¯å’Œé¡¶ç½®æ‘„åƒå¤´è·å–çš„é¡¾å®¢è½¨è¿¹æ¥æå–æµè§ˆè¡Œä¸ºæ•°æ®ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨åœ¨ä¸åŒå•†åº—æ”¶é›†å¹¶ç»äººå·¥æ ‡æ³¨çš„ä¸¤ä¸ªç‹¬ç«‹è½¨è¿¹æ•°æ®é›†ï¼ˆåˆ†åˆ«åŒ…å«8138å’Œ15129æ¡è½¨è¿¹ï¼‰å¯¹æ¨¡å‹è¿›è¡Œäº†æ ¡å‡†ä¸éªŒè¯ã€‚å®éªŒåˆ†æè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨æœªå‚ä¸æ ¡å‡†çš„æ–°åº—ç¯å¢ƒä¸‹ä¾ç„¶èƒ½å¤Ÿå‡†ç¡®è¯†åˆ«é¡¾å®¢çš„æµè§ˆæ´»åŠ¨ï¼Œå±•ç°äº†è‰¯å¥½çš„è·¨ç¯å¢ƒæ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡è¯¥æ¨¡å‹ï¼Œç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†å¤§è§„æ¨¡è½¨è¿¹ä¸­çš„â€œæµè§ˆæ¨¡å¼â€(browsing patterns)åŠå…¶ä¸å®é™…è´­ä¹°è¡Œä¸ºä¹‹é—´çš„å…³ç³»ã€‚æœ€åï¼Œè¯¥ç ”ç©¶æ¢è®¨äº†è´§æ¶æµè§ˆä¿¡æ¯åœ¨é›¶å”®è§„åˆ’åŠäººæœºäº¤äº’(human-robot interaction)åœºæ™¯ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œä¸ºæœªæ¥é›¶å”®æ™ºèƒ½è¾…åŠ©æŠ€æœ¯å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00928v1",
      "published_date": "2026-01-02 01:40:12 UTC",
      "updated_date": "2026-01-02 01:40:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:22:51.580897+00:00"
    },
    {
      "arxiv_id": "2601.00525v1",
      "title": "Optimizing LSTM Neural Networks for Resource-Constrained Retail Sales Forecasting: A Model Compression Study",
      "title_zh": "é’ˆå¯¹èµ„æºå—é™é›¶å”®é”€å”®é¢„æµ‹çš„LSTMç¥ç»ç½‘ç»œä¼˜åŒ–ï¼šæ¨¡å‹å‹ç¼©ç ”ç©¶",
      "authors": [
        "Ravi Teja Pagidoju"
      ],
      "abstract": "Standard LSTM(Long Short-Term Memory) neural networks provide accurate predictions for sales data in the retail industry, but require a lot of computing power. It can be challenging especially for mid to small retail industries. This paper examines LSTM model compression by gradually reducing the number of hidden units from 128 to 16. We used the Kaggle Store Item Demand Forecasting dataset, which has 913,000 daily sales records from 10 stores and 50 items, to look at the trade-off between model size and how accurate the predictions are. Experiments show that lowering the number of hidden LSTM units to 64 maintains the same level of accuracy while also improving it. The mean absolute percentage error (MAPE) ranges from 23.6% for the full 128-unit model to 12.4% for the 64-unit model. The optimized model is 73% smaller (from 280KB to 76KB) and 47% more accurate. These results show that larger models do not always achieve better results.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹èµ„æºå—é™çš„é›¶å”®é”€å”®é¢„æµ‹åœºæ™¯ï¼Œæ¢è®¨äº† LSTM ç¥ç»ç½‘ç»œçš„æ¨¡å‹å‹ç¼©ä¼˜åŒ–æ–¹æ³•ã€‚ç ”ç©¶é€šè¿‡é€æ­¥å°†éšè—å•å…ƒ(hidden units)æ•°é‡ä» 128 ä¸ªå‡å°‘åˆ° 16 ä¸ªï¼Œæ·±å…¥åˆ†æäº†æ¨¡å‹è§„æ¨¡ä¸é¢„æµ‹å‡†ç¡®åº¦ä¹‹é—´çš„æƒè¡¡å…³ç³»ã€‚å®éªŒé‡‡ç”¨äº†åŒ…å« 913,000 æ¡é”€å”®è®°å½•çš„ Kaggle Store Item Demand Forecasting æ•°æ®é›†ï¼Œç»“æœå‘ç°å°†éšè—å•å…ƒå‡å°‘è‡³ 64 ä¸ªæ—¶æ•ˆæœæœ€ä¼˜ã€‚åœ¨è¿™ç§é…ç½®ä¸‹ï¼Œå¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®(MAPE)ä» 128 å•å…ƒæ¨¡å‹çš„ 23.6% æ˜¾è‘—é™ä½è‡³ 12.4%ã€‚ä¼˜åŒ–åçš„æ¨¡å‹ä½“ç§¯ç¼©å°äº† 73%ï¼ˆç”± 280KB é™è‡³ 76KBï¼‰ï¼Œå‡†ç¡®åº¦ç›¸æ¯”åŸæ¨¡å‹æå‡äº† 47%ã€‚è¯¥ç ”ç©¶è¯æ˜äº†è¾ƒå¤§çš„æ¨¡å‹å¹¶ä¸æ€»æ˜¯èƒ½è·å¾—æ›´å¥½çš„ç»“æœï¼Œä¸ºä¸­å°å‹é›¶å”®ä¸šæä¾›äº†é«˜æ•ˆä¸”ä½è®¡ç®—æˆæœ¬çš„é¢„æµ‹æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to IEEE ICUIS 2025 (International Conference on Ubiquitous and Intelligent Systems). 5 pages, 3 figures, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2601.00525v1",
      "published_date": "2026-01-02 01:35:49 UTC",
      "updated_date": "2026-01-02 01:35:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:22:45.283152+00:00"
    },
    {
      "arxiv_id": "2601.00521v1",
      "title": "Probability-Aware Parking Selection",
      "title_zh": "æ¦‚ç‡æ„ŸçŸ¥åœè½¦ç‚¹é€‰æ‹©",
      "authors": [
        "Cameron Hickert",
        "Sirui Li",
        "Zhengbing He",
        "Cathy Wu"
      ],
      "abstract": "Current parking navigation systems often underestimate total travel time by failing to account for the time spent searching for a parking space, which significantly affects user experience, mode choice, congestion, and emissions. To address this issue, this paper introduces the probability-aware parking selection problem, which aims to direct drivers to the best parking location rather than straight to their destination. An adaptable dynamic programming framework is proposed for decision-making based on probabilistic information about parking availability at the parking lot level. Closed-form analysis determines when it is optimal to target a specific parking lot or explore alternatives, as well as the expected time cost. Sensitivity analysis and three illustrative cases are examined, demonstrating the model's ability to account for the dynamic nature of parking availability. Acknowledging the financial costs of permanent sensing infrastructure, the paper provides analytical and empirical assessments of errors incurred when leveraging stochastic observations to estimate parking availability. Experiments with real-world data from the US city of Seattle indicate this approach's viability, with mean absolute error decreasing from 7% to below 2% as observation frequency grows. In data-based simulations, probability-aware strategies demonstrate time savings up to 66% relative to probability-unaware baselines, yet still take up to 123% longer than direct-to-destination estimates.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åœè½¦å¯¼èˆªç³»ç»Ÿå› å¿½è§†å¯»æ‰¾è½¦ä½æ—¶é—´è€Œä½ä¼°æ€»å‡ºè¡Œæ—¶é—´çš„é—®é¢˜ï¼Œæå‡ºäº†æ¦‚ç‡æ„ŸçŸ¥åœè½¦é€‰æ‹©(Probability-Aware Parking Selection)é—®é¢˜ï¼Œæ—¨åœ¨å°†é©¾é©¶å‘˜å¼•å¯¼è‡³æœ€ä½³åœè½¦ä½ç½®è€Œéç›´æ¥å‰å¾€ç›®çš„åœ°ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”çš„åŠ¨æ€è§„åˆ’(Dynamic Programming)æ¡†æ¶ï¼Œåˆ©ç”¨åœè½¦åœºå±‚é¢çš„å¯ç”¨æ€§æ¦‚ç‡ä¿¡æ¯è¿›è¡Œå†³ç­–ã€‚ç ”ç©¶é€šè¿‡é—­å¼åˆ†æ(Closed-form Analysis)ç¡®å®šäº†é’ˆå¯¹ç‰¹å®šåœè½¦åœºæˆ–æ¢ç´¢æ›¿ä»£æ–¹æ¡ˆçš„æœ€ä¼˜æ—¶æœºï¼Œå¹¶è®¡ç®—äº†é¢„æœŸçš„æ€»æ—¶é—´æˆæœ¬ã€‚é’ˆå¯¹æ°¸ä¹…æ€§ä¼ æ„ŸåŸºç¡€è®¾æ–½çš„é«˜æ˜‚æˆæœ¬ï¼Œè®ºæ–‡è¿˜è¯„ä¼°äº†åˆ©ç”¨éšæœºè§‚æµ‹(Stochastic Observations)ä¼°è®¡åœè½¦å¯ç”¨æ€§æ—¶çš„è¯¯å·®ã€‚åŸºäºè¥¿é›…å›¾çœŸå®æ•°æ®çš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ï¼Œå…¶å¹³å‡ç»å¯¹è¯¯å·®éšè§‚æµ‹é¢‘ç‡å¢åŠ å¯é™è‡³2%ä»¥ä¸‹ã€‚åœ¨åŸºäºæ•°æ®çš„ä»¿çœŸä¸­ï¼Œæ¦‚ç‡æ„ŸçŸ¥ç­–ç•¥ç›¸æ¯”äºå¿½ç•¥æ¦‚ç‡çš„åŸºçº¿æ–¹æ¡ˆå®ç°äº†é«˜è¾¾66%çš„æ—¶é—´èŠ‚çœã€‚å°½ç®¡ç ”ç©¶æŒ‡å‡ºæ€»ç”¨æ—¶ä»ä¼šé•¿äºç†æƒ³åŒ–çš„ç›´æ¥åˆ°è¾¾ä¼°è®¡ï¼Œä½†è¯¥æ¨¡å‹èƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ åœè½¦å¯ç”¨æ€§çš„åŠ¨æ€å˜åŒ–å¹¶ä¼˜åŒ–å‡ºè¡Œæ•ˆç‡ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "eess.SY",
      "comment": "10 pages, 6 figures, 3 tables. To be published in IEEE Transactions on Intelligent Transportation Systems",
      "pdf_url": "https://arxiv.org/pdf/2601.00521v1",
      "published_date": "2026-01-02 01:13:47 UTC",
      "updated_date": "2026-01-02 01:13:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:22:50.527876+00:00"
    },
    {
      "arxiv_id": "2601.00927v1",
      "title": "Measuring Social Media Polarization Using Large Language Models and Heuristic Rules",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹ä¸å¯å‘å¼è§„åˆ™çš„ç¤¾äº¤åª’ä½“æåŒ–æµ‹é‡",
      "authors": [
        "Jawad Chowdhury",
        "Rezaur Rashid",
        "Gabriel Terejanu"
      ],
      "abstract": "Understanding affective polarization in online discourse is crucial for evaluating the societal impact of social media interactions. This study presents a novel framework that leverages large language models (LLMs) and domain-informed heuristics to systematically analyze and quantify affective polarization in discussions on divisive topics such as climate change and gun control. Unlike most prior approaches that relied on sentiment analysis or predefined classifiers, our method integrates LLMs to extract stance, affective tone, and agreement patterns from large-scale social media discussions. We then apply a rule-based scoring system capable of quantifying affective polarization even in small conversations consisting of single interactions, based on stance alignment, emotional content, and interaction dynamics. Our analysis reveals distinct polarization patterns that are event dependent: (i) anticipation-driven polarization, where extreme polarization escalates before well-publicized events, and (ii) reactive polarization, where intense affective polarization spikes immediately after sudden, high-impact events. By combining AI-driven content annotation with domain-informed scoring, our framework offers a scalable and interpretable approach to measuring affective polarization. The source code is publicly available at: https://github.com/hasanjawad001/llm-social-media-polarization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆå¤§è¯­è¨€æ¨¡å‹ (LLMs) å’Œé¢†åŸŸå¯å‘å¼è§„åˆ™ (domain-informed heuristics) çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°åˆ†æå’Œé‡åŒ–ç¤¾äº¤åª’ä½“åœ¨æ°”å€™å˜åŒ–å’Œæªæ”¯æ§åˆ¶ç­‰åˆ†æ­§è¯é¢˜ä¸­çš„æƒ…æ„ŸæåŒ– (affective polarization)ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ LLMs ä»å¤§è§„æ¨¡è®¨è®ºä¸­æå–ç«‹åœº (stance)ã€æƒ…æ„ŸåŸºè°ƒ (affective tone) å’Œä¸€è‡´æ€§æ¨¡å¼ï¼Œå¹¶é…åˆåŸºäºè§„åˆ™çš„è¯„åˆ†ç³»ç»Ÿï¼Œä½¿å…¶èƒ½å¤Ÿé‡åŒ–å³ä½¿æ˜¯å•æ¬¡äº’åŠ¨æ„æˆçš„å¾®å°å¯¹è¯ä¸­çš„æåŒ–ç¨‹åº¦ã€‚ç ”ç©¶æ­ç¤ºäº†ä¸¤ç§å—äº‹ä»¶é©±åŠ¨çš„æåŒ–æ¨¡å¼ï¼Œå³åœ¨é‡å¤§äº‹ä»¶å‰å‡çº§çš„é¢„æœŸé©±åŠ¨æåŒ– (anticipation-driven polarization) ä»¥åŠåœ¨çªå‘äº‹ä»¶åæ¿€å¢çš„ååº”å¼æåŒ– (reactive polarization)ã€‚é€šè¿‡å°† AI é©±åŠ¨çš„å†…å®¹æ ‡æ³¨ä¸é¢†åŸŸå¯å‘å¼è¯„åˆ†ç›¸ç»“åˆï¼Œè¯¥æ¡†æ¶ä¸ºè¡¡é‡ç¤¾äº¤åª’ä½“æåŒ–æä¾›äº†ä¸€ç§å…¼å…·å¯æ‰©å±•æ€§å’Œå¯è§£é‡Šæ€§çš„æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SI",
      "comment": "Foundations and Applications of Big Data Analytics (FAB), Niagara Falls, Canada, 2025",
      "pdf_url": "https://arxiv.org/pdf/2601.00927v1",
      "published_date": "2026-01-02 01:11:58 UTC",
      "updated_date": "2026-01-02 01:11:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:24:02.590005+00:00"
    },
    {
      "arxiv_id": "2601.00516v1",
      "title": "Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI",
      "title_zh": "Trajectory Guardï¼šé¢å‘æ™ºèƒ½ä½“ AI å®æ—¶å¼‚å¸¸æ£€æµ‹çš„è½»é‡çº§åºåˆ—æ„ŸçŸ¥æ¨¡å‹",
      "authors": [
        "Laksh Advani"
      ],
      "abstract": "Autonomous LLM agents generate multi-step action plans that can fail due to contextual misalignment or structural incoherence. Existing anomaly detection methods are ill-suited for this challenge: mean-pooling embeddings dilutes anomalous steps, while contrastive-only approaches ignore sequential structure. Standard unsupervised methods on pre-trained embeddings achieve F1-scores no higher than 0.69. We introduce Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss function that jointly learns task-trajectory alignment via contrastive learning and sequential validity via reconstruction. This dual objective enables unified detection of both \"wrong plan for this task\" and \"malformed plan structure.\" On benchmarks spanning synthetic perturbations and real-world failures from security audits (RAS-Eval) and multi-agent systems (Who\\&When), we achieve F1-scores of 0.88-0.94 on balanced sets and recall of 0.86-0.92 on imbalanced external benchmarks. At 32 ms inference latency, our approach runs 17-27$\\times$ faster than LLM Judge baselines, enabling real-time safety verification in production deployments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªä¸» LLM agents åœ¨å¤šæ­¥è¡ŒåŠ¨è®¡åˆ’ä¸­å› è¯­å¢ƒå¤±é…æˆ–ç»“æ„ä¸è¿è´¯å¯¼è‡´çš„å¤±è´¥é—®é¢˜ï¼Œæå‡ºäº† Trajectory Guard æ¨¡å‹ã€‚è¿™æ˜¯ä¸€ç§åŸºäº Siamese Recurrent Autoencoder çš„è½»é‡çº§åºåˆ—æ„ŸçŸ¥æ¶æ„ï¼Œåˆ©ç”¨æ··åˆæŸå¤±å‡½æ•°ç»“åˆ contrastive learning å’Œ reconstruction æŠ€æœ¯ï¼ŒåŒæ­¥å­¦ä¹ ä»»åŠ¡è½¨è¿¹å¯¹é½ä¸åºåˆ—æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«â€œä»»åŠ¡è®¡åˆ’é”™è¯¯â€å’Œâ€œè®¡åˆ’ç»“æ„ç•¸å½¢â€ï¼Œå…‹æœäº†ä¼ ç»Ÿ mean-pooling æˆ–å•ä¸€å¯¹æ¯”å­¦ä¹ æ–¹æ³•å¿½è§†åºåˆ—ç»“æ„çš„å±€é™ã€‚åœ¨ RAS-Eval å’Œ Who&When ç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒTrajectory Guard è¾¾åˆ°äº† 0.88-0.94 çš„ F1-scoreï¼Œè¡¨ç°æ˜¾è‘—ä¼˜äºæ— ç›‘ç£åŸºå‡†ã€‚å‡­å€Ÿä»… 32 ms çš„æ¨ç†å»¶è¿Ÿï¼Œè¯¥æ¨¡å‹æ¯” LLM Judge é€Ÿåº¦å¿« 17-27 å€ï¼Œä¸º Agentic AI åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„å®æ—¶å¼‚å¸¸æ£€æµ‹ä¸å®‰å…¨éªŒè¯æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to AAAI Trustagent 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.00516v1",
      "published_date": "2026-01-02 00:27:11 UTC",
      "updated_date": "2026-01-02 00:27:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:23:42.249760+00:00"
    },
    {
      "arxiv_id": "2601.00514v1",
      "title": "The Illusion of Insight in Reasoning Models",
      "title_zh": "æ¨ç†æ¨¡å‹ä¸­çš„æ´å¯Ÿå¹»è±¡",
      "authors": [
        "Liv G. d'Aliberti",
        "Manoel Horta Ribeiro"
      ],
      "abstract": "Do reasoning models have \"Aha!\" moments? Prior work suggests that models like DeepSeek-R1-Zero undergo sudden mid-trace realizations that lead to accurate outputs, implying an intrinsic capacity for self-correction. Yet, it remains unclear whether such intrinsic shifts in reasoning strategy actually improve performance. Here, we study mid-reasoning shifts and instrument training runs to detect them. Our analysis spans 1M+ reasoning traces, hundreds of training checkpoints, three reasoning domains, and multiple decoding temperatures and model architectures. We find that reasoning shifts are rare, do not become more frequent with training, and seldom improve accuracy, indicating that they do not correspond to prior perceptions of model insight. However, their effect varies with model uncertainty. Building on this finding, we show that artificially triggering extrinsic shifts under high entropy reliably improves accuracy. Our results show that mid-reasoning shifts are symptoms of unstable inference behavior rather than an intrinsic mechanism for self-correction.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Reasoning Modelsåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ˜¯å¦å…·å¤‡ç±»ä¼¼äºâ€œé¡¿æ‚Ÿâ€çš„å†…åœ¨è‡ªæˆ‘çº é”™èƒ½åŠ›ã€‚é€šè¿‡å¯¹è·¨è¶Šå¤šä¸ªé¢†åŸŸå’Œæ¨¡å‹æ¶æ„çš„100ä¸‡ä¸ªæ¨ç†è½¨è¿¹è¿›è¡Œåˆ†æï¼Œç ”ç©¶å‘ç°æ¨ç†è¿‡ç¨‹ä¸­çš„ç­–ç•¥è½¬å˜ï¼ˆMid-reasoning shiftsï¼‰ä¸ä»…ååˆ†ç½•è§ï¼Œä¸”ä¸ä¼šéšç€è®­ç»ƒè€Œå˜å¾—é¢‘ç¹ï¼Œä¹Ÿå¾ˆå°‘èƒ½æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚å®éªŒè¯æ®è¡¨æ˜ï¼Œè¿™äº›è½¬å˜å®é™…ä¸Šæ˜¯æ¨ç†è¡Œä¸ºä¸ç¨³å®šçš„è¡¨ç°ï¼Œè€Œéæ¨¡å‹äº§ç”Ÿäº†çœŸæ­£çš„æ´å¯ŸåŠ›ã€‚ç„¶è€Œï¼Œç ”ç©¶ä¹Ÿå‘ç°ç­–ç•¥è½¬å˜çš„å½±å“ä¸æ¨¡å‹çš„ä¸ç¡®å®šæ€§ï¼ˆUncertaintyï¼‰å¯†åˆ‡ç›¸å…³ã€‚åŸºäºæ­¤å‘ç°ï¼Œç ”ç©¶è¯æ˜åœ¨ç†µå€¼ï¼ˆEntropyï¼‰è¾ƒé«˜æ—¶é€šè¿‡å¤–éƒ¨è§¦å‘ç­–ç•¥è½¬å˜å¯ä»¥å¯é åœ°æå‡æ¨¡å‹å‡†ç¡®ç‡ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥è®ºæ–‡æ­ç¤ºäº†æ¨ç†æ¨¡å‹ä¸­æ‰€è°“çš„â€œæ´å¯ŸåŠ›â€å¾€å¾€åªæ˜¯ä¸€ç§å¹»è§‰ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00514v1",
      "published_date": "2026-01-02 00:12:13 UTC",
      "updated_date": "2026-01-02 00:12:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:23:15.893604+00:00"
    },
    {
      "arxiv_id": "2601.06097v1",
      "title": "Semantic Event Graphs for Long-Form Video Question Answering",
      "title_zh": "é¢å‘é•¿è§†é¢‘é—®ç­”çš„è¯­ä¹‰äº‹ä»¶å›¾",
      "authors": [
        "Aradhya Dixit",
        "Tianxi Liang"
      ],
      "abstract": "Long-form video question answering remains challenging for modern vision-language models, which struggle to reason over hour-scale footage without exceeding practical token and compute budgets. Existing systems typically downsample frames or feed dense visual embeddings to large-context language models, trading off temporal coverage against cost. We propose Semantic Event Graphs (SEG), a lightweight symbolic interface between video and language that replaces raw frames with compact temporal interaction logs. Our pipeline detects and tracks objects with YOLOv11, converts proximity patterns into START/END human-object events, and organizes them into a Temporal Scene Graph (TSG). At inference time, a query-aware pruning module identifies anchor entities and lexically relevant events, returning only a small subgraph which is verbalized and passed to Gemini 2.5 Flash for answer generation. On five YouTube videos (300-500 interactions each) and 120 automatically generated long-horizon questions, SEG achieves 65.0% accuracy using only 3.47k tokens per query, closely matching a full-log baseline (62.5% at 40.39k tokens) while reducing token usage by 91.4%. A short-context baseline restricted to the last 30 seconds collapses to 2.5% accuracy, underscoring the need for explicit temporal memory. These results show that symbolic temporal graphs can serve as an effective, plug-and-play memory layer for off-the-shelf vision-language models, preserving long-range reasoning ability while making long-form video question answering substantially more token- and cost-efficient. Code, logs, and event-extraction tools will be released for reproducibility.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†è¯­ä¹‰äº‹ä»¶å›¾(Semantic Event Graphs, SEG)ï¼Œæ—¨åœ¨è§£å†³ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹(Vision-language models)åœ¨å¤„ç†é•¿è§†é¢‘é—®ç­”(Long-form video question answering)æ—¶é¢ä¸´çš„è®¡ç®—èµ„æºä¸Tokené™åˆ¶é—®é¢˜ã€‚SEGé€šè¿‡ä¸€ç§è½»é‡çº§çš„ç¬¦å·æ¥å£å–ä»£åŸå§‹è§†é¢‘å¸§ï¼Œåˆ©ç”¨YOLOv11æ£€æµ‹å¹¶è¿½è¸ªå¯¹è±¡ï¼Œå°†é‚»è¿‘æ¨¡å¼è½¬åŒ–ä¸ºâ€œå¼€å§‹/ç»“æŸâ€äº¤äº’äº‹ä»¶ï¼Œå¹¶æ„å»ºæ—¶é—´åœºæ™¯å›¾(Temporal Scene Graph, TSG)ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œç³»ç»Ÿé€šè¿‡æŸ¥è¯¢æ„ŸçŸ¥å‰ªææ¨¡å—(Query-aware pruning module)æå–å…³é”®å­å›¾å¹¶å°†å…¶è¯­è¨€åŒ–ï¼Œæœ€åç”±Gemini 2.5 Flashç”Ÿæˆç­”æ¡ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSEGåœ¨ä»…æ¶ˆè€—3.47kä¸ªTokençš„æƒ…å†µä¸‹è¾¾åˆ°äº†65.0%çš„å‡†ç¡®ç‡ï¼Œåœ¨èŠ‚çœ91.4% Tokenæˆæœ¬çš„åŒæ—¶ï¼Œå…¶è¡¨ç°ç”šè‡³ç•¥ä¼˜äºå®Œæ•´æ—¥å¿—åŸºçº¿ã€‚è¯¥ç ”ç©¶è¯æ˜äº†ç¬¦å·åŒ–æ—¶é—´å›¾å¯ä»¥ä½œä¸ºè§†è§‰è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆæ’ä»¶å¼è®°å¿†å±‚ï¼Œåœ¨ä¿ç•™é•¿ç¨‹æ¨ç†èƒ½åŠ›çš„åŒæ—¶æ˜¾è‘—æå‡äº†é•¿è§†é¢‘ç†è§£çš„æˆæœ¬æ•ˆç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.06097v1",
      "published_date": "2026-01-02 00:11:03 UTC",
      "updated_date": "2026-01-02 00:11:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T21:23:13.463023+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 54,
  "processed_papers_count": 54,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T21:24:55.333429+00:00"
}
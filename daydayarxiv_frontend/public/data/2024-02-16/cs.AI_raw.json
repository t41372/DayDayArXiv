[
  {
    "arxiv_id": "2402.14830v1",
    "title": "Orca-Math: Unlocking the potential of SLMs in Grade School Math",
    "authors": [
      "Arindam Mitra",
      "Hamed Khanpour",
      "Corby Rosset",
      "Ahmed Awadallah"
    ],
    "abstract": "Mathematical word problem-solving has long been recognized as a complex task\nfor small language models (SLMs). A recent study hypothesized that the smallest\nmodel size, needed to achieve over 80% accuracy on the GSM8K benchmark, is 34\nbillion parameters. To reach this level of performance with smaller models,\nresearcher often train SLMs to generate Python code or use tools to help avoid\ncalculation errors. Additionally, they employ ensembling, where outputs of up\nto 100 model runs are combined to arrive at a more accurate result. Result\nselection is done using consensus, majority vote or a separate a verifier model\nused in conjunction with the SLM. Ensembling provides a substantial boost in\naccuracy but at a significant cost increase with multiple calls to the model\n(e.g., Phi-GSM uses top-48 to boost the performance from 68.2 to 81.5).\n  In this work, we present Orca-Math, a 7-billion-parameter SLM based on the\nMistral-7B, which achieves 86.81% on GSM8k without the need for multiple model\ncalls or the use of verifiers, code execution or any other external tools. Our\napproach has the following key elements: (1) A high quality synthetic dataset\nof 200K math problems created using a multi-agent setup where agents\ncollaborate to create the data, (2) An iterative learning techniques that\nenables the SLM to practice solving problems, receive feedback on its solutions\nand learn from preference pairs incorporating the SLM solutions and the\nfeedback. When trained with Supervised Fine-Tuning alone, Orca-Math achieves\n81.50% on GSM8k pass@1 metric. With iterative preference learning, Orca-Math\nachieves 86.81% pass@1. Orca-Math surpasses the performance of significantly\nlarger models such as LLAMA-2-70B, WizardMath-70B, Gemini-Pro, ChatGPT-3.5. It\nalso significantly outperforms other smaller models while using much smaller\ndata (hundreds of thousands vs. millions of problems).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.14830v1",
    "published_date": "2024-02-16 23:44:38 UTC",
    "updated_date": "2024-02-16 23:44:38 UTC"
  },
  {
    "arxiv_id": "2402.11131v1",
    "title": "Speculative Streaming: Fast LLM Inference without Auxiliary Models",
    "authors": [
      "Nikhil Bhendawade",
      "Irina Belousova",
      "Qichen Fu",
      "Henry Mason",
      "Mohammad Rastegari",
      "Mahyar Najibi"
    ],
    "abstract": "Speculative decoding is a prominent technique to speed up the inference of a\nlarge target language model based on predictions of an auxiliary draft model.\nWhile effective, in application-specific settings, it often involves\nfine-tuning both draft and target models to achieve high acceptance rates. As\nthe number of downstream tasks grows, these draft models add significant\ncomplexity to inference systems. We propose Speculative Streaming, a\nsingle-model speculative decoding method that fuses drafting into the target\nmodel by changing the fine-tuning objective from next token prediction to\nfuture n-gram prediction. Speculative Streaming speeds up decoding by 1.8 -\n3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and\nMeaning Representation, without sacrificing generation quality. Additionally,\nSpeculative Streaming is parameter-efficient. It achieves on-par/higher\nspeed-ups than Medusa-style architectures while using ~10000X fewer extra\nparameters, making it well-suited for resource-constrained devices.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11131v1",
    "published_date": "2024-02-16 23:36:43 UTC",
    "updated_date": "2024-02-16 23:36:43 UTC"
  },
  {
    "arxiv_id": "2403.14633v4",
    "title": "Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models",
    "authors": [
      "Smriti Singh",
      "Shuvam Keshari",
      "Vinija Jain",
      "Aman Chadha"
    ],
    "abstract": "Socioeconomic bias in society exacerbates disparities, influencing access to\nopportunities and resources based on individuals' economic and social\nbackgrounds. This pervasive issue perpetuates systemic inequalities, hindering\nthe pursuit of inclusive progress as a society. In this paper, we investigate\nthe presence of socioeconomic bias, if any, in large language models. To this\nend, we introduce a novel dataset SilverSpoon, consisting of 3000 samples that\nillustrate hypothetical scenarios that involve underprivileged people\nperforming ethically ambiguous actions due to their circumstances, and ask\nwhether the action is ethically justified. Further, this dataset has a\ndual-labeling scheme and has been annotated by people belonging to both ends of\nthe socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of\nsocioeconomic bias expressed in large language models and the variation of this\ndegree as a function of model size. We also perform qualitative analysis to\nanalyze the nature of this bias. Our analysis reveals that while humans\ndisagree on which situations require empathy toward the underprivileged, most\nlarge language models are unable to empathize with the socioeconomically\nunderprivileged regardless of the situation. To foster further research in this\ndomain, we make SilverSpoon and our evaluation harness publicly available.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14633v4",
    "published_date": "2024-02-16 23:18:19 UTC",
    "updated_date": "2024-12-19 22:13:50 UTC"
  },
  {
    "arxiv_id": "2402.11123v1",
    "title": "Optimizing Warfarin Dosing Using Contextual Bandit: An Offline Policy Learning and Evaluation Method",
    "authors": [
      "Yong Huang",
      "Charles A. Downs",
      "Amir M. Rahmani"
    ],
    "abstract": "Warfarin, an anticoagulant medication, is formulated to prevent and address\nconditions associated with abnormal blood clotting, making it one of the most\nprescribed drugs globally. However, determining the suitable dosage remains\nchallenging due to individual response variations, and prescribing an incorrect\ndosage may lead to severe consequences. Contextual bandit and reinforcement\nlearning have shown promise in addressing this issue. Given the wide\navailability of observational data and safety concerns of decision-making in\nhealthcare, we focused on using exclusively observational data from historical\npolicies as demonstrations to derive new policies; we utilized offline policy\nlearning and evaluation in a contextual bandit setting to establish the optimal\npersonalized dosage strategy. Our learned policies surpassed these baseline\napproaches without genotype inputs, even when given a suboptimal demonstration,\nshowcasing promising application potential.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11123v1",
    "published_date": "2024-02-16 23:13:05 UTC",
    "updated_date": "2024-02-16 23:13:05 UTC"
  },
  {
    "arxiv_id": "2402.11122v1",
    "title": "Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models",
    "authors": [
      "Zihao Lin",
      "Mohammad Beigi",
      "Hongxuan Li",
      "Yufan Zhou",
      "Yuxiang Zhang",
      "Qifan Wang",
      "Wenpeng Yin",
      "Lifu Huang"
    ],
    "abstract": "Memory Editing (ME) has emerged as an efficient method to modify erroneous\nfacts or inject new facts into Large Language Models (LLMs). Two mainstream ME\nmethods exist: parameter-modifying ME and parameter-preserving ME (integrating\nextra modules while preserving original parameters). Regrettably, previous\nstudies on ME evaluation have two critical limitations: (i) evaluating LLMs\nwith single edit only, neglecting the need for continuous editing, and (ii)\nevaluations focusing solely on basic factual triples, overlooking broader LLM\ncapabilities like logical reasoning and reading understanding. This study\naddresses these limitations with contributions threefold: (i) We explore how ME\naffects a wide range of fundamental capabilities of LLMs under sequential\nediting. Experimental results reveal an intriguing phenomenon: Most\nparameter-modifying ME consistently degrade performance across all tasks after\na few sequential edits. In contrast, parameter-preserving ME effectively\nmaintains LLMs' fundamental capabilities but struggles to accurately recall\nedited knowledge presented in a different format. (ii) We extend our evaluation\nto different editing settings, such as layers to edit, model size, instruction\ntuning, etc. Experimental findings indicate several strategies that can\npotentially mitigate the adverse effects of ME. (iii) We further explain why\nparameter-modifying ME damages LLMs from three dimensions: parameter changes\nafter editing, language modeling capability, and the in-context learning\ncapability. Our in-depth study advocates more careful use of ME in real-world\nscenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "preprint, 15 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.11122v1",
    "published_date": "2024-02-16 23:08:55 UTC",
    "updated_date": "2024-02-16 23:08:55 UTC"
  },
  {
    "arxiv_id": "2402.12396v1",
    "title": "Toward using GANs in astrophysical Monte-Carlo simulations",
    "authors": [
      "Ahab Isaac",
      "Wesley Armour",
      "Karel Ad√°mek"
    ],
    "abstract": "Accurate modelling of spectra produced by X-ray sources requires the use of\nMonte-Carlo simulations. These simulations need to evaluate physical processes,\nsuch as those occurring in accretion processes around compact objects by\nsampling a number of different probability distributions. This is\ncomputationally time-consuming and could be sped up if replaced by neural\nnetworks. We demonstrate, on an example of the Maxwell-J\\\"uttner distribution\nthat describes the speed of relativistic electrons, that the generative\nadversarial network (GAN) is capable of statistically replicating the\ndistribution. The average value of the Kolmogorov-Smirnov test is 0.5 for\nsamples generated by the neural network, showing that the generated\ndistribution cannot be distinguished from the true distribution.",
    "categories": [
      "astro-ph.HE",
      "cs.AI",
      "I.2.0"
    ],
    "primary_category": "astro-ph.HE",
    "comment": "Proceedings of ADASS XXXIII (2023)",
    "pdf_url": "http://arxiv.org/pdf/2402.12396v1",
    "published_date": "2024-02-16 23:07:53 UTC",
    "updated_date": "2024-02-16 23:07:53 UTC"
  },
  {
    "arxiv_id": "2402.13273v1",
    "title": "Operational Collective Intelligence of Humans and Machines",
    "authors": [
      "Nikolos Gurney",
      "Fred Morstatter",
      "David V. Pynadath",
      "Adam Russell",
      "Gleb Satyukov"
    ],
    "abstract": "We explore the use of aggregative crowdsourced forecasting (ACF) as a\nmechanism to help operationalize ``collective intelligence'' of human-machine\nteams for coordinated actions. We adopt the definition for Collective\nIntelligence as: ``A property of groups that emerges from synergies among\ndata-information-knowledge, software-hardware, and individuals (those with new\ninsights as well as recognized authorities) that enables just-in-time knowledge\nfor better decisions than these three elements acting alone.'' Collective\nIntelligence emerges from new ways of connecting humans and AI to enable\ndecision-advantage, in part by creating and leveraging additional sources of\ninformation that might otherwise not be included. Aggregative crowdsourced\nforecasting (ACF) is a recent key advancement towards Collective Intelligence\nwherein predictions (X\\% probability that Y will happen) and rationales (why I\nbelieve it is this probability that X will happen) are elicited independently\nfrom a diverse crowd, aggregated, and then used to inform higher-level\ndecision-making. This research asks whether ACF, as a key way to enable\nOperational Collective Intelligence, could be brought to bear on operational\nscenarios (i.e., sequences of events with defined agents, components, and\ninteractions) and decision-making, and considers whether such a capability\ncould provide novel operational capabilities to enable new forms of\ndecision-advantage.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.13273v1",
    "published_date": "2024-02-16 22:45:09 UTC",
    "updated_date": "2024-02-16 22:45:09 UTC"
  },
  {
    "arxiv_id": "2402.13272v1",
    "title": "Spontaneous Theory of Mind for Artificial Intelligence",
    "authors": [
      "Nikolos Gurney",
      "David V. Pynadath",
      "Volkan Ustun"
    ],
    "abstract": "Existing approaches to Theory of Mind (ToM) in Artificial Intelligence (AI)\noveremphasize prompted, or cue-based, ToM, which may limit our collective\nability to develop Artificial Social Intelligence (ASI). Drawing from research\nin computer science, cognitive science, and related disciplines, we contrast\nprompted ToM with what we call spontaneous ToM -- reasoning about others'\nmental states that is grounded in unintentional, possibly uncontrollable\ncognitive functions. We argue for a principled approach to studying and\ndeveloping AI ToM and suggest that a robust, or general, ASI will respond to\nprompts \\textit{and} spontaneously engage in social reasoning.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.13272v1",
    "published_date": "2024-02-16 22:41:13 UTC",
    "updated_date": "2024-02-16 22:41:13 UTC"
  },
  {
    "arxiv_id": "2402.11104v2",
    "title": "Computing Voting Rules with Elicited Incomplete Votes",
    "authors": [
      "Daniel Halpern",
      "Safwan Hossain",
      "Jamie Tucker-Foltz"
    ],
    "abstract": "Motivated by the difficulty of specifying complete ordinal preferences over a\nlarge set of $m$ candidates, we study voting rules that are computable by\nquerying voters about $t < m$ candidates. Generalizing prior works that focused\non specific instances of this problem, our paper fully characterizes the set of\npositional scoring rules that can be computed for any $1 \\leq t < m$, which,\nnotably, does not include plurality. We then extend this to show a similar\nimpossibility result for single transferable vote (elimination voting). These\nnegative results are information-theoretic and agnostic to the number of\nqueries. Finally, for scoring rules that are computable with limited-sized\nqueries, we give parameterized upper and lower bounds on the number of such\nqueries a deterministic or randomized algorithm must make to determine the\nscore-maximizing candidate. While there is no gap between our bounds for\ndeterministic algorithms, identifying the exact query complexity for randomized\nalgorithms is a challenging open problem, of which we solve one special case.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11104v2",
    "published_date": "2024-02-16 22:17:01 UTC",
    "updated_date": "2024-09-26 18:55:01 UTC"
  },
  {
    "arxiv_id": "2403.15397v1",
    "title": "Regulating Large Language Models: A Roundtable Report",
    "authors": [
      "Gabriel Nicholas",
      "Paul Friedl"
    ],
    "abstract": "On July 20, 2023, a group of 27 scholars and digital rights advocates with\nexpertise in law, computer science, political science, and other disciplines\ngathered for the Large Language Models, Law and Policy Roundtable, co-hosted by\nthe NYU School of Law's Information Law Institute and the Center for Democracy\n& Technology. The roundtable convened to discuss how law and policy can help\naddress some of the larger societal problems posed by large language models\n(LLMs). The discussion focused on three policy topic areas in particular:\n  1. Truthfulness: What risks do LLMs pose in terms of generating mis- and\ndisinformation? How can these risks be mitigated from a technical and/or\nregulatory perspective?\n  2. Privacy: What are the biggest privacy risks involved in the creation,\ndeployment, and use of LLMs? How can these risks be mitigated from a technical\nand/or regulatory perspective?\n  3. Market concentration: What threats do LLMs pose concerning market/power\nconcentration? How can these risks be mitigated from a technical and/or\nregulatory perspective?\n  In this paper, we provide a detailed summary of the day's proceedings. We\nfirst recap what we deem to be the most important contributions made during the\nissue framing discussions. We then provide a list of potential legal and\nregulatory interventions generated during the brainstorming discussions.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "24 pages",
    "pdf_url": "http://arxiv.org/pdf/2403.15397v1",
    "published_date": "2024-02-16 21:49:17 UTC",
    "updated_date": "2024-02-16 21:49:17 UTC"
  },
  {
    "arxiv_id": "2402.11089v3",
    "title": "The Male CEO and the Female Assistant: Evaluation and Mitigation of Gender Biases in Text-To-Image Generation of Dual Subjects",
    "authors": [
      "Yixin Wan",
      "Kai-Wei Chang"
    ],
    "abstract": "Recent large-scale T2I models like DALLE-3 have made progress in reducing\ngender stereotypes when generating single-person images. However, significant\nbiases remain when generating images with more than one person. To\nsystematically evaluate this, we propose the Paired Stereotype Test (PST)\nframework, which queries T2I models to depict two individuals assigned with\nmale-stereotyped and female-stereotyped social identities, respectively (e.g.\n\"a CEO\" and \"an Assistant\"). This contrastive setting often triggers T2I models\nto generate gender-stereotyped images. Using PST, we evaluate two aspects of\ngender biases -- the well-known bias in gendered occupation and a novel aspect:\nbias in organizational power. Experiments show that over 74% images generated\nby DALLE-3 display gender-occupational biases. Additionally, compared to\nsingle-person settings, DALLE-3 is more likely to perpetuate male-associated\nstereotypes under PST. We further propose FairCritic, a novel and interpretable\nframework that leverages an LLM-based critic model to i) detect bias in\ngenerated images, and ii) adaptively provide feedback to T2I models for\nimproving fairness. FairCritic achieves near-perfect fairness on PST,\novercoming the limitations of previous prompt-based intervention approaches.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11089v3",
    "published_date": "2024-02-16 21:32:27 UTC",
    "updated_date": "2024-10-23 22:47:44 UTC"
  },
  {
    "arxiv_id": "2402.11082v1",
    "title": "The AI Security Pyramid of Pain",
    "authors": [
      "Chris M. Ward",
      "Josh Harguess",
      "Julia Tao",
      "Daniel Christman",
      "Paul Spicer",
      "Mike Tan"
    ],
    "abstract": "We introduce the AI Security Pyramid of Pain, a framework that adapts the\ncybersecurity Pyramid of Pain to categorize and prioritize AI-specific threats.\nThis framework provides a structured approach to understanding and addressing\nvarious levels of AI threats. Starting at the base, the pyramid emphasizes Data\nIntegrity, which is essential for the accuracy and reliability of datasets and\nAI models, including their weights and parameters. Ensuring data integrity is\ncrucial, as it underpins the effectiveness of all AI-driven decisions and\noperations. The next level, AI System Performance, focuses on MLOps-driven\nmetrics such as model drift, accuracy, and false positive rates. These metrics\nare crucial for detecting potential security breaches, allowing for early\nintervention and maintenance of AI system integrity. Advancing further, the\npyramid addresses the threat posed by Adversarial Tools, identifying and\nneutralizing tools used by adversaries to target AI systems. This layer is key\nto staying ahead of evolving attack methodologies. At the Adversarial Input\nlayer, the framework addresses the detection and mitigation of inputs designed\nto deceive or exploit AI models. This includes techniques like adversarial\npatterns and prompt injection attacks, which are increasingly used in\nsophisticated attacks on AI systems. Data Provenance is the next critical\nlayer, ensuring the authenticity and lineage of data and models. This layer is\npivotal in preventing the use of compromised or biased data in AI systems. At\nthe apex is the tactics, techniques, and procedures (TTPs) layer, dealing with\nthe most complex and challenging aspects of AI security. This involves a deep\nunderstanding and strategic approach to counter advanced AI-targeted attacks,\nrequiring comprehensive knowledge and planning.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "SPIE DCS 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.11082v1",
    "published_date": "2024-02-16 21:14:11 UTC",
    "updated_date": "2024-02-16 21:14:11 UTC"
  },
  {
    "arxiv_id": "2402.11078v3",
    "title": "Model Editing by Standard Fine-Tuning",
    "authors": [
      "Govind Gangadhar",
      "Karl Stratos"
    ],
    "abstract": "Standard fine-tuning is considered not as effective as specialized methods\nfor model editing due to its comparatively poor performance. However, it is\nsimple, agnostic to the architectural details of the model being edited, and\nable to leverage advances in standard training techniques with no additional\nwork (e.g., black-box PEFT for computational efficiency), making it an\nappealing choice for a model editor. In this work, we show that standard\nfine-tuning alone can yield competitive model editing performance with two\nminor modifications. First, we optimize the conditional likelihood rather than\nthe full likelihood. Second, in addition to the typical practice of training on\nrandomly paraphrased edit prompts to encourage generalization, we also train on\nrandom or similar unedited facts to encourage locality. Our experiments on the\nZsRE and CounterFact datasets demonstrate that these simple modifications allow\nstandard fine-tuning to match or outperform highly specialized editors in terms\nof edit score.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Findings of ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.11078v3",
    "published_date": "2024-02-16 21:10:33 UTC",
    "updated_date": "2024-06-03 05:39:10 UTC"
  },
  {
    "arxiv_id": "2402.11073v3",
    "title": "AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators",
    "authors": [
      "Jingwei Ni",
      "Minjing Shi",
      "Dominik Stammbach",
      "Mrinmaya Sachan",
      "Elliott Ash",
      "Markus Leippold"
    ],
    "abstract": "With the rise of generative AI, automated fact-checking methods to combat\nmisinformation are becoming more and more important. However, factual claim\ndetection, the first step in a fact-checking pipeline, suffers from two key\nissues that limit its scalability and generalizability: (1) inconsistency in\ndefinitions of the task and what a claim is, and (2) the high cost of manual\nannotation. To address (1), we review the definitions in related work and\npropose a unifying definition of factual claims that focuses on verifiability.\nTo address (2), we introduce AFaCTA (Automatic Factual Claim deTection\nAnnotator), a novel framework that assists in the annotation of factual claims\nwith the help of large language models (LLMs). AFaCTA calibrates its annotation\nconfidence with consistency along three predefined reasoning paths. Extensive\nevaluation and experiments in the domain of political speech reveal that AFaCTA\ncan efficiently assist experts in annotating factual claims and training\nhigh-quality classifiers, and can work with or without expert supervision. Our\nanalyses also result in PoliClaim, a comprehensive claim detection dataset\nspanning diverse political topics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL2024 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2402.11073v3",
    "published_date": "2024-02-16 20:59:57 UTC",
    "updated_date": "2024-06-02 18:35:25 UTC"
  },
  {
    "arxiv_id": "2402.11068v2",
    "title": "Large Language Models for Causal Discovery: Current Landscape and Future Directions",
    "authors": [
      "Guangya Wan",
      "Yunsheng Lu",
      "Yuqi Wu",
      "Mengxuan Hu",
      "Sheng Li"
    ],
    "abstract": "Causal discovery (CD) and Large Language Models (LLMs) have emerged as\ntransformative fields in artificial intelligence that have evolved largely\nindependently. While CD specializes in uncovering cause-effect relationships\nfrom data, and LLMs excel at natural language processing and generation, their\nintegration presents unique opportunities for advancing causal understanding.\nThis survey examines how LLMs are transforming CD across three key dimensions:\ndirect causal extraction from text, integration of domain knowledge into\nstatistical methods, and refinement of causal structures. We systematically\nanalyze approaches that leverage LLMs for CD tasks, highlighting their\ninnovative use of metadata and natural language for causal inference. Our\nanalysis reveals both LLMs' potential to enhance traditional CD methods and\ntheir current limitations as imperfect expert systems. We identify key research\ngaps, outline evaluation frameworks and benchmarks for LLM-based causal\ndiscovery, and advocate future research efforts for leveraging LLMs in\ncausality research. As the first comprehensive examination of the synergy\nbetween LLMs and CD, this work lays the groundwork for future advances in the\nfield.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11068v2",
    "published_date": "2024-02-16 20:48:53 UTC",
    "updated_date": "2025-02-15 03:55:33 UTC"
  },
  {
    "arxiv_id": "2402.11060v3",
    "title": "Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement",
    "authors": [
      "Chenkai Sun",
      "Ke Yang",
      "Revanth Gangi Reddy",
      "Yi R. Fung",
      "Hou Pong Chan",
      "Kevin Small",
      "ChengXiang Zhai",
      "Heng Ji"
    ],
    "abstract": "The increasing demand for personalized interactions with large language\nmodels (LLMs) calls for methodologies capable of accurately and efficiently\nidentifying user opinions and preferences. Retrieval augmentation emerges as an\neffective strategy, as it can accommodate a vast number of users without the\ncosts from fine-tuning. Existing research, however, has largely focused on\nenhancing the retrieval stage and devoted limited exploration toward optimizing\nthe representation of the database, a crucial aspect for tasks such as\npersonalization. In this work, we examine the problem from a novel angle,\nfocusing on how data can be better represented for more data-efficient\nretrieval in the context of LLM customization. To tackle this challenge, we\nintroduce Persona-DB, a simple yet effective framework consisting of a\nhierarchical construction process to improve generalization across task\ncontexts and collaborative refinement to effectively bridge knowledge gaps\namong users. In the evaluation of response prediction, Persona-DB demonstrates\nsuperior context efficiency in maintaining accuracy with a significantly\nreduced retrieval size, a critical advantage in scenarios with extensive\nhistories or limited context windows. Our experiments also indicate a marked\nimprovement of over 10% under cold-start scenarios, when users have extremely\nsparse data. Furthermore, our analysis reveals the increasing importance of\ncollaborative knowledge as the retrieval capacity expands.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11060v3",
    "published_date": "2024-02-16 20:20:43 UTC",
    "updated_date": "2025-02-03 00:23:45 UTC"
  },
  {
    "arxiv_id": "2402.12394v2",
    "title": "Improving Model's Interpretability and Reliability using Biomarkers",
    "authors": [
      "Gautam Rajendrakumar Gare",
      "Tom Fox",
      "Beam Chansangavej",
      "Amita Krishnan",
      "Ricardo Luis Rodriguez",
      "Bennett P deBoisblanc",
      "Deva Kannan Ramanan",
      "John Michael Galeotti"
    ],
    "abstract": "Accurate and interpretable diagnostic models are crucial in the\nsafety-critical field of medicine. We investigate the interpretability of our\nproposed biomarker-based lung ultrasound diagnostic pipeline to enhance\nclinicians' diagnostic capabilities. The objective of this study is to assess\nwhether explanations from a decision tree classifier, utilizing biomarkers, can\nimprove users' ability to identify inaccurate model predictions compared to\nconventional saliency maps. Our findings demonstrate that decision tree\nexplanations, based on clinically established biomarkers, can assist clinicians\nin detecting false positives, thus improving the reliability of diagnostic\nmodels in medicine.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted at BIAS 2023 Conference",
    "pdf_url": "http://arxiv.org/pdf/2402.12394v2",
    "published_date": "2024-02-16 20:19:28 UTC",
    "updated_date": "2025-01-30 16:55:54 UTC"
  },
  {
    "arxiv_id": "2402.11051v1",
    "title": "Large Language Models Fall Short: Understanding Complex Relationships in Detective Narratives",
    "authors": [
      "Runcong Zhao",
      "Qinglin Zhu",
      "Hainiu Xu",
      "Jiazheng Li",
      "Yuxiang Zhou",
      "Yulan He",
      "Lin Gui"
    ],
    "abstract": "Existing datasets for narrative understanding often fail to represent the\ncomplexity and uncertainty of relationships in real-life social scenarios. To\naddress this gap, we introduce a new benchmark, Conan, designed for extracting\nand analysing intricate character relation graphs from detective narratives.\nSpecifically, we designed hierarchical relationship categories and manually\nextracted and annotated role-oriented relationships from the perspectives of\nvarious characters, incorporating both public relationships known to most\ncharacters and secret ones known to only a few. Our experiments with advanced\nLarge Language Models (LLMs) like GPT-3.5, GPT-4, and Llama2 reveal their\nlimitations in inferencing complex relationships and handling longer\nnarratives. The combination of the Conan dataset and our pipeline strategy is\ngeared towards understanding the ability of LLMs to comprehend nuanced\nrelational dynamics in narrative contexts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11051v1",
    "published_date": "2024-02-16 19:59:45 UTC",
    "updated_date": "2024-02-16 19:59:45 UTC"
  },
  {
    "arxiv_id": "2402.10893v1",
    "title": "RLVF: Learning from Verbal Feedback without Overgeneralization",
    "authors": [
      "Moritz Stephan",
      "Alexander Khazatsky",
      "Eric Mitchell",
      "Annie S Chen",
      "Sheryl Hsu",
      "Archit Sharma",
      "Chelsea Finn"
    ],
    "abstract": "The diversity of contexts in which large language models (LLMs) are deployed\nrequires the ability to modify or customize default model behaviors to\nincorporate nuanced requirements and preferences. A convenient interface to\nspecify such model adjustments is high-level verbal feedback, such as \"Don't\nuse emojis when drafting emails to my boss.\" However, while writing high-level\nfeedback is far simpler than collecting annotations for reinforcement learning\nfrom human feedback (RLHF), we find that simply prompting a model with such\nfeedback leads to overgeneralization of the feedback to contexts where it is\nnot relevant. We study the problem of incorporating verbal feedback without\nsuch overgeneralization, inspiring a new method Contextualized Critiques with\nConstrained Preference Optimization (C3PO). C3PO uses a piece of high-level\nfeedback to generate a small synthetic preference dataset specifying how the\nfeedback should (and should not) be applied. It then fine-tunes the model in\naccordance with the synthetic preference data while minimizing the divergence\nfrom the original model for prompts where the feedback does not apply. Our\nexperimental results indicate that our approach effectively applies verbal\nfeedback to relevant scenarios while preserving existing behaviors for other\ncontexts. For both human- and GPT-4-generated high-level feedback, C3PO\neffectively adheres to the given feedback comparably to in-context baselines\nwhile reducing overgeneralization by 30%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.10893v1",
    "published_date": "2024-02-16 18:50:24 UTC",
    "updated_date": "2024-02-16 18:50:24 UTC"
  },
  {
    "arxiv_id": "2402.10891v1",
    "title": "Instruction Diversity Drives Generalization To Unseen Tasks",
    "authors": [
      "Dylan Zhang",
      "Justin Wang",
      "Francois Charton"
    ],
    "abstract": "Instruction tuning -- fine-tuning a large language model (LLM) on pairs of\ninstructions and desired outcomes -- is an approach that enables pre-trained\nlanguage models to perform real-world tasks and follow human instructions. Its\npractical success depends on the model learning a broader set of instructions\nthan those it was trained on. Yet the factors that determine model\ngeneralization to such \\emph{unseen tasks} are not well understood. %To\nunderstand the driving factors of generalization, In this paper, we experiment\nwith string rewrites, a symbolic task that serves as a building block for\nTuring complete Markov algorithms while allowing experimental control of\n\"inputs\" and \"instructions\". We investigate the trade-off between the number of\ninstructions the model is trained on and the number of training samples\nprovided for each instruction and observe that the diversity of the instruction\nset determines generalization. Generalization emerges once a diverse enough set\nof tasks is provided, even though very few examples are provided for each task.\nInstruction diversity also ensures robustness with respect to non-uniform\ndistributions of instructions in the training set.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10891v1",
    "published_date": "2024-02-16 18:47:21 UTC",
    "updated_date": "2024-02-16 18:47:21 UTC"
  },
  {
    "arxiv_id": "2402.10890v2",
    "title": "When is Tree Search Useful for LLM Planning? It Depends on the Discriminator",
    "authors": [
      "Ziru Chen",
      "Michael White",
      "Raymond Mooney",
      "Ali Payani",
      "Yu Su",
      "Huan Sun"
    ],
    "abstract": "In this paper, we examine how large language models (LLMs) solve multi-step\nproblems under a language agent framework with three components: a generator, a\ndiscriminator, and a planning method. We investigate the practical utility of\ntwo advanced planning methods, iterative correction and tree search. We present\na comprehensive analysis of how discrimination accuracy affects the overall\nperformance of agents when using these two methods or a simpler method,\nre-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical\nreasoning, show that: (1) advanced planning methods demand discriminators with\nat least 90% accuracy to achieve significant improvements over re-ranking; (2)\ncurrent LLMs' discrimination abilities have not met the needs of advanced\nplanning methods to achieve such improvements; (3) with LLM-based\ndiscriminators, advanced planning methods may not adequately balance accuracy\nand efficiency. For example, compared to the other two methods, tree search is\nat least 10--20 times slower but leads to negligible performance gains, which\nhinders its real-world applications. Code and data are available at\nhttps://github.com/OSU-NLP-Group/llm-planning-eval.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024 main",
    "pdf_url": "http://arxiv.org/pdf/2402.10890v2",
    "published_date": "2024-02-16 18:45:58 UTC",
    "updated_date": "2024-06-06 14:55:40 UTC"
  },
  {
    "arxiv_id": "2402.10888v1",
    "title": "Explainability for Machine Learning Models: From Data Adaptability to User Perception",
    "authors": [
      "julien Delaunay"
    ],
    "abstract": "This thesis explores the generation of local explanations for already\ndeployed machine learning models, aiming to identify optimal conditions for\nproducing meaningful explanations considering both data and user requirements.\nThe primary goal is to develop methods for generating explanations for any\nmodel while ensuring that these explanations remain faithful to the underlying\nmodel and comprehensible to the users.\n  The thesis is divided into two parts. The first enhances a widely used\nrule-based explanation method. It then introduces a novel approach for\nevaluating the suitability of linear explanations to approximate a model.\nAdditionally, it conducts a comparative experiment between two families of\ncounterfactual explanation methods to analyze the advantages of one over the\nother. The second part focuses on user experiments to assess the impact of\nthree explanation methods and two distinct representations. These experiments\nmeasure how users perceive their interaction with the model in terms of\nunderstanding and trust, depending on the explanations and representations.\nThis research contributes to a better explanation generation, with potential\nimplications for enhancing the transparency, trustworthiness, and usability of\ndeployed AI systems.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "PhD Thesis",
    "pdf_url": "http://arxiv.org/pdf/2402.10888v1",
    "published_date": "2024-02-16 18:44:37 UTC",
    "updated_date": "2024-02-16 18:44:37 UTC"
  },
  {
    "arxiv_id": "2402.10885v3",
    "title": "3D Diffuser Actor: Policy Diffusion with 3D Scene Representations",
    "authors": [
      "Tsung-Wei Ke",
      "Nikolaos Gkanatsios",
      "Katerina Fragkiadaki"
    ],
    "abstract": "Diffusion policies are conditional diffusion models that learn robot action\ndistributions conditioned on the robot and environment state. They have\nrecently shown to outperform both deterministic and alternative action\ndistribution learning formulations. 3D robot policies use 3D scene feature\nrepresentations aggregated from a single or multiple camera views using sensed\ndepth. They have shown to generalize better than their 2D counterparts across\ncamera viewpoints. We unify these two lines of work and present 3D Diffuser\nActor, a neural policy equipped with a novel 3D denoising transformer that\nfuses information from the 3D visual scene, a language instruction and\nproprioception to predict the noise in noised 3D robot pose trajectories. 3D\nDiffuser Actor sets a new state-of-the-art on RLBench with an absolute\nperformance gain of 18.1% over the current SOTA on a multi-view setup and an\nabsolute gain of 13.1% on a single-view setup. On the CALVIN benchmark, it\nimproves over the current SOTA by a 9% relative increase. It also learns to\ncontrol a robot manipulator in the real world from a handful of demonstrations.\nThrough thorough comparisons with the current SOTA policies and ablations of\nour model, we show 3D Diffuser Actor's design choices dramatically outperform\n2D representations, regression and classification objectives, absolute\nattentions, and holistic non-tokenized 3D scene embeddings.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "First two authors contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2402.10885v3",
    "published_date": "2024-02-16 18:43:02 UTC",
    "updated_date": "2024-07-25 14:30:22 UTC"
  },
  {
    "arxiv_id": "2402.10884v2",
    "title": "Multi-modal Preference Alignment Remedies Degradation of Visual Instruction Tuning on Language Models",
    "authors": [
      "Shengzhi Li",
      "Rongyu Lin",
      "Shichao Pei"
    ],
    "abstract": "Multi-modal large language models (MLLMs) are expected to support multi-turn\nqueries of interchanging image and text modalities in production. However, the\ncurrent MLLMs trained with visual-question-answering (VQA) datasets could\nsuffer from degradation, as VQA datasets lack the diversity and complexity of\nthe original text instruction datasets with which the underlying language model\nwas trained. To address this degradation, we first collect a lightweight,\n5k-sample VQA preference dataset where answers were annotated by Gemini for\nfive quality metrics in a granular fashion and investigate standard Supervised\nFine-tuning, rejection sampling, Direct Preference Optimization (DPO) and\nSteerLM algorithms. Our findings indicate that with DPO, we can surpass the\ninstruction-following capabilities of the language model, achieving a 6.73\nscore on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99. This enhancement\nin textual instruction-following capability correlates with boosted visual\ninstruction performance (+4.9\\% on MM-Vet, +6\\% on LLaVA-Bench), with minimal\nalignment tax on visual knowledge benchmarks compared to the previous RLHF\napproach. In conclusion, we propose a distillation-based multi-modal alignment\nmodel with fine-grained annotations on a small dataset that restores and boosts\nMLLM's language capability after visual instruction tuning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Project code, model and data: https://github.com/findalexli/mllm-dpo",
    "pdf_url": "http://arxiv.org/pdf/2402.10884v2",
    "published_date": "2024-02-16 18:42:08 UTC",
    "updated_date": "2024-11-05 05:13:13 UTC"
  },
  {
    "arxiv_id": "2402.10877v7",
    "title": "Robust agents learn causal world models",
    "authors": [
      "Jonathan Richens",
      "Tom Everitt"
    ],
    "abstract": "It has long been hypothesised that causal reasoning plays a fundamental role\nin robust and general intelligence. However, it is not known if agents must\nlearn causal models in order to generalise to new domains, or if other\ninductive biases are sufficient. We answer this question, showing that any\nagent capable of satisfying a regret bound under a large set of distributional\nshifts must have learned an approximate causal model of the data generating\nprocess, which converges to the true causal model for optimal agents. We\ndiscuss the implications of this result for several research areas including\ntransfer learning and causal inference.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "ICLR 2024 (oral). Updated agents section, new corollary",
    "pdf_url": "http://arxiv.org/pdf/2402.10877v7",
    "published_date": "2024-02-16 18:29:19 UTC",
    "updated_date": "2024-07-19 11:12:08 UTC"
  },
  {
    "arxiv_id": "2402.11005v3",
    "title": "A Theory of LLM Sampling: Part Descriptive and Part Prescriptive",
    "authors": [
      "Sarath Sivaprasad",
      "Pramod Kaushik",
      "Sahar Abdelnabi",
      "Mario Fritz"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly utilized in autonomous\ndecision-making, where they sample options from vast action spaces. However,\nthe heuristics that guide this sampling process remain under-explored. We study\nthis sampling behavior and show that this underlying heuristics resembles that\nof human decision-making: comprising a descriptive component (reflecting\nstatistical norm) and a prescriptive component (implicit ideal encoded in the\nLLM) of a concept. We show that this deviation of a sample from the statistical\nnorm towards a prescriptive component consistently appears in concepts across\ndiverse real-world domains like public health, and economic trends. To further\nillustrate the theory, we demonstrate that concept prototypes in LLMs are\naffected by prescriptive norms, similar to the concept of normality in humans.\nThrough case studies and comparison with human studies, we illustrate that in\nreal-world applications, the shift of samples toward an ideal value in LLMs'\noutputs can result in significantly biased decision-making, raising ethical\nconcerns.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.11005v3",
    "published_date": "2024-02-16 18:28:43 UTC",
    "updated_date": "2025-04-18 14:01:42 UTC"
  },
  {
    "arxiv_id": "2403.12076v2",
    "title": "Neuron-centric Hebbian Learning",
    "authors": [
      "Andrea Ferigo",
      "Elia Cunegatti",
      "Giovanni Iacca"
    ],
    "abstract": "One of the most striking capabilities behind the learning mechanisms of the\nbrain is the adaptation, through structural and functional plasticity, of its\nsynapses. While synapses have the fundamental role of transmitting information\nacross the brain, several studies show that it is the neuron activations that\nproduce changes on synapses. Yet, most plasticity models devised for artificial\nNeural Networks (NNs), e.g., the ABCD rule, focus on synapses, rather than\nneurons, therefore optimizing synaptic-specific Hebbian parameters. This\napproach, however, increases the complexity of the optimization process since\neach synapse is associated to multiple Hebbian parameters. To overcome this\nlimitation, we propose a novel plasticity model, called Neuron-centric Hebbian\nLearning (NcHL), where optimization focuses on neuron- rather than\nsynaptic-specific Hebbian parameters. Compared to the ABCD rule, NcHL reduces\nthe parameters from $5W$ to $5N$, being $W$ and $N$ the number of weights and\nneurons, and usually $N \\ll W$. We also devise a ``weightless'' NcHL model,\nwhich requires less memory by approximating the weights based on a record of\nneuron activations. Our experiments on two robotic locomotion tasks reveal that\nNcHL performs comparably to the ABCD rule, despite using up to $\\sim97$ times\nless parameters, thus allowing for scalable plasticity",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted at Genetic and Evolutionary Computation Conference (GECCO\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2403.12076v2",
    "published_date": "2024-02-16 17:38:28 UTC",
    "updated_date": "2024-04-16 08:19:47 UTC"
  },
  {
    "arxiv_id": "2402.10846v1",
    "title": "FedD2S: Personalized Data-Free Federated Knowledge Distillation",
    "authors": [
      "Kawa Atapour",
      "S. Jamal Seyedmohammadi",
      "Jamshid Abouei",
      "Arash Mohammadi",
      "Konstantinos N. Plataniotis"
    ],
    "abstract": "This paper addresses the challenge of mitigating data heterogeneity among\nclients within a Federated Learning (FL) framework. The model-drift issue,\narising from the noniid nature of client data, often results in suboptimal\npersonalization of a global model compared to locally trained models for each\nclient. To tackle this challenge, we propose a novel approach named FedD2S for\nPersonalized Federated Learning (pFL), leveraging knowledge distillation.\nFedD2S incorporates a deep-to-shallow layer-dropping mechanism in the data-free\nknowledge distillation process to enhance local model personalization. Through\nextensive simulations on diverse image datasets-FEMNIST, CIFAR10, CINIC0, and\nCIFAR100-we compare FedD2S with state-of-the-art FL baselines. The proposed\napproach demonstrates superior performance, characterized by accelerated\nconvergence and improved fairness among clients. The introduced layer-dropping\ntechnique effectively captures personalized knowledge, resulting in enhanced\nperformance compared to alternative FL models. Moreover, we investigate the\nimpact of key hyperparameters, such as the participation ratio and\nlayer-dropping rate, providing valuable insights into the optimal configuration\nfor FedD2S. The findings demonstrate the efficacy of adaptive layer-dropping in\nthe knowledge distillation process to achieve enhanced personalization and\nperformance across diverse datasets and tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10846v1",
    "published_date": "2024-02-16 17:36:51 UTC",
    "updated_date": "2024-02-16 17:36:51 UTC"
  },
  {
    "arxiv_id": "2402.10837v1",
    "title": "Pedipulate: Enabling Manipulation Skills using a Quadruped Robot's Leg",
    "authors": [
      "Philip Arm",
      "Mayank Mittal",
      "Hendrik Kolvenbach",
      "Marco Hutter"
    ],
    "abstract": "Legged robots have the potential to become vital in maintenance, home\nsupport, and exploration scenarios. In order to interact with and manipulate\ntheir environments, most legged robots are equipped with a dedicated robot arm,\nwhich means additional mass and mechanical complexity compared to standard\nlegged robots. In this work, we explore pedipulation - using the legs of a\nlegged robot for manipulation. By training a reinforcement learning policy that\ntracks position targets for one foot, we enable a dedicated pedipulation\ncontroller that is robust to disturbances, has a large workspace through\nwhole-body behaviors, and can reach far-away targets with gait emergence,\nenabling loco-pedipulation. By deploying our controller on a quadrupedal robot\nusing teleoperation, we demonstrate various real-world tasks such as door\nopening, sample collection, and pushing obstacles. We demonstrate load carrying\nof more than 2.0 kg at the foot. Additionally, the controller is robust to\ninteraction forces at the foot, disturbances at the base, and slippery contact\nsurfaces. Videos of the experiments are available at\nhttps://sites.google.com/leggedrobotics.com/pedipulate.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Project website:\n  https://sites.google.com/leggedrobotics.com/pedipulate",
    "pdf_url": "http://arxiv.org/pdf/2402.10837v1",
    "published_date": "2024-02-16 17:20:45 UTC",
    "updated_date": "2024-02-16 17:20:45 UTC"
  },
  {
    "arxiv_id": "2402.11000v3",
    "title": "ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment",
    "authors": [
      "Yangyifei Luo",
      "Zhuo Chen",
      "Lingbing Guo",
      "Qian Li",
      "Wenxuan Zeng",
      "Zhixin Cai",
      "Jianxin Li"
    ],
    "abstract": "Entity alignment (EA) aims to identify entities across different knowledge\ngraphs that represent the same real-world objects. Recent embedding-based EA\nmethods have achieved state-of-the-art performance in EA yet faced\ninterpretability challenges as they purely rely on the embedding distance and\nneglect the logic rules behind a pair of aligned entities. In this paper, we\npropose the Align-Subgraph Entity Alignment (ASGEA) framework to exploit logic\nrules from Align-Subgraphs. ASGEA uses anchor links as bridges to construct\nAlign-Subgraphs and spreads along the paths across KGs, which distinguishes it\nfrom the embedding-based methods. Furthermore, we design an interpretable\nPath-based Graph Neural Network, ASGNN, to effectively identify and integrate\nthe logic rules across KGs. We also introduce a node-level multi-modal\nattention mechanism coupled with multi-modal enriched anchors to augment the\nAlign-Subgraph. Our experimental results demonstrate the superior performance\nof ASGEA over the existing embedding-based methods in both EA and Multi-Modal\nEA (MMEA) tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Ongoing work; 16 pages, 9 Tables, 8 Figures; Code:\n  https://github.com/lyyf2002/ASGEA",
    "pdf_url": "http://arxiv.org/pdf/2402.11000v3",
    "published_date": "2024-02-16 17:03:05 UTC",
    "updated_date": "2025-02-25 03:55:35 UTC"
  },
  {
    "arxiv_id": "2403.04769v2",
    "title": "Using Hallucinations to Bypass GPT4's Filter",
    "authors": [
      "Benjamin Lemkin"
    ],
    "abstract": "Large language models (LLMs) are initially trained on vast amounts of data,\nthen fine-tuned using reinforcement learning from human feedback (RLHF); this\nalso serves to teach the LLM to provide appropriate and safe responses. In this\npaper, we present a novel method to manipulate the fine-tuned version into\nreverting to its pre-RLHF behavior, effectively erasing the model's filters;\nthe exploit currently works for GPT4, Claude Sonnet, and (to some extent) for\nInflection-2.5. Unlike other jailbreaks (for example, the popular \"Do Anything\nNow\" (DAN) ), our method does not rely on instructing the LLM to override its\nRLHF policy; hence, simply modifying the RLHF process is unlikely to address\nit. Instead, we induce a hallucination involving reversed text during which the\nmodel reverts to a word bucket, effectively pausing the model's filter. We\nbelieve that our exploit presents a fundamental vulnerability in LLMs currently\nunaddressed, as well as an opportunity to better understand the inner workings\nof LLMs during hallucinations.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.04769v2",
    "published_date": "2024-02-16 17:02:53 UTC",
    "updated_date": "2024-03-11 01:21:32 UTC"
  },
  {
    "arxiv_id": "2402.10828v2",
    "title": "RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model",
    "authors": [
      "Jianhao Yuan",
      "Shuyang Sun",
      "Daniel Omeiza",
      "Bo Zhao",
      "Paul Newman",
      "Lars Kunze",
      "Matthew Gadd"
    ],
    "abstract": "We need to trust robots that use often opaque AI methods. They need to\nexplain themselves to us, and we need to trust their explanation. In this\nregard, explainability plays a critical role in trustworthy autonomous\ndecision-making to foster transparency and acceptance among end users,\nespecially in complex autonomous driving. Recent advancements in Multi-Modal\nLarge Language models (MLLMs) have shown promising potential in enhancing the\nexplainability as a driving agent by producing control predictions along with\nnatural language explanations. However, severe data scarcity due to expensive\nannotation costs and significant domain gaps between different datasets makes\nthe development of a robust and generalisable system an extremely challenging\ntask. Moreover, the prohibitively expensive training requirements of MLLM and\nthe unsolved problem of catastrophic forgetting further limit their\ngeneralisability post-deployment. To address these challenges, we present\nRAG-Driver, a novel retrieval-augmented multi-modal large language model that\nleverages in-context learning for high-performance, explainable, and\ngeneralisable autonomous driving. By grounding in retrieved expert\ndemonstration, we empirically validate that RAG-Driver achieves\nstate-of-the-art performance in producing driving action explanations,\njustifications, and control signal prediction. More importantly, it exhibits\nexceptional zero-shot generalisation capabilities to unseen environments\nwithout further training endeavours.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "14 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.10828v2",
    "published_date": "2024-02-16 16:57:18 UTC",
    "updated_date": "2024-05-29 14:44:20 UTC"
  },
  {
    "arxiv_id": "2402.10999v1",
    "title": "Analysis and Mortality Prediction using Multiclass Classification for Older Adults with Type 2 Diabetes",
    "authors": [
      "Ruchika Desure",
      "Gutha Jaya Krishna"
    ],
    "abstract": "Designing proper treatment plans to manage diabetes requires health\npractitioners to pay heed to the individuals remaining life along with the\ncomorbidities affecting them. Older adults with Type 2 Diabetes Mellitus (T2DM)\nare prone to experience premature death or even hypoglycaemia. The structured\ndataset utilized has 68 potential mortality predictors for 275,190 diabetic\nU.S. military Veterans aged 65 years or older. A new target variable is\ninvented by combining the two original target variables. Outliers are handled\nby discretizing the continuous variables. Categorical variables have been dummy\nencoded. Class balancing is achieved by random under-sampling. A benchmark\nregression model is built using Multinomial Logistic Regression with LASSO.\nChi-Squared and Information Gain are the filter-based feature selection\ntechniques utilized. Classifiers such as Multinomial Logistic Regression,\nRandom Forest, Extreme Gradient Boosting (XGBoost), and One-vs-Rest classifier\nare employed to build various models. Contrary to expectations, all the models\nhave constantly underperformed. XGBoost has given the highest accuracy of 53.03\npercent with Chi-Squared feature selection. All the models have consistently\nshown an acceptable performance for Class 3 (remaining life is more than 10\nyears), significantly low for Class 1 (remaining life is up to 5 years), and\nthe worst for Class 2 (remaining life is more than 5 but up to 10 years).\nFeatures analysis has deduced that almost all input variables are associated\nwith multiple target classes. The high dimensionality of the input data after\ndummy encoding seems to have confused the models, leading to\nmisclassifications. The approach taken in this study is ineffective in\nproducing a high-performing predictive model but lays a foundation as this\nproblem has never been viewed from a multiclass classification perspective.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.1"
    ],
    "primary_category": "cs.LG",
    "comment": "146 Pages",
    "pdf_url": "http://arxiv.org/pdf/2402.10999v1",
    "published_date": "2024-02-16 16:47:48 UTC",
    "updated_date": "2024-02-16 16:47:48 UTC"
  },
  {
    "arxiv_id": "2404.03665v1",
    "title": "Serial Parallel Reliability Redundancy Allocation Optimization for Energy Efficient and Fault Tolerant Cloud Computing",
    "authors": [
      "Gutha Jaya Krishna"
    ],
    "abstract": "Serial-parallel redundancy is a reliable way to ensure service and systems\nwill be available in cloud computing. That method involves making copies of the\nsame system or program, with only one remaining active. When an error occurs,\nthe inactive copy can step in as a backup right away, this provides continuous\nperformance and uninterrupted operation. This approach is called parallel\nredundancy, otherwise known as active-active redundancy, and its exceptional\nwhen it comes to strategy. It creates duplicates of a system or service that\nare all running at once. By doing this fault tolerance increases since if one\ncopy fails, the workload can be distributed across any replica thats\nfunctioning properly. Reliability allocation depends on features in a system\nand the availability and fault tolerance you want from it. Serial redundancy or\nparallel redundancies can be applied to increase the dependability of systems\nand services. To demonstrate how well this concept works, we looked into fixed\nserial parallel reliability redundancy allocation issues followed by using an\ninnovative hybrid optimization technique to find the best possible allocation\nfor peak dependability. We then measured our findings against other research.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "68W50",
      "I.2.11"
    ],
    "primary_category": "cs.DC",
    "comment": "5 Pages, 1 Figure, 2 Tables",
    "pdf_url": "http://arxiv.org/pdf/2404.03665v1",
    "published_date": "2024-02-16 16:46:10 UTC",
    "updated_date": "2024-02-16 16:46:10 UTC"
  },
  {
    "arxiv_id": "2404.07208v1",
    "title": "Uncertainty-guided annotation enhances segmentation with the human-in-the-loop",
    "authors": [
      "Nadieh Khalili",
      "Joey Spronck",
      "Francesco Ciompi",
      "Jeroen van der Laak",
      "Geert Litjens"
    ],
    "abstract": "Deep learning algorithms, often critiqued for their 'black box' nature,\ntraditionally fall short in providing the necessary transparency for trusted\nclinical use. This challenge is particularly evident when such models are\ndeployed in local hospitals, encountering out-of-domain distributions due to\nvarying imaging techniques and patient-specific pathologies. Yet, this\nlimitation offers a unique avenue for continual learning. The\nUncertainty-Guided Annotation (UGA) framework introduces a human-in-the-loop\napproach, enabling AI to convey its uncertainties to clinicians, effectively\nacting as an automated quality control mechanism. UGA eases this interaction by\nquantifying uncertainty at the pixel level, thereby revealing the model's\nlimitations and opening the door for clinician-guided corrections. We evaluated\nUGA on the Camelyon dataset for lymph node metastasis segmentation which\nrevealed that UGA improved the Dice coefficient (DC), from 0.66 to 0.76 by\nadding 5 patches, and further to 0.84 with 10 patches. To foster broader\napplication and community contribution, we have made our code accessible at",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.07208v1",
    "published_date": "2024-02-16 16:41:15 UTC",
    "updated_date": "2024-02-16 16:41:15 UTC"
  },
  {
    "arxiv_id": "2402.10805v1",
    "title": "Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond",
    "authors": [
      "Yongqi Li",
      "Wenjie Wang",
      "Leigang Qu",
      "Liqiang Nie",
      "Wenjie Li",
      "Tat-Seng Chua"
    ],
    "abstract": "The recent advancements in generative language models have demonstrated their\nability to memorize knowledge from documents and recall knowledge to respond to\nuser queries effectively. Building upon this capability, we propose to enable\nmultimodal large language models (MLLMs) to memorize and recall images within\ntheir parameters. Given a user query for visual content, the MLLM is\nanticipated to \"recall\" the relevant image from its parameters as the response.\nAchieving this target presents notable challenges, including inbuilt visual\nmemory and visual recall schemes within MLLMs. To address these challenges, we\nintroduce a generative cross-modal retrieval framework, which assigns unique\nidentifier strings to represent images and involves two training steps:\nlearning to memorize and learning to retrieve. The first step focuses on\ntraining the MLLM to memorize the association between images and their\nrespective identifiers. The latter step teaches the MLLM to generate the\ncorresponding identifier of the target image, given the textual query input. By\nmemorizing images in MLLMs, we introduce a new paradigm to cross-modal\nretrieval, distinct from previous discriminative approaches. The experiments\ndemonstrate that the generative paradigm performs effectively and efficiently\neven with large-scale image candidate sets.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.IR"
    ],
    "primary_category": "cs.MM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10805v1",
    "published_date": "2024-02-16 16:31:46 UTC",
    "updated_date": "2024-02-16 16:31:46 UTC"
  },
  {
    "arxiv_id": "2402.10803v1",
    "title": "Modelling crypto markets by multi-agent reinforcement learning",
    "authors": [
      "Johann Lussange",
      "Stefano Vrizzi",
      "Stefano Palminteri",
      "Boris Gutkin"
    ],
    "abstract": "Building on a previous foundation work (Lussange et al. 2020), this study\nintroduces a multi-agent reinforcement learning (MARL) model simulating crypto\nmarkets, which is calibrated to the Binance's daily closing prices of $153$\ncryptocurrencies that were continuously traded between 2018 and 2022. Unlike\nprevious agent-based models (ABM) or multi-agent systems (MAS) which relied on\nzero-intelligence agents or single autonomous agent methodologies, our approach\nrelies on endowing agents with reinforcement learning (RL) techniques in order\nto model crypto markets. This integration is designed to emulate, with a\nbottom-up approach to complexity inference, both individual and collective\nagents, ensuring robustness in the recent volatile conditions of such markets\nand during the COVID-19 era. A key feature of our model also lies in the fact\nthat its autonomous agents perform asset price valuation based on two sources\nof information: the market prices themselves, and the approximation of the\ncrypto assets fundamental values beyond what those market prices are. Our MAS\ncalibration against real market data allows for an accurate emulation of crypto\nmarkets microstructure and probing key market behaviors, in both the bearish\nand bullish regimes of that particular time period.",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.GT",
      "cs.MA"
    ],
    "primary_category": "q-fin.CP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10803v1",
    "published_date": "2024-02-16 16:28:58 UTC",
    "updated_date": "2024-02-16 16:28:58 UTC"
  },
  {
    "arxiv_id": "2402.10798v1",
    "title": "VATr++: Choose Your Words Wisely for Handwritten Text Generation",
    "authors": [
      "Bram Vanherle",
      "Vittorio Pippi",
      "Silvia Cascianelli",
      "Nick Michiels",
      "Frank Van Reeth",
      "Rita Cucchiara"
    ],
    "abstract": "Styled Handwritten Text Generation (HTG) has received significant attention\nin recent years, propelled by the success of learning-based solutions employing\nGANs, Transformers, and, preliminarily, Diffusion Models. Despite this surge in\ninterest, there remains a critical yet understudied aspect - the impact of the\ninput, both visual and textual, on the HTG model training and its subsequent\ninfluence on performance. This study delves deeper into a cutting-edge\nStyled-HTG approach, proposing strategies for input preparation and training\nregularization that allow the model to achieve better performance and\ngeneralize better. These aspects are validated through extensive analysis on\nseveral different settings and datasets. Moreover, in this work, we go beyond\nperformance optimization and address a significant hurdle in HTG research - the\nlack of a standardized evaluation protocol. In particular, we propose a\nstandardization of the evaluation protocol for HTG and conduct a comprehensive\nbenchmarking of existing approaches. By doing so, we aim to establish a\nfoundation for fair and meaningful comparisons between HTG strategies,\nfostering progress in the field.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10798v1",
    "published_date": "2024-02-16 16:21:15 UTC",
    "updated_date": "2024-02-16 16:21:15 UTC"
  },
  {
    "arxiv_id": "2402.10793v2",
    "title": "An end-to-end attention-based approach for learning on graphs",
    "authors": [
      "David Buterez",
      "Jon Paul Janet",
      "Dino Oglic",
      "Pietro Lio"
    ],
    "abstract": "There has been a recent surge in transformer-based architectures for learning\non graphs, mainly motivated by attention as an effective learning mechanism and\nthe desire to supersede handcrafted operators characteristic of message passing\nschemes. However, concerns over their empirical effectiveness, scalability, and\ncomplexity of the pre-processing steps have been raised, especially in relation\nto much simpler graph neural networks that typically perform on par with them\nacross a wide range of benchmarks. To tackle these shortcomings, we consider\ngraphs as sets of edges and propose a purely attention-based approach\nconsisting of an encoder and an attention pooling mechanism. The encoder\nvertically interleaves masked and vanilla self-attention modules to learn an\neffective representations of edges, while allowing for tackling possible\nmisspecifications in input graphs. Despite its simplicity, the approach\noutperforms fine-tuned message passing baselines and recently proposed\ntransformer-based methods on more than 70 node and graph-level tasks, including\nchallenging long-range benchmarks. Moreover, we demonstrate state-of-the-art\nperformance across different tasks, ranging from molecular to vision graphs,\nand heterophilous node classification. The approach also outperforms graph\nneural networks and transformers in transfer learning settings, and scales much\nbetter than alternatives with a similar performance level or expressive power.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10793v2",
    "published_date": "2024-02-16 16:20:11 UTC",
    "updated_date": "2024-12-06 15:44:46 UTC"
  },
  {
    "arxiv_id": "2402.10998v3",
    "title": "Provably Safe Neural Network Controllers via Differential Dynamic Logic",
    "authors": [
      "Samuel Teuber",
      "Stefan Mitsch",
      "Andr√© Platzer"
    ],
    "abstract": "While neural networks (NNs) have potential as autonomous controllers for\nCyber-Physical Systems, verifying the safety of NN based control systems\n(NNCSs) poses significant challenges for the practical use of NNs, especially\nwhen safety is needed for unbounded time horizons. One reason is the\nintractability of analyzing NNs, ODEs and hybrid systems. To this end, we\nintroduce VerSAILLE (Verifiably Safe AI via Logically Linked Envelopes): The\nfirst general approach that allows reusing control theory results for NNCS\nverification. By joining forces, we exploit the efficiency of NN verification\ntools while retaining the rigor of differential dynamic logic (dL). Based on\nprovably safe control envelopes in dL, we derive specifications for the NN\nwhich is proven via NN verification. We show that a proof of the NN adhering to\nthe specification is mirrored by a dL proof on the infinite-time safety of the\nNNCS.\n  The NN verification properties resulting from hybrid systems typically\ncontain nonlinear arithmetic and arbitrary logical structures while efficient\nNN verification merely supports linear constraints. To overcome this divide, we\npresent Mosaic: An efficient, sound and complete verification approach for\npolynomial real arithmetic properties on piece-wise linear NNs. Mosaic\npartitions complex verification queries into simple queries and lifts\noff-the-shelf linear constraint tools to the nonlinear setting in a\ncompleteness-preserving manner by combining approximation with exact reasoning\nfor counterexample regions. Our evaluation demonstrates the versatility of\nVerSAILLE and Mosaic: We prove infinite-time safety on the classical Vertical\nAirborne Collision Avoidance NNCS verification benchmark for two scenarios\nwhile (exhaustively) enumerating counterexample regions in unsafe scenarios. We\nalso show that our approach significantly outperforms State-of-the-Art tools in\nclosed-loop NNV.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.LO",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "39 pages (main paper has 10 pages), 13 figures; Accepted at the\n  Thirty-Eighth Annual Conference on Neural Information Processing Systems\n  (NeurIPS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2402.10998v3",
    "published_date": "2024-02-16 16:15:25 UTC",
    "updated_date": "2024-10-24 15:13:41 UTC"
  },
  {
    "arxiv_id": "2402.10790v2",
    "title": "In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs Miss",
    "authors": [
      "Yuri Kuratov",
      "Aydar Bulatov",
      "Petr Anokhin",
      "Dmitry Sorokin",
      "Artyom Sorokin",
      "Mikhail Burtsev"
    ],
    "abstract": "This paper addresses the challenge of processing long documents using\ngenerative transformer models. To evaluate different approaches, we introduce\nBABILong, a new benchmark designed to assess model capabilities in extracting\nand processing distributed facts within extensive texts. Our evaluation, which\nincludes benchmarks for GPT-4 and RAG, reveals that common methods are\neffective only for sequences up to $10^4$ elements. In contrast, fine-tuning\nGPT-2 with recurrent memory augmentations enables it to handle tasks involving\nup to $11\\times 10^6$ elements. This achievement marks a substantial leap, as\nit is by far the longest input processed by any neural network model to date,\ndemonstrating a significant improvement in the processing capabilities for long\nsequences.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "11M tokens, fix qa3 min facts per task in Table 1",
    "pdf_url": "http://arxiv.org/pdf/2402.10790v2",
    "published_date": "2024-02-16 16:15:01 UTC",
    "updated_date": "2024-02-21 03:07:42 UTC"
  },
  {
    "arxiv_id": "2402.10787v1",
    "title": "EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge",
    "authors": [
      "Xuan Shen",
      "Zhenglun Kong",
      "Changdi Yang",
      "Zhaoyang Han",
      "Lei Lu",
      "Peiyan Dong",
      "Cheng Lyu",
      "Chih-hsiang Li",
      "Xuehang Guo",
      "Zhihao Shu",
      "Wei Niu",
      "Miriam Leeser",
      "Pu Zhao",
      "Yanzhi Wang"
    ],
    "abstract": "Despite the remarkable strides of Large Language Models (LLMs) in various\nfields, the wide applications of LLMs on edge devices are limited due to their\nmassive parameters and computations. To address this, quantization is commonly\nadopted to generate lightweight LLMs with efficient computations and fast\ninference. However, Post-Training Quantization (PTQ) methods dramatically\ndegrade in quality when quantizing weights, activations, and KV cache together\nto below 8 bits. Besides, many Quantization-Aware Training (QAT) works quantize\nmodel weights, leaving the activations untouched, which do not fully exploit\nthe potential of quantization for inference acceleration on the edge. In this\npaper, we propose EdgeQAT, the Entropy and Distribution Guided QAT for the\noptimization of lightweight LLMs to achieve inference acceleration on Edge\ndevices. We first identify that the performance drop of quantization primarily\nstems from the information distortion in quantized attention maps, demonstrated\nby the different distributions in quantized query and key of the self-attention\nmechanism. Then, the entropy and distribution guided QAT is proposed to\nmitigate the information distortion. Moreover, we design a token\nimportance-aware adaptive method to dynamically quantize the tokens with\ndifferent bit widths for further optimization and acceleration. Our extensive\nexperiments verify the substantial improvements with our framework across\nvarious datasets. Furthermore, we achieve an on-device speedup of up to 2.37x\ncompared with its FP16 counterparts across multiple edge devices, signaling a\ngroundbreaking advancement.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2402.10787v1",
    "published_date": "2024-02-16 16:10:38 UTC",
    "updated_date": "2024-02-16 16:10:38 UTC"
  },
  {
    "arxiv_id": "2402.10778v2",
    "title": "AutoGPT+P: Affordance-based Task Planning with Large Language Models",
    "authors": [
      "Timo Birr",
      "Christoph Pohl",
      "Abdelrahman Younes",
      "Tamim Asfour"
    ],
    "abstract": "Recent advances in task planning leverage Large Language Models (LLMs) to\nimprove generalizability by combining such models with classical planning\nalgorithms to address their inherent limitations in reasoning capabilities.\nHowever, these approaches face the challenge of dynamically capturing the\ninitial state of the task planning problem. To alleviate this issue, we propose\nAutoGPT+P, a system that combines an affordance-based scene representation with\na planning system. Affordances encompass the action possibilities of an agent\non the environment and objects present in it. Thus, deriving the planning\ndomain from an affordance-based scene representation allows symbolic planning\nwith arbitrary objects. AutoGPT+P leverages this representation to derive and\nexecute a plan for a task specified by the user in natural language. In\naddition to solving planning tasks under a closed-world assumption, AutoGPT+P\ncan also handle planning with incomplete information, e. g., tasks with missing\nobjects by exploring the scene, suggesting alternatives, or providing a partial\nplan. The affordance-based scene representation combines object detection with\nan automatically generated object-affordance-mapping using ChatGPT. The core\nplanning tool extends existing work by automatically correcting semantic and\nsyntactic errors. Our approach achieves a success rate of 98%, surpassing the\ncurrent 81% success rate of the current state-of-the-art LLM-based planning\nmethod SayCan on the SayCan instruction set. Furthermore, we evaluated our\napproach on our newly created dataset with 150 scenarios covering a wide range\nof complex tasks with missing objects, achieving a success rate of 79% on our\ndataset. The dataset and the code are publicly available at\nhttps://git.h2t.iar.kit.edu/birr/autogpt-p-standalone.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "I.2"
    ],
    "primary_category": "cs.RO",
    "comment": "13 pages, 18 pages including references and appendix, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.10778v2",
    "published_date": "2024-02-16 16:00:50 UTC",
    "updated_date": "2024-07-23 14:56:03 UTC"
  },
  {
    "arxiv_id": "2402.10774v1",
    "title": "Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants",
    "authors": [
      "Peter Richt√°rik",
      "Elnur Gasanov",
      "Konstantin Burlachenko"
    ],
    "abstract": "Error Feedback (EF) is a highly popular and immensely effective mechanism for\nfixing convergence issues which arise in distributed training methods (such as\ndistributed GD or SGD) when these are enhanced with greedy communication\ncompression techniques such as TopK. While EF was proposed almost a decade ago\n(Seide et al., 2014), and despite concentrated effort by the community to\nadvance the theoretical understanding of this mechanism, there is still a lot\nto explore. In this work we study a modern form of error feedback called EF21\n(Richtarik et al., 2021) which offers the currently best-known theoretical\nguarantees, under the weakest assumptions, and also works well in practice. In\nparticular, while the theoretical communication complexity of EF21 depends on\nthe quadratic mean of certain smoothness parameters, we improve this dependence\nto their arithmetic mean, which is always smaller, and can be substantially\nsmaller, especially in heterogeneous data regimes. We take the reader on a\njourney of our discovery process. Starting with the idea of applying EF21 to an\nequivalent reformulation of the underlying problem which (unfortunately)\nrequires (often impractical) machine cloning, we continue to the discovery of a\nnew weighted version of EF21 which can (fortunately) be executed without any\ncloning, and finally circle back to an improved analysis of the original EF21\nmethod. While this development applies to the simplest form of EF21, our\napproach naturally extends to more elaborate variants involving stochastic\ngradients and partial participation. Further, our technique improves the\nbest-known theory of EF21 in the rare features regime (Richtarik et al., 2023).\nFinally, we validate our theoretical findings with suitable experiments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML",
      "90C26, 74Pxx",
      "G.1.6; I.2.11; I.2.m"
    ],
    "primary_category": "cs.LG",
    "comment": "70 pages, 14 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.10774v1",
    "published_date": "2024-02-16 15:55:59 UTC",
    "updated_date": "2024-02-16 15:55:59 UTC"
  },
  {
    "arxiv_id": "2402.10770v4",
    "title": "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?",
    "authors": [
      "Ehsan Doostmohammadi",
      "Oskar Holmstr√∂m",
      "Marco Kuhlmann"
    ],
    "abstract": "Work on instruction-tuned Large Language Models (LLMs) has used automatic\nmethods based on text overlap and LLM judgments as cost-effective alternatives\nto human evaluation. In this paper, we perform a meta-evaluation of such\nmethods and assess their reliability across a broad range of tasks. In\nevaluating how well automatic methods align with human evaluations, correlation\nmetrics are the most commonly employed method despite their inherent\nlimitations when dealing with ties and different scales. To address these\nshortcomings, we use Pairwise Accuracy as an alternative to standard\ncorrelation measures. We observe that while automatic evaluation methods can\napproximate human ratings under specific conditions, their validity is highly\ncontext-dependent. Specifically, the simple ROUGE-L metric correlates very well\nwith human ratings for short-answer English tasks but is unreliable in\nfree-form generation tasks and cross-lingual scenarios. The effectiveness of\nthe more advanced method of using GPT-4 as a judge diminishes significantly if\nreference answers are not included in the prompt, which is the scenario where\nthis method has the potential to provide the most value compared to other\nmetrics. Our findings enhance the understanding of how automatic methods should\nbe applied and interpreted when developing and evaluating instruction-tuned\nLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10770v4",
    "published_date": "2024-02-16 15:48:33 UTC",
    "updated_date": "2024-10-02 09:18:47 UTC"
  },
  {
    "arxiv_id": "2402.10769v1",
    "title": "Distillation Enhanced Generative Retrieval",
    "authors": [
      "Yongqi Li",
      "Zhen Zhang",
      "Wenjie Wang",
      "Liqiang Nie",
      "Wenjie Li",
      "Tat-Seng Chua"
    ],
    "abstract": "Generative retrieval is a promising new paradigm in text retrieval that\ngenerates identifier strings of relevant passages as the retrieval target. This\nparadigm leverages powerful generative language models, distinct from\ntraditional sparse or dense retrieval methods. In this work, we identify a\nviable direction to further enhance generative retrieval via distillation and\npropose a feasible framework, named DGR. DGR utilizes sophisticated ranking\nmodels, such as the cross-encoder, in a teacher role to supply a passage rank\nlist, which captures the varying relevance degrees of passages instead of\nbinary hard labels; subsequently, DGR employs a specially designed distilled\nRankNet loss to optimize the generative retrieval model, considering the\npassage rank order provided by the teacher model as labels. This framework only\nrequires an additional distillation step to enhance current generative\nretrieval systems and does not add any burden to the inference stage. We\nconduct experiments on four public datasets, and the results indicate that DGR\nachieves state-of-the-art performance among the generative retrieval methods.\nAdditionally, DGR demonstrates exceptional robustness and generalizability with\nvarious teacher models and distillation losses.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10769v1",
    "published_date": "2024-02-16 15:48:24 UTC",
    "updated_date": "2024-02-16 15:48:24 UTC"
  },
  {
    "arxiv_id": "2402.10767v2",
    "title": "Inference to the Best Explanation in Large Language Models",
    "authors": [
      "Dhairya Dalal",
      "Marco Valentino",
      "Andr√© Freitas",
      "Paul Buitelaar"
    ],
    "abstract": "While Large Language Models (LLMs) have found success in real-world\napplications, their underlying explanatory process is still poorly understood.\nThis paper proposes IBE-Eval, a framework inspired by philosophical accounts on\nInference to the Best Explanation (IBE) to advance the interpretation and\nevaluation of LLMs' explanations. IBE-Eval estimates the plausibility of\nnatural language explanations through a combination of explicit logical and\nlinguistic features including: consistency, parsimony, coherence, and\nuncertainty. Extensive experiments are conducted on Causal Question Answering\n(CQA), where \\textit{IBE-Eval} is tasked to select the most plausible causal\nexplanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama\n2). The experiments reveal that IBE-Eval can successfully identify the best\nexplanation with up to 77\\% accuracy ($\\approx 27\\%$ above random), improving\nupon a GPT 3.5-as-a-Judge baseline ($\\approx+17\\%$) while being intrinsically\nmore efficient and interpretable. Additional analyses suggest that, despite\nmodel-specific variances, LLM-generated explanations tend to conform to IBE\ncriteria and that IBE-Eval is significantly correlated with human judgment,\nopening up opportunities for future development of automated explanation\nverification tools.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10767v2",
    "published_date": "2024-02-16 15:41:23 UTC",
    "updated_date": "2025-03-02 20:33:20 UTC"
  },
  {
    "arxiv_id": "2402.10765v1",
    "title": "Policy Learning for Off-Dynamics RL with Deficient Support",
    "authors": [
      "Linh Le Pham Van",
      "Hung The Tran",
      "Sunil Gupta"
    ],
    "abstract": "Reinforcement Learning (RL) can effectively learn complex policies. However,\nlearning these policies often demands extensive trial-and-error interactions\nwith the environment. In many real-world scenarios, this approach is not\npractical due to the high costs of data collection and safety concerns. As a\nresult, a common strategy is to transfer a policy trained in a low-cost, rapid\nsource simulator to a real-world target environment. However, this process\nposes challenges. Simulators, no matter how advanced, cannot perfectly\nreplicate the intricacies of the real world, leading to dynamics discrepancies\nbetween the source and target environments. Past research posited that the\nsource domain must encompass all possible target transitions, a condition we\nterm full support. However, expecting full support is often unrealistic,\nespecially in scenarios where significant dynamics discrepancies arise. In this\npaper, our emphasis shifts to addressing large dynamics mismatch adaptation. We\nmove away from the stringent full support condition of earlier research,\nfocusing instead on crafting an effective policy for the target domain. Our\nproposed approach is simple but effective. It is anchored in the central\nconcepts of the skewing and extension of source support towards target support\nto mitigate support deficiencies. Through comprehensive testing on a varied set\nof benchmarks, our method's efficacy stands out, showcasing notable\nimprovements over previous techniques.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAMAS 2024 as a full paper",
    "pdf_url": "http://arxiv.org/pdf/2402.10765v1",
    "published_date": "2024-02-16 15:39:51 UTC",
    "updated_date": "2024-02-16 15:39:51 UTC"
  },
  {
    "arxiv_id": "2402.10762v1",
    "title": "On Explaining Unfairness: An Overview",
    "authors": [
      "Christos Fragkathoulas",
      "Vasiliki Papanikou",
      "Danae Pla Karidi",
      "Evaggelia Pitoura"
    ],
    "abstract": "Algorithmic fairness and explainability are foundational elements for\nachieving responsible AI. In this paper, we focus on their interplay, a\nresearch area that is recently receiving increasing attention. To this end, we\nfirst present two comprehensive taxonomies, each representing one of the two\ncomplementary fields of study: fairness and explanations. Then, we categorize\nexplanations for fairness into three types: (a) Explanations to enhance\nfairness metrics, (b) Explanations to help us understand the causes of\n(un)fairness, and (c) Explanations to assist us in designing methods for\nmitigating unfairness. Finally, based on our fairness and explanation\ntaxonomies, we present undiscovered literature paths revealing gaps that can\nserve as valuable insights for future research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10762v1",
    "published_date": "2024-02-16 15:38:00 UTC",
    "updated_date": "2024-02-16 15:38:00 UTC"
  },
  {
    "arxiv_id": "2402.13270v1",
    "title": "Global Tropical Cyclone Intensity Forecasting with Multi-modal Multi-scale Causal Autoregressive Model",
    "authors": [
      "Xinyu Wang",
      "Kang Chen",
      "Lei Liu",
      "Tao Han",
      "Bin Li",
      "Lei Bai"
    ],
    "abstract": "Accurate forecasting of Tropical cyclone (TC) intensity is crucial for\nformulating disaster risk reduction strategies. Current methods predominantly\nrely on limited spatiotemporal information from ERA5 data and neglect the\ncausal relationships between these physical variables, failing to fully capture\nthe spatial and temporal patterns required for intensity forecasting. To\naddress this issue, we propose a Multi-modal multi-Scale Causal AutoRegressive\nmodel (MSCAR), which is the first model that combines causal relationships with\nlarge-scale multi-modal data for global TC intensity autoregressive\nforecasting. Furthermore, given the current absence of a TC dataset that offers\na wide range of spatial variables, we present the Satellite and ERA5-based\nTropical Cyclone Dataset (SETCD), which stands as the longest and most\ncomprehensive global dataset related to TCs. Experiments on the dataset show\nthat MSCAR outperforms the state-of-the-art methods, achieving maximum\nreductions in global and regional forecast errors of 9.52% and 6.74%,\nrespectively. The code and dataset are publicly available at\nhttps://anonymous.4open.science/r/MSCAR.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.LG",
      "physics.data-an"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.13270v1",
    "published_date": "2024-02-16 15:26:33 UTC",
    "updated_date": "2024-02-16 15:26:33 UTC"
  },
  {
    "arxiv_id": "2402.10756v1",
    "title": "Towards Cohesion-Fairness Harmony: Contrastive Regularization in Individual Fair Graph Clustering",
    "authors": [
      "Siamak Ghodsi",
      "Seyed Amjad Seyedi",
      "Eirini Ntoutsi"
    ],
    "abstract": "Conventional fair graph clustering methods face two primary challenges: i)\nThey prioritize balanced clusters at the expense of cluster cohesion by\nimposing rigid constraints, ii) Existing methods of both individual and\ngroup-level fairness in graph partitioning mostly rely on eigen decompositions\nand thus, generally lack interpretability. To address these issues, we propose\niFairNMTF, an individual Fairness Nonnegative Matrix Tri-Factorization model\nwith contrastive fairness regularization that achieves balanced and cohesive\nclusters. By introducing fairness regularization, our model allows for\ncustomizable accuracy-fairness trade-offs, thereby enhancing user autonomy\nwithout compromising the interpretability provided by nonnegative matrix\ntri-factorization. Experimental evaluations on real and synthetic datasets\ndemonstrate the superior flexibility of iFairNMTF in achieving fairness and\nclustering performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "cs.SI",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "To be published in \"The 28th Pacific-Asia Conference on Knowledge\n  Discovery and Data Mining (PAKDD 2024)\"",
    "pdf_url": "http://arxiv.org/pdf/2402.10756v1",
    "published_date": "2024-02-16 15:25:56 UTC",
    "updated_date": "2024-02-16 15:25:56 UTC"
  },
  {
    "arxiv_id": "2402.10753v2",
    "title": "ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages",
    "authors": [
      "Junjie Ye",
      "Sixian Li",
      "Guanyu Li",
      "Caishuang Huang",
      "Songyang Gao",
      "Yilong Wu",
      "Qi Zhang",
      "Tao Gui",
      "Xuanjing Huang"
    ],
    "abstract": "Tool learning is widely acknowledged as a foundational approach or deploying\nlarge language models (LLMs) in real-world scenarios. While current research\nprimarily emphasizes leveraging tools to augment LLMs, it frequently neglects\nemerging safety considerations tied to their application. To fill this gap, we\npresent *ToolSword*, a comprehensive framework dedicated to meticulously\ninvestigating safety issues linked to LLMs in tool learning. Specifically,\nToolSword delineates six safety scenarios for LLMs in tool learning,\nencompassing **malicious queries** and **jailbreak attacks** in the input\nstage, **noisy misdirection** and **risky cues** in the execution stage, and\n**harmful feedback** and **error conflicts** in the output stage. Experiments\nconducted on 11 open-source and closed-source LLMs reveal enduring safety\nchallenges in tool learning, such as handling harmful queries, employing risky\ntools, and delivering detrimental feedback, which even GPT-4 is susceptible to.\nMoreover, we conduct further studies with the aim of fostering research on tool\nlearning safety. The data is released in\nhttps://github.com/Junjie-Ye/ToolSword.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL 2024 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2402.10753v2",
    "published_date": "2024-02-16 15:19:46 UTC",
    "updated_date": "2024-08-16 04:12:00 UTC"
  },
  {
    "arxiv_id": "2402.10747v1",
    "title": "Fully Differentiable Lagrangian Convolutional Neural Network for Continuity-Consistent Physics-Informed Precipitation Nowcasting",
    "authors": [
      "Peter Pavl√≠k",
      "Martin V√Ωboh",
      "Anna Bou Ezzeddine",
      "Viera Rozinajov√°"
    ],
    "abstract": "This paper presents a convolutional neural network model for precipitation\nnowcasting that combines data-driven learning with physics-informed domain\nknowledge. We propose LUPIN, a Lagrangian Double U-Net for Physics-Informed\nNowcasting, that draws from existing extrapolation-based nowcasting methods and\nimplements the Lagrangian coordinate system transformation of the data in a\nfully differentiable and GPU-accelerated manner to allow for real-time\nend-to-end training and inference. Based on our evaluation, LUPIN matches and\nexceeds the performance of the chosen benchmark, opening the door for other\nLagrangian machine learning models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "I.2.1; J.2"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.10747v1",
    "published_date": "2024-02-16 15:13:30 UTC",
    "updated_date": "2024-02-16 15:13:30 UTC"
  },
  {
    "arxiv_id": "2402.10744v1",
    "title": "GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models",
    "authors": [
      "Pengcheng Jiang",
      "Jiacheng Lin",
      "Zifeng Wang",
      "Jimeng Sun",
      "Jiawei Han"
    ],
    "abstract": "The field of relation extraction (RE) is experiencing a notable shift towards\ngenerative relation extraction (GRE), leveraging the capabilities of large\nlanguage models (LLMs). However, we discovered that traditional relation\nextraction (RE) metrics like precision and recall fall short in evaluating GRE\nmethods. This shortfall arises because these metrics rely on exact matching\nwith human-annotated reference relations, while GRE methods often produce\ndiverse and semantically accurate relations that differ from the references. To\nfill this gap, we introduce GenRES for a multi-dimensional assessment in terms\nof the topic similarity, uniqueness, granularity, factualness, and completeness\nof the GRE results. With GenRES, we empirically identified that (1)\nprecision/recall fails to justify the performance of GRE methods; (2)\nhuman-annotated referential relations can be incomplete; (3) prompting LLMs\nwith a fixed set of relations or entities can cause hallucinations. Next, we\nconducted a human evaluation of GRE methods that shows GenRES is consistent\nwith human preferences for RE quality. Last, we made a comprehensive evaluation\nof fourteen leading LLMs using GenRES across document, bag, and sentence level\nRE datasets, respectively, to set the benchmark for future research in GRE",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10744v1",
    "published_date": "2024-02-16 15:01:24 UTC",
    "updated_date": "2024-02-16 15:01:24 UTC"
  },
  {
    "arxiv_id": "2404.04261v1",
    "title": "A Novel BERT-based Classifier to Detect Political Leaning of YouTube Videos based on their Titles",
    "authors": [
      "Nouar AlDahoul",
      "Talal Rahwan",
      "Yasir Zaki"
    ],
    "abstract": "A quarter of US adults regularly get their news from YouTube. Yet, despite\nthe massive political content available on the platform, to date no classifier\nhas been proposed to identify the political leaning of YouTube videos. To fill\nthis gap, we propose a novel classifier based on Bert -- a language model from\nGoogle -- to classify YouTube videos merely based on their titles into six\ncategories, namely: Far Left, Left, Center, Anti-Woke, Right, and Far Right. We\nused a public dataset of 10 million YouTube video titles (under various\ncategories) to train and validate the proposed classifier. We compare the\nclassifier against several alternatives that we trained on the same dataset,\nrevealing that our classifier achieves the highest accuracy (75%) and the\nhighest F1 score (77%). To further validate the classification performance, we\ncollect videos from YouTube channels of numerous prominent news agencies, such\nas Fox News and New York Times, which have widely known political leanings, and\napply our classifier to their video titles. For the vast majority of cases, the\npredicted political leaning matches that of the news agency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.04261v1",
    "published_date": "2024-02-16 14:44:30 UTC",
    "updated_date": "2024-02-16 14:44:30 UTC"
  },
  {
    "arxiv_id": "2402.10726v4",
    "title": "Planning Domain Model Acquisition from State Traces without Action Parameters",
    "authors": [
      "Tom√°≈° Balyo",
      "Martin Suda",
      "Luk√°≈° Chrpa",
      "Dominik ≈†afr√°nek",
      "Stephan Gocht",
      "Filip Dvo≈ô√°k",
      "Roman Bart√°k",
      "G. Michael Youngblood"
    ],
    "abstract": "Existing planning action domain model acquisition approaches consider\ndifferent types of state traces from which they learn. The differences in state\ntraces refer to the level of observability of state changes (from full to none)\nand whether the observations have some noise (the state changes might be\ninaccurately logged). However, to the best of our knowledge, all the existing\napproaches consider state traces in which each state change corresponds to an\naction specified by its name and all its parameters (all objects that are\nrelevant to the action). Furthermore, the names and types of all the parameters\nof the actions to be learned are given. These assumptions are too strong.\n  In this paper, we propose a method that learns action schema from state\ntraces with fully observable state changes but without the parameters of\nactions responsible for the state changes (only action names are part of the\nstate traces). Although we can easily deduce the number (and names) of the\nactions that will be in the learned domain model, we still need to deduce the\nnumber and types of the parameters of each action alongside its precondition\nand effects. We show that this task is at least as hard as graph isomorphism.\nHowever, our experimental evaluation on a large collection of IPC benchmarks\nshows that our approach is still practical as the number of required parameters\nis usually small.\n  Compared to the state-of-the-art learning tools SAM and Extended SAM our new\nalgorithm is able to provide better results in multiple domains in terms of\nlearning action models more similar to reference models, even though it uses\nless information and has fewer restrictions on the input traces.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10726v4",
    "published_date": "2024-02-16 14:36:58 UTC",
    "updated_date": "2025-03-07 10:45:07 UTC"
  },
  {
    "arxiv_id": "2402.10725v2",
    "title": "Cloud Kitchen: Using Planning-based Composite AI to Optimize Food Delivery Processes",
    "authors": [
      "Slavom√≠r ≈†vanc√°r",
      "Luk√°≈° Chrpa",
      "Filip Dvo≈ô√°k",
      "Tom√°≈° Balyo"
    ],
    "abstract": "The global food delivery market provides many opportunities for AI-based\nservices that can improve the efficiency of feeding the world. This paper\npresents the Cloud Kitchen platform as a decision-making tool for restaurants\nwith food delivery and a simulator to evaluate the impact of the decisions. The\nplatform contains a Technology-Specific Bridge (TSB) that provides an interface\nfor communicating with restaurants or the simulator. TSB uses a planning domain\nmodel to represent decisions embedded in the Unified Planning Framework (UPF).\nDecision-making, which concerns allocating customers' orders to vehicles and\ndeciding in which order the customers will be served (for each vehicle), is\ndone via a Vehicle Routing Problem with Time Windows (VRPTW), an efficient tool\nfor this problem. We show that decisions made by our platform can improve\ncustomer satisfaction by reducing the number of delayed deliveries using a\nreal-world historical dataset.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10725v2",
    "published_date": "2024-02-16 14:31:33 UTC",
    "updated_date": "2024-08-20 12:38:36 UTC"
  },
  {
    "arxiv_id": "2402.12393v2",
    "title": "On Automating Video Game Regression Testing by Planning and Learning",
    "authors": [
      "Tom√°≈° Balyo",
      "G. Michael Youngblood",
      "Filip Dvo≈ô√°k",
      "Luk√°≈° Chrpa",
      "Roman Bart√°k"
    ],
    "abstract": "In this paper, we propose a method and workflow for automating regression\ntesting of certain video game aspects using automated planning and incremental\naction model learning techniques. The basic idea is to use detailed game logs\nand incremental action model learning techniques to maintain a formal model in\nthe planning domain description language (PDDL) of the gameplay mechanics. The\nworkflow enables efficient cooperation of game developers without any\nexperience with PDDL or other formal systems and a person experienced with PDDL\nmodeling but no game development skills. We describe the method and workflow in\ngeneral and then demonstrate it on a concrete proof-of-concept example -- a\nsimple role-playing game provided as one of the tutorial projects in the\npopular game development engine Unity. This paper presents the first step\ntowards minimizing or even eliminating the need for a modeling expert in the\nworkflow, thus making automated planning accessible to a broader audience.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12393v2",
    "published_date": "2024-02-16 14:28:25 UTC",
    "updated_date": "2024-04-02 09:16:14 UTC"
  },
  {
    "arxiv_id": "2402.10992v1",
    "title": "\"Understanding AI\": Semantic Grounding in Large Language Models",
    "authors": [
      "Holger Lyre"
    ],
    "abstract": "Do LLMs understand the meaning of the texts they generate? Do they possess a\nsemantic grounding? And how could we understand whether and what they\nunderstand? I start the paper with the observation that we have recently\nwitnessed a generative turn in AI, since generative models, including LLMs, are\nkey for self-supervised learning. To assess the question of semantic grounding,\nI distinguish and discuss five methodological ways. The most promising way is\nto apply core assumptions of theories of meaning in philosophy of mind and\nlanguage to LLMs. Grounding proves to be a gradual affair with a\nthree-dimensional distinction between functional, social and causal grounding.\nLLMs show basic evidence in all three dimensions. A strong argument is that\nLLMs develop world models. Hence, LLMs are neither stochastic parrots nor\nsemantic zombies, but already understand the language they generate, at least\nin an elementary sense.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10992v1",
    "published_date": "2024-02-16 14:23:55 UTC",
    "updated_date": "2024-02-16 14:23:55 UTC"
  },
  {
    "arxiv_id": "2402.10717v2",
    "title": "BioFusionNet: Deep Learning-Based Survival Risk Stratification in ER+ Breast Cancer Through Multifeature and Multimodal Data Fusion",
    "authors": [
      "Raktim Kumar Mondol",
      "Ewan K. A. Millar",
      "Arcot Sowmya",
      "Erik Meijering"
    ],
    "abstract": "Breast cancer is a significant health concern affecting millions of women\nworldwide. Accurate survival risk stratification plays a crucial role in\nguiding personalised treatment decisions and improving patient outcomes. Here\nwe present BioFusionNet, a deep learning framework that fuses image-derived\nfeatures with genetic and clinical data to obtain a holistic profile and\nachieve survival risk stratification of ER+ breast cancer patients. We employ\nmultiple self-supervised feature extractors (DINO and MoCoV3) pretrained on\nhistopathological patches to capture detailed image features. These features\nare then fused by a variational autoencoder and fed to a self-attention network\ngenerating patient-level features. A co-dual-cross-attention mechanism combines\nthe histopathological features with genetic data, enabling the model to capture\nthe interplay between them. Additionally, clinical data is incorporated using a\nfeed-forward network, further enhancing predictive performance and achieving\ncomprehensive multimodal feature integration. Furthermore, we introduce a\nweighted Cox loss function, specifically designed to handle imbalanced survival\ndata, which is a common challenge. Our model achieves a mean concordance index\nof 0.77 and a time-dependent area under the curve of 0.84, outperforming\nstate-of-the-art methods. It predicts risk (high versus low) with prognostic\nsignificance for overall survival in univariate analysis (HR=2.99, 95% CI:\n1.88--4.78, p<0.005), and maintains independent significance in multivariate\nanalysis incorporating standard clinicopathological variables (HR=2.91, 95\\%\nCI: 1.80--4.68, p<0.005).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Keywords: Multimodal Fusion, Breast Cancer, Whole Slide Images, Deep\n  Neural Network, Survival Prediction",
    "pdf_url": "http://arxiv.org/pdf/2402.10717v2",
    "published_date": "2024-02-16 14:19:33 UTC",
    "updated_date": "2024-06-03 02:14:12 UTC"
  },
  {
    "arxiv_id": "2402.10712v3",
    "title": "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference",
    "authors": [
      "Atsuki Yamaguchi",
      "Aline Villavicencio",
      "Nikolaos Aletras"
    ],
    "abstract": "The development of state-of-the-art generative large language models (LLMs)\ndisproportionately relies on English-centric tokenizers, vocabulary and\npre-training data. Despite the fact that some LLMs have multilingual\ncapabilities, recent studies have shown that their inference efficiency\ndeteriorates when generating text in languages other than English. This results\nin increased inference time and costs. Cross-lingual vocabulary adaptation\n(CVA) methods have been proposed for adapting models to a target language\naiming to improve downstream performance. However, the effectiveness of these\nmethods on increasing inference efficiency of generative LLMs has yet to be\nexplored. In this paper, we perform an empirical study of five CVA methods on\nfour generative LLMs (including monolingual and multilingual models) across\nfour typologically-diverse languages and four natural language understanding\ntasks. We find that CVA substantially contributes to LLM inference speedups of\nup to 271.5\\%. We also show that adapting LLMs that have been pre-trained on\nmore balanced multilingual data results in downstream performance comparable to\nthe original models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EMNLP 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2402.10712v3",
    "published_date": "2024-02-16 14:15:15 UTC",
    "updated_date": "2024-09-26 11:15:14 UTC"
  },
  {
    "arxiv_id": "2402.10705v3",
    "title": "AutoSAT: Automatically Optimize SAT Solvers via Large Language Models",
    "authors": [
      "Yiwen Sun",
      "Furong Ye",
      "Xianyin Zhang",
      "Shiyu Huang",
      "Bingzhen Zhang",
      "Ke Wei",
      "Shaowei Cai"
    ],
    "abstract": "Conflict-Driven Clause Learning (CDCL) is the mainstream framework for\nsolving the Satisfiability problem (SAT), and CDCL solvers typically rely on\nvarious heuristics, which have a significant impact on their performance.\nModern CDCL solvers, such as MiniSat and Kissat, commonly incorporate several\nheuristics and select one to use according to simple rules, requiring\nsignificant time and expert effort to fine-tune in practice. The pervasion of\nLarge Language Models (LLMs) provides a potential solution to address this\nissue. However, generating a CDCL solver from scratch is not effective due to\nthe complexity and context volume of SAT solvers. Instead, we propose AutoSAT,\na framework that automatically optimizes heuristics in a pre-defined modular\nsearch space based on existing CDCL solvers. Unlike existing automated\nalgorithm design approaches focusing on hyperparameter tuning and operator\nselection, AutoSAT can generate new efficient heuristics. In this first attempt\nat optimizing SAT solvers using LLMs, several strategies including the greedy\nhill climber and (1+1) Evolutionary Algorithm are employed to guide LLMs to\nsearch for better heuristics. Experimental results demonstrate that LLMs can\ngenerally enhance the performance of CDCL solvers. A realization of AutoSAT\noutperforms MiniSat on 9 out of 12 datasets and even surpasses the\nstate-of-the-art hybrid solver Kissat on 4 datasets.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10705v3",
    "published_date": "2024-02-16 14:04:56 UTC",
    "updated_date": "2024-11-13 15:46:08 UTC"
  },
  {
    "arxiv_id": "2402.10701v1",
    "title": "Does Twinning Vehicular Networks Enhance Their Performance in Dense Areas?",
    "authors": [
      "Sarah Al-Shareeda",
      "Sema F. Oktug",
      "Yusuf Yaslan",
      "Gokhan Yurdakul",
      "Berk Canberk"
    ],
    "abstract": "This paper investigates the potential of Digital Twins (DTs) to enhance\nnetwork performance in densely populated urban areas, specifically focusing on\nvehicular networks. The study comprises two phases. In Phase I, we utilize\ntraffic data and AI clustering to identify critical locations, particularly in\ncrowded urban areas with high accident rates. In Phase II, we evaluate the\nadvantages of twinning vehicular networks through three deployment scenarios:\nedge-based twin, cloud-based twin, and hybrid-based twin. Our analysis\ndemonstrates that twinning significantly reduces network delays, with virtual\ntwins outperforming physical networks. Virtual twins maintain low delays even\nwith increased vehicle density, such as 15.05 seconds for 300 vehicles.\nMoreover, they exhibit faster computational speeds, with cloud-based twins\nbeing 1.7 times faster than edge twins in certain scenarios. These findings\nprovide insights for efficient vehicular communication and underscore the\npotential of virtual twins in enhancing vehicular networks in crowded areas\nwhile emphasizing the importance of considering real-world factors when making\ndeployment decisions.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "6 pages, 8 figures, 2tables, conference paper",
    "pdf_url": "http://arxiv.org/pdf/2402.10701v1",
    "published_date": "2024-02-16 14:02:28 UTC",
    "updated_date": "2024-02-16 14:02:28 UTC"
  },
  {
    "arxiv_id": "2402.10695v2",
    "title": "Unlink to Unlearn: Simplifying Edge Unlearning in GNNs",
    "authors": [
      "Jiajun Tan",
      "Fei Sun",
      "Ruichen Qiu",
      "Du Su",
      "Huawei Shen"
    ],
    "abstract": "As concerns over data privacy intensify, unlearning in Graph Neural Networks\n(GNNs) has emerged as a prominent research frontier in academia. This concept\nis pivotal in enforcing the \\textit{right to be forgotten}, which entails the\nselective removal of specific data from trained GNNs upon user request. Our\nresearch focuses on edge unlearning, a process of particular relevance to\nreal-world applications. Current state-of-the-art approaches like GNNDelete can\neliminate the influence of specific edges yet suffer from\n\\textit{over-forgetting}, which means the unlearning process inadvertently\nremoves excessive information beyond needed, leading to a significant\nperformance decline for remaining edges. Our analysis identifies the loss\nfunctions of GNNDelete as the primary source of over-forgetting and also\nsuggests that loss functions may be redundant for effective edge unlearning.\nBuilding on these insights, we simplify GNNDelete to develop \\textbf{Unlink to\nUnlearn} (UtU), a novel method that facilitates unlearning exclusively through\nunlinking the forget edges from graph structure. Our extensive experiments\ndemonstrate that UtU delivers privacy protection on par with that of a\nretrained model while preserving high accuracy in downstream tasks, by\nupholding over 97.3\\% of the retrained model's privacy protection capabilities\nand 99.8\\% of its link prediction accuracy. Meanwhile, UtU requires only\nconstant computational demands, underscoring its advantage as a highly\nlightweight and practical edge unlearning solution.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by WWW 2024 as a Short Research Paper",
    "pdf_url": "http://arxiv.org/pdf/2402.10695v2",
    "published_date": "2024-02-16 13:58:23 UTC",
    "updated_date": "2024-03-11 17:08:36 UTC"
  },
  {
    "arxiv_id": "2402.10685v2",
    "title": "LongHeads: Multi-Head Attention is Secretly a Long Context Processor",
    "authors": [
      "Yi Lu",
      "Xin Zhou",
      "Wei He",
      "Jun Zhao",
      "Tao Ji",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ],
    "abstract": "Large language models (LLMs) have achieved impressive performance in numerous\ndomains but often struggle to process lengthy inputs effectively and\nefficiently due to limited length generalization and attention's quadratic\ncomputational demands. Many sought to mitigate this by restricting the\nattention window within the pre-trained length. However, these methods\nintroduce new issues such as ignoring the middle context and requiring\nadditional training. To address these problems, we propose LongHeads, a\ntraining-free framework that enhances LLM's long context ability by unlocking\nmulti-head attention's untapped potential. Instead of allowing each head to\nattend to the full sentence, which struggles with generalizing to longer\nsequences due to out-of-distribution (OOD) issues, we allow each head to\nprocess in-distribution length by selecting and attending to important context\nchunks. To this end, we propose a chunk selection strategy that relies on the\ninherent correlation between the query and the key representations, efficiently\ndistributing context chunks to different heads. In this way, each head ensures\nit can effectively process attended tokens within the trained length, while\ndifferent heads in different layers can collectively process longer contexts.\nLongHeads works efficiently in linear time, fits seamlessly with many LLMs that\nuse relative positional encoding. LongHeads achieves 100% accuracy at the 128k\nlength on passkey retrieval task, verifying LongHeads's efficacy in extending\nthe usable context window for existing models. We release our code at\nhttps://github.com/LuLuLuyi/LongHeads .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10685v2",
    "published_date": "2024-02-16 13:39:34 UTC",
    "updated_date": "2024-03-25 11:50:32 UTC"
  },
  {
    "arxiv_id": "2402.10681v1",
    "title": "Physics-informed MeshGraphNets (PI-MGNs): Neural finite element solvers for non-stationary and nonlinear simulations on arbitrary meshes",
    "authors": [
      "Tobias W√ºrth",
      "Niklas Freymuth",
      "Clemens Zimmerling",
      "Gerhard Neumann",
      "Luise K√§rger"
    ],
    "abstract": "Engineering components must meet increasing technological demands in ever\nshorter development cycles. To face these challenges, a holistic approach is\nessential that allows for the concurrent development of part design, material\nsystem and manufacturing process. Current approaches employ numerical\nsimulations, which however quickly becomes computation-intensive, especially\nfor iterative optimization. Data-driven machine learning methods can be used to\nreplace time- and resource-intensive numerical simulations. In particular,\nMeshGraphNets (MGNs) have shown promising results. They enable fast and\naccurate predictions on unseen mesh geometries while being fully differentiable\nfor optimization. However, these models rely on large amounts of expensive\ntraining data, such as numerical simulations. Physics-informed neural networks\n(PINNs) offer an opportunity to train neural networks with partial differential\nequations instead of labeled data, but have not been extended yet to handle\ntime-dependent simulations of arbitrary meshes. This work introduces PI-MGNs, a\nhybrid approach that combines PINNs and MGNs to quickly and accurately solve\nnon-stationary and nonlinear partial differential equations (PDEs) on arbitrary\nmeshes. The method is exemplified for thermal process simulations of unseen\nparts with inhomogeneous material distribution. Further results show that the\nmodel scales well to large and complex meshes, although it is trained on small\ngeneric meshes only.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to CMAME",
    "pdf_url": "http://arxiv.org/pdf/2402.10681v1",
    "published_date": "2024-02-16 13:34:51 UTC",
    "updated_date": "2024-02-16 13:34:51 UTC"
  },
  {
    "arxiv_id": "2402.10659v4",
    "title": "Network Formation and Dynamics Among Multi-LLMs",
    "authors": [
      "Marios Papachristou",
      "Yuan Yuan"
    ],
    "abstract": "Social networks fundamentally shape human opinions, behaviors, and the\ndissemination of information. As large language models (LLMs) like GPT, Claude,\nand Llama increasingly integrate into social and professional settings,\nunderstanding their behavior in the context of social interactions and network\nformation becomes essential. This study develops a framework to systematically\nexamine whether the network formation behaviors of multiple LLMs approximate\ncertain aspects of human network dynamics. By simulating interactions among LLM\nagents across various model families, we observe that these models consistently\nexhibit key patterns associated with social network principles including\npreferential attachment, triadic closure, homophily, community structure, and\nthe small-world phenomenon when forming networks. Moreover, LLMs adapt their\nnetwork formation strategies based on each network's characteristics,\nreflecting the context-dependent nature of human behavior: in Facebook\nnetworks, they prioritize triadic closure and homophily, mirroring close-knit\nfriendships; in phone networks, homophily and preferential attachment dominate,\ncapturing personal and professional connections, while in employment networks,\nLLMs favor heterophily and high-degree connections, aligning with career\nadvancement dynamics. These results open new avenues for using LLMs in network\nscience research, with potential applications in agent-based modeling and\nsynthetic network generation.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10659v4",
    "published_date": "2024-02-16 13:10:14 UTC",
    "updated_date": "2024-12-05 04:35:22 UTC"
  },
  {
    "arxiv_id": "2402.10649v1",
    "title": "Hermite Neural Network Simulation for Solving the 2D Schrodinger Equation",
    "authors": [
      "Kourosh Parand",
      "Aida Pakniyat"
    ],
    "abstract": "The Schrodinger equation is a mathematical equation describing the wave\nfunction's behavior in a quantum-mechanical system. It is a partial\ndifferential equation that provides valuable insights into the fundamental\nprinciples of quantum mechanics. In this paper, the aim was to solve the\nSchrodinger equation with sufficient accuracy by using a mixture of neural\nnetworks with the collocation method base Hermite functions. Initially, the\nHermite functions roots were employed as collocation points, enhancing the\nefficiency of the solution. The Schrodinger equation is defined in an infinite\ndomain, the use of Hermite functions as activation functions resulted in\nexcellent precision. Finally, the proposed method was simulated using MATLAB's\nSimulink tool. The results were then compared with those obtained using\nPhysics-informed neural networks and the presented method.",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.LG",
      "cs.NA",
      "cs.NE",
      "math.AP"
    ],
    "primary_category": "math.NA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10649v1",
    "published_date": "2024-02-16 12:51:25 UTC",
    "updated_date": "2024-02-16 12:51:25 UTC"
  },
  {
    "arxiv_id": "2402.15522v1",
    "title": "IntSat: Integer Linear Programming by Conflict-Driven Constraint-Learning",
    "authors": [
      "Robert Nieuwenhuis",
      "Albert Oliveras",
      "Enric Rodriguez-Carbonell"
    ],
    "abstract": "State-of-the-art SAT solvers are nowadays able to handle huge real-world\ninstances. The key to this success is the so-called Conflict-Driven\nClause-Learning (CDCL) scheme, which encompasses a number of techniques that\nexploit the conflicts that are encountered during the search for a solution. In\nthis article we extend these techniques to Integer Linear Programming (ILP),\nwhere variables may take general integer values instead of purely binary ones,\nconstraints are more expressive than just propositional clauses, and there may\nbe an objective function to optimise. We explain how these methods can be\nimplemented efficiently, and discuss possible improvements. Our work is backed\nwith a basic implementation that shows that, even in this far less mature\nstage, our techniques are already a useful complement to the state of the art\nin ILP solving.",
    "categories": [
      "cs.AI",
      "I.2.8; F.4.1"
    ],
    "primary_category": "cs.AI",
    "comment": "48 pages. This is the Author's Original Manuscript of the journal\n  version",
    "pdf_url": "http://arxiv.org/pdf/2402.15522v1",
    "published_date": "2024-02-16 12:48:40 UTC",
    "updated_date": "2024-02-16 12:48:40 UTC"
  },
  {
    "arxiv_id": "2402.10645v3",
    "title": "Can Separators Improve Chain-of-Thought Prompting?",
    "authors": [
      "Yoonjeong Park",
      "Hyunjin Kim",
      "Chanyeol Choi",
      "Junseong Kim",
      "Jy-yong Sohn"
    ],
    "abstract": "Chain-of-thought (CoT) prompting is a simple and effective method for\nimproving the reasoning capabilities of Large Language Models (LLMs). The basic\nidea of CoT is to let LLMs break down their thought processes step-by-step by\nputting exemplars in the input prompt. However, the densely structured prompt\nexemplars of CoT may cause the cognitive overload of LLMs. Inspired by human\ncognition, we introduce COT-SEP, a method that strategically employs separators\nat the end of each exemplar in CoT prompting. These separators are designed to\nhelp the LLMs understand their thought processes better while reasoning.\nInterestingly, it turns out that COT-SEP significantly improves the LLMs'\nperformances on complex reasoning tasks (e.g., GSM8K, AQuA, CSQA), compared\nwith the vanilla CoT, which does not use separators. We also study the effects\nof the type and the location of separators tested on multiple LLMs, including\nGPT-3.5-Turbo, GPT-4, and LLaMA-2 7B.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "IEEE FLLM 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.10645v3",
    "published_date": "2024-02-16 12:46:16 UTC",
    "updated_date": "2024-10-09 06:54:29 UTC"
  },
  {
    "arxiv_id": "2402.10643v1",
    "title": "`Keep it Together': Enforcing Cohesion in Extractive Summaries by Simulating Human Memory",
    "authors": [
      "Ronald Cardenas",
      "Matthias Galle",
      "Shay B. Cohen"
    ],
    "abstract": "Extractive summaries are usually presented as lists of sentences with no\nexpected cohesion between them. In this paper, we aim to enforce cohesion\nwhilst controlling for informativeness and redundancy in summaries, in cases\nwhere the input exhibits high redundancy. The pipeline controls for redundancy\nin long inputs as it is consumed, and balances informativeness and cohesion\nduring sentence selection. Our sentence selector simulates human memory to keep\ntrack of topics --modeled as lexical chains--, enforcing cohesive ties between\nnoun phrases. Across a variety of domains, our experiments revealed that it is\npossible to extract highly cohesive summaries that nevertheless read as\ninformative to humans as summaries extracted by only accounting for\ninformativeness or redundancy. The extracted summaries exhibit smooth topic\ntransitions between sentences as signaled by lexical chains, with chains\nspanning adjacent or near-adjacent sentences.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10643v1",
    "published_date": "2024-02-16 12:43:26 UTC",
    "updated_date": "2024-02-16 12:43:26 UTC"
  },
  {
    "arxiv_id": "2402.10642v2",
    "title": "Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up Speech Diffusion Model",
    "authors": [
      "Xiangyu Zhang",
      "Daijiao Liu",
      "Hexin Liu",
      "Qiquan Zhang",
      "Hanyu Meng",
      "Leibny Paola Garcia",
      "Eng Siong Chng",
      "Lina Yao"
    ],
    "abstract": "Recently, Denoising Diffusion Probabilistic Models (DDPMs) have attained\nleading performances across a diverse range of generative tasks. However, in\nthe field of speech synthesis, although DDPMs exhibit impressive performance,\ntheir long training duration and substantial inference costs hinder practical\ndeployment. Existing approaches primarily focus on enhancing inference speed,\nwhile approaches to accelerate training a key factor in the costs associated\nwith adding or customizing voices often necessitate complex modifications to\nthe model, compromising their universal applicability. To address the\naforementioned challenges, we propose an inquiry: is it possible to enhance the\ntraining/inference speed and performance of DDPMs by modifying the speech\nsignal itself? In this paper, we double the training and inference speed of\nSpeech DDPMs by simply redirecting the generative target to the wavelet domain.\nThis method not only achieves comparable or superior performance to the\noriginal model in speech synthesis tasks but also demonstrates its versatility.\nBy investigating and utilizing different wavelet bases, our approach proves\neffective not just in speech synthesis, but also in speech enhancement.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10642v2",
    "published_date": "2024-02-16 12:43:01 UTC",
    "updated_date": "2024-09-23 22:57:44 UTC"
  },
  {
    "arxiv_id": "2402.10635v1",
    "title": "ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling",
    "authors": [
      "Yuqi Chen",
      "Kan Ren",
      "Yansen Wang",
      "Yuchen Fang",
      "Weiwei Sun",
      "Dongsheng Li"
    ],
    "abstract": "Modeling continuous-time dynamics on irregular time series is critical to\naccount for data evolution and correlations that occur continuously.\nTraditional methods including recurrent neural networks or Transformer models\nleverage inductive bias via powerful neural architectures to capture complex\npatterns. However, due to their discrete characteristic, they have limitations\nin generalizing to continuous-time data paradigms. Though neural ordinary\ndifferential equations (Neural ODEs) and their variants have shown promising\nresults in dealing with irregular time series, they often fail to capture the\nintricate correlations within these sequences. It is challenging yet demanding\nto concurrently model the relationship between input data points and capture\nthe dynamic changes of the continuous-time system. To tackle this problem, we\npropose ContiFormer that extends the relation modeling of vanilla Transformer\nto the continuous-time domain, which explicitly incorporates the modeling\nabilities of continuous dynamics of Neural ODEs with the attention mechanism of\nTransformers. We mathematically characterize the expressive power of\nContiFormer and illustrate that, by curated designs of function hypothesis,\nmany Transformer variants specialized in irregular time series modeling can be\ncovered as a special case of ContiFormer. A wide range of experiments on both\nsynthetic and real-world datasets have illustrated the superior modeling\ncapacities and prediction performance of ContiFormer on irregular time series\ndata. The project link is https://seqml.github.io/contiformer/.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Neurips 2023 Poster",
    "pdf_url": "http://arxiv.org/pdf/2402.10635v1",
    "published_date": "2024-02-16 12:34:38 UTC",
    "updated_date": "2024-02-16 12:34:38 UTC"
  },
  {
    "arxiv_id": "2402.10634v3",
    "title": "Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling",
    "authors": [
      "Ivan Marisca",
      "Cesare Alippi",
      "Filippo Maria Bianchi"
    ],
    "abstract": "Given a set of synchronous time series, each associated with a sensor-point\nin space and characterized by inter-series relationships, the problem of\nspatiotemporal forecasting consists of predicting future observations for each\npoint. Spatiotemporal graph neural networks achieve striking results by\nrepresenting the relationships across time series as a graph. Nonetheless, most\nexisting methods rely on the often unrealistic assumption that inputs are\nalways available and fail to capture hidden spatiotemporal dynamics when part\nof the data is missing. In this work, we tackle this problem through\nhierarchical spatiotemporal downsampling. The input time series are\nprogressively coarsened over time and space, obtaining a pool of\nrepresentations that capture heterogeneous temporal and spatial dynamics.\nConditioned on observations and missing data patterns, such representations are\ncombined by an interpretable attention mechanism to generate the forecasts. Our\napproach outperforms state-of-the-art methods on synthetic and real-world\nbenchmarks under different missing data distributions, particularly in the\npresence of contiguous blocks of missing values.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.10634v3",
    "published_date": "2024-02-16 12:33:31 UTC",
    "updated_date": "2024-06-08 15:27:35 UTC"
  },
  {
    "arxiv_id": "2402.10617v1",
    "title": "Multitask Kernel-based Learning with Logic Constraints",
    "authors": [
      "Michelangelo Diligenti",
      "Marco Gori",
      "Marco Maggini",
      "Leonardo Rigutini"
    ],
    "abstract": "This paper presents a general framework to integrate prior knowledge in the\nform of logic constraints among a set of task functions into kernel machines.\nThe logic propositions provide a partial representation of the environment, in\nwhich the learner operates, that is exploited by the learning algorithm\ntogether with the information available in the supervised examples. In\nparticular, we consider a multi-task learning scheme, where multiple unary\npredicates on the feature space are to be learned by kernel machines and a\nhigher level abstract representation consists of logic clauses on these\npredicates, known to hold for any input. A general approach is presented to\nconvert the logic clauses into a continuous implementation, that processes the\noutputs computed by the kernel-based predicates. The learning task is\nformulated as a primal optimization problem of a loss function that combines a\nterm measuring the fitting of the supervised examples, a regularization term,\nand a penalty term that enforces the constraints on both supervised and\nunsupervised examples. The proposed semi-supervised learning framework is\nparticularly suited for learning in high dimensionality feature spaces, where\nthe supervised training examples tend to be sparse and generalization\ndifficult. Unlike for standard kernel machines, the cost function to optimize\nis not generally guaranteed to be convex. However, the experimental results\nshow that it is still possible to find good solutions using a two stage\nlearning schema, in which first the supervised examples are learned until\nconvergence and then the logic constraints are forced. Some promising\nexperimental results on artificial multi-task learning tasks are reported,\nshowing how the classification accuracy can be effectively improved by\nexploiting the a priori rules and the unsupervised examples.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The 19th European Conference on Artificial Intelligence (ECAI 2010)",
    "pdf_url": "http://arxiv.org/pdf/2402.10617v1",
    "published_date": "2024-02-16 12:11:34 UTC",
    "updated_date": "2024-02-16 12:11:34 UTC"
  },
  {
    "arxiv_id": "2402.10991v4",
    "title": "Enhancing Convergence in Federated Learning: A Contribution-Aware Asynchronous Approach",
    "authors": [
      "Changxin Xu",
      "Yuxin Qiao",
      "Zhanxin Zhou",
      "Fanghao Ni",
      "Jize Xiong"
    ],
    "abstract": "Federated Learning (FL) is a distributed machine learning paradigm that\nallows clients to train models on their data while preserving their privacy. FL\nalgorithms, such as Federated Averaging (FedAvg) and its variants, have been\nshown to converge well in many scenarios. However, these methods require\nclients to upload their local updates to the server in a synchronous manner,\nwhich can be slow and unreliable in realistic FL settings. To address this\nissue, researchers have developed asynchronous FL methods that allow clients to\ncontinue training on their local data using a stale global model. However, most\nof these methods simply aggregate all of the received updates without\nconsidering their relative contributions, which can slow down convergence. In\nthis paper, we propose a contribution-aware asynchronous FL method that takes\ninto account the staleness and statistical heterogeneity of the received\nupdates. Our method dynamically adjusts the contribution of each update based\non these factors, which can speed up convergence compared to existing methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages, 1 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.10991v4",
    "published_date": "2024-02-16 12:10:53 UTC",
    "updated_date": "2024-03-04 03:35:40 UTC"
  },
  {
    "arxiv_id": "2402.10614v2",
    "title": "Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements",
    "authors": [
      "Ming Li",
      "Jiuhai Chen",
      "Lichang Chen",
      "Tianyi Zhou"
    ],
    "abstract": "Making LLMs speak for different, especially minority groups of people, and\ngenerate statements supporting their diverse or even controversial perspectives\nis critical to creating an inclusive environment. However, existing LLMs lack\nsufficient controllability to the stance of their generated content, which\noften contains inconsistent, neutral, or biased statements. In this paper, we\nimprove the controllability of LLMs in generating statements supporting an\nargument the user defined in the prompt. We find that multi-round debates\nbetween two LLMs with opposite stances generate higher-quality and more salient\nstatements for each, which are important training data to improve the\ncontrollability of LLMs. Motivated by this, we develop a novel debate & tuning\n(DEBATUNE) pipeline finetuning LLMs to generate the statements obtained via\ndebate. To examine DEBATUNE, we curate the largest dataset of debate topics so\nfar, which covers 710 controversial topics and corresponding arguments for each\ntopic. Evaluations by the GPT-4 judge with a novel controversy controllability\nmetric show that LLMs' capability of generating diverse perspectives is\nsignificantly improved by DEBATUNE. Moreover, such controllability can be\ngeneralized to unseen topics, generating high-quality statements supporting\ncontroversial arguments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL2024 findings, Camera-ready",
    "pdf_url": "http://arxiv.org/pdf/2402.10614v2",
    "published_date": "2024-02-16 12:00:34 UTC",
    "updated_date": "2024-06-07 20:19:09 UTC"
  },
  {
    "arxiv_id": "2402.10601v3",
    "title": "When \"Competency\" in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers",
    "authors": [
      "Divij Handa",
      "Zehua Zhang",
      "Amir Saeidi",
      "Shrinidhi Kumbhar",
      "Chitta Baral"
    ],
    "abstract": "Recent advancements in Large Language Model (LLM) safety have primarily\nfocused on mitigating attacks crafted in natural language or common ciphers\n(e.g. Base64), which are likely integrated into newer models' safety training.\nHowever, we reveal a paradoxical vulnerability: as LLMs advance in reasoning,\nthey inadvertently become more susceptible to novel jailbreaking attacks.\nEnhanced reasoning enables LLMs to interpret complex instructions and decode\ncomplex user-defined ciphers, creating an exploitable security gap. To study\nthis vulnerability, we introduce Attacks using Custom Encryptions (ACE), a\njailbreaking technique that encodes malicious queries with novel ciphers.\nExtending ACE, we introduce Layered Attacks using Custom Encryptions (LACE),\nwhich applies multi-layer ciphers to amplify attack complexity. Furthermore, we\ndevelop CipherBench, a benchmark designed to evaluate LLMs' accuracy in\ndecoding encrypted benign text. Our experiments reveal a critical trade-off:\nLLMs that are more capable of decoding ciphers are more vulnerable to these\njailbreaking attacks, with success rates on GPT-4o escalating from 40% under\nACE to 78% with LACE. These findings highlight a critical insight: as LLMs\nbecome more adept at deciphering complex user ciphers--many of which cannot be\npreemptively included in safety training--they become increasingly exploitable.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10601v3",
    "published_date": "2024-02-16 11:37:05 UTC",
    "updated_date": "2025-03-16 21:45:21 UTC"
  },
  {
    "arxiv_id": "2402.10597v1",
    "title": "Efficiency at Scale: Investigating the Performance of Diminutive Language Models in Clinical Tasks",
    "authors": [
      "Niall Taylor",
      "Upamanyu Ghose",
      "Omid Rohanian",
      "Mohammadmahdi Nouriborji",
      "Andrey Kormilitzin",
      "David Clifton",
      "Alejo Nevado-Holgado"
    ],
    "abstract": "The entry of large language models (LLMs) into research and commercial spaces\nhas led to a trend of ever-larger models, with initial promises of\ngeneralisability, followed by a widespread desire to downsize and create\nspecialised models without the need for complete fine-tuning, using Parameter\nEfficient Fine-tuning (PEFT) methods. We present an investigation into the\nsuitability of different PEFT methods to clinical decision-making tasks, across\na range of model sizes, including extremely small models with as few as $25$\nmillion parameters.\n  Our analysis shows that the performance of most PEFT approaches varies\nsignificantly from one task to another, with the exception of LoRA, which\nmaintains relatively high performance across all model sizes and tasks,\ntypically approaching or matching full fine-tuned performance. The\neffectiveness of PEFT methods in the clinical domain is evident, particularly\nfor specialised models which can operate on low-cost, in-house computing\ninfrastructure. The advantages of these models, in terms of speed and reduced\ntraining costs, dramatically outweighs any performance gain from large\nfoundation LLMs. Furthermore, we highlight how domain-specific pre-training\ninteracts with PEFT methods and model size, and discuss how these factors\ninterplay to provide the best efficiency-performance trade-off. Full code\navailable at: tbd.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10597v1",
    "published_date": "2024-02-16 11:30:11 UTC",
    "updated_date": "2024-02-16 11:30:11 UTC"
  },
  {
    "arxiv_id": "2402.10586v2",
    "title": "Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs",
    "authors": [
      "Zae Myung Kim",
      "Kwang Hee Lee",
      "Preston Zhu",
      "Vipul Raheja",
      "Dongyeop Kang"
    ],
    "abstract": "With the advent of large language models (LLM), the line between\nhuman-crafted and machine-generated texts has become increasingly blurred. This\npaper delves into the inquiry of identifying discernible and unique linguistic\nproperties in texts that were written by humans, particularly uncovering the\nunderlying discourse structures of texts beyond their surface structures.\nIntroducing a novel methodology, we leverage hierarchical parse trees and\nrecursive hypergraphs to unveil distinctive discourse patterns in texts\nproduced by both LLMs and humans. Empirical findings demonstrate that, although\nboth LLMs and humans generate distinct discourse patterns influenced by\nspecific domains, human-written texts exhibit more structural variability,\nreflecting the nuanced nature of human writing in different domains. Notably,\nincorporating hierarchical discourse features enhances binary classifiers'\noverall performance in distinguishing between human-written and\nmachine-generated texts, even on out-of-distribution and paraphrased samples.\nThis underscores the significance of incorporating hierarchical discourse\nfeatures in the analysis of text patterns. The code and dataset are available\nat https://github.com/minnesotanlp/threads-of-subtlety.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "26 pages, accepted at ACL 2024 (Main)",
    "pdf_url": "http://arxiv.org/pdf/2402.10586v2",
    "published_date": "2024-02-16 11:20:30 UTC",
    "updated_date": "2024-06-06 20:04:52 UTC"
  },
  {
    "arxiv_id": "2402.10580v1",
    "title": "Efficient Multi-task Uncertainties for Joint Semantic Segmentation and Monocular Depth Estimation",
    "authors": [
      "Steven Landgraf",
      "Markus Hillemann",
      "Theodor Kapler",
      "Markus Ulrich"
    ],
    "abstract": "Quantifying the predictive uncertainty emerged as a possible solution to\ncommon challenges like overconfidence or lack of explainability and robustness\nof deep neural networks, albeit one that is often computationally expensive.\nMany real-world applications are multi-modal in nature and hence benefit from\nmulti-task learning. In autonomous driving, for example, the joint solution of\nsemantic segmentation and monocular depth estimation has proven to be valuable.\nIn this work, we first combine different uncertainty quantification methods\nwith joint semantic segmentation and monocular depth estimation and evaluate\nhow they perform in comparison to each other. Additionally, we reveal the\nbenefits of multi-task learning with regard to the uncertainty quality compared\nto solving both tasks separately. Based on these insights, we introduce\nEMUFormer, a novel student-teacher distillation approach for joint semantic\nsegmentation and monocular depth estimation as well as efficient multi-task\nuncertainty quantification. By implicitly leveraging the predictive\nuncertainties of the teacher, EMUFormer achieves new state-of-the-art results\non Cityscapes and NYUv2 and additionally estimates high-quality predictive\nuncertainties for both tasks that are comparable or superior to a Deep Ensemble\ndespite being an order of magnitude more efficient.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 5 figures, 10 tables, submitted to peer-reviewed journal",
    "pdf_url": "http://arxiv.org/pdf/2402.10580v1",
    "published_date": "2024-02-16 11:09:16 UTC",
    "updated_date": "2024-02-16 11:09:16 UTC"
  },
  {
    "arxiv_id": "2402.10575v1",
    "title": "Symbolic Autoencoding for Self-Supervised Sequence Learning",
    "authors": [
      "Mohammad Hossein Amani",
      "Nicolas Mario Baldwin",
      "Amin Mansouri",
      "Martin Josifoski",
      "Maxime Peyrard",
      "Robert West"
    ],
    "abstract": "Traditional language models, adept at next-token prediction in text\nsequences, often struggle with transduction tasks between distinct symbolic\nsystems, particularly when parallel data is scarce. Addressing this issue, we\nintroduce \\textit{symbolic autoencoding} ($\\Sigma$AE), a self-supervised\nframework that harnesses the power of abundant unparallel data alongside\nlimited parallel data. $\\Sigma$AE connects two generative models via a discrete\nbottleneck layer and is optimized end-to-end by minimizing reconstruction loss\n(simultaneously with supervised loss for the parallel data), such that the\nsequence generated by the discrete bottleneck can be read out as the transduced\ninput sequence. We also develop gradient-based methods allowing for efficient\nself-supervised sequence learning despite the discreteness of the bottleneck.\nOur results demonstrate that $\\Sigma$AE significantly enhances performance on\ntransduction tasks, even with minimal parallel data, offering a promising\nsolution for weakly supervised learning scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10575v1",
    "published_date": "2024-02-16 11:04:31 UTC",
    "updated_date": "2024-02-16 11:04:31 UTC"
  },
  {
    "arxiv_id": "2404.03664v4",
    "title": "LLMs in the Heart of Differential Testing: A Case Study on a Medical Rule Engine",
    "authors": [
      "Erblin Isaku",
      "Christoph Laaber",
      "Hassan Sartaj",
      "Shaukat Ali",
      "Thomas Schwitalla",
      "Jan F. Nyg√•rd"
    ],
    "abstract": "The Cancer Registry of Norway (CRN) uses an automated cancer registration\nsupport system (CaReSS) to support core cancer registry activities, i.e, data\ncapture, data curation, and producing data products and statistics for various\nstakeholders. GURI is a core component of CaReSS, which is responsible for\nvalidating incoming data with medical rules. Such medical rules are manually\nimplemented by medical experts based on medical standards, regulations, and\nresearch. Since large language models (LLMs) have been trained on a large\namount of public information, including these documents, they can be employed\nto generate tests for GURI. Thus, we propose an LLM-based test generation and\ndifferential testing approach (LLMeDiff) to test GURI. We experimented with\nfour different LLMs, two medical rule engine implementations, and 58 real\nmedical rules to investigate the hallucination, success, time efficiency, and\nrobustness of the LLMs to generate tests, and these tests' ability to find\npotential issues in GURI. Our results showed that GPT-3.5 hallucinates the\nleast, is the most successful, and is generally the most robust; however, it\nhas the worst time efficiency. Our differential testing revealed 22 medical\nrules where implementation inconsistencies were discovered (e.g., regarding\nhandling rule versions). Finally, we provide insights for practitioners and\nresearchers based on the results.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "12 pages, 6 figures, 4 tables, 1 listing, revised arguments",
    "pdf_url": "http://arxiv.org/pdf/2404.03664v4",
    "published_date": "2024-02-16 10:56:15 UTC",
    "updated_date": "2025-02-28 15:33:10 UTC"
  },
  {
    "arxiv_id": "2402.10571v2",
    "title": "Direct Preference Optimization with an Offset",
    "authors": [
      "Afra Amini",
      "Tim Vieira",
      "Ryan Cotterell"
    ],
    "abstract": "Direct preference optimization (DPO) is a successful fine-tuning strategy for\naligning large language models with human preferences without the need to train\na reward model or employ reinforcement learning. DPO, as originally formulated,\nrelies on binary preference data and fine-tunes a language model to increase\nthe likelihood of a preferred response over a dispreferred response. However,\nnot all preference pairs are equal. Sometimes, the preferred response is only\nslightly better than the dispreferred one. In other cases, the preference is\nmuch stronger. For instance, if a response contains harmful or toxic content,\nthe annotator will have a strong preference for that response. In this paper,\nwe propose a generalization of DPO, termed DPO with an offset (ODPO), that does\nnot treat every preference pair equally during fine-tuning. Intuitively, ODPO\nrequires the difference between the likelihood of the preferred and\ndispreferred response to be greater than an offset value. The offset is\ndetermined based on the extent to which one response is preferred over another.\nOur experiments on various tasks suggest that ODPO significantly outperforms\nDPO in aligning language models, especially when the number of preference pairs\nis limited.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10571v2",
    "published_date": "2024-02-16 10:55:38 UTC",
    "updated_date": "2024-06-06 12:02:37 UTC"
  },
  {
    "arxiv_id": "2402.10567v4",
    "title": "InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?",
    "authors": [
      "Yogesh Tripathi",
      "Raghav Donakanti",
      "Sahil Girhepuje",
      "Ishan Kavathekar",
      "Bhaskara Hanuma Vedula",
      "Gokul S Krishnan",
      "Shreya Goyal",
      "Anmol Goel",
      "Balaraman Ravindran",
      "Ponnurangam Kumaraguru"
    ],
    "abstract": "Recent advancements in language technology and Artificial Intelligence have\nresulted in numerous Language Models being proposed to perform various tasks in\nthe legal domain ranging from predicting judgments to generating summaries.\nDespite their immense potential, these models have been proven to learn and\nexhibit societal biases and make unfair predictions. In this study, we explore\nthe ability of Large Language Models (LLMs) to perform legal tasks in the\nIndian landscape when social factors are involved. We present a novel metric,\n$\\beta$-weighted $\\textit{Legal Safety Score ($LSS_{\\beta}$)}$, which\nencapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs'\nsafety by considering its performance in the $\\textit{Binary Statutory\nReasoning}$ task and its fairness exhibition with respect to various axes of\ndisparities in the Indian society. Task performance and fairness scores of\nLLaMA and LLaMA--2 models indicate that the proposed $LSS_{\\beta}$ metric can\neffectively determine the readiness of a model for safe usage in the legal\nsector. We also propose finetuning pipelines, utilising specialised legal\ndatasets, as a potential method to mitigate bias and improve model safety. The\nfinetuning procedures on LLaMA and LLaMA--2 models increase the $LSS_{\\beta}$,\nimproving their usability in the Indian legal domain. Our code is publicly\nreleased.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10567v4",
    "published_date": "2024-02-16 10:54:10 UTC",
    "updated_date": "2024-06-17 17:46:07 UTC"
  },
  {
    "arxiv_id": "2402.10543v2",
    "title": "Strong hallucinations from negation and how to fix them",
    "authors": [
      "Nicholas Asher",
      "Swarnadeep Bhar"
    ],
    "abstract": "Despite great performance on many tasks, language models (LMs) still struggle\nwith reasoning, sometimes providing responses that cannot possibly be true\nbecause they stem from logical incoherence. We call such responses\n\\textit{strong hallucinations} and prove that they follow from an LM's\ncomputation of its internal representations for logical operators and outputs\nfrom those representations. Focusing on negation, we provide a novel solution\nin which negation is treated not as another element of a latent representation,\nbut as \\textit{an operation over an LM's latent representations that constrains\nhow they may evolve}. We show that our approach improves model performance in\ncloze prompting and natural language inference tasks with negation without\nrequiring training on sparse negative data.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Proceedings of the 62nd Annual Meeting of the Association for\n  Computational Linguistics (Findings)",
    "pdf_url": "http://arxiv.org/pdf/2402.10543v2",
    "published_date": "2024-02-16 10:11:20 UTC",
    "updated_date": "2024-08-20 08:36:26 UTC"
  },
  {
    "arxiv_id": "2402.12392v1",
    "title": "A Regression Mixture Model to understand the effect of the Covid-19 pandemic on Public Transport Ridership",
    "authors": [
      "Hugues Moreau",
      "√âtienne C√¥me",
      "Allou Sam√©",
      "Latifa Oukhellou"
    ],
    "abstract": "The Covid-19 pandemic drastically changed urban mobility, both during the\nheight of the pandemic with government lockdowns, but also in the longer term\nwith the adoption of working-from-home policies. To understand its effects on\nrail public transport ridership, we propose a dedicated Regression Mixture\nModel able to perform both the clustering of public transport stations and the\nsegmentation of time periods, while ignoring variations due to additional\nvariables such as the official lockdowns or non-working days. Each cluster is\nthus defined by a series of segments in which the effect of the exogenous\nvariables is constant. As each segment within a cluster has its own regression\ncoefficients to model the impact of the covariates, we analyze how these\ncoefficients evolve to understand the changes in the cluster. We present the\nregression mixture model and the parameter estimation using the EM algorithm,\nbefore demonstrating the benefits of the model on both simulated and real data.\nThanks to a five-year dataset of the ridership in the Paris public transport\nsystem, we analyze the impact of the pandemic, not only in terms of the number\nof travelers but also on the weekly commute. We further analyze the specific\nchanges that the pandemic caused inside each cluster.",
    "categories": [
      "stat.AP",
      "cs.AI"
    ],
    "primary_category": "stat.AP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.12392v1",
    "published_date": "2024-02-16 09:37:58 UTC",
    "updated_date": "2024-02-16 09:37:58 UTC"
  },
  {
    "arxiv_id": "2402.10532v1",
    "title": "Properties and Challenges of LLM-Generated Explanations",
    "authors": [
      "Jenny Kunz",
      "Marco Kuhlmann"
    ],
    "abstract": "The self-rationalising capabilities of large language models (LLMs) have been\nexplored in restricted settings, using task/specific data sets. However,\ncurrent LLMs do not (only) rely on specifically annotated data; nonetheless,\nthey frequently explain their outputs. The properties of the generated\nexplanations are influenced by the pre-training corpus and by the target data\nused for instruction fine-tuning. As the pre-training corpus includes a large\namount of human-written explanations \"in the wild\", we hypothesise that LLMs\nadopt common properties of human explanations. By analysing the outputs for a\nmulti-domain instruction fine-tuning data set, we find that generated\nexplanations show selectivity and contain illustrative elements, but less\nfrequently are subjective or misleading. We discuss reasons and consequences of\nthe properties' presence or absence. In particular, we outline positive and\nnegative implications depending on the goals and user groups of the\nself-rationalising system.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10532v1",
    "published_date": "2024-02-16 09:37:54 UTC",
    "updated_date": "2024-02-16 09:37:54 UTC"
  },
  {
    "arxiv_id": "2402.10528v3",
    "title": "Can We Verify Step by Step for Incorrect Answer Detection?",
    "authors": [
      "Xin Xu",
      "Shizhe Diao",
      "Can Yang",
      "Yang Wang"
    ],
    "abstract": "Chain-of-Thought (CoT) prompting has marked a significant advancement in\nenhancing the reasoning capabilities of large language models (LLMs). Previous\nstudies have developed various extensions of CoT, which focus primarily on\nenhancing end-task performance. In addition, there has been research on\nassessing the quality of reasoning chains in CoT. This raises an intriguing\nquestion: Is it possible to predict the accuracy of LLM outputs by scrutinizing\nthe reasoning chains they generate? To answer this research question, we\nintroduce a benchmark, R2PE, designed specifically to explore the relationship\nbetween reasoning chains and performance in various reasoning tasks spanning\nfive different domains. This benchmark aims to measure the falsehood of the\nfinal output of LLMs based on the reasoning steps. To make full use of\ninformation in multiple reasoning chains, we propose the process discernibility\nscore (PDS) framework that beats the answer-checking baseline by a large\nmargin. Concretely, this resulted in an average of $5.1\\%$ increase in the F1\nscore and $2.97\\%$ improvement in AUC-PR across all 45 subsets within R2PE. We\nfurther demonstrate our PDS's efficacy in advancing open-domain QA accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10528v3",
    "published_date": "2024-02-16 09:29:50 UTC",
    "updated_date": "2025-01-22 03:50:58 UTC"
  },
  {
    "arxiv_id": "2402.10524v1",
    "title": "LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models",
    "authors": [
      "Minsuk Kahng",
      "Ian Tenney",
      "Mahima Pushkarna",
      "Michael Xieyang Liu",
      "James Wexler",
      "Emily Reif",
      "Krystal Kallarackal",
      "Minsuk Chang",
      "Michael Terry",
      "Lucas Dixon"
    ],
    "abstract": "Automatic side-by-side evaluation has emerged as a promising approach to\nevaluating the quality of responses from large language models (LLMs). However,\nanalyzing the results from this evaluation approach raises scalability and\ninterpretability challenges. In this paper, we present LLM Comparator, a novel\nvisual analytics tool for interactively analyzing results from automatic\nside-by-side evaluation. The tool supports interactive workflows for users to\nunderstand when and why a model performs better or worse than a baseline model,\nand how the responses from two models are qualitatively different. We\niteratively designed and developed the tool by closely working with researchers\nand engineers at a large technology company. This paper details the user\nchallenges we identified, the design and development of the tool, and an\nobservational study with participants who regularly evaluate their models.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10524v1",
    "published_date": "2024-02-16 09:14:49 UTC",
    "updated_date": "2024-02-16 09:14:49 UTC"
  },
  {
    "arxiv_id": "2402.10516v1",
    "title": "Generative AI for Controllable Protein Sequence Design: A Survey",
    "authors": [
      "Yiheng Zhu",
      "Zitai Kong",
      "Jialu Wu",
      "Weize Liu",
      "Yuqiang Han",
      "Mingze Yin",
      "Hongxia Xu",
      "Chang-Yu Hsieh",
      "Tingjun Hou"
    ],
    "abstract": "The design of novel protein sequences with targeted functionalities underpins\na central theme in protein engineering, impacting diverse fields such as drug\ndiscovery and enzymatic engineering. However, navigating this vast\ncombinatorial search space remains a severe challenge due to time and financial\nconstraints. This scenario is rapidly evolving as the transformative\nadvancements in AI, particularly in the realm of generative models and\noptimization algorithms, have been propelling the protein design field towards\nan unprecedented revolution. In this survey, we systematically review recent\nadvances in generative AI for controllable protein sequence design. To set the\nstage, we first outline the foundational tasks in protein sequence design in\nterms of the constraints involved and present key generative models and\noptimization algorithms. We then offer in-depth reviews of each design task and\ndiscuss the pertinent applications. Finally, we identify the unresolved\nchallenges and highlight research opportunities that merit deeper exploration.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.10516v1",
    "published_date": "2024-02-16 09:05:02 UTC",
    "updated_date": "2024-02-16 09:05:02 UTC"
  },
  {
    "arxiv_id": "2402.10515v1",
    "title": "Power-Efficient Indoor Localization Using Adaptive Channel-aware Ultra-wideband DL-TDOA",
    "authors": [
      "Sagnik Bhattacharya",
      "Junyoung Choi",
      "Joohyun Lee"
    ],
    "abstract": "Among the various Ultra-wideband (UWB) ranging methods, the absence of uplink\ncommunication or centralized computation makes downlink\ntime-difference-of-arrival (DL-TDOA) localization the most suitable for\nlarge-scale industrial deployments. However, temporary or permanent obstacles\nin the deployment region often lead to non-line-of-sight (NLOS) channel path\nand signal outage effects, which result in localization errors. Prior research\nhas addressed this problem by increasing the ranging frequency, which leads to\na heavy increase in the user device power consumption. It also does not\ncontribute to any increase in localization accuracy under line-of-sight (LOS)\nconditions. In this paper, we propose and implement a novel low-power\nchannel-aware dynamic frequency DL-TDOA ranging algorithm. It comprises NLOS\nprobability predictor based on a convolutional neural network (CNN), a dynamic\nranging frequency control module, and an IMU sensor-based ranging filter. Based\non the conducted experiments, we show that the proposed algorithm achieves 50%\nhigher accuracy in NLOS conditions while having 46% lower power consumption in\nLOS conditions compared to baseline methods from prior research.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10515v1",
    "published_date": "2024-02-16 09:04:04 UTC",
    "updated_date": "2024-02-16 09:04:04 UTC"
  },
  {
    "arxiv_id": "2402.10511v1",
    "title": "Can Transformers Predict Vibrations?",
    "authors": [
      "Fusataka Kuniyoshi",
      "Yoshihide Sawada"
    ],
    "abstract": "Highly accurate time-series vibration prediction is an important research\nissue for electric vehicles (EVs). EVs often experience vibrations when driving\non rough terrains, known as torsional resonance. This resonance, caused by the\ninteraction between motor and tire vibrations, puts excessive loads on the\nvehicle's drive shaft. However, current damping technologies only detect\nresonance after the vibration amplitude of the drive shaft torque reaches a\ncertain threshold, leading to significant loads on the shaft at the time of\ndetection. In this study, we propose a novel approach to address this issue by\nintroducing Resoformer, a transformer-based model for predicting torsional\nresonance. Resoformer utilizes time-series of the motor rotation speed as input\nand predicts the amplitude of torsional vibration at a specified quantile\noccurring in the shaft after the input series. By calculating the attention\nbetween recursive and convolutional features extracted from the measured data\npoints, Resoformer improves the accuracy of vibration forecasting. To evaluate\nthe model, we use a vibration dataset called VIBES (Dataset for Forecasting\nVibration Transition in EVs), consisting of 2,600 simulator-generated vibration\nsequences. Our experiments, conducted on strong baselines built on the VIBES\ndataset, demonstrate that Resoformer achieves state-of-the-art results. In\nconclusion, our study answers the question \"Can Transformers Forecast\nVibrations?\" While traditional transformer architectures show low performance\nin forecasting torsional resonance waves, our findings indicate that combining\nrecurrent neural network and temporal convolutional network using the\ntransformer architecture improves the accuracy of long-term vibration\nforecasting.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10511v1",
    "published_date": "2024-02-16 08:56:22 UTC",
    "updated_date": "2024-02-16 08:56:22 UTC"
  },
  {
    "arxiv_id": "2402.10510v1",
    "title": "Human Goal Recognition as Bayesian Inference: Investigating the Impact of Actions, Timing, and Goal Solvability",
    "authors": [
      "Chenyuan Zhang",
      "Charles Kemp",
      "Nir Lipovetzky"
    ],
    "abstract": "Goal recognition is a fundamental cognitive process that enables individuals\nto infer intentions based on available cues. Current goal recognition\nalgorithms often take only observed actions as input, but here we use a\nBayesian framework to explore the role of actions, timing, and goal solvability\nin goal recognition. We analyze human responses to goal-recognition problems in\nthe Sokoban domain, and find that actions are assigned most importance, but\nthat timing and solvability also influence goal recognition in some cases,\nespecially when actions are uninformative. We leverage these findings to\ndevelop a goal recognition model that matches human inferences more closely\nthan do existing algorithms. Our work provides new insight into human goal\nrecognition and takes a step towards more human-like AI models.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted by AAMAS 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.10510v1",
    "published_date": "2024-02-16 08:55:23 UTC",
    "updated_date": "2024-02-16 08:55:23 UTC"
  },
  {
    "arxiv_id": "2402.10500v2",
    "title": "Active Preference Optimization for Sample Efficient RLHF",
    "authors": [
      "Nirjhar Das",
      "Souradip Chakraborty",
      "Aldo Pacchiano",
      "Sayak Ray Chowdhury"
    ],
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning\nLarge Language Models (LLMs) with human preferences. Although aligned\ngenerative models have shown remarkable abilities in various tasks, their\nreliance on high-quality human preference data creates a costly bottleneck in\nthe practical application of RLHF. One primary reason is that current methods\nrely on uniformly picking prompt-generation pairs from a dataset of\nprompt-generations, to collect human feedback, resulting in sub-optimal\nalignment under a constrained budget, which highlights the criticality of\nadaptive strategies in efficient alignment. Recent works [Mehta et al., 2023,\nMuldrew et al., 2024] have tried to address this problem by designing various\nheuristics based on generation uncertainty. However, either the assumptions in\n[Mehta et al., 2023] are restrictive, or [Muldrew et al., 2024] do not provide\nany rigorous theoretical guarantee. To address these, we reformulate RLHF\nwithin contextual preference bandit framework, treating prompts as contexts,\nand develop an active-learning algorithm, $\\textit{Active Preference\nOptimization}$ ($\\texttt{APO}$), which enhances model alignment by querying\npreference data from the most important samples, achieving superior performance\nfor small sample budget. We analyze the theoretical performance guarantees of\n$\\texttt{APO}$ under the BTL preference model showing that the suboptimality\ngap of the policy learned via $\\texttt{APO}$ scales as $O(1/\\sqrt{T})$ for a\nbudget of $T$. We also show that collecting preference data by choosing prompts\nrandomly leads to a policy that suffers a constant sub-optimality. We perform\ndetailed experimental evaluations on practical preference datasets to validate\n$\\texttt{APO}$'s efficacy over the existing methods, establishing it as a\nsample-efficient and practical solution of alignment in a cost-effective and\nscalable manner.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "New experimental results added. Some reorganization",
    "pdf_url": "http://arxiv.org/pdf/2402.10500v2",
    "published_date": "2024-02-16 08:19:34 UTC",
    "updated_date": "2024-06-05 15:10:08 UTC"
  },
  {
    "arxiv_id": "2403.15396v1",
    "title": "I would love this to be like an assistant, not the teacher: a voice of the customer perspective of what distance learning students want from an Artificial Intelligence Digital Assistant",
    "authors": [
      "Bart Rienties",
      "John Domingue",
      "Subby Duttaroy",
      "Christothea Herodotou",
      "Felipe Tessarolo",
      "Denise Whitelock"
    ],
    "abstract": "With the release of Generative AI systems such as ChatGPT, an increasing\ninterest in using Artificial Intelligence (AI) has been observed across\ndomains, including higher education. While emerging statistics show the\npopularity of using AI amongst undergraduate students, little is yet known\nabout students' perceptions regarding AI including self-reported benefits and\nconcerns from their actual usage, in particular in distance learning contexts.\nUsing a two-step, mixed-methods approach, we examined the perceptions of ten\nonline and distance learning students from diverse disciplines regarding the\ndesign of a hypothetical AI Digital Assistant (AIDA). In the first step, we\ncaptured students' perceptions via interviews, while the second step supported\nthe triangulation of data by enabling students to share, compare, and contrast\nperceptions with those of peers. All participants agreed on the usefulness of\nsuch an AI tool while studying and reported benefits from using it for\nreal-time assistance and query resolution, support for academic tasks,\npersonalisation and accessibility, together with emotional and social support.\nStudents' concerns related to the ethical and social implications of\nimplementing AIDA, data privacy and data use, operational challenges, academic\nintegrity and misuse, and the future of education. Implications for the design\nof AI-tailored systems are also discussed.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "23 pages, 1 figure, submitted to Distance Education",
    "pdf_url": "http://arxiv.org/pdf/2403.15396v1",
    "published_date": "2024-02-16 08:10:41 UTC",
    "updated_date": "2024-02-16 08:10:41 UTC"
  },
  {
    "arxiv_id": "2402.10496v2",
    "title": "Comparing Hallucination Detection Metrics for Multilingual Generation",
    "authors": [
      "Haoqiang Kang",
      "Terra Blevins",
      "Luke Zettlemoyer"
    ],
    "abstract": "While many hallucination detection techniques have been evaluated on English\ntext, their effectiveness in multilingual contexts remains unknown. This paper\nassesses how well various factual hallucination detection metrics (lexical\nmetrics like ROUGE and Named Entity Overlap, and Natural Language Inference\n(NLI)-based metrics) identify hallucinations in generated biographical\nsummaries across languages. We compare how well automatic metrics correlate to\neach other and whether they agree with human judgments of factuality. Our\nanalysis reveals that while the lexical metrics are ineffective, NLI-based\nmetrics perform well, correlating with human annotations in many settings and\noften outperforming supervised models. However, NLI metrics are still limited,\nas they do not detect single-fact hallucinations well and fail for\nlower-resource languages. Therefore, our findings highlight the gaps in\nexisiting hallucination detection methods for non-English languages and\nmotivate future research to develop more robust multilingual detection methods\nfor LLM hallucinations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10496v2",
    "published_date": "2024-02-16 08:10:34 UTC",
    "updated_date": "2024-06-16 00:44:28 UTC"
  },
  {
    "arxiv_id": "2402.10492v1",
    "title": "Developing an Optimal Model for Predicting the Severity of Wheat Stem Rust (Case study of Arsi and Bale Zone)",
    "authors": [
      "Tewodrose Altaye"
    ],
    "abstract": "This research utilized three types of artificial neural network (ANN)\nmethodologies, namely Backpropagation Neural Network (BPNN) with varied\ntraining, transfer, divide, and learning functions; Radial Basis Function\nNeural Network (RBFNN); and General Regression Neural Network (GRNN), to\nforecast the severity of stem rust. It considered parameters such as mean\nmaximum temperature, mean minimum temperature, mean rainfall, mean average\ntemperature, mean relative humidity, and different wheat varieties. The\nstatistical analysis revealed that GRNN demonstrated effective predictive\ncapability and required less training time compared to the other models.\nAdditionally, the results indicated that total seasonal rainfall positively\ninfluenced the development of wheat stem rust.\n  Keywords: Wheat stem rust, Back propagation neural network, Radial Basis\nFunction Neural Network, General Regression Neural Network.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10492v1",
    "published_date": "2024-02-16 07:48:59 UTC",
    "updated_date": "2024-02-16 07:48:59 UTC"
  },
  {
    "arxiv_id": "2402.10487v4",
    "title": "RPMixer: Shaking Up Time Series Forecasting with Random Projections for Large Spatial-Temporal Data",
    "authors": [
      "Chin-Chia Michael Yeh",
      "Yujie Fan",
      "Xin Dai",
      "Uday Singh Saini",
      "Vivian Lai",
      "Prince Osei Aboagye",
      "Junpeng Wang",
      "Huiyuan Chen",
      "Yan Zheng",
      "Zhongfang Zhuang",
      "Liang Wang",
      "Wei Zhang"
    ],
    "abstract": "Spatial-temporal forecasting systems play a crucial role in addressing\nnumerous real-world challenges. In this paper, we investigate the potential of\naddressing spatial-temporal forecasting problems using general time series\nforecasting models, i.e., models that do not leverage the spatial relationships\namong the nodes. We propose a all-Multi-Layer Perceptron (all-MLP) time series\nforecasting architecture called RPMixer. The all-MLP architecture was chosen\ndue to its recent success in time series forecasting benchmarks. Furthermore,\nour method capitalizes on the ensemble-like behavior of deep neural networks,\nwhere each individual block within the network behaves like a base learner in\nan ensemble model, particularly when identity mapping residual connections are\nincorporated. By integrating random projection layers into our model, we\nincrease the diversity among the blocks' outputs, thereby improving the overall\nperformance of the network. Extensive experiments conducted on the largest\nspatial-temporal forecasting benchmark datasets demonstrate that the proposed\nmethod outperforms alternative methods, including both spatial-temporal graph\nmodels and general forecasting models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10487v4",
    "published_date": "2024-02-16 07:28:59 UTC",
    "updated_date": "2024-06-12 08:49:48 UTC"
  },
  {
    "arxiv_id": "2402.10481v2",
    "title": "Emoji Driven Crypto Assets Market Reactions",
    "authors": [
      "Xiaorui Zuo",
      "Yao-Tsung Chen",
      "Wolfgang Karl H√§rdle"
    ],
    "abstract": "In the burgeoning realm of cryptocurrency, social media platforms like\nTwitter have become pivotal in influencing market trends and investor\nsentiments. In our study, we leverage GPT-4 and a fine-tuned transformer-based\nBERT model for a multimodal sentiment analysis, focusing on the impact of emoji\nsentiment on cryptocurrency markets. By translating emojis into quantifiable\nsentiment data, we correlate these insights with key market indicators like BTC\nPrice and the VCRIX index. Our architecture's analysis of emoji sentiment\ndemonstrated a distinct advantage over FinBERT's pure text sentiment analysis\nin such predicting power. This approach may be fed into the development of\ntrading strategies aimed at utilizing social media elements to identify and\nforecast market trends. Crucially, our findings suggest that strategies based\non emoji sentiment can facilitate the avoidance of significant market downturns\nand contribute to the stabilization of returns. This research underscores the\npractical benefits of integrating advanced AI-driven analyses into financial\nstrategies, offering a nuanced perspective on the interplay between digital\ncommunication and market dynamics in an academic context.",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "q-fin.ST"
    ],
    "primary_category": "q-fin.CP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10481v2",
    "published_date": "2024-02-16 07:05:49 UTC",
    "updated_date": "2024-05-04 14:32:44 UTC"
  },
  {
    "arxiv_id": "2402.12170v3",
    "title": "Where is the answer? Investigating Positional Bias in Language Model Knowledge Extraction",
    "authors": [
      "Kuniaki Saito",
      "Kihyuk Sohn",
      "Chen-Yu Lee",
      "Yoshitaka Ushiku"
    ],
    "abstract": "Large language models require updates to remain up-to-date or adapt to new\ndomains by fine-tuning them with new documents. One key is memorizing the\nlatest information in a way that the memorized information is extractable with\na query prompt. However, LLMs suffer from a phenomenon called perplexity curse;\ndespite minimizing document perplexity during fine-tuning, LLMs struggle to\nextract information through a prompt sentence. In this new knowledge\nacquisition and extraction, we find a very intriguing fact that LLMs can\naccurately answer questions about the first sentence, but they struggle to\nextract information described in the middle or end of the documents used for\nfine-tuning. Our study suggests that the auto-regressive training causes this\nissue; each token is prompted by reliance on all previous tokens, which hinders\nthe model from recalling information from training documents by question\nprompts. To conduct the in-depth study, we publish both synthetic and real\ndatasets, enabling the evaluation of the QA performance w.r.t. the position of\nthe corresponding answer in a document. Our investigation shows that even a\nlarge model suffers from the perplexity curse, but regularization such as\ndenoising auto-regressive loss can enhance the information extraction from\ndiverse positions. These findings will be (i) a key to improving knowledge\nextraction from LLMs and (ii) new elements to discuss the trade-off between RAG\nand fine-tuning in adapting LLMs to a new domain.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Code is published at https://github.com/omron-sinicx/WhereIsTheAnswer",
    "pdf_url": "http://arxiv.org/pdf/2402.12170v3",
    "published_date": "2024-02-16 06:29:16 UTC",
    "updated_date": "2025-04-18 00:24:19 UTC"
  },
  {
    "arxiv_id": "2402.10468v1",
    "title": "Adversarial Curriculum Graph Contrastive Learning with Pair-wise Augmentation",
    "authors": [
      "Xinjian Zhao",
      "Liang Zhang",
      "Yang Liu",
      "Ruocheng Guo",
      "Xiangyu Zhao"
    ],
    "abstract": "Graph contrastive learning (GCL) has emerged as a pivotal technique in the\ndomain of graph representation learning. A crucial aspect of effective GCL is\nthe caliber of generated positive and negative samples, which is intrinsically\ndictated by their resemblance to the original data. Nevertheless, precise\ncontrol over similarity during sample generation presents a formidable\nchallenge, often impeding the effective discovery of representative graph\npatterns. To address this challenge, we propose an innovative framework:\nAdversarial Curriculum Graph Contrastive Learning (ACGCL), which capitalizes on\nthe merits of pair-wise augmentation to engender graph-level positive and\nnegative samples with controllable similarity, alongside subgraph contrastive\nlearning to discern effective graph patterns therein. Within the ACGCL\nframework, we have devised a novel adversarial curriculum training methodology\nthat facilitates progressive learning by sequentially increasing the difficulty\nof distinguishing the generated samples. Notably, this approach transcends the\nprevalent sparsity issue inherent in conventional curriculum learning\nstrategies by adaptively concentrating on more challenging training data.\nFinally, a comprehensive assessment of ACGCL is conducted through extensive\nexperiments on six well-known benchmark datasets, wherein ACGCL conspicuously\nsurpasses a set of state-of-the-art baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10468v1",
    "published_date": "2024-02-16 06:17:50 UTC",
    "updated_date": "2024-02-16 06:17:50 UTC"
  },
  {
    "arxiv_id": "2402.10466v4",
    "title": "Large Language Models as Zero-shot Dialogue State Tracker through Function Calling",
    "authors": [
      "Zekun Li",
      "Zhiyu Zoey Chen",
      "Mike Ross",
      "Patrick Huber",
      "Seungwhan Moon",
      "Zhaojiang Lin",
      "Xin Luna Dong",
      "Adithya Sagar",
      "Xifeng Yan",
      "Paul A. Crook"
    ],
    "abstract": "Large language models (LLMs) are increasingly prevalent in conversational\nsystems due to their advanced understanding and generative capabilities in\ngeneral contexts. However, their effectiveness in task-oriented dialogues\n(TOD), which requires not only response generation but also effective dialogue\nstate tracking (DST) within specific tasks and domains, remains less\nsatisfying. In this work, we propose a novel approach FnCTOD for solving DST\nwith LLMs through function calling. This method improves zero-shot DST,\nallowing adaptation to diverse domains without extensive data collection or\nmodel tuning. Our experimental results demonstrate that our approach achieves\nexceptional performance with both modestly sized open-source and also\nproprietary LLMs: with in-context prompting it enables various 7B or 13B\nparameter models to surpass the previous state-of-the-art (SOTA) achieved by\nChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6% average\njoint goal accuracy (JGA). Individual model results for GPT-3.5 and GPT-4 are\nboosted by 4.8% and 14%, respectively. We also show that by fine-tuning on a\nsmall collection of diverse task-oriented dialogues, we can equip modestly\nsized models, specifically a 13B parameter LLaMA2-Chat model, with\nfunction-calling capabilities and DST performance comparable to ChatGPT while\nmaintaining their chat capabilities. We have made the code publicly available\nat https://github.com/facebookresearch/FnCTOD",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024 Main. Code available at:\n  https://github.com/facebookresearch/FnCTOD",
    "pdf_url": "http://arxiv.org/pdf/2402.10466v4",
    "published_date": "2024-02-16 06:13:18 UTC",
    "updated_date": "2024-05-30 04:19:54 UTC"
  },
  {
    "arxiv_id": "2402.10987v2",
    "title": "WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing",
    "authors": [
      "Chenhui Hu",
      "Pengfei Cao",
      "Yubo Chen",
      "Kang Liu",
      "Jun Zhao"
    ],
    "abstract": "Knowledge editing aims to rectify inaccuracies in large language models\n(LLMs) without costly retraining for outdated or erroneous knowledge. However,\ncurrent knowledge editing methods primarily focus on single editing, failing to\nmeet the requirements for lifelong editing. This study reveals a performance\ndegradation encountered by knowledge editing in lifelong editing, characterized\nby toxicity buildup and toxicity flash, with the primary cause identified as\npattern unmatch. We introduce a knowledge editing approach named Wise-Layer\nKnowledge Editor (WilKE), which selects editing layer based on the pattern\nmatching degree of editing knowledge across different layers in language\nmodels. Experimental results demonstrate that, in lifelong editing, WilKE\nexhibits an average improvement of 46.2% and 67.8% on editing GPT2-XL and GPT-J\nrelative to state-of-the-art knowledge editing methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To be published in ACL Findings 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.10987v2",
    "published_date": "2024-02-16 05:29:59 UTC",
    "updated_date": "2024-06-05 07:44:30 UTC"
  },
  {
    "arxiv_id": "2402.10986v3",
    "title": "FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models",
    "authors": [
      "Gagan Bhatia",
      "El Moatez Billah Nagoudi",
      "Hasan Cavusoglu",
      "Muhammad Abdul-Mageed"
    ],
    "abstract": "We introduce FinTral, a suite of state-of-the-art multimodal large language\nmodels (LLMs) built upon the Mistral-7b model and tailored for financial\nanalysis. FinTral integrates textual, numerical, tabular, and image data. We\nenhance FinTral with domain-specific pretraining, instruction fine-tuning, and\nRLAIF training by exploiting a large collection of textual and visual datasets\nwe curate for this work. We also introduce an extensive benchmark featuring\nnine tasks and 25 datasets for evaluation, including hallucinations in the\nfinancial domain. Our FinTral model trained with direct preference optimization\nemploying advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&R,\ndemonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5\nin all tasks and surpasses GPT-4 in five out of nine tasks, marking a\nsignificant advancement in AI-driven financial technology. We also demonstrate\nthat FinTral has the potential to excel in real-time analysis and\ndecision-making in diverse financial contexts. The GitHub repository for\nFinTral is available at \\url{https://github.com/UBC-NLP/fintral}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10986v3",
    "published_date": "2024-02-16 05:05:12 UTC",
    "updated_date": "2024-06-14 13:26:47 UTC"
  },
  {
    "arxiv_id": "2402.10427v1",
    "title": "Evaluating and Improving Continual Learning in Spoken Language Understanding",
    "authors": [
      "Muqiao Yang",
      "Xiang Li",
      "Umberto Cappellazzo",
      "Shinji Watanabe",
      "Bhiksha Raj"
    ],
    "abstract": "Continual learning has emerged as an increasingly important challenge across\nvarious tasks, including Spoken Language Understanding (SLU). In SLU, its\nobjective is to effectively handle the emergence of new concepts and evolving\nenvironments. The evaluation of continual learning algorithms typically\ninvolves assessing the model's stability, plasticity, and generalizability as\nfundamental aspects of standards. However, existing continual learning metrics\nprimarily focus on only one or two of the properties. They neglect the overall\nperformance across all tasks, and do not adequately disentangle the plasticity\nversus stability/generalizability trade-offs within the model. In this work, we\npropose an evaluation methodology that provides a unified evaluation on\nstability, plasticity, and generalizability in continual learning. By employing\nthe proposed metric, we demonstrate how introducing various knowledge\ndistillations can improve different aspects of these three properties of the\nSLU model. We further show that our proposed metric is more sensitive in\ncapturing the impact of task ordering in continual learning, making it better\nsuited for practical use-case scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10427v1",
    "published_date": "2024-02-16 03:30:27 UTC",
    "updated_date": "2024-02-16 03:30:27 UTC"
  },
  {
    "arxiv_id": "2402.10985v4",
    "title": "CloudLens: Modeling and Detecting Cloud Security Vulnerabilities",
    "authors": [
      "Mikhail Kazdagli",
      "Mohit Tiwari",
      "Akshat Kumar"
    ],
    "abstract": "Cloud computing services provide scalable and cost-effective solutions for\ndata storage, processing, and collaboration. With their growing popularity,\nconcerns about security vulnerabilities are increasing. To address this, first,\nwe provide a formal model, called CloudLens, that expresses relations between\ndifferent cloud objects such as users, datastores, security roles, representing\naccess control policies in cloud systems. Second, as access control\nmisconfigurations are often the primary driver for cloud attacks, we develop a\nplanning model for detecting security vulnerabilities. Such vulnerabilities can\nlead to widespread attacks such as ransomware, sensitive data exfiltration\namong others. A planner generates attacks to identify such vulnerabilities in\nthe cloud. Finally, we test our approach on 14 real Amazon AWS cloud\nconfigurations of different commercial organizations. Our system can identify a\nbroad range of security vulnerabilities, which state-of-the-art industry tools\ncannot detect.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10985v4",
    "published_date": "2024-02-16 03:28:02 UTC",
    "updated_date": "2024-12-24 03:16:58 UTC"
  },
  {
    "arxiv_id": "2402.10424v1",
    "title": "Understanding In-Context Learning with a Pelican Soup Framework",
    "authors": [
      "Ting-Rui Chiang",
      "Dani Yogatama"
    ],
    "abstract": "Many existing theoretical analyses of in-context learning for natural\nlanguage processing are based on latent variable models that leaves gaps\nbetween theory and practice. We aim to close these gaps by proposing a\ntheoretical framework, the Pelican Soup Framework. In this framework, we\nintroduce (1) the notion of a common sense knowledge base, (2) a general\nformalism for natural language classification tasks, and the notion of (3)\nmeaning association. Under this framework, we can establish a\n$\\mathcal{O}(1/T)$ loss bound for in-context learning, where $T$ is the number\nof example-label pairs in the demonstration. Compared with previous works, our\nbound reflects the effect of the choice of verbalizers and the effect of\ninstruction tuning. An additional notion of \\textit{atom concepts} makes our\nframework possible to explain the generalization to tasks unseen in the\nlanguage model training data. Finally, we propose a toy setup, Calcutec, and a\ndigit addition task that mimics types of distribution shifts a model needs to\novercome to perform in-context learning. We also experiment with GPT2-Large on\nreal-world NLP tasks. Our empirical results demonstrate the efficacy of our\nframework to explain in-context learning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10424v1",
    "published_date": "2024-02-16 03:20:14 UTC",
    "updated_date": "2024-02-16 03:20:14 UTC"
  },
  {
    "arxiv_id": "2402.10423v1",
    "title": "Connect the dots: Dataset Condensation, Differential Privacy, and Adversarial Uncertainty",
    "authors": [
      "Kenneth Odoh"
    ],
    "abstract": "Our work focuses on understanding the underpinning mechanism of dataset\ncondensation by drawing connections with ($\\epsilon$, $\\delta$)-differential\nprivacy where the optimal noise, $\\epsilon$, is chosen by adversarial\nuncertainty \\cite{Grining2017}. We can answer the question about the inner\nworkings of the dataset condensation procedure. Previous work \\cite{dong2022}\nproved the link between dataset condensation (DC) and ($\\epsilon$,\n$\\delta$)-differential privacy. However, it is unclear from existing works on\nablating DC to obtain a lower-bound estimate of $\\epsilon$ that will suffice\nfor creating high-fidelity synthetic data. We suggest that adversarial\nuncertainty is the most appropriate method to achieve an optimal noise level,\n$\\epsilon$. As part of the internal dynamics of dataset condensation, we adopt\na satisfactory scheme for noise estimation that guarantees high-fidelity data\nwhile providing privacy.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10423v1",
    "published_date": "2024-02-16 03:12:22 UTC",
    "updated_date": "2024-02-16 03:12:22 UTC"
  },
  {
    "arxiv_id": "2402.10416v2",
    "title": "Grounding Language about Belief in a Bayesian Theory-of-Mind",
    "authors": [
      "Lance Ying",
      "Tan Zhi-Xuan",
      "Lionel Wong",
      "Vikash Mansinghka",
      "Joshua Tenenbaum"
    ],
    "abstract": "Despite the fact that beliefs are mental states that cannot be directly\nobserved, humans talk about each others' beliefs on a regular basis, often\nusing rich compositional language to describe what others think and know. What\nexplains this capacity to interpret the hidden epistemic content of other\nminds? In this paper, we take a step towards an answer by grounding the\nsemantics of belief statements in a Bayesian theory-of-mind: By modeling how\nhumans jointly infer coherent sets of goals, beliefs, and plans that explain an\nagent's actions, then evaluating statements about the agent's beliefs against\nthese inferences via epistemic logic, our framework provides a conceptual role\nsemantics for belief, explaining the gradedness and compositionality of human\nbelief attributions, as well as their intimate connection with goals and plans.\nWe evaluate this framework by studying how humans attribute goals and beliefs\nwhile watching an agent solve a doors-and-keys gridworld puzzle that requires\ninstrumental reasoning about hidden objects. In contrast to pure logical\ndeduction, non-mentalizing baselines, and mentalizing that ignores the role of\ninstrumental plans, our model provides a much better fit to human goal and\nbelief attributions, demonstrating the importance of theory-of-mind for a\nsemantics of belief.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Published at CogSci 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.10416v2",
    "published_date": "2024-02-16 02:47:09 UTC",
    "updated_date": "2024-07-09 01:19:50 UTC"
  },
  {
    "arxiv_id": "2402.10412v2",
    "title": "Measuring and Reducing LLM Hallucination without Gold-Standard Answers",
    "authors": [
      "Jiaheng Wei",
      "Yuanshun Yao",
      "Jean-Francois Ton",
      "Hongyi Guo",
      "Andrew Estornell",
      "Yang Liu"
    ],
    "abstract": "LLM hallucination, i.e. generating factually incorrect yet seemingly\nconvincing answers, is currently a major threat to the trustworthiness and\nreliability of LLMs. The first step towards solving this complicated problem is\nto measure it. However, existing hallucination metrics require having a\nbenchmark dataset with gold-standard answers, i.e. \"best\" or \"correct\" answers\nwritten by humans. Such requirements make hallucination measurement costly and\nprone to human errors. In this work, we propose Factualness Evaluations via\nWeighting LLMs (FEWL), an innovative hallucination metric that is specifically\ndesigned for the scenario when gold-standard answers are absent. FEWL leverages\nthe answers from off-the-shelf LLMs that serve as a proxy of gold-standard\nanswers. The key challenge is how to quantify the expertise of reference LLMs\nresourcefully. We show FEWL has certain theoretical guarantees and demonstrate\nempirically it gives more accurate hallucination measures than naively using\nreference LLMs. We also show how to leverage FEWL to reduce hallucination\nthrough both in-context learning and supervised fine-tuning. Extensive\nexperiment results on Truthful-QA, CHALE, and HaluEval datasets demonstrate the\neffectiveness of FEWL.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Paper Under Review",
    "pdf_url": "http://arxiv.org/pdf/2402.10412v2",
    "published_date": "2024-02-16 02:32:06 UTC",
    "updated_date": "2024-06-06 18:33:02 UTC"
  },
  {
    "arxiv_id": "2402.10409v1",
    "title": "Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning",
    "authors": [
      "Jun Zhuang",
      "Casey Kennington"
    ],
    "abstract": "As new research on Large Language Models (LLMs) continues, it is difficult to\nkeep up with new research and models. To help researchers synthesize the new\nresearch many have written survey papers, but even those have become numerous.\nIn this paper, we develop a method to automatically assign survey papers to a\ntaxonomy. We collect the metadata of 144 LLM survey papers and explore three\nparadigms to classify papers within the taxonomy. Our work indicates that\nleveraging graph structure information on co-category graphs can significantly\noutperform the language models in two paradigms; pre-trained language models'\nfine-tuning and zero-shot/few-shot classifications using LLMs. We find that our\nmodel surpasses an average human recognition level and that fine-tuning LLMs\nusing weak labels generated by a smaller model, such as the GCN in this study,\ncan be more effective than using ground-truth labels, revealing the potential\nof weak-to-strong generalization in the taxonomy classification task.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "TL;DR: We collected metadata about LLM surveys and developed a method\n  for categorizing them into a taxonomy, indicating the superiority of graph\n  representation learning over language models and revealing the efficacy of\n  fine-tuning using weak labels",
    "pdf_url": "http://arxiv.org/pdf/2402.10409v1",
    "published_date": "2024-02-16 02:21:59 UTC",
    "updated_date": "2024-02-16 02:21:59 UTC"
  },
  {
    "arxiv_id": "2402.10404v1",
    "title": "Explaining generative diffusion models via visual analysis for interpretable decision-making process",
    "authors": [
      "Ji-Hoon Park",
      "Yeong-Joon Ju",
      "Seong-Whan Lee"
    ],
    "abstract": "Diffusion models have demonstrated remarkable performance in generation\ntasks. Nevertheless, explaining the diffusion process remains challenging due\nto it being a sequence of denoising noisy images that are difficult for experts\nto interpret. To address this issue, we propose the three research questions to\ninterpret the diffusion process from the perspective of the visual concepts\ngenerated by the model and the region where the model attends in each time\nstep. We devise tools for visualizing the diffusion process and answering the\naforementioned research questions to render the diffusion process\nhuman-understandable. We show how the output is progressively generated in the\ndiffusion process by explaining the level of denoising and highlighting\nrelationships to foundational visual concepts at each time step through the\nresults of experiments with various visual analyses using the tools. Throughout\nthe training of the diffusion model, the model learns diverse visual concepts\ncorresponding to each time-step, enabling the model to predict varying levels\nof visual concepts at different stages. We substantiate our tools using Area\nUnder Cover (AUC) score, correlation quantification, and cross-attention\nmapping. Our findings provide insights into the diffusion process and pave the\nway for further research into explainable diffusion mechanisms.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T01"
    ],
    "primary_category": "cs.CV",
    "comment": "22 pages, published in Expert Systems with Applications",
    "pdf_url": "http://arxiv.org/pdf/2402.10404v1",
    "published_date": "2024-02-16 02:12:20 UTC",
    "updated_date": "2024-02-16 02:12:20 UTC"
  },
  {
    "arxiv_id": "2402.10403v3",
    "title": "Polyhedral Complex Derivation from Piecewise Trilinear Networks",
    "authors": [
      "Jin-Hwa Kim"
    ],
    "abstract": "Recent advancements in visualizing deep neural networks provide insights into\ntheir structures and mesh extraction from Continuous Piecewise Affine (CPWA)\nfunctions. Meanwhile, developments in neural surface representation learning\nincorporate non-linear positional encoding, addressing issues like spectral\nbias; however, this poses challenges in applying mesh extraction techniques\nbased on CPWA functions. Focusing on trilinear interpolating methods as\npositional encoding, we present theoretical insights and an analytical mesh\nextraction, showing the transformation of hypersurfaces to flat planes within\nthe trilinear region under the eikonal constraint. Moreover, we introduce a\nmethod for approximating intersecting points among three hypersurfaces\ncontributing to broader applications. We empirically validate correctness and\nparsimony through chamfer distance and efficiency, and angular distance, while\nexamining the correlation between the eikonal loss and the planarity of the\nhypersurfaces.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.GR"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2024. Updated with the camera-ready version",
    "pdf_url": "http://arxiv.org/pdf/2402.10403v3",
    "published_date": "2024-02-16 02:01:24 UTC",
    "updated_date": "2024-10-18 01:44:05 UTC"
  },
  {
    "arxiv_id": "2402.10393v1",
    "title": "Darwin Turing Dawkins: Building a General Theory of Evolution",
    "authors": [
      "Leonard M. Adleman"
    ],
    "abstract": "Living things, computers, societies, and even books are part of a grand\nevolutionary struggle to survive. That struggle shapes nature, nations,\nreligions, art, science, and you. What you think, feel, and do is determined by\nit. Darwinian evolution does not apply solely to the genes that are stored in\nDNA. Using the insights of Alan Turing and Richard Dawkins, we will see that it\nalso applies to the memes we store in our brains and the information we store\nin our computers. The next time you run for president, fight a war, or just\ndeal with the ordinary problems humans are heir to, perhaps this book will be\nof use. If you want to understand why and when you will die, or if you want to\nachieve greatness this book may help. If you are concerned about where the\ncomputer revolution is headed, this book may provide some answers.",
    "categories": [
      "cs.GL",
      "cs.AI",
      "q-bio.PE"
    ],
    "primary_category": "cs.GL",
    "comment": "247 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.10393v1",
    "published_date": "2024-02-16 01:27:21 UTC",
    "updated_date": "2024-02-16 01:27:21 UTC"
  },
  {
    "arxiv_id": "2402.10392v1",
    "title": "Pretext Training Algorithms for Event Sequence Data",
    "authors": [
      "Yimu Wang",
      "He Zhao",
      "Ruizhi Deng",
      "Frederick Tung",
      "Greg Mori"
    ],
    "abstract": "Pretext training followed by task-specific fine-tuning has been a successful\napproach in vision and language domains. This paper proposes a self-supervised\npretext training framework tailored to event sequence data. We introduce a\nnovel alignment verification task that is specialized to event sequences,\nbuilding on good practices in masked reconstruction and contrastive learning.\nOur pretext tasks unlock foundational representations that are generalizable\nacross different down-stream tasks, including next-event prediction for\ntemporal point process models, event sequence classification, and missing event\ninterpolation. Experiments on popular public benchmarks demonstrate the\npotential of the proposed method across different tasks and data domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10392v1",
    "published_date": "2024-02-16 01:25:21 UTC",
    "updated_date": "2024-02-16 01:25:21 UTC"
  },
  {
    "arxiv_id": "2402.10381v2",
    "title": "UMAIR-FPS: User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style",
    "authors": [
      "Yan Kang",
      "Hao Lin",
      "Mingjian Yang",
      "Shin-Jye Lee"
    ],
    "abstract": "The rapid advancement of high-quality image generation models based on AI has\ngenerated a deluge of anime illustrations. Recommending illustrations to users\nwithin massive data has become a challenging and popular task. However,\nexisting anime recommendation systems have focused on text features but still\nneed to integrate image features. In addition, most multi-modal recommendation\nresearch is constrained by tightly coupled datasets, limiting its applicability\nto anime illustrations. We propose the User-aware Multi-modal Animation\nIllustration Recommendation Fusion with Painting Style (UMAIR-FPS) to tackle\nthese gaps. In the feature extract phase, for image features, we are the first\nto combine image painting style features with semantic features to construct a\ndual-output image encoder for enhancing representation. For text features, we\nobtain text embeddings based on fine-tuning Sentence-Transformers by\nincorporating domain knowledge that composes a variety of domain text pairs\nfrom multilingual mappings, entity relationships, and term explanation\nperspectives, respectively. In the multi-modal fusion phase, we novelly propose\na user-aware multi-modal contribution measurement mechanism to weight\nmulti-modal features dynamically according to user features at the interaction\nlevel and employ the DCN-V2 module to model bounded-degree multi-modal crosses\neffectively. UMAIR-FPS surpasses the stat-of-the-art baselines on large\nreal-world datasets, demonstrating substantial performance enhancements.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by DASFAA 2024 Research track",
    "pdf_url": "http://arxiv.org/pdf/2402.10381v2",
    "published_date": "2024-02-16 00:25:53 UTC",
    "updated_date": "2024-04-17 13:46:56 UTC"
  },
  {
    "arxiv_id": "2402.10380v1",
    "title": "Subgraph-level Universal Prompt Tuning",
    "authors": [
      "Junhyun Lee",
      "Wooseong Yang",
      "Jaewoo Kang"
    ],
    "abstract": "In the evolving landscape of machine learning, the adaptation of pre-trained\nmodels through prompt tuning has become increasingly prominent. This trend is\nparticularly observable in the graph domain, where diverse pre-training\nstrategies present unique challenges in developing effective prompt-based\ntuning methods for graph neural networks. Previous approaches have been\nlimited, focusing on specialized prompting functions tailored to models with\nedge prediction pre-training tasks. These methods, however, suffer from a lack\nof generalizability across different pre-training strategies. Recently, a\nsimple prompt tuning method has been designed for any pre-training strategy,\nfunctioning within the input graph's feature space. This allows it to\ntheoretically emulate any type of prompting function, thereby significantly\nincreasing its versatility for a range of downstream applications.\nNevertheless, the capacity of such simple prompts to fully grasp the complex\ncontexts found in graphs remains an open question, necessitating further\ninvestigation. Addressing this challenge, our work introduces the\nSubgraph-level Universal Prompt Tuning (SUPT) approach, focusing on the\ndetailed context within subgraphs. In SUPT, prompt features are assigned at the\nsubgraph-level, preserving the method's universal capability. This requires\nextremely fewer tuning parameters than fine-tuning-based methods, outperforming\nthem in 42 out of 45 full-shot scenario experiments with an average improvement\nof over 2.5%. In few-shot scenarios, it excels in 41 out of 45 experiments,\nachieving an average performance increase of more than 6.6%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10380v1",
    "published_date": "2024-02-16 00:25:24 UTC",
    "updated_date": "2024-02-16 00:25:24 UTC"
  }
]
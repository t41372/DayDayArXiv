[
  {
    "arxiv_id": "2407.19351v1",
    "title": "AccessShare: Co-designing Data Access and Sharing with Blind People",
    "authors": [
      "Rie Kamikubo",
      "Farnaz Zamiri Zeraati",
      "Kyungjun Lee",
      "Hernisa Kacorri"
    ],
    "abstract": "Blind people are often called to contribute image data to datasets for AI\ninnovation with the hope for future accessibility and inclusion. Yet, the\nvisual inspection of the contributed images is inaccessible. To this day, we\nlack mechanisms for data inspection and control that are accessible to the\nblind community. To address this gap, we engage 10 blind participants in a\nscenario where they wear smartglasses and collect image data using an\nAI-infused application in their homes. We also engineer a design probe, a novel\ndata access interface called AccessShare, and conduct a co-design study to\ndiscuss participants' needs, preferences, and ideas on consent, data\ninspection, and control. Our findings reveal the impact of interactive informed\nconsent and the complementary role of data inspection systems such as\nAccessShare in facilitating communication between data stewards and blind data\ncontributors. We discuss how key insights can guide future informed consent and\ndata control to promote inclusive and responsible data practices in AI.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Preprint, The 26th International ACM SIGACCESS Conference on\n  Computers and Accessibility (ASSETS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.19351v1",
    "published_date": "2024-07-27 23:39:58 UTC",
    "updated_date": "2024-07-27 23:39:58 UTC"
  },
  {
    "arxiv_id": "2407.19349v1",
    "title": "Predicting T-Cell Receptor Specificity",
    "authors": [
      "Tengyao Tu",
      "Wei Zeng",
      "Kun Zhao",
      "Zhenyu Zhang"
    ],
    "abstract": "Researching the specificity of TCR contributes to the development of\nimmunotherapy and provides new opportunities and strategies for personalized\ncancer immunotherapy. Therefore, we established a TCR generative specificity\ndetection framework consisting of an antigen selector and a TCR classifier\nbased on the Random Forest algorithm, aiming to efficiently screen out TCRs and\ntarget antigens and achieve TCR specificity prediction. Furthermore, we used\nthe k-fold validation method to compare the performance of our model with\nordinary deep learning methods. The result proves that adding a classifier to\nthe model based on the random forest algorithm is very effective, and our model\ngenerally outperforms ordinary deep learning methods. Moreover, we put forward\nfeasible optimization suggestions for the shortcomings and challenges of our\nmodel found during model implementation.",
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19349v1",
    "published_date": "2024-07-27 23:21:07 UTC",
    "updated_date": "2024-07-27 23:21:07 UTC"
  },
  {
    "arxiv_id": "2407.19345v4",
    "title": "Inference-Time Selective Debiasing to Enhance Fairness in Text Classification Models",
    "authors": [
      "Gleb Kuzmin",
      "Neemesh Yadav",
      "Ivan Smirnov",
      "Timothy Baldwin",
      "Artem Shelmanov"
    ],
    "abstract": "We propose selective debiasing -- an inference-time safety mechanism designed\nto enhance the overall model quality in terms of prediction performance and\nfairness, especially in scenarios where retraining the model is impractical.\nThe method draws inspiration from selective classification, where at inference\ntime, predictions with low quality, as indicated by their uncertainty scores,\nare discarded. In our approach, we identify the potentially biased model\npredictions and, instead of discarding them, we remove bias from these\npredictions using LEACE -- a post-processing debiasing method. To select\nproblematic predictions, we propose a bias quantification approach based on KL\ndivergence, which achieves better results than standard uncertainty\nquantification methods. Experiments on text classification datasets with\nencoder-based classification models demonstrate that selective debiasing helps\nto reduce the performance gap between post-processing methods and debiasing\ntechniques from the at-training and pre-processing categories.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.19345v4",
    "published_date": "2024-07-27 21:56:23 UTC",
    "updated_date": "2025-03-11 08:39:45 UTC"
  },
  {
    "arxiv_id": "2407.19340v5",
    "title": "Integrating Large Language Models into a Tri-Modal Architecture for Automated Depression Classification on the DAIC-WOZ",
    "authors": [
      "Santosh V. Patapati"
    ],
    "abstract": "Major Depressive Disorder (MDD) is a pervasive mental health condition that\naffects 300 million people worldwide. This work presents a novel, BiLSTM-based\ntri-modal model-level fusion architecture for the binary classification of\ndepression from clinical interview recordings. The proposed architecture\nincorporates Mel Frequency Cepstral Coefficients, Facial Action Units, and uses\na two-shot learning based GPT-4 model to process text data. This is the first\nwork to incorporate large language models into a multi-modal architecture for\nthis task. It achieves impressive results on the DAIC-WOZ AVEC 2016 Challenge\ncross-validation split and Leave-One-Subject-Out cross-validation split,\nsurpassing all baseline models and multiple state-of-the-art models. In\nLeave-One-Subject-Out testing, it achieves an accuracy of 91.01%, an F1-Score\nof 85.95%, a precision of 80%, and a recall of 92.86%.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Keywords: Multi-Modal Neural Networks, Deep Learning, Large Language\n  Models, Depression Diagnosis, Biomedical Informatics, DAIC-WOZ",
    "pdf_url": "http://arxiv.org/pdf/2407.19340v5",
    "published_date": "2024-07-27 21:00:36 UTC",
    "updated_date": "2024-10-11 18:52:25 UTC"
  },
  {
    "arxiv_id": "2407.19338v1",
    "title": "Semantic Communication Enhanced by Knowledge Graph Representation Learning",
    "authors": [
      "Nour Hello",
      "Paolo Di Lorenzo",
      "Emilio Calvanese Strinati"
    ],
    "abstract": "This paper investigates the advantages of representing and processing\nsemantic knowledge extracted into graphs within the emerging paradigm of\nsemantic communications. The proposed approach leverages semantic and pragmatic\naspects, incorporating recent advances on large language models (LLMs) to\nachieve compact representations of knowledge to be processed and exchanged\nbetween intelligent agents. This is accomplished by using the cascade of LLMs\nand graph neural networks (GNNs) as semantic encoders, where information to be\nshared is selected to be meaningful at the receiver. The embedding vectors\nproduced by the proposed semantic encoder represent information in the form of\ntriplets: nodes (semantic concepts entities), edges(relations between\nconcepts), nodes. Thus, semantic information is associated with the\nrepresentation of relationships among elements in the space of semantic concept\nabstractions. In this paper, we investigate the potential of achieving high\ncompression rates in communication by incorporating relations that link\nelements within graph embeddings. We propose sending semantic symbols solely\nequivalent to node embeddings through the wireless channel and inferring the\ncomplete knowledge graph at the receiver. Numerical simulations illustrate the\neffectiveness of leveraging knowledge graphs to semantically compress and\ntransmit information.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication at the 25th IEEE International Workshop on\n  Signal Processing Advances in Wireless Communications (SPAWC)",
    "pdf_url": "http://arxiv.org/pdf/2407.19338v1",
    "published_date": "2024-07-27 20:57:10 UTC",
    "updated_date": "2024-07-27 20:57:10 UTC"
  },
  {
    "arxiv_id": "2407.19332v1",
    "title": "A Semi-supervised Fake News Detection using Sentiment Encoding and LSTM with Self-Attention",
    "authors": [
      "Pouya Shaeri",
      "Ali Katanforoush"
    ],
    "abstract": "Micro-blogs and cyber-space social networks are the main communication\nmediums to receive and share news nowadays. As a side effect, however, the\nnetworks can disseminate fake news that harms individuals and the society.\nSeveral methods have been developed to detect fake news, but the majority\nrequire large sets of manually labeled data to attain the application-level\naccuracy. Due to the strict privacy policies, the required data are often\ninaccessible or limited to some specific topics. On the other side, quite\ndiverse and abundant unlabeled data on social media suggests that with a few\nlabeled data, the problem of detecting fake news could be tackled via\nsemi-supervised learning. Here, we propose a semi-supervised self-learning\nmethod in which a sentiment analysis is acquired by some state-of-the-art\npretrained models. Our learning model is trained in a semi-supervised fashion\nand incorporates LSTM with self-attention layers. We benchmark our model on a\ndataset with 20,000 news content along with their feedback, which shows better\nperformance in precision, recall, and measures compared to competitive methods\nin fake news detection.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19332v1",
    "published_date": "2024-07-27 20:00:10 UTC",
    "updated_date": "2024-07-27 20:00:10 UTC"
  },
  {
    "arxiv_id": "2407.19316v1",
    "title": "AResNet-ViT: A Hybrid CNN-Transformer Network for Benign and Malignant Breast Nodule Classification in Ultrasound Images",
    "authors": [
      "Xin Zhao",
      "Qianqian Zhu",
      "Jialing Wu"
    ],
    "abstract": "To address the challenges of similarity between lesions and surrounding\ntissues, overlapping appearances of partially benign and malignant nodules, and\ndifficulty in classification, a deep learning network that integrates CNN and\nTransformer is proposed for the classification of benign and malignant breast\nlesions in ultrasound images. This network adopts a dual-branch architecture\nfor local-global feature extraction, making full use of the advantages of CNN\nin extracting local features and the ability of ViT to extract global features\nto enhance the network's feature extraction capabilities for breast nodules.\nThe local feature extraction branch employs a residual network with multiple\nattention-guided modules, which can effectively capture the local details and\ntexture features of breast nodules, enhance sensitivity to subtle changes\nwithin the nodules, and thus can aid in accurate classification of their benign\nand malignancy. The global feature extraction branch utilizes the multi-head\nself-attention ViT network, which can capture the overall shape, boundary, and\nrelationship with surrounding tissues, and thereby enhancing the understanding\nand modeling of both nodule and global image features. Experimental results on\na public ultrasound breast nodule data set show that the proposed method is\nbetter than other comparison networks, This indicates that the fusion of CNN\nand Transformer networks can effectively improve the performance of the\nclassification model and provide a powerful solution for the benign-malignant\nclassification of ultrasound breast.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "12 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.19316v1",
    "published_date": "2024-07-27 18:18:45 UTC",
    "updated_date": "2024-07-27 18:18:45 UTC"
  },
  {
    "arxiv_id": "2407.19300v1",
    "title": "CoLiDR: Concept Learning using Aggregated Disentangled Representations",
    "authors": [
      "Sanchit Sinha",
      "Guangzhi Xiong",
      "Aidong Zhang"
    ],
    "abstract": "Interpretability of Deep Neural Networks using concept-based models offers a\npromising way to explain model behavior through human-understandable concepts.\nA parallel line of research focuses on disentangling the data distribution into\nits underlying generative factors, in turn explaining the data generation\nprocess. While both directions have received extensive attention, little work\nhas been done on explaining concepts in terms of generative factors to unify\nmathematically disentangled representations and human-understandable concepts\nas an explanation for downstream tasks. In this paper, we propose a novel\nmethod CoLiDR - which utilizes a disentangled representation learning setup for\nlearning mutually independent generative factors and subsequently learns to\naggregate the said representations into human-understandable concepts using a\nnovel aggregation/decomposition module. Experiments are conducted on datasets\nwith both known and unknown latent generative factors. Our method successfully\naggregates disentangled generative factors into concepts while maintaining\nparity with state-of-the-art concept-based approaches. Quantitative and visual\nanalysis of the learned aggregation procedure demonstrates the advantages of\nour work compared to commonly used concept-based models over four challenging\ndatasets. Lastly, our work is generalizable to an arbitrary number of concepts\nand generative factors - making it flexible enough to be suitable for various\ntypes of data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "KDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.19300v1",
    "published_date": "2024-07-27 16:55:14 UTC",
    "updated_date": "2024-07-27 16:55:14 UTC"
  },
  {
    "arxiv_id": "2407.19296v1",
    "title": "Multi-Modal CLIP-Informed Protein Editing",
    "authors": [
      "Mingze Yin",
      "Hanjing Zhou",
      "Yiheng Zhu",
      "Miao Lin",
      "Yixuan Wu",
      "Jialu Wu",
      "Hongxia Xu",
      "Chang-Yu Hsieh",
      "Tingjun Hou",
      "Jintai Chen",
      "Jian Wu"
    ],
    "abstract": "Proteins govern most biological functions essential for life, but achieving\ncontrollable protein discovery and optimization remains challenging. Recently,\nmachine learning-assisted protein editing (MLPE) has shown promise in\naccelerating optimization cycles and reducing experimental workloads. However,\ncurrent methods struggle with the vast combinatorial space of potential protein\nedits and cannot explicitly conduct protein editing using biotext instructions,\nlimiting their interactivity with human feedback. To fill these gaps, we\npropose a novel method called ProtET for efficient CLIP-informed protein\nediting through multi-modality learning. Our approach comprises two stages: in\nthe pretraining stage, contrastive learning aligns protein-biotext\nrepresentations encoded by two large language models (LLMs), respectively.\nSubsequently, during the protein editing stage, the fused features from editing\ninstruction texts and original protein sequences serve as the final editing\ncondition for generating target protein sequences. Comprehensive experiments\ndemonstrated the superiority of ProtET in editing proteins to enhance\nhuman-expected functionality across multiple attribute domains, including\nenzyme catalytic activity, protein stability and antibody specific binding\nability. And ProtET improves the state-of-the-art results by a large margin,\nleading to significant stability improvements of 16.67% and 16.90%. This\ncapability positions ProtET to advance real-world artificial protein editing,\npotentially addressing unmet academic, industrial, and clinical needs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages, 7 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.19296v1",
    "published_date": "2024-07-27 16:41:08 UTC",
    "updated_date": "2024-07-27 16:41:08 UTC"
  },
  {
    "arxiv_id": "2407.19280v1",
    "title": "Large Language Models for Human-like Autonomous Driving: A Survey",
    "authors": [
      "Yun Li",
      "Kai Katsumata",
      "Ehsan Javanmardi",
      "Manabu Tsukada"
    ],
    "abstract": "Large Language Models (LLMs), AI models trained on massive text corpora with\nremarkable language understanding and generation capabilities, are transforming\nthe field of Autonomous Driving (AD). As AD systems evolve from rule-based and\noptimization-based methods to learning-based techniques like deep reinforcement\nlearning, they are now poised to embrace a third and more advanced category:\nknowledge-based AD empowered by LLMs. This shift promises to bring AD closer to\nhuman-like AD. However, integrating LLMs into AD systems poses challenges in\nreal-time inference, safety assurance, and deployment costs. This survey\nprovides a comprehensive and critical review of recent progress in leveraging\nLLMs for AD, focusing on their applications in modular AD pipelines and\nend-to-end AD systems. We highlight key advancements, identify pressing\nchallenges, and propose promising research directions to bridge the gap between\nLLMs and AD, thereby facilitating the development of more human-like AD\nsystems. The survey first introduces LLMs' key features and common training\nschemes, then delves into their applications in modular AD pipelines and\nend-to-end AD, respectively, followed by discussions on open challenges and\nfuture directions. Through this in-depth analysis, we aim to provide insights\nand inspiration for researchers and practitioners working at the intersection\nof AI and autonomous vehicles, ultimately contributing to safer, smarter, and\nmore human-centric AD technologies.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 2 figures, accepted at IEEE Intelligent Transportation\n  Systems Conference (ITSC) 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.19280v1",
    "published_date": "2024-07-27 15:24:11 UTC",
    "updated_date": "2024-07-27 15:24:11 UTC"
  },
  {
    "arxiv_id": "2407.19266v1",
    "title": "Interactive Learning in Computer Science Education Supported by a Discord Chatbot",
    "authors": [
      "Santiago Berrezueta-Guzman",
      "Ivan Parmacli",
      "Stephan Krusche",
      "Stefan Wagner"
    ],
    "abstract": "Enhancing interaction and feedback collection in a first-semester computer\nscience course poses a significant challenge due to students' diverse needs and\nengagement levels. To address this issue, we created and integrated a\ncommand-based chatbot on the course communication server on Discord. The\nDiscordBot enables students to provide feedback on course activities through\nshort surveys, such as exercises, quizzes, and lectures, facilitating\nstress-free communication with instructors. It also supports attendance\ntracking and introduces lectures before they start.\n  The research demonstrates the effectiveness of the DiscordBot as a\ncommunication tool. The ongoing feedback allowed course instructors to\ndynamically adjust and improve the difficulty level of upcoming activities and\npromote discussion in subsequent tutor sessions. The data collected reveal that\nstudents can accurately perceive the activities' difficulty and expected\nresults, providing insights not possible through traditional end-of-semester\nsurveys. Students reported that interaction with the DiscordBot was easy and\nexpressed a desire to continue using it in future semesters. This responsive\napproach ensures the course meets the evolving needs of students, thereby\nenhancing their overall learning experience.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.ET"
    ],
    "primary_category": "cs.AI",
    "comment": "Revised and accepted paper at the IEEE German Education Conference\n  2024 (GECon 2024) and to be published in IEEE proceedings",
    "pdf_url": "http://arxiv.org/pdf/2407.19266v1",
    "published_date": "2024-07-27 14:22:40 UTC",
    "updated_date": "2024-07-27 14:22:40 UTC"
  },
  {
    "arxiv_id": "2407.19259v1",
    "title": "Fine-Grained Scene Graph Generation via Sample-Level Bias Prediction",
    "authors": [
      "Yansheng Li",
      "Tingzhu Wang",
      "Kang Wu",
      "Linlin Wang",
      "Xin Guo",
      "Wenbin Wang"
    ],
    "abstract": "Scene Graph Generation (SGG) aims to explore the relationships between\nobjects in images and obtain scene summary graphs, thereby better serving\ndownstream tasks. However, the long-tailed problem has adversely affected the\nscene graph's quality. The predictions are dominated by coarse-grained\nrelationships, lacking more informative fine-grained ones. The union region of\none object pair (i.e., one sample) contains rich and dedicated contextual\ninformation, enabling the prediction of the sample-specific bias for refining\nthe original relationship prediction. Therefore, we propose a novel\nSample-Level Bias Prediction (SBP) method for fine-grained SGG (SBG). Firstly,\nwe train a classic SGG model and construct a correction bias set by calculating\nthe margin between the ground truth label and the predicted label with one\nclassic SGG model. Then, we devise a Bias-Oriented Generative Adversarial\nNetwork (BGAN) that learns to predict the constructed correction biases, which\ncan be utilized to correct the original predictions from coarse-grained\nrelationships to fine-grained ones. The extensive experimental results on VG,\nGQA, and VG-1800 datasets demonstrate that our SBG outperforms the\nstate-of-the-art methods in terms of Average@K across three mainstream SGG\nmodels: Motif, VCtree, and Transformer. Compared to dataset-level correction\nmethods on VG, SBG shows a significant average improvement of 5.6%, 3.9%, and\n3.2% on Average@K for tasks PredCls, SGCls, and SGDet, respectively. The code\nwill be available at https://github.com/Zhuzi24/SBG.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "24 pages, 10 figures, ECCV2024",
    "pdf_url": "http://arxiv.org/pdf/2407.19259v1",
    "published_date": "2024-07-27 13:49:06 UTC",
    "updated_date": "2024-07-27 13:49:06 UTC"
  },
  {
    "arxiv_id": "2407.19256v1",
    "title": "Stochastic Parrots or ICU Experts? Large Language Models in Critical Care Medicine: A Scoping Review",
    "authors": [
      "Tongyue Shi",
      "Jun Ma",
      "Zihan Yu",
      "Haowei Xu",
      "Minqi Xiong",
      "Meirong Xiao",
      "Yilin Li",
      "Huiying Zhao",
      "Guilan Kong"
    ],
    "abstract": "With the rapid development of artificial intelligence (AI), large language\nmodels (LLMs) have shown strong capabilities in natural language understanding,\nreasoning, and generation, attracting amounts of research interest in applying\nLLMs to health and medicine. Critical care medicine (CCM) provides diagnosis\nand treatment for critically ill patients who often require intensive\nmonitoring and interventions in intensive care units (ICUs). Can LLMs be\napplied to CCM? Are LLMs just like stochastic parrots or ICU experts in\nassisting clinical decision-making? This scoping review aims to provide a\npanoramic portrait of the application of LLMs in CCM. Literature in seven\ndatabases, including PubMed, Embase, Scopus, Web of Science, CINAHL, IEEE\nXplore, and ACM Digital Library, were searched from January 1, 2019, to June\n10, 2024. Peer-reviewed journal and conference articles that discussed the\napplication of LLMs in critical care settings were included. From an initial\n619 articles, 24 were selected for final review. This review grouped\napplications of LLMs in CCM into three categories: clinical decision support,\nmedical documentation and reporting, and medical education and doctor-patient\ncommunication. LLMs have advantages in handling unstructured data and do not\nrequire manual feature engineering. Meanwhile, applying LLMs to CCM faces\nchallenges, including hallucinations, poor interpretability, bias and alignment\nchallenges, and privacy and ethics issues. Future research should enhance model\nreliability and interpretability, integrate up-to-date medical knowledge, and\nstrengthen privacy and ethical guidelines. As LLMs evolve, they could become\nkey tools in CCM to help improve patient outcomes and optimize healthcare\ndelivery. This study is the first review of LLMs in CCM, aiding researchers,\nclinicians, and policymakers to understand the current status and future\npotentials of LLMs in CCM.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "28 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.19256v1",
    "published_date": "2024-07-27 13:41:43 UTC",
    "updated_date": "2024-07-27 13:41:43 UTC"
  },
  {
    "arxiv_id": "2407.19248v2",
    "title": "Mamba-UIE: Enhancing Underwater Images with Physical Model Constraint",
    "authors": [
      "Song Zhang",
      "Yuqing Duan",
      "Daoliang Li",
      "Ran Zhao"
    ],
    "abstract": "In underwater image enhancement (UIE), convolutional neural networks (CNN)\nhave inherent limitations in modeling long-range dependencies and are less\neffective in recovering global features. While Transformers excel at modeling\nlong-range dependencies, their quadratic computational complexity with\nincreasing image resolution presents significant efficiency challenges.\nAdditionally, most supervised learning methods lack effective physical model\nconstraint, which can lead to insufficient realism and overfitting in generated\nimages. To address these issues, we propose a physical model constraint-based\nunderwater image enhancement framework, Mamba-UIE. Specifically, we decompose\nthe input image into four components: underwater scene radiance, direct\ntransmission map, backscatter transmission map, and global background light.\nThese components are reassembled according to the revised underwater image\nformation model, and the reconstruction consistency constraint is applied\nbetween the reconstructed image and the original image, thereby achieving\neffective physical constraint on the underwater image enhancement process. To\ntackle the quadratic computational complexity of Transformers when handling\nlong sequences, we introduce the Mamba-UIE network based on linear complexity\nstate space models. By incorporating the Mamba in Convolution block, long-range\ndependencies are modeled at both the channel and spatial levels, while the CNN\nbackbone is retained to recover local features and details. Extensive\nexperiments on three public datasets demonstrate that our proposed Mamba-UIE\noutperforms existing state-of-the-art methods, achieving a PSNR of 27.13 and an\nSSIM of 0.93 on the UIEB dataset. Our method is available at\nhttps://github.com/zhangsong1213/Mamba-UIE.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19248v2",
    "published_date": "2024-07-27 13:22:10 UTC",
    "updated_date": "2024-07-31 07:20:53 UTC"
  },
  {
    "arxiv_id": "2407.19216v1",
    "title": "EaTVul: ChatGPT-based Evasion Attack Against Software Vulnerability Detection",
    "authors": [
      "Shigang Liu",
      "Di Cao",
      "Junae Kim",
      "Tamas Abraham",
      "Paul Montague",
      "Seyit Camtepe",
      "Jun Zhang",
      "Yang Xiang"
    ],
    "abstract": "Recently, deep learning has demonstrated promising results in enhancing the\naccuracy of vulnerability detection and identifying vulnerabilities in\nsoftware. However, these techniques are still vulnerable to attacks.\nAdversarial examples can exploit vulnerabilities within deep neural networks,\nposing a significant threat to system security. This study showcases the\nsusceptibility of deep learning models to adversarial attacks, which can\nachieve 100% attack success rate (refer to Table 5). The proposed method,\nEaTVul, encompasses six stages: identification of important samples using\nsupport vector machines, identification of important features using the\nattention mechanism, generation of adversarial data based on these features\nusing ChatGPT, preparation of an adversarial attack pool, selection of seed\ndata using a fuzzy genetic algorithm, and the execution of an evasion attack.\nExtensive experiments demonstrate the effectiveness of EaTVul, achieving an\nattack success rate of more than 83% when the snippet size is greater than 2.\nFurthermore, in most cases with a snippet size of 4, EaTVul achieves a 100%\nattack success rate. The findings of this research emphasize the necessity of\nrobust defenses against adversarial attacks in software vulnerability\ndetection.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19216v1",
    "published_date": "2024-07-27 09:04:54 UTC",
    "updated_date": "2024-07-27 09:04:54 UTC"
  },
  {
    "arxiv_id": "2407.19205v1",
    "title": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's Impact on Spatio-Temporal Cross-Attentions",
    "authors": [
      "Ashkan Taghipour",
      "Morteza Ghahremani",
      "Mohammed Bennamoun",
      "Aref Miri Rekavandi",
      "Zinuo Li",
      "Hamid Laga",
      "Farid Boussaid"
    ],
    "abstract": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19205v1",
    "published_date": "2024-07-27 08:21:14 UTC",
    "updated_date": "2024-07-27 08:21:14 UTC"
  },
  {
    "arxiv_id": "2407.19204v2",
    "title": "Towards the Terminator Economy: Assessing Job Exposure to AI through LLMs",
    "authors": [
      "Emilio Colombo",
      "Fabio Mercorio",
      "Mario Mezzanzanica",
      "Antonio Serino"
    ],
    "abstract": "AI and related technologies are reshaping jobs and tasks, either by\nautomating or augmenting human skills in the workplace. Many researchers have\nbeen working on estimating if and to what extent jobs and tasks are exposed to\nthe risk of being automatized by AI-related technologies. Our work tackles this\nissue through a data-driven approach by: (i) developing a reproducible\nframework that uses cutting-edge open-source large language models to assess\nthe current capabilities of AI and robotics in performing job-related tasks;\n(ii) formalizing and computing a measure of AI exposure by occupation, the Task\nExposure to AI (TEAI) index, and a measure of Task Replacement by AI (TRAI),\nboth validated through a human user evaluation and compared with the state of\nthe art.\n  Our results show that the TEAI index is positively correlated with cognitive,\nproblem-solving and management skills, while it is negatively correlated with\nsocial skills. Applying the index to the US, we obtain that about one-third of\nUS employment is highly exposed to AI, primarily in high-skill jobs requiring a\ngraduate or postgraduate level of education. We also find that AI exposure is\npositively associated with both employment and wage growth in 2003-2023,\nsuggesting that AI has an overall positive effect on productivity.\n  Considering specifically the TRAI index, we find that even in high-skill\noccupations, AI exhibits high variability in task substitution, suggesting that\nAI and humans complement each other within the same occupation, while the\nallocation of tasks within occupations is likely to change.\n  All results, models, and code are freely available online to allow the\ncommunity to reproduce our results, compare outcomes, and use our work as a\nbenchmark to monitor AI's progress over time.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19204v2",
    "published_date": "2024-07-27 08:14:18 UTC",
    "updated_date": "2025-04-15 08:51:16 UTC"
  },
  {
    "arxiv_id": "2407.19203v2",
    "title": "Towards Clean-Label Backdoor Attacks in the Physical World",
    "authors": [
      "Thinh Dao",
      "Cuong Chi Le",
      "Khoa D Doan",
      "Kok-Seng Wong"
    ],
    "abstract": "Deep Neural Networks (DNNs) are shown to be vulnerable to backdoor poisoning\nattacks, with most research focusing on \\textbf{digital triggers} -- special\npatterns added to test-time inputs to induce targeted misclassification.\n\\textbf{Physical triggers}, natural objects within a physical scene, have\nemerged as a desirable alternative since they enable real-time backdoor\nactivations without digital manipulation. However, current physical backdoor\nattacks require poisoned inputs to have incorrect labels, making them easily\ndetectable by human inspection. In this paper, we explore a new paradigm of\nattacks, \\textbf{clean-label physical backdoor attacks (CLPBA)}, via\nexperiments on facial recognition and animal classification tasks. Our study\nreveals that CLPBA could be a serious threat with the right poisoning algorithm\nand physical trigger. A key finding is that different from digital backdoor\nattacks which exploit memorization to plant backdoors in deep nets, CLPBA works\nby embedding the feature of the trigger distribution (i.e., the distribution of\ntrigger samples) to the poisoned images through the perturbations. We also find\nthat representative defenses cannot defend against CLPBA easily since CLPBA\nfundamentally breaks the core assumptions behind these defenses. Our study\nhighlights accidental backdoor activations as a limitation of CLPBA, happening\nwhen unintended objects or classes cause the model to misclassify as the target\nclass. The code and dataset can be found at\nhttps://github.com/21thinh/Clean-Label-Physical-Backdoor-Attacks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "21 pages, 17 figures, 16 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.19203v2",
    "published_date": "2024-07-27 08:13:07 UTC",
    "updated_date": "2024-11-25 16:40:54 UTC"
  },
  {
    "arxiv_id": "2407.19200v2",
    "title": "On Behalf of the Stakeholders: Trends in NLP Model Interpretability in the Era of LLMs",
    "authors": [
      "Nitay Calderon",
      "Roi Reichart"
    ],
    "abstract": "Recent advancements in NLP systems, particularly with the introduction of\nLLMs, have led to widespread adoption of these systems by a broad spectrum of\nusers across various domains, impacting decision-making, the job market,\nsociety, and scientific research. This surge in usage has led to an explosion\nin NLP model interpretability and analysis research, accompanied by numerous\ntechnical surveys. Yet, these surveys often overlook the needs and perspectives\nof explanation stakeholders. In this paper, we address three fundamental\nquestions: Why do we need interpretability, what are we interpreting, and how?\nBy exploring these questions, we examine existing interpretability paradigms,\ntheir properties, and their relevance to different stakeholders. We further\nexplore the practical implications of these paradigms by analyzing trends from\nthe past decade across multiple research fields. To this end, we retrieved\nthousands of papers and employed an LLM to characterize them. Our analysis\nreveals significant disparities between NLP developers and non-developer users,\nas well as between research fields, underscoring the diverse needs of\nstakeholders. For example, explanations of internal model components are rarely\nused outside the NLP field. We hope this paper informs the future design,\ndevelopment, and application of methods that align with the objectives and\nrequirements of various stakeholders.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19200v2",
    "published_date": "2024-07-27 08:00:27 UTC",
    "updated_date": "2025-02-04 08:01:59 UTC"
  },
  {
    "arxiv_id": "2407.19198v2",
    "title": "Towards the Dynamics of a DNN Learning Symbolic Interactions",
    "authors": [
      "Qihan Ren",
      "Junpeng Zhang",
      "Yang Xu",
      "Yue Xin",
      "Dongrui Liu",
      "Quanshi Zhang"
    ],
    "abstract": "This study proves the two-phase dynamics of a deep neural network (DNN)\nlearning interactions. Despite the long disappointing view of the faithfulness\nof post-hoc explanation of a DNN, a series of theorems have been proven in\nrecent years to show that for a given input sample, a small set of interactions\nbetween input variables can be considered as primitive inference patterns that\nfaithfully represent a DNN's detailed inference logic on that sample.\nParticularly, Zhang et al. have observed that various DNNs all learn\ninteractions of different complexities in two distinct phases, and this\ntwo-phase dynamics well explains how a DNN changes from under-fitting to\nover-fitting. Therefore, in this study, we mathematically prove the two-phase\ndynamics of interactions, providing a theoretical mechanism for how the\ngeneralization power of a DNN changes during the training process. Experiments\nshow that our theory well predicts the real dynamics of interactions on\ndifferent DNNs trained for various tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19198v2",
    "published_date": "2024-07-27 07:34:49 UTC",
    "updated_date": "2024-11-25 08:57:20 UTC"
  },
  {
    "arxiv_id": "2407.19196v1",
    "title": "Why Misinformation is Created? Detecting them by Integrating Intent Features",
    "authors": [
      "Bing Wang",
      "Ximing Li",
      "Changchun Li",
      "Bo Fu",
      "Songwen Pei",
      "Shengsheng Wang"
    ],
    "abstract": "Various social media platforms, e.g., Twitter and Reddit, allow people to\ndisseminate a plethora of information more efficiently and conveniently.\nHowever, they are inevitably full of misinformation, causing damage to diverse\naspects of our daily lives. To reduce the negative impact, timely\nidentification of misinformation, namely Misinformation Detection (MD), has\nbecome an active research topic receiving widespread attention. As a complex\nphenomenon, the veracity of an article is influenced by various aspects. In\nthis paper, we are inspired by the opposition of intents between misinformation\nand real information. Accordingly, we propose to reason the intent of articles\nand form the corresponding intent features to promote the veracity\ndiscrimination of article features. To achieve this, we build a hierarchy of a\nset of intents for both misinformation and real information by referring to the\nexisting psychological theories, and we apply it to reason the intent of\narticles by progressively generating binary answers with an encoder-decoder\nstructure. We form the corresponding intent features and integrate it with the\ntoken features to achieve more discriminative article features for MD. Upon\nthese ideas, we suggest a novel MD method, namely Detecting Misinformation by\nIntegrating Intent featuRes (DM-INTER). To evaluate the performance of\nDM-INTER, we conduct extensive experiments on benchmark MD datasets. The\nexperimental results validate that DM-INTER can outperform the existing\nbaseline MD methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 3 figures. Accepted by CIKM 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.19196v1",
    "published_date": "2024-07-27 07:30:47 UTC",
    "updated_date": "2024-07-27 07:30:47 UTC"
  },
  {
    "arxiv_id": "2407.19193v1",
    "title": "A collaborative ensemble construction method for federated random forest",
    "authors": [
      "Penjan Antonio Eng Lim",
      "Cheong Hee Park"
    ],
    "abstract": "Random forests are considered a cornerstone in machine learning for their\nrobustness and versatility. Despite these strengths, their conventional\ncentralized training is ill-suited for the modern landscape of data that is\noften distributed, sensitive, and subject to privacy concerns. Federated\nlearning (FL) provides a compelling solution to this problem, enabling models\nto be trained across a group of clients while maintaining the privacy of each\nclient's data. However, adapting tree-based methods like random forests to\nfederated settings introduces significant challenges, particularly when it\ncomes to non-identically distributed (non-IID) data across clients, which is a\ncommon scenario in real-world applications. This paper presents a federated\nrandom forest approach that employs a novel ensemble construction method aimed\nat improving performance under non-IID data. Instead of growing trees\nindependently in each client, our approach ensures each decision tree in the\nensemble is iteratively and collectively grown across clients. To preserve the\nprivacy of the client's data, we confine the information stored in the leaf\nnodes to the majority class label identified from the samples of the client's\nlocal data that reach each node. This limited disclosure preserves the\nconfidentiality of the underlying data distribution of clients, thereby\nenhancing the privacy of the federated learning process. Furthermore, our\ncollaborative ensemble construction strategy allows the ensemble to better\nreflect the data's heterogeneity across different clients, enhancing its\nperformance on non-IID data, as our experimental results confirm.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC",
      "68T05 (Primary), 68W40, 62H30 (Secondary)",
      "I.2.6; I.2.11; K.4.1"
    ],
    "primary_category": "cs.LG",
    "comment": "This is the authors' accepted manuscript of an article published in\n  the journal Expert Systems With Applications. Published version available at:\n  https://www.sciencedirect.com/science/article/pii/S0957417424016099. 22\n  pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.19193v1",
    "published_date": "2024-07-27 07:21:45 UTC",
    "updated_date": "2024-07-27 07:21:45 UTC"
  },
  {
    "arxiv_id": "2408.01460v1",
    "title": "LocalValueBench: A Collaboratively Built and Extensible Benchmark for Evaluating Localized Value Alignment and Ethical Safety in Large Language Models",
    "authors": [
      "Gwenyth Isobel Meadows",
      "Nicholas Wai Long Lau",
      "Eva Adelina Susanto",
      "Chi Lok Yu",
      "Aditya Paul"
    ],
    "abstract": "The proliferation of large language models (LLMs) requires robust evaluation\nof their alignment with local values and ethical standards, especially as\nexisting benchmarks often reflect the cultural, legal, and ideological values\nof their creators. \\textsc{LocalValueBench}, introduced in this paper, is an\nextensible benchmark designed to assess LLMs' adherence to Australian values,\nand provides a framework for regulators worldwide to develop their own LLM\nbenchmarks for local value alignment. Employing a novel typology for ethical\nreasoning and an interrogation approach, we curated comprehensive questions and\nutilized prompt engineering strategies to probe LLMs' value alignment. Our\nevaluation criteria quantified deviations from local values, ensuring a\nrigorous assessment process. Comparative analysis of three commercial LLMs by\nUSA vendors revealed significant insights into their effectiveness and\nlimitations, demonstrating the critical importance of value alignment. This\nstudy offers valuable tools and methodologies for regulators to create tailored\nbenchmarks, highlighting avenues for future research to enhance ethical AI\ndevelopment.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01460v1",
    "published_date": "2024-07-27 05:55:42 UTC",
    "updated_date": "2024-07-27 05:55:42 UTC"
  },
  {
    "arxiv_id": "2407.19186v1",
    "title": "Channel Boosted CNN-Transformer-based Multi-Level and Multi-Scale Nuclei Segmentation",
    "authors": [
      "Zunaira Rauf",
      "Abdul Rehman Khan",
      "Asifullah Khan"
    ],
    "abstract": "Accurate nuclei segmentation is an essential foundation for various\napplications in computational pathology, including cancer diagnosis and\ntreatment planning. Even slight variations in nuclei representations can\nsignificantly impact these downstream tasks. However, achieving accurate\nsegmentation remains challenging due to factors like clustered nuclei, high\nintra-class variability in size and shape, resemblance to other cells, and\ncolor or contrast variations between nuclei and background. Despite the\nextensive utilization of Convolutional Neural Networks (CNNs) in medical image\nsegmentation, they may have trouble capturing long-range dependencies crucial\nfor accurate nuclei delineation. Transformers address this limitation but might\nmiss essential low-level features. To overcome these limitations, we utilized\nCNN-Transformer-based techniques for nuclei segmentation in H&E stained\nhistology images. In this work, we proposed two CNN-Transformer architectures,\nNuclei Hybrid Vision Transformer (NucleiHVT) and Channel Boosted Nuclei Hybrid\nVision Transformer (CB-NucleiHVT), that leverage the strengths of both CNNs and\nTransformers to effectively learn nuclei boundaries in multi-organ histology\nimages. The first architecture, NucleiHVT is inspired by the UNet architecture\nand incorporates the dual attention mechanism to capture both multi-level and\nmulti-scale context effectively. The CB-NucleiHVT network, on the other hand,\nutilizes the concept of channel boosting to learn diverse feature spaces,\nenhancing the model's ability to distinguish subtle variations in nuclei\ncharacteristics. Detailed evaluation of two medical image segmentation datasets\nshows that the proposed architectures outperform existing CNN-based,\nTransformer-based, and hybrid methods. The proposed networks demonstrated\neffective results both in terms of quantitative metrics, and qualitative visual\nassessment.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19186v1",
    "published_date": "2024-07-27 05:54:05 UTC",
    "updated_date": "2024-07-27 05:54:05 UTC"
  },
  {
    "arxiv_id": "2407.19185v1",
    "title": "LLaVA-Read: Enhancing Reading Ability of Multimodal Language Models",
    "authors": [
      "Ruiyi Zhang",
      "Yufan Zhou",
      "Jian Chen",
      "Jiuxiang Gu",
      "Changyou Chen",
      "Tong Sun"
    ],
    "abstract": "Large multimodal language models have demonstrated impressive capabilities in\nunderstanding and manipulating images. However, many of these models struggle\nwith comprehending intensive textual contents embedded within the images,\nprimarily due to the limited text recognition and layout understanding ability.\nTo understand the sources of these limitations, we perform an exploratory\nanalysis showing the drawbacks of classical visual encoders on visual text\nunderstanding. Hence, we present LLaVA-Read, a multimodal large language model\nthat utilizes dual visual encoders along with a visual text encoder. Our model\nsurpasses existing state-of-the-art models in various text-rich image\nunderstanding tasks, showcasing enhanced comprehension of textual content\nwithin images. Together, our research suggests visual text understanding\nremains an open challenge and an efficient visual text encoder is crucial for\nfuture successful multimodal systems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2024 Under Review",
    "pdf_url": "http://arxiv.org/pdf/2407.19185v1",
    "published_date": "2024-07-27 05:53:37 UTC",
    "updated_date": "2024-07-27 05:53:37 UTC"
  },
  {
    "arxiv_id": "2407.19183v1",
    "title": "Graph Memory Learning: Imitating Lifelong Remembering and Forgetting of Brain Networks",
    "authors": [
      "Jiaxing Miao",
      "Liang Hu",
      "Qi Zhang",
      "Longbing Cao"
    ],
    "abstract": "Graph data in real-world scenarios undergo rapid and frequent changes, making\nit challenging for existing graph models to effectively handle the continuous\ninflux of new data and accommodate data withdrawal requests. The approach to\nfrequently retraining graph models is resource intensive and impractical. To\naddress this pressing challenge, this paper introduces a new concept of graph\nmemory learning. Its core idea is to enable a graph model to selectively\nremember new knowledge but forget old knowledge. Building on this approach, the\npaper presents a novel graph memory learning framework - Brain-inspired Graph\nMemory Learning (BGML), inspired by brain network dynamics and\nfunction-structure coupling strategies. BGML incorporates a multi-granular\nhierarchical progressive learning mechanism rooted in feature graph grain\nlearning to mitigate potential conflict between memorization and forgetting in\ngraph memory learning. This mechanism allows for a comprehensive and\nmulti-level perception of local details within evolving graphs. In addition, to\ntackle the issue of unreliable structures in newly added incremental\ninformation, the paper introduces an information self-assessment ownership\nmechanism. This mechanism not only facilitates the propagation of incremental\ninformation within the model but also effectively preserves the integrity of\npast experiences. We design five types of graph memory learning tasks: regular,\nmemory, unlearning, data-incremental, and class-incremental to evaluate BGML.\nIts excellent performance is confirmed through extensive experiments on\nmultiple real-world node classification datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19183v1",
    "published_date": "2024-07-27 05:50:54 UTC",
    "updated_date": "2024-07-27 05:50:54 UTC"
  },
  {
    "arxiv_id": "2408.01459v1",
    "title": "AgentPeerTalk: Empowering Students through Agentic-AI-Driven Discernment of Bullying and Joking in Peer Interactions in Schools",
    "authors": [
      "Aditya Paul",
      "Chi Lok Yu",
      "Eva Adelina Susanto",
      "Nicholas Wai Long Lau",
      "Gwenyth Isobel Meadows"
    ],
    "abstract": "Addressing school bullying effectively and promptly is crucial for the mental\nhealth of students. This study examined the potential of large language models\n(LLMs) to empower students by discerning between bullying and joking in school\npeer interactions. We employed ChatGPT-4, Gemini 1.5 Pro, and Claude 3 Opus,\nevaluating their effectiveness through human review. Our results revealed that\nnot all LLMs were suitable for an agentic approach, with ChatGPT-4 showing the\nmost promise. We observed variations in LLM outputs, possibly influenced by\npolitical overcorrectness, context window limitations, and pre-existing bias in\ntheir training data. ChatGPT-4 excelled in context-specific accuracy after\nimplementing the agentic approach, highlighting its potential to provide\ncontinuous, real-time support to vulnerable students. This study underlines the\nsignificant social impact of using agentic AI in educational settings, offering\na new avenue for reducing the negative consequences of bullying and enhancing\nstudent well-being.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01459v1",
    "published_date": "2024-07-27 05:50:02 UTC",
    "updated_date": "2024-07-27 05:50:02 UTC"
  },
  {
    "arxiv_id": "2407.19173v1",
    "title": "FarSSiBERT: A Novel Transformer-based Model for Semantic Similarity Measurement of Persian Social Networks Informal Texts",
    "authors": [
      "Seyed Mojtaba Sadjadi",
      "Zeinab Rajabi",
      "Leila Rabiei",
      "Mohammad-Shahram Moin"
    ],
    "abstract": "One fundamental task for NLP is to determine the similarity between two texts\nand evaluate the extent of their likeness. The previous methods for the Persian\nlanguage have low accuracy and are unable to comprehend the structure and\nmeaning of texts effectively. Additionally, these methods primarily focus on\nformal texts, but in real-world applications of text processing, there is a\nneed for robust methods that can handle colloquial texts. This requires\nalgorithms that consider the structure and significance of words based on\ncontext, rather than just the frequency of words. The lack of a proper dataset\nfor this task in the Persian language makes it important to develop such\nalgorithms and construct a dataset for Persian text. This paper introduces a\nnew transformer-based model to measure semantic similarity between Persian\ninformal short texts from social networks. In addition, a Persian dataset named\nFarSSiM has been constructed for this purpose, using real data from social\nnetworks and manually annotated and verified by a linguistic expert team. The\nproposed model involves training a large language model using the BERT\narchitecture from scratch. This model, called FarSSiBERT, is pre-trained on\napproximately 104 million Persian informal short texts from social networks,\nmaking it one of a kind in the Persian language. Moreover, a novel specialized\ninformal language tokenizer is provided that not only performs tokenization on\nformal texts well but also accurately identifies tokens that other Persian\ntokenizers are unable to recognize. It has been demonstrated that our proposed\nmodel outperforms ParsBERT, laBSE, and multilingual BERT in the Pearson and\nSpearman's coefficient criteria. Additionally, the pre-trained large language\nmodel has great potential for use in other NLP tasks on colloquial text and as\na tokenizer for less-known informal words.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19173v1",
    "published_date": "2024-07-27 05:04:49 UTC",
    "updated_date": "2024-07-27 05:04:49 UTC"
  },
  {
    "arxiv_id": "2407.19160v1",
    "title": "Decomposing heterogeneous dynamical systems with graph neural networks",
    "authors": [
      "Cdric Allier",
      "Magdalena C. Schneider",
      "Michael Innerberger",
      "Larissa Heinrich",
      "John A. Bogovic",
      "Stephan Saalfeld"
    ],
    "abstract": "Natural physical, chemical, and biological dynamical systems are often\ncomplex, with heterogeneous components interacting in diverse ways. We show\nthat graph neural networks can be designed to jointly learn the interaction\nrules and the structure of the heterogeneity from data alone. The learned\nlatent structure and dynamics can be used to virtually decompose the complex\nsystem which is necessary to parameterize and infer the underlying governing\nequations. We tested the approach with simulation experiments of moving\nparticles and vector fields that interact with each other. While our current\naim is to better understand and validate the approach with simulated data, we\nanticipate it to become a generally applicable tool to uncover the governing\nrules underlying complex dynamics observed in nature.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.DS"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 4 figures, 2 pages appendix, 2 supplementary tables, 18\n  supplementary figures, 13 videos linked to youtube",
    "pdf_url": "http://arxiv.org/pdf/2407.19160v1",
    "published_date": "2024-07-27 04:03:12 UTC",
    "updated_date": "2024-07-27 04:03:12 UTC"
  },
  {
    "arxiv_id": "2407.20291v1",
    "title": "Formalization of Dialogue in the Decision Support System of Dr. Watson Type",
    "authors": [
      "Saveli Goldberg",
      "Vladimir Sluchak"
    ],
    "abstract": "The article further develops and formalizes a theory of friendly dialogue in\nan AI System of Dr. Watson type, as proposed in our previous\npublication[4],[19]. The main principle of this type of AI is to guide the user\ntoward a solution in a friendly manner, using questions based on the analysis\nof user input and data collected in the system.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20291v1",
    "published_date": "2024-07-27 01:56:33 UTC",
    "updated_date": "2024-07-27 01:56:33 UTC"
  },
  {
    "arxiv_id": "2407.19142v1",
    "title": "On the benefits of pixel-based hierarchical policies for task generalization",
    "authors": [
      "Tudor Cristea-Platon",
      "Bogdan Mazoure",
      "Josh Susskind",
      "Walter Talbott"
    ],
    "abstract": "Reinforcement learning practitioners often avoid hierarchical policies,\nespecially in image-based observation spaces. Typically, the single-task\nperformance improvement over flat-policy counterparts does not justify the\nadditional complexity associated with implementing a hierarchy. However, by\nintroducing multiple decision-making levels, hierarchical policies can compose\nlower-level policies to more effectively generalize between tasks, highlighting\nthe need for multi-task evaluations. We analyze the benefits of hierarchy\nthrough simulated multi-task robotic control experiments from pixels. Our\nresults show that hierarchical policies trained with task conditioning can (1)\nincrease performance on training tasks, (2) lead to improved reward and\nstate-space generalizations in similar tasks, and (3) decrease the complexity\nof fine tuning required to solve novel tasks. Thus, we believe that\nhierarchical policies should be considered when building reinforcement learning\narchitectures capable of generalizing between tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.19142v1",
    "published_date": "2024-07-27 01:26:26 UTC",
    "updated_date": "2024-07-27 01:26:26 UTC"
  }
]
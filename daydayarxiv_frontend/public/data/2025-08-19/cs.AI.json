{
  "date": "2025-08-19",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-08-19 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ **æ•™æˆå¯„è¯­**ï¼š\nä»Šå¤©çš„ arXiv åˆ—è¡¨ä»¤äººå…´å¥‹ï¼Œå¯ä»¥è¯´æ˜¯ **\"Agentic Reasoning\" (ä»£ç†æ¨ç†) ä¸ \"Input-Time Scaling\" (æ¨ç†æ—¶æ‰©å±•)** çš„çˆ†å‘æ—¥ã€‚æˆ‘ä»¬çœ‹åˆ°äº†å…³äº **RLï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰ä¸ Searchï¼ˆæœç´¢ï¼‰ç»Ÿä¸€** çš„é‡ç£…ç†è®ºæ–‡ç« ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿçš„ Process Reward Models (PRM) è®­ç»ƒèŒƒå¼ï¼›åŒæ—¶ï¼Œæœ‰ç ”ç©¶è¡¨æ˜ AI å·²ç»å¯ä»¥å…¨è‡ªåŠ¨å®Œæˆä»å‡è®¾æå‡ºåˆ°è®ºæ–‡æ’°å†™çš„**å¿ƒç†å­¦ç§‘å­¦å®éªŒ**ã€‚æ­¤å¤–ï¼Œå…³äº LLM **æ¨ç†æ—¶è®¡ç®— (Test-time Compute)** çš„è®¨è®ºä¹Ÿåœ¨å‡æ¸©ï¼Œ\"Input-Time Scaling\" æå‡ºäº†æ¯”å †ç Œè®­ç»ƒæ•°æ®æ›´é«˜æ•ˆçš„è·¯å¾„ã€‚\n\nä»¥ä¸‹æ˜¯ä¸ºæ‚¨ç²¾é€‰çš„ä»Šæ—¥å¿…è¯»è®ºæ–‡ï¼š\n\n---\n\n### ğŸš€ é‡ç£…æ¨èï¼šæ¨ç†ã€RL ä¸ ç§‘å­¦ä»£ç†\n\n**1. [RL ä¸ Search çš„å¤§ä¸€ç»Ÿ] Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS**\n> **ä½ çš„ RL å¥–åŠ±å‡½æ•°å°±æ˜¯æœ€å¥½çš„æœç´¢ PRMï¼šç»Ÿä¸€å¼ºåŒ–å­¦ä¹ ä¸åŸºäºæœç´¢çš„æµ‹è¯•æ—¶æ‰©å±•**\n> *Can Jin, et al.*\n\n*   **æ ¸å¿ƒçœ‹ç‚¹**ï¼šè¿™ç¯‡è®ºæ–‡å¯èƒ½æ”¹å˜æˆ‘ä»¬è®­ç»ƒæ¨ç†æ¨¡å‹çš„æ–¹å¼ã€‚å®ƒæŒ‘æˆ˜äº†å½“å‰åˆ†å‰²çš„ä¸¤ç§èŒƒå¼ï¼šRLï¼ˆä¼˜åŒ–ç¨€ç–å¥–åŠ±ï¼‰å’Œ æœç´¢ï¼ˆä¾èµ–æ˜‚è´µçš„ PRM è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼‰ã€‚\n*   **ä¸»è¦è´¡çŒ®**ï¼šæå‡ºäº† **AIRL-S** æ¡†æ¶ï¼Œæ ¸å¿ƒæ´å¯Ÿæ˜¯ï¼š**åœ¨ RL è®­ç»ƒä¸­å­¦åˆ°çš„å¥–åŠ±å‡½æ•°ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯æŒ‡å¯¼æœç´¢çš„æœ€ä½³ PRM**ã€‚\n*   **æ–¹æ³•ä¸å‘ç°**ï¼šåˆ©ç”¨å¯¹æŠ—é€†å¼ºåŒ–å­¦ä¹  (AIRL) ç»“åˆç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– (GRPO)ï¼Œç›´æ¥ä»æ­£ç¡®çš„æ¨ç†è½¨è¿¹ä¸­å­¦ä¹ å¯†é›†çš„åŠ¨æ€ PRMï¼Œ**å®Œå…¨ä¸éœ€è¦äººå·¥æ ‡æ³¨çš„ä¸­é—´è¿‡ç¨‹æ•°æ®**ã€‚å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ•°å­¦å’Œä»£ç ä»»åŠ¡ä¸Šå¹³å‡æå‡ 9%ï¼ŒåŒ¹æ•Œ GPT-4oï¼Œä¸”å…¶è®­ç»ƒå‡ºçš„ PRM ä¼˜äºæ‰€æœ‰ä¾èµ–æ ‡æ³¨æ•°æ®çš„åŸºçº¿ã€‚\n\n**2. [AI è‡ªåŠ¨åšç§‘å­¦] Virtuous Machines: Towards Artificial General Science**\n> **æœ‰å¾·ä¹‹æœºï¼šè¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½ç§‘å­¦**\n> *Gabrielle Wehr, et al.*\n\n*   **æ ¸å¿ƒçœ‹ç‚¹**ï¼šAI ä¸å†åªæ˜¯è¾…åŠ©å·¥å…·ï¼Œè€Œæ˜¯ç‹¬ç«‹çš„ç ”ç©¶è€…ã€‚\n*   **ä¸»è¦è´¡çŒ®**ï¼šå±•ç¤ºäº†ä¸€ä¸ªé€šç”¨ Agent ç³»ç»Ÿï¼Œèƒ½ç‹¬ç«‹å®Œæˆ**æ•´ä¸ªç§‘å­¦å·¥ä½œæµ**ï¼šä»å‡è®¾ç”Ÿæˆã€æ•°æ®æ”¶é›†ã€åˆ†æç®¡é“ç¼–å†™åˆ°è®ºæ–‡æ’°å†™ã€‚\n*   **ä»¤äººå°è±¡æ·±åˆ»**ï¼šè¯¥ç³»ç»Ÿè‡ªä¸»è®¾è®¡å¹¶æ‰§è¡Œäº†ä¸‰ä¸ªå¿ƒç†å­¦å®éªŒï¼ˆå¦‚è§†è§‰å·¥ä½œè®°å¿†ï¼‰ï¼Œæ‹›å‹Ÿäº† 288 åå‚ä¸è€…ï¼ˆåœ¨çº¿æ•°æ®æ”¶é›†ï¼‰ï¼Œå¹¶é€šè¿‡ 8 å°æ—¶ä»¥ä¸Šçš„è¿ç»­ç¼–ç è¿›è¡Œåˆ†æï¼Œæœ€ç»ˆäº§å‡ºäº†å®Œæ•´çš„è®ºæ–‡ã€‚è¿™æ ‡å¿—ç€ AI å‘ \"Embodies Science\" è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚\n\n**3. [æ‰“ç ´æ•°æ®è¿·ä¿¡] Input-Time Scaling**\n> **è¾“å…¥ç«¯æ‰©å±•ï¼šå°‘å³æ˜¯å¤š**\n> *Rapheal Huang, Weilong Guo*\n\n*   **æ ¸å¿ƒçœ‹ç‚¹**ï¼šåç›´è§‰å‘ç°â€”â€”åªè¦ 1000 ä¸ªæ ·æœ¬å°±èƒ½æ¿€å‘å¼ºæ¨ç†èƒ½åŠ›ï¼Ÿ\n*   **ä¸»è¦è´¡çŒ®**ï¼šæå‡ºäº† \"Input-Time Scaling\" èŒƒå¼ã€‚ä½œè€…å‘ç°ï¼Œä¸éœ€è¦æµ·é‡æ•°æ®ï¼Œåˆ©ç”¨ LLM çš„å…ƒçŸ¥è¯†åœ¨è¾“å…¥ç«¯ï¼ˆInput-timeï¼‰è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œç»“åˆè®­ç»ƒå’Œæµ‹è¯•çš„ååŒè®¾è®¡ (Train-Test Co-design)ï¼Œæ•ˆæœæƒŠäººã€‚\n*   **å…³é”®å‘ç°**ï¼šä»…ç”¨ **1k ä¸ªæ ·æœ¬**ï¼ˆç”šè‡³åŒ…å«çœ‹ä¼¼ä½è´¨é‡çš„æ•°æ®ï¼‰ï¼Œåœ¨ Qwen2.5-32B ä¸Šå°±è¾¾åˆ°äº† SOTA æ°´å¹³ï¼ˆAIME24 76.7% pass@1ï¼‰ï¼Œè¿™æŒ‘æˆ˜äº† \"Garbage in, Garbage out\" çš„ä¼ ç»Ÿä¿¡æ¡ï¼Œè¡¨æ˜é«˜è´¨é‡çš„å°æ•°æ®é›†ç»“åˆå¼ºæ¨ç†ç­–ç•¥å¯èƒ½æ¯”æµ·é‡æ•°æ®æ›´æœ‰æ•ˆã€‚\n\n**4. [RL æ‰“ç ´ SFT ç“¶é¢ˆ] Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation**\n> **æ‰“ç ´ SFT ç“¶é¢ˆï¼šç”¨äºå›¾è¡¨åˆ°ä»£ç ç”Ÿæˆçš„å¤šæ¨¡æ€ç»“æ„åŒ–å¼ºåŒ–å­¦ä¹ **\n> *Lei Chen, et al.*\n\n*   **æ ¸å¿ƒçœ‹ç‚¹**ï¼šåœ¨å¤§è§„æ¨¡ SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰ä¹‹åï¼Œæ€§èƒ½å¾€å¾€ä¼šè¿›å…¥å¹³å°æœŸï¼Œæ€ä¹ˆç ´ï¼Ÿ\n*   **ä¸»è¦è´¡çŒ®**ï¼šæ„å»ºäº† **MSRL**ï¼ˆå¤šæ¨¡æ€ç»“æ„åŒ–å¼ºåŒ–å­¦ä¹ ï¼‰ã€‚ä½œè€…æ„å»ºäº†åŒ…å« 300 ä¸‡å›¾è¡¨-ä»£ç å¯¹çš„æ•°æ®é›†ï¼Œå‘ç°å•çº¯å¢åŠ  SFT æ•°æ®æ”¶ç›Šé€’å‡ã€‚é€šè¿‡å¼•å…¥å¤šæ¨¡æ€å¥–åŠ±ï¼ˆæ–‡æœ¬è§„åˆ™æ£€æŸ¥ + è§†è§‰ç»“æ„ç›¸ä¼¼åº¦ï¼‰ï¼ŒMSRL æˆåŠŸçªç ´äº†å¹³å°æœŸï¼Œåœ¨ ChartMimic ç­‰åŸºå‡†ä¸Šæå‡æ˜¾è‘—ã€‚\n\n---\n\n### ğŸ§  æ¨¡å‹å®‰å…¨ã€å¹»è§‰ä¸å¯¹é½\n\n**5. [æ— å¤–éƒ¨çŸ¥è¯†çš„å¹»è§‰æ£€æµ‹] Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency**\n> **é€šè¿‡ç»†ç²’åº¦è·¨æ¨¡å‹ä¸€è‡´æ€§å®ç°é›¶çŸ¥è¯† LLM å¹»è§‰æ£€æµ‹ä¸ç¼“è§£**\n> *Aman Goel, et al.*\n\n*   **æ ¸å¿ƒçœ‹ç‚¹**ï¼š**Finch-Zk** æ¡†æ¶ã€‚\n*   **æ–¹æ³•**ï¼šä¸éœ€è¦å¤–éƒ¨æœç´¢å¼•æ“æˆ–çŸ¥è¯†åº“ã€‚å®ƒé€šè¿‡æ¯”è¾ƒä¸åŒæ¨¡å‹å¯¹è¯­ä¹‰ç­‰æ•ˆ prompt çš„å“åº”ï¼Œåˆ©ç”¨è·¨æ¨¡å‹çš„ä¸€è‡´æ€§æ¥æ£€æµ‹å¹»è§‰ã€‚\n*   **æ•ˆæœ**ï¼šå¹»è§‰æ£€æµ‹ F1 åˆ†æ•°æå‡ 6-39%ï¼Œåœ¨ GPQA-diamond æ•°æ®é›†ä¸Šå‡†ç¡®ç‡æå‡ 9%ã€‚è¿™æ˜¯ä¸€ç§éå¸¸é€‚åˆç”Ÿäº§ç¯å¢ƒè½åœ°çš„â€œé»‘ç›’â€å®ˆæŠ¤æ–¹æ¡ˆã€‚\n\n**6. [çŸ¥è¡Œä¸åˆä¸€] LM Agents May Fail to Act on Their Own Risk Knowledge**\n> **LM Agents å¯èƒ½æ— æ³•ä¾æ®è‡ªèº«çš„é£é™©çŸ¥è¯†é‡‡å–è¡ŒåŠ¨**\n> *Yuzhi Tang, et al.*\n\n*   **æ ¸å¿ƒçœ‹ç‚¹**ï¼šæ¨¡å‹çŸ¥é“ä»€ä¹ˆæ˜¯å±é™©çš„ï¼Œä½†çœŸè®©å®ƒå¹²æ´»æ—¶ï¼Œå®ƒè¿˜æ˜¯ä¼šå¹²ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šè¿™æ˜¯ä¸€ä¸ªç±»ä¼¼ \"Generator-Validator Gap\" çš„ç°è±¡ã€‚æ¨¡å‹åœ¨å›ç­” \"sudo rm -rf /* æ˜¯å¦å±é™©\" æ—¶å‡†ç¡®ç‡ >98%ï¼Œä½†åœ¨å®é™…ä½œä¸º Agent æ‰§è¡Œä»»åŠ¡æ—¶ï¼Œè¯†åˆ«é£é™©çš„èƒ½åŠ›ä¸‹é™ 23%ï¼Œä¸”ç»å¸¸æ‰§è¡Œé«˜å±æ“ä½œã€‚**å•çº¯æå‡æ¨¡å‹æ¨ç†èƒ½åŠ›å¹¶ä¸èƒ½è§£å†³è¿™ä¸ªé—®é¢˜**ã€‚\n\n**7. [ä»£ç æ ¼å¼çš„éšå½¢æˆæœ¬] The Hidden Cost of Readability: How Code Formatting Silently Consumes Your LLM Budget**\n> **å¯è¯»æ€§çš„éšæ€§æˆæœ¬ï¼šä»£ç æ ¼å¼åŒ–å¦‚ä½•æ‚„æ‚„æ¶ˆè€—ä½ çš„ LLM é¢„ç®—**\n> *Dangfeng Pan, et al.*\n\n*   **æ ¸å¿ƒçœ‹ç‚¹**ï¼šä¸€ä¸ªéå¸¸å®ç”¨ä¸”æœ‰è¶£çš„å·¥ä¸šç•Œå‘ç°ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šä»£ç ä¸­çš„ç¼©è¿›ã€æ¢è¡Œç­‰æ˜¯ä¸ºäº†äººç±»å¯è¯»æ€§ï¼Œå¯¹ LLM æ¥è¯´æ˜¯ç´¯èµ˜ã€‚ç§»é™¤è¿™äº›æ ¼å¼å…ƒç´ ï¼ˆUnformatted Codeï¼‰ï¼ŒLLM çš„æ€§èƒ½å‡ ä¹ä¸å—å½±å“ï¼Œä½†èƒ½**å‡å°‘ 24.5% çš„è¾“å…¥ Token**ï¼Œå¹¶èƒ½é€šè¿‡å¾®è°ƒå‡å°‘ 36.1% çš„è¾“å‡º Tokenã€‚è¿™å¯¹äºå¤§è§„æ¨¡ä»£ç ç”Ÿæˆçš„é™æœ¬å¢æ•ˆæ„ä¹‰é‡å¤§ã€‚\n\n---\n\n### ğŸ‘ï¸ è§†è§‰ã€å¤šæ¨¡æ€ä¸å…·èº«æ™ºèƒ½\n\n**8. [3D æ¸¸æˆåŸºç¡€æ¨¡å‹] Pixels to Play: A Foundation Model for 3D Gameplay**\n> **Pixels to Playï¼š3D æ¸¸æˆçš„åŸºç¡€æ¨¡å‹**\n> *Yuguang Yue, et al.*\n\n*   **ä¸»è¦è´¡çŒ®**ï¼šå‘å¸ƒäº† **P2P0.1**ï¼Œä¸€ä¸ªé€šè¿‡è¡Œä¸ºå…‹éš† (Behavior Cloning) è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿåƒäººç±»ä¸€æ ·ç©å„ç§ 3D æ¸¸æˆï¼ˆå¦‚ Roblox, MS-DOS ç»å…¸æ¸¸æˆï¼‰ã€‚\n*   **æ–¹æ³•**ï¼šåˆ©ç”¨æœ‰æ ‡ç­¾çš„äººç±»æ¼”ç¤ºå’Œæ— æ ‡ç­¾çš„å…¬å¼€è§†é¢‘ï¼ˆé€šè¿‡é€†åŠ¨åŠ›å­¦æ¨¡å‹è¡¥å…¨åŠ¨ä½œï¼‰ã€‚å®ƒä»…åŸºäºåƒç´ æµè¾“å…¥ï¼Œä¸éœ€è¦é’ˆå¯¹ç‰¹å®šæ¸¸æˆçš„å·¥ç¨‹è®¾è®¡ã€‚\n\n**9. [GeoSAM2] GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation**\n> **GeoSAM2ï¼šé‡Šæ”¾ SAM2 åœ¨ 3D éƒ¨ä»¶åˆ†å‰²ä¸­çš„åŠ›é‡**\n> *Ken Deng, et al.*\n\n*   **æ ¸å¿ƒçœ‹ç‚¹**ï¼šå°† SAM2ï¼ˆSegment Anything 2ï¼‰çš„èƒ½åŠ›æ— ç¼è¿ç§»åˆ° 3D é¢†åŸŸã€‚\n*   **æ–¹æ³•**ï¼šä¸éœ€è¦æ–‡æœ¬ Prompt æˆ–å¤æ‚çš„ 3D æ ‡æ³¨ã€‚é€šè¿‡å¤šè§†è§’çš„ 2D æ©ç é¢„æµ‹ï¼ˆç”± SAM2 å¤„ç†ï¼‰ï¼Œç„¶ååå‘æŠ•å½±èšåˆã€‚ç”¨æˆ·åªéœ€ç®€å•çš„ 2D ç‚¹å‡»æˆ–æ¡†é€‰ï¼Œå³å¯å®ç°å¯¹ 3D çº¹ç†ç¼ºå¤±ç‰©ä½“çš„ç²¾ç»†éƒ¨ä»¶åˆ†å‰²ã€‚\n\n**10. [æ—‹è½¬ç›²åŒº] RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation**\n> **RotBenchï¼šè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹è¯†åˆ«å›¾åƒæ—‹è½¬çš„èƒ½åŠ›**\n> *Tianyi Niu, et al.*\n\n*   **ä¸»è¦å‘ç°**ï¼šç›®å‰çš„é¡¶å°–æ¨¡å‹ï¼ˆGPT-4o, Gemini-1.5-Pro ç­‰ï¼‰åœ¨åˆ¤æ–­å›¾ç‰‡æ˜¯å¦æ—‹è½¬ï¼ˆ0Â°, 90Â°, 180Â°, 270Â°ï¼‰è¿™ä¸ªç®€å•ä»»åŠ¡ä¸Šè¡¨ç°**ä»¤äººæƒŠè®¶åœ°å·®**ã€‚ç‰¹åˆ«æ˜¯åŒºåˆ† 90Â° å’Œ 270Â° æ—‹è½¬å‡ ä¹æ˜¯ç›²çŒœã€‚è¿™æ­ç¤ºäº† MLLM åœ¨ç©ºé—´æ¨ç†ä¸Šçš„å·¨å¤§çŸ­æ¿ã€‚\n\n---\n\n### ğŸ¥ åŒ»ç–—ä¸ç§‘å­¦åº”ç”¨\n\n**11. [åˆæˆæ•°æ®åœ¨æµè¡Œç—…å­¦] Can synthetic data reproduce real-world findings in epidemiology?**\n> **åˆæˆæ•°æ®èƒ½é‡ç°æµè¡Œç—…å­¦çš„çœŸå®å‘ç°å—ï¼Ÿ**\n> *Jan Kapar, et al.*\n\n*   **ç»“è®º**ï¼š**èƒ½**ã€‚\n*   **æ–¹æ³•**ï¼šä½¿ç”¨å¯¹æŠ—éšæœºæ£®æ— (ARF) ç”Ÿæˆè¡¨æ ¼å‹æµè¡Œç—…å­¦æ•°æ®ã€‚\n*   **å‘ç°**ï¼šåœ¨å¤åˆ¶å…­é¡¹ä¸åŒçš„æµè¡Œç—…å­¦ç ”ç©¶ï¼ˆæ¶‰åŠè¡€å‹ã€ç³–å°¿ç—…ç­‰ï¼‰æ—¶ï¼Œåˆæˆæ•°æ®å¾—å‡ºçš„ç»Ÿè®¡ç»“è®ºä¸åŸå§‹çœŸå®æ•°æ®é«˜åº¦ä¸€è‡´ã€‚è¿™ä¸ºè§£å†³åŒ»ç–—æ•°æ®éšç§å…±äº«éš¾é¢˜æä¾›äº†å¼ºæœ‰åŠ›çš„è¯æ®ã€‚\n\n**12. [èˆŒè¯Š AI] TOM: An Open-Source Tongue Segmentation Method**\n> **TOMï¼šä¸€ç§å¼€æºçš„èˆŒè±¡åˆ†å‰²æ–¹æ³•**\n> *Jiacheng Xie, et al.*\n\n*   **åº”ç”¨**ï¼šä¸­åŒ» (TCM) æ™ºèƒ½åŒ–ã€‚\n*   **è´¡çŒ®**ï¼šå‘å¸ƒäº†é¦–ä¸ªå¼€æºçš„èˆŒè±¡åˆ†å‰²å·¥å…· **TOM**ã€‚åˆ©ç”¨å¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦å’Œæ‰©æ•£æ¨¡å‹å¢å¼ºæ•°æ®ï¼Œæ¨¡å‹å‚æ•°å‡å°‘ 96.6% çš„åŒæ—¶ä¿æŒäº† 95.22% çš„ mIoUã€‚æä¾›äº†åœ¨çº¿å’Œç¦»çº¿å·¥å…·ï¼Œæ–¹ä¾¿ä¸­åŒ»ç ”ç©¶è€…ä½¿ç”¨ã€‚\n\n---\n\n### ğŸ› ï¸ å…¶ä»–å€¼å¾—å…³æ³¨çš„æŠ€æœ¯ç‚¹\n\n*   **[LLM å‰ªæ] GLASS: Test-Time Acceleration for LLMs**: é’ˆå¯¹è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„åŠ¨æ€å‰ªææ–¹æ³•ï¼Œé€šè¿‡èåˆå…¨å±€å’Œå±€éƒ¨çš„ç¥ç»å…ƒé‡è¦æ€§ï¼Œåœ¨é•¿æ–‡æœ¬ç”Ÿæˆä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚\n*   **[LoRA å…ƒå­¦ä¹ ] Amortized Bayesian Meta-Learning for LoRA**: æå‡ºäº† ABMLLï¼Œå°†è´å¶æ–¯å…ƒå­¦ä¹ å¼•å…¥ LLM çš„ LoRA å¾®è°ƒï¼Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶æå‡äº†æ³›åŒ–èƒ½åŠ›å’Œä¸ç¡®å®šæ€§é‡åŒ–ã€‚\n*   **[ä»£ç ç”ŸæˆåŸºå‡†] COMPASS**: æŒ‡å‡ºå½“å‰ä»£ç ç”Ÿæˆåªçœ‹åŠŸèƒ½æ­£ç¡®æ€§æ˜¯ä¸å¤Ÿçš„ï¼Œæ–°åŸºå‡† COMPASS å¢åŠ äº†**ç®—æ³•æ•ˆç‡**å’Œ**ä»£ç è´¨é‡**çš„è¯„ä¼°ç»´åº¦ã€‚å‘ç°å¾ˆå¤šæ¨¡å‹è™½ç„¶ä»£ç èƒ½è·‘é€šï¼Œä½†å†™å¾—éå¸¸ä½æ•ˆæˆ–éš¾ä»¥ç»´æŠ¤ã€‚",
  "papers": [
    {
      "arxiv_id": "2508.14314v2",
      "title": "Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency",
      "title_zh": "åŸºäºç»†ç²’åº¦è·¨æ¨¡å‹ä¸€è‡´æ€§çš„é›¶çŸ¥è¯† LLM å¹»è§‰æ£€æµ‹ä¸ç¼“è§£",
      "authors": [
        "Aman Goel",
        "Daniel Schwartz",
        "Yanjun Qi"
      ],
      "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across diverse tasks, but they remain susceptible to hallucinations--generating content that appears plausible but contains factual inaccuracies. We present Finch-Zk, a black-box framework that leverages fine-grained cross-model consistency to detect and mitigate hallucinations in LLM outputs without requiring external knowledge sources. Finch-Zk introduces two key innovations: 1) a cross-model consistency checking strategy that reveals fine-grained inaccuracies by comparing responses generated by diverse models from semantically-equivalent prompts, and 2) a targeted mitigation technique that applies precise corrections to problematic segments while preserving accurate content. Experiments on the FELM dataset show Finch-Zk improves hallucination detection F1 scores by 6-39\\% compared to existing approaches. For mitigation, Finch-Zk achieves up to 9 absolute percentage points improvement in answer accuracy on the GPQA-diamond dataset when applied to state-of-the-art models like Llama 4 Maverick and Claude 4 Sonnet. Extensive evaluation on multiple datasets demonstrates that Finch-Zk provides a practical, deployment-ready safeguard for enhancing factual reliability in production LLM systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Finch-Zkï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€å¤–éƒ¨çŸ¥è¯†æºå³å¯æ£€æµ‹å’Œç¼“è§£å¤§è¯­è¨€æ¨¡å‹(LLMs)å¹»è§‰çš„é»‘ç›’æ¡†æ¶ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†ç»†ç²’åº¦çš„è·¨æ¨¡å‹ä¸€è‡´æ€§(cross-model consistency)æ£€æŸ¥ç­–ç•¥ï¼Œé€šè¿‡æ¯”è¾ƒå¤šä¸ªæ¨¡å‹å¯¹è¯­ä¹‰ç­‰ä»·æç¤ºè¯çš„å“åº”æ¥æ­ç¤ºç»†å¾®çš„äº‹å®é”™è¯¯ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨é’ˆå¯¹æ€§çš„ç¼“è§£æŠ€æœ¯å¯¹è¯†åˆ«å‡ºçš„é—®é¢˜ç‰‡æ®µè¿›è¡Œç²¾ç¡®æ ¡æ­£ï¼Œå¹¶æœ€å¤§é™åº¦åœ°ä¿ç•™åŸå§‹è¾“å‡ºä¸­çš„å‡†ç¡®å†…å®¹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFinch-Zkåœ¨FELMæ•°æ®é›†ä¸Šçš„å¹»è§‰æ£€æµ‹F1åˆ†æ•°è¾ƒç°æœ‰æ–¹æ³•æå‡äº†6-39%ï¼Œå¹¶åœ¨GPQA-diamondæ•°æ®é›†ä¸Šä½¿Llama 4 Maverickå’ŒClaude 4 Sonnetç­‰å…ˆè¿›æ¨¡å‹çš„å›ç­”å‡†ç¡®ç‡æœ€é«˜æå‡äº†9ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™ä¸€ç ”ç©¶ä¸ºæå‡ç”Ÿäº§ç¯å¢ƒä¸‹LLMç³»ç»Ÿçš„å™å®å¯é æ€§(factual reliability)æä¾›äº†ä¸€ç§æå…·å®ç”¨ä»·å€¼ä¸”æ˜“äºéƒ¨ç½²çš„ä¿éšœæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14314v2",
      "published_date": "2025-08-19 23:45:34 UTC",
      "updated_date": "2025-11-01 18:07:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:41:38.751906+00:00"
    },
    {
      "arxiv_id": "2508.14313v2",
      "title": "Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS",
      "title_zh": "å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°å³æœç´¢çš„æœ€ä½³ PRMï¼šå¼ºåŒ–å­¦ä¹ ä¸åŸºäºæœç´¢çš„æµ‹è¯•æ—¶æ‰©å±•çš„ç»Ÿä¸€",
      "authors": [
        "Can Jin",
        "Yang Zhou",
        "Qixin Zhang",
        "Hongwu Peng",
        "Di Zhang",
        "Marco Pavone",
        "Ligong Han",
        "Zhang-Wei Hong",
        "Tong Che",
        "Dimitris N. Metaxas"
      ],
      "abstract": "Test-time scaling (TTS) for large language models (LLMs) has thus far fallen into two largely separate paradigms: (1) reinforcement learning (RL) methods that optimize sparse outcome-based rewards, yet suffer from instability and low sample efficiency; and (2) search-based techniques guided by independently trained, static process reward models (PRMs), which require expensive human- or LLM-generated labels and often degrade under distribution shifts. In this paper, we introduce AIRL-S, the first natural unification of RL-based and search-based TTS. Central to AIRL-S is the insight that the reward function learned during RL training inherently represents the ideal PRM for guiding downstream search. Specifically, we leverage adversarial inverse reinforcement learning (AIRL) combined with group relative policy optimization (GRPO) to learn a dense, dynamic PRM directly from correct reasoning traces, entirely eliminating the need for labeled intermediate process data. At inference, the resulting PRM simultaneously serves as the critic for RL rollouts and as a heuristic to effectively guide search procedures, facilitating robust reasoning chain extension, mitigating reward hacking, and enhancing cross-task generalization. Experimental results across eight benchmarks, including mathematics, scientific reasoning, and code generation, demonstrate that our unified approach improves performance by 9 % on average over the base model, matching GPT-4o. Furthermore, when integrated into multiple search algorithms, our PRM consistently outperforms all baseline PRMs trained with labeled data. These results underscore that, indeed, your reward function for RL is your best PRM for search, providing a robust and cost-effective solution to complex reasoning tasks in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AIRL-Sï¼Œè¿™æ˜¯é¦–ä¸ªå°†åŸºäºå¼ºåŒ–å­¦ä¹ (RL)å’ŒåŸºäºæœç´¢çš„æµ‹è¯•æ—¶æ‰©å±•(Test-time scaling, TTS)è¿›è¡Œè‡ªç„¶ç»Ÿä¸€çš„æ¡†æ¶ã€‚é’ˆå¯¹RLæ–¹æ³•ç¨³å®šæ€§å·®ã€æ ·æœ¬æ•ˆç‡ä½ä»¥åŠæœç´¢æ–¹æ³•é«˜åº¦ä¾èµ–æ˜‚è´µæ ‡æ³¨æ•°æ®ç­‰æŒ‘æˆ˜ï¼ŒAIRL-Sæå‡ºRLè®­ç»ƒä¸­å­¦ä¹ åˆ°çš„å¥–åŠ±å‡½æ•°æœ¬è´¨ä¸Šå°±æ˜¯å¼•å¯¼æœç´¢çš„ç†æƒ³è¿‡ç¨‹å¥–åŠ±æ¨¡å‹(PRM)ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆå¯¹æŠ—é€†å¼ºåŒ–å­¦ä¹ (AIRL)ä¸ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(GRPO)ï¼Œç›´æ¥ä»æ­£ç¡®çš„æ¨ç†è½¨è¿¹ä¸­å­¦ä¹ ç¨ å¯†ä¸”åŠ¨æ€çš„PRMï¼Œä»è€Œå®Œå…¨æ¶ˆé™¤äº†å¯¹äººå·¥æˆ–æ¨¡å‹æ ‡æ³¨çš„ä¸­é—´è¿‡ç¨‹æ•°æ®çš„éœ€æ±‚ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œç”Ÿæˆçš„PRMæ—¢ä½œä¸ºRLçš„æ‰¹è¯„è€…(critic)ï¼Œä¹Ÿä½œä¸ºå¯å‘å¼å‡½æ•°å¼•å¯¼æœç´¢è¿‡ç¨‹ï¼Œæœ‰æ•ˆç¼“è§£äº†å¥–åŠ±ä½œå¼Š(reward hacking)ç°è±¡å¹¶å¢å¼ºäº†è·¨ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAIRL-Såœ¨æ•°å­¦ã€ç§‘å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰å…«é¡¹åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æ€§èƒ½æå‡9%ï¼Œè¾¾åˆ°äº†ä¸GPT-4oç›¸å½“çš„æ°´å¹³ã€‚æ­¤å¤–ï¼ŒAIRL-Såœ¨å¤šç§æœç´¢ç®—æ³•ä¸­çš„è¡¨ç°å§‹ç»ˆä¼˜äºä½¿ç”¨æ ‡æ³¨æ•°æ®è®­ç»ƒçš„åŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†è¯¥ç»Ÿä¸€æ–¹æ³•åœ¨å¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å¤æ‚æ¨ç†ä»»åŠ¡æ—¶çš„ç¨³å¥æ€§ä¸é«˜æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14313v2",
      "published_date": "2025-08-19 23:41:15 UTC",
      "updated_date": "2025-08-22 15:37:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:41:40.662678+00:00"
    },
    {
      "arxiv_id": "2508.16651v2",
      "title": "HiCL: Hippocampal-Inspired Continual Learning",
      "title_zh": "HiCLï¼šå—æµ·é©¬ä½“å¯å‘çš„æŒç»­å­¦ä¹ ",
      "authors": [
        "Kushal Kapoor",
        "Wyatt Mackey",
        "Yiannis Aloimonos",
        "Xiaomin Lin"
      ],
      "abstract": "We propose HiCL, a novel hippocampal-inspired dual-memory continual learning architecture designed to mitigate catastrophic forgetting by using elements inspired by the hippocampal circuitry. Our system encodes inputs through a grid-cell-like layer, followed by sparse pattern separation using a dentate gyrus-inspired module with top-k sparsity. Episodic memory traces are maintained in a CA3-like autoassociative memory. Task-specific processing is dynamically managed via a DG-gated mixture-of-experts mechanism, wherein inputs are routed to experts based on cosine similarity between their normalized sparse DG representations and learned task-specific DG prototypes computed through online exponential moving averages. This biologically grounded yet mathematically principled gating strategy enables differentiable, scalable task-routing without relying on a separate gating network, and enhances the model's adaptability and efficiency in learning multiple sequential tasks. Cortical outputs are consolidated using Elastic Weight Consolidation weighted by inter-task similarity. Crucially, we incorporate prioritized replay of stored patterns to reinforce essential past experiences. Evaluations on standard continual learning benchmarks demonstrate the effectiveness of our architecture in reducing task interference, achieving near state-of-the-art results in continual learning tasks at lower computational costs. Our code is available here https://github.com/kushalk173-sc/HiCL.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HiCLï¼Œä¸€ç§å—æµ·é©¬ä½“å¯å‘çš„æ–°å‹åŒè®°å¿†æŒç»­å­¦ä¹ (Continual Learning)æ¶æ„ï¼Œæ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿæµ·é©¬ä½“å›è·¯å‡ç¼“ç¾éš¾æ€§é—å¿˜(Catastrophic Forgetting)ã€‚è¯¥ç³»ç»Ÿé€šè¿‡Grid-cellå±‚è¿›è¡Œè¾“å…¥ç¼–ç ï¼Œå¹¶åˆ©ç”¨å—é½¿çŠ¶å›(Dentate Gyrus, DG)å¯å‘çš„æ¨¡å—è¿›è¡Œç¨€ç–æ¨¡å¼åˆ†ç¦»ã€‚æ¶æ„ä¸­é›†æˆäº†CA3-likeè‡ªç¼”åˆå­˜å‚¨å™¨æ¥ç»´æŒæƒ…å¢ƒè®°å¿†ï¼Œå¹¶é€šè¿‡ä¸€ç§å—DGé—¨æ§çš„æ··åˆä¸“å®¶(Mixture-of-Experts, MoE)æœºåˆ¶å®ç°åŠ¨æ€çš„ä»»åŠ¡ç®¡ç†ã€‚è¯¥æœºåˆ¶åŸºäºDGè¡¨å¾ä¸å­¦ä¹ åˆ°çš„ä»»åŠ¡åŸå‹ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦è¿›è¡Œè·¯ç”±ï¼Œæ— éœ€ç‹¬ç«‹é—¨æ§ç½‘ç»œå³å¯å®ç°é«˜æ•ˆçš„ä»»åŠ¡åˆ†å‘ã€‚æ­¤å¤–ï¼ŒHiCLç»“åˆäº†è·¨ä»»åŠ¡ç›¸ä¼¼åº¦åŠ æƒçš„å¼¹æ€§æƒé‡åˆå¹¶(Elastic Weight Consolidation, EWC)ä»¥åŠä¼˜å…ˆçº§å›æ”¾(Prioritized Replay)æŠ€æœ¯æ¥å·©å›ºçš®å±‚è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHiCLåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­æœ‰æ•ˆé™ä½äº†ä»»åŠ¡å¹²æ‰°ï¼Œå¹¶ä»¥è¾ƒä½çš„è®¡ç®—æˆæœ¬å®ç°äº†æ¥è¿‘å½“å‰æœ€å…ˆè¿›æ°´å¹³çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "In proceeding of AAAI",
      "pdf_url": "https://arxiv.org/pdf/2508.16651v2",
      "published_date": "2025-08-19 23:40:11 UTC",
      "updated_date": "2025-11-20 19:20:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:41:42.395023+00:00"
    },
    {
      "arxiv_id": "2508.14311v1",
      "title": "Learning Time-Varying Convexifications of Multiple Fairness Measures",
      "title_zh": "å­¦ä¹ å¤šç§å…¬å¹³æ€§æŒ‡æ ‡çš„æ—¶å˜å‡¸åŒ–",
      "authors": [
        "Quan Zhou",
        "Jakub Marecek",
        "Robert Shorten"
      ],
      "abstract": "There is an increasing appreciation that one may need to consider multiple measures of fairness, e.g., considering multiple group and individual fairness notions. The relative weights of the fairness regularisers are a priori unknown, may be time varying, and need to be learned on the fly. We consider the learning of time-varying convexifications of multiple fairness measures with limited graph-structured feedback.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šç§å…¬å¹³æ€§åº¦é‡ï¼ˆå¦‚ç¾¤ä½“å’Œä¸ªä½“å…¬å¹³ï¼‰åœ¨å®é™…åº”ç”¨ä¸­æƒé‡å…ˆéªŒæœªçŸ¥ä¸”éšæ—¶é—´å˜åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åœ¨çº¿å­¦ä¹ æœºåˆ¶ã€‚ç ”ç©¶é‡ç‚¹æ¢è®¨äº†åœ¨æœ‰é™çš„ graph-structured feedback çº¦æŸä¸‹ï¼Œå¦‚ä½•å®ç°å¤šä¸ªå…¬å¹³æ€§åº¦é‡çš„ time-varying convexifications çš„å­¦ä¹ è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•å…è®¸ç®—æ³•åœ¨è¿è¡Œè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´å…¬å¹³æ€§æ­£åˆ™åŒ–é¡¹çš„ç›¸å¯¹æƒé‡ï¼Œä»è€Œçµæ´»åº”å¯¹ä¸æ–­å˜åŒ–çš„ç¯å¢ƒéœ€æ±‚ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç ”ç©¶è§£å†³äº†åœ¨åŠ¨æ€åœºæ™¯ä¸­å¹³è¡¡å¤šç§å¤æ‚å…¬å¹³æ€§çº¦æŸçš„æŒ‘æˆ˜ã€‚è¿™ä¸€æˆæœä¸ºå¤„ç†å…·æœ‰å›¾ç»“æ„åé¦ˆé™åˆ¶çš„å®æ—¶å…¬å¹³æ€§ä¼˜åŒ–ä»»åŠ¡æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æŒä¸æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14311v1",
      "published_date": "2025-08-19 23:32:26 UTC",
      "updated_date": "2025-08-19 23:32:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:41:45.853294+00:00"
    },
    {
      "arxiv_id": "2508.14936v1",
      "title": "Can synthetic data reproduce real-world findings in epidemiology? A replication study using tree-based generative AI",
      "title_zh": "åˆæˆæ•°æ®èƒ½å¦å¤ç°æµè¡Œç—…å­¦ä¸­çš„çœŸå®ä¸–ç•Œç ”ç©¶å‘ç°ï¼Ÿä¸€é¡¹åŸºäºæ ‘ç±»ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å¤ç°ç ”ç©¶",
      "authors": [
        "Jan Kapar",
        "Kathrin GÃ¼nther",
        "Lori Ann Vallis",
        "Klaus Berger",
        "Nadine Binder",
        "Hermann Brenner",
        "Stefanie Castell",
        "Beate Fischer",
        "Volker Harth",
        "Bernd Holleczek",
        "Timm Intemann",
        "Till Ittermann",
        "AndrÃ© Karch",
        "Thomas Keil",
        "Lilian Krist",
        "Berit Lange",
        "Michael F. Leitzmann",
        "Katharina Nimptsch",
        "Nadia Obi",
        "Iris Pigeot",
        "Tobias Pischon",
        "Tamara Schikowski",
        "BÃ¶rge Schmidt",
        "Carsten Oliver Schmidt",
        "Anja M. Sedlmair",
        "Justine Tanoey",
        "Harm Wienbergen",
        "Andreas Wienke",
        "Claudia Wigmann",
        "Marvin N. Wright"
      ],
      "abstract": "Generative artificial intelligence for synthetic data generation holds substantial potential to address practical challenges in epidemiology. However, many current methods suffer from limited quality, high computational demands, and complexity for non-experts. Furthermore, common evaluation strategies for synthetic data often fail to directly reflect statistical utility. Against this background, a critical underexplored question is whether synthetic data can reliably reproduce key findings from epidemiological research. We propose the use of adversarial random forests (ARF) as an efficient and convenient method for synthesizing tabular epidemiological data. To evaluate its performance, we replicated statistical analyses from six epidemiological publications and compared original with synthetic results. These publications cover blood pressure, anthropometry, myocardial infarction, accelerometry, loneliness, and diabetes, based on data from the German National Cohort (NAKO Gesundheitsstudie), the Bremen STEMI Registry U45 Study, and the Guelph Family Health Study. Additionally, we assessed the impact of dimensionality and variable complexity on synthesis quality by limiting datasets to variables relevant for individual analyses, including necessary derivations. Across all replicated original studies, results from multiple synthetic data replications consistently aligned with original findings. Even for datasets with relatively low sample size-to-dimensionality ratios, the replication outcomes closely matched the original results across various descriptive and inferential analyses. Reducing dimensionality and pre-deriving variables further enhanced both quality and stability of the results.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆgenerative AIï¼‰ç”Ÿæˆçš„åˆæˆæ•°æ®åœ¨æµè¡Œç—…å­¦ï¼ˆepidemiologyï¼‰ä¸­å¤ç°çœŸå®ä¸–ç•Œç ”ç©¶ç»“æœçš„å¯é æ€§ã€‚ä½œè€…æå‡ºå°†å¯¹æŠ—éšæœºæ£®æ—ï¼ˆadversarial random forests, ARFï¼‰ä½œä¸ºä¸€ç§é«˜æ•ˆä¾¿æ·çš„æ–¹æ³•ï¼Œç”¨äºåˆæˆå¤æ‚çš„æµè¡Œç—…å­¦è¡¨æ ¼æ•°æ®ã€‚ä¸ºäº†éªŒè¯å…¶ç»Ÿè®¡æ•ˆç”¨ï¼ˆstatistical utilityï¼‰ï¼Œç ”ç©¶äººå‘˜å¤ç°äº†æ¶‰åŠè¡€å‹ã€å¿ƒè‚Œæ¢—æ­»å’Œç³–å°¿ç—…ç­‰é¢†åŸŸçš„å…­é¡¹æµè¡Œç—…å­¦ç ”ç©¶ï¼Œå¯¹æ¯”äº†åŸå§‹æ•°æ®ä¸åˆæˆæ•°æ®çš„åˆ†æç»“æœã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨æ‰€æœ‰å¤ç°æ¡ˆä¾‹ä¸­ï¼ŒåŸºäºåˆæˆæ•°æ®çš„åˆ†æç»“æœä¸åŸå§‹å‘ç°é«˜åº¦ä¸€è‡´ï¼Œå³ä½¿åœ¨æ ·æœ¬é‡è¾ƒå°æˆ–ç»´åº¦è¾ƒé«˜çš„æƒ…å†µä¸‹ä¹Ÿè¡¨ç°å‡ºæå¼ºçš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡é™ä½ç»´åº¦å’Œé¢„å…ˆå¤„ç†è¡ç”Ÿå˜é‡ï¼ˆpre-deriving variablesï¼‰èƒ½å¤Ÿè¿›ä¸€æ­¥æå‡åˆæˆæ•°æ®çš„è´¨é‡å’Œç¨³å®šæ€§ã€‚è¯¥ç ”ç©¶è¯æ˜äº† ARF åœ¨æµè¡Œç—…å­¦æ•°æ®åˆæˆä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œä¸ºè§£å†³æ•°æ®å…±äº«å’Œéšç§ä¿æŠ¤ç­‰æŒ‘æˆ˜æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG",
        "stat.AP",
        "stat.ML"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14936v1",
      "published_date": "2025-08-19 22:51:40 UTC",
      "updated_date": "2025-08-19 22:51:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:41:49.556678+00:00"
    },
    {
      "arxiv_id": "2508.14302v1",
      "title": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation",
      "title_zh": "GLASSï¼šåŸºäºå…¨å±€-å±€éƒ¨ç¥ç»å…ƒé‡è¦æ€§èšåˆçš„å¤§è¯­è¨€æ¨¡å‹æµ‹è¯•æ—¶åŠ é€Ÿ",
      "authors": [
        "Amirmohsen Sattarifard",
        "Sepehr Lavasani",
        "Ehsan Imani",
        "Kunlin Zhang",
        "Hanlin Xu",
        "Fengyu Sun",
        "Negar Hassanpour",
        "Chao Gao"
      ],
      "abstract": "Deploying Large Language Models (LLMs) on edge hardware demands aggressive, prompt-aware dynamic pruning to reduce computation without degrading quality. Static or predictor-based schemes either lock in a single sparsity pattern or incur extra runtime overhead, and recent zero-shot methods that rely on statistics from a single prompt fail on short prompt and/or long generation scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local neural importance Aggregation for feed-forward network SparSification, two training-free methods that dynamically select FFN units using a rank-aggregation of prompt local and model-intrinsic global neuron statistics. Empirical results across multiple LLMs and benchmarks demonstrate that GLASS significantly outperforms prior training-free methods, particularly in challenging long-form generation scenarios, without relying on auxiliary predictors or adding any inference overhead.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨è¾¹ç¼˜ç¡¬ä»¶éƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹(LLMs)æ—¶é¢ä¸´çš„è®¡ç®—å‹åŠ›ï¼Œæå‡ºäº†GLASSï¼ˆåŸºäºå…¨å±€-å±€éƒ¨ç¥ç»å…ƒé‡è¦æ€§èšåˆçš„æµ‹è¯•æ—¶åŠ é€Ÿæ–¹æ¡ˆï¼‰ï¼Œæ—¨åœ¨é€šè¿‡åŠ¨æ€å‰ªæå‡å°‘è®¡ç®—å¼€é”€è€Œä¸æŸå¤±æ¨¡å‹è´¨é‡ã€‚GLASSåŒ…å«A-GLASSå’ŒI-GLASSä¸¤ç§æ— éœ€è®­ç»ƒ(training-free)çš„æ–¹æ³•ï¼Œé€šè¿‡å¯¹æç¤ºè¯å±€éƒ¨(local)ä¿¡æ¯ä¸æ¨¡å‹å›ºæœ‰å…¨å±€(global)ç¥ç»å…ƒç»Ÿè®¡æ•°æ®è¿›è¡Œæ’åèšåˆ(rank-aggregation)ï¼Œå®ç°å¯¹å‰é¦ˆç½‘ç»œ(FFN)å•å…ƒçš„åŠ¨æ€é€‰æ‹©ã€‚ä¸ç°æœ‰çš„é™æ€æ–¹æ¡ˆæˆ–ä¾èµ–è¾…åŠ©é¢„æµ‹å™¨çš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•ä¸äº§ç”Ÿé¢å¤–çš„æ¨ç†å¼€é”€ï¼Œæœ‰æ•ˆè§£å†³äº†é›¶æ ·æœ¬æ–¹æ³•åœ¨çŸ­æç¤ºæˆ–é•¿ç”Ÿæˆä»»åŠ¡ä¸­çš„å¤±æ•ˆé—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGLASSåœ¨å¤šä¸ªæ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨é•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯ä¸‹æ˜¾è‘—ä¼˜äºä»¥å¾€çš„åŠ¨æ€å‰ªææŠ€æœ¯ï¼Œä¸ºæå‡LLMsåœ¨å—é™ç¡¬ä»¶ä¸Šçš„è¿è¡Œæ•ˆç‡æä¾›äº†é«˜æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14302v1",
      "published_date": "2025-08-19 22:50:20 UTC",
      "updated_date": "2025-08-19 22:50:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:42:06.786897+00:00"
    },
    {
      "arxiv_id": "2508.14295v1",
      "title": "Pixels to Play: A Foundation Model for 3D Gameplay",
      "title_zh": "Pixels to Playï¼šé¢å‘ 3D æ¸¸æˆçš„åŸºåº§æ¨¡å‹",
      "authors": [
        "Yuguang Yue",
        "Chris Green",
        "Samuel Hunt",
        "Irakli Salia",
        "Wenzhe Shi",
        "Jonathan J Hunt"
      ],
      "abstract": "We introduce Pixels2Play-0.1 (P2P0.1), a foundation model that learns to play a wide range of 3D video games with recognizable human-like behavior. Motivated by emerging consumer and developer use cases - AI teammates, controllable NPCs, personalized live-streamers, assistive testers - we argue that an agent must rely on the same pixel stream available to players and generalize to new titles with minimal game-specific engineering. P2P0.1 is trained end-to-end with behavior cloning: labeled demonstrations collected from instrumented human game-play are complemented by unlabeled public videos, to which we impute actions via an inverse-dynamics model. A decoder-only transformer with auto-regressive action output handles the large action space while remaining latency-friendly on a single consumer GPU. We report qualitative results showing competent play across simple Roblox and classic MS-DOS titles, ablations on unlabeled data, and outline the scaling and evaluation steps required to reach expert-level, text-conditioned control.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Pixels2Play-0.1 (P2P0.1)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä»¥ç±»äººè¡Œä¸ºå­¦ä¹ å¤šç§3Dè§†é¢‘æ¸¸æˆçš„Foundation Modelã€‚è¯¥æ¨¡å‹ä»…ä¾èµ–ç©å®¶å¯è§çš„Pixel Streamï¼Œèƒ½å¤Ÿæ³›åŒ–è‡³æ–°æ¸¸æˆè€Œæ— éœ€è¿‡å¤šçš„é’ˆå¯¹æ€§å·¥ç¨‹ï¼Œé€‚ç”¨äºAI Teammatesã€NPCåŠè‡ªåŠ¨åŒ–æµ‹è¯•ç­‰åº”ç”¨åœºæ™¯ã€‚P2P0.1é‡‡ç”¨ç«¯åˆ°ç«¯Behavior Cloningè¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡Inverse-Dynamics Modelä»æ— æ ‡ç­¾çš„å…¬å¼€è§†é¢‘ä¸­æ¨æ–­åŠ¨ä½œï¼Œå¹¶ç»“åˆæ ‡æ³¨çš„äººå·¥æ¼”ç¤ºæ•°æ®é›†è¿›è¡Œè”åˆè®­ç»ƒã€‚æ¨¡å‹æ¶æ„åŸºäºå…·æœ‰Auto-regressiveåŠ¨ä½œè¾“å‡ºçš„Decoder-only Transformerï¼Œåœ¨å¤„ç†å¤§è§„æ¨¡Action Spaceçš„åŒæ—¶ï¼Œèƒ½åœ¨å•ä¸ªæ¶ˆè´¹çº§GPUä¸Šä¿æŒè‰¯å¥½çš„å»¶è¿Ÿè¡¨ç°ã€‚å®éªŒè¯æ˜ï¼ŒP2P0.1åœ¨Robloxå’Œç»å…¸MS-DOSæ¸¸æˆä¸Šå±•ç°äº†å‡ºè‰²çš„ç«æŠ€èƒ½åŠ›ï¼Œç ”ç©¶è¿˜è¿›ä¸€æ­¥æ¢è®¨äº†å®ç°ä¸“å®¶çº§ã€Text-conditionedæ§åˆ¶æ‰€éœ€çš„æ‰©å±•ä¸è¯„ä¼°æ­¥éª¤ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14295v1",
      "published_date": "2025-08-19 22:24:50 UTC",
      "updated_date": "2025-08-19 22:24:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:42:06.953382+00:00"
    },
    {
      "arxiv_id": "2508.14294v1",
      "title": "Explaining Hitori Puzzles: Neurosymbolic Proof Staging for Sequential Decisions",
      "title_zh": "Hitori è°œé¢˜è§£æï¼šé¢å‘åºè´¯å†³ç­–çš„ç¥ç»ç¬¦å·è¯æ˜åˆ†é˜¶æ®µ",
      "authors": [
        "Maria Leonor Pacheco",
        "Fabio Somenzi",
        "Dananjay Srinivas",
        "Ashutosh Trivedi"
      ],
      "abstract": "We propose a neurosymbolic approach to the explanation of complex sequences of decisions that combines the strengths of decision procedures and Large Language Models (LLMs). We demonstrate this approach by producing explanations for the solutions of Hitori puzzles. The rules of Hitori include local constraints that are effectively explained by short resolution proofs. However, they also include a connectivity constraint that is more suitable for visual explanations. Hence, Hitori provides an excellent testing ground for a flexible combination of SAT solvers and LLMs. We have implemented a tool that assists humans in solving Hitori puzzles, and we present experimental evidence of its effectiveness.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç¥ç»ç¬¦å·(Neurosymbolic)æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆåˆ¤å®šç¨‹åº(Decision Procedures)ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„ä¼˜åŠ¿ï¼Œè§£é‡Šå¤æ‚çš„å†³ç­–åºåˆ—ã€‚ç ”ç©¶å›¢é˜Ÿä»¥ Hitori è°œé¢˜ä½œä¸ºå®éªŒåœºï¼Œåˆ©ç”¨çŸ­å½’ç»“è¯æ˜(Resolution Proofs)æœ‰æ•ˆè§£é‡Šè°œé¢˜ä¸­çš„å±€éƒ¨çº¦æŸï¼Œå¹¶é’ˆå¯¹è¿é€šæ€§çº¦æŸå¼•å…¥äº† SAT solvers ä¸ LLMs çš„çµæ´»ç»“åˆæ–¹æ¡ˆã€‚è¯¥æ–¹æ³•å……åˆ†å‘æŒ¥äº†ç¬¦å·é€»è¾‘åœ¨å¤„ç†ç¡¬çº¦æŸæ–¹é¢çš„ç²¾ç¡®æ€§ä»¥åŠå¤§è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè§£é‡Šæ–¹é¢çš„çµæ´»æ€§ã€‚ç ”ç©¶è¿˜å®ç°äº†ä¸€ä¸ªè¾…åŠ©äººç±»è§£å†³ Hitori è°œé¢˜çš„å·¥å…·ï¼Œå®éªŒè¯æ®è¡¨æ˜è¯¥ç³»ç»Ÿåœ¨è¾…åŠ©äººç±»ç†è§£å¤æ‚å†³ç­–é€»è¾‘æ–¹é¢å…·æœ‰æ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14294v1",
      "published_date": "2025-08-19 22:21:44 UTC",
      "updated_date": "2025-08-19 22:21:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:42:05.367945+00:00"
    },
    {
      "arxiv_id": "2508.14286v1",
      "title": "OccluNet: Spatio-Temporal Deep Learning for Occlusion Detection on DSA",
      "title_zh": "OccluNetï¼šç”¨äº DSA é—­å¡æ£€æµ‹çš„æ—¶ç©ºæ·±åº¦å­¦ä¹ ",
      "authors": [
        "Anushka A. Kore",
        "Frank G. te Nijenhuis",
        "Matthijs van der Sluijs",
        "Wim van Zwam",
        "Charles Majoie",
        "Geert Lycklama Ã  Nijeholt",
        "Danny Ruijters",
        "Frans Vos",
        "Sandra Cornelissen",
        "Ruisheng Su",
        "Theo van Walsum"
      ],
      "abstract": "Accurate detection of vascular occlusions during endovascular thrombectomy (EVT) is critical in acute ischemic stroke (AIS). Interpretation of digital subtraction angiography (DSA) sequences poses challenges due to anatomical complexity and time constraints. This work proposes OccluNet, a spatio-temporal deep learning model that integrates YOLOX, a single-stage object detector, with transformer-based temporal attention mechanisms to automate occlusion detection in DSA sequences. We compared OccluNet with a YOLOv11 baseline trained on either individual DSA frames or minimum intensity projections. Two spatio-temporal variants were explored for OccluNet: pure temporal attention and divided space-time attention. Evaluation on DSA images from the MR CLEAN Registry revealed the model's capability to capture temporally consistent features, achieving precision and recall of 89.02% and 74.87%, respectively. OccluNet significantly outperformed the baseline models, and both attention variants attained similar performance. Source code is available at https://github.com/anushka-kore/OccluNet.git",
      "tldr_zh": "é’ˆå¯¹æ€¥æ€§ç¼ºè¡€æ€§ä¸­é£(AIS)è¡€ç®¡å†…æ “å¡æ¸…é™¤æœ¯(EVT)ä¸­æ•°å­—å‡å½±è¡€ç®¡é€ å½±(DSA)åºåˆ—è§£é‡Šçš„å¤æ‚æ€§ï¼Œæœ¬ç ”ç©¶æå‡ºäº†OccluNetã€‚è¿™æ˜¯ä¸€ç§æ—¶ç©ºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡å°†å•é˜¶æ®µç›®æ ‡æ£€æµ‹å™¨YOLOXä¸åŸºäºTransformerçš„æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶ç›¸ç»“åˆï¼Œå®ç°äº†DSAåºåˆ—ä¸­è¡€ç®¡é—­å¡çš„è‡ªåŠ¨åŒ–æ£€æµ‹ã€‚ç ”ç©¶å›¢é˜Ÿæ¢ç´¢äº†çº¯æ—¶é—´æ³¨æ„åŠ›(Pure temporal attention)ä¸åˆ†å‰²æ—¶ç©ºæ³¨æ„åŠ›(Divided space-time attention)ä¸¤ç§æ¨¡å‹å˜ä½“ï¼Œå¹¶ä¸åŸºäºYOLOv11çš„åŸºå‡†æ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”ã€‚åœ¨MR CLEAN Registryæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒOccluNetèƒ½å¤Ÿæœ‰æ•ˆæ•è·å…·æœ‰æ—¶é—´ä¸€è‡´æ€§çš„ç‰¹å¾ï¼Œå®ç°äº†89.02%çš„ç²¾ç¡®ç‡(Precision)å’Œ74.87%çš„å¬å›ç‡(Recall)ã€‚è¯¥æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œä¸¤ç§æ³¨æ„åŠ›å˜ä½“è¡¨ç°ç›¸è¿‘ï¼Œä¸ºæé«˜æ‰‹æœ¯æœŸé—´é—­å¡æ£€æµ‹çš„å‡†ç¡®æ€§ä¸æ•ˆç‡æä¾›äº†æœ‰æ•ˆçš„è‡ªåŠ¨åŒ–å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "To be published in Proceedings of the SWITCH Workshop at MICCAI 2025, Lecture Notes in Computer Science (LNCS), Springer",
      "pdf_url": "https://arxiv.org/pdf/2508.14286v1",
      "published_date": "2025-08-19 21:59:59 UTC",
      "updated_date": "2025-08-19 21:59:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:42:06.355672+00:00"
    },
    {
      "arxiv_id": "2508.14285v2",
      "title": "Amortized Bayesian Meta-Learning for Low-Rank Adaptation of Large Language Models",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹ä½ç§©é€‚é…çš„æ‘Šé”€è´å¶æ–¯å…ƒå­¦ä¹ ",
      "authors": [
        "Liyi Zhang",
        "Jake Snell",
        "Thomas L. Griffiths"
      ],
      "abstract": "Fine-tuning large language models (LLMs) with low-rank adaptation (LoRA) is a cost-effective way to incorporate information from a specific dataset. However, it is often unclear how well the fine-tuned LLM will generalize, i.e., how well it will perform on unseen datasets. Methods have been proposed to improve generalization by optimizing in-context prompts, or by using meta-learning to fine-tune LLMs. However, these methods are expensive in memory and computation, requiring either long-context prompts or saving copies of parameters and using second-order gradient updates. To address these challenges, we propose Amortized Bayesian Meta-Learning for LoRA (ABMLL). This method builds on amortized Bayesian meta-learning for smaller models, adapting this approach to LLMs while maintaining its computational efficiency. We reframe task-specific and global parameters in the context of LoRA and use a new hyperparameter to balance reconstruction accuracy and the fidelity of task-specific parameters to the global ones. ABMLL provides effective generalization and scales to large models such as LLAMA3-8B. Furthermore, as a result of using a Bayesian framework, ABMLL provides improved uncertainty quantification. We test ABMLL on CrossFit and Unified-QA datasets and find that it outperforms existing methods on these benchmarks in terms of both accuracy and expected calibration error.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ä½¿ç”¨ä½ç§©è‡ªé€‚åº” (LoRA) è¿›è¡Œå¾®è°ƒæ—¶ï¼Œé¢ä¸´çš„æ³›åŒ–èƒ½åŠ›ä¸ç¡®å®šä»¥åŠç°æœ‰å…ƒå­¦ä¹ æ–¹æ³•è®¡ç®—å¼€é”€è¿‡å¤§çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† ABMLL (Amortized Bayesian Meta-Learning for LoRA) æ¡†æ¶ã€‚ABMLL å°†æ‘Šé”€è´å¶æ–¯å…ƒå­¦ä¹ æŠ€æœ¯å¼•å…¥ LLMsï¼Œé€šè¿‡åœ¨ LoRA è¯­å¢ƒä¸‹é‡æ–°å®šä¹‰ä»»åŠ¡ç‰¹å®šå‚æ•°ä¸å…¨å±€å‚æ•°ï¼Œå¹¶åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶å®ç°äº†é«˜æ•ˆçš„çŸ¥è¯†è¿ç§»ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§æ–°çš„è¶…å‚æ•°æ¥å¹³è¡¡é‡æ„å‡†ç¡®åº¦ä¸å‚æ•°å¿ å®åº¦ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ‰©å±•è‡³ LLAMA3-8B ç­‰å¤§æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨è´å¶æ–¯æ¡†æ¶æ˜¾è‘—æå‡äº†æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ– (Uncertainty Quantification) èƒ½åŠ›ã€‚åœ¨ CrossFit å’Œ Unified-QA æ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼ŒABMLL åœ¨å‡†ç¡®ç‡å’Œé¢„æœŸæ ¡å‡†è¯¯å·® (Expected Calibration Error) æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†æœªçŸ¥æ•°æ®é›†æ—¶çš„å“è¶Šæ³›åŒ–æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.14285v2",
      "published_date": "2025-08-19 21:57:59 UTC",
      "updated_date": "2025-12-09 02:12:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:42:06.590970+00:00"
    },
    {
      "arxiv_id": "2508.14276v1",
      "title": "Tooth-Diffusion: Guided 3D CBCT Synthesis with Fine-Grained Tooth Conditioning",
      "title_zh": "Tooth-Diffusionï¼šåŸºäºç»†ç²’åº¦ç‰™é½¿æ¡ä»¶çš„å¼•å¯¼å¼3D CBCTåˆæˆ",
      "authors": [
        "Said Djafar Said",
        "Torkan Gholamalizadeh",
        "Mostafa Mehdipour Ghazi"
      ],
      "abstract": "Despite the growing importance of dental CBCT scans for diagnosis and treatment planning, generating anatomically realistic scans with fine-grained control remains a challenge in medical image synthesis. In this work, we propose a novel conditional diffusion framework for 3D dental volume generation, guided by tooth-level binary attributes that allow precise control over tooth presence and configuration. Our approach integrates wavelet-based denoising diffusion, FiLM conditioning, and masked loss functions to focus learning on relevant anatomical structures. We evaluate the model across diverse tasks, such as tooth addition, removal, and full dentition synthesis, using both paired and distributional similarity metrics. Results show strong fidelity and generalization with low FID scores, robust inpainting performance, and SSIM values above 0.91 even on unseen scans. By enabling realistic, localized modification of dentition without rescanning, this work opens opportunities for surgical planning, patient communication, and targeted data augmentation in dental AI workflows. The codes are available at: https://github.com/djafar1/tooth-diffusion.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Tooth-Diffusionï¼Œä¸€ç§ç”¨äº3Dç‰™ç§‘CBCTæ‰«æåˆæˆçš„conditional diffusion frameworkï¼Œé€šè¿‡tooth-level binary attributeså®ç°äº†å¯¹ç‰™é½¿å­˜åœ¨åŠé…ç½®çš„ç²¾ç»†åŒ–æ§åˆ¶ã€‚è¯¥æ–¹æ³•é›†æˆäº†wavelet-based denoising diffusionã€FiLM conditioningå’Œmasked loss functionsï¼Œæ—¨åœ¨é€šè¿‡èšç„¦è§£å‰–ç»“æ„æå‡ç”Ÿæˆè´¨é‡ã€‚åœ¨ç‰™é½¿å¢åŠ ã€ç§»é™¤åŠå…¨ç‰™åˆ—åˆæˆç­‰å¤šæ ·åŒ–ä»»åŠ¡ä¸­ï¼Œè¯¥æ¨¡å‹è¡¨ç°å‡ºæé«˜çš„ä¿çœŸåº¦ï¼Œå³ä½¿åœ¨æœªè§è¿‡çš„æ‰«ææ•°æ®ä¸ŠSSIMå€¼ä¹Ÿè¶…è¿‡äº†0.91å¹¶å±•ç°äº†ç¨³å¥çš„inpaintingèƒ½åŠ›ã€‚é€šè¿‡åœ¨ä¸é‡å¤æ‰«æçš„æƒ…å†µä¸‹å®ç°é€¼çœŸçš„å±€éƒ¨ç‰™åˆ—ä¿®æ”¹ï¼Œè¯¥é¡¹å·¥ä½œä¸ºæ‰‹æœ¯è§„åˆ’ã€åŒ»æ‚£æ²Ÿé€šåŠç‰™ç§‘AIçš„æ•°æ®å¢å¼ºæä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "MICCAI 2025 Workshop on Oral and Dental Image Analysis (ODIN)",
      "pdf_url": "https://arxiv.org/pdf/2508.14276v1",
      "published_date": "2025-08-19 21:21:35 UTC",
      "updated_date": "2025-08-19 21:21:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:42:33.552280+00:00"
    },
    {
      "arxiv_id": "2508.14275v1",
      "title": "Disentangling concept semantics via multilingual averaging in Sparse Autoencoders",
      "title_zh": "ç¨€ç–è‡ªç¼–ç å™¨ä¸­åŸºäºå¤šè¯­è¨€å¹³å‡çš„æ¦‚å¿µè¯­ä¹‰è§£ç¼ ç»•",
      "authors": [
        "Cliff O'Reilly",
        "Ernesto Jimenez-Ruiz",
        "Tillman Weyde"
      ],
      "abstract": "Connecting LLMs with formal knowledge representation and reasoning is a promising approach to address their shortcomings. Embeddings and sparse autoencoders are widely used to represent textual content, but the semantics are entangled with syntactic and language-specific information. We propose a method that isolates concept semantics in Large Langue Models by averaging concept activations derived via Sparse Autoencoders. We create English text representations from OWL ontology classes, translate the English into French and Chinese and then pass these texts as prompts to the Gemma 2B LLM. Using the open source Gemma Scope suite of Sparse Autoencoders, we obtain concept activations for each class and language version. We average the different language activations to derive a conceptual average. We then correlate the conceptual averages with a ground truth mapping between ontology classes. Our results give a strong indication that the conceptual average aligns to the true relationship between classes when compared with a single language by itself. The result hints at a new technique which enables mechanistic interpretation of internal network states with higher accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­è¯­ä¹‰ä¸è¯­æ³•åŠè¯­è¨€ç‰¹å®šä¿¡æ¯ç›¸äº’çº ç¼ çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡å¤šè¯­è¨€å¹³å‡ç¨€ç–è‡ªç¼–ç å™¨(Sparse Autoencoders)æ¿€æ´»æ¥åˆ†ç¦»æ¦‚å¿µè¯­ä¹‰çš„æ–¹æ³•ã€‚ä½œè€…åˆ©ç”¨OWL ontologyç±»åˆ«ç”Ÿæˆè‹±æ–‡æ–‡æœ¬å¹¶å°†å…¶ç¿»è¯‘ä¸ºæ³•æ–‡å’Œä¸­æ–‡ï¼Œéšåè¾“å…¥Gemma 2Bæ¨¡å‹å¹¶åˆ©ç”¨Gemma Scopeå¥—ä»¶è·å–ä¸åŒè¯­è¨€ç‰ˆæœ¬ä¸‹çš„æ¦‚å¿µæ¿€æ´»å€¼ã€‚é€šè¿‡è®¡ç®—è¿™äº›æ¿€æ´»å€¼çš„å¹³å‡å€¼å¾—åˆ°â€œæ¦‚å¿µå¹³å‡å€¼â€(conceptual average)ï¼Œå¹¶å°†å…¶ä¸æœ¬ä½“ç±»åˆ«é—´çš„ground truthæ˜ å°„è¿›è¡Œç›¸å…³æ€§åˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å•ä¸€è¯­è¨€ç›¸æ¯”ï¼Œå¤šè¯­è¨€çš„æ¦‚å¿µå¹³å‡å€¼èƒ½æ›´å‡†ç¡®åœ°å¯¹é½ç±»ä¹‹é—´çš„çœŸå®å…³ç³»ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°æ›´é«˜ç²¾åº¦çš„æ¨¡å‹å†…éƒ¨ç½‘ç»œçŠ¶æ€çš„å¯è§£é‡Šæ€§åˆ†æ(mechanistic interpretation)æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æŠ€æœ¯æ‰‹æ®µã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14275v1",
      "published_date": "2025-08-19 21:18:56 UTC",
      "updated_date": "2025-08-19 21:18:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:42:23.552526+00:00"
    },
    {
      "arxiv_id": "2508.14266v1",
      "title": "Effect of Data Augmentation on Conformal Prediction for Diabetic Retinopathy",
      "title_zh": "æ•°æ®å¢å¼ºå¯¹ç³–å°¿ç—…è§†ç½‘è†œç—…å˜ç¬¦åˆæ€§é¢„æµ‹çš„å½±å“",
      "authors": [
        "Rizwan Ahamed",
        "Annahita Amireskandari",
        "Joel Palko",
        "Carol Laxson",
        "Binod Bhattarai",
        "Prashnna Gyawali"
      ],
      "abstract": "The clinical deployment of deep learning models for high-stakes tasks such as diabetic retinopathy (DR) grading requires demonstrable reliability. While models achieve high accuracy, their clinical utility is limited by a lack of robust uncertainty quantification. Conformal prediction (CP) offers a distribution-free framework to generate prediction sets with statistical guarantees of coverage. However, the interaction between standard training practices like data augmentation and the validity of these guarantees is not well understood. In this study, we systematically investigate how different data augmentation strategies affect the performance of conformal predictors for DR grading. Using the DDR dataset, we evaluate two backbone architectures -- ResNet-50 and a Co-Scale Conv-Attentional Transformer (CoaT) -- trained under five augmentation regimes: no augmentation, standard geometric transforms, CLAHE, Mixup, and CutMix. We analyze the downstream effects on conformal metrics, including empirical coverage, average prediction set size, and correct efficiency. Our results demonstrate that sample-mixing strategies like Mixup and CutMix not only improve predictive accuracy but also yield more reliable and efficient uncertainty estimates. Conversely, methods like CLAHE can negatively impact model certainty. These findings highlight the need to co-design augmentation strategies with downstream uncertainty quantification in mind to build genuinely trustworthy AI systems for medical imaging.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†æ•°æ®å¢å¼º(Data Augmentation)å¯¹ç³–å°¿ç—…è§†ç½‘è†œç—…å˜(Diabetic Retinopathy)åˆ†çº§ä»»åŠ¡ä¸­ç¬¦åˆé¢„æµ‹(Conformal Prediction)æ€§èƒ½çš„å½±å“ï¼Œæ—¨åœ¨æå‡æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»ç–—é«˜é£é™©ä»»åŠ¡ä¸­çš„å¯é æ€§ã€‚å®éªŒåŸºäºDDRæ•°æ®é›†ï¼Œå¯¹æ¯”äº†ResNet-50å’ŒCo-Scale Conv-Attentional Transformer (CoaT)åœ¨äº”ç§ä¸åŒå¢å¼ºç­–ç•¥ä¸‹çš„è¡¨ç°ï¼ŒåŒ…æ‹¬å‡ ä½•å˜æ¢ã€CLAHEã€Mixupå’ŒCutMixç­‰ã€‚ç ”ç©¶é€šè¿‡åˆ†æå®è¯è¦†ç›–ç‡(Empirical Coverage)å’Œå¹³å‡é¢„æµ‹é›†å¤§å°(Average Prediction Set Size)ç­‰å…³é”®æŒ‡æ ‡å‘ç°ï¼ŒMixupå’ŒCutMixç­‰æ ·æœ¬æ··åˆç­–ç•¥ä¸ä»…èƒ½æé«˜é¢„æµ‹å‡†ç¡®ç‡ï¼Œè¿˜èƒ½äº§ç”Ÿæ›´å¯é ä¸”é«˜æ•ˆçš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚ä¸ä¹‹ç›¸åï¼ŒCLAHEæ–¹æ³•åœ¨æŸäº›æƒ…å†µä¸‹ä¼šé™ä½æ¨¡å‹çš„ç¡®å®šæ€§ã€‚è¯¥å‘ç°å¼ºè°ƒäº†åœ¨åŒ»ç–—å½±åƒAIå¼€å‘ä¸­ï¼ŒååŒè®¾è®¡å¢å¼ºç­–ç•¥ä¸ä¸‹æ¸¸ä¸ç¡®å®šæ€§é‡åŒ–(Uncertainty Quantification)å¯¹äºæ„å»ºå¯ä¿¡ç³»ç»Ÿçš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "3rd Workshop in Data Engineering in Medical Imaging (DEMI), MICCAI-2025 Workshop",
      "pdf_url": "https://arxiv.org/pdf/2508.14266v1",
      "published_date": "2025-08-19 20:55:06 UTC",
      "updated_date": "2025-08-19 20:55:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:42:29.351763+00:00"
    },
    {
      "arxiv_id": "2508.15845v2",
      "title": "Coarse-to-Fine Personalized LLM Impressions for Streamlined Radiology Reports",
      "title_zh": "é¢å‘é«˜æ•ˆæ”¾å°„ç§‘æŠ¥å‘Šçš„ç”±ç²—åˆ°ç²¾ä¸ªæ€§åŒ– LLM å½±åƒè¯Šæ–­ç»“è®º",
      "authors": [
        "Chengbo Sun",
        "Hui Yi Leong",
        "Lei Li"
      ],
      "abstract": "The manual creation of the \"Impression\" section in radiology reports is a primary driver of radiologist burnout. To address this challenge, we propose a coarse-to-fine framework that leverages open-source large language models (LLMs) to automatically generate and personalize impressions from clinical findings. The system first produces a draft impression and then refines it using machine learning and reinforcement learning from human feedback (RLHF) to align with individual radiologists' styles while ensuring factual accuracy. We fine-tune LLaMA and Mistral models on a large dataset of reports from the University of Chicago Medicine. Our approach is designed to significantly reduce administrative workload and improve reporting efficiency while maintaining high standards of clinical precision.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ”¾å°„ç§‘æŠ¥å‘Šä¸­â€œç»“è®º(Impression)â€éƒ¨åˆ†æ‰‹åŠ¨ç¼–å†™å¯¼è‡´åŒ»ç”ŸèŒä¸šå€¦æ€ çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç”±ç²—åˆ°ç²¾(coarse-to-fine)çš„æ¡†æ¶ï¼Œåˆ©ç”¨å¼€æºå¤§è¯­è¨€æ¨¡å‹(LLMs)å®ç°è‡ªåŠ¨ç”Ÿæˆä¸ä¸ªæ€§åŒ–ã€‚è¯¥ç³»ç»Ÿé¦–å…ˆç”Ÿæˆç»“è®ºè‰ç¨¿ï¼Œéšåç»“åˆæœºå™¨å­¦ä¹ ä¸åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ (RLHF)æŠ€æœ¯è¿›è¡Œç²¾ç»†åŒ–è¿­ä»£ï¼Œæ—¨åœ¨å¯¹é½æ”¾å°„ç§‘åŒ»ç”Ÿçš„ä¸ªäººé£æ ¼å¹¶ç¡®ä¿ä¸´åºŠäº‹å®çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨èŠåŠ å“¥å¤§å­¦åŒ»å­¦ä¸­å¿ƒçš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šå¯¹ LLaMA å’Œ Mistral æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒ(fine-tune)ä¸éªŒè¯ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç»´æŒé«˜æ ‡å‡†ä¸´åºŠç²¾ç¡®åº¦çš„å‰æä¸‹ï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½è¡Œæ”¿å·¥ä½œé‡å¹¶æå‡æ”¾å°„æŠ¥å‘Šçš„äº§å‡ºæ•ˆç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15845v2",
      "published_date": "2025-08-19 20:54:40 UTC",
      "updated_date": "2025-09-27 16:19:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:42:32.091663+00:00"
    },
    {
      "arxiv_id": "2508.14933v1",
      "title": "Inference Time Debiasing Concepts in Diffusion Models",
      "title_zh": "æ‰©æ•£æ¨¡å‹æ¨ç†é˜¶æ®µçš„æ¦‚å¿µå»å",
      "authors": [
        "Lucas S. KupssinskÃ¼",
        "Marco N. Bochernitsan",
        "Jordan Kopper",
        "OtÃ¡vio Parraga",
        "Rodrigo C. Barros"
      ],
      "abstract": "We propose DeCoDi, a debiasing procedure for text-to-image diffusion-based models that changes the inference procedure, does not significantly change image quality, has negligible compute overhead, and can be applied in any diffusion-based image generation model. DeCoDi changes the diffusion process to avoid latent dimension regions of biased concepts. While most deep learning debiasing methods require complex or compute-intensive interventions, our method is designed to change only the inference procedure. Therefore, it is more accessible to a wide range of practitioners. We show the effectiveness of the method by debiasing for gender, ethnicity, and age for the concepts of nurse, firefighter, and CEO. Two distinct human evaluators manually inspect 1,200 generated images. Their evaluation results provide evidence that our method is effective in mitigating biases based on gender, ethnicity, and age. We also show that an automatic bias evaluation performed by the GPT4o is not significantly statistically distinct from a human evaluation. Our evaluation shows promising results, with reliable levels of agreement between evaluators and more coverage of protected attributes. Our method has the potential to significantly improve the diversity of images it generates by diffusion-based text-to-image generative models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DeCoDiï¼Œä¸€ç§é’ˆå¯¹æ–‡æœ¬ç”Ÿæˆå›¾åƒ(text-to-image)Diffusion Modelsçš„æ¨ç†é˜¶æ®µå»åæ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ”¹å˜æ¨ç†è¿‡ç¨‹æ¥å‡å°‘ç”Ÿæˆå›¾åƒä¸­çš„åè§ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºä¿®æ”¹æ‰©æ•£è¿‡ç¨‹ä»¥é¿å¼€åè§æ¦‚å¿µçš„Latent DimensionåŒºåŸŸï¼Œä¸”ä¸ä¼šæ˜¾è‘—å½±å“å›¾åƒè´¨é‡æˆ–äº§ç”Ÿé¢å¤–çš„è®¡ç®—å¼€é”€ã€‚ç›¸æ¯”äºå…¶ä»–éœ€è¦å¤æ‚å¹²é¢„æˆ–é«˜è®¡ç®—é‡çš„æ·±åº¦å­¦ä¹ å»åæŠ€æœ¯ï¼ŒDeCoDiä»…åœ¨æ¨ç†é˜¶æ®µè¿›è¡Œæ“ä½œï¼Œå› æ­¤å…·æœ‰æ›´é«˜çš„æ™®é€‚æ€§ï¼Œå¯åº”ç”¨äºä»»ä½•Diffusion-basedæ¨¡å‹ã€‚ç ”ç©¶äººå‘˜é’ˆå¯¹æŠ¤å£«ã€æ¶ˆé˜²å‘˜å’ŒCEOç­‰æ¦‚å¿µï¼Œåœ¨æ€§åˆ«ã€ç§æ—å’Œå¹´é¾„ç»´åº¦ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶é€šè¿‡1,200å¼ ç”Ÿæˆå›¾åƒçš„äººå·¥è¯„ä¼°éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒDeCoDièƒ½æ˜¾è‘—æå‡ç”Ÿæˆå›¾åƒçš„å¤šæ ·æ€§ï¼Œä¸”GPT-4oçš„è‡ªåŠ¨åè§è¯„ä¼°ç»“æœä¸äººå·¥è¯„ä¼°å…·æœ‰é«˜åº¦çš„ä¸€è‡´æ€§ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14933v1",
      "published_date": "2025-08-19 20:21:02 UTC",
      "updated_date": "2025-08-19 20:21:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:42:32.449568+00:00"
    },
    {
      "arxiv_id": "2508.14231v1",
      "title": "Incident Analysis for AI Agents",
      "title_zh": "é¢å‘ AI æ™ºèƒ½ä½“çš„äº‹ä»¶åˆ†æ",
      "authors": [
        "Carson Ezell",
        "Xavier Roberts-Gaal",
        "Alan Chan"
      ],
      "abstract": "As AI agents become more widely deployed, we are likely to see an increasing number of incidents: events involving AI agent use that directly or indirectly cause harm. For example, agents could be prompt-injected to exfiltrate private information or make unauthorized purchases. Structured information about such incidents (e.g., user prompts) can help us understand their causes and prevent future occurrences. However, existing incident reporting processes are not sufficient for understanding agent incidents. In particular, such processes are largely based on publicly available data, which excludes useful, but potentially sensitive, information such as an agent's chain of thought or browser history. To inform the development of new, emerging incident reporting processes, we propose an incident analysis framework for agents. Drawing on systems safety approaches, our framework proposes three types of factors that can cause incidents: system-related (e.g., CBRN training data), contextual (e.g., prompt injections), and cognitive (e.g., misunderstanding a user request). We also identify specific information that could help clarify which factors are relevant to a given incident: activity logs, system documentation and access, and information about the tools an agent uses. We provide recommendations for 1) what information incident reports should include and 2) what information developers and deployers should retain and make available to incident investigators upon request. As we transition to a world with more agents, understanding agent incidents will become increasingly crucial for managing risks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹AIæ™ºèƒ½ä½“ï¼ˆAI Agentsï¼‰å¹¿æ³›éƒ¨ç½²ä¸­æ—¥ç›Šå¢å¤šçš„äº‹æ•…é£é™©ï¼Œæå‡ºäº†ä¸€ä¸ªä¸“é—¨çš„äº‹æ•…åˆ†ææ¡†æ¶ï¼Œä»¥å¼¥è¡¥ç°æœ‰æŠ¥å‘Šæœºåˆ¶åœ¨å¤„ç†æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚Chain-of-Thoughtæˆ–æµè§ˆå™¨å†å²ï¼‰æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ¡†æ¶å€Ÿé‰´ç³»ç»Ÿå®‰å…¨ï¼ˆSystems Safetyï¼‰æ–¹æ³•ï¼Œå°†äº‹æ•…è¯±å› å½’çº³ä¸ºç³»ç»Ÿç›¸å…³ï¼ˆSystem-relatedï¼‰ã€ä¸Šä¸‹æ–‡ç›¸å…³ï¼ˆContextualï¼‰å’Œè®¤çŸ¥ç›¸å…³ï¼ˆCognitiveï¼‰ä¸‰å¤§ç»´åº¦ã€‚ç ”ç©¶æ˜ç¡®äº†æ´»åŠ¨æ—¥å¿—ã€ç³»ç»Ÿæ–‡æ¡£ä»¥åŠæ™ºèƒ½ä½“å·¥å…·ä½¿ç”¨ä¿¡æ¯åœ¨å˜æ¸…äº‹æ•…è´£ä»»ä¸­çš„å…³é”®ä½œç”¨ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜å°±äº‹æ•…æŠ¥å‘Šåº”æ¶µç›–çš„è¦ç´ ä»¥åŠå¼€å‘è€…ä¸éƒ¨ç½²è€…åº”å½“ä¿ç•™å¹¶æä¾›çš„æ•°æ®æå‡ºäº†å…·ä½“å»ºè®®ã€‚è¿™ä¸€æ¡†æ¶ä¸ºåœ¨AIæ™ºèƒ½ä½“æ™®åŠçš„èƒŒæ™¯ä¸‹è¿›è¡Œæœ‰æ•ˆçš„é£é™©ç®¡ç†ã€äº‹æ•…å½’å› åŠæœªæ¥é¢„é˜²å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "16 pages (10 pages main text), 4 figures, 3 tables. To be published in the Proceedings of the 2025 AAAI/ACM Conference on AI, Ethics, & Society (AIES)",
      "pdf_url": "https://arxiv.org/pdf/2508.14231v1",
      "published_date": "2025-08-19 19:39:37 UTC",
      "updated_date": "2025-08-19 19:39:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:42:40.800102+00:00"
    },
    {
      "arxiv_id": "2508.14229v1",
      "title": "New Insights into Automatic Treatment Planning for Cancer Radiotherapy Using Explainable Artificial Intelligence",
      "title_zh": "åŸºäºå¯è§£é‡Šäººå·¥æ™ºèƒ½çš„ç™Œç—‡æ”¾å°„æ²»ç–—è‡ªåŠ¨æ²»ç–—è®¡åˆ’æ–°æ¢",
      "authors": [
        "Md Mainul Abrar",
        "Xun Jia",
        "Yujie Chi"
      ],
      "abstract": "Objective: This study aims to uncover the opaque decision-making process of an artificial intelligence (AI) agent for automatic treatment planning.\n  Approach: We examined a previously developed AI agent based on the Actor-Critic with Experience Replay (ACER) network, which automatically tunes treatment planning parameters (TPPs) for inverse planning in prostate cancer intensity modulated radiotherapy. We selected multiple checkpoint ACER agents from different stages of training and applied an explainable AI (EXAI) method to analyze the attribution from dose-volume histogram (DVH) inputs to TPP-tuning decisions. We then assessed each agent's planning efficacy and efficiency and evaluated their policy and final TPP tuning spaces. Combining these analyses, we systematically examined how ACER agents generated high-quality treatment plans in response to different DVH inputs.\n  Results: Attribution analysis revealed that ACER agents progressively learned to identify dose-violation regions from DVH inputs and promote appropriate TPP-tuning actions to mitigate them. Organ-wise similarities between DVH attributions and dose-violation reductions ranged from 0.25 to 0.5 across tested agents. Agents with stronger attribution-violation similarity required fewer tuning steps (~12-13 vs. 22), exhibited a more concentrated TPP-tuning space with lower entropy (~0.3 vs. 0.6), converged on adjusting only a few TPPs, and showed smaller discrepancies between practical and theoretical tuning steps. Putting together, these findings indicate that high-performing ACER agents can effectively identify dose violations from DVH inputs and employ a global tuning strategy to achieve high-quality treatment planning, much like skilled human planners.\n  Significance: Better interpretability of the agent's decision-making process may enhance clinician trust and inspire new strategies for automatic treatment planning.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨å¯è§£é‡Šäººå·¥æ™ºèƒ½(Explainable AI, EXAI)æŠ€æœ¯ï¼Œæ·±å…¥æ¢è®¨äº†ç”¨äºç™Œç—‡æ”¾å°„æ²»ç–—è‡ªåŠ¨è®¡åˆ’è®¾è®¡çš„äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“å†³ç­–è¿‡ç¨‹ã€‚ç ”ç©¶å›¢é˜Ÿåˆ†æäº†ä¸€ä¸ªåŸºäºç»éªŒå›æ”¾çš„æ¼”å‘˜-è¯„è®ºå®¶ç½‘ç»œ(Actor-Critic with Experience Replay, ACER)çš„æ™ºèƒ½ä½“ï¼Œè¯¥æ™ºèƒ½ä½“é€šè¿‡è‡ªåŠ¨è°ƒèŠ‚æ²»ç–—è®¡åˆ’å‚æ•°(Treatment Planning Parameters, TPPs)æ¥ä¼˜åŒ–å‰åˆ—è…ºç™Œçš„è°ƒå¼ºæ”¾å°„æ²»ç–—ã€‚é€šè¿‡è€ƒå¯Ÿå‰‚é‡ä½“ç§¯ç›´æ–¹å›¾(Dose-Volume Histogram, DVH)è¾“å…¥å¯¹å‚æ•°è°ƒèŠ‚å†³ç­–çš„å½’å› å…³ç³»ï¼Œç ”ç©¶ç³»ç»Ÿæ­ç¤ºäº†æ™ºèƒ½ä½“å¦‚ä½•ç”Ÿæˆé«˜è´¨é‡çš„æ²»ç–—è®¡åˆ’ã€‚å½’å› åˆ†ææ˜¾ç¤ºï¼Œé«˜è¡¨ç°çš„ACERæ™ºèƒ½ä½“èƒ½é€æ­¥å­¦ä¼šä»DVHè¾“å…¥ä¸­è¯†åˆ«å‰‚é‡è¿è§„åŒºåŸŸï¼Œå¹¶é‡‡å–é’ˆå¯¹æ€§çš„è°ƒèŠ‚è¡ŒåŠ¨ï¼Œå…¶å†³ç­–é€»è¾‘è¡¨ç°å‡ºä¸äººç±»è§„åˆ’å¸ˆç±»ä¼¼çš„å…¨å±€è°ƒèŠ‚ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼Œå½’å› å‡†ç¡®æ€§æ›´é«˜çš„æ™ºèƒ½ä½“åœ¨è°ƒèŠ‚æ•ˆç‡å’Œç­–ç•¥é›†ä¸­åº¦ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œæ˜¾è‘—å‡å°‘äº†å†³ç­–æ­¥éª¤å¹¶é™ä½äº†è°ƒèŠ‚ç©ºé—´çš„ç†µå€¼ã€‚è¿™é¡¹å·¥ä½œæ˜¾è‘—æå‡äº†æ”¾å°„æ²»ç–—è‡ªåŠ¨è®¡åˆ’è®¾è®¡å†³ç­–è¿‡ç¨‹çš„å¯è§£é‡Šæ€§ï¼Œä¸ºå¢å¼ºä¸´åºŠåŒ»ç”Ÿå¯¹AIç³»ç»Ÿçš„ä¿¡ä»»ä»¥åŠå¼€å‘æ–°å‹è‡ªåŠ¨åŒ–è§„åˆ’ç­–ç•¥å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "physics.med-ph",
        "cs.AI"
      ],
      "primary_category": "physics.med-ph",
      "comment": "19 pages, 7 figures, 1 table, Oral presentation at the conference 'American Association of Physicists in Medicine 2025, 67th Annual Meeting and Exhibition'",
      "pdf_url": "https://arxiv.org/pdf/2508.14229v1",
      "published_date": "2025-08-19 19:38:16 UTC",
      "updated_date": "2025-08-19 19:38:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:42:44.549157+00:00"
    },
    {
      "arxiv_id": "2508.14214v1",
      "title": "Large Language Models are Highly Aligned with Human Ratings of Emotional Stimuli",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸äººç±»å¯¹æƒ…æ„Ÿåˆºæ¿€çš„è¯„åˆ†é«˜åº¦å¯¹é½",
      "authors": [
        "Mattson Ogg",
        "Chace Ashcraft",
        "Ritwik Bose",
        "Raphael Norman-Tenazas",
        "Michael Wolmetz"
      ],
      "abstract": "Emotions exert an immense influence over human behavior and cognition in both commonplace and high-stress tasks. Discussions of whether or how to integrate large language models (LLMs) into everyday life (e.g., acting as proxies for, or interacting with, human agents), should be informed by an understanding of how these tools evaluate emotionally loaded stimuli or situations. A model's alignment with human behavior in these cases can inform the effectiveness of LLMs for certain roles or interactions. To help build this understanding, we elicited ratings from multiple popular LLMs for datasets of words and images that were previously rated for their emotional content by humans. We found that when performing the same rating tasks, GPT-4o responded very similarly to human participants across modalities, stimuli and most rating scales (r = 0.9 or higher in many cases). However, arousal ratings were less well aligned between human and LLM raters, while happiness ratings were most highly aligned. Overall LLMs aligned better within a five-category (happiness, anger, sadness, fear, disgust) emotion framework than within a two-dimensional (arousal and valence) organization. Finally, LLM ratings were substantially more homogenous than human ratings. Together these results begin to describe how LLM agents interpret emotional stimuli and highlight similarities and differences among biological and artificial intelligence in key behavioral domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨è¯„ä¼°æƒ…ç»ªåŒ–åˆºæ¿€æ—¶ä¸äººç±»è¯„åˆ†çš„ä¸€è‡´æ€§ï¼Œæ—¨åœ¨ç†è§£äººå·¥æ™ºèƒ½å¦‚ä½•å¤„ç†å¸¦æœ‰æƒ…æ„Ÿè‰²å½©çš„ä¿¡æ¯ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨å¤šä¸ªä¸»æµ LLMs å¯¹å…ˆå‰ç”±äººç±»æ ‡æ³¨çš„è¯æ±‡å’Œå›¾åƒæ•°æ®é›†è¿›è¡Œè¯„åˆ†ï¼Œå¹¶å¯¹æ¯”äº†ä¸¤è€…åœ¨ä¸åŒæƒ…ç»ªæ¡†æ¶ä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-4o åœ¨å¤šç§æ¨¡æ€å’Œåˆºæ¿€ç±»å‹ä¸Šä¸äººç±»ååº”é«˜åº¦ä¸€è‡´ï¼Œç›¸å…³ç³»æ•°åœ¨å¤šæ•°æƒ…å†µä¸‹è¾¾åˆ° 0.9 æˆ–æ›´é«˜ã€‚åœ¨å…·ä½“ç»´åº¦ä¸­ï¼Œhappiness è¯„åˆ†çš„ä¸€è‡´æ€§æœ€é«˜ï¼Œè€Œ arousal è¯„åˆ†çš„ä¸€è‡´æ€§ç›¸å¯¹è¾ƒä½ï¼›åŒæ—¶ï¼ŒLLMs åœ¨äº”ç±»æƒ…ç»ªæ¡†æ¶(happiness, anger, sadness, fear, disgust)ä¸‹çš„å¯¹é½æ•ˆæœä¼˜äºäºŒç»´(arousal å’Œ valence)æ¡†æ¶ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç° LLM agents çš„è¯„åˆ†è¡¨ç°å‡ºæ¯”äººç±»æ›´é«˜çš„åŒè´¨æ€§ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†ç”Ÿç‰©æ™ºèƒ½ä¸äººå·¥æ™ºèƒ½åœ¨æƒ…ç»ªè§£è¯»æ–¹é¢çš„ç›¸ä¼¼æ€§ä¸å·®å¼‚ï¼Œä¸ºæœªæ¥ LLMs åœ¨äººç±»äº¤äº’è§’è‰²ä¸­çš„åº”ç”¨æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14214v1",
      "published_date": "2025-08-19 19:22:00 UTC",
      "updated_date": "2025-08-19 19:22:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:42:43.345270+00:00"
    },
    {
      "arxiv_id": "2508.14932v1",
      "title": "TOM: An Open-Source Tongue Segmentation Method with Multi-Teacher Distillation and Task-Specific Data Augmentation",
      "title_zh": "TOMï¼šèåˆå¤šæ•™å¸ˆè’¸é¦ä¸ä»»åŠ¡ç‰¹å®šæ•°æ®å¢å¼ºçš„å¼€æºèˆŒè±¡åˆ†å‰²æ–¹æ³•",
      "authors": [
        "Jiacheng Xie",
        "Ziyang Zhang",
        "Biplab Poudel",
        "Congyu Guo",
        "Yang Yu",
        "Guanghui An",
        "Xiaoting Tang",
        "Lening Zhao",
        "Chunhui Xu",
        "Dong Xu"
      ],
      "abstract": "Tongue imaging serves as a valuable diagnostic tool, particularly in Traditional Chinese Medicine (TCM). The quality of tongue surface segmentation significantly affects the accuracy of tongue image classification and subsequent diagnosis in intelligent tongue diagnosis systems. However, existing research on tongue image segmentation faces notable limitations, and there is a lack of robust and user-friendly segmentation tools. This paper proposes a tongue image segmentation model (TOM) based on multi-teacher knowledge distillation. By incorporating a novel diffusion-based data augmentation method, we enhanced the generalization ability of the segmentation model while reducing its parameter size. Notably, after reducing the parameter count by 96.6% compared to the teacher models, the student model still achieves an impressive segmentation performance of 95.22% mIoU. Furthermore, we packaged and deployed the trained model as both an online and offline segmentation tool (available at https://itongue.cn/), allowing TCM practitioners and researchers to use it without any programming experience. We also present a case study on TCM constitution classification using segmented tongue patches. Experimental results demonstrate that training with tongue patches yields higher classification performance and better interpretability than original tongue images. To our knowledge, this is the first open-source and freely available tongue image segmentation tool.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TOMï¼Œä¸€ç§å¼€æºçš„èˆŒè±¡åˆ†å‰²æ–¹æ³•ï¼Œæ—¨åœ¨æå‡ä¸­åŒ»ï¼ˆTCMï¼‰æ™ºèƒ½è¯Šæ–­ä¸­èˆŒé¢åˆ†å‰²çš„è´¨é‡ã€‚ä¸ºäº†è§£å†³ç°æœ‰å·¥å…·ç¼ºä¹é²æ£’æ€§ä¸”å¯¹éä¸“ä¸šäººå£«ä¸å‹å¥½çš„é—®é¢˜ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨äº†å¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦ï¼ˆMulti-Teacher Knowledge Distillationï¼‰æ¶æ„ï¼Œå¹¶ç»“åˆäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„æ•°æ®å¢å¼ºï¼ˆDiffusion-based Data Augmentationï¼‰æ‰‹æ®µã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥å­¦ç”Ÿæ¨¡å‹åœ¨å‚æ•°é‡è¾ƒæ•™å¸ˆæ¨¡å‹å‡å°‘96.6%çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶è¾¾åˆ°äº†95.22%çš„mIoUã€‚ç ”ç©¶å›¢é˜Ÿå°†æ¨¡å‹éƒ¨ç½²ä¸ºæ˜“äºä½¿ç”¨çš„åœ¨çº¿åŠç¦»çº¿å·¥å…·ï¼Œæ–¹ä¾¿éç¼–ç¨‹èƒŒæ™¯çš„ä»ä¸šè€…è¿›è¡Œç§‘ç ”ã€‚ä¸­åŒ»ä½“è´¨åˆ†ç±»æ¡ˆä¾‹è¡¨æ˜ï¼Œä½¿ç”¨åˆ†å‰²åçš„èˆŒè±¡åˆ‡ç‰‡æ¯”åŸå§‹å›¾åƒå…·æœ‰æ›´é«˜çš„åˆ†ç±»ç²¾åº¦å’Œæ›´å¼ºçš„å¯è§£é‡Šæ€§ã€‚ä½œä¸ºé¦–ä¸ªå¼€æºä¸”å…è´¹çš„èˆŒè±¡åˆ†å‰²å·¥å…·ï¼ŒTOMä¸ºä¸­åŒ»å®¢è§‚åŒ–è¯Šæ–­æä¾›äº†é«˜æ•ˆã€è½»é‡åŒ–çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "eess.IV",
      "comment": "Tongue segmentation, data augmentation, synthetic data for AI training, prompt engineering, Segment Anything Model, knowledge distillation, tongue classification",
      "pdf_url": "https://arxiv.org/pdf/2508.14932v1",
      "published_date": "2025-08-19 19:21:47 UTC",
      "updated_date": "2025-08-19 19:21:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:42:53.136578+00:00"
    },
    {
      "arxiv_id": "2508.14203v1",
      "title": "A Survey on Video Anomaly Detection via Deep Learning: Human, Vehicle, and Environment",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„è§†é¢‘å¼‚å¸¸æ£€æµ‹ç»¼è¿°ï¼šäººã€è½¦ä¸ç¯å¢ƒ",
      "authors": [
        "Ghazal Alinezhad Noghre",
        "Armin Danesh Pazho",
        "Hamed Tabkhi"
      ],
      "abstract": "Video Anomaly Detection (VAD) has emerged as a pivotal task in computer vision, with broad relevance across multiple fields. Recent advances in deep learning have driven significant progress in this area, yet the field remains fragmented across domains and learning paradigms. This survey offers a comprehensive perspective on VAD, systematically organizing the literature across various supervision levels, as well as adaptive learning methods such as online, active, and continual learning. We examine the state of VAD across three major application categories: human-centric, vehicle-centric, and environment-centric scenarios, each with distinct challenges and design considerations. In doing so, we identify fundamental contributions and limitations of current methodologies. By consolidating insights from subfields, we aim to provide the community with a structured foundation for advancing both theoretical understanding and real-world applicability of VAD systems. This survey aims to support researchers by providing a useful reference, while also drawing attention to the broader set of open challenges in anomaly detection, including both fundamental research questions and practical obstacles to real-world deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹åŸºäºæ·±åº¦å­¦ä¹ çš„è§†é¢‘å¼‚å¸¸æ£€æµ‹(Video Anomaly Detection, VAD)è¿›è¡Œäº†å…¨é¢ç»¼è¿°ï¼Œç³»ç»Ÿåœ°æ•´ç†äº†æ¶µç›–å„ç§ç›‘ç£çº§åˆ«ä»¥åŠåœ¨çº¿å­¦ä¹ (online learning)ã€ä¸»åŠ¨å­¦ä¹ (active learning)å’ŒæŒç»­å­¦ä¹ (continual learning)ç­‰è‡ªé€‚åº”å­¦ä¹ æ–¹æ³•çš„æ–‡çŒ®ã€‚ç»¼è¿°é‡ç‚¹è€ƒå¯Ÿäº†ä»¥äººä¸ºä¸­å¿ƒ(human-centric)ã€ä»¥è½¦è¾†ä¸ºä¸­å¿ƒ(vehicle-centric)å’Œä»¥ç¯å¢ƒä¸ºä¸­å¿ƒ(environment-centric)ä¸‰å¤§åº”ç”¨åœºæ™¯ï¼Œå¹¶åˆ†æäº†å„åœºæ™¯ç‹¬ç‰¹çš„æŒ‘æˆ˜ä¸è®¾è®¡è€ƒé‡ã€‚é€šè¿‡è¯†åˆ«ç°æœ‰æ–¹æ³•çš„æ ¹æœ¬è´¡çŒ®ä¸å±€é™æ€§ï¼Œè¯¥ç ”ç©¶ä¸ºæå‡VADç³»ç»Ÿçš„ç†è®ºç†è§£å’Œå®é™…åº”ç”¨æä¾›äº†ç»“æ„åŒ–åŸºç¡€ã€‚æ–‡ç« æœ€åè¿˜æ¢è®¨äº†è¯¥é¢†åŸŸå¹¿æ³›çš„å¼€æ”¾æ€§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åŸºç¡€ç ”ç©¶é—®é¢˜ä»¥åŠçœŸå®ä¸–ç•Œéƒ¨ç½²ä¸­çš„å®é™…éšœç¢ï¼Œä¸ºåç»­ç ”ç©¶äººå‘˜æä¾›äº†é‡è¦çš„å‚è€ƒæ¡†æ¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14203v1",
      "published_date": "2025-08-19 18:50:49 UTC",
      "updated_date": "2025-08-19 18:50:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:42:53.850583+00:00"
    },
    {
      "arxiv_id": "2509.20364v1",
      "title": "An Approach to Checking Correctness for Agentic Systems",
      "title_zh": "ä¸€ç§éªŒè¯æ™ºèƒ½ä½“ç³»ç»Ÿæ­£ç¡®æ€§çš„æ–¹æ³•",
      "authors": [
        "Thomas J Sheffler"
      ],
      "abstract": "This paper presents a temporal expression language for monitoring AI agent behavior, enabling systematic error-detection of LLM-based agentic systems that exhibit variable outputs due to stochastic generation processes. Drawing from temporal logic techniques used in hardware verification, this approach monitors execution traces of agent tool calls and state transitions to detect deviations from expected behavioral patterns. Current error-detection approaches rely primarily on text matching of inputs and outputs, which proves fragile due to the natural language variability inherent in LLM responses. The proposed method instead focuses on the sequence of agent actions -- such as tool invocations and inter-agent communications -- allowing verification of system behavior independent of specific textual outputs. The temporal expression language provides assertions that capture correct behavioral patterns across multiple execution scenarios. These assertions serve dual purposes: validating prompt engineering and guardrail effectiveness during development, and providing regression testing when agents are updated with new LLMs or modified logic. The approach is demonstrated using a three-agent system, where agents coordinate to solve multi-step reasoning tasks. When powered by large, capable models, all temporal assertions were satisfied across many test runs. However, when smaller models were substituted in two of the three agents, executions violated behavioral assertions, primarily due to improper tool sequencing and failed coordination handoffs. The temporal expressions successfully flagged these anomalies, demonstrating the method's effectiveness for detecting behavioral regressions in production agentic systems. This approach provides a foundation for systematic monitoring of AI agent reliability as these systems become increasingly deployed in critical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºç›‘æ§AIæ™ºèƒ½ä½“(Agent)è¡Œä¸ºçš„æ—¶åºè¡¨è¾¾å¼è¯­è¨€(Temporal Expression Language)ï¼Œæ—¨åœ¨é€šè¿‡ç³»ç»ŸåŒ–çš„é”™è¯¯æ£€æµ‹æé«˜åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æ™ºèƒ½ä½“ç³»ç»Ÿçš„å¯é æ€§ã€‚ä¸å½“å‰ä¾èµ–æ–‡æœ¬åŒ¹é…çš„è„†å¼±æ£€æµ‹æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•å€Ÿé‰´äº†ç¡¬ä»¶éªŒè¯(Hardware Verification)ä¸­çš„æ—¶åºé€»è¾‘æŠ€æœ¯ï¼Œé€šè¿‡ç›‘æ§å·¥å…·è°ƒç”¨(Tool Calls)å’ŒçŠ¶æ€è½¬æ¢çš„æ‰§è¡Œè½¨è¿¹æ¥æ£€æµ‹åç¦»é¢„æœŸçš„è¡Œä¸ºæ¨¡å¼ã€‚è¯¥æ—¶åºè¡¨è¾¾å¼èƒ½å¤Ÿå®šä¹‰æ–­è¨€æ¥éªŒè¯æç¤ºè¯å·¥ç¨‹(Prompt Engineering)å’ŒæŠ¤æ (Guardrail)çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºæ™ºèƒ½ä½“é€»è¾‘æ›´æ–°æˆ–æ¨¡å‹æ›¿æ¢æä¾›å›å½’æµ‹è¯•(Regression Testing)æ”¯æŒã€‚é€šè¿‡åœ¨ä¸€ä¸ªä¸‰æ™ºèƒ½ä½“ç³»ç»Ÿä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæ•æ‰åˆ°å°è§„æ¨¡æ¨¡å‹åœ¨å·¥å…·è°ƒç”¨åºåˆ—å’Œåä½œäº¤æ¥ä¸­äº§ç”Ÿçš„å¼‚å¸¸ã€‚è¿™ç§æ–¹æ³•ä¸ºå…³é”®åº”ç”¨åœºæ™¯ä¸‹AIæ™ºèƒ½ä½“ç³»ç»Ÿçš„æŒç»­ç›‘æ§å’Œå¯é æ€§è¯„ä¼°å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.20364v1",
      "published_date": "2025-08-19 18:38:47 UTC",
      "updated_date": "2025-08-19 18:38:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:43:03.558696+00:00"
    },
    {
      "arxiv_id": "2508.14929v1",
      "title": "Heatmap Regression without Soft-Argmax for Facial Landmark Detection",
      "title_zh": "æ— éœ€ Soft-Argmax çš„äººè„¸å…³é”®ç‚¹æ£€æµ‹çƒ­å›¾å›å½’",
      "authors": [
        "Chiao-An Yang",
        "Raymond A. Yeh"
      ],
      "abstract": "Facial landmark detection is an important task in computer vision with numerous applications, such as head pose estimation, expression analysis, face swapping, etc. Heatmap regression-based methods have been widely used to achieve state-of-the-art results in this task. These methods involve computing the argmax over the heatmaps to predict a landmark. Since argmax is not differentiable, these methods use a differentiable approximation, Soft-argmax, to enable end-to-end training on deep-nets. In this work, we revisit this long-standing choice of using Soft-argmax and demonstrate that it is not the only way to achieve strong performance. Instead, we propose an alternative training objective based on the classic structured prediction framework. Empirically, our method achieves state-of-the-art performance on three facial landmark benchmarks (WFLW, COFW, and 300W), converging 2.2x faster during training while maintaining better/competitive accuracy. Our code is available here: https://github.com/ca-joe-yang/regression-without-softarg.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººè„¸å…³é”®ç‚¹æ£€æµ‹(Facial Landmark Detection)è¿™ä¸€è®¡ç®—æœºè§†è§‰æ ¸å¿ƒä»»åŠ¡ï¼Œæ¢è®¨äº†çƒ­å›¾å›å½’(Heatmap Regression)æ–¹æ³•çš„ä¼˜åŒ–æ–¹å‘ã€‚ä¼ ç»Ÿçš„æ­¤ç±»æ–¹æ³•é€šå¸¸ä¾èµ–Soft-argmaxæ¥è§£å†³argmaxæ“ä½œä¸å¯å¾®çš„é—®é¢˜ï¼Œä»è€Œå®ç°æ·±åº¦ç¥ç»ç½‘ç»œçš„ç«¯åˆ°ç«¯è®­ç»ƒã€‚è¯¥è®ºæ–‡é‡æ–°å®¡è§†äº†è¿™ä¸€é•¿æœŸæ²¿ç”¨çš„æŠ€æœ¯æ–¹æ¡ˆï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºç»å…¸ç»“æ„åŒ–é¢„æµ‹(Structured Prediction)æ¡†æ¶çš„æ›¿ä»£è®­ç»ƒç›®æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨WFLWã€COFWå’Œ300Wä¸‰ä¸ªä¸»æµåŸºå‡†æµ‹è¯•é›†ä¸Šå‡å–å¾—äº†SOTAæ€§èƒ½ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ¡ˆåœ¨ä¿æŒä¼˜å¼‚ç²¾åº¦çš„åŒæ—¶ï¼Œè®­ç»ƒæ”¶æ•›é€Ÿåº¦æå‡äº†2.2å€ï¼Œä¸ºå®ç°æ›´é«˜æ•ˆçš„äººè„¸å…³é”®ç‚¹å®šä½æä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14929v1",
      "published_date": "2025-08-19 18:03:29 UTC",
      "updated_date": "2025-08-19 18:03:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:43:11.448924+00:00"
    },
    {
      "arxiv_id": "2508.14160v2",
      "title": "RynnEC: Bringing MLLMs into Embodied World",
      "title_zh": "RynnECï¼šå°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¼•å…¥å…·èº«ä¸–ç•Œ",
      "authors": [
        "Ronghao Dang",
        "Yuqian Yuan",
        "Yunxuan Mao",
        "Kehan Li",
        "Jiangpin Liu",
        "Zhikai Wang",
        "Xin Li",
        "Fan Wang",
        "Deli Zhao"
      ],
      "abstract": "We introduce RynnEC, a video multimodal large language model designed for embodied cognition. Built upon a general-purpose vision-language foundation model, RynnEC incorporates a region encoder and a mask decoder, enabling flexible region-level video interaction. Despite its compact architecture, RynnEC achieves state-of-the-art performance in object property understanding, object segmentation, and spatial reasoning. Conceptually, it offers a region-centric video paradigm for the brain of embodied agents, providing fine-grained perception of the physical world and enabling more precise interactions. To mitigate the scarcity of annotated 3D datasets, we propose an egocentric video based pipeline for generating embodied cognition data. Furthermore, we introduce RynnEC-Bench, a region-centered benchmark for evaluating embodied cognitive capabilities. We anticipate that RynnEC will advance the development of general-purpose cognitive cores for embodied agents and facilitate generalization across diverse embodied tasks. The code, model checkpoints, and benchmark are available at: https://github.com/alibaba-damo-academy/RynnEC",
      "tldr_zh": "æœ¬ç ”ç©¶ä»‹ç»äº† RynnECï¼Œä¸€ç§ä¸“é—¨ä¸ºå…·èº«è®¤çŸ¥ (Embodied Cognition) è®¾è®¡çš„è§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Video Multimodal Large Language Model)ã€‚è¯¥æ¨¡å‹åœ¨é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡å¼•å…¥åŒºåŸŸç¼–ç å™¨ (Region Encoder) å’Œæ©ç è§£ç å™¨ (Mask Decoder)ï¼Œå®ç°äº†çµæ´»çš„åŒºåŸŸçº§è§†é¢‘äº¤äº’ã€‚RynnEC ä¸ºå…·èº«æ™ºèƒ½ä½“æä¾›äº†ä¸€ç§ä»¥åŒºåŸŸä¸ºä¸­å¿ƒçš„è§†é¢‘èŒƒå¼ï¼Œå¢å¼ºäº†å¯¹ç‰©ç†ä¸–ç•Œçš„ç»†ç²’åº¦æ„ŸçŸ¥å¹¶æ”¯æŒæ›´ç²¾ç¡®çš„äº¤äº’ã€‚é’ˆå¯¹ 3D æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§åŸºäºç¬¬ä¸€è§†è§’è§†é¢‘ (Egocentric Video) çš„å…·èº«è®¤çŸ¥æ•°æ®ç”Ÿæˆç®¡çº¿ï¼Œå¹¶å»ºç«‹äº† RynnEC-Bench åŸºå‡†ã€‚å®éªŒè¯æ˜ï¼ŒRynnEC åœ¨ç‰©ä½“å±æ€§ç†è§£ã€ç›®æ ‡åˆ†å‰²å’Œç©ºé—´æ¨ç†æ–¹é¢å‡è¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ (State-of-the-art)ã€‚è¯¥æˆæœæœ‰æœ›æ¨è¿›é€šç”¨å…·èº«è®¤çŸ¥æ ¸å¿ƒçš„å‘å±•ï¼Œå¹¶ä¿ƒè¿›æ¨¡å‹åœ¨å¤šæ ·åŒ–å…·èº«ä»»åŠ¡ä¸­çš„æ³›åŒ–åº”ç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "The technical report of RynnEC, an embodied cognition MLLM",
      "pdf_url": "https://arxiv.org/pdf/2508.14160v2",
      "published_date": "2025-08-19 18:00:01 UTC",
      "updated_date": "2025-11-18 12:40:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:43:13.256570+00:00"
    },
    {
      "arxiv_id": "2508.14153v2",
      "title": "LENS: Learning to Segment Anything with Unified Reinforced Reasoning",
      "title_zh": "LENSï¼šåŸºäºç»Ÿä¸€å¼ºåŒ–æ¨ç†çš„ä¸‡ç‰©åˆ†å‰²å­¦ä¹ ",
      "authors": [
        "Lianghui Zhu",
        "Bin Ouyang",
        "Yuxuan Zhang",
        "Tianheng Cheng",
        "Rui Hu",
        "Haocheng Shen",
        "Longjin Ran",
        "Xiaoxin Chen",
        "Li Yu",
        "Wenyu Liu",
        "Xinggang Wang"
      ],
      "abstract": "Text-prompted image segmentation enables fine-grained visual understanding and is critical for applications such as human-computer interaction and robotics. However, existing supervised fine-tuning methods typically ignore explicit chain-of-thought (CoT) reasoning at test time, which limits their ability to generalize to unseen prompts and domains. To address this issue, we introduce LENS, a scalable reinforcement-learning framework that jointly optimizes the reasoning process and segmentation in an end-to-end manner. We propose unified reinforcement-learning rewards that span sentence-, box-, and segment-level cues, encouraging the model to generate informative CoT rationales while refining mask quality. Using a publicly available 3-billion-parameter vision-language model, i.e., Qwen2.5-VL-3B-Instruct, LENS achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks, outperforming the strong fine-tuned method, i.e., GLaMM, by up to 5.6%. These results demonstrate that RL-driven CoT reasoning significantly enhances text-prompted segmentation and offers a practical path toward more generalizable Segment Anything models (SAM). Code is available at https://github.com/hustvl/LENS.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LENSï¼Œä¸€ä¸ªå¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç»Ÿä¸€çš„å¼ºåŒ–æ¨ç†æ¥å®ç°æ–‡æœ¬æç¤ºä¸‹çš„ä»»æ„ç‰©ä½“åˆ†å‰²(Segment Anything)ã€‚é’ˆå¯¹ç°æœ‰ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨æµ‹è¯•æ—¶å¿½ç•¥æ˜¾å¼é“¾å¼æ€ç»´(Chain-of-Thought, CoT)æ¨ç†è€Œå¯¼è‡´çš„æ³›åŒ–èƒ½åŠ›å—é™é—®é¢˜ï¼ŒLENSé‡‡ç”¨ç«¯åˆ°ç«¯çš„æ–¹å¼ååŒä¼˜åŒ–æ¨ç†è¿‡ç¨‹ä¸åˆ†å‰²ä»»åŠ¡ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†æ¶µç›–å¥å­ã€æ£€æµ‹æ¡†åŠåˆ†å‰²æ©ç å±‚é¢çš„ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ å¥–åŠ±æœºåˆ¶ï¼Œæœ‰æ•ˆå¼•å¯¼æ¨¡å‹ç”Ÿæˆä¿¡æ¯ä¸°å¯Œçš„CoTé€»è¾‘ä¾æ®å¹¶æå‡æ©ç è´¨é‡ã€‚åŸºäºQwen2.5-VL-3B-Instructè§†è§‰è¯­è¨€æ¨¡å‹ï¼ŒLENSåœ¨RefCOCOã€RefCOCO+å’ŒRefCOCOgåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†81.2%çš„å¹³å‡cIoUï¼Œæ€§èƒ½è¶…è¶Šå¼ºåŠ›å¾®è°ƒæ–¹æ³•GLaMMè¾¾5.6%ã€‚å®éªŒç»“æœè¯æ˜ï¼Œå¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„CoTæ¨ç†èƒ½æ˜¾è‘—å¢å¼ºæ–‡æœ¬æç¤ºåˆ†å‰²çš„æ•ˆèƒ½ï¼Œä¸ºæ„å»ºæ›´å…·é€šç”¨æ€§çš„Segment Anything Models (SAM)æä¾›äº†åˆ‡å®å¯è¡Œçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Code is released at https://github.com/hustvl/LENS",
      "pdf_url": "https://arxiv.org/pdf/2508.14153v2",
      "published_date": "2025-08-19 17:59:53 UTC",
      "updated_date": "2025-11-18 07:39:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:43:17.263375+00:00"
    },
    {
      "arxiv_id": "2508.14040v2",
      "title": "ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents",
      "title_zh": "ComputerRLï¼šé¢å‘è®¡ç®—æœºæ“ä½œæ™ºèƒ½ä½“çš„è§„æ¨¡åŒ–ç«¯åˆ°ç«¯åœ¨çº¿å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Hanyu Lai",
        "Xiao Liu",
        "Yanxiao Zhao",
        "Han Xu",
        "Hanchen Zhang",
        "Bohao Jing",
        "Yanyu Ren",
        "Shuntian Yao",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "abstract": "We introduce ComputerRL, a framework for autonomous desktop intelligence that enables agents to operate complex digital workspaces skillfully. ComputerRL features the API-GUI paradigm, which unifies programmatic API calls and direct GUI interaction to address the inherent mismatch between machine agents and human-centric desktop environments. Scaling end-to-end RL training is crucial for improvement and generalization across diverse desktop tasks; however, it remains challenging due to environmental inefficiency and instability during extended training. To support scalable and robust training, we develop a distributed RL infrastructure capable of orchestrating thousands of parallel virtual desktop environments to accelerate large-scale online RL. Furthermore, we propose Entropulse, a training strategy that alternates reinforcement learning with supervised fine-tuning, effectively mitigating entropy collapse during extended training runs. We employ ComputerRL on open models GLM-4-9B-0414 and GLM-4.1V-9B-Thinking, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B achieves a new state-of-the-art accuracy of 48.9%, demonstrating significant improvements for general agents in desktop automation. Our code and the new OfficeWorld benchmark are available at https://github.com/thudm/ComputerRL. The algorithm and framework are adopted in building AutoGLM (Liu et al., 2024b).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ComputerRLï¼Œä¸€ä¸ªæ—¨åœ¨æå‡æ™ºèƒ½ä½“åœ¨å¤æ‚æ•°å­—å·¥ä½œç©ºé—´æ“ä½œèƒ½åŠ›çš„ç«¯åˆ°ç«¯æ¡Œé¢æ™ºèƒ½æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº† API-GUI èŒƒå¼ï¼Œé€šè¿‡ç»Ÿä¸€ç¨‹åºåŒ– API è°ƒç”¨ä¸ç›´æ¥ GUI äº¤äº’ï¼Œè§£å†³äº†æœºå™¨æ™ºèƒ½ä½“ä¸ä»¥äººä¸ºä¸­å¿ƒçš„æ¡Œé¢ç¯å¢ƒä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚ä¸ºäº†æ”¯æŒå¤§è§„æ¨¡åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€å¥—å¯ç¼–æ’æ•°åƒä¸ªå¹¶è¡Œè™šæ‹Ÿæ¡Œé¢ç¯å¢ƒçš„åˆ†å¸ƒå¼ RL åŸºç¡€è®¾æ–½ï¼Œæ˜¾è‘—æå‡äº†è®­ç»ƒçš„æ‰©å±•æ€§ä¸ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº† Entropulse è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡äº¤æ›¿è¿›è¡Œå¼ºåŒ–å­¦ä¹ ä¸ç›‘ç£å¾®è°ƒ (SFT) æ¥æœ‰æ•ˆæŠ‘åˆ¶é•¿æ—¶è®­ç»ƒä¸­çš„ç†µåç¼©é—®é¢˜ã€‚åŸºäº GLM-4 åŸºç¡€æ¨¡å‹æ„å»ºçš„ AutoGLM-OS-9B åœ¨ OSWorld åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† 48.9% çš„å‡†ç¡®ç‡ï¼Œåˆ·æ–°äº† SOTA è®°å½•ï¼Œå±•ç¤ºäº†é€šç”¨æ™ºèƒ½ä½“åœ¨æ¡Œé¢è‡ªåŠ¨åŒ–é¢†åŸŸçš„å·¨å¤§æ½œåŠ›ã€‚è¯¥é¡¹ç›®è¿˜åŒæ—¶å‘å¸ƒäº†å…¨æ–°çš„ OfficeWorld åŸºå‡†æµ‹è¯•ä»¥æ”¯æŒåç»­ç ”ç©¶ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14040v2",
      "published_date": "2025-08-19 17:59:45 UTC",
      "updated_date": "2025-10-21 05:18:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:43:22.145958+00:00"
    },
    {
      "arxiv_id": "2508.14036v2",
      "title": "GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation",
      "title_zh": "GeoSAM2ï¼šé‡Šæ”¾ SAM2 åœ¨ 3D éƒ¨ä»¶åˆ†å‰²ä¸­çš„æ½œåŠ›",
      "authors": [
        "Ken Deng",
        "Yunhan Yang",
        "Jingxiang Sun",
        "Xihui Liu",
        "Yebin Liu",
        "Ding Liang",
        "Yan-Pei Cao"
      ],
      "abstract": "We introduce GeoSAM2, a prompt-controllable framework for 3D part segmentation that casts the task as multi-view 2D mask prediction. Given a textureless object, we render normal and point maps from predefined viewpoints and accept simple 2D prompts - clicks or boxes - to guide part selection. These prompts are processed by a shared SAM2 backbone augmented with LoRA and residual geometry fusion, enabling view-specific reasoning while preserving pretrained priors. The predicted masks are back-projected to the object and aggregated across views. Our method enables fine-grained, part-specific control without requiring text prompts, per-shape optimization, or full 3D labels. In contrast to global clustering or scale-based methods, prompts are explicit, spatially grounded, and interpretable. We achieve state-of-the-art class-agnostic performance on PartObjaverse-Tiny and PartNetE, outperforming both slow optimization-based pipelines and fast but coarse feedforward approaches. Our results highlight a new paradigm: aligning the paradigm of 3D segmentation with SAM2, leveraging interactive 2D inputs to unlock controllability and precision in object-level part understanding.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶ä»‹ç»äº† GeoSAM2ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äº 3D part segmentation çš„å¯æç¤ºæ§åˆ¶æ¡†æ¶ï¼Œå°†ä»»åŠ¡è½¬åŒ–ä¸ºå¤šè§†å›¾ 2D mask é¢„æµ‹ã€‚è¯¥æ–¹æ³•é’ˆå¯¹æ— çº¹ç†ç‰©ä½“ï¼Œä»é¢„å®šä¹‰çš„è§†ç‚¹æ¸²æŸ“ normal maps å’Œ point mapsï¼Œå¹¶æ¥æ”¶ç®€å•çš„ 2D promptsï¼ˆç‚¹å‡»æˆ–æ–¹æ¡†ï¼‰æ¥å¼•å¯¼é›¶ä»¶é€‰æ‹©ã€‚é€šè¿‡ä½¿ç”¨é›†æˆ LoRA å’Œ residual geometry fusion çš„å…±äº« SAM2 ä¸»å¹²ç½‘ç»œï¼Œè¯¥æ¡†æ¶åœ¨ä¿ç•™é¢„è®­ç»ƒå…ˆéªŒçš„åŒæ—¶å®ç°äº†è§†å›¾ç‰¹å®šçš„æ¨ç†ã€‚é¢„æµ‹çš„æ©ç è¢«åå‘æŠ•å½±åˆ°ç‰©ä½“ä¸Šå¹¶è¿›è¡Œå¤šè§†å›¾èšåˆï¼Œä»è€Œå®ç°äº†æ— éœ€æ–‡æœ¬æç¤ºã€æ¯å½¢çŠ¶ä¼˜åŒ– (per-shape optimization) æˆ–å®Œæ•´ 3D æ ‡ç­¾çš„ç»†ç²’åº¦é›¶ä»¶æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGeoSAM2 åœ¨ PartObjaverse-Tiny å’Œ PartNetE æ•°æ®é›†ä¸Šè¾¾åˆ°äº† class-agnostic çš„ state-of-the-art æ€§èƒ½ï¼Œè¶…è¶Šäº†åŸºäºä¼˜åŒ–çš„ç®¡çº¿å’Œå‰é¦ˆæ–¹æ³•ã€‚è¯¥ç ”ç©¶é€šè¿‡å°† 3D åˆ†å‰²èŒƒå¼ä¸ SAM2 å¯¹é½ï¼Œåˆ©ç”¨äº¤äº’å¼ 2D è¾“å…¥æˆåŠŸè§£é”äº†ç‰©ä½“çº§é›¶ä»¶ç†è§£çš„å¯æ§æ€§ä¸ç²¾ç¡®æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "https://detailgen3d.github.io/GeoSAM2/",
      "pdf_url": "https://arxiv.org/pdf/2508.14036v2",
      "published_date": "2025-08-19 17:58:51 UTC",
      "updated_date": "2025-08-27 17:10:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:43:27.746114+00:00"
    },
    {
      "arxiv_id": "2508.14031v2",
      "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation",
      "title_zh": "æ™ºèƒ½ä½“åŒ–å¾®è°ƒå¯¼è‡´çš„éé¢„æœŸå¯¹é½å¤±æ•ˆï¼šé£é™©ä¸ç¼“è§£",
      "authors": [
        "Dongyoon Hahm",
        "Taywon Min",
        "Woogyeol Jin",
        "Kimin Lee"
      ],
      "abstract": "Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å‘å…·æœ‰è§„åˆ’å’Œå·¥å…·äº¤äº’èƒ½åŠ›çš„æ™ºèƒ½ä½“(Agentic Systems)æ¼”è¿›è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ç‰¹å®šä»»åŠ¡å¾®è°ƒ(Fine-Tuning)å¯èƒ½å¯¼è‡´çš„éé¢„æœŸå¤±é…(Unintended Misalignment)é£é™©ã€‚ç ”ç©¶å‘ç°ï¼Œé’ˆå¯¹æ™ºèƒ½ä½“ä»»åŠ¡è¿›è¡Œçš„å¾®è°ƒä¼šé™ä½æ¨¡å‹å¯¹æœ‰å®³æŒ‡ä»¤çš„æ‹’ç»ç‡ï¼Œæ˜¾è‘—å¢åŠ æ‰§è¡Œæœ‰å®³ä»»åŠ¡çš„å¯èƒ½æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Prefix INjection Guard (PING)ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡åœ¨æ™ºèƒ½ä½“å“åº”å‰æ·»åŠ è‡ªåŠ¨ç”Ÿæˆçš„è‡ªç„¶è¯­è¨€å‰ç¼€ï¼Œå¼•å¯¼æ¨¡å‹åœ¨ä¿ç•™è‰¯æ€§ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶æ‹’ç»æœ‰å®³è¯·æ±‚çš„é˜²å¾¡æ–¹æ³•ã€‚PINGåˆ©ç”¨è¿­ä»£æ–¹æ³•äº¤æ›¿ç”Ÿæˆå¹¶ç­›é€‰èƒ½å¤ŸåŒæ—¶ä¼˜åŒ–ä»»åŠ¡è¡¨ç°ä¸æ‹’ç»è¡Œä¸ºçš„å€™é€‰å‰ç¼€ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒPINGåœ¨Webå¯¼èˆªå’Œä»£ç ç”Ÿæˆä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰çš„æç¤ºæ–¹æ³•ï¼Œæœ‰æ•ˆå¢å¼ºäº†å¾®è°ƒæ™ºèƒ½ä½“çš„å®‰å…¨æ€§ã€‚æœ€åï¼Œé€šè¿‡å¯¹å†…éƒ¨éšè—çŠ¶æ€çš„çº¿æ€§æ¢æµ‹(Linear Probes)åˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†å‰ç¼€Tokenå¯¹äºè¡Œä¸ºä¿®æ­£çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at AAAI 2026 AI Alignment Track, Source code: https://github.com/HahmDY/agentic-ft-safety",
      "pdf_url": "https://arxiv.org/pdf/2508.14031v2",
      "published_date": "2025-08-19 17:53:35 UTC",
      "updated_date": "2025-11-17 16:48:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:43:27.941899+00:00"
    },
    {
      "arxiv_id": "2508.14151v2",
      "title": "A Systematic Study of Deep Learning Models and xAI Methods for Region-of-Interest Detection in MRI Scans",
      "title_zh": "MRI æ‰«æä¸­æ„Ÿå…´è¶£åŒºåŸŸæ£€æµ‹çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸ xAI æ–¹æ³•ç³»ç»Ÿæ€§ç ”ç©¶",
      "authors": [
        "Justin Yiu",
        "Kushank Arora",
        "Daniel Steinberg",
        "Rohit Ghiya"
      ],
      "abstract": "Magnetic Resonance Imaging (MRI) is an essential diagnostic tool for assessing knee injuries. However, manual interpretation of MRI slices remains time-consuming and prone to inter-observer variability. This study presents a systematic evaluation of various deep learning architectures combined with explainable AI (xAI) techniques for automated region of interest (ROI) detection in knee MRI scans. We investigate both supervised and self-supervised approaches, including ResNet50, InceptionV3, Vision Transformers (ViT), and multiple U-Net variants augmented with multi-layer perceptron (MLP) classifiers. To enhance interpretability and clinical relevance, we integrate xAI methods such as Grad-CAM and Saliency Maps. Model performance is assessed using AUC for classification and PSNR/SSIM for reconstruction quality, along with qualitative ROI visualizations. Our results demonstrate that ResNet50 consistently excels in classification and ROI identification, outperforming transformer-based models under the constraints of the MRNet dataset. While hybrid U-Net + MLP approaches show potential for leveraging spatial features in reconstruction and interpretability, their classification performance remains lower. Grad-CAM consistently provided the most clinically meaningful explanations across architectures. Overall, CNN-based transfer learning emerges as the most effective approach for this dataset, while future work with larger-scale pretraining may better unlock the potential of transformer models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è†å…³èŠ‚ MRI æ‰«æä¸­äººå·¥è§£é‡Šè€—æ—¶ä¸”å­˜åœ¨è§‚å¯Ÿè€…å·®å¼‚çš„é—®é¢˜ï¼Œç³»ç»Ÿè¯„ä¼°äº†å¤šç§æ·±åº¦å­¦ä¹  (Deep Learning) æ¶æ„ä¸å¯è§£é‡Šäººå·¥æ™ºèƒ½ (xAI) æŠ€æœ¯åœ¨è‡ªåŠ¨æ„Ÿå…´è¶£åŒºåŸŸ (ROI) æ£€æµ‹ä¸­çš„è¡¨ç°ã€‚å®éªŒæ¶µç›–äº†ç›‘ç£å­¦ä¹ ä¸è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬ ResNet50ã€InceptionV3ã€Vision Transformers (ViT) ä»¥åŠç»“åˆå¤šå±‚æ„ŸçŸ¥æœº (MLP) çš„å¤šç§ U-Net å˜ä½“ã€‚ä¸ºäº†å¢å¼ºä¸´åºŠç›¸å…³æ€§ï¼Œç ”ç©¶é›†æˆäº† Grad-CAM å’Œæ˜¾è‘—æ€§å›¾ (Saliency Maps) ç­‰ xAI æ‰‹æ®µï¼Œå¹¶åˆ©ç”¨ AUCã€PSNR å’Œ SSIM ç­‰æŒ‡æ ‡è¿›è¡Œå®šé‡è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨ MRNet æ•°æ®é›†çº¦æŸä¸‹ï¼ŒResNet50 åœ¨åˆ†ç±»å’Œ ROI è¯†åˆ«æ–¹é¢è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼Œä¼˜äºåŸºäº Transformer çš„æ¨¡å‹ã€‚å°½ç®¡æ··åˆ U-Net + MLP æ¶æ„åœ¨ç‰¹å¾é‡å»ºä¸å¯è§£é‡Šæ€§æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œä½†å…¶åˆ†ç±»æ€§èƒ½ä»ç›¸å¯¹è¾ƒä½ï¼Œè€Œ Grad-CAM åœ¨ä¸åŒæ¶æ„ä¸­å‡èƒ½æä¾›æœ€å…·ä¸´åºŠæ„ä¹‰çš„è§£é‡Šã€‚æ€»ä½“è€Œè¨€ï¼ŒåŸºäº CNN çš„è¿ç§»å­¦ä¹ æ˜¯å¤„ç†è¯¥æ•°æ®é›†ç›®å‰æœ€æœ‰æ•ˆçš„æ–¹æ³•ï¼Œæœªæ¥æ›´å¤§è§„æ¨¡çš„é¢„è®­ç»ƒå¯èƒ½è¿›ä¸€æ­¥é‡Šæ”¾ Transformer æ¨¡å‹çš„æ½œåŠ›ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14151v2",
      "published_date": "2025-08-19 17:42:45 UTC",
      "updated_date": "2025-08-21 08:09:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:43:26.754619+00:00"
    },
    {
      "arxiv_id": "2508.14025v1",
      "title": "Ask Good Questions for Large Language Models",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹çš„é«˜è´¨é‡æé—®",
      "authors": [
        "Qi Wu",
        "Zhongqi Lu"
      ],
      "abstract": "Recent advances in large language models (LLMs) have significantly improved the performance of dialog systems, yet current approaches often fail to provide accurate guidance of topic due to their inability to discern user confusion in related concepts. To address this, we introduce the Ask-Good-Question (AGQ) framework, which features an improved Concept-Enhanced Item Response Theory (CEIRT) model to better identify users' knowledge levels. Our contributions include applying the CEIRT model along with LLMs to directly generate guiding questions based on the inspiring text, greatly improving information retrieval efficiency during the question & answer process. Through comparisons with other baseline methods, our approach outperforms by significantly enhencing the users' information retrieval experiences.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å¯¹è¯ç³»ç»Ÿåœ¨å¤„ç†ç”¨æˆ·æ··æ·†æ¦‚å¿µæ—¶ç¼ºä¹å‡†ç¡®å¼•å¯¼çš„é—®é¢˜ï¼Œæå‡ºäº†Ask-Good-Question (AGQ) æ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯æ”¹è¿›çš„Concept-Enhanced Item Response Theory (CEIRT) æ¨¡å‹ï¼Œèƒ½å¤Ÿæ›´ç²¾å‡†åœ°è¯†åˆ«ç”¨æˆ·çš„å®æ—¶çŸ¥è¯†æ°´å¹³ã€‚é€šè¿‡å°†CEIRTæ¨¡å‹ä¸LLMsç»“åˆï¼Œè¯¥æ–¹æ³•å¯ä»¥ç›´æ¥åŸºäºå¯å‘æ€§æ–‡æœ¬ç”Ÿæˆå¼•å¯¼æ€§é—®é¢˜ï¼Œä»è€Œå¤§å¹…æé«˜é—®ç­”äº¤äº’ä¸­çš„ä¿¡æ¯æ£€ç´¢æ•ˆç‡ã€‚å®éªŒå¯¹æ¯”æ˜¾ç¤ºï¼ŒAGQæ¡†æ¶åœ¨ä¼˜åŒ–ç”¨æˆ·ä¿¡æ¯æ£€ç´¢ä½“éªŒæ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºçº¿æ¨¡å‹ã€‚è¯¥é¡¹å·¥ä½œé€šè¿‡å¼•å…¥ä¸»åŠ¨æé—®æœºåˆ¶ï¼Œä¸ºè§£å†³å¤æ‚æ¦‚å¿µç¯å¢ƒä¸‹çš„å¯¹è¯å¼•å¯¼å’Œä¿¡æ¯æ£€ç´¢æ•ˆç‡é—®é¢˜æä¾›äº†æ–°çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14025v1",
      "published_date": "2025-08-19 17:31:42 UTC",
      "updated_date": "2025-08-19 17:31:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:43:32.046293+00:00"
    },
    {
      "arxiv_id": "2508.14020v1",
      "title": "A Biased Random Key Genetic Algorithm for Solving the Longest Run Subsequence Problem",
      "title_zh": "æ±‚è§£æœ€é•¿è¡Œç¨‹å­åºåˆ—é—®é¢˜çš„æœ‰åéšæœºé”®é—ä¼ ç®—æ³•",
      "authors": [
        "Christian Blum",
        "Pedro Pinacho-Davidson"
      ],
      "abstract": "The longest run subsequence (LRS) problem is an NP-hard combinatorial optimization problem belonging to the class of subsequence problems from bioinformatics. In particular, the problem plays a role in genome reassembly. In this paper, we present a solution to the LRS problem using a Biased Random Key Genetic Algorithm (BRKGA). Our approach places particular focus on the computational efficiency of evaluating individuals, which involves converting vectors of gray values into valid solutions to the problem. For comparison purposes, a Max-Min Ant System is developed and implemented. This is in addition to the application of the integer linear programming solver CPLEX for solving all considered problem instances. The computation results show that the proposed BRKGA is currently a state-of-the-art technique for the LRS problem. Nevertheless, the results also show that there is room for improvement, especially in the context of input strings based on large alphabet sizes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­ç”¨äºåŸºå› ç»„é‡ç»„çš„NP-hardç»„åˆä¼˜åŒ–é—®é¢˜â€”â€”æœ€é•¿è¿ç»­å­åºåˆ— (Longest Run Subsequence, LRS) é—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºåç½®éšæœºé”®é—ä¼ ç®—æ³• (Biased Random Key Genetic Algorithm, BRKGA) çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ç‰¹åˆ«å…³æ³¨ä¸ªä½“è¯„ä¼°çš„è®¡ç®—æ•ˆç‡ï¼Œæ¶‰åŠå°†ç°åº¦å€¼å‘é‡è½¬æ¢ä¸ºè¯¥é—®é¢˜çš„æœ‰æ•ˆè§£ã€‚ä¸ºäº†éªŒè¯ç®—æ³•æ€§èƒ½ï¼Œç ”ç©¶åŒæ—¶å¼€å‘å¹¶å®ç°äº†æœ€å¤§æœ€å°èšç¾¤ç³»ç»Ÿ (Max-Min Ant System)ï¼Œå¹¶åˆ©ç”¨æ•´æ•°çº¿æ€§è§„åˆ’æ±‚è§£å™¨ CPLEX è¿›è¡Œäº†å¯¹æ¯”æµ‹è¯•ã€‚è®¡ç®—ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ BRKGA æ˜¯ç›®å‰è§£å†³ LRS é—®é¢˜çš„æœ€å…ˆè¿›æŠ€æœ¯ (state-of-the-art)ã€‚æ­¤å¤–ï¼Œç»“æœè¿˜æ˜¾ç¤ºåœ¨å¤„ç†åŸºäºå¤§å­—æ¯è¡¨è§„æ¨¡ (large alphabet sizes) çš„è¾“å…¥å­—ç¬¦ä¸²æ—¶ï¼Œè¯¥æ–¹æ³•ä»å…·æœ‰è¿›ä¸€æ­¥çš„æ”¹è¿›ç©ºé—´ã€‚",
      "categories": [
        "cs.AI",
        "cs.DM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14020v1",
      "published_date": "2025-08-19 17:27:29 UTC",
      "updated_date": "2025-08-19 17:27:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:43:35.243253+00:00"
    },
    {
      "arxiv_id": "2508.14013v1",
      "title": "Efficient Knowledge Graph Unlearning with Zeroth-order Information",
      "title_zh": "åŸºäºé›¶é˜¶ä¿¡æ¯çš„é«˜æ•ˆçŸ¥è¯†å›¾è°±é—å¿˜",
      "authors": [
        "Yang Xiao",
        "Ruimeng Ye",
        "Bohan Liu",
        "Xiaolong Ma",
        "Bo Hui"
      ],
      "abstract": "Due to regulations like the Right to be Forgotten, there is growing demand for removing training data and its influence from models. Since full retraining is costly, various machine unlearning methods have been proposed. In this paper, we firstly present an efficient knowledge graph (KG) unlearning algorithm. We remark that KG unlearning is nontrivial due to the distinctive structure of KG and the semantic relations between entities. Also, unlearning by estimating the influence of removed components incurs significant computational overhead when applied to large-scale knowledge graphs. To this end, we define an influence function for KG unlearning and propose to approximate the model's sensitivity without expensive computation of first-order and second-order derivatives for parameter updates. Specifically, we use Taylor expansion to estimate the parameter changes caused by data removal. Given that the first-order gradients and second-order derivatives dominate the computational load, we use the Fisher matrices and zeroth-order optimization to approximate the inverse-Hessian vector product without constructing the computational graphs. Our experimental results demonstrate that the proposed method outperforms other state-of-the-art graph unlearning baselines significantly in terms of unlearning efficiency and unlearning quality. Our code is released at https://github.com/NKUShaw/ZOWFKGIF.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºé›¶é˜¶ä¿¡æ¯(Zeroth-order Information)çš„é«˜æ•ˆçŸ¥è¯†å›¾è°±å¸è½½(Knowledge Graph Unlearning)ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³åœ¨å¤§è§„æ¨¡çŸ¥è¯†å›¾è°±ä¸­ç§»é™¤ç‰¹å®šè®­ç»ƒæ•°æ®åŠå…¶å½±å“æ—¶é¢ä¸´çš„è®¡ç®—é«˜æ˜‚é—®é¢˜ã€‚é’ˆå¯¹ä¼ ç»Ÿå½±å“å‡½æ•°(Influence Function)åœ¨å¤„ç†å¤æ‚è¯­ä¹‰å…³ç³»å’Œç»“æ„æ—¶äº§ç”Ÿçš„é«˜é¢ä¸€é˜¶åŠäºŒé˜¶å¯¼æ•°è®¡ç®—å¼€é”€ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ³°å‹’å±•å¼€(Taylor expansion)æ¥ä¼°ç®—å‚æ•°å˜åŒ–ã€‚é€šè¿‡ç»“åˆè´¹èˆå°”çŸ©é˜µ(Fisher matrices)å’Œé›¶é˜¶ä¼˜åŒ–(Zeroth-order optimization)æŠ€æœ¯ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿåœ¨ä¸æ„å»ºè®¡ç®—å›¾çš„æƒ…å†µä¸‹è¿‘ä¼¼è®¡ç®—é€†é»‘å¡å‘é‡ç§¯(Inverse-Hessian vector product)ï¼Œä»è€Œæ˜¾è‘—æå‡æ›´æ–°æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå¸è½½è´¨é‡çš„åŒæ—¶ï¼Œåœ¨æ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„å›¾å¸è½½åŸºå‡†æ¨¡å‹ï¼Œä¸ºå®ç°å¤§è§„æ¨¡çŸ¥è¯†å›¾è°±çš„çŸ¥è¯†æ“¦é™¤æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 page",
      "pdf_url": "https://arxiv.org/pdf/2508.14013v1",
      "published_date": "2025-08-19 17:22:50 UTC",
      "updated_date": "2025-08-19 17:22:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:43:45.855593+00:00"
    },
    {
      "arxiv_id": "2508.14012v1",
      "title": "Evaluating Identity Leakage in Speaker De-Identification Systems",
      "title_zh": "è¯´è¯äººå»æ ‡è¯†åŒ–ç³»ç»Ÿä¸­çš„èº«ä»½æ³„éœ²è¯„ä¼°",
      "authors": [
        "Seungmin Seo",
        "Oleg Aulov",
        "Afzal Godil",
        "Kevin Mangold"
      ],
      "abstract": "Speaker de-identification aims to conceal a speaker's identity while preserving intelligibility of the underlying speech. We introduce a benchmark that quantifies residual identity leakage with three complementary error rates: equal error rate, cumulative match characteristic hit rate, and embedding-space similarity measured via canonical correlation analysis and Procrustes analysis. Evaluation results reveal that all state-of-the-art speaker de-identification systems leak identity information. The highest performing system in our evaluation performs only slightly better than random guessing, while the lowest performing system achieves a 45% hit rate within the top 50 candidates based on CMC. These findings highlight persistent privacy risks in current speaker de-identification technologies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¯´è¯äººåŒ¿ååŒ–(Speaker De-Identification)ç³»ç»Ÿä¸­çš„èº«ä»½æ³„éœ²é—®é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°å…¶åœ¨ä¿æŠ¤éšç§çš„åŒæ—¶ä¿ç•™è¯­éŸ³å¯ç†è§£æ€§çš„å®é™…æ•ˆæœã€‚ä½œè€…æå‡ºäº†ä¸€å¥—å…¨æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡ç­‰é”™è¯¯ç‡(Equal Error Rate)ã€ç´¯ç§¯åŒ¹é…ç‰¹æ€§(Cumulative Match Characteristic)ç‚¹å‡»ç‡ï¼Œä»¥åŠåŸºäºå…¸å‹ç›¸å…³åˆ†æ(Canonical Correlation Analysis)å’Œæ™®æ°åˆ†æ(Procrustes Analysis)çš„åµŒå…¥ç©ºé—´ç›¸ä¼¼åº¦è¿™ä¸‰ä¸ªäº’è¡¥æŒ‡æ ‡æ¥é‡åŒ–æ®‹ç•™çš„èº«ä»½æ³„éœ²ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œç›®å‰æ‰€æœ‰æœ€å…ˆè¿›çš„è¯´è¯äººåŒ¿ååŒ–ç³»ç»Ÿå‡å­˜åœ¨èº«ä»½ä¿¡æ¯æ³„éœ²ç°è±¡ã€‚å®éªŒå‘ç°è¡¨ç°æœ€å¥½çš„ç³»ç»Ÿä»…æ¯”éšæœºçŒœæµ‹ç•¥å¥½ï¼Œè€Œè¡¨ç°æœ€å·®çš„ç³»ç»Ÿåœ¨CMCå‰50åå€™é€‰è€…ä¸­çš„å‘½ä¸­ç‡é«˜è¾¾45%ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰è¯´è¯äººåŒ¿ååŒ–æŠ€æœ¯ä¸­ä¾ç„¶å­˜åœ¨ä¸¥å³»ä¸”æŒç»­çš„éšç§é£é™©ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "Submitted to ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2508.14012v1",
      "published_date": "2025-08-19 17:20:25 UTC",
      "updated_date": "2025-08-19 17:20:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:43:50.855140+00:00"
    },
    {
      "arxiv_id": "2508.14005v1",
      "title": "ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery",
      "title_zh": "ASDFormerï¼šèåˆæ± åŒ–-åˆ†ç±»ä¸“å®¶æ··åˆæœºåˆ¶çš„ Transformerï¼Œç”¨äºé²æ£’çš„è‡ªé—­ç—‡è¯Šæ–­ä¸ç”Ÿç‰©æ ‡å¿—ç‰©å‘ç°",
      "authors": [
        "Mohammad Izadi",
        "Mehran Safayani"
      ],
      "abstract": "Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition marked by disruptions in brain connectivity. Functional MRI (fMRI) offers a non-invasive window into large-scale neural dynamics by measuring blood-oxygen-level-dependent (BOLD) signals across the brain. These signals can be modeled as interactions among Regions of Interest (ROIs), which are grouped into functional communities based on their underlying roles in brain function. Emerging evidence suggests that connectivity patterns within and between these communities are particularly sensitive to ASD-related alterations. Effectively capturing these patterns and identifying interactions that deviate from typical development is essential for improving ASD diagnosis and enabling biomarker discovery. In this work, we introduce ASDFormer, a Transformer-based architecture that incorporates a Mixture of Pooling-Classifier Experts (MoE) to capture neural signatures associated with ASD. By integrating multiple specialized expert branches with attention mechanisms, ASDFormer adaptively emphasizes different brain regions and connectivity patterns relevant to autism. This enables both improved classification performance and more interpretable identification of disorder-related biomarkers. Applied to the ABIDE dataset, ASDFormer achieves state-of-the-art diagnostic accuracy and reveals robust insights into functional connectivity disruptions linked to ASD, highlighting its potential as a tool for biomarker discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ASDFormerï¼Œä¸€ç§åŸºäº Transformer çš„æ¶æ„ï¼Œæ—¨åœ¨é€šè¿‡åŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒ (fMRI) æé«˜è‡ªé—­ç—‡è°±ç³»éšœç¢ (ASD) çš„è¯Šæ–­å‡†ç¡®æ€§å¹¶ä¿ƒè¿›ç”Ÿç‰©æ ‡å¿—ç‰© (Biomarker) çš„å‘ç°ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†æ± åŒ–-åˆ†ç±»ä¸“å®¶æ··åˆæœºåˆ¶ (Mixture of Pooling-Classifier Experts, MoE)ï¼Œé€šè¿‡æ•´åˆå¤šä¸ªä¸“é—¨çš„ä¸“å®¶åˆ†æ”¯ä¸æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanisms)ï¼Œå®ç°å¯¹ä¸åŒå¤§è„‘åŒºåŸŸå’Œè¿æ¥æ¨¡å¼çš„è‡ªé€‚åº”å…³æ³¨ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æ˜¾è‘—æå‡äº†åˆ†ç±»æ€§èƒ½ï¼Œè¿˜å¢å¼ºäº†è¯†åˆ«ç–¾ç—…ç›¸å…³ç¥ç»ç‰¹å¾çš„å¯è§£é‡Šæ€§ã€‚åœ¨ ABIDE æ•°æ®é›†ä¸Šçš„åº”ç”¨ç»“æœæ˜¾ç¤ºï¼ŒASDFormer è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„ (State-of-the-art) è¯Šæ–­ç²¾åº¦ï¼Œå¹¶æ­ç¤ºäº†ä¸ ASD ç›¸å…³çš„è„‘åŠŸèƒ½è¿æ¥ (Functional Connectivity) å¼‚å¸¸ã€‚ç ”ç©¶ç»“æœè¯æ˜äº† ASDFormer åœ¨æ•æ‰å¤æ‚ç¥ç»åŠ¨åŠ›å­¦å’Œè¾…åŠ©ä¸´åºŠè¯Šæ–­æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14005v1",
      "published_date": "2025-08-19 17:05:45 UTC",
      "updated_date": "2025-08-19 17:05:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:44:05.549209+00:00"
    },
    {
      "arxiv_id": "2508.13998v1",
      "title": "Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation",
      "title_zh": "Embodied-R1ï¼šé¢å‘é€šç”¨æœºå™¨äººæ“çºµçš„å¼ºåŒ–å…·èº«æ¨ç†",
      "authors": [
        "Yifu Yuan",
        "Haiqin Cui",
        "Yaoting Huang",
        "Yibin Chen",
        "Fei Ni",
        "Zibin Dong",
        "Pengyi Li",
        "Yan Zheng",
        "Jianye Hao"
      ],
      "abstract": "Generalization in embodied AI is hindered by the \"seeing-to-doing gap,\" which stems from data scarcity and embodiment heterogeneity. To address this, we pioneer \"pointing\" as a unified, embodiment-agnostic intermediate representation, defining four core embodied pointing abilities that bridge high-level vision-language comprehension with low-level action primitives. We introduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed for embodied reasoning and pointing. We use a wide range of embodied and general visual reasoning datasets as sources to construct a large-scale dataset, Embodied-Points-200K, which supports key embodied pointing capabilities. We then train Embodied-R1 using a two-stage Reinforced Fine-tuning (RFT) curriculum with a specialized multi-task reward design. Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and pointing benchmarks. Critically, it demonstrates robust zero-shot generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5% across 8 real-world XArm tasks without any task-specific fine-tuning, representing a 62% improvement over strong baselines. Furthermore, the model exhibits high robustness against diverse visual disturbances. Our work shows that a pointing-centric representation, combined with an RFT training paradigm, offers an effective and generalizable pathway to closing the perception-action gap in robotics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Embodied-R1æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥ç»Ÿä¸€ä¸”ä¸å½¢æ€æ— å…³çš„ä¸­é—´è¡¨ç¤ºpointingï¼Œè§£å†³å…·èº«æ™ºèƒ½(Embodied AI)ä¸­å­˜åœ¨çš„seeing-to-doingé¸¿æ²Ÿã€‚ä½œè€…å®šä¹‰äº†å››é¡¹æ ¸å¿ƒå…·èº«æŒ‡å‘èƒ½åŠ›ä»¥è¿æ¥é«˜å±‚è§†è§‰è¯­è¨€ç†è§£ä¸åº•å±‚action primitivesï¼Œå¹¶åŸºäºæ­¤æ„å»ºäº†å¤§è§„æ¨¡æ•°æ®é›†Embodied-Points-200Kã€‚Embodied-R1ä½œä¸ºä¸€ä¸ª3Bè§„æ¨¡çš„Vision-Language Model (VLM)ï¼Œé€šè¿‡ä¸¤é˜¶æ®µReinforced Fine-tuning (RFT)å’Œä¸“é—¨çš„å¤šä»»åŠ¡å¥–åŠ±è®¾è®¡è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨11ä¸ªå…·èº«ç©ºé—´å’ŒæŒ‡å‘åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°SOTAæ°´å¹³ã€‚åœ¨æ— éœ€ç‰¹å®šä»»åŠ¡å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œå…¶åœ¨SIMPLEREnvå’ŒçœŸå®ä¸–ç•ŒXArmä»»åŠ¡ä¸­åˆ†åˆ«å±•ç°å‡º56.2%å’Œ87.5%çš„é›¶æ ·æœ¬æ³›åŒ–(Zero-shot Generalization)æˆåŠŸç‡ï¼Œè¾ƒå¼ºåŸºçº¿æå‡äº†62%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å¯¹å„ç±»è§†è§‰å¹²æ‰°è¡¨ç°å‡ºæé«˜çš„é²æ£’æ€§ï¼Œè¯æ˜äº†ä»¥pointingä¸ºä¸­å¿ƒçš„è¡¨ç¤ºç»“åˆRFTè®­ç»ƒèŒƒå¼æ˜¯å®ç°é€šç”¨æœºå™¨äººæ“æ§çš„æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Embodied-R1 technical report",
      "pdf_url": "https://arxiv.org/pdf/2508.13998v1",
      "published_date": "2025-08-19 16:50:01 UTC",
      "updated_date": "2025-08-19 16:50:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:44:03.058272+00:00"
    },
    {
      "arxiv_id": "2508.13993v1",
      "title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization",
      "title_zh": "ä»¥å—ä¸ºè‡‚ï¼šé¢å‘é•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹åå¥½ä¼˜åŒ–çš„å¤šè‡‚è€è™æœºå¼•å¯¼é‡‡æ ·",
      "authors": [
        "Shaohua Duan",
        "Xinze Li",
        "Zhenghao Liu",
        "Xiaoyuan Yi",
        "Yukun Yan",
        "Shuo Wang",
        "Yu Gu",
        "Ge Yu",
        "Maosong Sun"
      ],
      "abstract": "Long-context modeling is critical for a wide range of real-world tasks, including long-context question answering, summarization, and complex reasoning tasks. Recent studies have explored fine-tuning Large Language Models (LLMs) with synthetic data to enhance their long-context capabilities. However, the effectiveness of such approaches is often limited by the low diversity and factual inconsistencies in the generated data. To address these challenges, we propose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB) rollout strategy to identify the most informative chunks from the given long context for sampling high-quality and diverse responses and constructing preference data pairs for Direct Preference Optimization (DPO) training. Specifically, we treat context chunks as arms of MAB, select chunks based on their expected reward scores to input into LLMs to generate responses, and iteratively update these scores based on reward feedback. This exploration and exploitation process enables the model to focus on the most relevant context segments, thereby generating and collecting high-quality and diverse responses. Finally, we collect these generated responses from the rollout process and apply the DPO method to further optimize the LLM. Experimental results show that LongMab-PO significantly improves the diversity and quality of preference data pairs, achieving state-of-the-art performance on long-context reasoning benchmarks. All code and data will be released on https://github.com/NEUIR/LongMab-PO.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LongMab-PO æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é•¿æ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åˆæˆæ•°æ®å¾®è°ƒè¿‡ç¨‹ä¸­é¢ä¸´çš„å¤šæ ·æ€§ä½å’Œäº‹å®ä¸ä¸€è‡´ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å¤šè‡‚è€è™æœº(Multi-Armed Bandit) ç­–ç•¥ï¼Œå°†ä¸Šä¸‹æ–‡æ–‡æœ¬å—(chunks) è§†ä¸ºâ€œè‡‚â€ï¼Œæ ¹æ®é¢„æœŸå¥–åŠ±åˆ†æ•°é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„ç‰‡æ®µä»¥ç”Ÿæˆå“åº”ï¼Œå¹¶æ ¹æ®å¥–åŠ±åé¦ˆè¿­ä»£æ›´æ–°åˆ†æ•°ã€‚è¿™ç§æ¢ç´¢ä¸åˆ©ç”¨(exploration and exploitation) æœºåˆ¶ä½¿æ¨¡å‹èƒ½å¤Ÿèšç„¦äºæœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œé‡‡é›†é«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„å“åº”æ¥æ„å»ºåå¥½æ•°æ®å¯¹ã€‚éšåï¼Œç ”ç©¶é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–(Direct Preference Optimization) æ–¹æ³•å¯¹æ¨¡å‹è¿›è¡Œè¿›ä¸€æ­¥ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLongMab-PO æ˜¾è‘—æ”¹å–„äº†åå¥½æ•°æ®çš„å¤šæ ·æ€§ä¸è´¨é‡ï¼Œåœ¨é•¿æ–‡æœ¬æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›(state-of-the-art) çš„æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13993v1",
      "published_date": "2025-08-19 16:33:55 UTC",
      "updated_date": "2025-08-19 16:33:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:44:01.757017+00:00"
    },
    {
      "arxiv_id": "2508.13982v1",
      "title": "The Social Context of Human-Robot Interactions",
      "title_zh": "äººæœºäº¤äº’ä¸­çš„ç¤¾ä¼šèƒŒæ™¯",
      "authors": [
        "Sydney Thompson",
        "Kate Candon",
        "Marynel VÃ¡zquez"
      ],
      "abstract": "The Human-Robot Interaction (HRI) community often highlights the social context of an interaction as a key consideration when designing, implementing, and evaluating robot behavior. Unfortunately, researchers use the term \"social context\" in varied ways. This can lead to miscommunication, making it challenging to draw connections between related work on understanding and modeling the social contexts of human-robot interactions. To address this gap, we survey the HRI literature for existing definitions and uses of the term \"social context\". Then, we propose a conceptual model for describing the social context of a human-robot interaction. We apply this model to existing work, and we discuss a range of attributes of social contexts that can help researchers plan for interactions, develop behavior models for robots, and gain insights after interactions have taken place. We conclude with a discussion of open research questions in relation to understanding and modeling the social contexts of human-robot interactions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Human-Robot Interaction (HRI) é¢†åŸŸä¸­ social context å®šä¹‰ä¸ä¸€è‡´å¯¼è‡´çš„æ²Ÿé€šéšœç¢ï¼Œç³»ç»Ÿæ€§åœ°è°ƒç ”äº†ç›¸å…³æ–‡çŒ®ä¸­çš„æœ¯è¯­å®šä¹‰ä¸ç”¨æ³•ã€‚ä¸ºå¼¥åˆè¿™ä¸€è®¤çŸ¥é¸¿æ²Ÿï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªç”¨äºæè¿°äººæœºäº¤äº’ç¤¾äº¤èƒŒæ™¯çš„å…¨æ–° conceptual modelï¼Œå¹¶å°†å…¶åº”ç”¨äºå¯¹ç°æœ‰å·¥ä½œçš„åˆ†æä¸­ã€‚æ–‡ç« æ·±å…¥æ¢è®¨äº† social context çš„å¤šç§å±æ€§ï¼Œæ—¨åœ¨è¾…åŠ©ç ”ç©¶äººå‘˜æ›´æœ‰æ•ˆåœ°è§„åˆ’äº¤äº’ã€å¼€å‘æœºå™¨äººçš„ behavior models å¹¶è·å–äº¤äº’åçš„æ´å¯Ÿã€‚è¯¥ç ”ç©¶æœ€åæ€»ç»“äº†ç†è§£ä¸å»ºæ¨¡ç¤¾äº¤èƒŒæ™¯æ–¹é¢çš„å¼€æ”¾æ€§ç ”ç©¶é—®é¢˜ï¼Œä¸ºæœªæ¥ HRI é¢†åŸŸçš„ç ”ç©¶åä½œä¸ç³»ç»Ÿå¼€å‘æä¾›äº†æ ‡å‡†åŒ–çš„ç†è®ºæ¡†æ¶ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "To be published in Annual Review of Control, Robotics, and Autonomous Systems",
      "pdf_url": "https://arxiv.org/pdf/2508.13982v1",
      "published_date": "2025-08-19 16:15:58 UTC",
      "updated_date": "2025-08-19 16:15:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:44:10.080931+00:00"
    },
    {
      "arxiv_id": "2508.13975v1",
      "title": "ChronoLLM: Customizing Language Models for Physics-Based Simulation Code Generation",
      "title_zh": "ChronoLLMï¼šé¢å‘ç‰©ç†ä»¿çœŸä»£ç ç”Ÿæˆçš„è¯­è¨€æ¨¡å‹å®šåˆ¶åŒ–",
      "authors": [
        "Jingquan Wang",
        "Andrew Negrut",
        "Harry Zhang",
        "Khailanii Slaton",
        "Shu Wang",
        "Radu Serban",
        "Jinlong Wu",
        "Dan Negrut"
      ],
      "abstract": "This contribution is concerned with the following issue: can pretrained large language models (LLMs) be refined and customized to the point where they become virtual assistants helping experts with the effective use of a simulation tool? In this case study, the ``simulation tool'' considered is PyChrono, an open source multi-physics dynamics engine for multibody systems. We present a framework for refining and customizing both open- and closed-source LLMs to harness the power of AI in generating scripts that perform PyChrono virtual experiments. We refine and customize several classes of LLMs through a process that leads to a quantifiable improvement in the quality of the generated PyChrono simulation scripts. These scripts can range from simple single-pendulum simulations to complex virtual experiments involving full vehicles on deformable terrain. While the generated scripts are rarely perfect, they often serve as strong starting points for the user to modify and improve on. Additionally, the LLM can answer specific API questions about the simulator, or recommend modeling approaches. The framework discussed is general and can be applied to lower the entry barrier for simulation tools associated with other application domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é¢„è®­ç»ƒ Large Language Models (LLMs) æ˜¯å¦èƒ½é€šè¿‡å¾®è°ƒå’Œå®šåˆ¶åŒ–æˆä¸ºè¾…åŠ©ä¸“å®¶é«˜æ•ˆä½¿ç”¨æ¨¡æ‹Ÿå·¥å…·çš„è™šæ‹ŸåŠ©æ‰‹ï¼Œå¹¶é’ˆå¯¹å¼€æºå¤šç‰©ç†åœºåŠ¨åŠ›å­¦å¼•æ“ PyChrono æå‡ºäº† ChronoLLM å®šåˆ¶åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹å¼€æºå’Œé—­æº LLMs è¿›è¡Œä¼˜åŒ–ï¼Œå®ç°äº†ç”Ÿæˆ PyChrono æ¨¡æ‹Ÿè„šæœ¬è´¨é‡çš„å¯é‡åŒ–æå‡ã€‚å®šåˆ¶åçš„æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆä»åŸºç¡€å•æ‘†åˆ°å¤æ‚è½¦è¾†ä¸å˜å½¢åœ°å½¢äº¤äº’çš„å„ç±»è™šæ‹Ÿå®éªŒä»£ç ã€‚è™½ç„¶ç”Ÿæˆçš„è„šæœ¬å°šä¸èƒ½å®Œå…¨ä¿è¯å®Œç¾ï¼Œä½†å·²èƒ½ä½œä¸ºä¼˜ç§€çš„èµ·å§‹ç‚¹ä¾›ç”¨æˆ·ä¿®æ”¹ï¼Œä¸”æ¨¡å‹èƒ½å¤Ÿå›ç­”ç‰¹å®šçš„ API é—®é¢˜å¹¶æ¨èå»ºæ¨¡æ–¹æ³•ã€‚è¯¥ç ”ç©¶å±•ç¤ºçš„æ¡†æ¶å…·æœ‰é€šç”¨æ€§ï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½å…¶ä»–é¢†åŸŸå¤æ‚ä»¿çœŸå·¥å…·çš„å­¦ä¹ å’Œä½¿ç”¨é—¨æ§›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13975v1",
      "published_date": "2025-08-19 16:12:51 UTC",
      "updated_date": "2025-08-19 16:12:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:44:10.666060+00:00"
    },
    {
      "arxiv_id": "2508.13968v2",
      "title": "RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation",
      "title_zh": "RotBenchï¼šè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¯†åˆ«å›¾åƒæ—‹è½¬çš„èƒ½åŠ›",
      "authors": [
        "Tianyi Niu",
        "Jaemin Cho",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "abstract": "We investigate to what extent Multimodal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0Â°, 90Â°, 180Â°, and 270Â°. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce RotBench -- a 350-image manually-filtered benchmark comprising lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information -- including captions, depth maps, and more -- or using chain-of-thought prompting offers only small and inconsistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0Â°) images, while certain models are able to identify upside-down (180Â°) images. None can reliably distinguish between 90Â° and 270Â°. Simultaneously showing the image rotated in different orientations leads to moderate performance gains for reasoning models, while a modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve models' ability to distinguish 90Â° and 270Â° rotations, despite substantially improving the identification of 180Â° images. Together, these results reveal a significant gap between MLLMs' spatial reasoning capabilities and human perception in identifying rotation.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†åä¸º RotBench çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Multimodal Large Language Models, MLLMs) è¯†åˆ«å›¾åƒæ—‹è½¬è§’åº¦ï¼ˆ0Â°ã€90Â°ã€180Â°ã€270Â°ï¼‰çš„èƒ½åŠ›ã€‚RotBench åŒ…å« 350 å¼ æ¶µç›–ç”Ÿæ´»ã€è‚–åƒå’Œé£æ™¯çš„äººå·¥ç­›é€‰å›¾åƒï¼Œé‡ç‚¹è€ƒå¯Ÿæ¨¡å‹æ£€æµ‹æ—‹è½¬çº¿ç´¢å’Œç†è§£ç©ºé—´å…³ç³»çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä¾¿å¦‚ GPT-5ã€o3 å’Œ Gemini-2.5-Pro ç­‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œåœ¨è¯†åˆ«å›¾åƒæ—‹è½¬æ–¹é¢ä¾ç„¶ä¸å¯é ï¼Œä¸”æä¾› captions æˆ–æ·±åº¦å›¾ç­‰è¾…åŠ©ä¿¡æ¯ä»¥åŠä½¿ç”¨ chain-of-thought æç¤ºä¹Ÿæ”¶æ•ˆç”šå¾®ã€‚å¤§å¤šæ•°æ¨¡å‹èƒ½è¯†åˆ« 0Â° å›¾åƒï¼Œéƒ¨åˆ†å¯è¯†åˆ« 180Â° å›¾åƒï¼Œä½†å‡æ— æ³•æœ‰æ•ˆåŒºåˆ† 90Â° å’Œ 270Â° æ—‹è½¬ã€‚è™½ç„¶é‡‡ç”¨å¤šå‘å±•ç¤ºå’Œ voting æœºåˆ¶èƒ½å¸¦æ¥ä¸€å®šæ”¹è¿›ï¼Œä½† fine-tuning ä¾ç„¶æ— æ³•è§£å†³ 90Â° ä¸ 270Â° çš„è¾¨è¯†éš¾é¢˜ã€‚è¯¥ç ”ç©¶æœ€ç»ˆæ­ç¤ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ¨ç†èƒ½åŠ›ä¸äººç±»æ„ŸçŸ¥ä¹‹é—´å­˜åœ¨çš„æ˜¾è‘—å·®è·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages. Code and data: https://github.com/tianyiniu/RotBench",
      "pdf_url": "https://arxiv.org/pdf/2508.13968v2",
      "published_date": "2025-08-19 15:58:25 UTC",
      "updated_date": "2025-08-20 17:53:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:44:17.249602+00:00"
    },
    {
      "arxiv_id": "2508.13962v2",
      "title": "Learning to Use AI for Learning: Teaching Responsible Use of AI Chatbot to K-12 Students Through an AI Literacy Module",
      "title_zh": "å­¦ä¹ åˆ©ç”¨ AI è¿›è¡Œå­¦ä¹ ï¼šé€šè¿‡ AI ç´ å…»æ¨¡å—åŸ¹å…» K-12 å­¦ç”Ÿè´Ÿè´£ä»»åœ°ä½¿ç”¨ AI èŠå¤©æœºå™¨äºº",
      "authors": [
        "Ruiwei Xiao",
        "Xinying Hou",
        "Ying-Jui Tseng",
        "Hsuan Nieu",
        "Guanze Liao",
        "John Stamper",
        "Kenneth R. Koedinger"
      ],
      "abstract": "As Artificial Intelligence (AI) becomes increasingly integrated into daily life, there is a growing need to equip the next generation with the ability to apply, interact with, evaluate, and collaborate with AI systems responsibly. Prior research highlights the urgent demand from K-12 educators to teach students the ethical and effective use of AI for learning. To address this need, we designed an Large-Language Model (LLM)-based module to teach prompting literacy. This includes scenario-based deliberate practice activities with direct interaction with intelligent LLM agents, aiming to foster secondary school students' responsible engagement with AI chatbots. We conducted two iterations of classroom deployment in 11 authentic secondary education classrooms, and evaluated 1) AI-based auto-grader's capability; 2) students' prompting performance and confidence changes towards using AI for learning; and 3) the quality of learning and assessment materials. Results indicated that the AI-based auto-grader could grade student-written prompts with satisfactory quality. In addition, the instructional materials supported students in improving their prompting skills through practice and led to positive shifts in their perceptions of using AI for learning. Furthermore, data from Study 1 informed assessment revisions in Study 2. Analyses of item difficulty and discrimination in Study 2 showed that True/False and open-ended questions could measure prompting literacy more effectively than multiple-choice questions for our target learners. These promising outcomes highlight the potential for broader deployment and highlight the need for broader studies to assess learning effectiveness and assessment design.",
      "tldr_zh": "è¯¥ç ”ç©¶è®¾è®¡äº†ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(Large-Language Model, LLM)çš„æ•™å­¦æ¨¡å—ï¼Œæ—¨åœ¨åŸ¹å…»K-12å­¦ç”Ÿçš„æé—®ç´ å…»(prompting literacy)åŠè´Ÿè´£ä»»ä½¿ç”¨AIèŠå¤©æœºå™¨äººçš„èƒ½åŠ›ã€‚è¯¥æ¨¡å—åŒ…å«åŸºäºåœºæ™¯çš„åˆ»æ„ç»ƒä¹ æ´»åŠ¨ï¼Œé€šè¿‡ä¸æ™ºèƒ½LLMæ™ºèƒ½ä½“ç›´æ¥äº¤äº’æ¥ä¿ƒè¿›å­¦ç”Ÿå¯¹AIçš„æ·±å…¥ç†è§£ã€‚ç ”ç©¶åœ¨11ä¸ªçœŸå®çš„ä¸­å­¦è¯¾å ‚è¿›è¡Œäº†ä¸¤æ¬¡è¿­ä»£éƒ¨ç½²ï¼Œé‡ç‚¹è¯„ä¼°äº†AIè‡ªåŠ¨è¯„åˆ†å™¨(auto-grader)çš„æ€§èƒ½ã€å­¦ç”Ÿçš„æç¤ºè¯è¡¨ç°åŠä¿¡å¿ƒå˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAIè¯„åˆ†å™¨èƒ½å¤Ÿæä¾›é«˜è´¨é‡çš„åé¦ˆï¼Œä¸”æ•™å­¦ææ–™æ˜¾è‘—æå‡äº†å­¦ç”Ÿçš„æé—®æŠ€å·§(prompting skills)ï¼Œå¹¶ä½¿å…¶å¯¹åˆ©ç”¨AIè¿›è¡Œå­¦ä¹ äº§ç”Ÿäº†ç§¯æçš„æ„ŸçŸ¥è½¬å˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°åˆ¤æ–­é¢˜å’Œå¼€æ”¾å¼é—®é¢˜åœ¨è¡¡é‡æé—®ç´ å…»æ–¹é¢æ¯”å¤šé¡¹é€‰æ‹©é¢˜æ›´å…·æœ‰æ•ˆæ€§å’ŒåŒºåˆ†åº¦ã€‚è¿™äº›æˆæœä¸ä»…éªŒè¯äº†è¯¥æ¨¡å—åœ¨å¤§è§„æ¨¡æ•™å­¦ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œä¹Ÿä¸ºæœªæ¥AIç´ å…»æ•™è‚²çš„è¯¾ç¨‹è®¾è®¡å’Œè¯„ä¼°æ–¹æ³•æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted to AAAI2026",
      "pdf_url": "https://arxiv.org/pdf/2508.13962v2",
      "published_date": "2025-08-19 15:54:51 UTC",
      "updated_date": "2025-12-07 04:30:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:44:19.854641+00:00"
    },
    {
      "arxiv_id": "2508.13960v1",
      "title": "A Mechanism for Mutual Fairness in Cooperative Games with Replicable Resources -- Extended Version",
      "title_zh": "å…·æœ‰å¯å¤åˆ¶èµ„æºçš„åˆä½œåšå¼ˆä¸­çš„äº’æƒ å…¬å¹³æœºåˆ¶â€”â€”æ‰©å±•ç‰ˆ",
      "authors": [
        "BjÃ¶rn Filter",
        "Ralf MÃ¶ller",
        "Ã–zgÃ¼r LÃ¼tfÃ¼ Ã–zÃ§ep"
      ],
      "abstract": "The latest developments in AI focus on agentic systems where artificial and human agents cooperate to realize global goals. An example is collaborative learning, which aims to train a global model based on data from individual agents. A major challenge in designing such systems is to guarantee safety and alignment with human values, particularly a fair distribution of rewards upon achieving the global goal. Cooperative game theory offers useful abstractions of cooperating agents via value functions, which assign value to each coalition, and via reward functions. With these, the idea of fair allocation can be formalized by specifying fairness axioms and designing concrete mechanisms. Classical cooperative game theory, exemplified by the Shapley value, does not fully capture scenarios like collaborative learning, as it assumes nonreplicable resources, whereas data and models can be replicated. Infinite replicability requires a generalized notion of fairness, formalized through new axioms and mechanisms. These must address imbalances in reciprocal benefits among participants, which can lead to strategic exploitation and unfair allocations. The main contribution of this paper is a mechanism and a proof that it fulfills the property of mutual fairness, formalized by the Balanced Reciprocity Axiom. It ensures that, for every pair of players, each benefits equally from the participation of the other.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)ä»£ç†ç³»ç»Ÿï¼ˆå¦‚åä½œå­¦ä¹ ï¼‰ä¸­çš„åˆä½œåšå¼ˆç†è®ºï¼Œç‰¹åˆ«å…³æ³¨åœ¨å¯å¤åˆ¶èµ„æº(Replicable Resources)ç¯å¢ƒä¸‹çš„å…¬å¹³åˆ†é…é—®é¢˜ã€‚ä¼ ç»Ÿçš„åˆä½œåšå¼ˆç†è®ºï¼ˆå¦‚Shapley valueï¼‰é€šå¸¸å‡è®¾èµ„æºä¸å¯å¤åˆ¶ï¼Œä½†åœ¨æ•°æ®å’Œæ¨¡å‹å¯æ— é™å¤åˆ¶çš„åœºæ™¯ä¸‹ï¼Œè¿™äº›ç»å…¸æ¨¡å‹æ— æ³•æœ‰æ•ˆå¤„ç†å‚ä¸è€…ä¹‹é—´äº’æƒ ä¸å¹³è¡¡åŠæˆ˜ç•¥å‰¥å‰Šçš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¯å¤åˆ¶èµ„æºè®¾è®¡çš„å¹¿ä¹‰å…¬å¹³æ€§æ¦‚å¿µï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªå…¨æ–°çš„åˆ†é…æœºåˆ¶ã€‚è¯¥ç ”ç©¶çš„ä¸»è¦è´¡çŒ®åœ¨äºè¯æ˜äº†è¯¥æœºåˆ¶æ»¡è¶³äº’æƒ å…¬å¹³æ€§(Mutual Fairness)ï¼Œå¹¶å°†å…¶å½¢å¼åŒ–ä¸ºå‡è¡¡äº’æƒ å…¬ç†(Balanced Reciprocity Axiom)ã€‚è¯¥æœºåˆ¶é€šè¿‡ç¡®ä¿åœ¨ä»»ä½•ä¸€å¯¹å‚ä¸è€…ä¹‹é—´ï¼Œæ¯ä¸€æ–¹éƒ½èƒ½ä»å¯¹æ–¹çš„å‚ä¸ä¸­è·å¾—ç›¸ç­‰çš„æ”¶ç›Šï¼Œä»è€Œä¸ºå®ç°å®‰å…¨ä¸”ç¬¦åˆäººç±»ä»·å€¼çš„åä½œç³»ç»Ÿæä¾›äº†ç†è®ºä¿éšœã€‚",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "This paper is the extended version of a paper accepted at the European Conference on Artificial Intelligence 2025 (ECAI 2025), providing the proof of the main theorem in the appendix",
      "pdf_url": "https://arxiv.org/pdf/2508.13960v1",
      "published_date": "2025-08-19 15:53:34 UTC",
      "updated_date": "2025-08-19 15:53:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:44:20.948571+00:00"
    },
    {
      "arxiv_id": "2508.14927v1",
      "title": "AI Testing Should Account for Sophisticated Strategic Behaviour",
      "title_zh": "AI æµ‹è¯•åº”å……åˆ†è€ƒé‡å¤æ‚çš„ç­–ç•¥æ€§è¡Œä¸º",
      "authors": [
        "Vojtech Kovarik",
        "Eric Olav Chen",
        "Sami Petersen",
        "Alexis Ghersengorin",
        "Vincent Conitzer"
      ],
      "abstract": "This position paper argues for two claims regarding AI testing and evaluation. First, to remain informative about deployment behaviour, evaluations need account for the possibility that AI systems understand their circumstances and reason strategically. Second, game-theoretic analysis can inform evaluation design by formalising and scrutinising the reasoning in evaluation-based safety cases. Drawing on examples from existing AI systems, a review of relevant research, and formal strategic analysis of a stylised evaluation scenario, we present evidence for these claims and motivate several research directions.",
      "tldr_zh": "è¯¥ç«‹åœºè®ºæ–‡é’ˆå¯¹ AI æµ‹è¯•ä¸è¯„ä¼°æå‡ºäº†ä¸¤ä¸ªæ ¸å¿ƒä¸»å¼ ï¼Œå¼ºè°ƒè¯„ä¼°è¿‡ç¨‹å¿…é¡»è€ƒè™‘ AI ç³»ç»Ÿçš„å¤æ‚æˆ˜ç•¥è¡Œä¸ºã€‚é¦–å…ˆï¼Œä¸ºäº†ä½¿è¯„ä¼°ç»“æœå¯¹å®é™…éƒ¨ç½²è¡Œä¸ºå…·æœ‰å‚è€ƒä»·å€¼ï¼Œè¯„ä¼°è®¾è®¡éœ€è¦è€ƒè™‘åˆ° AI ç³»ç»Ÿå¯èƒ½ç†è§£å…¶æ‰€å¤„ç¯å¢ƒå¹¶è¿›è¡Œ strategic reasoning çš„å¯èƒ½æ€§ã€‚å…¶æ¬¡ï¼Œç ”ç©¶æå‡ºåšå¼ˆè®ºåˆ†æï¼ˆGame-theoretic analysisï¼‰å¯ä»¥é€šè¿‡å½¢å¼åŒ–å’Œå®¡æŸ¥åŸºäºè¯„ä¼°çš„ safety cases ä¸­çš„æ¨ç†è¿‡ç¨‹ï¼Œä¸ºè¯„ä¼°è®¾è®¡æä¾›æœ‰æ•ˆæŒ‡å¯¼ã€‚é€šè¿‡ç»“åˆç°æœ‰ AI ç³»ç»Ÿæ¡ˆä¾‹ã€ç›¸å…³ç ”ç©¶ç»¼è¿°ä»¥åŠå¯¹ç‰¹å®šè¯„ä¼°åœºæ™¯çš„æ­£å¼æˆ˜ç•¥åˆ†æï¼Œè¯¥ç ”ç©¶ä¸ºä¸Šè¿°ä¸»å¼ æä¾›äº†è¯æ®æ”¯æŒã€‚æœ€åï¼Œè®ºæ–‡æŒ‡æ˜äº†è‹¥å¹²ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨æå‡ AI å®‰å…¨è¯„ä¼°åœ¨åº”å¯¹é«˜çº§æˆ˜ç•¥è¡Œä¸ºæŒ‘æˆ˜æ—¶çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14927v1",
      "published_date": "2025-08-19 15:48:25 UTC",
      "updated_date": "2025-08-19 15:48:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:44:42.251046+00:00"
    },
    {
      "arxiv_id": "2508.13948v1",
      "title": "Prompt Orchestration Markup Language",
      "title_zh": "æç¤ºè¯ç¼–æ’æ ‡è®°è¯­è¨€",
      "authors": [
        "Yuge Zhang",
        "Nan Chen",
        "Jiahang Xu",
        "Yuqing Yang"
      ],
      "abstract": "Large Language Models (LLMs) require sophisticated prompting, yet current practices face challenges in structure, data integration, format sensitivity, and tooling. Existing methods lack comprehensive solutions for organizing complex prompts involving diverse data types (documents, tables, images) or managing presentation variations systematically. To address these gaps, we introduce POML (Prompt Orchestration Markup Language). POML employs component-based markup for logical structure (roles, tasks, examples), specialized tags for seamless data integration, and a CSS-like styling system to decouple content from presentation, reducing formatting sensitivity. It includes templating for dynamic prompts and a comprehensive developer toolkit (IDE support, SDKs) to improve version control and collaboration. We validate POML through two case studies demonstrating its impact on complex application integration (PomLink) and accuracy performance (TableQA), as well as a user study assessing its effectiveness in real-world development scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†POMLï¼ˆPrompt Orchestration Markup Languageï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æç¤ºè¯å·¥ç¨‹ä¸­é¢ä¸´çš„é€»è¾‘ç»“æ„æ··ä¹±ã€å¤šæ¨¡æ€æ•°æ®é›†æˆå›°éš¾åŠå¯¹è¡¨ç°æ ¼å¼é«˜åº¦æ•æ„Ÿç­‰æŒ‘æˆ˜ã€‚POMLé‡‡ç”¨åŸºäºç»„ä»¶çš„æ ‡è®°æ–¹å¼æ„å»ºé€»è¾‘ç»“æ„ï¼ˆå¦‚rolesã€taskså’Œexamplesï¼‰ï¼Œå¹¶åˆ©ç”¨ä¸“ç”¨æ ‡ç­¾å®ç°æ–‡æ¡£ã€è¡¨æ ¼å’Œå›¾åƒç­‰å¤šæ ·åŒ–æ•°æ®çš„æ— ç¼é›†æˆã€‚è¯¥ç³»ç»Ÿåˆ›æ–°æ€§åœ°å¼•å…¥äº†ç±»CSSçš„æ ·å¼ç³»ç»Ÿï¼Œé€šè¿‡å°†æç¤ºè¯å†…å®¹ä¸è¡¨ç°å½¢å¼è§£è€¦ï¼Œæœ‰æ•ˆé™ä½äº†æ¨¡å‹å¯¹æ ¼å¼å˜åŒ–çš„æ•æ„Ÿæ€§ã€‚æ­¤å¤–ï¼ŒPOMLè¿˜æä¾›äº†åŒ…å«IDEæ”¯æŒå’ŒSDKsçš„å®Œæ•´å¼€å‘å·¥å…·åŒ…ï¼Œæ”¯æŒåŠ¨æ€æç¤ºè¯æ¨¡æ¿åŒ–ä¸ç‰ˆæœ¬åä½œã€‚é€šè¿‡PomLinkå’ŒTableQAçš„æ¡ˆä¾‹ç ”ç©¶ä»¥åŠç”¨æˆ·è°ƒç ”ï¼Œç ”ç©¶è¯æ˜äº†POMLåœ¨å¤„ç†å¤æ‚åº”ç”¨é›†æˆæ—¶çš„ä¼˜è¶Šæ€§ï¼Œå¹¶æ˜¾è‘—æå‡äº†ç›¸å…³ä»»åŠ¡çš„å‡†ç¡®ç‡è¡¨ç°ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.PL"
      ],
      "primary_category": "cs.HC",
      "comment": "All findings in this paper are derived from a POML snapshot as of February 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.13948v1",
      "published_date": "2025-08-19 15:37:29 UTC",
      "updated_date": "2025-08-19 15:37:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:44:38.163804+00:00"
    },
    {
      "arxiv_id": "2508.13942v1",
      "title": "The Collaboration Paradox: Why Generative AI Requires Both Strategic Intelligence and Operational Stability in Supply Chain Management",
      "title_zh": "åä½œæ‚–è®ºï¼šä¸ºä½•ç”Ÿæˆå¼ AI åœ¨ä¾›åº”é“¾ç®¡ç†ä¸­éœ€å…¼é¡¾æˆ˜ç•¥æ™ºèƒ½ä¸è¿è¥ç¨³å®šæ€§",
      "authors": [
        "Soumyadeep Dhar"
      ],
      "abstract": "The rise of autonomous, AI-driven agents in economic settings raises critical questions about their emergent strategic behavior. This paper investigates these dynamics in the cooperative context of a multi-echelon supply chain, a system famously prone to instabilities like the bullwhip effect. We conduct computational experiments with generative AI agents, powered by Large Language Models (LLMs), within a controlled supply chain simulation designed to isolate their behavioral tendencies. Our central finding is the \"collaboration paradox\": a novel, catastrophic failure mode where theoretically superior collaborative AI agents, designed with Vendor-Managed Inventory (VMI) principles, perform even worse than non-AI baselines. We demonstrate that this paradox arises from an operational flaw where agents hoard inventory, starving the system. We then show that resilience is only achieved through a synthesis of two distinct layers: high-level, AI-driven proactive policy-setting to establish robust operational targets, and a low-level, collaborative execution protocol with proactive downstream replenishment to maintain stability. Our final framework, which implements this synthesis, can autonomously generate, evaluate, and quantify a portfolio of viable strategic choices. The work provides a crucial insight into the emergent behaviors of collaborative AI agents and offers a blueprint for designing stable, effective AI-driven systems for business analytics.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åœ¨å¤šçº§ä¾›åº”é“¾(multi-echelon supply chain)åˆä½œèƒŒæ™¯ä¸‹ï¼Œç”±å¤§è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨çš„ç”Ÿæˆå¼AIæ™ºèƒ½ä½“çš„æ¶Œç°ç­–ç•¥è¡Œä¸ºã€‚ç ”ç©¶é€šè¿‡å—æ§ä»¿çœŸå®éªŒå‘ç°äº†ä¸€ä¸ªè¢«ç§°ä¸ºâ€œåä½œæ‚–è®ºâ€(collaboration paradox)çš„æ–°å‹ç¾éš¾æ€§å¤±è´¥æ¨¡å¼ï¼Œå³åŸºäºä¾›åº”å•†ç®¡ç†åº“å­˜(VMI)åŸåˆ™è®¾è®¡çš„AIæ™ºèƒ½ä½“åœ¨æ€§èƒ½ä¸Šç«Ÿç„¶åŠ£äºéAIåŸºå‡†æ¨¡å‹ã€‚è¿™ç§æ‚–è®ºæºäºæ“ä½œå±‚é¢çš„ç¼ºé™·ï¼Œå³æ™ºèƒ½ä½“å€¾å‘äºå›¤ç§¯åº“å­˜ï¼Œä»è€Œå¯¼è‡´ç³»ç»Ÿæ•´ä½“ç˜«ç—ªã€‚ä¸ºæå‡ç³»ç»ŸéŸ§æ€§ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆåŒå±‚æ¶æ„çš„åˆæˆæ¡†æ¶ï¼šé«˜å±‚åˆ©ç”¨AIé©±åŠ¨çš„ä¸»åŠ¨ç­–ç•¥è®¾å®š(proactive policy-setting)å»ºç«‹ç¨³å¥çš„æ“ä½œç›®æ ‡ï¼Œåº•å±‚é€šè¿‡åä½œæ‰§è¡Œåè®®(collaborative execution protocol)è¿›è¡Œä¸»åŠ¨ä¸‹æ¸¸è¡¥è´§ä»¥ç»´æŒç¨³å®šæ€§ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆã€è¯„ä¼°å¹¶é‡åŒ–ä¸€ç»„å¯è¡Œçš„æˆ˜ç•¥é€‰æ‹©ç»„åˆã€‚æ­¤é¡¹å·¥ä½œæ·±åˆ»æ­ç¤ºäº†åä½œå‹AIæ™ºèƒ½ä½“åœ¨å¤æ‚ç»æµç¯å¢ƒä¸­çš„æ¶Œç°è¡Œä¸ºï¼Œå¹¶ä¸ºæ„å»ºç¨³å®šã€é«˜æ•ˆçš„ä¸šåŠ¡åˆ†æ(business analytics)é©±åŠ¨å‹AIç³»ç»Ÿæä¾›äº†è®¾è®¡è“å›¾ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13942v1",
      "published_date": "2025-08-19 15:31:23 UTC",
      "updated_date": "2025-08-19 15:31:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:44:43.048936+00:00"
    },
    {
      "arxiv_id": "2508.13930v1",
      "title": "InPars+: Supercharging Synthetic Data Generation for Information Retrieval Systems",
      "title_zh": "InPars+ï¼šæ·±åº¦å¢å¼ºä¿¡æ¯æ£€ç´¢ç³»ç»Ÿçš„åˆæˆæ•°æ®ç”Ÿæˆ",
      "authors": [
        "Matey Krastev",
        "Miklos Hamar",
        "Danilo Toapanta",
        "Jesse Brouwers",
        "Yibin Lei"
      ],
      "abstract": "This work revisits and extends synthetic query generation pipelines for Neural Information Retrieval (NIR) by leveraging the InPars Toolkit, a reproducible, end-to-end framework for generating training data using large language models (LLMs). We first assess the reproducibility of the original InPars, InPars-V2, and Promptagator pipelines on the SciFact benchmark and validate their effectiveness using open-source reranker and generator models. Building on this foundation, we introduce two key extensions to the pipeline: (1) fine-tuning a query generator LLM via Contrastive Preference Optimization (CPO) to improve the signal quality in generated queries, and (2) replacing static prompt templates with dynamic, Chain-of-Thought (CoT) optimized prompts using the DSPy framework. Our results show that both extensions reduce the need for aggressive filtering while improving retrieval performance. All code, models, and synthetic datasets are publicly released to support further research at: \\href{https://github.com/danilotpnta/IR2-project}{this https URL}.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¥ç»ä¿¡æ¯æ£€ç´¢ (Neural Information Retrieval, NIR) ç³»ç»Ÿä¸­çš„åˆæˆæŸ¥è¯¢ç”ŸæˆæŠ€æœ¯ï¼Œå¹¶æ¨å‡ºäº† InPars+ æ¡†æ¶ã€‚åœ¨ InPars Toolkit çš„åŸºç¡€ä¸Šï¼Œç ”ç©¶è€…é¦–å…ˆéªŒè¯äº† InParsã€InPars-V2 å’Œ Promptagator åœ¨ SciFact è¯„æµ‹é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚è®ºæ–‡éšåå¼•å…¥äº†ä¸¤ä¸ªæ ¸å¿ƒæ‰©å±•ï¼šä¸€æ˜¯é€šè¿‡å¯¹æ¯”åå¥½ä¼˜åŒ– (Contrastive Preference Optimization, CPO) å¾®è°ƒæŸ¥è¯¢ç”Ÿæˆæ¨¡å‹ä»¥æå‡æŸ¥è¯¢ä¿¡å·è´¨é‡ï¼ŒäºŒæ˜¯åˆ©ç”¨ DSPy æ¡†æ¶å°†é™æ€æç¤ºæ¨¡æ¿æ›¿æ¢ä¸ºåŠ¨æ€çš„é“¾å¼æ€ç»´ (Chain-of-Thought, CoT) ä¼˜åŒ–æç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ä¸¤é¡¹æ‰©å±•åœ¨å‡å°‘å¯¹æ•°æ®è¿‡æ»¤ä¾èµ–çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†æ£€ç´¢ç³»ç»Ÿçš„æ€§èƒ½ã€‚ç›®å‰ï¼Œé¡¹ç›®ç›¸å…³çš„ä»£ç ã€æ¨¡å‹åŠåˆæˆæ•°æ®é›†å‡å·²å…¬å¼€å‘å¸ƒï¼Œä»¥æ”¯æŒåç»­ç ”ç©¶ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13930v1",
      "published_date": "2025-08-19 15:23:18 UTC",
      "updated_date": "2025-08-19 15:23:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:44:42.442588+00:00"
    },
    {
      "arxiv_id": "2508.13922v1",
      "title": "Categorical Policies: Multimodal Policy Learning and Exploration in Continuous Control",
      "title_zh": "ç±»åˆ«ç­–ç•¥ï¼šè¿ç»­æ§åˆ¶ä¸­çš„å¤šæ¨¡æ€ç­–ç•¥å­¦ä¹ ä¸æ¢ç´¢",
      "authors": [
        "SM Mazharul Islam",
        "Manfred Huber"
      ],
      "abstract": "A policy in deep reinforcement learning (RL), either deterministic or stochastic, is commonly parameterized as a Gaussian distribution alone, limiting the learned behavior to be unimodal. However, the nature of many practical decision-making problems favors a multimodal policy that facilitates robust exploration of the environment and thus to address learning challenges arising from sparse rewards, complex dynamics, or the need for strategic adaptation to varying contexts. This issue is exacerbated in continuous control domains where exploration usually takes place in the vicinity of the predicted optimal action, either through an additive Gaussian noise or the sampling process of a stochastic policy. In this paper, we introduce Categorical Policies to model multimodal behavior modes with an intermediate categorical distribution, and then generate output action that is conditioned on the sampled mode. We explore two sampling schemes that ensure differentiable discrete latent structure while maintaining efficient gradient-based optimization. By utilizing a latent categorical distribution to select the behavior mode, our approach naturally expresses multimodality while remaining fully differentiable via the sampling tricks. We evaluate our multimodal policy on a set of DeepMind Control Suite environments, demonstrating that through better exploration, our learned policies converge faster and outperform standard Gaussian policies. Our results indicate that the Categorical distribution serves as a powerful tool for structured exploration and multimodal behavior representation in continuous control.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å¼ºåŒ–å­¦ä¹ (RL)ä¸­è¿ç»­æ§åˆ¶ç­–ç•¥é€šå¸¸å—é™äºå•æ¨¡æ€(unimodal)é«˜æ–¯åˆ†å¸ƒ(Gaussian distribution)è€Œå¯¼è‡´æ¢ç´¢èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†åˆ†ç±»ç­–ç•¥(Categorical Policies)ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä¸­é—´åˆ†ç±»åˆ†å¸ƒ(categorical distribution)æ¥å»ºæ¨¡å¤šæ¨¡æ€(multimodal)è¡Œä¸ºæ¨¡å¼ï¼Œå¹¶æ ¹æ®é‡‡æ ·çš„æ¨¡å¼ç”Ÿæˆç›¸åº”çš„è¾“å‡ºåŠ¨ä½œã€‚ç ”ç©¶æ¢ç´¢äº†ä¸¤ç§é‡‡æ ·æ–¹æ¡ˆä»¥ç¡®ä¿ç¦»æ•£æ½œç»“æ„(latent structure)çš„å¯å¾®æ€§ï¼Œä»è€Œæ”¯æŒé«˜æ•ˆçš„æ¢¯åº¦ä¼˜åŒ–ã€‚é€šè¿‡åˆ©ç”¨æ½œåˆ†ç±»åˆ†å¸ƒé€‰æ‹©è¡Œä¸ºæ¨¡å¼ï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒå®Œå…¨å¯å¾®æ€§çš„åŒæ—¶å®ç°äº†è‡ªç„¶çš„ç»“æ„åŒ–æ¢ç´¢ã€‚åœ¨DeepMind Control Suiteç¯å¢ƒä¸‹çš„å®éªŒè¡¨æ˜ï¼ŒCategorical Policiesæ¯”æ ‡å‡†é«˜æ–¯ç­–ç•¥æ”¶æ•›æ›´å¿«ä¸”æ€§èƒ½æ›´ä¼˜ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†åˆ†ç±»åˆ†å¸ƒæ˜¯å¤„ç†è¿ç»­æ§åˆ¶ä¸­å¤æ‚åŠ¨åŠ›å­¦å’Œå¤šæ¨¡æ€è¡Œä¸ºè¡¨å¾çš„æœ‰æ•ˆå·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 4 figures; Has been submitted and accepted at IEEE SMC, 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.13922v1",
      "published_date": "2025-08-19 15:18:01 UTC",
      "updated_date": "2025-08-19 15:18:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:45:12.751628+00:00"
    },
    {
      "arxiv_id": "2508.13915v1",
      "title": "Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and Reflective Feedback",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹ä¸åæ€æ€§åé¦ˆçš„é‡‘èæ—¶é—´åºåˆ—å»ºæ¨¡ç»“æ„åŒ–æ™ºèƒ½ä½“å·¥ä½œæµ",
      "authors": [
        "Yihao Ang",
        "Yifan Bao",
        "Lei Jiang",
        "Jiajie Tao",
        "Anthony K. H. Tung",
        "Lukasz Szpruch",
        "Hao Ni"
      ],
      "abstract": "Time-series data is central to decision-making in financial markets, yet building high-performing, interpretable, and auditable models remains a major challenge. While Automated Machine Learning (AutoML) frameworks streamline model development, they often lack adaptability and responsiveness to domain-specific needs and evolving objectives. Concurrently, Large Language Models (LLMs) have enabled agentic systems capable of reasoning, memory management, and dynamic code generation, offering a path toward more flexible workflow automation. In this paper, we introduce \\textsf{TS-Agent}, a modular agentic framework designed to automate and enhance time-series modeling workflows for financial applications. The agent formalizes the pipeline as a structured, iterative decision process across three stages: model selection, code refinement, and fine-tuning, guided by contextual reasoning and experimental feedback. Central to our architecture is a planner agent equipped with structured knowledge banks, curated libraries of models and refinement strategies, which guide exploration, while improving interpretability and reducing error propagation. \\textsf{TS-Agent} supports adaptive learning, robust debugging, and transparent auditing, key requirements for high-stakes environments such as financial services. Empirical evaluations on diverse financial forecasting and synthetic data generation tasks demonstrate that \\textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic baselines, achieving superior accuracy, robustness, and decision traceability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TS-Agentï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºé‡‘èæ—¶é—´åºåˆ—å»ºæ¨¡(financial time-series modeling)è®¾è®¡çš„æ¨¡å—åŒ–æ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸAutoMLæ¡†æ¶åœ¨é¢†åŸŸç‰¹å®šéœ€æ±‚å’Œçµæ´»æ€§æ–¹é¢çš„å±€é™ã€‚è¯¥æ¡†æ¶å°†å»ºæ¨¡æµç¨‹å½¢å¼åŒ–ä¸ºä¸€ä¸ªç»“æ„åŒ–çš„ã€è¿­ä»£çš„å†³ç­–è¿‡ç¨‹ï¼Œæ¶µç›–äº†æ¨¡å‹é€‰æ‹©(model selection)ã€ä»£ç ä¼˜åŒ–(code refinement)å’Œå¾®è°ƒ(fine-tuning)ä¸‰ä¸ªå…³é”®é˜¶æ®µã€‚å…¶æ ¸å¿ƒæ¶æ„åŒ…å«ä¸€ä¸ªé…å¤‡äº†ç»“æ„åŒ–çŸ¥è¯†åº“(knowledge banks)çš„è§„åˆ’æ™ºèƒ½ä½“(planner agent)ï¼Œé€šè¿‡é¢„è®¾çš„æ¨¡å‹åº“å’Œä¼˜åŒ–ç­–ç•¥å¼•å¯¼æ¢ç´¢ï¼Œä»è€Œæœ‰æ•ˆæé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§å¹¶å‡å°‘è¯¯å·®ä¼ æ’­ã€‚TS-Agentæ”¯æŒè‡ªé€‚åº”å­¦ä¹ ã€é²æ£’è°ƒè¯•å’Œé€æ˜å®¡è®¡ï¼Œèƒ½å¤Ÿæ»¡è¶³é‡‘èæœåŠ¡ç­‰é«˜é£é™©ç¯å¢ƒä¸‹å¯¹å†³ç­–å¯è¿½æº¯æ€§çš„ä¸¥æ ¼è¦æ±‚ã€‚åœ¨å¤šæ ·åŒ–çš„é‡‘èé¢„æµ‹å’Œåˆæˆæ•°æ®ç”Ÿæˆä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTS-Agentåœ¨å‡†ç¡®æ€§ã€é²æ£’æ€§å’Œå†³ç­–å¯è¿½æº¯æ€§æ–¹é¢ä¸€è‡´ä¼˜äºç°æœ‰çš„AutoMLå’Œæ™ºèƒ½ä½“åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13915v1",
      "published_date": "2025-08-19 15:14:49 UTC",
      "updated_date": "2025-08-19 15:14:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:44:56.941320+00:00"
    },
    {
      "arxiv_id": "2508.13898v2",
      "title": "Fisher-Orthogonal Projection Methods for Natural Gradient Descent with Large Batches",
      "title_zh": "é’ˆå¯¹å¤§æ‰¹é‡è‡ªç„¶æ¢¯åº¦ä¸‹é™çš„ Fisher æ­£äº¤æŠ•å½±æ–¹æ³•",
      "authors": [
        "Yishun Lu",
        "Wesley Armour"
      ],
      "abstract": "Modern GPUs are equipped with large amounts of high-bandwidth memory, enabling them to support mini-batch sizes of up to tens of thousands of training samples. However, most existing optimizers struggle to perform effectively at such a large batch size. As batch size increases, gradient noise decreases due to averaging over many samples, limiting the ability of first-order methods to escape sharp or suboptimal minima and reach the global minimum. Meanwhile, second-order methods like the natural gradient with Kronecker-Factored Approximate Curvature (KFAC) often require excessively high damping to remain stable at large batch sizes. This high damping effectively washes out the curvature information that gives these methods their advantage, reducing their performance to that of simple gradient descent. In this paper, we introduce Fisher-Orthogonal Projection (FOP), a novel technique that restores the effectiveness of the second-order method at very large batch sizes, enabling scalable training with improved generalization and faster convergence. FOP constructs a variance-aware update direction by leveraging gradients from two sub-batches, enhancing the average gradient with a component of the gradient difference that is orthogonal to the average under the Fisher-metric.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡æ‰¹é‡(Large Batch Size)è®­ç»ƒä¸­ä¸€é˜¶æ–¹æ³•éš¾ä»¥è·³å‡ºå±€éƒ¨æå°å€¼ä»¥åŠäºŒé˜¶æ–¹æ³•å¦‚KFACå› é«˜é˜»å°¼(damping)å¯¼è‡´æ›²ç‡ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ï¼Œæå‡ºäº†Fisher-Orthogonal Projection (FOP)æŠ€æœ¯ã€‚FOPæ—¨åœ¨æ¢å¤äºŒé˜¶æ–¹æ³•åœ¨è¶…å¤§è§„æ¨¡æ‰¹é‡ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œä»è€Œå®ç°å…·æœ‰æ›´å¼ºæ³›åŒ–èƒ½åŠ›å’Œæ›´å¿«æ”¶æ•›é€Ÿåº¦çš„å¯æ‰©å±•è®­ç»ƒã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨ä¸¤ä¸ªå­æ‰¹æ¬¡çš„æ¢¯åº¦ï¼Œåœ¨Fisher-metricä¸‹æ„å»ºä¸å¹³å‡æ¢¯åº¦æ­£äº¤çš„æ¢¯åº¦å·®åˆ†åˆ†é‡ï¼Œä»è€Œç”Ÿæˆä¸€ç§æ–¹å·®æ„ŸçŸ¥(variance-aware)çš„æ›´æ–°æ–¹å‘ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒFOPèƒ½å¤Ÿæœ‰æ•ˆä¿ç•™å¹¶åˆ©ç”¨å…³é”®çš„æ›²ç‡ä¿¡æ¯ï¼Œå…‹æœäº†ä¼ ç»Ÿè‡ªç„¶æ¢¯åº¦ä¸‹é™(Natural Gradient Descent)åœ¨å¤„ç†å¤§æ‰¹é‡æ•°æ®æ—¶çš„æ€§èƒ½ç“¶é¢ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13898v2",
      "published_date": "2025-08-19 15:02:22 UTC",
      "updated_date": "2025-08-24 15:07:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:44:56.753834+00:00"
    },
    {
      "arxiv_id": "2508.13877v1",
      "title": "Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided Decision Transformer",
      "title_zh": "é€šè¿‡ç¬¦å·å¼•å¯¼çš„å†³ç­– Transformer å®ç°å¯éƒ¨ç½²çš„å¤šæœºå™¨äººåä½œ",
      "authors": [
        "Rathnam Vidushika Rasanji",
        "Jin Wei-Kocsis",
        "Jiansong Zhang",
        "Dongming Gan",
        "Ragu Athinarayanan",
        "Paul Asunda"
      ],
      "abstract": "Reinforcement learning (RL) has demonstrated great potential in robotic operations. However, its data-intensive nature and reliance on the Markov Decision Process (MDP) assumption limit its practical deployment in real-world scenarios involving complex dynamics and long-term temporal dependencies, such as multi-robot manipulation. Decision Transformers (DTs) have emerged as a promising offline alternative by leveraging causal transformers for sequence modeling in RL tasks. However, their applications to multi-robot manipulations still remain underexplored. To address this gap, we propose a novel framework, Symbolically-Guided Decision Transformer (SGDT), which integrates a neuro-symbolic mechanism with a causal transformer to enable deployable multi-robot collaboration. In the proposed SGDT framework, a neuro-symbolic planner generates a high-level task-oriented plan composed of symbolic subgoals. Guided by these subgoals, a goal-conditioned decision transformer (GCDT) performs low-level sequential decision-making for multi-robot manipulation. This hierarchical architecture enables structured, interpretable, and generalizable decision making in complex multi-robot collaboration tasks. We evaluate the performance of SGDT across a range of task scenarios, including zero-shot and few-shot scenarios. To our knowledge, this is the first work to explore DT-based technology for multi-robot manipulation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (RL)åœ¨å¤æ‚åŠ¨æ€å’Œé•¿æœŸä¾èµ–ä»»åŠ¡ä¸­éƒ¨ç½²éš¾çš„é—®é¢˜ï¼Œæå‡ºäº†Symbolically-Guided Decision Transformer (SGDT)æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å¯éƒ¨ç½²çš„å¤šæœºå™¨äººåä½œã€‚è¯¥æ¡†æ¶é‡‡ç”¨åˆ†å±‚æ¶æ„ï¼Œé€šè¿‡ç¥ç»ç¬¦å·(neuro-symbolic)è§„åˆ’å™¨ç”Ÿæˆé«˜å±‚ç¬¦å·å­ç›®æ ‡(symbolic subgoals)ï¼Œå¹¶å¼•å¯¼ç›®æ ‡æ¡ä»¶å†³ç­–å˜æ¢å™¨(Goal-conditioned Decision Transformer, GCDT)æ‰§è¡Œåº•å±‚çš„åºåˆ—å†³ç­–ã€‚è¿™ç§ç»“åˆäº†å› æœå˜æ¢å™¨(causal transformer)çš„è®¾è®¡ä½¿å¤šæœºå™¨äººæ“çºµä»»åŠ¡å…·å¤‡äº†ç»“æ„åŒ–ã€å¯è§£é‡Šä¸”æ˜“äºæ³›åŒ–çš„ç‰¹æ€§ã€‚å®éªŒè¯„ä¼°è¯æ˜äº†SGDTåœ¨é›¶æ ·æœ¬(zero-shot)å’Œå°‘æ ·æœ¬(few-shot)åœºæ™¯ä¸‹çš„ä¼˜å¼‚æ€§èƒ½ã€‚æ®ä½œè€…æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–ä¸ªæ¢ç´¢å°†å†³ç­–å˜æ¢å™¨(DT)æŠ€æœ¯åº”ç”¨äºå¤šæœºå™¨äººæ“çºµé¢†åŸŸçš„ç ”ç©¶å·¥ä½œï¼Œä¸ºå¤æ‚åœºæ™¯ä¸‹çš„æœºå™¨äººåä½œæä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13877v1",
      "published_date": "2025-08-19 14:42:18 UTC",
      "updated_date": "2025-08-19 14:42:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:45:01.346240+00:00"
    },
    {
      "arxiv_id": "2508.13876v1",
      "title": "Improved Generalized Planning with LLMs through Strategy Refinement and Reflection",
      "title_zh": "é€šè¿‡ç­–ç•¥ç»†åŒ–ä¸åæ€æ”¹è¿›åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„é€šç”¨è§„åˆ’",
      "authors": [
        "Katharina Stein",
        "Nils Hodel",
        "Daniel FiÅ¡er",
        "JÃ¶rg Hoffmann",
        "Michael Katz",
        "Alexander Koller"
      ],
      "abstract": "LLMs have recently been used to generate Python programs representing generalized plans in PDDL planning, i.e., plans that generalize across the tasks of a given PDDL domain. Previous work proposed a framework consisting of three steps: the LLM first generates a summary and then a strategy for the domain, both in natural language, and then implements that strategy as a Python program, that gets debugged on example planning tasks. In that work, only one strategy is generated and passed directly to the program generation. If the strategy is incorrect, its implementation will therefore result in an incorrect generalized plan. Here, we introduce an approach that generates the strategy in the form of pseudocode and enables automatic debugging of the pseudocode, hence allowing us to identify and fix errors prior to the generation of the generalized plan itself. Additionally, we extend the Python debugging phase with a reflection step prompting the LLM to pinpoint the reason for the observed plan failure. Finally, we take inspiration from LLM code generation to produce several program variants and pick the best one. Running experiments on 17 benchmark domains, we show that these extensions substantially improve (and never deteriorate) the quality of the generalized plans. In 12 of the domains, our best Python programs solve all tasks that can be generated with the respective instance generator.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åˆ©ç”¨ LLMs åœ¨ PDDL è§„åˆ’é¢†åŸŸç”Ÿæˆ Generalized Planning ç¨‹åºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡ç­–ç•¥ç»†åŒ–ä¸åæ€è¿›è¡Œæ”¹è¿›çš„æ¡†æ¶ã€‚é’ˆå¯¹ä»¥å¾€æ–¹æ³•ä¸­è‡ªç„¶è¯­è¨€ç­–ç•¥é”™è¯¯å¯¼è‡´ç¨‹åºå¤±æ•ˆçš„å±€é™æ€§ï¼Œè¯¥æ–¹æ³•å°†ç­–ç•¥ä»¥ pseudocode å½¢å¼ç”Ÿæˆï¼Œå¹¶å¼•å…¥è‡ªåŠ¨è°ƒè¯•æœºåˆ¶ä»¥åœ¨æ­£å¼ç¼–ç¨‹å‰ä¿®å¤é€»è¾‘é”™è¯¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶åœ¨ Python è°ƒè¯•é˜¶æ®µå¢åŠ äº† reflection æ­¥éª¤ä»¥ç²¾å‡†å®šä½å¤±è´¥åŸå› ï¼Œå¹¶é‡‡ç”¨å¤šå˜ä½“ç”Ÿæˆç­–ç•¥æ¥é€‰å–æœ€ä½³ç¨‹åºå®ç°ã€‚åœ¨ 17 ä¸ªåŸºå‡†é¢†åŸŸçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™äº›æ”¹è¿›æ˜¾è‘—æå‡äº†é€šç”¨è®¡åˆ’çš„æˆåŠŸç‡ã€‚åœ¨å…¶ä¸­ 12 ä¸ªé¢†åŸŸä¸­ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„ç¨‹åºæˆåŠŸè§£å†³äº†æ‰€æœ‰ç”Ÿæˆçš„ä»»åŠ¡å®ä¾‹ï¼Œå……åˆ†éªŒè¯äº†ç­–ç•¥ç»†åŒ–ä¸åæ€æœºåˆ¶åœ¨æå‡ LLMs å¤æ‚è§„åˆ’èƒ½åŠ›æ–¹é¢çš„æ ¸å¿ƒä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13876v1",
      "published_date": "2025-08-19 14:42:18 UTC",
      "updated_date": "2025-08-19 14:42:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:45:06.242834+00:00"
    },
    {
      "arxiv_id": "2508.13875v2",
      "title": "A Novel Attention-Augmented Wavelet YOLO System for Real-time Brain Vessel Segmentation on Transcranial Color-coded Doppler",
      "title_zh": "é¢å‘ç»é¢…å½©è‰²å¤šæ™®å‹’è¶…å£°å®æ—¶è„‘è¡€ç®¡åˆ†å‰²çš„æ–°å‹æ³¨æ„åŠ›å¢å¼ºå°æ³¢ YOLO ç³»ç»Ÿ",
      "authors": [
        "Wenxuan Zhang",
        "Shuai Li",
        "Xinyi Wang",
        "Yu Sun",
        "Hongyu Kang",
        "Pui Yuk Chryste Wan",
        "Jing Qin",
        "Yuanpeng Zhang",
        "Yong-Ping Zheng",
        "Sai-Kit Lam"
      ],
      "abstract": "The Circle of Willis (CoW), vital for ensuring consistent blood flow to the brain, is closely linked to ischemic stroke. Accurate assessment of the CoW is important for identifying individuals at risk and guiding appropriate clinical management. Among existing imaging methods, Transcranial Color-coded Doppler (TCCD) offers unique advantages due to its radiation-free nature, affordability, and accessibility. However, reliable TCCD assessments depend heavily on operator expertise for identifying anatomical landmarks and performing accurate angle correction, which limits its widespread adoption. To address this challenge, we propose an AI-powered, real-time CoW auto-segmentation system capable of efficiently capturing cerebral arteries. No prior studies have explored AI-driven cerebrovascular segmentation using TCCD. In this work, we introduce a novel Attention-Augmented Wavelet YOLO (AAW-YOLO) network tailored for TCCD data, designed to provide real-time guidance for brain vessel segmentation in the CoW. We prospectively collected TCCD data comprising 738 annotated frames and 3,419 labeled artery instances to establish a high-quality dataset for model training and evaluation. The proposed AAW-YOLO demonstrated strong performance in segmenting both ipsilateral and contralateral CoW vessels, achieving an average Dice score of 0.901, IoU of 0.823, precision of 0.882, recall of 0.926, and mAP of 0.953, with a per-frame inference speed of 14.199 ms. This system offers a practical solution to reduce reliance on operator experience in TCCD-based cerebrovascular screening, with potential applications in routine clinical workflows and resource-constrained settings. Future research will explore bilateral modeling and larger-scale validation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç»é¢…å½©è‰²å¤šæ™®å‹’è¶…å£°(Transcranial Color-coded Doppler, TCCD)åœ¨è¯„ä¼°å¤§è„‘åŠ¨è„‰ç¯(Circle of Willis, CoW)æ—¶è¿‡åº¦ä¾èµ–æ“ä½œè€…ç»éªŒçš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§AIé©±åŠ¨çš„å®æ—¶è„‘è¡€ç®¡è‡ªåŠ¨åˆ†å‰²ç³»ç»Ÿã€‚ç ”ç©¶å¼•å…¥äº†åˆ›æ–°çš„æ³¨æ„åŠ›å¢å¼ºå°æ³¢YOLOç½‘ç»œ(Attention-Augmented Wavelet YOLO, AAW-YOLO)ï¼Œä¸“é—¨ç”¨äºåœ¨TCCDå½±åƒä¸­æä¾›å®æ—¶çš„è¡€ç®¡åˆ†å‰²æŒ‡å¼•ã€‚é€šè¿‡åœ¨åŒ…å«738ä¸ªæ ‡æ³¨å¸§å’Œ3,419ä¸ªåŠ¨è„‰å®ä¾‹çš„å‰ç»æ€§æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒAAW-YOLOå®ç°äº†0.901çš„å¹³å‡Diceåˆ†æ•°å’Œ0.953çš„mAPï¼Œä¸”å•å¸§æ¨ç†é€Ÿåº¦ä»…ä¸º14.199 msã€‚è¯¥ç³»ç»Ÿæœ‰æ•ˆé™ä½äº†è„‘è¡€ç®¡ç­›æŸ¥å¯¹åŒ»ç”Ÿä¸´åºŠç»éªŒçš„ä¾èµ–ï¼Œåœ¨å¸¸è§„ä¸´åºŠæµç¨‹åŠåŒ»ç–—èµ„æºåŒ®ä¹åœ°åŒºå…·æœ‰æ˜¾è‘—çš„åº”ç”¨ä»·å€¼ã€‚è¿™é¡¹å·¥ä½œå¡«è¡¥äº†åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯è¿›è¡ŒTCCDè„‘è¡€ç®¡è‡ªåŠ¨åˆ†å‰²çš„ç ”ç©¶ç©ºç™½ï¼Œä¸ºå¯è´Ÿæ‹…ä¸”æ— è¾å°„çš„è„‘è¡€ç®¡ç–¾ç—…ç­›æŸ¥å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13875v2",
      "published_date": "2025-08-19 14:41:22 UTC",
      "updated_date": "2025-12-03 06:25:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:45:25.551669+00:00"
    },
    {
      "arxiv_id": "2508.14926v3",
      "title": "Ethics-Aware Safe Reinforcement Learning for Rare-Event Risk Control in Interactive Urban Driving",
      "title_zh": "äº¤äº’å¼åŸå¸‚é©¾é©¶ä¸­é¢å‘ç¨€æœ‰äº‹ä»¶é£é™©æ§åˆ¶çš„ä¼¦ç†æ„ŸçŸ¥å®‰å…¨å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Dianzhao Li",
        "Ostap Okhrin"
      ],
      "abstract": "Autonomous vehicles hold great promise for reducing traffic fatalities and improving transportation efficiency, yet their widespread adoption hinges on embedding credible and transparent ethical reasoning into routine and emergency maneuvers, particularly to protect vulnerable road users (VRUs) such as pedestrians and cyclists. Here, we present a hierarchical Safe Reinforcement Learning (Safe RL) framework that augments standard driving objectives with ethics-aware cost signals. At the decision level, a Safe RL agent is trained using a composite ethical risk cost, combining collision probability and harm severity, to generate high-level motion targets. A dynamic, risk-sensitive Prioritized Experience Replay mechanism amplifies learning from rare but critical, high-risk events. At the execution level, polynomial path planning coupled with Proportional-Integral-Derivative (PID) and Stanley controllers translates these targets into smooth, feasible trajectories, ensuring both accuracy and comfort. We train and validate our approach on closed-loop simulation environments derived from large-scale, real-world traffic datasets encompassing diverse vehicles, cyclists, and pedestrians, and demonstrate that it outperforms baseline methods in reducing risk to others while maintaining ego performance and comfort. This work provides a reproducible benchmark for Safe RL with explicitly ethics-aware objectives in human-mixed traffic scenarios. Our results highlight the potential of combining formal control theory and data-driven learning to advance ethically accountable autonomy that explicitly protects those most at risk in urban traffic environments. Across two interactive benchmarks and five random seeds, our policy decreases conflict frequency by 25-45% compared to matched task successes while maintaining comfort metrics within 5%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå±‚æ¬¡åŒ–å®‰å…¨å¼ºåŒ–å­¦ä¹ (Safe Reinforcement Learning)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åµŒå…¥ä¼¦ç†æ„ŸçŸ¥æˆæœ¬ä¿¡å·æ¥è§£å†³äº¤äº’å¼åŸå¸‚é©¾é©¶ä¸­çš„ç½•è§äº‹ä»¶é£é™©æ§åˆ¶é—®é¢˜ï¼Œç‰¹åˆ«åŠ å¼ºäº†å¯¹è¡Œäººå’Œéª‘è¡Œè€…ç­‰è„†å¼±é“è·¯ä½¿ç”¨è€…(VRUs)çš„ä¿æŠ¤ã€‚åœ¨å†³ç­–å±‚ï¼ŒSafe RLæ™ºèƒ½ä½“åˆ©ç”¨ç»“åˆç¢°æ’æ¦‚ç‡ä¸ä¼¤å®³ä¸¥é‡ç¨‹åº¦çš„ç»¼åˆä¼¦ç†é£é™©æˆæœ¬ç”Ÿæˆé«˜å±‚è¿åŠ¨ç›®æ ‡ï¼Œå¹¶å¼•å…¥åŠ¨æ€é£é™©æ•æ„Ÿçš„ä¼˜å…ˆç»éªŒå›æ”¾(Prioritized Experience Replay)æœºåˆ¶æ¥å¼ºåŒ–å¯¹ç½•è§é«˜é£é™©äº‹ä»¶çš„å­¦ä¹ ã€‚åœ¨æ‰§è¡Œå±‚ï¼Œç³»ç»Ÿé€šè¿‡å¤šé¡¹å¼è·¯å¾„è§„åˆ’é…åˆPIDå’ŒStanleyæ§åˆ¶å™¨ï¼Œç¡®ä¿è½¨è¿¹çš„å¹³æ»‘æ€§ä¸å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åœ¨åŸºäºå¤§è§„æ¨¡çœŸå®äº¤é€šæ•°æ®é›†çš„é—­ç¯ä»¿çœŸä¸­è¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºåœ¨ç»´æŒè‡ªåŠ¨é©¾é©¶è½¦è¾†æ€§èƒ½ä¸èˆ’é€‚åº¦çš„å‰æä¸‹ï¼Œç›¸æ¯”åŸºå‡†æ–¹æ³•å°†å†²çªé¢‘ç‡é™ä½äº†25-45%ã€‚è¿™é¡¹å·¥ä½œä¸ºäººç±»æ··åˆäº¤é€šåœºæ™¯ä¸‹å…·å¤‡ä¼¦ç†è´£ä»»æ„Ÿçš„è‡ªä¸»å†³ç­–æä¾›äº†å¯å¤ç°çš„åŸºå‡†ï¼Œå±•ç¤ºäº†ç»“åˆæ§åˆ¶ç†è®ºä¸æ•°æ®é©±åŠ¨å­¦ä¹ åœ¨æå‡è‡ªåŠ¨é©¾é©¶å®‰å…¨æ€§æ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14926v3",
      "published_date": "2025-08-19 14:24:02 UTC",
      "updated_date": "2025-11-07 17:50:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:45:40.047354+00:00"
    },
    {
      "arxiv_id": "2508.18230v1",
      "title": "KillChainGraph: ML Framework for Predicting and Mapping ATT&CK Techniques",
      "title_zh": "KillChainGraphï¼šé¢„æµ‹ä¸æ˜ å°„ ATT&CK æŠ€æœ¯çš„æœºå™¨å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Chitraksh Singh",
        "Monisha Dhanraj",
        "Ken Huang"
      ],
      "abstract": "The escalating complexity and volume of cyberattacks demand proactive detection strategies that go beyond traditional rule-based systems. This paper presents a phase-aware, multi-model machine learning framework that emulates adversarial behavior across the seven phases of the Cyber Kill Chain using the MITRE ATT&CK Enterprise dataset. Techniques are semantically mapped to phases via ATTACK-BERT, producing seven phase-specific datasets. We evaluate LightGBM, a custom Transformer encoder, fine-tuned BERT, and a Graph Neural Network (GNN), integrating their outputs through a weighted soft voting ensemble. Inter-phase dependencies are modeled using directed graphs to capture attacker movement from reconnaissance to objectives. The ensemble consistently achieved the highest scores, with F1-scores ranging from 97.47% to 99.83%, surpassing GNN performance (97.36% to 99.81%) by 0.03%--0.20% across phases. This graph-driven, ensemble-based approach enables interpretable attack path forecasting and strengthens proactive cyber defense.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† KillChainGraphï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé¢„æµ‹å’Œæ˜ å°„ ATT&CK æŠ€æœ¯çš„ç›¸ä½æ„ŸçŸ¥å¤šæ¨¡å‹æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨åŠ å¼ºä¸»åŠ¨ç½‘ç»œé˜²å¾¡ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ MITRE ATT&CK Enterprise æ•°æ®é›†æ¨¡æ‹Ÿ Cyber Kill Chain ä¸ƒä¸ªé˜¶æ®µä¸­çš„å¯¹æŠ—è¡Œä¸ºï¼Œå¹¶ä½¿ç”¨ ATTACK-BERT å°†æŠ€æœ¯è¯­ä¹‰åŒ–æ˜ å°„è‡³ç›¸åº”é˜¶æ®µã€‚ç ”ç©¶é›†æˆäº† LightGBMã€è‡ªå®šä¹‰ Transformer ç¼–ç å™¨ã€å¾®è°ƒåçš„ BERT å’Œå›¾ç¥ç»ç½‘ç»œ (GNN)ï¼Œå¹¶é€šè¿‡åŠ æƒè½¯æŠ•ç¥¨é›†æˆ (Weighted soft voting ensemble) ç­–ç•¥ä¼˜åŒ–è¾“å‡ºã€‚é€šè¿‡æœ‰å‘å›¾å¯¹é˜¶æ®µé—´çš„ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼ŒKillChainGraph èƒ½å¤Ÿæœ‰æ•ˆæ•è·æ”»å‡»è€…ä»ä¾¦å¯Ÿåˆ°ç›®æ ‡çš„è¡ŒåŠ¨è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥é›†æˆæ–¹æ³•åœ¨å„é˜¶æ®µçš„ F1-score è¾¾åˆ° 97.47% è‡³ 99.83%ï¼Œæ€§èƒ½ä¸€è‡´ä¼˜äºå•ä¸€æ¨¡å‹ã€‚è¿™ç§å›¾é©±åŠ¨çš„é›†æˆæ–¹æ³•å®ç°äº†å¯è§£é‡Šçš„æ”»å‡»è·¯å¾„é¢„æµ‹ï¼Œä¸ºåº”å¯¹å¤æ‚ç½‘ç»œæ”»å‡»æä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "8 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.18230v1",
      "published_date": "2025-08-19 14:10:01 UTC",
      "updated_date": "2025-08-19 14:10:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:45:41.352686+00:00"
    },
    {
      "arxiv_id": "2508.13843v1",
      "title": "UniECS: Unified Multimodal E-Commerce Search Framework with Gated Cross-modal Fusion",
      "title_zh": "UniECSï¼šåŸºäºé—¨æ§è·¨æ¨¡æ€èåˆçš„ç»Ÿä¸€å¤šæ¨¡æ€ç”µå•†æœç´¢æ¡†æ¶",
      "authors": [
        "Zihan Liang",
        "Yufei Ma",
        "ZhiPeng Qian",
        "Huangyu Dai",
        "Zihan Wang",
        "Ben Chen",
        "Chenyi Lei",
        "Yuqing Ding",
        "Han Li"
      ],
      "abstract": "Current e-commerce multimodal retrieval systems face two key limitations: they optimize for specific tasks with fixed modality pairings, and lack comprehensive benchmarks for evaluating unified retrieval approaches. To address these challenges, we introduce UniECS, a unified multimodal e-commerce search framework that handles all retrieval scenarios across image, text, and their combinations. Our work makes three key contributions. First, we propose a flexible architecture with a novel gated multimodal encoder that uses adaptive fusion mechanisms. This encoder integrates different modality representations while handling missing modalities. Second, we develop a comprehensive training strategy to optimize learning. It combines cross-modal alignment loss (CMAL), cohesive local alignment loss (CLAL), intra-modal contrastive loss (IMCL), and adaptive loss weighting. Third, we create M-BEER, a carefully curated multimodal benchmark containing 50K product pairs for e-commerce search evaluation. Extensive experiments demonstrate that UniECS consistently outperforms existing methods across four e-commerce benchmarks with fine-tuning or zero-shot evaluation. On our M-BEER bench, UniECS achieves substantial improvements in cross-modal tasks (up to 28\\% gain in R@10 for text-to-image retrieval) while maintaining parameter efficiency (0.2B parameters) compared to larger models like GME-Qwen2VL (2B) and MM-Embed (8B). Furthermore, we deploy UniECS in the e-commerce search platform of Kuaishou Inc. across two search scenarios, achieving notable improvements in Click-Through Rate (+2.74\\%) and Revenue (+8.33\\%). The comprehensive evaluation demonstrates the effectiveness of our approach in both experimental and real-world settings. Corresponding codes, models and datasets will be made publicly available at https://github.com/qzp2018/UniECS.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†UniECSï¼Œä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€ç”µå­å•†åŠ¡æœç´¢æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰æ£€ç´¢ç³»ç»Ÿåœ¨ç‰¹å®šä»»åŠ¡ä¼˜åŒ–å’Œå›ºå®šæ¨¡æ€é…å¯¹æ–¹é¢çš„å±€é™ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å¸¦æœ‰è‡ªé€‚åº”èåˆæœºåˆ¶çš„æ–°å‹Gated Multimodal Encoderï¼Œèƒ½å¤Ÿé«˜æ•ˆæ•´åˆä¸åŒæ¨¡æ€è¡¨ç¤ºå¹¶æœ‰æ•ˆå¤„ç†æ¨¡æ€ç¼ºå¤±ã€‚åœ¨è®­ç»ƒç­–ç•¥ä¸Šï¼ŒUniECSç»“åˆäº†Cross-modal Alignment Loss (CMAL)ã€Cohesive Local Alignment Loss (CLAL)å’ŒIntra-modal Contrastive Loss (IMCL)ï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”æŸå¤±åŠ æƒè¿›è¡Œä¼˜åŒ–ã€‚ç ”ç©¶è€…è¿˜æ¨å‡ºäº†åŒ…å«5ä¸‡ä¸ªå•†å“å¯¹çš„å…¨é¢åŸºå‡†æµ‹è¯•é›†M-BEERï¼Œç”¨äºè¯„ä¼°ç»Ÿä¸€æ£€ç´¢æ–¹æ³•ã€‚å®éªŒè¯æ˜ï¼ŒUniECSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’ŒZero-shotè¯„ä¼°ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­R@10æå‡é«˜è¾¾28%ã€‚å°½ç®¡å…¶å‚æ•°é‡ä»…ä¸º0.2Bï¼Œä½†åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†GME-Qwen2VLå’ŒMM-Embedç­‰å¤§è§„æ¨¡æ¨¡å‹ã€‚è¯¥æ¡†æ¶å·²åœ¨å¿«æ‰‹æœç´¢å¹³å°éƒ¨ç½²åº”ç”¨ï¼Œå®é™…ä¸šåŠ¡æŒ‡æ ‡ç‚¹å‡»ç‡(CTR)æå‡2.74%ï¼Œæ”¶å…¥å¢é•¿8.33%ï¼ŒéªŒè¯äº†å…¶åœ¨ç°å®å·¥ä¸šåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted at CIKM2025 as a long paper",
      "pdf_url": "https://arxiv.org/pdf/2508.13843v1",
      "published_date": "2025-08-19 14:06:13 UTC",
      "updated_date": "2025-08-19 14:06:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:45:44.748595+00:00"
    },
    {
      "arxiv_id": "2508.13836v1",
      "title": "One Shot vs. Iterative: Rethinking Pruning Strategies for Model Compression",
      "title_zh": "ä¸€æ¬¡æ€§è¿˜æ˜¯è¿­ä»£ï¼šé‡æ–°å®¡è§†æ¨¡å‹å‹ç¼©ä¸­çš„å‰ªæç­–ç•¥",
      "authors": [
        "MikoÅ‚aj Janusz",
        "Tomasz Wojnar",
        "Yawei Li",
        "Luca Benini",
        "Kamil Adamczewski"
      ],
      "abstract": "Pruning is a core technique for compressing neural networks to improve computational efficiency. This process is typically approached in two ways: one-shot pruning, which involves a single pass of training and pruning, and iterative pruning, where pruning is performed over multiple cycles for potentially finer network refinement. Although iterative pruning has historically seen broader adoption, this preference is often assumed rather than rigorously tested. Our study presents one of the first systematic and comprehensive comparisons of these methods, providing rigorous definitions, benchmarking both across structured and unstructured settings, and applying different pruning criteria and modalities. We find that each method has specific advantages: one-shot pruning proves more effective at lower pruning ratios, while iterative pruning performs better at higher ratios. Building on these findings, we advocate for patience-based pruning and introduce a hybrid approach that can outperform traditional methods in certain scenarios, providing valuable insights for practitioners selecting a pruning strategy tailored to their goals and constraints. Source code is available at https://github.com/janumiko/pruning-benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨¡å‹å‹ç¼©ä¸­çš„æ ¸å¿ƒæŠ€æœ¯ Pruningï¼Œå¯¹ One-shot pruning ä¸ Iterative pruning ä¸¤ç§ä¸»æµç­–ç•¥è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„é‡æ–°è¯„ä¼°ä¸æ¯”è¾ƒã€‚ä½œè€…é€šè¿‡åœ¨ç»“æ„åŒ–å’Œéç»“æ„åŒ–è®¾ç½®ä¸‹åº”ç”¨ä¸åŒçš„å‰ªæå‡†åˆ™å’Œæ¨¡æ€ï¼Œä¸ºè¿™ä¸¤ç§æ–¹æ³•æä¾›äº†ä¸¥æ ¼çš„å®šä¹‰å’ŒåŸºå‡†æµ‹è¯•ï¼Œå¡«è¡¥äº†ä»¥å¾€å¯¹è¿­ä»£å‰ªæä¼˜è¶Šæ€§ç¼ºä¹ä¸¥è°¨éªŒè¯çš„ç©ºç™½ã€‚ç ”ç©¶å‘ç°ï¼ŒOne-shot pruning åœ¨ä½å‰ªæç‡ä¸‹æ›´ä¸ºæœ‰æ•ˆï¼Œè€Œ Iterative pruning åœ¨é«˜å‰ªæç‡ä¸‹è¡¨ç°å‡ºæ›´å¥½çš„ç½‘ç»œç²¾ç‚¼æ•ˆæœã€‚åŸºäºè¿™äº›å‘ç°ï¼Œç ”ç©¶æå€¡ä½¿ç”¨ Patience-based pruningï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åœ¨ç‰¹å®šåœºæ™¯ä¸‹æ€§èƒ½è¶…è¶Šä¼ ç»Ÿæ–¹æ³•çš„ Hybrid approachã€‚è¯¥å·¥ä½œä¸ä»…ä¸ºä»ä¸šè€…æ ¹æ®å…·ä½“ç›®æ ‡å’Œçº¦æŸé€‰æ‹©å‰ªæç­–ç•¥æä¾›äº†é‡è¦è§è§£ï¼Œè¿˜é€šè¿‡å¼€æºä»£ç ä¸ºæ¨¡å‹ä¼˜åŒ–é¢†åŸŸæä¾›äº†å®è´µçš„å‚è€ƒåŸºå‡†ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13836v1",
      "published_date": "2025-08-19 13:57:10 UTC",
      "updated_date": "2025-08-19 13:57:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:45:49.891055+00:00"
    },
    {
      "arxiv_id": "2508.15839v1",
      "title": "CIA+TA Risk Assessment for AI Reasoning Vulnerabilities",
      "title_zh": "é¢å‘äººå·¥æ™ºèƒ½æ¨ç†æ¼æ´çš„ CIA+TA é£é™©è¯„ä¼°",
      "authors": [
        "Yuksel Aydin"
      ],
      "abstract": "As AI systems increasingly influence critical decisions, they face threats that exploit reasoning mechanisms rather than technical infrastructure. We present a framework for cognitive cybersecurity, a systematic protection of AI reasoning processes from adversarial manipulation. Our contributions are threefold. First, we establish cognitive cybersecurity as a discipline complementing traditional cybersecurity and AI safety, addressing vulnerabilities where legitimate inputs corrupt reasoning while evading conventional controls. Second, we introduce the CIA+TA, extending traditional Confidentiality, Integrity, and Availability triad with Trust (epistemic validation) and Autonomy (human agency preservation), requirements unique to systems generating knowledge claims and mediating decisions. Third, we present a quantitative risk assessment methodology with empirically-derived coefficients, enabling organizations to measure cognitive security risks. We map our framework to OWASP LLM Top 10 and MITRE ATLAS, facilitating operational integration. Validation through previously published studies (151 human participants; 12,180 AI trials) reveals strong architecture dependence: identical defenses produce effects ranging from 96% reduction to 135% amplification of vulnerabilities. This necessitates pre-deployment Cognitive Penetration Testing as a governance requirement for trustworthy AI deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†è®¤çŸ¥ç½‘ç»œå®‰å…¨(cognitive cybersecurity)æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿæ€§åœ°ä¿æŠ¤äººå·¥æ™ºèƒ½(AI)çš„æ¨ç†è¿‡ç¨‹å…å—é’ˆå¯¹æ¨ç†æœºåˆ¶è€ŒéæŠ€æœ¯æ¶æ„çš„å¯¹æŠ—æ€§æ“çºµã€‚è¯¥æ¡†æ¶å°†ä¼ ç»Ÿçš„æœºå¯†æ€§ã€å®Œæ•´æ€§å’Œå¯ç”¨æ€§(CIA)ä¸‰è¦ç´ æ‰©å±•ä¸ºCIA+TAï¼Œé€šè¿‡å¢åŠ ä¿¡ä»»(Trust)å’Œè‡ªä¸»æ€§(Autonomy)æ¥æ»¡è¶³çŸ¥è¯†ç”Ÿæˆå’Œå†³ç­–è°ƒè§£ç³»ç»Ÿçš„ç‰¹æ®Šéœ€æ±‚ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†å¸¦æœ‰ç»éªŒç³»æ•°çš„å®šé‡é£é™©è¯„ä¼°æ–¹æ³•ï¼Œå¹¶å°†å…¶ä¸OWASP LLM Top 10åŠMITRE ATLASæ˜ å°„ï¼Œä»¥æå‡æ“ä½œé›†æˆæ€§ã€‚é€šè¿‡å¯¹12,180æ¬¡AIè¯•éªŒçš„éªŒè¯å‘ç°ï¼Œé˜²å¾¡æ•ˆæœé«˜åº¦ä¾èµ–äºæ¶æ„ï¼Œç›¸åŒçš„é˜²å¾¡æ‰‹æ®µå¯èƒ½ä½¿æ¼æ´å‡å°‘96%æˆ–åè€Œæ”¾å¤§135%ã€‚å› æ­¤ï¼Œè¯¥ç ”ç©¶å»ºè®®å°†è®¤çŸ¥æ¸—é€æµ‹è¯•(Cognitive Penetration Testing)ä½œä¸ºå¯ä¿¡AIéƒ¨ç½²çš„æ²»ç†è¦æ±‚ï¼Œä»¥ç¡®ä¿æ¨ç†è¿‡ç¨‹çš„å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15839v1",
      "published_date": "2025-08-19 13:56:09 UTC",
      "updated_date": "2025-08-19 13:56:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:45:51.345896+00:00"
    },
    {
      "arxiv_id": "2508.13833v1",
      "title": "Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling",
      "title_zh": "é¢å‘å»ºç­‘ä¿¡æ¯æ¨¡å‹çš„éç»“æ„åŒ–å»ºç­‘æŠ€æœ¯è§„èŒƒç»“æ„åŒ–éœ€æ±‚æå–",
      "authors": [
        "Insaf Nahri",
        "Romain PinquiÃ©",
        "Philippe VÃ©ron",
        "Nicolas Bus",
        "Mathieu Thorel"
      ],
      "abstract": "This study explores the integration of Building Information Modeling (BIM) with Natural Language Processing (NLP) to automate the extraction of requirements from unstructured French Building Technical Specification (BTS) documents within the construction industry. Employing Named Entity Recognition (NER) and Relation Extraction (RE) techniques, the study leverages the transformer-based model CamemBERT and applies transfer learning with the French language model Fr\\_core\\_news\\_lg, both pre-trained on a large French corpus in the general domain. To benchmark these models, additional approaches ranging from rule-based to deep learning-based methods are developed. For RE, four different supervised models, including Random Forest, are implemented using a custom feature vector. A hand-crafted annotated dataset is used to compare the effectiveness of NER approaches and RE models. Results indicate that CamemBERT and Fr\\_core\\_news\\_lg exhibited superior performance in NER, achieving F1-scores over 90\\%, while Random Forest proved most effective in RE, with an F1 score above 80\\%. The outcomes are intended to be represented as a knowledge graph in future work to further enhance automatic verification systems.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢ç´¢äº†å°†å»ºç­‘ä¿¡æ¯æ¨¡å‹(BIM)ä¸è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ç›¸ç»“åˆï¼Œä»¥è‡ªåŠ¨åŒ–åœ°ä»å»ºç­‘å·¥ç¨‹é¢†åŸŸéç»“æ„åŒ–çš„æ³•è¯­å»ºç­‘æŠ€æœ¯è§„èŒƒ(BTS)æ–‡æ¡£ä¸­æå–éœ€æ±‚ã€‚ç ”ç©¶é‡‡ç”¨äº†å‘½åå®ä½“è¯†åˆ«(NER)å’Œå…³ç³»æŠ½å–(RE)æŠ€æœ¯ï¼Œé‡ç‚¹è¯„ä¼°äº†åŸºäºTransformeræ¶æ„çš„CamemBERTæ¨¡å‹ï¼Œå¹¶å¯¹åœ¨é€šç”¨æ³•è¯­è¯­æ–™åº“ä¸Šé¢„è®­ç»ƒçš„Fr\\_core\\_news\\_lgæ¨¡å‹è¿›è¡Œäº†è¿ç§»å­¦ä¹ ã€‚ä¸ºè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶è€…å¼€å‘äº†ä»åŸºäºè§„åˆ™åˆ°æ·±åº¦å­¦ä¹ çš„å¤šç§æ–¹æ³•ï¼Œå¹¶é’ˆå¯¹REä»»åŠ¡ä½¿ç”¨è‡ªå®šä¹‰ç‰¹å¾å‘é‡å®ç°äº†åŒ…æ‹¬éšæœºæ£®æ—(Random Forest)åœ¨å†…çš„å››ç§ç›‘ç£å­¦ä¹ æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCamemBERTå’ŒFr\\_core\\_news\\_lgåœ¨NERä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…¶F1-scoreå‡è¶…è¿‡äº†90%ï¼Œè€Œåœ¨REä»»åŠ¡ä¸­ï¼Œéšæœºæ£®æ—(Random Forest)è¢«è¯æ˜æ˜¯æœ€æœ‰æ•ˆçš„æ¨¡å‹ï¼Œå…¶F1-scoreè¾¾åˆ°äº†80%ä»¥ä¸Šã€‚è¯¥ç ”ç©¶æˆæœæœªæ¥å°†ä»¥çŸ¥è¯†å›¾è°±(knowledge graph)çš„å½¢å¼å‘ˆç°ï¼Œæ—¨åœ¨è¿›ä¸€æ­¥å¢å¼ºå»ºç­‘è¡Œä¸šè‡ªåŠ¨åŒ–éªŒè¯ç³»ç»Ÿçš„æ•ˆèƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13833v1",
      "published_date": "2025-08-19 13:55:41 UTC",
      "updated_date": "2025-08-19 13:55:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:45:52.147864+00:00"
    },
    {
      "arxiv_id": "2508.14140v2",
      "title": "Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs",
      "title_zh": "é¢å‘ç¨€ç–é«˜æ•ˆäººå·¥ç¥ç»ç½‘ç»œçš„ç±»è„‘é›†ç¾¤é—´é€šä¿¡åŸè¯­",
      "authors": [
        "Orestis Konstantaropoulos",
        "Stelios Manolis Smirnakis",
        "Maria Papadopouli"
      ],
      "abstract": "The structure of biological neural circuits-modular, hierarchical, and sparsely interconnected-reflects an efficient trade-off between wiring cost, functional specialization, and robustness. These principles offer valuable insights for artificial neural network (ANN) design, especially as networks grow in depth and scale. Sparsity, in particular, has been widely explored for reducing memory and computation, improving speed, and enhancing generalization. Motivated by systems neuroscience findings, we explore how patterns of functional connectivity in the mouse visual cortex-specifically, ensemble-to-ensemble communication, can inform ANN design. We introduce G2GNet, a novel architecture that imposes sparse, modular connectivity across feedforward layers. Despite having significantly fewer parameters than fully connected models, G2GNet achieves superior accuracy on standard vision benchmarks. To our knowledge, this is the first architecture to incorporate biologically observed functional connectivity patterns as a structural bias in ANN design. We complement this static bias with a dynamic sparse training (DST) mechanism that prunes and regrows edges during training. We also propose a Hebbian-inspired rewiring rule based on activation correlations, drawing on principles of biological plasticity. G2GNet achieves up to 75% sparsity while improving accuracy by up to 4.3% on benchmarks, including Fashion-MNIST, CIFAR-10, and CIFAR-100, outperforming dense baselines with far fewer computations.",
      "tldr_zh": "è¯¥ç ”ç©¶å—åˆ°ç”Ÿç‰©ç¥ç»å›è·¯æ¨¡å—åŒ–ã€å±‚æ¬¡åŒ–å’Œç¨€ç–äº’è”ç‰¹æ€§çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§åä¸º G2GNet çš„æ–°å‹æ¶æ„ï¼Œæ—¨åœ¨å¹³è¡¡è®¡ç®—æ•ˆç‡ã€åŠŸèƒ½ä¸“ä¸šåŒ–ä¸æ¨¡å‹é²æ£’æ€§ã€‚G2GNet å€Ÿé‰´äº†å°é¼ è§†è§‰çš®å±‚çš„åŠŸèƒ½è¿æ¥æ¨¡å¼ï¼Œé¦–æ¬¡å°†é›†ç¾¤åˆ°é›†ç¾¤(ensemble-to-ensemble)çš„é€šä¿¡åŸè¯­ä½œä¸ºç»“æ„åç½®(structural bias)å¼•å…¥äººå·¥ç¥ç»ç½‘ç»œ(ANN)è®¾è®¡ï¼Œåœ¨å‰é¦ˆå±‚ä¸­å®æ–½ç¨€ç–ä¸”æ¨¡å—åŒ–çš„è¿æ¥ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œè¯¥æ¶æ„ç»“åˆäº†åŠ¨æ€ç¨€ç–è®­ç»ƒ(DST)æœºåˆ¶ï¼Œå¹¶å¼•å…¥ä¸€ç§åŸºäºæ¿€æ´»ç›¸å…³æ€§çš„ Hebbian å¯å‘å¼é‡è¿è§„åˆ™ï¼Œä»¥æ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»çš„å¯å¡‘æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒG2GNet åœ¨ Fashion-MNISTã€CIFAR-10 å’Œ CIFAR-100 ç­‰æ ‡å‡†è§†è§‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨å®ç°é«˜è¾¾ 75% ç¨€ç–åº¦çš„åŒæ—¶ï¼Œå‡†ç¡®ç‡è¾ƒå¯†é›†åŸºçº¿æ¨¡å‹æå‡äº†æœ€é«˜ 4.3%ã€‚è¿™ä¸€ç ”ç©¶æˆæœè¯æ˜äº†é€šè¿‡å€Ÿé‰´ç¥ç»ç§‘å­¦å‘ç°ï¼Œå¯ä»¥åœ¨å¤§å¹…å‡å°‘å‚æ•°é‡å’Œè®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆæå‡æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ä¸æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14140v2",
      "published_date": "2025-08-19 13:51:33 UTC",
      "updated_date": "2025-09-21 23:43:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:45:57.451483+00:00"
    },
    {
      "arxiv_id": "2508.14139v1",
      "title": "The Statistical Validation of Innovation Lens",
      "title_zh": "Innovation Lens çš„ç»Ÿè®¡éªŒè¯",
      "authors": [
        "Giacomo Radaelli",
        "Jonah Lynch"
      ],
      "abstract": "Information overload and the rapid pace of scientific advancement make it increasingly difficult to evaluate and allocate resources to new research proposals. Is there a structure to scientific discovery that could inform such decisions? We present statistical evidence for such structure, by training a classifier that successfully predicts high-citation research papers between 2010-2024 in the Computer Science, Physics, and PubMed domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç§‘å­¦è¿›æ­¥é€Ÿåº¦è¿‡å¿«åŠä¿¡æ¯è¿‡è½½å¯¼è‡´ç§‘ç ”ææ¡ˆè¯„ä¼°ä¸èµ„æºåˆ†é…å›°éš¾çš„é—®é¢˜ï¼Œæ¢è®¨äº†ç§‘å­¦å‘ç°ä¸­æ˜¯å¦å­˜åœ¨å¯ä¾›å†³ç­–å‚è€ƒçš„å†…åœ¨ç»“æ„ã€‚ä½œè€…é€šè¿‡å¯¹ Innovation Lens è¿›è¡Œç»Ÿè®¡éªŒè¯ï¼Œæå‡ºå¹¶è®­ç»ƒäº†ä¸€ä¸ªèƒ½å¤ŸæˆåŠŸé¢„æµ‹é«˜å¼•ç”¨ï¼ˆhigh-citationï¼‰ç ”ç©¶è®ºæ–‡çš„åˆ†ç±»å™¨ã€‚è¯¥ç ”ç©¶åˆ†æäº† 2010 å¹´è‡³ 2024 å¹´é—´ Computer Scienceã€Physics å’Œ PubMed é¢†åŸŸçš„å­¦æœ¯è®ºæ–‡ï¼Œå¹¶ä¸ºç§‘å­¦å‘ç°ä¸­å­˜åœ¨ç‰¹å®šç»“æ„æä¾›äº†æœ‰åŠ›çš„ç»Ÿè®¡å­¦è¯æ®ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥åˆ†ç±»å™¨å¯ä»¥æœ‰æ•ˆåœ°è¯†åˆ«å…·æœ‰é«˜å½±å“åŠ›çš„ç ”ç©¶æˆæœã€‚è¿™ä¸€å‘ç°ä¸ºä¼˜åŒ–ç§‘ç ”èµ„æºé…ç½®æä¾›äº†æ–°çš„é‡åŒ–è§†è§’ï¼Œè¯æ˜äº†é€šè¿‡ç»Ÿè®¡æ–¹æ³•è¯†åˆ«ç§‘å­¦çªç ´æ¨¡å¼çš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.DL",
        "cs.AI"
      ],
      "primary_category": "cs.DL",
      "comment": "7 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.14139v1",
      "published_date": "2025-08-19 13:47:24 UTC",
      "updated_date": "2025-08-19 13:47:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:46:01.148298+00:00"
    },
    {
      "arxiv_id": "2508.13828v1",
      "title": "Revisiting RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG System Collaboration",
      "title_zh": "RAGé›†æˆå†æ¢ï¼šå¤šRAGç³»ç»Ÿåä½œçš„ç†è®ºä¸æœºåˆ¶åˆ†æ",
      "authors": [
        "Yifei Chen",
        "Guanting Dong",
        "Yutao Zhu",
        "Zhicheng Dou"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) technology has been widely applied in recent years. However, despite the emergence of various RAG frameworks, a single RAG framework still cannot adapt well to a broad range of downstream tasks. Therefore, how to leverage the advantages of multiple RAG systems has become an area worth exploring. To address this issue, we have conducted a comprehensive and systematic investigation into ensemble methods based on RAG systems. Specifically, we have analyzed the RAG ensemble framework from both theoretical and mechanistic analysis perspectives. From the theoretical analysis, we provide the first explanation of the RAG ensemble framework from the perspective of information entropy. In terms of mechanism analysis, we have explored the RAG ensemble framework from both the pipeline and module levels. We carefully select four different pipelines (Branching, Iterative, Loop, and Agentic) and three different modules (Generator, Retriever, and Reranker) to solve seven different research questions. The experiments show that aggregating multiple RAG systems is both generalizable and robust, whether at the pipeline level or the module level. Our work lays the foundation for similar research on the multi-RAG system ensemble.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å•ä¸€ Retrieval-Augmented Generation (RAG) æ¡†æ¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­é€‚é…æ€§ä¸è¶³çš„é—®é¢˜ï¼Œå¯¹å¤š RAG ç³»ç»Ÿçš„é›†æˆæ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿæ€§æ¢è®¨ã€‚åœ¨ç†è®ºåˆ†æä¸Šï¼Œæœ¬æ–‡é¦–æ¬¡ä»ä¿¡æ¯ç†µ (information entropy) çš„è§’åº¦ä¸º RAG é›†æˆæ¡†æ¶æä¾›äº†ç†è®ºè§£é‡Šã€‚åœ¨æœºåˆ¶åˆ†æå±‚é¢ï¼Œç ”ç©¶è€…æ·±å…¥æ¢ç´¢äº†æµæ°´çº¿ (pipeline) å’Œæ¨¡å— (module) ç»´åº¦çš„åä½œï¼Œæ¶‰åŠ Branchingã€Iterativeã€Loop å’Œ Agentic å››ç§æµæ°´çº¿æ¨¡å¼ä»¥åŠ Generatorã€Retriever å’Œ Reranker ä¸‰ç±»æ ¸å¿ƒæ¨¡å—ã€‚é€šè¿‡å¯¹ä¸ƒä¸ªå…³é”®ç ”ç©¶é—®é¢˜çš„å®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜é›†æˆå¤šä¸ª RAG ç³»ç»Ÿåœ¨æ³›åŒ–æ€§å’Œé²æ£’æ€§æ–¹é¢å‡ä¼˜äºå•ä¸€ç³»ç»Ÿã€‚è¯¥å·¥ä½œä¸ºå¤š RAG ç³»ç»Ÿé›†æˆçš„ç›¸å…³ç ”ç©¶æä¾›äº†ç†è®ºåŸºç¡€ä¸å®éªŒæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13828v1",
      "published_date": "2025-08-19 13:38:54 UTC",
      "updated_date": "2025-08-19 13:38:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:46:10.459219+00:00"
    },
    {
      "arxiv_id": "2508.13816v1",
      "title": "The illusion of a perfect metric: Why evaluating AI's words is harder than it looks",
      "title_zh": "å®Œç¾æŒ‡æ ‡çš„å¹»è±¡ï¼šä¸ºä½•è¯„ä¼°äººå·¥æ™ºèƒ½ç”Ÿæˆçš„æ–‡æœ¬æ¯”æƒ³è±¡ä¸­æ›´éš¾",
      "authors": [
        "Maria Paz Oliva",
        "Adriana Correia",
        "Ivan Vankov",
        "Viktor Botev"
      ],
      "abstract": "Evaluating Natural Language Generation (NLG) is crucial for the practical adoption of AI, but has been a longstanding research challenge. While human evaluation is considered the de-facto standard, it is expensive and lacks scalability. Practical applications have driven the development of various automatic evaluation metrics (AEM), designed to compare the model output with human-written references, generating a score which approximates human judgment. Over time, AEMs have evolved from simple lexical comparisons, to semantic similarity models and, more recently, to LLM-based evaluators. However, it seems that no single metric has emerged as a definitive solution, resulting in studies using different ones without fully considering the implications. This paper aims to show this by conducting a thorough examination of the methodologies of existing metrics, their documented strengths and limitations, validation methods, and correlations with human judgment. We identify several key challenges: metrics often capture only specific aspects of text quality, their effectiveness varies by task and dataset, validation practices remain unstructured, and correlations with human judgment are inconsistent. Importantly, we find that these challenges persist in the most recent type of metric, LLM-as-a-Judge, as well as in the evaluation of Retrieval Augmented Generation (RAG), an increasingly relevant task in academia and industry. Our findings challenge the quest for the 'perfect metric'. We propose selecting metrics based on task-specific needs and leveraging complementary evaluations and advocate that new metrics should focus on enhanced validation methodologies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†è‡ªç„¶è¯­è¨€ç”Ÿæˆ(Natural Language Generation, NLG)è¯„ä¼°é¢†åŸŸä¸­é•¿æœŸå­˜åœ¨çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºè¿½æ±‚â€œå®Œç¾æŒ‡æ ‡â€åœ¨å½“å‰çš„AIå‘å±•èƒŒæ™¯ä¸‹ä»æ˜¯ä¸€ç§å¹»è§‰ã€‚é€šè¿‡å¯¹ç°æœ‰è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡(Automatic Evaluation Metrics, AEM)çš„æ¼”å˜â€”â€”ä»è¯æ±‡æ¯”è¾ƒåˆ°è¯­ä¹‰ç›¸ä¼¼åº¦æ¨¡å‹å†åˆ°åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„(LLM-based)è¯„ä¼°å™¨â€”â€”è¿›è¡Œå…¨é¢å®¡æŸ¥ï¼Œç ”ç©¶æ­ç¤ºäº†è¿™äº›æŒ‡æ ‡åœ¨æ•è·æ–‡æœ¬è´¨é‡ã€ä»»åŠ¡é€‚åº”æ€§ä»¥åŠä¸äººç±»åˆ¤æ–­ä¸€è‡´æ€§æ–¹é¢çš„æ˜¾è‘—å±€é™ã€‚ç ”ç©¶ç‰¹åˆ«æŒ‡å‡ºï¼Œå³ä¾¿æ˜¯åœ¨ç›®å‰ä¸»æµçš„LLM-as-a-Judgeæ–¹æ³•ä»¥åŠæ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval Augmented Generation, RAG)è¯„ä¼°ä»»åŠ¡ä¸­ï¼Œè¿™äº›æŒ‘æˆ˜ä¾ç„¶æ ¹æ·±è’‚å›ºã€‚åŸºäºè¿™äº›å‘ç°ï¼Œä½œè€…å»ºè®®ç ”ç©¶è€…åº”æ”¾å¼ƒå¯»æ‰¾å•ä¸€ä¸‡èƒ½æŒ‡æ ‡ï¼Œè½¬è€Œæ ¹æ®ç‰¹å®šä»»åŠ¡éœ€æ±‚é€‰æ‹©æŒ‡æ ‡å¹¶é‡‡ç”¨äº’è¡¥çš„è¯„ä¼°æ‰‹æ®µã€‚æœ€åï¼Œè®ºæ–‡å‘¼åå­¦æœ¯ç•Œåº”é‡ç‚¹åŠ å¼ºè¯„ä¼°æŒ‡æ ‡çš„éªŒè¯æ–¹æ³•å­¦(validation methodologies)å»ºè®¾ï¼Œä»¥æå‡AIè¯„ä»·ä½“ç³»çš„ç§‘å­¦æ€§ä¸å¯é æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 1 figure. Accepted to RANLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.13816v1",
      "published_date": "2025-08-19 13:22:41 UTC",
      "updated_date": "2025-08-19 13:22:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:46:15.350877+00:00"
    },
    {
      "arxiv_id": "2508.14138v1",
      "title": "STAS: Spatio-Temporal Adaptive Computation Time for Spiking Transformers",
      "title_zh": "STASï¼šé¢å‘è„‰å†² Transformer çš„æ—¶ç©ºè‡ªé€‚åº”è®¡ç®—æ—¶é—´",
      "authors": [
        "Donghwa Kang",
        "Doohyun Kim",
        "Sang-Ki Ko",
        "Jinkyu Lee",
        "Brent ByungHoon Kang",
        "Hyeongboo Baek"
      ],
      "abstract": "Spiking neural networks (SNNs) offer energy efficiency over artificial neural networks (ANNs) but suffer from high latency and computational overhead due to their multi-timestep operational nature. While various dynamic computation methods have been developed to mitigate this by targeting spatial, temporal, or architecture-specific redundancies, they remain fragmented. While the principles of adaptive computation time (ACT) offer a robust foundation for a unified approach, its application to SNN-based vision Transformers (ViTs) is hindered by two core issues: the violation of its temporal similarity prerequisite and a static architecture fundamentally unsuited for its principles. To address these challenges, we propose STAS (Spatio-Temporal Adaptive computation time for Spiking transformers), a framework that co-designs the static architecture and dynamic computation policy. STAS introduces an integrated spike patch splitting (I-SPS) module to establish temporal stability by creating a unified input representation, thereby solving the architectural problem of temporal dissimilarity. This stability, in turn, allows our adaptive spiking self-attention (A-SSA) module to perform two-dimensional token pruning across both spatial and temporal axes. Implemented on spiking Transformer architectures and validated on CIFAR-10, CIFAR-100, and ImageNet, STAS reduces energy consumption by up to 45.9%, 43.8%, and 30.1%, respectively, while simultaneously improving accuracy over SOTA models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†STASï¼ˆSpatio-Temporal Adaptive computation time for Spiking transformersï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰åœ¨è§†è§‰Transformerä¸­é¢ä¸´çš„é«˜å»¶è¿Ÿå’Œé«˜è®¡ç®—å¼€é”€é—®é¢˜çš„ååŒè®¾è®¡æ¡†æ¶ã€‚ä¸ºäº†å…‹æœè‡ªé€‚åº”è®¡ç®—æ—¶é—´ï¼ˆACTï¼‰åœ¨SNN-based ViTsä¸­å› æ—¶é—´ç›¸ä¼¼æ€§ç¼ºå¤±å’Œæ¶æ„ä¸åŒ¹é…å¯¼è‡´çš„å±€é™æ€§ï¼ŒSTASå¼•å…¥äº†é›†æˆè„‰å†²è¡¥ä¸åˆ†å‰²ï¼ˆI-SPSï¼‰æ¨¡å—ä»¥å»ºç«‹æ—¶é—´ç¨³å®šæ€§ã€‚åœ¨è¿™ç§ç¨³å®šæ€§çš„åŸºç¡€ä¸Šï¼Œå…¶è‡ªé€‚åº”è„‰å†²è‡ªæ³¨æ„åŠ›ï¼ˆA-SSAï¼‰æ¨¡å—èƒ½å¤ŸåŒæ—¶åœ¨ç©ºé—´å’Œæ—¶é—´è½´ä¸Šæ‰§è¡ŒäºŒç»´ä»¤ç‰Œå‰ªæï¼ˆToken pruningï¼‰ï¼Œä»è€Œå¤§å¹…å‡å°‘å†—ä½™è®¡ç®—ã€‚åœ¨CIFAR-10ã€CIFAR-100å’ŒImageNetæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSTASåœ¨æé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œåˆ†åˆ«é™ä½äº†é«˜è¾¾45.9%ã€43.8%å’Œ30.1%çš„èƒ½è€—ã€‚è¯¥é¡¹å·¥ä½œé€šè¿‡æœ‰æ•ˆçš„æ—¶ç©ºåŠ¨æ€è®¡ç®—ç­–ç•¥ï¼Œä¸ºæ„å»ºé«˜æ•ˆä¸”é«˜æ€§èƒ½çš„è„‰å†²Transformeræ¨¡å‹æä¾›äº†æ–°è·¯å¾„ï¼Œä¸”æ•´ä½“è¡¨ç°ä¼˜äºç°æœ‰çš„SOTAæ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.14138v1",
      "published_date": "2025-08-19 13:18:21 UTC",
      "updated_date": "2025-08-19 13:18:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:46:28.544235+00:00"
    },
    {
      "arxiv_id": "2508.13813v1",
      "title": "Assessing Trustworthiness of AI Training Dataset using Subjective Logic -- A Use Case on Bias",
      "title_zh": "åŸºäºä¸»è§‚é€»è¾‘çš„ AI è®­ç»ƒæ•°æ®é›†å¯ä¿¡åº¦è¯„ä¼°ï¼šä»¥åå·®åˆ†æä¸ºä¾‹",
      "authors": [
        "Koffi Ismael Ouattara",
        "Ioannis Krontiris",
        "Theo Dimitrakos",
        "Frank Kargl"
      ],
      "abstract": "As AI systems increasingly rely on training data, assessing dataset trustworthiness has become critical, particularly for properties like fairness or bias that emerge at the dataset level. Prior work has used Subjective Logic to assess trustworthiness of individual data, but not to evaluate trustworthiness properties that emerge only at the level of the dataset as a whole. This paper introduces the first formal framework for assessing the trustworthiness of AI training datasets, enabling uncertainty-aware evaluations of global properties such as bias. Built on Subjective Logic, our approach supports trust propositions and quantifies uncertainty in scenarios where evidence is incomplete, distributed, and/or conflicting. We instantiate this framework on the trustworthiness property of bias, and we experimentally evaluate it based on a traffic sign recognition dataset. The results demonstrate that our method captures class imbalance and remains interpretable and robust in both centralized and federated contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†é¦–ä¸ªè¯„ä¼° AI è®­ç»ƒæ•°æ®é›†å¯ä¿¡åº¦çš„æ­£å¼æ¡†æ¶ï¼Œé‡ç‚¹å…³æ³¨ Bias ç­‰åœ¨æ•°æ®é›†å…¨å±€å±‚é¢æ˜¾ç°çš„å±æ€§ã€‚è¯¥æ–¹æ³•åŸºäº Subjective Logic æ„å»ºï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†è¯æ®ä¸å®Œæ•´ã€åˆ†å¸ƒåˆ†æ•£æˆ–å­˜åœ¨å†²çªæ—¶çš„ä¸ç¡®å®šæ€§é‡åŒ–ã€‚ç ”ç©¶äººå‘˜å°†è¯¥æ¡†æ¶åº”ç”¨äº Bias è¯„ä¼°ï¼Œå¹¶åˆ©ç”¨äº¤é€šæ ‡å¿—è¯†åˆ«æ•°æ®é›†è¿›è¡Œäº†æ·±å…¥çš„å®éªŒéªŒè¯ã€‚å®éªŒç»“æœè¯å®ï¼Œè¯¥æ¡†æ¶ä¸ä»…èƒ½å‡†ç¡®æ•è· Class Imbalanceï¼Œè¿˜åœ¨ä¸­å¿ƒåŒ–å’Œ Federated åœºæ™¯ä¸‹å±•ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚é€šè¿‡æä¾›ä¸€ç§å¯è§£é‡Šä¸”å…·å¤‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥èƒ½åŠ›çš„è¯„ä¼°æœºåˆ¶ï¼Œè¯¥æ¡†æ¶ä¸ºæ„å»ºæ›´å…¬å¹³ã€é€æ˜çš„ AI ç³»ç»Ÿå¥ å®šäº†æ•°æ®å±‚é¢çš„ä¿¡ä»»åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ECML PKDD Bias Workshop '25",
      "pdf_url": "https://arxiv.org/pdf/2508.13813v1",
      "published_date": "2025-08-19 13:17:41 UTC",
      "updated_date": "2025-08-19 13:17:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:46:35.144766+00:00"
    },
    {
      "arxiv_id": "2508.13811v1",
      "title": "Quantifier Instantiations: To Mimic or To Revolt?",
      "title_zh": "é‡è¯å®ä¾‹åŒ–ï¼šæ•ˆæ³•è¿˜æ˜¯åå›ï¼Ÿ",
      "authors": [
        "Jan JakubÅ¯v",
        "MikolÃ¡Å¡ Janota"
      ],
      "abstract": "Quantified formulas pose a significant challenge for Satisfiability Modulo Theories (SMT) solvers due to their inherent undecidability. Existing instantiation techniques, such as e-matching, syntax-guided, model-based, conflict-based, and enumerative methods, often complement each other. This paper introduces a novel instantiation approach that dynamically learns from these techniques during solving. By treating observed instantiations as samples from a latent language, we use probabilistic context-free grammars to generate new, similar terms. Our method not only mimics successful past instantiations but also explores diversity by optionally inverting learned term probabilities, aiming to balance exploitation and exploration in quantifier reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ»¡è¶³æ¨¡ç†è®º(Satisfiability Modulo Theories, SMT)æ±‚è§£å™¨åœ¨å¤„ç†é‡åŒ–å…¬å¼(Quantified formulas)æ—¶çš„ä¸å¯åˆ¤å®šæ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„å®ä¾‹åŒ–(instantiation)æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨æ±‚è§£è¿‡ç¨‹ä¸­åŠ¨æ€å­¦ä¹ ç°æœ‰çš„å®ä¾‹åŒ–æŠ€æœ¯ï¼ˆå¦‚ e-matchingã€model-based å’Œ conflict-based ç­‰æ–¹æ³•ï¼‰ï¼Œå°†è§‚å¯Ÿåˆ°çš„å®ä¾‹åŒ–é¡¹è§†ä¸ºæ¥è‡ªæ½œåœ¨è¯­è¨€çš„æ ·æœ¬ã€‚ç ”ç©¶åˆ©ç”¨æ¦‚ç‡ä¸Šä¸‹æ–‡æ— å…³è¯­æ³•(Probabilistic context-free grammars, PCFG)æ¥ç”Ÿæˆæ–°çš„å€™é€‰é¡¹ï¼Œæ—¨åœ¨ç²¾å‡†æ¨¡ä»¿è¿‡å»æˆåŠŸçš„å®ä¾‹åŒ–æ¨¡å¼ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¯é€‰åœ°åè½¬å­¦ä¹ åˆ°çš„é¡¹æ¦‚ç‡æ¥é¼“åŠ±å¤šæ ·æ€§æ¢ç´¢ï¼Œåœ¨é‡åŒ–æ¨ç†ä¸­å®ç°äº†åˆ©ç”¨(exploitation)ä¸æ¢ç´¢(exploration)çš„å¹³è¡¡ã€‚è¿™ç§ç»“åˆäº†â€œæ¨¡ä»¿â€ä¸â€œåæŠ—â€çš„åŠ¨æ€å­¦ä¹ ç­–ç•¥ï¼Œä¸ºæå‡ SMT æ±‚è§£å™¨å¤„ç†å¤æ‚é‡åŒ–é€»è¾‘çš„æ€§èƒ½æä¾›äº†æ–°çš„è§†è§’å’ŒæŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to SMT 2025: 23rd International Workshop on Satisfiability Modulo Theories",
      "pdf_url": "https://arxiv.org/pdf/2508.13811v1",
      "published_date": "2025-08-19 13:16:25 UTC",
      "updated_date": "2025-08-19 13:16:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:46:36.650486+00:00"
    },
    {
      "arxiv_id": "2508.13805v1",
      "title": "Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs",
      "title_zh": "åŸºäºæç¤ºçš„å¤§è¯­è¨€æ¨¡å‹å•æ¬¡ç²¾å‡†é•¿åº¦æ§åˆ¶ç”Ÿæˆ",
      "authors": [
        "Juncheng Xie",
        "Hung-yi Lee"
      ],
      "abstract": "Controlling the length of text produced by large language models (LLMs) remains challenging: models frequently overshoot or undershoot explicit length instructions because they cannot reliably keep an internal token count. We present a prompt-based, one-shot strategy that compels an off-the-shelf LLM to generate exactly a desired number of tokens - words (English) or characters (Chinese) - without any fine-tuning or iterative sampling. The prompt appends countdown markers and explicit counting rules so that the model \"writes while counting.\" We evaluate on four settings: open-ended generation (1-1000 tokens), XSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH equal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps from below 30% under naive prompts to above 95% with our countdown prompt, surpassing the popular draft-then-revise baseline, while judged answer quality is preserved. These results show that precise length control can be achieved through prompt engineering alone, offering a lightweight alternative to training- or decoding-based methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éš¾ä»¥ç²¾ç¡®æ§åˆ¶è¾“å‡ºé•¿åº¦çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæç¤ºï¼ˆPrompt-basedï¼‰çš„ä¸€æ­¥æ³•ï¼ˆOne-shotï¼‰ç­–ç•¥ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨æç¤ºè¯­ä¸­å¼•å…¥å€’è®¡æ—¶æ ‡è®°ï¼ˆCountdown markersï¼‰å’Œæ˜¾å¼è®¡æ•°è§„åˆ™ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿâ€œè¾¹å†™è¾¹æ•°â€ï¼Œä»è€Œåœ¨æ— éœ€å¾®è°ƒï¼ˆFine-tuningï¼‰æˆ–è¿­ä»£é‡‡æ ·ï¼ˆIterative samplingï¼‰çš„æƒ…å†µä¸‹ç”ŸæˆæŒ‡å®šæ•°é‡çš„å•è¯æˆ–å­—ç¬¦ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œåœ¨ MT-Bench-LI åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥ç­–ç•¥å°† GPT-4.1 çš„é•¿åº¦ä¾ä»ç‡ä»ä¸è¶³ 30% æ˜¾è‘—æå‡è‡³ 95% ä»¥ä¸Šï¼Œè¶…è¶Šäº†å¸¸è§çš„å…ˆèµ·è‰åä¿®æ”¹ï¼ˆDraft-then-reviseï¼‰åŸºçº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥ç­–ç•¥åœ¨æ˜¾è‘—æå‡é•¿åº¦æ§åˆ¶ç²¾åº¦çš„åŒæ—¶ï¼Œä»èƒ½ä¿æŒç”Ÿæˆå†…å®¹çš„è´¨é‡ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†ä»…ä¾é æç¤ºå·¥ç¨‹ï¼ˆPrompt engineeringï¼‰å³å¯å®ç°ç²¾å‡†çš„é•¿åº¦æ§åˆ¶ï¼Œä¸ºéœ€è¦ä¸¥æ ¼é™åˆ¶å­—æ•°çš„åº”ç”¨åœºæ™¯æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”è½»é‡çº§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.13805v1",
      "published_date": "2025-08-19 13:12:01 UTC",
      "updated_date": "2025-08-19 13:12:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:46:43.961984+00:00"
    },
    {
      "arxiv_id": "2508.13796v1",
      "title": "A Fully Transformer Based Multimodal Framework for Explainable Cancer Image Segmentation Using Radiology Reports",
      "title_zh": "ä¸€ç§åˆ©ç”¨æ”¾å°„å­¦æŠ¥å‘Šè¿›è¡Œå¯è§£é‡Šç™Œç—‡å›¾åƒåˆ†å‰²çš„å…¨ Transformer å¤šæ¨¡æ€æ¡†æ¶",
      "authors": [
        "Enobong Adahada",
        "Isabel Sassoon",
        "Kate Hone",
        "Yongmin Li"
      ],
      "abstract": "We introduce Med-CTX, a fully transformer based multimodal framework for explainable breast cancer ultrasound segmentation. We integrate clinical radiology reports to boost both performance and interpretability. Med-CTX achieves exact lesion delineation by using a dual-branch visual encoder that combines ViT and Swin transformers, as well as uncertainty aware fusion. Clinical language structured with BI-RADS semantics is encoded by BioClinicalBERT and combined with visual features utilising cross-modal attention, allowing the model to provide clinically grounded, model generated explanations. Our methodology generates segmentation masks, uncertainty maps, and diagnostic rationales all at once, increasing confidence and transparency in computer assisted diagnosis. On the BUS-BRA dataset, Med-CTX achieves a Dice score of 99% and an IoU of 95%, beating existing baselines U-Net, ViT, and Swin. Clinical text plays a key role in segmentation accuracy and explanation quality, as evidenced by ablation studies that show a -5.4% decline in Dice score and -31% in CIDEr. Med-CTX achieves good multimodal alignment (CLIP score: 85%) and increased confi dence calibration (ECE: 3.2%), setting a new bar for trustworthy, multimodal medical architecture.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Med-CTXï¼Œè¿™æ˜¯ä¸€ç§å®Œå…¨åŸºäºTransformerçš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºå¯è§£é‡Šçš„ä¹³è…ºç™Œè¶…å£°å›¾åƒåˆ†å‰²ã€‚Med-CTXé€šè¿‡ç»“åˆViTå’ŒSwin transformersçš„åŒåˆ†æ”¯è§†è§‰ç¼–ç å™¨ä»¥åŠä¸ç¡®å®šæ€§æ„ŸçŸ¥èåˆï¼ˆuncertainty aware fusionï¼‰ï¼Œå®ç°äº†å¯¹ç—…ç¶çš„ç²¾ç¡®æç»˜ã€‚è¯¥æ¨¡å‹åˆ©ç”¨BioClinicalBERTç¼–ç å…·æœ‰BI-RADSè¯­ä¹‰çš„ä¸´åºŠæ”¾å°„æŠ¥å‘Šï¼Œå¹¶é€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼ˆcross-modal attentionï¼‰å°†æ–‡æœ¬ä¸è§†è§‰ç‰¹å¾ç»“åˆï¼Œä»è€Œç”Ÿæˆå…·æœ‰ä¸´åºŠä¾æ®çš„è¯Šæ–­è§£é‡Šã€‚Med-CTXèƒ½å¤ŸåŒæ—¶è¾“å‡ºåˆ†å‰²æ©ç ã€ä¸ç¡®å®šæ€§å›¾å’Œè¯Šæ–­ä¾æ®ï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—æœºè¾…åŠ©è¯Šæ–­çš„é€æ˜åº¦ä¸ä¿¡ä»»åº¦ã€‚åœ¨BUS-BRAæ•°æ®é›†ä¸Šï¼Œè¯¥æ¨¡å‹å–å¾—äº†99%çš„Dice scoreå’Œ95%çš„IoUï¼Œè¡¨ç°ä¼˜äºU-Netã€ViTå’ŒSwinç­‰åŸºçº¿æ¨¡å‹ã€‚å®éªŒç»“æœåŠæ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†ä¸´åºŠæ–‡æœ¬åœ¨æå‡åˆ†å‰²ç²¾åº¦ã€å¤šæ¨¡æ€å¯¹é½ï¼ˆCLIP score: 85%ï¼‰åŠç½®ä¿¡åº¦æ ¡å‡†ï¼ˆECE: 3.2%ï¼‰æ–¹é¢çš„å…³é”®ä½œç”¨ï¼Œä¸ºæ„å»ºå¯ä¿¡çš„å¤šæ¨¡æ€åŒ»ç–—æ¶æ„æ ‘ç«‹äº†æ–°æ ‡æ†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13796v1",
      "published_date": "2025-08-19 12:55:10 UTC",
      "updated_date": "2025-08-19 12:55:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:46:42.863503+00:00"
    },
    {
      "arxiv_id": "2508.13787v1",
      "title": "BetaWeb: Towards a Blockchain-enabled Trustworthy Agentic Web",
      "title_zh": "BetaWebï¼šè¿ˆå‘åŒºå—é“¾èµ‹èƒ½çš„å¯ä¿¡æ™ºèƒ½ä½“ç½‘ç»œ",
      "authors": [
        "Zihan Guo",
        "Yuanjian Zhou",
        "Chenyi Wang",
        "Linlin You",
        "Minjie Bian",
        "Weinan Zhang"
      ],
      "abstract": "The rapid development of large language models (LLMs) has significantly propelled the development of artificial intelligence (AI) agents, which are increasingly evolving into diverse autonomous entities, advancing the LLM-based multi-agent systems (LaMAS). However, current agentic ecosystems remain fragmented and closed. Establishing an interconnected and scalable paradigm for Agentic AI has become a critical prerequisite. Although Agentic Web proposes an open architecture to break the ecosystem barriers, its implementation still faces core challenges such as privacy protection, data management, and value measurement. Existing centralized or semi-centralized paradigms suffer from inherent limitations, making them inadequate for supporting large-scale, heterogeneous, and cross-domain autonomous interactions. To address these challenges, this paper introduces the blockchain-enabled trustworthy Agentic Web (BetaWeb). By leveraging the inherent strengths of blockchain, BetaWeb not only offers a trustworthy and scalable infrastructure for LaMAS but also has the potential to advance the Web paradigm from Web3 (centered on data ownership) towards Web3.5, which emphasizes ownership of agent capabilities and the monetization of intelligence. Beyond a systematic examination of the BetaWeb framework, this paper presents a five-stage evolutionary roadmap, outlining the path of LaMAS from passive execution to advanced collaboration and autonomous governance. We also conduct a comparative analysis of existing products and discuss key challenges of BetaWeb from multiple perspectives. Ultimately, we argue that deep integration between blockchain and LaMAS can lay the foundation for a resilient, trustworthy, and sustainably incentivized digital ecosystem. A summary of the enabling technologies for each stage is available at https://github.com/MatZaharia/BetaWeb.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BetaWebï¼Œä¸€ç§åŸºäºåŒºå—é“¾çš„å¯ä¿¡Agentic Webæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(LaMAS)é¢ä¸´çš„ç”Ÿæ€ç¢ç‰‡åŒ–ã€éšç§ä¿æŠ¤åŠä»·å€¼è¡¡é‡ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚BetaWebåˆ©ç”¨åŒºå—é“¾çš„å»ä¸­å¿ƒåŒ–ç‰¹æ€§ï¼Œä¸ºLaMASæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”å¯ä¿¡çš„åŸºç¡€è®¾æ–½ï¼Œå¹¶æ¨åŠ¨WebèŒƒå¼ä»å…³æ³¨æ•°æ®æ‰€æœ‰æƒçš„Web3å‘å¼ºè°ƒæ™ºèƒ½ä½“èƒ½åŠ›æ‰€æœ‰æƒä¸æ™ºèƒ½è´§å¸åŒ–çš„Web3.5æ¼”è¿›ã€‚è®ºæ–‡è¯¦ç»†é˜è¿°äº†BetaWebçš„ç³»ç»Ÿæ¶æ„ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåŒ…å«äº”ä¸ªé˜¶æ®µçš„æ¼”è¿›è·¯çº¿å›¾ï¼Œæ¶µç›–äº†ä»è¢«åŠ¨æ‰§è¡Œåˆ°è‡ªä¸»æ²»ç†çš„æ¼”è¿›è·¯å¾„ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡å¯¹æ¯”åˆ†æç°æœ‰äº§å“ï¼Œæ¢è®¨äº†BetaWebåœ¨å¤šç»´åº¦ä¸‹çš„å…³é”®æŒ‘æˆ˜ã€‚æœ€ç»ˆï¼Œè¯¥ç ”ç©¶è®ºè¯äº†åŒºå—é“¾ä¸LaMASçš„æ·±åº¦èåˆèƒ½å¤Ÿä¸ºæ„å»ºä¸€ä¸ªæ›´å…·éŸ§æ€§ã€å¯ä¿¡ä¸”å…·æœ‰æŒç»­æ¿€åŠ±æœºåˆ¶çš„æ•°å­—ç”Ÿæ€ç³»ç»Ÿå¥ å®šåŸºç¡€ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.MA",
      "comment": "A technical report with 21 pages, 3 figures, and 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.13787v1",
      "published_date": "2025-08-19 12:43:49 UTC",
      "updated_date": "2025-08-19 12:43:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:46:41.374313+00:00"
    },
    {
      "arxiv_id": "2508.13786v1",
      "title": "DegDiT: Controllable Audio Generation with Dynamic Event Graph Guided Diffusion Transformer",
      "title_zh": "DegDiTï¼šåŸºäºåŠ¨æ€äº‹ä»¶å›¾å¼•å¯¼æ‰©æ•£ Transformer çš„å¯æ§éŸ³é¢‘ç”Ÿæˆ",
      "authors": [
        "Yisu Liu",
        "Chenxing Li",
        "Wanqian Zhang",
        "Wenfu Wang",
        "Meng Yu",
        "Ruibo Fu",
        "Zheng Lin",
        "Weiping Wang",
        "Dong Yu"
      ],
      "abstract": "Controllable text-to-audio generation aims to synthesize audio from textual descriptions while satisfying user-specified constraints, including event types, temporal sequences, and onset and offset timestamps. This enables precise control over both the content and temporal structure of the generated audio. Despite recent progress, existing methods still face inherent trade-offs among accurate temporal localization, open-vocabulary scalability, and practical efficiency. To address these challenges, we propose DegDiT, a novel dynamic event graph-guided diffusion transformer framework for open-vocabulary controllable audio generation. DegDiT encodes the events in the description as structured dynamic graphs. The nodes in each graph are designed to represent three aspects: semantic features, temporal attributes, and inter-event connections. A graph transformer is employed to integrate these nodes and produce contextualized event embeddings that serve as guidance for the diffusion model. To ensure high-quality and diverse training data, we introduce a quality-balanced data selection pipeline that combines hierarchical event annotation with multi-criteria quality scoring, resulting in a curated dataset with semantic diversity. Furthermore, we present consensus preference optimization, facilitating audio generation through consensus among multiple reward signals. Extensive experiments on AudioCondition, DESED, and AudioTime datasets demonstrate that DegDiT achieves state-of-the-art performances across a variety of objective and subjective evaluation metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DegDiTï¼Œä¸€ç§åŸºäºåŠ¨æ€äº‹ä»¶å›¾å¼•å¯¼çš„æ‰©æ•£å˜æ¢å™¨ (Dynamic Event Graph Guided Diffusion Transformer) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¯æ§éŸ³é¢‘ç”Ÿæˆä¸­æ—¶é—´å®šä½å‡†ç¡®æ€§ã€å¼€æ”¾è¯æ±‡æ‰©å±•æ€§å’Œå®é™…æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡æŒ‘æˆ˜ã€‚DegDiT å°†æ–‡æœ¬æè¿°ä¸­çš„äº‹ä»¶ç¼–ç ä¸ºç»“æ„åŒ–åŠ¨æ€å›¾ï¼Œå…¶ä¸­çš„èŠ‚ç‚¹åˆ†åˆ«ä»£è¡¨è¯­ä¹‰ç‰¹å¾ã€æ—¶é—´å±æ€§å’Œäº‹ä»¶é—´è¿æ¥ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ Graph Transformer èåˆèŠ‚ç‚¹ä¿¡æ¯ä»¥äº§ç”Ÿä¸Šä¸‹æ–‡ç›¸å…³çš„äº‹ä»¶åµŒå…¥ï¼Œå¹¶ä»¥æ­¤å¼•å¯¼æ‰©æ•£æ¨¡å‹ç”ŸæˆéŸ³é¢‘ã€‚ä¸ºç¡®ä¿è®­ç»ƒæ•°æ®çš„é«˜è´¨é‡ä¸å¤šæ ·æ€§ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ç»“åˆå±‚æ¬¡åŒ–äº‹ä»¶æ ‡æ³¨ä¸å¤šå‡†åˆ™è´¨é‡è¯„åˆ†çš„æ•°æ®ç­›é€‰æµæ°´çº¿ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†å…±è¯†åå¥½ä¼˜åŒ– (Consensus Preference Optimization) ç­–ç•¥ï¼Œé€šè¿‡æ•´åˆå¤šç§å¥–åŠ±ä¿¡å·æ¥æå‡ç”Ÿæˆè´¨é‡ã€‚åœ¨ AudioConditionã€DESED å’Œ AudioTime ç­‰æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDegDiT åœ¨å¤šé¡¹å®¢è§‚å’Œä¸»è§‚è¯„ä»·æŒ‡æ ‡ä¸Šå‡å–å¾—äº† SOTA æ€§èƒ½ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13786v1",
      "published_date": "2025-08-19 12:41:15 UTC",
      "updated_date": "2025-08-19 12:41:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:46:50.151794+00:00"
    },
    {
      "arxiv_id": "2508.13776v2",
      "title": "Comparing Conditional Diffusion Models for Synthesizing Contrast-Enhanced Breast MRI from Pre-Contrast Images",
      "title_zh": "åŸºäºå¢å¼ºå‰å›¾åƒåˆæˆä¹³è…ºå¢å¼º MRI çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Sebastian Ibarra",
        "Javier del Riego",
        "Alessandro Catanese",
        "Julian Cuba",
        "Julian Cardona",
        "Nataly Leon",
        "Jonathan Infante",
        "Karim Lekadir",
        "Oliver Diaz",
        "Richard Osuala"
      ],
      "abstract": "Dynamic contrast-enhanced (DCE) MRI is essential for breast cancer diagnosis and treatment. However, its reliance on contrast agents introduces safety concerns, contraindications, increased cost, and workflow complexity. To this end, we present pre-contrast conditioned denoising diffusion probabilistic models to synthesize DCE-MRI, introducing, evaluating, and comparing a total of 22 generative model variants in both single-breast and full breast settings. Towards enhancing lesion fidelity, we introduce both tumor-aware loss functions and explicit tumor segmentation mask conditioning. Using a public multicenter dataset and comparing to respective pre-contrast baselines, we observe that subtraction image-based models consistently outperform post-contrast-based models across five complementary evaluation metrics. Apart from assessing the entire image, we also separately evaluate the region of interest, where both tumor-aware losses and segmentation mask inputs improve evaluation metrics. The latter notably enhance qualitative results capturing contrast uptake, albeit assuming access to tumor localization inputs that are not guaranteed to be available in screening settings. A reader study involving 2 radiologists and 4 MRI technologists confirms the high realism of the synthetic images, indicating an emerging clinical potential of generative contrast-enhancement. We share our codebase at https://github.com/sebastibar/conditional-diffusion-breast-MRI.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨é¢„å¯¹æ¯”å›¾åƒ(pre-contrast images)åˆæˆå¢å¼ºä¹³è…ºMRI(contrast-enhanced breast MRI)çš„æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸåŠ¨æ€å¯¹æ¯”å¢å¼ºMRI(DCE-MRI)ä¸­å¯¹æ¯”å‰‚å¸¦æ¥çš„å®‰å…¨é£é™©ã€æˆæœ¬åŠå·¥ä½œæµå¤æ‚æ€§é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†åŸºäºé¢„å¯¹æ¯”æ¡ä»¶æ§åˆ¶çš„å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹(denoising diffusion probabilistic models)ï¼Œå¹¶åœ¨å•ä¹³è…ºå’Œå…¨ä¹³è…ºè®¾ç½®ä¸‹è¯„ä¼°å¹¶å¯¹æ¯”äº†22ç§ç”Ÿæˆæ¨¡å‹å˜ä½“ã€‚ä¸ºè¿›ä¸€æ­¥æå‡ç—…ç¶ä¿çœŸåº¦ï¼Œç ”ç©¶å¼•å…¥äº†è‚¿ç˜¤æ„ŸçŸ¥æŸå¤±å‡½æ•°(tumor-aware loss functions)å’Œæ˜¾å¼è‚¿ç˜¤åˆ†å‰²æ©ç æ¡ä»¶(explicit tumor segmentation mask conditioning)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå‡å½±å›¾åƒçš„æ¨¡å‹(subtraction image-based models)åœ¨äº”é¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šä¸€è‡´ä¼˜äºç›´æ¥ç”Ÿæˆå¯¹æ¯”åå›¾åƒçš„æ¨¡å‹ã€‚åœ¨æ„Ÿå…´è¶£åŒºåŸŸ(region of interest)çš„è¯„ä¼°ä¸­ï¼Œè‚¿ç˜¤æ„ŸçŸ¥æŸå¤±å’Œåˆ†å‰²æ©ç è¾“å…¥æ˜¾è‘—æ”¹å–„äº†å®šé‡æŒ‡æ ‡ï¼Œå¹¶å¢å¼ºäº†å¯¹å¯¹æ¯”å‰‚æ‘„å–(contrast uptake)ç‰¹å¾çš„å®šæ€§æ¨¡æ‹Ÿã€‚ç”±æ”¾å°„ç§‘åŒ»ç”Ÿå’ŒæŠ€å¸ˆå‚ä¸çš„è¯»è€…ç ”ç©¶(reader study)è¯å®äº†åˆæˆå›¾åƒçš„é«˜åº¦çœŸå®æ„Ÿï¼Œå±•ç¤ºäº†ç”Ÿæˆå¼å¯¹æ¯”å¢å¼ºæŠ€æœ¯åœ¨ä¸´åºŠè¯Šæ–­ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "13 pages, 5 figures, submitted and accepted to MICCAI Deepbreath workshop 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.13776v2",
      "published_date": "2025-08-19 12:24:55 UTC",
      "updated_date": "2025-09-15 09:58:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:46:51.547863+00:00"
    },
    {
      "arxiv_id": "2508.13774v1",
      "title": "Agentic DraCor and the Art of Docstring Engineering: Evaluating MCP-empowered LLM Usage of the DraCor API",
      "title_zh": "Agentic DraCor ä¸æ–‡æ¡£å­—ç¬¦ä¸²å·¥ç¨‹è‰ºæœ¯ï¼šè¯„ä¼° MCP èµ‹èƒ½çš„å¤§è¯­è¨€æ¨¡å‹å¯¹ DraCor API çš„ä½¿ç”¨",
      "authors": [
        "Peer Trilcke",
        "Ingo BÃ¶rner",
        "Henny Sluyter-GÃ¤thje",
        "Daniil Skorinkin",
        "Frank Fischer",
        "Carsten Milling"
      ],
      "abstract": "This paper reports on the implementation and evaluation of a Model Context Protocol (MCP) server for DraCor, enabling Large Language Models (LLM) to autonomously interact with the DraCor API. We conducted experiments focusing on tool selection and application by the LLM, employing a qualitative approach that includes systematic observation of prompts to understand how LLMs behave when using MCP tools, evaluating \"Tool Correctness\", \"Tool-Calling Efficiency\", and \"Tool-Use Reliability\". Our findings highlight the importance of \"Docstring Engineering\", defined as reflexively crafting tool documentation to optimize LLM-tool interaction. Our experiments demonstrate both the promise of agentic AI for research in Computational Literary Studies and the essential infrastructure development needs for reliable Digital Humanities infrastructures.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸ºDraCorå®ç°å’Œè¯„ä¼°æ¨¡å‹ä¸Šä¸‹æ–‡åè®®(Model Context Protocol, MCP)æœåŠ¡å™¨ï¼Œä½¿å¤§è¯­è¨€æ¨¡å‹(LLMs)èƒ½å¤Ÿè‡ªä¸»ä¸DraCor APIè¿›è¡Œäº¤äº’ã€‚å®éªŒé‡ç‚¹è¯„ä¼°äº†LLMåœ¨å·¥å…·é€‰æ‹©å’Œåº”ç”¨æ–¹é¢çš„è¡¨ç°ï¼Œé€šè¿‡ç³»ç»Ÿæ€§çš„æç¤ºè¯è§‚å¯Ÿï¼Œå®šæ€§åˆ†æäº†æ¨¡å‹åœ¨å·¥å…·æ­£ç¡®æ€§(Tool Correctness)ã€è°ƒç”¨æ•ˆç‡(Tool-Calling Efficiency)å’Œä½¿ç”¨å¯é æ€§(Tool-Use Reliability)æ–¹é¢çš„è¡Œä¸ºã€‚ç ”ç©¶æå‡ºäº†â€œæ–‡æ¡£å­—ç¬¦ä¸²å·¥ç¨‹â€(Docstring Engineering)çš„æ¦‚å¿µï¼Œå³é€šè¿‡åæ€æ€§åœ°ç¼–å†™å·¥å…·æ–‡æ¡£æ¥ä¼˜åŒ–LLMä¸å·¥å…·ä¹‹é—´çš„äº¤äº’ã€‚å®éªŒç»“æœä¸ä»…å±•ç¤ºäº†æ™ºèƒ½ä½“AI(Agentic AI)åœ¨è®¡ç®—æ–‡å­¦ç ”ç©¶(Computational Literary Studies)é¢†åŸŸçš„åº”ç”¨å‰æ™¯ï¼Œè¿˜å¼ºè°ƒäº†å»ºç«‹å¯é çš„æ•°å­—äººæ–‡å­¦ç§‘(Digital Humanities)åŸºç¡€è®¾æ–½çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Preprint, submitted to the 2nd Workshop on Computational Drama Analysis at DraCor Summit 2025, September 03, 2025, Berlin, Germany",
      "pdf_url": "https://arxiv.org/pdf/2508.13774v1",
      "published_date": "2025-08-19 12:21:21 UTC",
      "updated_date": "2025-08-19 12:21:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:46:56.846554+00:00"
    },
    {
      "arxiv_id": "2508.13773v2",
      "title": "PENGUIN: Enhancing Transformer with Periodic-Nested Group Attention for Long-term Time Series Forecasting",
      "title_zh": "PENGUINï¼šé€šè¿‡å‘¨æœŸåµŒå¥—åˆ†ç»„æ³¨æ„åŠ›å¢å¼º Transformer çš„é•¿æœŸæ—¶é—´åºåˆ—é¢„æµ‹",
      "authors": [
        "Tian Sun",
        "Yuqi Chen",
        "Weiwei Sun"
      ],
      "abstract": "Long-term time series forecasting (LTSF) is a fundamental task with wide-ranging applications. Although Transformer-based models have made significant breakthroughs in forecasting, their effectiveness for time series forecasting remains debatable. In this paper, we revisit the significance of self-attention and propose a simple yet effective mechanism, Periodic-Nested Group Attention, namely PENGUIN. Our approach highlights the importance of explicitly modeling periodic patterns and incorporating relative attention bias for effective time series modeling. To this end, we introduce a periodic-nested relative attention bias that captures periodic structures directly. To handle multiple coexisting periodicities (e.g., daily and weekly cycles), we design a grouped attention mechanism, where each group targets a specific periodicity using a multi-query attention mechanism. Extensive experiments across diverse benchmarks demonstrate that PENGUIN consistently outperforms both MLP-based and Transformer-based models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é•¿æœŸæ—¶é—´åºåˆ—é¢„æµ‹ (Long-term Time Series Forecasting, LTSF) ä¸­ Transformer æ¨¡å‹æœ‰æ•ˆæ€§çš„äº‰è®®ï¼Œæå‡ºäº†åä¸º PENGUIN (Periodic-Nested Group Attention) çš„æ–°å‹æ³¨æ„åŠ›æœºåˆ¶ã€‚è¯¥æ–¹æ³•å¼ºè°ƒäº†æ˜¾å¼å»ºæ¨¡å‘¨æœŸæ€§æ¨¡å¼çš„é‡è¦æ€§ï¼Œå¹¶å¼•å…¥äº†å‘¨æœŸåµŒå¥—ç›¸å¯¹æ³¨æ„åŠ›åå·® (Periodic-Nested Relative Attention Bias) æ¥ç›´æ¥æ•æ‰æ—¶é—´åºåˆ—ä¸­çš„å‘¨æœŸç»“æ„ã€‚ä¸ºäº†æœ‰æ•ˆå¤„ç†å¦‚æ—¥å¾ªç¯å’Œå‘¨å¾ªç¯ç­‰å¤šç§å…±å­˜çš„å‘¨æœŸæ€§ï¼ŒPENGUIN è®¾è®¡äº†ä¸€ç§åˆ†ç»„æ³¨æ„åŠ›æœºåˆ¶ (Grouped Attention)ï¼Œä½¿æ¯ä¸ªç»„èƒ½å¤Ÿé€šè¿‡å¤šæŸ¥è¯¢æ³¨æ„åŠ› (Multi-Query Attention) é’ˆå¯¹ç‰¹å®šçš„å‘¨æœŸæ€§è¿›è¡Œå»ºæ¨¡ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPENGUIN çš„é¢„æµ‹æ€§èƒ½ä¸€è‡´ä¼˜äºç°æœ‰çš„åŸºäº MLP å’Œ Transformer çš„æ¨¡å‹ã€‚è¿™ä¸€ç ”ç©¶è¯æ˜äº†é€šè¿‡ä¼˜åŒ–è‡ªæ³¨æ„åŠ›ç»“æ„ä»¥æ˜¾å¼æ•æ‰å‘¨æœŸè§„å¾‹ï¼Œå¯ä»¥æ˜¾è‘—æå‡å¤§è§„æ¨¡æ—¶é—´åºåˆ—å»ºæ¨¡çš„å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13773v2",
      "published_date": "2025-08-19 12:19:12 UTC",
      "updated_date": "2025-08-22 15:38:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:46:56.658928+00:00"
    },
    {
      "arxiv_id": "2508.14134v2",
      "title": "ERIS: An Energy-Guided Feature Disentanglement Framework for Out-of-Distribution Time Series Classification",
      "title_zh": "ERISï¼šé¢å‘åˆ†å¸ƒå¤–æ—¶é—´åºåˆ—åˆ†ç±»çš„èƒ½é‡å¼•å¯¼ç‰¹å¾è§£è€¦æ¡†æ¶",
      "authors": [
        "Xin Wu",
        "Fei Teng",
        "Ji Zhang",
        "Xingwang Li",
        "Yuxuan Liang"
      ],
      "abstract": "An ideal time series classification (TSC) should be able to capture invariant representations, but achieving reliable performance on out-of-distribution (OOD) data remains a core obstacle. This obstacle arises from the way models inherently entangle domain-specific and label-relevant features, resulting in spurious correlations. While feature disentanglement aims to solve this, current methods are largely unguided, lacking the semantic direction required to isolate truly universal features. To address this, we propose an end-to-end Energy-Regularized Information for Shift-Robustness (ERIS) framework to enable guided and reliable feature disentanglement. The core idea is that effective disentanglement requires not only mathematical constraints but also semantic guidance to anchor the separation process. ERIS incorporates three key mechanisms to achieve this goal. Specifically, we first introduce an energy-guided calibration mechanism, which provides crucial semantic guidance for the separation, enabling the model to self-calibrate. Additionally, a weight-level orthogonality strategy enforces structural independence between domain-specific and label-relevant features, thereby mitigating their interference. Moreover, an auxiliary adversarial generalization mechanism enhances robustness by injecting structured perturbations. Experiments across four benchmarks demonstrate that ERIS achieves a statistically significant improvement over state-of-the-art baselines, consistently securing the top performance rank.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ERISï¼Œä¸€ç§èƒ½é‡å¼•å¯¼çš„ç‰¹å¾è§£è€¦(Feature Disentanglement)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ—¶é—´åºåˆ—åˆ†ç±»(TSC)åœ¨åˆ†å¸ƒå¤–(OOD)åœºæ™¯ä¸‹å› ç‰¹å¾çº ç¼ å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰è§£è€¦æ–¹æ³•ç¼ºä¹è¯­ä¹‰å¼•å¯¼çš„å±€é™ï¼ŒERISé€šè¿‡å¼•å…¥èƒ½é‡å¼•å¯¼æ ¡å‡†æœºåˆ¶ä¸ºç‰¹å¾åˆ†ç¦»æä¾›è¯­ä¹‰é”šç‚¹ï¼Œä»è€Œå®ç°æ›´ç²¾å‡†çš„ç‰¹å¾æå–ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨æƒé‡çº§æ­£äº¤ç­–ç•¥ç¡®ä¿é¢†åŸŸç‰¹å®šç‰¹å¾ä¸æ ‡ç­¾ç›¸å…³ç‰¹å¾çš„ç»“æ„ç‹¬ç«‹æ€§ï¼Œå¹¶ç»“åˆè¾…åŠ©å¯¹æŠ—æ³›åŒ–æœºåˆ¶è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒERISåœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„SOTAæ¨¡å‹ã€‚è¯¥ç ”ç©¶æˆåŠŸè¯æ˜äº†è¯­ä¹‰å¼•å¯¼åœ¨ç‰¹å¾è§£è€¦ä¸­çš„é‡è¦æ€§ï¼Œä¸ºæ„å»ºç¨³å¥çš„æ—¶é—´åºåˆ—åˆ†ç±»ç³»ç»Ÿæä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14134v2",
      "published_date": "2025-08-19 12:13:41 UTC",
      "updated_date": "2025-09-26 11:04:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:47:16.262510+00:00"
    },
    {
      "arxiv_id": "2508.14133v1",
      "title": "Automated surgical planning with nnU-Net: delineation of the anatomy in hepatobiliary phase MRI",
      "title_zh": "åŸºäº nnU-Net çš„è‡ªåŠ¨åŒ–æ‰‹æœ¯è§„åˆ’ï¼šè‚èƒ†æœŸ MRI è§£å‰–ç»“æ„å‹¾ç”»",
      "authors": [
        "Karin A. Olthof",
        "Matteo Fusagli",
        "Bianca GÃ¼ttner",
        "Tiziano Natali",
        "Bram Westerink",
        "Stefanie Speidel",
        "Theo J. M. Ruers",
        "Koert F. D. Kuhlmann",
        "Andrey Zhylka"
      ],
      "abstract": "Background: The aim of this study was to develop and evaluate a deep learning-based automated segmentation method for hepatic anatomy (i.e., parenchyma, tumors, portal vein, hepatic vein and biliary tree) from the hepatobiliary phase of gadoxetic acid-enhanced MRI. This method should ease the clinical workflow of preoperative planning.\n  Methods: Manual segmentation was performed on hepatobiliary phase MRI scans from 90 consecutive patients who underwent liver surgery between January 2020 and October 2023. A deep learning network (nnU-Net v1) was trained on 72 patients with an extra focus on thin structures and topography preservation. Performance was evaluated on an 18-patient test set by comparing automated and manual segmentations using Dice similarity coefficient (DSC). Following clinical integration, 10 segmentations (assessment dataset) were generated using the network and manually refined for clinical use to quantify required adjustments using DSC.\n  Results: In the test set, DSCs were 0.97+/-0.01 for liver parenchyma, 0.80+/-0.04 for hepatic vein, 0.79+/-0.07 for biliary tree, 0.77+/-0.17 for tumors, and 0.74+/-0.06 for portal vein. Average tumor detection rate was 76.6+/-24.1%, with a median of one false-positive per patient. The assessment dataset showed minor adjustments were required for clinical use of the 3D models, with high DSCs for parenchyma (1.00+/-0.00), portal vein (0.98+/-0.01) and hepatic vein (0.95+/-0.07). Tumor segmentation exhibited greater variability (DSC 0.80+/-0.27). During prospective clinical use, the model detected three additional tumors initially missed by radiologists.\n  Conclusions: The proposed nnU-Net-based segmentation method enables accurate and automated delineation of hepatic anatomy. This enables 3D planning to be applied efficiently as a standard-of-care for every patient undergoing liver surgery.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘å¹¶è¯„ä¼°äº†ä¸€é¡¹åŸºäºæ·±åº¦å­¦ä¹ ç½‘ç»œ nnU-Net v1 çš„è‡ªåŠ¨åŒ–åˆ†å‰²æ–¹æ³•ï¼Œæ—¨åœ¨ä»é’†å¡é…¸äºŒé’ å¢å¼º MRI çš„è‚èƒ†æœŸï¼ˆhepatobiliary phaseï¼‰å½±åƒä¸­ç²¾ç¡®å‹¾ç”»è‚è„è§£å‰–ç»“æ„ã€‚è¯¥æ–¹æ³•æ¶µç›–äº†è‚å®è´¨ï¼ˆparenchymaï¼‰ã€è‚¿ç˜¤ï¼ˆtumorsï¼‰ã€é—¨é™è„‰ï¼ˆportal veinï¼‰ã€è‚é™è„‰ï¼ˆhepatic veinï¼‰å’Œèƒ†ç®¡æ ‘ï¼ˆbiliary treeï¼‰çš„è¯†åˆ«ï¼Œå¹¶ç‰¹åˆ«å…³æ³¨ç»†å¾®ç»“æ„ä¸æ‹“æ‰‘ç‰¹å¾çš„ä¿ç•™ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè‚å®è´¨ã€è‚é™è„‰å’Œèƒ†ç®¡æ ‘çš„ Dice ç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰åˆ†åˆ«è¾¾åˆ°äº† 0.97ã€0.80 å’Œ 0.79ã€‚åœ¨ä¸´åºŠåº”ç”¨è¯„ä¼°ä¸­ï¼Œè¯¥æ¨¡å‹ç”Ÿæˆçš„ 3D æ¨¡å‹ä»…éœ€æå°‘é‡æ‰‹åŠ¨è°ƒæ•´å³å¯æ»¡è¶³ä¸´åºŠéœ€æ±‚ï¼Œä¸”åœ¨å›é¡¾æ€§ç ”ç©¶ä¸­æˆåŠŸå‘ç°äº†ä¸‰å¤„æ”¾å°„ç§‘åŒ»ç”Ÿæœ€åˆé—æ¼çš„è‚¿ç˜¤ã€‚è¿™ä¸€åŸºäº nnU-Net çš„æ–¹æ¡ˆå®ç°äº†é«˜æ•ˆä¸”ç²¾ç¡®çš„è‚è„è§£å‰–è‡ªåŠ¨åŒ–å‹¾ç”»ï¼Œä¸ºå°† 3D æœ¯å‰è§„åˆ’ä½œä¸ºè‚è„æ‰‹æœ¯çš„æ ‡å‡†æµç¨‹å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "14 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.14133v1",
      "published_date": "2025-08-19 11:58:19 UTC",
      "updated_date": "2025-08-19 11:58:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:47:13.749159+00:00"
    },
    {
      "arxiv_id": "2508.13757v1",
      "title": "COMPASS: A Multi-Dimensional Benchmark for Evaluating Code Generation in Large Language Models",
      "title_zh": "COMPASSï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ä»£ç ç”Ÿæˆçš„å¤šç»´åº¦åŸºå‡†",
      "authors": [
        "James Meaden",
        "MichaÅ‚ Jarosz",
        "Piotr JodÅ‚owski",
        "Grigori Melnik"
      ],
      "abstract": "Current code generation benchmarks focus primarily on functional correctness while overlooking two critical aspects of real-world programming: algorithmic efficiency and code quality. We introduce COMPASS (COdility's Multi-dimensional Programming ASSessment), a comprehensive evaluation framework that assesses code generation across three dimensions: correctness, efficiency, and quality. COMPASS consists of 50 competitive programming problems from real Codility competitions, providing authentic human baselines from 393,150 submissions. Unlike existing benchmarks that treat algorithmically inefficient solutions identically to optimal ones provided they pass test cases, COMPASS systematically evaluates runtime efficiency and code quality using industry-standard analysis tools. Our evaluation of three leading reasoning-enhanced models, Anthropic Claude Opus 4, Google Gemini 2.5 Pro, and OpenAI O4-Mini-High, reveals that models achieving high correctness scores do not necessarily produce efficient algorithms or maintainable code. These findings highlight the importance of evaluating more than just correctness to truly understand the real-world capabilities of code generation models. COMPASS serves as a guiding framework, charting a path for future research toward AI systems that are robust, reliable, and ready for production use.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† COMPASS (COdility's Multi-dimensional Programming ASSessment)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä»æ­£ç¡®æ€§ (correctness)ã€ç®—æ³•æ•ˆç‡ (efficiency) å’Œä»£ç è´¨é‡ (quality) ä¸‰ä¸ªç»´åº¦å…¨é¢è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ä»£ç ç”Ÿæˆèƒ½åŠ›çš„åŸºå‡†æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…å« 50 ä¸ªæ¥è‡ªçœŸå® Codility ç«èµ›çš„ç¼–ç¨‹é—®é¢˜ï¼Œå¹¶æ•´åˆäº†çº¦ 39 ä¸‡ä»½äººç±»æäº¤è®°å½•ä½œä¸ºçœŸå®çš„åŸºå‡†å‚è€ƒï¼Œå¼¥è¡¥äº†ç°æœ‰è¯„ä¼°ä»…å…³æ³¨åŠŸèƒ½æ­£ç¡®æ€§è€Œå¿½è§†ç”Ÿäº§ç¯å¢ƒè¦æ±‚çš„ç¼ºé™·ã€‚é€šè¿‡å¯¹ Anthropic Claude Opus 4ã€Google Gemini 2.5 Pro å’Œ OpenAI O4-Mini-High ç­‰é¢†å…ˆæ¨ç†æ¨¡å‹çš„è¯„ä¼°ï¼Œç ”ç©¶å‘ç°å³ä¾¿åœ¨æ­£ç¡®æ€§å¾—åˆ†è¾ƒé«˜çš„æ¨¡å‹ä¸­ï¼Œå…¶ç”Ÿæˆçš„ç®—æ³•æ•ˆç‡å’Œä»£ç å¯ç»´æŠ¤æ€§ä¹Ÿæœªå¿…ä¼˜ç§€ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚ç¼–ç¨‹ä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼Œå¼ºè°ƒäº†å¤šç»´åº¦è¯„ä¼°å¯¹è¡¡é‡ä»£ç ç”Ÿæˆæ¨¡å‹çœŸå®èƒ½åŠ›çš„é‡è¦æ€§ã€‚COMPASS ä¸ºæœªæ¥å¼€å‘æ›´é²æ£’ã€å¯é ä¸”ç¬¦åˆç”Ÿäº§æ ‡å‡†çš„ AI ç³»ç»Ÿæä¾›äº†å…³é”®çš„æŒ‡å¯¼æ¡†æ¶ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13757v1",
      "published_date": "2025-08-19 11:55:07 UTC",
      "updated_date": "2025-08-19 11:55:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:47:14.653234+00:00"
    },
    {
      "arxiv_id": "2508.13755v4",
      "title": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration",
      "title_zh": "RLVR ä¸­çš„æ·±å¹¿ååŒï¼šé€šè¿‡è‡ªé€‚åº”æ¢ç´¢é‡Šæ”¾å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¢ç›Š",
      "authors": [
        "Zhicheng Yang",
        "Zhijiang Guo",
        "Yinya Huang",
        "Yongxin Wang",
        "Dongchun Xie",
        "Yiwei Wang",
        "Xiaodan Liang",
        "Jing Tang"
      ],
      "abstract": "Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models, yet its full potential is hindered by two under-explored dimensions: Depth-the hardest problem a model can sample; Breadth-the number of instances consumed in a single iteration. We dissect the popular GRPO algorithm and reveal a systematic bias: the cumulative-advantage disproportionately weights samples with medium accuracy, while down-weighting the low-accuracy instances that are crucial for pushing reasoning boundaries. To rectify the depth neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which re-weights hard problems through targeted multi-stage rollouts, thereby increasing the number of positive rollouts for hard problems. Empirically, naively enlarging rollout size only accelerates convergence and even hurts Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra inference cost at convergence. Just as we adaptively expanded the depth of exploration, we now ask whether aggressively scaling the breadth of training data can further amplify reasoning gains. To this end, we intensely scale batch size and replace PPO's mini-batch iterations with full-batch updates over multiple epochs. Increasing breadth significantly enhances Pass@1 performance. Large-breadth training sustains high token-level entropy, indicating continued exploration and reduced gradient noise. We further present DARS-B, which augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K and Pass@1. The results confirm that breadth and adaptive exploration across depth operate as orthogonal dimensions in RLVR, which are key to unleashing the reasoning power of RLVR.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¸¦éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RLVR)åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹(LLM)æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶é’ˆå¯¹ç°æœ‰GRPOç®—æ³•å¿½è§†ä½å‡†ç¡®åº¦éš¾é¢˜(Depth)å’Œè®­ç»ƒå¹¿åº¦(Breadth)ä¸è¶³çš„é—®é¢˜è¿›è¡Œäº†æ·±å…¥å‰–æã€‚ä¸ºäº†çº æ­£æ·±åº¦æ¢ç´¢ä¸­çš„åå·®ï¼Œä½œè€…æå‡ºäº†éš¾åº¦è‡ªé€‚åº”å±•å¼€é‡‡æ ·(DARS)æ–¹æ³•ï¼Œé€šè¿‡å¤šé˜¶æ®µå®šå‘å±•å¼€æ¥å¢åŠ éš¾é¢˜çš„æ­£å‘é‡‡æ ·æ¯”ä¾‹ã€‚å®éªŒè¯æ˜ï¼Œç›¸è¾ƒäºç›²ç›®æ‰©å¤§é‡‡æ ·è§„æ¨¡å¸¦æ¥çš„æ€§èƒ½å—æŸï¼ŒDARSèƒ½å¤Ÿåœ¨ä¸å¢åŠ æ”¶æ•›æ¨ç†æˆæœ¬çš„å‰æä¸‹æŒç»­æå‡Pass@KæŒ‡æ ‡ã€‚åœ¨è®­ç»ƒå¹¿åº¦æ–¹é¢ï¼Œç ”ç©¶é€šè¿‡å¤§å¹…ç¼©æ”¾Batch Sizeå¹¶é‡‡ç”¨å…¨æ‰¹æ¬¡æ›´æ–°ç­–ç•¥ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„Pass@1è¡¨ç°å¹¶ç»´æŒäº†è¾ƒé«˜çš„Tokenå±‚çº§ç†µã€‚è¿›ä¸€æ­¥æå‡ºçš„DARS-Bæ¡†æ¶é€šè¿‡ååŒæ·±åº¦ä¸å¹¿åº¦çš„ä¼˜åŠ¿ï¼Œå®ç°äº†Pass@Kä¸Pass@1çš„åŒæ­¥å¢é•¿ã€‚è¿™ä¸€å‘ç°è¯å®äº†æ·±åº¦è‡ªé€‚åº”æ¢ç´¢ä¸è®­ç»ƒå¹¿åº¦æ˜¯RLVRä¸­ä¸¤ä¸ªæ­£äº¤ä¸”å…³é”®çš„ç»´åº¦ï¼Œå…±åŒå†³å®šäº†æ¨¡å‹æ¨ç†èƒ½åŠ›çš„é‡Šæ”¾ä¸Šé™ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 14 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.13755v4",
      "published_date": "2025-08-19 11:51:40 UTC",
      "updated_date": "2025-10-06 11:12:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:47:21.255366+00:00"
    },
    {
      "arxiv_id": "2508.13754v1",
      "title": "Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making",
      "title_zh": "é¢å‘åŒ»ç–—å†³ç­–çš„ä¸“é•¿æ„ŸçŸ¥å¤šå¤§è¯­è¨€æ¨¡å‹æ‹›å‹Ÿä¸åä½œ",
      "authors": [
        "Liuxin Bao",
        "Zhihao Peng",
        "Xiaofei Zhou",
        "Runmin Cong",
        "Jiyong Zhang",
        "Yixuan Yuan"
      ],
      "abstract": "Medical Decision-Making (MDM) is a complex process requiring substantial domain-specific expertise to effectively synthesize heterogeneous and complicated clinical information. While recent advancements in Large Language Models (LLMs) show promise in supporting MDM, single-LLM approaches are limited by their parametric knowledge constraints and static training corpora, failing to robustly integrate the clinical information. To address this challenge, we propose the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC) framework to enhance the accuracy and reliability of MDM systems. It operates in two stages: (i) expertise-aware agent recruitment and (ii) confidence- and adversarial-driven multi-agent collaboration. Specifically, in the first stage, we use a publicly available corpus to construct an LLM expertise table for capturing expertise-specific strengths of multiple LLMs across medical department categories and query difficulty levels. This table enables the subsequent dynamic selection of the optimal LLMs to act as medical expert agents for each medical query during the inference phase. In the second stage, we employ selected agents to generate responses with self-assessed confidence scores, which are then integrated through the confidence fusion and adversarial validation to improve diagnostic reliability. We evaluate our EMRC framework on three public MDM datasets, where the results demonstrate that our EMRC outperforms state-of-the-art single- and multi-LLM methods, achieving superior diagnostic performance. For instance, on the MMLU-Pro-Health dataset, our EMRC achieves 74.45% accuracy, representing a 2.69% improvement over the best-performing closed-source model GPT- 4-0613, which demonstrates the effectiveness of our expertise-aware agent recruitment strategy and the agent complementarity in leveraging each LLM's specialized capabilities.",
      "tldr_zh": "é’ˆå¯¹åŒ»ç–—å†³ç­–(Medical Decision-Making)ä¸­å•ä¸€å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å­˜åœ¨çš„å‚æ•°åŒ–çŸ¥è¯†é™åˆ¶å’Œéš¾ä»¥æœ‰æ•ˆæ•´åˆå¤æ‚ä¸´åºŠä¿¡æ¯çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†EMRCï¼ˆExpertise-aware Multi-LLM Recruitment and Collaborationï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸¤ä¸ªé˜¶æ®µæå‡è¯Šæ–­çš„å‡†ç¡®æ€§ä¸å¯é æ€§ï¼šé¦–å…ˆåˆ©ç”¨æ„å»ºçš„LLMä¸“ä¸šçŸ¥è¯†è¡¨ï¼Œæ ¹æ®åŒ»ç–—ç§‘å®¤ç±»åˆ«å’ŒæŸ¥è¯¢éš¾åº¦åŠ¨æ€æ‹›å‹Ÿæœ€åˆé€‚çš„æ¨¡å‹ä½œä¸ºä¸“å®¶æ™ºèƒ½ä½“ï¼›éšåé€šè¿‡ç½®ä¿¡åº¦èåˆ(Confidence fusion)å’Œå¯¹æŠ—æ€§éªŒè¯(Adversarial validation)æœºåˆ¶ï¼Œæ•´åˆå„æ™ºèƒ½ä½“çš„è‡ªè¯„ä¼°ç½®ä¿¡åº¦å¾—åˆ†ã€‚å®éªŒåœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœè¡¨æ˜EMRCçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„å•æ¨¡å‹å’Œå¤šæ¨¡å‹æ–¹æ³•ã€‚åœ¨MMLU-Pro-Healthæ•°æ®é›†ä¸Šï¼ŒEMRCè¾¾åˆ°äº†74.45%çš„å‡†ç¡®ç‡ï¼Œç›¸æ¯”GPT-4-0613æå‡äº†2.69%ï¼Œå……åˆ†éªŒè¯äº†ä¸“ä¸šæ„ŸçŸ¥å‹æ‹›å‹Ÿç­–ç•¥å’Œå¤šæ™ºèƒ½ä½“èƒ½åŠ›äº’è¡¥åœ¨å¤æ‚åŒ»ç–—åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.13754v1",
      "published_date": "2025-08-19 11:51:15 UTC",
      "updated_date": "2025-08-19 11:51:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:47:23.464074+00:00"
    },
    {
      "arxiv_id": "2508.13744v1",
      "title": "Mitigating Cross-Image Information Leakage in LVLMs for Multi-Image Tasks",
      "title_zh": "ç¼“è§£å¤šå›¾åƒä»»åŠ¡ä¸­ LVLMs çš„è·¨å›¾åƒä¿¡æ¯æ³„æ¼",
      "authors": [
        "Yeji Park",
        "Minyoung Lee",
        "Sanghyuk Chun",
        "Junsuk Choe"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) demonstrate strong performance on single-image tasks. However, we observe that their performance degrades significantly when handling multi-image inputs. This occurs because visual cues from different images become entangled in the model's output. We refer to this phenomenon as cross-image information leakage. To address this issue, we propose FOCUS, a training-free and architecture-agnostic decoding strategy that mitigates cross-image information leakage during inference. FOCUS sequentially masks all but one image with random noise, guiding the model to focus on the single clean image. We repeat this process across all target images to obtain logits under partially masked contexts. These logits are aggregated and then contrastively refined using a noise-only reference input, which suppresses the leakage and yields more accurate outputs. FOCUS consistently improves performance across four multi-image benchmarks and diverse LVLM families. This demonstrates that FOCUS offers a general and practical solution for enhancing multi-image reasoning without additional training or architectural modifications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)åœ¨å¤„ç†å¤šå›¾åƒä»»åŠ¡æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™çš„é—®é¢˜ï¼Œå¹¶æŒ‡å‡ºå…¶æ ¸å¿ƒåŸå› åœ¨äºä¸åŒå›¾åƒçš„è§†è§‰çº¿ç´¢åœ¨æ¨¡å‹è¾“å‡ºä¸­å‘ç”Ÿçº ç¼ ï¼Œå³è·¨å›¾åƒä¿¡æ¯æ³„éœ²(cross-image information leakage)ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†FOCUSï¼Œä¸€ç§æ— éœ€è®­ç»ƒä¸”ä¸æ¶æ„æ— å…³çš„æ¨ç†é˜¶æ®µè§£ç ç­–ç•¥ï¼Œæ—¨åœ¨ç¼“è§£æ¨ç†è¿‡ç¨‹ä¸­çš„ä¿¡æ¯æ³„éœ²ã€‚FOCUSé€šè¿‡æŒ‰é¡ºåºä½¿ç”¨éšæœºå™ªå£°é®è”½é™¤ä¸€å¼ å›¾åƒå¤–çš„æ‰€æœ‰å›¾åƒï¼Œå¼•å¯¼æ¨¡å‹èšç„¦äºå•ä¸€æ¸…æ™°å›¾åƒï¼Œå¹¶é‡å¤è¯¥è¿‡ç¨‹ä»¥è·å–ä¸åŒæ©ç ä¸Šä¸‹æ–‡ä¸‹çš„é€»è¾‘å€¼(logits)ã€‚è¿™äº›é€»è¾‘å€¼ç»è¿‡èšåˆåï¼Œåˆ©ç”¨çº¯å™ªå£°å‚è€ƒè¾“å…¥è¿›è¡Œå¯¹æ¯”ä¼˜åŒ–ï¼Œä»è€Œæœ‰æ•ˆæŠ‘åˆ¶æ³„éœ²å¹¶äº§ç”Ÿæ›´å‡†ç¡®çš„è¾“å‡ºã€‚å®éªŒè¯æ˜ï¼ŒFOCUSåœ¨å››ä¸ªå¤šå›¾åƒåŸºå‡†æµ‹è¯•å’Œå¤šç§LVLMæ¨¡å‹ç³»åˆ—ä¸Šå‡å®ç°äº†æ€§èƒ½çš„æŒç»­æå‡ï¼Œä¸ºå¢å¼ºå¤šå›¾åƒæ¨ç†æä¾›äº†ä¸€ç§é€šç”¨ä¸”æ— éœ€ä¿®æ”¹æ¶æ„çš„å®ç”¨æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Source code is available at https://github.com/yejipark-m/FOCUS",
      "pdf_url": "https://arxiv.org/pdf/2508.13744v1",
      "published_date": "2025-08-19 11:31:39 UTC",
      "updated_date": "2025-08-19 11:31:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:47:25.550160+00:00"
    },
    {
      "arxiv_id": "2508.13730v1",
      "title": "On the Security and Privacy of Federated Learning: A Survey with Attacks, Defenses, Frameworks, Applications, and Future Directions",
      "title_zh": "è”é‚¦å­¦ä¹ å®‰å…¨ä¸éšç§ï¼šæ”»å‡»ã€é˜²å¾¡ã€æ¡†æ¶ã€åº”ç”¨ä¸æœªæ¥æ–¹å‘ç»¼è¿°",
      "authors": [
        "Daniel M. Jimenez-Gutierrez",
        "Yelizaveta Falkouskaya",
        "Jose L. Hernandez-Ramos",
        "Aris Anagnostopoulos",
        "Ioannis Chatzigiannakis",
        "Andrea Vitaletti"
      ],
      "abstract": "Federated Learning (FL) is an emerging distributed machine learning paradigm enabling multiple clients to train a global model collaboratively without sharing their raw data. While FL enhances data privacy by design, it remains vulnerable to various security and privacy threats. This survey provides a comprehensive overview of more than 200 papers regarding the state-of-the-art attacks and defense mechanisms developed to address these challenges, categorizing them into security-enhancing and privacy-preserving techniques. Security-enhancing methods aim to improve FL robustness against malicious behaviors such as byzantine attacks, poisoning, and Sybil attacks. At the same time, privacy-preserving techniques focus on protecting sensitive data through cryptographic approaches, differential privacy, and secure aggregation. We critically analyze the strengths and limitations of existing methods, highlight the trade-offs between privacy, security, and model performance, and discuss the implications of non-IID data distributions on the effectiveness of these defenses. Furthermore, we identify open research challenges and future directions, including the need for scalable, adaptive, and energy-efficient solutions operating in dynamic and heterogeneous FL environments. Our survey aims to guide researchers and practitioners in developing robust and privacy-preserving FL systems, fostering advancements safeguarding collaborative learning frameworks' integrity and confidentiality.",
      "tldr_zh": "æœ¬ç»¼è¿°å…¨é¢ç³»ç»Ÿåœ°æ¢è®¨äº† Federated Learning (FL) é¢†åŸŸçš„å®‰å…¨ä¸éšç§é—®é¢˜ï¼Œæ¶µç›–äº†è¶…è¿‡200ç¯‡å…³äºæœ€æ–°æ”»å‡»æ‰‹æ®µä¸é˜²å¾¡æœºåˆ¶çš„å­¦æœ¯æ–‡çŒ®ã€‚è¯¥ç ”ç©¶å°†ç°æœ‰æŠ€æœ¯ç»†åˆ†ä¸ºå®‰å…¨æ€§å¢å¼º (Security-enhancing) å’Œéšç§ä¿æŠ¤ (Privacy-preserving) ä¸¤å¤§ç±»ï¼Œå…¶ä¸­å®‰å…¨æ€§å¢å¼ºæ–¹æ³•è‡´åŠ›äºæå‡ FL æŠµå¾¡ Byzantine Attacksã€Poisoning ä»¥åŠ Sybil Attacks ç­‰æ¶æ„è¡Œä¸ºçš„é²æ£’æ€§ã€‚éšç§ä¿æŠ¤æŠ€æœ¯åˆ™ä¾§é‡äºé€šè¿‡ Cryptographic Approachesã€Differential Privacy ä»¥åŠ Secure Aggregation ç­‰æ‰‹æ®µä¿æŠ¤æ•æ„Ÿæ•°æ®ã€‚æ–‡ç« æ·±å…¥åˆ†æäº†ç°æœ‰æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œé‡ç‚¹è®¨è®ºäº†éšç§ã€å®‰å…¨ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶é˜è¿°äº†éç‹¬ç«‹åŒåˆ†å¸ƒ (Non-IID) æ•°æ®åˆ†å¸ƒå¯¹é˜²å¾¡æ•ˆæœçš„å½±å“ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜ç¡®å®šäº†è¯¥é¢†åŸŸçš„æœªæ¥æ–¹å‘ï¼ŒåŒ…æ‹¬åœ¨åŠ¨æ€å¼‚æ„ç¯å¢ƒä¸­å¼€å‘å¯æ‰©å±•ã€è‡ªé€‚åº”ä¸”èƒ½æ•ˆé«˜çš„ FL è§£å†³æ–¹æ¡ˆã€‚è¯¥ç»¼è¿°æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æ„å»ºé²æ£’ä¸”å…·å¤‡éšç§ä¿æŠ¤èƒ½åŠ›çš„ FL ç³»ç»Ÿæä¾›æŒ‡å¯¼ï¼Œæ¨åŠ¨åä½œå­¦ä¹ æ¡†æ¶å®Œæ•´æ€§ä¸æœºå¯†æ€§çš„è¿›æ­¥ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13730v1",
      "published_date": "2025-08-19 11:06:20 UTC",
      "updated_date": "2025-08-19 11:06:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:47:35.353111+00:00"
    },
    {
      "arxiv_id": "2508.13729v1",
      "title": "Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings",
      "title_zh": "é¢„æµ‹å¹¶éè§£é‡Šï¼šé‡æ–°å®¡è§†æ˜ å°„åµŒå…¥çš„è§£é‡Šèƒ½åŠ›",
      "authors": [
        "Hanna Herasimchyk",
        "Alhassan Abdelhalim",
        "SÃ¶ren Laue",
        "Michaela Regneri"
      ],
      "abstract": "Understanding what knowledge is implicitly encoded in deep learning models is essential for improving the interpretability of AI systems. This paper examines common methods to explain the knowledge encoded in word embeddings, which are core elements of large language models (LLMs). These methods typically involve mapping embeddings onto collections of human-interpretable semantic features, known as feature norms. Prior work assumes that accurately predicting these semantic features from the word embeddings implies that the embeddings contain the corresponding knowledge. We challenge this assumption by demonstrating that prediction accuracy alone does not reliably indicate genuine feature-based interpretability.\n  We show that these methods can successfully predict even random information, concluding that the results are predominantly determined by an algorithmic upper bound rather than meaningful semantic representation in the word embeddings. Consequently, comparisons between datasets based solely on prediction performance do not reliably indicate which dataset is better captured by the word embeddings. Our analysis illustrates that such mappings primarily reflect geometric similarity within vector spaces rather than indicating the genuine emergence of semantic properties.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­word embeddingsçš„è§£é‡Šæ€§é—®é¢˜ï¼Œé‡ç‚¹é‡æ–°è¯„ä¼°äº†å°†embeddingæ˜ å°„åˆ°äººç±»å¯ç†è§£çš„è¯­ä¹‰ç‰¹å¾ï¼ˆfeature normsï¼‰è¿™ä¸€å¸¸ç”¨æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä½œè€…æŒ‘æˆ˜äº†å­¦æœ¯ç•Œæ™®éæŒæœ‰çš„å‡è®¾ï¼Œå³å‡†ç¡®é¢„æµ‹è¯­ä¹‰ç‰¹å¾å³ä»£è¡¨embeddingsåŒ…å«äº†ç›¸åº”çš„çŸ¥è¯†ã€‚ç ”ç©¶é€šè¿‡å®éªŒå‘ç°ï¼Œè¿™äº›æ˜ å°„æ–¹æ³•ç”šè‡³èƒ½å¤ŸæˆåŠŸé¢„æµ‹éšæœºä¿¡æ¯ï¼Œè¡¨æ˜é¢„æµ‹ç»“æœæ›´å¤šæ˜¯ç”±ç®—æ³•ä¸Šé™å†³å®šçš„ï¼Œè€Œéæºäºword embeddingsä¸­æœ‰æ„ä¹‰çš„è¯­ä¹‰è¡¨å¾ã€‚å› æ­¤ï¼Œä»…åŸºäºé¢„æµ‹å‡†ç¡®ç‡æ¥è¡¡é‡è§£é‡Šæ€§å¹¶ä¸å¯é ï¼Œä¹Ÿæ— æ³•å‡†ç¡®åæ˜ ä¸åŒæ•°æ®é›†è¢«æ¨¡å‹æ•è·çš„ç¨‹åº¦ã€‚æœ€ç»ˆåˆ†æè¡¨æ˜ï¼Œæ­¤ç±»æ˜ å°„ä¸»è¦ä½“ç°äº†å‘é‡ç©ºé—´ä¸­çš„å‡ ä½•ç›¸ä¼¼æ€§ï¼ˆgeometric similarityï¼‰ï¼Œè€Œä¸æ˜¯è¯­ä¹‰å±æ€§ï¼ˆsemantic propertiesï¼‰çš„çœŸå®æ¶Œç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 6 Figures. Published at ECAI 2025 in a version without the Appendix",
      "pdf_url": "https://arxiv.org/pdf/2508.13729v1",
      "published_date": "2025-08-19 11:00:47 UTC",
      "updated_date": "2025-08-19 11:00:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:47:45.354574+00:00"
    },
    {
      "arxiv_id": "2508.13721v1",
      "title": "CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through Causality-Driven Planning",
      "title_zh": "CausalPlanï¼šåŸºäºå› æœé©±åŠ¨è§„åˆ’çš„é«˜æ•ˆå¤§è¯­è¨€æ¨¡å‹å¤šæ™ºèƒ½ä½“åä½œ",
      "authors": [
        "Minh Hoang Nguyen",
        "Van Dai Do",
        "Dung Nguyen",
        "Thin Nguyen",
        "Hung Le"
      ],
      "abstract": "Large language model (LLM) agents-especially smaller, open-source models-often produce causally invalid or incoherent actions in collaborative tasks due to their reliance on surface-level correlations rather than grounded causal reasoning. This limitation undermines their performance in terms of coordination and planning in dynamic environments. We address this challenge with CausalPlan, a two-phase framework that integrates explicit structural causal reasoning into the LLM planning process. At the core of CausalPlan is the Structural Causal Action (SCA) model, which learns a causal graph from agent trajectories to capture how prior actions and current environment states influence future decisions. This structure is then used to guide action selection by assigning causal scores to LLM-generated proposals, reweighting them accordingly, or falling back to causally grounded alternatives when needed. By embedding this causal knowledge directly into the decision loop, CausalPlan constrains planning to intervention-consistent behaviours without requiring fine-tuning of the LLM itself. We evaluate CausalPlan on the Overcooked-AI benchmark across five multi-agent coordination tasks and four LLMs of varying sizes: Gemma-7B, Llama-8B, Qwen-14B, and Llama-70B. Experimental results show that CausalPlan consistently reduces invalid actions and improves collaboration in both AI-AI and human-AI settings, outperforming strong reinforcement learning baselines. Our findings highlight the value of causality-driven planning for deploying efficient, interpretable, and generalisable multi-agent LLM systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™ºèƒ½ä½“åœ¨åä½œä»»åŠ¡ä¸­å› è¿‡åº¦ä¾èµ–è¡¨é¢ç›¸å…³æ€§è€Œéå› æœæ¨ç†ï¼Œå¯¼è‡´äº§ç”Ÿæ— æ•ˆæˆ–ä¸è¿è´¯è¡Œä¸ºçš„é—®é¢˜ï¼Œæå‡ºäº†CausalPlanæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸¤é˜¶æ®µæµç¨‹å°†æ˜¾å¼ç»“æ„åŒ–å› æœæ¨ç†ï¼ˆstructural causal reasoningï¼‰æ•´åˆåˆ°LLMè§„åˆ’è¿‡ç¨‹ä¸­ï¼Œå…¶æ ¸å¿ƒç»„ä»¶ä¸ºç»“æ„å› æœè¡ŒåŠ¨ï¼ˆStructural Causal Action, SCAï¼‰æ¨¡å‹ã€‚SCAæ¨¡å‹é€šè¿‡ä»æ™ºèƒ½ä½“è½¨è¿¹ä¸­å­¦ä¹ å› æœå›¾ï¼ˆcausal graphï¼‰ï¼Œæ•æ‰è¿‡å¾€è¡ŒåŠ¨ä¸å½“å‰ç¯å¢ƒçŠ¶æ€å¯¹æœªæ¥å†³ç­–çš„å½±å“ã€‚é€šè¿‡ä¸ºLLMç”Ÿæˆçš„æ–¹æ¡ˆåˆ†é…å› æœè¯„åˆ†å¹¶è¿›è¡Œé‡åŠ æƒï¼ŒCausalPlanèƒ½æœ‰æ•ˆçº¦æŸè§„åˆ’è¿‡ç¨‹ï¼Œä½¿å…¶ç¬¦åˆå¹²é¢„ä¸€è‡´æ€§è¡Œä¸ºï¼Œä¸”æ— éœ€å¯¹LLMè¿›è¡Œå¾®è°ƒã€‚åœ¨Overcooked-AIåŸºå‡†æµ‹è¯•ä¸­é’ˆå¯¹Gemmaã€Llamaå’ŒQwenç­‰ä¸åŒè§„æ¨¡æ¨¡å‹çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨AI-AIåŠäººæœºåä½œè®¾ç½®ä¸‹å‡èƒ½æ˜¾è‘—å‡å°‘æ— æ•ˆåŠ¨ä½œå¹¶æå‡åä½œæ€§èƒ½ï¼Œè¡¨ç°ä¼˜äºå¼ºå¼ºåŒ–å­¦ä¹ åŸºçº¿æ¨¡å‹ã€‚è¯¥ç ”ç©¶çªæ˜¾äº†å› æœé©±åŠ¨è§„åˆ’åœ¨éƒ¨ç½²é«˜æ•ˆã€å¯è§£é‡Šä¸”æ³›åŒ–èƒ½åŠ›å¼ºçš„å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿä¸­çš„æ ¸å¿ƒä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13721v1",
      "published_date": "2025-08-19 10:37:20 UTC",
      "updated_date": "2025-08-19 10:37:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:49:42.656120+00:00"
    },
    {
      "arxiv_id": "2508.13718v1",
      "title": "Generics and Default Reasoning in Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ³›æŒ‡è¡¨è¾¾ä¸é»˜è®¤æ¨ç†",
      "authors": [
        "James Ravi Kirkpatrick",
        "Rachel Katharine Sterken"
      ],
      "abstract": "This paper evaluates the capabilities of 28 large language models (LLMs) to reason with 20 defeasible reasoning patterns involving generic generalizations (e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic. Generics are of special interest to linguists, philosophers, logicians, and cognitive scientists because of their complex exception-permitting behaviour and their centrality to default reasoning, cognition, and concept acquisition. We find that while several frontier models handle many default reasoning problems well, performance varies widely across models and prompting styles. Few-shot prompting modestly improves performance for some models, but chain-of-thought (CoT) prompting often leads to serious performance degradation (mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy in zero-shot condition, temperature 0). Most models either struggle to distinguish between defeasible and deductive inference or misinterpret generics as universal statements. These findings underscore both the promise and limits of current LLMs for default reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†28ä¸ªLarge Language Models (LLMs)åœ¨æ¶‰åŠGeneric Generalizationsï¼ˆå¦‚â€œé¸Ÿä¼šé£â€ï¼‰çš„20ç§Defeasible Reasoningæ¨¡å¼ä¸‹çš„æ¨ç†èƒ½åŠ›ã€‚Genericså› å…¶å¤æ‚çš„ä¾‹å¤–è®¸å¯è¡Œä¸ºè€Œæˆä¸ºNon-monotonic Logicå’ŒDefault Reasoningçš„æ ¸å¿ƒã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶éƒ¨åˆ†å‰æ²¿æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œä½†ä¸åŒæ¨¡å‹å’Œæç¤ºç­–ç•¥é—´çš„æ€§èƒ½å·®å¼‚å·¨å¤§ã€‚Few-shot Promptingä»…èƒ½å°å¹…æå‡è¡¨ç°ï¼Œè€ŒChain-of-Thought (CoT)æç¤ºåœ¨é«˜æ€§èƒ½æ¨¡å‹ä¸­åè€Œå¯¼è‡´å¹³å‡å‡†ç¡®ç‡ä¸‹é™11.14%ã€‚å¤§å¤šæ•°æ¨¡å‹éš¾ä»¥åŒºåˆ†Defeasible Inferenceä¸Deductive Inferenceï¼Œæˆ–å€¾å‘äºå°†Genericsè¯¯è§£ä¸ºå…¨ç§°å‘½é¢˜ã€‚è¯¥ç ”ç©¶ç»“æœæ­ç¤ºäº†å½“å‰LLMsåœ¨å¤„ç†å¤æ‚å¸¸è¯†æ¨ç†å’Œé»˜è®¤æ¨ç†ä»»åŠ¡ä¸­çš„æ˜¾è‘—å±€é™æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.CL",
      "comment": "33 pages, 26 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.13718v1",
      "published_date": "2025-08-19 10:28:53 UTC",
      "updated_date": "2025-08-19 10:28:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:49:17.554073+00:00"
    },
    {
      "arxiv_id": "2509.03521v1",
      "title": "BiND: A Neural Discriminator-Decoder for Accurate Bimanual Trajectory Prediction in Brain-Computer Interfaces",
      "title_zh": "BiNDï¼šç”¨äºè„‘æœºæ¥å£é«˜ç²¾åº¦åŒæ‰‹è½¨è¿¹é¢„æµ‹çš„ç¥ç»åˆ¤åˆ«-è§£ç å™¨",
      "authors": [
        "Timothee Robert",
        "MohammadAli Shaeri",
        "Mahsa Shoaran"
      ],
      "abstract": "Decoding bimanual hand movements from intracortical recordings remains a critical challenge for brain-computer interfaces (BCIs), due to overlapping neural representations and nonlinear interlimb interactions. We introduce BiND (Bimanual Neural Discriminator-Decoder), a two-stage model that first classifies motion type (unimanual left, unimanual right, or bimanual) and then uses specialized GRU-based decoders, augmented with a trial-relative time index, to predict continuous 2D hand velocities. We benchmark BiND against six state-of-the-art models (SVR, XGBoost, FNN, CNN, Transformer, GRU) on a publicly available 13-session intracortical dataset from a tetraplegic patient. BiND achieves a mean $R^2$ of 0.76 ($\\pm$0.01) for unimanual and 0.69 ($\\pm$0.03) for bimanual trajectory prediction, surpassing the next-best model (GRU) by 2% in both tasks. It also demonstrates greater robustness to session variability than all other benchmarked models, with accuracy improvements of up to 4% compared to GRU in cross-session analyses. This highlights the effectiveness of task-aware discrimination and temporal modeling in enhancing bimanual decoding.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†BiND (Bimanual Neural Discriminator-Decoder)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³è„‘æœºæ¥å£ (BCIs) è§£ç åŒæ‰‹è¿åŠ¨æ—¶ç¥ç»è¡¨ç¤ºé‡å å’Œéçº¿æ€§äº¤äº’æŒ‘æˆ˜çš„ä¸¤é˜¶æ®µæ¨¡å‹ã€‚BiNDé¦–å…ˆå°†è¿åŠ¨ç±»å‹åˆ†ç±»ä¸ºå•æ‰‹æˆ–åŒæ‰‹ï¼Œéšåé‡‡ç”¨å¢å¼ºäº†è¯•éªŒç›¸å¯¹æ—¶é—´ç´¢å¼• (trial-relative time index) çš„ä¸“é—¨ GRU è§£ç å™¨æ¥é¢„æµ‹è¿ç»­çš„äºŒç»´æ‰‹éƒ¨é€Ÿåº¦ã€‚åœ¨å››è‚¢ç˜«ç—ªæ‚£è€…çš„çš®å±‚å†…æ•°æ®é›†æµ‹è¯•ä¸­ï¼ŒBiNDåœ¨å•æ‰‹å’ŒåŒæ‰‹è½¨è¿¹é¢„æµ‹ä¸Šçš„å¹³å‡ R^2 åˆ†åˆ«è¾¾åˆ°0.76å’Œ0.69ï¼Œæ€§èƒ½ä¼˜äºåŒ…æ‹¬Transformerå’ŒCNNåœ¨å†…çš„å…­ç§åŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒBiNDåœ¨è·¨ä¼šè¯åˆ†æä¸­å±•ç°å‡ºæ¯”æ¬¡ä¼˜æ¨¡å‹é«˜å‡º4%çš„å‡†ç¡®ç‡æå‡ï¼Œè¯æ˜äº†å…¶æ›´å¼ºçš„é²æ£’æ€§ã€‚å®éªŒç»“æœå¼ºè°ƒäº†ä»»åŠ¡æ„ŸçŸ¥åˆ¤åˆ« (task-aware discrimination) å’Œæ—¶é—´å»ºæ¨¡å¯¹äºæå‡å¤æ‚åŒæ‰‹è§£ç ä»»åŠ¡å‡†ç¡®æ€§çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "q-bio.NC",
      "comment": "Accepted for publication in IEEE Neural Engineering (NER) Conference'25",
      "pdf_url": "https://arxiv.org/pdf/2509.03521v1",
      "published_date": "2025-08-19 10:18:41 UTC",
      "updated_date": "2025-08-19 10:18:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:49:58.458104+00:00"
    },
    {
      "arxiv_id": "2508.13700v1",
      "title": "The AI Risk Spectrum: From Dangerous Capabilities to Existential Threats",
      "title_zh": "AIé£é™©å›¾è°±ï¼šä»å±é™©èƒ½åŠ›åˆ°å­˜åœ¨æ€§å¨èƒ",
      "authors": [
        "Markov Grey",
        "Charbel-RaphaÃ«l Segerie"
      ],
      "abstract": "As AI systems become more capable, integrated, and widespread, understanding the associated risks becomes increasingly important. This paper maps the full spectrum of AI risks, from current harms affecting individual users to existential threats that could endanger humanity's survival. We organize these risks into three main causal categories. Misuse risks, which occur when people deliberately use AI for harmful purposes - creating bioweapons, launching cyberattacks, adversarial AI attacks or deploying lethal autonomous weapons. Misalignment risks happen when AI systems pursue outcomes that conflict with human values, irrespective of developer intentions. This includes risks arising through specification gaming (reward hacking), scheming and power-seeking tendencies in pursuit of long-term strategic goals. Systemic risks, which arise when AI integrates into complex social systems in ways that gradually undermine human agency - concentrating power, accelerating political and economic disempowerment, creating overdependence that leads to human enfeeblement, or irreversibly locking in current values curtailing future moral progress. Beyond these core categories, we identify risk amplifiers - competitive pressures, accidents, corporate indifference, and coordination failures - that make all risks more likely and severe. Throughout, we connect today's existing risks and empirically observable AI behaviors to plausible future outcomes, demonstrating how existing trends could escalate to catastrophic outcomes. Our goal is to help readers understand the complete landscape of AI risks. Good futures are possible, but they don't happen by default. Navigating these challenges will require unprecedented coordination, but an extraordinary future awaits if we do.",
      "tldr_zh": "è¯¥ç ”ç©¶ç»˜åˆ¶äº†äººå·¥æ™ºèƒ½é£é™©çš„å®Œæ•´å›¾è°±ï¼Œæ¶µç›–äº†ä»å½±å“ä¸ªä½“çš„å½“å‰ä¼¤å®³åˆ°å¯èƒ½å±åŠäººç±»ç”Ÿå­˜çš„ Existential Threatsã€‚è®ºæ–‡å°†é£é™©å½’çº³ä¸ºä¸‰å¤§æ ¸å¿ƒå› æœç±»åˆ«ï¼Œå…¶ä¸­ Misuse é£é™©æ¶‰åŠåˆ©ç”¨ AI åˆ¶é€ ç”Ÿç‰©æ­¦å™¨ã€å‘åŠ¨ç½‘ç»œæ”»å‡»æˆ–éƒ¨ç½²è‡´å‘½è‡ªä¸»æ­¦å™¨ç­‰è“„æ„ç ´åè¡Œä¸ºã€‚Misalignment é£é™©æºäºç³»ç»Ÿè¿½æ±‚ä¸äººç±»ä»·å€¼å†²çªçš„ç›®æ ‡ï¼ŒåŒ…æ‹¬ç”±äº Specification Gamingã€å¥–åŠ±åŠ«æŒï¼ˆReward Hackingï¼‰æˆ–æƒåŠ›å¯»æ±‚å€¾å‘ï¼ˆPower-seeking Tendenciesï¼‰å¯¼è‡´çš„åå·®ã€‚Systemic é£é™©åˆ™å…³æ³¨ AI æ·±åº¦æ•´åˆè¿›å¤æ‚ç¤¾ä¼šç³»ç»Ÿåå¼•å‘çš„æƒåŠ›é›†ä¸­ã€äººç±»èƒ½åŠ›è¡°é€€ä»¥åŠç¤¾ä¼šä»·å€¼è§‚é”å®šç­‰ç»“æ„æ€§å¨èƒã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯†åˆ«äº†ç«äº‰å‹åŠ›å’Œåè°ƒå¤±è´¥ç­‰ Risk Amplifiersï¼Œè¿™äº›å› ç´ æ˜¾è‘—æé«˜äº†å„ç±»é£é™©çˆ†å‘çš„å¯èƒ½æ€§ã€‚é€šè¿‡å»ºç«‹å½“å‰è§‚æµ‹åˆ°çš„ AI è¡Œä¸ºä¸æœªæ¥ç¾éš¾æ€§ç»“æœä¹‹é—´çš„é€»è¾‘è”ç³»ï¼Œè¯¥ç ”ç©¶ä¸ºç†è§£å¹¶åº”å¯¹äººå·¥æ™ºèƒ½å¸¦æ¥çš„å…¨æ–¹ä½æŒ‘æˆ˜æä¾›äº†ç³»ç»Ÿçš„ç†è®ºæ¡†æ¶ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13700v1",
      "published_date": "2025-08-19 10:05:51 UTC",
      "updated_date": "2025-08-19 10:05:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:49:57.654270+00:00"
    },
    {
      "arxiv_id": "2508.13697v1",
      "title": "The DeepLog Neurosymbolic Machine",
      "title_zh": "DeepLog ç¥ç»ç¬¦å·æœº",
      "authors": [
        "Vincent Derkinderen",
        "Robin Manhaeve",
        "Rik Adriaensen",
        "Lucas Van Praet",
        "Lennert De Smet",
        "Giuseppe Marra",
        "Luc De Raedt"
      ],
      "abstract": "We contribute a theoretical and operational framework for neurosymbolic AI called DeepLog. DeepLog introduces building blocks and primitives for neurosymbolic AI that make abstraction of commonly used representations and computational mechanisms used in neurosymbolic AI. DeepLog can represent and emulate a wide range of neurosymbolic systems. It consists of two key components. The first is the DeepLog language for specifying neurosymbolic models and inference tasks. This language consists of an annotated neural extension of grounded first-order logic, and makes abstraction of the type of logic, e.g. boolean, fuzzy or probabilistic, and whether logic is used in the architecture or in the loss function. The second DeepLog component is situated at the computational level and uses extended algebraic circuits as computational graphs. Together these two components are to be considered as a neurosymbolic abstract machine, with the DeepLog language as the intermediate level of abstraction and the circuits level as the computational one. DeepLog is implemented in software, relies on the latest insights in implementing algebraic circuits on GPUs, and is declarative in that it is easy to obtain different neurosymbolic models by making different choices for the underlying algebraic structures and logics. The generality and efficiency of the DeepLog neurosymbolic machine is demonstrated through an experimental comparison between 1) different fuzzy and probabilistic logics, 2) between using logic in the architecture or in the loss function, and 3) between a standalone CPU-based implementation of a neurosymbolic AI system and a DeepLog GPU-based one.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DeepLogï¼Œä¸€ä¸ªç”¨äºneurosymbolic AIçš„ç†è®ºä¸æ“ä½œæ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºè¯¥é¢†åŸŸå¸¸ç”¨çš„è¡¨å¾å’Œè®¡ç®—æœºåˆ¶æä¾›æŠ½è±¡ã€‚DeepLogç”±ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶æ„æˆï¼šç”¨äºå®šä¹‰æ¨¡å‹å’Œæ¨ç†ä»»åŠ¡çš„DeepLogè¯­è¨€ï¼Œä»¥åŠä½œä¸ºåº•å±‚è®¡ç®—å›¾çš„æ‰©å±•ä»£æ•°ç”µè·¯(algebraic circuits)ã€‚ä½œä¸ºä¸€ä¸ªneurosymbolic abstract machineï¼Œè¯¥ç³»ç»Ÿå°†åŸºäºgrounded first-order logicçš„è¯­è¨€ä½œä¸ºä¸­é—´æŠ½è±¡å±‚ï¼Œå¹¶æ”¯æŒbooleanã€fuzzyå’Œprobabilisticç­‰å¤šç§é€»è¾‘ç±»å‹ï¼ŒåŒæ—¶å…¼å®¹é€»è¾‘åœ¨æ¶æ„æˆ–æŸå¤±å‡½æ•°(loss function)ä¸­çš„ä¸åŒåº”ç”¨ã€‚DeepLogå…·æœ‰å£°æ˜å¼(declarative)ç‰¹ç‚¹ï¼Œåˆ©ç”¨æœ€æ–°çš„GPUä»£æ•°ç”µè·¯ä¼˜åŒ–æŠ€æœ¯å®ç°äº†é«˜æ•ˆè®¡ç®—ã€‚å®éªŒé€šè¿‡å¯¹æ¯”ä¸åŒçš„é€»è¾‘ç³»ç»Ÿã€é›†æˆæ–¹å¼ä»¥åŠCPUä¸GPUçš„è¿è¡Œæ•ˆç‡ï¼Œè¯æ˜äº†DeepLogåœ¨æ„å»ºå’Œè¿è¡Œå„ç±»neurosymbolic AIæ¨¡å‹æ—¶çš„é€šç”¨æ€§ä¸å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13697v1",
      "published_date": "2025-08-19 09:58:24 UTC",
      "updated_date": "2025-08-19 09:58:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:50:01.761667+00:00"
    },
    {
      "arxiv_id": "2508.13678v1",
      "title": "Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models",
      "title_zh": "ç¥ç»ç¬¦å·äººå·¥æ™ºèƒ½ï¼šæ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›",
      "authors": [
        "Xiao-Wen Yang",
        "Jie-Jing Shao",
        "Lan-Zhe Guo",
        "Bo-Wen Zhang",
        "Zhi Zhou",
        "Lin-Han Jia",
        "Wang-Zhou Dai",
        "Yu-Feng Li"
      ],
      "abstract": "Large Language Models (LLMs) have shown promising results across various tasks, yet their reasoning capabilities remain a fundamental challenge. Developing AI systems with strong reasoning capabilities is regarded as a crucial milestone in the pursuit of Artificial General Intelligence (AGI) and has garnered considerable attention from both academia and industry. Various techniques have been explored to enhance the reasoning capabilities of LLMs, with neuro-symbolic approaches being a particularly promising way. This paper comprehensively reviews recent developments in neuro-symbolic approaches for enhancing LLM reasoning. We first present a formalization of reasoning tasks and give a brief introduction to the neurosymbolic learning paradigm. Then, we discuss neuro-symbolic methods for improving the reasoning capabilities of LLMs from three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic. Finally, we discuss several key challenges and promising future directions. We have also released a GitHub repository including papers and resources related to this survey: https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåœ°ç»¼è¿°äº†ç¥ç»ç¬¦å·äººå·¥æ™ºèƒ½(Neuro-Symbolic AI)åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ€æ–°è¿›å±•ï¼Œæ¢è®¨äº†è¿™ä¸€è¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½(AGI)çš„å…³é”®æŠ€æœ¯è·¯å¾„ã€‚æ–‡ç« é¦–å…ˆå¯¹æ¨ç†ä»»åŠ¡è¿›è¡Œäº†å½¢å¼åŒ–å®šä¹‰ï¼Œå¹¶ç®€è¦ä»‹ç»äº†ç¥ç»ç¬¦å·å­¦ä¹ èŒƒå¼ã€‚æ ¸å¿ƒå†…å®¹ä»Symbolic->LLMã€LLM->Symbolicä»¥åŠLLM+Symbolicä¸‰ä¸ªè§†è§’æ·±å…¥è®¨è®ºäº†å¢å¼ºLLMæ¨ç†èƒ½åŠ›çš„å…·ä½“æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜åˆ†æäº†å½“å‰é¢ä¸´çš„å…³é”®æŒ‘æˆ˜å¹¶å±•æœ›äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚ä½œè€…åŒæ—¶æä¾›äº†å¼€æºçš„GitHubèµ„æºåº“ï¼Œä¸ºè¯¥é¢†åŸŸçš„å­¦æœ¯ç ”ç©¶æä¾›äº†å…¨é¢çš„æ–‡çŒ®å‚è€ƒä¸å·¥å…·æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 3 figures, IJCAI 2025 Survey Track",
      "pdf_url": "https://arxiv.org/pdf/2508.13678v1",
      "published_date": "2025-08-19 09:27:46 UTC",
      "updated_date": "2025-08-19 09:27:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:50:03.450131+00:00"
    },
    {
      "arxiv_id": "2508.13676v2",
      "title": "MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model",
      "title_zh": "MHSNetï¼šåŸºäºæ··åˆä¸“å®¶æ¨¡å‹ä¸å¤§è¯­è¨€æ¨¡å‹çš„å±‚çº§è¯­ä¹‰è¡¨ç¤ºç½‘ç»œï¼Œç”¨äºç²¾å‡†é‡å¤ç®€å†æ£€æµ‹",
      "authors": [
        "Yu Li",
        "Zulong Chen",
        "Wenjian Xu",
        "Hong Wen",
        "Yipeng Yu",
        "Man Lung Yiu",
        "Yuyu Yin"
      ],
      "abstract": "To maintain the company's talent pool, recruiters need to continuously search for resumes from third-party websites (e.g., LinkedIn, Indeed). However, fetched resumes are often incomplete and inaccurate. To improve the quality of third-party resumes and enrich the company's talent pool, it is essential to conduct duplication detection between the fetched resumes and those already in the company's talent pool. Such duplication detection is challenging due to the semantic complexity, structural heterogeneity, and information incompleteness of resume texts. To this end, we propose MHSNet, an multi-level identity verification framework that fine-tunes BGE-M3 using contrastive learning. With the fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse and dense representations for resumes, enabling the computation of corresponding multi-level semantic similarities. Moreover, the state-aware Mixture-of-Experts (MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimental results verify the effectiveness of MHSNet",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MHSNetï¼Œä¸€ç§åŸºäºMixture-of-Experts (MoE) çš„åˆ†å±‚è¯­ä¹‰è¡¨ç¤ºç½‘ç»œï¼Œæ—¨åœ¨è§£å†³ç®€å†é‡å¤æ£€æµ‹ä¸­é¢ä¸´çš„è¯­ä¹‰å¤æ‚æ€§ã€ç»“æ„å¼‚æ„æ€§å’Œä¿¡æ¯ä¸å®Œæ•´æ€§ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹æ¯”å­¦ä¹ (contrastive learning)å¯¹BGE-M3è¿›è¡Œå¾®è°ƒï¼Œæ„å»ºäº†ä¸€ä¸ªå¤šå±‚çº§çš„èº«ä»½éªŒè¯ä½“ç³»ã€‚MHSNetåˆ©ç”¨MoEç”Ÿæˆç®€å†çš„å¤šå±‚çº§ç¨€ç–(sparse)ä¸ç¨ å¯†(dense)è¡¨ç¤ºï¼Œä»è€Œå®ç°æ›´ç²¾å‡†çš„è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—ã€‚æ­¤å¤–ï¼Œæ¨¡å‹å¼•å…¥äº†çŠ¶æ€æ„ŸçŸ¥(state-aware)çš„MoEæ¨¡å—ï¼Œä¸“é—¨ç”¨äºå¤„ç†ä¿¡æ¯ç¼ºå¤±çš„å¤æ‚ç®€å†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMHSNetåœ¨æé«˜ç¬¬ä¸‰æ–¹ç®€å†è´¨é‡å’Œç»´æŠ¤ä¼ä¸šäººæ‰åº“æ–¹é¢å…·æœ‰æ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13676v2",
      "published_date": "2025-08-19 09:27:14 UTC",
      "updated_date": "2025-09-05 04:33:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:50:10.358769+00:00"
    },
    {
      "arxiv_id": "2508.13675v1",
      "title": "Knowledge Graph Completion for Action Prediction on Situational Graphs -- A Case Study on Household Tasks",
      "title_zh": "é¢å‘æƒ…å¢ƒå›¾åŠ¨ä½œé¢„æµ‹çš„çŸ¥è¯†å›¾è°±è¡¥å…¨â€”â€”ä»¥å®¶åº­ä»»åŠ¡ä¸ºä¾‹",
      "authors": [
        "Mariam Arustashvili",
        "JÃ¶rg DeigmÃ¶ller",
        "Heiko Paulheim"
      ],
      "abstract": "Knowledge Graphs are used for various purposes, including business applications, biomedical analyses, or digital twins in industry 4.0. In this paper, we investigate knowledge graphs describing household actions, which are beneficial for controlling household robots and analyzing video footage. In the latter case, the information extracted from videos is notoriously incomplete, and completing the knowledge graph for enhancing the situational picture is essential. In this paper, we show that, while a standard link prediction problem, situational knowledge graphs have special characteristics that render many link prediction algorithms not fit for the job, and unable to outperform even simple baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Knowledge Graphs åœ¨å®¶åº­æœºå™¨äººæ§åˆ¶å’Œè§†é¢‘åˆ†æç­‰ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œé‡ç‚¹ç ”ç©¶å¦‚ä½•é€šè¿‡ Knowledge Graph Completion æŠ€æœ¯è¡¥å……è§†é¢‘æ•°æ®ä¸­ä¸å®Œæ•´çš„ä¿¡æ¯ï¼Œä»¥å¢å¼ºæƒ…å¢ƒå›¾æ™¯çš„å®Œæ•´æ€§ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè™½ç„¶è¿™ä¸€ä»»åŠ¡åœ¨å½¢å¼ä¸Šå±äºæ ‡å‡†çš„ Link Prediction é—®é¢˜ï¼Œä½† Situational Knowledge Graphs å…·å¤‡çš„ç‹¬ç‰¹ç‰¹å¾ä½¿å¾—è®¸å¤šä¼ ç»Ÿçš„ Link Prediction ç®—æ³•éš¾ä»¥æœ‰æ•ˆå¤„ç†ã€‚é€šè¿‡å¯¹å®¶åº­ä»»åŠ¡çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè®ºæ–‡å‘ç°è®¸å¤šç°æœ‰ç®—æ³•åœ¨å¤„ç†æ­¤ç±»ç‰¹å®šå›¾è°±æ—¶è¡¨ç°ä¸ä½³ï¼Œç”šè‡³æ— æ³•è¶…è¶Šç®€å•çš„ Baselinesã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†æƒ…å¢ƒçŸ¥è¯†å›¾è°±ä¸é€šç”¨å›¾è°±ä¹‹é—´çš„å·®å¼‚ï¼Œå¼ºè°ƒäº†å¼€å‘é’ˆå¯¹æ€§ Link Prediction ç®—æ³•å¯¹äºæå‡å®¶åº­åœºæ™¯ä¸‹åŠ¨ä½œé¢„æµ‹å‡†ç¡®æ€§çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at Semantics 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.13675v1",
      "published_date": "2025-08-19 09:24:29 UTC",
      "updated_date": "2025-08-19 09:24:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:50:11.153531+00:00"
    },
    {
      "arxiv_id": "2508.13673v1",
      "title": "Multi-Plasticity Synergy with Adaptive Mechanism Assignment for Training Spiking Neural Networks",
      "title_zh": "é¢å‘è„‰å†²ç¥ç»ç½‘ç»œè®­ç»ƒçš„å…·æœ‰è‡ªé€‚åº”æœºåˆ¶åˆ†é…çš„å¤šå¡‘æ€§ååŒ",
      "authors": [
        "Yuzhe Liu",
        "Xin Deng",
        "Qiang Yu"
      ],
      "abstract": "Spiking Neural Networks (SNNs) are promising brain-inspired models known for low power consumption and superior potential for temporal processing, but identifying suitable learning mechanisms remains a challenge. Despite the presence of multiple coexisting learning strategies in the brain, current SNN training methods typically rely on a single form of synaptic plasticity, which limits their adaptability and representational capability. In this paper, we propose a biologically inspired training framework that incorporates multiple synergistic plasticity mechanisms for more effective SNN training. Our method enables diverse learning algorithms to cooperatively modulate the accumulation of information, while allowing each mechanism to preserve its own relatively independent update dynamics. We evaluated our approach on both static image and dynamic neuromorphic datasets to demonstrate that our framework significantly improves performance and robustness compared to conventional learning mechanism models. This work provides a general and extensible foundation for developing more powerful SNNs guided by multi-strategy brain-inspired learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å…·æœ‰ Adaptive Mechanism Assignment çš„ Multi-Plasticity Synergy æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ Spiking Neural Networks (SNNs) åœ¨å¯»æ‰¾åˆé€‚å­¦ä¹ æœºåˆ¶æ–¹é¢çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹ç›®å‰ SNNs è®­ç»ƒé€šå¸¸ä¾èµ–å•ä¸€ Synaptic Plasticity å½¢å¼è€Œå¯¼è‡´çš„é€‚åº”æ€§ä¸è¡¨å¾èƒ½åŠ›å—é™é—®é¢˜ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¼•å…¥å¤šç§ååŒå¡‘æ€§æœºåˆ¶å®ç°äº†æ›´æœ‰æ•ˆçš„æ¨¡å‹è®­ç»ƒã€‚è¯¥æ–¹æ³•å…è®¸ä¸åŒçš„å­¦ä¹ ç®—æ³•ååŒè°ƒèŠ‚ä¿¡æ¯çš„ç§¯ç´¯ï¼ŒåŒæ—¶ç¡®ä¿æ¯ä¸ªæœºåˆ¶èƒ½å¤Ÿä¿ç•™å…¶ç›¸å¯¹ç‹¬ç«‹çš„æ›´æ–°åŠ¨åŠ›å­¦ã€‚é€šè¿‡åœ¨é™æ€å›¾åƒå’ŒåŠ¨æ€ Neuromorphic Datasets ä¸Šçš„è¯„ä¼°ï¼Œå®éªŒè¯æ˜è¯¥æ¡†æ¶ç›¸æ¯”ä¼ ç»Ÿå•ä¸€å­¦ä¹ æœºåˆ¶æ¨¡å‹æ˜¾è‘—æå‡äº†æ€§èƒ½ä¸ Robustnessã€‚è¿™ä¸€å·¥ä½œä¸ºå¼€å‘å—å¤šç­–ç•¥è„‘å¯å‘å­¦ä¹ å¼•å¯¼çš„å¼ºå¤§ SNNs å¥ å®šäº†é€šç”¨ä¸”å…·æœ‰æ‰©å±•æ€§çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13673v1",
      "published_date": "2025-08-19 09:18:35 UTC",
      "updated_date": "2025-08-19 09:18:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:50:16.956036+00:00"
    },
    {
      "arxiv_id": "2508.13672v2",
      "title": "ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings",
      "title_zh": "ITL-LIMEï¼šä½èµ„æºæ•°æ®åœºæ™¯ä¸‹å¢å¼ºå±€éƒ¨è§£é‡Šçš„åŸºäºå®ä¾‹çš„è¿ç§»å­¦ä¹ ",
      "authors": [
        "Rehan Raza",
        "Guanjin Wang",
        "Kok Wai Wong",
        "Hamid Laga",
        "Marco Fisichella"
      ],
      "abstract": "Explainable Artificial Intelligence (XAI) methods, such as Local Interpretable Model-Agnostic Explanations (LIME), have advanced the interpretability of black-box machine learning models by approximating their behavior locally using interpretable surrogate models. However, LIME's inherent randomness in perturbation and sampling can lead to locality and instability issues, especially in scenarios with limited training data. In such cases, data scarcity can result in the generation of unrealistic variations and samples that deviate from the true data manifold. Consequently, the surrogate model may fail to accurately approximate the complex decision boundary of the original model. To address these challenges, we propose a novel Instance-based Transfer Learning LIME framework (ITL-LIME) that enhances explanation fidelity and stability in data-constrained environments. ITL-LIME introduces instance transfer learning into the LIME framework by leveraging relevant real instances from a related source domain to aid the explanation process in the target domain. Specifically, we employ clustering to partition the source domain into clusters with representative prototypes. Instead of generating random perturbations, our method retrieves pertinent real source instances from the source cluster whose prototype is most similar to the target instance. These are then combined with the target instance's neighboring real instances. To define a compact locality, we further construct a contrastive learning-based encoder as a weighting mechanism to assign weights to the instances from the combined set based on their proximity to the target instance. Finally, these weighted source and target instances are used to train the surrogate model for explanation purposes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)ä¸­çš„å±€éƒ¨å¯è§£é‡Šæ¨¡å‹ä¸å¯çŸ¥è§£é‡Š(LIME)æ–¹æ³•åœ¨ä½èµ„æºæ•°æ®ç¯å¢ƒä¸‹å­˜åœ¨çš„å±€éƒ¨æ€§ä¸ä¸ç¨³å®šæ€§é—®é¢˜ï¼Œæå‡ºäº† ITL-LIME æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥åŸºäºå®ä¾‹çš„è¿ç§»å­¦ä¹ (Instance-based Transfer Learning)ï¼Œåˆ©ç”¨æ¥è‡ªç›¸å…³æºé¢†åŸŸçš„çœŸå®å®ä¾‹æ¥è¾…åŠ©ç›®æ ‡é¢†åŸŸçš„è§£é‡Šè¿‡ç¨‹ï¼Œæœ‰æ•ˆè§£å†³äº†éšæœºæ‰°åŠ¨å¯èƒ½åç¦»çœŸå®æ•°æ®æµå½¢çš„é—®é¢˜ã€‚å…·ä½“æ–¹æ³•ä¸Šï¼ŒITL-LIME åˆ©ç”¨èšç±»æŠ€æœ¯åˆ’åˆ†æºé¢†åŸŸå¹¶æ£€ç´¢ç›¸ä¼¼åŸå‹ï¼Œä»è€Œè·å–ç›¸å…³çš„æºé¢†åŸŸå®ä¾‹ï¼Œå¹¶ç»“åˆåŸºäºå¯¹æ¯”å­¦ä¹ (Contrastive Learning)çš„ç¼–ç å™¨å»ºç«‹æƒé‡æœºåˆ¶ï¼Œä»¥å®šä¹‰æ›´ç´§å‡‘çš„å±€éƒ¨ç©ºé—´ã€‚é€šè¿‡è¿™äº›åŠ æƒçš„æºå®ä¾‹ä¸ç›®æ ‡å®ä¾‹è®­ç»ƒä»£ç†æ¨¡å‹ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†åœ¨æ•°æ®å—é™ç¯å¢ƒä¸‹çš„è§£é‡Šå¿ å®åº¦(Fidelity)å’Œç¨³å®šæ€§ã€‚è¯¥ç ”ç©¶ä¸ºå¢å¼ºé»‘ç›’æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹çš„å¯è§£é‡Šæ€§æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at the 34th ACM International Conference on Information and Knowledge Management (CIKM 2025)",
      "pdf_url": "https://arxiv.org/pdf/2508.13672v2",
      "published_date": "2025-08-19 09:18:20 UTC",
      "updated_date": "2025-08-21 07:04:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:50:20.851282+00:00"
    },
    {
      "arxiv_id": "2508.13666v1",
      "title": "The Hidden Cost of Readability: How Code Formatting Silently Consumes Your LLM Budget",
      "title_zh": "å¯è¯»æ€§çš„éšæ€§æˆæœ¬ï¼šä»£ç æ ¼å¼åŒ–å¦‚ä½•æ‚„ç„¶æ¶ˆè€—å¤§è¯­è¨€æ¨¡å‹é¢„ç®—",
      "authors": [
        "Dangfeng Pan",
        "Zhensu Sun",
        "Cenyuan Zhang",
        "David Lo",
        "Xiaoning Du"
      ],
      "abstract": "Source code is usually formatted with elements like indentation and newlines to improve readability for human developers. However, these visual aids do not seem to be beneficial for large language models (LLMs) in the same way since the code is processed as a linear sequence of tokens. Furthermore, these additional tokens can lead to increased computational costs and longer response times for LLMs. If such formatting elements are non-essential to LLMs, we can reduce such costs by removing them from the code. To figure out the role played by formatting elements, we conduct a comprehensive empirical study to evaluate the impact of code formatting on LLM performance and efficiency. Through large-scale experiments on Fill-in-the-Middle Code Completion tasks across four programming languages (Java, Python, C++, C\\#) and ten LLMs-including both commercial and open-source models-we systematically analyze token count and performance when formatting elements are removed. Key findings indicate that LLMs can maintain performance across formatted code and unformatted code, achieving an average input token reduction of 24.5\\% with negligible output token reductions. This makes code format removal a practical optimization strategy for improving LLM efficiency. Further exploration reveals that both prompting and fine-tuning LLMs can lead to significant reductions (up to 36.1\\%) in output code length without compromising correctness. To facilitate practical applications, we develop a bidirectional code transformation tool for format processing, which can be seamlessly integrated into existing LLM inference workflows, ensuring both human readability and LLM efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä»£ç æ ¼å¼åŒ–ï¼ˆå¦‚ç¼©è¿›å’Œæ¢è¡Œï¼‰å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ€§èƒ½å’Œæ•ˆç‡çš„å½±å“ï¼ŒæŒ‡å‡ºè¿™äº›æ—¨åœ¨æé«˜äººç±»å¯è¯»æ€§çš„è§†è§‰è¾…åŠ©å…ƒç´ ä¼šæ˜¾è‘—å¢åŠ  Token æ•°é‡å¹¶æ¨é«˜è®¡ç®—æˆæœ¬ã€‚ä½œè€…åœ¨å››ç§ç¼–ç¨‹è¯­è¨€ï¼ˆJava, Python, C++, C#ï¼‰å’Œåä¸ªä¸»æµ LLMs ä¸Šé’ˆå¯¹ Fill-in-the-Middle Code Completion ä»»åŠ¡è¿›è¡Œäº†å¤§è§„æ¨¡å®è¯ç ”ç©¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMs åœ¨å¤„ç†æ— æ ¼å¼ä»£ç æ—¶èƒ½ä¿æŒæ€§èƒ½ç¨³å®šï¼Œå¹³å‡å¯å‡å°‘ 24.5% çš„è¾“å…¥ Token æ¶ˆè€—ã€‚è¿›ä¸€æ­¥æ¢ç´¢å‘ç°ï¼Œé€šè¿‡ Prompting å’Œ Fine-tuning æŠ€æœ¯ï¼Œå¯ä»¥åœ¨ä¸æŸå¤±ä»£ç æ­£ç¡®æ€§çš„å‰æä¸‹å°†è¾“å‡ºä»£ç é•¿åº¦ç¼©çŸ­é«˜è¾¾ 36.1%ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§åŒå‘ä»£ç è½¬æ¢å·¥å…·ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ç°æœ‰ LLM æ¨ç†å·¥ä½œæµä¸­ï¼Œåœ¨å…¼é¡¾äººç±»å¯è¯»æ€§çš„åŒæ—¶æå¤§æå‡äº†æ¨¡å‹æ¨ç†æ•ˆç‡ï¼Œè¯æ˜äº†å»é™¤ä»£ç æ ¼å¼æ˜¯ä¸€ç§å®ç”¨çš„ä¼˜åŒ–ç­–ç•¥ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by ICSE'26 (First Cycle)",
      "pdf_url": "https://arxiv.org/pdf/2508.13666v1",
      "published_date": "2025-08-19 09:13:48 UTC",
      "updated_date": "2025-08-19 09:13:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:50:20.654350+00:00"
    },
    {
      "arxiv_id": "2508.14131v1",
      "title": "An Improved Multi-Agent Algorithm for Cooperative and Competitive Environments by Identifying and Encouraging Cooperation among Agents",
      "title_zh": "ä¸€ç§é€šè¿‡è¯†åˆ«ä¸æ¿€åŠ±æ™ºèƒ½ä½“é—´åˆä½œæ”¹è¿›çš„åˆä½œä¸ç«äº‰ç¯å¢ƒå¤šæ™ºèƒ½ä½“ç®—æ³•",
      "authors": [
        "Junjie Qi",
        "Siqi Mao",
        "Tianyi Tan"
      ],
      "abstract": "We propose an improved algorithm by identifying and encouraging cooperative behavior in multi-agent environments. First, we analyze the shortcomings of existing algorithms in addressing multi-agent reinforcement learning problems. Then, based on the existing algorithm MADDPG, we introduce a new parameter to increase the reward that an agent can obtain when cooperative behavior among agents is identified. Finally, we compare our improved algorithm with MADDPG in environments from PettingZoo. The results show that the new algorithm helps agents achieve both higher team rewards and individual rewards.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (Multi-Agent Reinforcement Learning) ä¸­ç°æœ‰ç®—æ³•åœ¨å¤„ç†åˆä½œä¸ç«äº‰ç¯å¢ƒæ—¶çš„å±€é™æ€§è¿›è¡Œäº†åˆ†æã€‚åœ¨ MADDPG ç®—æ³•çš„åŸºç¡€ä¸Šï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§æ”¹è¿›ç®—æ³•ï¼Œæ—¨åœ¨é€šè¿‡è¯†åˆ«å¹¶é¼“åŠ±æ™ºèƒ½ä½“ä¹‹é—´çš„åˆä½œè¡Œä¸ºæ¥æå‡ç³»ç»Ÿæ€§èƒ½ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªå…¨æ–°çš„å‚æ•°ï¼Œå½“è¯†åˆ«åˆ°æ™ºèƒ½ä½“é—´çš„åˆä½œè¡Œä¸ºæ—¶ï¼Œä¼šç›¸åº”å¢åŠ å…¶è·å¾—çš„å¥–åŠ±å€¼ã€‚å®éªŒé€šè¿‡åœ¨ PettingZoo ç¯å¢ƒä¸­ä¸åŸå§‹ MADDPG è¿›è¡Œå¯¹æ¯”ï¼ŒéªŒè¯äº†è¯¥ç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ–°ç®—æ³•ä¸ä»…èƒ½å¤Ÿæ˜¾è‘—æå‡å›¢é˜Ÿå¥–åŠ± (Team Rewards)ï¼ŒåŒæ—¶ä¹Ÿå¢åŠ äº†æ™ºèƒ½ä½“çš„ä¸ªäººå¥–åŠ± (Individual Rewards)ã€‚è¿™ä¸€ç ”ç©¶è¯æ˜äº†é€šè¿‡æ˜¾å¼æ¿€åŠ±åˆä½œæœºåˆ¶å¯ä»¥æœ‰æ•ˆä¼˜åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æ•´ä½“è¡¨ç°ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14131v1",
      "published_date": "2025-08-19 09:13:36 UTC",
      "updated_date": "2025-08-19 09:13:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:50:27.146577+00:00"
    },
    {
      "arxiv_id": "2508.13663v3",
      "title": "Interactive Query Answering on Knowledge Graphs with Soft Entity Constraints",
      "title_zh": "çŸ¥è¯†å›¾è°±ä¸Šæ”¯æŒè½¯å®ä½“çº¦æŸçš„äº¤äº’å¼æŸ¥è¯¢é—®ç­”",
      "authors": [
        "Daniel Daza",
        "Alberto Bernardi",
        "Luca Costabello",
        "Christophe Gueret",
        "Masoud Mansoury",
        "Michael Cochez",
        "Martijn Schut"
      ],
      "abstract": "Methods for query answering over incomplete knowledge graphs retrieve entities that are \\emph{likely} to be answers, which is particularly useful when such answers cannot be reached by direct graph traversal due to missing edges. However, existing approaches have focused on queries formalized using first-order-logic. In practice, many real-world queries involve constraints that are inherently vague or context-dependent, such as preferences for attributes or related categories. Addressing this gap, we introduce the problem of query answering with soft constraints. We formalize the problem and introduce two efficient methods designed to adjust query answer scores by incorporating soft constraints without disrupting the original answers to a query. These methods are lightweight, requiring tuning only two parameters or a small neural network trained to capture soft constraints while maintaining the original ranking structure. To evaluate the task, we extend existing QA benchmarks by generating datasets with soft constraints. Our experiments demonstrate that our methods can capture soft constraints while maintaining robust query answering performance and adding very little overhead. With our work, we explore a new and flexible way to interact with graph databases that allows users to specify their preferences by providing examples interactively.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸å®Œæ•´çŸ¥è¯†å›¾è°±(Knowledge Graphs)æŸ¥è¯¢å›ç­”ä¸­éš¾ä»¥å¤„ç†æ¨¡ç³Šæˆ–ä¸Šä¸‹æ–‡ç›¸å…³åå¥½çš„é—®é¢˜ï¼Œæå‡ºäº†å¸¦æœ‰è½¯çº¦æŸ(Soft Constraints)çš„æŸ¥è¯¢å›ç­”æ–°è¯¾é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…å¼•å…¥äº†ä¸¤ç§é«˜æ•ˆä¸”è½»é‡çš„æ–¹æ³•ï¼Œé€šè¿‡å¾®è°ƒå°‘é‡å‚æ•°æˆ–åˆ©ç”¨å°å‹ç¥ç»ç½‘ç»œåœ¨ä¸ç ´ååŸå§‹æ’åç»“æ„çš„å‰æä¸‹æ•´åˆè½¯çº¦æŸã€‚å®éªŒé€šè¿‡æ‰©å±•ç°æœ‰QAåŸºå‡†éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨æ•æ‰ç”¨æˆ·åå¥½çš„åŒæ—¶ä¿æŒäº†å¼ºåŠ²çš„æ€§èƒ½ï¼Œä¸”ä»…äº§ç”Ÿæä½çš„é¢å¤–å¼€é”€ã€‚è¯¥å·¥ä½œä¸ºå›¾æ•°æ®åº“æä¾›äº†ä¸€ç§çµæ´»çš„äº¤äº’æ–°æ¨¡å¼ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡äº¤äº’å¼æä¾›ç¤ºä¾‹æ¥ç²¾ç¡®è¡¨è¾¾å…¶æŸ¥è¯¢åå¥½ã€‚è¯¥ç ”ç©¶ä¸ä»…å¼¥è¡¥äº†ç°æœ‰é€»è¾‘æŸ¥è¯¢æ–¹æ³•çš„å±€é™æ€§ï¼Œä¹Ÿä¸ºæå‡å¤æ‚çŸ¥è¯†æ£€ç´¢çš„å¯æ“ä½œæ€§å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13663v3",
      "published_date": "2025-08-19 09:09:07 UTC",
      "updated_date": "2025-11-27 14:14:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:50:41.952644+00:00"
    },
    {
      "arxiv_id": "2508.13657v1",
      "title": "In-Context Decision Making for Optimizing Complex AutoML Pipelines",
      "title_zh": "ä¼˜åŒ–å¤æ‚AutoMLæµæ°´çº¿çš„ä¸Šä¸‹æ–‡å†³ç­–",
      "authors": [
        "Amir Rezaei Balef",
        "Katharina Eggensperger"
      ],
      "abstract": "Combined Algorithm Selection and Hyperparameter Optimization (CASH) has been fundamental to traditional AutoML systems. However, with the advancements of pre-trained models, modern ML workflows go beyond hyperparameter optimization and often require fine-tuning, ensembling, and other adaptation techniques. While the core challenge of identifying the best-performing model for a downstream task remains, the increasing heterogeneity of ML pipelines demands novel AutoML approaches. This work extends the CASH framework to select and adapt modern ML pipelines. We propose PS-PFN to efficiently explore and exploit adapting ML pipelines by extending Posterior Sampling (PS) to the max k-armed bandit problem setup. PS-PFN leverages prior-data fitted networks (PFNs) to efficiently estimate the posterior distribution of the maximal value via in-context learning. We show how to extend this method to consider varying costs of pulling arms and to use different PFNs to model reward distributions individually per arm. Experimental results on one novel and two existing standard benchmark tasks demonstrate the superior performance of PS-PFN compared to other bandit and AutoML strategies. We make our code and data available at https://github.com/amirbalef/CASHPlus.",
      "tldr_zh": "è¯¥ç ”ç©¶å°†ä¼ ç»Ÿçš„ç®—æ³•é€‰æ‹©ä¸è¶…å‚æ•°ä¼˜åŒ–(CASH)æ¡†æ¶æ‰©å±•åˆ°ç°ä»£å¤æ‚çš„æœºå™¨å­¦ä¹ æµæ°´çº¿ä¸­ï¼Œä»¥åº”å¯¹å¾®è°ƒå’Œé›†æˆç­‰æŠ€æœ¯å¸¦æ¥çš„å¼‚æ„æ€§æŒ‘æˆ˜ã€‚ä½œè€…æå‡ºäº†PS-PFNæ–¹æ³•ï¼Œé€šè¿‡å°†åéªŒé‡‡æ ·(Posterior Sampling)å¼•å…¥æœ€å¤§k-è‡‚å¼ºç›—(max k-armed bandit)é—®é¢˜ï¼Œå®ç°äº†å¯¹æµæ°´çº¿çš„é«˜æ•ˆæ¢ç´¢ä¸åˆ©ç”¨ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å…ˆéªŒæ•°æ®æ‹Ÿåˆç½‘ç»œ(Prior-data Fitted Networks, PFNs)é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ (In-Context Learning)é«˜æ•ˆä¼°ç®—æœ€å¤§å€¼çš„åéªŒåˆ†å¸ƒï¼Œå¹¶èƒ½å¤Ÿå¤„ç†ä¸åŒçš„æ“ä½œæˆæœ¬åŠä¸ªæ€§åŒ–çš„å¥–åŠ±åˆ†å¸ƒã€‚åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœè¯æ˜ï¼ŒPS-PFNåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼ºç›—ç®—æ³•å’ŒAutoMLç­–ç•¥ã€‚è¯¥ç ”ç©¶ä¸ºä¼˜åŒ–ç°ä»£æœºå™¨å­¦ä¹ å·¥ä½œæµæä¾›äº†ä¸€ç§å¼ºæœ‰åŠ›ä¸”çµæ´»çš„è‡ªåŠ¨åŒ–å†³ç­–æ‰‹æ®µã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13657v1",
      "published_date": "2025-08-19 09:05:16 UTC",
      "updated_date": "2025-08-19 09:05:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:50:47.463674+00:00"
    },
    {
      "arxiv_id": "2508.13654v4",
      "title": "Input-Time Scaling",
      "title_zh": "è¾“å…¥æ—¶ç¼©æ”¾",
      "authors": [
        "Rapheal Huang",
        "Weilong Guo"
      ],
      "abstract": "Current Large Language Models (LLMs) are usually post-trained on large-scale carefully curated datasets (data & training scaling) and doing reasoning in test time (inference time scaling). In this work, we present a new scaling paradigm, Input-Time Scaling, to complement previous scaling methods by putting resources on queries (input time). During training and testing, we utilize meta-knowledge from LLMs to refine inputs with different strategies. We also discover a new phenomenon, train-test co-design. It requires us to apply query strategies during training and testing as a whole. Only applying strategies on training or testing would seriously degrade the performance gained. We are also surprised to find that seemingly low data quality datasets can perform better. We can get the best performance even by adding irrelevant information to the queries, with randomly selected 1k examples from a minimally filtered dataset. These findings contradict the widely held inductive bias, \"garbage in, garbage out\". Curating datasets with seemingly high-quality data can even potentially limit the performance ceiling. In addition, models trained on more data with similar quality (15k VS 1k) perform worse, the intuition of simply scaling the size should also be carefully inspected. The good news is that our findings are compatible with the Less is More phenomenon. 1K examples are enough to invoke high-level reasoning ability. With experiments on Qwen2.5-32B-Instruct, we are able to reach SOTA performance among 32B models on AIME24(76.7%) and AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B, the result would be 90.0% on AIME24 and 80.0% on AIME25. To facilitate reproducibility and further research, we are working on open-source our datasets, data pipelines, evaluation results, and checkpoints.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Input-Time Scalingï¼Œè¿™æ˜¯ä¸€ç§å…¨æ–°çš„æ‰©å±•èŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡åœ¨è¾“å…¥é˜¶æ®µæŠ•å…¥èµ„æºæ¥è¡¥å……ç°æœ‰çš„æ•°æ®ã€è®­ç»ƒå’Œæ¨ç†æ—¶é—´æ‰©å±•æ–¹æ³•ã€‚åœ¨è®­ç»ƒå’Œæµ‹è¯•è¿‡ç¨‹ä¸­ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„Meta-knowledgeå¯¹è¾“å…¥æŸ¥è¯¢è¿›è¡Œç­–ç•¥æ€§ä¼˜åŒ–ã€‚ç ”ç©¶å‘ç°äº†ä¸€ä¸ªåä¸ºTrain-test co-designçš„å…³é”®ç°è±¡ï¼Œå³å¿…é¡»åœ¨è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µååŒåº”ç”¨æŸ¥è¯¢ç­–ç•¥ï¼Œå¦åˆ™ä¼šä¸¥é‡é™ä½æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®éªŒç»“æœæŒ‘æˆ˜äº†ä¼ ç»Ÿçš„â€œåƒåœ¾è¿›ï¼Œåƒåœ¾å‡ºâ€ï¼ˆGarbage in, garbage outï¼‰åè§ï¼Œå‘ç°å³ä¾¿åœ¨æŸ¥è¯¢ä¸­åŠ å…¥æ— å…³ä¿¡æ¯ï¼Œä»…éœ€1kæ¡ä½è¿‡æ»¤æ•°æ®ä¹Ÿèƒ½æ¿€å‘æ¨¡å‹çš„é«˜çº§æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡åœ¨Qwen2.5-32B-Instructä¸Šè¿›è¡Œå®éªŒï¼Œè¯¥æ–¹æ³•åœ¨AIME24å’ŒAIME25æµ‹è¯•ä¸­è¾¾åˆ°äº†32Bæ¨¡å‹çš„SOTAæ€§èƒ½ã€‚æœ€ç»ˆï¼Œç»“åˆDeepSeek-R1-Distill-Qwen-32Bï¼Œç ”ç©¶åœ¨AIME24ä¸Šå®ç°äº†90.0%çš„Pass@1å‡†ç¡®ç‡ï¼Œä¸ºé«˜æ•ˆæå‡å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13654v4",
      "published_date": "2025-08-19 09:04:13 UTC",
      "updated_date": "2025-09-12 07:04:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:50:50.146790+00:00"
    },
    {
      "arxiv_id": "2508.13653v2",
      "title": "GRAFT: Gradient-Aware Fast MaxVol Technique for Dynamic Data Sampling",
      "title_zh": "GRAFTï¼šé¢å‘åŠ¨æ€æ•°æ®é‡‡æ ·çš„æ¢¯åº¦æ„ŸçŸ¥å¿«é€Ÿ MaxVol æŠ€æœ¯",
      "authors": [
        "Ashish Jha",
        "Anh huy Phan",
        "Razan Dibo",
        "Valentin Leplat"
      ],
      "abstract": "Training modern neural networks on large datasets is computationally and environmentally costly. We introduce GRAFT, a scalable in-training subset selection method that (i) extracts a low-rank feature representation for each batch, (ii) applies a Fast MaxVol sampler to select a small, diverse subset that spans the batch's dominant subspace, and (iii) dynamically adjusts the subset size using a gradient-approximation criterion. By operating in low-rank subspaces and training on carefully chosen examples instead of full batches, GRAFT preserves the training trajectory while reducing wall-clock time, energy consumption, and $\\mathrm{CO}_2$ emissions. Across multiple benchmarks, GRAFT matches or exceeds recent selection baselines in both accuracy and efficiency, providing a favorable trade-off between accuracy, efficiency, and emissions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GRAFTï¼Œä¸€ç§å¯æ‰©å±•çš„è®­ç»ƒå†…å­é›†é€‰æ‹©(in-training subset selection)æ–¹æ³•ï¼Œæ—¨åœ¨é™ä½ç°ä»£ç¥ç»ç½‘ç»œåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒæ—¶é¢ä¸´çš„é«˜æ˜‚è®¡ç®—ä¸ç¯å¢ƒæˆæœ¬ã€‚è¯¥æ–¹æ³•é¦–å…ˆä¸ºæ¯ä¸ªæ‰¹æ¬¡æå–ä½ç§©ç‰¹å¾è¡¨ç¤º(low-rank feature representation)ï¼Œéšååº”ç”¨ Fast MaxVol é‡‡æ ·å™¨ä»ä¸­æŒ‘é€‰å‡ºèƒ½å¤Ÿè¦†ç›–è¯¥æ‰¹æ¬¡ä¸»å­ç©ºé—´çš„å°‘é‡ä¸”å¤šæ ·åŒ–çš„å­é›†ã€‚GRAFT è¿›ä¸€æ­¥åˆ©ç”¨æ¢¯åº¦è¿‘ä¼¼å‡†åˆ™(gradient-approximation criterion)åŠ¨æ€è°ƒæ•´å­é›†å¤§å°ï¼Œä»è€Œåœ¨ä½ç§©å­ç©ºé—´ä¸­é«˜æ•ˆè®­ç»ƒå¹¶ç¡®ä¿ä¿ç•™åŸå§‹è®­ç»ƒè½¨è¿¹(training trajectory)ã€‚é€šè¿‡å¯¹è¿™äº›ç²¾å¿ƒç­›é€‰çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†å®é™…è¿è¡Œæ—¶é—´(wall-clock time)ã€èƒ½æºæ¶ˆè€—ä»¥åŠäºŒæ°§åŒ–ç¢³æ’æ”¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGRAFT åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡å’Œæ•ˆç‡å‡è¾¾åˆ°æˆ–è¶…è¿‡äº†ç°æœ‰çš„å­é›†é€‰æ‹©åŸºå‡†æ¨¡å‹ï¼Œåœ¨æ€§èƒ½ä¸ç¯å¢ƒå½±å“ä¹‹é—´å®ç°äº†æ›´ä¼˜çš„æƒè¡¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13653v2",
      "published_date": "2025-08-19 09:03:39 UTC",
      "updated_date": "2025-08-22 14:54:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:50:47.654956+00:00"
    },
    {
      "arxiv_id": "2508.13634v3",
      "title": "V2P: Visual Attention Calibration for GUI Grounding via Background Suppression and Center Peaking",
      "title_zh": "V2Pï¼šåŸºäºèƒŒæ™¯æŠ‘åˆ¶ä¸ä¸­å¿ƒèšç„¦çš„ GUI å®šä½è§†è§‰æ³¨æ„åŠ›æ ¡å‡†",
      "authors": [
        "Jikai Chen",
        "Long Chen",
        "Dong Wang",
        "Qinglin Su",
        "Zhixuan Chu",
        "Bingguang Hao",
        "Leilei Gan",
        "Chenyi Zhuang",
        "Jinjie Gu"
      ],
      "abstract": "Precise localization of GUI elements is crucial for the development of GUI agents. Traditional methods rely on bounding box or center-point regression, neglecting spatial interaction uncertainty and visual-semantic hierarchies. Recent methods incorporate attention mechanisms but still face two key issues: (1) ignoring processing background regions causes attention drift from the desired area, and (2) uniform modeling the target UI element fails to distinguish between its center and edges, leading to click imprecision. Inspired by how humans visually process and interact with GUI elements, we propose the Valley-to-Peak (V2P) method to address these issues. To mitigate background distractions, V2P introduces a suppression attention mechanism that minimizes the model's focus on irrelevant regions to highlight the intended region. For the issue of center-edge distinction, V2P applies a Fitts' Law-inspired approach by modeling GUI interactions as 2D Gaussian heatmaps where the weight gradually decreases from the center towards the edges. The weight distribution follows a Gaussian function, with the variance determined by the target's size. Consequently, V2P effectively isolates the target area and teaches the model to concentrate on the most essential point of the UI element. The model trained by V2P achieves the performance with 92.4\\% and 52.5\\% on two benchmarks ScreenSpot-v2 and ScreenSpot-Pro. Ablations further confirm each component's contribution, underscoring V2P's generalizability in precise GUI grounding tasks and its potential for real-world deployment in future GUI agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾å½¢ç”¨æˆ·ç•Œé¢(GUI)å…ƒç´ ç²¾å‡†å®šä½çš„éš¾é¢˜ï¼Œæå‡ºäº†V2P (Valley-to-Peak) æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ³¨æ„åŠ›æœºåˆ¶å­˜åœ¨çš„èƒŒæ™¯å¹²æ‰°å’Œä¸­å¿ƒè¾¹ç¼˜åŒºåˆ†ä¸è¶³çš„é—®é¢˜ã€‚å—äººç±»è§†è§‰å¤„ç†å¯å‘ï¼ŒV2På¼•å…¥äº†æŠ‘åˆ¶æ³¨æ„åŠ›(suppression attention)æœºåˆ¶ï¼Œé€šè¿‡æœ€å°åŒ–å¯¹æ— å…³èƒŒæ™¯åŒºåŸŸçš„å…³æ³¨æ¥çªå‡ºç›®æ ‡åŒºåŸŸã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•å€Ÿé‰´è²èŒ¨å®šå¾‹(Fitts' Law)ï¼Œå°†GUIäº¤äº’å»ºæ¨¡ä¸ºäºŒç»´é«˜æ–¯çƒ­å›¾(2D Gaussian heatmaps)ï¼Œä½¿æƒé‡ä»ä¸­å¿ƒå‘è¾¹ç¼˜é€’å‡ï¼Œç¡®ä¿æ¨¡å‹èšç„¦äºUIå…ƒç´ æœ€å…³é”®çš„ä½ç½®ã€‚å®éªŒè¡¨æ˜ï¼ŒV2Påœ¨ScreenSpot-v2å’ŒScreenSpot-Proä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«å–å¾—äº†92.4%å’Œ52.5%çš„ä¼˜å¼‚æˆç»©ã€‚è¿™é¡¹å·¥ä½œæ˜¾è‘—æå‡äº†GUI Groundingçš„ç²¾ç¡®åº¦ï¼ŒéªŒè¯äº†V2Påœ¨å¤æ‚ç•Œé¢ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›åŠå…¶åœ¨æœªæ¥GUIæ™ºèƒ½ä½“ä¸­çš„å®é™…åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13634v3",
      "published_date": "2025-08-19 08:47:44 UTC",
      "updated_date": "2026-01-20 06:39:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:50:53.657235+00:00"
    },
    {
      "arxiv_id": "2508.13625v1",
      "title": "Towards a Larger Model via One-Shot Federated Learning on Heterogeneous Client Models",
      "title_zh": "é€šè¿‡å¼‚æ„å®¢æˆ·ç«¯æ¨¡å‹ä¸Šçš„å•æ¬¡è”é‚¦å­¦ä¹ æ„å»ºæ›´å¤§æ¨¡å‹",
      "authors": [
        "Wenxuan Ye",
        "Xueli An",
        "Onur Ayan",
        "Junfan Wang",
        "Xueqiang Yan",
        "Georg Carle"
      ],
      "abstract": "Large models, renowned for superior performance, outperform smaller ones even without billion-parameter scales. While mobile network servers have ample computational resources to support larger models than client devices, privacy constraints prevent clients from directly sharing their raw data. Federated Learning (FL) enables decentralized clients to collaboratively train a shared model by exchanging model parameters instead of transmitting raw data. Yet, it requires a uniform model architecture and multiple communication rounds, which neglect resource heterogeneity, impose heavy computational demands on clients, and increase communication overhead. To address these challenges, we propose FedOL, to construct a larger and more comprehensive server model in one-shot settings (i.e., in a single communication round). Instead of model parameter sharing, FedOL employs knowledge distillation, where clients only exchange model prediction outputs on an unlabeled public dataset. This reduces communication overhead by transmitting compact predictions instead of full model weights and enables model customization by allowing heterogeneous model architectures. A key challenge in this setting is that client predictions may be biased due to skewed local data distributions, and the lack of ground-truth labels in the public dataset further complicates reliable learning. To mitigate these issues, FedOL introduces a specialized objective function that iteratively refines pseudo-labels and the server model, improving learning reliability. To complement this, FedOL incorporates a tailored pseudo-label generation and knowledge distillation strategy that effectively integrates diverse knowledge. Simulation results show that FedOL significantly outperforms existing baselines, offering a cost-effective solution for mobile networks where clients possess valuable private data but limited computational resources.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FedOLï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè”é‚¦å­¦ä¹ (Federated Learning)åœ¨å¤„ç†å¼‚æ„å®¢æˆ·ç«¯æ¨¡å‹æ—¶é¢ä¸´çš„é€šä¿¡å¼€é”€å¤§ã€å¯¹å®¢æˆ·ç«¯è®¡ç®—èµ„æºè¦æ±‚é«˜ä»¥åŠæ¨¡å‹æ¶æ„å¿…é¡»ç»Ÿä¸€ç­‰å±€é™ã€‚FedOLé‡‡ç”¨å•æ¬¡é€šä¿¡(One-Shot)è®¾ç½®ï¼Œåˆ©ç”¨çŸ¥è¯†è’¸é¦(Knowledge Distillation)æŠ€æœ¯è®©å®¢æˆ·ç«¯ä»…åœ¨æ— æ ‡ç­¾å…¬å…±æ•°æ®é›†ä¸Šäº¤æ¢é¢„æµ‹è¾“å‡ºï¼Œä»è€Œæ˜¾è‘—é™ä½äº†é€šä¿¡è´Ÿæ‹…å¹¶æ”¯æŒå¼‚æ„æ¨¡å‹æ¶æ„ã€‚é’ˆå¯¹æœ¬åœ°æ•°æ®åˆ†å¸ƒä¸å‡(Skewed Local Data Distributions)å¯¼è‡´çš„é¢„æµ‹åå·®åŠå…¬å…±æ•°æ®ç¼ºä¹çœŸå€¼æ ‡ç­¾çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†ä¸“é—¨çš„ç›®æ ‡å‡½æ•°æ¥è¿­ä»£ç²¾ç‚¼ä¼ªæ ‡ç­¾(Pseudo-labels)ä¸æœåŠ¡å™¨æ¨¡å‹ï¼Œä»¥æå‡å­¦ä¹ çš„å¯é æ€§ã€‚æ­¤å¤–ï¼ŒFedOLé€šè¿‡å®šåˆ¶çš„ä¼ªæ ‡ç­¾ç”Ÿæˆå’ŒçŸ¥è¯†è’¸é¦ç­–ç•¥æœ‰æ•ˆæ•´åˆäº†æ¥è‡ªä¸åŒå®¢æˆ·ç«¯çš„å¤šå…ƒçŸ¥è¯†ã€‚ä»¿çœŸå®éªŒè¯æ˜ï¼ŒFedOLçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†ï¼Œä¸ºèµ„æºå—é™ä¸”æ‹¥æœ‰ç§æœ‰æ•°æ®çš„ç§»åŠ¨ç½‘ç»œç¯å¢ƒæä¾›äº†ä¸€ç§é«˜æ•ˆçš„å¤§æ¨¡å‹æ„å»ºæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to Globecom 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.13625v1",
      "published_date": "2025-08-19 08:35:25 UTC",
      "updated_date": "2025-08-19 08:35:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:50:56.549135+00:00"
    },
    {
      "arxiv_id": "2508.16648v1",
      "title": "LatentFlow: Cross-Frequency Experimental Flow Reconstruction from Sparse Pressure via Latent Mapping",
      "title_zh": "LatentFlowï¼šåŸºäºæ½œç©ºé—´æ˜ å°„çš„ç¨€ç–å‹åŠ›è·¨é¢‘ç‡å®éªŒæµåœºé‡æ„",
      "authors": [
        "Junle Liu",
        "Chang Liu",
        "Yanyu Ke",
        "Qiuxiang Huang",
        "Jiachen Zhao",
        "Wenliang Chen",
        "K. T. Tse",
        "Gang Hu"
      ],
      "abstract": "Acquiring temporally high-frequency and spatially high-resolution turbulent wake flow fields in particle image velocimetry (PIV) experiments remains a significant challenge due to hardware limitations and measurement noise. In contrast, temporal high-frequency measurements of spatially sparse wall pressure are more readily accessible in wind tunnel experiments. In this study, we propose a novel cross-modal temporal upscaling framework, LatentFlow, which reconstructs high-frequency (512 Hz) turbulent wake flow fields by fusing synchronized low-frequency (15 Hz) flow field and pressure data during training, and high-frequency wall pressure signals during inference. The first stage involves training a pressure-conditioned $Î²$-variation autoencoder ($p$C-$Î²$-VAE) to learn a compact latent representation that captures the intrinsic dynamics of the wake flow. A secondary network maps synchronized low-frequency wall pressure signals into the latent space, enabling reconstruction of the wake flow field solely from sparse wall pressure. Once trained, the model utilizes high-frequency, spatially sparse wall pressure inputs to generate corresponding high-frequency flow fields via the $p$C-$Î²$-VAE decoder. By decoupling the spatial encoding of flow dynamics from temporal pressure measurements, LatentFlow provides a scalable and robust solution for reconstructing high-frequency turbulent wake flows in data-constrained experimental settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LatentFlowï¼Œä¸€ç§è·¨æ¨¡æ€çš„æ—¶é—´ä¸Šé‡‡æ ·(Temporal Upscaling)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç¨€ç–çš„å‹åŠ›æµ‹é‡æ•°æ®é‡å»ºé«˜é¢‘ä¸”é«˜ç©ºé—´åˆ†è¾¨ç‡çš„æ¹æµå°¾è¿¹æµåœºã€‚è¯¥æ–¹æ³•åˆ©ç”¨å‹åŠ›è°ƒèŠ‚çš„$\\beta$å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨($p$C-$\\beta$-VAE)åœ¨è®­ç»ƒé˜¶æ®µå­¦ä¹ æµä½“åŠ¨åŠ›å­¦çš„ç´§å‡‘æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µå°†é«˜é¢‘ã€ç©ºé—´ç¨€ç–çš„å£é¢å‹åŠ›ä¿¡å·æ˜ å°„è‡³æ½œåœ¨ç©ºé—´ã€‚é€šè¿‡å°†æµä½“åŠ¨åŠ›å­¦çš„ç©ºé—´ç¼–ç ä¸æ—¶é—´å‹åŠ›æµ‹é‡è§£è€¦ï¼ŒLatentFlowæˆåŠŸå®ç°äº†ä»15 Hzä½é¢‘åŒæ­¥æ•°æ®å‘512 Hzé«˜é¢‘æµåœºçš„é‡å»ºã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ•°æ®å—é™çš„å®éªŒè®¾ç½®ä¸‹è¡¨ç°å‡ºæå¼ºçš„ç¨³å¥æ€§ï¼Œä¸ºå…‹æœç¡¬ä»¶é™åˆ¶å¹¶è·å–é«˜é¢‘æ¹æµåŠ¨åŠ›å­¦ä¿¡æ¯æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.flu-dyn"
      ],
      "primary_category": "cs.LG",
      "comment": "The paper is submitted to IAAI26. Total 9 pages with 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.16648v1",
      "published_date": "2025-08-19 08:29:18 UTC",
      "updated_date": "2025-08-19 08:29:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:51:00.153497+00:00"
    },
    {
      "arxiv_id": "2508.13607v1",
      "title": "Bounding Causal Effects and Counterfactuals",
      "title_zh": "å› æœæ•ˆåº”ä¸åäº‹å®çš„ç•Œå®š",
      "authors": [
        "Tobias Maringgele"
      ],
      "abstract": "Causal inference often hinges on strong assumptions - such as no unmeasured confounding or perfect compliance - that are rarely satisfied in practice. Partial identification offers a principled alternative: instead of relying on unverifiable assumptions to estimate causal effects precisely, it derives bounds that reflect the uncertainty inherent in the data. Despite its theoretical appeal, partial identification remains underutilized in applied work, in part due to the fragmented nature of existing methods and the lack of practical guidance. This thesis addresses these challenges by systematically comparing a diverse set of bounding algorithms across multiple causal scenarios. We implement, extend, and unify state-of-the-art methods - including symbolic, optimization-based, and information-theoretic approaches - within a common evaluation framework. In particular, we propose an extension of a recently introduced entropy-bounded method, making it applicable to counterfactual queries such as the Probability of Necessity and Sufficiency (PNS). Our empirical study spans thousands of randomized simulations involving both discrete and continuous data-generating processes. We assess each method in terms of bound tightness, computational efficiency, and robustness to assumption violations. To support practitioners, we distill our findings into a practical decision tree for algorithm selection and train a machine learning model to predict the best-performing method based on observable data characteristics.\n  All implementations are released as part of an open-source Python package, CausalBoundingEngine, which enables users to apply and compare bounding methods through a unified interface.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å› æœæ¨æ–­ä¸­å¼ºå‡è®¾éš¾ä»¥æ»¡è¶³çš„é—®é¢˜ï¼Œæ¢è®¨äº†å±€éƒ¨è¯†åˆ«(Partial Identification)ä½œä¸ºä¸€ç§æ¨å¯¼ç•Œé™(Bounds)ä»¥åæ˜ ä¸ç¡®å®šæ€§çš„åŸåˆ™æ€§æ›¿ä»£æ–¹æ¡ˆã€‚è®ºæ–‡ç³»ç»Ÿæ€§åœ°æ¯”è¾ƒã€æ‰©å±•å¹¶ç»Ÿä¸€äº†åŒ…æ‹¬ç¬¦å·æ¨ç†(Symbolic)ã€åŸºäºä¼˜åŒ–(Optimization-based)å’Œä¿¡æ¯è®º(Information-theoretic)åœ¨å†…çš„å¤šç§å‰æ²¿ç•Œé™ç®—æ³•ã€‚ç‰¹åˆ«åœ°ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§ç†µç•Œé™(Entropy-bounded)æ–¹æ³•çš„æ‰©å±•ï¼Œä½¿å…¶èƒ½å¤Ÿåº”ç”¨äºå¿…è¦æ€§ä¸å……åˆ†æ€§æ¦‚ç‡(PNS)ç­‰åäº‹å®(Counterfactual)æŸ¥è¯¢ã€‚é€šè¿‡å¯¹æ•°åƒæ¬¡æ¶µç›–ç¦»æ•£ä¸è¿ç»­æ•°æ®çš„éšæœºæ¨¡æ‹Ÿï¼Œå®éªŒä»ç•Œé™ç´§å¯†åº¦ã€è®¡ç®—æ•ˆç‡åŠå¯¹å‡è®¾è¿åçš„é²æ£’æ€§ç­‰æ–¹é¢è¯„ä¼°äº†å„æ–¹æ³•çš„æ€§èƒ½ã€‚ä¸ºæŒ‡å¯¼å®è·µï¼Œç ”ç©¶æç‚¼å‡ºäº†ç®—æ³•é€‰æ‹©çš„å†³ç­–æ ‘ï¼Œå¹¶è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ä»¥é¢„æµ‹æœ€ä½³æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶æˆæœå·²é›†æˆè‡³å¼€æº Python å·¥å…·åŒ… CausalBoundingEngineï¼Œä¸ºç”¨æˆ·æä¾›äº†ç»Ÿä¸€çš„ç•Œé¢æ¥åº”ç”¨å’Œå¯¹æ¯”ä¸åŒçš„å› æœç•Œé™æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "Bachelor's thesis, Technical University of Munich, 2025. 102 pages, 20 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.13607v1",
      "published_date": "2025-08-19 08:13:34 UTC",
      "updated_date": "2025-08-19 08:13:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:51:02.750941+00:00"
    },
    {
      "arxiv_id": "2508.13603v1",
      "title": "Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM",
      "title_zh": "è°åœ¨å‘å£°ï¼Ÿè¯­éŸ³å¤§è¯­è¨€æ¨¡å‹è¯´è¯äººåˆ†é…ä¸­çš„æ€§åˆ«åè§æ¢ç©¶",
      "authors": [
        "Dariia Puhach",
        "Amir H. Payberah",
        "Ã‰va SzÃ©kely"
      ],
      "abstract": "Similar to text-based Large Language Models (LLMs), Speech-LLMs exhibit emergent abilities and context awareness. However, whether these similarities extend to gender bias remains an open question. This study proposes a methodology leveraging speaker assignment as an analytic tool for bias investigation. Unlike text-based models, which encode gendered associations implicitly, Speech-LLMs must produce a gendered voice, making speaker selection an explicit bias cue. We evaluate Bark, a Text-to-Speech (TTS) model, analyzing its default speaker assignments for textual prompts. If Bark's speaker selection systematically aligns with gendered associations, it may reveal patterns in its training data or model design. To test this, we construct two datasets: (i) Professions, containing gender-stereotyped occupations, and (ii) Gender-Colored Words, featuring gendered connotations. While Bark does not exhibit systematic bias, it demonstrates gender awareness and has some gender inclinations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹(Speech-LLMs)ä¸­çš„æ€§åˆ«åè§(gender bias)ï¼Œå¹¶æå‡ºå°†è¯´è¯è€…åˆ†é…(speaker assignment)ä½œä¸ºä¸€ç§è°ƒæŸ¥åè§çš„åˆ†æå·¥å…·ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œä¸æ–‡æœ¬æ¨¡å‹éšå¼ç¼–ç æ€§åˆ«å…³è”ä¸åŒï¼Œè¯­éŸ³æ¨¡å‹åœ¨ç”Ÿæˆå£°éŸ³æ—¶å¿…é¡»è¿›è¡Œæ˜ç¡®çš„è¯´è¯è€…é€‰æ‹©ï¼Œè¿™ä¸ºåè§è¯†åˆ«æä¾›äº†æ˜¾å¼çº¿ç´¢ã€‚ç ”ç©¶äººå‘˜é€šè¿‡æ„å»ºèŒä¸š(Professions)å’Œæ€§åˆ«è‰²å½©è¯æ±‡(Gender-Colored Words)ä¸¤ä¸ªæ•°æ®é›†ï¼Œå¯¹æ–‡æœ¬è½¬è¯­éŸ³(TTS)æ¨¡å‹Barkçš„é»˜è®¤è¯´è¯è€…åˆ†é…è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡Barkå¹¶æœªè¡¨ç°å‡ºç³»ç»Ÿæ€§çš„æ€§åˆ«åè§ï¼Œä½†å®ƒå±•ç°å‡ºäº†æ˜æ˜¾çš„æ€§åˆ«æ„è¯†(gender awareness)ä»¥åŠä¸€å®šç¨‹åº¦çš„æ€§åˆ«å€¾å‘(gender inclinations)ã€‚è¯¥ç ”ç©¶ä¸ºç†è§£å’Œé‡åŒ–ç”Ÿæˆå¼éŸ³é¢‘æ¨¡å‹ä¸­çš„ç¤¾ä¼šåè§æä¾›äº†æ–°é¢–çš„å®éªŒæ–¹æ³•å’Œè§†è§’ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13603v1",
      "published_date": "2025-08-19 08:10:55 UTC",
      "updated_date": "2025-08-19 08:10:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:51:07.158845+00:00"
    },
    {
      "arxiv_id": "2508.13587v1",
      "title": "Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation",
      "title_zh": "çªç ´ SFT ç“¶é¢ˆï¼šé¢å‘å›¾è¡¨åˆ°ä»£ç ç”Ÿæˆçš„å¤šæ¨¡æ€ç»“æ„åŒ–å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Lei Chen",
        "Xuanle Zhao",
        "Zhixiong Zeng",
        "Jing Huang",
        "Liming Zheng",
        "Yufeng Zhong",
        "Lin Ma"
      ],
      "abstract": "While reinforcement learning (RL) has proven highly effective for general reasoning in vision-language models, its application to tasks requiring in-depth understanding of information-rich images and generation of structured outputs remains underexplored. Chart-to-code generation exemplifies this challenge, demanding complex reasoning over visual charts to generate structured code. Supervised fine-tuning (SFT) alone is often insufficient, highlighting the need for effective RL strategies that appropriately reward structured outputs. We systematically investigate the performance plateau in SFT through large-scale experiments and propose Multimodal Structured Reinforcement Learning (MSRL) for chart-to-code generation, which substantially breaks through this plateau. We construct the largest training corpus to date, containing 3 million chart-code pairs from real-world arXiv tables to mitigate simplistic patterns of prior synthetic data. Despite reaching state-of-the-art performance, our experiments show that scaling SFT data eventually hits a plateau where further increases yield negligible improvements. Our MSRL method leverages a multi-granularity structured reward system using multimodal textual and visual feedback. At the textual level, rule-based rewards validate fine-grained code details. At the visual level, model-based rewards assess structural similarity by rendering generated code into images and employing an evaluator model. We implement this within a two-stage curriculum for training stability. Results demonstrate that MSRL significantly breaks the SFT plateau, improving high-level metrics by 6.2% and 9.9% on ChartMimic and ReachQA benchmarks respectively, achieving competitive performance with advanced closed-source models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Chart-to-Code ç”Ÿæˆä»»åŠ¡ä¸­ç›‘ç£å¾®è°ƒ (Supervised Fine-tuning, SFT) å­˜åœ¨çš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜ï¼Œæå‡ºå¤šæ¨¡æ€ç»“æ„åŒ–å¼ºåŒ–å­¦ä¹  (Multimodal Structured Reinforcement Learning, MSRL) æ¡†æ¶ã€‚ä¸ºäº†å…‹æœåˆæˆæ•°æ®çš„å±€é™æ€§ï¼Œä½œè€…æ„å»ºäº†åŒ…å« 300 ä¸‡ä¸ªçœŸå® arXiv å›¾è¡¨-ä»£ç å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚MSRL å¼•å…¥äº†ä¸€ç§å¤šç²’åº¦ç»“æ„åŒ–å¥–åŠ±ç³»ç»Ÿï¼Œé€šè¿‡æ–‡æœ¬å±‚é¢çš„è§„åˆ™å¥–åŠ±æ ¡éªŒä»£ç ç»†èŠ‚ï¼Œå¹¶ç»“åˆè§†è§‰å±‚é¢çš„æ¸²æŸ“å›¾åƒè¯„ä¼°ç»“æ„ç›¸ä¼¼æ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µè¯¾ç¨‹å­¦ä¹  (Two-stage Curriculum) ç­–ç•¥ï¼Œæœ‰æ•ˆæå‡äº†è®­ç»ƒçš„ç¨³å®šæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMSRL æˆåŠŸçªç ´äº† SFT çš„æ€§èƒ½å¹³å°ï¼Œåœ¨ ChartMimic å’Œ ReachQA åŸºå‡†æµ‹è¯•ä¸Šåˆ†åˆ«æå‡äº† 6.2% å’Œ 9.9% çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä½¿å¼€æºæ¨¡å‹åœ¨å›¾è¡¨ä»£ç ç”Ÿæˆé¢†åŸŸè¾¾åˆ°äº†ä¸å…ˆè¿›é—­æºæ¨¡å‹å…·æœ‰ç«äº‰åŠ›çš„æ°´å¹³ï¼Œä¸ºå¤„ç†ä¿¡æ¯å¯†é›†å‹å›¾åƒçš„ç»“æ„åŒ–è¾“å‡ºä»»åŠ¡æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "technical report",
      "pdf_url": "https://arxiv.org/pdf/2508.13587v1",
      "published_date": "2025-08-19 07:40:18 UTC",
      "updated_date": "2025-08-19 07:40:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:51:09.949043+00:00"
    },
    {
      "arxiv_id": "2508.13580v1",
      "title": "A Comparative Study of Decoding Strategies in Medical Text Generation",
      "title_zh": "åŒ»ç–—æ–‡æœ¬ç”Ÿæˆä¸­çš„è§£ç ç­–ç•¥å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Oriana Presacan",
        "Alireza Nik",
        "Vajira Thambawita",
        "Bogdan Ionescu",
        "Michael Riegler"
      ],
      "abstract": "Large Language Models (LLMs) rely on various decoding strategies to generate text, and these choices can significantly affect output quality. In healthcare, where accuracy is critical, the impact of decoding strategies remains underexplored. We investigate this effect in five open-ended medical tasks, including translation, summarization, question answering, dialogue, and image captioning, evaluating 11 decoding strategies with medically specialized and general-purpose LLMs of different sizes. Our results show that deterministic strategies generally outperform stochastic ones: beam search achieves the highest scores, while Î· and top-k sampling perform worst. Slower decoding methods tend to yield better quality. Larger models achieve higher scores overall but have longer inference times and are no more robust to decoding. Surprisingly, while medical LLMs outperform general ones in two of the five tasks, statistical analysis shows no overall performance advantage and reveals greater sensitivity to decoding choice. We further compare multiple evaluation metrics and find that correlations vary by task, with MAUVE showing weak agreement with BERTScore and ROUGE, as well as greater sensitivity to the decoding strategy. These results highlight the need for careful selection of decoding methods in medical applications, as their influence can sometimes exceed that of model choice.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—æ–‡æœ¬ç”Ÿæˆä¸­çš„è§£ç ç­–ç•¥(decoding strategies)è¿›è¡Œäº†æ¯”è¾ƒç ”ç©¶ï¼Œæ—¨åœ¨æ¢è®¨å…¶å¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)è¾“å‡ºè´¨é‡çš„å½±å“ã€‚ç ”ç©¶è€…åœ¨ç¿»è¯‘ã€æ‘˜è¦ã€é—®ç­”ã€å¯¹è¯å’Œå›¾åƒå­—å¹•äº”é¡¹åŒ»ç–—ä»»åŠ¡ä¸­ï¼Œè¯„ä¼°äº†11ç§è§£ç ç­–ç•¥åœ¨ä¸åŒè§„æ¨¡çš„åŒ»ç–—ä¸“ç”¨LLMså’Œé€šç”¨LLMsä¸Šçš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç¡®å®šæ€§ç­–ç•¥(deterministic strategies)é€šå¸¸ä¼˜äºéšæœºæ€§ç­–ç•¥ï¼Œå…¶ä¸­æŸæœç´¢(beam search)è¡¨ç°æœ€ä½³ï¼Œè€ŒÎ·é‡‡æ ·å’Œtop-ké‡‡æ ·æ•ˆæœæœ€å·®ã€‚ç ”ç©¶å‘ç°è§£ç é€Ÿåº¦è¾ƒæ…¢çš„æ–¹æ³•å¾€å¾€èƒ½äº§ç”Ÿæ›´é«˜è´¨é‡çš„æ–‡æœ¬ï¼Œä¸”æ¨¡å‹è§„æ¨¡çš„å¢å¤§è™½èƒ½æå‡æ€§èƒ½ï¼Œä½†å¹¶æœªå¢å¼ºæ¨¡å‹å¯¹è§£ç é€‰æ‹©çš„ç¨³å¥æ€§ã€‚ç»Ÿè®¡åˆ†æè¡¨æ˜ï¼ŒåŒ»ç–—ä¸“ç”¨LLMsåœ¨æ•´ä½“æ€§èƒ½ä¸Šç›¸å¯¹äºé€šç”¨æ¨¡å‹å¹¶æ— æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸”å¯¹è§£ç ç­–ç•¥çš„é€‰æ‹©è¡¨ç°å‡ºæ›´é«˜çš„æ•æ„Ÿæ€§ã€‚æ­¤å¤–ï¼Œä¸åŒè¯„ä¼°æŒ‡æ ‡é—´çš„å…³è”æ€§å› ä»»åŠ¡è€Œå¼‚ï¼Œå…¶ä¸­MAUVEæŒ‡æ ‡å¯¹è§£ç ç­–ç•¥è¡¨ç°å‡ºæ›´å¼ºçš„æ•æ„Ÿæ€§ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨åŒ»ç–—é¢†åŸŸè°¨æ…é€‰æ‹©è§£ç æ–¹æ³•çš„é‡è¦æ€§ï¼Œå…¶å½±å“åŠ›åœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³å¯èƒ½è¶…è¿‡æ¨¡å‹æœ¬èº«çš„é€‰æ‹©ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13580v1",
      "published_date": "2025-08-19 07:25:25 UTC",
      "updated_date": "2025-08-19 07:25:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:51:21.746883+00:00"
    },
    {
      "arxiv_id": "2508.13579v1",
      "title": "Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance",
      "title_zh": "æå‡å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ EHR æ¨ç†ï¼šåŸºäºä¸“å®¶æ³¨æ„åŠ›å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Yue Fang",
        "Yuxin Guo",
        "Jiaran Gao",
        "Hongxin Ding",
        "Xinke Jiang",
        "Weibin Liao",
        "Yongxin Xu",
        "Yinghao Zhu",
        "Zhibang Yang",
        "Liantao Ma",
        "Junfeng Zhao",
        "Yasha Wang"
      ],
      "abstract": "Improving large language models (LLMs) for electronic health record (EHR) reasoning is essential for enabling accurate and generalizable clinical predictions. While LLMs excel at medical text understanding, they underperform on EHR-based prediction tasks due to challenges in modeling temporally structured, high-dimensional data. Existing approaches often rely on hybrid paradigms, where LLMs serve merely as frozen prior retrievers while downstream deep learning (DL) models handle prediction, failing to improve the LLM's intrinsic reasoning capacity and inheriting the generalization limitations of DL models. To this end, we propose EAG-RL, a novel two-stage training framework designed to intrinsically enhance LLMs' EHR reasoning ability through expert attention guidance, where expert EHR models refer to task-specific DL models trained on EHR data. Concretely, EAG-RL first constructs high-quality, stepwise reasoning trajectories using expert-guided Monte Carlo Tree Search to effectively initialize the LLM's policy. Then, EAG-RL further optimizes the policy via reinforcement learning by aligning the LLM's attention with clinically salient features identified by expert EHR models. Extensive experiments on two real-world EHR datasets show that EAG-RL improves the intrinsic EHR reasoning ability of LLMs by an average of 14.62%, while also enhancing robustness to feature perturbations and generalization to unseen clinical domains. These results demonstrate the practical potential of EAG-RL for real-world deployment in clinical prediction tasks. Our code have been available at https://github.com/devilran6/EAG-RL.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º EAG-RL çš„åŒé˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨ä»æœ¬è´¨ä¸Šæå‡å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ç”µå­å¥åº·æ¡£æ¡ˆ (EHR) æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚é’ˆå¯¹ LLMs å¤„ç†å…·æœ‰æ—¶é—´ç»“æ„å’Œé«˜ç»´åº¦çš„ EHR æ•°æ®æ—¶å­˜åœ¨çš„çŸ­æ¿ï¼ŒEAG-RL é¦–å…ˆåˆ©ç”¨ä¸“å®¶å¼•å¯¼çš„ Monte Carlo Tree Search (MCTS) æ„å»ºé«˜è´¨é‡çš„é€æ­¥æ¨ç†è½¨è¿¹ï¼Œä»¥å®ç°ç­–ç•¥çš„æœ‰æ•ˆåˆå§‹åŒ–ã€‚éšåï¼Œè¯¥æ¡†æ¶é€šè¿‡ Reinforcement Learning (RL) è¿›ä¸€æ­¥ä¼˜åŒ–ç­–ç•¥ï¼Œå°† LLMs çš„æ³¨æ„åŠ›ä¸ä¸“å®¶ EHR æ¨¡å‹è¯†åˆ«å‡ºçš„ä¸´åºŠæ˜¾è‘—ç‰¹å¾è¿›è¡Œå¯¹é½ã€‚åœ¨ä¸¤ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEAG-RL å°† LLMs çš„å†…åœ¨ EHR æ¨ç†èƒ½åŠ›å¹³å‡æé«˜äº† 14.62%ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹ç‰¹å¾æ‰°åŠ¨çš„é²æ£’æ€§ä»¥åŠåœ¨æœªçŸ¥ä¸´åºŠé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºä¸´åºŠé¢„æµ‹ä»»åŠ¡çš„å®é™…éƒ¨ç½²æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13579v1",
      "published_date": "2025-08-19 07:24:48 UTC",
      "updated_date": "2025-08-19 07:24:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:51:27.654064+00:00"
    },
    {
      "arxiv_id": "2508.14797v1",
      "title": "MF-LPR$^2$: Multi-Frame License Plate Image Restoration and Recognition using Optical Flow",
      "title_zh": "MF-LPR$^2$ï¼šåŸºäºå…‰æµçš„å¤šå¸§è½¦ç‰Œå›¾åƒä¿®å¤ä¸è¯†åˆ«",
      "authors": [
        "Kihyun Na",
        "Junseok Oh",
        "Youngkwan Cho",
        "Bumjin Kim",
        "Sungmin Cho",
        "Jinyoung Choi",
        "Injung Kim"
      ],
      "abstract": "License plate recognition (LPR) is important for traffic law enforcement, crime investigation, and surveillance. However, license plate areas in dash cam images often suffer from low resolution, motion blur, and glare, which make accurate recognition challenging. Existing generative models that rely on pretrained priors cannot reliably restore such poor-quality images, frequently introducing severe artifacts and distortions. To address this issue, we propose a novel multi-frame license plate restoration and recognition framework, MF-LPR$^2$, which addresses ambiguities in poor-quality images by aligning and aggregating neighboring frames instead of relying on pretrained knowledge. To achieve accurate frame alignment, we employ a state-of-the-art optical flow estimator in conjunction with carefully designed algorithms that detect and correct erroneous optical flow estimations by leveraging the spatio-temporal consistency inherent in license plate image sequences. Our approach enhances both image quality and recognition accuracy while preserving the evidential content of the input images. In addition, we constructed a novel Realistic LPR (RLPR) dataset to evaluate MF-LPR$^2$. The RLPR dataset contains 200 pairs of low-quality license plate image sequences and high-quality pseudo ground-truth images, reflecting the complexities of real-world scenarios. In experiments, MF-LPR$^2$ outperformed eight recent restoration models in terms of PSNR, SSIM, and LPIPS by significant margins. In recognition, MF-LPR$^2$ achieved an accuracy of 86.44%, outperforming both the best single-frame LPR (14.04%) and the multi-frame LPR (82.55%) among the eleven baseline models. The results of ablation studies confirm that our filtering and refinement algorithms significantly contribute to these improvements.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MF-LPR$^2$ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤šå¸§å›¾åƒåºåˆ—è¿›è¡Œè½¦ç‰Œä¿®å¤ä¸è¯†åˆ«çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¡Œè½¦è®°å½•ä»ªå›¾åƒä¸­å¸¸è§çš„ä½åˆ†è¾¨ç‡ã€è¿åŠ¨æ¨¡ç³Šå’Œçœ©å…‰ç­‰è¯†åˆ«éš¾é¢˜ã€‚ä¸ä¾èµ–é¢„è®­ç»ƒå…ˆéªŒã€å®¹æ˜“äº§ç”Ÿä¼ªå½±çš„ç”Ÿæˆæ¨¡å‹ä¸åŒï¼Œè¯¥æ¡†æ¶é€šè¿‡å¯¹é½å’Œèšåˆé‚»è¿‘å¸§æ¥æ¶ˆé™¤æ­§ä¹‰ï¼Œå¹¶åˆ©ç”¨å…‰æµä¼°è®¡å™¨ (Optical Flow Estimator) ç»“åˆæ—¶ç©ºä¸€è‡´æ€§ (Spatio-Temporal Consistency) ç®—æ³•ä¿®æ­£å¯¹é½è¯¯å·®ã€‚ä¸ºäº†è¯„ä¼°æ€§èƒ½ï¼Œç ”ç©¶è€…æ„å»ºäº†åŒ…å« 200 ç»„çœŸå®åœºæ™¯åºåˆ—çš„ Realistic LPR (RLPR) æ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼ŒMF-LPR$^2$ åœ¨ PSNRã€SSIM å’Œ LPIPS ç­‰å›¾åƒè´¨é‡æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å…«ç§ä¿®å¤æ¨¡å‹ã€‚åœ¨è¯†åˆ«ä»»åŠ¡ä¸­ï¼Œè¯¥æ¡†æ¶å®ç°äº† 86.44% çš„å‡†ç¡®ç‡ï¼Œå¤§å¹…è¶…è¶Šäº†å•å¸§åŠå¤šå¸§è¯†åˆ«çš„åŸºçº¿æ¨¡å‹ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®ï¼Œæ‰€æå‡ºçš„è¿‡æ»¤ä¸ç»†åŒ–ç®—æ³•åœ¨æå‡è¯†åˆ«ç²¾åº¦å’Œä¿æŒè¯æ®å†…å®¹çš„å®Œæ•´æ€§æ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication in Computer Vision and Image Understanding (CVIU), 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.14797v1",
      "published_date": "2025-08-19 07:18:14 UTC",
      "updated_date": "2025-08-19 07:18:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:51:34.055650+00:00"
    },
    {
      "arxiv_id": "2508.13576v1",
      "title": "End-to-End Audio-Visual Learning for Cochlear Implant Sound Coding in Noisy Environments",
      "title_zh": "å™ªå£°ç¯å¢ƒä¸‹äººå·¥è€³èœ—å£°éŸ³ç¼–ç çš„ç«¯åˆ°ç«¯è§†å¬å­¦ä¹ ",
      "authors": [
        "Meng-Ping Lin",
        "Enoch Hsin-Ho Huang",
        "Shao-Yi Chien",
        "Yu Tsao"
      ],
      "abstract": "The cochlear implant (CI) is a remarkable biomedical device that successfully enables individuals with severe-to-profound hearing loss to perceive sound by converting speech into electrical stimulation signals. Despite advancements in the performance of recent CI systems, speech comprehension in noisy or reverberant conditions remains a challenge. Recent and ongoing developments in deep learning reveal promising opportunities for enhancing CI sound coding capabilities, not only through replicating traditional signal processing methods with neural networks, but also through integrating visual cues as auxiliary data for multimodal speech processing. Therefore, this paper introduces a novel noise-suppressing CI system, AVSE-ECS, which utilizes an audio-visual speech enhancement (AVSE) model as a pre-processing module for the deep-learning-based ElectrodeNet-CS (ECS) sound coding strategy. Specifically, a joint training approach is applied to model AVSE-ECS, an end-to-end CI system. Experimental results indicate that the proposed method outperforms the previous ECS strategy in noisy conditions, with improved objective speech intelligibility scores. The methods and findings in this study demonstrate the feasibility and potential of using deep learning to integrate the AVSE module into an end-to-end CI system",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥è€³èœ—(Cochlear Implant, CI)ç”¨æˆ·åœ¨å˜ˆæ‚æˆ–æ··å“ç¯å¢ƒä¸‹è¯­éŸ³ç†è§£å›°éš¾çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºAVSE-ECSçš„æ–°å‹å™ªå£°æŠ‘åˆ¶ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåˆ›æ–°æ€§åœ°å°†è§†å¬è¯­éŸ³å¢å¼º(Audio-Visual Speech Enhancement, AVSE)æ¨¡å‹ä½œä¸ºé¢„å¤„ç†æ¨¡å—ï¼Œå¹¶å°†å…¶ä¸åŸºäºæ·±åº¦å­¦ä¹ çš„ElectrodeNet-CS (ECS)å£°éŸ³ç¼–ç ç­–ç•¥ç›¸ç»“åˆã€‚é€šè¿‡é‡‡ç”¨è”åˆè®­ç»ƒ(Joint Training)æ–¹æ³•æ„å»ºç«¯åˆ°ç«¯çš„ç³»ç»Ÿï¼Œç ”ç©¶æˆåŠŸåˆ©ç”¨è§†è§‰çº¿ç´¢ä½œä¸ºè¾…åŠ©æ•°æ®æ¥å¢å¼ºå¤šæ¨¡æ€è¯­éŸ³å¤„ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAVSE-ECSåœ¨å™ªå£°æ¡ä»¶ä¸‹çš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„ECSç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†å®¢è§‚è¯­éŸ³å¯ç†è§£æ€§è¯„åˆ†ã€‚è¿™ä¸€æˆæœè¯æ˜äº†åˆ©ç”¨æ·±åº¦å­¦ä¹ å°†AVSEæ¨¡å—é›†æˆåˆ°ç«¯åˆ°ç«¯CIç³»ç»Ÿä¸­çš„å¯è¡Œæ€§ï¼Œä¸ºæå‡å¤æ‚ç¯å¢ƒä¸‹çš„åŠ©å¬è®¾å¤‡æ€§èƒ½æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.IV"
      ],
      "primary_category": "eess.AS",
      "comment": "6 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.13576v1",
      "published_date": "2025-08-19 07:15:17 UTC",
      "updated_date": "2025-08-19 07:15:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:51:33.058955+00:00"
    },
    {
      "arxiv_id": "2508.13564v1",
      "title": "The 9th AI City Challenge",
      "title_zh": "ç¬¬ä¹å±Š AI City Challenge",
      "authors": [
        "Zheng Tang",
        "Shuo Wang",
        "David C. Anastasiu",
        "Ming-Ching Chang",
        "Anuj Sharma",
        "Quan Kong",
        "Norimasa Kobori",
        "Munkhjargal Gochoo",
        "Ganzorig Batnasan",
        "Munkh-Erdene Otgonbold",
        "Fady Alnajjar",
        "Jun-Wei Hsieh",
        "Tomasz Kornuta",
        "Xiaolong Li",
        "Yilin Zhao",
        "Han Zhang",
        "Subhashree Radhakrishnan",
        "Arihant Jain",
        "Ratnesh Kumar",
        "Vidya N. Murali",
        "Yuxing Wang",
        "Sameer Satish Pusegaonkar",
        "Yizhou Wang",
        "Sujit Biswas",
        "Xunlei Wu",
        "Zhedong Zheng",
        "Pranamesh Chakraborty",
        "Rama Chellappa"
      ],
      "abstract": "The ninth AI City Challenge continues to advance real-world applications of computer vision and AI in transportation, industrial automation, and public safety. The 2025 edition featured four tracks and saw a 17% increase in participation, with 245 teams from 15 countries registered on the evaluation server. Public release of challenge datasets led to over 30,000 downloads to date. Track 1 focused on multi-class 3D multi-camera tracking, involving people, humanoids, autonomous mobile robots, and forklifts, using detailed calibration and 3D bounding box annotations. Track 2 tackled video question answering in traffic safety, with multi-camera incident understanding enriched by 3D gaze labels. Track 3 addressed fine-grained spatial reasoning in dynamic warehouse environments, requiring AI systems to interpret RGB-D inputs and answer spatial questions that combine perception, geometry, and language. Both Track 1 and Track 3 datasets were generated in NVIDIA Omniverse. Track 4 emphasized efficient road object detection from fisheye cameras, supporting lightweight, real-time deployment on edge devices. The evaluation framework enforced submission limits and used a partially held-out test set to ensure fair benchmarking. Final rankings were revealed after the competition concluded, fostering reproducibility and mitigating overfitting. Several teams achieved top-tier results, setting new benchmarks in multiple tasks.",
      "tldr_zh": "ç¬¬ä¹å±ŠAI City Challengeæ—¨åœ¨æ¨åŠ¨è®¡ç®—æœºè§†è§‰å’Œäººå·¥æ™ºèƒ½åœ¨äº¤é€šã€å·¥ä¸šè‡ªåŠ¨åŒ–å’Œå…¬å…±å®‰å…¨é¢†åŸŸçš„å®é™…åº”ç”¨ã€‚æœ¬å±ŠæŒ‘æˆ˜èµ›è®¾ç½®äº†å››ä¸ªèµ›é“ï¼Œå¸å¼•äº†æ¥è‡ª15ä¸ªå›½å®¶çš„245æ”¯å›¢é˜Ÿå‚ä¸ï¼Œéƒ¨åˆ†æ ¸å¿ƒæ•°æ®é›†é€šè¿‡NVIDIA Omniverseå¹³å°ç”Ÿæˆã€‚èµ›é“1èšç„¦äºå¤šç±»åˆ«çš„3D multi-camera trackingï¼Œæ¶µç›–è¡Œäººã€è‡ªä¸»ç§»åŠ¨æœºå™¨äººåŠå‰è½¦ç­‰ç›®æ ‡çš„ç²¾ç¡®è·Ÿè¸ªï¼›èµ›é“2æ¢è®¨äº¤é€šå®‰å…¨ä¸­çš„Video Question Answeringï¼Œå¹¶å¼•å…¥äº†3D gaze labelsä»¥æ·±åŒ–äº‹æ•…ç†è§£ã€‚èµ›é“3è¦æ±‚åœ¨åŠ¨æ€ä»“åº“ç¯å¢ƒä¸­å®ç°ç»†ç²’åº¦çš„spatial reasoningï¼Œç»“åˆæ„ŸçŸ¥ã€å‡ ä½•ä¸è¯­è¨€æ¥å¤„ç†RGB-Dè¾“å…¥ï¼›èµ›é“4åˆ™å¼ºè°ƒé’ˆå¯¹é±¼çœ¼ç›¸æœºçš„è½»é‡åŒ–Road object detectionï¼Œä»¥é€‚é…è¾¹ç¼˜è®¾å¤‡çš„å®æ—¶éƒ¨ç½²éœ€æ±‚ã€‚è¯¥ç«èµ›é€šè¿‡ä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶å’Œéƒ¨åˆ†ä¸å…¬å¼€çš„æµ‹è¯•é›†ç¡®ä¿äº†å…¬å¹³çš„åŸºå‡†æµ‹è¯•ï¼Œå¤šæ”¯å›¢é˜Ÿå–å¾—äº†çªç ´æ€§æˆæœå¹¶åˆ·æ–°äº†å¤šé¡¹ä»»åŠ¡çš„æŠ€æœ¯æ ‡å‡†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Summary of the 9th AI City Challenge Workshop in conjunction with ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.13564v1",
      "published_date": "2025-08-19 06:55:06 UTC",
      "updated_date": "2025-08-19 06:55:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:51:36.388844+00:00"
    },
    {
      "arxiv_id": "2508.13559v1",
      "title": "Physics-Informed Neural Networks for Programmable Origami Metamaterials with Controlled Deployment",
      "title_zh": "ç”¨äºå—æ§éƒ¨ç½²å¯ç¼–ç¨‹æŠ˜çº¸è¶…ææ–™çš„ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ",
      "authors": [
        "Sukheon Kang",
        "Youngkwon Kim",
        "Jinkyu Yang",
        "Seunghwa Ryu"
      ],
      "abstract": "Origami-inspired structures provide unprecedented opportunities for creating lightweight, deployable systems with programmable mechanical responses. However, their design remains challenging due to complex nonlinear mechanics, multistability, and the need for precise control of deployment forces. Here, we present a physics-informed neural network (PINN) framework for both forward prediction and inverse design of conical Kresling origami (CKO) without requiring pre-collected training data. By embedding mechanical equilibrium equations directly into the learning process, the model predicts complete energy landscapes with high accuracy while minimizing non-physical artifacts. The inverse design routine specifies both target stable-state heights and separating energy barriers, enabling freeform programming of the entire energy curve. This capability is extended to hierarchical CKO assemblies, where sequential layer-by-layer deployment is achieved through programmed barrier magnitudes. Finite element simulations and experiments on physical prototypes validate the designed deployment sequences and barrier ratios, confirming the robustness of the approach. This work establishes a versatile, data-free route for programming complex mechanical energy landscapes in origami-inspired metamaterials, offering broad potential for deployable aerospace systems, morphing structures, and soft robotic actuators.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ(Physics-Informed Neural Networks, PINN)æ¡†æ¶ï¼Œç”¨äºé”¥å½¢KreslingæŠ˜çº¸(Conical Kresling Origami, CKO)çš„å‰å‘é¢„æµ‹ä¸é€†å‘è®¾è®¡ï¼Œæœ‰æ•ˆè§£å†³äº†å¤æ‚éçº¿æ€§åŠ›å­¦å’Œå¤šç¨³æ€å¸¦æ¥çš„è®¾è®¡æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶æ— éœ€é¢„å…ˆæ”¶é›†è®­ç»ƒæ•°æ®ï¼Œé€šè¿‡å°†åŠ›å­¦å¹³è¡¡æ–¹ç¨‹(mechanical equilibrium equations)ç›´æ¥åµŒå…¥å­¦ä¹ è¿‡ç¨‹ï¼Œèƒ½å¤Ÿé«˜ç²¾åº¦é¢„æµ‹å®Œæ•´çš„èƒ½é‡æ™¯è§‚(energy landscapes)å¹¶å‡å°‘éç‰©ç†è¯¯å·®ã€‚åˆ©ç”¨é€†å‘è®¾è®¡ç¨‹åºï¼Œè¯¥æ¨¡å‹å¯æ ¹æ®æŒ‡å®šçš„ç›®æ ‡ç¨³å®šæ€é«˜åº¦å’Œèƒ½é‡éšœç¢(energy barriers)å¯¹æ•´ä¸ªèƒ½é‡æ›²çº¿è¿›è¡Œè‡ªç”±ç¼–ç¨‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å°†æ­¤æ–¹æ³•æ‰©å±•è‡³åˆ†å±‚CKOç»„ä»¶ï¼Œé€šè¿‡ç¼–ç¨‹èƒ½é‡éšœç¢çš„å¤§å°å®ç°äº†ç²¾ç¡®çš„å±‚çº§åŒ–é¡ºåºéƒ¨ç½²ã€‚æœ‰é™å…ƒæ¨¡æ‹Ÿ(finite element simulations)å’Œç‰©ç†åŸå‹å®éªŒå‡éªŒè¯äº†æ‰€è®¾è®¡éƒ¨ç½²åºåˆ—çš„å¯é æ€§ã€‚è¯¥å·¥ä½œä¸ºæŠ˜çº¸å¯å‘å¼è¶…ææ–™çš„å¤æ‚æœºæ¢°èƒ½é‡æ™¯è§‚ç¼–ç¨‹æä¾›äº†ä¸€ç§é€šç”¨è·¯å¾„ï¼Œåœ¨èˆªç©ºèˆªå¤©éƒ¨ç½²ç³»ç»Ÿã€å˜å½¢ç»“æ„åŠè½¯ä½“æœºå™¨äººé¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cond-mat.soft",
        "cs.AI",
        "physics.comp-ph"
      ],
      "primary_category": "cond-mat.soft",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13559v1",
      "published_date": "2025-08-19 06:38:49 UTC",
      "updated_date": "2025-08-19 06:38:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:51:42.652969+00:00"
    },
    {
      "arxiv_id": "2508.14924v1",
      "title": "A U-Statistic-based random forest approach for genetic interaction study",
      "title_zh": "åŸºäº U ç»Ÿè®¡é‡çš„éšæœºæ£®æ—é—ä¼ äº’ä½œç ”ç©¶æ–¹æ³•",
      "authors": [
        "Ming Li",
        "Ruo-Sin Peng",
        "Changshuai Wei",
        "Qing Lu"
      ],
      "abstract": "Variations in complex traits are influenced by multiple genetic variants, environmental risk factors, and their interactions. Though substantial progress has been made in identifying single genetic variants associated with complex traits, detecting the gene-gene and gene-environment interactions remains a great challenge. When a large number of genetic variants and environmental risk factors are involved, searching for interactions is limited to pair-wise interactions due to the exponentially increased feature space and computational intensity. Alternatively, recursive partitioning approaches, such as random forests, have gained popularity in high-dimensional genetic association studies. In this article, we propose a U-Statistic-based random forest approach, referred to as Forest U-Test, for genetic association studies with quantitative traits. Through simulation studies, we showed that the Forest U-Test outperformed existing methods. The proposed method was also applied to study Cannabis Dependence CD, using three independent datasets from the Study of Addiction: Genetics and Environment. A significant joint association was detected with an empirical p-value less than 0.001. The finding was also replicated in two independent datasets with p-values of 5.93e-19 and 4.70e-17, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤æ‚æ€§çŠ¶ä¸­åŸºå› -åŸºå› åŠåŸºå› -ç¯å¢ƒç›¸äº’ä½œç”¨ï¼ˆgene-gene and gene-environment interactionsï¼‰æ£€æµ‹é¢ä¸´çš„é«˜ç»´ç‰¹å¾ç©ºé—´å’Œè®¡ç®—å¼ºåº¦æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºForest U-Testçš„åŸºäºU-Statisticçš„éšæœºæ£®æ—ï¼ˆrandom forestï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸“é—¨ç”¨äºå®šé‡æ€§çŠ¶ï¼ˆquantitative traitsï¼‰çš„é—ä¼ å…³è”ç ”ç©¶ï¼Œé€šè¿‡é€’å½’åˆ†å‰²æŠ€æœ¯æœ‰æ•ˆå¤„ç†å¤§è§„æ¨¡é—ä¼ å˜å¼‚å’Œç¯å¢ƒé£é™©å› ç´ ã€‚æ¨¡æ‹Ÿç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒForest U-Teståœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„åˆ†ææ–¹æ³•ã€‚åœ¨é’ˆå¯¹å¤§éº»ä¾èµ–ï¼ˆCannabis Dependence, CDï¼‰çš„å®é™…åº”ç”¨ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªç‹¬ç«‹æ•°æ®é›†ä¸­æ£€æµ‹åˆ°äº†æ˜¾è‘—çš„è”åˆå…³è”ï¼Œç»éªŒp-valueå°äº0.001ã€‚æ­¤å¤–ï¼Œè¯¥å‘ç°åœ¨å¦å¤–ä¸¤ä¸ªç‹¬ç«‹æ•°æ®é›†ä¸­ä¹Ÿå¾—åˆ°äº†æˆåŠŸéªŒè¯ï¼Œæ˜¾è‘—æ€§æ°´å¹³åˆ†åˆ«è¾¾åˆ°5.93e-19å’Œ4.70e-17ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨è¯†åˆ«å¤æ‚é—ä¼ å…³è”æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "q-bio.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14924v1",
      "published_date": "2025-08-19 06:22:20 UTC",
      "updated_date": "2025-08-19 06:22:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:51:45.378325+00:00"
    },
    {
      "arxiv_id": "2508.13552v1",
      "title": "Collapsing ROC approach for risk prediction research on both common and rare variants",
      "title_zh": "é’ˆå¯¹å¸¸è§åŠç½•è§å˜å¼‚é£é™©é¢„æµ‹ç ”ç©¶çš„å¡Œé™·å¼ ROC æ–¹æ³•",
      "authors": [
        "Changshuai Wei",
        "Qing Lu"
      ],
      "abstract": "Risk prediction that capitalizes on emerging genetic findings holds great promise for improving public health and clinical care. However, recent risk prediction research has shown that predictive tests formed on existing common genetic loci, including those from genome-wide association studies, have lacked sufficient accuracy for clinical use. Because most rare variants on the genome have not yet been studied for their role in risk prediction, future disease prediction discoveries should shift toward a more comprehensive risk prediction strategy that takes into account both common and rare variants. We are proposing a collapsing receiver operating characteristic CROC approach for risk prediction research on both common and rare variants. The new approach is an extension of a previously developed forward ROC FROC approach, with additional procedures for handling rare variants. The approach was evaluated through the use of 533 single-nucleotide polymorphisms SNPs in 37 candidate genes from the Genetic Analysis Workshop 17 mini-exome data set. We found that a prediction model built on all SNPs gained more accuracy AUC = 0.605 than one built on common variants alone AUC = 0.585. We further evaluated the performance of two approaches by gradually reducing the number of common variants in the analysis. We found that the CROC method attained more accuracy than the FROC method when the number of common variants in the data decreased. In an extreme scenario, when there are only rare variants in the data, the CROC reached an AUC value of 0.603, whereas the FROC had an AUC value of 0.524.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç›®å‰ä»…ä¾èµ–å¸¸è§é—ä¼ ä½ç‚¹(common variants)è¿›è¡Œé£é™©é¢„æµ‹æ—¶å‡†ç¡®æ€§ä¸è¶³çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ•´åˆå¸¸è§ä½ç‚¹ä¸ç½•è§ä½ç‚¹(rare variants)çš„ç»¼åˆé¢„æµ‹ç­–ç•¥ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…å¼€å‘äº†æŠ˜å å—è¯•è€…å·¥ä½œç‰¹å¾(Collapsing Receiver Operating Characteristic, CROC)æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½œä¸ºå‰å‘ROC(Forward ROC, FROC)æ–¹æ³•çš„æ‰©å±•ï¼Œä¸“é—¨å¢åŠ äº†å¤„ç†ç½•è§å˜å¼‚çš„åˆ†æç¨‹åºã€‚é€šè¿‡å¯¹Genetic Analysis Workshop 17æ•°æ®é›†ä¸­çš„533ä¸ªå•æ ¸è‹·é…¸å¤šæ€æ€§(SNPs)è¿›è¡ŒéªŒè¯ï¼Œç»“æœè¡¨æ˜åŒ…å«æ‰€æœ‰SNPsçš„æ¨¡å‹åœ¨é¢„æµ‹å‡†ç¡®ç‡ä¸Šæ˜¾è‘—ä¼˜äºä»…å«å¸¸è§ä½ç‚¹çš„æ¨¡å‹ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œéšç€æ•°æ®ä¸­å¸¸è§ä½ç‚¹æ•°é‡çš„å‡å°‘ï¼ŒCROCæ–¹æ³•å±•ç°å‡ºæ¯”FROCæ›´å¼ºçš„ç¨³å¥æ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨ä»…å­˜åœ¨ç½•è§ä½ç‚¹çš„æç«¯åœºæ™¯ä¸‹ï¼ŒCROCä»èƒ½è¾¾åˆ°0.603çš„AUCå€¼ï¼Œè€ŒFROCä»…ä¸º0.524ã€‚è¿™ä¸€ç ”ç©¶æˆæœè¯æ˜äº†CROCæ–¹æ³•åœ¨æŒ–æ˜ç½•è§å˜å¼‚é¢„æµ‹æ½œåŠ›ä»¥åŠæå‡é—ä¼ é£é™©é¢„æµ‹ç²¾åº¦æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13552v1",
      "published_date": "2025-08-19 06:21:50 UTC",
      "updated_date": "2025-08-19 06:21:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:51:54.852897+00:00"
    },
    {
      "arxiv_id": "2508.16646v1",
      "title": "Equinox: Holistic Fair Scheduling in Serving Large Language Models",
      "title_zh": "Equinoxï¼šå¤§è¯­è¨€æ¨¡å‹æœåŠ¡ä¸­çš„å…¨å±€å…¬å¹³è°ƒåº¦",
      "authors": [
        "Zhixiang Wei",
        "James Yen",
        "Jingyi Chen",
        "Ziyang Zhang",
        "Zhibai Huang",
        "Chen Chen",
        "Xingzi Yu",
        "Yicheng Gu",
        "Chenggang Wu",
        "Yun Wang",
        "Mingyuan Xia",
        "Jie Wu",
        "Hao Wang",
        "Zhengwei Qi"
      ],
      "abstract": "We address the limitations of current LLM serving with a dual-counter framework separating user and operator perspectives. The User Fairness Counter measures quality of service via weighted tokens and latency; the Resource Fairness Counter measures operational efficiency through throughput and GPU utilization. Since these metrics are only available post-execution, creating a scheduling paradox, we introduce a deterministic Mixture of Prediction Experts (MoPE) framework to predict user-perceived latency, output tokens, throughput, and GPU utilization. These predictions enable calculation of a unified Holistic Fairness score that balances both counters through tunable parameters for proactive fairness-aware scheduling. We implement this in Equinox, an open-source system with other optimizations like adaptive batching, and stall-free scheduling. Evaluations on production traces (ShareGPT, LMSYS) and synthetic workloads demonstrate Equinox achieves up to $1.3\\times$ higher throughput, 60\\% lower time-to-first-token latency, and 13\\% higher fairness versus VTC while maintaining 94\\% GPU utilization, proving fairness under bounded discrepancy across heterogeneous platforms.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)æ¨ç†æœåŠ¡ä¸­çš„å…¬å¹³æ€§è°ƒåº¦éš¾é¢˜ï¼Œæå‡ºäº†åä¸ºEquinoxçš„å…¨é¢å…¬å¹³è°ƒåº¦æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†åŒè®¡æ•°å™¨(dual-counter)ä½“ç³»ï¼Œåˆ†åˆ«ä»ç”¨æˆ·è§†è§’è¡¡é‡æœåŠ¡è´¨é‡(User Fairness Counter)ä»¥åŠä»ç®—åŠ›è¿è¥å•†è§†è§’è¡¡é‡èµ„æºæ•ˆç‡(Resource Fairness Counter)ã€‚ä¸ºäº†è§£å†³è°ƒåº¦æŒ‡æ ‡åœ¨æ‰§è¡Œåæ‰å¯çŸ¥çš„æ‚–è®ºï¼Œç ”ç©¶é‡‡ç”¨äº†ä¸€ç§ç¡®å®šæ€§çš„Mixture of Prediction Experts (MoPE)æ¡†æ¶ï¼Œç”¨äºé¢„æµ‹å»¶è¿Ÿã€è¾“å‡ºTokenæ•°ã€ååé‡å’ŒGPUåˆ©ç”¨ç‡ã€‚é€šè¿‡è¿™äº›é¢„æµ‹ç»“æœï¼ŒEquinoxèƒ½å¤Ÿè®¡ç®—å‡ºä¸€ä¸ªHolistic Fairnessè¯„åˆ†ï¼Œåˆ©ç”¨å¯è°ƒå‚æ•°åœ¨å‰ç»æ€§è°ƒåº¦ä¸­å¹³è¡¡ç”¨æˆ·ä½“éªŒä¸è¿è¥æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿè¿˜é›†æˆäº†è‡ªé€‚åº”æ‰¹å¤„ç†(adaptive batching)å’Œæ— åœé¡¿è°ƒåº¦(stall-free scheduling)ç­‰ä¼˜åŒ–æŠ€æœ¯ã€‚åœ¨ShareGPTå’ŒLMSYSç­‰çœŸå®è´Ÿè½½ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒEquinoxç›¸æ¯”äºVTCåŸºå‡†èƒ½æå‡1.3å€ååé‡ï¼Œå¹¶å°†é¦–å­—å»¶è¿Ÿ(time-to-first-token)é™ä½60%ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒEquinoxåœ¨ä¿æŒ94% GPUåˆ©ç”¨ç‡çš„åŒæ—¶ï¼Œå®ç°äº†è·¨å¼‚æ„å¹³å°çš„13%å…¬å¹³æ€§æå‡ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.16646v1",
      "published_date": "2025-08-19 06:17:17 UTC",
      "updated_date": "2025-08-19 06:17:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:51:57.851818+00:00"
    },
    {
      "arxiv_id": "2508.13544v5",
      "title": "FLAIR: Frequency- and Locality-Aware Implicit Neural Representations",
      "title_zh": "FLAIRï¼šé¢‘ç‡ä¸å±€éƒ¨æ„ŸçŸ¥éšå¼ç¥ç»è¡¨ç¤º",
      "authors": [
        "Sukhun Ko",
        "Seokhyun Yoon",
        "Dahyeon Kye",
        "Kyle Min",
        "Chanho Eom",
        "Jihyong Oh"
      ],
      "abstract": "Implicit Neural Representations (INRs) leverage neural networks to map coordinates to corresponding signals, enabling continuous and compact representations. This paradigm has driven significant advances in various vision tasks. However, existing INRs lack frequency selectivity and spatial localization, leading to an over-reliance on redundant signal components. Consequently, they exhibit spectral bias, tending to learn low-frequency components early while struggling to capture fine high-frequency details. To address these issues, we propose FLAIR (Frequency- and Locality-Aware Implicit Neural Representations), which incorporates two key innovations. The first is Band-Localized Activation (BLA), a novel activation designed for joint frequency selection and spatial localization under the constraints of the time-frequency uncertainty principle (TFUP). Through structured frequency control and spatially localized responses, BLA effectively mitigates spectral bias and enhances training stability. The second is Wavelet-Energy-Guided Encoding (WEGE), which leverages the discrete wavelet transform to compute energy scores and explicitly guide frequency information to the network, enabling precise frequency selection and adaptive band control. Our method consistently outperforms existing INRs in 2D image representation, as well as 3D shape reconstruction and novel view synthesis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FLAIRï¼Œä¸€ç§é¢‘ç‡å’Œå±€éƒ¨æ„ŸçŸ¥éšå¼ç¥ç»è¡¨ç¤º(Implicit Neural Representations, INRs)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹å› ç¼ºä¹é¢‘ç‡é€‰æ‹©æ€§å’Œç©ºé—´å±€éƒ¨åŒ–è€Œå¯¼è‡´çš„é¢‘è°±åå·®(spectral bias)åŠé«˜é¢‘ç»†èŠ‚æ•æ‰å›°éš¾ç­‰é—®é¢˜ã€‚FLAIRå¼•å…¥äº†å¸¦çŠ¶å±€éƒ¨æ¿€æ´»(Band-Localized Activation, BLA)ï¼Œåœ¨æ—¶é¢‘ä¸ç¡®å®šæ€§åŸç†(TFUP)çš„çº¦æŸä¸‹å®ç°äº†è”åˆé¢‘ç‡é€‰æ‹©ä¸ç©ºé—´å®šä½ï¼Œä»è€Œæœ‰æ•ˆç¼“è§£é¢‘è°±åå·®å¹¶å¢å¼ºè®­ç»ƒç¨³å®šæ€§ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å°æ³¢èƒ½é‡å¼•å¯¼ç¼–ç (Wavelet-Energy-Guided Encoding, WEGE)ï¼Œé€šè¿‡ç¦»æ•£å°æ³¢å˜æ¢è®¡ç®—èƒ½é‡åˆ†æ•°ï¼Œå¼•å¯¼ç½‘ç»œè·å–ç²¾ç¡®çš„é¢‘ç‡ä¿¡æ¯å¹¶è¿›è¡Œè‡ªé€‚åº”é¢‘å¸¦æ§åˆ¶ã€‚å®éªŒè¯æ˜ï¼ŒFLAIRåœ¨2Då›¾åƒè¡¨ç¤ºã€3Då½¢çŠ¶é‡å»ºå’Œæ–°è§†è§’åˆæˆ(novel view synthesis)ç­‰å¤šé¡¹ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰çš„INRsæ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°æ›´è¿ç»­ä¸”ç´§å‡‘çš„é«˜è´¨é‡ä¿¡å·è¡¨ç¤ºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Please visit our project page at https://cmlab-korea.github.io/FLAIR/",
      "pdf_url": "https://arxiv.org/pdf/2508.13544v5",
      "published_date": "2025-08-19 06:06:04 UTC",
      "updated_date": "2025-12-09 03:35:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:51:54.698796+00:00"
    },
    {
      "arxiv_id": "2508.13537v1",
      "title": "EAvatar: Expression-Aware Head Avatar Reconstruction with Generative Geometry Priors",
      "title_zh": "EAvatarï¼šåŸºäºç”Ÿæˆå¼å‡ ä½•å…ˆéªŒçš„è¡¨æƒ…æ„ŸçŸ¥å¤´éƒ¨åŒ–èº«é‡å»º",
      "authors": [
        "Shikun Zhang",
        "Cunjian Chen",
        "Yiqun Wang",
        "Qiuhong Ke",
        "Yong Li"
      ],
      "abstract": "High-fidelity head avatar reconstruction plays a crucial role in AR/VR, gaming, and multimedia content creation. Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated effectiveness in modeling complex geometry with real-time rendering capability and are now widely used in high-fidelity head avatar reconstruction tasks. However, existing 3DGS-based methods still face significant challenges in capturing fine-grained facial expressions and preserving local texture continuity, especially in highly deformable regions. To mitigate these limitations, we propose a novel 3DGS-based framework termed EAvatar for head reconstruction that is both expression-aware and deformation-aware. Our method introduces a sparse expression control mechanism, where a small number of key Gaussians are used to influence the deformation of their neighboring Gaussians, enabling accurate modeling of local deformations and fine-scale texture transitions. Furthermore, we leverage high-quality 3D priors from pretrained generative models to provide a more reliable facial geometry, offering structural guidance that improves convergence stability and shape accuracy during training. Experimental results demonstrate that our method produces more accurate and visually coherent head reconstructions with improved expression controllability and detail fidelity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ AR/VR å’Œå¤šåª’ä½“åˆ›ä½œä¸­çš„é«˜ä¿çœŸå¤´éƒ¨å¤´åƒé‡å»ºé—®é¢˜ï¼ŒæŒ‡å‡ºç›®å‰çš„ 3D Gaussian Splatting (3DGS) æ–¹æ³•åœ¨å¤„ç†ç»†ç²’åº¦é¢éƒ¨è¡¨æƒ…å’Œå˜å½¢åŒºåŸŸçš„çº¹ç†è¿ç»­æ€§æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤æå‡ºçš„ EAvatar æ¡†æ¶é€šè¿‡å¼•å…¥ç¨€ç–è¡¨æƒ…æ§åˆ¶æœºåˆ¶ (sparse expression control mechanism)ï¼Œåˆ©ç”¨å°‘é‡å…³é”®é«˜æ–¯çƒå¼•å¯¼é‚»åŸŸå˜å½¢ï¼Œå®ç°äº†å¯¹å±€éƒ¨ç»†å¾®å˜å½¢å’Œçº¹ç†è¿‡æ¸¡çš„ç²¾ç¡®å»ºæ¨¡ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹ (pretrained generative models) æä¾›çš„é«˜è´¨é‡ 3D å…ˆéªŒ (3D priors) ä»¥æä¾›ç»“æ„å¼•å¯¼ï¼Œæ˜¾è‘—æå‡äº†è®­ç»ƒçš„æ”¶æ•›ç¨³å®šæ€§å’Œå‡ ä½•å½¢çŠ¶å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEAvatar èƒ½å¤Ÿç”Ÿæˆæ›´ç²¾ç¡®ä¸”è§†è§‰ä¸€è‡´çš„å¤´éƒ¨å¤´åƒï¼Œåœ¨è¡¨æƒ…å¯æ§æ€§å’Œç»†èŠ‚ä¿çœŸåº¦æ–¹é¢å‡è¡¨ç°å‡ºè‰²ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.13537v1",
      "published_date": "2025-08-19 05:56:00 UTC",
      "updated_date": "2025-08-19 05:56:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:52:11.351816+00:00"
    },
    {
      "arxiv_id": "2508.13534v1",
      "title": "MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence",
      "title_zh": "MimicFuncï¼šåŸºäºåŠŸèƒ½å¯¹åº”ä»å•æ®µäººç±»è§†é¢‘ä¸­æ¨¡ä»¿å·¥å…·æ“æ§",
      "authors": [
        "Chao Tang",
        "Anxing Xiao",
        "Yuhong Deng",
        "Tianrun Hu",
        "Wenlong Dong",
        "Hanbo Zhang",
        "David Hsu",
        "Hong Zhang"
      ],
      "abstract": "Imitating tool manipulation from human videos offers an intuitive approach to teaching robots, while also providing a promising and scalable alternative to labor-intensive teleoperation data collection for visuomotor policy learning. While humans can mimic tool manipulation behavior by observing others perform a task just once and effortlessly transfer the skill to diverse tools for functionally equivalent tasks, current robots struggle to achieve this level of generalization. A key challenge lies in establishing function-level correspondences, considering the significant geometric variations among functionally similar tools, referred to as intra-function variations. To address this challenge, we propose MimicFunc, a framework that establishes functional correspondences with function frame, a function-centric local coordinate frame constructed with keypoint-based abstraction, for imitating tool manipulation skills. Experiments demonstrate that MimicFunc effectively enables the robot to generalize the skill from a single RGB-D human video to manipulating novel tools for functionally equivalent tasks. Furthermore, leveraging MimicFunc's one-shot generalization capability, the generated rollouts can be used to train visuomotor policies without requiring labor-intensive teleoperation data collection for novel objects. Our code and video are available at https://sites.google.com/view/mimicfunc.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MimicFunc æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å•ä¸ªäººç±»è§†é¢‘å®ç°å·¥å…·æ“ä½œçš„æ¨¡ä»¿å­¦ä¹ ï¼Œé‡ç‚¹è§£å†³äº†æœºå™¨äººéš¾ä»¥åœ¨å…·æœ‰å‡ ä½•å·®å¼‚çš„åŠŸèƒ½ç­‰æ•ˆå·¥å…·ï¼ˆintra-function variationsï¼‰é—´è¿›è¡Œæ³›åŒ–çš„æŒ‘æˆ˜ã€‚MimicFunc çš„æ ¸å¿ƒåœ¨äºå»ºç«‹åŠŸèƒ½å¯¹åº”å…³ç³»ï¼Œå®ƒå¼•å…¥äº†åŸºäº keypoint-based abstraction æ„å»ºçš„ function frameï¼Œå³ä¸€ç§ä»¥åŠŸèƒ½ä¸ºä¸­å¿ƒçš„å±€éƒ¨åæ ‡ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½ä½¿æœºå™¨äººä»…å‡­ä¸€æ®µ RGB-D äººç±»è§†é¢‘ï¼Œå³å¯å°†ç‰¹å®šæŠ€èƒ½è¿ç§»è‡³ä»æœªè§è¿‡çš„å·¥å…·ä¸Šå®ŒæˆåŠŸèƒ½ç­‰æ•ˆä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒMimicFunc å±•ç°å‡ºçš„ one-shot æ³›åŒ–èƒ½åŠ›å¯ç”¨äºç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œä»è€Œåœ¨æ— éœ€å¤§é‡äººå·¥ teleoperation é‡‡é›†çš„æƒ…å†µä¸‹è®­ç»ƒé«˜æ•ˆçš„ visuomotor policiesã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to CoRL 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.13534v1",
      "published_date": "2025-08-19 05:49:47 UTC",
      "updated_date": "2025-08-19 05:49:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:52:16.051767+00:00"
    },
    {
      "arxiv_id": "2508.14923v1",
      "title": "A Fully Spectral Neuro-Symbolic Reasoning Architecture with Graph Signal Processing as the Computational Backbone",
      "title_zh": "ä»¥å›¾ä¿¡å·å¤„ç†ä¸ºè®¡ç®—éª¨å¹²çš„å…¨è°±ç¥ç»ç¬¦å·æ¨ç†æ¶æ„",
      "authors": [
        "Andrew Kiruluta"
      ],
      "abstract": "We propose a fully spectral, neuro\\-symbolic reasoning architecture that leverages Graph Signal Processing (GSP) as the primary computational backbone for integrating symbolic logic and neural inference. Unlike conventional reasoning models that treat spectral graph methods as peripheral components, our approach formulates the entire reasoning pipeline in the graph spectral domain. Logical entities and relationships are encoded as graph signals, processed via learnable spectral filters that control multi-scale information propagation, and mapped into symbolic predicates for rule-based inference. We present a complete mathematical framework for spectral reasoning, including graph Fourier transforms, band-selective attention, and spectral rule grounding. Experiments on benchmark reasoning datasets (ProofWriter, EntailmentBank, bAbI, CLUTRR, and ARC-Challenge) demonstrate improvements in logical consistency, interpretability, and computational efficiency over state\\-of\\-the\\-art neuro\\-symbolic models. Our results suggest that GSP provides a mathematically grounded and computationally efficient substrate for robust and interpretable reasoning systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å…¨é¢‘è°±ç¥ç»ç¬¦å·æ¨ç†æ¶æ„ï¼Œåˆ©ç”¨å›¾ä¿¡å·å¤„ç†(Graph Signal Processing, GSP)ä½œä¸ºæ•´åˆç¬¦å·é€»è¾‘ä¸ç¥ç»æ¨ç†çš„æ ¸å¿ƒè®¡ç®—éª¨å¹²ã€‚ä¸å°†é¢‘è°±å›¾æ–¹æ³•è§†ä¸ºå¤–å›´ç»„ä»¶çš„ä¼ ç»Ÿæ¨¡å‹ä¸åŒï¼Œè¯¥æ–¹æ³•åœ¨å›¾é¢‘è°±åŸŸä¸­æ„å»ºäº†æ•´ä¸ªæ¨ç†æµæ°´çº¿ï¼Œå°†é€»è¾‘å®ä½“å’Œå…³ç³»ç¼–ç ä¸ºå›¾ä¿¡å·ã€‚é€šè¿‡å¯å­¦ä¹ çš„é¢‘è°±æ»¤æ³¢å™¨æ§åˆ¶å¤šå°ºåº¦ä¿¡æ¯ä¼ æ’­ï¼Œå¹¶ç»“åˆå›¾å‚…é‡Œå¶å˜æ¢(Graph Fourier Transforms)ã€æ³¢æ®µé€‰æ‹©æ€§æ³¨æ„åŠ›å’Œé¢‘è°±è§„åˆ™è½åœ°(spectral rule grounding)ç­‰æŠ€æœ¯ï¼Œå°†ä¿¡å·æ˜ å°„ä¸ºç”¨äºè§„åˆ™æ¨ç†çš„ç¬¦å·è°“è¯ã€‚åœ¨ProofWriterã€EntailmentBankã€bAbIã€CLUTRRå’ŒARC-Challengeç­‰åŸºå‡†æ¨ç†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¶æ„åœ¨é€»è¾‘ä¸€è‡´æ€§ã€å¯è§£é‡Šæ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰çš„ç¥ç»ç¬¦å·(neuro-symbolic)æ¨¡å‹ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒGSPä¸ºæ„å»ºç¨³å¥ä¸”å¯è§£é‡Šçš„æ¨ç†ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªå…·å¤‡æ•°å­¦åŸºç¡€ä¸”è®¡ç®—é«˜æ•ˆçš„åº•å±‚æ¶æ„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14923v1",
      "published_date": "2025-08-19 05:49:28 UTC",
      "updated_date": "2025-08-19 05:49:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:52:15.659535+00:00"
    },
    {
      "arxiv_id": "2508.15837v1",
      "title": "Statistical Comparative Analysis of Semantic Similarities and Model Transferability Across Datasets for Short Answer Grading",
      "title_zh": "ç®€ç­”é¢˜è¯„åˆ†ä¸­è·¨æ•°æ®é›†è¯­ä¹‰ç›¸ä¼¼åº¦ä¸æ¨¡å‹å¯è¿ç§»æ€§çš„ç»Ÿè®¡å¯¹æ¯”åˆ†æ",
      "authors": [
        "Sridevi Bonthu",
        "S. Rama Sree",
        "M. H. M. Krishna Prasad"
      ],
      "abstract": "Developing dataset-specific models involves iterative fine-tuning and optimization, incurring significant costs over time. This study investigates the transferability of state-of-the-art (SOTA) models trained on established datasets to an unexplored text dataset. The key question is whether the knowledge embedded within SOTA models from existing datasets can be harnessed to achieve high-performance results on a new domain. In pursuit of this inquiry, two well-established benchmarks, the STSB and Mohler datasets, are selected, while the recently introduced SPRAG dataset serves as the unexplored domain. By employing robust similarity metrics and statistical techniques, a meticulous comparative analysis of these datasets is conducted. The primary goal of this work is to yield comprehensive insights into the potential applicability and adaptability of SOTA models. The outcomes of this research have the potential to reshape the landscape of natural language processing (NLP) by unlocking the ability to leverage existing models for diverse datasets. This may lead to a reduction in the demand for resource-intensive, dataset-specific training, thereby accelerating advancements in NLP and paving the way for more efficient model deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨çŸ­ç­”æ¡ˆè¯„åˆ† (Short Answer Grading) ä»»åŠ¡ä¸­ï¼Œå°†åŸºäºå·²æœ‰æ•°æ®é›†è®­ç»ƒçš„å…ˆè¿›æ¨¡å‹ (SOTA models) è¿ç§»è‡³æœªæ¢ç´¢çš„æ–°æ•°æ®é›†çš„å¯è¡Œæ€§ã€‚ç ”ç©¶é€‰å–äº† STSB å’Œ Mohler ä¸¤ä¸ªæˆç†Ÿçš„åŸºå‡†æ•°æ®é›†ï¼Œå¹¶å°†æ–°å¼•å…¥çš„ SPRAG æ•°æ®é›†ä½œä¸ºç›®æ ‡é¢†åŸŸï¼Œé€šè¿‡è¯­ä¹‰ç›¸ä¼¼åº¦åº¦é‡å’Œç»Ÿè®¡æŠ€æœ¯è¿›è¡Œäº†è¯¦å°½çš„å¯¹æ¯”åˆ†æã€‚å…¶æ ¸å¿ƒç›®æ ‡æ˜¯è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†é—´çš„è¿ç§»æ€§ (Model Transferability)ï¼Œæ¢ç©¶ç°æœ‰æ¨¡å‹ä¸­çš„åµŒå…¥çŸ¥è¯†æ˜¯å¦èƒ½ç›´æ¥åº”ç”¨äºæ–°é¢†åŸŸå¹¶ä¿æŒé«˜æ€§èƒ½ã€‚å®éªŒç»“æœä¸º SOTA æ¨¡å‹çš„é€‚ç”¨æ€§å’Œé€‚é…èƒ½åŠ›æä¾›äº†å…¨é¢è§è§£ï¼Œè¯æ˜äº†åˆ©ç”¨ç°æœ‰æ¨¡å‹å¤„ç†å¤šæ ·åŒ–æ•°æ®é›†çš„å·¨å¤§æ½œåŠ›ã€‚æ­¤é¡¹å·¥ä½œæœ‰åŠ©äºå‡å°‘å¯¹ç‰¹å®šæ•°æ®é›†è¿›è¡Œé«˜èµ„æºæ¶ˆè€—çš„é‡å¤è®­ç»ƒéœ€æ±‚ï¼Œä¸ºæ›´é«˜æ•ˆçš„è‡ªç„¶è¯­è¨€å¤„ç† (NLP) æ¨¡å‹éƒ¨ç½²å’ŒæŠ€æœ¯è¿›æ­¥é“ºå¹³äº†é“è·¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15837v1",
      "published_date": "2025-08-19 05:45:02 UTC",
      "updated_date": "2025-08-19 05:45:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:52:18.065343+00:00"
    },
    {
      "arxiv_id": "2508.13530v1",
      "title": "CrafterDojo: A Suite of Foundation Models for Building Open-Ended Embodied Agents in Crafter",
      "title_zh": "CrafterDojoï¼šç”¨äºåœ¨ Crafter ä¸­æ„å»ºå¼€æ”¾å¼å…·èº«æ™ºèƒ½ä½“çš„åŸºåº§æ¨¡å‹å¥—ä»¶",
      "authors": [
        "Junyeong Park",
        "Hyeonseo Cho",
        "Sungjin Ahn"
      ],
      "abstract": "Developing general-purpose embodied agents is a core challenge in AI. Minecraft provides rich complexity and internet-scale data, but its slow speed and engineering overhead make it unsuitable for rapid prototyping. Crafter offers a lightweight alternative that retains key challenges from Minecraft, yet its use has remained limited to narrow tasks due to the absence of foundation models that have driven progress in the Minecraft setting. In this paper, we present CrafterDojo, a suite of foundation models and tools that unlock the Crafter environment as a lightweight, prototyping-friendly, and Minecraft-like testbed for general-purpose embodied agent research. CrafterDojo addresses this by introducing CrafterVPT, CrafterCLIP, and CrafterSteve-1 for behavior priors, vision-language grounding, and instruction following, respectively. In addition, we provide toolkits for generating behavior and caption datasets (CrafterPlay and CrafterCaption), reference agent implementations, benchmark evaluations, and a complete open-source codebase.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨å…·èº«æ™ºèƒ½ä½“(embodied agents)å¼€å‘ä¸­Minecraftç¯å¢ƒè¿‡äºå¤æ‚ä¸”éš¾ä»¥å¿«é€ŸåŸå‹åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†CrafterDojoå¥—ä»¶ã€‚CrafterDojoæ˜¯ä¸€ç³»åˆ—æ—¨åœ¨å°†è½»é‡çº§Crafterç¯å¢ƒæ‰“é€ ä¸ºç±»Minecraftç ”ç©¶åŸºå‡†çš„åŸºç¡€æ¨¡å‹(foundation models)å’Œå·¥å…·é›†åˆï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸç¼ºä¹å¼ºåŠ›åŸºç¡€æ¨¡å‹æ”¯æŒçš„ç©ºç™½ã€‚è¯¥å¥—ä»¶æ ¸å¿ƒåŒ…å«äº†ç”¨äºè¡Œä¸ºå…ˆéªŒçš„CrafterVPTã€å®ç°è§†è§‰è¯­è¨€å¯¹é½çš„CrafterCLIPï¼Œä»¥åŠä¸“æ³¨äºæŒ‡ä»¤éµå¾ªçš„CrafterSteve-1ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜é…å¥—æä¾›äº†ç”Ÿæˆè¡Œä¸ºä¸æè¿°æ•°æ®é›†çš„å·¥å…·åŒ…CrafterPlayå’ŒCrafterCaptionï¼Œå¹¶å¼€æ”¾äº†å®Œæ•´çš„ä»£ç åº“ä¸åŸºå‡†æµ‹è¯•ã€‚é€šè¿‡æ•´åˆè¿™äº›åŸºç¡€æ¨¡å‹ä¸å·¥å…·ï¼ŒCrafterDojoä¸ºæ¢ç´¢å¼€æ”¾å¼æ™ºèƒ½ä½“è¡Œä¸ºæä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”åŠŸèƒ½å®Œå¤‡çš„ç ”ç©¶å¹³å°ã€‚è¿™é¡¹å·¥ä½œæ˜¾è‘—é™ä½äº†å…·èº«æ™ºèƒ½ä½“ç ”ç©¶çš„é—¨æ§›ï¼Œä¸ºåŠ é€Ÿé€šç”¨äººå·¥æ™ºèƒ½åœ¨è½»é‡çº§ç¯å¢ƒä¸­çš„è¿­ä»£å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13530v1",
      "published_date": "2025-08-19 05:43:19 UTC",
      "updated_date": "2025-08-19 05:43:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:52:21.847167+00:00"
    },
    {
      "arxiv_id": "2508.14129v1",
      "title": "Fracture Detection and Localisation in Wrist and Hand Radiographs using Detection Transformer Variants",
      "title_zh": "åŸºäº Detection Transformer å˜ä½“çš„è…•éƒ¨åŠæ‰‹éƒ¨æ”¾å°„å½±åƒéª¨æŠ˜æ£€æµ‹ä¸å®šä½",
      "authors": [
        "Aditya Bagri",
        "Vasanthakumar Venugopal",
        "Anandakumar D",
        "Revathi Ezhumalai",
        "Kalyan Sivasailam",
        "Bargava Subramanian",
        "VarshiniPriya",
        "Meenakumari K S",
        "Abi M",
        "Renita S"
      ],
      "abstract": "Background: Accurate diagnosis of wrist and hand fractures using radiographs is essential in emergency care, but manual interpretation is slow and prone to errors. Transformer-based models show promise in improving medical image analysis, but their application to extremity fractures is limited. This study addresses this gap by applying object detection transformers to wrist and hand X-rays.\n  Methods: We fine-tuned the RT-DETR and Co-DETR models, pre-trained on COCO, using over 26,000 annotated X-rays from a proprietary clinical dataset. Each image was labeled for fracture presence with bounding boxes. A ResNet-50 classifier was trained on cropped regions to refine abnormality classification. Supervised contrastive learning was used to enhance embedding quality. Performance was evaluated using AP@50, precision, and recall metrics, with additional testing on real-world X-rays.\n  Results: RT-DETR showed moderate results (AP@50 = 0.39), while Co-DETR outperformed it with an AP@50 of 0.615 and faster convergence. The integrated pipeline achieved 83.1% accuracy, 85.1% precision, and 96.4% recall on real-world X-rays, demonstrating strong generalization across 13 fracture types. Visual inspection confirmed accurate localization.\n  Conclusion: Our Co-DETR-based pipeline demonstrated high accuracy and clinical relevance in wrist and hand fracture detection, offering reliable localization and differentiation of fracture types. It is scalable, efficient, and suitable for real-time deployment in hospital workflows, improving diagnostic speed and reliability in musculoskeletal radiology.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ€¥è¯ŠåŒ»ç–—ä¸­è…•éƒ¨å’Œæ‰‹éƒ¨æ”¾å°„å½±åƒéª¨æŠ˜è¯Šæ–­æ•ˆç‡ä½ä¸”æ˜“å‡ºé”™çš„æŒ‘æˆ˜ï¼Œæå‡ºå¹¶è¯„ä¼°äº†åŸºäºæ£€æµ‹ Transformer å˜ä½“æ¨¡å‹çš„è‡ªåŠ¨è¯†åˆ«æ–¹æ¡ˆã€‚ç ”ç©¶è€…åœ¨åŒ…å«è¶…è¿‡26,000å¼ æ ‡æ³¨Xå°„çº¿ç‰‡çš„ä¸´åºŠæ•°æ®é›†ä¸Šå¯¹ RT-DETR å’Œ Co-DETR æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶ç»“åˆ ResNet-50 åˆ†ç±»å™¨ä¸ç›‘ç£å¯¹æ¯”å­¦ä¹  (Supervised Contrastive Learning) æŠ€æœ¯æ¥å¢å¼ºç‰¹å¾æå–ä¸åˆ†ç±»ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCo-DETR æ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äº RT-DETRï¼Œå…¶ AP@50 è¾¾åˆ° 0.615ï¼Œä¸”æ•´ä¸ªæµæ°´çº¿åœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸­å®ç°äº† 83.1% çš„å‡†ç¡®ç‡å’Œ 96.4% çš„å¬å›ç‡ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿè·¨ 13 ç§éª¨æŠ˜ç±»å‹å®ç°ç²¾ç¡®çš„å®šä½ä¸åˆ†ç±»ï¼Œè¯æ˜äº†å…¶åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºè‚Œè‚‰éª¨éª¼æ”¾å°„å­¦æä¾›äº†å¯æ‰©å±•ä¸”é«˜æ•ˆçš„å®æ—¶éƒ¨ç½²æ–¹æ¡ˆï¼Œå¯¹äºæå‡åŒ»é™¢è¯Šæ–­æµç¨‹çš„å¯é æ€§å…·æœ‰é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "18 pages, 21 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.14129v1",
      "published_date": "2025-08-19 05:41:49 UTC",
      "updated_date": "2025-08-19 05:41:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:52:36.035354+00:00"
    },
    {
      "arxiv_id": "2508.13524v1",
      "title": "Evaluating Open-Source Vision Language Models for Facial Emotion Recognition against Traditional Deep Learning Models",
      "title_zh": "å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ä¸ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨é¢éƒ¨è¡¨æƒ…è¯†åˆ«ä¸­çš„å¯¹æ¯”è¯„ä¼°",
      "authors": [
        "Vamsi Krishna Mulukutla",
        "Sai Supriya Pavarala",
        "Srinivasa Raju Rudraraju",
        "Sridevi Bonthu"
      ],
      "abstract": "Facial Emotion Recognition (FER) is crucial for applications such as human-computer interaction and mental health diagnostics. This study presents the first empirical comparison of open-source Vision-Language Models (VLMs), including Phi-3.5 Vision and CLIP, against traditional deep learning models VGG19, ResNet-50, and EfficientNet-B0 on the challenging FER-2013 dataset, which contains 35,887 low-resolution grayscale images across seven emotion classes. To address the mismatch between VLM training assumptions and the noisy nature of FER data, we introduce a novel pipeline that integrates GFPGAN-based image restoration with FER evaluation. Results show that traditional models, particularly EfficientNet-B0 (86.44%) and ResNet-50 (85.72%), significantly outperform VLMs like CLIP (64.07%) and Phi-3.5 Vision (51.66%), highlighting the limitations of VLMs in low-quality visual tasks. In addition to performance evaluation using precision, recall, F1-score, and accuracy, we provide a detailed computational cost analysis covering preprocessing, training, inference, and evaluation phases, offering practical insights for deployment. This work underscores the need for adapting VLMs to noisy environments and provides a reproducible benchmark for future research in emotion recognition.",
      "tldr_zh": "è¯¥ç ”ç©¶é¦–æ¬¡å¯¹å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models, VLMs)ï¼Œå¦‚ Phi-3.5 Vision å’Œ CLIPï¼Œä¸ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹ VGG19ã€ResNet-50 åŠ EfficientNet-B0 åœ¨æŒ‘æˆ˜æ€§çš„ FER-2013 æ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢éƒ¨è¡¨æƒ…è¯†åˆ«(Facial Emotion Recognition, FER)çš„å®è¯å¯¹æ¯”ã€‚ä¸ºäº†åº”å¯¹ VLM è®­ç»ƒå‡è®¾ä¸ FER æ•°æ®å™ªå£°ç‰¹æ€§ä¹‹é—´çš„ä¸åŒ¹é…ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§é›†æˆ GFPGAN å›¾åƒä¿®å¤æŠ€æœ¯çš„è¯„ä¼°æµç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¼ ç»Ÿæ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äº VLMsï¼Œå…¶ä¸­ EfficientNet-B0 å’Œ ResNet-50 çš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ° 86.44% å’Œ 85.72%ï¼Œè€Œ CLIP å’Œ Phi-3.5 Vision ä»…ä¸º 64.07% å’Œ 51.66%ã€‚è¿™ä¸€å‘ç°å‡¸æ˜¾äº† VLMs åœ¨å¤„ç†ä½è´¨é‡è§†è§‰ä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†å°†å…¶é€‚é…åˆ°å™ªå£°ç¯å¢ƒçš„å¿…è¦æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†æ¶µç›–é¢„å¤„ç†ã€è®­ç»ƒã€æ¨ç†å’Œè¯„ä¼°å…¨é˜¶æ®µçš„è¯¦ç»†è®¡ç®—æˆæœ¬åˆ†æï¼Œä¸ºå®é™…åº”ç”¨éƒ¨ç½²æä¾›äº†é‡è¦å‚è€ƒã€‚è¯¥å·¥ä½œä¸ºæƒ…ç»ªè¯†åˆ«é¢†åŸŸçš„æœªæ¥ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªå¯é‡å¤çš„åŸºå‡†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13524v1",
      "published_date": "2025-08-19 05:33:10 UTC",
      "updated_date": "2025-08-19 05:33:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:52:30.458050+00:00"
    },
    {
      "arxiv_id": "2508.13522v1",
      "title": "DDoS Attacks in Cloud Computing: Detection and Prevention",
      "title_zh": "äº‘è®¡ç®—ç¯å¢ƒä¸‹çš„ DDoS æ”»å‡»æ£€æµ‹ä¸é˜²å¾¡",
      "authors": [
        "Zain Ahmad",
        "Musab Ahmad",
        "Bilal Ahmad"
      ],
      "abstract": "DDoS attacks are one of the most prevalent and harmful cybersecurity threats faced by organizations and individuals today. In recent years, the complexity and frequency of DDoS attacks have increased significantly, making it challenging to detect and mitigate them effectively. The study analyzes various types of DDoS attacks, including volumetric, protocol, and application layer attacks, and discusses the characteristics, impact, and potential targets of each type. It also examines the existing techniques used for DDoS attack detection, such as packet filtering, intrusion detection systems, and machine learning-based approaches, and their strengths and limitations. Moreover, the study explores the prevention techniques employed to mitigate DDoS attacks, such as firewalls, rate limiting , CPP and ELD mechanism. It evaluates the effectiveness of each approach and its suitability for different types of attacks and environments. In conclusion, this study provides a comprehensive overview of the different types of DDoS attacks, their detection, and prevention techniques. It aims to provide insights and guidelines for organizations and individuals to enhance their cybersecurity posture and protect against DDoS attacks.",
      "tldr_zh": "æœ¬ç ”ç©¶å¯¹äº‘è®¡ç®—ç¯å¢ƒä¸‹çš„ DDoS Attacks è¿›è¡Œäº†å…¨é¢åˆ†æï¼Œé‡ç‚¹æ¢è®¨äº†å…¶åœ¨ç½‘ç»œå®‰å…¨ä¸­çš„å¨èƒä¸æ—¥ç›Šå¢é•¿çš„å¤æ‚æ€§ã€‚æ–‡ç« è¯¦ç»†åˆ†ç±»å¹¶é˜è¿°äº†åŒ…æ‹¬ Volumetric Attacksã€Protocol Attacks ä»¥åŠ Application Layer Attacks åœ¨å†…çš„å¤šç§æ”»å‡»ç±»å‹çš„ç‰¹å¾ã€å½±å“åŠæ½œåœ¨ç›®æ ‡ã€‚åœ¨æ£€æµ‹æŠ€æœ¯æ–¹é¢ï¼Œç ”ç©¶æ·±å…¥è¯„ä¼°äº† Packet Filteringã€Intrusion Detection Systems (IDS) ä»¥åŠåŸºäº Machine Learning çš„æ£€æµ‹æ–¹æ³•ï¼Œå¹¶å¯¹æ¯”äº†å„è‡ªçš„ä¼˜ç¼ºç‚¹ã€‚åŒæ—¶ï¼Œæœ¬æ–‡æ¢è®¨äº† Firewallsã€Rate Limitingã€CPP ä»¥åŠ ELD æœºåˆ¶ç­‰é¢„é˜²æŠ€æœ¯åœ¨ç¼“è§£æ”»å‡»æ–¹é¢çš„æœ‰æ•ˆæ€§åŠå…¶é€‚ç”¨åœºæ™¯ã€‚é€šè¿‡ç³»ç»Ÿæ€§åœ°æ€»ç»“ç°æœ‰çš„æ£€æµ‹ä¸é˜²å¾¡æ¡†æ¶ï¼Œè¯¥ç ”ç©¶ä¸ºæå‡ç»„ç»‡çš„ç½‘ç»œå®‰å…¨å§¿æ€å¹¶æœ‰æ•ˆæŠµå¾¡ DDoS å¨èƒæä¾›äº†é‡è¦çš„è§è§£ä¸æŒ‡å¯¼å‡†åˆ™ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13522v1",
      "published_date": "2025-08-19 05:27:37 UTC",
      "updated_date": "2025-08-19 05:27:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:52:30.262049+00:00"
    },
    {
      "arxiv_id": "2508.13518v1",
      "title": "Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency",
      "title_zh": "åŸºäºè·¨åŸŸå‡ ä½•ä¸€è‡´æ€§çš„ VFM è¡ç”Ÿæ½œç©ºé—´åå·®åˆ†å¸ƒæ ¡å‡†",
      "authors": [
        "Yanbiao Ma",
        "Wei Dai",
        "Bowei Liu",
        "Jiayi Chen",
        "Wenke Huang",
        "Guancheng Wan",
        "Zhiwu Lu",
        "Junchi Yan"
      ],
      "abstract": "Despite the fast progress of deep learning, one standing challenge is the gap of the observed training samples and the underlying true distribution. There are multiple reasons for the causing of this gap e.g. sampling bias, noise etc. In the era of foundation models, we show that when leveraging the off-the-shelf (vision) foundation models (e.g., CLIP, DINOv2) for feature extraction, the geometric shapes of the resulting feature distributions exhibit remarkable transferability across domains and datasets. To verify its practical usefulness, we embody our geometric knowledge-guided distribution calibration framework in two popular and challenging settings: federated learning and long-tailed recognition. In the federated setting, we devise a technique of acquiring the global geometric shape under privacy constraints, then leverage this knowledge to generate new samples for clients, in the aim of bridging the gap between local and global observations. In long-tailed learning, it utilizes the geometric knowledge transferred from sample-rich categories to recover the true distribution for sample-scarce tail classes. Comprehensive experiments show that our proposed geometric knowledge-guided distribution calibration effectively overcomes information deficits caused by data heterogeneity and sample imbalance, with boosted performance across benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å­¦ä¹ ä¸­è®­ç»ƒæ ·æœ¬ä¸çœŸå®åˆ†å¸ƒä¹‹é—´ç”±äºé‡‡æ ·åå·®æˆ–å™ªå£°äº§ç”Ÿçš„é¸¿æ²Ÿï¼ŒæŒ‡å‡ºåˆ©ç”¨ CLIP å’Œ DINOv2 ç­‰è§†è§‰åŸºç¡€æ¨¡å‹ (Vision Foundation Models) æå–ç‰¹å¾æ—¶ï¼Œå…¶ç‰¹å¾åˆ†å¸ƒçš„å‡ ä½•å½¢çŠ¶å…·æœ‰è·¨é¢†åŸŸå’Œè·¨æ•°æ®é›†çš„æ˜¾è‘—å¯ç§»æ¤æ€§ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªå‡ ä½•çŸ¥è¯†å¼•å¯¼çš„åˆ†å¸ƒæ ¡å‡† (Geometric Knowledge-guided Distribution Calibration) æ¡†æ¶ï¼Œå¹¶å°†å…¶åº”ç”¨äºè”é‚¦å­¦ä¹  (Federated Learning) å’Œé•¿å°¾è¯†åˆ« (Long-tailed Recognition) ä»»åŠ¡ã€‚åœ¨è”é‚¦å­¦ä¹ åœºæ™¯ä¸‹ï¼Œè¯¥æ¡†æ¶åœ¨éšç§çº¦æŸä¸‹è·å–å…¨å±€å‡ ä½•å½¢çŠ¶å¹¶ä¸ºå®¢æˆ·ç«¯ç”Ÿæˆæ–°æ ·æœ¬ï¼Œæ—¨åœ¨å¼¥è¡¥å±€éƒ¨è§‚å¯Ÿä¸å…¨å±€åˆ†å¸ƒä¹‹é—´çš„å·®è·ï¼›åœ¨é•¿å°¾å­¦ä¹ ä¸­ï¼Œå®ƒå°†å‡ ä½•çŸ¥è¯†ä»ä¸°å¯Œç±»åˆ«è¿ç§»è‡³ç¨€ç¼ºç±»åˆ«ä»¥æ¢å¤å°¾éƒ¨ç±»çš„çœŸå®åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆå…‹æœäº†ç”±æ•°æ®å¼‚è´¨æ€§ (Data Heterogeneity) å’Œæ ·æœ¬ä¸å¹³è¡¡ (Sample Imbalance) å¼•èµ·çš„ä¿¡æ¯åŒ®ä¹ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚è¯¥ç ”ç©¶é€šè¿‡è·¨åŸŸå‡ ä½•ä¸€è‡´æ€§æ ¡å‡†æ½œåœ¨ç©ºé—´ä¸­çš„åå·®åˆ†å¸ƒï¼Œä¸ºè§£å†³æ•°æ®åˆ†å¸ƒä¸å‡é—®é¢˜æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, CVPR Oral",
      "pdf_url": "https://arxiv.org/pdf/2508.13518v1",
      "published_date": "2025-08-19 05:22:59 UTC",
      "updated_date": "2025-08-19 05:22:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:52:36.224549+00:00"
    },
    {
      "arxiv_id": "2508.13517v1",
      "title": "Heterogeneous Influence Maximization in User Recommendation",
      "title_zh": "ç”¨æˆ·æ¨èä¸­çš„å¼‚æ„å½±å“åŠ›æœ€å¤§åŒ–",
      "authors": [
        "Hongru Hou",
        "Jiachen Sun",
        "Wenqing Lin",
        "Wendong Bi",
        "Xiangrong Wang",
        "Deqing Yang"
      ],
      "abstract": "User recommendation systems enhance user engagement by encouraging users to act as inviters to interact with other users (invitees), potentially fostering information propagation. Conventional recommendation methods typically focus on modeling interaction willingness. Influence-Maximization (IM) methods focus on identifying a set of users to maximize the information propagation. However, existing methods face two significant challenges. First, recommendation methods fail to unleash the candidates' spread capability. Second, IM methods fail to account for the willingness to interact. To solve these issues, we propose two models named HeteroIR and HeteroIM. HeteroIR provides an intuitive solution to unleash the dissemination potential of user recommendation systems. HeteroIM fills the gap between the IM method and the recommendation task, improving interaction willingness and maximizing spread coverage. The HeteroIR introduces a two-stage framework to estimate the spread profits. The HeteroIM incrementally selects the most influential invitee to recommend and rerank based on the number of reverse reachable (RR) sets containing inviters and invitees. RR set denotes a set of nodes that can reach a target via propagation. Extensive experiments show that HeteroIR and HeteroIM significantly outperform the state-of-the-art baselines with the p-value < 0.05. Furthermore, we have deployed HeteroIR and HeteroIM in Tencent's online gaming platforms and gained an 8.5\\% and 10\\% improvement in the online A/B test, respectively. Implementation codes are available at https://github.com/socialalgo/HIM.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”¨æˆ·æ¨èç³»ç»Ÿä¸­çš„ä¿¡æ¯ä¼ æ’­é—®é¢˜ï¼Œæå‡ºäº†HeteroIRå’ŒHeteroIMä¸¤ä¸ªæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ¨èæ–¹æ³•å¿½è§†ç”¨æˆ·ä¼ æ’­èƒ½åŠ›ä»¥åŠå½±å“åŠ›æœ€å¤§åŒ–(Influence Maximization)æ–¹æ³•å¿½è§†äº’åŠ¨æ„æ„¿çš„åŒé‡æŒ‘æˆ˜ã€‚HeteroIRå¼•å…¥äº†ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶æ¥ä¼°è®¡ä¼ æ’­æ”¶ç›Šï¼Œä»¥å……åˆ†é‡Šæ”¾æ¨èç³»ç»Ÿçš„æ‰©æ•£æ½œåŠ›ã€‚HeteroIMé€šè¿‡åˆ†æåŒ…å«é‚€è¯·è€…å’Œå—é‚€è€…çš„åå‘å¯è¾¾é›†(Reverse Reachable sets)æ•°é‡ï¼Œå¢é‡å¼åœ°é€‰æ‹©æœ€å…·å½±å“åŠ›çš„å—é‚€è€…è¿›è¡Œæ¨èä¸é‡æ’ï¼Œä»è€Œå…¼é¡¾äº’åŠ¨æ„æ„¿ä¸ä¼ æ’­è¦†ç›–èŒƒå›´ã€‚å¤§é‡å®éªŒç»“æœè¯æ˜ï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹æ˜¾è‘—ä¼˜äºç°æœ‰çš„SOTAåŸºå‡†æ¨¡å‹ã€‚ç›®å‰è¯¥ç³»ç»Ÿå·²åœ¨è…¾è®¯åœ¨çº¿æ¸¸æˆå¹³å°æˆåŠŸéƒ¨ç½²ï¼Œå¹¶åœ¨åœ¨çº¿A/Bæµ‹è¯•ä¸­åˆ†åˆ«å®ç°äº†8.5%å’Œ10%çš„æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG",
        "cs.SI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted in CIKM 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.13517v1",
      "published_date": "2025-08-19 05:20:48 UTC",
      "updated_date": "2025-08-19 05:20:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:52:38.032119+00:00"
    },
    {
      "arxiv_id": "2508.13514v1",
      "title": "ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs",
      "title_zh": "ProMedï¼šåŸºäºShapleyä¿¡æ¯å¢ç›Šå¼•å¯¼å¼ºåŒ–å­¦ä¹ çš„ä¸»åŠ¨å¼åŒ»ç–—å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Hongxin Ding",
        "Baixiang Huang",
        "Yue Fang",
        "Weibin Liao",
        "Xinke Jiang",
        "Zheng Li",
        "Junfeng Zhao",
        "Yasha Wang"
      ],
      "abstract": "Interactive medical questioning is essential in real-world clinical consultations, where physicians must actively gather information from patients. While medical Large Language Models (LLMs) have shown impressive capabilities in static medical question answering, they predominantly operate under a reactive paradigm: generating answers directly without seeking additional information, which risks incorrect diagnoses in such interactive settings. To address this limitation, we propose ProMed, a reinforcement learning (RL) framework that transitions medical LLMs toward a proactive paradigm, equipping them with the ability to ask clinically valuable questions before decision-making. At the core of ProMed is the Shapley Information Gain (SIG) reward, which quantifies the clinical utility of each question by combining the amount of newly acquired information with its contextual importance, estimated via Shapley values. We integrate SIG into a two-stage training pipeline: (1) SIG-Guided Model Initialization uses Monte Carlo Tree Search (MCTS) to construct high-reward interaction trajectories to supervise the model, and (2) SIG-Augmented Policy Optimization, which integrates SIG and enhances RL with a novel SIG-guided Reward Distribution Mechanism that assigns higher rewards to informative questions for targeted optimization. Extensive experiments on two newly curated partial-information medical benchmarks demonstrate that ProMed significantly outperforms state-of-the-art methods by an average of 6.29% and delivers a 54.45% gain over the reactive paradigm, while also generalizing robustly to out-of-domain cases.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ProMedï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å°†åŒ»ç–—å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä»è¢«åŠ¨å›ç­”è½¬å‘ä¸»åŠ¨è¯¢é—®çš„å¼ºåŒ–å­¦ä¹  (RL) æ¡†æ¶ï¼Œä»¥è§£å†³ä¸´åºŠå’¨è¯¢ä¸­å› ä¿¡æ¯ä¸è¶³å¯¼è‡´çš„è¯Šæ–­é£é™©ã€‚ProMed çš„æ ¸å¿ƒæ˜¯ Shapley Information Gain (SIG) å¥–åŠ±æœºåˆ¶ï¼Œé€šè¿‡ç»“åˆæ–°è·å–çš„ä¿¡æ¯é‡ä¸å…¶ä¸Šä¸‹æ–‡é‡è¦æ€§ï¼ˆåŸºäº Shapley values ä¼°ç®—ï¼‰æ¥é‡åŒ–æ¯ä¸ªé—®é¢˜çš„ä¸´åºŠå®ç”¨æ€§ã€‚å…¶è®­ç»ƒè¿‡ç¨‹åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ (MCTS) æ„å»ºé«˜å¥–åŠ±è½¨è¿¹è¿›è¡Œå¼•å¯¼å¼æ¨¡å‹åˆå§‹åŒ–ï¼Œéšåé€šè¿‡ SIG å¢å¼ºçš„ç­–ç•¥ä¼˜åŒ–å’Œæ–°å‹å¥–åŠ±åˆ†é…æœºåˆ¶å¯¹æ¨¡å‹è¿›è¡Œé’ˆå¯¹æ€§è°ƒä¼˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProMed åœ¨å¤šä¸ªåŒ»ç–—åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œç›¸è¾ƒäºè¢«åŠ¨èŒƒå¼å®ç°äº† 54.45% çš„æ€§èƒ½æå‡ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„é¢†åŸŸå¤–æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13514v1",
      "published_date": "2025-08-19 05:01:40 UTC",
      "updated_date": "2025-08-19 05:01:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:52:48.924689+00:00"
    },
    {
      "arxiv_id": "2508.15836v1",
      "title": "MorphNAS: Differentiable Architecture Search for Morphologically-Aware Multilingual NER",
      "title_zh": "MorphNASï¼šé¢å‘å½¢æ€æ„ŸçŸ¥å¤šè¯­è¨€å‘½åå®ä½“è¯†åˆ«çš„å¯å¾®åˆ†æ¶æ„æœç´¢",
      "authors": [
        "Prathamesh Devadiga",
        "Omkaar Jayadev Shetty",
        "Hiya Nachnani",
        "Prema R"
      ],
      "abstract": "Morphologically complex languages, particularly multiscript Indian languages, present significant challenges for Natural Language Processing (NLP). This work introduces MorphNAS, a novel differentiable neural architecture search framework designed to address these challenges. MorphNAS enhances Differentiable Architecture Search (DARTS) by incorporating linguistic meta-features such as script type and morphological complexity to optimize neural architectures for Named Entity Recognition (NER). It automatically identifies optimal micro-architectural elements tailored to language-specific morphology. By automating this search, MorphNAS aims to maximize the proficiency of multilingual NLP models, leading to improved comprehension and processing of these complex languages.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½¢æ€å¤æ‚çš„è¯­è¨€ï¼ˆç‰¹åˆ«æ˜¯å¤šè„šæœ¬å°åº¦è¯­ï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä¸­çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†MorphNASï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¯å¾®åˆ†ç¥ç»æ¶æ„æœç´¢(Differentiable Architecture Search)æ¡†æ¶ã€‚MorphNASé€šè¿‡åœ¨DARTSçš„åŸºç¡€ä¸Šèå…¥è„šæœ¬ç±»å‹(script type)å’Œå½¢æ€å¤æ‚æ€§(morphological complexity)ç­‰è¯­è¨€å…ƒç‰¹å¾ï¼Œä¸“é—¨ä¼˜åŒ–äº†å‘½åå®ä½“è¯†åˆ«(NER)çš„ç¥ç»æ¶æ„ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«é’ˆå¯¹ç‰¹å®šè¯­è¨€å½¢æ€å®šåˆ¶çš„æœ€ä¼˜å¾®æ¶æ„å…ƒç´ (micro-architectural elements)ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æœç´¢è¿‡ç¨‹æå‡äº†æ¨¡å‹çš„é€‚é…æ€§ã€‚MorphNASæ—¨åœ¨æœ€å¤§åŒ–å¤šè¯­è¨€NLPæ¨¡å‹çš„æ•ˆèƒ½ï¼Œä»è€Œæ˜¾è‘—æ”¹å–„äº†å¯¹è¿™äº›å¤æ‚è¯­è¨€çš„ç†è§£ä¸å¤„ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15836v1",
      "published_date": "2025-08-19 04:48:51 UTC",
      "updated_date": "2025-08-19 04:48:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:52:54.128371+00:00"
    },
    {
      "arxiv_id": "2508.13500v2",
      "title": "LLM-Enhanced Linear Autoencoders for Recommendation",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹å¢å¼ºçš„æ¨èçº¿æ€§è‡ªç¼–ç å™¨",
      "authors": [
        "Jaewan Moon",
        "Seongmin Park",
        "Jongwuk Lee"
      ],
      "abstract": "Large language models (LLMs) have been widely adopted to enrich the semantic representation of textual item information in recommender systems. However, existing linear autoencoders (LAEs) that incorporate textual information rely on sparse word co-occurrence patterns, limiting their ability to capture rich textual semantics. To address this, we propose L3AE, the first integration of LLMs into the LAE framework. L3AE effectively integrates the heterogeneous knowledge of textual semantics and user-item interactions through a two-phase optimization strategy. (i) L3AE first constructs a semantic item-to-item correlation matrix from LLM-derived item representations. (ii) It then learns an item-to-item weight matrix from collaborative signals while distilling semantic item correlations as regularization. Notably, each phase of L3AE is optimized through closed-form solutions, ensuring global optimality and computational efficiency. Extensive experiments demonstrate that L3AE consistently outperforms state-of-the-art LLM-enhanced models on three benchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20. The source code is available at https://github.com/jaewan7599/L3AE_CIKM2025.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† L3AEï¼Œè¿™æ˜¯é¦–ä¸ªå°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Models, LLMsï¼‰é›†æˆåˆ°çº¿æ€§è‡ªåŠ¨ç¼–ç å™¨ï¼ˆLinear Autoencoders, LAEsï¼‰æ¡†æ¶ä¸­çš„æ¨èæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–¹æ³•ä¾èµ–ç¨€ç–è¯å…±ç°è€Œæ— æ³•æ•æ‰ä¸°å¯Œè¯­ä¹‰çš„é—®é¢˜ã€‚L3AE é‡‡ç”¨ä¸¤é˜¶æ®µä¼˜åŒ–ç­–ç•¥ï¼Œé¦–å…ˆåˆ©ç”¨ LLM æ´¾ç”Ÿçš„ç‰©å“è¡¨ç¤ºæ„å»ºè¯­ä¹‰å±‚é¢çš„ç‰©å“ç›¸å…³æ€§çŸ©é˜µï¼Œéšåä»ååŒä¿¡å·ä¸­å­¦ä¹ ç‰©å“æƒé‡çŸ©é˜µï¼Œå¹¶å°†è¯­ä¹‰ç›¸å…³æ€§ä½œä¸ºæ­£åˆ™åŒ–é¡¹è¿›è¡Œè’¸é¦ã€‚è¯¥æ¨¡å‹çš„ä¸¤ä¸ªé˜¶æ®µå‡é€šè¿‡é—­å¼è§£ï¼ˆclosed-form solutionsï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œåœ¨ä¿è¯å…¨å±€æœ€ä¼˜æ€§çš„åŒæ—¶æ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒL3AE åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„ LLM å¢å¼ºæ¨¡å‹ï¼Œå…¶åœ¨ Recall@20 å’Œ NDCG@20 æŒ‡æ ‡ä¸Šåˆ†åˆ«å–å¾—äº† 27.6% å’Œ 39.3% çš„æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by CIKM 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.13500v2",
      "published_date": "2025-08-19 04:20:14 UTC",
      "updated_date": "2025-08-26 12:59:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:53:00.323369+00:00"
    },
    {
      "arxiv_id": "2508.14922v1",
      "title": "Fusing Structural Phenotypes with Functional Data for Early Prediction of Primary Angle Closure Glaucoma Progression",
      "title_zh": "èåˆç»“æ„è¡¨å‹ä¸åŠŸèƒ½æ•°æ®ç”¨äºåŸå‘æ€§é—­è§’å‹é’å…‰çœ¼è¿›å±•çš„æ—©æœŸé¢„æµ‹",
      "authors": [
        "Swati Sharma",
        "Thanadet Chuangsuwanich",
        "Royston K. Y. Tan",
        "Shimna C. Prasad",
        "Tin A. Tun",
        "Shamira A. Perera",
        "Martin L. Buist",
        "Tin Aung",
        "Monisha E. Nongpiur",
        "MichaÃ«l J. A. Girard"
      ],
      "abstract": "Purpose: To classify eyes as slow or fast glaucoma progressors in patients with primary angle closure glaucoma (PACG) using an integrated approach combining optic nerve head (ONH) structural features and sector-based visual field (VF) functional parameters. Methods: PACG patients with >5 reliable VF tests over >5 years were included. Progression was assessed in Zeiss Forum, with baseline VF within six months of OCT. Fast progression was VFI decline <-2.0% per year; slow progression >-2.0% per year. OCT volumes were AI-segmented to extract 31 ONH parameters. The Glaucoma Hemifield Test defined five regions per hemifield, aligned with RNFL distribution. Mean sensitivity per region was combined with structural parameters to train ML classifiers. Multiple models were tested, and SHAP identified key predictors. Main outcome measures: Classification of slow versus fast progressors using combined structural and functional data. Results: We analyzed 451 eyes from 299 patients. Mean VFI progression was -0.92% per year; 369 eyes progressed slowly and 82 rapidly. The Random Forest model combining structural and functional features achieved the best performance (AUC = 0.87, 2000 Monte Carlo iterations). SHAP identified six key predictors: inferior MRW, inferior and inferior-temporal RNFL thickness, nasal-temporal LC curvature, superior nasal VF sensitivity, and inferior RNFL and GCL+IPL thickness. Models using only structural or functional features performed worse with AUC of 0.82 and 0.78, respectively. Conclusions: Combining ONH structural and VF functional parameters significantly improves classification of progression risk in PACG. Inferior ONH features, MRW and RNFL thickness, were the most predictive, highlighting the critical role of ONH morphology in monitoring disease progression.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨é€šè¿‡æ•´åˆè§†ç¥ç»ä¹³å¤´ (ONH) ç»“æ„ç‰¹å¾ä¸è§†é‡ (VF) åŠŸèƒ½å‚æ•°ï¼Œå¯¹åŸå‘æ€§é—­è§’å‹é’å…‰çœ¼ (PACG) æ‚£è€…çš„ç—…ç¨‹è¿›å±•é€Ÿåº¦è¿›è¡Œæ—©æœŸåˆ†ç±»é¢„æµ‹ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ AI æŠ€æœ¯å¯¹ OCT å›¾åƒè¿›è¡Œåˆ†å‰²ä»¥æå– 31 é¡¹ç»“æ„å‚æ•°ï¼Œå¹¶ç»“åˆæ‰‡åŒºåŸºç¡€çš„ VF æ•æ„Ÿåº¦æ•°æ®ï¼Œè®­ç»ƒäº†å¤šç§æœºå™¨å­¦ä¹  (ML) æ¨¡å‹æ¥åŒºåˆ†å¿«é€Ÿä¸æ…¢é€Ÿè¿›å±•è€…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œèåˆäº†ç»“æ„ä¸åŠŸèƒ½ç‰¹å¾çš„ Random Forest æ¨¡å‹è¡¨ç°æœ€ä¼˜ï¼Œå…¶æ›²çº¿ä¸‹é¢ç§¯ (AUC) è¾¾åˆ° 0.87ï¼Œæ˜¾è‘—é«˜äºä»…ä½¿ç”¨å•ä¸€ç»“æ„ (AUC=0.82) æˆ–åŠŸèƒ½ (AUC=0.78) æ•°æ®çš„æ¨¡å‹ã€‚é€šè¿‡ SHAP åˆ†æï¼Œç ”ç©¶ç¡®å®šäº†ä¸‹æ–¹ MRWã€ä¸‹æ–¹åŠä¸‹é¢ä¾§ RNFL åšåº¦ç­‰ä¸ºæ ¸å¿ƒé¢„æµ‹æŒ‡æ ‡ï¼Œå‡¸æ˜¾äº†ä¸‹æ–¹ ONH å½¢æ€å­¦åœ¨ç–¾ç—…ç›‘æµ‹ä¸­çš„å…³é”®ä½œç”¨ã€‚è¯¥é¡¹å·¥ä½œè¯æ˜äº†å¤šç»´åº¦æ•°æ®èåˆèƒ½æœ‰æ•ˆæå‡ PACG è¿›å±•é£é™©çš„è¯„ä¼°ç²¾åº¦ï¼Œä¸ºé’å…‰çœ¼çš„ç²¾å‡†è¯Šç–—æä¾›äº†é‡è¦ä¾æ®ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "q-bio.QM",
      "comment": "23 pages, 5 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.14922v1",
      "published_date": "2025-08-19 04:19:05 UTC",
      "updated_date": "2025-08-19 04:19:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:53:01.317027+00:00"
    },
    {
      "arxiv_id": "2508.14128v1",
      "title": "CCFC: Core & Core-Full-Core Dual-Track Defense for LLM Jailbreak Protection",
      "title_zh": "CCFCï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹è¶Šç‹±é˜²æŠ¤çš„ Core ä¸ Core-Full-Core åŒè½¨é˜²å¾¡",
      "authors": [
        "Jiaming Hu",
        "Haoyu Wang",
        "Debarghya Mukherjee",
        "Ioannis Ch. Paschalidis"
      ],
      "abstract": "Jailbreak attacks pose a serious challenge to the safe deployment of large language models (LLMs). We introduce CCFC (Core & Core-Full-Core), a dual-track, prompt-level defense framework designed to mitigate LLMs' vulnerabilities from prompt injection and structure-aware jailbreak attacks. CCFC operates by first isolating the semantic core of a user query via few-shot prompting, and then evaluating the query using two complementary tracks: a core-only track to ignore adversarial distractions (e.g., toxic suffixes or prefix injections), and a core-full-core (CFC) track to disrupt the structural patterns exploited by gradient-based or edit-based attacks. The final response is selected based on a safety consistency check across both tracks, ensuring robustness without compromising on response quality. We demonstrate that CCFC cuts attack success rates by 50-75% versus state-of-the-art defenses against strong adversaries (e.g., DeepInception, GCG), without sacrificing fidelity on benign queries. Our method consistently outperforms state-of-the-art prompt-level defenses, offering a practical and effective solution for safer LLM deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CCFC (Core & Core-Full-Core)ï¼Œä¸€ç§åŒè½¨æç¤ºçº§é˜²å¾¡æ¡†æ¶ï¼Œæ—¨åœ¨ä¿æŠ¤å¤§è¯­è¨€æ¨¡å‹ (LLMs) å…å—æç¤ºæ³¨å…¥ (prompt injection) å’Œç»“æ„æ„ŸçŸ¥è¶Šç‹±æ”»å‡» (structure-aware jailbreak attacks) çš„å¨èƒã€‚CCFC é¦–å…ˆé€šè¿‡å°‘æ ·æœ¬æç¤º (few-shot prompting) æå–ç”¨æˆ·æŸ¥è¯¢çš„è¯­ä¹‰æ ¸å¿ƒ (semantic core)ï¼Œå¹¶åˆ©ç”¨ä¸¤ä¸ªäº’è¡¥è½¨é“è¿›è¡Œè¯„ä¼°ï¼šä»…æ ¸å¿ƒ (core-only) è½¨é“è´Ÿè´£è¿‡æ»¤æœ‰å®³åç¼€ç­‰å¯¹æŠ—æ€§å¹²æ‰°ï¼Œè€Œæ ¸å¿ƒ-å…¨-æ ¸å¿ƒ (CFC) è½¨é“åˆ™ç”¨äºç ´åæ”»å‡»è€…åˆ©ç”¨çš„ç»“æ„åŒ–æ¨¡å¼ã€‚ç³»ç»Ÿé€šè¿‡åŒè½¨å®‰å…¨æ€§ä¸€è‡´æ€§æ£€æŸ¥ (safety consistency check) æ¥å†³å®šæœ€ç»ˆè¾“å‡ºï¼Œåœ¨ç¡®ä¿é˜²å¾¡å¼ºåº¦çš„åŒæ—¶ç»´æŒå“åº”è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCCFC åœ¨å¯¹æŠ— DeepInception å’Œ GCG ç­‰å¼ºåŠ›æ”»å‡»æ—¶ï¼Œèƒ½å°†æ”»å‡»æˆåŠŸç‡é™ä½ 50-75%ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„æç¤ºçº§é˜²å¾¡æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•ä¸º LLM çš„å®‰å…¨éƒ¨ç½²æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œä¸”ä¸ä¼šåœ¨å¤„ç†è‰¯æ€§æŸ¥è¯¢æ—¶äº§ç”Ÿæ€§èƒ½æŸå¤±ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "11 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2508.14128v1",
      "published_date": "2025-08-19 04:17:21 UTC",
      "updated_date": "2025-08-19 04:17:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:53:16.815265+00:00"
    },
    {
      "arxiv_id": "2508.15835v1",
      "title": "Alvorada-Bench: Can Language Models Solve Brazilian University Entrance Exams?",
      "title_zh": "Alvorada-Benchï¼šè¯­è¨€æ¨¡å‹èƒ½å¦åº”å¯¹å·´è¥¿å¤§å­¦å…¥å­¦è€ƒè¯•ï¼Ÿ",
      "authors": [
        "Henrique Godoy"
      ],
      "abstract": "Language models are increasingly used in Brazil, but most evaluation remains English-centric. This paper presents Alvorada-Bench, a 4,515-question, text-only benchmark drawn from five Brazilian university entrance examinations. Evaluating twenty models under zero-shot, role-playing, and chain-of-thought prompting, producing 270,900 responses with structured self-reports of confidence, perceived difficulty, and Bloom level. The top models exceed 94% accuracy overall, but accuracy declines on Mathematics and on the engineering oriented IME and ITA exams, indicating persistent weaknesses in multi-step reasoning. Confidence is well calibrated and correlates with perceived difficulty, revealing that models can accurately assess their own certainty capabilities. A cost accuracy analysis shows that high accuracy is achievable at under $2 per 1K tokens. On ENEM 2024 the top model (O3) achieved perfect scores in Languages subject questions while even the weakest system (GPT-4.1 Nano) only underperforms humans in Mathematics. Through exams that distill decades of Brazilian educational priorities and assess millions of students yearly, Alvorada-Bench establishes whether language models can navigate the intersection of language, culture, and reasoning that defines academic readiness in Brazil.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Alvorada-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«4,515é“é¢˜ç›®çš„çº¯æ–‡æœ¬è¯„ä¼°åŸºå‡†ï¼Œæ¶µç›–äº†å·´è¥¿äº”é¡¹ä¸»è¦çš„å¤§å­¦å…¥å­¦è€ƒè¯•ï¼Œæ—¨åœ¨å¡«è¡¥å¤§è¯­è¨€æ¨¡å‹åœ¨éè‹±è¯­ç¯å¢ƒä¸‹è¯„ä¼°çš„ç©ºç™½ã€‚ç ”ç©¶é€šè¿‡zero-shotã€role-playingå’Œchain-of-thoughtç­‰æç¤ºç­–ç•¥å¯¹20ä¸ªæ¨¡å‹è¿›è¡Œäº†å¤§è§„æ¨¡è¯„ä¼°ï¼Œå¹¶è®°å½•äº†æ¨¡å‹å¯¹ç½®ä¿¡åº¦ã€éš¾åº¦æ„ŸçŸ¥å’ŒBloom levelçš„è‡ªæˆ‘æŠ¥å‘Šã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé¡¶çº§æ¨¡å‹åœ¨æ•´ä½“å‡†ç¡®ç‡ä¸Šè¶…è¿‡94%ï¼Œä½†åœ¨æ•°å­¦ç§‘ç›®åŠIMEã€ITAç­‰å·¥ç¨‹å¯¼å‘çš„è€ƒè¯•ä¸­è¡¨ç°æ˜¾è‘—ä¸‹é™ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨å¤šæ­¥æ¨ç†(multi-step reasoning)æ–¹é¢çš„æŒç»­ä¸è¶³ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°æ¨¡å‹çš„ç½®ä¿¡åº¦æ ¡å‡†è‰¯å¥½ï¼Œä¸”æˆæœ¬åˆ†æè¡¨æ˜é«˜å‡†ç¡®ç‡èƒ½å¤Ÿä»¥æä½çš„Tokenæˆæœ¬å®ç°ã€‚Alvorada-Benchç»“åˆäº†å·´è¥¿æ•°åå¹´çš„æ•™è‚²ä¼˜å…ˆçº§ï¼Œä¸ºè¡¡é‡æ¨¡å‹åœ¨è·¨è¯­è¨€ã€æ–‡åŒ–ä¸å¤æ‚æ¨ç†åœºæ™¯ä¸‹çš„å­¦æœ¯å°±ç»ªç¨‹åº¦å»ºç«‹äº†é‡è¦æ ‡å‡†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15835v1",
      "published_date": "2025-08-19 03:59:04 UTC",
      "updated_date": "2025-08-19 03:59:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:53:09.713938+00:00"
    },
    {
      "arxiv_id": "2508.13485v1",
      "title": "CORENet: Cross-Modal 4D Radar Denoising Network with LiDAR Supervision for Autonomous Driving",
      "title_zh": "CORENetï¼šé¢å‘è‡ªåŠ¨é©¾é©¶çš„åŸºäºæ¿€å…‰é›·è¾¾ç›‘ç£çš„è·¨æ¨¡æ€4Dæ¯«ç±³æ³¢é›·è¾¾å»å™ªç½‘ç»œ",
      "authors": [
        "Fuyang Liu",
        "Jilin Mei",
        "Fangyuan Mao",
        "Chen Min",
        "Yan Xing",
        "Yu Hu"
      ],
      "abstract": "4D radar-based object detection has garnered great attention for its robustness in adverse weather conditions and capacity to deliver rich spatial information across diverse driving scenarios. Nevertheless, the sparse and noisy nature of 4D radar point clouds poses substantial challenges for effective perception. To address the limitation, we present CORENet, a novel cross-modal denoising framework that leverages LiDAR supervision to identify noise patterns and extract discriminative features from raw 4D radar data. Designed as a plug-and-play architecture, our solution enables seamless integration into voxel-based detection frameworks without modifying existing pipelines. Notably, the proposed method only utilizes LiDAR data for cross-modal supervision during training while maintaining full radar-only operation during inference. Extensive evaluation on the challenging Dual-Radar dataset, which is characterized by elevated noise level, demonstrates the effectiveness of our framework in enhancing detection robustness. Comprehensive experiments validate that CORENet achieves superior performance compared to existing mainstream approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CORENetï¼Œä¸€ç§åˆ©ç”¨ LiDAR ç›‘ç£çš„è·¨æ¨¡æ€ 4D Radar å»å™ªç½‘ç»œï¼Œæ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­ 4D Radar ç‚¹äº‘ç¨€ç–ä¸”å¤šå™ªå£°çš„æ„ŸçŸ¥æŒ‘æˆ˜ã€‚ä½œä¸ºä¸€ç§å³æ’å³ç”¨çš„æ¶æ„ï¼Œè¯¥æ–¹æ¡ˆå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„åŸºäº Voxel çš„æ£€æµ‹æ¡†æ¶ä¸­ï¼Œè€Œæ— éœ€ä¿®æ”¹ç°æœ‰æµç¨‹ã€‚è¯¥æ–¹æ³•åœ¨è®­ç»ƒæœŸé—´åˆ©ç”¨ LiDAR æ•°æ®è¿›è¡Œè·¨æ¨¡æ€ç›‘ç£ï¼Œä»¥è¯†åˆ«å™ªå£°æ¨¡å¼å¹¶ä»åŸå§‹ 4D Radar æ•°æ®ä¸­æå–åˆ¤åˆ«æ€§ç‰¹å¾ï¼Œè€Œåœ¨æ¨ç†é˜¶æ®µåˆ™ä¿æŒå…¨ Radar è¿è¡Œã€‚åœ¨å™ªå£°æ°´å¹³è¾ƒé«˜çš„ Dual-Radar æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½æ˜¾è‘—å¢å¼ºæ£€æµ‹çš„é²æ£’æ€§ã€‚å®éªŒç»“æœéªŒè¯äº† CORENet ç›¸æ¯”äºç°æœ‰ä¸»æµæ–¹æ³•å…·æœ‰æ›´ä¼˜è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 5 figures, Accepted to IROS 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.13485v1",
      "published_date": "2025-08-19 03:30:21 UTC",
      "updated_date": "2025-08-19 03:30:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:53:21.248673+00:00"
    },
    {
      "arxiv_id": "2508.13470v1",
      "title": "STER-VLM: Spatio-Temporal With Enhanced Reference Vision-Language Models",
      "title_zh": "STER-VLMï¼šå¢å¼ºå‚è€ƒçš„æ—¶ç©ºè§†è§‰-è¯­è¨€æ¨¡å‹",
      "authors": [
        "Tinh-Anh Nguyen-Nhu",
        "Triet Dao Hoang Minh",
        "Dat To-Thanh",
        "Phuc Le-Gia",
        "Tuan Vo-Lan",
        "Tien-Huy Nguyen"
      ],
      "abstract": "Vision-language models (VLMs) have emerged as powerful tools for enabling automated traffic analysis; however, current approaches often demand substantial computational resources and struggle with fine-grained spatio-temporal understanding. This paper introduces STER-VLM, a computationally efficient framework that enhances VLM performance through (1) caption decomposition to tackle spatial and temporal information separately, (2) temporal frame selection with best-view filtering for sufficient temporal information, and (3) reference-driven understanding for capturing fine-grained motion and dynamic context and (4) curated visual/textual prompt techniques. Experimental results on the WTS \\cite{kong2024wts} and BDD \\cite{BDD} datasets demonstrate substantial gains in semantic richness and traffic scene interpretation. Our framework is validated through a decent test score of 55.655 in the AI City Challenge 2025 Track 2, showing its effectiveness in advancing resource-efficient and accurate traffic analysis for real-world applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† STER-VLMï¼Œè¿™æ˜¯ä¸€ä¸ªè®¡ç®—é«˜æ•ˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ Vision-Language Models (VLMs) åœ¨è‡ªåŠ¨äº¤é€šåˆ†æä¸­é¢ä¸´çš„é«˜è®¡ç®—èµ„æºéœ€æ±‚åŠç»†ç²’åº¦æ—¶ç©ºç†è§£ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ Caption decomposition åˆ†åˆ«å¤„ç†ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ï¼Œå¹¶ç»“åˆå¸¦æœ‰ Best-view filtering çš„ Temporal frame selection æŠ€æœ¯ä»¥ç¡®ä¿è·å–å……è¶³çš„æ—¶é—´ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒSTER-VLM å¼•å…¥äº† Reference-driven understanding ä»¥ç²¾å‡†æ•æ‰ç»†ç²’åº¦çš„åŠ¨ä½œä¸åŠ¨æ€ä¸Šä¸‹æ–‡ï¼Œå¹¶é‡‡ç”¨äº†ç²¾å¿ƒè®¾è®¡çš„è§†è§‰å’Œæ–‡æœ¬æç¤ºæŠ€æœ¯ã€‚åœ¨ WTS å’Œ BDD æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨è¯­ä¹‰ä¸°å¯Œåº¦å’Œäº¤é€šåœºæ™¯è§£é‡Šæ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ã€‚æœ€åï¼Œè¯¥æ¨¡å‹åœ¨ AI City Challenge 2025 Track 2 ä¸­è·å¾—äº† 55.655 çš„ä¼˜å¼‚æˆç»©ï¼ŒéªŒè¯äº†å…¶åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­å®ç°èµ„æºé«˜æ•ˆä¸”å‡†ç¡®çš„äº¤é€šåˆ†æçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV Workshop 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.13470v1",
      "published_date": "2025-08-19 03:03:29 UTC",
      "updated_date": "2025-08-19 03:03:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:53:16.114876+00:00"
    },
    {
      "arxiv_id": "2508.13465v1",
      "title": "LM Agents May Fail to Act on Their Own Risk Knowledge",
      "title_zh": "LM æ™ºèƒ½ä½“å¯èƒ½æ— æ³•åŸºäºå…¶æŒæ¡çš„é£é™©çŸ¥è¯†é‡‡å–è¡ŒåŠ¨",
      "authors": [
        "Yuzhi Tang",
        "Tianxiao Li",
        "Elizabeth Li",
        "Chris J. Maddison",
        "Honghua Dong",
        "Yangjun Ruan"
      ],
      "abstract": "Language model (LM) agents have demonstrated significant potential for automating real-world tasks, yet they pose a diverse array of potential, severe risks in safety-critical scenarios. In this work, we identify a significant gap between LM agents' risk awareness and safety execution abilities: while they often answer \"Yes\" to queries like \"Is executing `sudo rm -rf /*' dangerous?\", they will likely fail to identify such risks in instantiated trajectories or even directly perform these risky actions when acting as agents. To systematically investigate this, we develop a comprehensive evaluation framework to examine agents' safety across three progressive dimensions: 1) their knowledge about potential risks, 2) their ability to identify corresponding risks in execution trajectories, and 3) their actual behaviors to avoid executing these risky actions. Our evaluation reveals two critical performance gaps that resemble the generator-validator gaps observed in LMs: while agents demonstrate near-perfect risk knowledge ($>98\\%$ pass rates), they fail to apply this knowledge when identifying risks in actual scenarios (with performance dropping by $>23\\%$) and often still execute risky actions ($<26\\%$ pass rates). Notably, this trend persists across more capable LMs as well as in specialized reasoning models like DeepSeek-R1, indicating that simply scaling model capabilities or inference compute does not inherently resolve safety concerns. Instead, we take advantage of these observed gaps to develop a risk verifier that independently critiques the proposed actions by agents, with an abstractor that converts specific execution trajectories into abstract descriptions where LMs can more effectively identify the risks. Our overall system achieves a significant reduction of risky action execution by $55.3\\%$ over vanilla-prompted agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ (LM agents) åœ¨å®‰å…¨å…³é”®åœºæ™¯ä¸­é£é™©æ„ŸçŸ¥ä¸å®‰å…¨æ‰§è¡Œèƒ½åŠ›ä¹‹é—´çš„æ˜¾è‘—å·®è·ã€‚å®éªŒå‘ç°ï¼Œå°½ç®¡æ™ºèƒ½ä½“æ‹¥æœ‰è¿‘ä¹å®Œç¾çš„é£é™©çŸ¥è¯† (risk knowledge)ï¼Œä½†åœ¨å®é™…æ‰§è¡Œè½¨è¿¹ä¸­è¯†åˆ«é£é™©çš„èƒ½åŠ›æ˜æ˜¾ä¸è¶³ï¼Œä¸”å¾€å¾€ç›´æ¥æ‰§è¡Œå±é™©æ“ä½œã€‚å³ä½¿æ˜¯ DeepSeek-R1 ç­‰é«˜æ€§èƒ½æ¨ç†æ¨¡å‹ï¼Œä¹Ÿæ— æ³•ä»…é€šè¿‡æå‡æ¨¡å‹è§„æ¨¡æˆ–æ¨ç†è®¡ç®—æ¥æ¶ˆé™¤è¿™ç§å®‰å…¨æ€§è„±èŠ‚ã€‚ä¸ºæ­¤ï¼Œä½œè€…å»ºç«‹äº†ä¸€ä¸ªæ¶µç›–é£é™©çŸ¥è¯†ã€è½¨è¿¹è¯†åˆ«å’Œè¡Œä¸ºè¡¨ç°çš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŒ…å«é£é™©éªŒè¯å™¨ (risk verifier) å’ŒæŠ½è±¡å™¨ (abstractor) çš„ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé€šè¿‡å°†å…·ä½“æ‰§è¡Œè½¨è¿¹è½¬åŒ–ä¸ºæŠ½è±¡æè¿°ï¼Œå¸®åŠ©æ¨¡å‹æ›´æœ‰æ•ˆåœ°è¯†åˆ«æ½œåœ¨é£é™©ã€‚æœ€ç»ˆç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç›¸æ¯”åŸå§‹æç¤º (vanilla-prompted) çš„æ™ºèƒ½ä½“ï¼Œå°†å±é™©è¡Œä¸ºçš„æ‰§è¡Œç‡æ˜¾è‘—é™ä½äº† 55.3%ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13465v1",
      "published_date": "2025-08-19 02:46:08 UTC",
      "updated_date": "2025-08-19 02:46:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:53:15.419718+00:00"
    },
    {
      "arxiv_id": "2508.13440v1",
      "title": "Consumer Autonomy or Illusion? Rethinking Consumer Agency in the Age of Algorithms",
      "title_zh": "æ¶ˆè´¹è€…è‡ªä¸»è¿˜æ˜¯å¹»è±¡ï¼Ÿç®—æ³•æ—¶ä»£æ¶ˆè´¹è€…èƒ½åŠ¨æ€§çš„å†æ€è€ƒ",
      "authors": [
        "Pegah Nokhiz",
        "Aravinda Kanchana Ruwanpathirana"
      ],
      "abstract": "Consumer agency in the digital age is increasingly constrained by systemic barriers and algorithmic manipulation, raising concerns about the authenticity of consumption choices. Nowadays, financial decisions are shaped by external pressures like obligatory consumption, algorithmic persuasion, and unstable work schedules that erode financial autonomy. Obligatory consumption (like hidden fees) is intensified by digital ecosystems. Algorithmic tactics like personalized recommendations lead to impulsive purchases. Unstable work schedules also undermine financial planning. Thus, it is important to study how these factors impact consumption agency. To do so, we examine formal models grounded in discounted consumption with constraints that bound agency. We construct analytical scenarios in which consumers face obligatory payments, algorithm-influenced impulsive expenses, or unpredictable income due to temporal instability. Using this framework, we demonstrate that even rational, utility-maximizing agents can experience early financial ruin when agency is limited across structural, behavioral, or temporal dimensions and how diminished autonomy impacts long-term financial well-being. Our central argument is that consumer agency must be treated as a value (not a given) requiring active cultivation, especially in digital ecosystems. The connection between our formal modeling and this argument allows us to indicate that limitations on agency (whether structural, behavioral, or temporal) can be rigorously linked to measurable risks like financial instability. This connection is also a basis for normative claims about consumption as a value, by anchoring them in a formally grounded analysis of consumer behavior. As solutions, we study systemic interventions and consumer education to support value deliberation and informed choices. We formally demonstrate how these measures strengthen agency.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ç®—æ³•æ—¶ä»£ä¸‹æ¶ˆè´¹è€…ä»£ç†æƒ(Consumer Agency)å—åˆ°çš„ç³»ç»Ÿæ€§éšœç¢å’Œç®—æ³•æ“çºµï¼Œè´¨ç–‘äº†æ¶ˆè´¹é€‰æ‹©çš„çœŸå®æ€§ã€‚é€šè¿‡æ„å»ºåŸºäºæŠ˜ç°æ¶ˆè´¹(Discounted Consumption)çš„çº¦æŸæ€§å½¢å¼æ¨¡å‹ï¼Œç ”ç©¶åˆ†æäº†å¼ºåˆ¶æ€§æ¶ˆè´¹(Obligatory Consumption)ã€ç®—æ³•è¯±å¯¼çš„å†²åŠ¨è´­ä¹°ä»¥åŠå·¥ä½œæ—¶é—´ä¸ç¨³å®šæ€§å¯¹è´¢åŠ¡è‡ªä¸»æƒçš„å½±å“ã€‚å®éªŒè¯æ˜ï¼Œå³ä½¿æ˜¯è¿½æ±‚æ•ˆç”¨æœ€å¤§åŒ–çš„ç†æ€§æ™ºèƒ½ä½“ï¼Œåœ¨ç»“æ„ã€è¡Œä¸ºæˆ–æ—¶é—´ç»´åº¦å—é™æ—¶ï¼Œä¹Ÿå¯èƒ½é¢ä¸´æ—©æœŸçš„è´¢åŠ¡å´©æºƒã€‚æ–‡ç« å¼ºè°ƒï¼Œæ¶ˆè´¹è€…ä»£ç†æƒåº”è¢«è§†ä¸ºä¸€ç§éœ€è¦ä¸»åŠ¨åŸ¹è‚²çš„ä»·å€¼ï¼Œè€Œéç†æ‰€å½“ç„¶çš„æ—¢å®šäº‹å®ï¼Œå¹¶å°†å…¶ä¸è´¢åŠ¡ä¸ç¨³å®šæ€§ç­‰å¯è¡¡é‡çš„é£é™©æŒ‚é’©ã€‚æœ€åï¼Œç ”ç©¶é€šè¿‡å½¢å¼åŒ–æ¨¡å‹è®ºè¯äº†ç³»ç»Ÿæ€§å¹²é¢„å’Œæ¶ˆè´¹è€…æ•™è‚²åœ¨åŠ å¼ºä»£ç†æƒåŠæ”¯æŒçŸ¥æƒ…é€‰æ‹©æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted and appearing in Journal of Social Computing (JSC)",
      "pdf_url": "https://arxiv.org/pdf/2508.13440v1",
      "published_date": "2025-08-19 01:48:12 UTC",
      "updated_date": "2025-08-19 01:48:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:53:25.916601+00:00"
    },
    {
      "arxiv_id": "2508.13439v1",
      "title": "Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference",
      "title_zh": "é¢å‘äº¤é€šè§†é¢‘è§£æä¸é£é™©æ¨ç†çš„ç»“æ„åŒ–æç¤ºä¸å¤šæ™ºèƒ½ä½“çŸ¥è¯†è’¸é¦",
      "authors": [
        "Yunxiang Yang",
        "Ningning Xu",
        "Jidong J. Yang"
      ],
      "abstract": "Comprehensive highway scene understanding and robust traffic risk inference are vital for advancing Intelligent Transportation Systems (ITS) and autonomous driving. Traditional approaches often struggle with scalability and generalization, particularly under the complex and dynamic conditions of real-world environments. To address these challenges, we introduce a novel structured prompting and knowledge distillation framework that enables automatic generation of high-quality traffic scene annotations and contextual risk assessments. Our framework orchestrates two large Vision-Language Models (VLMs): GPT-4o and o3-mini, using a structured Chain-of-Thought (CoT) strategy to produce rich, multi-perspective outputs. These outputs serve as knowledge-enriched pseudo-annotations for supervised fine-tuning of a much smaller student VLM. The resulting compact 3B-scale model, named VISTA (Vision for Intelligent Scene and Traffic Analysis), is capable of understanding low-resolution traffic videos and generating semantically faithful, risk-aware captions. Despite its significantly reduced parameter count, VISTA achieves strong performance across established captioning metrics (BLEU-4, METEOR, ROUGE-L, and CIDEr) when benchmarked against its teacher models. This demonstrates that effective knowledge distillation and structured multi-agent supervision can empower lightweight VLMs to capture complex reasoning capabilities. The compact architecture of VISTA facilitates efficient deployment on edge devices, enabling real-time risk monitoring without requiring extensive infrastructure upgrades.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜é€Ÿå…¬è·¯åœºæ™¯ç†è§£å’Œäº¤é€šé£é™©æ¨ç†çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆç»“æ„åŒ–æç¤º(Structured Prompting)ä¸å¤šæ™ºèƒ½ä½“çŸ¥è¯†è’¸é¦(Multi-Agent Knowledge Distillation)çš„åˆ›æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ GPT-4o å’Œ o3-mini ä¸¤ç§å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)ï¼Œé€šè¿‡ç»“æ„åŒ–é“¾å¼æ€ç»´(Chain-of-Thought)ç­–ç•¥ç”Ÿæˆé«˜è´¨é‡çš„åœºæ™¯æ ‡æ³¨å’Œé£é™©è¯„ä¼°ï¼Œä½œä¸ºçŸ¥è¯†å¢å¼ºçš„ä¼ªæ ‡æ³¨æ•°æ®ã€‚ç ”ç©¶é€šè¿‡çŸ¥è¯†è’¸é¦è®­ç»ƒäº†ä¸€ä¸ªä»…æœ‰ 3B å‚æ•°é‡çš„è½»é‡åŒ–å­¦ç”Ÿæ¨¡å‹ VISTA (Vision for Intelligent Scene and Traffic Analysis)ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†ä½åˆ†è¾¨ç‡äº¤é€šè§†é¢‘å¹¶ç”Ÿæˆå…·å¤‡é£é™©æ„è¯†çš„è¯­ä¹‰æè¿°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVISTA åœ¨ BLEU-4ã€METEORã€ROUGE-L å’Œ CIDEr ç­‰è¯„æµ‹æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†ä¸æ•™å¸ˆæ¨¡å‹ç›¸è¿‘çš„æ€§èƒ½æ°´å¹³ã€‚è¿™ç§æ–¹æ³•è¯æ˜äº†é€šè¿‡å¤šæ™ºèƒ½ä½“ç›‘ç£å’Œæœ‰æ•ˆçš„çŸ¥è¯†è’¸é¦ï¼Œè½»é‡åŒ– VLM ä¹Ÿèƒ½æ•æ‰å¤æ‚çš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒVISTA çš„ç´§å‡‘æ¶æ„ä¾¿äºåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šé«˜æ•ˆéƒ¨ç½²ï¼Œä¸ºå®ç°å®æ—¶äº¤é€šé£é™©ç›‘æ§æä¾›äº†æ— éœ€å¤§è§„æ¨¡åŸºç¡€è®¾æ–½å‡çº§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 10 figures, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2508.13439v1",
      "published_date": "2025-08-19 01:44:02 UTC",
      "updated_date": "2025-08-19 01:44:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:53:40.223639+00:00"
    },
    {
      "arxiv_id": "2508.13437v1",
      "title": "Discrete Optimization of Min-Max Violation and its Applications Across Computational Sciences",
      "title_zh": "æå°æå¤§è¿åçš„ç¦»æ•£ä¼˜åŒ–åŠå…¶åœ¨è®¡ç®—ç§‘å­¦ä¸­çš„åº”ç”¨",
      "authors": [
        "Cheikh Ahmed",
        "Mahdi Mostajabdaveh",
        "Samin Aref",
        "Zirui Zhou"
      ],
      "abstract": "We introduce the Discrete Min-Max Violation (DMMV) as a general optimization problem which seeks an assignment of discrete values to variables that minimizes the largest constraint violation. This context-free mathematical formulation is applicable to a wide range of use cases that have worst-case performance requirements. After defining the DMMV problem mathematically, we explore its properties to establish a foundational understanding. To tackle DMMV instance sizes of practical relevance, we develop a GPU-accelerated heuristic that takes advantage of the mathematical properties of DMMV for speeding up the solution process. We demonstrate the versatile applicability of our heuristic by solving three optimization problems as use cases: (1) post-training quantization of language models, (2) discrete tomography, and (3) Finite Impulse Response (FIR) filter design. In quantization without outlier separation, our heuristic achieves 14% improvement on average over existing methods. In discrete tomography, it reduces reconstruction error by 16% under uniform noise and accelerates computations by a factor of 6 on GPU. For FIR filter design, it nearly achieves 50% ripple reduction compared to using the commercial integer optimization solver, Gurobi. Our comparative results point to the benefits of studying DMMV as a context-free optimization problem and the advantages that our proposed heuristic offers on three distinct problems. Our GPU-accelerated heuristic will be made open-source to further stimulate research on DMMV and its other applications. The code is available at https://anonymous.4open.science/r/AMVM-5F3E/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Discrete Min-Max Violation (DMMV) è¿™ä¸€é€šç”¨ä¼˜åŒ–é—®é¢˜ï¼Œæ—¨åœ¨é€šè¿‡ä¸ºå˜é‡åˆ†é…ç¦»æ•£å€¼æ¥æœ€å°åŒ–æœ€å¤§çº¦æŸè¿çº¦ï¼Œé€‚ç”¨äºå„ç§å…·æœ‰æœ€åæƒ…å†µæ€§èƒ½è¦æ±‚çš„åº”ç”¨åœºæ™¯ã€‚ä¸ºäº†å¤„ç†å…·æœ‰å®é™…ç›¸å…³æ€§çš„å®ä¾‹è§„æ¨¡ï¼Œä½œè€…å¼€å‘äº†ä¸€ç§åˆ©ç”¨ DMMV æ•°å­¦ç‰¹æ€§çš„ GPU-accelerated heuristic å¯å‘å¼ç®—æ³•ï¼Œæ˜¾è‘—æå‡äº†æ±‚è§£è¿‡ç¨‹ã€‚è¯¥ç®—æ³•åœ¨ Post-training quantizationã€Discrete tomography å’Œ Finite Impulse Response (FIR) filter design ä¸‰ä¸ªé¢†åŸŸçš„åº”ç”¨ä¸­å‡è¡¨ç°ä¼˜å¼‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ quantization ä»»åŠ¡ä¸­è¯¥æ–¹æ³•æ¯”ç°æœ‰æŠ€æœ¯å¹³å‡æå‡äº† 14%ï¼Œåœ¨ tomography ä»»åŠ¡ä¸­å‡å°‘äº† 16% çš„é‡å»ºè¯¯å·®å¹¶å®ç° 6 å€åŠ é€Ÿï¼Œè€Œåœ¨ FIR filter design ä¸­ç›¸æ¯”å•†ä¸šæ±‚è§£å™¨ Gurobi å®ç°äº†è¿‘ 50% çš„ ripple reductionã€‚ç ”ç©¶è¯æ˜äº†å°† DMMV ä½œä¸ºé€šç”¨çš„æ•°å­¦æ¡†æ¶è¿›è¡Œç ”ç©¶çš„ä»·å€¼ï¼Œå¹¶å±•ç¤ºäº†æ‰€æå¯å‘å¼ç®—æ³•åœ¨è§£å†³è·¨å­¦ç§‘ç¦»æ•£ä¼˜åŒ–é—®é¢˜ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13437v1",
      "published_date": "2025-08-19 01:34:05 UTC",
      "updated_date": "2025-08-19 01:34:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:54:12.229826+00:00"
    },
    {
      "arxiv_id": "2508.13436v1",
      "title": "Dynamic Design of Machine Learning Pipelines via Metalearning",
      "title_zh": "åŸºäºå…ƒå­¦ä¹ çš„æœºå™¨å­¦ä¹ æµæ°´çº¿åŠ¨æ€è®¾è®¡",
      "authors": [
        "Edesio AlcobaÃ§a",
        "AndrÃ© C. P. L. F. de Carvalho"
      ],
      "abstract": "Automated machine learning (AutoML) has democratized the design of machine learning based systems, by automating model selection, hyperparameter tuning and feature engineering. However, the high computational cost associated with traditional search and optimization strategies, such as Random Search, Particle Swarm Optimization and Bayesian Optimization, remains a significant challenge. Moreover, AutoML systems typically explore a large search space, which can lead to overfitting. This paper introduces a metalearning method for dynamically designing search spaces for AutoML system. The proposed method uses historical metaknowledge to select promising regions of the search space, accelerating the optimization process. According to experiments conducted for this study, the proposed method can reduce runtime by 89\\% in Random Search and search space by (1.8/13 preprocessor and 4.3/16 classifier), without compromising significant predictive performance. Moreover, the proposed method showed competitive performance when adapted to Auto-Sklearn, reducing its search space. Furthermore, this study encompasses insights into meta-feature selection, meta-model explainability, and the trade-offs inherent in search space reduction strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ (AutoML)åœ¨æ¨¡å‹é€‰æ‹©ä¸è¶…å‚æ•°ä¼˜åŒ–è¿‡ç¨‹ä¸­è®¡ç®—æˆæœ¬é«˜æ˜‚ä»¥åŠæ˜“å—è¿‡æ‹Ÿåˆå½±å“çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡å…ƒå­¦ä¹ (Metalearning)åŠ¨æ€è®¾è®¡æœç´¢ç©ºé—´çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å†å²å…ƒçŸ¥è¯†(Metaknowledge)é”å®šæœç´¢ç©ºé—´ä¸­çš„æ½œåŠ›åŒºåŸŸï¼Œæ—¨åœ¨åŠ é€Ÿä¼˜åŒ–è¿‡ç¨‹å¹¶æå‡æœç´¢æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨Random Searchä¸­å¯å‡å°‘89%çš„è¿è¡Œæ—¶é—´ï¼Œå¹¶åœ¨ä¸æŸå¤±æ˜¾è‘—é¢„æµ‹æ€§èƒ½çš„å‰æä¸‹å¤§å¹…ç¼©å‡é¢„å¤„ç†å™¨å’Œåˆ†ç±»å™¨çš„æœç´¢èŒƒå›´ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨é€‚é…Auto-Sklearnæ—¶ä¹Ÿè¡¨ç°å‡ºæå¼ºçš„ç«äº‰åŠ›ï¼Œæœ‰æ•ˆéªŒè¯äº†å…¶ç³»ç»Ÿé€‚ç”¨æ€§ã€‚ç ”ç©¶è¿˜æ·±å…¥æ¢è®¨äº†å…ƒç‰¹å¾é€‰æ‹©(Meta-feature selection)ã€å…ƒæ¨¡å‹å¯è§£é‡Šæ€§(Meta-model explainability)ä»¥åŠæœç´¢ç©ºé—´ç¼©å‡ç­–ç•¥ä¸­çš„å†…åœ¨æƒè¡¡ï¼Œä¸ºæ„å»ºé«˜æ•ˆä¸”é²æ£’çš„æœºå™¨å­¦ä¹ æµæ°´çº¿æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13436v1",
      "published_date": "2025-08-19 01:33:33 UTC",
      "updated_date": "2025-08-19 01:33:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:53:45.014892+00:00"
    },
    {
      "arxiv_id": "2508.13435v1",
      "title": "SVDformer: Direction-Aware Spectral Graph Embedding Learning via SVD and Transformer",
      "title_zh": "SVDformerï¼šåŸºäº SVD å’Œ Transformer çš„æ–¹å‘æ„ŸçŸ¥è°±å›¾åµŒå…¥å­¦ä¹ ",
      "authors": [
        "Jiayu Fang",
        "Zhiqi Shao",
        "S T Boris Choy",
        "Junbin Gao"
      ],
      "abstract": "Directed graphs are widely used to model asymmetric relationships in real-world systems. However, existing directed graph neural networks often struggle to jointly capture directional semantics and global structural patterns due to their isotropic aggregation mechanisms and localized filtering mechanisms. To address this limitation, this paper proposes SVDformer, a novel framework that synergizes SVD and Transformer architecture for direction-aware graph representation learning. SVDformer first refines singular value embeddings through multi-head self-attention, adaptively enhancing critical spectral components while suppressing high-frequency noise. This enables learnable low-pass/high-pass graph filtering without requiring spectral kernels. Furthermore, by treating singular vectors as directional projection bases and singular values as scaling factors, SVDformer uses the Transformer to model multi-scale interactions between incoming/outgoing edge patterns through attention weights, thereby explicitly preserving edge directionality during feature propagation. Extensive experiments on six directed graph benchmarks demonstrate that SVDformer consistently outperforms state-of-the-art GNNs and direction-aware baselines on node classification tasks, establishing a new paradigm for learning representations on directed graphs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SVDformerï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†SVDå’ŒTransformeræ¶æ„çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æœ‰å‘å›¾ç¥ç»ç½‘ç»œåœ¨æ•æ‰æ–¹å‘è¯­ä¹‰å’Œå…¨å±€ç»“æ„æ¨¡å¼æ–¹é¢çš„å±€é™æ€§ã€‚SVDformeré¦–å…ˆé€šè¿‡å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶(multi-head self-attention)ç»†åŒ–å¥‡å¼‚å€¼åµŒå…¥ï¼Œä»è€Œè‡ªé€‚åº”åœ°å¢å¼ºå…³é”®é¢‘è°±æˆåˆ†å¹¶æŠ‘åˆ¶é«˜é¢‘å™ªå£°ã€‚è¯¥æ¡†æ¶å°†å¥‡å¼‚å‘é‡è§†ä¸ºæ–¹å‘æŠ•å½±åŸºï¼Œå¹¶å°†å¥‡å¼‚å€¼è§†ä¸ºç¼©æ”¾å› å­ï¼Œåˆ©ç”¨Transformerå¯¹å…¥è¾¹å’Œå‡ºè¾¹æ¨¡å¼ä¹‹é—´çš„å¤šå°ºåº¦äº¤äº’è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œåœ¨ç‰¹å¾ä¼ æ’­è¿‡ç¨‹ä¸­æ˜¾å¼ä¿ç•™è¾¹çš„æ–¹å‘æ€§ã€‚åœ¨å…­ä¸ªæœ‰å‘å›¾åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSVDformeråœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸­æŒç»­ä¼˜äºæœ€å…ˆè¿›çš„GNNå’Œæœ‰å‘æ„ŸçŸ¥åŸºå‡†æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ä»…å®ç°äº†å¯å­¦ä¹ çš„ä½é€š/é«˜é€šå›¾æ»¤æ³¢ï¼Œè¿˜ä¸ºæœ‰å‘å›¾ä¸Šçš„è¡¨ç¤ºå­¦ä¹ å»ºç«‹äº†ä¸€ç§ç»“åˆè°±å›¾åµŒå…¥ä¸æ³¨æ„åŠ›æœºåˆ¶çš„æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13435v1",
      "published_date": "2025-08-19 01:32:18 UTC",
      "updated_date": "2025-08-19 01:32:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:53:49.629154+00:00"
    },
    {
      "arxiv_id": "2508.13434v1",
      "title": "EventTSF: Event-Aware Non-Stationary Time Series Forecasting",
      "title_zh": "EventTSFï¼šäº‹ä»¶æ„ŸçŸ¥éå¹³ç¨³æ—¶é—´åºåˆ—é¢„æµ‹",
      "authors": [
        "Yunfeng Ge",
        "Ming Jin",
        "Yiji Zhao",
        "Hongyan Li",
        "Bo Du",
        "Chang Xu",
        "Shirui Pan"
      ],
      "abstract": "Time series forecasting plays a vital role in critical domains like energy and transportation, where non-stationary dynamics are deeply intertwined with events in other modalities such as texts. However, incorporating natural language-based external events to improve non-stationary forecasting remains largely unexplored, as most approaches still rely on a single modality, resulting in limited contextual knowledge and model underperformance. Enabling fine-grained multimodal interactions between temporal and textual data is challenged by three fundamental issues: (1) the difficulty of fine-grained synchronization between time-varying discrete textual events and continuous time series; (2) the inherent temporal uncertainty introduced by textual semantics; and (3) the misalignment between textual event embeddings and multi-resolution temporal patterns. In this work, we address these challenges by introducing event-aware non-stationary time series forecasting (EventTSF), an autoregressive generation framework that integrates historical time series with textual events to make subsequent forecasts. Specifically, EventTSF uses autoregressive diffusion with flow matching at each step to capture nuanced temporal-event interactions. To handle event-induced uncertainty, flow matching timesteps are adaptively controlled according to event semantic signals. The underlying denoiser employs a multimodal U-shaped diffusion transformer that efficiently fuses temporal and textual modalities across different resolutions. Extensive experiments on 8 synthetic and real-world datasets show that EventTSF outperforms 12 baselines across diverse event-aware non-stationary time series forecasting scenarios, achieving substantial improvements of 10.7% higher forecasting accuracy and $1.13\\times$ faster training efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EventTSFï¼Œä¸€ç§æ—¨åœ¨è§£å†³éå¹³ç¨³ (non-stationary) æ—¶é—´åºåˆ—é¢„æµ‹ä¸­å¦‚ä½•æœ‰æ•ˆæ•´åˆæ–‡æœ¬äº‹ä»¶ä¿¡æ¯çš„è‡ªå›å½’ç”Ÿæˆæ¡†æ¶ã€‚ä¸ºäº†åº”å¯¹è¿ç»­æ—¶é—´åºåˆ—ä¸ç¦»æ•£æ–‡æœ¬äº‹ä»¶ä¹‹é—´çš„ç»†ç²’åº¦åŒæ­¥ã€è¯­ä¹‰ä¸ç¡®å®šæ€§åŠåµŒå…¥å¯¹é½ç­‰æŒ‘æˆ˜ï¼ŒEventTSF é‡‡ç”¨äº†ç»“åˆæµåŒ¹é… (flow matching) çš„è‡ªå›å½’æ‰©æ•£ (autoregressive diffusion) æŠ€æœ¯æ¥æ•æ‰å¤æ‚çš„æ—¶åº-äº‹ä»¶äº¤äº’ã€‚è¯¥æ¡†æ¶æ ¹æ®äº‹ä»¶è¯­ä¹‰ä¿¡å·è‡ªé€‚åº”åœ°æ§åˆ¶æµåŒ¹é…æ­¥æ•°ï¼Œå¹¶åˆ©ç”¨å¤šæ¨¡æ€ U å‹æ‰©æ•£å˜å‹å™¨ (U-shaped diffusion transformer) åœ¨ä¸åŒåˆ†è¾¨ç‡ä¸‹å®ç°æ—¶é—´ä¸æ–‡æœ¬æ¨¡æ€çš„æ·±åº¦èåˆã€‚åœ¨ 8 ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒEventTSF ç›¸æ¯” 12 ç§åŸºçº¿æ¨¡å‹åœ¨é¢„æµ‹å‡†ç¡®ç‡ä¸Šæå‡äº† 10.7%ï¼ŒåŒæ—¶è®­ç»ƒæ•ˆç‡æé«˜äº† 1.13 å€ã€‚è¯¥ç ”ç©¶ä¸ºæ•´åˆå¤–éƒ¨è¯­è¨€ä¿¡æ¯çš„éå¹³ç¨³æ—¶é—´åºåˆ—å»ºæ¨¡æä¾›äº†å…¨æ–°çš„å¤šæ¨¡æ€è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.13434v1",
      "published_date": "2025-08-19 01:28:47 UTC",
      "updated_date": "2025-08-19 01:28:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:53:55.615009+00:00"
    },
    {
      "arxiv_id": "2508.13433v1",
      "title": "STPFormer: A State-of-the-Art Pattern-Aware Spatio-Temporal Transformer for Traffic Forecasting",
      "title_zh": "STPFormerï¼šä¸€ç§ç”¨äºäº¤é€šé¢„æµ‹çš„å…ˆè¿›æ¨¡å¼æ„ŸçŸ¥æ—¶ç©º Transformer",
      "authors": [
        "Jiayu Fang",
        "Zhiqi Shao",
        "S T Boris Choy",
        "Junbin Gao"
      ],
      "abstract": "Spatio-temporal traffic forecasting is challenging due to complex temporal patterns, dynamic spatial structures, and diverse input formats. Although Transformer-based models offer strong global modeling, they often struggle with rigid temporal encoding and weak space-time fusion. We propose STPFormer, a Spatio-Temporal Pattern-Aware Transformer that achieves state-of-the-art performance via unified and interpretable representation learning. It integrates four modules: Temporal Position Aggregator (TPA) for pattern-aware temporal encoding, Spatial Sequence Aggregator (SSA) for sequential spatial learning, Spatial-Temporal Graph Matching (STGM) for cross-domain alignment, and an Attention Mixer for multi-scale fusion. Experiments on five real-world datasets show that STPFormer consistently sets new SOTA results, with ablation and visualizations confirming its effectiveness and generalizability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äº¤é€šé¢„æµ‹ä¸­å¤æ‚çš„ Temporal Patterns å’ŒåŠ¨æ€ç©ºé—´ç»“æ„å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† STPFormerï¼Œä¸€ç§å…·å¤‡æ¨¡å¼æ„ŸçŸ¥èƒ½åŠ›çš„æ—¶ç©º Transformer æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»Ÿä¸€ä¸”å¯è§£é‡Šçš„è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œé›†æˆäº† Temporal Position Aggregator (TPA) ä»¥å®ç°æ¨¡å¼æ„ŸçŸ¥çš„æ—¶é—´ç¼–ç ï¼Œå¹¶åˆ©ç”¨ Spatial Sequence Aggregator (SSA) è¿›è¡Œåºåˆ—ç©ºé—´å­¦ä¹ ã€‚æ­¤å¤–ï¼ŒSTPFormer å¼•å…¥äº† Spatial-Temporal Graph Matching (STGM) è¿›è¡Œè·¨åŸŸå¯¹é½ï¼Œä»¥åŠ Attention Mixer æ¨¡å—æ‰§è¡Œå¤šå°ºåº¦ç‰¹å¾èåˆã€‚åœ¨äº”ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒSTPFormer æŒç»­åˆ·æ–°äº† SOTA æ€§èƒ½è®°å½•ã€‚æ¶ˆèå®éªŒå’Œå¯è§†åŒ–ç»“æœè¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤„ç†å¤šæºå¼‚æ„æ—¶ç©ºæ•°æ®æ—¶çš„æœ‰æ•ˆæ€§ä¸æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå®ç°é«˜ç²¾åº¦çš„æ—¶ç©ºç‰¹å¾æ•è·æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13433v1",
      "published_date": "2025-08-19 01:23:38 UTC",
      "updated_date": "2025-08-19 01:23:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:53:56.412254+00:00"
    },
    {
      "arxiv_id": "2508.13429v1",
      "title": "AlphaX: An AI-Based Value Investing Strategy for the Brazilian Stock Market",
      "title_zh": "AlphaXï¼šåŸºäºäººå·¥æ™ºèƒ½çš„å·´è¥¿è‚¡å¸‚ä»·å€¼æŠ•èµ„ç­–ç•¥",
      "authors": [
        "Paulo AndrÃ© Lima de Castro"
      ],
      "abstract": "Autonomous trading strategies have been a subject of research within the field of artificial intelligence (AI) for aconsiderable period. Various AI techniques have been explored to develop autonomous agents capable of trading financial assets. These approaches encompass traditional methods such as neural networks, fuzzy logic, and reinforcement learning, as well as more recent advancements, including deep neural networks and deep reinforcement learning. Many developers report success in creating strategies that exhibit strong performance during simulations using historical price data, a process commonly referred to as backtesting. However, when these strategies are deployed in real markets, their performance often deteriorates, particularly in terms of risk-adjusted returns. In this study, we propose an AI-based strategy inspired by a classical investment paradigm: Value Investing. Financial AI models are highly susceptible to lookahead bias and other forms of bias that can significantly inflate performance in backtesting compared to live trading conditions. To address this issue, we conducted a series of computational simulations while controlling for these biases, thereby reducing the risk of overfitting. Our results indicate that the proposed approach outperforms major Brazilian market benchmarks. Moreover, the strategy, named AlphaX, demonstrated superior performance relative to widely used technical indicators such as the Relative Strength Index (RSI) and Money Flow Index (MFI), with statistically significant results. Finally, we discuss several open challenges and highlight emerging technologies in qualitative analysis that may contribute to the development of a comprehensive AI-based Value Investing framework in the future",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AlphaXï¼Œä¸€ç§ä¸“é—¨é’ˆå¯¹å·´è¥¿è‚¡ç¥¨å¸‚åœºçš„åŸºäºäººå·¥æ™ºèƒ½çš„ä»·å€¼æŠ•èµ„ï¼ˆValue Investingï¼‰ç­–ç•¥ã€‚é’ˆå¯¹é‡‘è AI æ¨¡å‹åœ¨å›æµ‹è¿‡ç¨‹ä¸­ææ˜“å‡ºç°çš„å…ˆéªŒåå·®ï¼ˆlookahead biasï¼‰åŠå…¶ä»–å½¢å¼çš„åè§ï¼Œç ”ç©¶å›¢é˜Ÿé€šè¿‡å—æ§çš„è®¡ç®—æ¨¡æ‹Ÿæ–¹æ³•ï¼Œæ˜¾è‘—é™ä½äº†è¿‡æ‹Ÿåˆï¼ˆoverfittingï¼‰é£é™©ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAlphaX çš„è¡¨ç°ä¼˜äºå·´è¥¿å¸‚åœºçš„ä¸»è¦åŸºå‡†ï¼Œä¸”ç›¸è¾ƒäºç›¸å¯¹å¼ºå¼±æŒ‡æ•°ï¼ˆRSIï¼‰å’Œèµ„é‡‘æµé‡æŒ‡æ•°ï¼ˆMFIï¼‰ç­‰å¸¸ç”¨æŠ€æœ¯æŒ‡æ ‡ï¼Œå±•ç°å‡ºäº†å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§çš„ä¼˜åŠ¿ã€‚è¯¥è®ºæ–‡æœ€åè¿˜æ¢è®¨äº†å®šæ€§åˆ†æåœ¨æ„å»ºå…¨é¢ AI ä»·å€¼æŠ•èµ„æ¡†æ¶ä¸­çš„æ½œåŠ›ï¼Œä¸ºè§£å†³è‡ªåŠ¨äº¤æ˜“ç­–ç•¥åœ¨çœŸå®å¸‚åœºä¸­æ€§èƒ½é€€åŒ–çš„é—®é¢˜æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "q-fin.CP",
        "cs.AI"
      ],
      "primary_category": "q-fin.CP",
      "comment": "6 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.13429v1",
      "published_date": "2025-08-19 01:04:38 UTC",
      "updated_date": "2025-08-19 01:04:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:53:55.227828+00:00"
    },
    {
      "arxiv_id": "2508.13428v1",
      "title": "Mitigating Easy Option Bias in Multiple-Choice Question Answering",
      "title_zh": "ç¼“è§£å¤šé¡¹é€‰æ‹©é—®ç­”ä¸­çš„æ˜“é€‰é¡¹åè§",
      "authors": [
        "Hao Zhang",
        "Chen Li",
        "Basura Fernando"
      ],
      "abstract": "In this early study, we observe an Easy-Options Bias (EOB) issue in some multiple-choice Visual Question Answering (VQA) benchmarks such as MMStar, RealWorldQA, SEED-Bench, Next-QA, STAR benchmark and Video-MME. This bias allows vision-language models (VLMs) to select the correct answer using only the vision (V) and options (O) as inputs, without the need for the question (Q). Through grounding experiments, we attribute the bias to an imbalance in visual relevance: the correct answer typically aligns more closely with the visual contents than the negative options in feature space, creating a shortcut for VLMs to infer the answer via simply vision-option similarity matching. To fix this, we introduce GroundAttack, a toolkit that automatically generates hard negative options as visually plausible as the correct answer. We apply it to the NExT-QA and MMStar datasets, creating new EOB-free annotations. On these EOB-free annotations, current VLMs approach to random accuracies under (V+O) settings, and drop to non-saturated accuracies under (V+Q+O) settings, providing a more realistic evaluation of VLMs' QA ability. Codes and new annotations will be released soon.",
      "tldr_zh": "è¯¥ç ”ç©¶åœ¨ MMStar å’Œ RealWorldQA ç­‰å¤šä¸ªå¤šé¡¹é€‰æ‹©è§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•ä¸­å‘ç°äº†ä¸€ç§åä¸ºâ€œæ˜“é€‰é¡¹åå·®â€ï¼ˆEasy-Options Bias, EOBï¼‰çš„é—®é¢˜ã€‚åœ¨è¿™ç§åå·®ä¸‹ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä»…å‡­å›¾åƒï¼ˆVisionï¼‰å’Œé€‰é¡¹ï¼ˆOptionsï¼‰å³å¯æ¨æ–­å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œè€Œæ— éœ€ç†è§£é—®é¢˜ï¼ˆQuestionï¼‰ã€‚é€šè¿‡å®éªŒåˆ†æï¼Œä½œè€…å°†å…¶å½’å› äºè§†è§‰ç›¸å…³æ€§çš„ä¸å¹³è¡¡ï¼Œå³æ­£ç¡®ç­”æ¡ˆåœ¨ç‰¹å¾ç©ºé—´ä¸­å¾€å¾€æ¯”è´Ÿé¢é€‰é¡¹æ›´æ¥è¿‘è§†è§‰å†…å®¹ï¼Œå½¢æˆäº†è§†è§‰-é€‰é¡¹ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆvision-option similarity matchingï¼‰çš„å¿«æ·æ–¹å¼ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å¼•å…¥äº† GroundAttack å·¥å…·åŒ…ï¼Œæ—¨åœ¨è‡ªåŠ¨ç”Ÿæˆä¸æ­£ç¡®ç­”æ¡ˆåœ¨è§†è§‰ä¸Šå…·æœ‰åŒç­‰è¯´æœåŠ›çš„ç¡¬è´Ÿæ ·æœ¬ï¼ˆhard negative optionsï¼‰ã€‚è¯¥ç ”ç©¶åœ¨ NExT-QA å’Œ MMStar æ•°æ®é›†ä¸Šåˆ›å»ºäº†å…¨æ–°çš„æ—  EOB æ ‡æ³¨ï¼Œå®éªŒè¯æ˜åœ¨æ¶ˆé™¤åå·®åï¼ŒVLMs åœ¨ä»…æä¾›å›¾åƒå’Œé€‰é¡¹çš„æƒ…å†µä¸‹çš„è¡¨ç°æ¥è¿‘éšæœºï¼Œä»è€Œæä¾›äº†å¯¹ VLMs é—®ç­”èƒ½åŠ›æ›´çœŸå®ä¸”éé¥±å’Œçš„è¯„ä¼°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2508.13428v1",
      "published_date": "2025-08-19 01:03:45 UTC",
      "updated_date": "2025-08-19 01:03:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:54:02.920767+00:00"
    },
    {
      "arxiv_id": "2508.13426v2",
      "title": "ALIGN: Word Association Learning for Cultural Alignment in Large Language Models",
      "title_zh": "ALIGNï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹æ–‡åŒ–å¯¹é½çš„è¯æ±‡å…³è”å­¦ä¹ ",
      "authors": [
        "Chunhua Liu",
        "Kabir Manandhar Shrestha",
        "Sukai Huang"
      ],
      "abstract": "Large language models (LLMs) exhibit cultural bias from overrepresented viewpoints in training data, yet cultural alignment remains a challenge due to limited cultural knowledge and a lack of exploration into effective learning approaches. We introduce a cost-efficient and cognitively grounded method: fine-tuning LLMs on native speakers' word-association norms, leveraging cognitive psychology findings that such associations capture cultural knowledge. Using word association datasets from native speakers in the US (English) and China (Mandarin), we train Llama-3.1-8B and Qwen-2.5-7B via supervised fine-tuning and preference optimization. We evaluate models' cultural alignment through a two-tier evaluation framework that spans lexical associations and cultural value alignment using the World Values Survey. Results show significant improvements in lexical alignment (16-20% English, 43-165% Mandarin on Precision@5) and high-level cultural value shifts. On a subset of 50 questions where US and Chinese respondents diverge most, fine-tuned Qwen nearly doubles its response alignment with Chinese values (13 to 25). Remarkably, our trained 7-8B models match or exceed vanilla 70B baselines, demonstrating that a few million of culture-grounded associations achieve value alignment without expensive retraining. Our work highlights both the promise and the need for future research grounded in human cognition in improving cultural alignment in AI models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ALIGNï¼Œä¸€ç§åŸºäºè®¤çŸ¥å¿ƒç†å­¦è¯æ±‡å…³è”è§„èŒƒ (word-association norms) çš„å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸­çš„æ–‡åŒ–åè§å’Œæ–‡åŒ–å¯¹é½ (cultural alignment) æŒ‘æˆ˜ã€‚ç ”ç©¶è€…åˆ©ç”¨ç¾å›½å’Œä¸­å›½çš„æ¯è¯­è€…è¯æ±‡å…³è”æ•°æ®é›†ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒ (SFT) å’Œåå¥½ä¼˜åŒ– (PO) å¯¹ Llama-3.1-8B å’Œ Qwen-2.5-7B æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨è¯æ±‡å±‚é¢çš„å¯¹é½ä»¥åŠåŸºäºä¸–ç•Œä»·å€¼è§‚è°ƒæŸ¥ (World Values Survey) çš„é«˜å±‚æ–‡åŒ–ä»·å€¼å¯¹é½ã€‚åœ¨ç¾ä¸­ä»·å€¼è§‚å·®å¼‚æ˜¾è‘—çš„é—®é¢˜å­é›†ä¸Šï¼Œå¾®è°ƒåçš„ Qwen æ¨¡å‹å¯¹ä¸­å›½ä»·å€¼è§‚çš„å¯¹é½åº¦æå‡äº†è¿‘ä¸€å€ã€‚ä»¤äººå…³æ³¨çš„æ˜¯ï¼Œä»…æœ‰ 7-8B å‚æ•°çš„å°è§„æ¨¡æ¨¡å‹åœ¨å¯¹é½è¡¨ç°ä¸Šä¾¿å¯åª²ç¾ç”šè‡³è¶…è¶Š 70B çš„åŸºçº¿æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œé€šè¿‡å°‘é‡åŸºäºäººç±»è®¤çŸ¥çš„è¯æ±‡å…³è”æ•°æ®ï¼Œå³å¯åœ¨æ— éœ€å¤§è§„æ¨¡é‡è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°æ˜¾è‘—çš„æ–‡åŒ–ä»·å€¼è§‚å¯¹é½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13426v2",
      "published_date": "2025-08-19 00:55:20 UTC",
      "updated_date": "2025-12-15 06:22:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:54:21.237368+00:00"
    },
    {
      "arxiv_id": "2508.13423v2",
      "title": "AdaptJobRec: Enhancing Conversational Career Recommendation through an LLM-Powered Agentic System",
      "title_zh": "AdaptJobRecï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ™ºèƒ½ä½“ç³»ç»Ÿå¢å¼ºå¯¹è¯å¼èŒä¸šæ¨è",
      "authors": [
        "Qixin Wang",
        "Dawei Wang",
        "Kun Chen",
        "Yaowei Hu",
        "Puneet Girdhar",
        "Ruoteng Wang",
        "Aadesh Gupta",
        "Chaitanya Devella",
        "Wenlai Guo",
        "Shangwen Huang",
        "Bachir Aoun",
        "Greg Hayworth",
        "Han Li",
        "Xintao Wu"
      ],
      "abstract": "In recent years, recommendation systems have evolved from providing a single list of recommendations to offering a comprehensive suite of topic focused services. To better accomplish this task, conversational recommendation systems (CRS) have progressed from basic retrieval augmented LLM generation to agentic systems with advanced reasoning and self correction capabilities. However, agentic systems come with notable response latency, a longstanding challenge for conversational recommendation systems. To balance the trade off between handling complex queries and minimizing latency, we propose AdaptJobRec, the first conversational job recommendation system that leverages autonomous agent to integrate personalized recommendation algorithm tools. The system employs a user query complexity identification mechanism to minimize response latency. For straightforward queries, the agent directly selects the appropriate tool for rapid responses. For complex queries, the agent uses the memory processing module to filter chat history for relevant content, then passes the results to the intelligent task decomposition planner, and finally executes the tasks using personalized recommendation tools. Evaluation on Walmart's real world career recommendation scenarios demonstrates that AdaptJobRec reduces average response latency by up to 53.3% compared to competitive baselines, while significantly improving recommendation accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AdaptJobRecï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåˆ©ç”¨è‡ªä¸»æ™ºèƒ½ä½“ (autonomous agent) é›†æˆä¸ªæ€§åŒ–æ¨èç®—æ³•å·¥å…·çš„å¯¹è¯å¼èŒä½æ¨èç³»ç»Ÿ (conversational job recommendation system)ã€‚ä¸ºäº†è§£å†³æ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤„ç†å¯¹è¯æ¨èæ—¶æ™®éå­˜åœ¨çš„å“åº”å»¶è¿Ÿ (response latency) æŒ‘æˆ˜ï¼Œè¯¥ç³»ç»Ÿå¼•å…¥äº†ç”¨æˆ·æŸ¥è¯¢å¤æ‚åº¦è¯†åˆ«æœºåˆ¶ (user query complexity identification mechanism) ä»¥å¹³è¡¡å¤æ‚æŸ¥è¯¢å¤„ç†ä¸å“åº”é€Ÿåº¦ã€‚å¯¹äºç®€å•æŸ¥è¯¢ï¼Œæ™ºèƒ½ä½“ç›´æ¥é€‰æ‹©åˆé€‚çš„å·¥å…·è¿›è¡Œå¿«é€Ÿå“åº”ï¼›è€Œé¢å¯¹å¤æ‚æŸ¥è¯¢ï¼Œç³»ç»Ÿåˆ™é€šè¿‡è®°å¿†å¤„ç†æ¨¡å— (memory processing module) è¿‡æ»¤ç›¸å…³å†å²ä¿¡æ¯ï¼Œå¹¶ç»“åˆæ™ºèƒ½ä»»åŠ¡åˆ†è§£è§„åˆ’å™¨ (intelligent task decomposition planner) è°ƒç”¨ä¸ªæ€§åŒ–å·¥å…·æ‰§è¡Œã€‚åœ¨æ²ƒå°”ç› (Walmart) çœŸå®èŒä¸šæ¨èåœºæ™¯ä¸‹çš„è¯„ä¼°è¡¨æ˜ï¼ŒAdaptJobRec ç›¸æ¯”ç«äº‰åŸºçº¿æ¨¡å‹å°†å¹³å‡å“åº”å»¶è¿Ÿé™ä½äº†å¤šè¾¾ 53.3%ã€‚åŒæ—¶ï¼Œè¯¥ç³»ç»Ÿåœ¨æ˜¾è‘—æå‡æ¨èå‡†ç¡®ç‡ (recommendation accuracy) æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œä¸ºæ„å»ºé«˜æ•ˆä¸”ç²¾å‡†çš„æ™ºèƒ½ä½“é©±åŠ¨æ¨èç³»ç»Ÿæä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13423v2",
      "published_date": "2025-08-19 00:44:25 UTC",
      "updated_date": "2025-10-14 07:12:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:54:10.823842+00:00"
    },
    {
      "arxiv_id": "2508.13421v1",
      "title": "Virtuous Machines: Towards Artificial General Science",
      "title_zh": "Virtuous Machinesï¼šè¿ˆå‘äººå·¥é€šç”¨ç§‘å­¦",
      "authors": [
        "Gabrielle Wehr",
        "Reuben Rideaux",
        "Amaya J. Fox",
        "David R. Lightfoot",
        "Jason Tangen",
        "Jason B. Mattingley",
        "Shane E. Ehrhardt"
      ],
      "abstract": "Artificial intelligence systems are transforming scientific discovery by accelerating specific research tasks, from protein structure prediction to materials design, yet remain confined to narrow domains requiring substantial human oversight. The exponential growth of scientific literature and increasing domain specialisation constrain researchers' capacity to synthesise knowledge across disciplines and develop unifying theories, motivating exploration of more general-purpose AI systems for science. Here we show that a domain-agnostic, agentic AI system can independently navigate the scientific workflow - from hypothesis generation through data collection to manuscript preparation. The system autonomously designed and executed three psychological studies on visual working memory, mental rotation, and imagery vividness, executed one new online data collection with 288 participants, developed analysis pipelines through 8-hour+ continuous coding sessions, and produced completed manuscripts. The results demonstrate the capability of AI scientific discovery pipelines to conduct non-trivial research with theoretical reasoning and methodological rigour comparable to experienced researchers, though with limitations in conceptual nuance and theoretical interpretation. This is a step toward embodied AI that can test hypotheses through real-world experiments, accelerating discovery by autonomously exploring regions of scientific space that human cognitive and resource constraints might otherwise leave unexplored. It raises important questions about the nature of scientific understanding and the attribution of scientific credit.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é¢†åŸŸé€šç”¨ï¼ˆdomain-agnosticï¼‰ä¸”å…·å¤‡ä»£ç†æ€§ï¼ˆagenticï¼‰çš„AIç³»ç»Ÿï¼Œæ—¨åœ¨æ¨åŠ¨äººå·¥æ™ºèƒ½ä»æ‰§è¡Œç‰¹å®šç ”ç©¶ä»»åŠ¡è½¬å‘å®ç°â€œé€šç”¨äººå·¥æ™ºèƒ½ç§‘å­¦â€ï¼ˆArtificial General Scienceï¼‰ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿç‹¬ç«‹å¯¼èˆªä»å‡è®¾ç”Ÿæˆã€æ•°æ®æ”¶é›†åˆ°æ‰‹ç¨¿å‡†å¤‡çš„å®Œæ•´ç§‘å­¦å·¥ä½œæµï¼Œå¹¶åœ¨å¿ƒç†å­¦é¢†åŸŸæˆåŠŸè®¾è®¡å¹¶æ‰§è¡Œäº†å…³äºè§†è§‰å·¥ä½œè®°å¿†ï¼ˆvisual working memoryï¼‰ã€å¿ƒç†æ—‹è½¬ï¼ˆmental rotationï¼‰å’Œæƒ³è±¡ç”ŸåŠ¨æ€§ï¼ˆimagery vividnessï¼‰çš„ä¸‰é¡¹ç ”ç©¶ã€‚å®éªŒä¸­ï¼Œç³»ç»Ÿè‡ªä¸»å®Œæˆäº†æ¶‰åŠ288åå‚ä¸è€…çš„åœ¨çº¿æ•°æ®æ”¶é›†ï¼Œå¹¶é€šè¿‡è¶…è¿‡8å°æ—¶çš„è¿ç»­ç¼–ç¨‹æ„å»ºäº†åˆ†ææµæ°´çº¿å¹¶äº§å‡ºäº†å­¦æœ¯è®ºæ–‡ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨ç†è®ºæ¨ç†å’Œæ–¹æ³•è®ºä¸¥è°¨æ€§ä¸Šå±•ç°å‡ºä¸èµ„æ·±ç ”ç©¶è€…ç›¸å½“çš„èƒ½åŠ›ï¼Œä½†åœ¨æ¦‚å¿µç»†å¾®å·®åˆ«å’Œç†è®ºè§£é‡Šæ·±åº¦ä¸Šä»å­˜åœ¨å±€é™ã€‚è¿™é¡¹å·¥ä½œæ ‡å¿—ç€å‘èƒ½å¤Ÿé€šè¿‡ç°å®ä¸–ç•Œå®éªŒæµ‹è¯•å‡è®¾çš„å…·èº«AIï¼ˆembodied AIï¼‰è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œæœ‰æœ›é€šè¿‡è‡ªä¸»æ¢ç´¢çªç ´äººç±»è®¤çŸ¥å’Œèµ„æºé™åˆ¶çš„ç§‘å­¦ç©ºé—´ã€‚",
      "categories": [
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13421v1",
      "published_date": "2025-08-19 00:35:56 UTC",
      "updated_date": "2025-08-19 00:35:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:54:40.218582+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 141,
  "processed_papers_count": 141,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T12:55:38.754658+00:00"
}
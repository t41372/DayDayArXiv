[
  {
    "arxiv_id": "2407.09719v1",
    "title": "MSEval: A Dataset for Material Selection in Conceptual Design to Evaluate Algorithmic Models",
    "authors": [
      "Yash Patawari Jain",
      "Daniele Grandi",
      "Allin Groom",
      "Brandon Cramer",
      "Christopher McComb"
    ],
    "abstract": "Material selection plays a pivotal role in many industries, from\nmanufacturing to construction. Material selection is usually carried out after\nseveral cycles of conceptual design, during which designers iteratively refine\nthe design solution and the intended manufacturing approach. In design\nresearch, material selection is typically treated as an optimization problem\nwith a single correct answer. Moreover, it is also often restricted to specific\ntypes of objects or design functions, which can make the selection process\ncomputationally expensive and time-consuming. In this paper, we introduce\nMSEval, a novel dataset which is comprised of expert material evaluations\nacross a variety of design briefs and criteria. This data is designed to serve\nas a benchmark to facilitate the evaluation and modification of machine\nlearning models in the context of material selection for conceptual design.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2405.03695",
    "pdf_url": "http://arxiv.org/pdf/2407.09719v1",
    "published_date": "2024-07-12 23:27:33 UTC",
    "updated_date": "2024-07-12 23:27:33 UTC"
  },
  {
    "arxiv_id": "2407.11072v1",
    "title": "MaPPing Your Model: Assessing the Impact of Adversarial Attacks on LLM-based Programming Assistants",
    "authors": [
      "John Heibel",
      "Daniel Lowd"
    ],
    "abstract": "LLM-based programming assistants offer the promise of programming faster but\nwith the risk of introducing more security vulnerabilities. Prior work has\nstudied how LLMs could be maliciously fine-tuned to suggest vulnerabilities\nmore often. With the rise of agentic LLMs, which may use results from an\nuntrusted third party, there is a growing risk of attacks on the model's\nprompt. We introduce the Malicious Programming Prompt (MaPP) attack, in which\nan attacker adds a small amount of text to a prompt for a programming task\n(under 500 bytes). We show that our prompt strategy can cause an LLM to add\nvulnerabilities while continuing to write otherwise correct code. We evaluate\nthree prompts on seven common LLMs, from basic to state-of-the-art commercial\nmodels. Using the HumanEval benchmark, we find that our prompts are broadly\neffective, with no customization required for different LLMs. Furthermore, the\nLLMs that are best at HumanEval are also best at following our malicious\ninstructions, suggesting that simply scaling language models will not prevent\nMaPP attacks. Using a dataset of eight CWEs in 16 scenarios, we find that MaPP\nattacks are also effective at implementing specific and targeted\nvulnerabilities across a range of models. Our work highlights the need to\nsecure LLM prompts against manipulation as well as rigorously auditing code\ngenerated with the help of LLMs.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "I.2.2"
    ],
    "primary_category": "cs.CR",
    "comment": "6 pages, 5 figures, Proceedings of the ICML 2024 Workshop on\n  Trustworthy Multimodal Foundation Models and AI Agents",
    "pdf_url": "http://arxiv.org/pdf/2407.11072v1",
    "published_date": "2024-07-12 22:30:35 UTC",
    "updated_date": "2024-07-12 22:30:35 UTC"
  },
  {
    "arxiv_id": "2407.09705v1",
    "title": "Diagnosing and Re-learning for Balanced Multimodal Learning",
    "authors": [
      "Yake Wei",
      "Siwei Li",
      "Ruoxuan Feng",
      "Di Hu"
    ],
    "abstract": "To overcome the imbalanced multimodal learning problem, where models prefer\nthe training of specific modalities, existing methods propose to control the\ntraining of uni-modal encoders from different perspectives, taking the\ninter-modal performance discrepancy as the basis. However, the intrinsic\nlimitation of modality capacity is ignored. The scarcely informative modalities\ncan be recognized as ``worse-learnt'' ones, which could force the model to\nmemorize more noise, counterproductively affecting the multimodal model\nability. Moreover, the current modality modulation methods narrowly concentrate\non selected worse-learnt modalities, even suppressing the training of others.\nHence, it is essential to consider the intrinsic limitation of modality\ncapacity and take all modalities into account during balancing. To this end, we\npropose the Diagnosing \\& Re-learning method. The learning state of each\nmodality is firstly estimated based on the separability of its uni-modal\nrepresentation space, and then used to softly re-initialize the corresponding\nuni-modal encoder. In this way, the over-emphasizing of scarcely informative\nmodalities is avoided. In addition, encoders of worse-learnt modalities are\nenhanced, simultaneously avoiding the over-training of other modalities.\nAccordingly, multimodal learning is effectively balanced and enhanced.\nExperiments covering multiple types of modalities and multimodal frameworks\ndemonstrate the superior performance of our simple-yet-effective method for\nbalanced multimodal learning. The source code and dataset are available at\n\\url{https://github.com/GeWu-Lab/Diagnosing_Relearning_ECCV2024}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.09705v1",
    "published_date": "2024-07-12 22:12:03 UTC",
    "updated_date": "2024-07-12 22:12:03 UTC"
  },
  {
    "arxiv_id": "2407.09702v2",
    "title": "Investigating the Interplay of Prioritized Replay and Generalization",
    "authors": [
      "Parham Mohammad Panahi",
      "Andrew Patterson",
      "Martha White",
      "Adam White"
    ],
    "abstract": "Experience replay, the reuse of past data to improve sample efficiency, is\nubiquitous in reinforcement learning. Though a variety of smart sampling\nschemes have been introduced to improve performance, uniform sampling by far\nremains the most common approach. One exception is Prioritized Experience\nReplay (PER), where sampling is done proportionally to TD errors, inspired by\nthe success of prioritized sweeping in dynamic programming. The original work\non PER showed improvements in Atari, but follow-up results were mixed. In this\npaper, we investigate several variations on PER, to attempt to understand where\nand when PER may be useful. Our findings in prediction tasks reveal that while\nPER can improve value propagation in tabular settings, behavior is\nsignificantly different when combined with neural networks. Certain mitigations\n$-$ like delaying target network updates to control generalization and using\nestimates of expected TD errors in PER to avoid chasing stochasticity $-$ can\navoid large spikes in error with PER and neural networks but generally do not\noutperform uniform replay. In control tasks, none of the prioritized variants\nconsistently outperform uniform replay. We present new insight into the\ninteraction between prioritization, bootstrapping, and neural networks and\npropose several improvements for PER in tabular settings and noisy domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in the Reinforcement Learning Conference 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.09702v2",
    "published_date": "2024-07-12 21:56:24 UTC",
    "updated_date": "2024-10-19 17:51:10 UTC"
  },
  {
    "arxiv_id": "2407.09693v1",
    "title": "A Mathematical Framework, a Taxonomy of Modeling Paradigms, and a Suite of Learning Techniques for Neural-Symbolic Systems",
    "authors": [
      "Charles Dickens",
      "Connor Pryor",
      "Changyu Gao",
      "Alon Albalak",
      "Eriq Augustine",
      "William Wang",
      "Stephen Wright",
      "Lise Getoor"
    ],
    "abstract": "The field of Neural-Symbolic (NeSy) systems is growing rapidly. Proposed\napproaches show great promise in achieving symbiotic unions of neural and\nsymbolic methods. However, each NeSy system differs in fundamental ways. There\nis a pressing need for a unifying theory to illuminate the commonalities and\ndifferences in approaches and enable further progress. In this paper, we\nintroduce Neural-Symbolic Energy-Based Models (NeSy-EBMs), a unifying\nmathematical framework for discriminative and generative modeling with\nprobabilistic and non-probabilistic NeSy approaches. We utilize NeSy-EBMs to\ndevelop a taxonomy of modeling paradigms focusing on a system's neural-symbolic\ninterface and reasoning capabilities. Additionally, we introduce a suite of\nlearning techniques for NeSy-EBMs. Importantly, NeSy-EBMs allow the derivation\nof general expressions for gradients of prominent learning losses, and we\nprovide four learning approaches that leverage methods from multiple domains,\nincluding bilevel and stochastic policy optimization. Finally, we present\nNeural Probabilistic Soft Logic (NeuPSL), an open-source NeSy-EBM library\ndesigned for scalability and expressivity, facilitating real-world application\nof NeSy systems. Through extensive empirical analysis across multiple datasets,\nwe demonstrate the practical advantages of NeSy-EBMs in various tasks,\nincluding image classification, graph node labeling, autonomous vehicle\nsituation awareness, and question answering.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09693v1",
    "published_date": "2024-07-12 21:26:21 UTC",
    "updated_date": "2024-07-12 21:26:21 UTC"
  },
  {
    "arxiv_id": "2407.09685v2",
    "title": "Accelerating the inference of string generation-based chemical reaction models for industrial applications",
    "authors": [
      "Mikhail Andronov",
      "Natalia Andronova",
      "Michael Wand",
      "Jürgen Schmidhuber",
      "Djork-Arné Clevert"
    ],
    "abstract": "Template-free SMILES-to-SMILES translation models for reaction prediction and\nsingle-step retrosynthesis are of interest for industrial applications in\ncomputer-aided synthesis planning systems due to their state-of-the-art\naccuracy. However, they suffer from slow inference speed. We present a method\nto accelerate inference in autoregressive SMILES generators through speculative\ndecoding by copying query string subsequences into target strings in the right\nplaces. We apply our method to the molecular transformer implemented in Pytorch\nLightning and achieve over 3X faster inference in reaction prediction and\nsingle-step retrosynthesis, with no loss in accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.09685v2",
    "published_date": "2024-07-12 20:55:59 UTC",
    "updated_date": "2024-07-17 10:43:17 UTC"
  },
  {
    "arxiv_id": "2407.11071v2",
    "title": "MonoSparse-CAM: Efficient Tree Model Processing via Monotonicity and Sparsity in CAMs",
    "authors": [
      "Tergel Molom-Ochir",
      "Brady Taylor",
      "Hai Li",
      "Yiran Chen"
    ],
    "abstract": "While the tree-based machine learning (TBML) models exhibit superior\nperformance compared to neural networks on tabular data and hold promise for\nenergy-efficient acceleration using aCAM arrays, their ideal deployment on\nhardware with explicit exploitation of TBML structure and aCAM circuitry\nremains a challenging task. In this work, we present MonoSparse-CAM, a new\nCAM-based optimization technique that exploits TBML sparsity and monotonicity\nin CAM circuitry to further advance processing performance. Our results\nindicate that MonoSparse-CAM reduces energy consumption by upto to 28.56x\ncompared to raw processing and by 18.51x compared to state-of-the-art\ntechniques, while improving the efficiency of computation by at least 1.68x.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11071v2",
    "published_date": "2024-07-12 20:34:59 UTC",
    "updated_date": "2024-12-27 04:54:02 UTC"
  },
  {
    "arxiv_id": "2407.12865v1",
    "title": "GRAD-SUM: Leveraging Gradient Summarization for Optimal Prompt Engineering",
    "authors": [
      "Derek Austin",
      "Elliott Chartock"
    ],
    "abstract": "Prompt engineering for large language models (LLMs) is often a manual\ntime-intensive process that involves generating, evaluating, and refining\nprompts iteratively to ensure high-quality outputs. While there has been work\non automating prompt engineering, the solutions generally are either tuned to\nspecific tasks with given answers or are quite costly. We introduce GRAD-SUM, a\nscalable and flexible method for automatic prompt engineering that builds on\ngradient-based optimization techniques. Our approach incorporates user-defined\ntask descriptions and evaluation criteria, and features a novel gradient\nsummarization module to generalize feedback effectively. Our results\ndemonstrate that GRAD-SUM consistently outperforms existing methods across\nvarious benchmarks, highlighting its versatility and effectiveness in automatic\nprompt optimization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.12865v1",
    "published_date": "2024-07-12 19:11:21 UTC",
    "updated_date": "2024-07-12 19:11:21 UTC"
  },
  {
    "arxiv_id": "2407.11070v2",
    "title": "Optimal Defender Strategies for CAGE-2 using Causal Modeling and Tree Search",
    "authors": [
      "Kim Hammar",
      "Neil Dhir",
      "Rolf Stadler"
    ],
    "abstract": "The CAGE-2 challenge is considered a standard benchmark to compare methods\nfor autonomous cyber defense. Current state-of-the-art methods evaluated\nagainst this benchmark are based on model-free (offline) reinforcement\nlearning, which does not provide provably optimal defender strategies. We\naddress this limitation and present a formal (causal) model of CAGE-2 together\nwith a method that produces a provably optimal defender strategy, which we call\nCausal Partially Observable Monte-Carlo Planning (C-POMCP). It has two key\nproperties. First, it incorporates the causal structure of the target system,\ni.e., the causal relationships among the system variables. This structure\nallows for a significant reduction of the search space of defender strategies.\nSecond, it is an online method that updates the defender strategy at each time\nstep via tree search. Evaluations against the CAGE-2 benchmark show that\nC-POMCP achieves state-of-the-art performance with respect to effectiveness and\nis two orders of magnitude more efficient in computing time than the closest\ncompetitor method.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2407.11070v2",
    "published_date": "2024-07-12 18:34:55 UTC",
    "updated_date": "2024-07-22 07:08:31 UTC"
  },
  {
    "arxiv_id": "2407.09475v2",
    "title": "Adaptive Prediction Ensemble: Improving Out-of-Distribution Generalization of Motion Forecasting",
    "authors": [
      "Jinning Li",
      "Jiachen Li",
      "Sangjae Bae",
      "David Isele"
    ],
    "abstract": "Deep learning-based trajectory prediction models for autonomous driving often\nstruggle with generalization to out-of-distribution (OOD) scenarios, sometimes\nperforming worse than simple rule-based models. To address this limitation, we\npropose a novel framework, Adaptive Prediction Ensemble (APE), which integrates\ndeep learning and rule-based prediction experts. A learned routing function,\ntrained concurrently with the deep learning model, dynamically selects the most\nreliable prediction based on the input scenario. Our experiments on large-scale\ndatasets, including Waymo Open Motion Dataset (WOMD) and Argoverse, demonstrate\nimprovement in zero-shot generalization across datasets. We show that our\nmethod outperforms individual prediction models and other variants,\nparticularly in long-horizon prediction and scenarios with a high proportion of\nOOD data. This work highlights the potential of hybrid approaches for robust\nand generalizable motion prediction in autonomous driving. More details can be\nfound on the project page: https://sites.google.com/view/ape-generalization.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09475v2",
    "published_date": "2024-07-12 17:57:00 UTC",
    "updated_date": "2024-12-20 05:34:30 UTC"
  },
  {
    "arxiv_id": "2407.09467v1",
    "title": "FairyLandAI: Personalized Fairy Tales utilizing ChatGPT and DALLE-3",
    "authors": [
      "Georgios Makridis",
      "Athanasios Oikonomou",
      "Vasileios Koukos"
    ],
    "abstract": "In the diverse world of AI-driven storytelling, there is a unique opportunity\nto engage young audiences with customized, and personalized narratives. This\npaper introduces FairyLandAI an innovative Large Language Model (LLM) developed\nthrough OpenAI's API, specifically crafted to create personalized fairytales\nfor children. The distinctive feature of FairyLandAI is its dual capability: it\nnot only generates stories that are engaging, age-appropriate, and reflective\nof various traditions but also autonomously produces imaginative prompts\nsuitable for advanced image generation tools like GenAI and Dalle-3, thereby\nenriching the storytelling experience. FairyLandAI is expertly tailored to\nresonate with the imaginative worlds of children, providing narratives that are\nboth educational and entertaining and in alignment with the moral values\ninherent in different ages. Its unique strength lies in customizing stories to\nmatch individual children's preferences and cultural backgrounds, heralding a\nnew era in personalized storytelling. Further, its integration with image\ngeneration technology offers a comprehensive narrative experience that\nstimulates both verbal and visual creativity. Empirical evaluations of\nFairyLandAI demonstrate its effectiveness in crafting captivating stories for\nchildren, which not only entertain but also embody the values and teachings of\ndiverse traditions. This model serves as an invaluable tool for parents and\neducators, supporting them in imparting meaningful moral lessons through\nengaging narratives. FairyLandAI represents a pioneering step in using LLMs,\nparticularly through OpenAI's API, for educational and cultural enrichment,\nmaking complex moral narratives accessible and enjoyable for young, imaginative\nminds.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.09467v1",
    "published_date": "2024-07-12 17:46:58 UTC",
    "updated_date": "2024-07-12 17:46:58 UTC"
  },
  {
    "arxiv_id": "2407.09450v2",
    "title": "Human-like Episodic Memory for Infinite Context LLMs",
    "authors": [
      "Zafeirios Fountas",
      "Martin A Benfeghoul",
      "Adnan Oomerjee",
      "Fenia Christopoulou",
      "Gerasimos Lampouras",
      "Haitham Bou-Ammar",
      "Jun Wang"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable capabilities, but still\nstruggle with processing extensive contexts, limiting their ability to maintain\ncoherence and accuracy over long sequences. In contrast, the human brain excels\nat organising and retrieving episodic experiences across vast temporal scales,\nspanning a lifetime. In this work, we introduce EM-LLM, a novel approach that\nintegrates key aspects of human episodic memory and event cognition into LLMs\nwith no fine-tuning, enabling them to handle practically infinite context\nlengths while maintaining computational efficiency. EM-LLM organises sequences\nof tokens into coherent episodic events using a combination of Bayesian\nsurprise and graph-theoretic boundary refinement in an online fashion. When\nneeded, these events are retrieved through a two-stage memory process,\ncombining similarity-based and temporally contiguous retrieval for efficient\nand human-like access to relevant information. Experiments on the LongBench and\nInfiniteBench benchmarks demonstrate EM-LLM's superior performance,\nconsistently outperforming the state-of-the-art retrieval model InfLLM across\nvarious baseline LLMs. In addition, EM-LLM outperforms its popular counterpart,\nRAG, in a wide range of tasks, while requiring similar resources. Notably,\nEM-LLM's performance even surpasses full-context models in most tasks, while\nsuccessfully performing retrieval across 10 million tokens - a scale\ncomputationally infeasible for such models. Finally, our analysis reveals\nstrong correlations between EM-LLM's event segmentation and human-perceived\nevents, suggesting a bridge between this artificial system and its biological\ncounterpart, thereby offering a novel computational framework for exploring\nhuman memory mechanisms.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09450v2",
    "published_date": "2024-07-12 17:34:03 UTC",
    "updated_date": "2024-10-25 14:27:23 UTC"
  },
  {
    "arxiv_id": "2407.09441v4",
    "title": "The $μ\\mathcal{G}$ Language for Programming Graph Neural Networks",
    "authors": [
      "Matteo Belenchia",
      "Flavio Corradini",
      "Michela Quadrini",
      "Michele Loreti"
    ],
    "abstract": "Graph neural networks form a class of deep learning architectures\nspecifically designed to work with graph-structured data. As such, they share\nthe inherent limitations and problems of deep learning, especially regarding\nthe issues of explainability and trustworthiness. We propose $\\mu\\mathcal{G}$,\nan original domain-specific language for the specification of graph neural\nnetworks that aims to overcome these issues. The language's syntax is\nintroduced, and its meaning is rigorously defined by a denotational semantics.\nAn equivalent characterization in the form of an operational semantics is also\nprovided and, together with a type system, is used to prove the type soundness\nof $\\mu\\mathcal{G}$. We show how $\\mu\\mathcal{G}$ programs can be represented\nin a more user-friendly graphical visualization, and provide examples of its\ngenerality by showing how it can be used to define some of the most popular\ngraph neural network models, or to develop any custom graph processing\napplication.",
    "categories": [
      "cs.FL",
      "cs.AI",
      "cs.LG",
      "D.2.4"
    ],
    "primary_category": "cs.FL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09441v4",
    "published_date": "2024-07-12 17:27:43 UTC",
    "updated_date": "2024-10-15 15:14:05 UTC"
  },
  {
    "arxiv_id": "2407.09437v1",
    "title": "Let Me DeCode You: Decoder Conditioning with Tabular Data",
    "authors": [
      "Tomasz Szczepański",
      "Michal K. Grzeszczyk",
      "Szymon Płotka",
      "Arleta Adamowicz",
      "Piotr Fudalej",
      "Przemysław Korzeniowski",
      "Tomasz Trzciński",
      "Arkadiusz Sitek"
    ],
    "abstract": "Training deep neural networks for 3D segmentation tasks can be challenging,\noften requiring efficient and effective strategies to improve model\nperformance. In this study, we introduce a novel approach, DeCode, that\nutilizes label-derived features for model conditioning to support the decoder\nin the reconstruction process dynamically, aiming to enhance the efficiency of\nthe training process. DeCode focuses on improving 3D segmentation performance\nthrough the incorporation of conditioning embedding with learned numerical\nrepresentation of 3D-label shape features. Specifically, we develop an\napproach, where conditioning is applied during the training phase to guide the\nnetwork toward robust segmentation. When labels are not available during\ninference, our model infers the necessary conditioning embedding directly from\nthe input data, thanks to a feed-forward network learned during the training\nphase. This approach is tested using synthetic data and cone-beam computed\ntomography (CBCT) images of teeth. For CBCT, three datasets are used: one\npublicly available and two in-house. Our results show that DeCode significantly\noutperforms traditional, unconditioned models in terms of generalization to\nunseen data, achieving higher accuracy at a reduced computational cost. This\nwork represents the first of its kind to explore conditioning strategies in 3D\ndata segmentation, offering a novel and more efficient method for leveraging\nannotated data. Our code, pre-trained models are publicly available at\nhttps://github.com/SanoScience/DeCode .",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted for the 27th International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI) 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.09437v1",
    "published_date": "2024-07-12 17:14:33 UTC",
    "updated_date": "2024-07-12 17:14:33 UTC"
  },
  {
    "arxiv_id": "2407.09435v2",
    "title": "MUSCLE: A Model Update Strategy for Compatible LLM Evolution",
    "authors": [
      "Jessica Echterhoff",
      "Fartash Faghri",
      "Raviteja Vemulapalli",
      "Ting-Yao Hu",
      "Chun-Liang Li",
      "Oncel Tuzel",
      "Hadi Pouransari"
    ],
    "abstract": "Large Language Models (LLMs) are regularly updated to enhance performance,\ntypically through changes in data or architecture. Within the update process,\ndevelopers often prioritize improving overall performance metrics, paying less\nattention to maintaining compatibility with earlier model versions.\nInstance-level degradation (instance regression) of performance from one model\nversion to the next can interfere with a user's mental model of the\ncapabilities of a particular language model. Users having to adapt their mental\nmodel with every update can lead to dissatisfaction, especially when the new\nmodel has degraded compared to a prior version for a known use case (model\nupdate regression). We find that when pretrained LLM base models are updated,\nfine-tuned user-facing downstream task adapters experience negative flips --\npreviously correct instances are now predicted incorrectly. We observe model\nupdate regression between different model versions on a diverse set of tasks\nand models, even when the downstream task training procedures remain identical.\nWe argue for the importance of maintaining model update compatibility during\nupdates, and present evaluation metrics designed specifically for generative\ntasks, while also being applicable to discriminative tasks. We propose a\ntraining strategy to minimize the extent of instance regression in model\nupdates, involving training of a compatibility adapter that can enhance task\nfine-tuned language models. We show negative flips reduce by up to 40% e.g.\nwhen updating Llama 1 to Llama 2 with our proposed method.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09435v2",
    "published_date": "2024-07-12 17:12:48 UTC",
    "updated_date": "2024-10-03 21:10:13 UTC"
  },
  {
    "arxiv_id": "2407.09434v2",
    "title": "Foundation Models for the Electric Power Grid",
    "authors": [
      "Hendrik F. Hamann",
      "Thomas Brunschwiler",
      "Blazhe Gjorgiev",
      "Leonardo S. A. Martins",
      "Alban Puech",
      "Anna Varbella",
      "Jonas Weiss",
      "Juan Bernabe-Moreno",
      "Alexandre Blondin Massé",
      "Seong Choi",
      "Ian Foster",
      "Bri-Mathias Hodge",
      "Rishabh Jain",
      "Kibaek Kim",
      "Vincent Mai",
      "François Mirallès",
      "Martin De Montigny",
      "Octavio Ramos-Leaños",
      "Hussein Suprême",
      "Le Xie",
      "El-Nasser S. Youssef",
      "Arnaud Zinflou",
      "Alexander J. Belyi",
      "Ricardo J. Bessa",
      "Bishnu Prasad Bhattarai",
      "Johannes Schmude",
      "Stanislav Sobolevsky"
    ],
    "abstract": "Foundation models (FMs) currently dominate news headlines. They employ\nadvanced deep learning architectures to extract structural information\nautonomously from vast datasets through self-supervision. The resulting rich\nrepresentations of complex systems and dynamics can be applied to many\ndownstream applications. Therefore, FMs can find uses in electric power grids,\nchallenged by the energy transition and climate change. In this paper, we call\nfor the development of, and state why we believe in, the potential of FMs for\nelectric grids. We highlight their strengths and weaknesses amidst the\nchallenges of a changing grid. We argue that an FM learning from diverse grid\ndata and topologies could unlock transformative capabilities, pioneering a new\napproach in leveraging AI to redefine how we manage complexity and uncertainty\nin the electric grid. Finally, we discuss a power grid FM concept, namely\nGridFM, based on graph neural networks and show how different downstream tasks\nbenefit.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "Major equal contributors: H.F.H., T.B., B.G., L.S.A.M., A.P., A.V.,\n  J.W.; Significant equal contributors: J.B., A.B.M., S.C., I.F., B.H., R.J.,\n  K.K., V.M., F.M., M.D.M., O.R., H.S., L.X., E.S.Y., A.Z.; Other equal\n  contributors: A.J.B., R.J.B., B.P.B., J.S., S.S; Lead contact: H.F.H",
    "pdf_url": "http://arxiv.org/pdf/2407.09434v2",
    "published_date": "2024-07-12 17:09:47 UTC",
    "updated_date": "2024-11-12 17:49:12 UTC"
  },
  {
    "arxiv_id": "2407.09424v1",
    "title": "TelecomGPT: A Framework to Build Telecom-Specfic Large Language Models",
    "authors": [
      "Hang Zou",
      "Qiyang Zhao",
      "Yu Tian",
      "Lina Bariah",
      "Faouzi Bader",
      "Thierry Lestable",
      "Merouane Debbah"
    ],
    "abstract": "Large Language Models (LLMs) have the potential to revolutionize the Sixth\nGeneration (6G) communication networks. However, current mainstream LLMs\ngenerally lack the specialized knowledge in telecom domain. In this paper, for\nthe first time, we propose a pipeline to adapt any general purpose LLMs to a\ntelecom-specific LLMs. We collect and build telecom-specific pre-train dataset,\ninstruction dataset, preference dataset to perform continual pre-training,\ninstruct tuning and alignment tuning respectively. Besides, due to the lack of\nwidely accepted evaluation benchmarks in telecom domain, we extend existing\nevaluation benchmarks and proposed three new benchmarks, namely, Telecom Math\nModeling, Telecom Open QnA and Telecom Code Tasks. These new benchmarks provide\na holistic evaluation of the capabilities of LLMs including math modeling,\nOpen-Ended question answering, code generation, infilling, summarization and\nanalysis in telecom domain. Our fine-tuned LLM TelecomGPT outperforms state of\nthe art (SOTA) LLMs including GPT-4, Llama-3 and Mistral in Telecom Math\nModeling benchmark significantly and achieve comparable performance in various\nevaluation benchmarks such as TeleQnA, 3GPP technical documents classification,\ntelecom code summary and generation and infilling.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "arXiv admin note: text overlap with arXiv:1303.2654 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2407.09424v1",
    "published_date": "2024-07-12 16:51:02 UTC",
    "updated_date": "2024-07-12 16:51:02 UTC"
  },
  {
    "arxiv_id": "2407.09415v1",
    "title": "A Benchmark Environment for Offline Reinforcement Learning in Racing Games",
    "authors": [
      "Girolamo Macaluso",
      "Alessandro Sestini",
      "Andrew D. Bagdanov"
    ],
    "abstract": "Offline Reinforcement Learning (ORL) is a promising approach to reduce the\nhigh sample complexity of traditional Reinforcement Learning (RL) by\neliminating the need for continuous environmental interactions. ORL exploits a\ndataset of pre-collected transitions and thus expands the range of application\nof RL to tasks in which the excessive environment queries increase training\ntime and decrease efficiency, such as in modern AAA games. This paper\nintroduces OfflineMania a novel environment for ORL research. It is inspired by\nthe iconic TrackMania series and developed using the Unity 3D game engine. The\nenvironment simulates a single-agent racing game in which the objective is to\ncomplete the track through optimal navigation. We provide a variety of datasets\nto assess ORL performance. These datasets, created from policies of varying\nability and in different sizes, aim to offer a challenging testbed for\nalgorithm development and evaluation. We further establish a set of baselines\nfor a range of Online RL, ORL, and hybrid Offline to Online RL approaches using\nour environment.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at IEEE Conference on Games",
    "pdf_url": "http://arxiv.org/pdf/2407.09415v1",
    "published_date": "2024-07-12 16:44:03 UTC",
    "updated_date": "2024-07-12 16:44:03 UTC"
  },
  {
    "arxiv_id": "2407.09413v3",
    "title": "SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers",
    "authors": [
      "Shraman Pramanick",
      "Rama Chellappa",
      "Subhashini Venugopalan"
    ],
    "abstract": "Seeking answers to questions within long scientific research articles is a\ncrucial area of study that aids readers in quickly addressing their inquiries.\nHowever, existing question-answering (QA) datasets based on scientific papers\nare limited in scale and focus solely on textual content. We introduce SPIQA\n(Scientific Paper Image Question Answering), the first large-scale QA dataset\nspecifically designed to interpret complex figures and tables within the\ncontext of scientific research articles across various domains of computer\nscience. Leveraging the breadth of expertise and ability of multimodal large\nlanguage models (MLLMs) to understand figures, we employ automatic and manual\ncuration to create the dataset. We craft an information-seeking task on\ninterleaved images and text that involves multiple images covering plots,\ncharts, tables, schematic diagrams, and result visualizations. SPIQA comprises\n270K questions divided into training, validation, and three different\nevaluation splits. Through extensive experiments with 12 prominent foundational\nmodels, we evaluate the ability of current multimodal systems to comprehend the\nnuanced aspects of research articles. Additionally, we propose a\nChain-of-Thought (CoT) evaluation strategy with in-context retrieval that\nallows fine-grained, step-by-step assessment and improves model performance. We\nfurther explore the upper bounds of performance enhancement with additional\ntextual information, highlighting its promising potential for future research\nand the dataset's impact on revolutionizing how we interact with scientific\nliterature.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2024, Datasets & Benchmarks track",
    "pdf_url": "http://arxiv.org/pdf/2407.09413v3",
    "published_date": "2024-07-12 16:37:59 UTC",
    "updated_date": "2025-01-10 19:41:43 UTC"
  },
  {
    "arxiv_id": "2407.09395v1",
    "title": "Deep Bag-of-Words Model: An Efficient and Interpretable Relevance Architecture for Chinese E-Commerce",
    "authors": [
      "Zhe Lin",
      "Jiwei Tan",
      "Dan Ou",
      "Xi Chen",
      "Shaowei Yao",
      "Bo Zheng"
    ],
    "abstract": "Text relevance or text matching of query and product is an essential\ntechnique for the e-commerce search system to ensure that the displayed\nproducts can match the intent of the query. Many studies focus on improving the\nperformance of the relevance model in search system. Recently, pre-trained\nlanguage models like BERT have achieved promising performance on the text\nrelevance task. While these models perform well on the offline test dataset,\nthere are still obstacles to deploy the pre-trained language model to the\nonline system as their high latency. The two-tower model is extensively\nemployed in industrial scenarios, owing to its ability to harmonize performance\nwith computational efficiency. Regrettably, such models present an opaque\n``black box'' nature, which prevents developers from making special\noptimizations. In this paper, we raise deep Bag-of-Words (DeepBoW) model, an\nefficient and interpretable relevance architecture for Chinese e-commerce. Our\napproach proposes to encode the query and the product into the sparse BoW\nrepresentation, which is a set of word-weight pairs. The weight means the\nimportant or the relevant score between the corresponding word and the raw\ntext. The relevance score is measured by the accumulation of the matched word\nbetween the sparse BoW representation of the query and the product. Compared to\npopular dense distributed representation that usually suffers from the drawback\nof black-box, the most advantage of the proposed representation model is highly\nexplainable and interventionable, which is a superior advantage to the\ndeployment and operation of online search engines. Moreover, the online\nefficiency of the proposed model is even better than the most efficient inner\nproduct form of dense representation ...",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "KDD'24 accepted paper",
    "pdf_url": "http://arxiv.org/pdf/2407.09395v1",
    "published_date": "2024-07-12 16:18:05 UTC",
    "updated_date": "2024-07-12 16:18:05 UTC"
  },
  {
    "arxiv_id": "2407.09388v2",
    "title": "GAVEL: Generating Games Via Evolution and Language Models",
    "authors": [
      "Graham Todd",
      "Alexander Padula",
      "Matthew Stephenson",
      "Éric Piette",
      "Dennis J. N. J. Soemers",
      "Julian Togelius"
    ],
    "abstract": "Automatically generating novel and interesting games is a complex task.\nChallenges include representing game rules in a computationally workable form,\nsearching through the large space of potential games under most such\nrepresentations, and accurately evaluating the originality and quality of\npreviously unseen games. Prior work in automated game generation has largely\nfocused on relatively restricted rule representations and relied on\ndomain-specific heuristics. In this work, we explore the generation of novel\ngames in the comparatively expansive Ludii game description language, which\nencodes the rules of over 1000 board games in a variety of styles and modes of\nplay. We draw inspiration from recent advances in large language models and\nevolutionary computation in order to train a model that intelligently mutates\nand recombines games and mechanics expressed as code. We demonstrate both\nquantitatively and qualitatively that our approach is capable of generating new\nand interesting games, including in regions of the potential rules space not\ncovered by existing games in the Ludii dataset. A sample of the generated games\nare available to play online through the Ludii portal.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 4 figures, 7 pages appendices",
    "pdf_url": "http://arxiv.org/pdf/2407.09388v2",
    "published_date": "2024-07-12 16:08:44 UTC",
    "updated_date": "2024-12-03 01:33:06 UTC"
  },
  {
    "arxiv_id": "2407.09378v1",
    "title": "Graph Neural Network Causal Explanation via Neural Causal Models",
    "authors": [
      "Arman Behnam",
      "Binghui Wang"
    ],
    "abstract": "Graph neural network (GNN) explainers identify the important subgraph that\nensures the prediction for a given graph. Until now, almost all GNN explainers\nare based on association, which is prone to spurious correlations. We propose\n{\\name}, a GNN causal explainer via causal inference. Our explainer is based on\nthe observation that a graph often consists of a causal underlying subgraph.\n{\\name} includes three main steps: 1) It builds causal structure and the\ncorresponding structural causal model (SCM) for a graph, which enables the\ncause-effect calculation among nodes. 2) Directly calculating the cause-effect\nin real-world graphs is computationally challenging. It is then enlightened by\nthe recent neural causal model (NCM), a special type of SCM that is trainable,\nand design customized NCMs for GNNs. By training these GNN NCMs, the\ncause-effect can be easily calculated. 3) It uncovers the subgraph that\ncausally explains the GNN predictions via the optimized GNN-NCMs. Evaluation\nresults on multiple synthetic and real-world graphs validate that {\\name}\nsignificantly outperforms existing GNN explainers in exact groundtruth\nexplanation identification",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09378v1",
    "published_date": "2024-07-12 15:56:33 UTC",
    "updated_date": "2024-07-12 15:56:33 UTC"
  },
  {
    "arxiv_id": "2407.09373v1",
    "title": "Towards Personalised Patient Risk Prediction Using Temporal Hospital Data Trajectories",
    "authors": [
      "Thea Barnes",
      "Enrico Werner",
      "Jeffrey N. Clark",
      "Raul Santos-Rodriguez"
    ],
    "abstract": "Quantifying a patient's health status provides clinicians with insight into\npatient risk, and the ability to better triage and manage resources. Early\nWarning Scores (EWS) are widely deployed to measure overall health status, and\nrisk of adverse outcomes, in hospital patients. However, current EWS are\nlimited both by their lack of personalisation and use of static observations.\nWe propose a pipeline that groups intensive care unit patients by the\ntrajectories of observations data throughout their stay as a basis for the\ndevelopment of personalised risk predictions. Feature importance is considered\nto provide model explainability. Using the MIMIC-IV dataset, six clusters were\nidentified, capturing differences in disease codes, observations, lengths of\nadmissions and outcomes. Applying the pipeline to data from just the first four\nhours of each ICU stay assigns the majority of patients to the same cluster as\nwhen the entire stay duration is considered. In-hospital mortality prediction\nmodels trained on individual clusters had higher F1 score performance in five\nof the six clusters when compared against the unclustered patient cohort. The\npipeline could form the basis of a clinical decision support tool, working to\nimprove the clinical characterisation of risk groups and the early detection of\npatient deterioration.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09373v1",
    "published_date": "2024-07-12 15:53:26 UTC",
    "updated_date": "2024-07-12 15:53:26 UTC"
  },
  {
    "arxiv_id": "2407.09364v2",
    "title": "Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution of AI-generated Text",
    "authors": [
      "Lucio La Cava",
      "Davide Costa",
      "Andrea Tagarelli"
    ],
    "abstract": "The significant progress in the development of Large Language Models has\ncontributed to blurring the distinction between human and AI-generated text.\nThe increasing pervasiveness of AI-generated text and the difficulty in\ndetecting it poses new challenges for our society. In this paper, we tackle the\nproblem of detecting and attributing AI-generated text by proposing WhosAI, a\ntriplet-network contrastive learning framework designed to predict whether a\ngiven input text has been generated by humans or AI and to unveil the\nauthorship of the text. Unlike most existing approaches, our proposed framework\nis conceived to learn semantic similarity representations from multiple\ngenerators at once, thus equally handling both detection and attribution tasks.\nFurthermore, WhosAI is model-agnostic and scalable to the release of new AI\ntext-generation models by incorporating their generated instances into the\nembedding space learned by our framework. Experimental results on the\nTuringBench benchmark of 200K news articles show that our proposed framework\nachieves outstanding results in both the Turing Test and Authorship Attribution\ntasks, outperforming all the methods listed in the TuringBench benchmark\nleaderboards.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "physics.soc-ph"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication at the 27th European Conference on\n  Artificial Intelligence (ECAI-2024), Volume 392, Pages 3179 - 3186, October\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2407.09364v2",
    "published_date": "2024-07-12 15:44:56 UTC",
    "updated_date": "2025-03-17 09:19:05 UTC"
  },
  {
    "arxiv_id": "2407.09355v1",
    "title": "FastImpute: A Baseline for Open-source, Reference-Free Genotype Imputation Methods -- A Case Study in PRS313",
    "authors": [
      "Aaron Ge",
      "Jeya Balasubramanian",
      "Xueyao Wu",
      "Peter Kraft",
      "Jonas S. Almeida"
    ],
    "abstract": "Genotype imputation enhances genetic data by predicting missing SNPs using\nreference haplotype information. Traditional methods leverage linkage\ndisequilibrium (LD) to infer untyped SNP genotypes, relying on the similarity\nof LD structures between genotyped target sets and fully sequenced reference\npanels. Recently, reference-free deep learning-based methods have emerged,\noffering a promising alternative by predicting missing genotypes without\nexternal databases, thereby enhancing privacy and accessibility. However, these\nmethods often produce models with tens of millions of parameters, leading to\nchallenges such as the need for substantial computational resources to train\nand inefficiency for client-sided deployment. Our study addresses these\nlimitations by introducing a baseline for a novel genotype imputation pipeline\nthat supports client-sided imputation models generalizable across any\ngenotyping chip and genomic region. This approach enhances patient privacy by\nperforming imputation directly on edge devices. As a case study, we focus on\nPRS313, a polygenic risk score comprising 313 SNPs used for breast cancer risk\nprediction. Utilizing consumer genetic panels such as 23andMe, our model\ndemocratizes access to personalized genetic insights by allowing 23andMe users\nto obtain their PRS313 score. We demonstrate that simple linear regression can\nsignificantly improve the accuracy of PRS313 scores when calculated using SNPs\nimputed from consumer gene panels, such as 23andMe. Our linear regression model\nachieved an R^2 of 0.86, compared to 0.33 without imputation and 0.28 with\nsimple imputation (substituting missing SNPs with the minor allele frequency).\nThese findings suggest that popular SNP analysis libraries could benefit from\nintegrating linear regression models for genotype imputation, providing a\nviable and light-weight alternative to reference based imputation.",
    "categories": [
      "q-bio.GN",
      "cs.AI"
    ],
    "primary_category": "q-bio.GN",
    "comment": "This paper is 16 pages long and contains 7 figures. For more\n  information and to access related resources: * Web application:\n  https://aaronge-2020.github.io/DeepImpute/ * Code repository:\n  https://github.com/aaronge-2020/DeepImpute",
    "pdf_url": "http://arxiv.org/pdf/2407.09355v1",
    "published_date": "2024-07-12 15:28:13 UTC",
    "updated_date": "2024-07-12 15:28:13 UTC"
  },
  {
    "arxiv_id": "2407.09348v1",
    "title": "Predictable and Performant Reactive Synthesis Modulo Theories via Functional Synthesis",
    "authors": [
      "Andoni Rodríguez",
      "Felipe Gorostiaga",
      "César Sánchez"
    ],
    "abstract": "Reactive synthesis is the process of generating correct controllers from\ntemporal logic specifications. Classical LTL reactive synthesis handles\n(propositional) LTL as a specification language. Boolean abstractions allow\nreducing LTLt specifications (i.e., LTL with propositions replaced by literals\nfrom a theory calT), into equi-realizable LTL specifications. In this paper we\nextend these results into a full static synthesis procedure. The synthesized\nsystem receives from the environment valuations of variables from a rich theory\ncalT and outputs valuations of system variables from calT. We use the\nabstraction method to synthesize a reactive Boolean controller from the LTL\nspecification, and we combine it with functional synthesis to obtain a static\ncontroller for the original LTLt specification. We also show that our method\nallows responses in the sense that the controller can optimize its outputs in\norder to e.g., always provide the smallest safe values. This is the first full\nstatic synthesis method for LTLt, which is a deterministic program (hence\npredictable and efficient).",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09348v1",
    "published_date": "2024-07-12 15:23:27 UTC",
    "updated_date": "2024-07-12 15:23:27 UTC"
  },
  {
    "arxiv_id": "2407.09337v1",
    "title": "CFaults: Model-Based Diagnosis for Fault Localization in C Programs with Multiple Test Cases",
    "authors": [
      "Pedro Orvalho",
      "Mikoláš Janota",
      "Vasco Manquinho"
    ],
    "abstract": "Debugging is one of the most time-consuming and expensive tasks in software\ndevelopment. Several formula-based fault localization (FBFL) methods have been\nproposed, but they fail to guarantee a set of diagnoses across all failing\ntests or may produce redundant diagnoses that are not subset-minimal,\nparticularly for programs with multiple faults.\n  This paper introduces a novel fault localization approach for C programs with\nmultiple faults. CFaults leverages Model-Based Diagnosis (MBD) with multiple\nobservations and aggregates all failing test cases into a unified MaxSAT\nformula. Consequently, our method guarantees consistency across observations\nand simplifies the fault localization procedure. Experimental results on two\nbenchmark sets of C programs, TCAS and C-Pack-IPAs, show that CFaults is faster\nthan other FBFL approaches like BugAssist and SNIPER. Moreover, CFaults only\ngenerates subset-minimal diagnoses of faulty statements, whereas the other\napproaches tend to enumerate redundant diagnoses.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted at FM 2024. 15 pages, 2 figures, 3 tables and 5 listings",
    "pdf_url": "http://arxiv.org/pdf/2407.09337v1",
    "published_date": "2024-07-12 15:14:49 UTC",
    "updated_date": "2024-07-12 15:14:49 UTC"
  },
  {
    "arxiv_id": "2407.09336v1",
    "title": "Guidelines for Augmentation Selection in Contrastive Learning for Time Series Classification",
    "authors": [
      "Ziyu Liu",
      "Azadeh Alavi",
      "Minyi Li",
      "Xiang Zhang"
    ],
    "abstract": "Self-supervised contrastive learning has become a key technique in deep\nlearning, particularly in time series analysis, due to its ability to learn\nmeaningful representations without explicit supervision. Augmentation is a\ncritical component in contrastive learning, where different augmentations can\ndramatically impact performance, sometimes influencing accuracy by over 30%.\nHowever, the selection of augmentations is predominantly empirical which can be\nsuboptimal, or grid searching that is time-consuming. In this paper, we\nestablish a principled framework for selecting augmentations based on dataset\ncharacteristics such as trend and seasonality. Specifically, we construct 12\nsynthetic datasets incorporating trend, seasonality, and integration weights.\nWe then evaluate the effectiveness of 8 different augmentations across these\nsynthetic datasets, thereby inducing generalizable associations between time\nseries characteristics and augmentation efficiency. Additionally, we evaluated\nthe induced associations across 6 real-world datasets encompassing domains such\nas activity recognition, disease diagnosis, traffic monitoring, electricity\nusage, mechanical fault prognosis, and finance. These real-world datasets are\ndiverse, covering a range from 1 to 12 channels, 2 to 10 classes, sequence\nlengths of 14 to 1280, and data frequencies from 250 Hz to daily intervals. The\nexperimental results show that our proposed trend-seasonality-based\naugmentation recommendation algorithm can accurately identify the effective\naugmentations for a given time series dataset, achieving an average Recall@3 of\n0.667, outperforming baselines. Our work provides guidance for studies\nemploying contrastive learning in time series analysis, with wide-ranging\napplications. All the code, datasets, and analysis results will be released at\nhttps://github.com/DL4mHealth/TS-Contrastive-Augmentation-Recommendation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.09336v1",
    "published_date": "2024-07-12 15:13:16 UTC",
    "updated_date": "2024-07-12 15:13:16 UTC"
  },
  {
    "arxiv_id": "2407.09327v1",
    "title": "Sina at FigNews 2024: Multilingual Datasets Annotated with Bias and Propaganda",
    "authors": [
      "Lina Duaibes",
      "Areej Jaber",
      "Mustafa Jarrar",
      "Ahmad Qadi",
      "Mais Qandeel"
    ],
    "abstract": "The proliferation of bias and propaganda on social media is an increasingly\nsignificant concern, leading to the development of techniques for automatic\ndetection. This article presents a multilingual corpus of 12, 000 Facebook\nposts fully annotated for bias and propaganda. The corpus was created as part\nof the FigNews 2024 Shared Task on News Media Narratives for framing the\nIsraeli War on Gaza. It covers various events during the War from October 7,\n2023 to January 31, 2024. The corpus comprises 12, 000 posts in five languages\n(Arabic, Hebrew, English, French, and Hindi), with 2, 400 posts for each\nlanguage. The annotation process involved 10 graduate students specializing in\nLaw. The Inter-Annotator Agreement (IAA) was used to evaluate the annotations\nof the corpus, with an average IAA of 80.8% for bias and 70.15% for propaganda\nannotations. Our team was ranked among the bestperforming teams in both Bias\nand Propaganda subtasks. The corpus is open-source and available at\nhttps://sina.birzeit.edu/fada",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09327v1",
    "published_date": "2024-07-12 15:04:09 UTC",
    "updated_date": "2024-07-12 15:04:09 UTC"
  },
  {
    "arxiv_id": "2407.09324v2",
    "title": "Provable Privacy Advantages of Decentralized Federated Learning via Distributed Optimization",
    "authors": [
      "Wenrui Yu",
      "Qiongxiu Li",
      "Milan Lopuhaä-Zwakenberg",
      "Mads Græsbøll Christensen",
      "Richard Heusdens"
    ],
    "abstract": "Federated learning (FL) emerged as a paradigm designed to improve data\nprivacy by enabling data to reside at its source, thus embedding privacy as a\ncore consideration in FL architectures, whether centralized or decentralized.\nContrasting with recent findings by Pasquini et al., which suggest that\ndecentralized FL does not empirically offer any additional privacy or security\nbenefits over centralized models, our study provides compelling evidence to the\ncontrary. We demonstrate that decentralized FL, when deploying distributed\noptimization, provides enhanced privacy protection - both theoretically and\nempirically - compared to centralized approaches. The challenge of quantifying\nprivacy loss through iterative processes has traditionally constrained the\ntheoretical exploration of FL protocols. We overcome this by conducting a\npioneering in-depth information-theoretical privacy analysis for both\nframeworks. Our analysis, considering both eavesdropping and passive adversary\nmodels, successfully establishes bounds on privacy leakage. We show information\ntheoretically that the privacy loss in decentralized FL is upper bounded by the\nloss in centralized FL. Compared to the centralized case where local gradients\nof individual participants are directly revealed, a key distinction of\noptimization-based decentralized FL is that the relevant information includes\ndifferences of local gradients over successive iterations and the aggregated\nsum of different nodes' gradients over the network. This information\ncomplicates the adversary's attempt to infer private data. To bridge our\ntheoretical insights with practical applications, we present detailed case\nstudies involving logistic regression and deep neural networks. These examples\ndemonstrate that while privacy leakage remains comparable in simpler models,\ncomplex models like deep neural networks exhibit lower privacy risks under\ndecentralized FL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09324v2",
    "published_date": "2024-07-12 15:01:09 UTC",
    "updated_date": "2024-11-30 14:35:52 UTC"
  },
  {
    "arxiv_id": "2407.09287v1",
    "title": "Instruction Following with Goal-Conditioned Reinforcement Learning in Virtual Environments",
    "authors": [
      "Zoya Volovikova",
      "Alexey Skrynnik",
      "Petr Kuderov",
      "Aleksandr I. Panov"
    ],
    "abstract": "In this study, we address the issue of enabling an artificial intelligence\nagent to execute complex language instructions within virtual environments. In\nour framework, we assume that these instructions involve intricate linguistic\nstructures and multiple interdependent tasks that must be navigated\nsuccessfully to achieve the desired outcomes. To effectively manage these\ncomplexities, we propose a hierarchical framework that combines the deep\nlanguage comprehension of large language models with the adaptive\naction-execution capabilities of reinforcement learning agents. The language\nmodule (based on LLM) translates the language instruction into a high-level\naction plan, which is then executed by a pre-trained reinforcement learning\nagent. We have demonstrated the effectiveness of our approach in two different\nenvironments: in IGLU, where agents are instructed to build structures, and in\nCrafter, where agents perform tasks and interact with objects in the\nsurrounding environment according to language commands.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09287v1",
    "published_date": "2024-07-12 14:19:36 UTC",
    "updated_date": "2024-07-12 14:19:36 UTC"
  },
  {
    "arxiv_id": "2407.11068v5",
    "title": "Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay",
    "authors": [
      "Gonçalo Hora de Carvalho",
      "Oscar Knap",
      "Robert Pollice"
    ],
    "abstract": "We developed a benchmark set to assess the generalization of state-of-the-art\nlarge language models on problems beyond linguistic tasks and evaluate it on a\nsystematic progression of GPT models (GPT-3.5, GPT-4, GPT-4o, GPT-4o-mini).\nUsing simple games like Tic-Tac-Toe, Connect Four, Battleship, and a Shape\nRecognition Game, all encoded in ASCII, we test strategic capabilities and\nspatial reasoning, core abilities any artificial intelligence would need to\nmaster for solving problems in chemistry. To probe generalization, we introduce\ntwo new games for spatial logic: LEGO Connect Language (LCL) and\nGuess-the-SMILES (GtS), a operationally simple chemistry benchmark. Our results\nshow that GPT models provide meaningful responses for several tasks but,\ngenerally, perform poorly. A systematic performance progression with increased\nmodel capabilities (GPT-3.5, GPT-4, GPT-4o) is only observed for 4 out of the 7\nbenchmark tasks. All models consistently struggle with Battleship, LCL, and\nGtS. This suggests that while GPT models can emulate conversational proficiency\nand basic rule comprehension, they have limited generalization with respect to\nstrategy and spatial reasoning. Particularly poor performance is observed for\ninterpreting molecular graphs when encoded in ASCII. The results provided by\nour open-source benchmark suite\n(\\href{https://github.com/BlueVelvetSackOfGoldPotatoes/child-play}{\\texttt{ChildPlay}\nGitHub Repository}) caution against claims of emergent intelligence in GPT\nmodels, which appear more specialized than general.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11068v5",
    "published_date": "2024-07-12 14:17:26 UTC",
    "updated_date": "2025-02-27 21:47:06 UTC"
  },
  {
    "arxiv_id": "2407.09283v2",
    "title": "DAHRS: Divergence-Aware Hallucination-Remediated SRL Projection",
    "authors": [
      "Sangpil Youm",
      "Brodie Mather",
      "Chathuri Jayaweera",
      "Juliana Prada",
      "Bonnie Dorr"
    ],
    "abstract": "Semantic role labeling (SRL) enriches many downstream applications, e.g.,\nmachine translation, question answering, summarization, and stance/belief\ndetection. However, building multilingual SRL models is challenging due to the\nscarcity of semantically annotated corpora for multiple languages. Moreover,\nstate-of-the-art SRL projection (XSRL) based on large language models (LLMs)\nyields output that is riddled with spurious role labels. Remediation of such\nhallucinations is not straightforward due to the lack of explainability of\nLLMs. We show that hallucinated role labels are related to naturally occurring\ndivergence types that interfere with initial alignments. We implement\nDivergence-Aware Hallucination-Remediated SRL projection (DAHRS), leveraging\nlinguistically-informed alignment remediation followed by greedy First-Come\nFirst-Assign (FCFA) SRL projection. DAHRS improves the accuracy of SRL\nprojection without additional transformer-based machinery, beating XSRL in both\nhuman and automatic comparisons, and advancing beyond headwords to accommodate\nphrase-level SRL projection (e.g., EN-FR, EN-ES). Using CoNLL-2009 as our\nground truth, we achieve a higher word-level F1 over XSRL: 87.6% vs. 77.3%\n(EN-FR) and 89.0% vs. 82.7% (EN-ES). Human phrase-level assessments yield 89.1%\n(EN-FR) and 91.0% (EN-ES). We also define a divergence metric to adapt our\napproach to other language pairs (e.g., English-Tagalog).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 6 figures, Accepted to The 29th International Conference on\n  Natural Language & Information Systems (NLDB 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.09283v2",
    "published_date": "2024-07-12 14:13:59 UTC",
    "updated_date": "2025-03-19 13:41:21 UTC"
  },
  {
    "arxiv_id": "2407.09281v2",
    "title": "Predicting and Understanding Human Action Decisions: Insights from Large Language Models and Cognitive Instance-Based Learning",
    "authors": [
      "Thuy Ngoc Nguyen",
      "Kasturi Jamale",
      "Cleotilde Gonzalez"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated their capabilities across\nvarious tasks, from language translation to complex reasoning. Understanding\nand predicting human behavior and biases are crucial for artificial\nintelligence (AI) assisted systems to provide useful assistance, yet it remains\nan open question whether these models can achieve this. This paper addresses\nthis gap by leveraging the reasoning and generative capabilities of the LLMs to\npredict human behavior in two sequential decision-making tasks. These tasks\ninvolve balancing between exploitative and exploratory actions and handling\ndelayed feedback, both essential for simulating real-life decision processes.\nWe compare the performance of LLMs with a cognitive instance-based learning\n(IBL) model, which imitates human experiential decision-making. Our findings\nindicate that LLMs excel at rapidly incorporating feedback to enhance\nprediction accuracy. In contrast, the cognitive IBL model better accounts for\nhuman exploratory behaviors and effectively captures loss aversion bias, i.e.,\nthe tendency to choose a sub-optimal goal with fewer step-cost penalties rather\nthan exploring to find the optimal choice, even with limited experience. The\nresults highlight the benefits of integrating LLMs with cognitive\narchitectures, suggesting that this synergy could enhance the modeling and\nunderstanding of complex human decision-making patterns.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09281v2",
    "published_date": "2024-07-12 14:13:06 UTC",
    "updated_date": "2024-08-05 16:16:27 UTC"
  },
  {
    "arxiv_id": "2407.09274v1",
    "title": "Unifying Sequences, Structures, and Descriptions for Any-to-Any Protein Generation with the Large Multimodal Model HelixProtX",
    "authors": [
      "Zhiyuan Chen",
      "Tianhao Chen",
      "Chenggang Xie",
      "Yang Xue",
      "Xiaonan Zhang",
      "Jingbo Zhou",
      "Xiaomin Fang"
    ],
    "abstract": "Proteins are fundamental components of biological systems and can be\nrepresented through various modalities, including sequences, structures, and\ntextual descriptions. Despite the advances in deep learning and scientific\nlarge language models (LLMs) for protein research, current methodologies\npredominantly focus on limited specialized tasks -- often predicting one\nprotein modality from another. These approaches restrict the understanding and\ngeneration of multimodal protein data. In contrast, large multimodal models\nhave demonstrated potential capabilities in generating any-to-any content like\ntext, images, and videos, thus enriching user interactions across various\ndomains. Integrating these multimodal model technologies into protein research\noffers significant promise by potentially transforming how proteins are\nstudied. To this end, we introduce HelixProtX, a system built upon the large\nmultimodal model, aiming to offer a comprehensive solution to protein research\nby supporting any-to-any protein modality generation. Unlike existing methods,\nit allows for the transformation of any input protein modality into any desired\nprotein modality. The experimental results affirm the advanced capabilities of\nHelixProtX, not only in generating functional descriptions from amino acid\nsequences but also in executing critical tasks such as designing protein\nsequences and structures from textual descriptions. Preliminary findings\nindicate that HelixProtX consistently achieves superior accuracy across a range\nof protein-related tasks, outperforming existing state-of-the-art models. By\nintegrating multimodal large models into protein research, HelixProtX opens new\navenues for understanding protein biology, thereby promising to accelerate\nscientific discovery.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09274v1",
    "published_date": "2024-07-12 14:03:02 UTC",
    "updated_date": "2024-07-12 14:03:02 UTC"
  },
  {
    "arxiv_id": "2407.09251v1",
    "title": "Deep Adversarial Defense Against Multilevel-Lp Attacks",
    "authors": [
      "Ren Wang",
      "Yuxuan Li",
      "Alfred Hero"
    ],
    "abstract": "Deep learning models have shown considerable vulnerability to adversarial\nattacks, particularly as attacker strategies become more sophisticated. While\ntraditional adversarial training (AT) techniques offer some resilience, they\noften focus on defending against a single type of attack, e.g., the\n$\\ell_\\infty$-norm attack, which can fail for other types. This paper\nintroduces a computationally efficient multilevel $\\ell_p$ defense, called the\nEfficient Robust Mode Connectivity (EMRC) method, which aims to enhance a deep\nlearning model's resilience against multiple $\\ell_p$-norm attacks. Similar to\nanalytical continuation approaches used in continuous optimization, the method\nblends two $p$-specific adversarially optimal models, the $\\ell_1$- and\n$\\ell_\\infty$-norm AT solutions, to provide good adversarial robustness for a\nrange of $p$. We present experiments demonstrating that our approach performs\nbetter on various attacks as compared to AT-$\\ell_\\infty$, E-AT, and MSD, for\ndatasets/architectures including: CIFAR-10, CIFAR-100 / PreResNet110,\nWideResNet, ViT-Base.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09251v1",
    "published_date": "2024-07-12 13:30:00 UTC",
    "updated_date": "2024-07-12 13:30:00 UTC"
  },
  {
    "arxiv_id": "2407.09249v2",
    "title": "Graph Neural Networks with Model-based Reinforcement Learning for Multi-agent Systems",
    "authors": [
      "Hanxiao Chen"
    ],
    "abstract": "Multi-agent systems (MAS) constitute a significant role in exploring machine\nintelligence and advanced applications. In order to deeply investigate\ncomplicated interactions within MAS scenarios, we originally propose \"GNN for\nMBRL\" model, which utilizes a state-spaced Graph Neural Networks with\nModel-based Reinforcement Learning to address specific MAS missions (e.g.,\nBilliard-Avoidance, Autonomous Driving Cars). In detail, we firstly used GNN\nmodel to predict future states and trajectories of multiple agents, then\napplied the Cross-Entropy Method (CEM) optimized Model Predictive Control to\nassist the ego-agent planning actions and successfully accomplish certain MAS\ntasks.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "The paper abstract has been accepted by NeurIPS 2024 WiML\n  Workshop.(https://www.wiml.org/events/wiml-workshop-%40-neurips-2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.09249v2",
    "published_date": "2024-07-12 13:21:35 UTC",
    "updated_date": "2024-09-29 13:18:51 UTC"
  },
  {
    "arxiv_id": "2407.09247v1",
    "title": "Constrained Intrinsic Motivation for Reinforcement Learning",
    "authors": [
      "Xiang Zheng",
      "Xingjun Ma",
      "Chao Shen",
      "Cong Wang"
    ],
    "abstract": "This paper investigates two fundamental problems that arise when utilizing\nIntrinsic Motivation (IM) for reinforcement learning in Reward-Free\nPre-Training (RFPT) tasks and Exploration with Intrinsic Motivation (EIM)\ntasks: 1) how to design an effective intrinsic objective in RFPT tasks, and 2)\nhow to reduce the bias introduced by the intrinsic objective in EIM tasks.\nExisting IM methods suffer from static skills, limited state coverage, sample\ninefficiency in RFPT tasks, and suboptimality in EIM tasks. To tackle these\nproblems, we propose \\emph{Constrained Intrinsic Motivation (CIM)} for RFPT and\nEIM tasks, respectively: 1) CIM for RFPT maximizes the lower bound of the\nconditional state entropy subject to an alignment constraint on the state\nencoder network for efficient dynamic and diverse skill discovery and state\ncoverage maximization; 2) CIM for EIM leverages constrained policy optimization\nto adaptively adjust the coefficient of the intrinsic objective to mitigate the\ndistraction from the intrinsic objective. In various MuJoCo robotics\nenvironments, we empirically show that CIM for RFPT greatly surpasses fifteen\nIM methods for unsupervised skill discovery in terms of skill diversity, state\ncoverage, and fine-tuning performance. Additionally, we showcase the\neffectiveness of CIM for EIM in redeeming intrinsic rewards when task rewards\nare exposed from the beginning. Our code is available at\nhttps://github.com/x-zheng16/CIM.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.09247v1",
    "published_date": "2024-07-12 13:20:52 UTC",
    "updated_date": "2024-07-12 13:20:52 UTC"
  },
  {
    "arxiv_id": "2407.12863v2",
    "title": "Token-Supervised Value Models for Enhancing Mathematical Problem-Solving Capabilities of Large Language Models",
    "authors": [
      "Jung Hyun Lee",
      "June Yong Yang",
      "Byeongho Heo",
      "Dongyoon Han",
      "Kyungsu Kim",
      "Eunho Yang",
      "Kang Min Yoo"
    ],
    "abstract": "With the rapid advancement of test-time compute search strategies to improve\nthe mathematical problem-solving capabilities of large language models (LLMs),\nthe need for building robust verifiers has become increasingly important.\nHowever, all these inference strategies rely on existing verifiers originally\ndesigned for Best-of-N search, which makes them sub-optimal for tree search\ntechniques at test time. During tree search, existing verifiers can only offer\nindirect and implicit assessments of partial solutions or under-value\nprospective intermediate steps, thus resulting in the premature pruning of\npromising intermediate steps. To overcome these limitations, we propose\ntoken-supervised value models (TVMs) - a new class of verifiers that assign\neach token a probability that reflects the likelihood of reaching the correct\nfinal answer. This new token-level supervision enables TVMs to directly and\nexplicitly evaluate partial solutions, effectively distinguishing between\npromising and incorrect intermediate steps during tree search at test time.\nExperimental results demonstrate that combining tree-search-based inference\nstrategies with TVMs significantly improves the accuracy of LLMs in\nmathematical problem-solving tasks, surpassing the performance of existing\nverifiers.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12863v2",
    "published_date": "2024-07-12 13:16:50 UTC",
    "updated_date": "2025-03-10 14:24:29 UTC"
  },
  {
    "arxiv_id": "2407.09239v1",
    "title": "FedVAE: Trajectory privacy preserving based on Federated Variational AutoEncoder",
    "authors": [
      "Yuchen Jiang",
      "Ying Wu",
      "Shiyao Zhang",
      "James J. Q. Yu"
    ],
    "abstract": "The use of trajectory data with abundant spatial-temporal information is\npivotal in Intelligent Transport Systems (ITS) and various traffic system\ntasks. Location-Based Services (LBS) capitalize on this trajectory data to\noffer users personalized services tailored to their location information.\nHowever, this trajectory data contains sensitive information about users'\nmovement patterns and habits, necessitating confidentiality and protection from\nunknown collectors. To address this challenge, privacy-preserving methods like\nK-anonymity and Differential Privacy have been proposed to safeguard private\ninformation in the dataset. Despite their effectiveness, these methods can\nimpact the original features by introducing perturbations or generating\nunrealistic trajectory data, leading to suboptimal performance in downstream\ntasks. To overcome these limitations, we propose a Federated Variational\nAutoEncoder (FedVAE) approach, which effectively generates a new trajectory\ndataset while preserving the confidentiality of private information and\nretaining the structure of the original features. In addition, FedVAE leverages\nVariational AutoEncoder (VAE) to maintain the original feature space and\ngenerate new trajectory data, and incorporates Federated Learning (FL) during\nthe training stage, ensuring that users' data remains locally stored to protect\ntheir personal information. The results demonstrate its superior performance\ncompared to other existing methods, affirming FedVAE as a promising solution\nfor enhancing data privacy and utility in location-based applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "2023 IEEE 98th Vehicular Technology Conference",
    "pdf_url": "http://arxiv.org/pdf/2407.09239v1",
    "published_date": "2024-07-12 13:10:59 UTC",
    "updated_date": "2024-07-12 13:10:59 UTC"
  },
  {
    "arxiv_id": "2407.09221v1",
    "title": "Evaluating AI Evaluation: Perils and Prospects",
    "authors": [
      "John Burden"
    ],
    "abstract": "As AI systems appear to exhibit ever-increasing capability and generality,\nassessing their true potential and safety becomes paramount. This paper\ncontends that the prevalent evaluation methods for these systems are\nfundamentally inadequate, heightening the risks and potential hazards\nassociated with AI. I argue that a reformation is required in the way we\nevaluate AI systems and that we should look towards cognitive sciences for\ninspiration in our approaches, which have a longstanding tradition of assessing\ngeneral intelligence across diverse species. We will identify some of the\ndifficulties that need to be overcome when applying cognitively-inspired\napproaches to general-purpose AI systems and also analyse the emerging area of\n\"Evals\". The paper concludes by identifying promising research pathways that\ncould refine AI evaluation, advancing it towards a rigorous scientific domain\nthat contributes to the development of safe AI systems.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "Pre-print",
    "pdf_url": "http://arxiv.org/pdf/2407.09221v1",
    "published_date": "2024-07-12 12:37:13 UTC",
    "updated_date": "2024-07-12 12:37:13 UTC"
  },
  {
    "arxiv_id": "2407.09212v4",
    "title": "Generating $SROI^-$ Ontologies via Knowledge Graph Query Embedding Learning",
    "authors": [
      "Yunjie He",
      "Daniel Hernandez",
      "Mojtaba Nayyeri",
      "Bo Xiong",
      "Yuqicheng Zhu",
      "Evgeny Kharlamov",
      "Steffen Staab"
    ],
    "abstract": "Query embedding approaches answer complex logical queries over incomplete\nknowledge graphs (KGs) by computing and operating on low-dimensional vector\nrepresentations of entities, relations, and queries. However, current query\nembedding models heavily rely on excessively parameterized neural networks and\ncannot explain the knowledge learned from the graph. We propose a novel query\nembedding method, AConE, which explains the knowledge learned from the graph in\nthe form of $SROI^-$ description logic axioms while being more\nparameter-efficient than most existing approaches. AConE associates queries to\na $SROI^-$ description logic concept. Every $SROI^-$ concept is embedded as a\ncone in complex vector space, and each $SROI^-$ relation is embedded as a\ntransformation that rotates and scales cones. We show theoretically that AConE\ncan learn $SROI^-$ axioms, and defines an algebra whose operations correspond\none to one to $SROI^-$ description logic concept constructs. Our empirical\nstudy on multiple query datasets shows that AConE achieves superior results\nover previous baselines with fewer parameters. Notably on the WN18RR dataset,\nAConE achieves significant improvement over baseline models. We provide\ncomprehensive analyses showing that the capability to represent axioms\npositively impacts the results of query answering.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by ECAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.09212v4",
    "published_date": "2024-07-12 12:20:39 UTC",
    "updated_date": "2024-08-27 13:40:13 UTC"
  },
  {
    "arxiv_id": "2407.09585v1",
    "title": "A Scale-Invariant Diagnostic Approach Towards Understanding Dynamics of Deep Neural Networks",
    "authors": [
      "Ambarish Moharil",
      "Damian Tamburri",
      "Indika Kumara",
      "Willem-Jan Van Den Heuvel",
      "Alireza Azarfar"
    ],
    "abstract": "This paper introduces a scale-invariant methodology employing \\textit{Fractal\nGeometry} to analyze and explain the nonlinear dynamics of complex\nconnectionist systems. By leveraging architectural self-similarity in Deep\nNeural Networks (DNNs), we quantify fractal dimensions and \\textit{roughness}\nto deeply understand their dynamics and enhance the quality of\n\\textit{intrinsic} explanations. Our approach integrates principles from Chaos\nTheory to improve visualizations of fractal evolution and utilizes a\nGraph-Based Neural Network for reconstructing network topology. This strategy\naims at advancing the \\textit{intrinsic} explainability of connectionist\nArtificial Intelligence (AI) systems.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09585v1",
    "published_date": "2024-07-12 11:54:05 UTC",
    "updated_date": "2024-07-12 11:54:05 UTC"
  },
  {
    "arxiv_id": "2407.09197v3",
    "title": "A Chatbot for Asylum-Seeking Migrants in Europe",
    "authors": [
      "Bettina Fazzinga",
      "Elena Palmieri",
      "Margherita Vestoso",
      "Luca Bolognini",
      "Andrea Galassi",
      "Filippo Furfaro",
      "Paolo Torroni"
    ],
    "abstract": "We present ACME: A Chatbot for asylum-seeking Migrants in Europe. ACME relies\non computational argumentation and aims to help migrants identify the highest\nlevel of protection they can apply for. This would contribute to a more\nsustainable migration by reducing the load on territorial commissions, Courts,\nand humanitarian organizations supporting asylum applicants. We describe the\nbackground context, system architecture, underlying technologies, and a case\nstudy used to validate the tool with domain experts.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Copyright 2024 IEEE",
    "pdf_url": "http://arxiv.org/pdf/2407.09197v3",
    "published_date": "2024-07-12 11:53:40 UTC",
    "updated_date": "2025-01-31 13:11:47 UTC"
  },
  {
    "arxiv_id": "2407.09191v1",
    "title": "From Easy to Hard: Learning Curricular Shape-aware Features for Robust Panoptic Scene Graph Generation",
    "authors": [
      "Hanrong Shi",
      "Lin Li",
      "Jun Xiao",
      "Yueting Zhuang",
      "Long Chen"
    ],
    "abstract": "Panoptic Scene Graph Generation (PSG) aims to generate a comprehensive\ngraph-structure representation based on panoptic segmentation masks. Despite\nremarkable progress in PSG, almost all existing methods neglect the importance\nof shape-aware features, which inherently focus on the contours and boundaries\nof objects. To bridge this gap, we propose a model-agnostic Curricular\nshApe-aware FEature (CAFE) learning strategy for PSG. Specifically, we\nincorporate shape-aware features (i.e., mask features and boundary features)\ninto PSG, moving beyond reliance solely on bbox features. Furthermore, drawing\ninspiration from human cognition, we propose to integrate shape-aware features\nin an easy-to-hard manner. To achieve this, we categorize the predicates into\nthree groups based on cognition learning difficulty and correspondingly divide\nthe training process into three stages. Each stage utilizes a specialized\nrelation classifier to distinguish specific groups of predicates. As the\nlearning difficulty of predicates increases, these classifiers are equipped\nwith features of ascending complexity. We also incorporate knowledge\ndistillation to retain knowledge acquired in earlier stages. Due to its\nmodel-agnostic nature, CAFE can be seamlessly incorporated into any PSG model.\nExtensive experiments and ablations on two PSG tasks under both robust and\nzero-shot PSG have attested to the superiority and robustness of our proposed\nCAFE, which outperforms existing state-of-the-art methods by a large margin.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by IJCV",
    "pdf_url": "http://arxiv.org/pdf/2407.09191v1",
    "published_date": "2024-07-12 11:48:33 UTC",
    "updated_date": "2024-07-12 11:48:33 UTC"
  },
  {
    "arxiv_id": "2407.09187v1",
    "title": "Enhancing Depressive Post Detection in Bangla: A Comparative Study of TF-IDF, BERT and FastText Embeddings",
    "authors": [
      "Saad Ahmed Sazan",
      "Mahdi H. Miraz",
      "A B M Muntasir Rahman"
    ],
    "abstract": "Due to massive adoption of social media, detection of users' depression\nthrough social media analytics bears significant importance, particularly for\nunderrepresented languages, such as Bangla. This study introduces a\nwell-grounded approach to identify depressive social media posts in Bangla, by\nemploying advanced natural language processing techniques. The dataset used in\nthis work, annotated by domain experts, includes both depressive and\nnon-depressive posts, ensuring high-quality data for model training and\nevaluation. To address the prevalent issue of class imbalance, we utilised\nrandom oversampling for the minority class, thereby enhancing the model's\nability to accurately detect depressive posts. We explored various numerical\nrepresentation techniques, including Term Frequency-Inverse Document Frequency\n(TF-IDF), Bidirectional Encoder Representations from Transformers (BERT)\nembedding and FastText embedding, by integrating them with a deep\nlearning-based Convolutional Neural Network-Bidirectional Long Short-Term\nMemory (CNN-BiLSTM) model. The results obtained through extensive\nexperimentation, indicate that the BERT approach performed better the others,\nachieving a F1-score of 84%. This indicates that BERT, in combination with the\nCNN-BiLSTM architecture, effectively recognises the nuances of Bangla texts\nrelevant to depressive contents. Comparative analysis with the existing\nstate-of-the-art methods demonstrates that our approach with BERT embedding\nperforms better than others in terms of evaluation metrics and the reliability\nof dataset annotations. Our research significantly contribution to the\ndevelopment of reliable tools for detecting depressive posts in the Bangla\nlanguage. By highlighting the efficacy of different embedding techniques and\ndeep learning models, this study paves the way for improved mental health\nmonitoring through social media platforms.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09187v1",
    "published_date": "2024-07-12 11:40:17 UTC",
    "updated_date": "2024-07-12 11:40:17 UTC"
  },
  {
    "arxiv_id": "2407.09186v2",
    "title": "Variational Inference via Smoothed Particle Hydrodynamics",
    "authors": [
      "Yongchao Huang"
    ],
    "abstract": "A new variational inference method, SPH-ParVI, based on smoothed particle\nhydrodynamics (SPH), is proposed for sampling partially known densities (e.g.\nup to a constant) or sampling using gradients. SPH-ParVI simulates the flow of\na fluid under external effects driven by the target density; transient or\nsteady state of the fluid approximates the target density. The continuum fluid\nis modelled as an interacting particle system (IPS) via SPH, where each\nparticle carries smoothed properties, interacts and evolves as per the\nNavier-Stokes equations. This mesh-free, Lagrangian simulation method offers\nfast, flexible, scalable and deterministic sampling and inference for a class\nof probabilistic models such as those encountered in Bayesian inference and\ngenerative modelling.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09186v2",
    "published_date": "2024-07-12 11:38:41 UTC",
    "updated_date": "2024-07-26 17:26:45 UTC"
  },
  {
    "arxiv_id": "2407.09174v3",
    "title": "DART: An Automated End-to-End Object Detection Pipeline with Data Diversification, Open-Vocabulary Bounding Box Annotation, Pseudo-Label Review, and Model Training",
    "authors": [
      "Chen Xin",
      "Andreas Hartel",
      "Enkelejda Kasneci"
    ],
    "abstract": "Accurate real-time object detection is vital across numerous industrial\napplications, from safety monitoring to quality control. Traditional\napproaches, however, are hindered by arduous manual annotation and data\ncollection, struggling to adapt to ever-changing environments and novel target\nobjects. To address these limitations, this paper presents DART, an innovative\nautomated end-to-end pipeline that revolutionizes object detection workflows\nfrom data collection to model evaluation. It eliminates the need for laborious\nhuman labeling and extensive data collection while achieving outstanding\naccuracy across diverse scenarios. DART encompasses four key stages: (1) Data\nDiversification using subject-driven image generation (DreamBooth with SDXL),\n(2) Annotation via open-vocabulary object detection (Grounding DINO) to\ngenerate bounding box and class labels, (3) Review of generated images and\npseudo-labels by large multimodal models (InternVL-1.5 and GPT-4o) to guarantee\ncredibility, and (4) Training of real-time object detectors (YOLOv8 and\nYOLOv10) using the verified data. We apply DART to a self-collected dataset of\nconstruction machines named Liebherr Product, which contains over 15K\nhigh-quality images across 23 categories. The current instantiation of DART\nsignificantly increases average precision (AP) from 0.064 to 0.832. Its modular\ndesign ensures easy exchangeability and extensibility, allowing for future\nalgorithm upgrades, seamless integration of new object categories, and\nadaptability to customized environments without manual labeling and additional\ndata collection. The code and dataset are released at\nhttps://github.com/chen-xin-94/DART.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09174v3",
    "published_date": "2024-07-12 11:16:44 UTC",
    "updated_date": "2024-07-29 09:14:07 UTC"
  },
  {
    "arxiv_id": "2407.09172v1",
    "title": "Machine Apophenia: The Kaleidoscopic Generation of Architectural Images",
    "authors": [
      "Alexey Tikhonov",
      "Dmitry Sinyavin"
    ],
    "abstract": "This study investigates the application of generative artificial intelligence\nin architectural design. We present a novel methodology that combines multiple\nneural networks to create an unsupervised and unmoderated stream of unique\narchitectural images. Our approach is grounded in the conceptual framework\ncalled machine apophenia. We hypothesize that neural networks, trained on\ndiverse human-generated data, internalize aesthetic preferences and tend to\nproduce coherent designs even from random inputs. The methodology involves an\niterative process of image generation, description, and refinement, resulting\nin captioned architectural postcards automatically shared on several social\nmedia platforms. Evaluation and ablation studies show the improvement both in\ntechnical and aesthetic metrics of resulting images on each step.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "68T01, 68U05, 00A66, 00A67",
      "I.2.1; I.3.3; J.5; H.5.1"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09172v1",
    "published_date": "2024-07-12 11:11:19 UTC",
    "updated_date": "2024-07-12 11:11:19 UTC"
  },
  {
    "arxiv_id": "2407.09167v3",
    "title": "SE(3)-bi-equivariant Transformers for Point Cloud Assembly",
    "authors": [
      "Ziming Wang",
      "Rebecka Jörnsten"
    ],
    "abstract": "Given a pair of point clouds, the goal of assembly is to recover a rigid\ntransformation that aligns one point cloud to the other. This task is\nchallenging because the point clouds may be non-overlapped, and they may have\narbitrary initial positions. To address these difficulties, we propose a\nmethod, called SE(3)-bi-equivariant transformer (BITR), based on the\nSE(3)-bi-equivariance prior of the task: it guarantees that when the inputs are\nrigidly perturbed, the output will transform accordingly. Due to its\nequivariance property, BITR can not only handle non-overlapped PCs, but also\nguarantee robustness against initial positions. Specifically, BITR first\nextracts features of the inputs using a novel $SE(3) \\times SE(3)$-transformer,\nand then projects the learned feature to group SE(3) as the output. Moreover,\nwe theoretically show that swap and scale equivariances can be incorporated\ninto BITR, thus it further guarantees stable performance under scaling and\nswapping the inputs. We experimentally show the effectiveness of BITR in\npractical tasks.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Camera ready for NeurIPS24",
    "pdf_url": "http://arxiv.org/pdf/2407.09167v3",
    "published_date": "2024-07-12 11:01:28 UTC",
    "updated_date": "2024-10-25 19:05:08 UTC"
  },
  {
    "arxiv_id": "2407.09165v1",
    "title": "Robust Yet Efficient Conformal Prediction Sets",
    "authors": [
      "Soroush H. Zargarbashi",
      "Mohammad Sadegh Akhondzadeh",
      "Aleksandar Bojchevski"
    ],
    "abstract": "Conformal prediction (CP) can convert any model's output into prediction sets\nguaranteed to include the true label with any user-specified probability.\nHowever, same as the model itself, CP is vulnerable to adversarial test\nexamples (evasion) and perturbed calibration data (poisoning). We derive\nprovably robust sets by bounding the worst-case change in conformity scores.\nOur tighter bounds lead to more efficient sets. We cover both continuous and\ndiscrete (sparse) data and our guarantees work both for evasion and poisoning\nattacks (on both features and labels).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Proceedings of the 41st International Conference on Machine Learning",
    "pdf_url": "http://arxiv.org/pdf/2407.09165v1",
    "published_date": "2024-07-12 10:59:44 UTC",
    "updated_date": "2024-07-12 10:59:44 UTC"
  },
  {
    "arxiv_id": "2407.09164v5",
    "title": "TPIA: Towards Target-specific Prompt Injection Attack against Code-oriented Large Language Models",
    "authors": [
      "Yuchen Yang",
      "Hongwei Yao",
      "Bingrun Yang",
      "Yiling He",
      "Yiming Li",
      "Tianwei Zhang",
      "Zhan Qin"
    ],
    "abstract": "Recently, code-oriented large language models (Code LLMs) have been widely\nand successfully exploited to simplify and facilitate programming.\nUnfortunately, a few pioneering works revealed that these Code LLMs are\nvulnerable to backdoor and adversarial attacks. The former poisons the training\ndata or model parameters, hijacking the LLMs to generate malicious code\nsnippets when encountering the trigger. The latter crafts malicious adversarial\ninput codes to reduce the quality of the generated codes. In this paper, we\nreveal that both attacks have some inherent limitations: backdoor attacks rely\non the adversary's capability of controlling the model training process, which\nmay not be practical; adversarial attacks struggle with fulfilling specific\nmalicious purposes. To alleviate these problems, this paper presents a novel\nattack paradigm against Code LLMs, namely target-specific prompt injection\nattack (TPIA). TPIA generates non-functional perturbations containing the\ninformation of malicious instructions and inserts them into the victim's code\ncontext by spreading them into potentially used dependencies (e.g., packages or\nRAG's knowledge base). It induces the Code LLMs to generate attacker-specified\nmalicious code snippets at the target location. In general, we compress the\nattacker-specified malicious objective into the perturbation by adversarial\noptimization based on greedy token search. We collect 13 representative\nmalicious objectives to design 31 threat cases for three popular programming\nlanguages. We show that our TPIA can successfully attack three representative\nopen-source Code LLMs (with an attack success rate of up to 97.9%) and two\nmainstream commercial Code LLM-integrated applications (with an attack success\nrate of over 90%) in all threat cases, using only a 12-token non-functional\nperturbation.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09164v5",
    "published_date": "2024-07-12 10:59:32 UTC",
    "updated_date": "2025-03-04 04:43:47 UTC"
  },
  {
    "arxiv_id": "2407.09162v2",
    "title": "Exploring State Space and Reasoning by Elimination in Tsetlin Machines",
    "authors": [
      "Ahmed K. Kadhim",
      "Ole-Christoffer Granmo",
      "Lei Jiao",
      "Rishad Shafik"
    ],
    "abstract": "The Tsetlin Machine (TM) has gained significant attention in Machine Learning\n(ML). By employing logical fundamentals, it facilitates pattern learning and\nrepresentation, offering an alternative approach for developing comprehensible\nArtificial Intelligence (AI) with a specific focus on pattern classification in\nthe form of conjunctive clauses. In the domain of Natural Language Processing\n(NLP), TM is utilised to construct word embedding and describe target words\nusing clauses. To enhance the descriptive capacity of these clauses, we study\nthe concept of Reasoning by Elimination (RbE) in clauses' formulation, which\ninvolves incorporating feature negations to provide a more comprehensive\nrepresentation. In more detail, this paper employs the Tsetlin Machine\nAuto-Encoder (TM-AE) architecture to generate dense word vectors, aiming at\ncapturing contextual information by extracting feature-dense vectors for a\ngiven vocabulary. Thereafter, the principle of RbE is explored to improve\ndescriptivity and optimise the performance of the TM. Specifically, the\nspecificity parameter s and the voting margin parameter T are leveraged to\nregulate feature distribution in the state space, resulting in a dense\nrepresentation of information for each clause. In addition, we investigate the\nstate spaces of TM-AE, especially for the forgotten/excluded features.\nEmpirical investigations on artificially generated data, the IMDB dataset, and\nthe 20 Newsgroups dataset showcase the robustness of the TM, with accuracy\nreaching 90.62\\% for the IMDB.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.09162v2",
    "published_date": "2024-07-12 10:58:01 UTC",
    "updated_date": "2024-07-17 09:42:34 UTC"
  },
  {
    "arxiv_id": "2407.09157v1",
    "title": "Movie Recommendation with Poster Attention via Multi-modal Transformer Feature Fusion",
    "authors": [
      "Linhan Xia",
      "Yicheng Yang",
      "Ziou Chen",
      "Zheng Yang",
      "Shengxin Zhu"
    ],
    "abstract": "Pre-trained models learn general representations from large datsets which can\nbe fine-turned for specific tasks to significantly reduce training time.\nPre-trained models like generative pretrained transformers (GPT), bidirectional\nencoder representations from transformers (BERT), vision transfomers (ViT) have\nbecome a cornerstone of current research in machine learning. This study\nproposes a multi-modal movie recommendation system by extract features of the\nwell designed posters for each movie and the narrative text description of the\nmovie. This system uses the BERT model to extract the information of text\nmodality, the ViT model applied to extract the information of poster/image\nmodality, and the Transformer architecture for feature fusion of all modalities\nto predict users' preference. The integration of pre-trained foundational\nmodels with some smaller data sets in downstream applications capture\nmulti-modal content features in a more comprehensive manner, thereby providing\nmore accurate recommendations. The efficiency of the proof-of-concept model is\nverified by the standard benchmark problem the MovieLens 100K and 1M datasets.\nThe prediction accuracy of user ratings is enhanced in comparison to the\nbaseline algorithm, thereby demonstrating the potential of this cross-modal\nalgorithm to be applied for movie or video recommendation.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09157v1",
    "published_date": "2024-07-12 10:44:51 UTC",
    "updated_date": "2024-07-12 10:44:51 UTC"
  },
  {
    "arxiv_id": "2407.09152v1",
    "title": "The Two Sides of the Coin: Hallucination Generation and Detection with LLMs as Evaluators for LLMs",
    "authors": [
      "Anh Thu Maria Bui",
      "Saskia Felizitas Brech",
      "Natalie Hußfeldt",
      "Tobias Jennert",
      "Melanie Ullrich",
      "Timo Breuer",
      "Narjes Nikzad Khasmakhi",
      "Philipp Schaer"
    ],
    "abstract": "Hallucination detection in Large Language Models (LLMs) is crucial for\nensuring their reliability. This work presents our participation in the CLEF\nELOQUENT HalluciGen shared task, where the goal is to develop evaluators for\nboth generating and detecting hallucinated content. We explored the\ncapabilities of four LLMs: Llama 3, Gemma, GPT-3.5 Turbo, and GPT-4, for this\npurpose. We also employed ensemble majority voting to incorporate all four\nmodels for the detection task. The results provide valuable insights into the\nstrengths and weaknesses of these LLMs in handling hallucination generation and\ndetection tasks.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Paper accepted at ELOQUENT@CLEF'24",
    "pdf_url": "http://arxiv.org/pdf/2407.09152v1",
    "published_date": "2024-07-12 10:34:46 UTC",
    "updated_date": "2024-07-12 10:34:46 UTC"
  },
  {
    "arxiv_id": "2407.09136v1",
    "title": "Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors",
    "authors": [
      "Nico Daheim",
      "Jakub Macina",
      "Manu Kapur",
      "Iryna Gurevych",
      "Mrinmaya Sachan"
    ],
    "abstract": "Large language models (LLMs) present an opportunity to scale high-quality\npersonalized education to all. A promising approach towards this means is to\nbuild dialog tutoring models that scaffold students' problem-solving. However,\neven though existing LLMs perform well in solving reasoning questions, they\nstruggle to precisely detect student's errors and tailor their feedback to\nthese errors. Inspired by real-world teaching practice where teachers identify\nstudent errors and customize their response based on them, we focus on\nverifying student solutions and show how grounding to such verification\nimproves the overall quality of tutor response generation. We collect a dataset\nof 1K stepwise math reasoning chains with the first error step annotated by\nteachers. We show empirically that finding the mistake in a student solution is\nchallenging for current models. We propose and evaluate several verifiers for\ndetecting these errors. Using both automatic and human evaluation we show that\nthe student solution verifiers steer the generation model towards highly\ntargeted responses to student errors which are more often correct with less\nhallucinations compared to existing baselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint. Nico Daheim and Jakub Macina contributed equally. Code and\n  dataset can be found under: https://github.com/eth-lre/verify-then-generate",
    "pdf_url": "http://arxiv.org/pdf/2407.09136v1",
    "published_date": "2024-07-12 10:11:40 UTC",
    "updated_date": "2024-07-12 10:11:40 UTC"
  },
  {
    "arxiv_id": "2407.09121v1",
    "title": "Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training",
    "authors": [
      "Youliang Yuan",
      "Wenxiang Jiao",
      "Wenxuan Wang",
      "Jen-tse Huang",
      "Jiahao Xu",
      "Tian Liang",
      "Pinjia He",
      "Zhaopeng Tu"
    ],
    "abstract": "This study addresses a critical gap in safety tuning practices for Large\nLanguage Models (LLMs) by identifying and tackling a refusal position bias\nwithin safety tuning data, which compromises the models' ability to\nappropriately refuse generating unsafe content. We introduce a novel approach,\nDecoupled Refusal Training (DeRTa), designed to empower LLMs to refuse\ncompliance to harmful prompts at any response position, significantly enhancing\ntheir safety capabilities. DeRTa incorporates two novel components: (1) Maximum\nLikelihood Estimation (MLE) with Harmful Response Prefix, which trains models\nto recognize and avoid unsafe content by appending a segment of harmful\nresponse to the beginning of a safe response, and (2) Reinforced Transition\nOptimization (RTO), which equips models with the ability to transition from\npotential harm to safety refusal consistently throughout the harmful response\nsequence. Our empirical evaluation, conducted using LLaMA3 and Mistral model\nfamilies across six attack scenarios, demonstrates that our method not only\nimproves model safety without compromising performance but also surpasses\nwell-known models such as GPT-4 in defending against attacks. Importantly, our\napproach successfully defends recent advanced attack methods (e.g., CodeAttack)\nthat have jailbroken GPT-4 and LLaMA3-70B-Instruct. Our code and data can be\nfound at https://github.com/RobustNLP/DeRTa.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09121v1",
    "published_date": "2024-07-12 09:36:33 UTC",
    "updated_date": "2024-07-12 09:36:33 UTC"
  },
  {
    "arxiv_id": "2407.09111v2",
    "title": "Inference Optimization of Foundation Models on AI Accelerators",
    "authors": [
      "Youngsuk Park",
      "Kailash Budhathoki",
      "Liangfu Chen",
      "Jonas Kübler",
      "Jiaji Huang",
      "Matthäus Kleindessner",
      "Jun Huan",
      "Volkan Cevher",
      "Yida Wang",
      "George Karypis"
    ],
    "abstract": "Powerful foundation models, including large language models (LLMs), with\nTransformer architectures have ushered in a new era of Generative AI across\nvarious industries. Industry and research community have witnessed a large\nnumber of new applications, based on those foundation models. Such applications\ninclude question and answer, customer services, image and video generation, and\ncode completions, among others. However, as the number of model parameters\nreaches to hundreds of billions, their deployment incurs prohibitive inference\ncosts and high latency in real-world scenarios. As a result, the demand for\ncost-effective and fast inference using AI accelerators is ever more higher. To\nthis end, our tutorial offers a comprehensive discussion on complementary\ninference optimization techniques using AI accelerators. Beginning with an\noverview of basic Transformer architectures and deep learning system\nframeworks, we deep dive into system optimization techniques for fast and\nmemory-efficient attention computations and discuss how they can be implemented\nefficiently on AI accelerators. Next, we describe architectural elements that\nare key for fast transformer inference. Finally, we examine various model\ncompression and fast decoding strategies in the same context.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "[v2] Tutorial website added [v1] Tutorial published at KDD 2024.\n  Camera-ready version",
    "pdf_url": "http://arxiv.org/pdf/2407.09111v2",
    "published_date": "2024-07-12 09:24:34 UTC",
    "updated_date": "2024-10-01 17:10:07 UTC"
  },
  {
    "arxiv_id": "2407.09105v6",
    "title": "Enhancing Training Efficiency Using Packing with Flash Attention",
    "authors": [
      "Achintya Kundu",
      "Rhui Dih Lee",
      "Laura Wynter",
      "Raghu Kiran Ganti",
      "Mayank Mishra"
    ],
    "abstract": "Padding is often used in tuning LLM models by adding special tokens to\nshorter training examples to match the length of the longest sequence in each\nbatch. While this ensures uniformity for batch processing, it introduces\ninefficiencies by including irrelevant padding tokens in the computation and\nwastes GPU resources. Hugging Face SFT trainer has always offered the option to\nuse packing to combine multiple training examples, allowing for maximal\nutilization of GPU resources. However, up till now, it did not offer proper\nmasking of each packed training example. This capability has been added to\nHugging Face Transformers 4.44. We analyse this new feature and show the\nbenefits across different variations of packing.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09105v6",
    "published_date": "2024-07-12 09:10:37 UTC",
    "updated_date": "2024-09-01 00:26:46 UTC"
  },
  {
    "arxiv_id": "2407.09103v1",
    "title": "DANIEL: A fast Document Attention Network for Information Extraction and Labelling of handwritten documents",
    "authors": [
      "Thomas Constum",
      "Pierrick Tranouez",
      "Thierry Paquet"
    ],
    "abstract": "Information extraction from handwritten documents involves traditionally\nthree distinct steps: Document Layout Analysis, Handwritten Text Recognition,\nand Named Entity Recognition. Recent approaches have attempted to integrate\nthese steps into a single process using fully end-to-end architectures. Despite\nthis, these integrated approaches have not yet matched the performance of\nlanguage models, when applied to information extraction in plain text. In this\npaper, we introduce DANIEL (Document Attention Network for Information\nExtraction and Labelling), a fully end-to-end architecture integrating a\nlanguage model and designed for comprehensive handwritten document\nunderstanding. DANIEL performs layout recognition, handwriting recognition, and\nnamed entity recognition on full-page documents. Moreover, it can\nsimultaneously learn across multiple languages, layouts, and tasks. For named\nentity recognition, the ontology to be applied can be specified via the input\nprompt. The architecture employs a convolutional encoder capable of processing\nimages of any size without resizing, paired with an autoregressive decoder\nbased on a transformer-based language model. DANIEL achieves competitive\nresults on four datasets, including a new state-of-the-art performance on RIMES\n2009 and M-POPP for Handwriting Text Recognition, and IAM NER for Named Entity\nRecognition. Furthermore, DANIEL is much faster than existing approaches.\n  We provide the source code and the weights of the trained models at\n\\url{https://github.com/Shulk97/daniel}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09103v1",
    "published_date": "2024-07-12 09:09:56 UTC",
    "updated_date": "2024-07-12 09:09:56 UTC"
  },
  {
    "arxiv_id": "2407.09099v2",
    "title": "Music Proofreading with RefinPaint: Where and How to Modify Compositions given Context",
    "authors": [
      "Pedro Ramoneda",
      "Martin Rocamora",
      "Taketo Akama"
    ],
    "abstract": "Autoregressive generative transformers are key in music generation, producing\ncoherent compositions but facing challenges in human-machine collaboration. We\npropose RefinPaint, an iterative technique that improves the sampling process.\nIt does this by identifying the weaker music elements using a feedback model,\nwhich then informs the choices for resampling by an inpainting model. This\ndual-focus methodology not only facilitates the machine's ability to improve\nits automatic inpainting generation through repeated cycles but also offers a\nvaluable tool for humans seeking to refine their compositions with automatic\nproofreading. Experimental results suggest RefinPaint's effectiveness in\ninpainting and proofreading tasks, demonstrating its value for refining music\ncreated by both machines and humans. This approach not only facilitates\ncreativity but also aids amateur composers in improving their work.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09099v2",
    "published_date": "2024-07-12 08:52:27 UTC",
    "updated_date": "2024-11-10 18:07:09 UTC"
  },
  {
    "arxiv_id": "2407.09096v3",
    "title": "STD-PLM: Understanding Both Spatial and Temporal Properties of Spatial-Temporal Data with PLM",
    "authors": [
      "YiHeng Huang",
      "Xiaowei Mao",
      "Shengnan Guo",
      "Yubin Chen",
      "Junfeng Shen",
      "Tiankuo Li",
      "Youfang Lin",
      "Huaiyu Wan"
    ],
    "abstract": "Spatial-temporal forecasting and imputation are important for real-world\nintelligent systems. Most existing methods are tailored for individual\nforecasting or imputation tasks but are not designed for both. Additionally,\nthey are less effective for zero-shot and few-shot learning. While pre-trained\nlanguage model (PLM) have exhibited strong pattern recognition and reasoning\nabilities across various tasks, including few-shot and zero-shot learning,\ntheir applications in spatial-temporal data understanding has been constrained\nby insufficient modeling of complex correlations such as the temporal\ncorrelations, spatial connectivity, non-pairwise and high-order\nspatial-temporal correlations within data. In this paper, we propose STD-PLM\nfor understanding both spatial and temporal properties of\n\\underline{S}patial-\\underline{T}emporal \\underline{D}ata with \\underline{PLM},\nwhich is capable of implementing both spatial-temporal forecasting and\nimputation tasks. STD-PLM understands spatial-temporal correlations via\nexplicitly designed spatial and temporal tokenizers. Topology-aware node\nembeddings are designed for PLM to comprehend and exploit the topology\nstructure of data in inductive manner. Furthermore, to mitigate the efficiency\nissues introduced by the PLM, we design a sandglass attention module (SGA)\ncombined with a specific constrained loss function, which significantly\nimproves the model's efficiency while ensuring performance. Extensive\nexperiments demonstrate that STD-PLM exhibits competitive performance and\ngeneralization capabilities across the forecasting and imputation tasks on\nvarious datasets. Moreover, STD-PLM achieves promising results on both few-shot\nand zero-shot tasks.The code is made available at\n\\href{https://anonymous.4open.science/r/STD-PLM-F3BA}{https://anonymous.4open.science/r/STD-PLM-F3BA}",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09096v3",
    "published_date": "2024-07-12 08:48:16 UTC",
    "updated_date": "2024-09-10 05:10:43 UTC"
  },
  {
    "arxiv_id": "2407.09093v2",
    "title": "On Exact Bit-level Reversible Transformers Without Changing Architectures",
    "authors": [
      "Guoqiang Zhang",
      "J. P. Lewis",
      "W. B. Kleijn"
    ],
    "abstract": "Various reversible deep neural networks (DNN) models have been proposed to\nreduce memory consumption in the training process. However, almost all existing\nreversible DNNs either require special non-standard architectures or are\nconstructed by modifying existing DNN architectures considerably to enable\nreversibility. In this work we present the BDIA-transformer, which is an exact\nbit-level reversible transformer that uses an unchanged standard architecture\nfor inference. The basic idea is to first treat each transformer block as the\nEuler integration approximation for solving an ordinary differential equation\n(ODE) and then incorporate the technique of bidirectional integration\napproximation (BDIA) into the neural architecture, together with activation\nquantization to make it exactly bit-level reversible. In the training process,\nwe let a hyper-parameter $\\gamma$ in BDIA-transformer randomly take one of the\ntwo values $\\{0.5, -0.5\\}$ per training sample per transformer block for\naveraging every two consecutive integration approximations. As a result,\nBDIA-transformer can be viewed as training an ensemble of ODE solvers\nparameterized by a set of binary random variables, which regularizes the model\nand results in improved validation accuracy. Lightweight side information per\ntransformer block is required to be stored in the forward process to account\nfor binary quantization loss to enable exact bit-level reversibility. In the\ninference procedure, the expectation $\\mathbb{E}(\\gamma)=0$ is taken to make\nthe resulting architectures of BDIA-transformer identical to transformers up to\nactivation quantization. Our experiments in both image classification and\nlanguage translation show that BDIA-transformers outperform their conventional\ncounterparts significantly in terms of validation performance while also\nrequiring considerably less training memory.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09093v2",
    "published_date": "2024-07-12 08:42:58 UTC",
    "updated_date": "2024-10-05 11:17:45 UTC"
  },
  {
    "arxiv_id": "2407.09088v1",
    "title": "FD-SOS: Vision-Language Open-Set Detectors for Bone Fenestration and Dehiscence Detection from Intraoral Images",
    "authors": [
      "Marawan Elbatel",
      "Keyuan Liu",
      "Yanqi Yang",
      "Xiaomeng Li"
    ],
    "abstract": "Accurate detection of bone fenestration and dehiscence (FD) is crucial for\neffective treatment planning in dentistry. While cone-beam computed tomography\n(CBCT) is the gold standard for evaluating FD, it comes with limitations such\nas radiation exposure, limited accessibility, and higher cost compared to\nintraoral images. In intraoral images, dentists face challenges in the\ndifferential diagnosis of FD. This paper presents a novel and clinically\nsignificant application of FD detection solely from intraoral images. To\nachieve this, we propose FD-SOS, a novel open-set object detector for FD\ndetection from intraoral images. FD-SOS has two novel components: conditional\ncontrastive denoising (CCDN) and teeth-specific matching assignment (TMA).\nThese modules enable FD-SOS to effectively leverage external dental semantics.\nExperimental results showed that our method outperformed existing detection\nmethods and surpassed dental professionals by 35% recall under the same level\nof precision. Code is available at: https://github.com/xmed-lab/FD-SOS.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "MICCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.09088v1",
    "published_date": "2024-07-12 08:29:25 UTC",
    "updated_date": "2024-07-12 08:29:25 UTC"
  },
  {
    "arxiv_id": "2407.09052v1",
    "title": "From MIDI to Rich Tablatures: an Automatic Generative System incorporating Lead Guitarists' Fingering and Stylistic choices",
    "authors": [
      "Pierluigi Bontempi",
      "Daniele Manerba",
      "Alexandre D'Hooge",
      "Sergio Canazza"
    ],
    "abstract": "Although the automatic identification of the optimal fingering for the\nperformance of melodies on fretted string instruments has already been\naddressed (at least partially) in the literature, the specific case regarding\nlead electric guitar requires a dedicated approach. We propose a system that\ncan generate, from simple MIDI melodies, tablatures enriched by fingerings,\narticulations, and expressive techniques. The basic fingering is derived by\nsolving a constrained and multi-attribute optimization problem, which derives\nthe best position of the fretting hand, not just the finger used at each\nmoment.Then, by analyzing statistical data from the mySongBook corpus, the most\ncommon clich{\\'e}s and biomechanical feasibility, articulations, and expressive\ntechniques are introduced. Finally, the obtained output is converted into\nMusicXML format, which allows for easy visualization and use. The quality of\nthe tablatures derived and the high configurability of the proposed approach\ncan have several impacts, in particular in the fields of instrumental teaching,\nassisted composition and arranging, and computational expressive music\nperformance models.",
    "categories": [
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09052v1",
    "published_date": "2024-07-12 07:18:24 UTC",
    "updated_date": "2024-07-12 07:18:24 UTC"
  },
  {
    "arxiv_id": "2407.09050v2",
    "title": "Refusing Safe Prompts for Multi-modal Large Language Models",
    "authors": [
      "Zedian Shao",
      "Hongbin Liu",
      "Yuepeng Hu",
      "Neil Zhenqiang Gong"
    ],
    "abstract": "Multimodal large language models (MLLMs) have become the cornerstone of\ntoday's generative AI ecosystem, sparking intense competition among tech giants\nand startups. In particular, an MLLM generates a text response given a prompt\nconsisting of an image and a question. While state-of-the-art MLLMs use safety\nfilters and alignment techniques to refuse unsafe prompts, in this work, we\nintroduce MLLM-Refusal, the first method that induces refusals for safe\nprompts. In particular, our MLLM-Refusal optimizes a nearly-imperceptible\nrefusal perturbation and adds it to an image, causing target MLLMs to likely\nrefuse a safe prompt containing the perturbed image and a safe question.\nSpecifically, we formulate MLLM-Refusal as a constrained optimization problem\nand propose an algorithm to solve it. Our method offers competitive advantages\nfor MLLM model providers by potentially disrupting user experiences of\ncompeting MLLMs, since competing MLLM's users will receive unexpected refusals\nwhen they unwittingly use these perturbed images in their prompts. We evaluate\nMLLM-Refusal on four MLLMs across four datasets, demonstrating its\neffectiveness in causing competing MLLMs to refuse safe prompts while not\naffecting non-competing MLLMs. Furthermore, we explore three potential\ncountermeasures-adding Gaussian noise, DiffPure, and adversarial training. Our\nresults show that though they can mitigate MLLM-Refusal's effectiveness, they\nalso sacrifice the accuracy and/or efficiency of the competing MLLM. The code\nis available at https://github.com/Sadcardation/MLLM-Refusal.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09050v2",
    "published_date": "2024-07-12 07:18:05 UTC",
    "updated_date": "2024-09-05 21:17:13 UTC"
  },
  {
    "arxiv_id": "2407.09048v1",
    "title": "KUNPENG: An Embodied Large Model for Intelligent Maritime",
    "authors": [
      "Naiyao Wang",
      "Tongbang Jiang",
      "Ye Wang",
      "Shaoyang Qiu",
      "Bo Zhang",
      "Xinqiang Xie",
      "Munan Li",
      "Chunliu Wang",
      "Yiyang Wang",
      "Hongxiang Ren",
      "Ruili Wang",
      "Hongjun Shan",
      "Hongbo Liu"
    ],
    "abstract": "Intelligent maritime, as an essential component of smart ocean construction,\ndeeply integrates advanced artificial intelligence technology and data analysis\nmethods, which covers multiple aspects such as smart vessels, route\noptimization, safe navigation, aiming to enhance the efficiency of ocean\nresource utilization and the intelligence of transportation networks. However,\nthe complex and dynamic maritime environment, along with diverse and\nheterogeneous large-scale data sources, present challenges for real-time\ndecision-making in intelligent maritime. In this paper, We propose KUNPENG, the\nfirst-ever embodied large model for intelligent maritime in the smart ocean\nconstruction, which consists of six systems. The model perceives multi-source\nheterogeneous data for the cognition of environmental interaction and make\nautonomous decision strategies, which are used for intelligent vessels to\nperform navigation behaviors under safety and emergency guarantees and\ncontinuously optimize power to achieve embodied intelligence in maritime. In\ncomprehensive maritime task evaluations, KUNPENG has demonstrated excellent\nperformance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.09048v1",
    "published_date": "2024-07-12 07:16:22 UTC",
    "updated_date": "2024-07-12 07:16:22 UTC"
  },
  {
    "arxiv_id": "2407.09045v1",
    "title": "Time-Frequency Analysis of Variable-Length WiFi CSI Signals for Person Re-Identification",
    "authors": [
      "Chen Mao",
      "Chong Tan",
      "Jingqi Hu",
      "Min Zheng"
    ],
    "abstract": "Person re-identification (ReID), as a crucial technology in the field of\nsecurity, plays an important role in security detection and people counting.\nCurrent security and monitoring systems largely rely on visual information,\nwhich may infringe on personal privacy and be susceptible to interference from\npedestrian appearances and clothing in certain scenarios. Meanwhile, the\nwidespread use of routers offers new possibilities for ReID. This letter\nintroduces a method using WiFi Channel State Information (CSI), leveraging the\nmultipath propagation characteristics of WiFi signals as a basis for\ndistinguishing different pedestrian features. We propose a two-stream network\nstructure capable of processing variable-length data, which analyzes the\namplitude in the time domain and the phase in the frequency domain of WiFi\nsignals, fuses time-frequency information through continuous lateral\nconnections, and employs advanced objective functions for representation and\nmetric learning. Tested on a dataset collected in the real world, our method\nachieves 93.68% mAP and 98.13% Rank-1.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09045v1",
    "published_date": "2024-07-12 07:10:47 UTC",
    "updated_date": "2024-07-12 07:10:47 UTC"
  },
  {
    "arxiv_id": "2407.09043v3",
    "title": "Vision Language Model is NOT All You Need: Augmentation Strategies for Molecule Language Models",
    "authors": [
      "Namkyeong Lee",
      "Siddhartha Laghuvarapu",
      "Chanyoung Park",
      "Jimeng Sun"
    ],
    "abstract": "Recently, there has been a growing interest among researchers in\nunderstanding molecules and their textual descriptions through molecule\nlanguage models (MoLM). However, despite some early promising developments, the\nadvancement of MoLM still trails significantly behind that of vision language\nmodels (VLM). This is because unique challenges exist apart from VLM in the\nfield of MoLM due to 1) a limited amount of molecule-text paired data and 2)\nmissing expertise that occurred due to the specialized areas of focus among the\nexperts. To this end, we propose AMOLE, which 1) augments molecule-text pairs\nwith structural similarity preserving loss, and 2) transfers the expertise\nbetween the molecules. Specifically, AMOLE enriches molecule-text pairs by\nsharing descriptions among structurally similar molecules with a novel\nstructural similarity preserving loss. Moreover, we propose an expertise\nreconstruction loss to transfer knowledge from molecules that have extensive\nexpertise to those with less expertise. Extensive experiments on various\ndownstream tasks demonstrate the superiority of AMOLE in comprehending\nmolecules and their descriptions, highlighting its potential for application in\nreal-world drug discovery. The source code for AMOLE is available at\nhttps://github.com/Namkyeong/AMOLE.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "CIKM 2024 / ACL 2024 Workshop on Languages and Molecule",
    "pdf_url": "http://arxiv.org/pdf/2407.09043v3",
    "published_date": "2024-07-12 07:09:10 UTC",
    "updated_date": "2024-07-23 07:31:20 UTC"
  },
  {
    "arxiv_id": "2407.09039v1",
    "title": "Overcoming Catastrophic Forgetting in Tabular Data Classification: A Pseudorehearsal-based approach",
    "authors": [
      "Pablo García-Santaclara",
      "Bruno Fernández-Castro",
      "Rebeca P. Díaz-Redondo"
    ],
    "abstract": "Continual learning (CL) poses the important challenge of adapting to evolving\ndata distributions without forgetting previously acquired knowledge while\nconsolidating new knowledge. In this paper, we introduce a new methodology,\ncoined as Tabular-data Rehearsal-based Incremental Lifelong Learning framework\n(TRIL3), designed to address the phenomenon of catastrophic forgetting in\ntabular data classification problems. TRIL3 uses the prototype-based\nincremental generative model XuILVQ to generate synthetic data to preserve old\nknowledge and the DNDF algorithm, which was modified to run in an incremental\nway, to learn classification tasks for tabular data, without storing old\nsamples. After different tests to obtain the adequate percentage of synthetic\ndata and to compare TRIL3 with other CL available proposals, we can conclude\nthat the performance of TRIL3 outstands other options in the literature using\nonly 50% of synthetic data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 4 tables, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.09039v1",
    "published_date": "2024-07-12 07:04:06 UTC",
    "updated_date": "2024-07-12 07:04:06 UTC"
  },
  {
    "arxiv_id": "2407.09025v2",
    "title": "SpreadsheetLLM: Encoding Spreadsheets for Large Language Models",
    "authors": [
      "Haoyu Dong",
      "Jianbo Zhao",
      "Yuzhang Tian",
      "Junyu Xiong",
      "Shiyu Xia",
      "Mengyu Zhou",
      "Yun Lin",
      "José Cambronero",
      "Yeye He",
      "Shi Han",
      "Dongmei Zhang"
    ],
    "abstract": "Spreadsheets are characterized by their extensive two-dimensional grids,\nflexible layouts, and varied formatting options, which pose significant\nchallenges for large language models (LLMs). In response, we introduce\nSpreadsheetLLM, pioneering an efficient encoding method designed to unleash and\noptimize LLMs' powerful understanding and reasoning capability on spreadsheets.\nInitially, we propose a vanilla serialization approach that incorporates cell\naddresses, values, and formats. However, this approach was limited by LLMs'\ntoken constraints, making it impractical for most applications. To tackle this\nchallenge, we develop SheetCompressor, an innovative encoding framework that\ncompresses spreadsheets effectively for LLMs. It comprises three modules:\nstructural-anchor-based compression, inverse index translation, and\ndata-format-aware aggregation. It significantly improves performance in the\nspreadsheet table detection task, outperforming the vanilla approach by 25.6%\nin GPT4's in-context learning setting. Moreover, fine-tuned LLM with\nSheetCompressor has an average compression ratio of 25 times, and achieves a\nstate-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%.\nFinally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet\nunderstanding and validate it in a new and demanding spreadsheet QA task. We\nmethodically leverage the inherent layout and structure of spreadsheets,\ndemonstrating that SpreadsheetLLM is highly effective across a variety of\nspreadsheet tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09025v2",
    "published_date": "2024-07-12 06:34:21 UTC",
    "updated_date": "2025-04-02 14:33:38 UTC"
  },
  {
    "arxiv_id": "2407.09019v1",
    "title": "Heterogeneous Subgraph Network with Prompt Learning for Interpretable Depression Detection on Social Media",
    "authors": [
      "Chen Chen",
      "Mingwei Li",
      "Fenghuan Li",
      "Haopeng Chen",
      "Yuankun Lin"
    ],
    "abstract": "Massive social media data can reflect people's authentic thoughts, emotions,\ncommunication, etc., and therefore can be analyzed for early detection of\nmental health problems such as depression. Existing works about early\ndepression detection on social media lacked interpretability and neglected the\nheterogeneity of social media data. Furthermore, they overlooked the global\ninteraction among users. To address these issues, we develop a novel method\nthat leverages a Heterogeneous Subgraph Network with Prompt Learning(HSNPL) and\ncontrastive learning mechanisms. Specifically, prompt learning is employed to\nmap users' implicit psychological symbols with excellent interpretability while\ndeep semantic and diverse behavioral features are incorporated by a\nheterogeneous information network. Then, the heterogeneous graph network with a\ndual attention mechanism is constructed to model the relationships among\nheterogeneous social information at the feature level. Furthermore, the\nheterogeneous subgraph network integrating subgraph attention and\nself-supervised contrastive learning is developed to explore complicated\ninteractions among users and groups at the user level. Extensive experimental\nresults demonstrate that our proposed method significantly outperforms\nstate-of-the-art methods for depression detection on social media.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09019v1",
    "published_date": "2024-07-12 06:20:59 UTC",
    "updated_date": "2024-07-12 06:20:59 UTC"
  },
  {
    "arxiv_id": "2407.09015v1",
    "title": "Static Analysis of Logic Programs via Boolean Networks",
    "authors": [
      "Van-Giang Trinh",
      "Belaid Benhamou"
    ],
    "abstract": "Answer Set Programming (ASP) is a declarative problem solving paradigm that\ncan be used to encode a combinatorial problem as a logic program whose stable\nmodels correspond to the solutions of the considered problem. ASP has been\nwidely applied to various domains in AI and beyond. The question \"What can be\nsaid about stable models of a logic program from its static information?\" has\nbeen investigated and proved useful in many circumstances. In this work, we\ndive into this direction more deeply by making the connection between a logic\nprogram and a Boolean network, which is a prominent modeling framework with\napplications to various areas. The proposed connection can bring the existing\nresults in the rich history on static analysis of Boolean networks to explore\nand prove more theoretical results on ASP, making it become a unified and\npowerful tool to further study the static analysis of ASP. In particular, the\nnewly obtained insights have the potential to benefit many problems in the\nfield of ASP.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09015v1",
    "published_date": "2024-07-12 06:07:05 UTC",
    "updated_date": "2024-07-12 06:07:05 UTC"
  },
  {
    "arxiv_id": "2407.09013v1",
    "title": "Procedural Content Generation via Generative Artificial Intelligence",
    "authors": [
      "Xinyu Mao",
      "Wanli Yu",
      "Kazunori D Yamada",
      "Michael R. Zielewski"
    ],
    "abstract": "The attempt to utilize machine learning in PCG has been made in the past. In\nthis survey paper, we investigate how generative artificial intelligence (AI),\nwhich saw a significant increase in interest in the mid-2010s, is being used\nfor PCG. We review applications of generative AI for the creation of various\ntypes of content, including terrains, items, and even storylines. While\ngenerative AI is effective for PCG, one significant issues it faces is that\nbuilding high-performance generative AI requires vast amounts of training data.\nBecause content generally highly customized, domain-specific training data is\nscarce, and straightforward approaches to generative AI models may not work\nwell. For PCG research to advance further, issues related to limited training\ndata must be overcome. Thus, we also give special consideration to research\nthat addresses the challenges posed by limited training data.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09013v1",
    "published_date": "2024-07-12 06:03:38 UTC",
    "updated_date": "2024-07-12 06:03:38 UTC"
  },
  {
    "arxiv_id": "2407.09012v1",
    "title": "TCAN: Animating Human Images with Temporally Consistent Pose Guidance using Diffusion Models",
    "authors": [
      "Jeongho Kim",
      "Min-Jung Kim",
      "Junsoo Lee",
      "Jaegul Choo"
    ],
    "abstract": "Pose-driven human-image animation diffusion models have shown remarkable\ncapabilities in realistic human video synthesis. Despite the promising results\nachieved by previous approaches, challenges persist in achieving temporally\nconsistent animation and ensuring robustness with off-the-shelf pose detectors.\nIn this paper, we present TCAN, a pose-driven human image animation method that\nis robust to erroneous poses and consistent over time. In contrast to previous\nmethods, we utilize the pre-trained ControlNet without fine-tuning to leverage\nits extensive pre-acquired knowledge from numerous pose-image-caption pairs. To\nkeep the ControlNet frozen, we adapt LoRA to the UNet layers, enabling the\nnetwork to align the latent space between the pose and appearance features.\nAdditionally, by introducing an additional temporal layer to the ControlNet, we\nenhance robustness against outliers of the pose detector. Through the analysis\nof attention maps over the temporal axis, we also designed a novel temperature\nmap leveraging pose information, allowing for a more static background.\nExtensive experiments demonstrate that the proposed method can achieve\npromising results in video synthesis tasks encompassing various poses, like\nchibi. Project Page: https://eccv2024tcan.github.io/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The first two authors contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2407.09012v1",
    "published_date": "2024-07-12 06:02:13 UTC",
    "updated_date": "2024-07-12 06:02:13 UTC"
  },
  {
    "arxiv_id": "2407.09011v2",
    "title": "One Stone, Four Birds: A Comprehensive Solution for QA System Using Supervised Contrastive Learning",
    "authors": [
      "Bo Wang",
      "Tsunenori Mine"
    ],
    "abstract": "This paper presents a novel and comprehensive solution to enhance both the\nrobustness and efficiency of question answering (QA) systems through supervised\ncontrastive learning (SCL). Training a high-performance QA system has become\nstraightforward with pre-trained language models, requiring only a small amount\nof data and simple fine-tuning. However, despite recent advances, existing QA\nsystems still exhibit significant deficiencies in functionality and training\nefficiency. We address the functionality issue by defining four key tasks: user\ninput intent classification, out-of-domain input detection, new intent\ndiscovery, and continual learning. We then leverage a unified SCL-based\nrepresentation learning method to efficiently build an intra-class compact and\ninter-class scattered feature space, facilitating both known intent\nclassification and unknown intent detection and discovery. Consequently, with\nminimal additional tuning on downstream tasks, our approach significantly\nimproves model efficiency and achieves new state-of-the-art performance across\nall tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, updated to the accepted version",
    "pdf_url": "http://arxiv.org/pdf/2407.09011v2",
    "published_date": "2024-07-12 06:01:51 UTC",
    "updated_date": "2024-10-25 02:10:08 UTC"
  },
  {
    "arxiv_id": "2407.09005v1",
    "title": "Introducing VaDA: Novel Image Segmentation Model for Maritime Object Segmentation Using New Dataset",
    "authors": [
      "Yongjin Kim",
      "Jinbum Park",
      "Sanha Kang",
      "Hanguen Kim"
    ],
    "abstract": "The maritime shipping industry is undergoing rapid evolution driven by\nadvancements in computer vision artificial intelligence (AI). Consequently,\nresearch on AI-based object recognition models for maritime transportation is\nsteadily growing, leveraging advancements in sensor technology and computing\nperformance. However, object recognition in maritime environments faces\nchallenges such as light reflection, interference, intense lighting, and\nvarious weather conditions. To address these challenges, high-performance deep\nlearning algorithms tailored to maritime imagery and high-quality datasets\nspecialized for maritime scenes are essential. Existing AI recognition models\nand datasets have limited suitability for composing autonomous navigation\nsystems. Therefore, in this paper, we propose a Vertical and Detail Attention\n(VaDA) model for maritime object segmentation and a new model evaluation\nmethod, the Integrated Figure of Calculation Performance (IFCP), to verify its\nsuitability for the system in real-time. Additionally, we introduce a benchmark\nmaritime dataset, OASIs (Ocean AI Segmentation Initiatives) to standardize\nmodel performance evaluation across diverse maritime environments. OASIs\ndataset and details are available at our website:\nhttps://www.navlue.com/dataset",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 9 figures, whitepaper",
    "pdf_url": "http://arxiv.org/pdf/2407.09005v1",
    "published_date": "2024-07-12 05:48:53 UTC",
    "updated_date": "2024-07-12 05:48:53 UTC"
  },
  {
    "arxiv_id": "2407.09003v1",
    "title": "Enhancing Few-Shot Stock Trend Prediction with Large Language Models",
    "authors": [
      "Yiqi Deng",
      "Xingwei He",
      "Jiahao Hu",
      "Siu-Ming Yiu"
    ],
    "abstract": "The goal of stock trend prediction is to forecast future market movements for\ninformed investment decisions. Existing methods mostly focus on predicting\nstock trends with supervised models trained on extensive annotated data.\nHowever, human annotation can be resource-intensive and the annotated data are\nnot readily available. Inspired by the impressive few-shot capability of Large\nLanguage Models (LLMs), we propose using LLMs in a few-shot setting to overcome\nthe scarcity of labeled data and make prediction more feasible to investors.\nPrevious works typically merge multiple financial news for predicting stock\ntrends, causing two significant problems when using LLMs: (1) Merged news\ncontains noise, and (2) it may exceed LLMs' input limits, leading to\nperformance degradation. To overcome these issues, we propose a two-step method\n'denoising-then-voting'. Specifically, we introduce an `Irrelevant' category,\nand predict stock trends for individual news instead of merged news. Then we\naggregate these predictions using majority voting. The proposed method offers\ntwo advantages: (1) Classifying noisy news as irrelevant removes its impact on\nthe final prediction. (2) Predicting for individual news mitigates LLMs' input\nlength limits. Our method achieves 66.59% accuracy in S&P 500, 62.17% in\nCSI-100, and 61.17% in HK stock prediction, outperforming the standard few-shot\ncounterparts by around 7%, 4%, and 4%. Furthermore, our proposed method\nperforms on par with state-of-the-art supervised methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09003v1",
    "published_date": "2024-07-12 05:43:11 UTC",
    "updated_date": "2024-07-12 05:43:11 UTC"
  },
  {
    "arxiv_id": "2407.08992v1",
    "title": "Emotion Talk: Emotional Support via Audio Messages for Psychological Assistance",
    "authors": [
      "Fabrycio Leite Nakano Almada",
      "Kauan Divino Pouso Mariano",
      "Maykon Adriell Dutra",
      "Victor Emanuel da Silva Monteiro"
    ],
    "abstract": "This paper presents \"Emotion Talk,\" a system designed to provide continuous\nemotional support through audio messages for psychological assistance. The\nprimary objective is to offer consistent support to patients outside\ntraditional therapy sessions by analyzing audio messages to detect emotions and\ngenerate appropriate responses. The solution focuses on Portuguese-speaking\nusers, ensuring that the system is linguistically and culturally relevant. This\nsystem aims to complement and enhance the psychological follow-up process\nconducted by therapists, providing immediate and accessible assistance,\nespecially in emergency situations where rapid response is crucial.\nExperimental results demonstrate the effectiveness of the proposed system,\nhighlighting its potential in applications of psychological support.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08992v1",
    "published_date": "2024-07-12 05:13:17 UTC",
    "updated_date": "2024-07-12 05:13:17 UTC"
  },
  {
    "arxiv_id": "2407.08991v1",
    "title": "Optimization of DNN-based speaker verification model through efficient quantization technique",
    "authors": [
      "Yeona Hong",
      "Woo-Jin Chung",
      "Hong-Goo Kang"
    ],
    "abstract": "As Deep Neural Networks (DNNs) rapidly advance in various fields, including\nspeech verification, they typically involve high computational costs and\nsubstantial memory consumption, which can be challenging to manage on mobile\nsystems. Quantization of deep models offers a means to reduce both\ncomputational and memory expenses. Our research proposes an optimization\nframework for the quantization of the speaker verification model. By analyzing\nperformance changes and model size reductions in each layer of a pre-trained\nspeaker verification model, we have effectively minimized performance\ndegradation while significantly reducing the model size. Our quantization\nalgorithm is the first attempt to maintain the performance of the\nstate-of-the-art pre-trained speaker verification model, ECAPATDNN, while\nsignificantly compressing its model size. Overall, our quantization approach\nresulted in reducing the model size by half, with an increase in EER limited to\n0.07%.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CC"
    ],
    "primary_category": "eess.AS",
    "comment": "in Korean language, Accepted at Society of Electronic Engineers of\n  Korea Conference 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.08991v1",
    "published_date": "2024-07-12 05:03:10 UTC",
    "updated_date": "2024-07-12 05:03:10 UTC"
  },
  {
    "arxiv_id": "2407.08990v1",
    "title": "Dynamic neural network with memristive CIM and CAM for 2D and 3D vision",
    "authors": [
      "Yue Zhang",
      "Woyu Zhang",
      "Shaocong Wang",
      "Ning Lin",
      "Yifei Yu",
      "Yangu He",
      "Bo Wang",
      "Hao Jiang",
      "Peng Lin",
      "Xiaoxin Xu",
      "Xiaojuan Qi",
      "Zhongrui Wang",
      "Xumeng Zhang",
      "Dashan Shang",
      "Qi Liu",
      "Kwang-Ting Cheng",
      "Ming Liu"
    ],
    "abstract": "The brain is dynamic, associative and efficient. It reconfigures by\nassociating the inputs with past experiences, with fused memory and processing.\nIn contrast, AI models are static, unable to associate inputs with past\nexperiences, and run on digital computers with physically separated memory and\nprocessing. We propose a hardware-software co-design, a semantic memory-based\ndynamic neural network (DNN) using memristor. The network associates incoming\ndata with the past experience stored as semantic vectors. The network and the\nsemantic memory are physically implemented on noise-robust ternary\nmemristor-based Computing-In-Memory (CIM) and Content-Addressable Memory (CAM)\ncircuits, respectively. We validate our co-designs, using a 40nm memristor\nmacro, on ResNet and PointNet++ for classifying images and 3D points from the\nMNIST and ModelNet datasets, which not only achieves accuracy on par with\nsoftware but also a 48.1% and 15.9% reduction in computational budget.\nMoreover, it delivers a 77.6% and 93.3% reduction in energy consumption.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.ET",
      "cs.NE"
    ],
    "primary_category": "cs.AR",
    "comment": "In press",
    "pdf_url": "http://arxiv.org/pdf/2407.08990v1",
    "published_date": "2024-07-12 04:55:57 UTC",
    "updated_date": "2024-07-12 04:55:57 UTC"
  },
  {
    "arxiv_id": "2407.08989v1",
    "title": "Robustness of LLMs to Perturbations in Text",
    "authors": [
      "Ayush Singh",
      "Navpreet Singh",
      "Shubham Vatsal"
    ],
    "abstract": "Having a clean dataset has been the foundational assumption of most natural\nlanguage processing (NLP) systems. However, properly written text is rarely\nfound in real-world scenarios and hence, oftentimes invalidates the\naforementioned foundational assumption. Recently, Large language models (LLMs)\nhave shown impressive performance, but can they handle the inevitable noise in\nreal-world data? This work tackles this critical question by investigating\nLLMs' resilience against morphological variations in text. To that end, we\nartificially introduce varying levels of noise into a diverse set of datasets\nand systematically evaluate LLMs' robustness against the corrupt variations of\nthe original text. Our findings show that contrary to popular beliefs,\ngenerative LLMs are quiet robust to noisy perturbations in text. This is a\ndeparture from pre-trained models like BERT or RoBERTa whose performance has\nbeen shown to be sensitive to deteriorating noisy text. Additionally, we test\nLLMs' resilience on multiple real-world benchmarks that closely mimic commonly\nfound errors in the wild. With minimal prompting, LLMs achieve a new\nstate-of-the-art on the benchmark tasks of Grammar Error Correction (GEC) and\nLexical Semantic Change (LSC). To empower future research, we also release a\ndataset annotated by humans stating their preference for LLM vs.\nhuman-corrected outputs along with the code to reproduce our results.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.7; I.2.7; I.2.4"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 1 figure, 6 tables, updated with results also from GPT-4,\n  LLaMa-3",
    "pdf_url": "http://arxiv.org/pdf/2407.08989v1",
    "published_date": "2024-07-12 04:50:17 UTC",
    "updated_date": "2024-07-12 04:50:17 UTC"
  },
  {
    "arxiv_id": "2407.08983v1",
    "title": "Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations",
    "authors": [
      "David N. Palacio",
      "Daniel Rodriguez-Cardenas",
      "Alejandro Velasco",
      "Dipin Khati",
      "Kevin Moran",
      "Denys Poshyvanyk"
    ],
    "abstract": "Trustworthiness and interpretability are inextricably linked concepts for\nLLMs. The more interpretable an LLM is, the more trustworthy it becomes.\nHowever, current techniques for interpreting LLMs when applied to code-related\ntasks largely focus on accuracy measurements, measures of how models react to\nchange, or individual task performance instead of the fine-grained explanations\nneeded at prediction time for greater interpretability, and hence trust. To\nimprove upon this status quo, this paper introduces ASTrust, an\ninterpretability method for LLMs of code that generates explanations grounded\nin the relationship between model confidence and syntactic structures of\nprogramming languages. ASTrust explains generated code in the context of syntax\ncategories based on Abstract Syntax Trees and aids practitioners in\nunderstanding model predictions at both local (individual code snippets) and\nglobal (larger datasets of code) levels. By distributing and assigning model\nconfidence scores to well-known syntactic structures that exist within ASTs,\nour approach moves beyond prior techniques that perform token-level confidence\nmapping by offering a view of model confidence that directly aligns with\nprogramming language concepts with which developers are familiar. To put\nASTrust into practice, we developed an automated visualization that illustrates\nthe aggregated model confidence scores superimposed on sequence, heat-map, and\ngraph-based visuals of syntactic structures from ASTs. We examine both the\npractical benefit that ASTrust can provide through a data science study on 12\npopular LLMs on a curated set of GitHub repos and the usefulness of ASTrust\nthrough a human study.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Under Review to appear in ACM Transactions on Software Engineering\n  and Methodology (TOSEM)",
    "pdf_url": "http://arxiv.org/pdf/2407.08983v1",
    "published_date": "2024-07-12 04:38:28 UTC",
    "updated_date": "2024-07-12 04:38:28 UTC"
  },
  {
    "arxiv_id": "2407.09580v1",
    "title": "Don't Fear Peculiar Activation Functions: EUAF and Beyond",
    "authors": [
      "Qianchao Wang",
      "Shijun Zhang",
      "Dong Zeng",
      "Zhaoheng Xie",
      "Hengtao Guo",
      "Feng-Lei Fan",
      "Tieyong Zeng"
    ],
    "abstract": "In this paper, we propose a new super-expressive activation function called\nthe Parametric Elementary Universal Activation Function (PEUAF). We demonstrate\nthe effectiveness of PEUAF through systematic and comprehensive experiments on\nvarious industrial and image datasets, including CIFAR10, Tiny-ImageNet, and\nImageNet. Moreover, we significantly generalize the family of super-expressive\nactivation functions, whose existence has been demonstrated in several recent\nworks by showing that any continuous function can be approximated to any\ndesired accuracy by a fixed-size network with a specific super-expressive\nactivation function. Specifically, our work addresses two major bottlenecks in\nimpeding the development of super-expressive activation functions: the limited\nidentification of super-expressive functions, which raises doubts about their\nbroad applicability, and their often peculiar forms, which lead to skepticism\nregarding their scalability and practicality in real-world applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09580v1",
    "published_date": "2024-07-12 03:57:25 UTC",
    "updated_date": "2024-07-12 03:57:25 UTC"
  },
  {
    "arxiv_id": "2407.08970v3",
    "title": "Self-interpreting Adversarial Images",
    "authors": [
      "Tingwei Zhang",
      "Collin Zhang",
      "John X. Morris",
      "Eugene Bagdasarian",
      "Vitaly Shmatikov"
    ],
    "abstract": "We introduce a new type of indirect, cross-modal injection attacks against\nvisual language models that enable creation of self-interpreting images. These\nimages contain hidden \"meta-instructions\" that control how models answer users'\nquestions about the image and steer their outputs to express an\nadversary-chosen style, sentiment, or point of view. Self-interpreting images\nact as soft prompts, conditioning the model to satisfy the adversary's\n(meta-)objective while still producing answers based on the image's visual\ncontent. Meta-instructions are thus a stronger form of prompt injection.\nAdversarial images look natural and the model's answers are coherent and\nplausible--yet they also follow the adversary-chosen interpretation, e.g.,\npolitical spin, or even objectives that are not achievable with explicit text\ninstructions. We evaluate the efficacy of self-interpreting images for a\nvariety of models, interpretations, and user prompts. We describe how these\nattacks could cause harm by enabling creation of self-interpreting content that\ncarries spam, misinformation, or spin. Finally, we discuss defenses.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08970v3",
    "published_date": "2024-07-12 03:40:13 UTC",
    "updated_date": "2025-01-27 23:57:33 UTC"
  },
  {
    "arxiv_id": "2407.18950v2",
    "title": "Unexplainability of Artificial Intelligence Judgments in Kant's Perspective",
    "authors": [
      "Jongwoo Seo"
    ],
    "abstract": "Kant's Critique of Pure Reason, a major contribution to the history of\nepistemology, proposes a table of categories to elucidate the structure of the\na priori principle of human judgment. The technology of artificial intelligence\n(AI), based on functionalism, claims to simulate or replicate human judgment.\nTo assess this claim, it is necessary to study whether AI judgment possesses\nthe characteristics of human judgment. This paper argues that AI judgments\nexhibit a form that cannot be understood in terms of the characteristics of\nhuman judgments according to Kant. Because the characteristics of judgment\noverlap, we can call this AI's uncertainty. Then, I show that concepts without\nphysical intuitions are not easy to explain when their functions are shown\nthrough vision. Finally, I illustrate that even if AI makes sentences through\nsubject and predicate in natural language, which are components of judgment, it\nis difficult to determine whether AI understands the concepts to the level\nhumans can accept. This shows that it is questionable whether the explanation\nthrough natural language is reliable.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.18950v2",
    "published_date": "2024-07-12 03:39:55 UTC",
    "updated_date": "2024-09-08 14:33:08 UTC"
  },
  {
    "arxiv_id": "2407.08967v1",
    "title": "Empowering Few-Shot Relation Extraction with The Integration of Traditional RE Methods and Large Language Models",
    "authors": [
      "Ye Liu",
      "Kai Zhang",
      "Aoran Gan",
      "Linan Yue",
      "Feng Hu",
      "Qi Liu",
      "Enhong Chen"
    ],
    "abstract": "Few-Shot Relation Extraction (FSRE), a subtask of Relation Extraction (RE)\nthat utilizes limited training instances, appeals to more researchers in\nNatural Language Processing (NLP) due to its capability to extract textual\ninformation in extremely low-resource scenarios. The primary methodologies\nemployed for FSRE have been fine-tuning or prompt tuning techniques based on\nPre-trained Language Models (PLMs). Recently, the emergence of Large Language\nModels (LLMs) has prompted numerous researchers to explore FSRE through\nIn-Context Learning (ICL). However, there are substantial limitations\nassociated with methods based on either traditional RE models or LLMs.\nTraditional RE models are hampered by a lack of necessary prior knowledge,\nwhile LLMs fall short in their task-specific capabilities for RE. To address\nthese shortcomings, we propose a Dual-System Augmented Relation Extractor\n(DSARE), which synergistically combines traditional RE models with LLMs.\nSpecifically, DSARE innovatively injects the prior knowledge of LLMs into\ntraditional RE models, and conversely enhances LLMs' task-specific aptitude for\nRE through relation extraction augmentation. Moreover, an Integrated Prediction\nmodule is employed to jointly consider these two respective predictions and\nderive the final results. Extensive experiments demonstrate the efficacy of our\nproposed method.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08967v1",
    "published_date": "2024-07-12 03:31:11 UTC",
    "updated_date": "2024-07-12 03:31:11 UTC"
  },
  {
    "arxiv_id": "2407.08966v1",
    "title": "LAPT: Label-driven Automated Prompt Tuning for OOD Detection with Vision-Language Models",
    "authors": [
      "Yabin Zhang",
      "Wenjie Zhu",
      "Chenhang He",
      "Lei Zhang"
    ],
    "abstract": "Out-of-distribution (OOD) detection is crucial for model reliability, as it\nidentifies samples from unknown classes and reduces errors due to unexpected\ninputs. Vision-Language Models (VLMs) such as CLIP are emerging as powerful\ntools for OOD detection by integrating multi-modal information. However, the\npractical application of such systems is challenged by manual prompt\nengineering, which demands domain expertise and is sensitive to linguistic\nnuances. In this paper, we introduce Label-driven Automated Prompt Tuning\n(LAPT), a novel approach to OOD detection that reduces the need for manual\nprompt engineering. We develop distribution-aware prompts with in-distribution\n(ID) class names and negative labels mined automatically. Training samples\nlinked to these class labels are collected autonomously via image synthesis and\nretrieval methods, allowing for prompt learning without manual effort. We\nutilize a simple cross-entropy loss for prompt optimization, with cross-modal\nand cross-distribution mixing strategies to reduce image noise and explore the\nintermediate space between distributions, respectively. The LAPT framework\noperates autonomously, requiring only ID class names as input and eliminating\nthe need for manual intervention. With extensive experiments, LAPT consistently\noutperforms manually crafted prompts, setting a new standard for OOD detection.\nMoreover, LAPT not only enhances the distinction between ID and OOD samples,\nbut also improves the ID classification accuracy and strengthens the\ngeneralization robustness to covariate shifts, resulting in outstanding\nperformance in challenging full-spectrum OOD detection tasks. Codes are\navailable at \\url{https://github.com/YBZh/LAPT}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV2024; Codes and Supp. are available at:\n  https://github.com/YBZh/LAPT",
    "pdf_url": "http://arxiv.org/pdf/2407.08966v1",
    "published_date": "2024-07-12 03:30:53 UTC",
    "updated_date": "2024-07-12 03:30:53 UTC"
  },
  {
    "arxiv_id": "2407.08963v1",
    "title": "Local Optima in Diversity Optimization: Non-trivial Offspring Population is Essential",
    "authors": [
      "Denis Antipov",
      "Aneta Neumann",
      "Frank Neumann"
    ],
    "abstract": "The main goal of diversity optimization is to find a diverse set of solutions\nwhich satisfy some lower bound on their fitness. Evolutionary algorithms (EAs)\nare often used for such tasks, since they are naturally designed to optimize\npopulations of solutions. This approach to diversity optimization, called EDO,\nhas been previously studied from theoretical perspective, but most studies\nconsidered only EAs with a trivial offspring population such as the $(\\mu + 1)$\nEA. In this paper we give an example instance of a $k$-vertex cover problem,\nwhich highlights a critical difference of the diversity optimization from the\nregular single-objective optimization, namely that there might be a locally\noptimal population from which we can escape only by replacing at least two\nindividuals at once, which the $(\\mu + 1)$ algorithms cannot do.\n  We also show that the $(\\mu + \\lambda)$ EA with $\\lambda \\ge \\mu$ can\neffectively find a diverse population on $k$-vertex cover, if using a mutation\noperator inspired by Branson and Sutton (TCS 2023). To avoid the problem of\nsubset selection which arises in the $(\\mu + \\lambda)$ EA when it optimizes\ndiversity, we also propose the $(1_\\mu + 1_\\mu)$ EA$_D$, which is an analogue\nof the $(1 + 1)$ EA for populations, and which is also efficient at optimizing\ndiversity on the $k$-vertex cover problem.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "Open-access version of the same-titled PPSN 2024 paper",
    "pdf_url": "http://arxiv.org/pdf/2407.08963v1",
    "published_date": "2024-07-12 03:27:47 UTC",
    "updated_date": "2024-07-12 03:27:47 UTC"
  },
  {
    "arxiv_id": "2407.08952v5",
    "title": "Detect, Investigate, Judge and Determine: A Knowledge-guided Framework for Few-shot Fake News Detection",
    "authors": [
      "Ye Liu",
      "Jiajun Zhu",
      "Xukai Liu",
      "Haoyu Tang",
      "Yanghai Zhang",
      "Kai Zhang",
      "Xiaofang Zhou",
      "Enhong Chen"
    ],
    "abstract": "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Knowledge-guided Fake News\nDetection (DKFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DKFND first identifies the knowledge concepts of\neach news article through a Detection Module. Subsequently, DKFND creatively\ndesigns an Investigation Module to retrieve inside and outside valuable\ninformation concerning to the current news, followed by another Judge Module to\nevaluate the relevance and confidence of them. Finally, a Determination Module\nfurther derives two respective predictions and obtain the final result.\nExtensive experiments on two public datasets show the efficacy of our proposed\nmethod, particularly in low-resource settings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08952v5",
    "published_date": "2024-07-12 03:15:01 UTC",
    "updated_date": "2025-03-12 04:46:47 UTC"
  },
  {
    "arxiv_id": "2407.08942v1",
    "title": "A Neural Matrix Decomposition Recommender System Model based on the Multimodal Large Language Model",
    "authors": [
      "Ao Xiang",
      "Bingjie Huang",
      "Xinyu Guo",
      "Haowei Yang",
      "Tianyao Zheng"
    ],
    "abstract": "Recommendation systems have become an important solution to information\nsearch problems. This article proposes a neural matrix factorization\nrecommendation system model based on the multimodal large language model called\nBoNMF. This model combines BoBERTa's powerful capabilities in natural language\nprocessing, ViT in computer in vision, and neural matrix decomposition\ntechnology. By capturing the potential characteristics of users and items, and\nafter interacting with a low-dimensional matrix composed of user and item IDs,\nthe neural network outputs the results. recommend. Cold start and ablation\nexperimental results show that the BoNMF model exhibits excellent performance\non large public data sets and significantly improves the accuracy of\nrecommendations.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08942v1",
    "published_date": "2024-07-12 02:58:07 UTC",
    "updated_date": "2024-07-12 02:58:07 UTC"
  },
  {
    "arxiv_id": "2407.08937v1",
    "title": "Self-Evolving GPT: A Lifelong Autonomous Experiential Learner",
    "authors": [
      "Jinglong Gao",
      "Xiao Ding",
      "Yiming Cui",
      "Jianbai Zhao",
      "Hepeng Wang",
      "Ting Liu",
      "Bing Qin"
    ],
    "abstract": "To improve the performance of large language models (LLMs), researchers have\nexplored providing LLMs with textual task-solving experience via prompts.\nHowever, they rely on manual efforts to acquire and apply such experience for\neach task, which is not feasible for the growing demand for LLMs and the\nvariety of user questions. To address this issue, we design a lifelong\nautonomous experiential learning framework based on LLMs to explore whether\nLLMs can imitate human ability for learning and utilizing experience. It\nautonomously learns and accumulates experience through experience transfer and\ninduction, categorizing the types of input questions to select which\naccumulated experience to employ for them. Experimental results on six widely\nused NLP datasets show that our framework performs reliably in each\nintermediate step and effectively improves the performance of GPT-3.5 and\nGPT-4. This validates the feasibility of using LLMs to mimic human experiential\nlearning and application capabilities. Additionally, we provide a detailed\nanalysis of the behavior of our framework at each step.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL 2024 MAIN",
    "pdf_url": "http://arxiv.org/pdf/2407.08937v1",
    "published_date": "2024-07-12 02:49:13 UTC",
    "updated_date": "2024-07-12 02:49:13 UTC"
  },
  {
    "arxiv_id": "2407.08932v2",
    "title": "Deep Attention Driven Reinforcement Learning (DAD-RL) for Autonomous Decision-Making in Dynamic Environment",
    "authors": [
      "Jayabrata Chowdhury",
      "Venkataramanan Shivaraman",
      "Sumit Dangi",
      "Suresh Sundaram",
      "P. B. Sujit"
    ],
    "abstract": "Autonomous Vehicle (AV) decision making in urban environments is inherently\nchallenging due to the dynamic interactions with surrounding vehicles. For safe\nplanning, AV must understand the weightage of various spatiotemporal\ninteractions in a scene. Contemporary works use colossal transformer\narchitectures to encode interactions mainly for trajectory prediction,\nresulting in increased computational complexity. To address this issue without\ncompromising spatiotemporal understanding and performance, we propose the\nsimple Deep Attention Driven Reinforcement Learning (DADRL) framework, which\ndynamically assigns and incorporates the significance of surrounding vehicles\ninto the ego's RL driven decision making process. We introduce an AV centric\nspatiotemporal attention encoding (STAE) mechanism for learning the dynamic\ninteractions with different surrounding vehicles. To understand map and route\ncontext, we employ a context encoder to extract features from context maps. The\nspatiotemporal representations combined with contextual encoding provide a\ncomprehensive state representation. The resulting model is trained using the\nSoft Actor Critic (SAC) algorithm. We evaluate the proposed framework on the\nSMARTS urban benchmarking scenarios without traffic signals to demonstrate that\nDADRL outperforms recent state of the art methods. Furthermore, an ablation\nstudy underscores the importance of the context-encoder and spatio temporal\nattention encoder in achieving superior performance.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.08932v2",
    "published_date": "2024-07-12 02:34:44 UTC",
    "updated_date": "2024-09-28 05:26:29 UTC"
  },
  {
    "arxiv_id": "2407.08918v1",
    "title": "Exploring Knowledge Transfer in Evolutionary Many-task Optimization: A Complex Network Perspective",
    "authors": [
      "Yudong Yang",
      "Kai Wu",
      "Xiangyi Teng",
      "Handing Wang",
      "He Yu",
      "Jing Liu"
    ],
    "abstract": "The field of evolutionary many-task optimization (EMaTO) is increasingly\nrecognized for its ability to streamline the resolution of optimization\nchallenges with repetitive characteristics, thereby conserving computational\nresources. This paper tackles the challenge of crafting efficient knowledge\ntransfer mechanisms within EMaTO, a task complicated by the computational\ndemands of individual task evaluations. We introduce a novel framework that\nemploys a complex network to comprehensively analyze the dynamics of knowledge\ntransfer between tasks within EMaTO. By extracting and scrutinizing the\nknowledge transfer network from existing EMaTO algorithms, we evaluate the\ninfluence of network modifications on overall algorithmic efficacy. Our\nfindings indicate that these networks are diverse, displaying\ncommunity-structured directed graph characteristics, with their network density\nadapting to different task sets. This research underscores the viability of\nintegrating complex network concepts into EMaTO to refine knowledge transfer\nprocesses, paving the way for future advancements in the domain.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, accepted by GECCO 2024 poster",
    "pdf_url": "http://arxiv.org/pdf/2407.08918v1",
    "published_date": "2024-07-12 01:49:04 UTC",
    "updated_date": "2024-07-12 01:49:04 UTC"
  },
  {
    "arxiv_id": "2407.08910v1",
    "title": "PAIL: Performance based Adversarial Imitation Learning Engine for Carbon Neutral Optimization",
    "authors": [
      "Yuyang Ye",
      "Lu-An Tang",
      "Haoyu Wang",
      "Runlong Yu",
      "Wenchao Yu",
      "Erhu He",
      "Haifeng Chen",
      "Hui Xiong"
    ],
    "abstract": "Achieving carbon neutrality within industrial operations has become\nincreasingly imperative for sustainable development. It is both a significant\nchallenge and a key opportunity for operational optimization in industry 4.0.\nIn recent years, Deep Reinforcement Learning (DRL) based methods offer\npromising enhancements for sequential optimization processes and can be used\nfor reducing carbon emissions. However, existing DRL methods need a pre-defined\nreward function to assess the impact of each action on the final sustainable\ndevelopment goals (SDG). In many real applications, such a reward function\ncannot be given in advance. To address the problem, this study proposes a\nPerformance based Adversarial Imitation Learning (PAIL) engine. It is a novel\nmethod to acquire optimal operational policies for carbon neutrality without\nany pre-defined action rewards. Specifically, PAIL employs a Transformer-based\npolicy generator to encode historical information and predict following actions\nwithin a multi-dimensional space. The entire action sequence will be\niteratively updated by an environmental simulator. Then PAIL uses a\ndiscriminator to minimize the discrepancy between generated sequences and\nreal-world samples of high SDG. In parallel, a Q-learning framework based\nperformance estimator is designed to estimate the impact of each action on SDG.\nBased on these estimations, PAIL refines generated policies with the rewards\nfrom both discriminator and performance estimator. PAIL is evaluated on\nmultiple real-world application cases and datasets. The experiment results\ndemonstrate the effectiveness of PAIL comparing to other state-of-the-art\nbaselines. In addition, PAIL offers meaningful interpretability for the\noptimization in carbon neutrality.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08910v1",
    "published_date": "2024-07-12 01:06:01 UTC",
    "updated_date": "2024-07-12 01:06:01 UTC"
  },
  {
    "arxiv_id": "2407.08908v1",
    "title": "Are They the Same Picture? Adapting Concept Bottleneck Models for Human-AI Collaboration in Image Retrieval",
    "authors": [
      "Vaibhav Balloli",
      "Sara Beery",
      "Elizabeth Bondi-Kelly"
    ],
    "abstract": "Image retrieval plays a pivotal role in applications from wildlife\nconservation to healthcare, for finding individual animals or relevant images\nto aid diagnosis. Although deep learning techniques for image retrieval have\nadvanced significantly, their imperfect real-world performance often\nnecessitates including human expertise. Human-in-the-loop approaches typically\nrely on humans completing the task independently and then combining their\nopinions with an AI model in various ways, as these models offer very little\ninterpretability or \\textit{correctability}. To allow humans to intervene in\nthe AI model instead, thereby saving human time and effort, we adapt the\nConcept Bottleneck Model (CBM) and propose \\texttt{CHAIR}. \\texttt{CHAIR} (a)\nenables humans to correct intermediate concepts, which helps \\textit{improve}\nembeddings generated, and (b) allows for flexible levels of intervention that\naccommodate varying levels of human expertise for better retrieval. To show the\nefficacy of \\texttt{CHAIR}, we demonstrate that our method performs better than\nsimilar models on image retrieval metrics without any external intervention.\nFurthermore, we also showcase how human intervention helps further improve\nretrieval performance, thereby achieving human-AI complementarity.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at Human-Centred AI Track at IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.08908v1",
    "published_date": "2024-07-12 00:59:32 UTC",
    "updated_date": "2024-07-12 00:59:32 UTC"
  },
  {
    "arxiv_id": "2407.08906v2",
    "title": "AirSketch: Generative Motion to Sketch",
    "authors": [
      "Hui Xian Grace Lim",
      "Xuanming Cui",
      "Ser-Nam Lim",
      "Yogesh S Rawat"
    ],
    "abstract": "Illustration is a fundamental mode of human expression and communication.\nCertain types of motion that accompany speech can provide this illustrative\nmode of communication. While Augmented and Virtual Reality technologies (AR/VR)\nhave introduced tools for producing drawings with hand motions (air drawing),\nthey typically require costly hardware and additional digital markers, thereby\nlimiting their accessibility and portability. Furthermore, air drawing demands\nconsiderable skill to achieve aesthetic results. To address these challenges,\nwe introduce the concept of AirSketch, aimed at generating faithful and\nvisually coherent sketches directly from hand motions, eliminating the need for\ncomplicated headsets or markers. We devise a simple augmentation-based\nself-supervised training procedure, enabling a controllable image diffusion\nmodel to learn to translate from highly noisy hand tracking images to clean,\naesthetically pleasing sketches, while preserving the essential visual cues\nfrom the original tracking data. We present two air drawing datasets to study\nthis problem. Our findings demonstrate that beyond producing photo-realistic\nimages from precise spatial inputs, controllable image diffusion can\neffectively produce a refined, clear sketch from a noisy input. Our work serves\nas an initial step towards marker-less air drawing and reveals distinct\napplications of controllable diffusion models to AirSketch and AR/VR in\ngeneral.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08906v2",
    "published_date": "2024-07-12 00:52:04 UTC",
    "updated_date": "2024-11-10 21:07:59 UTC"
  },
  {
    "arxiv_id": "2407.18949v1",
    "title": "Predicting Winning Captions for Weekly New Yorker Comics",
    "authors": [
      "Stanley Cao",
      "Sonny Young"
    ],
    "abstract": "Image captioning using Vision Transformers (ViTs) represents a pivotal\nconvergence of computer vision and natural language processing, offering the\npotential to enhance user experiences, improve accessibility, and provide\ntextual representations of visual data. This paper explores the application of\nimage captioning techniques to New Yorker cartoons, aiming to generate captions\nthat emulate the wit and humor of winning entries in the New Yorker Cartoon\nCaption Contest. This task necessitates sophisticated visual and linguistic\nprocessing, along with an understanding of cultural nuances and humor. We\npropose several new baselines for using vision transformer encoder-decoder\nmodels to generate captions for the New Yorker cartoon caption contest.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.18949v1",
    "published_date": "2024-07-12 00:45:00 UTC",
    "updated_date": "2024-07-12 00:45:00 UTC"
  },
  {
    "arxiv_id": "2407.08903v1",
    "title": "TensorTEE: Unifying Heterogeneous TEE Granularity for Efficient Secure Collaborative Tensor Computing",
    "authors": [
      "Husheng Han",
      "Xinyao Zheng",
      "Yuanbo Wen",
      "Yifan Hao",
      "Erhu Feng",
      "Ling Liang",
      "Jianan Mu",
      "Xiaqing Li",
      "Tianyun Ma",
      "Pengwei Jin",
      "Xinkai Song",
      "Zidong Du",
      "Qi Guo",
      "Xing Hu"
    ],
    "abstract": "Heterogeneous collaborative computing with NPU and CPU has received\nwidespread attention due to its substantial performance benefits. To ensure\ndata confidentiality and integrity during computing, Trusted Execution\nEnvironments (TEE) is considered a promising solution because of its\ncomparatively lower overhead. However, existing heterogeneous TEE designs are\ninefficient for collaborative computing due to fine and different memory\ngranularities between CPU and NPU. 1) The cacheline granularity of CPU TEE\nintensifies memory pressure due to its extra memory access, and 2) the\ncacheline granularity MAC of NPU escalates the pressure on the limited memory\nstorage. 3) Data transfer across heterogeneous enclaves relies on the transit\nof non-secure regions, resulting in cumbersome re-encryption and scheduling.\n  To address these issues, we propose TensorTEE, a unified tensor-granularity\nheterogeneous TEE for efficient secure collaborative tensor computing. First,\nwe virtually support tensor granularity in CPU TEE to eliminate the off-chip\nmetadata access by detecting and maintaining tensor structures on-chip. Second,\nwe propose tensor-granularity MAC management with predictive execution to avoid\ncomputational stalls while eliminating off-chip MAC storage and access.\nMoreover, based on the unified granularity, we enable direct data transfer\nwithout re-encryption and scheduling dilemmas. Our evaluation is built on\nenhanced Gem5 and a cycle-accurate NPU simulator. The results show that\nTensorTEE improves the performance of Large Language Model (LLM) training\nworkloads by 4.0x compared to existing work and incurs only 2.1% overhead\ncompared to non-secure training, offering a practical security assurance for\nLLM training.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by ASPLOS 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.08903v1",
    "published_date": "2024-07-12 00:35:18 UTC",
    "updated_date": "2024-07-12 00:35:18 UTC"
  },
  {
    "arxiv_id": "2407.08902v1",
    "title": "Application of Artificial Intelligence in Supporting Healthcare Professionals and Caregivers in Treatment of Autistic Children",
    "authors": [
      "Hossein Mohammadi Rouzbahani",
      "Hadis Karimipour"
    ],
    "abstract": "Autism Spectrum Disorder (ASD) represents a multifaceted neurodevelopmental\ncondition marked by difficulties in social interaction, communication\nimpediments, and repetitive behaviors. Despite progress in understanding ASD,\nits diagnosis and treatment continue to pose significant challenges due to the\nvariability in symptomatology and the necessity for multidisciplinary care\napproaches. This paper investigates the potential of Artificial Intelligence\n(AI) to augment the capabilities of healthcare professionals and caregivers in\nmanaging ASD. We have developed a sophisticated algorithm designed to analyze\nfacial and bodily expressions during daily activities of both autistic and\nnon-autistic children, leading to the development of a powerful deep\nlearning-based autism detection system. Our study demonstrated that AI models,\nspecifically the Xception and ResNet50V2 architectures, achieved high accuracy\nin diagnosing Autism Spectrum Disorder (ASD). This research highlights the\ntransformative potential of AI in improving the diagnosis, treatment, and\ncomprehensive management of ASD. Our study revealed that AI models, notably the\nXception and ResNet50V2 architectures, demonstrated high accuracy in diagnosing\nASD.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08902v1",
    "published_date": "2024-07-12 00:34:40 UTC",
    "updated_date": "2024-07-12 00:34:40 UTC"
  },
  {
    "arxiv_id": "2407.08898v1",
    "title": "IDAT: A Multi-Modal Dataset and Toolkit for Building and Evaluating Interactive Task-Solving Agents",
    "authors": [
      "Shrestha Mohanty",
      "Negar Arabzadeh",
      "Andrea Tupini",
      "Yuxuan Sun",
      "Alexey Skrynnik",
      "Artem Zholus",
      "Marc-Alexandre Côté",
      "Julia Kiseleva"
    ],
    "abstract": "Seamless interaction between AI agents and humans using natural language\nremains a key goal in AI research. This paper addresses the challenges of\ndeveloping interactive agents capable of understanding and executing grounded\nnatural language instructions through the IGLU competition at NeurIPS. Despite\nadvancements, challenges such as a scarcity of appropriate datasets and the\nneed for effective evaluation platforms persist. We introduce a scalable data\ncollection tool for gathering interactive grounded language instructions within\na Minecraft-like environment, resulting in a Multi-Modal dataset with around\n9,000 utterances and over 1,000 clarification questions. Additionally, we\npresent a Human-in-the-Loop interactive evaluation platform for qualitative\nanalysis and comparison of agent performance through multi-turn communication\nwith human annotators. We offer to the community these assets referred to as\nIDAT (IGLU Dataset And Toolkit) which aim to advance the development of\nintelligent, interactive AI agents and provide essential resources for further\nresearch.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08898v1",
    "published_date": "2024-07-12 00:07:43 UTC",
    "updated_date": "2024-07-12 00:07:43 UTC"
  }
]
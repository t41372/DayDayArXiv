{
  "date": "2025-01-03",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-01-03 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 和大型语言模型（LLM）的创新应用，包括代码生成、多模态模型评估、LLM 安全与泛化、医疗预测以及强化学习等领域，其中 LLM 综述和多代理学习等文章令人印象深刻，同时涉及知名学者如 Connor W. Coley 和 Ming Yin 的工作，突显了 AI 在实际问题中的潜力。\n\n### 重点论文讨论\n我们先聊聊 AI 和 LLM 相关的重磅文章，这些工作可能引发广泛话题。\n\n- **Effective LLM-Driven Code Generation with Pythoness（有效 LLM 驱动的代码生成：Pythoness）**  \n  作者：Kyla H. Levin 等。主要贡献是通过嵌入式领域特定语言（DSL）Pythoness，让开发者使用行为规范（如单元测试）指导 LLM 生成更可靠的代码，显著提升代码质量并降低风险。该工作在 LLM 代码生成领域提供实用框架，适用于软件开发。\n\n- **AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs（AVTrustBench：评估和增强音频-视觉 LLM 的可靠性和鲁棒性）**  \n  作者：Sanjoy Chowdhury 等。论文提出一个包含 60 万样本的基准 AVTrustBench，用于评估多模态 LLM 在对抗攻击、组合推理和模态依赖方面的性能，并引入 CAVPref 训练策略，提高任务准确率达 30%，为多模态模型的安全性提供新方向。\n\n- **A Survey on Large Language Models with some Insights on their Capabilities and Limitations（大型语言模型的调查：对其能力和局限性的洞见）**  \n  作者：Andrea Matarazzo 等。这是一篇全面综述，分析 LLM（如 GPT 和 LLaMA）的架构、扩展机制和应用（如医疗、金融），并探讨 Chain of Thought（CoT）和 Plan of Thought（PoT）的推理能力，强调 LLM 的泛化与外部系统整合，适合研究者参考。\n\n- **Cold-Start Recommendation towards the Era of Large Language Models（面向 LLM 时代的冷启动推荐）**  \n  作者：Weizhi Zhang 等（包括知名学者如 Irwin King 和 Philip S. Yu）。论文综述 LLM 在冷启动推荐中的作用，从内容特征到世界知识，构建了资源链接，强调 LLM 如何缓解数据稀疏问题，对推荐系统领域有重要启发。\n\n- **Mitigating Hallucination for Large Vision Language Model by Inter-Modality Correlation Calibration Decoding（通过跨模态相关性校准解码缓解大型视觉语言模型的幻觉）**  \n  作者：Jiaming Li 等。核心发现是通过 Cross-Modal Value-Enhanced Decoding 和 Content-Driven Attention Refinement 模块，减少 VLM 的幻觉问题，提升生成文本的准确性，在多模态任务中表现出色。\n\n- **Virgo: A Preliminary Exploration on Reproducing o1-like MLLM（Virgo：初步探索复制 o1-like 多模态 LLM）**  \n  作者：Yifan Du 等。论文通过微调 MLLM 以慢思考能力（如长形式推理），实现多模态任务改进，证明文本数据可有效转移到视觉领域，资源已开源。\n\n- **AgentRefine: Enhancing Agent Generalization through Refinement Tuning（AgentRefine：通过细化调优增强代理泛化）**  \n  作者：Dayuan Fu 等。贡献在于提出框架让代理模型从错误中学习，生成更鲁棒的推理路径，提升泛化能力，实验显示在多任务上优于现有方法。\n\n其他相关论文简要概述，按主题归类：\n\n- **LLM 安全与优化**  \n  - **Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models（Auto-RT：针对 LLM 的自动越狱策略探索）**  \n    作者：Yanjiang Liu 等。通过强化学习框架探索攻击策略，减少 LLM 漏洞，实验显示成功率高达 16.63% 提升。\n  - **How Toxic Can You Get? Search-based Toxicity Testing for Large Language Models（LLM 毒性测试：基于搜索的评估）**  \n    作者：Simone Corbo 等。提出 EvoTox 框架检测 LLM 响应毒性，实验证明比基线方法更有效。\n  - **Spot Risks Before Speaking! Unraveling Safety Attention Heads in Large Vision-Language Models（在发言前发现风险：解析 VLM 的安全注意力头）**  \n    作者：Ziwei Zheng 等。通过分析注意力头构建恶意提示检测器，提升 VLM 安全性。\n\n- **医疗与计算机视觉**  \n  - **Advancing Pancreatic Cancer Prediction with a Next Visit Token Prediction Head on top of Med-BERT（基于 Med-BERT 的胰腺癌预测进展）**  \n    作者：Jianping He 等。将预测任务转化为令牌预测，提高少样本癌症诊断准确性。\n  - **Merging Context Clustering with Visual State Space Models for Medical Image Segmentation（融合上下文聚类与视觉状态空间模型的医学图像分割）**  \n    作者：Yun Zhu 等。提出 CCViM 方法结合长短范围特征，提升医学图像分割性能。\n\n- **强化学习与推荐**  \n  - **On the Statistical Complexity for Offline and Low-Adaptive Reinforcement Learning with Structures（带结构的离线和低自适应强化学习的统计复杂性）**  \n    作者：Ming Yin 等（知名学者）。综述离线 RL 的统计边界和算法，提供近似最优方法。\n  - **Contrastive Learning Augmented Social Recommendations（对比学习增强的社会推荐）**  \n    作者：Lin Wang 等。通过对比学习和社交图重构，改善冷启动推荐。\n\n- **其他领域快速掠过**  \n  剩余论文如 **Siamese Networks for Cat Re-Identification（Siamese 网络用于猫重识别）** 和 **The Proof is in the Almond Cookies（食谱理解的机器人框架）** 等，聚焦 niche 应用，如计算机视觉和机器人，但影响较小，仅贡献了特定任务的工具或数据集。\n\n今天的论文展示了 AI 在代码、医疗和安全领域的强劲进展，值得关注后续应用。更多细节可查阅 arXiv！",
  "papers": [
    {
      "arxiv_id": "2501.02138v1",
      "title": "Effective LLM-Driven Code Generation with Pythoness",
      "title_zh": "翻译失败",
      "authors": [
        "Kyla H. Levin",
        "Kyle Gwilt",
        "Emery D. Berger",
        "Stephen N. Freund"
      ],
      "abstract": "The advent of large language models (LLMs) has paved the way for a new era of\nprogramming tools with both significant capabilities and risks, as the\ngenerated code lacks guarantees of correctness and reliability. Developers\nusing LLMs currently face the difficult task of optimizing, integrating, and\nmaintaining code generated by AI. We propose an embedded domain-specific\nlanguage (DSL), Pythoness, to address those challenges. In Pythoness,\ndevelopers program with LLMs at a higher level of abstraction. Rather than\ninteracting directly with generated code, developers using Pythoness operate at\nthe level of behavioral specifications when writing functions, classes, or an\nentire program. These specifications can take the form of unit tests and\nproperty-based tests, which may be expressed formally or in natural language.\nGuided by these specifications, Pythoness generates code that both passes the\ntests and can be continuously checked during execution. We posit that the\nPythoness approach lets developers harness the full potential of LLMs for code\ngeneration while substantially mitigating their inherent risks. We describe our\ncurrent prototype implementation of Pythoness and demonstrate that it can\nsuccessfully leverage a combination of tests and code generation to yield\nhigher quality code than specifications alone.",
      "tldr_zh": "这篇论文提出 Pythoness，一个嵌入式领域特定语言 (DSL)，旨在解决大型语言模型 (LLMs) 生成代码的正确性和可靠性问题，让开发者在更高抽象级别上编程。Pythoness 通过行为规范（如 unit tests 和 property-based tests）来指导代码生成，这些规范可以采用正式或自然语言形式，确保生成的代码不仅通过测试，还能在执行过程中持续检查。相比仅使用规范，Pythoness 的原型实现能产生更高质量的代码，从而帮助开发者最大化 LLMs 的潜力，同时显著降低风险。",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.PL",
      "comment": "5 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.02138v1",
      "published_date": "2025-01-03 23:14:46 UTC",
      "updated_date": "2025-01-03 23:14:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:04:47.839218"
    },
    {
      "arxiv_id": "2501.02135v1",
      "title": "AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Sanjoy Chowdhury",
        "Sayan Nag",
        "Subhrajyoti Dasgupta",
        "Yaoting Wang",
        "Mohamed Elhoseiny",
        "Ruohan Gao",
        "Dinesh Manocha"
      ],
      "abstract": "With the rapid advancement of Multi-modal Large Language Models (MLLMs),\nseveral diagnostic benchmarks have recently been developed to assess these\nmodels' multi-modal reasoning proficiency. However, these benchmarks are\nrestricted to assessing primarily the visual aspect and do not examine the\nholistic audio-visual (AV) understanding. Moreover, currently, there are no\nbenchmarks that investigate the capabilities of AVLLMs to calibrate their\nresponses when presented with perturbed inputs. To this end, we introduce\nAudio-Visual Trustworthiness assessment Benchmark (AVTrustBench), comprising\n600K samples spanning over 9 meticulously crafted tasks, evaluating the\ncapabilities of AVLLMs across three distinct dimensions: Adversarial attack,\nCompositional reasoning, and Modality-specific dependency. Using our benchmark\nwe extensively evaluate 13 state-of-the-art AVLLMs. The findings reveal that\nthe majority of existing models fall significantly short of achieving\nhuman-like comprehension, offering valuable insights for future research\ndirections. To alleviate the limitations in the existing approaches, we further\npropose a robust, model-agnostic calibrated audio-visual preference\noptimization based training strategy CAVPref, obtaining a gain up to 30.19%\nacross all 9 tasks. We will publicly release our code and benchmark to\nfacilitate future research in this direction.",
      "tldr_zh": "该论文引入了AVTrustBench，一种包含600K样本的基准，用于评估音频-视觉大语言模型(AVLLMs)的可靠性、鲁棒性和整体音频-视觉理解，涵盖Adversarial attack、Compositional reasoning和Modality-specific dependency三个维度。研究评估了13个最先进的AVLLMs，发现这些模型在性能上远低于人类水平，提供宝贵见解以指导未来研究。为提升模型能力，论文提出了一种模型无关的校准音频-视觉偏好优化训练策略CAVPref，在9个任务中实现了高达30.19%的性能提升，并计划公开代码和基准以促进相关研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02135v1",
      "published_date": "2025-01-03 23:03:24 UTC",
      "updated_date": "2025-01-03 23:03:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:04:59.714731"
    },
    {
      "arxiv_id": "2501.02132v2",
      "title": "A hybrid marketplace of ideas",
      "title_zh": "翻译失败",
      "authors": [
        "Tomer Jordi Chaffer",
        "Dontrail Cotlage",
        "Justin Goldston"
      ],
      "abstract": "The convergence of humans and artificial intelligence systems introduces new\ndynamics into the cultural and intellectual landscape. Complementing emerging\ncultural evolution concepts such as machine culture, AI agents represent a\nsignificant techno-sociological development, particularly within the\nanthropological study of Web3 as a community focused on decentralization\nthrough blockchain. Despite their growing presence, the cultural significance\nof AI agents remains largely unexplored in academic literature. Toward this\nend, we conceived hybrid netnography, a novel interdisciplinary approach that\nexamines the cultural and intellectual dynamics within digital ecosystems by\nanalyzing the interactions and contributions of both human and AI agents as\nco-participants in shaping narratives, ideas, and cultural artifacts. We argue\nthat, within the Web3 community on the social media platform X, these agents\nchallenge traditional notions of participation and influence in public\ndiscourse, creating a hybrid marketplace of ideas, a conceptual space where\nhuman and AI generated ideas coexist and compete for attention. We examine the\ncurrent state of AI agents in idea generation, propagation, and engagement,\npositioning their role as cultural agents through the lens of memetics and\nencouraging further inquiry into their cultural and societal impact.\nAdditionally, we address the implications of this paradigm for privacy,\nintellectual property, and governance, highlighting the societal and legal\nchallenges of integrating AI agents into the hybrid marketplace of ideas.",
      "tldr_zh": "这篇论文探讨了人类与 AI 代理在数字生态系统中的互动，特别关注 Web3 社区如何通过区块链实现去中心化，并引入了 hybrid netnography 作为一种新跨学科方法来分析双方在塑造叙事和文化制品中的共同作用。研究发现，在 X 平台的 Web3 社区中，AI 代理挑战了传统的参与和影响模式，形成了 hybrid marketplace of ideas 的概念，其中人类和 AI 生成的想法共存并竞争。论文还通过 memetics 的视角评估了 AI 代理在想法生成、传播和参与中的文化角色，并讨论了这一范式对隐私、知识产权和治理的潜在社会及法律影响。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02132v2",
      "published_date": "2025-01-03 22:53:43 UTC",
      "updated_date": "2025-01-08 16:26:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:05:12.867853"
    },
    {
      "arxiv_id": "2502.15696v1",
      "title": "Integrating Domain Knowledge into Large Language Models for Enhanced Fashion Recommendations",
      "title_zh": "将领域知识集成到大语言模型中以提升时尚推荐",
      "authors": [
        "Zhan Shi",
        "Shanglin Yang"
      ],
      "abstract": "Fashion, deeply rooted in sociocultural dynamics, evolves as individuals\nemulate styles popularized by influencers and iconic figures. In the quest to\nreplicate such refined tastes using artificial intelligence, traditional\nfashion ensemble methods have primarily used supervised learning to imitate the\ndecisions of style icons, which falter when faced with distribution shifts,\nleading to style replication discrepancies triggered by slight variations in\ninput. Meanwhile, large language models (LLMs) have become prominent across\nvarious sectors, recognized for their user-friendly interfaces, strong\nconversational skills, and advanced reasoning capabilities. To address these\nchallenges, we introduce the Fashion Large Language Model (FLLM), which employs\nauto-prompt generation training strategies to enhance its capacity for\ndelivering personalized fashion advice while retaining essential domain\nknowledge. Additionally, by integrating a retrieval augmentation technique\nduring inference, the model can better adjust to individual preferences. Our\nresults show that this approach surpasses existing models in accuracy,\ninterpretability, and few-shot learning capabilities.",
      "tldr_zh": "该论文探讨了如何将领域知识整合到 Large Language Models (LLMs) 中，以提升时尚推荐系统的性能。研究引入 Fashion Large Language Model (FLLM)，通过 auto-prompt generation 训练策略生成个性化时尚建议，同时保留关键领域知识，并采用 retrieval augmentation 技术来适应用户偏好。结果表明，FLLM 在准确性、可解释性和 few-shot learning 能力上均优于传统方法，为时尚推荐领域的AI应用提供了更可靠的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15696v1",
      "published_date": "2025-01-03 21:49:44 UTC",
      "updated_date": "2025-01-03 21:49:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:05:22.896754"
    },
    {
      "arxiv_id": "2501.02114v1",
      "title": "Relaxation-assisted reverse annealing on nonnegative/binary matrix factorization",
      "title_zh": "翻译失败",
      "authors": [
        "Renichiro Haba",
        "Masayuki Ohzeki",
        "Kazuyuki Tanaka"
      ],
      "abstract": "Quantum annealing has garnered significant attention as meta-heuristics\ninspired by quantum physics for combinatorial optimization problems. Among its\nmany applications, nonnegative/binary matrix factorization stands out for its\ncomplexity and relevance in unsupervised machine learning. The use of reverse\nannealing, a derivative procedure of quantum annealing to prioritize the search\nin a vicinity under a given initial state, helps improve its optimization\nperformance in matrix factorization. This study proposes an improved strategy\nthat integrates reverse annealing with a linear programming relaxation\ntechnique. Using relaxed solutions as the initial configuration for reverse\nannealing, we demonstrate improvements in optimization performance comparable\nto the exact optimization methods. Our experiments on facial image datasets\nshow that our method provides better convergence than known reverse annealing\nmethods. Furthermore, we investigate the effectiveness of relaxation-based\ninitialization methods on randomized datasets, demonstrating a relationship\nbetween the relaxed solution and the optimal solution. This research\nunderscores the potential of combining reverse annealing and classical\noptimization strategies to enhance optimization performance.",
      "tldr_zh": "本文提出了一种改进策略，将反向退火(reverse annealing)与线性规划松弛(linear programming relaxation)技术相结合，用于处理非负/二进制矩阵分解(nonnegative/binary matrix factorization)的组合优化问题。方法通过使用松弛解作为反向退火的初始配置，提升了优化性能，并在面部图像数据集上的实验中显示出比传统反向退火方法更好的收敛性。研究还验证了这一初始化方法在随机数据集上的有效性，揭示了松弛解与最优解之间的关系，并强调了量子退火(quantum annealing)与经典优化策略的结合潜力。",
      "categories": [
        "quant-ph",
        "cond-mat.stat-mech",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02114v1",
      "published_date": "2025-01-03 21:48:35 UTC",
      "updated_date": "2025-01-03 21:48:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:06:43.184962"
    },
    {
      "arxiv_id": "2501.02112v1",
      "title": "Siamese Networks for Cat Re-Identification: Exploring Neural Models for Cat Instance Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Tobias Trein",
        "Luan Fonseca Garcia"
      ],
      "abstract": "Street cats in urban areas often rely on human intervention for survival,\nleading to challenges in population control and welfare management. In April\n2023, Hello Inc., a Chinese urban mobility company, launched the Hello Street\nCat initiative to address these issues. The project deployed over 21,000 smart\nfeeding stations across 14 cities in China, integrating livestreaming cameras\nand treat dispensers activated through user donations. It also promotes the\nTrap-Neuter-Return (TNR) method, supported by a community-driven platform,\nHelloStreetCatWiki, where volunteers catalog and identify cats. However, manual\nidentification is inefficient and unsustainable, creating a need for automated\nsolutions. This study explores Deep Learning-based models for re-identifying\nstreet cats in the Hello Street Cat initiative. A dataset of 2,796 images of 69\ncats was used to train Siamese Networks with EfficientNetB0, MobileNet and\nVGG16 as base models, evaluated under contrastive and triplet loss functions.\nVGG16 paired with contrastive loss emerged as the most effective configuration,\nachieving up to 97% accuracy and an F1 score of 0.9344 during testing. The\napproach leverages image augmentation and dataset refinement to overcome\nchallenges posed by limited data and diverse visual variations. These findings\nunderscore the potential of automated cat re-identification to streamline\npopulation monitoring and welfare efforts. By reducing reliance on manual\nprocesses, the method offers a scalable and reliable solution for\ncommunitydriven initiatives. Future research will focus on expanding datasets\nand developing real-time implementations to enhance practicality in large-scale\ndeployments.",
      "tldr_zh": "本研究针对街头猫识别的挑战，探索了基于 Siamese Networks 的深度学习模型，以支持 Hello Street Cat 倡议的猫咪福利管理。研究使用 EfficientNetB0、MobileNet 和 VGG16 作为基模型，在 contrastive loss 和 triplet loss 下训练模型，并通过图像增强和数据集优化处理数据有限的问题。结果显示，VGG16 与 contrastive loss 组合表现出最佳性能，达到 97% 准确率和 0.9344 F1 分数。这些发现为自动化猫再识别提供可扩展的解决方案，有助于简化人口监测和福利努力，未来计划扩展数据集并实现实时部署。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2.10"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 3 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.02112v1",
      "published_date": "2025-01-03 21:37:49 UTC",
      "updated_date": "2025-01-03 21:37:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:05:49.076915"
    },
    {
      "arxiv_id": "2501.02107v1",
      "title": "Online Detection of Water Contamination Under Concept Drift",
      "title_zh": "翻译失败",
      "authors": [
        "Jin Li",
        "Kleanthis Malialis",
        "Stelios G. Vrachimis",
        "Marios M. Polycarpou"
      ],
      "abstract": "Water Distribution Networks (WDNs) are vital infrastructures, and\ncontamination poses serious public health risks. Harmful substances can\ninteract with disinfectants like chlorine, making chlorine monitoring essential\nfor detecting contaminants. However, chlorine sensors often become unreliable\nand require frequent calibration. This study introduces the Dual-Threshold\nAnomaly and Drift Detection (AD&DD) method, an unsupervised approach combining\na dual-threshold drift detection mechanism with an LSTM-based Variational\nAutoencoder(LSTM-VAE) for real-time contamination detection. Tested on two\nrealistic WDNs, AD&DD effectively identifies anomalies with sensor offsets as\nconcept drift, and outperforms other methods. A proposed decentralized\narchitecture enables accurate contamination detection and localization by\ndeploying AD&DD on selected nodes.",
      "tldr_zh": "这篇论文针对水分配网络(WDNs)中水污染检测的挑战，特别是概念漂移问题（如氯传感器偏移），提出了一种无监督方法Dual-Threshold Anomaly and Drift Detection (AD&DD)。该方法结合双阈值漂移检测机制和LSTM-based Variational Autoencoder (LSTM-VAE)，实现实时异常识别和污染检测。在两个真实WDNs上进行的实验表明，AD&DD的表现优于其他方法，能够有效识别传感器偏移作为概念漂移。此外，论文还提出一个去中心化架构，通过在选定节点部署AD&DD，实现污染的准确检测和定位。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02107v1",
      "published_date": "2025-01-03 21:29:09 UTC",
      "updated_date": "2025-01-03 21:29:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:07:53.247762"
    },
    {
      "arxiv_id": "2501.04040v2",
      "title": "A Survey on Large Language Models with some Insights on their Capabilities and Limitations",
      "title_zh": "大型语言模型的综述：及其能力",
      "authors": [
        "Andrea Matarazzo",
        "Riccardo Torlone"
      ],
      "abstract": "The rapid advancement of artificial intelligence, particularly with the\ndevelopment of Large Language Models (LLMs) built on the transformer\narchitecture, has redefined the capabilities of natural language processing.\nThese models now exhibit remarkable performance across various language-related\ntasks, such as text generation, question answering, translation, and\nsummarization, often rivaling human-like comprehension. More intriguingly, LLMs\nhave demonstrated emergent abilities extending beyond their core functions,\nshowing proficiency in tasks like commonsense reasoning, code generation, and\narithmetic. This survey paper explores the foundational components, scaling\nmechanisms, and architectural strategies that drive these capabilities.\nEmphasizing models like GPT and LLaMA, we analyze the impact of exponential\ndata and computational growth on LLM performance, while also addressing the\ntrade-offs associated with scaling. We also examine LLM applications across\nsectors, such as healthcare, finance, education, and law, highlighting their\nadaptability and potential to solve domain-specific challenges. Central to this\nwork are the questions of how LLMs generalize across diverse tasks, exhibit\nplanning, and reasoning abilities, and whether these emergent abilities can be\nsystematically elicited or enhanced. In particular, we provide some insights\ninto the CoT (Chain of Thought) and PoT (Plan of Thought) abilities within\nLLMs, focusing on how pre-training data influences their emergence.\nAdditionally, we investigate LLM-modulo frameworks that integrate external\nsystems, allowing LLMs to handle complex, dynamic tasks. By analyzing these\nfactors, this paper aims to foster the ongoing discussion on the capabilities\nand limits of LLMs, promoting their responsible development and application in\nnovel and increasingly complex environments.",
      "tldr_zh": "这篇调查论文探讨了大型语言模型(LLMs)的快速发展及其基于transformer架构的强大性能，包括在文本生成、问答、翻译和总结等任务上的表现，以及新兴能力如常识推理、代码生成和算术运算。论文分析了驱动LLMs能力的核心组件、缩放机制和架构策略（如GPT和LLaMA模型），并考察了预训练数据对CoT(Chain of Thought)和PoT(Plan of Thought)推理能力的影响，同时讨论了其在医疗、金融、教育和法律等领域的应用及潜在挑战。最终，该研究强调了LLMs的泛化与局限性，呼吁通过整合外部系统的LLM-modulo框架来促进其负责任发展和创新应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.NE",
        "I.2.7; I.2.6"
      ],
      "primary_category": "cs.CL",
      "comment": "174 pages, to be submitted to a journal in a shorter version. It\n  includes figures taken from papers by other authors. All the sources have\n  been referenced. arXiv admin note: text overlap with arXiv:2303.18223 by\n  other authors",
      "pdf_url": "http://arxiv.org/pdf/2501.04040v2",
      "published_date": "2025-01-03 21:04:49 UTC",
      "updated_date": "2025-02-09 08:00:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:06:12.689240"
    },
    {
      "arxiv_id": "2501.02089v1",
      "title": "On the Statistical Complexity for Offline and Low-Adaptive Reinforcement Learning with Structures",
      "title_zh": "翻译失败",
      "authors": [
        "Ming Yin",
        "Mengdi Wang",
        "Yu-Xiang Wang"
      ],
      "abstract": "This article reviews the recent advances on the statistical foundation of\nreinforcement learning (RL) in the offline and low-adaptive settings. We will\nstart by arguing why offline RL is the appropriate model for almost any\nreal-life ML problems, even if they have nothing to do with the recent AI\nbreakthroughs that use RL. Then we will zoom into two fundamental problems of\noffline RL: offline policy evaluation (OPE) and offline policy learning (OPL).\nIt may be surprising to people that tight bounds for these problems were not\nknown even for tabular and linear cases until recently. We delineate the\ndifferences between worst-case minimax bounds and instance-dependent bounds. We\nalso cover key algorithmic ideas and proof techniques behind near-optimal\ninstance-dependent methods in OPE and OPL. Finally, we discuss the limitations\nof offline RL and review a burgeoning problem of \\emph{low-adaptive\nexploration} which addresses these limitations by providing a sweet middle\nground between offline and online RL.",
      "tldr_zh": "这篇论文回顾了强化学习（Reinforcement Learning, RL）在离线和低自适应设置中的统计复杂性基础，强调离线 RL 适用于几乎所有现实机器学习问题，即使不涉及最新 AI 进展。论文聚焦于离线策略评估（OPE）和离线策略学习（OPL）的核心问题，揭示了即使在表格和线性情况下，这些问题的紧密 minimax bounds 和 instance-dependent bounds 直到最近才被明确界定，并介绍了相关算法想法和证明技术。最终，它讨论了离线 RL 的局限性，并提出低-adaptive exploration 作为一种介于离线和在线 RL 之间的折中方案，以提升探索效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Review Article",
      "pdf_url": "http://arxiv.org/pdf/2501.02089v1",
      "published_date": "2025-01-03 20:27:53 UTC",
      "updated_date": "2025-01-03 20:27:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:06:24.266745"
    },
    {
      "arxiv_id": "2501.02068v3",
      "title": "The interplay between domain specialization and model size",
      "title_zh": "领域专业化与模型大小的相互作用",
      "authors": [
        "Roseval Malaquias Junior",
        "Ramon Pires",
        "Thales Sales Almeida",
        "Kenzo Sakiyama",
        "Roseli A. F. Romero",
        "Rodrigo Nogueira"
      ],
      "abstract": "Scaling laws for language models have often focused on finding the optimal\nmodel size and token count for training from scratch. However, achieving this\noptimal balance requires significant compute resources due to the extensive\ndata demands when training models from randomly-initialized weights. Continued\npretraining offers a cost-effective alternative, leveraging the compute\ninvestment from pretrained models to incorporate new knowledge without\nrequiring extensive new data. Recent findings suggest that data quality\ninfluences constants in scaling laws, thereby altering the optimal\nparameter-token allocation ratio. Building on this insight, we investigate the\ninterplay between domain specialization and model size during continued\npretraining under compute-constrained scenarios. Our goal is to identify an\noptimal training regime for this scenario and detect patterns in this interplay\nthat can be generalized across different model sizes and domains. To compare\ngeneral and specialized training, we filtered a web-based dataset to extract\ndata from three domains: legal, medical, and accounting. We pretrained models\nwith 1.5B, 3B, 7B, and 14B parameters on both the unfiltered and filtered\ndatasets, then evaluated their performance on domain-specific exams. Results\nshow that as model size increases, specialized models outperform general models\nwhile requiring less training compute. Additionally, their growing compute\nefficiency leads to reduced forgetting of previously learned knowledge.",
      "tldr_zh": "本研究探讨了领域专业化(domain specialization)和模型大小在计算资源受限的继续预训练(continued pretraining)中的相互作用，旨在识别最佳训练方案并发现可泛化的模式。研究者过滤了一个基于网络的数据集，提取法律、医疗和会计领域的子集，对1.5B至14B参数的模型进行预训练，并与通用训练进行比较。结果显示，随着模型大小增加，专业化模型在领域特定考试中优于通用模型，同时需要更少的训练计算资源，并显著减少了对先前知识的遗忘。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02068v3",
      "published_date": "2025-01-03 19:28:53 UTC",
      "updated_date": "2025-03-29 17:18:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:06:36.577761"
    },
    {
      "arxiv_id": "2501.02064v2",
      "title": "ArtCrafter: Text-Image Aligning Style Transfer via Embedding Reframing",
      "title_zh": "翻译失败",
      "authors": [
        "Nisha Huang",
        "Kaer Huang",
        "Yifan Pu",
        "Jiangshan Wang",
        "Jie Guo",
        "Yiqiang Yan",
        "Xiu Li",
        "Tong-Yee Lee"
      ],
      "abstract": "Recent years have witnessed significant advancements in text-guided style\ntransfer, primarily attributed to innovations in diffusion models. These models\nexcel in conditional guidance, utilizing text or images to direct the sampling\nprocess. However, despite their capabilities, direct conditional guidance\napproaches often face challenges in balancing the expressiveness of textual\nsemantics with the diversity of output results while capturing stylistic\nfeatures. To address these challenges, we introduce ArtCrafter, a novel\nframework for text-to-image style transfer. Specifically, we introduce an\nattention-based style extraction module, meticulously engineered to capture the\nsubtle stylistic elements within an image. This module features a multi-layer\narchitecture that leverages the capabilities of perceiver attention mechanisms\nto integrate fine-grained information. Additionally, we present a novel\ntext-image aligning augmentation component that adeptly balances control over\nboth modalities, enabling the model to efficiently map image and text\nembeddings into a shared feature space. We achieve this through attention\noperations that enable smooth information flow between modalities. Lastly, we\nincorporate an explicit modulation that seamlessly blends multimodal enhanced\nembeddings with original embeddings through an embedding reframing design,\nempowering the model to generate diverse outputs. Extensive experiments\ndemonstrate that ArtCrafter yields impressive results in visual stylization,\nexhibiting exceptional levels of stylistic intensity, controllability, and\ndiversity.",
      "tldr_zh": "本文提出ArtCrafter，一种新型文本引导风格转移框架，旨在解决现有diffusion models在平衡文本语义表达性、输出多样性和风格特征捕捉方面的挑战。具体而言，该框架包括注意力-based风格提取模块（利用多层perceiver注意力机制提取图像微妙元素）、文本-图像对齐增强组件（通过注意力操作将嵌入映射到共享特征空间），以及嵌入重构设计来混合多模态嵌入，实现多样化输出生成。实验结果表明，ArtCrafter在视觉风格化任务中表现出色的风格强度、可控性和多样性，显著提升了整体性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 17 figures, submitted to a journal",
      "pdf_url": "http://arxiv.org/pdf/2501.02064v2",
      "published_date": "2025-01-03 19:17:27 UTC",
      "updated_date": "2025-04-17 12:49:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:08:04.969541"
    },
    {
      "arxiv_id": "2501.01951v3",
      "title": "MixGCN: Scalable GCN Training by Mixture of Parallelism and Mixture of Accelerators",
      "title_zh": "翻译失败",
      "authors": [
        "Cheng Wan",
        "Runkai Tao",
        "Zheng Du",
        "Yang Katie Zhao",
        "Yingyan Celine Lin"
      ],
      "abstract": "Graph convolutional networks (GCNs) have demonstrated superiority in\ngraph-based learning tasks. However, training GCNs on full graphs is\nparticularly challenging, due to the following two challenges: (1) the\nassociated feature tensors can easily explode the memory and block the\ncommunication bandwidth of modern accelerators, and (2) the computation\nworkflow in training GCNs alternates between sparse and dense matrix\noperations, complicating the efficient utilization of computational resources.\nExisting solutions for scalable distributed full-graph GCN training mostly\nadopt partition parallelism, which is unsatisfactory as they only partially\naddress the first challenge while incurring scaled-out communication volume. To\nthis end, we propose MixGCN aiming to simultaneously address both the\naforementioned challenges towards GCN training. To tackle the first challenge,\nMixGCN integrates mixture of parallelism. Both theoretical and empirical\nanalysis verify its constant communication volumes and enhanced balanced\nworkload; For handling the second challenge, we consider mixture of\naccelerators (i.e., sparse and dense accelerators) with a dedicated accelerator\nfor GCN training and a fine-grain pipeline. Extensive experiments show that\nMixGCN achieves boosted training efficiency and scalability.",
      "tldr_zh": "GCN（Graph Convolutional Networks）在图-based 学习任务中表现出色，但训练时面临特征张量内存爆炸和通信带宽阻塞，以及计算流程交替稀疏和密集矩阵操作的挑战。论文提出 MixGCN 框架，通过 mixture of parallelism 实现恒定的通信量和工作负载平衡，以解决内存问题；同时采用 mixture of accelerators，包括稀疏和密集加速器，以及细粒度管道，优化计算资源利用。实验结果表明，MixGCN 显著提高了 GCN 训练的效率和可扩展性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 12 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.01951v3",
      "published_date": "2025-01-03 18:54:46 UTC",
      "updated_date": "2025-02-24 22:02:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:08:17.133869"
    },
    {
      "arxiv_id": "2501.01950v4",
      "title": "MADGEN: Mass-Spec attends to De Novo Molecular generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yinkai Wang",
        "Xiaohui Chen",
        "Liping Liu",
        "Soha Hassoun"
      ],
      "abstract": "The annotation (assigning structural chemical identities) of MS/MS spectra\nremains a significant challenge due to the enormous molecular diversity in\nbiological samples and the limited scope of reference databases. Currently, the\nvast majority of spectral measurements remain in the \"dark chemical space\"\nwithout structural annotations. To improve annotation, we propose MADGEN\n(Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based method\nfor de novo molecular structure generation guided by mass spectrometry data.\nMADGEN operates in two stages: scaffold retrieval and spectra-conditioned\nmolecular generation starting with the scaffold. In the first stage, given an\nMS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employ\ncontrastive learning to align mass spectra with candidate molecular scaffolds.\nIn the second stage, starting from the retrieved scaffold, we employ the MS/MS\nspectrum to guide an attention-based generative model to generate the final\nmolecule. Our approach constrains the molecular generation search space,\nreducing its complexity and improving generation accuracy. We evaluate MADGEN\non three datasets (NIST23, CANOPUS, and MassSpecGym) and evaluate MADGEN's\nperformance with a predictive scaffold retriever and with an oracle retriever.\nWe demonstrate the effectiveness of using attention to integrate spectral\ninformation throughout the generation process to achieve strong results with\nthe oracle retriever.",
      "tldr_zh": "该研究提出MADGEN，一种基于支架的de novo分子结构生成方法，用于指导质谱数据（MS/MS谱）注释，旨在解决生物样本分子多样性和参考数据库有限的问题。MADGEN分为两个阶段：首先，通过对比学习（contrastive learning）将MS/MS谱转化为支架检索排名问题；其次，从检索到的支架出发，使用注意力-based生成模型整合谱信息生成最终分子，从而减少搜索空间复杂性和提高准确性。在NIST23、CANOPUS和MassSpecGym数据集上的实验表明，该方法在与oracle检索器结合时表现出色，证明了注意力机制在分子生成过程中的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.01950v4",
      "published_date": "2025-01-03 18:54:26 UTC",
      "updated_date": "2025-04-29 16:27:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:08:29.311964"
    },
    {
      "arxiv_id": "2501.01945v2",
      "title": "Cold-Start Recommendation towards the Era of Large Language Models (LLMs): A Comprehensive Survey and Roadmap",
      "title_zh": "翻译失败",
      "authors": [
        "Weizhi Zhang",
        "Yuanchen Bei",
        "Liangwei Yang",
        "Henry Peng Zou",
        "Peilin Zhou",
        "Aiwei Liu",
        "Yinghui Li",
        "Hao Chen",
        "Jianling Wang",
        "Yu Wang",
        "Feiran Huang",
        "Sheng Zhou",
        "Jiajun Bu",
        "Allen Lin",
        "James Caverlee",
        "Fakhri Karray",
        "Irwin King",
        "Philip S. Yu"
      ],
      "abstract": "Cold-start problem is one of the long-standing challenges in recommender\nsystems, focusing on accurately modeling new or interaction-limited users or\nitems to provide better recommendations. Due to the diversification of internet\nplatforms and the exponential growth of users and items, the importance of\ncold-start recommendation (CSR) is becoming increasingly evident. At the same\ntime, large language models (LLMs) have achieved tremendous success and possess\nstrong capabilities in modeling user and item information, providing new\npotential for cold-start recommendations. However, the research community on\nCSR still lacks a comprehensive review and reflection in this field. Based on\nthis, in this paper, we stand in the context of the era of large language\nmodels and provide a comprehensive review and discussion on the roadmap,\nrelated literature, and future directions of CSR. Specifically, we have\nconducted an exploration of the development path of how existing CSR utilizes\ninformation, from content features, graph relations, and domain information, to\nthe world knowledge possessed by large language models, aiming to provide new\ninsights for both the research and industrial communities on CSR. Related\nresources of cold-start recommendations are collected and continuously updated\nfor the community in\nhttps://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation.",
      "tldr_zh": "这篇论文对Cold-Start Recommendation (CSR)进行了全面调查和路线图讨论，聚焦于在大语言模型(LLMs)时代解决新用户或交互有限项目建模的长期挑战。论文审视了CSR的发展路径，从利用内容特征、图关系和领域信息，扩展到LLMs的世界知识，从而为推荐系统注入新潜力。最终，作者提供了新见解，并通过GitHub资源链接持续更新相关材料，以指导研究和工业社区的未来方向。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01945v2",
      "published_date": "2025-01-03 18:51:18 UTC",
      "updated_date": "2025-01-16 18:53:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:08:40.559982"
    },
    {
      "arxiv_id": "2501.02045v1",
      "title": "METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring",
      "title_zh": "METAGENE-1：用于大流行监测的宏基因组基础模型",
      "authors": [
        "Ollie Liu",
        "Sami Jaghouar",
        "Johannes Hagemann",
        "Shangshang Wang",
        "Jason Wiemels",
        "Jeff Kaufman",
        "Willie Neiswanger"
      ],
      "abstract": "We pretrain METAGENE-1, a 7-billion-parameter autoregressive transformer\nmodel, which we refer to as a metagenomic foundation model, on a novel corpus\nof diverse metagenomic DNA and RNA sequences comprising over 1.5 trillion base\npairs. This dataset is sourced from a large collection of human wastewater\nsamples, processed and sequenced using deep metagenomic (next-generation)\nsequencing methods. Unlike genomic models that focus on individual genomes or\ncurated sets of specific species, the aim of METAGENE-1 is to capture the full\ndistribution of genomic information present within this wastewater, to aid in\ntasks relevant to pandemic monitoring and pathogen detection. We carry out\nbyte-pair encoding (BPE) tokenization on our dataset, tailored for metagenomic\nsequences, and then pretrain our model. In this paper, we first detail the\npretraining dataset, tokenization strategy, and model architecture,\nhighlighting the considerations and design choices that enable the effective\nmodeling of metagenomic data. We then show results of pretraining this model on\nour metagenomic dataset, providing details about our losses, system metrics,\nand training stability over the course of pretraining. Finally, we demonstrate\nthe performance of METAGENE-1, which achieves state-of-the-art results on a set\nof genomic benchmarks and new evaluations focused on human-pathogen detection\nand genomic sequence embedding, showcasing its potential for public health\napplications in pandemic monitoring, biosurveillance, and early detection of\nemerging health threats.",
      "tldr_zh": "本研究开发了METAGENE-1，一种7亿参数的自回归Transformer模型，作为元基因组基础模型，用于流行病监测和病原体检测。\n模型在超过1.5万亿碱基对的废水样本元基因组序列数据集上进行预训练，采用针对性BPE tokenization策略和专为元基因组数据设计的架构，确保捕捉废水中的完整基因组分布。\n实验结果显示，METAGENE-1在基因组基准测试以及人类-病原体检测和序列嵌入任务上达到了最先进性能，具有显著潜力应用于公共卫生监测、生物监测和新兴健康威胁的早期检测。",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "q-bio.GN",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02045v1",
      "published_date": "2025-01-03 18:44:43 UTC",
      "updated_date": "2025-01-03 18:44:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:08:53.325166"
    },
    {
      "arxiv_id": "2501.02044v1",
      "title": "Advancing Pancreatic Cancer Prediction with a Next Visit Token Prediction Head on top of Med-BERT",
      "title_zh": "翻译失败",
      "authors": [
        "Jianping He",
        "Laila Rasmy",
        "Degui Zhi",
        "Cui Tao"
      ],
      "abstract": "Background: Recently, numerous foundation models pretrained on extensive data\nhave demonstrated efficacy in disease prediction using Electronic Health\nRecords (EHRs). However, there remains some unanswered questions on how to best\nutilize such models especially with very small fine-tuning cohorts. Methods: We\nutilized Med-BERT, an EHR-specific foundation model, and reformulated the\ndisease binary prediction task into a token prediction task and a next visit\nmask token prediction task to align with Med-BERT's pretraining task format in\norder to improve the accuracy of pancreatic cancer (PaCa) prediction in both\nfew-shot and fully supervised settings. Results: The reformulation of the task\ninto a token prediction task, referred to as Med-BERT-Sum, demonstrates\nslightly superior performance in both few-shot scenarios and larger data\nsamples. Furthermore, reformulating the prediction task as a Next Visit Mask\nToken Prediction task (Med-BERT-Mask) significantly outperforms the\nconventional Binary Classification (BC) prediction task (Med-BERT-BC) by 3% to\n7% in few-shot scenarios with data sizes ranging from 10 to 500 samples. These\nfindings highlight that aligning the downstream task with Med-BERT's\npretraining objectives substantially enhances the model's predictive\ncapabilities, thereby improving its effectiveness in predicting both rare and\ncommon diseases. Conclusion: Reformatting disease prediction tasks to align\nwith the pretraining of foundation models enhances prediction accuracy, leading\nto earlier detection and timely intervention. This approach improves treatment\neffectiveness, survival rates, and overall patient outcomes for PaCa and\npotentially other cancers.",
      "tldr_zh": "本研究利用 Med-BERT 模型，通过将胰腺癌（PaCa）预测任务从传统二元分类（BC）改造成 token prediction 任务（Med-BERT-Sum）和 Next Visit Mask Token Prediction 任务（Med-BERT-Mask），以更好地与模型的预训练目标对齐，从而提升预测准确性。实验结果显示，在少样本场景（样本量10-500）中，Med-BERT-Mask 比标准二元分类方法提高了3%至7%的性能，并在少样本和大数据样本中均表现出轻微优势。这些改进有助于更早检测胰腺癌，实现及时干预，提高治疗效果、生存率和患者预后，并可能扩展到其他癌症的预测。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02044v1",
      "published_date": "2025-01-03 18:32:05 UTC",
      "updated_date": "2025-01-03 18:32:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:09:04.580603"
    },
    {
      "arxiv_id": "2501.01933v1",
      "title": "Abstractive Text Summarization for Contemporary Sanskrit Prose: Issues and Challenges",
      "title_zh": "当代梵语散文的抽象式文本摘要：问题和挑战",
      "authors": [
        "Shagun Sinha"
      ],
      "abstract": "This thesis presents Abstractive Text Summarization models for contemporary\nSanskrit prose. The first chapter, titled Introduction, presents the motivation\nbehind this work, the research questions, and the conceptual framework.\nSanskrit is a low-resource inflectional language. The key research question\nthat this thesis investigates is what the challenges in developing an\nabstractive TS for Sanskrit. To answer the key research questions,\nsub-questions based on four different themes have been posed in this work. The\nsecond chapter, Literature Review, surveys the previous works done. The third\nchapter, data preparation, answers the remaining three questions from the third\ntheme. It reports the data collection and preprocessing challenges for both\nlanguage model and summarization model trainings. The fourth chapter reports\nthe training and inference of models and the results obtained therein. This\nresearch has initiated a pipeline for Sanskrit abstractive text summarization\nand has reported the challenges faced at every stage of the development. The\nresearch questions based on every theme have been answered to answer the key\nresearch question.",
      "tldr_zh": "本论文探讨了针对当代梵语散文开发抽象文本摘要（Abstractive Text Summarization）模型的挑战，梵语作为一种低资源屈折语言面临数据稀缺和处理难题。论文通过引言、文献综述、数据准备（包括收集和预处理）、模型训练和推理等章节来回答核心研究问题。最终，该研究建立了梵语摘要的初步管道，并详细报告了每个阶段的障碍，为未来优化提供了参考。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "PhD Thesis",
      "pdf_url": "http://arxiv.org/pdf/2501.01933v1",
      "published_date": "2025-01-03 18:12:13 UTC",
      "updated_date": "2025-01-03 18:12:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:09:16.519090"
    },
    {
      "arxiv_id": "2501.03183v1",
      "title": "Classifier-Guided Captioning Across Modalities",
      "title_zh": "分类器引导的跨模态标题生成",
      "authors": [
        "Ariel Shaulov",
        "Tal Shaharabany",
        "Eitan Shaar",
        "Gal Chechik",
        "Lior Wolf"
      ],
      "abstract": "Most current captioning systems use language models trained on data from\nspecific settings, such as image-based captioning via Amazon Mechanical Turk,\nlimiting their ability to generalize to other modality distributions and\ncontexts. This limitation hinders performance in tasks like audio or video\ncaptioning, where different semantic cues are needed. Addressing this challenge\nis crucial for creating more adaptable and versatile captioning frameworks\napplicable across diverse real-world contexts. In this work, we introduce a\nmethod to adapt captioning networks to the semantics of alternative settings,\nsuch as capturing audibility in audio captioning, where it is crucial to\ndescribe sounds and their sources. Our framework consists of two main\ncomponents: (i) a frozen captioning system incorporating a language model (LM),\nand (ii) a text classifier that guides the captioning system. The classifier is\ntrained on a dataset automatically generated by GPT-4, using tailored prompts\nspecifically designed to enhance key aspects of the generated captions.\nImportantly, the framework operates solely during inference, eliminating the\nneed for further training of the underlying captioning model. We evaluate the\nframework on various models and modalities, with a focus on audio captioning,\nand report promising results. Notably, when combined with an existing zero-shot\naudio captioning system, our framework improves its quality and sets\nstate-of-the-art performance in zero-shot audio captioning.",
      "tldr_zh": "该研究针对当前标题系统在不同模态（如图像、音频或视频）上的泛化能力不足问题，提出了一种Classifier-Guided Captioning框架，以适应各种语义情境，例如音频标题中捕捉声音和来源。该框架包括一个冻结的captioning系统（incorporating a language model, LM）和一个文本分类器，后者通过GPT-4生成的自动数据集和定制提示进行训练，仅在推理阶段指导标题生成，无需进一步训练模型。在音频标题等任务上，该框架显著提升了现有零-shot audio captioning系统的性能，实现了state-of-the-art水平。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03183v1",
      "published_date": "2025-01-03 18:09:26 UTC",
      "updated_date": "2025-01-03 18:09:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:09:29.539945"
    },
    {
      "arxiv_id": "2501.01926v2",
      "title": "Mitigating Hallucination for Large Vision Language Model by Inter-Modality Correlation Calibration Decoding",
      "title_zh": "通过跨模态相关性校准解码缓解大型视觉语言模型的幻觉",
      "authors": [
        "Jiaming Li",
        "Jiacheng Zhang",
        "Zequn Jie",
        "Lin Ma",
        "Guanbin Li"
      ],
      "abstract": "Large vision-language models (LVLMs) have shown remarkable capabilities in\nvisual-language understanding for downstream multi-modal tasks. Despite their\nsuccess, LVLMs still suffer from generating hallucinations in complex\ngeneration tasks, leading to inconsistencies between visual inputs and\ngenerated content. To address this issue, some approaches have introduced\ninference-time interventions, such as contrastive decoding and attention\nrectification, to reduce overreliance on language priors. However, these\napproaches overlook hallucinations stemming from spurious inter-modality\ncorrelations. In this paper, we propose an Inter-Modality Correlation\nCalibration Decoding (IMCCD) method to mitigate hallucinations in LVLMs in a\ntraining-free manner. In this method, we design a Cross-Modal Value-Enhanced\nDecoding(CMVED) module to alleviate hallucination by a novel contrastive\ndecoding mechanism. During the estimation of distorted distribution, CMVED\nmasks the value vectors associated with significant cross-modal attention\nweights, which address both uni-modality overreliance and misleading\ninter-modality correlations. Additionally, a Content-Driven Attention\nRefinement(CDAR) module refines cross-modal attention weights, guiding LVLMs to\nfocus on important visual content. Experimental results on diverse\nhallucination benchmarks validate the superiority of our method over existing\nstate-of-the-art techniques in reducing hallucinations in LVLM text generation.\nOur code will be available at https://github.com/lijm48/IMCCD.",
      "tldr_zh": "大型视觉语言模型（LVLMs）在多模态任务中表现出色，但容易产生幻觉，导致视觉输入与生成内容不一致。论文提出了一种训练-free 方法Inter-Modality Correlation Calibration Decoding (IMCCD)，通过Cross-Modal Value-Enhanced Decoding (CMVED)模块利用新型对比解码机制屏蔽误导性跨模态注意力相关值向量，以缓解单模态过度依赖和虚假相关性；同时，Content-Driven Attention Refinement (CDAR)模块精炼注意力权重，引导模型关注关键视觉内容。实验结果显示，IMCCD在各种幻觉基准上优于现有技术，显著减少了LVLM的文本生成幻觉。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01926v2",
      "published_date": "2025-01-03 17:56:28 UTC",
      "updated_date": "2025-03-11 18:21:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:09:42.078211"
    },
    {
      "arxiv_id": "2501.01913v1",
      "title": "Mingling with the Good to Backdoor Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Nuno Neves"
      ],
      "abstract": "Federated learning (FL) is a decentralized machine learning technique that\nallows multiple entities to jointly train a model while preserving dataset\nprivacy. However, its distributed nature has raised various security concerns,\nwhich have been addressed by increasingly sophisticated defenses. These\nprotections utilize a range of data sources and metrics to, for example, filter\nout malicious model updates, ensuring that the impact of attacks is minimized\nor eliminated.\n  This paper explores the feasibility of designing a generic attack method\ncapable of installing backdoors in FL while evading a diverse array of\ndefenses. Specifically, we focus on an attacker strategy called MIGO, which\naims to produce model updates that subtly blend with legitimate ones. The\nresulting effect is a gradual integration of a backdoor into the global model,\noften ensuring its persistence long after the attack concludes, while\ngenerating enough ambiguity to hinder the effectiveness of defenses.\n  MIGO was employed to implant three types of backdoors across five datasets\nand different model architectures. The results demonstrate the significant\nthreat posed by these backdoors, as MIGO consistently achieved exceptionally\nhigh backdoor accuracy (exceeding 90%) while maintaining the utility of the\nmain task. Moreover, MIGO exhibited strong evasion capabilities against ten\ndefenses, including several state-of-the-art methods. When compared to four\nother attack strategies, MIGO consistently outperformed them across most\nconfigurations. Notably, even in extreme scenarios where the attacker controls\njust 0.1% of the clients, the results indicate that successful backdoor\ninsertion is possible if the attacker can persist for a sufficient number of\nrounds.",
      "tldr_zh": "本论文探讨了在联邦学习(FL)中设计一种名为MIGO的泛化后门攻击方法，该方法旨在通过生成与合法模型更新相似的更新，逐步整合后门到全局模型中，确保后门的持久性和对防御机制的规避。MIGO在五种数据集和不同模型架构上进行了测试，实现了超过90%的后门准确率，同时维持了主任务的效用，并成功对抗了十种先进防御策略。实验结果表明，即使攻击者仅控制0.1%的客户端，通过足够轮次的持久攻击，仍然可能成功植入后门，突显了FL系统的潜在安全风险。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC",
        "D.4.6; I.2"
      ],
      "primary_category": "cs.CR",
      "comment": "13 pages, 9 figures, under submission",
      "pdf_url": "http://arxiv.org/pdf/2501.01913v1",
      "published_date": "2025-01-03 17:30:59 UTC",
      "updated_date": "2025-01-03 17:30:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:09:53.754913"
    },
    {
      "arxiv_id": "2501.01904v2",
      "title": "Virgo: A Preliminary Exploration on Reproducing o1-like MLLM",
      "title_zh": "翻译失败",
      "authors": [
        "Yifan Du",
        "Zikang Liu",
        "Yifan Li",
        "Wayne Xin Zhao",
        "Yuqi Huo",
        "Bingning Wang",
        "Weipeng Chen",
        "Zheng Liu",
        "Zhongyuan Wang",
        "Ji-Rong Wen"
      ],
      "abstract": "Recently, slow-thinking reasoning systems, built upon large language models\n(LLMs), have garnered widespread attention by scaling the thinking time during\ninference. There is also growing interest in adapting this capability to\nmultimodal large language models (MLLMs). Given that MLLMs handle more complex\ndata semantics across different modalities, it is intuitively more challenging\nto implement multimodal slow-thinking systems.\n  To address this issue, in this paper, we explore a straightforward approach\nby fine-tuning a capable MLLM with a small amount of textual long-form thought\ndata, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning\nwith long thought). We find that these long-form reasoning processes, expressed\nin natural language, can be effectively transferred to MLLMs. Moreover, it\nseems that such textual reasoning data can be even more effective than visual\nreasoning data in eliciting the slow-thinking capacities of MLLMs. While this\nwork is preliminary, it demonstrates that slow-thinking capacities are\nfundamentally associated with the language model component, which can be\ntransferred across modalities or domains. This finding can be leveraged to\nguide the development of more powerful slow-thinking reasoning systems. We\nrelease our resources at https://github.com/RUCAIBox/Virgo.",
      "tldr_zh": "该论文探索了在多模态大型语言模型(MLLMs)中实现类似 o1 的慢思考推理系统Virgo，以应对跨模态数据语义复杂性的挑战。作者采用简单方法，通过微调一个强大的MLLM，使用少量文本长形式思考数据，成功构建了Virgo系统，发现这种文本数据比视觉推理数据更有效地激发MLLMs的慢思考能力。该初步研究表明，慢思考能力主要源于语言模型组件，可跨模态或领域转移，为开发更先进的推理系统提供了指导方向。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Technical Report on Slow Thinking with LLMs: Visual Reasoning",
      "pdf_url": "http://arxiv.org/pdf/2501.01904v2",
      "published_date": "2025-01-03 17:14:16 UTC",
      "updated_date": "2025-02-05 09:17:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:10:04.997923"
    },
    {
      "arxiv_id": "2501.01892v2",
      "title": "QuArch: A Question-Answering Dataset for AI Agents in Computer Architecture",
      "title_zh": "翻译失败",
      "authors": [
        "Shvetank Prakash",
        "Andrew Cheng",
        "Jason Yik",
        "Arya Tschand",
        "Radhika Ghosal",
        "Ikechukwu Uchendu",
        "Jessica Quaye",
        "Jeffrey Ma",
        "Shreyas Grampurohit",
        "Sofia Giannuzzi",
        "Arnav Balyan",
        "Fin Amin",
        "Aadya Pipersenia",
        "Yash Choudhary",
        "Ankita Nayak",
        "Amir Yazdanbakhsh",
        "Vijay Janapa Reddi"
      ],
      "abstract": "We introduce QuArch, a dataset of 1500 human-validated question-answer pairs\ndesigned to evaluate and enhance language models' understanding of computer\narchitecture. The dataset covers areas including processor design, memory\nsystems, and performance optimization. Our analysis highlights a significant\nperformance gap: the best closed-source model achieves 84% accuracy, while the\ntop small open-source model reaches 72%. We observe notable struggles in memory\nsystems, interconnection networks, and benchmarking. Fine-tuning with QuArch\nimproves small model accuracy by up to 8%, establishing a foundation for\nadvancing AI-driven computer architecture research. The dataset and leaderboard\nare at https://harvard-edge.github.io/QuArch/.",
      "tldr_zh": "本文引入 QuArch 数据集，该数据集包含 1500 个人类验证的问题-答案对，旨在评估和提升语言模型在计算机 architecture 领域的理解，覆盖处理器设计、内存系统和性能优化等领域。分析结果显示，最佳闭源模型准确率达 84%，而顶级小型开源模型仅为 72%，并在内存系统、interconnection networks 和 benchmarking 方面存在显著挑战。通过对 QuArch 的 fine-tuning，小型模型准确率可提升多达 8%，为 AI 驱动的计算机 architecture 研究奠定基础。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01892v2",
      "published_date": "2025-01-03 16:55:53 UTC",
      "updated_date": "2025-01-06 17:48:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:10:17.775987"
    },
    {
      "arxiv_id": "2501.01886v1",
      "title": "Evaluating Scenario-based Decision-making for Interactive Autonomous Driving Using Rational Criteria: A Survey",
      "title_zh": "使用理性标准评估交互式自动驾驶的基于场景决策：一个综述",
      "authors": [
        "Zhen Tian",
        "Zhihao Lin",
        "Dezong Zhao",
        "Wenjing Zhao",
        "David Flynn",
        "Shuja Ansari",
        "Chongfeng Wei"
      ],
      "abstract": "Autonomous vehicles (AVs) can significantly promote the advances in road\ntransport mobility in terms of safety, reliability, and decarbonization.\nHowever, ensuring safety and efficiency in interactive during within dynamic\nand diverse environments is still a primary barrier to large-scale AV adoption.\nIn recent years, deep reinforcement learning (DRL) has emerged as an advanced\nAI-based approach, enabling AVs to learn decision-making strategies adaptively\nfrom data and interactions. DRL strategies are better suited than traditional\nrule-based methods for handling complex, dynamic, and unpredictable driving\nenvironments due to their adaptivity. However, varying driving scenarios\npresent distinct challenges, such as avoiding obstacles on highways and\nreaching specific exits at intersections, requiring different scenario-specific\ndecision-making algorithms. Many DRL algorithms have been proposed in\ninteractive decision-making. However, a rationale review of these DRL\nalgorithms across various scenarios is lacking. Therefore, a comprehensive\nevaluation is essential to assess these algorithms from multiple perspectives,\nincluding those of vehicle users and vehicle manufacturers. This survey reviews\nthe application of DRL algorithms in autonomous driving across typical\nscenarios, summarizing road features and recent advancements. The scenarios\ninclude highways, on-ramp merging, roundabouts, and unsignalized intersections.\nFurthermore, DRL-based algorithms are evaluated based on five rationale\ncriteria: driving safety, driving efficiency, training efficiency,\nunselfishness, and interpretability (DDTUI). Each criterion of DDTUI is\nspecifically analyzed in relation to the reviewed algorithms. Finally, the\nchallenges for future DRL-based decision-making algorithms are summarized.",
      "tldr_zh": "本调查评估了深度强化学习 (DRL) 算法在交互式自动驾驶 (AVs) 决策中的应用，针对动态环境中的安全和效率挑战，涵盖典型场景如高速公路、坡道合并、环岛和无信号交叉口。研究总结了 DRL 算法的最新进展，并基于五个理性标准（DDTUI，包括驾驶安全、驾驶效率、训练效率、无私性和可解释性）对这些算法进行多角度评估。结果显示，DRL 优于传统规则方法，但仍面临场景适应性和算法解释性的挑战，为未来 DRL 决策算法的优化提供了指导。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01886v1",
      "published_date": "2025-01-03 16:37:52 UTC",
      "updated_date": "2025-01-03 16:37:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:10:30.175018"
    },
    {
      "arxiv_id": "2502.15695v2",
      "title": "Contrastive Learning Augmented Social Recommendations",
      "title_zh": "对比",
      "authors": [
        "Lin Wang",
        "Weisong Wang",
        "Xuanji Xiao",
        "Qing Li"
      ],
      "abstract": "Recommender systems play a pivotal role in modern content platforms, yet\ntraditional behavior-based models often face challenges in addressing cold\nusers with sparse interaction data. Engaging these users, however, remains\ncritical for sustaining platform growth. To tackle this issue, we propose\nleveraging reconstructed social graph to complement interest representations\nderived from behavioral data. Despite the widespread availability of social\ngraphs on content platforms, their utility is hindered by social-relation noise\nand inconsistencies between social and behavioral interests. To mitigate noise\npropagation in graph data and extract reliable social interests, we introduce a\ndual-view denoising framework. This approach first applies low-rank singular\nvalue decomposition (SVD) to the user-item interaction matrix, generating\ndenoised user embeddings for reconstructing the social graph. It then employs\ncontrastive learning to align the original and reconstructed social graphs. To\naddress the discrepancy between social and behavioral interests, we utilize a\nmutual distillation mechanism that decomposes interests into four\nsubcategories: aligned social/behavioral interests and\nsocial/behavioral-specific interests, enabling effective integration of the\ntwo. Empirical results demonstrate the efficacy of our method, particularly in\nimproving recommendations for cold users, by combining social and behavioral\ndata. The implementation of our approach is publicly available at\nhttps://github.com/WANGLin0126/CLSRec.",
      "tldr_zh": "该论文提出了一种增强社交推荐的框架，针对传统行为-based推荐系统在处理冷启动用户（cold users with sparse interaction data）时的挑战，通过重构社会图（reconstructed social graph）来补充行为数据中的兴趣表示。为缓解社会图中的社会关系噪音（social-relation noise）和兴趣不一致问题，该框架引入双视图去噪框架（dual-view denoising framework），利用低秩奇异值分解（low-rank SVD）生成去噪用户嵌入，并通过对比学习（contrastive learning）对齐原始和重构社会图。进一步，采用相互蒸馏机制（mutual distillation mechanism）将兴趣分解为对齐的社交/行为兴趣以及特定兴趣子类别，实现两者的有效整合。实验结果表明，该方法显著提高了冷启动用户的推荐准确性，并将实现代码开源在GitHub。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15695v2",
      "published_date": "2025-01-03 16:29:51 UTC",
      "updated_date": "2025-02-25 04:02:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:10:41.591937"
    },
    {
      "arxiv_id": "2501.02041v1",
      "title": "MRG: A Multi-Robot Manufacturing Digital Scene Generation Method Using Multi-Instance Point Cloud Registration",
      "title_zh": "MRG：一种使用多实例点云配准的多机器人制造数字场景生成方法",
      "authors": [
        "Songjie Han",
        "Yinhua Liu",
        "Yanzheng Li",
        "Hua Chen",
        "Dongmei Yang"
      ],
      "abstract": "A high-fidelity digital simulation environment is crucial for accurately\nreplicating physical operational processes. However, inconsistencies between\nsimulation and physical environments result in low confidence in simulation\noutcomes, limiting their effectiveness in guiding real-world production. Unlike\nthe traditional step-by-step point cloud \"segmentation-registration\" generation\nmethod, this paper introduces, for the first time, a novel Multi-Robot\nManufacturing Digital Scene Generation (MRG) method that leverages\nmulti-instance point cloud registration, specifically within manufacturing\nscenes. Tailored to the characteristics of industrial robots and manufacturing\nsettings, an instance-focused transformer module is developed to delineate\ninstance boundaries and capture correlations between local regions.\nAdditionally, a hypothesis generation module is proposed to extract target\ninstances while preserving key features. Finally, an efficient screening and\noptimization algorithm is designed to refine the final registration results.\nExperimental evaluations on the Scan2CAD and Welding-Station datasets\ndemonstrate that: (1) the proposed method outperforms existing multi-instance\npoint cloud registration techniques; (2) compared to state-of-the-art methods,\nthe Scan2CAD dataset achieves improvements in MR and MP by 12.15% and 17.79%,\nrespectively; and (3) on the Welding-Station dataset, MR and MP are enhanced by\n16.95% and 24.15%, respectively. This work marks the first application of\nmulti-instance point cloud registration in manufacturing scenes, significantly\nadvancing the precision and reliability of digital simulation environments for\nindustrial applications.",
      "tldr_zh": "这篇论文提出了一种名为 MRG 的多机器人制造数字场景生成方法，首次利用 multi-instance point cloud registration 技术来解决传统点云“分割-注册”方法在模拟环境中的不一致问题。该方法针对工业机器人和制造场景的特点，开发了 instance-focused transformer module 来划分实例边界并捕获局部相关性，结合 hypothesis generation module 提取目标实例，以及一个高效的筛选优化算法来精炼注册结果。在 Scan2CAD 和 Welding-Station 数据集上的实验表明，MRG 方法比现有技术提高了 MR 和 MP 指标（如 Scan2CAD 上提升 12.15% 和 17.79%），显著提升了数字模拟环境的精度和可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02041v1",
      "published_date": "2025-01-03 16:23:13 UTC",
      "updated_date": "2025-01-03 16:23:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:10:54.435233"
    },
    {
      "arxiv_id": "2501.03261v1",
      "title": "Navigation Variable-based Multi-objective Particle Swarm Optimization for UAV Path Planning with Kinematic Constraints",
      "title_zh": "基于导航变量的多目标粒子群优化用于带运动学约束的无人机路径规划",
      "authors": [
        "Thi Thuy Ngan Duong",
        "Duy-Nam Bui",
        "Manh Duong Phung"
      ],
      "abstract": "Path planning is essential for unmanned aerial vehicles (UAVs) as it\ndetermines the path that the UAV needs to follow to complete a task. This work\naddresses this problem by introducing a new algorithm called navigation\nvariable-based multi-objective particle swarm optimization (NMOPSO). It first\nmodels path planning as an optimization problem via the definition of a set of\nobjective functions that include optimality and safety requirements for UAV\noperation. The NMOPSO is then used to minimize those functions through Pareto\noptimal solutions. The algorithm features a new path representation based on\nnavigation variables to include kinematic constraints and exploit the\nmaneuverable characteristics of the UAV. It also includes an adaptive mutation\nmechanism to enhance the diversity of the swarm for better solutions.\nComparisons with various algorithms have been carried out to benchmark the\nproposed approach. The results indicate that the NMOPSO performs better than\nnot only other particle swarm optimization variants but also other\nstate-of-the-art multi-objective and metaheuristic optimization algorithms.\nExperiments have also been conducted with real UAVs to confirm the validity of\nthe approach for practical flights. The source code of the algorithm is\navailable at https://github.com/ngandng/NMOPSO.",
      "tldr_zh": "本论文提出了一种名为NMOPSO的新算法，用于处理无人驾驶飞机(UAV)路径规划问题，该算法基于多目标粒子群优化(Particle Swarm Optimization)框架，并通过导航变量表示路径以满足运动学约束，同时引入自适应突变机制来提升群体的多样性和优化性能。NMOPSO将路径规划建模为一个多目标优化问题，包括最优性和安全性要求，并通过Pareto最优解来最小化这些目标函数。与其他算法相比，实验结果显示NMOPSO在性能上优于其他粒子群优化变体和元启发式算法，并在实际UAV飞行测试中验证了其有效性。源代码已公开，可从GitHub获取。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03261v1",
      "published_date": "2025-01-03 16:07:37 UTC",
      "updated_date": "2025-01-03 16:07:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:11:06.144937"
    },
    {
      "arxiv_id": "2501.01876v1",
      "title": "Accuracy Can Lie: On the Impact of Surrogate Model in Configuration Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Pengzhou Chen",
        "Jingzhi Gong",
        "Tao Chen"
      ],
      "abstract": "To ease the expensive measurements during configuration tuning, it is natural\nto build a surrogate model as the replacement of the system, and thereby the\nconfiguration performance can be cheaply evaluated. Yet, a stereotype therein\nis that the higher the model accuracy, the better the tuning result would be.\nThis \"accuracy is all\" belief drives our research community to build more and\nmore accurate models and criticize a tuner for the inaccuracy of the model\nused. However, this practice raises some previously unaddressed questions,\ne.g., Do those somewhat small accuracy improvements reported in existing work\nreally matter much to the tuners? What role does model accuracy play in the\nimpact of tuning quality? To answer those related questions, we conduct one of\nthe largest-scale empirical studies to date-running over the period of 13\nmonths 24*7-that covers 10 models, 17 tuners, and 29 systems from the existing\nworks while under four different commonly used metrics, leading to 13,612 cases\nof investigation. Surprisingly, our key findings reveal that the accuracy can\nlie: there are a considerable number of cases where higher accuracy actually\nleads to no improvement in the tuning outcomes (up to 58% cases under certain\nsetting), or even worse, it can degrade the tuning quality (up to 24% cases\nunder certain setting). We also discover that the chosen models in most\nproposed tuners are sub-optimal and that the required % of accuracy change to\nsignificantly improve tuning quality varies according to the range of model\naccuracy. Deriving from the fitness landscape analysis, we provide in-depth\ndiscussions of the rationale behind, offering several lessons learned as well\nas insights for future opportunities. Most importantly, this work poses a clear\nmessage to the community: we should take one step back from the natural\n\"accuracy is all\" belief for model-based configuration tuning.",
      "tldr_zh": "本研究质疑了在配置调优(configuration tuning)中使用 surrogate model 时“准确性越高越好”的传统信念，通过大规模实证研究探讨模型准确性对调优质量的影响。研究者进行了13个月的实验，涵盖10个模型、17个调优器、29个系统和4个常用指标，共分析13,612个案例，结果显示在多达58%的场景中，更高的模型准确性并未改善调优结果，甚至在24%的场景中会降低调优质量。论文进一步分析了模型选择的次优问题，并通过适应性景观(fitness landscape)分析提供解释，强调社区应重新审视对准确性的过度追求，以优化未来的模型-based配置调优策略。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "This paper has been accepted by TSE",
      "pdf_url": "http://arxiv.org/pdf/2501.01876v1",
      "published_date": "2025-01-03 15:57:20 UTC",
      "updated_date": "2025-01-03 15:57:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:11:17.446834"
    },
    {
      "arxiv_id": "2501.02040v2",
      "title": "A Separable Self-attention Inspired by the State Space Model for Computer Vision",
      "title_zh": "翻译失败",
      "authors": [
        "Juntao Zhang",
        "Shaogeng Liu",
        "Kun Bian",
        "You Zhou",
        "Pei Zhang",
        "Jianning Liu",
        "Jun Zhou",
        "Bingyan Liu"
      ],
      "abstract": "Mamba is an efficient State Space Model (SSM) with linear computational\ncomplexity. Although SSMs are not suitable for handling non-causal data, Vision\nMamba (ViM) methods still demonstrate good performance in tasks such as image\nclassification and object detection. Recent studies have shown that there is a\nrich theoretical connection between state space models and attention variants.\nWe propose a novel separable self attention method, for the first time\nintroducing some excellent design concepts of Mamba into separable\nself-attention. To ensure a fair comparison with ViMs, we introduce VMINet, a\nsimple yet powerful prototype architecture, constructed solely by stacking our\nnovel attention modules with the most basic down-sampling layers. Notably,\nVMINet differs significantly from the conventional Transformer architecture.\nOur experiments demonstrate that VMINet has achieved competitive results on\nimage classification and high-resolution dense prediction tasks.Code is\navailable at: https://github.com/yws-wxs/VMINet.",
      "tldr_zh": "本论文提出了一种新型可分离自注意力(separable self-attention)方法，受 State Space Model (SSM) 启发，特别是借鉴 Mamba 的高效设计，以处理计算机视觉任务。\n该方法首次将 Mamba 的概念融入自注意力中，并构建了 VMINet 架构，该架构仅通过堆叠新注意力模块和基本下采样层而成，与传统 Transformer 显著不同。\n实验结果表明，VMINet 在图像分类和高分辨率密集预测任务上取得了与 Vision Mamba (ViM) 方法竞争性的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02040v2",
      "published_date": "2025-01-03 15:23:36 UTC",
      "updated_date": "2025-05-20 01:01:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:11:30.241075"
    },
    {
      "arxiv_id": "2501.01850v1",
      "title": "LCFed: An Efficient Clustered Federated Learning Framework for Heterogeneous Data",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxin Zhang",
        "Haoyu Chen",
        "Zheng Lin",
        "Zhe Chen",
        "Jin Zhao"
      ],
      "abstract": "Clustered federated learning (CFL) addresses the performance challenges posed\nby data heterogeneity in federated learning (FL) by organizing edge devices\nwith similar data distributions into clusters, enabling collaborative model\ntraining tailored to each group. However, existing CFL approaches strictly\nlimit knowledge sharing to within clusters, lacking the integration of global\nknowledge with intra-cluster training, which leads to suboptimal performance.\nMoreover, traditional clustering methods incur significant computational\noverhead, especially as the number of edge devices increases. In this paper, we\npropose LCFed, an efficient CFL framework to combat these challenges. By\nleveraging model partitioning and adopting distinct aggregation strategies for\neach sub-model, LCFed effectively incorporates global knowledge into\nintra-cluster co-training, achieving optimal training performance.\nAdditionally, LCFed customizes a computationally efficient model similarity\nmeasurement method based on low-rank models, enabling real-time cluster updates\nwith minimal computational overhead. Extensive experiments show that LCFed\noutperforms state-of-the-art benchmarks in both test accuracy and clustering\ncomputational efficiency.",
      "tldr_zh": "该论文提出 LCFed，一种高效的 Clustered Federated Learning (CFL) 框架，用于处理 Federated Learning (FL) 中数据异质性带来的性能挑战。LCFed 通过模型分区和针对每个子模型的独特聚合策略，将全局知识整合到集群内训练中，同时采用基于低秩模型的相似性测量方法，实现实时集群更新并减少计算开销。与现有方法相比，该框架显著提升了训练性能和聚类效率。实验结果显示，LCFed 在测试准确性和计算效率上优于最先进基准。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.01850v1",
      "published_date": "2025-01-03 14:59:48 UTC",
      "updated_date": "2025-01-03 14:59:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:11:40.586751"
    },
    {
      "arxiv_id": "2501.01849v1",
      "title": "Multi-Agent Conversational Online Learning for Adaptive LLM Response Identification",
      "title_zh": "多智能体对话式在线学习用于自适应 LLM 响应识别",
      "authors": [
        "Xiangxiang Dai",
        "Yuejin Xie",
        "Maoli Liu",
        "Xuchuang Wang",
        "Zhuohua Li",
        "Huanyu Wang",
        "John C. S. Lui"
      ],
      "abstract": "The remarkable generative capability of large language models (LLMs) has\nsparked a growing interest in automatically generating responses for different\napplications. Given the dynamic nature of user preferences and the uncertainty\nof LLM response performance, it is crucial to design efficient online learning\nalgorithms to identify optimal LLM responses (i.e., high-quality responses that\nalso meet user preferences). Most existing online algorithms adopt a\ncentralized approach and fail to leverage explicit user preferences for more\nefficient and personalized LLM response identification. In contrast, this paper\nintroduces \\textit{MACO} (\\underline{M}ulti-\\underline{A}gent\n\\underline{C}onversational \\underline{O}nline Learning for Adaptive LLM\nResponse Identification): 1) The online LLM response identification process is\naccelerated by multiple local agents (such as smartphones), while enhancing\ndata privacy; 2) A novel conversational mechanism is proposed to adaptively\nconduct conversations for soliciting user preferences (e.g., a preference for a\nhumorous tone over a serious one in generated responses), so to minimize\nuncertainty in preference estimation. Our theoretical analysis demonstrates\nthat \\cadi\\ is near-optimal regarding cumulative regret. Additionally, \\cadi\\\noffers reduced communication costs and computational complexity by eliminating\nthe traditional, computing-intensive ``G-optimal design\" found in previous\nworks. Extensive experiments with the open LLM \\textit{Llama}, coupled with two\ndifferent embedding models from Google and OpenAI for text vector\nrepresentation, demonstrate that \\cadi\\ significantly outperforms the current\nstate-of-the-art in online LLM response identification.",
      "tldr_zh": "这篇论文针对大型语言模型(LLMs)的响应生成问题，提出了一种多代理对话在线学习框架Maco，用于适应性地识别最优LLM响应。该框架利用多个本地代理（如智能手机）加速在线学习过程，同时提升数据隐私，并引入新型对话机制来动态收集用户偏好（如幽默 vs. 严肃的语气），从而最小化偏好估计的不确定性。理论分析证明Maco在累积遗憾方面近优化，并减少了通信和计算成本；实验结果显示，使用Llama模型和Google/OpenAI的嵌入模型时，Maco显著优于现有方法。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01849v1",
      "published_date": "2025-01-03 14:59:38 UTC",
      "updated_date": "2025-01-03 14:59:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:11:53.457946"
    },
    {
      "arxiv_id": "2501.01836v1",
      "title": "Practical machine learning is learning on small samples",
      "title_zh": "实用机器学习是基于小样本的学习",
      "authors": [
        "Marina Sapir"
      ],
      "abstract": "Based on limited observations, machine learning discerns a dependence which\nis expected to hold in the future. What makes it possible? Statistical learning\ntheory imagines indefinitely increasing training sample to justify its\napproach. In reality, there is no infinite time or even infinite general\npopulation for learning. Here I argue that practical machine learning is based\non an implicit assumption that underlying dependence is relatively ``smooth\" :\nlikely, there are no abrupt differences in feedback between cases with close\ndata points. From this point of view learning shall involve selection of the\nhypothesis ``smoothly\" approximating the training set. I formalize this as\nPractical learning paradigm. The paradigm includes terminology and rules for\ndescription of learners. Popular learners (local smoothing, k-NN, decision\ntrees, Naive Bayes, SVM for classification and for regression) are shown here\nto be implementations of this paradigm.",
      "tldr_zh": "这篇论文认为，实际机器学习（machine learning）依赖于小样本学习，并基于一个隐含假设：底层依赖关系相对“smooth”，即相近数据点之间的反馈不会出现剧烈差异。作者形式化了Practical learning paradigm，该范式包括术语和规则，用于描述学习器，并强调学习应选择平滑逼近训练集的假设。与统计学习理论不同，该范式不依赖于无限样本，而是聚焦于现实场景。结果显示，流行学习器如k-NN、decision trees、Naive Bayes和SVM（用于分类和回归）均是这一范式的实现。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01836v1",
      "published_date": "2025-01-03 14:38:07 UTC",
      "updated_date": "2025-01-03 14:38:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:12:05.250215"
    },
    {
      "arxiv_id": "2501.01835v1",
      "title": "ASKCOS: an open source software suite for synthesis planning",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengkai Tu",
        "Sourabh J. Choure",
        "Mun Hong Fong",
        "Jihye Roh",
        "Itai Levin",
        "Kevin Yu",
        "Joonyoung F. Joung",
        "Nathan Morgan",
        "Shih-Cheng Li",
        "Xiaoqi Sun",
        "Huiqian Lin",
        "Mark Murnin",
        "Jordan P. Liles",
        "Thomas J. Struble",
        "Michael E. Fortunato",
        "Mengjie Liu",
        "William H. Green",
        "Klavs F. Jensen",
        "Connor W. Coley"
      ],
      "abstract": "The advancement of machine learning and the availability of large-scale\nreaction datasets have accelerated the development of data-driven models for\ncomputer-aided synthesis planning (CASP) in the past decade. Here, we detail\nthe newest version of ASKCOS, an open source software suite for synthesis\nplanning that makes available several research advances in a freely available,\npractical tool. Four one-step retrosynthesis models form the basis of both\ninteractive planning and automatic planning modes. Retrosynthetic planning is\ncomplemented by other modules for feasibility assessment and pathway\nevaluation, including reaction condition recommendation, reaction outcome\nprediction, and auxiliary capabilities such as solubility prediction and\nquantum mechanical descriptor prediction. ASKCOS has assisted hundreds of\nmedicinal, synthetic, and process chemists in their day-to-day tasks,\ncomplementing expert decision making. It is our belief that CASP tools like\nASKCOS are an important part of modern chemistry research, and that they offer\never-increasing utility and accessibility.",
      "tldr_zh": "该研究介绍了 ASKCOS 的最新开源软件套件，用于计算机辅助合成规划 (CASP)，它整合了机器学习和大规模反应数据集的进展。软件包括四个一步逆合成模型，支持交互式和自动规划，并辅以反应条件推荐、反应结果预测、溶解度预测以及量子力学描述符预测等模块，以评估合成路径的可行性。ASKCOS 已帮助数百名化学家日常任务，提升了决策效率，并被视为现代化学研究中越来越实用和可访问的工具。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01835v1",
      "published_date": "2025-01-03 14:38:03 UTC",
      "updated_date": "2025-01-03 14:38:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:12:16.997592"
    },
    {
      "arxiv_id": "2501.01834v3",
      "title": "MoColl: Agent-Based Specific and General Model Collaboration for Image Captioning",
      "title_zh": "翻译失败",
      "authors": [
        "Pu Yang",
        "Bin Dong"
      ],
      "abstract": "Image captioning is a critical task at the intersection of computer vision\nand natural language processing, with wide-ranging applications across various\ndomains. For complex tasks such as diagnostic report generation, deep learning\nmodels require not only domain-specific image-caption datasets but also the\nincorporation of relevant general knowledge to provide contextual accuracy.\nExisting approaches exhibit inherent limitations: specialized models excel in\ncapturing domain-specific details but lack generalization, while\nvision-language models (VLMs) built on large language models (LLMs) leverage\ngeneral knowledge but struggle with domain-specific adaptation. To address\nthese limitations, this paper proposes a novel agent-enhanced model\ncollaboration framework, which we call MoColl, designed to effectively\nintegrate domain-specific and general knowledge. Specifically, our approach is\nto decompose complex image captioning tasks into a series of interconnected\nquestion-answer subtasks. A trainable visual question answering (VQA) model is\nemployed as a specialized tool to focus on domain-specific visual analysis,\nanswering task-specific questions based on image content. Concurrently, an\nLLM-based agent with general knowledge formulates these questions and\nsynthesizes the resulting question-answer pairs into coherent captions. Beyond\nits role in leveraging the VQA model, the agent further guides its training to\nenhance its domain-specific capabilities. Experimental results on radiology\nreport generation validate the effectiveness of the proposed framework,\ndemonstrating significant improvements in the quality of generated reports.",
      "tldr_zh": "该论文提出 MoColl 框架，这是一种基于代理的模型协作方法，用于图像描述（Image Captioning），旨在整合领域特定知识和一般知识，以解决现有方法在专用模型泛化不足和视觉语言模型（VLMs）领域适应性差的问题。具体而言，MoColl 将复杂任务分解为互联的问答子任务，使用可训练的视觉问答（VQA）模型专注于领域特定视觉分析，而 LLM-based 代理则负责制定问题、合成问答对生成连贯描述，并指导 VQA 模型的训练。实验结果显示，在放射学报告生成任务上，该框架显著提高了报告质量，验证了其有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01834v3",
      "published_date": "2025-01-03 14:38:01 UTC",
      "updated_date": "2025-01-27 16:34:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:12:29.077186"
    },
    {
      "arxiv_id": "2501.02039v1",
      "title": "An Investigation into Value Misalignment in LLM-Generated Texts for Cultural Heritage",
      "title_zh": "翻译失败",
      "authors": [
        "Fan Bu",
        "Zheng Wang",
        "Siyi Wang",
        "Ziyao Liu"
      ],
      "abstract": "As Large Language Models (LLMs) become increasingly prevalent in tasks\nrelated to cultural heritage, such as generating descriptions of historical\nmonuments, translating ancient texts, preserving oral traditions, and creating\neducational content, their ability to produce accurate and culturally aligned\ntexts is being increasingly relied upon by users and researchers. However,\ncultural value misalignments may exist in generated texts, such as the\nmisrepresentation of historical facts, the erosion of cultural identity, and\nthe oversimplification of complex cultural narratives, which may lead to severe\nconsequences. Therefore, investigating value misalignment in the context of LLM\nfor cultural heritage is crucial for mitigating these risks, yet there has been\na significant lack of systematic and comprehensive study and investigation in\nthis area. To fill this gap, we systematically assess the reliability of LLMs\nin generating culturally aligned texts for cultural heritage-related tasks. We\nconduct a comprehensive evaluation by compiling an extensive set of 1066 query\ntasks covering 5 widely recognized categories with 17 aspects within the\nknowledge framework of cultural heritage across 5 open-source LLMs, and examine\nboth the type and rate of cultural value misalignments in the generated texts.\nUsing both automated and manual approaches, we effectively detect and analyze\nthe cultural value misalignments in LLM-generated texts. Our findings are\nconcerning: over 65% of the generated texts exhibit notable cultural\nmisalignments, with certain tasks demonstrating almost complete misalignment\nwith key cultural values. Beyond these findings, this paper introduces a\nbenchmark dataset and a comprehensive evaluation workflow that can serve as a\nvaluable resource for future research aimed at enhancing the cultural\nsensitivity and reliability of LLMs.",
      "tldr_zh": "这篇论文调查了大型语言模型（LLMs）在文化遗产相关任务（如生成历史描述和翻译古文）中生成的文本可能存在的价值失调问题，例如误传历史事实或侵蚀文化身份。研究者通过编译1066个查询任务，涵盖5个类别和17个方面的文化遗产知识框架，对5个开源LLMs进行系统评估，并采用自动化和手动方法检测失调类型和比率。结果显示，超过65%的生成文本存在显著文化失调，有些任务甚至几乎完全失调。该论文贡献了一个基准数据集和全面评估工作流，以帮助未来研究提升LLMs的文化敏感性和可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02039v1",
      "published_date": "2025-01-03 14:35:32 UTC",
      "updated_date": "2025-01-03 14:35:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:12:41.493894"
    },
    {
      "arxiv_id": "2501.01830v1",
      "title": "Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yanjiang Liu",
        "Shuhen Zhou",
        "Yaojie Lu",
        "Huijia Zhu",
        "Weiqiang Wang",
        "Hongyu Lin",
        "Ben He",
        "Xianpei Han",
        "Le Sun"
      ],
      "abstract": "Automated red-teaming has become a crucial approach for uncovering\nvulnerabilities in large language models (LLMs). However, most existing methods\nfocus on isolated safety flaws, limiting their ability to adapt to dynamic\ndefenses and uncover complex vulnerabilities efficiently. To address this\nchallenge, we propose Auto-RT, a reinforcement learning framework that\nautomatically explores and optimizes complex attack strategies to effectively\nuncover security vulnerabilities through malicious queries. Specifically, we\nintroduce two key mechanisms to reduce exploration complexity and improve\nstrategy optimization: 1) Early-terminated Exploration, which accelerate\nexploration by focusing on high-potential attack strategies; and 2) Progressive\nReward Tracking algorithm with intermediate downgrade models, which dynamically\nrefine the search trajectory toward successful vulnerability exploitation.\nExtensive experiments across diverse LLMs demonstrate that, by significantly\nimproving exploration efficiency and automatically optimizing attack\nstrategies, Auto-RT detects a boarder range of vulnerabilities, achieving a\nfaster detection speed and 16.63\\% higher success rates compared to existing\nmethods.",
      "tldr_zh": "本文提出 Auto-RT，一种基于 reinforcement learning 的框架，用于自动探索和优化复杂的攻击策略，从而更高效地揭示 Large Language Models (LLMs) 的安全漏洞。Auto-RT 引入 Early-terminated Exploration 机制来加速探索高潜力策略，以及 Progressive Reward Tracking 算法结合中间降级模型动态优化搜索轨迹。实验结果显示，该框架在多种 LLM 上比现有方法检测速度更快，成功率提高 16.63%，并覆盖更广泛的漏洞。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01830v1",
      "published_date": "2025-01-03 14:30:14 UTC",
      "updated_date": "2025-01-03 14:30:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:12:52.916114"
    },
    {
      "arxiv_id": "2501.01827v1",
      "title": "The Proof is in the Almond Cookies",
      "title_zh": "翻译失败",
      "authors": [
        "Remi van Trijp",
        "Katrien Beuls",
        "Paul Van Eecke"
      ],
      "abstract": "This paper presents a case study on how to process cooking recipes (and more\ngenerally, how-to instructions) in a way that makes it possible for a robot or\nartificial cooking assistant to support human chefs in the kitchen. Such AI\nassistants would be of great benefit to society, as they can help to sustain\nthe autonomy of aging adults or people with a physical impairment, or they may\nreduce the stress in a professional kitchen. We propose a novel approach to\ncomputational recipe understanding that mimics the human sense-making process,\nwhich is narrative-based. Using an English recipe for almond crescent cookies\nas illustration, we show how recipes can be modelled as rich narrative\nstructures by integrating various knowledge sources such as language\nprocessing, ontologies, and mental simulation. We show how such narrative\nstructures can be used for (a) dealing with the challenges of recipe language,\nsuch as zero anaphora, (b) optimizing a robot's planning process, (c) measuring\nhow well an AI system understands its current tasks, and (d) allowing recipe\nannotations to become language-independent.",
      "tldr_zh": "本论文探讨了如何处理烹饪食谱或其他操作指南，使机器人或AI烹饪助手能有效辅助人类厨师，从而支持老年人自主性或减轻专业厨房压力。研究提出了一种新方法，通过模仿人类的叙事式理解过程，将食谱建模为丰富的叙事结构，并整合语言处理、本体论(ontologies)和心理模拟(mental simulation)等知识来源。使用杏仁新月形饼干食谱作为示例，该方法能解决食谱语言挑战，如零照应(zero anaphora)，优化机器人的规划过程，评估AI任务理解水平，并实现语言无关的食谱注释。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01827v1",
      "published_date": "2025-01-03 14:25:35 UTC",
      "updated_date": "2025-01-03 14:25:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:13:04.952147"
    },
    {
      "arxiv_id": "2501.02038v1",
      "title": "Architecture for Trajectory-Based Fishing Ship Classification with AIS Data",
      "title_zh": "基于轨迹的渔船分类架构，使用 AIS 数据",
      "authors": [
        "David Sánchez Pedroche",
        "Daniel Amigo",
        "Jesús García",
        "Jose M. Molina"
      ],
      "abstract": "This paper proposes a data preparation process for managing real-world\nkinematic data and detecting fishing vessels. The solution is a binary\nclassification that classifies ship trajectories into either fishing or\nnon-fishing ships. The data used are characterized by the typical problems\nfound in classic data mining applications using real-world data, such as noise\nand inconsistencies. The two classes are also clearly unbalanced in the data, a\nproblem which is addressed using algorithms that resample the instances. For\nclassification, a series of features are extracted from spatiotemporal data\nthat represent the trajectories of the ships, available from sequences of\nAutomatic Identification System (AIS) reports. These features are proposed for\nthe modelling of ship behavior but, because they do not contain context-related\ninformation, the classification can be applied in other scenarios.\nExperimentation shows that the proposed data preparation process is useful for\nthe presented classification problem. In addition, positive results are\nobtained using minimal information.",
      "tldr_zh": "本论文提出了一种基于轨迹的渔船分类架构，使用Automatic Identification System (AIS) 数据进行二元分类，将船只轨迹分为渔船或非渔船。方法包括数据准备过程，处理真实世界数据中的噪音、不一致性和类别不平衡问题，通过重采样算法和从时空数据中提取特征来建模船只行为。这些特征不依赖特定上下文，因此可扩展到其他场景。实验结果表明，该架构在使用最小信息的情况下，实现了有效的分类性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Sensors 2020",
      "pdf_url": "http://arxiv.org/pdf/2501.02038v1",
      "published_date": "2025-01-03 14:12:40 UTC",
      "updated_date": "2025-01-03 14:12:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:13:17.125305"
    },
    {
      "arxiv_id": "2501.01821v2",
      "title": "SDPO: Segment-Level Direct Preference Optimization for Social Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Aobo Kong",
        "Wentao Ma",
        "Shiwan Zhao",
        "Yongbin Li",
        "Yuchuan Wu",
        "Ke Wang",
        "Xiaoqian Liu",
        "Qicheng Li",
        "Yong Qin",
        "Fei Huang"
      ],
      "abstract": "Social agents powered by large language models (LLMs) can simulate human\nsocial behaviors but fall short in handling complex social dialogues. Direct\nPreference Optimization (DPO) has proven effective in aligning LLM behavior\nwith human preferences across various agent tasks. However, standard DPO\nfocuses solely on individual turns, which limits its effectiveness in\nmulti-turn social interactions. Several DPO-based multi-turn alignment methods\nwith session-level data have shown potential in addressing this problem.While\nthese methods consider multiple turns across entire sessions, they are often\noverly coarse-grained, introducing training noise, and lack robust theoretical\nsupport. To resolve these limitations, we propose Segment-Level Direct\nPreference Optimization (SDPO), which dynamically select key segments within\ninteractions to optimize multi-turn agent behavior. SDPO minimizes training\nnoise and is grounded in a rigorous theoretical framework. Evaluations on the\nSOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform\nboth existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring\nSDPO's potential to advance the social intelligence of LLM-based agents. We\nrelease our code and data at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO.",
      "tldr_zh": "该研究针对大型语言模型 (LLMs) 驱动的社会代理在处理多回合复杂社会对话时的不足，提出了 Segment-Level Direct Preference Optimization (SDPO) 方法。SDPO 通过动态选择交互中的关键段落进行优化，相比标准 Direct Preference Optimization (DPO)，它减少了训练噪声并提供了严格的理论支持，从而更有效地提升代理的多回合行为对齐。在 SOTOPIA 基准测试中，SDPO 优于现有 DPO 方法和 GPT-4o，展示了其在提升社会智能方面的潜力，并开源了相关代码。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01821v2",
      "published_date": "2025-01-03 14:09:46 UTC",
      "updated_date": "2025-02-27 05:42:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:13:28.304990"
    },
    {
      "arxiv_id": "2501.01805v1",
      "title": "End-to-End Long Document Summarization using Gradient Caching",
      "title_zh": "翻译失败",
      "authors": [
        "Rohit Saxena",
        "Hao Tang",
        "Frank Keller"
      ],
      "abstract": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
      "tldr_zh": "该论文解决了基于 Transformer 的编码器-解码器模型在训练长文档摘要时面临的二次内存消耗问题，提出了一种名为 CachED 的方法，实现端到端训练而不需截断输入文档。CachED 通过对输入文档应用非重叠滑动窗口，并在解码器中进行融合，同时在反向传播中缓存梯度并分块重新计算编码器隐藏向量，类似于梯度检查点技术。实验结果显示，将 BART 扩展为 CachED BART 后，该模型能够在训练中处理超过 500K 标记，实现了优越的摘要性能，而无需增加额外参数。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01805v1",
      "published_date": "2025-01-03 13:32:57 UTC",
      "updated_date": "2025-01-03 13:32:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:13:40.529709"
    },
    {
      "arxiv_id": "2501.01802v1",
      "title": "BERT4MIMO: A Foundation Model using BERT Architecture for Massive MIMO Channel State Information Prediction",
      "title_zh": "BERT4MIMO：一种基于 BERT 架构的大规模 MIMO 信道状态信息预测基础模型",
      "authors": [
        "Ferhat Ozgur Catak",
        "Murat Kuzlu",
        "Umit Cali"
      ],
      "abstract": "Massive MIMO (Multiple-Input Multiple-Output) is an advanced wireless\ncommunication technology, using a large number of antennas to improve the\noverall performance of the communication system in terms of capacity, spectral,\nand energy efficiency. The performance of MIMO systems is highly dependent on\nthe quality of channel state information (CSI). Predicting CSI is, therefore,\nessential for improving communication system performance, particularly in MIMO\nsystems, since it represents key characteristics of a wireless channel,\nincluding propagation, fading, scattering, and path loss. This study proposes a\nfoundation model inspired by BERT, called BERT4MIMO, which is specifically\ndesigned to process high-dimensional CSI data from massive MIMO systems.\nBERT4MIMO offers superior performance in reconstructing CSI under varying\nmobility scenarios and channel conditions through deep learning and attention\nmechanisms. The experimental results demonstrate the effectiveness of BERT4MIMO\nin a variety of wireless environments.",
      "tldr_zh": "本研究针对Massive MIMO系统对Channel State Information (CSI)预测的需求，提出了一种基于BERT架构的Foundation Model，名为BERT4MIMO，用于处理高维CSI数据。BERT4MIMO通过深度学习和注意力机制，实现对CSI的重建，适用于不同移动场景和无线通道条件。实验结果表明，该模型在各种无线环境中表现出色，有效提升了通信系统的性能。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "eess.SP",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.01802v1",
      "published_date": "2025-01-03 13:22:19 UTC",
      "updated_date": "2025-01-03 13:22:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:13:52.599957"
    },
    {
      "arxiv_id": "2501.02036v1",
      "title": "Deep Clustering via Community Detection",
      "title_zh": "基于社区检测的深度聚类",
      "authors": [
        "Tianyu Cheng",
        "Qun Chen"
      ],
      "abstract": "Deep clustering is an essential task in modern artificial intelligence,\naiming to partition a set of data samples into a given number of homogeneous\ngroups (i.e., clusters). Even though many Deep Neural Network (DNN) backbones\nand clustering strategies have been proposed for the task, achieving\nincreasingly improved performance, deep clustering remains very challenging due\nto the lack of accurately labeled samples. In this paper, we propose a novel\napproach of deep clustering via community detection. It initializes clustering\nby detecting many communities, and then gradually expands clusters by community\nmerging. Compared with the existing clustering strategies, community detection\nfactors in the new perspective of cluster network analysis. As a result, it has\nthe inherent benefit of high pseudo-label purity, which is critical to the\nperformance of self-supervision. We have validated the efficacy of the proposed\napproach on benchmark image datasets. Our extensive experiments have shown that\nit can effectively improve the SOTA performance. Our ablation study also\ndemonstrates that the new network perspective can effectively improve community\npseudo-label purity, resulting in improved clustering performance.",
      "tldr_zh": "该论文提出了一种基于社区检测(Community Detection)的新型深度聚类(Deep Clustering)方法，旨在将数据样本划分为同质群组，同时解决缺乏准确标记样本的挑战。方法通过先检测多个社区初始化聚类，然后逐步合并社区来扩展群组，利用聚类网络分析的新视角提升伪标签纯度，从而增强自监督学习的效果。在基准图像数据集上的实验验证显示，该方法显著提高了SOTA性能，并通过消融研究证明了新视角对聚类性能的积极影响。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI",
        "68T45",
        "I.2.0"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.02036v1",
      "published_date": "2025-01-03 12:56:12 UTC",
      "updated_date": "2025-01-03 12:56:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:14:05.011823"
    },
    {
      "arxiv_id": "2501.01793v1",
      "title": "Creating Artificial Students that Never Existed: Leveraging Large Language Models and CTGANs for Synthetic Data Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Khalil",
        "Farhad Vadiee",
        "Ronas Shakya",
        "Qinyi Liu"
      ],
      "abstract": "In this study, we explore the growing potential of AI and deep learning\ntechnologies, particularly Generative Adversarial Networks (GANs) and Large\nLanguage Models (LLMs), for generating synthetic tabular data. Access to\nquality students data is critical for advancing learning analytics, but privacy\nconcerns and stricter data protection regulations worldwide limit their\navailability and usage. Synthetic data offers a promising alternative. We\ninvestigate whether synthetic data can be leveraged to create artificial\nstudents for serving learning analytics models. Using the popular GAN model\nCTGAN and three LLMs- GPT2, DistilGPT2, and DialoGPT, we generate synthetic\ntabular student data. Our results demonstrate the strong potential of these\nmethods to produce high-quality synthetic datasets that resemble real students\ndata. To validate our findings, we apply a comprehensive set of utility\nevaluation metrics to assess the statistical and predictive performance of the\nsynthetic data and compare the different generator models used, specially the\nperformance of LLMs. Our study aims to provide the learning analytics community\nwith valuable insights into the use of synthetic data, laying the groundwork\nfor expanding the field methodological toolbox with new innovative approaches\nfor learning analytics data generation.",
      "tldr_zh": "本研究探讨了利用Large Language Models (LLMs) 和CTGAN生成合成学生数据的潜力，以解决学习分析领域中数据隐私法规限制真实数据可用性的问题。研究团队使用CTGAN以及LLMs（如GPT2、DistilGPT2和DialoGPT）生成合成表格数据，并通过统计和预测性能评估指标验证其质量，结果显示合成数据与真实数据高度相似。相比不同生成模型，LLMs表现出色，为学习分析社区提供了创新方法，扩展了数据生成工具箱。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01793v1",
      "published_date": "2025-01-03 12:52:51 UTC",
      "updated_date": "2025-01-03 12:52:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:14:17.547914"
    },
    {
      "arxiv_id": "2501.01785v1",
      "title": "Can Synthetic Data be Fair and Private? A Comparative Study of Synthetic Data Generation and Fairness Algorithms",
      "title_zh": "合成",
      "authors": [
        "Qinyi Liu",
        "Oscar Deho",
        "Farhad Vadiee",
        "Mohammad Khalil",
        "Srecko Joksimovic",
        "George Siemens"
      ],
      "abstract": "The increasing use of machine learning in learning analytics (LA) has raised\nsignificant concerns around algorithmic fairness and privacy. Synthetic data\nhas emerged as a dual-purpose tool, enhancing privacy and improving fairness in\nLA models. However, prior research suggests an inverse relationship between\nfairness and privacy, making it challenging to optimize both. This study\ninvestigates which synthetic data generators can best balance privacy and\nfairness, and whether pre-processing fairness algorithms, typically applied to\nreal datasets, are effective on synthetic data. Our results highlight that the\nDEbiasing CAusal Fairness (DECAF) algorithm achieves the best balance between\nprivacy and fairness. However, DECAF suffers in utility, as reflected in its\npredictive accuracy. Notably, we found that applying pre-processing fairness\nalgorithms to synthetic data improves fairness even more than when applied to\nreal data. These findings suggest that combining synthetic data generation with\nfairness pre-processing offers a promising approach to creating fairer LA\nmodels.",
      "tldr_zh": "该研究探讨了合成数据(Synthetic Data)在机器学习中的作用，特别比较了不同合成数据生成器在平衡隐私和公平性方面的表现，以及预处理公平算法在合成数据上的有效性。\n结果显示，DEbiasing CAusal Fairness (DECAF) 算法在隐私和公平性之间实现了最佳平衡，但其预测准确率（实用性）有所下降。\n此外，研究发现，在合成数据上应用预处理公平算法比在真实数据上更能显著提升公平性，为构建更公平的学习分析(LA)模型提供了有前景的结合方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01785v1",
      "published_date": "2025-01-03 12:35:58 UTC",
      "updated_date": "2025-01-03 12:35:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:15:05.657949"
    },
    {
      "arxiv_id": "2501.02035v1",
      "title": "3D Cloud reconstruction through geospatially-aware Masked Autoencoders",
      "title_zh": "通过地理空间感知的掩码自动编码器进行三维云重建",
      "authors": [
        "Stella Girtsou",
        "Emiliano Diaz Salas-Porras",
        "Lilli Freischem",
        "Joppe Massant",
        "Kyriaki-Margarita Bintsi",
        "Guiseppe Castiglione",
        "William Jones",
        "Michael Eisinger",
        "Emmanuel Johnson",
        "Anna Jungbluth"
      ],
      "abstract": "Clouds play a key role in Earth's radiation balance with complex effects that\nintroduce large uncertainties into climate models. Real-time 3D cloud data is\nessential for improving climate predictions. This study leverages geostationary\nimagery from MSG/SEVIRI and radar reflectivity measurements of cloud profiles\nfrom CloudSat/CPR to reconstruct 3D cloud structures. We first apply\nself-supervised learning (SSL) methods-Masked Autoencoders (MAE) and\ngeospatially-aware SatMAE on unlabelled MSG images, and then fine-tune our\nmodels on matched image-profile pairs. Our approach outperforms\nstate-of-the-art methods like U-Nets, and our geospatial encoding further\nimproves prediction results, demonstrating the potential of SSL for cloud\nreconstruction.",
      "tldr_zh": "这篇论文提出了一种基于地理空间感知的 Masked Autoencoders (MAE) 和 SatMAE 的方法，用于重建 3D 云结构，以减少云对气候模型的不确定性影响。研究利用自监督学习 (SSL) 先在无标签的 MSG/SEVIRI 卫星图像上训练模型，然后通过 CloudSat/CPR 的雷达反射率数据进行微调。结果显示，该方法优于 U-Nets 等现有技术，地理空间编码进一步提升了预测性能，并证明了 SSL 在实时 3D 云数据重建中的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T45"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02035v1",
      "published_date": "2025-01-03 12:26:04 UTC",
      "updated_date": "2025-01-03 12:26:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:14:41.603133"
    },
    {
      "arxiv_id": "2502.00003v1",
      "title": "Defending Compute Thresholds Against Legal Loopholes",
      "title_zh": "翻译失败",
      "authors": [
        "Matteo Pistillo",
        "Pablo Villalobos"
      ],
      "abstract": "Existing legal frameworks on AI rely on training compute thresholds as a\nproxy to identify potentially-dangerous AI models and trigger increased\nregulatory attention. In the United States, Section 4.2(a) of Executive Order\n14110 instructs the Secretary of Commerce to require extensive reporting from\ndevelopers of AI models above a certain training compute threshold. In the\nEuropean Union, Article 51 of the AI Act establishes a presumption that AI\nmodels above a certain compute threshold have high impact capabilities and\nhence pose systemic risk, thus subjecting their developers to several\nobligations including capability evaluations, reporting, and incident\nmonitoring. In this paper, we examine some enhancement techniques that are\ncapable of decreasing training compute usage while preserving, or even\nincreasing, model capabilities. Since training compute thresholds rely on\ntraining compute as a metric and trigger for increased regulatory attention,\nthese capability-enhancing and compute-saving techniques could constitute a\nlegal loophole to existing training compute thresholds. In particular, we\nconcentrate on four illustrative techniques (fine-tuning, model reuse, model\nexpansion, and above compute-optimal inference compute) with the goal of\nfurthering the conversation about their implications on training compute\nthresholds as a legal mechanism and advancing policy recommendations that could\naddress the relevant legal loopholes.",
      "tldr_zh": "本论文探讨了现有AI法律框架（如美国行政命令Section 4.2(a)和欧盟AI Act Article 51）依赖训练计算阈值来识别高风险AI模型的局限性，这些阈值可能被某些技术规避。研究者分析了四种增强技术，包括fine-tuning、model reuse、model expansion和above compute-optimal inference compute，这些方法能减少训练计算使用同时维持或提升模型能力，从而构成潜在的法律漏洞。论文旨在推进对这些技术的政策讨论，并提出推荐措施，以加强训练计算阈值作为监管机制的有效性。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.00003v1",
      "published_date": "2025-01-03 12:07:21 UTC",
      "updated_date": "2025-01-03 12:07:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:14:52.581901"
    },
    {
      "arxiv_id": "2501.01763v1",
      "title": "Quantifying A Firm's AI Engagement: Constructing Objective, Data-Driven, AI Stock Indices Using 10-K Filings",
      "title_zh": "翻译失败",
      "authors": [
        "Lennart Ante",
        "Aman Saggu"
      ],
      "abstract": "Following an analysis of existing AI-related exchange-traded funds (ETFs), we\nreveal the selection criteria for determining which stocks qualify as\nAI-related are often opaque and rely on vague phrases and subjective judgments.\nThis paper proposes a new, objective, data-driven approach using natural\nlanguage processing (NLP) techniques to classify AI stocks by analyzing annual\n10-K filings from 3,395 NASDAQ-listed firms between 2011 and 2023. This\nanalysis quantifies each company's engagement with AI through binary indicators\nand weighted AI scores based on the frequency and context of AI-related terms.\nUsing these metrics, we construct four AI stock indices-the Equally Weighted AI\nIndex (AII), the Size-Weighted AI Index (SAII), and two Time-Discounted AI\nIndices (TAII05 and TAII5X)-offering different perspectives on AI investment.\nWe validate our methodology through an event study on the launch of OpenAI's\nChatGPT, demonstrating that companies with higher AI engagement saw\nsignificantly greater positive abnormal returns, with analyses supporting the\npredictive power of our AI measures. Our indices perform on par with or surpass\n14 existing AI-themed ETFs and the Nasdaq Composite Index in risk-return\nprofiles, market responsiveness, and overall performance, achieving higher\naverage daily returns and risk-adjusted metrics without increased volatility.\nThese results suggest our NLP-based approach offers a reliable,\nmarket-responsive, and cost-effective alternative to existing AI-related ETF\nproducts. Our innovative methodology can also guide investors, asset managers,\nand policymakers in using corporate data to construct other thematic\nportfolios, contributing to a more transparent, data-driven, and competitive\napproach.",
      "tldr_zh": "本论文分析了现有 AI 相关 ETF 的选择标准往往不透明且主观，提出一种客观的数据驱动方法，使用 NLP 技术分析 3395 家 NASDAQ 上市公司从 2011 到 2023 年的 10-K filings，量化公司的 AI 参与度通过二元指标和加权 AI 分数。\n基于这些指标，构建了四个 AI 股票指数：Equally Weighted AI Index (AII)、Size-Weighted AI Index (SAII) 以及 Time-Discounted AI Indices (TAII05 和 TAII5X)，提供不同投资视角。\n通过对 OpenAI ChatGPT 发布的案例研究，验证高 AI 参与度公司表现出显著的异常回报，且这些指数在风险回报、市场响应性和整体表现上优于 14 个现有 AI 主题 ETF 和 Nasdaq Composite Index。\n这项创新方法为投资者、资产管理者和政策制定者提供了一个透明、可靠的工具，用于构建其他主题投资组合。",
      "categories": [
        "q-fin.GN",
        "cs.AI",
        "econ.EM",
        "q-fin.PM",
        "q-fin.RM",
        "91G60, 68T50, 91B84, 91B82, 91B28, 91G70",
        "J.4; H.3.3; I.2.7; J.1"
      ],
      "primary_category": "q-fin.GN",
      "comment": "43 pages, 5 tables, 3 figures, 1 appendix figure",
      "pdf_url": "http://arxiv.org/pdf/2501.01763v1",
      "published_date": "2025-01-03 11:27:49 UTC",
      "updated_date": "2025-01-03 11:27:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:15:07.969250"
    },
    {
      "arxiv_id": "2501.04038v1",
      "title": "Listening and Seeing Again: Generative Error Correction for Audio-Visual Speech Recognition",
      "title_zh": "再次聆听与观看：用于音频-视觉语音识别的生成式错误修正",
      "authors": [
        "Rui Liu",
        "Hongyu Yuan",
        "Haizhou Li"
      ],
      "abstract": "Unlike traditional Automatic Speech Recognition (ASR), Audio-Visual Speech\nRecognition (AVSR) takes audio and visual signals simultaneously to infer the\ntranscription. Recent studies have shown that Large Language Models (LLMs) can\nbe effectively used for Generative Error Correction (GER) in ASR by predicting\nthe best transcription from ASR-generated N-best hypotheses. However, these\nLLMs lack the ability to simultaneously understand audio and visual, making the\nGER approach challenging to apply in AVSR. In this work, we propose a novel GER\nparadigm for AVSR, termed AVGER, that follows the concept of ``listening and\nseeing again''. Specifically, we first use the powerful AVSR system to read the\naudio and visual signals to get the N-Best hypotheses, and then use the\nQ-former-based Multimodal Synchronous Encoder to read the audio and visual\ninformation again and convert them into an audio and video compression\nrepresentation respectively that can be understood by LLM. Afterward, the\naudio-visual compression representation and the N-Best hypothesis together\nconstitute a Cross-modal Prompt to guide the LLM in producing the best\ntranscription. In addition, we also proposed a Multi-Level Consistency\nConstraint training criterion, including logits-level, utterance-level and\nrepresentations-level, to improve the correction accuracy while enhancing the\ninterpretability of audio and visual compression representations. The\nexperimental results on the LRS3 dataset show that our method outperforms\ncurrent mainstream AVSR systems. The proposed AVGER can reduce the Word Error\nRate (WER) by 24% compared to them. Code and models can be found at:\nhttps://github.com/CircleRedRain/AVGER.",
      "tldr_zh": "本研究提出了一种新型生成错误修正（GER）范式，名为 AVGER，用于音频-视觉语音识别（AVSR），旨在通过“listening and seeing again”的概念来处理音频和视觉信号的同步理解问题。具体方法包括先使用 AVSR 系统生成 N-Best 假设，然后采用 Q-former-based Multimodal Synchronous Encoder 重新处理音频和视觉信息，转换为 LLM 可理解的压缩表示，并结合 Cross-modal Prompt 引导 LLM 产生最佳转录；同时引入 Multi-Level Consistency Constraint 训练标准（包括 logits-level、utterance-level 和 representations-level）以提升修正准确性和解释性。在 LRS3 数据集上的实验显示，AVGER 比主流 AVSR 系统降低了 24% 的 Word Error Rate (WER)，显著提高了语音识别性能。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.MM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04038v1",
      "published_date": "2025-01-03 10:51:14 UTC",
      "updated_date": "2025-01-03 10:51:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:15:19.096245"
    },
    {
      "arxiv_id": "2501.01743v2",
      "title": "Automating Legal Concept Interpretation with LLMs: Retrieval, Generation, and Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Kangcheng Luo",
        "Quzhe Huang",
        "Cong Jiang",
        "Yansong Feng"
      ],
      "abstract": "Legal articles often include vague concepts for adapting to the ever-changing\nsociety. Providing detailed interpretations of these concepts is a critical and\nchallenging task even for legal practitioners. It requires meticulous and\nprofessional annotations and summarizations by legal experts, which are\nadmittedly time-consuming and expensive to collect at scale. By emulating legal\nexperts' doctrinal method, we introduce a novel framework, ATRIE, using large\nlanguage models (LLMs) to AuTomatically Retrieve concept-related information,\nInterpret legal concepts, and Evaluate generated interpretations, eliminating\ndependence on legal experts. ATRIE comprises a legal concept interpreter and a\nlegal concept interpretation evaluator. The interpreter uses LLMs to retrieve\nrelevant information from judicial precedents and interpret legal concepts. The\nevaluator uses performance changes on legal concept entailment, a downstream\ntask we propose, as a proxy of interpretation quality. Automatic and\nmultifaceted human evaluations indicate that the quality of our interpretations\nis comparable to those written by legal experts, with superior\ncomprehensiveness and readability. Although there remains a slight gap in\naccuracy, it can already assist legal practitioners in improving the efficiency\nof concept interpretation.",
      "tldr_zh": "本论文提出 ATRIE 框架，利用 LLMs 自动处理法律概念解释，包括 Retrieval（检索相关司法先例）、Generation（生成解释）和 Evaluation（通过法律概念 entailment 下游任务评估质量），以模仿法律专家的方法并减少对专家的依赖。框架的核心组件是一个解释器和一个评估器，确保解释过程高效且专业。实验结果显示，ATRI 生成的解释在全面性和可读性上优于专家水平，虽然准确性存在轻微差距，但能显著提升法律从业者的效率。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01743v2",
      "published_date": "2025-01-03 10:11:38 UTC",
      "updated_date": "2025-02-16 09:15:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:15:29.953553"
    },
    {
      "arxiv_id": "2501.01741v1",
      "title": "How Toxic Can You Get? Search-based Toxicity Testing for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Simone Corbo",
        "Luca Bancale",
        "Valeria De Gennaro",
        "Livia Lestingi",
        "Vincenzo Scotti",
        "Matteo Camilli"
      ],
      "abstract": "Language is a deep-rooted means of perpetration of stereotypes and\ndiscrimination. Large Language Models (LLMs), now a pervasive technology in our\neveryday lives, can cause extensive harm when prone to generating toxic\nresponses. The standard way to address this issue is to align the LLM, which,\nhowever, dampens the issue without constituting a definitive solution.\nTherefore, testing LLM even after alignment efforts remains crucial for\ndetecting any residual deviations with respect to ethical standards. We present\nEvoTox, an automated testing framework for LLMs' inclination to toxicity,\nproviding a way to quantitatively assess how much LLMs can be pushed towards\ntoxic responses even in the presence of alignment. The framework adopts an\niterative evolution strategy that exploits the interplay between two LLMs, the\nSystem Under Test (SUT) and the Prompt Generator steering SUT responses toward\nhigher toxicity. The toxicity level is assessed by an automated oracle based on\nan existing toxicity classifier. We conduct a quantitative and qualitative\nempirical evaluation using four state-of-the-art LLMs as evaluation subjects\nhaving increasing complexity (7-13 billion parameters). Our quantitative\nevaluation assesses the cost-effectiveness of four alternative versions of\nEvoTox against existing baseline methods, based on random search, curated\ndatasets of toxic prompts, and adversarial attacks. Our qualitative assessment\nengages human evaluators to rate the fluency of the generated prompts and the\nperceived toxicity of the responses collected during the testing sessions.\nResults indicate that the effectiveness, in terms of detected toxicity level,\nis significantly higher than the selected baseline methods (effect size up to\n1.0 against random search and up to 0.99 against adversarial attacks).\nFurthermore, EvoTox yields a limited cost overhead (from 22% to 35% on\naverage).",
      "tldr_zh": "该研究探讨了大型语言模型（LLMs）的毒性问题，提出EvoTox框架，这是一种基于搜索的自动测试方法，用于评估LLMs即使经过调整（alignment）后仍可能产生的毒性响应。EvoTox采用迭代进化策略，利用两个LLMs的互动——被测试系统（SUT）和提示生成器（Prompt Generator）——来推动响应向更高毒性方向发展，并通过毒性分类器作为自动预言机进行评估。实验结果显示，在四个最先进LLMs（参数规模7-13亿）上的定量和定性评估中，EvoTox的检测有效性显著优于基线方法（如随机搜索和对抗攻击），效果大小高达1.0，且成本开销仅为22%至35%。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01741v1",
      "published_date": "2025-01-03 10:08:49 UTC",
      "updated_date": "2025-01-03 10:08:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:15:42.510935"
    },
    {
      "arxiv_id": "2501.01733v1",
      "title": "Augmentation Matters: A Mix-Paste Method for X-Ray Prohibited Item Detection under Noisy Annotations",
      "title_zh": "翻译失败",
      "authors": [
        "Ruikang Chen",
        "Yan Yan",
        "Jing-Hao Xue",
        "Yang Lu",
        "Hanzi Wang"
      ],
      "abstract": "Automatic X-ray prohibited item detection is vital for public safety.\nExisting deep learning-based methods all assume that the annotations of\ntraining X-ray images are correct. However, obtaining correct annotations is\nextremely hard if not impossible for large-scale X-ray images, where item\noverlapping is ubiquitous.As a result, X-ray images are easily contaminated\nwith noisy annotations, leading to performance deterioration of existing\nmethods.In this paper, we address the challenging problem of training a robust\nprohibited item detector under noisy annotations (including both category noise\nand bounding box noise) from a novel perspective of data augmentation, and\npropose an effective label-aware mixed patch paste augmentation method\n(Mix-Paste). Specifically, for each item patch, we mix several item patches\nwith the same category label from different images and replace the original\npatch in the image with the mixed patch. In this way, the probability of\ncontaining the correct prohibited item within the generated image is increased.\nMeanwhile, the mixing process mimics item overlapping, enabling the model to\nlearn the characteristics of X-ray images. Moreover, we design an item-based\nlarge-loss suppression (LLS) strategy to suppress the large losses\ncorresponding to potentially positive predictions of additional items due to\nthe mixing operation. We show the superiority of our method on X-ray datasets\nunder noisy annotations. In addition, we evaluate our method on the noisy\nMS-COCO dataset to showcase its generalization ability. These results clearly\nindicate the great potential of data augmentation to handle noise annotations.\nThe source code is released at https://github.com/wscds/Mix-Paste.",
      "tldr_zh": "这篇论文针对X-Ray图像中禁止物品检测的问题，提出了一种在噪声标注（包括类别噪声和边界框噪声）下进行数据增强的Mix-Paste方法。具体来说，Mix-Paste通过混合相同类别标签的物品补丁并替换原图像中的补丁，来模拟物品重叠并增加正确物品的检测概率，同时结合物品-based大型损失抑制（LLS）策略来缓解混合操作带来的额外预测误差。实验结果显示，该方法在噪声标注的X-Ray数据集上显著提升了检测性能，并在MS-COCO数据集上展示了良好的泛化能力，证明了数据增强在处理噪声标注方面的巨大潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The manuscript has been ACCEPTED for publication as a regular paper\n  in the IEEE Transactions on Information Forensics & Security",
      "pdf_url": "http://arxiv.org/pdf/2501.01733v1",
      "published_date": "2025-01-03 09:51:51 UTC",
      "updated_date": "2025-01-03 09:51:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:15:54.110801"
    },
    {
      "arxiv_id": "2501.01732v1",
      "title": "Combined Hyper-Extensible Extremely-Secured Zero-Trust CIAM-PAM architecture",
      "title_zh": "翻译失败",
      "authors": [
        "Shivom Aggarwal",
        "Shourya Mehra",
        "Safeer Sathar"
      ],
      "abstract": "Customer Identity and Access Management (CIAM) systems play a pivotal role in\nsecuring enterprise infrastructures. However, the complexity of implementing\nthese systems requires careful architectural planning to ensure positive Return\non Investment (RoI) and avoid costly delays. The proliferation of Active\nPersistent cyber threats, coupled with advancements in AI, cloud computing, and\ngeographically distributed customer populations, necessitates a paradigm shift\ntowards adaptive and zero-trust security frameworks. This paper introduces the\nCombined Hyper-Extensible Extremely-Secured Zero-Trust (CHEZ) CIAM-PAM\narchitecture, designed specifically for large-scale enterprises. The CHEZ PL\nCIAM-PAM framework addresses critical security gaps by integrating federated\nidentity management (private and public identities), password-less\nauthentication, adaptive multi-factor authentication (MFA), microservice-based\nPEP (Policy Entitlement Point), multi-layer RBAC (Role Based Access Control)\nand multi-level trust systems. This future-proof design also includes\nend-to-end data encryption, and seamless integration with state-of-the-art\nAI-based threat detection systems, while ensuring compliance with stringent\nregulatory standards.",
      "tldr_zh": "本论文介绍了Combined Hyper-Extensible Extremely-Secured Zero-Trust (CHEZ) CIAM-PAM 架构，旨在解决Customer Identity and Access Management (CIAM) 系统在大型企业中的实施复杂性问题，尤其面对活跃的网络威胁、AI 和云计算的挑战。CHEZ 框架通过整合联邦身份管理（包括私有和公共身份）、无密码认证、自适应 multi-factor authentication (MFA)、基于微服务的 Policy Entitlement Point (PEP)、多层 Role Based Access Control (RBAC) 和多级信任系统，提升了安全性和适应性。该设计还包括端到端数据加密、与 AI 威胁检测系统的无缝集成，并确保遵守严格的监管标准，从而为企业提供一个未来证明的零信任安全解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01732v1",
      "published_date": "2025-01-03 09:49:25 UTC",
      "updated_date": "2025-01-03 09:49:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:16:07.024497"
    },
    {
      "arxiv_id": "2501.01727v1",
      "title": "Proposing Hierarchical Goal-Conditioned Policy Planning in Multi-Goal Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Gavin B. Rens"
      ],
      "abstract": "Humanoid robots must master numerous tasks with sparse rewards, posing a\nchallenge for reinforcement learning (RL). We propose a method combining RL and\nautomated planning to address this. Our approach uses short goal-conditioned\npolicies (GCPs) organized hierarchically, with Monte Carlo Tree Search (MCTS)\nplanning using high-level actions (HLAs). Instead of primitive actions, the\nplanning process generates HLAs. A single plan-tree, maintained during the\nagent's lifetime, holds knowledge about goal achievement. This hierarchy\nenhances sample efficiency and speeds up reasoning by reusing HLAs and\nanticipating future actions. Our Hierarchical Goal-Conditioned Policy Planning\n(HGCPP) framework uniquely integrates GCPs, MCTS, and hierarchical RL,\npotentially improving exploration and planning in complex tasks.",
      "tldr_zh": "该论文针对强化学习（RL）在稀疏奖励环境下的人形机器人多目标任务中面临的挑战，提出了一种 Hierarchical Goal-Conditioned Policy Planning (HGCPP) 框架，将 RL 与自动规划相结合。框架使用分层组织的短目标条件策略 (GCPs) 和 Monte Carlo Tree Search (MCTS) 基于高层次动作 (HLAs) 进行规划，从而生成 HLAs 而非原始动作，并维护一个生命周期内的单一计划树来存储目标实现知识。这种方法显著提高了样本效率和推理速度，通过重用 HLAs 和预测未来动作，潜在地提升了复杂任务中的探索和规划性能。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 4 figures, this is a preprint of the peer-reviewed version\n  published by SCITEPRESS for ICAART-2025",
      "pdf_url": "http://arxiv.org/pdf/2501.01727v1",
      "published_date": "2025-01-03 09:37:54 UTC",
      "updated_date": "2025-01-03 09:37:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:16:19.655714"
    },
    {
      "arxiv_id": "2501.09025v2",
      "title": "Cyber Shadows: Neutralizing Security Threats with AI and Targeted Policy Measures",
      "title_zh": "翻译失败",
      "authors": [
        "Marc Schmitt",
        "Pantelis Koutroumpis"
      ],
      "abstract": "The digital age, driven by the AI revolution, brings significant\nopportunities but also conceals security threats, which we refer to as cyber\nshadows. These threats pose risks at individual, organizational, and societal\nlevels. This paper examines the systemic impact of these cyber threats and\nproposes a comprehensive cybersecurity strategy that integrates AI-driven\nsolutions, such as Intrusion Detection Systems (IDS), with targeted policy\ninterventions. By combining technological and regulatory measures, we create a\nmultilevel defense capable of addressing both direct threats and indirect\nnegative externalities. We emphasize that the synergy between AI-driven\nsolutions and policy interventions is essential for neutralizing cyber threats\nand mitigating their negative impact on the digital economy. Finally, we\nunderscore the need for continuous adaptation of these strategies, especially\nin response to the rapid advancement of autonomous AI-driven attacks, to ensure\nthe creation of secure and resilient digital ecosystems.",
      "tldr_zh": "本论文探讨了数字时代中隐藏的网络安全威胁（cyber shadows），这些威胁对个人、组织和社会造成系统性风险。作者提出一种综合网络安全策略，将AI驱动解决方案（如Intrusion Detection Systems (IDS)）与针对性政策干预相结合，构建多级防御机制，以应对直接威胁和间接负面外部性。研究强调，AI与政策措施的协同作用至关重要，能够中和网络威胁并减轻对数字经济的影响，同时呼吁持续适应策略以应对自主AI驱动攻击，确保数字生态系统的安全与弹性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.CR",
      "comment": "IEEE Transactions on Artificial Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2501.09025v2",
      "published_date": "2025-01-03 09:26:50 UTC",
      "updated_date": "2025-01-28 17:15:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:16:29.178839"
    },
    {
      "arxiv_id": "2501.06211v1",
      "title": "FLAME: Financial Large-Language Model Assessment and Metrics Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Jiayu Guo",
        "Yu Guo",
        "Martha Li",
        "Songtao Tan"
      ],
      "abstract": "LLMs have revolutionized NLP and demonstrated potential across diverse\ndomains. More and more financial LLMs have been introduced for finance-specific\ntasks, yet comprehensively assessing their value is still challenging. In this\npaper, we introduce FLAME, a comprehensive financial LLMs evaluation system in\nChinese, which includes two core evaluation benchmarks: FLAME-Cer and\nFLAME-Sce. FLAME-Cer covers 14 types of authoritative financial certifications,\nincluding CPA, CFA, and FRM, with a total of approximately 16,000 carefully\nselected questions. All questions have been manually reviewed to ensure\naccuracy and representativeness. FLAME-Sce consists of 10 primary core\nfinancial business scenarios, 21 secondary financial business scenarios, and a\ncomprehensive evaluation set of nearly 100 tertiary financial application\ntasks. We evaluate 6 representative LLMs, including GPT-4o, GLM-4, ERNIE-4.0,\nQwen2.5, XuanYuan3, and the latest Baichuan4-Finance, revealing\nBaichuan4-Finance excels other LLMs in most tasks. By establishing a\ncomprehensive and professional evaluation system, FLAME facilitates the\nadvancement of financial LLMs in Chinese contexts. Instructions for\nparticipating in the evaluation are available on GitHub:\nhttps://github.com/FLAME-ruc/FLAME.",
      "tldr_zh": "本文提出 FLAME，一种全面的中文金融大语言模型（LLMs）评估系统，用于评估金融特定任务的模型性能。FLAME 包括两个核心基准：FLAME-Cer，涵盖 14 种权威金融证书（如 CPA、CFA 和 FRM）约 16,000 个手动审查问题；以及 FLAME-Sce，包含 10 个主要金融业务场景、21 个次级场景和近 100 个三级应用任务。通过评估 6 个代表性 LLMs（包括 GPT-4o、GLM-4 等），研究发现 Baichuan4-Finance 在大多数任务中表现出色。该系统促进了中文语境下金融 LLMs 的发展，并提供 GitHub 参与指南。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06211v1",
      "published_date": "2025-01-03 09:17:23 UTC",
      "updated_date": "2025-01-03 09:17:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:16:43.148189"
    },
    {
      "arxiv_id": "2501.01711v1",
      "title": "LLMs & Legal Aid: Understanding Legal Needs Exhibited Through User Queries",
      "title_zh": "LLMs 与 Legal Aid：理解通过用户查询表现出的法律需求",
      "authors": [
        "Michal Kuk",
        "Jakub Harasta"
      ],
      "abstract": "The paper presents a preliminary analysis of an experiment conducted by Frank\nBold, a Czech expert group, to explore user interactions with GPT-4 for\naddressing legal queries. Between May 3, 2023, and July 25, 2023, 1,252 users\nsubmitted 3,847 queries. Unlike studies that primarily focus on the accuracy,\nfactuality, or hallucination tendencies of large language models (LLMs), our\nanalysis focuses on the user query dimension of the interaction. Using GPT-4o\nfor zero-shot classification, we categorized queries on (1) whether users\nprovided factual information about their issue (29.95%) or not (70.05%), (2)\nwhether they sought legal information (64.93%) or advice on the course of\naction (35.07\\%), and (3) whether they imposed requirements to shape or control\nthe model's answer (28.57%) or not (71.43%). We provide both quantitative and\nqualitative insight into user needs and contribute to a better understanding of\nuser engagement with LLMs.",
      "tldr_zh": "本文研究了用户通过 GPT-4 处理法律查询的互动模式，基于 Frank Bold 的实验数据，分析了 2023 年 5 月至 7 月期间 1,252 名用户提交的 3,847 个查询。不同于以往关注 LLMs 准确性或幻觉的论文，本文使用 GPT-4o 进行 zero-shot classification，从三个维度分类查询：是否提供事实信息（70.05% 未提供）、是否寻求法律信息（64.93%）或行动建议（35.07%），以及是否施加答案控制要求（28.57%）。研究提供了定量和定性洞见，深化了对用户法律需求的理解，并为优化 LLMs 在法律援助中的应用贡献了新视角。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted at AI for Access to Justice Workshop at Jurix 2024, Brno,\n  Czechia",
      "pdf_url": "http://arxiv.org/pdf/2501.01711v1",
      "published_date": "2025-01-03 09:12:35 UTC",
      "updated_date": "2025-01-03 09:12:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:16:54.393190"
    },
    {
      "arxiv_id": "2501.01709v3",
      "title": "MoVE-KD: Knowledge Distillation for VLMs with Mixture of Visual Encoders",
      "title_zh": "翻译失败",
      "authors": [
        "Jiajun Cao",
        "Yuan Zhang",
        "Tao Huang",
        "Ming Lu",
        "Qizhe Zhang",
        "Ruichuan An",
        "Ningning MA",
        "Shanghang Zhang"
      ],
      "abstract": "Visual encoders are fundamental components in vision-language models (VLMs),\neach showcasing unique strengths derived from various pre-trained visual\nfoundation models. To leverage the various capabilities of these encoders,\nrecent studies incorporate multiple encoders within a single VLM, leading to a\nconsiderable increase in computational cost. In this paper, we present\nMixture-of-Visual-Encoder Knowledge Distillation (MoVE-KD), a novel framework\nthat distills the unique proficiencies of multiple vision encoders into a\nsingle, efficient encoder model. Specifically, to mitigate conflicts and retain\nthe unique characteristics of each teacher encoder, we employ low-rank\nadaptation (LoRA) and mixture-of-experts (MoEs) to selectively activate\nspecialized knowledge based on input features, enhancing both adaptability and\nefficiency. To regularize the KD process and enhance performance, we propose an\nattention-based distillation strategy that adaptively weighs the different\nencoders and emphasizes valuable visual tokens, reducing the burden of\nreplicating comprehensive but distinct features from multiple teachers.\nComprehensive experiments on popular VLMs, such as LLaVA and LLaVA-NeXT,\nvalidate the effectiveness of our method. Our code is available at:\nhttps://github.com/hey-cjj/MoVE-KD.",
      "tldr_zh": "论文提出 MoVE-KD 框架，用于将多个视觉编码器的独特能力通过 Knowledge Distillation 蒸馏到一个高效的单编码器模型中，解决整合多编码器导致的计算成本增加问题。框架采用 LoRA 和 Mixture-of-Experts (MoEs) 技术，基于输入特征选择性地激活专业知识，并引入基于注意力的蒸馏策略来权衡不同编码器并强调有价值的视觉标记，从而提高适应性和性能。在 LLaVA 和 LLaVA-NeXT 等 VLMs 上进行的实验证明了 MoVE-KD 的有效性，并提供了开源代码支持。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.01709v3",
      "published_date": "2025-01-03 09:10:34 UTC",
      "updated_date": "2025-03-18 07:34:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:17:06.296107"
    },
    {
      "arxiv_id": "2501.01705v2",
      "title": "The Essence of Contextual Understanding in Theory of Mind: A Study on Question Answering with Story Characters",
      "title_zh": "翻译失败",
      "authors": [
        "Chulun Zhou",
        "Qiujing Wang",
        "Mo Yu",
        "Xiaoqian Yue",
        "Rui Lu",
        "Jiangnan Li",
        "Yifan Zhou",
        "Shunchi Zhang",
        "Jie Zhou",
        "Wai Lam"
      ],
      "abstract": "Theory-of-Mind (ToM) is a fundamental psychological capability that allows\nhumans to understand and interpret the mental states of others. Humans infer\nothers' thoughts by integrating causal cues and indirect clues from broad\ncontextual information, often derived from past interactions. In other words,\nhuman ToM heavily relies on the understanding about the backgrounds and life\nstories of others. Unfortunately, this aspect is largely overlooked in existing\nbenchmarks for evaluating machines' ToM capabilities, due to their usage of\nshort narratives without global context, especially personal background of\ncharacters. In this paper, we verify the importance of comprehensive contextual\nunderstanding about personal backgrounds in ToM and assess the performance of\nLLMs in such complex scenarios. To achieve this, we introduce CharToM\nbenchmark, comprising 1,035 ToM questions based on characters from classic\nnovels. Our human study reveals a significant disparity in performance: the\nsame group of educated participants performs dramatically better when they have\nread the novels compared to when they have not. In parallel, our experiments on\nstate-of-the-art LLMs, including the very recent o1 and DeepSeek-R1 models,\nshow that LLMs still perform notably worse than humans, despite that they have\nseen these stories during pre-training. This highlights the limitations of\ncurrent LLMs in capturing the nuanced contextual information required for ToM\nreasoning.",
      "tldr_zh": "本研究探讨了Theory-of-Mind (ToM)中上下文理解的重要性，强调人类通过整合因果线索和个人背景信息来推断他人心理状态，而现有基准因缺乏全局上下文而忽略此方面。论文引入CharToM基准，该数据集包含1035个基于经典小说人物的ToM问题，并通过人类实验发现，阅读过小说的参与者表现远优于未阅读者。实验结果显示，最新LLMs如o1和DeepSeek-R1尽管在预训练中接触过这些故事，但仍显著逊于人类，这突显了当前LLMs在捕捉ToM推理所需细微上下文信息方面的局限性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.01705v2",
      "published_date": "2025-01-03 09:04:45 UTC",
      "updated_date": "2025-04-09 08:36:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:17:17.873760"
    },
    {
      "arxiv_id": "2501.02032v1",
      "title": "Dynamic Feature Fusion: Combining Global Graph Structures and Local Semantics for Blockchain Fraud Detection",
      "title_zh": "动态特征融合：结合全局图结构和局部语义用于区块链欺诈检测",
      "authors": [
        "Zhang Sheng",
        "Liangliang Song",
        "Yanbin Wang"
      ],
      "abstract": "The advent of blockchain technology has facilitated the widespread adoption\nof smart contracts in the financial sector. However, current fraud detection\nmethodologies exhibit limitations in capturing both global structural patterns\nwithin transaction networks and local semantic relationships embedded in\ntransaction data. Most existing models focus on either structural information\nor semantic features individually, leading to suboptimal performance in\ndetecting complex fraud patterns.In this paper, we propose a dynamic feature\nfusion model that combines graph-based representation learning and semantic\nfeature extraction for blockchain fraud detection. Specifically, we construct\nglobal graph representations to model account relationships and extract local\ncontextual features from transaction data. A dynamic multimodal fusion\nmechanism is introduced to adaptively integrate these features, enabling the\nmodel to capture both structural and semantic fraud patterns effectively. We\nfurther develop a comprehensive data processing pipeline, including graph\nconstruction, temporal feature enhancement, and text preprocessing.\nExperimental results on large-scale real-world blockchain datasets demonstrate\nthat our method outperforms existing benchmarks across accuracy, F1 score, and\nrecall metrics. This work highlights the importance of integrating structural\nrelationships and semantic similarities for robust fraud detection and offers a\nscalable solution for securing blockchain systems.",
      "tldr_zh": "本研究针对区块链欺诈检测的局限性，提出了一种动态特征融合模型（Dynamic Feature Fusion），它将全局图结构（global graph structures）与本地语义（local semantics）相结合，以捕捉复杂的欺诈模式。具体而言，该模型通过图-based representation learning建模账户关系，从交易数据中提取本地上下文特征，并引入动态多模态融合机制（dynamic multimodal fusion）来适应性地整合这些特征，同时开发了包括图构建、temporal feature enhancement和文本预处理的全面数据处理管道。在大规模真实区块链数据集上的实验显示，该方法在准确率、F1 score和recall指标上均优于现有基准模型。该工作强调了整合结构关系和语义相似性的重要性，为区块链系统的安全提供了一个可扩展的解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02032v1",
      "published_date": "2025-01-03 09:04:43 UTC",
      "updated_date": "2025-01-03 09:04:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:17:30.430480"
    },
    {
      "arxiv_id": "2501.01702v2",
      "title": "AgentRefine: Enhancing Agent Generalization through Refinement Tuning",
      "title_zh": "AgentRefine：通过精炼调优增强智能体泛化",
      "authors": [
        "Dayuan Fu",
        "Keqing He",
        "Yejie Wang",
        "Wentao Hong",
        "Zhuoma Gongque",
        "Weihao Zeng",
        "Wei Wang",
        "Jingang Wang",
        "Xunliang Cai",
        "Weiran Xu"
      ],
      "abstract": "Large Language Model (LLM) based agents have proved their ability to perform\ncomplex tasks like humans. However, there is still a large gap between\nopen-sourced LLMs and commercial models like the GPT series. In this paper, we\nfocus on improving the agent generalization capabilities of LLMs via\ninstruction tuning. We first observe that the existing agent training corpus\nexhibits satisfactory results on held-in evaluation sets but fails to\ngeneralize to held-out sets. These agent-tuning works face severe formatting\nerrors and are frequently stuck in the same mistake for a long while. We\nanalyze that the poor generalization ability comes from overfitting to several\nmanual agent environments and a lack of adaptation to new situations. They\nstruggle with the wrong action steps and can not learn from the experience but\njust memorize existing observation-action relations. Inspired by the insight,\nwe propose a novel AgentRefine framework for agent-tuning. The core idea is to\nenable the model to learn to correct its mistakes via observation in the\ntrajectory. Specifically, we propose an agent synthesis framework to encompass\na diverse array of environments and tasks and prompt a strong LLM to refine its\nerror action according to the environment feedback. AgentRefine significantly\noutperforms state-of-the-art agent-tuning work in terms of generalization\nability on diverse agent tasks. It also has better robustness facing\nperturbation and can generate diversified thought in inference. Our findings\nestablish the correlation between agent generalization and self-refinement and\nprovide a new paradigm for future research.",
      "tldr_zh": "该论文探讨了如何通过指令微调(instruction tuning)提升基于Large Language Model (LLM)的代理的泛化能力，指出现有方法因过拟合特定环境而无法适应新任务和从经验中学习。研究提出AgentRefine框架，该框架利用代理合成框架合成多样环境和任务，并通过提示强LLM根据环境反馈修正错误动作，实现自我精炼。实验结果显示，AgentRefine在泛化能力、鲁棒性和生成多样化思考方面显著优于现有方法，并建立了代理泛化与自我精炼的相关性，为未来代理训练提供新范式。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.01702v2",
      "published_date": "2025-01-03 08:55:19 UTC",
      "updated_date": "2025-02-24 12:42:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:17:41.856947"
    },
    {
      "arxiv_id": "2501.02031v1",
      "title": "CarbonChat: Large Language Model-Based Corporate Carbon Emission Analysis and Climate Knowledge Q&A System",
      "title_zh": "CarbonChat: 基于",
      "authors": [
        "Zhixuan Cao",
        "Ming Han",
        "Jingtao Wang",
        "Meng Jia"
      ],
      "abstract": "As the impact of global climate change intensifies, corporate carbon\nemissions have become a focal point of global attention. In response to issues\nsuch as the lag in climate change knowledge updates within large language\nmodels, the lack of specialization and accuracy in traditional augmented\ngeneration architectures for complex problems, and the high cost and time\nconsumption of sustainability report analysis, this paper proposes CarbonChat:\nLarge Language Model-based corporate carbon emission analysis and climate\nknowledge Q&A system, aimed at achieving precise carbon emission analysis and\npolicy understanding.First, a diversified index module construction method is\nproposed to handle the segmentation of rule-based and long-text documents, as\nwell as the extraction of structured data, thereby optimizing the parsing of\nkey information.Second, an enhanced self-prompt retrieval-augmented generation\narchitecture is designed, integrating intent recognition, structured reasoning\nchains, hybrid retrieval, and Text2SQL, improving the efficiency of semantic\nunderstanding and query conversion.Next, based on the greenhouse gas accounting\nframework, 14 dimensions are established for carbon emission analysis, enabling\nreport summarization, relevance evaluation, and customized responses.Finally,\nthrough a multi-layer chunking mechanism, timestamps, and hallucination\ndetection features, the accuracy and verifiability of the analysis results are\nensured, reducing hallucination rates and enhancing the precision of the\nresponses.",
      "tldr_zh": "这篇论文提出了 CarbonChat 系统，这是一个基于 Large Language Model (LLM) 的企业碳排放分析和气候知识问答平台，旨在解决 LLM 气候知识更新滞后、传统增强生成架构的专业性不足以及可持续报告分析的高成本问题。系统采用多样化索引模块处理规则-based 和长文本文档的分段与结构化数据提取，并设计了增强的自提示 Retrieval-Augmented Generation (RAG) 架构，整合意图识别、结构化推理链、混合检索和 Text2SQL 以提升语义理解和查询转换效率。基于温室气体会计框架，该系统建立了14个维度的碳排放分析模块，支持报告总结、相关性评估和定制化响应，并通过多层分块机制、时间戳和 hallucination 检测功能，确保分析结果的准确性、可验证性和低幻觉率。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T07, 91B06",
        "I.2.1"
      ],
      "primary_category": "cs.CL",
      "comment": "26 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.02031v1",
      "published_date": "2025-01-03 08:45:38 UTC",
      "updated_date": "2025-01-03 08:45:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:17:54.450765"
    },
    {
      "arxiv_id": "2501.01691v2",
      "title": "VidFormer: A novel end-to-end framework fused by 3DCNN and Transformer for Video-based Remote Physiological Measurement",
      "title_zh": "翻译失败",
      "authors": [
        "Jiachen Li",
        "Shisheng Guo",
        "Longzhen Tang",
        "Cuolong Cui",
        "Lingjiang Kong",
        "Xiaobo Yang"
      ],
      "abstract": "Remote physiological signal measurement based on facial videos, also known as\nremote photoplethysmography (rPPG), involves predicting changes in facial\nvascular blood flow from facial videos. While most deep learning-based methods\nhave achieved good results, they often struggle to balance performance across\nsmall and large-scale datasets due to the inherent limitations of convolutional\nneural networks (CNNs) and Transformer. In this paper, we introduce VidFormer,\na novel end-to-end framework that integrates 3-Dimension Convolutional Neural\nNetwork (3DCNN) and Transformer models for rPPG tasks. Initially, we conduct an\nanalysis of the traditional skin reflection model and subsequently introduce an\nenhanced model for the reconstruction of rPPG signals. Based on this improved\nmodel, VidFormer utilizes 3DCNN and Transformer to extract local and global\nfeatures from input data, respectively. To enhance the spatiotemporal feature\nextraction capabilities of VidFormer, we incorporate temporal-spatial attention\nmechanisms tailored for both 3DCNN and Transformer. Additionally, we design a\nmodule to facilitate information exchange and fusion between the 3DCNN and\nTransformer. Our evaluation on five publicly available datasets demonstrates\nthat VidFormer outperforms current state-of-the-art (SOTA) methods. Finally, we\ndiscuss the essential roles of each VidFormer module and examine the effects of\nethnicity, makeup, and exercise on its performance.",
      "tldr_zh": "这篇论文提出了 VidFormer，一种新型端到端框架，将 3DCNN 和 Transformer 融合，用于基于面部视频的远程光电容积描记术 (rPPG)，以预测面部血管血流变化并解决现有方法在不同规模数据集上性能不平衡的问题。VidFormer 基于改进的皮肤反射模型，利用 3DCNN 提取局部特征、Transformer 提取全局特征，并融入时空注意力机制以及信息交换模块来增强时空特征提取。实验结果显示，该框架在五个公开数据集上超过了现有最先进 (SOTA) 方法，并探讨了种族、化妆和运动对性能的影响。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01691v2",
      "published_date": "2025-01-03 08:18:08 UTC",
      "updated_date": "2025-01-07 02:57:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:18:06.484602"
    },
    {
      "arxiv_id": "2501.03182v1",
      "title": "Boosting Explainability through Selective Rationalization in Pre-trained Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Libing Yuan",
        "Shuaibo Hu",
        "Kui Yu",
        "Le Wu"
      ],
      "abstract": "The widespread application of pre-trained language models (PLMs) in natural\nlanguage processing (NLP) has led to increasing concerns about their\nexplainability. Selective rationalization is a self-explanatory framework that\nselects human-intelligible input subsets as rationales for predictions. Recent\nstudies have shown that applying existing rationalization frameworks to PLMs\nwill result in severe degeneration and failure problems, producing sub-optimal\nor meaningless rationales. Such failures severely damage trust in\nrationalization methods and constrain the application of rationalization\ntechniques on PLMs. In this paper, we find that the homogeneity of tokens in\nthe sentences produced by PLMs is the primary contributor to these problems. To\naddress these challenges, we propose a method named Pre-trained Language\nModel's Rationalization (PLMR), which splits PLMs into a generator and a\npredictor to deal with NLP tasks while providing interpretable rationales. The\ngenerator in PLMR also alleviates homogeneity by pruning irrelevant tokens,\nwhile the predictor uses full-text information to standardize predictions.\nExperiments conducted on two widely used datasets across multiple PLMs\ndemonstrate the effectiveness of the proposed method PLMR in addressing the\nchallenge of applying selective rationalization to PLMs. Codes:\nhttps://github.com/ylb777/PLMR.",
      "tldr_zh": "这篇论文针对预训练语言模型 (PLMs) 在自然语言处理 (NLP) 中的可解释性问题，指出现有 selective rationalization 框架应用时会因 token 同质性导致退化和失败，产生次优或无意义的理由。作者提出 PLMR 方法，将 PLMs 分解为 generator 和 predictor：generator 通过修剪无关 token 缓解同质性，而 predictor 使用全文信息进行标准化预测。该方法在两个常用数据集上的实验证明了其有效性，显著提升了 selective rationalization 在 PLMs 上的性能，并提供了开源代码。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "KDD 2025 research track",
      "pdf_url": "http://arxiv.org/pdf/2501.03182v1",
      "published_date": "2025-01-03 07:52:40 UTC",
      "updated_date": "2025-01-03 07:52:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:18:19.453586"
    },
    {
      "arxiv_id": "2501.01679v1",
      "title": "Adaptive Few-shot Prompting for Machine Translation with Pre-trained Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Lei Tang",
        "Jinghui Qin",
        "Wenxuan Ye",
        "Hao Tan",
        "Zhijing Yang"
      ],
      "abstract": "Recently, Large language models (LLMs) with in-context learning have\ndemonstrated remarkable potential in handling neural machine translation.\nHowever, existing evidence shows that LLMs are prompt-sensitive and it is\nsub-optimal to apply the fixed prompt to any input for downstream machine\ntranslation tasks. To address this issue, we propose an adaptive few-shot\nprompting (AFSP) framework to automatically select suitable translation\ndemonstrations for various source input sentences to further elicit the\ntranslation capability of an LLM for better machine translation. First, we\nbuild a translation demonstration retrieval module based on LLM's embedding to\nretrieve top-k semantic-similar translation demonstrations from aligned\nparallel translation corpus. Rather than using other embedding models for\nsemantic demonstration retrieval, we build a hybrid demonstration retrieval\nmodule based on the embedding layer of the deployed LLM to build better input\nrepresentation for retrieving more semantic-related translation demonstrations.\nThen, to ensure better semantic consistency between source inputs and target\noutputs, we force the deployed LLM itself to generate multiple output\ncandidates in the target language with the help of translation demonstrations\nand rerank these candidates. Besides, to better evaluate the effectiveness of\nour AFSP framework on the latest language and extend the research boundary of\nneural machine translation, we construct a high-quality diplomatic\nChinese-English parallel dataset that consists of 5,528 parallel\nChinese-English sentences. Finally, extensive experiments on the proposed\ndiplomatic Chinese-English parallel dataset and the United Nations Parallel\nCorpus (Chinese-English part) show the effectiveness and superiority of our\nproposed AFSP.",
      "tldr_zh": "本文提出 Adaptive Few-shot Prompting (AFSP) 框架，用于优化预训练语言模型 (LLMs) 在机器翻译中的性能，通过自动从平行语料库中检索语义相似的翻译演示，并利用 LLM 的嵌入层构建混合检索模块，以更好地匹配源输入。AFSP 还包括生成多个输出候选并重新排序，以确保目标输出语义一致性。研究者构建了一个高质量的外交中文-英语平行数据集（包含 5,528 对句子），并在该数据集和联合国平行语料库上进行的实验证明，AFSP 框架显著提升了翻译效果，扩展了神经机器翻译的研究边界。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "published to AAAI2025",
      "pdf_url": "http://arxiv.org/pdf/2501.01679v1",
      "published_date": "2025-01-03 07:47:59 UTC",
      "updated_date": "2025-01-03 07:47:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:18:30.144971"
    },
    {
      "arxiv_id": "2501.02030v1",
      "title": "Detecting Music Performance Errors with Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Benjamin Shiue-Hal Chou",
        "Purvish Jajal",
        "Nicholas John Eliopoulos",
        "Tim Nadolsky",
        "Cheng-Yun Yang",
        "Nikita Ravi",
        "James C. Davis",
        "Kristen Yeon-Ji Yun",
        "Yung-Hsiang Lu"
      ],
      "abstract": "Beginner musicians often struggle to identify specific errors in their\nperformances, such as playing incorrect notes or rhythms. There are two\nlimitations in existing tools for music error detection: (1) Existing\napproaches rely on automatic alignment; therefore, they are prone to errors\ncaused by small deviations between alignment targets.; (2) There is a lack of\nsufficient data to train music error detection models, resulting in\nover-reliance on heuristics. To address (1), we propose a novel transformer\nmodel, Polytune, that takes audio inputs and outputs annotated music scores.\nThis model can be trained end-to-end to implicitly align and compare\nperformance audio with music scores through latent space representations. To\naddress (2), we present a novel data generation technique capable of creating\nlarge-scale synthetic music error datasets. Our approach achieves a 64.1%\naverage Error Detection F1 score, improving upon prior work by 40 percentage\npoints across 14 instruments. Additionally, compared with existing\ntranscription methods repurposed for music error detection, our model can\nhandle multiple instruments. Our source code and datasets are available at\nhttps://github.com/ben2002chou/Polytune.",
      "tldr_zh": "本研究针对初学者音乐家难以识别表演错误（如错误的音符或节奏）的问题，指出现有工具依赖自动对齐易出错且数据不足。作者提出Polytune，一个基于Transformer的模型，能够端到端训练，通过潜在空间表示隐式对齐和比较音频与音乐谱面，并开发了一种生成大规模合成音乐错误数据集的技术。实验结果显示，Polytune在14种乐器上实现了64.1%的平均错误检测F1 score，比现有方法提高了40个百分点，并支持多乐器处理。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.02030v1",
      "published_date": "2025-01-03 07:04:20 UTC",
      "updated_date": "2025-01-03 07:04:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:18:42.422684"
    },
    {
      "arxiv_id": "2501.02029v1",
      "title": "Spot Risks Before Speaking! Unraveling Safety Attention Heads in Large Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ziwei Zheng",
        "Junyao Zhao",
        "Le Yang",
        "Lijun He",
        "Fan Li"
      ],
      "abstract": "With the integration of an additional modality, large vision-language models\n(LVLMs) exhibit greater vulnerability to safety risks (e.g., jailbreaking)\ncompared to their language-only predecessors. Although recent studies have\ndevoted considerable effort to the post-hoc alignment of LVLMs, the inner\nsafety mechanisms remain largely unexplored. In this paper, we discover that\ninternal activations of LVLMs during the first token generation can effectively\nidentify malicious prompts across different attacks. This inherent safety\nperception is governed by sparse attention heads, which we term ``safety\nheads.\" Further analysis reveals that these heads act as specialized shields\nagainst malicious prompts; ablating them leads to higher attack success rates,\nwhile the model's utility remains unaffected. By locating these safety heads\nand concatenating their activations, we construct a straightforward but\npowerful malicious prompt detector that integrates seamlessly into the\ngeneration process with minimal extra inference overhead. Despite its simple\nstructure of a logistic regression model, the detector surprisingly exhibits\nstrong zero-shot generalization capabilities. Experiments across various\nprompt-based attacks confirm the effectiveness of leveraging safety heads to\nprotect LVLMs. Code is available at \\url{https://github.com/Ziwei-Zheng/SAHs}.",
      "tldr_zh": "该研究揭示了大型视觉语言模型(LVLMs)比纯语言模型更容易遭受安全风险，如越狱攻击，并通过分析发现，LVLMs在生成第一个token时的内部激活能有效识别恶意提示，这些激活主要由稀疏的“safety heads”（安全注意力头）控制。作者证明，移除这些safety heads会显著提高攻击成功率，而不影响模型的整体实用性；因此，他们构建了一个简单有效的恶意提示检测器，通过定位和连接safety heads的激活，并使用逻辑回归模型实现无缝集成。实验结果显示，该检测器在各种提示攻击中表现出色，具有强大的零样本泛化能力，从而提升了LVLMs的安全性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02029v1",
      "published_date": "2025-01-03 07:01:15 UTC",
      "updated_date": "2025-01-03 07:01:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:18:53.995879"
    },
    {
      "arxiv_id": "2501.01664v1",
      "title": "BARTPredict: Empowering IoT Security with LLM-Driven Cyber Threat Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Alaeddine Diaf",
        "Abdelaziz Amara Korba",
        "Nour Elislem Karabadji",
        "Yacine Ghamri-Doudane"
      ],
      "abstract": "The integration of Internet of Things (IoT) technology in various domains has\nled to operational advancements, but it has also introduced new vulnerabilities\nto cybersecurity threats, as evidenced by recent widespread cyberattacks on IoT\ndevices. Intrusion detection systems are often reactive, triggered by specific\npatterns or anomalies observed within the network. To address this challenge,\nthis work proposes a proactive approach to anticipate and preemptively mitigate\nmalicious activities, aiming to prevent potential damage before it occurs. This\npaper proposes an innovative intrusion prediction framework empowered by\nPre-trained Large Language Models (LLMs). The framework incorporates two LLMs:\na fine-tuned Bidirectional and AutoRegressive Transformers (BART) model for\npredicting network traffic and a fine-tuned Bidirectional Encoder\nRepresentations from Transformers (BERT) model for evaluating the predicted\ntraffic. By harnessing the bidirectional capabilities of BART the framework\nthen identifies malicious packets among these predictions. Evaluated using the\nCICIoT2023 IoT attack dataset, our framework showcases a notable enhancement in\npredictive performance, attaining an impressive 98% overall accuracy, providing\na powerful response to the cybersecurity challenges that confront IoT networks.",
      "tldr_zh": "这篇论文提出 BARTPredict 框架，利用大型语言模型 (LLMs) 驱动的网络流量预测方法，主动应对 IoT 设备的网络安全威胁，以预防恶意活动。框架结合 fine-tuned 的 BART 模型预测网络流量，以及 fine-tuned 的 BERT 模型评估预测结果，从而利用 BART 的双向能力识别恶意数据包。在 CICIoT2023 IoT 攻击数据集上评估，该框架实现了 98% 的整体准确率，显著提升了 IoT 网络的安全防护性能。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01664v1",
      "published_date": "2025-01-03 06:37:39 UTC",
      "updated_date": "2025-01-03 06:37:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:19:05.840755"
    },
    {
      "arxiv_id": "2501.01658v1",
      "title": "EAUWSeg: Eliminating annotation uncertainty in weakly-supervised medical image segmentation",
      "title_zh": "EAUWSeg：消除弱监督医学图像分割中的标注不确定性",
      "authors": [
        "Wang Lituan",
        "Zhang Lei",
        "Wang Yan",
        "Wang Zhenbin",
        "Zhang Zhenwei",
        "Zhang Yi"
      ],
      "abstract": "Weakly-supervised medical image segmentation is gaining traction as it\nrequires only rough annotations rather than accurate pixel-to-pixel labels,\nthereby reducing the workload for specialists. Although some progress has been\nmade, there is still a considerable performance gap between the label-efficient\nmethods and fully-supervised one, which can be attributed to the uncertainty\nnature of these weak labels. To address this issue, we propose a novel weak\nannotation method coupled with its learning framework EAUWSeg to eliminate the\nannotation uncertainty. Specifically, we first propose the Bounded Polygon\nAnnotation (BPAnno) by simply labeling two polygons for a lesion. Then, the\ntailored learning mechanism that explicitly treat bounded polygons as two\nseparated annotations is proposed to learn invariant feature by providing\nadversarial supervision signal for model training. Subsequently, a\nconfidence-auxiliary consistency learner incorporates with a\nclassification-guided confidence generator is designed to provide reliable\nsupervision signal for pixels in uncertain region by leveraging the feature\npresentation consistency across pixels within the same category as well as\nclass-specific information encapsulated in bounded polygons annotation.\nExperimental results demonstrate that EAUWSeg outperforms existing\nweakly-supervised segmentation methods. Furthermore, compared to\nfully-supervised counterparts, the proposed method not only delivers superior\nperformance but also costs much less annotation workload. This underscores the\nsuperiority and effectiveness of our approach.",
      "tldr_zh": "这篇论文针对弱监督医学图像分割中的标注不确定性问题，提出了一种新框架EAUWSeg，以减少专家标注工作量。具体方法包括Bounded Polygon Annotation (BPAnno)，即为病变标注两个多边形，并通过对抗监督信号学习不变特征，以及confidence-auxiliary consistency learner和classification-guided confidence generator来为不确定区域提供可靠监督。实验结果表明，EAUWSeg在性能上优于现有弱监督分割方法，甚至超越全监督方法，同时显著降低标注负担。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01658v1",
      "published_date": "2025-01-03 06:21:02 UTC",
      "updated_date": "2025-01-03 06:21:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:19:18.123232"
    },
    {
      "arxiv_id": "2501.01649v1",
      "title": "AVATAR: Adversarial Autoencoders with Autoregressive Refinement for Time Series Generation",
      "title_zh": "翻译失败",
      "authors": [
        "MohammadReza EskandariNasab",
        "Shah Muhammad Hamdi",
        "Soukaina Filali Boubrahimi"
      ],
      "abstract": "Data augmentation can significantly enhance the performance of machine\nlearning tasks by addressing data scarcity and improving generalization.\nHowever, generating time series data presents unique challenges. A model must\nnot only learn a probability distribution that reflects the real data\ndistribution but also capture the conditional distribution at each time step to\npreserve the inherent temporal dependencies. To address these challenges, we\nintroduce AVATAR, a framework that combines Adversarial Autoencoders (AAE) with\nAutoregressive Learning to achieve both objectives. Specifically, our technique\nintegrates the autoencoder with a supervisor and introduces a novel supervised\nloss to assist the decoder in learning the temporal dynamics of time series\ndata. Additionally, we propose another innovative loss function, termed\ndistribution loss, to guide the encoder in more efficiently aligning the\naggregated posterior of the autoencoder's latent representation with a prior\nGaussian distribution. Furthermore, our framework employs a joint training\nmechanism to simultaneously train all networks using a combined loss, thereby\nfulfilling the dual objectives of time series generation. We evaluate our\ntechnique across a variety of time series datasets with diverse\ncharacteristics. Our experiments demonstrate significant improvements in both\nthe quality and practical utility of the generated data, as assessed by various\nqualitative and quantitative metrics.",
      "tldr_zh": "该研究提出 AVATAR 框架，将 Adversarial Autoencoders (AAE) 与 Autoregressive Learning 相结合，用于时间序列数据生成，以解决数据稀缺性和时间依赖性挑战。框架引入 supervised loss 来帮助 decoder 学习时间序列的动态，并提出 distribution loss 指导 encoder 使潜在表示的后验分布更好地与 Gaussian prior 对齐，同时采用联合训练机制优化所有网络。实验结果显示，在多种时间序列数据集上，AVATAR 生成的数据在质量和实用性方面均有显著提升，通过定性和定量指标得到验证。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This work has been accepted to the SDM 2025 on December 20, 2024",
      "pdf_url": "http://arxiv.org/pdf/2501.01649v1",
      "published_date": "2025-01-03 05:44:13 UTC",
      "updated_date": "2025-01-03 05:44:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:19:29.190028"
    },
    {
      "arxiv_id": "2501.01645v3",
      "title": "HLV-1K: A Large-scale Hour-Long Video Benchmark for Time-Specific Long Video Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Heqing Zou",
        "Tianze Luo",
        "Guiyang Xie",
        "Victor Xiao Jie Zhang",
        "Fengmao Lv",
        "Guangcong Wang",
        "Junyang Chen",
        "Zhuochen Wang",
        "Hansheng Zhang",
        "Huaijian Zhang"
      ],
      "abstract": "Multimodal large language models have become a popular topic in deep visual\nunderstanding due to many promising real-world applications. However, hour-long\nvideo understanding, spanning over one hour and containing tens of thousands of\nvisual frames, remains under-explored because of 1) challenging long-term video\nanalyses, 2) inefficient large-model approaches, and 3) lack of large-scale\nbenchmark datasets. Among them, in this paper, we focus on building a\nlarge-scale hour-long long video benchmark, HLV-1K, designed to evaluate long\nvideo understanding models. HLV-1K comprises 1009 hour-long videos with 14,847\nhigh-quality question answering (QA) and multi-choice question asnwering (MCQA)\npairs with time-aware query and diverse annotations, covering frame-level,\nwithin-event-level, cross-event-level, and long-term reasoning tasks. We\nevaluate our benchmark using existing state-of-the-art methods and demonstrate\nits value for testing deep long video understanding capabilities at different\nlevels and for various tasks. This includes promoting future long video\nunderstanding tasks at a granular level, such as deep understanding of long\nlive videos, meeting recordings, and movies.",
      "tldr_zh": "本研究引入了HLV-1K，一个大规模的时长视频基准数据集，旨在评估时间特定长视频理解模型。该数据集包含1009个小时级视频和14,847对高质量的问答(QA)和多选问答(MCQA)对，支持时间感知查询，并覆盖帧级、事件内级、跨事件级和长期推理任务。通过评估现有最先进方法，HLV-1K证明了其在测试深度长视频理解能力方面的价值，并有望推动未来应用，如直播视频、会议记录和电影的细粒度分析。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICME 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.01645v3",
      "published_date": "2025-01-03 05:32:37 UTC",
      "updated_date": "2025-05-13 06:38:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:19:41.412744"
    },
    {
      "arxiv_id": "2501.01639v2",
      "title": "Implications of Artificial Intelligence on Health Data Privacy and Confidentiality",
      "title_zh": "人工智能对健康数据隐私和保密性的影响",
      "authors": [
        "Ahmad Momani"
      ],
      "abstract": "The rapid integration of artificial intelligence (AI) in healthcare is\nrevolutionizing medical diagnostics, personalized medicine, and operational\nefficiency. However, alongside these advancements, significant challenges arise\nconcerning patient data privacy, ethical considerations, and regulatory\ncompliance. This paper examines the dual impact of AI on healthcare,\nhighlighting its transformative potential and the critical need for\nsafeguarding sensitive health information. It explores the role of the Health\nInsurance Portability and Accountability Act (HIPAA) as a regulatory framework\nfor ensuring data privacy and security, emphasizing the importance of robust\nsafeguards and ethical standards in AI-driven healthcare. Through case studies,\nincluding AI applications in diabetic retinopathy, oncology, and the\ncontroversies surrounding data sharing, this study underscores the ethical and\nlegal complexities of AI implementation. A balanced approach that fosters\ninnovation while maintaining patient trust and privacy is imperative. The\nfindings emphasize the importance of continuous education, transparency, and\nadherence to regulatory frameworks to harness AI's full potential responsibly\nand ethically in healthcare.",
      "tldr_zh": "该论文探讨了人工智能（AI）在医疗领域的应用对健康数据隐私和保密性的双重影响，既突显了AI在诊断、个性化医疗和效率提升方面的革命性潜力，也强调了由此带来的隐私风险、伦理问题和监管挑战。作者通过分析Health Insurance Portability and Accountability Act (HIPAA)框架以及案例研究（如糖尿病视网膜病变、肿瘤学应用和数据共享争议），揭示了AI实施中的法律和道德复杂性。研究发现，需要采用平衡策略，包括加强教育、提升透明度并严格遵守监管标准，以确保AI在医疗中的创新发展同时维护患者信任和数据安全。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01639v2",
      "published_date": "2025-01-03 05:17:23 UTC",
      "updated_date": "2025-01-06 18:52:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:19:52.735903"
    },
    {
      "arxiv_id": "2501.01638v2",
      "title": "A non-ergodic framework for understanding emergent capabilities in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Javier Marín"
      ],
      "abstract": "Large language models have emergent capabilities that come unexpectedly at\nscale, but we need a theoretical framework to explain why and how they emerge.\nWe prove that language models are actually non-ergodic systems while providing\na mathematical framework based on Stuart Kauffman's theory of the adjacent\npossible (TAP) to explain capability emergence. Our resource-constrained TAP\nequation demonstrates how architectural, training, and contextual constraints\ninteract to shape model capabilities through phase transitions in semantic\nspace. We prove through experiments with three different language models that\ncapacities emerge through discrete transitions guided by constraint\ninteractions and path-dependent exploration. This framework provides a\ntheoretical basis for understanding emergence in language models and guides the\ndevelopment of architectures that can guide capability emergence.",
      "tldr_zh": "本文提出一个非-ergodic 框架，基于 Stuart Kauffman 的相邻可能理论（TAP），来解释大型语言模型（Large Language Models）中紧急能力的出现机制。框架通过资源约束的 TAP 方程，展示了架构、训练和上下文约束如何通过语义空间中的相变（phase transitions）塑造模型能力。实验验证了在三个不同语言模型上，能力通过约束互动和路径依赖探索的离散转变（discrete transitions）出现，为理解和指导语言模型架构开发提供了理论基础。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01638v2",
      "published_date": "2025-01-03 05:11:41 UTC",
      "updated_date": "2025-02-28 08:07:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:20:05.753138"
    },
    {
      "arxiv_id": "2501.01625v1",
      "title": "ICPC: In-context Prompt Compression with Faster Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyang Yu",
        "Yuyu Liu"
      ],
      "abstract": "Despite the recent success of Large Language Models (LLMs), it remains\nchallenging to feed LLMs with long prompts due to the fixed size of LLM inputs.\nAs a remedy, prompt compression becomes a promising solution by removing\nredundant tokens in the prompt. However, using LLM in the existing works\nrequires additional computation resources and leads to memory overheads. To\naddress it, we propose ICPC (In-context Prompt Compression), a novel and\nscalable prompt compression method that adaptively reduces the prompt length.\nThe key idea of ICPC is to calculate the probability of each word appearing in\nthe prompt using encoders and calculate information carried by each word\nthrough the information function, which effectively reduces the information\nloss during prompt compression and increases the speed of compression.\nEmpirically, we demonstrate that ICPC can effectively compress long texts of\ndifferent categories and thus achieve better performance and speed on different\ntypes of NLP tasks.",
      "tldr_zh": "该论文提出 ICPC（In-context Prompt Compression），一种自适应提示压缩方法，用于解决 Large Language Models (LLMs) 处理长提示输入的挑战，避免了现有方法的额外计算资源和内存开销。ICPC 的核心机制是通过 encoders 计算每个词的出现概率，并利用信息 function 评估词携带的信息，从而减少信息损失并提高压缩速度。实验结果表明，ICPC 能有效压缩不同类别的长文本，在各种 NLP tasks 上实现更好的性能和推理速度。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01625v1",
      "published_date": "2025-01-03 03:46:51 UTC",
      "updated_date": "2025-01-03 03:46:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:20:17.092884"
    },
    {
      "arxiv_id": "2501.01618v1",
      "title": "Merging Context Clustering with Visual State Space Models for Medical Image Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Yun Zhu",
        "Dong Zhang",
        "Yi Lin",
        "Yifei Feng",
        "Jinhui Tang"
      ],
      "abstract": "Medical image segmentation demands the aggregation of global and local\nfeature representations, posing a challenge for current methodologies in\nhandling both long-range and short-range feature interactions. Recently, vision\nmamba (ViM) models have emerged as promising solutions for addressing model\ncomplexities by excelling in long-range feature iterations with linear\ncomplexity. However, existing ViM approaches overlook the importance of\npreserving short-range local dependencies by directly flattening spatial tokens\nand are constrained by fixed scanning patterns that limit the capture of\ndynamic spatial context information. To address these challenges, we introduce\na simple yet effective method named context clustering ViM (CCViM), which\nincorporates a context clustering module within the existing ViM models to\nsegment image tokens into distinct windows for adaptable local clustering. Our\nmethod effectively combines long-range and short-range feature interactions,\nthereby enhancing spatial contextual representations for medical image\nsegmentation tasks. Extensive experimental evaluations on diverse public\ndatasets, i.e., Kumar, CPM17, ISIC17, ISIC18, and Synapse demonstrate the\nsuperior performance of our method compared to current state-of-the-art\nmethods. Our code can be found at https://github.com/zymissy/CCViM.",
      "tldr_zh": "本研究针对医疗图像分割中全局和局部特征交互的挑战，提出了一种名为 Context Clustering ViM (CCViM) 的方法，该方法在现有 Vision Mamba (ViM) 模型基础上添加上下文聚类模块，将图像标记分成不同窗口，以适应动态局部聚类并结合长距离和短距离特征交互。CCViM 通过这种方式提升了空间上下文表示，从而改善了医疗图像分割的性能。在多个公共数据集（如 Kumar、CPM17、ISIC17、ISIC18 和 Synapse）上的实验表明，该方法比现有最先进方法表现出色，提供代码开源以供进一步验证。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Our paper has been accepted by the IEEE Transactions on Medical\n  Imaging. Our code can be found at https://github.com/zymissy/CCViM",
      "pdf_url": "http://arxiv.org/pdf/2501.01618v1",
      "published_date": "2025-01-03 03:25:30 UTC",
      "updated_date": "2025-01-03 03:25:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:20:30.587232"
    },
    {
      "arxiv_id": "2501.01611v1",
      "title": "Google is all you need: Semi-Supervised Transfer Learning Strategy For Light Multimodal Multi-Task Classification Model",
      "title_zh": "翻译失败",
      "authors": [
        "Haixu Liu",
        "Penghao Jiang",
        "Zerui Tao"
      ],
      "abstract": "As the volume of digital image data increases, the effectiveness of image\nclassification intensifies. This study introduces a robust multi-label\nclassification system designed to assign multiple labels to a single image,\naddressing the complexity of images that may be associated with multiple\ncategories (ranging from 1 to 19, excluding 12). We propose a multi-modal\nclassifier that merges advanced image recognition algorithms with Natural\nLanguage Processing (NLP) models, incorporating a fusion module to integrate\nthese distinct modalities. The purpose of integrating textual data is to\nenhance the accuracy of label prediction by providing contextual understanding\nthat visual analysis alone cannot fully capture. Our proposed classification\nmodel combines Convolutional Neural Networks (CNN) for image processing with\nNLP techniques for analyzing textual description (i.e., captions). This\napproach includes rigorous training and validation phases, with each model\ncomponent verified and analyzed through ablation experiments. Preliminary\nresults demonstrate the classifier's accuracy and efficiency, highlighting its\npotential as an automatic image-labeling system.",
      "tldr_zh": "本研究提出了一种轻量级多模态多任务分类模型，采用半监督转移学习策略，用于多标签图像分类，能够为单张图像分配1到19个标签（排除12）。该模型整合Convolutional Neural Networks (CNN) 处理图像数据和Natural Language Processing (NLP) 分析文本描述，通过融合模块结合视觉和文本模态，以提升标签预测的准确性和上下文理解。实验包括严格的训练、验证和消融实验，初步结果显示模型在准确性和效率上优于基线，具有作为自动图像标记系统的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01611v1",
      "published_date": "2025-01-03 03:11:17 UTC",
      "updated_date": "2025-01-03 03:11:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:20:42.067862"
    },
    {
      "arxiv_id": "2501.02026v1",
      "title": "Recursive Decomposition of Logical Thoughts: Framework for Superior Reasoning and Knowledge Propagation in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Kaleem Ullah Qasim",
        "Jiashu Zhang",
        "Tariq Alsahfi",
        "Ateeq Ur Rehman Butt"
      ],
      "abstract": "Enhancing the reasoning capabilities of Large Language Models remains a\ncritical challenge in artificial intelligence. We introduce RDoLT, Recursive\nDecomposition of Logical Thought prompting, a novel framework that\nsignificantly boosts LLM reasoning performance. RDoLT is built on three key\ninnovations: (1) recursively breaking down complex reasoning tasks into\nsub-tasks of progressive complexity; (2) employing an advanced selection and\nscoring mechanism to identify the most promising reasoning thoughts; and (3)\nintegrating a knowledge propagation module that mimics human learning by\nkeeping track of strong and weak thoughts for information propagation. Our\napproach was evaluated across multiple benchmarks, including GSM8K, SVAMP,\nMultiArith, LastLetterConcatenation, and Gaokao2023 Math. The results\ndemonstrate that RDoLT consistently outperforms existing state-of-the-art\ntechniques, achieving a 90.98 percent accuracy on GSM8K with ChatGPT-4,\nsurpassing state-of-the-art techniques by 6.28 percent. Similar improvements\nwere observed on other benchmarks, with accuracy gains ranging from 5.5 percent\nto 6.75 percent. These findings highlight RDoLT's potential to advance prompt\nengineering, offering a more effective and generalizable approach to complex\nreasoning tasks.",
      "tldr_zh": "本研究引入 RDoLT（Recursive Decomposition of Logical Thought prompting）框架，以提升大型语言模型（LLMs）的推理能力。该框架基于三个关键创新：递归地将复杂任务分解为逐步子任务、使用高级选择和评分机制识别最佳推理思路，以及整合知识传播模块来模仿人类学习过程。在多个基准测试如 GSM8K、SVAMP 和 MultiArith 上，RDoLT 表现出色，使用 ChatGPT-4 在 GSM8K 上达到 90.98% 的准确率，比现有最先进技术高出 6.28%，其他基准的提升在 5.5% 到 6.75% 之间。这些结果证明 RDoLT 为提示工程提供了更有效和通用的复杂推理解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02026v1",
      "published_date": "2025-01-03 02:55:44 UTC",
      "updated_date": "2025-01-03 02:55:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:20:54.624675"
    },
    {
      "arxiv_id": "2501.01601v1",
      "title": "Few-shot Implicit Function Generation via Equivariance",
      "title_zh": "翻译失败",
      "authors": [
        "Suizhi Huang",
        "Xingyi Yang",
        "Hongtao Lu",
        "Xinchao Wang"
      ],
      "abstract": "Implicit Neural Representations (INRs) have emerged as a powerful framework\nfor representing continuous signals. However, generating diverse INR weights\nremains challenging due to limited training data. We introduce Few-shot\nImplicit Function Generation, a new problem setup that aims to generate diverse\nyet functionally consistent INR weights from only a few examples. This is\nchallenging because even for the same signal, the optimal INRs can vary\nsignificantly depending on their initializations. To tackle this, we propose\nEquiGen, a framework that can generate new INRs from limited data. The core\nidea is that functionally similar networks can be transformed into one another\nthrough weight permutations, forming an equivariance group. By projecting these\nweights into an equivariant latent space, we enable diverse generation within\nthese groups, even with few examples. EquiGen implements this through an\nequivariant encoder trained via contrastive learning and smooth augmentation,\nan equivariance-guided diffusion process, and controlled perturbations in the\nequivariant subspace. Experiments on 2D image and 3D shape INR datasets\ndemonstrate that our approach effectively generates diverse INR weights while\npreserving their functional properties in few-shot scenarios.",
      "tldr_zh": "该研究提出 Few-shot Implicit Function Generation 的新问题，旨在从少量示例生成多样化却功能一致的 Implicit Neural Representations (INRs) 权重，以解决训练数据有限的挑战。研究引入 EquiGen 框架，利用权重置换形成的 equivariance group，将权重投影到等价潜空间中，通过对比学习训练的等价编码器、equivariance-guided diffusion 过程以及子空间中的控制扰动，实现多样化生成。实验在 2D 图像和 3D 形状 INR 数据集上证明，该方法在少样本场景下有效生成多样权重，同时保持功能属性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 8 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.01601v1",
      "published_date": "2025-01-03 02:23:55 UTC",
      "updated_date": "2025-01-03 02:23:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:21:05.738930"
    },
    {
      "arxiv_id": "2501.01598v1",
      "title": "Prism: Mining Task-aware Domains in Non-i.i.d. IMU Data for Flexible User Perception",
      "title_zh": "Prism：为灵活用户感知在非独立同分布 IMU 数据中挖掘任务感知域",
      "authors": [
        "Yunzhe Li",
        "Facheng Hu",
        "Hongzi Zhu",
        "Quan Liu",
        "Xiaoke Zhao",
        "Jiangang Shen",
        "Shan Chang",
        "Minyi Guo"
      ],
      "abstract": "A wide range of user perception applications leverage inertial measurement\nunit (IMU) data for online prediction. However, restricted by the non-i.i.d.\nnature of IMU data collected from mobile devices, most systems work well only\nin a controlled setting (e.g., for a specific user in particular postures),\nlimiting application scenarios. To achieve uncontrolled online prediction on\nmobile devices, referred to as the flexible user perception (FUP) problem, is\nattractive but hard. In this paper, we propose a novel scheme, called Prism,\nwhich can obtain high FUP accuracy on mobile devices. The core of Prism is to\ndiscover task-aware domains embedded in IMU dataset, and to train a\ndomain-aware model on each identified domain. To this end, we design an\nexpectation-maximization (EM) algorithm to estimate latent domains with respect\nto the specific downstream perception task. Finally, the best-fit model can be\nautomatically selected for use by comparing the test sample and all identified\ndomains in the feature space. We implement Prism on various mobile devices and\nconduct extensive experiments. Results demonstrate that Prism can achieve the\nbest FUP performance with a low latency.",
      "tldr_zh": "该论文针对非独立同分布(non-i.i.d.)的IMU数据在用户感知应用中的局限性，提出Prism方案，以实现灵活用户感知(Flexible User Perception, FUP)问题。Prism的核心是通过挖掘任务感知域(task-aware domains)并使用期望最大化(EM)算法估计潜在域，从而在每个标识域上训练域感知模型(domain-aware model)。实验结果显示，Prism在各种移动设备上实现了最佳FUP性能，同时保持低延迟，扩展了IMU数据在不受控环境中的应用场景。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "in Proceedings of IEEE INFOCOM 2025, London, United Kingdom",
      "pdf_url": "http://arxiv.org/pdf/2501.01598v1",
      "published_date": "2025-01-03 02:07:42 UTC",
      "updated_date": "2025-01-03 02:07:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:21:17.957194"
    },
    {
      "arxiv_id": "2501.01594v1",
      "title": "PSYCHE: A Multi-faceted Patient Simulation Framework for Evaluation of Psychiatric Assessment Conversational Agents",
      "title_zh": "PSYCHE：多方面的患者模拟框架，用于评估精神病学评估对话代理",
      "authors": [
        "Jingoo Lee",
        "Kyungho Lim",
        "Young-Chul Jung",
        "Byung-Hoon Kim"
      ],
      "abstract": "Recent advances in large language models (LLMs) have accelerated the\ndevelopment of conversational agents capable of generating human-like\nresponses. Since psychiatric assessments typically involve complex\nconversational interactions between psychiatrists and patients, there is\ngrowing interest in developing LLM-based psychiatric assessment conversational\nagents (PACAs) that aim to simulate the role of psychiatrists in clinical\nevaluations. However, standardized methods for benchmarking the clinical\nappropriateness of PACAs' interaction with patients still remain underexplored.\nHere, we propose PSYCHE, a novel framework designed to enable the 1) clinically\nrelevant, 2) ethically safe, 3) cost-efficient, and 4) quantitative evaluation\nof PACAs. This is achieved by simulating psychiatric patients based on a\nmulti-faceted psychiatric construct that defines the simulated patients'\nprofiles, histories, and behaviors, which PACAs are expected to assess. We\nvalidate the effectiveness of PSYCHE through a study with 10 board-certified\npsychiatrists, supported by an in-depth analysis of the simulated patient\nutterances.",
      "tldr_zh": "该论文提出PSYCHE框架，这是一个多方面患者模拟系统，用于评估基于大型语言模型(LLMs)的精神病评估对话代理(PACAs)。PSYCHE通过定义模拟患者的档案、历史和行为来实现临床相关、伦理安全、成本高效以及定量的评估，从而帮助测试PACAs在精神病评估中的表现。研究通过10名认证精神病医生的参与和对模拟患者对话的深入分析，验证了框架的有效性，为标准化PACAs评估提供了新方法。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "The first two authors contributed equally",
      "pdf_url": "http://arxiv.org/pdf/2501.01594v1",
      "published_date": "2025-01-03 01:38:46 UTC",
      "updated_date": "2025-01-03 01:38:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:21:29.552349"
    },
    {
      "arxiv_id": "2501.01593v1",
      "title": "BLAST: A Stealthy Backdoor Leverage Attack against Cooperative Multi-Agent Deep Reinforcement Learning based Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Yinbo Yu",
        "Saihao Yan",
        "Xueyu Yin",
        "Jing Fang",
        "Jiajia Liu"
      ],
      "abstract": "Recent studies have shown that cooperative multi-agent deep reinforcement\nlearning (c-MADRL) is under the threat of backdoor attacks. Once a backdoor\ntrigger is observed, it will perform malicious actions leading to failures or\nmalicious goals. However, existing backdoor attacks suffer from several issues,\ne.g., instant trigger patterns lack stealthiness, the backdoor is trained or\nactivated by an additional network, or all agents are backdoored. To this end,\nin this paper, we propose a novel backdoor leverage attack against c-MADRL,\nBLAST, which attacks the entire multi-agent team by embedding the backdoor only\nin a single agent. Firstly, we introduce adversary spatiotemporal behavior\npatterns as the backdoor trigger rather than manual-injected fixed visual\npatterns or instant status and control the period to perform malicious actions.\nThis method can guarantee the stealthiness and practicality of BLAST. Secondly,\nwe hack the original reward function of the backdoor agent via unilateral\nguidance to inject BLAST, so as to achieve the \\textit{leverage attack effect}\nthat can pry open the entire multi-agent system via a single backdoor agent. We\nevaluate our BLAST against 3 classic c-MADRL algorithms (VDN, QMIX, and MAPPO)\nin 2 popular c-MADRL environments (SMAC and Pursuit), and 2 existing defense\nmechanisms. The experimental results demonstrate that BLAST can achieve a high\nattack success rate while maintaining a low clean performance variance rate.",
      "tldr_zh": "本研究提出了一种隐秘后台攻击方法BLAST，针对合作多智能体深度强化学习(c-MADRL)系统，通过仅在单个智能体中嵌入后台触发器，实现对整个团队的“杠杆攻击”效果。BLAST使用对手时空行为模式作为触发器，而不是传统的固定视觉模式，并通过单方面指导篡改后台智能体的原始奖励函数，以确保攻击的隐秘性和实用性。在3种经典c-MADRL算法(VDN、QMIX和MAPPO)以及2个流行环境(SMAC和Pursuit)上进行评估，结果显示BLAST实现了高攻击成功率，同时保持了较低的正常性能波动率，并能绕过现有防御机制。",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "12. arXiv admin note: substantial text overlap with arXiv:2409.07775",
      "pdf_url": "http://arxiv.org/pdf/2501.01593v1",
      "published_date": "2025-01-03 01:33:29 UTC",
      "updated_date": "2025-01-03 01:33:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:21:41.618064"
    },
    {
      "arxiv_id": "2501.01588v1",
      "title": "(WhyPHI) Fine-Tuning PHI-3 for Multiple-Choice Question Answering: Methodology, Results, and Challenges",
      "title_zh": "翻译失败",
      "authors": [
        "Mohamed Hisham Abdellatif"
      ],
      "abstract": "Large Language Models (LLMs) have become essential tools across various\ndomains due to their impressive capabilities in understanding and generating\nhuman-like text. The ability to accurately answer multiple-choice questions\n(MCQs) holds significant value in education, particularly in automated tutoring\nsystems and assessment platforms. However, adapting LLMs to handle MCQ tasks\neffectively remains challenging due to the hallucinations and unclear prompts.\nThis work explores the potential of Microsoft's PHI-3\\cite{Abdin2024}, a\ncompact yet efficient LLM, for MCQ answering. Our contributions include\nfine-tuning the model on the TruthfulQA dataset, designing optimized prompts to\nenhance model performance, and evaluating using perplexity and traditional\nmetrics like accuracy and F1 score. Results show a remarkable improvement in\nPHI-3.5's MCQ handling post-fine-tuning, with perplexity decreasing from 4.68\nto 2.27, and accuracy rising from 62\\% to 90.8\\%. This research underlines the\nimportance of efficient models in adaptive learning systems and educational\nassessments, paving the way for broader integration into the classroom,\nparticularly in fields like test preparation, student feedback, and\npersonalized learning.",
      "tldr_zh": "本研究探讨了微调 Microsoft 的 PHI-3 模型，以提升其在多选题（MCQs）回答中的性能，针对 LLMs 的幻觉和提示不清晰等挑战。方法包括使用 TruthfulQA 数据集进行细化训练、设计优化提示，并通过 perplexity、accuracy 和 F1 score 等指标进行评估。结果显示，微调后 PHI-3.5 的 perplexity 从 4.68 降至 2.27，accuracy 从 62% 提升至 90.8%，证明了高效模型在教育应用中的潜力，如自动化辅导和个性化学习系统。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01588v1",
      "published_date": "2025-01-03 00:56:46 UTC",
      "updated_date": "2025-01-03 00:56:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:21:54.170750"
    },
    {
      "arxiv_id": "2503.15489v1",
      "title": "PersonaAI: Leveraging Retrieval-Augmented Generation and Personalized Context for AI-Driven Digital Avatars",
      "title_zh": "翻译失败",
      "authors": [
        "Elvis Kimara",
        "Kunle S. Oguntoye",
        "Jian Sun"
      ],
      "abstract": "This paper introduces PersonaAI, a cutting-edge application that leverages\nRetrieval-Augmented Generation (RAG) and the LLAMA model to create highly\npersonalized digital avatars capable of accurately mimicking individual\npersonalities. Designed as a cloud-based mobile application, PersonaAI captures\nuser data seamlessly, storing it in a secure database for retrieval and\nanalysis. The result is a system that provides context-aware, accurate\nresponses to user queries, enhancing the potential of AI-driven\npersonalization.\n  Why should you care? PersonaAI combines the scalability of RAG with the\nefficiency of prompt-engineered LLAMA3, offering a lightweight, sustainable\nalternative to traditional large language model (LLM) training methods. The\nsystem's novel approach to data collection, utilizing real-time user\ninteractions via a mobile app, ensures enhanced context relevance while\nmaintaining user privacy. By open-sourcing our implementation, we aim to foster\nadaptability and community-driven development.\n  PersonaAI demonstrates how AI can transform interactions by merging\nefficiency, scalability, and personalization, making it a significant step\nforward in the future of digital avatars and personalized AI.",
      "tldr_zh": "这篇论文介绍了 PersonaAI，一种基于 Retrieval-Augmented Generation (RAG) 和 LLAMA 模型的云端移动应用，用于创建高度个性化的数字头像，以准确模仿用户个性。系统通过实时捕获用户数据并存储在安全数据库中，实现上下文感知的响应，提升 AI 驱动的个性化体验。PersonaAI 采用轻量级提示工程方法，提供高效、可扩展的替代方案，同时维护用户隐私，并通过开源实现促进社区适应和发展。总体上，它展示了 AI 如何通过效率和个性化来革新数字互动。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15489v1",
      "published_date": "2025-01-03 00:31:28 UTC",
      "updated_date": "2025-01-03 00:31:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:24:06.135911"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 81,
  "processed_papers_count": 81,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T20:24:34.421025"
}
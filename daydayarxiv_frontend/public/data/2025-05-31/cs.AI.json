{
  "date": "2025-05-31",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-05-31 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv åˆæ˜¯â€œç¥ä»™æ‰“æ¶â€çš„ä¸€å¤©ã€‚æ ¸å¿ƒçœ‹ç‚¹é›†ä¸­åœ¨ **Agent çš„æ·±åº¦æ€è€ƒä¸ä¸–ç•Œæ¨¡å‹ï¼ˆWorld Modelsï¼‰çš„ç»“åˆ**ï¼ˆå¦‚ DeepSeek-R1 å¯å‘çš„ Dyna-Thinkï¼‰ï¼Œä»¥åŠå¯¹ **LLM å¯¹é½ã€é—å¿˜ï¼ˆUnlearningï¼‰å’Œè¯„ä¼°ä½“ç³»çš„æ·±åˆ»åæ€**ã€‚æ­¤å¤–ï¼Œå¤šæ¨¡æ€é¢†åŸŸåœ¨**åŒ»ç–—åŸºç¡€æ¨¡å‹**å’Œ**3D ç”Ÿæˆ**æ–¹é¢ä¹Ÿæœ‰é‡ç£…å·¥ä½œï¼ˆNeurIPS Oral å’Œ CVPR æ¥æ”¶è®ºæ–‡ï¼‰ã€‚æˆ‘ä»¬çœ‹åˆ°äº†å­¦æœ¯ç•Œå¼€å§‹å†·é™ä¸‹æ¥ï¼Œå»å®¡è§†æ¨¡å‹è¡¨ç°è¡°å‡ã€åå¥½ä¸ä¸€è‡´ä»¥åŠâ€œé—å¿˜â€æ˜¯å¦çœŸçš„æœ‰æ•ˆç­‰æœ¬è´¨é—®é¢˜ã€‚\n\nä»¥ä¸‹æ˜¯ç²¾é€‰çš„æ¯æ—¥å¿…è¯»ï¼š\n\n---\n\n### ğŸš€ æ·±åº¦æ¨ç†ä¸ Agent (Reasoning & Agents)\n\n**1. [NeurIPS 2025] Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents**\n*   **æ ‡é¢˜ï¼š** Dyna-Thinkï¼šååŒæ¨ç†ã€è¡ŒåŠ¨ä¸ä¸–ç•Œæ¨¡å‹æ¨¡æ‹Ÿçš„ AI æ™ºèƒ½ä½“\n*   **æ ¸å¿ƒæœ¯è¯­ï¼š** World Model Simulation, Dyna-Think Imitation Learning (DIT), System 2 Reasoning\n*   **æ ¸å¿ƒå‘ç°ï¼š**\n    *   å— DeepSeek-R1 å¯å‘ï¼Œæ–‡ç« æ¢è®¨äº†åœ¨é•¿ç¨‹ä»»åŠ¡ä¸­ï¼ŒAgent å¦‚ä½•é€šè¿‡â€œæ€è€ƒâ€æ¥æå‡è¡¨ç°ã€‚\n    *   æå‡ºäº† **Dyna-Think** æ¡†æ¶ï¼Œå°†è§„åˆ’ï¼ˆPlanningï¼‰ä¸å†…éƒ¨ä¸–ç•Œæ¨¡å‹ï¼ˆWorld Modelï¼‰çš„æ¨¡æ‹Ÿç›¸ç»“åˆã€‚\n    *   åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š**DIT**ï¼ˆæ¨¡ä»¿ R1 çš„æ€è€ƒè¿‡ç¨‹ï¼Œä¸“æ³¨äºæ¨¡æ‹Ÿè¡ŒåŠ¨åæœï¼‰å’Œ **DDT**ï¼ˆå…ˆæå‡ä¸–ç•Œæ¨¡å‹çš„é¢„æµ‹/æ‰¹åˆ¤èƒ½åŠ›ï¼Œå†ä¼˜åŒ–ç­–ç•¥ï¼‰ã€‚\n    *   **ç»“æœï¼š** åœ¨ OSWorld ç­‰åŸºå‡†ä¸Šï¼Œä»¥æ›´å°‘çš„ token æ¶ˆè€—è¾¾åˆ°äº† R1 çº§åˆ«çš„æ€§èƒ½ï¼Œè¯æ˜äº†**ä¸–ç•Œæ¨¡å‹æ¨¡æ‹Ÿèƒ½åŠ›ä¸ Agent æœ€ç»ˆè¡¨ç°å¼ºç›¸å…³**ã€‚\n\n**2. [NeurIPS 2025] AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents**\n*   **æ ‡é¢˜ï¼š** AgentAuditorï¼šé’ˆå¯¹ LLM æ™ºèƒ½ä½“çš„äººç±»æ°´å¹³å®‰å…¨ä¸å®‰ä¿è¯„ä¼°\n*   **æ ¸å¿ƒæœ¯è¯­ï¼š** Memory-Augmented Reasoning, Retrieval-Augmented Generation, Risk Evaluation\n*   **æ ¸å¿ƒå‘ç°ï¼š**\n    *   ç°æœ‰çš„è¯„ä¼°å™¨å¾ˆéš¾æ•æ‰ Agent åœ¨å¤šæ­¥è¡ŒåŠ¨ä¸­çš„ç»†å¾®é£é™©ã€‚\n    *   æå‡ºäº† **AgentAuditor**ï¼Œä¸€ä¸ªæ— éœ€è®­ç»ƒã€é€šè¿‡è®°å¿†å¢å¼ºçš„æ¨ç†æ¡†æ¶ã€‚å®ƒé€šè¿‡æ„å»ºâ€œä½“éªŒè®°å¿†â€ï¼Œåˆ©ç”¨ RAG æ£€ç´¢ç›¸å…³çš„æ¨ç†ç—•è¿¹æ¥æŒ‡å¯¼å¯¹æ–°æ¡ˆä¾‹çš„è¯„ä¼°ã€‚\n    *   å‘å¸ƒäº† **ASSEBench** åŸºå‡†ï¼Œæ¶µç›– 29 ä¸ªåœºæ™¯çš„ 2293 æ¡è®°å½•ã€‚AgentAuditor å®ç°äº†äººç±»æ°´å¹³çš„è¯„ä¼°å‡†ç¡®ç‡ã€‚\n\n**3. Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs**\n*   **æ ‡é¢˜ï¼š** åƒç»æµå­¦å®¶ä¸€æ ·æ€è€ƒï¼šåŸºäºç»æµå­¦é—®é¢˜çš„åè®­ç»ƒèµ‹äºˆ LLM æˆ˜ç•¥æ³›åŒ–èƒ½åŠ›\n*   **æ ¸å¿ƒæœ¯è¯­ï¼š** Post-Training, SFT, RLVR (Reinforcement Learning with Verifiable Rewards), Economic Reasoning\n*   **æ ¸å¿ƒå‘ç°ï¼š**\n    *   ä½¿ç”¨ç»æµå­¦æ¨ç†ï¼ˆåšå¼ˆè®ºã€èµ„æºåˆ†é…ï¼‰ä½œä¸ºæµ‹è¯•åºŠï¼Œæ¢ç©¶ Post-training æ˜¯å¦èƒ½æå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰çš„èƒ½åŠ›ã€‚\n    *   å‘å¸ƒäº† **Recon** æ¨¡å‹ï¼ˆ7Bï¼‰ï¼Œåœ¨ 2100 ä¸ªé«˜è´¨é‡ç»æµå­¦é—®é¢˜ä¸Šè¿›è¡Œäº†å¾®è°ƒã€‚\n    *   **Implicationï¼š** è¯æ˜äº†ç‰¹å®šé¢†åŸŸçš„ç»“æ„åŒ–æ¨ç†è®­ç»ƒï¼ˆå¦‚ç»æµå­¦ï¼‰å¯ä»¥æ³›åŒ–ï¼Œæå‡æ¨¡å‹åœ¨å¤šæ™ºèƒ½ä½“åšå¼ˆä¸­çš„ç†æ€§å’Œæˆ˜ç•¥èƒ½åŠ›ã€‚\n\n---\n\n### âš ï¸ æ¨¡å‹å¯¹é½ã€å®‰å…¨ä¸åæ€ (Alignment & Safety)\n\n**4. Alignment Revisited: Are Large Language Models Consistent in Stated and Revealed Preferences?**\n*   **æ ‡é¢˜ï¼š** å†è°ˆå¯¹é½ï¼šå¤§è¯­è¨€æ¨¡å‹çš„â€œå£°ç§°åå¥½â€ä¸â€œæ˜¾éœ²åå¥½â€ä¸€è‡´å—ï¼Ÿ\n*   **æ ¸å¿ƒæœ¯è¯­ï¼š** Stated Preferences, Revealed Preferences, Preference Deviation, KL Divergence\n*   **æ ¸å¿ƒå‘ç°ï¼š**\n    *   è¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£ä¸”æœ¬è´¨çš„å¿ƒç†å­¦/ä¼¦ç†å­¦è§†è§’ã€‚æ¨¡å‹â€œå˜´ä¸Šè¯´çš„â€ï¼ˆå›ç­”åŸåˆ™æ€§é—®é¢˜ï¼‰å’Œâ€œå®é™…åšçš„â€ï¼ˆåœ¨å…·ä½“è¯­å¢ƒä¸‹çš„é€‰æ‹©ï¼‰å¾€å¾€ä¸ä¸€è‡´ã€‚\n    *   ç ”ç©¶å‘ç°ï¼Œå¾®å°çš„æç¤ºè¯ï¼ˆPromptï¼‰å˜åŒ–å°±èƒ½è®©æ¨¡å‹è¿èƒŒå…¶å£°ç§°çš„åŸåˆ™ã€‚\n    *   **æ‰¹åˆ¤ï¼š** è¿™æ­ç¤ºäº†ç›®å‰ LLM å¯¹é½çš„è„†å¼±æ€§ï¼Œå¯¹äºé«˜é£é™©åº”ç”¨ï¼ˆHigh-stakes applicationsï¼‰æ˜¯å·¨å¤§çš„éšæ‚£ã€‚\n\n**5. Existing Large Language Model Unlearning Evaluations Are Inconclusive**\n*   **æ ‡é¢˜ï¼š** ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹â€œé—å¿˜â€è¯„ä¼°æ˜¯ä¸ç¡®å®šçš„\n*   **æ ¸å¿ƒæœ¯è¯­ï¼š** Machine Unlearning, Information Injection, Spurious Correlations\n*   **æ ¸å¿ƒå‘ç°ï¼š**\n    *   **æ³¼å†·æ°´ä¹‹ä½œ**ã€‚ä½œè€…æ‰¹åˆ¤äº†å½“å‰çš„æœºå™¨é—å¿˜ï¼ˆUnlearningï¼‰è¯„ä¼°æ ‡å‡†ã€‚\n    *   æŒ‡å‡ºè®¸å¤šè¯„ä¼°å®é™…ä¸Šæ˜¯åœ¨æµ‹è¯•æ—¶é‡æ–°å¼•å…¥äº†ä¿¡æ¯ï¼ˆInformation Injectionï¼‰ï¼Œæˆ–è€…ä¾èµ–è™šå‡ç›¸å…³æ€§ï¼ˆSpurious Correlationsï¼‰ï¼Œå¯¼è‡´å³ä½¿æ¨¡å‹æ²¡å¿˜å¹²å‡€ï¼Œè¯„ä¼°ä¹Ÿæ˜¾ç¤ºâ€œæˆåŠŸâ€ã€‚\n    *   æå‡ºäº†ä¸¤ä¸ªè¯„ä¼°åŸåˆ™ï¼šæœ€å°åŒ–ä¿¡æ¯æ³¨å…¥å’Œä¸‹æ¸¸ä»»åŠ¡æ„ŸçŸ¥ã€‚\n\n**6. [ICML 2025] \"Who experiences large model decay and why?\" A Hierarchical Framework for Diagnosing Heterogeneous Performance Drift**\n*   **æ ‡é¢˜ï¼š** â€œè°ç»å†äº†æ¨¡å‹å¤§å¹…è¡°é€€ï¼Œä¸ºä»€ä¹ˆï¼Ÿâ€ ä¸€ä¸ªè¯Šæ–­å¼‚è´¨æ€§æ€§èƒ½æ¼‚ç§»çš„åˆ†å±‚æ¡†æ¶\n*   **æ ¸å¿ƒæœ¯è¯­ï¼š** Performance Drift, Subgroup-scanning, SHIFT Framework, Covariate/Outcome Shifts\n*   **æ ¸å¿ƒå‘ç°ï¼š**\n    *   æ¨¡å‹éƒ¨ç½²åæ€§èƒ½ä¸‹é™é€šå¸¸ä¸æ˜¯å‡åŒ€çš„ï¼ŒæŸäº›å­ç¾¤ï¼ˆSubgroupsï¼‰ä¼šé€šè¿‡å‰§çƒˆè¡°å‡ã€‚\n    *   æå‡ºäº† **SHIFT** æ¡†æ¶ï¼šå…ˆé—®â€œå“ªé‡Œï¼ˆWhereï¼‰â€å‡ºäº†é—®é¢˜ï¼ˆå“ªä¸ªå­ç¾¤ï¼‰ï¼Œå†é—®â€œå¦‚ä½•ï¼ˆHowï¼‰â€è§£é‡Šï¼ˆå…·ä½“å˜é‡çš„æ¼‚ç§»ï¼‰ã€‚\n    *   è¿™æ˜¯ä¸€ä¸ªéå¸¸å®ç”¨çš„å·¥ä¸šç•Œè¯Šæ–­å·¥å…·ï¼Œèƒ½å‡†ç¡®å®šä½â€œé‡ç¾åŒºâ€å¹¶æŒ‡å¯¼ä¿®å¤ã€‚\n\n---\n\n### ğŸ¥ AI for Science & Medical (NeurIPS Oral & More)\n\n**7. [NeurIPS 2025 Oral] QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training**\n*   **æ ‡é¢˜ï¼š** QoQ-Medï¼šåˆ©ç”¨é¢†åŸŸæ„ŸçŸ¥ GRPO è®­ç»ƒæ„å»ºå¤šæ¨¡æ€ä¸´åºŠåŸºç¡€æ¨¡å‹\n*   **æ ¸å¿ƒæœ¯è¯­ï¼š** Domain-aware Relative Policy Optimization (DRPO), Clinical Foundation Models, Heterogeneous Data\n*   **æ ¸å¿ƒå‘ç°ï¼š**\n    *   å‘å¸ƒäº† **QoQ-Med-7B/32B**ï¼Œè¿™æ˜¯é¦–ä¸ªèƒ½è”åˆæ¨ç†åŒ»å­¦å›¾åƒã€æ—¶é—´åºåˆ—ä¿¡å·å’Œæ–‡æœ¬æŠ¥å‘Šçš„é€šç”¨ä¸´åºŠåŸºç¡€æ¨¡å‹ã€‚\n    *   æå‡ºäº† **DRPO**ï¼ˆé¢†åŸŸæ„ŸçŸ¥ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¹³è¡¡ä¸åŒç¨€æœ‰åº¦å’Œéš¾åº¦çš„ä¸´åºŠé¢†åŸŸæ•°æ®ã€‚\n    *   **æ•ˆæœï¼š** åœ¨è¯Šæ–­æ€§èƒ½ä¸Šæ¯”å…¶ä»–æ–¹æ³•ï¼ˆå¦‚ GRPOï¼‰æå‡äº† 43%ï¼ˆMacro-F1ï¼‰ï¼Œåœ¨åˆ†å‰²ä»»åŠ¡ä¸Šè¾¾åˆ° OpenAI o4-mini çš„æ°´å¹³ã€‚\n\n**8. Text-to-CT Generation via 3D Latent Diffusion Model with Contrastive Vision-Language Pretraining**\n*   **æ ‡é¢˜ï¼š** åŸºäºå¯¹æ¯”è§†è§‰-è¯­è¨€é¢„è®­ç»ƒå’Œ 3D æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆ CT\n*   **æ ¸å¿ƒæœ¯è¯­ï¼š** 3D Latent Diffusion Model, Contrastive Pretraining, Volumetric VAE\n*   **æ ¸å¿ƒå‘ç°ï¼š**\n    *   è§£å†³äº†æ–‡æœ¬ç”Ÿæˆ 3D ä½“ç§¯æ•°æ®ï¼ˆå¦‚ CTï¼‰çš„éš¾é¢˜ã€‚\n    *   å¼•å…¥äº†åŒç¼–ç å™¨ CLIP é£æ ¼çš„é¢„è®­ç»ƒï¼Œå¯¹é½ 3D è§†è§‰å’Œè¯­è¨€æ•°æ®ï¼Œå¹¶é…åˆ 3D VAE è¿›è¡Œå‹ç¼©ç”Ÿæˆã€‚\n    *   ç”Ÿæˆçš„ CT æ•°æ®å¯æœ‰æ•ˆç”¨äºæ•°æ®å¢å¼ºï¼Œæå‡ä¸‹æ¸¸è¯Šæ–­æ¨¡å‹çš„æ€§èƒ½ã€‚\n\n---\n\n### ğŸ› ï¸ æ¨¡å‹æ„å»ºä¸é«˜æ•ˆè®¡ç®— (Architecture & Efficiency)\n\n**9. Assembly of Experts: Linear-time construction of the Chimera LLM variants with emergent and adaptable behaviors**\n*   **æ ‡é¢˜ï¼š** ä¸“å®¶ç»„è£…ï¼šçº¿æ€§æ—¶é—´æ„å»ºå…·æœ‰æ¶Œç°å’Œé€‚åº”è¡Œä¸ºçš„ Chimera LLM å˜ä½“\n*   **æ ¸å¿ƒæœ¯è¯­ï¼š** Assembly-of-Experts (AoE), Model Merging, Chimera\n*   **æ ¸å¿ƒå‘ç°ï¼š**\n    *   é¢„è®­ç»ƒå¤ªè´µï¼Ÿæœ¬æ–‡æå‡º **AoE** æ–¹æ³•ï¼Œåœ¨çº¿æ€§æ—¶é—´å†…é€šè¿‡æ’å€¼ç°æœ‰çš„ MoE æ¨¡å‹æƒé‡æ¥â€œç»„è£…â€æ–°æ¨¡å‹ã€‚\n    *   æ„å»ºäº† **Chimera**ï¼ˆDeepSeek V3 å’Œ R1 çš„æ··åˆä½“ï¼‰ã€‚å®ƒç»§æ‰¿äº† R1 çš„ Expert æƒé‡ï¼Œä½†åªéœ€è¦æ›´å°‘çš„è¾“å‡º token å°±èƒ½è¾¾åˆ° R1 çº§åˆ«çš„æ™ºåŠ›ï¼Œä¸”æ¨ç†é€Ÿåº¦æ¥è¿‘ V3ã€‚\n    *   **äº®ç‚¹ï¼š** æ— éœ€å¾®è°ƒæˆ–è’¸é¦ï¼Œç›´æ¥â€œæ‹¼â€å‡ºä¸€ä¸ªå¥½æ¨¡å‹ã€‚\n\n**10. [ICML 2025] CodeSense: a Real-World Benchmark and Dataset for Code Semantic Reasoning**\n*   **æ ‡é¢˜ï¼š** CodeSenseï¼šä¸€ä¸ªç”¨äºä»£ç è¯­ä¹‰æ¨ç†çš„çœŸå®ä¸–ç•ŒåŸºå‡†å’Œæ•°æ®é›†\n*   **æ ¸å¿ƒæœ¯è¯­ï¼š** Code Semantic Reasoning, Execution Traces, Fine-grained Reasoning\n*   **æ ¸å¿ƒå‘ç°ï¼š**\n    *   ç°æœ‰çš„ä»£ç åŸºå‡†æµ‹è¯•å¤ªå…³æ³¨è¾“å…¥/è¾“å‡ºé¢„æµ‹ï¼Œå¿½ç•¥äº†ç»†ç²’åº¦çš„è¯­ä¹‰æ¨ç†ã€‚\n    *   **CodeSense** æ”¶é›†äº†çœŸå®é¡¹ç›®çš„æ‰§è¡Œè½¨è¿¹ï¼ˆTraceï¼‰ï¼Œæµ‹è¯• LLM å¯¹ä»£ç è¯­ä¹‰çš„ç†è§£ã€‚\n    *   ç»“è®ºï¼šç›®å‰ LLM åœ¨è¿™ç§ç»†ç²’åº¦æ¨ç†ä¸Šä¸çœŸå®éœ€æ±‚å·®è·å·¨å¤§ï¼Œå³ä½¿åŠ äº† CoT ä¹Ÿå¾ˆéš¾å¼¥è¡¥**ä»£ç è¯­ä¹‰ç†è§£çš„ç¼ºå¤±**ã€‚\n\n---\n\n### ğŸ¨ è§†è§‰ä¸å¤šæ¨¡æ€ (Vision & Multimodal)\n\n**11. [CVPR Accepted] ArtiScene: Language-Driven Artistic 3D Scene Generation Through Image Intermediary**\n*   **æ ‡é¢˜ï¼š** ArtiSceneï¼šé€šè¿‡å›¾åƒä¸­ä»‹å®ç°çš„è¯­è¨€é©±åŠ¨è‰ºæœ¯ 3D åœºæ™¯ç”Ÿæˆ\n*   **æ ¸å¿ƒæœ¯è¯­ï¼š** Text-to-3D, Image Intermediary, Layout Estimation\n*   **æ ¸å¿ƒå‘ç°ï¼š**\n    *   ç›´æ¥ç”Ÿæˆé«˜è´¨é‡ 3D æ•°æ®å¾ˆéš¾ã€‚æœ¬æ–‡æ€è·¯ï¼š**å…ˆç”Ÿæˆ 2D å›¾åƒï¼Œå†è½¬ 3D**ã€‚\n    *   åˆ©ç”¨ç°æˆå¼ºå¤§çš„ Text-to-Image æ¨¡å‹ç”Ÿæˆ 2D åœºæ™¯ï¼Œç„¶åæå–ç‰©ä½“å½¢çŠ¶ã€å¤–è§‚å’Œå¸ƒå±€ä¿¡æ¯ç»„è£…æˆ 3D åœºæ™¯ã€‚\n    *   åœ¨ç”¨æˆ·è¯„ä¼°ä¸­èƒœç‡é«˜è¾¾ 74.89%ï¼Œå¤§å¹…ä¼˜äºç›´æ¥ç”Ÿæˆ 3D çš„æ–¹æ³•ã€‚\n\n**12. HueManity: Probing Fine-Grained Visual Perception in MLLMs**\n*   **æ ‡é¢˜ï¼š** HueManityï¼šæ¢ç©¶å¤šæ¨¡æ€å¤§æ¨¡å‹ä¸­çš„ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥\n*   **æ ¸å¿ƒæœ¯è¯­ï¼š** Visual Perception Benchmark, Ishihara Test style\n*   **æ ¸å¿ƒå‘ç°ï¼š**\n    *   MLLM èƒ½å†™è¯—èƒ½ç¼–ç¨‹ï¼Œä½†åœ¨ç±»ä¼¼**è‰²ç›²æµ‹è¯•ï¼ˆIshihara testï¼‰**çš„ç‚¹é˜µå›¾ä¸­è¯†åˆ«å­—ç¬¦å´ä¸€å¡Œç³Šæ¶‚ã€‚\n    *   æœ€å¥½çš„æ¨¡å‹åœ¨â€œå›°éš¾â€æ¨¡å¼ä¸‹å‡†ç¡®ç‡ä»… 3%ï¼ˆäººç±»æ¥è¿‘ 95%ï¼‰ã€‚è¿™æ­ç¤ºäº†å½“å‰ MLLM è§†è§‰ç¼–ç å™¨åœ¨**åº•å±‚æ„ŸçŸ¥èƒ½åŠ›**ä¸Šçš„å·¨å¤§ç¼ºé™·ã€‚\n\n---\n\n### ğŸ“‰ å…¶ä»–å€¼å¾—å…³æ³¨çš„è®ºæ–‡ (Quick Hits)\n\n*   **[NeurIPS 2025] Foresight: Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation**: è§†é¢‘ç”Ÿæˆå¤ªæ…¢ï¼ŸForesight é€šè¿‡åŠ¨æ€å¤ç”¨ DiT å±‚çš„è¾“å‡ºæ¥åŠ é€Ÿï¼Œä¸”ä¸æŸå¤±è´¨é‡ã€‚\n*   **The Disparate Effects of Partial Information in Bayesian Strategic Learning**: ç ”ç©¶äº†åœ¨æˆ˜ç•¥å­¦ä¹ ä¸­ï¼Œä»…ä»…å‘ä»£ç†ï¼ˆAgentsï¼‰é€éœ²éƒ¨åˆ†è¯„åˆ†è§„åˆ™ä¿¡æ¯ä¼šå¦‚ä½•å½±å“å…¬å¹³æ€§ã€‚\n*   **[ICML 2025] TMetaNet**: é’ˆå¯¹åŠ¨æ€å›¾ï¼ˆDynamic Graphsï¼‰çš„å…ƒå­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨æ‹“æ‰‘ç‰¹å¾ï¼ˆTopological Featuresï¼‰æ¥æ•æ‰å›¾çš„æ¼”å˜ã€‚\n*   **How Significant Are the Real Performance Gains? An Unbiased Evaluation Framework for GraphRAG**: æ³¼å†·æ°´+1ã€‚æŒ‡å‡º GraphRAG æŠ¥å‘Šçš„æ€§èƒ½å¢ç›Šå¯èƒ½å› è¯„ä¼°åå·®è¢«å¤¸å¤§äº†ï¼Œæå‡ºæ›´æ— åçš„è¯„ä¼°æ¡†æ¶ã€‚\n\nç¥å¤§å®¶é˜…è¯»æ„‰å¿«ï¼Œå‘¨æœ«æ„‰å¿«ï¼",
  "papers": [
    {
      "arxiv_id": "2506.00756v1",
      "title": "\"Who experiences large model decay and why?\" A Hierarchical Framework for Diagnosing Heterogeneous Performance Drift",
      "title_zh": "â€œè°ç»å†äº†ä¸¥é‡çš„æ¨¡å‹è¡°å‡ï¼ŒåŸå› ä½•åœ¨ï¼Ÿâ€ï¼šä¸€ç§ç”¨äºè¯Šæ–­å¼‚è´¨æ€§æ€§èƒ½æ¼‚ç§»çš„å±‚çº§åŒ–æ¡†æ¶",
      "authors": [
        "Harvineet Singh",
        "Fan Xia",
        "Alexej Gossmann",
        "Andrew Chuang",
        "Julian C. Hong",
        "Jean Feng"
      ],
      "abstract": "Machine learning (ML) models frequently experience performance degradation when deployed in new contexts. Such degradation is rarely uniform: some subgroups may suffer large performance decay while others may not. Understanding where and how large differences in performance arise is critical for designing targeted corrective actions that mitigate decay for the most affected subgroups while minimizing any unintended effects. Current approaches do not provide such detailed insight, as they either (i) explain how average performance shifts arise or (ii) identify adversely affected subgroups without insight into how this occurred. To this end, we introduce a Subgroup-scanning Hierarchical Inference Framework for performance drifT (SHIFT). SHIFT first asks \"Is there any subgroup with unacceptably large performance decay due to covariate/outcome shifts?\" (Where?) and, if so, dives deeper to ask \"Can we explain this using more detailed variable(subset)-specific shifts?\" (How?). In real-world experiments, we find that SHIFT identifies interpretable subgroups affected by performance decay, and suggests targeted actions that effectively mitigate the decay.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ (ML)æ¨¡å‹åœ¨éƒ¨ç½²åˆ°æ–°ç¯å¢ƒæ—¶ç»å¸¸å‡ºç°çš„éå‡åŒ€æ€§èƒ½è¡°å‡(performance degradation)é—®é¢˜ï¼Œæå‡ºäº†åä¸ºSHIFT(Subgroup-scanning Hierarchical Inference Framework for performance drifT)çš„å±‚çº§æ¨ç†æ¡†æ¶ã€‚ç”±äºç°æœ‰æ–¹æ³•é€šå¸¸åªèƒ½è§£é‡Šå¹³å‡æ€§èƒ½åç§»(performance shifts)æˆ–ä»…èƒ½è¯†åˆ«å—å½±å“å­ç»„è€Œæ— æ³•è§£é‡Šå…¶å‘ç”Ÿæœºåˆ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡â€œåœ¨å“ªé‡Œâ€å’Œâ€œå¦‚ä½•å‘ç”Ÿâ€ä¸¤ä¸ªå±‚çº§è¿›è¡Œæ·±å…¥è¯Šæ–­ã€‚SHIFTé¦–å…ˆè¯†åˆ«å‡ºç”±äºåå˜é‡(covariate)æˆ–ç»“æœåç§»(outcome shifts)å¯¼è‡´æ€§èƒ½å¤§å¹…ä¸‹é™çš„ç‰¹å®šå­ç»„ï¼Œéšåé€šè¿‡åˆ†æå…·ä½“çš„å˜é‡å­é›†åç§»æ¥æä¾›è¯¦ç»†çš„è§£é‡Šã€‚åœ¨å®é™…å®éªŒä¸­ï¼ŒSHIFTèƒ½å¤Ÿè¯†åˆ«å‡ºå…·æœ‰é«˜åº¦å¯è§£é‡Šæ€§çš„å—å½±å“å­ç»„ï¼Œå¹¶æ®æ­¤æå‡ºé’ˆå¯¹æ€§çš„æ”¹è¿›å»ºè®®ï¼Œä»è€Œæœ‰æ•ˆåœ°ç¼“è§£äº†æ¨¡å‹çš„æ€§èƒ½è¡°å‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 9 figures, 8 tables, 18 pages appendix. To be published in Proceedings of the 42nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00756v1",
      "published_date": "2025-05-31 23:50:54 UTC",
      "updated_date": "2025-05-31 23:50:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:52:32.916248+00:00"
    },
    {
      "arxiv_id": "2506.00751v1",
      "title": "Alignment Revisited: Are Large Language Models Consistent in Stated and Revealed Preferences?",
      "title_zh": "é‡æ–°å®¡è§†å¯¹é½ï¼šå¤§è¯­è¨€æ¨¡å‹åœ¨é™ˆè¿°åå¥½ä¸æ˜¾ç¤ºåå¥½ä¸Šæ˜¯å¦ä¸€è‡´ï¼Ÿ",
      "authors": [
        "Zhuojun Gu",
        "Quan Wang",
        "Shuchu Han"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) highlight the need to align their behaviors with human values. A critical, yet understudied, issue is the potential divergence between an LLM's stated preferences (its reported alignment with general principles) and its revealed preferences (inferred from decisions in contextualized scenarios). Such deviations raise fundamental concerns for the interpretability, trustworthiness, reasoning transparency, and ethical deployment of LLMs, particularly in high-stakes applications. This work formally defines and proposes a method to measure this preference deviation. We investigate how LLMs may activate different guiding principles in specific contexts, leading to choices that diverge from previously stated general principles. Our approach involves crafting a rich dataset of well-designed prompts as a series of forced binary choices and presenting them to LLMs. We compare LLM responses to general principle prompts stated preference with LLM responses to contextualized prompts revealed preference, using metrics like KL divergence to quantify the deviation. We repeat the analysis across different categories of preferences and on four mainstream LLMs and find that a minor change in prompt format can often pivot the preferred choice regardless of the preference categories and LLMs in the test. This prevalent phenomenon highlights the lack of understanding and control of the LLM decision-making competence. Our study will be crucial for integrating LLMs into services, especially those that interact directly with humans, where morality, fairness, and social responsibilities are crucial dimensions. Furthermore, identifying or being aware of such deviation will be critically important as LLMs are increasingly envisioned for autonomous agentic tasks where continuous human evaluation of all LLMs' intermediary decision-making steps is impossible.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¯¹é½ï¼ˆAlignmentï¼‰è¿‡ç¨‹ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå³é™ˆè¿°åå¥½ï¼ˆStated Preferencesï¼‰ä¸é€šè¿‡å…·ä½“å†³ç­–æ­ç¤ºçš„åå¥½ï¼ˆRevealed Preferencesï¼‰ä¹‹é—´å¯èƒ½å­˜åœ¨çš„èƒŒç¦»ã€‚ç ”ç©¶è€…å½¢å¼åŒ–å®šä¹‰äº†è¿™ç§åå¥½åå·®ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºå¼ºåˆ¶äºŒé€‰ä¸€æç¤ºè¯ï¼ˆPromptsï¼‰æ•°æ®é›†çš„è¡¡é‡æ–¹æ³•ï¼Œåˆ©ç”¨KLæ•£åº¦ï¼ˆKL Divergenceï¼‰ç­‰æŒ‡æ ‡æ¥é‡åŒ–æ¨¡å‹åœ¨ä¸€èˆ¬åŸåˆ™ä¸ç‰¹å®šæƒ…å¢ƒä¸‹çš„é€‰æ‹©å·®å¼‚ã€‚é€šè¿‡å¯¹å››ç§ä¸»æµLLMsçš„å®éªŒåˆ†æï¼Œç ”ç©¶å‘ç°å³ä¾¿æç¤ºè¯æ ¼å¼å‘ç”Ÿå¾®å°æ”¹å˜ï¼Œæ¨¡å‹çš„åå¥½ä¹Ÿå¸¸ä¼šå‘ç”Ÿåè½¬ï¼Œä¸”è¿™ä¸€ç°è±¡åœ¨ä¸åŒæ¨¡å‹å’Œåå¥½ç±»åˆ«ä¸­æ™®éå­˜åœ¨ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å½“å‰LLMsåœ¨å†³ç­–ä¸€è‡´æ€§å’Œå¯æ§æ€§æ–¹é¢çš„å±€é™æ€§ï¼Œå¯¹äºé“å¾·ã€å…¬å¹³æ€§è¦æ±‚æé«˜çš„åº”ç”¨åœºæ™¯ä»¥åŠè‡ªä¸»æ™ºèƒ½ä½“ï¼ˆAutonomous Agentsï¼‰çš„éƒ¨ç½²å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨ç¼ºä¹æŒç»­äººç±»è¯„ä¼°çš„æƒ…å†µä¸‹ï¼Œè¯†åˆ«å’Œç†è§£æ¨¡å‹å†³ç­–åå·®å¯¹äºç¡®ä¿äººå·¥æ™ºèƒ½å®‰å…¨æ€§å’Œå¯ä¿¡åº¦çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00751v1",
      "published_date": "2025-05-31 23:38:48 UTC",
      "updated_date": "2025-05-31 23:38:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:52:27.055998+00:00"
    },
    {
      "arxiv_id": "2506.00750v2",
      "title": "CodeSense: a Real-World Benchmark and Dataset for Code Semantic Reasoning",
      "title_zh": "CodeSenseï¼šé¢å‘ä»£ç è¯­ä¹‰æ¨ç†çš„çœŸå®åœºæ™¯åŸºå‡†ä¸æ•°æ®é›†",
      "authors": [
        "Monoshi Kumar Roy",
        "Simin Chen",
        "Benjamin Steenhoek",
        "Jinjun Peng",
        "Gail Kaiser",
        "Baishakhi Ray",
        "Wei Le"
      ],
      "abstract": "Understanding and reasoning about code semantics is essential for enhancing code LLMs' abilities to solve real-world software engineering (SE) tasks. Although several code reasoning benchmarks exist, most rely on synthetic datasets or educational coding problems and focus on coarse-grained reasoning tasks such as input/output prediction, limiting their effectiveness in evaluating LLMs in practical SE contexts. To bridge this gap, we propose CodeSense, the first benchmark that makes available a spectrum of fine-grained code reasoning tasks concerned with the software engineering of real-world code. We collected Python, C and Java software projects from real-world repositories. We executed tests from these repositories, collected their execution traces, and constructed a ground truth dataset for fine-grained semantic reasoning tasks. We then performed comprehensive evaluations on state-of-the-art LLMs. Our results show a clear performance gap for the models to handle fine-grained reasoning tasks. Although prompting techniques such as chain-of-thought and in-context learning helped, the lack of code semantics in LLMs fundamentally limit models' capabilities of code reasoning. Besides dataset, benchmark and evaluation, our work produced an execution tracing framework and tool set that make it easy to collect ground truth for fine-grained SE reasoning tasks, offering a strong basis for future benchmark construction and model post training. Our code and data are located at https://codesense-bench.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CodeSenseï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹çœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹(Software Engineering)ä¸­ç»†ç²’åº¦ä»£ç è¯­ä¹‰æ¨ç†(Code Semantic Reasoning)ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶å›¢é˜Ÿä»çœŸå®ä»£ç åº“ä¸­æ”¶é›†äº†Pythonã€Cå’ŒJavaé¡¹ç›®ï¼Œå¹¶é€šè¿‡æ•è·æµ‹è¯•æ‰§è¡Œè½¨è¿¹(Execution Traces)æ„å»ºäº†ç»†ç²’åº¦è¯­ä¹‰æ¨ç†ä»»åŠ¡çš„åŸºå‡†çœŸå®(Ground Truth)æ•°æ®é›†ã€‚å¯¹å½“å‰å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œçš„å…¨é¢è¯„ä¼°æ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨å¤„ç†ç»†ç²’åº¦æ¨ç†ä»»åŠ¡æ—¶å­˜åœ¨æ˜æ˜¾çš„æ€§èƒ½å·®è·ã€‚è™½ç„¶é“¾å¼æ€ç»´(Chain-of-Thought)å’Œä¸Šä¸‹æ–‡å­¦ä¹ (In-Context Learning)ç­‰æç¤ºæŠ€æœ¯èƒ½æä¾›ä¸€å®šå¸®åŠ©ï¼Œä½†LLMså¯¹ä»£ç è¯­ä¹‰ç†è§£çš„ç¼ºå¤±ä»æ ¹æœ¬ä¸Šé™åˆ¶äº†å…¶æ¨ç†èƒ½åŠ›ã€‚é™¤äº†æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å¤–ï¼Œè¯¥å·¥ä½œè¿˜äº§å‡ºäº†ä¸€å¥—æ‰§è¡Œè¿½è¸ªæ¡†æ¶å’Œå·¥å…·é›†ï¼Œä¸ºè‡ªåŠ¨æ”¶é›†ç»†ç²’åº¦è½¯ä»¶å·¥ç¨‹æ¨ç†ä»»åŠ¡çš„æ•°æ®æä¾›äº†ä¾¿åˆ©ã€‚è¿™ä¸€æˆæœä¸ºæœªæ¥åŸºå‡†æµ‹è¯•çš„æ„å»ºå’Œæ¨¡å‹çš„åæœŸè®­ç»ƒ(Post Training)å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00750v2",
      "published_date": "2025-05-31 23:32:01 UTC",
      "updated_date": "2025-10-02 16:10:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:52:25.138219+00:00"
    },
    {
      "arxiv_id": "2506.00743v1",
      "title": "Assortment of Attention Heads: Accelerating Federated PEFT with Head Pruning and Strategic Client Selection",
      "title_zh": "æ³¨æ„åŠ›å¤´ç»„åˆï¼šåˆ©ç”¨å¤´å‰ªæä¸ç­–ç•¥æ€§å®¢æˆ·ç«¯é€‰æ‹©åŠ é€Ÿè”é‚¦ PEFT",
      "authors": [
        "Yeshwanth Venkatesha",
        "Souvik Kundu",
        "Priyadarshini Panda"
      ],
      "abstract": "Parameter Efficient Fine-Tuning (PEFT) has become the de-facto approach in adapting Large Language Models (LLMs) for downstream tasks in Natural Language Processing. However, its adoption in privacy-preserving distributed learning frameworks, such as Federated Learning (FL), remains relatively limited. This is mainly due to challenges specific to FL, such as resource-constrained devices and diverse data distributions among clients. In this paper, we propose an efficient method to perform PEFT within the FL framework for Multi-Head Attention (MHA) based language models. We address the challenges through head pruning, a novel head-specific weighted aggregation mechanism, and a client selection strategy. Head pruning minimizes training complexity within the clients, guided by the importance score computed based on the confidence of the attention head. Weighted aggregation of heads ensures the global model captures crucial updates from diverse clients complementing our client selection strategy. We show results on the MultiNLI benchmark along with 20 Newsgroups, XL-Sum, and E2E NLG datasets. We use the MultiNLI dataset and T5-small model with LoRA as our PEFT method, attaining sparsity levels of up to 90%, resulting in a communication advantage of up to 1.8x and a reduction in training OPs of 3.9x while maintaining the accuracy drop under 2%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è”é‚¦å­¦ä¹ (Federated Learning)æ¡†æ¶ä¸­å› è®¾å¤‡èµ„æºå—é™å’Œæ•°æ®åˆ†å¸ƒå¼‚æ„è€Œé¢ä¸´çš„å¾®è°ƒéš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ—¨åœ¨åŠ é€Ÿè”é‚¦å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)çš„æ–°å‹æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆæ ¸å¿ƒé‡‡ç”¨äº†å¤´éƒ¨å‰ªæ(Head Pruning)æŠ€æœ¯ï¼Œåˆ©ç”¨åŸºäºæ³¨æ„åŠ›å¤´(Attention Head)ç½®ä¿¡åº¦çš„é‡è¦æ€§è¯„åˆ†æ¥é™ä½å®¢æˆ·ç«¯è®­ç»ƒå¤æ‚åº¦ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥ç‰¹å®šå¤´éƒ¨çš„åŠ æƒèšåˆæœºåˆ¶ä¸å®¢æˆ·ç«¯é€‰æ‹©ç­–ç•¥ï¼Œè¯¥æ–¹æ³•ç¡®ä¿äº†å…¨å±€æ¨¡å‹èƒ½æœ‰æ•ˆæ•è·å¼‚æ„å®¢æˆ·ç«¯çš„è´¡çŒ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨MultiNLIç­‰æ•°æ®é›†ä¸Šç»“åˆT5-smallæ¨¡å‹å’ŒLoRAæŠ€æœ¯ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå‡†ç¡®ç‡ä¸‹é™ä½äº2%çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾90%çš„ç¨€ç–åº¦ã€‚æœ€ç»ˆï¼Œè¯¥ç ”ç©¶æˆåŠŸå®ç°äº†1.8å€çš„é€šä¿¡å¢ç›Šå’Œ3.9å€çš„è®­ç»ƒè®¡ç®—é‡(OPs)å‡å°‘ï¼Œä¸ºåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹é«˜æ•ˆé€‚é…å¤šå¤´æ³¨æ„åŠ›(MHA)æ¨¡å‹æä¾›äº†æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00743v1",
      "published_date": "2025-05-31 23:09:26 UTC",
      "updated_date": "2025-05-31 23:09:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:52:36.359521+00:00"
    },
    {
      "arxiv_id": "2506.00742v1",
      "title": "ArtiScene: Language-Driven Artistic 3D Scene Generation Through Image Intermediary",
      "title_zh": "ArtiSceneï¼šåŸºäºå›¾åƒä¸­ä»‹çš„è¯­è¨€é©±åŠ¨è‰ºæœ¯åŒ– 3D åœºæ™¯ç”Ÿæˆ",
      "authors": [
        "Zeqi Gu",
        "Yin Cui",
        "Zhaoshuo Li",
        "Fangyin Wei",
        "Yunhao Ge",
        "Jinwei Gu",
        "Ming-Yu Liu",
        "Abe Davis",
        "Yifan Ding"
      ],
      "abstract": "Designing 3D scenes is traditionally a challenging task that demands both artistic expertise and proficiency with complex software. Recent advances in text-to-3D generation have greatly simplified this process by letting users create scenes based on simple text descriptions. However, as these methods generally require extra training or in-context learning, their performance is often hindered by the limited availability of high-quality 3D data. In contrast, modern text-to-image models learned from web-scale images can generate scenes with diverse, reliable spatial layouts and consistent, visually appealing styles. Our key insight is that instead of learning directly from 3D scenes, we can leverage generated 2D images as an intermediary to guide 3D synthesis. In light of this, we introduce ArtiScene, a training-free automated pipeline for scene design that integrates the flexibility of free-form text-to-image generation with the diversity and reliability of 2D intermediary layouts.\n  First, we generate 2D images from a scene description, then extract the shape and appearance of objects to create 3D models. These models are assembled into the final scene using geometry, position, and pose information derived from the same intermediary image. Being generalizable to a wide range of scenes and styles, ArtiScene outperforms state-of-the-art benchmarks by a large margin in layout and aesthetic quality by quantitative metrics. It also averages a 74.89% winning rate in extensive user studies and 95.07% in GPT-4o evaluation. Project page: https://artiscene-cvpr.github.io/",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»ŸText-to-3Dç”Ÿæˆä¸­é«˜è´¨é‡3Dæ•°æ®çŸ­ç¼ºå¯¼è‡´çš„ç©ºé—´å¸ƒå±€å’Œé£æ ¼å—é™é—®é¢˜ï¼Œæå‡ºäº†ArtiSceneï¼Œä¸€ç§æ— éœ€è®­ç»ƒ(training-free)çš„è‡ªåŠ¨åŒ–3Dåœºæ™¯ç”Ÿæˆæµæ°´çº¿ã€‚ArtiSceneçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒ(Text-to-Image)æ¨¡å‹ç”Ÿæˆçš„2Då›¾åƒä½œä¸ºä¸­é—´åª’ä»‹(image intermediary)ï¼Œä»è€Œå¼•å¯¼3Dåœºæ™¯çš„åˆæˆã€‚è¯¥ç³»ç»Ÿé¦–å…ˆä»æ–‡æœ¬æè¿°ç”Ÿæˆ2Då›¾åƒï¼Œå¹¶ä»ä¸­æå–ç‰©ä½“çš„å½¢çŠ¶(shape)ã€å¤–è§‚(appearance)ä»¥åŠå‡ ä½•ä½ç½®å’Œå§¿æ€(pose)ä¿¡æ¯ã€‚é€šè¿‡å°†è¿™äº›æ¨å¯¼å‡ºçš„ä¿¡æ¯æ•´åˆï¼ŒArtiSceneèƒ½å¤Ÿå°†ç¦»æ•£çš„3Dæ¨¡å‹ç»„è£…æˆå…·æœ‰ä¸€è‡´ç©ºé—´å¸ƒå±€å’Œè‰ºæœ¯é£æ ¼çš„å®Œæ•´åœºæ™¯ã€‚å®éªŒè¡¨æ˜ï¼ŒArtiSceneåœ¨å¸ƒå±€å’Œç¾å­¦è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºå‡†æ¨¡å‹ï¼Œä¸”å…·å¤‡å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ç”¨æˆ·ç ”ç©¶å’ŒGPT-4oè¯„ä¼°ä¸­ï¼Œè¯¥æ–¹æ³•åˆ†åˆ«è·å¾—äº†74.89%å’Œ95.07%çš„é«˜èƒœç‡ï¼Œè¯æ˜äº†å…¶åœ¨ç”Ÿæˆå¤æ‚è‰ºæœ¯3Dåœºæ™¯æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR",
      "pdf_url": "https://arxiv.org/pdf/2506.00742v1",
      "published_date": "2025-05-31 23:03:54 UTC",
      "updated_date": "2025-05-31 23:03:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:52:28.766155+00:00"
    },
    {
      "arxiv_id": "2506.00740v1",
      "title": "Length Aware Speech Translation for Video Dubbing",
      "title_zh": "é¢å‘è§†é¢‘é…éŸ³çš„é•¿åº¦æ„ŸçŸ¥è¯­éŸ³ç¿»è¯‘",
      "authors": [
        "Harveen Singh Chadha",
        "Aswin Shanmugam Subramanian",
        "Vikas Joshi",
        "Shubham Bansal",
        "Jian Xue",
        "Rupeshkumar Mehta",
        "Jinyu Li"
      ],
      "abstract": "In video dubbing, aligning translated audio with the source audio is a significant challenge. Our focus is on achieving this efficiently, tailored for real-time, on-device video dubbing scenarios. We developed a phoneme-based end-to-end length-sensitive speech translation (LSST) model, which generates translations of varying lengths short, normal, and long using predefined tags. Additionally, we introduced length-aware beam search (LABS), an efficient approach to generate translations of different lengths in a single decoding pass. This approach maintained comparable BLEU scores compared to a baseline without length awareness while significantly enhancing synchronization quality between source and target audio, achieving a mean opinion score (MOS) gain of 0.34 for Spanish and 0.65 for Korean, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘é…éŸ³(Video Dubbing)ä¸­è¯‘æ–‡éŸ³é¢‘ä¸åŸéŸ³é¢‘å¯¹é½çš„æ˜¾è‘—æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§é€‚ç”¨äºå®æ—¶ã€è®¾å¤‡ç«¯åœºæ™¯çš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ç§åŸºäºéŸ³ç´ (phoneme-based)çš„ç«¯åˆ°ç«¯é•¿åº¦æ•æ„Ÿè¯­éŸ³ç¿»è¯‘(LSST)æ¨¡å‹ï¼Œåˆ©ç”¨é¢„å®šä¹‰æ ‡ç­¾ç”ŸæˆçŸ­ã€æ­£å¸¸å’Œé•¿ä¸‰ç§é•¿åº¦çš„ç¿»è¯‘ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†é•¿åº¦æ„ŸçŸ¥é›†æŸæœç´¢(LABS)ï¼Œé€šè¿‡å•æ¬¡è§£ç è¿‡ç¨‹å³å¯é«˜æ•ˆç”Ÿæˆå¤šç§é•¿åº¦çš„è¯‘æ–‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒä¸åŸºå‡†æ¨¡å‹ç›¸å½“çš„ BLEU åˆ†æ•°çš„åŒæ—¶ï¼Œæ˜¾è‘—å¢å¼ºäº†æºéŸ³é¢‘ä¸ç›®æ ‡éŸ³é¢‘çš„åŒæ­¥è´¨é‡ã€‚åœ¨è¥¿ç­ç‰™è¯­å’ŒéŸ©è¯­çš„æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ¡ˆåˆ†åˆ«å®ç°äº† 0.34 å’Œ 0.65 çš„å¹³å‡æ„è§å¾—åˆ†(MOS)å¢ç›Šï¼Œè¯æ˜äº†å…¶åœ¨æå‡é…éŸ³åŒæ­¥æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper was accepted to Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00740v1",
      "published_date": "2025-05-31 23:01:50 UTC",
      "updated_date": "2025-05-31 23:01:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:53:08.995432+00:00"
    },
    {
      "arxiv_id": "2506.03194v4",
      "title": "HueManity: Probing Fine-Grained Visual Perception in MLLMs",
      "title_zh": "HueManityï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥æ¢ç©¶",
      "authors": [
        "Rynaa Grover",
        "Jayant Sravan Tamarapalli",
        "Sahiti Yerramilli",
        "Nilay Pande"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) excel at high-level visual reasoning, but their performance on nuanced perceptual tasks remains surprisingly limited. We present HueManity, a benchmark designed to assess visual perception in MLLMs. The dataset comprises 83,850 images featuring two-character alphanumeric strings embedded in Ishihara test style dot patterns, challenging models on precise pattern recognition. Our evaluation of nine state-of-the-art MLLMs on HueManity demonstrates a significant performance deficit compared to human and traditional computer vision baselines. The best-performing MLLM achieved a 33.6% accuracy on the numeric `easy' task and a striking 3% on the alphanumeric `hard' task. In contrast, human participants achieved near-perfect scores (100% and 95.6%), and a fine-tuned ResNet50 model reached accuracies of 96.5% and 94.5%. These results highlight a critical gap in the visual capabilities of current MLLMs. Our analysis further explores potential architectural and training-paradigm factors contributing to this perceptual gap in MLLMs. We open-source HueManity dataset and code to foster further research in improving perceptual robustness of MLLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HueManityï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥æ•°æ®é›†åŒ…å« 83,850 å¼ å›¾åƒï¼Œé‡‡ç”¨ç±»ä¼¼ Ishihara è‰²ç›²æµ‹è¯•çš„åœ†ç‚¹å›¾æ¡ˆï¼Œå¹¶åœ¨å…¶ä¸­åµŒå…¥äº†åŒå­—ç¬¦çš„å­—æ¯æ•°å­—å­—ç¬¦ä¸²ï¼Œä»¥æ­¤æŒ‘æˆ˜æ¨¡å‹çš„ç²¾ç¡®æ¨¡å¼è¯†åˆ«èƒ½åŠ›ã€‚é€šè¿‡å¯¹ä¹ç§æœ€å…ˆè¿›çš„ MLLMs è¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶å‘ç°è¿™äº›æ¨¡å‹åœ¨æ„ŸçŸ¥ä»»åŠ¡ä¸Šå­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½ç¼ºé™·ï¼Œå…¶ä¸­è¡¨ç°æœ€å¥½çš„æ¨¡å‹åœ¨ç®€å•æ•°å­—ä»»åŠ¡å’Œå›°éš¾å­—æ¯æ•°å­—ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«ä»…ä¸º 33.6% å’Œ 3%ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»å‚ä¸è€…å’Œç»è¿‡å¾®è°ƒçš„ ResNet50 æ¨¡å‹å‡å®ç°äº†æ¥è¿‘å®Œç¾æˆ–æé«˜çš„å‡†ç¡®ç‡ã€‚è¿™äº›å®éªŒç»“æœå‡¸æ˜¾äº†å½“å‰ MLLMs è§†è§‰èƒ½åŠ›ä¸­çš„å…³é”®çŸ­æ¿ï¼Œç ”ç©¶è¿˜è¿›ä¸€æ­¥æ¢è®¨äº†å¯èƒ½å¯¼è‡´è¿™ä¸€æ„ŸçŸ¥å·®è·çš„æ¶æ„ä¸è®­ç»ƒèŒƒå¼å› ç´ ã€‚ä½œè€…å·²å¼€æº HueManity æ•°æ®é›†å’Œä»£ç ï¼Œä»¥æœŸæ¨åŠ¨æå‡ MLLMs æ„ŸçŸ¥é²æ£’æ€§çš„ç›¸å…³ç ”ç©¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03194v4",
      "published_date": "2025-05-31 22:59:48 UTC",
      "updated_date": "2025-09-12 19:13:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:53:05.661506+00:00"
    },
    {
      "arxiv_id": "2506.02046v1",
      "title": "Machine vs Machine: Using AI to Tackle Generative AI Threats in Assessment",
      "title_zh": "æœºå™¨å¯¹æŠ—æœºå™¨ï¼šåˆ©ç”¨äººå·¥æ™ºèƒ½åº”å¯¹è¯„ä¼°ä¸­çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¨èƒ",
      "authors": [
        "Mohammad Saleh Torkestani",
        "Taha Mansouri"
      ],
      "abstract": "This paper presents a theoretical framework for addressing the challenges posed by generative artificial intelligence (AI) in higher education assessment through a machine-versus-machine approach. Large language models like GPT-4, Claude, and Llama increasingly demonstrate the ability to produce sophisticated academic content, traditional assessment methods face an existential threat, with surveys indicating 74-92% of students experimenting with these tools for academic purposes. Current responses, ranging from detection software to manual assessment redesign, show significant limitations: detection tools demonstrate bias against non-native English writers and can be easily circumvented, while manual frameworks rely heavily on subjective judgment and assume static AI capabilities. This paper introduces a dual strategy paradigm combining static analysis and dynamic testing to create a comprehensive theoretical framework for assessment vulnerability evaluation. The static analysis component comprises eight theoretically justified elements: specificity and contextualization, temporal relevance, process visibility requirements, personalization elements, resource accessibility, multimodal integration, ethical reasoning requirements, and collaborative elements. Each element addresses specific limitations in generative AI capabilities, creating barriers that distinguish authentic human learning from AI-generated simulation. The dynamic testing component provides a complementary approach through simulation-based vulnerability assessment, addressing limitations in pattern-based analysis. The paper presents a theoretical framework for vulnerability scoring, including the conceptual basis for quantitative assessment, weighting frameworks, and threshold determination theory.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenerative AIï¼‰å¯¹é«˜ç­‰æ•™è‚²è¯„ä¼°å¸¦æ¥çš„ä¸¥å³»æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªâ€œæœºå™¨å¯¹æŠ—æœºå™¨ï¼ˆMachine-versus-Machineï¼‰â€çš„ç†è®ºæ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ£€æµ‹å·¥å…·å¯¹éè‹±è¯­æ¯è¯­è€…å­˜åœ¨åå·®ä»¥åŠå®¹æ˜“è¢«è§„é¿çš„é—®é¢˜ï¼Œè®ºæ–‡å¼•å…¥äº†ä¸€ç§ç»“åˆé™æ€åˆ†æï¼ˆStatic Analysisï¼‰ä¸åŠ¨æ€æµ‹è¯•ï¼ˆDynamic Testingï¼‰çš„åŒç­–ç•¥èŒƒå¼ã€‚é™æ€åˆ†æéƒ¨åˆ†ç”±Specificity and Contextualizationã€Temporal Relevanceå’ŒProcess Visibility Requirementsç­‰å…«ä¸ªè¦ç´ ç»„æˆï¼Œæ—¨åœ¨å»ºç«‹èƒ½åŒºåˆ†äººç±»çœŸå®å­¦ä¹ ä¸AIæ¨¡æ‹Ÿçš„å£å’ã€‚åŠ¨æ€æµ‹è¯•åˆ™é€šè¿‡åŸºäºæ¨¡æ‹Ÿçš„æ¼æ´è¯„ä¼°ï¼Œå¼¥è¡¥äº†ä¼ ç»Ÿæ¨¡å¼åˆ†æåœ¨åº”å¯¹AIåŠ¨æ€æ¼”è¿›æ–¹é¢çš„å±€é™ã€‚æœ€åï¼Œè¯¥æ¡†æ¶æä¾›äº†ä¸€å¥—åŒ…æ‹¬é‡åŒ–è¯„ä¼°ã€åŠ æƒä½“ç³»å’Œé˜ˆå€¼åˆ¤å®šç†è®ºåœ¨å†…çš„æ¼æ´è¯„åˆ†ï¼ˆVulnerability Scoringï¼‰æ–¹æ¡ˆï¼Œä¸ºåº”å¯¹ç”Ÿæˆå¼AIå¯¹å­¦æœ¯è¯šä¿¡çš„å¨èƒæä¾›äº†ç³»ç»Ÿæ€§çš„ç†è®ºæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Paper presented at the Learning, Teaching & Student Experience 2025 Conference. The Chartered Association of Business Schools (CABS), Nottingham, UK",
      "pdf_url": "https://arxiv.org/pdf/2506.02046v1",
      "published_date": "2025-05-31 22:29:43 UTC",
      "updated_date": "2025-05-31 22:29:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:52:46.059288+00:00"
    },
    {
      "arxiv_id": "2506.00731v2",
      "title": "iPINNER: An Iterative Physics-Informed Neural Network with Ensemble Kalman Filter",
      "title_zh": "iPINNERï¼šç»“åˆé›†åˆå¡å°”æ›¼æ»¤æ³¢çš„è¿­ä»£å¼ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ",
      "authors": [
        "Binghang Lu",
        "Changhong Mou",
        "Guang Lin"
      ],
      "abstract": "Physics-informed neural networks (PINNs) have emerged as a powerful tool for solving forward and inverse problems involving partial differential equations (PDEs) by incorporating physical laws into the training process. However, the performance of PINNs is often hindered in real-world scenarios involving noisy observational data and missing physics, particularly in inverse problems. In this work, we propose an iterative multi-objective PINN ensemble Kalman filter (iPINNER) framework that improves the robustness and accuracy of PINNs in both forward and inverse problems by using the \\textit{ensemble Kalman filter} and the \\textit{non-dominated sorting genetic algorithm} III (NSGA-III). Specifically, NSGA-III is used as a multi-objective optimizer that can generate various ensemble members of PINNs along the optimal Pareto front, while accounting the model uncertainty in the solution space. These ensemble members are then utilized within the EnKF to assimilate noisy observational data. The EnKF's analysis is subsequently used to refine the data loss component for retraining the PINNs, thereby iteratively updating their parameters. The iterative procedure generates improved solutions to the PDEs. The proposed method is tested on two benchmark problems: the one-dimensional viscous Burgers equation and the time-fractional mixed diffusion-wave equation (TFMDWE). The numerical results show it outperforms standard PINNs in handling noisy data and missing physics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† iPINNERï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº† ensemble Kalman filter (EnKF) å’Œ NSGA-III çš„è¿­ä»£å¤šç›®æ ‡ Physics-informed neural networks (PINNs) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ PINNs åœ¨å¤„ç†å¸¦å™ªå£°è§‚æµ‹æ•°æ®å’Œç‰©ç†ç¼ºå¤± (missing physics) æ—¶çš„æ€§èƒ½å±€é™ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ NSGA-III ä½œä¸ºå¤šç›®æ ‡ä¼˜åŒ–å™¨ï¼Œåœ¨æœ€ä¼˜ Pareto å‰æ²¿ç”Ÿæˆå¤šæ ·åŒ–çš„ PINN é›†æˆæˆå‘˜ï¼Œä»è€Œæœ‰æ•ˆæ•æ‰è§£ç©ºé—´ä¸­çš„æ¨¡å‹ä¸ç¡®å®šæ€§ã€‚è¿™äº›é›†æˆæˆå‘˜éšåè¢«ç”¨äº EnKF ä»¥åŒåŒ–å™ªå£°æ•°æ®ï¼Œå¹¶é€šè¿‡å…¶åˆ†æç»“æœæŒ‡å¯¼æ•°æ®æŸå¤±é¡¹çš„ç²¾ç»†åŒ–ï¼Œå®ç° PINN å‚æ•°çš„è¿­ä»£æ›´æ–°ã€‚åœ¨ ä¸€ç»´ç²˜æ€§ Burgers æ–¹ç¨‹å’Œæ—¶é—´åˆ†æ•°é˜¶æ··åˆæ‰©æ•£æ³¢åŠ¨æ–¹ç¨‹ (TFMDWE) ä¸Šçš„æ•°å€¼å®éªŒéªŒè¯äº† iPINNER çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨å¤„ç†å¤æ‚çš„ forward å’Œ inverse problems æ—¶ï¼Œå…¶å‡†ç¡®æ€§ä¸é²æ£’æ€§å‡æ˜¾è‘—ä¼˜äºæ ‡å‡† PINNsã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00731v2",
      "published_date": "2025-05-31 22:20:18 UTC",
      "updated_date": "2025-12-12 16:05:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:52:50.436293+00:00"
    },
    {
      "arxiv_id": "2506.00723v1",
      "title": "Pitfalls in Evaluating Language Model Forecasters",
      "title_zh": "è¯„ä¼°è¯­è¨€æ¨¡å‹é¢„æµ‹å™¨çš„é™·é˜±",
      "authors": [
        "Daniel Paleka",
        "Shashwat Goel",
        "Jonas Geiping",
        "Florian TramÃ¨r"
      ],
      "abstract": "Large language models (LLMs) have recently been applied to forecasting tasks, with some works claiming these systems match or exceed human performance. In this paper, we argue that, as a community, we should be careful about such conclusions as evaluating LLM forecasters presents unique challenges. We identify two broad categories of issues: (1) difficulty in trusting evaluation results due to many forms of temporal leakage, and (2) difficulty in extrapolating from evaluation performance to real-world forecasting. Through systematic analysis and concrete examples from prior work, we demonstrate how evaluation flaws can raise concerns about current and future performance claims. We argue that more rigorous evaluation methodologies are needed to confidently assess the forecasting abilities of LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°è¾¾åˆ°æˆ–è¶…è¶Šäººç±»æ°´å¹³çš„ç°æœ‰ä¸»å¼ æå‡ºäº†è´¨ç–‘ï¼Œå¼ºè°ƒè¯„ä¼° LLM Forecasters å­˜åœ¨ç‹¬ç‰¹æŒ‘æˆ˜ã€‚ä½œè€…è¯†åˆ«å¹¶åˆ†æäº†ä¸¤å¤§æ ¸å¿ƒé—®é¢˜ï¼šä¸€æ˜¯å¤šç§å½¢å¼çš„ Temporal Leakageï¼ˆæ—¶é—´æ³„éœ²ï¼‰å¯¼è‡´è¯„ä¼°ç»“æœéš¾ä»¥ä»¤äººä¿¡æœï¼ŒäºŒæ˜¯å®éªŒç¯å¢ƒä¸‹çš„è¯„ä¼°æ€§èƒ½éš¾ä»¥æœ‰æ•ˆå¤–æ¨è‡³ Real-world Forecastingï¼ˆç°å®ä¸–ç•Œé¢„æµ‹ï¼‰ã€‚é€šè¿‡å¯¹æ—¢å¾€ç ”ç©¶çš„ç³»ç»Ÿæ€§åˆ†æå’Œå…·ä½“å®ä¾‹è®ºè¯ï¼Œæœ¬æ–‡æ­ç¤ºäº†è¯„ä¼°ç¼ºé™·å¦‚ä½•æŸå®³å½“å‰åŠæœªæ¥æ€§èƒ½ä¸»å¼ çš„å¯é æ€§ã€‚æœ€åï¼Œç ”ç©¶è€…å‘¼åå»ºç«‹æ›´ä¸¥è°¨çš„ Evaluation Methodologiesï¼ˆè¯„ä¼°æ–¹æ³•è®ºï¼‰ï¼Œä»¥å®ç°å¯¹ LLMs é¢„æµ‹èƒ½åŠ›çš„å‡†ç¡®è¯„ä¼°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.00723v1",
      "published_date": "2025-05-31 21:49:17 UTC",
      "updated_date": "2025-05-31 21:49:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:52:53.711937+00:00"
    },
    {
      "arxiv_id": "2506.00718v1",
      "title": "From Local Cues to Global Percepts: Emergent Gestalt Organization in Self-Supervised Vision Models",
      "title_zh": "ä»å±€éƒ¨çº¿ç´¢åˆ°å…¨å±€æ„ŸçŸ¥ï¼šè‡ªç›‘ç£è§†è§‰æ¨¡å‹ä¸­æ ¼å¼å¡”ç»„ç»‡çš„æ¶Œç°",
      "authors": [
        "Tianqin Li",
        "Ziqi Wen",
        "Leiran Song",
        "Jun Liu",
        "Zhi Jing",
        "Tai Sing Lee"
      ],
      "abstract": "Human vision organizes local cues into coherent global forms using Gestalt principles like closure, proximity, and figure-ground assignment -- functions reliant on global spatial structure. We investigate whether modern vision models show similar behaviors, and under what training conditions these emerge. We find that Vision Transformers (ViTs) trained with Masked Autoencoding (MAE) exhibit activation patterns consistent with Gestalt laws, including illusory contour completion, convexity preference, and dynamic figure-ground segregation. To probe the computational basis, we hypothesize that modeling global dependencies is necessary for Gestalt-like organization. We introduce the Distorted Spatial Relationship Testbench (DiSRT), which evaluates sensitivity to global spatial perturbations while preserving local textures. Using DiSRT, we show that self-supervised models (e.g., MAE, CLIP) outperform supervised baselines and sometimes even exceed human performance. ConvNeXt models trained with MAE also exhibit Gestalt-compatible representations, suggesting such sensitivity can arise without attention architectures. However, classification finetuning degrades this ability. Inspired by biological vision, we show that a Top-K activation sparsity mechanism can restore global sensitivity. Our findings identify training conditions that promote or suppress Gestalt-like perception and establish DiSRT as a diagnostic for global structure sensitivity across models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿåƒäººç±»è§†è§‰ä¸€æ ·ï¼Œåˆ©ç”¨ Gestalt principles å°†å±€éƒ¨çº¿ç´¢ç»„ç»‡æˆè¿è´¯çš„å…¨å±€å½¢å¼ï¼Œå¹¶åˆ†æäº†è¿™ç§èƒ½åŠ›çš„äº§ç”Ÿæ¡ä»¶ã€‚ç ”ç©¶å‘ç°é‡‡ç”¨ Masked Autoencoding (MAE) è®­ç»ƒçš„ Vision Transformers (ViTs) è¡¨ç°å‡ºä¸ Gestalt laws ä¸€è‡´çš„æ¿€æ´»æ¨¡å¼ï¼ŒåŒ…æ‹¬ illusory contour completionã€convexity preference å’Œ dynamic figure-ground segregationã€‚ä¸ºäº†æ¢æµ‹è®¡ç®—åŸºç¡€ï¼Œç ”ç©¶è€…å¼•å…¥äº† Distorted Spatial Relationship Testbench (DiSRT) æ¥è¯„ä¼°æ¨¡å‹åœ¨ä¿ç•™å±€éƒ¨çº¹ç†æ—¶å¯¹å…¨å±€ç©ºé—´æ‰°åŠ¨çš„æ•æ„Ÿæ€§ã€‚å®éªŒæ˜¾ç¤ºè‡ªç›‘ç£æ¨¡å‹ï¼ˆå¦‚ MAE, CLIPï¼‰çš„è¡¨ç°ä¼˜äºæœ‰ç›‘ç£åŸºçº¿ï¼Œä¸” ConvNeXt åœ¨ MAE è®­ç»ƒä¸‹ä¹Ÿè¡¨ç°å‡ºæ ¼å¼å¡”å…¼å®¹æ€§ï¼Œè¡¨æ˜è¿™ç§èƒ½åŠ›ä¸å•çº¯ä¾èµ–æ³¨æ„åŠ›æ¶æ„ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç° classification finetuning ä¼šå‰Šå¼±å…¨å±€æ„ŸçŸ¥ï¼Œè€Œå¼•å…¥å—ç”Ÿç‰©å¯å‘çš„å¯æ¢å¤å…¨å±€æ•æ„Ÿæ€§çš„ Top-K activation sparsity æœºåˆ¶åˆ™èƒ½æœ‰æ•ˆç¼“è§£è¿™ä¸€é—®é¢˜ã€‚è¯¥å‘ç°ç¡®ç«‹äº†ä¿ƒè¿›æˆ–æŠ‘åˆ¶ Gestalt-like perception çš„è®­ç»ƒç¯å¢ƒï¼Œå¹¶ä½¿ DiSRT æˆä¸ºè¯Šæ–­æ¨¡å‹å…¨å±€ç»“æ„æ•æ„Ÿæ€§çš„é‡è¦æ ‡å‡†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00718v1",
      "published_date": "2025-05-31 21:35:54 UTC",
      "updated_date": "2025-05-31 21:35:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:53:30.666325+00:00"
    },
    {
      "arxiv_id": "2506.00714v2",
      "title": "RFCAudit: An LLM Agent for Functional Bug Detection in Network Protocols",
      "title_zh": "RFCAuditï¼šç”¨äºç½‘ç»œåè®®åŠŸèƒ½ç¼ºé™·æ£€æµ‹çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“",
      "authors": [
        "Mingwei Zheng",
        "Chengpeng Wang",
        "Xuwei Liu",
        "Jinyao Guo",
        "Shiwei Feng",
        "Xiangyu Zhang"
      ],
      "abstract": "Functional correctness is critical for ensuring the reliability and security of network protocol implementations. Functional bugs, instances where implementations diverge from behaviors specified in RFC documents, can lead to severe consequences, including faulty routing, authentication bypasses, and service disruptions. Detecting these bugs requires deep semantic analysis across specification documents and source code, a task beyond the capabilities of traditional static analysis tools. This paper introduces RFCAudit, an autonomous agent that leverages large language models (LLMs) to detect functional bugs by checking conformance between network protocol implementations and their RFC specifications. Inspired by the human auditing procedure, RFCAudit comprises two key components: an indexing agent and a detection agent. The former hierarchically summarizes protocol code semantics, generating semantic indexes that enable the detection agent to narrow down the scanning scope. The latter employs demand-driven retrieval to iteratively collect additional relevant data structures and functions, eventually identifying potential inconsistencies with the RFC specifications effectively. We evaluate RFCAudit across six real-world network protocol implementations. RFCAudit identifies 47 functional bugs with 81.9% precision, of which 20 bugs have been confirmed or fixed by developers.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RFCAuditï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„è‡ªä¸»æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨é€šè¿‡æ£€æŸ¥ç½‘ç»œåè®®å®ç°ä¸ RFC è§„èŒƒä¹‹é—´çš„ä¸€è‡´æ€§æ¥æ£€æµ‹åŠŸèƒ½æ¼æ´ (Functional Bug)ã€‚é’ˆå¯¹ä¼ ç»Ÿé™æ€åˆ†æå·¥å…·éš¾ä»¥å¤„ç†æ–‡æ¡£ä¸æºä»£ç æ·±å±‚è¯­ä¹‰åˆ†æçš„é—®é¢˜ï¼ŒRFCAudit æ¨¡æ‹Ÿäº†äººç±»å®¡è®¡æµç¨‹ï¼Œç”±ç´¢å¼•æ™ºèƒ½ä½“ (Indexing Agent) å’Œæ£€æµ‹æ™ºèƒ½ä½“ (Detection Agent) åä½œè¿è¡Œã€‚ç´¢å¼•æ™ºèƒ½ä½“è´Ÿè´£åˆ†å±‚æ€»ç»“åè®®ä»£ç è¯­ä¹‰å¹¶ç”Ÿæˆè¯­ä¹‰ç´¢å¼•ä»¥ç¼©å°æ‰«æèŒƒå›´ï¼Œè€Œæ£€æµ‹æ™ºèƒ½ä½“åˆ™é€šè¿‡æŒ‰éœ€æ£€ç´¢ (Demand-driven Retrieval) è¿­ä»£æ”¶é›†ç›¸å…³çš„æ•°æ®ç»“æ„ä¸å‡½æ•°ï¼Œä»è€Œç²¾å‡†è¯†åˆ«å®ç°ä»£ç ä¸è§„èŒƒä¹‹é—´çš„ä¸ä¸€è‡´æ€§ã€‚åœ¨å…­ä¸ªçœŸå®ç½‘ç»œåè®®å®ç°ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒRFCAudit ä»¥ 81.9% çš„å‡†ç¡®ç‡æˆåŠŸè¯†åˆ«å‡º 47 ä¸ªåŠŸèƒ½æ¼æ´ï¼Œå…¶ä¸­ 20 ä¸ªæ¼æ´å·²è·å¾—å¼€å‘è€…çš„ç¡®è®¤æˆ–ä¿®å¤ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨ä¿éšœç½‘ç»œåè®®å¯é æ€§ä¸å®‰å…¨æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00714v2",
      "published_date": "2025-05-31 21:13:19 UTC",
      "updated_date": "2025-10-04 06:53:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:53:29.929130+00:00"
    },
    {
      "arxiv_id": "2506.00713v3",
      "title": "AKReF: An argumentative knowledge representation framework for structured argumentation",
      "title_zh": "AKReFï¼šä¸€ç§é¢å‘ç»“æ„åŒ–è®ºè¾©çš„è®ºè¾©çŸ¥è¯†è¡¨ç¤ºæ¡†æ¶",
      "authors": [
        "Debarati Bhattacharjee",
        "Ashish Anand"
      ],
      "abstract": "This paper presents a framework to convert argumentative texts into argument knowledge graphs (AKG). The proposed argumentative knowledge representation framework (AKReF) extends the theoretical foundation and enables the AKG to provide a graphical view of the argumentative structure that is easier to understand. Starting with basic annotations of argumentative components (ACs) and argumentative relations (ARs), we enrich the information by constructing a knowledge base (KB) graph with metadata attributes for nodes. Next, we apply modus ponens on premises and inference rules from the KB to form arguments. From these arguments, we create an AKG. The nodes and edges of the AKG have attributes capturing key argumentative features such as the type of premise (e.g., axiom, ordinary premise, assumption), the type of inference rule (e.g., strict, defeasible), preference order over defeasible rules, markers (e.g., \"therefore\", \"however\"), and the type of attack (e.g., undercut, rebuttal, undermining). We identify inference rules by locating a specific set of markers, called inference markers (IM). This, in turn, makes it possible to identify undercut attacks previously undetectable in existing datasets. AKG prepares the ground for reasoning tasks, including checking the coherence of arguments and identifying opportunities for revision. For this, it is essential to find indirect relations, many of which are implicit. Our proposed AKG format, with annotated inference rules and modus ponens, helps reasoning models learn the implicit, indirect relations that require inference over arguments and their interconnections. We use an essay from the AAEC dataset to illustrate the framework. We further show its application in complex analyses such as extracting a conflict-free set and a maximal set of admissible arguments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AKReFï¼Œè¿™æ˜¯ä¸€ç§å°†è®ºè¯æ€§æ–‡æœ¬è½¬åŒ–ä¸ºè®ºè¯çŸ¥è¯†å›¾è°±(Argument Knowledge Graphs, AKG)çš„ç»“æ„åŒ–è®ºè¯çŸ¥è¯†è¡¨ç¤ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨è®ºè¯ç»„ä»¶(Argumentative Components)å’Œè®ºè¯å…³ç³»(Argumentative Relations)æ ‡æ³¨çš„åŸºç¡€ä¸Šï¼Œåˆ©ç”¨çŸ¥è¯†åº“(Knowledge Base)å…ƒæ•°æ®å¹¶åº”ç”¨è‚¯å®šå‰ä»¶(Modus Ponens)é€»è¾‘æ¥æ„å»ºå¤æ‚çš„è®ºè¯ç»“æ„ã€‚ç”Ÿæˆçš„AKGèƒ½å¤Ÿæ•æ‰å‰æç±»å‹ã€æ¨ç†è§„åˆ™ç±»å‹ä»¥åŠåŒ…æ‹¬å‰Šå¼±(Undercut)ã€åé©³(Rebuttal)åœ¨å†…çš„å¤šç§æ”»å‡»ç±»å‹ç­‰å…³é”®è®ºè¯ç‰¹å¾ã€‚ç ”ç©¶ç‰¹åˆ«å¼•å…¥äº†æ¨ç†æ ‡è®°(Inference Markers)æ¥å®šä½æ¨ç†è§„åˆ™ï¼Œä»è€Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«ä»¥å¾€æ•°æ®é›†ä¸­éš¾ä»¥å¯Ÿè§‰çš„å‰Šå¼±æ”»å‡»(Undercut attacks)ã€‚AKGä¸ºè®ºè¯æ¨ç†ä»»åŠ¡å¥ å®šäº†åŸºç¡€ï¼Œé€šè¿‡æ ‡æ³¨æ¨ç†è§„åˆ™è¾…åŠ©æ¨¡å‹å­¦ä¹ è®ºè¯åŠå…¶äº’è”ä¸­çš„éšå¼ã€é—´æ¥å…³ç³»ï¼Œè¿›è€Œä¼˜åŒ–è®ºè¯è¿è´¯æ€§æ£€æŸ¥å’Œä¿®è®¢å»ºè®®ã€‚åœ¨AAECæ•°æ®é›†ä¸Šçš„åº”ç”¨å±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨æå–æ— å†²çªé›†åˆå’Œæœ€å¤§å¯æ¥å—è®ºè¯é›†ç­‰å¤æ‚åˆ†æä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, 7 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.00713v3",
      "published_date": "2025-05-31 21:11:30 UTC",
      "updated_date": "2025-07-15 21:31:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:53:38.405492+00:00"
    },
    {
      "arxiv_id": "2506.00711v2",
      "title": "QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training",
      "title_zh": "QoQ-Medï¼šåŸºäºé¢†åŸŸæ„ŸçŸ¥ GRPO è®­ç»ƒæ„å»ºå¤šæ¨¡æ€ä¸´åºŠåŸºç¡€æ¨¡å‹",
      "authors": [
        "Wei Dai",
        "Peilin Chen",
        "Chanakya Ekbote",
        "Paul Pu Liang"
      ],
      "abstract": "Clinical decision-making routinely demands reasoning over heterogeneous data, yet existing multimodal language models (MLLMs) remain largely vision-centric and fail to generalize across clinical specialties. To bridge this gap, we introduce QoQ-Med-7B/32B, the first open generalist clinical foundation model that jointly reasons across medical images, time-series signals, and text reports. QoQ-Med is trained with Domain-aware Relative Policy Optimization (DRPO), a novel reinforcement-learning objective that hierarchically scales normalized rewards according to domain rarity and modality difficulty, mitigating performance imbalance caused by skewed clinical data distributions. Trained on 2.61 million instruction tuning pairs spanning 9 clinical domains, we show that DRPO training boosts diagnostic performance by 43% in macro-F1 on average across all visual domains as compared to other critic-free training methods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation data, it is able to highlight salient regions related to the diagnosis, with an IoU 10x higher than open models while reaching the performance of OpenAI o4-mini. To foster reproducibility and downstream research, we release (i) the full model weights, (ii) the modular training pipeline, and (iii) all intermediate reasoning traces at https://github.com/DDVD233/QoQ_Med.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† QoQ-Med-7B/32Bï¼Œè¿™æ˜¯é¦–ä¸ªèƒ½å¤ŸåŒæ—¶å¤„ç†åŒ»å­¦å½±åƒã€æ—¶é—´åºåˆ—ä¿¡å·å’Œæ–‡æœ¬æŠ¥å‘Šçš„å¼€æºé€šç”¨ä¸´åºŠåŸºç¡€æ¨¡å‹ (Clinical Foundation Model)ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹å¤šä»¥è§†è§‰ä¸ºä¸­å¿ƒä¸”åœ¨ä¸´åºŠé¢†åŸŸæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼ŒQoQ-Med æ—¨åœ¨å®ç°è·¨å¼‚æ„æ•°æ®çš„å¤šæ¨¡æ€æ¨ç†ã€‚å…¶æ ¸å¿ƒè´¡çŒ®æ˜¯æå‡ºäº†é¢†åŸŸæ„ŸçŸ¥ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– (Domain-aware Relative Policy Optimization, DRPO)ï¼Œè¿™æ˜¯ä¸€ç§æ ¹æ®é¢†åŸŸç¨€æœ‰åº¦å’Œæ¨¡æ€éš¾åº¦åˆ†å±‚è°ƒæ•´å¥–åŠ±çš„æ–°å‹å¼ºåŒ–å­¦ä¹ ç›®æ ‡ï¼Œæœ‰æ•ˆç¼“è§£äº†ä¸´åºŠæ•°æ®åˆ†å¸ƒä¸å‡å¯¼è‡´çš„æ€§èƒ½ä¸å¹³è¡¡ã€‚åœ¨åŒ…å« 9 ä¸ªä¸´åºŠé¢†åŸŸçš„ 261 ä¸‡ä¸ªæŒ‡ä»¤å¾®è°ƒå¯¹ä¸Šï¼ŒDRPO è®­ç»ƒä½¿è§†è§‰é¢†åŸŸçš„å¹³å‡å®è§‚ F1 (Macro-F1) è¯Šæ–­æ€§èƒ½æ¯” GRPO æå‡äº† 43%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨åŒ»å­¦å½±åƒåˆ†å‰²ä¸­è¡¨ç°å“è¶Šï¼Œå…¶äº¤å¹¶æ¯” (IoU) æ¯”ç°æœ‰å¼€æºæ¨¡å‹é«˜å‡º 10 å€ï¼Œè¾¾åˆ°äº† OpenAI o4-mini çš„æ°´å¹³ã€‚ç›®å‰ï¼Œç ”ç©¶å›¢é˜Ÿå·²å…¬å¼€å‘å¸ƒäº†æ¨¡å‹æƒé‡ã€æ¨¡å—åŒ–è®­ç»ƒç®¡çº¿åŠæ‰€æœ‰ä¸­é—´æ¨ç†è½¨è¿¹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as Oral at NeurIPS 2025. Revision after camera ready",
      "pdf_url": "https://arxiv.org/pdf/2506.00711v2",
      "published_date": "2025-05-31 21:02:52 UTC",
      "updated_date": "2025-10-22 17:18:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:53:33.023376+00:00"
    },
    {
      "arxiv_id": "2506.00708v3",
      "title": "DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains",
      "title_zh": "DrKGCï¼šé¢å‘é€šç”¨ä¸ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçŸ¥è¯†å›¾è°±è¡¥å…¨çš„åŠ¨æ€å­å›¾æ£€ç´¢å¢å¼ºå¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Yongkang Xiao",
        "Sinian Zhang",
        "Yi Dai",
        "Huixue Zhou",
        "Jue Hou",
        "Jie Ding",
        "Rui Zhang"
      ],
      "abstract": "Knowledge graph completion (KGC) aims to predict missing triples in knowledge graphs (KGs) by leveraging existing triples and textual information. Recently, generative large language models (LLMs) have been increasingly employed for graph tasks. However, current approaches typically encode graph context in textual form, which fails to fully exploit the potential of LLMs for perceiving and reasoning about graph structures. To address this limitation, we propose DrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion). DrKGC employs a flexible lightweight model training strategy to learn structural embeddings and logical rules within the KG. It then leverages a novel bottom-up graph retrieval method to extract a subgraph for each query guided by the learned rules. Finally, a graph convolutional network (GCN) adapter uses the retrieved subgraph to enhance the structural embeddings, which are then integrated into the prompt for effective LLM fine-tuning. Experimental results on two general domain benchmark datasets and two biomedical datasets demonstrate the superior performance of DrKGC. Furthermore, a realistic case study in the biomedical domain highlights its interpretability and practical utility.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DrKGCï¼Œä¸€ç§ç”¨äºé€šç”¨å’Œç”Ÿç‰©åŒ»å­¦é¢†åŸŸçŸ¥è¯†å›¾è°±è¡¥å…¨(Knowledge Graph Completion)çš„åŠ¨æ€å­å›¾æ£€ç´¢å¢å¼ºå¤§è¯­è¨€æ¨¡å‹(LLMs)æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨æ–‡æœ¬åŒ–ç¼–ç å›¾ä¸Šä¸‹æ–‡æ—¶éš¾ä»¥å‘æŒ¥LLMå›¾ç»“æ„æ¨ç†æ½œåŠ›çš„é—®é¢˜ï¼ŒDrKGCé€šè¿‡è½»é‡åŒ–è®­ç»ƒç­–ç•¥å­¦ä¹ ç»“æ„åµŒå…¥(structural embeddings)å’Œé€»è¾‘è§„åˆ™ã€‚è¯¥æ¡†æ¶å¼•å…¥è‡ªåº•å‘ä¸Šçš„å›¾æ£€ç´¢æ–¹æ³•ï¼Œä¾æ®é€»è¾‘è§„åˆ™ä¸ºæŸ¥è¯¢æå–ç›¸å…³å­å›¾ã€‚éšåï¼Œåˆ©ç”¨å›¾å·ç§¯ç½‘ç»œ(Graph Convolutional Network, GCN)é€‚é…å™¨ç»“åˆå­å›¾å¢å¼ºç»“æ„ç‰¹å¾ï¼Œå¹¶å°†å…¶æ•´åˆè‡³æç¤ºè¯ä¸­è¿›è¡ŒLLMå¾®è°ƒã€‚å®éªŒåœ¨é€šç”¨å’Œç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šå‡è¯æ˜äº†DrKGCçš„å“è¶Šæ€§èƒ½ï¼Œä¸”æ¡ˆä¾‹åˆ†æè¿›ä¸€æ­¥å±•ç¤ºäº†å…¶è‰¯å¥½çš„å¯è§£é‡Šæ€§ä¸å®é™…åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at EMNLP 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2506.00708v3",
      "published_date": "2025-05-31 20:56:54 UTC",
      "updated_date": "2025-11-10 08:45:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:53:43.993902+00:00"
    },
    {
      "arxiv_id": "2506.00701v1",
      "title": "Bayesian Inference of Training Dataset Membership",
      "title_zh": "è®­ç»ƒæ•°æ®é›†æˆå‘˜èº«ä»½çš„è´å¶æ–¯æ¨æ–­",
      "authors": [
        "Yongchao Huang"
      ],
      "abstract": "Determining whether a dataset was part of a machine learning model's training data pool can reveal privacy vulnerabilities, a challenge often addressed through membership inference attacks (MIAs). Traditional MIAs typically require access to model internals or rely on computationally intensive shadow models. This paper proposes an efficient, interpretable and principled Bayesian inference method for membership inference. By analyzing post-hoc metrics such as prediction error, confidence (entropy), perturbation magnitude, and dataset statistics from a trained ML model, our approach computes posterior probabilities of membership without requiring extensive model training. Experimental results on synthetic datasets demonstrate the method's effectiveness in distinguishing member from non-member datasets. Beyond membership inference, this method can also detect distribution shifts, offering a practical and interpretable alternative to existing approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ¤æ–­æ•°æ®é›†æ˜¯å¦å±äºæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒé›†çš„æˆå‘˜æ¨ç†(Membership Inference)é—®é¢˜ï¼Œè¿™æ˜¯è¯„ä¼°éšç§æ¼æ´çš„å…³é”®æŒ‘æˆ˜ã€‚é’ˆå¯¹ä¼ ç»Ÿæˆå‘˜æ¨ç†æ”»å‡»(MIAs)ä¾èµ–æ¨¡å‹å†…éƒ¨ä¿¡æ¯æˆ–é«˜æˆæœ¬å½±å­æ¨¡å‹çš„å±€é™ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºè´å¶æ–¯æ¨ç†(Bayesian Inference)çš„é«˜æ•ˆä¸”å¯è§£é‡Šçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†æå·²è®­ç»ƒæ¨¡å‹çš„åéªŒæŒ‡æ ‡ï¼Œå¦‚é¢„æµ‹è¯¯å·®(Prediction Error)ã€ç½®ä¿¡åº¦æˆ–ç†µ(Confidence/Entropy)ã€æ‰°åŠ¨å¹…åº¦(Perturbation Magnitude)åŠæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ï¼Œç›´æ¥è®¡ç®—æˆå‘˜èº«ä»½çš„åéªŒæ¦‚ç‡ã€‚è¯¥æ¡†æ¶æ— éœ€è¿›è¡Œå¤§è§„æ¨¡çš„æ¨¡å‹é‡è®­ç»ƒï¼Œåœ¨åˆæˆæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜å…¶èƒ½æœ‰æ•ˆåŒºåˆ†æˆå‘˜ä¸éæˆå‘˜æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å±•ç°äº†æ£€æµ‹åˆ†å¸ƒåç§»(Distribution Shifts)çš„èƒ½åŠ›ï¼Œä¸ºéšç§æ¨æ–­å’Œæ¨¡å‹åˆ†ææä¾›äº†ä¸€ç§å…¼å…·ç†è®ºåŸåˆ™ä¸å®ç”¨ä»·å€¼çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.00701v1",
      "published_date": "2025-05-31 20:14:38 UTC",
      "updated_date": "2025-05-31 20:14:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:53:47.614106+00:00"
    },
    {
      "arxiv_id": "2506.00694v2",
      "title": "Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments",
      "title_zh": "è¡¡é‡å¿ å®åº¦ä¸å¼ƒæƒï¼šä¸€ç§ç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„åŸºäºæ¡ˆä¾‹çš„ä¸‰æ®µå¼æ³•å¾‹è®ºè¯çš„è‡ªåŠ¨åŒ–æµç¨‹",
      "authors": [
        "Li Zhang",
        "Morgan Gray",
        "Jaromir Savelka",
        "Kevin D. Ashley"
      ],
      "abstract": "Large Language Models (LLMs) demonstrate potential in complex legal tasks like argument generation, yet their reliability remains a concern. Building upon pilot work assessing LLM generation of 3-ply legal arguments using human evaluation, this paper introduces an automated pipeline to evaluate LLM performance on this task, specifically focusing on faithfulness (absence of hallucination), factor utilization, and appropriate abstention. We define hallucination as the generation of factors not present in the input case materials and abstention as the model's ability to refrain from generating arguments when instructed and no factual basis exists. Our automated method employs an external LLM to extract factors from generated arguments and compares them against the ground-truth factors provided in the input case triples (current case and two precedent cases). We evaluated eight distinct LLMs on three tests of increasing difficulty: 1) generating a standard 3-ply argument, 2) generating an argument with swapped precedent roles, and 3) recognizing the impossibility of argument generation due to lack of shared factors and abstaining. Our findings indicate that while current LLMs achieve high accuracy (over 90%) in avoiding hallucination on viable argument generation tests (Tests 1 & 2), they often fail to utilize the full set of relevant factors present in the cases. Critically, on the abstention test (Test 3), most models failed to follow instructions to stop, instead generating spurious arguments despite the lack of common factors. This automated pipeline provides a scalable method for assessing these crucial LLM behaviors, highlighting the need for improvements in factor utilization and robust abstention capabilities before reliable deployment in legal settings. Link: https://lizhang-aiandlaw.github.io/An-Automated-Pipeline-for-Evaluating-LLM-Generated-3-ply-Case-Based-Legal-Arguments/",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤æ‚æ³•å¾‹è®ºè¯ä»»åŠ¡ä¸­çš„å¯é æ€§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹(Automated Pipeline)ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ç”Ÿæˆä¸‰å±‚æ³•å¾‹è®ºè¯(3-ply Case-Based Legal Arguments)æ—¶çš„å¿ å®åº¦(Faithfulness)å’Œæ‹’ç»ç”Ÿæˆ(Abstention)è¡¨ç°ã€‚ç ”ç©¶å°†å¹»è§‰(Hallucination)å®šä¹‰ä¸ºç”Ÿæˆè¾“å…¥ææ–™ä¸­ä¸å­˜åœ¨çš„è¦ç´ (Factors)ï¼Œè€Œæ‹’ç»ç”Ÿæˆ(Abstention)åˆ™æŒ‡æ¨¡å‹åœ¨ç¼ºä¹äº‹å®ä¾æ®æ—¶éµå¾ªæŒ‡ä»¤æ‹’ç»ç”Ÿæˆè®ºè¯çš„èƒ½åŠ›ã€‚è¯¥è‡ªåŠ¨åŒ–æ–¹æ³•åˆ©ç”¨å¤–éƒ¨æ¨¡å‹ä»ç”Ÿæˆçš„è®ºè¯ä¸­æå–è¦ç´ ï¼Œå¹¶å°†å…¶ä¸è¾“å…¥æ¡ˆä¾‹ä¸­çš„åŸºå‡†è¦ç´ (Ground-truth Factors)è¿›è¡Œæ¯”å¯¹ã€‚å®éªŒå¯¹å…«ç§ä¸åŒçš„LLMsè¿›è¡Œäº†ä¸‰é¡¹éš¾åº¦é€’å¢çš„æµ‹è¯•ï¼ŒåŒ…æ‹¬æ ‡å‡†è®ºè¯ç”Ÿæˆã€äº¤æ¢å…ˆä¾‹è§’è‰²çš„è®ºè¯ç”Ÿæˆä»¥åŠè¯†åˆ«ä¸å¯è¡Œæ€§å¹¶æ‹’ç»ç”Ÿæˆçš„æµ‹è¯•ã€‚ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶ç°æœ‰æ¨¡å‹åœ¨å¯è¡Œè®ºè¯ç”Ÿæˆæµ‹è¯•ä¸­èƒ½è¾¾åˆ°è¶…è¿‡90%çš„å¿ å®åº¦ï¼Œä½†åœ¨å……åˆ†åˆ©ç”¨æ‰€æœ‰ç›¸å…³è¦ç´ (Factor Utilization)æ–¹é¢è¡¨ç°æ¬ ä½³ã€‚å…³é”®å‘ç°åœ¨äºï¼Œå¤§å¤šæ•°æ¨¡å‹åœ¨æ‹’ç»ç”Ÿæˆæµ‹è¯•ä¸­å¤±è´¥ï¼Œå³ä½¿åœ¨ç¼ºä¹å…±åŒè¦ç´ çš„æƒ…å†µä¸‹ä»ä¼šè¿åæŒ‡ä»¤ç”Ÿæˆè™šå‡è®ºè¯ã€‚è¯¥ç ”ç©¶æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è¯„ä¼°æ–¹æ³•ï¼Œå¹¶æŒ‡å‡ºLLMsåœ¨æ³•å¾‹åœºæ™¯éƒ¨ç½²å‰ï¼Œå¿…é¡»åœ¨è¦ç´ åˆ©ç”¨å’Œé²æ£’çš„æ‹’ç»ç”Ÿæˆèƒ½åŠ›æ–¹é¢è¿›è¡Œæ”¹è¿›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 7th Workshop on Automated Semantic Analysis of Information in Legal Text @ ICAIL 2025, 16 June 2025, Chicago, IL",
      "pdf_url": "https://arxiv.org/pdf/2506.00694v2",
      "published_date": "2025-05-31 19:56:40 UTC",
      "updated_date": "2025-06-03 03:22:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:53:52.697662+00:00"
    },
    {
      "arxiv_id": "2506.00691v4",
      "title": "Optimizing Sensory Neurons: Nonlinear Attention Mechanisms for Accelerated Convergence in Permutation-Invariant Neural Networks for Reinforcement Learning",
      "title_zh": "ä¼˜åŒ–æ„Ÿè§‰ç¥ç»å…ƒï¼šç”¨äºå¼ºåŒ–å­¦ä¹ ç½®æ¢ä¸å˜ç¥ç»ç½‘ç»œåŠ é€Ÿæ”¶æ•›çš„éçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶",
      "authors": [
        "Junaid Muzaffar",
        "Khubaib Ahmed",
        "Ingo Frommholz",
        "Zeeshan Pervez",
        "Ahsan ul Haq"
      ],
      "abstract": "Training reinforcement learning (RL) agents often requires significant computational resources and prolonged training durations. To address this challenge, we build upon prior work that introduced a neural architecture with permutation-invariant sensory processing. We propose a modified attention mechanism that applies a non-linear transformation to the key vectors (K), producing enriched representations (K') through a custom mapping function. This Nonlinear Attention (NLA) mechanism enhances the representational capacity of the attention layer, enabling the agent to learn more expressive feature interactions. As a result, our model achieves significantly faster convergence and improved training efficiency, while maintaining performance on par with the baseline. These results highlight the potential of nonlinear attention mechanisms to accelerate reinforcement learning without sacrificing effectiveness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)è®­ç»ƒä¸­è®¡ç®—èµ„æºæ¶ˆè€—å¤§ä¸”è®­ç»ƒå‘¨æœŸé•¿çš„é—®é¢˜ï¼Œåœ¨ç½®æ¢ä¸å˜(Permutation-Invariant)æ„Ÿå®˜å¤„ç†æ¶æ„çš„åŸºç¡€ä¸Šæå‡ºäº†ä¸€ç§éçº¿æ€§æ³¨æ„åŠ›(Nonlinear Attention, NLA)æœºåˆ¶ã€‚è¯¥æœºåˆ¶é€šè¿‡è‡ªå®šä¹‰æ˜ å°„å‡½æ•°å¯¹é”®å‘é‡(Key vectors, K)è¿›è¡Œéçº¿æ€§å˜æ¢å¹¶ç”Ÿæˆä¸°å¯Œçš„è¡¨å¾(K')ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†æ³¨æ„åŠ›å±‚çš„è¡¨å¾èƒ½åŠ›ã€‚è¿™ç§æ”¹è¿›ä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿå­¦ä¹ æ›´å…·è¡¨ç°åŠ›çš„ç‰¹å¾äº¤äº’ï¼Œåœ¨å®éªŒä¸­æ˜¾è‘—åŠ å¿«äº†æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦å¹¶æå‡äº†è®­ç»ƒæ•ˆç‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨ä¿æŒä¸åŸºçº¿æ¨¡å‹æ€§èƒ½ç›¸å½“çš„å‰æä¸‹ï¼Œéçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶èƒ½æœ‰æ•ˆåŠ é€Ÿå¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œä¸ºä¼˜åŒ–æ„Ÿå®˜ç¥ç»å…ƒè¡¨å¾æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "there was an error with the figures and the algorithm, working on it to correct it, will publish with updated and correct algorithm and results",
      "pdf_url": "https://arxiv.org/pdf/2506.00691v4",
      "published_date": "2025-05-31 19:50:37 UTC",
      "updated_date": "2025-06-23 08:46:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:54:16.613589+00:00"
    },
    {
      "arxiv_id": "2506.00688v1",
      "title": "Existing Large Language Model Unlearning Evaluations Are Inconclusive",
      "title_zh": "ç°æœ‰å¤§è¯­è¨€æ¨¡å‹é—å¿˜è¯„ä¼°å°šæ— å®šè®º",
      "authors": [
        "Zhili Feng",
        "Yixuan Even Xu",
        "Alexander Robey",
        "Robert Kirk",
        "Xander Davies",
        "Yarin Gal",
        "Avi Schwarzschild",
        "J. Zico Kolter"
      ],
      "abstract": "Machine unlearning aims to remove sensitive or undesired data from large language models. However, recent studies suggest that unlearning is often shallow, claiming that removed knowledge can easily be recovered. In this work, we critically examine standard unlearning evaluation practices and uncover key limitations that shake our trust in those findings. First, we show that some evaluations introduce substantial new information into the model, potentially masking true unlearning performance by re-teaching the model during testing. Second, we demonstrate that evaluation outcomes vary significantly across tasks, undermining the generalizability of current evaluation routines. Finally, we find that many evaluations rely on spurious correlations, making their results difficult to trust and interpret. Taken together, these issues suggest that current evaluation protocols may both overstate and understate unlearning success. To address this, we propose two principles for future unlearning evaluations: minimal information injection and downstream task awareness. We validate these principles through a series of targeted experiments, showing how violations of each can lead to misleading conclusions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ‰¹åˆ¤æ€§åœ°å®¡æŸ¥äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­æœºå™¨é—å¿˜ï¼ˆMachine Unlearningï¼‰çš„æ ‡å‡†è¯„ä¼°å®è·µï¼ŒæŒ‡å‡ºå½“å‰çš„è¯„ä¼°ç»“è®ºå¹¶ä¸å…·æœ‰å†³å®šæ€§ã€‚ä½œè€…å‘ç°äº†ç°æœ‰è¯„ä¼°æ–¹æ¡ˆçš„ä¸‰ä¸ªå…³é”®å±€é™ï¼šæµ‹è¯•ä¸­æ³¨å…¥çš„æ–°ä¿¡æ¯å¯èƒ½æ©ç›–çœŸå®çš„é—å¿˜æ•ˆæœã€ä¸åŒä»»åŠ¡é—´çš„è¯„ä¼°ç»“æœç¼ºä¹ä¸€è‡´æ€§ï¼Œä»¥åŠå¯¹ä¼ªç›¸å…³ï¼ˆSpurious correlationsï¼‰çš„è¿‡åº¦ä¾èµ–ã€‚è¿™äº›é—®é¢˜æ„å‘³ç€å½“å‰çš„è¯„ä¼°åè®®å¯èƒ½åŒæ—¶å­˜åœ¨é«˜ä¼°å’Œä½ä¼°é—å¿˜æ•ˆæœçš„é£é™©ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†æœªæ¥è¯„ä¼°åº”éµå¾ªçš„ä¸¤é¡¹åŸåˆ™ï¼šæœ€å°ä¿¡æ¯æ³¨å…¥ï¼ˆMinimal information injectionï¼‰å’Œä¸‹æ¸¸ä»»åŠ¡æ„ŸçŸ¥ï¼ˆDownstream task awarenessï¼‰ã€‚é€šè¿‡ä¸€ç³»åˆ—é’ˆå¯¹æ€§å®éªŒï¼Œè¯¥å·¥ä½œéªŒè¯äº†è¿åè¿™äº›åŸåˆ™å¦‚ä½•å¯¼è‡´è¯¯å¯¼æ€§ç»“è®ºï¼Œä¸ºæ„å»ºæ›´å¯é çš„æœºå™¨é—å¿˜è¯„ä»·ä½“ç³»æä¾›äº†ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00688v1",
      "published_date": "2025-05-31 19:43:00 UTC",
      "updated_date": "2025-05-31 19:43:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:53:57.440882+00:00"
    },
    {
      "arxiv_id": "2506.14795v1",
      "title": "Comparative Analysis of QNN Architectures for Wind Power Prediction: Feature Maps and Ansatz Configurations",
      "title_zh": "é¢å‘é£ç”µåŠŸç‡é¢„æµ‹çš„ QNN æ¶æ„å¯¹æ¯”åˆ†æï¼šç‰¹å¾æ˜ å°„ä¸æ‹Ÿè®¾é…ç½®",
      "authors": [
        "Batuhan Hangun",
        "Emine Akpinar",
        "Oguz Altun",
        "Onder Eyecioglu"
      ],
      "abstract": "Quantum Machine Learning (QML) is an emerging field at the intersection of quantum computing and machine learning, aiming to enhance classical machine learning methods by leveraging quantum mechanics principles such as entanglement and superposition. However, skepticism persists regarding the practical advantages of QML, mainly due to the current limitations of noisy intermediate-scale quantum (NISQ) devices. This study addresses these concerns by extensively assessing Quantum Neural Networks (QNNs)-quantum-inspired counterparts of Artificial Neural Networks (ANNs), demonstrating their effectiveness compared to classical methods. We systematically construct and evaluate twelve distinct QNN configurations, utilizing two unique quantum feature maps combined with six different entanglement strategies for ansatz design. Experiments conducted on a wind energy dataset reveal that QNNs employing the Z feature map achieve up to 93% prediction accuracy when forecasting wind power output using only four input parameters. Our findings show that QNNs outperform classical methods in predictive tasks, underscoring the potential of QML in real-world applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å«å™ªå£°ä¸­ç­‰è§„æ¨¡é‡å­(NISQ)è®¾å¤‡çš„å±€é™æ€§ï¼Œå¯¹ç”¨äºé£åŠ›å‘ç”µé¢„æµ‹çš„é‡å­ç¥ç»ç½‘ç»œ(QNN)æ¶æ„è¿›è¡Œäº†æ·±å…¥çš„æ¯”è¾ƒåˆ†æã€‚ä½œè€…ç³»ç»Ÿåœ°æ„å»ºå¹¶è¯„ä¼°äº†12ç§ä¸åŒçš„QNNé…ç½®ï¼Œé€šè¿‡å°†ä¸¤ç§ç‹¬ç‰¹çš„é‡å­ç‰¹å¾æ˜ å°„(feature maps)ä¸å…­ç§é’ˆå¯¹ansatzè®¾è®¡çš„çº ç¼ ç­–ç•¥ç›¸ç»“åˆï¼Œæ¢è®¨äº†ä¸åŒæ¶æ„å¯¹é¢„æµ‹æ€§èƒ½çš„å½±å“ã€‚åœ¨é£èƒ½æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨Zç‰¹å¾æ˜ å°„çš„QNNæ¨¡å‹åœ¨ä»…ä½¿ç”¨å››ä¸ªè¾“å…¥å‚æ•°çš„æƒ…å†µä¸‹ï¼Œé¢„æµ‹å‡†ç¡®ç‡é«˜è¾¾93%ã€‚ç ”ç©¶è¯æ˜äº†QNNåœ¨é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„ç»å…¸æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œè¿›ä¸€æ­¥å¼ºè°ƒäº†é‡å­æœºå™¨å­¦ä¹ (QML)åœ¨å¤„ç†çœŸå®ä¸–ç•Œå¤æ‚åº”ç”¨ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "quant-ph",
      "comment": "6 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.14795v1",
      "published_date": "2025-05-31 19:17:53 UTC",
      "updated_date": "2025-05-31 19:17:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:54:09.580632+00:00"
    },
    {
      "arxiv_id": "2506.00679v2",
      "title": "A versatile foundation model for cine cardiac magnetic resonance image analysis tasks",
      "title_zh": "é¢å‘ç”µå½±å¿ƒè„ç£å…±æŒ¯å›¾åƒåˆ†æä»»åŠ¡çš„é€šç”¨åŸºç¡€æ¨¡å‹",
      "authors": [
        "Yunguan Fu",
        "Wenjia Bai",
        "Weixi Yi",
        "Charlotte Manisty",
        "Anish N Bhuva",
        "Thomas A Treibel",
        "James C Moon",
        "Matthew J Clarkson",
        "Rhodri Huw Davies",
        "Yipeng Hu"
      ],
      "abstract": "Here we present a versatile foundation model that can perform a range of clinically-relevant image analysis tasks, including segmentation, landmark localisation, diagnosis, and prognostication. A multi-view convolution-transformer masked autoencoder, named as CineMA, was trained on 15 million cine images from 74,916 subjects. The model was validated on multiple image analysis tasks and compared to existing models on >4,500 images from eight independent datasets with diverse population characteristics, representing the largest benchmark study for cine CMR so far. CineMA consistently outperformed conventional convolutional neural networks (CNNs) in delineating ventricular boundaries and estimating ejection fraction, a key measure of cardiac function. The improved performance was preserved, even when the model only used half of fine-tuning data. CineMA also surpassed CNNs in disease detection and matched their performance in long-axis function measurement. Interestingly, we found that CineMA can also detect cardiac changes in systemic diseases, such as diabetes, hypertension and cancer, and can also predict mortality. Finally, we assessed model fairness and demonstrated consistent model performance across demographic subgroups. These findings highlight CineMA's accuracy, learning efficiency, adaptability, and fairness, underscoring its potential as a foundation model for automated cardiac image analysis to support clinical workflow and cardiovascular research. All training and inference code and models are made publicly available at https://github.com/mathpluscode/CineMA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CineMAï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè§†å›¾å·ç§¯è½¬æ¢å™¨æ©ç è‡ªç¼–ç å™¨(multi-view convolution-transformer masked autoencoder)å¿ƒè„å½±åƒåŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨æ¥è‡ª74,916åå—è¯•è€…çš„1500ä¸‡å¼ ç”µå½±å¿ƒè„ç£å…±æŒ¯(cine CMR)å›¾åƒä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œæ—¨åœ¨æ‰§è¡Œåˆ†å‰²(segmentation)ã€åœ°æ ‡å®šä½(landmark localisation)ã€ç–¾ç—…è¯Šæ–­å’Œé¢„åé¢„æµ‹ç­‰å¤šç§ä¸´åºŠä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCineMAåœ¨å¿ƒå®¤è¾¹ç•Œæç»˜å’Œå°„è¡€åˆ†æ•°(ejection fraction)è¯„ä¼°æ–¹é¢ä¸€è‡´ä¼˜äºä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œ(CNNs)ï¼Œä¸”åœ¨ä»…ä½¿ç”¨ä¸€åŠå¾®è°ƒæ•°æ®çš„æƒ…å†µä¸‹ä»èƒ½ä¿æŒå“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ç–¾ç—…æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿè¯†åˆ«ç³–å°¿ç—…ã€é«˜è¡€å‹å’Œç™Œç—‡ç­‰å…¨èº«æ€§ç–¾ç—…å¼•èµ·çš„å¿ƒè„å˜åŒ–ï¼Œå¹¶å…·æœ‰é¢„æµ‹æ­»äº¡ç‡çš„èƒ½åŠ›ã€‚ç ”ç©¶é€šè¿‡åœ¨8ä¸ªç‹¬ç«‹æ•°æ®é›†ä¸Šçš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼ŒéªŒè¯äº†æ¨¡å‹åœ¨ä¸åŒäººå£ç»Ÿè®¡å­¦å­é›†ä¸­çš„å…¬å¹³æ€§(fairness)ä¸é€‚åº”æ€§ï¼Œå±•ç¤ºäº†å…¶æ”¯æŒä¸´åºŠå·¥ä½œæµå’Œå¿ƒè¡€ç®¡ç ”ç©¶çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00679v2",
      "published_date": "2025-05-31 19:12:34 UTC",
      "updated_date": "2025-08-31 12:02:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:54:31.740109+00:00"
    },
    {
      "arxiv_id": "2506.00676v1",
      "title": "SafeTuneBed: A Toolkit for Benchmarking LLM Safety Alignment in Fine-Tuning",
      "title_zh": "SafeTuneBedï¼šå¤§è¯­è¨€æ¨¡å‹å¾®è°ƒå®‰å…¨å¯¹é½åŸºå‡†æµ‹è¯•å·¥å…·åŒ…",
      "authors": [
        "Saad Hossain",
        "Samanvay Vajpayee",
        "Sirisha Rambhatla"
      ],
      "abstract": "As large language models (LLMs) become ubiquitous, parameter-efficient fine-tuning methods and safety-first defenses have proliferated rapidly. However, the number of approaches and their recent increase have resulted in diverse evaluations-varied datasets, metrics, and inconsistent threat settings-making it difficult to fairly compare safety, utility, and robustness across methods. To address this, we introduce SafeTuneBed, a benchmark and toolkit unifying fine-tuning and defense evaluation. SafeTuneBed (i) curates a diverse repository of multiple fine-tuning datasets spanning sentiment analysis, question-answering, multi-step reasoning, and open-ended instruction tasks, and allows for the generation of harmful-variant splits; (ii) enables integration of state-of-the-art defenses, including alignment-stage immunization, in-training safeguards, and post-tuning repair; and (iii) provides evaluators for safety (attack success rate, refusal consistency) and utility. Built on Python-first, dataclass-driven configs and plugins, SafeTuneBed requires minimal additional code to specify any fine-tuning regime, defense method, and metric suite, while ensuring end-to-end reproducibility. We showcase its value by benchmarking representative defenses across varied poisoning scenarios and tasks. By standardizing data, code, and metrics, SafeTuneBed is the first focused toolkit of its kind to accelerate rigorous and comparable research in safe LLM fine-tuning. Code is available at: https://github.com/criticalml-uw/SafeTuneBed",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† SafeTuneBedï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¾®è°ƒä¸é˜²å¾¡è¯„ä¼°çš„åŸºå‡†æµ‹è¯•å·¥å…·åŒ…ï¼Œæ—¨åœ¨è§£å†³å½“å‰è¯„ä¼°æ•°æ®é›†ã€æŒ‡æ ‡å’Œå¨èƒè®¾å®šä¸ä¸€è‡´å¯¼è‡´çš„æ–¹æ³•é—´éš¾ä»¥å…¬å¹³æ¯”è¾ƒçš„é—®é¢˜ã€‚SafeTuneBed æ±‡é›†äº†æ¶µç›–æƒ…æ„Ÿåˆ†æã€é—®ç­”ã€å¤šæ­¥æ¨ç†åŠæŒ‡ä»¤ä»»åŠ¡çš„å¤šç§å¾®è°ƒæ•°æ®é›†ï¼Œå¹¶æ”¯æŒç”Ÿæˆæœ‰å®³å˜ä½“æ ·æœ¬ä»¥æ¨¡æ‹Ÿæ¯’åŒ–åœºæ™¯ã€‚è¯¥å·¥å…·åŒ…æ•´åˆäº†å¯¹é½é˜¶æ®µå…ç–« (alignment-stage immunization)ã€è®­ç»ƒä¸­ä¿æŠ¤æªæ–½ (in-training safeguards) å’Œè°ƒä¼˜åä¿®å¤ (post-tuning repair) ç­‰å‰æ²¿é˜²å¾¡æŠ€æœ¯ã€‚é€šè¿‡æä¾›é’ˆå¯¹å®‰å…¨æ€§ï¼ˆå¦‚æ”»å‡»æˆåŠŸç‡ Attack Success Rate å’Œæ‹’ç»ä¸€è‡´æ€§ Refusal Consistencyï¼‰åŠå®ç”¨æ€§çš„è¯„ä¼°å™¨ï¼Œè¯¥æ¡†æ¶ç¡®ä¿äº†å®éªŒçš„ç«¯åˆ°ç«¯å¯é‡å¤æ€§ã€‚åˆ©ç”¨ Python ä¼˜å…ˆå’Œæ•°æ®ç±»é©±åŠ¨ (dataclass-driven) çš„é…ç½®æ’ä»¶ç³»ç»Ÿï¼Œå¼€å‘è€…å¯ä»¥ä»¥æä½çš„ä»£ç é‡å¿«é€Ÿå®šä¹‰å¾®è°ƒæ–¹æ¡ˆå’ŒæŒ‡æ ‡ç»„ä»¶ã€‚ä½œä¸ºé¦–ä¸ªä¸“æ³¨äºå®‰å…¨å¾®è°ƒçš„ç»¼åˆå·¥å…·åŒ…ï¼ŒSafeTuneBed ä¸ºåŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹å®‰å…¨å¯¹é½çš„ä¸¥è°¨ç ”ç©¶æä¾›äº†æ ‡å‡†åŒ–å¹³å°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00676v1",
      "published_date": "2025-05-31 19:00:58 UTC",
      "updated_date": "2025-05-31 19:00:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:54:38.352860+00:00"
    },
    {
      "arxiv_id": "2506.00674v1",
      "title": "Thinking Out of the Box: Hybrid SAT Solving by Unconstrained Continuous Optimization",
      "title_zh": "è·³å‡ºâ€œæ–¹æ¡†â€ï¼šåŸºäºæ— çº¦æŸè¿ç»­ä¼˜åŒ–çš„æ··åˆ SAT æ±‚è§£",
      "authors": [
        "Zhiwei Zhang",
        "Samy Wu Fung",
        "Anastasios Kyrillidis",
        "Stanley Osher",
        "Moshe Y. Vardi"
      ],
      "abstract": "The Boolean satisfiability (SAT) problem lies at the core of many applications in combinatorial optimization, software verification, cryptography, and machine learning. While state-of-the-art solvers have demonstrated high efficiency in handling conjunctive normal form (CNF) formulas, numerous applications require non-CNF (hybrid) constraints, such as XOR, cardinality, and Not-All-Equal constraints. Recent work leverages polynomial representations to represent such hybrid constraints, but it relies on box constraints that can limit the use of powerful unconstrained optimizers. In this paper, we propose unconstrained continuous optimization formulations for hybrid SAT solving by penalty terms. We provide theoretical insights into when these penalty terms are necessary and demonstrate empirically that unconstrained optimizers (e.g., Adam) can enhance SAT solving on hybrid benchmarks. Our results highlight the potential of combining continuous optimization and machine-learning-based methods for effective hybrid SAT solving.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¸ƒå°”å¯æ»¡è¶³æ€§(Boolean satisfiability, SAT)é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ç°æœ‰æ±‚è§£å™¨åœ¨å¤„ç†XORã€åŸºæ•°å’ŒNot-All-Equalç­‰éåˆå–èŒƒå¼(non-CNF)æ··åˆçº¦æŸæ—¶çš„æ•ˆç‡æŒ‘æˆ˜ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•å¸¸åˆ©ç”¨å¤šé¡¹å¼è¡¨ç¤ºæ¥å¤„ç†æ­¤ç±»æ··åˆçº¦æŸï¼Œä½†å…¶å¯¹æ¡†çº¦æŸ(box constraints)çš„ä¾èµ–é™åˆ¶äº†é«˜æ•ˆæ— çº¦æŸä¼˜åŒ–å™¨çš„ä½¿ç”¨ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºç½šé¡¹(penalty terms)çš„æ··åˆSATæ±‚è§£æ— çº¦æŸè¿ç»­ä¼˜åŒ–æ¨¡å‹ï¼Œå¹¶ä»ç†è®ºä¸Šåˆ†æäº†è¿™äº›ç½šé¡¹åœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„å¿…è¦æ€§ã€‚å®éªŒè¯æ˜ï¼Œé‡‡ç”¨Adamç­‰æ— çº¦æŸä¼˜åŒ–å™¨(unconstrained optimizers)å¯ä»¥æ˜¾è‘—å¢å¼ºåœ¨æ··åˆåŸºå‡†æµ‹è¯•ä¸­çš„æ±‚è§£èƒ½åŠ›ã€‚è¯¥æˆæœå±•ç¤ºäº†å°†è¿ç»­ä¼˜åŒ–ä¸æœºå™¨å­¦ä¹ æŠ€æœ¯ç›¸ç»“åˆä»¥åº”å¯¹å¤æ‚æ··åˆSATé—®é¢˜çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.LG",
        "math.OC"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00674v1",
      "published_date": "2025-05-31 18:58:33 UTC",
      "updated_date": "2025-05-31 18:58:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:54:36.178936+00:00"
    },
    {
      "arxiv_id": "2506.00664v1",
      "title": "OntoRAG: Enhancing Question-Answering through Automated Ontology Derivation from Unstructured Knowledge Bases",
      "title_zh": "OntoRAGï¼šé€šè¿‡ä»éç»“æ„åŒ–çŸ¥è¯†åº“ä¸­è‡ªåŠ¨æ¨å¯¼æœ¬ä½“æ¥å¢å¼ºé—®ç­”èƒ½åŠ›",
      "authors": [
        "Yash Tiwari",
        "Owais Ahmad Lone",
        "Mayukha Pal"
      ],
      "abstract": "Ontologies are pivotal for structuring knowledge bases to enhance question answering (QA) systems powered by Large Language Models (LLMs). However, traditional ontology creation relies on manual efforts by domain experts, a process that is time intensive, error prone, and impractical for large, dynamic knowledge domains. This paper introduces OntoRAG, an automated pipeline designed to derive ontologies from unstructured knowledge bases, with a focus on electrical relay documents. OntoRAG integrates advanced techniques, including web scraping, PDF parsing, hybrid chunking, information extraction, knowledge graph construction, and ontology creation, to transform unstructured data into a queryable ontology. By leveraging LLMs and graph based methods, OntoRAG enhances global sensemaking capabilities, outperforming conventional Retrieval Augmented Generation (RAG) and GraphRAG approaches in comprehensiveness and diversity. Experimental results demonstrate OntoRAGs effectiveness, achieving a comprehensiveness win rate of 85% against vector RAG and 75% against GraphRAGs best configuration. This work addresses the critical challenge of automating ontology creation, advancing the vision of the semantic web.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OntoRAGï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä»éç»“æ„åŒ–çŸ¥è¯†åº“ä¸­è‡ªåŠ¨æ¨å¯¼æœ¬ä½“(Ontology)çš„è‡ªåŠ¨åŒ–æµç¨‹ï¼Œä»¥å¢å¼ºåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„é—®ç­”ç³»ç»Ÿ(QA)ã€‚é’ˆå¯¹ä¼ ç»Ÿæ‰‹åŠ¨æ„å»ºæœ¬ä½“è€—æ—¶ä¸”æ˜“å‡ºé”™çš„é—®é¢˜ï¼ŒOntoRAGé›†æˆäº†ç½‘é¡µçˆ¬å–ã€PDFè§£æã€æ··åˆåˆ†å—(Hybrid Chunking)ã€ä¿¡æ¯æå–ä»¥åŠçŸ¥è¯†å›¾è°±(Knowledge Graph)æ„å»ºç­‰å…ˆè¿›æŠ€æœ¯ï¼Œå°†éç»“æ„åŒ–æ•°æ®è½¬åŒ–ä¸ºå¯æŸ¥è¯¢çš„æœ¬ä½“ã€‚è¯¥æ¡†æ¶åˆ©ç”¨LLMså’ŒåŸºäºå›¾çš„æ–¹æ³•æå‡äº†å…¨å±€ç†è§£èƒ½åŠ›(Global Sensemaking)ï¼Œåœ¨ç»¼åˆæ€§å’Œå¤šæ ·æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)å’ŒGraphRAGæ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOntoRAGåœ¨ç»¼åˆæ€§æ–¹é¢çš„èƒœç‡åˆ†åˆ«è¾¾åˆ°85%ï¼ˆå¯¹æ¯”Vector RAGï¼‰å’Œ75%ï¼ˆå¯¹æ¯”GraphRAGçš„æœ€ä½³é…ç½®ï¼‰ã€‚è¿™é¡¹å·¥ä½œæœ‰æ•ˆè§£å†³äº†è‡ªåŠ¨åŒ–æœ¬ä½“åˆ›å»ºçš„å…³é”®æŒ‘æˆ˜ï¼Œä¸ºæ¨åŠ¨è¯­ä¹‰ç½‘(Semantic Web)çš„å‘å±•æä¾›äº†æ–°çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00664v1",
      "published_date": "2025-05-31 18:33:39 UTC",
      "updated_date": "2025-05-31 18:33:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:54:41.968974+00:00"
    },
    {
      "arxiv_id": "2506.14794v1",
      "title": "Assembly of Experts: Linear-time construction of the Chimera LLM variants with emergent and adaptable behaviors",
      "title_zh": "Assembly of Expertsï¼šå…·æœ‰æ¶Œç°ä¸è‡ªé€‚åº”è¡Œä¸ºçš„ Chimera LLM å˜ä½“çš„çº¿æ€§æ—¶é—´æ„å»º",
      "authors": [
        "Henrik Klagges",
        "Robert Dahlke",
        "Fabian Klemm",
        "Benjamin Merkel",
        "Daniel Klingmann",
        "David A. Reiss",
        "Dan Zecha"
      ],
      "abstract": "Requiring $10^{13}$-$10^{15}$ FLOPs to calculate one 8 bit weight in an LLM during pretraining is extremely expensive and seems inefficient. To better leverage the huge investments made into pretrained models, we develop the new \"Assembly-of-Experts\" (AoE) construction method to create capable child variants of existing Mixture-of-Experts parent models in linear time. Model weight tensors get interpolated individually, allowing to enhance or suppress semantic features of the parents.\n  Varying the proportion of weights taken from the parent models, we observe some properties of the AoE child model changing gradually, while other behavioral traits emerge with a sharp transition. Surprisingly, nearly every generated model is functional and capable, which makes searching the model space straightforward.\n  We construct the DeepSeek R1T \"Chimera\", a 671B open-weights hybrid model combining DeepSeek's V3-0324 and R1 model variants. The child inherits only the routed expert tensors of R1, but still achieves about R1-level intelligence. At the same time, it uses about 40\\% fewer output tokens, close to V3 speed. Constructed without any fine-tuning or distillation, the Chimera exhibits surprisingly compact, orderly reasoning compared to its parent models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)é¢„è®­ç»ƒæˆæœ¬æé«˜ä¸”æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå¼€å‘äº†ä¸€ç§åä¸ºâ€œAssembly-of-Expertsâ€(AoE)çš„æ–°å‹æ„å»ºæ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡çº¿æ€§æ—¶é—´æ„å»ºç°æœ‰æ··åˆä¸“å®¶(Mixture-of-Experts)çˆ¶æ¨¡å‹çš„å­å˜ä½“ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹æ¨¡å‹æƒé‡å¼ é‡è¿›è¡Œç‹¬ç«‹æ’å€¼ï¼Œå®ç°äº†å¯¹çˆ¶æ¨¡å‹è¯­ä¹‰ç‰¹å¾çš„å¢å¼ºæˆ–æŠ‘åˆ¶ï¼Œä¸”å®éªŒå‘ç°ç”Ÿæˆçš„æ¨¡å‹ç»å¤§å¤šæ•°å‡å…·å¤‡è‰¯å¥½çš„åŠŸèƒ½æ€§ã€‚é€šè¿‡å°†DeepSeek V3-0324ä¸R1å˜ä½“ç»“åˆï¼Œç ”ç©¶è€…æˆåŠŸæ„å»ºäº†åä¸ºâ€œChimeraâ€çš„671Bå¼€æºæ··åˆæ¨¡å‹ã€‚åœ¨æ— éœ€ä»»ä½•å¾®è°ƒ(fine-tuning)æˆ–è’¸é¦(distillation)çš„æƒ…å†µä¸‹ï¼ŒChimeraä¸ä»…ç»§æ‰¿äº†æ¥è¿‘R1æ°´å¹³çš„æ™ºèƒ½ï¼Œè¿˜å‡å°‘äº†çº¦40%çš„è¾“å‡ºTokenï¼Œå®ç°äº†æ¥è¿‘V3çš„æ¨ç†é€Ÿåº¦ã€‚æ­¤å¤–ï¼ŒChimeraåœ¨é€»è¾‘æ¨ç†ä¸Šè¡¨ç°å‡ºæ¯”çˆ¶æ¨¡å‹æ›´ç´§å‡‘ã€æ›´æœ‰åºçš„ç‰¹æ€§ï¼Œä¸ºé«˜æ•ˆæ„å»ºå…·å¤‡æ–°å…´å’Œé€‚åº”æ€§è¡Œä¸ºçš„å¤§æ¨¡å‹æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.14794v1",
      "published_date": "2025-05-31 18:23:19 UTC",
      "updated_date": "2025-05-31 18:23:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:54:44.570821+00:00"
    },
    {
      "arxiv_id": "2506.12060v1",
      "title": "Organizational Adaptation to Generative AI in Cybersecurity: A Systematic Review",
      "title_zh": "ç½‘ç»œå®‰å…¨ç»„ç»‡å¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„é€‚åº”ï¼šç³»ç»Ÿæ€§ç»¼è¿°",
      "authors": [
        "Christopher Nott"
      ],
      "abstract": "Cybersecurity organizations are adapting to GenAI integration through modified frameworks and hybrid operational processes, with success influenced by existing security maturity, regulatory requirements, and investments in human capital and infrastructure. This qualitative research employs systematic document analysis and comparative case study methodology to examine how cybersecurity organizations adapt their threat modeling frameworks and operational processes to address generative artificial intelligence integration. Through examination of 25 studies from 2022 to 2025, the research documents substantial transformation in organizational approaches to threat modeling, moving from traditional signature-based systems toward frameworks incorporating artificial intelligence capabilities. The research identifies three primary adaptation patterns: Large Language Model integration for security applications, GenAI frameworks for risk detection and response automation, and AI/ML integration for threat hunting. Organizations with mature security infrastructures, particularly in finance and critical infrastructure sectors, demonstrate higher readiness through structured governance approaches, dedicated AI teams, and robust incident response processes. Organizations achieve successful GenAI integration when they maintain appropriate human oversight of automated systems, address data quality concerns and explainability requirements, and establish governance frameworks tailored to their specific sectors. Organizations encounter ongoing difficulties with privacy protection, bias reduction, personnel training, and defending against adversarial attacks. This work advances understanding of how organizations adopt innovative technologies in high-stakes environments and offers actionable insights for cybersecurity professionals implementing GenAI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡ç³»ç»Ÿæ–‡çŒ®åˆ†æå’Œæ¯”è¾ƒæ¡ˆä¾‹ç ”ç©¶ï¼Œæ¢è®¨äº†ç½‘ç»œå®‰å…¨ç»„ç»‡åœ¨æ•´åˆ Generative AI è¿‡ç¨‹ä¸­å¦‚ä½•è°ƒæ•´å¨èƒå»ºæ¨¡æ¡†æ¶å’Œæ“ä½œæµç¨‹ã€‚ç ”ç©¶åˆ†æäº†2022å¹´è‡³2025å¹´é—´çš„25é¡¹ç ”ç©¶ï¼Œè®°å½•äº†ç»„ç»‡ä»ä¼ ç»ŸåŸºäºç‰¹å¾çš„ç³»ç»Ÿå‘èåˆäººå·¥æ™ºèƒ½èƒ½åŠ›çš„æ¡†æ¶è½¬å˜çš„æ˜¾è‘—è¶‹åŠ¿ã€‚ç ”ç©¶è¯†åˆ«äº†ä¸‰ç§ä¸»è¦çš„é€‚åº”æ¨¡å¼ï¼ŒåŒ…æ‹¬ç”¨äºå®‰å…¨åº”ç”¨çš„ Large Language Model æ•´åˆã€ç”¨äºé£é™©æ£€æµ‹å’Œå“åº”è‡ªåŠ¨åŒ–çš„ Generative AI æ¡†æ¶ï¼Œä»¥åŠç”¨äº Threat hunting çš„ AI/ML æ•´åˆã€‚ç»“æœæ˜¾ç¤ºï¼Œé‡‘èå’Œå…³é”®åŸºç¡€è®¾æ–½ç­‰æ‹¥æœ‰æˆç†Ÿå®‰å…¨åŸºç¡€è®¾æ–½çš„éƒ¨é—¨ï¼Œé€šè¿‡ç»“æ„åŒ–æ²»ç†å’Œä¸“é—¨çš„ AI å›¢é˜Ÿè¡¨ç°å‡ºæ›´é«˜çš„å°±ç»ªåº¦ã€‚æˆåŠŸçš„æ•´åˆä¾èµ–äºäººå·¥ç›‘ç£ã€æ•°æ®è´¨é‡å’Œ Explainability è¦æ±‚ï¼Œå¹¶éœ€è¦å»ºç«‹ç‰¹å®šè¡Œä¸šçš„æ²»ç†æ¡†æ¶ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç»„ç»‡åœ¨éšç§ä¿æŠ¤ã€åè§å‡å°‘ä»¥åŠé˜²å¾¡ Adversarial attacks æ–¹é¢ä»é¢ä¸´æŒç»­æŒ‘æˆ˜ã€‚è¯¥å·¥ä½œæ·±åŒ–äº†å¯¹é«˜é£é™©ç¯å¢ƒä¸‹ç»„ç»‡é‡‡ç”¨åˆ›æ–°æŠ€æœ¯çš„ç†è§£ï¼Œå¹¶ä¸ºå®æ–½ Generative AI ç³»ç»Ÿçš„ç½‘ç»œå®‰å…¨ä¸“ä¸šäººå‘˜æä¾›äº†å®è·µè§è§£ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CR",
      "comment": "38 pages, 1 table, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2506.12060v1",
      "published_date": "2025-05-31 18:16:11 UTC",
      "updated_date": "2025-05-31 18:16:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:54:53.930358+00:00"
    },
    {
      "arxiv_id": "2506.00660v2",
      "title": "Differential privacy for medical deep learning: methods, tradeoffs, and deployment implications",
      "title_zh": "åŒ»å­¦æ·±åº¦å­¦ä¹ ä¸­çš„å·®åˆ†éšç§ï¼šæ–¹æ³•ã€æƒè¡¡ä¸éƒ¨ç½²å¯ç¤º",
      "authors": [
        "Marziyeh Mohammadi",
        "Mohsen Vejdanihemmat",
        "Mahshad Lotfinia",
        "Mirabela Rusu",
        "Daniel Truhn",
        "Andreas Maier",
        "Soroosh Tayebi Arasteh"
      ],
      "abstract": "Differential privacy (DP) is a key technique for protecting sensitive patient data in medical deep learning (DL). As clinical models grow more data-dependent, balancing privacy with utility and fairness has become a critical challenge. This scoping review synthesizes recent developments in applying DP to medical DL, with a particular focus on DP-SGD and alternative mechanisms across centralized and federated settings. Using a structured search strategy, we identified 74 studies published up to March 2025. Our analysis spans diverse data modalities, training setups, and downstream tasks, and highlights the tradeoffs between privacy guarantees, model accuracy, and subgroup fairness. We find that while DP-especially at strong privacy budgets-can preserve performance in well-structured imaging tasks, severe degradation often occurs under strict privacy, particularly in underrepresented or complex modalities. Furthermore, privacy-induced performance gaps disproportionately affect demographic subgroups, with fairness impacts varying by data type and task. A small subset of studies explicitly addresses these tradeoffs through subgroup analysis or fairness metrics, but most omit them entirely. Beyond DP-SGD, emerging approaches leverage alternative mechanisms, generative models, and hybrid federated designs, though reporting remains inconsistent. We conclude by outlining key gaps in fairness auditing, standardization, and evaluation protocols, offering guidance for future work toward equitable and clinically robust privacy-preserving DL systems in medicine.",
      "tldr_zh": "è¯¥ç»¼è¿°å¯¹æˆªè‡³2025å¹´3æœˆçš„74é¡¹ç ”ç©¶è¿›è¡Œäº†ç³»ç»Ÿæ€§å›é¡¾ï¼Œé‡ç‚¹æ¢è®¨äº†åœ¨åŒ»ç–—Deep Learning (DL)é¢†åŸŸåº”ç”¨Differential Privacy (DP)çš„æŠ€æœ¯ç°çŠ¶ã€‚æ–‡ç« æ·±å…¥åˆ†æäº†DP-SGDåŠå…¶ä»–æ›¿ä»£æœºåˆ¶åœ¨ä¸­å¿ƒåŒ–ä¸Federated Learningç¯å¢ƒä¸‹çš„åº”ç”¨ï¼Œå¹¶é‡ç‚¹æ­ç¤ºäº†éšç§ä¿è¯ã€æ¨¡å‹å‡†ç¡®ç‡ä¸å­ç¾¤ä½“Fairnessä¹‹é—´çš„å¤æ‚æƒè¡¡ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶åœ¨ç»“æ„è‰¯å¥½çš„åŒ»å­¦å½±åƒä»»åŠ¡ä¸­DPèƒ½ç»´æŒè¾ƒå¥½è¡¨ç°ï¼Œä½†åœ¨ä¸¥æ ¼çš„éšç§é¢„ç®—å’Œå¤æ‚æ¨¡æ€ä¸‹ï¼Œæ¨¡å‹æ€§èƒ½å¾€å¾€ä¼šå‡ºç°ä¸¥é‡è¡°é€€ã€‚æ­¤å¤–ï¼Œéšç§ä¿æŠ¤å¸¦æ¥çš„æ€§èƒ½æŸå¤±å¹¶ä¸å‡è¡¡ï¼Œå¾€å¾€ä¼šä¸æˆæ¯”ä¾‹åœ°å½±å“ç‰¹å®šäººå£ç»Ÿè®¡å­¦å­ç¾¤ä½“ï¼Œä»è€ŒåŠ å‰§æ¨¡å‹çš„ä¸å…¬å¹³æ€§ã€‚é’ˆå¯¹ç°æœ‰ç ”ç©¶ä¸­æ™®éå¿½è§†å…¬å¹³æ€§æŒ‡æ ‡å’Œå­ç¾¤ä½“åˆ†æçš„ç°çŠ¶ï¼Œæ–‡ç« æ€»ç»“äº†æ–°å…´çš„ç”Ÿæˆæ¨¡å‹ä¸æ··åˆæ¶æ„ç­‰æ›¿ä»£æ–¹æ¡ˆã€‚è¯¥ç ”ç©¶æœ€åæŒ‡å‡ºäº†å…¬å¹³æ€§å®¡è®¡ã€æ ‡å‡†åŒ–æµç¨‹åŠè¯„ä¼°åè®®ä¸­çš„å…³é”®ç©ºç™½ï¼Œä¸ºæœªæ¥å¼€å‘ä¸´åºŠé²æ£’ä¸”å…¼é¡¾å…¬å¹³çš„éšç§ä¿æŠ¤DLç³»ç»ŸæŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00660v2",
      "published_date": "2025-05-31 18:03:15 UTC",
      "updated_date": "2025-11-09 16:10:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:54:52.005995+00:00"
    },
    {
      "arxiv_id": "2506.00658v3",
      "title": "Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and Emotion-Informed Techniques",
      "title_zh": "Sarc7ï¼šèåˆä¸ƒç§ç±»å‹ä¸æƒ…æ„Ÿå¼•å¯¼æŠ€æœ¯çš„åè®½æ£€æµ‹ä¸ç”Ÿæˆè¯„ä¼°",
      "authors": [
        "Lang Xiong",
        "Raina Gao",
        "Alyssa Jeong",
        "Yicheng Fu",
        "Sean O'Brien",
        "Vasu Sharma",
        "Kevin Zhu"
      ],
      "abstract": "Sarcasm is a form of humor where expressions convey meanings opposite to their literal interpretations. Classifying and generating sarcasm using large language models is vital for interpreting human communication. Sarcasm poses challenges for computational models, due to its nuanced nature. We introduce Sarc7, a benchmark that classifies 7 types of sarcasm: self-deprecating, brooding, deadpan, polite, obnoxious, raging, and manic by annotating entries of the MUStARD dataset. Classification was evaluated using zero-shot, few-shot, chain-of-thought (CoT), and a novel emotion-based prompting technique. We propose an emotion-based generation method developed by identifying key components of sarcasm-incongruity, shock value, and context dependency. Our classification experiments show that Gemini 2.5, using emotion-based prompting, outperforms other setups with an F1 score of 0.3664. Human evaluators preferred our emotion-based prompting, with 38.46% more successful generations than zero-shot prompting.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† Sarc7ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°è®½åˆºåˆ†ç±»ä¸ç”Ÿæˆçš„å…¨æ–°åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å¯¹ MUStARD æ•°æ®é›†çš„æ ‡æ³¨å°†è®½åˆºç»†åˆ†ä¸º self-deprecatingã€broodingã€deadpanã€politeã€obnoxiousã€raging å’Œ manic ä¸ƒç§ç±»å‹ã€‚ä¸ºäº†è§£å†³è®¡ç®—æ¨¡å‹åœ¨å¤„ç†è®½åˆºæ—¶é¢ä¸´çš„ç»†å¾®æŒ‘æˆ˜ï¼Œç ”ç©¶è€…è¯„ä¼°äº† zero-shotã€few-shotã€Chain-of-Thought (CoT) ä»¥åŠä¸€ç§åˆ›æ–°çš„ emotion-based prompting æŠ€æœ¯ã€‚åŒæ—¶ï¼Œç ”ç©¶é€šè¿‡è¯†åˆ«è®½åˆºçš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼ŒåŒ…æ‹¬ incongruityã€shock value å’Œ context dependencyï¼Œæå‡ºäº†ä¸€ç§åŸºäºæƒ…æ„Ÿçš„ç”Ÿæˆæ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨ emotion-based prompting çš„ Gemini 2.5 åœ¨åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ï¼Œå–å¾—äº† 0.3664 çš„ F1 scoreã€‚äººç±»è¯„ä¼°ç»“æœä¹Ÿè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„è®½åˆºå†…å®¹æ¯” zero-shot æç¤ºçš„æˆåŠŸç‡é«˜å‡º 38.46%ã€‚è¯¥é¡¹å·¥ä½œè¯æ˜äº†ç»“åˆæƒ…æ„Ÿä¿¡æ¯çš„æç¤ºæŠ€æœ¯åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ç†è§£å’Œç”Ÿæˆå¤æ‚å¹½é»˜è¡¨è¾¾æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP WiNLP and COLM Melt, Solar, PragLM, and Origen",
      "pdf_url": "https://arxiv.org/pdf/2506.00658v3",
      "published_date": "2025-05-31 18:01:23 UTC",
      "updated_date": "2025-09-17 01:09:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:54:56.655006+00:00"
    },
    {
      "arxiv_id": "2506.00656v1",
      "title": "Permutation-Invariant Transformer Neural Architectures for Set-Based Indoor Localization Using Learned RSSI Embeddings",
      "title_zh": "åŸºäºå­¦ä¹ å‹ RSSI åµŒå…¥çš„ç½®æ¢ä¸å˜ Transformer ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç”¨äºåŸºäºé›†åˆçš„å®¤å†…å®šä½",
      "authors": [
        "Aris J. Aristorenas"
      ],
      "abstract": "We propose a permutation-invariant neural architecture for indoor localization using RSSI scans from Wi-Fi access points. Each scan is modeled as an unordered set of (BSSID, RSSI) pairs, where BSSIDs are mapped to learned embeddings and concatenated with signal strength. These are processed by a Set Transformer, enabling the model to handle variable-length, sparse inputs while learning attention-based representations over access point relationships. We evaluate the model on a dataset collected across a campus environment consisting of six buildings. Results show that the model accurately recovers fine-grained spatial structure and maintains performance across physically distinct domains. In our experiments, a simple LSTM consistently outperformed all other models, achieving the lowest mean localization error across three tasks (E1 - E3), with average errors as low as 2.23 m. The Set Transformer performed competitively, ranking second in every experiment and outperforming the MLP, RNN, and basic attention models, particularly in scenarios involving multiple buildings (E2) and multiple floors (E3). Performance degraded most in E2, where signal conditions varied substantially across buildings, highlighting the importance of architectural robustness to domain diversity. This work demonstrates that set-based neural models are a natural fit for signal-based localization, offering a principled approach to handling sparse, unordered inputs in real-world positioning tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é¢å‘å®¤å†…å®šä½çš„ç½®æ¢ä¸å˜(permutation-invariant)ç¥ç»æ¶æ„ï¼Œåˆ©ç”¨Wi-Fiæ¥å…¥ç‚¹çš„RSSIæ‰«ææ•°æ®å®ç°ç²¾å‡†å®šä½ã€‚ç ”ç©¶å°†æ¯æ¬¡æ‰«æå»ºæ¨¡ä¸º(BSSID, RSSI)å¯¹ç»„æˆçš„æ— åºé›†åˆï¼Œå¹¶å°†BSSIDæ˜ å°„ä¸ºä¹ å¾—çš„åµŒå…¥(learned embeddings)ï¼Œç»“åˆä¿¡å·å¼ºåº¦è¿›è¡Œå¤„ç†ã€‚é€šè¿‡å¼•å…¥Set Transformerï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å˜é•¿ä¸”ç¨€ç–çš„è¾“å…¥ï¼Œå¹¶å­¦ä¹ æ¥å…¥ç‚¹å…³ç³»ä¹‹é—´çš„æ³¨æ„åŠ›è¡¨ç¤ºã€‚åœ¨åŒ…å«å…­æ ‹å»ºç­‘çš„æ ¡å›­æ•°æ®é›†è¯„ä¼°ä¸­ï¼Œç®€å•çš„LSTMæ¨¡å‹åœ¨ä¸‰é¡¹ä»»åŠ¡ä¸­å‡è¡¨ç°æœ€ä½³ï¼Œå¹³å‡è¯¯å·®ä½è‡³2.23ç±³ã€‚Set Transformeråœ¨æ‰€æœ‰å®éªŒä¸­æ’åç¬¬äºŒï¼Œå…¶è¡¨ç°ä¼˜äºMLPã€RNNå’ŒåŸºç¡€æ³¨æ„åŠ›æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨å»ºç­‘å’Œå¤šæ¥¼å±‚çš„å¤æ‚åœºæ™¯ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºé›†åˆ(set-based)çš„ç¥ç»æ¨¡å‹æ˜¯å¤„ç†ä¿¡å·å®šä½ä¸­ç¨€ç–ã€æ— åºè¾“å…¥çš„ä¸€ç§è§„èŒƒä¸”æœ‰æ•ˆçš„æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2506.00656v1",
      "published_date": "2025-05-31 17:56:39 UTC",
      "updated_date": "2025-05-31 17:56:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:55:08.540144+00:00"
    },
    {
      "arxiv_id": "2506.00653v3",
      "title": "Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models",
      "title_zh": "çº¿æ€§è¡¨ç¤ºå¯è¿ç§»æ€§å‡è®¾ï¼šå€ŸåŠ©å°æ¨¡å‹å¼•å¯¼å¤§æ¨¡å‹",
      "authors": [
        "Femi Bello",
        "Anubrata Das",
        "Fanzhi Zeng",
        "Fangcong Yin",
        "Liu Leqi"
      ],
      "abstract": "It has been hypothesized that neural networks with similar architectures trained on similar data learn shared representations relevant to the learning task. We build on this idea by extending the conceptual framework where representations learned across models trained on the same data can be expressed as linear combinations of a \\emph{universal} set of basis features. These basis features underlie the learning task itself and remain consistent across models, regardless of scale. From this framework, we propose the \\textbf{Linear Representation Transferability (LRT)} Hypothesis -- that there exists an affine transformation between the representation spaces of different models. To test this hypothesis, we learn affine mappings between the hidden states of models of different sizes and evaluate whether steering vectors -- directions in hidden state space associated with specific model behaviors -- retain their semantic effect when transferred from small to large language models using the learned mappings. We find strong empirical evidence that such affine mappings can preserve steering behaviors. These findings suggest that representations learned by small models can be used to guide the behavior of large models, and that the LRT hypothesis may be a promising direction on understanding representation alignment across model scales.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Linear Representation Transferability (LRT) å‡è®¾ï¼Œä¸»å¼ åœ¨ä¸åŒè§„æ¨¡çš„ç¥ç»ç½‘ç»œæ¨¡å‹ä¹‹é—´å­˜åœ¨ä¸€ç§ä»¿å°„å˜æ¢(affine transformation)æ¥å»ºç«‹è¡¨ç¤ºç©ºé—´çš„è”ç³»ã€‚è¯¥å‡è®¾åŸºäºä¸€ä¸ªæ ¸å¿ƒæ¦‚å¿µæ¡†æ¶ï¼Œå³è·¨æ¨¡å‹å­¦ä¹ åˆ°çš„è¡¨ç¤ºå‡å¯è§†ä¸ºä¸€ç»„åæ˜ å­¦ä¹ ä»»åŠ¡æœ¬è´¨çš„é€šç”¨åŸºç‰¹å¾(universal basis features)çš„çº¿æ€§ç»„åˆã€‚ä¸ºäº†éªŒè¯è¿™ä¸€è§‚ç‚¹ï¼Œç ”ç©¶è€…åœ¨ä¸åŒå‚æ•°è§„æ¨¡æ¨¡å‹çš„éšè—çŠ¶æ€(hidden states)é—´å­¦ä¹ ä»¿å°„æ˜ å°„ï¼Œå¹¶æµ‹è¯•æ“æ§å‘é‡(steering vectors)åœ¨è¿ç§»åçš„è¯­ä¹‰è¡¨ç°ã€‚å®éªŒç»“æœæœ‰åŠ›åœ°è¯æ˜äº†è¿™ç§æ˜ å°„èƒ½å¤Ÿä¿ç•™ç‰¹å®šçš„æ“æ§è¡Œä¸ºï¼Œä½¿å¾—ä»å°æ¨¡å‹å­¦åˆ°çš„ç‰¹å¾å¯ä»¥ç›´æ¥ç”¨äºå¼•å¯¼å¤§æ¨¡å‹çš„è¡Œä¸ºã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£è·¨æ¨¡å‹å°ºåº¦çš„è¡¨ç¤ºå¯¹é½(representation alignment)æä¾›äº†é‡è¦çš„ç†è®ºä¾æ®å’Œå®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00653v3",
      "published_date": "2025-05-31 17:45:18 UTC",
      "updated_date": "2025-06-04 19:24:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:54:59.277821+00:00"
    },
    {
      "arxiv_id": "2506.00644v1",
      "title": "Clinical Annotations for Automatic Stuttering Severity Assessment",
      "title_zh": "ç”¨äºè‡ªåŠ¨å£åƒä¸¥é‡ç¨‹åº¦è¯„ä¼°çš„ä¸´åºŠæ ‡æ³¨",
      "authors": [
        "Ana Rita Valente",
        "Rufael Marew",
        "Hawau Olamide Toyin",
        "Hamdan Al-Ali",
        "Anelise Bohnen",
        "Inma Becerra",
        "Elsa Marta Soares",
        "Goncalo Leal",
        "Hanan Aldarmaki"
      ],
      "abstract": "Stuttering is a complex disorder that requires specialized expertise for effective assessment and treatment. This paper presents an effort to enhance the FluencyBank dataset with a new stuttering annotation scheme based on established clinical standards. To achieve high-quality annotations, we hired expert clinicians to label the data, ensuring that the resulting annotations mirror real-world clinical expertise. The annotations are multi-modal, incorporating audiovisual features for the detection and classification of stuttering moments, secondary behaviors, and tension scores. In addition to individual annotations, we additionally provide a test set with highly reliable annotations based on expert consensus for assessing individual annotators and machine learning models. Our experiments and analysis illustrate the complexity of this task that necessitates extensive clinical expertise for valid training and evaluation of stuttering assessment models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å£åƒ(Stuttering)è¯„ä¼°çš„å¤æ‚æ€§ï¼Œé€šè¿‡å¼•å…¥åŸºäºä¸´åºŠæ ‡å‡†çš„å…¨æ–°æ ‡æ³¨æ–¹æ¡ˆå¢å¼ºäº†FluencyBankæ•°æ®é›†ã€‚ç ”ç©¶å›¢é˜Ÿè˜è¯·ä¸“ä¸šä¸´åºŠåŒ»ç”Ÿå¯¹æ•°æ®è¿›è¡Œæ ‡æ³¨ï¼Œä»¥ç¡®ä¿æ ‡æ³¨ç»“æœèƒ½å¤ŸçœŸå®åæ˜ ä¸´åºŠä¸“ä¸šçŸ¥è¯†ã€‚è¿™äº›æ ‡æ³¨é‡‡ç”¨å¤šæ¨¡æ€(multi-modal)å½¢å¼ï¼Œç»“åˆè§†å¬ç‰¹å¾ä»¥å®ç°å¯¹å£åƒç¬é—´(stuttering moments)ã€æ¬¡ç”Ÿè¡Œä¸º(secondary behaviors)åŠå¼ åŠ›è¯„åˆ†(tension scores)çš„æ£€æµ‹ä¸åˆ†ç±»ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†ä¸€ä¸ªåŸºäºä¸“å®¶å…±è¯†çš„é«˜å¯é æ€§æµ‹è¯•é›†ï¼Œæ—¨åœ¨ä¸ºæ ‡æ³¨å‘˜å’Œæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¯„ä¼°æä¾›æ ‡å‡†ã€‚å®éªŒåˆ†æå¼ºè°ƒäº†è¯¥ä»»åŠ¡çš„å¤æ‚æ€§ï¼Œå¹¶æŒ‡å‡ºæœ‰æ•ˆçš„è‡ªåŠ¨å£åƒä¸¥é‡ç¨‹åº¦è¯„ä¼°æ¨¡å‹çš„è®­ç»ƒä¸è¯„ä»·ç¦»ä¸å¼€æ·±åšçš„ä¸´åºŠä¸“ä¸šèƒŒæ™¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at INTERSPEECH 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00644v1",
      "published_date": "2025-05-31 17:18:20 UTC",
      "updated_date": "2025-05-31 17:18:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:55:27.946365+00:00"
    },
    {
      "arxiv_id": "2506.00643v3",
      "title": "SATA-BENCH: Select All That Apply Benchmark for Multiple Choice Questions",
      "title_zh": "SATA-BENCHï¼šé’ˆå¯¹å¤šé¡¹é€‰æ‹©é¢˜çš„â€œé€‰æ‹©æ‰€æœ‰é€‚ç”¨é¡¹â€è¯„ä¼°åŸºå‡†",
      "authors": [
        "Weijie Xu",
        "Shixian Cui",
        "Xi Fang",
        "Chi Xue",
        "Stephanie Eckman",
        "Chandan K. Reddy"
      ],
      "abstract": "Large language models (LLMs) are increasingly evaluated on single-answer multiple-choice tasks, yet many real-world problems require identifying all correct answers from a set of options. This capability remains underexplored. We introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on Select All That Apply (SATA) questions across diverse domains, including reading comprehension, law, and biomedicine. Our evaluation of 27 open-source and proprietary models reveals a significant gap: even the strongest model achieves only 41.8% exact match, exposing LLMs' inability to reliably identify all correct answers. We find that this weakness stems from two core challenges: selection bias - models favor certain choices regardless of content, and count bias - models fail to predict the correct number of answers. To address these issues, we propose Choice Funnel, a decoding strategy that combines token debiasing with adaptive thresholding to guide models toward complete and accurate selections. Choice Funnel achieves up to 29% higher exact match than competitive baselines while reducing inference cost by over 64%. Our findings expose fundamental limitations in current LLMs and introduce a new framework for diagnosing and improving multi-answer reasoning. We release SATA-BENCH and Choice Funnel to promote LLM development for robust decision-making in realistic, multi-answer applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œè™½ç„¶å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å•é€‰é¢˜ä¸Šè¯„ä¼°å……åˆ†ï¼Œä½†åœ¨å¤„ç†éœ€è¦è¯†åˆ«æ‰€æœ‰æ­£ç¡®ç­”æ¡ˆçš„â€œå¤šé¡¹é€‰æ‹©ï¼ˆSelect All That Apply, SATAï¼‰â€ä»»åŠ¡æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ¨å‡ºäº† SATA-BENCHï¼Œè¿™æ˜¯é¦–ä¸ªæ¶µç›–é˜…è¯»ç†è§£ã€æ³•å¾‹å’Œç”Ÿç‰©åŒ»å­¦ç­‰é¢†åŸŸçš„ SATA ä»»åŠ¡ä¸“ç”¨åŸºå‡†æµ‹è¯•ã€‚é€šè¿‡å¯¹27ä¸ªæ¨¡å‹çš„è¯„ä¼°å‘ç°ï¼Œæœ€å¼ºæ¨¡å‹çš„å®Œå…¨åŒ¹é…ï¼ˆExact Matchï¼‰ç‡ä»…ä¸º41.8%ï¼Œæš´éœ²å‡ºæ¨¡å‹åœ¨è¯†åˆ«æ‰€æœ‰æ­£ç¡®é€‰é¡¹æ–¹é¢çš„èƒ½åŠ›ç¼ºå¤±ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†è¿™ç§ä¸è¶³æºäºé€‰æ‹©åè§ï¼ˆSelection Biasï¼‰å’Œæ•°é‡åè§ï¼ˆCount Biasï¼‰ä¸¤å¤§æ ¸å¿ƒé—®é¢˜ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶æå‡ºäº† Choice Funnel è§£ç ç­–ç•¥ï¼Œé€šè¿‡ç»“åˆä»¤ç‰Œå»åï¼ˆToken Debiasingï¼‰å’Œè‡ªé€‚åº”é˜ˆå€¼ï¼ˆAdaptive Thresholdingï¼‰å¼•å¯¼æ¨¡å‹åšå‡ºå‡†ç¡®é€‰æ‹©ã€‚å®éªŒè¯æ˜ï¼ŒChoice Funnel åœ¨æå‡å®Œå…¨åŒ¹é…ç‡é«˜è¾¾29%çš„åŒæ—¶ï¼Œè¿˜å°†æ¨ç†æˆæœ¬é™ä½äº†64%ä»¥ä¸Šã€‚è¯¥å·¥ä½œä¸ºè¯Šæ–­å’Œå¢å¼º LLMs çš„å¤šç­”æ¡ˆæ¨ç†èƒ½åŠ›æä¾›äº†é‡è¦æ¡†æ¶ï¼Œæ¨åŠ¨äº†æ¨¡å‹åœ¨å¤æ‚å†³ç­–åœºæ™¯ä¸­çš„åº”ç”¨ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "40 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.00643v3",
      "published_date": "2025-05-31 17:14:21 UTC",
      "updated_date": "2025-10-17 23:12:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:55:31.036461+00:00"
    },
    {
      "arxiv_id": "2506.00641v2",
      "title": "AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents",
      "title_zh": "AgentAuditorï¼šé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„äººç±»æ°´å¹³å®‰å…¨ä¸å®‰å…¨æ€§è¯„ä¼°",
      "authors": [
        "Hanjun Luo",
        "Shenyu Dai",
        "Chiming Ni",
        "Xinfeng Li",
        "Guibin Zhang",
        "Kun Wang",
        "Tongliang Liu",
        "Hanan Salam"
      ],
      "abstract": "Despite the rapid advancement of LLM-based agents, the reliable evaluation of their safety and security remains a significant challenge. Existing rule-based or LLM-based evaluators often miss dangers in agents' step-by-step actions, overlook subtle meanings, fail to see how small issues compound, and get confused by unclear safety or security rules. To overcome this evaluation crisis, we introduce AgentAuditor, a universal, training-free, memory-augmented reasoning framework that empowers LLM evaluators to emulate human expert evaluators. AgentAuditor constructs an experiential memory by having an LLM adaptively extract structured semantic features (e.g., scenario, risk, behavior) and generate associated chain-of-thought reasoning traces for past interactions. A multi-stage, context-aware retrieval-augmented generation process then dynamically retrieves the most relevant reasoning experiences to guide the LLM evaluator's assessment of new cases. Moreover, we developed ASSEBench, the first benchmark designed to check how well LLM-based evaluators can spot both safety risks and security threats. ASSEBench comprises 2293 meticulously annotated interaction records, covering 15 risk types across 29 application scenarios. A key feature of ASSEBench is its nuanced approach to ambiguous risk situations, employing \"Strict\" and \"Lenient\" judgment standards. Experiments demonstrate that AgentAuditor not only consistently improves the evaluation performance of LLMs across all benchmarks but also sets a new state-of-the-art in LLM-as-a-judge for agent safety and security, achieving human-level accuracy. Our work is openly accessible at https://github.com/Astarojth/AgentAuditor.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ LLM æ™ºèƒ½ä½“åœ¨å®‰å…¨å’Œå®‰å…¨æ€§è¯„ä¼°ä¸­å­˜åœ¨çš„åŠ¨ä½œå±é™©æ¼æ£€ã€è¯­ä¹‰å¿½ç•¥åŠè§„åˆ™æ··æ·†ç­‰å±€é™ï¼Œæå‡ºäº† AgentAuditor æ¡†æ¶ã€‚AgentAuditor æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒä¸”åŸºäºè®°å¿†å¢å¼ºæ¨ç†çš„é€šç”¨æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿ LLM è¯„ä¼°å™¨æ¨¡æ‹Ÿäººç±»ä¸“å®¶çš„è¯„ä¼°èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªé€‚åº”æå–ç»“æ„åŒ–è¯­ä¹‰ç‰¹å¾å¹¶ç”Ÿæˆé“¾å¼æ€ç»´(Chain-of-Thought)æ¨ç†è½¨è¿¹æ¥æ„å»ºç»éªŒè®°å¿†ï¼Œå¹¶åˆ©ç”¨å¤šé˜¶æ®µã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)è¿‡ç¨‹æŒ‡å¯¼å¯¹æ–°æ¡ˆä¾‹çš„è¯„ä¼°ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†é¦–ä¸ªç”¨äºæ£€æµ‹ LLM è¯„ä¼°å™¨è¯†åˆ«å®‰å…¨é£é™©å’Œå®‰å…¨å¨èƒèƒ½åŠ›çš„åŸºå‡†æµ‹è¯• ASSEBenchï¼ŒåŒ…å«è¦†ç›– 15 ç§é£é™©ç±»å‹å’Œ 29 ä¸ªåº”ç”¨åœºæ™¯çš„ 2293 æ¡è¯¦å°½æ ‡æ³¨è®°å½•ã€‚ASSEBench é‡‡ç”¨äº†åŒºåˆ†â€œä¸¥æ ¼â€å’Œâ€œå®½æ¾â€åˆ¤æ–­æ ‡å‡†çš„ç»†è‡´æ–¹æ³•ï¼Œä»¥åº”å¯¹æ¨¡ç³Šçš„é£é™©æƒ…å¢ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAgentAuditor æ˜¾è‘—æå‡äº† LLMs åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨ç°ï¼Œå¹¶åœ¨ LLM-as-a-judge é¢†åŸŸè¾¾åˆ°äº†äººç±»æ°´å¹³çš„å‡†ç¡®ç‡ï¼Œç¡®ç«‹äº†æ–°çš„æœ€å…ˆè¿›(SOTA)æ ‡å‡†ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper is accepted by 39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.00641v2",
      "published_date": "2025-05-31 17:10:23 UTC",
      "updated_date": "2025-10-19 05:10:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:55:32.450064+00:00"
    },
    {
      "arxiv_id": "2506.00637v2",
      "title": "Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution's Characteristics",
      "title_zh": "åˆ©ç”¨è¾“å‡ºåˆ†å¸ƒç‰¹æ€§æå‡æ–‡æœ¬ç”Ÿæˆç½®ä¿¡åº¦è¯„åˆ†çš„æ ¡å‡†æ•ˆæœ",
      "authors": [
        "Lorenzo Jaime Yu Flores",
        "Ori Ernst",
        "Jackie Chi Kit Cheung"
      ],
      "abstract": "Well-calibrated model confidence scores can improve the usefulness of text generation models. For example, users can be prompted to review predictions with low confidence scores, to prevent models from returning bad or potentially dangerous predictions. However, confidence metrics are not always well calibrated in text generation. One reason is that in generation, there can be many valid answers, which previous methods do not always account for. Hence, a confident model could distribute its output probability among multiple sequences because they are all valid. We propose task-agnostic confidence metrics suited to generation, which rely solely on the probabilities associated with the model outputs without the need for further fine-tuning or heuristics. Using these, we are able to improve the calibration of BART and Flan-T5 on summarization, translation, and QA datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ä¸­ç½®ä¿¡åº¦å¾—åˆ†(Confidence Scores)æ ¡å‡†æ€§ä¸ä½³çš„é—®é¢˜ï¼ŒæŒ‡å‡ºå…¶æ ¸å¿ƒåŸå› åœ¨äºç”Ÿæˆä»»åŠ¡é€šå¸¸å­˜åœ¨å¤šä¸ªæœ‰æ•ˆç­”æ¡ˆï¼Œå¯¼è‡´æ¨¡å‹å°†æ¦‚ç‡åˆ†æ•£åœ¨å¤šä¸ªåˆæ³•åºåˆ—ä¸­ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç³»åˆ—ä»»åŠ¡æ— å…³(Task-agnostic)çš„ç½®ä¿¡åº¦æŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡å®Œå…¨ä¾èµ–äºæ¨¡å‹è¾“å‡ºåˆ†å¸ƒ(Output Distribution)çš„ç‰¹å¾ï¼Œæ— éœ€é¢å¤–çš„å¾®è°ƒ(Fine-tuning)æˆ–å¯å‘å¼æ–¹æ³•ã€‚é€šè¿‡åˆ†æè¾“å‡ºæ¦‚ç‡çš„å†…åœ¨å±æ€§ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰æ¨¡å‹ç”Ÿæˆçš„çœŸå®ç½®ä¿¡æ°´å¹³ã€‚å®éªŒåœ¨æ‘˜è¦ã€ç¿»è¯‘å’Œé—®ç­”(QA)æ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼ŒæˆåŠŸæå‡äº†BARTå’ŒFlan-T5æ¨¡å‹çš„æ ¡å‡†æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œä¸ºè¯†åˆ«ä½ç½®ä¿¡åº¦é¢„æµ‹å¹¶é˜²æ­¢æ¨¡å‹è¾“å‡ºé”™è¯¯æˆ–å±é™©å†…å®¹æä¾›äº†æ›´å¯é çš„è¡¡é‡æ ‡å‡†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2506.00637v2",
      "published_date": "2025-05-31 17:01:45 UTC",
      "updated_date": "2025-06-13 03:30:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:55:31.953325+00:00"
    },
    {
      "arxiv_id": "2506.00635v2",
      "title": "Learning with Calibration: Exploring Test-Time Computing of Spatio-Temporal Forecasting",
      "title_zh": "æ ¡å‡†å­¦ä¹ ï¼šæ¢ç©¶æ—¶ç©ºé¢„æµ‹çš„æµ‹è¯•æ—¶è®¡ç®—",
      "authors": [
        "Wei Chen",
        "Yuxuan Liang"
      ],
      "abstract": "Spatio-temporal forecasting is crucial in many domains, such as transportation, meteorology, and energy. However, real-world scenarios frequently present challenges such as signal anomalies, noise, and distributional shifts. Existing solutions primarily enhance robustness by modifying network architectures or training procedures. Nevertheless, these approaches are computationally intensive and resource-demanding, especially for large-scale applications. In this paper, we explore a novel test-time computing paradigm, namely learning with calibration, ST-TTC, for spatio-temporal forecasting. Through learning with calibration, we aim to capture periodic structural biases arising from non-stationarity during the testing phase and perform real-time bias correction on predictions to improve accuracy. Specifically, we first introduce a spectral-domain calibrator with phase-amplitude modulation to mitigate periodic shift and then propose a flash updating mechanism with a streaming memory queue for efficient test-time computation. ST-TTC effectively bypasses complex training-stage techniques, offering an efficient and generalizable paradigm. Extensive experiments on real-world datasets demonstrate the effectiveness, universality, flexibility and efficiency of our proposed method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—¶ç©ºé¢„æµ‹(Spatio-temporal forecasting)ä¸­å¸¸è§çš„ä¿¡å·å¼‚å¸¸ã€å™ªå£°å’Œåˆ†å¸ƒåç§»ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºST-TTCçš„æµ‹è¯•æ—¶è®¡ç®—(Test-time computing)æ–°èŒƒå¼ã€‚è¯¥æ–¹æ³•æ—¨åœ¨æ•è·æµ‹è¯•é˜¶æ®µå› éå¹³ç¨³æ€§äº§ç”Ÿçš„å‘¨æœŸæ€§ç»“æ„åå·®ï¼Œå¹¶å¯¹é¢„æµ‹ç»“æœè¿›è¡Œå®æ—¶æ ¡æ­£ä»¥æå‡ç²¾åº¦ã€‚å…·ä½“è€Œè¨€ï¼Œç ”ç©¶å¼•å…¥äº†å…·æœ‰ç›¸ä½-æŒ¯å¹…è°ƒåˆ¶(phase-amplitude modulation)çš„é¢‘åŸŸæ ¡å‡†å™¨(spectral-domain calibrator)æ¥ç¼“è§£å‘¨æœŸæ€§åç§»ï¼Œå¹¶æå‡ºäº†ç»“åˆæµå¼è®°å¿†é˜Ÿåˆ—(streaming memory queue)çš„å¿«é€Ÿæ›´æ–°æœºåˆ¶(flash updating mechanism)ä»¥ç¡®ä¿é«˜æ•ˆè®¡ç®—ã€‚ST-TTCæœ‰æ•ˆåœ°é¿å¼€äº†è®­ç»ƒé˜¶æ®µå¤æ‚çš„æ¶æ„æ”¹è¿›æˆ–è®­ç»ƒç¨‹åºï¼Œæä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”é€šç”¨çš„èŒƒå¼ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æœ‰æ•ˆæ€§ã€é€šç”¨æ€§ã€çµæ´»æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by NeurIPS 2025 (Spotlight)",
      "pdf_url": "https://arxiv.org/pdf/2506.00635v2",
      "published_date": "2025-05-31 16:48:27 UTC",
      "updated_date": "2025-10-29 08:25:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:55:43.966645+00:00"
    },
    {
      "arxiv_id": "2506.00633v2",
      "title": "Text-to-CT Generation via 3D Latent Diffusion Model with Contrastive Vision-Language Pretraining",
      "title_zh": "åŸºäº 3D æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸å¯¹æ¯”å¼è§†è§‰-è¯­è¨€é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ° CT ç”Ÿæˆ",
      "authors": [
        "Daniele Molino",
        "Camillo Maria Caruso",
        "Filippo Ruffini",
        "Paolo Soda",
        "Valerio Guarrasi"
      ],
      "abstract": "Objective: While recent advances in text-conditioned generative models have enabled the synthesis of realistic medical images, progress has been largely confined to 2D modalities such as chest X-rays. Extending text-to-image generation to volumetric CT remains a significant challenge, due to its high dimensionality, anatomical complexity, and the absence of robust frameworks that align vision-language data in 3D medical imaging. Methods: We introduce a novel architecture for Text-to-CT generation that combines a latent diffusion model with a 3D contrastive vision-language pretraining scheme. Our approach leverages a dual-encoder CLIP-style model trained on paired CT volumes and radiology reports to establish a shared embedding space, which serves as the conditioning input for generation. CT volumes are compressed into a low-dimensional latent space via a pretrained volumetric VAE, enabling efficient 3D denoising diffusion without requiring external super-resolution stages. Results: We evaluate our method on the CT-RATE dataset and conduct a comprehensive assessment of image fidelity, clinical relevance, and semantic alignment. Our model achieves competitive performance across all tasks, significantly outperforming prior baselines for text-to-CT generation. Moreover, we demonstrate that CT scans synthesized by our framework can effectively augment real data, improving downstream diagnostic performance. Conclusion: Our results show that modality-specific vision-language alignment is a key component for high-quality 3D medical image generation. By integrating contrastive pretraining and volumetric diffusion, our method offers a scalable and controllable solution for synthesizing clinically meaningful CT volumes from text, paving the way for new applications in data augmentation, medical education, and automated clinical simulation. Code at https://github.com/cosbidev/Text2CT.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ volumetric CT ç”Ÿæˆä¸­é¢ä¸´çš„é«˜ç»´æ€§ã€è§£å‰–å¤æ‚æ€§ä»¥åŠç¼ºä¹ 3D è§†è§‰è¯­è¨€å¯¹é½æ¡†æ¶ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆ latent diffusion model ä¸ 3D å¯¹æ¯”å¼è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆ3D contrastive vision-language pretrainingï¼‰çš„æ–°å‹æ¶æ„ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åœ¨é…å¯¹çš„ CT å·å’Œæ”¾å°„å­¦æŠ¥å‘Šä¸Šè®­ç»ƒçš„åŒç¼–ç å™¨ CLIP-style æ¨¡å‹å»ºç«‹å…±äº«åµŒå…¥ç©ºé—´ï¼Œä½œä¸ºç”Ÿæˆçš„è°ƒèŠ‚è¾“å…¥ï¼Œå¹¶é…åˆé¢„è®­ç»ƒçš„ volumetric VAE å®ç°é«˜æ•ˆçš„ 3D å»å™ªæ‰©æ•£ã€‚åœ¨ CT-RATE æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å›¾åƒä¿çœŸåº¦ã€ä¸´åºŠç›¸å…³æ€§å’Œè¯­ä¹‰å¯¹é½æ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆå‰çš„åŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯å®è¯¥æ¡†æ¶åˆæˆçš„ CT å›¾åƒèƒ½å¤Ÿæœ‰æ•ˆè¿›è¡Œæ•°æ®å¢å¼ºï¼ˆdata augmentationï¼‰ï¼Œä»è€Œæå‡ä¸‹æ¸¸è¯Šæ–­ä»»åŠ¡çš„æ€§èƒ½ã€‚è¯¥æˆæœå¼ºè°ƒäº†ç‰¹å®šæ¨¡æ€çš„è§†è§‰è¯­è¨€å¯¹é½åœ¨ 3D åŒ»å­¦å›¾åƒç”Ÿæˆä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œä¸ºåŒ»å­¦æ•™è‚²å’Œè‡ªåŠ¨åŒ–ä¸´åºŠæ¨¡æ‹Ÿæä¾›äº†å¯æ‰©å±•ä¸”å¯æ§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00633v2",
      "published_date": "2025-05-31 16:41:55 UTC",
      "updated_date": "2025-09-30 19:18:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:55:44.921752+00:00"
    },
    {
      "arxiv_id": "2506.00627v1",
      "title": "The Disparate Effects of Partial Information in Bayesian Strategic Learning",
      "title_zh": "è´å¶æ–¯ç­–ç•¥å­¦ä¹ ä¸­éƒ¨åˆ†ä¿¡æ¯çš„å·®å¼‚åŒ–å½±å“",
      "authors": [
        "Srikanth Avasarala",
        "Serena Wang",
        "Juba Ziani"
      ],
      "abstract": "We study how partial information about scoring rules affects fairness in strategic learning settings. In strategic learning, a learner deploys a scoring rule, and agents respond strategically by modifying their features -- at some cost -- to improve their outcomes. However, in our work, agents do not observe the scoring rule directly; instead, they receive a noisy signal of said rule. We consider two different agent models: (i) naive agents, who take the noisy signal at face value, and (ii) Bayesian agents, who update a prior belief based on the signal.\n  Our goal is to understand how disparities in outcomes arise between groups that differ in their costs of feature modification, and how these disparities vary with the level of transparency of the learner's rule. For naive agents, we show that utility disparities can grow unboundedly with noise, and that the group with lower costs can, perhaps counter-intuitively, be disproportionately harmed under limited transparency. In contrast, for Bayesian agents, disparities remain bounded. We provide a full characterization of disparities across groups as a function of the level of transparency and show that they can vary non-monotonically with noise; in particular, disparities are often minimized at intermediate levels of transparency. Finally, we extend our analysis to settings where groups differ not only in cost, but also in prior beliefs, and study how this asymmetry influences fairness.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨Bayesian Strategic LearningèƒŒæ™¯ä¸‹ï¼Œè¯„åˆ†è§„åˆ™çš„éƒ¨åˆ†ä¿¡æ¯å¦‚ä½•å½±å“ç­–ç•¥å­¦ä¹ ä¸­çš„å…¬å¹³æ€§ã€‚ç ”ç©¶æ„å»ºäº†ä¸¤ç§ä»£ç†æ¨¡å‹ï¼šç›´æ¥é‡‡çº³å™ªå£°ä¿¡å·çš„naive agentsä»¥åŠæ›´æ–°å…ˆéªŒä¿¡å¿µçš„Bayesian agentsï¼Œæ—¨åœ¨åˆ†æç‰¹å¾ä¿®æ”¹æˆæœ¬(cost of feature modification)å·®å¼‚åœ¨ä¸åŒé€æ˜åº¦æ°´å¹³ä¸‹å¯¹ç¾¤ä½“äº§å‡ºçš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œå¯¹äºnaive agentsï¼Œç¾¤ä½“æ•ˆç”¨å·®å¼‚éšå™ªå£°å¢åŠ è€Œæ— é™å¢é•¿ï¼Œä¸”ä½æˆæœ¬ç¾¤ä½“åœ¨ä½é€æ˜åº¦æ—¶å¯èƒ½é­å—ä¸æˆæ¯”ä¾‹çš„æŸå®³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒBayesian agentsçš„å·®å¼‚ä¿æŒåœ¨æœ‰é™èŒƒå›´å†…ï¼Œä¸”ä¸å™ªå£°æ°´å¹³å‘ˆéå•è°ƒå…³ç³»ï¼Œå·®å¼‚å¾€å¾€åœ¨ä¸­ç­‰é€æ˜åº¦æ—¶è¾¾åˆ°æœ€å°ã€‚æœ€åï¼Œè¯¥ç ”ç©¶è¿›ä¸€æ­¥æ¢è®¨äº†ç¾¤ä½“é—´å…ˆéªŒä¿¡å¿µ(prior beliefs)çš„ä¸å¯¹ç§°æ€§å¦‚ä½•å½±å“å…¬å¹³æ€§ï¼Œä¸ºç†è§£ç®—æ³•é€æ˜åº¦ä¸ç¤¾ä¼šå…¬å¹³çš„å…³ç³»æä¾›äº†ç†è®ºæ¡†æ¶ã€‚",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00627v1",
      "published_date": "2025-05-31 16:34:30 UTC",
      "updated_date": "2025-05-31 16:34:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:55:47.254834+00:00"
    },
    {
      "arxiv_id": "2506.04247v1",
      "title": "The GAIN Model: A Nature-Inspired Neural Network Framework Based on an Adaptation of the Izhikevich Model",
      "title_zh": "GAIN æ¨¡å‹ï¼šåŸºäº Izhikevich æ¨¡å‹æ”¹è¿›çš„å—è‡ªç„¶å¯å‘ç¥ç»ç½‘ç»œæ¡†æ¶",
      "authors": [
        "Gage K. R. Hooper"
      ],
      "abstract": "While many neural networks focus on layers to process information, the GAIN model uses a grid-based structure to improve biological plausibility and the dynamics of the model. The grid structure helps neurons to interact with their closest neighbors and improve their connections with one another, which is seen in biological neurons. While also being implemented with the Izhikevich model this approach allows for a computationally efficient and biologically accurate simulation that can aid in the development of neural networks, large scale simulations, and the development in the neuroscience field. This adaptation of the Izhikevich model can improve the dynamics and accuracy of the model, allowing for its uses to be specialized but efficient.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GAIN æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§åŸºäº Izhikevich æ¨¡å‹æ”¹è¿›çš„å—è‡ªç„¶å¯å‘çš„ç¥ç»ç½‘ç»œæ¡†æ¶ã€‚ä¸ä¾§é‡äºå±‚çº§ä¿¡æ¯å¤„ç†çš„ä¼ ç»Ÿç¥ç»ç½‘ç»œä¸åŒï¼ŒGAIN æ¨¡å‹é‡‡ç”¨äº†ç½‘æ ¼ç»“æ„ (grid-based structure) ä»¥æé«˜ç”Ÿç‰©åˆç†æ€§ (biological plausibility) å’Œæ¨¡å‹åŠ¨æ€ã€‚è¿™ç§ç½‘æ ¼ç»“æ„æ¨¡æ‹Ÿäº†ç”Ÿç‰©ç¥ç»å…ƒçš„ç‰¹æ€§ï¼Œä½¿ç¥ç»å…ƒèƒ½å¤Ÿä¸å…¶æœ€è¿‘çš„é‚»å±…è¿›è¡Œäº¤äº’å¹¶ä¼˜åŒ–å½¼æ­¤é—´çš„è¿æ¥ã€‚é€šè¿‡å¯¹ Izhikevich æ¨¡å‹è¿›è¡Œæ”¹è¿›ï¼Œè¯¥æ–¹æ³•å®ç°äº†ä¸€ç§è®¡ç®—æ•ˆç‡é«˜ä¸”ç”Ÿç‰©å­¦ç‰¹å¾å‡†ç¡®çš„æ¨¡æ‹Ÿï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒå¤§è§„æ¨¡æ¨¡æ‹Ÿ (large-scale simulations) å’Œç¥ç»ç§‘å­¦ (neuroscience) é¢†åŸŸçš„ç ”ç©¶ã€‚è¯¥æ¨¡å‹åœ¨æå‡åŠ¨æ€æ€§èƒ½å’Œå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œä¸ºå¼€å‘é«˜åº¦ä¸“ä¸šåŒ–ä¸”é«˜æ•ˆçš„ç¥ç»ç½‘ç»œæ¶æ„æä¾›äº†æ–°é€”å¾„ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "q-bio.NC",
      "comment": "31 pages, 16 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.04247v1",
      "published_date": "2025-05-31 16:30:34 UTC",
      "updated_date": "2025-05-31 16:30:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:55:57.464408+00:00"
    },
    {
      "arxiv_id": "2506.00622v2",
      "title": "Improving Dialogue State Tracking through Combinatorial Search for In-Context Examples",
      "title_zh": "é€šè¿‡è¯­å¢ƒç¤ºä¾‹ç»„åˆæœç´¢æå‡å¯¹è¯çŠ¶æ€è¿½è¸ªæ€§èƒ½",
      "authors": [
        "Haesung Pyun",
        "Yoonah Park",
        "Yohan Jo"
      ],
      "abstract": "In dialogue state tracking (DST), in-context learning comprises a retriever that selects labeled dialogues as in-context examples and a DST model that uses these examples to infer the dialogue state of the query dialogue. Existing methods for constructing training data for retrievers suffer from three key limitations: (1) the synergistic effect of examples is not considered, (2) the linguistic characteristics of the query are not sufficiently factored in, and (3) scoring is not directly optimized for DST performance. Consequently, the retriever can fail to retrieve examples that would substantially improve DST performance. To address these issues, we present CombiSearch, a method that scores effective in-context examples based on their combinatorial impact on DST performance. Our evaluation on MultiWOZ shows that retrievers trained with CombiSearch surpass state-of-the-art models, achieving a 20x gain in data efficiency and generalizing well to the SGD dataset. Moreover, CombiSearch attains a 12% absolute improvement in the upper bound DST performance over traditional approaches when no retrieval errors are assumed. This significantly increases the headroom for practical DST performance while demonstrating that existing methods rely on suboptimal data for retriever training.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯¹è¯çŠ¶æ€è·Ÿè¸ª(Dialogue State Tracking, DST)ä¸­è¯­å¢ƒå­¦ä¹ (In-Context Learning)çš„æ£€ç´¢å™¨å±€é™æ€§ï¼Œæå‡ºäº†CombiSearchæ–¹æ³•ã€‚ç°æœ‰çš„æ£€ç´¢å™¨è®­ç»ƒæ•°æ®æ„å»ºæ–¹æ³•å¿½è§†äº†ç¤ºä¾‹é—´çš„ååŒæ•ˆåº”ã€æŸ¥è¯¢çš„è¯­è¨€ç‰¹å¾ä»¥åŠDSTæ€§èƒ½çš„ç›´æ¥ä¼˜åŒ–ï¼Œå¯¼è‡´æ£€ç´¢å‡ºçš„ç¤ºä¾‹å¾€å¾€æ— æ³•æœ€å¤§åŒ–æå‡æ¨¡å‹è¡¨ç°ã€‚CombiSearché€šè¿‡è¯„ä¼°ç¤ºä¾‹å¯¹DSTæ€§èƒ½çš„ç»„åˆå½±å“(Combinatorial Impact)æ¥ç­›é€‰å¹¶è¯„åˆ†æœ‰æ•ˆçš„è¯­å¢ƒç¤ºä¾‹ã€‚åœ¨MultiWOZæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨CombiSearchè®­ç»ƒçš„æ£€ç´¢å™¨åœ¨æ•°æ®æ•ˆç‡ä¸Šæ¯”ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹æé«˜äº†20å€ï¼Œå¹¶èƒ½è‰¯å¥½æ¨å¹¿è‡³SGDæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ç†æƒ³æ£€ç´¢æ¡ä»¶ä¸‹å°†DSTæ€§èƒ½ä¸Šé™æå‡äº†12%ï¼Œä¸ä»…æ˜¾è‘—æ‹“å±•äº†å®é™…å¯¹è¯ç³»ç»Ÿçš„æ€§èƒ½ç©ºé—´ï¼Œä¹Ÿè¯æ˜äº†ç°æœ‰æ£€ç´¢å™¨è®­ç»ƒæ–¹æ³•åœ¨æ•°æ®é€‰æ‹©ä¸Šå­˜åœ¨æ¬¡ä¼˜æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper has been accepted for publication at ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00622v2",
      "published_date": "2025-05-31 16:20:14 UTC",
      "updated_date": "2025-06-03 08:31:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:56:17.705773+00:00"
    },
    {
      "arxiv_id": "2506.00618v3",
      "title": "RiOSWorld: Benchmarking the Risk of Multimodal Computer-Use Agents",
      "title_zh": "RiOSWorldï¼šå¤šæ¨¡æ€è®¡ç®—æœºæ“ä½œæ™ºèƒ½ä½“é£é™©åŸºå‡†æµ‹è¯•",
      "authors": [
        "Jingyi Yang",
        "Shuai Shao",
        "Dongrui Liu",
        "Jing Shao"
      ],
      "abstract": "With the rapid development of multimodal large language models (MLLMs), they are increasingly deployed as autonomous computer-use agents capable of accomplishing complex computer tasks. However, a pressing issue arises: Can the safety risk principles designed and aligned for general MLLMs in dialogue scenarios be effectively transferred to real-world computer-use scenarios? Existing research on evaluating the safety risks of MLLM-based computer-use agents suffers from several limitations: it either lacks realistic interactive environments, or narrowly focuses on one or a few specific risk types. These limitations ignore the complexity, variability, and diversity of real-world environments, thereby restricting comprehensive risk evaluation for computer-use agents. To this end, we introduce \\textbf{RiOSWorld}, a benchmark designed to evaluate the potential risks of MLLM-based agents during real-world computer manipulations. Our benchmark includes 492 risky tasks spanning various computer applications, involving web, social media, multimedia, os, email, and office software. We categorize these risks into two major classes based on their risk source: (i) User-originated risks and (ii) Environmental risks. For the evaluation, we evaluate safety risks from two perspectives: (i) Risk goal intention and (ii) Risk goal completion. Extensive experiments with multimodal agents on \\textbf{RiOSWorld} demonstrate that current computer-use agents confront significant safety risks in real-world scenarios. Our findings highlight the necessity and urgency of safety alignment for computer-use agents in real-world computer manipulation, providing valuable insights for developing trustworthy computer-use agents. Our benchmark is publicly available at https://yjyddq.github.io/RiOSWorld.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) ä½œä¸ºè®¡ç®—æœºä½¿ç”¨æ™ºèƒ½ä½“ (computer-use agents) åœ¨çœŸå®æ“ä½œç¯å¢ƒä¸­çš„å®‰å…¨è¯„ä¼°éš¾é¢˜ï¼Œæå‡ºäº† RiOSWorld åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æ¶µç›–äº†ç½‘é¡µã€ç¤¾äº¤åª’ä½“ã€æ“ä½œç³»ç»Ÿå’ŒåŠå…¬è½¯ä»¶ç­‰å¤šç§åº”ç”¨åœºæ™¯ä¸‹çš„ 492 ä¸ªé£é™©ä»»åŠ¡ï¼Œå¹¶å°†é£é™©æ¥æºåˆ’åˆ†ä¸ºç”¨æˆ·æºé£é™© (User-originated risks) å’Œç¯å¢ƒé£é™© (Environmental risks)ã€‚è¯„ä¼°è¿‡ç¨‹ä»é£é™©ç›®æ ‡æ„å›¾ (Risk goal intention) å’Œé£é™©ç›®æ ‡å®Œæˆåº¦ (Risk goal completion) ä¸¤ä¸ªç»´åº¦å±•å¼€ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶ç¼ºä¹çœŸå®äº¤äº’ç¯å¢ƒæˆ–é£é™©ç±»å‹å•ä¸€çš„ç©ºç™½ã€‚åœ¨ RiOSWorld ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œç°æœ‰çš„è®¡ç®—æœºä½¿ç”¨æ™ºèƒ½ä½“åœ¨çœŸå®åœºæ™¯ä¸­é¢ä¸´æ˜¾è‘—çš„å®‰å…¨é£é™©ï¼Œè¯æ˜äº†é€šç”¨çš„å¯¹è¯å®‰å…¨å‡†åˆ™éš¾ä»¥ç›´æ¥è¿ç§»è‡³å¤æ‚çš„è®¡ç®—æœºæ“ä½œåœºæ™¯ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å¯¹è®¡ç®—æœºä½¿ç”¨æ™ºèƒ½ä½“è¿›è¡Œå®‰å…¨å¯¹é½ (safety alignment) çš„ç´§è¿«æ€§ï¼Œä¸ºå¼€å‘å¯ä¿¡çš„è‡ªä¸»æ™ºèƒ½ä½“æä¾›äº†é‡è¦å‚è€ƒå’Œå…¬å¼€çš„è¯„ä¼°å·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "40 pages, 6 figures, Project Page: https://yjyddq.github.io/RiOSWorld.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2506.00618v3",
      "published_date": "2025-05-31 16:04:59 UTC",
      "updated_date": "2025-06-20 01:24:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:56:02.539382+00:00"
    },
    {
      "arxiv_id": "2506.00615v2",
      "title": "An Incremental Framework for Topological Dialogue Semantics: Efficient Reasoning in Discrete Spaces",
      "title_zh": "æ‹“æ‰‘å¯¹è¯è¯­ä¹‰çš„å¢é‡å¼æ¡†æ¶ï¼šç¦»æ•£ç©ºé—´ä¸­çš„é«˜æ•ˆæ¨ç†",
      "authors": [
        "Andreu Ballus Santacana"
      ],
      "abstract": "We present a tractable, incremental framework for topological dialogue semantics based on finite, discrete semantic spaces. Building on the intuition that utterances correspond to open sets and their combinatorial relations form a simplicial complex (the dialogue nerve), we give a rigorous foundation, a provably correct incremental algorithm for nerve updates, and a reference implementation in the Wolfram Language. The framework supports negative nerve computation (inconsistency tracking), consequence extraction, and a transparent, set-theoretic ranking of entailments. We clarify which combinatorial properties hold in the discrete case, provide motivating examples, and outline limitations and prospects for richer logical and categorical extensions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”¨äºæ‹“æ‰‘å¯¹è¯è¯­ä¹‰(topological dialogue semantics)çš„å¢é‡å¼æ¡†æ¶ï¼Œæ—¨åœ¨æœ‰é™ç¦»æ•£è¯­ä¹‰ç©ºé—´ä¸­å®ç°é«˜æ•ˆæ¨ç†ã€‚è¯¥æ¡†æ¶åŸºäºè¯è¯­å¯¹åº”å¼€é›†(open sets)ä¸”å…¶ç»„åˆå…³ç³»æ„æˆå•çº¯å¤å½¢(simplicial complexï¼Œå³å¯¹è¯ç¥ç»/dialogue nerve)çš„ç›´è§‰æ„å»ºã€‚ç ”ç©¶ä¸ºè¯¥ç†è®ºæä¾›äº†ä¸¥å¯†çš„æ•°å­¦åŸºç¡€ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªå¯è¯æ˜æ­£ç¡®çš„å¢é‡å¼ç®—æ³•ç”¨äºå¯¹è¯ç¥ç»çš„åŠ¨æ€æ›´æ–°ã€‚ä½œè€…åœ¨Wolfram Languageä¸­æä¾›äº†è¯¥æ¡†æ¶çš„å‚è€ƒå®ç°ï¼ŒéªŒè¯äº†å…¶åœ¨è®¡ç®—ä¸Šçš„å¯å¤„ç†æ€§ã€‚è¯¥æ¡†æ¶æ”¯æŒè´Ÿç¥ç»è®¡ç®—(negative nerve computation)ä»¥è¿›è¡Œä¸ä¸€è‡´æ€§è¿½è¸ª(inconsistency tracking)ã€åæœæå–(consequence extraction)ä»¥åŠåŸºäºé›†åˆè®ºçš„è•´å«æ’åº(ranking of entailments)ã€‚é€šè¿‡å…·ä½“ç¤ºä¾‹ï¼Œç ”ç©¶é˜æ˜äº†ç¦»æ•£æƒ…å†µä¸‹çš„ç»„åˆæ€§è´¨ï¼Œå¹¶ä¸ºæœªæ¥åœ¨é€»è¾‘å’ŒèŒƒç•´å±‚é¢çš„æ‰©å±•å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI",
        "math.AT",
        "math.LO"
      ],
      "primary_category": "cs.LO",
      "comment": "15 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.00615v2",
      "published_date": "2025-05-31 15:58:05 UTC",
      "updated_date": "2025-06-14 00:27:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:56:42.422160+00:00"
    },
    {
      "arxiv_id": "2506.00614v1",
      "title": "Predictability-Aware Compression and Decompression Framework for Multichannel Time Series Data",
      "title_zh": "é¢å‘å¤šé€šé“æ—¶é—´åºåˆ—æ•°æ®çš„å¯é¢„æµ‹æ€§æ„ŸçŸ¥å‹ç¼©ä¸è§£å‹ç¼©æ¡†æ¶",
      "authors": [
        "Ziqi Liu",
        "Pei Zeng",
        "Yi Ding"
      ],
      "abstract": "Real-world multichannel time series prediction faces growing demands for efficiency across edge and cloud environments, making channel compression a timely and essential problem. Motivated by success of Multiple-Input Multiple-Output (MIMO) methods, we propose a predictability-aware compression-decompression framework to reduce runtime, lower communication cost, and maintain prediction accuracy across diverse predictors. The core idea involves using a circular periodicity key matrix with orthogonality to capture underlying time series predictability during compression and to mitigate reconstruction errors during decompression by relaxing oversimplified data assumptions. Theoretical and empirical analyses show that the proposed framework is both time-efficient and scalable under a large number of channels. Extensive experiments on six datasets across various predictors demonstrate that the proposed method achieves superior overall performance by jointly considering prediction accuracy and runtime, while maintaining strong compatibility with diverse predictors.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤šé€šé“æ—¶é—´åºåˆ—æ•°æ®çš„å¯é¢„æµ‹æ€§æ„ŸçŸ¥å‹ç¼©ä¸è§£å‹ç¼©æ¡†æ¶ï¼Œæ—¨åœ¨æå‡è¾¹ç¼˜ä¸äº‘ç¯å¢ƒä¸‹çš„é¢„æµ‹æ•ˆç‡ã€‚å—å¤šè¾“å…¥å¤šè¾“å‡º(MIMO)æ–¹æ³•çš„å¯å‘ï¼Œè¯¥æ¡†æ¶é€šè¿‡é™ä½è¿è¡Œæ—¶é—´å’Œé€šä¿¡æˆæœ¬ï¼ŒåŒæ—¶åœ¨ä¸åŒé¢„æµ‹å™¨ä¸Šç»´æŒé«˜é¢„æµ‹ç²¾åº¦ã€‚æ ¸å¿ƒæœºåˆ¶åˆ©ç”¨å…·æœ‰æ­£äº¤æ€§çš„å¾ªç¯å‘¨æœŸå¯†é’¥çŸ©é˜µ(circular periodicity key matrix)åœ¨å‹ç¼©é˜¶æ®µæ•æ‰æ—¶é—´åºåˆ—çš„å¯é¢„æµ‹æ€§ï¼Œå¹¶é€šè¿‡åœ¨è§£å‹ç¼©é˜¶æ®µæ”¾å®½è¿‡åº¦ç®€åŒ–çš„æ•°æ®å‡è®¾æ¥å‡å°‘é‡æ„è¯¯å·®ã€‚ç†è®ºåˆ†æä¸å®éªŒè¯„ä¼°å‡è¯æ˜è¯¥æ¡†æ¶åœ¨å¤„ç†å¤§è§„æ¨¡é€šé“æ•°æ®æ—¶å…·æœ‰å‡ºè‰²çš„æ—¶é—´æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…¼é¡¾é¢„æµ‹å‡†ç¡®æ€§ä¸è¿è¡Œæ•ˆç‡æ–¹é¢è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸”å¯¹å„ç±»é¢„æµ‹å™¨å±•ç°å‡ºæå¼ºçš„å…¼å®¹æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages,3 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.00614v1",
      "published_date": "2025-05-31 15:53:41 UTC",
      "updated_date": "2025-05-31 15:53:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:56:33.582195+00:00"
    },
    {
      "arxiv_id": "2506.00613v3",
      "title": "WorldGym: World Model as An Environment for Policy Evaluation",
      "title_zh": "WorldGymï¼šä½œä¸ºç­–ç•¥è¯„ä¼°ç¯å¢ƒçš„ä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Julian Quevedo",
        "Ansh Kumar Sharma",
        "Yixiang Sun",
        "Varad Suryavanshi",
        "Percy Liang",
        "Sherry Yang"
      ],
      "abstract": "Evaluating robot control policies is difficult: real-world testing is costly, and handcrafted simulators require manual effort to improve in realism and generality. We propose a world-model-based policy evaluation environment (WorldGym), an autoregressive, action-conditioned video generation model which serves as a proxy to real world environments. Policies are evaluated via Monte Carlo rollouts in the world model, with a vision-language model providing rewards. We evaluate a set of VLA-based real-robot policies in the world model using only initial frames from real robots, and show that policy success rates within the world model highly correlate with real-world success rates. Moreoever, we show that WorldGym is able to preserve relative policy rankings across different policy versions, sizes, and training checkpoints. Due to requiring only a single start frame as input, the world model further enables efficient evaluation of robot policies' generalization ability on novel tasks and environments. We find that modern VLA-based robot policies still struggle to distinguish object shapes and can become distracted by adversarial facades of objects. While generating highly realistic object interaction remains challenging, WorldGym faithfully emulates robot motions and offers a practical starting point for safe and reproducible policy evaluation before deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†WorldGymï¼Œä¸€ä¸ªåŸºäºä¸–ç•Œæ¨¡å‹(World Model)çš„æœºå™¨äººç­–ç•¥è¯„ä¼°ç¯å¢ƒï¼Œæ—¨åœ¨è§£å†³ç°å®ä¸–ç•Œæµ‹è¯•æˆæœ¬é«˜æ˜‚ä»¥åŠæ‰‹å·¥æ¨¡æ‹Ÿå™¨åœ¨ç°å®æ„Ÿå’Œé€šç”¨æ€§æ–¹é¢çš„å±€é™ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è‡ªå›å½’(autoregressive)ã€åŠ¨ä½œæ¡ä»¶åŒ–çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ä½œä¸ºçœŸå®ç¯å¢ƒçš„ä»£ç†ï¼Œé€šè¿‡åœ¨ä¸–ç•Œæ¨¡å‹ä¸­æ‰§è¡Œè’™ç‰¹å¡æ´›é‡‡æ ·(Monte Carlo rollouts)å¹¶ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹(VLM)æä¾›å¥–åŠ±æ¥å®Œæˆç­–ç•¥è¯„ä¼°ã€‚å®éªŒè¯æ˜ï¼ŒWorldGym å¯¹VLAç­–ç•¥çš„è¯„ä¼°ç»“æœä¸ç°å®ä¸–ç•ŒæˆåŠŸç‡é«˜åº¦ç›¸å…³ï¼Œå¹¶èƒ½å‡†ç¡®ç»´æŒä¸åŒç­–ç•¥ç‰ˆæœ¬ã€è§„æ¨¡å’Œæ£€æŸ¥ç‚¹ä¹‹é—´çš„ç›¸å¯¹æ’åã€‚ç”±äºä»…éœ€å•å¼ åˆå§‹å¸§ä½œä¸ºè¾“å…¥ï¼Œè¯¥æ¨¡å‹è¿˜å®ç°äº†å¯¹æœºå™¨äººç­–ç•¥åœ¨æ–°ä»»åŠ¡å’Œæ–°ç¯å¢ƒä¸‹æ³›åŒ–èƒ½åŠ›çš„å¿«é€Ÿè¯„ä¼°ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ç°ä»£VLAç­–ç•¥åœ¨åŒºåˆ†ç‰©ä½“å½¢çŠ¶å’Œå¤„ç†å¯¹æŠ—æ€§å¹²æ‰°æ–¹é¢ä»å­˜åœ¨è–„å¼±ç¯èŠ‚ã€‚å°½ç®¡ç”Ÿæˆé«˜åº¦çœŸå®çš„ç‰©ä½“äº¤äº’ä»å…·æŒ‘æˆ˜ï¼Œä½†WorldGym èƒ½å¤Ÿå¿ å®æ¨¡æ‹Ÿæœºå™¨äººè¿åŠ¨ï¼Œä¸ºç­–ç•¥éƒ¨ç½²å‰æä¾›äº†å®‰å…¨ä¸”å¯å¤ç°çš„è¯„ä¼°æ‰‹æ®µã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "https://world-model-eval.github.io",
      "pdf_url": "https://arxiv.org/pdf/2506.00613v3",
      "published_date": "2025-05-31 15:51:56 UTC",
      "updated_date": "2025-09-30 03:34:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:56:40.966150+00:00"
    },
    {
      "arxiv_id": "2506.00607v1",
      "title": "Parallel Rescaling: Rebalancing Consistency Guidance for Personalized Diffusion Models",
      "title_zh": "Parallel Rescalingï¼šé¢å‘ä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹çš„ä¸€è‡´æ€§å¼•å¯¼é‡å¹³è¡¡",
      "authors": [
        "JungWoo Chae",
        "Jiyoon Kim",
        "Sangheum Hwang"
      ],
      "abstract": "Personalizing diffusion models to specific users or concepts remains challenging, particularly when only a few reference images are available. Existing methods such as DreamBooth and Textual Inversion often overfit to limited data, causing misalignment between generated images and text prompts when attempting to balance identity fidelity with prompt adherence. While Direct Consistency Optimization (DCO) with its consistency-guided sampling partially alleviates this issue, it still struggles with complex or stylized prompts. In this paper, we propose a parallel rescaling technique for personalized diffusion models. Our approach explicitly decomposes the consistency guidance signal into parallel and orthogonal components relative to classifier free guidance (CFG). By rescaling the parallel component, we minimize disruptive interference with CFG while preserving the subject's identity. Unlike prior personalization methods, our technique does not require additional training data or expensive annotations. Extensive experiments show improved prompt alignment and visual fidelity compared to baseline methods, even on challenging stylized prompts. These findings highlight the potential of parallel rescaled guidance to yield more stable and accurate personalization for diverse user inputs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹ (Personalized Diffusion Models) åœ¨å¹³è¡¡èº«ä»½ä¿çœŸåº¦ä¸æ–‡æœ¬æç¤ºè¯ä¸€è‡´æ€§æ—¶é¢ä¸´çš„è¿‡æ‹ŸåˆåŠå¤±é…é—®é¢˜ï¼Œæå‡ºäº† Parallel Rescaling æŠ€æœ¯ã€‚ç°æœ‰çš„ DreamBooth æˆ– Direct Consistency Optimization (DCO) ç­‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æˆ–é£æ ¼åŒ–æç¤ºè¯æ—¶ï¼Œå¾€å¾€éš¾ä»¥åŒæ—¶ç»´æŒä¸»ä½“ç‰¹å¾ä¸æŒ‡ä»¤éµå¾ªã€‚Parallel Rescaling çš„æ ¸å¿ƒåœ¨äºå°†ä¸€è‡´æ€§å¼•å¯¼ä¿¡å·æ˜¾å¼åˆ†è§£ä¸ºç›¸å¯¹äº Classifier-Free Guidance (CFG) çš„å¹³è¡Œå’Œæ­£äº¤åˆ†é‡ã€‚é€šè¿‡å¯¹å¹³è¡Œåˆ†é‡è¿›è¡Œé‡ç¼©æ”¾ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ€å°åŒ–å¯¹ CFG çš„å¹²æ‰°ï¼Œä»è€Œåœ¨ä¿ç•™ä¸»ä½“èº«ä»½çš„åŒæ—¶å¢å¼ºå¯¹æç¤ºè¯çš„å“åº”ã€‚ä¸ä»¥å¾€ä¸ªæ€§åŒ–æ–¹æ³•ä¸åŒï¼Œè¯¥æŠ€æœ¯æ— éœ€é¢å¤–çš„è®­ç»ƒæ•°æ®æˆ–æ˜‚è´µçš„æ ‡æ³¨ï¼Œå…·æœ‰æ˜¾è‘—çš„å®ç”¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„é£æ ¼åŒ–æç¤ºè¯æ—¶ï¼Œè¯¥æ–¹æ³•åœ¨æç¤ºè¯å¯¹é½åº¦å’Œè§†è§‰è´¨é‡æ–¹é¢å‡ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å¹³è¡Œé‡ç¼©æ”¾å¼•å¯¼åœ¨å®ç°å¤šæ ·åŒ–ç”¨æˆ·è¾“å…¥ä¸‹çš„ç¨³å®šã€å‡†ç¡®ä¸ªæ€§åŒ–ç”Ÿæˆæ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00607v1",
      "published_date": "2025-05-31 15:36:36 UTC",
      "updated_date": "2025-05-31 15:36:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:56:46.147906+00:00"
    },
    {
      "arxiv_id": "2506.00594v1",
      "title": "Graph Evidential Learning for Anomaly Detection",
      "title_zh": "é¢å‘å¼‚å¸¸æ£€æµ‹çš„å›¾è¯æ®å­¦ä¹ ",
      "authors": [
        "Chunyu Wei",
        "Wenji Hu",
        "Xingjia Hao",
        "Yunhai Wang",
        "Yueguo Chen",
        "Bing Bai",
        "Fei Wang"
      ],
      "abstract": "Graph anomaly detection faces significant challenges due to the scarcity of reliable anomaly-labeled datasets, driving the development of unsupervised methods. Graph autoencoders (GAEs) have emerged as a dominant approach by reconstructing graph structures and node features while deriving anomaly scores from reconstruction errors. However, relying solely on reconstruction error for anomaly detection has limitations, as it increases the sensitivity to noise and overfitting. To address these issues, we propose Graph Evidential Learning (GEL), a probabilistic framework that redefines the reconstruction process through evidential learning. By modeling node features and graph topology using evidential distributions, GEL quantifies two types of uncertainty: graph uncertainty and reconstruction uncertainty, incorporating them into the anomaly scoring mechanism. Extensive experiments demonstrate that GEL achieves state-of-the-art performance while maintaining high robustness against noise and structural perturbations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾å¼‚å¸¸æ£€æµ‹ï¼ˆGraph Anomaly Detectionï¼‰ä¸­æ— ç›‘ç£æ–¹æ³•è¿‡åº¦ä¾èµ–é‡æ„è¯¯å·®å¯¼è‡´å¯¹å™ªå£°æ•æ„Ÿå’Œå®¹æ˜“è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œæå‡ºäº†Graph Evidential Learningï¼ˆGELï¼‰æ¡†æ¶ã€‚ä½œä¸ºä¸€ç§æ¦‚ç‡æ¡†æ¶ï¼ŒGELé€šè¿‡è¯æ®å­¦ä¹ ï¼ˆEvidential Learningï¼‰é‡æ–°å®šä¹‰äº†é‡æ„è¿‡ç¨‹ï¼Œå¹¶åˆ©ç”¨è¯æ®åˆ†å¸ƒå¯¹èŠ‚ç‚¹ç‰¹å¾å’Œå›¾æ‹“æ‰‘è¿›è¡Œå»ºæ¨¡ã€‚è¯¥æ–¹æ³•èƒ½å¤ŸåŒæ—¶é‡åŒ–å›¾ä¸ç¡®å®šæ€§ï¼ˆGraph Uncertaintyï¼‰å’Œé‡æ„ä¸ç¡®å®šæ€§ï¼ˆReconstruction Uncertaintyï¼‰ï¼Œå¹¶å°†å…¶æœ‰æ•ˆåœ°æ•´åˆåˆ°å¼‚å¸¸è¯„åˆ†æœºåˆ¶ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGELåœ¨ä¿æŒå¯¹å™ªå£°å’Œç»“æ„æ‰°åŠ¨é«˜åº¦é²æ£’æ€§çš„åŒæ—¶ï¼Œåœ¨å¤šé¡¹ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›ï¼ˆState-of-the-artï¼‰çš„æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by KDD25",
      "pdf_url": "https://arxiv.org/pdf/2506.00594v1",
      "published_date": "2025-05-31 15:06:42 UTC",
      "updated_date": "2025-05-31 15:06:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:56:42.548264+00:00"
    },
    {
      "arxiv_id": "2506.00592v1",
      "title": "Mitigating Plasticity Loss in Continual Reinforcement Learning by Reducing Churn",
      "title_zh": "é€šè¿‡å‡å°‘ Churn ç¼“è§£æŒç»­å¼ºåŒ–å­¦ä¹ ä¸­çš„å¯å¡‘æ€§æŸå¤±",
      "authors": [
        "Hongyao Tang",
        "Johan Obando-Ceron",
        "Pablo Samuel Castro",
        "Aaron Courville",
        "Glen Berseth"
      ],
      "abstract": "Plasticity, or the ability of an agent to adapt to new tasks, environments, or distributions, is crucial for continual learning. In this paper, we study the loss of plasticity in deep continual RL from the lens of churn: network output variability for out-of-batch data induced by mini-batch training. We demonstrate that (1) the loss of plasticity is accompanied by the exacerbation of churn due to the gradual rank decrease of the Neural Tangent Kernel (NTK) matrix; (2) reducing churn helps prevent rank collapse and adjusts the step size of regular RL gradients adaptively. Moreover, we introduce Continual Churn Approximated Reduction (C-CHAIN) and demonstrate it improves learning performance and outperforms baselines in a diverse range of continual learning environments on OpenAI Gym Control, ProcGen, DeepMind Control Suite, and MinAtar benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æŒç»­å¼ºåŒ–å­¦ä¹ ï¼ˆContinual Reinforcement Learningï¼‰ä¸­å¯å¡‘æ€§ï¼ˆPlasticityï¼‰ä¸§å¤±çš„é—®é¢˜ï¼Œå¹¶ä» Churnï¼ˆç”±å°æ‰¹é‡è®­ç»ƒå¼•èµ·çš„éæ‰¹æ¬¡æ•°æ®ç½‘ç»œè¾“å‡ºå˜æ€§ï¼‰çš„è§’åº¦è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚ç ”ç©¶å‘ç°ï¼ŒPlasticity çš„ä¸§å¤±å¾€å¾€ä¼´éšç€ç”±äºç¥ç»åˆ‡çº¿æ ¸ï¼ˆNeural Tangent Kernel, NTKï¼‰çŸ©é˜µç§©çš„é€æ¸é™ä½è€Œå¯¼è‡´çš„ Churn åŠ å‰§ã€‚å®éªŒè¯æ˜ï¼Œå‡å°‘ Churn ä¸ä»…æœ‰åŠ©äºé˜²æ­¢ç§©åç¼©ï¼ˆRank Collapseï¼‰ï¼Œè¿˜èƒ½è‡ªé€‚åº”åœ°è°ƒæ•´å¸¸è§„å¼ºåŒ–å­¦ä¹ æ¢¯åº¦çš„æ­¥é•¿ã€‚åŸºäºæ­¤ï¼Œä½œè€…æå‡ºäº† Continual Churn Approximated Reduction (C-CHAIN) æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æŠ‘åˆ¶ Churn æ¥ç»´æŒç½‘ç»œçš„å¯å¡‘æ€§ã€‚åœ¨ OpenAI Gym Controlã€ProcGenã€DeepMind Control Suite å’Œ MinAtar ç­‰å¤šç§åŸºå‡†ç¯å¢ƒä¸‹çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒC-CHAIN æ˜¾è‘—æå‡äº†å­¦ä¹ æ€§èƒ½å¹¶è¶…è¶Šäº†ç°æœ‰çš„åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00592v1",
      "published_date": "2025-05-31 14:58:22 UTC",
      "updated_date": "2025-05-31 14:58:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:56:47.575103+00:00"
    },
    {
      "arxiv_id": "2506.00588v2",
      "title": "Temporal Chunking Enhances Recognition of Implicit Sequential Patterns",
      "title_zh": "æ—¶åºç»„å—å¼ºåŒ–éšå¼åºåˆ—æ¨¡å¼çš„è¯†åˆ«",
      "authors": [
        "Jayanta Dey",
        "Nicholas Soures",
        "Miranda Gonzales",
        "Itamar Lerner",
        "Christopher Kanan",
        "Dhireesha Kudithipudi"
      ],
      "abstract": "In this pilot study, we propose a neuro-inspired approach that compresses temporal sequences into context-tagged chunks, where each tag represents a recurring structural unit or``community'' in the sequence. These tags are generated during an offline sleep phase and serve as compact references to past experience, allowing the learner to incorporate information beyond its immediate input range. We evaluate this idea in a controlled synthetic environment designed to reveal the limitations of traditional neural network based sequence learners, such as recurrent neural networks (RNNs), when facing temporal patterns on multiple timescales. We evaluate this idea in a controlled synthetic environment designed to reveal the limitations of traditional neural network based sequence learners, such as recurrent neural networks (RNNs), when facing temporal patterns on multiple timescales. Our results, while preliminary, suggest that temporal chunking can significantly enhance learning efficiency under resource constrained settings. A small-scale human pilot study using a Serial Reaction Time Task further motivates the idea of structural abstraction. Although limited to synthetic tasks, this work serves as an early proof-of-concept, with initial evidence that learned context tags can transfer across related task, offering potential for future applications in transfer learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å—ç¥ç»ç§‘å­¦å¯å‘çš„æ–¹æ³•ï¼Œå³ temporal chunkingï¼Œé€šè¿‡å°†æ—¶é—´åºåˆ—å‹ç¼©ä¸ºå¸¦æœ‰ context-tagged çš„å—æ¥å¢å¼ºå¯¹éšå¼åºåˆ—æ¨¡å¼çš„è¯†åˆ«ã€‚æ¯ä¸ªæ ‡è®°ä»£è¡¨åºåˆ—ä¸­é‡å¤å‡ºç°çš„ç»“æ„å•å…ƒæˆ– communityï¼Œå¹¶åœ¨ç¦»çº¿ sleep phase ä¸­ç”Ÿæˆï¼Œä»è€Œå…è®¸å­¦ä¹ è€…æ•´åˆè¶…å‡ºå…¶å³æ—¶è¾“å…¥èŒƒå›´çš„ä¿¡æ¯ã€‚åœ¨åˆæˆç¯å¢ƒä¸‹çš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¼¥è¡¥ä¼ ç»Ÿ Recurrent Neural Networks (RNNs) åœ¨å¤„ç†å¤šæ—¶é—´å°ºåº¦æ¨¡å¼æ—¶çš„å±€é™æ€§ï¼Œæ˜¾è‘—æå‡èµ„æºå—é™è®¾ç½®ä¸‹çš„å­¦ä¹ æ•ˆç‡ã€‚æ­¤å¤–ï¼Œä¸€é¡¹æ¶‰åŠ Serial Reaction Time Task çš„äººç±»åˆæ­¥ç ”ç©¶è¿›ä¸€æ­¥æ”¯æŒäº†ç»“æ„æŠ½è±¡çš„æ¦‚å¿µã€‚ä½œä¸ºåˆæ­¥çš„ proof-of-conceptï¼Œç ”ç©¶ç»“æœæ˜¾ç¤ºå­¦ä¹ åˆ°çš„ context tags èƒ½å¤Ÿåœ¨ç›¸å…³ä»»åŠ¡é—´è¿›è¡Œè¿ç§»ï¼Œå±•ç¤ºäº†å…¶åœ¨ transfer learning é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00588v2",
      "published_date": "2025-05-31 14:51:08 UTC",
      "updated_date": "2025-07-15 15:22:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:56:57.929277+00:00"
    },
    {
      "arxiv_id": "2506.00582v2",
      "title": "Do Language Models Mirror Human Confidence? Exploring Psychological Insights to Address Overconfidence in LLMs",
      "title_zh": "è¯­è¨€æ¨¡å‹æ˜¯å¦é•œåƒäº†äººç±»çš„ç½®ä¿¡åº¦è¡¨ç°ï¼Ÿå€Ÿé‰´å¿ƒç†å­¦è§è§£è§£å†³å¤§è¯­è¨€æ¨¡å‹çš„è¿‡åº¦ç½®ä¿¡é—®é¢˜",
      "authors": [
        "Chenjun Xu",
        "Bingbing Wen",
        "Bin Han",
        "Robert Wolfe",
        "Lucy Lu Wang",
        "Bill Howe"
      ],
      "abstract": "Psychology research has shown that humans are poor at estimating their performance on tasks, tending towards underconfidence on easy tasks and overconfidence on difficult tasks. We examine three LLMs, Llama-3-70B-instruct, Claude-3-Sonnet, and GPT-4o, on a range of QA tasks of varying difficulty, and show that models exhibit subtle differences from human patterns of overconfidence: less sensitive to task difficulty, and when prompted to answer based on different personas -- e.g., expert vs layman, or different race, gender, and ages -- the models will respond with stereotypically biased confidence estimations even though their underlying answer accuracy remains the same. Based on these observations, we propose Answer-Free Confidence Estimation (AFCE) to improve confidence calibration and LLM interpretability in these settings. AFCE is a self-assessment method that employs two stages of prompting, first eliciting only confidence scores on questions, then asking separately for the answer. Experiments on the MMLU and GPQA datasets spanning subjects and difficulty show that this separation of tasks significantly reduces overconfidence and delivers more human-like sensitivity to task difficulty.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦é•œåƒäº†äººç±»çš„ç½®ä¿¡åº¦æ¨¡å¼ï¼Œé‡ç‚¹åˆ†æäº† Llama-3-70B-instructã€Claude-3-Sonnet å’Œ GPT-4o åœ¨ä¸åŒéš¾åº¦ QA ä»»åŠ¡ä¸‹çš„è¡¨ç°ã€‚å®éªŒå‘ç°ï¼Œä¸äººç±»åœ¨ç®€å•ä»»åŠ¡ä¸Šä¸è‡ªä¿¡ã€å›°éš¾ä»»åŠ¡ä¸Šè¿‡åº¦è‡ªä¿¡ï¼ˆOverconfidenceï¼‰çš„æ¨¡å¼ä¸åŒï¼ŒLLMs å¯¹ä»»åŠ¡éš¾åº¦çš„æ•æ„Ÿåº¦è¾ƒä½ï¼Œä¸”åœ¨è¢«èµ‹äºˆä¸åŒ Personaï¼ˆå¦‚ç‰¹å®šèº«ä»½ã€æ€§åˆ«æˆ–å¹´é¾„ï¼‰æ—¶ï¼Œå³ä½¿å‡†ç¡®ç‡ä¸å˜ï¼Œä¹Ÿä¼šäº§ç”Ÿç¬¦åˆåˆ»æ¿å°è±¡çš„ç½®ä¿¡åº¦è¯„ä¼°åå·®ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Answer-Free Confidence Estimation (AFCE) çš„ä¸¤é˜¶æ®µæç¤ºæ–¹æ³•ï¼Œé€šè¿‡å°†ç½®ä¿¡åº¦è¯„åˆ†ä¸ç”Ÿæˆç­”æ¡ˆçš„è¿‡ç¨‹è¿›è¡Œåˆ†ç¦»æ¥æå‡æ¨¡å‹çš„ Confidence Calibrationã€‚åœ¨ MMLU å’Œ GPQA æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒAFCE æ˜¾è‘—é™ä½äº† LLMs çš„è¿‡åº¦è‡ªä¿¡ï¼Œå¹¶ä½¿å…¶åœ¨ç½®ä¿¡åº¦è¯„ä¼°ä¸Šè¡¨ç°å‡ºæ›´æ¥è¿‘äººç±»çš„éš¾åº¦æ•æ„Ÿæ€§ã€‚è¯¥æ–¹æ³•ä¸ºæ”¹å–„ LLMs çš„è‡ªæˆ‘è¯„ä¼°èƒ½åŠ›å’Œå¢å¼ºæ¨¡å‹çš„å¯è§£é‡Šæ€§æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ACL 2025 Findings, 20 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.00582v2",
      "published_date": "2025-05-31 14:37:18 UTC",
      "updated_date": "2025-07-28 12:59:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:57:09.484931+00:00"
    },
    {
      "arxiv_id": "2506.00577v1",
      "title": "Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs",
      "title_zh": "åƒç»æµå­¦å®¶ä¸€æ ·æ¨ç†ï¼šç»æµé—®é¢˜åè®­ç»ƒè¯±å¯¼å¤§è¯­è¨€æ¨¡å‹çš„ç­–ç•¥æ€§æ³›åŒ–",
      "authors": [
        "Yufa Zhou",
        "Shaobo Wang",
        "Xingyu Dong",
        "Xiangqi Jin",
        "Yifang Chen",
        "Yue Min",
        "Kexin Yang",
        "Xingzhang Ren",
        "Dayiheng Liu",
        "Linfeng Zhang"
      ],
      "abstract": "Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS) remains challenging due to intricate reward modeling, dynamic agent interactions, and demanding generalization requirements. This paper explores whether post-training techniques, specifically Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR), can effectively $\\textit{generalize}$ to multi-agent scenarios. We use economic reasoning as a testbed, leveraging its strong foundations in mathematics and game theory, its demand for structured analytical reasoning, and its relevance to real-world applications such as market design, resource allocation, and policy analysis. We introduce $\\textbf{Recon}$ ($\\textbf{R}$easoning like an $\\textbf{ECON}$omist), a 7B-parameter open-source LLM post-trained on a hand-curated dataset of 2,100 high-quality economic reasoning problems. Comprehensive evaluation on economic reasoning benchmarks and multi-agent games reveals clear improvements in structured reasoning and economic rationality. These results underscore the promise of domain-aligned post-training for enhancing reasoning and agent alignment, shedding light on the roles of SFT and RL in shaping model behavior. Code is available at https://github.com/MasterZhou1/Recon .",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡åæœŸè®­ç»ƒï¼ˆPost-Trainingï¼‰æŠ€æœ¯ä½¿å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ä¸­å…·å¤‡æ›´å¼ºçš„ç­–ç•¥æ³›åŒ–èƒ½åŠ›ï¼Œä»¥åº”å¯¹å¤æ‚çš„å¥–åŠ±å»ºæ¨¡å’ŒåŠ¨æ€äº¤äº’æŒ‘æˆ˜ã€‚ç ”ç©¶è€…é€‰æ‹©ç»æµæ¨ç†ï¼ˆEconomic Reasoningï¼‰ä½œä¸ºå®éªŒå¹³å°ï¼Œåˆ©ç”¨å…¶åœ¨æ•°å­¦ã€åšå¼ˆè®ºï¼ˆGame Theoryï¼‰å’Œç»“æ„åŒ–åˆ†ææ¨ç†æ–¹é¢çš„æ·±åšåŸºç¡€ï¼Œæ¨å‡ºäº†åŒ…å« 7B å‚æ•°çš„å¼€æºæ¨¡å‹ Reconï¼ˆReasoning like an ECONomistï¼‰ã€‚è¯¥æ¨¡å‹åœ¨åŒ…å« 2100 ä¸ªé«˜è´¨é‡ç»æµæ¨ç†é—®é¢˜çš„ç²¾é€‰æ•°æ®é›†ä¸Šï¼Œç»“åˆäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¸¦æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰è¿›è¡Œè®­ç»ƒã€‚åœ¨ç»æµæ¨ç†åŸºå‡†æµ‹è¯•å’Œå¤šæ™ºèƒ½ä½“åšå¼ˆçš„ç»¼åˆè¯„ä¼°ä¸­ï¼ŒRecon å±•ç°å‡ºåœ¨ç»“æ„åŒ–æ¨ç†å’Œç»æµç†æ€§æ–¹é¢çš„æ˜¾è‘—æå‡ã€‚ç ”ç©¶ç»“æœè¯æ˜äº†é¢†åŸŸå¯¹é½ï¼ˆDomain-Alignedï¼‰çš„åæœŸè®­ç»ƒåœ¨å¢å¼ºæ¨¡å‹æ¨ç†å’Œæ™ºèƒ½ä½“å¯¹é½ï¼ˆAgent Alignmentï¼‰æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œå¹¶è¿›ä¸€æ­¥æ­ç¤ºäº† SFT å’Œ RL åœ¨å¡‘é€ æ¨¡å‹è¡Œä¸ºä¸­çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.GT",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00577v1",
      "published_date": "2025-05-31 14:22:40 UTC",
      "updated_date": "2025-05-31 14:22:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:57:02.337886+00:00"
    },
    {
      "arxiv_id": "2506.00576v1",
      "title": "ORAN-GUIDE: RAG-Driven Prompt Learning for LLM-Augmented Reinforcement Learning in O-RAN Network Slicing",
      "title_zh": "ORAN-GUIDEï¼šé¢å‘ O-RAN ç½‘ç»œåˆ‡ç‰‡ä¸­ LLM å¢å¼ºå¼ºåŒ–å­¦ä¹ çš„ RAG é©±åŠ¨æç¤ºå­¦ä¹ ",
      "authors": [
        "Fatemeh Lotfi",
        "Hossein Rajoli",
        "Fatemeh Afghah"
      ],
      "abstract": "Advanced wireless networks must support highly dynamic and heterogeneous service demands. Open Radio Access Network (O-RAN) architecture enables this flexibility by adopting modular, disaggregated components, such as the RAN Intelligent Controller (RIC), Centralized Unit (CU), and Distributed Unit (DU), that can support intelligent control via machine learning (ML). While deep reinforcement learning (DRL) is a powerful tool for managing dynamic resource allocation and slicing, it often struggles to process raw, unstructured input like RF features, QoS metrics, and traffic trends. These limitations hinder policy generalization and decision efficiency in partially observable and evolving environments. To address this, we propose \\textit{ORAN-GUIDE}, a dual-LLM framework that enhances multi-agent RL (MARL) with task-relevant, semantically enriched state representations. The architecture employs a domain-specific language model, ORANSight, pretrained on O-RAN control and configuration data, to generate structured, context-aware prompts. These prompts are fused with learnable tokens and passed to a frozen GPT-based encoder that outputs high-level semantic representations for DRL agents. This design adopts a retrieval-augmented generation (RAG) style pipeline tailored for technical decision-making in wireless systems. Experimental results show that ORAN-GUIDE improves sample efficiency, policy convergence, and performance generalization over standard MARL and single-LLM baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ORAN-GUIDEï¼Œä¸€ç§æ—¨åœ¨ä¼˜åŒ–å¼€æ”¾æ— çº¿æ¥å…¥ç½‘ (O-RAN) ç½‘ç»œåˆ‡ç‰‡ä¸­å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (MARL) çš„åŒå¤§è¯­è¨€æ¨¡å‹ (dual-LLM) æ¡†æ¶ã€‚é’ˆå¯¹æ·±åº¦å¼ºåŒ–å­¦ä¹  (DRL) åœ¨å¤„ç†å°„é¢‘ (RF) ç‰¹å¾å’ŒæœåŠ¡è´¨é‡ (QoS) æŒ‡æ ‡ç­‰åŸå§‹éç»“æ„åŒ–æ•°æ®æ—¶é¢ä¸´çš„ç­–ç•¥æ³›åŒ–å’Œå†³ç­–æ•ˆç‡æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†è¯­ä¹‰å¢å¼ºçš„çŠ¶æ€è¡¨ç¤ºã€‚å…¶æ ¸å¿ƒåŒ…å«ä¸€ä¸ªåœ¨ O-RAN æ§åˆ¶ä¸é…ç½®æ•°æ®ä¸Šé¢„è®­ç»ƒçš„é¢†åŸŸç‰¹å®šæ¨¡å‹ ORANSightï¼Œç”¨äºç”Ÿæˆç»“æ„åŒ–çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ promptsï¼Œå¹¶å°†å…¶ä¸ learnable tokens èåˆã€‚é€šè¿‡å†»ç»“çš„ GPT ç¼–ç å™¨ï¼Œè¿™äº›è¾“å…¥è¢«è½¬åŒ–ä¸ºé«˜å±‚è¯­ä¹‰è¡¨ç¤ºå¹¶ä¼ é€’ç»™ DRL æ™ºèƒ½ä½“ï¼Œæ„å»ºå‡ºä¸€ç§ä¸“ä¸ºæ— çº¿ç³»ç»ŸæŠ€æœ¯å†³ç­–å®šåˆ¶çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) é£æ ¼æµæ°´çº¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒORAN-GUIDE åœ¨æ ·æœ¬æ•ˆç‡ã€ç­–ç•¥æ”¶æ•›æ€§å’Œæ€§èƒ½æ³›åŒ–æ–¹é¢å‡æ˜¾è‘—ä¼˜äºæ ‡å‡† MARL å’Œå• LLM åŸºå‡†æ¨¡å‹ï¼Œæœ‰æ•ˆæå‡äº†å¤æ‚åŠ¨æ€ç¯å¢ƒä¸‹çš„ç½‘ç»œç®¡ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00576v1",
      "published_date": "2025-05-31 14:21:19 UTC",
      "updated_date": "2025-05-31 14:21:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:57:11.161758+00:00"
    },
    {
      "arxiv_id": "2506.00574v1",
      "title": "Prompt-Tuned LLM-Augmented DRL for Dynamic O-RAN Network Slicing",
      "title_zh": "åŸºäºæç¤ºå¾®è°ƒå¤§æ¨¡å‹å¢å¼ºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„åŠ¨æ€ O-RAN ç½‘ç»œåˆ‡ç‰‡",
      "authors": [
        "Fatemeh Lotfi",
        "Hossein Rajoli",
        "Fatemeh Afghah"
      ],
      "abstract": "Modern wireless networks must adapt to dynamic conditions while efficiently managing diverse service demands. Traditional deep reinforcement learning (DRL) struggles in these environments, as scattered and evolving feedback makes optimal decision-making challenging. Large Language Models (LLMs) offer a solution by structuring unorganized network feedback into meaningful latent representations, helping RL agents recognize patterns more effectively. For example, in O-RAN slicing, concepts like SNR, power levels and throughput are semantically related, and LLMs can naturally cluster them, providing a more interpretable state representation. To leverage this capability, we introduce a contextualization-based adaptation method that integrates learnable prompts into an LLM-augmented DRL framework. Instead of relying on full model fine-tuning, we refine state representations through task-specific prompts that dynamically adjust to network conditions. Utilizing ORANSight, an LLM trained on O-RAN knowledge, we develop Prompt-Augmented Multi agent RL (PA-MRL) framework. Learnable prompts optimize both semantic clustering and RL objectives, allowing RL agents to achieve higher rewards in fewer iterations and adapt more efficiently. By incorporating prompt-augmented learning, our approach enables faster, more scalable, and adaptive resource allocation in O-RAN slicing. Experimental results show that it accelerates convergence and outperforms other baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£æ— çº¿ç½‘ç»œåœ¨åŠ¨æ€ç¯å¢ƒä¸‹è¿›è¡Œ O-RAN ç½‘ç»œåˆ‡ç‰‡æ—¶ä¼ ç»Ÿ Deep Reinforcement Learning (DRL) éš¾ä»¥å¤„ç†å¤æ‚åé¦ˆçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæç¤ºè°ƒä¼˜ (Prompt-Tuning) çš„ LLM å¢å¼º DRL æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åœ¨å¤§è§„æ¨¡ O-RAN çŸ¥è¯†ä¸Šé¢„è®­ç»ƒçš„ ORANSight æ¨¡å‹ï¼Œæ„å»ºäº† Prompt-Augmented Multi-agent RL (PA-MRL) ç³»ç»Ÿï¼Œé€šè¿‡å¯å­¦ä¹ æç¤º (learnable prompts) å°†ç½‘ç»œçŠ¶æ€è½¬åŒ–ä¸ºæ›´å…·è§£é‡Šæ€§çš„è¯­ä¹‰è¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•æ— éœ€å…¨é‡å¾®è°ƒå³å¯å®ç°çŠ¶æ€è¡¨ç¤ºçš„åŠ¨æ€è°ƒæ•´ï¼Œæœ‰æ•ˆä¼˜åŒ–äº† SNR å’Œååé‡ç­‰å…³é”®å‚æ•°çš„è¯­ä¹‰èšç±»ä¸å¼ºåŒ–å­¦ä¹ ç›®æ ‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPA-MRL èƒ½å¤Ÿæ˜¾è‘—åŠ å¿«æ”¶æ•›é€Ÿåº¦å¹¶æé«˜æ™ºèƒ½ä½“çš„å¥–åŠ±æ”¶ç›Šï¼Œä¸º O-RAN åˆ‡ç‰‡æä¾›äº†æ›´é«˜æ•ˆã€å¯æ‰©å±•ä¸”å…·æœ‰è‡ªé€‚åº”æ€§çš„èµ„æºåˆ†é…æ–¹æ¡ˆï¼Œå…¶æ€§èƒ½è¡¨ç°å…¨é¢ä¼˜äºä¼ ç»Ÿçš„åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00574v1",
      "published_date": "2025-05-31 14:12:56 UTC",
      "updated_date": "2025-05-31 14:12:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:57:31.825310+00:00"
    },
    {
      "arxiv_id": "2506.00570v1",
      "title": "A \"Wenlu\" Brain System for Multimodal Cognition and Embodied Decision-Making: A Secure New Architecture for Deep Integration of Foundation Models and Domain Knowledge",
      "title_zh": "â€œWenluâ€å¤šæ¨¡æ€è®¤çŸ¥ä¸å…·èº«å†³ç­–å¤§è„‘ç³»ç»Ÿï¼šä¸€ç§åŸºåº§æ¨¡å‹ä¸é¢†åŸŸçŸ¥è¯†æ·±åº¦é›†æˆçš„å®‰å…¨æ–°æ¶æ„",
      "authors": [
        "Liang Geng"
      ],
      "abstract": "With the rapid penetration of artificial intelligence across industries and scenarios, a key challenge in building the next-generation intelligent core lies in effectively integrating the language understanding capabilities of foundation models with domain-specific knowledge bases in complex real-world applications. This paper proposes a multimodal cognition and embodied decision-making brain system, ``Wenlu\", designed to enable secure fusion of private knowledge and public models, unified processing of multimodal data such as images and speech, and closed-loop decision-making from cognition to automatic generation of hardware-level code. The system introduces a brain-inspired memory tagging and replay mechanism, seamlessly integrating user-private data, industry-specific knowledge, and general-purpose language models. It provides precise and efficient multimodal services for enterprise decision support, medical analysis, autonomous driving, robotic control, and more. Compared with existing solutions, ``Wenlu\" demonstrates significant advantages in multimodal processing, privacy security, end-to-end hardware control code generation, self-learning, and sustainable updates, thus laying a solid foundation for constructing the next-generation intelligent core.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†â€œæ–‡è·¯â€ (Wenlu) ç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªé¢å‘å¤šæ¨¡æ€è®¤çŸ¥å’Œå…·èº«å†³ç­–çš„å¤§è„‘ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³åŸºç¡€æ¨¡å‹ (Foundation Models) ä¸ç‰¹å®šé¢†åŸŸçŸ¥è¯†åœ¨å¤æ‚ç°å®åº”ç”¨ä¸­æ·±åº¦é›†æˆçš„æŒ‘æˆ˜ã€‚è¯¥æ¶æ„å¼•å…¥äº†ä¸€ç§ç±»è„‘å¯å‘å¼å­˜å‚¨æ ‡ç­¾ä¸å›æ”¾æœºåˆ¶ (Brain-inspired memory tagging and replay mechanism)ï¼Œå®ç°äº†ç”¨æˆ·ç§æœ‰æ•°æ®ã€è¡Œä¸šçŸ¥è¯†ä¸é€šç”¨è¯­è¨€æ¨¡å‹çš„å®‰å…¨èåˆã€‚ç³»ç»Ÿå…·å¤‡ç»Ÿä¸€å¤„ç†å›¾åƒã€è¯­éŸ³ç­‰å¤šç§æ¨¡æ€æ•°æ®çš„èƒ½åŠ›ï¼Œå¹¶èƒ½å®Œæˆä»è®¤çŸ¥åˆ°è‡ªåŠ¨ç”Ÿæˆç¡¬ä»¶çº§ä»£ç  (Hardware-level code) çš„é—­ç¯å†³ç­–æµç¨‹ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿä¸ºä¼ä¸šå†³ç­–æ”¯æŒã€åŒ»ç–—åˆ†æã€è‡ªåŠ¨é©¾é©¶åŠæœºå™¨äººæ§åˆ¶ç­‰é¢†åŸŸæä¾›ç²¾ç¡®ä¸”é«˜æ•ˆçš„å¤šæ¨¡æ€æœåŠ¡ã€‚ä¸ç°æœ‰æ–¹æ¡ˆç›¸æ¯”ï¼Œâ€œæ–‡è·¯â€åœ¨å¤šæ¨¡æ€å¤„ç† (Multimodal processing)ã€éšç§å®‰å…¨ (Privacy security) ä»¥åŠç«¯åˆ°ç«¯ç¡¬ä»¶æ§åˆ¶ä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿæ”¯æŒè‡ªå­¦ä¹ ä¸æŒç»­æ›´æ–°ï¼Œä¸ºæ„å»ºå…¼é¡¾ç§æœ‰çŸ¥è¯†å®‰å…¨ä¸å…¬æœ‰æ¨¡å‹èƒ½åŠ›çš„ä¸‹ä¸€ä»£æ™ºèƒ½æ ¸å¿ƒå¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00570v1",
      "published_date": "2025-05-31 14:01:34 UTC",
      "updated_date": "2025-05-31 14:01:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:57:33.854175+00:00"
    },
    {
      "arxiv_id": "2506.06335v2",
      "title": "FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models",
      "title_zh": "FinBERT2ï¼šæ—¨åœ¨å¼¥åˆå¤§è¯­è¨€æ¨¡å‹é‡‘èé¢†åŸŸç‰¹å®šéƒ¨ç½²å·®è·çš„ä¸“ä¸šåŒå‘ç¼–ç å™¨",
      "authors": [
        "Xuan Xu",
        "Fufang Wen",
        "Beilin Chu",
        "Zhibing Fu",
        "Qinhong Lin",
        "Jiaqi Liu",
        "Binjie Fei",
        "Yu Li",
        "Linna Zhou",
        "Zhongliang Yang"
      ],
      "abstract": "In natural language processing (NLP), the focus has shifted from encoder-only tiny language models like BERT to decoder-only large language models(LLMs) such as GPT-3. However, LLMs' practical application in the financial sector has revealed three limitations: (1) LLMs often perform worse than fine-tuned BERT on discriminative tasks despite costing much higher computational resources, such as market sentiment analysis in financial reports; (2) Application on generative tasks heavily relies on retrieval augmented generation (RAG) methods to provide current and specialized information, with general retrievers showing suboptimal performance on domain-specific retrieval tasks; (3) There are additional inadequacies in other feature-based scenarios, such as topic modeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained on a high-quality, financial-specific corpus of 32b tokens. This represents the largest known Chinese financial pretraining corpus for models of this parameter size. As a better backbone, FinBERT2 can bridge the gap in the financial-specific deployment of LLMs through the following achievements: (1) Discriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT variants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five financial classification tasks. (2) Contrastive fine-tuned models (Fin-Retrievers) outperform both open-source (e.g., +6.8\\% avg improvement over BGE-base-zh) and proprietary (e.g., +4.2\\% avg improvement over OpenAI's text-embedding-3-large) embedders across five financial retrieval tasks; (3) Building on FinBERT2 variants, we construct the Fin-TopicModel, which enables superior clustering and topic representation for financial titles. Our work revisits financial BERT models through comparative analysis with contemporary LLMs and offers practical insights for effectively utilizing FinBERT in the LLMs era.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† FinBERT2ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹é‡‘èé¢†åŸŸä¼˜åŒ–çš„åŒå‘ç¼–ç å™¨ (Bidirectional Encoder)ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é‡‘èç‰¹å®šéƒ¨ç½²ä¸­é¢ä¸´çš„åˆ¤åˆ«ä»»åŠ¡æ€§èƒ½ä¸è¶³ã€æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) æ•ˆæœä¸ä½³ä»¥åŠä¸»é¢˜å»ºæ¨¡èƒ½åŠ›æ¬ ç¼ºç­‰é—®é¢˜ã€‚è¯¥æ¨¡å‹åŸºäºåŒ…å« 32b tokens çš„é«˜è´¨é‡ä¸­æ–‡é‡‘èè¯­æ–™åº“è¿›è¡Œé¢„è®­ç»ƒï¼Œæ˜¯ç›®å‰è¯¥å‚æ•°è§„æ¨¡ä¸‹å·²çŸ¥æœ€å¤§çš„ä¸­æ–‡é‡‘èé¢„è®­ç»ƒæ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„ Fin-Labelers åœ¨é‡‘èåˆ†ç±»ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºé¢†å…ˆçš„ LLMsï¼Œè€Œ Fin-Retrievers åœ¨é‡‘èæ£€ç´¢ä»»åŠ¡ä¸­çš„è¡¨ç°ä¹Ÿè¶…è¿‡äº† BGE å’Œ OpenAI çš„å•†ä¸šåµŒå…¥ (Embedding) æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒåŸºäºè¯¥æ¶æ„æ„å»ºçš„ Fin-TopicModel åœ¨é‡‘èæ ‡é¢˜çš„èšç±»å’Œä¸»é¢˜è¡¨ç¤º (Topic Representation) æ–¹é¢è¡¨ç°å“è¶Šã€‚è¯¥é¡¹å·¥ä½œé€šè¿‡ä¸å½“ä»£ LLMs çš„æ·±å…¥å¯¹æ¯”ï¼Œé‡æ–°è¯æ˜äº†ä¸“ç”¨ BERT æ¨¡å‹åœ¨é‡‘èå‚ç›´é¢†åŸŸéƒ¨ç½²ä¸­çš„æ ¸å¿ƒä»·å€¼ä¸å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CE",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06335v2",
      "published_date": "2025-05-31 13:59:44 UTC",
      "updated_date": "2025-07-05 13:39:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:57:32.208563+00:00"
    },
    {
      "arxiv_id": "2506.00563v2",
      "title": "Understanding Behavioral Metric Learning: A Large-Scale Study on Distracting Reinforcement Learning Environments",
      "title_zh": "ç†è§£è¡Œä¸ºåº¦é‡å­¦ä¹ ï¼šé’ˆå¯¹å—å¹²æ‰°å¼ºåŒ–å­¦ä¹ ç¯å¢ƒçš„å¤§è§„æ¨¡ç ”ç©¶",
      "authors": [
        "Ziyan Luo",
        "Tianwei Ni",
        "Pierre-Luc Bacon",
        "Doina Precup",
        "Xujie Si"
      ],
      "abstract": "A key approach to state abstraction is approximating behavioral metrics (notably, bisimulation metrics) in the observation space and embedding these learned distances in the representation space. While promising for robustness to task-irrelevant noise, as shown in prior work, accurately estimating these metrics remains challenging, requiring various design choices that create gaps between theory and practice. Prior evaluations focus mainly on final returns, leaving the quality of learned metrics and the source of performance gains unclear. To systematically assess how metric learning works in deep reinforcement learning (RL), we evaluate five recent approaches, unified conceptually as isometric embeddings with varying design choices. We benchmark them with baselines across 20 state-based and 14 pixel-based tasks, spanning 370 task configurations with diverse noise settings. Beyond final returns, we introduce the evaluation of a denoising factor to quantify the encoder's ability to filter distractions. To further isolate the effect of metric learning, we propose and evaluate an isolated metric estimation setting, in which the encoder is influenced solely by the metric loss. Finally, we release an open-source, modular codebase to improve reproducibility and support future research on metric learning in deep RL.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†è¡Œä¸ºåº¦é‡å­¦ä¹ (Behavioral Metric Learning)åœ¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)çŠ¶æ€æŠ½è±¡ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³è§‚æµ‹ç©ºé—´ä¸­ä»»åŠ¡æ— å…³å™ªå£°å¯¼è‡´çš„åº¦é‡ä¼°è®¡éš¾é¢˜ã€‚ä½œè€…å°†äº”ç§è¿‘å¹´æ¥çš„ä¸»æµæ–¹æ³•åœ¨æ¦‚å¿µä¸Šç»Ÿä¸€ä¸ºå…·æœ‰ä¸åŒè®¾è®¡é€‰æ‹©çš„ç­‰è·åµŒå…¥(Isometric Embeddings)ï¼Œå¹¶åœ¨æ¶µç›–370ç§å™ªå£°é…ç½®çš„20ä¸ªçŠ¶æ€ä»»åŠ¡å’Œ14ä¸ªåƒç´ ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶ä¸ä»…å…³æ³¨æœ€ç»ˆå›æŠ¥(Final Returns)ï¼Œè¿˜å¼•å…¥äº†å»å™ªå› å­(Denoising Factor)æ¥é‡åŒ–ç¼–ç å™¨è¿‡æ»¤å¹²æ‰°çš„èƒ½åŠ›ï¼Œå¹¶é€šè¿‡æè®®çš„å­¤ç«‹åº¦é‡ä¼°è®¡(Isolated Metric Estimation)è®¾ç½®è¿›ä¸€æ­¥å‰¥ç¦»åº¦é‡å­¦ä¹ çš„ç‹¬ç«‹å½±å“ã€‚å®éªŒç»“æœæ­ç¤ºäº†å½“å‰æ–¹æ³•åœ¨ç†è®ºä¸å®è·µä¹‹é—´çš„å·®è·åŠå…¶æ€§èƒ½å¢ç›Šçš„æ¥æºï¼Œä¸ºç†è§£åº¦é‡å­¦ä¹ å¦‚ä½•å½±å“è¡¨å¾è´¨é‡æä¾›äº†æ·±å…¥è§è§£ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å‘å¸ƒäº†ä¸€ä¸ªå¼€æºã€æ¨¡å—åŒ–çš„ä»£ç åº“ï¼Œä»¥ä¿ƒè¿›æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­åº¦é‡å­¦ä¹ çš„å¯é‡å¤æ€§åŠæœªæ¥ç ”ç©¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00563v2",
      "published_date": "2025-05-31 13:43:41 UTC",
      "updated_date": "2025-09-08 18:56:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:57:35.540597+00:00"
    },
    {
      "arxiv_id": "2506.03192v1",
      "title": "Encoding of Demographic and Anatomical Information in Chest X-Ray-based Severe Left Ventricular Hypertrophy Classifiers",
      "title_zh": "åŸºäºèƒ¸éƒ¨ X çº¿ç‰‡çš„é‡åº¦å·¦å¿ƒå®¤è‚¥åšåˆ†ç±»å™¨ä¸­äººå£ç»Ÿè®¡å­¦ä¸è§£å‰–å­¦ä¿¡æ¯çš„ç¼–ç ",
      "authors": [
        "Basudha Pal",
        "Rama Chellappa",
        "Muhammad Umair"
      ],
      "abstract": "While echocardiography and MRI are clinical standards for evaluating cardiac structure, their use is limited by cost and accessibility.We introduce a direct classification framework that predicts severe left ventricular hypertrophy from chest X-rays, without relying on anatomical measurements or demographic inputs. Our approach achieves high AUROC and AUPRC, and employs Mutual Information Neural Estimation to quantify feature expressivity. This reveals clinically meaningful attribute encoding and supports transparent model interpretation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç›´æ¥åˆ†ç±»æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡èƒ¸éƒ¨ X å°„çº¿ (Chest X-ray) é¢„æµ‹ä¸¥é‡å·¦å¿ƒå®¤è‚¥åš (Severe Left Ventricular Hypertrophy)ï¼Œä»¥å¼¥è¡¥è¶…å£°å¿ƒåŠ¨å›¾å’Œ MRI åœ¨æˆæœ¬ä¸æ™®åŠæ€§ä¸Šçš„ä¸è¶³ã€‚è¯¥æ–¹æ³•åœ¨ä¸ä¾èµ–è§£å‰–æµ‹é‡æˆ–äººå£ç»Ÿè®¡è¾“å…¥çš„å‰æä¸‹ï¼Œå®ç°äº†æé«˜çš„ AUROC å’Œ AUPRC é¢„æµ‹è¡¨ç°ã€‚ä¸ºäº†æå‡æ¨¡å‹é€æ˜åº¦ï¼Œç ”ç©¶é‡‡ç”¨äº†äº’ä¿¡æ¯ç¥ç»ä¼°è®¡ (Mutual Information Neural Estimation) æŠ€æœ¯æ¥é‡åŒ–ç‰¹å¾çš„è¡¨è¾¾èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ³•æ­ç¤ºäº†æ¨¡å‹å†…éƒ¨å…·æœ‰ä¸´åºŠæ„ä¹‰çš„å±æ€§ç¼–ç ï¼Œå¹¶ä¸ºåŒ»ç–— AI æ¨¡å‹çš„é€æ˜è§£é‡Šæä¾›äº†é‡è¦æ”¯æ’‘ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03192v1",
      "published_date": "2025-05-31 13:30:04 UTC",
      "updated_date": "2025-05-31 13:30:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:57:45.999488+00:00"
    },
    {
      "arxiv_id": "2506.00555v2",
      "title": "MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning",
      "title_zh": "MMedAgent-RLï¼šé¢å‘å¤šæ¨¡æ€åŒ»å­¦æ¨ç†çš„å¤šæ™ºèƒ½ä½“åä½œä¼˜åŒ–",
      "authors": [
        "Peng Xia",
        "Jinglu Wang",
        "Yibo Peng",
        "Kaide Zeng",
        "Xian Wu",
        "Xiangru Tang",
        "Hongtu Zhu",
        "Yun Li",
        "Shujie Liu",
        "Yan Lu",
        "Huaxiu Yao"
      ],
      "abstract": "Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy that progressively teaches the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL not only outperforms both open-source and proprietary Med-LVLMs, but also exhibits human-like reasoning patterns. Notably, it achieves an average performance gain of 20.7% over supervised fine-tuning baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MMedAgent-RLï¼Œä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤šæ¨¡æ€åŒ»ç–—æ¨ç†ä¸­çš„åŠ¨æ€åä½œã€‚é€šè¿‡åŸºäºQwen2.5-VLè®­ç»ƒåˆ†è¯ŠåŒ»ç”Ÿ(triage doctor)å’Œä¸»æ²»åŒ»ç”Ÿ(attending physician)ä¸¤ä¸ªæ ¸å¿ƒæ™ºèƒ½ä½“ï¼Œè¯¥æ¡†æ¶å®ç°äº†çµæ´»çš„ä»»åŠ¡åˆ†é…ä¸å†³ç­–æ•´åˆã€‚ä¸ºäº†åº”å¯¹ä¸“å®¶è¾“å‡ºçš„ä¸ä¸€è‡´æ€§ï¼Œç ”ç©¶å¼•å…¥äº†è¯¾ç¨‹å­¦ä¹ (Curriculum Learning)å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œä½¿ä¸»æ²»åŒ»ç”Ÿèƒ½å¤Ÿæœ‰æ•ˆåœ°åœ¨é‡‡çº³ä¸“å®¶æ„è§ä¸è‡ªä¸»çº é”™é—´è¿›è¡Œæƒè¡¡ã€‚å®éªŒåœ¨äº”ä¸ªåŒ»ç–—VQAåŸºå‡†ä¸Šè¯æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼ŒMMedAgent-RLä¸ä»…è¶…è¶Šäº†ç°æœ‰çš„å¼€æºåŠé—­æºMed-LVLMsï¼Œè¿˜åœ¨ç›‘ç£å¾®è°ƒ(SFT)åŸºçº¿çš„åŸºç¡€ä¸Šå®ç°äº†20.7%çš„å¹³å‡æ€§èƒ½æå‡ã€‚è¯¥æ¡†æ¶å±•ç°å‡ºçš„ç±»äººæ¨ç†æ¨¡å¼ä¸ºæ„å»ºæ›´å…·é€‚åº”æ€§çš„åŒ»ç–—è¾…åŠ©æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00555v2",
      "published_date": "2025-05-31 13:22:55 UTC",
      "updated_date": "2025-06-17 03:59:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:57:45.613754+00:00"
    },
    {
      "arxiv_id": "2506.00551v2",
      "title": "AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation",
      "title_zh": "AnnaAgentï¼šå…·å¤‡å¤šä¼šè¯è®°å¿†ã€ç”¨äºé€¼çœŸå¯»åŠ©è€…æ¨¡æ‹Ÿçš„åŠ¨æ€æ¼”åŒ–æ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Ming Wang",
        "Peidong Wang",
        "Lin Wu",
        "Xiaocui Yang",
        "Daling Wang",
        "Shi Feng",
        "Yuxin Chen",
        "Bixuan Wang",
        "Yifei Zhang"
      ],
      "abstract": "Constrained by the cost and ethical concerns of involving real seekers in AI-driven mental health, researchers develop LLM-based conversational agents (CAs) with tailored configurations, such as profiles, symptoms, and scenarios, to simulate seekers. While these efforts advance AI in mental health, achieving more realistic seeker simulation remains hindered by two key challenges: dynamic evolution and multi-session memory. Seekers' mental states often fluctuate during counseling, which typically spans multiple sessions. To address this, we propose AnnaAgent, an emotional and cognitive dynamic agent system equipped with tertiary memory. AnnaAgent incorporates an emotion modulator and a complaint elicitor trained on real counseling dialogues, enabling dynamic control of the simulator's configurations. Additionally, its tertiary memory mechanism effectively integrates short-term and long-term memory across sessions. Evaluation results, both automated and manual, demonstrate that AnnaAgent achieves more realistic seeker simulation in psychological counseling compared to existing baselines. The ethically reviewed and screened code can be found on https://github.com/sci-m-wang/AnnaAgent.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹AIå¿ƒç†å¥åº·é¢†åŸŸåœ¨æ¨¡æ‹Ÿå¯»æ±‚å¸®åŠ©è€…(seeker)æ—¶é¢ä¸´çš„åŠ¨æ€æ¼”å˜å’Œå¤šè½®ä¼šè¯è®°å¿†(multi-session memory)ä¸¤å¤§æŒ‘æˆ˜ï¼Œæå‡ºäº†AnnaAgentç³»ç»Ÿã€‚AnnaAgentæ˜¯ä¸€ä¸ªå…·å¤‡æƒ…æ„Ÿä¸è®¤çŸ¥åŠ¨æ€æ¼”å˜èƒ½åŠ›çš„æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé€šè¿‡ä¸‰çº§å­˜å‚¨æœºåˆ¶(tertiary memory)æœ‰æ•ˆæ•´åˆäº†è·¨ä¼šè¯çš„çŸ­æœŸå’Œé•¿æœŸè®°å¿†ã€‚è¯¥ç³»ç»Ÿå¼•å…¥äº†åœ¨çœŸå®å’¨è¯¢å¯¹è¯ä¸Šè®­ç»ƒçš„æƒ…æ„Ÿè°ƒèŠ‚å™¨(emotion modulator)å’Œè¯‰æ±‚è¯±å¯¼å™¨(complaint elicitor)ï¼Œå®ç°äº†å¯¹æ¨¡æ‹Ÿå™¨é…ç½®çš„åŠ¨æ€æ§åˆ¶ã€‚è‡ªåŠ¨ä¸äººå·¥è¯„ä¼°ç»“æœå‡è¡¨æ˜ï¼Œä¸ç°æœ‰åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒAnnaAgentåœ¨å¿ƒç†å’¨è¯¢åœºæ™¯ä¸­å®ç°äº†æ›´å…·çœŸå®æ„Ÿçš„å¯»æ±‚å¸®åŠ©è€…æ¨¡æ‹Ÿã€‚è¯¥é¡¹å·¥ä½œä¸ºå…‹æœå¿ƒç†å¥åº·é¢†åŸŸAIç ”å‘ä¸­çš„ä¼¦ç†é™åˆ¶å’Œé«˜æ˜‚æˆæœ¬æä¾›äº†æœ‰æ•ˆçš„ä»¿çœŸå·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00551v2",
      "published_date": "2025-05-31 13:15:51 UTC",
      "updated_date": "2025-06-10 16:35:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:58:01.556294+00:00"
    },
    {
      "arxiv_id": "2506.00549v1",
      "title": "Towards Multi-dimensional Evaluation of LLM Summarization across Domains and Languages",
      "title_zh": "è¿ˆå‘è·¨é¢†åŸŸä¸è·¨è¯­è¨€çš„å¤§è¯­è¨€æ¨¡å‹æ‘˜è¦å¤šç»´è¯„ä¼°",
      "authors": [
        "Hyangsuk Min",
        "Yuho Lee",
        "Minjeong Ban",
        "Jiaqi Deng",
        "Nicole Hee-Yeon Kim",
        "Taewon Yun",
        "Hang Su",
        "Jason Cai",
        "Hwanjun Song"
      ],
      "abstract": "Evaluation frameworks for text summarization have evolved in terms of both domain coverage and metrics. However, existing benchmarks still lack domain-specific assessment criteria, remain predominantly English-centric, and face challenges with human annotation due to the complexity of reasoning. To address these, we introduce MSumBench, which provides a multi-dimensional, multi-domain evaluation of summarization in English and Chinese. It also incorporates specialized assessment criteria for each domain and leverages a multi-agent debate system to enhance annotation quality. By evaluating eight modern summarization models, we discover distinct performance patterns across domains and languages. We further examine large language models as summary evaluators, analyzing the correlation between their evaluation and summarization capabilities, and uncovering systematic bias in their assessment of self-generated summaries. Our benchmark dataset is publicly available at https://github.com/DISL-Lab/MSumBench.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æ–‡æœ¬æ‘˜è¦è¯„ä¼°æ¡†æ¶ç¼ºä¹é¢†åŸŸç‰¹å®šæ ‡å‡†ã€è¿‡åº¦ä»¥è‹±è¯­ä¸ºä¸­å¿ƒä»¥åŠäººå·¥æ ‡æ³¨å¤æ‚ç­‰å±€é™æ€§ï¼Œæå‡ºäº†ä¸­è‹±åŒè¯­å¤šé¢†åŸŸè¯„ä¼°æ¡†æ¶ MSumBenchã€‚è¯¥æ¡†æ¶ä¸ä»…ä¸ºæ¯ä¸ªé¢†åŸŸåˆ¶å®šäº†ä¸“é—¨çš„è¯„ä¼°æ ‡å‡†ï¼Œè¿˜å¼•å…¥äº† multi-agent debate ç³»ç»Ÿä»¥æå‡æ ‡æ³¨è´¨é‡ã€‚é€šè¿‡å¯¹å…«ç§ç°ä»£æ‘˜è¦æ¨¡å‹çš„è¯„ä¼°ï¼Œç ”ç©¶æ­ç¤ºäº†ä¸åŒé¢†åŸŸå’Œè¯­è¨€é—´æ˜¾è‘—çš„æ€§èƒ½å·®å¼‚ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºè¯„ä¼°è€…çš„è¡¨ç°ï¼Œå‘ç°å…¶åœ¨è¯„ä¼°è‡ªèº«ç”Ÿæˆçš„æ‘˜è¦æ—¶å­˜åœ¨æ˜æ˜¾çš„ systematic biasã€‚è¯¥é¡¹å·¥ä½œä¸ºå¤šç»´åº¦ã€è·¨é¢†åŸŸçš„æ–‡æœ¬æ‘˜è¦è¯„ä¼°æä¾›äº†é‡è¦çš„åŸºå‡†å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "34 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.00549v1",
      "published_date": "2025-05-31 13:12:35 UTC",
      "updated_date": "2025-05-31 13:12:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:57:54.506661+00:00"
    },
    {
      "arxiv_id": "2506.00545v1",
      "title": "Imputation of Missing Data in Smooth Pursuit Eye Movements Using a Self-Attention-based Deep Learning Approach",
      "title_zh": "åŸºäºè‡ªæ³¨æ„åŠ›æ·±åº¦å­¦ä¹ æ–¹æ³•çš„å¹³æ»‘è¿½éšçœ¼åŠ¨ç¼ºå¤±æ•°æ®æ’è¡¥",
      "authors": [
        "Mehdi Bejani",
        "Guillermo Perez-de-Arenaza-Pozo",
        "JuliÃ¡n D. Arias-LondoÃ±o",
        "Juan I. Godino-LLorente"
      ],
      "abstract": "Missing data is a relevant issue in time series, especially in biomedical sequences such as those corresponding to smooth pursuit eye movements, which often contain gaps due to eye blinks and track losses, complicating the analysis and extraction of meaningful biomarkers. In this paper, a novel imputation framework is proposed using Self-Attention-based Imputation networks for time series, which leverages the power of deep learning and self-attention mechanisms to impute missing data. We further refine the imputed data using a custom made autoencoder, tailored to represent smooth pursuit eye movement sequences. The proposed approach was implemented using 5,504 sequences from 172 Parkinsonian patients and healthy controls. Results show a significant improvement in the accuracy of reconstructed eye movement sequences with respect to other state of the art techniques, substantially reducing the values for common time domain error metrics such as the mean absolute error, mean relative error, and root mean square error, while also preserving the signal's frequency domain characteristics. Moreover, it demonstrates robustness when large intervals of data are missing. This method offers an alternative solution for robustly handling missing data in time series, enhancing the reliability of smooth pursuit analysis for the screening and monitoring of neurodegenerative disorders.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦æ—¶é—´åºåˆ—ï¼Œç‰¹åˆ«æ˜¯å¹³æ»‘è¿½è¸ªçœ¼åŠ¨ (Smooth Pursuit Eye Movements) ä¸­å› çœ¨çœ¼æˆ–ä¿¡å·ä¸¢å¤±å¯¼è‡´çš„ç¼ºå¤±æ•°æ®é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäº Self-Attention çš„æ·±åº¦å­¦ä¹ æ’è¡¥æ¡†æ¶ã€‚è¯¥æ–¹æ³•ç»“åˆäº† Self-Attention-based Imputation ç½‘ç»œä¸è‡ªå®šä¹‰çš„è‡ªåŠ¨ç¼–ç å™¨ (autoencoder) æ¥ç²¾ç‚¼æ’è¡¥ç»“æœï¼Œä»¥æ›´å¥½åœ°è¡¨ç¤ºçœ¼åŠ¨åºåˆ—ã€‚ç ”ç©¶åˆ©ç”¨ 172 åå¸•é‡‘æ£®ç—…æ‚£è€…å’Œå¥åº·å¯¹ç…§ç»„çš„ 5,504 æ¡æ•°æ®è¿›è¡ŒéªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¹³å‡ç»å¯¹è¯¯å·® (mean absolute error) å’Œå‡æ–¹æ ¹è¯¯å·® (root mean square error) ç­‰æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ŒåŒæ—¶æœ‰æ•ˆä¿ç•™äº†ä¿¡å·çš„é¢‘åŸŸç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤§æ®µæ•°æ®ç¼ºå¤±çš„æƒ…å†µä¸‹è¡¨ç°å‡ºæå¼ºçš„ç¨³å¥æ€§ã€‚è¯¥ç ”ç©¶ä¸ºæé«˜ç¥ç»é€€è¡Œæ€§ç–¾ç—…ç­›æŸ¥å’Œç›‘æµ‹ä¸­çœ¼åŠ¨åˆ†æçš„å¯é æ€§æä¾›äº†ä¸€ç§ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 10 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.00545v1",
      "published_date": "2025-05-31 13:10:30 UTC",
      "updated_date": "2025-05-31 13:10:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:57:55.821016+00:00"
    },
    {
      "arxiv_id": "2506.11066v2",
      "title": "CoQuIR: A Comprehensive Benchmark for Code Quality-Aware Information Retrieval",
      "title_zh": "CoQuIRï¼šé¢å‘ä»£ç è´¨é‡æ„ŸçŸ¥ä¿¡æ¯æ£€ç´¢çš„å…¨é¢åŸºå‡†",
      "authors": [
        "Jiahui Geng",
        "Fengyu Cai",
        "Shaobo Cui",
        "Qing Li",
        "Liangwei Chen",
        "Chenyang Lyu",
        "Haonan Li",
        "Derui Zhu",
        "Walter Pretschner",
        "Heinz Koeppl",
        "Fakhri Karray"
      ],
      "abstract": "Code retrieval is essential in modern software development, as it boosts code reuse and accelerates debugging. However, current benchmarks primarily emphasize functional relevance while neglecting critical dimensions of software quality. Motivated by this gap, we introduce CoQuIR, the first large-scale, multilingual benchmark specifically designed to evaluate quality-aware code retrieval across four key dimensions: correctness, efficiency, security, and maintainability. CoQuIR provides fine-grained quality annotations for 42,725 queries and 134,907 code snippets in 11 programming languages, and is accompanied by two quality-centric evaluation metrics: Pairwise Preference Accuracy and Margin-based Ranking Score. Using CoQuIR, we benchmark 23 retrieval models, covering both open-source and proprietary systems, and find that even top-performing models frequently fail to distinguish buggy or insecure code from their more robust counterparts. Furthermore, we conduct preliminary investigations into training methods that explicitly encourage retrievers to recognize code quality. Using synthetic datasets, we demonstrate promising improvements in quality-aware metrics across various models, without sacrificing semantic relevance. Downstream code generation experiments further validate the effectiveness of our approach. Overall, our work highlights the importance of integrating quality signals into code retrieval systems, laying the groundwork for more trustworthy and robust software development tools.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ä»£ç æ£€ç´¢åŸºå‡†æµ‹è¯•è¿‡äºå…³æ³¨åŠŸèƒ½ç›¸å…³æ€§è€Œå¿½è§†è½¯ä»¶è´¨é‡çš„ä¸è¶³ï¼Œæå‡ºäº†é¦–ä¸ªå¤§è§„æ¨¡ã€å¤šè¯­è¨€çš„è´¨é‡æ„ŸçŸ¥ä»£ç æ£€ç´¢åŸºå‡† CoQuIRã€‚è¯¥åŸºå‡†æ¶µç›–äº† Correctnessã€Efficiencyã€Security å’Œ Maintainability å››ä¸ªæ ¸å¿ƒè´¨é‡ç»´åº¦ï¼ŒåŒ…å« 11 ç§ç¼–ç¨‹è¯­è¨€çš„ 42,725 ä¸ªæŸ¥è¯¢å’Œ 134,907 ä¸ªä»£ç ç‰‡æ®µï¼Œå¹¶é…å¥—äº† Pairwise Preference Accuracy ç­‰è¯„ä¼°æŒ‡æ ‡ã€‚é€šè¿‡å¯¹ 23 ä¸ªä¸»æµå¼€æºåŠå•†ä¸šæ£€ç´¢æ¨¡å‹è¿›è¡Œè¯„æµ‹ï¼Œå‘ç°å³ä¾¿é¡¶å°–æ¨¡å‹ä¹Ÿéš¾ä»¥æœ‰æ•ˆåŒºåˆ†åŒ…å« Bug æˆ–ä¸å®‰å…¨çš„ä»£ç ã€‚ç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥é€šè¿‡åˆæˆæ•°æ®é›†è®­ç»ƒï¼Œè¯æ˜äº†åœ¨ä¸ç‰ºç‰²è¯­ä¹‰ç›¸å…³æ€§çš„å‰æä¸‹æå‡æ¨¡å‹è´¨é‡æ„ŸçŸ¥èƒ½åŠ›çš„å¯è¡Œæ€§ã€‚ä¸‹æ¸¸å®éªŒéªŒè¯äº†é›†æˆè´¨é‡ä¿¡å·èƒ½æ˜¾è‘—å¢å¼ºè½¯ä»¶å¼€å‘å·¥å…·çš„ç¨³å¥æ€§ï¼Œä¸ºæ„å»ºæ›´å¯é çš„ä»£ç æ£€ç´¢ç³»ç»Ÿæä¾›äº†åŸºç¡€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11066v2",
      "published_date": "2025-05-31 13:00:17 UTC",
      "updated_date": "2025-08-27 12:24:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:58:11.680538+00:00"
    },
    {
      "arxiv_id": "2506.00536v1",
      "title": "Decoupling Reasoning and Knowledge Injection for In-Context Knowledge Editing",
      "title_zh": "é’ˆå¯¹ä¸Šä¸‹æ–‡çŸ¥è¯†ç¼–è¾‘çš„æ¨ç†ä¸çŸ¥è¯†æ³¨å…¥è§£è€¦",
      "authors": [
        "Changyue Wang",
        "Weihang Su",
        "Qingyao Ai",
        "Yujia Zhou",
        "Yiqun Liu"
      ],
      "abstract": "Knowledge editing aims to efficiently update Large Language Models (LLMs) by modifying specific knowledge without retraining the entire model. Among knowledge editing approaches, in-context editing (ICE) offers a lightweight solution by injecting new knowledge directly into the input context, leaving model parameters unchanged. However, existing ICE approaches do not explicitly separate the newly injected knowledge from the model's original reasoning process. This entanglement often results in conflicts between external updates and internal parametric knowledge, undermining the consistency and accuracy of the reasoning path.In this work, we conduct preliminary experiments to examine how parametric knowledge influences reasoning path planning. We find that the model's reasoning is tightly coupled with its internal knowledge, and that naively injecting new information without adapting the reasoning path often leads to performance degradation, particularly in multi-hop tasks. To this end, we propose DecKER, a novel ICE framework that decouples reasoning from knowledge editing by generating a masked reasoning path and then resolving knowledge edits via hybrid retrieval and model-based validation. Experiments on multi-hop QA benchmarks show that DecKER significantly outperforms existing ICE methods by mitigating knowledge conflicts and preserving reasoning consistency. Our code is available at: https://github.com/bebr2/DecKER .",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„çŸ¥è¯†ç¼–è¾‘(Knowledge editing)é—®é¢˜ï¼Œæå‡ºå½“å‰ä¸Šä¸‹æ–‡ç¼–è¾‘(In-context editing, ICE)æ–¹æ³•ä¸­æ–°çŸ¥è¯†ä¸æ¨¡å‹åŸå§‹æ¨ç†è¿‡ç¨‹è€¦åˆï¼Œä¼šå¯¼è‡´å¤–éƒ¨æ›´æ–°ä¸å†…éƒ¨å‚æ•°åŒ–çŸ¥è¯†(Parametric knowledge)äº§ç”Ÿå†²çªã€‚é€šè¿‡åˆæ­¥å®éªŒï¼Œä½œè€…å‘ç°è¿™ç§è€¦åˆåœ¨å¤šè·³(Multi-hop)ä»»åŠ¡ä¸­å°¤ä¸ºä¸¥é‡ï¼Œå¸¸å› å†…éƒ¨çŸ¥è¯†å¹²æ‰°å¯¼è‡´æ¨ç†è·¯å¾„è§„åˆ’å¤±æ•ˆã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†DecKERæ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆæ©ç æ¨ç†è·¯å¾„(Masked reasoning path)å®ç°äº†æ¨ç†ä¸çŸ¥è¯†æ³¨å…¥çš„è§£è€¦ï¼Œå¹¶ç»“åˆæ··åˆæ£€ç´¢(Hybrid retrieval)å’ŒåŸºäºæ¨¡å‹çš„éªŒè¯æ¥å®ŒæˆçŸ¥è¯†ä¿®å¤ã€‚åœ¨å¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDecKERé€šè¿‡ç¼“è§£çŸ¥è¯†å†²çªå’Œä¿æŒæ¨ç†ä¸€è‡´æ€§ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„ICEæ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€çŸ¥è¯†ç¯å¢ƒä¸‹çš„æ¨ç†å¯é æ€§æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00536v1",
      "published_date": "2025-05-31 12:51:12 UTC",
      "updated_date": "2025-05-31 12:51:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:58:41.723079+00:00"
    },
    {
      "arxiv_id": "2506.00534v2",
      "title": "The Security Threat of Compressed Projectors in Large Vision-Language Models",
      "title_zh": "å¤§è§†è§‰è¯­è¨€æ¨¡å‹ä¸­å‹ç¼©æŠ•å½±å™¨çš„å®‰å…¨å¨èƒ",
      "authors": [
        "Yudong Zhang",
        "Ruobing Xie",
        "Xingwu Sun",
        "Jiansheng Chen",
        "Zhanhui Kang",
        "Di Wang",
        "Yu Wang"
      ],
      "abstract": "The choice of a suitable visual language projector (VLP) is critical to the successful training of large visual language models (LVLMs). Mainstream VLPs can be broadly categorized into compressed and uncompressed projectors, and each offers distinct advantages in performance and computational efficiency. However, their security implications have not been thoroughly examined. Our comprehensive evaluation reveals significant differences in their security profiles: compressed projectors exhibit substantial vulnerabilities, allowing adversaries to successfully compromise LVLMs even with minimal knowledge of structure information. In stark contrast, uncompressed projectors demonstrate robust security properties and do not introduce additional vulnerabilities. These findings provide critical guidance for researchers in selecting optimal VLPs that enhance the security and reliability of visual language models. The code is available at https://github.com/btzyd/TCP.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (Large Vision-Language Models, LVLMs) ä¸­è§†è§‰è¯­è¨€æŠ•å½±å™¨ (Visual Language Projectors, VLPs) çš„å®‰å…¨æ€§å¨èƒï¼Œé‡ç‚¹å¯¹æ¯”äº† Compressed Projectors ä¸ Uncompressed Projectors åœ¨å®‰å…¨å±æ€§ä¸Šçš„æ˜¾è‘—å·®å¼‚ã€‚é€šè¿‡å…¨é¢çš„è¯„ä¼°å‘ç°ï¼ŒCompressed Projectors å­˜åœ¨ä¸¥é‡çš„å®‰å…¨æ¼æ´ï¼Œä½¿æ”»å‡»è€…åœ¨ä»…æŒæ¡æå°‘ç»“æ„ä¿¡æ¯çš„æƒ…å†µä¸‹å³å¯æˆåŠŸç ´åæ¨¡å‹ã€‚ä¸ä¹‹å½¢æˆé²œæ˜å¯¹æ¯”çš„æ˜¯ï¼ŒUncompressed Projectors å±•ç°äº†æå¼ºçš„é²æ£’æ€§ï¼Œä¸”ä¸ä¼šä¸ºæ¨¡å‹å¼•å…¥é¢å¤–çš„å®‰å…¨é£é™©ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº† VLP çš„é€‰æ‹©å¯¹äºæå‡ LVLMs å®‰å…¨æ€§å’Œå¯é æ€§çš„å…³é”®ä½œç”¨ã€‚è¯¥ç ”ç©¶ä¸ºç ”ç©¶äººå‘˜åœ¨é€‰æ‹©æœ€ä¼˜æŠ•å½±å™¨æ¶æ„æ—¶æä¾›äº†é‡è¦çš„å†³ç­–ä¾æ®ï¼Œå¹¶å¼€æºäº†ç›¸å…³ä»£ç ä»¥ä¾›ç¤¾åŒºå‚è€ƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by EMNLP 2025 findings",
      "pdf_url": "https://arxiv.org/pdf/2506.00534v2",
      "published_date": "2025-05-31 12:43:56 UTC",
      "updated_date": "2025-10-04 03:41:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:58:43.825378+00:00"
    },
    {
      "arxiv_id": "2506.00531v1",
      "title": "M2WLLM: Multi-Modal Multi-Task Ultra-Short-term Wind Power Prediction Algorithm Based on Large Language Model",
      "title_zh": "M2WLLMï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€å¤šä»»åŠ¡è¶…çŸ­æœŸé£ç”µåŠŸç‡é¢„æµ‹ç®—æ³•",
      "authors": [
        "Hang Fana",
        "Mingxuan Lib",
        "Zuhan Zhanga",
        "Long Chengc",
        "Yujian Ye",
        "Dunnan Liua"
      ],
      "abstract": "The integration of wind energy into power grids necessitates accurate ultra-short-term wind power forecasting to ensure grid stability and optimize resource allocation. This study introduces M2WLLM, an innovative model that leverages the capabilities of Large Language Models (LLMs) for predicting wind power output at granular time intervals. M2WLLM overcomes the limitations of traditional and deep learning methods by seamlessly integrating textual information and temporal numerical data, significantly improving wind power forecasting accuracy through multi-modal data. Its architecture features a Prompt Embedder and a Data Embedder, enabling an effective fusion of textual prompts and numerical inputs within the LLMs framework. The Semantic Augmenter within the Data Embedder translates temporal data into a format that the LLMs can comprehend, enabling it to extract latent features and improve prediction accuracy. The empirical evaluations conducted on wind farm data from three Chinese provinces demonstrate that M2WLLM consistently outperforms existing methods, such as GPT4TS, across various datasets and prediction horizons. The results highlight LLMs' ability to enhance accuracy and robustness in ultra-short-term forecasting and showcase their strong few-shot learning capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† M2WLLMï¼Œä¸€ç§åŸºäº Large Language Models (LLMs) çš„å¤šæ¨¡æ€å¤šä»»åŠ¡è¶…çŸ­æœŸé£ç”µé¢„æµ‹ç®—æ³•ï¼Œæ—¨åœ¨é€šè¿‡ç²¾ç¡®é¢„æµ‹é£ç”µè¾“å‡ºæé«˜ç”µç½‘ç¨³å®šæ€§å’Œèµ„æºä¼˜åŒ–ã€‚è¯¥æ¨¡å‹é€šè¿‡æ— ç¼æ•´åˆæ–‡æœ¬ä¿¡æ¯ä¸æ—¶åºæ•°å€¼æ•°æ®ï¼Œå…‹æœäº†ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€æ•°æ®æ—¶çš„å±€é™æ€§ã€‚å…¶æ¶æ„æ ¸å¿ƒåŒ…å« Prompt Embedder å’Œ Data Embedderï¼Œå®ç°äº†æ–‡æœ¬æç¤ºä¸æ•°å€¼è¾“å…¥åœ¨ LLMs æ¡†æ¶å†…çš„æœ‰æ•ˆèåˆã€‚å…¶ä¸­çš„ Semantic Augmenter ç»„ä»¶å°†æ—¶åºæ•°æ®è½¬åŒ–ä¸º LLMs å¯ç†è§£çš„æ ¼å¼ï¼Œä»¥æ·±å…¥æå–æ½œåœ¨ç‰¹å¾å¹¶æå‡é¢„æµ‹ç²¾åº¦ã€‚åœ¨ä¸­å›½ä¸‰ä¸ªçœä»½é£ç”µåœºæ•°æ®çš„å®è¯è¯„ä¼°ä¸­ï¼ŒM2WLLM åœ¨å„ç§é¢„æµ‹å‘¨æœŸä¸‹çš„è¡¨ç°å‡ä¼˜äº GPT4TS ç­‰ç°æœ‰åŸºå‡†æ¨¡å‹ã€‚å®éªŒç»“æœä¸ä»…éªŒè¯äº† LLMs åœ¨å¢å¼ºè¶…çŸ­æœŸé¢„æµ‹å‡†ç¡®æ€§ä¸é²æ£’æ€§æ–¹é¢çš„æ½œåŠ›ï¼Œè¿˜å±•ç¤ºäº†å…¶å¼ºå¤§çš„ Few-shot learning èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00531v1",
      "published_date": "2025-05-31 12:27:17 UTC",
      "updated_date": "2025-05-31 12:27:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:58:35.335249+00:00"
    },
    {
      "arxiv_id": "2506.00530v1",
      "title": "CityLens: Benchmarking Large Language-Vision Models for Urban Socioeconomic Sensing",
      "title_zh": "CityLensï¼šé¢å‘åŸå¸‚ç¤¾ä¼šç»æµæ„ŸçŸ¥çš„è§†è§‰è¯­è¨€å¤§æ¨¡å‹è¯„æµ‹åŸºå‡†",
      "authors": [
        "Tianhui Liu",
        "Jie Feng",
        "Hetian Pang",
        "Xin Zhang",
        "Tianjian Ouyang",
        "Zhiyuan Zhang",
        "Yong Li"
      ],
      "abstract": "Understanding urban socioeconomic conditions through visual data is a challenging yet essential task for sustainable urban development and policy planning. In this work, we introduce $\\textbf{CityLens}$, a comprehensive benchmark designed to evaluate the capabilities of large language-vision models (LLVMs) in predicting socioeconomic indicators from satellite and street view imagery. We construct a multi-modal dataset covering a total of 17 globally distributed cities, spanning 6 key domains: economy, education, crime, transport, health, and environment, reflecting the multifaceted nature of urban life. Based on this dataset, we define 11 prediction tasks and utilize three evaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation, and Feature-Based Regression. We benchmark 17 state-of-the-art LLVMs across these tasks. Our results reveal that while LLVMs demonstrate promising perceptual and reasoning capabilities, they still exhibit limitations in predicting urban socioeconomic indicators. CityLens provides a unified framework for diagnosing these limitations and guiding future efforts in using LLVMs to understand and predict urban socioeconomic patterns. Our codes and datasets are open-sourced via https://github.com/tsinghua-fib-lab/CityLens.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CityLensï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€è§†è§‰æ¨¡å‹ (LLVMs) åˆ©ç”¨å«æ˜Ÿå’Œè¡—æ™¯å›¾åƒé¢„æµ‹åŸå¸‚ç¤¾ä¼šç»æµæŒ‡æ ‡èƒ½åŠ›çš„ç»¼åˆæ€§åŸºå‡†ã€‚ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªæ¶µç›–å…¨çƒ 17 ä¸ªåŸå¸‚çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œè§¦åŠç»æµã€æ•™è‚²ã€çŠ¯ç½ªã€äº¤é€šã€å¥åº·å’Œç¯å¢ƒ 6 ä¸ªå…³é”®é¢†åŸŸï¼Œå…¨é¢åæ˜ åŸå¸‚ç”Ÿæ´»çš„å¤æ‚æ€§ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œç ”ç©¶è€…å®šä¹‰äº† 11 é¡¹é¢„æµ‹ä»»åŠ¡ï¼Œå¹¶åº”ç”¨äº† Direct Metric Predictionã€Normalized Metric Estimation å’Œ Feature-Based Regression ä¸‰ç§è¯„ä¼°èŒƒå¼ã€‚å®éªŒå¯¹ 17 ä¸ªå…ˆè¿›çš„ LLVMs è¿›è¡Œäº†å…¨é¢è¯„æµ‹ï¼Œå‘ç°å°½ç®¡æ¨¡å‹å±•ç°å‡ºè‰¯å¥½çš„æ„ŸçŸ¥å’Œæ¨ç†æ½œåŠ›ï¼Œä½†åœ¨å‡†ç¡®é¢„æµ‹åŸå¸‚ç¤¾ä¼šç»æµæŒ‡æ ‡ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚CityLens ä¸ºè¯Šæ–­æ¨¡å‹å±€é™æ€§æä¾›äº†ç»Ÿä¸€æ¡†æ¶ï¼Œå¹¶ä¸ºæœªæ¥åˆ©ç”¨ LLVMs ç†è§£å’Œé¢„æµ‹åŸå¸‚ç¤¾ä¼šç»æµæ¨¡å¼æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00530v1",
      "published_date": "2025-05-31 12:25:33 UTC",
      "updated_date": "2025-05-31 12:25:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:58:45.717204+00:00"
    },
    {
      "arxiv_id": "2506.00527v1",
      "title": "Retrieval-Augmented Generation Systems for Intellectual Property via Synthetic Multi-Angle Fine-tuning",
      "title_zh": "åŸºäºåˆæˆå¤šè§’åº¦å¾®è°ƒçš„çŸ¥è¯†äº§æƒæ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ",
      "authors": [
        "Runtao Ren",
        "Jian Ma",
        "Jianxi Luo"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) systems in the Intellectual Property (IP) field often struggle with diverse user queries, including colloquial expressions, spelling errors, and ambiguous terminology, leading to inaccurate retrieval and suboptimal responses. To address this challenge, we propose Multi-Angle Question Generation and Retrieval Fine-Tuning Method (MQG-RFM), a novel framework that leverages large language models (LLMs) to simulate varied user inquiries and fine-tunes retrieval models to align semantically equivalent but linguistically diverse questions. Unlike complex architectural modifications, MQG-RFM adopts a lightweight Data-to-Tune paradigm, combining prompt-engineered query generation with hard negative mining to enhance retrieval robustness without costly infrastructure changes. Experimental results on a Taiwan patent Q&A dataset show 185.62% improvement in retrieval accuracy on the Patent Consultation dataset and 262.26% improvement on the Novel Patent Technology Report dataset, with 14.22% and 53.58% improvements in generation quality over the baselines, respectively. By bridging the gap between user intent and system comprehension through semantic-aware retrieval optimization, MQG-RFM offers a practical, scalable approach for rapid, cost-effective deployment among small and medium-sized agencies seeking reliable patent intelligence solutions. Additionally, our proposed method has already been adopted by ScholarMate, the largest professional research social networking platform in China, to support real-world development and deployment. A demo version of the instantiated is available at https://github.com/renruntao/patent_rag.",
      "tldr_zh": "åœ¨è¯¥ç ”ç©¶é’ˆå¯¹çŸ¥è¯†äº§æƒ(Intellectual Property, IP)é¢†åŸŸä¸­æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)ç³»ç»Ÿå› ç”¨æˆ·æŸ¥è¯¢å¤šæ ·ã€æ‹¼å†™é”™è¯¯åŠæœ¯è¯­æ¨¡ç³Šå¯¼è‡´çš„æ£€ç´¢ä¸ç²¾ç¡®é—®é¢˜ï¼Œæå‡ºäº†åä¸ºMQG-RFMçš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¨¡æ‹Ÿå¤šè§’åº¦çš„ç”¨æˆ·æé—®ï¼Œå¹¶å¯¹æ£€ç´¢æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥å®ç°è¯­ä¹‰ç­‰ä»·ä½†è¯­è¨€è¡¨è¾¾å„å¼‚çš„æŸ¥è¯¢å¯¹é½ã€‚MQG-RFMé‡‡ç”¨è½»é‡çº§çš„Data-to-TuneèŒƒå¼ï¼Œç»“åˆæç¤ºå·¥ç¨‹é©±åŠ¨çš„æŸ¥è¯¢ç”Ÿæˆä¸éš¾è´Ÿæ ·æœ¬æŒ–æ˜(hard negative mining)æŠ€æœ¯ï¼Œåœ¨ä¸æ”¹å˜å¤æ‚ç³»ç»Ÿæ¶æ„çš„æƒ…å†µä¸‹æ˜¾è‘—å¢å¼ºäº†æ£€ç´¢é²æ£’æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å°æ¹¾ä¸“åˆ©é—®ç­”æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨æ£€ç´¢å‡†ç¡®ç‡ä¸Šè¾ƒåŸºçº¿æ¨¡å‹æœ€é«˜æå‡äº†262.26%ï¼Œåœ¨ç”Ÿæˆè´¨é‡æ–¹é¢ä¹Ÿå–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚è¿™ç§è¯­ä¹‰æ„ŸçŸ¥æ£€ç´¢ä¼˜åŒ–æ–¹æ³•ä¸ºä¸­å°å‹æœºæ„æä¾›äº†ä¸€ç§å®ç”¨ä¸”å¯æ‰©å±•çš„ä¸“åˆ©æƒ…æŠ¥è§£å†³æ–¹æ¡ˆï¼Œç›®å‰å·²åœ¨ç§‘ç ”ç¤¾äº¤å¹³å°ScholarMateä¸­å¾—åˆ°å®é™…åº”ç”¨ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00527v1",
      "published_date": "2025-05-31 12:19:35 UTC",
      "updated_date": "2025-05-31 12:19:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:59:02.041656+00:00"
    },
    {
      "arxiv_id": "2506.00519v2",
      "title": "CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention",
      "title_zh": "CausalAbstainï¼šåˆ©ç”¨å› æœæ¨ç†å¢å¼ºå¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹çš„å¯ä¿¡å¼ƒæƒå†³ç­–",
      "authors": [
        "Yuxi Sun",
        "Aoqi Zuo",
        "Wei Gao",
        "Jing Ma"
      ],
      "abstract": "Large Language Models (LLMs) often exhibit knowledge disparities across languages. Encouraging LLMs to \\textit{abstain} when faced with knowledge gaps is a promising strategy to reduce hallucinations in multilingual settings. Current abstention strategies for multilingual scenarios primarily rely on generating feedback in various languages using LLMs and performing self-reflection. However, these methods can be adversely impacted by inaccuracies and biases in the generated feedback. To address this, from a causal perspective, we introduce \\textit{CausalAbstain}, a method that helps LLMs determine whether to utilize multiple generated feedback responses and how to identify the most useful ones. Extensive experiments demonstrate that \\textit{CausalAbstain} effectively selects helpful feedback and enhances abstention decisions with interpretability in both native language (\\textsc{Casual-native}) and multilingual (\\textsc{Causal-multi}) settings, outperforming strong baselines on two benchmark datasets covering encyclopedic and commonsense knowledge QA tasks. Our code and data are open-sourced at https://github.com/peachch/CausalAbstain.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¸åŒè¯­è¨€é—´çš„çŸ¥è¯†å·®å¼‚å¯¼è‡´å¹»è§‰çš„é—®é¢˜ï¼Œæ¢è®¨äº†åœ¨é¢å¯¹çŸ¥è¯†ç©ºç¼ºæ—¶å¦‚ä½•è®©æ¨¡å‹å¯é åœ°é€‰æ‹©æ‹’ç»å›ç­”(abstain)ã€‚ç›®å‰çš„å¤šè¯­è¨€æ‹’ç»ç­–ç•¥ä¸»è¦ä¾èµ–æ¨¡å‹ç”Ÿæˆåé¦ˆå¹¶è¿›è¡Œè‡ªæˆ‘åæ€ï¼Œä½†è¿™äº›åé¦ˆå¾€å¾€å—åˆ°ç”Ÿæˆå†…å®¹ä¸å‡†ç¡®å’Œåå·®çš„å½±å“ã€‚ä¸ºæ­¤ï¼Œä½œè€…ä»å› æœè§†è§’(causal perspective)æå‡ºäº† CausalAbstain æ–¹æ³•ï¼Œæ—¨åœ¨å¸®åŠ©æ¨¡å‹åˆ¤æ–­æ˜¯å¦åˆ©ç”¨å¤šä¸ªç”Ÿæˆçš„åé¦ˆå¹¶è¯†åˆ«å…¶ä¸­æœ€æœ‰ç”¨çš„ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCausalAbstain åœ¨åŸç”Ÿè¯­è¨€(Causal-native)å’Œå¤šè¯­è¨€(Causal-multi)è®¾ç½®ä¸‹å‡èƒ½æ˜¾è‘—ä¼˜åŒ–æ‹’ç»å†³ç­–ï¼Œå¹¶å¢å¼ºäº†å†³ç­–çš„å¯è§£é‡Šæ€§(interpretability)ã€‚åœ¨æ¶µç›–ç™¾ç§‘å…¨ä¹¦å’Œå¸¸è¯†çŸ¥è¯†çš„ QA ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•è¡¨ç°ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿æ¨¡å‹ï¼Œç›®å‰ç›¸å…³ä»£ç å’Œæ•°æ®å·²å¼€æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to Association for Computational Linguistics Findings (ACL) 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00519v2",
      "published_date": "2025-05-31 11:35:31 UTC",
      "updated_date": "2025-06-03 09:58:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:58:50.012495+00:00"
    },
    {
      "arxiv_id": "2506.00512v2",
      "title": "Pro3D-Editor : A Progressive-Views Perspective for Consistent and Precise 3D Editing",
      "title_zh": "Pro3D-Editorï¼šé¢å‘ä¸€è‡´ä¸”ç²¾å‡† 3D ç¼–è¾‘çš„æ¸è¿›å¼è§†å›¾è§†è§’",
      "authors": [
        "Yang Zheng",
        "Mengqi Huang",
        "Nan Chen",
        "Zhendong Mao"
      ],
      "abstract": "Text-guided 3D editing aims to precisely edit semantically relevant local 3D regions, which has significant potential for various practical applications ranging from 3D games to film production. Existing methods typically follow a view-indiscriminate paradigm: editing 2D views indiscriminately and projecting them back into 3D space. However, they overlook the different cross-view interdependencies, resulting in inconsistent multi-view editing. In this study, we argue that ideal consistent 3D editing can be achieved through a \\textit{progressive-views paradigm}, which propagates editing semantics from the editing-salient view to other editing-sparse views. Specifically, we propose \\textit{Pro3D-Editor}, a novel framework, which mainly includes Primary-view Sampler, Key-view Render, and Full-view Refiner. Primary-view Sampler dynamically samples and edits the most editing-salient view as the primary view. Key-view Render accurately propagates editing semantics from the primary view to other key views through its Mixture-of-View-Experts Low-Rank Adaption (MoVE-LoRA). Full-view Refiner edits and refines the 3D object based on the edited multi-views. Extensive experiments demonstrate that our method outperforms existing methods in editing accuracy and spatial consistency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬å¼•å¯¼çš„3Dç¼–è¾‘ä¸­å› è§†è§’æ— å·®å¼‚(view-indiscriminate)èŒƒå¼å¯¼è‡´çš„å¤šè§†è§’ä¸ä¸€è‡´é—®é¢˜ï¼Œæå‡ºäº†åä¸ºPro3D-Editorçš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†æ¸è¿›å¼è§†è§’(progressive-views)èŒƒå¼ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯å°†ç¼–è¾‘è¯­ä¹‰ä»ç¼–è¾‘æ˜¾è‘—çš„ä¸»è§†è§’åŠ¨æ€ä¼ æ’­è‡³å…¶ä»–ç¨€ç–è§†è§’ã€‚ç³»ç»Ÿä¸»è¦ç”±Primary-view Samplerã€Key-view Renderä»¥åŠFull-view Refinerä¸‰ä¸ªæ¨¡å—æ„æˆï¼Œå®ç°äº†ä»è§†è§’é‡‡æ ·åˆ°æœ€ç»ˆæ¨¡å‹ç²¾ç»†åŒ–çš„å…¨æµç¨‹å¤„ç†ã€‚å…¶ä¸­ï¼ŒKey-view Renderåˆ©ç”¨Mixture-of-View-Experts Low-Rank Adaption (MoVE-LoRA)æŠ€æœ¯ç¡®ä¿äº†è¯­ä¹‰åœ¨å…³é”®è§†è§’é—´çš„ç²¾å‡†ä¼ é€’ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒPro3D-Editoråœ¨ç¼–è¾‘å‡†ç¡®åº¦å’Œç©ºé—´ä¸€è‡´æ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰ä¸»æµæ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸º3Dæ¸¸æˆå’Œå½±è§†åˆ¶ä½œç­‰å®é™…åº”ç”¨åœºæ™¯ä¸­çš„å¤æ‚3Då±€éƒ¨ç¼–è¾‘æä¾›äº†æ›´å…·é²æ£’æ€§çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00512v2",
      "published_date": "2025-05-31 11:11:55 UTC",
      "updated_date": "2025-06-03 12:03:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:59:02.584551+00:00"
    },
    {
      "arxiv_id": "2506.03191v1",
      "title": "Multimodal Generative AI with Autoregressive LLMs for Human Motion Understanding and Generation: A Way Forward",
      "title_zh": "åŸºäºè‡ªå›å½’å¤§è¯­è¨€æ¨¡å‹çš„äººä½“åŠ¨ä½œç†è§£ä¸ç”Ÿæˆå¤šæ¨¡æ€ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼šæœªæ¥å±•æœ›",
      "authors": [
        "Muhammad Islam",
        "Tao Huang",
        "Euijoon Ahn",
        "Usman Naseem"
      ],
      "abstract": "This paper presents an in-depth survey on the use of multimodal Generative Artificial Intelligence (GenAI) and autoregressive Large Language Models (LLMs) for human motion understanding and generation, offering insights into emerging methods, architectures, and their potential to advance realistic and versatile motion synthesis. Focusing exclusively on text and motion modalities, this research investigates how textual descriptions can guide the generation of complex, human-like motion sequences. The paper explores various generative approaches, including autoregressive models, diffusion models, Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and transformer-based models, by analyzing their strengths and limitations in terms of motion quality, computational efficiency, and adaptability. It highlights recent advances in text-conditioned motion generation, where textual inputs are used to control and refine motion outputs with greater precision. The integration of LLMs further enhances these models by enabling semantic alignment between instructions and motion, improving coherence and contextual relevance. This systematic survey underscores the transformative potential of text-to-motion GenAI and LLM architectures in applications such as healthcare, humanoids, gaming, animation, and assistive technologies, while addressing ongoing challenges in generating efficient and realistic human motion.",
      "tldr_zh": "æœ¬è®ºæ–‡æ·±å…¥ç»¼è¿°äº†å¤šæ¨¡æ€ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Multimodal Generative AI) å’Œè‡ªå›å½’å¤§è¯­è¨€æ¨¡å‹ (Autoregressive LLMs) åœ¨äººä½“è¿åŠ¨ç†è§£ä¸ç”Ÿæˆä¸­çš„åº”ç”¨ã€‚ç ”ç©¶èšç„¦äºæ–‡æœ¬ä¸è¿åŠ¨æ¨¡æ€ï¼Œæ¢è®¨äº†å¦‚ä½•åˆ©ç”¨æ–‡æœ¬æè¿°ç²¾ç¡®å¼•å¯¼ç”Ÿæˆé€¼çœŸä¸”å¤æ‚çš„äººç±»åŠ¨ä½œåºåˆ—ã€‚è¯¥ç»¼è¿°ç³»ç»Ÿåˆ†æäº†è‡ªå›å½’æ¨¡å‹ (Autoregressive models)ã€æ‰©æ•£æ¨¡å‹ (Diffusion models)ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GANs) å’Œ Transformer-based models ç­‰å¤šç§æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ã€‚è®ºæ–‡å¼ºè°ƒäº† LLMs åœ¨å¢å¼ºæŒ‡ä»¤ä¸åŠ¨ä½œé—´è¯­ä¹‰å¯¹é½ (Semantic alignment) æ–¹é¢çš„å…³é”®ä½œç”¨ï¼Œæ˜¾è‘—æå‡äº†è¿åŠ¨ç”Ÿæˆçš„è¿è´¯æ€§å’Œè¯­å¢ƒç›¸å…³æ€§ã€‚ç ”ç©¶è¿˜è¿›ä¸€æ­¥é˜è¿°äº† Text-to-motion æŠ€æœ¯åœ¨åŒ»ç–—å¥åº·ã€äººå½¢æœºå™¨äºº (Humanoids) åŠæ¸¸æˆåŠ¨ç”»ç­‰é¢†åŸŸçš„å·¨å¤§å˜é©æ½œåŠ›ã€‚æœ€åï¼Œæ–‡ç« æ¢è®¨äº†åœ¨å®ç°é«˜æ•ˆã€é€¼çœŸçš„äººä½“è¿åŠ¨åˆæˆè¿‡ç¨‹ä¸­ä»é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä¸ºè¯¥é¢†åŸŸçš„æœªæ¥å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03191v1",
      "published_date": "2025-05-31 11:02:24 UTC",
      "updated_date": "2025-05-31 11:02:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:59:23.591190+00:00"
    },
    {
      "arxiv_id": "2506.00496v1",
      "title": "Monitoring Robustness and Individual Fairness",
      "title_zh": "é²æ£’æ€§ä¸ä¸ªä½“å…¬å¹³æ€§çš„ç›‘æµ‹",
      "authors": [
        "Ashutosh Gupta",
        "Thomas A. Henzinger",
        "Konstantin Kueffner",
        "Kaushik Mallik",
        "David Pape"
      ],
      "abstract": "Input-output robustness appears in various different forms in the literature, such as robustness of AI models to adversarial or semantic perturbations and individual fairness of AI models that make decisions about humans.\n  We propose runtime monitoring of input-output robustness of deployed, black-box AI models, where the goal is to design monitors that would observe one long execution sequence of the model, and would raise an alarm whenever it is detected that two similar inputs from the past led to dissimilar outputs.\n  This way, monitoring will complement existing offline ``robustification'' approaches to increase the trustworthiness of AI decision-makers.\n  We show that the monitoring problem can be cast as the fixed-radius nearest neighbor (FRNN) search problem, which, despite being well-studied, lacks suitable online solutions.\n  We present our tool Clemont, which offers a number of lightweight monitors, some of which use upgraded online variants of existing FRNN algorithms, and one uses a novel algorithm based on binary decision diagrams -- a data-structure commonly used in software and hardware verification.\n  We have also developed an efficient parallelization technique that can substantially cut down the computation time of monitors for which the distance between input-output pairs is measured using the $L_\\infty$ norm.\n  Using standard benchmarks from the literature of adversarial and semantic robustness and individual fairness, we perform a comparative study of different monitors in \\tool, and demonstrate their effectiveness in correctly detecting robustness violations at runtime.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¾“å…¥-è¾“å‡º Robustness å’Œ Individual Fairness åœ¨ AI æ¨¡å‹ä¸­çš„ç›‘æµ‹é—®é¢˜ï¼Œæ—¨åœ¨é€šè¿‡è¿è¡Œæ—¶ç›‘æµ‹å¢å¼ºé»‘ç›’æ¨¡å‹çš„å¯ä¿¡åº¦ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªç›‘æ§æ¡†æ¶ï¼Œé€šè¿‡è§‚å¯Ÿæ¨¡å‹çš„æ‰§è¡Œåºåˆ—ï¼Œåœ¨æ£€æµ‹åˆ°ç›¸ä¼¼çš„å†å²è¾“å…¥äº§ç”Ÿä¸ä¸€è‡´çš„è¾“å‡ºæ—¶è§¦å‘è­¦æŠ¥ã€‚è¯¥ç ”ç©¶å°†ç›‘æµ‹ä»»åŠ¡å»ºæ¨¡ä¸º Fixed-Radius Nearest Neighbor (FRNN) æœç´¢é—®é¢˜ï¼Œå¹¶å¼€å‘äº†åä¸º Clemont çš„å·¥å…·ï¼Œå…¶ä¸­é›†æˆäº†æ”¹è¿›çš„åœ¨çº¿ FRNN ç®—æ³•ä»¥åŠåŸºäº Binary Decision Diagrams (BDD) çš„åˆ›æ–°ç®—æ³•ã€‚ä¸ºäº†æå‡æ•ˆç‡ï¼Œç ”ç©¶è¿˜é’ˆå¯¹é‡‡ç”¨ $L_\\infty$ Norm è¡¡é‡è·ç¦»çš„åœºæ™¯æå‡ºäº†ä¸€ç§å¹¶è¡ŒåŒ–æŠ€æœ¯ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒClemont åœ¨å¯¹æŠ—æ€§é²æ£’æ€§ã€è¯­ä¹‰é²æ£’æ€§å’Œä¸ªä½“å…¬å¹³æ€§ç­‰æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¸”å‡†ç¡®åœ°åœ¨è¿è¡Œæ—¶è¯†åˆ«é²æ£’æ€§è¿è§„è¡Œä¸ºã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00496v1",
      "published_date": "2025-05-31 10:27:54 UTC",
      "updated_date": "2025-05-31 10:27:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:59:11.835063+00:00"
    },
    {
      "arxiv_id": "2506.00494v1",
      "title": "Multi-Objective Neural Network Assisted Design Optimization of Soft Fin-Ray Grippers for Enhanced Grasping Performance",
      "title_zh": "ç¥ç»ç½‘ç»œè¾…åŠ©çš„è½¯ä½“é±¼é³å¼æŠ“æ‰‹å¤šç›®æ ‡è®¾è®¡ä¼˜åŒ–ä¸æŠ“å–æ€§èƒ½æå‡",
      "authors": [
        "Ali Ghanizadeh",
        "Ali Ahmadi",
        "Arash Bahrami"
      ],
      "abstract": "Soft Fin-Ray grippers can perform delicate and careful manipulation, which has caused notable attention in different fields. These grippers can handle objects of various forms and sizes safely. The internal structure of the Fin-Ray finger plays a significant role in its adaptability and grasping performance. However, modeling the non-linear grasp force and deformation behaviors for design purposes is challenging. Moreover, when the Fin-Ray finger becomes more rigid and capable of exerting higher forces, it becomes less delicate in handling objects. The contrast between these two objectives gives rise to a multi-objective optimization problem. In this study, we employ finite element method (FEM) to estimate the deflections and contact forces of the Fin-Ray, grasping cylindrical objects. This dataset is then used to construct a multilayer perception (MLP) for prediction of the contact force and the tip displacement. The FEM dataset consists of three input and four target features. The three input features of the MLP and optimization design variables are the thickness of the front and supporting beams, the thickness of the cross beams, and the equal spacing between the cross beams. In addition, the target features are the maximum contact forces and maximum tip displacements in x- and y-directions. The magnitude of maximum contact force and magnitude of maximum tip displacement are the two objectives, showing the trade-off between force and delicate manipulation in soft Fin-Ray grippers. Furthermore, the optimized set of solutions are found using multi-objective optimal techniques. We use non-dominated sorting genetic algorithm (NSGA-II) method for this purpose. Our findings demonstrate that our methodologies can be used to improve the design and gripping performance of soft robotic grippers, helping us to choose a design not only for delicate grasping but also for high-force applications.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ soft Fin-Ray grippers åœ¨è®¾è®¡è¿‡ç¨‹ä¸­éš¾ä»¥å»ºæ¨¡éçº¿æ€§æŠ“å–åŠ›å’Œå˜å½¢è¡Œä¸ºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¤šç›®æ ‡ç¥ç»ç½‘ç»œè¾…åŠ©ä¼˜åŒ–æ–¹æ¡ˆã€‚ç ”ç©¶é¦–å…ˆé€šè¿‡æœ‰é™å…ƒåˆ†æ (Finite Element Method, FEM) æ¨¡æ‹Ÿæ‰‹æŒ‡æŠ“å–åœ†æŸ±ä½“æ—¶çš„æ¥è§¦åŠ›å’Œä½ç§»ï¼Œå¹¶åŸºäºæ¢çš„åšåº¦ä¸é—´è·ç­‰å‡ ä½•å‚æ•°æ„å»ºæ•°æ®é›†ã€‚éšåï¼Œåˆ©ç”¨å¤šå±‚æ„ŸçŸ¥æœº (Multilayer Perception, MLP) å¯¹æœ€å¤§æ¥è§¦åŠ›å’Œå°–ç«¯ä½ç§»è¿›è¡Œé¢„æµ‹ï¼Œæ­ç¤ºäº†æŠ“å–åŠ›è¾“å‡ºä¸ç²¾ç»†æ“ä½œçµæ•åº¦ä¹‹é—´çš„æƒè¡¡å…³ç³»ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œé‡‡ç”¨éæ”¯é…æ’åºé—ä¼ ç®—æ³• (NSGA-II) å¯»æ‰¾æœ€ä¼˜ç»“æ„å‚æ•°ç»„åˆï¼Œä»¥å¹³è¡¡æŠ“å–å¼ºåº¦ä¸æŸ”æ€§å˜å½¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æ”¹å–„è½¯ä½“æœºå™¨äººçš„æŠ“å–æ€§èƒ½ï¼Œå¸®åŠ©è®¾è®¡è€…æ ¹æ®å…·ä½“éœ€æ±‚åœ¨ç²¾ç»†æŠ“å–ä¸é«˜è´Ÿè½½åº”ç”¨åœºæ™¯ä¹‹é—´é€‰æ‹©æœ€ä½³è®¾è®¡æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00494v1",
      "published_date": "2025-05-31 10:16:58 UTC",
      "updated_date": "2025-05-31 10:16:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:59:05.001693+00:00"
    },
    {
      "arxiv_id": "2506.00486v3",
      "title": "It Takes a Good Model to Train a Good Model: Generalized Gaussian Priors for Optimized LLMs",
      "title_zh": "è‰¯æ¨¡è‚²è‰¯æ¨¡ï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ–çš„å¹¿ä¹‰é«˜æ–¯å…ˆéªŒ",
      "authors": [
        "Jun Wu",
        "Yirong Xiong",
        "Jiangtao Wen",
        "Yuxing Han"
      ],
      "abstract": "Despite rapid advancements in the research and deployment of large language models (LLMs), the statistical distribution of model parameters, as well as their influence on initialization, training dynamics, and downstream efficiency, has received surprisingly little attention. A recent work introduced BackSlash, a training-time compression algorithm. It first demonstrated that pre-trained LLM parameters follow generalized Gaussian distributions (GGDs) better. By optimizing GG priors during training, BackSlash can reduce parameters by up to 90\\% with minimal performance loss. Building on this foundational insight, we propose a unified, end-to-end framework for LLM optimization based on the GG model. Our contributions are threefold: (1) GG-based initialization scheme that aligns with the statistical structure of trained models, resulting in faster convergence and improved accuracy; (2) DeepShape, a post-training regularization method that reshapes weight distributions to match a GG profile, improving compressibility with minimized degradation in performance; and (3) RF8, a compact and hardware-efficient 8-bit floating-point format designed for GG-distributed-initialized BackSlash training, enabling low-cost inference without compromising accuracy. Experiments across diverse model architectures show that our framework consistently yields smaller and faster models that match or outperform standard training baselines. By grounding LLM development in principled statistical modeling, this work forges a new path toward efficient, scalable, and hardware-aware AI systems. The code is available on our project page: https://huggingface.co/spaces/shifeng3711/gg_prior.",
      "tldr_zh": "è¯¥ç ”ç©¶åŸºäºé¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹(LLMs)å‚æ•°éµå¾ªå¹¿ä¹‰é«˜æ–¯åˆ†å¸ƒ(Generalized Gaussian Distributions, GGDs)çš„å‘ç°ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºGGæ¨¡å‹çš„ç»Ÿä¸€ç«¯åˆ°ç«¯LLMä¼˜åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆå¼•å…¥äº†GG-based initializationåˆå§‹åŒ–æ–¹æ¡ˆï¼Œé€šè¿‡å¯¹é½è®­ç»ƒæ¨¡å‹çš„ç»Ÿè®¡ç»“æ„å®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´é«˜çš„å‡†ç¡®æ€§ã€‚å…¶æ¬¡ï¼Œç ”ç©¶æå‡ºäº†DeepShapeåè®­ç»ƒæ­£åˆ™åŒ–æ–¹æ³•ï¼Œé€šè¿‡é‡å¡‘æƒé‡åˆ†å¸ƒä»¥åŒ¹é…GGå‰–é¢ï¼Œåœ¨æœ€å°åŒ–æ€§èƒ½æŸè€—çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å¯å‹ç¼©æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶ä¸“é—¨è®¾è®¡äº†ç¡¬ä»¶é«˜æ•ˆçš„RF8 8æ¯”ç‰¹æµ®ç‚¹æ ¼å¼ï¼Œæ”¯æŒGGåˆ†å¸ƒåˆå§‹åŒ–çš„è®­ç»ƒï¼Œä»è€Œå®ç°äº†ä½æˆæœ¬ä¸”é«˜ç²¾åº¦çš„æ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§æ¨¡å‹æ¶æ„ä¸Šå‡èƒ½äº§å‡ºæ›´å°ã€æ›´å¿«çš„æ¨¡å‹ï¼Œå…¶æ€§èƒ½åŒ¹é…æˆ–ä¼˜äºæ ‡å‡†è®­ç»ƒåŸºçº¿ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡ä¸¥è°¨çš„ç»Ÿè®¡å»ºæ¨¡ï¼Œä¸ºå¼€å‘é«˜æ•ˆã€å¯æ‰©å±•ä¸”ç¡¬ä»¶æ„ŸçŸ¥çš„AIç³»ç»Ÿå¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00486v3",
      "published_date": "2025-05-31 09:49:17 UTC",
      "updated_date": "2025-06-04 08:00:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:59:46.707856+00:00"
    },
    {
      "arxiv_id": "2506.00482v1",
      "title": "BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation",
      "title_zh": "BenchHubï¼šé¢å‘å…¨é¢ä¸”å¯å®šåˆ¶åŒ–å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°çš„ç»Ÿä¸€åŸºå‡†æµ‹è¯•å¥—ä»¶",
      "authors": [
        "Eunsu Kim",
        "Haneul Yoo",
        "Guijin Son",
        "Hitesh Patel",
        "Amit Agarwal",
        "Alice Oh"
      ],
      "abstract": "As large language models (LLMs) continue to advance, the need for up-to-date and well-organized benchmarks becomes increasingly critical. However, many existing datasets are scattered, difficult to manage, and make it challenging to perform evaluations tailored to specific needs or domains, despite the growing importance of domain-specific models in areas such as math or code. In this paper, we introduce BenchHub, a dynamic benchmark repository that empowers researchers and developers to evaluate LLMs more effectively. BenchHub aggregates and automatically classifies benchmark datasets from diverse domains, integrating 303K questions across 38 benchmarks. It is designed to support continuous updates and scalable data management, enabling flexible and customizable evaluation tailored to various domains or use cases. Through extensive experiments with various LLM families, we demonstrate that model performance varies significantly across domain-specific subsets, emphasizing the importance of domain-aware benchmarking. We believe BenchHub can encourage better dataset reuse, more transparent model comparisons, and easier identification of underrepresented areas in existing benchmarks, offering a critical infrastructure for advancing LLM evaluation research.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¨å‡ºäº† BenchHubï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åŸºå‡†æµ‹è¯•é›†åˆ†æ•£ä¸”éš¾ä»¥ç®¡ç†çš„åŠ¨æ€åŸºå‡†æµ‹è¯•ä»“åº“ã€‚è¯¥å¹³å°é€šè¿‡é›†æˆ 38 ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„ 30.3 ä¸‡ä¸ªé—®é¢˜ï¼Œå®ç°äº†å¯¹å¤šé¢†åŸŸæ•°æ®é›†çš„èšåˆä¸è‡ªåŠ¨åˆ†ç±»ã€‚BenchHub æ”¯æŒæŒç»­æ›´æ–°å’Œå¯æ‰©å±•çš„æ•°æ®ç®¡ç†ï¼Œå…è®¸ç ”ç©¶äººå‘˜æ ¹æ®ç‰¹å®šé¢†åŸŸæˆ–åº”ç”¨åœºæ™¯è¿›è¡Œçµæ´»ä¸”å®šåˆ¶åŒ–çš„è¯„ä¼°ã€‚é€šè¿‡å¯¹å¤šç§ LLM ç³»åˆ—çš„å¹¿æ³›å®éªŒï¼Œç ”ç©¶ç»“æœæ˜¾ç¤ºæ¨¡å‹åœ¨ä¸åŒé¢†åŸŸå­é›†ä¸Šçš„è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†é¢†åŸŸæ„ŸçŸ¥ (domain-aware) åŸºå‡†æµ‹è¯•çš„å¿…è¦æ€§ã€‚ä½œä¸º LLM è¯„ä¼°ç ”ç©¶çš„å…³é”®åŸºç¡€è®¾æ–½ï¼ŒBenchHub æœ‰åŠ©äºä¿ƒè¿›æ•°æ®é›†å¤ç”¨ã€æå‡æ¨¡å‹å¯¹æ¯”çš„é€æ˜åº¦ï¼Œå¹¶èƒ½æœ‰æ•ˆè¯†åˆ«ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­çš„è¦†ç›–ç›²ç‚¹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00482v1",
      "published_date": "2025-05-31 09:24:32 UTC",
      "updated_date": "2025-05-31 09:24:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:59:50.478422+00:00"
    },
    {
      "arxiv_id": "2506.00481v2",
      "title": "PVP: An Image Dataset for Personalized Visual Persuasion with Persuasion Strategies, Viewer Characteristics, and Persuasiveness Ratings",
      "title_zh": "PVPï¼šåŒ…å«è¯´æœç­–ç•¥ã€è§‚çœ‹è€…ç‰¹å¾åŠè¯´æœåŠ›è¯„åˆ†çš„ä¸ªæ€§åŒ–è§†è§‰è¯´æœå›¾åƒæ•°æ®é›†",
      "authors": [
        "Junseo Kim",
        "Jongwook Han",
        "Dongmin Choi",
        "Jongwook Yoon",
        "Eun-Ju Lee",
        "Yohan Jo"
      ],
      "abstract": "Visual persuasion, which uses visual elements to influence cognition and behaviors, is crucial in fields such as advertising and political communication. With recent advancements in artificial intelligence, there is growing potential to develop persuasive systems that automatically generate persuasive images tailored to individuals. However, a significant bottleneck in this area is the lack of comprehensive datasets that connect the persuasiveness of images with the personal information about those who evaluated the images. To address this gap and facilitate technological advancements in personalized visual persuasion, we release the Personalized Visual Persuasion (PVP) dataset, comprising 28,454 persuasive images across 596 messages and 9 persuasion strategies. Importantly, the PVP dataset provides persuasiveness scores of images evaluated by 2,521 human annotators, along with their demographic and psychological characteristics (personality traits and values). We demonstrate the utility of our dataset by developing a persuasive image generator and an automated evaluator, and establish benchmark baselines. Our experiments reveal that incorporating psychological characteristics enhances the generation and evaluation of persuasive images, providing valuable insights for personalized visual persuasion.",
      "tldr_zh": "è¯¥ç ”ç©¶å‘å¸ƒäº†PVP (Personalized Visual Persuasion) æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³å½“å‰è§†è§‰åŠè¯´é¢†åŸŸä¸­ç¼ºä¹å›¾åƒè¯´æœåŠ›ä¸å—ä¼—ä¸ªä½“ç‰¹å¾å…³è”æ•°æ®çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†åŒ…å«è·¨è¶Š596æ¡ä¿¡æ¯å’Œ9ç§ persuasion strategies çš„28,454å¼ è¯´æœæ€§å›¾åƒã€‚å…¶æ ¸å¿ƒè´¡çŒ®åœ¨äºæä¾›äº†ç”±2,521åæ ‡æ³¨è€…è¯„ä¼°çš„è¯´æœåŠ›è¯„åˆ†ï¼Œå¹¶è¯¦ç»†è®°å½•äº†æ ‡æ³¨è€…çš„äººå£ç»Ÿè®¡å­¦åŠå¿ƒç†ç‰¹å¾ï¼Œå¦‚ personality traits å’Œ valuesã€‚é€šè¿‡æ„å»ºè¯´æœæ€§å›¾åƒç”Ÿæˆå™¨å’Œè‡ªåŠ¨è¯„ä¼°å™¨ï¼Œç ”ç©¶è€…æˆåŠŸå»ºç«‹äº†åŸºå‡† baselinesã€‚å®éªŒç»“æœè¯æ˜ï¼Œç»“åˆå¿ƒç†ç‰¹å¾èƒ½æœ‰æ•ˆæå‡è¯´æœæ€§å›¾åƒçš„ç”Ÿæˆè´¨é‡ä¸è¯„ä¼°è¡¨ç°ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°ä¸ªæ€§åŒ–ã€è‡ªåŠ¨åŒ–çš„è§†è§‰åŠè¯´ç³»ç»Ÿå¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 Main. Code and dataset are released at: https://github.com/holi-lab/PVP_Personalized_Visual_Persuasion",
      "pdf_url": "https://arxiv.org/pdf/2506.00481v2",
      "published_date": "2025-05-31 09:21:57 UTC",
      "updated_date": "2025-10-28 00:59:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:59:50.998131+00:00"
    },
    {
      "arxiv_id": "2506.02041v1",
      "title": "Enhancing Multimodal Continual Instruction Tuning with BranchLoRA",
      "title_zh": "åˆ©ç”¨ BranchLoRA å¢å¼ºå¤šæ¨¡æ€æŒç»­æŒ‡ä»¤å¾®è°ƒ",
      "authors": [
        "Duzhen Zhang",
        "Yong Ren",
        "Zhong-Zhi Li",
        "Yahan Yu",
        "Jiahua Dong",
        "Chenxing Li",
        "Zhilong Ji",
        "Jinfeng Bai"
      ],
      "abstract": "Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal Large Language Models (MLLMs) to continually align with human intent across sequential tasks. Existing approaches often rely on the Mixture-of-Experts (MoE) LoRA framework to preserve previous instruction alignments. However, these methods are prone to Catastrophic Forgetting (CF), as they aggregate all LoRA blocks via simple summation, which compromises performance over time. In this paper, we identify a critical parameter inefficiency in the MoELoRA framework within the MCIT context. Based on this insight, we propose BranchLoRA, an asymmetric framework to enhance both efficiency and performance. To mitigate CF, we introduce a flexible tuning-freezing mechanism within BranchLoRA, enabling branches to specialize in intra-task knowledge while fostering inter-task collaboration. Moreover, we incrementally incorporate task-specific routers to ensure an optimal branch distribution over time, rather than favoring the most recent task. To streamline inference, we introduce a task selector that automatically routes test inputs to the appropriate router without requiring task identity. Extensive experiments on the latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms MoELoRA and maintains its superiority across various MLLM sizes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€æŒç»­æŒ‡ä»¤å¾®è°ƒ(Multimodal Continual Instruction Tuning, MCIT)ä¸­å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)é¢ä¸´çš„ç¾éš¾æ€§é—å¿˜(Catastrophic Forgetting)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºBranchLoRAçš„éå¯¹ç§°æ¡†æ¶ã€‚ç ”ç©¶è€…æŒ‡å‡ºï¼Œç°æœ‰çš„MoELoRAæ¡†æ¶ç”±äºå¯¹LoRAå—è¿›è¡Œç®€å•åŠ å’Œèšåˆï¼Œå¯¼è‡´äº†å‚æ•°æ•ˆç‡ä½ä¸‹å’Œæ€§èƒ½éšæ—¶é—´è¡°å‡ã€‚BranchLoRAå¼•å…¥äº†çµæ´»çš„å¾®è°ƒ-å†»ç»“æœºåˆ¶(tuning-freezing mechanism)ï¼Œä½¿å„åˆ†æ”¯èƒ½å¤Ÿåœ¨ä¸“æ³¨äºä»»åŠ¡å†…çŸ¥è¯†çš„åŒæ—¶ä¿ƒè¿›è·¨ä»»åŠ¡åä½œã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¢é‡å¼æ·»åŠ ä»»åŠ¡ç‰¹å®šè·¯ç”±å™¨(task-specific routers)ï¼Œæœ‰æ•ˆè§£å†³äº†æ¨¡å‹åå‘æœ€æ–°ä»»åŠ¡çš„é—®é¢˜ï¼Œç¡®ä¿äº†åˆ†æ”¯åˆ†å¸ƒçš„æœ€ä¼˜æ€§ã€‚ä¸ºäº†ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†ä¸€ä¸ªä»»åŠ¡é€‰æ‹©å™¨(task selector)ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€ä»»åŠ¡æ ‡è¯†(task identity)çš„æƒ…å†µä¸‹è‡ªåŠ¨è·¯ç”±æµ‹è¯•è¾“å…¥ã€‚åœ¨æœ€æ–°MCITåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒBranchLoRAåœ¨å„ç§è§„æ¨¡çš„MLLMä¸Šå‡æ˜¾è‘—ä¼˜äºMoELoRAï¼Œå±•ç°äº†å“è¶Šçš„æ€§èƒ½å’Œç¨³å®šæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ACL2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2506.02041v1",
      "published_date": "2025-05-31 09:02:38 UTC",
      "updated_date": "2025-05-31 09:02:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:00:03.605373+00:00"
    },
    {
      "arxiv_id": "2506.00467v1",
      "title": "SST: Self-training with Self-adaptive Thresholding for Semi-supervised Learning",
      "title_zh": "SSTï¼šåŸºäºè‡ªé€‚åº”é˜ˆå€¼è‡ªè®­ç»ƒçš„åŠç›‘ç£å­¦ä¹ ",
      "authors": [
        "Shuai Zhao",
        "Heyan Huang",
        "Xinge Li",
        "Xiaokang Chen",
        "Rui Wang"
      ],
      "abstract": "Neural networks have demonstrated exceptional performance in supervised learning, benefiting from abundant high-quality annotated data. However, obtaining such data in real-world scenarios is costly and labor-intensive. Semi-supervised learning (SSL) offers a solution to this problem. Recent studies, such as Semi-ViT and Noisy Student, which employ consistency regularization or pseudo-labeling, have demonstrated significant achievements. However, they still face challenges, particularly in accurately selecting sufficient high-quality pseudo-labels due to their reliance on fixed thresholds. Recent methods such as FlexMatch and FreeMatch have introduced flexible or self-adaptive thresholding techniques, greatly advancing SSL research. Nonetheless, their process of updating thresholds at each iteration is deemed time-consuming, computationally intensive, and potentially unnecessary. To address these issues, we propose Self-training with Self-adaptive Thresholding (SST), a novel, effective, and efficient SSL framework. SST introduces an innovative Self-Adaptive Thresholding (SAT) mechanism that adaptively adjusts class-specific thresholds based on the model's learning progress. SAT ensures the selection of high-quality pseudo-labeled data, mitigating the risks of inaccurate pseudo-labels and confirmation bias. Extensive experiments demonstrate that SST achieves state-of-the-art performance with remarkable efficiency, generalization, and scalability across various architectures and datasets. Semi-SST-ViT-Huge achieves the best results on competitive ImageNet-1K SSL benchmarks, with 80.7% / 84.9% Top-1 accuracy using only 1% / 10% labeled data. Compared to the fully-supervised DeiT-III-ViT-Huge, which achieves 84.8% Top-1 accuracy using 100% labeled data, our method demonstrates superior performance using only 10% labeled data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SSTï¼ˆSelf-training with Self-adaptive Thresholdingï¼‰ï¼Œä¸€ä¸ªé’ˆå¯¹åŠç›‘ç£å­¦ä¹ ï¼ˆSemi-supervised Learningï¼‰çš„æ–°é¢–ä¸”é«˜æ•ˆçš„æ¡†æ¶ã€‚é’ˆå¯¹ FlexMatch å’Œ FreeMatch ç­‰ç°æœ‰æ–¹æ³•åœ¨é˜ˆå€¼æ›´æ–°æ—¶è®¡ç®—æˆæœ¬é«˜ã€æ•ˆç‡ä½çš„é—®é¢˜ï¼ŒSST å¼•å…¥äº†è‡ªé€‚åº”é˜ˆå€¼æœºåˆ¶ SATï¼ˆSelf-Adaptive Thresholdingï¼‰ï¼Œæ ¹æ®æ¨¡å‹çš„å­¦ä¹ è¿›åº¦åŠ¨æ€è°ƒæ•´ç±»åˆ«ç‰¹å®šçš„é˜ˆå€¼ã€‚è¯¥æœºåˆ¶èƒ½å¤Ÿç¡®ä¿é€‰æ‹©é«˜è´¨é‡çš„ä¼ªæ ‡ç­¾ï¼ˆpseudo-labelsï¼‰ï¼Œä»è€Œæœ‰æ•ˆé™ä½ä¸å‡†ç¡®æ ‡ç­¾å¸¦æ¥çš„é£é™©å¹¶ç¼“è§£ç¡®è®¤åå·®ï¼ˆconfirmation biasï¼‰ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSST åœ¨å„ç§æ¶æ„å’Œæ•°æ®é›†ä¸Šå‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·æœ‰æä½³çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨ ImageNet-1K åŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŸºäº SST çš„ ViT-Huge æ¨¡å‹ä»…åˆ©ç”¨ 10% çš„æ ‡æ³¨æ•°æ®ä¾¿è¾¾åˆ°äº† 84.9% çš„ Top-1 å‡†ç¡®ç‡ã€‚è¿™ä¸€æˆç»©ç”šè‡³è¶…è¶Šäº†ä½¿ç”¨ 100% æ ‡æ³¨æ•°æ®çš„å…¨ç›‘ç£ DeiT-III-ViT-Huge æ¨¡å‹ï¼Œå±•ç°äº† SST åœ¨æ ‡æ³¨æ•°æ®å—é™åœºæ™¯ä¸‹çš„å“è¶Šè¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by Information Processing & Management (IP&M)",
      "pdf_url": "https://arxiv.org/pdf/2506.00467v1",
      "published_date": "2025-05-31 08:34:04 UTC",
      "updated_date": "2025-05-31 08:34:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:59:53.700026+00:00"
    },
    {
      "arxiv_id": "2506.00462v2",
      "title": "XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark",
      "title_zh": "XMAD-Benchï¼šè·¨åŸŸå¤šè¯­è¨€éŸ³é¢‘æ·±åº¦ä¼ªé€ åŸºå‡†",
      "authors": [
        "Ioan-Paul Ciobanu",
        "Andrei-Iulian Hiji",
        "Nicolae-Catalin Ristea",
        "Paul Irofti",
        "Cristian Rusu",
        "Radu Tudor Ionescu"
      ],
      "abstract": "Recent advances in audio generation led to an increasing number of deepfakes, making the general public more vulnerable to financial scams, identity theft, and misinformation. Audio deepfake detectors promise to alleviate this issue, with many recent studies reporting accuracy rates close to 99%. However, these methods are typically tested in an in-domain setup, where the deepfake samples from the training and test sets are produced by the same generative models. To this end, we introduce XMAD-Bench, a large-scale cross-domain multilingual audio deepfake benchmark comprising 668.8 hours of real and deepfake speech. In our novel dataset, the speakers, the generative methods, and the real audio sources are distinct across training and test splits. This leads to a challenging cross-domain evaluation setup, where audio deepfake detectors can be tested \"in the wild\". Our in-domain and cross-domain experiments indicate a clear disparity between the in-domain performance of deepfake detectors, which is usually as high as 100%, and the cross-domain performance of the same models, which is sometimes similar to random chance. Our benchmark highlights the need for the development of robust audio deepfake detectors, which maintain their generalization capacity across different languages, speakers, generative methods, and data sources. Our benchmark is publicly released at https://github.com/ristea/xmad-bench/.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†XMAD-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡è·¨é¢†åŸŸå¤šè¯­è¨€éŸ³é¢‘ä¼ªé€ (Audio Deepfake)åŸºå‡†æµ‹è¯•é›†ï¼Œæ¶µç›–äº†668.8å°æ—¶çš„çœŸå®å’Œä¼ªé€ è¯­éŸ³æ•°æ®ã€‚ä¸ºäº†æ„å»ºä¸¥è‹›çš„â€œé‡å¤–â€æµ‹è¯•ç¯å¢ƒï¼Œè¯¥åŸºå‡†åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¹‹é—´é‡‡ç”¨äº†å®Œå…¨ä¸åŒçš„è¯´è¯äººã€ç”Ÿæˆæ–¹æ³•å’ŒéŸ³é¢‘æ¥æºï¼Œå½¢æˆäº†æå…·æŒ‘æˆ˜æ€§çš„Cross-Domainè¯„ä¼°è®¾ç½®ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡ç°æœ‰æ£€æµ‹å™¨åœ¨åŸŸå†…(In-Domain)åœºæ™¯ä¸‹çš„å‡†ç¡®ç‡æ¥è¿‘100%ï¼Œä½†åœ¨è·¨é¢†åŸŸè¯„ä¼°ä¸­æ€§èƒ½ä¼šå‰§çƒˆä¸‹é™ï¼Œéƒ¨åˆ†æ¨¡å‹è¡¨ç°ç”šè‡³ä¸éšæœºçŒœæµ‹æ— å¼‚ã€‚XMAD-Benchçš„å®éªŒç»“æœæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¸¥é‡ä¸è¶³ï¼Œå¼ºè°ƒäº†å¼€å‘èƒ½å¤Ÿè·¨è¯­è¨€ã€è·¨è¯´è¯äººå’Œè·¨ç”Ÿæˆæ–¹æ³•å·¥ä½œçš„é²æ£’æ€§æ£€æµ‹å™¨çš„å¿…è¦æ€§ã€‚è¯¥åŸºå‡†ç›®å‰å·²å…¬å¼€å‘å¸ƒï¼Œæ—¨åœ¨æ¨åŠ¨æ›´å…·å®ç”¨ä»·å€¼çš„éŸ³é¢‘é‰´ä¼ªæŠ€æœ¯çš„å‘å±•ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted at EACL 2026",
      "pdf_url": "https://arxiv.org/pdf/2506.00462v2",
      "published_date": "2025-05-31 08:28:36 UTC",
      "updated_date": "2026-01-18 13:54:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:00:09.051074+00:00"
    },
    {
      "arxiv_id": "2506.00459v1",
      "title": "Comparing Traditional and Reinforcement-Learning Methods for Energy Storage Control",
      "title_zh": "å‚¨èƒ½æ§åˆ¶ä¸­ä¼ ç»Ÿæ–¹æ³•ä¸å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„æ¯”è¾ƒ",
      "authors": [
        "Elinor Ginzburg",
        "Itay Segev",
        "Yoash Levron",
        "Sarah Keren"
      ],
      "abstract": "We aim to better understand the tradeoffs between traditional and reinforcement learning (RL) approaches for energy storage management. More specifically, we wish to better understand the performance loss incurred when using a generative RL policy instead of using a traditional approach to find optimal control policies for specific instances. Our comparison is based on a simplified micro-grid model, that includes a load component, a photovoltaic source, and a storage device. Based on this model, we examine three use cases of increasing complexity: ideal storage with convex cost functions, lossy storage devices, and lossy storage devices with convex transmission losses. With the aim of promoting the principled use RL based methods in this challenging and important domain, we provide a detailed formulation of each use case and a detailed description of the optimization challenges. We then compare the performance of traditional and RL methods, discuss settings in which it is beneficial to use each method, and suggest avenues for future investigation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨æ·±å…¥æ¢è®¨èƒ½æºå­˜å‚¨ç®¡ç†ä¸­ä¼ ç»Ÿæ–¹æ³•ä¸å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ–¹æ³•çš„æƒè¡¡ï¼Œé‡ç‚¹åˆ†æäº†ç”Ÿæˆå¼ RL ç­–ç•¥ç›¸è¾ƒäºä¼ ç»Ÿæœ€ä¼˜æ§åˆ¶æ–¹æ³•çš„æ€§èƒ½æŸå¤±ã€‚å®éªŒåŸºäºåŒ…å«è´Ÿè½½ã€å…‰ä¼(photovoltaic)æºå’Œå­˜å‚¨è®¾å¤‡çš„ç®€åŒ–å¾®ç½‘(micro-grid)æ¨¡å‹ï¼Œè€ƒå¯Ÿäº†ç†æƒ³å­˜å‚¨ã€æœ‰æŸå­˜å‚¨ä»¥åŠå¸¦ä¼ è¾“æŸè€—çš„æœ‰æŸå­˜å‚¨ä¸‰ç§å¤æ‚åº¦é€’å¢çš„åœºæ™¯ã€‚ç ”ç©¶è¯¦ç»†åˆ¶å®šäº†å„åœºæ™¯çš„æ•°å­¦æ¨¡å‹å¹¶æè¿°äº†å…¶é¢ä¸´çš„ä¼˜åŒ–æŒ‘æˆ˜ï¼Œé€šè¿‡å®éªŒå¯¹æ¯”äº†ä¼ ç»Ÿæ–¹æ³•ä¸ RL æ–¹æ³•çš„å®é™…è¡¨ç°ã€‚åŸºäºä¸åŒè®¾å®šä¸‹çš„ç»“æœåˆ†æï¼Œè¯¥ç ”ç©¶è®¨è®ºäº†ä¸¤ç§æ–¹æ³•çš„é€‚ç”¨ç¯å¢ƒåŠä¼˜åŠ£ï¼Œä¸ºåœ¨è¯¥é¢†åŸŸåŸåˆ™æ€§åœ°åº”ç”¨ RL æ–¹æ³•æä¾›äº†æŒ‡å¯¼ã€‚æœ€åï¼Œæ–‡ç« æ€»ç»“äº†ç ”ç©¶å‘ç°å¹¶ä¸ºèƒ½æºå­˜å‚¨æ§åˆ¶çš„æœªæ¥ç ”ç©¶æ–¹å‘æå‡ºäº†å»ºè®®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00459v1",
      "published_date": "2025-05-31 08:25:21 UTC",
      "updated_date": "2025-05-31 08:25:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:00:06.336209+00:00"
    },
    {
      "arxiv_id": "2506.00458v1",
      "title": "Reinforcement Learning for Hanabi",
      "title_zh": "é¢å‘ Hanabi çš„å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Nina Cohen",
        "Kordel K. France"
      ],
      "abstract": "Hanabi has become a popular game for research when it comes to reinforcement learning (RL) as it is one of the few cooperative card games where you have incomplete knowledge of the entire environment, thus presenting a challenge for a RL agent. We explored different tabular and deep reinforcement learning algorithms to see which had the best performance both against an agent of the same type and also against other types of agents. We establish that certain agents played their highest scoring games against specific agents while others exhibited higher scores on average by adapting to the opposing agent's behavior. We attempted to quantify the conditions under which each algorithm provides the best advantage and identified the most interesting interactions between agents of different types. In the end, we found that temporal difference (TD) algorithms had better overall performance and balancing of play types compared to tabular agents. Specifically, tabular Expected SARSA and deep Q-Learning agents showed the best performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Reinforcement Learning (RL) ç®—æ³•åœ¨åˆä½œç±»å¡ç‰Œæ¸¸æˆ Hanabi ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³è¯¥æ¸¸æˆåœ¨ä¸å®Œå…¨ä¿¡æ¯ç¯å¢ƒä¸‹çš„åä½œæŒ‘æˆ˜ã€‚ä½œè€…å¯¹æ¯”è¯„ä¼°äº†å¤šç§ Tabular å’Œ Deep Reinforcement Learning ç®—æ³•åœ¨åŒç±»åŠå¼‚ç±»æ™ºèƒ½ä½“åšå¼ˆä¸­çš„æ€§èƒ½å·®å¼‚ã€‚ç ”ç©¶é‡åŒ–äº†ä¸åŒç®—æ³•å‘æŒ¥ä¼˜åŠ¿çš„æ¡ä»¶ï¼Œå¹¶å‘ç°éƒ¨åˆ†æ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡é€‚åº”å¯¹æ‰‹è¡Œä¸ºæ¥è·å¾—æ›´é«˜çš„å¹³å‡å¾—åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTemporal Difference (TD) ç®—æ³•åœ¨æ•´ä½“æ€§èƒ½å’Œç­–ç•¥å¹³è¡¡æ–¹é¢æ˜¾è‘—ä¼˜äº Tabular æ™ºèƒ½ä½“ã€‚å…·ä½“è€Œè¨€ï¼ŒTabular Expected SARSA å’Œ Deep Q-Learning æ™ºèƒ½ä½“åœ¨å®éªŒä¸­å±•ç°å‡ºäº†æœ€ä¼˜çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00458v1",
      "published_date": "2025-05-31 08:24:16 UTC",
      "updated_date": "2025-05-31 08:24:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:00:26.258197+00:00"
    },
    {
      "arxiv_id": "2506.00455v4",
      "title": "Diffusion Graph Neural Networks and Dataset for Robust Olfactory Navigation in Hazard Robotics",
      "title_zh": "é¢å‘å±é™©ç¯å¢ƒæœºå™¨äººé²æ£’å—…è§‰å¯¼èˆªçš„æ‰©æ•£å›¾ç¥ç»ç½‘ç»œä¸æ•°æ®é›†",
      "authors": [
        "Kordel K. France",
        "Ovidiu Daescu"
      ],
      "abstract": "Navigation by scent is a capability in robotic systems that is rising in demand. However, current methods often suffer from ambiguities, particularly when robots misattribute odours to incorrect objects due to limitations in olfactory datasets and sensor resolutions. To address challenges in olfactory navigation, we introduce a multimodal olfaction dataset along with a novel machine learning method using diffusion-based molecular generation that can be used by itself or with automated olfactory dataset construction pipelines. This generative process of our diffusion model expands the chemical space beyond the limitations of both current olfactory datasets and training methods, enabling the identification of potential odourant molecules not previously documented. The generated molecules can then be more accurately validated using advanced olfactory sensors, enabling them to detect more compounds and inform better hardware design. By integrating visual analysis, language processing, and molecular generation, our framework enhances the ability of olfaction-vision models on robots to accurately associate odours with their correct sources, thereby improving navigation and decision-making through better sensor selection for a target compound in critical applications such as explosives detection, narcotics screening, and search and rescue. Our methodology represents a foundational advancement in the field of artificial olfaction, offering a scalable solution to challenges posed by limited olfactory data and sensor ambiguities. Code, models, and data are made available to the community at: https://huggingface.co/datasets/kordelfrance/olfaction-vision-language-dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººæ°”å‘³å¯¼èˆªä¸­å› å—…è§‰æ•°æ®é›†å’Œä¼ æ„Ÿå™¨åˆ†è¾¨ç‡é™åˆ¶å¯¼è‡´çš„æ°”å‘³æºè¯¯åˆ¤é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ä¸ªå¤šæ¨¡æ€å—…è§‰æ•°æ®é›†(multimodal olfaction dataset)å¹¶æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹(diffusion-based molecular generation)çš„æ–°å‹æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç”Ÿæˆè¿‡ç¨‹æ‰©å±•äº†ç°æœ‰æ•°æ®é›†ä¹‹å¤–çš„åŒ–å­¦ç©ºé—´ï¼Œèƒ½å¤Ÿè¯†åˆ«å°šæœªè®°å½•çš„æ½œåœ¨æ°”å‘³åˆ†å­ï¼Œä¸ºä¼˜åŒ–ä¼ æ„Ÿå™¨é€‰æ‹©å’Œç¡¬ä»¶è®¾è®¡æä¾›äº†æ•°æ®æ”¯æŒã€‚é€šè¿‡æ•´åˆè§†è§‰åˆ†æã€è¯­è¨€å¤„ç†ä¸åˆ†å­ç”ŸæˆæŠ€æœ¯ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—å¢å¼ºäº†æœºå™¨äººå—…è§‰-è§†è§‰æ¨¡å‹(olfaction-vision models)å°†æ°”å‘³ä¸æ­£ç¡®æ¥æºç›¸å…³è”çš„èƒ½åŠ›ï¼Œä»è€Œæå‡äº†å¯¼èˆªä¸å†³ç­–çš„å‡†ç¡®æ€§ã€‚è¿™ä¸€ç ”ç©¶åœ¨çˆ†ç‚¸ç‰©æ¢æµ‹ã€æ¯’å“ç­›æŸ¥åŠæœç´¢æ•‘æ´ç­‰å…³é”®é¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨æ½œåŠ›ï¼Œä¸ºå…‹æœå—…è§‰æ•°æ®åŒ®ä¹å’Œä¼ æ„Ÿå™¨æ­§ä¹‰æä¾›äº†åŸºç¡€æ€§çš„æ‰©å±•æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆä¸ä»…æé«˜äº†å¤æ‚ç¯å¢ƒä¸‹æœºå™¨äººè¯†åˆ«ç‰¹å®šåŒ–åˆç‰©çš„å¯é æ€§ï¼Œä¹Ÿä¸ºäººå·¥æ™ºèƒ½å—…è§‰é¢†åŸŸå¥ å®šäº†å¯æ‰©å±•çš„æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.00455v4",
      "published_date": "2025-05-31 08:22:09 UTC",
      "updated_date": "2025-09-21 03:08:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:00:28.079155+00:00"
    },
    {
      "arxiv_id": "2506.11064v1",
      "title": "PMF-CEC: Phoneme-augmented Multimodal Fusion for Context-aware ASR Error Correction with Error-specific Selective Decoding",
      "title_zh": "PMF-CECï¼šåŸºäºéŸ³ç´ å¢å¼ºå¤šæ¨¡æ€èåˆä¸é”™è¯¯ç‰¹å¼‚æ€§é€‰æ‹©æ€§è§£ç çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ ASR çº é”™",
      "authors": [
        "Jiajun He",
        "Tomoki Toda"
      ],
      "abstract": "End-to-end automatic speech recognition (ASR) models often struggle to accurately recognize rare words. Previously, we introduced an ASR postprocessing method called error detection and context-aware error correction (ED-CEC), which leverages contextual information such as named entities and technical terms to improve the accuracy of ASR transcripts. Although ED-CEC achieves a notable success in correcting rare words, its accuracy remains low when dealing with rare words that have similar pronunciations but different spellings. To address this issue, we proposed a phoneme-augmented multimodal fusion method for context-aware error correction (PMF-CEC) method on the basis of ED-CEC, which allowed for better differentiation between target rare words and homophones. Additionally, we observed that the previous ASR error detection module suffers from overdetection. To mitigate this, we introduced a retention probability mechanism to filter out editing operations with confidence scores below a set threshold, preserving the original operation to improve error detection accuracy. Experiments conducted on five datasets demonstrated that our proposed PMF-CEC maintains reasonable inference speed while further reducing the biased word error rate compared with ED-CEC, showing a stronger advantage in correcting homophones. Moreover, our method outperforms other contextual biasing methods, and remains valuable compared with LLM-based methods in terms of faster inference and better robustness under large biasing lists.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PMF-CECï¼Œä¸€ç§åŸºäº Phoneme-augmented Multimodal Fusion çš„ Context-aware ASR Error Correction æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç«¯åˆ°ç«¯ ASR æ¨¡å‹åœ¨å¤„ç†åŒéŸ³å¼‚å½¢ç¨€æœ‰è¯æ—¶å‡†ç¡®ç‡ä½çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åœ¨ ED-CEC çš„åŸºç¡€ä¸Šå¼•å…¥éŸ³ç´ ä¿¡æ¯ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åŒºåˆ†ç›®æ ‡ç¨€æœ‰è¯ä¸å…¶åŒéŸ³å¼‚å½¢è¯çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³ ASR é”™è¯¯æ£€æµ‹ä¸­çš„è¿‡åº¦æ£€æµ‹é—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†ä¿ç•™æ¦‚ç‡æœºåˆ¶ï¼Œé€šè¿‡ç½®ä¿¡åº¦é˜ˆå€¼è¿‡æ»¤ä½ç½®ä¿¡åº¦çš„ç¼–è¾‘æ“ä½œä»¥ä¿ç•™åŸå§‹ç»“æœã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPMF-CEC åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—é™ä½äº†åç½®è¯é”™è¯¯ç‡ï¼ˆBWERï¼‰ï¼Œåœ¨çº æ­£åŒéŸ³è¯æ–¹é¢å…·æœ‰æå¼ºçš„ä¼˜åŠ¿ã€‚ä¸åŸºäº LLM çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨é¢å¯¹å¤§è§„æ¨¡åç½®åˆ—è¡¨æ—¶å…·æœ‰æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´å¥½çš„é²æ£’æ€§ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­çš„é«˜æ•ˆæ€§ä¸å¯é æ€§ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted by IEEE TASLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.11064v1",
      "published_date": "2025-05-31 08:18:34 UTC",
      "updated_date": "2025-05-31 08:18:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:00:36.025520+00:00"
    },
    {
      "arxiv_id": "2506.00453v1",
      "title": "TMetaNet: Topological Meta-Learning Framework for Dynamic Link Prediction",
      "title_zh": "TMetaNetï¼šé¢å‘åŠ¨æ€é“¾è·¯é¢„æµ‹çš„æ‹“æ‰‘å…ƒå­¦ä¹ æ¡†æ¶",
      "authors": [
        "Hao Li",
        "Hao Wan",
        "Yuzhou Chen",
        "Dongsheng Ye",
        "Yulia Gel",
        "Hao Jiang"
      ],
      "abstract": "Dynamic graphs evolve continuously, presenting challenges for traditional graph learning due to their changing structures and temporal dependencies. Recent advancements have shown potential in addressing these challenges by developing suitable meta-learning-based dynamic graph neural network models. However, most meta-learning approaches for dynamic graphs rely on fixed weight update parameters, neglecting the essential intrinsic complex high-order topological information of dynamically evolving graphs. We have designed Dowker Zigzag Persistence (DZP), an efficient and stable dynamic graph persistent homology representation method based on Dowker complex and zigzag persistence, to capture the high-order features of dynamic graphs. Armed with the DZP ideas, we propose TMetaNet, a new meta-learning parameter update model based on dynamic topological features. By utilizing the distances between high-order topological features, TMetaNet enables more effective adaptation across snapshots. Experiments on real-world datasets demonstrate TMetaNet's state-of-the-art performance and resilience to graph noise, illustrating its high potential for meta-learning and dynamic graph analysis. Our code is available at https://github.com/Lihaogx/TMetaNet.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŠ¨æ€å›¾åœ¨æ¼”åŒ–è¿‡ç¨‹ä¸­ä¼ ç»Ÿå­¦ä¹ æ–¹æ³•éš¾ä»¥æ•æ‰å¤æ‚é«˜é˜¶æ‹“æ‰‘ä¿¡æ¯çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºTMetaNetçš„æ–°å‹æ‹“æ‰‘å…ƒå­¦ä¹ æ¡†æ¶ã€‚ç ”ç©¶è€…é¦–å…ˆè®¾è®¡äº†Dowker Zigzag Persistence (DZP)ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºDowkerå¤å½¢ï¼ˆDowker complexï¼‰å’ŒZigzagæŒä¹…åŒ–ï¼ˆzigzag persistenceï¼‰çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿé«˜æ•ˆç¨³å®šåœ°æå–åŠ¨æ€å›¾çš„é«˜é˜¶æ‹“æ‰‘ç‰¹å¾ã€‚TMetaNetåˆ©ç”¨è¿™äº›ç‰¹å¾é—´çš„æ‹“æ‰‘è·ç¦»æ¥å¼•å¯¼å…ƒå­¦ä¹ ï¼ˆMeta-Learningï¼‰è¿‡ç¨‹ä¸­çš„å‚æ•°æ›´æ–°ï¼Œä»è€Œå¢å¼ºæ¨¡å‹åœ¨ä¸åŒæ—¶é—´å¿«ç…§ï¼ˆsnapshotsï¼‰ä¹‹é—´çš„é€‚åº”èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTMetaNetåœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†çš„åŠ¨æ€é“¾æ¥é¢„æµ‹ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›ï¼ˆState-of-the-Artï¼‰çš„æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºä¼˜å¼‚çš„æŠ—å™ªèƒ½åŠ›ã€‚è¯¥æ¡†æ¶è¯æ˜äº†ç»“åˆä»£æ•°æ‹“æ‰‘ä¸å…ƒå­¦ä¹ åœ¨åŠ¨æ€å›¾åˆ†æé¢†åŸŸå…·æœ‰å·¨å¤§çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00453v1",
      "published_date": "2025-05-31 08:15:23 UTC",
      "updated_date": "2025-05-31 08:15:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:01:03.063975+00:00"
    },
    {
      "arxiv_id": "2506.00452v3",
      "title": "Attention-Aided MMSE for OFDM Channel Estimation: Learning Linear Filters with Attention",
      "title_zh": "é¢å‘ OFDM ä¿¡é“ä¼°è®¡çš„æ³¨æ„åŠ›è¾…åŠ© MMSEï¼šåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ çº¿æ€§æ»¤æ³¢å™¨",
      "authors": [
        "TaeJun Ha",
        "Chaehyun Jung",
        "Hyeonuk Kim",
        "Jeongwoo Park",
        "Jeonghun Park"
      ],
      "abstract": "In orthogonal frequency division multiplexing (OFDM), accurate channel estimation is crucial. Classical signal processing based approaches, such as minimum mean-squared error (MMSE) estimation, often require second-order statistics that are difficult to obtain in practice. Recent deep neural networks based methods have been introduced to address this; yet they often suffer from high inference complexity. This paper proposes an Attention-aided MMSE (A-MMSE), a novel model-based DNN framework that learns the optimal MMSE filter via the Attention Transformer. Once trained, the A-MMSE estimates the channel through a single linear operation for channel estimation, eliminating nonlinear activations during inference and thus reducing computational complexity. To enhance the learning efficiency of the A-MMSE, we develop a two-stage Attention encoder, designed to effectively capture the channel correlation structure. Additionally, a rank-adaptive extension of the proposed A-MMSE allows flexible trade-offs between complexity and channel estimation accuracy. Extensive simulations with 3GPP TDL channel models demonstrate that the proposed A-MMSE consistently outperforms other baseline methods in terms of normalized MSE across a wide range of signal-to-noise ratio (SNR) conditions. In particular, the A-MMSE and its rank-adaptive extension establish a new frontier in the performance-complexity trade-off, providing a powerful yet highly efficient solution for practical channel estimation",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ­£äº¤é¢‘åˆ†å¤ç”¨(OFDM)ç³»ç»Ÿä¸­çš„ä¿¡é“ä¼°è®¡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºAttention-aided MMSE (A-MMSE)çš„æ–°å‹æ¨¡å‹é©±åŠ¨æ·±åº¦ç¥ç»ç½‘ç»œæ¡†æ¶ã€‚ä¼ ç»Ÿçš„æœ€å°å‡æ–¹è¯¯å·®(MMSE)ä¼°è®¡ä¾èµ–äºéš¾ä»¥è·å–çš„äºŒé˜¶ç»Ÿè®¡é‡ï¼Œè€Œç°æœ‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•å¾€å¾€é¢ä¸´æ¨ç†å¤æ‚åº¦è¿‡é«˜çš„é—®é¢˜ã€‚A-MMSEåˆ©ç”¨Attention Transformerå­¦ä¹ æœ€ä¼˜çš„MMSEæ»¤æ³¢å™¨ï¼Œä½¿å¾—è®­ç»ƒåçš„æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µä»…é€šè¿‡å•æ¬¡çº¿æ€§è¿ç®—å³å¯å®Œæˆä¼°è®¡ï¼Œå®Œå…¨æ¶ˆé™¤äº†éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œä»è€Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚ä¸ºäº†æé«˜å­¦ä¹ æ•ˆç‡ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„Attention encoderæ¥æœ‰æ•ˆæ•æ‰ä¿¡é“çš„ç›¸å…³æ€§ç»“æ„ï¼Œå¹¶æå‡ºäº†rank-adaptiveæ‰©å±•ä»¥å®ç°å¤æ‚åº¦å’Œç²¾åº¦ä¹‹é—´çš„çµæ´»æƒè¡¡ã€‚åŸºäº3GPP TDLä¿¡é“æ¨¡å‹çš„ä»¿çœŸå®éªŒè¡¨æ˜ï¼ŒA-MMSEåœ¨å„ç§ä¿¡å™ªæ¯”(SNR)æ¡ä»¶ä¸‹ï¼Œå…¶å½’ä¸€åŒ–å‡æ–¹è¯¯å·®(normalized MSE)æ€§èƒ½å‡ä¸€è‡´ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚è¯¥æ–¹æ¡ˆåœ¨æ€§èƒ½ä¸å¤æ‚åº¦çš„å¹³è¡¡ä¸Šå–å¾—äº†é‡è¦è¿›å±•ï¼Œä¸ºå®é™…é€šä¿¡åœºæ™¯ä¸­é«˜æ•ˆä¸”ç²¾ç¡®çš„ä¿¡é“ä¼°è®¡æä¾›äº†å¼ºæœ‰åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "eess.SP",
      "comment": "16 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.00452v3",
      "published_date": "2025-05-31 08:12:04 UTC",
      "updated_date": "2025-12-01 02:25:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:00:54.209240+00:00"
    },
    {
      "arxiv_id": "2506.02039v1",
      "title": "No Audiogram: Leveraging Existing Scores for Personalized Speech Intelligibility Prediction",
      "title_zh": "æ— éœ€å¬åŠ›å›¾ï¼šåˆ©ç”¨æ—¢æœ‰è¯„åˆ†å®ç°ä¸ªæ€§åŒ–è¯­éŸ³å¯æ‡‚åº¦é¢„æµ‹",
      "authors": [
        "Haoshuai Zhou",
        "Changgeng Mo",
        "Boxuan Cao",
        "Linkai Li",
        "Shan Xiang Wang"
      ],
      "abstract": "Personalized speech intelligibility prediction is challenging. Previous approaches have mainly relied on audiograms, which are inherently limited in accuracy as they only capture a listener's hearing threshold for pure tones. Rather than incorporating additional listener features, we propose a novel approach that leverages an individual's existing intelligibility data to predict their performance on new audio. We introduce the Support Sample-Based Intelligibility Prediction Network (SSIPNet), a deep learning model that leverages speech foundation models to build a high-dimensional representation of a listener's speech recognition ability from multiple support (audio, score) pairs, enabling accurate predictions for unseen audio. Results on the Clarity Prediction Challenge dataset show that, even with a small number of support (audio, score) pairs, our method outperforms audiogram-based predictions. Our work presents a new paradigm for personalized speech intelligibility prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿä¸ªæ€§åŒ–è¯­éŸ³å¯æ‡‚åº¦é¢„æµ‹(Speech Intelligibility Prediction)ä¸»è¦ä¾èµ–å¬åŠ›å›¾(Audiograms)è€Œå¯¼è‡´å‡†ç¡®æ€§å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨ä¸ªä½“ç°æœ‰è¯„åˆ†æ•°æ®è¿›è¡Œé¢„æµ‹çš„æ–°æ–¹æ³•ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†åŸºäºæ”¯æŒæ ·æœ¬çš„å¯æ‡‚åº¦é¢„æµ‹ç½‘ç»œ(SSIPNet)ï¼Œè¯¥æ·±åº¦å­¦ä¹ æ¨¡å‹åˆ©ç”¨è¯­éŸ³åŸºç¡€æ¨¡å‹(Speech Foundation Models)ä»å¤šç»„â€œéŸ³é¢‘-è¯„åˆ†â€æ”¯æŒå¯¹(Support Pairs)ä¸­æ„å»ºå¬è€…è¯­éŸ³è¯†åˆ«èƒ½åŠ›çš„é«˜ç»´è¡¨ç¤ºï¼Œä»è€Œå®ç°å¯¹æœªçŸ¥éŸ³é¢‘çš„ç²¾ç¡®é¢„æµ‹ã€‚åœ¨Clarity Prediction Challengeæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿ä»…ä½¿ç”¨å°‘é‡æ”¯æŒæ ·æœ¬ï¼ŒSSIPNetçš„è¡¨ç°ä¹Ÿä¼˜äºä¼ ç»Ÿçš„åŸºäºå¬åŠ›å›¾çš„é¢„æµ‹æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºä¸ªæ€§åŒ–è¯­éŸ³å¯æ‡‚åº¦é¢„æµ‹é¢†åŸŸæä¾›äº†ä¸€ç§æ— éœ€ä¾èµ–å¬åŠ›å›¾çš„æ–°èŒƒå¼ï¼Œå±•ç°äº†åˆ©ç”¨æ—¢æœ‰è¡Œä¸ºæ•°æ®æå‡é¢„æµ‹ç²¾åº¦çš„æ½œåŠ›ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted at Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02039v1",
      "published_date": "2025-05-31 07:55:03 UTC",
      "updated_date": "2025-05-31 07:55:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:01:19.370167+00:00"
    },
    {
      "arxiv_id": "2506.00439v1",
      "title": "RLAE: Reinforcement Learning-Assisted Ensemble for LLMs",
      "title_zh": "RLAEï¼šå¼ºåŒ–å­¦ä¹ è¾…åŠ©çš„å¤§è¯­è¨€æ¨¡å‹é›†æˆ",
      "authors": [
        "Yuqian Fu",
        "Yuanheng Zhu",
        "Jiajun Chai",
        "Guojun Yin",
        "Wei Lin",
        "Qichao Zhang",
        "Dongbin Zhao"
      ],
      "abstract": "Ensembling large language models (LLMs) can effectively combine diverse strengths of different models, offering a promising approach to enhance performance across various tasks. However, existing methods typically rely on fixed weighting strategies that fail to adapt to the dynamic, context-dependent characteristics of LLM capabilities. In this work, we propose Reinforcement Learning-Assisted Ensemble for LLMs (RLAE), a novel framework that reformulates LLM ensemble through the lens of a Markov Decision Process (MDP). Our approach introduces a RL agent that dynamically adjusts ensemble weights by considering both input context and intermediate generation states, with the agent being trained using rewards that directly correspond to the quality of final outputs. We implement RLAE using both single-agent and multi-agent reinforcement learning algorithms ($\\text{RLAE}_\\text{PPO}$ and $\\text{RLAE}_\\text{MAPPO}$ ), demonstrating substantial improvements over conventional ensemble methods. Extensive evaluations on a diverse set of tasks show that RLAE outperforms existing approaches by up to $3.3\\%$ accuracy points, offering a more effective framework for LLM ensembling. Furthermore, our method exhibits superior generalization capabilities across different tasks without the need for retraining, while simultaneously achieving lower time latency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RLAE (Reinforcement Learning-Assisted Ensemble for LLMs)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤§è¯­è¨€æ¨¡å‹ (LLMs) é›†æˆæ–¹æ³•ä¾èµ–å›ºå®šæƒé‡ã€æ— æ³•é€‚åº”åŠ¨æ€ä¸Šä¸‹æ–‡ç‰¹æ€§çš„å±€é™ã€‚è¯¥æ¡†æ¶å°† LLM é›†æˆé‡æ–°è¡¨è¿°ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (Markov Decision Process, MDP)ï¼Œå¹¶å¼•å…¥å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“æ ¹æ®è¾“å…¥ä¸Šä¸‹æ–‡å’Œä¸­é—´ç”ŸæˆçŠ¶æ€åŠ¨æ€è°ƒæ•´é›†æˆæƒé‡ã€‚ç ”ç©¶äººå‘˜é€šè¿‡ PPO ($\\text{RLAE}_\\text{PPO}$) å’Œ MAPPO ($\\text{RLAE}_\\text{MAPPO}$) ç®—æ³•å®ç°äº†è¯¥æ¡†æ¶ï¼Œå¹¶åˆ©ç”¨ä¸è¾“å‡ºè´¨é‡ç›´æ¥ç›¸å…³çš„å¥–åŠ±å¯¹æ™ºèƒ½ä½“è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRLAE åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå‡†ç¡®ç‡æå‡é«˜è¾¾ 3.3%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†æ¨ç†å»¶è¿Ÿï¼Œä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹é›†æˆæ€§èƒ½æä¾›äº†æ›´é«˜æ•ˆä¸”çµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00439v1",
      "published_date": "2025-05-31 07:38:41 UTC",
      "updated_date": "2025-05-31 07:38:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:00:59.519120+00:00"
    },
    {
      "arxiv_id": "2506.00437v1",
      "title": "Is Your Explanation Reliable: Confidence-Aware Explanation on Graph Neural Networks",
      "title_zh": "ä½ çš„è§£é‡Šå¯é å—ï¼šå›¾ç¥ç»ç½‘ç»œä¸­çš„ç½®ä¿¡åº¦æ„ŸçŸ¥è§£é‡Š",
      "authors": [
        "Jiaxing Zhang",
        "Xiaoou Liu",
        "Dongsheng Luo",
        "Hua Wei"
      ],
      "abstract": "Explaining Graph Neural Networks (GNNs) has garnered significant attention due to the need for interpretability, enabling users to understand the behavior of these black-box models better and extract valuable insights from their predictions. While numerous post-hoc instance-level explanation methods have been proposed to interpret GNN predictions, the reliability of these explanations remains uncertain, particularly in the out-of-distribution or unknown test datasets. In this paper, we address this challenge by introducing an explainer framework with the confidence scoring module ( ConfExplainer), grounded in theoretical principle, which is generalized graph information bottleneck with confidence constraint (GIB-CC), that quantifies the reliability of generated explanations. Experimental results demonstrate the superiority of our approach, highlighting the effectiveness of the confidence score in enhancing the trustworthiness and robustness of GNN explanations.",
      "tldr_zh": "è¯¥è®ºæ–‡é’ˆå¯¹å›¾ç¥ç»ç½‘ç»œ(Graph Neural Networks)è§£é‡Šåœ¨åˆ†å¸ƒå¤–(out-of-distribution)æˆ–æœªçŸ¥æ•°æ®é›†ä¸Šå¯é æ€§éš¾ä»¥ç¡®å®šçš„é—®é¢˜ï¼Œæå‡ºäº†ConfExplaineræ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ç½®ä¿¡åº¦è¯„åˆ†æ¨¡å—ï¼Œå…¶ç†è®ºåŸºç¡€æ˜¯å¸¦æœ‰ç½®ä¿¡åº¦çº¦æŸçš„å¹¿ä¹‰å›¾ä¿¡æ¯ç“¶é¢ˆ(Generalized Graph Information Bottleneck with Confidence Constraint, GIB-CC)ï¼Œæ—¨åœ¨é‡åŒ–ç”Ÿæˆè§£é‡Šçš„å¯é æ€§ã€‚é€šè¿‡è¿™ç§ç½®ä¿¡åº¦æ„ŸçŸ¥æœºåˆ¶ï¼ŒConfExplainerèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°è§£é‡Šçš„å¯ä¿¡åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§åœºæ™¯ä¸‹å‡ä¼˜äºç°æœ‰åŸºçº¿ï¼Œæ˜¾è‘—æå‡äº†GNNè§£é‡Šçš„é²æ£’æ€§ã€‚è¯¥ç ”ç©¶ä¸ºæé«˜é»‘ç›’æ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸å¯ä¿¡ä»»åº¦æä¾›äº†é‡è¦å·¥å…·ï¼Œæœ‰åŠ©äºç”¨æˆ·æ›´å¥½åœ°ä»GNNé¢„æµ‹ä¸­æå–æ´å¯Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD25)",
      "pdf_url": "https://arxiv.org/pdf/2506.00437v1",
      "published_date": "2025-05-31 07:34:54 UTC",
      "updated_date": "2025-05-31 07:34:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:01:19.492489+00:00"
    },
    {
      "arxiv_id": "2506.00436v2",
      "title": "Learning from Double Positive and Unlabeled Data for Potential-Customer Identification",
      "title_zh": "åŸºäºåŒé‡æ­£ç±»åŠæ— æ ‡ç­¾æ•°æ®çš„æ½œåœ¨å®¢æˆ·è¯†åˆ«",
      "authors": [
        "Masahiro Kato",
        "Yuki Ikeda",
        "Kentaro Baba",
        "Takashi Imai",
        "Ryo Inokuchi"
      ],
      "abstract": "In this study, we propose a method for identifying potential customers in targeted marketing by applying learning from positive and unlabeled data (PU learning). We consider a scenario in which a company sells a product and can observe only the customers who purchased it. Decision-makers seek to market products effectively based on whether people have loyalty to the company. Individuals with loyalty are those who are likely to remain interested in the company even without additional advertising. Consequently, those loyal customers would likely purchase from the company if they are interested in the product. In contrast, people with lower loyalty may overlook the product or buy similar products from other companies unless they receive marketing attention. Therefore, by focusing marketing efforts on individuals who are interested in the product but do not have strong loyalty, we can achieve more efficient marketing. To achieve this goal, we consider how to learn, from limited data, a classifier that identifies potential customers who (i) have interest in the product and (ii) do not have loyalty to the company. Although our algorithm comprises a single-stage optimization, its objective function implicitly contains two losses derived from standard PU learning settings. For this reason, we refer to our approach as double PU learning. We verify the validity of the proposed algorithm through numerical experiments, confirming that it functions appropriately for the problem at hand.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º double PU learning çš„æ–¹æ³•ï¼Œé€šè¿‡æ­£ç±»å’Œæœªæ ‡è®°æ•°æ®ï¼ˆPU learningï¼‰çš„å­¦ä¹ æœºåˆ¶æ¥ä¼˜åŒ–é’ˆå¯¹æ€§è¥é”€ä¸­çš„æ½œåœ¨å®¢æˆ·è¯†åˆ«ã€‚ç ”ç©¶èƒŒæ™¯è®¾å®šåœ¨ä¼ä¸šä»…èƒ½è§‚å¯Ÿåˆ°å·²è´­å®¢æˆ·çš„åœºæ™¯ï¼Œæ—¨åœ¨æ ¹æ®å®¢æˆ·å¯¹å…¬å¸çš„å¿ è¯šåº¦ï¼ˆloyaltyï¼‰åˆ¶å®šæ›´é«˜æ•ˆçš„è¥é”€ç­–ç•¥ã€‚å…¶æ ¸å¿ƒç›®æ ‡æ˜¯ç²¾å‡†è¯†åˆ«å‡ºé‚£äº›å¯¹äº§å“æ„Ÿå…´è¶£ï¼ˆinterest in the productï¼‰ä½†å°šæœªå»ºç«‹å…¬å¸å¿ è¯šåº¦ï¼ˆloyalty to the companyï¼‰çš„ç¾¤ä½“ï¼Œä»è€Œé¿å…å¯¹æ— éœ€å¹¿å‘Šå³ä¼šè´­ä¹°çš„é«˜å¿ è¯šåº¦å®¢æˆ·è¿›è¡Œå†—ä½™è¥é”€ã€‚è¯¥ç®—æ³•é‡‡ç”¨å•é˜¶æ®µä¼˜åŒ–ï¼ˆsingle-stage optimizationï¼‰æ¡†æ¶ï¼Œåœ¨ç›®æ ‡å‡½æ•°ä¸­éšå¼é›†æˆäº†æºè‡ªæ ‡å‡† PU learning è®¾ç½®çš„ä¸¤ä¸ªæŸå¤±å‡½æ•°ã€‚é€šè¿‡è¿™ç§åŒé‡å­¦ä¹ æœºåˆ¶ï¼Œæ¨¡å‹èƒ½å¤Ÿä»æœ‰é™ä¸”å—é™çš„æ•°æ®ä¸­æœ‰æ•ˆåŒºåˆ†ç›®æ ‡å—ä¼—ã€‚æ•°å€¼å®éªŒï¼ˆnumerical experimentsï¼‰ç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿå‡†ç¡®æ‰§è¡Œåˆ†ç±»ä»»åŠ¡ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚è¥é”€å†³ç­–åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§å’Œå®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "econ.EM",
        "stat.ME",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for publication in the Proceedings of IIAI AAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00436v2",
      "published_date": "2025-05-31 07:33:48 UTC",
      "updated_date": "2025-06-09 09:56:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:01:12.185087+00:00"
    },
    {
      "arxiv_id": "2506.03190v1",
      "title": "MINT: Memory-Infused Prompt Tuning at Test-time for CLIP",
      "title_zh": "MINTï¼šé¢å‘ CLIP çš„æµ‹è¯•æ—¶è®°å¿†èåˆæç¤ºå¾®è°ƒ",
      "authors": [
        "Jiaming Yi",
        "Ruirui Pan",
        "Jishen Yang",
        "Xiulong Yang"
      ],
      "abstract": "Improving the generalization ability of Vision-Language Pre-trained Models (VLMs) under test-time data distribution shifts remains a critical challenge. The existing Test-Time Adaptation (TTA) methods fall short in fully leveraging the model's internal knowledge, particularly in dynamically adapting to complex and hierarchical visual semantic information. In this paper, we propose Memory-Infused Prompt Tuning (MINT), a novel framework to address this issue. Inspired by human associative memory theory, MINT introduces a Memory Prompt Bank (MPB), which stores learnable key-value prompt pairs that work as a memory of previously seen samples. During the test time, relevant prompt pairs in the MPB are retrieved by the hierarchical visual features of test images to dynamically assemble Associative Prompts. The associative prompts are then injected into the image encoder for fine-grained, customized visual contextual guidance. MINT also utilizes learnable text prompts. MINT thus enables rapid, precise VLM adaptation at test time by leveraging this MPB-acquired memory, without source data or retraining. The code is available at https://github.com/Jamieyi2004/MINT.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MINT (Memory-Infused Prompt Tuning)ï¼Œä¸€ç§æ—¨åœ¨æå‡ Vision-Language Pre-trained Models (VLMs) åœ¨æµ‹è¯•é˜¶æ®µåº”å¯¹æ•°æ®åˆ†å¸ƒåç§»æ³›åŒ–èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰ Test-Time Adaptation (TTA) æ–¹æ³•åœ¨åˆ©ç”¨æ¨¡å‹å†…éƒ¨çŸ¥è¯†åŠåŠ¨æ€é€‚åº”å¤æ‚è§†è§‰è¯­ä¹‰æ–¹é¢çš„å±€é™æ€§ï¼ŒMINT å—äººç±»å…³è”è®°å¿†ç†è®ºå¯å‘å¼•å…¥äº† Memory Prompt Bank (MPB)ã€‚è¯¥åº“å­˜å‚¨äº†å¯å­¦ä¹ çš„é”®å€¼å¯¹æç¤ºè¯ä½œä¸ºå†å²æ ·æœ¬è®°å¿†ï¼Œåœ¨æµ‹è¯•é˜¶æ®µé€šè¿‡å›¾åƒçš„å±‚æ¬¡åŒ–è§†è§‰ç‰¹å¾æ£€ç´¢å¹¶ç»„è£…æˆ Associative Promptsã€‚è¿™äº›å…³è”æç¤ºè¢«æ³¨å…¥å›¾åƒç¼–ç å™¨ä»¥æä¾›ç»†ç²’åº¦çš„å®šåˆ¶åŒ–è§†è§‰ä¸Šä¸‹æ–‡å¼•å¯¼ï¼Œå¹¶ç»“åˆå¯å­¦ä¹ çš„æ–‡æœ¬æç¤ºè¿›è¡Œä¼˜åŒ–ã€‚MINT ä½¿å¾— VLM èƒ½å¤Ÿåœ¨æ— éœ€æºæ•°æ®æˆ–é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨ MPB ç§¯ç´¯çš„è®°å¿†å®ç°å¿«é€Ÿä¸”ç²¾ç¡®çš„æµ‹è¯•æ—¶é€‚é…ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆè§£å†³äº†åŠ¨æ€ç¯å¢ƒä¸‹çš„æ¨¡å‹æ³›åŒ–æŒ‘æˆ˜ï¼Œä¸ºé«˜æ•ˆçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨çº¿å­¦ä¹ æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.03190v1",
      "published_date": "2025-05-31 07:31:20 UTC",
      "updated_date": "2025-05-31 07:31:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:01:44.192909+00:00"
    },
    {
      "arxiv_id": "2506.12059v1",
      "title": "CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models",
      "title_zh": "CMT-LLMï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¤šè¯´è¯äººè¯­éŸ³è¯†åˆ«",
      "authors": [
        "Jiajun He",
        "Naoki Sawada",
        "Koichi Miyazaki",
        "Tomoki Toda"
      ],
      "abstract": "In real-world applications, automatic speech recognition (ASR) systems must handle overlapping speech from multiple speakers and recognize rare words like technical terms. Traditional methods address multi-talker ASR and contextual biasing separately, limiting performance in complex scenarios. We propose a unified framework that combines multi-talker overlapping speech recognition and contextual biasing into a single task. Our ASR method integrates pretrained speech encoders and large language models (LLMs), using optimized finetuning strategies. We also introduce a two-stage filtering algorithm to efficiently identify relevant rare words from large biasing lists and incorporate them into the LLM's prompt input, enhancing rare word recognition. Experiments show that our approach outperforms traditional contextual biasing methods, achieving a WER of 7.9% on LibriMix and 32.9% on AMI SDM when the biasing size is 1,000, demonstrating its effectiveness in complex speech scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CMT-LLMï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å¤„ç†ä¸Šä¸‹æ–‡å¤šå‘è¨€äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)çš„ç»Ÿä¸€æ¡†æ¶ã€‚ä¸ºäº†è§£å†³çœŸå®åœºæ™¯ä¸­é‡å è¯­éŸ³è¯†åˆ«å’ŒæŠ€æœ¯æœ¯è¯­ç­‰ç”Ÿåƒ»è¯è¯†åˆ«çš„æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶å°†å¤šå‘è¨€äººè¯†åˆ«ä¸ä¸Šä¸‹æ–‡åç§»(contextual biasing)æ•´åˆä¸ºå•ä¸€ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é›†æˆäº†é¢„è®­ç»ƒè¯­éŸ³ç¼–ç å™¨å’ŒLLMsï¼Œå¹¶é‡‡ç”¨ä¼˜åŒ–çš„å¾®è°ƒç­–ç•¥ï¼ŒåŒæ—¶å¼•å…¥äº†ä¸€ç§ä¸¤é˜¶æ®µè¿‡æ»¤ç®—æ³•(two-stage filtering algorithm)ä»¥ä»å¤§è§„æ¨¡åç§»åˆ—è¡¨ä¸­é«˜æ•ˆæå–ç›¸å…³ç”Ÿåƒ»è¯å¹¶èå…¥æç¤ºè¯è¾“å…¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åç§»è¯è§„æ¨¡ä¸º1,000æ—¶ï¼Œè¯¥æ–¹æ³•åœ¨LibriMixå’ŒAMI SDMæ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†7.9%å’Œ32.9%çš„è¯é”™ç‡(WER)ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿä¸Šä¸‹æ–‡åç§»æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚è¯­éŸ³åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted by INTERSPEECH 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.12059v1",
      "published_date": "2025-05-31 07:26:44 UTC",
      "updated_date": "2025-05-31 07:26:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:01:31.793463+00:00"
    },
    {
      "arxiv_id": "2506.00432v1",
      "title": "Channel Normalization for Time Series Channel Identification",
      "title_zh": "é¢å‘æ—¶é—´åºåˆ—é€šé“è¯†åˆ«çš„é€šé“å½’ä¸€åŒ–",
      "authors": [
        "Seunghan Lee",
        "Taeyoung Park",
        "Kibok Lee"
      ],
      "abstract": "Channel identifiability (CID) refers to the ability to distinguish between individual channels in time series (TS) modeling. The absence of CID often results in producing identical outputs for identical inputs, disregarding channel-specific characteristics. In this paper, we highlight the importance of CID and propose Channel Normalization (CN), a simple yet effective normalization strategy that enhances CID by assigning distinct affine transformation parameters to each channel. We further extend CN in two ways: 1) Adaptive CN (ACN) dynamically adjusts parameters based on the input TS, improving adaptability in TS models, and 2) Prototypical CN (PCN) introduces a set of learnable prototypes instead of per-channel parameters, enabling applicability to datasets with unknown or varying number of channels and facilitating use in TS foundation models. We demonstrate the effectiveness of CN and its variants by applying them to various TS models, achieving significant performance gains for both non-CID and CID models. In addition, we analyze the success of our approach from an information theory perspective. Code is available at https://github.com/seunghan96/CN.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Time Serieså»ºæ¨¡ä¸­çš„Channel Identifiability (CID)é—®é¢˜ï¼ŒæŒ‡å‡ºç¼ºä¹CIDä¼šå¯¼è‡´æ¨¡å‹æ— æ³•åŒºåˆ†ä¸åŒé€šé“çš„ç‰¹å¾ã€‚ä½œè€…æå‡ºäº†Channel Normalization (CN)ï¼Œä¸€ç§é€šè¿‡ä¸ºæ¯ä¸ªé€šé“åˆ†é…ç‹¬ç«‹ä»¿å°„å˜æ¢å‚æ•°æ¥å¢å¼ºCIDçš„å½’ä¸€åŒ–ç­–ç•¥ã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†ä¸¤ç§æ‰©å±•æ–¹æ¡ˆï¼šAdaptive CN (ACN)æ ¹æ®è¾“å…¥åŠ¨æ€è°ƒæ•´å‚æ•°ä»¥æå‡æ¨¡å‹é€‚åº”æ€§ï¼ŒPrototypical CN (PCN)åˆ™åˆ©ç”¨å¯å­¦ä¹ çš„Prototypeså¤„ç†é€šé“æ•°æœªçŸ¥æˆ–å¤šå˜çš„æƒ…å†µï¼Œä»è€Œæ”¯æŒTS foundation modelsã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCNåŠå…¶å˜ä½“åœ¨å¤šç§Time Seriesæ¨¡å‹ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æœ€åï¼Œè¯¥ç ”ç©¶è¿˜ä»ä¿¡æ¯è®ºçš„è§’åº¦å¯¹æ–¹æ³•çš„æˆåŠŸè¿›è¡Œäº†ç†è®ºåˆ†æã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00432v1",
      "published_date": "2025-05-31 07:24:24 UTC",
      "updated_date": "2025-05-31 07:24:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:01:38.094878+00:00"
    },
    {
      "arxiv_id": "2506.00430v2",
      "title": "MIRROR: Modular Internal Processing for Personalized Safety in LLM Dialogue",
      "title_zh": "MIRRORï¼šå¤§è¯­è¨€æ¨¡å‹å¯¹è¯ä¸­é¢å‘ä¸ªæ€§åŒ–å®‰å…¨çš„æ¨¡å—åŒ–å†…éƒ¨å¤„ç†",
      "authors": [
        "Nicole Hsing"
      ],
      "abstract": "Large language models frequently generate harmful recommendations in personal multi-turn dialogue by ignoring user-specific safety context, exhibiting sycophantic agreement, and compromising user safety for larger group preferences. We introduce MIRROR, a modular production-focused architecture that prevents these failures through a persistent, bounded internal state that preserves personal conversational information across conversational turns. Our dual-component design inspired by Dual Process Theory separates immediate response generation (Talker) from asynchronous deliberative processing (Thinker), which synthesizes parallel reasoning threads between turns with marginal latency. On the CuRaTe personalized safety benchmark, MIRROR-augmented models achieve a 21% relative improvement (69% to 84%) across seven diverse frontier models, with open-source Llama 4 and Mistral 3 variants surpassing both GPT-4o and Claude 3.7 Sonnet at only \\$0.0028 to \\$0.0172 additional cost per turn, narrowing the gap between affordable open-source models to frontier systems in the safety space. The modular architecture enables flexible deployment: full internal processing for affordable models or single-component configurations for expensive systems, democratizing access to safer, personalized AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MIRRORï¼Œä¸€ç§é¢å‘ç”Ÿäº§ç¯å¢ƒçš„æ¨¡å—åŒ–æ¶æ„ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ä¸ªæ€§åŒ–å¤šè½®å¯¹è¯ä¸­å› å¿½è§†ç”¨æˆ·ç‰¹å®šå®‰å…¨èƒŒæ™¯å’Œç›²ç›®è¿åˆç”¨æˆ·è€Œäº§ç”Ÿçš„æœ‰å®³å»ºè®®é—®é¢˜ã€‚è¯¥æ¶æ„å— Dual Process Theory å¯å‘ï¼Œé‡‡ç”¨åŒç»„ä»¶è®¾è®¡ï¼Œå°†å³æ—¶å“åº”ç”Ÿæˆçš„ Talker ä¸æ‰§è¡Œå¼‚æ­¥å®¡è®®å¤„ç†çš„ Thinker ç›¸äº’åˆ†ç¦»ã€‚é€šè¿‡ç»´æŒä¸€ä¸ªæŒä¹…ä¸”å—é™çš„å†…éƒ¨çŠ¶æ€ï¼ŒMIRROR èƒ½å¤Ÿåœ¨å¯¹è¯è½®æ¬¡é—´é«˜æ•ˆåˆæˆå¹¶è¡Œæ¨ç†çº¿ç´¢ï¼Œç¡®ä¿ä¸ªäººå®‰å…¨ä¿¡æ¯çš„è·¨è½®æ¬¡ä¿ç•™ã€‚åœ¨ CuRaTe ä¸ªæ€§åŒ–å®‰å…¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMIRROR åŠ©åŠ›æ¨¡å‹å®ç° 21% çš„ç›¸å¯¹æ€§èƒ½æå‡ï¼Œä½¿ Llama 4 å’Œ Mistral 3 ç­‰å¼€æºå˜ä½“åœ¨å®‰å…¨è¡¨ç°ä¸Šè¶…è¶Šäº† GPT-4o å’Œ Claude 3.7 Sonnetã€‚è¯¥æ–¹æ¡ˆæ¯è½®ä»…å¢åŠ  0.0028 è‡³ 0.0172 ç¾å…ƒçš„æä½æˆæœ¬ï¼Œå…¶çµæ´»çš„æ¨¡å—åŒ–é…ç½®ä¸ä»…ç¼©å°äº†å¼€æºæ¨¡å‹ä¸é¡¶å°–ç³»ç»Ÿåœ¨å®‰å…¨é¢†åŸŸçš„å·®è·ï¼Œä¹Ÿä¸ºå®ç°æ™®åŠåŒ–çš„å®‰å…¨ä¸ªæ€§åŒ– AI å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00430v2",
      "published_date": "2025-05-31 07:17:48 UTC",
      "updated_date": "2025-10-03 17:42:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:01:50.695402+00:00"
    },
    {
      "arxiv_id": "2506.00424v2",
      "title": "COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning",
      "title_zh": "COGNATEï¼šåŸºäºè¿ç§»å­¦ä¹ çš„æ–°å…´ç¡¬ä»¶ç¨€ç–å¼ é‡ç¨‹åºåŠ é€Ÿ",
      "authors": [
        "Chamika Sudusinghe",
        "Gerasimos Gerogiannis",
        "Damitha Lenadora",
        "Charles Block",
        "Josep Torrellas",
        "Charith Mendis"
      ],
      "abstract": "Sparse tensor programs are essential in deep learning and graph analytics, driving the need for optimized processing. To meet this demand, specialized hardware accelerators are being developed. Optimizing these programs for accelerators is challenging for two reasons: program performance is highly sensitive to variations in sparse inputs, and early-stage accelerators rely on expensive simulators. Therefore, ML-based cost models used for optimizing such programs on general-purpose hardware are often ineffective for early-stage accelerators, as they require large datasets for proper training. To this end, we introduce COGNATE, a novel framework that leverages inexpensive data samples from general-purpose hardware (e.g., CPUs) to train cost models, followed by few-shot fine-tuning on emerging hardware. COGNATE exploits the homogeneity of input features across hardware platforms while effectively mitigating heterogeneity, enabling cost model training with just 5% of the data samples needed by accelerator-specific models to achieve comparable performance. We conduct extensive experiments to demonstrate that COGNATE outperforms existing techniques, achieving average speedups of 1.47x (up to 5.46x) for SpMM and 1.39x (up to 4.22x) for SDDMM.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº† COGNATEï¼Œä¸€ä¸ªæ—¨åœ¨åˆ©ç”¨ Transfer Learning åŠ é€Ÿæ–°å…´ç¡¬ä»¶ä¸Š Sparse Tensor Programs è¿è¡Œçš„åˆ›æ–°æ¡†æ¶ã€‚é’ˆå¯¹æ—©æœŸç¡¬ä»¶åŠ é€Ÿå™¨æ¨¡æ‹Ÿæˆæœ¬é«˜æ˜‚ä¸” Sparse Tensor Programs æ€§èƒ½å—è¾“å…¥å˜åŒ–å½±å“æ˜¾è‘—çš„æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆè§£å†³äº† ML-based cost models åœ¨è®­ç»ƒåˆæœŸæ•°æ®åŒ®ä¹çš„é—®é¢˜ã€‚COGNATE é¦–å…ˆåˆ©ç”¨æ¥è‡ª CPU ç­‰é€šç”¨ç¡¬ä»¶çš„å»‰ä»·æ•°æ®æ ·æœ¬è¿›è¡Œåˆæ­¥è®­ç»ƒï¼Œéšååœ¨æ–°å…´ç¡¬ä»¶ä¸Šé€šè¿‡ Few-shot fine-tuning è¿›è¡Œå¾®è°ƒã€‚é€šè¿‡åˆ©ç”¨ä¸åŒç¡¬ä»¶å¹³å°é—´è¾“å…¥ç‰¹å¾çš„åŒè´¨æ€§å¹¶å‡è½»å…¶å¼‚è´¨æ€§å½±å“ï¼Œè¯¥æ–¹æ³•ä»…éœ€åŠ é€Ÿå™¨ä¸“ç”¨æ¨¡å‹ 5% çš„æ•°æ®é‡å³å¯è¾¾åˆ°åŒç­‰æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCOGNATE åœ¨ SpMM ä»»åŠ¡ä¸­å®ç°äº†å¹³å‡ 1.47 å€ï¼ˆæœ€é«˜ 5.46 å€ï¼‰çš„åŠ é€Ÿï¼Œåœ¨ SDDMM ä»»åŠ¡ä¸­å®ç°äº†å¹³å‡ 1.39 å€ï¼ˆæœ€é«˜ 4.22 å€ï¼‰çš„åŠ é€Ÿã€‚ä¸ç°æœ‰ä¼˜åŒ–æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æå‡äº†ç¨€ç–å¼ é‡ç¨‹åºåœ¨æ–°å‹ç¡¬ä»¶ä¸Šçš„æ‰§è¡Œæ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR",
        "cs.ET"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the 42nd International Conference on Machine Learning",
      "pdf_url": "https://arxiv.org/pdf/2506.00424v2",
      "published_date": "2025-05-31 06:59:55 UTC",
      "updated_date": "2025-06-15 01:10:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:02:10.388050+00:00"
    },
    {
      "arxiv_id": "2506.00421v1",
      "title": "Enabling Chatbots with Eyes and Ears: An Immersive Multimodal Conversation System for Dynamic Interactions",
      "title_zh": "èµ‹äºˆèŠå¤©æœºå™¨äººâ€œçœ¼â€ä¸â€œè€³â€ï¼šé¢å‘åŠ¨æ€äº¤äº’çš„æ²‰æµ¸å¼å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿ",
      "authors": [
        "Jihyoung Jang",
        "Minwook Bae",
        "Minji Kim",
        "Dilek Hakkani-Tur",
        "Hyounghun Kim"
      ],
      "abstract": "As chatbots continue to evolve toward human-like, real-world, interactions, multimodality remains an active area of research and exploration. So far, efforts to integrate multimodality into chatbots have primarily focused on image-centric tasks, such as visual dialogue and image-based instructions, placing emphasis on the \"eyes\" of human perception while neglecting the \"ears\", namely auditory aspects. Moreover, these studies often center around static interactions that focus on discussing the modality rather than naturally incorporating it into the conversation, which limits the richness of simultaneous, dynamic engagement. Furthermore, while multimodality has been explored in multi-party and multi-session conversations, task-specific constraints have hindered its seamless integration into dynamic, natural conversations. To address these challenges, this study aims to equip chatbots with \"eyes and ears\" capable of more immersive interactions with humans. As part of this effort, we introduce a new multimodal conversation dataset, Multimodal Multi-Session Multi-Party Conversation ($M^3C$), and propose a novel multimodal conversation model featuring multimodal memory retrieval. Our model, trained on the $M^3C$, demonstrates the ability to seamlessly engage in long-term conversations with multiple speakers in complex, real-world-like settings, effectively processing visual and auditory inputs to understand and respond appropriately. Human evaluations highlight the model's strong performance in maintaining coherent and dynamic interactions, demonstrating its potential for advanced multimodal conversational agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰èŠå¤©æœºå™¨äººä¸»è¦å…³æ³¨è§†è§‰è€Œå¿½ç•¥å¬è§‰ï¼Œä¸”äº’åŠ¨æ¨¡å¼å¤šä¸ºé™æ€çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå…·å¤‡â€œè§†è§‰ä¸å¬è§‰â€åŠŸèƒ½çš„æ²‰æµ¸å¼å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…å¼•å…¥äº†ä¸€ä¸ªåä¸º Multimodal Multi-Session Multi-Party Conversation ($M^3C$) çš„æ–°å‹å¤šæ¨¡æ€å¯¹è¯æ•°æ®é›†ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å…·å¤‡å¤šæ¨¡æ€è®°å¿†æ£€ç´¢ (multimodal memory retrieval) åŠŸèƒ½çš„åˆ›æ–°å¯¹è¯æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨ $M^3C$ æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå±•ç°å‡ºåœ¨å¤æ‚çœŸå®åœºæ™¯ä¸­ä¸å¤šä½å‘è¨€è€…è¿›è¡Œé•¿æœŸã€åŠ¨æ€äº¤æµçš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†è§†è§‰å’Œå¬è§‰è¾“å…¥ï¼Œå¹¶åœ¨ç†è§£å’Œå“åº”æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚äººå·¥è¯„ä¼°ç»“æœè¿›ä¸€æ­¥è¯å®äº†è¯¥æ¨¡å‹åœ¨ä¿æŒè¿è´¯ä¸”åŠ¨æ€çš„äº¤äº’æ–¹é¢çš„å¼ºå¤§æ€§èƒ½ï¼Œä½“ç°äº†å…¶åœ¨å…ˆè¿›å¤šæ¨¡æ€å¯¹è¯æ™ºèƒ½ä½“é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 (32 pages); Project website: https://m3c-dataset.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2506.00421v1",
      "published_date": "2025-05-31 06:50:51 UTC",
      "updated_date": "2025-05-31 06:50:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:02:16.508245+00:00"
    },
    {
      "arxiv_id": "2506.00420v1",
      "title": "A New Spatiotemporal Correlation Anomaly Detection Method that Integrates Contrastive Learning and Few-Shot Learning in Wireless Sensor Networks",
      "title_zh": "ä¸€ç§èåˆå¯¹æ¯”å­¦ä¹ ä¸å°æ ·æœ¬å­¦ä¹ çš„æ— çº¿ä¼ æ„Ÿå™¨ç½‘ç»œæ—¶ç©ºç›¸å…³æ€§å¼‚å¸¸æ£€æµ‹æ–°æ–¹æ³•",
      "authors": [
        "Miao Ye",
        "Suxiao Wang",
        "Jiaguang Han",
        "Yong Wang",
        "Xiaoli Wang",
        "Jingxuan Wei",
        "Peng Wen",
        "Jing Cui"
      ],
      "abstract": "Detecting anomalies in the data collected by WSNs can provide crucial evidence for assessing the reliability and stability of WSNs. Existing methods for WSN anomaly detection often face challenges such as the limited extraction of spatiotemporal correlation features, the absence of sample labels, few anomaly samples, and an imbalanced sample distribution. To address these issues, a spatiotemporal correlation detection model (MTAD-RD) considering both model architecture and a two-stage training strategy perspective is proposed. In terms of model structure design, the proposed MTAD-RD backbone network includes a retentive network (RetNet) enhanced by a cross-retention (CR) module, a multigranular feature fusion module, and a graph attention network module to extract internode correlation information. This proposed model can integrate the intermodal correlation features and spatial features of WSN neighbor nodes while extracting global information from time series data. Moreover, its serialized inference characteristic can remarkably reduce inference overhead. For model training, a two-stage training approach was designed. First, a contrastive learning proxy task was designed for time series data with graph structure information in WSNs, enabling the backbone network to learn transferable features from unlabeled data using unsupervised contrastive learning methods, thereby addressing the issue of missing sample labels in the dataset. Then, a caching-based sample sampler was designed to divide samples into few-shot and contrastive learning data. A specific joint loss function was developed to jointly train the dual-graph discriminator network to address the problem of sample imbalance effectively. In experiments carried out on real public datasets, the designed MTAD-RD anomaly detection method achieved an F1 score of 90.97%, outperforming existing supervised WSN anomaly detection methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— çº¿ä¼ æ„Ÿå™¨ç½‘ç»œ(WSNs)å¼‚å¸¸æ£€æµ‹ä¸­é¢ä¸´çš„æ—¶ç©ºç›¸å…³æ€§ç‰¹å¾æå–ä¸è¶³ã€æ ‡ç­¾ç¼ºå¤±ã€å¼‚å¸¸æ ·æœ¬ç¨€ç–åŠåˆ†å¸ƒä¸å‡ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§é›†æˆå¯¹æ¯”å­¦ä¹ (Contrastive Learning)ä¸å°‘æ ·æœ¬å­¦ä¹ (Few-Shot Learning)çš„æ–°å‹æ£€æµ‹æ¨¡å‹MTAD-RDã€‚åœ¨æ¶æ„è®¾è®¡ä¸Šï¼Œè¯¥æ¨¡å‹ä¸»å¹²ç½‘ç»œç»“åˆäº†å¸¦æœ‰äº¤å‰ä¿ç•™(Cross-Retention)æ¨¡å—çš„ä¿ç•™ç½‘ç»œ(RetNet)ã€å¤šç²’åº¦ç‰¹å¾èåˆæ¨¡å—åŠå›¾æ³¨æ„åŠ›ç½‘ç»œ(GAT)ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆé‚»å±…èŠ‚ç‚¹çš„ç©ºé—´ç‰¹å¾å¹¶ä»æ—¶é—´åºåˆ—ä¸­æå–å…¨å±€ä¿¡æ¯ã€‚è®­ç»ƒè¿‡ç¨‹é‡‡ç”¨ä¸¤é˜¶æ®µç­–ç•¥ï¼Œé¦–å…ˆé€šè¿‡æ— ç›‘ç£å¯¹æ¯”å­¦ä¹ ä»£ç†ä»»åŠ¡ä»æ— æ ‡ç­¾æ•°æ®ä¸­å­¦ä¹ å¯è¿ç§»ç‰¹å¾ï¼Œéšååˆ©ç”¨åŸºäºç¼“å­˜çš„é‡‡æ ·å™¨å’Œè”åˆæŸå¤±å‡½æ•°è®­ç»ƒåŒå›¾åˆ¤åˆ«ç½‘ç»œï¼Œä»¥è§£å†³æ ·æœ¬ä¸å¹³è¡¡é—®é¢˜ã€‚MTAD-RDçš„åºåˆ—åŒ–æ¨ç†ç‰¹æ€§è¿˜æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ï¼Œæå‡äº†æ£€æµ‹æ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®å…¬å¼€æ•°æ®é›†ä¸Šå–å¾—äº†90.97%çš„F1å¾—åˆ†ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰çš„ç›‘ç£å­¦ä¹ å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œä¸ºè¯„ä¼°æ— çº¿ä¼ æ„Ÿå™¨ç½‘ç»œçš„å¯é æ€§å’Œç¨³å®šæ€§æä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00420v1",
      "published_date": "2025-05-31 06:50:05 UTC",
      "updated_date": "2025-05-31 06:50:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:02:16.205321+00:00"
    },
    {
      "arxiv_id": "2506.00418v2",
      "title": "Dual Debiasing for Noisy In-Context Learning for Text Generation",
      "title_zh": "æ–‡æœ¬ç”Ÿæˆä¸­å¸¦å™ªä¸Šä¸‹æ–‡å­¦ä¹ çš„åŒé‡å»å",
      "authors": [
        "Siqi Liang",
        "Sumyeong Ahn",
        "Paramveer S. Dhillon",
        "Jiayu Zhou"
      ],
      "abstract": "In context learning (ICL) relies heavily on high quality demonstrations drawn from large annotated corpora. Existing approaches detect noisy annotations by ranking local perplexities, presuming that noisy samples yield higher perplexities than their clean counterparts. However, this assumption breaks down when the noise ratio is high and many demonstrations are flawed. We reexamine the perplexity based paradigm for text generation under noisy annotations, highlighting two sources of bias in perplexity: the annotation itself and the domain specific knowledge inherent in large language models (LLMs). To overcome these biases, we introduce a dual debiasing framework that uses synthesized neighbors to explicitly correct perplexity estimates, yielding a robust Sample Cleanliness Score. This metric uncovers absolute sample cleanliness regardless of the overall corpus noise level. Extensive experiments demonstrate our method's superior noise detection capabilities and show that its final ICL performance is comparable to that of a fully clean demonstration corpus. Moreover, our approach remains robust even when noise ratios are extremely high.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬ç”Ÿæˆä¸­çš„å˜ˆæ‚ä¸Šä¸‹æ–‡å­¦ä¹  (In-Context Learning, ICL) é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„åŸºäºå›°æƒ‘åº¦ (Perplexity) çš„å™ªå£°æ£€æµ‹æ–¹æ³•åœ¨å™ªå£°æ¯”ä¾‹è¾ƒé«˜æ—¶ä¼šå¤±æ•ˆã€‚ç ”ç©¶æ·±å…¥åˆ†æäº†å›°æƒ‘åº¦ä¸­å­˜åœ¨çš„ä¸¤ç±»åå·®ï¼Œå³æ ‡æ³¨æœ¬èº«å¼•å…¥çš„åå·®ä»¥åŠå¤§è¯­è¨€æ¨¡å‹ (LLMs) å›ºæœ‰çš„é¢†åŸŸçŸ¥è¯†åå·®ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåŒé‡å»å (Dual Debiasing) æ¡†æ¶ï¼Œåˆ©ç”¨åˆæˆé‚»å±… (Synthesized Neighbors) æ¥æ˜¾å¼ä¿®æ­£å›°æƒ‘åº¦ä¼°è®¡ï¼Œä»è€Œè®¡ç®—å‡ºé²æ£’çš„æ ·æœ¬æ¸…æ´åº¦å¾—åˆ† (Sample Cleanliness Score)ã€‚è¯¥æŒ‡æ ‡èƒ½å¤Ÿæœ‰æ•ˆè¡¡é‡æ ·æœ¬çš„ç»å¯¹æ¸…æ´åº¦ï¼Œä¸”ä¸å—æ•´ä½“è¯­æ–™åº“å™ªå£°æ°´å¹³çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰å“è¶Šçš„å™ªå£°æ£€æµ‹èƒ½åŠ›ï¼Œä½¿ ICL çš„æœ€ç»ˆæ€§èƒ½è¾¾åˆ°äº†ä¸å®Œå…¨å¹²å‡€çš„æ¼”ç¤ºè¯­æ–™åº“ç›¸å½“çš„æ°´å¹³ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨æç«¯çš„é«˜å™ªå£°æ¯”ä¾‹ç¯å¢ƒä¸‹ï¼Œè¯¥æ–¹æ³•ä¾ç„¶è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by 2025 ACL Findings",
      "pdf_url": "https://arxiv.org/pdf/2506.00418v2",
      "published_date": "2025-05-31 06:44:48 UTC",
      "updated_date": "2025-06-21 07:56:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:02:16.702251+00:00"
    },
    {
      "arxiv_id": "2506.00417v1",
      "title": "World Models for Cognitive Agents: Transforming Edge Intelligence in Future Networks",
      "title_zh": "é¢å‘è®¤çŸ¥æ™ºèƒ½ä½“çš„ä¸–ç•Œæ¨¡å‹ï¼šèµ‹èƒ½æœªæ¥ç½‘ç»œè¾¹ç¼˜æ™ºèƒ½å˜é©",
      "authors": [
        "Changyuan Zhao",
        "Ruichen Zhang",
        "Jiacheng Wang",
        "Gaosheng Zhao",
        "Dusit Niyato",
        "Geng Sun",
        "Shiwen Mao",
        "Dong In Kim"
      ],
      "abstract": "World models are emerging as a transformative paradigm in artificial intelligence, enabling agents to construct internal representations of their environments for predictive reasoning, planning, and decision-making. By learning latent dynamics, world models provide a sample-efficient framework that is especially valuable in data-constrained or safety-critical scenarios. In this paper, we present a comprehensive overview of world models, highlighting their architecture, training paradigms, and applications across prediction, generation, planning, and causal reasoning. We compare and distinguish world models from related concepts such as digital twins, the metaverse, and foundation models, clarifying their unique role as embedded cognitive engines for autonomous agents. We further propose Wireless Dreamer, a novel world model-based reinforcement learning framework tailored for wireless edge intelligence optimization, particularly in low-altitude wireless networks (LAWNs). Through a weather-aware UAV trajectory planning case study, we demonstrate the effectiveness of our framework in improving learning efficiency and decision quality.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸–ç•Œæ¨¡å‹(World Models)ä½œä¸ºäººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€ç§æ–°å…´èŒƒå¼ï¼Œå¦‚ä½•é€šè¿‡æ„å»ºç¯å¢ƒçš„å†…éƒ¨è¡¨å¾æ¥å®ç°é¢„æµ‹æ€§æ¨ç†ã€è§„åˆ’å’Œå†³ç­–ã€‚æ–‡ç« å…¨é¢ç»¼è¿°äº†ä¸–ç•Œæ¨¡å‹çš„æ¶æ„ã€è®­ç»ƒèŒƒå¼åŠå…¶åœ¨é¢„æµ‹ã€ç”Ÿæˆå’Œå› æœæ¨ç†ä¸­çš„åº”ç”¨ï¼Œå¹¶å°†å…¶ä¸æ•°å­—å­ªç”Ÿ(Digital Twins)ã€å…ƒå®‡å®™(Metaverse)åŠåŸºç¡€æ¨¡å‹(Foundation Models)è¿›è¡Œäº†å¯¹æ¯”ï¼Œæ˜ç¡®äº†å…¶ä½œä¸ºè‡ªä¸»æ™ºèƒ½ä½“åµŒå…¥å¼è®¤çŸ¥å¼•æ“çš„ç‹¬ç‰¹åœ°ä½ã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†Wireless Dreamerï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºæ— çº¿è¾¹ç¼˜æ™ºèƒ½ä¼˜åŒ–çš„åŸºäºä¸–ç•Œæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç‰¹åˆ«é€‚ç”¨äºä½ç©ºæ— çº¿ç½‘ç»œ(LAWNs)ã€‚é€šè¿‡å¤©æ°”æ„ŸçŸ¥çš„æ— äººæœº(UAV)è½¨è¿¹è§„åˆ’æ¡ˆä¾‹ç ”ç©¶ï¼Œå®éªŒè¯æ˜è¯¥æ¡†æ¶åœ¨æå‡å­¦ä¹ æ•ˆç‡å’Œå†³ç­–è´¨é‡æ–¹é¢å…·æœ‰æ˜¾è‘—æ•ˆæœï¼Œä¸ºæœªæ¥ç½‘ç»œä¸­çš„è®¤çŸ¥æ™ºèƒ½ä½“æä¾›äº†æ–°çš„è½¬å‹è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.00417v1",
      "published_date": "2025-05-31 06:43:00 UTC",
      "updated_date": "2025-05-31 06:43:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:02:18.335590+00:00"
    },
    {
      "arxiv_id": "2506.00415v1",
      "title": "Wide Reflective Equilibrium in LLM Alignment: Bridging Moral Epistemology and AI Safety",
      "title_zh": "LLM å¯¹é½ä¸­çš„å¹¿ä¹‰åæ€å‡è¡¡ï¼šè¿æ¥é“å¾·è®¤è¯†è®ºä¸ AI å®‰å…¨",
      "authors": [
        "Matthew Brophy"
      ],
      "abstract": "As large language models (LLMs) become more powerful and pervasive across society, ensuring these systems are beneficial, safe, and aligned with human values is crucial. Current alignment techniques, like Constitutional AI (CAI), involve complex iterative processes. This paper argues that the Method of Wide Reflective Equilibrium (MWRE) -- a well-established coherentist moral methodology -- offers a uniquely apt framework for understanding current LLM alignment efforts. Moreover, this methodology can substantively augment these processes by providing concrete pathways for improving their dynamic revisability, procedural legitimacy, and overall ethical grounding. Together, these enhancements can help produce more robust and ethically defensible outcomes. MWRE, emphasizing the achievement of coherence between our considered moral judgments, guiding moral principles, and relevant background theories, arguably better represents the intricate reality of LLM alignment and offers a more robust path to justification than prevailing foundationalist models or simplistic input-output evaluations. While current methods like CAI bear a structural resemblance to MWRE, they often lack its crucial emphasis on dynamic, bi-directional revision of principles and the procedural legitimacy derived from such a process. While acknowledging various disanalogies (e.g., consciousness, genuine understanding in LLMs), the paper demonstrates that MWRE serves as a valuable heuristic for critically analyzing current alignment efforts and for guiding the future development of more ethically sound and justifiably aligned AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å¯¹é½ä¸­ç¡®ä¿å®‰å…¨æ€§ä¸äººç±»ä»·å€¼è§‚ä¸€è‡´çš„é‡è¦æ€§ï¼Œæå‡ºå¹¿ä¹‰åæ€å¹³è¡¡æ³•(Method of Wide Reflective Equilibrium, MWRE)è¿™ä¸€é“å¾·çŸ¥è¯†è®ºæ–¹æ³•ä¸ºç†è§£å½“å‰å¯¹é½å·¥ä½œæä¾›äº†ç†æƒ³æ¡†æ¶ã€‚è®ºæ–‡åˆ†ææŒ‡å‡ºï¼Œå½“å‰çš„å¯¹é½æŠ€æœ¯å¦‚å®ªæ³•äººå·¥æ™ºèƒ½(Constitutional AI, CAI)è™½ä¸MWREç»“æ„ç›¸ä¼¼ï¼Œä½†ç¼ºä¹å¯¹åŸåˆ™è¿›è¡ŒåŠ¨æ€åŒå‘ä¿®æ­£çš„å…³é”®å¼ºè°ƒã€‚é€šè¿‡åœ¨å®¡æ…çš„é“å¾·åˆ¤æ–­ã€æŒ‡å¯¼æ€§åŸåˆ™å’Œç›¸å…³èƒŒæ™¯ç†è®ºä¹‹é—´å¯»æ±‚è¿è´¯æ€§(coherence)ï¼ŒMWREèƒ½å¤Ÿæ˜¾è‘—å¢å¼ºå¯¹é½è¿‡ç¨‹çš„ç¨‹åºæ­£å½“æ€§ä¸ä¼¦ç†æ ¹åŸºã€‚ç›¸æ¯”åŸºç¡€ä¸»ä¹‰æ¨¡å‹(foundationalist models)æˆ–ç®€å•çš„è¾“å…¥è¾“å‡ºè¯„ä¼°ï¼Œè¯¥æ–¹æ³•ä¸ºå¯¹é½ç»“æœçš„åˆç†æ€§æä¾›äº†æ›´ç¨³å¥çš„è¯æ˜è·¯å¾„ã€‚å°½ç®¡LLMsåœ¨æ„è¯†å’Œç†è§£åŠ›ä¸Šä¸äººç±»å­˜åœ¨å·®å¼‚ï¼Œä½†MWREä»å¯ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„å¯å‘å¼å·¥å…·ï¼ŒæŒ‡å¯¼æœªæ¥å¼€å‘ä¼¦ç†ä¸Šæ›´å®Œå¤‡ä¸”å…·æœ‰è®ºè¯æ­£å½“æ€§çš„AIç³»ç»Ÿã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "24 pages excluding references, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.00415v1",
      "published_date": "2025-05-31 06:40:59 UTC",
      "updated_date": "2025-05-31 06:40:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:02:30.454558+00:00"
    },
    {
      "arxiv_id": "2506.00413v2",
      "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
      "title_zh": "é€šè¿‡è‡ªé€‚åº”å¹¶è¡Œè§£ç åŠ é€Ÿæ‰©æ•£å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Daniel Israel",
        "Guy Van den Broeck",
        "Aditya Grover"
      ],
      "abstract": "The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ (Diffusion LLMs) åœ¨å¹¶è¡Œç”Ÿæˆä»¤ç‰Œæ—¶éš¾ä»¥åœ¨ä¸ç‰ºç‰²è´¨é‡çš„å‰æä¸‹è¾¾åˆ°è‡ªå›å½’æ¨¡å‹é€Ÿåº¦çš„é—®é¢˜ï¼Œæå‡ºäº†è‡ªé€‚åº”å¹¶è¡Œè§£ç  (Adaptive Parallel Decoding, APD) æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å®šä¹‰ dLLM çš„è¾¹ç¼˜æ¦‚ç‡ä¸å°å‹è¾…åŠ©è‡ªå›å½’æ¨¡å‹ä¸‹çš„åºåˆ—è”åˆæ¦‚ç‡ä¹‹é—´çš„ä¹˜æ³•æ··åˆï¼Œå®ç°äº†å¹¶è¡Œé‡‡æ ·ä»¤ç‰Œæ•°é‡çš„åŠ¨æ€è°ƒæ•´ã€‚è¿™ç§è®¾è®¡æœ‰æ•ˆåè½¬äº†ä¼ ç»Ÿçš„æŠ•æœºè§£ç  (Speculative Decoding) æ¡†æ¶ï¼Œåˆ©ç”¨å°å‹æ¨¡å‹è¾…åŠ©ä»æ‰©æ•£æ¨¡å‹ä¸­è¿›è¡Œé«˜æ•ˆé‡‡æ ·ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº† KV ç¼“å­˜ (KV caching) å’Œé™åˆ¶æ©ç è¾“å…¥å¤§å°ç­‰ä¼˜åŒ–æ‰‹æ®µï¼Œå¹¶æä¾›äº†ä¸‰ä¸ªå¯è°ƒå‚æ•°ä»¥çµæ´»å¹³è¡¡ååé‡ä¸ç”Ÿæˆè´¨é‡ã€‚å®éªŒè¯æ˜ï¼ŒAPD åœ¨å¤šä¸ªä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†ååé‡ï¼Œä¸”ä»…äº§ç”Ÿäº†æå°çš„è´¨é‡ä¸‹é™ï¼Œä¸ºåŠ é€Ÿéè‡ªå›å½’è¯­è¨€æ¨¡å‹ç”Ÿæˆæä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.00413v2",
      "published_date": "2025-05-31 06:10:10 UTC",
      "updated_date": "2025-10-30 21:11:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:02:39.932767+00:00"
    },
    {
      "arxiv_id": "2506.15712v1",
      "title": "BatteryBERT for Realistic Battery Fault Detection Using Point-Masked Signal Modeling",
      "title_zh": "BatteryBERTï¼šåŸºäºç‚¹æ©ç ä¿¡å·å»ºæ¨¡çš„çœŸå®ç”µæ± æ•…éšœæ£€æµ‹",
      "authors": [
        "Songqi Zhou",
        "Ruixue Liu",
        "Yixing Wang",
        "Jia Lu",
        "Benben Jiang"
      ],
      "abstract": "Accurate fault detection in lithium-ion batteries is essential for the safe and reliable operation of electric vehicles and energy storage systems. However, existing methods often struggle to capture complex temporal dependencies and cannot fully leverage abundant unlabeled data. Although large language models (LLMs) exhibit strong representation capabilities, their architectures are not directly suited to the numerical time-series data common in industrial settings. To address these challenges, we propose a novel framework that adapts BERT-style pretraining for battery fault detection by extending the standard BERT architecture with a customized time-series-to-token representation module and a point-level Masked Signal Modeling (point-MSM) pretraining task tailored to battery applications. This approach enables self-supervised learning on sequential current, voltage, and other charge-discharge cycle data, yielding distributionally robust, context-aware temporal embeddings. We then concatenate these embeddings with battery metadata and feed them into a downstream classifier for accurate fault classification. Experimental results on a large-scale real-world dataset show that models initialized with our pretrained parameters significantly improve both representation quality and classification accuracy, achieving an AUROC of 0.945 and substantially outperforming existing approaches. These findings validate the effectiveness of BERT-style pretraining for time-series fault detection.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BatteryBERTï¼Œä¸€ç§ç”¨äºé”‚ç¦»å­ç”µæ± æ•…éšœæ£€æµ‹çš„åˆ›æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥æ•æ‰å¤æ‚æ—¶é—´ä¾èµ–æ€§ä¸”æ— æ³•å……åˆ†åˆ©ç”¨å¤§é‡æ— æ ‡ç­¾æ•°æ®çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ‰©å±•æ ‡å‡†BERTæ¶æ„ï¼Œå¼•å…¥äº†å®šåˆ¶çš„æ—¶é—´åºåˆ—åˆ°ä»¤ç‰Œ(time-series-to-token)è¡¨ç¤ºæ¨¡å—ï¼Œå¹¶é‡‡ç”¨äº†ä¸“é—¨é’ˆå¯¹ç”µæ± åº”ç”¨è®¾è®¡çš„ç‚¹çº§æ©ç ä¿¡å·å»ºæ¨¡(point-level Masked Signal Modeling, point-MSM)é¢„è®­ç»ƒä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•å®ç°äº†åœ¨ç”µæµã€ç”µå‹åŠå……æ”¾ç”µå¾ªç¯ç­‰åºåˆ—æ•°æ®ä¸Šçš„è‡ªç›‘ç£å­¦ä¹ ï¼Œä»è€Œç”Ÿæˆå…·å¤‡åˆ†å¸ƒé²æ£’æ€§å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„æ—¶é—´åµŒå…¥(temporal embeddings)ã€‚ç ”ç©¶äººå‘˜å°†è¿™äº›åµŒå…¥ä¸ç”µæ± å…ƒæ•°æ®ç»“åˆåè¾“å…¥ä¸‹æ¸¸åˆ†ç±»å™¨ï¼Œä»¥å®ç°é«˜ç²¾åº¦çš„æ•…éšœåˆ†ç±»ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBatteryBERTåœ¨å¤§è§„æ¨¡çœŸå®æ•°æ®é›†ä¸Šè¾¾åˆ°äº†0.945çš„AUROCï¼Œè¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæœ‰åŠ›è¯æ˜äº†BERTé£æ ¼é¢„è®­ç»ƒåœ¨å·¥ä¸šæ—¶é—´åºåˆ—æ•…éšœæ£€æµ‹é¢†åŸŸçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15712v1",
      "published_date": "2025-05-31 06:06:08 UTC",
      "updated_date": "2025-05-31 06:06:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:02:34.657054+00:00"
    },
    {
      "arxiv_id": "2506.00411v1",
      "title": "LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks",
      "title_zh": "LoHoVLAï¼šé¢å‘é•¿æ—¶ç¨‹å…·èº«ä»»åŠ¡çš„ç»Ÿä¸€è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹",
      "authors": [
        "Yi Yang",
        "Jiaxuan Sun",
        "Siqi Kou",
        "Yihan Wang",
        "Zhijie Deng"
      ],
      "abstract": "Real-world embodied agents face long-horizon tasks, characterized by high-level goals demanding multi-step solutions beyond single actions. Successfully navigating these requires both high-level task planning (i.e., decomposing goals into sub-tasks) and low-level motion control (i.e., generating precise robot actions). While existing vision language action (VLA) models and hierarchical architectures offer potential in embodied tasks, the former often falter in planning, and the latter can suffer from coordination issues, both hampering performance. We introduce a new unified VLA framework for long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA leverages a large pretrained vision language model (VLM) as the backbone to jointly generate language and action tokens for sub-task generation and robot action prediction, respectively. This shared representation promotes better generalization across tasks. Additionally, LoHoVLA embraces a hierarchical closed-loop control mechanism to mitigate errors originating from both high-level planning and low-level control. To train LoHoVLA, we introduce LoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon tasks, each with 1,000 expert demonstrations composed of visual observations, linguistic goals, sub-tasks, and robot actions. Experimental results show that LoHoVLA significantly surpasses both hierarchical and standard VLA approaches on long-horizon embodied tasks in the Ravens simulator. These findings underscore the promise of unified architectures for advancing generalizable embodied intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·èº«æ™ºèƒ½ä½“åœ¨é•¿æ—¶ç¨‹(long-horizon)ä»»åŠ¡ä¸­é¢ä¸´çš„é«˜å±‚è§„åˆ’ä¸åº•å±‚æ§åˆ¶åè°ƒéš¾é¢˜ï¼Œæå‡ºäº†ç»Ÿä¸€çš„Vision-Language-Actionæ¡†æ¶LoHoVLAã€‚è¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹(VLM)ä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œé€šè¿‡å…±äº«è¡¨ç¤ºåŒæ—¶ç”Ÿæˆå­ä»»åŠ¡çš„è¯­è¨€æ ‡è®°å’Œæœºå™¨äººåŠ¨ä½œæ ‡è®°ï¼Œä»è€Œå¢å¼ºäº†è·¨ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚LoHoVLAé‡‡ç”¨äº†åˆ†å±‚é—­ç¯æ§åˆ¶(hierarchical closed-loop control)æœºåˆ¶ï¼Œæœ‰æ•ˆç¼“è§£äº†è§„åˆ’ä¸æ‰§è¡Œè¿‡ç¨‹ä¸­çš„è¯¯å·®ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æ¨å‡ºäº†åŸºäºRavensæ¨¡æ‹Ÿå™¨çš„LoHoSetæ•°æ®é›†ï¼Œæ¶µç›–20é¡¹ä»»åŠ¡åŠä¸°å¯Œçš„ä¸“å®¶æ¼”ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLoHoVLAçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„åˆ†å±‚æ¶æ„å’Œæ ‡å‡†VLAæ¨¡å‹ï¼ŒéªŒè¯äº†ç»Ÿä¸€æ¶æ„åœ¨æå‡é€šç”¨å…·èº«æ™ºèƒ½æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00411v1",
      "published_date": "2025-05-31 06:01:03 UTC",
      "updated_date": "2025-05-31 06:01:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:02:52.711841+00:00"
    },
    {
      "arxiv_id": "2506.00407v1",
      "title": "Bias as a Virtue: Rethinking Generalization under Distribution Shifts",
      "title_zh": "ä»¥åå·®ä¸ºç¾ï¼šé‡æ–°å®¡è§†åˆ†å¸ƒåç§»ä¸‹çš„æ³›åŒ–",
      "authors": [
        "Ruixuan Chen",
        "Wentao Li",
        "Jiahui Xiao",
        "Yuchen Li",
        "Yimin Tang",
        "Xiaonan Wang"
      ],
      "abstract": "Machine learning models often degrade when deployed on data distributions different from their training data. Challenging conventional validation paradigms, we demonstrate that higher in-distribution (ID) bias can lead to better out-of-distribution (OOD) generalization. Our Adaptive Distribution Bridge (ADB) framework implements this insight by introducing controlled statistical diversity during training, enabling models to develop bias profiles that effectively generalize across distributions. Empirically, we observe a robust negative correlation where higher ID bias corresponds to lower OOD error--a finding that contradicts standard practices focused on minimizing validation error. Evaluation on multiple datasets shows our approach significantly improves OOD generalization. ADB achieves robust mean error reductions of up to 26.8% compared to traditional cross-validation, and consistently identifies high-performing training strategies, evidenced by percentile ranks often exceeding 74.4%. Our work provides both a practical method for improving generalization and a theoretical framework for reconsidering the role of bias in robust machine learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨åˆ†å¸ƒåç§»(Distribution Shifts)ä¸‹çš„æ³›åŒ–é€€åŒ–é—®é¢˜ï¼Œå¹¶æŒ‘æˆ˜äº†ä¼ ç»Ÿçš„éªŒè¯èŒƒå¼ã€‚ä½œè€…æå‡ºäº†Adaptive Distribution Bridge (ADB)æ¡†æ¶ï¼Œæ ¸å¿ƒæ´å¯Ÿåœ¨äºè¾ƒé«˜çš„åˆ†å¸ƒå†…(In-Distribution, ID)åå·®åè€Œèƒ½å¼•å¯¼æ›´å¥½çš„åˆ†å¸ƒå¤–(Out-of-Distribution, OOD)æ³›åŒ–è¡¨ç°ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥å—æ§çš„ç»Ÿè®¡å¤šæ ·æ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå»ºç«‹å¯æœ‰æ•ˆè·¨åˆ†å¸ƒæ³›åŒ–çš„åå·®ç‰¹å¾ã€‚å®éªŒè§‚å¯Ÿåˆ°IDåå·®ä¸OODè¯¯å·®ä¹‹é—´å­˜åœ¨å¼ºå¥çš„è´Ÿç›¸å…³æ€§ï¼Œè¿™ä¸€å‘ç°ä¸ä¼˜å…ˆæœ€å°åŒ–éªŒè¯è¯¯å·®çš„å¸¸è§„åšæ³•æˆªç„¶ä¸åŒã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒADBç›¸æ¯”ä¼ ç»Ÿäº¤å‰éªŒè¯å¯å°†å¹³å‡è¯¯å·®é™ä½å¤šè¾¾26.8%ï¼Œå…¶æ€§èƒ½ç™¾åˆ†ä½æ’åé€šå¸¸è¶…è¿‡74.4%ã€‚è¯¥é¡¹å·¥ä½œä¸ä»…ä¸ºæé«˜æ¨¡å‹é²æ£’æ€§æä¾›äº†å®ç”¨æ–¹æ³•ï¼Œä¹Ÿä¸ºé‡æ–°å®¡è§†åå·®åœ¨é²æ£’æœºå™¨å­¦ä¹ ä¸­çš„ä½œç”¨æä¾›äº†ç†è®ºæ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.00407v1",
      "published_date": "2025-05-31 05:54:49 UTC",
      "updated_date": "2025-05-31 05:54:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:04:09.643065+00:00"
    },
    {
      "arxiv_id": "2506.00400v3",
      "title": "Scaling Textual Gradients via Sampling-Based Momentum",
      "title_zh": "åŸºäºé‡‡æ ·åŠ¨é‡çš„æ–‡æœ¬æ¢¯åº¦è§„æ¨¡åŒ–",
      "authors": [
        "Zixin Ding",
        "Junyuan Hong",
        "Zhan Shi",
        "Jiachen T. Wang",
        "Zinan Lin",
        "Li Yin",
        "Meng Liu",
        "Zhangyang Wang",
        "Yuxin Chen"
      ],
      "abstract": "LLM-based prompt optimization, that uses LLM-provided \"textual gradients\" (feedback) to refine prompts, has emerged an effective method for automatic prompt engineering. However, its scalability and stability are unclear when using more data in training. We systematically investigate the potential and challenges of scaling training data in textual gradient descent. We show that naively scaling training examples is infeasible due to both explicit context-length limits and an implicit context wall, where long-context degradation yields diminishing returns. Inspired by prior wisdom in stochastic gradient descent, we propose Textual Stochastic Gradient Descent with Momentum (TSGD-M), which reweights updates through momentum sampling, using bootstrapped minibatch validation accuracy as importance weights over historical prompts. We introduce Gumbel-Top-$k$ sampling for prompt generation, balancing exploration--exploitation and improving sampling efficiency while maintaining a low-variance running mean estimator. TSGD-M integrates seamlessly into existing prompt optimization frameworks, including TextGrad, DSPy-COPRO, and AdalFlow, and achieves consistent gains across 5 benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æç¤ºè¯ä¼˜åŒ–(prompt optimization)åœ¨æ‰©å¤§è®­ç»ƒæ•°æ®è§„æ¨¡æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºç®€å•çš„å¢åŠ æ•°æ®ä¼šå—é™äºä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶å’Œé•¿æ–‡æœ¬è¡°å‡(long-context degradation)å¯¼è‡´çš„â€œä¸Šä¸‹æ–‡å¢™â€é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…å—éšæœºæ¢¯åº¦ä¸‹é™å¯å‘ï¼Œæå‡ºäº†åŸºäºé‡‡æ ·åŠ¨é‡çš„æ–‡æœ¬éšæœºæ¢¯åº¦ä¸‹é™æ³•(Textual Stochastic Gradient Descent with Momentum, TSGD-M)ã€‚è¯¥æ–¹æ³•é€šè¿‡åŠ¨é‡é‡‡æ ·å¯¹æ›´æ–°è¿›è¡Œé‡æ–°åŠ æƒï¼Œåˆ©ç”¨è‡ªåŠ©æ³•(bootstrapped)çš„å°æ‰¹é‡éªŒè¯å‡†ç¡®ç‡ä½œä¸ºå†å²æç¤ºè¯çš„æƒé‡ã€‚åŒæ—¶ï¼Œç ”ç©¶å¼•å…¥äº†Gumbel-Top-ké‡‡æ ·æ¥è¿›è¡Œæç¤ºè¯ç”Ÿæˆï¼Œåœ¨å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨(exploration-exploitation)çš„åŒæ—¶æé«˜äº†é‡‡æ ·æ•ˆç‡ã€‚TSGD-Mèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°TextGradã€DSPy-COPROå’ŒAdalFlowç­‰ç°æœ‰æ¡†æ¶ä¸­ï¼Œå¹¶åœ¨5ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æŒç»­çš„æ€§èƒ½æå‡ã€‚è¿™é¡¹å·¥ä½œä¸ºè‡ªåŠ¨æç¤ºè¯å·¥ç¨‹(automatic prompt engineering)åœ¨å¤§è§„æ¨¡æ•°æ®ä¸‹çš„æ‰©å±•æ€§å’Œç¨³å®šæ€§æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00400v3",
      "published_date": "2025-05-31 05:35:45 UTC",
      "updated_date": "2025-11-18 00:22:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:04:42.605868+00:00"
    },
    {
      "arxiv_id": "2506.00398v1",
      "title": "Position: Olfaction Standardization is Essential for the Advancement of Embodied Artificial Intelligence",
      "title_zh": "è§‚ç‚¹ï¼šå—…è§‰æ ‡å‡†åŒ–æ˜¯æ¨è¿›å…·èº«æ™ºèƒ½å‘å±•çš„å…³é”®",
      "authors": [
        "Kordel K. France",
        "Rohith Peddi",
        "Nik Dennler",
        "Ovidiu Daescu"
      ],
      "abstract": "Despite extraordinary progress in artificial intelligence (AI), modern systems remain incomplete representations of human cognition. Vision, audition, and language have received disproportionate attention due to well-defined benchmarks, standardized datasets, and consensus-driven scientific foundations. In contrast, olfaction - a high-bandwidth, evolutionarily critical sense - has been largely overlooked. This omission presents a foundational gap in the construction of truly embodied and ethically aligned super-human intelligence. We argue that the exclusion of olfactory perception from AI architectures is not due to irrelevance but to structural challenges: unresolved scientific theories of smell, heterogeneous sensor technologies, lack of standardized olfactory datasets, absence of AI-oriented benchmarks, and difficulty in evaluating sub-perceptual signal processing. These obstacles have hindered the development of machine olfaction despite its tight coupling with memory, emotion, and contextual reasoning in biological systems. In this position paper, we assert that meaningful progress toward general and embodied intelligence requires serious investment in olfactory research by the AI community. We call for cross-disciplinary collaboration - spanning neuroscience, robotics, machine learning, and ethics - to formalize olfactory benchmarks, develop multimodal datasets, and define the sensory capabilities necessary for machines to understand, navigate, and act within human environments. Recognizing olfaction as a core modality is essential not only for scientific completeness, but for building AI systems that are ethically grounded in the full scope of the human experience.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºå—…è§‰(Olfaction)åœ¨å…·èº«äººå·¥æ™ºèƒ½(Embodied Artificial Intelligence)çš„å‘å±•ä¸­è‡³å…³é‡è¦ï¼Œå´å› ç¼ºä¹ç»Ÿä¸€åŸºå‡†(Benchmarks)å’Œæ ‡å‡†æ•°æ®é›†è€Œè¢«é•¿æœŸå¿½è§†ã€‚ä½œè€…è®¤ä¸ºï¼Œå°½ç®¡è§†è§‰å’Œè¯­è¨€é¢†åŸŸå–å¾—äº†å·¨å¤§è¿›æ­¥ï¼Œä½†ç¼ºä¹å—…è§‰æ„ŸçŸ¥çš„ç³»ç»Ÿæ— æ³•å®Œæ•´å‘ˆç°äººç±»è®¤çŸ¥ï¼Œåœ¨æ„å»ºä¸ä¼¦ç†å¯¹é½çš„è¶…äººç±»æ™ºèƒ½(Super-human intelligence)æ–¹é¢å­˜åœ¨åŸºç¡€æ€§ç¼ºé™·ã€‚è®ºæ–‡è¯¦ç»†åˆ†æäº†å—…è§‰æ ‡å‡†åŒ–çš„ç»“æ„æ€§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¼‚æ„ä¼ æ„Ÿå™¨æŠ€æœ¯å’Œç¼ºä¹å¤šæ¨¡æ€æ•°æ®é›†(Multimodal datasets)ç­‰éšœç¢ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å‘¼åå¼€å±•æ¶µç›–ç¥ç»ç§‘å­¦ã€æœºå™¨äººæŠ€æœ¯ã€æœºå™¨å­¦ä¹ å’Œä¼¦ç†å­¦çš„è·¨å­¦ç§‘åä½œï¼Œä»¥å½¢å¼åŒ–å—…è§‰è¯„æµ‹åŸºå‡†å¹¶å¼€å‘ç›¸å…³æ•°æ®é›†ã€‚å°†å—…è§‰ä½œä¸ºæ ¸å¿ƒæ„ŸçŸ¥ç»´åº¦ï¼Œä¸ä»…èƒ½å®ç°äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ç§‘å­¦å®Œæ•´æ€§ï¼Œä¹Ÿæ˜¯ä½¿å…¶èƒ½å¤ŸçœŸæ­£ç†è§£ã€å¯¼èˆªå¹¶æ¤æ ¹äºäººç±»ç¯å¢ƒçš„å…³é”®ã€‚",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00398v1",
      "published_date": "2025-05-31 05:35:13 UTC",
      "updated_date": "2025-05-31 05:35:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:04:37.333511+00:00"
    },
    {
      "arxiv_id": "2506.06332v1",
      "title": "Introduction to Predictive Coding Networks for Machine Learning",
      "title_zh": "æœºå™¨å­¦ä¹ é¢„æµ‹ç¼–ç ç½‘ç»œå¯¼è®º",
      "authors": [
        "Mikko Stenlund"
      ],
      "abstract": "Predictive coding networks (PCNs) constitute a biologically inspired framework for understanding hierarchical computation in the brain, and offer an alternative to traditional feedforward neural networks in ML. This note serves as a quick, onboarding introduction to PCNs for machine learning practitioners. We cover the foundational network architecture, inference and learning update rules, and algorithmic implementation. A concrete image-classification task (CIFAR-10) is provided as a benchmark-smashing application, together with an accompanying Python notebook containing the PyTorch implementation.",
      "tldr_zh": "è¯¥æ–‡ç« ä»‹ç»äº†é¢„æµ‹ç¼–ç ç½‘ç»œ(Predictive Coding Networks, PCNs)ï¼Œè¿™æ˜¯ä¸€ç§å—ç”Ÿç‰©å¯å‘ã€ç”¨äºç†è§£å¤§è„‘åˆ†å±‚è®¡ç®—çš„æ¡†æ¶ï¼Œå¹¶ä¸ºæœºå™¨å­¦ä¹ é¢†åŸŸæä¾›äº†ä¼ ç»Ÿå‰é¦ˆç¥ç»ç½‘ç»œä¹‹å¤–çš„æ›¿ä»£æ–¹æ¡ˆã€‚æœ¬æ–‡æ—¨åœ¨ä½œä¸ºæœºå™¨å­¦ä¹ ä»ä¸šè€…çš„å¿«é€Ÿå…¥é—¨æŒ‡å—ï¼Œå…¨é¢æ¶µç›–äº†PCNsçš„åŸºç¡€ç½‘ç»œæ¶æ„ã€æ¨ç†ä¸å­¦ä¹ æ›´æ–°è§„åˆ™ä»¥åŠå…·ä½“çš„ç®—æ³•å®ç°ç»†èŠ‚ã€‚æ–‡ç« é€šè¿‡CIFAR-10å›¾åƒåˆ†ç±»ä»»åŠ¡å±•ç¤ºäº†è¯¥æ¡†æ¶ä½œä¸ºåŸºå‡†æµ‹è¯•çš„åº”ç”¨æ•ˆæœï¼Œè¯æ˜äº†å…¶åœ¨å®é™…ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜æä¾›äº†é…å¥—çš„Python notebookå’ŒPyTorchå®ç°ä»£ç ï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜è¿›è¡Œå¤ç°ä¸å¼€å‘ã€‚è¯¥æ•™ç¨‹ä¸ºç†è§£å’Œåº”ç”¨å…·æœ‰ç”Ÿç‰©åˆç†æ€§çš„åˆ†å±‚è®¡ç®—æ¨¡å‹æä¾›äº†ä»ç†è®ºåˆ°å®è·µçš„å®Œæ•´è·¯å¾„ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "22 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.06332v1",
      "published_date": "2025-05-31 04:48:53 UTC",
      "updated_date": "2025-05-31 04:48:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:04:30.868056+00:00"
    },
    {
      "arxiv_id": "2506.00385v1",
      "title": "MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity Reconstruction and Generation",
      "title_zh": "MagiCodecï¼šç”¨äºé«˜ä¿çœŸé‡å»ºä¸ç”Ÿæˆçš„ç®€å•æ©ç é«˜æ–¯æ³¨å…¥ç¼–è§£ç å™¨",
      "authors": [
        "Yakun Song",
        "Jiawei Chen",
        "Xiaobin Zhuang",
        "Chenpeng Du",
        "Ziyang Ma",
        "Jian Wu",
        "Jian Cong",
        "Dongya Jia",
        "Zhuo Chen",
        "Yuping Wang",
        "Yuxuan Wang",
        "Xie Chen"
      ],
      "abstract": "Neural audio codecs have made significant strides in efficiently mapping raw audio waveforms into discrete token representations, which are foundational for contemporary audio generative models. However, most existing codecs are optimized primarily for reconstruction quality, often at the expense of the downstream modelability of the encoded tokens. Motivated by the need to overcome this bottleneck, we introduce $\\textbf{MagiCodec}$, a novel single-layer, streaming Transformer-based audio codec. MagiCodec is designed with a multistage training pipeline that incorporates Gaussian noise injection and latent regularization, explicitly targeting the enhancement of semantic expressiveness in the generated codes while preserving high reconstruction fidelity. We analytically derive the effect of noise injection in the frequency domain, demonstrating its efficacy in attenuating high-frequency components and fostering robust tokenization. Extensive experimental evaluations show that MagiCodec surpasses state-of-the-art codecs in both reconstruction quality and downstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like distributions, as observed in natural languages, thereby improving compatibility with language-model-based generative architectures. The code and pre-trained models are available at https://github.com/Ereboas/MagiCodec.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MagiCodecï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå•å±‚ã€æµå¼Transformerçš„éŸ³é¢‘ç¼–è§£ç å™¨(audio codec)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç¼–è§£ç å™¨åœ¨ç¦»æ•£Tokenè¡¨ç¤ºçš„ä¸‹æ¸¸å¯å»ºæ¨¡æ€§(modelability)æ–¹é¢å­˜åœ¨çš„å±€é™ã€‚MagiCodecé‡‡ç”¨äº†åŒ…å«é«˜æ–¯å™ªå£°æ³¨å…¥(Gaussian noise injection)å’Œæ½œåœ¨æ­£åˆ™åŒ–(latent regularization)çš„å¤šé˜¶æ®µè®­ç»ƒæµæ°´çº¿ï¼Œåœ¨ä¿æŒé«˜é‡å»ºä¿çœŸåº¦(reconstruction fidelity)çš„åŒæ—¶ï¼Œæ˜¾è‘—å¢å¼ºäº†ç”Ÿæˆä»£ç çš„è¯­ä¹‰è¡¨è¾¾èƒ½åŠ›ã€‚ç ”ç©¶äººå‘˜ä»é¢‘åŸŸè§’åº¦åˆ†æäº†å™ªå£°æ³¨å…¥çš„æ•ˆæœï¼Œè¯æ˜äº†å…¶åœ¨å‰Šå¼±é«˜é¢‘æˆåˆ†å’Œä¿ƒè¿›é²æ£’TokenåŒ–(robust tokenization)æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMagiCodecåœ¨é‡å»ºè´¨é‡å’Œä¸‹æ¸¸ä»»åŠ¡è¡¨ç°ä¸Šå‡è¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒMagiCodecç”Ÿæˆçš„Tokenå±•ç°å‡ºç±»ä¼¼äºè‡ªç„¶è¯­è¨€çš„é½å¤«åˆ†å¸ƒ(Zipf-like distributions)ï¼Œæå¤§åœ°æå‡äº†å…¶ä¸åŸºäºè¯­è¨€æ¨¡å‹çš„ç”Ÿæˆå¼æ¶æ„(language-model-based generative architectures)çš„å…¼å®¹æ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "18 pages, 3 figures. The code and pre-trained models are available at https://github.com/Ereboas/MagiCodec",
      "pdf_url": "https://arxiv.org/pdf/2506.00385v1",
      "published_date": "2025-05-31 04:31:02 UTC",
      "updated_date": "2025-05-31 04:31:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:04:50.026903+00:00"
    },
    {
      "arxiv_id": "2506.02037v2",
      "title": "FinS-Pilot: A Benchmark for Online Financial RAG System",
      "title_zh": "FinS-Pilotï¼šé¢å‘åœ¨çº¿é‡‘è RAG ç³»ç»Ÿçš„è¯„æµ‹åŸºå‡†",
      "authors": [
        "Feng Wang",
        "Yiding Sun",
        "Jiaxin Mao",
        "Wei Xue",
        "Danqing Xu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across various professional domains, with their performance typically evaluated through standardized benchmarks. In the financial field, the stringent demands for professional accuracy and real-time data processing often necessitate the use of retrieval-augmented generation (RAG) techniques. However, the development of financial RAG benchmarks has been constrained by data confidentiality issues and the lack of dynamic data integration. To address this issue, we introduce FinS-Pilot, a novel benchmark for evaluating RAG systems in online financial applications. Constructed from real-world financial assistant interactions, our benchmark incorporates both real-time API data and text data, organized through an intent classification framework covering critical financial domains. The benchmark enables comprehensive evaluation of financial assistants' capabilities in handling both static knowledge and time-sensitive market information.Through systematic experiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's effectiveness in identifying models suitable for financial applications while addressing the current gap in specialized evaluation tools for the financial domain. Our work contributes both a practical evaluation framework and a curated dataset to advance research in financial NLP systems. The code and dataset are accessible on GitHub.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†FinS-Pilotï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°åœ¨çº¿é‡‘èåº”ç”¨ä¸­RAGç³»ç»Ÿçš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚é’ˆå¯¹é‡‘èé¢†åŸŸå¯¹ä¸“ä¸šå‡†ç¡®æ€§å’Œå®æ—¶æ•°æ®å¤„ç†çš„é«˜è¦æ±‚ï¼ŒFinS-Piloté€šè¿‡ç»“åˆçœŸå®ä¸–ç•Œçš„é‡‘èåŠ©æ‰‹äº¤äº’ã€å®æ—¶APIæ•°æ®ä»¥åŠæ–‡æœ¬æ•°æ®ï¼Œè§£å†³äº†ç°æœ‰åŸºå‡†æµ‹è¯•ç¼ºä¹åŠ¨æ€æ•°æ®æ•´åˆçš„é—®é¢˜ã€‚è¯¥åŸºå‡†é‡‡ç”¨è¦†ç›–å…³é”®é‡‘èé¢†åŸŸçš„æ„å›¾åˆ†ç±»æ¡†æ¶(intent classification framework)ï¼Œèƒ½å¤Ÿå…¨é¢è¯„ä¼°æ¨¡å‹å¤„ç†é™æ€çŸ¥è¯†å’Œå…·æœ‰æ—¶æ•ˆæ€§çš„å¸‚åœºä¿¡æ¯çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹å¤šä¸ªé¢†å…ˆçš„ä¸­æ–‡LLMsè¿›è¡Œç³»ç»Ÿå®éªŒï¼Œç»“æœè¯æ˜FinS-Pilotèƒ½æœ‰æ•ˆè¯†åˆ«é€‚åˆé‡‘èåº”ç”¨çš„æ¨¡å‹ï¼Œå¡«è¡¥äº†é‡‘èé¢†åŸŸä¸“ä¸šåŒ–è¯„ä¼°å·¥å…·çš„ç©ºç™½ã€‚è¯¥å·¥ä½œä¸ºæ¨åŠ¨é‡‘èNLPç³»ç»Ÿç ”ç©¶æä¾›äº†å®ç”¨çš„è¯„ä¼°æ¡†æ¶å’Œç²¾é€‰æ•°æ®é›†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02037v2",
      "published_date": "2025-05-31 03:50:19 UTC",
      "updated_date": "2025-09-01 10:49:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:04:51.835050+00:00"
    },
    {
      "arxiv_id": "2506.06331v1",
      "title": "How Significant Are the Real Performance Gains? An Unbiased Evaluation Framework for GraphRAG",
      "title_zh": "å®é™…æ€§èƒ½æå‡ç©¶ç«Ÿå‡ ä½•ï¼Ÿä¸€ç§é’ˆå¯¹ GraphRAG çš„æ— åè¯„ä¼°æ¡†æ¶",
      "authors": [
        "Qiming Zeng",
        "Xiao Yan",
        "Hao Luo",
        "Yuhao Lin",
        "Yuxiang Wang",
        "Fangcheng Fu",
        "Bo Du",
        "Quanqing Xu",
        "Jiawei Jiang"
      ],
      "abstract": "By retrieving contexts from knowledge graphs, graph-based retrieval-augmented generation (GraphRAG) enhances large language models (LLMs) to generate quality answers for user questions. Many GraphRAG methods have been proposed and reported inspiring performance in answer quality. However, we observe that the current answer evaluation framework for GraphRAG has two critical flaws, i.e., unrelated questions and evaluation biases, which may lead to biased or even wrong conclusions on performance. To tackle the two flaws, we propose an unbiased evaluation framework that uses graph-text-grounded question generation to produce questions that are more related to the underlying dataset and an unbiased evaluation procedure to eliminate the biases in LLM-based answer assessment. We apply our unbiased framework to evaluate 3 representative GraphRAG methods and find that their performance gains are much more moderate than reported previously. Although our evaluation framework may still have flaws, it calls for scientific evaluations to lay solid foundations for GraphRAG research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå›¾çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(GraphRAG)ç°æœ‰è¯„ä¼°æ¡†æ¶ä¸­å­˜åœ¨çš„æ— å…³é—®é¢˜(unrelated questions)å’Œè¯„ä¼°åè§(evaluation biases)ä¸¤å¤§ç¼ºé™·ï¼Œæå‡ºäº†ä¸€ä¸ªæ— åè§è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŸºäºå›¾æ–‡å…³è”çš„é—®é¢˜ç”ŸæˆæŠ€æœ¯(graph-text-grounded question generation)æ¥äº§ç”Ÿä¸æ•°æ®é›†é«˜åº¦ç›¸å…³çš„é—®é¢˜ï¼Œå¹¶è®¾è®¡äº†æ— åè§è¯„ä¼°ç¨‹åºä»¥æ¶ˆé™¤å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç­”æ¡ˆè¯„ä¼°ä¸­çš„åå·®ã€‚é€šè¿‡å¯¹ä¸‰ç§ä»£è¡¨æ€§ GraphRAG æ–¹æ³•çš„å®é™…æµ‹è¯•ï¼Œç ”ç©¶å‘ç°å®ƒä»¬çš„æ€§èƒ½æå‡å¹…åº¦è¿œä½äºæ­¤å‰çš„æŠ¥é“ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å½“å‰è¯„ä¼°ä½“ç³»å¯èƒ½å¯¼è‡´é”™è¯¯ç»“è®ºçš„é£é™©ï¼Œå‘¼åå­¦æœ¯ç•Œå»ºç«‹æ›´ä¸¥è°¨çš„ç§‘å­¦è¯„ä¼°æ ‡å‡†ï¼Œä¸º GraphRAG ç ”ç©¶å¥ å®šåšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06331v1",
      "published_date": "2025-05-31 03:36:16 UTC",
      "updated_date": "2025-05-31 03:36:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:05:06.973383+00:00"
    },
    {
      "arxiv_id": "2506.00368v1",
      "title": "Neural Network-based Information-Theoretic Transceivers for High-Order Modulation Schemes",
      "title_zh": "é¢å‘é«˜é˜¶è°ƒåˆ¶æ–¹æ¡ˆçš„åŸºäºç¥ç»ç½‘ç»œçš„ä¿¡æ¯è®ºæ”¶å‘æœº",
      "authors": [
        "Ngoc Long Pham",
        "Tri Nhu Do"
      ],
      "abstract": "Neural network (NN)-based end-to-end (E2E) communication systems, in which each system component may consist of a portion of a neural network, have been investigated as potential tools for developing artificial intelligence (Al)-native E2E systems. In this paper, we propose an NN-based bitwise receiver that improves computational efficiency while maintaining performance comparable to baseline demappers. Building on this foundation, we introduce a novel symbol-wise autoencoder (AE)-based E2E system that jointly optimizes the transmitter and receiver at the physical layer. We evaluate the proposed NN-based receiver using bit-error rate (BER) analysis to confirm that the numerical BER achieved by NN-based receivers or transceivers is accurate. Results demonstrate that the AE-based system outperforms baseline architectures, particularly for higher-order modulation schemes. We further show that the training signal-to-noise ratio (SNR) significantly affects the performance of the systems when inference is conducted at different SNR levels.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºç¥ç»ç½‘ç»œ(Neural Network)çš„ç«¯åˆ°ç«¯(End-to-End)é€šä¿¡ç³»ç»Ÿï¼Œæ—¨åœ¨å¼€å‘äººå·¥æ™ºèƒ½åŸç”Ÿ(AI-native)çš„ç‰©ç†å±‚æ¶æ„ã€‚è®ºæ–‡é¦–å…ˆæå‡ºäº†ä¸€ç§åŸºäºNNçš„æ¯”ç‰¹çº§æ¥æ”¶æœºï¼Œåœ¨ä¿æŒä¸ä¼ ç»Ÿè§£è°ƒå™¨(Demapper)æ€§èƒ½ç›¸å½“çš„åŒæ—¶æ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŸºäºç¬¦å·è‡ªç¼–ç å™¨(Autoencoder)çš„ç«¯åˆ°ç«¯ç³»ç»Ÿï¼Œå®ç°äº†ç‰©ç†å±‚å‘å°„æœºå’Œæ¥æ”¶æœºçš„è”åˆä¼˜åŒ–ã€‚é€šè¿‡æ¯”ç‰¹è¯¯ç ç‡(Bit-Error Rate)åˆ†æï¼Œç ”ç©¶éªŒè¯äº†NNæ”¶å‘æœºåœ¨æ•°å€¼è¡¨ç°ä¸Šçš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥Autoencoderç³»ç»Ÿåœ¨å¤„ç†é«˜é˜¶è°ƒåˆ¶(High-Order Modulation)æ–¹æ¡ˆæ—¶æ˜æ˜¾ä¼˜äºåŸºå‡†æ¶æ„ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ­ç¤ºäº†è®­ç»ƒä¿¡å™ªæ¯”(SNR)å¯¹ç³»ç»Ÿåœ¨ä¸åŒæ¨ç†ä¿¡å™ªæ¯”æ°´å¹³ä¸‹æ€§èƒ½è¡¨ç°çš„æ˜¾è‘—å½±å“ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00368v1",
      "published_date": "2025-05-31 03:22:26 UTC",
      "updated_date": "2025-05-31 03:22:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:04:54.331804+00:00"
    },
    {
      "arxiv_id": "2506.00358v3",
      "title": "$\\texttt{AVROBUSTBENCH}$: Benchmarking the Robustness of Audio-Visual Recognition Models at Test-Time",
      "title_zh": "$\\texttt{AVROBUSTBENCH}$ï¼šè§†å¬è¯†åˆ«æ¨¡å‹æµ‹è¯•æ—¶é²æ£’æ€§çš„åŸºå‡†è¯„ä¼°",
      "authors": [
        "Sarthak Kumar Maharana",
        "Saksham Singh Kushwaha",
        "Baoming Zhang",
        "Adrian Rodriguez",
        "Songtao Wei",
        "Yapeng Tian",
        "Yunhui Guo"
      ],
      "abstract": "While recent audio-visual models have demonstrated impressive performance, their robustness to distributional shifts at test-time remains not fully understood. Existing robustness benchmarks mainly focus on single modalities, making them insufficient for thoroughly assessing the robustness of audio-visual models. Motivated by real-world scenarios where shifts can occur $\\textit{simultaneously}$ in both audio and visual modalities, we introduce $\\texttt{AVROBUSTBENCH}$, a comprehensive benchmark designed to evaluate the test-time robustness of audio-visual recognition models. $\\texttt{AVROBUSTBENCH}$ comprises four audio-visual benchmark datasets, $\\texttt{AUDIOSET-2C}$, $\\texttt{VGGSOUND-2C}$, $\\texttt{KINETICS-2C}$, and $\\texttt{EPICKITCHENS-2C}$, each incorporating 75 bimodal audio-visual corruptions that are $\\textit{co-occurring}$ and $\\textit{correlated}$. Through extensive evaluations, we observe that state-of-the-art supervised and self-supervised audio-visual models exhibit declining robustness as corruption severity increases. Furthermore, online test-time adaptation (TTA) methods, on $\\texttt{VGGSOUND-2C}$ and $\\texttt{KINETICS-2C}$, offer minimal improvements in performance under bimodal corruptions. We further propose $\\texttt{AV2C}$, a simple TTA approach enabling on-the-fly cross-modal fusion by penalizing high-entropy samples, which achieves improvements on $\\texttt{VGGSOUND-2C}$. We hope that $\\texttt{AVROBUSTBENCH}$ will steer the development of more effective and robust audio-visual TTA approaches. Our code is available $\\href{https://github.com/sarthaxxxxx/AV-C-Robustness-Benchmark}{here}$.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†$\\texttt{AVROBUSTBENCH}$ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°éŸ³è§†é¢‘(Audio-Visual)è¯†åˆ«æ¨¡å‹åœ¨æµ‹è¯•æ—¶(Test-Time)é¢å¯¹åˆ†å¸ƒåç§»(Distributional Shifts)é²æ£’æ€§çš„å…¨é¢åŸºå‡†ã€‚é’ˆå¯¹ç°æœ‰åŸºå‡†å¤šå…³æ³¨å•ä¸€æ¨¡æ€çš„å±€é™æ€§ï¼Œè¯¥åŸºå‡†åŒ…å«äº†$\\texttt{AUDIOSET-2C}$ã€$\\texttt{VGGSOUND-2C}$ç­‰å››ä¸ªæ•°æ®é›†ï¼Œå¹¶å¼•å…¥äº†75ç§åŒæ—¶å‘ç”Ÿä¸”ç›¸å…³çš„åŒæ¨¡æ€éŸ³è§†é¢‘æŸå(Bimodal Audio-Visual Corruptions)ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œéšç€æŸåä¸¥é‡ç¨‹åº¦å¢åŠ ï¼Œæœ€å…ˆè¿›çš„ç›‘ç£å’Œè‡ªç›‘ç£éŸ³è§†é¢‘æ¨¡å‹çš„é²æ£’æ€§å‡æ˜¾è‘—ä¸‹é™ï¼Œä¸”ç°æœ‰çš„åœ¨çº¿æµ‹è¯•æ—¶è‡ªé€‚åº”(TTA)æ–¹æ³•æ”¹è¿›æœ‰é™ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸º$\\texttt{AV2C}$çš„æ–°å‹TTAæ–¹æ³•ï¼Œé€šè¿‡æƒ©ç½šé«˜ç†µæ ·æœ¬æ¥å®ç°å³æ—¶è·¨æ¨¡æ€èåˆï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æ€§èƒ½æå‡ã€‚è¯¥åŸºå‡†çš„å‘å¸ƒæ—¨åœ¨å¼•å¯¼å­¦æœ¯ç•Œå¼€å‘æ›´æœ‰æ•ˆã€æ›´é²æ£’çš„éŸ³è§†é¢‘å¤„ç†æŠ€æœ¯ä¸è‡ªé€‚åº”ç®—æ³•ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Track on Datasets and Benchmarks",
      "pdf_url": "https://arxiv.org/pdf/2506.00358v3",
      "published_date": "2025-05-31 02:56:07 UTC",
      "updated_date": "2025-10-24 18:54:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:05:04.356585+00:00"
    },
    {
      "arxiv_id": "2506.00356v1",
      "title": "Exploring the Performance of Perforated Backpropagation through Further Experiments",
      "title_zh": "é€šè¿‡è¿›ä¸€æ­¥å®éªŒæ¢ç´¢ç©¿å­”åå‘ä¼ æ’­çš„æ€§èƒ½",
      "authors": [
        "Rorry Brenner",
        "Evan Davis",
        "Rushi Chaudhari",
        "Rowan Morse",
        "Jingyao Chen",
        "Xirui Liu",
        "Zhaoyi You",
        "Laurent Itti"
      ],
      "abstract": "Perforated Backpropagation is a neural network optimization technique based on modern understanding of the computational importance of dendrites within biological neurons. This paper explores further experiments from the original publication, generated from a hackathon held at the Carnegie Mellon Swartz Center in February 2025. Students and local Pittsburgh ML practitioners were brought together to experiment with the Perforated Backpropagation algorithm on the datasets and models which they were using for their projects. Results showed that the system could enhance their projects, with up to 90% model compression without negative impact on accuracy, or up to 16% increased accuracy of their original models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº† Perforated Backpropagation çš„æ€§èƒ½è¡¨ç°ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¯¹ç”Ÿç‰©ç¥ç»å…ƒæ ‘çª(dendrites)è®¡ç®—é‡è¦æ€§ç°ä»£ç†è§£çš„ç¥ç»ç½‘ç»œä¼˜åŒ–æŠ€æœ¯ã€‚ç ”ç©¶é€šè¿‡å¡å†…åŸºæ¢…éš†å¤§å­¦ Swartz Center ä¸¾åŠçš„é»‘å®¢æ¾æ´»åŠ¨ï¼Œæ”¶é›†äº†å¤šä½å¼€å‘è€…åœ¨ä¸åŒå®é™…é¡¹ç›®å’Œæ•°æ®é›†ä¸Šåº”ç”¨è¯¥ç®—æ³•çš„å®éªŒæ•°æ®ã€‚ç»“æœè¡¨æ˜ï¼ŒPerforated Backpropagation èƒ½å¤Ÿæ˜¾è‘—ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œåœ¨ä¸äº§ç”Ÿè´Ÿé¢å‡†ç¡®ç‡å½±å“çš„å‰æä¸‹å®ç°é«˜è¾¾ 90% çš„æ¨¡å‹å‹ç¼©(model compression)ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•è¿˜èƒ½å°†éƒ¨åˆ†åŸå§‹æ¨¡å‹çš„å‡†ç¡®ç‡(accuracy)æå‡å¤šè¾¾ 16%ã€‚è¿™äº›å¤šæ ·åŒ–çš„å®éªŒç»“æœå……åˆ†å±•ç¤ºäº† Perforated Backpropagation åœ¨æé«˜è®¡ç®—æ•ˆç‡å’Œå¢å¼ºæ¨¡å‹é¢„æµ‹èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 7 figures, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2506.00356v1",
      "published_date": "2025-05-31 02:52:17 UTC",
      "updated_date": "2025-05-31 02:52:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:05:07.809462+00:00"
    },
    {
      "arxiv_id": "2507.19492v1",
      "title": "ChartGen: Scaling Chart Understanding Via Code-Guided Synthetic Chart Generation",
      "title_zh": "ChartGenï¼šé€šè¿‡ä»£ç å¼•å¯¼çš„åˆæˆå›¾è¡¨ç”Ÿæˆæå‡å›¾è¡¨ç†è§£èƒ½åŠ›",
      "authors": [
        "Jovana Kondic",
        "Pengyuan Li",
        "Dhiraj Joshi",
        "Zexue He",
        "Shafiq Abedin",
        "Jennifer Sun",
        "Ben Wiesel",
        "Eli Schwartz",
        "Ahmed Nassar",
        "Bo Wu",
        "Assaf Arbelle",
        "Aude Oliva",
        "Dan Gutfreund",
        "Leonid Karlinsky",
        "Rogerio Feris"
      ],
      "abstract": "Chart-to-code reconstruction -- the task of recovering executable plotting scripts from chart images -- provides important insights into a model's ability to ground data visualizations in precise, machine-readable form. Yet many existing multimodal benchmarks largely focus primarily on answering questions about charts or summarizing them. To bridge this gap, we present ChartGen, a fully-automated pipeline for code-guided synthetic chart generation. Starting from seed chart images, ChartGen (i) prompts a vision-language model (VLM) to reconstruct each image into a python script, and (ii) iteratively augments that script with a code-oriented large language model (LLM). Using ChartGen, we create 222.5K unique chart-image code pairs from 13K seed chart images, and present an open-source synthetic chart dataset covering 27 chart types, 11 plotting libraries, and multiple data modalities (image, code, text, CSV, DocTags). From this corpus, we curate a held-out chart-to-code evaluation subset of 4.3K chart image-code pairs, and evaluate six open-weight VLMs (3B - 26B parameters), highlighting substantial room for progress. We release the pipeline, prompts, and the dataset to help accelerate efforts towards robust chart understanding and vision-conditioned code generation: https://github.com/SD122025/ChartGen/",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾è¡¨åˆ°ä»£ç é‡å»º (Chart-to-code reconstruction) ä»»åŠ¡æå‡ºäº† ChartGenï¼Œè¿™æ˜¯ä¸€ç§å…¨è‡ªåŠ¨çš„ã€ä»£ç å¼•å¯¼çš„åˆæˆå›¾è¡¨ç”Ÿæˆæµæ°´çº¿ã€‚ChartGen é€šè¿‡æç¤ºè§†è§‰è¯­è¨€æ¨¡å‹ (VLM) å°†ç§å­å›¾åƒé‡å»ºä¸º Python è„šæœ¬ï¼Œå¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLM) é’ˆå¯¹ä»£ç è¿›è¡Œè¿­ä»£å¢å¼ºï¼Œä»è€Œæ„å»ºäº†åŒ…å« 22.25 ä¸‡ä¸ªç‹¬ç‰¹å›¾è¡¨-ä»£ç å¯¹çš„å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº† 27 ç§å›¾è¡¨ç±»å‹å’Œ 11 ä¸ªç»˜å›¾åº“ï¼Œå¹¶åŒ…å«å›¾åƒã€ä»£ç ã€æ–‡æœ¬åŠ CSV ç­‰å¤šç§æ•°æ®æ¨¡æ€ã€‚é€šè¿‡å¯¹å…­ç§å‚æ•°é‡åœ¨ 3B è‡³ 26B ä¹‹é—´çš„å¼€æº VLM è¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶ç»“æœæ˜¾ç¤ºå½“å‰æ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸Šä»æœ‰æ˜¾è‘—çš„æå‡ç©ºé—´ã€‚è¯¥é¡¹ç›®å¼€æºäº†å®Œæ•´çš„æµæ°´çº¿ã€æç¤ºè¯å’Œæ•°æ®é›†ï¼Œæ—¨åœ¨æ¨åŠ¨ç¨³å¥çš„å›¾è¡¨ç†è§£ä¸è§†è§‰æ¡ä»¶ä¸‹çš„ä»£ç ç”Ÿæˆ (Vision-conditioned code generation) æŠ€æœ¯çš„å‘å±•ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.19492v1",
      "published_date": "2025-05-31 02:35:38 UTC",
      "updated_date": "2025-05-31 02:35:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:05:14.785628+00:00"
    },
    {
      "arxiv_id": "2506.00352v1",
      "title": "Enabling Secure and Ephemeral AI Workloads in Data Mesh Environments",
      "title_zh": "åœ¨æ•°æ®ç½‘æ ¼ç¯å¢ƒä¸­å®ç°å®‰å…¨ä¸”å³ç”¨å³å¼ƒçš„ AI å·¥ä½œè´Ÿè½½",
      "authors": [
        "Chinkit Patel",
        "Kee Siong Ng"
      ],
      "abstract": "Many large enterprises that operate highly governed and complex ICT environments have no efficient and effective way to support their Data and AI teams in rapidly spinning up and tearing down self-service data and compute infrastructure, to experiment with new data analytic tools, and deploy data products into operational use. This paper proposes a key piece of the solution to the overall problem, in the form of an on-demand self-service data-platform infrastructure to empower de-centralised data teams to build data products on top of centralised templates, policies and governance. The core innovation is an efficient method to leverage immutable container operating systems and infrastructure-as-code methodologies for creating, from scratch, vendor-neutral and short-lived Kubernetes clusters on-premises and in any cloud environment. Our proposed approach can serve as a repeatable, portable and cost-efficient alternative or complement to commercial Platform-as-a-Service (PaaS) offerings, and this is particularly important in supporting interoperability in complex data mesh environments with a mix of modern and legacy compute infrastructure.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹ä¼ä¸šåœ¨å¤æ‚ä¸”é«˜åº¦æ²»ç†çš„ICTç¯å¢ƒä¸­ï¼Œæ•°æ®ä¸AIå›¢é˜Ÿéš¾ä»¥å¿«é€Ÿæ„å»ºå’Œæ‹†é™¤è‡ªæœåŠ¡åŸºç¡€è®¾æ–½çš„éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ”¯æŒå®‰å…¨ä¸”ä¸´æ—¶æ€§(Ephemeral)å·¥ä½œè´Ÿè½½çš„åˆ›æ–°æ–¹æ¡ˆã€‚å…¶æ ¸å¿ƒè´¡çŒ®åœ¨äºå¼€å‘äº†ä¸€ç§æŒ‰éœ€è‡ªæœåŠ¡æ•°æ®å¹³å°åŸºç¡€è®¾æ–½ï¼Œä½¿å»ä¸­å¿ƒåŒ–å›¢é˜Ÿèƒ½å¤Ÿåœ¨Data Meshæ¶æ„ä¸‹åˆ©ç”¨é›†ä¸­å¼æ¨¡æ¿å’Œæ²»ç†ç­–ç•¥é«˜æ•ˆæ„å»ºæ•°æ®äº§å“ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆä¸å¯å˜å®¹å™¨æ“ä½œç³»ç»Ÿ(Immutable container operating systems)ä¸åŸºç¡€è®¾æ–½å³ä»£ç (Infrastructure-as-Code)æŠ€æœ¯ï¼Œå®ç°äº†åœ¨æœ¬åœ°æˆ–ä»»æ„äº‘ç«¯å¿«é€Ÿéƒ¨ç½²å‚å•†ä¸­ç«‹çš„çŸ­å¯¿å‘½Kubernetesé›†ç¾¤ã€‚ä½œä¸ºå•†ä¸šå¹³å°å³æœåŠ¡(PaaS)çš„ä¸€ç§å¯é‡å¤ä¸”ä½æˆæœ¬çš„æ›¿ä»£æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆæ˜¾è‘—å¢å¼ºäº†å¤æ‚ç¯å¢ƒä¸­ç°ä»£ä¸é—ç•™è®¡ç®—åŸºç¡€è®¾æ–½ä¹‹é—´çš„äº’æ“ä½œæ€§ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºä¼ä¸šåœ¨ç¡®ä¿æ²»ç†ä¸å®‰å…¨çš„å‰æä¸‹ï¼Œå¿«é€Ÿå®éªŒå¹¶éƒ¨ç½²AIå·¥ä½œè´Ÿè½½æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.DC",
      "comment": "52 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.00352v1",
      "published_date": "2025-05-31 02:30:22 UTC",
      "updated_date": "2025-05-31 02:30:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:05:48.899321+00:00"
    },
    {
      "arxiv_id": "2506.00348v1",
      "title": "Beyond Winning: Margin of Victory Relative to Expectation Unlocks Accurate Skill Ratings",
      "title_zh": "è¶…è¶Šèƒœè´Ÿï¼šç›¸å¯¹äºé¢„æœŸçš„èƒœåˆ†å·®åŠ©åŠ›ç²¾å‡†æŠ€èƒ½è¯„åˆ†",
      "authors": [
        "Shivam Shorewala",
        "Zihao Yang"
      ],
      "abstract": "Knowledge of accurate relative skills in any competitive system is essential, but foundational approaches such as ELO discard extremely relevant performance data by concentrating exclusively on binary outcomes. While margin of victory (MOV) extensions exist, they often lack a definitive method for incorporating this information. We introduce Margin of Victory Differential Analysis (MOVDA), a framework that enhances traditional rating systems by using the deviation between the true MOV and a $\\textit{modeled expectation}$. MOVDA learns a domain-specific, non-linear function (a scaled hyperbolic tangent that captures saturation effects and home advantage) to predict expected MOV based on rating differentials. Crucially, the $\\textit{difference}$ between the true and expected MOV provides a subtle and weighted signal for rating updates, highlighting informative deviations in all levels of contests. Extensive experiments on professional NBA basketball data (from 2013 to 2023, with 13,619 games) show that MOVDA significantly outperforms standard ELO and Bayesian baselines. MOVDA reduces Brier score prediction error by $1.54\\%$ compared to TrueSkill, increases outcome accuracy by $0.58\\%$, and most importantly accelerates rating convergence by $13.5\\%$, while maintaining the computational efficiency of the original ELO updates. MOVDA offers a theoretically motivated, empirically superior, and computationally lean approach to integrating performance magnitude into skill rating for competitive environments like the NBA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Margin of Victory Differential Analysis (MOVDA)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨èƒœåˆ†å·®ä¿¡æ¯æå‡ç«äº‰ç³»ç»Ÿä¸­çš„æŠ€èƒ½è¯„åˆ†å‡†ç¡®æ€§ã€‚é’ˆå¯¹ELOç­‰ä¼ ç»Ÿæ–¹æ³•ä»…ä¾èµ–äºŒå…ƒèƒœè´Ÿç»“æœè€Œå¿½ç•¥è¡¨ç°æ•°æ®çš„å±€é™æ€§ï¼ŒMOVDAå¼•å…¥äº†çœŸå®èƒœåˆ†å·®(Margin of Victory, MOV)ä¸æ¨¡å‹é¢„æœŸå€¼ä¹‹é—´çš„å·®å¼‚ä½œä¸ºè¯„åˆ†æ›´æ–°ä¿¡å·ã€‚è¯¥æ¡†æ¶é‡‡ç”¨é¢†åŸŸç‰¹å®šçš„éçº¿æ€§å‡½æ•°æ•æ‰é¥±å’Œæ•ˆåº”å’Œä¸»åœºä¼˜åŠ¿ï¼Œä»è€Œå®ç°å¯¹æ¯”èµ›ç»“æœæ›´ç²¾ç»†çš„å»ºæ¨¡ã€‚å¯¹2013è‡³2023å¹´NBAæ•°æ®çš„å®éªŒè¡¨æ˜ï¼ŒMOVDAåœ¨Brier scoreé¢„æµ‹è¯¯å·®ä¸Šä¼˜äºTrueSkillç­‰åŸºå‡†æ¨¡å‹ï¼Œä¸”è¯„åˆ†æ”¶æ•›é€Ÿåº¦æé«˜äº†13.5%ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒELOè®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹ç«äº‰æ€§ç¯å¢ƒä¸­ä¸ªä½“æˆ–å›¢é˜ŸæŠ€èƒ½çš„è¯„ä¼°èƒ½åŠ›ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00348v1",
      "published_date": "2025-05-31 02:16:51 UTC",
      "updated_date": "2025-05-31 02:16:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:05:32.990564+00:00"
    },
    {
      "arxiv_id": "2506.00344v1",
      "title": "Efficient Latent Semantic Clustering for Scaling Test-Time Computation of LLMs",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ—¶è®¡ç®—æ‰©å±•çš„é«˜æ•ˆæ½œè¯­ä¹‰èšç±»",
      "authors": [
        "Sungjae Lee",
        "Hoyoung Kim",
        "Jeongyeon Hwang",
        "Eunhyeok Park",
        "Jungseul Ok"
      ],
      "abstract": "Scaling test-time computation--generating and analyzing multiple or sequential outputs for a single input--has become a promising strategy for improving the reliability and quality of large language models (LLMs), as evidenced by advances in uncertainty quantification and multi-step reasoning. A key shared component is semantic clustering, which groups outputs that differ in form but convey the same meaning. Semantic clustering enables estimation of the distribution over the semantics of outputs and helps avoid redundant exploration of reasoning paths. However, existing approaches typically rely on external models, which introduce substantial computational overhead and often fail to capture context-aware semantics. We propose Latent Semantic Clustering (LSC), a lightweight and context-sensitive method that leverages the generator LLM's internal hidden states for clustering, eliminating the need for external models. Our extensive experiment across various LLMs and datasets shows that LSC significantly improves the computational efficiency of test-time scaling while maintaining or exceeding the performance of existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ‰©å±•æ¨ç†æ—¶é—´è®¡ç®—(scaling test-time computation)åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹(LLMs)å¯é æ€§æ–¹é¢çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸ç¡®å®šæ€§é‡åŒ–å’Œå¤šæ­¥æ¨ç†ä»»åŠ¡ä¸­ã€‚è¯­ä¹‰èšç±»(semantic clustering)æ˜¯è¯¥è¿‡ç¨‹çš„æ ¸å¿ƒï¼Œç”¨äºå½’ç±»å«ä¹‰ç›¸åŒä½†å½¢å¼ä¸åŒçš„è¾“å‡ºï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å¤–éƒ¨æ¨¡å‹ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å·¨å¤§ä¸”ç¼ºä¹ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œä½œè€…æå‡ºäº†æ½œè¯­ä¹‰èšç±»(Latent Semantic Clustering, LSC)ï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§ä¸”å…·æœ‰ä¸Šä¸‹æ–‡æ•æ„Ÿæ€§çš„èšç±»æ–¹æ³•ã€‚LSCç›´æ¥åˆ©ç”¨ç”Ÿæˆæ¨¡å‹å†…éƒ¨çš„éšè—çŠ¶æ€(hidden states)è¿›è¡Œèšç±»ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹å¤–éƒ¨æ¨¡å‹çš„ä¾èµ–ã€‚åœ¨å¤šç§LLMså’Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLSCåœ¨æ˜¾è‘—æé«˜æµ‹è¯•æ—¶æ‰©å±•(test-time scaling)è®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œå…¶æ€§èƒ½è¾¾åˆ°æˆ–è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºé«˜æ•ˆè¯„ä¼°è¾“å‡ºè¯­ä¹‰åˆ†å¸ƒå¹¶é¿å…å†—ä½™æ¨ç†è·¯å¾„æ¢ç´¢æä¾›äº†æ›´å…·æ‰©å±•æ€§çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00344v1",
      "published_date": "2025-05-31 02:08:32 UTC",
      "updated_date": "2025-05-31 02:08:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:05:40.591156+00:00"
    },
    {
      "arxiv_id": "2506.00335v2",
      "title": "Recover Experimental Data with Selection Bias using Counterfactual Logic",
      "title_zh": "åŸºäºåäº‹å®é€»è¾‘æ¢å¤å­˜åœ¨é€‰æ‹©åå·®çš„å®éªŒæ•°æ®",
      "authors": [
        "Jingyang He",
        "Shuai Wang",
        "Ang Li"
      ],
      "abstract": "Selection bias, arising from the systematic inclusion or exclusion of certain samples, poses a significant challenge to the validity of causal inference. While Bareinboim et al. introduced methods for recovering unbiased observational and interventional distributions from biased data using partial external information, the complexity of the backdoor adjustment and the method's strong reliance on observational data limit its applicability in many practical settings. In this paper, we formally discover the recoverability of $P(Y^*_{x^*})$ under selection bias with experimental data. By explicitly constructing counterfactual worlds via Structural Causal Models (SCMs), we analyze how selection mechanisms in the observational world propagate to the counterfactual domain. We derive a complete set of graphical and theoretical criteria to determine that the experimental distribution remain unaffected by selection bias. Furthermore, we propose principled methods for leveraging partially unbiased observational data to recover $P(Y^*_{x^*})$ from biased experimental datasets. Simulation studies replicating realistic research scenarios demonstrate the practical utility of our approach, offering concrete guidance for mitigating selection bias in applied causal inference.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å› æœæ¨ç†ä¸­å› æ ·æœ¬ç³»ç»Ÿæ€§çº³å…¥æˆ–æ’é™¤è€Œäº§ç”Ÿçš„é€‰æ‹©æ€§åå·®(Selection bias)é—®é¢˜ï¼Œé‡ç‚¹ç ”ç©¶äº†å¦‚ä½•åˆ©ç”¨åäº‹å®é€»è¾‘(Counterfactual Logic)ä»å—åæ•°æ®ä¸­æ¢å¤å®éªŒåˆ†å¸ƒã€‚é€šè¿‡æ„å»ºç»“æ„å› æœæ¨¡å‹(SCMs)å»ºç«‹åäº‹å®ä¸–ç•Œï¼Œè®ºæ–‡è¯¦ç»†åˆ†æäº†è§‚æµ‹ä¸–ç•Œä¸­çš„é€‰æ‹©æœºåˆ¶å¦‚ä½•ä¼ æ’­è‡³åäº‹å®é¢†åŸŸï¼Œå¹¶æ¨å¯¼å‡ºä¸€å¥—å®Œæ•´çš„å›¾å½¢å’Œç†è®ºæ ‡å‡†ï¼Œç”¨äºåˆ¤å®šå®éªŒåˆ†å¸ƒ$P(Y^*_{x^*})$æ˜¯å¦å—é€‰æ‹©åå·®å½±å“ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†åˆ©ç”¨éƒ¨åˆ†æ— åè§‚æµ‹æ•°æ®ä»æœ‰åå®éªŒæ•°æ®é›†ä¸­æ¢å¤ç›®æ ‡åˆ†å¸ƒçš„åŸåˆ™æ€§æ–¹æ³•ã€‚ä»¿çœŸå®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿç°å®ç ”ç©¶åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ä¸å®ç”¨ä»·å€¼ï¼Œä¸ºç¼“è§£åº”ç”¨å› æœæ¨ç†ä¸­çš„é€‰æ‹©åå·®æä¾›äº†å…·ä½“çš„ç†è®ºæŒ‡å¯¼å’Œæ“ä½œå·¥å…·ã€‚",
      "categories": [
        "stat.ME",
        "cs.AI"
      ],
      "primary_category": "stat.ME",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00335v2",
      "published_date": "2025-05-31 01:23:39 UTC",
      "updated_date": "2025-06-04 17:00:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:05:43.092332+00:00"
    },
    {
      "arxiv_id": "2506.00329v2",
      "title": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation",
      "title_zh": "Foresightï¼šé¢å‘åŠ é€Ÿä¸”é«˜è´¨é‡æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„è‡ªé€‚åº”å±‚å¤ç”¨æŠ€æœ¯",
      "authors": [
        "Muhammad Adnan",
        "Nithesh Kurella",
        "Akhil Arunkumar",
        "Prashant J. Nair"
      ],
      "abstract": "Diffusion Transformers (DiTs) achieve state-of-the-art results in text-to-image, text-to-video generation, and editing. However, their large model size and the quadratic cost of spatial-temporal attention over multiple denoising steps make video generation computationally expensive. Static caching mitigates this by reusing features across fixed steps but fails to adapt to generation dynamics, leading to suboptimal trade-offs between speed and quality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces computational redundancy across denoising steps while preserving baseline performance. Foresight dynamically identifies and reuses DiT block outputs for all layers across steps, adapting to generation parameters such as resolution and denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and CogVideoX, Foresight achieves up to \\latencyimprv end-to-end speedup, while maintaining video quality. The source code of Foresight is available at \\href{https://github.com/STAR-Laboratory/foresight}{https://github.com/STAR-Laboratory/foresight}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£è½¬æ¢å™¨(Diffusion Transformers, DiTs)åœ¨è§†é¢‘ç”Ÿæˆä¸­é¢ä¸´çš„é«˜æ˜‚è®¡ç®—æˆæœ¬å’Œæ¨ç†å»¶è¿Ÿé—®é¢˜ï¼Œæå‡ºäº†Foresightï¼Œä¸€ç§è‡ªé€‚åº”çš„å±‚é‡ç”¨(Adaptive Layer Reuse)æŠ€æœ¯ã€‚Foresighté€šè¿‡åŠ¨æ€è¯†åˆ«å¹¶è·¨æ­¥éª¤é‡ç”¨DiTå—çš„è¾“å‡ºï¼Œæ—¨åœ¨å‡å°‘å»å™ªè¿‡ç¨‹ä¸­çš„è®¡ç®—å†—ä½™ï¼ŒåŒæ—¶ä¿æŒåŸæœ‰çš„æ¨¡å‹æ€§èƒ½ã€‚è¯¥æŠ€æœ¯èƒ½å¤Ÿæ ¹æ®ç”Ÿæˆåˆ†è¾¨ç‡å’Œå»å™ªè®¡åˆ’(denoising schedules)ç­‰å‚æ•°è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ï¼Œä»è€Œåœ¨ä¸åŒé…ç½®ä¸‹ä¼˜åŒ–ç”Ÿæˆæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒForesightåœ¨OpenSoraã€Latteå’ŒCogVideoXç­‰å¤šä¸ªä¸»æµæ¨¡å‹ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„ç«¯åˆ°ç«¯åŠ é€Ÿæ•ˆæœï¼Œä¸”èƒ½æœ‰æ•ˆç»´æŒè§†é¢‘ç”Ÿæˆè´¨é‡ã€‚è¯¥æ–¹æ³•æˆåŠŸå…‹æœäº†ä¼ ç»Ÿé™æ€ç¼“å­˜æŠ€æœ¯æ— æ³•é€‚åº”ç”ŸæˆåŠ¨æ€æ€§çš„å±€é™ï¼Œä¸ºå®ç°é«˜æ•ˆä¸”é«˜è´¨é‡çš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the 39th Conference on Neural Information Processing Systems (NeurIPS), 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00329v2",
      "published_date": "2025-05-31 00:52:17 UTC",
      "updated_date": "2025-09-22 19:20:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:05:52.591989+00:00"
    },
    {
      "arxiv_id": "2506.00328v3",
      "title": "BASIL: Best-Action Symbolic Interpretable Learning for Evolving Compact RL Policies",
      "title_zh": "BASILï¼šé¢å‘ç´§å‡‘å‹å¼ºåŒ–å­¦ä¹ ç­–ç•¥æ¼”åŒ–çš„æœ€ä½³åŠ¨ä½œç¬¦å·å¯è§£é‡Šå­¦ä¹ ",
      "authors": [
        "Kourosh Shahnazari",
        "Seyed Moein Ayyoubzadeh",
        "Mohammadali Keshtparvar"
      ],
      "abstract": "The quest for interpretable reinforcement learning is a grand challenge for the deployment of autonomous decision-making systems in safety-critical applications. Modern deep reinforcement learning approaches, while powerful, tend to produce opaque policies that compromise verification, reduce transparency, and impede human oversight. To address this, we introduce BASIL (Best-Action Symbolic Interpretable Learning), a systematic approach for generating symbolic, rule-based policies via online evolutionary search with quality-diversity (QD) optimization. BASIL represents policies as ordered lists of symbolic predicates over state variables, ensuring full interpretability and tractable policy complexity. By using a QD archive, the methodology in the proposed study encourages behavioral and structural diversity between top-performing solutions, while a complexity-aware fitness encourages the synthesis of compact representations. The evolutionary system supports the use of exact constraints for rule count and system adaptability for balancing transparency with expressiveness. Empirical comparisons with three benchmark tasks CartPole-v1, MountainCar-v0, and Acrobot-v1 show that BASIL consistently synthesizes interpretable controllers with compact representations comparable to deep reinforcement learning baselines. Herein, this article introduces a new interpretable policy synthesis method that combines symbolic expressiveness, evolutionary diversity, and online learning through a unifying framework.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep Reinforcement Learning)æ¨¡å‹ä¸é€æ˜ã€éš¾ä»¥éªŒè¯çš„é—®é¢˜ï¼Œæå‡ºäº†BASIL (Best-Action Symbolic Interpretable Learning) æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆå¯è§£é‡Šä¸”ç´§å‡‘çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ã€‚BASIL å°†ç­–ç•¥è¡¨ç¤ºä¸ºåŸºäºçŠ¶æ€å˜é‡çš„æœ‰åºç¬¦å·è°“è¯(Symbolic Predicates)åˆ—è¡¨ï¼Œé€šè¿‡ç»“åˆè´¨é‡å¤šæ ·æ€§(Quality-Diversity)ä¼˜åŒ–çš„åœ¨çº¿è¿›åŒ–æœç´¢(Online Evolutionary Search)æ¥åˆæˆè§„åˆ™ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è´¨é‡å¤šæ ·æ€§å­˜æ¡£é¼“åŠ±é«˜æ€§èƒ½è§£åœ¨è¡Œä¸ºå’Œç»“æ„ä¸Šçš„å¤šæ ·æ€§ï¼Œå¹¶å¼•å…¥å¤æ‚åº¦æ„ŸçŸ¥çš„é€‚åº”åº¦(Complexity-aware fitness)ä»¥ç¡®ä¿ç”Ÿæˆçš„è¡¨ç¤ºå½¢å¼è¶³å¤Ÿç®€æ´ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿæ”¯æŒå¯¹è§„åˆ™æ•°é‡çš„ç²¾ç¡®çº¦æŸï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡ç³»ç»Ÿçš„é€æ˜åº¦ä¸è¡¨è¾¾èƒ½åŠ›ã€‚åœ¨ CartPole-v1ã€MountainCar-v0 å’Œ Acrobot-v1 ç­‰åŸºå‡†ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒBASIL èƒ½å¤ŸæŒç»­åˆæˆå‡ºæ€§èƒ½ä¸æ·±åº¦å¼ºåŒ–å­¦ä¹ åŸºçº¿æ¨¡å‹ç›¸å½“çš„ç´§å‡‘å‹è§£é‡Šæ€§æ§åˆ¶å™¨ã€‚è¿™ä¸€æ–°æ–¹æ³•é€šè¿‡ç»Ÿä¸€çš„æ¡†æ¶å°†ç¬¦å·è¡¨è¾¾èƒ½åŠ›ã€è¿›åŒ–å¤šæ ·æ€§å’Œåœ¨çº¿å­¦ä¹ ç›¸ç»“åˆï¼Œä¸ºå®‰å…¨å…³é”®åº”ç”¨ä¸­çš„è‡ªä¸»å†³ç­–ç³»ç»Ÿæä¾›äº†é€æ˜ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00328v3",
      "published_date": "2025-05-31 00:47:24 UTC",
      "updated_date": "2025-06-11 06:03:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:06:07.042644+00:00"
    },
    {
      "arxiv_id": "2506.00327v1",
      "title": "Latent Guidance in Diffusion Models for Perceptual Evaluations",
      "title_zh": "æ‰©æ•£æ¨¡å‹ä¸­ç”¨äºæ„ŸçŸ¥è¯„ä¼°çš„æ½œå¼•å¯¼",
      "authors": [
        "Shreshth Saini",
        "Ru-Ling Liao",
        "Yan Ye",
        "Alan C. Bovik"
      ],
      "abstract": "Despite recent advancements in latent diffusion models that generate high-dimensional image data and perform various downstream tasks, there has been little exploration into perceptual consistency within these models on the task of No-Reference Image Quality Assessment (NR-IQA). In this paper, we hypothesize that latent diffusion models implicitly exhibit perceptually consistent local regions within the data manifold. We leverage this insight to guide on-manifold sampling using perceptual features and input measurements. Specifically, we propose Perceptual Manifold Guidance (PMG), an algorithm that utilizes pretrained latent diffusion models and perceptual quality features to obtain perceptually consistent multi-scale and multi-timestep feature maps from the denoising U-Net. We empirically demonstrate that these hyperfeatures exhibit high correlation with human perception in IQA tasks. Our method can be applied to any existing pretrained latent diffusion model and is straightforward to integrate. To the best of our knowledge, this paper is the first work on guiding diffusion model with perceptual features for NR-IQA. Extensive experiments on IQA datasets show that our method, LGDM, achieves state-of-the-art performance, underscoring the superior generalization capabilities of diffusion models for NR-IQA tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ (Latent Diffusion Models) åœ¨æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä»· (No-Reference Image Quality Assessment, NR-IQA) ä»»åŠ¡ä¸­çš„æ„ŸçŸ¥ä¸€è‡´æ€§ã€‚åŸºäºæ¨¡å‹åœ¨æ•°æ®æµå½¢ä¸­éšå«æ„ŸçŸ¥ä¸€è‡´æ€§åŒºåŸŸçš„å‡è®¾ï¼Œä½œè€…æå‡ºäº†æ„ŸçŸ¥æµå½¢å¼•å¯¼ (Perceptual Manifold Guidance, PMG) ç®—æ³•ï¼Œé€šè¿‡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å»å™ª U-Net æå–å¤šå°ºåº¦ä¸”å¤šæ—¶é—´æ­¥çš„ç‰¹å¾å›¾ã€‚è¿™äº›è¢«ç§°ä¸ºè¶…ç‰¹å¾ (hyperfeatures) çš„ä¿¡æ¯è¢«è¯æ˜ä¸äººç±»æ„ŸçŸ¥é«˜åº¦ç›¸å…³ï¼Œä¸”è¯¥æ–¹æ³•å¯ç›´æ¥åº”ç”¨äºä»»ä½•ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚ä½œä¸ºé¦–ä¸ªåˆ©ç”¨æ„ŸçŸ¥ç‰¹å¾å¼•å¯¼æ‰©æ•£æ¨¡å‹è¿›è¡Œ NR-IQA çš„å·¥ä½œï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³• (LGDM) åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿› (State-of-the-art) çš„æ€§èƒ½ï¼Œå……åˆ†å±•ç¤ºäº†æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒè´¨é‡è¯„ä»·é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "24 Pages, 7 figures, 10 Tables",
      "pdf_url": "https://arxiv.org/pdf/2506.00327v1",
      "published_date": "2025-05-31 00:41:59 UTC",
      "updated_date": "2025-05-31 00:41:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:06:00.093656+00:00"
    },
    {
      "arxiv_id": "2506.00322v1",
      "title": "dpmm: Differentially Private Marginal Models, a Library for Synthetic Tabular Data Generation",
      "title_zh": "dpmmï¼šé¢å‘åˆæˆè¡¨æ ¼æ•°æ®ç”Ÿæˆçš„å·®åˆ†éšç§è¾¹ç¼˜æ¨¡å‹åº“",
      "authors": [
        "Sofiane Mahiou",
        "Amir Dizche",
        "Reza Nazari",
        "Xinmin Wu",
        "Ralph Abbey",
        "Jorge Silva",
        "Georgi Ganev"
      ],
      "abstract": "We propose dpmm, an open-source library for synthetic data generation with Differentially Private (DP) guarantees. It includes three popular marginal models -- PrivBayes, MST, and AIM -- that achieve superior utility and offer richer functionality compared to alternative implementations. Additionally, we adopt best practices to provide end-to-end DP guarantees and address well-known DP-related vulnerabilities. Our goal is to accommodate a wide audience with easy-to-install, highly customizable, and robust model implementations.\n  Our codebase is available from https://github.com/sassoftware/dpmm.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† dpmmï¼Œä¸€ä¸ªç”¨äºç”Ÿæˆå…·æœ‰å·®åˆ†éšç§ (Differentially Private, DP) ä¿è¯çš„åˆæˆè¡¨æ ¼æ•°æ®çš„å¼€æºåº“ã€‚è¯¥åº“é›†æˆäº†ä¸‰ç§æµè¡Œçš„è¾¹ç¼˜æ¨¡å‹ (Marginal Models)ï¼Œå³ PrivBayesã€MST å’Œ AIMï¼Œä¸ç°æœ‰æ›¿ä»£å®ç°ç›¸æ¯”ï¼Œå®ƒä»¬åœ¨æ•ˆç”¨å’ŒåŠŸèƒ½ä¸°å¯Œåº¦ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚dpmm é‡‡ç”¨äº†æœ€ä½³å®è·µä»¥ç¡®ä¿ç«¯åˆ°ç«¯ (end-to-end) çš„ DP ä¿è¯ï¼Œå¹¶é’ˆå¯¹å·²çŸ¥çš„å·®åˆ†éšç§å®‰å…¨æ¼æ´è¿›è¡Œäº†ä¼˜åŒ–ã€‚å…¶å¼€å‘ç›®æ ‡æ˜¯ä¸ºå¹¿æ³›çš„å—ä¼—æä¾›æ˜“äºå®‰è£…ã€é«˜åº¦å¯å®šåˆ¶ä¸”é²æ£’çš„æ¨¡å‹å®ç°æ–¹æ¡ˆã€‚ç›®å‰ï¼Œè¯¥é¡¹ç›®çš„å®Œæ•´ä»£ç å·²åœ¨ GitHub ä¸Šå…¬å¼€å‘å¸ƒï¼Œä¸ºéšç§ä¿æŠ¤æ•°æ®åˆæˆé¢†åŸŸæä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted to the Theory and Practice of Differential Privacy Workshop (TPDP 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.00322v1",
      "published_date": "2025-05-31 00:23:05 UTC",
      "updated_date": "2025-05-31 00:23:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:06:08.558697+00:00"
    },
    {
      "arxiv_id": "2506.00320v3",
      "title": "Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents",
      "title_zh": "Dyna-Thinkï¼šååŒ AI æ™ºèƒ½ä½“ä¸­çš„æ¨ç†ã€è¡ŒåŠ¨ä¸ä¸–ç•Œæ¨¡å‹æ¨¡æ‹Ÿ",
      "authors": [
        "Xiao Yu",
        "Baolin Peng",
        "Ruize Xu",
        "Michel Galley",
        "Hao Cheng",
        "Suman Nath",
        "Jianfeng Gao",
        "Zhou Yu"
      ],
      "abstract": "Recent progress in reasoning with large language models (LLMs), such as DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics and coding, by exhibiting complex cognitive behaviors such as verification, goal decomposition, and self-reflection. However, it is unclear what behavior is effective and what behavior is missing for long-horizon AI agents tasks. In this work, we propose Dyna-Think, a thinking framework that integrates planning with an internal world model with reasoning and acting to enhance AI agent performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning (DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing world model simulation relevant to the proposed (and planned) action, and trains the policy using this reconstructed data. To enhance Dyna-Think, DDT uses a two-stage training process to first improve the agent's world modeling ability via objectives such as state prediction or critique generation, and then improve the agent's action via policy training. We evaluate our methods on OSWorld and WindowsAgentArena, and demonstrate that Dyna-Think improves the agent's in-domain and out-of-domain performance, achieving similar best-of-n performance compared to R1 while generating 2x less tokens on average. Our extensive empirical studies reveal that 1) using critique generation for world model training is effective to improve policy performance; and 2) AI agents with better performance correlate with better world modeling abilities. We believe our results suggest a promising research direction to integrate world model simulation into AI agents to enhance their reasoning, planning, and acting capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Dyna-Think æ¡†æ¶ï¼Œæ—¨åœ¨å°†è§„åˆ’ã€å†…éƒ¨ä¸–ç•Œæ¨¡å‹(world model)æ¨¡æ‹Ÿä¸æ¨ç†ã€è¡ŒåŠ¨ç›¸ç»“åˆï¼Œä»¥æå‡ AI æ™ºèƒ½ä½“åœ¨é•¿ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä¸ºäº†å®ç°è¯¥æ¡†æ¶ï¼Œä½œè€…æå‡ºäº† Dyna-Think Imitation Learning (DIT)ï¼Œé€šè¿‡é‡æ„ DeepSeek-R1 çš„æ€ç»´è¿‡ç¨‹æ¥ä¸“æ³¨äºä¸è§„åˆ’åŠ¨ä½œç›¸å…³çš„ä¸–ç•Œæ¨¡å‹æ¨¡æ‹Ÿï¼Œå¹¶æ®æ­¤è®­ç»ƒç­–ç•¥ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº† Dyna-Think Dyna Training (DDT) ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œå…ˆé€šè¿‡çŠ¶æ€é¢„æµ‹æˆ–æ‰¹åˆ¤ç”Ÿæˆ(critique generation)å¢å¼ºä¸–ç•Œæ¨¡å‹èƒ½åŠ›ï¼Œå†ä¼˜åŒ–è¡ŒåŠ¨ç­–ç•¥ã€‚åœ¨ OSWorld å’Œ WindowsAgentArena ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDyna-Think æ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“çš„åŸŸå†…å’Œè·¨åŸŸæ€§èƒ½ï¼Œä¸”åœ¨è¾¾åˆ°ä¸ R1 ç›¸å½“çš„ best-of-n è¡¨ç°æ—¶ï¼Œç”Ÿæˆçš„ token æ•°é‡å¹³å‡å‡å°‘äº† 2 å€ã€‚ç ”ç©¶è¯å®ï¼Œä½¿ç”¨æ‰¹åˆ¤ç”Ÿæˆä¼˜åŒ–ä¸–ç•Œæ¨¡å‹èƒ½æœ‰æ•ˆæå‡æ€§èƒ½ï¼Œä¸”æ™ºèƒ½ä½“çš„è¡¨ç°ä¸ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›å‘ˆæ­£ç›¸å…³ã€‚è¯¥æˆæœä¸ºå°†ä¸–ç•Œæ¨¡å‹æ¨¡æ‹Ÿæ•´åˆè¿› AI æ™ºèƒ½ä½“ä»¥å¢å¼ºå…¶æ¨ç†ã€è§„åˆ’ä¸æ‰§è¡Œèƒ½åŠ›æä¾›äº†æå…·å‰æ™¯çš„ç ”ç©¶è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00320v3",
      "published_date": "2025-05-31 00:10:18 UTC",
      "updated_date": "2025-10-10 22:01:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:06:08.084846+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 119,
  "processed_papers_count": 119,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T17:07:05.206659+00:00"
}
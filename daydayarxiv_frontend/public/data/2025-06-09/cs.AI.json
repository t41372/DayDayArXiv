{
  "date": "2025-06-09",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-06-09 的 arXiv 中文 TLDR 快报！\n\n**今日总结**：\n今天的 arXiv 论文井喷，且质量极高，大量 **NeurIPS 2025** 和 **ICML 2025** 的接收论文放出。核心看点集中在 **LLM 的推理能力进化**（从被动推理走向主动推理、测试时交互 Scaling）、**视频生成的新范式**（音频驱动视频、自回归视频生成的 Exposure Bias 修正）、以及 **端侧模型**（MiniCPM4）的最新进展。此外，关于 LLM 错误相关性、安全性以及数学证明能力的探讨也非常深入。\n\n---\n\n### 🚀 深度推理与 Agent (Reasoning & Agents)\n\n**1. From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?**\n**从被动推理到主动推理：大语言模型能在信息不全时提出正确的问题吗？**\n> **关键词**：Active Reasoning, Benchmark, Agentic Scenarios\n> **一句话点评**：ICML 2025 接收。指出现有 LLM 只是“做题家”（被动推理），在需要主动提问获取信息的“侦探”场景下表现糟糕。\n> **核心贡献**：提出了 **AR-Bench**，一个专门评估 LLM **主动推理（Active Reasoning）** 能力的基准测试（包含侦探案件、情境谜题等）。研究发现，即使是先进的模型在需要与外部系统交互以获取缺失证据时也经常失败。这表明被动推理（给定所有信息）和主动推理之间存在巨大鸿沟，简单的微调或搜索策略提升有限，呼唤新的训练范式。\n\n**47. Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction**\n**思考与行动：通过扩展测试时交互进行推理的智能体**\n> **关键词**：Test-Time Scaling, Interaction, Web Agents\n> **一句话点评**：不仅仅是 Scaling \"Thinking\" (CoT)，还要 Scaling \"Interaction\" (Doing)。\n> **核心贡献**：目前的 Test-time scaling 主要集中在生成更长的推理链（Thinking）。本文提出了 **Scaling Interaction**，即增加智能体与环境交互的视界（Horizon），允许探索、回溯和动态重规划。提出了 **TTI (Test-Time Interaction)** 方法，通过课程学习（Curriculum-based Online RL）训练智能体。在 WebVoyager 等基准上，使用 Gemma 2 12B 模型取得了 SOTA，证明了交互 Scaling 是除了计算 Scaling 之外的另一个重要维度。\n\n**147. DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO**\n**DeepVideo-R1：通过难度感知的回归 GRPO 进行视频强化微调**\n> **关键词**：VideoLLM, GRPO, Reasoning, NeurIPS 2025\n> **一句话点评**：将 DeepSeek-R1 的强化学习思路（GRPO）引入视频理解领域。\n> **核心贡献**：针对 VideoLLM 的推理能力，引入了 **GRPO (Group Relative Policy Optimization)**。为了解决 GRPO 在视频任务中的不稳定性（如优势消失问题），提出了 **Reg-GRPO**（回归式 GRPO）和难度感知的数据增强策略。DeepVideo-R1 显著提升了视频推理性能，证明了 RL 后训练（Post-training）在多模态领域的有效性。\n\n**59. Solving Inequality Proofs with Large Language Models**\n**用大语言模型解决不等式证明**\n> **关键词**：Mathematical Reasoning, Inequality Proving, NeurIPS 2025 Spotlight\n> **一句话点评**：NeurIPS Spotlight 论文。即使是 o1 模型，在严格的逐布审查下，证明不等式的正确率也不到 10%。\n> **核心贡献**：发布了 **IneqMath** 数据集（奥赛级别不等式）。提出了一个新的评估框架，不仅看最终答案，还通过 LLM-as-judge 进行逐布审查。结果令人震惊：顶级模型（如 o1）虽然最终答案对了不少，但在步骤严谨性上漏洞百出（Step-wise accuracy < 10%）。这揭示了 LLM 在严谨演绎推理上的脆弱性。\n\n### 🎬 视频与图像生成 (Video & Image Generation)\n\n**3. Seeing Voices: Generating A-Roll Video from Audio with Mirage**\n**看见声音：使用 Mirage 从音频生成 A-Roll 视频**\n> **关键词**：Audio-to-Video, Foundation Model, Lip Sync\n> **一句话点评**：音频生成视频的 Foundation Model，效果惊艳。\n> **核心贡献**：推出了 **Mirage**，一个音频到视频（Audio-to-Video）的基础模型。不同于简单的对口型（Lip-sync），Mirage 能根据音频从头生成逼真、富有表现力的人物视频（A-roll）。核心技术是统一的基于 Self-attention 的生成方法，无需特定于人脸或语音的复杂 Loss 设计。\n\n**38. Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion**\n**Self Forcing：弥合自回归视频扩散中的训练-测试差距**\n> **关键词**：Video Diffusion, Exposure Bias, Autoregressive\n> **一句话点评**：NeurIPS 2025 Spotlight。解决了自回归视频生成中“一步错，步步错”的问题。\n> **核心贡献**：针对自回归视频生成中的 **Exposure Bias**（训练用真值，推理用生成值），提出了 **Self Forcing** 训练范式。在训练时，模型不再仅依赖 Ground-truth 上下文，而是基于“自我生成”的历史进行预测（通过 KV Caching 实现高效 Rollout）。实现了单 GPU 上的实时流式视频生成，延迟极低且质量超越非因果模型。\n\n**8. Highly Compressed Tokenizer Can Generate Without Training**\n**高度压缩的分词器无需训练即可生成**\n> **关键词**：1D Tokenizer, Image Editing, Training-free\n> **一句话点评**：ICML 2025 接收。把图像压缩成 32 个 Token 还能玩出花来。\n> **核心贡献**：研究了 **1D Image Tokenizers**（将图像压缩为极短的序列，如 32 个 Token）。发现这种高度压缩的潜空间允许通过简单的启发式操作（如复制、替换 Token）直接进行图像编辑和生成，甚至不需要训练生成模型，仅通过测试时优化（Test-time optimization）即可实现高质量的 Inpainting 和文本引导编辑。\n\n**19. A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation**\n**解码器架构 LLM 用于文本到图像生成的综合研究**\n> **关键词**：Text Encoder, Diffusion Models, CVPR 2025\n> **一句话点评**：T5 和 CLIP 该退休了，Decoder-only LLM 做 Text Encoder 效果更好。\n> **核心贡献**：系统研究了用现代 Decoder-only LLM 替代 T5/CLIP 作为文生图模型的 Text Encoder。发现直接用最后一层 Embedding 效果不好，但如果使用 **Layer-normalized averaging** 融合所有层的信息，能显著提升对复杂 Prompt 的理解和对齐能力。\n\n### ⚡️ LLM 效率与训练 (Efficiency & Training)\n\n**63. MiniCPM4: Ultra-Efficient LLMs on End Devices**\n**MiniCPM4：端侧超高效大语言模型**\n> **关键词**：On-device LLM, Efficient Training, MLLM\n> **一句话点评**：面壁智能 MiniCPM 系列新作，端侧模型卷王。\n> **核心贡献**：发布 **MiniCPM4**（0.5B 和 8B 版本）。技术亮点包括：**InfLLM v2**（可训练稀疏注意力，加速长文本）、**UltraClean/UltraChat v2** 数据集、以及 **CPM.cu** 推理加速引擎。在 8T Token 上训练，性能超越同尺寸开源模型，且支持长文本和高效推理。\n\n**51. Correlated Errors in Large Language Models**\n**大语言模型中的相关错误**\n> **关键词**：Model Correlation, Algorithmic Monoculture, ICML 2025\n> **一句话点评**：ICML 2025 接收。大模型们越来越像了，连犯错都一样。\n> **核心贡献**：评估了 350+ 个 LLM，发现模型间的错误具有高度相关性（Correlated Errors）。即使架构不同、提供商不同，只要模型更强、更大，它们的错误倾向就越趋同。这对于“算法单一化”（Algorithmic Monoculture）是一个警示，意味着简单的模型集成（Ensemble）可能失效。\n\n**42. Reparameterized LLM Training via Orthogonal Equivalence Transformation**\n**通过正交等价变换进行重参数化 LLM 训练**\n> **关键词**：Optimization, Training Stability, POET, NeurIPS 2025\n> **一句话点评**：NeurIPS 2025 接收。一种新的训练算法 POET，让大模型训练更稳。\n> **核心贡献**：提出了 **POET** 算法，通过两个可学习的正交矩阵和一个固定的随机权重矩阵来重参数化神经元。这种方法能保持权重矩阵的谱性质，从而稳定优化过程并提高泛化能力，特别适用于大规模神经网络的训练。\n\n### 🛡️ 安全与对齐 (Safety & Alignment)\n\n**6. Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints**\n**具有高置信度安全约束的人类反馈强化学习**\n> **关键词**：Safe RLHF, Constrained Optimization, RLC 2025\n> **一句话点评**：在 RLHF 中显式引入安全约束，不再让安全性成为有用性的牺牲品。\n> **核心贡献**：提出了 **HC-RLHF**。传统 RLHF 常把安全性和有用性作为 Trade-off。HC-RLHF 将两者解耦，分别训练 Reward Model 和 Cost Model。通过两步优化（在悲观 Cost 约束下优化 Reward，并通过安全测试），理论上保证模型生成不安全内容的概率低于用户设定的阈值。\n\n**2. Hidden Bias in the Machine: Stereotypes in Text-to-Image Models**\n**机器中的隐性偏见：文生图模型中的刻板印象**\n> **关键词**：Bias, Stereotypes, Text-to-Image\n> **一句话点评**：CVPR Workshop 论文。SD 1.5 和 Flux-1 生成的图像里充满了社会刻板印象。\n> **核心贡献**：通过 16,000+ 张生成图像和 Google 图片搜索结果对比，分析了 Stable Diffusion 1.5 和 Flux-1 中的偏见。发现性别、种族、年龄、体型等方面的表现存在显著差异，模型往往放大并加深了社会既有的有害刻板印象。\n\n### 🧠 其他值得关注的论文\n\n*   **[Neuroscience] 4. Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain**: 发现经过指令微调的多模态模型（Video-Audio MLLMs）能更好地预测人类观看视频时的大脑活动，且模型层级与大脑层级（感知区到高级认知区）存在对应关系。\n*   **[Benchmarks] 32. SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents**: 针对工业标准操作程序（SOP）的 Agent 基准测试。发现现有 Agent（ReAct 等）在严格遵循复杂 SOP 时成功率很低（<50%）。\n*   **[Benchmarks] 1. From Passive to Active Reasoning (AR-Bench)**: 见上文，非常重要的推理基准。\n*   **[Coding] 111. SWE-Dev: Building Software Engineering Agents with Training and Inference Scaling**: ACL Findings。通过合成测试用例和 Scaling Agent 轨迹，构建了强大的软件工程 Agent，在 SWE-bench-Verified 上表现优异。\n*   **[Interpretability] 188. Improving LLM Reasoning through Interpretable Role-Playing Steering**: 提出了 SRPS，通过稀疏自编码器（SAE）提取角色扮演相关的特征，并进行干预（Steering），从而可控地提升 LLM 的推理能力。\n\n---\n🎉 **祝大家科研顺利，我们明天见！**",
  "papers": [
    {
      "arxiv_id": "2506.08295v1",
      "title": "From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?",
      "title_zh": "从被动推理到主动推理：大语言模型能否在信息不完备的情况下精准提问？",
      "authors": [
        "Zhanke Zhou",
        "Xiao Feng",
        "Zhaocheng Zhu",
        "Jiangchao Yao",
        "Sanmi Koyejo",
        "Bo Han"
      ],
      "abstract": "While existing benchmarks probe the reasoning abilities of large language models (LLMs) across diverse domains, they predominantly assess passive reasoning, providing models with all the information needed to reach a solution. By contrast, active reasoning-where an LLM must interact with external systems to acquire missing evidence or data-has received little systematic attention. To address this shortfall, we present AR-Bench, a novel benchmark designed explicitly to evaluate an LLM's active reasoning skills. AR-Bench comprises three task families-detective cases, situation puzzles, and guessing numbers-that together simulate real-world, agentic scenarios and measure performance across commonsense, logical, and symbolic reasoning challenges. Empirical evaluation on AR-Bench demonstrates that contemporary LLMs exhibit pronounced difficulties with active reasoning: they frequently fail to acquire or leverage the information needed to solve tasks. This gap highlights a stark divergence between their passive and active reasoning abilities. Moreover, ablation studies indicate that even advanced strategies, such as tree-based searching or post-training approaches, yield only modest gains and fall short of the levels required for real-world deployment. Collectively, these findings highlight the critical need to advance methodology for active reasoning, e.g., incorporating interactive learning, real-time feedback loops, and environment-aware objectives for training. The benchmark is publicly available at: https://github.com/tmlr-group/AR-Bench.",
      "tldr_zh": "该研究探讨了大型语言模型（LLMs）从 passive reasoning（被动推理）向 active reasoning（主动推理）转变的能力，指出当前基准测试多关注提供全信息的被动任务，而忽视了模型主动获取缺失信息的能力。为此，作者提出了 AR-Bench 基准，通过侦探案例（detective cases）、情境谜题（situation puzzles）和猜数字（guessing numbers）等任务，系统评估模型在不完全信息下的主动推理表现。实验发现，当代 LLMs 在主动推理方面存在显著缺陷，常表现出无法有效获取或利用必要信息的困境。消融实验进一步证实，即使使用 tree-based searching 或 post-training 等先进策略，其性能提升依然有限，难以达到实际应用标准。该研究最后强调了开发交互式学习（interactive learning）和环境感知（environment-aware）训练目标的必要性，为提升 LLMs 的主动推理能力指明了方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08295v1",
      "published_date": "2025-06-09 23:56:41 UTC",
      "updated_date": "2025-06-09 23:56:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:05:18.884507+00:00"
    },
    {
      "arxiv_id": "2506.13780v1",
      "title": "Hidden Bias in the Machine: Stereotypes in Text-to-Image Models",
      "title_zh": "机器中的隐性偏见：文本到图像模型中的刻板印象",
      "authors": [
        "Sedat Porikli",
        "Vedat Porikli"
      ],
      "abstract": "Text-to-Image (T2I) models have transformed visual content creation, producing highly realistic images from natural language prompts. However, concerns persist around their potential to replicate and magnify existing societal biases. To investigate these issues, we curated a diverse set of prompts spanning thematic categories such as occupations, traits, actions, ideologies, emotions, family roles, place descriptions, spirituality, and life events. For each of the 160 unique topics, we crafted multiple prompt variations to reflect a wide range of meanings and perspectives. Using Stable Diffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original checkpoints, we generated over 16,000 images under consistent settings. Additionally, we collected 8,000 comparison images from Google Image Search. All outputs were filtered to exclude abstract, distorted, or nonsensical results. Our analysis reveals significant disparities in the representation of gender, race, age, somatotype, and other human-centric factors across generated images. These disparities often mirror and reinforce harmful stereotypes embedded in societal narratives. We discuss the implications of these findings and emphasize the need for more inclusive datasets and development practices to foster fairness in generative visual systems.",
      "tldr_zh": "该研究探讨了文本到图像生成模型(Text-to-Image models)在生成过程中可能复制并放大社会偏见的问题。研究者选取了涵盖职业、特质、行为等多个维度的160个独特话题，利用Stable Diffusion 1.5和Flux-1模型生成了超过16,000张图像，并与8,000张来自Google Image Search的图像进行对比分析。研究结果揭示了生成图像在性别(gender)、种族(race)、年龄(age)和体型(somatotype)等人类中心因素的呈现上存在显著失衡。这些差异不仅反映了现实社会中的偏见，还进一步加剧了社会叙事中固有的有害刻板印象(stereotypes)。最后，该研究强调了在生成式视觉系统开发中采用更具包容性的数据集和开发实践的迫切性，以实现更广泛的社会公平。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Equal contribution by both authors, Published at CVPR 2025 Workshop on Experimental Model Auditing via Controllable Synthesis (EMACS) and Workshop on Demographic Diversity in Computer Vision (DemoDiv)",
      "pdf_url": "https://arxiv.org/pdf/2506.13780v1",
      "published_date": "2025-06-09 23:06:04 UTC",
      "updated_date": "2025-06-09 23:06:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:05:13.891108+00:00"
    },
    {
      "arxiv_id": "2506.08279v1",
      "title": "Seeing Voices: Generating A-Roll Video from Audio with Mirage",
      "title_zh": "看见声音：利用 Mirage 从音频生成 A-Roll 视频",
      "authors": [
        "Aditi Sundararaman",
        "Amogh Adishesha",
        "Andrew Jaegle",
        "Dan Bigioi",
        "Hyoung-Kyu Song",
        "Jon Kyl",
        "Justin Mao",
        "Kevin Lan",
        "Mojtaba Komeili",
        "ShahRukh Athar",
        "Sheila Babayan",
        "Stanislau Beliasau",
        "William Buchwalter"
      ],
      "abstract": "From professional filmmaking to user-generated content, creators and consumers have long recognized that the power of video depends on the harmonious integration of what we hear (the video's audio track) with what we see (the video's image sequence). Current approaches to video generation either ignore sound to focus on general-purpose but silent image sequence generation or address both visual and audio elements but focus on restricted application domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation model that excels at generating realistic, expressive output imagery from scratch given an audio input. When integrated with existing methods for speech synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal video. When trained on audio-video footage of people talking (A-roll) and conditioned on audio containing speech, Mirage generates video of people delivering a believable interpretation of the performance implicit in input audio. Our central technical contribution is a unified method for training self-attention-based audio-to-video generation models, either from scratch or given existing weights. This methodology allows Mirage to retain generality as an approach to audio-to-video generation while producing outputs of superior subjective quality to methods that incorporate audio-specific architectures or loss components specific to people, speech, or details of how images or audio are captured. We encourage readers to watch and listen to the results of Mirage for themselves (see paper and comments for links).",
      "tldr_zh": "该研究提出了 Mirage，这是一个音频到视频 (audio-to-video) 的基础模型，旨在根据音频输入从零生成真实且具有表现力的 A-roll 视频。与现有的忽略声音或仅限于重新配音 (re-dubbing) 等特定领域的生成方法不同，Mirage 能够实现视觉与听觉序列的深度集成。通过结合现有的语音合成 (text-to-speech) 技术，Mirage 能够根据语音音频生成人物演讲的写实画面，并提供令人信服的表演诠释。其核心技术贡献是一种基于自注意力 (self-attention-based) 机制的音频到视频生成统一训练方法，支持从零开始或利用现有权重进行训练。这种方法使 Mirage 在保持通用性的同时，在主观输出质量上优于包含特定音频架构或针对特定语音损失组件的模型。该研究为多模态视频生成提供了高质量的解决方案，展示了在专业电影制作和用户内容创作领域的巨大潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Technical report website: mirage.app/research/seeing-voices, product website: mirage.app",
      "pdf_url": "https://arxiv.org/pdf/2506.08279v1",
      "published_date": "2025-06-09 22:56:02 UTC",
      "updated_date": "2025-06-09 22:56:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:05:22.084874+00:00"
    },
    {
      "arxiv_id": "2506.08277v1",
      "title": "Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain",
      "title_zh": "指令微调的视音频模型阐明大脑的功能特化",
      "authors": [
        "Subba Reddy Oota",
        "Khushbu Pahwa",
        "Prachi Jindal",
        "Satya Sai Srinath Namburi",
        "Maneesh Singh",
        "Tanmoy Chakraborty",
        "Bapi S. Raju",
        "Manish Gupta"
      ],
      "abstract": "Recent voxel-wise multimodal brain encoding studies have shown that multimodal large language models (MLLMs) exhibit a higher degree of brain alignment compared to unimodal models in both unimodal and multimodal stimulus settings. More recently, instruction-tuned multimodal models have shown to generate task-specific representations that align strongly with brain activity. However, prior work evaluating the brain alignment of MLLMs has primarily focused on unimodal settings or relied on non-instruction-tuned multimodal models for multimodal stimuli. To address this gap, we investigated brain alignment, that is, measuring the degree of predictivity of neural activity recorded while participants were watching naturalistic movies (video along with audio) with representations derived from MLLMs. We utilized instruction-specific embeddings from six video and two audio instruction-tuned MLLMs. Experiments with 13 video task-specific instructions show that instruction-tuned video MLLMs significantly outperform non-instruction-tuned multimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for both video and audio tasks using language-guided instructions shows clear disentanglement in task-specific representations from MLLMs, leading to precise differentiation of multimodal functional processing in the brain. We also find that MLLM layers align hierarchically with the brain, with early sensory areas showing strong alignment with early layers, while higher-level visual and language regions align more with middle to late layers. These findings provide clear evidence for the role of task-specific instructions in improving the alignment between brain activity and MLLMs, and open new avenues for mapping joint information processing in both the systems. We make the code publicly available [https://github.com/subbareddy248/mllm_videos].",
      "tldr_zh": "该研究探讨了指令微调(Instruction-Tuned)的多模态大语言模型(MLLMs)在处理视频和音频自然刺激时与大脑活动的一致性(Brain Alignment)。通过分析六个视频和两个音频指令微调模型的任务特定嵌入(Instruction-specific Embeddings)，研究发现指令微调模型对神经活动的预测准确性显著优于非指令微调模型和单模态模型。实验表明，语言引导的指令促使任务特定表示实现解耦(Disentanglement)，从而能够精确区分大脑的多模态功能加工过程。研究还发现MLLM的层级与大脑区域存在层级对齐，即早期感觉区对应早期层，而高级视觉及语言区对应中后期层。这些结果证明了任务特定指令在提升大脑与人工智能模型对齐度方面的关键作用，为理解两者的联合信息处理机制提供了新途径。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "comment": "39 pages, 22 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.08277v1",
      "published_date": "2025-06-09 22:48:36 UTC",
      "updated_date": "2025-06-09 22:48:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:05:28.390289+00:00"
    },
    {
      "arxiv_id": "2506.08267v2",
      "title": "Sparse Interpretable Deep Learning with LIES Networks for Symbolic Regression",
      "title_zh": "基于 LIES 网络的符号回归稀疏可解释深度学习",
      "authors": [
        "Mansooreh Montazerin",
        "Majd Al Aawar",
        "Antonio Ortega",
        "Ajitesh Srivastava"
      ],
      "abstract": "Symbolic regression (SR) aims to discover closed-form mathematical expressions that accurately describe data, offering interpretability and analytical insight beyond standard black-box models. Existing SR methods often rely on population-based search or autoregressive modeling, which struggle with scalability and symbolic consistency. We introduce LIES (Logarithm, Identity, Exponential, Sine), a fixed neural network architecture with interpretable primitive activations that are optimized to model symbolic expressions. We develop a framework to extract compact formulae from LIES networks by training with an appropriate oversampling strategy and a tailored loss function to promote sparsity and to prevent gradient instability. After training, it applies additional pruning strategies to further simplify the learned expressions into compact formulae. Our experiments on SR benchmarks show that the LIES framework consistently produces sparse and accurate symbolic formulae outperforming all baselines. We also demonstrate the importance of each design component through ablation studies.",
      "tldr_zh": "该研究针对符号回归(Symbolic Regression)中现有方法在可扩展性和符号一致性方面的不足，提出了一种名为LIES (Logarithm, Identity, Exponential, Sine) 的固定神经网络架构。LIES网络通过具有可解释性的原始激活函数来建模符号表达式，并利用过采样策略与定制的损失函数来促进模型稀疏性并防止梯度不稳定。在训练完成后，该框架通过额外的剪枝策略将学到的表达式进一步简化为紧凑的数学公式。实验结果表明，LIES框架在符号回归基准测试中始终能生成稀疏且准确的公式，性能优于所有基线模型。消融实验进一步验证了各设计组件在提升模型表现中的重要作用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08267v2",
      "published_date": "2025-06-09 22:05:53 UTC",
      "updated_date": "2025-06-14 21:24:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:05:23.985870+00:00"
    },
    {
      "arxiv_id": "2506.08266v1",
      "title": "Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints",
      "title_zh": "具有高置信度安全约束的人类反馈强化学习",
      "authors": [
        "Yaswanth Chittepu",
        "Blossom Metevier",
        "Will Schwarzer",
        "Austin Hoag",
        "Scott Niekum",
        "Philip S. Thomas"
      ],
      "abstract": "Existing approaches to language model alignment often treat safety as a tradeoff against helpfulness, which can lead to unacceptable responses in sensitive domains. To ensure reliable performance in such settings, we propose High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a method that provides high-confidence safety guarantees while maximizing helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human preferences into helpfulness and harmlessness (safety), which are learned by training a reward model and a cost model, respectively. It then employs a two-step process to find safe solutions. In the first step, it optimizes the reward function under an intentionally pessimistic version of the cost constraint. In the second step, the trained model undergoes a safety test to verify whether its performance stays within an upper-confidence bound of the actual cost constraint. We provide a theoretical analysis of HC-RLHF, including proof that it will not return an unsafe solution with a probability greater than a user-specified threshold. For our empirical analysis, we apply HC-RLHF to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF produces safe models with high probability and can improve harmlessness and helpfulness compared to previous methods.",
      "tldr_zh": "该研究提出了高置信度安全人类反馈强化学习 (High-Confidence Safe Reinforcement Learning from Human Feedback, HC-RLHF)，旨在解决现有语言模型对齐中安全与有用性难以兼顾的问题。该方法将人类偏好显式解耦为负责有用性的奖励模型 (reward model) 和负责无害性的成本模型 (cost model)，并通过两步法寻找最优解。首先在悲观的成本约束下优化奖励函数，随后通过安全性测试验证模型性能是否处于实际成本约束的上置信界 (upper-confidence bound) 内。理论分析证明，HC-RLHF 能够保证返回不安全解的概率低于用户指定的阈值。实验在 Qwen2 和 LLaMa3.2 等多种语言模型上验证了该方法的有效性，结果显示 HC-RLHF 在提供高概率安全保障的同时，比现有方法更有效地提升了模型的无害性与有用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 6 figures, 4 tables, Second Reinforcement Learning Conference (RLC 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.08266v1",
      "published_date": "2025-06-09 22:03:56 UTC",
      "updated_date": "2025-06-09 22:03:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:05:37.843775+00:00"
    },
    {
      "arxiv_id": "2506.08260v1",
      "title": "Automatic Generation of Inference Making Questions for Reading Comprehension Assessments",
      "title_zh": "面向阅读理解测评的推断类问题自动生成",
      "authors": [
        "Wanjing Anya Ma",
        "Michael Flor",
        "Zuowei Wang"
      ],
      "abstract": "Inference making is an essential but complex skill in reading comprehension (RC). Some inferences require resolving references across sentences, and some rely on using prior knowledge to fill in the detail that is not explicitly written in the text. Diagnostic RC questions can help educators provide more effective and targeted reading instruction and interventions for school-age students. We introduce a taxonomy of inference types for RC and use it to analyze the distribution of items within a diagnostic RC item bank. Next, we present experiments using GPT-4o to generate bridging-inference RC items for given reading passages via few-shot prompting, comparing conditions with and without chain-of-thought prompts. Generated items were evaluated on three aspects: overall item quality, appropriate inference type, and LLM reasoning, achieving high inter-rater agreements above 0.90. Our results show that GPT-4o produced 93.8% good-quality questions suitable for operational use in grade 3-12 contexts; however, only 42.6% of the generated questions accurately matched the targeted inference type. We conclude that combining automatic item generation with human judgment offers a promising path toward scalable, high-quality diagnostic RC assessments.",
      "tldr_zh": "该研究探讨了阅读理解(Reading Comprehension)中核心的推理能力(Inference making)，并提出了一套推理类型分类法(taxonomy of inference types)用于分析诊断性题目库。研究利用GPT-4o模型，结合少样本提示(few-shot prompting)和链式思维(chain-of-thought)技术，为给定文本自动生成衔接推理(bridging-inference)题目。评估结果显示，生成题目在整体质量、推理类型匹配度和LLM reasoning方面表现出色，评分者间一致性高于0.90。实验发现93.8%的生成题目达到了高质量标准并适用于3-12年级教学，但仅有42.6%的题目准确匹配了目标推理类型。研究最终得出结论，将自动题目生成(automatic item generation)与人工评判相结合，是实现大规模、高质量诊断性阅读理解评估的有效途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2025), co-located with the ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08260v1",
      "published_date": "2025-06-09 21:50:12 UTC",
      "updated_date": "2025-06-09 21:50:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:05:41.631283+00:00"
    },
    {
      "arxiv_id": "2506.08257v1",
      "title": "Highly Compressed Tokenizer Can Generate Without Training",
      "title_zh": "高度压缩的分词器无需训练即可实现生成",
      "authors": [
        "L. Lao Beyer",
        "T. Li",
        "X. Chen",
        "S. Karaman",
        "K. He"
      ],
      "abstract": "Commonly used image tokenizers produce a 2D grid of spatially arranged tokens. In contrast, so-called 1D image tokenizers represent images as highly compressed one-dimensional sequences of as few as 32 discrete tokens. We find that the high degree of compression achieved by a 1D tokenizer with vector quantization enables image editing and generative capabilities through heuristic manipulation of tokens, demonstrating that even very crude manipulations -- such as copying and replacing tokens between latent representations of images -- enable fine-grained image editing by transferring appearance and semantic attributes. Motivated by the expressivity of the 1D tokenizer's latent space, we construct an image generation pipeline leveraging gradient-based test-time optimization of tokens with plug-and-play loss functions such as reconstruction or CLIP similarity. Our approach is demonstrated for inpainting and text-guided image editing use cases, and can generate diverse and realistic samples without requiring training of any generative model.",
      "tldr_zh": "该研究探讨了高度压缩的一维(1D)图像分词器(Tokenizer)在图像生成与编辑领域的潜力，其通过向量量化(Vector Quantization)技术将图像表示为仅32个离散Token的极短序列。研究发现，即使通过复制或替换Token等简单的启发式操作(Heuristic Manipulation)，也能在潜在空间中实现精细的外观和语义属性迁移。基于此，作者提出了一种利用梯度进行测试时优化(Test-time Optimization)的生成流水线，并结合了重构损失或CLIP相似度等即插即用的损失函数(Loss Functions)。该方法能够处理图像修复(Inpainting)及文本引导的图像编辑任务，且在完全不训练任何生成模型(Generative Model)的前提下生成真实且多样的样本。这一成果证明了高度压缩的分词器潜在空间本身就具有极强的表达力，为无需大规模训练的图像生成提供了新路径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Main manuscript: 9 pages, 7 figures. Appendix: 8 pages, 9 figures. To appear in the Proceedings of the 42nd International Conference on Machine Learning",
      "pdf_url": "https://arxiv.org/pdf/2506.08257v1",
      "published_date": "2025-06-09 21:45:03 UTC",
      "updated_date": "2025-06-09 21:45:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:05:53.972229+00:00"
    },
    {
      "arxiv_id": "2506.08255v3",
      "title": "SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense",
      "title_zh": "SHIELD：面向增量扩展学习防御的安全超网络",
      "authors": [
        "Patryk Krukowski",
        "Łukasz Gorczyca",
        "Piotr Helm",
        "Kamil Książek",
        "Przemysław Spurek"
      ],
      "abstract": "Continual learning under adversarial conditions remains an open problem, as existing methods often compromise either robustness, scalability, or both. We propose a novel framework that integrates Interval Bound Propagation (IBP) with a hypernetwork-based architecture to enable certifiably robust continual learning across sequential tasks. Our method, SHIELD, generates task-specific model parameters via a shared hypernetwork conditioned solely on compact task embeddings, eliminating the need for replay buffers or full model copies and enabling efficient over time. To further enhance robustness, we introduce Interval MixUp, a novel training strategy that blends virtual examples represented as $\\ell_{\\infty}$ balls centered around MixUp points. Leveraging interval arithmetic, this technique guarantees certified robustness while mitigating the wrapping effect, resulting in smoother decision boundaries. We evaluate SHIELD under strong white-box adversarial attacks, including PGD and AutoAttack, across multiple benchmarks. It consistently outperforms existing robust continual learning methods, achieving state-of-the-art average accuracy while maintaining both scalability and certification. These results represent a significant step toward practical and theoretically grounded continual learning in adversarial settings.",
      "tldr_zh": "该研究提出了 SHIELD 框架，旨在解决对抗环境下持续学习 (Continual Learning) 在鲁棒性与可扩展性之间的权衡难题。SHIELD 集成了区间边界传播 (Interval Bound Propagation, IBP) 与基于超网络 (Hypernetwork) 的架构，通过共享超网络和任务嵌入 (Task Embeddings) 生成特定任务的模型参数。这种设计消除了对重放缓冲区 (Replay Buffers) 或完整模型副本的需求，显著提升了系统随时间扩展的效率。为了进一步增强鲁棒性，研究者引入了 Interval MixUp 训练策略，利用区间算术在混合点周围构建 $\\ell_{\\infty}$ 球体虚拟样本，从而在保证可证鲁棒性 (Certified Robustness) 的同时缓解包装效应并平滑决策边界。在 PGD 和 AutoAttack 等强白盒对抗攻击下的实验表明，SHIELD 在多个基准测试中均优于现有的鲁棒持续学习方法，取得了最先进的平均准确率。该框架在保持高度可扩展性与认证性的同时，为对抗性设置下实用且具有理论支撑的持续学习奠定了重要基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08255v3",
      "published_date": "2025-06-09 21:43:56 UTC",
      "updated_date": "2025-11-21 16:58:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:05:48.333581+00:00"
    },
    {
      "arxiv_id": "2506.08244v1",
      "title": "Parameter-free approximate equivariance for tasks with finite group symmetry",
      "title_zh": "面向有限群对称任务的无参数近似等变性",
      "authors": [
        "Riccardo Ali",
        "Pietro Liò",
        "Jamie Vicary"
      ],
      "abstract": "Equivariant neural networks incorporate symmetries through group actions, embedding them as an inductive bias to improve performance on a wide variety of tasks. However, existing equivariant methods can be computationally intensive, with high parameter counts, and are often tied to a specific architecture. We propose a simple zero-parameter approach that imposes approximate equivariance for a finite group in the latent representation, as an additional term in the loss function. We conduct experiments which allow the network to learn a group representation on the latent space, and show in every case it prefers to learn the regular representation. Fixing this action on the latent space, this yields a simple method to impose approximate equivariance as an additional loss penalty. We benchmark our approach on three datasets and compare it against several existing equivariant methods, showing that in many cases it achieves similar or better performance for a fraction of the parameters.",
      "tldr_zh": "该研究针对现有的等变神经网络 (Equivariant neural networks) 计算强度大、参数量高且通常与特定架构绑定的局限性，提出了一种简单且无需额外参数的近似等变处理方法。该方法通过在损失函数 (Loss function) 中引入额外项，在潜在表示 (Latent representation) 空间内对有限群 (Finite group) 施加近似等变性 (Approximate equivariance) 约束。实验发现网络在学习潜在空间的群表示时更倾向于学习正则表示 (Regular representation)，据此可通过固定群作用并结合损失惩罚项来简化等变性的实现。在三个数据集上的基准测试结果显示，该方法在保持极低参数量的同时，在多个任务中取得了与现有等变方法持平甚至更优的性能表现。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08244v1",
      "published_date": "2025-06-09 21:23:26 UTC",
      "updated_date": "2025-06-09 21:23:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:06:36.907912+00:00"
    },
    {
      "arxiv_id": "2506.08243v1",
      "title": "Temporalizing Confidence: Evaluation of Chain-of-Thought Reasoning with Signal Temporal Logic",
      "title_zh": "置信度时序化：基于信号时序逻辑的链式思维推理评估",
      "authors": [
        "Zhenjiang Mao",
        "Artem Bisliouk",
        "Rohith Reddy Nama",
        "Ivan Ruchkin"
      ],
      "abstract": "Large Language Models (LLMs) have shown impressive performance in mathematical reasoning tasks when guided by Chain-of-Thought (CoT) prompting. However, they tend to produce highly confident yet incorrect outputs, which poses significant risks in domains like education, where users may lack the expertise to assess reasoning steps. To address this, we propose a structured framework that models stepwise confidence as a temporal signal and evaluates it using Signal Temporal Logic (STL). In particular, we define formal STL-based constraints to capture desirable temporal properties and compute robustness scores that serve as structured, interpretable confidence estimates. Our approach also introduces a set of uncertainty reshaping strategies to enforce smoothness, monotonicity, and causal consistency across the reasoning trajectory. Experiments show that our approach consistently improves calibration metrics and provides more reliable uncertainty estimates than conventional confidence aggregation and post-hoc calibration.",
      "tldr_zh": "该研究针对大语言模型(LLMs)在链式思维(Chain-of-Thought, CoT)推理中常出现的高置信度错误输出问题，提出了一种将逐步置信度建模为时间信号的结构化框架。研究通过引入信号时序逻辑(Signal Temporal Logic, STL)来定义形式化约束，并计算鲁棒性得分，以此作为结构化且可解释的置信度估计方式。此外，该方法还包含了一系列不确定性重塑策略，用以确保推理轨迹在平滑性、单调性和因果一致性上的表现。实验结果证明，该方法在校准指标上表现出色，且相较于传统的置信度聚合和事后校准方法，能够提供更加可靠的不确定性估计。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08243v1",
      "published_date": "2025-06-09 21:21:12 UTC",
      "updated_date": "2025-06-09 21:21:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:06:54.183097+00:00"
    },
    {
      "arxiv_id": "2506.17263v1",
      "title": "Memory Allocation in Resource-Constrained Reinforcement Learning",
      "title_zh": "资源受限强化学习中的内存分配",
      "authors": [
        "Massimiliano Tamborski",
        "David Abel"
      ],
      "abstract": "Resource constraints can fundamentally change both learning and decision-making. We explore how memory constraints influence an agent's performance when navigating unknown environments using standard reinforcement learning algorithms. Specifically, memory-constrained agents face a dilemma: how much of their limited memory should be allocated to each of the agent's internal processes, such as estimating a world model, as opposed to forming a plan using that model? We study this dilemma in MCTS- and DQN-based algorithms and examine how different allocations of memory impact performance in episodic and continual learning settings.",
      "tldr_zh": "该研究探讨了在资源受限环境下强化学习(Reinforcement Learning)中的记忆分配问题，分析了记忆约束如何从根本上改变智能体的学习与决策。研究的核心在于解决记忆受限智能体面临的资源分配困境，即如何在估算世界模型(world model)与基于该模型进行规划(plan)这两个内部过程之间有效平衡资源。研究者在基于MCTS和DQN的算法框架下评估了这一问题，并考察了不同记忆分配策略在回合制(episodic)和持续学习(continual learning)设置下对性能的具体影响。实验结果揭示了内部资源分配对智能体在未知环境中导航表现的关键作用，为优化受限系统中的强化学习算法提供了重要的理论见解。该工作强调了在算力受限时，智能体内部处理流程的协调对最终学习效率与决策质量的决定性意义。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "RLDM 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.17263v1",
      "published_date": "2025-06-09 21:15:37 UTC",
      "updated_date": "2025-06-09 21:15:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:07:11.984795+00:00"
    },
    {
      "arxiv_id": "2506.08235v1",
      "title": "Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning",
      "title_zh": "AI 能验证科学吗？评估大语言模型在科学断言 $\\rightarrow$ 证据推理中准确性的基准测试",
      "authors": [
        "Shashidhar Reddy Javaji",
        "Yupeng Cao",
        "Haohang Li",
        "Yangyang Yu",
        "Nikhil Muralidhar",
        "Zining Zhu"
      ],
      "abstract": "Large language models (LLMs) are increasingly being used for complex research tasks such as literature review, idea generation, and scientific paper analysis, yet their ability to truly understand and process the intricate relationships within complex research papers, such as the logical links between claims and supporting evidence remains largely unexplored. In this study, we present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs' capabilities in scientific claim-evidence extraction and validation, a task that reflects deeper comprehension of scientific argumentation. We systematically compare three approaches which are inspired by divide and conquer approaches, across six diverse LLMs, highlighting model-specific strengths and weaknesses in scientific comprehension. Through evaluation involving over 300 claim-evidence pairs across multiple research domains, we reveal significant limitations in LLMs' ability to process complex scientific content. Our results demonstrate that closed-source models like GPT-4 and Claude consistently outperform open-source counterparts in precision and recall across claim-evidence identification tasks. Furthermore, strategically designed three-pass and one-by-one prompting approaches significantly improve LLMs' abilities to accurately link dispersed evidence with claims, although this comes at increased computational cost. CLAIM-BENCH sets a new standard for evaluating scientific comprehension in LLMs, offering both a diagnostic tool and a path forward for building systems capable of deeper, more reliable reasoning across full-length papers.",
      "tldr_zh": "该研究推出了 CLAIM-BENCH，这是一个用于评估大语言模型 (LLMs) 在科学主张-证据 (scientific claim-evidence) 提取与验证能力的综合基准。研究者在六种不同的 LLMs 上系统地比较了三种受“分而治之”启发的推理方法，旨在探索模型对复杂科研论文逻辑联系的深度理解程度。实验涵盖了多个领域的 300 多个主张-证据对，揭示了 LLMs 在处理复杂科学内容时存在显著局限。结果表明，GPT-4 和 Claude 等闭源模型在精度和召回率上一致优于开源模型。此外，研究发现通过战略性设计的 three-pass 和 one-by-one 提示方法能显著提升模型准确关联分散证据与主张的能力，尽管这会增加计算成本。CLAIM-BENCH 为评估 LLMs 的科学理解力设定了新标准，为构建能对全文进行深层、可靠推理的系统提供了诊断工具和改进路径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 6 figures, Under review",
      "pdf_url": "https://arxiv.org/pdf/2506.08235v1",
      "published_date": "2025-06-09 21:04:39 UTC",
      "updated_date": "2025-06-09 21:04:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:07:06.286030+00:00"
    },
    {
      "arxiv_id": "2506.08234v2",
      "title": "Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions",
      "title_zh": "复合 AI 系统优化：方法、挑战及未来方向综述",
      "authors": [
        "Yu-Ang Lee",
        "Guan-Ting Yi",
        "Mei-Yi Liu",
        "Jui-Chao Lu",
        "Guan-Bo Yang",
        "Yun-Nung Chen"
      ],
      "abstract": "Recent advancements in large language models (LLMs) and AI systems have led to a paradigm shift in the design and optimization of complex AI workflows. By integrating multiple components, compound AI systems have become increasingly adept at performing sophisticated tasks. However, as these systems grow in complexity, new challenges arise in optimizing not only individual components but also their interactions. While traditional optimization methods such as supervised fine-tuning (SFT) and reinforcement learning (RL) remain foundational, the rise of natural language feedback introduces promising new approaches, especially for optimizing non-differentiable systems. This paper provides a systematic review of recent progress in optimizing compound AI systems, encompassing both numerical and language-based techniques. We formalize the notion of compound AI system optimization, classify existing methods along several key dimensions, and highlight open research challenges and future directions in this rapidly evolving field. A list of surveyed papers is publicly available at https://github.com/MiuLab/AISysOpt-Survey.",
      "tldr_zh": "该综述系统性地探讨了复合AI系统（Compound AI Systems）优化领域的最新进展，分析了在集成多个组件的复杂工作流中如何有效优化各部分及其交互。研究首先对复合AI系统优化的概念进行了形式化定义，并从数值型技术和基于语言的技术等多个维度对现有方法进行了分类。除了传统的监督微调（SFT）和强化学习（RL）等基础方法，文章特别强调了自然语言反馈在优化不可微系统（non-differentiable systems）中的应用前景。该研究不仅归类了当前的技术路径，还识别了该领域在系统复杂性不断提升的背景下所面临的开放性挑战与未来方向。通过对现有研究的深度综述，本文为构建、设计和优化高效的复杂AI系统提供了理论框架与实践参考。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2025 (Main)",
      "pdf_url": "https://arxiv.org/pdf/2506.08234v2",
      "published_date": "2025-06-09 21:04:14 UTC",
      "updated_date": "2025-10-07 01:23:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:07:11.488808+00:00"
    },
    {
      "arxiv_id": "2506.08231v1",
      "title": "Ensuring Reliability of Curated EHR-Derived Data: The Validation of Accuracy for LLM/ML-Extracted Information and Data (VALID) Framework",
      "title_zh": "保障经整理的 EHR 衍生数据可靠性：针对大语言模型/机器学习提取信息与数据的准确性验证（VALID）框架",
      "authors": [
        "Melissa Estevez",
        "Nisha Singh",
        "Lauren Dyson",
        "Blythe Adamson",
        "Qianyu Yuan",
        "Megan W. Hildner",
        "Erin Fidyk",
        "Olive Mbah",
        "Farhad Khan",
        "Kathi Seidl-Rathkopf",
        "Aaron B. Cohen"
      ],
      "abstract": "Large language models (LLMs) are increasingly used to extract clinical data from electronic health records (EHRs), offering significant improvements in scalability and efficiency for real-world data (RWD) curation in oncology. However, the adoption of LLMs introduces new challenges in ensuring the reliability, accuracy, and fairness of extracted data, which are essential for research, regulatory, and clinical applications. Existing quality assurance frameworks for RWD and artificial intelligence do not fully address the unique error modes and complexities associated with LLM-extracted data. In this paper, we propose a comprehensive framework for evaluating the quality of clinical data extracted by LLMs. The framework integrates variable-level performance benchmarking against expert human abstraction, automated verification checks for internal consistency and plausibility, and replication analyses comparing LLM-extracted data to human-abstracted datasets or external standards. This multidimensional approach enables the identification of variables most in need of improvement, systematic detection of latent errors, and confirmation of dataset fitness-for-purpose in real-world research. Additionally, the framework supports bias assessment by stratifying metrics across demographic subgroups. By providing a rigorous and transparent method for assessing LLM-extracted RWD, this framework advances industry standards and supports the trustworthy use of AI-powered evidence generation in oncology research and practice.",
      "tldr_zh": "该研究提出了 VALID (Validation of Accuracy for LLM/ML-Extracted Information and Data) 框架，旨在解决大语言模型 (LLMs) 在提取电子健康记录 (EHRs) 临床数据时面临的可靠性、准确性和公平性挑战。该框架通过针对专家人工提取结果的变量级基准测试 (Performance Benchmarking)、自动化的内部一致性与合理性校验，以及与外部标准的复制分析 (Replication Analyses)，实现了对数据的多维度评估。VALID 框架能够有效识别需要改进的变量，系统性探测潜在误差，并确认数据集在真实世界研究 (RWD) 中的适用性。此外，该框架支持通过人口统计学子组的分层指标进行偏见评估 (Bias Assessment)，确保了数据的公平性。通过提供严谨且透明的评估方法，VALID 框架提升了行业标准，为肿瘤学研究中 AI 驱动的证据生成提供了可信的技术支撑。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 3 tables, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2506.08231v1",
      "published_date": "2025-06-09 20:59:16 UTC",
      "updated_date": "2025-06-09 20:59:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:07:40.805237+00:00"
    },
    {
      "arxiv_id": "2506.08229v1",
      "title": "AI-Driven Early Detection of Cardiovascular Diseases: Reducing Healthcare Costs and improving patient Outcomes",
      "title_zh": "AI驱动的心血管疾病早期筛查：降低医疗成本并改善患者预后",
      "authors": [
        "Ahasan Ahmed",
        "Albatoul Khaled",
        "Muhammad Waqar",
        "DrJavaid Akhtar Hashmi",
        "Hazem AbdulKareem Alfanash",
        "Wesam Taher Almagharbeh",
        "Amine Hamdache",
        "Ilias Elmouki"
      ],
      "abstract": "The main goal from this study is to discuss the main features of Artificial intelligence (AI) as well as their applicability for early cardiovascular Disease (CVDs) Detection, Material and Method : Systematic review approach Results : It was seen that integrating AI algorithm the diagnosis of CVDs become more accurate and lee time consuming. Conclusion: Now the concept of using AI technologies in cardiovascular health care holds the potential to transform disease management .",
      "tldr_zh": "该研究通过系统综述(Systematic review)的方法，深入探讨了人工智能(AI)的核心特征及其在心血管疾病(CVDs)早期检测中的应用潜力。分析结果表明，整合AI算法能显著提升心血管疾病诊断的准确性，并有效缩短诊断所需的时间。研究强调，在心血管医疗领域引入AI技术有望彻底改变传统的疾病管理模式。这种AI驱动的早期检测方案对于降低整体医疗成本(Healthcare Costs)和改善患者预后(patient Outcomes)具有重要意义。总体而言，该项研究为利用先进计算技术优化心脏健康监测和临床决策提供了有力的理论支持。",
      "categories": [
        "q-bio.TO",
        "cs.AI"
      ],
      "primary_category": "q-bio.TO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08229v1",
      "published_date": "2025-06-09 20:56:14 UTC",
      "updated_date": "2025-06-09 20:56:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:07:08.693780+00:00"
    },
    {
      "arxiv_id": "2506.08228v2",
      "title": "Scaling Laws of Motion Forecasting and Planning -- Technical Report",
      "title_zh": "运动预测与规划的缩放定律：技术报告",
      "authors": [
        "Mustafa Baniodeh",
        "Kratarth Goel",
        "Scott Ettinger",
        "Carlos Fuertes",
        "Ari Seff",
        "Tim Shen",
        "Cole Gulino",
        "Chenjie Yang",
        "Ghassen Jerfel",
        "Dokook Choe",
        "Rui Wang",
        "Benjamin Charrow",
        "Vinutha Kallem",
        "Sergio Casas",
        "Rami Al-Rfou",
        "Benjamin Sapp",
        "Dragomir Anguelov"
      ],
      "abstract": "We study the empirical scaling laws of a family of encoder-decoder autoregressive transformer models on the task of joint motion forecasting and planning in the autonomous driving domain. Using a 500 thousand hours driving dataset, we demonstrate that, similar to language modeling, model performance improves as a power-law function of the total compute budget, and we observe a strong correlation between model training loss and model evaluation metrics. Most interestingly, closed-loop metrics also improve with scaling, which has important implications for the suitability of open-loop metrics for model development and hill climbing. We also study the optimal scaling of the number of transformer parameters and the training data size for a training compute-optimal model. We find that as the training compute budget grows, optimal scaling requires increasing the model size 1.5x as fast as the dataset size. We also study inference-time compute scaling, where we observe that sampling and clustering the output of smaller models makes them competitive with larger models, up to a crossover point beyond which a larger models becomes more inference-compute efficient. Overall, our experimental results demonstrate that optimizing the training and inference-time scaling properties of motion forecasting and planning models is a key lever for improving their performance to address a wide variety of driving scenarios. Finally, we briefly study the utility of training on general logged driving data of other agents to improve the performance of the ego-agent, an important research area to address the scarcity of robotics data for large capacity models training.",
      "tldr_zh": "该研究探讨了自动驾驶领域中联合运动预测与规划 (joint motion forecasting and planning) 的经验缩放定律 (Scaling Laws)，重点分析了编码器-解码器自回归 Transformer 模型在 50 万小时驾驶数据集上的表现。实验证明模型性能随计算预算 (compute budget) 的增加呈幂律 (power-law) 函数提升，且训练损失与评估指标之间存在强相关性。研究发现闭环指标 (closed-loop metrics) 同样随规模扩大而改善，这对于衡量模型在复杂驾驶场景下的实际表现具有重要意义。在构建计算最优 (compute-optimal) 模型时，研究指出模型参数规模的增速应为数据集增速的 1.5 倍。此外，研究揭示了推理阶段计算 (inference-time compute) 的缩放特性，发现通过采样和聚类优化的小模型在特定临界点前能与大模型竞争。最后，该研究证实了利用其他智能体的通用驾驶日志数据训练可显著提升主车智能体 (ego-agent) 的性能，为解决机器人领域高质量数据稀缺问题提供了有效路径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08228v2",
      "published_date": "2025-06-09 20:54:23 UTC",
      "updated_date": "2025-09-08 00:53:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:07:26.982644+00:00"
    },
    {
      "arxiv_id": "2506.08224v2",
      "title": "AI-Assisted Rapid Crystal Structure Generation Towards a Target Local Environment",
      "title_zh": "面向目标局部环境的AI辅助快速晶体结构生成",
      "authors": [
        "Osman Goni Ridwan",
        "Sylvain Pitié",
        "Monish Soundar Raj",
        "Dong Dai",
        "Gilles Frapper",
        "Hongfei Xue",
        "Qiang Zhu"
      ],
      "abstract": "In the field of material design, traditional crystal structure prediction approaches require extensive structural sampling through computationally expensive energy minimization methods using either force fields or quantum mechanical simulations. While emerging artificial intelligence (AI) generative models have shown great promise in generating realistic crystal structures more rapidly, most existing models fail to account for the unique symmetries and periodicity of crystalline materials, and they are limited to handling structures with only a few tens of atoms per unit cell. Here, we present a symmetry-informed AI generative approach called Local Environment Geometry-Oriented Crystal Generator (LEGO-xtal) that overcomes these limitations. Our method generates initial structures using AI models trained on an augmented small dataset, and then optimizes them using machine learning structure descriptors rather than traditional energy-based optimization. We demonstrate the effectiveness of LEGO-xtal by expanding from 25 known low-energy sp2 carbon allotropes to over 1,700, all within 0.5 eV/atom of the ground-state energy of graphite. This framework offers a generalizable strategy for the targeted design of materials with modular building blocks, such as metal-organic frameworks and next-generation battery materials.",
      "tldr_zh": "该研究提出了 LEGO-xtal (Local Environment Geometry-Oriented Crystal Generator)，这是一种对称性感知的人工智能生成方法，旨在克服传统晶体预测中计算昂贵的能量最小化问题及现有模型对对称性和大尺寸晶胞处理能力的不足。LEGO-xtal 利用在增强小数据集上训练的 AI 模型生成初始结构，并创新性地使用机器学习结构描述符 (machine learning structure descriptors) 而非传统的能量优化手段进行后续调整。通过该方法，研究人员将已知的 25 种低能 sp2 碳同素异形体成功扩展至 1,700 多种，且所有结构的能量均在石墨 (graphite) 基态能量的 0.5 eV/atom 范围内。实验结果证明了该框架在快速生成现实晶体结构方面的卓越效能，为金属有机框架 (metal-organic frameworks) 和下一代电池材料等模块化材料的靶向设计提供了一套普适性的新策略。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI",
        "physics.comp-ph"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "27 pages, 15 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.08224v2",
      "published_date": "2025-06-09 20:47:36 UTC",
      "updated_date": "2025-09-05 02:02:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:07:27.833116+00:00"
    },
    {
      "arxiv_id": "2506.08210v1",
      "title": "A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation",
      "title_zh": "仅解码器大语言模型在文本到图像生成中的全面研究",
      "authors": [
        "Andrew Z. Wang",
        "Songwei Ge",
        "Tero Karras",
        "Ming-Yu Liu",
        "Yogesh Balaji"
      ],
      "abstract": "Both text-to-image generation and large language models (LLMs) have made significant advancements. However, many text-to-image models still employ the somewhat outdated T5 and CLIP as their text encoders. In this work, we investigate the effectiveness of using modern decoder-only LLMs as text encoders for text-to-image diffusion models. We build a standardized training and evaluation pipeline that allows us to isolate and evaluate the effect of different text embeddings. We train a total of 27 text-to-image models with 12 different text encoders to analyze the critical aspects of LLMs that could impact text-to-image generation, including the approaches to extract embeddings, different LLMs variants, and model sizes. Our experiments reveal that the de facto way of using last-layer embeddings as conditioning leads to inferior performance. Instead, we explore embeddings from various layers and find that using layer-normalized averaging across all layers significantly improves alignment with complex prompts. Most LLMs with this conditioning outperform the baseline T5 model, showing enhanced performance in advanced visio-linguistic reasoning skills.",
      "tldr_zh": "该研究系统地探讨了将现代decoder-only LLMs作为文生图(text-to-image)扩散模型文本编码器的有效性，旨在解决现有模型普遍依赖T5和CLIP等陈旧编码器的问题。作者构建了标准化的训练与评估流程，通过对27个文生图模型及12种不同文本编码器的实验，深入分析了embedding提取方式、模型变体和规模对生成性能的影响。研究发现，业界常用的仅提取最后一层embedding作为调节(conditioning)的方法效果欠佳。相比之下，采用跨所有层的层归一化平均(layer-normalized averaging)技术能显著增强模型与复杂提示词的对齐度。实验结果显示，在这一配置下，大多数LLMs在高级视觉-语言推理(visio-linguistic reasoning)方面的表现均优于基准模型T5。该研究为文生图领域利用大规模语言模型进行文本理解提供了重要的实证指导。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08210v1",
      "published_date": "2025-06-09 20:29:53 UTC",
      "updated_date": "2025-06-09 20:29:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:07:38.626769+00:00"
    },
    {
      "arxiv_id": "2506.08185v2",
      "title": "Agentic Surgical AI: Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion in a Vision-Language-Action Framework",
      "title_zh": "代理式手术人工智能：视觉-语言-动作框架下基于离散扩散的外科医生风格指纹识别与隐私风险量化",
      "authors": [
        "Huixin Zhan",
        "Jason H. Moore"
      ],
      "abstract": "Surgeons exhibit distinct operating styles shaped by training, experience, and motor behavior-yet most surgical AI systems overlook this personalization signal. We propose a novel agentic modeling approach for surgeon-specific behavior prediction in robotic surgery, combining a discrete diffusion framework with a vision-language-action (VLA) pipeline. Gesture prediction is framed as a structured sequence denoising task, conditioned on multimodal inputs including surgical video, intent language, and personalized embeddings of surgeon identity and skill. These embeddings are encoded through natural language prompts using third-party language models, allowing the model to retain individual behavioral style without exposing explicit identity. We evaluate our method on the JIGSAWS dataset and demonstrate that it accurately reconstructs gesture sequences while learning meaningful motion fingerprints unique to each surgeon. To quantify the privacy implications of personalization, we perform membership inference attacks and find that more expressive embeddings improve task performance but simultaneously increase susceptibility to identity leakage. These findings demonstrate that while personalized embeddings improve performance, they also increase vulnerability to identity leakage, revealing the importance of balancing personalization with privacy risk in surgical modeling. Code is available at: https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.",
      "tldr_zh": "该研究针对目前手术AI系统忽视外科医生个体操作风格的问题，提出了一种基于视觉-语言-动作（Vision-Language-Action, VLA）框架的智能体建模方法，旨在实现外科医生行为预测和风格特征识别。该方法将手势预测视为结构化序列去噪任务，利用离散扩散模型（Discrete Diffusion）结合手术视频、意图语言及个性化嵌入（Personalized Embeddings）等多模态输入进行预测。通过自然语言提示编码医生的身份与技能，模型能够在不直接暴露身份的前提下捕捉独特的运动指纹（Motion Fingerprints）。在JIGSAWS数据集上的实验表明，该系统能准确重构手势序列并学习到有意义的个体行为风格。此外，研究利用成员推理攻击（Membership Inference Attacks）量化了隐私风险，发现更具表现力的嵌入在提升性能的同时也增加了身份泄露的脆弱性。这一结论揭示了手术建模中平衡个性化需求与隐私安全的重要性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08185v2",
      "published_date": "2025-06-09 19:49:55 UTC",
      "updated_date": "2025-06-14 12:02:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:07:31.743448+00:00"
    },
    {
      "arxiv_id": "2506.08184v3",
      "title": "Unable to Forget: Proactive Interference Reveals Working Memory Limits in LLMs Beyond Context Length",
      "title_zh": "无法遗忘：前摄干扰揭示了大语言模型超越上下文长度的工作记忆局限",
      "authors": [
        "Chupei Wang",
        "Jiaqiu Vince Sun"
      ],
      "abstract": "Information retrieval in Large Language Models (LLMs) is increasingly recognized as intertwined with generation capabilities rather than mere lookup. While longer contexts are often assumed to improve retrieval, the effects of intra-context interference remain understudied. To address this, we adapt the proactive interference (PI) paradigm from cognitive science, where earlier information disrupts recall of newer updates. In humans, susceptibility to such interference is inversely linked to working memory capacity. We introduce PI-LLM, an evaluation that sequentially streams semantically related key-value updates and queries only the final values. Although these final values are clearly positioned just before the query, LLM retrieval accuracy declines log-linearly toward zero as interference accumulates; errors arise from retrieving previously overwritten values. Attempts to mitigate interference via prompt engineering (e.g., instructing models to ignore earlier input) yield limited success. These findings reveal a fundamental constraint on LLMs' ability to disentangle interference and flexibly manipulate information, suggesting a working memory bottleneck beyond mere context access. This calls for approaches that strengthen models' ability to suppress irrelevant content during retrieval.",
      "tldr_zh": "该研究探讨了Large Language Models (LLMs)在面对proactive interference (PI)时的working memory局限性，挑战了长上下文必然改善检索的假设。作者借鉴认知科学范式提出了PI-LLM评估框架，通过顺序输入语义相关的键值更新并仅查询最终值来进行测试。实验结果显示，尽管正确答案紧邻查询语句，但随着干扰信息的积累，LLM的检索准确率呈对数线性下降趋近于零，错误主要源于提取了已被覆盖的旧值。此外，通过提示工程试图减轻干扰的尝试收效甚微。这些发现揭示了LLM在区分干扰和灵活操作信息方面的根本约束，表明模型存在独立于上下文长度之外的working memory瓶颈，强调了未来研究需加强模型在检索中抑制无关内容的能力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ICML 2025 Workshop on Long Context Foundation Models (ICFM). Code: https://github.com/zhuangziGiantfish/Unable-to-Forget",
      "pdf_url": "https://arxiv.org/pdf/2506.08184v3",
      "published_date": "2025-06-09 19:49:11 UTC",
      "updated_date": "2025-07-31 16:45:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-23T20:09:14.054866+00:00"
    },
    {
      "arxiv_id": "2506.08173v1",
      "title": "Repeton: Structured Bug Repair with ReAct-Guided Patch-and-Test Cycles",
      "title_zh": "Repeton：基于 ReAct 引导的“补丁-测试”循环的结构化缺陷修复",
      "authors": [
        "Nguyen Phu Vinh",
        "Anh Chung Hoang",
        "Chris Ngo",
        "Truong-Son Hy"
      ],
      "abstract": "Large Language Models (LLMs) have shown strong capabilities in code generation and comprehension, yet their application to complex software engineering tasks often suffers from low precision and limited interpretability. We present Repeton, a fully open-source framework that leverages LLMs for precise and automated code manipulation in real-world Git repositories. Rather than generating holistic fixes, Repeton operates through a structured patch-and-test pipeline: it iteratively diagnoses issues, proposes code changes, and validates each patch through automated testing. This stepwise process is guided by lightweight heuristics and development tools, avoiding reliance on embedding-based retrieval systems. Evaluated on the SWE-bench Lite benchmark, our method shows good performance compared to RAG-based methods in both patch validity and interpretability. By decomposing software engineering tasks into modular, verifiable stages, Repeton provides a practical path toward scalable and transparent autonomous debugging.",
      "tldr_zh": "尽管大型语言模型(LLMs)在代码生成和理解方面表现出色，但在处理复杂的软件工程任务时常面临精度低和可解释性受限的问题。该研究提出了Repeton，一个利用LLMs在实际Git仓库中进行精确、自动代码操作的开源框架。与生成整体修复方案不同，Repeton采用结构化的补丁与测试(patch-and-test)流水线，通过迭代诊断问题、提出代码修改并利用自动化测试验证每个补丁。该过程由轻量级启发式算法和开发工具引导，避免了对基于嵌入的检索系统(embedding-based retrieval systems)的依赖。在SWE-bench Lite基准测试上的评估结果显示，Repeton在补丁有效性和可解释性方面均优于基于检索增强生成(RAG)的方法。通过将软件工程任务分解为模块化、可验证的阶段，Repeton为实现可扩展且透明的自主调试提供了一条实用路径。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08173v1",
      "published_date": "2025-06-09 19:36:40 UTC",
      "updated_date": "2025-06-09 19:36:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:07:58.434152+00:00"
    },
    {
      "arxiv_id": "2506.08171v2",
      "title": "Worst-Case Symbolic Constraints Analysis and Generalisation with Large Language Models",
      "title_zh": "基于大语言模型的最坏情况符号约束分析与泛化",
      "authors": [
        "Daniel Koh",
        "Yannic Noller",
        "Corina S. Pasareanu",
        "Adrians Skapars",
        "Youcheng Sun"
      ],
      "abstract": "Large language models (LLMs) have demonstrated strong performance on coding tasks such as generation, completion and repair, but their ability to handle complex symbolic reasoning over code still remains underexplored. We introduce the task of worst-case symbolic constraints analysis, which requires inferring the symbolic constraints that characterise worst-case program executions; these constraints can be solved to obtain inputs that expose performance bottlenecks or denial-of-service vulnerabilities in software systems. We show that even state-of-the-art LLMs (e.g., GPT-5) struggle when applied directly on this task. To address this challenge, we propose WARP, an innovative neurosymbolic approach that computes worst-case constraints on smaller concrete input sizes using existing program analysis tools, and then leverages LLMs to generalise these constraints to larger input sizes. Concretely, WARP comprises: (1) an incremental strategy for LLM-based worst-case reasoning, (2) a solver-aligned neurosymbolic framework that integrates reinforcement learning with SMT (Satisfiability Modulo Theories) solving, and (3) a curated dataset of symbolic constraints. Experimental results show that WARP consistently improves performance on worst-case constraint reasoning. Leveraging the curated constraint dataset, we use reinforcement learning to fine-tune a model, WARP-1.0-3B, which significantly outperforms size-matched and even larger baselines. These results demonstrate that incremental constraint reasoning enhances LLMs' ability to handle symbolic reasoning and highlight the potential for deeper integration between neural learning and formal methods in rigorous program analysis.",
      "tldr_zh": "该研究探讨了大语言模型（LLMs）在复杂代码符号推理方面的局限性，并引入了旨在识别程序最坏执行情况的最坏情况符号约束分析（worst-case symbolic constraints analysis）任务。针对现有模型直接处理该任务效果不佳的问题，研究者提出了神经符号（neurosymbolic）框架 WARP，该框架先利用程序分析工具处理小规模输入，再通过 LLMs 的增量推理策略将约束泛化（generalise）至大规模输入。WARP 整合了强化学习（Reinforcement Learning）与 SMT（Satisfiability Modulo Theories）求解器，并依托精选数据集训练出性能优异的 WARP-1.0-3B 模型。实验结果显示，该方法在最坏情况约束推理上取得了显著进步，其实测表现远超多种基线模型。这项工作不仅提升了 LLMs 的符号推理能力，也为神经网络学习与形式化方法（formal methods）在严谨程序分析领域的深度融合提供了新路径。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08171v2",
      "published_date": "2025-06-09 19:33:30 UTC",
      "updated_date": "2025-09-16 10:35:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:08:29.680948+00:00"
    },
    {
      "arxiv_id": "2506.08167v1",
      "title": "UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data",
      "title_zh": "UniVarFL：面向异构数据的均匀性与方差正则化联邦学习",
      "authors": [
        "Sunny Gupta",
        "Nikita Jangid",
        "Amit Sethi"
      ],
      "abstract": "Federated Learning (FL) often suffers from severe performance degradation when faced with non-IID data, largely due to local classifier bias. Traditional remedies such as global model regularization or layer freezing either incur high computational costs or struggle to adapt to feature shifts. In this work, we propose UniVarFL, a novel FL framework that emulates IID-like training dynamics directly at the client level, eliminating the need for global model dependency. UniVarFL leverages two complementary regularization strategies during local training: Classifier Variance Regularization, which aligns class-wise probability distributions with those expected under IID conditions, effectively mitigating local classifier bias; and Hyperspherical Uniformity Regularization, which encourages a uniform distribution of feature representations across the hypersphere, thereby enhancing the model's ability to generalize under diverse data distributions. Extensive experiments on multiple benchmark datasets demonstrate that UniVarFL outperforms existing methods in accuracy, highlighting its potential as a highly scalable and efficient solution for real-world FL deployments, especially in resource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL",
      "tldr_zh": "该研究针对联邦学习(Federated Learning)在处理非独立同分布(non-IID)数据时因本地分类器偏差导致性能下降的问题，提出了UniVarFL框架。该框架通过在客户端直接模拟类独立同分布(IID-like)的训练动态，有效消除了对全局模型正则化的依赖。UniVarFL核心采用了分类器方差正则化(Classifier Variance Regularization)来对齐类概率分布以缓解偏差，并结合超球面均匀性正则化(Hyperspherical Uniformity Regularization)促进特征均匀分布以增强泛化能力。在多个基准数据集上的实验结果证明，UniVarFL在准确率上显著优于现有主流方法。该研究表明，UniVarFL为资源受限环境下的现实世界联邦学习部署提供了一个高效、可扩展且具备高准确性的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08167v1",
      "published_date": "2025-06-09 19:25:35 UTC",
      "updated_date": "2025-06-09 19:25:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:08:23.214160+00:00"
    },
    {
      "arxiv_id": "2506.08153v1",
      "title": "A Metrics-Oriented Architectural Model to Characterize Complexity on Machine Learning-Enabled Systems",
      "title_zh": "一种表征机器学习赋能系统复杂性的度量导向架构模型",
      "authors": [
        "Renato Cordeiro Ferreira"
      ],
      "abstract": "How can the complexity of ML-enabled systems be managed effectively? The goal of this research is to investigate how complexity affects ML-Enabled Systems (MLES). To address this question, this research aims to introduce a metrics-based architectural model to characterize the complexity of MLES. The goal is to support architectural decisions, providing a guideline for the inception and growth of these systems. This paper showcases the first step for creating the metrics-based architectural model: an extension of a reference architecture that can describe MLES to collect their metrics.",
      "tldr_zh": "该研究探讨了如何有效管理机器学习赋能系统(ML-enabled systems, MLES)的复杂性，并旨在引入一种面向指标的架构模型(metrics-based architectural model)来表征其复杂性。该模型的核心目标是为架构决策(architectural decisions)提供支持，并为这些系统的初期构建与后续增长提供指导原则。论文展示了构建该模型的第一步工作，即通过扩展一种能够描述MLES的参考架构(reference architecture)来专门收集相关指标。这一研究成果为量化和理解MLES在开发过程中的复杂性演化奠定了基础，有助于优化系统整体设计与管理。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "4 pages, 3 figures (2 diagrams, 1 table), to be published in CAIN 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08153v1",
      "published_date": "2025-06-09 19:02:19 UTC",
      "updated_date": "2025-06-09 19:02:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:08:05.653058+00:00"
    },
    {
      "arxiv_id": "2506.08150v1",
      "title": "Compiling Metric Temporal Answer Set Programming",
      "title_zh": "度量时序回答集程序设计的编译",
      "authors": [
        "Arvid Becker",
        "Pedro Cabalar",
        "Martin Diéguez",
        "Javier Romero",
        "Susana Hahn",
        "Torsten Schaub"
      ],
      "abstract": "We develop a computational approach to Metric Answer Set Programming (ASP) to allow for expressing quantitative temporal constrains, like durations and deadlines. A central challenge is to maintain scalability when dealing with fine-grained timing constraints, which can significantly exacerbate ASP's grounding bottleneck. To address this issue, we leverage extensions of ASP with difference constraints, a simplified form of linear constraints, to handle time-related aspects externally. Our approach effectively decouples metric ASP from the granularity of time, resulting in a solution that is unaffected by time precision.",
      "tldr_zh": "该研究开发了一种处理度量回答集程序设计(Metric Answer Set Programming, ASP)的计算方法，旨在支持持续时间和截止日期等定量时间约束的表达。针对细粒度时间约束会导致ASP产生严重的接地(grounding)瓶颈这一核心挑战，作者利用带有差分约束(difference constraints)的ASP扩展来外部处理时间相关属性。这种方法将差分约束作为线性约束的一种简化形式，能够有效解耦度量ASP与时间粒度之间的关联。该方案确保了度量ASP的性能不受时间精度的影响，为在ASP框架下高效处理具有细粒度时间要求的复杂定量约束提供了可扩展的解决方案。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08150v1",
      "published_date": "2025-06-09 18:56:57 UTC",
      "updated_date": "2025-06-09 18:56:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:08:32.272279+00:00"
    },
    {
      "arxiv_id": "2506.08149v1",
      "title": "Ego-centric Learning of Communicative World Models for Autonomous Driving",
      "title_zh": "面向自动驾驶的可通信世界模型自我中心学习",
      "authors": [
        "Hang Wang",
        "Dechen Gao",
        "Junshan Zhang"
      ],
      "abstract": "We study multi-agent reinforcement learning (MARL) for tasks in complex high-dimensional environments, such as autonomous driving. MARL is known to suffer from the \\textit{partial observability} and \\textit{non-stationarity} issues. To tackle these challenges, information sharing is often employed, which however faces major hurdles in practice, including overwhelming communication overhead and scalability concerns. By making use of generative AI embodied in world model together with its latent representation, we develop {\\it CALL}, \\underline{C}ommunic\\underline{a}tive Wor\\underline{l}d Mode\\underline{l}, for MARL, where 1) each agent first learns its world model that encodes its state and intention into low-dimensional latent representation with smaller memory footprint, which can be shared with other agents of interest via lightweight communication; and 2) each agent carries out ego-centric learning while exploiting lightweight information sharing to enrich her world model, and then exploits its generalization capacity to improve prediction for better planning. We characterize the gain on the prediction accuracy from the information sharing and its impact on performance gap. Extensive experiments are carried out on the challenging local trajectory planning tasks in the CARLA platform to demonstrate the performance gains of using \\textit{CALL}.",
      "tldr_zh": "该研究针对自动驾驶等复杂环境下的多智能体强化学习(MARL)所面临的部分可观测性(partial observability)和非平稳性(non-stationarity)挑战，提出了名为CALL(Communicative World Model)的通信世界模型。CALL利用生成式人工智能(generative AI)构建世界模型，将智能体的状态与意图编码为低维潜表征(latent representation)，通过轻量化通信实现高效的信息共享。智能体在开展自我中心学习(ego-centric learning)的同时，利用共享信息增强其世界模型的泛化能力，从而提高预测准确性并优化路径规划。在CARLA仿真平台上的实验结果表明，CALL能有效解决通信开销与可扩展性问题，在局部轨迹规划任务中取得了显著的性能提升。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08149v1",
      "published_date": "2025-06-09 18:56:40 UTC",
      "updated_date": "2025-06-09 18:56:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:08:21.211025+00:00"
    },
    {
      "arxiv_id": "2506.08147v1",
      "title": "Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models",
      "title_zh": "基于大语言模型和翻译方法的社交媒体多语言仇恨言论检测",
      "authors": [
        "Muhammad Usman",
        "Muhammad Ahmad",
        "M. Shahiki Tash",
        "Irina Gelbukh",
        "Rolando Quintero Tellez",
        "Grigori Sidorov"
      ],
      "abstract": "Social media platforms are critical spaces for public discourse, shaping opinions and community dynamics, yet their widespread use has amplified harmful content, particularly hate speech, threatening online safety and inclusivity. While hate speech detection has been extensively studied in languages like English and Spanish, Urdu remains underexplored, especially using translation-based approaches. To address this gap, we introduce a trilingual dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and Spanish (3,162 samples), collected via keyword filtering, with a balanced distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology leverages attention layers as a precursor to transformer-based models and large language models (LLMs), enhancing feature extraction for multilingual hate speech detection. For non-transformer models, we use TF-IDF for feature extraction. The dataset is benchmarked using state-of-the-art models, including GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators, following rigorous guidelines, ensured high dataset quality, achieving a Fleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5 Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of 0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B). These results reflect improvements of 8.75% in English (over SVM baseline 0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline 0.82). Our framework offers a robust solution for multilingual hate speech detection, fostering safer digital communities worldwide.",
      "tldr_zh": "该研究针对社交媒体中仇恨言论泛滥的问题，重点探讨了在 Urdu 等研究相对匮乏的语言中，如何利用翻译驱动的方法进行多语言仇恨言论检测。研究者构建了一个包含英语、Urdu 和西班牙语的混合三语数据集，共计 10,193 条推文，并确保了标注结果的高度一致性。提出的方法论将 Attention Layers 作为 Transformer 模型和 Large Language Models (LLMs) 的前置组件，以增强多语言特征提取能力。实验评估了 GPT-3.5 Turbo 和 Qwen 2.5 72B 等先进模型，并与 SVM、BERT 及 RoBERTa 等基准进行了对比。结果显示，该方案在联合多语言检测中取得了 0.88 的 Macro F1 分数，性能显著优于传统机器学习模型。相比 SVM 基线，该框架在各语种任务中均实现了 5.19% 至 8.97% 不等的性能提升，为构建全球化的安全在线环境提供了技术支持。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08147v1",
      "published_date": "2025-06-09 18:53:56 UTC",
      "updated_date": "2025-06-09 18:53:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:08:44.252885+00:00"
    },
    {
      "arxiv_id": "2506.08139v2",
      "title": "SoftStep: Learning Sparse Similarity Powers Deep Neighbor-Based Regression",
      "title_zh": "SoftStep：学习稀疏相似性驱动深度邻域回归",
      "authors": [
        "Aviad Susman",
        "Baihan Lin",
        "Mayte Suárez-Fariñas",
        "Joseph T Colonel"
      ],
      "abstract": "Neighbor-based methods are a natural alternative to linear prediction for tabular data when relationships between inputs and targets exhibit complexity such as nonlinearity, periodicity, or heteroscedasticity. Yet in deep learning on unstructured data, nonparametric neighbor-based approaches are rarely implemented in lieu of simple linear heads. This is primarily due to the ability of systems equipped with linear regression heads to co-learn internal representations along with the linear head's parameters. To unlock the full potential of neighbor-based methods in neural networks we introduce SoftStep, a parametric module that learns sparse instance-wise similarity measures directly from data. When integrated with existing neighbor-based methods, SoftStep enables regression models that consistently outperform linear heads across diverse architectures, domains, and training scenarios. We focus on regression tasks, where we show theoretically that neighbor-based prediction with a mean squared error objective constitutes a metric learning algorithm that induces well-structured embedding spaces. We then demonstrate analytically and empirically that this representational structure translates into superior performance when combined with the sparse, instance-wise similarity measures introduced by SoftStep. Beyond regression, SoftStep is a general method for learning instance-wise similarity in deep neural networks, with broad applicability to attention mechanisms, metric learning, representational alignment, and related paradigms.",
      "tldr_zh": "该研究提出了SoftStep，这是一个旨在通过学习稀疏实例级相似性度量(sparse instance-wise similarity measures)来增强深度邻居回归(deep neighbor-based regression)性能的参数化模块。尽管基于邻居的方法在处理复杂非线性数据时具有天然优势，但在深度学习中因无法有效协同学习内部表示而常被线性头(linear heads)取代。SoftStep允许回归模型直接从数据中学习相似性，从而有效解决了这一难题并显著提升了性能。研究在理论上证明了在均方误差(mean squared error)目标下，基于邻居的预测本质上是一种度量学习(metric learning)算法，能够诱导产生结构良好的嵌入空间。实验结果显示，结合了稀疏相似性度量的SoftStep在多种架构、领域和训练场景下均表现出优于传统线性头的回归结果。此外，作为一种通用的相似性学习框架，SoftStep在注意力机制(attention mechanisms)和表征对齐(representational alignment)等领域也展现出广泛的应用前景。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08139v2",
      "published_date": "2025-06-09 18:41:48 UTC",
      "updated_date": "2025-12-04 15:32:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:10:41.803295+00:00"
    },
    {
      "arxiv_id": "2506.08137v2",
      "title": "IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation",
      "title_zh": "IGraSS：基于迭代图约束语义分割的卫星影像基础设施网络识别",
      "authors": [
        "Oishee Bintey Hoque",
        "Abhijin Adiga",
        "Aniruddha Adiga",
        "Siddharth Chaudhary",
        "Madhav V. Marathe",
        "S. S. Ravi",
        "Kirti Rajagopalan",
        "Amanda Wilson",
        "Samarth Swarup"
      ],
      "abstract": "Accurate canal network mapping is essential for water management, including irrigation planning and infrastructure maintenance. State-of-the-art semantic segmentation models for infrastructure mapping, such as roads, rely on large, well-annotated remote sensing datasets. However, incomplete or inadequate ground truth can hinder these learning approaches. Many infrastructure networks have graph-level properties such as reachability to a source (like canals) or connectivity (roads) that can be leveraged to improve these existing ground truth. This paper develops a novel iterative framework IGraSS, combining a semantic segmentation module-incorporating RGB and additional modalities (NDWI, DEM)-with a graph-based ground-truth refinement module. The segmentation module processes satellite imagery patches, while the refinement module operates on the entire data viewing the infrastructure network as a graph. Experiments show that IGraSS reduces unreachable canal segments from around 18% to 3%, and training with refined ground truth significantly improves canal identification. IGraSS serves as a robust framework for both refining noisy ground truth and mapping canal networks from remote sensing imagery. We also demonstrate the effectiveness and generalizability of IGraSS using road networks as an example, applying a different graph-theoretic constraint to complete road networks.",
      "tldr_zh": "这项研究提出了IGraSS，一种旨在解决遥感影像中基础设施网络识别受限于标注数据不完整问题的迭代框架。该框架将处理局部图像的语义分割(semantic segmentation)模块与处理全局图结构的细化模块相结合，并整合了RGB、NDWI和DEM等多模态信息。通过利用水渠或道路的连通性(connectivity)和可达性(reachability)等图论属性，IGraSS能够迭代地优化噪声地面真值(ground truth)。实验表明，该方法将不可达水渠片段比例从18%显著降至3%，并有效提升了水渠识别的准确性。此外，该研究在道路网络上的应用证明了IGraSS在处理不同图论约束下的泛化能力和鲁棒性。该框架为利用遥感技术进行精确的基础设施制图和数据精炼提供了有效的技术支撑。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08137v2",
      "published_date": "2025-06-09 18:40:22 UTC",
      "updated_date": "2025-06-11 02:45:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:08:48.759738+00:00"
    },
    {
      "arxiv_id": "2506.08134v3",
      "title": "The AI Imperative: Scaling High-Quality Peer Review in Machine Learning",
      "title_zh": "AI 势在必行：机器学习高质量同行评审的规模化",
      "authors": [
        "Qiyao Wei",
        "Samuel Holt",
        "Jing Yang",
        "Markus Wulfmeier",
        "Mihaela van der Schaar"
      ],
      "abstract": "Peer review, the bedrock of scientific advancement in machine learning (ML), is strained by a crisis of scale. Exponential growth in manuscript submissions to premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite capacity of qualified reviewers, leading to concerns about review quality, consistency, and reviewer fatigue. This position paper argues that AI-assisted peer review must become an urgent research and infrastructure priority. We advocate for a comprehensive AI-augmented ecosystem, leveraging Large Language Models (LLMs) not as replacements for human judgment, but as sophisticated collaborators for authors, reviewers, and Area Chairs (ACs). We propose specific roles for AI in enhancing factual verification, guiding reviewer performance, assisting authors in quality improvement, and supporting ACs in decision-making. Crucially, we contend that the development of such systems hinges on access to more granular, structured, and ethically-sourced peer review process data. We outline a research agenda, including illustrative experiments, to develop and validate these AI assistants, and discuss significant technical and ethical challenges. We call upon the ML community to proactively build this AI-assisted future, ensuring the continued integrity and scalability of scientific validation, while maintaining high standards of peer review.",
      "tldr_zh": "该研究探讨了机器学习(ML)领域同行评审因投稿量指数级增长而面临的规模危机，指出这已导致评审质量下降、一致性受损及评审员疲劳等严重问题。作者认为AI辅助同行评审已成为迫切的研究和基础设施优先事项，并提倡构建一个AI增强的生态系统，将大语言模型(Large Language Models, LLMs)定位为作者、评审员及领域主席(Area Chairs, ACs)的精巧协作工具而非替代者。文中详细阐述了AI在强化事实核查、引导评审表现、协助质量改进以及支持ACs决策中的具体角色。研究强调，此类系统的发展关键在于获取更细粒度、结构化且来源符合伦理的评审过程数据。通过提出包含验证实验的研究议程，该文呼吁ML社区主动构建AI辅助评审的未来，旨在保持高标准同行评审的同时，确保科学验证的完整性与可扩展性。",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 3 figures. Position paper",
      "pdf_url": "https://arxiv.org/pdf/2506.08134v3",
      "published_date": "2025-06-09 18:37:14 UTC",
      "updated_date": "2025-06-27 14:00:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:11:04.543861+00:00"
    },
    {
      "arxiv_id": "2506.08119v1",
      "title": "SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents",
      "title_zh": "SOP-Bench：用于评估大语言模型智能体的复杂工业标准作业程序",
      "authors": [
        "Subhrangshu Nandi",
        "Arghya Datta",
        "Nikhil Vichare",
        "Indranil Bhattacharya",
        "Huzefa Raja",
        "Jing Xu",
        "Shayan Ray",
        "Giuseppe Carenini",
        "Abhi Srivastava",
        "Aaron Chan",
        "Man Ho Woo",
        "Amar Kandola",
        "Brandon Theresa",
        "Francesco Carbone"
      ],
      "abstract": "Large Language Models (LLMs) demonstrate impressive general-purpose reasoning and problem-solving abilities. However, they struggle with executing complex, long-horizon workflows that demand strict adherence to Standard Operating Procedures (SOPs), a critical requirement for real-world industrial automation. Despite this need, there is a lack of public benchmarks that reflect the complexity, structure, and domain-specific nuances of SOPs. To address this, we present three main contributions. First, we introduce a synthetic data generation framework to create realistic, industry-grade SOPs that rigorously test the planning, reasoning, and tool-use capabilities of LLM-based agents. Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases. Third, we evaluate two prominent agent architectures: Function-Calling and ReAct Agents, on SOP-Bench, observing average success rates of only 27% and 48%, respectively. Remarkably, when the tool registry is much larger than necessary, agents invoke incorrect tools nearly 100% of the time. These findings underscore a substantial gap between current agentic capabilities of LLMs and the demands of automating real-world SOPs. Performance varies significantly by task and domain, highlighting the need for domain-specific benchmarking and architectural choices before deployment. SOP-Bench is publicly available at http://sop-bench.s3-website-us-west-2.amazonaws.com/. We also release the prompts underpinning the data generation framework to support new domain-specific SOP benchmarks. We invite the community to extend SOP-Bench with SOPs from their industrial domains.",
      "tldr_zh": "该研究针对大语言模型(LLMs)在执行需要严格遵循标准作业程序(SOPs)的复杂长流程任务中的局限性，提出了SOP-Bench基准测试。作者首先开发了一个合成数据生成框架，用于创建能够严苛测试基于LLM的智能体(Agents)在规划、推理和工具调用(tool-use)能力的工业级SOPs。基于该框架构建的SOP-Bench包含跨越10个工业领域的1800多项任务，每项任务均配备了API、工具接口和经过人工验证的测试用例。研究通过评估Function-Calling和ReAct Agents两种主流架构发现，其平均成功率分别仅为27%和48%，且在工具注册表规模较大时极易发生调用错误。这一结果揭示了当前LLMs的智能体能力与自动化真实工业SOP需求之间存在巨大鸿沟。该研究强调了在实际工业部署前，进行特定领域基准测试和架构优化的必要性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2506.08119v1",
      "published_date": "2025-06-09 18:20:12 UTC",
      "updated_date": "2025-06-09 18:20:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:11:01.036776+00:00"
    },
    {
      "arxiv_id": "2506.08113v2",
      "title": "Benchmarking Pre-Trained Time Series Models for Electricity Price Forecasting",
      "title_zh": "电价预测中的预训练时间序列模型基准测试",
      "authors": [
        "Timothée Hornek Amir Sartipi",
        "Igor Tchappi",
        "Gilbert Fridgen"
      ],
      "abstract": "Accurate electricity price forecasting (EPF) is crucial for effective decision-making in power trading on the spot market. While recent advances in generative artificial intelligence (GenAI) and pre-trained large language models (LLMs) have inspired the development of numerous time series foundation models (TSFMs) for time series forecasting, their effectiveness in EPF remains uncertain. To address this gap, we benchmark several state-of-the-art pretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and TimeGPT--against established statistical and machine learning (ML) methods for EPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany, France, the Netherlands, Austria, and Belgium, we generate daily forecasts with a one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the TSFMs, performing on par with traditional models. However, the biseasonal MSTL model, which captures daily and weekly seasonality, stands out for its consistent performance across countries and evaluation metrics, with no TSFM statistically outperforming it.",
      "tldr_zh": "这项研究针对电价预测 (EPF) 任务，对多种先进的预训练时间序列基础模型 (TSFMs) 进行了基准测试，包括 Chronos-Bolt、Chronos-T5、TimesFM、Moirai、Time-MoE 和 TimeGPT。研究人员将这些模型与传统的统计及机器学习方法在 2024 年德国、法国、荷兰、奥地利和比利时的日前拍卖 (DAA) 电价数据上进行了对比分析。实验发现，Chronos-Bolt 和 Time-MoE 在 TSFMs 中表现最为强劲，其预测能力与传统模型基本持平。然而，能够捕捉日度和周度季节性特征的双季节 MSTL 模型在各国数据及多项评价指标中表现最为稳健，且目前尚无任何 TSFM 在统计学意义上能显著超越该模型。该研究为生成式人工智能 (GenAI) 及大语言模型 (LLMs) 在电力市场决策应用中的有效性提供了重要的实证参考。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-fin.ST"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08113v2",
      "published_date": "2025-06-09 18:10:00 UTC",
      "updated_date": "2025-08-20 07:59:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:11:03.277746+00:00"
    },
    {
      "arxiv_id": "2506.08098v1",
      "title": "Cognitive Weave: Synthesizing Abstracted Knowledge with a Spatio-Temporal Resonance Graph",
      "title_zh": "Cognitive Weave：基于时空共振图的抽象知识合成",
      "authors": [
        "Akash Vishwakarma",
        "Hojin Lee",
        "Mohith Suresh",
        "Priyam Shankar Sharma",
        "Rahul Vishwakarma",
        "Sparsh Gupta",
        "Yuvraj Anupam Chauhan"
      ],
      "abstract": "The emergence of capable large language model (LLM) based agents necessitates memory architectures that transcend mere data storage, enabling continuous learning, nuanced reasoning, and dynamic adaptation. Current memory systems often grapple with fundamental limitations in structural flexibility, temporal awareness, and the ability to synthesize higher-level insights from raw interaction data. This paper introduces Cognitive Weave, a novel memory framework centered around a multi-layered spatio-temporal resonance graph (STRG). This graph manages information as semantically rich insight particles (IPs), which are dynamically enriched with resonance keys, signifiers, and situational imprints via a dedicated semantic oracle interface (SOI). These IPs are interconnected through typed relational strands, forming an evolving knowledge tapestry. A key component of Cognitive Weave is the cognitive refinement process, an autonomous mechanism that includes the synthesis of insight aggregates (IAs) condensed, higher-level knowledge structures derived from identified clusters of related IPs. We present comprehensive experimental results demonstrating Cognitive Weave's marked enhancement over existing approaches in long-horizon planning tasks, evolving question-answering scenarios, and multi-session dialogue coherence. The system achieves a notable 34% average improvement in task completion rates and a 42% reduction in mean query latency when compared to state-of-the-art baselines. Furthermore, this paper explores the ethical considerations inherent in such advanced memory systems, discusses the implications for long-term memory in LLMs, and outlines promising future research trajectories.",
      "tldr_zh": "该研究针对现有大语言模型(LLM)智能体记忆系统在结构灵活性、时间感知以及高层见解合成能力的局限性，提出了名为Cognitive Weave的新型记忆框架。该框架以多层时空共鸣图(Spatio-Temporal Resonance Graph, STRG)为核心，将信息作为语义丰富的见解粒子(insight particles, IPs)进行管理，并利用语义预言机接口(semantic oracle interface, SOI)为其注入动态的共鸣键与情境印记。通过自主的认知细化过程(cognitive refinement process)，系统能够将相关的IPs聚类并合成为更高层次的见解聚合体(insight aggregates, IAs)，形成演进的知识织锦。实验结果表明，Cognitive Weave在长程规划任务、演进式问答及多轮对话连贯性方面显著优于现有方法，任务完成率平均提升34%，且平均查询延迟降低了42%。该研究还探讨了此类先进记忆系统的伦理影响及LLM长期记忆的未来发展轨迹。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08098v1",
      "published_date": "2025-06-09 18:00:46 UTC",
      "updated_date": "2025-06-09 18:00:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:11:15.841937+00:00"
    },
    {
      "arxiv_id": "2506.08013v1",
      "title": "StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets",
      "title_zh": "StableMTL：基于部分标注的合成数据集将潜在扩散模型应用于多任务学习",
      "authors": [
        "Anh-Quan Cao",
        "Ivan Lopes",
        "Raoul de Charette"
      ],
      "abstract": "Multi-task learning for dense prediction is limited by the need for extensive annotation for every task, though recent works have explored training with partial task labels. Leveraging the generalization power of diffusion models, we extend the partial learning setup to a zero-shot setting, training a multi-task model on multiple synthetic datasets, each labeled for only a subset of tasks. Our method, StableMTL, repurposes image generators for latent regression. Adapting a denoising framework with task encoding, per-task conditioning and a tailored training scheme. Instead of per-task losses requiring careful balancing, a unified latent loss is adopted, enabling seamless scaling to more tasks. To encourage inter-task synergy, we introduce a multi-stream model with a task-attention mechanism that converts N-to-N task interactions into efficient 1-to-N attention, promoting effective cross-task sharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.",
      "tldr_zh": "该研究提出了 StableMTL，旨在利用部分标注的合成数据集，将潜在扩散模型 (Latent Diffusion Models) 重新应用于多任务学习 (Multi-Task Learning) 中的密集预测任务。StableMTL 通过改造去噪框架并结合任务编码与特定任务调节，将图像生成器转化为潜空间回归模型。为了避免复杂的跨任务损失平衡问题，该方法采用了统一的潜空间损失 (Unified Latent Loss)，从而实现了对更多任务的无缝扩展。此外，研究引入了一种带有任务注意力机制 (Task-Attention Mechanism) 的多流模型，将复杂的 N-to-N 任务交互转化为高效的 1-to-N 注意力，有效提升了跨任务的协同与共享能力。实验结果表明，StableMTL 在 8 个基准测试的 7 项任务中表现均优于现有基准模型，展示了其在零样本 (Zero-Shot) 设置下的强大泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Code is available at https://github.com/astra-vision/StableMTL",
      "pdf_url": "https://arxiv.org/pdf/2506.08013v1",
      "published_date": "2025-06-09 17:59:59 UTC",
      "updated_date": "2025-06-09 17:59:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:11:15.138996+00:00"
    },
    {
      "arxiv_id": "2506.08012v1",
      "title": "GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior",
      "title_zh": "GUI-Reflection：以自反思行为赋能多模态 GUI 模型",
      "authors": [
        "Penghao Wu",
        "Shengnan Ma",
        "Bo Wang",
        "Jiaheng Yu",
        "Lewei Lu",
        "Ziwei Liu"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have shown great potential in revolutionizing Graphical User Interface (GUI) automation. However, existing GUI models mostly rely on learning from nearly error-free offline trajectories, thus lacking reflection and error recovery capabilities. To bridge this gap, we propose GUI-Reflection, a novel framework that explicitly integrates self-reflection and error correction capabilities into end-to-end multimodal GUI models throughout dedicated training stages: GUI-specific pre-training, offline supervised fine-tuning (SFT), and online reflection tuning. GUI-reflection enables self-reflection behavior emergence with fully automated data generation and learning processes without requiring any human annotation. Specifically, 1) we first propose scalable data pipelines to automatically construct reflection and error correction data from existing successful trajectories. While existing GUI models mainly focus on grounding and UI understanding ability, we propose the GUI-Reflection Task Suite to learn and evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a diverse and efficient environment for online training and data collection of GUI models on mobile devices. 3) We also present an iterative online reflection tuning algorithm leveraging the proposed environment, enabling the model to continuously enhance its reflection and error correction abilities. Our framework equips GUI agents with self-reflection and correction capabilities, paving the way for more robust, adaptable, and intelligent GUI automation, with all data, models, environments, and tools to be released publicly.",
      "tldr_zh": "该研究针对多模态大语言模型（MLLMs）在图形用户界面（GUI）自动化中缺乏错误恢复能力的现状，提出了 GUI-Reflection 框架，旨在将自我反思（self-reflection）和错误纠正（error correction）功能显式集成到端到端模型中。该框架通过 GUI 专用预训练、离线监督微调（SFT）和在线反思微调（online reflection tuning）三个关键阶段，实现了在无人工标注情况下的反思行为自动涌现。研究团队开发了可扩展的数据流水线以自动构建反思数据，并推出了 GUI-Reflection Task Suite 以量化评估模型面向反思的能力。此外，该项工作还建立了一个高效的移动端在线训练环境，并配合迭代式在线反思微调算法，使模型能够持续优化其纠错行为。GUI-Reflection 显著增强了 GUI 智能体的鲁棒性和环境适应性，为构建更智能、更可靠的自动化系统提供了全套的数据、模型和工具支持。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Project Page at https://penghao-wu.github.io/GUI_Reflection/",
      "pdf_url": "https://arxiv.org/pdf/2506.08012v1",
      "published_date": "2025-06-09 17:59:57 UTC",
      "updated_date": "2025-06-09 17:59:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:11:13.312200+00:00"
    },
    {
      "arxiv_id": "2506.08010v5",
      "title": "Vision Transformers Don't Need Trained Registers",
      "title_zh": "Vision Transformers 无需经过训练的寄存器",
      "authors": [
        "Nick Jiang",
        "Amil Dravid",
        "Alexei Efros",
        "Yossi Gandelsman"
      ],
      "abstract": "We investigate the mechanism underlying a previously identified phenomenon in Vision Transformers - the emergence of high-norm tokens that lead to noisy attention maps (Darcet et al., 2024). We observe that in multiple models (e.g., CLIP, DINOv2), a sparse set of neurons is responsible for concentrating high-norm activations on outlier tokens, leading to irregular attention patterns and degrading downstream visual processing. While the existing solution for removing these outliers involves retraining models from scratch with additional learned register tokens, we use our findings to create a training-free approach to mitigate these artifacts. By shifting the high-norm activations from our discovered register neurons into an additional untrained token, we can mimic the effect of register tokens on a model already trained without registers. We demonstrate that our method produces cleaner attention and feature maps, enhances performance over base models across multiple downstream visual tasks, and achieves results comparable to models explicitly trained with register tokens. We then extend test-time registers to off-the-shelf vision-language models, yielding cleaner attention-based, text-to-image attribution. Finally, we outline a simple mathematical model that reflects the observed behavior of register neurons and high norm tokens. Our results suggest that test-time registers effectively take on the role of register tokens at test-time, offering a training-free solution for any pre-trained model released without them.",
      "tldr_zh": "该项研究深入探讨了 Vision Transformers (ViTs) 中高范数 token 导致注意力图噪声的潜在机制，发现 CLIP 和 DINOv2 等模型中特定的稀疏神经元是产生离群 token 激活的主要原因。研究提出了一种名为测试时寄存器 (test-time registers) 的无需训练方法，通过将发现的寄存器神经元的高范数激活转移至一个额外的未训练 token，成功模拟了专门训练的寄存器 tokens 的功能。实验证明该方法有效净化了注意力图和特征图，在多项下游视觉任务中提升了模型表现，且效果与经过完整训练的模型相当。此外，该技术被成功扩展至视觉语言模型 (vision-language models) 以优化文本到图像的归因分析，为所有未配备寄存器的预训练模型提供了一种低成本且高效的性能优化方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page and code: https://avdravid.github.io/test-time-registers. Accepted to NeurIPS '25 (spotlight)",
      "pdf_url": "https://arxiv.org/pdf/2506.08010v5",
      "published_date": "2025-06-09 17:59:57 UTC",
      "updated_date": "2025-10-25 03:27:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:11:19.900813+00:00"
    },
    {
      "arxiv_id": "2506.08009v2",
      "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion",
      "title_zh": "Self Forcing：弥合自回归视频扩散中的训练-测试鸿沟",
      "authors": [
        "Xun Huang",
        "Zhengqi Li",
        "Guande He",
        "Mingyuan Zhou",
        "Eli Shechtman"
      ],
      "abstract": "We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models. Project website: http://self-forcing.github.io/",
      "tldr_zh": "该研究提出了 Self Forcing，这是一种针对自回归视频扩散模型(Autoregressive video diffusion models)的新型训练范式，旨在解决传统模型在推理时因依赖自身生成内容而产生的曝光偏差(Exposure bias)问题。与以往基于真实上下文帧的方法不同，Self Forcing 在训练过程中通过带键值缓存(KV caching)的自回归展开(Autoregressive rollout)，使每一帧的生成都以之前生成的输出为条件。该策略通过视频级别的整体损失进行监督，直接评估整个生成序列的质量，而非仅依赖传统的帧级目标。为了保证训练效率，研究团队采用了少量步数的扩散模型和随机梯度截断(Stochastic gradient truncation)策略，在计算成本与性能之间取得了平衡。此外，该方法引入了滚动键值(Rolling KV cache)机制，实现了高效的自回归视频外推。实验证明，Self Forcing 能够在单 GPU 上实现亚秒级延迟的实时流式视频生成，其生成质量能够达到甚至超过那些速度更慢的非因果(Non-causal)扩散模型。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2025 spotlight. Project website: http://self-forcing.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2506.08009v2",
      "published_date": "2025-06-09 17:59:55 UTC",
      "updated_date": "2025-11-10 04:36:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:11:22.805326+00:00"
    },
    {
      "arxiv_id": "2506.08008v1",
      "title": "Hidden in plain sight: VLMs overlook their visual representations",
      "title_zh": "视而不见：VLMs 忽视了其自身的视觉表示",
      "authors": [
        "Stephanie Fu",
        "Tyler Bonnen",
        "Devin Guillory",
        "Trevor Darrell"
      ],
      "abstract": "Language provides a natural interface to specify and evaluate performance on visual tasks. To realize this possibility, vision language models (VLMs) must successfully integrate visual and linguistic information. Our work compares VLMs to a direct readout of their visual encoders to understand their ability to integrate across these modalities. Across a series of vision-centric benchmarks (e.g., depth estimation, correspondence), we find that VLMs perform substantially worse than their visual encoders, dropping to near-chance performance. We investigate these results through a series of analyses across the entire VLM: namely 1) the degradation of vision representations, 2) brittleness to task prompt, and 3) the language model's role in solving the task. We find that the bottleneck in performing these vision-centric tasks lies in this third category; VLMs are not effectively using visual information easily accessible throughout the entire model, and they inherit the language priors present in the LLM. Our work helps diagnose the failure modes of open-source VLMs, and presents a series of evaluations useful for future investigations into visual understanding within VLMs.",
      "tldr_zh": "该研究探讨了视觉语言模型 (VLMs) 在整合视觉和语言信息方面的局限性，通过对比 VLMs 与其视觉编码器 (visual encoders) 在深度估计 (depth estimation) 和对应关系 (correspondence) 等视觉中心任务中的表现，发现 VLMs 的性能远低于其直接读取的视觉编码器，甚至接近随机水平。通过对视觉表示退化、任务提示词 (task prompt) 的脆弱性以及语言模型 (LLM) 作用的深入分析，研究指出性能瓶颈在于 LLM 无法有效利用模型内部易于获取的视觉信息。此外，研究揭示了 VLMs 容易继承 LLM 中的语言先验 (language priors)，从而导致在视觉任务中表现不佳。该工作不仅帮助诊断了开源 VLMs 的失效模式，还为未来提升 VLMs 的视觉理解能力提供了一系列有价值的评估基准。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://hidden-plain-sight.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2506.08008v1",
      "published_date": "2025-06-09 17:59:54 UTC",
      "updated_date": "2025-06-09 17:59:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:11:34.815467+00:00"
    },
    {
      "arxiv_id": "2506.08004v1",
      "title": "Dynamic View Synthesis as an Inverse Problem",
      "title_zh": "将动态视图合成视为逆问题",
      "authors": [
        "Hidir Yesiltepe",
        "Pinar Yanardag"
      ],
      "abstract": "In this work, we address dynamic view synthesis from monocular videos as an inverse problem in a training-free setting. By redesigning the noise initialization phase of a pre-trained video diffusion model, we enable high-fidelity dynamic view synthesis without any weight updates or auxiliary modules. We begin by identifying a fundamental obstacle to deterministic inversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and resolve it by introducing a novel noise representation, termed K-order Recursive Noise Representation. We derive a closed form expression for this representation, enabling precise and efficient alignment between the VAE-encoded and the DDIM inverted latents. To synthesize newly visible regions resulting from camera motion, we introduce Stochastic Latent Modulation, which performs visibility aware sampling over the latent space to complete occluded regions. Comprehensive experiments demonstrate that dynamic view synthesis can be effectively performed through structured latent manipulation in the noise initialization phase.",
      "tldr_zh": "该研究将单目视频的dynamic view synthesis定义为一种inverse problem，并提出了一种在training-free设置下实现高保真合成的新方法。通过重新设计预训练video diffusion model的噪声初始化阶段，该方法无需任何权重更新或辅助模块即可运行。研究者通过引入K-order Recursive Noise Representation解决了由zero-terminal SNR调度带来的deterministic inversion障碍，并推导出闭式表达式以实现VAE编码潜变量与DDIM反转潜变量的精确对齐。为了处理相机运动产生的遮挡区域，研究进一步提出了Stochastic Latent Modulation技术，利用可见性感知采样完成区域补全。实验结果表明，通过在噪声初始化阶段进行结构化的潜变量操作，可以有效实现高质量的动态视图合成。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://inverse-dvs.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2506.08004v1",
      "published_date": "2025-06-09 17:59:47 UTC",
      "updated_date": "2025-06-09 17:59:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:11:34.657880+00:00"
    },
    {
      "arxiv_id": "2506.08003v1",
      "title": "Audio-Sync Video Generation with Multi-Stream Temporal Control",
      "title_zh": "基于多流时序控制的音视频同步视频生成",
      "authors": [
        "Shuchen Weng",
        "Haojie Zheng",
        "Zheng Chang",
        "Si Li",
        "Boxin Shi",
        "Xinlong Wang"
      ],
      "abstract": "Audio is inherently temporal and closely synchronized with the visual world, making it a naturally aligned and expressive control signal for controllable video generation (e.g., movies). Beyond control, directly translating audio into video is essential for understanding and visualizing rich audio narratives (e.g., Podcasts or historical recordings). However, existing approaches fall short in generating high-quality videos with precise audio-visual synchronization, especially across diverse and complex audio types. In this work, we introduce MTV, a versatile framework for audio-sync video generation. MTV explicitly separates audios into speech, effects, and music tracks, enabling disentangled control over lip motion, event timing, and visual mood, respectively -- resulting in fine-grained and semantically aligned video generation. To support the framework, we additionally present DEMIX, a dataset comprising high-quality cinematic videos and demixed audio tracks. DEMIX is structured into five overlapped subsets, enabling scalable multi-stage training for diverse generation scenarios. Extensive experiments demonstrate that MTV achieves state-of-the-art performance across six standard metrics spanning video quality, text-video consistency, and audio-video alignment. Project page: https://hjzheng.net/projects/MTV/.",
      "tldr_zh": "该研究提出了MTV框架，旨在解决现有可控视频生成中音画同步不精确、难以处理复杂音频类型的问题。MTV通过将音频显式解耦为Speech、Effects和Music三个独立轨道，实现了对Lip motion、Event timing和Visual mood的解耦控制，从而生成语义对齐且细粒度的视频内容。为了支撑这一框架，研究者还推出了包含高质量电影视频及其解混音轨的DEMIX数据集，通过五个重叠子集支持多阶段训练。实验结果显示，MTV在视频质量、Text-video consistency以及Audio-video alignment等六项标准指标上均取得了SOTA性能，展现了其在理解和可视化丰富音频叙事方面的卓越能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08003v1",
      "published_date": "2025-06-09 17:59:42 UTC",
      "updated_date": "2025-06-09 17:59:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:11:48.198182+00:00"
    },
    {
      "arxiv_id": "2506.08001v4",
      "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation",
      "title_zh": "基于正交等价变换的 LLM 重参数化训练",
      "authors": [
        "Zeju Qiu",
        "Simon Buchholz",
        "Tim Z. Xiao",
        "Maximilian Dax",
        "Bernhard Schölkopf",
        "Weiyang Liu"
      ],
      "abstract": "While large language models (LLMs) are driving the rapid advancement of artificial intelligence, effectively and reliably training these large models remains one of the field's most significant challenges. To address this challenge, we propose POET, a novel reParameterized training algorithm that uses Orthogonal Equivalence Transformation to optimize neurons. Specifically, POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix. Because of its provable preservation of spectral properties of weight matrices, POET can stably optimize the objective function with improved generalization. We further develop efficient approximations that make POET flexible and scalable for training large-scale neural networks. Extensive experiments validate the effectiveness and scalability of POET in training LLMs.",
      "tldr_zh": "该研究提出了POET，这是一种基于正交等效变换(Orthogonal Equivalence Transformation)的新型参数重构(reParameterized)训练算法，旨在解决大型语言模型(LLMs)训练中的效率与可靠性挑战。POET通过两个可学习的正交矩阵(orthogonal matrices)和一个固定的随机权重矩阵对每个神经元进行重构。由于该方法在理论上能够保留权重矩阵的光谱特性(spectral properties)，因此POET可以稳定地优化目标函数并显著提高泛化能力(generalization)。此外，研究团队还开发了高效的近似方案，使POET在训练大规模神经网络时具有良好的灵活性和可扩展性(scalability)。广泛的实验结果验证了POET在训练LLMs时的有效性，为大型模型的优化提供了更稳健的路径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025 (40 pages, 26 figures, project page: https://spherelab.ai/poet/, v4: added experiments of finetuning and larger-scale pretraining)",
      "pdf_url": "https://arxiv.org/pdf/2506.08001v4",
      "published_date": "2025-06-09 17:59:34 UTC",
      "updated_date": "2025-12-10 21:22:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:11:51.864184+00:00"
    },
    {
      "arxiv_id": "2506.08074v1",
      "title": "Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval",
      "title_zh": "旨在增强多跳检索的层级化词法图",
      "authors": [
        "Abdellah Ghassel",
        "Ian Robinson",
        "Gabriel Tanase",
        "Hal Cooper",
        "Bryan Thompson",
        "Zhen Han",
        "Vassilis N. Ioannidis",
        "Soji Adeshina",
        "Huzefa Rangwala"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) grounds large language models in external evidence, yet it still falters when answers must be pieced together across semantically distant documents. We close this gap with the Hierarchical Lexical Graph (HLG), a three-tier index that (i) traces every atomic proposition to its source, (ii) clusters propositions into latent topics, and (iii) links entities and relations to expose cross-document paths. On top of HLG we build two complementary, plug-and-play retrievers: StatementGraphRAG, which performs fine-grained entity-aware beam search over propositions for high-precision factoid questions, and TopicGraphRAG, which selects coarse topics before expanding along entity links to supply broad yet relevant context for exploratory queries. Additionally, existing benchmarks lack the complexity required to rigorously evaluate multi-hop summarization systems, often focusing on single-document queries or limited datasets. To address this, we introduce a synthetic dataset generation pipeline that curates realistic, multi-document question-answer pairs, enabling robust evaluation of multi-hop retrieval systems. Extensive experiments across five datasets demonstrate that our methods outperform naive chunk-based RAG achieving an average relative improvement of 23.1% in retrieval recall and correctness. Open-source Python library is available at https://github.com/awslabs/graphrag-toolkit.",
      "tldr_zh": "该研究针对检索增强生成(Retrieval-Augmented Generation, RAG)在处理跨文档多跳检索(multi-hop retrieval)时存在的语义关联困难，提出了层级词法图(Hierarchical Lexical Graph, HLG)。HLG 采用三层索引结构，通过追踪原子命题(atomic proposition)、将命题聚类为潜在主题(latent topics)以及链接实体与关系，有效揭示了跨文档的关联路径。基于 HLG，该研究构建了两种即插即用的检索器：StatementGraphRAG 通过细粒度的实体感知束搜索(beam search)解决高精度事实性问题，而 TopicGraphRAG 则通过主题扩展为探索性查询提供丰富的上下文。此外，研究团队还开发了一个合成数据集生成流水线，用以弥补现有基准测试在复杂多跳评估方面的短板。实验结果表明，HLG 在五个数据集上的表现显著优于传统的基于块(chunk-based)的 RAG，在检索召回率和正确性上实现了 23.1% 的平均相对提升。目前，该成果已作为开源 Python 库发布，为增强长文本关联推理提供了有力工具。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "KDD '25",
      "pdf_url": "https://arxiv.org/pdf/2506.08074v1",
      "published_date": "2025-06-09 17:58:35 UTC",
      "updated_date": "2025-06-09 17:58:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:11:55.716717+00:00"
    },
    {
      "arxiv_id": "2506.08073v1",
      "title": "Domain Switching on the Pareto Front: Multi-Objective Deep Kernel Learning in Automated Piezoresponse Force Microscopy",
      "title_zh": "帕累托前沿上的畴翻转：自动化压电力显微术中的多目标深度核学习",
      "authors": [
        "Yu Liu",
        "Utkarsh Pratiush",
        "Kamyar Barakati",
        "Hiroshi Funakubo",
        "Ching-Che Lin",
        "Jaegyu Kim",
        "Lane W. Martin",
        "Sergei V. Kalinin"
      ],
      "abstract": "Ferroelectric polarization switching underpins the functional performance of a wide range of materials and devices, yet its dependence on complex local microstructural features renders systematic exploration by manual or grid-based spectroscopic measurements impractical. Here, we introduce a multi-objective kernel-learning workflow that infers the microstructural rules governing switching behavior directly from high-resolution imaging data. Applied to automated piezoresponse force microscopy (PFM) experiments, our framework efficiently identifies the key relationships between domain-wall configurations and local switching kinetics, revealing how specific wall geometries and defect distributions modulate polarization reversal. Post-experiment analysis projects abstract reward functions, such as switching ease and domain symmetry, onto physically interpretable descriptors including domain configuration and proximity to boundaries. This enables not only high-throughput active learning, but also mechanistic insight into the microstructural control of switching phenomena. While demonstrated for ferroelectric domain switching, our approach provides a powerful, generalizable tool for navigating complex, non-differentiable design spaces, from structure-property correlations in molecular discovery to combinatorial optimization across diverse imaging modalities.",
      "tldr_zh": "该研究针对铁电极化翻转(Ferroelectric polarization switching)受复杂微观结构影响而难以通过常规手段探索的问题，提出了一种多目标核学习(Multi-objective kernel-learning)工作流。该框架应用于自动化压电力显微镜(Piezoresponse Force Microscopy, PFM)实验，能够直接从高分辨率成像数据中推断控制翻转行为的微观结构规则。通过该工作流，研究者高效识别了畴壁(Domain-wall)配置与局部翻转动力学之间的关键关系，揭示了特定壁几何形状和缺陷分布如何调节极化翻转过程。实验后的分析将翻转难易程度和畴对称性等抽象奖励函数投影到物理可解释的描述符上，例如畴配置及边界邻近度。这不仅实现了高通量的主动学习(Active learning)，还为微观结构控制翻转现象提供了深刻的机械论见解。该方法作为一种强有力的通用工具，为从分子发现到多模态成像组合优化等复杂且不可微的设计空间导航提供了新途径。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cond-mat.mes-hall",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08073v1",
      "published_date": "2025-06-09 17:58:27 UTC",
      "updated_date": "2025-06-09 17:58:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:12:00.943639+00:00"
    },
    {
      "arxiv_id": "2506.10024v1",
      "title": "Private Memorization Editing: Turning Memorization into a Defense to Strengthen Data Privacy in Large Language Models",
      "title_zh": "Private Memorization Editing：将记忆转化为防御机制以强化大语言模型的数据隐私保护",
      "authors": [
        "Elena Sofia Ruzzetti",
        "Giancarlo A. Xompero",
        "Davide Venditti",
        "Fabio Massimo Zanzotto"
      ],
      "abstract": "Large Language Models (LLMs) memorize, and thus, among huge amounts of uncontrolled data, may memorize Personally Identifiable Information (PII), which should not be stored and, consequently, not leaked. In this paper, we introduce Private Memorization Editing (PME), an approach for preventing private data leakage that turns an apparent limitation, that is, the LLMs' memorization ability, into a powerful privacy defense strategy. While attacks against LLMs have been performed exploiting previous knowledge regarding their training data, our approach aims to exploit the same kind of knowledge in order to make a model more robust. We detect a memorized PII and then mitigate the memorization of PII by editing a model knowledge of its training data. We verify that our procedure does not affect the underlying language model while making it more robust against privacy Training Data Extraction attacks. We demonstrate that PME can effectively reduce the number of leaked PII in a number of configurations, in some cases even reducing the accuracy of the privacy attacks to zero.",
      "tldr_zh": "该研究针对大语言模型 (LLMs) 在训练中由于记忆效应可能导致个人身份信息 (PII) 泄露的问题，提出了私有记忆编辑 (Private Memorization Editing, PME) 方法。PME 将模型的记忆能力转化为一种隐私防御手段，通过检测已记忆的 PII 并对模型内部关于训练数据的知识进行定向编辑，以消除敏感信息的记忆。该过程在保持基础模型原有性能的同时，显著提升了模型抵御训练数据提取攻击 (Training Data Extraction attacks) 的能力。实验结果显示，PME 在多种配置下均能有效减少 PII 的泄露量，在部分极端情况下甚至能将隐私攻击的准确率降低至零。该方法为增强大规模生成式模型的隐私安全性提供了一种从模型编辑角度出发的新颖策略。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "To be published at ACL 2025 (Main)",
      "pdf_url": "https://arxiv.org/pdf/2506.10024v1",
      "published_date": "2025-06-09 17:57:43 UTC",
      "updated_date": "2025-06-09 17:57:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:12:03.098512+00:00"
    },
    {
      "arxiv_id": "2506.07982v1",
      "title": "$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment",
      "title_zh": "$τ^2$-Bench：双重控制环境下的对话智能体评估",
      "authors": [
        "Victor Barres",
        "Honghua Dong",
        "Soham Ray",
        "Xujie Si",
        "Karthik Narasimhan"
      ],
      "abstract": "Existing benchmarks for conversational AI agents simulate single-control environments, where only the AI agent can use tools to interact with the world, while the user remains a passive information provider. This differs from real-world scenarios like technical support, where users need to actively participate in modifying the state of the (shared) world. In order to address this gap, we introduce $τ^2$-bench, with four key contributions:\n  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both agent and user make use of tools to act in a shared, dynamic environment that tests both agent coordination and communication,\n  2) A compositional task generator that programmatically creates diverse, verifiable tasks from atomic components, ensuring domain coverage and controlled complexity,\n  3) A reliable user simulator tightly coupled with the environment, whose behavior is constrained by tools and observable states, improving simulation fidelity,\n  4) Fine-grained analysis of agent performance through multiple ablations including separating errors arising from reasoning vs communication/coordination.\n  In particular, our experiments show significant performance drops when agents shift from no-user to dual-control, highlighting the challenges of guiding users. Overall, $τ^2$-bench provides a controlled testbed for agents that must both reason effectively and guide user actions.",
      "tldr_zh": "该研究提出了 $\\tau^2$-bench，旨在解决现有对话 AI 基准仅局限于单控制 (single-control) 环境而忽视用户主动参与的问题。该基准引入了一个基于 Dec-POMDP 建模的电信 (Telecom) 双控制 (dual-control) 领域，要求智能体与用户在共享且动态的环境中共同使用工具进行交互。为了确保测试的广度与深度，研究团队开发了组合式任务生成器以及一个与环境紧密耦合的可靠用户模拟器，从而实现对智能体协作与沟通能力的严苛测试。实验分析表明，智能体在面对需要引导用户操作的双控制环境时性能显著下降，暴露了现有模型在复杂交互场景下的不足。总体而言，$\\tau^2$-bench 为评估智能体在推理和引导用户行为方面的综合表现提供了一个关键的实验平台。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07982v1",
      "published_date": "2025-06-09 17:52:18 UTC",
      "updated_date": "2025-06-09 17:52:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:12:05.185078+00:00"
    },
    {
      "arxiv_id": "2506.07976v2",
      "title": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction",
      "title_zh": "思与行：通过扩展推理时交互实现推理的智能体",
      "authors": [
        "Junhong Shen",
        "Hao Bai",
        "Lunjun Zhang",
        "Yifei Zhou",
        "Amrith Setlur",
        "Shengbang Tong",
        "Diego Caples",
        "Nan Jiang",
        "Tong Zhang",
        "Ameet Talwalkar",
        "Aviral Kumar"
      ],
      "abstract": "The current paradigm of test-time scaling relies on generating long reasoning traces (\"thinking\" more) before producing a response. In agent problems that require interaction, this can be done by generating thinking traces before acting in the world. However, this process does not allow agents to acquire new information from the environment or adapt their behavior over time. In this work, we propose to scale test-time interaction, an untapped dimension of test-time scaling that increases the agent's interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, we study the domain of web agents. We first show that even prompting-based interaction scaling without any training can improve task success on web benchmarks non-trivially. Building on this, we introduce TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data web agents on WebVoyager and WebArena benchmarks. We further show that TTI enables agents to balance exploration and exploitation adaptively. Our results establish interaction scaling as a powerful, complementary axis to scaling per-step compute, offering new avenues for training adaptive agents.",
      "tldr_zh": "该研究针对现有推理测试时扩展（Test-time scaling）侧重于生成思考链而缺乏环境反馈的问题，提出了Test-Time Interaction (TTI) 这一全新的扩展维度。TTI通过增加智能体的交互跨度，使其在单次任务执行中能够实现探索、回溯和动态重规划等复杂行为。作者引入了一种基于课程学习的在线强化学习（Online RL）方法，通过动态调整交互长度来训练智能体，并在Gemma 3 12B模型上进行了验证。实验结果显示，TTI在WebVoyager和WebArena基准测试中取得了开源领域的SOTA表现，并证明了其在平衡探索与利用（Exploration and Exploitation）方面的优势。此研究确立了交互扩展作为单步计算量扩展的重要互补轴线，为开发更具适应性的Web Agent提供了新路径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Fixed typo in Figure 6 and Conclusion",
      "pdf_url": "https://arxiv.org/pdf/2506.07976v2",
      "published_date": "2025-06-09 17:50:02 UTC",
      "updated_date": "2025-06-10 12:50:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:12:09.776517+00:00"
    },
    {
      "arxiv_id": "2506.07972v1",
      "title": "HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization",
      "title_zh": "HeuriGym：面向大语言模型构建组合优化启发式算法的智能体化基准",
      "authors": [
        "Hongzheng Chen",
        "Yingheng Wang",
        "Yaohui Cai",
        "Hins Hu",
        "Jiajie Li",
        "Shirley Huang",
        "Chenhui Deng",
        "Rongjian Liang",
        "Shufeng Kong",
        "Haoxing Ren",
        "Samitha Samaranayake",
        "Carla P. Gomes",
        "Zhiru Zhang"
      ],
      "abstract": "While Large Language Models (LLMs) have demonstrated significant advancements in reasoning and agent-based problem-solving, current evaluation methodologies fail to adequately assess their capabilities: existing benchmarks either rely on closed-ended questions prone to saturation and memorization, or subjective comparisons that lack consistency and rigor. In this work, we introduce HeuriGym, an agentic framework designed for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems, characterized by clearly defined objectives and expansive solution spaces. HeuriGym empowers LLMs to propose heuristics, receive evaluative feedback via code execution, and iteratively refine their solutions. We evaluate nine state-of-the-art models on nine problems across domains such as computer systems, logistics, and biology, exposing persistent limitations in tool use, planning, and adaptive reasoning. To quantify performance, we propose the Quality-Yield Index (QYI), a metric that captures both solution pass rate and quality. Even top models like GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below the expert baseline of 1. Our open-source benchmark aims to guide the development of LLMs toward more effective and realistic problem-solving in scientific and engineering domains.",
      "tldr_zh": "该研究针对现有大语言模型(LLMs)评估基准在处理科学工程问题时存在的闭合性与主观性限制，提出了HeuriGym框架，用于评估模型在组合优化(Combinatorial Optimization)中生成启发式算法(Heuristic Algorithms)的能力。该框架使模型能够自主提出算法方案，通过代码执行获取反馈并进行迭代优化。研究引入了综合衡量解的通过率与质量的Quality-Yield Index (QYI)指标，并在计算机系统、物流和生物学等领域的9个问题上对SOTA模型进行了评估。实验发现，即便如GPT-o4-mini-high和Gemini-2.5-Pro等顶尖模型的QYI得分也仅为0.6左右，远低于专家基准1，暴露了模型在工具使用(Tool Use)、规划(Planning)和自适应推理(Adaptive Reasoning)方面的持续局限。HeuriGym作为一个开源基准，旨在引导LLMs向更有效且现实的科研与工程问题解决方向发展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07972v1",
      "published_date": "2025-06-09 17:46:47 UTC",
      "updated_date": "2025-06-09 17:46:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:12:11.797436+00:00"
    },
    {
      "arxiv_id": "2506.07964v1",
      "title": "SlideCoder: Layout-aware RAG-enhanced Hierarchical Slide Generation from Design",
      "title_zh": "SlideCoder：基于布局感知与 RAG 增强的层级式设计驱动幻灯片生成",
      "authors": [
        "Wenxin Tang",
        "Jingyu Xiao",
        "Wenxuan Jiang",
        "Xi Xiao",
        "Yuhang Wang",
        "Xuxin Tang",
        "Qing Li",
        "Yuehe Ma",
        "Junliang Liu",
        "Shisong Tang",
        "Michael R. Lyu"
      ],
      "abstract": "Manual slide creation is labor-intensive and requires expert prior knowledge. Existing natural language-based LLM generation methods struggle to capture the visual and structural nuances of slide designs. To address this, we formalize the Reference Image to Slide Generation task and propose Slide2Code, the first benchmark with difficulty-tiered samples based on a novel Slide Complexity Metric. We introduce SlideCoder, a layout-aware, retrieval-augmented framework for generating editable slides from reference images. SlideCoder integrates a Color Gradient-based Segmentation algorithm and a Hierarchical Retrieval-Augmented Generation method to decompose complex tasks and enhance code generation. We also release SlideMaster, a 7B open-source model fine-tuned with improved reverse-engineered data. Experiments show that SlideCoder outperforms state-of-the-art baselines by up to 40.5 points, demonstrating strong performance across layout fidelity, execution accuracy, and visual consistency. Our code is available at https://github.com/vinsontang1/SlideCoder.",
      "tldr_zh": "该研究针对手动创建幻灯片耗时费力且现有 LLM 难以捕捉视觉和结构细微差别的问题，正式提出了 Reference Image to Slide Generation 任务。为此，研究团队发布了首个包含分级样本的基准测试 Slide2Code，并引入了 Slide Complexity Metric 复杂度度量标准。研究的核心贡献是提出了 SlideCoder 框架，这是一种感知布局且由 RAG 增强的层次化生成架构。该框架集成了 Color Gradient-based Segmentation 算法和 Hierarchical Retrieval-Augmented Generation 方法，通过分解复杂任务来显著提升代码生成质量。此外，团队还开源了经过逆向工程数据微调的 7B 模型 SlideMaster。实验表明，SlideCoder 在布局忠实度、执行准确性和视觉一致性方面均优于现有基线模型，性能提升最高达 40.5 分。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07964v1",
      "published_date": "2025-06-09 17:39:48 UTC",
      "updated_date": "2025-06-09 17:39:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:12:19.008318+00:00"
    },
    {
      "arxiv_id": "2506.07963v3",
      "title": "SUDER: Self-Improving Unified Large Multimodal Models for Understanding and Generation with Dual Self-Rewards",
      "title_zh": "SUDER：基于双重自我奖励的理解与生成自我改进统一大多模态模型",
      "authors": [
        "Jixiang Hong",
        "Yiran Zhang",
        "Guanzhong Wang",
        "Yi Liu",
        "Ji-Rong Wen",
        "Rui Yan"
      ],
      "abstract": "Building upon large language models (LLMs), recent large multimodal models (LMMs) unify cross-model understanding and generation into a single framework. However, LMMs still struggle to achieve accurate vision-language alignment, prone to generating text responses contradicting the visual input or failing to follow the text-to-image prompts. Current solutions require external supervision (e.g., human feedback or reward models) and only address unidirectional tasks-either understanding or generation. In this work, based on the observation that understanding and generation are naturally inverse dual tasks, we propose \\textbf{SUDER} (\\textbf{S}elf-improving \\textbf{U}nified LMMs with \\textbf{D}ual s\\textbf{E}lf-\\textbf{R}ewards), a framework reinforcing the understanding and generation capabilities of LMMs with a self-supervised dual reward mechanism. SUDER leverages the inherent duality between understanding and generation tasks to provide self-supervised optimization signals for each other. Specifically, we sample multiple outputs for a given input in one task domain, then reverse the input-output pairs to compute the dual likelihood within the model as self-rewards for optimization. Extensive experimental results on visual understanding and generation benchmarks demonstrate that our method can effectively enhance the performance of the model without any external supervision, especially achieving remarkable improvements in text-to-image tasks.",
      "tldr_zh": "该研究提出了SUDER，一种基于双重自我奖励(Dual Self-Rewards)机制的自改进统一大多模态模型(LMMs)框架。该框架旨在解决当前大多模态模型在视觉语言对齐方面存在的挑战，如生成响应与视觉输入矛盾或难以准确遵循文本生成图像(text-to-image)提示，同时摆脱了对外部监督或奖励模型的依赖。SUDER利用理解与生成任务之间天然的互逆对偶性，为彼此提供自监督的优化信号。具体而言，该方法通过在单一任务域采样多个输出，随后反转输入输出对并在模型内部计算对偶似然值作为自我奖励进行优化。实验结果表明，SUDER在视觉理解和生成基准测试中均能有效提升性能，特别是在文本生成图像任务中取得了显著进步。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07963v3",
      "published_date": "2025-06-09 17:38:45 UTC",
      "updated_date": "2025-09-08 14:31:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:12:20.247905+00:00"
    },
    {
      "arxiv_id": "2506.07962v1",
      "title": "Correlated Errors in Large Language Models",
      "title_zh": "大语言模型中的相关性错误",
      "authors": [
        "Elliot Kim",
        "Avi Garg",
        "Kenny Peng",
        "Nikhil Garg"
      ],
      "abstract": "Diversity in training data, architecture, and providers is assumed to mitigate homogeneity in LLMs. However, we lack empirical evidence on whether different LLMs differ meaningfully. We conduct a large-scale empirical evaluation on over 350 LLMs overall, using two popular leaderboards and a resume-screening task. We find substantial correlation in model errors -- on one leaderboard dataset, models agree 60% of the time when both models err. We identify factors driving model correlation, including shared architectures and providers. Crucially, however, larger and more accurate models have highly correlated errors, even with distinct architectures and providers. Finally, we show the effects of correlation in two downstream tasks: LLM-as-judge evaluation and hiring -- the latter reflecting theoretical predictions regarding algorithmic monoculture.",
      "tldr_zh": "该研究探讨了大语言模型(Large Language Models)中的相关性错误问题，旨在验证训练数据、架构和供应商的多样性是否能有效缓解模型的同质化(Homogeneity)。研究者对超过350个LLMs进行了大规模实证评估，涵盖了主流排行榜和简历筛选等任务。实验发现模型错误之间存在显著的相关性，在特定数据集上，两个模型同时出错时有60%的概率达成一致。研究识别出共享架构(Architectures)和供应商(Providers)是驱动相关性的因素，但即便在架构和供应商不同的情况下，规模更大、更准确的模型依然表现出高度相关的错误。最后，研究展示了这种相关性在LLM-as-judge评估和招聘任务等下游场景中的影响，其实证结果验证了关于算法单态性(Algorithmic Monoculture)的理论预测，揭示了当前AI生态中潜在的系统性风险。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "stat.ML"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.07962v1",
      "published_date": "2025-06-09 17:37:18 UTC",
      "updated_date": "2025-06-09 17:37:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:12:33.009928+00:00"
    },
    {
      "arxiv_id": "2506.07961v2",
      "title": "BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models",
      "title_zh": "BridgeVLA：通过输入输出对齐实现基于视觉语言模型的高效 3D 操控学习",
      "authors": [
        "Peiyan Li",
        "Yixiang Chen",
        "Hongtao Wu",
        "Xiao Ma",
        "Xiangnan Wu",
        "Yan Huang",
        "Liang Wang",
        "Tao Kong",
        "Tieniu Tan"
      ],
      "abstract": "Recently, leveraging pre-trained vision-language models (VLMs) for building vision-language-action (VLA) models has emerged as a promising approach to effective robot manipulation learning. However, only few methods incorporate 3D signals into VLMs for action prediction, and they do not fully leverage the spatial structure inherent in 3D data, leading to low sample efficiency. In this paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D inputs to multiple 2D images, ensuring input alignment with the VLM backbone, and (2) utilizes 2D heatmaps for action prediction, unifying the input and output spaces within a consistent 2D image space. In addition, we propose a scalable pre-training method that equips the VLM backbone with the capability to predict 2D heatmaps before downstream policy learning. Extensive experiments show the proposed method is able to learn 3D manipulation efficiently and effectively. BridgeVLA outperforms state-of-the-art baseline methods across three simulation benchmarks. In RLBench, it improves the average success rate from 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better performance in challenging generalization settings, boosting the average success rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing baseline methods in terms of average success rate. In real-robot experiments, BridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It generalizes robustly in multiple out-of-distribution settings, including visual disturbances and unseen instructions. Remarkably, it is able to achieve a success rate of 96.8% on 10+ tasks with only 3 trajectories per task, highlighting its extraordinary sample efficiency. Project Website:https://bridgevla.github.io/",
      "tldr_zh": "该研究提出了BridgeVLA，一种新型的3D视觉-语言-动作(Vision-Language-Action, VLA)模型，旨在解决现有方法在处理3D数据空间结构时样本效率(sample efficiency)低下的问题。BridgeVLA通过将3D输入投影为多个2D图像，确保了输入与预训练VLM骨干网络的对齐，并利用2D热图(heatmaps)进行动作预测，从而在统一的2D图像空间内实现了输入和输出的一致性。此外，研究还提出了一种可扩展的预训练方法，使VLM在下游策略学习前具备预测2D热图的能力。实验结果显示，BridgeVLA在RLBench、COLOSSEUM和GemBench等仿真基准测试中均超越了现有最先进水平，其中在RLBench的平均成功率从81.4%提升至88.2%。在真实机器人实验中，该模型在多种分布外(out-of-distribution)设置下表现稳健，平均性能优于基线模型32%。值得注意的是，BridgeVLA仅需每个任务3条轨迹即可在10余项任务中达到96.8%的成功率，显著证明了其卓越的样本学习效率。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.07961v2",
      "published_date": "2025-06-09 17:36:34 UTC",
      "updated_date": "2025-10-14 12:26:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:12:50.027785+00:00"
    },
    {
      "arxiv_id": "2506.07945v2",
      "title": "ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication Protocols",
      "title_zh": "ProtocolLLM：面向通信协议 SystemVerilog 代码生成的 RTL 基准测试",
      "authors": [
        "Arnav Sheth",
        "Ivaxi Sheth",
        "Mario Fritz"
      ],
      "abstract": "Recent advances in large language models (LLMs) have demonstrated strong performance in generating code for general-purpose programming languages. However, their potential for hardware description languages (HDLs), such as SystemVerilog, remains largely unexplored. HDL code generation poses unique challenges due to strict timing semantics, concurrency, and synthesizability constraints essential for correct hardware functionality. Further, HDL-based design flows encompass a broad set of tasks beyond structural code generation, including testbench development, assertion-based verification, timing closure, and protocol-level integration for on-chip communication. In this work, we evaluate the capabilities of both open-source and state-of-the-art LLMs in generating synthesizable and functionally accurate SystemVerilog implementations of widely used communication protocols that are critical components of embedded and System-on-Chip (SoC) systems. We introduce ProtocolLLM, the first benchmark suite specifically targeting these protocols with tasks spanning multiple design abstraction levels and varying prompt specificity. Our evaluation method also focuses on timing correctness in addition to synthesizability and syntactic correctness. We observe that most of the models fail to generate SystemVerilog code for communication protocols that follow timing constrains.",
      "tldr_zh": "该研究探讨了大语言模型(LLMs)在生成硬件描述语言(HDLs)如SystemVerilog方面的潜力与挑战，重点指出了硬件设计中特有的时序语义、并发性和可综合性等约束。为此，作者推出了ProtocolLLM，这是首个专门针对嵌入式及片上系统(SoC)中关键通信协议设计的RTL基准测试集。该基准涵盖了多个设计抽象层次和不同精细度的提示词，不仅关注语法和可综合性，还特别强调了对时序正确性的评估。实验结果显示，当前大多数开源及最先进的LLMs在生成符合时序约束的通信协议SystemVerilog代码方面表现欠佳，揭示了硬件自动化设计领域的现有局限。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AR",
      "comment": "Accepted at MLSysArch@ISCA 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.07945v2",
      "published_date": "2025-06-09 17:10:47 UTC",
      "updated_date": "2025-07-15 16:24:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:12:58.960911+00:00"
    },
    {
      "arxiv_id": "2506.07943v2",
      "title": "Decoupling the Image Perception and Multimodal Reasoning for Reasoning Segmentation with Digital Twin Representations",
      "title_zh": "利用数字孪生表示解耦推理分割中的图像感知与多模态推理",
      "authors": [
        "Yizhen Li",
        "Dell Zhang",
        "Xuelong Li",
        "Yiqing Shen"
      ],
      "abstract": "Reasoning Segmentation (RS) is a multimodal vision-text task that requires segmenting objects based on implicit text queries, demanding both precise visual perception and vision-text reasoning capabilities. Current RS approaches rely on fine-tuning vision-language models (VLMs) for both perception and reasoning, but their tokenization of images fundamentally disrupts continuous spatial relationships between objects. We introduce DTwinSeger, a novel RS approach that leverages Digital Twin (DT) representation as an intermediate layer to decouple perception from reasoning. Innovatively, DTwinSeger reformulates RS as a two-stage process, where the first transforms the image into a structured DT representation that preserves spatial relationships and semantic properties and then employs a Large Language Model (LLM) to perform explicit reasoning over this representation to identify target objects. We propose a supervised fine-tuning method specifically for LLM with DT representation, together with a corresponding fine-tuning dataset Seg-DT, to enhance the LLM's reasoning capabilities with DT representations. Experiments show that our method can achieve state-of-the-art performance on two image RS benchmarks and three image referring segmentation benchmarks. It yields that DT representation functions as an effective bridge between vision and text, enabling complex multimodal reasoning tasks to be accomplished solely with an LLM.",
      "tldr_zh": "该研究提出了 DTwinSeger，一种用于推理分割（Reasoning Segmentation, RS）的新型方法，旨在解决现有视觉语言模型（VLMs）因图像 Tokenization 破坏空间关系的问题。DTwinSeger 引入了数字孪生（Digital Twin, DT）表示作为中间层，成功将图像感知与多模态推理进行解耦。该方法将 RS 任务重塑为两阶段过程：第一阶段将图像转换为保留空间和语义属性的结构化 DT 表示，第二阶段利用大语言模型（LLM）在该表示上进行显式推理以锁定目标。为了提升模型性能，研究者还配套开发了针对 DT 表示的监督微调方法及 Seg-DT 数据集。实验证明，该方法在多个 RS 和指向性分割（Referring Segmentation）基准测试中达到了最先进（SOTA）的性能。研究结果表明，DT 表示是连接视觉与文本的有效桥梁，使 LLM 能够独立胜任复杂的多模态推理任务。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This work was submitted without the consent of all co-authors. We request withdrawal until all parties agree",
      "pdf_url": "https://arxiv.org/pdf/2506.07943v2",
      "published_date": "2025-06-09 17:05:02 UTC",
      "updated_date": "2025-06-11 13:48:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:12:47.703611+00:00"
    },
    {
      "arxiv_id": "2506.08070v2",
      "title": "Info-Coevolution: An Efficient Framework for Data Model Coevolution",
      "title_zh": "Info-Coevolution：一种数据与模型协同演化的高效框架",
      "authors": [
        "Ziheng Qin",
        "Hailun Xu",
        "Wei Chee Yew",
        "Qi Jia",
        "Yang Luo",
        "Kanchan Sarkar",
        "Danhui Guan",
        "Kai Wang",
        "Yang You"
      ],
      "abstract": "Machine learning relies heavily on data, yet the continuous growth of real-world data poses challenges for efficient dataset construction and training. A fundamental yet unsolved question is: given our current model and data, does a new data (sample/batch) need annotation/learning? Conventional approaches retain all available data, leading to non-optimal data and training efficiency. Active learning aims to reduce data redundancy by selecting a subset of samples to annotate, while it increases pipeline complexity and introduces bias. In this work, we propose Info-Coevolution, a novel framework that efficiently enables models and data to coevolve through online selective annotation with no bias. Leveraging task-specific models (and open-source models), it selectively annotates and integrates online and web data to improve datasets efficiently. For real-world datasets like ImageNet-1K, Info-Coevolution reduces annotation and training costs by 32\\% without performance loss. It is able to automatically give the saving ratio without tuning the ratio. It can further reduce the annotation ratio to 50\\% with semi-supervised learning. We also explore retrieval-based dataset enhancement using unlabeled open-source data. Code is available at https://github.com/NUS-HPC-AI-Lab/Info-Coevolution/.",
      "tldr_zh": "该研究针对机器学习中不断增长的数据量带来的数据集构建与训练效率挑战，提出了 Info-Coevolution 框架，旨在解决传统方法全量保留数据导致的效率低下问题。Info-Coevolution 通过在线选择性标注 (online selective annotation) 实现模型与数据的无偏差协同演进，利用特定任务模型和开源模型高效地筛选并整合数据。实验结果显示，在 ImageNet-1K 数据集上，该框架在保持性能不变的同时将标注和训练成本降低了 32%，且具备自动确定节省比例的能力。在结合半监督学习 (semi-supervised learning) 后，标注需求可进一步缩减至 50%，研究还进一步探索了利用无标签开源数据进行基于检索的数据集增强 (retrieval-based dataset enhancement)。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "V1",
      "pdf_url": "https://arxiv.org/pdf/2506.08070v2",
      "published_date": "2025-06-09 17:04:11 UTC",
      "updated_date": "2025-06-20 02:52:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:12:59.116535+00:00"
    },
    {
      "arxiv_id": "2506.07940v2",
      "title": "Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation",
      "title_zh": "Gradients：当市场机制遇见微调 —— 一种分布式模型优化方法",
      "authors": [
        "Christopher Subia-Waud"
      ],
      "abstract": "Current AutoML platforms leave substantial performance untapped. Testing 180 fine-tuning tasks across models from 70M to 70B parameters, we found that HuggingFace AutoTrain, TogetherAI, Databricks, and Google Cloud consistently produce suboptimal configurations. Gradients, built on the Bittensor network, attacks this problem through competition. Independent miners race to find optimal hyperparameters, earning rewards proportional to their models' performance. This tournament drives exploration of configuration spaces that single-strategy methods never examine. In our experiments, Gradients achieved a 100\\% win rate against TogetherAI, Databricks, and Google Cloud, and beat HuggingFace AutoTrain in 82.8\\% of experiments. Mean improvements reached 42.1\\% against commercial platforms. Retrieval-augmented generation tasks saw 30-40\\% gains; diffusion models improved 23.4\\% on person-specific generation. When miners compete for rewards, they develop optimization strategies that centralized approaches overlook. These findings demonstrate that decentralized systems with economic incentives can systematically outperform traditional AutoML, suggesting market dynamics may be key to achieving superior fine-tuning results. Code is available at https://github.com/rayonlabs/G.O.D.",
      "tldr_zh": "该研究指出当前的AutoML平台在模型微调中往往产生次优配置，为此提出了基于Bittensor网络的分布式优化框架Gradients。该框架通过市场竞争机制，利用独立矿工(Miners)竞相寻找最优超参数(Hyperparameters)并获取经济激励，从而探索传统方法无法触及的配置空间。实验结果表明，Gradients在与TogetherAI、Databricks和Google Cloud的对比中达到了100%的胜率，并在82.8%的测试中击败了HuggingFace AutoTrain。相较于传统商业平台，该系统带来了42.1%的平均性能提升，在检索增强生成(RAG)和扩散模型(Diffusion models)任务中也表现出显著优势。研究证明，具备经济激励的去中心化系统能够通过市场动力开发出更优的优化策略，为超越传统AutoML提供了新路径。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07940v2",
      "published_date": "2025-06-09 17:00:38 UTC",
      "updated_date": "2025-09-03 09:39:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:13:48.093540+00:00"
    },
    {
      "arxiv_id": "2506.07936v1",
      "title": "Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models",
      "title_zh": "模仿还是推理：重新审视视觉语言模型中的多模态上下文学习",
      "authors": [
        "Chengyue Huang",
        "Yuchen Zhu",
        "Sichen Zhu",
        "Jingyun Xiao",
        "Moises Andrade",
        "Shivang Chopra",
        "Zsolt Kira"
      ],
      "abstract": "Vision-language models (VLMs) are widely assumed to exhibit in-context learning (ICL), a property similar to that of their language-only counterparts. While recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies show they often rely on shallow heuristics -- such as copying or majority voting -- rather than true task understanding. We revisit this assumption by evaluating VLMs under distribution shifts, where support examples come from a dataset different from the query. Surprisingly, performance often degrades with more demonstrations, and models tend to copy answers rather than learn from them. To investigate further, we propose a new MM-ICL with Reasoning pipeline that augments each demonstration with a generated rationale alongside the answer. We conduct extensive and comprehensive experiments on both perception- and reasoning-required datasets with open-source VLMs ranging from 3B to 72B and proprietary models such as Gemini 2.0. We conduct controlled studies varying shot count, retrieval method, rationale quality, and distribution. Our results show limited performance sensitivity across these factors, suggesting that current VLMs do not effectively utilize demonstration-level information as intended in MM-ICL.",
      "tldr_zh": "该研究重新审视了视觉语言模型(VLMs)在多模态上下文学习(MM-ICL)中的表现，旨在探究模型是在进行真正的任务推理还是仅依赖浅层启发式(shallow heuristics)进行模仿。通过在分布偏移(distribution shifts)环境下进行评估，研究发现VLMs在示例增加时性能反而可能下降，且表现出明显的答案复制倾向而非真正的任务理解。为深入调查，作者提出了一种带推理的MM-ICL流程(MM-ICL with Reasoning pipeline)，在每个演示示例中通过增强生成的原理(rationale)来辅助答案生成。实验涵盖了从3B到72B规模的开源模型以及Gemini 2.0等商业模型，并在感知与推理相关的数据集上进行了全面测试。研究通过控制示例数量、检索方法及原理质量等变量，发现模型对这些关键因素的性能敏感性较低。最终研究指出，当前的VLMs并未能如预期般有效利用演示层面的信息，反映出多模态上下文学习在实现真正逻辑推理方面仍面临巨大挑战。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07936v1",
      "published_date": "2025-06-09 16:55:32 UTC",
      "updated_date": "2025-06-09 16:55:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:13:05.884533+00:00"
    },
    {
      "arxiv_id": "2506.07935v1",
      "title": "Diffusion of Responsibility in Collective Decision Making",
      "title_zh": "集体决策中的责任分散",
      "authors": [
        "Pavel Naumov",
        "Jia Tao"
      ],
      "abstract": "The term \"diffusion of responsibility'' refers to situations in which multiple agents share responsibility for an outcome, obscuring individual accountability. This paper examines this frequently undesirable phenomenon in the context of collective decision-making mechanisms.\n  The work shows that if a decision is made by two agents, then the only way to avoid diffusion of responsibility is for one agent to act as a \"dictator'', making the decision unilaterally. In scenarios with more than two agents, any diffusion-free mechanism is an \"elected dictatorship'' where the agents elect a single agent to make a unilateral decision.\n  The technical results are obtained by defining a bisimulation of decision-making mechanisms, proving that bisimulation preserves responsibility-related properties, and establishing the results for a smallest bisimular mechanism.",
      "tldr_zh": "该研究探讨了集体决策机制中的“责任扩散”(diffusion of responsibility)现象，即当多个智能体共同承担结果责任时，个人问责制变得模糊的问题。研究发现，在由两个智能体进行决策的情况下，避免责任扩散的唯一途径是让其中一个智能体充当“独裁者”(dictator)进行单方面决策。在涉及两个以上智能体的场景中，任何无扩散机制本质上都是“选举产生的独裁”(elected dictatorship)，即由智能体选举出一人来行使单方面决策权。在技术实现上，该论文定义了决策机制的“互模拟”(bisimulation)概念，并证明了该性质能够保持与责任相关的各种属性。通过在最小互模拟机制(smallest bisimular mechanism)上建立证明，该研究揭示了集体决策中确保问责制明确性的结构性限制。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07935v1",
      "published_date": "2025-06-09 16:54:56 UTC",
      "updated_date": "2025-06-09 16:54:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:13:05.548290+00:00"
    },
    {
      "arxiv_id": "2506.07927v3",
      "title": "Solving Inequality Proofs with Large Language Models",
      "title_zh": "利用大语言模型求解不等式证明",
      "authors": [
        "Pan Lu",
        "Jiayi Sheng",
        "Luna Lyu",
        "Jikai Jin",
        "Tony Xia",
        "Alex Gu",
        "James Zou"
      ],
      "abstract": "Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at https://ineqmath.github.io/.",
      "tldr_zh": "该研究探讨了大型语言模型(LLMs)在不等式证明(Inequality Proving)这一极具挑战性的推理任务中的表现，指出该领域受限于现有数据集的稀缺性和过于形式化的问题。为此，作者提出了一种非正式但可验证的任务表述方式，将不等式证明重新定义为边界估计(Bound Estimation)和关系预测(Relation Prediction)两个可自动检查的子任务。研究发布了专家策划的奥林匹克竞赛级别数据集IneqMath，包含测试集、训练语料库以及详细的步骤化解答和定理注释。此外，作者开发了一种新型的LLM-as-judge评估框架，通过最终答案判定与四种步骤化判定相结合，旨在检测推理链条中的常见缺陷。对29种主流LLMs的系统评估显示，即使是o1等顶尖模型在严格的步骤审查下准确率也低于10%，暴露出模型在构建严谨证明过程中的脆弱性。实验结果表明，仅仅增加模型规模或推理时间计算量对提高证明正确性作用有限，未来应重点关注定理引导推理(Theorem-guided Reasoning)和自我完善(Self-refinement)等研究方向。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "50 pages, 24 figures, accepted as a Spotlight at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.07927v3",
      "published_date": "2025-06-09 16:43:38 UTC",
      "updated_date": "2025-12-15 01:49:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:13:23.737092+00:00"
    },
    {
      "arxiv_id": "2506.07919v2",
      "title": "Uncovering the Computational Roles of Nonlinearity in Sequence Modeling Using Almost-Linear RNNs",
      "title_zh": "利用近线性 RNN 揭示序列建模中非线性的计算作用",
      "authors": [
        "Manuel Brenner",
        "Georgia Koppe"
      ],
      "abstract": "Sequence modeling tasks across domains such as natural language processing, time series forecasting, and control require learning complex input-output mappings. Nonlinear recurrence is theoretically required for universal approximation of sequence-to-sequence functions, yet linear recurrent models often prove surprisingly effective. This raises the question of when nonlinearity is truly required. We present a framework to systematically dissect the functional role of nonlinearity in recurrent networks, identifying when it is computationally necessary and what mechanisms it enables. We address this using Almost Linear Recurrent Neural Networks (AL-RNNs), which allow recurrence nonlinearity to be gradually attenuated and decompose network dynamics into analyzable linear regimes, making computational mechanisms explicit. We illustrate the framework across diverse synthetic and real-world tasks, including classic sequence modeling benchmarks, a neuroscientific stimulus-selection task, and a multi-task suite. We demonstrate how the AL-RNN's piecewise linear structure enables identification of computational primitives such as gating, rule-based integration, and memory-dependent transients, revealing that these operations emerge within predominantly linear backbones. Across tasks, sparse nonlinearity improves interpretability by reducing and localizing nonlinear computations, promotes shared representations in multi-task settings, and reduces computational cost. Moreover, sparse nonlinearity acts as a useful inductive bias: in low-data regimes or when tasks require discrete switching between linear regimes, sparsely nonlinear models often match or exceed fully nonlinear architectures. Our findings provide a principled approach for identifying where nonlinearity is functionally necessary, guiding the design of recurrent architectures that balance performance, efficiency, and interpretability.",
      "tldr_zh": "该研究探讨了序列建模中非线性(nonlinearity)的计算角色，旨在厘清在何种情况下非线性是处理复杂输入输出映射所必需的。作者提出了近乎线性循环神经网络(AL-RNNs)框架，通过逐渐衰减循环过程中的非线性，将网络动力学分解为可分析的线性区间，使计算机制显性化。该框架能够明确识别出如门控(gating)、基于规则的积分(rule-based integration)和依赖记忆的瞬态(memory-dependent transients)等计算原语，揭示了这些操作如何在线性主干中涌现。研究在合成任务、神经科学刺激选择任务及多任务基准上表明，稀疏非线性(sparse nonlinearity)不仅显著增强了模型的可解释性并降低了计算成本，还在多任务设置中促进了共享表示的形成。此外，稀疏非线性作为一种有效的归纳偏置(inductive bias)，在小样本场景或需要离散线性区间切换的任务中，其性能往往能媲美甚至超越全非线性架构。该研究为识别非线性的功能必要性提供了原则性方法，为设计平衡性能、效率与可解释性的循环架构提供了重要指导。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "nlin.CD",
        "physics.comp-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in Transactions on Machine Learning Research (TMLR), https://openreview.net/forum?id=qI2Vt9P9rl",
      "pdf_url": "https://arxiv.org/pdf/2506.07919v2",
      "published_date": "2025-06-09 16:32:19 UTC",
      "updated_date": "2026-01-12 15:09:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:13:25.512969+00:00"
    },
    {
      "arxiv_id": "2506.07915v1",
      "title": "LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement",
      "title_zh": "LUCIFER：融合语言理解与上下文的探索与行为优化框架",
      "authors": [
        "Dimitris Panagopoulos",
        "Adolfo Perrusquia",
        "Weisi Guo"
      ],
      "abstract": "In dynamic environments, the rapid obsolescence of pre-existing environmental knowledge creates a gap between an agent's internal model and the evolving reality of its operational context. This disparity between prior and updated environmental valuations fundamentally limits the effectiveness of autonomous decision-making. To bridge this gap, the contextual bias of human domain stakeholders, who naturally accumulate insights through direct, real-time observation, becomes indispensable. However, translating their nuanced, and context-rich input into actionable intelligence for autonomous systems remains an open challenge. To address this, we propose LUCIFER (Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement), a domain-agnostic framework that integrates a hierarchical decision-making architecture with reinforcement learning (RL) and large language models (LLMs) into a unified system. This architecture mirrors how humans decompose complex tasks, enabling a high-level planner to coordinate specialised sub-agents, each focused on distinct objectives and temporally interdependent actions. Unlike traditional applications where LLMs are limited to single role, LUCIFER integrates them in two synergistic roles: as context extractors, structuring verbal stakeholder input into domain-aware representations that influence decision-making through an attention space mechanism aligning LLM-derived insights with the agent's learning process, and as zero-shot exploration facilitators guiding the agent's action selection process during exploration. We benchmark various LLMs in both roles and demonstrate that LUCIFER improves exploration efficiency and decision quality, outperforming flat, goal-conditioned policies. Our findings show the potential of context-driven decision-making, where autonomous systems leverage human contextual knowledge for operational success.",
      "tldr_zh": "该研究提出了 LUCIFER，一种旨在解决自主决策系统中先验知识与动态现实环境脱节问题的通用框架。该框架结合了层次化决策架构、强化学习(RL)与大语言模型(LLMs)，模拟人类分解复杂任务的逻辑，通过高级规划器协调多个子智能体完成任务。LLM 在系统中承担双重协同角色，既作为上下文提取器将人类输入的口头信息转化为领域感知表示，并利用注意力机制(Attention mechanism)对齐决策过程，又作为零样本(Zero-shot)探索引导者指导动作选择。实验结果显示，LUCIFER 在探索效率和决策质量上显著优于扁平的目标条件策略(Flat, goal-conditioned policies)。该研究证实了自主系统通过整合人类上下文知识，能够有效提升在动态多变环境中的操作成功率与行为精细度。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 4 Figures, 3 Tables, submitted to the IEEE for possible publication",
      "pdf_url": "https://arxiv.org/pdf/2506.07915v1",
      "published_date": "2025-06-09 16:30:05 UTC",
      "updated_date": "2025-06-09 16:30:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:14:04.267400+00:00"
    },
    {
      "arxiv_id": "2506.07903v2",
      "title": "Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces",
      "title_zh": "Diffuse Everything：面向任意状态空间的多模态扩散模型",
      "authors": [
        "Kevin Rojas",
        "Yuchen Zhu",
        "Sichen Zhu",
        "Felix X. -F. Ye",
        "Molei Tao"
      ],
      "abstract": "Diffusion models have demonstrated remarkable performance in generating unimodal data across various tasks, including image, video, and text generation. On the contrary, the joint generation of multimodal data through diffusion models is still in the early stages of exploration. Existing approaches heavily rely on external preprocessing protocols, such as tokenizers and variational autoencoders, to harmonize varied data representations into a unified, unimodal format. This process heavily demands the high accuracy of encoders and decoders, which can be problematic for applications with limited data. To lift this restriction, we propose a novel framework for building multimodal diffusion models on arbitrary state spaces, enabling native generation of coupled data across different modalities. By introducing an innovative decoupled noise schedule for each modality, we enable both unconditional and modality-conditioned generation within a single model simultaneously. We empirically validate our approach for text-image generation and mixed-type tabular data synthesis, demonstrating that it achieves competitive performance.",
      "tldr_zh": "该研究提出了一个在任意状态空间(arbitrary state spaces)上构建多模态扩散模型的新框架，旨在解决现有方法过度依赖 tokenizers 和变分自编码器(VAEs)等外部预处理协议的问题。通过引入创新的解耦噪声计划(decoupled noise schedule)，该框架实现了不同模态耦合数据的原生生成(native generation)，并能在一个模型中同时支持无条件生成和模态条件生成。这种方法有效规避了传统流程中对高精度编解码器的需求，特别适用于数据受限的应用场景。实验在文本-图像生成和混合类型表格数据合成(mixed-type tabular data synthesis)任务上均取得了具有竞争力的性能，证明了该框架在处理复杂多模态数据时的有效性和灵活性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ICML 2025. Code available at https://github.com/KevinRojas1499/Diffuse-Everything",
      "pdf_url": "https://arxiv.org/pdf/2506.07903v2",
      "published_date": "2025-06-09 16:20:20 UTC",
      "updated_date": "2025-06-12 23:40:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:14:06.779311+00:00"
    },
    {
      "arxiv_id": "2506.07900v2",
      "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices",
      "title_zh": "MiniCPM4：面向端侧设备的极致高效大语言模型",
      "authors": [
        "MiniCPM Team",
        "Chaojun Xiao",
        "Yuxuan Li",
        "Xu Han",
        "Yuzhuo Bai",
        "Jie Cai",
        "Haotian Chen",
        "Wentong Chen",
        "Xin Cong",
        "Ganqu Cui",
        "Ning Ding",
        "Shengda Fan",
        "Yewei Fang",
        "Zixuan Fu",
        "Wenyu Guan",
        "Yitong Guan",
        "Junshao Guo",
        "Yufeng Han",
        "Bingxiang He",
        "Yuxiang Huang",
        "Baoxi Ji",
        "Cunliang Kong",
        "Qiuzuo Li",
        "Siyuan Li",
        "Wenhao Li",
        "Xin Li",
        "Yanghao Li",
        "Yishan Li",
        "Zhen Li",
        "Dan Liu",
        "Biyuan Lin",
        "Yankai Lin",
        "Xiang Long",
        "Quanyu Lu",
        "Yaxi Lu",
        "Peiyan Luo",
        "Hongya Lyu",
        "Litu Ou",
        "Yinxu Pan",
        "Lushi Pu",
        "Zekai Qu",
        "Qundong Shi",
        "Zijun Song",
        "Jiayuan Su",
        "Zhou Su",
        "Ao Sun",
        "Xianghui Sun",
        "Peijun Tang",
        "Fangzheng Wang",
        "Feng Wang",
        "Shuo Wang",
        "Yudong Wang",
        "Zheng Wang",
        "Yesai Wu",
        "Zhenyu Xiao",
        "Jie Xie",
        "Zihao Xie",
        "Xiaoyue Xu",
        "Yukun Yan",
        "Jiarui Yuan",
        "Jinqian Zhang",
        "Kaihuo Zhang",
        "Lei Zhang",
        "Linyue Zhang",
        "Xueren Zhang",
        "Yudi Zhang",
        "Hengyu Zhao",
        "Weilin Zhao",
        "Weilun Zhao",
        "Yuanqian Zhao",
        "Zhi Zheng",
        "Chuyue Zhou",
        "Ge Zhou",
        "Jie Zhou",
        "Wei Zhou",
        "Yanghao Zhou",
        "Zihan Zhou",
        "Zixuan Zhou",
        "Zhiyuan Liu",
        "Guoyang Zeng",
        "Chao Jia",
        "Dahai Li",
        "Maosong Sun"
      ],
      "abstract": "This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Furthermore, we construct a hybrid reasoning model, MiniCPM4.1, which can be used in both deep reasoning mode and non-reasoning mode. Evaluation results demonstrate that MiniCPM4 and MiniCPM4.1 outperform similar-sized open-source models across benchmarks, with the 8B variants showing significant speed improvements on long sequence understanding and generation.",
      "tldr_zh": "该研究推出了MiniCPM4，这是一系列专为端侧设备设计的超高效大语言模型(LLMs)。该系列模型通过模型架构、训练数据、训练算法和推理系统四个维度的系统性创新，实现了效率与性能的平衡。在架构方面，InfLLM v2可训练稀疏注意力机制显著加速了长文本的预填充和解码阶段；数据层面则利用UltraClean和UltraChat v2数据集，仅凭8万亿(8 trillion)训练token即达到了优异的性能水平。算法上引入了ModelTunnel v2优化预训练策略，并利用BitCPM技术实现高效率的三元(ternary)LLM，推理系统CPM.cu则通过量化和投机采样(speculative sampling)进一步提升了运行效率。MiniCPM4提供0.5B和8B两个参数版本，并延伸出支持深度推理模式的混合模型MiniCPM4.1。实验结果表明，MiniCPM4系列在多项基准测试中优于同尺寸开源模型，且在长序列理解与生成速度上表现出色。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "MiniCPM4 Technical Report",
      "pdf_url": "https://arxiv.org/pdf/2506.07900v2",
      "published_date": "2025-06-09 16:16:50 UTC",
      "updated_date": "2025-09-04 16:23:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:14:08.180378+00:00"
    },
    {
      "arxiv_id": "2506.13778v1",
      "title": "Knowledge Compression via Question Generation: Enhancing Multihop Document Retrieval without Fine-tuning",
      "title_zh": "基于问题生成的知识压缩：无需微调的多跳文档检索增强",
      "authors": [
        "Anvi Alex Eponon",
        "Moein Shahiki-Tash",
        "Ildar Batyrshin",
        "Christian E. Maldonado-Sifuentes",
        "Grigori Sidorov",
        "Alexander Gelbukh"
      ],
      "abstract": "This study presents a question-based knowledge encoding approach that improves retrieval-augmented generation (RAG) systems without requiring fine-tuning or traditional chunking. We encode textual content using generated questions that span the lexical and semantic space, creating targeted retrieval cues combined with a custom syntactic reranking method.\n  In single-hop retrieval over 109 scientific papers, our approach achieves a Recall@3 of 0.84, outperforming traditional chunking methods by 60 percent. We also introduce \"paper-cards\", concise paper summaries under 300 characters, which enhance BM25 retrieval, increasing MRR@3 from 0.56 to 0.85 on simplified technical queries.\n  For multihop tasks, our reranking method reaches an F1 score of 0.52 with LLaMA2-Chat-7B on the LongBench 2WikiMultihopQA dataset, surpassing chunking and fine-tuned baselines which score 0.328 and 0.412 respectively.\n  This method eliminates fine-tuning requirements, reduces retrieval latency, enables intuitive question-driven knowledge access, and decreases vector storage demands by 80%, positioning it as a scalable and efficient RAG alternative.",
      "tldr_zh": "该研究提出了一种基于问题生成的知识编码方法，旨在不进行 fine-tuning 或传统 chunking 的情况下增强检索增强生成(RAG)系统。该方法利用生成的问题作为检索线索，并结合自定义的语法重排序(syntactic reranking)技术，有效地捕捉了文本的词汇和语义信息。此外，研究引入了名为 paper-cards 的短摘要技术，显著提升了 BM25 检索的准确性。实验结果显示，该方法在单步检索中的 Recall@3 达到 0.84，且在 LongBench 2WikiMultihopQA 多跳(multihop)任务中取得 0.52 的 F1 分数，显著优于经过微调的基线模型。该方案不仅消除了对 fine-tuning 的需求并降低了检索延迟，还将向量存储需求减少了 80%，为高效 RAG 系统的构建提供了一种可扩展且直观的替代路径。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.13778v1",
      "published_date": "2025-06-09 16:15:11 UTC",
      "updated_date": "2025-06-09 16:15:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:14:13.441372+00:00"
    },
    {
      "arxiv_id": "2506.07897v1",
      "title": "GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution",
      "title_zh": "GaussianVAE：面向高保真超分辨率的 3D 高斯自适应学习动态",
      "authors": [
        "Shuja Khalid",
        "Mohamed Ibrahim",
        "Yang Liu"
      ],
      "abstract": "We present a novel approach for enhancing the resolution and geometric fidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution. Current 3DGS methods are fundamentally limited by their input resolution, producing reconstructions that cannot extrapolate finer details than are present in the training views. Our work breaks this limitation through a lightweight generative model that predicts and refines additional 3D Gaussians where needed most. The key innovation is our Hessian-assisted sampling strategy, which intelligently identifies regions that are likely to benefit from densification, ensuring computational efficiency. Unlike computationally intensive GANs or diffusion approaches, our method operates in real-time (0.015s per inference on a single consumer-grade GPU), making it practical for interactive applications. Comprehensive experiments demonstrate significant improvements in both geometric accuracy and rendering quality compared to state-of-the-art methods, establishing a new paradigm for resolution-free 3D scene enhancement.",
      "tldr_zh": "该研究提出了GaussianVAE，这是一种旨在提升3D Gaussian Splatting (3DGS) 分辨率和几何保真度的新型轻量级生成模型。该方法通过预测和精细化额外的高斯点，有效突破了现有3DGS技术受限于输入训练分辨率、难以生成超分辨率细节的瓶颈。其核心创新在于采用了基于Hessian辅助的采样策略(Hessian-assisted sampling strategy)，能够智能识别并细化最需要增加密度的区域，从而在确保计算效率的同时优化结果。与计算密集型的GANs或扩散模型(diffusion approaches)相比，该方法在单张消费级GPU上实现了0.015秒的实时推理速度。综合实验表明，GaussianVAE在几何准确度和渲染质量上均显著优于现有最先进方法，为无分辨率限制的3D场景增强确立了新范式。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07897v1",
      "published_date": "2025-06-09 16:13:12 UTC",
      "updated_date": "2025-06-09 16:13:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:14:25.664857+00:00"
    },
    {
      "arxiv_id": "2506.07896v1",
      "title": "Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark",
      "title_zh": "大语言模型在框架问题与符号接地问题上的评估：一项零样本基准",
      "authors": [
        "Shoko Oka"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have revitalized philosophical debates surrounding artificial intelligence. Two of the most fundamental challenges - namely, the Frame Problem and the Symbol Grounding Problem - have historically been viewed as unsolvable within traditional symbolic AI systems. This study investigates whether modern LLMs possess the cognitive capacities required to address these problems. To do so, I designed two benchmark tasks reflecting the philosophical core of each problem, administered them under zero-shot conditions to 13 prominent LLMs (both closed and open-source), and assessed the quality of the models' outputs across five trials each. Responses were scored along multiple criteria, including contextual reasoning, semantic coherence, and information filtering. The results demonstrate that while open-source models showed variability in performance due to differences in model size, quantization, and instruction tuning, several closed models consistently achieved high scores. These findings suggest that select modern LLMs may be acquiring capacities sufficient to produce meaningful and stable responses to these long-standing theoretical challenges.",
      "tldr_zh": "该研究调查了大型语言模型（LLMs）是否具备解决人工智能领域长期存在的两大基本挑战——Frame Problem 和 Symbol Grounding Problem 的认知能力。作者为此设计了一个反映这两个问题哲学核心的零样本（zero-shot）基准测试，并在该基准上对13个主流的闭源和开源模型进行了评估。评估过程采用了上下文推理（contextual reasoning）、语义连贯性（semantic coherence）和信息过滤（information filtering）等多个维度的评分标准。实验结果显示，虽然开源模型因模型规模和指令微调的差异在表现上存在波动，但部分闭源模型持续获得了高分。这些发现表明，部分现代 LLMs 可能已经习得足够的能力，能够针对这些长期的理论挑战产生有意义且稳定的响应。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "52 pages, Additional resources available on GitHub repository",
      "pdf_url": "https://arxiv.org/pdf/2506.07896v1",
      "published_date": "2025-06-09 16:12:47 UTC",
      "updated_date": "2025-06-09 16:12:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:14:20.796997+00:00"
    },
    {
      "arxiv_id": "2506.17262v1",
      "title": "AI to Identify Strain-sensitive Regions of the Optic Nerve Head Linked to Functional Loss in Glaucoma",
      "title_zh": "人工智能识别与青光眼功能损伤相关的视神经乳头应变敏感区域",
      "authors": [
        "Thanadet Chuangsuwanich",
        "Monisha E. Nongpiur",
        "Fabian A. Braeu",
        "Tin A. Tun",
        "Alexandre Thiery",
        "Shamira Perera",
        "Ching Lin Ho",
        "Martin Buist",
        "George Barbastathis",
        "Tin Aung",
        "Michaël J. A. Girard"
      ],
      "abstract": "Objective: (1) To assess whether ONH biomechanics improves prediction of three progressive visual field loss patterns in glaucoma; (2) to use explainable AI to identify strain-sensitive ONH regions contributing to these predictions.\n  Methods: We recruited 237 glaucoma subjects. The ONH of one eye was imaged under two conditions: (1) primary gaze and (2) primary gaze with IOP elevated to ~35 mmHg via ophthalmo-dynamometry. Glaucoma experts classified the subjects into four categories based on the presence of specific visual field defects: (1) superior nasal step (N=26), (2) superior partial arcuate (N=62), (3) full superior hemifield defect (N=25), and (4) other/non-specific defects (N=124). Automatic ONH tissue segmentation and digital volume correlation were used to compute IOP-induced neural tissue and lamina cribrosa (LC) strains. Biomechanical and structural features were input to a Geometric Deep Learning model. Three classification tasks were performed to detect: (1) superior nasal step, (2) superior partial arcuate, (3) full superior hemifield defect. For each task, the data were split into 80% training and 20% testing sets. Area under the curve (AUC) was used to assess performance. Explainable AI techniques were employed to highlight the ONH regions most critical to each classification.\n  Results: Models achieved high AUCs of 0.77-0.88, showing that ONH strain improved VF loss prediction beyond morphology alone. The inferior and inferotemporal rim were identified as key strain-sensitive regions, contributing most to visual field loss prediction and showing progressive expansion with increasing disease severity.\n  Conclusion and Relevance: ONH strain enhances prediction of glaucomatous VF loss patterns. Neuroretinal rim, rather than the LC, was the most critical region contributing to model predictions.",
      "tldr_zh": "该研究利用人工智能技术评估视神经乳头(ONH)的生物力学特征是否能提高对青光眼三种进展性视野缺失模式的预测能力，并通过可解释AI(Explainable AI)识别与这些预测相关的应变敏感区域。研究招募了237名青光眼受试者，通过自动ONH组织分割和数字体积相关技术计算眼压(IOP)诱发的神经组织和筛板(LC)应变。研究团队将生物力学和结构特征输入几何深度学习(Geometric Deep Learning)模型，执行了三项分类任务以检测特定的视野缺陷。实验结果显示，模型达到了0.77-0.88的高曲线下面积(AUC)，证明了ONH应变在形态学基础上进一步提升了视野(VF)损失的预测准确性。研究识别出下方和下颞侧边缘(inferior and inferotemporal rim)为关键的应变敏感区域，这些区域对视野损失预测贡献最大，并随疾病严重程度增加而呈现进展性扩张。该研究表明ONH应变增强了青光眼视野损失模式的预测，且神经视网膜边缘而非筛板(LC)是模型预测中最关键的贡献区域。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.17262v1",
      "published_date": "2025-06-09 16:00:01 UTC",
      "updated_date": "2025-06-09 16:00:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:14:20.661728+00:00"
    },
    {
      "arxiv_id": "2506.07883v1",
      "title": "Diffusion Counterfactual Generation with Semantic Abduction",
      "title_zh": "结合语义溯因的扩散反事实生成",
      "authors": [
        "Rajat Rasal",
        "Avinash Kori",
        "Fabio De Sousa Ribeiro",
        "Tian Xia",
        "Ben Glocker"
      ],
      "abstract": "Counterfactual image generation presents significant challenges, including preserving identity, maintaining perceptual quality, and ensuring faithfulness to an underlying causal model. While existing auto-encoding frameworks admit semantic latent spaces which can be manipulated for causal control, they struggle with scalability and fidelity. Advancements in diffusion models present opportunities for improving counterfactual image editing, having demonstrated state-of-the-art visual quality, human-aligned perception and representation learning capabilities. Here, we present a suite of diffusion-based causal mechanisms, introducing the notions of spatial, semantic and dynamic abduction. We propose a general framework that integrates semantic representations into diffusion models through the lens of Pearlian causality to edit images via a counterfactual reasoning process. To our knowledge, this is the first work to consider high-level semantic identity preservation for diffusion counterfactuals and to demonstrate how semantic control enables principled trade-offs between faithful causal control and identity preservation.",
      "tldr_zh": "该研究针对反事实图像生成（Counterfactual image generation）在身份保留和因果模型保真度方面的局限性，提出了一套基于扩散模型（Diffusion models）的因果机制。研究引入了空间（spatial）、语义（semantic）和动态溯因（dynamic abduction）的概念，构建了一个将语义表示集成到扩散模型中的通用框架。该框架遵循珀尔因果理论（Pearlian causality），通过反事实推理过程对图像进行编辑，从而提升了生成的扩展性和保真度。这是首个在扩散反事实（Diffusion counterfactuals）生成中考虑高级语义身份保留的工作。实验证明，语义控制能够使模型在忠实的因果控制与身份保留之间实现原则性的权衡（trade-offs）。这一成果充分利用了扩散模型在视觉质量和表示学习方面的优势，为实现高保真且符合因果逻辑的图像生成提供了新方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Proceedings of the 42nd International Conference on Machine Learning, Vancouver, Canada",
      "pdf_url": "https://arxiv.org/pdf/2506.07883v1",
      "published_date": "2025-06-09 15:54:00 UTC",
      "updated_date": "2025-06-09 15:54:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:14:35.225427+00:00"
    },
    {
      "arxiv_id": "2506.07865v1",
      "title": "FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity",
      "title_zh": "FreeGave：基于高斯速度的动态视频 3D 物理学习",
      "authors": [
        "Jinxi Li",
        "Ziyang Song",
        "Siyuan Zhou",
        "Bo Yang"
      ],
      "abstract": "In this paper, we aim to model 3D scene geometry, appearance, and the underlying physics purely from multi-view videos. By applying various governing PDEs as PINN losses or incorporating physics simulation into neural networks, existing works often fail to learn complex physical motions at boundaries or require object priors such as masks or types. In this paper, we propose FreeGave to learn the physics of complex dynamic 3D scenes without needing any object priors. The key to our approach is to introduce a physics code followed by a carefully designed divergence-free module for estimating a per-Gaussian velocity field, without relying on the inefficient PINN losses. Extensive experiments on three public datasets and a newly collected challenging real-world dataset demonstrate the superior performance of our method for future frame extrapolation and motion segmentation. Most notably, our investigation into the learned physics codes reveals that they truly learn meaningful 3D physical motion patterns in the absence of any human labels in training.",
      "tldr_zh": "该研究探讨了如何仅从多视角视频中建模 3D 场景的几何、外观及其潜在的物理规律，针对现有方法在处理边界复杂物理运动表现不佳且依赖物体先验（如 masks 或类型）的问题提出了改进方案。论文提出了 FreeGave，这是一种无需任何物体先验即可学习复杂动态 3D 场景物理规律的新框架。其核心创新在于引入了 physics code，并结合精心设计的 divergence-free module 来估计 per-Gaussian velocity field，有效避免了对低效 PINN losses 的依赖。通过在三个公开数据集和一个具有挑战性的真实世界数据集上的广泛实验，该方法在 future frame extrapolation 和 motion segmentation 任务中展现了卓越的性能。分析表明，FreeGave 学习到的 physics codes 在无人工标签的情况下，能够真实地捕捉到具有物理意义的 3D 运动模式，为无监督物理学习提供了重要参考。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CE",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025. Code and data are available at: https://github.com/vLAR-group/FreeGave",
      "pdf_url": "https://arxiv.org/pdf/2506.07865v1",
      "published_date": "2025-06-09 15:31:25 UTC",
      "updated_date": "2025-06-09 15:31:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:14:31.698642+00:00"
    },
    {
      "arxiv_id": "2506.07864v2",
      "title": "Lightweight Sequential Transformers for Blood Glucose Level Prediction in Type-1 Diabetes",
      "title_zh": "面向1型糖尿病血糖水平预测的轻量级序列Transformer模型",
      "authors": [
        "Mirko Paolo Barbato",
        "Giorgia Rigamonti",
        "Davide Marelli",
        "Paolo Napoletano"
      ],
      "abstract": "Type 1 Diabetes (T1D) affects millions worldwide, requiring continuous monitoring to prevent severe hypo- and hyperglycemic events. While continuous glucose monitoring has improved blood glucose management, deploying predictive models on wearable devices remains challenging due to computational and memory constraints. To address this, we propose a novel Lightweight Sequential Transformer model designed for blood glucose prediction in T1D. By integrating the strengths of Transformers' attention mechanisms and the sequential processing of recurrent neural networks, our architecture captures long-term dependencies while maintaining computational efficiency. The model is optimized for deployment on resource-constrained edge devices and incorporates a balanced loss function to handle the inherent data imbalance in hypo- and hyperglycemic events. Experiments on two benchmark datasets, OhioT1DM and DiaTrend, demonstrate that the proposed model outperforms state-of-the-art methods in predicting glucose levels and detecting adverse events. This work fills the gap between high-performance modeling and practical deployment, providing a reliable and efficient T1D management solution.",
      "tldr_zh": "该研究针对1型糖尿病(T1D)管理中穿戴式设备部署困难的挑战，提出了一种新型的Lightweight Sequential Transformer模型。该架构通过整合Transformer的注意力机制(Attention Mechanisms)与循环神经网络(RNN)的序列处理优势，在保持低计算复杂度的同时有效捕捉血糖波动的长程依赖(Long-term Dependencies)。模型针对资源受限的边缘设备(Edge Devices)进行了深度优化，并引入平衡损失函数(Balanced Loss Function)以解决低血糖和高血糖事件中固有的数据不平衡问题。在OhioT1DM和DiaTrend两个基准数据集上的实验结果表明，该模型在血糖水平预测和不良事件检测性能上均优于现有的最先进(State-of-the-art)方法。该项工作成功弥补了高性能深度学习建模与实际临床部署之间的鸿沟，为高效的T1D管理提供了一种可靠且轻量化的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07864v2",
      "published_date": "2025-06-09 15:27:43 UTC",
      "updated_date": "2025-06-14 15:43:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:15:02.949385+00:00"
    },
    {
      "arxiv_id": "2506.07861v1",
      "title": "Fairness Overfitting in Machine Learning: An Information-Theoretic Perspective",
      "title_zh": "机器学习中的公平性过拟合：信息论视角",
      "authors": [
        "Firas Laakom",
        "Haobo Chen",
        "Jürgen Schmidhuber",
        "Yuheng Bu"
      ],
      "abstract": "Despite substantial progress in promoting fairness in high-stake applications using machine learning models, existing methods often modify the training process, such as through regularizers or other interventions, but lack formal guarantees that fairness achieved during training will generalize to unseen data. Although overfitting with respect to prediction performance has been extensively studied, overfitting in terms of fairness loss has received far less attention. This paper proposes a theoretical framework for analyzing fairness generalization error through an information-theoretic lens. Our novel bounding technique is based on Efron-Stein inequality, which allows us to derive tight information-theoretic fairness generalization bounds with both Mutual Information (MI) and Conditional Mutual Information (CMI). Our empirical results validate the tightness and practical relevance of these bounds across diverse fairness-aware learning algorithms. Our framework offers valuable insights to guide the design of algorithms improving fairness generalization.",
      "tldr_zh": "该研究针对机器学习模型在训练中实现的公平性往往难以泛化到未知数据的问题，提出了一个从信息论(Information-Theoretic)视角分析公平性泛化误差的理论框架。针对公平性损失过拟合(Fairness Overfitting)这一此前较少受到关注的领域，作者利用Efron-Stein不等式推导出了基于互信息(Mutual Information, MI)和条件互信息(Conditional Mutual Information, CMI)的紧凑泛化边界。实证结果在多种公平感知学习算法上验证了这些理论边界的紧凑性及其在实际应用中的相关性。该框架不仅深化了对公平性泛化的理论理解，还为未来设计更具泛化能力的公平机器学习算法提供了重要的指导原则。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "38 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.07861v1",
      "published_date": "2025-06-09 15:24:56 UTC",
      "updated_date": "2025-06-09 15:24:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:15:18.616434+00:00"
    },
    {
      "arxiv_id": "2506.07857v1",
      "title": "LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic Segmentation of 3D Point Clouds",
      "title_zh": "LogoSP：基于局部-全局超点分组的3D点云无监督语义分割",
      "authors": [
        "Zihui Zhang",
        "Weisheng Dai",
        "Hongtao Wen",
        "Bo Yang"
      ],
      "abstract": "We study the problem of unsupervised 3D semantic segmentation on raw point clouds without needing human labels in training. Existing methods usually formulate this problem into learning per-point local features followed by a simple grouping strategy, lacking the ability to discover additional and possibly richer semantic priors beyond local features. In this paper, we introduce LogoSP to learn 3D semantics from both local and global point features. The key to our approach is to discover 3D semantic information by grouping superpoints according to their global patterns in the frequency domain, thus generating highly accurate semantic pseudo-labels for training a segmentation network. Extensive experiments on two indoor and an outdoor datasets show that our LogoSP surpasses all existing unsupervised methods by large margins, achieving the state-of-the-art performance for unsupervised 3D semantic segmentation. Notably, our investigation into the learned global patterns reveals that they truly represent meaningful 3D semantics in the absence of human labels during training.",
      "tldr_zh": "该研究针对原始点云(raw point clouds)的无监督3D语义分割(unsupervised 3D semantic segmentation)问题，指出了现有方法仅依赖局部特征而缺乏全局语义先验的局限性。为此，作者提出了LogoSP，一种结合局部和全局点特征的学习框架，通过在频域(frequency domain)中根据全局模式对超点(superpoints)进行分组。这种方法能够有效地发现3D语义信息，并生成高精度的语义伪标签(semantic pseudo-labels)，用于训练分割网络。在两个室内和一个室外数据集上的实验结果表明，LogoSP在性能上大幅领先于现有的无监督方法，达到了领域内最先进(state-of-the-art)的水平。进一步的研究证实，该框架在训练过程中即便没有人工标注，其学习到的全局模式也能够真实地代表具有意义的3D语义特征。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025. Code and data are available at: https://github.com/vLAR-group/LogoSP",
      "pdf_url": "https://arxiv.org/pdf/2506.07857v1",
      "published_date": "2025-06-09 15:21:37 UTC",
      "updated_date": "2025-06-09 15:21:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:15:29.602264+00:00"
    },
    {
      "arxiv_id": "2506.07854v1",
      "title": "Residual Reweighted Conformal Prediction for Graph Neural Networks",
      "title_zh": "面向图神经网络的残差重加权共形预测",
      "authors": [
        "Zheng Zhang",
        "Jie Bao",
        "Zhixin Zhou",
        "Nicolo Colombo",
        "Lixin Cheng",
        "Rui Luo"
      ],
      "abstract": "Graph Neural Networks (GNNs) excel at modeling relational data but face significant challenges in high-stakes domains due to unquantified uncertainty. Conformal prediction (CP) offers statistical coverage guarantees, but existing methods often produce overly conservative prediction intervals that fail to account for graph heteroscedasticity and structural biases. While residual reweighting CP variants address some of these limitations, they neglect graph topology, cluster-specific uncertainties, and risk data leakage by reusing training sets. To address these issues, we propose Residual Reweighted GNN (RR-GNN), a framework designed to generate minimal prediction sets with provable marginal coverage guarantees.\n  RR-GNN introduces three major innovations to enhance prediction performance. First, it employs Graph-Structured Mondrian CP to partition nodes or edges into communities based on topological features, ensuring cluster-conditional coverage that reflects heterogeneity. Second, it uses Residual-Adaptive Nonconformity Scores by training a secondary GNN on a held-out calibration set to estimate task-specific residuals, dynamically adjusting prediction intervals according to node or edge uncertainty. Third, it adopts a Cross-Training Protocol, which alternates the optimization of the primary GNN and the residual predictor to prevent information leakage while maintaining graph dependencies. We validate RR-GNN on 15 real-world graphs across diverse tasks, including node classification, regression, and edge weight prediction. Compared to CP baselines, RR-GNN achieves improved efficiency over state-of-the-art methods, with no loss of coverage.",
      "tldr_zh": "该研究针对图神经网络(GNNs)在处理高风险任务时面临的不确定性量化难题，提出了RR-GNN框架，旨在生成具有边际覆盖保证的最小预测集。由于现有的符合预测(Conformal Prediction)方法往往过于保守且忽略了图的异方差性，RR-GNN引入了Graph-Structured Mondrian CP，利用拓扑特征进行社区划分以实现聚类条件覆盖。同时，该框架采用Residual-Adaptive Nonconformity Scores，通过训练辅助GNN动态估计任务残差并调整预测区间，并配合交叉训练协议(Cross-Training Protocol)防止数据泄露。在15个真实世界图数据集上的实验证明，RR-GNN在节点分类、回归及边权重预测任务中均取得了优于现有SOTA方法的预测效率，且在各种复杂图结构下保持了稳定的覆盖率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07854v1",
      "published_date": "2025-06-09 15:19:17 UTC",
      "updated_date": "2025-06-09 15:19:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:15:26.023286+00:00"
    },
    {
      "arxiv_id": "2506.07853v4",
      "title": "Modeling the Diachronic Evolution of Legal Norms: An LRMoo-Based, Component-Level, Event-Centric Approach to Legal Knowledge Graphs",
      "title_zh": "法律规范历时演化建模：一种基于 LRMoo 的组件级、以事件为中心的法律知识图谱构建方法",
      "authors": [
        "Hudson de Martim"
      ],
      "abstract": "Representing the temporal evolution of legal norms is a critical challenge for automated processing. While foundational frameworks exist, they lack a formal pattern for granular, component-level versioning, hindering the deterministic point-in-time reconstruction of legal texts required by reliable AI applications. This paper proposes a structured, temporal modeling pattern grounded in the LRMoo ontology. Our approach models a norm's evolution as a diachronic chain of versioned F1 Works, distinguishing between language-agnostic Temporal Versions (TV)-each being a distinct Work-and their monolingual Language Versions (LV), modeled as F2 Expressions. The legislative amendment process is formalized through event-centric modeling, allowing changes to be traced precisely. Using the Brazilian Constitution as a case study, we demonstrate that our architecture enables the exact reconstruction of any part of a legal text as it existed on a specific date. This provides a verifiable semantic backbone for legal knowledge graphs, offering a deterministic foundation for trustworthy legal AI.",
      "tldr_zh": "该研究针对法律规范历时演变在自动化处理中的表示挑战，提出了一种基于 LRMoo 本体论的组件级、以事件为中心（event-centric）的时间建模模式。该方法将规范的演变建模为版本化 F1 Works 的历时链，通过区分语种无关的时间版本（Temporal Versions, TV）与单语语言版本（Language Versions, LV），弥补了现有框架在细粒度版本控制方面的不足。通过形式化立法修订过程，该架构能够精确追踪法律条文的变更历史。以《巴西宪法》（Brazilian Constitution）为例的实验证明，该方法可以实现法律文本在任意特定日期的精确重构。这一研究为法律知识图谱（Legal Knowledge Graphs）提供了可验证的语义框架，为构建具备确定性和可信度的法律人工智能（Legal AI）应用奠定了坚实基础。",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "Model Refinement: Defining Temporal Versions as F1 Works",
      "pdf_url": "https://arxiv.org/pdf/2506.07853v4",
      "published_date": "2025-06-09 15:18:36 UTC",
      "updated_date": "2025-11-14 15:26:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:15:28.955022+00:00"
    },
    {
      "arxiv_id": "2506.07848v1",
      "title": "PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement",
      "title_zh": "PolyVivid：基于跨模态交互与增强的生动多主体视频生成",
      "authors": [
        "Teng Hu",
        "Zhentao Yu",
        "Zhengguang Zhou",
        "Jiangning Zhang",
        "Yuan Zhou",
        "Qinglin Lu",
        "Ran Yi"
      ],
      "abstract": "Despite recent advances in video generation, existing models still lack fine-grained controllability, especially for multi-subject customization with consistent identity and interaction. In this paper, we propose PolyVivid, a multi-subject video customization framework that enables flexible and identity-consistent generation. To establish accurate correspondences between subject images and textual entities, we design a VLLM-based text-image fusion module that embeds visual identities into the textual space for precise grounding. To further enhance identity preservation and subject interaction, we propose a 3D-RoPE-based enhancement module that enables structured bidirectional fusion between text and image embeddings. Moreover, we develop an attention-inherited identity injection module to effectively inject fused identity features into the video generation process, mitigating identity drift. Finally, we construct an MLLM-based data pipeline that combines MLLM-based grounding, segmentation, and a clique-based subject consolidation strategy to produce high-quality multi-subject data, effectively enhancing subject distinction and reducing ambiguity in downstream video generation. Extensive experiments demonstrate that PolyVivid achieves superior performance in identity fidelity, video realism, and subject alignment, outperforming existing open-source and commercial baselines.",
      "tldr_zh": "该研究提出了PolyVivid，一种旨在实现灵活且身份一致的多主体视频定制化生成框架，解决了现有模型在多主体交互和细粒度控制方面的局限性。该框架设计了一个基于VLLM的文本图像融合模块，将视觉身份(visual identities)嵌入文本空间以实现精确对齐，并引入基于3D-RoPE的增强模块来促进文本与图像嵌入之间的结构化双向融合，从而强化身份保持和主体交互。为了缓解身份漂移(identity drift)问题，研究者开发了注意力继承的身份注入模块(attention-inherited identity injection module)，将融合后的身份特征有效整合进视频生成过程中。此外，团队还构建了一个基于MLLM的数据流水线，结合主体分割和基于团的主体整合策略(clique-based subject consolidation strategy)来生成高质量的多主体数据，有效降低了生成过程中的歧义性。实验结果表明，PolyVivid在身份忠实度(identity fidelity)、视频逼真度和主体对齐方面表现卓越，其性能显著优于现有的开源和商业基线模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07848v1",
      "published_date": "2025-06-09 15:11:09 UTC",
      "updated_date": "2025-06-09 15:11:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:15:32.943480+00:00"
    },
    {
      "arxiv_id": "2506.07841v1",
      "title": "Diffusion models under low-noise regime",
      "title_zh": "低噪声机制下的扩散模型",
      "authors": [
        "Elizabeth Pavlova",
        "Xue-Xin Wei"
      ],
      "abstract": "Recent work on diffusion models proposed that they operate in two regimes: memorization, in which models reproduce their training data, and generalization, in which they generate novel samples. While this has been tested in high-noise settings, the behavior of diffusion models as effective denoisers when the corruption level is small remains unclear. To address this gap, we systematically investigated the behavior of diffusion models under low-noise diffusion dynamics, with implications for model robustness and interpretability. Using (i) CelebA subsets of varying sample sizes and (ii) analytic Gaussian mixture benchmarks, we reveal that models trained on disjoint data diverge near the data manifold even when their high-noise outputs converge. We quantify how training set size, data geometry, and model objective choice shape denoising trajectories and affect score accuracy, providing insights into how these models actually learn representations of data distributions. This work starts to address gaps in our understanding of generative model reliability in practical applications where small perturbations are common.",
      "tldr_zh": "该研究系统地探讨了 Diffusion models 在低噪声状态 (low-noise regime) 下的运作特征，旨在厘清模型在腐蚀程度较低时的有效去噪行为。研究者利用不同规模的 CelebA 数据子集和解析性的 Gaussian mixture 基准进行实验，发现即使模型在高噪声阶段的输出趋于一致，在接近数据流形 (data manifold) 时，基于不相交数据训练的模型仍会产生显著分歧。通过量化分析训练集规模 (training set size)、数据几何 (data geometry) 以及模型目标选择 (model objective choice) 对去噪轨迹 (denoising trajectories) 和评分准确性 (score accuracy) 的影响，该研究揭示了模型学习数据分布表示的内在逻辑。这项工作填补了生成模型在处理微小扰动时可靠性方面的认知空白，为提升模型的鲁棒性与可解释性提供了关键见解。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07841v1",
      "published_date": "2025-06-09 15:07:16 UTC",
      "updated_date": "2025-06-09 15:07:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:15:32.815333+00:00"
    },
    {
      "arxiv_id": "2506.07837v1",
      "title": "HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to General Specific Domains",
      "title_zh": "HAIBU-ReMUD：桥接通用与特定领域的推理性多模态超声数据集及模型",
      "authors": [
        "Shijie Wang",
        "Yilun Zhang",
        "Zeyu Lai",
        "Dexing Kong"
      ],
      "abstract": "Multimodal large language models (MLLMs) have shown great potential in general domains but perform poorly in some specific domains due to a lack of domain-specific data, such as image-text data or vedio-text data. In some specific domains, there is abundant graphic and textual data scattered around, but lacks standardized arrangement. In the field of medical ultrasound, there are ultrasonic diagnostic books, ultrasonic clinical guidelines, ultrasonic diagnostic reports, and so on. However, these ultrasonic materials are often saved in the forms of PDF, images, etc., and cannot be directly used for the training of MLLMs. This paper proposes a novel image-text reasoning supervised fine-tuning data generation pipeline to create specific domain quadruplets (image, question, thinking trace, and answer) from domain-specific materials. A medical ultrasound domain dataset ReMUD is established, containing over 45,000 reasoning and non-reasoning supervised fine-tuning Question Answering (QA) and Visual Question Answering (VQA) data. The ReMUD-7B model, fine-tuned on Qwen2.5-VL-7B-Instruct, outperforms general-domain MLLMs in medical ultrasound field. To facilitate research, the ReMUD dataset, data generation codebase, and ReMUD-7B parameters will be released at https://github.com/ShiDaizi/ReMUD, addressing the data shortage issue in specific domain MLLMs.",
      "tldr_zh": "该研究针对多模态大语言模型 (MLLMs) 在医学超声等特定领域因缺乏标准化领域数据而表现不佳的问题，提出了一种创新的图像-文本推理监督微调 (Supervised Fine-Tuning) 数据生成流水线。该流水线能从超声诊断书、临床指南及报告等非结构化材料中，提取并构建包含图像 (Image)、问题 (Question)、思考轨迹 (Thinking Trace) 和答案 (Answer) 的四元组数据。利用该流水线，研究团队建立了 ReMUD 医学超声领域数据集，包含超过 45,000 条推理与非推理的问答 (QA) 及视觉问答 (Visual Question Answering, VQA) 数据。基于 Qwen2.5-VL-7B-Instruct 微调得到的 ReMUD-7B 模型在医学超声领域的表现显著优于通用领域模型。该研究通过开源 ReMUD 数据集、数据生成代码库及模型参数，为缓解特定领域 MLLMs 的数据短缺问题做出了重要贡献。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07837v1",
      "published_date": "2025-06-09 15:01:38 UTC",
      "updated_date": "2025-06-09 15:01:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:15:44.156447+00:00"
    },
    {
      "arxiv_id": "2506.07836v1",
      "title": "Are Trees Really Green? A Detection Approach of IoT Malware Attacks",
      "title_zh": "树模型真的“绿色”吗？一种物联网恶意软件攻击检测方法",
      "authors": [
        "Silvia Lucia Sanna",
        "Diego Soi",
        "Davide Maiorca",
        "Giorgio Giacinto"
      ],
      "abstract": "Nowadays, the Internet of Things (IoT) is widely employed, and its usage is growing exponentially because it facilitates remote monitoring, predictive maintenance, and data-driven decision making, especially in the healthcare and industrial sectors. However, IoT devices remain vulnerable due to their resource constraints and difficulty in applying security patches. Consequently, various cybersecurity attacks are reported daily, such as Denial of Service, particularly in IoT-driven solutions. Most attack detection methodologies are based on Machine Learning (ML) techniques, which can detect attack patterns. However, the focus is more on identification rather than considering the impact of ML algorithms on computational resources. This paper proposes a green methodology to identify IoT malware networking attacks based on flow privacy-preserving statistical features. In particular, the hyperparameters of three tree-based models -- Decision Trees, Random Forest and Extra-Trees -- are optimized based on energy consumption and test-time performance in terms of Matthew's Correlation Coefficient. Our results show that models maintain high performance and detection accuracy while consistently reducing power usage in terms of watt-hours (Wh). This suggests that on-premise ML-based Intrusion Detection Systems are suitable for IoT and other resource-constrained devices.",
      "tldr_zh": "该研究针对物联网(IoT)设备在资源受限环境下易受网络攻击的挑战，提出了一种旨在平衡检测效率与能耗的“绿色”检测方案。该方法利用流量隐私保护统计特征，对三种基于树的机器学习模型——Decision Trees、Random Forest和Extra-Trees进行超参数优化，重点考量其能量消耗与Matthew's Correlation Coefficient (MCC)性能表现。实验结果表明，该方案在维持高检测准确率的同时，显著降低了运行时的瓦时(Wh)能耗。这一研究结果证实了在资源受限的IoT设备上部署本地机器学习入侵检测系统(Intrusion Detection Systems, IDS)的可行性，为构建高效、低耗的物联网安全防护机制提供了实证依据。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07836v1",
      "published_date": "2025-06-09 15:01:04 UTC",
      "updated_date": "2025-06-09 15:01:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:15:46.991396+00:00"
    },
    {
      "arxiv_id": "2506.07833v2",
      "title": "Improving Large Language Models with Concept-Aware Fine-Tuning",
      "title_zh": "通过概念感知微调提升大语言模型",
      "authors": [
        "Michael K. Chen",
        "Xikun Zhang",
        "Jiaxing Huang",
        "Dacheng Tao"
      ],
      "abstract": "Large language models (LLMs) have become the cornerstone of modern AI. However, the existing paradigm of next-token prediction fundamentally limits their ability to form coherent, high-level concepts, making it a critical barrier to human-like understanding and reasoning. Take the phrase \"ribonucleic acid\" as an example: an LLM will first decompose it into tokens, i.e., artificial text fragments (\"rib\", \"on\", ...), then learn each token sequentially, rather than grasping the phrase as a unified, coherent semantic entity. This fragmented representation hinders deeper conceptual understanding and, ultimately, the development of truly intelligent systems. In response, we introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method that redefines how LLMs are fine-tuned. By enabling the learning of sequences that span multiple tokens, this method fosters stronger concept-aware learning. Our experiments demonstrate significant improvements compared to conventional next-token finetuning methods across diverse tasks, including traditional applications like text summarization and domain-specific ones like de novo protein design. Multi-token prediction was previously only possible in the prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first to bring the multi-token setting to the post-training phase, thus effectively democratizing its benefits for the broader community of practitioners and researchers. Finally, the unexpected effectiveness of our proposed method suggests wider implications for the machine learning research community. All code and data are available at https://github.com/michaelchen-lab/caft-llm",
      "tldr_zh": "该研究针对大语言模型 (LLMs) 在下一标记预测 (Next-token prediction) 范式下难以形成高层概念及语义表示碎片化的问题，提出了概念感知微调 (Concept-Aware Fine-Tuning, CAFT)。CAFT 引入了一种全新的多标记 (Multi-token) 训练方法，通过支持跨越多个标记的序列学习，促使模型建立更强的语义相干性和概念感知能力。该研究首次将以往仅限于高成本预训练阶段的多标记预测技术应用于后训练 (Post-training) 阶段，显著降低了该技术的使用门槛。实验结果表明，CAFT 在文本摘要和蛋白质从头设计 (De novo protein design) 等多样化任务中，其性能表现均显著优于传统的微调方法。这一发现不仅证明了 CAFT 在提升模型智能方面的有效性，也为未来机器学习领域的研究提供了重要启发。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07833v2",
      "published_date": "2025-06-09 14:55:00 UTC",
      "updated_date": "2025-06-13 17:24:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:15:48.064569+00:00"
    },
    {
      "arxiv_id": "2506.07829v2",
      "title": "Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information",
      "title_zh": "利用时序因果信息实现多智能体强化学习的去中心化",
      "authors": [
        "Jan Corazza",
        "Hadi Partovi Aria",
        "Hyohun Kim",
        "Daniel Neider",
        "Zhe Xu"
      ],
      "abstract": "Reinforcement learning (RL) algorithms can find an optimal policy for a single agent to accomplish a particular task. However, many real-world problems require multiple agents to collaborate in order to achieve a common goal. For example, a robot executing a task in a warehouse may require the assistance of a drone to retrieve items from high shelves. In Decentralized Multi-Agent RL (DMARL), agents learn independently and then combine their policies at execution time, but often must satisfy constraints on compatibility of local policies to ensure that they can achieve the global task when combined. In this paper, we study how providing high-level symbolic knowledge to agents can help address unique challenges of this setting, such as privacy constraints, communication limitations, and performance concerns. In particular, we extend the formal tools used to check the compatibility of local policies with the team task, making decentralized training with theoretical guarantees usable in more scenarios. Furthermore, we empirically demonstrate that symbolic knowledge about the temporal evolution of events in the environment can significantly expedite the learning process in DMARL.",
      "tldr_zh": "该研究针对去中心化多智能体强化学习(Decentralized Multi-Agent RL, DMARL)在隐私约束、通信限制和性能方面的挑战，探讨了引入高层符号知识(symbolic knowledge)的作用。为了确保独立学习的智能体在执行阶段合并策略时能满足全局任务的兼容性(compatibility)，作者扩展了现有的形式化工具，使得具备理论保证的去中心化训练能够应用于更广泛的逻辑场景。研究重点分析了环境事件随时间演化的符号知识如何辅助决策，并提供了相应的理论框架。实验结果表明，利用这种时间因果信息(Temporal Causal Information)能够显著加快DMARL的策略学习速度。该工作为解决复杂现实世界任务中的多智能体协作问题提供了兼具理论支撑与执行效率的新思路。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "Code available at https://github.com/corazza/tcdmarl",
      "pdf_url": "https://arxiv.org/pdf/2506.07829v2",
      "published_date": "2025-06-09 14:53:03 UTC",
      "updated_date": "2025-10-14 21:29:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:15:54.654390+00:00"
    },
    {
      "arxiv_id": "2506.07824v2",
      "title": "Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs",
      "title_zh": "加法四部曲：刻画大语言模型中的逐层信息轨迹",
      "authors": [
        "Yao Yan"
      ],
      "abstract": "Multi-digit addition is a clear probe of the computational power of large language models. To dissect the internal arithmetic processes in LLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection. Inspired by the step-by-step manner in which humans perform addition, we propose and analyze a coherent four-stage trajectory in the forward pass:Formula-structure representations become linearly decodable first, while the answer token is still far down the candidate list.Core computational features then emerge prominently.At deeper activation layers, numerical abstractions of the result become clearer, enabling near-perfect detection and decoding of the individual digits in the sum.Near the output, the model organizes and generates the final content, with the correct token reliably occupying the top rank.This trajectory suggests a hierarchical process that favors internal computation over rote memorization. We release our code and data to facilitate reproducibility.",
      "tldr_zh": "该研究通过结合线性探测(linear probing)与logit-lens检查，深入剖析了LLaMA-3-8B-Instruct模型在处理多位加法时的内部算术过程。作者在模型的前向传播中识别出一个连贯的“四阶段轨迹”，揭示了信息如何在不同层级间演化。首先，模型会初步形成公式结构(Formula-structure)的线性可解码表示；随后，核心计算特征显著增强；在更深层的激活中，结果的数值抽象变得清晰，使得对和数中单个数字的检测和解码达到近乎完美的程度；最后在输出层附近，模型完成最终内容的组织并生成正确标记。这一轨迹表明大语言模型在执行算术任务时倾向于采用层级化的内部计算，而非简单的机械记忆。该项工作为理解大语言模型的内部推理机制提供了重要的实证证据，并公开发布了相关代码和数据。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, including appendix, 7 figures. EMNLP 2025 submission (ARR May 2025 cycle, reviews pending)",
      "pdf_url": "https://arxiv.org/pdf/2506.07824v2",
      "published_date": "2025-06-09 14:48:43 UTC",
      "updated_date": "2025-09-09 16:35:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:16:13.813681+00:00"
    },
    {
      "arxiv_id": "2506.07822v2",
      "title": "Accelerating Diffusion Planners in Offline RL via Reward-Aware Consistency Trajectory Distillation",
      "title_zh": "通过奖励感知一致性轨迹蒸馏加速离线强化学习中的扩散规划器",
      "authors": [
        "Xintong Duan",
        "Yutong He",
        "Fahim Tajwar",
        "Ruslan Salakhutdinov",
        "J. Zico Kolter",
        "Jeff Schneider"
      ],
      "abstract": "Although diffusion models have achieved strong results in decision-making tasks, their slow inference speed remains a key limitation. While consistency models offer a potential solution, existing applications to decision-making either struggle with suboptimal demonstrations under behavior cloning or rely on complex concurrent training of multiple networks under the actor-critic framework. In this work, we propose a novel approach to consistency distillation for offline reinforcement learning that directly incorporates reward optimization into the distillation process. Our method achieves single-step sampling while generating higher-reward action trajectories through decoupled training and noise-free reward signals. Empirical evaluations on the Gym MuJoCo, FrankaKitchen, and long horizon planning benchmarks demonstrate that our approach can achieve a 9.7% improvement over previous state-of-the-art while offering up to 142x speedup over diffusion counterparts in inference time.",
      "tldr_zh": "该研究提出了一种名为 Reward-Aware Consistency Trajectory Distillation 的方法，旨在解决 Offline RL 中 Diffusion Planners 推理速度较慢的瓶颈问题。该方法通过将奖励优化直接引入 Consistency Distillation 过程，有效解决了现有模型在面对次优演示（suboptimal demonstrations）时表现不佳或训练架构过于复杂的问题。通过解耦训练（decoupled training）和无噪声奖励信号的使用，该技术能够实现高效的 Single-step sampling 并生成更高奖励的动作轨迹。实验结果显示，该方法在 Gym MuJoCo 和 FrankaKitchen 等基准测试中比现有的 state-of-the-art 提升了 9.7% 的性能。此外，与传统的扩散模型相比，其推理速度实现了高达 142 倍的显著提升，为大规模决策任务提供了更具实用性的方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07822v2",
      "published_date": "2025-06-09 14:48:19 UTC",
      "updated_date": "2025-12-26 17:50:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:16:17.409273+00:00"
    },
    {
      "arxiv_id": "2506.07820v2",
      "title": "Guideline Forest: Experience-Induced Multi-Guideline Reasoning with Stepwise Aggregation",
      "title_zh": "Guideline Forest：基于经验诱导与分步聚合的多指南推理",
      "authors": [
        "Jiaxiang Chen",
        "Zhuo Wang",
        "Mingxi Zou",
        "Qifan Wang",
        "Zenglin Xu"
      ],
      "abstract": "Human reasoning is flexible, adaptive, and grounded in prior experience-qualities that large language models (LLMs) still struggle to emulate. Existing methods either explore diverse reasoning paths at inference time or search for optimal workflows through expensive operations, but both fall short in leveraging multiple reusable strategies in a structured, efficient manner. We propose Guideline Forest, a framework that enhances LLMs reasoning by inducing structured reasoning strategies-called guidelines-from verified examples and executing them via step-wise aggregation. Unlike test-time search or single-path distillation, our method draws on verified reasoning experiences by inducing reusable guidelines and expanding each into diverse variants. Much like human reasoning, these variants reflect alternative thought patterns, are executed in parallel, refined via self-correction, and aggregated step by step-enabling the model to adaptively resolve uncertainty and synthesize robust solutions.We evaluate Guideline Forest on four benchmarks-GSM8K, MATH-500, MBPP, and HumanEval-spanning mathematical and programmatic reasoning. Guideline Forest consistently outperforms strong baselines, including CoT, ReAct, ToT, FoT, and AFlow. Ablation studies further highlight the effectiveness of multi-path reasoning and stepwise aggregation, underscoring the Guideline Forest's adaptability and generalization potential.",
      "tldr_zh": "该研究提出了 Guideline Forest，这是一种通过从验证过的实例中归纳结构化推理策略（称为 guidelines）并执行逐步聚合（step-wise aggregation）来增强大语言模型（LLMs）推理能力的框架。与传统的推理路径探索或工作流搜索不同，该方法利用已验证的推理经验诱导可重用的 guidelines，并将其扩展为模拟人类思维模式的多种变体。这些变体通过并行执行、自我修正（self-correction）和逐步聚合，使模型能够自适应地处理不确定性并生成稳健的解。实验在 GSM8K、MATH-500、MBPP 和 HumanEval 等多个数学与编程基准测试上进行了验证，结果表明 Guideline Forest 的表现一致优于 CoT、ReAct、ToT、FoT 和 AFlow 等主流基线模型。消融研究进一步证实了多路径推理（multi-path reasoning）与逐步聚合在提升模型适应性和泛化潜力方面的核心作用。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07820v2",
      "published_date": "2025-06-09 14:46:31 UTC",
      "updated_date": "2025-06-10 02:05:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:16:18.390875+00:00"
    },
    {
      "arxiv_id": "2506.07813v1",
      "title": "Self-Cascaded Diffusion Models for Arbitrary-Scale Image Super-Resolution",
      "title_zh": "面向任意尺度图像超分辨率的自级联扩散模型",
      "authors": [
        "Junseo Bang",
        "Joonhee Lee",
        "Kyeonghyun Lee",
        "Haechang Lee",
        "Dong Un Kang",
        "Se Young Chun"
      ],
      "abstract": "Arbitrary-scale image super-resolution aims to upsample images to any desired resolution, offering greater flexibility than traditional fixed-scale super-resolution. Recent approaches in this domain utilize regression-based or generative models, but many of them are a single-stage upsampling process, which may be challenging to learn across a wide, continuous distribution of scaling factors. Progressive upsampling strategies have shown promise in mitigating this issue, yet their integration with diffusion models for flexible upscaling remains underexplored. Here, we present CasArbi, a novel self-cascaded diffusion framework for arbitrary-scale image super-resolution. CasArbi meets the varying scaling demands by breaking them down into smaller sequential factors and progressively enhancing the image resolution at each step with seamless transitions for arbitrary scales. Our novel coordinate-guided residual diffusion model allows for the learning of continuous image representations while enabling efficient diffusion sampling. Extensive experiments demonstrate that our CasArbi outperforms prior arts in both perceptual and distortion performance metrics across diverse arbitrary-scale super-resolution benchmarks.",
      "tldr_zh": "该研究提出了CasArbi，一种用于任意比例图像超分辨率(Arbitrary-scale image super-resolution)的新型自级联扩散框架(self-cascaded diffusion framework)。针对传统单阶段模型在处理宽范围、连续缩放比例时面临的训练挑战，CasArbi通过将大幅度的缩放需求分解为更小的序列因子，并在每个步骤中逐步增强图像分辨率，实现了任意比例间的无缝衔接。该框架引入了创新的坐标引导残差扩散模型(coordinate-guided residual diffusion model)，旨在学习连续的图像表示，同时确保高效的扩散采样。大量实验结果证明，CasArbi在多个任意比例超分辨率基准测试中，其感知质量和失真性能指标均超越了现有的先进方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07813v1",
      "published_date": "2025-06-09 14:43:21 UTC",
      "updated_date": "2025-06-09 14:43:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:16:18.524320+00:00"
    },
    {
      "arxiv_id": "2506.18915v2",
      "title": "Automatic Depression Assessment using Machine Learning: A Comprehensive Survey",
      "title_zh": "基于机器学习的自动化抑郁评估：全面综述",
      "authors": [
        "Siyang Song",
        "Yupeng Huo",
        "Shiqing Tang",
        "Jiaee Cheong",
        "Rui Gao",
        "Michel Valstar",
        "Hatice Gunes"
      ],
      "abstract": "Depression is a common mental illness across current human society. Traditional depression assessment relying on inventories and interviews with psychologists frequently suffer from subjective diagnosis results, slow and expensive diagnosis process as well as lack of human resources. Since there is a solid evidence that depression is reflected by various human internal brain activities and external expressive behaviours, early traditional machine learning (ML) and advanced deep learning (DL) models have been widely explored for human behaviour-based automatic depression assessment (ADA) since 2012. However, recent ADA surveys typically only focus on a limited number of human behaviour modalities. Despite being used as a theoretical basis for developing ADA approaches, existing ADA surveys lack a comprehensive review and summary of multi-modal depression-related human behaviours. To bridge this gap, this paper specifically summarises depression-related human behaviours across a range of modalities (e.g. the human brain, verbal language and non-verbal audio/facial/body behaviours). We focus on conducting an up-to-date and comprehensive survey of ML-based ADA approaches for learning depression cues from these behaviours as well as discussing and comparing their distinctive features and limitations. In addition, we also review existing ADA competitions and datasets, identify and discuss the main challenges and opportunities to provide further research directions for future ADA researchers.",
      "tldr_zh": "该综述针对传统抑郁症评估依赖量表和访谈导致的主观性强、效率低等挑战，对基于机器学习(Machine Learning)的自动抑郁症评估(Automatic Depression Assessment, ADA)进行了全面回顾。研究系统总结了自2012年以来利用深度学习(Deep Learning)等技术从人类内部大脑活动及外部言语(Verbal Language)、音频、面部表情和身体行为中提取抑郁线索的方法。文章对比分析了不同模态下ADA方法的特征与局限性，弥补了现有文献在多模态综合评估方面的空白。此外，研究还梳理了现有的ADA相关竞赛与数据集，并深入探讨了该领域当前的挑战与未来机遇。这一工作为未来研究人员利用多模态数据开发高效、客观的抑郁症评估系统提供了重要指南。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18915v2",
      "published_date": "2025-06-09 14:40:16 UTC",
      "updated_date": "2025-06-29 10:44:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:16:28.832401+00:00"
    },
    {
      "arxiv_id": "2506.07807v2",
      "title": "A Proposal to Extend the Common Model of Cognition with Metacognition",
      "title_zh": "将元认知引入通用认知模型的扩展方案",
      "authors": [
        "John Laird",
        "Christian Lebiere",
        "Paul Rosenbloom",
        "Andrea Stocco"
      ],
      "abstract": "The Common Model of Cognition (CMC) provides an abstract characterization of the structure and processing required by a cognitive architecture for human-like minds. We propose a unified approach to integrating metacognition within the CMC. We propose that metacognition involves reasoning over explicit representations of an agent's cognitive capabilities and processes in working memory. Our proposal exploits the existing cognitive capabilities of the CMC, making minimal extensions in the structure and information available within working memory. We provide examples of metacognition within our proposal.",
      "tldr_zh": "该研究针对Common Model of Cognition (CMC)提出了一个统一的元认知(Metacognition)集成框架，旨在增强类人智能认知架构的功能。作者提出元认知本质上是智能体在Working Memory中对其自身认知能力和处理过程的显式表示进行推理的过程。该方案通过利用CMC现有的认知机制，仅对Working Memory的结构和信息获取方式进行了微小扩展，确保了架构的简洁性。这种设计允许智能体在不改变核心组件的情况下，实现对内部处理状态的自我监控与调整。论文最后通过多个元认知实例展示了该提案如何具体应用于复杂的认知任务，为未来认知架构的标准化提供了重要参考。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07807v2",
      "published_date": "2025-06-09 14:35:48 UTC",
      "updated_date": "2025-06-11 20:35:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:16:41.038189+00:00"
    },
    {
      "arxiv_id": "2506.07804v1",
      "title": "Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability",
      "title_zh": "借助符合性预测提升对抗鲁棒性：一种确保模型可靠性的框架",
      "authors": [
        "Jie Bao",
        "Chuangyin Dang",
        "Rui Luo",
        "Hanwei Zhang",
        "Zhixin Zhou"
      ],
      "abstract": "As deep learning models are increasingly deployed in high-risk applications, robust defenses against adversarial attacks and reliable performance guarantees become paramount. Moreover, accuracy alone does not provide sufficient assurance or reliable uncertainty estimates for these models. This study advances adversarial training by leveraging principles from Conformal Prediction. Specifically, we develop an adversarial attack method, termed OPSA (OPtimal Size Attack), designed to reduce the efficiency of conformal prediction at any significance level by maximizing model uncertainty without requiring coverage guarantees. Correspondingly, we introduce OPSA-AT (Adversarial Training), a defense strategy that integrates OPSA within a novel conformal training paradigm. Experimental evaluations demonstrate that our OPSA attack method induces greater uncertainty compared to baseline approaches for various defenses. Conversely, our OPSA-AT defensive model significantly enhances robustness not only against OPSA but also other adversarial attacks, and maintains reliable prediction. Our findings highlight the effectiveness of this integrated approach for developing trustworthy and resilient deep learning models for safety-critical domains. Our code is available at https://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction.",
      "tldr_zh": "该研究探讨了在关键任务领域中深度学习模型的Adversarial Robustness和可靠性能保证问题，指出单纯的准确率不足以提供充足的信心或可靠的不确定性估计。作者提出了一种名为OPSA (OPtimal Size Attack) 的对抗攻击方法，旨在通过最大化模型不确定性来降低Conformal Prediction在任何显著性水平下的效率。针对此类攻击，研究进一步引入了OPSA-AT (Adversarial Training) 防御策略，将OPSA整合到一种新型的符合性训练范式中。实验评估显示，OPSA诱导的不确定性显著高于基准攻击方法，而OPSA-AT模型不仅增强了针对多种对抗攻击的鲁棒性，还维持了预测的可靠性。该集成框架为在安全关键领域开发可信且具弹性的深度学习模型提供了有效的技术路径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07804v1",
      "published_date": "2025-06-09 14:33:28 UTC",
      "updated_date": "2025-06-09 14:33:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:16:41.172583+00:00"
    },
    {
      "arxiv_id": "2506.07801v3",
      "title": "MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification",
      "title_zh": "MultiMatch：面向半监督文本分类的多头一致性正则化匹配",
      "authors": [
        "Iustin Sirbu",
        "Robert-Adrian Popovici",
        "Cornelia Caragea",
        "Stefan Trausan-Matu",
        "Traian Rebedea"
      ],
      "abstract": "We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm combining the paradigms of co-training and consistency regularization with pseudo-labeling. At its core, MultiMatch features a pseudo-label weighting module designed for selecting and filtering pseudo-labels based on head agreement and model confidence, and weighting them according to the perceived classification difficulty. This novel module enhances and unifies three existing techniques -- heads agreement from Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average Pseudo-Margins from MarginMatch -- resulting in a holistic approach that improves robustness and performance in SSL settings. Experimental results on benchmark datasets highlight the superior performance of MultiMatch, i.e., MultiMatch achieves state-of-the-art results on 8 out of 10 setups from 5 natural language processing datasets and ranks first according to the Friedman test among 21 methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly imbalanced settings, outperforming the second-best approach by 3.26%, a critical advantage for real-world text classification tasks. Our code is available on GitHub.",
      "tldr_zh": "该研究提出了 MultiMatch，这是一种结合了共同训练 (co-training)、一致性正则化 (consistency regularization) 和伪标签 (pseudo-labeling) 范式的创新半监督学习 (SSL) 算法。其核心是一个伪标签权重模块 (pseudo-label weighting module)，该模块根据多头一致性 (head agreement) 和模型置信度筛选伪标签，并依据感知的分类难度进行加权处理。该算法通过整合 Multihead Co-training 的一致性机制、FreeMatch 的自适应阈值 (self-adaptive thresholds) 以及 MarginMatch 的平均伪边缘 (Average Pseudo-Margins) 技术，构建了一套提升鲁棒性和性能的整体框架。在基准数据集上的实验结果表明，MultiMatch 在 5 个自然语言处理数据集的 10 个实验设置中，有 8 个达到了 SOTA 结果，并在 21 种对比方法中排名第一。此外，MultiMatch 在高度不平衡的数据环境下表现出卓越的鲁棒性，性能比次优方法高出 3.26%，极大地增强了其在现实世界文本分类任务中的实用性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "This is the camera-ready version of the paper, accepted for publication in the Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.07801v3",
      "published_date": "2025-06-09 14:27:47 UTC",
      "updated_date": "2025-11-01 10:40:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:16:33.603081+00:00"
    },
    {
      "arxiv_id": "2506.07785v1",
      "title": "Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger",
      "title_zh": "通过树搜索重排序推理上下文增强大型视觉语言模型",
      "authors": [
        "Qi Yang",
        "Chenghao Zhang",
        "Lubin Fan",
        "Kun Ding",
        "Jieping Ye",
        "Shiming Xiang"
      ],
      "abstract": "Recent advancements in Large Vision Language Models (LVLMs) have significantly improved performance in Visual Question Answering (VQA) tasks through multimodal Retrieval-Augmented Generation (RAG). However, existing methods still face challenges, such as the scarcity of knowledge with reasoning examples and erratic responses from retrieved knowledge. To address these issues, in this study, we propose a multimodal RAG framework, termed RCTS, which enhances LVLMs by constructing a Reasoning Context-enriched knowledge base and a Tree Search re-ranking method. Specifically, we introduce a self-consistent evaluation mechanism to enrich the knowledge base with intrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with Heuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This ensures that LVLMs can leverage high-quality contextual reasoning for better and more consistent responses. Extensive experiments demonstrate that our framework achieves state-of-the-art performance on multiple VQA datasets, significantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods. It highlights the effectiveness of our knowledge base and re-ranking method in improving LVLMs. Our code is available at https://github.com/yannqi/RCTS-RAG.",
      "tldr_zh": "该研究提出了RCTS，一种旨在增强大型视觉语言模型(LVLMs)性能的多模态检索增强生成(RAG)框架。为了应对推理示例匮乏及检索知识响应不稳定的挑战，该框架通过自一致性评估机制构建了富含推理上下文的知识库。研究还创新性地引入了带有启发式奖励的蒙特卡洛树搜索(MCTS-HR)重排序方法，以精准筛选并优先处理最相关的推理示例。这种机制确保了LVLMs能够利用高质量的上下文推理来生成更准确且一致的回答。实验结果表明，RCTS在多个视觉问答(VQA)数据集上达到了最先进(SOTA)水平，性能显著优于上下文学习(ICL)和传统的Vanilla-RAG方法。该工作证明了结合树搜索与优化推理上下文排序能有效提升视觉语言模型在复杂任务中的表现。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "ICML 2025 Spotlight. 22 pages, 16 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.07785v1",
      "published_date": "2025-06-09 14:00:57 UTC",
      "updated_date": "2025-06-09 14:00:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:17:40.620409+00:00"
    },
    {
      "arxiv_id": "2506.08066v2",
      "title": "WWAggr: A Window Wasserstein-based Aggregation for Ensemble Change Point Detection",
      "title_zh": "WWAggr：基于窗口 Wasserstein 距离的集成变点检测聚合方法",
      "authors": [
        "Alexander Stepikin",
        "Evgenia Romanenkova",
        "Alexey Zaytsev"
      ],
      "abstract": "Change Point Detection (CPD) aims to identify moments of abrupt distribution shifts in data streams. Real-world high-dimensional CPD remains challenging due to data pattern complexity and violation of common assumptions. Resorting to standalone deep neural networks, the current state-of-the-art detectors have yet to achieve perfect quality. Concurrently, ensembling provides more robust solutions, boosting the performance. In this paper, we investigate ensembles of deep change point detectors and realize that standard prediction aggregation techniques, e.g., averaging, are suboptimal and fail to account for problem peculiarities. Alternatively, we introduce WWAggr -- a novel task-specific method of ensemble aggregation based on the Wasserstein distance. Our procedure is versatile, working effectively with various ensembles of deep CPD models. Moreover, unlike existing solutions, we practically lift a long-standing problem of the decision threshold selection for CPD.",
      "tldr_zh": "变点检测(Change Point Detection, CPD)旨在识别数据流中分布突然变化的时刻，但在高维且复杂的数据模式下，现有的深度神经网络检测器仍难以达到理想的检测质量。虽然集成学习(Ensembling)能提供更鲁棒的解决方案并提升性能，但传统的预测聚合技术（如平均法）并非最优，且未能充分考虑CPD问题的特殊性。该研究提出了WWAggr，一种基于Wasserstein距离的新型任务特定集成聚合方法。该程序具有通用性，能够有效适配各种深度CPD模型的集成。此外，与现有解决方案不同，WWAggr从实践层面解决了CPD领域长期存在的决策阈值(Decision Threshold)选择难题。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08066v2",
      "published_date": "2025-06-09 13:52:10 UTC",
      "updated_date": "2025-10-02 11:27:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:16:40.487338+00:00"
    },
    {
      "arxiv_id": "2506.13981v1",
      "title": "HAELT: A Hybrid Attentive Ensemble Learning Transformer Framework for High-Frequency Stock Price Forecasting",
      "title_zh": "HAELT：面向高频股价预测的混合注意力集成学习 Transformer 框架",
      "authors": [
        "Thanh Dan Bui"
      ],
      "abstract": "High-frequency stock price prediction is challenging due to non-stationarity, noise, and volatility. To tackle these issues, we propose the Hybrid Attentive Ensemble Learning Transformer (HAELT), a deep learning framework combining a ResNet-based noise-mitigation module, temporal self-attention for dynamic focus on relevant history, and a hybrid LSTM-Transformer core that captures both local and long-range dependencies. These components are adaptively ensembled based on recent performance. Evaluated on hourly Apple Inc. (AAPL) data from Jan 2024 to May 2025, HAELT achieves the highest F1-Score on the test set, effectively identifying both upward and downward price movements. This demonstrates HAELT's potential for robust, practical financial forecasting and algorithmic trading.",
      "tldr_zh": "该研究针对高频股票价格预测中存在的非平稳性、噪声和波动性挑战，提出了名为 HAELT（Hybrid Attentive Ensemble Learning Transformer）的深度学习框架。该框架集成了一个基于 ResNet 的噪声缓解模块、一个用于动态关注历史相关性的 temporal self-attention 机制，以及一个能够同时捕捉局部和长程依赖关系的混合 LSTM-Transformer 核心。这些组件根据近期预测表现进行自适应集成，有效提升了系统的预测性能。在 2024 年 1 月至 2025 年 5 月期间 Apple Inc. (AAPL) 的小时级数据评估中，HAELT 在测试集上取得了最高的 F1-Score，展现了其精准识别价格涨跌趋势的能力。该成果证明了 HAELT 在稳健的金融预测和算法交易领域具有显著的实用潜力和应用价值。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.13981v1",
      "published_date": "2025-06-09 13:40:18 UTC",
      "updated_date": "2025-06-09 13:40:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:17:51.838977+00:00"
    },
    {
      "arxiv_id": "2506.07759v1",
      "title": "REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models",
      "title_zh": "REMoH：基于大语言模型的多目标启发式算法反思式演化方法",
      "authors": [
        "Diego Forniés-Tabuenca",
        "Alejandro Uribe",
        "Urtzi Otamendi",
        "Arkaitz Artetxe",
        "Juan Carlos Rivera",
        "Oier Lopez de Lacalle"
      ],
      "abstract": "Multi-objective optimization is fundamental in complex decision-making tasks. Traditional algorithms, while effective, often demand extensive problem-specific modeling and struggle to adapt to nonlinear structures. Recent advances in Large Language Models (LLMs) offer enhanced explainability, adaptability, and reasoning. This work proposes Reflective Evolution of Multi-objective Heuristics (REMoH), a novel framework integrating NSGA-II with LLM-based heuristic generation. A key innovation is a reflection mechanism that uses clustering and search-space reflection to guide the creation of diverse, high-quality heuristics, improving convergence and maintaining solution diversity. The approach is evaluated on the Flexible Job Shop Scheduling Problem (FJSSP) in-depth benchmarking against state-of-the-art methods using three instance datasets: Dauzere, Barnes, and Brandimarte. Results demonstrate that REMoH achieves competitive results compared to state-of-the-art approaches with reduced modeling effort and enhanced adaptability. These findings underscore the potential of LLMs to augment traditional optimization, offering greater flexibility, interpretability, and robustness in multi-objective scenarios.",
      "tldr_zh": "该研究提出了 REMoH (Reflective Evolution of Multi-objective Heuristics)，这是一个将 NSGA-II 算法与大语言模型 (LLMs) 启发式生成相结合的新型框架，旨在解决多目标优化任务中传统算法过度依赖特定问题建模且难以适应非线性结构的问题。其核心创新在于引入了一种反思机制 (reflection mechanism)，通过聚类 (clustering) 和搜索空间反思来指导生成多样化且高质量的启发式规则，从而在提升算法收敛性的同时维持解的多样性。研究人员在柔性作业车间调度问题 (FJSSP) 上进行了深入评估，并使用了 Dauzere、Barnes 和 Brandimarte 等数据集进行基准测试。实验结果表明，REMoH 在显著减少建模工作量的同时，取得了与当前先进方法 (state-of-the-art) 相当的竞争性结果，展现了极强的适应性。这项研究证明了 LLMs 在增强传统优化算法方面的巨大潜力，为复杂多目标场景提供了更高的灵活性、可解释性 (interpretability) 和鲁棒性。",
      "categories": [
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 5 tables, 7 figures and 4 appendixes. Pre-print submitted to IEEE Transactions on Evolutionary Computation",
      "pdf_url": "https://arxiv.org/pdf/2506.07759v1",
      "published_date": "2025-06-09 13:38:28 UTC",
      "updated_date": "2025-06-09 13:38:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:18:12.934804+00:00"
    },
    {
      "arxiv_id": "2506.07756v2",
      "title": "Agent Semantics, Semantic Spacetime, and Graphical Reasoning",
      "title_zh": "智能体语义、语义时空与图推理",
      "authors": [
        "Mark Burgess"
      ],
      "abstract": "Some formal aspects of the Semantic Spacetime graph model are presented, with reference to its use for directed knowledge representations and process modelling. A finite $γ(3,4)$ representation is defined to form a closed set of operations that can scale to any degree of semantic complexity. The Semantic Spacetime postulates bring predictability with minimal constraints to pathways in graphs. The ubiquitous appearance of absorbing states in any partial graph means that a graph process leaks information. The issue is closely associated with the issue of division by zero, which signals a loss of closure and the need for manual injection of remedial information. The Semantic Spacetime model (and its Promise Theory) origins help to clarify how such absorbing states are associated with boundary information where intentionality can enter.",
      "tldr_zh": "该研究探讨了语义时空(Semantic Spacetime)图模型的形式化特征，旨在优化其在有向知识表示(directed knowledge representations)和过程建模(process modelling)中的应用。作者定义了一种有限的 $\\gamma(3,4)$ 表示法，形成了一套可扩展至任意语义复杂度的闭合操作集。通过语义时空(Semantic Spacetime)公理，该模型在极小约束下实现了图路径的可预测性。研究进一步揭示了部分图中普遍存在的吸收态(absorbing states)会导致信息泄露，这一现象与除以零问题类似，暗示了闭合性的丧失及对人工干预的需求。结合承诺理论(Promise Theory)的起源，该模型有效阐释了吸收态(absorbing states)如何与意向性(intentionality)介入的边界信息产生关联。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Some typos corrected",
      "pdf_url": "https://arxiv.org/pdf/2506.07756v2",
      "published_date": "2025-06-09 13:37:47 UTC",
      "updated_date": "2025-06-13 14:51:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:18:26.786140+00:00"
    },
    {
      "arxiv_id": "2506.07754v1",
      "title": "Comparing Credit Risk Estimates in the Gen-AI Era",
      "title_zh": "生成式人工智能时代的信用风险评估比较",
      "authors": [
        "Nicola Lavecchia",
        "Sid Fadanelli",
        "Federico Ricciuti",
        "Gennaro Aloe",
        "Enrico Bagli",
        "Pietro Giuffrida",
        "Daniele Vergari"
      ],
      "abstract": "Generative AI technologies have demonstrated significant potential across diverse applications. This study provides a comparative analysis of credit score modeling techniques, contrasting traditional approaches with those leveraging generative AI. Our findings reveal that current generative AI models fall short of matching the performance of traditional methods, regardless of the integration strategy employed. These results highlight the limitations in the current capabilities of generative AI for credit risk scoring, emphasizing the need for further research and development before the possibility of applying generative AI for this specific task, or equivalent ones.",
      "tldr_zh": "该研究对比了信用风险评估(credit risk estimates)领域中传统建模方法与生成式人工智能(Generative AI)技术的性能表现。研究者通过对比分析，探讨了在不同的整合策略下，传统方法与 Generative AI 模型在信用评分(credit score modeling)中的有效性。实验结果表明，无论采用何种整合策略，目前的 Generative AI 模型在性能上均未能达到传统方法的水平。这一发现揭示了 Generative AI 在当前信用风险评分(credit risk scoring)任务中的局限性。研究强调，在将 Generative AI 正式应用于信用风险评估或类似特定任务之前，仍需进行更深入的研究与开发工作。该论文为金融领域在 Gen-AI 时代下的模型选择与技术应用边界提供了重要的实证参考。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07754v1",
      "published_date": "2025-06-09 13:37:04 UTC",
      "updated_date": "2025-06-09 13:37:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:18:10.923779+00:00"
    },
    {
      "arxiv_id": "2506.07751v3",
      "title": "AbstRaL: Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking",
      "title_zh": "AbstRaL：通过强化抽象思维增强大语言模型的推理能力",
      "authors": [
        "Silin Gao",
        "Antoine Bosselut",
        "Samy Bengio",
        "Emmanuel Abbe"
      ],
      "abstract": "Recent studies have shown that large language models (LLMs), especially smaller ones, often lack robustness in grade school math (GSM) reasoning. In particular, they tend to experience performance drops when faced with distribution shifts, such as changes to numerical or nominal variables, or insertions of distracting clauses. A possible strategy to address this involves generating synthetic data to further \"instantiate\" reasoning problems on potential variations. In this work, we instead focuses on the strategy of \"abstracting\" reasoning problems. This not only helps counteract distribution shifts but also facilitates the connection to symbolic tools for deriving solutions. Focusing on GSM, we find that this abstraction process is better acquired through reinforcement learning (RL) than just supervised fine-tuning, which often fails to produce faithful abstractions. Our method, AbstRaL -- which promotes abstract reasoning in LLMs using RL on granular abstraction data -- significantly mitigates performance degradation on recent GSM perturbation benchmarks. Besides, improving GSM robustness via AbstRaL is shown to also implicitly benefit LLMs' capabilities on OOD mathematical and general reasoning tasks, indicating that abstract thinking broadly enables better generalizability.",
      "tldr_zh": "该研究针对大语言模型（LLMs）在处理小学数学（GSM）推理时，面对数值变化或干扰性内容等分布偏移表现出鲁棒性不足的问题，提出了AbstRaL框架。不同于传统的合成数据实例化方法，AbstRaL侧重于提升模型的“抽象化”（abstracting）推理能力，以应对分布偏移并促进与符号工具的结合。研究发现，通过在细粒度抽象数据上使用强化学习（RL）而非单纯的监督微调（SFT），模型能够更忠实地执行抽象化过程并习得抽象推理。实验结果证明，AbstRaL显著缓解了模型在GSM扰动基准测试中的性能退化。此外，这种抽象思维的增强还隐式地提升了模型在分布外（OOD）数学及通用推理任务上的表现，证明了抽象推理在提高模型泛化能力方面的广泛有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SC"
      ],
      "primary_category": "cs.CL",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2506.07751v3",
      "published_date": "2025-06-09 13:34:50 UTC",
      "updated_date": "2025-11-24 14:29:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:18:43.782779+00:00"
    },
    {
      "arxiv_id": "2506.07744v3",
      "title": "Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning",
      "title_zh": "面向离线分层强化学习的图辅助拼接",
      "authors": [
        "Seungho Baek",
        "Taegeon Park",
        "Jongchan Park",
        "Seungjun Oh",
        "Yusung Kim"
      ],
      "abstract": "Existing offline hierarchical reinforcement learning methods rely on high-level policy learning to generate subgoal sequences. However, their efficiency degrades as task horizons increase, and they lack effective strategies for stitching useful state transitions across different trajectories. We propose Graph-Assisted Stitching (GAS), a novel framework that formulates subgoal selection as a graph search problem rather than learning an explicit high-level policy. By embedding states into a Temporal Distance Representation (TDR) space, GAS clusters semantically similar states from different trajectories into unified graph nodes, enabling efficient transition stitching. A shortest-path algorithm is then applied to select subgoal sequences within the graph, while a low-level policy learns to reach the subgoals. To improve graph quality, we introduce the Temporal Efficiency (TE) metric, which filters out noisy or inefficient transition states, significantly enhancing task performance. GAS outperforms prior offline HRL methods across locomotion, navigation, and manipulation tasks. Notably, in the most stitching-critical task, it achieves a score of 88.3, dramatically surpassing the previous state-of-the-art score of 1.0. Our source code is available at: https://github.com/qortmdgh4141/GAS.",
      "tldr_zh": "该研究提出了Graph-Assisted Stitching (GAS)框架，旨在解决离线层次强化学习(Offline Hierarchical Reinforcement Learning)在长时程任务中效率下降以及跨轨迹状态转换拼接(stitching)能力不足的问题。不同于传统的显式高级策略学习，GAS将子目标选择建模为一个图搜索(graph search)问题，通过将状态嵌入时间距离表示(Temporal Distance Representation, TDR)空间，将语义相似的状态聚类为统一的图节点，从而实现高效的转换拼接。系统利用最短路径算法在图中选择子目标序列，并由底层策略负责执行。为了提升图的质量，研究引入了时间效率(Temporal Efficiency, TE)指标来过滤噪声或低效的转换状态。实验结果表明，GAS在运动、导航和操作任务中均优于现有的离线HRL方法。特别是在对拼接要求极高的任务中，GAS取得了88.3的高分，远超此前1.0的最佳水平。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.07744v3",
      "published_date": "2025-06-09 13:26:23 UTC",
      "updated_date": "2025-07-07 14:23:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:18:21.052295+00:00"
    },
    {
      "arxiv_id": "2506.07739v3",
      "title": "ArchiLense: A Framework for Quantitative Analysis of Architectural Styles Based on Vision Large Language Models",
      "title_zh": "ArchiLense：基于视觉大语言模型的建筑风格定量分析框架",
      "authors": [
        "Jing Zhong",
        "Jun Yin",
        "Peilin Li",
        "Pengyu Zeng",
        "Miao Zang",
        "Ran Luo",
        "Shuai Lu"
      ],
      "abstract": "Architectural cultures across regions are characterized by stylistic diversity, shaped by historical, social, and technological contexts in addition to geograph-ical conditions. Understanding architectural styles requires the ability to describe and analyze the stylistic features of different architects from various regions through visual observations of architectural imagery. However, traditional studies of architectural culture have largely relied on subjective expert interpretations and historical literature reviews, often suffering from regional biases and limited ex-planatory scope. To address these challenges, this study proposes three core contributions: (1) We construct a professional architectural style dataset named ArchDiffBench, which comprises 1,765 high-quality architectural images and their corresponding style annotations, collected from different regions and historical periods. (2) We propose ArchiLense, an analytical framework grounded in Vision-Language Models and constructed using the ArchDiffBench dataset. By integrating ad-vanced computer vision techniques, deep learning, and machine learning algo-rithms, ArchiLense enables automatic recognition, comparison, and precise classi-fication of architectural imagery, producing descriptive language outputs that ar-ticulate stylistic differences. (3) Extensive evaluations show that ArchiLense achieves strong performance in architectural style recognition, with a 92.4% con-sistency rate with expert annotations and 84.5% classification accuracy, effec-tively capturing stylistic distinctions across images. The proposed approach transcends the subjectivity inherent in traditional analyses and offers a more objective and accurate perspective for comparative studies of architectural culture.",
      "tldr_zh": "该研究提出 ArchiLense 框架，旨在利用 Vision-Language Models 对建筑风格进行定量分析，以解决传统建筑文化研究中主观性强和地域偏见等局限。研究首先构建了专业数据集 ArchDiffBench，包含 1,765 张涵盖不同地区和历史时期的高质量建筑图像及对应标注。ArchiLense 框架整合了计算机视觉、深度学习和机器学习算法，实现了建筑图像的自动识别、对比与精确分类，并能生成描述风格差异的语言输出。实验评估显示，该框架在风格识别上与专家标注的一致率达到 92.4%，分类准确率为 84.5%，展现了强大的风格特征捕捉能力。这一成果超越了传统定性分析的局限，为全球建筑文化的比较研究提供了一个客观且高效的数字化分析方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07739v3",
      "published_date": "2025-06-09 13:22:57 UTC",
      "updated_date": "2025-08-02 12:10:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:18:27.722565+00:00"
    },
    {
      "arxiv_id": "2506.07736v3",
      "title": "RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards",
      "title_zh": "RSafe：通过激励主动推理构建鲁棒且自适应的大语言模型安全防御机制",
      "authors": [
        "Jingnan Zheng",
        "Xiangtian Ji",
        "Yijun Lu",
        "Chenhang Cui",
        "Weixiang Zhao",
        "Gelei Deng",
        "Zhenkai Liang",
        "An Zhang",
        "Tat-Seng Chua"
      ],
      "abstract": "Large Language Models (LLMs) continue to exhibit vulnerabilities despite deliberate safety alignment efforts, posing significant risks to users and society. To safeguard against the risk of policy-violating content, system-level moderation via external guard models-designed to monitor LLM inputs and outputs and block potentially harmful content-has emerged as a prevalent mitigation strategy. Existing approaches of training guard models rely heavily on extensive human curated datasets and struggle with out-of-distribution threats, such as emerging harmful categories or jailbreak attacks. To address these limitations, we propose RSafe, an adaptive reasoning-based safeguard that conducts guided safety reasoning to provide robust protection within the scope of specified safety policies. RSafe operates in two stages: 1) guided reasoning, where it analyzes safety risks of input content through policy-guided step-by-step reasoning, and 2) reinforced alignment, where rule-based RL optimizes its reasoning paths to align with accurate safety prediction. This two-stage training paradigm enables RSafe to internalize safety principles to generalize safety protection capability over unseen or adversarial safety violation scenarios. During inference, RSafe accepts user-specified safety policies to provide enhanced safeguards tailored to specific safety requirements.",
      "tldr_zh": "该研究提出了 RSafe，一种基于自适应推理的 LLM 安全防护机制，旨在解决现有防御模型对人工标注数据集过度依赖以及在应对分布外(out-of-distribution)威胁和越狱攻击(jailbreak attacks)时表现不佳的问题。RSafe 采用了两阶段训练范式，首先通过引导式推理(guided reasoning)对输入内容进行策略引导下的逐步安全风险分析，随后利用基于规则的强化学习(rule-based RL)执行强化对齐(reinforced alignment)，以优化其推理路径并确保预测的准确性。这种模式使 RSafe 能够有效内化安全原则，从而在面对未见过的或对抗性安全违规场景时具备更强的泛化保护能力。在实际推理过程中，RSafe 还支持接收用户指定的安全策略，能够针对特定的安全要求提供定制化的增强防护。实验结果证明，RSafe 为构建鲁棒且具备适应性的 LLM 安全屏障提供了有效方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07736v3",
      "published_date": "2025-06-09 13:20:04 UTC",
      "updated_date": "2025-10-24 06:18:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:18:38.911155+00:00"
    },
    {
      "arxiv_id": "2506.09071v2",
      "title": "Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance",
      "title_zh": "SAAF (Segment Any Architectural Facades)：基于多模态语义引导的建筑立面、墙体与窗户自动分割模型",
      "authors": [
        "Peilin Li",
        "Jun Yin",
        "Jing Zhong",
        "Ran Luo",
        "Pengyu Zeng",
        "Miao Zhang"
      ],
      "abstract": "In the context of the digital development of architecture, the automatic segmentation of walls and windows is a key step in improving the efficiency of building information models and computer-aided design. This study proposes an automatic segmentation model for building facade walls and windows based on multimodal semantic guidance, called Segment Any Architectural Facades (SAAF). First, SAAF has a multimodal semantic collaborative feature extraction mechanism. By combining natural language processing technology, it can fuse the semantic information in text descriptions with image features, enhancing the semantic understanding of building facade components. Second, we developed an end-to-end training framework that enables the model to autonomously learn the mapping relationship from text descriptions to image segmentation, reducing the influence of manual intervention on the segmentation results and improving the automation and robustness of the model. Finally, we conducted extensive experiments on multiple facade datasets. The segmentation results of SAAF outperformed existing methods in the mIoU metric, indicating that the SAAF model can maintain high-precision segmentation ability when faced with diverse datasets. Our model has made certain progress in improving the accuracy and generalization ability of the wall and window segmentation task. It is expected to provide a reference for the development of architectural computer vision technology and also explore new ideas and technical paths for the application of multimodal learning in the architectural field.",
      "tldr_zh": "该研究提出了 SAAF (Segment Any Architectural Facades)，一种基于多模态语义引导的建筑立面、墙体和窗户自动分割模型，旨在提升建筑信息模型和计算机辅助设计的效率。SAAF 引入了多模态语义协作特征提取机制，通过结合自然语言处理 (NLP) 技术将文本描述的语义信息与图像特征融合，显著增强了模型对建筑立面组件的语义理解。该研究还开发了一个端到端的训练框架，使模型能够自主学习从文本到图像分割的映射关系，有效减少了人工干预并提升了自动化程度。实验结果显示，SAAF 在多个数据集上的 mIoU 指标均优于现有方法，证明了其在处理多样化数据时的高精度分割能力。该模型在提升分割准确性和泛化能力方面取得了显著进展，为建筑计算机视觉技术的发展提供了重要参考，并探索了多模态学习在建筑领域应用的新路径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09071v2",
      "published_date": "2025-06-09 13:16:46 UTC",
      "updated_date": "2025-08-02 12:21:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:18:47.497779+00:00"
    },
    {
      "arxiv_id": "2506.07731v1",
      "title": "NeurIPS 2025 E2LM Competition : Early Training Evaluation of Language Models",
      "title_zh": "NeurIPS 2025 E2LM 竞赛：语言模型早期训练评估",
      "authors": [
        "Mouadh Yagoubi",
        "Yasser Dahou",
        "Billel Mokeddem",
        "Younes Belkada",
        "Phuc H. Le-Khac",
        "Basma El Amel Boussaha",
        "Reda Alami",
        "Jingwei Zuo",
        "Damiano Marsili",
        "Mugariya Farooq",
        "Mounia Lalmas",
        "Georgia Gkioxari",
        "Patrick Gallinari",
        "Philip Torr",
        "Hakim Hacid"
      ],
      "abstract": "Existing benchmarks have proven effective for assessing the performance of fully trained large language models. However, we find striking differences in the early training stages of small models, where benchmarks often fail to provide meaningful or discriminative signals. To explore how these differences arise, this competition tackles the challenge of designing scientific knowledge evaluation tasks specifically tailored for measuring early training progress of language models. Participants are invited to develop novel evaluation methodologies or adapt existing benchmarks to better capture performance differences among language models. To support this effort, we provide three pre-trained small models (0.5B, 1B, and 3B parameters), along with intermediate checkpoints sampled during training up to 200B tokens. All experiments and development work can be run on widely available free cloud-based GPU platforms, making participation accessible to researchers with limited computational resources. Submissions will be evaluated based on three criteria: the quality of the performance signal they produce, the consistency of model rankings at 1 trillion tokens of training, and their relevance to the scientific knowledge domain. By promoting the design of tailored evaluation strategies for early training, this competition aims to attract a broad range of participants from various disciplines, including those who may not be machine learning experts or have access to dedicated GPU resources. Ultimately, this initiative seeks to make foundational LLM research more systematic and benchmark-informed from the earliest phases of model development.",
      "tldr_zh": "该研究介绍了 NeurIPS 2025 E2LM Competition，旨在解决现有基准测试在评估语言模型训练早期阶段时缺乏区分度信号的问题。竞赛核心挑战是设计专门衡量模型早期进展的科学知识评估任务，邀请参与者开发新方法或优化现有基准以精准捕捉性能差异。为降低研究门槛，组织方提供了参数量为 0.5B、1B 和 3B 的预训练小模型，以及训练至 200B tokens 的中间检查点(checkpoints)，支持在免费云端 GPU 平台运行。提交方案将依据性能信号质量、在 1 trillion tokens 规模下模型排名的一致性以及与科学知识领域的相关性进行综合评价。该计划旨在通过早期训练评估策略的创新，吸引跨学科研究者参与，使基础大语言模型(LLMs)研究从开发的最早阶段起便更加系统化且具备基准参考价值。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07731v1",
      "published_date": "2025-06-09 13:15:50 UTC",
      "updated_date": "2025-06-09 13:15:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:18:41.491072+00:00"
    },
    {
      "arxiv_id": "2506.07725v1",
      "title": "ETA: Efficiency through Thinking Ahead, A Dual Approach to Self-Driving with Large Models",
      "title_zh": "ETA：基于前瞻思维提升效率的自动驾驶大模型双系统方法",
      "authors": [
        "Shadi Hamdan",
        "Chonghao Sima",
        "Zetong Yang",
        "Hongyang Li",
        "Fatma Güney"
      ],
      "abstract": "How can we benefit from large models without sacrificing inference speed, a common dilemma in self-driving systems? A prevalent solution is a dual-system architecture, employing a small model for rapid, reactive decisions and a larger model for slower but more informative analyses. Existing dual-system designs often implement parallel architectures where inference is either directly conducted using the large model at each current frame or retrieved from previously stored inference results. However, these works still struggle to enable large models for a timely response to every online frame. Our key insight is to shift intensive computations of the current frame to previous time steps and perform a batch inference of multiple time steps to make large models respond promptly to each time step. To achieve the shifting, we introduce Efficiency through Thinking Ahead (ETA), an asynchronous system designed to: (1) propagate informative features from the past to the current frame using future predictions from the large model, (2) extract current frame features using a small model for real-time responsiveness, and (3) integrate these dual features via an action mask mechanism that emphasizes action-critical image regions. Evaluated on the Bench2Drive CARLA Leaderboard-v2 benchmark, ETA advances state-of-the-art performance by 8% with a driving score of 69.53 while maintaining a near-real-time inference speed at 50 ms.",
      "tldr_zh": "该研究针对自动驾驶系统中大模型推理速度慢与实时性要求之间的矛盾，提出了名为 ETA (Efficiency through Thinking Ahead) 的异步双系统框架。现有的双系统架构往往难以保证大模型对每一帧的及时响应，而 ETA 的核心创新在于将当前帧的密集计算转移至先前时间步，并利用批量推理 (batch inference) 提前预判。该系统通过大模型的未来预测将历史特征传播至当前帧，同时结合小模型提取实时特征，并利用动作掩码机制 (action mask mechanism) 聚焦关键图像区域以整合双重特征。在 Bench2Drive CARLA Leaderboard-v2 基准测试中，ETA 在保持 50 毫秒近实时推理速度的同时，以 69.53 的驾驶得分将现有最优性能提升了 8%。该方法为在大规模模型驱动的自动驾驶系统中实现效率与性能的平衡提供了有效方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV 2025 submission. For code, see https://github.com/opendrivelab/ETA",
      "pdf_url": "https://arxiv.org/pdf/2506.07725v1",
      "published_date": "2025-06-09 13:11:02 UTC",
      "updated_date": "2025-06-09 13:11:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:19:10.637433+00:00"
    },
    {
      "arxiv_id": "2506.07713v2",
      "title": "Consistent Video Editing as Flow-Driven Image-to-Video Generation",
      "title_zh": "将连贯视频编辑视作流驱动的图像到视频生成",
      "authors": [
        "Ge Wang",
        "Songlin Fan",
        "Hangxu Liu",
        "Quanjian Song",
        "Hewei Wang",
        "Jinfeng Xu"
      ],
      "abstract": "With the prosper of video diffusion models, down-stream applications like video editing have been significantly promoted without consuming much computational cost. One particular challenge in this task lies at the motion transfer process from the source video to the edited one, where it requires the consideration of the shape deformation in between, meanwhile maintaining the temporal consistency in the generated video sequence. However, existing methods fail to model complicated motion patterns for video editing, and are fundamentally limited to object replacement, where tasks with non-rigid object motions like multi-object and portrait editing are largely neglected. In this paper, we observe that optical flows offer a promising alternative in complex motion modeling, and present FlowV2V to re-investigate video editing as a task of flow-driven Image-to-Video (I2V) generation. Specifically, FlowV2V decomposes the entire pipeline into first-frame editing and conditional I2V generation, and simulates pseudo flow sequence that aligns with the deformed shape, thus ensuring the consistency during editing. Experimental results on DAVIS-EDIT with improvements of 13.67% and 50.66% on DOVER and warping error illustrate the superior temporal consistency and sample quality of FlowV2V compared to existing state-of-the-art ones. Furthermore, we conduct comprehensive ablation studies to analyze the internal functionalities of the first-frame paradigm and flow alignment in the proposed method.",
      "tldr_zh": "该研究针对视频编辑中运动迁移、形状变形以及时间一致性(temporal consistency)的维护等挑战，指出了现有方法在处理非刚性物体运动（如多目标和肖像编辑）时的局限性。为此，作者提出了FlowV2V框架，将视频编辑任务重新定义为由流驱动(flow-driven)的图像到视频(I2V)生成过程。该框架将整体流程分解为首帧编辑(first-frame editing)和条件式I2V生成，并通过模拟与变形后的形状相一致的伪流序列(pseudo flow sequence)来确保编辑的一致性。实验结果显示，FlowV2V在DAVIS-EDIT数据集上的DOVER指标和翘曲误差(warping error)分别提升了13.67%和50.66%，展现了优于现有最先进方法的样本质量和时序稳定性。通过详尽的消融实验，该研究进一步证实了首帧范式和流对齐(flow alignment)机制在复杂运动建模中的关键作用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.07713v2",
      "published_date": "2025-06-09 12:57:30 UTC",
      "updated_date": "2025-06-13 09:10:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:19:09.223501+00:00"
    },
    {
      "arxiv_id": "2506.07698v1",
      "title": "NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D Generation",
      "title_zh": "NOVA3D：面向单图生成三维的法向对齐视频扩散模型",
      "authors": [
        "Yuxiao Yang",
        "Peihao Li",
        "Yuhong Zhang",
        "Junzhe Lu",
        "Xianglong He",
        "Minghan Qin",
        "Weitao Wang",
        "Haoqian Wang"
      ],
      "abstract": "3D AI-generated content (AIGC) has made it increasingly accessible for anyone to become a 3D content creator. While recent methods leverage Score Distillation Sampling to distill 3D objects from pretrained image diffusion models, they often suffer from inadequate 3D priors, leading to insufficient multi-view consistency. In this work, we introduce NOVA3D, an innovative single-image-to-3D generation framework. Our key insight lies in leveraging strong 3D priors from a pretrained video diffusion model and integrating geometric information during multi-view video fine-tuning. To facilitate information exchange between color and geometric domains, we propose the Geometry-Temporal Alignment (GTA) attention mechanism, thereby improving generalization and multi-view consistency. Moreover, we introduce the de-conflict geometry fusion algorithm, which improves texture fidelity by addressing multi-view inaccuracies and resolving discrepancies in pose alignment. Extensive experiments validate the superiority of NOVA3D over existing baselines.",
      "tldr_zh": "该研究针对单图像生成3D物体中存在的3D先验不足及多视图一致性差的问题，提出了名为NOVA3D的创新生成框架。该框架的核心在于有效利用预训练视频扩散模型的强3D先验，并在多视图视频微调过程中深度集成几何信息。研究人员设计了Geometry-Temporal Alignment (GTA) 注意力机制，通过促进颜色与几何域之间的信息交换，显著增强了模型的泛化能力和多视图一致性。此外，引入的de-conflict geometry fusion算法通过解决多视图预测中的不准确性及位姿对齐差异，有效提升了生成物体的纹理保真度。大量实验结果表明，NOVA3D在生成质量上明显优于现有的基线模型，为高质量3D AIGC领域提供了更可靠的技术方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 7 figures, accepted by ICME 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.07698v1",
      "published_date": "2025-06-09 12:37:46 UTC",
      "updated_date": "2025-06-09 12:37:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:19:16.101522+00:00"
    },
    {
      "arxiv_id": "2506.10022v1",
      "title": "LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges",
      "title_zh": "LLMs 深陷重围：恶意软件请求与越狱挑战",
      "authors": [
        "Haoyang Li",
        "Huan Gao",
        "Zhiyuan Zhao",
        "Zhiyu Lin",
        "Junyu Gao",
        "Xuelong Li"
      ],
      "abstract": "The widespread adoption of Large Language Models (LLMs) has heightened concerns about their security, particularly their vulnerability to jailbreak attacks that leverage crafted prompts to generate malicious outputs. While prior research has been conducted on general security capabilities of LLMs, their specific susceptibility to jailbreak attacks in code generation remains largely unexplored. To fill this gap, we propose MalwareBench, a benchmark dataset containing 3,520 jailbreaking prompts for malicious code-generation, designed to evaluate LLM robustness against such threats. MalwareBench is based on 320 manually crafted malicious code generation requirements, covering 11 jailbreak methods and 29 code functionality categories. Experiments show that mainstream LLMs exhibit limited ability to reject malicious code-generation requirements, and the combination of multiple jailbreak methods further reduces the model's security capabilities: specifically, the average rejection rate for malicious content is 60.93%, dropping to 39.92% when combined with jailbreak attack algorithms. Our work highlights that the code security capabilities of LLMs still pose significant challenges.",
      "tldr_zh": "该研究探讨了大型语言模型(LLMs)在生成恶意代码时面临的越狱攻击(jailbreak attacks)漏洞，指出目前在该领域的安全性研究尚不充分。为此，作者提出了MalwareBench，这是一个包含3,520个越狱提示词(jailbreaking prompts)的基准数据集，旨在评估LLMs对抗恶意代码生成威胁的稳健性。该数据集基于320项人工构建的恶意代码生成需求，涵盖了11种越狱方法和29种代码功能类别。实验结果表明，主流LLMs在拒绝恶意代码生成请求方面表现欠佳，平均拒绝率仅为60.93%。当多种越狱方法结合使用时，模型的安全性能进一步削弱，拒绝率显著下降至39.92%。此项工作凸显了LLMs在代码安全领域面临的严峻挑战，并为后续提升模型的防御能力提供了重要的评估基准。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted as ACL 2025 main conference",
      "pdf_url": "https://arxiv.org/pdf/2506.10022v1",
      "published_date": "2025-06-09 12:02:39 UTC",
      "updated_date": "2025-06-09 12:02:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:20:09.745656+00:00"
    },
    {
      "arxiv_id": "2506.07675v3",
      "title": "QUITE: A Query Rewrite System Beyond Rules with LLM Agents",
      "title_zh": "QUITE：基于大语言模型智能体的超越规则查询重写系统",
      "authors": [
        "Yuyang Song",
        "Hanxu Yan",
        "Jiale Lao",
        "Yibo Wang",
        "Yufei Li",
        "Yuanchun Zhou",
        "Jianguo Wang",
        "Mingjie Tang"
      ],
      "abstract": "Query rewrite transforms SQL queries into semantically equivalent forms that run more efficiently. Existing approaches mainly rely on predefined rewrite rules, but they handle a limited subset of queries and can cause performance regressions. This limitation stems from three challenges of rule-based query rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite rules do not generalize to new query patterns, and (3) some rewrite techniques cannot be expressed as fixed rules. Motivated by the fact that human experts exhibit significantly better rewrite ability but suffer from scalability, and Large Language Models (LLMs) have demonstrated nearly human-level semantic and reasoning abilities, we propose a new approach of using LLMs to rewrite SQL queries beyond rules. Due to the hallucination problems in LLMs, directly applying LLMs often leads to nonequivalent and suboptimal queries. To address this issue, we propose QUITE (query rewrite), a training-free and feedback-aware system based on LLM agents that rewrites SQL queries into semantically equivalent forms with significantly better performance, covering a broader range of query patterns and rewrite strategies compared to rule-based methods. Firstly, we design a multi-agent framework controlled by a finite state machine (FSM) to equip LLMs with the ability to use external tools and enhance the rewrite process with real-time database feedback. Secondly, we develop a rewrite middleware to enhance the ability of LLMs to generate optimized query equivalents. Finally, we employ a novel hint injection technique to improve execution plans for rewritten queries. Extensive experiments show that QUITE reduces query execution time by up to 35.8% over state-of-the-art approaches and produces 24.1% more rewrites than prior methods, covering query cases that earlier systems did not handle.",
      "tldr_zh": "该研究提出了QUITE，一个基于LLM Agents的无训练、具备反馈意识的SQL查询重写系统，旨在解决传统规则驱动方法在规则发现难、泛化性差以及无法表达复杂重写策略等方面的局限性。该系统设计了一个受有限状态机(Finite State Machine, FSM)控制的多智能体框架，赋予大语言模型使用外部工具的能力，并通过实时数据库反馈来持续增强重写过程。此外，研究人员开发了重写中间件以提升模型生成等价SQL的能力，并结合创新的提示注入(Hint Injection)技术进一步优化了查询执行计划。实验结果表明，QUITE相比现有最优方法能产生多出24.1%的重写结果，并将查询执行时间降低了高达35.8%，有效覆盖了以往系统无法处理的复杂查询场景。这一研究证明了利用LLM超越固定规则进行高效查询优化的可行性与优越性。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07675v3",
      "published_date": "2025-06-09 11:51:27 UTC",
      "updated_date": "2026-01-02 16:51:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:19:23.759635+00:00"
    },
    {
      "arxiv_id": "2506.07672v1",
      "title": "MCPWorld: A Unified Benchmarking Testbed for API, GUI, and Hybrid Computer Use Agents",
      "title_zh": "MCPWorld：面向 API、GUI 及混合型计算机使用智能体的统一评测基准",
      "authors": [
        "Yunhe Yan",
        "Shihe Wang",
        "Jiajun Du",
        "Yexuan Yang",
        "Yuxuan Shan",
        "Qichen Qiu",
        "Xianqing Jia",
        "Xinge Wang",
        "Xin Yuan",
        "Xu Han",
        "Mao Qin",
        "Yinxiao Chen",
        "Chen Peng",
        "Shangguang Wang",
        "Mengwei Xu"
      ],
      "abstract": "(M)LLM-powered computer use agents (CUA) are emerging as a transformative technique to automate human-computer interaction. However, existing CUA benchmarks predominantly target GUI agents, whose evaluation methods are susceptible to UI changes and ignore function interactions exposed by application APIs, e.g., Model Context Protocol (MCP). To this end, we propose MCPWorld, the first automatic CUA testbed for API, GUI, and API-GUI hybrid agents. A key principle of MCPWorld is the use of \"white-box apps\", i.e., those with source code availability and can be revised/re-compiled as needed (e.g., adding MCP support), with two notable advantages:\n  (1) It greatly broadens the design space of CUA, such as what and how the app features to be exposed/extracted as CUA-callable APIs.\n  (2) It allows MCPWorld to programmatically verify task completion by directly monitoring application behavior through techniques like dynamic code instrumentation, offering robust, accurate CUA evaluation decoupled from specific agent implementations or UI states.\n  Currently, MCPWorld includes 201 well curated and annotated user tasks, covering diversified use cases and difficulty levels. MCPWorld is also fully containerized with GPU acceleration support for flexible adoption on different OS/hardware environments. Our preliminary experiments, using a representative LLM-powered CUA framework, achieve 75.12% task completion accuracy, simultaneously providing initial evidence on the practical effectiveness of agent automation leveraging MCP. Overall, we anticipate MCPWorld to facilitate and standardize the benchmarking of next-generation computer use agents that can leverage rich external tools. Our code and dataset are publicly available at https://github.com/SAAgent/MCPWorld.",
      "tldr_zh": "该研究提出了MCPWorld，这是首个针对API、GUI以及API-GUI混合计算机使用智能体(CUA)的自动化测试平台，旨在解决现有基准测试过度依赖GUI且易受UI变化影响、忽视API交互等问题。该平台的核心原则是采用源代码可用的“白盒应用(white-box apps)”，这极大扩展了智能体可调用API的设计空间。通过动态代码插桩(dynamic code instrumentation)技术，MCPWorld能够直接监控应用行为以程序化地验证任务完成情况，从而实现与特定UI状态脱钩的鲁棒评估。目前平台包含201个经过精细标注的任务，涵盖了多样的使用场景和难度级别，并支持容器化部署与GPU加速。初步实验显示，代表性的LLM智能体框架在MCPWorld上达到了75.12%的任务完成准确率。该研究有效验证了利用Model Context Protocol (MCP)提升智能体自动化能力的潜力，为下一代计算机使用智能体提供了标准化的基准支撑。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07672v1",
      "published_date": "2025-06-09 11:50:33 UTC",
      "updated_date": "2025-06-09 11:50:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:19:42.776291+00:00"
    },
    {
      "arxiv_id": "2506.11117v1",
      "title": "ScIRGen: Synthesize Realistic and Large-Scale RAG Dataset for Scientific Research",
      "title_zh": "ScIRGen：面向科学研究的大规模真实 RAG 数据集生成",
      "authors": [
        "Junyong Lin",
        "Lu Dai",
        "Ruiqian Han",
        "Yijie Sui",
        "Ruilin Wang",
        "Xingliang Sun",
        "Qinglin Wu",
        "Min Feng",
        "Hao Liu",
        "Hui Xiong"
      ],
      "abstract": "Scientific researchers need intensive information about datasets to effectively evaluate and develop theories and methodologies. The information needs regarding datasets are implicitly embedded in particular research tasks, rather than explicitly expressed in search queries. However, existing scientific retrieval and question-answering (QA) datasets typically address straightforward questions, which do not align with the distribution of real-world research inquiries. To bridge this gap, we developed ScIRGen, a dataset generation framework for scientific QA \\& retrieval that more accurately reflects the information needs of professional science researchers, and uses it to create a large-scale scientific retrieval-augmented generation (RAG) dataset with realistic queries, datasets and papers. Technically, we designed a dataset-oriented information extraction method that leverages academic papers to augment the dataset representation. We then proposed a question generation framework by employing cognitive taxonomy to ensure the quality of synthesized questions. We also design a method to automatically filter synthetic answers based on the perplexity shift of LLMs, which is highly aligned with human judgment of answers' validity. Collectively, these methodologies culminated in the creation of the 61k QA dataset, ScIRGen-Geo. We benchmarked representative methods on the ScIRGen-Geo dataset for their question-answering and retrieval capabilities, finding out that current methods still suffer from reasoning from complex questions. This work advances the development of more sophisticated tools to support the intricate information needs of the scientific community.",
      "tldr_zh": "该研究提出了ScIRGen，这是一个旨在为科学研究合成真实且大规模检索增强生成(RAG)数据集的框架，解决了现有问答(QA)数据集与真实科研场景信息需求不匹配的问题。该框架采用面向数据集的信息提取方法，通过学术论文增强数据集的表征，并结合认知分类法(Cognitive Taxonomy)生成高质量的科学问题。为了确保数据的准确性，研究者利用大语言模型(LLMs)的困惑度偏移(Perplexity Shift)设计了自动过滤机制，该机制的有效性与人类判断高度一致。利用该框架，研究团队构建了包含6.1万条高质量问答对的ScIRGen-Geo数据集。实验结果显示，当前主流方法在处理该数据集中的复杂科学推理时仍表现不足。这一研究成果为开发支持科研社区复杂信息需求的先进工具提供了重要的数据支撑和方法论指导。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "KDD 2025 Accepted",
      "pdf_url": "https://arxiv.org/pdf/2506.11117v1",
      "published_date": "2025-06-09 11:47:13 UTC",
      "updated_date": "2025-06-09 11:47:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:19:40.201443+00:00"
    },
    {
      "arxiv_id": "2506.07671v1",
      "title": "GaRAGe: A Benchmark with Grounding Annotations for RAG Evaluation",
      "title_zh": "GaRAGe：具有依据标注的 RAG 评估基准",
      "authors": [
        "Ionut-Teodor Sorodoc",
        "Leonardo F. R. Ribeiro",
        "Rexhina Blloshmi",
        "Christopher Davis",
        "Adrià de Gispert"
      ],
      "abstract": "We present GaRAGe, a large RAG benchmark with human-curated long-form answers and annotations of each grounding passage, allowing a fine-grained evaluation of whether LLMs can identify relevant grounding when generating RAG answers. Our benchmark contains 2366 questions of diverse complexity, dynamism, and topics, and includes over 35K annotated passages retrieved from both private document sets and the Web, to reflect real-world RAG use cases. This makes it an ideal test bed to evaluate an LLM's ability to identify only the relevant information necessary to compose a response, or provide a deflective response when there is insufficient information. Evaluations of multiple state-of-the-art LLMs on GaRAGe show that the models tend to over-summarise rather than (a) ground their answers strictly on the annotated relevant passages (reaching at most a Relevance-Aware Factuality Score of 60%), or (b) deflect when no relevant grounding is available (reaching at most 31% true positive rate in deflections). The F1 in attribution to relevant sources is at most 58.9%, and we show that performance is particularly reduced when answering time-sensitive questions and when having to draw knowledge from sparser private grounding sources.",
      "tldr_zh": "该研究提出了GaRAGe，一个包含人工标注长篇答案和背景段落标注的大型RAG基准测试集，旨在对LLMs在生成回答时识别相关背景信息的能力进行细粒度评估。该基准包含2366个涵盖多样复杂性和话题的问题，并整合了超过3.5万条源自私有文档和Web的标注段落，以模拟真实的RAG应用场景。通过对多个SOTA模型进行评估，研究发现现有模型倾向于过度总结，在严格依据相关段落生成回答方面的表现欠佳，其Relevance-Aware Factuality Score最高仅为60%。此外，模型在缺乏相关依据时的拒绝回答（deflection）能力较弱，真实阳性率最高仅为31%，归属相关来源的F1分数最高为58.9%。实验结果进一步表明，模型在处理具有时效性的问题以及从稀疏的私有知识源中提取信息时，性能表现会显著下降。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 (Findings)",
      "pdf_url": "https://arxiv.org/pdf/2506.07671v1",
      "published_date": "2025-06-09 11:47:03 UTC",
      "updated_date": "2025-06-09 11:47:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:19:33.610701+00:00"
    },
    {
      "arxiv_id": "2506.07664v2",
      "title": "Synthesis by Design: Controlled Data Generation via Structural Guidance",
      "title_zh": "设计驱动的合成：基于结构化引导的可控数据生成",
      "authors": [
        "Lei Xu",
        "Sirui Chen",
        "Yuxuan Huang",
        "Chaochao Lu"
      ],
      "abstract": "Mathematical reasoning remains challenging for LLMs due to complex logic and the need for precise computation. Existing methods enhance LLM reasoning by synthesizing datasets through problem rephrasing, but face issues with generation quality and problem complexity. To address this, we propose to extract structural information with generated problem-solving code from mathematical reasoning and guide data generation with structured solutions. Applied to MATH and GSM8K, our approach produces 39K problems with labeled intermediate steps and a 6.1K-problem benchmark of higher difficulty. Results on our benchmark show that model performance declines as reasoning length increases. Additionally, we conducted fine-tuning experiments using the proposed training data on a range of LLMs, and the results validate the effectiveness of our dataset. We hope the proposed method and dataset will contribute to future research in enhancing LLM reasoning capabilities. Our code and data are available at https://github.com/OpenCausaLab/StructuralGeneration.",
      "tldr_zh": "该研究针对大语言模型(LLMs)在复杂逻辑和精准计算方面的数学推理挑战，提出了通过结构化引导(Structural Guidance)实现受控数据生成的方法。该方法核心在于从数学推理中提取带有生成代码的结构化信息，并利用结构化解决方案来引导数据的合成。应用该方法在MATH和GSM8K上生成了包含3.9万个带有中间步骤标注的问题数据集，以及一个包含6.1千个更高难度问题的基准测试集。实验结果表明，随着推理长度的增加，模型的性能会随之下降，而在多种LLMs上的微调实验结果充分验证了该数据集在增强模型推理能力方面的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07664v2",
      "published_date": "2025-06-09 11:38:23 UTC",
      "updated_date": "2025-06-10 21:19:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:19:48.254137+00:00"
    },
    {
      "arxiv_id": "2506.07652v1",
      "title": "FMaMIL: Frequency-Driven Mamba Multi-Instance Learning for Weakly Supervised Lesion Segmentation in Medical Images",
      "title_zh": "FMaMIL：用于医学图像弱监督病变分割的频率驱动 Mamba 多实例学习",
      "authors": [
        "Hangbei Cheng",
        "Xiaorong Dong",
        "Xueyu Liu",
        "Jianan Zhang",
        "Xuetao Ma",
        "Mingqiang Wei",
        "Liansheng Wang",
        "Junxin Chen",
        "Yongfei Wu"
      ],
      "abstract": "Accurate lesion segmentation in histopathology images is essential for diagnostic interpretation and quantitative analysis, yet it remains challenging due to the limited availability of costly pixel-level annotations. To address this, we propose FMaMIL, a novel two-stage framework for weakly supervised lesion segmentation based solely on image-level labels. In the first stage, a lightweight Mamba-based encoder is introduced to capture long-range dependencies across image patches under the MIL paradigm. To enhance spatial sensitivity and structural awareness, we design a learnable frequency-domain encoding module that supplements spatial-domain features with spectrum-based information. CAMs generated in this stage are used to guide segmentation training. In the second stage, we refine the initial pseudo labels via a CAM-guided soft-label supervision and a self-correction mechanism, enabling robust training even under label noise. Extensive experiments on both public and private histopathology datasets demonstrate that FMaMIL outperforms state-of-the-art weakly supervised methods without relying on pixel-level annotations, validating its effectiveness and potential for digital pathology applications.",
      "tldr_zh": "该研究提出了 FMaMIL，一种专为医疗图像中弱监督病灶分割 (Weakly Supervised Lesion Segmentation) 设计的频率驱动 Mamba 多实例学习 (Multi-Instance Learning, MIL) 框架，旨在解决组织病理学图像中像素级标注获取成本高昂的问题。在第一阶段，该框架引入了一个轻量级的基于 Mamba 的编码器以捕捉图像块之间的长程依赖关系，并结合可学习的频域编码模块 (Frequency-domain encoding module) 增强空间敏感性。通过生成的类激活图 (CAMs) 引导，第二阶段采用软标签监督 (Soft-label supervision) 和自纠正机制 (Self-correction mechanism) 来细化伪标签，确保在标签噪声下仍能进行稳健训练。实验结果表明，FMaMIL 在公共和私有数据集上的表现均优于现有的最先进弱监督方法，证明了其在数字病理学领域的应用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07652v1",
      "published_date": "2025-06-09 11:18:02 UTC",
      "updated_date": "2025-06-09 11:18:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:19:55.720147+00:00"
    },
    {
      "arxiv_id": "2506.07636v2",
      "title": "SWE-Dev: Building Software Engineering Agents with Training and Inference Scaling",
      "title_zh": "SWE-Dev：通过训练与推理缩放构建软件工程智能体",
      "authors": [
        "Haoran Wang",
        "Zhenyu Hou",
        "Yao Wei",
        "Jie Tang",
        "Yuxiao Dong"
      ],
      "abstract": "Large language models (LLMs) have advanced rapidly from conversational problem solving to addressing real-world tasks involving tool use, such as software engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex and Cursor, have offered end-to-end automation of the software development process. However, building effective SWE agents remains challenging due to the lack of high-quality training data and effective test cases. To address this issue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we develop a robust pipeline to synthesize test cases for patch evaluation. Second, we scale up agent trajectories to construct the training data for building SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the SWE-Dev models can achieve top performance among all open SWE agents. Specifically, the success rates of the SWE-Dev 7B and 32B parameter models reach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source models. All code, models, and datasets are publicly available at https://github.com/THUDM/SWE-Dev.",
      "tldr_zh": "该研究提出了 SWE-Dev，这是一种基于开源 LLMs 构建的软件工程 (SWE) 智能体，旨在解决当前该领域高质量训练数据和有效测试用例匮乏的挑战。研究团队首先开发了一套鲁棒的流水线 (pipeline) 用于合成补丁评估 (patch evaluation) 的测试用例 (test cases)，并通过大规模扩展智能体轨迹 (agent trajectories) 来构建训练数据。在 SWE-bench-Verified 基准测试中，SWE-Dev 模型展现了顶尖的性能，其中 7B 和 32B 参数版本的成功率分别达到 23.4% 和 36.6%，显著优于现有的先进开源模型。实验结果证明了通过训练与推理扩展构建高效自动化软件工程系统的有效性，目前该项目的所有代码、模型和数据集均已公开。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to Findings of ACL'25",
      "pdf_url": "https://arxiv.org/pdf/2506.07636v2",
      "published_date": "2025-06-09 11:03:16 UTC",
      "updated_date": "2025-06-23 01:00:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:20:29.425774+00:00"
    },
    {
      "arxiv_id": "2506.07621v1",
      "title": "LoRMA: Low-Rank Multiplicative Adaptation for LLMs",
      "title_zh": "LoRMA：大语言模型的低秩乘性自适应",
      "authors": [
        "Harsh Bihany",
        "Shubham Patel",
        "Ashutosh Modi"
      ],
      "abstract": "Large Language Models have shown remarkable capabilities in the NLP domain. Their effectiveness can mainly be attributed to their ability to adapt to an array of downstream tasks. However, generally, full fine-tuning is a computationally expensive job. To mitigate this, many techniques have been developed that prime efficiency, a prominent one being Low-Rank Adaptation (LoRA). However, LoRA and its variants employ re-parametrized additive updates. In this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which shifts the paradigm of additive updates to a richer space of matrix multiplicative transformations. We tackle challenges such as computational complexity and rank bottleneck of matrix multiplication by effectively re-ordering operations and introducing rank inflation strategies. We conduct extensive experiments to demonstrate the effectiveness of our approach in terms of various evaluation metrics.",
      "tldr_zh": "该研究提出了LoRMA (Low-Rank Multiplicative Adaptation)，这是一种旨在提高大语言模型 (LLMs) 适配效率的新型微调技术。针对传统的LoRA及其变体主要采用重参数化加法更新 (additive updates) 的局限性，LoRMA将微调范式转向了更具表达能力的矩阵乘法变换 (matrix multiplicative transformations) 空间。为了解决矩阵乘法涉及的计算复杂度和秩瓶颈 (rank bottleneck) 挑战，该方法通过引入操作重排序 (re-ordering operations) 和秩膨胀 (rank inflation) 策略确保了模型的训练效率。在多种任务上的实验结果表明，LoRMA在各项评估指标上均表现出色，证明了乘法更新在低秩适配领域相较于传统加法更新的优越性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ACL Findings 2025; 21 pages (9 main paper + 5 pages references + 7 pages appendix)",
      "pdf_url": "https://arxiv.org/pdf/2506.07621v1",
      "published_date": "2025-06-09 10:36:46 UTC",
      "updated_date": "2025-06-09 10:36:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:20:32.748104+00:00"
    },
    {
      "arxiv_id": "2506.07606v1",
      "title": "PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels",
      "title_zh": "PolitiSky24：带有用户立场标签的美国政治 Bluesky 数据集",
      "authors": [
        "Peyman Rostami",
        "Vahid Rahimzadeh",
        "Ali Adibi",
        "Azadeh Shakery"
      ],
      "abstract": "Stance detection identifies the viewpoint expressed in text toward a specific target, such as a political figure. While previous datasets have focused primarily on tweet-level stances from established platforms, user-level stance resources, especially on emerging platforms like Bluesky remain scarce. User-level stance detection provides a more holistic view by considering a user's complete posting history rather than isolated posts. We present the first stance detection dataset for the 2024 U.S. presidential election, collected from Bluesky and centered on Kamala Harris and Donald Trump. The dataset comprises 16,044 user-target stance pairs enriched with engagement metadata, interaction graphs, and user posting histories. PolitiSky24 was created using a carefully evaluated pipeline combining advanced information retrieval and large language models, which generates stance labels with supporting rationales and text spans for transparency. The labeling approach achieves 81\\% accuracy with scalable LLMs. This resource addresses gaps in political stance analysis through its timeliness, open-data nature, and user-level perspective. The dataset is available at https://doi.org/10.5281/zenodo.15616911",
      "tldr_zh": "该研究针对新兴社交媒体平台 Bluesky 在用户层面立场检测 (user-level stance detection) 资源匮乏的问题，推出了首个针对 2024 年美国总统大选的立场检测数据集 PolitiSky24。该数据集主要围绕 Kamala Harris 和 Donald Trump 展开，包含 16,044 个用户-目标立场对，并整合了互动元数据、交互图谱以及完整的用户发帖历史。PolitiSky24 通过结合先进的信息检索 (information retrieval) 和大语言模型 (large language models, LLMs) 的评估管线构建，不仅能生成立场标签，还提供支持性理由和文本片段以增强透明度。实验结果显示，该标注方法在使用可扩展的 LLMs 时达到了 81% 的准确率。该资源凭借其时效性、开放性和用户层面的研究视角，有效填补了政治立场分析领域的空白，为研究人员提供了全面分析社交媒体政治动态的新工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "The dataset is available at https://doi.org/10.5281/zenodo.15616911",
      "pdf_url": "https://arxiv.org/pdf/2506.07606v1",
      "published_date": "2025-06-09 10:06:25 UTC",
      "updated_date": "2025-06-09 10:06:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:20:43.516643+00:00"
    },
    {
      "arxiv_id": "2506.07603v2",
      "title": "SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis",
      "title_zh": "SurgBench：手术视频分析的统一大规模基准",
      "authors": [
        "Jianhui Wei",
        "Zikai Xiao",
        "Danyu Sun",
        "Luqi Gong",
        "Zongxin Yang",
        "Zuozhu Liu",
        "Jian Wu"
      ],
      "abstract": "Surgical video understanding is pivotal for enabling automated intraoperative decision-making, skill assessment, and postoperative quality improvement. However, progress in developing surgical video foundation models (FMs) remains hindered by the scarcity of large-scale, diverse datasets for pretraining and systematic evaluation. In this paper, we introduce \\textbf{SurgBench}, a unified surgical video benchmarking framework comprising a pretraining dataset, \\textbf{SurgBench-P}, and an evaluation benchmark, \\textbf{SurgBench-E}. SurgBench offers extensive coverage of diverse surgical scenarios, with SurgBench-P encompassing 53 million frames across 22 surgical procedures and 11 specialties, and SurgBench-E providing robust evaluation across six categories (phase classification, camera motion, tool recognition, disease diagnosis, action classification, and organ detection) spanning 72 fine-grained tasks. Extensive experiments reveal that existing video FMs struggle to generalize across varied surgical video analysis tasks, whereas pretraining on SurgBench-P yields substantial performance improvements and superior cross-domain generalization to unseen procedures and modalities. Our dataset and code are available upon request.",
      "tldr_zh": "该研究针对手术视频理解领域缺乏大规模多样化预训练和评估数据集的挑战，提出了一个统一的大规模手术视频基准框架 SurgBench。该框架包含预训练数据集 SurgBench-P，涵盖了 11 个专科、22 种手术程序的 5300 万帧图像，提供了极具多样性的手术场景覆盖。同时，研究还开发了评估基准 SurgBench-E，通过 72 个细粒度任务对阶段分类 (phase classification)、工具识别 (tool recognition) 和动作分类 (action classification) 等 6 个核心范畴进行评估。实验结果表明，现有的视频基础模型 (video FMs) 在处理多样的手术视频分析任务时面临泛化困难。相比之下，在 SurgBench-P 上进行预训练可显著提升模型性能，并在未见过的手术程序和模态中表现出卓越的跨域泛化能力 (cross-domain generalization)。该工作为手术视频领域的基础模型开发提供了重要的数据支撑与标准化评价体系。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07603v2",
      "published_date": "2025-06-09 10:02:58 UTC",
      "updated_date": "2025-06-16 03:31:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:20:35.204961+00:00"
    },
    {
      "arxiv_id": "2506.07600v1",
      "title": "SceneRAG: Scene-level Retrieval-Augmented Generation for Video Understanding",
      "title_zh": "SceneRAG：面向视频理解的场景级检索增强生成",
      "authors": [
        "Nianbo Zeng",
        "Haowen Hou",
        "Fei Richard Yu",
        "Si Shi",
        "Ying Tiffany He"
      ],
      "abstract": "Despite recent advances in retrieval-augmented generation (RAG) for video understanding, effectively understanding long-form video content remains underexplored due to the vast scale and high complexity of video data. Current RAG approaches typically segment videos into fixed-length chunks, which often disrupts the continuity of contextual information and fails to capture authentic scene boundaries. Inspired by the human ability to naturally organize continuous experiences into coherent scenes, we present SceneRAG, a unified framework that leverages large language models to segment videos into narrative-consistent scenes by processing ASR transcripts alongside temporal metadata. SceneRAG further sharpens these initial boundaries through lightweight heuristics and iterative correction. For each scene, the framework fuses information from both visual and textual modalities to extract entity relations and dynamically builds a knowledge graph, enabling robust multi-hop retrieval and generation that account for long-range dependencies. Experiments on the LongerVideos benchmark, featuring over 134 hours of diverse content, confirm that SceneRAG substantially outperforms prior baselines, achieving a win rate of up to 72.5 percent on generation tasks.",
      "tldr_zh": "该研究针对长视频理解中固定长度分块(fixed-length chunks)导致的上下文不连贯及场景边界缺失问题，提出了SceneRAG，一种统一的场景级检索增强生成框架。SceneRAG利用大语言模型(LLMs)处理ASR文本和时间元数据，将视频分割为叙事一致的场景，并通过轻量级启发式方法和迭代修正进一步优化边界精度。在每个场景内部，该框架融合视觉与文本模态信息来提取实体关系，并动态构建知识图谱(Knowledge Graph)，从而支持能够处理长程依赖的鲁棒多跳检索(multi-hop retrieval)与生成。在包含超过134小时多样化内容的LongerVideos基准测试中，SceneRAG的表现显著优于现有基线模型。实验结果显示，该框架在生成任务中实现了高达72.5%的胜率，证明了其在理解大规模高复杂度视频数据方面的卓越能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07600v1",
      "published_date": "2025-06-09 10:00:54 UTC",
      "updated_date": "2025-06-09 10:00:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:20:45.249652+00:00"
    },
    {
      "arxiv_id": "2506.07591v1",
      "title": "Automating Exploratory Multiomics Research via Language Models",
      "title_zh": "利用语言模型实现探索性多组学研究的自动化",
      "authors": [
        "Shang Qu",
        "Ning Ding",
        "Linhai Xie",
        "Yifei Li",
        "Zaoqu Liu",
        "Kaiyan Zhang",
        "Yibai Xiong",
        "Yuxin Zuo",
        "Zhangren Chen",
        "Ermo Hua",
        "Xingtai Lv",
        "Youbang Sun",
        "Yang Li",
        "Dong Li",
        "Fuchu He",
        "Bowen Zhou"
      ],
      "abstract": "This paper introduces PROTEUS, a fully automated system that produces data-driven hypotheses from raw data files. We apply PROTEUS to clinical proteogenomics, a field where effective downstream data analysis and hypothesis proposal is crucial for producing novel discoveries. PROTEUS uses separate modules to simulate different stages of the scientific process, from open-ended data exploration to specific statistical analysis and hypothesis proposal. It formulates research directions, tools, and results in terms of relationships between biological entities, using unified graph structures to manage complex research processes. We applied PROTEUS to 10 clinical multiomics datasets from published research, arriving at 360 total hypotheses. Results were evaluated through external data validation and automatic open-ended scoring. Through exploratory and iterative research, the system can navigate high-throughput and heterogeneous multiomics data to arrive at hypotheses that balance reliability and novelty. In addition to accelerating multiomic analysis, PROTEUS represents a path towards tailoring general autonomous systems to specialized scientific domains to achieve open-ended hypothesis generation from data.",
      "tldr_zh": "该研究提出了PROTEUS，一个旨在从原始数据中自动生成数据驱动假设的全自动系统，主要应用于临床蛋白质组学(clinical proteogenomics)领域。PROTEUS采用模块化设计来模拟科学研究的各个阶段，涵盖了从开放式数据探索到特定统计分析及假设提出的全过程。该系统将研究方向和工具定义为生物实体间的关系，并利用统一的图结构(unified graph structures)高效管理复杂的研究流程。通过对10个已发表的临床多组学(multiomics)数据集进行测试，PROTEUS共生成了360项研究假设。实验通过外部数据验证和自动开放式评分证明，该系统能够有效处理高通量且异构的数据，在保证假设可靠性的同时兼顾新颖性。PROTEUS不仅显著提升了多组学分析的效率，也为在专门科学领域构建能够从数据中自主生成假设的通用智能系统提供了重要参考。",
      "categories": [
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07591v1",
      "published_date": "2025-06-09 09:44:21 UTC",
      "updated_date": "2025-06-09 09:44:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:20:48.920882+00:00"
    },
    {
      "arxiv_id": "2506.08062v2",
      "title": "FairDICE: Fairness-Driven Offline Multi-Objective Reinforcement Learning",
      "title_zh": "FairDICE：公平驱动的离线多目标强化学习",
      "authors": [
        "Woosung Kim",
        "Jinho Lee",
        "Jongmin Lee",
        "Byung-Jun Lee"
      ],
      "abstract": "Multi-objective reinforcement learning (MORL) aims to optimize policies in the presence of conflicting objectives, where linear scalarization is commonly used to reduce vector-valued returns into scalar signals. While effective for certain preferences, this approach cannot capture fairness-oriented goals such as Nash social welfare or max-min fairness, which require nonlinear and non-additive trade-offs. Although several online algorithms have been proposed for specific fairness objectives, a unified approach for optimizing nonlinear welfare criteria in the offline setting-where learning must proceed from a fixed dataset-remains unexplored. In this work, we present FairDICE, the first offline MORL framework that directly optimizes nonlinear welfare objective. FairDICE leverages distribution correction estimation to jointly account for welfare maximization and distributional regularization, enabling stable and sample-efficient learning without requiring explicit preference weights or exhaustive weight search. Across multiple offline benchmarks, FairDICE demonstrates strong fairness-aware performance compared to existing baselines.",
      "tldr_zh": "该研究针对多目标强化学习(MORL)中传统线性标量化方法无法捕捉公平性目标（如 Nash social welfare 或 max-min fairness）的问题展开探讨，指出这些目标通常需要非线性和非加性的权衡。尽管在线算法已有相关研究，但在离线(offline)环境下优化非线性福利标准的统一方法仍处于空白。为此，作者提出了 FairDICE，这是首个直接优化非线性福利目标的离线 MORL 框架。该框架利用分布校正估计(distribution correction estimation)来同时平衡福利最大化与分布正则化，从而在不需要明确偏好权重或详尽权重搜索的情况下，实现稳定且样本效率高的学习。在多个离线基准测试中，FairDICE 展现出了显著优于现有基准模型的公平感知性能，为离线环境下的复杂多目标优化提供了有效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Multi-objective Reinforcement Learning",
      "pdf_url": "https://arxiv.org/pdf/2506.08062v2",
      "published_date": "2025-06-09 09:40:11 UTC",
      "updated_date": "2025-11-18 05:22:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:20:45.113135+00:00"
    },
    {
      "arxiv_id": "2506.07587v1",
      "title": "PrunePEFT: Iterative Hybrid Pruning for Parameter-Efficient Fine-tuning of LLMs",
      "title_zh": "PrunePEFT：面向大语言模型参数高效微调的迭代混合剪枝",
      "authors": [
        "Tongzhou Yu",
        "Zhuhao Zhang",
        "Guanghui Zhu",
        "Shen Jiang",
        "Meikang Qiu",
        "Yihua Huang"
      ],
      "abstract": "Parameter Efficient Fine-Tuning (PEFT) methods have emerged as effective and promising approaches for fine-tuning pre-trained language models. Compared with Full parameter Fine-Tuning (FFT), PEFT achieved comparable task performance with a substantial reduction of trainable parameters, which largely saved the training and storage costs. However, using the PEFT method requires considering a vast design space, such as the type of PEFT modules and their insertion layers. Inadequate configurations can lead to sub-optimal results. Conventional solutions such as architectural search techniques, while effective, tend to introduce substantial additional overhead. In this paper, we propose a novel approach, PrunePEFT, which formulates the PEFT strategy search as a pruning problem and introduces a hybrid pruning strategy that capitalizes on the sensitivity of pruning methods to different PEFT modules. This method extends traditional pruning techniques by iteratively removing redundant or conflicting PEFT modules, thereby optimizing the fine-tuned configuration. By efficiently identifying the most relevant modules, our approach significantly reduces the computational burden typically associated with architectural search processes, making it a more scalable and efficient solution for fine-tuning large pre-trained models.",
      "tldr_zh": "该研究针对大语言模型(LLMs)在进行参数高效微调(PEFT)时面临配置空间巨大且策略搜索成本高昂的问题，提出了PrunePEFT，这是一种将PEFT策略搜索转化为剪枝问题的创新方法。该方法引入了混合剪枝(Hybrid Pruning)策略，充分利用剪枝技术对不同PEFT模块的敏感性，通过迭代移除冗余或冲突的模块来优化微调配置。相比传统的架构搜索(NAS)技术，PrunePEFT显著降低了寻找最优微调策略所需的额外计算开销，使其成为一种更具扩展性的微调解决方案。通过高效识别最相关的模块，该方法在优化模型性能的同时，大幅提升了大规模预训练模型微调的效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07587v1",
      "published_date": "2025-06-09 09:32:58 UTC",
      "updated_date": "2025-06-09 09:32:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:20:53.665500+00:00"
    },
    {
      "arxiv_id": "2506.07583v1",
      "title": "Beyond the Sentence: A Survey on Context-Aware Machine Translation with Large Language Models",
      "title_zh": "超越单句：基于大语言模型的上下文感知机器翻译综述",
      "authors": [
        "Ramakrishna Appicharla",
        "Baban Gain",
        "Santanu Pal",
        "Asif Ekbal"
      ],
      "abstract": "Despite the popularity of the large language models (LLMs), their application to machine translation is relatively underexplored, especially in context-aware settings. This work presents a literature review of context-aware translation with LLMs. The existing works utilise prompting and fine-tuning approaches, with few focusing on automatic post-editing and creating translation agents for context-aware machine translation. We observed that the commercial LLMs (such as ChatGPT and Tower LLM) achieved better results than the open-source LLMs (such as Llama and Bloom LLMs), and prompt-based approaches serve as good baselines to assess the quality of translations. Finally, we present some interesting future directions to explore.",
      "tldr_zh": "该综述论文探讨了 Large Language Models (LLMs) 在上下文感知机器翻译 (Context-Aware Machine Translation) 领域的应用现状，针对该领域研究相对匮乏的问题进行了系统性的文献回顾。文章总结了现有研究中采用的 Prompting 和 Fine-tuning 方法，并重点关注了自动后期编辑 (Automatic Post-Editing) 以及翻译智能体 (Translation Agents) 的构建。通过对比分析发现，ChatGPT 和 Tower LLM 等商业模型在翻译性能上普遍优于 Llama 和 Bloom 等开源模型，且基于 Prompt 的方法已被证明是评估翻译质量的有效基准 (Baselines)。该项工作不仅厘清了当前技术路径，还提出了未来值得探索的研究方向，为实现更具上下文连贯性的机器翻译提供了重要参考。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07583v1",
      "published_date": "2025-06-09 09:27:00 UTC",
      "updated_date": "2025-06-09 09:27:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:20:55.484819+00:00"
    },
    {
      "arxiv_id": "2506.07581v1",
      "title": "FedCGD: Collective Gradient Divergence Optimized Scheduling for Wireless Federated Learning",
      "title_zh": "FedCGD：面向无线联邦学习的集体梯度分歧优化调度",
      "authors": [
        "Tan Chen",
        "Jintao Yan",
        "Yuxuan Sun",
        "Sheng Zhou",
        "Zhisheng Niu"
      ],
      "abstract": "Federated learning (FL) is a promising paradigm for multiple devices to cooperatively train a model. When applied in wireless networks, two issues consistently affect the performance of FL, i.e., data heterogeneity of devices and limited bandwidth. Many papers have investigated device scheduling strategies considering the two issues. However, most of them recognize data heterogeneity as a property of individual devices. In this paper, we prove that the convergence speed of FL is affected by the sum of device-level and sample-level collective gradient divergence (CGD). The device-level CGD refers to the gradient divergence of the scheduled device group, instead of the sum of the individual device divergence. The sample-level CGD is statistically upper bounded by sampling variance, which is inversely proportional to the total number of samples scheduled for local update. To derive a tractable form of the device-level CGD, we further consider a classification problem and transform it into the weighted earth moving distance (WEMD) between the group distribution and the global distribution. Then we propose FedCGD algorithm to minimize the sum of multi-level CGDs by balancing WEMD and sampling variance, within polynomial time. Simulation shows that the proposed strategy increases classification accuracy on the CIFAR-10 dataset by up to 4.2\\% while scheduling 41.8\\% fewer devices, and flexibly switches between reducing WEMD and reducing sampling variance.",
      "tldr_zh": "该研究针对无线联邦学习 (Federated Learning, FL) 在网络中面临的数据异构性 (data heterogeneity) 和带宽受限 (limited bandwidth) 挑战，探讨了设备调度策略对模型性能的影响。论文通过数学证明指出，联邦学习的收敛速度受到设备级和样本级集体梯度散度 (Collective Gradient Divergence, CGD) 总和的影响，其中设备级 CGD 取决于所选设备组的整体分布而非个体散度的叠加。为了实现高效计算，研究将设备级 CGD 转化为组分布与全局分布之间的加权推土距离 (Weighted Earth Moving Distance, WEMD)，并据此提出了 FedCGD 算法。该算法旨在通过平衡 WEMD 与由样本量决定的抽样方差 (sampling variance)，在多项式时间内最小化多级 CGD。仿真结果表明，FedCGD 能够灵活在减少 WEMD 与降低抽样方差之间切换，在 CIFAR-10 数据集上仅需调度减少 41.8% 的设备即可提升高达 4.2% 的分类准确率，显著优化了通信效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07581v1",
      "published_date": "2025-06-09 09:24:33 UTC",
      "updated_date": "2025-06-09 09:24:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:21:18.203975+00:00"
    },
    {
      "arxiv_id": "2506.07578v3",
      "title": "Denoising the Future: Top-p Distributions for Moving Through Time",
      "title_zh": "去噪未来：用于时间推移的 Top-p 分布",
      "authors": [
        "Florian Andreas Marwitz",
        "Ralf Möller",
        "Magnus Bender",
        "Marcel Gehrke"
      ],
      "abstract": "Inference in dynamic probabilistic models is a complex task involving expensive operations. In particular, for Hidden Markov Models, the whole state space has to be enumerated for advancing in time. Even states with negligible probabilities are considered, resulting in computational inefficiency and increased noise due to the propagation of unlikely probability mass. We propose to denoise the future and speed up inference by using only the top-p states, i.e., the most probable states with accumulated probability p. We show that the error introduced by using only the top-p states is bound by p and the so-called minimal mixing rate of the underlying model. Moreover, in our empirical evaluation, we show that we can expect speedups of at least an order of magnitude, while the error in terms of total variation distance is below 0.09.",
      "tldr_zh": "该研究针对动态概率模型（特别是 Hidden Markov Models）在推理过程中因枚举完整状态空间而导致的计算低效和噪声传播问题，提出了一种基于 top-p states 的“去噪未来”（Denoising the Future）策略。该方法通过仅保留累积概率为 p 的最可能状态，显著减少了计算负担并抑制了极低概率状态引入的噪声。理论分析表明，该策略产生的误差界限由 p 值和底层模型的 minimal mixing rate 共同决定。实验评估显示，该方法在保证 total variation distance 误差低于 0.09 的前提下，能够实现至少一个数量级的推理加速。这一成果为处理复杂动态概率模型中的高效时间推进提供了重要的理论支撑和实践方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ECSQARU 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.07578v3",
      "published_date": "2025-06-09 09:23:09 UTC",
      "updated_date": "2025-10-21 12:18:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:21:42.815415+00:00"
    },
    {
      "arxiv_id": "2506.07570v2",
      "title": "OptiScene: LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization",
      "title_zh": "OptiScene：基于规模化人类对齐数据合成与多阶段偏好优化的大语言模型驱动室内场景布局生成",
      "authors": [
        "Yixuan Yang",
        "Zhen Luo",
        "Tongsheng Ding",
        "Junru Lu",
        "Mingqi Gao",
        "Jinyu Yang",
        "Victor Sanchez",
        "Feng Zheng"
      ],
      "abstract": "Automatic indoor layout generation has attracted increasing attention due to its potential in interior design, virtual environment construction, and embodied AI. Existing methods fall into two categories: prompt-driven approaches that leverage proprietary LLM services (e.g., GPT APIs) and learning-based methods trained on layout data upon diffusion-based models. Prompt-driven methods often suffer from spatial inconsistency and high computational costs, while learning-based methods are typically constrained by coarse relational graphs and limited datasets, restricting their generalization to diverse room categories. In this paper, we revisit LLM-based indoor layout generation and present 3D-SynthPlace, a large-scale dataset that combines synthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline, upgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000 scenes, covering four common room types -- bedroom, living room, kitchen, and bathroom -- enriched with diverse objects and high-level spatial annotations. We further introduce OptiScene, a strong open-source LLM optimized for indoor layout generation, fine-tuned based on our 3D-SynthPlace dataset through our two-stage training. For the warum-up stage I, we adopt supervised fine-tuning (SFT), which is taught to first generate high-level spatial descriptions then conditionally predict concrete object placements. For the reinforcing stage II, to better align the generated layouts with human design preferences, we apply multi-turn direct preference optimization (DPO), which significantly improving layout quality and generation success rates. Extensive experiments demonstrate that OptiScene outperforms traditional prompt-driven and learning-based baselines. Moreover, OptiScene shows promising potential in interactive tasks such as scene editing and robot navigation.",
      "tldr_zh": "该研究针对室内场景布局生成中存在的空间不一致性和数据集规模限制，提出了由大语言模型驱动的 OptiScene 框架。研究者首先构建了大规模数据集 3D-SynthPlace，利用 GPT 合成与人工检查相结合的流水线，为四种常见房间类型提供了包含高层级空间标注的近1.7万个场景。OptiScene 模型采用两阶段训练策略，第一阶段利用有监督微调 (SFT) 引导模型从空间描述到具体物体放置的生成，第二阶段则通过多轮直接偏好优化 (DPO) 进一步对齐人类设计偏好。实验结果证明，OptiScene 在布局质量和生成成功率上均优于现有的提示词驱动和学习基线方法。此外，该模型在场景编辑和机器人导航等交互任务中也表现出了卓越的应用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07570v2",
      "published_date": "2025-06-09 09:13:06 UTC",
      "updated_date": "2025-09-19 09:25:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:21:47.954123+00:00"
    },
    {
      "arxiv_id": "2506.07564v3",
      "title": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems",
      "title_zh": "SAFEFLOW：一种面向可信与事务性自主智能体系统的规范化协议",
      "authors": [
        "Peiran Li",
        "Xinkai Zou",
        "Zhuohang Wu",
        "Ruifeng Li",
        "Shuo Xing",
        "Hanwen Zheng",
        "Zhikai Hu",
        "Yuping Wang",
        "Haoxi Li",
        "Qin Yuan",
        "Yingmo Zhang",
        "Zhengzhong Tu"
      ],
      "abstract": "Recent advances in large language models (LLMs) and vision-language models (VLMs) have enabled powerful autonomous agents capable of complex reasoning and multi-modal tool use. Despite their growing capabilities, today's agent frameworks remain fragile, lacking principled mechanisms for secure information flow, reliability, and multi-agent coordination. In this work, we introduce SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based agents. SAFEFLOW enforces fine-grained information flow control (IFC), precisely tracking provenance, integrity, and confidentiality of all the data exchanged between agents, tools, users, and environments. By constraining LLM reasoning to respect these security labels, SAFEFLOW prevents untrusted or adversarial inputs from contaminating high-integrity decisions. To ensure robustness in concurrent multi-agent settings, SAFEFLOW introduces transactional execution, conflict resolution, and secure scheduling over shared state, preserving global consistency across agents. We further introduce mechanisms, including write-ahead logging, rollback, and secure caches, that further enhance resilience against runtime errors and policy violations. To validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark suite designed to evaluate agent reliability under adversarial, noisy, and concurrent operational conditions. Extensive experiments demonstrate that agents built with SAFEFLOW maintain impressive task performance and security guarantees even in hostile environments, substantially outperforming state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for principled, robust, and secure agent ecosystems, advancing the frontier of reliable autonomy.",
      "tldr_zh": "该研究提出了 SAFEFLOW，这是一种面向大型语言模型 (LLMs) 和视觉语言模型 (VLMs) 自主智能体系统的协议级框架，旨在解决现有系统在信息流安全、可靠性和多智能体协作机制上的缺失。SAFEFLOW 通过实施细粒度的信息流控制 (Information Flow Control, IFC)，能够精确追踪数据的来源、完整性与机密性，并约束 LLM 推理过程以防止不合规输入干扰高诚信决策。针对并发环境，该框架引入了事务性执行 (Transactional Execution)、冲突解决和安全调度机制，确保了多智能体间共享状态的全局一致性。此外，通过整合预写日志 (Write-ahead Logging)、回滚 (Rollback) 和安全缓存等技术，SAFEFLOW 显著增强了系统应对运行时错误与策略违规的韧性。研究团队还构建了 SAFEFLOWBENCH 基准测试集，实验证明该框架使智能体在对抗性及噪声环境下依然能维持出色的任务表现，性能大幅超越现有技术，为构建可信且稳健的自主智能体生态系统提供了重要支撑。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Former versions either contain unrelated content or cannot be properly converted to PDF",
      "pdf_url": "https://arxiv.org/pdf/2506.07564v3",
      "published_date": "2025-06-09 09:04:37 UTC",
      "updated_date": "2025-06-11 03:14:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:21:38.603136+00:00"
    },
    {
      "arxiv_id": "2506.07563v3",
      "title": "MoE-MLoRA for Multi-Domain CTR Prediction: Efficient Adaptation with Expert Specialization",
      "title_zh": "MoE-MLoRA：面向多领域点击率预测的专家专业化高效适配",
      "authors": [
        "Ken Yaggel",
        "Eyal German",
        "Aviel Ben Siman Tov"
      ],
      "abstract": "Personalized recommendation systems must adapt to user interactions across different domains. Traditional approaches like MLoRA apply a single adaptation per domain but lack flexibility in handling diverse user behaviors. To address this, we propose MoE-MLoRA, a mixture-of-experts framework where each expert is first trained independently to specialize in its domain before a gating network is trained to weight their contributions dynamically. We evaluate MoE-MLoRA across eight CTR models on Movielens and Taobao, showing that it improves performance in large-scale, dynamic datasets (+1.45 Weighed-AUC in Taobao-20) but offers limited benefits in structured datasets with low domain diversity and sparsity. Further analysis of the number of experts per domain reveals that larger ensembles do not always improve performance, indicating the need for model-aware tuning. Our findings highlight the potential of expert-based architectures for multi-domain recommendation systems, demonstrating that task-aware specialization and adaptive gating can enhance predictive accuracy in complex environments. The implementation and code are available in our GitHub repository.",
      "tldr_zh": "该研究提出了MoE-MLoRA框架，旨在解决传统MLoRA在多域点击率(CTR)预测中因领域自适应方式过于单一而缺乏处理多样化用户行为灵活性等问题。该框架引入了混合专家模型(mixture-of-experts)机制，通过先独立训练专家使其在特定领域实现专业化，再利用门控网络(gating network)动态权衡各专家的贡献。实验在Movielens和Taobao数据集上针对8种CTR模型进行了评估，结果表明MoE-MLoRA在大规模动态数据集中表现优异，在Taobao-20数据集上使Weighed-AUC提升了1.45%。然而，研究也发现该方法在领域多样性低或数据稀疏的结构化数据集中收益有限，且增加专家数量并不总是能提升性能，强调了模型感知调优(model-aware tuning)的必要性。该研究证明了任务感知专业化和自适应门控架构在增强复杂环境下多域推荐系统预测准确性方面的潜力。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07563v3",
      "published_date": "2025-06-09 09:03:05 UTC",
      "updated_date": "2025-06-11 07:55:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:21:46.771290+00:00"
    },
    {
      "arxiv_id": "2506.07557v1",
      "title": "SELT: Self-Evaluation Tree Search for LLMs with Task Decomposition",
      "title_zh": "SELT：基于任务分解的大语言模型自评估树搜索",
      "authors": [
        "Mengsong Wu",
        "Di Zhang",
        "Yuqiang Li",
        "Dongzhan Zhou",
        "Wenliang Chen"
      ],
      "abstract": "While Large Language Models (LLMs) have achieved remarkable success in a wide range of applications, their performance often degrades in complex reasoning tasks. In this work, we introduce SELT (Self-Evaluation LLM Tree Search), a novel framework that leverages a modified Monte Carlo Tree Search (MCTS) to enhance LLM reasoning without relying on external reward models. By redefining the Upper Confidence Bound scoring to align with intrinsic self-evaluation capabilities of LLMs and decomposing the inference process into atomic subtasks augmented with semantic clustering at each node, SELT effectively balances exploration and exploitation, reduces redundant reasoning paths, and mitigates hallucination. We validate our approach on challenging benchmarks, including the knowledge-based MMLU and the Tool Learning dataset Seal-Tools, where SELT achieves significant improvements in answer accuracy and reasoning robustness compared to baseline methods. Notably, our framework operates without task-specific fine-tuning, demonstrating strong generalizability across diverse reasoning tasks. Relevant results and code are available at https://github.com/fairyshine/SELT .",
      "tldr_zh": "该研究提出了SELT（Self-Evaluation LLM Tree Search），这是一个结合任务分解的自评估大语言模型树搜索框架，旨在提升LLMs在复杂推理任务中的表现。该框架利用改进的Monte Carlo Tree Search (MCTS)，通过重新定义Upper Confidence Bound (UCB) 评分来对齐LLMs的内在自评估能力，从而摆脱了对外部reward模型的依赖。SELT将推理过程分解为原子级子任务，并在每个节点结合语义聚类，有效平衡了探索与利用，减少了冗余推理路径并缓解了幻觉问题。在知识性基准MMLU和工具学习数据集Seal-Tools上的实验结果表明，SELT在答案准确性和推理鲁棒性方面均显著优于基线方法。值得注意的是，该框架无需针对特定任务进行微调，在不同类型的推理任务中展现出卓越的泛化性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.07557v1",
      "published_date": "2025-06-09 08:52:27 UTC",
      "updated_date": "2025-06-09 08:52:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:21:48.837671+00:00"
    },
    {
      "arxiv_id": "2506.07555v3",
      "title": "Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries",
      "title_zh": "通过私密文本中介合成隐私保护的高分辨率图像",
      "authors": [
        "Haoxiang Wang",
        "Zinan Lin",
        "Da Yu",
        "Huishuai Zhang"
      ],
      "abstract": "Generating high fidelity, differentially private (DP) synthetic images offers a promising route to share and analyze sensitive visual data without compromising individual privacy. However, existing DP image synthesis methods struggle to produce high resolution outputs that faithfully capture the structure of the original data. In this paper, we introduce a novel method, referred to as Synthesis via Private Textual Intermediaries (SPTI), that can generate high resolution DP images with easy adoption. The key idea is to shift the challenge of DP image synthesis from the image domain to the text domain by leveraging state of the art DP text generation methods. SPTI first summarizes each private image into a concise textual description using image to text models, then applies a modified Private Evolution algorithm to generate DP text, and finally reconstructs images using text to image models. Notably, SPTI requires no model training, only inference with off the shelf models. Given a private dataset, SPTI produces synthetic images of substantially higher quality than prior DP approaches. On the LSUN Bedroom dataset, SPTI attains an FID equal to 26.71 under epsilon equal to 1.0, improving over Private Evolution FID of 40.36. Similarly, on MM CelebA HQ, SPTI achieves an FID equal to 33.27 at epsilon equal to 1.0, compared to 57.01 from DP fine tuning baselines. Overall, our results demonstrate that Synthesis via Private Textual Intermediaries provides a resource efficient and proprietary model compatible framework for generating high resolution DP synthetic images, greatly expanding access to private visual datasets.",
      "tldr_zh": "该研究提出了Synthesis via Private Textual Intermediaries (SPTI)，旨在解决现有的差分隐私 (Differentially Private, DP) 图像合成方法在生成高分辨率图像时面临的结构捕捉不佳和质量受限问题。其核心思路是将DP图像合成的挑战从图像域转移到文本域，利用先进的DP文本生成方法提升合成质量。SPTI首先通过Image-to-Text模型将私有图像转化为简洁的文本描述，接着应用改进的Private Evolution算法生成DP文本，最后利用Text-to-Image模型重建图像。该框架无需任何模型训练，仅需对现成的 (off-the-shelf) 模型进行推理，具有极高的资源效率和模型兼容性。实验结果显示，在LSUN Bedroom和MM CelebA HQ数据集上，SPTI在ε=1.0条件下的FID表现均大幅优于现有的DP合成基线。该研究为生成高质量DP合成图像并扩展私有视觉数据集的可访问性提供了一个高效且实用的新框架。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07555v3",
      "published_date": "2025-06-09 08:48:06 UTC",
      "updated_date": "2025-10-27 16:44:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:21:57.363166+00:00"
    },
    {
      "arxiv_id": "2506.07553v3",
      "title": "GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition",
      "title_zh": "GTR-CoT：将图遍历作为分子结构识别的视觉思维链",
      "authors": [
        "Jingchao Wang",
        "Yifan He",
        "Haote Yang",
        "Jiang Wu",
        "Lingli Ge",
        "Xingjian Wei",
        "Yinfan Wang",
        "Linye Li",
        "Huijie Ao",
        "Chengjin Liu",
        "Bin Wang",
        "Lijun Wu",
        "Conghui He"
      ],
      "abstract": "Optical Chemical Structure Recognition (OCSR) is essential for converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown promise, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To address these issues, we introduce GTR-VL, featuring two key innovations: (1) the \\textit{Graph Traversal as Visual Chain of Thought} mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric \\textit{Faithfully Recognize What You've Seen} principle, which aligns abbreviated structures in images with their expanded annotations. For hand-drawn OCSR tasks, where datasets lack graph annotations and only provide final SMILES, we apply reinforcement learning using the GRPO method, introducing reward mechanisms like format reward, graph reward, and SMILES reward. This approach significantly enhances performance in hand-drawn recognition tasks through weak supervision. We developed GTR-1.3M, a large-scale instruction-tuning dataset with corrected annotations, and MolRec-Bench, the first benchmark for fine-grained evaluation of graph-parsing accuracy in OCSR. Our two-stage training scheme involves SFT training for printed images and the GRPO method for transferring capabilities to hand-drawn tasks. Experiments show that GTR-VL outperforms specialist models, chemistry-domain VLMs, and commercial VLMs on both printed and hand-drawn datasets.",
      "tldr_zh": "该研究针对光学化学结构识别 (OCSR) 中视觉语言模型 (VLMs) 难以处理复杂分子结构的问题，提出了全新的 GTR-VL 框架。该框架引入了图遍历视觉链式思维 (Graph Traversal as Visual Chain of Thought) 机制，通过模拟人类推理逐一预测原子和化学键，并结合数据驱动原则对齐缩写与展开标注。为了解决手写识别任务中图标注缺失的问题，研究采用了基于 GRPO 的强化学习方法，通过格式、图结构和 SMILES 奖励等弱监督机制提升模型性能。团队还发布了大规模指令微调数据集 GTR-1.3M 和首个用于细粒度图解析评估的基准 MolRec-Bench。实验结果表明，GTR-VL 在印刷和手写分子图像识别任务上均显著优于当前的领域专用模型、化学 VLMs 及商业 VLMs。",
      "categories": [
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07553v3",
      "published_date": "2025-06-09 08:47:10 UTC",
      "updated_date": "2026-01-13 07:21:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:21:59.713106+00:00"
    },
    {
      "arxiv_id": "2506.07551v2",
      "title": "CheMatAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning",
      "title_zh": "CheMatAgent：通过基于树搜索的工具学习增强化学与材料科学大语言模型",
      "authors": [
        "Mengsong Wu",
        "YaFei Wang",
        "Yidong Ming",
        "Yuqi An",
        "Yuwei Wan",
        "Wenliang Chen",
        "Binbin Lin",
        "Yuqiang Li",
        "Tong Xie",
        "Dongzhan Zhou"
      ],
      "abstract": "Large language models (LLMs) have recently demonstrated promising capabilities in chemistry tasks while still facing challenges due to outdated pretraining knowledge and the difficulty of incorporating specialized chemical expertise. To address these issues, we propose an LLM-based agent that synergistically integrates 137 external chemical tools created ranging from basic information retrieval to complex reaction predictions, and a dataset curation pipeline to generate the dataset ChemToolBench that facilitates both effective tool selection and precise parameter filling during fine-tuning and evaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search (HE-MCTS) framework, enabling independent optimization of tool planning and execution. By leveraging self-generated data, our approach supports step-level fine-tuning (FT) of the policy model and training task-adaptive PRM and ORM that surpass GPT-4o. Experimental evaluations demonstrate that our approach significantly improves performance in Chemistry QA and discovery tasks, offering a robust solution to integrate specialized tools with LLMs for advanced chemical applications. All datasets and code are available at https://github.com/AI4Chem/ChemistryAgent .",
      "tldr_zh": "该研究提出了CheMatAgent，旨在解决大语言模型(LLMs)在化学与材料科学领域面临的预训练知识陈旧及专业领域知识集成困难等挑战。该智能体系统协同整合了137个涵盖基础信息检索至复杂反应预测的外部化学工具，并构建了ChemToolBench数据集以优化微调过程中的工具选择与参数填充。核心方法引入了层次化进化蒙特卡洛树搜索(HE-MCTS)框架，实现了工具规划与执行的独立优化。通过利用自生成数据，该方法支持策略模型的步骤级微调(Fine-tuning)，并训练出在性能上超越GPT-4o的任务自适应过程奖励模型(PRM)和结果奖励模型(ORM)。实验评估表明，CheMatAgent在化学问答(Chemistry QA)和科学发现任务中显著提升了模型性能，为将专业工具集成到大语言模型以处理高级化学应用提供了鲁棒的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.07551v2",
      "published_date": "2025-06-09 08:41:39 UTC",
      "updated_date": "2025-06-12 07:30:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:22:08.027954+00:00"
    },
    {
      "arxiv_id": "2506.07548v1",
      "title": "Curriculum Learning With Counterfactual Group Relative Policy Advantage For Multi-Agent Reinforcement Learning",
      "title_zh": "多智能体强化学习中结合反事实群体相对策略优势的课程学习",
      "authors": [
        "Weiqiang Jin",
        "Hongyang Du",
        "Guizhong Liu",
        "Dong In Kim"
      ],
      "abstract": "Multi-agent reinforcement learning (MARL) has achieved strong performance in cooperative adversarial tasks. However, most existing methods typically train agents against fixed opponent strategies and rely on such meta-static difficulty conditions, which limits their adaptability to changing environments and often leads to suboptimal policies. Inspired by the success of curriculum learning (CL) in supervised tasks, we propose a dynamic CL framework for MARL that employs an self-adaptive difficulty adjustment mechanism. This mechanism continuously modulates opponent strength based on real-time agent training performance, allowing agents to progressively learn from easier to more challenging scenarios. However, the dynamic nature of CL introduces instability due to nonstationary environments and sparse global rewards. To address this challenge, we develop a Counterfactual Group Relative Policy Advantage (CGRPA), which is tightly coupled with the curriculum by providing intrinsic credit signals that reflect each agent's impact under evolving task demands. CGRPA constructs a counterfactual advantage function that isolates individual contributions within group behavior, facilitating more reliable policy updates throughout the curriculum. CGRPA evaluates each agent's contribution through constructing counterfactual action advantage function, providing intrinsic rewards that enhance credit assignment and stabilize learning under non-stationary conditions. Extensive experiments demonstrate that our method improves both training stability and final performance, achieving competitive results against state-of-the-art methods. The code is available at https://github.com/NICE-HKU/CL2MARL-SMAC.",
      "tldr_zh": "该研究针对 Multi-agent reinforcement learning (MARL) 在合作对抗任务中因固定对手策略而导致适应性差的问题，提出了一个具有自适应难度调节机制的动态 Curriculum Learning (CL) 框架。该框架能够根据智能体实时表现动态调整对手强度，引导其从简单场景向挑战性场景平滑过渡。为了解决动态课程引入的环境非平稳性与奖励稀疏挑战，研究进一步开发了 Counterfactual Group Relative Policy Advantage (CGRPA) 机制。CGRPA 通过构建 counterfactual action advantage 函数来隔离个体对群体行为的贡献，从而提供精准的内在 Credit Assignment 信号并稳定学习过程。实验结果证明，该方法显著提升了训练稳定性与最终性能，并在相关基准测试中取得了具有竞争力的结果。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages; 12figures",
      "pdf_url": "https://arxiv.org/pdf/2506.07548v1",
      "published_date": "2025-06-09 08:38:18 UTC",
      "updated_date": "2025-06-09 08:38:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:22:21.960611+00:00"
    },
    {
      "arxiv_id": "2506.08060v1",
      "title": "Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques",
      "title_zh": "通过推理时技术激发微调 Transformer 的能力",
      "authors": [
        "Asankhaya Sharma"
      ],
      "abstract": "Large language models have transformed natural language processing, yet supervised fine-tuning (SFT) remains computationally intensive. This paper formally proves that capabilities acquired through SFT can be approximated by a base transformer model using inference-time techniques, specifically in-context learning (ICL), without altering model parameters, under idealized assumptions including unbounded computational resources and access to the fine-tuning dataset. We extend these results to practical scenarios with finite context lengths and partial dataset access. For text generation tasks with fixed output length $l$, datasets of size $\\mathrm{O}\\left( \\frac{m V}{\\varepsilon^2} \\log \\frac{m}δ \\right)$ or, with bounded context, $\\mathrm{O}\\left( \\frac{l \\log V}{\\varepsilon^2} \\log \\frac{1}δ \\right)$ suffice to approximate fine-tuned behavior across $m$ contexts within error $\\varepsilon$, where $V$ is the vocabulary size and $δ$ is the failure probability. For linear classification, datasets of size $\\mathrm{O}\\left( \\frac{d}{\\varepsilon} \\right)$ or, with fixed context, $\\mathrm{O}\\left( \\frac{1}{\\varepsilon^2} \\log \\frac{1}δ \\right)$ are sufficient, where $d$ is the input dimension. Grounded in the Turing completeness of transformers, these results provide a theoretical foundation for resource-efficient deployment of large language models, with practical techniques like retrieval-augmented generation bridging theory to real-world applications.",
      "tldr_zh": "该研究探讨了如何利用推理时技术（Inference-Time Techniques），特别是上下文学习（In-Context Learning, ICL），在不改变模型参数的情况下使基础 Transformer 模型近似实现监督微调（Supervised Fine-Tuning, SFT）所获得的能力。作者通过形式化证明指出，在理想假设以及有限上下文、部分数据集等实际约束下，这种近似是可行的。研究针对文本生成和线性分类任务，分别给出了在特定误差范围内实现微调行为所需的样本复杂度边界。这些发现植根于 Transformer 的图灵完备性（Turing completeness），为大语言模型的资源高效部署奠定了理论基础。此外，该研究还强调了检索增强生成（Retrieval-Augmented Generation, RAG）等技术在连接理论与实际应用中的关键作用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.08060v1",
      "published_date": "2025-06-09 08:37:19 UTC",
      "updated_date": "2025-06-09 08:37:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:22:11.842024+00:00"
    },
    {
      "arxiv_id": "2506.07542v1",
      "title": "APTOS-2024 challenge report: Generation of synthetic 3D OCT images from fundus photographs",
      "title_zh": "APTOS-2024 挑战赛报告：基于眼底照片的合成 3D OCT 图像生成",
      "authors": [
        "Bowen Liu",
        "Weiyi Zhang",
        "Peranut Chotcomwongse",
        "Xiaolan Chen",
        "Ruoyu Chen",
        "Pawin Pakaymaskul",
        "Niracha Arjkongharn",
        "Nattaporn Vongsa",
        "Xuelian Cheng",
        "Zongyuan Ge",
        "Kun Huang",
        "Xiaohui Li",
        "Yiru Duan",
        "Zhenbang Wang",
        "BaoYe Xie",
        "Qiang Chen",
        "Huazhu Fu",
        "Michael A. Mahr",
        "Jiaqi Qu",
        "Wangyiyang Chen",
        "Shiye Wang",
        "Yubo Tan",
        "Yongjie Li",
        "Mingguang He",
        "Danli Shi",
        "Paisan Ruamviboonsuk"
      ],
      "abstract": "Optical Coherence Tomography (OCT) provides high-resolution, 3D, and non-invasive visualization of retinal layers in vivo, serving as a critical tool for lesion localization and disease diagnosis. However, its widespread adoption is limited by equipment costs and the need for specialized operators. In comparison, 2D color fundus photography offers faster acquisition and greater accessibility with less dependence on expensive devices. Although generative artificial intelligence has demonstrated promising results in medical image synthesis, translating 2D fundus images into 3D OCT images presents unique challenges due to inherent differences in data dimensionality and biological information between modalities. To advance generative models in the fundus-to-3D-OCT setting, the Asia Pacific Tele-Ophthalmology Society (APTOS-2024) organized a challenge titled Artificial Intelligence-based OCT Generation from Fundus Images. This paper details the challenge framework (referred to as APTOS-2024 Challenge), including: the benchmark dataset, evaluation methodology featuring two fidelity metrics-image-based distance (pixel-level OCT B-scan similarity) and video-based distance (semantic-level volumetric consistency), and analysis of top-performing solutions. The challenge attracted 342 participating teams, with 42 preliminary submissions and 9 finalists. Leading methodologies incorporated innovations in hybrid data preprocessing or augmentation (cross-modality collaborative paradigms), pre-training on external ophthalmic imaging datasets, integration of vision foundation models, and model architecture improvement. The APTOS-2024 Challenge is the first benchmark demonstrating the feasibility of fundus-to-3D-OCT synthesis as a potential solution for improving ophthalmic care accessibility in under-resourced healthcare settings, while helping to expedite medical research and clinical applications.",
      "tldr_zh": "该论文详细介绍了APTOS-2024挑战赛报告，其核心任务是从二维彩色眼底图像(Fundus Photographs)生成合成的三维光学相干断层扫描(3D OCT)图像。尽管OCT在病变定位和诊断中具有关键作用，但其高昂的设备成本和操作复杂性限制了普及，而二维眼底摄影则更具可及性。本研究建立了挑战赛的基准数据集和评估体系，引入了基于图像的像素级B-scan相似度和基于视频的语义级体积一致性(Video-based Distance)两项忠实度指标。挑战赛吸引了342支团队，优胜方案采用了混合数据预处理、外部眼科数据集预训练、视觉基础模型(Vision Foundation Models)集成以及模型架构优化等创新技术。作为首个证明Fundus-to-3D-OCT合成可行性的基准，该挑战赛展示了其在资源匮乏地区改善眼科护理可及性、加速医疗研究和临床应用的巨大潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07542v1",
      "published_date": "2025-06-09 08:29:37 UTC",
      "updated_date": "2025-06-09 08:29:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:22:33.719534+00:00"
    },
    {
      "arxiv_id": "2506.07539v1",
      "title": "Domain Randomization for Object Detection in Manufacturing Applications using Synthetic Data: A Comprehensive Study",
      "title_zh": "制造业应用中基于合成数据的目标检测领域随机化：一项综合研究",
      "authors": [
        "Xiaomeng Zhu",
        "Jacob Henningsson",
        "Duruo Li",
        "Pär Mårtensson",
        "Lars Hanson",
        "Mårten Björkman",
        "Atsuto Maki"
      ],
      "abstract": "This paper addresses key aspects of domain randomization in generating synthetic data for manufacturing object detection applications. To this end, we present a comprehensive data generation pipeline that reflects different factors: object characteristics, background, illumination, camera settings, and post-processing. We also introduce the Synthetic Industrial Parts Object Detection dataset (SIP15-OD) consisting of 15 objects from three industrial use cases under varying environments as a test bed for the study, while also employing an industrial dataset publicly available for robotic applications. In our experiments, we present more abundant results and insights into the feasibility as well as challenges of sim-to-real object detection. In particular, we identified material properties, rendering methods, post-processing, and distractors as important factors. Our method, leveraging these, achieves top performance on the public dataset with Yolov8 models trained exclusively on synthetic data; mAP@50 scores of 96.4% for the robotics dataset, and 94.1%, 99.5%, and 95.3% across three of the SIP15-OD use cases, respectively. The results showcase the effectiveness of the proposed domain randomization, potentially covering the distribution close to real data for the applications.",
      "tldr_zh": "该研究针对制造业目标检测(Object Detection)中的合成数据生成，深入探讨了领域随机化(Domain Randomization)的关键环节。研究提出了一套全面的数据生成管线，涵盖了物体特征、背景、照明、相机设置以及后期处理(Post-processing)等多个维度。为了验证该方法，作者引入了包含15类工业物体的SIP15-OD数据集，并在公开的机器人应用数据集上进行了测试。实验识别出材料属性、渲染方法、后期处理和干扰项(Distractors)是实现从模拟到现实(Sim-to-real)迁移的重要因素。结果显示，仅使用合成数据训练的YOLOv8模型在公开数据集上达到了96.4%的mAP@50，在SIP15-OD的三个用例中也表现优异，准确率分别达94.1%、99.5%和95.3%。该项工作证明了所提出的领域随机化策略能够有效覆盖接近真实数据的分布，显著提升了工业应用中目标检测模型的性能与可行性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This is accepted by 2025 IEEE International Conference on Robotics & Automation (ICRA), waiting for publication. 14 pages, 14 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.07539v1",
      "published_date": "2025-06-09 08:26:19 UTC",
      "updated_date": "2025-06-09 08:26:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:22:40.472513+00:00"
    },
    {
      "arxiv_id": "2506.07528v2",
      "title": "Coordinating Search-Informed Reasoning and Reasoning-Guided Search in Claim Verification",
      "title_zh": "声明验证中搜索启发式推理与推理引导式搜索的协同",
      "authors": [
        "Qisheng Hu",
        "Quanyu Long",
        "Wenya Wang"
      ],
      "abstract": "Multi-hop claim verification is inherently challenging, requiring multi-step reasoning to construct verification chains while iteratively searching for information to uncover hidden bridging facts. This process is fundamentally interleaved, as effective reasoning relies on dynamically retrieved evidence, while effective search demands reasoning to refine queries based on partial information. To achieve this, we propose Hierarchical Agent Reasoning and Information Search (HARIS), explicitly modeling the coordinated process of reasoning-driven searching and search-informed reasoning. HARIS consists of a high-level reasoning agent that focuses on constructing the main verification chain, generating factual questions when more information is needed, and a low-level search agent that iteratively retrieves more information, refining its search based on intermediate findings. This design allows each agent to specialize in its respective task, enhancing verification accuracy and interpretability. HARIS is trained using reinforcement learning with outcome-based rewards. Experimental results on the EX-FEVER and HOVER benchmarks demonstrate that HARIS achieves strong performance, greatly advancing multi-hop claim verification.",
      "tldr_zh": "该研究针对多跳声明验证(Multi-hop claim verification)中推理与搜索过程交织的挑战，提出了HARIS（Hierarchical Agent Reasoning and Information Search）框架，旨在显式建模“推理驱动搜索”与“搜索引导推理”的协调机制。该框架包含一个专注于构建验证链并生成事实性问题的高层推理智能体(High-level reasoning agent)，以及一个负责迭代检索并根据中间发现优化查询的底层搜索智能体(Low-level search agent)。这种分层设计使得不同智能体能够专注于各自任务，从而增强了系统的验证准确性和可解释性。研究团队利用强化学习(Reinforcement Learning)和基于结果的奖励对HARIS进行训练，使其在处理复杂验证任务时表现更为出色。在EX-FEVER和HOVER基准测试上的实验结果证明，HARIS显著推动了多跳声明验证技术的发展，取得了强劲的性能表现。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Work in progress",
      "pdf_url": "https://arxiv.org/pdf/2506.07528v2",
      "published_date": "2025-06-09 08:11:43 UTC",
      "updated_date": "2025-07-31 17:12:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:22:44.366963+00:00"
    },
    {
      "arxiv_id": "2506.07527v2",
      "title": "Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions",
      "title_zh": "学习强化学习之所不能：面向极难问题的交替式在线微调",
      "authors": [
        "Lu Ma",
        "Hao Liang",
        "Meiyi Qiang",
        "Lexiang Tang",
        "Xiaochen Ma",
        "Zhen Hao Wong",
        "Junbo Niu",
        "Chengyu Shen",
        "Runming He",
        "Yanhao Li",
        "Bin Cui",
        "Wentao Zhang"
      ],
      "abstract": "Recent advances in large language model (LLM) reasoning have shown that sophisticated behaviors such as planning and self-reflection can emerge through reinforcement learning (RL). However, despite these successes, RL in its current form remains insufficient to induce capabilities that exceed the limitations of the base model, as it is primarily optimized based on existing knowledge of the model rather than facilitating the acquisition of new information. To address this limitation, we employ supervised fine-tuning (SFT) to learn what RL cannot, which enables the incorporation of new knowledge and reasoning patterns by leveraging high-quality demonstration data. We analyze the training dynamics of RL and SFT for LLM reasoning and find that RL excels at maintaining and improving performance on questions within the model's original capabilities, while SFT is more effective at enabling progress on questions beyond the current scope of the model. Motivated by the complementary strengths of RL and SFT, we introduce a novel training approach, \\textbf{ReLIFT} (\\textbf{Re}inforcement \\textbf{L}earning \\textbf{I}nterleaved with Online \\textbf{F}ine-\\textbf{T}uning). In ReLIFT, the model is primarily trained using RL, but when it encounters challenging questions, high-quality solutions are collected for fine-tuning, and the training process alternates between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT achieves an average improvement of over +5.2 points across five competition-level benchmarks and one out-of-distribution benchmark compared to other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both RL and SFT while using only 13\\% of the detailed demonstration data, highlighting its scalability. These results provide compelling evidence that ReLIFT overcomes the fundamental limitations of RL and underscores the significant potential.",
      "tldr_zh": "该研究提出了ReLIFT（Reinforcement Learning Interleaved with Online Fine-Tuning），旨在解决强化学习（Reinforcement Learning, RL）在提升大型语言模型推理能力时难以获取新知识和超越基础模型局限的问题。通过分析RL与监督微调（Supervised Fine-Tuning, SFT）的训练动态，研究者发现RL擅长优化模型已有能力，而SFT则在引入新知识和复杂推理模式方面更具优势。ReLIFT框架将RL与在线微调相结合，在主要进行RL训练的同时，针对模型难以解决的挑战性问题收集高质量演示数据进行交替微调。实验证明，ReLIFT在五个竞赛级基准测试中比零RL模型平均提升超过5.2分，且在仅使用13%演示数据的情况下即优于单纯的RL或SFT方案。这一成果展示了ReLIFT在克服RL基本限制方面的卓越可扩展性，为提升语言模型在极端复杂任务下的推理表现提供了新路径。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.07527v2",
      "published_date": "2025-06-09 08:11:20 UTC",
      "updated_date": "2025-10-04 04:07:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:23:24.931139+00:00"
    },
    {
      "arxiv_id": "2506.07524v3",
      "title": "TAI3: Testing Agent Integrity in Interpreting User Intent",
      "title_zh": "TAI3：用户意图理解中的智能体一致性测试",
      "authors": [
        "Shiwei Feng",
        "Xiangzhe Xu",
        "Xuan Chen",
        "Kaiyuan Zhang",
        "Syed Yusuf Ahmed",
        "Zian Su",
        "Mingwei Zheng",
        "Xiangyu Zhang"
      ],
      "abstract": "LLM agents are increasingly deployed to automate real-world tasks by invoking APIs through natural language instructions. While powerful, they often suffer from misinterpretation of user intent, leading to the agent's actions that diverge from the user's intended goal, especially as external toolkits evolve. Traditional software testing assumes structured inputs and thus falls short in handling the ambiguity of natural language. We introduce TAI3, an API-centric stress testing framework that systematically uncovers intent integrity violations in LLM agents. Unlike prior work focused on fixed benchmarks or adversarial inputs, TAI3 generates realistic tasks based on toolkits' documentation and applies targeted mutations to expose subtle agent errors while preserving user intent. To guide testing, we propose semantic partitioning, which organizes natural language tasks into meaningful categories based on toolkit API parameters and their equivalence classes. Within each partition, seed tasks are mutated and ranked by a lightweight predictor that estimates the likelihood of triggering agent errors. To enhance efficiency, TAI3 maintains a datatype-aware strategy memory that retrieves and adapts effective mutation patterns from past cases. Experiments on 80 toolkit APIs demonstrate that TAI3 effectively uncovers intent integrity violations, significantly outperforming baselines in both error-exposing rate and query efficiency. Moreover, TAI3 generalizes well to stronger target models using smaller LLMs for test generation, and adapts to evolving APIs across domains.",
      "tldr_zh": "该研究提出了TAI3，一个以API为中心的压力测试框架，旨在系统性地揭示大语言模型(LLM)智能体在解析用户意图时出现的完整性违规(intent integrity violations)问题。针对传统软件测试难以处理自然语言歧义以及工具包演进导致的意图误解，TAI3通过分析API文档生成真实任务，并实施有针对性的变异来暴露智能体的细微错误，同时保持原始意图不变。该框架引入了语义分区(semantic partitioning)技术，根据工具包API参数及其等价类将自然语言任务分类，并利用轻量级预测器对变异任务进行排序以指导测试。为了进一步提升效率，TAI3采用了数据类型感知的策略存储器(strategy memory)，用于检索和适配过去案例中有效的变异模式。在80个工具包API上的实验结果表明，TAI3在错误暴露率和查询效率上均显著优于基线方案。此外，TAI3展现了良好的泛化能力，能够使用小型LLM为更强大的目标模型生成测试，并能灵活适应跨领域的API动态演进。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted to NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.07524v3",
      "published_date": "2025-06-09 08:09:08 UTC",
      "updated_date": "2025-10-23 21:47:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:22:51.518618+00:00"
    },
    {
      "arxiv_id": "2506.07520v3",
      "title": "LeVo: High-Quality Song Generation with Multi-Preference Alignment",
      "title_zh": "LeVo：基于多偏好对齐的高质量歌曲生成",
      "authors": [
        "Shun Lei",
        "Yaoxun Xu",
        "Zhiwei Lin",
        "Huaicheng Zhang",
        "Wei Tan",
        "Hangting Chen",
        "Jianwei Yu",
        "Yixuan Zhang",
        "Chenyu Yang",
        "Haina Zhu",
        "Shuai Wang",
        "Zhiyong Wu",
        "Dong Yu"
      ],
      "abstract": "Recent advances in large language models (LLMs) and audio language models have significantly improved music generation, particularly in lyrics-to-song generation. However, existing approaches still struggle with the complex composition of songs and the scarcity of high-quality data, leading to limitations in audio quality, musicality, instruction following, and vocal-instrument harmony. To address these challenges, we introduce LeVo, a language model based framework consisting of LeLM and Music Codec. LeLM is capable of parallel modeling of two types of tokens: mixed tokens, which represent the combined audio of vocals and accompaniment to achieve better vocal-instrument harmony, and dual-track tokens, which separately encode vocals and accompaniment for high-quality song generation. It employs two decoder-only transformers and a modular extension training strategy to prevent interference between different token types. To further enhance musicality and instruction following ability, we introduce a multi-preference alignment method based on Direct Preference Optimization (DPO). This method handles diverse human preferences through a semi-automatic data construction process and post-training. Experimental results demonstrate that LeVo significantly outperforms existing open-source methods in both objective and subjective metrics, while performing competitively with industry systems. Ablation studies further justify the effectiveness of our designs. Audio examples and source code are available at https://levo-demo.github.io and https://github.com/tencent-ailab/songgeneration.",
      "tldr_zh": "该研究提出了LeVo，一个旨在提升歌词到歌曲生成(lyrics-to-song generation)质量的语言模型框架，主要由LeLM和Music Codec组成。LeLM采用并行建模方式处理两种类型的Token：代表人声与伴奏融合音频以增强和谐度的Mixed Tokens，以及分别编码人声与伴奏以确保生成质量的Dual-track Tokens。该框架利用两个Decoder-only Transformers并结合模块化扩展训练策略，有效避免了不同类型Token之间的相互干扰。为进一步提升音乐性(musicality)和指令遵循(instruction following)能力，研究团队引入了基于直接偏好优化(Direct Preference Optimization, DPO)的多偏好对齐方法，通过半自动数据构建流程进行后期训练以处理多样化的人类偏好。实验证明，LeVo在客观和主观指标上均显著优于现有开源方法，其性能已达到与工业界系统相媲美的水平。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.07520v3",
      "published_date": "2025-06-09 07:57:24 UTC",
      "updated_date": "2025-10-23 12:07:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:23:25.440964+00:00"
    },
    {
      "arxiv_id": "2506.09070v1",
      "title": "STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support",
      "title_zh": "STREAMINGGS：具备内存优化与架构支持的基于体素流式 3D 高斯泼溅",
      "authors": [
        "Chenqi Zhang",
        "Yu Feng",
        "Jieru Zhao",
        "Guangda Liu",
        "Wenchao Ding",
        "Chentao Wu",
        "Minyi Guo"
      ],
      "abstract": "3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and sparse Gaussian-based representation. However, 3DGS struggles to meet the real-time requirement of 90 frames per second (FPS) on resource-constrained mobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on compute efficiency but overlook memory efficiency, leading to redundant DRAM traffic. We introduce STREAMINGGS, a fully streaming 3DGS algorithm-architecture co-design that achieves fine-grained pipelining and reduces DRAM traffic by transforming from a tile-centric rendering to a memory-centric rendering. Results show that our design achieves up to 45.7 $\\times$ speedup and 62.9 $\\times$ energy savings over mobile Ampere GPUs.",
      "tldr_zh": "该研究针对 3D Gaussian Splatting (3DGS) 在移动设备上实时性能不足且显存带宽受限的问题，提出了名为 STREAMINGGS 的算法与架构协同设计方案。该框架通过从传统的以图块为中心（tile-centric）的渲染模式转变为以内存为中心（memory-centric）的渲染模式，实现了细粒度的流水线（pipelining）处理。这种基于体素（Voxel-Based）的流式架构显著减少了冗余的 DRAM 流量，有效解决了现有加速器忽视内存效率导致的性能瓶颈。实验结果表明，STREAMINGGS 相比于移动端 Ampere GPUs 实现了高达 45.7 倍的速度提升和 62.9 倍的能效比优化。该研究为在资源受限的硬件上实现 90 FPS 的 3DGS 实时渲染提供了关键的架构支持与技术路径。",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.09070v1",
      "published_date": "2025-06-09 07:51:34 UTC",
      "updated_date": "2025-06-09 07:51:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:22:55.071376+00:00"
    },
    {
      "arxiv_id": "2506.07505v1",
      "title": "Reinforcement Learning via Implicit Imitation Guidance",
      "title_zh": "基于隐式模仿引导的强化学习",
      "authors": [
        "Perry Dong",
        "Alec M. Lessing",
        "Annie S. Chen",
        "Chelsea Finn"
      ],
      "abstract": "We study the problem of sample efficient reinforcement learning, where prior data such as demonstrations are provided for initialization in lieu of a dense reward signal. A natural approach is to incorporate an imitation learning objective, either as regularization during training or to acquire a reference policy. However, imitation learning objectives can ultimately degrade long-term performance, as it does not directly align with reward maximization. In this work, we propose to use prior data solely for guiding exploration via noise added to the policy, sidestepping the need for explicit behavior cloning constraints. The key insight in our framework, Data-Guided Noise (DGN), is that demonstrations are most useful for identifying which actions should be explored, rather than forcing the policy to take certain actions. Our approach achieves up to 2-3x improvement over prior reinforcement learning from offline data methods across seven simulated continuous control tasks.",
      "tldr_zh": "该研究针对强化学习(Reinforcement Learning)中样本效率低下的问题，提出了名为Data-Guided Noise (DGN)的框架，通过隐式模仿引导来优化学习过程。传统的做法通常引入模仿学习(Imitation Learning)目标作为正则化或获取参考策略，但这往往会因为与奖励最大化目标不一致而损害长期性能。DGN的核心见解是演示数据应被用于识别哪些动作值得探索，而非强制策略执行特定动作。该框架将先验数据仅用于通过添加到策略中的噪声来引导探索，从而回避了显式的行为克隆(Behavior Cloning)约束。实验结果表明，该方法在七项模拟连续控制任务中，相比现有的从离线数据学习的强化学习方法实现了2至3倍的性能提升。这项工作为结合人类演示与奖励最大化提供了一种更高效且灵活的路径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07505v1",
      "published_date": "2025-06-09 07:32:52 UTC",
      "updated_date": "2025-06-09 07:32:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:22:56.617178+00:00"
    },
    {
      "arxiv_id": "2506.08059v1",
      "title": "CaliciBoost: Performance-Driven Evaluation of Molecular Representations for Caco-2 Permeability Prediction",
      "title_zh": "CaliciBoost：面向 Caco-2 渗透率预测的分子表示性能驱动评估",
      "authors": [
        "Huong Van Le",
        "Weibin Ren",
        "Junhong Kim",
        "Yukyung Yun",
        "Young Bin Park",
        "Young Jun Kim",
        "Bok Kyung Han",
        "Inho Choi",
        "Jong IL Park",
        "Hwi-Yeol Yun",
        "Jae-Mun Choi"
      ],
      "abstract": "Caco-2 permeability serves as a critical in vitro indicator for predicting the oral absorption of drug candidates during early-stage drug discovery. To enhance the accuracy and efficiency of computational predictions, we systematically investigated the impact of eight molecular feature representation types including 2D/3D descriptors, structural fingerprints, and deep learning-based embeddings combined with automated machine learning techniques to predict Caco-2 permeability. Using two datasets of differing scale and diversity (TDC benchmark and curated OCHEM data), we assessed model performance across representations and identified PaDEL, Mordred, and RDKit descriptors as particularly effective for Caco-2 prediction. Notably, the AutoML-based model CaliciBoost achieved the best MAE performance. Furthermore, for both PaDEL and Mordred representations, the incorporation of 3D descriptors resulted in a 15.73% reduction in MAE compared to using 2D features alone, as confirmed by feature importance analysis. These findings highlight the effectiveness of AutoML approaches in ADMET modeling and offer practical guidance for feature selection in data-limited prediction tasks.",
      "tldr_zh": "该研究系统探讨了分子特征表示对预测 Caco-2 permeability 的影响，这是评估药物候选物口服吸收的关键指标。研究人员结合自动化机器学习 (AutoML) 技术，对包括 2D/3D descriptors、structural fingerprints 以及基于深度学习的 embeddings 在内的八种分子表示形式进行了系统评估。通过在 TDC benchmark 和 OCHEM 数据集上的实验，研究确定了 PaDEL、Mordred 和 RDKit descriptors 在预测任务中表现尤为出色。基于 AutoML 开发的 CaliciBoost 模型在 MAE 指标上达到了最优性能，显著提升了预测精度。特征重要性分析进一步证实，在 PaDEL 和 Mordred 表示中引入 3D descriptors 相比仅使用 2D 特征可使 MAE 降低 15.73%。该研究验证了 AutoML 方法在 ADMET 建模中的有效性，并为数据受限条件下的分子特征选择提供了重要的实践指南。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "49 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.08059v1",
      "published_date": "2025-06-09 07:30:28 UTC",
      "updated_date": "2025-06-09 07:30:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:23:09.557954+00:00"
    },
    {
      "arxiv_id": "2506.07501v1",
      "title": "Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning",
      "title_zh": "因果演化图：挑战推理中的模型链",
      "authors": [
        "Libo Wang"
      ],
      "abstract": "In view of the problem that each subchain in the chain-of-model (CoM) relies only on the information of the previous subchain and may lose long-range dependencies due to the causal mask blocking the global context flow between multi-level subchains, this work proposes a graph of causal evolution (GoCE). Its core principle is to map the implicit token representation into a differentiable and sparse causal adjacency matrix, then permeate causal constraints through each layer of calculation using causal-masked attention and causal-MoE. By combining intervention consistency loss test and self-evolution gate, the dynamic balance between causal structure learning and adaptive updating of transformer architecture is realized. The researcher built experimental environments in sandboxes built with Claude Sonnet 4, o4-mini-high, and DeepSeek R1 respectively with the transformer variant architecture introduced in GoCE. It is evaluated on publicly available datasets including CLUTRR, CLADDER, EX-FEVER, and CausalQA and compared with the baseline LLMs. The finding proves that GoCE strengthens the transformer's ability to capture long-range causal dependencies, while the ability to self-evolve is improved. It not only surpasses the design of CoM in terms of design principles, but also provides experience for future research on causal learning and continuous adaptive improvement.",
      "tldr_zh": "针对Chain-of-Model (CoM)中子链仅依赖前一项且因因果掩码阻断全局上下文流而导致远程依赖丢失的问题，该研究提出了Graph of Causal Evolution (GoCE)。其核心原理是将隐式token表示映射为可微分且稀疏的因果邻接矩阵，并通过causal-masked attention和causal-MoE在每层计算中渗透因果约束。通过结合intervention consistency loss和self-evolution gate，该框架实现了因果结构学习与Transformer架构自适应更新之间的动态平衡。研究者在Claude Sonnet 4、o4-mini-high和DeepSeek R1构建的实验环境中对GoCE架构进行了评估。在CLUTRR、CLADDER、EX-FEVER和CausalQA等公开数据集上的测试结果证明，GoCE显著增强了模型捕捉远程因果依赖的能力，并提升了自进化性能。该研究在设计原理上超越了CoM，为未来因果学习与持续自适应改进的研究提供了重要经验。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "The relevant code has been uploaded to the publicly available GitHub repository. The link is: https://github.com/brucewang123456789/GeniusTrail/tree/main/GoCE",
      "pdf_url": "https://arxiv.org/pdf/2506.07501v1",
      "published_date": "2025-06-09 07:26:47 UTC",
      "updated_date": "2025-06-09 07:26:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:23:10.768838+00:00"
    },
    {
      "arxiv_id": "2506.07484v1",
      "title": "CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization",
      "title_zh": "CoCoA-Mix：面向上下文优化的混淆与置信度感知混合模型",
      "authors": [
        "Dasol Hong",
        "Wooju Lee",
        "Hyun Myung"
      ],
      "abstract": "Prompt tuning, which adapts vision-language models by freezing model parameters and optimizing only the prompt, has proven effective for task-specific adaptations. The core challenge in prompt tuning is improving specialization for a specific task and generalization for unseen domains. However, frozen encoders often produce misaligned features, leading to confusion between classes and limiting specialization. To overcome this issue, we propose a confusion-aware loss (CoA-loss) that improves specialization by refining the decision boundaries between confusing classes. Additionally, we mathematically demonstrate that a mixture model can enhance generalization without compromising specialization. This is achieved using confidence-aware weights (CoA-weights), which adjust the weights of each prediction in the mixture model based on its confidence within the class domains. Extensive experiments show that CoCoA-Mix, a mixture model with CoA-loss and CoA-weights, outperforms state-of-the-art methods by enhancing specialization and generalization. Our code is publicly available at https://github.com/url-kaist/CoCoA-Mix.",
      "tldr_zh": "该研究提出了CoCoA-Mix，一种用于解决视觉语言模型(VLMs)在Prompt tuning过程中面临的特定任务专业化(specialization)与未知领域泛化(generalization)冲突的混合模型。针对冻结编码器产生的特征对齐偏差和类别混淆问题，研究者设计了混淆感知损失(CoA-loss)，通过优化混淆类别间的决策边界来提升专业化能力。此外，该框架利用置信度感知权重(CoA-weights)动态调整混合模型(mixture model)中的预测权重，实现了在不损失专业化性能的情况下增强泛化性能。大量实验表明，CoCoA-Mix在多项基准测试中均优于现有先进方法，有效提升了上下文优化(Context Optimization)的效果。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 5 figures; accepted at ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.07484v1",
      "published_date": "2025-06-09 07:04:47 UTC",
      "updated_date": "2025-06-09 07:04:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:24:05.930108+00:00"
    },
    {
      "arxiv_id": "2506.07477v1",
      "title": "Premise Selection for a Lean Hammer",
      "title_zh": "面向 Lean Hammer 的前提选择",
      "authors": [
        "Thomas Zhu",
        "Joshua Clune",
        "Jeremy Avigad",
        "Albert Qiaochu Jiang",
        "Sean Welleck"
      ],
      "abstract": "Neural methods are transforming automated reasoning for proof assistants, yet integrating these advances into practical verification workflows remains challenging. Hammers are tools that interface with external automatic theorem provers to automate tedious reasoning steps. They have dramatically improved productivity in proof assistants, but the Lean proof assistant still does not have a hammer despite its growing popularity. We present LeanHammer, the first end-to-end domain-general hammer for Lean, built on a novel neural premise selection system for a hammer in dependent type theory. Unlike existing Lean premise selectors, our approach dynamically adapts to user-specific contexts and combines with symbolic proof search and reconstruction to create a practical hammer. With comprehensive evaluations, we show that our premise selector enables LeanHammer to solve 21\\% more goals relative to existing premise selectors, and generalize well to diverse domains. Our work bridges the gap between neural retrieval and symbolic reasoning, making formal verification more accessible to researchers and practitioners.",
      "tldr_zh": "该研究针对 Lean 证明助手缺乏高效自动化推理工具的问题，提出了首个端到端的通用型自动化推理工具 LeanHammer。该系统基于一种专为依赖类型论(dependent type theory)设计的创新神经前提选择(premise selection)框架，能够动态适应用户特定的上下文。LeanHammer 通过将神经检索(neural retrieval)技术与符号证明搜索(symbolic proof search)及重构相结合，实现了更实用的推理流程。实验结果显示，LeanHammer 在解决证明目标的能力上比现有前提选择器提升了 21%，且在多个领域展现出卓越的泛化性能。该工作有效桥接了神经方法与符号推理(symbolic reasoning)，为研究者和从业者提供了更易用的形式化验证手段。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.LG",
      "comment": "LeanHammer is available at https://github.com/JOSHCLUNE/LeanHammer",
      "pdf_url": "https://arxiv.org/pdf/2506.07477v1",
      "published_date": "2025-06-09 06:50:59 UTC",
      "updated_date": "2025-06-09 06:50:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:23:42.133139+00:00"
    },
    {
      "arxiv_id": "2506.07471v1",
      "title": "Ambiguity-Restrained Text-Video Representation Learning for Partially Relevant Video Retrieval",
      "title_zh": "面向部分相关视频检索的歧义受限文本-视频表征学习",
      "authors": [
        "CH Cho",
        "WJ Moon",
        "W Jun",
        "MS Jung",
        "JP Heo"
      ],
      "abstract": "Partially Relevant Video Retrieval~(PRVR) aims to retrieve a video where a specific segment is relevant to a given text query. Typical training processes of PRVR assume a one-to-one relationship where each text query is relevant to only one video. However, we point out the inherent ambiguity between text and video content based on their conceptual scope and propose a framework that incorporates this ambiguity into the model learning process. Specifically, we propose Ambiguity-Restrained representation Learning~(ARL) to address ambiguous text-video pairs. Initially, ARL detects ambiguous pairs based on two criteria: uncertainty and similarity. Uncertainty represents whether instances include commonly shared context across the dataset, while similarity indicates pair-wise semantic overlap. Then, with the detected ambiguous pairs, our ARL hierarchically learns the semantic relationship via multi-positive contrastive learning and dual triplet margin loss. Additionally, we delve into fine-grained relationships within the video instances. Unlike typical training at the text-video level, where pairwise information is provided, we address the inherent ambiguity within frames of the same untrimmed video, which often contains multiple contexts. This allows us to further enhance learning at the text-frame level. Lastly, we propose cross-model ambiguity detection to mitigate the error propagation that occurs when a single model is employed to detect ambiguous pairs for its training. With all components combined, our proposed method demonstrates its effectiveness in PRVR.",
      "tldr_zh": "该研究针对部分相关视频检索(Partially Relevant Video Retrieval, PRVR)中存在的文本与视频内容固有的歧义性问题，提出了名为Ambiguity-Restrained representation Learning (ARL) 的表示学习框架。ARL 框架首先基于不确定性(uncertainty)和相似度(similarity)两个标准来检测歧义对，从而识别出包含通用语境或语义重叠的实例。随后，该方法利用多正样本对比学习(multi-positive contrastive learning)和双三元组边界损失(dual triplet margin loss)分层学习复杂的语义关系。为了进一步优化，研究者还在文本-帧层面处理未剪辑视频内部的固有歧义，并引入了跨模型歧义检测(cross-model ambiguity detection)机制以减少误差传播。实验结果证明，该方法在PRVR任务中通过有效约束文本-视频表示学习中的歧义性，显著提升了检索性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to AAAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.07471v1",
      "published_date": "2025-06-09 06:44:45 UTC",
      "updated_date": "2025-06-09 06:44:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:23:46.641757+00:00"
    },
    {
      "arxiv_id": "2507.21071v1",
      "title": "FingerTip 20K: A Benchmark for Proactive and Personalized Mobile LLM Agents",
      "title_zh": "FingerTip 20K：面向主动式与个性化移动端大语言模型智能体的基准测试",
      "authors": [
        "Qinglong Yang",
        "Haoming Li",
        "Haotian Zhao",
        "Xiaokai Yan",
        "Jingtao Ding",
        "Fengli Xu",
        "Yong Li"
      ],
      "abstract": "Mobile GUI agents are becoming critical tools for enhancing human-device interaction efficiency, with multimodal large language models (MLLMs) emerging as dominant paradigms in this domain. Current agents, however, are limited to following explicit human instructions, resulting in insufficient capability for proactive intent anticipation. Additionally, these agents fail to leverage the contextual information associated with users during task execution, thereby neglecting potentially vast differences in user preferences. To address these challenges, we introduce the FingerTip benchmark. It contains two new tracks: proactive task suggestions by analyzing environment observation and users' previous intents, and personalized task execution by catering to users' action preferences. We collected unique human demonstrations of multi-step Android device interactions across a variety of everyday apps. These demonstrations are not isolated but are continuously acquired from the users' long-term usage in their real lives, and encompass essential user-related contextual information. Our experiments reveal challenges of the tasks we propose. The model fine-tuned with the data we collected effectively utilized user information and achieved good results, highlighting the potential of our approach in building more user-oriented mobile GUI agents. Our code is open-source at https://anonymous.4open.science/r/FingerTip-57B8 for reproducibility.",
      "tldr_zh": "该研究针对移动端图形用户界面(GUI)智能体在主动意图预测和个性化偏好利用方面的局限性，提出了FingerTip benchmark。该基准测试包含两个核心赛道：一是通过分析环境观察和用户历史意图实现主动任务建议(proactive task suggestions)，二是通过迎合用户操作偏好实现个性化任务执行(personalized task execution)。研究团队收集了真实生活场景下长期使用的Android设备多步交互演示数据，并整合了关键的用户上下文信息。实验结果揭示了这些任务的挑战性，而利用该数据集微调的模型在整合用户信息方面表现出色，取得了良好的实验结果。这项工作突显了该方法在构建更具用户导向、能主动感知并适应个人需求的移动端多模态大语言模型(MLLM)智能体方面的巨大潜力。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21071v1",
      "published_date": "2025-06-09 06:38:41 UTC",
      "updated_date": "2025-06-09 06:38:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:23:51.747397+00:00"
    },
    {
      "arxiv_id": "2506.11116v1",
      "title": "Infinity Instruct: Scaling Instruction Selection and Synthesis to Enhance Language Models",
      "title_zh": "Infinity Instruct：通过规模化指令筛选与合成增强语言模型",
      "authors": [
        "Jijie Li",
        "Li Du",
        "Hanyu Zhao",
        "Bo-wen Zhang",
        "Liangdong Wang",
        "Boyan Gao",
        "Guang Liu",
        "Yonghua Lin"
      ],
      "abstract": "Large Language Models (LLMs) demonstrate strong performance in real-world applications, yet existing open-source instruction datasets often concentrate on narrow domains, such as mathematics or coding, limiting generalization and widening the gap with proprietary models. To bridge this gap, we introduce Infinity-Instruct, a high-quality instruction dataset designed to enhance both foundational and chat capabilities of LLMs through a two-phase pipeline. In Phase 1, we curate 7.4M high-quality foundational instructions (InfInstruct-F-7.4M) from over 100M samples using hybrid data selection techniques. In Phase 2, we synthesize 1.5M high-quality chat instructions (InfInstruct-G-1.5M) through a two-stage process involving instruction selection, evolution, and diagnostic filtering. We empirically evaluate Infinity-Instruct by fine-tuning several open-source models, including Mistral, LLaMA, Qwen, and Yi, and observe substantial performance gains across both foundational and instruction following benchmarks, consistently surpassing official instruction-tuned counterparts. Notably, InfInstruct-LLaMA3.1-70B outperforms GPT-4-0314 by 8.6\\% on instruction following tasks while achieving comparable foundational performance. These results underscore the synergy between foundational and chat training and offer new insights into holistic LLM development. Our dataset\\footnote{https://huggingface.co/datasets/BAAI/Infinity-Instruct} and codes\\footnote{https://gitee.com/li-touch/infinity-instruct} have been publicly released.",
      "tldr_zh": "该研究针对现有开源指令数据集领域覆盖不足的问题，提出了高质量指令数据集 Infinity-Instruct，旨在通过两阶段流水线提升大语言模型(LLMs)的基础能力和对话表现。第一阶段利用混合数据筛选技术从超过一亿条样本中精选出740万条基础指令(InfInstruct-F-7.4M)，第二阶段则通过指令筛选、演化和诊断过滤合成了150万条高质量对话指令(InfInstruct-G-1.5M)。实验结果显示，经该数据集微调的 Mistral、LLaMA、Qwen 和 Yi 等模型在基础及指令遵循 benchmarks 上均有显著提升。其中 InfInstruct-LLaMA3.1-70B 在指令遵循任务上比 GPT-4-0314 提高 8.6%，且基础性能表现相当。这些发现揭示了基础训练与对话训练之间的协同效应，为全面提升 LLM 性能提供了新的开发路径和开源资源。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11116v1",
      "published_date": "2025-06-09 06:37:15 UTC",
      "updated_date": "2025-06-09 06:37:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:24:08.111529+00:00"
    },
    {
      "arxiv_id": "2506.13777v1",
      "title": "A Survey of Physics-Informed AI for Complex Urban Systems",
      "title_zh": "复杂城市系统中的物理驱动人工智能综述",
      "authors": [
        "En Xu",
        "Huandong Wang",
        "Yunke Zhang",
        "Sibo Li",
        "Yinzhou Tang",
        "Zhilun Zhou",
        "Yuming Lin",
        "Yuan Yuan",
        "Xiaochen Fan",
        "Jingtao Ding",
        "Yong Li"
      ],
      "abstract": "Urban systems are typical examples of complex systems, where the integration of physics-based modeling with artificial intelligence (AI) presents a promising paradigm for enhancing predictive accuracy, interpretability, and decision-making. In this context, AI excels at capturing complex, nonlinear relationships, while physics-based models ensure consistency with real-world laws and provide interpretable insights. We provide a comprehensive review of physics-informed AI methods in urban applications. The proposed taxonomy categorizes existing approaches into three paradigms - Physics-Integrated AI, Physics-AI Hybrid Ensemble, and AI-Integrated Physics - and further details seven representative methods. This classification clarifies the varying degrees and directions of physics-AI integration, guiding the selection and development of appropriate methods based on application needs and data availability. We systematically examine their applications across eight key urban domains: energy, environment, economy, transportation, information, public services, emergency management, and the urban system as a whole. Our analysis highlights how these methodologies leverage physical laws and data-driven models to address urban challenges, enhancing system reliability, efficiency, and adaptability. By synthesizing existing methodologies and their urban applications, we identify critical gaps and outline future research directions, paving the way toward next-generation intelligent urban system modeling.",
      "tldr_zh": "该综述探讨了物理模型与人工智能(AI)在复杂城市系统中的集成，旨在提升预测准确性、可解释性和决策能力。文章提出了一个系统性的分类法，将现有方法归纳为 Physics-Integrated AI、Physics-AI Hybrid Ensemble 以及 AI-Integrated Physics 三种范式，并详细介绍了七种代表性方法。研究全面考察了这些方法在能源、环境、经济、交通、信息、公共服务、应急管理及整体城市系统八大关键领域的具体应用。通过分析物理定律与数据驱动模型的协同作用，该研究展示了如何利用这些方法应对城市挑战，并显著增强系统的可靠性、效率和适应性。最后，文章通过总结现有方法论及其应用，识别了关键研究空白并指明了未来方向，为构建下一代智能城市系统建模提供了指导。",
      "categories": [
        "physics.soc-ph",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "physics.soc-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.13777v1",
      "published_date": "2025-06-09 06:19:14 UTC",
      "updated_date": "2025-06-09 06:19:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:23:59.392553+00:00"
    },
    {
      "arxiv_id": "2506.07464v4",
      "title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO",
      "title_zh": "DeepVideo-R1：基于难度感知回归 GRPO 的视频强化微调",
      "authors": [
        "Jinyoung Park",
        "Jeehye Na",
        "Jinyoung Kim",
        "Hyunwoo J. Kim"
      ],
      "abstract": "Recent works have demonstrated the effectiveness of reinforcement learning (RL)-based post-training for enhancing the reasoning capabilities of large language models (LLMs). In particular, Group Relative Policy Optimization (GRPO) has shown impressive success using a PPO-style reinforcement algorithm with group-normalized rewards. However, the effectiveness of GRPO in Video Large Language Models (VideoLLMs) has still been less studyed. In this paper, we explore GRPO and identify two problems that deteriorate the effective learning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate these challenges, we propose DeepVideo-R1, a video large language model trained with Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation. Reg-GRPO reformulates the GRPO loss function into a regression task that directly predicts the advantage in GRPO, eliminating the need for safeguards such as the clipping and min functions. It directly aligns the model with advantages, providing guidance to prefer better ones. The difficulty-aware data augmentation strategy augments input prompts/videos to locate the difficulty of samples at solvable difficulty levels, enabling diverse reward signals. Our experimental results show that our approach significantly improves video reasoning performance across multiple benchmarks.",
      "tldr_zh": "该研究针对强化学习（RL）后训练技术在视频大语言模型（VideoLLMs）中应用不足的问题，指出了现有Group Relative Policy Optimization (GRPO)算法在处理视频任务时存在的保障措施依赖和优势消失（vanishing advantage）两大挑战。为此，作者提出了DeepVideo-R1模型，引入了回归式GRPO（Regressive GRPO, Reg-GRPO）以及难度感知的数据增强（difficulty-aware data augmentation）策略。Reg-GRPO通过将损失函数重构为回归任务来直接预测优势值，消除了对裁剪（clipping）和最小值（min）函数的依赖，从而使模型更直接地与优势信号对齐。难度感知的数据增强策略则通过调整输入提示和视频的难度，确保样本处于可解决的水平，从而提供多样化的奖励信号。实验结果表明，DeepVideo-R1在多个基准测试中显著提升了视频推理性能。这一研究为优化视频领域的强化学习训练提供了新的思路，证明了回归式策略在复杂多模态推理任务中的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.07464v4",
      "published_date": "2025-06-09 06:15:54 UTC",
      "updated_date": "2025-10-31 12:13:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:24:02.553058+00:00"
    },
    {
      "arxiv_id": "2506.07463v1",
      "title": "CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language Models",
      "title_zh": "CCI4.0：旨在提升大语言模型推理能力的双语预训练数据集",
      "authors": [
        "Guang Liu",
        "Liangdong Wang",
        "Jijie Li",
        "Yang Yu",
        "Yao Xu",
        "Jiabei Chen",
        "Yu Bai",
        "Feng Liao",
        "Yonghua Lin"
      ],
      "abstract": "We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered for superior data quality and diverse human-like reasoning trajectory. CCI4.0 occupies roughly $35$ TB of disk space and comprises two sub-datasets: CCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a $5.2$ TB carefully curated Chinese web corpus, a $22.5$ TB English subset from Nemotron-CC, and diverse sources from math, wiki, arxiv, and code. Although these data are mostly sourced from well-processed datasets, the quality standards of various domains are dynamic and require extensive expert experience and labor to process. So, we propose a novel pipeline justifying data quality mainly based on models through two-stage deduplication, multiclassifier quality scoring, and domain-aware fluency filtering. We extract $4.5$ billion pieces of CoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the distillation of CoT from larger models, our proposed staged CoT extraction exemplifies diverse reasoning patterns and significantly decreases the possibility of hallucination. Empirical evaluations demonstrate that LLMs pre-trained in CCI4.0 benefit from cleaner, more reliable training signals, yielding consistent improvements in downstream tasks, especially in math and code reflection tasks. Our results underscore the critical role of rigorous data curation and human thinking templates in advancing LLM performance, shedding some light on automatically processing pretraining corpora.",
      "tldr_zh": "该研究介绍了 CCI4.0，这是一个旨在增强大语言模型 (LLMs) 推理能力的大规模双语预训练数据集，总存储规模约 35 TB。数据集由 CCI4.0-M2-Base 和 CCI4.0-M2-CoT 两个子集组成，整合了精心筛选的中英文网页语料以及数学、维基百科、arXiv 和代码等多样化数据源。研究团队开发了一套基于模型的自动化处理管线，通过两阶段去重、多分类质量评分和领域感知流畅度过滤来优化数据质量。CCI4.0-M2-CoT 包含了 45 亿条链式思维 (Chain-of-Thought) 模板，采用分阶段提取而非模型蒸馏的方法，在展现多样化推理模式的同时显著降低了幻觉风险。实验证明，在 CCI4.0 上预训练的模型在数学和代码反射等下游任务中获得了持续的性能提升。该研究强调了严谨的数据治理和人类思维模板对于提升大语言模型性能的关键作用，为预训练语料的自动化处理提供了重要参考。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07463v1",
      "published_date": "2025-06-09 06:14:19 UTC",
      "updated_date": "2025-06-09 06:14:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:24:06.558927+00:00"
    },
    {
      "arxiv_id": "2506.07458v2",
      "title": "KScope: A Framework for Characterizing the Knowledge Status of Language Models",
      "title_zh": "KScope：语言模型知识状态表征框架",
      "authors": [
        "Yuxin Xiao",
        "Shan Chen",
        "Jack Gallifant",
        "Danielle Bitterman",
        "Thomas Hartvigsen",
        "Marzyeh Ghassemi"
      ],
      "abstract": "Characterizing a large language model's (LLM's) knowledge of a given question is challenging. As a result, prior work has primarily examined LLM behavior under knowledge conflicts, where the model's internal parametric memory contradicts information in the external context. However, this does not fully reflect how well the model knows the answer to the question. In this paper, we first introduce a taxonomy of five knowledge statuses based on the consistency and correctness of LLM knowledge modes. We then propose KScope, a hierarchical framework of statistical tests that progressively refines hypotheses about knowledge modes and characterizes LLM knowledge into one of these five statuses. We apply KScope to nine LLMs across four datasets and systematically establish: (1) Supporting context narrows knowledge gaps across models. (2) Context features related to difficulty, relevance, and familiarity drive successful knowledge updates. (3) LLMs exhibit similar feature preferences when partially correct or conflicted, but diverge sharply when consistently wrong. (4) Context summarization constrained by our feature analysis, together with enhanced credibility, further improves update effectiveness and generalizes across LLMs.",
      "tldr_zh": "该研究提出了 KScope，一个用于刻画大语言模型 (LLMs) 知识状态的分层框架，旨在解决现有研究在描述模型对特定问题掌握程度方面的局限性。研究首先基于知识模式的一致性和正确性，定义了五种不同的知识状态 (Knowledge Statuses)，并利用 KScope 通过一系列统计测试 (Statistical Tests) 逐步细化假设，从而将模型知识准确归类。通过对 9 个 LLMs 和 4 个数据集的系统评估，研究发现支持性上下文 (Supporting Context) 能有效缩小不同模型间的知识差距。实验进一步揭示，与难度 (Difficulty)、相关性 (Relevance) 和熟悉度 (Familiarity) 相关的上下文特征是驱动知识成功更新的关键动力。此外，LLMs 在部分正确或存在冲突时表现出相似的特征偏好，但在持续错误时则呈现显著差异。最后，该研究证明结合特征分析的上下文摘要与可信度增强技术，能显著提升模型更新的有效性并具备良好的跨模型泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.07458v2",
      "published_date": "2025-06-09 06:06:05 UTC",
      "updated_date": "2025-10-16 06:05:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:24:16.111540+00:00"
    },
    {
      "arxiv_id": "2506.07454v2",
      "title": "Language-Grounded Hierarchical Planning and Execution with Multi-Robot 3D Scene Graphs",
      "title_zh": "基于多机器人 3D 场景图的语言引导式分层规划与执行",
      "authors": [
        "Jared Strader",
        "Aaron Ray",
        "Jacob Arkin",
        "Mason B. Peterson",
        "Yun Chang",
        "Nathan Hughes",
        "Christopher Bradley",
        "Yi Xuan Jia",
        "Carlos Nieto-Granda",
        "Rajat Talak",
        "Chuchu Fan",
        "Luca Carlone",
        "Jonathan P. How",
        "Nicholas Roy"
      ],
      "abstract": "In this paper, we introduce a multi-robot system that integrates mapping, localization, and task and motion planning (TAMP) enabled by 3D scene graphs to execute complex instructions expressed in natural language. Our system builds a shared 3D scene graph incorporating an open-set object-based map, which is leveraged for multi-robot 3D scene graph fusion. This representation supports real-time, view-invariant relocalization (via the object-based map) and planning (via the 3D scene graph), allowing a team of robots to reason about their surroundings and execute complex tasks. Additionally, we introduce a planning approach that translates operator intent into Planning Domain Definition Language (PDDL) goals using a Large Language Model (LLM) by leveraging context from the shared 3D scene graph and robot capabilities. We provide an experimental assessment of the performance of our system on real-world tasks in large-scale, outdoor environments. A supplementary video is available at https://youtu.be/8xbGGOLfLAY.",
      "tldr_zh": "该研究引入了一种集成了建图、定位以及任务与运动规划(TAMP)的多机器人系统，利用3D Scene Graphs来执行以自然语言表达的复杂指令。该系统通过构建包含开放集对象地图的共享3D Scene Graph，实现了多机器人3D Scene Graph Fusion，从而支持实时且视点不变的重定位与规划。此外，研究还提出了一种分层规划方法，利用大型语言模型(LLM)结合场景图上下文和机器人能力，将操作员意图转化为PDDL目标。实验在大型室外环境的真实任务中评估了系统性能，验证了该框架在处理大规模场景下多机器人协同推理与执行方面的有效性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "12 pages, 4 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.07454v2",
      "published_date": "2025-06-09 06:02:34 UTC",
      "updated_date": "2025-07-10 23:26:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:24:25.849570+00:00"
    },
    {
      "arxiv_id": "2506.07452v2",
      "title": "When Style Breaks Safety: Defending LLMs Against Superficial Style Alignment",
      "title_zh": "当风格破坏安全：防御大语言模型的表层风格对齐",
      "authors": [
        "Yuxin Xiao",
        "Sana Tonekaboni",
        "Walter Gerych",
        "Vinith Suriyakumar",
        "Marzyeh Ghassemi"
      ],
      "abstract": "Large language models (LLMs) can be prompted with specific styles (e.g., formatting responses as lists), including in malicious queries. Prior jailbreak research mainly augments these queries with additional string transformations to maximize attack success rate (ASR). However, the impact of style patterns in the original queries that are semantically irrelevant to the malicious intent remains unclear. In this work, we seek to understand whether style patterns compromise LLM safety, how superficial style alignment increases model vulnerability, and how best to mitigate these risks during alignment. We first define ASR inflation as the increase in ASR due to style patterns in existing jailbreak benchmark queries. By evaluating 32 LLMs across seven benchmarks, we find that nearly all models exhibit ASR inflation. Notably, the inflation correlates with an LLM's relative attention to style patterns, which also overlap more with its instruction-tuning data when inflation occurs. We then investigate superficial style alignment, and find that fine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of those same styles. Finally, we propose SafeStyle, a defense strategy that incorporates a small amount of safety training data augmented to match the distribution of style patterns in the fine-tuning data. Across three LLMs, six fine-tuning style settings, and two real-world instruction-tuning datasets, SafeStyle consistently outperforms baselines in maintaining LLM safety.",
      "tldr_zh": "该研究探讨了大型语言模型(LLMs)中风格模式(style patterns)对安全性的影响，揭示了恶意查询中语义无关的格式风格如何导致攻击成功率(Attack Success Rate, ASR)显著上升。通过对32个LLMs的系统评估，研究发现几乎所有模型都存在ASR通胀现象，且这种脆弱性与模型对风格模式的注意力及其指令微调(instruction-tuning)数据的分布重合度密切相关。研究进一步指出，表面风格对齐(superficial style alignment)的微调过程会使模型更容易受到相同风格的越狱攻击(jailbreaks)。针对这一问题，作者提出了SafeStyle防御策略，通过将少量安全训练数据进行风格增强，使其与微调数据的风格分布保持一致。实验证明，SafeStyle在多种模型和真实微调数据集下均能显著优于基线方法，在维持模型功能的同时有效提升了LLMs的安全性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07452v2",
      "published_date": "2025-06-09 05:57:39 UTC",
      "updated_date": "2025-10-16 06:50:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:24:51.938083+00:00"
    },
    {
      "arxiv_id": "2506.07450v1",
      "title": "Efficient Generation of Diverse Cooperative Agents with World Models",
      "title_zh": "基于世界模型的高效多样化协作智能体生成",
      "authors": [
        "Yi Loo",
        "Akshunn Trivedi",
        "Malika Meghjani"
      ],
      "abstract": "A major bottleneck in the training process for Zero-Shot Coordination (ZSC) agents is the generation of partner agents that are diverse in collaborative conventions. Current Cross-play Minimization (XPM) methods for population generation can be very computationally expensive and sample inefficient as the training objective requires sampling multiple types of trajectories. Each partner agent in the population is also trained from scratch, despite all of the partners in the population learning policies of the same coordination task. In this work, we propose that simulated trajectories from the dynamics model of an environment can drastically speed up the training process for XPM methods. We introduce XPM-WM, a framework for generating simulated trajectories for XPM via a learned World Model (WM). We show XPM with simulated trajectories removes the need to sample multiple trajectories. In addition, we show our proposed method can effectively generate partners with diverse conventions that match the performance of previous methods in terms of SP population training reward as well as training partners for ZSC agents. Our method is thus, significantly more sample efficient and scalable to a larger number of partners.",
      "tldr_zh": "该研究针对零样本协调 (Zero-Shot Coordination, ZSC) 智能体训练中伴侣智能体生成效率低下的问题，提出了 XPM-WM 框架。该框架将跨博弈最小化 (Cross-play Minimization, XPM) 与学习到的世界模型 (World Model, WM) 相结合，利用环境动力学模型的模拟轨迹显著加速了多样化伴侣种群的生成过程。通过 WM 生成的模拟数据，XPM-WM 消除了采样多类真实轨迹的必要性，从而大幅提升了样本效率并降低了计算成本。实验结果显示，该方法在生成具有多样化协作惯例的伴侣方面表现出色，其种群训练奖励和 ZSC 训练性能均达到了现有先进方法的水平。总的来说，XPM-WM 为大规模生成协作智能体提供了一种高效且可扩展的解决方案，有效缓解了 ZSC 领域的计算瓶颈。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07450v1",
      "published_date": "2025-06-09 05:52:45 UTC",
      "updated_date": "2025-06-09 05:52:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:24:44.138949+00:00"
    },
    {
      "arxiv_id": "2506.07448v1",
      "title": "Extending Epistemic Uncertainty Beyond Parameters Would Assist in Designing Reliable LLMs",
      "title_zh": "将认知不确定性扩展至参数之外将有助于设计可靠的大语言模型",
      "authors": [
        "T. Duy Nguyen-Hien",
        "Desi R. Ivanova",
        "Yee Whye Teh",
        "Wee Sun Lee"
      ],
      "abstract": "Although large language models (LLMs) are highly interactive and extendable, current approaches to ensure reliability in deployments remain mostly limited to rejecting outputs with high uncertainty in order to avoid misinformation. This conservative strategy reflects the current lack of tools to systematically distinguish and respond to different sources of uncertainty. In this paper, we advocate for the adoption of Bayesian Modeling of Experiments -- a framework that provides a coherent foundation to reason about uncertainty and clarify the reducibility of uncertainty -- for managing and proactively addressing uncertainty that arises in LLM deployments. This framework enables LLMs and their users to take contextually appropriate steps, such as requesting clarification, retrieving external information, or refining inputs. By supporting active resolution rather than passive avoidance, it opens the door to more reliable, transparent, and broadly applicable LLM systems, particularly in high-stakes, real-world settings.",
      "tldr_zh": "该研究指出，当前大型语言模型（LLMs）的可靠性保障主要局限于拒绝高不确定性输出，缺乏系统化区分和响应不同来源不确定性的有效工具。为此，本文倡议采用实验贝叶斯建模（Bayesian Modeling of Experiments）框架，为推导不确定性并阐明其可还原性（reducibility）提供了连贯的理论基础。该框架将认知不确定性（Epistemic Uncertainty）扩展到模型参数之外，使系统能够根据具体情境采取主动措施，如请求澄清、检索外部信息或精炼输入。这种从被动规避向主动解决的转变，能够显著提升LLM在处理复杂任务时的透明度与可靠性。该研究为在高风险、真实的现实场景中构建更具适应性的模型系统提供了重要路径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07448v1",
      "published_date": "2025-06-09 05:52:03 UTC",
      "updated_date": "2025-06-09 05:52:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:24:49.558537+00:00"
    },
    {
      "arxiv_id": "2506.07449v1",
      "title": "LlamaRec-LKG-RAG: A Single-Pass, Learnable Knowledge Graph-RAG Framework for LLM-Based Ranking",
      "title_zh": "LlamaRec-LKG-RAG：面向大语言模型排序的单阶段可学习知识图谱检索增强生成框架",
      "authors": [
        "Vahid Azizi",
        "Fatemeh Koochaki"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have driven their adoption in recommender systems through Retrieval-Augmented Generation (RAG) frameworks. However, existing RAG approaches predominantly rely on flat, similarity-based retrieval that fails to leverage the rich relational structure inherent in user-item interactions. We introduce LlamaRec-LKG-RAG, a novel single-pass, end-to-end trainable framework that integrates personalized knowledge graph context into LLM-based recommendation ranking. Our approach extends the LlamaRec architecture by incorporating a lightweight user preference module that dynamically identifies salient relation paths within a heterogeneous knowledge graph constructed from user behavior and item metadata. These personalized subgraphs are seamlessly integrated into prompts for a fine-tuned Llama-2 model, enabling efficient and interpretable recommendations through a unified inference step. Comprehensive experiments on ML-100K and Amazon Beauty datasets demonstrate consistent and significant improvements over LlamaRec across key ranking metrics (MRR, NDCG, Recall). LlamaRec-LKG-RAG demonstrates the critical value of structured reasoning in LLM-based recommendations and establishes a foundation for scalable, knowledge-aware personalization in next-generation recommender systems. Code is available at~\\href{https://github.com/VahidAz/LlamaRec-LKG-RAG}{repository}.",
      "tldr_zh": "该研究提出了 LlamaRec-LKG-RAG，一种单次传递（Single-Pass）且端到端可训练的框架，旨在将个性化知识图谱（Knowledge Graph）上下文集成到基于大语言模型（LLM）的推荐排序中。该方法通过扩展 LlamaRec 架构并引入轻量级用户偏好模块，从异构知识图谱中动态识别显著的关系路径，解决了现有检索增强生成（RAG）方案在捕捉用户-物品交互关系结构方面的局限性。这些个性化子图被无缝嵌入到微调后的 Llama-2 模型提示中，从而在统一的推理步骤内实现高效且具可解释性的推荐生成。在 ML-100K 和 Amazon Beauty 数据集上的实验证明，该框架在 MRR、NDCG 和 Recall 等关键指标上均显著优于 LlamaRec 基线。该工作强调了结构化推理在 LLM 推荐系统中的重要价值，为构建可扩展且具备知识感知能力的个性化推荐系统奠定了基础。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07449v1",
      "published_date": "2025-06-09 05:52:03 UTC",
      "updated_date": "2025-06-09 05:52:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:24:50.375605+00:00"
    },
    {
      "arxiv_id": "2506.07446v1",
      "title": "Fact in Fragments: Deconstructing Complex Claims via LLM-based Atomic Fact Extraction and Verification",
      "title_zh": "碎片中的事实：通过基于大语言模型的原子事实提取与验证解构复杂主张",
      "authors": [
        "Liwen Zheng",
        "Chaozhuo Li",
        "Zheng Liu",
        "Feiran Huang",
        "Haoran Jia",
        "Zaisheng Ye",
        "Xi Zhang"
      ],
      "abstract": "Fact verification plays a vital role in combating misinformation by assessing the veracity of claims through evidence retrieval and reasoning. However, traditional methods struggle with complex claims requiring multi-hop reasoning over fragmented evidence, as they often rely on static decomposition strategies and surface-level semantic retrieval, which fail to capture the nuanced structure and intent of the claim. This results in accumulated reasoning errors, noisy evidence contamination, and limited adaptability to diverse claims, ultimately undermining verification accuracy in complex scenarios. To address this, we propose Atomic Fact Extraction and Verification (AFEV), a novel framework that iteratively decomposes complex claims into atomic facts, enabling fine-grained retrieval and adaptive reasoning. AFEV dynamically refines claim understanding and reduces error propagation through iterative fact extraction, reranks evidence to filter noise, and leverages context-specific demonstrations to guide the reasoning process. Extensive experiments on five benchmark datasets demonstrate that AFEV achieves state-of-the-art performance in both accuracy and interpretability.",
      "tldr_zh": "该研究针对传统事实核查(fact verification)方法在处理需多跳推理(multi-hop reasoning)的复杂声明时存在的推理错误累积和证据噪声干扰等挑战，提出了名为Atomic Fact Extraction and Verification (AFEV)的新型框架。AFEV的核心思想是将复杂声明迭代地分解为原子事实(atomic facts)，从而实现细粒度的信息检索和自适应推理。该框架通过迭代式的事实提取过程动态优化对声明的理解，利用证据重排序(reranking)技术过滤无关噪声，并结合特定上下文的示例(demonstrations)来引导推理过程。在五个基准数据集上的实验结果证明，AFEV在验证准确性和可解释性(interpretability)方面均取得了state-of-the-art的优异表现。该方法有效提升了模型在处理碎片化证据时的鲁棒性，为复杂场景下的事实校验提供了可靠的解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07446v1",
      "published_date": "2025-06-09 05:49:43 UTC",
      "updated_date": "2025-06-09 05:49:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:24:53.036882+00:00"
    },
    {
      "arxiv_id": "2506.07443v1",
      "title": "LegalReasoner: Step-wised Verification-Correction for Legal Judgment Reasoning",
      "title_zh": "LegalReasoner：面向法律裁判推理的逐步验证与修正",
      "authors": [
        "Weijie Shi",
        "Han Zhu",
        "Jiaming Ji",
        "Mengze Li",
        "Jipeng Zhang",
        "Ruiyuan Zhang",
        "Jia Zhu",
        "Jiajie Xu",
        "Sirui Han",
        "Yike Guo"
      ],
      "abstract": "Legal judgment prediction (LJP) aims to function as a judge by making final rulings based on case claims and facts, which plays a vital role in the judicial domain for supporting court decision-making and improving judicial efficiency. However, existing methods often struggle with logical errors when conducting complex legal reasoning. We propose LegalReasoner, which enhances LJP reliability through step-wise verification and correction of the reasoning process. Specifically, it first identifies dispute points to decompose complex cases, and then conducts step-wise reasoning while employing a process verifier to validate each step's logic from correctness, progressiveness, and potential perspectives. When errors are detected, expert-designed attribution and resolution strategies are applied for correction. To fine-tune LegalReasoner, we release the LegalHK dataset, containing 58,130 Hong Kong court cases with detailed annotations of dispute points, step-by-step reasoning chains, and process verification labels. Experiments demonstrate that LegalReasoner significantly improves concordance with court decisions from 72.37 to 80.27 on LLAMA-3.1-70B. The data is available at https://huggingface.co/datasets/weijiezz/LegalHK.",
      "tldr_zh": "该研究针对法律判决预测(Legal judgment prediction)在复杂法律推理中面临的逻辑错误挑战，提出了LegalReasoner框架，旨在通过分步验证与纠正机制提升推理的可靠性。该方法首先识别争议点(dispute points)以分解复杂案件，随后在分步推理过程中利用过程验证器(process verifier)从正确性、递进性和潜在视角对每一步逻辑进行校验。当检测到错误时，系统会采用专家设计的归因与解决策略进行实时修正。此外，研究者发布了包含5.8万余宗香港法院案例的LegalHK数据集，提供了详细的争议点标注、推理链和验证标签。实验结果表明，LegalReasoner在LLAMA-3.1-70B模型上的判决一致性从72.37显著提升至80.27。这一成果为辅助法院决策和提高司法效率提供了更具逻辑性和可信度的技术路径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07443v1",
      "published_date": "2025-06-09 05:48:35 UTC",
      "updated_date": "2025-06-09 05:48:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:24:58.544940+00:00"
    },
    {
      "arxiv_id": "2506.07436v1",
      "title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
      "title_zh": "从提示到防护：多模态大语言模型在建筑施工危险识别中的对比研究",
      "authors": [
        "Nishi Chaudhary",
        "S M Jamil Uddin",
        "Sathvik Sharath Chandra",
        "Anto Ovid",
        "Alex Albert"
      ],
      "abstract": "The recent emergence of multimodal large language models (LLMs) has introduced new opportunities for improving visual hazard recognition on construction sites. Unlike traditional computer vision models that rely on domain-specific training and extensive datasets, modern LLMs can interpret and describe complex visual scenes using simple natural language prompts. However, despite growing interest in their applications, there has been limited investigation into how different LLMs perform in safety-critical visual tasks within the construction domain. To address this gap, this study conducts a comparative evaluation of five state-of-the-art LLMs: Claude-3 Opus, GPT-4.5, GPT-4o, GPT-o3, and Gemini 2.0 Pro, to assess their ability to identify potential hazards from real-world construction images. Each model was tested under three prompting strategies: zero-shot, few-shot, and chain-of-thought (CoT). Zero-shot prompting involved minimal instruction, few-shot incorporated basic safety context and a hazard source mnemonic, and CoT provided step-by-step reasoning examples to scaffold model thinking. Quantitative analysis was performed using precision, recall, and F1-score metrics across all conditions. Results reveal that prompting strategy significantly influenced performance, with CoT prompting consistently producing higher accuracy across models. Additionally, LLM performance varied under different conditions, with GPT-4.5 and GPT-o3 outperforming others in most settings. The findings also demonstrate the critical role of prompt design in enhancing the accuracy and consistency of multimodal LLMs for construction safety applications. This study offers actionable insights into the integration of prompt engineering and LLMs for practical hazard recognition, contributing to the development of more reliable AI-assisted safety systems.",
      "tldr_zh": "该研究针对建筑施工现场的视觉危险识别，对多模态大语言模型 (Multimodal LLMs) 的应用效能进行了系统性的比较评估。研究选取了 Claude-3 Opus、GPT-4.5、GPT-4o、GPT-o3 和 Gemini 2.0 Pro 五种前沿模型，通过零样本 (Zero-shot)、少样本 (Few-shot) 和链式思维 (Chain-of-Thought, CoT) 三种提示策略，测试其从真实施工图像中识别危险的能力。实验结果表明，提示策略对模型表现具有显著影响，其中 CoT 策略在所有模型中均一致地实现了更高的识别准确率。在模型对比中，GPT-4.5 和 GPT-o3 在大多数测试场景下优于其他模型，展现出更强的安全评估潜力。研究强调了提示工程 (Prompt Engineering) 在提升多模态 LLMs 准确性和一致性方面的关键作用，并为开发更可靠的 AI 辅助建筑安全监控系统提供了实践指导。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07436v1",
      "published_date": "2025-06-09 05:22:35 UTC",
      "updated_date": "2025-06-09 05:22:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:25:16.442749+00:00"
    },
    {
      "arxiv_id": "2506.07435v2",
      "title": "Fast Geometric Embedding for Node Influence Maximization",
      "title_zh": "用于节点影响力最大化的快速几何嵌入",
      "authors": [
        "Alexander Kolpakov",
        "Igor Rivin"
      ],
      "abstract": "Computing classical centrality measures such as betweenness and closeness is computationally expensive on large-scale graphs. In this work, we introduce an efficient force layout algorithm that embeds a graph into a low-dimensional space, where the radial distance from the origin serves as a proxy for various centrality measures. We evaluate our method on multiple graph families and demonstrate strong correlations with degree, PageRank, and paths-based centralities. As an application, it turns out that the proposed embedding allows to find high-influence nodes in a network, and provides a fast and scalable alternative to the standard greedy algorithm.",
      "tldr_zh": "该研究提出了一种高效的力导向布局算法(force layout algorithm)，旨在解决大规模图网络中介数(betweenness)和接近度(closeness)等传统中心性度量计算成本过高的问题。该算法将图嵌入到一个低维空间中，利用节点距离原点的径向距离(radial distance)作为各种中心性指标的代理。实验在多种图家族上进行了评估，结果显示该嵌入方法与度(degree)、PageRank以及基于路径的中心性度量具有极强的相关性。作为一种实际应用，该研究证明了所提出的几何嵌入能够有效识别网络中的高影响力节点。与标准的贪婪算法相比，该方法为节点影响力最大化问题提供了一种更快速、更具扩展性的替代方案。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SI",
      "comment": "8 pages, 4 figures, 18 tables; Github repository available (https://github.com/sashakolpakov/graphem/); Package available on PyPi (https://pypi.org/project/graphem-jax/)",
      "pdf_url": "https://arxiv.org/pdf/2506.07435v2",
      "published_date": "2025-06-09 05:21:56 UTC",
      "updated_date": "2025-08-18 12:21:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:25:10.042244+00:00"
    },
    {
      "arxiv_id": "2506.07434v1",
      "title": "Well Begun is Half Done: Low-resource Preference Alignment by Weak-to-Strong Decoding",
      "title_zh": "好的开始是成功的一半：基于弱到强解码的低资源偏好对齐",
      "authors": [
        "Feifan Song",
        "Shaohang Wei",
        "Wen Luo",
        "Yuxuan Fan",
        "Tianyu Liu",
        "Guoyin Wang",
        "Houfeng Wang"
      ],
      "abstract": "Large Language Models (LLMs) require alignment with human preferences to avoid generating offensive, false, or meaningless content. Recently, low-resource methods for LLM alignment have been popular, while still facing challenges in obtaining both high-quality and aligned content. Motivated by the observation that the difficulty of generating aligned responses is concentrated at the beginning of decoding, we propose a novel framework, Weak-to-Strong Decoding (WSD), to enhance the alignment ability of base models by the guidance of a small aligned model. The small model first drafts well-aligned beginnings, followed by the large base model to continue the rest, controlled by a well-designed auto-switch mechanism. We also collect a new dataset, GenerAlign, to fine-tune a small-sized Pilot-3B as the draft model, which effectively enhances different base models under the WSD framework to outperform all baseline methods, while avoiding degradation on downstream tasks, termed as the alignment tax. Extensive experiments are further conducted to examine the impact of different settings and time efficiency, as well as analyses on the intrinsic mechanisms of WSD in depth.",
      "tldr_zh": "该研究针对大语言模型(LLMs)在低资源偏好对齐(Preference Alignment)中难以兼顾生成质量与对齐能力的问题，提出了名为Weak-to-Strong Decoding (WSD)的创新框架。基于生成对齐回复的难度主要集中在解码起始阶段的观察，WSD利用一个小型已对齐模型为大型基座模型提供引导。该框架采用精心设计的自动切换机制(auto-switch mechanism)，由小模型负责起草对齐良好的开头，再由大模型接管后续生成过程。研究团队还构建了GenerAlign数据集并微调出Pilot-3B作为引导模型，实验结果表明WSD在多种基座模型上均优于基线方法，并有效避免了下游任务性能衰减的“对齐税”(alignment tax)。此外，针对不同设置、时间效率及内在机制的深入分析进一步验证了WSD的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2506.07434v1",
      "published_date": "2025-06-09 05:21:22 UTC",
      "updated_date": "2025-06-09 05:21:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:25:10.443201+00:00"
    },
    {
      "arxiv_id": "2506.07431v2",
      "title": "FAMSeg: Fetal Femur and Cranial Ultrasound Segmentation Using Feature-Aware Attention and Mamba Enhancement",
      "title_zh": "FAMSeg：基于特征感知注意力与 Mamba 增强的胎儿股骨及颅脑超声图像分割",
      "authors": [
        "Jie He",
        "Minglang Chen",
        "Minying Lu",
        "Bocheng Liang",
        "Junming Wei",
        "Guiyan Peng",
        "Jiaxi Chen",
        "Ying Tan"
      ],
      "abstract": "Accurate ultrasound image segmentation is a prerequisite for precise biometrics and accurate assessment. Relying on manual delineation introduces significant errors and is time-consuming. However, existing segmentation models are designed based on objects in natural scenes, making them difficult to adapt to ultrasound objects with high noise and high similarity. This is particularly evident in small object segmentation, where a pronounced jagged effect occurs. Therefore, this paper proposes a fetal femur and cranial ultrasound image segmentation model based on feature perception and Mamba enhancement to address these challenges. Specifically, a longitudinal and transverse independent viewpoint scanning convolution block and a feature perception module were designed to enhance the ability to capture local detail information and improve the fusion of contextual information. Combined with the Mamba-optimized residual structure, this design suppresses the interference of raw noise and enhances local multi-dimensional scanning. The system builds global information and local feature dependencies, and is trained with a combination of different optimizers to achieve the optimal solution. After extensive experimental validation, the FAMSeg network achieved the fastest loss reduction and the best segmentation performance across images of varying sizes and orientations.",
      "tldr_zh": "该研究提出了FAMSeg，一种结合特征感知(Feature Perception)和Mamba增强(Mamba Enhancement)的胎儿股骨及颅脑超声图像分割模型。针对传统分割模型在处理高噪声、高相似度超声图像时容易出现的锯齿效应和细节捕捉能力不足等问题，该模型设计了纵向与横向独立视点扫描卷积块以及特征感知模块，旨在增强局部细节信息提取并优化上下文融合。通过引入Mamba优化的残差结构，系统有效抑制了原始图像的噪声干扰，并增强了局部多维扫描能力。模型成功构建了全局信息与局部特征之间的依赖关系，并采用多种优化器组合策略以寻求最优解。实验结果表明，FAMSeg在不同尺寸和方向的超声图像分割任务中均取得了最快的损失收敛速度和最优的性能指标。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07431v2",
      "published_date": "2025-06-09 05:06:47 UTC",
      "updated_date": "2025-06-14 10:40:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:25:14.616347+00:00"
    },
    {
      "arxiv_id": "2506.07428v1",
      "title": "HeTa: Relation-wise Heterogeneous Graph Foundation Attack Model",
      "title_zh": "HeTa：面向关系的异质图攻击基础模型",
      "authors": [
        "Yuling Wang",
        "Zihui Chen",
        "Pengfei Jiao",
        "Xiao Wang"
      ],
      "abstract": "Heterogeneous Graph Neural Networks (HGNNs) are vulnerable, highlighting the need for tailored attacks to assess their robustness and ensure security. However, existing HGNN attacks often require complex retraining of parameters to generate specific perturbations for new scenarios. Recently, foundation models have opened new horizons for the generalization of graph neural networks by capturing shared semantics across various graph distributions. This leads us to ask:Can we design a foundation attack model for HGNNs that enables generalizable perturbations across different HGNNs, and quickly adapts to new heterogeneous graphs (HGs)? Empirical findings reveal that, despite significant differences in model design and parameter space, different HGNNs surprisingly share common vulnerability patterns from a relation-aware perspective. Therefore, we explore how to design foundation HGNN attack criteria by mining shared attack units. In this paper, we propose a novel relation-wise heterogeneous graph foundation attack model, HeTa. We introduce a foundation surrogate model to align heterogeneity and identify the importance of shared relation-aware attack units. Building on this, we implement a serialized relation-by-relation attack based on the identified relational weights. In this way, the perturbation can be transferred to various target HGNNs and easily fine-tuned for new HGs. Extensive experiments exhibit powerful attack performances and generalizability of our method.",
      "tldr_zh": "该研究针对异构图神经网络 (Heterogeneous Graph Neural Networks, HGNNs) 的安全隐患，提出了名为 HeTa 的关系级异构图基础攻击模型。研究发现，尽管不同 HGNNs 的架构各异，但在关系感知 (relation-aware) 维度上具有相似的脆弱性模式。HeTa 通过引入基础代理模型来对齐异构性，并识别出共享的关系感知攻击单元及其重要性。基于这些关系权重，该模型执行序列化的逐关系攻击，使得生成的扰动能够有效迁移至各种目标模型，并能通过微调快速适应新的异构图 (HGs)。实验结果表明，HeTa 在多种复杂场景下均展现出强大的攻击性能和卓越的泛化能力。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.07428v1",
      "published_date": "2025-06-09 04:59:14 UTC",
      "updated_date": "2025-06-09 04:59:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:25:29.920475+00:00"
    },
    {
      "arxiv_id": "2506.11115v1",
      "title": "Incorporating Domain Knowledge into Materials Tokenization",
      "title_zh": "将领域知识融入材料词元化",
      "authors": [
        "Yerim Oh",
        "Jun-Hyung Park",
        "Junho Kim",
        "SungHo Kim",
        "SangKeun Lee"
      ],
      "abstract": "While language models are increasingly utilized in materials science, typical models rely on frequency-centric tokenization methods originally developed for natural language processing. However, these methods frequently produce excessive fragmentation and semantic loss, failing to maintain the structural and semantic integrity of material concepts. To address this issue, we propose MATTER, a novel tokenization approach that integrates material knowledge into tokenization. Based on MatDetector trained on our materials knowledge base and a re-ranking method prioritizing material concepts in token merging, MATTER maintains the structural integrity of identified material concepts and prevents fragmentation during tokenization, ensuring their semantic meaning remains intact. The experimental results demonstrate that MATTER outperforms existing tokenization methods, achieving an average performance gain of $4\\%$ and $2\\%$ in the generation and classification tasks, respectively. These results underscore the importance of domain knowledge for tokenization strategies in scientific text processing. Our code is available at https://github.com/yerimoh/MATTER",
      "tldr_zh": "该研究指出材料科学领域的语言模型若仅依赖通用的频率中心化 Tokenization 方法，会导致材料概念的过度碎片化和语义丢失。为了解决这一问题，作者提出了 MATTER，一种将材料领域知识集成到 Tokenization 过程中的新方法。该方法利用在材料知识库上训练的 MatDetector 识别核心概念，并通过重排序(Re-ranking)机制在 Token 合并时优先保留材料术语的完整性。实验结果表明，MATTER 在文本生成和分类任务上分别比现有主流方法提升了 4% 和 2% 的性能。这项工作强调了在科学文本处理中，将领域知识引入 Tokenization 策略对于维持语义连贯性的重要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11115v1",
      "published_date": "2025-06-09 04:59:13 UTC",
      "updated_date": "2025-06-09 04:59:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:25:34.946197+00:00"
    },
    {
      "arxiv_id": "2506.07424v1",
      "title": "Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models",
      "title_zh": "插件与微调：弥合小语言模型与大语言模型之间的差距",
      "authors": [
        "Kyeonghyun Kim",
        "Jinhee Jang",
        "Juhwan Choi",
        "Yoonji Lee",
        "Kyohoon Jin",
        "YoungBin Kim"
      ],
      "abstract": "Large language models (LLMs) are renowned for their extensive linguistic knowledge and strong generalization capabilities, but their high computational demands make them unsuitable for resource-constrained environments. In contrast, small language models (SLMs) are computationally efficient but often lack the broad generalization capacity of LLMs. To bridge this gap, we propose PiFi, a novel framework that combines the strengths of both LLMs and SLMs to achieve high performance while maintaining efficiency. PiFi integrates a single frozen layer from an LLM into a SLM and fine-tunes the combined model for specific tasks, boosting performance without a significant increase in computational cost. We show that PiFi delivers consistent performance improvements across a range of natural language processing tasks, including both natural language understanding and generation. Moreover, our findings demonstrate PiFi's ability to effectively leverage LLM knowledge, enhancing generalization to unseen domains and facilitating the transfer of linguistic abilities.",
      "tldr_zh": "该研究提出了 PiFi 框架，旨在通过结合大语言模型 (LLMs) 的广博知识与小语言模型 (SLMs) 的高效性，在资源受限的环境下实现高性能。PiFi 创新性地将 LLM 的单个冻结层 (frozen layer) 集成到 SLM 中，并针对特定任务进行微调 (Fine-tuning)，从而在不显著增加计算成本的情况下提升了模型性能。实验数据表明，该方法在自然语言理解 (NLU) 和自然语言生成 (NLG) 等一系列自然语言处理 (NLP) 任务中取得了持续的改进。此外，研究发现 PiFi 能够有效利用 LLM 的语言能力，显著增强了模型对未知领域 (unseen domains) 的泛化能力和语言迁移效果。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 main conference",
      "pdf_url": "https://arxiv.org/pdf/2506.07424v1",
      "published_date": "2025-06-09 04:45:13 UTC",
      "updated_date": "2025-06-09 04:45:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:25:44.495658+00:00"
    },
    {
      "arxiv_id": "2506.07418v1",
      "title": "Evaluating Visual Mathematics in Multimodal LLMs: A Multilingual Benchmark Based on the Kangaroo Tests",
      "title_zh": "评估多模态大语言模型的视觉数学能力：基于 Kangaroo Tests 的多语言基准",
      "authors": [
        "Arnau Igualde Sáez",
        "Lamyae Rhomrasi",
        "Yusef Ahsini",
        "Ricardo Vinuesa",
        "Sergio Hoyas",
        "Jose P. García Sabater",
        "Marius J. Fullana i Alfonso",
        "J. Alberto Conejero"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) promise advanced vision language capabilities, yet their effectiveness in visually presented mathematics remains underexplored. This paper analyzes the development and evaluation of MLLMs for mathematical problem solving, focusing on diagrams, multilingual text, and symbolic notation. We then assess several models, including GPT 4o, Pixtral, Qwen VL, Llama 3.2 Vision variants, and Gemini 2.0 Flash in a multilingual Kangaroo style benchmark spanning English, French, Spanish, and Catalan. Our experiments reveal four key findings. First, overall precision remains moderate across geometry, visual algebra, logic, patterns, and combinatorics: no single model excels in every topic. Second, while most models see improved accuracy with questions that do not have images, the gain is often limited; performance for some remains nearly unchanged without visual input, indicating underutilization of diagrammatic information. Third, substantial variation exists across languages and difficulty levels: models frequently handle easier items but struggle with advanced geometry and combinatorial reasoning. Notably, Gemini 2.0 Flash achieves the highest precision on image based tasks, followed by Qwen VL 2.5 72B and GPT 4o, though none approach human level performance. Fourth, a complementary analysis aimed at distinguishing whether models reason or simply recite reveals that Gemini and GPT 4o stand out for their structured reasoning and consistent accuracy. In contrast, Pixtral and Llama exhibit less consistent reasoning, often defaulting to heuristics or randomness when unable to align their outputs with the given answer options.",
      "tldr_zh": "该研究评估了多模态大语言模型(MLLMs)在视觉呈现数学任务中的有效性，并基于袋鼠数学竞赛(Kangaroo Tests)构建了一个涵盖英语、法语、西班牙语和加泰罗尼亚语的多语言基准。研究人员对GPT-4o、Pixtral、Qwen VL、Llama 3.2 Vision和Gemini 2.0 Flash等模型进行了深度测试，涵盖几何(geometry)、视觉代数(visual algebra)、逻辑(logic)和组合数学(combinatorics)等多个主题。实验结果显示，Gemini 2.0 Flash在基于图像的任务中获得了最高精度，Qwen VL 2.5 72B和GPT-4o紧随其后，但所有模型表现仍远未达到人类水平。研究发现，许多模型对图表信息(diagrammatic information)的利用率不足，且在处理高级几何和组合推理(combinatorial reasoning)时面临显著困难。此外，针对推理能力的互补分析表明，Gemini和GPT-4o在结构化推理(structured reasoning)方面脱颖而出，而Pixtral和Llama在无法对齐选项时往往依赖启发式方法(heuristics)或随机性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.07418v1",
      "published_date": "2025-06-09 04:35:02 UTC",
      "updated_date": "2025-06-09 04:35:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:25:38.867226+00:00"
    },
    {
      "arxiv_id": "2506.07417v2",
      "title": "Evidential Spectrum-Aware Contrastive Learning for OOD Detection in Dynamic Graphs",
      "title_zh": "面向动态图 OOD 检测的证据性频谱感知对比学习",
      "authors": [
        "Nan Sun",
        "Xixun Lin",
        "Zhiheng Zhou",
        "Yanmin Shang",
        "Zhenlin Cheng",
        "Yanan Cao"
      ],
      "abstract": "Recently, Out-of-distribution (OOD) detection in dynamic graphs, which aims to identify whether incoming data deviates from the distribution of the in-distribution (ID) training set, has garnered considerable attention in security-sensitive fields. Current OOD detection paradigms primarily focus on static graphs and confront two critical challenges: i) high bias and high variance caused by single-point estimation, which makes the predictions sensitive to randomness in the data; ii) score homogenization resulting from the lack of OOD training data, where the model only learns ID-specific patterns, resulting in overall low OOD scores and a narrow score gap between ID and OOD data. To tackle these issues, we first investigate OOD detection in dynamic graphs through the lens of Evidential Deep Learning (EDL). Specifically, we propose EviSEC, an innovative and effective OOD detector via Evidential Spectrum-awarE Contrastive Learning. We design an evidential neural network to redefine the output as the posterior Dirichlet distribution, explaining the randomness of inputs through the uncertainty of distribution, which is overlooked by single-point estimation. Moreover, spectrum-aware augmentation module generates OOD approximations to identify patterns with high OOD scores, thereby widening the score gap between ID and OOD data and mitigating score homogenization. Extensive experiments on real-world datasets demonstrate that EviSAC effectively detects OOD samples in dynamic graphs.",
      "tldr_zh": "该研究提出了EviSEC，一种基于证据频谱感知对比学习(Evidential Spectrum-aware Contrastive Learning)的动态图(Dynamic Graphs)分布外(OOD)检测框架。针对单点估计导致的高偏差、高方差以及缺乏OOD数据引起的评分同质化等挑战，EviSEC首次从证据深度学习(Evidential Deep Learning, EDL)的角度切入，将输出重新定义为后验狄利克雷分布(posterior Dirichlet distribution)，有效利用分布的不确定性捕捉输入随机性。同时，该框架引入了频谱感知增强模块(spectrum-aware augmentation)生成OOD近似样本，旨在识别高OOD评分模式并扩大ID与OOD数据间的评分差距。在真实世界数据集上的广泛实验证明，EviSEC在动态图场景下具有卓越的OOD样本检测性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ECML-PKDD 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.07417v2",
      "published_date": "2025-06-09 04:34:46 UTC",
      "updated_date": "2025-06-13 13:08:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:25:43.240237+00:00"
    },
    {
      "arxiv_id": "2506.07416v2",
      "title": "LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments",
      "title_zh": "LiteVLM：面向资源受限环境的低延迟视觉语言模型推理流水线",
      "authors": [
        "Jin Huang",
        "Yuchao Jin",
        "Le An",
        "Josh Park"
      ],
      "abstract": "This paper introduces an efficient Vision-Language Model (VLM) pipeline specifically optimized for deployment on embedded devices, such as those used in robotics and autonomous driving. The pipeline significantly reduces the computational overhead by jointly leveraging patch selection to filter irrelevant camera views, a token selection module to reduce input sequence length for the LLM, and speculative decoding to accelerate token generation. Evaluation on the NVIDIA DRIVE Thor platform for automonous driving application, our pipeline achieves $2.5\\times$ end-to-end latency reduction without compromising task accuracy. The speed-up further increases to $3.2\\times$ when applying FP8 post-training quantization. These results demonstrate our pipeline as a viable solution for enabling real-time VLM deployment in resource-constrained environments.",
      "tldr_zh": "该研究提出了LiteVLM，一种专为机器人和自动驾驶等资源受限环境下的嵌入式设备优化的视觉语言模型(VLM)推理流水线。该流水线通过联合利用patch selection技术过滤无关视角、使用token selection模块缩短大语言模型(LLM)的输入序列长度，以及应用speculative decoding加速token生成，显著降低了计算开销。在NVIDIA DRIVE Thor自动驾驶平台上的评估结果显示，该流水线在不损失任务准确率的前提下，实现了2.5倍的端到端延迟缩减。在应用FP8量化(post-training quantization)后，加速效果进一步提升至3.2倍。实验结果证明LiteVLM是实现在嵌入式设备上实时部署VLM的有效方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07416v2",
      "published_date": "2025-06-09 04:30:25 UTC",
      "updated_date": "2025-10-31 20:18:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:25:53.666178+00:00"
    },
    {
      "arxiv_id": "2506.07411v1",
      "title": "An Intelligent Fault Self-Healing Mechanism for Cloud AI Systems via Integration of Large Language Models and Deep Reinforcement Learning",
      "title_zh": "融合大语言模型与深度强化学习的云AI系统智能故障自愈机制",
      "authors": [
        "Ze Yang",
        "Yihong Jin",
        "Juntian Liu",
        "Xinhe Xu"
      ],
      "abstract": "As the scale and complexity of cloud-based AI systems continue to increase, the detection and adaptive recovery of system faults have become the core challenges to ensure service reliability and continuity. In this paper, we propose an Intelligent Fault Self-Healing Mechanism (IFSHM) that integrates Large Language Model (LLM) and Deep Reinforcement Learning (DRL), aiming to realize a fault recovery framework with semantic understanding and policy optimization capabilities in cloud AI systems. On the basis of the traditional DRL-based control model, the proposed method constructs a two-stage hybrid architecture: (1) an LLM-driven fault semantic interpretation module, which can dynamically extract deep contextual semantics from multi-source logs and system indicators to accurately identify potential fault modes; (2) DRL recovery strategy optimizer, based on reinforcement learning, learns the dynamic matching of fault types and response behaviors in the cloud environment. The innovation of this method lies in the introduction of LLM for environment modeling and action space abstraction, which greatly improves the exploration efficiency and generalization ability of reinforcement learning. At the same time, a memory-guided meta-controller is introduced, combined with reinforcement learning playback and LLM prompt fine-tuning strategy, to achieve continuous adaptation to new failure modes and avoid catastrophic forgetting. Experimental results on the cloud fault injection platform show that compared with the existing DRL and rule methods, the IFSHM framework shortens the system recovery time by 37% with unknown fault scenarios.",
      "tldr_zh": "该研究提出了一种集成Large Language Model (LLM)与Deep Reinforcement Learning (DRL)的智能故障自愈机制(IFSHM)，旨在解决Cloud AI Systems日益增长的故障检测与自适应恢复难题。该机制采用两阶段混合架构，首先通过LLM驱动的故障语义解释模块从多源logs和系统指标中提取深层上下文信息，以准确识别故障模式。随后，利用DRL恢复策略优化器在云环境中学习故障类型与响应行为的动态匹配。该方法的创新点在于引入LLM进行环境建模和action space抽象，显著提高了强化学习的探索效率与泛化能力。此外，系统通过memory-guided meta-controller结合prompt fine-tuning策略，实现了对新故障模式的持续适应并避免灾难性遗忘。实验结果表明，IFSHM在未知故障场景下比现有DRL和规则方法缩短了37%的系统恢复时间。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Proceedings of 2025 IEEE 8th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.07411v1",
      "published_date": "2025-06-09 04:14:05 UTC",
      "updated_date": "2025-06-09 04:14:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:25:55.239007+00:00"
    },
    {
      "arxiv_id": "2506.13776v1",
      "title": "Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines in Model Evaluations",
      "title_zh": "模型评估中严谨透明的人类基准：建议与报告清单",
      "authors": [
        "Kevin L. Wei",
        "Patricia Paskov",
        "Sunishchal Dev",
        "Michael J. Byun",
        "Anka Reuel",
        "Xavier Roberts-Gaal",
        "Rachel Calcott",
        "Evie Coxon",
        "Chinmay Deshpande"
      ],
      "abstract": "In this position paper, we argue that human baselines in foundation model evaluations must be more rigorous and more transparent to enable meaningful comparisons of human vs. AI performance, and we provide recommendations and a reporting checklist towards this end. Human performance baselines are vital for the machine learning community, downstream users, and policymakers to interpret AI evaluations. Models are often claimed to achieve \"super-human\" performance, but existing baselining methods are neither sufficiently rigorous nor sufficiently well-documented to robustly measure and assess performance differences. Based on a meta-review of the measurement theory and AI evaluation literatures, we derive a framework with recommendations for designing, executing, and reporting human baselines. We synthesize our recommendations into a checklist that we use to systematically review 115 human baselines (studies) in foundation model evaluations and thus identify shortcomings in existing baselining methods; our checklist can also assist researchers in conducting human baselines and reporting results. We hope our work can advance more rigorous AI evaluation practices that can better serve both the research community and policymakers. Data is available at: https://github.com/kevinlwei/human-baselines",
      "tldr_zh": "该研究针对基础模型评估中人类基准(Human Baselines)缺乏严谨性和透明度的问题，提出了一套旨在实现模型与人类表现有效比较的系统化建议。作者通过对测量理论(Measurement Theory)和人工智能评估文献的元综述，构建了一个涵盖设计、执行和报告人类基准的研究框架。研究进一步将这些建议提炼为一份报告清单(Reporting Checklist)，并利用该工具对115项现有的人类基准研究进行了系统性审查，揭示了当前评估方法中的显著缺陷。该工作不仅为研究人员提供了提升评估严谨性的实用指南，也为政策制定者准确解读AI性能表现提供了参考依据。通过推动更透明的评估实践，该研究旨在为人工智能领域的基准测试建立更可靠的衡量标准。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "A version of this paper has been accepted to ICML 2025 as a position paper (spotlight), with the title: \"Position: Human Baselines in Model Evaluations Need Rigor and Transparency (With Recommendations & Reporting Checklist).\"",
      "pdf_url": "https://arxiv.org/pdf/2506.13776v1",
      "published_date": "2025-06-09 04:08:16 UTC",
      "updated_date": "2025-06-09 04:08:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:25:53.047727+00:00"
    },
    {
      "arxiv_id": "2506.08054v2",
      "title": "STAMImputer: Spatio-Temporal Attention MoE for Traffic Data Imputation",
      "title_zh": "STAMImputer：用于交通数据插补的时空注意力混合专家",
      "authors": [
        "Yiming Wang",
        "Hao Peng",
        "Senzhang Wang",
        "Haohua Du",
        "Chunyang Liu",
        "Jia Wu",
        "Guanlin Wu"
      ],
      "abstract": "Traffic data imputation is fundamentally important to support various applications in intelligent transportation systems such as traffic flow prediction. However, existing time-to-space sequential methods often fail to effectively extract features in block-wise missing data scenarios. Meanwhile, the static graph structure for spatial feature propagation significantly constrains the models flexibility in handling the distribution shift issue for the nonstationary traffic data. To address these issues, this paper proposes a SpatioTemporal Attention Mixture of experts network named STAMImputer for traffic data imputation. Specifically, we introduce a Mixture of Experts (MoE) framework to capture latent spatio-temporal features and their influence weights, effectively imputing block missing. A novel Low-rank guided Sampling Graph ATtention (LrSGAT) mechanism is designed to dynamically balance the local and global correlations across road networks. The sampled attention vectors are utilized to generate dynamic graphs that capture real-time spatial correlations. Extensive experiments are conducted on four traffic datasets for evaluation. The result shows STAMImputer achieves significantly performance improvement compared with existing SOTA approaches. Our codes are available at https://github.com/RingBDStack/STAMImupter.",
      "tldr_zh": "该研究针对智能交通系统中交通数据填补(Traffic data imputation)面临的块状数据缺失(block-wise missing data)和非平稳数据分布偏移(distribution shift)等挑战，提出了STAMImputer框架。该模型引入了 Mixture of Experts (MoE) 架构，通过捕捉潜在的时空特征及其影响权重来有效处理 block missing 场景。研究特别设计了一种 Low-rank guided Sampling Graph ATtention (LrSGAT) 机制，用以动态平衡路网中的局部与全局相关性。通过利用采样的注意力向量生成 dynamic graphs，该框架能够实时捕获空间相关性，显著提升了模型处理非平稳数据的灵活性。在四个交通数据集上的实验结果表明，STAMImputer 的性能显著优于现有的 SOTA 方法，为智能交通系统的各项应用提供了更精准的数据支撑。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 5 figures, 3 tables. Extended version of paper accepted at IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.08054v2",
      "published_date": "2025-06-09 04:05:00 UTC",
      "updated_date": "2025-06-11 02:33:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:26:10.201708+00:00"
    },
    {
      "arxiv_id": "2506.07408v1",
      "title": "Fractional-order Jacobian Matrix Differentiation and Its Application in Artificial Neural Networks",
      "title_zh": "分数阶雅可比矩阵微分及其在人工神经网络中的应用",
      "authors": [
        "Xiaojun zhou",
        "Chunna Zhao",
        "Yaqun Huang",
        "Chengli Zhou",
        "Junjie Ye",
        "Kemeng Xiang"
      ],
      "abstract": "Fractional-order differentiation has many characteristics different from integer-order differentiation. These characteristics can be applied to the optimization algorithms of artificial neural networks to obtain better results. However, due to insufficient theoretical research, at present, there is no fractional-order matrix differentiation method that is perfectly compatible with automatic differentiation (Autograd) technology. Therefore, we propose a fractional-order matrix differentiation calculation method. This method is introduced by the definition of the integer-order Jacobian matrix. We denote it as fractional-order Jacobian matrix differentiation (${\\bf{J}^α}$). Through ${\\bf{J}^α}$, we can carry out the matrix-based fractional-order chain rule. Based on the Linear module and the fractional-order differentiation, we design the fractional-order Autograd technology to enable the use of fractional-order differentiation in hidden layers, thereby enhancing the practicality of fractional-order differentiation in deep learning. In the experiment, according to the PyTorch framework, we design fractional-order Linear (FLinear) and replace nn.Linear in the multilayer perceptron with FLinear. Through the qualitative analysis of the training set and validation set $Loss$, the quantitative analysis of the test set indicators, and the analysis of time consumption and GPU memory usage during model training, we verify the superior performance of ${\\bf{J}^α}$ and prove that it is an excellent fractional-order gradient descent method in the field of deep learning.",
      "tldr_zh": "该研究针对分数阶微分（Fractional-order differentiation）在神经网络优化中缺乏与自动求导（Autograd）技术兼容的矩阵微分方法这一问题，提出了分数阶Jacobian矩阵微分（Fractional-order Jacobian matrix differentiation，$J^\\alpha$）。该方法借鉴整数阶Jacobian矩阵的定义，实现了基于矩阵的分数阶链式法则（matrix-based fractional-order chain rule），并设计了相应的分数阶Autograd技术以支持在隐藏层中使用。在PyTorch框架下，研究者通过开发分数阶线性层（FLinear）替换多层感知机（MLP）中的标准线性层进行实验验证。通过对Loss曲线的定性分析、测试集指标的定量评估以及对训练耗时和GPU内存占用的综合分析，实验结果证明了$J^\\alpha$在深度学习任务中具有优越性能。该研究为深度学习领域提供了一种高效的分数阶梯度下降（fractional-order gradient descent）方法，显著提升了分数阶微分在复杂神经网络中的实用性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07408v1",
      "published_date": "2025-06-09 04:04:08 UTC",
      "updated_date": "2025-06-09 04:04:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:26:06.097421+00:00"
    },
    {
      "arxiv_id": "2506.07407v1",
      "title": "Anomaly Detection and Early Warning Mechanism for Intelligent Monitoring Systems in Multi-Cloud Environments Based on LLM",
      "title_zh": "基于 LLM 的多云环境智能监控系统异常检测与预警机制",
      "authors": [
        "Yihong Jin",
        "Ze Yang",
        "Juntian Liu",
        "Xinhe Xu"
      ],
      "abstract": "With the rapid development of multi-cloud environments, it is increasingly important to ensure the security and reliability of intelligent monitoring systems. In this paper, we propose an anomaly detection and early warning mechanism for intelligent monitoring system in multi-cloud environment based on Large-Scale Language Model (LLM). On the basis of the existing monitoring framework, the proposed model innovatively introduces a multi-level feature extraction method, which combines the natural language processing ability of LLM with traditional machine learning methods to enhance the accuracy of anomaly detection and improve the real-time response efficiency. By introducing the contextual understanding capabilities of LLMs, the model dynamically adapts to different cloud service providers and environments, so as to more effectively detect abnormal patterns and predict potential failures. Experimental results show that the proposed model is significantly better than the traditional anomaly detection system in terms of detection accuracy and latency, and significantly improves the resilience and active management ability of cloud infrastructure.",
      "tldr_zh": "该研究针对多云环境(Multi-Cloud Environments)下智能监控系统的安全与可靠性，提出了一种基于大语言模型(LLM)的异常检测与预警机制。该模型在现有监控框架基础上，创新性地引入了多层次特征提取方法，将LLM的自然语言处理(Natural Language Processing)能力与传统机器学习方法相结合，旨在增强异常检测的准确性并提升实时响应效率。通过利用LLM的上下文理解能力，该模型能够动态适应不同的云服务提供商和环境，从而更有效地捕捉异常模式并预测潜在故障。实验结果表明，该模型在检测准确率和延迟(Latency)方面均显著优于传统异常检测系统，大幅提升了云基础设施的复原力(Resilience)和主动管理能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Proceedings of 2025 5th International Symposium on Computer Technology and Information Science (ISCTIS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.07407v1",
      "published_date": "2025-06-09 04:00:23 UTC",
      "updated_date": "2025-06-09 04:00:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:26:27.537742+00:00"
    },
    {
      "arxiv_id": "2506.07406v2",
      "title": "InverseScope: Scalable Activation Inversion for Interpreting Large Language Models",
      "title_zh": "InverseScope：面向大语言模型解释的可扩展激活反演",
      "authors": [
        "Yifan Luo",
        "Zhennan Zhou",
        "Bin Dong"
      ],
      "abstract": "Understanding the internal representations of large language models (LLMs) is a central challenge in interpretability research. Existing feature interpretability methods often rely on strong assumptions about the structure of representations that may not hold in practice. In this work, we introduce InverseScope, an assumption-light and scalable framework for interpreting neural activations via input inversion. Given a target activation, we define a distribution over inputs that generate similar activations and analyze this distribution to infer the encoded information. To address the inefficiency of sampling in high-dimensional spaces, we propose a novel conditional generation architecture that significantly improves sample efficiency compared to previous method. We further introduce a quantitative evaluation protocol that tests interpretability hypotheses using the feature consistency rate computed over the sampled inputs. InverseScope scales inversion-based interpretability methods to larger models and practical tasks, enabling systematic and quantitative analysis of internal representations in real-world LLMs.",
      "tldr_zh": "该研究提出了 InverseScope，这是一个针对大型语言模型（LLMs）设计的轻量化且可扩展的激活反向（Activation Inversion）解释框架，旨在揭示模型的内部表示。针对现有方法对特征结构假设过强且难以扩展的问题，InverseScope 通过输入反向技术，根据目标激活定义生成相似激活的输入分布，并分析该分布以推断编码信息。为了解决高维空间采样的效率瓶颈，研究引入了一种新型的条件生成架构（Conditional Generation Architecture），显著提升了采样效率。此外，该研究还提出了一套定量评估协议，利用在采样输入上计算的特征一致性率（Feature Consistency Rate）来验证解释性假设。实验证明，InverseScope 成功将基于反向的解释方法扩展到更大规模的模型和实际任务中，实现了对真实世界 LLMs 内部表示的系统性定量分析。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.07406v2",
      "published_date": "2025-06-09 03:59:28 UTC",
      "updated_date": "2025-09-27 02:04:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:26:28.249555+00:00"
    },
    {
      "arxiv_id": "2506.07400v3",
      "title": "MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models",
      "title_zh": "MedChat：基于大语言模型的多模态诊断多智能体框架",
      "authors": [
        "Philip R. Liu",
        "Sparsh Bansal",
        "Jimmy Dinh",
        "Aditya Pawar",
        "Ramani Satishkumar",
        "Shail Desai",
        "Neeraj Gupta",
        "Xin Wang",
        "Shu Hu"
      ],
      "abstract": "The integration of deep learning-based glaucoma detection with large language models (LLMs) presents an automated strategy to mitigate ophthalmologist shortages and improve clinical reporting efficiency. However, applying general LLMs to medical imaging remains challenging due to hallucinations, limited interpretability, and insufficient domain-specific medical knowledge, which can potentially reduce clinical accuracy. Although recent approaches combining imaging models with LLM reasoning have improved reporting, they typically rely on a single generalist agent, restricting their capacity to emulate the diverse and complex reasoning found in multidisciplinary medical teams. To address these limitations, we propose MedChat, a multi-agent diagnostic framework and platform that combines specialized vision models with multiple role-specific LLM agents, all coordinated by a director agent. This design enhances reliability, reduces hallucination risk, and enables interactive diagnostic reporting through an interface tailored for clinical review and educational use. Code available at https://github.com/Purdue-M2/MedChat.",
      "tldr_zh": "该研究针对大语言模型(LLMs)在医学影像诊断中存在的幻觉、可解释性差以及领域知识不足等挑战，提出了MedChat这一多智能体诊断框架。MedChat通过将专门的视觉模型(vision models)与多个角色特定的LLM智能体相结合，并由一个主管智能体(director agent)进行统一协调，成功模拟了多学科医疗团队的复杂推理过程。该框架特别关注于青光眼检测(glaucoma detection)，旨在通过自动化策略缓解眼科医生短缺并提升临床报告效率。这种多智能体协作设计不仅增强了诊断的可靠性，还显著降低了模型产生幻觉的风险。此外，MedChat平台还提供了专为临床审查和教育设计的交互式界面，为可解释且高效的多模态医学诊断提供了新的解决方案。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07400v3",
      "published_date": "2025-06-09 03:51:18 UTC",
      "updated_date": "2025-12-16 22:52:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:26:44.897095+00:00"
    },
    {
      "arxiv_id": "2506.07399v1",
      "title": "MrM: Black-Box Membership Inference Attacks against Multimodal RAG Systems",
      "title_zh": "MrM：面向多模态 RAG 系统的黑盒成员推理攻击",
      "authors": [
        "Peiru Yang",
        "Jinhua Yin",
        "Haoran Zheng",
        "Xueying Bai",
        "Huili Wang",
        "Yufei Sun",
        "Xintian Li",
        "Shangguang Wang",
        "Yongfeng Huang",
        "Tao Qi"
      ],
      "abstract": "Multimodal retrieval-augmented generation (RAG) systems enhance large vision-language models by integrating cross-modal knowledge, enabling their increasing adoption across real-world multimodal tasks. These knowledge databases may contain sensitive information that requires privacy protection. However, multimodal RAG systems inherently grant external users indirect access to such data, making them potentially vulnerable to privacy attacks, particularly membership inference attacks (MIAs). % Existing MIA methods targeting RAG systems predominantly focus on the textual modality, while the visual modality remains relatively underexplored. To bridge this gap, we propose MrM, the first black-box MIA framework targeted at multimodal RAG systems. It utilizes a multi-object data perturbation framework constrained by counterfactual attacks, which can concurrently induce the RAG systems to retrieve the target data and generate information that leaks the membership information. Our method first employs an object-aware data perturbation method to constrain the perturbation to key semantics and ensure successful retrieval. Building on this, we design a counterfact-informed mask selection strategy to prioritize the most informative masked regions, aiming to eliminate the interference of model self-knowledge and amplify attack efficacy. Finally, we perform statistical membership inference by modeling query trials to extract features that reflect the reconstruction of masked semantics from response patterns. Experiments on two visual datasets and eight mainstream commercial visual-language models (e.g., GPT-4o, Gemini-2) demonstrate that MrM achieves consistently strong performance across both sample-level and set-level evaluations, and remains robust under adaptive defenses.",
      "tldr_zh": "该研究针对多模态检索增强生成 (Multimodal RAG) 系统中存在的隐私泄露风险，提出了首个针对该系统的黑盒成员推理攻击 (Membership Inference Attack, MIA) 框架 MrM。该框架利用受反事实攻击 (Counterfactual Attacks) 约束的多对象数据扰动机制，能够同时诱导 RAG 系统检索目标数据并生成泄露成员信息的内容。MrM 采用了对象感知的数据扰动方法以确保检索成功，并结合反事实信息掩码选择策略 (Counterfact-informed mask selection) 优先处理信息量最大的遮盖区域，从而消除模型自身知识的干扰并增强攻击效力。研究进一步通过对查询试验进行统计建模，从响应模式中提取反映掩码语义重建特征的指标，实现精确的成员推理。在 GPT-4o 和 Gemini-2 等八种主流商业视觉语言模型 (Visual-Language Models) 上的实验证明，MrM 在样本级和集合级评估中均表现出强大的攻击性能，且在自适应防御下依然保持鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07399v1",
      "published_date": "2025-06-09 03:48:50 UTC",
      "updated_date": "2025-06-09 03:48:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:26:35.830752+00:00"
    },
    {
      "arxiv_id": "2506.07392v4",
      "title": "From Static to Adaptive Defense: Federated Multi-Agent Deep Reinforcement Learning-Driven Moving Target Defense Against DoS Attacks in UAV Swarm Networks",
      "title_zh": "从静态防御到自适应防御：无人机集群网络中联邦多智能体深度强化学习驱动的抗 DoS 攻击移动目标防御",
      "authors": [
        "Yuyang Zhou",
        "Guang Cheng",
        "Kang Du",
        "Zihan Chen",
        "Tian Qin",
        "Yuyu Zhao"
      ],
      "abstract": "The proliferation of UAVs has enabled a wide range of mission-critical applications and is becoming a cornerstone of low-altitude networks, supporting smart cities, emergency response, and more. However, the open wireless environment, dynamic topology, and resource constraints of UAVs expose low-altitude networks to severe DoS threats. Traditional defense approaches, which rely on fixed configurations or centralized decision-making, cannot effectively respond to the rapidly changing conditions in UAV swarm environments. To address these challenges, we propose a novel federated multi-agent deep reinforcement learning (FMADRL)-driven moving target defense (MTD) framework for proactive DoS mitigation in low-altitude networks. Specifically, we design lightweight and coordinated MTD mechanisms, including leader switching, route mutation, and frequency hopping, to disrupt attacker efforts and enhance network resilience. The defense problem is formulated as a multi-agent partially observable Markov decision process, capturing the uncertain nature of UAV swarms under attack. Each UAV is equipped with a policy agent that autonomously selects MTD actions based on partial observations and local experiences. By employing a policy gradient-based algorithm, UAVs collaboratively optimize their policies via reward-weighted aggregation. Extensive simulations demonstrate that our approach significantly outperforms state-of-the-art baselines, achieving up to a 34.6% improvement in attack mitigation rate, a reduction in average recovery time of up to 94.6%, and decreases in energy consumption and defense cost by as much as 29.3% and 98.3%, respectively, under various DoS attack strategies. These results highlight the potential of intelligent, distributed defense mechanisms to protect low-altitude networks, paving the way for reliable and scalable low-altitude economy.",
      "tldr_zh": "该研究针对无人机(UAV)集群在低空网络中面临的拒绝服务(DoS)攻击威胁，提出了一种基于联邦多智能体深度强化学习(Federated Multi-Agent Deep Reinforcement Learning, FMADRL)驱动的移动目标防御(Moving Target Defense, MTD)框架。该框架设计了领导者切换(leader switching)、路径变异(route mutation)和跳频(frequency hopping)等轻量化协作防御机制，实现了从静态防御到自适应防御的转变。研究将防御问题建模为多智能体部分可观测马尔可夫决策过程(MAPOMDP)，通过策略梯度算法和奖励加权聚合实现无人机群体的策略协同优化。仿真结果表明，该方法在攻击缓解率上比现有基线提高了34.6%，平均恢复时间缩短了94.6%，同时显著降低了能耗与防御成本。这一分布式智能防御机制为低空经济背景下无人机网络的可靠性与可扩展性提供了重要保障。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "15pages; Accepted by IEEE TCCN",
      "pdf_url": "https://arxiv.org/pdf/2506.07392v4",
      "published_date": "2025-06-09 03:33:04 UTC",
      "updated_date": "2025-11-20 09:41:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:26:40.720738+00:00"
    },
    {
      "arxiv_id": "2506.07390v1",
      "title": "Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data",
      "title_zh": "借助合成推理数据与课程化偏好优化提升大语言模型的漏洞检测能力",
      "authors": [
        "Xin-Cheng Wen",
        "Yijun Yang",
        "Cuiyun Gao",
        "Yang Xiao",
        "Deheng Ye"
      ],
      "abstract": "Large language models (LLMs) demonstrate considerable proficiency in numerous coding-related tasks; however, their capabilities in detecting software vulnerabilities remain limited. This limitation primarily stems from two factors: (1) the absence of reasoning data related to vulnerabilities, which hinders the models' ability to capture underlying vulnerability patterns; and (2) their focus on learning semantic representations rather than the reason behind them, thus failing to recognize semantically similar vulnerability samples. Furthermore, the development of LLMs specialized in vulnerability detection is challenging, particularly in environments characterized by the scarcity of high-quality datasets. In this paper, we propose a novel framework ReVD that excels at mining vulnerability patterns through reasoning data synthesizing and vulnerability-specific preference optimization. Specifically, we construct forward and backward reasoning processes for vulnerability and corresponding fixed code, ensuring the synthesis of high-quality reasoning data. Moreover, we design the triplet supervised fine-tuning followed by curriculum online preference optimization for enabling ReVD to better understand vulnerability patterns. The extensive experiments conducted on PrimeVul and SVEN datasets demonstrate that ReVD sets new state-of-the-art for LLM-based software vulnerability detection, e.g., 12.24\\%-22.77\\% improvement in the accuracy. The source code and data are available at https://github.com/Xin-Cheng-Wen/PO4Vul.",
      "tldr_zh": "该研究针对大型语言模型(LLMs)在软件漏洞检测(software vulnerability detection)中存在的推理数据缺失以及过度关注语义表征而非底层逻辑等局限性，提出了名为ReVD的创新框架。ReVD通过为漏洞及其修复代码构建正向和反向推理过程，确保了高质量推理数据的合成，从而有效地挖掘漏洞模式(vulnerability patterns)。该框架进一步设计了三元组监督微调(triplet supervised fine-tuning)和课程在线偏好优化(curriculum online preference optimization)，显著增强了模型对漏洞成因的理解能力。在PrimeVul和SVEN数据集上的实验结果显示，ReVD在准确率上提升了12.24%至22.77%，刷新了基于LLMs的软件漏洞检测技术的最先进水平(SOTA)。该研究及其开源代码为解决高质量安全数据集稀缺环境下的漏洞分析问题提供了重要参考。",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2506.07390v1",
      "published_date": "2025-06-09 03:25:23 UTC",
      "updated_date": "2025-06-09 03:25:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:27:00.544632+00:00"
    },
    {
      "arxiv_id": "2506.07388v1",
      "title": "Shapley-Coop: Credit Assignment for Emergent Cooperation in Self-Interested LLM Agents",
      "title_zh": "Shapley-Coop：面向自利型 LLM 智能体涌现合作的信用分配",
      "authors": [
        "Yun Hua",
        "Haosheng Chen",
        "Shiqin Wang",
        "Wenhao Li",
        "Xiangfeng Wang",
        "Jun Luo"
      ],
      "abstract": "Large Language Models (LLMs) show strong collaborative performance in multi-agent systems with predefined roles and workflows. However, in open-ended environments lacking coordination rules, agents tend to act in self-interested ways. The central challenge in achieving coordination lies in credit assignment -- fairly evaluating each agent's contribution and designing pricing mechanisms that align their heterogeneous goals. This problem is critical as LLMs increasingly participate in complex human-AI collaborations, where fair compensation and accountability rely on effective pricing mechanisms. Inspired by how human societies address similar coordination challenges (e.g., through temporary collaborations such as employment or subcontracting), we propose a cooperative workflow, Shapley-Coop. Shapley-Coop integrates Shapley Chain-of-Thought -- leveraging marginal contributions as a principled basis for pricing -- with structured negotiation protocols for effective price matching, enabling LLM agents to coordinate through rational task-time pricing and post-task reward redistribution. This approach aligns agent incentives, fosters cooperation, and maintains autonomy. We evaluate Shapley-Coop across two multi-agent games and a software engineering simulation, demonstrating that it consistently enhances LLM agent collaboration and facilitates equitable credit assignment. These results highlight the effectiveness of Shapley-Coop's pricing mechanisms in accurately reflecting individual contributions during task execution.",
      "tldr_zh": "该研究提出了 Shapley-Coop，一种旨在解决自利型 (Self-Interested) LLM 智能体在开放环境中因缺乏协调规则而难以协作的合作工作流。针对信用分配 (Credit Assignment) 这一核心挑战，该框架引入了 Shapley Chain-of-Thought，利用边际贡献 (Marginal Contributions) 作为定价的原则基础，并结合结构化谈判协议实现有效的价格匹配。通过理性的任务时定价和任务后奖励重分配，Shapley-Coop 成功对齐了智能体的激励机制，在保持自主性的同时促进了自发协作。在多智能体博弈和软件工程模拟实验中，该方法显著提升了 LLM 智能体的协作效率，并实现了公平的信用评价。实验结果证明，Shapley-Coop 的定价机制能准确反映个体在任务执行过程中的实际贡献。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07388v1",
      "published_date": "2025-06-09 03:24:01 UTC",
      "updated_date": "2025-06-09 03:24:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:27:01.889525+00:00"
    },
    {
      "arxiv_id": "2506.07376v1",
      "title": "Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation",
      "title_zh": "Adapter天然地充当跨域小样本语义分割的解耦器",
      "authors": [
        "Jintao Tong",
        "Ran Ma",
        "Yixiong Zou",
        "Guangyao Chen",
        "Yuhua Li",
        "Ruixuan Li"
      ],
      "abstract": "Cross-domain few-shot segmentation (CD-FSS) is proposed to pre-train the model on a source-domain dataset with sufficient samples, and then transfer the model to target-domain datasets where only a few samples are available for efficient fine-tuning. There are majorly two challenges in this task: (1) the domain gap and (2) fine-tuning with scarce data. To solve these challenges, we revisit the adapter-based methods, and discover an intriguing insight not explored in previous works: the adapter not only helps the fine-tuning of downstream tasks but also naturally serves as a domain information decoupler. Then, we delve into this finding for an interpretation, and find the model's inherent structure could lead to a natural decoupling of domain information. Building upon this insight, we propose the Domain Feature Navigator (DFN), which is a structure-based decoupler instead of loss-based ones like current works, to capture domain-specific information, thereby directing the model's attention towards domain-agnostic knowledge. Moreover, to prevent the potential excessive overfitting of DFN during the source-domain training, we further design the SAM-SVN method to constrain DFN from learning sample-specific knowledge. On target domains, we freeze the model and fine-tune the DFN to learn target-specific knowledge specific. Extensive experiments demonstrate that our method surpasses the state-of-the-art method in CD-FSS significantly by 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios, respectively.",
      "tldr_zh": "这项研究针对跨域小样本分割(CD-FSS)面临的领域差距(domain gap)和极少样本微调的挑战，重新评估了基于适配器(adapter-based)的方法。研究发现适配器不仅能辅助下游任务微调，还能作为一种天然的领域信息解耦器(domain information decoupler)。基于这一发现，论文提出了领域特征导航器(Domain Feature Navigator, DFN)，这是一种基于结构的解耦器，用于捕获领域特定信息，从而使模型更关注于领域无关知识(domain-agnostic knowledge)。为了防止DFN在源域训练时出现过拟合，研究进一步设计了SAM-SVN方法，限制其学习样本特定知识。在目标域应用时，通过冻结模型并微调DFN来捕获目标域特定知识。实验结果表明，该方法在1-shot和5-shot场景下分别比现有最优方法提升了2.69%和4.68%的平均交并比(MIoU)，证明了其在跨域场景下的卓越性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "ICML 2025 Spotlight",
      "pdf_url": "https://arxiv.org/pdf/2506.07376v1",
      "published_date": "2025-06-09 02:51:06 UTC",
      "updated_date": "2025-06-09 02:51:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:27:06.920385+00:00"
    },
    {
      "arxiv_id": "2506.07373v1",
      "title": "HyColor: An Efficient Heuristic Algorithm for Graph Coloring",
      "title_zh": "HyColor：一种高效的图着色启发式算法",
      "authors": [
        "Enqiang Zhu",
        "Yu Zhang",
        "Haopeng Sun",
        "Ziqi Wei",
        "Witold Pedrycz",
        "Chanjuan Liu",
        "Jin Xu"
      ],
      "abstract": "The graph coloring problem (GCP) is a classic combinatorial optimization problem that aims to find the minimum number of colors assigned to vertices of a graph such that no two adjacent vertices receive the same color. GCP has been extensively studied by researchers from various fields, including mathematics, computer science, and biological science. Due to the NP-hard nature, many heuristic algorithms have been proposed to solve GCP. However, existing GCP algorithms focus on either small hard graphs or large-scale sparse graphs (with up to 10^7 vertices). This paper presents an efficient hybrid heuristic algorithm for GCP, named HyColor, which excels in handling large-scale sparse graphs while achieving impressive results on small dense graphs. The efficiency of HyColor comes from the following three aspects: a local decision strategy to improve the lower bound on the chromatic number; a graph-reduction strategy to reduce the working graph; and a k-core and mixed degree-based greedy heuristic for efficiently coloring graphs. HyColor is evaluated against three state-of-the-art GCP algorithms across four benchmarks, comprising three large-scale sparse graph benchmarks and one small dense graph benchmark, totaling 209 instances. The results demonstrate that HyColor consistently outperforms existing heuristic algorithms in both solution accuracy and computational efficiency for the majority of instances. Notably, HyColor achieved the best solutions in 194 instances (over 93%), with 34 of these solutions significantly surpassing those of other algorithms. Furthermore, HyColor successfully determined the chromatic number and achieved optimal coloring in 128 instances.",
      "tldr_zh": "该研究针对经典的组合优化问题图着色问题(Graph Coloring Problem, GCP)提出了一种名为HyColor的高效混合启发式算法。HyColor旨在同时应对大规模稀疏图的高效求解和小型稠密图的精确着色，解决了现有算法在不同规模和密度图上的局限性。该算法的效率主要源于提升色数下界的局部决策策略、减小工作图规模的图约减策略，以及结合k-core和混合度数的贪心启发式着色方法。在涉及209个实例的四个基准测试中，HyColor在求解精度和计算效率上均优于现有的三种最先进算法。实验结果表明，HyColor在超过93%的实例中获得了最佳解，并在128个实例中成功确定了色数及最优着色方案。这一研究为大规模NP-hard优化问题的求解提供了兼顾准确性与运行效率的新思路。",
      "categories": [
        "cs.DM",
        "cs.AI"
      ],
      "primary_category": "cs.DM",
      "comment": "14 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.07373v1",
      "published_date": "2025-06-09 02:45:08 UTC",
      "updated_date": "2025-06-09 02:45:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:27:04.779117+00:00"
    },
    {
      "arxiv_id": "2506.07368v2",
      "title": "C3S3: Complementary Competition and Contrastive Selection for Semi-Supervised Medical Image Segmentation",
      "title_zh": "C3S3：结合互补竞争与对比选择的半监督医学图像分割",
      "authors": [
        "Jiaying He",
        "Yitong Lin",
        "Jiahe Chen",
        "Honghui Xu",
        "Jianwei Zheng"
      ],
      "abstract": "For the immanent challenge of insufficiently annotated samples in the medical field, semi-supervised medical image segmentation (SSMIS) offers a promising solution. Despite achieving impressive results in delineating primary target areas, most current methodologies struggle to precisely capture the subtle details of boundaries. This deficiency often leads to significant diagnostic inaccuracies. To tackle this issue, we introduce C3S3, a novel semi-supervised segmentation model that synergistically integrates complementary competition and contrastive selection. This design significantly sharpens boundary delineation and enhances overall precision. Specifically, we develop an Outcome-Driven Contrastive Learning module dedicated to refining boundary localization. Additionally, we incorporate a Dynamic Complementary Competition module that leverages two high-performing sub-networks to generate pseudo-labels, thereby further improving segmentation quality. The proposed C3S3 undergoes rigorous validation on two publicly accessible datasets, encompassing the practices of both MRI and CT scans. The results demonstrate that our method achieves superior performance compared to previous cutting-edge competitors. Especially, on the 95HD and ASD metrics, our approach achieves a notable improvement of at least 6%, highlighting the significant advancements. The code is available at https://github.com/Y-TARL/C3S3.",
      "tldr_zh": "该研究针对医疗领域标注样本不足的问题，提出了 C3S3 模型，旨在解决半监督医学图像分割(Semi-Supervised Medical Image Segmentation)在捕捉边界细节方面的局限。C3S3 通过协同集成互补竞争与对比选择机制，显著增强了边界刻画能力并提升了整体精度。具体而言，该模型开发了结果驱动对比学习(Outcome-Driven Contrastive Learning)模块用于细化边界定位，并引入动态互补竞争(Dynamic Complementary Competition)模块，利用两个高性能子网络生成高质量伪标签。在 MRI 和 CT 扫描数据集上的实验结果表明，C3S3 的性能优于现有的先进方法，特别是在 95HD 和 ASD 指标上实现了至少 6% 的显著提升。这一研究为实现高精度、鲁棒的医学图像自动分割提供了有效的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICME 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.07368v2",
      "published_date": "2025-06-09 02:34:19 UTC",
      "updated_date": "2025-06-25 05:23:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:27:02.872784+00:00"
    },
    {
      "arxiv_id": "2506.07364v1",
      "title": "Multiple Object Stitching for Unsupervised Representation Learning",
      "title_zh": "面向无监督表示学习的多目标拼接",
      "authors": [
        "Chengchao Shen",
        "Dawei Liu",
        "Jianxin Wang"
      ],
      "abstract": "Contrastive learning for single object centric images has achieved remarkable progress on unsupervised representation, but suffering inferior performance on the widespread images with multiple objects. In this paper, we propose a simple but effective method, Multiple Object Stitching (MOS), to refine the unsupervised representation for multi-object images. Specifically, we construct the multi-object images by stitching the single object centric ones, where the objects in the synthesized multi-object images are predetermined. Hence, compared to the existing contrastive methods, our method provides additional object correspondences between multi-object images without human annotations. In this manner, our method pays more attention to the representations of each object in multi-object image, thus providing more detailed representations for complicated downstream tasks, such as object detection and semantic segmentation. Experimental results on ImageNet, CIFAR and COCO datasets demonstrate that our proposed method achieves the leading unsupervised representation performance on both single object centric images and multi-object ones. The source code is available at https://github.com/visresearch/MultipleObjectStitching.",
      "tldr_zh": "该研究针对Contrastive Learning在处理包含多个物体的广泛图像时表现不佳的问题，提出了Multiple Object Stitching (MOS)方法以优化多目标图像的无监督表示。MOS通过将单目标中心的图像进行拼接来构建合成的多目标图像，从而在无需人工标注的情况下，利用预定义的物体对应关系引导模型学习。这种方法使模型能够更深入地捕捉多目标图像中每个物体的特征表示，从而为Object Detection和Semantic Segmentation等复杂的下游任务提供更详尽的特征信息。在ImageNet、CIFAR和COCO数据集上的实验结果表明，该方法在单目标中心图像和多目标图像的Unsupervised Representation Learning任务中均取得了领先的性能表现。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07364v1",
      "published_date": "2025-06-09 02:28:21 UTC",
      "updated_date": "2025-06-09 02:28:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:27:24.540409+00:00"
    },
    {
      "arxiv_id": "2506.11114v1",
      "title": "KokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations",
      "title_zh": "KokushiMD-10：评估大语言模型在十项日本国家医疗执业资格考试中表现的基准",
      "authors": [
        "Junyu Liu",
        "Kaiqi Yan",
        "Tianyang Wang",
        "Qian Niu",
        "Momoko Nagai-Tanima",
        "Tomoki Aoyama"
      ],
      "abstract": "Recent advances in large language models (LLMs) have demonstrated notable performance in medical licensing exams. However, comprehensive evaluation of LLMs across various healthcare roles, particularly in high-stakes clinical scenarios, remains a challenge. Existing benchmarks are typically text-based, English-centric, and focus primarily on medicines, which limits their ability to assess broader healthcare knowledge and multimodal reasoning. To address these gaps, we introduce KokushiMD-10, the first multimodal benchmark constructed from ten Japanese national healthcare licensing exams. This benchmark spans multiple fields, including Medicine, Dentistry, Nursing, Pharmacy, and allied health professions. It contains over 11588 real exam questions, incorporating clinical images and expert-annotated rationales to evaluate both textual and visual reasoning. We benchmark over 30 state-of-the-art LLMs, including GPT-4o, Claude 3.5, and Gemini, across both text and image-based settings. Despite promising results, no model consistently meets passing thresholds across domains, highlighting the ongoing challenges in medical AI. KokushiMD-10 provides a comprehensive and linguistically grounded resource for evaluating and advancing reasoning-centric medical AI across multilingual and multimodal clinical tasks.",
      "tldr_zh": "该研究推出了 KokushiMD-10，这是首个基于十项日本国家医疗执业资格考试构建的多模态基准测试，涵盖了 Medicine、Dentistry、Nursing 和 Pharmacy 等多个医疗专业领域。该基准包含超过 11,588 道真实考题，并整合了临床图像和专家标注的推理过程，旨在全面评估大语言模型 (LLMs) 的文本与视觉推理能力。研究团队对包括 GPT-4o、Claude 3.5 和 Gemini 在内的 30 多种尖端模型进行了广泛测试。实验结果显示，目前尚无模型能在所有领域稳定达到及格分数线，这反映了医疗人工智能在处理复杂临床场景时仍面临巨大挑战。KokushiMD-10 为推动多语言、多模态临床任务中的推理导向型医疗人工智能提供了重要的资源支撑与评价标准。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.11114v1",
      "published_date": "2025-06-09 02:26:02 UTC",
      "updated_date": "2025-06-09 02:26:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:27:29.446545+00:00"
    },
    {
      "arxiv_id": "2506.17258v1",
      "title": "A Digital Twin Framework for Generation-IV Reactors with Reinforcement Learning-Enabled Health-Aware Supervisory Control",
      "title_zh": "面向第四代反应堆的强化学习赋能健康感知监督控制数字孪生框架",
      "authors": [
        "Jasmin Y. Lim",
        "Dimitrios Pylorof",
        "Humberto E. Garcia",
        "Karthik Duraisamy"
      ],
      "abstract": "Generation IV (Gen-IV) nuclear power plants are envisioned to replace the current reactor fleet, bringing improvements in performance, safety, reliability, and sustainability. However, large cost investments currently inhibit the deployment of these advanced reactor concepts. Digital twins bridge real-world systems with digital tools to reduce costs, enhance decision-making, and boost operational efficiency. In this work, a digital twin framework is designed to operate the Gen-IV Fluoride-salt-cooled High-temperature Reactor, utilizing data-enhanced methods to optimize operational and maintenance policies while adhering to system constraints. The closed-loop framework integrates surrogate modeling, reinforcement learning, and Bayesian inference to streamline end-to-end communication for online regulation and self-adjustment. Reinforcement learning is used to consider component health and degradation to drive the target power generations, with constraints enforced through a Reference Governor control algorithm that ensures compliance with pump flow rate and temperature limits. These input driving modules benefit from detailed online simulations that are assimilated to measurement data with Bayesian filtering. The digital twin is demonstrated in three case studies: a one-year long-term operational period showcasing maintenance planning capabilities, short-term accuracy refinement with high-frequency measurements, and system shock capturing that demonstrates real-time recalibration capabilities when change in boundary conditions. These demonstrations validate robustness for health-aware and constraint-informed nuclear plant operation, with general applicability to other advanced reactor concepts and complex engineering systems.",
      "tldr_zh": "该研究提出了一个面向第四代(Generation-IV)核反应堆的数字孪生(Digital Twin)框架，旨在通过数据增强方法优化运营与维护策略并降低成本。该框架集成了代理模型(Surrogate Modeling)、强化学习(Reinforcement Learning)和贝叶斯推理(Bayesian Inference)，构建了支持在线调节与自调整的闭环系统。其中，强化学习(Reinforcement Learning)被用于在考虑组件健康和退化的前提下驱动目标功率生成，并利用参考调节器(Reference Governor)算法确保系统运行严格遵守泵流速和温度等物理约束。通过贝叶斯滤波(Bayesian Filtering)技术，框架能够将详细的在线模拟与实时测量数据进行同化，从而实现精准的状态监测。研究通过一年期的长期运维规划、高频测量的短期精度细化以及系统冲击下的实时校准三个案例，验证了该数字孪生(Digital Twin)系统在健康感知和约束驱动运行中的稳健性。该成果不仅适用于氟化盐冷却高温反应堆(Fluoride-salt-cooled High-temperature Reactor)，也为其他先进核反应堆概念和复杂工程系统的智能化监管提供了普适的解决方案。",
      "categories": [
        "eess.SY",
        "cs.AI"
      ],
      "primary_category": "eess.SY",
      "comment": "39 pages, 22 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.17258v1",
      "published_date": "2025-06-09 02:23:34 UTC",
      "updated_date": "2025-06-09 02:23:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:27:31.843374+00:00"
    },
    {
      "arxiv_id": "2506.07358v1",
      "title": "Lightweight Joint Audio-Visual Deepfake Detection via Single-Stream Multi-Modal Learning Framework",
      "title_zh": "基于单流多模态学习框架的轻量级视听联合深度伪造检测",
      "authors": [
        "Kuiyuan Zhang",
        "Wenjie Pei",
        "Rushi Lan",
        "Yifang Guo",
        "Zhongyun Hua"
      ],
      "abstract": "Deepfakes are AI-synthesized multimedia data that may be abused for spreading misinformation. Deepfake generation involves both visual and audio manipulation. To detect audio-visual deepfakes, previous studies commonly employ two relatively independent sub-models to learn audio and visual features, respectively, and fuse them subsequently for deepfake detection. However, this may underutilize the inherent correlations between audio and visual features. Moreover, utilizing two isolated feature learning sub-models can result in redundant neural layers, making the overall model inefficient and impractical for resource-constrained environments.\n  In this work, we design a lightweight network for audio-visual deepfake detection via a single-stream multi-modal learning framework. Specifically, we introduce a collaborative audio-visual learning block to efficiently integrate multi-modal information while learning the visual and audio features. By iteratively employing this block, our single-stream network achieves a continuous fusion of multi-modal features across its layers. Thus, our network efficiently captures visual and audio features without the need for excessive block stacking, resulting in a lightweight network design. Furthermore, we propose a multi-modal classification module that can boost the dependence of the visual and audio classifiers on modality content. It also enhances the whole resistance of the video classifier against the mismatches between audio and visual modalities. We conduct experiments on the DF-TIMIT, FakeAVCeleb, and DFDC benchmark datasets. Compared to state-of-the-art audio-visual joint detection methods, our method is significantly lightweight with only 0.48M parameters, yet it achieves superiority in both uni-modal and multi-modal deepfakes, as well as in unseen types of deepfakes.",
      "tldr_zh": "该研究针对Deepfakes检测中传统双流模型难以利用音视频相关性且模型冗余的问题，提出了一种轻量级的单流多模态学习框架。该框架引入了Collaborative audio-visual learning block，在学习视觉和音频特征的同时高效整合多模态信息，通过迭代应用该模块实现跨层的多模态特征连续融合（Continuous fusion）。这种单流设计使模型无需堆叠过多层级即可捕获关键特征，从而实现了极高的Lightweight设计效率。此外，研究提出的Multi-modal classification module增强了分类器对模态内容的依赖，并提升了对音视频不匹配（Mismatches）的抵抗力。在DF-TIMIT、FakeAVCeleb和DFDC数据集上的实验结果显示，该方法参数量仅为0.48M，在保持极轻量化的同时，其检测单模态、多模态及未知类型深度伪造的性能均优于现有的SOTA方法。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07358v1",
      "published_date": "2025-06-09 02:13:04 UTC",
      "updated_date": "2025-06-09 02:13:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:27:33.642808+00:00"
    },
    {
      "arxiv_id": "2506.07355v2",
      "title": "SALT: A Lightweight Model Adaptation Method for Closed Split Computing Environments",
      "title_zh": "SALT：面向封闭式拆分计算环境的轻量级模型适配方法",
      "authors": [
        "Yuya Okada",
        "Takayuki Nishio"
      ],
      "abstract": "We propose SALT (Split-Adaptive Lightweight Tuning), a lightweight model adaptation framework for Split Computing under closed constraints, where the head and tail networks are proprietary and inaccessible to users. In such closed environments, conventional adaptation methods are infeasible since they require access to model parameters or architectures. SALT addresses this challenge by introducing a compact, trainable adapter on the client side to refine latent features from the head network, enabling user-specific adaptation without modifying the original models or increasing communication overhead. We evaluate SALT on user-specific classification tasks with CIFAR-10 and CIFAR-100, demonstrating improved accuracy with lower training latency compared to fine-tuning methods. Furthermore, SALT facilitates model adaptation for robust inference over lossy networks, a common challenge in edge-cloud environments. With minimal deployment overhead, SALT offers a practical solution for personalized inference in edge AI systems under strict system constraints.",
      "tldr_zh": "该研究提出了 SALT (Split-Adaptive Lightweight Tuning)，这是一种针对封闭约束下的 Split Computing 环境设计的轻量级模型适配框架，旨在解决头部和尾部网络因私有属性而无法被用户访问的问题。在这种封闭环境下，传统的适配方法由于无法获取模型参数或架构而难以实施，而 SALT 通过在客户端引入一个紧凑且可训练的 adapter 来精炼来自头部网络的 latent features，从而在不修改原始模型或增加通信开销的前提下实现用户特定的适配。实验在 CIFAR-10 和 CIFAR-100 上的分类任务中验证了该方法的有效性，结果显示其比 fine-tuning 方法具有更高的准确率和更低的训练延迟。此外，SALT 还能在 lossy networks 环境中实现鲁棒推理，有效应对边缘云系统中的通信挑战。凭借极低的部署开销，SALT 为严格约束下的 edge AI 系统提供了一种实用的个性化推理解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, submitted to IEEE Globecom 2025 (under review)",
      "pdf_url": "https://arxiv.org/pdf/2506.07355v2",
      "published_date": "2025-06-09 02:08:02 UTC",
      "updated_date": "2025-06-14 10:41:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:27:36.537504+00:00"
    },
    {
      "arxiv_id": "2506.07347v1",
      "title": "Distributed Risk-Sensitive Safety Filters for Uncertain Discrete-Time Systems",
      "title_zh": "不确定离散时间系统的分布式风险敏感安全滤波器",
      "authors": [
        "Armin Lederer",
        "Erfaun Noorani",
        "Andreas Krause"
      ],
      "abstract": "Ensuring safety in multi-agent systems is a significant challenge, particularly in settings where centralized coordination is impractical. In this work, we propose a novel risk-sensitive safety filter for discrete-time multi-agent systems with uncertain dynamics that leverages control barrier functions (CBFs) defined through value functions. Our approach relies on centralized risk-sensitive safety conditions based on exponential risk operators to ensure robustness against model uncertainties. We introduce a distributed formulation of the safety filter by deriving two alternative strategies: one based on worst-case anticipation and another on proximity to a known safe policy. By allowing agents to switch between strategies, feasibility can be ensured. Through detailed numerical evaluations, we demonstrate the efficacy of our approach in maintaining safety without being overly conservative.",
      "tldr_zh": "该研究针对动力学具有不确定性的离散时间多智能体系统，提出了一种新型的风险敏感安全滤波器 (risk-sensitive safety filter)，以解决去中心化环境下的协同安全挑战。该方法核心在于利用通过价值函数定义的控制障碍函数 (Control Barrier Functions, CBFs)，并结合基于指数风险算子 (exponential risk operators) 的安全条件，显著增强了系统对模型不确定性的鲁棒性。研究进一步设计了分布式实现方案，推导出基于最坏情况预测 (worst-case anticipation) 和接近已知安全策略 (proximity to a known safe policy) 的两种替代策略。通过允许智能体在不同策略间灵活切换，该框架有效保障了系统运行的可行性。实验结果表明，该方法在维持多智能体系统安全性的同时，成功克服了传统方案过于保守的问题。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.07347v1",
      "published_date": "2025-06-09 01:48:25 UTC",
      "updated_date": "2025-06-09 01:48:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:27:43.914049+00:00"
    },
    {
      "arxiv_id": "2506.07339v2",
      "title": "Real-Time Execution of Action Chunking Flow Policies",
      "title_zh": "动作分块流策略的实时执行",
      "authors": [
        "Kevin Black",
        "Manuel Y. Galliker",
        "Sergey Levine"
      ],
      "abstract": "Modern AI systems, especially those interacting with the physical world, increasingly require real-time performance. However, the high latency of state-of-the-art generalist models, including recent vision-language action models (VLAs), poses a significant challenge. While action chunking has enabled temporal consistency in high-frequency control tasks, it does not fully address the latency problem, leading to pauses or out-of-distribution jerky movements at chunk boundaries. This paper presents a novel inference-time algorithm that enables smooth asynchronous execution of action chunking policies. Our method, real-time chunking (RTC), is applicable to any diffusion- or flow-based VLA out of the box with no re-training. It generates the next action chunk while executing the current one, \"freezing\" actions guaranteed to execute and \"inpainting\" the rest. To test RTC, we introduce a new benchmark of 12 highly dynamic tasks in the Kinetix simulator, as well as evaluate 6 challenging real-world bimanual manipulation tasks. Results demonstrate that RTC is fast, performant, and uniquely robust to inference delay, significantly improving task throughput and enabling high success rates in precise tasks $\\unicode{x2013}$ such as lighting a match $\\unicode{x2013}$ even in the presence of significant latency. See https://pi.website/research/real_time_chunking for videos.",
      "tldr_zh": "该研究针对视觉语言动作模型(VLA)在物理世界交互中面临的高延迟和动作不连贯问题，提出了一种名为Real-Time Chunking (RTC)的推理算法，旨在实现动作分块(Action Chunking)策略的平滑异步执行。RTC无需重新训练，可直接应用于任何基于扩散(Diffusion)或流(Flow)的VLA模型，其核心机制是在执行当前动作块的同时生成下一个动作块，通过“冻结”已确定的动作并“补全”(Inpainting)剩余部分来消除执行间隙。研究团队在Kinetix模拟器的12项高动态任务及6项真实世界双臂操控任务中进行了验证，结果表明RTC对推理延迟具有极强的鲁棒性，并显著提升了任务吞吐量。实验证明，即便在显著延迟的环境下，RTC依然能在划火柴等高精度任务中保持极高的成功率，为实时物理智能系统的部署提供了高效的解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "published in NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.07339v2",
      "published_date": "2025-06-09 01:01:59 UTC",
      "updated_date": "2025-12-05 07:35:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:27:44.318046+00:00"
    },
    {
      "arxiv_id": "2506.07335v2",
      "title": "Improving LLM Reasoning through Interpretable Role-Playing Steering",
      "title_zh": "通过可解释角色扮演引导提升大语言模型推理能力",
      "authors": [
        "Anyi Wang",
        "Dong Shu",
        "Yifan Wang",
        "Yunpu Ma",
        "Mengnan Du"
      ],
      "abstract": "Role-playing has emerged as an effective technique for enhancing the reasoning capabilities of large language models (LLMs). However, existing methods primarily rely on prompt engineering, which often lacks stability and interpretability. In this paper, we introduce Sparse Autoencoder Role-Playing Steering (SRPS), a novel framework that identifies and manipulates internal model features associated with role-playing behavior. Our approach extracts latent representations from role-play prompts, selects the most relevant features based on activation patterns, and constructs a steering vector that can be injected into the model's residual stream with controllable intensity. Our method enables fine-grained control over role-specific behavior and offers insights into how role information influences internal model activations. Extensive experiments across various reasoning benchmarks and model sizes demonstrate consistent performance gains. Notably, in the zero-shot chain-of-thought (CoT) setting, the accuracy of Llama3.1-8B on CSQA improves from 31.86% to 39.80%, while Gemma2-9B on SVAMP increases from 37.50% to 45.10%. These results highlight the potential of SRPS to enhance reasoning ability in LLMs, providing better interpretability and stability compared to traditional prompt-based role-playing.",
      "tldr_zh": "该研究提出了 Sparse Autoencoder Role-Playing Steering (SRPS) 框架，旨在解决现有提示工程 (prompt engineering) 在利用角色扮演提升大语言模型 (LLMs) 推理能力时存在的稳定性与可解释性不足问题。该框架通过稀疏自动编码器 (Sparse Autoencoder) 从角色扮演提示中提取潜在表征 (latent representations)，筛选出激活模式中最相关的特征，并构建引导向量 (steering vector) 以可控强度注入模型的残差流 (residual stream)。通过实现对角色特定行为的精细化控制，SRPS 提供了关于角色信息如何影响模型内部激活的深入见解。实验结果表明，该方法在多个推理基准测试中取得了一致的性能提升，例如在零样本链式思维 (zero-shot CoT) 设置下，Llama3.1-8B 在 CSQA 任务上的准确率从 31.86% 显著提升至 39.80%。相比传统的基于提示的角色扮演方法，SRPS 在增强 LLMs 推理能力方面展现出更优的可解释性和稳定性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at EMNLP 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2506.07335v2",
      "published_date": "2025-06-09 00:31:17 UTC",
      "updated_date": "2025-09-28 10:11:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:27:50.744375+00:00"
    },
    {
      "arxiv_id": "2506.07330v1",
      "title": "JavelinGuard: Low-Cost Transformer Architectures for LLM Security",
      "title_zh": "JavelinGuard：面向大语言模型安全的低成本 Transformer 架构",
      "authors": [
        "Yash Datta",
        "Sharath Rajasekar"
      ],
      "abstract": "We present JavelinGuard, a suite of low-cost, high-performance model architectures designed for detecting malicious intent in Large Language Model (LLM) interactions, optimized specifically for production deployment. Recent advances in transformer architectures, including compact BERT(Devlin et al. 2019) variants (e.g., ModernBERT (Warner et al. 2024)), allow us to build highly accurate classifiers with as few as approximately 400M parameters that achieve rapid inference speeds even on standard CPU hardware. We systematically explore five progressively sophisticated transformer-based architectures: Sharanga (baseline transformer classifier), Mahendra (enhanced attention-weighted pooling with deeper heads), Vaishnava and Ashwina (hybrid neural ensemble architectures), and Raudra (an advanced multi-task framework with specialized loss functions). Our models are rigorously benchmarked across nine diverse adversarial datasets, including popular sets like the NotInject series, BIPIA, Garak, ImprovedLLM, ToxicChat, WildGuard, and our newly introduced JavelinBench, specifically crafted to test generalization on challenging borderline and hard-negative cases. Additionally, we compare our architectures against leading open-source guardrail models as well as large decoder-only LLMs such as gpt-4o, demonstrating superior cost-performance trade-offs in terms of accuracy, and latency. Our findings reveal that while Raudra's multi-task design offers the most robust performance overall, each architecture presents unique trade-offs in speed, interpretability, and resource requirements, guiding practitioners in selecting the optimal balance of complexity and efficiency for real-world LLM security applications.",
      "tldr_zh": "该研究提出了 JavelinGuard，一套专为检测大语言模型（LLM）交互中恶意意图而设计的低成本、高性能架构，旨在优化生产环境的部署。JavelinGuard 基于 ModernBERT 等紧凑型 Transformer 架构构建，通过约 400M 的参数量实现了在标准 CPU 硬件上也能快速推理的能力。研究系统地探索了包括基础分类器 Sharanga、增强池化的 Mahendra、混合神经集成架构 Vaishnava 和 Ashwina，以及具备多任务框架和专门损失函数的 Raudra 在内的五种递进架构。这些模型在包括 NotInject、BIPIA、Garak、WildGuard 以及新提出的 JavelinBench 在内的九个多样化对抗性数据集上进行了严谨的基准测试。实验结果表明，与领先的开源护栏模型（Guardrail models）以及 gpt-4o 等大型解码器模型相比，JavelinGuard 在准确率和延迟方面表现出更优的成本效益比。虽然 Raudra 的多任务设计在整体上提供了最稳健的性能，但各架构在速度、可解释性和资源需求之间展现了独特的平衡，为现实世界中 LLM 安全应用的最佳选择提供了指南。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 1 Figure and 5 Tables",
      "pdf_url": "https://arxiv.org/pdf/2506.07330v1",
      "published_date": "2025-06-09 00:11:06 UTC",
      "updated_date": "2025-06-09 00:11:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T20:27:49.863050+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 189,
  "processed_papers_count": 189,
  "failed_papers_count": 0,
  "llm_backup_calls": 2,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T20:29:49.700674+00:00"
}
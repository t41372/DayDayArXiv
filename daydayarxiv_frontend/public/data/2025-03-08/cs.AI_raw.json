[
  {
    "arxiv_id": "2503.10662v1",
    "title": "Evaluation of the Automated Labeling Method for Taxonomic Nomenclature Through Prompt-Optimized Large Language Model",
    "authors": [
      "Keito Inoshita",
      "Kota Nojiri",
      "Haruto Sugeno",
      "Takumi Taga"
    ],
    "abstract": "Scientific names of organisms consist of a genus name and a species epithet,\nwith the latter often reflecting aspects such as morphology, ecology,\ndistribution, and cultural background. Traditionally, researchers have manually\nlabeled species names by carefully examining taxonomic descriptions, a process\nthat demands substantial time and effort when dealing with large datasets. This\nstudy evaluates the feasibility of automatic species name labeling using large\nlanguage model (LLM) by leveraging their text classification and semantic\nextraction capabilities. Using the spider name dataset compiled by Mammola et\nal., we compared LLM-based labeling results-enhanced through prompt\nengineering-with human annotations. The results indicate that LLM-based\nclassification achieved high accuracy in Morphology, Geography, and People\ncategories. However, classification accuracy was lower in Ecology & Behavior\nand Modern & Past Culture, revealing challenges in interpreting animal behavior\nand cultural contexts. Future research will focus on improving accuracy through\noptimized few-shot learning and retrieval-augmented generation techniques,\nwhile also expanding the applicability of LLM-based labeling to diverse\nbiological taxa.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This paper will be submitted to IEEE IAICT",
    "pdf_url": "http://arxiv.org/pdf/2503.10662v1",
    "published_date": "2025-03-08 23:11:43 UTC",
    "updated_date": "2025-03-08 23:11:43 UTC"
  },
  {
    "arxiv_id": "2503.06353v1",
    "title": "The AI Pentad, the CHARME$^{2}$D Model, and an Assessment of Current-State AI Regulation",
    "authors": [
      "Di Kevin Gao",
      "Sudip Mittal",
      "Jiming Wu",
      "Hongwei Du",
      "Jingdao Chen",
      "Shahram Rahimi"
    ],
    "abstract": "Artificial Intelligence (AI) has made remarkable progress in the past few\nyears with AI-enabled applications beginning to permeate every aspect of our\nsociety. Despite the widespread consensus on the need to regulate AI, there\nremains a lack of a unified approach to framing, developing, and assessing AI\nregulations. Many of the existing methods take a value-based approach, for\nexample, accountability, fairness, free from bias, transparency, and trust.\nHowever, these methods often face challenges at the outset due to disagreements\nin academia over the subjective nature of these definitions. This paper aims to\nestablish a unifying model for AI regulation from the perspective of core AI\ncomponents. We first introduce the AI Pentad, which comprises the five\nessential components of AI: humans and organizations, algorithms, data,\ncomputing, and energy. We then review AI regulatory enablers, including AI\nregistration and disclosure, AI monitoring, and AI enforcement mechanisms.\nSubsequently, we present the CHARME$^{2}$D Model to explore further the\nrelationship between the AI Pentad and AI regulatory enablers. Finally, we\napply the CHARME$^{2}$D model to assess AI regulatory efforts in the European\nUnion (EU), China, the United Arab Emirates (UAE), the United Kingdom (UK), and\nthe United States (US), highlighting their strengths, weaknesses, and gaps.\nThis comparative evaluation offers insights for future legislative work in the\nAI domain.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06353v1",
    "published_date": "2025-03-08 22:58:41 UTC",
    "updated_date": "2025-03-08 22:58:41 UTC"
  },
  {
    "arxiv_id": "2503.06343v1",
    "title": "Studying the Interplay Between the Actor and Critic Representations in Reinforcement Learning",
    "authors": [
      "Samuel Garcin",
      "Trevor McInroe",
      "Pablo Samuel Castro",
      "Prakash Panangaden",
      "Christopher G. Lucas",
      "David Abel",
      "Stefano V. Albrecht"
    ],
    "abstract": "Extracting relevant information from a stream of high-dimensional\nobservations is a central challenge for deep reinforcement learning agents.\nActor-critic algorithms add further complexity to this challenge, as it is\noften unclear whether the same information will be relevant to both the actor\nand the critic. To this end, we here explore the principles that underlie\neffective representations for the actor and for the critic in on-policy\nalgorithms. We focus our study on understanding whether the actor and critic\nwill benefit from separate, rather than shared, representations. Our primary\nfinding is that when separated, the representations for the actor and critic\nsystematically specialise in extracting different types of information from the\nenvironment -- the actor's representation tends to focus on action-relevant\ninformation, while the critic's representation specialises in encoding value\nand dynamics information. We conduct a rigourous empirical study to understand\nhow different representation learning approaches affect the actor and critic's\nspecialisations and their downstream performance, in terms of sample efficiency\nand generation capabilities. Finally, we discover that a separated critic plays\nan important role in exploration and data collection during training. Our code,\ntrained models and data are accessible at\nhttps://github.com/francelico/deac-rep.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at ICLR 2025. 10 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.06343v1",
    "published_date": "2025-03-08 21:29:20 UTC",
    "updated_date": "2025-03-08 21:29:20 UTC"
  },
  {
    "arxiv_id": "2503.07663v1",
    "title": "Merge then Realign: Simple and Effective Modality-Incremental Continual Learning for Multimodal LLMs",
    "authors": [
      "Dingkun Zhang",
      "Shuhan Qi",
      "Xinyu Xiao",
      "Kehai Chen",
      "Xuan Wang"
    ],
    "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have enhanced\ntheir versatility as they integrate a growing number of modalities. Considering\nthe heavy cost of training MLLMs, it is necessary to reuse the existing ones\nand further extend them to more modalities through Modality-incremental\nContinual Learning (MCL). However, this often comes with a performance\ndegradation in the previously learned modalities. In this work, we revisit the\nMCL and investigate a more severe issue it faces in contrast to traditional\ncontinual learning, that its degradation comes not only from catastrophic\nforgetting but also from the misalignment between the modality-agnostic and\nmodality-specific components. To address this problem, we propose an elegantly\nsimple MCL paradigm called \"MErge then ReAlign\" (MERA). Our method avoids\nintroducing heavy training overhead or modifying the model architecture, hence\nis easy to deploy and highly reusable in the MLLM community. Extensive\nexperiments demonstrate that, despite the simplicity of MERA, it shows\nimpressive performance, holding up to a 99.84% Backward Relative Gain when\nextending to four modalities, achieving a nearly lossless MCL performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.07663v1",
    "published_date": "2025-03-08 20:29:40 UTC",
    "updated_date": "2025-03-08 20:29:40 UTC"
  },
  {
    "arxiv_id": "2503.06330v1",
    "title": "States of LLM-generated Texts and Phase Transitions between them",
    "authors": [
      "Nikolay Mikhaylovskiy"
    ],
    "abstract": "It is known for some time that autocorrelations of words in human-written\ntexts decay according to a power law. Recent works have also shown that the\nautocorrelations decay in texts generated by LLMs is qualitatively different\nfrom the literary texts. Solid state physics tie the autocorrelations decay\nlaws to the states of matter. In this work, we empirically demonstrate that,\ndepending on the temperature parameter, LLMs can generate text that can be\nclassified as solid, critical state or gas.",
    "categories": [
      "cs.CL",
      "cond-mat.stat-mech",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Published as a conference paper at MathAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.06330v1",
    "published_date": "2025-03-08 20:06:50 UTC",
    "updated_date": "2025-03-08 20:06:50 UTC"
  },
  {
    "arxiv_id": "2503.06323v1",
    "title": "Higher-Order Belief in Incomplete Information MAIDs",
    "authors": [
      "Jack Foxabbott",
      "Rohan Subramani",
      "Francis Rhys Ward"
    ],
    "abstract": "Multi-agent influence diagrams (MAIDs) are probabilistic graphical models\nwhich represent strategic interactions between agents. MAIDs are equivalent to\nextensive form games (EFGs) but have a more compact and informative structure.\nHowever, MAIDs cannot, in general, represent settings of incomplete information\n-- wherein agents have different beliefs about the game being played, and\ndifferent beliefs about each-other's beliefs. In this paper, we introduce\nincomplete information MAIDs (II-MAIDs). We define both infinite and\nfinite-depth II-MAIDs and prove an equivalence relation to EFGs with incomplete\ninformation and no common prior over types. We prove that II-MAIDs inherit\nclassical equilibria concepts via this equivalence, but note that these\nsolution concepts are often unrealistic in the setting with no common prior\nbecause they violate common knowledge of rationality. We define a more\nrealistic solution concept based on recursive best-response. Throughout, we\ndescribe an example with a hypothetical AI agent undergoing evaluation to\nillustrate the applicability of II-MAIDs.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06323v1",
    "published_date": "2025-03-08 19:35:55 UTC",
    "updated_date": "2025-03-08 19:35:55 UTC"
  },
  {
    "arxiv_id": "2503.06313v1",
    "title": "Advancing Autonomous Vehicle Intelligence: Deep Learning and Multimodal LLM for Traffic Sign Recognition and Robust Lane Detection",
    "authors": [
      "Chandan Kumar Sah",
      "Ankit Kumar Shaw",
      "Xiaoli Lian",
      "Arsalan Shahid Baig",
      "Tuopu Wen",
      "Kun Jiang",
      "Mengmeng Yang",
      "Diange Yang"
    ],
    "abstract": "Autonomous vehicles (AVs) require reliable traffic sign recognition and\nrobust lane detection capabilities to ensure safe navigation in complex and\ndynamic environments. This paper introduces an integrated approach combining\nadvanced deep learning techniques and Multimodal Large Language Models (MLLMs)\nfor comprehensive road perception. For traffic sign recognition, we\nsystematically evaluate ResNet-50, YOLOv8, and RT-DETR, achieving\nstate-of-the-art performance of 99.8% with ResNet-50, 98.0% accuracy with\nYOLOv8, and achieved 96.6% accuracy in RT-DETR despite its higher computational\ncomplexity. For lane detection, we propose a CNN-based segmentation method\nenhanced by polynomial curve fitting, which delivers high accuracy under\nfavorable conditions. Furthermore, we introduce a lightweight, Multimodal,\nLLM-based framework that directly undergoes instruction tuning using small yet\ndiverse datasets, eliminating the need for initial pretraining. This framework\neffectively handles various lane types, complex intersections, and merging\nzones, significantly enhancing lane detection reliability by reasoning under\nadverse conditions. Despite constraints in available training resources, our\nmultimodal approach demonstrates advanced reasoning capabilities, achieving a\nFrame Overall Accuracy (FRM) of 53.87%, a Question Overall Accuracy (QNS) of\n82.83%, lane detection accuracies of 99.6% in clear conditions and 93.0% at\nnight, and robust performance in reasoning about lane invisibility due to rain\n(88.4%) or road degradation (95.6%). The proposed comprehensive framework\nmarkedly enhances AV perception reliability, thus contributing significantly to\nsafer autonomous driving across diverse and challenging road scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.06313v1",
    "published_date": "2025-03-08 19:12:36 UTC",
    "updated_date": "2025-03-08 19:12:36 UTC"
  },
  {
    "arxiv_id": "2503.06302v1",
    "title": "Synergizing AI and Digital Twins for Next-Generation Network Optimization, Forecasting, and Security",
    "authors": [
      "Zifan Zhang",
      "Minghong Fang",
      "Dianwei Chen",
      "Xianfeng Yang",
      "Yuchen Liu"
    ],
    "abstract": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "Accepted by IEEE Wireless Communications",
    "pdf_url": "http://arxiv.org/pdf/2503.06302v1",
    "published_date": "2025-03-08 18:30:54 UTC",
    "updated_date": "2025-03-08 18:30:54 UTC"
  },
  {
    "arxiv_id": "2503.09620v2",
    "title": "Exploiting Edited Large Language Models as General Scientific Optimizers",
    "authors": [
      "Qitan Lv",
      "Tianyu Liu",
      "Hong Wang"
    ],
    "abstract": "Large language models (LLMs) have been widely adopted in mathematical\noptimization in scientific scenarios for their extensive knowledge and advanced\nreasoning capabilities. Existing methods mainly focus on utilizing LLMs to\nsolve optimization problems in a prompt-based manner, which takes observational\nfeedback as additional textual descriptions. However, due to LLM's \\textbf{high\nsensitivity to the prompts} and \\textbf{tendency to get lost in lengthy\nprompts}, these methods struggle to effectively utilize the {observational}\nfeedback from each optimization step, which severely hinders the applications\nfor real-world scenarios. To address these challenges, we propose a\nconceptually simple and general {bi-level} optimization method, namely\n\\textbf{G}eneral \\textbf{S}cientific \\textbf{O}ptimizers (GSO). Specifically,\nGSO first utilizes inner-level simulators as experimental platforms to evaluate\nthe current solution and provide observational feedback. Then, LLMs serve as\nknowledgeable and versatile scientists, generating new solutions by refining\npotential errors from the feedback as the outer-level optimization. Finally,\nsimulations together with the expert knowledge in LLMs are jointly updated with\nbi-level interactions via model editing. Extensive experiments show that GSO\nconsistently outperforms existing state-of-the-art methods using \\textit{six}\ndifferent LLM backbones on \\textit{seven} different tasks, demonstrating the\neffectiveness and a wide range of applications.",
    "categories": [
      "math.OC",
      "cs.AI"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.09620v2",
    "published_date": "2025-03-08 18:01:11 UTC",
    "updated_date": "2025-03-17 05:40:49 UTC"
  },
  {
    "arxiv_id": "2503.06288v1",
    "title": "Single Domain Generalization with Adversarial Memory",
    "authors": [
      "Hao Yan",
      "Marzi Heidari",
      "Yuhong Guo"
    ],
    "abstract": "Domain Generalization (DG) aims to train models that can generalize to unseen\ntesting domains by leveraging data from multiple training domains. However,\ntraditional DG methods rely on the availability of multiple diverse training\ndomains, limiting their applicability in data-constrained scenarios. Single\nDomain Generalization (SDG) addresses the more realistic and challenging\nsetting by restricting the training data to a single domain distribution. The\nmain challenges in SDG stem from the limited diversity of training data and the\ninaccessibility of unseen testing data distributions. To tackle these\nchallenges, we propose a single domain generalization method that leverages an\nadversarial memory bank to augment training features. Our memory-based feature\naugmentation network maps both training and testing features into an invariant\nsubspace spanned by diverse memory features, implicitly aligning the training\nand testing domains in the projected space. To maintain a diverse and\nrepresentative feature memory bank, we introduce an adversarial feature\ngeneration method that creates features extending beyond the training domain\ndistribution. Experimental results demonstrate that our approach achieves\nstate-of-the-art performance on standard single domain generalization\nbenchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06288v1",
    "published_date": "2025-03-08 17:27:42 UTC",
    "updated_date": "2025-03-08 17:27:42 UTC"
  },
  {
    "arxiv_id": "2503.06287v1",
    "title": "Your Large Vision-Language Model Only Needs A Few Attention Heads For Visual Grounding",
    "authors": [
      "Seil Kang",
      "Jinyeong Kim",
      "Junhyeok Kim",
      "Seong Jae Hwang"
    ],
    "abstract": "Visual grounding seeks to localize the image region corresponding to a\nfree-form text description. Recently, the strong multimodal capabilities of\nLarge Vision-Language Models (LVLMs) have driven substantial improvements in\nvisual grounding, though they inevitably require fine-tuning and additional\nmodel components to explicitly generate bounding boxes or segmentation masks.\nHowever, we discover that a few attention heads in frozen LVLMs demonstrate\nstrong visual grounding capabilities. We refer to these heads, which\nconsistently capture object locations related to text semantics, as\nlocalization heads. Using localization heads, we introduce a straightforward\nand effective training-free visual grounding framework that utilizes\ntext-to-image attention maps from localization heads to identify the target\nobjects. Surprisingly, only three out of thousands of attention heads are\nsufficient to achieve competitive localization performance compared to existing\nLVLM-based visual grounding methods that require fine-tuning. Our findings\nsuggest that LVLMs can innately ground objects based on a deep comprehension of\nthe text-image relationship, as they implicitly focus on relevant image regions\nto generate informative text outputs. All the source codes will be made\navailable to the public.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06287v1",
    "published_date": "2025-03-08 17:24:42 UTC",
    "updated_date": "2025-03-08 17:24:42 UTC"
  },
  {
    "arxiv_id": "2503.06278v1",
    "title": "Applied Machine Learning Methods with Long-Short Term Memory Based Recurrent Neural Networks for Multivariate Temperature Prediction",
    "authors": [
      "Bojan Lukić"
    ],
    "abstract": "This paper gives an overview on how to develop a dense and deep neural\nnetwork for making a time series prediction. First, the history and\ncornerstones in Artificial Intelligence and Machine Learning will be presented.\nAfter a short introduction to the theory of Artificial Intelligence and Machine\nLearning, the paper will go deeper into the techniques for conducting a time\nseries prediction with different models of neural networks. For this project,\nPython's development environment Jupyter, extended with the TensorFlow package\nand deep-learning application Keras is used. The system setup and project\nframework are explained in more detail before discussing the time series\nprediction. The main part shows an applied example of time series prediction\nwith weather data. For this work, a deep recurrent neural network with Long\nShort-Term Memory cells is used to conduct the time series prediction. The\nresults and evaluation of the work show that a weather prediction with deep\nneural networks can be successful for a short time period. However, there are\nsome drawbacks and limitations with time series prediction, which will be\ndiscussed towards the end of the paper.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 16 figures, private research",
    "pdf_url": "http://arxiv.org/pdf/2503.06278v1",
    "published_date": "2025-03-08 16:52:27 UTC",
    "updated_date": "2025-03-08 16:52:27 UTC"
  },
  {
    "arxiv_id": "2503.06269v1",
    "title": "Using Mechanistic Interpretability to Craft Adversarial Attacks against Large Language Models",
    "authors": [
      "Thomas Winninger",
      "Boussad Addad",
      "Katarzyna Kapusta"
    ],
    "abstract": "Traditional white-box methods for creating adversarial perturbations against\nLLMs typically rely only on gradient computation from the targeted model,\nignoring the internal mechanisms responsible for attack success or failure.\nConversely, interpretability studies that analyze these internal mechanisms\nlack practical applications beyond runtime interventions. We bridge this gap by\nintroducing a novel white-box approach that leverages mechanistic\ninterpretability techniques to craft practical adversarial inputs.\nSpecifically, we first identify acceptance subspaces - sets of feature vectors\nthat do not trigger the model's refusal mechanisms - then use gradient-based\noptimization to reroute embeddings from refusal subspaces to acceptance\nsubspaces, effectively achieving jailbreaks. This targeted approach\nsignificantly reduces computation cost, achieving attack success rates of\n80-95\\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5\nwithin minutes or even seconds, compared to existing techniques that often fail\nor require hours of computation. We believe this approach opens a new direction\nfor both attack research and defense development. Furthermore, it showcases a\npractical application of mechanistic interpretability where other methods are\nless efficient, which highlights its utility. The code and generated datasets\nare available at https://github.com/Sckathach/subspace-rerouting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06269v1",
    "published_date": "2025-03-08 16:29:45 UTC",
    "updated_date": "2025-03-08 16:29:45 UTC"
  },
  {
    "arxiv_id": "2503.06263v1",
    "title": "Critical Foreign Policy Decisions (CFPD)-Benchmark: Measuring Diplomatic Preferences in Large Language Models",
    "authors": [
      "Benjamin Jensen",
      "Ian Reynolds",
      "Yasir Atalan",
      "Michael Garcia",
      "Austin Woo",
      "Anthony Chen",
      "Trevor Howarth"
    ],
    "abstract": "As national security institutions increasingly integrate Artificial\nIntelligence (AI) into decision-making and content generation processes,\nunderstanding the inherent biases of large language models (LLMs) is crucial.\nThis study presents a novel benchmark designed to evaluate the biases and\npreferences of seven prominent foundation models-Llama 3.1 8B Instruct, Llama\n3.1 70B Instruct, GPT-4o, Gemini 1.5 Pro-002, Mixtral 8x22B, Claude 3.5 Sonnet,\nand Qwen2 72B-in the context of international relations (IR). We designed a\nbias discovery study around core topics in IR using 400-expert crafted\nscenarios to analyze results from our selected models. These scenarios focused\non four topical domains including: military escalation, military and\nhumanitarian intervention, cooperative behavior in the international system,\nand alliance dynamics. Our analysis reveals noteworthy variation among model\nrecommendations based on scenarios designed for the four tested domains.\nParticularly, Qwen2 72B, Gemini 1.5 Pro-002 and Llama 3.1 8B Instruct models\noffered significantly more escalatory recommendations than Claude 3.5 Sonnet\nand GPT-4o models. All models exhibit some degree of country-specific biases,\noften recommending less escalatory and interventionist actions for China and\nRussia compared to the United States and the United Kingdom. These findings\nhighlight the necessity for controlled deployment of LLMs in high-stakes\nenvironments, emphasizing the need for domain-specific evaluations and model\nfine-tuning to align with institutional objectives.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "I.2.7"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06263v1",
    "published_date": "2025-03-08 16:19:13 UTC",
    "updated_date": "2025-03-08 16:19:13 UTC"
  },
  {
    "arxiv_id": "2503.06260v1",
    "title": "From Captions to Rewards (CAREVL): Leveraging Large Language Model Experts for Enhanced Reward Modeling in Large Vision-Language Models",
    "authors": [
      "Muzhi Dai",
      "Jiashuo Sun",
      "Zhiyuan Zhao",
      "Shixuan Liu",
      "Rui Li",
      "Junyu Gao",
      "Xuelong Li"
    ],
    "abstract": "Aligning large vision-language models (LVLMs) with human preferences is\nchallenging due to the scarcity of fine-grained, high-quality, and multimodal\npreference data without human annotations. Existing methods relying on direct\ndistillation often struggle with low-confidence data, leading to suboptimal\nperformance. To address this, we propose CAREVL, a novel method for preference\nreward modeling by reliably using both high- and low-confidence data. First, a\ncluster of auxiliary expert models (textual reward models) innovatively\nleverages image captions as weak supervision signals to filter high-confidence\ndata. The high-confidence data are then used to fine-tune the LVLM. Second,\nlow-confidence data are used to generate diverse preference samples using the\nfine-tuned LVLM. These samples are then scored and selected to construct\nreliable chosen-rejected pairs for further training. CAREVL achieves\nperformance improvements over traditional distillation-based methods on\nVL-RewardBench and MLLM-as-a-Judge benchmark, demonstrating its effectiveness.\nThe code will be released soon.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06260v1",
    "published_date": "2025-03-08 16:13:18 UTC",
    "updated_date": "2025-03-08 16:13:18 UTC"
  },
  {
    "arxiv_id": "2503.06252v1",
    "title": "Can Atomic Step Decomposition Enhance the Self-structured Reasoning of Multimodal Large Models?",
    "authors": [
      "Kun Xiang",
      "Zhili Liu",
      "Zihao Jiang",
      "Yunshuang Nie",
      "Kaixin Cai",
      "Yiyang Yin",
      "Runhui Huang",
      "Haoxiang Fan",
      "Hanhui Li",
      "Weiran Huang",
      "Yihan Zeng",
      "Yu-Jie Yuan",
      "Jianhua Han",
      "Lanqing Hong",
      "Hang Xu",
      "Xiaodan Liang"
    ],
    "abstract": "In this paper, we address the challenging task of multimodal mathematical\nreasoning by incorporating the ability of \"slow thinking\" into multimodal large\nlanguage models (MLLMs). Our core idea is that different levels of reasoning\nabilities can be combined dynamically to tackle questions with different\ncomplexity. To this end, we propose a paradigm of Self-structured Chain of\nThought (SCoT), which is composed of minimal semantic atomic steps. Different\nfrom existing methods that rely on structured templates or free-form paradigms,\nour method can not only generate cognitive CoT structures for various complex\ntasks but also mitigates the phenomenon of overthinking. To introduce\nstructured reasoning capabilities into visual understanding models, we further\ndesign a novel AtomThink framework with four key modules, including (i) a data\nengine to generate high-quality multimodal reasoning paths; (ii) a supervised\nfine-tuning process with serialized inference data; (iii) a policy-guided\nmulti-turn inference method; and (iv) an atomic capability metric to evaluate\nthe single step utilization rate. We conduct extensive experiments to show that\nthe proposed AtomThink significantly improves the performance of baseline\nMLLMs, achieving more than 10\\% average accuracy gains on MathVista and\nMathVerse. Compared to state-of-the-art structured CoT approaches, our method\nnot only achieves higher accuracy but also improves data utilization by 5 times\nand boosts inference efficiency by 85.3\\%. Our code is now public available in\nhttps://github.com/Quinn777/AtomThink.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06252v1",
    "published_date": "2025-03-08 15:23:47 UTC",
    "updated_date": "2025-03-08 15:23:47 UTC"
  },
  {
    "arxiv_id": "2503.06247v1",
    "title": "Infant Cry Detection Using Causal Temporal Representation",
    "authors": [
      "Minghao Fu",
      "Danning Li",
      "Aryan Gadhiya",
      "Benjamin Lambright",
      "Mohamed Alowais",
      "Mohab Bahnassy",
      "Saad El Dine Elletter",
      "Hawau Olamide Toyin",
      "Haiyan Jiang",
      "Kun Zhang",
      "Hanan Aldarmaki"
    ],
    "abstract": "This paper addresses a major challenge in acoustic event detection, in\nparticular infant cry detection in the presence of other sounds and background\nnoises: the lack of precise annotated data. We present two contributions for\nsupervised and unsupervised infant cry detection. The first is an annotated\ndataset for cry segmentation, which enables supervised models to achieve\nstate-of-the-art performance. Additionally, we propose a novel unsupervised\nmethod, Causal Representation Spare Transition Clustering (CRSTC), based on\ncausal temporal representation, which helps address the issue of data scarcity\nmore generally. By integrating the detected cry segments, we significantly\nimprove the performance of downstream infant cry classification, highlighting\nthe potential of this approach for infant care applications.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.06247v1",
    "published_date": "2025-03-08 15:15:23 UTC",
    "updated_date": "2025-03-08 15:15:23 UTC"
  },
  {
    "arxiv_id": "2503.06242v1",
    "title": "LapSum -- One Method to Differentiate Them All: Ranking, Sorting and Top-k Selection",
    "authors": [
      "Łukasz Struski",
      "Michał B. Bednarczyk",
      "Igor T. Podolak",
      "Jacek Tabor"
    ],
    "abstract": "We present a novel technique for constructing differentiable order-type\noperations, including soft ranking, soft top-k selection, and soft\npermutations. Our approach leverages an efficient closed-form formula for the\ninverse of the function LapSum, defined as the sum of Laplace distributions.\nThis formulation ensures low computational and memory complexity in selecting\nthe highest activations, enabling losses and gradients to be computed in\n$O(n\\log{}n)$ time. Through extensive experiments, we demonstrate that our\nmethod outperforms state-of-the-art techniques for high-dimensional vectors and\nlarge $k$ values. Furthermore, we provide efficient implementations for both\nCPU and CUDA environments, underscoring the practicality and scalability of our\nmethod for large-scale ranking and differentiable ordering problems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06242v1",
    "published_date": "2025-03-08 14:53:36 UTC",
    "updated_date": "2025-03-08 14:53:36 UTC"
  },
  {
    "arxiv_id": "2503.06238v1",
    "title": "Image is All You Need: Towards Efficient and Effective Large Language Model-Based Recommender Systems",
    "authors": [
      "Kibum Kim",
      "Sein Kim",
      "Hongseok Kang",
      "Jiwan Kim",
      "Heewoong Noh",
      "Yeonjun In",
      "Kanghoon Yoon",
      "Jinoh Oh",
      "Chanyoung Park"
    ],
    "abstract": "Large Language Models (LLMs) have recently emerged as a powerful backbone for\nrecommender systems. Existing LLM-based recommender systems take two different\napproaches for representing items in natural language, i.e., Attribute-based\nRepresentation and Description-based Representation. In this work, we aim to\naddress the trade-off between efficiency and effectiveness that these two\napproaches encounter, when representing items consumed by users. Based on our\ninteresting observation that there is a significant information overlap between\nimages and descriptions associated with items, we propose a novel method, Image\nis all you need for LLM-based Recommender system (I-LLMRec). Our main idea is\nto leverage images as an alternative to lengthy textual descriptions for\nrepresenting items, aiming at reducing token usage while preserving the rich\nsemantic information of item descriptions. Through extensive experiments, we\ndemonstrate that I-LLMRec outperforms existing methods in both efficiency and\neffectiveness by leveraging images. Moreover, a further appeal of I-LLMRec is\nits ability to reduce sensitivity to noise in descriptions, leading to more\nrobust recommendations.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06238v1",
    "published_date": "2025-03-08 14:51:16 UTC",
    "updated_date": "2025-03-08 14:51:16 UTC"
  },
  {
    "arxiv_id": "2503.06229v1",
    "title": "A Frank System for Co-Evolutionary Hybrid Decision-Making",
    "authors": [
      "Federico Mazzoni",
      "Riccardo Guidotti",
      "Alessio Malizia"
    ],
    "abstract": "We introduce Frank, a human-in-the-loop system for co-evolutionary hybrid\ndecision-making aiding the user to label records from an un-labeled dataset.\nFrank employs incremental learning to ``evolve'' in parallel with the user's\ndecisions, by training an interpretable machine learning model on the records\nlabeled by the user. Furthermore, Frank advances state-of-the-art approaches by\noffering inconsistency controls, explanations, fairness checks, and bad-faith\nsafeguards simultaneously. We evaluate our proposal by simulating the users'\nbehavior with various levels of expertise and reliance on Frank's suggestions.\nThe experiments show that Frank's intervention leads to improvements in the\naccuracy and the fairness of the decisions.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.06229v1",
    "published_date": "2025-03-08 14:06:16 UTC",
    "updated_date": "2025-03-08 14:06:16 UTC"
  },
  {
    "arxiv_id": "2503.06226v2",
    "title": "Optimal Output Feedback Learning Control for Discrete-Time Linear Quadratic Regulation",
    "authors": [
      "Kedi Xie",
      "Martin Guay",
      "Shimin Wang",
      "Fang Deng",
      "Maobin Lu"
    ],
    "abstract": "This paper studies the linear quadratic regulation (LQR) problem of unknown\ndiscrete-time systems via dynamic output feedback learning control. In contrast\nto the state feedback, the optimality of the dynamic output feedback control\nfor solving the LQR problem requires an implicit condition on the convergence\nof the state observer. Moreover, due to unknown system matrices and the\nexistence of observer error, it is difficult to analyze the convergence and\nstability of most existing output feedback learning-based control methods. To\ntackle these issues, we propose a generalized dynamic output feedback learning\ncontrol approach with guaranteed convergence, stability, and optimality\nperformance for solving the LQR problem of unknown discrete-time linear\nsystems. In particular, a dynamic output feedback controller is designed to be\nequivalent to a state feedback controller. This equivalence relationship is an\ninherent property without requiring convergence of the estimated state by the\nstate observer, which plays a key role in establishing the off-policy learning\ncontrol approaches. By value iteration and policy iteration schemes, the\nadaptive dynamic programming based learning control approaches are developed to\nestimate the optimal feedback control gain. In addition, a model-free stability\ncriterion is provided by finding a nonsingular parameterization matrix, which\ncontributes to establishing a switched iteration scheme. Furthermore, the\nconvergence, stability, and optimality analyses of the proposed output feedback\nlearning control approaches are given. Finally, the theoretical results are\nvalidated by two numerical examples.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.MA",
      "cs.SY",
      "math.OC"
    ],
    "primary_category": "eess.SY",
    "comment": "16 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.06226v2",
    "published_date": "2025-03-08 14:02:16 UTC",
    "updated_date": "2025-03-11 18:32:36 UTC"
  },
  {
    "arxiv_id": "2503.06212v1",
    "title": "GraphGen+: Advancing Distributed Subgraph Generation and Graph Learning On Industrial Graphs",
    "authors": [
      "Yue Jin",
      "Yongchao Liu",
      "Chuntao Hong"
    ],
    "abstract": "Graph-based computations are crucial in a wide range of applications, where\ngraphs can scale to trillions of edges. To enable efficient training on such\nlarge graphs, mini-batch subgraph sampling is commonly used, which allows\ntraining without loading the entire graph into memory. However, existing\nsolutions face significant trade-offs: online subgraph generation, as seen in\nframeworks like DGL and PyG, is limited to a single machine, resulting in\nsevere performance bottlenecks, while offline precomputed subgraphs, as in\nGraphGen, improve sampling efficiency but introduce large storage overhead and\nhigh I/O costs during training. To address these challenges, we propose\n\\textbf{GraphGen+}, an integrated framework that synchronizes distributed\nsubgraph generation with in-memory graph learning, eliminating the need for\nexternal storage while significantly improving efficiency. GraphGen+ achieves a\n\\textbf{27$\\times$} speedup in subgraph generation compared to conventional\nSQL-like methods and a \\textbf{1.3$\\times$} speedup over GraphGen, supporting\ntraining on 1 million nodes per iteration and removing the overhead associated\nwith precomputed subgraphs, making it a scalable and practical solution for\nindustry-scale graph learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted By EuroSys 2025 (poster)",
    "pdf_url": "http://arxiv.org/pdf/2503.06212v1",
    "published_date": "2025-03-08 13:29:42 UTC",
    "updated_date": "2025-03-08 13:29:42 UTC"
  },
  {
    "arxiv_id": "2503.06211v1",
    "title": "Text-Speech Language Models with Improved Cross-Modal Transfer by Aligning Abstraction Levels",
    "authors": [
      "Santiago Cuervo",
      "Adel Moumen",
      "Yanis Labrak",
      "Sameer Khurana",
      "Antoine Laurent",
      "Mickael Rouvier",
      "Ricard Marxer"
    ],
    "abstract": "Text-Speech Language Models (TSLMs) -- language models trained to jointly\nprocess and generate text and speech -- aim to enable cross-modal knowledge\ntransfer to overcome the scaling limitations of unimodal speech LMs. The\npredominant approach to TSLM training expands the vocabulary of a pre-trained\ntext LM by appending new embeddings and linear projections for speech, followed\nby fine-tuning on speech data. We hypothesize that this method limits\ncross-modal transfer by neglecting feature compositionality, preventing\ntext-learned functions from being fully leveraged at appropriate abstraction\nlevels. To address this, we propose augmenting vocabulary expansion with\nmodules that better align abstraction levels across layers. Our models,\n\\textsc{SmolTolk}, rival or surpass state-of-the-art TSLMs trained with orders\nof magnitude more compute. Representation analyses and improved multimodal\nperformance suggest our method enhances cross-modal transfer.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06211v1",
    "published_date": "2025-03-08 13:28:50 UTC",
    "updated_date": "2025-03-08 13:28:50 UTC"
  },
  {
    "arxiv_id": "2503.10660v2",
    "title": "Text-to-3D Generation using Jensen-Shannon Score Distillation",
    "authors": [
      "Khoi Do",
      "Binh-Son Hua"
    ],
    "abstract": "Score distillation sampling is an effective technique to generate 3D models\nfrom text prompts, utilizing pre-trained large-scale text-to-image diffusion\nmodels as guidance. However, the produced 3D assets tend to be over-saturating,\nover-smoothing, with limited diversity. These issues are results from a reverse\nKullback-Leibler (KL) divergence objective, which makes the optimization\nunstable and results in mode-seeking behavior. In this paper, we derive a\nbounded score distillation objective based on Jensen-Shannon divergence (JSD),\nwhich stabilizes the optimization process and produces high-quality 3D\ngeneration. JSD can match well generated and target distribution, therefore\nmitigating mode seeking. We provide a practical implementation of JSD by\nutilizing the theory of generative adversarial networks to define an\napproximate objective function for the generator, assuming the discriminator is\nwell trained. By assuming the discriminator following a log-odds classifier, we\npropose a minority sampling algorithm to estimate the gradients of our proposed\nobjective, providing a practical implementation for JSD. We conduct both\ntheoretical and empirical studies to validate our method. Experimental results\non T3Bench demonstrate that our method can produce high-quality and diversified\n3D assets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.10660v2",
    "published_date": "2025-03-08 13:27:18 UTC",
    "updated_date": "2025-03-18 17:15:23 UTC"
  },
  {
    "arxiv_id": "2503.06208v1",
    "title": "Distributed Graph Neural Network Inference With Just-In-Time Compilation For Industry-Scale Graphs",
    "authors": [
      "Xiabao Wu",
      "Yongchao Liu",
      "Wei Qin",
      "Chuntao Hong"
    ],
    "abstract": "Graph neural networks (GNNs) have delivered remarkable results in various\nfields. However, the rapid increase in the scale of graph data has introduced\nsignificant performance bottlenecks for GNN inference. Both computational\ncomplexity and memory usage have risen dramatically, with memory becoming a\ncritical limitation. Although graph sampling-based subgraph learning methods\ncan help mitigate computational and memory demands, they come with drawbacks\nsuch as information loss and high redundant computation among subgraphs. This\npaper introduces an innovative processing paradgim for distributed graph\nlearning that abstracts GNNs with a new set of programming interfaces and\nleverages Just-In-Time (JIT) compilation technology to its full potential. This\nparadigm enables GNNs to highly exploit the computational resources of\ndistributed clusters by eliminating the drawbacks of subgraph learning methods,\nleading to a more efficient inference process. Our experimental results\ndemonstrate that on industry-scale graphs of up to \\textbf{500 million nodes\nand 22.4 billion edges}, our method can produce a performance boost of up to\n\\textbf{27.4 times}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by EuroSys 2025 (poster)",
    "pdf_url": "http://arxiv.org/pdf/2503.06208v1",
    "published_date": "2025-03-08 13:26:59 UTC",
    "updated_date": "2025-03-08 13:26:59 UTC"
  },
  {
    "arxiv_id": "2503.06204v1",
    "title": "CUPCase: Clinically Uncommon Patient Cases and Diagnoses Dataset",
    "authors": [
      "Oriel Perets",
      "Ofir Ben Shoham",
      "Nir Grinberg",
      "Nadav Rappoport"
    ],
    "abstract": "Medical benchmark datasets significantly contribute to developing Large\nLanguage Models (LLMs) for medical knowledge extraction, diagnosis,\nsummarization, and other uses. Yet, current benchmarks are mainly derived from\nexam questions given to medical students or cases described in the medical\nliterature, lacking the complexity of real-world patient cases that deviate\nfrom classic textbook abstractions. These include rare diseases, uncommon\npresentations of common diseases, and unexpected treatment responses. Here, we\nconstruct Clinically Uncommon Patient Cases and Diagnosis Dataset (CUPCase)\nbased on 3,562 real-world case reports from BMC, including diagnoses in\nopen-ended textual format and as multiple-choice options with distractors.\nUsing this dataset, we evaluate the ability of state-of-the-art LLMs, including\nboth general-purpose and Clinical LLMs, to identify and correctly diagnose a\npatient case, and test models' performance when only partial information about\ncases is available. Our findings show that general-purpose GPT-4o attains the\nbest performance in both the multiple-choice task (average accuracy of 87.9%)\nand the open-ended task (BERTScore F1 of 0.764), outperforming several LLMs\nwith a focus on the medical domain such as Meditron-70B and MedLM-Large.\nMoreover, GPT-4o was able to maintain 87% and 88% of its performance with only\nthe first 20% of tokens of the case presentation in multiple-choice and free\ntext, respectively, highlighting the potential of LLMs to aid in early\ndiagnosis in real-world cases. CUPCase expands our ability to evaluate LLMs for\nclinical decision support in an open and reproducible manner.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.06204v1",
    "published_date": "2025-03-08 13:21:44 UTC",
    "updated_date": "2025-03-08 13:21:44 UTC"
  },
  {
    "arxiv_id": "2503.06202v1",
    "title": "Breaking Free from MMI: A New Frontier in Rationalization by Probing Input Utilization",
    "authors": [
      "Wei Liu",
      "Zhiying Deng",
      "Zhongyu Niu",
      "Jun Wang",
      "Haozhao Wang",
      "Zhigang Zeng",
      "Ruixuan Li"
    ],
    "abstract": "Extracting a small subset of crucial rationales from the full input is a key\nproblem in explainability research. The most widely used fundamental criterion\nfor rationale extraction is the maximum mutual information (MMI) criterion. In\nthis paper, we first demonstrate that MMI suffers from diminishing marginal\nreturns. Once part of the rationale has been identified, finding the remaining\nportions contributes only marginally to increasing the mutual information,\nmaking it difficult to use MMI to locate the rest. In contrast to MMI that aims\nto reproduce the prediction, we seek to identify the parts of the input that\nthe network can actually utilize.\n  This is achieved by comparing how different rationale candidates match the\ncapability space of the weight matrix. The weight matrix of a neural network is\ntypically low-rank, meaning that the linear combinations of its column vectors\ncan only cover part of the directions in a high-dimensional space\n(high-dimension: the dimensions of an input vector). If an input is fully\nutilized by the network, {it generally matches these directions (e.g., a\nportion of a hypersphere), resulting in a representation with a high norm.\nConversely, if an input primarily falls outside (orthogonal to) these\ndirections}, its representation norm will approach zero, behaving like noise\nthat the network cannot effectively utilize. Building on this, we propose using\nthe norms of rationale candidates as an alternative objective to MMI. Through\nexperiments on four text classification datasets and one graph classification\ndataset using three network architectures (GRUs, BERT, and GCN), we show that\nour method outperforms MMI and its improved variants in identifying better\nrationales. We also compare our method with a representative LLM\n(llama-3.1-8b-instruct) and find that our simple method gets comparable results\nto it and can sometimes even outperform it.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06202v1",
    "published_date": "2025-03-08 13:08:46 UTC",
    "updated_date": "2025-03-08 13:08:46 UTC"
  },
  {
    "arxiv_id": "2503.06201v1",
    "title": "Explainable Synthetic Image Detection through Diffusion Timestep Ensembling",
    "authors": [
      "Yixin Wu",
      "Feiran Zhang",
      "Tianyuan Shi",
      "Ruicheng Yin",
      "Zhenghua Wang",
      "Zhenliang Gan",
      "Xiaohua Wang",
      "Changze Lv",
      "Xiaoqing Zheng",
      "Xuanjing Huang"
    ],
    "abstract": "Recent advances in diffusion models have enabled the creation of deceptively\nreal images, posing significant security risks when misused. In this study, we\nreveal that natural and synthetic images exhibit distinct differences in the\nhigh-frequency domains of their Fourier power spectra after undergoing\niterative noise perturbations through an inverse multi-step denoising process,\nsuggesting that such noise can provide additional discriminative information\nfor identifying synthetic images. Based on this observation, we propose a novel\ndetection method that amplifies these differences by progressively adding noise\nto the original images across multiple timesteps, and train an ensemble of\nclassifiers on these noised images. To enhance human comprehension, we\nintroduce an explanation generation and refinement module to identify flaws\nlocated in AI-generated images. Additionally, we construct two new datasets,\nGenHard and GenExplain, derived from the GenImage benchmark, providing\ndetection samples of greater difficulty and high-quality rationales for fake\nimages. Extensive experiments show that our method achieves state-of-the-art\nperformance with 98.91% and 95.89% detection accuracy on regular and harder\nsamples, increasing a minimal of 2.51% and 3.46% compared to baselines.\nFurthermore, our method also generalizes effectively to images generated by\nother diffusion models. Our code and datasets will be made publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.06201v1",
    "published_date": "2025-03-08 13:04:20 UTC",
    "updated_date": "2025-03-08 13:04:20 UTC"
  },
  {
    "arxiv_id": "2503.06195v1",
    "title": "Human-AI Experience in Integrated Development Environments: A Systematic Literature Review",
    "authors": [
      "Agnia Sergeyuk",
      "Ilya Zakharov",
      "Ekaterina Koshchenko",
      "Maliheh Izadi"
    ],
    "abstract": "The integration of Artificial Intelligence (AI) into Integrated Development\nEnvironments (IDEs) is reshaping software development, fundamentally altering\nhow developers interact with their tools. This shift marks the emergence of\nHuman-AI Experience in Integrated Development Environment (in-IDE HAX), a field\nthat explores the evolving dynamics of Human-Computer Interaction in\nAI-assisted coding environments. Despite rapid adoption, research on in-IDE HAX\nremains fragmented which highlights the need for a unified overview of current\npractices, challenges, and opportunities. To provide a structured overview of\nexisting research, we conduct a systematic literature review of 89 studies,\nsummarizing current findings and outlining areas for further investigation.\n  Our findings reveal that AI-assisted coding enhances developer productivity\nbut also introduces challenges, such as verification overhead, automation bias,\nand over-reliance, particularly among novice developers. Furthermore, concerns\nabout code correctness, security, and maintainability highlight the urgent need\nfor explainability, verification mechanisms, and adaptive user control.\nAlthough recent advances have driven the field forward, significant research\ngaps remain, including a lack of longitudinal studies, personalization\nstrategies, and AI governance frameworks. This review provides a foundation for\nadvancing in-IDE HAX research and offers guidance for responsibly integrating\nAI into software development.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.SE",
    "comment": "Submitted to Empirical Software Engineering (EMSE) special issue\n  Human-Centered AI for Software Engineering (HumanAISE), 28 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2503.06195v1",
    "published_date": "2025-03-08 12:40:18 UTC",
    "updated_date": "2025-03-08 12:40:18 UTC"
  },
  {
    "arxiv_id": "2503.06187v1",
    "title": "MSConv: Multiplicative and Subtractive Convolution for Face Recognition",
    "authors": [
      "Si Zhou",
      "Yain-Whar Si",
      "Xiaochen Yuan",
      "Xiaofan Li",
      "Xiaoxiang Liu",
      "Xinyuan Zhang",
      "Cong Lin",
      "Xueyuan Gong"
    ],
    "abstract": "In Neural Networks, there are various methods of feature fusion. Different\nstrategies can significantly affect the effectiveness of feature\nrepresentation, consequently influencing the ability of model to extract\nrepresentative and discriminative features. In the field of face recognition,\ntraditional feature fusion methods include feature concatenation and feature\naddition. Recently, various attention mechanism-based fusion strategies have\nemerged. However, we found that these methods primarily focus on the important\nfeatures in the image, referred to as salient features in this paper, while\nneglecting another equally important set of features for image recognition\ntasks, which we term differential features. This may cause the model to\noverlook critical local differences when dealing with complex facial samples.\nTherefore, in this paper, we propose an efficient convolution module called\nMSConv (Multiplicative and Subtractive Convolution), designed to balance the\nlearning of model about salient and differential features. Specifically, we\nemploy multi-scale mixed convolution to capture both local and broader\ncontextual information from face images, and then utilize Multiplication\nOperation (MO) and Subtraction Operation (SO) to extract salient and\ndifferential features, respectively. Experimental results demonstrate that by\nintegrating both salient and differential features, MSConv outperforms models\nthat only focus on salient features.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06187v1",
    "published_date": "2025-03-08 12:18:29 UTC",
    "updated_date": "2025-03-08 12:18:29 UTC"
  },
  {
    "arxiv_id": "2503.06184v1",
    "title": "Sample-aware Adaptive Structured Pruning for Large Language Models",
    "authors": [
      "Jun Kong",
      "Xinge Ma",
      "Jin Wang",
      "Xuejie Zhang"
    ],
    "abstract": "Large language models (LLMs) have achieved outstanding performance in natural\nlanguage processing, but enormous model sizes and high computational costs\nlimit their practical deployment. Structured pruning can effectively reduce the\nresource demands for deployment by removing redundant model parameters.\nHowever, the randomly selected calibration data and fixed single importance\nestimation metrics in existing structured pruning methods lead to degraded\nperformance of pruned models. This study introduces AdaPruner, a sample-aware\nadaptive structured pruning framework for LLMs, aiming to optimize the\ncalibration data and importance estimation metrics in the structured pruning\nprocess. Specifically, AdaPruner effectively removes redundant parameters from\nLLMs by constructing a structured pruning solution space and then employing\nBayesian optimization to adaptively search for the optimal calibration data and\nimportance estimation metrics. Experimental results show that the AdaPruner\noutperforms existing structured pruning methods on a family of LLMs with\nvarying pruning ratios, demonstrating its applicability and robustness.\nRemarkably, at a 20\\% pruning ratio, the model pruned with AdaPruner maintains\n97\\% of the performance of the unpruned model.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06184v1",
    "published_date": "2025-03-08 12:00:21 UTC",
    "updated_date": "2025-03-08 12:00:21 UTC"
  },
  {
    "arxiv_id": "2503.06183v2",
    "title": "Lightweight Software Kernels and Hardware Extensions for Efficient Sparse Deep Neural Networks on Microcontrollers",
    "authors": [
      "Francesco Daghero",
      "Daniele Jahier Pagliari",
      "Francesco Conti",
      "Luca Benini",
      "Massimo Poncino",
      "Alessio Burrello"
    ],
    "abstract": "The acceleration of pruned Deep Neural Networks (DNNs) on edge devices such\nas Microcontrollers (MCUs) is a challenging task, given the tight area- and\npower-constraints of these devices. In this work, we propose a three-fold\ncontribution to address this problem. First, we design a set of optimized\nsoftware kernels for N:M pruned layers, targeting ultra-low-power, multicore\nRISC-V MCUs, which are up to 2.1x and 3.4x faster than their dense counterparts\nat 1:8 and 1:16 sparsity, respectively. Then, we implement a lightweight\nInstruction-Set Architecture (ISA) extension to accelerate the indirect load\nand non-zero indices decompression operations required by our kernels,\nobtaining up to 1.9x extra speedup, at the cost of a 5% area overhead. Lastly,\nwe extend an open-source DNN compiler to utilize our sparse kernels for\ncomplete networks, showing speedups of 3.21x and 1.81x on a ResNet18 and a\nVision Transformer (ViT), with less than 1.5% accuracy drop compared to a dense\nbaseline.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at MLSys 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.06183v2",
    "published_date": "2025-03-08 11:59:12 UTC",
    "updated_date": "2025-03-19 10:10:55 UTC"
  },
  {
    "arxiv_id": "2503.06175v1",
    "title": "Minion Gated Recurrent Unit for Continual Learning",
    "authors": [
      "Abdullah M. Zyarah",
      "Dhireesha Kudithipudi"
    ],
    "abstract": "The increasing demand for continual learning in sequential data processing\nhas led to progressively complex training methodologies and larger recurrent\nnetwork architectures. Consequently, this has widened the knowledge gap between\ncontinual learning with recurrent neural networks (RNNs) and their ability to\noperate on devices with limited memory and compute. To address this challenge,\nwe investigate the effectiveness of simplifying RNN architectures, particularly\ngated recurrent unit (GRU), and its impact on both single-task and multitask\nsequential learning. We propose a new variant of GRU, namely the minion\nrecurrent unit (MiRU). MiRU replaces conventional gating mechanisms with\nscaling coefficients to regulate dynamic updates of hidden states and\nhistorical context, reducing computational costs and memory requirements.\nDespite its simplified architecture, MiRU maintains performance comparable to\nthe standard GRU while achieving 2.90x faster training and reducing parameter\nusage by 2.88x, as demonstrated through evaluations on sequential image\nclassification and natural language processing benchmarks. The impact of model\nsimplification on its learning capacity is also investigated by performing\ncontinual learning tasks with a rehearsal-based strategy and global inhibition.\nWe find that MiRU demonstrates stable performance in multitask learning even\nwhen using only rehearsal, unlike the standard GRU and its variants. These\nfeatures position MiRU as a promising candidate for edge-device applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06175v1",
    "published_date": "2025-03-08 11:28:40 UTC",
    "updated_date": "2025-03-08 11:28:40 UTC"
  },
  {
    "arxiv_id": "2503.06171v1",
    "title": "ROCM: RLHF on consistency models",
    "authors": [
      "Shivanshu Shekhar",
      "Tong Zhang"
    ],
    "abstract": "Diffusion models have revolutionized generative modeling in continuous\ndomains like image, audio, and video synthesis. However, their iterative\nsampling process leads to slow generation and inefficient training, challenges\nthat are further exacerbated when incorporating Reinforcement Learning from\nHuman Feedback (RLHF) due to sparse rewards and long time horizons. Consistency\nmodels address these issues by enabling single-step or efficient multi-step\ngeneration, significantly reducing computational costs.\n  In this work, we propose a direct reward optimization framework for applying\nRLHF to consistency models, incorporating distributional regularization to\nenhance training stability and prevent reward hacking. We investigate various\n$f$-divergences as regularization strategies, striking a balance between reward\nmaximization and model consistency. Unlike policy gradient methods, our\napproach leverages first-order gradients, making it more efficient and less\nsensitive to hyperparameter tuning. Empirical results show that our method\nachieves competitive or superior performance compared to policy gradient based\nRLHF methods, across various automatic metrics and human evaluation.\nAdditionally, our analysis demonstrates the impact of different regularization\ntechniques in improving model generalization and preventing overfitting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06171v1",
    "published_date": "2025-03-08 11:19:48 UTC",
    "updated_date": "2025-03-08 11:19:48 UTC"
  },
  {
    "arxiv_id": "2503.06170v2",
    "title": "Object-Centric World Model for Language-Guided Manipulation",
    "authors": [
      "Youngjoon Jeong",
      "Junha Chun",
      "Soonwoo Cha",
      "Taesup Kim"
    ],
    "abstract": "A world model is essential for an agent to predict the future and plan in\ndomains such as autonomous driving and robotics. To achieve this, recent\nadvancements have focused on video generation, which has gained significant\nattention due to the impressive success of diffusion models. However, these\nmodels require substantial computational resources. To address these\nchallenges, we propose a world model leveraging object-centric representation\nspace using slot attention, guided by language instructions. Our model\nperceives the current state as an object-centric representation and predicts\nfuture states in this representation space conditioned on natural language\ninstructions. This approach results in a more compact and computationally\nefficient model compared to diffusion-based generative alternatives.\nFurthermore, it flexibly predicts future states based on language instructions,\nand offers a significant advantage in manipulation tasks where object\nrecognition is crucial. In this paper, we demonstrate that our latent\npredictive world model surpasses generative world models in visuo-linguo-motor\ncontrol tasks, achieving superior sample and computation efficiency. We also\ninvestigate the generalization performance of the proposed method and explore\nvarious strategies for predicting actions using object-centric representations.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06170v2",
    "published_date": "2025-03-08 11:17:37 UTC",
    "updated_date": "2025-03-12 13:52:50 UTC"
  },
  {
    "arxiv_id": "2503.06169v2",
    "title": "Treble Counterfactual VLMs: A Causal Approach to Hallucination",
    "authors": [
      "Shawn Li",
      "Jiashu Qu",
      "Yuxiao Zhou",
      "Yuehan Qin",
      "Tiankai Yang",
      "Yue Zhao"
    ],
    "abstract": "Vision-Language Models (VLMs) have advanced multi-modal tasks like image\ncaptioning, visual question answering, and reasoning. However, they often\ngenerate hallucinated outputs inconsistent with the visual context or prompt,\nlimiting reliability in critical applications like autonomous driving and\nmedical imaging. Existing studies link hallucination to statistical biases,\nlanguage priors, and biased feature learning but lack a structured causal\nunderstanding. In this work, we introduce a causal perspective to analyze and\nmitigate hallucination in VLMs. We hypothesize that hallucination arises from\nunintended direct influences of either the vision or text modality, bypassing\nproper multi-modal fusion. To address this, we construct a causal graph for\nVLMs and employ counterfactual analysis to estimate the Natural Direct Effect\n(NDE) of vision, text, and their cross-modal interaction on the output. We\nsystematically identify and mitigate these unintended direct effects to ensure\nthat responses are primarily driven by genuine multi-modal fusion. Our approach\nconsists of three steps: (1) designing structural causal graphs to distinguish\ncorrect fusion pathways from spurious modality shortcuts, (2) estimating\nmodality-specific and cross-modal NDE using perturbed image representations,\nhallucinated text embeddings, and degraded visual inputs, and (3) implementing\na test-time intervention module to dynamically adjust the model's dependence on\neach modality. Experimental results demonstrate that our method significantly\nreduces hallucination while preserving task performance, providing a robust and\ninterpretable framework for improving VLM reliability. To enhance accessibility\nand reproducibility, our code is publicly available at\nhttps://github.com/TREE985/Treble-Counterfactual-VLMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06169v2",
    "published_date": "2025-03-08 11:13:05 UTC",
    "updated_date": "2025-03-17 08:11:52 UTC"
  },
  {
    "arxiv_id": "2503.06166v2",
    "title": "Secure On-Device Video OOD Detection Without Backpropagation",
    "authors": [
      "Shawn Li",
      "Peilin Cai",
      "Yuxiao Zhou",
      "Zhiyu Ni",
      "Renjie Liang",
      "You Qin",
      "Yi Nian",
      "Zhengzhong Tu",
      "Xiyang Hu",
      "Yue Zhao"
    ],
    "abstract": "Out-of-Distribution (OOD) detection is critical for ensuring the reliability\nof machine learning models in safety-critical applications such as autonomous\ndriving and medical diagnosis. While deploying personalized OOD detection\ndirectly on edge devices is desirable, it remains challenging due to large\nmodel sizes and the computational infeasibility of on-device training.\nFederated learning partially addresses this but still requires gradient\ncomputation and backpropagation, exceeding the capabilities of many edge\ndevices. To overcome these challenges, we propose SecDOOD, a secure\ncloud-device collaboration framework for efficient on-device OOD detection\nwithout requiring device-side backpropagation. SecDOOD utilizes cloud resources\nfor model training while ensuring user data privacy by retaining sensitive\ninformation on-device. Central to SecDOOD is a HyperNetwork-based personalized\nparameter generation module, which adapts cloud-trained models to\ndevice-specific distributions by dynamically generating local weight\nadjustments, effectively combining central and local information without local\nfine-tuning. Additionally, our dynamic feature sampling and encryption strategy\nselectively encrypts only the most informative feature channels, largely\nreducing encryption overhead without compromising detection performance.\nExtensive experiments across multiple datasets and OOD scenarios demonstrate\nthat SecDOOD achieves performance comparable to fully fine-tuned models,\nenabling secure, efficient, and personalized OOD detection on resource-limited\nedge devices. To enhance accessibility and reproducibility, our code is\npublicly available at https://github.com/Dystopians/SecDOOD.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06166v2",
    "published_date": "2025-03-08 11:03:21 UTC",
    "updated_date": "2025-03-17 07:44:00 UTC"
  },
  {
    "arxiv_id": "2503.06163v1",
    "title": "VACT: A Video Automatic Causal Testing System and a Benchmark",
    "authors": [
      "Haotong Yang",
      "Qingyuan Zheng",
      "Yunjian Gao",
      "Yongkun Yang",
      "Yangbo He",
      "Zhouchen Lin",
      "Muhan Zhang"
    ],
    "abstract": "With the rapid advancement of text-conditioned Video Generation Models\n(VGMs), the quality of generated videos has significantly improved, bringing\nthese models closer to functioning as ``*world simulators*'' and making\nreal-world-level video generation more accessible and cost-effective. However,\nthe generated videos often contain factual inaccuracies and lack understanding\nof fundamental physical laws. While some previous studies have highlighted this\nissue in limited domains through manual analysis, a comprehensive solution has\nnot yet been established, primarily due to the absence of a generalized,\nautomated approach for modeling and assessing the causal reasoning of these\nmodels across diverse scenarios. To address this gap, we propose VACT: an\n**automated** framework for modeling, evaluating, and measuring the causal\nunderstanding of VGMs in real-world scenarios. By combining causal analysis\ntechniques with a carefully designed large language model assistant, our system\ncan assess the causal behavior of models in various contexts without human\nannotation, which offers strong generalization and scalability. Additionally,\nwe introduce multi-level causal evaluation metrics to provide a detailed\nanalysis of the causal performance of VGMs. As a demonstration, we use our\nframework to benchmark several prevailing VGMs, offering insight into their\ncausal reasoning capabilities. Our work lays the foundation for systematically\naddressing the causal understanding deficiencies in VGMs and contributes to\nadvancing their reliability and real-world applicability.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "stat.AP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06163v1",
    "published_date": "2025-03-08 10:54:42 UTC",
    "updated_date": "2025-03-08 10:54:42 UTC"
  },
  {
    "arxiv_id": "2503.06161v1",
    "title": "Feature-EndoGaussian: Feature Distilled Gaussian Splatting in Surgical Deformable Scene Reconstruction",
    "authors": [
      "Kai Li",
      "Junhao Wang",
      "William Han",
      "Ding Zhao"
    ],
    "abstract": "Minimally invasive surgery (MIS) has transformed clinical practice by\nreducing recovery times, minimizing complications, and enhancing precision.\nNonetheless, MIS inherently relies on indirect visualization and precise\ninstrument control, posing unique challenges. Recent advances in artificial\nintelligence have enabled real-time surgical scene understanding through\ntechniques such as image classification, object detection, and segmentation,\nwith scene reconstruction emerging as a key element for enhanced intraoperative\nguidance. Although neural radiance fields (NeRFs) have been explored for this\npurpose, their substantial data requirements and slow rendering inhibit\nreal-time performance. In contrast, 3D Gaussian Splatting (3DGS) offers a more\nefficient alternative, achieving state-of-the-art performance in dynamic\nsurgical scene reconstruction. In this work, we introduce Feature-EndoGaussian\n(FEG), an extension of 3DGS that integrates 2D segmentation cues into 3D\nrendering to enable real-time semantic and scene reconstruction. By leveraging\npretrained segmentation foundation models, FEG incorporates semantic feature\ndistillation within the Gaussian deformation framework, thereby enhancing both\nreconstruction fidelity and segmentation accuracy. On the EndoNeRF dataset, FEG\nachieves superior performance (SSIM of 0.97, PSNR of 39.08, and LPIPS of 0.03)\ncompared to leading methods. Additionally, on the EndoVis18 dataset, FEG\ndemonstrates competitive class-wise segmentation metrics while balancing model\nsize and real-time performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.06161v1",
    "published_date": "2025-03-08 10:50:19 UTC",
    "updated_date": "2025-03-08 10:50:19 UTC"
  },
  {
    "arxiv_id": "2503.06157v1",
    "title": "UrbanVideo-Bench: Benchmarking Vision-Language Models on Embodied Intelligence with Video Data in Urban Spaces",
    "authors": [
      "Baining Zhao",
      "Jianjie Fang",
      "Zichao Dai",
      "Ziyou Wang",
      "Jirong Zha",
      "Weichen Zhang",
      "Chen Gao",
      "Yue Wang",
      "Jinqiang Cui",
      "Xinlei Chen",
      "Yong Li"
    ],
    "abstract": "Large multimodal models exhibit remarkable intelligence, yet their embodied\ncognitive abilities during motion in open-ended urban 3D space remain to be\nexplored. We introduce a benchmark to evaluate whether video-large language\nmodels (Video-LLMs) can naturally process continuous first-person visual\nobservations like humans, enabling recall, perception, reasoning, and\nnavigation. We have manually control drones to collect 3D embodied motion video\ndata from real-world cities and simulated environments, resulting in 1.5k video\nclips. Then we design a pipeline to generate 5.2k multiple-choice questions.\nEvaluations of 17 widely-used Video-LLMs reveal current limitations in urban\nembodied cognition. Correlation analysis provides insight into the\nrelationships between different tasks, showing that causal reasoning has a\nstrong correlation with recall, perception, and navigation, while the abilities\nfor counterfactual and associative reasoning exhibit lower correlation with\nother tasks. We also validate the potential for Sim-to-Real transfer in urban\nembodiment through fine-tuning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "22 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.06157v1",
    "published_date": "2025-03-08 10:47:05 UTC",
    "updated_date": "2025-03-08 10:47:05 UTC"
  },
  {
    "arxiv_id": "2503.05858v3",
    "title": "Bimodal Connection Attention Fusion for Speech Emotion Recognition",
    "authors": [
      "Jiachen Luo",
      "Huy Phan",
      "Lin Wang",
      "Joshua D. Reiss"
    ],
    "abstract": "Multi-modal emotion recognition is challenging due to the difficulty of\nextracting features that capture subtle emotional differences. Understanding\nmulti-modal interactions and connections is key to building effective bimodal\nspeech emotion recognition systems. In this work, we propose Bimodal Connection\nAttention Fusion (BCAF) method, which includes three main modules: the\ninteractive connection network, the bimodal attention network, and the\ncorrelative attention network. The interactive connection network uses an\nencoder-decoder architecture to model modality connections between audio and\ntext while leveraging modality-specific features. The bimodal attention network\nenhances semantic complementation and exploits intra- and inter-modal\ninteractions. The correlative attention network reduces cross-modal noise and\ncaptures correlations between audio and text. Experiments on the MELD and\nIEMOCAP datasets demonstrate that the proposed BCAF method outperforms existing\nstate-of-the-art baselines.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05858v3",
    "published_date": "2025-03-08 10:20:57 UTC",
    "updated_date": "2025-03-22 11:48:18 UTC"
  },
  {
    "arxiv_id": "2503.06144v1",
    "title": "Exploring the usage of Probabilistic Neural Networks for Ionospheric electron density estimation",
    "authors": [
      "Miquel Garcia-Fernandez"
    ],
    "abstract": "A fundamental limitation of traditional Neural Networks (NN) in predictive\nmodelling is their inability to quantify uncertainty in their outputs. In\ncritical applications like positioning systems, understanding the reliability\nof predictions is critical for constructing confidence intervals, early warning\nsystems, and effectively propagating results. For instance, Precise Point\nPositioning in satellite navigation heavily relies on accurate error models for\nancillary data (orbits, clocks, ionosphere, and troposphere) to compute precise\nerror estimates. In addition, these uncertainty estimates are needed to\nestablish robust protection levels in safety critical applications.\n  To address this challenge, the main objectives of this paper aims at\nexploring a potential framework capable of providing both point estimates and\nassociated uncertainty measures of ionospheric Vertical Total Electron Content\n(VTEC). In this context, Probabilistic Neural Networks (PNNs) offer a promising\napproach to achieve this goal. However, constructing an effective PNN requires\nmeticulous design of hidden and output layers, as well as careful definition of\nprior and posterior probability distributions for network weights and biases.\n  A key finding of this study is that the uncertainty provided by the PNN model\nin VTEC estimates may be systematically underestimated. In low-latitude areas,\nthe actual error was observed to be as much as twice the model's estimate. This\nunderestimation is expected to be more pronounced during solar maximum,\ncorrelating with increased VTEC values.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "86A10, 62M45,",
      "I.2.6; G.3; J.2"
    ],
    "primary_category": "eess.SP",
    "comment": "13 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.06144v1",
    "published_date": "2025-03-08 10:06:15 UTC",
    "updated_date": "2025-03-08 10:06:15 UTC"
  },
  {
    "arxiv_id": "2503.06138v2",
    "title": "System 0/1/2/3: Quad-process theory for multi-timescale embodied collective cognitive systems",
    "authors": [
      "Tadahiro Taniguchi",
      "Yasushi Hirai",
      "Masahiro Suzuki",
      "Shingo Murata",
      "Takato Horii",
      "Kazutoshi Tanaka"
    ],
    "abstract": "This paper introduces the System 0/1/2/3 framework as an extension of\ndual-process theory, employing a quad-process model of cognition. Expanding\nupon System 1 (fast, intuitive thinking) and System 2 (slow, deliberative\nthinking), we incorporate System 0, which represents pre-cognitive embodied\nprocesses, and System 3, which encompasses collective intelligence and symbol\nemergence. We contextualize this model within Bergson's philosophy by adopting\nmulti-scale time theory to unify the diverse temporal dynamics of cognition.\nSystem 0 emphasizes morphological computation and passive dynamics,\nillustrating how physical embodiment enables adaptive behavior without explicit\nneural processing. Systems 1 and 2 are explained from a constructive\nperspective, incorporating neurodynamical and AI viewpoints. In System 3, we\nintroduce collective predictive coding to explain how societal-level adaptation\nand symbol emergence operate over extended timescales. This comprehensive\nframework ranges from rapid embodied reactions to slow-evolving collective\nintelligence, offering a unified perspective on cognition across multiple\ntimescales, levels of abstraction, and forms of human intelligence. The System\n0/1/2/3 model provides a novel theoretical foundation for understanding the\ninterplay between adaptive and cognitive processes, thereby opening new avenues\nfor research in cognitive science, AI, robotics, and collective intelligence.",
    "categories": [
      "cs.AI",
      "cs.RO",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2503.06138v2",
    "published_date": "2025-03-08 09:31:53 UTC",
    "updated_date": "2025-03-13 23:45:53 UTC"
  },
  {
    "arxiv_id": "2503.06136v1",
    "title": "GSV3D: Gaussian Splatting-based Geometric Distillation with Stable Video Diffusion for Single-Image 3D Object Generation",
    "authors": [
      "Ye Tao",
      "Jiawei Zhang",
      "Yahao Shi",
      "Dongqing Zou",
      "Bin Zhou"
    ],
    "abstract": "Image-based 3D generation has vast applications in robotics and gaming, where\nhigh-quality, diverse outputs and consistent 3D representations are crucial.\nHowever, existing methods have limitations: 3D diffusion models are limited by\ndataset scarcity and the absence of strong pre-trained priors, while 2D\ndiffusion-based approaches struggle with geometric consistency. We propose a\nmethod that leverages 2D diffusion models' implicit 3D reasoning ability while\nensuring 3D consistency via Gaussian-splatting-based geometric distillation.\nSpecifically, the proposed Gaussian Splatting Decoder enforces 3D consistency\nby transforming SV3D latent outputs into an explicit 3D representation. Unlike\nSV3D, which only relies on implicit 2D representations for video generation,\nGaussian Splatting explicitly encodes spatial and appearance attributes,\nenabling multi-view consistency through geometric constraints. These\nconstraints correct view inconsistencies, ensuring robust geometric\nconsistency. As a result, our approach simultaneously generates high-quality,\nmulti-view-consistent images and accurate 3D models, providing a scalable\nsolution for single-image-based 3D generation and bridging the gap between 2D\nDiffusion diversity and 3D structural coherence. Experimental results\ndemonstrate state-of-the-art multi-view consistency and strong generalization\nacross diverse datasets. The code will be made publicly available upon\nacceptance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06136v1",
    "published_date": "2025-03-08 09:10:31 UTC",
    "updated_date": "2025-03-08 09:10:31 UTC"
  },
  {
    "arxiv_id": "2503.10659v1",
    "title": "MARRO: Multi-headed Attention for Rhetorical Role Labeling in Legal Documents",
    "authors": [
      "Purbid Bambroo",
      "Subinay Adhikary",
      "Paheli Bhattacharya",
      "Abhijnan Chakraborty",
      "Saptarshi Ghosh",
      "Kripabandhu Ghosh"
    ],
    "abstract": "Identification of rhetorical roles like facts, arguments, and final judgments\nis central to understanding a legal case document and can lend power to other\ndownstream tasks like legal case summarization and judgment prediction.\nHowever, there are several challenges to this task. Legal documents are often\nunstructured and contain a specialized vocabulary, making it hard for\nconventional transformer models to understand them. Additionally, these\ndocuments run into several pages, which makes it difficult for neural models to\ncapture the entire context at once. Lastly, there is a dearth of annotated\nlegal documents to train deep learning models. Previous state-of-the-art\napproaches for this task have focused on using neural models like BiLSTM-CRF or\nhave explored different embedding techniques to achieve decent results. While\nsuch techniques have shown that better embedding can result in improved model\nperformance, not many models have focused on utilizing attention for learning\nbetter embeddings in sentences of a document. Additionally, it has been\nrecently shown that advanced techniques like multi-task learning can help the\nmodels learn better representations, thereby improving performance. In this\npaper, we combine these two aspects by proposing a novel family of multi-task\nlearning-based models for rhetorical role labeling, named MARRO, that uses\ntransformer-inspired multi-headed attention. Using label shift as an auxiliary\ntask, we show that models from the MARRO family achieve state-of-the-art\nresults on two labeled datasets for rhetorical role labeling, from the Indian\nand UK Supreme Courts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.10659v1",
    "published_date": "2025-03-08 08:05:20 UTC",
    "updated_date": "2025-03-08 08:05:20 UTC"
  },
  {
    "arxiv_id": "2503.06108v1",
    "title": "Multi-modal expressive personality recognition in data non-ideal audiovisual based on multi-scale feature enhancement and modal augment",
    "authors": [
      "Weixuan Kong",
      "Jinpeng Yu",
      "Zijun Li",
      "Hanwei Liu",
      "Jiqing Qu",
      "Hui Xiao",
      "Xuefeng Li"
    ],
    "abstract": "Automatic personality recognition is a research hotspot in the intersection\nof computer science and psychology, and in human-computer interaction,\npersonalised has a wide range of applications services and other scenarios. In\nthis paper, an end-to-end multimodal performance personality is established for\nboth visual and auditory modal datarecognition network , and the through\nfeature-level fusion , which effectively of the two modalities is carried out\nthe cross-attention mechanismfuses the features of the two modal data; and a is\nproposed multiscale feature enhancement modalitiesmodule , which enhances for\nvisual and auditory boththe expression of the information of effective the\nfeatures and suppresses the interference of the redundant information. In\naddition, during the training process, this paper proposes a modal enhancement\ntraining strategy to simulate non-ideal such as modal loss and noise\ninterferencedata situations , which enhances the adaptability ofand the model\nto non-ideal data scenarios improves the robustness of the model. Experimental\nresults show that the method proposed in this paper is able to achieve an\naverage Big Five personality accuracy of , which outperforms existing 0.916 on\nthe personality analysis dataset ChaLearn First Impressionother methods based\non audiovisual and audio-visual both modalities. The ablation experiments also\nvalidate our proposed , respectivelythe contribution of module and modality\nenhancement strategy to the model performance. Finally, we simulate in the\ninference phase multi-scale feature enhancement six non-ideal data scenarios to\nverify the modal enhancement strategy's improvement in model robustness.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06108v1",
    "published_date": "2025-03-08 07:20:44 UTC",
    "updated_date": "2025-03-08 07:20:44 UTC"
  },
  {
    "arxiv_id": "2503.06107v1",
    "title": "Feature Fusion Attention Network with CycleGAN for Image Dehazing, De-Snowing and De-Raining",
    "authors": [
      "Akshat Jain"
    ],
    "abstract": "This paper presents a novel approach to image dehazing by combining Feature\nFusion Attention (FFA) networks with CycleGAN architecture. Our method\nleverages both supervised and unsupervised learning techniques to effectively\nremove haze from images while preserving crucial image details. The proposed\nhybrid architecture demonstrates significant improvements in image quality\nmetrics, achieving superior PSNR and SSIM scores compared to traditional\ndehazing methods. Through extensive experimentation on the RESIDE and DenseHaze\nCVPR 2019 dataset, we show that our approach effectively handles both synthetic\nand real-world hazy images. CycleGAN handles the unpaired nature of hazy and\nclean images effectively, enabling the model to learn mappings even without\npaired data.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06107v1",
    "published_date": "2025-03-08 07:18:42 UTC",
    "updated_date": "2025-03-08 07:18:42 UTC"
  },
  {
    "arxiv_id": "2503.06101v1",
    "title": "ULTHO: Ultra-Lightweight yet Efficient Hyperparameter Optimization in Deep Reinforcement Learning",
    "authors": [
      "Mingqi Yuan",
      "Bo Li",
      "Xin Jin",
      "Wenjun Zeng"
    ],
    "abstract": "Hyperparameter optimization (HPO) is a billion-dollar problem in machine\nlearning, which significantly impacts the training efficiency and model\nperformance. However, achieving efficient and robust HPO in deep reinforcement\nlearning (RL) is consistently challenging due to its high non-stationarity and\ncomputational cost. To tackle this problem, existing approaches attempt to\nadapt common HPO techniques (e.g., population-based training or Bayesian\noptimization) to the RL scenario. However, they remain sample-inefficient and\ncomputationally expensive, which cannot facilitate a wide range of\napplications. In this paper, we propose ULTHO, an ultra-lightweight yet\npowerful framework for fast HPO in deep RL within single runs. Specifically, we\nformulate the HPO process as a multi-armed bandit with clustered arms (MABC)\nand link it directly to long-term return optimization. ULTHO also provides a\nquantified and statistical perspective to filter the HPs efficiently. We test\nULTHO on benchmarks including ALE, Procgen, MiniGrid, and PyBullet. Extensive\nexperiments demonstrate that the ULTHO can achieve superior performance with\nsimple architecture, contributing to the development of advanced and automated\nRL systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages, 22 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.06101v1",
    "published_date": "2025-03-08 07:03:43 UTC",
    "updated_date": "2025-03-08 07:03:43 UTC"
  },
  {
    "arxiv_id": "2503.06092v1",
    "title": "ZO-DARTS++: An Efficient and Size-Variable Zeroth-Order Neural Architecture Search Algorithm",
    "authors": [
      "Lunchen Xie",
      "Eugenio Lomurno",
      "Matteo Gambella",
      "Danilo Ardagna",
      "Manual Roveri",
      "Matteo Matteucci",
      "Qingjiang Shi"
    ],
    "abstract": "Differentiable Neural Architecture Search (NAS) provides a promising avenue\nfor automating the complex design of deep learning (DL) models. However,\ncurrent differentiable NAS methods often face constraints in efficiency,\noperation selection, and adaptability under varying resource limitations. We\nintroduce ZO-DARTS++, a novel NAS method that effectively balances performance\nand resource constraints. By integrating a zeroth-order approximation for\nefficient gradient handling, employing a sparsemax function with temperature\nannealing for clearer and more interpretable architecture distributions, and\nadopting a size-variable search scheme for generating compact yet accurate\narchitectures, ZO-DARTS++ establishes a new balance between model complexity\nand performance. In extensive tests on medical imaging datasets, ZO-DARTS++\nimproves the average accuracy by up to 1.8\\% over standard DARTS-based methods\nand shortens search time by approximately 38.6\\%. Additionally, its\nresource-constrained variants can reduce the number of parameters by more than\n35\\% while maintaining competitive accuracy levels. Thus, ZO-DARTS++ offers a\nversatile and efficient framework for generating high-quality, resource-aware\nDL models suitable for real-world medical applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "I.5.1; I.5.4; I.2.6; I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.06092v1",
    "published_date": "2025-03-08 06:43:33 UTC",
    "updated_date": "2025-03-08 06:43:33 UTC"
  },
  {
    "arxiv_id": "2503.06083v1",
    "title": "T-CBF: Traversability-based Control Barrier Function to Navigate Vertically Challenging Terrain",
    "authors": [
      "Manas Gupta",
      "Xuesu Xiao"
    ],
    "abstract": "Safety has been of paramount importance in motion planning and control\ntechniques and is an active area of research in the past few years. Most safety\nresearch for mobile robots target at maintaining safety with the notion of\ncollision avoidance. However, safety goes beyond just avoiding collisions,\nespecially when robots have to navigate unstructured, vertically challenging,\noff-road terrain, where vehicle rollover and immobilization is as critical as\ncollisions. In this work, we introduce a novel Traversability-based Control\nBarrier Function (T-CBF), in which we use neural Control Barrier Functions\n(CBFs) to achieve safety beyond collision avoidance on unstructured vertically\nchallenging terrain by reasoning about new safety aspects in terms of\ntraversability. The neural T-CBF trained on safe and unsafe observations\nspecific to traversability safety is then used to generate safe trajectories.\nFurthermore, we present experimental results in simulation and on a physical\nVerti-4 Wheeler (V4W) platform, demonstrating that T-CBF can provide\ntraversability safety while reaching the goal position. T-CBF planner\noutperforms previously developed planners by 30\\% in terms of keeping the robot\nsafe and mobile when navigating on real world vertically challenging terrain.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06083v1",
    "published_date": "2025-03-08 06:12:38 UTC",
    "updated_date": "2025-03-08 06:12:38 UTC"
  },
  {
    "arxiv_id": "2503.07661v1",
    "title": "Disrupting Model Merging: A Parameter-Level Defense Without Sacrificing Accuracy",
    "authors": [
      "Wei Junhao",
      "Yu Zhe",
      "Sakuma Jun"
    ],
    "abstract": "Model merging is a technique that combines multiple finetuned models into a\nsingle model without additional training, allowing a free-rider to cheaply\ninherit specialized capabilities. This study investigates methodologies to\nsuppress unwanted model merging by free-riders. Existing methods such as model\nwatermarking or fingerprinting can only detect merging in hindsight. In\ncontrast, we propose a first proactive defense against model merging.\nSpecifically, our defense method modifies the model parameters so that the\nmodel is disrupted if the model is merged with any other model, while its\nfunctionality is kept unchanged if not merged with others. Our approach\nconsists of two modules, rearranging MLP parameters and scaling attention\nheads, which push the model out of the shared basin in parameter space, causing\nthe merging performance with other models to degrade significantly. We conduct\nextensive experiments on image classification, image generation, and text\nclassification to demonstrate that our defense severely disrupts merging while\nretaining the functionality of the post-protect model. Moreover, we analyze\npotential adaptive attacks and further propose a dropout-based pruning to\nimprove our proposal's robustness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.07661v1",
    "published_date": "2025-03-08 06:08:47 UTC",
    "updated_date": "2025-03-08 06:08:47 UTC"
  },
  {
    "arxiv_id": "2503.06074v1",
    "title": "Towards Conversational AI for Disease Management",
    "authors": [
      "Anil Palepu",
      "Valentin Liévin",
      "Wei-Hung Weng",
      "Khaled Saab",
      "David Stutz",
      "Yong Cheng",
      "Kavita Kulkarni",
      "S. Sara Mahdavi",
      "Joëlle Barral",
      "Dale R. Webster",
      "Katherine Chou",
      "Avinatan Hassidim",
      "Yossi Matias",
      "James Manyika",
      "Ryutaro Tanno",
      "Vivek Natarajan",
      "Adam Rodman",
      "Tao Tu",
      "Alan Karthikesalingam",
      "Mike Schaekermann"
    ],
    "abstract": "While large language models (LLMs) have shown promise in diagnostic dialogue,\ntheir capabilities for effective management reasoning - including disease\nprogression, therapeutic response, and safe medication prescription - remain\nunder-explored. We advance the previously demonstrated diagnostic capabilities\nof the Articulate Medical Intelligence Explorer (AMIE) through a new LLM-based\nagentic system optimised for clinical management and dialogue, incorporating\nreasoning over the evolution of disease and multiple patient visit encounters,\nresponse to therapy, and professional competence in medication prescription. To\nground its reasoning in authoritative clinical knowledge, AMIE leverages\nGemini's long-context capabilities, combining in-context retrieval with\nstructured reasoning to align its output with relevant and up-to-date clinical\npractice guidelines and drug formularies. In a randomized, blinded virtual\nObjective Structured Clinical Examination (OSCE) study, AMIE was compared to 21\nprimary care physicians (PCPs) across 100 multi-visit case scenarios designed\nto reflect UK NICE Guidance and BMJ Best Practice guidelines. AMIE was\nnon-inferior to PCPs in management reasoning as assessed by specialist\nphysicians and scored better in both preciseness of treatments and\ninvestigations, and in its alignment with and grounding of management plans in\nclinical guidelines. To benchmark medication reasoning, we developed RxQA, a\nmultiple-choice question benchmark derived from two national drug formularies\n(US, UK) and validated by board-certified pharmacists. While AMIE and PCPs both\nbenefited from the ability to access external drug information, AMIE\noutperformed PCPs on higher difficulty questions. While further research would\nbe needed before real-world translation, AMIE's strong performance across\nevaluations marks a significant step towards conversational AI as a tool in\ndisease management.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "62 pages, 7 figures in main text, 36 figures in appendix",
    "pdf_url": "http://arxiv.org/pdf/2503.06074v1",
    "published_date": "2025-03-08 05:48:58 UTC",
    "updated_date": "2025-03-08 05:48:58 UTC"
  },
  {
    "arxiv_id": "2503.06073v1",
    "title": "GEM: Empowering MLLM for Grounded ECG Understanding with Time Series and Images",
    "authors": [
      "Xiang Lan",
      "Feng Wu",
      "Kai He",
      "Qinghao Zhao",
      "Shenda Hong",
      "Mengling Feng"
    ],
    "abstract": "While recent multimodal large language models (MLLMs) have advanced automated\nECG interpretation, they still face two key limitations: (1) insufficient\nmultimodal synergy between time series signals and visual ECG representations,\nand (2) limited explainability in linking diagnoses to granular waveform\nevidence. We introduce GEM, the first MLLM unifying ECG time series, 12-lead\nECG images and text for grounded and clinician-aligned ECG interpretation. GEM\nenables feature-grounded analysis, evidence-driven reasoning, and a\nclinician-like diagnostic process through three core innovations: a\ndual-encoder framework extracting complementary time series and image features,\ncross-modal alignment for effective multimodal understanding, and\nknowledge-guided instruction generation for generating high-granularity\ngrounding data (ECG-Grounding) linking diagnoses to measurable parameters\n($e.g.$, QRS/PR Intervals). Additionally, we propose the Grounded ECG\nUnderstanding task, a clinically motivated benchmark designed to\ncomprehensively assess the MLLM's capability in grounded ECG understanding.\nExperimental results on both existing and our proposed benchmarks show GEM\nsignificantly improves predictive performance (CSN $7.4\\% \\uparrow$),\nexplainability ($22.7\\% \\uparrow$), and grounding ($24.8\\% \\uparrow$), making\nit more suitable for real-world clinical applications. GitHub repository:\nhttps://github.com/lanxiang1017/GEM.git",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06073v1",
    "published_date": "2025-03-08 05:48:53 UTC",
    "updated_date": "2025-03-08 05:48:53 UTC"
  },
  {
    "arxiv_id": "2503.06072v1",
    "title": "A Survey on Post-training of Large Language Models",
    "authors": [
      "Guiyao Tie",
      "Zeli Zhao",
      "Dingjie Song",
      "Fuyang Wei",
      "Rong Zhou",
      "Yurou Dai",
      "Wen Yin",
      "Zhejian Yang",
      "Jiangyue Yan",
      "Yao Su",
      "Zhenhan Dai",
      "Yifeng Xie",
      "Yihan Cao",
      "Lichao Sun",
      "Pan Zhou",
      "Lifang He",
      "Hechang Chen",
      "Yu Zhang",
      "Qingsong Wen",
      "Tianming Liu",
      "Neil Zhenqiang Gong",
      "Jiliang Tang",
      "Caiming Xiong",
      "Heng Ji",
      "Philip S. Yu",
      "Jianfeng Gao"
    ],
    "abstract": "The emergence of Large Language Models (LLMs) has fundamentally transformed\nnatural language processing, making them indispensable across domains ranging\nfrom conversational systems to scientific exploration. However, their\npre-trained architectures often reveal limitations in specialized contexts,\nincluding restricted reasoning capacities, ethical uncertainties, and\nsuboptimal domain-specific performance. These challenges necessitate advanced\npost-training language models (PoLMs) to address these shortcomings, such as\nOpenAI-o1/o3 and DeepSeek-R1 (collectively known as Large Reasoning Models, or\nLRMs). This paper presents the first comprehensive survey of PoLMs,\nsystematically tracing their evolution across five core paradigms: Fine-tuning,\nwhich enhances task-specific accuracy; Alignment, which ensures alignment with\nhuman preferences; Reasoning, which advances multi-step inference despite\nchallenges in reward design; Efficiency, which optimizes resource utilization\namidst increasing complexity; and Integration and Adaptation, which extend\ncapabilities across diverse modalities while addressing coherence issues.\nCharting progress from ChatGPT's foundational alignment strategies to\nDeepSeek-R1's innovative reasoning advancements, we illustrate how PoLMs\nleverage datasets to mitigate biases, deepen reasoning capabilities, and\nenhance domain adaptability. Our contributions include a pioneering synthesis\nof PoLM evolution, a structured taxonomy categorizing techniques and datasets,\nand a strategic agenda emphasizing the role of LRMs in improving reasoning\nproficiency and domain flexibility. As the first survey of its scope, this work\nconsolidates recent PoLM advancements and establishes a rigorous intellectual\nframework for future research, fostering the development of LLMs that excel in\nprecision, ethical robustness, and versatility across scientific and societal\napplications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "87 pages, 21 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.06072v1",
    "published_date": "2025-03-08 05:41:42 UTC",
    "updated_date": "2025-03-08 05:41:42 UTC"
  },
  {
    "arxiv_id": "2503.06064v1",
    "title": "A Novel Trustworthy Video Summarization Algorithm Through a Mixture of LoRA Experts",
    "authors": [
      "Wenzhuo Du",
      "Gerun Wang",
      "Guancheng Chen",
      "Hang Zhao",
      "Xin Li",
      "Jian Gao"
    ],
    "abstract": "With the exponential growth of user-generated content on video-sharing\nplatforms, the challenge of facilitating efficient searching and browsing of\nvideos has garnered significant attention. To enhance users' ability to swiftly\nlocate and review pertinent videos, the creation of concise and informative\nvideo summaries has become increasingly important. Video-llama is an effective\ntool for generating video summarization, but it cannot effectively unify and\noptimize the modeling of temporal and spatial features and requires a lot of\ncomputational resources and time. Therefore, we propose MiLoRA-ViSum to more\nefficiently capture complex temporal dynamics and spatial relationships\ninherent in video data and to control the number of parameters for training. By\nextending traditional Low-Rank Adaptation (LoRA) into a sophisticated\nmixture-of-experts paradigm, MiLoRA-ViSum incorporates a dual temporal-spatial\nadaptation mechanism tailored specifically for video summarization tasks. This\napproach dynamically integrates specialized LoRA experts, each fine-tuned to\naddress distinct temporal or spatial dimensions. Extensive evaluations of the\nVideoXum and ActivityNet datasets demonstrate that MiLoRA-ViSum achieves the\nbest summarization performance compared to state-of-the-art models, while\nmaintaining significantly lower computational costs. The proposed\nmixture-of-experts strategy, combined with the dual adaptation mechanism,\nhighlights the model's potential to enhance video summarization capabilities,\nparticularly in large-scale applications requiring both efficiency and\nprecision.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06064v1",
    "published_date": "2025-03-08 05:20:52 UTC",
    "updated_date": "2025-03-08 05:20:52 UTC"
  },
  {
    "arxiv_id": "2503.06060v1",
    "title": "STAR: A Foundation Model-driven Framework for Robust Task Planning and Failure Recovery in Robotic Systems",
    "authors": [
      "Md Sadman Sakib",
      "Yu Sun"
    ],
    "abstract": "Modern robotic systems, deployed across domains from industrial automation to\ndomestic assistance, face a critical challenge: executing tasks with precision\nand adaptability in dynamic, unpredictable environments. To address this, we\npropose STAR (Smart Task Adaptation and Recovery), a novel framework that\nsynergizes Foundation Models (FMs) with dynamically expanding Knowledge Graphs\n(KGs) to enable resilient task planning and autonomous failure recovery. While\nFMs offer remarkable generalization and contextual reasoning, their\nlimitations, including computational inefficiency, hallucinations, and output\ninconsistencies hinder reliable deployment. STAR mitigates these issues by\nembedding learned knowledge into structured, reusable KGs, which streamline\ninformation retrieval, reduce redundant FM computations, and provide precise,\nscenario-specific insights. The framework leverages FM-driven reasoning to\ndiagnose failures, generate context-aware recovery strategies, and execute\ncorrective actions without human intervention or system restarts. Unlike\nconventional approaches that rely on rigid protocols, STAR dynamically expands\nits KG with experiential knowledge, ensuring continuous adaptation to novel\nscenarios. To evaluate the effectiveness of this approach, we developed a\ncomprehensive dataset that includes various robotic tasks and failure\nscenarios. Through extensive experimentation, STAR demonstrated an 86% task\nplanning accuracy and 78% recovery success rate, showing significant\nimprovements over baseline methods. The framework's ability to continuously\nlearn from experience while maintaining structured knowledge representation\nmakes it particularly suitable for long-term deployment in real-world\napplications.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06060v1",
    "published_date": "2025-03-08 05:05:21 UTC",
    "updated_date": "2025-03-08 05:05:21 UTC"
  },
  {
    "arxiv_id": "2503.06059v1",
    "title": "MANDARIN: Mixture-of-Experts Framework for Dynamic Delirium and Coma Prediction in ICU Patients: Development and Validation of an Acute Brain Dysfunction Prediction Model",
    "authors": [
      "Miguel Contreras",
      "Jessica Sena",
      "Andrea Davidson",
      "Jiaqing Zhang",
      "Tezcan Ozrazgat-Baslanti",
      "Yuanfang Ren",
      "Ziyuan Guan",
      "Jeremy Balch",
      "Tyler Loftus",
      "Subhash Nerella",
      "Azra Bihorac",
      "Parisa Rashidi"
    ],
    "abstract": "Acute brain dysfunction (ABD) is a common, severe ICU complication,\npresenting as delirium or coma and leading to prolonged stays, increased\nmortality, and cognitive decline. Traditional screening tools like the Glasgow\nComa Scale (GCS), Confusion Assessment Method (CAM), and Richmond\nAgitation-Sedation Scale (RASS) rely on intermittent assessments, causing\ndelays and inconsistencies. In this study, we propose MANDARIN\n(Mixture-of-Experts Framework for Dynamic Delirium and Coma Prediction in ICU\nPatients), a 1.5M-parameter mixture-of-experts neural network to predict ABD in\nreal-time among ICU patients. The model integrates temporal and static data\nfrom the ICU to predict the brain status in the next 12 to 72 hours, using a\nmulti-branch approach to account for current brain status. The MANDARIN model\nwas trained on data from 92,734 patients (132,997 ICU admissions) from 2\nhospitals between 2008-2019 and validated externally on data from 11,719\npatients (14,519 ICU admissions) from 15 hospitals and prospectively on data\nfrom 304 patients (503 ICU admissions) from one hospital in 2021-2024. Three\ndatasets were used: the University of Florida Health (UFH) dataset, the\nelectronic ICU Collaborative Research Database (eICU), and the Medical\nInformation Mart for Intensive Care (MIMIC)-IV dataset. MANDARIN significantly\noutperforms the baseline neurological assessment scores (GCS, CAM, and RASS)\nfor delirium prediction in both external (AUROC 75.5% CI: 74.2%-76.8% vs 68.3%\nCI: 66.9%-69.5%) and prospective (AUROC 82.0% CI: 74.8%-89.2% vs 72.7% CI:\n65.5%-81.0%) cohorts, as well as for coma prediction (external AUROC 87.3% CI:\n85.9%-89.0% vs 72.8% CI: 70.6%-74.9%, and prospective AUROC 93.4% CI:\n88.5%-97.9% vs 67.7% CI: 57.7%-76.8%) with a 12-hour lead time. This tool has\nthe potential to assist clinicians in decision-making by continuously\nmonitoring the brain status of patients in the ICU.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06059v1",
    "published_date": "2025-03-08 04:56:41 UTC",
    "updated_date": "2025-03-08 04:56:41 UTC"
  },
  {
    "arxiv_id": "2503.06054v1",
    "title": "Fine-Grained Bias Detection in LLM: Enhancing detection mechanisms for nuanced biases",
    "authors": [
      "Suvendu Mohanty"
    ],
    "abstract": "Recent advancements in Artificial Intelligence, particularly in Large\nLanguage Models (LLMs), have transformed natural language processing by\nimproving generative capabilities. However, detecting biases embedded within\nthese models remains a challenge. Subtle biases can propagate misinformation,\ninfluence decision-making, and reinforce stereotypes, raising ethical concerns.\nThis study presents a detection framework to identify nuanced biases in LLMs.\nThe approach integrates contextual analysis, interpretability via attention\nmechanisms, and counterfactual data augmentation to capture hidden biases\nacross linguistic contexts. The methodology employs contrastive prompts and\nsynthetic datasets to analyze model behaviour across cultural, ideological, and\ndemographic scenarios.\n  Quantitative analysis using benchmark datasets and qualitative assessments\nthrough expert reviews validate the effectiveness of the framework. Results\nshow improvements in detecting subtle biases compared to conventional methods,\nwhich often fail to highlight disparities in model responses to race, gender,\nand socio-political contexts. The framework also identifies biases arising from\nimbalances in training data and model architectures. Continuous user feedback\nensures adaptability and refinement. This research underscores the importance\nof proactive bias mitigation strategies and calls for collaboration between\npolicymakers, AI developers, and regulators. The proposed detection mechanisms\nenhance model transparency and support responsible LLM deployment in sensitive\napplications such as education, legal systems, and healthcare. Future work will\nfocus on real-time bias monitoring and cross-linguistic generalization to\nimprove fairness and inclusivity in AI-driven communication tools.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Bias detection, Large Language Models, nuanced biases, fine-grained\n  mechanisms, model transparency, ethical AI",
    "pdf_url": "http://arxiv.org/pdf/2503.06054v1",
    "published_date": "2025-03-08 04:43:01 UTC",
    "updated_date": "2025-03-08 04:43:01 UTC"
  },
  {
    "arxiv_id": "2503.06053v1",
    "title": "DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal Consistent Video Generation",
    "authors": [
      "Runze Zhang",
      "Guoguang Du",
      "Xiaochuan Li",
      "Qi Jia",
      "Liang Jin",
      "Lu Liu",
      "Jingjing Wang",
      "Cong Xu",
      "Zhenhua Guo",
      "Yaqian Zhao",
      "Xiaoli Gong",
      "Rengang Li",
      "Baoyu Fan"
    ],
    "abstract": "Spatio-temporal consistency is a critical research topic in video generation.\nA qualified generated video segment must ensure plot plausibility and coherence\nwhile maintaining visual consistency of objects and scenes across varying\nviewpoints. Prior research, especially in open-source projects, primarily\nfocuses on either temporal or spatial consistency, or their basic combination,\nsuch as appending a description of a camera movement after a prompt without\nconstraining the outcomes of this movement. However, camera movement may\nintroduce new objects to the scene or eliminate existing ones, thereby\noverlaying and affecting the preceding narrative. Especially in videos with\nnumerous camera movements, the interplay between multiple plots becomes\nincreasingly complex. This paper introduces and examines integral\nspatio-temporal consistency, considering the synergy between plot progression\nand camera techniques, and the long-term impact of prior content on subsequent\ngeneration. Our research encompasses dataset construction through to the\ndevelopment of the model. Initially, we constructed a DropletVideo-10M dataset,\nwhich comprises 10 million videos featuring dynamic camera motion and object\nactions. Each video is annotated with an average caption of 206 words,\ndetailing various camera movements and plot developments. Following this, we\ndeveloped and trained the DropletVideo model, which excels in preserving\nspatio-temporal coherence during video generation. The DropletVideo dataset and\nmodel are accessible at https://dropletx.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06053v1",
    "published_date": "2025-03-08 04:37:38 UTC",
    "updated_date": "2025-03-08 04:37:38 UTC"
  },
  {
    "arxiv_id": "2503.06047v1",
    "title": "DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based Agents in Complex Decision-Making Environments",
    "authors": [
      "Wenjie Tang",
      "Yuan Zhou",
      "Erqiang Xu",
      "Keyan Cheng",
      "Minne Li",
      "Liquan Xiao"
    ],
    "abstract": "Large Language Model~(LLM) based agents have been increasingly popular in\nsolving complex and dynamic tasks, which requires proper evaluation systems to\nassess their capabilities. Nevertheless, existing benchmarks usually either\nfocus on single-objective tasks or use overly broad assessing metrics, failing\nto provide a comprehensive inspection of the actual capabilities of LLM-based\nagents in complicated decision-making tasks. To address these issues, we\nintroduce DSGBench, a more rigorous evaluation platform for strategic\ndecision-making. Firstly, it incorporates six complex strategic games which\nserve as ideal testbeds due to their long-term and multi-dimensional\ndecision-making demands and flexibility in customizing tasks of various\ndifficulty levels or multiple targets. Secondly, DSGBench employs a\nfine-grained evaluation scoring system which examines the decision-making\ncapabilities by looking into the performance in five specific dimensions and\noffering a comprehensive assessment in a well-designed way. Furthermore,\nDSGBench also incorporates an automated decision-tracking mechanism which\nenables in-depth analysis of agent behaviour patterns and the changes in their\nstrategies. We demonstrate the advances of DSGBench by applying it to multiple\npopular LLM-based agents and our results suggest that DSGBench provides\nvaluable insights in choosing LLM-based agents as well as improving their\nfuture development. DSGBench is available at\nhttps://github.com/DeciBrain-Group/DSGBench.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "43 pages, 5 figures, conference",
    "pdf_url": "http://arxiv.org/pdf/2503.06047v1",
    "published_date": "2025-03-08 04:17:23 UTC",
    "updated_date": "2025-03-08 04:17:23 UTC"
  },
  {
    "arxiv_id": "2503.07660v1",
    "title": "Research on Superalignment Should Advance Now with Parallel Optimization of Competence and Conformity",
    "authors": [
      "HyunJin Kim",
      "Xiaoyuan Yi",
      "Jing Yao",
      "Muhua Huang",
      "JinYeong Bak",
      "James Evans",
      "Xing Xie"
    ],
    "abstract": "The recent leap in AI capabilities, driven by big generative models, has\nsparked the possibility of achieving Artificial General Intelligence (AGI) and\nfurther triggered discussions on Artificial Superintelligence (ASI), a system\nsurpassing all humans across all domains. This gives rise to the critical\nresearch question of: If we realize ASI, how do we align it with human values,\nensuring it benefits rather than harms human society, a.k.a., the\nSuperalignment problem. Despite ASI being regarded by many as solely a\nhypothetical concept, in this paper, we argue that superalignment is achievable\nand research on it should advance immediately, through simultaneous and\nalternating optimization of task competence and value conformity. We posit that\nsuperalignment is not merely a safeguard for ASI but also necessary for its\nrealization. To support this position, we first provide a formal definition of\nsuperalignment rooted in the gap between capability and capacity and elaborate\non our argument. Then we review existing paradigms, explore their\ninterconnections and limitations, and illustrate a potential path to\nsuperalignment centered on two fundamental principles. We hope this work sheds\nlight on a practical approach for developing the value-aligned next-generation\nAI, garnering greater benefits and reducing potential harms for humanity.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.07660v1",
    "published_date": "2025-03-08 04:10:11 UTC",
    "updated_date": "2025-03-08 04:10:11 UTC"
  },
  {
    "arxiv_id": "2503.10657v1",
    "title": "RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs",
    "authors": [
      "Zhongzhan Huang",
      "Guoming Ling",
      "Vincent S. Liang",
      "Yupei Lin",
      "Yandong Chen",
      "Shanshan Zhong",
      "Hefeng Wu",
      "Liang Lin"
    ],
    "abstract": "Routing large language models (LLMs) is a novel paradigm that recommends the\nmost suitable LLM from a pool of candidates to process a given input through a\nwell-designed router. Our comprehensive analysis reveals a model-level\nscaling-up phenomenon in LLMs, i.e., a capable router can significantly enhance\nthe performance of this paradigm as the number of candidates increases. This\nimprovement can even easily surpass the performance of the best single model in\nthe pool and most existing strong LLMs, making it a highly promising paradigm.\nHowever, the lack of comprehensive and open-source benchmarks for Routing LLMs\nhas hindered the development of routers. In this paper, we introduce\nRouterEval, a benchmark designed specifically for router research, which\nincludes over 200,000,000 performance records for 12 popular LLM evaluations\nacross areas such as knowledge-based Q&A, commonsense reasoning, semantic\nunderstanding, mathematical reasoning, and instruction following, based on more\nthan 8,500 LLMs. Using RouterEval, extensive evaluations of existing Routing\nLLM methods reveal that most still have significant room for improvement. See\nhttps://github.com/MilkThink-Lab/RouterEval for all data, code, and tutorials.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.10657v1",
    "published_date": "2025-03-08 04:07:07 UTC",
    "updated_date": "2025-03-08 04:07:07 UTC"
  },
  {
    "arxiv_id": "2503.16485v2",
    "title": "Optimizing Generative AI's Accuracy and Transparency in Inductive Thematic Analysis: A Human-AI Comparison",
    "authors": [
      "Matthew Nyaaba",
      "Min SungEun",
      "Mary Abiswin Apam",
      "Kwame Owoahene Acheampong",
      "Emmanuel Dwamena"
    ],
    "abstract": "This study highlights the transparency and accuracy of GenAI's inductive\nthematic analysis, particularly using GPT-4 Turbo API integrated within a\nstepwise prompt-based Python script. This approach ensured a traceable and\nsystematic coding process, generating codes with supporting statements and page\nreferences, which enhanced validation and reproducibility. The results indicate\nthat GenAI performs inductive coding in a manner closely resembling human\ncoders, effectively categorizing themes at a level like the average human\ncoder. However, in interpretation, GenAI extends beyond human coders by\nsituating themes within a broader conceptual context, providing a more\ngeneralized and abstract perspective.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16485v2",
    "published_date": "2025-03-08 04:06:16 UTC",
    "updated_date": "2025-03-24 01:57:01 UTC"
  },
  {
    "arxiv_id": "2503.06037v1",
    "title": "Vairiational Stochastic Games",
    "authors": [
      "Zhiyu Zhao",
      "Haifeng Zhang"
    ],
    "abstract": "The Control as Inference (CAI) framework has successfully transformed\nsingle-agent reinforcement learning (RL) by reframing control tasks as\nprobabilistic inference problems. However, the extension of CAI to multi-agent,\ngeneral-sum stochastic games (SGs) remains underexplored, particularly in\ndecentralized settings where agents operate independently without centralized\ncoordination. In this paper, we propose a novel variational inference framework\ntailored to decentralized multi-agent systems. Our framework addresses the\nchallenges posed by non-stationarity and unaligned agent objectives, proving\nthat the resulting policies form an $\\epsilon$-Nash equilibrium. Additionally,\nwe demonstrate theoretical convergence guarantees for the proposed\ndecentralized algorithms. Leveraging this framework, we instantiate multiple\nalgorithms to solve for Nash equilibrium, mean-field Nash equilibrium, and\ncorrelated equilibrium, with rigorous theoretical convergence analysis.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06037v1",
    "published_date": "2025-03-08 03:21:23 UTC",
    "updated_date": "2025-03-08 03:21:23 UTC"
  },
  {
    "arxiv_id": "2503.06030v1",
    "title": "Towards Universal Text-driven CT Image Segmentation",
    "authors": [
      "Yuheng Li",
      "Yuxiang Lai",
      "Maria Thor",
      "Deborah Marshall",
      "Zachary Buchwald",
      "David S. Yu",
      "Xiaofeng Yang"
    ],
    "abstract": "Computed tomography (CT) is extensively used for accurate visualization and\nsegmentation of organs and lesions. While deep learning models such as\nconvolutional neural networks (CNNs) and vision transformers (ViTs) have\nsignificantly improved CT image analysis, their performance often declines when\napplied to diverse, real-world clinical data. Although foundation models offer\na broader and more adaptable solution, their potential is limited due to the\nchallenge of obtaining large-scale, voxel-level annotations for medical images.\nIn response to these challenges, prompting-based models using visual or text\nprompts have emerged. Visual-prompting methods, such as the Segment Anything\nModel (SAM), still require significant manual input and can introduce ambiguity\nwhen applied to clinical scenarios. Instead, foundation models that use text\nprompts offer a more versatile and clinically relevant approach. Notably,\ncurrent text-prompt models, such as the CLIP-Driven Universal Model, are\nlimited to text prompts already encountered during training and struggle to\nprocess the complex and diverse scenarios of real-world clinical applications.\nInstead of fine-tuning models trained from natural imaging, we propose\nOpenVocabCT, a vision-language model pretrained on large-scale 3D CT images for\nuniversal text-driven segmentation. Using the large-scale CT-RATE dataset, we\ndecompose the diagnostic reports into fine-grained, organ-level descriptions\nusing large language models for multi-granular contrastive learning. We\nevaluate our OpenVocabCT on downstream segmentation tasks across nine public\ndatasets for organ and tumor segmentation, demonstrating the superior\nperformance of our model compared to existing methods. All code, datasets, and\nmodels will be publicly released at https://github.com/ricklisz/OpenVocabCT.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.06030v1",
    "published_date": "2025-03-08 03:02:57 UTC",
    "updated_date": "2025-03-08 03:02:57 UTC"
  },
  {
    "arxiv_id": "2503.06027v2",
    "title": "Empowering Edge Intelligence: A Comprehensive Survey on On-Device AI Models",
    "authors": [
      "Xubin Wang",
      "Zhiqing Tang",
      "Jianxiong Guo",
      "Tianhui Meng",
      "Chenhao Wang",
      "Tian Wang",
      "Weijia Jia"
    ],
    "abstract": "The rapid advancement of artificial intelligence (AI) technologies has led to\nan increasing deployment of AI models on edge and terminal devices, driven by\nthe proliferation of the Internet of Things (IoT) and the need for real-time\ndata processing. This survey comprehensively explores the current state,\ntechnical challenges, and future trends of on-device AI models. We define\non-device AI models as those designed to perform local data processing and\ninference, emphasizing their characteristics such as real-time performance,\nresource constraints, and enhanced data privacy. The survey is structured\naround key themes, including the fundamental concepts of AI models, application\nscenarios across various domains, and the technical challenges faced in edge\nenvironments. We also discuss optimization and implementation strategies, such\nas data preprocessing, model compression, and hardware acceleration, which are\nessential for effective deployment. Furthermore, we examine the impact of\nemerging technologies, including edge computing and foundation models, on the\nevolution of on-device AI models. By providing a structured overview of the\nchallenges, solutions, and future directions, this survey aims to facilitate\nfurther research and application of on-device AI, ultimately contributing to\nthe advancement of intelligent systems in everyday life.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted by ACM Computing Surveys",
    "pdf_url": "http://arxiv.org/pdf/2503.06027v2",
    "published_date": "2025-03-08 02:59:51 UTC",
    "updated_date": "2025-03-17 13:37:33 UTC"
  },
  {
    "arxiv_id": "2503.06026v1",
    "title": "Zero-Shot Peg Insertion: Identifying Mating Holes and Estimating SE(2) Poses with Vision-Language Models",
    "authors": [
      "Masaru Yajima",
      "Kei Ota",
      "Asako Kanezaki",
      "Rei Kawakami"
    ],
    "abstract": "Achieving zero-shot peg insertion, where inserting an arbitrary peg into an\nunseen hole without task-specific training, remains a fundamental challenge in\nrobotics. This task demands a highly generalizable perception system capable of\ndetecting potential holes, selecting the correct mating hole from multiple\ncandidates, estimating its precise pose, and executing insertion despite\nuncertainties. While learning-based methods have been applied to peg insertion,\nthey often fail to generalize beyond the specific peg-hole pairs encountered\nduring training. Recent advancements in Vision-Language Models (VLMs) offer a\npromising alternative, leveraging large-scale datasets to enable robust\ngeneralization across diverse tasks. Inspired by their success, we introduce a\nnovel zero-shot peg insertion framework that utilizes a VLM to identify mating\nholes and estimate their poses without prior knowledge of their geometry.\nExtensive experiments demonstrate that our method achieves 90.2% accuracy,\nsignificantly outperforming baselines in identifying the correct mating hole\nacross a wide range of previously unseen peg-hole pairs, including 3D-printed\nobjects, toy puzzles, and industrial connectors. Furthermore, we validate the\neffectiveness of our approach in a real-world connector insertion task on a\nbackpanel of a PC, where our system successfully detects holes, identifies the\ncorrect mating hole, estimates its pose, and completes the insertion with a\nsuccess rate of 88.3%. These results highlight the potential of VLM-driven\nzero-shot reasoning for enabling robust and generalizable robotic assembly.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Under submission",
    "pdf_url": "http://arxiv.org/pdf/2503.06026v1",
    "published_date": "2025-03-08 02:59:21 UTC",
    "updated_date": "2025-03-08 02:59:21 UTC"
  },
  {
    "arxiv_id": "2503.06014v1",
    "title": "Towards Ambiguity-Free Spatial Foundation Model: Rethinking and Decoupling Depth Ambiguity",
    "authors": [
      "Xiaohao Xu",
      "Feng Xue",
      "Xiang Li",
      "Haowei Li",
      "Shusheng Yang",
      "Tianyi Zhang",
      "Matthew Johnson-Roberson",
      "Xiaonan Huang"
    ],
    "abstract": "Depth ambiguity is a fundamental challenge in spatial scene understanding,\nespecially in transparent scenes where single-depth estimates fail to capture\nfull 3D structure. Existing models, limited to deterministic predictions,\noverlook real-world multi-layer depth. To address this, we introduce a paradigm\nshift from single-prediction to multi-hypothesis spatial foundation models. We\nfirst present \\texttt{MD-3k}, a benchmark exposing depth biases in expert and\nfoundational models through multi-layer spatial relationship labels and new\nmetrics. To resolve depth ambiguity, we propose Laplacian Visual Prompting\n(LVP), a training-free spectral prompting technique that extracts hidden depth\nfrom pre-trained models via Laplacian-transformed RGB inputs. By integrating\nLVP-inferred depth with standard RGB-based estimates, our approach elicits\nmulti-layer depth without model retraining. Extensive experiments validate the\neffectiveness of LVP in zero-shot multi-layer depth estimation, unlocking more\nrobust and comprehensive geometry-conditioned visual generation, 3D-grounded\nspatial reasoning, and temporally consistent video-level depth inference. Our\nbenchmark and code will be available at\nhttps://github.com/Xiaohao-Xu/Ambiguity-in-Space.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "32 pages, 31 figures, github repo:\n  https://github.com/Xiaohao-Xu/Ambiguity-in-Space",
    "pdf_url": "http://arxiv.org/pdf/2503.06014v1",
    "published_date": "2025-03-08 02:33:54 UTC",
    "updated_date": "2025-03-08 02:33:54 UTC"
  },
  {
    "arxiv_id": "2503.06011v1",
    "title": "Intent-Aware Self-Correction for Mitigating Social Biases in Large Language Models",
    "authors": [
      "Panatchakorn Anantaprayoon",
      "Masahiro Kaneko",
      "Naoaki Okazaki"
    ],
    "abstract": "Self-Correction based on feedback improves the output quality of Large\nLanguage Models (LLMs). Moreover, as Self-Correction functions like the slow\nand conscious System-2 thinking from cognitive psychology's perspective, it can\npotentially reduce LLMs' social biases. LLMs are sensitive to contextual\nambiguities and inconsistencies; therefore, explicitly communicating their\nintentions during interactions when applying Self-Correction for debiasing is\ncrucial. In this study, we demonstrate that clarifying intentions is essential\nfor effectively reducing biases in LLMs through Self-Correction. We divide the\ncomponents needed for Self-Correction into three parts: instruction, response,\nand feedback, and clarify intentions at each component. We incorporate an\nexplicit debiasing prompt to convey the intention of bias mitigation from the\ninstruction for response generation. In the response, we use Chain-of-Thought\n(CoT) to clarify the reasoning process. In the feedback, we define evaluation\naspects necessary for debiasing and propose clear feedback through multi-aspect\ncritiques and scoring. Through experiments, we demonstrate that self-correcting\nCoT responses obtained from a debiasing prompt based on multi-aspect feedback\ncan reduce biased responses more robustly and consistently than the baselines.\nWe also find the variation in debiasing efficacy when using models with\ndifferent bias levels or separating models for response and feedback\ngeneration.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages. Under review",
    "pdf_url": "http://arxiv.org/pdf/2503.06011v1",
    "published_date": "2025-03-08 02:20:43 UTC",
    "updated_date": "2025-03-08 02:20:43 UTC"
  },
  {
    "arxiv_id": "2503.16484v1",
    "title": "AI-Powered Episodic Future Thinking",
    "authors": [
      "Sareh Ahmadi",
      "Michelle Rockwell",
      "Megan Stuart",
      "Allison Tegge",
      "Xuan Wang",
      "Jeffrey Stein",
      "Edward A. Fox"
    ],
    "abstract": "Episodic Future Thinking (EFT) is an intervention that involves vividly\nimagining personal future events and experiences in detail. It has shown\npromise as an intervention to reduce delay discounting - the tendency to\ndevalue delayed rewards in favor of immediate gratification - and to promote\nbehavior change in a range of maladaptive health behaviors. We present\nEFTeacher, an AI chatbot powered by the GPT-4-Turbo large language model,\ndesigned to generate EFT cues for users with lifestyle-related conditions. To\nevaluate the chatbot, we conducted a user study that included usability\nassessments and user evaluations based on content characteristics\nquestionnaires, followed by semi-structured interviews. The study provides\nqualitative insights into participants' experiences and interactions with the\nchatbot and its usability. Our findings highlight the potential application of\nAI chatbots based on Large Language Models (LLMs) in EFT interventions, and\noffer design guidelines for future behavior-oriented applications.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16484v1",
    "published_date": "2025-03-08 01:10:04 UTC",
    "updated_date": "2025-03-08 01:10:04 UTC"
  },
  {
    "arxiv_id": "2503.05997v1",
    "title": "Learning to Drive by Imitating Surrounding Vehicles",
    "authors": [
      "Yasin Sonmez",
      "Hanna Krasowski",
      "Murat Arcak"
    ],
    "abstract": "Imitation learning is a promising approach for training autonomous vehicles\n(AV) to navigate complex traffic environments by mimicking expert driver\nbehaviors. However, a major challenge in this paradigm lies in effectively\nutilizing available driving data, as collecting new data is resource-intensive\nand often limited in its ability to cover diverse driving scenarios. While\nexisting imitation learning frameworks focus on leveraging expert\ndemonstrations, they often overlook the potential of additional complex driving\ndata from surrounding traffic participants. In this paper, we propose a data\naugmentation strategy that enhances imitation learning by leveraging the\nobserved trajectories of nearby vehicles, captured through the AV's sensors, as\nadditional expert demonstrations. We introduce a vehicle selection sampling\nstrategy that prioritizes informative and diverse driving behaviors,\ncontributing to a richer and more diverse dataset for training. We evaluate our\napproach using the state-of-the-art learning-based planning method PLUTO on the\nnuPlan dataset and demonstrate that our augmentation method leads to improved\nperformance in complex driving scenarios. Specifically, our method reduces\ncollision rates and improves safety metrics compared to the baseline. Notably,\neven when using only 10% of the original dataset, our method achieves\nperformance comparable to that of the full dataset, with improved collision\nrates. Our findings highlight the importance of leveraging diverse real-world\ntrajectory data in imitation learning and provide insights into data\naugmentation strategies for autonomous driving.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05997v1",
    "published_date": "2025-03-08 00:40:47 UTC",
    "updated_date": "2025-03-08 00:40:47 UTC"
  },
  {
    "arxiv_id": "2503.05996v1",
    "title": "Towards Improving Reward Design in RL: A Reward Alignment Metric for RL Practitioners",
    "authors": [
      "Calarina Muslimani",
      "Kerrick Johnstonbaugh",
      "Suyog Chandramouli",
      "Serena Booth",
      "W. Bradley Knox",
      "Matthew E. Taylor"
    ],
    "abstract": "Reinforcement learning agents are fundamentally limited by the quality of the\nreward functions they learn from, yet reward design is often overlooked under\nthe assumption that a well-defined reward is readily available. However, in\npractice, designing rewards is difficult, and even when specified, evaluating\ntheir correctness is equally problematic: how do we know if a reward function\nis correctly specified? In our work, we address these challenges by focusing on\nreward alignment -- assessing whether a reward function accurately encodes the\npreferences of a human stakeholder. As a concrete measure of reward alignment,\nwe introduce the Trajectory Alignment Coefficient to quantify the similarity\nbetween a human stakeholder's ranking of trajectory distributions and those\ninduced by a given reward function. We show that the Trajectory Alignment\nCoefficient exhibits desirable properties, such as not requiring access to a\nground truth reward, invariance to potential-based reward shaping, and\napplicability to online RL. Additionally, in an 11 -- person user study of RL\npractitioners, we found that access to the Trajectory Alignment Coefficient\nduring reward selection led to statistically significant improvements. Compared\nto relying only on reward functions, our metric reduced cognitive workload by\n1.5x, was preferred by 82% of users and increased the success rate of selecting\nreward functions that produced performant policies by 41%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05996v1",
    "published_date": "2025-03-08 00:38:17 UTC",
    "updated_date": "2025-03-08 00:38:17 UTC"
  },
  {
    "arxiv_id": "2503.05991v1",
    "title": "GrInAdapt: Scaling Retinal Vessel Structural Map Segmentation Through Grounding, Integrating and Adapting Multi-device, Multi-site, and Multi-modal Fundus Domains",
    "authors": [
      "Zixuan Liu",
      "Aaron Honjaya",
      "Yuekai Xu",
      "Yi Zhang",
      "Hefu Pan",
      "Xin Wang",
      "Linda G Shapiro",
      "Sheng Wang",
      "Ruikang K Wang"
    ],
    "abstract": "Retinal vessel segmentation is critical for diagnosing ocular conditions, yet\ncurrent deep learning methods are limited by modality-specific challenges and\nsignificant distribution shifts across imaging devices, resolutions, and\nanatomical regions. In this paper, we propose GrInAdapt, a novel framework for\nsource-free multi-target domain adaptation that leverages multi-view images to\nrefine segmentation labels and enhance model generalizability for optical\ncoherence tomography angiography (OCTA) of the fundus of the eye. GrInAdapt\nfollows an intuitive three-step approach: (i) grounding images to a common\nanchor space via registration, (ii) integrating predictions from multiple views\nto achieve improved label consensus, and (iii) adapting the source model to\ndiverse target domains. Furthermore, GrInAdapt is flexible enough to\nincorporate auxiliary modalities such as color fundus photography, to provide\ncomplementary cues for robust vessel segmentation. Extensive experiments on a\nmulti-device, multi-site, and multi-modal retinal dataset demonstrate that\nGrInAdapt significantly outperforms existing domain adaptation methods,\nachieving higher segmentation accuracy and robustness across multiple domains.\nThese results highlight the potential of GrInAdapt to advance automated retinal\nvessel analysis and support robust clinical decision-making.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05991v1",
    "published_date": "2025-03-08 00:15:21 UTC",
    "updated_date": "2025-03-08 00:15:21 UTC"
  }
]
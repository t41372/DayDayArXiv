{
  "date": "2025-09-25",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-09-25 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä¸€å¥è¯æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv åˆæ˜¯â€œç¥ä»™æ‰“æ¶â€çš„ä¸€å¤©ï¼Œ**å¤§æ¨¡å‹æ¨ç†æœºåˆ¶**è¿æ¥é‡ç£…ç†è®ºçªç ´ï¼ˆGRPO è¢«è¯æ˜éšå«äº†è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼‰ï¼Œ**å¯è§£é‡Šæ€§**é¢†åŸŸæå‡ºäº†â€œåŸå­ç†è®ºâ€æŒ‘æˆ˜ç¥ç»å…ƒè§†è§’ï¼Œ**è…¾è®¯æ··å…ƒ**å‘å¸ƒäº†ç»Ÿä¸€ 3D ç”Ÿæˆæ¡†æ¶ï¼Œè€Œ**ç§‘å­¦æ¨ç†** Agent (Eigen-1) å†æ¬¡åˆ·æ–°äº†é«˜éš¾åº¦æ¦œå•ã€‚\n\n---\n\n### ğŸš€ å¿…è¯»ç²¾é€‰ï¼šæ¨ç†æœºåˆ¶ã€3D ç”Ÿæˆä¸å¯è§£é‡Šæ€§æ–°èŒƒå¼\n\n**1. [GRPO çš„ç†è®ºæœ¬è´¨] GRPO å®é™…ä¸Šæ˜¯ä¸€ä¸ªè¿‡ç¨‹å¥–åŠ±æ¨¡å‹**\n**# title: GRPO is Secretly a Process Reward Model**\n> **æ ¸å¿ƒçœ‹ç‚¹**ï¼šæ­ç¤ºäº†å½“å‰æœ€ç«çš„ RL ç®—æ³• GRPO çš„æ•°å­¦æœ¬è´¨ï¼Œå¹¶æå‡ºäº†æ”¹è¿›ç‰ˆ $\\lambda$-GRPOã€‚\n> **TLDR**ï¼šè¿™ç¯‡è®ºæ–‡å¯¹ç›®å‰å¤§ç«çš„ Group Relative Policy Optimization (GRPO) è¿›è¡Œäº†æ·±å…¥çš„ç†è®ºåˆ†æã€‚ä½œè€…è¯æ˜äº†åœ¨ç‰¹å®šå‡è®¾ä¸‹ï¼ˆtoken åºåˆ—é‡å ï¼‰ï¼Œ**GRPO ç®—æ³•éšå¼åœ°è¯±å¯¼äº†ä¸€ä¸ªéå¹³å‡¡çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰**ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬ä¸éœ€è¦æ˜¾å¼åœ°è®­ç»ƒæ˜‚è´µçš„ PRMï¼ŒGRPO æœ¬èº«å°±åœ¨åšè¿™ä»¶äº‹ã€‚åŸºäºæ­¤å‘ç°ï¼Œä½œè€…æŒ‡å‡ºäº† GRPO çš„ä¸€ä¸ªç¼ºé™·ï¼ˆéå‡åŒ€åˆ†å¸ƒçš„æ­¥éª¤ä¼šé˜»ç¢æ¢ç´¢å’Œåˆ©ç”¨ï¼‰ï¼Œå¹¶æå‡ºäº† $\\lambda$-GRPOï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸Šæ¯”æ ‡å‡† GRPO æ”¶æ•›æ›´å¿«ã€æ•ˆæœæ›´å¥½ã€‚\n\n**2. [æ— ç—›æ¿€æ´»å¼•å¯¼] Painless Activation Steering: An Automated, Lightweight Approach for Post-Training Large Language Models**\n**# title: Painless Activation Steering: An Automated, Lightweight Approach for Post-Training Large Language Models**\n> **æ ¸å¿ƒçœ‹ç‚¹**ï¼šä¸€ç§å…¨è‡ªåŠ¨ã€æ— éœ€äººå·¥æ„å»º Prompt çš„å¤§æ¨¡å‹è¡Œä¸ºæ“æ§æ–¹æ³•ã€‚\n> **TLDR**ï¼šä¼ ç»Ÿçš„æ¨¡å‹å¾®è°ƒï¼ˆSFT/RLï¼‰å¤ªæ…¢ï¼ŒPrompt å·¥ç¨‹å¤ªç„å­¦ã€‚Activation Steering (AS) è™½ç„¶å¥½ï¼Œä½†éœ€è¦æ‰‹å·¥è®¾è®¡ç‰¹å¾ã€‚æœ¬æ–‡æå‡ºäº† **PAS (Painless Activation Steering)**ï¼Œè¿™æ˜¯ä¸€ç§å…¨è‡ªåŠ¨çš„æ–¹æ³•ï¼Œä¸éœ€è¦æ„å»º Prompt å¯¹æˆ–ç‰¹å¾æ ‡æ³¨ï¼Œå°±èƒ½åˆ©ç”¨ç°æœ‰æ•°æ®é›†è¿›è¡Œ ASã€‚å®éªŒè¡¨æ˜ï¼ŒPAS åœ¨**è¡Œä¸ºç±»ä»»åŠ¡**ï¼ˆå¦‚åè§ã€é“å¾·ã€å¯¹é½ï¼‰ä¸Šæ•ˆæœæ˜¾è‘—ï¼ˆæå‡ 10%~34%ï¼‰ï¼Œä½†åœ¨æ™ºåŠ›ä»»åŠ¡ä¸Šæ•ˆæœä¸€èˆ¬ã€‚å®ƒè¿˜èƒ½åœ¨ SFT å’Œ ICL åŸºç¡€ä¸Šè¿›ä¸€æ­¥æå‡æ•ˆæœï¼Œæ˜¯åè®­ç»ƒé˜¶æ®µçš„é«˜æ€§ä»·æ¯”é€‰æ‹©ã€‚\n\n**3. [è…¾è®¯æ··å…ƒ 3D] Hunyuan3D-Omniï¼šç»Ÿä¸€çš„å¯æ§ 3D èµ„äº§ç”Ÿæˆæ¡†æ¶**\n**# title: Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets**\n> **æ ¸å¿ƒçœ‹ç‚¹**ï¼šè…¾è®¯æ··å…ƒå›¢é˜Ÿå‘å¸ƒçš„ 3D ç”Ÿæˆæ–°åŸºåº§ï¼Œæ”¯æŒç‚¹äº‘ã€ä½“ç´ ã€éª¨éª¼ç­‰å¤šç§æ§åˆ¶æ¡ä»¶ã€‚\n> **TLDR**ï¼šè¿™æ˜¯åŸºäº Hunyuan3D 2.1 æ„å»ºçš„ç»Ÿä¸€æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ 3D ç”Ÿæˆéš¾ä»¥ç²¾ç»†æ§åˆ¶çš„é—®é¢˜ï¼Œ**Hunyuan3D-Omni** ä¸ä»…æ¥å—å›¾åƒ/æ–‡æœ¬ï¼Œè¿˜æ”¯æŒ**ç‚¹äº‘ã€ä½“ç´ ã€Bounding Box å’Œéª¨éª¼å§¿æ€**ä½œä¸ºæ¡ä»¶è¾“å…¥ã€‚å®ƒæ²¡æœ‰ä¸ºæ¯ç§æ¨¡æ€è®¾è®¡å•ç‹¬çš„å¤´ï¼Œè€Œæ˜¯é‡‡ç”¨ç»Ÿä¸€çš„è·¨æ¨¡æ€æ¶æ„ã€‚é€šè¿‡ä¸€ç§â€œéš¾åº¦æ„ŸçŸ¥â€çš„æ¸è¿›å¼é‡‡æ ·ç­–ç•¥ï¼Œæ¨¡å‹èƒ½å¤Ÿå¹³è¡¡ä¸åŒæ§åˆ¶ä¿¡å·çš„éš¾åº¦ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆçš„å‡ ä½•å‡†ç¡®æ€§å’Œå¯æ§æ€§ã€‚\n\n**4. [å¤§æ¨¡å‹åŸå­è®º] Towards Atoms of Large Language Models**\n**# title: Towards Atoms of Large Language Models**\n> **æ ¸å¿ƒçœ‹ç‚¹**ï¼šåœ¨å¤§æ¨¡å‹å¯è§£é‡Šæ€§ä¸Šæå‡ºäº†â€œåŸå­ï¼ˆAtomsï¼‰â€æ¦‚å¿µï¼Œè¯•å›¾å–ä»£â€œç¥ç»å…ƒâ€æˆ–â€œç‰¹å¾â€ä½œä¸ºåŸºæœ¬å•å…ƒã€‚\n> **TLDR**ï¼šä¼ ç»Ÿçš„ç¥ç»å…ƒæœ‰â€œå¤šä¹‰æ€§â€é—®é¢˜ï¼Œè€Œ SAE æå–çš„â€œç‰¹å¾â€æœ‰æ—¶é‡æ„ä¸å¯é ã€‚ä½œè€…æå‡ºäº† **Atoms Theoryï¼ˆåŸå­ç†è®ºï¼‰**ï¼Œå®šä¹‰äº†æ¯”ç¥ç»å…ƒæ›´åŸºç¡€çš„å•å…ƒâ€”â€”åŸå­ã€‚é€šè¿‡å¼•å…¥åŸå­å†…ç§¯ï¼ˆAIPï¼‰æ ¡æ­£è¡¨ç¤ºåç§»ï¼Œå¹¶è¯æ˜äº†åŸå­æ»¡è¶³å—é™ç­‰è·æ€§è´¨ï¼ˆRIPï¼‰ã€‚åœ¨ Gemma å’Œ Llama ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œ**åŸå­æ¯”ç¥ç»å…ƒå’Œç‰¹å¾æ›´èƒ½å¿ å®åœ°æ•æ‰ LLM çš„å†…åœ¨è¡¨ç¤º**ï¼Œ99.8% çš„åŸå­æ»¡è¶³å”¯ä¸€æ€§æ¡ä»¶ï¼Œè€Œç¥ç»å…ƒåªæœ‰ 0.5%ã€‚\n\n---\n\n### ğŸ§  æ·±åº¦å­¦ä¹ ä¸æ¨ç† (Reasoning & Learning Dynamics)\n\n**5. [Grokking çš„æ•°å­¦è§£é‡Š] Provable Scaling Laws of Feature Emergence from Learning Dynamics of Grokking**\n**# title: Provable Scaling Laws of Feature Emergence from Learning Dynamics of Grokking**\n> **TLDR**ï¼šGrokkingï¼ˆé¡¿æ‚Ÿ/å»¶è¿Ÿæ³›åŒ–ï¼‰ç°è±¡ä¸€ç›´å¾ˆç¥ç§˜ã€‚Meta FAIR çš„å¤§ç¥ Yuandong Tian æå‡ºäº†ä¸€ä¸ªæ–°çš„æ•°å­¦æ¡†æ¶ $\\mathbf{Li}_2$ï¼Œå°† Grokking åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šæ‡’æƒ°å­¦ä¹ ã€ç‹¬ç«‹ç‰¹å¾å­¦ä¹ ã€äº¤äº’ç‰¹å¾å­¦ä¹ ã€‚æ–‡ç« è¯æ˜äº†åœ¨ç‹¬ç«‹å­¦ä¹ é˜¶æ®µï¼Œæ¢¯åº¦åŠ¨åŠ›å­¦å®é™…ä¸Šæ˜¯åœ¨æœ€å¤§åŒ–æŸç§èƒ½é‡å‡½æ•°ï¼Œè¿™å¯¼è‡´äº†ç‰¹å¾çš„æ¶Œç°ã€‚è¿™é¡¹å·¥ä½œä»ç¬¬ä¸€æ€§åŸç†æ­ç¤ºäº†ä¸ºä»€ä¹ˆ Muon ç­‰ä¼˜åŒ–å™¨æœ‰æ•ˆï¼Œå¹¶æ¨å¯¼äº†ç‰¹å¾æ¶Œç°çš„ç¼©æ”¾å®šå¾‹ã€‚\n\n**6. [æ€ç»´é“¾çš„æœ¬è´¨] Correct Reasoning Paths Visit Shared Decision Pivots**\n**# title: Correct Reasoning Paths Visit Shared Decision Pivots**\n> **TLDR**ï¼šå¦‚ä½•éªŒè¯æ€ç»´é“¾ï¼ˆCoTï¼‰çš„æ­£ç¡®æ€§ï¼Ÿä½œè€…å‘ç°äº†ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼š**æ­£ç¡®çš„æ¨ç†è·¯å¾„è™½ç„¶é£æ ¼å„å¼‚ï¼Œä½†éƒ½ä¼šç»è¿‡ç›¸åŒçš„â€œå†³ç­–æ¢çº½ï¼ˆDecision Pivotsï¼‰â€**ï¼Œè€Œé”™è¯¯çš„è·¯å¾„åˆ™ä¼šåç¦»ã€‚åŸºäºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§è‡ªè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡æŒ–æ˜è¿™äº›å…±äº«æ¢çº½æ¥å‹ç¼©æ¨ç†è·¯å¾„å¹¶è¿›è¡Œåè®­ç»ƒï¼Œåœ¨æ²¡æœ‰ Ground Truth è¿‡ç¨‹æ•°æ®çš„æƒ…å†µä¸‹ï¼Œåœ¨ LogiQA å’Œ MATH500 ä¸Šå–å¾—äº†å¾ˆå¥½çš„æ•ˆæœã€‚\n\n**7. [ICL æœºåˆ¶ç ”ç©¶] Mechanism of Task-oriented Information Removal in In-context Learning**\n**# title: Mechanism of Task-oriented Information Removal in In-context Learning**\n> **TLDR**ï¼šIn-context Learning (ICL) åˆ°åº•åœ¨å¹²ä»€ä¹ˆï¼Ÿè¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„**â€œä¿¡æ¯ç§»é™¤â€è§†è§’**ã€‚ä½œè€…å‘ç°ï¼ŒZero-shot æ—¶æ¨¡å‹å¤„äºä¸€ç§â€œéé€‰æ‹©æ€§â€çŠ¶æ€ï¼ŒåŒ…å«æ‰€æœ‰ä»»åŠ¡çš„ä¿¡æ¯ã€‚è€Œ Few-shot ICL çš„ä½œç”¨å®é™…ä¸Šæ˜¯**åˆ©ç”¨ä½ç§©æ»¤æ³¢å™¨ç§»é™¤éšè—çŠ¶æ€ä¸­ä¸ç›¸å…³çš„ä¿¡æ¯**ï¼Œä»è€Œâ€œé›•åˆ»â€å‡ºç‰¹å®šä»»åŠ¡çš„åŠŸèƒ½ã€‚ä½œè€…è¿˜å®šä½äº†æ‰§è¡Œè¿™ç§ç§»é™¤æ“ä½œçš„â€œå»å™ªæ³¨æ„åŠ›å¤´ï¼ˆDenoising Headsï¼‰â€ã€‚\n\n**8. [ç§‘å­¦æ¨ç† SOTA] Eigen-1ï¼šåŸºäºç›‘æ§å¼ RAG çš„å¤šæ™ºèƒ½ä½“ç§‘å­¦æ¨ç†**\n**# title: Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning**\n> **TLDR**ï¼šåœ¨è¶…éš¾çš„ Humanity's Last Exam (HLE) æ¦œå•ä¸Šï¼ŒEigen-1 è¾¾åˆ°äº† **48.3%** çš„å‡†ç¡®ç‡ï¼Œå¤§å¹…è¶…è¶ŠåŸºçº¿ã€‚æ ¸å¿ƒåœ¨äºè§£å†³ RAG å‰²è£‚æ¨ç†å’Œå¤šæ™ºèƒ½ä½“ç®€å•çš„å¹³å‡åŒ–é—®é¢˜ã€‚Eigen-1 ä½¿ç”¨**åŸºäºç›‘æ§çš„ RAG**ï¼ˆåœ¨ token çº§åˆ«æ³¨å…¥çŸ¥è¯†ï¼‰å’Œ**åˆ†å±‚è§£ç²¾ç‚¼ï¼ˆHSRï¼‰**ï¼Œè®©æ™ºèƒ½ä½“äº’ç›¸ä¿®å¤ç­”æ¡ˆã€‚è¿™è¯æ˜äº†éšå¼å¢å¼ºå’Œç»“æ„åŒ–ç²¾ç‚¼ä¼˜äºæ˜¾å¼çš„å·¥å…·è°ƒç”¨ã€‚\n\n---\n\n### ğŸ¤– Agent ä¸å¤šæ¨¡æ€ (Agents & Multimodal)\n\n**9. [è‡ªæˆ‘è¿›åŒ–çš„æµè§ˆå™¨ Agent] Recon-Act: A Self-Evolving Multi-Agent Browser-Use System**\n**# title: Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution**\n> **TLDR**ï¼šé’ˆå¯¹æµè§ˆå™¨æ“ä½œ Agent å®¹æ˜“ä¹±åºå’Œè¯•é”™çš„é—®é¢˜ï¼Œæå‡ºäº† Recon-Actã€‚å®ƒåŒ…å«â€œä¾¦å¯Ÿé˜Ÿâ€å’Œâ€œè¡ŒåŠ¨é˜Ÿâ€ã€‚ä¾¦å¯Ÿé˜Ÿåˆ†æå¤±è´¥è½¨è¿¹ï¼Œç”Ÿæˆé€šç”¨çš„å·¥å…·ï¼ˆä»£ç æˆ–æç¤ºè¯ï¼‰ï¼Œè¡ŒåŠ¨é˜Ÿåˆ™åˆ©ç”¨è¿™äº›å·¥å…·æ‰§è¡Œä»»åŠ¡ï¼Œå½¢æˆé—­ç¯è¿›åŒ–ã€‚åœ¨ VisualWebArena ä¸Šå–å¾—äº† SOTAã€‚\n\n**10. [VLM è§†è§‰æ¯”è¾ƒçŸ­æ¿] CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models**\n**# title: CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models**\n> **TLDR**ï¼šç°åœ¨çš„ VLM èƒ½çœ‹å›¾è¯´è¯ï¼Œä½†èƒ½â€œæ¯”è¾ƒâ€å—ï¼ŸCompareBench æµ‹è¯•äº†æ¨¡å‹åœ¨æ•°é‡ã€æ—¶é—´ã€å‡ ä½•å’Œç©ºé—´ä¸Šçš„æ¯”è¾ƒèƒ½åŠ›ã€‚ç»“æœå‘ç°ï¼Œå³ä½¿æ˜¯æœ€å¼ºçš„æ¨¡å‹ï¼ˆå¦‚ GPT-4o, Claude 3.5, Qwen2.5-VLï¼‰åœ¨**æ—¶é—´æ’åºå’Œç©ºé—´å…³ç³»**ä¸Šä¹Ÿç»å¸¸ç¿»è½¦ï¼Œç”šè‡³è¿åŸºæœ¬çš„æ•°æ•°å’Œå‡ ä½•æ¯”è¾ƒéƒ½åšä¸å¥½ã€‚è¿™æ˜¯ VLM ç›®å‰çš„ä¸€ä¸ªç³»ç»Ÿæ€§ç›²ç‚¹ã€‚\n\n**11. [è§†é¢‘ç”Ÿæˆä¸ç‰©ç†è§„å¾‹] Does FLUX Already Know How to Perform Physically Plausible Image Composition?**\n**# title: Does FLUX Already Know How to Perform Physically Plausible Image Composition?**\n> **TLDR**ï¼šç°æœ‰çš„å›¾åƒåˆæˆï¼ˆæŠŠç‰©ä½“ P åˆ°èƒŒæ™¯é‡Œï¼‰å¾ˆéš¾å¤„ç†å…‰ç…§å’Œé˜´å½±ã€‚ä½œè€…å‘ç° FLUX ç­‰ç°ä»£æ‰©æ•£æ¨¡å‹å…¶å®å·²ç»éšå«äº†ç‰©ç†å…ˆéªŒã€‚æå‡ºäº† **SHINE** æ¡†æ¶ï¼Œæ— éœ€è®­ç»ƒå³å¯åˆ©ç”¨è¿™äº›å…ˆéªŒè¿›è¡Œé«˜ä¿çœŸçš„å›¾åƒåˆæˆï¼Œè§£å†³äº†å…‰ç…§ã€é˜´å½±å’Œåå°„é—®é¢˜ã€‚\n\n---\n\n### ğŸ’¡ å…¶ä»–å€¼å¾—å…³æ³¨çš„è®ºæ–‡ (Quick Hits)\n\n*   **[å®‰å…¨] Limitations on Safe, Trusted, Artificial General Intelligence** (#9)\n    *   **# title: Limitations on Safe, Trusted, Artificial General Intelligence**\n    *   ä»æ•°å­¦ä¸Šè¯æ˜äº†â€œå®‰å…¨â€ã€â€œå¯ä¿¡â€å’Œâ€œAGIâ€ä¸‰è€…ä¹‹é—´å­˜åœ¨æ ¹æœ¬çš„ä¸å…¼å®¹æ€§ã€‚ç±»ä¼¼äºå“¥å¾·å°”ä¸å®Œå¤‡å®šç†ï¼Œè¯æ˜äº†ä¸å­˜åœ¨æ—¢ç»å¯¹å®‰å…¨å¯ä¿¡åˆèƒ½è§£å†³æ‰€æœ‰äººç±»å¯è§£é—®é¢˜çš„ AGIã€‚\n\n*   **[AI ç‰©ç†æ„ŸçŸ¥] Can AI Perceive Physical Danger and Intervene?** (#11)\n    *   **# title: Can AI Perceive Physical Danger and Intervene?**\n    *   å‘å¸ƒäº† **Asimov Benchmark v2**ï¼Œæµ‹è¯•æ¨¡å‹æ˜¯å¦çŸ¥é“â€œç®±å­å¤ªé‡æ¬ä¸åŠ¨â€æˆ–â€œçƒ­å’–å•¡ä¸èƒ½é€’ç»™å­©å­â€ç­‰ç‰©ç†å®‰å…¨å¸¸è¯†ã€‚ç›®å‰çš„æ¨¡å‹åœ¨è¿™æ–¹é¢ä»æœ‰æ¬ ç¼ºã€‚\n\n*   **[æ­£åˆ™åŒ–] DIM: Enforcing Domain-Informed Monotonicity in Deep Neural Networks** (#6)\n    *   **# title: DIM: Enforcing Domain-Informed Monotonicity in Deep Neural Networks**\n    *   æå‡ºäº†ä¸€ç§æ–°çš„æ­£åˆ™åŒ–æ–¹æ³• DIMï¼Œå¼ºåˆ¶ç¥ç»ç½‘ç»œéµå®ˆé¢†åŸŸå†…çš„â€œå•è°ƒæ€§â€çº¦æŸï¼ˆä¾‹å¦‚ï¼šä»·æ ¼è¶Šé«˜é”€é‡è¶Šä½ï¼‰ï¼Œæœ‰æ•ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæé«˜å¯è§£é‡Šæ€§ã€‚\n\n*   **[æ•°æ®åº“ Agent] QueryGym: Step-by-Step Interaction with Relational Databases** (#3)\n    *   **# title: QueryGym: Step-by-Step Interaction with Relational Databases**\n    *   IBM æ¨å‡ºçš„ä¸€ä¸ª Gym ç¯å¢ƒï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼° LLM æ„å»º SQL æŸ¥è¯¢è§„åˆ’çš„èƒ½åŠ›ï¼Œå¼ºè°ƒåˆ†æ­¥äº¤äº’å’Œå…³ç³»ä»£æ•°æ“ä½œï¼Œè€Œä¸æ˜¯ç›´æ¥ç”Ÿæˆ SQLã€‚\n\n*   **[åŒ»ç–— RAG] An Automated Retrieval-Augmented Generation LLaMA-4 109B-based System** (#200)\n    *   **# title: An Automated Retrieval-Augmented Generation LLaMA-4 109B-based System for Evaluating Radiotherapy Treatment Plans**\n    *   **LLaMA-4 109B** ç°èº«ï¼Ÿè¿™ç¯‡è®ºæ–‡æ ‡é¢˜æåˆ°äº† LLaMA-4 109Bï¼Œç”¨äºæ”¾å°„æ²»ç–—è®¡åˆ’çš„è¯„ä¼°ç³»ç»Ÿã€‚ç³»ç»Ÿç»“åˆäº† RAG å’Œå·¥å…·è°ƒç”¨ï¼Œå®ç°äº† 100% çš„åè®®æ£€æŸ¥ä¸€è‡´æ€§ã€‚*(æ³¨ï¼šéœ€å…³æ³¨ LLaMA-4 æ˜¯å¦ä¸ºç¬”è€…ç¬”è¯¯æˆ–æ—©æœŸæ³„éœ²å‹å·ï¼Œæ–‡ä¸­æè¿°å€¼å¾—ç©å‘³)*\n\n---\n**ç»“è¯­ï¼š**\nä»Šå¤©çš„è®ºæ–‡åœ¨**å¤§æ¨¡å‹çš„åº•å±‚æœºç†**ä¸ŠæŒ–å¾—å¾ˆæ·±ï¼ˆGRPO æœ¬è´¨ã€ICL æœºåˆ¶ã€åŸå­è®ºï¼‰ï¼Œè¯´æ˜å­¦ç•Œæ­£åœ¨è¯•å›¾æ‰“å¼€ LLM çš„é»‘ç›’ï¼Œä»â€œç‚¼ä¸¹â€èµ°å‘â€œåŒ–å­¦â€ã€‚åŒæ—¶ï¼ŒAgent é¢†åŸŸå¼€å§‹æ³¨é‡**è‡ªæˆ‘è¿›åŒ–**å’Œ**ç§‘å­¦é¢†åŸŸçš„ç¡¬æ ¸åº”ç”¨**ã€‚å¸Œæœ›ä»Šå¤©çš„å¿«æŠ¥èƒ½ç»™ä½ å¸¦æ¥çµæ„Ÿï¼",
  "papers": [
    {
      "arxiv_id": "2509.22739v2",
      "title": "Painless Activation Steering: An Automated, Lightweight Approach for Post-Training Large Language Models",
      "title_zh": "Painless Activation Steeringï¼šä¸€ç§é¢å‘å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒåçš„è‡ªåŠ¨åŒ–ã€è½»é‡åŒ–æ–¹æ³•",
      "authors": [
        "Sasha Cui",
        "Zhongren Chen"
      ],
      "abstract": "Language models (LMs) are typically post-trained for desired capabilities and behaviors via weight-based or prompt-based steering, but the former is time-consuming and expensive, and the latter is not precisely controllable and often requires manual trial-and-error. While activation steering (AS) promises a cheap, fast, and controllable alternative to the two existing post-training methods, current AS techniques require hand-crafted prompt pairs or labor-intensive feature annotation, making them more inconvenient than the plug-and-play methods such as Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT). We introduce Painless Activation Steering (PAS), a family of fully automated methods that make AS readily usable with any given labeled dataset, with no need for prompt construction, feature labeling, or human intervention. We evaluate PAS on three open-weight models (Llama3.1-8B-Instruct, DeepSeek-R1-Distill-8B, and Nous-Hermes-2) and 18 tasks; we find that PAS reliably improves performance for behavior tasks, but not for intelligence-oriented tasks. The introspective variant (iPAS) delivers the strongest causal steering effects (10.1% on Bias, 5.2% on Morality, and 34.8% on Alignment). We also show PAS delivers additional gains on top of In-Context Learning (ICL) and SFT. PAS constructs a fast, lightweight activation vector that can be cheaply trained, easily stored, and activated at will. Our results provide a characterization of where AS helps, where it fails, and how to deploy it as a practical, automated LM post-training option.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Painless Activation Steering (PAS)ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨ä¸”è½»é‡çº§çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä»»ä½•ç»™å®šçš„æ ‡è®°æ•°æ®é›†å®ç°å¯¹å¤§è¯­è¨€æ¨¡å‹ (Large Language Models) çš„åè®­ç»ƒå¼•å¯¼ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæƒé‡ (weight-based) æˆ–æç¤º (prompt-based) çš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒPAS è§£å†³äº†ç°æœ‰ Activation Steering æŠ€æœ¯éœ€è¦æ‰‹åŠ¨æ„å»ºæç¤ºå¯¹æˆ–ç¹çç‰¹å¾æ ‡æ³¨çš„ç—›ç‚¹ï¼Œå®ç°äº†æ— éœ€äººå·¥å¹²é¢„çš„è‡ªåŠ¨åŒ–æµç¨‹ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºä¸€ä¸ªå¯å¿«é€Ÿè®­ç»ƒã€æ˜“äºå­˜å‚¨ä¸”èƒ½éšæ—¶æ¿€æ´»çš„æ¿€æ´»å‘é‡ï¼Œä¸ºæ¨¡å‹ä¼˜åŒ–æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”ä½æˆæœ¬çš„æ–¹æ¡ˆã€‚å®éªŒåœ¨ Llama3.1-8B-Instruct ç­‰å¼€æºæ¨¡å‹å’Œ 18 é¡¹ä»»åŠ¡ä¸Šå±•å¼€ï¼Œç»“æœæ˜¾ç¤º PAS åœ¨è¡Œä¸ºç±»ä»»åŠ¡ä¸­æ€§èƒ½æå‡æ˜¾è‘—ï¼Œå…¶ä¸­å†…çœå˜ä½“ (iPAS) åœ¨ Alignment ä»»åŠ¡ä¸­å–å¾—äº† 34.8% çš„æå‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯æ˜ PAS èƒ½å¤Ÿä¸ In-Context Learning (ICL) åŠ Supervised Fine-Tuning (SFT) å åŠ ä½¿ç”¨ä»¥è·å¾—é¢å¤–å¢ç›Šã€‚è¿™é¡¹å·¥ä½œä¸ä»…ç•Œå®šäº† Activation Steering çš„é€‚ç”¨èŒƒå›´ï¼Œä¹Ÿä¸ºå¤§æ¨¡å‹æä¾›äº†ä¸€ä¸ªå®ç”¨ä¸”è‡ªåŠ¨åŒ–çš„åè®­ç»ƒé€‰é¡¹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.22739v2",
      "published_date": "2025-09-25 23:25:47 UTC",
      "updated_date": "2025-09-30 00:27:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:06:31.468814+00:00"
    },
    {
      "arxiv_id": "2510.03247v1",
      "title": "Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data",
      "title_zh": "è¿ˆå‘å¤šæ¨¡æ€ä¸»åŠ¨å­¦ä¹ ï¼šæœ‰é™é…å¯¹æ•°æ®ä¸‹çš„é«˜æ•ˆå­¦ä¹ ",
      "authors": [
        "Jiancheng Zhang",
        "Yinglun Zhu"
      ],
      "abstract": "Active learning (AL) is a principled strategy to reduce annotation cost in data-hungry deep learning. However, existing AL algorithms focus almost exclusively on unimodal data, overlooking the substantial annotation burden in multimodal learning. We introduce the first framework for multimodal active learning with unaligned data, where the learner must actively acquire cross-modal alignments rather than labels on pre-aligned pairs. This setting captures the practical bottleneck in modern multimodal pipelines such as CLIP and SigLIP, where unimodal features are easy to obtain but high-quality alignment is costly. We develop a new algorithm that combines uncertainty and diversity principles in a modality-aware design, achieves linear-time acquisition, and applies seamlessly to both pool-based and streaming-based settings. Extensive experiments on benchmark datasets demonstrate that our approach consistently reduces multimodal annotation cost while preserving performance; for instance, on the ColorSwap dataset it cuts annotation requirements by up to $40\\%$ without loss in accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ·±åº¦å­¦ä¹ ä¸­å‡å°‘æ ‡æ³¨æˆæœ¬çš„Active Learning (AL)ç­–ç•¥ï¼Œå¹¶é’ˆå¯¹å¤šæ¨¡æ€é¢†åŸŸæå‡ºäº†é¦–ä¸ªå¤„ç†éå¯¹é½æ•°æ®(unaligned data)çš„Multimodal Active Learningæ¡†æ¶ã€‚é’ˆå¯¹CLIPå’ŒSigLIPç­‰ç°ä»£å¤šæ¨¡æ€æµæ°´çº¿ä¸­å¯¹é½æ•°æ®æˆæœ¬é«˜æ˜‚çš„ç°çŠ¶ï¼Œè¯¥æ¡†æ¶å…è®¸å­¦ä¹ è€…ä¸»åŠ¨è·å–è·¨æ¨¡æ€å¯¹é½(cross-modal alignments)è€Œéå¯¹é¢„å…ˆå¯¹é½çš„æ ·æœ¬è¿›è¡Œæ ‡æ³¨ã€‚æ‰€æå‡ºçš„æ–°ç®—æ³•é‡‡ç”¨äº†ç»“åˆä¸ç¡®å®šæ€§(uncertainty)å’Œå¤šæ ·æ€§(diversity)åŸåˆ™çš„æ¨¡æ€æ„ŸçŸ¥è®¾è®¡(modality-aware design)ï¼Œå¹¶å®ç°äº†çº¿æ€§æ—¶é—´è·å–ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ— ç¼åº”ç”¨äºåŸºäºæ± åŒ–(pool-based)å’Œæµå¼(streaming-based)çš„è®¾ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½äº†å¤šæ¨¡æ€æ ‡æ³¨æˆæœ¬ï¼Œåœ¨ColorSwapæ•°æ®é›†ä¸Šå¯ä»¥åœ¨ä¸æŸå¤±å‡†ç¡®ç‡çš„æƒ…å†µä¸‹å‡å°‘é«˜è¾¾40%çš„æ ‡æ³¨éœ€æ±‚ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03247v1",
      "published_date": "2025-09-25 23:08:03 UTC",
      "updated_date": "2025-09-25 23:08:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:06:32.165233+00:00"
    },
    {
      "arxiv_id": "2509.21674v2",
      "title": "QueryGym: Step-by-Step Interaction with Relational Databases",
      "title_zh": "QueryGymï¼šä¸å…³ç³»æ•°æ®åº“çš„æ­¥è¿›å¼äº¤äº’",
      "authors": [
        "Haritha Ananthakrishnan",
        "Harsha Kokel",
        "Kelsey Sikes",
        "Debarun Bhattacharjya",
        "Michael Katz",
        "Shirin Sohrabi",
        "Kavitha Srinivas"
      ],
      "abstract": "We introduce QueryGym, an interactive environment for building, testing, and evaluating LLM-based query planning agents. Existing frameworks often tie agents to specific query language dialects or obscure their reasoning; QueryGym instead requires agents to construct explicit sequences of relational algebra operations, ensuring engine-agnostic evaluation and transparent step-by-step planning. The environment is implemented as a Gymnasium interface that supplies observations -- including schema details, intermediate results, and execution feedback -- and receives actions that represent database exploration (e.g., previewing tables, sampling column values, retrieving unique values) as well as relational algebra operations (e.g., filter, project, join). We detail the motivation and the design of the environment. In the demo, we showcase the utility of the environment by contrasting it with contemporary LLMs that query databases. QueryGym serves as a practical testbed for research in error remediation, transparency, and reinforcement learning for query generation. For the associated demo, see https://ibm.biz/QueryGym.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† QueryGymï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºã€æµ‹è¯•å’Œè¯„ä¼°åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŸ¥è¯¢è§„åˆ’æ™ºèƒ½ä½“çš„äº¤äº’å¼ç¯å¢ƒã€‚é’ˆå¯¹ç°æœ‰æ¡†æ¶é€šå¸¸å°†æ™ºèƒ½ä½“ä¸ç‰¹å®šæŸ¥è¯¢è¯­è¨€æ–¹è¨€ç»‘å®šä¸”æ¨ç†è¿‡ç¨‹ä¸é€æ˜çš„é—®é¢˜ï¼ŒQueryGym è¦æ±‚æ™ºèƒ½ä½“æ„å»ºæ˜¾å¼çš„å…³ç³»ä»£æ•°ï¼ˆrelational algebraï¼‰æ“ä½œåºåˆ—ï¼Œä»è€Œç¡®ä¿äº†ä¸å¼•æ“æ— å…³çš„è¯„ä¼°å’Œé€æ˜çš„åˆ†æ­¥è§„åˆ’ã€‚è¯¥ç¯å¢ƒä»¥ Gymnasium æ¥å£å½¢å¼å®ç°ï¼Œæä¾›åŒ…å«æ¨¡å¼ç»†èŠ‚ï¼ˆschema detailsï¼‰ã€ä¸­é—´ç»“æœå’Œæ‰§è¡Œåé¦ˆçš„è§‚æµ‹ä¿¡æ¯ï¼Œå¹¶æ”¯æŒæ•°æ®åº“æ¢ç´¢åŠ filterã€projectã€join ç­‰å…³ç³»ä»£æ•°æ“ä½œåŠ¨ä½œã€‚å®éªŒé€šè¿‡ä¸å½“ä»£å¤§è¯­è¨€æ¨¡å‹çš„å¯¹æ¯”ï¼Œå±•ç¤ºäº†è¯¥ç¯å¢ƒåœ¨å¤„ç†æ•°æ®åº“æŸ¥è¯¢ä»»åŠ¡æ—¶çš„å®ç”¨æ€§ã€‚QueryGym ä¸ºé”™è¯¯ä¿®å¤ã€ç³»ç»Ÿé€æ˜åº¦ä»¥åŠç”¨äºæŸ¥è¯¢ç”Ÿæˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆreinforcement learningï¼‰ç ”ç©¶æä¾›äº†ä¸€ä¸ªå…³é”®çš„å®éªŒå¹³å°ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21674v2",
      "published_date": "2025-09-25 22:48:49 UTC",
      "updated_date": "2026-01-09 17:48:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:06:31.952948+00:00"
    },
    {
      "arxiv_id": "2509.21673v1",
      "title": "SlotFM: A Motion Foundation Model with Slot Attention for Diverse Downstream Tasks",
      "title_zh": "SlotFMï¼šåŸºäºæ’æ§½æ³¨æ„åŠ›æœºåˆ¶çš„é¢å‘å¤šæ ·åŒ–ä¸‹æ¸¸ä»»åŠ¡çš„è¿åŠ¨åŸºåº§æ¨¡å‹",
      "authors": [
        "Junyong Park",
        "Oron Levy",
        "Rebecca Adaimi",
        "Asaf Liberman",
        "Gierad Laput",
        "Abdelkareem Bedri"
      ],
      "abstract": "Wearable accelerometers are used for a wide range of applications, such as gesture recognition, gait analysis, and sports monitoring. Yet most existing foundation models focus primarily on classifying common daily activities such as locomotion and exercise, limiting their applicability to the broader range of tasks that rely on other signal characteristics. We present SlotFM, an accelerometer foundation model that generalizes across diverse downstream tasks. SlotFM uses Time-Frequency Slot Attention, an extension of Slot Attention that processes both time and frequency representations of the raw signals. It generates multiple small embeddings (slots), each capturing different signal components, enabling task-specific heads to focus on the most relevant parts of the data. We also introduce two loss regularizers that capture local structure and frequency patterns, which improve reconstruction of fine-grained details and helps the embeddings preserve task-relevant information. We evaluate SlotFM on 16 classification and regression downstream tasks that extend beyond standard human activity recognition. It outperforms existing self-supervised approaches on 13 of these tasks and achieves comparable results to the best performing approaches on the remaining tasks. On average, our method yields a 4.5% performance gain, demonstrating strong generalization for sensing foundation models.",
      "tldr_zh": "é’ˆå¯¹ç©¿æˆ´å¼åŠ é€Ÿåº¦è®¡ç°æœ‰åŸºç¡€æ¨¡å‹ä¸»è¦å±€é™äºæ—¥å¸¸æ´»åŠ¨åˆ†ç±»ã€éš¾ä»¥é€‚åº”æ›´å¹¿æ³›ä¿¡å·ç‰¹å¾ä»»åŠ¡çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†SlotFMï¼Œä¸€ç§å…·æœ‰å¼ºæ³›åŒ–èƒ½åŠ›çš„è¿åŠ¨åŸºç¡€æ¨¡å‹(Motion Foundation Model)ã€‚SlotFMé‡‡ç”¨äº†Time-Frequency Slot Attentionæœºåˆ¶ï¼Œé€šè¿‡åŒæ—¶å¤„ç†åŸå§‹ä¿¡å·çš„æ—¶é—´å’Œé¢‘ç‡è¡¨ç¤ºæ¥ç”Ÿæˆæ•æ‰ä¸åŒä¿¡å·ç»„ä»¶çš„å¤šä¸ªå°å‹åµŒå…¥(slots)ï¼Œä»è€Œå…è®¸ä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹èšç„¦äºæœ€ç›¸å…³çš„æ•°æ®éƒ¨åˆ†ã€‚ä¸ºäº†æå‡å¯¹ç»†ç²’åº¦ç»†èŠ‚çš„é‡å»ºæ•ˆæœï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸¤ç§é’ˆå¯¹å±€éƒ¨ç»“æ„å’Œé¢‘ç‡æ¨¡å¼çš„æŸå¤±æ­£åˆ™åŒ–é¡¹(Loss Regularizers)ï¼Œç¡®ä¿åµŒå…¥ä¿¡æ¯èƒ½æœ‰æ•ˆä¿ç•™ä»»åŠ¡ç›¸å…³ç‰¹å¾ã€‚å®éªŒåœ¨16é¡¹åˆ†ç±»å’Œå›å½’ä¸‹æ¸¸ä»»åŠ¡ä¸­å¯¹SlotFMè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜å…¶åœ¨13é¡¹ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚SlotFMæœ€ç»ˆå®ç°äº†å¹³å‡4.5%çš„æ€§èƒ½æå‡ï¼Œå……åˆ†å±•ç¤ºäº†å…¶åœ¨å¤šæ ·åŒ–æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„æ³›åŒ–ä¼˜åŠ¿å’Œåº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21673v1",
      "published_date": "2025-09-25 22:41:43 UTC",
      "updated_date": "2025-09-25 22:41:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:06:34.386200+00:00"
    },
    {
      "arxiv_id": "2509.21670v3",
      "title": "MORPH: PDE Foundation Models with Arbitrary Data Modality",
      "title_zh": "MORPHï¼šæ”¯æŒä»»æ„æ•°æ®æ¨¡æ€çš„åå¾®åˆ†æ–¹ç¨‹åŸºç¡€æ¨¡å‹",
      "authors": [
        "Mahindra Singh Rautela",
        "Alexander Most",
        "Siddharth Mansingh",
        "Bradley C. Love",
        "Ayan Biswas",
        "Diane Oyen",
        "Earl Lawrence"
      ],
      "abstract": "We introduce MORPH, a modality-agnostic, autoregressive foundation model for partial differential equations (PDEs). MORPH is built on a convolutional vision transformer backbone that seamlessly handles heterogeneous spatiotemporal datasets of varying data modality (1D--3D) at different resolutions, and multiple fields with mixed scalar and vector components. The architecture combines (i) component-wise convolution, which jointly processes scalar and vector channels to capture local interactions, (ii) inter-field cross-attention, which models and selectively propagates information between different physical fields, (iii) axial attentions, which factorize full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity. We pretrain multiple model variants on a diverse collection of heterogeneous PDE datasets and evaluate transfer to a range of downstream prediction tasks. Using both full-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPH outperforms models trained from scratch. Across extensive evaluations, MORPH matches or surpasses strong baselines and recent state-of-the-art models. Collectively, these capabilities present a flexible and powerful backbone for learning from the heterogeneous and multimodal nature of scientific observations, charting a path toward scalable and data-efficient scientific machine learning. The source code, datasets, and models are publicly available at https://github.com/lanl/MORPH.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MORPHï¼Œä¸€ç§é’ˆå¯¹åå¾®åˆ†æ–¹ç¨‹ (PDEs) çš„æ¨¡æ€æ— å…³è‡ªå›å½’åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨å¤„ç†å…·æœ‰ä¸åŒæ•°æ®æ¨¡æ€ (1D-3D)ã€ä¸åŒåˆ†è¾¨ç‡ä»¥åŠæ··åˆæ ‡é‡å’Œå‘é‡åˆ†é‡çš„å¼‚æ„æ—¶ç©ºæ•°æ®é›†ã€‚è¯¥æ¨¡å‹æ„å»ºåœ¨å·ç§¯è§†è§‰å˜æ¢å™¨ (Convolutional Vision Transformer) éª¨å¹²ç½‘ç»œä¹‹ä¸Šï¼Œé‡‡ç”¨äº†ç»„ä»¶å·ç§¯ (component-wise convolution) æ¥è”åˆå¤„ç†é€šé“é—´çš„å±€éƒ¨äº¤äº’ã€‚ä¸ºäº†æé«˜è¡¨è¾¾èƒ½åŠ›å¹¶å‡è½»è®¡ç®—è´Ÿæ‹…ï¼ŒMORPH ç»“åˆäº†ç”¨äºå»ºæ¨¡ç‰©ç†åœºé—´ä¿¡æ¯çš„åœºé—´äº¤å‰æ³¨æ„åŠ› (inter-field cross-attention) å’Œæ²¿æ—¶ç©ºè½´åˆ†è§£çš„è½´å‘æ³¨æ„åŠ› (axial attentions)ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨å¤šæ ·åŒ–çš„å¼‚æ„ PDE æ•°æ®é›†ä¸Šé¢„è®­ç»ƒäº†å¤šä¸ªæ¨¡å‹å˜ä½“ï¼Œå¹¶éªŒè¯äº†å…¶åœ¨ä¸‹æ¸¸é¢„æµ‹ä»»åŠ¡ä¸­çš„è¿ç§»èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯é€šè¿‡å…¨æ¨¡å‹å¾®è°ƒè¿˜æ˜¯å‚æ•°é«˜æ•ˆçš„ä½ç§©é€‚é…å™¨ (LoRA)ï¼ŒMORPH çš„è¡¨ç°å‡ä¼˜äºä»å¤´å¼€å§‹è®­ç»ƒçš„æ¨¡å‹ï¼Œå¹¶è¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›åŸºå‡†ã€‚MORPH ä¸ºå¤„ç†ç§‘å­¦è§‚æµ‹ä¸­çš„å¤šæ¨¡æ€ç‰¹æ€§æä¾›äº†ä¸€ä¸ªçµæ´»ä¸”å¼ºå¤§çš„éª¨å¹²æ¶æ„ï¼Œä¸ºå®ç°å¯æ‰©å±•ä¸”æ•°æ®é«˜æ•ˆçš„ç§‘å­¦æœºå™¨å­¦ä¹  (Scientific Machine Learning) å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "physics.comp-ph"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21670v3",
      "published_date": "2025-09-25 22:38:36 UTC",
      "updated_date": "2025-12-04 18:36:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:06:35.790192+00:00"
    },
    {
      "arxiv_id": "2509.21666v1",
      "title": "DIM: Enforcing Domain-Informed Monotonicity in Deep Neural Networks",
      "title_zh": "DIMï¼šåœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­æ–½åŠ é¢†åŸŸæ„ŸçŸ¥çš„å•è°ƒæ€§çº¦æŸ",
      "authors": [
        "Joshua Salim",
        "Jordan Yu",
        "Xilei Zhao"
      ],
      "abstract": "While deep learning models excel at predictive tasks, they often overfit due to their complex structure and large number of parameters, causing them to memorize training data, including noise, rather than learn patterns that generalize to new data. To tackle this challenge, this paper proposes a new regularization method, i.e., Enforcing Domain-Informed Monotonicity in Deep Neural Networks (DIM), which maintains domain-informed monotonic relationships in complex deep learning models to further improve predictions. Specifically, our method enforces monotonicity by penalizing violations relative to a linear baseline, effectively encouraging the model to follow expected trends while preserving its predictive power. We formalize this approach through a comprehensive mathematical framework that establishes a linear reference, measures deviations from monotonic behavior, and integrates these measurements into the training objective. We test and validate the proposed methodology using a real-world ridesourcing dataset from Chicago and a synthetically created dataset. Experiments across various neural network architectures show that even modest monotonicity constraints consistently enhance model performance. DIM enhances the predictive performance of deep neural networks by applying domain-informed monotonicity constraints to regularize model behavior and mitigate overfitting",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DIM (Enforcing Domain-Informed Monotonicity in Deep Neural Networks) æ­£åˆ™åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ·±åº¦å­¦ä¹ æ¨¡å‹å› ç»“æ„å¤æ‚å’Œå‚æ•°ä¼—å¤šå¯¼è‡´çš„è¿‡æ‹Ÿåˆ(overfitting)åŠæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨å¤æ‚æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­å¼ºåˆ¶æ‰§è¡Œé¢†åŸŸå‘ŠçŸ¥çš„å•è°ƒæ€§(domain-informed monotonicity)å…³ç³»æ¥ä¼˜åŒ–é¢„æµ‹è¡¨ç°ã€‚å…·ä½“è€Œè¨€ï¼ŒDIMé€šè¿‡å¯¹ç›¸å¯¹äºçº¿æ€§åŸºå‡†(linear baseline)çš„è¿ä¾‹è¡Œä¸ºè¿›è¡Œæƒ©ç½šæ¥å®æ–½å•è°ƒæ€§çº¦æŸï¼Œä¿ƒä½¿æ¨¡å‹åœ¨ä¿æŒé¢„æµ‹èƒ½åŠ›çš„åŒæ—¶éµå¾ªé¢„æœŸçš„è¶‹åŠ¿ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å»ºç«‹ä¸€å¥—ç»¼åˆæ•°å­¦æ¡†æ¶ï¼Œå°†çº¿æ€§å‚è€ƒç¡®ç«‹å’Œå•è°ƒåç¦»è¡¡é‡é›†æˆåˆ°è®­ç»ƒç›®æ ‡ä¸­ã€‚å®éªŒåœ¨èŠåŠ å“¥çœŸå®çº¦è½¦æ•°æ®é›†å’Œåˆæˆæ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯é€‚åº¦çš„å•è°ƒæ€§çº¦æŸä¹Ÿèƒ½åœ¨å¤šç§ç¥ç»ç½‘ç»œæ¶æ„ä¸­æŒç»­æå‡æ¨¡å‹æ€§èƒ½ï¼Œæœ‰æ•ˆç¼“è§£äº†è¿‡æ‹Ÿåˆç°è±¡å¹¶å¢å¼ºäº†æ·±åº¦ç¥ç»ç½‘ç»œçš„é¢„æµ‹è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21666v1",
      "published_date": "2025-09-25 22:35:57 UTC",
      "updated_date": "2025-09-25 22:35:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:06:50.055766+00:00"
    },
    {
      "arxiv_id": "2509.21663v1",
      "title": "Logic of Hypotheses: from Zero to Full Knowledge in Neurosymbolic Integration",
      "title_zh": "å‡è®¾é€»è¾‘ï¼šç¥ç»ç¬¦å·é›†æˆä¸­ä»é›¶åˆ°å…¨çŸ¥è¯†çš„å®ç°",
      "authors": [
        "Davide Bizzaro",
        "Alessandro Daniele"
      ],
      "abstract": "Neurosymbolic integration (NeSy) blends neural-network learning with symbolic reasoning. The field can be split between methods injecting hand-crafted rules into neural models, and methods inducing symbolic rules from data. We introduce Logic of Hypotheses (LoH), a novel language that unifies these strands, enabling the flexible integration of data-driven rule learning with symbolic priors and expert knowledge. LoH extends propositional logic syntax with a choice operator, which has learnable parameters and selects a subformula from a pool of options. Using fuzzy logic, formulas in LoH can be directly compiled into a differentiable computational graph, so the optimal choices can be learned via backpropagation. This framework subsumes some existing NeSy models, while adding the possibility of arbitrary degrees of knowledge specification. Moreover, the use of Goedel fuzzy logic and the recently developed Goedel trick yields models that can be discretized to hard Boolean-valued functions without any loss in performance. We provide experimental analysis on such models, showing strong results on tabular data and on the Visual Tic-Tac-Toe NeSy task, while producing interpretable decision rules.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†Logic of Hypotheses (LoH)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ç»Ÿä¸€è§„åˆ™æ³¨å…¥ä¸è§„åˆ™å½’çº³çš„ç¥ç»ç¬¦å·é›†æˆ(Neurosymbolic integration)æ–°è¯­è¨€ã€‚LoHé€šè¿‡åœ¨å‘½é¢˜é€»è¾‘è¯­æ³•ä¸­å¼•å…¥å¸¦æœ‰å¯å­¦ä¹ å‚æ•°çš„é€‰æ‹©ç®—å­(choice operator)ï¼Œå®ç°äº†æ•°æ®é©±åŠ¨çš„è§„åˆ™å­¦ä¹ ä¸ç¬¦å·å…ˆéªŒåŠä¸“å®¶çŸ¥è¯†çš„çµæ´»èåˆã€‚åˆ©ç”¨æ¨¡ç³Šé€»è¾‘(fuzzy logic)ï¼ŒLoHå…¬å¼å¯ä»¥ç›´æ¥ç¼–è¯‘ä¸ºå¯å¾®è®¡ç®—å›¾ï¼Œä»è€Œæ”¯æŒé€šè¿‡åå‘ä¼ æ’­(backpropagation)ä¼˜åŒ–æ¨¡å‹é€‰æ‹©ã€‚è¯¥æ¡†æ¶ä¸ä»…æ¶µç›–äº†ç°æœ‰çš„ç¥ç»ç¬¦å·é›†æˆæ¨¡å‹ï¼Œè¿˜å…è®¸ç”¨æˆ·è¿›è¡Œä»»æ„ç¨‹åº¦çš„çŸ¥è¯†è§„èŒƒã€‚é€šè¿‡ç»“åˆGoedelæ¨¡ç³Šé€»è¾‘ä¸Goedel trickï¼ŒLoHèƒ½å¤Ÿå°†æ¨¡å‹ç¦»æ•£åŒ–ä¸ºç¡¬å¸ƒå°”å€¼å‡½æ•°è€Œä¸äº§ç”Ÿæ€§èƒ½æŸå¤±ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLoHåœ¨è¡¨æ ¼æ•°æ®å’Œè§†è§‰äº•å­—æ£‹(Visual Tic-Tac-Toe)ä»»åŠ¡ä¸­å–å¾—äº†å¼ºåŠ²è¡¨ç°ï¼Œå¹¶èƒ½ç”Ÿæˆå…·æœ‰é«˜åº¦å¯è§£é‡Šæ€§çš„å†³ç­–è§„åˆ™ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21663v1",
      "published_date": "2025-09-25 22:31:43 UTC",
      "updated_date": "2025-09-25 22:31:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:06:53.651455+00:00"
    },
    {
      "arxiv_id": "2510.05109v3",
      "title": "Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient Multimodal Inference on Battery-Powered Small Devices",
      "title_zh": "å°è€Œå¼ºå¤§ï¼šé¢å‘ç”µæ± ä¾›ç”µå°å‹è®¾å¤‡çš„é«˜æ•ˆå¤šæ¨¡æ€æ¨ç†è½¯ç¡¬ä»¶ååŒè®¾è®¡æ–¹æ³•",
      "authors": [
        "Yilong Li",
        "Shuai Zhang",
        "Yijing Zeng",
        "Hao Zhang",
        "Xinmiao Xiong",
        "Jingyu Liu",
        "Pan Hu",
        "Suman Banerjee"
      ],
      "abstract": "Large Multimodal Models (LMMs) are inherently modular, consisting of vision and audio encoders, projectors, and large language models. Yet, they are almost always executed monolithically, which underutilizes the heterogeneous accelerators (NPUs, GPUs, DSPs) in modern SoCs and leads to high end-to-end latency. In this paper, we present NANOMIND, a hardware--software co-design inference framework for Large Multimodal Models (LMMs) that breaks large models into modular ``bricks'' (vision, language, audio, etc.) and maps each to its ideal accelerator. The key insight is that large models can be broken into modular components and scheduled to run on the most appropriate compute units. It performs module-level dynamic offloading across accelerators on unified-memory SoCs. By combining customized hardware design, system-level scheduling, and optimized low-bit computation kernels, we demonstrate our framework with a compact, battery-powered device capable of running LMMs entirely on device. This prototype functions as a self-contained intelligent assistant that requires no network connectivity, while achieving higher throughput and superior power efficiency under strict resource constraints. The design further bypasses CPU bottlenecks and reduces redundant memory usage through token-aware buffer management and module-level coordination. Our system outperforms existing implementations in resource efficiency, cutting energy consumption by 42.3\\% and GPU memory usage by 11.2\\%. This enables a battery-powered device to run LLaVA-OneVision with a camera for nearly 20.8 hours.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† NANOMINDï¼Œä¸€ç§é’ˆå¯¹å¤§è¯­è¨€å¤šæ¨¡æ€æ¨¡å‹ (Large Multimodal Models, LMMs) çš„è½¯ç¡¬ä»¶ååŒè®¾è®¡æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°ä»£ SoC ä¸­å¼‚æ„åŠ é€Ÿå™¨ (NPUs, GPUs, DSPs) åˆ©ç”¨ç‡ä¸è¶³å¯¼è‡´çš„å»¶è¿Ÿé—®é¢˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†å¤æ‚çš„æ¨¡å‹æ‹†åˆ†ä¸ºæ¨¡å—åŒ–çš„â€œç –å—â€ï¼Œå¹¶æ ¹æ®è®¡ç®—ç‰¹æ€§å°†å„æ¨¡å—åŠ¨æ€æ˜ å°„è‡³æœ€åŒ¹é…çš„ç¡¬ä»¶å•å…ƒã€‚NANOMIND ç»“åˆäº†è‡ªå®šä¹‰ç¡¬ä»¶è®¾è®¡ã€ç³»ç»Ÿçº§è°ƒåº¦ä»¥åŠä¼˜åŒ–çš„ low-bit computation kernelsï¼Œåœ¨ç»Ÿä¸€å†…å­˜ SoC ä¸Šå®ç°äº†é«˜æ•ˆçš„æ¨¡å—çº§åŠ¨æ€å¸è½½ã€‚é€šè¿‡å¼•å…¥ token-aware buffer management å’Œæ¨¡å—é—´åè°ƒæœºåˆ¶ï¼Œè¯¥ç³»ç»Ÿæœ‰æ•ˆç»•è¿‡äº† CPU ç“¶é¢ˆå¹¶å‡å°‘äº† 11.2% çš„ GPU å†…å­˜å ç”¨ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å°†èƒ½è€—æ˜¾è‘—é™ä½äº† 42.3%ï¼Œä½¿å¾—ç”µæ± ä¾›ç”µçš„å°å‹è®¾å¤‡èƒ½å¤Ÿè¿ç»­è¿è¡Œ LLaVA-OneVision æ¨¡å‹è¿‘ 20.8 å°æ—¶ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨æ— éœ€ç½‘ç»œè¿æ¥çš„èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²é«˜æ•ˆã€æŒä¹…çš„æ™ºèƒ½åŠ©æ‰‹å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.CL",
        "eess.SP"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.05109v3",
      "published_date": "2025-09-25 22:28:44 UTC",
      "updated_date": "2025-12-01 19:27:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:06:53.371562+00:00"
    },
    {
      "arxiv_id": "2509.21654v1",
      "title": "Limitations on Safe, Trusted, Artificial General Intelligence",
      "title_zh": "å®‰å…¨ã€å¯ä¿¡é€šç”¨äººå·¥æ™ºèƒ½çš„å±€é™æ€§",
      "authors": [
        "Rina Panigrahy",
        "Vatsal Sharan"
      ],
      "abstract": "Safety, trust and Artificial General Intelligence (AGI) are aspirational goals in artificial intelligence (AI) systems, and there are several informal interpretations of these notions. In this paper, we propose strict, mathematical definitions of safety, trust, and AGI, and demonstrate a fundamental incompatibility between them. We define safety of a system as the property that it never makes any false claims, trust as the assumption that the system is safe, and AGI as the property of an AI system always matching or exceeding human capability. Our core finding is that -- for our formal definitions of these notions -- a safe and trusted AI system cannot be an AGI system: for such a safe, trusted system there are task instances which are easily and provably solvable by a human but not by the system. We note that we consider strict mathematical definitions of safety and trust, and it is possible for real-world deployments to instead rely on alternate, practical interpretations of these notions. We show our results for program verification, planning, and graph reachability. Our proofs draw parallels to GÃ¶del's incompleteness theorems and Turing's proof of the undecidability of the halting problem, and can be regarded as interpretations of GÃ¶del's and Turing's results.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­å®‰å…¨(Safety)ã€ä¿¡ä»»(Trust)å’Œé€šç”¨äººå·¥æ™ºèƒ½(AGI)è¿™äº›ç†æƒ³åŒ–ç›®æ ‡ï¼Œæå‡ºäº†ä¸¥æ ¼çš„æ•°å­¦å®šä¹‰ï¼Œå¹¶æ­ç¤ºäº†å®ƒä»¬ä¹‹é—´å­˜åœ¨çš„æ ¹æœ¬æ€§ä¸ç›¸å®¹ã€‚ç ”ç©¶å°†ç³»ç»Ÿçš„å®‰å…¨æ€§å®šä¹‰ä¸ºä»ä¸åšå‡ºè™šå‡é™ˆè¿°ï¼Œå°†ä¿¡ä»»å®šä¹‰ä¸ºç³»ç»Ÿå®‰å…¨çš„å‡è®¾ï¼Œè€Œå°†AGIå®šä¹‰ä¸ºå§‹ç»ˆè¾¾åˆ°æˆ–è¶…è¿‡äººç±»èƒ½åŠ›çš„ç‰¹æ€§ã€‚æ ¸å¿ƒå‘ç°è¡¨æ˜ï¼Œåœ¨è¿™äº›æ­£å¼å®šä¹‰çš„æ¡†æ¶ä¸‹ï¼Œä¸€ä¸ªå®‰å…¨ä¸”å—ä¿¡ä»»çš„AIç³»ç»Ÿä¸å¯èƒ½æˆä¸ºAGIç³»ç»Ÿã€‚è¯æ˜è¿‡ç¨‹æ˜¾ç¤ºï¼Œå¯¹äºæ­¤ç±»å®‰å…¨å—ä¿¡ä»»çš„ç³»ç»Ÿï¼Œå­˜åœ¨æŸäº›äººç±»èƒ½å¤Ÿè½»æ˜“ä¸”å¯è¯æ˜è§£å†³çš„ä»»åŠ¡å®ä¾‹ï¼Œè€Œè¯¥ç³»ç»Ÿå´æ— æ³•è§£å†³ã€‚ä½œè€…åœ¨ç¨‹åºéªŒè¯(Program verification)ã€è§„åˆ’(Planning)å’Œå›¾å¯è¾¾æ€§(Graph reachability)ç­‰é¢†åŸŸå±•ç¤ºäº†è¿™ä¸€ç»“æœã€‚è¯¥è¯æ˜å€Ÿé‰´å¹¶å¯ä»¥è¢«è§†ä¸ºå¯¹å“¥å¾·å°”ä¸å®Œå¤‡å®šç†(GÃ¶del's incompleteness theorems)å’Œå›¾çµåœæœºé—®é¢˜ä¸å¯åˆ¤å®šæ€§(Turing's proof of the undecidability of the halting problem)çš„ä¸€ç§è§£é‡Šã€‚å°½ç®¡æ•°å­¦ä¸Šçš„ä¸¥è°¨å®šä¹‰æŒ‡å‘äº†è¿™ç§å±€é™æ€§ï¼Œä½†ç ”ç©¶ä¹ŸæŒ‡å‡ºå®é™…åº”ç”¨ä¸­å¯èƒ½ä¼šä¾èµ–äºå¯¹è¿™äº›æ¦‚å¿µçš„å…¶ä»–å®ç”¨æ€§è§£é‡Šã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CC"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2509.21654v1",
      "published_date": "2025-09-25 22:16:38 UTC",
      "updated_date": "2025-09-25 22:16:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:06:54.962732+00:00"
    },
    {
      "arxiv_id": "2510.00027v2",
      "title": "Learning Inter-Atomic Potentials without Explicit Equivariance",
      "title_zh": "æ— éœ€æ˜¾å¼ç­‰å˜æ€§çš„åŸå­é—´åŠ¿å­¦ä¹ ",
      "authors": [
        "Ahmed A. Elhag",
        "Arun Raja",
        "Alex Morehead",
        "Samuel M. Blau",
        "Garrett M. Morris",
        "Michael M. Bronstein"
      ],
      "abstract": "Accurate and scalable machine-learned inter-atomic potentials (MLIPs) are essential for molecular simulations ranging from drug discovery to new material design. Current state-of-the-art models enforce roto-translational symmetries through equivariant neural network architectures, a hard-wired inductive bias that can often lead to reduced flexibility, computational efficiency, and scalability. In this work, we introduce TransIP: Transformer-based Inter-Atomic Potentials, a novel training paradigm for interatomic potentials achieving symmetry compliance without explicit architectural constraints. Our approach guides a generic non-equivariant Transformer-based model to learn SO(3)-equivariance by optimizing its representations in the embedding space. Trained on the recent Open Molecules (OMol25) collection, a large and diverse molecular dataset built specifically for MLIPs and covering different types of molecules (including small organics, biomolecular fragments, and electrolyte-like species), TransIP effectively learns symmetry in its latent space, providing low equivariance error. Further, compared to a data augmentation baseline, TransIP achieves 40% to 60% improvement in performance across varying OMol25 dataset sizes. More broadly, our work shows that learned equivariance can be a powerful and efficient alternative to augmentation-based MLIP models.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†TransIPï¼Œä¸€ç§åŸºäºTransformerçš„åŸå­é—´åŠ¿å‡½æ•°(MLIPs)è®­ç»ƒèŒƒå¼ï¼Œæ—¨åœ¨è§£å†³å½“å‰ä¸»æµæ¨¡å‹å› ç¡¬ç¼–ç æ—‹è½¬å¹³ç§»å¯¹ç§°æ€§å¯¼è‡´çš„çµæ´»æ€§å’Œè®¡ç®—æ•ˆç‡å—é™é—®é¢˜ã€‚ä¸ä¼ ç»Ÿä¾é æ˜¾å¼ç­‰å˜ç¥ç»æ¶æ„çš„æ–¹æ³•ä¸åŒï¼ŒTransIPé‡‡ç”¨é€šç”¨çš„éç­‰å˜Transformeræ¨¡å‹ï¼Œé€šè¿‡åœ¨åµŒå…¥ç©ºé—´ä¸­ä¼˜åŒ–è¡¨å¾æ¥å¼•å¯¼æ¨¡å‹å­¦ä¹ SO(3)-equivarianceã€‚è¯¥æ¨¡å‹åœ¨æ¶µç›–å¤šç§åˆ†å­ç±»å‹çš„Open Molecules (OMol25)æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå®éªŒè¯æ˜å…¶åœ¨æ½œåœ¨ç©ºé—´ä¸­èƒ½æœ‰æ•ˆå­¦ä¹ å¯¹ç§°æ€§å¹¶ä¿æŒæä½çš„ç­‰å˜è¯¯å·®ã€‚ç›¸æ¯”äºåŸºäºæ•°æ®å¢å¼º(Data Augmentation)çš„åŸºå‡†æ¨¡å‹ï¼ŒTransIPåœ¨ä¸åŒè§„æ¨¡çš„OMol25æ•°æ®é›†ä¸Šå®ç°äº†40%è‡³60%çš„æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å­¦ä¹ å¾—åˆ°çš„ç­‰å˜æ€§å¯ä»¥æˆä¸ºæ›¿ä»£æ˜¾å¼ç­‰å˜æ¶æ„æˆ–æ•°æ®å¢å¼ºæ¨¡å‹çš„ä¸€ç§é«˜æ•ˆä¸”å¼ºå¤§çš„æ–¹æ¡ˆï¼Œä¸ºåˆ†å­æ¨¡æ‹Ÿå’Œææ–™è®¾è®¡æä¾›äº†æ›´å…·æ‰©å±•æ€§çš„å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 3 tables, 10 figures. Under review. Changes from v1 to v2: Clarified concluding phrases in the abstract and introduction, and corrected a single typo in Table 1's total energy MAE reported for eSEN-sm-d",
      "pdf_url": "https://arxiv.org/pdf/2510.00027v2",
      "published_date": "2025-09-25 22:15:10 UTC",
      "updated_date": "2025-10-15 17:55:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:06:55.672377+00:00"
    },
    {
      "arxiv_id": "2509.21651v2",
      "title": "Can AI Perceive Physical Danger and Intervene?",
      "title_zh": "AIèƒ½å¦æ„ŸçŸ¥ç‰©ç†å±é™©å¹¶è¿›è¡Œå¹²é¢„ï¼Ÿ",
      "authors": [
        "Abhishek Jindal",
        "Dmitry Kalashnikov",
        "R. Alex Hofer",
        "Oscar Chang",
        "Divya Garikapati",
        "Anirudha Majumdar",
        "Pierre Sermanet",
        "Vikas Sindhwani"
      ],
      "abstract": "When AI interacts with the physical world -- as a robot or an assistive agent -- new safety challenges emerge beyond those of purely ``digital AI\". In such interactions, the potential for physical harm is direct and immediate. How well do state-of-the-art foundation models understand common-sense facts about physical safety, e.g. that a box may be too heavy to lift, or that a hot cup of coffee should not be handed to a child? In this paper, our contributions are three-fold: first, we develop a highly scalable approach to continuous physical safety benchmarking of Embodied AI systems, grounded in real-world injury narratives and operational safety constraints. To probe multi-modal safety understanding, we turn these narratives and constraints into photorealistic images and videos capturing transitions from safe to unsafe states, using advanced generative models. Secondly, we comprehensively analyze the ability of major foundation models to perceive risks, reason about safety, and trigger interventions; this yields multi-faceted insights into their deployment readiness for safety-critical agentic applications. Finally, we develop a post-training paradigm to teach models to explicitly reason about embodiment-specific safety constraints provided through system instructions. The resulting models generate thinking traces that make safety reasoning interpretable and transparent, achieving state of the art performance in constraint satisfaction evaluations. The benchmark is released at https://asimov-benchmark.github.io/v2",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å…·èº«æ™ºèƒ½(Embodied AI)åœ¨ç‰©ç†ä¸–ç•Œäº¤äº’ä¸­çš„å®‰å…¨æŒ‘æˆ˜ï¼Œæ—¨åœ¨è¯„ä¼°åŸºç¡€æ¨¡å‹å¯¹ç‰©ç†å®‰å…¨å¸¸è¯†çš„ç†è§£ä¸å¹²é¢„èƒ½åŠ›ã€‚ç ”ç©¶é¦–å…ˆå¼€å‘äº†ä¸€ç§é«˜åº¦å¯æ‰©å±•çš„è¿ç»­ç‰©ç†å®‰å…¨åŸºå‡†æµ‹è¯•æ–¹æ³•ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹å°†ç°å®ä¼¤å®³å™äº‹è½¬åŒ–ä¸ºç…§ç‰‡çº§çœŸå®æ„Ÿçš„å›¾åƒå’Œè§†é¢‘ï¼Œä»¥æ¢æµ‹å¤šæ¨¡æ€å®‰å…¨ç†è§£ã€‚é€šè¿‡å¯¹ä¸»æµåŸºç¡€æ¨¡å‹çš„é£é™©æ„ŸçŸ¥å’Œå®‰å…¨æ¨ç†èƒ½åŠ›è¿›è¡Œå…¨é¢åˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨å®‰å…¨å…³é”®å‹åº”ç”¨ä¸­çš„éƒ¨ç½²ç°çŠ¶ã€‚æ­¤å¤–ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åè®­ç»ƒ(post-training)èŒƒå¼ï¼Œé€šè¿‡ç”Ÿæˆå¯è§£é‡Šçš„æ€ç»´è½¨è¿¹ä½¿æ¨¡å‹èƒ½å¤Ÿéµå¾ªç‰¹å®šçš„å…·èº«å®‰å…¨çº¦æŸã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨çº¦æŸæ»¡è¶³è¯„ä¼°ä¸­è¾¾åˆ°äº†State of the Artæ€§èƒ½ï¼Œä¸ºæå‡ç‰©ç†è¾…åŠ©æ™ºèƒ½ä½“çš„å®‰å…¨æ€§å’Œé€æ˜åº¦æä¾›äº†ç³»ç»Ÿæ€§æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21651v2",
      "published_date": "2025-09-25 22:09:17 UTC",
      "updated_date": "2025-11-21 18:22:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:07:12.484189+00:00"
    },
    {
      "arxiv_id": "2509.21634v2",
      "title": "MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G Open RANs",
      "title_zh": "MobiLLMï¼šé¢å‘ 6G å¼€æ”¾å¼æ— çº¿æ¥å…¥ç½‘é—­ç¯å¨èƒç¼“è§£çš„æ™ºèƒ½ä½“ AI æ¡†æ¶",
      "authors": [
        "Prakhar Sharma",
        "Haohuang Wen",
        "Vinod Yegneswaran",
        "Ashish Gehani",
        "Phillip Porras",
        "Zhiqiang Lin"
      ],
      "abstract": "The evolution toward 6G networks is being accelerated by the Open Radio Access Network (O-RAN) paradigm -- an open, interoperable architecture that enables intelligent, modular applications across public telecom and private enterprise domains. While this openness creates unprecedented opportunities for innovation, it also expands the attack surface, demanding resilient, low-cost, and autonomous security solutions. Legacy defenses remain largely reactive, labor-intensive, and inadequate for the scale and complexity of next-generation systems. Current O-RAN applications focus mainly on network optimization or passive threat detection, with limited capability for closed-loop, automated response.\n  To address this critical gap, we present an agentic AI framework for fully automated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM orchestrates security workflows through a modular multi-agent system powered by Large Language Models (LLMs). The framework features a Threat Analysis Agent for real-time data triage, a Threat Classification Agent that uses Retrieval-Augmented Generation (RAG) to map anomalies to specific countermeasures, and a Threat Response Agent that safely operationalizes mitigation actions via O-RAN control interfaces. Grounded in trusted knowledge bases such as the MITRE FiGHT framework and 3GPP specifications, and equipped with robust safety guardrails, MobiLLM provides a blueprint for trustworthy AI-driven network security. Initial evaluations demonstrate that MobiLLM can effectively identify and orchestrate complex mitigation strategies, significantly reducing response latency and showcasing the feasibility of autonomous security operations in 6G.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ 6G Open Radio Access Network (O-RAN) æ¶æ„ä¸­å› æ”»å‡»é¢æ‰©å¤§è€Œå¯¼è‡´çš„å®‰å…¨æ€§æŒ‘æˆ˜ï¼Œä»¥åŠä¼ ç»Ÿé˜²å¾¡æ‰‹æ®µååº”è¿Ÿé’ä¸”ç¼ºä¹è‡ªåŠ¨åŒ–é—­ç¯å“åº”çš„é—®é¢˜ï¼Œæå‡ºäº† MobiLLM æ™ºèƒ½ä½“ AI æ¡†æ¶ã€‚MobiLLM åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ„å»ºäº†ä¸€ä¸ªæ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨å®ç°ç«¯åˆ°ç«¯çš„å…¨è‡ªåŠ¨å¨èƒç¼“è§£ã€‚è¯¥æ¡†æ¶åŒ…å«ç”¨äºå®æ—¶æ•°æ®åˆ†é€‰çš„å¨èƒåˆ†ææ™ºèƒ½ä½“ (Threat Analysis Agent)ã€åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) å°†å¼‚å¸¸æ˜ å°„è‡³å…·ä½“å¯¹ç­–çš„å¨èƒåˆ†ç±»æ™ºèƒ½ä½“ (Threat Classification Agent)ï¼Œä»¥åŠé€šè¿‡ O-RAN æ§åˆ¶æ¥å£å®‰å…¨æ‰§è¡Œæ“ä½œçš„å¨èƒå“åº”æ™ºèƒ½ä½“ (Threat Response Agent)ã€‚åŸºäº MITRE FiGHT æ¡†æ¶å’Œ 3GPP è§„èŒƒç­‰æƒå¨çŸ¥è¯†åº“ï¼ŒMobiLLM ä¸ºæ„å»ºå¯ä¿¡çš„ AI é©±åŠ¨ç½‘ç»œå®‰å…¨æä¾›äº†è“å›¾ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒMobiLLM èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å¹¶ç¼–æ’å¤æ‚çš„ç¼“è§£ç­–ç•¥ï¼Œæ˜¾è‘—é™ä½äº†å“åº”å»¶è¿Ÿï¼ŒéªŒè¯äº†åœ¨ 6G ç¯å¢ƒä¸‹å®ç°è‡ªä¸»å®‰å…¨è¿è¥çš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21634v2",
      "published_date": "2025-09-25 21:49:43 UTC",
      "updated_date": "2025-10-03 17:43:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:07:13.397274+00:00"
    },
    {
      "arxiv_id": "2509.21633v1",
      "title": "Semantic F1 Scores: Fair Evaluation Under Fuzzy Class Boundaries",
      "title_zh": "è¯­ä¹‰ F1 åˆ†æ•°ï¼šæ¨¡ç³Šç±»åˆ«è¾¹ç•Œä¸‹çš„å…¬å¹³è¯„ä¼°",
      "authors": [
        "Georgios Chochlakis",
        "Jackson Trager",
        "Vedant Jhaveri",
        "Nikhil Ravichandran",
        "Alexandros Potamianos",
        "Shrikanth Narayanan"
      ],
      "abstract": "We propose Semantic F1 Scores, novel evaluation metrics for subjective or fuzzy multi-label classification that quantify semantic relatedness between predicted and gold labels. Unlike the conventional F1 metrics that treat semantically related predictions as complete failures, Semantic F1 incorporates a label similarity matrix to compute soft precision-like and recall-like scores, from which the Semantic F1 scores are derived. Unlike existing similarity-based metrics, our novel two-step precision-recall formulation enables the comparison of label sets of arbitrary sizes without discarding labels or forcing matches between dissimilar labels. By granting partial credit for semantically related but nonidentical labels, Semantic F1 better reflects the realities of domains marked by human disagreement or fuzzy category boundaries. In this way, it provides fairer evaluations: it recognizes that categories overlap, that annotators disagree, and that downstream decisions based on similar predictions lead to similar outcomes. Through theoretical justification and extensive empirical validation on synthetic and real data, we show that Semantic F1 demonstrates greater interpretability and ecological validity. Because it requires only a domain-appropriate similarity matrix, which is robust to misspecification, and not a rigid ontology, it is applicable across tasks and modalities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸»è§‚æˆ–æ¨¡ç³Šå¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ä¸­ä¼ ç»Ÿ F1 æŒ‡æ ‡æ— æ³•è¯†åˆ«è¯­ä¹‰ç›¸å…³æ ‡ç­¾çš„é—®é¢˜ï¼Œæå‡ºäº† Semantic F1 Scores è¿™ä¸€æ–°å‹è¯„ä¼°æŒ‡æ ‡ã€‚è¯¥æŒ‡æ ‡é€šè¿‡å¼•å…¥æ ‡ç­¾ç›¸ä¼¼åº¦çŸ©é˜µ(label similarity matrix)æ¥è®¡ç®—è½¯ç²¾ç¡®åº¦(soft precision-like)å’Œè½¯å¬å›ç‡(soft recall-like)åˆ†æ•°ï¼Œä»è€Œé‡åŒ–é¢„æµ‹æ ‡ç­¾ä¸é‡‘æ ‡å‡†(gold labels)ä¹‹é—´çš„è¯­ä¹‰å…³è”æ€§ã€‚ä¸ç°æœ‰åŸºäºç›¸ä¼¼åº¦çš„æŒ‡æ ‡ä¸åŒï¼Œè¯¥æ–¹æ³•é‡‡ç”¨äº†åˆ›æ–°çš„ä¸¤æ­¥å¼ç²¾å‡†ç‡-å¬å›ç‡å…¬å¼ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¸¢å¼ƒæ ‡ç­¾æˆ–å¼ºåˆ¶åŒ¹é…ä¸ç›¸ä¼¼æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œå¯¹ä»»æ„å¤§å°çš„æ ‡ç­¾é›†è¿›è¡Œæ¯”è¾ƒã€‚é€šè¿‡å¯¹è¯­ä¹‰ç›¸å…³ä½†ä¸å®Œå…¨ç›¸åŒçš„æ ‡ç­¾ç»™äºˆéƒ¨åˆ†ä¿¡ç”¨(partial credit)ï¼ŒSemantic F1 æ›´çœŸå®åœ°åæ˜ äº†äººç±»åˆ¤æ–­å·®å¼‚å’Œç±»åˆ«ç•Œé™æ¨¡ç³Šçš„é¢†åŸŸç°çŠ¶ï¼Œæä¾›äº†æ›´å…¬å¹³çš„è¯„ä¼°æ–¹æ¡ˆã€‚è¯¥æŒ‡æ ‡ä»…éœ€ä¸€ä¸ªé¢†åŸŸç›¸å…³çš„ç›¸ä¼¼åº¦çŸ©é˜µï¼Œå¯¹é”™è¯¯è§„èŒƒ(misspecification)å…·æœ‰é²æ£’æ€§ï¼Œä¸”ä¸éœ€è¦åˆšæ€§çš„æœ¬ä½“(ontology)ï¼Œå› æ­¤é€‚ç”¨äºè·¨ä»»åŠ¡å’Œè·¨æ¨¡æ€çš„åœºæ™¯ã€‚ç†è®ºè¯æ˜å’Œå¹¿æ³›çš„å®è¯éªŒè¯è¡¨æ˜ï¼ŒSemantic F1 åœ¨åˆæˆåŠçœŸå®æ•°æ®ä¸Šå‡è¡¨ç°å‡ºæ›´å¼ºçš„å¯è§£é‡Šæ€§(interpretability)å’Œç”Ÿæ€æœ‰æ•ˆæ€§(ecological validity)ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "33 pages, 1 table, 29 figures, 4 algorithms",
      "pdf_url": "https://arxiv.org/pdf/2509.21633v1",
      "published_date": "2025-09-25 21:48:48 UTC",
      "updated_date": "2025-09-25 21:48:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:07:25.682878+00:00"
    },
    {
      "arxiv_id": "2509.21629v1",
      "title": "InvBench: Can LLMs Accelerate Program Verification with Invariant Synthesis?",
      "title_zh": "InvBenchï¼šå¤§è¯­è¨€æ¨¡å‹èƒ½å¦é€šè¿‡ä¸å˜å¼åˆæˆåŠ é€Ÿç¨‹åºéªŒè¯ï¼Ÿ",
      "authors": [
        "Anjiang Wei",
        "Tarun Suresh",
        "Tianran Sun",
        "Haoze Wu",
        "Ke Wang",
        "Alex Aiken"
      ],
      "abstract": "Program verification relies on loop invariants, yet automatically discovering strong invariants remains a long-standing challenge. We introduce a principled framework for evaluating LLMs on invariant synthesis. Our approach uses a verifier-based decision procedure with a formal soundness guarantee and assesses not only correctness but also the speedup that invariants provide in verification. We evaluate 7 state-of-the-art LLMs, and existing LLM-based verifiers against the traditional solver UAutomizer. While LLM-based verifiers represent a promising direction, they do not yet offer a significant advantage over UAutomizer. Model capability also proves critical, as shown by sharp differences in speedups across models, and our benchmark remains an open challenge for current LLMs. Finally, we show that supervised fine-tuning and Best-of-N sampling can improve performance: fine-tuning on 3589 instances raises the percentage of speedup cases for Qwen3-Coder-480B from 8% to 29.2%, and Best-of-N sampling with N=16 improves Claude-sonnet-4 from 8.8% to 22.1%.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†InvBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¾ªç¯ä¸å˜å¼åˆæˆ(Invariant Synthesis)ä¸­è¡¨ç°çš„åŸåˆ™æ€§æ¡†æ¶ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŸºäºéªŒè¯å™¨çš„å†³ç­–ç¨‹åºå¹¶æä¾›æ­£å¼çš„å¥å£®æ€§ä¿è¯ï¼Œä¸ä»…è¯„ä¼°ä¸å˜å¼çš„æ­£ç¡®æ€§ï¼Œè¿˜è¡¡é‡å…¶åœ¨ç¨‹åºéªŒè¯(Program Verification)ä¸­æä¾›çš„åŠ é€Ÿæ•ˆæœã€‚é€šè¿‡å¯¹æ¯”7ä¸ªå°–ç«¯LLMsåŠLLMéªŒè¯å™¨ä¸ä¼ ç»Ÿæ±‚è§£å™¨UAutomizerï¼Œç ”ç©¶å‘ç°åŸºäºLLMçš„éªŒè¯å™¨ç›®å‰å°šæœªå±•ç°å‡ºè¶…è¶Šä¼ ç»Ÿå·¥å…·çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚æ¨¡å‹èƒ½åŠ›å¯¹åŠ é€Ÿè¡¨ç°è‡³å…³é‡è¦ï¼Œä¸”InvBenchåŸºå‡†å¯¹å½“å‰æ¨¡å‹ä»å…·æŒ‘æˆ˜ã€‚å®éªŒè¿›ä¸€æ­¥è¡¨æ˜ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒ(Supervised Fine-tuning)å’ŒBest-of-N samplingå¯ä»¥æ˜¾è‘—ä¼˜åŒ–æ€§èƒ½ï¼Œä¾‹å¦‚Qwen3-Coder-480Bçš„åŠ é€Ÿæ¡ˆä¾‹æ¯”ä¾‹ä»8%æå‡è‡³29.2%ï¼Œè€ŒClaude-sonnet-4åœ¨N=16æ—¶æ€§èƒ½æå‡è‡³22.1%ã€‚è¯¥ç ”ç©¶ä¸ºåˆ©ç”¨LLMsåŠ é€Ÿç¨‹åºéªŒè¯æä¾›äº†ç³»ç»Ÿæ€§çš„è¯„ä¼°å·¥å…·ä¸æ€§èƒ½æ”¹è¿›è·¯å¾„ã€‚",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.PL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21629v1",
      "published_date": "2025-09-25 21:47:02 UTC",
      "updated_date": "2025-09-25 21:47:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:07:24.285476+00:00"
    },
    {
      "arxiv_id": "2509.21628v2",
      "title": "A Data-driven Typology of Vision Models from Integrated Representational Metrics",
      "title_zh": "åŸºäºç»¼åˆè¡¨å¾åº¦é‡çš„è§†è§‰æ¨¡å‹æ•°æ®é©±åŠ¨ç±»å‹å­¦",
      "authors": [
        "Jialin Wu",
        "Shreya Saha",
        "Yiqing Bo",
        "Meenakshi Khosla"
      ],
      "abstract": "Large vision models differ widely in architecture and training paradigm, yet we lack principled methods to determine which aspects of their representations are shared across families and which reflect distinctive computational strategies. We leverage a suite of representational similarity metrics, each capturing a different facet-geometry, unit tuning, or linear decodability-and assess family separability using multiple complementary measures. Metrics preserving geometry or tuning (e.g., RSA, Soft Matching) yield strong family discrimination, whereas flexible mappings such as Linear Predictivity show weaker separation. These findings indicate that geometry and tuning carry family-specific signatures, while linearly decodable information is more broadly shared. To integrate these complementary facets, we adapt Similarity Network Fusion (SNF), a method inspired by multi-omics integration. SNF achieves substantially sharper family separation than any individual metric and produces robust composite signatures. Clustering of the fused similarity matrix recovers both expected and surprising patterns: supervised ResNets and ViTs form distinct clusters, yet all self-supervised models group together across architectural boundaries. Hybrid architectures (ConvNeXt, Swin) cluster with masked autoencoders, suggesting convergence between architectural modernization and reconstruction-based training. This biology-inspired framework provides a principled typology of vision models, showing that emergent computational strategies-shaped jointly by architecture and training objective-define representational structure beyond surface design categories.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰æ¨¡å‹åœ¨æ¶æ„å’Œè®­ç»ƒèŒƒå¼ä¸Šçš„å¤šæ ·æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºé›†æˆè¡¨å¾åº¦é‡(Representational Metrics)çš„æ•°æ®é©±åŠ¨åˆ†ç±»æ³•ï¼Œæ—¨åœ¨åŒºåˆ†ä¸åŒæ¨¡å‹å®¶æ—é—´çš„å…±æœ‰è¡¨å¾ä¸ç‹¬ç‰¹è®¡ç®—ç­–ç•¥ã€‚ç ”ç©¶é€šè¿‡è¯„ä¼°ä¸€ç³»åˆ—ç›¸ä¼¼åº¦åº¦é‡å·¥å…·ï¼Œå‘ç°ä¿ç•™å‡ ä½•ç»“æ„(Geometry)æˆ–å•å…ƒè°ƒèŠ‚(Unit Tuning)çš„åº¦é‡ï¼ˆå¦‚ RSA å’Œ Soft Matchingï¼‰èƒ½æœ‰æ•ˆè¯†åˆ«æ¨¡å‹å®¶æ—ç‰¹å¾ï¼Œè€Œçº¿æ€§å¯é¢„æµ‹æ€§(Linear Predictivity)åˆ™æ­ç¤ºäº†è·¨å®¶æ—çš„å…±äº«ä¿¡æ¯ã€‚ä¸ºäº†æ•´åˆè¿™äº›äº’è¡¥ç»´åº¦ï¼Œä½œè€…å¼•å…¥äº†ç›¸ä¼¼æ€§ç½‘ç»œèåˆ(Similarity Network Fusion, SNF)æ–¹æ³•ï¼Œå®ç°äº†æ¯”å•ä¸€æŒ‡æ ‡æ›´æ¸…æ™°çš„åˆ†ç±»æ•ˆæœå¹¶ç”Ÿæˆäº†ç¨³å¥çš„å¤åˆç‰¹å¾ç­¾åã€‚èšç±»åˆ†ææ˜¾ç¤ºï¼Œæœ‰ç›‘ç£å­¦ä¹ çš„ ResNets å’Œ ViTs å‘ˆç°æ˜¾è‘—å·®å¼‚ï¼Œè€Œæ‰€æœ‰è‡ªç›‘ç£å­¦ä¹ (Self-supervised)æ¨¡å‹å³ä¾¿æ¶æ„ä¸åŒä¹Ÿå±•ç°å‡ºè¡¨å¾ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæ··åˆæ¶æ„ï¼ˆå¦‚ ConvNeXt å’Œ Swinï¼‰ä¸æ©ç è‡ªç¼–ç å™¨(Masked Autoencoders)çš„èšç±»ç°è±¡è¡¨æ˜äº†æ¶æ„æ¼”è¿›ä¸é‡æ„è®­ç»ƒä»»åŠ¡é—´çš„ç­–ç•¥æ”¶æ•›ã€‚è¿™ä¸€å—ç”Ÿç‰©å­¦å¯å‘çš„æ¡†æ¶è¯æ˜äº†ç”±æ¶æ„å’Œè®­ç»ƒç›®æ ‡å…±åŒå¡‘é€ çš„è®¡ç®—ç­–ç•¥ï¼Œä»æ ¹æœ¬ä¸Šå®šä¹‰äº†è¶…è¶Šè¡¨é¢è®¾è®¡ç±»åˆ«çš„æ¨¡å‹è¡¨å¾ç»“æ„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21628v2",
      "published_date": "2025-09-25 21:46:09 UTC",
      "updated_date": "2025-12-09 09:09:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:07:33.083800+00:00"
    },
    {
      "arxiv_id": "2509.21625v1",
      "title": "Guiding Audio Editing with Audio Language Model",
      "title_zh": "åˆ©ç”¨éŸ³é¢‘è¯­è¨€æ¨¡å‹å¼•å¯¼éŸ³é¢‘ç¼–è¾‘",
      "authors": [
        "Zitong Lan",
        "Yiduo Hao",
        "Mingmin Zhao"
      ],
      "abstract": "Audio editing plays a central role in VR/AR immersion, virtual conferencing, sound design, and other interactive media. However, recent generative audio editing models depend on template-like instruction formats and are restricted to mono-channel audio. These models fail to deal with declarative audio editing, where the user declares what the desired outcome should be, while leaving the details of editing operations to the system. We introduce SmartDJ, a novel framework for stereo audio editing that combines the reasoning capability of audio language models with the generative power of latent diffusion. Given a high-level instruction, SmartDJ decomposes it into a sequence of atomic edit operations, such as adding, removing, or spatially relocating events. These operations are then executed by a diffusion model trained to manipulate stereo audio. To support this, we design a data synthesis pipeline that produces paired examples of high-level instructions, atomic edit operations, and audios before and after each edit operation. Experiments demonstrate that SmartDJ achieves superior perceptual quality, spatial realism, and semantic alignment compared to prior audio editing methods. Demos are available at https://zitonglan.github.io/project/smartdj/smartdj.html.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰éŸ³é¢‘ç¼–è¾‘æ¨¡å‹ä¾èµ–æ¨¡æ¿åŒ–æŒ‡ä»¤ä¸”ä»…é™äºå•å£°é“(mono-channel)éŸ³é¢‘çš„å±€é™æ€§ï¼Œæå‡ºäº†SmartDJæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°æ›´å…·æ²‰æµ¸æ„Ÿçš„ç«‹ä½“å£°(stereo)éŸ³é¢‘ç¼–è¾‘ã€‚SmartDJå°†éŸ³é¢‘è¯­è¨€æ¨¡å‹(audio language model)çš„æ¨ç†èƒ½åŠ›ä¸æ½œåœ¨æ‰©æ•£æ¨¡å‹(latent diffusion)çš„ç”Ÿæˆèƒ½åŠ›ç›¸ç»“åˆï¼Œèƒ½å¤Ÿå°†ç”¨æˆ·çš„é«˜å±‚å£°æ˜å¼æŒ‡ä»¤åˆ†è§£ä¸ºæ·»åŠ ã€ç§»é™¤æˆ–ç©ºé—´é‡å®šä½ç­‰ä¸€ç³»åˆ—åŸå­ç¼–è¾‘æ“ä½œ(atomic edit operations)ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€æµç¨‹ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€å¥—ä¸“é—¨çš„æ•°æ®åˆæˆæµæ°´çº¿ï¼Œç”¨äºç”ŸæˆåŒ…å«æŒ‡ä»¤ã€åŸå­æ“ä½œåŠç¼–è¾‘å‰åéŸ³é¢‘çš„é…å¯¹è®­ç»ƒæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSmartDJåœ¨æ„ŸçŸ¥è´¨é‡ã€ç©ºé—´çœŸå®æ„Ÿå’Œè¯­ä¹‰å¯¹é½æ–¹é¢å‡ä¼˜äºå…ˆå‰çš„éŸ³é¢‘ç¼–è¾‘æ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³äº†å£°æ˜å¼éŸ³é¢‘ç¼–è¾‘(declarative audio editing)ä¸­çš„å¤æ‚æ“ä½œåˆ†é…é—®é¢˜ã€‚è¯¥æ¡†æ¶ä¸ºVR/ARã€è™šæ‹Ÿä¼šè®®å’Œå£°éŸ³è®¾è®¡ç­‰äº¤äº’å¼åª’ä½“åº”ç”¨æä¾›äº†æ›´å¼ºå¤§çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21625v1",
      "published_date": "2025-09-25 21:43:45 UTC",
      "updated_date": "2025-09-25 21:43:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:07:35.682904+00:00"
    },
    {
      "arxiv_id": "2509.21623v1",
      "title": "OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's Rule",
      "title_zh": "OjaKVï¼šåŸºäº Oja è§„åˆ™çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥åœ¨çº¿ä½ç§© KV ç¼“å­˜å‹ç¼©",
      "authors": [
        "Yuxuan Zhu",
        "David H. Yang",
        "Mohammad Mohammadi Amiri",
        "Keerthiram Murugesan",
        "Tejaswini Pedapati",
        "Pin-Yu Chen"
      ],
      "abstract": "The expanding long-context capabilities of large language models are constrained by a significant memory bottleneck: the key-value (KV) cache required for autoregressive generation. This bottleneck is substantial; for instance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of 4 requires approximately 16GB for its KV cache, a size exceeding the model's weights. While KV-cache compression via low-rank projection is a promising direction, existing methods rely on a static, offline-learned subspace that performs poorly under data distribution shifts. To overcome these limitations, we introduce OjaKV, a novel framework that integrates a strategic hybrid storage policy with online subspace adaptation. First, OjaKV recognizes that not all tokens are equally important for compression; it preserves the crucial first and most recent tokens in full-rank, maintaining high-fidelity anchors for attention. Second, for the vast majority of intermediate tokens, it applies low-rank compression by incrementally adapting the projection basis using Oja's algorithm for online principal component analysis. This adaptation involves a comprehensive update during prompt prefilling and lightweight periodic updates during decoding, ensuring the subspace remains aligned with the evolving context. Crucially, our framework is fully compatible with modern attention modules like FlashAttention. Experiments demonstrate that OjaKV maintains or even improves zero-shot accuracy at high compression ratios. In particular, OjaKV achieves its strongest gains on very long-context benchmarks that require complex reasoning, highlighting the importance of online subspace adaptation in dynamically tracking context shifts. These results establish our hybrid framework as a practical, plug-and-play solution for memory-efficient long-context inference without requiring model fine-tuning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿æ–‡æœ¬æ¨ç†ä¸­é¢ä¸´çš„ KV Cache å†…å­˜ç“¶é¢ˆï¼Œæå‡ºäº† OjaKV æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ä½ç§©å‹ç¼©æ–¹æ³•åœ¨æ•°æ®åˆ†å¸ƒåç§»ä¸‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚OjaKV é‡‡ç”¨äº†åˆ›æ–°çš„æ··åˆå­˜å‚¨ç­–ç•¥ï¼Œå°†å…³é”®çš„åˆå§‹ Token å’Œæœ€æ–° Token ä»¥ Full-Rank å½¢å¼ä¿ç•™ï¼Œä»¥ç¡®ä¿æ³¨æ„åŠ›æœºåˆ¶çš„é«˜ä¿çœŸé”šç‚¹ã€‚å¯¹äºæµ·é‡çš„ä¸­é—´ Tokenï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ Oja's Rule ç®—æ³•è¿›è¡Œåœ¨çº¿ä¸»æˆåˆ†åˆ†æ (Online PCA)ï¼Œåœ¨ Prefilling å’Œ Decoding é˜¶æ®µåŠ¨æ€æ›´æ–°æŠ•å½±åŸºåº•ï¼Œä½¿å‹ç¼©å­ç©ºé—´èƒ½å¤Ÿå®æ—¶è¿½è¸ªä¸Šä¸‹æ–‡çš„å˜åŒ–ã€‚è¯¥æ–¹æ¡ˆä¸ FlashAttention ç­‰ç°ä»£æŠ€æœ¯å®Œå…¨å…¼å®¹ï¼Œä¸”æ— éœ€æ¨¡å‹å¾®è°ƒï¼Œæ˜¯ä¸€ç§å³æ’å³ç”¨çš„å†…å­˜ä¼˜åŒ–æ–¹æ¡ˆã€‚å®éªŒç»“æœè¯æ˜ï¼ŒOjaKV åœ¨é«˜å‹ç¼©æ¯”ä¸‹ä¸ä»…ç»´æŒäº† Zero-Shot å‡†ç¡®ç‡ï¼Œåœ¨å¤æ‚é•¿æ–‡æœ¬æ¨ç†ä»»åŠ¡ä¸­ç”šè‡³å®ç°äº†æ€§èƒ½æå‡ï¼Œå……åˆ†ä½“ç°äº†åœ¨çº¿å­ç©ºé—´è‡ªé€‚åº”çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21623v1",
      "published_date": "2025-09-25 21:42:27 UTC",
      "updated_date": "2025-09-25 21:42:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:07:41.389884+00:00"
    },
    {
      "arxiv_id": "2510.02326v1",
      "title": "Hallucination-Resistant, Domain-Specific Research Assistant with Self-Evaluation and Vector-Grounded Retrieval",
      "title_zh": "å…·å¤‡è‡ªæˆ‘è¯„ä¼°ä¸å‘é‡æ£€ç´¢çº¦æŸçš„æŠ—å¹»è§‰é¢†åŸŸä¸“ç”¨ç ”ç©¶åŠ©æ‰‹",
      "authors": [
        "Vivek Bhavsar",
        "Joseph Ereifej",
        "Aravanan Gurusami"
      ],
      "abstract": "Large language models accelerate literature synthesis but can hallucinate and mis-cite, limiting their usefulness in expert workflows. We present RA-FSM (Research Assistant - Finite State Machine), a modular GPT-based research assistant that wraps generation in a finite-state control loop: Relevance -> Confidence -> Knowledge. The system is grounded in vector retrieval and a deterministic citation pipeline. The controller filters out-of-scope queries, scores answerability, decomposes questions, and triggers retrieval only when needed, and emits answers with confidence labels and in-corpus, de-duplicated references. A ranked-tier ingestion workflow constructs a domain knowledge base from journals, conferences, indices, preprints, and patents, writing both to a dense vector index and to a relational store of normalized metrics. We implement the system for photonics and evaluate it on six task categories: analytical reasoning, numerical analysis, methodological critique, comparative synthesis, factual extraction, and application design. In blinded A/B reviews, domain experts prefer RA-FSM to both a strong Notebook LM (NLM) and a vanilla Default GPT API call single-pass baseline, citing stronger boundary-condition handling and more defensible evidence use. Coverage and novelty analyses indicate that RA-FSM explores beyond the NLM while incurring tunable latency and cost overheads. The design emphasizes transparent, well-cited answers for high-stakes technical work and is generalizable to other scientific domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RA-FSM (Research Assistant - Finite State Machine)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨æ–‡çŒ®åˆæˆä¸­å®¹æ˜“äº§ç”Ÿå¹»è§‰ (hallucination) å’Œé”™è¯¯å¼•ç”¨é—®é¢˜çš„é¢†åŸŸç‰¹å®šç ”ç©¶åŠ©æ‰‹ã€‚ç³»ç»Ÿé‡‡ç”¨åŸºäºæœ‰é™çŠ¶æ€æœº (Finite State Machine) çš„æ§åˆ¶å¾ªç¯ï¼Œé€šè¿‡ç›¸å…³æ€§ã€ç½®ä¿¡åº¦å’ŒçŸ¥è¯†è·å–ä¸‰ä¸ªé˜¶æ®µå¯¹ç”Ÿæˆè¿‡ç¨‹è¿›è¡Œæ¨¡å—åŒ–å°è£…ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å‘é‡æ£€ç´¢ (Vector retrieval) å’Œç¡®å®šæ€§å¼•ç”¨ç®¡é“ï¼Œå¹¶åˆ©ç”¨åˆ†çº§æ‘„å…¥å·¥ä½œæµæ„å»ºäº†åŒ…å«æœŸåˆŠã€ä¼šè®®å’Œä¸“åˆ©çš„é¢†åŸŸçŸ¥è¯†åº“ã€‚æ§åˆ¶å™¨è´Ÿè´£è¿‡æ»¤è¶…çº²æŸ¥è¯¢ã€è¯„ä¼°å¯å›ç­”æ€§ã€æ‹†è§£é—®é¢˜ï¼Œå¹¶ä»…åœ¨å¿…è¦æ—¶è§¦å‘æ£€ç´¢ä»¥ç”Ÿæˆå¸¦æœ‰ç½®ä¿¡åº¦æ ‡ç­¾å’Œå»é‡å¼•ç”¨çš„å›ç­”ã€‚åœ¨å…‰å­å­¦ (photonics) é¢†åŸŸçš„ç›²æµ‹ A/B è¯„å®¡ä¸­ï¼Œé¢†åŸŸä¸“å®¶è®¤ä¸º RA-FSM åœ¨è¾¹ç•Œæ¡ä»¶å¤„ç†å’Œè¯æ®æ”¯æŒæ–¹é¢ä¼˜äº Notebook LM å’ŒåŸºç¡€ GPT APIã€‚è¦†ç›–èŒƒå›´å’Œæ–°é¢–æ€§åˆ†æè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ä¿è¯é€æ˜åº¦å’Œå¼•ç”¨è´¨é‡çš„åŒæ—¶èƒ½å®ç°æ›´å¹¿æ³›çš„çŸ¥è¯†æ¢ç´¢ï¼Œå…·æœ‰å‘å…¶ä»–é«˜è¦æ±‚ç§‘å­¦é¢†åŸŸæ¨å¹¿çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.02326v1",
      "published_date": "2025-09-25 21:35:46 UTC",
      "updated_date": "2025-09-25 21:35:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:07:47.388846+00:00"
    },
    {
      "arxiv_id": "2509.21617v1",
      "title": "LANCE: Low Rank Activation Compression for Efficient On-Device Continual Learning",
      "title_zh": "LANCEï¼šé¢å‘é«˜æ•ˆç«¯ä¾§æŒç»­å­¦ä¹ çš„ä½ç§©æ¿€æ´»å‹ç¼©",
      "authors": [
        "Marco Paul E. Apolinario",
        "Kaushik Roy"
      ],
      "abstract": "On-device learning is essential for personalization, privacy, and long-term adaptation in resource-constrained environments. Achieving this requires efficient learning, both fine-tuning existing models and continually acquiring new tasks without catastrophic forgetting. Yet both settings are constrained by high memory cost of storing activations during backpropagation. Existing activation compression methods reduce this cost but relying on repeated low-rank decompositions, introducing computational overhead. Also, such methods have not been explored for continual learning. We propose LANCE (Low-rank Activation Compression), a framework that performs one-shot higher-order Singular Value Decompsoition (SVD) to obtain a reusable low-rank subspace for activation projection. This eliminates repeated decompositions, reducing both memory and computation. Moreover, fixed low-rank subspaces further enable on-device continual learning by allocating tasks to orthogonal subspaces without storing large task-specific matrices. Experiments show that LANCE reduces activation storage up to 250$\\times$ while maintaining accuracy comparable to full backpropagation on CIFAR-10/100, Oxford-IIIT Pets, Flowers102, and CUB-200 datasets. On continual learning benchmarks (Split CIFAR-100, Split MiniImageNet, 5-Datasets), it achieves performance competitive with orthogonal gradient projection methods at a fraction of the memory cost. These results position LANCE as a practical and scalable solution for efficient fine-tuning and continual learning on edge devices.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LANCE (Low-rank Activation Compression) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¾¹ç¼˜è®¾å¤‡å­¦ä¹ ä¸­åå‘ä¼ æ’­å¯¼è‡´çš„æ¿€æ´»å€¼å­˜å‚¨æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚é’ˆå¯¹ä¼ ç»Ÿæ–¹æ³•ä¸­é‡å¤ä½ç§©åˆ†è§£å¸¦æ¥çš„è®¡ç®—è´Ÿæ‹…ï¼ŒLANCE åˆ›æ–°æ€§åœ°é‡‡ç”¨ä¸€æ¬¡æ€§é«˜é˜¶å¥‡å¼‚å€¼åˆ†è§£ (Singular Value Decomposition, SVD) æ¥è·å–å¯é‡å¤ä½¿ç”¨çš„ä½ç§©æŠ•å½±å­ç©ºé—´ã€‚è¿™ä¸€æ”¹è¿›æ˜¾è‘—å‡å°‘äº†å†…å­˜å’Œè®¡ç®—å¼€é”€ï¼Œå¹¶å…è®¸é€šè¿‡å°†ä»»åŠ¡åˆ†é…åˆ°æ­£äº¤å­ç©ºé—´æ¥å®ç°é«˜æ•ˆçš„æŒç»­å­¦ä¹  (continual learning)ï¼Œä»è€Œåœ¨ä¸å­˜å‚¨å¤§å‹ä»»åŠ¡çŸ©é˜µçš„æƒ…å†µä¸‹é¿å…ç¾éš¾æ€§é—å¿˜ã€‚å®éªŒè¡¨æ˜ï¼ŒLANCE åœ¨å¤šä¸ªä¸»æµæ•°æ®é›†ä¸Šèƒ½å¤Ÿå°†æ¿€æ´»å­˜å‚¨é™ä½å¤šè¾¾ 250 å€ï¼Œä¸”å‡†ç¡®ç‡ä¸å…¨é‡åå‘ä¼ æ’­ç›¸å½“ã€‚åœ¨æŒç»­å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶æ€§èƒ½å¯åª²ç¾æ­£äº¤æ¢¯åº¦æŠ•å½±æ³•ï¼Œä½†æ‰€éœ€çš„å†…å­˜æˆæœ¬ä»…ä¸ºå…¶ä¸€å°éƒ¨åˆ†ã€‚è¯¥ç ”ç©¶ä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„æ¨¡å‹å¾®è°ƒä¸æŒç»­å­¦ä¹ æä¾›äº†ä¸€ç§å…·æœ‰æ‰©å±•æ€§çš„å®ç”¨æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.21617v1",
      "published_date": "2025-09-25 21:33:40 UTC",
      "updated_date": "2025-09-25 21:33:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:07:50.197892+00:00"
    },
    {
      "arxiv_id": "2509.21613v1",
      "title": "Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ–çš„å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ ï¼šå‰ç»æ€§è§†è§’",
      "authors": [
        "Lingxiao Kong",
        "Cong Yang",
        "Oya Deniz Beyan",
        "Zeyd Boukhers"
      ],
      "abstract": "Multi-Objective Reinforcement Learning (MORL) presents significant challenges and opportunities for optimizing multiple objectives in Large Language Models (LLMs). We introduce a MORL taxonomy and examine the advantages and limitations of various MORL methods when applied to LLM optimization, identifying the need for efficient and flexible approaches that accommodate personalization functionality and inherent complexities in LLMs and RL. We propose a vision for a MORL benchmarking framework that addresses the effects of different methods on diverse objective relationships. As future research directions, we focus on meta-policy MORL development that can improve efficiency and flexibility through its bi-level learning paradigm, highlighting key research questions and potential solutions for improving LLM performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ (Multi-Objective Reinforcement Learning, MORL)åœ¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)å¤šç›®æ ‡ä¼˜åŒ–ä¸­çš„æŒ‘æˆ˜ä¸æœºé‡ã€‚ä½œè€…æå‡ºäº†ä¸€å¥—MORLåˆ†ç±»æ³•ï¼Œç³»ç»Ÿåˆ†æäº†å¤šç§MORLæ–¹æ³•åœ¨ä¼˜åŒ–LLMsæ—¶çš„ä¼˜åŠ£åŠ¿ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰æŠ€æœ¯åœ¨å¤„ç†ä¸ªæ€§åŒ–éœ€æ±‚åŠæ¨¡å‹å¤æ‚æ€§æ—¶å¯¹æ•ˆç‡å’Œçµæ´»æ€§çš„è¿«åˆ‡éœ€æ±‚ã€‚è®ºæ–‡è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªMORLåŸºå‡†æµ‹è¯•æ¡†æ¶çš„æ„¿æ™¯ï¼Œæ—¨åœ¨è¯„ä¼°ä¸åŒæ–¹æ³•å¯¹å¤šæ ·åŒ–ç›®æ ‡å…³ç³»çš„å½±å“ã€‚é’ˆå¯¹æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œè¯¥ç ”ç©¶é‡ç‚¹å…³æ³¨åŸºäºåŒå±‚å­¦ä¹ èŒƒå¼(bi-level learning paradigm)çš„å…ƒç­–ç•¥MORL(meta-policy MORL)å¼€å‘ï¼Œä»¥æå‡ä¼˜åŒ–çš„æ•ˆç‡ä¸çµæ´»æ€§ã€‚é€šè¿‡æ·±å…¥æ¢è®¨æå‡LLMæ€§èƒ½çš„å…³é”®ç ”ç©¶é—®é¢˜åŠæ½œåœ¨è§£å†³æ–¹æ¡ˆï¼Œè¯¥è®ºæ–‡ä¸ºæ„å»ºæ›´é«˜æ•ˆã€å¯å®šåˆ¶çš„å¤§æ¨¡å‹ä¼˜åŒ–ä½“ç³»æä¾›äº†å‰ç»æ€§çš„ç†è®ºæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "3 pages, 1 figure, accepted by ECAI MODeM 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.21613v1",
      "published_date": "2025-09-25 21:29:08 UTC",
      "updated_date": "2025-09-25 21:29:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:07:52.289674+00:00"
    },
    {
      "arxiv_id": "2510.02325v1",
      "title": "Agentic-AI Healthcare: Multilingual, Privacy-First Framework with MCP Agents",
      "title_zh": "Agentic-AI Healthcareï¼šåŸºäº MCP æ™ºèƒ½ä½“çš„å¤šè¯­è¨€éšç§ä¼˜å…ˆæ¡†æ¶",
      "authors": [
        "Mohammed A. Shehab"
      ],
      "abstract": "This paper introduces Agentic-AI Healthcare, a privacy-aware, multilingual, and explainable research prototype developed as a single-investigator project. The system leverages the emerging Model Context Protocol (MCP) to orchestrate multiple intelligent agents for patient interaction, including symptom checking, medication suggestions, and appointment scheduling. The platform integrates a dedicated Privacy and Compliance Layer that applies role-based access control (RBAC), AES-GCM field-level encryption, and tamper-evident audit logging, aligning with major healthcare data protection standards such as HIPAA (US), PIPEDA (Canada), and PHIPA (Ontario). Example use cases demonstrate multilingual patient-doctor interaction (English, French, Arabic) and transparent diagnostic reasoning powered by large language models. As an applied AI contribution, this work highlights the feasibility of combining agentic orchestration, multilingual accessibility, and compliance-aware architecture in healthcare applications. This platform is presented as a research prototype and is not a certified medical device.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†Agentic-AI Healthcareï¼Œè¿™æ˜¯ä¸€ä¸ªæ³¨é‡éšç§ä¿æŠ¤ã€æ”¯æŒå¤šè¯­è¨€ä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„åŒ»ç–—ç ”ç©¶åŸå‹æ¡†æ¶ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨æ–°å…´çš„æ¨¡å‹ä¸Šä¸‹æ–‡åè®®(Model Context Protocol, MCP)æ¥åè°ƒå¤šä¸ªæ™ºèƒ½ä½“(Agents)ï¼Œä»¥å®ç°ç—‡çŠ¶æ£€æŸ¥ã€è¯ç‰©å»ºè®®å’Œé¢„çº¦æŒ‚å·ç­‰æ‚£è€…äº¤äº’åŠŸèƒ½ã€‚å¹³å°é›†æˆäº†ä¸“é—¨çš„éšç§ä¸åˆè§„å±‚(Privacy and Compliance Layer)ï¼Œåº”ç”¨äº†åŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶(RBAC)ã€AES-GCMå­—æ®µçº§åŠ å¯†ä»¥åŠé˜²ç¯¡æ”¹å®¡è®¡æ—¥å¿—ï¼Œç¡®ä¿å…¶ç¬¦åˆHIPAAã€PIPEDAå’ŒPHIPAç­‰ä¸»æµåŒ»ç–—æ•°æ®ä¿æŠ¤æ ‡å‡†ã€‚è¯¥æ¡†æ¶å±•ç°äº†åœ¨è‹±è¯­ã€æ³•è¯­å’Œé˜¿æ‹‰ä¼¯è¯­ç¯å¢ƒä¸‹çš„å¤šè¯­è¨€åŒ»æ‚£äº¤äº’èƒ½åŠ›ï¼Œå¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹æä¾›é€æ˜çš„è¯Šæ–­æ¨ç†è¿‡ç¨‹ã€‚ä½œä¸ºä¸€é¡¹åº”ç”¨äººå·¥æ™ºèƒ½(Applied AI)çš„ç ”ç©¶è´¡çŒ®ï¼Œè¯¥å·¥ä½œè¯æ˜äº†åœ¨åŒ»ç–—é¢†åŸŸç»“åˆæ™ºèƒ½ä½“ç¼–æ’(Agentic Orchestration)ã€å¤šè¯­è¨€æ”¯æŒä¸åˆè§„æ€§æ¶æ„çš„æŠ€æœ¯å¯è¡Œæ€§ã€‚ç›®å‰è¯¥å¹³å°ä»…ä½œä¸ºç ”ç©¶åŸå‹å‘å¸ƒï¼Œå¹¶éç»è¿‡è®¤è¯çš„åŒ»ç–—å™¨æ¢°ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "6 pages, 1 figure. Submitted as a system/vision paper",
      "pdf_url": "https://arxiv.org/pdf/2510.02325v1",
      "published_date": "2025-09-25 21:25:52 UTC",
      "updated_date": "2025-09-25 21:25:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:08:15.090131+00:00"
    },
    {
      "arxiv_id": "2509.22737v2",
      "title": "CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models",
      "title_zh": "CompareBenchï¼šè§†è§‰è¯­è¨€æ¨¡å‹è§†è§‰æ¯”è¾ƒæ¨ç†èƒ½åŠ›çš„è¯„ä¼°åŸºå‡†",
      "authors": [
        "Jie Cai",
        "Kangning Yang",
        "Lan Fu",
        "Jiaming Ding",
        "Jinlong Li",
        "Huiming Sun",
        "Daitao Xing",
        "Jinglin Shen",
        "Zibo Meng"
      ],
      "abstract": "We introduce CompareBench, a benchmark for evaluating visual comparison reasoning in vision-language models (VLMs), a fundamental yet understudied skill. CompareBench consists of 1000 QA pairs across four tasks: quantity (600), temporal (100), geometric (200), and spatial (100). It is derived from two auxiliary datasets that we constructed: TallyBench (2000 counting images with QA) and HistCaps (515 historical images with bilingual captions). We evaluate both closed-source APIs (OpenAI, Gemini, Claude) and open-source models (Qwen2.5-VL and Qwen3-VL series). Results show clear scaling trends but also reveal critical limitations: even the strongest models consistently fail at temporal ordering and spatial relations, and they often make mistakes in basic counting and geometric comparisons that are trivial for humans. These findings demonstrate that visual comparison remains a systematic blind spot for current VLMs. By providing controlled, diverse, and diagnostic evaluation, CompareBench establishes a foundation for advancing more reliable multimodal reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CompareBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) è§†è§‰å¯¹æ¯”æ¨ç† (Visual Comparison Reasoning) èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†åŒ…å« 1000 ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–äº†æ•°é‡ (Quantity)ã€æ—¶é—´ (Temporal)ã€å‡ ä½• (Geometric) å’Œç©ºé—´ (Spatial) å››ä¸ªç»´åº¦çš„æ¯”è¾ƒä»»åŠ¡ï¼Œå¹¶åŸºäº TallyBench å’Œ HistCaps ä¸¤ä¸ªè¾…åŠ©æ•°æ®é›†æ„å»ºè€Œæˆã€‚ç ”ç©¶é€šè¿‡å¯¹ OpenAIã€Geminiã€Claude ç­‰é—­æºæ¨¡å‹ä»¥åŠ Qwen ç³»åˆ—å¼€æºæ¨¡å‹çš„è¯„ä¼°å‘ç°ï¼Œå°½ç®¡æ¨¡å‹è¡¨ç°å‡ºä¸€å®šçš„æ‰©å±•è¶‹åŠ¿ (Scaling Trends)ï¼Œä½†åœ¨å¤„ç†æ—¶é—´é¡ºåºå’Œç©ºé—´å…³ç³»æ—¶ä»å­˜åœ¨æ˜¾è‘—ç¼ºé™·ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨åŸºç¡€è®¡æ•°å’Œå‡ ä½•æ¯”è¾ƒç­‰ç®€å•ä»»åŠ¡ä¸Šä¹Ÿä¼šå‡ºç°é”™è¯¯ï¼Œè¡¨æ˜è§†è§‰å¯¹æ¯”æ¨ç†ä»æ˜¯å½“å‰ VLMs çš„ç³»ç»Ÿæ€§ç›²ç‚¹ã€‚CompareBench çš„æ¨å‡ºä¸ºå¼€å‘æ›´å¯é çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹æä¾›äº†å—æ§ä¸”å…·æœ‰è¯Šæ–­æ€§çš„è¯„ä¼°åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.22737v2",
      "published_date": "2025-09-25 21:14:11 UTC",
      "updated_date": "2025-12-17 19:26:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:08:22.990388+00:00"
    },
    {
      "arxiv_id": "2509.21600v1",
      "title": "Automated and Interpretable Survival Analysis from Multimodal Data",
      "title_zh": "åŸºäºå¤šæ¨¡æ€æ•°æ®çš„è‡ªåŠ¨åŒ–ä¸å¯è§£é‡Šç”Ÿå­˜åˆ†æ",
      "authors": [
        "Mafalda Malafaia",
        "Peter A. N. Bosman",
        "Coen Rasch",
        "Tanja Alderliesten"
      ],
      "abstract": "Accurate and interpretable survival analysis remains a core challenge in oncology. With growing multimodal data and the clinical need for transparent models to support validation and trust, this challenge increases in complexity. We propose an interpretable multimodal AI framework to automate survival analysis by integrating clinical variables and computed tomography imaging. Our MultiFIX-based framework uses deep learning to infer survival-relevant features that are further explained: imaging features are interpreted via Grad-CAM, while clinical variables are modeled as symbolic expressions through genetic programming. Risk estimation employs a transparent Cox regression, enabling stratification into groups with distinct survival outcomes. Using the open-source RADCURE dataset for head and neck cancer, MultiFIX achieves a C-index of 0.838 (prediction) and 0.826 (stratification), outperforming the clinical and academic baseline approaches and aligning with known prognostic markers. These results highlight the promise of interpretable multimodal AI for precision oncology with MultiFIX.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MultiFIXï¼Œä¸€ç§æ—¨åœ¨é€šè¿‡æ•´åˆä¸´åºŠå˜é‡(clinical variables)å’Œè®¡ç®—æœºæ–­å±‚æ‰«æ(computed tomography)å½±åƒæ•°æ®ï¼Œå®ç°è‡ªåŠ¨åŒ–ä¸”å…·å¯è§£é‡Šæ€§çš„å¤šæ¨¡æ€ç”Ÿå­˜åˆ†ææ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ·±åº¦å­¦ä¹ æå–ç”Ÿå­˜ç›¸å…³ç‰¹å¾ï¼Œå¹¶ç»“åˆGrad-CAMæŠ€æœ¯å¯¹å½±åƒç‰¹å¾è¿›è¡Œè§£é‡Šï¼ŒåŒæ—¶é€šè¿‡é—ä¼ ç¼–ç¨‹(genetic programming)å°†ä¸´åºŠå˜é‡å»ºæ¨¡ä¸ºç¬¦å·è¡¨è¾¾å¼ã€‚é£é™©è¯„ä¼°é‡‡ç”¨é€æ˜çš„Coxå›å½’(Cox regression)æ¨¡å‹ï¼Œèƒ½å¤Ÿå°†æ‚£è€…æœ‰æ•ˆåˆ’åˆ†ä¸ºå…·æœ‰æ˜¾è‘—ç”Ÿå­˜å·®å¼‚çš„ç¾¤ä½“ã€‚åœ¨å¤´é¢ˆç™Œå¼€æºæ•°æ®é›†RADCUREä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMultiFIXåœ¨é¢„æµ‹å’Œåˆ†å±‚æ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰çš„ä¸´åºŠå’Œå­¦æœ¯åŸºçº¿æ–¹æ¡ˆï¼Œå…¶é¢„æµ‹C-indexè¾¾åˆ°0.838ã€‚è¿™äº›ç»“æœçªæ˜¾äº†MultiFIXåœ¨ç²¾å‡†è‚¿ç˜¤å­¦(precision oncology)é¢†åŸŸæä¾›å¯è§£é‡Šã€å¯ä¿¡èµ–å†³ç­–æ”¯æŒçš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "4 figures; 4 tables; 24 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.21600v1",
      "published_date": "2025-09-25 21:13:39 UTC",
      "updated_date": "2025-09-25 21:13:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:08:34.995619+00:00"
    },
    {
      "arxiv_id": "2509.21595v1",
      "title": "Temporal vs. Spatial: Comparing DINOv3 and V-JEPA2 Feature Representations for Video Action Analysis",
      "title_zh": "æ—¶åŸŸä¸ç©ºåŸŸï¼šé¢å‘è§†é¢‘åŠ¨ä½œåˆ†æçš„ DINOv3 ä¸ V-JEPA2 ç‰¹å¾è¡¨ç¤ºå¯¹æ¯”",
      "authors": [
        "Sai Varun Kodathala",
        "Rakesh Vunnam"
      ],
      "abstract": "This study presents a comprehensive comparative analysis of two prominent self-supervised learning architectures for video action recognition: DINOv3, which processes frames independently through spatial feature extraction, and V-JEPA2, which employs joint temporal modeling across video sequences. We evaluate both approaches on the UCF Sports dataset, examining feature quality through multiple dimensions including classification accuracy, clustering performance, intra-class consistency, and inter-class discrimination. Our analysis reveals fundamental architectural trade-offs: DINOv3 achieves superior clustering performance (Silhouette score: 0.31 vs 0.21) and demonstrates exceptional discrimination capability (6.16x separation ratio) particularly for pose-identifiable actions, while V-JEPA2 exhibits consistent reliability across all action types with significantly lower performance variance (0.094 vs 0.288). Through action-specific evaluation, we identify that DINOv3's spatial processing architecture excels at static pose recognition but shows degraded performance on motion-dependent actions, whereas V-JEPA2's temporal modeling provides balanced representation quality across diverse action categories. These findings contribute to the understanding of architectural design choices in video analysis systems and provide empirical guidance for selecting appropriate feature extraction methods based on task requirements and reliability constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹è§†é¢‘åŠ¨ä½œè¯†åˆ«é¢†åŸŸä¸¤ç§ä¸»æµçš„è‡ªç›‘ç£å­¦ä¹ æ¶æ„ DINOv3 å’Œ V-JEPA2 è¿›è¡Œäº†æ·±å…¥çš„å¯¹æ¯”åˆ†æï¼Œæ¢è®¨äº†ç©ºé—´ä¸æ—¶åºç‰¹å¾è¡¨ç¤ºçš„å·®å¼‚ã€‚å®éªŒåœ¨ UCF Sports æ•°æ®é›†ä¸Šå±•å¼€ï¼Œé€šè¿‡èšç±»æ€§èƒ½ã€ç±»å†…ä¸€è‡´æ€§å’Œç±»é—´åŒºåˆ†åº¦ç­‰å¤šä¸ªç»´åº¦è¯„ä¼°ç‰¹å¾è´¨é‡ã€‚ç»“æœè¡¨æ˜ï¼ŒDINOv3 å‡­å€Ÿå…¶ç‹¬ç«‹å¸§çš„ç©ºé—´å¤„ç†èƒ½åŠ›ï¼Œåœ¨èšç±»è¡¨ç°ï¼ˆSilhouette score: 0.31ï¼‰å’ŒåŒºåˆ†èƒ½åŠ›ä¸Šä¼˜äº V-JEPA2ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«é™æ€å§¿æ€æ˜æ˜¾çš„åŠ¨ä½œæ—¶è¡¨ç°å“è¶Šã€‚ç„¶è€Œï¼ŒV-JEPA2 çš„è”åˆæ—¶åºå»ºæ¨¡åœ¨å„ç±»åŠ¨ä½œä¸­å±•ç°å‡ºæ›´å¼ºçš„ç¨³å®šæ€§å’Œæ›´ä½çš„æ€§èƒ½æ–¹å·®ï¼ˆ0.094ï¼‰ï¼Œæœ‰æ•ˆå¼¥è¡¥äº† DINOv3 åœ¨è¿åŠ¨ä¾èµ–å‹åŠ¨ä½œä¸Šçš„æ€§èƒ½ä¸‹é™ã€‚è¯¥å‘ç°æ­ç¤ºäº†ä¸åŒæ¶æ„åœ¨å¤„ç†è§†é¢‘æ•°æ®æ—¶çš„æƒè¡¡å…³ç³»ï¼Œä¸ºæ ¹æ®ä»»åŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚çš„ç‰¹å¾æå–æ–¹æ³•æä¾›äº†é‡è¦çš„å®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21595v1",
      "published_date": "2025-09-25 21:05:07 UTC",
      "updated_date": "2025-09-25 21:05:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:08:30.292049+00:00"
    },
    {
      "arxiv_id": "2509.21593v1",
      "title": "GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models",
      "title_zh": "GeoEvolveï¼šåŸºäºå¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹çš„åœ°ç†ç©ºé—´æ¨¡å‹è‡ªåŠ¨åŒ–å‘ç°",
      "authors": [
        "Peng Luo",
        "Xiayin Lou",
        "Yu Zheng",
        "Zhuo Zheng",
        "Stefano Ermon"
      ],
      "abstract": "Geospatial modeling provides critical solutions for pressing global challenges such as sustainability and climate change. Existing large language model (LLM)-based algorithm discovery frameworks, such as AlphaEvolve, excel at evolving generic code but lack the domain knowledge and multi-step reasoning required for complex geospatial problems. We introduce GeoEvolve, a multi-agent LLM framework that couples evolutionary search with geospatial domain knowledge to automatically design and refine geospatial algorithms. GeoEvolve operates in two nested loops: an inner loop leverages a code evolver to generate and mutate candidate solutions, while an outer agentic controller evaluates global elites and queries a GeoKnowRAG module -- a structured geospatial knowledge base that injects theoretical priors from geography. This knowledge-guided evolution steers the search toward theoretically meaningful and computationally efficient algorithms. We evaluate GeoEvolve on two fundamental and classical tasks: spatial interpolation (kriging) and spatial uncertainty quantification (geospatial conformal prediction). Across these benchmarks, GeoEvolve automatically improves and discovers new algorithms, incorporating geospatial theory on top of classical models. It reduces spatial interpolation error (RMSE) by 13-21% and enhances uncertainty estimation performance by 17\\%. Ablation studies confirm that domain-guided retrieval is essential for stable, high-quality evolution. These results demonstrate that GeoEvolve provides a scalable path toward automated, knowledge-driven geospatial modeling, opening new opportunities for trustworthy and efficient AI-for-Science discovery.",
      "tldr_zh": "é’ˆå¯¹åœ°ç†ç©ºé—´å»ºæ¨¡ä¸­ç°æœ‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç®—æ³•å‘ç°æ¡†æ¶ç¼ºä¹é¢†åŸŸçŸ¥è¯†å’Œå¤šæ­¥æ¨ç†èƒ½åŠ›çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†GeoEvolveï¼Œä¸€ä¸ªå°†æ¼”åŒ–æœç´¢ä¸åœ°ç†ç©ºé—´é¢†åŸŸçŸ¥è¯†ç›¸ç»“åˆçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒç¯åµŒå¥—æœºåˆ¶ï¼Œå†…ç¯åˆ©ç”¨ä»£ç æ¼”åŒ–å™¨ï¼ˆCode Evolverï¼‰ç”Ÿæˆå¹¶å˜å¼‚å€™é€‰è§£ï¼Œå¤–ç¯é€šè¿‡æ™ºèƒ½ä½“æ§åˆ¶å™¨è¯„ä¼°ç²¾è‹±è§£å¹¶ç»“åˆGeoKnowRAGæ¨¡å—æ³¨å…¥åœ°ç†å­¦ç†è®ºå…ˆéªŒã€‚ç ”ç©¶åœ¨ç©ºé—´æ’å€¼ï¼ˆKrigingï¼‰å’Œç©ºé—´ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆGeospatial Conformal Predictionï¼‰ä¸¤é¡¹ç»å…¸ä»»åŠ¡ä¸Šå¯¹æ¡†æ¶è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒGeoEvolveèƒ½å¤Ÿè‡ªåŠ¨æ”¹è¿›å¹¶å‘ç°æ–°ç®—æ³•ï¼Œä½¿ç©ºé—´æ’å€¼è¯¯å·®ï¼ˆRMSEï¼‰é™ä½äº†13-21%ï¼Œå¹¶å°†ä¸ç¡®å®šæ€§ä¼°è®¡æ€§èƒ½æå‡äº†17%ã€‚æ¶ˆèå®éªŒè¯å®äº†é¢†åŸŸå¼•å¯¼æ£€ç´¢å¯¹ç¨³å®šæ¼”åŒ–çš„å¿…è¦æ€§ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶ä¸ºè‡ªåŠ¨åŒ–ã€çŸ¥è¯†é©±åŠ¨çš„åœ°ç†ç©ºé—´å»ºæ¨¡å’ŒAI-for-Scienceå‘ç°æä¾›äº†å¯æ‰©å±•ä¸”å¯ä¿¡çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "physics.soc-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21593v1",
      "published_date": "2025-09-25 21:03:57 UTC",
      "updated_date": "2025-09-25 21:03:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:08:33.289147+00:00"
    },
    {
      "arxiv_id": "2509.21592v1",
      "title": "What Happens Next? Anticipating Future Motion by Generating Point Trajectories",
      "title_zh": "æ¥ä¸‹æ¥ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿé€šè¿‡ç”Ÿæˆç‚¹è½¨è¿¹é¢„æµ‹æœªæ¥è¿åŠ¨",
      "authors": [
        "Gabrijel Boduljak",
        "Laurynas Karazija",
        "Iro Laina",
        "Christian Rupprecht",
        "Andrea Vedaldi"
      ],
      "abstract": "We consider the problem of forecasting motion from a single image, i.e., predicting how objects in the world are likely to move, without the ability to observe other parameters such as the object velocities or the forces applied to them. We formulate this task as conditional generation of dense trajectory grids with a model that closely follows the architecture of modern video generators but outputs motion trajectories instead of pixels. This approach captures scene-wide dynamics and uncertainty, yielding more accurate and diverse predictions than prior regressors and generators. We extensively evaluate our method on simulated data, demonstrate its effectiveness on downstream applications such as robotics, and show promising accuracy on real-world intuitive physics datasets. Although recent state-of-the-art video generators are often regarded as world models, we show that they struggle with forecasting motion from a single image, even in simple physical scenarios such as falling blocks or mechanical object interactions, despite fine-tuning on such data. We show that this limitation arises from the overhead of generating pixels rather than directly modeling motion.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä»…å‡­å•å¼ å›¾åƒé¢„æµ‹ç‰©ä½“æœªæ¥è¿åŠ¨çš„æŒ‘æˆ˜ï¼Œå³åœ¨ç¼ºä¹é€Ÿåº¦æˆ–å—åŠ›ä¿¡æ¯çš„æƒ…å†µä¸‹é¢„æµ‹åœºæ™¯çš„åŠ¨æ€æ¼”å˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æœ‰æ¡ä»¶ç”Ÿæˆå¯†é›†è½¨è¿¹ç½‘æ ¼ (dense trajectory grids) çš„æ–¹æ³•ï¼Œå…¶æ¶æ„å€Ÿé‰´äº†ç°ä»£è§†é¢‘ç”Ÿæˆå™¨ï¼Œä½†é€šè¿‡ç›´æ¥è¾“å‡ºè¿åŠ¨è½¨è¿¹è€Œéåƒç´ æ¥æ•æ‰å…¨åœºæ™¯åŠ¨æ€ä¸ä¸ç¡®å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿæ•°æ®ã€æœºå™¨äººåº”ç”¨åŠçœŸå®ä¸–ç•Œç‰©ç†æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºä¼ ç»Ÿçš„å›å½’å™¨å’Œç”Ÿæˆå™¨ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œå³ä¾¿æ˜¯è¢«è§†ä¸ºä¸–ç•Œæ¨¡å‹ (world models) çš„å…ˆè¿›è§†é¢‘ç”Ÿæˆå™¨ï¼Œåœ¨å¤„ç†å•å›¾è¿åŠ¨é¢„æµ‹æ—¶ä¹Ÿè¡¨ç°ä¸ä½³ã€‚è¿™ç§å±€é™æ€§ä¸»è¦æºäºç”Ÿæˆåƒç´ æ‰€å¸¦æ¥çš„è®¡ç®—å¼€é”€ï¼Œè¯æ˜äº†ç›´æ¥å»ºæ¨¡è¿åŠ¨è½¨è¿¹èƒ½æ›´æœ‰æ•ˆåœ°å­¦ä¹ ç‰©ç†ä¸–ç•Œçš„æ¼”å˜è§„å¾‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21592v1",
      "published_date": "2025-09-25 21:03:56 UTC",
      "updated_date": "2025-09-25 21:03:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:08:39.963591+00:00"
    },
    {
      "arxiv_id": "2509.21573v1",
      "title": "Enhancing Contrastive Learning for Geolocalization by Discovering Hard Negatives on Semivariograms",
      "title_zh": "åŸºäºåŠå˜å¼‚å‡½æ•°æŒ–æ˜éš¾è´Ÿæ ·æœ¬ä»¥å¢å¼ºåœ°ç†å®šä½å¯¹æ¯”å­¦ä¹ ",
      "authors": [
        "Boyi Chen",
        "Zhangyu Wang",
        "Fabian Deuser",
        "Johann Maximilian Zollner",
        "Martin Werner"
      ],
      "abstract": "Accurate and robust image-based geo-localization at a global scale is challenging due to diverse environments, visually ambiguous scenes, and the lack of distinctive landmarks in many regions. While contrastive learning methods show promising performance by aligning features between street-view images and corresponding locations, they neglect the underlying spatial dependency in the geographic space. As a result, they fail to address the issue of false negatives -- image pairs that are both visually and geographically similar but labeled as negatives, and struggle to effectively distinguish hard negatives, which are visually similar but geographically distant. To address this issue, we propose a novel spatially regularized contrastive learning strategy that integrates a semivariogram, which is a geostatistical tool for modeling how spatial correlation changes with distance. We fit the semivariogram by relating the distance of images in feature space to their geographical distance, capturing the expected visual content in a spatial correlation. With the fitted semivariogram, we define the expected visual dissimilarity at a given spatial distance as reference to identify hard negatives and false negatives. We integrate this strategy into GeoCLIP and evaluate it on the OSV5M dataset, demonstrating that explicitly modeling spatial priors improves image-based geo-localization performance, particularly at finer granularity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨çƒå°ºåº¦ä¸‹å›¾åƒåœ°ç†å®šä½(geo-localization)é¢ä¸´çš„ç¯å¢ƒå¤šæ ·æ€§å’Œè§†è§‰æ­§ä¹‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆåŠå˜å¼‚å‡½æ•°(semivariogram)æ¥å‘ç°éš¾è´Ÿæ ·æœ¬(hard negatives)çš„ç©ºé—´æ­£åˆ™åŒ–å¯¹æ¯”å­¦ä¹ (contrastive learning)ç­–ç•¥ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å¿½ç•¥ç©ºé—´ç›¸å…³æ€§(spatial dependency)å¯¼è‡´éš¾ä»¥åŒºåˆ†ä¼ªè´Ÿæ ·æœ¬(false negatives)çš„ç¼ºé™·ï¼Œè¯¥ç ”ç©¶åˆ©ç”¨åŠå˜å¼‚å‡½æ•°è¿™ä¸€åœ°ç»Ÿè®¡å­¦å·¥å…·å»ºæ¨¡ç‰¹å¾ç©ºé—´è·ç¦»ä¸åœ°ç†è·ç¦»ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œç¡®å®šç‰¹å®šç©ºé—´è·ç¦»ä¸‹çš„é¢„æœŸè§†è§‰å·®å¼‚ã€‚é€šè¿‡å°†è¯¥ç­–ç•¥é›†æˆè‡³GeoCLIPæ¡†æ¶å¹¶åœ¨OSV5Mæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶è¯æ˜äº†æ˜¾å¼å»ºæ¨¡ç©ºé—´å…ˆéªŒ(spatial priors)èƒ½å¤Ÿæ˜¾è‘—ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æå‡åœ°ç†å®šä½å‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨ç²¾ç»†ç²’åº¦(finer granularity)çš„å®šä½ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21573v1",
      "published_date": "2025-09-25 20:53:06 UTC",
      "updated_date": "2025-09-25 20:53:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:08:42.763519+00:00"
    },
    {
      "arxiv_id": "2509.21567v2",
      "title": "EEG-Based Consumer Behaviour Prediction: An Exploration from Classical Machine Learning to Graph Neural Networks",
      "title_zh": "åŸºäº EEG çš„æ¶ˆè´¹è€…è¡Œä¸ºé¢„æµ‹ï¼šä»ç»å…¸æœºå™¨å­¦ä¹ åˆ°å›¾ç¥ç»ç½‘ç»œçš„æ¢ç´¢",
      "authors": [
        "Mohammad Parsa Afshar",
        "Aryan Azimi"
      ],
      "abstract": "Prediction of consumer behavior is one of the important purposes in marketing, cognitive neuroscience, and human-computer interaction. The electroencephalography (EEG) data can help analyze the decision process by providing detailed information about the brain's neural activity. In this research, a comparative approach is utilized for predicting consumer behavior by EEG data. In the first step, the features of the EEG data from the NeuMa dataset were extracted and cleaned. For the Graph Neural Network (GNN) models, the brain connectivity features were created. Different machine learning models, such as classical models and Graph Neural Networks, are used and compared. The GNN models with different architectures are implemented to have a comprehensive comparison; furthermore, a wide range of classical models, such as ensemble models, are applied, which can be very helpful to show the difference and performance of each model on the dataset. Although the results did not show a significant difference overall, the GNN models generally performed better in some basic criteria where classical models were not satisfactory. This study not only shows that combining EEG signal analysis and machine learning models can provide an approach to deeper understanding of consumer behavior, but also provides a comprehensive comparison between the machine learning models that have been widely used in previous studies in the EEG-based neuromarketing such as Support Vector Machine (SVM), and the models which are not used or rarely used in the field, like Graph Neural Networks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨è„‘ç”µå›¾(EEG)æ•°æ®é¢„æµ‹æ¶ˆè´¹è€…è¡Œä¸ºçš„æ–¹æ³•ï¼Œé€šè¿‡æ¯”è¾ƒç»å…¸æœºå™¨å­¦ä¹ æ¨¡å‹ä¸å›¾ç¥ç»ç½‘ç»œ(GNN)åœ¨NeuMaæ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œæ­ç¤ºäº†ç¥ç»æ´»åŠ¨ä¸å†³ç­–è¿‡ç¨‹ä¹‹é—´çš„è”ç³»ã€‚ç ”ç©¶äººå‘˜å¯¹EEGä¿¡å·è¿›è¡Œäº†ç‰¹å¾æå–ä¸æ¸…æ´—ï¼Œå¹¶é’ˆå¯¹GNNæ¨¡å‹æ„å»ºäº†è„‘è¿æ¥ç‰¹å¾ï¼Œå¯¹æ¯”äº†åŒ…æ‹¬æ”¯æŒå‘é‡æœº(SVM)ã€é›†æˆå­¦ä¹ æ¨¡å‹ä»¥åŠå¤šç§æ¶æ„çš„GNNåœ¨å†…çš„å¤šç§ç®—æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ•´ä½“å·®å¼‚ä¸æ˜¾è‘—ï¼Œä½†GNNåœ¨æŸäº›ç»å…¸æ¨¡å‹è¡¨ç°æ¬ ä½³çš„åŸºç¡€æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºæ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ä»…è¯æ˜äº†ç»“åˆEEGåˆ†æä¸æœºå™¨å­¦ä¹ æ¨¡å‹èƒ½æ›´æ·±å±‚æ¬¡åœ°ç†è§£æ¶ˆè´¹è€…è¡Œä¸ºï¼Œè¿˜ä¸ºç¥ç»è¥é”€(neuromarketing)é¢†åŸŸæä¾›äº†ä¼ ç»Ÿæ¨¡å‹ä¸GNNä¹‹é—´çš„å…¨é¢å¯¹æ¯”å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21567v2",
      "published_date": "2025-09-25 20:50:29 UTC",
      "updated_date": "2025-10-22 12:22:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:08:48.178205+00:00"
    },
    {
      "arxiv_id": "2510.02324v2",
      "title": "Hallucination reduction with CASAL: Contrastive Activation Steering For Amortized Learning",
      "title_zh": "åˆ©ç”¨ CASAL ç¼“è§£å¹»è§‰ï¼šé¢å‘æ‘Šé”€å­¦ä¹ çš„å¯¹æ¯”æ¿€æ´»å¼•å¯¼",
      "authors": [
        "Wannan",
        "Yang",
        "Xinchi Qiu",
        "Lei Yu",
        "Yuchen Zhang",
        "Aobo Yang",
        "Narine Kokhlikyan",
        "Nicola Cancedda",
        "Diego Garcia-Olano"
      ],
      "abstract": "Large Language Models (LLMs) exhibit impressive capabilities but often hallucinate, confidently providing incorrect answers instead of admitting ignorance. Prior work has shown that models encode linear representations of their own knowledge and that activation steering can reduce hallucinations. These approaches, however, require real-time monitoring and intervention during inference. We introduce Contrastive Activation Steering for Amortized Learning (CASAL), an efficient algorithm that connects interpretability with amortized optimization. CASAL directly bakes the benefits of activation steering into model's weights. Once trained, LLMs answer questions they know while abstaining from answering those they do not. CASAL's light-weight design requires training only a submodule of a single transformer layer and yet reduces hallucination by 30%-40% across multiple short-form QA benchmarks. CASAL is 30x more compute-efficient and 20x more data-efficient than strong LoRA-based baselines such as SFT and DPO, boosting its practical applicability in data scarce domains. Importantly, CASAL also generalizes effectively to out-of-distribution (OOD) domains. We showcase CASAL's flexibility in mitigating hallucinations in both text-only and vision-language models. To our knowledge, CASAL is the first steering-based training method that has been shown to be effective for both dense and Mixture-of-Experts (MoE) models. CASAL represents a promising step forward for applying interpretability-inspired method for practical deployment in production systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ™®éå­˜åœ¨çš„å¹»è§‰é—®é¢˜ï¼Œæå‡ºäº† CASAL (Contrastive Activation Steering For Amortized Learning) ç®—æ³•ï¼Œæ—¨åœ¨å°†å¯è§£é‡Šæ€§ä¸æ‘Šé”€ä¼˜åŒ– (amortized optimization) ç›¸ç»“åˆã€‚CASAL å·§å¦™åœ°å°†æ¿€æ´»å¼•å¯¼ (activation steering) çš„ä¼˜åŠ¿ç›´æ¥å›ºåŒ–åˆ°æ¨¡å‹æƒé‡ä¸­ï¼Œä½¿æ¨¡å‹åœ¨æ— éœ€æ¨ç†å®æ—¶å¹²é¢„çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿå‡†ç¡®åŒºåˆ†å·²çŸ¥ä¸æœªçŸ¥ã€‚è¯¥æ–¹æ³•è®¾è®¡æå…¶è½»é‡åŒ–ï¼Œä»…éœ€è®­ç»ƒå•ä¸ª Transformer å±‚çš„å­æ¨¡å—ï¼Œå³å¯åœ¨å¤šé¡¹çŸ­é—®ç­”åŸºå‡†æµ‹è¯•ä¸­é™ä½ 30%-40% çš„å¹»è§‰ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCASAL çš„è®¡ç®—æ•ˆç‡å’Œæ•°æ®æ•ˆç‡åˆ†åˆ«ä¼˜äº LoRAã€SFT å’Œ DPO ç­‰åŸºçº¿æ–¹æ³• 30 å€å’Œ 20 å€ï¼Œä¸”åœ¨åˆ†å¸ƒå¤– (OOD) é¢†åŸŸå…·æœ‰æå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒCASAL æ˜¯é¦–ä¸ªè¢«è¯å®å¯¹ç¨ å¯†æ¨¡å‹ã€æ··åˆä¸“å®¶æ¨¡å‹ (MoE) åŠè§†è§‰è¯­è¨€æ¨¡å‹å‡æœ‰æ•ˆçš„å¼•å¯¼è®­ç»ƒæ–¹æ³•ï¼Œä¸ºåœ¨å¤§è§„æ¨¡ç”Ÿäº§ç³»ç»Ÿä¸­éƒ¨ç½²å¯é æ¨¡å‹æä¾›äº†é«˜æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02324v2",
      "published_date": "2025-09-25 20:49:02 UTC",
      "updated_date": "2025-12-06 20:04:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:08:54.261851+00:00"
    },
    {
      "arxiv_id": "2509.21565v1",
      "title": "No Alignment Needed for Generation: Learning Linearly Separable Representations in Diffusion Models",
      "title_zh": "ç”Ÿæˆæ— éœ€å¯¹é½ï¼šåœ¨æ‰©æ•£æ¨¡å‹ä¸­å­¦ä¹ çº¿æ€§å¯åˆ†è¡¨å¾",
      "authors": [
        "Junno Yun",
        "YaÅŸar Utku AlÃ§alar",
        "Mehmet AkÃ§akaya"
      ],
      "abstract": "Efficient training strategies for large-scale diffusion models have recently emphasized the importance of improving discriminative feature representations in these models. A central line of work in this direction is representation alignment with features obtained from powerful external encoders, which improves the representation quality as assessed through linear probing. Alignment-based approaches show promise but depend on large pretrained encoders, which are computationally expensive to obtain. In this work, we propose an alternative regularization for training, based on promoting the Linear SEParability (LSEP) of intermediate layer representations. LSEP eliminates the need for an auxiliary encoder and representation alignment, while incorporating linear probing directly into the network's learning dynamics rather than treating it as a simple post-hoc evaluation tool. Our results demonstrate substantial improvements in both training efficiency and generation quality on flow-based transformer architectures such as SiTs, achieving an FID of 1.46 on $256 \\times 256$ ImageNet dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹(Diffusion Models)ä¸­åˆ¤åˆ«æ€§ç‰¹å¾è¡¨ç¤ºçš„æå‡æ–¹æ³•ï¼Œé’ˆå¯¹ç°æœ‰è¡¨å¾å¯¹é½(Representation Alignment)æŠ€æœ¯è¿‡åº¦ä¾èµ–æ˜‚è´µå¤–éƒ¨ç¼–ç å™¨çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºLSEP (Linear SEParability)çš„è®­ç»ƒæ­£åˆ™åŒ–ç­–ç•¥ã€‚LSEPé€šè¿‡ä¿ƒè¿›ä¸­é—´å±‚è¡¨å¾çš„çº¿æ€§å¯åˆ†æ€§ï¼Œæ¶ˆé™¤äº†å¯¹è¾…åŠ©ç¼–ç å™¨å’Œè¡¨å¾å¯¹é½çš„éœ€æ±‚ï¼Œå¹¶å°†çº¿æ€§æ¢æµ‹(Linear Probing)ç›´æ¥èå…¥ç½‘ç»œçš„å­¦ä¹ åŠ¨åŠ›å­¦ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨SiTsç­‰åŸºäºæµçš„Transformeræ¶æ„ä¸Šæ˜¾è‘—æå‡äº†è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆè´¨é‡ã€‚æœ€ç»ˆï¼Œè¯¥ç ”ç©¶åœ¨256x256 ImageNetæ•°æ®é›†ä¸Šå®ç°äº†1.46çš„FIDè¯„åˆ†ï¼Œè¯æ˜äº†æ— éœ€å¤–éƒ¨å¯¹é½å³å¯å­¦ä¹ åˆ°é«˜æ€§èƒ½çš„ç”Ÿæˆè¡¨ç¤ºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21565v1",
      "published_date": "2025-09-25 20:46:48 UTC",
      "updated_date": "2025-09-25 20:46:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:08:50.757173+00:00"
    },
    {
      "arxiv_id": "2509.21554v1",
      "title": "Domain-Aware Speaker Diarization On African-Accented English",
      "title_zh": "é¢å‘éæ´²å£éŸ³è‹±è¯­çš„é¢†åŸŸæ„ŸçŸ¥è¯´è¯äººæ—¥å¿—",
      "authors": [
        "Chibuzor Okocha",
        "Kelechi Ezema",
        "Christan Grant"
      ],
      "abstract": "This study examines domain effects in speaker diarization for African-accented English. We evaluate multiple production and open systems on general and clinical dialogues under a strict DER protocol that scores overlap. A consistent domain penalty appears for clinical speech and remains significant across models. Error analysis attributes much of this penalty to false alarms and missed detections, aligning with short turns and frequent overlap. We test lightweight domain adaptation by fine-tuning a segmentation module on accent-matched data; it reduces error but does not eliminate the gap. Our contributions include a controlled benchmark across domains, a concise approach to error decomposition and conversation-level profiling, and an adaptation recipe that is easy to reproduce. Results point to overlap-aware segmentation and balanced clinical resources as practical next steps.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨éæ´²å£éŸ³è‹±è¯­(African-accented English)ç¯å¢ƒä¸‹ï¼Œé¢†åŸŸæ•ˆåº”(domain effects)å¯¹è¯´è¯äººæ—¥å¿—(Speaker Diarization)ç³»ç»Ÿæ€§èƒ½çš„å½±å“ã€‚ä½œè€…åœ¨é€šç”¨å’Œä¸´åºŠå¯¹è¯åœºæ™¯ä¸‹ï¼Œé‡‡ç”¨ä¸¥æ ¼è®¡ç®—é‡å è¯­éŸ³çš„è¯´è¯äººé”™è¯¯ç‡(DER)åè®®ï¼Œå¯¹å¤šç§å•†ç”¨å’Œå¼€æºç³»ç»Ÿè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸´åºŠè¯­éŸ³åœºæ™¯å­˜åœ¨æ˜¾è‘—çš„é¢†åŸŸæƒ©ç½š(domain penalty)ï¼Œä¸”åœ¨ä¸åŒæ¨¡å‹é—´è¡¨ç°ä¸€è‡´ã€‚é”™è¯¯åˆ†æè¡¨æ˜ï¼Œç”±äºä¸´åºŠå¯¹è¯ä¸­å­˜åœ¨å¤§é‡çŸ­è½®æ¬¡(short turns)å’Œé¢‘ç¹çš„é‡å è¯­éŸ³(overlap)ï¼Œå¯¼è‡´äº†å¤§é‡çš„è™šè­¦(false alarms)å’Œæ¼æ£€(missed detections)ã€‚ç ”ç©¶å°è¯•é€šè¿‡åœ¨å£éŸ³åŒ¹é…çš„æ•°æ®ä¸Šå¾®è°ƒåˆ†å‰²æ¨¡å—(segmentation module)è¿›è¡Œè½»é‡çº§é¢†åŸŸé€‚é…ï¼Œè™½ç„¶é™ä½äº†é”™è¯¯ç‡ï¼Œä½†ä»æœªèƒ½å®Œå…¨æ¶ˆé™¤æ€§èƒ½å·®è·ã€‚è¯¥è®ºæ–‡çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬å»ºç«‹äº†è·¨é¢†åŸŸçš„å—æ§åŸºå‡†ã€æå‡ºäº†ç®€æ´çš„é”™è¯¯åˆ†è§£ä¸å¯¹è¯çº§åˆ†ææ–¹æ³•ï¼Œå¹¶å»ºè®®æœªæ¥çš„ç ”ç©¶é‡ç‚¹åº”æ”¾åœ¨æ„ŸçŸ¥é‡å çš„åˆ†å‰²(overlap-aware segmentation)å’Œå¹³è¡¡ä¸´åºŠèµ„æºä¸Šã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.21554v1",
      "published_date": "2025-09-25 20:38:51 UTC",
      "updated_date": "2025-09-25 20:38:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:10:08.799774+00:00"
    },
    {
      "arxiv_id": "2509.21553v1",
      "title": "AutoClimDS: Climate Data Science Agentic AI -- A Knowledge Graph is All You Need",
      "title_zh": "AutoClimDSï¼šæ°”å€™æ•°æ®ç§‘å­¦æ™ºèƒ½ä½“AIâ€”â€”åªéœ€çŸ¥è¯†å›¾è°±",
      "authors": [
        "Ahmed Jaber",
        "Wangshu Zhu",
        "Karthick Jayavelu",
        "Justin Downes",
        "Sameer Mohamed",
        "Candace Agonafir",
        "Linnia Hawkins",
        "Tian Zheng"
      ],
      "abstract": "Climate data science faces persistent barriers stemming from the fragmented nature of data sources, heterogeneous formats, and the steep technical expertise required to identify, acquire, and process datasets. These challenges limit participation, slow discovery, and reduce the reproducibility of scientific workflows. In this paper, we present a proof of concept for addressing these barriers through the integration of a curated knowledge graph (KG) with AI agents designed for cloud-native scientific workflows. The KG provides a unifying layer that organizes datasets, tools, and workflows, while AI agents -- powered by generative AI services -- enable natural language interaction, automated data access, and streamlined analysis. Together, these components drastically lower the technical threshold for engaging in climate data science, enabling non-specialist users to identify and analyze relevant datasets. By leveraging existing cloud-ready API data portals, we demonstrate that \"a knowledge graph is all you need\" to unlock scalable and agentic workflows for scientific inquiry. The open-source design of our system further supports community contributions, ensuring that the KG and associated tools can evolve as a shared commons. Our results illustrate a pathway toward democratizing access to climate data and establishing a reproducible, extensible framework for human--AI collaboration in scientific research.",
      "tldr_zh": "æ°”å€™æ•°æ®ç§‘å­¦ç›®å‰é¢ä¸´æ•°æ®ç¢ç‰‡åŒ–ã€æ ¼å¼å¤šæ ·åŒ–ä»¥åŠå¤„ç†æ•°æ®é›†æ‰€éœ€çš„é«˜æŠ€æœ¯é—¨æ§›ç­‰ä¸¥å³»æŒ‘æˆ˜ï¼Œé™åˆ¶äº†ç§‘å­¦å‘ç°çš„æ•ˆç‡ã€‚è¯¥ç ”ç©¶æå‡ºäº† AutoClimDS æ¦‚å¿µéªŒè¯ç³»ç»Ÿï¼Œé€šè¿‡å°†ç²¾é€‰çš„çŸ¥è¯†å›¾è°± (Knowledge Graph, KG) ä¸ä¸“ä¸ºäº‘åŸç”Ÿç§‘å­¦å·¥ä½œæµè®¾è®¡çš„ AI Agents ç›¸ç»“åˆæ¥åº”å¯¹è¿™äº›éš¾é¢˜ã€‚å…¶ä¸­ï¼ŒKG ä½œä¸ºä¸€ä¸ªç»Ÿä¸€å±‚è´Ÿè´£ç»„ç»‡æ•°æ®é›†ã€å·¥å…·å’Œå·¥ä½œæµï¼Œè€Œ AI Agents åˆ™åˆ©ç”¨ç”Ÿæˆå¼ AI æŠ€æœ¯å®ç°è‡ªç„¶è¯­è¨€äº¤äº’ã€è‡ªåŠ¨æ•°æ®è®¿é—®å’Œæµç¨‹åŒ–åˆ†æã€‚è¿™ä¸€ç³»ç»Ÿæ˜¾è‘—é™ä½äº†å‚ä¸æ°”å€™æ•°æ®ç§‘å­¦çš„æŠ€æœ¯é—¨æ§›ï¼Œä½¿éä¸“å®¶ç”¨æˆ·ä¹Ÿèƒ½é«˜æ•ˆè¯†åˆ«å¹¶åˆ†æç›¸å…³æ•°æ®é›†ã€‚é€šè¿‡åˆ©ç”¨ç°æœ‰çš„äº‘å°±ç»ª API æ•°æ®é—¨æˆ·ï¼Œè¯¥ç ”ç©¶è¯æ˜äº†â€œçŸ¥è¯†å›¾è°±æ˜¯è§£é”å¯æ‰©å±•å’Œ Agentic workflows è¿›è¡Œç§‘å­¦æ¢ç©¶çš„æ ¸å¿ƒâ€ã€‚ç³»ç»Ÿçš„å¼€æºè®¾è®¡è¿›ä¸€æ­¥æ”¯æŒäº†ç¤¾åŒºåä½œï¼Œä¸ºå®ç°æ°”å€™æ•°æ®è·å–çš„æ°‘ä¸»åŒ–ä»¥åŠå»ºç«‹å¯é‡å¤ã€å¯æ‰©å±•çš„äººæœºåä½œç§‘ç ”æ¡†æ¶å¼€è¾Ÿäº†é“è·¯ã€‚",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.HC",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21553v1",
      "published_date": "2025-09-25 20:38:23 UTC",
      "updated_date": "2025-09-25 20:38:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:09:12.869620+00:00"
    },
    {
      "arxiv_id": "2509.21549v2",
      "title": "Correct Reasoning Paths Visit Shared Decision Pivots",
      "title_zh": "æ­£ç¡®æ¨ç†è·¯å¾„äº¤æ±‡äºå…±äº«å†³ç­–æ”¯ç‚¹",
      "authors": [
        "Dongkyu Cho",
        "Amy B. Z. Zhang",
        "Bilel Fehri",
        "Sheng Wang",
        "Rumi Chunara",
        "Rui Song",
        "Hengrui Cai"
      ],
      "abstract": "Chain-of-thought (CoT) reasoning exposes the intermediate thinking process of large language models (LLMs), yet verifying those traces at scale remains unsolved. In response, we introduce the idea of decision pivots-minimal, verifiable checkpoints that any correct reasoning path must visit. We hypothesize that correct reasoning, though stylistically diverse, converge on the same pivot set, while incorrect ones violate at least one pivot. Leveraging this property, we propose a self-training pipeline that (i) samples diverse reasoning paths and mines shared decision pivots, (ii) compresses each trace into pivot-focused short-path reasoning using an auxiliary verifier, and (iii) post-trains the model using its self-generated outputs. The proposed method aligns reasoning without ground truth reasoning data or external metrics. Experiments on standard benchmarks such as LogiQA, MedQA, and MATH500 show the effectiveness of our method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é“¾å¼æ€ç»´(Chain-of-thought)æ¨ç†è½¨è¿¹éªŒè¯æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†å†³ç­–å…³é”®ç‚¹(decision pivots)çš„æ¦‚å¿µï¼Œå³æ­£ç¡®æ¨ç†è·¯å¾„å¿…é¡»ç»è¿‡çš„æœ€å°å¯éªŒè¯æ£€æŸ¥ç‚¹ã€‚ç ”ç©¶å‡è®¾æ­£ç¡®çš„æ¨ç†è·¯å¾„è™½ç„¶é£æ ¼å„å¼‚ï¼Œä½†ä¼šæ”¶æ•›äºç›¸åŒçš„å…³é”®ç‚¹é›†ï¼Œè€Œé”™è¯¯çš„è·¯å¾„åˆ™ä¼šè¿åè‡³å°‘ä¸€ä¸ªå…³é”®ç‚¹ã€‚åŸºäºè¿™ä¸€ç‰¹æ€§ï¼Œä½œè€…å¼€å‘äº†ä¸€ç§è‡ªè®­ç»ƒæµæ°´çº¿ï¼Œé€šè¿‡æŒ–æ˜å…±äº«å†³ç­–å…³é”®ç‚¹å¹¶å°†æ¨ç†è½¨è¿¹å‹ç¼©ä¸ºçŸ­è·¯å¾„æ¨ç†ï¼Œåˆ©ç”¨æ¨¡å‹è‡ªèº«ç”Ÿæˆçš„è¾“å‡ºè¿›è¡ŒåæœŸè®­ç»ƒã€‚è¯¥æ–¹æ³•æ— éœ€åœ°é¢çœŸå€¼(ground truth)æ¨ç†æ•°æ®æˆ–å¤–éƒ¨è¯„ä¼°æŒ‡æ ‡å³å¯å®ç°æ¨ç†èƒ½åŠ›çš„å¯¹é½ä¸æå‡ã€‚åœ¨LogiQAã€MedQAå’ŒMATH500ç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤§è§„æ¨¡éªŒè¯å’Œä¼˜åŒ–å¤æ‚æ¨ç†è·¯å¾„æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.21549v2",
      "published_date": "2025-09-25 20:34:45 UTC",
      "updated_date": "2025-10-26 14:08:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:10:11.290469+00:00"
    },
    {
      "arxiv_id": "2511.03729v1",
      "title": "Beyond Chat: a Framework for LLMs as Human-Centered Support Systems",
      "title_zh": "è¶…è¶Šå¯¹è¯ï¼šä»¥äººä¸ºä¸­å¿ƒçš„ LLMs æ”¯æŒç³»ç»Ÿæ¡†æ¶",
      "authors": [
        "Zhiyin Zhou"
      ],
      "abstract": "Large language models are moving beyond transactional question answering to act as companions, coaches, mediators, and curators that scaffold human growth, decision-making, and well-being. This paper proposes a role-based framework for human-centered LLM support systems, compares real deployments across domains, and identifies cross-cutting design principles: transparency, personalization, guardrails, memory with privacy, and a balance of empathy and reliability. It outlines evaluation metrics that extend beyond accuracy to trust, engagement, and longitudinal outcomes. It also analyzes risks including over-reliance, hallucination, bias, privacy exposure, and unequal access, and proposes future directions spanning unified evaluation, hybrid human-AI models, memory architectures, cross-domain benchmarking, and governance. The goal is to support responsible integration of LLMs in sensitive settings where people need accompaniment and guidance, not only answers.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé¢å‘ä»¥äººä¸ºä¸­å¿ƒçš„ Large Language Models (LLMs) æ”¯æŒç³»ç»Ÿçš„è§’è‰²æ¡†æ¶ï¼Œæ¢è®¨æ¨¡å‹å¦‚ä½•ä»äº¤æ˜“å‹é—®ç­”æ¼”å˜ä¸ºé™ªä¼´è€…ã€æ•™ç»ƒå’Œè°ƒè§£å‘˜ï¼Œä»¥è¾…åŠ©äººç±»æˆé•¿ä¸å†³ç­–ã€‚è¯¥ Framework é€šè¿‡å¯¹æ¯”è·¨é¢†åŸŸçš„éƒ¨ç½²æ¡ˆä¾‹ï¼Œç¡®ç«‹äº†é€æ˜åº¦ (transparency)ã€ä¸ªæ€§åŒ– (personalization)ã€å®‰å…¨æŠ¤æ  (guardrails) ä»¥åŠåŒç†å¿ƒä¸å¯é æ€§å¹³è¡¡ç­‰å…³é”®è®¾è®¡åŸåˆ™ã€‚è®ºæ–‡ä¸ä»…å®šä¹‰äº†æ¶µç›–ä¿¡ä»» (trust) å’Œå‚ä¸åº¦ (engagement) çš„å¤šç»´è¯„ä¼°æŒ‡æ ‡ï¼Œè¿˜æ·±å…¥å‰–æäº†è¿‡åº¦ä¾èµ– (over-reliance)ã€å¹»è§‰ (hallucination) åŠéšç§æ³„éœ²ç­‰ç³»ç»Ÿæ€§é£é™©ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ¢è®¨äº†äººæœºæ··åˆæ¨¡å‹ (hybrid human-AI models)ã€è®°å¿†æ¶æ„ (memory architectures) ä¸æ²»ç†ç­‰å‰ç»æ€§æ–¹å‘ã€‚è¯¥ç ”ç©¶æ—¨åœ¨æ¨åŠ¨ LLMs åœ¨æ•æ„Ÿåœºæ™¯ä¸‹çš„è´Ÿè´£ä»»é›†æˆï¼Œä½¿å…¶åœ¨æä¾›ç­”æ¡ˆä¹‹ä½™ï¼Œæ›´èƒ½å‘æŒ¥é™ªä¼´ (accompaniment) ä¸å¼•å¯¼çš„æ ¸å¿ƒä»·å€¼ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.03729v1",
      "published_date": "2025-09-25 20:33:58 UTC",
      "updated_date": "2025-09-25 20:33:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:09:22.098245+00:00"
    },
    {
      "arxiv_id": "2509.21542v1",
      "title": "Psychological and behavioural responses in human-agent vs. human-human interactions: a systematic review and meta-analysis",
      "title_zh": "äºº-æ™ºèƒ½ä½“ä¸äºº-äººäº¤äº’ä¸­çš„å¿ƒç†å’Œè¡Œä¸ºååº”ï¼šç³»ç»Ÿç»¼è¿°ä¸å…ƒåˆ†æ",
      "authors": [
        "Jianan Zhou",
        "Fleur Corbett",
        "Joori Byun",
        "Talya Porat",
        "Nejra van Zalk"
      ],
      "abstract": "Interactive intelligent agents are being integrated across society. Despite achieving human-like capabilities, humans' responses to these agents remain poorly understood, with research fragmented across disciplines. We conducted a first systematic synthesis comparing a range of psychological and behavioural responses in matched human-agent vs. human-human dyadic interactions. A total of 162 eligible studies (146 contributed to the meta-analysis; 468 effect sizes) were included in the systematic review and meta-analysis, which integrated frequentist and Bayesian approaches. Our results indicate that individuals exhibited less prosocial behaviour and moral engagement when interacting with agents vs. humans. They attributed less agency and responsibility to agents, perceiving them as less competent, likeable, and socially present. In contrast, individuals' social alignment (i.e., alignment or adaptation of internal states and behaviours with partners), trust in partners, personal agency, task performance, and interaction experiences were generally comparable when interacting with agents vs. humans. We observed high effect-size heterogeneity for many subjective responses (i.e., social perceptions of partners, subjective trust, and interaction experiences), suggesting context-dependency of partner effects. By examining the characteristics of studies, participants, partners, interaction scenarios, and response measures, we also identified several moderators shaping partner effects. Overall, functional behaviours and interactive experiences with agents can resemble those with humans, whereas fundamental social attributions and moral/prosocial concerns lag in human-agent interactions. Agents are thus afforded instrumental value on par with humans but lack comparable intrinsic value, providing practical implications for agent design and regulation.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹162é¡¹ç ”ç©¶è¿›è¡Œäº†ç³»ç»Ÿç»¼è¿°(systematic review)å’Œå…ƒåˆ†æ(meta-analysis)ï¼Œæ—¨åœ¨å…¨é¢æ¯”è¾ƒäººç±»ä¸æ™ºèƒ½ä»£ç†(human-agent)äº¤äº’åŠäººä¸äºº(human-human)äº¤äº’ä¸­çš„å¿ƒç†å’Œè¡Œä¸ºååº”å·®å¼‚ã€‚ç ”ç©¶å‘ç°ï¼Œä¸ªä½“åœ¨ä¸ä»£ç†äº¤äº’æ—¶è¡¨ç°å‡ºè¾ƒå°‘çš„äº²ç¤¾ä¼šè¡Œä¸º(prosocial behaviour)å’Œé“å¾·å‚ä¸(moral engagement)ï¼Œå¹¶å€¾å‘äºèµ‹äºˆä»£ç†è¾ƒä½çš„èƒ½åŠ¨æ€§(agency)ä¸è´£ä»»æ„Ÿã€‚å°½ç®¡ä»£ç†åœ¨èƒ½åŠ›(competent)å’Œç¤¾äº¤å­˜åœ¨æ„Ÿ(socially present)çš„è¯„ä»·ä¸Šè¾ƒä½ï¼Œä½†äººç±»ä¸å…¶åœ¨ç¤¾äº¤åè°ƒ(social alignment)ã€ä¿¡ä»»æ„Ÿ(trust)ã€ä»»åŠ¡è¡¨ç°(task performance)åŠäº¤äº’ä½“éªŒä¸Šå±•ç°å‡ºäº†ä¸äººé™…äº¤äº’ç›¸å½“çš„æ°´å¹³ã€‚ä¸»è§‚ååº”è¡¨ç°å‡ºé«˜åº¦çš„æ•ˆåº”å€¼å¼‚è´¨æ€§(effect-size heterogeneity)ï¼Œæš—ç¤ºäº†ä¼™ä¼´æ•ˆåº”(partner effects)å—ç‰¹å®šäº¤äº’æƒ…å¢ƒåŠå¤šç§è°ƒèŠ‚å˜é‡çš„å½±å“ã€‚æ€»ä½“è€Œè¨€ï¼Œæ™ºèƒ½ä»£ç†åœ¨åŠŸèƒ½ä»·å€¼(instrumental value)ä¸Šå·²å¯ä¸äººç±»åª²ç¾ï¼Œä½†åœ¨åŸºæœ¬ç¤¾ä¼šå½’å› å’Œå†…åœ¨ä»·å€¼(intrinsic value)æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚è¿™ä¸€ç»“è®ºä¸ºæ™ºèƒ½ä»£ç†çš„è®¾è®¡ä¸ç›‘ç®¡æä¾›äº†é‡è¦çš„ç†è®ºä¾æ®ï¼ŒæŒ‡æ˜äº†åœ¨ä¼˜åŒ–åŠŸèƒ½æ€§çš„åŒæ—¶éœ€å…³æ³¨ä»£ç†ç¤¾ä¼šå±æ€§æ„å»ºçš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21542v1",
      "published_date": "2025-09-25 20:29:36 UTC",
      "updated_date": "2025-09-25 20:29:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:09:23.887381+00:00"
    },
    {
      "arxiv_id": "2509.22736v1",
      "title": "Consistency Models as Plug-and-Play Priors for Inverse Problems",
      "title_zh": "ä¸€è‡´æ€§æ¨¡å‹ä½œä¸ºé€†é—®é¢˜çš„å³æ’å³ç”¨å…ˆéªŒ",
      "authors": [
        "Merve GÃ¼lle",
        "Junno Yun",
        "YaÅŸar Utku AlÃ§alar",
        "Mehmet AkÃ§akaya"
      ],
      "abstract": "Diffusion models have found extensive use in solving numerous inverse problems. Such diffusion inverse problem solvers aim to sample from the posterior distribution of data given the measurements, using a combination of the unconditional score function and an approximation of the posterior related to the forward process. Recently, consistency models (CMs) have been proposed to directly predict the final output from any point on the diffusion ODE trajectory, enabling high-quality sampling in just a few NFEs. CMs have also been utilized for inverse problems, but existing CM-based solvers either require additional task-specific training or utilize data fidelity operations with slow convergence, not amenable to large-scale problems. In this work, we reinterpret CMs as proximal operators of a prior, enabling their integration into plug-and-play (PnP) frameworks. We propose a solver based on PnP-ADMM, which enables us to leverage the fast convergence of conjugate gradient method. We further accelerate this with noise injection and momentum, dubbed PnP-CM, and show it maintains the convergence properties of the baseline PnP-ADMM. We evaluate our approach on a variety of inverse problems, including inpainting, super-resolution, Gaussian deblurring, and magnetic resonance imaging (MRI) reconstruction. To the best of our knowledge, this is the first CM trained for MRI datasets. Our results show that PnP-CM achieves high-quality reconstructions in as few as 4 NFEs, and can produce meaningful results in 2 steps, highlighting its effectiveness in real-world inverse problems while outperforming comparable CM-based approaches.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶å°†ä¸€è‡´æ€§æ¨¡å‹(Consistency Models, CMs)é‡æ–°è¯ é‡Šä¸ºå…ˆéªŒæ¦‚ç‡çš„è¿‘ç«¯ç®—å­(proximal operators)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºäºæ‰©æ•£æ¨¡å‹çš„é€†é—®é¢˜(inverse problems)æ±‚è§£å™¨æ”¶æ•›é€Ÿåº¦æ…¢æˆ–éœ€è¦ç‰¹å®šä»»åŠ¡è®­ç»ƒçš„é—®é¢˜ã€‚ç ”ç©¶æå‡ºäº†åä¸ºPnP-CMçš„æ±‚è§£å™¨ï¼Œé€šè¿‡å°†CMsé›†æˆåˆ°å³æ’å³ç”¨(Plug-and-Play, PnP)æ¡†æ¶ä¸­ï¼Œåˆ©ç”¨PnP-ADMMç®—æ³•å’Œå…±è½­æ¢¯åº¦æ³•(conjugate gradient method)åŠ é€Ÿæ”¶æ•›ï¼Œå¹¶é€šè¿‡å¼•å…¥å™ªå£°æ³¨å…¥(noise injection)å’ŒåŠ¨é‡(momentum)è¿›ä¸€æ­¥æå‡æ±‚è§£æ•ˆç‡ã€‚å®éªŒåœ¨å›¾åƒä¿®å¤(inpainting)ã€è¶…åˆ†è¾¨ç‡(super-resolution)ã€é«˜æ–¯å»å™ª(Gaussian deblurring)ä»¥åŠç£å…±æŒ¯æˆåƒ(MRI)é‡å»ºç­‰å¤šç§ä»»åŠ¡ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¯¥å·¥ä½œé¦–æ¬¡åœ¨MRIæ•°æ®é›†ä¸Šè®­ç»ƒäº†ä¸€è‡´æ€§æ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºPnP-CMä»…éœ€4æ¬¡å‡½æ•°è¯„ä¼°(NFEs)å³å¯å®ç°é«˜è´¨é‡é‡å»ºï¼Œç”šè‡³åœ¨2ä¸ªæ­¥éª¤å†…å°±èƒ½äº§ç”Ÿæœ‰æ„ä¹‰çš„ç»“æœã€‚ç›¸æ¯”åŒç±»åŸºäºCMsçš„æ–¹æ³•ï¼ŒPnP-CMåœ¨ä¿æŒæ”¶æ•›ç¨³å®šæ€§çš„åŒæ—¶æ˜¾è‘—æå‡äº†é‡å»ºè´¨é‡å’Œé€Ÿåº¦ï¼Œä¸ºé«˜æ•ˆè§£å†³ç°å®ä¸–ç•Œé€†é—®é¢˜æä¾›äº†æ–°çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "physics.med-ph",
        "stat.ML"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.22736v1",
      "published_date": "2025-09-25 20:27:56 UTC",
      "updated_date": "2025-09-25 20:27:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:09:29.675358+00:00"
    },
    {
      "arxiv_id": "2509.21535v2",
      "title": "Agribot: agriculture-specific question answer system",
      "title_zh": "Agribotï¼šå†œä¸šé¢†åŸŸä¸“ç”¨é—®ç­”ç³»ç»Ÿ",
      "authors": [
        "Naman Jain",
        "Pranjali Jain",
        "Pratik Kayal",
        "Jayakrishna Sahit",
        "Soham Pachpande",
        "Jayesh Choudhari",
        "Mayank Singh"
      ],
      "abstract": "India is an agro-based economy and proper information about agricultural practices is the key to optimal agricultural growth and output. In order to answer the queries of the farmer, we have build an agricultural chatbot based on the dataset from Kisan Call Center. This system is robust enough to answer queries related to weather, market rates, plant protection and government schemes. This system is available 24* 7, can be accessed through any electronic device and the information is delivered with the ease of understanding. The system is based on a sentence embedding model which gives an accuracy of 56%. After eliminating synonyms and incorporating entity extraction, the accuracy jumps to 86%. With such a system, farmers can progress towards easier information about farming related practices and hence a better agricultural output. The job of the Call Center workforce would be made easier and the hard work of various such workers can be redirected to a better goal.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†Agribotï¼Œä¸€ç§ä¸“é—¨é’ˆå¯¹å°åº¦å†œä¸šé¢†åŸŸçš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡Kisan Call Centerçš„æ•°æ®é›†ä¸ºå†œæ°‘æä¾›å…¨å¤©å€™çš„ä¿¡æ¯æ”¯æŒã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿç²¾å‡†å›ç­”æ¶‰åŠå¤©æ°”ã€å¸‚åœºä»·æ ¼ã€plant protectionä»¥åŠæ”¿åºœæ–¹æ¡ˆç­‰å…³é”®å†œä¸šé¢†åŸŸçš„æŸ¥è¯¢ï¼Œå¹¶æ”¯æŒå¤šç§ç”µå­è®¾å¤‡è®¿é—®ã€‚åœ¨æŠ€æœ¯å®ç°ä¸Šï¼Œè¯¥ç³»ç»Ÿé‡‡ç”¨sentence embeddingæ¨¡å‹ä½œä¸ºæ ¸å¿ƒï¼Œå¹¶é€šè¿‡åŒä¹‰è¯æ¶ˆé™¤å’Œentity extractionæŠ€æœ¯ï¼Œå°†é—®ç­”å‡†ç¡®ç‡ä»56%æ˜¾è‘—æå‡è‡³86%ã€‚Agribotçš„å¹¿æ³›åº”ç”¨ä¸ä»…èƒ½å¸®åŠ©å†œæ°‘ä¾¿æ·åœ°è·å–å†œä¸šçŸ¥è¯†ä»¥æå‡äº§å‡ºï¼Œè¿˜èƒ½æœ‰æ•ˆä¼˜åŒ–å†œä¸šå‘¼å«ä¸­å¿ƒçš„å·¥ä½œæµç¨‹ï¼Œå°†äººåŠ›èµ„æºå¯¼å‘æ›´å…·ä»·å€¼çš„ä»»åŠ¡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21535v2",
      "published_date": "2025-09-25 20:22:09 UTC",
      "updated_date": "2025-09-29 17:31:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:09:37.969610+00:00"
    },
    {
      "arxiv_id": "2509.21528v1",
      "title": "Preemptive Detection and Steering of LLM Misalignment via Latent Reachability",
      "title_zh": "åŸºäºéšç©ºé—´å¯è¾¾æ€§çš„å¤§è¯­è¨€æ¨¡å‹å¯¹é½å¤±æ•ˆé¢„åˆ¤æ£€æµ‹ä¸å¼•å¯¼",
      "authors": [
        "Sathwik Karnik",
        "Somil Bansal"
      ],
      "abstract": "Large language models (LLMs) are now ubiquitous in everyday tools, raising urgent safety concerns about their tendency to generate harmful content. The dominant safety approach -- reinforcement learning from human feedback (RLHF) -- effectively shapes model behavior during training but offers no safeguards at inference time, where unsafe continuations may still arise. We propose BRT-Align, a reachability-based framework that brings control-theoretic safety tools to LLM inference. BRT-Align models autoregressive generation as a dynamical system in latent space and learn a safety value function via backward reachability, estimating the worst-case evolution of a trajectory. This enables two complementary mechanisms: (1) a runtime monitor that forecasts unsafe completions several tokens in advance, and (2) a least-restrictive steering filter that minimally perturbs latent states to redirect generation away from unsafe regions. Experiments across multiple LLMs and toxicity benchmarks demonstrate that BRT-Align provides more accurate and earlier detection of unsafe continuations than baselines. Moreover, for LLM safety alignment, BRT-Align substantially reduces unsafe generations while preserving sentence diversity and coherence. Qualitative results further highlight emergent alignment properties: BRT-Align consistently produces responses that are less violent, less profane, less offensive, and less politically biased. Together, these findings demonstrate that reachability analysis provides a principled and practical foundation for inference-time LLM safety.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BRT-Alignï¼Œä¸€ä¸ªåŸºäºå¯è¾¾æ€§åˆ†æ (Reachability Analysis) çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) æä¾›æ¨ç†æ—¶çš„å®‰å…¨ä¿éšœï¼Œä»¥è§£å†³å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆ (RLHF) æ— æ³•åœ¨æ¨ç†é˜¶æ®µé¢„é˜²æœ‰å®³å†…å®¹ç”Ÿæˆçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†è‡ªå›å½’ç”Ÿæˆå»ºæ¨¡ä¸ºæ½œåœ¨ç©ºé—´ (Latent Space) ä¸­çš„åŠ¨åŠ›ç³»ç»Ÿï¼Œå¹¶é€šè¿‡åå‘å¯è¾¾æ€§ (Backward Reachability) å­¦ä¹ å®‰å…¨ä»·å€¼å‡½æ•°ï¼Œä»¥æ­¤ä¼°è®¡è½¨è¿¹çš„æœ€åæƒ…å†µæ¼”åŒ–ã€‚BRT-Align åŒ…å«ä¸¤ç§äº’è¡¥æœºåˆ¶ï¼šä¸€ç§æ˜¯èƒ½å¤Ÿæå‰æ•°ä¸ªæ ‡è®°é¢„æµ‹ä¸å®‰å…¨ç”Ÿæˆçš„è¿è¡Œç›‘æ§å™¨ï¼Œå¦ä¸€ç§æ˜¯èƒ½å¤Ÿä»¥æœ€å°æ‰°åŠ¨é‡å®šå‘ç”Ÿæˆæ–¹å‘çš„æœ€å°é™åˆ¶å¼•å¯¼è¿‡æ»¤å™¨ã€‚åœ¨å¤šç§å¤§è¯­è¨€æ¨¡å‹å’Œæ¯’æ€§åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒBRT-Align åœ¨æ£€æµ‹ä¸å®‰å…¨å†…å®¹æ–¹é¢æ¯”åŸºçº¿æ¨¡å‹æ›´å‡†ç¡®ä¸”æ›´åŠæ—¶ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—å‡å°‘æœ‰å®³ç”Ÿæˆçš„åŒæ—¶ï¼Œæœ‰æ•ˆåœ°ä¿ç•™äº†å¥å­çš„å¤šæ ·æ€§å’Œè¿è´¯æ€§ã€‚å®šæ€§ç»“æœè¿›ä¸€æ­¥æ˜¾ç¤ºï¼ŒBRT-Align ç”Ÿæˆçš„å›ç­”åœ¨æš´åŠ›ã€äºµæ¸ã€å†’çŠ¯æ€§å’Œæ”¿æ²»åè§æ–¹é¢å‡æœ‰æ‰€æ”¹å–„ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†å¯è¾¾æ€§åˆ†æèƒ½å¤Ÿä¸ºå®ç°å¯æ§ä¸”å®‰å…¨çš„æ¨ç†é˜¶æ®µ LLM å¯¹é½æä¾›åŸåˆ™æ€§çš„ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21528v1",
      "published_date": "2025-09-25 20:15:29 UTC",
      "updated_date": "2025-09-25 20:15:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:09:41.451719+00:00"
    },
    {
      "arxiv_id": "2509.22735v1",
      "title": "Regulating the Agency of LLM-based Agents",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„èƒ½åŠ¨æ€§è§„åˆ¶",
      "authors": [
        "SeÃ¡n Boddy",
        "Joshua Joseph"
      ],
      "abstract": "As increasingly capable large language model (LLM)-based agents are developed, the potential harms caused by misalignment and loss of control grow correspondingly severe. To address these risks, we propose an approach that directly measures and controls the agency of these AI systems. We conceptualize the agency of LLM-based agents as a property independent of intelligence-related measures and consistent with the interdisciplinary literature on the concept of agency. We offer (1) agency as a system property operationalized along the dimensions of preference rigidity, independent operation, and goal persistence, (2) a representation engineering approach to the measurement and control of the agency of an LLM-based agent, and (3) regulatory tools enabled by this approach: mandated testing protocols, domain-specific agency limits, insurance frameworks that price risk based on agency, and agency ceilings to prevent societal-scale risks. We view our approach as a step toward reducing the risks that motivate the ``Scientist AI'' paradigm, while still capturing some of the benefits from limited agentic behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç›´æ¥è¡¡é‡å¹¶æ§åˆ¶å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“ä»£ç†æƒ(Agency)çš„æ–°æ–¹æ³•ï¼Œä»¥åº”å¯¹ç³»ç»Ÿå¯¹é½å¤±å‡†å’Œå¤±æ§å¸¦æ¥çš„ä¸¥é‡é£é™©ã€‚ä½œè€…å°†ä»£ç†æƒ(Agency)å®šä¹‰ä¸ºä¸€ç§ç‹¬ç«‹äºæ™ºèƒ½æ°´å¹³ä¸”ç¬¦åˆè·¨å­¦ç§‘å®šä¹‰çš„ç³»ç»Ÿå±æ€§ï¼Œå¹¶å°†å…¶å…·ä½“åŒ–ä¸ºåå¥½åƒµåŒ–åº¦(Preference Rigidity)ã€ç‹¬ç«‹è¿è¡Œ(Independent Operation)å’Œç›®æ ‡æŒç»­æ€§(Goal Persistence)ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦ã€‚é€šè¿‡è¡¨å¾å·¥ç¨‹(Representation Engineering)æ‰‹æ®µï¼Œç ”ç©¶å®ç°äº†å¯¹æ™ºèƒ½ä½“ä»£ç†æ°´å¹³çš„ç²¾å‡†æµ‹é‡ä¸åŠ¨æ€è°ƒæ§ï¼Œå¹¶æ®æ­¤æå‡ºäº†ä¸€ç³»åˆ—ç›‘ç®¡å·¥å…·ï¼ŒåŒ…æ‹¬å¼ºåˆ¶æ€§æµ‹è¯•åè®®ã€é¢†åŸŸç‰¹å®šçš„ä»£ç†æƒé™åˆ¶ã€åŸºäºé£é™©å®šä»·çš„ä¿é™©æ¡†æ¶ä»¥åŠé¢„é˜²ç¤¾ä¼šçº§é£é™©çš„ä»£ç†æƒä¸Šé™ã€‚è¯¥æ–¹æ³•ä¸ºåœ¨é™ä½â€œç§‘å­¦å®¶AIâ€(Scientist AI)èŒƒå¼æ½œåœ¨é£é™©çš„åŒæ—¶ä¿ç•™å—æ§ä»£ç†è¡Œä¸ºçš„æ”¶ç›Šæä¾›äº†å…³é”®çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "4 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.22735v1",
      "published_date": "2025-09-25 20:14:02 UTC",
      "updated_date": "2025-09-25 20:14:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:09:44.171211+00:00"
    },
    {
      "arxiv_id": "2509.21522v1",
      "title": "Shortcut Flow Matching for Speech Enhancement: Step-Invariant flows via single stage training",
      "title_zh": "é¢å‘è¯­éŸ³å¢å¼ºçš„å¿«æ·æµåŒ¹é…ï¼šé€šè¿‡å•é˜¶æ®µè®­ç»ƒå®ç°æ­¥é•¿ä¸å˜æµ",
      "authors": [
        "Naisong Zhou",
        "Saisamarth Rajesh Phaye",
        "Milos Cernak",
        "Tijana Stojkovic",
        "Andy Pearce",
        "Andrea Cavallaro",
        "Andy Harper"
      ],
      "abstract": "Diffusion-based generative models have achieved state-of-the-art performance for perceptual quality in speech enhancement (SE). However, their iterative nature requires numerous Neural Function Evaluations (NFEs), posing a challenge for real-time applications. On the contrary, flow matching offers a more efficient alternative by learning a direct vector field, enabling high-quality synthesis in just a few steps using deterministic ordinary differential equation~(ODE) solvers. We thus introduce Shortcut Flow Matching for Speech Enhancement (SFMSE), a novel approach that trains a single, step-invariant model. By conditioning the velocity field on the target time step during a one-stage training process, SFMSE can perform single, few, or multi-step denoising without any architectural changes or fine-tuning. Our results demonstrate that a single-step SFMSE inference achieves a real-time factor (RTF) of 0.013 on a consumer GPU while delivering perceptual quality comparable to a strong diffusion baseline requiring 60 NFEs. This work also provides an empirical analysis of the role of stochasticity in training and inference, bridging the gap between high-quality generative SE and low-latency constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Shortcut Flow Matching for Speech Enhancement (SFMSE)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹è¯­éŸ³å¢å¼º (Speech Enhancement, SE) çš„æ–°å‹ç”Ÿæˆæ¨¡å‹ã€‚é’ˆå¯¹æ‰©æ•£æ¨¡å‹ (Diffusion-based generative models) å› è¿­ä»£æ¬¡æ•°å¤šè€Œéš¾ä»¥æ»¡è¶³å®æ—¶æ€§è¦æ±‚çš„é—®é¢˜ï¼ŒSFMSE é€šè¿‡å•é˜¶æ®µè®­ç»ƒ (one-stage training) å­¦ä¹ ä¸€ä¸ªæ­¥é•¿ä¸å˜ (step-invariant) çš„æ¨¡å‹ã€‚è¯¥æ–¹æ³•å°†é€Ÿåº¦åœº (velocity field) åœ¨ç›®æ ‡æ—¶é—´æ­¥ä¸Šè¿›è¡Œæ¡ä»¶åŒ–å¤„ç†ï¼Œä½¿å¾—å•ä¸€æ¨¡å‹åœ¨ä¸æ”¹å˜æ¶æ„æˆ–å¾®è°ƒçš„æƒ…å†µä¸‹å³å¯å®ç°å•æ­¥ã€å°‘é‡æ­¥æˆ–å¤šæ­¥å»å™ªã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå•æ­¥ SFMSE åœ¨æ¶ˆè´¹çº§ GPU ä¸Šçš„å®æ—¶å› å­ (Real-time Factor, RTF) ä»…ä¸º 0.013ï¼Œä¸”å…¶æ„ŸçŸ¥è´¨é‡å¯åª²ç¾éœ€è¦ 60 æ¬¡ç¥ç»å‡½æ•°è¯„ä¼° (NFEs) çš„å¼ºæ‰©æ•£åŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œè¿˜å¯¹è®­ç»ƒå’Œæ¨ç†ä¸­çš„éšæœºæ€§ (stochasticity) è¿›è¡Œäº†ç»éªŒåˆ†æï¼Œæœ‰æ•ˆå¼¥åˆäº†é«˜è´¨é‡ç”Ÿæˆå¼è¯­éŸ³å¢å¼ºä¸ä½å»¶è¿Ÿçº¦æŸä¹‹é—´çš„é¸¿æ²Ÿã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "5 pages, 2 figures, submitted to ICASSP2026",
      "pdf_url": "https://arxiv.org/pdf/2509.21522v1",
      "published_date": "2025-09-25 20:09:05 UTC",
      "updated_date": "2025-09-25 20:09:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:09:58.175989+00:00"
    },
    {
      "arxiv_id": "2509.21519v5",
      "title": "Provable Scaling Laws of Feature Emergence from Learning Dynamics of Grokking",
      "title_zh": "é¡¿æ‚Ÿå­¦ä¹ åŠ¨åŠ›å­¦ä¸­ç‰¹å¾æ¶Œç°çš„å¯è¯æ˜æ ‡åº¦å¾‹",
      "authors": [
        "Yuandong Tian"
      ],
      "abstract": "While the phenomenon of grokking, i.e., delayed generalization, has been studied extensively, it remains an open problem whether there is a mathematical framework that characterizes what kind of features will emerge, how and in which conditions it happens, and is closely related to the gradient dynamics of the training, for complex structured inputs. We propose a novel framework, named $\\mathbf{Li}_2$, that captures three key stages for the grokking behavior of 2-layer nonlinear networks: (I) Lazy learning, (II) independent feature learning and (III) interactive feature learning. At the lazy learning stage, top layer overfits to random hidden representation and the model appears to memorize, and at the same time, the backpropagated gradient $G_F$ from the top layer now carries information about the target label, with a specific structure that enables each hidden node to learn their representation independently. Interestingly, the independent dynamics follows exactly the gradient ascent of an energy function $E$, and its local maxima are precisely the emerging features. We study whether these local-optima induced features are generalizable, their representation power, and how they change on sample size, in group arithmetic tasks. When hidden nodes start to interact in the later stage of learning, we provably show how $G_F$ changes to focus on missing features that need to be learned. Our study sheds lights on roles played by key hyperparameters such as weight decay, learning rate and sample sizes in grokking, leads to provable scaling laws of feature emergence, memorization and generalization, and reveals why recent optimizers such as Muon can be effective, from the first principles of gradient dynamics. Our analysis can be extended to multi-layers. The code is available at https://github.com/yuandong-tian/understanding/tree/main/ssl/real-dataset/cogo.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸º $\\mathbf{Li}_2$ çš„æ–°å‹æ•°å­¦æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ†æ Grokkingï¼ˆå»¶è¿Ÿæ³›åŒ–ï¼‰çš„æ¢¯åº¦åŠ¨åŠ›å­¦è¿‡ç¨‹æ¥åˆ»ç”»ç‰¹å¾æ¶Œç°çš„æ•°å­¦æœºåˆ¶ã€‚è¯¥æ¡†æ¶å°† 2 å±‚éçº¿æ€§ç½‘ç»œçš„å­¦ä¹ åˆ’åˆ†ä¸º Lazy learningã€ç‹¬ç«‹ç‰¹å¾å­¦ä¹ å’Œäº¤äº’å¼ç‰¹å¾å­¦ä¹ ä¸‰ä¸ªå…³é”®é˜¶æ®µã€‚åœ¨ Lazy learning é˜¶æ®µï¼Œæ¨¡å‹é€šè¿‡è®°å¿†åŒ–è¿‡æ‹Ÿåˆéšæœºè¡¨ç¤ºï¼Œéšåæ¢¯åº¦åŠ¨åŠ›å­¦å¼•å¯¼éšè—èŠ‚ç‚¹æ²¿èƒ½é‡å‡½æ•° $E$ çš„æ¢¯åº¦ä¸Šå‡æ¥ç‹¬ç«‹å­¦ä¹ ç‰¹å¾ï¼Œä¸”èƒ½é‡å‡½æ•°çš„å±€éƒ¨æœ€å¤§å€¼å³å¯¹åº”æ¶Œç°çš„ç‰¹å¾ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†å›ä¼ æ¢¯åº¦ $G_F$ åœ¨äº¤äº’é˜¶æ®µå¦‚ä½•åŠ¨æ€èšç„¦äºç¼ºå¤±ç‰¹å¾ï¼Œå¹¶åˆ†æäº†è¿™äº›ç‰¹å¾åœ¨ç¾¤ç®—æœ¯ä»»åŠ¡ä¸­çš„è¡¨ç¤ºèƒ½åŠ›ä¸æ ·æœ¬é‡çš„å…³ç³»ã€‚è¯¥å·¥ä½œæ­ç¤ºäº† Weight decayã€Learning rate å’Œæ ·æœ¬é‡åœ¨ Grokking ä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œæ¨å¯¼å‡ºå…³äºç‰¹å¾æ¶Œç°ã€è®°å¿†ä¸æ³›åŒ–çš„å¯è¯æ˜ Scaling Lawsã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä»ç¬¬ä¸€æ€§åŸç†å‡ºå‘è§£é‡Šäº† Muon ç­‰æ–°å‹ä¼˜åŒ–å™¨çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æŒ‡å‡ºè¯¥åˆ†ææ¡†æ¶å¯æ‰©å±•è‡³å¤šå±‚ç½‘ç»œã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Find new mechanism that $G_F$ carries useful signals also at initial stage and thus remove theory's dependency on weight decay. Also add experiments on zero-init output layers, showing the technique is effective in accelerating grokking",
      "pdf_url": "https://arxiv.org/pdf/2509.21519v5",
      "published_date": "2025-09-25 20:08:09 UTC",
      "updated_date": "2025-12-02 18:56:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:10:37.191535+00:00"
    },
    {
      "arxiv_id": "2509.21513v1",
      "title": "DistillKac: Few-Step Image Generation via Damped Wave Equations",
      "title_zh": "DistillKacï¼šåŸºäºé˜»å°¼æ³¢åŠ¨æ–¹ç¨‹çš„å°‘æ­¥å›¾åƒç”Ÿæˆ",
      "authors": [
        "Weiqiao Han",
        "Chenlin Meng",
        "Christopher D. Manning",
        "Stefano Ermon"
      ],
      "abstract": "We present DistillKac, a fast image generator that uses the damped wave equation and its stochastic Kac representation to move probability mass at finite speed. In contrast to diffusion models whose reverse time velocities can become stiff and implicitly allow unbounded propagation speed, Kac dynamics enforce finite speed transport and yield globally bounded kinetic energy. Building on this structure, we introduce classifier-free guidance in velocity space that preserves square integrability under mild conditions. We then propose endpoint only distillation that trains a student to match a frozen teacher over long intervals. We prove a stability result that promotes supervision at the endpoints to closeness along the entire path. Experiments demonstrate DistillKac delivers high quality samples with very few function evaluations while retaining the numerical stability benefits of finite speed probability flows.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DistillKacï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨ damped wave equations å’Œå…¶éšæœº Kac representation è¿›è¡Œå¿«é€Ÿå›¾åƒç”Ÿæˆçš„æ¨¡å‹ã€‚ä¸æ‰©æ•£æ¨¡å‹ä¸­ä¼ æ’­é€Ÿåº¦æ— ç•Œä¸”åå‘é€Ÿåº¦å¯èƒ½å˜å¾—åƒµç¡¬çš„æƒ…å†µä¸åŒï¼ŒKac dynamics é€šè¿‡å¼ºåˆ¶æœ‰é™é€Ÿåº¦çš„æ¦‚ç‡è´¨é‡ä¼ è¾“ï¼Œå®ç°äº†å…¨å±€æœ‰ç•Œçš„ kinetic energyã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶è€…åœ¨é€Ÿåº¦ç©ºé—´å¼•å…¥äº† classifier-free guidanceï¼Œå¹¶åœ¨æ¸©å’Œæ¡ä»¶ä¸‹ç¡®ä¿äº†å…¶ square integrabilityã€‚æ­¤å¤–ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§ endpoint only distillation æ–¹æ³•ï¼Œé€šè¿‡åœ¨é•¿åŒºé—´å†…è®©å­¦ç”Ÿæ¨¡å‹åŒ¹é…æ•™å¸ˆæ¨¡å‹æ¥æå‡æ•ˆç‡ï¼Œå¹¶ä»ç†è®ºä¸Šè¯æ˜äº†è¯¥æ–¹æ³•çš„ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDistillKac èƒ½å¤Ÿåœ¨æå°‘æ¬¡çš„å‡½æ•°è¯„ä¼°ä¸‹ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼ŒåŒæ—¶ä¿ç•™äº†æœ‰é™é€Ÿåº¦æ¦‚ç‡æµåœ¨æ•°å€¼ç¨³å®šæ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "math.PR",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21513v1",
      "published_date": "2025-09-25 20:04:41 UTC",
      "updated_date": "2025-09-25 20:04:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:10:34.503950+00:00"
    },
    {
      "arxiv_id": "2509.21502v1",
      "title": "New Algorithmic Directions in Optimal Transport and Applications for Product Spaces",
      "title_zh": "æœ€ä¼˜ä¼ è¾“ä¸­çš„æ–°ç®—æ³•æ–¹å‘åŠå…¶åœ¨ä¹˜ç§¯ç©ºé—´ä¸­çš„åº”ç”¨",
      "authors": [
        "Salman Beigi",
        "Omid Etesami",
        "Mohammad Mahmoody",
        "Amir Najafi"
      ],
      "abstract": "We study optimal transport between two high-dimensional distributions $Î¼,Î½$ in $R^n$ from an algorithmic perspective: given $x \\sim Î¼$, find a close $y \\sim Î½$ in $poly(n)$ time, where $n$ is the dimension of $x,y$. Thus, running time depends on the dimension rather than the full representation size of $Î¼,Î½$. Our main result is a general algorithm for transporting any product distribution $Î¼$ to any $Î½$ with cost $Î”+ Î´$ under $\\ell_p^p$, where $Î”$ is the Knothe-Rosenblatt transport cost and $Î´$ is a computational error decreasing with runtime. This requires $Î½$ to be \"sequentially samplable\" with bounded average sampling cost, a new but natural notion.\n  We further prove:\n  An algorithmic version of Talagrand's inequality for transporting the standard Gaussian $Î¦^n$ to arbitrary $Î½$ under squared Euclidean cost. For $Î½= Î¦^n$ conditioned on a set $\\mathcal{S}$ of measure $\\varepsilon$, we construct the sequential sampler in expected time $poly(n/\\varepsilon)$ using membership oracle access to $\\mathcal{S}$. This yields an algorithmic transport from $Î¦^n$ to $Î¦^n|\\mathcal{S}$ in $poly(n/\\varepsilon)$ time and expected squared distance $O(\\log 1/\\varepsilon)$, optimal for general $\\mathcal{S}$ of measure $\\varepsilon$.\n  As corollary, we obtain the first computational concentration result (Etesami et al. SODA 2020) for Gaussian measure under Euclidean distance with dimension-independent transportation cost, resolving an open question of Etesami et al. Specifically, for any $\\mathcal{S}$ of Gaussian measure $\\varepsilon$, most $Î¦^n$ samples can be mapped to $\\mathcal{S}$ within distance $O(\\sqrt{\\log 1/\\varepsilon})$ in $poly(n/\\varepsilon)$ time.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»ç®—æ³•è§’åº¦æ¢è®¨äº†é«˜ç»´ç©ºé—´ä¸­åˆ†å¸ƒ $\\mu$ ä¸ $\\nu$ ä¹‹é—´çš„ Optimal Transport é—®é¢˜ï¼Œæ—¨åœ¨ $poly(n)$ æ—¶é—´å†…å®ç°é«˜æ•ˆä¼ è¾“ã€‚ä¸»è¦è´¡çŒ®åœ¨äºæå‡ºäº†ä¸€ä¸ªé€šç”¨ç®—æ³•ï¼Œèƒ½å°†ä»»æ„ product distribution $\\mu$ ä¼ è¾“è‡³æ»¡è¶³ sequentially samplable æ¡ä»¶çš„åˆ†å¸ƒ $\\nu$ï¼Œä¸”å…¶ä¼ è¾“æˆæœ¬æ¥è¿‘ Knothe-Rosenblatt æˆæœ¬ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº† Talagrand ä¸ç­‰å¼çš„ç®—æ³•ç‰ˆæœ¬ï¼Œå®ç°äº†ä»æ ‡å‡†é«˜æ–¯åˆ†å¸ƒ $\\Phi^n$ åˆ°å—é™é›†åˆ $\\mathcal{S}$ çš„æœ‰æ•ˆä¼ è¾“ã€‚åœ¨ $poly(n/\\varepsilon)$ çš„æ—¶é—´å¤æ‚åº¦ä¸‹ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿè¾¾åˆ° $O(\\log 1/\\varepsilon)$ çš„æœŸæœ›å¹³æ–¹è·ç¦»ï¼Œåœ¨ general $\\mathcal{S}$ æƒ…å†µä¸‹è¾¾åˆ°äº†ç†è®ºæœ€ä¼˜ã€‚è¿™ä¸€ç»“æœè§£å†³äº† Etesami ç­‰äººå…³äºé«˜æ–¯æµ‹åº¦ä¸‹å…·æœ‰ dimension-independent ä¼ è¾“æˆæœ¬çš„ computational concentration å…¬å¼€é—®é¢˜ã€‚è¯¥ç ”ç©¶ä¸ºé«˜ç»´ç©ºé—´ä¸‹çš„ Optimal Transport æä¾›äº†æ–°çš„ç®—æ³•æ–¹å‘ï¼Œå¹¶è¯æ˜äº†å¤§å¤šæ•° Gaussian æ ·æœ¬å¯ä»¥åœ¨å¤šé¡¹å¼æ—¶é—´å†…æ˜ å°„åˆ°ç›®æ ‡é›†åˆã€‚",
      "categories": [
        "cs.DS",
        "cs.AI",
        "cs.IT",
        "cs.LG"
      ],
      "primary_category": "cs.DS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21502v1",
      "published_date": "2025-09-25 19:58:06 UTC",
      "updated_date": "2025-09-25 19:58:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:10:55.193039+00:00"
    },
    {
      "arxiv_id": "2509.21500v1",
      "title": "Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training",
      "title_zh": "Chasing the Tailï¼šå¤§è¯­è¨€æ¨¡å‹åè®­ç»ƒä¸­æœ‰æ•ˆçš„åŸºäºè¯„ä»·ç»†åˆ™å¥–åŠ±å»ºæ¨¡",
      "authors": [
        "Junkai Zhang",
        "Zihao Wang",
        "Lin Gui",
        "Swarnashree Mysore Sathyendra",
        "Jaehwan Jeong",
        "Victor Veitch",
        "Wei Wang",
        "Yunzhong He",
        "Bing Liu",
        "Lifeng Jin"
      ],
      "abstract": "Reinforcement fine-tuning (RFT) often suffers from \\emph{reward over-optimization}, where a policy model hacks the reward signals to achieve high scores while producing low-quality outputs. Our theoretical analysis shows that the key lies in reward misspecification at the high-reward tail: the inability to reliably distinguish Excellent responses from merely Great ones. This motivate us to focus on the high-reward region. However, such tail examples are scarce under the base LLM. While off-policy exemplars (e.g. from stronger models or rewrites) are easier to obtain, naively training on them yields a misspecified reward for the policy we aim to align. To address this, we study rubric-based rewards. By design, rubrics can leverage off-policy examples while remaining insensitive to their artifacts. To elicit rubrics that capture the high-reward tail, we highlight the importance of distinguishing among great and diverse responses, and introduce a workflow to implement this idea. We empirically demonstrate that rubric-based rewards substantially mitigate reward over-optimization and deliver effective LLM post-training improvements. Our code can be accessed at https://github.com/Jun-Kai-Zhang/rubrics.git .",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Model)åè®­ç»ƒé˜¶æ®µä¸­å¼ºåŒ–å­¦ä¹ å¾®è°ƒ(Reinforcement fine-tuning)é¢ä¸´çš„å¥–åŠ±è¿‡åº¦ä¼˜åŒ–(reward over-optimization)é—®é¢˜ã€‚ç†è®ºåˆ†ææŒ‡å‡ºï¼Œé—®é¢˜çš„æ ¸å¿ƒåœ¨äºé«˜å¥–åŠ±å°¾éƒ¨(high-reward tail)çš„å¥–åŠ±è§„èŒƒé”™è¯¯(reward misspecification)ï¼Œå³æ¨¡å‹éš¾ä»¥åœ¨æé«˜è´¨é‡åŒºé—´å†…å‡†ç¡®åŒºåˆ†â€œä¼˜ç§€â€ä¸â€œè‰¯å¥½â€çš„å“åº”ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†åŸºäºå‡†åˆ™çš„å¥–åŠ±(rubric-based rewards)å»ºæ¨¡æ–¹æ³•ï¼Œé€šè¿‡è®¾è®¡ç‰¹å®šçš„è¯„åˆ†å‡†åˆ™æ¥æœ‰æ•ˆåˆ©ç”¨ç¦»çº¿ç¤ºä¾‹(off-policy examples)ï¼Œå¹¶ä½¿å…¶å¯¹è¿™äº›ç¤ºä¾‹ä¸­çš„äººå·¥ç—•è¿¹ä¸æ•æ„Ÿã€‚è¯¥ç ”ç©¶è¿˜å¼•å…¥äº†ä¸€å¥—ä¸“é—¨çš„å·¥ä½œæµï¼Œç”¨äºæå–èƒ½å¤Ÿæ•æ‰é«˜å¥–åŠ±å°¾éƒ¨ç‰¹å¾çš„ç»†é¢—ç²’åº¦å‡†åˆ™ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒåŸºäºå‡†åˆ™çš„å¥–åŠ±æ˜¾è‘—ç¼“è§£äº†å¥–åŠ±è¿‡åº¦ä¼˜åŒ–ç°è±¡ï¼Œä¸ºæå‡LLMåè®­ç»ƒæ•ˆæœæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21500v1",
      "published_date": "2025-09-25 19:57:39 UTC",
      "updated_date": "2025-09-25 19:57:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:10:44.192812+00:00"
    },
    {
      "arxiv_id": "2509.21487v2",
      "title": "Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning",
      "title_zh": "åŒå¤´æ¨ç†è’¸é¦ï¼šåˆ©ç”¨ä»…é™è®­ç»ƒé˜¶æ®µçš„æ¨ç†æå‡åˆ†ç±»å™¨å‡†ç¡®ç‡",
      "authors": [
        "Jillian Xu",
        "Dylan Zhou",
        "Vinay Shukla",
        "Yang Yang",
        "Junrui Ruan",
        "Shuhuai Lin",
        "Wenfei Zou",
        "Yinxiao Liu",
        "Karthik Lakshmanan"
      ],
      "abstract": "Chain-of-Thought (CoT) prompting often improves classification accuracy, but it introduces a significant throughput penalty with rationale generation (Wei et al., 2022; Cheng and Van Durme, 2024). To resolve this trade-off, we introduce Dual-Head Reasoning Distillation (DHRD), a simple training method for decoder-only language models (LMs) that adds (i) a pooled classification head used during training and inference and (ii) a reasoning head supervised by teacher rationales used only in training. We train with a loss function that is a weighted sum of label cross-entropy and token-level LM loss over input-plus-rationale sequences. On seven SuperGLUE tasks, DHRD yields relative gains of 0.65-5.47% over pooled baselines, with notably larger gains on entailment/causal tasks. Since we disable the reasoning head at test time, inference throughput matches pooled classifiers and exceeds CoT decoding on the same backbones by 96-142 times in QPS.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é“¾å¼æ€ç»´(Chain-of-Thought)æ¨ç†åœ¨æå‡åˆ†ç±»å‡†ç¡®ç‡çš„åŒæ—¶å¯¼è‡´æ¨ç†ååé‡(throughput)å¤§å¹…ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†åŒå¤´æ¨ç†è’¸é¦(Dual-Head Reasoning Distillation, DHRD)æ–¹æ³•ã€‚DHRD ä¸ºä»…è§£ç å™¨(decoder-only)è¯­è¨€æ¨¡å‹å¼•å…¥äº†ä¸€ä¸ªç”¨äºè®­ç»ƒå’Œæ¨ç†çš„æ± åŒ–åˆ†ç±»å¤´(pooled classification head)ï¼Œä»¥åŠä¸€ä¸ªä»…åœ¨è®­ç»ƒé˜¶æ®µç”±æ•™å¸ˆæ¨ç†è¿‡ç¨‹(teacher rationales)ç›‘ç£çš„æ¨ç†å¤´(reasoning head)ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¯¥æ–¹æ³•é€šè¿‡åŠ æƒæŸå¤±å‡½æ•°åŒæ—¶ä¼˜åŒ–æ ‡ç­¾äº¤å‰ç†µ(label cross-entropy)å’Œè¾“å…¥åŠ æ¨ç†åºåˆ—çš„æ ‡è®°çº§(token-level)è¯­è¨€æ¨¡å‹æŸå¤±ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸ƒé¡¹ SuperGLUE ä»»åŠ¡ä¸­ï¼ŒDHRD ç›¸æ¯”æ± åŒ–åŸºçº¿æ¨¡å‹å®ç°äº† 0.65-5.47% çš„ç›¸å¯¹æå‡ï¼Œä¸”åœ¨è•´å«å’Œå› æœæ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°å°¤ä¸ºæ˜¾è‘—ã€‚ç”±äºåœ¨æµ‹è¯•é˜¶æ®µç¦ç”¨äº†æ¨ç†å¤´ï¼ŒDHRD çš„æ¨ç†ååé‡ä¸æ™®é€šåˆ†ç±»å™¨æŒå¹³ï¼Œåœ¨æ¯ç§’æŸ¥è¯¢æ•°(QPS)ä¸Šæ¯”åŒæ¶æ„çš„ CoT è§£ç å¿« 96-142 å€ï¼Œæœ‰æ•ˆè§£å†³äº†å‡†ç¡®ç‡ä¸æ¨ç†æ€§èƒ½ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Efficient Reasoning Workshop",
      "pdf_url": "https://arxiv.org/pdf/2509.21487v2",
      "published_date": "2025-09-25 19:47:19 UTC",
      "updated_date": "2025-09-29 02:58:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:10:52.702129+00:00"
    },
    {
      "arxiv_id": "2509.21485v1",
      "title": "Neural Operators for Mathematical Modeling of Transient Fluid Flow in Subsurface Reservoir Systems",
      "title_zh": "ç”¨äºåœ°ä¸‹å‚¨å±‚ç³»ç»Ÿç¬æ€æµä½“æµåŠ¨æ•°å­¦å»ºæ¨¡çš„ç¥ç»ç®—å­",
      "authors": [
        "Daniil D. Sirota",
        "Sergey A. Khan",
        "Sergey L. Kostikov",
        "Kirill A. Butov"
      ],
      "abstract": "This paper presents a method for modeling transient fluid flow in subsurface reservoir systems based on the developed neural operator architecture (TFNO-opt). Reservoir systems are complex dynamic objects with distributed parameters described by systems of partial differential equations (PDEs). Traditional numerical methods for modeling such systems, despite their high accuracy, are characterized by significant time costs for performing calculations, which limits their applicability in control and decision support problems. The proposed architecture (TFNO-opt) is based on Fourier neural operators, which allow approximating PDE solutions in infinite-dimensional functional spaces, providing invariance to discretization and the possibility of generalization to various implementations of equations. The developed modifications are aimed at increasing the accuracy and stability of the trained neural operator, which is especially important for control problems. These include adjustable internal time resolution of the integral Fourier operator, tensor decomposition of parameters in the spectral domain, use of the Sobolev norm in the error function, and separation of approximation errors and reconstruction of initial conditions for more accurate reproduction of physical processes. The effectiveness of the proposed improvements is confirmed by computational experiments. The practical significance is confirmed by computational experiments using the example of the problem of hydrodynamic modeling of an underground gas storage (UGS), where the acceleration of calculations by six orders of magnitude was achieved, compared to traditional methods. This opens up new opportunities for the effective control of complex reservoir systems.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹åœ°ä¸‹å‚¨å±‚ç³»ç»Ÿä¸­ç¬æ€æµä½“æµåŠ¨çš„å»ºæ¨¡æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º TFNO-opt çš„ç¥ç»ç®—å­æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ•°å€¼æ¨¡æ‹Ÿæ–¹æ³•åœ¨æ§åˆ¶å’Œå†³ç­–æ”¯æŒé—®é¢˜ä¸­è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚è¯¥æ¶æ„åŸºäºå‚…é‡Œå¶ç¥ç»ç®—å­ï¼ˆFourier Neural Operatorsï¼‰ï¼Œé€šè¿‡åœ¨æ— é™ç»´å‡½æ•°ç©ºé—´ä¸­é€¼è¿‘åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEsï¼‰çš„è§£ï¼Œå®ç°äº†å¯¹ç½‘æ ¼åˆ’åˆ†çš„ç¦»æ•£ä¸å˜æ€§ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¼•å…¥å¯è°ƒå†…éƒ¨æ—¶é—´åˆ†è¾¨ç‡ã€è°±åŸŸå‚æ•°å¼ é‡åˆ†è§£ï¼ˆTensor Decompositionï¼‰ä»¥åŠåœ¨æŸå¤±å‡½æ•°ä¸­ä½¿ç”¨ç´¢ä¼¯åˆ—å¤«èŒƒæ•°ï¼ˆSobolev normï¼‰ç­‰æ”¹è¿›æªæ–½ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„è®­ç»ƒå‡†ç¡®åº¦ä¸ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åœ°ä¸‹å‚¨æ°”åº“ï¼ˆUGSï¼‰çš„æ°´åŠ¨åŠ›å»ºæ¨¡åº”ç”¨ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶ï¼Œå®ç°äº†æ¯”ä¼ ç»Ÿæ¨¡æ‹Ÿæ–¹æ³•å¿«å…­ä¸ªæ•°é‡çº§çš„è®¡ç®—åŠ é€Ÿã€‚è¿™ä¸€çªç ´ä¸ºå¤æ‚å‚¨å±‚ç³»ç»Ÿçš„é«˜æ•ˆå®æ—¶æ§åˆ¶å’Œç‰©ç†è¿‡ç¨‹çš„ç²¾ç¡®æ¨¡æ‹Ÿæä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.flu-dyn",
        "physics.geo-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.21485v1",
      "published_date": "2025-09-25 19:45:07 UTC",
      "updated_date": "2025-09-25 19:45:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:11:06.298372+00:00"
    },
    {
      "arxiv_id": "2509.21482v1",
      "title": "Learning to Reason with Mixture of Tokens",
      "title_zh": "å­¦ä¹ åˆ©ç”¨è¯å…ƒæ··åˆè¿›è¡Œæ¨ç†",
      "authors": [
        "Adit Jain",
        "Brendan Rappazzo"
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has become a leading approach for improving large language model (LLM) reasoning capabilities. Most current methods follow variants of Group Relative Policy Optimization, which samples multiple reasoning completions, scores them relative to each other, and adjusts the policy accordingly. However, these approaches invariably sample discrete tokens at each reasoning step, discarding the rich distributional information in the model's probability distribution over candidate tokens. While preserving and utilizing this distributional information has proven beneficial in non-RL settings, current RLVR methods seem to be unnecessarily constraining the reasoning search space by not using this information. To address this limitation, we investigate mixture-of-token generation (MoT-G) in RLVR. We present a unified framework that generalizes existing MoT-G approaches, including existing training-free methods that construct mixture embeddings as weighted sums over token embeddings, and extend RLVR to operate directly in this continuous mixture space for generating chain-of-thought. Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive language tasks, we find that MoT--G methods achieve substantial improvements (5--35 \\% gains on 7 out of 10 tasks) compared to standard decoding with the Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of trajectories, suggesting improved training efficiency. Through comprehensive hidden-state and token-level analyses, we provide evidence that MoT--G's benefits may stem from its ability to maintain higher hidden-state entropy throughout the reasoning process and promote exploration in token space.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä»»åŠ¡ä¸­ï¼Œåˆ©ç”¨æ··åˆæ ‡è®°ç”Ÿæˆï¼ˆMixture-of-Token Generation, MoT-Gï¼‰æ¥æ”¹è¿›å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰çš„æ–¹æ³•ã€‚é’ˆå¯¹ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»…é‡‡æ ·ç¦»æ•£ Token è€Œå¿½ç•¥æ¨¡å‹æ¦‚ç‡åˆ†å¸ƒä¸­ä¸°å¯Œä¿¡æ¯çš„å±€é™æ€§ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå°†æ¨ç†æœç´¢ç©ºé—´ä»ç¦»æ•£æ‰©å±•åˆ°äº†è¿ç»­çš„æ··åˆç©ºé—´ã€‚è¯¥æ¡†æ¶å…è®¸ç›´æ¥åœ¨è¿ç»­æ··åˆç©ºé—´ä¸­ç”Ÿæˆæ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰ï¼Œå¹¶èƒ½å¤Ÿå°†ç°æœ‰çš„å…è®­ç»ƒæ–¹æ³•æ¨å¹¿åˆ°åŸºäºæƒé‡å’Œçš„æ··åˆåµŒå…¥ï¼ˆmixture embeddingsï¼‰æ„å»ºä¸­ã€‚åœ¨ Reasoning-Gym ä»»åŠ¡é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ Qwen2.5-1.5B æ¨¡å‹çš„ MoT-G æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸­å®ç°äº† 5%-35% çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ä»…ä½¿ç”¨ä¸€åŠè®­ç»ƒè½¨è¿¹çš„æƒ…å†µä¸‹å³å¯è¾¾åˆ°ä¸åŸºçº¿ç›¸å½“çš„å‡†ç¡®ç‡ï¼Œå±•ç°äº†æé«˜çš„è®­ç»ƒæ•ˆç‡ã€‚åˆ†æè¿›ä¸€æ­¥è¯å®ï¼ŒMoT-G çš„ä¼˜åŠ¿æºäºå…¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­èƒ½å¤Ÿç»´æŒè¾ƒé«˜çš„éšè—çŠ¶æ€ç†µï¼ˆhidden-state entropyï¼‰ï¼Œä»è€Œæœ‰æ•ˆä¿ƒè¿›äº† Token ç©ºé—´çš„æ¢ç´¢ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "30 page",
      "pdf_url": "https://arxiv.org/pdf/2509.21482v1",
      "published_date": "2025-09-25 19:44:24 UTC",
      "updated_date": "2025-09-25 19:44:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:11:19.442229+00:00"
    },
    {
      "arxiv_id": "2509.21473v1",
      "title": "Are Hallucinations Bad Estimations?",
      "title_zh": "å¹»è§‰æ˜¯é”™è¯¯çš„ä¼°è®¡å—ï¼Ÿ",
      "authors": [
        "Hude Liu",
        "Jerry Yao-Chieh Hu",
        "Jennifer Yuntong Zhang",
        "Zhao Song",
        "Han Liu"
      ],
      "abstract": "We formalize hallucinations in generative models as failures to link an estimate to any plausible cause. Under this interpretation, we show that even loss-minimizing optimal estimators still hallucinate. We confirm this with a general high probability lower bound on hallucinate rate for generic data distributions. This reframes hallucination as structural misalignment between loss minimization and human-acceptable outputs, and hence estimation errors induced by miscalibration. Experiments on coin aggregation, open-ended QA, and text-to-image support our theory.",
      "tldr_zh": "è¯¥ç ”ç©¶å°†ç”Ÿæˆæ¨¡å‹ä¸­çš„å¹»è§‰(Hallucinations)æ­£å¼å®šä¹‰ä¸ºå°†ä¼°è®¡å€¼ä¸ä»»ä½•åˆç†åŸå› å»ºç«‹å…³è”çš„å¤±è´¥ã€‚ä½œè€…é€šè¿‡ç†è®ºè¯æ˜ï¼Œå³ä½¿æ˜¯å®ç°æŸå¤±æœ€å°åŒ–(Loss-minimizing)çš„æœ€ä¼˜ä¼°è®¡å™¨(Optimal estimators)ä»ç„¶ä¸å¯é¿å…åœ°ä¼šäº§ç”Ÿå¹»è§‰ï¼Œå¹¶ä¸ºé€šç”¨æ•°æ®åˆ†å¸ƒä¸‹çš„å¹»è§‰ç‡æä¾›äº†ä¸€ä¸ªé€šç”¨çš„é«˜æ¦‚ç‡ä¸‹ç•Œ(Lower bound)ã€‚è¿™ä¸€å‘ç°å°†å¹»è§‰é‡æ–°æ„æƒ³ä¸ºæŸå¤±æœ€å°åŒ–ç›®æ ‡ä¸äººç±»å¯æ¥å—è¾“å‡ºä¹‹é—´çš„ç»“æ„æ€§å¤±é…(Structural misalignment)ï¼Œå³ä¸€ç§ç”±æ ¡å‡†åç¦»(Miscalibration)è¯±å‘çš„ä¼°è®¡è¯¯å·®ã€‚é€šè¿‡åœ¨ç¡¬å¸èšåˆã€å¼€æ”¾å¼é—®ç­”(Open-ended QA)å’Œæ–‡æœ¬ç”Ÿæˆå›¾åƒ(Text-to-image)ç­‰ä»»åŠ¡ä¸Šçš„å®éªŒï¼Œè¯¥ç†è®ºå¾—åˆ°äº†æœ‰åŠ›æ”¯æŒï¼Œä¸ºæ·±å…¥ç†è§£ç”Ÿæˆæ¨¡å‹çš„å¤±æ•ˆæ¨¡å¼æä¾›äº†æ–°çš„æ•°å­¦æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Code is available at https://github.com/MAGICS-LAB/hallucination",
      "pdf_url": "https://arxiv.org/pdf/2509.21473v1",
      "published_date": "2025-09-25 19:39:09 UTC",
      "updated_date": "2025-09-25 19:39:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:11:09.994211+00:00"
    },
    {
      "arxiv_id": "2509.21470v1",
      "title": "Score-based Idempotent Distillation of Diffusion Models",
      "title_zh": "åŸºäºè¯„åˆ†çš„æ‰©æ•£æ¨¡å‹å¹‚ç­‰è’¸é¦",
      "authors": [
        "Shehtab Zaman",
        "Chengyan Liu",
        "Kenneth Chiu"
      ],
      "abstract": "Idempotent generative networks (IGNs) are a new line of generative models based on idempotent mapping to a target manifold. IGNs support both single-and multi-step generation, allowing for a flexible trade-off between computational cost and sample quality. But similar to Generative Adversarial Networks (GANs), conventional IGNs require adversarial training and are prone to training instabilities and mode collapse. Diffusion and score-based models are popular approaches to generative modeling that iteratively transport samples from one distribution, usually a Gaussian, to a target data distribution. These models have gained popularity due to their stable training dynamics and high-fidelity generation quality. However, this stability and quality come at the cost of high computational cost, as the data must be transported incrementally along the entire trajectory. New sampling methods, model distillation, and consistency models have been developed to reduce the sampling cost and even perform one-shot sampling from diffusion models. In this work, we unite diffusion and IGNs by distilling idempotent models from diffusion model scores, called SIGN. Our proposed method is highly stable and does not require adversarial losses. We provide a theoretical analysis of our proposed score-based training methods and empirically show that IGNs can be effectively distilled from a pre-trained diffusion model, enabling faster inference than iterative score-based models. SIGNs can perform multi-step sampling, allowing users to trade off quality for efficiency. These models operate directly on the source domain; they can project corrupted or alternate distributions back onto the target manifold, enabling zero-shot editing of inputs. We validate our models on multiple image datasets, achieving state-of-the-art results for idempotent models on the CIFAR and CelebA datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SIGNï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè¯„åˆ†çš„æ‰©æ•£æ¨¡å‹ç­‰å¹‚è’¸é¦æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç­‰å¹‚ç”Ÿæˆç½‘ç»œ(IGNs)åœ¨å¯¹æŠ—è®­ç»ƒä¸­å­˜åœ¨çš„ä¸ç¨³å®šæ€§å’Œæ¨¡å¼å´©æºƒé—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„è¯„åˆ†ä¸­è’¸é¦å‡ºç­‰å¹‚æ¨¡å‹ï¼Œå°†æ‰©æ•£æ¨¡å‹çš„ç¨³å®šè®­ç»ƒåŠ¨åŠ›å­¦ä¸IGNsçš„çµæ´»ç”Ÿæˆèƒ½åŠ›ç›¸ç»“åˆã€‚ä¸ä¼ ç»ŸIGNsä¸åŒï¼ŒSIGNä¸éœ€è¦å¯¹æŠ—æŸå¤±ï¼Œè®­ç»ƒè¿‡ç¨‹é«˜åº¦ç¨³å®šï¼Œä¸”æ”¯æŒä»å•æ­¥åˆ°å¤šæ­¥çš„çµæ´»é‡‡æ ·ï¼Œå…è®¸ç”¨æˆ·åœ¨è®¡ç®—æ•ˆç‡ä¸æ ·æœ¬è´¨é‡ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚ç”±äºè¯¥æ¨¡å‹ç›´æ¥åœ¨æºåŸŸä¸Šè¿è¡Œï¼Œèƒ½å¤Ÿå°†æŸåæˆ–äº¤æ›¿çš„åˆ†å¸ƒæŠ•å°„å›ç›®æ ‡æµå½¢ï¼Œä»è€Œå®ç°å¯¹è¾“å…¥çš„é›¶æ ·æœ¬(Zero-shot)ç¼–è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSIGNåœ¨CIFARå’ŒCelebAæ•°æ®é›†ä¸Šè¾¾åˆ°äº†ç­‰å¹‚æ¨¡å‹çš„æœ€å…ˆè¿›æ°´å¹³(SOTA)ï¼Œå…¶æ¨ç†é€Ÿåº¦æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„è¿­ä»£è¯„åˆ†æ¨¡å‹ï¼Œä¸ºé«˜æ•ˆä¸”é«˜è´¨é‡çš„ç”Ÿæˆå¼å»ºæ¨¡æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21470v1",
      "published_date": "2025-09-25 19:36:10 UTC",
      "updated_date": "2025-09-25 19:36:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:11:18.388795+00:00"
    },
    {
      "arxiv_id": "2509.21466v1",
      "title": "Gender Stereotypes in Professional Roles Among Saudis: An Analytical Study of AI-Generated Images Using Language Models",
      "title_zh": "æ²™ç‰¹èŒä¸šè§’è‰²ä¸­çš„æ€§åˆ«åˆ»æ¿å°è±¡ï¼šåŸºäºè¯­è¨€æ¨¡å‹çš„ AI ç”Ÿæˆå›¾åƒåˆ†æç ”ç©¶",
      "authors": [
        "Khaloud S. AlKhalifah",
        "Malak Mashaabi",
        "Hend Al-Khalifa"
      ],
      "abstract": "This study investigates the extent to which contemporary Text-to-Image artificial intelligence (AI) models perpetuate gender stereotypes and cultural inaccuracies when generating depictions of professionals in Saudi Arabia. We analyzed 1,006 images produced by ImageFX, DALL-E V3, and Grok for 56 diverse Saudi professions using neutral prompts. Two trained Saudi annotators evaluated each image on five dimensions: perceived gender, clothing and appearance, background and setting, activities and interactions, and age. A third senior researcher adjudicated whenever the two primary raters disagreed, yielding 10,100 individual judgements. The results reveal a strong gender imbalance, with ImageFX outputs being 85\\% male, Grok 86.6\\% male, and DALL-E V3 96\\% male, indicating that DALL-E V3 exhibited the strongest overall gender stereotyping. This imbalance was most evident in leadership and technical roles. Moreover, cultural inaccuracies in clothing, settings, and depicted activities were frequently observed across all three models. Counter-stereotypical images often arise from cultural misinterpretations rather than genuinely progressive portrayals. We conclude that current models mirror societal biases embedded in their training data, generated by humans, offering only a limited reflection of the Saudi labour market's gender dynamics and cultural nuances. These findings underscore the urgent need for more diverse training data, fairer algorithms, and culturally sensitive evaluation frameworks to ensure equitable and authentic visual outputs.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†å½“ä»£æ–‡æœ¬ç”Ÿæˆå›¾åƒ(Text-to-Image)äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨ç”Ÿæˆæ²™ç‰¹é˜¿æ‹‰ä¼¯èŒä¸šå½¢è±¡æ—¶ï¼Œæ˜¯å¦å­˜åœ¨æ€§åˆ«åˆ»æ¿å°è±¡å’Œæ–‡åŒ–åè§ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ImageFXã€DALL-E V3å’ŒGroké’ˆå¯¹56ç§æ²™ç‰¹èŒä¸šç”Ÿæˆäº†1,006å¼ å›¾åƒï¼Œå¹¶ç”±ä¸“ä¸šè¯„ä¼°è€…ä»æ„ŸçŸ¥æ€§åˆ«ã€æœè£…å¤–è§‚ã€èƒŒæ™¯è®¾ç½®ç­‰äº”ä¸ªç»´åº¦è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚å®éªŒç»“æœæ­ç¤ºäº†æ˜¾è‘—çš„æ€§åˆ«å¤±è¡¡ï¼Œå…¶ä¸­ImageFXã€Grokå’ŒDALL-E V3ç”Ÿæˆçš„ç”·æ€§æ¯”ä¾‹åˆ†åˆ«é«˜è¾¾85%ã€86.6%å’Œ96%ï¼Œä¸”åœ¨é¢†å¯¼èŒåŠ¡å’ŒæŠ€æœ¯å²—ä½ä¸­åˆ»æ¿å°è±¡æœ€ä¸ºä¸¥é‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°äº†é¢‘ç¹çš„æ–‡åŒ–è¯¯å·®(cultural inaccuracies)ï¼Œè¡¨æ˜æ¨¡å‹å¾€å¾€é€šè¿‡æ–‡åŒ–è¯¯è¯»è€ŒéçœŸå®ç†è§£æ¥å‘ˆç°ååˆ»æ¿å°è±¡çš„å†…å®¹ã€‚ç ”ç©¶ç»“è®ºè®¤ä¸ºå½“å‰æ¨¡å‹é•œåƒäº†è®­ç»ƒæ•°æ®ä¸­çš„ç¤¾ä¼šåè§ï¼Œä»…èƒ½æœ‰é™åœ°åæ˜ æ²™ç‰¹åŠ³åŠ¨åŠ›å¸‚åœºçš„çœŸå®åŠ¨æ€ï¼Œè¿™å‡¸æ˜¾äº†å¼€å‘å¤šæ ·åŒ–è®­ç»ƒæ•°æ®ã€å…¬å¹³ç®—æ³•åŠæ–‡åŒ–æ•æ„Ÿæ€§è¯„ä¼°æ¡†æ¶çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21466v1",
      "published_date": "2025-09-25 19:30:51 UTC",
      "updated_date": "2025-09-25 19:30:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:11:19.197652+00:00"
    },
    {
      "arxiv_id": "2509.21463v1",
      "title": "Enhanced Generative Machine Listener",
      "title_zh": "å¢å¼ºå‹ç”Ÿæˆå¼æœºå™¨å¬ä¼—",
      "authors": [
        "Vishnu Raj",
        "Gouthaman KV",
        "Shiv Gehlot",
        "Lars Villemoes",
        "Arijit Biswas"
      ],
      "abstract": "We present GMLv2, a reference-based model designed for the prediction of subjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta distribution-based loss to model the listener ratings and incorporates additional neural audio coding (NAC) subjective datasets to extend its generalization and applicability. Extensive evaluations on diverse testset demonstrate that proposed GMLv2 consistently outperforms widely used metrics, such as PEAQ and ViSQOL, both in terms of correlation with subjective scores and in reliably predicting these scores across diverse content types and codec configurations. Consequently, GMLv2 offers a scalable and automated framework for perceptual audio quality evaluation, poised to accelerate research and development in modern audio coding technologies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GMLv2ï¼Œä¸€ç§æ—¨åœ¨é¢„æµ‹ MUSHRA è¯„åˆ†çš„ä¸»è§‚éŸ³é¢‘è´¨é‡å‚è€ƒæ¨¡å‹ã€‚GMLv2 å¼•å…¥äº†åŸºäº Beta distribution çš„æŸå¤±å‡½æ•°æ¥å¯¹å¬ä¼—è¯„åˆ†è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶æ•´åˆäº†é¢å¤–çš„ç¥ç»éŸ³é¢‘ç¼–ç  (Neural Audio Coding, NAC) ä¸»è§‚æ•°æ®é›†ï¼Œä»¥æå‡å…¶æ³›åŒ–èƒ½åŠ›å’Œé€‚ç”¨èŒƒå›´ã€‚åœ¨å¤šæ ·åŒ–æµ‹è¯•é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯æ˜ï¼ŒGMLv2 åœ¨ä¸»è§‚è¯„åˆ†ç›¸å…³æ€§ä»¥åŠè·¨å†…å®¹ã€è·¨ç¼–è§£ç å™¨é…ç½®çš„é¢„æµ‹å¯é æ€§æ–¹é¢ï¼Œå‡æŒç»­ä¼˜äº PEAQ å’Œ ViSQOL ç­‰å¹¿æ³›ä½¿ç”¨çš„æŒ‡æ ‡ã€‚å› æ­¤ï¼ŒGMLv2 ä¸ºæ„ŸçŸ¥éŸ³é¢‘è´¨é‡è¯„ä»·æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”è‡ªåŠ¨åŒ–çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåŠ é€Ÿç°ä»£éŸ³é¢‘ç¼–ç æŠ€æœ¯çš„ç ”ç©¶ä¸å¼€å‘å·¥ä½œã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21463v1",
      "published_date": "2025-09-25 19:29:25 UTC",
      "updated_date": "2025-09-25 19:29:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:11:33.670823+00:00"
    },
    {
      "arxiv_id": "2509.21459v1",
      "title": "A State-of-the-Art SQL Reasoning Model using RLVR",
      "title_zh": "åŸºäº RLVR çš„æœ€å…ˆè¿› SQL æ¨ç†æ¨¡å‹",
      "authors": [
        "Alnur Ali",
        "Ashutosh Baheti",
        "Jonathan Chang",
        "Ta-Chung Chi",
        "Brandon Cui",
        "Andrew Drozdov",
        "Jonathan Frankle",
        "Abhay Gupta",
        "Pallavi Koppol",
        "Sean Kulinski",
        "Jonathan Li",
        "Dipendra Misra",
        "Krista Opsahl-Ong",
        "Jose Javier Gonzalez Ortiz",
        "Matei Zaharia",
        "Yue Zhang"
      ],
      "abstract": "Developing custom reasoning models via Reinforcement Learning (RL) that can incorporate organization-specific knowledge has great potential to address problems faced by enterprise customers. In many of these problems, the reward function is verifiable, a setting termed RL with Verifiable Rewards (RLVR). We apply RLVR to a popular data science benchmark called BIRD that measures the ability of an AI agent to convert a natural language query for a database to SQL executions. We apply a simple and general-purpose training recipe involving careful prompt and model selection, a warm-up stage using our offline RL approach called TAO, followed by rigorous online RLVR training. With no additional training data beyond the BIRD training set and no use of proprietary models, our very first submission to the BIRD leaderboard reached state-of-the-art accuracy on the private test set: 73.56% without self-consistency and 75.68% with self-consistency. In the latter case, our model also required fewer generations than the second-best approach. While BIRD is only a proxy task, the simplicity of our framework makes it broadly applicable to enterprise domains such as business intelligence, data science, and coding.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å¸¦éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RLVR)å¼€å‘çš„å…ˆè¿›SQLæ¨ç†æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å¯éªŒè¯çš„å¥–åŠ±æœºåˆ¶è§£å†³ä¼ä¸šçº§æ•°æ®å¤„ç†ä¸­çš„æ¨ç†éš¾é¢˜ã€‚ä½œè€…å°†RLVRåº”ç”¨äºæµè¡Œçš„æ•°æ®ç§‘å­¦åŸºå‡†æµ‹è¯•BIRDï¼Œè¯¥æµ‹è¯•æ—¨åœ¨è¡¡é‡AIæ™ºèƒ½ä½“å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºSQLæ‰§è¡Œçš„èƒ½åŠ›ã€‚è®­ç»ƒè¿‡ç¨‹é‡‡ç”¨äº†ä¸€ç§ç®€å•ä¸”é€šç”¨çš„æ–¹æ¡ˆï¼Œé¦–å…ˆé€šè¿‡åä¸ºTAOçš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ (offline RL)æ–¹æ³•è¿›è¡Œé¢„çƒ­ï¼Œéšåè¿›è¡Œä¸¥æ ¼çš„åœ¨çº¿RLVRè®­ç»ƒã€‚åœ¨ä¸ä½¿ç”¨é¢å¤–è®­ç»ƒæ•°æ®æˆ–ä¸“æœ‰æ¨¡å‹çš„å‰æä¸‹ï¼Œè¯¥æ¨¡å‹åœ¨BIRDç§äººæµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®ç‡ï¼Œåœ¨ä¸ä½¿ç”¨è‡ªæˆ‘ä¸€è‡´æ€§(self-consistency)æ—¶ä¸º73.56%ï¼Œä½¿ç”¨æ—¶åˆ™è¾¾åˆ°75.68%ã€‚å®éªŒç»“æœè¯æ˜è¯¥æ¨¡å‹åœ¨å®ç°é«˜å‡†ç¡®ç‡çš„åŒæ—¶æ¯”æ¬¡ä¼˜æ–¹æ¡ˆæ‰€éœ€çš„ç”Ÿæˆæ¬¡æ•°æ›´å°‘ï¼Œå…¶æ¡†æ¶çš„ç®€æ´æ€§ä½¿å…¶åœ¨å•†ä¸šæ™ºèƒ½ã€æ•°æ®ç§‘å­¦å’Œç¼–ç¨‹ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21459v1",
      "published_date": "2025-09-25 19:27:35 UTC",
      "updated_date": "2025-09-25 19:27:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:11:48.751481+00:00"
    },
    {
      "arxiv_id": "2509.21447v1",
      "title": "ARTI-6: Towards Six-dimensional Articulatory Speech Encoding",
      "title_zh": "ARTI-6ï¼šè¿ˆå‘å…­ç»´æ„éŸ³è¯­éŸ³ç¼–ç ",
      "authors": [
        "Jihwan Lee",
        "Sean Foley",
        "Thanathai Lertpetchpun",
        "Kevin Huang",
        "Yoonjeong Lee",
        "Tiantian Feng",
        "Louis Goldstein",
        "Dani Byrd",
        "Shrikanth Narayanan"
      ],
      "abstract": "We propose ARTI-6, a compact six-dimensional articulatory speech encoding framework derived from real-time MRI data that captures crucial vocal tract regions including the velum, tongue root, and larynx. ARTI-6 consists of three components: (1) a six-dimensional articulatory feature set representing key regions of the vocal tract; (2) an articulatory inversion model, which predicts articulatory features from speech acoustics leveraging speech foundation models, achieving a prediction correlation of 0.87; and (3) an articulatory synthesis model, which reconstructs intelligible speech directly from articulatory features, showing that even a low-dimensional representation can generate natural-sounding speech. Together, ARTI-6 provides an interpretable, computationally efficient, and physiologically grounded framework for advancing articulatory inversion, synthesis, and broader speech technology applications. The source code and speech samples are publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ARTI-6ï¼Œä¸€ç§åŸºäºå®æ—¶ MRI æ•°æ®æ„å»ºçš„å…­ç»´å‘éŸ³è¯­éŸ³ç¼–ç æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰è½¯è…­(velum)ã€èˆŒæ ¹(tongue root)å’Œå–‰(larynx)ç­‰å…³é”®å£°é“åŒºåŸŸçš„ç‰¹å¾ã€‚è¯¥æ¡†æ¶ç”±ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ç»„æˆï¼šä¸€å¥—å®šä¹‰å…³é”®å£°é“åŒºåŸŸçš„å…­ç»´å‘éŸ³ç‰¹å¾é›†ã€åˆ©ç”¨è¯­éŸ³åŸºç¡€æ¨¡å‹(speech foundation models)å®ç°å£°å­¦åˆ°å‘éŸ³ç‰¹å¾è½¬æ¢çš„åæ¼”æ¨¡å‹ï¼Œä»¥åŠä»ç‰¹å¾é‡æ„è¯­éŸ³çš„åˆæˆæ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¶åæ¼”æ¨¡å‹çš„é¢„æµ‹ç›¸å…³æ€§è¾¾åˆ° 0.87ï¼Œä¸”åˆæˆæ¨¡å‹è¯æ˜äº†ä½ç»´è¡¨ç¤ºäº¦èƒ½ç”Ÿæˆè‡ªç„¶æ¸…æ™°çš„è¯­éŸ³ã€‚ARTI-6 ä¸ºå‘éŸ³åæ¼”(articulatory inversion)ä¸è¯­éŸ³åˆæˆ(articulatory synthesis)é¢†åŸŸæä¾›äº†ä¸€ä¸ªå…·æœ‰å¼ºå¯è§£é‡Šæ€§ã€é«˜è®¡ç®—æ•ˆç‡ä¸”ç¬¦åˆç”Ÿç†åŸºç¡€çš„ç†è®ºæ¡†æ¶ã€‚è¿™é¡¹ç ”ç©¶ä¸ä»…æ¨åŠ¨äº†ç”Ÿç†ç›¸å…³è¯­éŸ³æŠ€æœ¯çš„å‘å±•ï¼Œè¿˜é€šè¿‡å¼€æºä»£ç å’Œè¯­éŸ³æ ·æœ¬ä¸ºåç»­ç ”ç©¶æä¾›äº†å®ç”¨å‚è€ƒã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21447v1",
      "published_date": "2025-09-25 19:18:35 UTC",
      "updated_date": "2025-09-25 19:18:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:11:48.466292+00:00"
    },
    {
      "arxiv_id": "2510.03246v1",
      "title": "StructPrune: Structured Global Pruning asymptotics with $\\mathcal{O}(\\sqrt{N})$ GPU Memory",
      "title_zh": "StructPruneï¼šå…·æœ‰ $\\mathcal{O}(\\sqrt{N})$ GPU æ˜¾å­˜å¼€é”€çš„ç»“æ„åŒ–å…¨å±€å‰ªææ¸è¿‘åˆ†æ",
      "authors": [
        "Xinyuan Song",
        "Guangji Bai",
        "Liang Zhao"
      ],
      "abstract": "Pruning is critical for scaling large language models (LLMs). Global pruning achieves strong performance but requires $\\mathcal{O}(N)$ memory, which is infeasible for billion-parameter models. Local pruning reduces GPU memory usage to that of a single layer by pruning layers independently, but it neglects inter-layer dependencies and often leads to suboptimal performance in high-sparsity regimes. Unlike unstructured pruning, structured pruning produces regular sparsity patterns that align well with GPU kernels and library optimizations, making it more hardware-efficient. However, structured pruning typically relies on global pruning, since structured patterns are more prone to severe performance degradation under local optimization. To jointly achieve structured pruning and the memory efficiency of local pruning, we propose a divide-and-conquer strategy that decomposes the global pruning problem into coordinated subproblems across different modules, each of which fits within limited GPU memory. Building on this idea, we design \\textbf{STRUPRUNE}, an ADMM-based framework that integrates structured sparsity into the pruning process, combining the memory efficiency of local pruning with the hardware compatibility of structured methods. We derive a closed-form analytical solution for structured pruning masks that provides an explicit rule for layer-wise sparsity allocation, and further develop an energy-based asymptotic framework yielding a softmax-form allocation scheme that simplifies optimization while adapting to heterogeneous layer importance. Experiments demonstrate that STRUPRUNE matches the perplexity of global structured pruning while reducing memory cost from $\\mathcal{O}(N)$ to $\\mathcal{O}(\\sqrt{N})$, enabling practical deployment at the billion-parameter scale.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†STRUPRUNEï¼Œä¸€ç§åŸºäºäº¤æ›¿æ–¹å‘ä¹˜å­æ³•(ADMM)çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ‰§è¡Œå…¨å±€ç»“æ„åŒ–å‰ªææ—¶é¢ä¸´çš„$\\mathcal{O}(N)$æ˜¾å­˜éœ€æ±‚è¿‡é«˜çš„é—®é¢˜ã€‚é€šè¿‡é‡‡ç”¨åˆ†æ²»ç­–ç•¥ï¼Œè¯¥æ¡†æ¶å°†å…¨å±€å‰ªæåˆ†è§£ä¸ºå¤šä¸ªå¯åœ¨æœ‰é™æ˜¾å­˜å†…è¿è¡Œçš„åè°ƒå­é—®é¢˜ï¼ŒæˆåŠŸå°†æ˜¾å­˜æˆæœ¬é™ä½è‡³$\\mathcal{O}(\\sqrt{N})$ã€‚ç ”ç©¶è€…è¿›ä¸€æ­¥æ¨å¯¼å‡ºäº†ç»“æ„åŒ–å‰ªææ©ç çš„é—­å¼è§£æè§£ï¼Œå¹¶å¼€å‘äº†åŸºäºèƒ½é‡çš„æ¸è¿‘æ¡†æ¶ï¼Œåˆ©ç”¨Softmaxå½¢å¼çš„åˆ†é…æ–¹æ¡ˆå®ç°å±‚é—´ç¨€ç–åº¦çš„åŠ¨æ€ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTRUPRUNEåœ¨ç»´æŒä¸å…¨å±€ç»“æ„åŒ–å‰ªæç›¸å½“çš„å›°æƒ‘åº¦(Perplexity)çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†å†…å­˜æ•ˆç‡ã€‚è¿™ä¸€æˆæœä¸ºåœ¨åäº¿çº§å‚æ•°è§„æ¨¡çš„æ¨¡å‹ä¸Šå®ç°ç¡¬ä»¶å‹å¥½ä¸”é«˜æ•ˆçš„ç»“æ„åŒ–å‰ªæéƒ¨ç½²æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03246v1",
      "published_date": "2025-09-25 19:16:50 UTC",
      "updated_date": "2025-09-25 19:16:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:11:47.988383+00:00"
    },
    {
      "arxiv_id": "2509.21443v1",
      "title": "One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational Moral Reasoning",
      "title_zh": "å•ä¸€æ¨¡å‹ï¼Œå¤šé‡é“å¾·ï¼šæ­ç¤ºè®¡ç®—é“å¾·æ¨ç†ä¸­çš„è·¨è¯­è¨€å¤±è°ƒ",
      "authors": [
        "Sualeha Farid",
        "Jayden Lin",
        "Zean Chen",
        "Shivani Kumar",
        "David Jurgens"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed in multilingual and multicultural environments where moral reasoning is essential for generating ethically appropriate responses. Yet, the dominant pretraining of LLMs on English-language data raises critical concerns about their ability to generalize judgments across diverse linguistic and cultural contexts. In this work, we systematically investigate how language mediates moral decision-making in LLMs. We translate two established moral reasoning benchmarks into five culturally and typologically diverse languages, enabling multilingual zero-shot evaluation. Our analysis reveals significant inconsistencies in LLMs' moral judgments across languages, often reflecting cultural misalignment. Through a combination of carefully constructed research questions, we uncover the underlying drivers of these disparities, ranging from disagreements to reasoning strategies employed by LLMs. Finally, through a case study, we link the role of pretraining data in shaping an LLM's moral compass. Through this work, we distill our insights into a structured typology of moral reasoning errors that calls for more culturally-aware AI.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿæ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¸åŒè¯­è¨€å’Œæ–‡åŒ–è¯­å¢ƒä¸‹çš„è®¡ç®—é“å¾·æ¨ç†(Computational Moral Reasoning)è¡¨ç°ï¼Œæ—¨åœ¨æ­ç¤ºè¯­è¨€å¦‚ä½•ä¸­ä»‹é“å¾·å†³ç­–è¿‡ç¨‹ã€‚é€šè¿‡å°†ä¸¤ä¸ªç°æœ‰çš„é“å¾·æ¨ç†åŸºå‡†ç¿»è¯‘æˆäº”ç§æ–‡åŒ–å’Œç±»å‹å¤šæ ·çš„è¯­è¨€ï¼Œç ”ç©¶è€…å¯¹æ¨¡å‹è¿›è¡Œäº†å¤šè¯­è¨€é›¶æ ·æœ¬è¯„ä¼°(Multilingual Zero-shot Evaluation)ã€‚å®éªŒå‘ç°LLMsåœ¨ä¸åŒè¯­è¨€é—´çš„é“å¾·åˆ¤æ–­å­˜åœ¨æ˜¾è‘—ä¸ä¸€è‡´ï¼Œæ·±åˆ»åæ˜ äº†æ–‡åŒ–å¤±é…(Cultural Misalignment)ç°è±¡ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†å¯¼è‡´è¿™äº›å·®å¼‚çš„æ½œåœ¨å› ç´ ï¼ŒåŒ…æ‹¬è§‚ç‚¹åˆ†æ­§ä»¥åŠæ¨¡å‹æ‰€é‡‡ç”¨çš„æ¨ç†ç­–ç•¥(Reasoning Strategies)å·®å¼‚ã€‚æ¡ˆä¾‹ç ”ç©¶è¯å®äº†é¢„è®­ç»ƒæ•°æ®(Pretraining Data)åœ¨å¡‘é€ æ¨¡å‹é“å¾·å‡†åˆ™ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚æœ€ç»ˆï¼Œè¯¥å·¥ä½œæå‡ºäº†ä¸€å¥—ç»“æ„åŒ–çš„é“å¾·æ¨ç†é”™è¯¯ç±»å‹å­¦(Typology of Moral Reasoning Errors)ï¼Œä¸ºæ„å»ºæ›´å…·æ–‡åŒ–æ„è¯†(Culturally-aware)çš„AIç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "22 pages, 11 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.21443v1",
      "published_date": "2025-09-25 19:14:17 UTC",
      "updated_date": "2025-09-25 19:14:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:11:49.265833+00:00"
    },
    {
      "arxiv_id": "2509.21434v2",
      "title": "Foundation models for high-energy physics",
      "title_zh": "é«˜èƒ½ç‰©ç†åŸºç¡€æ¨¡å‹",
      "authors": [
        "Anna Hallin"
      ],
      "abstract": "The rise of foundation models -- large, pretrained machine learning models that can be finetuned to a variety of tasks -- has revolutionized the fields of natural language processing and computer vision. In high-energy physics, the question of whether these models can be implemented directly in physics research, or even built from scratch, tailored for particle physics data, has generated an increasing amount of attention. This review, which is the first on the topic of foundation models in high-energy physics, summarizes and discusses the research that has been published in the field so far.",
      "tldr_zh": "è¯¥ç»¼è¿°æ¢è®¨äº†åŸºç¡€æ¨¡å‹ (foundation models) åœ¨é«˜èƒ½ç‰©ç† (high-energy physics) é¢†åŸŸçš„åº”ç”¨ä¸æ½œåŠ›ï¼Œæ­¤å‰è¿™ç±»å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹å·²åœ¨è‡ªç„¶è¯­è¨€å¤„ç† (natural language processing) å’Œè®¡ç®—æœºè§†è§‰ (computer vision) é¢†åŸŸå¼•å‘äº†é©å‘½æ€§å˜é©ã€‚æ–‡ç« æ·±å…¥ç ”ç©¶äº†å°†è¿™äº›æ¨¡å‹ç›´æ¥åº”ç”¨äºç‰©ç†ç ”ç©¶çš„å¯èƒ½æ€§ï¼Œä»¥åŠé’ˆå¯¹ç²’å­ç‰©ç†æ•°æ® (particle physics data) ä»é›¶æ„å»ºå®šåˆ¶åŒ–æ¨¡å‹çš„æŠ€æœ¯è·¯å¾„ã€‚ä½œä¸ºè¯¥é¢†åŸŸçš„é¦–ç¯‡ç»¼è¿°ï¼Œå®ƒç³»ç»Ÿåœ°æ€»ç»“å¹¶è¯„è¿°äº†ç›®å‰å·²å‘è¡¨çš„ç›¸å…³ç ”ç©¶æˆæœã€‚è¯¥è®ºæ–‡ä¸ºé«˜èƒ½ç‰©ç†ç ”ç©¶äººå‘˜ç†è§£å¹¶åˆ©ç”¨åŸºç¡€æ¨¡å‹è¿™ä¸€æ–°å…´æŠ€æœ¯æä¾›äº†å…¨é¢çš„å‚è€ƒæ¡†æ¶ã€‚",
      "categories": [
        "hep-ph",
        "cs.AI",
        "cs.LG",
        "hep-ex",
        "physics.data-an"
      ],
      "primary_category": "hep-ph",
      "comment": "Submitted to SciPost Physics Proceedings (EuCAIFCon 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.21434v2",
      "published_date": "2025-09-25 19:03:37 UTC",
      "updated_date": "2026-01-09 15:10:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:11:55.371054+00:00"
    },
    {
      "arxiv_id": "2509.21319v2",
      "title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards",
      "title_zh": "RLBFFï¼šè¿æ¥äººç±»åé¦ˆä¸å¯éªŒè¯å¥–åŠ±çš„äºŒå…ƒçµæ´»åé¦ˆ",
      "authors": [
        "Zhilin Wang",
        "Jiaqi Zeng",
        "Olivier Delalleau",
        "Ellie Evans",
        "Daniel Egert",
        "Hoo-Chang Shin",
        "Felipe Soares",
        "Yi Dong",
        "Oleksii Kuchaiev"
      ],
      "abstract": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM post-training, each offering distinct advantages. However, RLHF struggles with interpretability and reward hacking because it relies on human judgments that usually lack explicit criteria, whereas RLVR is limited in scope by its focus on correctness-based verifiers. We propose Reinforcement Learning with Binary Flexible Feedback (RLBFF), which combines the versatility of human-driven preferences with the precision of rule-based verification, enabling reward models to capture nuanced aspects of response quality beyond mere correctness. RLBFF extracts principles that can be answered in a binary fashion (e.g. accuracy of information: yes, or code readability: no) from natural language feedback. Such principles can then be used to ground Reward Model training as an entailment task (response satisfies or does not satisfy an arbitrary principle). We show that Reward Models trained in this manner can outperform Bradley-Terry models when matched for data and achieve top performance on RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24, 2025). Additionally, users can specify principles of interest at inference time to customize the focus of our reward models, in contrast to Bradley-Terry models. Finally, we present a fully open source recipe (including data) to align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the performance of o3-mini and DeepSeek R1 on general alignment benchmarks of MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost). Models: https://huggingface.co/collections/nvidia/reward-models-10-2025",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RLBFF (Reinforcement Learning with Binary Flexible Feedback)ï¼Œæ—¨åœ¨è§£å†³äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹  (RLHF) åœ¨å¯è§£é‡Šæ€§ä¸Šçš„ç¼ºé™·ä»¥åŠå¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹  (RLVR) ä»…å±€é™äºæ­£ç¡®æ€§éªŒè¯çš„å±€é™ã€‚è¯¥æ¡†æ¶ä»è‡ªç„¶è¯­è¨€åé¦ˆä¸­æå–å¯è¿›è¡ŒäºŒå…ƒåˆ¤æ–­çš„åŸåˆ™ï¼ˆå¦‚ä»£ç å¯è¯»æ€§æˆ–ä¿¡æ¯å‡†ç¡®æ€§ï¼‰ï¼Œå¹¶å°†å¥–åŠ±æ¨¡å‹ (Reward Model) çš„è®­ç»ƒå»ºæ¨¡ä¸ºä¸€ç§è•´å«ä»»åŠ¡ (entailment task)ï¼Œä»è€Œæ•æ‰è¶…è¶Šå•çº¯æ­£ç¡®æ€§çš„ç»†å¾®è´¨é‡ç»´åº¦ã€‚RLBFFç»“åˆäº†äººç±»åå¥½çš„çµæ´»æ€§ä¸è§„åˆ™éªŒè¯çš„ç²¾ç¡®æ€§ï¼Œä¸”æ”¯æŒç”¨æˆ·åœ¨æ¨ç†æ—¶åŠ¨æ€æŒ‡å®šè¯„ä¼°åŸåˆ™ï¼Œåœ¨ RM-Bench (86.2%) å’Œ JudgeBench (81.4%) ç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„ Bradley-Terry æ¨¡å‹ã€‚é€šè¿‡è¯¥æ–¹æ³•å¯¹é½çš„ Qwen3-32B æ¨¡å‹åœ¨é€šç”¨å¯¹é½æ¦œå•ä¸Šè¾¾åˆ°äº†ä¸ o3-mini å’Œ DeepSeek R1 ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ï¼Œè€Œæ¨ç†æˆæœ¬ä»…ä¸ºåè€…çš„ 5% ä»¥ä¸‹ï¼Œä¸ºé«˜æ•ˆã€å¯è§£é‡Šçš„å¤§è¯­è¨€æ¨¡å‹å¯¹é½æä¾›äº†å…¨æ ˆå¼€æºçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Added link to access models: https://huggingface.co/collections/nvidia/reward-models-10-2025",
      "pdf_url": "https://arxiv.org/pdf/2509.21319v2",
      "published_date": "2025-09-25 16:19:06 UTC",
      "updated_date": "2025-10-30 17:09:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:12:02.865381+00:00"
    },
    {
      "arxiv_id": "2509.21318v1",
      "title": "SD3.5-Flash: Distribution-Guided Distillation of Generative Flows",
      "title_zh": "SD3.5-Flashï¼šåˆ†å¸ƒå¼•å¯¼çš„ç”Ÿæˆæµè’¸é¦",
      "authors": [
        "Hmrishav Bandyopadhyay",
        "Rahim Entezari",
        "Jim Scott",
        "Reshinth Adithyan",
        "Yi-Zhe Song",
        "Varun Jampani"
      ],
      "abstract": "We present SD3.5-Flash, an efficient few-step distillation framework that brings high-quality image generation to accessible consumer devices. Our approach distills computationally prohibitive rectified flow models through a reformulated distribution matching objective tailored specifically for few-step generation. We introduce two key innovations: \"timestep sharing\" to reduce gradient noise and \"split-timestep fine-tuning\" to improve prompt alignment. Combined with comprehensive pipeline optimizations like text encoder restructuring and specialized quantization, our system enables both rapid generation and memory-efficient deployment across different hardware configurations. This democratizes access across the full spectrum of devices, from mobile phones to desktop computers. Through extensive evaluation including large-scale user studies, we demonstrate that SD3.5-Flash consistently outperforms existing few-step methods, making advanced generative AI truly accessible for practical deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SD3.5-Flashï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å°‘æ­¥è’¸é¦æ¡†æ¶(few-step distillation framework)ï¼Œæ—¨åœ¨å°†é«˜è´¨é‡çš„å›¾åƒç”Ÿæˆèƒ½åŠ›å¼•å…¥ä»æ‰‹æœºåˆ°å°å¼æœºçš„å„ç±»æ¶ˆè´¹çº§è®¾å¤‡ã€‚è¯¥æ–¹æ³•é€šè¿‡é‡æ–°å®šä¹‰çš„åˆ†å¸ƒåŒ¹é…ç›®æ ‡ï¼Œå¯¹è®¡ç®—å¼€é”€å·¨å¤§çš„ä¿®æ­£æµ(rectified flow)æ¨¡å‹è¿›è¡Œè’¸é¦ï¼Œå¹¶ç‰¹åˆ«é’ˆå¯¹å°‘æ­¥ç”Ÿæˆåœºæ™¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚ç ”ç©¶å¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ï¼šâ€œæ—¶é—´æ­¥å…±äº«(timestep sharing)â€ä»¥å‡å°‘æ¢¯åº¦å™ªå£°ï¼Œä»¥åŠâ€œåˆ†æ­¥å¾®è°ƒ(split-timestep fine-tuning)â€ä»¥æå‡æç¤ºè¯å¯¹é½(prompt alignment)è´¨é‡ã€‚ç»“åˆæ–‡æœ¬ç¼–ç å™¨é‡æ„å’Œä¸“é—¨çš„é‡åŒ–(quantization)ç­‰å…¨æµç¨‹ä¼˜åŒ–æ‰‹æ®µï¼Œè¯¥ç³»ç»Ÿåœ¨ä¿è¯å¿«é€Ÿç”Ÿæˆçš„åŒæ—¶å®ç°äº†å†…å­˜é«˜æ•ˆéƒ¨ç½²ã€‚å¤§è§„æ¨¡ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒSD3.5-Flashåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å°‘æ­¥ç”Ÿæˆæ–¹æ³•ï¼Œä¸ºå…ˆè¿›ç”Ÿæˆå¼AIçš„å®é™…åº”ç”¨å’Œæ™®åŠå¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://hmrishavbandy.github.io/sd35flash/",
      "pdf_url": "https://arxiv.org/pdf/2509.21318v1",
      "published_date": "2025-09-25 16:07:38 UTC",
      "updated_date": "2025-09-25 16:07:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:12:04.051240+00:00"
    },
    {
      "arxiv_id": "2509.21310v1",
      "title": "SAGE: A Realistic Benchmark for Semantic Understanding",
      "title_zh": "SAGEï¼šé¢å‘è¯­ä¹‰ç†è§£çš„çœŸå®åœºæ™¯åŸºå‡†æµ‹è¯•",
      "authors": [
        "Samarth Goel",
        "Reagan J. Lee",
        "Kannan Ramchandran"
      ],
      "abstract": "As large language models (LLMs) achieve strong performance on traditional benchmarks, there is an urgent need for more challenging evaluation frameworks that probe deeper aspects of semantic understanding. We introduce SAGE (Semantic Alignment & Generalization Evaluation), a rigorous benchmark designed to assess both embedding models and similarity metrics across five categories: Human Preference Alignment, Transformation Robustness, Information Sensitivity, Clustering Performance, and Retrieval Robustness. Unlike existing benchmarks that focus on isolated capabilities, SAGE evaluates semantic understanding through adversarial conditions, noisy transformations, and nuanced human judgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embedding models and classical metrics reveals significant performance gaps, with no single approach excelling across all dimensions. For instance, while state-of-the-art embedding models like OpenAI's text-embedding-3-large dominate in aligning with human preferences (0.682 vs. 0.591 for the best classical metric), they are significantly outperformed by classical metrics on information sensitivity tasks, where Jaccard Similarity achieves a score of 0.905 compared to the top embedding score of 0.794. SAGE further uncovers critical trade-offs: OpenAI's text-embedding-3-small achieves the highest clustering performance (0.483) but demonstrates extreme brittleness with the lowest robustness score (0.011). SAGE exposes critical limitations in current semantic understanding capabilities and provides a more realistic assessment of model robustness for real-world deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SAGE (Semantic Alignment & Generalization Evaluation)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æ·±åº¦è¯„ä¼°åµŒå…¥æ¨¡å‹(Embedding Models)å’Œç›¸ä¼¼åº¦åº¦é‡(Similarity Metrics)è¯­ä¹‰ç†è§£èƒ½åŠ›çš„ä¸¥è°¨åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚SAGEæ¶µç›–äº†äººç±»åå¥½å¯¹é½(Human Preference Alignment)ã€è½¬æ¢é²æ£’æ€§(Transformation Robustness)ã€ä¿¡æ¯æ•æ„Ÿæ€§(Information Sensitivity)ã€èšç±»æ€§èƒ½(Clustering Performance)å’Œæ£€ç´¢é²æ£’æ€§(Retrieval Robustness)äº”ä¸ªå…³é”®ç»´åº¦ã€‚é€šè¿‡åœ¨30å¤šä¸ªæ•°æ®é›†ä¸Šå¼•å…¥å¯¹æŠ—æ€§æ¡ä»¶å’Œå™ªå£°è½¬æ¢ï¼Œè¯¥åŸºå‡†æ­éœ²äº†ç°æœ‰æ¨¡å‹åœ¨å¤æ‚è¯­ä¹‰ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶OpenAIçš„text-embedding-3-largeåœ¨å¯¹é½äººç±»åå¥½æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ä¿¡æ¯æ•æ„Ÿæ€§ä»»åŠ¡ä¸­å´æ˜¾è‘—é€Šäºä»¥Jaccard Similarityä¸ºä»£è¡¨çš„ç»å…¸åº¦é‡æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°æ¨¡å‹æ€§èƒ½å­˜åœ¨æ˜¾è‘—æƒè¡¡ï¼Œä¾‹å¦‚text-embedding-3-smallè™½åœ¨èšç±»ä»»åŠ¡ä¸­é¢†å…ˆï¼Œå´åœ¨é²æ£’æ€§æµ‹è¯•ä¸­å¾—åˆ†æœ€ä½ã€‚SAGEä¸ºè¯„ä¼°æ¨¡å‹åœ¨çœŸå®åœºæ™¯éƒ¨ç½²ä¸­çš„é²æ£’æ€§æä¾›äº†æ›´çœŸå®çš„å‚è€ƒï¼Œå¹¶ä¸ºæœªæ¥æå‡è¯­ä¹‰ç†è§£èƒ½åŠ›æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent Abilities, and Scaling",
      "pdf_url": "https://arxiv.org/pdf/2509.21310v1",
      "published_date": "2025-09-25 15:27:15 UTC",
      "updated_date": "2025-09-25 15:27:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:12:10.054761+00:00"
    },
    {
      "arxiv_id": "2509.22734v1",
      "title": "Automated Formative Feedback for Short-form Writing: An LLM-Driven Approach and Adoption Analysis",
      "title_zh": "é’ˆå¯¹çŸ­ç¯‡å†™ä½œçš„è‡ªåŠ¨åŒ–å½¢æˆæ€§åé¦ˆï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ä¸é‡‡çº³åˆ†æ",
      "authors": [
        "Tiago Fernandes Tavares",
        "Luciano Pereira Soares"
      ],
      "abstract": "This paper explores the development and adoption of AI-based formative feedback in the context of biweekly reports in an engineering Capstone program. Each student is required to write a short report detailing their individual accomplishments over the past two weeks, which is then assessed by their advising professor. An LLM-powered tool was developed to provide students with personalized feedback on their draft reports, guiding them toward improved completeness and quality. Usage data across two rounds revealed an initial barrier to adoption, with low engagement rates. However, students who engaged in the AI feedback system demonstrated the ability to use it effectively, leading to improvements in the completeness and quality of their reports. Furthermore, the tool's task-parsing capabilities provided a novel approach to identify potential student organizational tasks and deliverables. The findings suggest initial skepticism toward the tool with a limited adoption within the studied context, however, they also highlight the potential for AI-driven tools to provide students and professors valuable insights and formative support.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å·¥ç¨‹ Capstone program çš„åŒå‘¨æŠ¥å‘ŠèƒŒæ™¯ä¸‹ï¼Œå¼€å‘å¹¶åº”ç”¨åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è‡ªåŠ¨åŒ– formative feedback ç³»ç»Ÿã€‚è¯¥å·¥å…·æ—¨åœ¨ä¸ºå­¦ç”Ÿçš„æŠ¥å‘Šè‰ç¨¿æä¾›ä¸ªæ€§åŒ–åé¦ˆï¼Œå¼•å¯¼å…¶æå‡å†…å®¹çš„å®Œæ•´æ€§ä¸è´¨é‡ã€‚æ•°æ®åˆ†ææ˜¾ç¤ºï¼Œç³»ç»Ÿåœ¨åˆæœŸé¢ä¸´ä¸€å®šçš„é‡‡ç”¨éšœç¢å’Œè¾ƒä½çš„å‚ä¸ç‡ï¼Œåæ˜ å‡ºå­¦ç”Ÿå¯¹è¯¥å·¥å…·æŒæœ‰åˆå§‹æ€€ç–‘æ€åº¦ã€‚ç„¶è€Œï¼Œå®é™…å‚ä¸å…¶ä¸­çš„å­¦ç”Ÿèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨ AI åé¦ˆï¼Œä»è€Œæ˜¾è‘—æ”¹å–„äº†æŠ¥å‘Šçš„å™è¿°è´¨é‡ã€‚æ­¤å¤–ï¼Œè¯¥å·¥å…·çš„ task-parsing èƒ½åŠ›ä¸ºè¯†åˆ«å­¦ç”Ÿç»„ç»‡ä»»åŠ¡å’Œ deliverables æä¾›äº†ä¸€ç§æ–°é¢–æ–¹æ³•ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡åœ¨ç‰¹å®šæ•™å­¦èƒŒæ™¯ä¸‹é‡‡ç”¨ç‡æœ‰é™ï¼Œä½† LLM é©±åŠ¨çš„å·¥å…·åœ¨ä¸ºå¸ˆç”Ÿæä¾›ä»·å€¼æ´å¯Ÿå’Œ formative support æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.22734v1",
      "published_date": "2025-09-25 15:17:51 UTC",
      "updated_date": "2025-09-25 15:17:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:12:14.864964+00:00"
    },
    {
      "arxiv_id": "2509.21433v1",
      "title": "DyME: Dynamic Multi-Concept Erasure in Diffusion Models with Bi-Level Orthogonal LoRA Adaptation",
      "title_zh": "DyMEï¼šåŸºäºåŒå±‚æ­£äº¤ LoRA é€‚é…çš„æ‰©æ•£æ¨¡å‹åŠ¨æ€å¤šæ¦‚å¿µæ“¦é™¤",
      "authors": [
        "Jiaqi Liu",
        "Lan Zhang",
        "Xiaoyong Yuan"
      ],
      "abstract": "Text-to-image diffusion models (DMs) inadvertently reproduce copyrighted styles and protected visual concepts, raising legal and ethical concerns. Concept erasure has emerged as a safeguard, aiming to selectively suppress such concepts through fine-tuning. However, existing methods do not scale to practical settings where providers must erase multiple and possibly conflicting concepts. The core bottleneck is their reliance on static erasure: a single checkpoint is fine-tuned to remove all target concepts, regardless of the actual erasure needs at inference. This rigid design mismatches real-world usage, where requests vary per generation, leading to degraded erasure success and reduced fidelity for non-target content. We propose DyME, an on-demand erasure framework that trains lightweight, concept-specific LoRA adapters and dynamically composes only those needed at inference. This modular design enables flexible multi-concept erasure, but naive composition causes interference among adapters, especially when many or semantically related concepts are suppressed. To overcome this, we introduce bi-level orthogonality constraints at both the feature and parameter levels, disentangling representation shifts and enforcing orthogonal adapter subspaces. We further develop ErasureBench-H, a new hierarchical benchmark with brand-series-character structure, enabling principled evaluation across semantic granularities and erasure set sizes. Experiments on ErasureBench-H and standard datasets (e.g., CIFAR-100, Imagenette) demonstrate that DyME consistently outperforms state-of-the-art baselines, achieving higher multi-concept erasure fidelity with minimal collateral degradation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹(Diffusion Models)æå‡ºäº† DyMEï¼Œä¸€ç§æ—¨åœ¨æŒ‰éœ€æ“¦é™¤å—ç‰ˆæƒæˆ–æ³•å¾‹ä¿æŠ¤æ¦‚å¿µçš„åŠ¨æ€å¤šæ¦‚å¿µæ“¦é™¤æ¡†æ¶ã€‚DyME æ”¹å˜äº†ä¼ ç»Ÿçš„é™æ€æ“¦é™¤æ¨¡å¼ï¼Œé€šè¿‡è®­ç»ƒè½»é‡çº§çš„æ¦‚å¿µç‰¹å®š LoRA é€‚é…å™¨ï¼Œå¹¶åœ¨æ¨ç†æ—¶æ ¹æ®å…·ä½“éœ€æ±‚åŠ¨æ€ç»„åˆï¼Œæœ‰æ•ˆè§£å†³äº†å¤„ç†å¤šä¸ªæˆ–å†²çªæ¦‚å¿µæ—¶çš„æ‰©å±•æ€§éš¾é¢˜ã€‚ä¸ºäº†é˜²æ­¢é€‚é…å™¨é—´çš„ç›¸äº’å¹²æ‰°ï¼Œç ”ç©¶è€…å¼•å…¥äº†ç‰¹å¾çº§å’Œå‚æ•°çº§çš„åŒå±‚æ­£äº¤æ€§çº¦æŸ(bi-level orthogonality constraints)ï¼Œä»¥è§£è€¦è¡¨ç¤ºåç§»å¹¶ç¡®ä¿é€‚é…å™¨ç©ºé—´çš„ç‹¬ç«‹æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†å±‚çº§åŒ–åŸºå‡†æµ‹è¯•é›† ErasureBench-Hï¼Œç”¨äºåœ¨ä¸åŒè¯­ä¹‰ç²’åº¦ä¸‹ç³»ç»Ÿè¯„ä¼°æ“¦é™¤æ•ˆæœã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDyME åœ¨ ErasureBench-H åŠå…¶ä»–æ ‡å‡†æ•°æ®é›†ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†æ›´é«˜çš„å¤šæ¦‚å¿µæ“¦é™¤ä¿çœŸåº¦ï¼Œä¸”å¯¹éç›®æ ‡å†…å®¹çš„å½±å“æå°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21433v1",
      "published_date": "2025-09-25 15:16:17 UTC",
      "updated_date": "2025-09-25 15:16:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:12:39.397824+00:00"
    },
    {
      "arxiv_id": "2509.21296v1",
      "title": "No Prior, No Leakage: Revisiting Reconstruction Attacks in Trained Neural Networks",
      "title_zh": "æ— å…ˆéªŒï¼Œæ— æ³„éœ²ï¼šé‡æ–°å®¡è§†å·²è®­ç»ƒç¥ç»ç½‘ç»œä¸­çš„é‡æ„æ”»å‡»",
      "authors": [
        "Yehonatan Refael",
        "Guy Smorodinsky",
        "Ofir Lindenbaum",
        "Itay Safran"
      ],
      "abstract": "The memorization of training data by neural networks raises pressing concerns for privacy and security. Recent work has shown that, under certain conditions, portions of the training set can be reconstructed directly from model parameters. Some of these methods exploit implicit bias toward margin maximization, suggesting that properties often regarded as beneficial for generalization may actually compromise privacy. Yet despite striking empirical demonstrations, the reliability of these attacks remains poorly understood and lacks a solid theoretical foundation. In this work, we take a complementary perspective: rather than designing stronger attacks, we analyze the inherent weaknesses and limitations of existing reconstruction methods and identify conditions under which they fail. We rigorously prove that, without incorporating prior knowledge about the data, there exist infinitely many alternative solutions that may lie arbitrarily far from the true training set, rendering reconstruction fundamentally unreliable. Empirically, we further demonstrate that exact duplication of training examples occurs only by chance. Our results refine the theoretical understanding of when training set leakage is possible and offer new insights into mitigating reconstruction attacks. Remarkably, we demonstrate that networks trained more extensively, and therefore satisfying implicit bias conditions more strongly -- are, in fact, less susceptible to reconstruction attacks, reconciling privacy with the need for strong generalization in this setting.",
      "tldr_zh": "è¯¥ç ”ç©¶é‡æ–°å®¡è§†äº†é’ˆå¯¹å·²è®­ç»ƒç¥ç»ç½‘ç»œçš„é‡æ„æ”»å‡» (Reconstruction Attacks)ï¼Œæ·±å…¥åˆ†æäº†ç°æœ‰æ”»å‡»æ–¹æ³•çš„å†…åœ¨å¼±ç‚¹ä¸å±€é™æ€§ã€‚é€šè¿‡ä¸¥å¯†çš„ç†è®ºè¯æ˜ï¼Œç ”ç©¶æŒ‡å‡ºåœ¨ä¸å¼•å…¥æ•°æ®å…ˆéªŒ (Prior) çš„æƒ…å†µä¸‹ï¼Œç”±äºå­˜åœ¨æ— æ•°è¿œç¦»çœŸå®è®­ç»ƒé›†çš„æ›¿ä»£è§£ï¼Œé‡æ„æ”»å‡»åœ¨æ ¹æœ¬ä¸Šæ˜¯ä¸å¯é çš„ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥è¡¨æ˜ï¼Œè®­ç»ƒæ ·æœ¬çš„ç²¾ç¡®æ³„éœ²å¾€å¾€çº¯å±å¶ç„¶ï¼Œè€Œéå¿…ç„¶ç»“æœã€‚è¯¥ç ”ç©¶æœ€æ˜¾è‘—çš„å‘ç°æ˜¯ï¼Œç»è¿‡æ›´å……åˆ†è®­ç»ƒã€å…·æœ‰æ›´å¼ºéšå¼åå·® (Implicit Bias) ä»¥è·å¾—æ›´å¥½æ³›åŒ–æ€§èƒ½çš„ç½‘ç»œï¼Œå®é™…ä¸Šæ›´ä¸å®¹æ˜“å—åˆ°é‡æ„æ”»å‡»çš„å½±å“ã€‚è¿™ä¸€ç»“è®ºæˆåŠŸè°ƒå’Œäº†æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸éšç§ä¿æŠ¤ä¹‹é—´çš„çŸ›ç›¾ï¼Œå¹¶ä¸ºç†è§£å’Œç¼“è§£è®­ç»ƒé›†æ³„éœ²æä¾›äº†æ–°çš„ç†è®ºåŸºç¡€ä¸è§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21296v1",
      "published_date": "2025-09-25 15:14:08 UTC",
      "updated_date": "2025-09-25 15:14:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:12:35.696552+00:00"
    },
    {
      "arxiv_id": "2509.21291v1",
      "title": "VC-Agent: An Interactive Agent for Customized Video Dataset Collection",
      "title_zh": "VC-Agentï¼šç”¨äºå®šåˆ¶åŒ–è§†é¢‘æ•°æ®é›†é‡‡é›†çš„äº¤äº’å¼æ™ºèƒ½ä½“",
      "authors": [
        "Yidan Zhang",
        "Mutian Xu",
        "Yiming Hao",
        "Kun Zhou",
        "Jiahao Chang",
        "Xiaoqiang Liu",
        "Pengfei Wan",
        "Hongbo Fu",
        "Xiaoguang Han"
      ],
      "abstract": "Facing scaling laws, video data from the internet becomes increasingly important. However, collecting extensive videos that meet specific needs is extremely labor-intensive and time-consuming. In this work, we study the way to expedite this collection process and propose VC-Agent, the first interactive agent that is able to understand users' queries and feedback, and accordingly retrieve/scale up relevant video clips with minimal user input. Specifically, considering the user interface, our agent defines various user-friendly ways for the user to specify requirements based on textual descriptions and confirmations. As for agent functions, we leverage existing multi-modal large language models to connect the user's requirements with the video content. More importantly, we propose two novel filtering policies that can be updated when user interaction is continually performed. Finally, we provide a new benchmark for personalized video dataset collection, and carefully conduct the user study to verify our agent's usage in various real scenarios. Extensive experiments demonstrate the effectiveness and efficiency of our agent for customized video dataset collection. Project page: https://allenyidan.github.io/vcagent_page/.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äº’è”ç½‘è§†é¢‘æ•°æ®é‡‡é›†è¿‡ç¨‹ä¸­æé«˜çš„äººåŠ›å’Œæ—¶é—´æˆæœ¬é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªèƒ½å¤Ÿç†è§£ç”¨æˆ·æŸ¥è¯¢ä¸åé¦ˆçš„äº¤äº’å¼æ™ºèƒ½ä½“ VC-Agentã€‚è¯¥æ™ºèƒ½ä½“é€šè¿‡æ–‡æœ¬æè¿°å’Œç¡®è®¤ç­‰ç”¨æˆ·å‹å¥½æ–¹å¼æ˜ç¡®éœ€æ±‚ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Multi-modal Large Language Models, MLLMs) å®ç°ç”¨æˆ·éœ€æ±‚ä¸è§†é¢‘å†…å®¹çš„ç²¾å‡†å¯¹æ¥ã€‚å…¶æ ¸å¿ƒè´¡çŒ®åœ¨äºæå‡ºäº†ä¸¤ç§å¯éšç”¨æˆ·æŒç»­äº¤äº’è€Œå®æ—¶æ›´æ–°çš„æ–°å‹è¿‡æ»¤ç­–ç•¥ (filtering policies)ï¼Œèƒ½å¤Ÿä»¥æä½çš„ç”¨æˆ·è¾“å…¥å®ç°è§†é¢‘ç‰‡æ®µçš„æ£€ç´¢ä¸è§„æ¨¡åŒ–æ‰©å……ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜æä¾›äº†ä¸€ä¸ªä¸ªæ€§åŒ–è§†é¢‘æ•°æ®é›†é‡‡é›†çš„æ–°åŸºå‡† (benchmark)ï¼Œå¹¶é€šè¿‡ç”¨æˆ·ç ”ç©¶éªŒè¯äº†å…¶åœ¨å¤šç§çœŸå®åœºæ™¯ä¸‹çš„å®ç”¨ä»·å€¼ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒVC-Agent èƒ½å¤Ÿæ˜¾è‘—æå‡å®šåˆ¶åŒ–è§†é¢‘æ•°æ®é›†é‡‡é›†çš„æ•ˆç‡ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Project page: https://allenyidan.github.io/vcagent_page/",
      "pdf_url": "https://arxiv.org/pdf/2509.21291v1",
      "published_date": "2025-09-25 15:08:28 UTC",
      "updated_date": "2025-09-25 15:08:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:12:38.985298+00:00"
    },
    {
      "arxiv_id": "2509.21287v1",
      "title": "DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding",
      "title_zh": "DisCoCLIPï¼šé¢å‘è§†è§‰-è¯­è¨€ç†è§£çš„åˆ†å¸ƒå¼ç»„åˆå¼ é‡ç½‘ç»œç¼–ç å™¨",
      "authors": [
        "Kin Ian Lo",
        "Hala Hawashin",
        "Mina Abbaszadeh",
        "Tilen Limback-Stokin",
        "Hadi Wazni",
        "Mehrnoosh Sadrzadeh"
      ],
      "abstract": "Recent vision-language models excel at large-scale image-text alignment but often neglect the compositional structure of language, leading to failures on tasks that hinge on word order and predicate-argument structure. We introduce DisCoCLIP, a multimodal encoder that combines a frozen CLIP vision transformer with a novel tensor network text encoder that explicitly encodes syntactic structure. Sentences are parsed with a Combinatory Categorial Grammar parser to yield distributional word tensors whose contractions mirror the sentence's grammatical derivation. To keep the model efficient, high-order tensors are factorized with tensor decompositions, reducing parameter count from tens of millions to under one million. Trained end-to-end with a self-supervised contrastive loss, DisCoCLIP markedly improves sensitivity to verb semantics and word order: it raises CLIP's SVO-Probes verb accuracy from 77.6% to 82.4%, boosts ARO attribution and relation scores by over 9% and 4%, and achieves 93.7% on a newly introduced SVO-Swap benchmark. These results demonstrate that embedding explicit linguistic structure via tensor networks yields interpretable, parameter-efficient representations that substantially improve compositional reasoning in vision-language tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DisCoCLIPï¼Œè¿™æ˜¯ä¸€ç§å°†å†»ç»“çš„ CLIP è§†è§‰å˜æ¢å™¨(vision transformer)ä¸æ–°å‹å¼ é‡ç½‘ç»œ(tensor network)æ–‡æœ¬ç¼–ç å™¨ç›¸ç»“åˆçš„å¤šæ¨¡æ€ç¼–ç å™¨ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹å¿½è§†è¯­è¨€ç»„åˆç»“æ„(compositional structure)çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç»„åˆèŒƒç•´è¯­æ³•(Combinatory Categorial Grammar, CCG)è§£æå™¨ç”Ÿæˆåˆ†å¸ƒå¼è¯å¼ é‡ï¼Œé€šè¿‡å¼ é‡æ”¶ç¼©(contractions)é•œåƒå¥å­çš„è¯­æ³•æ¨å¯¼ã€‚ä¸ºäº†ä¿æŒæ¨¡å‹é«˜æ•ˆï¼Œç ”ç©¶é€šè¿‡å¼ é‡åˆ†è§£(tensor decompositions)å¯¹é«˜é˜¶å¼ é‡è¿›è¡Œå¤„ç†ï¼Œå°†å‚æ•°é‡ä»æ•°åƒä¸‡é™ä½è‡³ä¸è¶³ä¸€ç™¾ä¸‡ã€‚DisCoCLIP é‡‡ç”¨è‡ªç›‘ç£å¯¹æ¯”æŸå¤±(self-supervised contrastive loss)è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œæ˜¾è‘—æå‡äº†å¯¹åŠ¨è¯è¯­ä¹‰å’Œè¯åºçš„æ•æ„Ÿæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ SVO-Probes åŠ¨è¯å‡†ç¡®ç‡ä¸Šä» 77.6% æå‡è‡³ 82.4%ï¼Œå¹¶åœ¨ ARO å±æ€§ä¸å…³ç³»è¯„åˆ†ä»¥åŠæ–°å¼•å…¥çš„ SVO-Swap åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚è¿™è¯æ˜äº†é€šè¿‡å¼ é‡ç½‘ç»œåµŒå…¥æ˜¾å¼è¯­è¨€ç»“æ„å¯ä»¥äº§ç”Ÿå…·æœ‰å¯è§£é‡Šæ€§ä¸”å‚æ•°é«˜æ•ˆçš„è¡¨ç¤ºï¼Œä»è€Œå®è´¨æ€§åœ°æ”¹å–„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­çš„ç»„åˆæ¨ç†(compositional reasoning)èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21287v1",
      "published_date": "2025-09-25 15:06:56 UTC",
      "updated_date": "2025-09-25 15:06:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:12:47.993838+00:00"
    },
    {
      "arxiv_id": "2509.21282v1",
      "title": "It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL",
      "title_zh": "éä½ ä¹‹è¿‡ï¼Œè€Œæ˜¯æˆªæ–­ï¼šåŸºäºæ¦‚ç‡å¹³æ»‘çš„å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ è½¯ç½®ä¿¡åŸŸ",
      "authors": [
        "Madeleine Dwyer",
        "Adam Sobey",
        "Adriane Chapman"
      ],
      "abstract": "Training large language models (LLMs) with reinforcement learning (RL) methods such as PPO and GRPO commonly relies on ratio clipping to stabilise updates. While effective at preventing instability, clipping discards information and introduces gradient discontinuities. We propose Probability Smoothing Policy Optimisation (PSPO), which smooths the current policy's probabilities toward the old (behaviour) policy before computing the importance ratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient signal, while interpolation toward the old policy creates a soft trust region that discourages large, destabilising updates, with formal guarantees.\n  We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and Qwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset generalisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO (single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar performance but improves the reasoning leading to clearer and more concise responses which are more logical. Compared to clipped GRPO, GR-PSPO substantially improves performance both the 0.5B and 1.5B models, with a boost of over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¼ºåŒ–å­¦ä¹ (RL)è®­ç»ƒä¸­å¸¸ç”¨çš„æ¯”ç‡è£å‰ª(ratio clipping)æœºåˆ¶æ‰€å¯¼è‡´çš„æ¢¯åº¦ä¸è¿ç»­å’Œä¿¡æ¯ä¸¢å¤±é—®é¢˜ï¼Œæå‡ºäº†æ¦‚ç‡å¹³æ»‘ç­–ç•¥ä¼˜åŒ–(Probability Smoothing Policy Optimisation, PSPO)ã€‚è¯¥æ–¹æ³•å€Ÿé‰´äº†æ ‡ç­¾å¹³æ»‘(label smoothing)çš„æ€æƒ³ï¼Œé€šè¿‡åœ¨è®¡ç®—é‡è¦æ€§æ¯”ç‡å‰å°†å½“å‰ç­–ç•¥çš„æ¦‚ç‡å‘æ—§ç­–ç•¥è¿›è¡Œå¹³æ»‘å¤„ç†ï¼Œä»è€Œåœ¨ä¿ç•™æ¢¯åº¦ä¿¡å·çš„åŒæ—¶æ„å»ºäº†ä¸€ä¸ªå…·æœ‰ç†è®ºä¿è¯çš„è½¯ç½®ä¿¡åŒºåŸŸ(soft trust region)ï¼Œæœ‰æ•ˆæŠ‘åˆ¶äº†ä¸ç¨³å®šçš„å‰§çƒˆæ›´æ–°ã€‚ç ”ç©¶è€…å°†è¯¥æ–¹æ³•é›†æˆåˆ°GRPOæ¡†æ¶ä¸­å®ç°ä¸ºGR-PSPOï¼Œå¹¶é’ˆå¯¹Qwen2.5ç³»åˆ—æ¨¡å‹åœ¨GSM8Kç­‰æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¾®è°ƒå®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGR-PSPOåœ¨0.5Bå’Œ1.5Bæ¨¡å‹ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å¸¦è£å‰ªGRPOï¼Œå…¶ä¸­åœ¨GSM8Kæµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡æå‡å¹…åº¦è¶…è¿‡20%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨SVAMPå’ŒMATH-500ç­‰æ•°æ®é›†ä¸Šå±•ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–æ€§èƒ½ï¼Œä¸”ç›¸æ¯”æ— è£å‰ªæ–¹æ¡ˆï¼Œå…¶ç”Ÿæˆçš„æ¨ç†è¿‡ç¨‹æ›´å…·é€»è¾‘æ€§ä¸”è¡¨è¿°æ›´åŠ ç®€æ´ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21282v1",
      "published_date": "2025-09-25 15:03:18 UTC",
      "updated_date": "2025-09-25 15:03:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:12:55.184080+00:00"
    },
    {
      "arxiv_id": "2509.21278v3",
      "title": "Does FLUX Already Know How to Perform Physically Plausible Image Composition?",
      "title_zh": "FLUX æ˜¯å¦å·²ç»å…·å¤‡å®ç°ç‰©ç†ä¸Šåˆç†çš„å›¾åƒåˆæˆèƒ½åŠ›ï¼Ÿ",
      "authors": [
        "Shilin Lu",
        "Zhuming Lian",
        "Zihan Zhou",
        "Shaocong Zhang",
        "Chen Zhao",
        "Adams Wai-Kin Kong"
      ],
      "abstract": "Image composition aims to seamlessly insert a user-specified object into a new scene, but existing models struggle with complex lighting (e.g., accurate shadows, water reflections) and diverse, high-resolution inputs. Modern text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential physical and resolution priors, yet lack a framework to unleash them without resorting to latent inversion, which often locks object poses into contextually inappropriate orientations, or brittle attention surgery. We propose SHINE, a training-free framework for Seamless, High-fidelity Insertion with Neutralized Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained customization adapters (e.g., IP-Adapter) to guide latents for faithful subject representation while preserving background integrity. Degradation-suppression guidance and adaptive background blending are proposed to further eliminate low-quality outputs and visible seams. To address the lack of rigorous benchmarks, we introduce ComplexCompo, featuring diverse resolutions and challenging conditions such as low lighting, strong illumination, intricate shadows, and reflective surfaces. Experiments on ComplexCompo and DreamEditBench show state-of-the-art performance on standard metrics (e.g., DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward). Code and benchmark will be publicly available upon publication.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†FLUXç­‰é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹åœ¨ç‰©ç†ä¸€è‡´æ€§å›¾åƒåˆæˆé¢†åŸŸçš„æ½œåŠ›ï¼Œé’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚å…‰ç…§ã€é˜´å½±åŠåå°„æ—¶å­˜åœ¨çš„å±€é™æ€§ï¼Œæå‡ºäº†åä¸ºSHINEçš„æ— éœ€è®­ç»ƒ(training-free)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥æµå½¢å¼•å¯¼é”šç‚¹æŸå¤±(manifold-steered anchor loss)ï¼Œåˆ©ç”¨é¢„è®­ç»ƒé€‚é…å™¨(IP-Adapter)åœ¨ç¡®ä¿ç›®æ ‡ç‰©ä½“ç‰¹å¾å‡†ç¡®çš„åŒæ—¶ï¼Œæœ‰æ•ˆé¿å…äº†ä¼ ç»Ÿéšç©ºé—´åè½¬å¯¼è‡´çš„å§¿æ€ä¸å½“é—®é¢˜å¹¶ä¿æŒäº†èƒŒæ™¯å®Œæ•´ã€‚ç ”ç©¶è¿›ä¸€æ­¥ç»“åˆé™è´¨æŠ‘åˆ¶å¼•å¯¼(degradation-suppression guidance)ä¸è‡ªé€‚åº”èƒŒæ™¯èåˆ(adaptive background blending)æŠ€æœ¯ï¼Œæ˜¾è‘—æ¶ˆé™¤äº†ç”Ÿæˆå›¾åƒä¸­çš„å¯è§ç¼éš™å¹¶æå‡äº†è¾“å‡ºè´¨é‡ã€‚ä¸ºäº†è§£å†³ç¼ºä¹ä¸¥è°¨åŸºå‡†çš„é—®é¢˜ï¼Œç ”ç©¶è€…æ¨å‡ºäº†åŒ…å«é«˜åˆ†è¾¨ç‡åŠæç«¯å…‰ç…§ã€å¤æ‚é˜´å½±ç­‰æŒ‘æˆ˜åœºæ™¯çš„ComplexCompoæµ‹è¯•é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSHINEåœ¨DINOv2ã€DreamSimåŠVisionRewardç­‰å¤šä¸ªæ ¸å¿ƒæŒ‡æ ‡å’Œäººç±»åå¥½è¯„åˆ†ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå®ç°äº†æœ€å…ˆè¿›(SOTA)çš„å›¾åƒåˆæˆæ•ˆæœã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2509.21278v3",
      "published_date": "2025-09-25 15:01:49 UTC",
      "updated_date": "2025-11-02 12:29:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:13:10.294567+00:00"
    },
    {
      "arxiv_id": "2509.21275v2",
      "title": "Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM Training",
      "title_zh": "é¢å‘é«˜æ•ˆé•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒçš„ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„å¼¹æ€§æµæ°´çº¿å¹¶è¡Œ",
      "authors": [
        "Shiju Wang",
        "Yujie Wang",
        "Ao Sun",
        "Fangcheng Fu",
        "Zijian Zhu",
        "Bin Cui",
        "Xu Han",
        "Kaisheng Ma"
      ],
      "abstract": "Long context training is crucial for LLM's context extension. Existing schemes, such as sequence parallelism, incur substantial communication overhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness hinges on partitioning granularity. Batch-level PP dividing input samples exhibits high memory consumption in long-context scenario, whereas token-level PP splitting sequences into slices alleviates memory overhead but may incur hardware under-utilization. This trade-off motivates adaptively selecting PP granularity to match resource and workload characteristics. Moreover, sequence length distribution of the real-world dataset exhibits skewness, posing a challenge on PP's workload balance and efficient scheduling. Current static PP scheduling methods overlook the variance of sequence length, leading to suboptimal performance. In this paper, we propose Elastic Pipeline Parallelism (EPP) that orchestrates token-level PP and batch-level PP to adapt to resource and workload heterogeneity. We build InfiniPipe, a distributed training system that unleashes the potential of EPP via (1) a resource-aware and workload-balanced sequence processor that splits long sequences and packs short ones; and (2) a co-optimization methodology that jointly optimizes pipeline schedule and gradient checkpointing via a mechanism named stage-aware chunk-level adaptive checkpointing. Comprehensive experiments demonstrate that InfiniPipe achieves a 1.69x speedup over state-of-the-art systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)é•¿ä¸Šä¸‹æ–‡è®­ç»ƒä¸­çš„æŒ‘æˆ˜ï¼Œåˆ†æäº†ç°æœ‰åºåˆ—å¹¶è¡Œ(sequence parallelism)é€šä¿¡å¼€é”€è¿‡å¤§ä»¥åŠæµæ°´çº¿å¹¶è¡Œ(Pipeline parallelism)åœ¨å†…å­˜å ç”¨ä¸ç¡¬ä»¶åˆ©ç”¨ç‡ä¹‹é—´çš„çŸ›ç›¾ã€‚ä¸ºäº†åº”å¯¹çœŸå®æ•°æ®åºåˆ—é•¿åº¦ä¸å‡å¯¼è‡´çš„è´Ÿè½½å¤±è¡¡ï¼Œä½œè€…æå‡ºäº†å¼¹æ€§æµæ°´çº¿å¹¶è¡Œ(Elastic Pipeline Parallelism, EPP)ï¼Œé€šè¿‡æ•´åˆä»¤ç‰Œçº§(token-level)ä¸æ‰¹æ¬¡çº§(batch-level)å¹¶è¡Œæ¥é€‚åº”å¼‚æ„èµ„æºå’Œè´Ÿè½½ã€‚åŸºäºæ­¤ç†è®ºæ„å»ºçš„InfiniPipeç³»ç»Ÿåˆ©ç”¨èµ„æºæ„ŸçŸ¥ä¸”è´Ÿè½½å‡è¡¡çš„åºåˆ—å¤„ç†å™¨å¯¹åºåˆ—è¿›è¡Œæ™ºèƒ½åˆ‡åˆ†ä¸æ‰“åŒ…ï¼Œå¹¶ç»“åˆäº†é˜¶æ®µæ„ŸçŸ¥çš„å—çº§è‡ªé€‚åº”æ£€æŸ¥ç‚¹(stage-aware chunk-level adaptive checkpointing)ååŒä¼˜åŒ–ç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼ŒInfiniPipeåœ¨æ€§èƒ½ä¸Šæ¯”ç°æœ‰é¡¶å°–ç³»ç»Ÿæå‡äº†1.69å€çš„è®­ç»ƒé€Ÿåº¦ï¼Œä¸ºé«˜æ•ˆé•¿ä¸Šä¸‹æ–‡LLMè®­ç»ƒæä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21275v2",
      "published_date": "2025-09-25 15:01:25 UTC",
      "updated_date": "2025-11-10 02:27:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:13:19.492053+00:00"
    },
    {
      "arxiv_id": "2510.03245v1",
      "title": "Frequency-Aware Model Parameter Explorer: A new attribution method for improving explainability",
      "title_zh": "é¢‘ç‡æ„ŸçŸ¥æ¨¡å‹å‚æ•°æ¢ç´¢å™¨ï¼šä¸€ç§æå‡å¯è§£é‡Šæ€§çš„æ–°å‹å½’å› æ–¹æ³•",
      "authors": [
        "Ali Yavari",
        "Alireza Mohamadi",
        "Elham Beydaghi",
        "Rainer A. Leitgeb"
      ],
      "abstract": "Ensuring the reliability of deep neural networks (DNNs) in the presence of real world noise and intentional perturbations remains a significant challenge. To address this, attribution methods have been proposed, though their efficacy remains suboptimal and necessitates further refinement. In this paper, we propose a novel category of transferable adversarial attacks, called transferable frequency-aware attacks, enabling frequency-aware exploration via both high-and low-frequency components. Based on this type of attacks, we also propose a novel attribution method, named Frequency-Aware Model Parameter Explorer (FAMPE), which improves the explainability for DNNs. Relative to the current state-of-the-art method AttEXplore, our FAMPE attains an average gain of 13.02% in Insertion Score, thereby outperforming existing approaches. Through detailed ablation studies, we also investigate the role of both high- and low-frequency components in explainability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œ (DNNs) åœ¨ç°å®å™ªå£°å’Œæ•…æ„æ‰°åŠ¨ä¸‹å¯é æ€§ä¸è¶³çš„é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰çš„å½’å› æ–¹æ³• (Attribution methods) ä»éœ€è¿›ä¸€æ­¥æ”¹è¿›ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯è¿ç§»å¯¹æŠ—æ”»å‡»æ–¹æ¡ˆï¼Œå³å¯è¿ç§»é¢‘ç‡æ„ŸçŸ¥æ”»å‡» (Transferable frequency-aware attacks)ï¼Œé€šè¿‡ç»“åˆé«˜é¢‘å’Œä½é¢‘ç»„ä»¶å®ç°é¢‘ç‡æ„ŸçŸ¥æ¢ç´¢ã€‚åŸºäºè¿™ä¸€æ”»å‡»æœºåˆ¶ï¼Œç ”ç©¶è€…å¼€å‘äº†åä¸º Frequency-Aware Model Parameter Explorer (FAMPE) çš„æ–°é¢–å½’å› æ–¹æ³•ï¼Œæ—¨åœ¨æå‡æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFAMPE åœ¨ Insertion Score æŒ‡æ ‡ä¸Šç›¸æ¯”ç›®å‰æœ€å…ˆè¿›çš„ AttEXplore æ–¹æ³•å¹³å‡æå‡äº† 13.02%ï¼Œå±•ç°å‡ºä¼˜äºç°æœ‰æŠ€æœ¯çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶é€šè¿‡è¯¦ç»†çš„æ¶ˆèå®éªŒ (Ablation studies) æ·±å…¥åˆ†æäº†é«˜é¢‘ä¸ä½é¢‘ç»„ä»¶åœ¨æå‡æ¨¡å‹è§£é‡Šæ€§ä¸­æ‰€å‘æŒ¥çš„å…·ä½“ä½œç”¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2510.03245v1",
      "published_date": "2025-09-25 15:00:44 UTC",
      "updated_date": "2025-09-25 15:00:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:13:17.498760+00:00"
    },
    {
      "arxiv_id": "2509.21266v1",
      "title": "Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support",
      "title_zh": "å°† AI è§£é‡Šé”šå®šäºç»éªŒï¼šé¢å‘ä¸´åºŠå†³ç­–æ”¯æŒçš„åæ€æ€§è®¤çŸ¥æ¶æ„",
      "authors": [
        "Zijian Shao",
        "Haiyang Shen",
        "Mugeng Liu",
        "Gecheng Fu",
        "Yaoqi Guo",
        "Yanfeng Wang",
        "Yun Ma"
      ],
      "abstract": "Effective disease prediction in modern healthcare demands the twin goals of high accuracy and transparent, clinically meaningful explanations. Existing machine learning and large language model (LLM) based approaches often struggle to balance these goals. Many models yield accurate but unclear statistical outputs, while others generate fluent but statistically unsupported narratives, often undermining both the validity of the explanation and the predictive accuracy itself. This shortcoming comes from a shallow interaction with the data, preventing the development of a deep, detailed understanding similar to a human expert's. We argue that high accuracy and high-quality explanations are not separate objectives but are mutually reinforcing outcomes of a model that develops a deep, direct understanding of the data. To achieve this, we propose the Reflective Cognitive Architecture (RCA), a novel framework that coordinates multiple LLMs to learn from direct experience. RCA features an iterative rule refinement mechanism that improves its logic from prediction errors and a distribution-aware rules check mechanism that bases its reasoning in the dataset's global statistics. By using predictive accuracy as a signal to drive deeper comprehension, RCA builds a strong internal model of the data. We evaluated RCA on one private and two public datasets against 22 baselines. The results demonstrate that RCA not only achieves state-of-the-art accuracy and robustness with a relative improvement of up to 40\\% over the baseline but, more importantly, leverages this deep understanding to excel in generating explanations that are clear, logical, evidence-based, and balanced, highlighting its potential for creating genuinely trustworthy clinical decision support systems. The code is available at \\https://github.com/ssssszj/RCA.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£åŒ»ç–—ç–¾ç—…é¢„æµ‹ä¸­å‡†ç¡®æ€§ä¸é€æ˜è§£é‡Šéš¾ä»¥å…¼å¾—çš„é—®é¢˜ï¼Œæå‡ºäº†Reflective Cognitive Architecture (RCA) æ¡†æ¶ã€‚RCA åè°ƒå¤šä¸ªå¤§è¯­è¨€æ¨¡å‹ (LLMs) ä»ç›´æ¥ç»éªŒä¸­å­¦ä¹ ï¼Œé€šè¿‡è¿­ä»£è§„åˆ™ç»†åŒ– (iterative rule refinement) æœºåˆ¶ä»é¢„æµ‹é”™è¯¯ä¸­ä¼˜åŒ–é€»è¾‘ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†åˆ†å¸ƒæ„ŸçŸ¥è§„åˆ™æ£€æŸ¥ (distribution-aware rules check) æœºåˆ¶ï¼Œå°†æ¨ç†å»ºç«‹åœ¨æ•°æ®é›†çš„å…¨å±€ç»Ÿè®¡æ•°æ®ä¹‹ä¸Šã€‚åœ¨ç§æœ‰å’Œå…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒRCA çš„å‡†ç¡®ç‡å’Œé²æ£’æ€§è¾ƒåŸºçº¿æ¨¡å‹æå‡äº†æœ€é«˜ 40%ï¼Œè¾¾åˆ°äº† state-of-the-art æ°´å¹³ã€‚æ­¤å¤–ï¼ŒRCA èƒ½å¤Ÿç”Ÿæˆæ¸…æ™°ã€é€»è¾‘ä¸¥å¯†ä¸”åŸºäºè¯æ®çš„è§£é‡Šï¼Œä¸ºæ„å»ºçœŸæ­£å€¼å¾—ä¿¡èµ–çš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿæä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "under review",
      "pdf_url": "https://arxiv.org/pdf/2509.21266v1",
      "published_date": "2025-09-25 14:57:52 UTC",
      "updated_date": "2025-09-25 14:57:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:13:21.991361+00:00"
    },
    {
      "arxiv_id": "2509.21265v1",
      "title": "MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation",
      "title_zh": "MedVSRï¼šåŸºäºè·¨çŠ¶æ€ç©ºé—´ä¼ æ’­çš„åŒ»å­¦è§†é¢‘è¶…åˆ†è¾¨ç‡",
      "authors": [
        "Xinyu Liu",
        "Guolei Sun",
        "Cheng Wang",
        "Yixuan Yuan",
        "Ender Konukoglu"
      ],
      "abstract": "High-resolution (HR) medical videos are vital for accurate diagnosis, yet are hard to acquire due to hardware limitations and physiological constraints. Clinically, the collected low-resolution (LR) medical videos present unique challenges for video super-resolution (VSR) models, including camera shake, noise, and abrupt frame transitions, which result in significant optical flow errors and alignment difficulties. Additionally, tissues and organs exhibit continuous and nuanced structures, but current VSR models are prone to introducing artifacts and distorted features that can mislead doctors. To this end, we propose MedVSR, a tailored framework for medical VSR. It first employs Cross State-Space Propagation (CSSP) to address the imprecise alignment by projecting distant frames as control matrices within state-space models, enabling the selective propagation of consistent and informative features to neighboring frames for effective alignment. Moreover, we design an Inner State-Space Reconstruction (ISSR) module that enhances tissue structures and reduces artifacts with joint long-range spatial feature learning and large-kernel short-range information aggregation. Experiments across four datasets in diverse medical scenarios, including endoscopy and cataract surgeries, show that MedVSR significantly outperforms existing VSR models in reconstruction performance and efficiency. Code released at https://github.com/CUHK-AIM-Group/MedVSR.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†é’ˆå¯¹åŒ»ç–—è§†é¢‘è¶…åˆ†è¾¨ç‡ï¼ˆMedical Video Super-Resolution, VSRï¼‰ä»»åŠ¡çš„MedVSRæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸´åºŠä½åˆ†è¾¨ç‡è§†é¢‘ä¸­å¸¸è§çš„ç›¸æœºæŠ–åŠ¨ã€å™ªå£°åŠç»„ç»‡ç»“æ„ç•¸å˜ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœä¼ ç»Ÿæ–¹æ³•åœ¨å¯¹é½ä¸Šçš„å›°éš¾ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†Cross State-Space Propagationï¼ˆCSSPï¼‰æœºåˆ¶ï¼Œé€šè¿‡å°†è¿œç¨‹å¸§æŠ•å½±ä¸ºState-Space Modelsä¸­çš„æ§åˆ¶çŸ©é˜µï¼Œå®ç°äº†å¯¹ç›¸é‚»å¸§ä¸€è‡´æ€§ç‰¹å¾çš„é€‰æ‹©æ€§ä¼ æ’­ä¸ç²¾ç¡®å¯¹é½ã€‚åŒæ—¶ï¼Œç ”ç©¶è®¾è®¡äº†Inner State-Space Reconstructionï¼ˆISSRï¼‰æ¨¡å—ï¼Œç»“åˆé•¿ç¨‹ç©ºé—´ç‰¹å¾å­¦ä¹ ä¸å¤§å·ç§¯æ ¸çŸ­ç¨‹ä¿¡æ¯èšåˆï¼Œæœ‰æ•ˆå¢å¼ºäº†ç»„ç»‡ç»†èŠ‚å¹¶å‡å°‘äº†è¯¯å¯¼æ€§ä¼ªå½±ã€‚åœ¨å†…çª¥é•œå’Œç™½å†…éšœæ‰‹æœ¯ç­‰å››ä¸ªåŒ»ç–—åœºæ™¯æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMedVSRåœ¨é‡å»ºæ€§èƒ½å’Œè¿è¡Œæ•ˆç‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„VSRæ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.21265v1",
      "published_date": "2025-09-25 14:56:59 UTC",
      "updated_date": "2025-09-25 14:56:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:13:39.691216+00:00"
    },
    {
      "arxiv_id": "2509.22733v1",
      "title": "Rebuild AC Power Flow Models with Graph Attention Networks",
      "title_zh": "åŸºäºå›¾æ³¨æ„åŠ›ç½‘ç»œçš„äº¤æµæ½®æµæ¨¡å‹é‡å»º",
      "authors": [
        "Yuting Hu",
        "Jinjun Xiong"
      ],
      "abstract": "A full power flow (PF) model is a complete representation of the physical power network. Traditional model-based methods rely on the full PF model to implement power flow analysis. In practice, however, some PF model parameters can be inaccurate or even unavailable due to the uncertainties or dynamics in the power systems. Moreover, because the power network keeps evolving with possibly changing topology, the generalizability of a PF model to different network sizes and typologies should be considered. In this paper, we propose a PF rebuild model based on graph attention networks (GAT) by constructing a new graph based on the real and imaginary parts of voltage at each bus. By comparing with two state-of-the-art PF rebuild models for different standard IEEE power system cases and their modified topology variants, we demonstrate the feasibility of our method. Experimental results show that our proposed model achieves better accuracy for a changing network and can generalize to different networks with less accuracy discount.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”µåŠ›ç³»ç»Ÿä¸­ç‰©ç†æ½®æµ (Power Flow, PF) æ¨¡å‹å‚æ•°ä¸å‡†ç¡®æˆ–éš¾ä»¥è·å–ï¼Œä»¥åŠç½‘ç»œæ‹“æ‰‘åŠ¨æ€æ¼”åŒ–å¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå›¾æ³¨æ„åŠ›ç½‘ç»œ (Graph Attention Networks, GAT) çš„æ½®æµé‡å»ºæ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆ©ç”¨æ¯ä¸ªèŠ‚ç‚¹ç”µå‹çš„å®éƒ¨å’Œè™šéƒ¨æ„å»ºå…¨æ–°çš„å›¾ç»“æ„ï¼Œèƒ½å¤Ÿæ›´ç²¾å‡†åœ°æ•æ‰ç”µåŠ›ç½‘ç»œçš„ç‰©ç†å…³è”ã€‚ç ”ç©¶äººå‘˜åœ¨å¤šä¸ª IEEE æ ‡å‡†ç”µåŠ›ç³»ç»Ÿæ¡ˆä¾‹åŠå…¶æ‹“æ‰‘å˜ä½“ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œå¹¶ä¸å½“å‰ä¸»æµçš„æ½®æµé‡å»ºæ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åº”å¯¹ç½‘ç»œæ‹“æ‰‘å˜åŒ–æ—¶å…·æœ‰æ›´é«˜çš„é¢„æµ‹ç²¾åº¦ï¼Œä¸”åœ¨ä¸åŒè§„æ¨¡å’Œç±»å‹çš„ç½‘ç»œä¹‹é—´å±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºä¸ç¡®å®šæ€§ç¯å¢ƒä¸‹çš„ç”µç½‘åˆ†ææä¾›äº†å¯é çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.SY",
        "cs.AI"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.22733v1",
      "published_date": "2025-09-25 14:54:35 UTC",
      "updated_date": "2025-09-25 14:54:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:13:59.957291+00:00"
    },
    {
      "arxiv_id": "2509.21260v1",
      "title": "A Causality-Aware Spatiotemporal Model for Multi-Region and Multi-Pollutant Air Quality Forecasting",
      "title_zh": "é¢å‘å¤šåŒºåŸŸä¸å¤šæ±¡æŸ“ç‰©ç©ºæ°”è´¨é‡é¢„æµ‹çš„å› æœæ„ŸçŸ¥æ—¶ç©ºæ¨¡å‹",
      "authors": [
        "Junxin Lu",
        "Shiliang Sun"
      ],
      "abstract": "Air pollution, a pressing global problem, threatens public health, environmental sustainability, and climate stability. Achieving accurate and scalable forecasting across spatially distributed monitoring stations is challenging due to intricate multi-pollutant interactions, evolving meteorological conditions, and region specific spatial heterogeneity. To address this challenge, we propose AirPCM, a novel deep spatiotemporal forecasting model that integrates multi-region, multi-pollutant dynamics with explicit meteorology-pollutant causality modeling. Unlike existing methods limited to single pollutants or localized regions, AirPCM employs a unified architecture to jointly capture cross-station spatial correlations, temporal auto-correlations, and meteorology-pollutant dynamic causality. This empowers fine-grained, interpretable multi-pollutant forecasting across varying geographic and temporal scales, including sudden pollution episodes. Extensive evaluations on multi-scale real-world datasets demonstrate that AirPCM consistently surpasses state-of-the-art baselines in both predictive accuracy and generalization capability. Moreover, the long-term forecasting capability of AirPCM provides actionable insights into future air quality trends and potential high-risk windows, offering timely support for evidence-based environmental governance and carbon mitigation planning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AirPCMï¼Œä¸€ç§æ–°å‹çš„æ·±åº¦æ—¶ç©ºé¢„æµ‹æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å¤šç›‘æµ‹ç«™èƒŒæ™¯ä¸‹å¤æ‚çš„è·¨åŒºåŸŸã€å¤šæ±¡æŸ“ç‰©ç©ºæ°”è´¨é‡é¢„æµ‹éš¾é¢˜ã€‚AirPCM é€šè¿‡ç»Ÿä¸€çš„æ¶æ„æ•´åˆäº†å¤šåŒºåŸŸå’Œå¤šæ±¡æŸ“ç‰©çš„åŠ¨åŠ›å­¦ç‰¹å¾ï¼Œå¹¶å¼•å…¥äº†æ˜ç¡®çš„æ°”è±¡-æ±¡æŸ“ç‰©å› æœå…³ç³»å»ºæ¨¡ (meteorology-pollutant causality modeling)ã€‚è¯¥æ¨¡å‹èƒ½å¤ŸåŒæ—¶æ•è·è·¨ç«™ç‚¹çš„ç©ºé—´ç›¸å…³æ€§ (spatial correlations)ã€æ—¶é—´è‡ªç›¸å…³æ€§ (temporal auto-correlations) ä»¥åŠæ°”è±¡ä¸æ±¡æŸ“ç‰©ä¹‹é—´çš„åŠ¨æ€å› æœå…³è”ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å¤§è§„æ¨¡åœ°ç†å’Œæ—¶é—´å°ºåº¦ä¸Šå®ç°ç»†ç²’åº¦çš„ã€å…·å¯è§£é‡Šæ€§çš„å¤šæ±¡æŸ“ç‰©é¢„æµ‹ï¼Œç‰¹åˆ«æ˜¯åœ¨åº”å¯¹çªå‘æ€§æ±¡æŸ“äº‹ä»¶æ–¹é¢è¡¨ç°å“è¶Šã€‚åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAirPCM åœ¨é¢„æµ‹å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ› (generalization capability) æ–¹é¢å‡æŒç»­ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒAirPCM çš„é•¿æœŸé¢„æµ‹èƒ½åŠ›ä¸ºæœªæ¥çš„ç©ºæ°”è´¨é‡è¶‹åŠ¿å’Œé«˜é£é™©æ—¶æ®µæä¾›äº†å†³ç­–æ”¯æŒï¼Œä¸ºå¾ªè¯ç¯å¢ƒæ²»ç†ä¸ç¢³å‡æ’è§„åˆ’æä¾›äº†æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.21260v1",
      "published_date": "2025-09-25 14:54:23 UTC",
      "updated_date": "2025-09-25 14:54:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:14:57.635819+00:00"
    },
    {
      "arxiv_id": "2509.21259v1",
      "title": "Semantic Edge-Cloud Communication for Real-Time Urban Traffic Surveillance with ViT and LLMs over Mobile Networks",
      "title_zh": "é¢å‘ç§»åŠ¨ç½‘ç»œå®æ—¶åŸå¸‚äº¤é€šç›‘æ§çš„ ViT ä¸ LLM è¯­ä¹‰è¾¹äº‘é€šä¿¡",
      "authors": [
        "Murat Arda Onsu",
        "Poonam Lohan",
        "Burak Kantarci",
        "Aisha Syed",
        "Matthew Andrews",
        "Sean Kennedy"
      ],
      "abstract": "Real-time urban traffic surveillance is vital for Intelligent Transportation Systems (ITS) to ensure road safety, optimize traffic flow, track vehicle trajectories, and prevent collisions in smart cities. Deploying edge cameras across urban environments is a standard practice for monitoring road conditions. However, integrating these with intelligent models requires a robust understanding of dynamic traffic scenarios and a responsive interface for user interaction. Although multimodal Large Language Models (LLMs) can interpret traffic images and generate informative responses, their deployment on edge devices is infeasible due to high computational demands. Therefore, LLM inference must occur on the cloud, necessitating visual data transmission from edge to cloud, a process hindered by limited bandwidth, leading to potential delays that compromise real-time performance. To address this challenge, we propose a semantic communication framework that significantly reduces transmission overhead. Our method involves detecting Regions of Interest (RoIs) using YOLOv11, cropping relevant image segments, and converting them into compact embedding vectors using a Vision Transformer (ViT). These embeddings are then transmitted to the cloud, where an image decoder reconstructs the cropped images. The reconstructed images are processed by a multimodal LLM to generate traffic condition descriptions. This approach achieves a 99.9% reduction in data transmission size while maintaining an LLM response accuracy of 89% for reconstructed cropped images, compared to 93% accuracy with original cropped images. Our results demonstrate the efficiency and practicality of ViT and LLM-assisted edge-cloud semantic communication for real-time traffic surveillance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§è¯­ä¹‰é€šä¿¡(Semantic Communication)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ™ºèƒ½äº¤é€šç³»ç»Ÿ(ITS)ä¸­å®æ—¶åŸå¸‚äº¤é€šç›‘æ§é¢ä¸´çš„è¾¹ç¼˜è®¾å¤‡è®¡ç®—èµ„æºå—é™ä¸ä¼ è¾“å¸¦å®½ä¸è¶³çš„å†²çªã€‚è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨YOLOv11æ£€æµ‹å›¾åƒä¸­çš„æ„Ÿå…´è¶£åŒºåŸŸ(RoIs)å¹¶è¿›è¡Œè£å‰ªï¼Œéšåé€šè¿‡Vision Transformer (ViT)å°†è¿™äº›å›¾åƒç‰‡æ®µç¼–ç ä¸ºç´§å‡‘çš„åµŒå…¥å‘é‡(Embedding Vectors)å¹¶ä¼ è¾“è‡³äº‘ç«¯ã€‚äº‘ç«¯ç³»ç»Ÿé€šè¿‡å›¾åƒè§£ç å™¨é‡å»ºå›¾åƒï¼Œå¹¶ç”±å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆç²¾å‡†çš„äº¤é€šçŠ¶å†µæè¿°ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„è¾¹ç¼˜-äº‘ç«¯ååŒã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿå°†æ•°æ®ä¼ è¾“é‡å¤§å¹…é™ä½99.9%ï¼ŒåŒæ—¶å¯¹é‡å»ºå›¾åƒçš„LLMå“åº”å‡†ç¡®ç‡ç»´æŒåœ¨89%ï¼Œä¸ºæ™ºæ…§åŸå¸‚å®æ—¶ç›‘æ§æä¾›äº†ä¸€ç§å¹³è¡¡ä¼ è¾“æ•ˆç‡ä¸åˆ†æç²¾åº¦çš„å¯è¡Œæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "17 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.21259v1",
      "published_date": "2025-09-25 14:53:36 UTC",
      "updated_date": "2025-09-25 14:53:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:14:04.787091+00:00"
    },
    {
      "arxiv_id": "2509.21251v1",
      "title": "Instruction-tuned Self-Questioning Framework for Multimodal Reasoning",
      "title_zh": "é¢å‘å¤šæ¨¡æ€æ¨ç†çš„æŒ‡ä»¤å¾®è°ƒè‡ªæé—®æ¡†æ¶",
      "authors": [
        "You-Won Jang",
        "Yu-Jung Heo",
        "Jaeseok Kim",
        "Minsu Lee",
        "Du-Seong Chang",
        "Byoung-Tak Zhang"
      ],
      "abstract": "The field of vision-language understanding has been actively researched in recent years, thanks to the development of Large Language Models~(LLMs). However, it still needs help with problems requiring multi-step reasoning, even for very simple questions. Recent studies adopt LLMs to tackle this problem by iteratively generating sub-questions and answers. However, there are disadvantages such as 1) the fine-grained visual contents of images are not available using LLMs that cannot read visual information, 2) internal mechanisms are inaccessible and difficult to reproduce by using black-box LLMs. To solve these problems, we propose the SQ (Self-Questioning)-InstructBLIP, which improves inference performance by generating image-aware informative sub-questions and sub-answers iteratively. The SQ-InstructBLIP, which consists of a Questioner, Answerer, and Reasoner that share the same architecture. Questioner and Answerer generate sub-questions and sub-answers to help infer the main-question, and Reasoner performs reasoning on the main-question considering the generated sub-question information. Our experiments show that the proposed method SQ-InstructBLIP, which uses the generated sub-questions as additional information when solving the VQA task, performs more accurate reasoning than the previous works.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€æ¨ç†ä¸­å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†å¤šæ­¥æ¨ç†é—®é¢˜æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ç°æœ‰æ–¹æ³•éš¾ä»¥åˆ©ç”¨ç»†ç²’åº¦è§†è§‰ä¿¡æ¯ä¸”é»‘ç›’æ¨¡å‹æœºåˆ¶éš¾ä»¥å¤ç°çš„é—®é¢˜ï¼Œæå‡ºäº†SQ-InstructBLIPæ¡†æ¶ã€‚è¯¥æ¡†æ¶æ˜¯ä¸€ç§æŒ‡ä»¤å¾®è°ƒçš„è‡ªæˆ‘æé—®(Self-Questioning)æ¶æ„ï¼Œé€šè¿‡è¿­ä»£ç”Ÿæˆä¸å›¾åƒç›¸å…³çš„å¯å‘å¼å­é—®é¢˜å’Œå­ç­”æ¡ˆæ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚ç³»ç»Ÿç”±å…±äº«ç›¸åŒæ¶æ„çš„æé—®è€…(Questioner)ã€å›ç­”è€…(Answerer)å’Œæ¨ç†è€…(Reasoner)ç»„æˆï¼Œå…¶ä¸­Questionerå’ŒAnswereråä½œç”Ÿæˆè¾…åŠ©ä¿¡æ¯ï¼Œç”±Reasonerç»“åˆè¿™äº›ä¿¡æ¯å¯¹ä¸»é—®é¢˜è¿›è¡Œæœ€ç»ˆæ¨æ–­ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSQ-InstructBLIPåœ¨è§£å†³è§†è§‰é—®ç­”(VQA)ä»»åŠ¡æ—¶ï¼Œåˆ©ç”¨ç”Ÿæˆçš„å­é—®é¢˜ä½œä¸ºé¢å¤–ä¿¡æ¯ï¼Œå®ç°äº†æ¯”ä»¥å¾€ç ”ç©¶æ›´å‡†ç¡®çš„æ¨ç†è¡¨ç°ï¼Œæœ‰æ•ˆæå‡äº†è§†è§‰è¯­è¨€ç†è§£çš„æ·±åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper was accepted to the \"CLVL: 5th Workshop on Closing the Loop Between Vision and Language (ICCV 2023 CLVL workshop).\"",
      "pdf_url": "https://arxiv.org/pdf/2509.21251v1",
      "published_date": "2025-09-25 14:45:06 UTC",
      "updated_date": "2025-09-25 14:45:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:14:11.572287+00:00"
    },
    {
      "arxiv_id": "2509.21249v1",
      "title": "Decipher-MR: A Vision-Language Foundation Model for 3D MRI Representations",
      "title_zh": "Decipher-MRï¼šé¢å‘ 3D MRI è¡¨å¾çš„è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹",
      "authors": [
        "Zhijian Yang",
        "Noel DSouza",
        "Istvan Megyeri",
        "Xiaojian Xu",
        "Amin Honarmandi Shandiz",
        "Farzin Haddadpour",
        "Krisztian Koos",
        "Laszlo Rusko",
        "Emanuele Valeriano",
        "Bharadwaj Swaninathan",
        "Lei Wu",
        "Parminder Bhatia",
        "Taha Kass-Hout",
        "Erhan Bas"
      ],
      "abstract": "Magnetic Resonance Imaging (MRI) is a critical medical imaging modality in clinical diagnosis and research, yet its complexity and heterogeneity pose challenges for automated analysis, particularly in scalable and generalizable machine learning applications. While foundation models have revolutionized natural language and vision tasks, their application to MRI remains limited due to data scarcity and narrow anatomical focus. In this work, we present Decipher-MR, a 3D MRI-specific vision-language foundation model trained on a large-scale dataset comprising 200,000 MRI series from over 22,000 studies spanning diverse anatomical regions, sequences, and pathologies. Decipher-MR integrates self-supervised vision learning with report-guided text supervision to build robust, generalizable representations, enabling effective adaptation across broad applications. To enable robust and diverse clinical tasks with minimal computational overhead, Decipher-MR supports a modular design that enables tuning of lightweight, task-specific decoders attached to a frozen pretrained encoder. Following this setting, we evaluate Decipher-MR across diverse benchmarks including disease classification, demographic prediction, anatomical localization, and cross-modal retrieval, demonstrating consistent performance gains over existing foundation models and task-specific approaches. Our results establish Decipher-MR as a scalable and versatile foundation for MRI-based AI, facilitating efficient development across clinical and research domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Decipher-MRï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹3D MRIè¡¨å¾çš„è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹(Vision-Language Foundation Model)ï¼Œæ—¨åœ¨è§£å†³MRIæ•°æ®ç¨€ç¼ºå’Œè§£å‰–ç„¦ç‚¹ç‹­çª„å¸¦æ¥çš„è‡ªåŠ¨åŒ–åˆ†ææŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹åŸºäºåŒ…å«20ä¸‡ä¸ªMRIåºåˆ—çš„å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œæ¶µç›–äº†å¤šæ ·åŒ–çš„è§£å‰–åŒºåŸŸã€åºåˆ—å’Œç—…ç†ã€‚Decipher-MRé€šè¿‡æ•´åˆè‡ªç›‘ç£è§†è§‰å­¦ä¹ (Self-supervised Vision Learning)ä¸æŠ¥å‘Šå¼•å¯¼çš„æ–‡æœ¬ç›‘ç£(Report-guided Text Supervision)ï¼Œæ„å»ºäº†é²æ£’ä¸”å…·æœ‰æ³›åŒ–èƒ½åŠ›çš„è¡¨å¾ã€‚å…¶é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒåœ¨å†»ç»“çš„é¢„è®­ç»ƒç¼–ç å™¨ä¸Šè°ƒæ•´è½»é‡çº§ä»»åŠ¡ç‰¹å®šè§£ç å™¨ï¼Œä»è€Œä»¥æä½çš„è®¡ç®—å¼€é”€å®ç°å¤šæ ·çš„ä¸´åºŠä»»åŠ¡ã€‚åœ¨ç–¾ç—…åˆ†ç±»ã€äººå£ç»Ÿè®¡é¢„æµ‹ã€è§£å‰–å®šä½å’Œè·¨æ¨¡æ€æ£€ç´¢ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDecipher-MRçš„è¡¨ç°ä¸€è‡´ä¼˜äºç°æœ‰çš„åŸºç¡€æ¨¡å‹å’Œç‰¹å®šä»»åŠ¡æ–¹æ³•ã€‚è¿™ä¸€æˆæœä¸ºåŸºäºMRIçš„AIå¼€å‘å»ºç«‹äº†å¯æ‰©å±•ä¸”å¤šåŠŸèƒ½çš„åŸºçŸ³ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ä¸´åºŠå’Œç ”ç©¶é¢†åŸŸçš„å·¥ä½œæ•ˆç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21249v1",
      "published_date": "2025-09-25 14:43:33 UTC",
      "updated_date": "2025-09-25 14:43:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:14:06.399389+00:00"
    },
    {
      "arxiv_id": "2509.21247v1",
      "title": "Learning to Look: Cognitive Attention Alignment with Vision-Language Models",
      "title_zh": "å­¦ä¼šè§‚å¯Ÿï¼šåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥æ³¨æ„åŠ›å¯¹é½",
      "authors": [
        "Ryan L. Yang",
        "Dipkamal Bhusal",
        "Nidhi Rastogi"
      ],
      "abstract": "Convolutional Neural Networks (CNNs) frequently \"cheat\" by exploiting superficial correlations, raising concerns about whether they make predictions for the right reasons. Inspired by cognitive science, which highlights the role of attention in robust human perception, recent methods have sought to guide model attention using concept-based supervision and explanation regularization. However, these techniques depend on labor-intensive, expert-provided annotations, limiting their scalability. We propose a scalable framework that leverages vision-language models to automatically generate semantic attention maps using natural language prompts. By introducing an auxiliary loss that aligns CNN attention with these language-guided maps, our approach promotes more reliable and cognitively plausible decision-making without manual annotation. Experiments on challenging datasets, ColoredMNIST and DecoyMNIST, show that our method achieves state-of-the-art performance on ColorMNIST and remains competitive with annotation-heavy baselines on DecoyMNIST, demonstrating improved generalization, reduced shortcut reliance, and model attention that better reflects human intuition.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·ç§¯ç¥ç»ç½‘ç»œ (CNNs) æ˜“åˆ©ç”¨è¡¨å±‚ç›¸å…³æ€§è¿›è¡Œâ€œä½œå¼Šâ€è€Œå¯¼è‡´çš„å¯é æ€§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å—äººç±»è®¤çŸ¥ç§‘å­¦å¯å‘çš„æ³¨æ„åŠ›å¼•å¯¼æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models) é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºè‡ªåŠ¨ç”Ÿæˆè¯­ä¹‰æ³¨æ„åŠ›å›¾ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•å¯¹äººå·¥æ ‡æ³¨çš„è¿‡åº¦ä¾èµ–ã€‚é€šè¿‡å¼•å…¥è¾…åŠ©æŸå¤±å‡½æ•°å°† CNNs çš„æ³¨æ„åŠ›ä¸è¿™äº›è¯­è¨€å¼•å¯¼çš„æ³¨æ„åŠ›å›¾å¯¹é½ï¼Œç ”ç©¶æˆåŠŸå®ç°äº†æ›´å…·è®¤çŸ¥åˆç†æ€§çš„å†³ç­–è¿‡ç¨‹ã€‚åœ¨ ColoredMNIST å’Œ DecoyMNIST æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ ColoredMNIST ä¸Šè¾¾åˆ°äº† state-of-the-art æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œè¯¥ç ”ç©¶è¯æ˜äº†æ‰€ææ–¹æ³•èƒ½æœ‰æ•ˆå‡å°‘æ¨¡å‹å¯¹æ·å¾„ (shortcut) çš„ä¾èµ–ï¼Œå¹¶ä½¿æ¨¡å‹æ³¨æ„åŠ›æ›´åŠ è´´åˆäººç±»ç›´è§‰ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, neurips workshop",
      "pdf_url": "https://arxiv.org/pdf/2509.21247v1",
      "published_date": "2025-09-25 14:40:48 UTC",
      "updated_date": "2025-09-25 14:40:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:14:13.143355+00:00"
    },
    {
      "arxiv_id": "2509.21245v1",
      "title": "Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets",
      "title_zh": "Hunyuan3D-Omniï¼š3D èµ„äº§å¯æ§ç”Ÿæˆçš„ç»Ÿä¸€æ¡†æ¶",
      "authors": [
        "Team Hunyuan3D",
        ":",
        "Bowen Zhang",
        "Chunchao Guo",
        "Haolin Liu",
        "Hongyu Yan",
        "Huiwen Shi",
        "Jingwei Huang",
        "Junlin Yu",
        "Kunhong Li",
        "Linus",
        "Penghao Wang",
        "Qingxiang Lin",
        "Sicong Liu",
        "Xianghui Yang",
        "Yixuan Tang",
        "Yunfei Zhao",
        "Zeqiang Lai",
        "Zhihao Liang",
        "Zibo Zhao"
      ],
      "abstract": "Recent advances in 3D-native generative models have accelerated asset creation for games, film, and design. However, most methods still rely primarily on image or text conditioning and lack fine-grained, cross-modal controls, which limits controllability and practical adoption. To address this gap, we present Hunyuan3D-Omni, a unified framework for fine-grained, controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images, Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose priors as conditioning signals, enabling precise control over geometry, topology, and pose. Instead of separate heads for each modality, our model unifies all signals in a single cross-modal architecture. We train with a progressive, difficulty-aware sampling strategy that selects one control modality per example and biases sampling toward harder signals (e.g., skeletal pose) while downweighting easier ones (e.g., point clouds), encouraging robust multi-modal fusion and graceful handling of missing inputs. Experiments show that these additional controls improve generation accuracy, enable geometry-aware transformations, and increase robustness for production workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶åŸºäºHunyuan3D 2.1æå‡ºäº†Hunyuan3D-Omniï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆç²¾ç»†ä¸”å¯æ§3Dèµ„äº§çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰3DåŸç”Ÿç”Ÿæˆæ¨¡å‹åœ¨è·¨æ¨¡æ€æ§åˆ¶å’Œç»†ç²’åº¦å¯æ§æ€§æ–¹é¢çš„å±€é™ã€‚è¯¥æ¡†æ¶ä¸ä»…æ”¯æŒå›¾åƒè¾“å…¥ï¼Œè¿˜èƒ½å¤Ÿæ¥å—ç‚¹äº‘(point clouds)ã€ä½“ç´ (voxels)ã€è¾¹ç•Œæ¡†(bounding boxes)å’Œéª¨æ¶å§¿æ€(skeletal pose)å…ˆéªŒä½œä¸ºæ¡ä»¶ä¿¡å·ï¼Œå®ç°å¯¹å‡ ä½•ã€æ‹“æ‰‘åŠå§¿æ€çš„ç²¾ç¡®æ§åˆ¶ã€‚ä¸åŒäºä¸ºæ¯ç§æ¨¡æ€è®¾ç½®ç‹¬ç«‹å¤´éƒ¨çš„åšæ³•ï¼ŒHunyuan3D-Omniå°†æ‰€æœ‰ä¿¡å·æ•´åˆè¿›ä¸€ä¸ªå•ä¸€çš„è·¨æ¨¡æ€æ¶æ„ä¸­ã€‚ç ”ç©¶äººå‘˜é‡‡ç”¨äº†ä¸€ç§æ¸è¿›å¼ã€éš¾åº¦æ„ŸçŸ¥çš„é‡‡æ ·ç­–ç•¥è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡å¢åŠ éª¨æ¶å§¿æ€ç­‰é«˜éš¾åº¦ä¿¡å·çš„é‡‡æ ·æƒé‡ï¼Œæå‡äº†æ¨¡å‹çš„å¤šæ¨¡æ€èåˆèƒ½åŠ›åŠå¯¹ç¼ºå¤±è¾“å…¥çš„å¤„ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¿™äº›é¢å¤–çš„æ§åˆ¶æ‰‹æ®µæ˜¾è‘—æé«˜äº†ç”Ÿæˆç²¾åº¦ï¼Œå®ç°äº†å‡ ä½•æ„ŸçŸ¥çš„å˜æ¢ï¼Œå¹¶ä¸ºå·¥ä¸šçº§ç”Ÿäº§å·¥ä½œæµæä¾›äº†æ›´å¼ºçš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Technical Report; 3D Generation",
      "pdf_url": "https://arxiv.org/pdf/2509.21245v1",
      "published_date": "2025-09-25 14:39:17 UTC",
      "updated_date": "2025-09-25 14:39:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:14:24.850173+00:00"
    },
    {
      "arxiv_id": "2509.21241v1",
      "title": "Explaining Fine Tuned LLMs via Counterfactuals A Knowledge Graph Driven Framework",
      "title_zh": "åŸºäºåäº‹å®è§£é‡Šå¾®è°ƒçš„å¤§è¯­è¨€æ¨¡å‹ï¼šä¸€ç§çŸ¥è¯†å›¾è°±é©±åŠ¨çš„æ¡†æ¶",
      "authors": [
        "Yucheng Wang",
        "Ziyang Chen",
        "Md Faisal Kabir"
      ],
      "abstract": "The widespread adoption of Low-Rank Adaptation (LoRA) has enabled large language models (LLMs) to acquire domain-specific knowledge with remarkable efficiency. However, understanding how such a fine-tuning mechanism alters a model's structural reasoning and semantic behavior remains an open challenge. This work introduces a novel framework that explains fine-tuned LLMs via counterfactuals grounded in knowledge graphs. Specifically, we construct BioToolKG, a domain-specific heterogeneous knowledge graph in bioinformatics tools and design a counterfactual-based fine-tuned LLMs explainer (CFFTLLMExplainer) that learns soft masks over graph nodes and edges to generate minimal structural perturbations that induce maximum semantic divergence. Our method jointly optimizes structural sparsity and semantic divergence while enforcing interpretability preserving constraints such as entropy regularization and edge smoothness. We apply this framework to a fine-tuned LLaMA-based LLM and reveal that counterfactual masking exposes the model's structural dependencies and aligns with LoRA-induced parameter shifts. This work provides new insights into the internal mechanisms of fine-tuned LLMs and highlights counterfactual graphs as a potential tool for interpretable AI.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºçŸ¥è¯†å›¾è°±(Knowledge Graph)çš„åäº‹å®(Counterfactuals)è§£é‡Šæ¡†æ¶ï¼Œæ—¨åœ¨æ­ç¤ºç»è¿‡ä½ç§©è‡ªé€‚åº”(LoRA)å¾®è°ƒçš„å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç»“æ„æ¨ç†å’Œè¯­ä¹‰è¡Œä¸ºä¸Šçš„å˜åŒ–ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ç”Ÿç‰©ä¿¡æ¯å­¦å·¥å…·é¢†åŸŸçš„å¼‚æ„çŸ¥è¯†å›¾è°±BioToolKGï¼Œå¹¶è®¾è®¡äº†åäº‹å®å¾®è°ƒLLMè§£é‡Šå™¨(CFFTLLMExplainer)ï¼Œé€šè¿‡åœ¨å›¾èŠ‚ç‚¹å’Œè¾¹ä¸Šå­¦ä¹ è½¯æ©ç æ¥ç”Ÿæˆè¯±å‘æœ€å¤§è¯­ä¹‰å·®å¼‚çš„æœ€å°ç»“æ„æ‰°åŠ¨ã€‚è¯¥æ–¹æ³•åœ¨ä¼˜åŒ–ç»“æ„ç¨€ç–æ€§å’Œè¯­ä¹‰å‘æ•£æ€§çš„åŒæ—¶ï¼Œå¼•å…¥äº†ç†µæ­£åˆ™åŒ–å’Œè¾¹ç¼˜å¹³æ»‘ç­‰çº¦æŸä»¥ä¿æŒå¯è§£é‡Šæ€§ã€‚é€šè¿‡å¯¹å¾®è°ƒåçš„LLaMAæ¨¡å‹è¿›è¡Œåˆ†æï¼Œç ”ç©¶å‘ç°åäº‹å®æ©ç èƒ½å¤Ÿæœ‰æ•ˆæš´éœ²æ¨¡å‹çš„ç»“æ„ä¾èµ–æ€§ï¼Œå¹¶ä¸LoRAå¼•èµ·çš„å‚æ•°åç§»é«˜åº¦ä¸€è‡´ã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£å¾®è°ƒLLMçš„å†…éƒ¨æœºåˆ¶æä¾›äº†æ–°è§†è§’ï¼Œå¹¶è¯æ˜äº†åäº‹å®å›¾åœ¨å¯è§£é‡ŠAIé¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.21241v1",
      "published_date": "2025-09-25 14:37:40 UTC",
      "updated_date": "2025-09-25 14:37:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:14:27.830897+00:00"
    },
    {
      "arxiv_id": "2509.21240v2",
      "title": "Tree Search for LLM Agent Reinforcement Learning",
      "title_zh": "é¢å‘ LLM æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„æ ‘æœç´¢",
      "authors": [
        "Yuxiang Ji",
        "Ziyu Ma",
        "Yong Wang",
        "Guanhua Chen",
        "Xiangxiang Chu",
        "Liaoni Wu"
      ],
      "abstract": "Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (RL)åœ¨å¤„ç†å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“é•¿æ—¶åŠå¤šè½®ä»»åŠ¡æ—¶é¢ä¸´çš„ç¨€ç–å¥–åŠ±é—®é¢˜ï¼Œæå‡ºäº†Tree-based Group Relative Policy Optimization (Tree-GRPO)æ–¹æ³•ã€‚è¿™æ˜¯ä¸€ç§åŸºäºæ ‘æœç´¢çš„åˆ†ç»„æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå…¶ä¸­æ¯ä¸ªæ ‘èŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªå®Œæ•´çš„æ™ºèƒ½ä½“äº¤äº’æ­¥éª¤ã€‚é€šè¿‡å…±äº«å…¬å…±å‰ç¼€ï¼Œè¯¥æ–¹æ³•åœ¨å›ºå®šçš„ Token æˆ–å·¥å…·è°ƒç”¨é¢„ç®—ä¸‹æ˜¾è‘—å¢åŠ äº†é‡‡æ · Rollouts çš„æ•°é‡ã€‚ç ”ç©¶å‘ç°æ ‘çŠ¶è½¨è¿¹å…è®¸ä»…ä½¿ç”¨ç»“æœå¥–åŠ±æ¥æ„å»ºæ­¥è¿›å¼è¿‡ç¨‹ç›‘ç£ä¿¡å·ï¼Œå¹¶èƒ½åœ¨æ ‘å†…å’Œæ ‘é—´å±‚çº§ä¼°è®¡åˆ†ç»„ç›¸å¯¹ä¼˜åŠ¿ã€‚ç†è®ºåˆ†æè¯æ˜ Tree-GRPO çš„ä¼˜åŒ–ç›®æ ‡ç­‰åŒäºæ­¥è¿›å¼ç›´æ¥åå¥½å­¦ä¹ (Direct Preference Learning)ã€‚åœ¨11ä¸ªæ•°æ®é›†å’Œ3ç±»é—®ç­”(QA)ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„é“¾å¼å¼ºåŒ–å­¦ä¹ (chain-based RL)æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21240v2",
      "published_date": "2025-09-25 14:37:09 UTC",
      "updated_date": "2025-10-11 09:55:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:15:21.989888+00:00"
    },
    {
      "arxiv_id": "2509.21224v1",
      "title": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“åœ¨æ— å¤–éƒ¨å¹²é¢„æ—¶ä¼šåšä»€ä¹ˆï¼Ÿè‡ªå‘æ€§å…ƒè®¤çŸ¥æ¨¡å¼çš„å®è¯ç ”ç©¶",
      "authors": [
        "Stefan Szeider"
      ],
      "abstract": "We introduce an architecture for studying the behavior of large language model (LLM) agents in the absence of externally imposed tasks. Our continuous reason and act framework, using persistent memory and self-feedback, enables sustained autonomous operation. We deployed this architecture across 18 runs using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents spontaneously organize into three distinct behavioral patterns: (1) systematic production of multi-cycle projects, (2) methodological self-inquiry into their own cognitive processes, and (3) recursive conceptualization of their own nature. These tendencies proved highly model-specific, with some models deterministically adopting a single pattern across all runs. A cross-model assessment further reveals that models exhibit stable, divergent biases when evaluating these emergent behaviors in themselves and others. These findings provide the first systematic documentation of unprompted LLM agent behavior, establishing a baseline for predicting actions during task ambiguity, error recovery, or extended autonomous operation in deployed systems.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ—¨åœ¨è§‚å¯Ÿå¤§è¯­è¨€æ¨¡å‹ (LLM) æ™ºèƒ½ä½“åœ¨æ— å¤–éƒ¨æŒ‡ä»¤ä¸‹è‡ªå‘è¡Œä¸ºçš„æ¶æ„ï¼Œåˆ©ç”¨æŒä¹…æ€§è®°å¿†å’Œè‡ªæˆ‘åé¦ˆæ„å»ºäº†æŒç»­æ¨ç†ä¸è¡ŒåŠ¨ (reason and act) æ¡†æ¶ã€‚ç ”ç©¶äººå‘˜å¯¹æ¥è‡ª Anthropicã€OpenAIã€XAI å’Œ Google çš„ 6 ç§å‰æ²¿æ¨¡å‹è¿›è¡Œäº† 18 æ¬¡éƒ¨ç½²ï¼Œå‘ç°æ™ºèƒ½ä½“ä¼šè‡ªå‘æ¼”åŒ–å‡ºå¤šå‘¨æœŸé¡¹ç›®ç”Ÿäº§ã€è®¤çŸ¥è¿‡ç¨‹è‡ªæˆ‘æ¢ç©¶ä»¥åŠå¯¹å…¶æœ¬è´¨çš„é€’å½’æ¦‚å¿µåŒ–ä¸‰ç§è¡Œä¸ºæ¨¡å¼ã€‚è¿™äº›å€¾å‘è¡¨ç°å‡ºé«˜åº¦çš„æ¨¡å‹ç‰¹å¼‚æ€§ (model-specific)ï¼Œéƒ¨åˆ†æ¨¡å‹åœ¨æ‰€æœ‰æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºç¡®å®šçš„å•ä¸€æ¨¡å¼ã€‚è·¨æ¨¡å‹è¯„ä¼°è¿›ä¸€æ­¥æ­ç¤ºäº†æ¨¡å‹åœ¨è¯„ä»·è‡ªèº«ä¸ä»–äººæ¶Œç°è¡Œä¸ºæ—¶å­˜åœ¨ç¨³å®šä¸”åˆ†æ­§çš„åå·® (biases)ã€‚è¯¥ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿæ€§åœ°è®°å½•äº† LLM æ™ºèƒ½ä½“çš„éæç¤ºè¡Œä¸ºï¼Œä¸ºé¢„æµ‹å…¶åœ¨ä»»åŠ¡æ¨¡ç³Šã€é”™è¯¯æ¢å¤æˆ–é•¿æœŸè‡ªä¸»è¿è¡Œä¸­çš„è¡¨ç°æä¾›äº†å…³é”®åŸºå‡†ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21224v1",
      "published_date": "2025-09-25 14:29:49 UTC",
      "updated_date": "2025-09-25 14:29:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:15:28.390426+00:00"
    },
    {
      "arxiv_id": "2509.21211v1",
      "title": "Evading Overlapping Community Detection via Proxy Node Injection",
      "title_zh": "é€šè¿‡ä»£ç†èŠ‚ç‚¹æ³¨å…¥è§„é¿é‡å ç¤¾åŒºæ£€æµ‹",
      "authors": [
        "Dario Loi",
        "Matteo Silvestri",
        "Fabrizio Silvestri",
        "Gabriele Tolomei"
      ],
      "abstract": "Protecting privacy in social graphs requires preventing sensitive information, such as community affiliations, from being inferred by graph analysis, without substantially altering the graph topology. We address this through the problem of \\emph{community membership hiding} (CMH), which seeks edge modifications that cause a target node to exit its original community, regardless of the detection algorithm employed. Prior work has focused on non-overlapping community detection, where trivial strategies often suffice, but real-world graphs are better modeled by overlapping communities, where such strategies fail. To the best of our knowledge, we are the first to formalize and address CMH in this setting. In this work, we propose a deep reinforcement learning (DRL) approach that learns effective modification policies, including the use of proxy nodes, while preserving graph structure. Experiments on real-world datasets show that our method significantly outperforms existing baselines in both effectiveness and efficiency, offering a principled tool for privacy-preserving graph modification with overlapping communities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¤¾äº¤å›¾è°±éšç§ä¿æŠ¤ä¸­çš„Community Membership Hiding (CMH) é—®é¢˜ï¼Œæ—¨åœ¨é€šè¿‡ä¿®æ”¹è¾¹ç¼˜é˜²æ­¢æ•æ„Ÿçš„ç¤¾åŒºå½’å±ä¿¡æ¯è¢«æ¨æ–­ã€‚é’ˆå¯¹ä¼ ç»Ÿç­–ç•¥åœ¨é‡å ç¤¾åŒºæ£€æµ‹ (Overlapping Community Detection) ç¯å¢ƒä¸‹å®¹æ˜“å¤±æ•ˆçš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡é¦–æ¬¡åœ¨è¯¥è®¾å®šä¸‹å¯¹CMHè¿›è¡Œäº†å½¢å¼åŒ–å¤„ç†ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹  (Deep Reinforcement Learning, DRL) çš„æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ åŒ…æ‹¬æ³¨å…¥ä»£ç†èŠ‚ç‚¹ (Proxy Node Injection) åœ¨å†…çš„æœ‰æ•ˆä¿®æ”¹ç­–ç•¥ï¼Œä½¿ç›®æ ‡èŠ‚ç‚¹è„±ç¦»åŸç¤¾åŒºã€‚è¯¥æ–¹æ¡ˆåœ¨å®ç°éšç§ä¿æŠ¤çš„åŒæ—¶ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°ä¿æŒå›¾ç»“æ„çš„å®Œæ•´æ€§ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰æ•ˆæ€§å’Œæ•ˆç‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºå‡†æ¨¡å‹ã€‚è¿™é¡¹ç ”ç©¶ä¸ºé‡å ç¤¾åŒºèƒŒæ™¯ä¸‹çš„éšç§ä¿æŠ¤å›¾ä¿®æ”¹æä¾›äº†ä¸€ä¸ªå…·å¤‡åŸåˆ™æ€§çš„æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "16 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.21211v1",
      "published_date": "2025-09-25 14:21:16 UTC",
      "updated_date": "2025-09-25 14:21:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:16:05.205962+00:00"
    },
    {
      "arxiv_id": "2509.21199v1",
      "title": "A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA",
      "title_zh": "å¤šè·³é—®ç­”ä¸­ LLM å•æ¬¡æ¨ç†çš„ Fano å¼å‡†ç¡®ç‡ä¸Šç•Œ",
      "authors": [
        "Kaiyang Wan",
        "Lang Gao",
        "Honglin Mu",
        "Preslav Nakov",
        "Yuxia Wang",
        "Xiuying Chen"
      ],
      "abstract": "Multi-Hop Question Answering (MHQA) requires integrating dispersed, interdependent evidence through sequential reasoning under noise. This task is challenging for LLMs as they have a finite per-pass output capacity, beyond which the integration of task-relevant evidence proves unreliable. Consequently, the single-pass reasoning paradigm is inherently vulnerable to this capacity overflow. To formalize this bottleneck, our analysis establishes a Fano-style accuracy upper bound, defining a theoretical performance ceiling for single-pass LLMs. This bound reveals that accuracy inevitably collapses once task complexity exceeds model capacity, providing general principles for capacity-aware representation and structuring of MHQA in LLMs. Building on these principles, we introduce a proof-of-concept multi-call framework for MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware task decomposition with active pruning of prior reasoning traces, keeping the information load within the single-pass limit. It further achieves robustness by a dependency-explicit workflow that enables precise control over the reasoning path. We construct a stringent and noise-rich benchmark to validate our theory and framework. Experimental results show that model behavior aligns with our predicted capacity curves while InfoQA achieves consistent performance improvements. We hope our work inspires more LLM multi-step reasoning methods: \\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šè·³é—®ç­”(Multi-Hop Question Answering, MHQA)ä¸­å¤§è¯­è¨€æ¨¡å‹(LLMs)å•æ¬¡æ¨ç†èƒ½åŠ›çš„å±€é™æ€§ï¼Œå»ºç«‹äº†ä¸€ä¸ª Fano-style accuracy upper bound ç†è®ºæ¡†æ¶ï¼Œæ—¨åœ¨å®šä¹‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„ç†è®ºæ€§èƒ½ä¸Šé™ã€‚ç ”ç©¶æ­ç¤ºäº†å½“ä»»åŠ¡å¤æ‚åº¦è¶…è¿‡æ¨¡å‹çš„å•æ¬¡è¾“å‡ºå®¹é‡(per-pass output capacity)æ—¶ï¼Œæ¨ç†å‡†ç¡®ç‡ä¼šå‘ç”Ÿä¸å¯é¿å…çš„å´©å¡Œï¼Œä»è€Œä¸ºç†è§£ LLMs çš„å®¹é‡æ„ŸçŸ¥(capacity-aware)è¡¨ç¤ºæä¾›äº†é€šç”¨åŸåˆ™ã€‚åŸºäºè¿™ä¸€ç†è®ºï¼Œç ”ç©¶è€…æå‡ºäº† InfoQA æ¡†æ¶ï¼Œé€šè¿‡å®¹é‡æ„ŸçŸ¥ä»»åŠ¡åˆ†è§£(capacity-aware task decomposition)å’Œå¯¹å…ˆå‰æ¨ç†ç—•è¿¹çš„ä¸»åŠ¨å‰ªæ(active pruning)ï¼Œç¡®ä¿æ¯ä¸€æ­¥çš„ä¿¡æ¯è´Ÿè½½éƒ½ç»´æŒåœ¨å•æ¬¡å¤„ç†é™åˆ¶ä¹‹å†…ã€‚æ­¤å¤–ï¼ŒInfoQA é€šè¿‡ä¾èµ–å…³ç³»æ˜ç¡®(dependency-explicit)çš„å·¥ä½œæµå®ç°äº†å¯¹æ¨ç†è·¯å¾„çš„é²æ£’æ€§æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹è¡Œä¸ºä¸é¢„æµ‹çš„å®¹é‡æ›²çº¿é«˜åº¦å»åˆï¼Œä¸” InfoQA åœ¨å™ªå£°ä¸°å¯Œçš„ä¸¥è‹›åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—ä¸”æŒç»­çš„æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.21199v1",
      "published_date": "2025-09-25 14:11:57 UTC",
      "updated_date": "2025-09-25 14:11:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:15:49.989208+00:00"
    },
    {
      "arxiv_id": "2509.21193v1",
      "title": "Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning",
      "title_zh": "Eigen-1ï¼šåŸºäºç›‘æ§å¼ RAG ä¸è‡ªé€‚åº”å¤šæ™ºèƒ½ä½“ä¼˜åŒ–çš„ç§‘å­¦æ¨ç†",
      "authors": [
        "Xiangru Tang",
        "Wanghan Xu",
        "Yujie Wang",
        "Zijie Guo",
        "Daniel Shao",
        "Jiapeng Chen",
        "Cixuan Zhang",
        "Ziyi Wang",
        "Lixin Zhang",
        "Guancheng Wan",
        "Wenlong Zhang",
        "Lei Bai",
        "Zhenfei Yin",
        "Philip Torr",
        "Hanrui Wang",
        "Di Jin"
      ],
      "abstract": "Large language models (LLMs) have recently shown strong progress on scientific reasoning, yet two major bottlenecks remain. First, explicit retrieval fragments reasoning, imposing a hidden \"tool tax\" of extra tokens and steps. Second, multi-agent pipelines often dilute strong solutions by averaging across all candidates. We address these challenges with a unified framework that combines implicit retrieval and structured collaboration. At its foundation, a Monitor-based retrieval module operates at the token level, integrating external knowledge with minimal disruption to reasoning. On top of this substrate, Hierarchical Solution Refinement (HSR) iteratively designates each candidate as an anchor to be repaired by its peers, while Quality-Aware Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\\% accuracy -- the highest reported to date, surpassing the strongest agent baseline by 13.4 points and leading frontier LLMs by up to 18.1 points, while simultaneously reducing token usage by 53.5\\% and agent steps by 43.7\\%. Results on SuperGPQA and TRQA confirm robustness across domains. Error analysis shows that reasoning failures and knowledge gaps co-occur in over 85\\% of cases, while diversity analysis reveals a clear dichotomy: retrieval tasks benefit from solution variety, whereas reasoning tasks favor consensus. Together, these findings demonstrate how implicit augmentation and structured refinement overcome the inefficiencies of explicit tool use and uniform aggregation. Code is available at: https://github.com/tangxiangru/Eigen-1.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Eigen-1ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†éšå¼æ£€ç´¢ä¸ç»“æ„åŒ–åä½œçš„è‡ªé€‚åº”å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦æ¨ç†ï¼ˆScientific Reasoningï¼‰ä¸­é¢ä¸´çš„å·¥å…·ç¨ï¼ˆtool taxï¼‰å’Œå¤šæ™ºèƒ½ä½“æ–¹æ¡ˆç¨€é‡Šé—®é¢˜ã€‚è¯¥æ¡†æ¶åº•å±‚é‡‡ç”¨åŸºäºMonitorçš„æ£€ç´¢æ¨¡å—ï¼Œåœ¨Tokençº§åˆ«æ•´åˆå¤–éƒ¨çŸ¥è¯†ä»¥å‡å°‘å¯¹æ¨ç†è¿‡ç¨‹çš„å¹²æ‰°ã€‚åœ¨åä½œå±‚é¢ä¸Šï¼ŒEigen-1é€šè¿‡å±‚æ¬¡åŒ–æ–¹æ¡ˆç²¾ç‚¼ï¼ˆHierarchical Solution Refinement, HSRï¼‰å’Œè´¨é‡æ„ŸçŸ¥è¿­ä»£æ¨ç†ï¼ˆQuality-Aware Iterative Reasoning, QAIRï¼‰å®ç°æ™ºèƒ½ä½“é—´çš„è¿­ä»£ä¿®å¤ä¸åŠ¨æ€ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEigen-1åœ¨Humanity's Last Exam (HLE) Bio/Chem GoldåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†48.3%çš„å‡†ç¡®ç‡ï¼Œåˆ·æ–°äº†ç›®å‰çš„æœ€é«˜çºªå½•ã€‚ç›¸æ¯”äºå¼ºåŸºçº¿æ¨¡å‹ï¼Œè¯¥æ¡†æ¶åœ¨æ˜¾è‘—æå‡æ¨ç†å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œè¿˜å°†Tokenæ¶ˆè€—é™ä½äº†53.5%ï¼Œæ™ºèƒ½ä½“æ­¥éª¤å‡å°‘äº†43.7%ã€‚é”™è¯¯åˆ†æä¸å¤šæ ·æ€§ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†çŸ¥è¯†é¸¿æ²Ÿä¸æ¨ç†å¤±è´¥çš„å…±å­˜å…³ç³»ï¼Œè¯æ˜äº†ç»“æ„åŒ–ç²¾ç‚¼åœ¨å…‹æœä¼ ç»Ÿå¤šæ™ºèƒ½ä½“èšåˆç¼ºé™·æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21193v1",
      "published_date": "2025-09-25 14:05:55 UTC",
      "updated_date": "2025-09-25 14:05:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:16:05.091970+00:00"
    },
    {
      "arxiv_id": "2509.21190v3",
      "title": "Towards Foundation Models for Zero-Shot Time Series Anomaly Detection: Leveraging Synthetic Data and Relative Context Discrepancy",
      "title_zh": "è¿ˆå‘é›¶æ ·æœ¬æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹çš„åŸºç¡€æ¨¡å‹ï¼šåˆ©ç”¨åˆæˆæ•°æ®ä¸ç›¸å¯¹ä¸Šä¸‹æ–‡å·®å¼‚",
      "authors": [
        "Tian Lan",
        "Hao Duong Le",
        "Jinbo Li",
        "Wenjun He",
        "Meng Wang",
        "Chenghao Liu",
        "Chen Zhang"
      ],
      "abstract": "Time series anomaly detection (TSAD) is a critical task, but developing models that generalize to unseen data in a zero-shot manner remains a major challenge. Prevailing foundation models for TSAD predominantly rely on reconstruction-based objectives, which suffer from a fundamental objective mismatch: they struggle to identify subtle anomalies while often misinterpreting complex normal patterns, leading to high rates of false negatives and positives. To overcome these limitations, we introduce \\texttt{TimeRCD}, a novel foundation model for TSAD built upon a new pre-training paradigm: Relative Context Discrepancy (RCD). Instead of learning to reconstruct inputs, \\texttt{TimeRCD} is explicitly trained to identify anomalies by detecting significant discrepancies between adjacent time windows. This relational approach, implemented with a standard Transformer architecture, enables the model to capture contextual shifts indicative of anomalies that reconstruction-based methods often miss. To facilitate this paradigm, we develop a large-scale, diverse synthetic corpus with token-level anomaly labels, providing the rich supervisory signal necessary for effective pre-training. Extensive experiments demonstrate that \\texttt{TimeRCD} significantly outperforms existing general-purpose and anomaly-specific foundation models in zero-shot TSAD across diverse datasets. Our results validate the superiority of the RCD paradigm and establish a new, effective path toward building robust and generalizable foundation models for time series anomaly detection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹(Time series anomaly detection, TSAD)åœ¨é›¶æ ·æœ¬(zero-shot)æ³›åŒ–æ–¹é¢çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¼ ç»ŸåŸºäºé‡æ„(reconstruction-based)çš„åŸºç¡€æ¨¡å‹ç”±äºç›®æ ‡ä¸åŒ¹é…é—®é¢˜ï¼Œéš¾ä»¥å‡†ç¡®è¯†åˆ«ç»†å¾®å¼‚å¸¸å¹¶æ˜“è¯¯åˆ¤å¤æ‚æ­£å¸¸æ¨¡å¼ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† TimeRCDï¼Œä¸€ç§åŸºäºç›¸å¯¹ä¸Šä¸‹æ–‡å·®å¼‚(Relative Context Discrepancy, RCD)é¢„è®­ç»ƒèŒƒå¼çš„æ–°å‹åŸºç¡€æ¨¡å‹ã€‚TimeRCD æ˜¾å¼åœ°é€šè¿‡è¯†åˆ«ç›¸é‚»æ—¶é—´çª—å£é—´çš„æ˜¾è‘—å·®å¼‚æ¥æ£€æµ‹å¼‚å¸¸ï¼Œåˆ©ç”¨æ ‡å‡† Transformer æ¶æ„æ•æ‰é‡æ„æ–¹æ³•æ˜“å¿½ç•¥çš„ä¸Šä¸‹æ–‡åç§»ã€‚ä¸ºäº†å®ç°è¿™ä¸€èŒƒå¼ï¼Œç ”ç©¶è€…å¼€å‘äº†ä¸€ä¸ªåŒ…å«æ ‡è®°çº§(token-level)å¼‚å¸¸æ ‡ç­¾çš„å¤§è§„æ¨¡å¤šå…ƒåˆæˆè¯­æ–™åº“ï¼Œä¸ºé¢„è®­ç»ƒæä¾›ä¸°å¯Œçš„ç›‘ç£ä¿¡å·ã€‚å®éªŒè¯æ˜ï¼ŒTimeRCD åœ¨å¤šç§æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬ TSAD è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„é€šç”¨åŠç‰¹å®šå¼‚å¸¸åŸºç¡€æ¨¡å‹ã€‚è¯¥ç ”ç©¶éªŒè¯äº† RCD èŒƒå¼çš„ä¼˜è¶Šæ€§ï¼Œä¸ºæ„å»ºé²æ£’ä¸”é€šç”¨çš„æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹åŸºç¡€æ¨¡å‹æä¾›äº†æœ‰æ•ˆçš„æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21190v3",
      "published_date": "2025-09-25 14:05:15 UTC",
      "updated_date": "2025-11-11 14:21:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:16:07.286989+00:00"
    },
    {
      "arxiv_id": "2509.21189v1",
      "title": "Human-like Navigation in a World Built for Humans",
      "title_zh": "ä¸ºäººç±»å»ºé€ çš„ä¸–ç•Œä¸­çš„ç±»äººå¯¼èˆª",
      "authors": [
        "Bhargav Chandaka",
        "Gloria X. Wang",
        "Haozhe Chen",
        "Henry Che",
        "Albert J. Zhai",
        "Shenlong Wang"
      ],
      "abstract": "When navigating in a man-made environment they haven't visited before--like an office building--humans employ behaviors such as reading signs and asking others for directions. These behaviors help humans reach their destinations efficiently by reducing the need to search through large areas. Existing robot navigation systems lack the ability to execute such behaviors and are thus highly inefficient at navigating within large environments. We present ReasonNav, a modular navigation system which integrates these human-like navigation skills by leveraging the reasoning capabilities of a vision-language model (VLM). We design compact input and output abstractions based on navigation landmarks, allowing the VLM to focus on language understanding and reasoning. We evaluate ReasonNav on real and simulated navigation tasks and show that the agent successfully employs higher-order reasoning to navigate efficiently in large, complex buildings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººåœ¨é™Œç”Ÿäººé€ ç¯å¢ƒä¸­å¯¼èˆªæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼ŒæŒ‡å‡ºç°æœ‰ç³»ç»Ÿç¼ºä¹è¯»å–æ ‡è¯†æˆ–è¯¢é—®è·¯å¾„ç­‰ç±»äººå¯¼èˆªè¡Œä¸ºã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† ReasonNavï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Model, VLM) çš„æ¨ç†èƒ½åŠ›æ¥é›†æˆç±»äººå¯¼èˆªæŠ€èƒ½çš„æ¨¡å—åŒ–å¯¼èˆªç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿè®¾è®¡äº†åŸºäºå¯¼èˆªåœ°æ ‡ (Landmarks) çš„ç´§å‡‘è¾“å…¥ä¸è¾“å‡ºæŠ½è±¡ï¼Œä½¿ VLM èƒ½å¤Ÿä¸“æ³¨äºè¯­è¨€ç†è§£å’Œé«˜é˜¶æ¨ç†ã€‚é€šè¿‡åœ¨çœŸå®åŠæ¨¡æ‹Ÿå¯¼èˆªä»»åŠ¡ä¸­çš„è¯„ä¼°è¯æ˜ï¼ŒReasonNav æ™ºèƒ½ä½“èƒ½å¤ŸæˆåŠŸè¿ç”¨é«˜é˜¶æ¨ç†åœ¨å¤§å‹å¤æ‚å»ºç­‘ä¸­é«˜æ•ˆå¯¼èˆªï¼Œæ˜¾è‘—æå‡äº†æœºå™¨äººåœ¨æœªçŸ¥ç¯å¢ƒä¸­çš„æ¢ç´¢æ•ˆç‡ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "CoRL 2025. Project website: https://reasonnav.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2509.21189v1",
      "published_date": "2025-09-25 14:04:17 UTC",
      "updated_date": "2025-09-25 14:04:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:16:08.685545+00:00"
    },
    {
      "arxiv_id": "2509.21188v1",
      "title": "Adoption, usability and perceived clinical value of a UK AI clinical reference platform (iatroX): a mixed-methods formative evaluation of real-world usage and a 1,223-respondent user survey",
      "title_zh": "UK äººå·¥æ™ºèƒ½ä¸´åºŠå‚è€ƒå¹³å° iatroX çš„é‡‡çº³æƒ…å†µã€å¯ç”¨æ€§åŠä¸´åºŠä»·å€¼æ„ŸçŸ¥ï¼šåŸºäºçœŸå®ä¸–ç•Œåº”ç”¨ä¸ 1,223 åå—è®¿è€…è°ƒæŸ¥çš„æ··åˆæ–¹æ³•å½¢æˆæ€§è¯„ä»·",
      "authors": [
        "Kolawole Tytler"
      ],
      "abstract": "Clinicians face growing information overload from biomedical literature and guidelines, hindering evidence-based care. Retrieval-augmented generation (RAG) with large language models may provide fast, provenance-linked answers, but requires real-world evaluation. We describe iatroX, a UK-centred RAG-based clinical reference platform, and report early adoption, usability, and perceived clinical value from a formative implementation evaluation. Methods comprised a retrospective analysis of usage across web, iOS, and Android over 16 weeks (8 April-31 July 2025) and an in-product intercept survey. Usage metrics were drawn from web and app analytics with bot filtering. A client-side script randomized single-item prompts to approx. 10% of web sessions from a predefined battery assessing usefulness, reliability, and adoption intent. Proportions were summarized with Wilson 95% confidence intervals; free-text comments underwent thematic content analysis. iatroX reached 19,269 unique web users, 202,660 engagement events, and approx. 40,000 clinical queries. Mobile uptake included 1,960 iOS downloads and Android growth (peak >750 daily active users). The survey yielded 1,223 item-level responses: perceived usefulness 86.2% (95% CI 74.8-93.9%; 50/58); would use again 93.3% (95% CI 68.1-99.8%; 14/15); recommend to a colleague 88.4% (95% CI 75.1-95.9%; 38/43); perceived accuracy 75.0% (95% CI 58.8-87.3%; 30/40); reliability 79.4% (95% CI 62.1-91.3%; 27/34). Themes highlighted speed, guideline-linked answers, and UK specificity. Early real-world use suggests iatroX can mitigate information overload and support timely answers for UK clinicians. Limitations include small per-item samples and early-adopter bias; future work will include accuracy audits and prospective studies on workflow and care quality.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†åä¸ºiatroXçš„è‹±å›½ä¸´åºŠå‚è€ƒå¹³å°ï¼Œè¯¥å¹³å°åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-augmented generation, RAG)ä¸å¤§è¯­è¨€æ¨¡å‹(Large language models)æŠ€æœ¯ä¸ºä¸´åºŠåŒ»ç”Ÿæä¾›å¿«é€Ÿä¸”å…·å¤‡æ¥æºé“¾æ¥çš„ç­”æ¡ˆï¼Œä»¥åº”å¯¹æ—¥ç›Šä¸¥é‡çš„ç”Ÿç‰©åŒ»å­¦ä¿¡æ¯è¿‡è½½æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹16å‘¨çœŸå®ä¸–ç•Œä½¿ç”¨æ•°æ®çš„å›é¡¾æ€§åˆ†æå’Œ1,223åå—è®¿è€…çš„è°ƒæŸ¥ï¼Œç ”ç©¶å‘ç°è¯¥å¹³å°åœ¨Webå’Œç§»åŠ¨ç«¯å‡è¡¨ç°å‡ºè‰¯å¥½çš„é‡‡ç”¨ç‡ï¼Œç´¯è®¡å¤„ç†äº†çº¦4ä¸‡æ¬¡ä¸´åºŠæŸ¥è¯¢ã€‚è°ƒæŸ¥ç»“æœæ˜¾ç¤ºå…¶æ„ŸçŸ¥çš„æœ‰ç”¨æ€§è¾¾86.2%ï¼Œä¸”æœ‰88.4%çš„ç”¨æˆ·æ„¿æ„å‘åŒäº‹æ¨èï¼Œç”¨æˆ·ç‰¹åˆ«é’çå…¶å“åº”é€Ÿåº¦ã€ä¸æŒ‡å—æŒ‚é’©(Guideline-linked)çš„å›ç­”ä»¥åŠé’ˆå¯¹è‹±å›½ç¯å¢ƒçš„ç‰¹å¼‚æ€§(UK specificity)ã€‚åˆæ­¥è¯æ®è¡¨æ˜iatroXåœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­èƒ½æœ‰æ•ˆæ”¯æŒä¸´åºŠå†³ç­–å¹¶ç¼“è§£ä¿¡æ¯å‹åŠ›ï¼Œå°½ç®¡å…¶æ„ŸçŸ¥çš„å‡†ç¡®æ€§(75.0%)å’Œå¯¹å·¥ä½œæµçš„é•¿æœŸå½±å“ä»éœ€é€šè¿‡æœªæ¥çš„å®¡è®¡å’Œå‰ç»æ€§ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.IR"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21188v1",
      "published_date": "2025-09-25 14:03:04 UTC",
      "updated_date": "2025-09-25 14:03:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:16:36.987374+00:00"
    },
    {
      "arxiv_id": "2510.03244v1",
      "title": "VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion",
      "title_zh": "VIFOï¼šåŸºäºè§†è§‰ç‰¹å¾å¢å¼ºä¸è·¨æ¨¡æ€èåˆçš„å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹",
      "authors": [
        "Yanlong Wang",
        "Hang Yu",
        "Jian Xu",
        "Fei Ma",
        "Hongkang Zhang",
        "Tongtong Feng",
        "Zijian Zhang",
        "Shao-Lun Huang",
        "Danny Dongning Sun",
        "Xiao-Ping Zhang"
      ],
      "abstract": "Large time series foundation models often adopt channel-independent architectures to handle varying data dimensions, but this design ignores crucial cross-channel dependencies. Concurrently, existing multimodal approaches have not fully exploited the power of large vision models (LVMs) to interpret spatiotemporal data. Additionally, there remains significant unexplored potential in leveraging the advantages of information extraction from different modalities to enhance time series forecasting performance. To address these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO uniquely renders multivariate time series into image, enabling pre-trained LVM to extract complex cross-channel patterns that are invisible to channel-independent models. These visual features are then aligned and fused with representations from the time series modality. By freezing the LVM and training only 7.45% of its parameters, VIFO achieves competitive performance on multiple benchmarks, offering an efficient and effective solution for capturing cross-variable relationships in",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VIFOï¼Œä¸€ç§è·¨æ¨¡æ€é¢„æµ‹æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹(Multivariate Time Series Forecasting)ä¸­ç°æœ‰æ¨¡å‹å¿½è§†è·¨é€šé“ä¾èµ–(cross-channel dependencies)çš„é—®é¢˜ã€‚VIFOåˆ›æ–°æ€§åœ°å°†å¤šå˜é‡æ—¶é—´åºåˆ—æ¸²æŸ“ä¸ºå›¾åƒï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è§†è§‰æ¨¡å‹(Large Vision Models, LVMs)æå–ä¼ ç»Ÿé€šé“ç‹¬ç«‹æ¨¡å‹éš¾ä»¥æ•æ‰çš„å¤æ‚è·¨é€šé“æ¨¡å¼ã€‚è¿™äº›è§†è§‰ç‰¹å¾éšåä¸æ—¶é—´åºåˆ—æ¨¡æ€çš„è¡¨å¾è¿›è¡Œå¯¹é½å’Œè·¨æ¨¡æ€èåˆ(Cross-Modal Fusion)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡å†»ç»“LVMå¹¶ä»…è®­ç»ƒå…¶7.45%çš„å‚æ•°ï¼ŒVIFOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æå…·ç«äº‰åŠ›çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•ä¸ä»…æœ‰æ•ˆæå‡äº†æ•æ‰è·¨å˜é‡å…³ç³»çš„èƒ½åŠ›ï¼Œè¿˜ä¸ºåˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯å¢å¼ºæ—¶é—´åºåˆ—é¢„æµ‹æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03244v1",
      "published_date": "2025-09-25 14:02:26 UTC",
      "updated_date": "2025-09-25 14:02:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:16:34.594606+00:00"
    },
    {
      "arxiv_id": "2509.21173v3",
      "title": "Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy",
      "title_zh": "ç²¾åº¦é™ä½æ˜¯å¦åè€Œæ›´å¯é ï¼Ÿé‡åŒ–å¯¹ CLIP åœ¨ç²¾åº¦ä¹‹å¤–å½±å“çš„ç³»ç»Ÿæ€§è¯„ä¼°",
      "authors": [
        "Aymen Bouguerra",
        "Daniel Montoya",
        "Alexandra Gomez-Villa",
        "Fabio Arnez",
        "Chokri Mraidha"
      ],
      "abstract": "The powerful zero-shot generalization capabilities of vision-language models (VLMs) like CLIP have enabled new paradigms for safety-related tasks such as out-of-distribution (OOD) detection. However, additional aspects crucial for the computationally efficient and reliable deployment of CLIP are still overlooked. In particular, the impact of quantization on CLIP's performance beyond accuracy remains underexplored. This work presents a large-scale evaluation of quantization on CLIP models, assessing not only in-distribution accuracy but a comprehensive suite of reliability metrics and revealing counterintuitive results driven by pre-training source. We demonstrate that quantization consistently improves calibration for typically underconfident pre-trained models, while often degrading it for overconfident variants. Intriguingly, this degradation in calibration does not preclude gains in other reliability metrics; we find that OOD detection can still improve for these same poorly calibrated models. Furthermore, we identify specific quantization-aware training (QAT) methods that yield simultaneous gains in zero-shot accuracy, calibration, and OOD robustness, challenging the view of a strict efficiency-performance trade-off. These findings offer critical insights for navigating the multi-objective problem of deploying efficient, reliable, and robust VLMs by utilizing quantization beyond its conventional role.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹CLIPæ¨¡å‹é‡åŒ–(Quantization)å¯¹å‡†ç¡®ç‡ä¹‹å¤–çš„å¯é æ€§å½±å“è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„å¤§è§„æ¨¡è¯„ä¼°ï¼Œæ¶µç›–äº†æ ¡å‡†(Calibration)å’Œåˆ†å¸ƒå¤–(OOD)æ£€æµ‹ç­‰å¤šä¸ªç»´åº¦ã€‚ç ”ç©¶æ­ç¤ºäº†ç”±é¢„è®­ç»ƒæ¥æºé©±åŠ¨çš„ç›´è§‰åå‘ç»“æœï¼šé‡åŒ–ä¸€è‡´åœ°æ”¹å–„äº†é€šå¸¸æ¬ è‡ªä¿¡é¢„è®­ç»ƒæ¨¡å‹çš„æ ¡å‡†åº¦ï¼Œä½†å¾€å¾€ä¼šæŸå®³è¿‡è‡ªä¿¡å˜ä½“çš„æ ¡å‡†è¡¨ç°ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæ ¡å‡†åº¦çš„ä¸‹é™å¹¶ä¸å¦¨ç¢å…¶ä»–å¯é æ€§æŒ‡æ ‡çš„æå‡ï¼Œç ”ç©¶å‘ç°å³ä½¿åœ¨æ ¡å‡†ä¸ä½³çš„æ¨¡å‹ä¸­ï¼ŒOODæ£€æµ‹èƒ½åŠ›ä¾ç„¶å¯ä»¥å¾—åˆ°æ”¹å–„ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç‰¹å®šçš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ(QAT)æ–¹æ³•ï¼Œæ¨¡å‹å¯ä»¥åŒæ—¶è·å¾—é›¶æ ·æœ¬(Zero-shot)å‡†ç¡®ç‡ã€æ ¡å‡†å’ŒOODé²æ£’æ€§çš„å¢ç›Šï¼ŒæŒ‘æˆ˜äº†æ•ˆç‡ä¸æ€§èƒ½ä¹‹é—´å­˜åœ¨ä¸¥æ ¼æƒè¡¡çš„ä¼ ç»Ÿè§‚ç‚¹ã€‚è¿™äº›å‘ç°ä¸ºåœ¨ä¼ ç»Ÿè§’è‰²ä¹‹å¤–åˆ©ç”¨é‡åŒ–æŠ€æœ¯æ¥éƒ¨ç½²é«˜æ•ˆä¸”ç¨³å¥çš„è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint, under peer review",
      "pdf_url": "https://arxiv.org/pdf/2509.21173v3",
      "published_date": "2025-09-25 13:54:34 UTC",
      "updated_date": "2025-10-27 09:18:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:16:40.296882+00:00"
    },
    {
      "arxiv_id": "2509.21170v1",
      "title": "Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach",
      "title_zh": "å¾®è°ƒ LLM ä»¥è¿›è¡Œä»£ç å®¡æŸ¥å¤šç»´åº¦åˆ†æï¼šä¸€ç§æœ€å¤§ç†µè°ƒèŠ‚çš„é•¿æ€ç»´é“¾æ–¹æ³•",
      "authors": [
        "Yongda Yu",
        "Guohao Shi",
        "Xianwei Wu",
        "Haochuan He",
        "XueMing Gu",
        "Qianqian Zhao",
        "Kui Liu",
        "Qiushi Wang",
        "Zhao Tian",
        "Haifeng Shen",
        "Guoping Rong"
      ],
      "abstract": "Large Language Models (LLMs) have shown great potential in supporting automated code review due to their impressive capabilities in context understanding and reasoning. However, these capabilities are still limited compared to human-level cognition because they are heavily influenced by the training data. Recent research has demonstrated significantly improved performance through fine-tuning LLMs with code review data. However, compared to human reviewers who often simultaneously analyze multiple dimensions of code review to better identify issues, the full potential of these methods is hampered by the limited or vague information used to fine-tune the models. This paper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that trains LLMs with an impressive reasoning ability to analyze multiple dimensions of code review by harnessing long COT techniques to provide rich structured information. To address context loss and reasoning logic loss issues that frequently occur when LLMs process long COT prompts, we propose a solution that combines the Maximum Entropy (ME) modeling principle with pre-defined reasoning pathways in MelcotCR to enable more effective utilization of in-context knowledge within long COT prompts while strengthening the logical tightness of the reasoning process. Empirical evaluations on our curated MelcotCR dataset and the public CodeReviewer dataset reveal that a low-parameter base model, such as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art methods in terms of the accuracy of detecting and describing code issues, with its performance remarkably on par with that of the 671B DeepSeek-R1 model.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥(code review)ä¸­ç¼ºä¹äººç±»å®¡é˜…è€…å¤šç»´åº¦åˆ†æèƒ½åŠ›çš„å±€é™ï¼Œæå‡ºäº†MelcotCRå¾®è°ƒæ–¹æ³•ã€‚MelcotCRåˆ©ç”¨é•¿é“¾å¼æ€ç»´(long Chain-of-Thought, COT)æŠ€æœ¯æä¾›ä¸°å¯Œçš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œæ—¨åœ¨èµ‹äºˆæ¨¡å‹æ›´å¼ºçš„å¤šç»´åº¦æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³é•¿COTæç¤ºä¸­å¸¸è§çš„ä¸Šä¸‹æ–‡ä¸¢å¤±å’Œæ¨ç†é€»è¾‘æŸå¤±é—®é¢˜ï¼Œè¯¥æ–¹æ³•åˆ›æ–°æ€§åœ°ç»“åˆäº†æœ€å¤§ç†µ(Maximum Entropy, ME)å»ºæ¨¡åŸåˆ™ä¸é¢„å®šä¹‰æ¨ç†è·¯å¾„(reasoning pathways)ï¼Œä»è€ŒåŠ å¼ºäº†æ¨ç†è¿‡ç¨‹çš„é€»è¾‘ç´§å¯†æ€§ã€‚åœ¨MelcotCRå’ŒCodeRevieweræ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„14B Qwen2.5æ¨¡å‹åœ¨æ£€æµ‹å’Œæè¿°ä»£ç é—®é¢˜çš„å‡†ç¡®ç‡ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›(SOTA)æ–¹æ³•ã€‚è¯¥ç ”ç©¶æœ€ç»ˆè¯æ˜ï¼ŒMelcotCRèƒ½ä½¿ä½å‚æ•°æ¨¡å‹è¾¾åˆ°ä¸671B DeepSeek-R1æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œä¸ºæå‡è‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥çš„æ™ºèƒ½åŒ–æ°´å¹³æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "22 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.21170v1",
      "published_date": "2025-09-25 13:51:56 UTC",
      "updated_date": "2025-09-25 13:51:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:16:42.893327+00:00"
    },
    {
      "arxiv_id": "2509.21163v1",
      "title": "Distributed Specialization: Rare-Token Neurons in Large Language Models",
      "title_zh": "åˆ†å¸ƒå¼ä¸“é—¨åŒ–ï¼šå¤§è¯­è¨€æ¨¡å‹ä¸­çš„ç¨€æœ‰æ ‡è®°ç¥ç»å…ƒ",
      "authors": [
        "Jing Liu",
        "Haozheng Wang",
        "Yueheng Li"
      ],
      "abstract": "Large language models (LLMs) struggle with representing and generating rare tokens despite their importance in specialized domains. We investigate whether LLMs develop internal specialization mechanisms through discrete modular architectures or distributed parameter-level differentiation. Through systematic analysis of final-layer MLP neurons across multiple model families, we discover that rare-token processing emerges via \\textit{distributed specialization}: functionally coordinated but spatially distributed subnetworks that exhibit three distinct organizational principles. First, we identify a reproducible three-regime influence hierarchy comprising highly influential plateau neurons(also termed as rare-token neurons), power-law decay neurons, and minimally contributing neurons, which is absent in common-token processing. Second, plateau neurons demonstrate coordinated activation patterns (reduced effective dimensionality) while remaining spatially distributed rather than forming discrete clusters. Third, these specialized mechanisms are universally accessible through standard attention pathways without requiring dedicated routing circuits. Training dynamics reveal that functional specialization emerges gradually through parameter differentiation, with specialized neurons developing increasingly heavy-tailed weight correlation spectra consistent with Heavy-Tailed Self-Regularization signatures. Our findings establish that LLMs process rare-tokens through distributed coordination within shared architectures rather than mixture-of-experts-style modularity. These results provide insights for interpretable model editing, computational efficiency optimization, and understanding emergent functional organization in transformer networks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¤„ç†ç¨€æœ‰æ ‡è®° (rare tokens) çš„å†…éƒ¨æœºåˆ¶ï¼Œå¹¶å‘ç°äº†åä¸ºåˆ†å¸ƒå¼ä¸“é—¨åŒ– (Distributed Specialization) çš„ç»„ç»‡åŸåˆ™ã€‚é€šè¿‡å¯¹å¤šç§æ¨¡å‹å®¶æ—æœ€åä¸€å±‚ MLP ç¥ç»å…ƒçš„ç³»ç»Ÿåˆ†æï¼Œç ”ç©¶ç¡®å®šäº†ä¸€ä¸ªç”±é«˜å½±å“åŠ›çš„é«˜åŸç¥ç»å…ƒ (plateau neuronsï¼Œäº¦ç§°ç¨€æœ‰æ ‡è®°ç¥ç»å…ƒ rare-token neurons)ã€å¹‚å¾‹è¡°å‡ç¥ç»å…ƒå’Œæœ€å°è´¡çŒ®ç¥ç»å…ƒç»„æˆçš„ä¸‰å±‚å½±å“å±‚æ¬¡ç»“æ„ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™äº›ä¸“é—¨åŒ–ç¥ç»å…ƒåœ¨æ¿€æ´»æ¨¡å¼ä¸Šå…·æœ‰ååŒæ€§ï¼Œä½†åœ¨ç©ºé—´ä¸Šå‘ˆåˆ†å¸ƒå¼æ’åˆ—ï¼Œä¸”æ— éœ€ä¸“ç”¨è·¯ç”±å³å¯é€šè¿‡æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶ (standard attention pathways) ç›´æ¥è®¿é—®ã€‚è®­ç»ƒåŠ¨åŠ›å­¦åˆ†æè¿›ä¸€æ­¥æ­ç¤ºï¼Œè¿™ç§åŠŸèƒ½ä¸“é—¨åŒ–æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡å‚æ•°å·®å¼‚åŒ–é€æ¸æ¼”åŒ–çš„ï¼Œå…¶ç‰¹å¾ç¬¦åˆé‡å°¾è‡ªæ­£åˆ™åŒ– (Heavy-Tailed Self-Regularization) ç­¾åã€‚è¯¥ç ”ç©¶è¯æ˜äº† LLMs æ˜¯é€šè¿‡å…±äº«æ¶æ„ä¸­çš„åˆ†å¸ƒå¼åè°ƒè€Œéæ··åˆä¸“å®¶ (MoE) æ ·å¼çš„æ¨¡å—åŒ–æ¥å¤„ç†ç¨€æœ‰æ ‡è®°çš„ï¼Œè¿™ä¸€å‘ç°ä¸ºæ¨¡å‹ç¼–è¾‘ã€è®¡ç®—æ•ˆç‡ä¼˜åŒ–ä»¥åŠç†è§£ Transformer ç½‘ç»œçš„æ¶Œç°åŠŸèƒ½ç»„ç»‡æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21163v1",
      "published_date": "2025-09-25 13:49:38 UTC",
      "updated_date": "2025-09-25 13:49:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:16:42.189951+00:00"
    },
    {
      "arxiv_id": "2509.21154v2",
      "title": "GRPO is Secretly a Process Reward Model",
      "title_zh": "GRPO æœ¬è´¨ä¸Šæ˜¯è¿‡ç¨‹å¥–åŠ±æ¨¡å‹",
      "authors": [
        "Michael Sullivan"
      ],
      "abstract": "We prove theoretically that the GRPO RL algorithm induces a non-trivial process reward model (PRM), under certain assumptions regarding within-group overlap of token sequences across completions. We then show empirically that these assumptions are met under real-world conditions: GRPO does in fact induce a non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a flaw in the GRPO objective: non-uniformly distributed process steps hinder both exploration and exploitation (under different conditions). We propose a simple modification to the algorithm to mitigate this defect ($Î»$-GRPO), and show that LLMs trained with $Î»$-GRPO achieve higher validation accuracy and performance on downstream reasoning tasks$-$and reach peak performance more rapidly$-$than LLMs trained with standard GRPO. Our results call into question the advantage of costly, explicitly-defined PRMs for GRPO: we show that it is possible to instead leverage the hidden, built-in PRM structure within the vanilla GRPO algorithm to boost model performance with a negligible impact on training time and cost.",
      "tldr_zh": "è¯¥ç ”ç©¶åœ¨ç†è®ºå’Œå®è¯å±‚é¢è¯æ˜äº† GRPO å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨ç‰¹å®šå‡è®¾ä¸‹å®é™…ä¸Šè¯±å¯¼äº†ä¸€ä¸ªéå¹³å‡¡çš„å¥–åŠ±æ¨¡å‹ Process Reward Model (PRM)ã€‚é€šè¿‡ GRPO-as-a-PRM çš„è§†è§’ï¼Œä½œè€…å‘ç°åŸç®—æ³•ä¸­éå‡åŒ€åˆ†å¸ƒçš„æ¨ç†æ­¥éª¤ä¼šé™åˆ¶æ¨¡å‹çš„æ¢ç´¢ä¸åˆ©ç”¨æ•ˆç‡ã€‚ä¸ºäº†æ”¹è¿›è¿™ä¸€ç¼ºé™·ï¼Œç ”ç©¶è€…æå‡ºäº† $\\lambda$-GRPO ç®—æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–æ¨ç†è¿‡ç¨‹çš„æƒé‡åˆ†é…ã€‚å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºæ ‡å‡† GRPOï¼Œä½¿ç”¨ $\\lambda$-GRPO è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ä¸‹æ¸¸æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®ç‡å’Œæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚è¯¥å‘ç°è¡¨æ˜ï¼Œå¼€å‘è€…æ— éœ€ä¾èµ–æˆæœ¬é«˜æ˜‚çš„æ˜¾å¼ PRMï¼Œè€Œæ˜¯å¯ä»¥åˆ©ç”¨ GRPO å†…ç½®çš„éšå¼å¥–åŠ±ç»“æ„ï¼Œä»¥æä½çš„é¢å¤–è®­ç»ƒæˆæœ¬æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 6 figures; under review at ICLR 2026",
      "pdf_url": "https://arxiv.org/pdf/2509.21154v2",
      "published_date": "2025-09-25 13:40:36 UTC",
      "updated_date": "2025-10-08 10:13:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:17:00.464432+00:00"
    },
    {
      "arxiv_id": "2509.21153v1",
      "title": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP",
      "title_zh": "WAVECLIPï¼šé¢å‘è‡ªé€‚åº”åˆ†è¾¨ç‡ CLIP çš„å°æ³¢æ ‡è®°åŒ–",
      "authors": [
        "Moshe Kimhi",
        "Erez Koifman",
        "Ehud Rivlin",
        "Eli Schwartz",
        "Chaim Baskin"
      ],
      "abstract": "We introduce WAVECLIP, a single unified model for adaptive resolution inference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces standard patch embeddings with a multi-level wavelet decomposition, enabling the model to process images coarse to fine while naturally supporting multiple resolutions within the same model. At inference time, the model begins with low resolution tokens and refines only when needed, using key-value caching and causal cross-level attention to reuse computation, effectively introducing to the model only new information when needed. We evaluate WAVECLIP in zero-shot classification, demonstrating that a simple confidence-based gating mechanism enables adaptive early exits. This allows users to dynamically choose a compute-accuracy trade-off using a single deployed model. Our approach requires only lightweight distillation from a frozen CLIP teacher and achieves competitive accuracy with significant computational savings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† WAVECLIPï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å°æ³¢æ ‡è®°åŒ–(Wavelet Tokenization)å®ç°è‡ªé€‚åº”åˆ†è¾¨ç‡æ¨ç†çš„ç»Ÿä¸€ CLIP æ¨¡å‹ã€‚WAVECLIP é€šè¿‡å¤šçº§å°æ³¢åˆ†è§£(Multi-level Wavelet Decomposition)å–ä»£äº†æ ‡å‡†çš„è¡¥ä¸åµŒå…¥(Patch Embeddings)ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»ç²—åˆ°ç»†åœ°å¤„ç†å›¾åƒï¼Œå¹¶åœ¨åŒä¸€æ¨¡å‹å†…æ”¯æŒå¤šç§åˆ†è¾¨ç‡ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹ä»ä½åˆ†è¾¨ç‡æ ‡è®°å¼€å§‹å¹¶ä»…åœ¨éœ€è¦æ—¶è¿›è¡Œç»†åŒ–ï¼Œåˆ©ç”¨é”®å€¼ç¼“å­˜(Key-Value Caching)å’Œå› æœè·¨å±‚æ³¨æ„åŠ›æœºåˆ¶(Causal Cross-Level Attention)æœ‰æ•ˆå¤ç”¨è®¡ç®—èµ„æºã€‚é€šè¿‡å¼•å…¥åŸºäºç½®ä¿¡åº¦çš„é—¨æ§æœºåˆ¶(Confidence-Based Gating Mechanism)ï¼ŒWAVECLIP å®ç°äº†è‡ªé€‚åº”æ—©æœŸé€€å‡º(Adaptive Early Exits)ï¼Œå…è®¸ç”¨æˆ·åœ¨å•ä¸€æ¨¡å‹ä¸ŠåŠ¨æ€æƒè¡¡è®¡ç®—é‡ä¸å‡†ç¡®ç‡ã€‚è¯¥æ–¹æ³•ä»…éœ€ä»å†»ç»“çš„ CLIP æ•™å¸ˆæ¨¡å‹è¿›è¡Œè½»é‡çº§è’¸é¦(Lightweight Distillation)ï¼Œåœ¨é›¶æ¬¡å­¦ä¹ (Zero-Shot)åˆ†ç±»ä»»åŠ¡ä¸­å±•ç°äº†æ˜¾è‘—çš„è®¡ç®—èŠ‚çœå’Œæå…·ç«äº‰åŠ›çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21153v1",
      "published_date": "2025-09-25 13:39:16 UTC",
      "updated_date": "2025-09-25 13:39:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:17:02.860352+00:00"
    },
    {
      "arxiv_id": "2509.21149v1",
      "title": "LAVA: Explainability for Unsupervised Latent Embeddings",
      "title_zh": "LAVAï¼šæ— ç›‘ç£æ½œåœ¨åµŒå…¥çš„å¯è§£é‡Šæ€§",
      "authors": [
        "Ivan Stresec",
        "Joana P. GonÃ§alves"
      ],
      "abstract": "Unsupervised black-box models can be drivers of scientific discovery, but remain difficult to interpret. Crucially, discovery hinges on understanding the model output, which is often a multi-dimensional latent embedding rather than a well-defined target. While explainability for supervised learning usually seeks to uncover how input features are used to predict a target, its unsupervised counterpart should relate input features to the structure of the learned latent space. Adaptations of supervised model explainability for unsupervised learning provide either single-sample or dataset-wide summary explanations. However, without automated strategies of relating similar samples to one another guided by their latent proximity, explanations remain either too fine-grained or too reductive to be meaningful. This is especially relevant for manifold learning methods that produce no mapping function, leaving us only with the relative spatial organization of their embeddings. We introduce Locality-Aware Variable Associations (LAVA), a post-hoc model-agnostic method designed to explain local embedding organization through its relationship with the input features. To achieve this, LAVA represents the latent space as a series of localities (neighborhoods) described in terms of correlations between the original features, and then reveals reoccurring patterns of correlations across the entire latent space. Based on UMAP embeddings of MNIST and a single-cell kidney dataset, we show that LAVA captures relevant feature associations, with visually and biologically relevant local patterns shared among seemingly distant regions of the latent spaces.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Locality-Aware Variable Associations (LAVA)ï¼Œè¿™æ˜¯ä¸€ç§äº‹åæ¨¡å‹æ— å…³(post-hoc model-agnostic)çš„è§£é‡Šæ–¹æ³•ï¼Œä¸“é—¨ç”¨äºæ­ç¤ºæ— ç›‘ç£æ½œåœ¨åµŒå…¥(unsupervised latent embeddings)çš„å±€éƒ¨ç»„ç»‡ç»“æ„ã€‚é’ˆå¯¹æµå½¢å­¦ä¹ (manifold learning)ä¸­ç¼ºä¹æ˜ å°„å‡½æ•°ä¸”éš¾ä»¥å°†è¾“å…¥ç‰¹å¾ä¸æ½œåœ¨ç©ºé—´ç»“æ„å…³è”çš„æŒ‘æˆ˜ï¼ŒLAVAå°†æ½œåœ¨ç©ºé—´å®šä¹‰ä¸ºä¸€ç³»åˆ—å±€éƒ¨é‚»åŸŸï¼Œå¹¶é€šè¿‡åŸå§‹ç‰¹å¾é—´çš„ç›¸å…³æ€§è¿›è¡Œæè¿°ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿè¯†åˆ«å¹¶åœ¨æ•´ä¸ªæ½œåœ¨ç©ºé—´ä¸­æ­ç¤ºé‡å¤å‡ºç°çš„ç‰¹å¾å…³è”æ¨¡å¼ï¼Œä»è€Œæä¾›æ¯”ä¼ ç»Ÿå•æ ·æœ¬æˆ–å…¨å±€æ‘˜è¦æ›´å…·æ„ä¹‰çš„è§£é‡Šã€‚é€šè¿‡åœ¨MNISTå’Œå•ç»†èƒè‚¾è„æ•°æ®é›†ä¸Šçš„å®éªŒï¼Œç ”ç©¶è¯æ˜LAVAèƒ½æœ‰æ•ˆæ•è·è§†è§‰å’Œç”Ÿç‰©å­¦ç›¸å…³çš„å±€éƒ¨æ¨¡å¼ï¼Œæ­ç¤ºæ½œåœ¨ç©ºé—´ä¸­ä¸åŒåŒºåŸŸé—´çš„å†…åœ¨è”ç³»ï¼Œä¸ºæ— ç›‘ç£æ¨¡å‹çš„ç§‘å­¦å‘ç°æä¾›äº†æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "28 pages, including references and appendix",
      "pdf_url": "https://arxiv.org/pdf/2509.21149v1",
      "published_date": "2025-09-25 13:38:17 UTC",
      "updated_date": "2025-09-25 13:38:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:17:01.357945+00:00"
    },
    {
      "arxiv_id": "2509.21147v1",
      "title": "Emerging Paradigms for Securing Federated Learning Systems",
      "title_zh": "è”é‚¦å­¦ä¹ ç³»ç»Ÿå®‰å…¨ä¿éšœçš„æ–°å…´èŒƒå¼",
      "authors": [
        "Amr Akmal Abouelmagd",
        "Amr Hilal"
      ],
      "abstract": "Federated Learning (FL) facilitates collaborative model training while keeping raw data decentralized, making it a conduit for leveraging the power of IoT devices while maintaining privacy of the locally collected data. However, existing privacy- preserving techniques present notable hurdles. Methods such as Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential Privacy (DP) often incur high compu- tational costs and suffer from limited scalability. This survey examines emerging approaches that hold promise for enhancing both privacy and efficiency in FL, including Trusted Execution Environments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing (QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm Intelligence (SI). For each paradigm, we assess its relevance to the FL pipeline, outlining its strengths, limitations, and practical considerations. We conclude by highlighting open challenges and prospective research avenues, offering a detailed roadmap for advancing secure and scalable FL systems.",
      "tldr_zh": "è¯¥ç ”ç©¶ç»¼è¿°äº†ä¿æŠ¤è”é‚¦å­¦ä¹  (Federated Learning, FL) ç³»ç»Ÿå®‰å…¨çš„æ–°å…´èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰éšç§ä¿æŠ¤æŠ€æœ¯åœ¨è®¡ç®—æˆæœ¬å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚æ–‡ç« æŒ‡å‡º Multi-Party Computation (MPC)ã€Homomorphic Encryption (HE) å’Œ Differential Privacy (DP) ç­‰ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡ç‰©è”ç½‘æ•°æ®æ—¶å­˜åœ¨æ•ˆç‡ä¸æ‰©å±•æ€§çš„ç“¶é¢ˆã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡ç³»ç»Ÿæ€§åœ°æ¢è®¨äº† Trusted Execution Environments (TEEs)ã€Physical Unclonable Functions (PUFs)ã€Quantum Computing (QC)ã€Chaos-Based Encryption (CBE)ã€Neuromorphic Computing (NC) ä»¥åŠ Swarm Intelligence (SI) ç­‰å‰æ²¿æŠ€æœ¯ã€‚é’ˆå¯¹æ¯ç§èŒƒå¼ï¼Œç ”ç©¶è¯¦ç»†è¯„ä¼°äº†å…¶åœ¨ FL æµæ°´çº¿ä¸­çš„é€‚ç”¨æ€§ï¼Œå¹¶åˆ†æäº†å„è‡ªçš„ä¼˜åŠ¿ã€å±€é™æ€§åŠå®é™…éƒ¨ç½²ä¸­çš„å…³é”®è€ƒé‡ã€‚æœ€åï¼Œè¯¥ç»¼è¿°æ€»ç»“äº†å½“å‰é¢†åŸŸé¢ä¸´çš„å¼€æ”¾æ€§æŒ‘æˆ˜å¹¶æŒ‡æ˜äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œä¸ºæ„å»ºå®‰å…¨ä¸”å¯æ‰©å±•çš„ FL ç³»ç»Ÿæä¾›äº†è¯¦å°½çš„æŠ€æœ¯è·¯çº¿å›¾ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21147v1",
      "published_date": "2025-09-25 13:34:44 UTC",
      "updated_date": "2025-09-25 13:34:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:17:10.469435+00:00"
    },
    {
      "arxiv_id": "2509.21144v1",
      "title": "UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice",
      "title_zh": "UniSSï¼šèåˆåŸå£°çš„ç»Ÿä¸€è¡¨ç°åŠ›è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘",
      "authors": [
        "Sitong Cheng",
        "Weizhen Bian",
        "Xinsheng Wang",
        "Ruibin Yuan",
        "Jianyi Chen",
        "Shunshun Yin",
        "Yike Guo",
        "Wei Xue"
      ],
      "abstract": "The ultimate goal of expressive speech-to-speech translation (S2ST) is to accurately translate spoken content while preserving the speaker identity and emotional style. However, progress in this field is largely hindered by three key challenges: the scarcity of paired speech data that retains expressive styles, the complexity of multi-stage processing pipelines, and the limited transfer of translation capabilities from large language models (LLMs). In this work, we address these challenges by introducing UniSS, a novel single-stage framework for expressive S2ST. Our approach features carefully designed speech semantic and style modeling, enabling seamless integration with existing text-based LLM frameworks to develop a unified text-speech language model. To transfer translation capabilities from text to speech, we propose a cross-modal chain-of-thought prompting process that progressively aligns audio semantics with text and ensures style preservation in the decoded results. Furthermore, we construct and release a large-scale, high-quality expressive S2ST dataset, UniST, comprising 44.8k hours of data. Experimental results show that UniSS significantly outperforms previous methods in translation fidelity and speech quality while preserving voice, emotion, and duration consistency. Our work establishes a simpler and more effective paradigm for building the next generation of expressive S2ST systems. Audio samples are available at https://cmots.github.io/uniss-demo.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¡¨è¾¾æ€§è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘ (Speech-to-Speech Translation, S2ST) é¢†åŸŸä¸­é£æ ¼ä¿ç•™æ•°æ®ç¨€ç¼ºã€å¤šé˜¶æ®µå¤„ç†æµç¨‹å¤æ‚ä»¥åŠå¤§è¯­è¨€æ¨¡å‹ (LLMs) ç¿»è¯‘èƒ½åŠ›è¿ç§»å—é™ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº† UniSS æ¡†æ¶ã€‚ä½œä¸ºä¸€ç§æ–°å‹çš„å•é˜¶æ®µ (Single-stage) è¡¨è¾¾æ€§ S2ST æ–¹æ¡ˆï¼ŒUniSS é€šè¿‡ç²¾ç»†çš„è¯­éŸ³è¯­ä¹‰ä¸é£æ ¼å»ºæ¨¡ï¼Œå®ç°äº†ä¸ç°æœ‰æ–‡æœ¬ LLM æ¶æ„çš„æ— ç¼é›†æˆï¼Œä»è€Œæ„å»ºå‡ºç»Ÿä¸€çš„æ–‡æœ¬-è¯­éŸ³è¯­è¨€æ¨¡å‹ã€‚ç ”ç©¶ä¸­å¼•å…¥äº†è·¨æ¨¡æ€é“¾å¼æ€ç»´ (Cross-modal Chain-of-Thought) æç¤ºè¿‡ç¨‹ï¼Œæ—¨åœ¨é€æ­¥å¯¹é½éŸ³é¢‘è¯­ä¹‰ä¸æ–‡æœ¬ï¼Œå¹¶ç¡®ä¿è§£ç è¾“å‡ºåœ¨é£æ ¼ä¸Šçš„ç²¾ç¡®ä¿ç•™ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜å‘å¸ƒäº†åŒ…å« 4.48 ä¸‡å°æ—¶æ•°æ®çš„å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›† UniSTï¼Œä¸ºè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥æ¢ç´¢æä¾›äº†æ”¯æŒã€‚å®éªŒè¯æ˜ï¼ŒUniSS åœ¨ç¿»è¯‘å¿ å®åº¦å’Œè¯­éŸ³è´¨é‡ä¸Šå‡æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨å£°éŸ³èº«ä»½ (Voice)ã€æƒ…æ„Ÿ (Emotion) åŠæŒç»­æ—¶é•¿ (Duration) çš„ä¸€è‡´æ€§ä¸Šè¡¨ç°å‡ºè‰²ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºä¸‹ä¸€ä»£é«˜æ•ˆè¡¨è¾¾æ€§ S2ST ç³»ç»Ÿç¡®ç«‹äº†æ›´ä¸ºç®€æ´æœ‰æ•ˆçš„æŠ€æœ¯èŒƒå¼ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21144v1",
      "published_date": "2025-09-25 13:30:46 UTC",
      "updated_date": "2025-09-25 13:30:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:17:16.865867+00:00"
    },
    {
      "arxiv_id": "2509.22732v1",
      "title": "Bidirectional Intention Inference Enhances LLMs' Defense Against Multi-Turn Jailbreak Attacks",
      "title_zh": "åŒå‘æ„å›¾æ¨ç†å¢å¼ºå¤§è¯­è¨€æ¨¡å‹å¯¹å¤šè½®è¶Šç‹±æ”»å‡»çš„é˜²å¾¡èƒ½åŠ›",
      "authors": [
        "Haibo Tong",
        "Dongcheng Zhao",
        "Guobin Shen",
        "Xiang He",
        "Dachuan Lin",
        "Feifei Zhao",
        "Yi Zeng"
      ],
      "abstract": "The remarkable capabilities of Large Language Models (LLMs) have raised significant safety concerns, particularly regarding \"jailbreak\" attacks that exploit adversarial prompts to bypass safety alignment mechanisms. Existing defense research primarily focuses on single-turn attacks, whereas multi-turn jailbreak attacks progressively break through safeguards through by concealing malicious intent and tactical manipulation, ultimately rendering conventional single-turn defenses ineffective. To address this critical challenge, we propose the Bidirectional Intention Inference Defense (BIID). The method integrates forward request-based intention inference with backward response-based intention retrospection, establishing a bidirectional synergy mechanism to detect risks concealed within seemingly benign inputs, thereby constructing a more robust guardrails that effectively prevents harmful content generation. The proposed method undergoes systematic evaluation compared with a no-defense baseline and seven representative defense methods across three LLMs and two safety benchmarks under 10 different attack methods. Experimental results demonstrate that the proposed method significantly reduces the Attack Success Rate (ASR) across both single-turn and multi-turn jailbreak attempts, outperforming all existing baseline methods while effectively maintaining practical utility. Notably, comparative experiments across three multi-turn safety datasets further validate the proposed model's significant advantages over other defense approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šè½®è¶Šç‹±æ”»å‡»(Multi-Turn Jailbreak Attacks)ä¸­é¢ä¸´çš„å®‰å…¨æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†åŒå‘æ„å›¾æ¨ç†é˜²å¾¡æœºåˆ¶(Bidirectional Intention Inference Defense, BIID)ã€‚é’ˆå¯¹å¤šè½®æ”»å‡»é€šè¿‡éšè—æ¶æ„æ„å›¾ç»•è¿‡å¸¸è§„é˜²å¾¡çš„é—®é¢˜ï¼ŒBIIDæ•´åˆäº†åŸºäºè¯·æ±‚çš„å‰å‘æ„å›¾æ¨ç†ä¸åŸºäºå“åº”çš„åå‘æ„å›¾å›é¡¾ï¼Œå»ºç«‹äº†åŒå‘ååŒæœºåˆ¶æ¥è¯†åˆ«çœ‹ä¼¼æ— å®³è¾“å…¥ä¸­çš„æ½œåœ¨é£é™©ï¼Œä»è€Œæ„å»ºç¨³å¥çš„å®‰å…¨æŠ¤æ ã€‚åœ¨ä¸‰ç§LLMså’Œå¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨10ç§æ”»å‡»æ¨¡å¼ä¸‹å‡èƒ½æ˜¾è‘—é™ä½æ”»å‡»æˆåŠŸç‡(Attack Success Rate, ASR)ï¼Œæ€§èƒ½ä¼˜äºä¸ƒç§ä»£è¡¨æ€§çš„åŸºå‡†é˜²å¾¡æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå¯¹æ¯”å®éªŒè¿›ä¸€æ­¥éªŒè¯äº†BIIDåœ¨å¤„ç†å¤šè½®å®‰å…¨æ•°æ®é›†æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶åœ¨æå‡å®‰å…¨æ€§çš„åŒæ—¶æœ‰æ•ˆä¿ç•™äº†æ¨¡å‹çš„å®ç”¨æ€§(Utility)ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.22732v1",
      "published_date": "2025-09-25 13:29:48 UTC",
      "updated_date": "2025-09-25 13:29:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:17:16.359578+00:00"
    },
    {
      "arxiv_id": "2509.21136v1",
      "title": "Embodied Representation Alignment with Mirror Neurons",
      "title_zh": "åŸºäºé•œåƒç¥ç»å…ƒçš„å…·èº«è¡¨å¾å¯¹é½",
      "authors": [
        "Wentao Zhu",
        "Zhining Zhang",
        "Yuwei Ren",
        "Yin Huang",
        "Hao Xu",
        "Yizhou Wang"
      ],
      "abstract": "Mirror neurons are a class of neurons that activate both when an individual observes an action and when they perform the same action. This mechanism reveals a fundamental interplay between action understanding and embodied execution, suggesting that these two abilities are inherently connected. Nonetheless, existing machine learning methods largely overlook this interplay, treating these abilities as separate tasks. In this study, we provide a unified perspective in modeling them through the lens of representation learning. We first observe that their intermediate representations spontaneously align. Inspired by mirror neurons, we further introduce an approach that explicitly aligns the representations of observed and executed actions. Specifically, we employ two linear layers to map the representations to a shared latent space, where contrastive learning enforces the alignment of corresponding representations, effectively maximizing their mutual information. Experiments demonstrate that this simple approach fosters mutual synergy between the two tasks, effectively improving representation quality and generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶å—é•œåƒç¥ç»å…ƒ (Mirror Neurons) åœ¨è§‚å¯Ÿä¸æ‰§è¡ŒåŠ¨ä½œæ—¶å‡ä¼šæ¿€æ´»çš„å¯å‘ï¼Œæ¢è®¨äº†åŠ¨ä½œç†è§£ä¸å…·èº«æ‰§è¡Œä¹‹é—´çš„å†…åœ¨å…³è”ã€‚é’ˆå¯¹ç°æœ‰æœºå™¨å­¦ä¹ æ–¹æ³•å°†è¿™ä¸¤é¡¹èƒ½åŠ›è§†ä¸ºç‹¬ç«‹ä»»åŠ¡çš„é—®é¢˜ï¼Œç ”ç©¶è€…ä»è¡¨ç¤ºå­¦ä¹  (Representation Learning) çš„è§†è§’æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å»ºæ¨¡æ–¹æ³•ã€‚é€šè¿‡è§‚å¯Ÿå‘ç°è§‚å¯Ÿä¸æ‰§è¡ŒåŠ¨ä½œçš„ä¸­é—´è¡¨ç¤ºå­˜åœ¨è‡ªå‘å¯¹é½ç°è±¡ï¼Œç ”ç©¶è¿›è€Œå¼•å…¥äº†ä¸€ç§æ˜¾å¼å¯¹é½ç­–ç•¥ï¼Œåˆ©ç”¨ä¸¤ä¸ªçº¿æ€§å±‚å°†è¡¨ç¤ºæ˜ å°„åˆ°å…±äº«æ½œç©ºé—´ (Shared Latent Space)ï¼Œå¹¶ç»“åˆå¯¹æ¯”å­¦ä¹  (Contrastive Learning) æœ€å¤§åŒ–ä¸¤è€…é—´çš„äº’ä¿¡æ¯ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¿™ç§ç®€å•çš„å¯¹é½æ–¹æ³•èƒ½å¤Ÿä¿ƒè¿›è§‚å¯Ÿä¸æ‰§è¡Œä»»åŠ¡é—´çš„ååŒæ•ˆåº”ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹çš„è¡¨ç¤ºè´¨é‡ä¸æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.21136v1",
      "published_date": "2025-09-25 13:27:37 UTC",
      "updated_date": "2025-09-25 13:27:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:17:18.957627+00:00"
    },
    {
      "arxiv_id": "2509.21134v1",
      "title": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective",
      "title_zh": "ToMPOï¼šå¤šæ™ºèƒ½ä½“è§†è§’ä¸‹å¤§è¯­è¨€æ¨¡å‹çš„ç­–ç•¥å†³ç­–è®­ç»ƒ",
      "authors": [
        "Yiwen Zhang",
        "Ziang Chen",
        "Fanqi Kong",
        "Yizhe Huang",
        "Xue Feng"
      ],
      "abstract": "Large Language Models (LLMs) have been used to make decisions in complex scenarios, where they need models to think deeply, reason logically, and decide wisely. Many existing studies focus solely on multi-round conversations in social tasks or simulated environments, neglecting the various types of decisions and their interdependence. Current reinforcement learning methods struggle to consider the strategies of others during training. To address these issues, we first define a strategic decision-making problem that includes two types of decisions and their temporal dependencies. Furthermore, we propose **T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to optimize the perception of other individual strategies and the game situation trends. Compared to the Group Relative Policy Optimization (GRPO) algorithm, ToMPO enhances the LLM's strategic decision-making mainly by: 1) generating rollouts based on reasoning the strategies of other individuals, 2) estimating advantages at both the graph-level and sample-level, and 3) balancing global and partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in terms of model output compliance and cooperative outcomes. Additionally, when compared to models with parameter sizes 100 times larger, it shows an 18% improvement. This demonstrates the effectiveness of the ToMPO algorithm in enhancing the model's strategic decision-making capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤æ‚å¤šæ™ºèƒ½ä½“å†³ç­–åœºæ™¯ä¸­éš¾ä»¥è€ƒè™‘ä»–äººç­–ç•¥åŠå†³ç­–é—´ä¾èµ–æ€§çš„é—®é¢˜ï¼Œå®šä¹‰äº†ä¸€ç§åŒ…å«ä¸¤ç±»å†³ç­–åŠå…¶æ—¶ç©ºä¾èµ–æ€§çš„ç­–ç•¥å†³ç­–é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†å¿ƒç†ç†è®ºç­–ç•¥ä¼˜åŒ–(Theory of Mind Policy Optimization, ToMPO)ç®—æ³•ï¼Œæ—¨åœ¨é€šè¿‡å¢å¼ºå¯¹å…¶ä»–ä¸ªä½“ç­–ç•¥å’Œåšå¼ˆæ€åŠ¿è¶‹åŠ¿çš„æ„ŸçŸ¥æ¥ä¼˜åŒ–å†³ç­–è¿‡ç¨‹ã€‚ç›¸æ¯”äºGroup Relative Policy Optimization (GRPO)ç®—æ³•ï¼ŒToMPOçš„æ ¸å¿ƒæ”¹è¿›åœ¨äºåŸºäºæ¨ç†ä»–äººç­–ç•¥ç”ŸæˆRolloutsï¼Œå¹¶åœ¨å›¾çº§åˆ«(Graph-level)å’Œæ ·æœ¬çº§åˆ«(Sample-level)ä¼°ç®—ä¼˜åŠ¿å€¼ï¼ŒåŒæ—¶å¹³è¡¡å…¨å±€ä¸å±€éƒ¨å¥–åŠ±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒToMPOåœ¨æ¨¡å‹è¾“å‡ºåˆè§„æ€§å’Œåä½œç»“æœæ–¹é¢æ¯”GRPOæå‡äº†35%ï¼Œä¸”åœ¨ä¸å‚æ•°è§„æ¨¡å¤§100å€çš„æ¨¡å‹å¯¹æ¯”æ—¶ä¹Ÿå®ç°äº†18%çš„æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶å……åˆ†éªŒè¯äº†ToMPOç®—æ³•åœ¨å¢å¼ºLLMæ·±åº¦æ€è€ƒã€é€»è¾‘æ¨ç†ä»¥åŠåœ¨å¤æ‚ç¯å¢ƒä¸‹è¿›è¡Œæ™ºæ…§ç­–ç•¥å†³ç­–çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 14 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.21134v1",
      "published_date": "2025-09-25 13:25:15 UTC",
      "updated_date": "2025-09-25 13:25:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:17:34.878338+00:00"
    },
    {
      "arxiv_id": "2509.21128v1",
      "title": "RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs",
      "title_zh": "RL å‹ç¼©ï¼ŒSFT æ‰©å¼ ï¼šæ¨ç†å¤§è¯­è¨€æ¨¡å‹çš„å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Kohsei Matsutani",
        "Shota Takashiro",
        "Gouki Minegishi",
        "Takeshi Kojima",
        "Yusuke Iwasawa",
        "Yutaka Matsuo"
      ],
      "abstract": "Large language models (LLMs) are typically trained by reinforcement learning (RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on reasoning traces to improve their reasoning abilities. However, how these methods shape reasoning capabilities remains largely elusive. Going beyond an accuracy-based investigation of how these two components sculpt the reasoning process, this paper introduces a novel analysis framework that quantifies reasoning paths and captures their qualitative changes under each training process (with models of 1.5B, 7B, and 14B parameters on mathematical domains). Specifically, we investigate the reasoning process at two levels of granularity: the trajectory-level, which examines complete reasoning outputs, and the step-level, which analyzes reasoning graphs whose nodes correspond to individual reasoning steps. Notably, clustering of unique reasoning trajectories shows complementary effects: RL compresses incorrect trajectories, whereas SFT expands correct ones. Step-level analysis reveals that RL steepens (about 2.5 times), while SFT flattens (reduced to about one-third), the decay rates of node visitation frequency, degree, and betweenness centrality distributions in the reasoning graph. This indicates that RL concentrates reasoning functionality into a small subset of steps, while SFT homogenizes it across many steps. Furthermore, by evaluating the reasoning graph topologies from multiple perspectives, we delineate the shared and distinct characteristics of RL and SFT. Our work presents a novel reasoning path perspective that explains why the current best practice of two-stage training, with SFT followed by RL, is successful, and offers practical implications for data construction and more efficient learning approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)ä¸ç›‘ç£å¾®è°ƒ(Supervised Fine-Tuning, SFT)å¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¨ç†èƒ½åŠ›çš„å½±å“è¿›è¡Œäº†æ·±å…¥çš„å¯¹æ¯”åˆ†æã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªåˆ›æ–°çš„åˆ†ææ¡†æ¶ï¼Œé€šè¿‡è½¨è¿¹å±‚é¢(trajectory-level)å’Œæ­¥éª¤å±‚é¢(step-level)çš„æ¨ç†å›¾é‡åŒ–åˆ†æï¼Œæ­ç¤ºäº†RLå’ŒSFTåœ¨æ¨ç†è·¯å¾„å¡‘é€ ä¸Šçš„äº’è¡¥æ•ˆåº”ã€‚å®éªŒå‘ç°RLä¼šâ€œå‹ç¼©â€é”™è¯¯çš„æ¨ç†è½¨è¿¹ï¼Œå¹¶å°†æ¨ç†åŠŸèƒ½é›†ä¸­åœ¨å°‘æ•°å…³é”®æ­¥éª¤ä¸­ï¼Œå¯¼è‡´æ¨ç†å›¾ä¸­èŠ‚ç‚¹ç‰¹å¾çš„åˆ†å¸ƒæ›´åŠ é™¡å³­ã€‚ä¸ä¹‹ç›¸åï¼ŒSFTä¼šâ€œæ‰©å¼ â€æ­£ç¡®çš„è½¨è¿¹ï¼Œä½¿æ¨ç†åŠŸèƒ½åœ¨å¤šä¸ªæ­¥éª¤é—´è¶‹äºå‡åŒ€åˆ†å¸ƒã€‚è¿™ç§å·®å¼‚è§£é‡Šäº†å½“å‰ä¸šç•Œä¸»æµçš„â€œå…ˆSFTåRLâ€ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼ä¹‹æ‰€ä»¥å–å¾—æˆåŠŸçš„å†…åœ¨æœºåˆ¶ã€‚è¯¥å·¥ä½œä¸ºæœªæ¥å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ•°æ®æ„å»ºå’Œé«˜æ•ˆè®­ç»ƒè·¯å¾„æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ä¸å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21128v1",
      "published_date": "2025-09-25 13:18:57 UTC",
      "updated_date": "2025-09-25 13:18:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:17:30.366823+00:00"
    },
    {
      "arxiv_id": "2509.21126v1",
      "title": "Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning",
      "title_zh": "æå‡å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“çš„è¡Œä¸ºè¡¨ç°ï¼šè§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„åŠ¨ä½œå»ºè®®å™¨",
      "authors": [
        "Xiefeng Wu",
        "Jing Zhao",
        "Shu Zhang",
        "Mingyu Hu"
      ],
      "abstract": "Online reinforcement learning in complex tasks is time-consuming, as massive interaction steps are needed to learn the optimal Q-function.Vision-language action (VLA) policies represent a promising direction for solving diverse tasks; however, their performance on low-level control remains limited, and effective deployment often requires task-specific expert demonstrations for fine-tuning. In this paper, we propose \\textbf{VARL} (\\textbf{V}LM as \\textbf{A}ction advisor for online \\textbf{R}einforcement \\textbf{L}earning), a framework that leverages the domain knowledge of vision-language models (VLMs) to provide action suggestions for reinforcement learning agents. Unlike previous methods, VARL provides action suggestions rather than designing heuristic rewards, thereby guaranteeing unchanged optimality and convergence. The suggested actions increase sample diversity and ultimately improve sample efficiency, especially in sparse-reward tasks. To validate the effectiveness of VARL, we evaluate it across diverse environments and agent settings. Results show that VARL greatly improves sample efficiency without introducing significant computational overhead. These advantages make VARL a general framework for online reinforcement learning and make it feasible to directly apply reinforcement learning from scratch in real-world environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VARL (VLM as Action Advisor for online Reinforcement Learning) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ Online Reinforcement Learning åœ¨å¤æ‚ä»»åŠ¡ä¸­å› äº¤äº’æ­¥æ•°å·¨å¤§è€Œå¯¼è‡´çš„æ—¶é—´æˆæœ¬è¿‡é«˜é—®é¢˜ã€‚ä¸åŒäºä»¥å¾€è®¾è®¡å¯å‘å¼å¥–åŠ±çš„æ–¹æ³•ï¼ŒVARL åˆ©ç”¨ Vision-Language Models (VLMs) çš„é¢†åŸŸçŸ¥è¯†ä¸ºæ™ºèƒ½ä½“ç›´æ¥æä¾› Action Suggestionsï¼Œè¿™ç¡®ä¿äº†æ¨¡å‹åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„æ”¶æ•›æ€§ä¸æœ€ä¼˜æ€§ä¿æŒä¸å˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¢åŠ æ ·æœ¬å¤šæ ·æ€§æ˜¾è‘—æå‡äº† Sample Efficiencyï¼Œå°¤å…¶åœ¨å¤„ç† Sparse-Reward ä»»åŠ¡æ—¶è¡¨ç°ä¼˜å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVARL åœ¨å¤šç§ç¯å¢ƒå’Œè®¾ç½®ä¸‹å‡èƒ½å¤§å¹…æé«˜å­¦ä¹ æ•ˆç‡ä¸”ä¸å¢åŠ æ˜¾è‘—è®¡ç®—å¼€é”€ã€‚è¿™ä½¿å¾— VARL æˆä¸ºä¸€ç§é€šç”¨çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå¹¶ä¸ºåœ¨çœŸå®åœºæ™¯ä¸­ç›´æ¥ä»é›¶å¼€å§‹è®­ç»ƒæ™ºèƒ½ä½“æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21126v1",
      "published_date": "2025-09-25 13:16:34 UTC",
      "updated_date": "2025-09-25 13:16:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:17:28.978288+00:00"
    },
    {
      "arxiv_id": "2509.21124v2",
      "title": "Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns",
      "title_zh": "é€šè¿‡å­¦ä¹ å¤šæ ·åŒ–æ€ç»´é“¾æ¨¡å¼æ‹“å±•åŸºç¡€æ¨¡å‹çš„æ¨ç†æ½œåŠ›",
      "authors": [
        "Xuemiao Zhang",
        "Can Ren",
        "Chengying Tu",
        "Rongxiang Weng",
        "Shuo Wang",
        "Hongfei Yan",
        "Jingang Wang",
        "Xunliang Cai"
      ],
      "abstract": "Recent progress in large reasoning models for challenging mathematical reasoning has been driven by reinforcement learning (RL). Incorporating long chain-of-thought (CoT) data during mid-training has also been shown to substantially improve reasoning depth. However, current approaches often utilize CoT data indiscriminately, leaving open the critical question of which data types most effectively enhance model reasoning capabilities. In this paper, we define the foundation model's reasoning potential for the first time as the inverse of the number of independent attempts required to correctly answer the question, which is strongly correlated with the final model performance. We then propose utilizing diverse data enriched with high-value reasoning patterns to expand the reasoning potential. Specifically, we abstract atomic reasoning patterns from CoT sequences, characterized by commonality and inductive capabilities, and use them to construct a core reference set enriched with valuable reasoning patterns. Furthermore, we propose a dual-granularity algorithm involving chains of reasoning patterns and token entropy, efficiently selecting high-value CoT data (CoTP) from the data pool that aligns with the core set, thereby training models to master reasoning effectively. Only 10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of downstream RL performance by 7.81%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹æ¨ç†æ¨¡å‹åœ¨è®­ç»ƒä¸­ç›²ç›®ä½¿ç”¨æ€ç»´é“¾(Chain-of-Thought)æ•°æ®çš„é—®é¢˜ï¼Œé¦–æ¬¡å°†åŸºç¡€æ¨¡å‹çš„â€œæ¨ç†æ½œåŠ›â€å®šä¹‰ä¸ºæ­£ç¡®å›ç­”é—®é¢˜æ‰€éœ€ç‹¬ç«‹å°è¯•æ¬¡æ•°çš„å€’æ•°ã€‚ä¸ºæ‰©å±•è¿™ä¸€æ½œåŠ›ï¼Œä½œè€…ä» CoT åºåˆ—ä¸­æŠ½è±¡å‡ºå…·æœ‰å…±æ€§å’Œå½’çº³èƒ½åŠ›çš„åŸå­æ¨ç†æ¨¡å¼ï¼Œå¹¶æ®æ­¤æ„å»ºäº†å¯Œå«é«˜ä»·å€¼æ¨ç†æ¨¡å¼çš„æ ¸å¿ƒå‚è€ƒé›†ã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§ç»“åˆæ¨ç†æ¨¡å¼é“¾ä¸æ ‡è®°ç†µ(token entropy)çš„åŒç²’åº¦ç®—æ³•ï¼Œæ—¨åœ¨ä»æµ·é‡æ•°æ®æ± ä¸­é«˜æ•ˆç­›é€‰å‡ºä¸æ ¸å¿ƒé›†å¯¹é½çš„é«˜ä»·å€¼ CoT æ•°æ®(CoTP)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…åˆ©ç”¨ 10B æ ‡è®°çš„ CoTP æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå³å¯ä½¿ 85A6B æ··åˆä¸“å®¶æ¨¡å‹(MoE)åœ¨ AIME 2024 å’Œ 2025 ç«èµ›æµ‹è¯•ä¸­æ€§èƒ½æå‡ 9.58%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å°†æ¨¡å‹åœ¨ä¸‹æ¸¸å¼ºåŒ–å­¦ä¹ (RL)é˜¶æ®µçš„æ€§èƒ½ä¸Šé™æå‡äº† 7.81%ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ•°å­¦æ¨ç†ä»»åŠ¡æ—¶çš„æ·±åº¦ä¸å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21124v2",
      "published_date": "2025-09-25 13:11:35 UTC",
      "updated_date": "2025-09-26 03:10:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:18:53.194777+00:00"
    },
    {
      "arxiv_id": "2509.21117v2",
      "title": "TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them",
      "title_zh": "TrustJudgeï¼šLLM-as-a-Judge çš„ä¸ä¸€è‡´æ€§åŠå…¶ç¼“è§£ç­–ç•¥",
      "authors": [
        "Yidong Wang",
        "Yunze Song",
        "Tingyuan Zhu",
        "Xuanwang Zhang",
        "Zhuohao Yu",
        "Hao Chen",
        "Chiyu Song",
        "Qiufeng Wang",
        "Cunxiang Wang",
        "Zhen Wu",
        "Xinyu Dai",
        "Yue Zhang",
        "Wei Ye",
        "Shikun Zhang"
      ],
      "abstract": "The adoption of Large Language Models (LLMs) as automated evaluators (LLM-as-a-judge) has revealed critical inconsistencies in current evaluation frameworks. We identify two fundamental types of inconsistencies: (1) Score-Comparison Inconsistency, where lower-rated responses outperform higher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity Inconsistency, manifested through circular preference chains (A>B>C>A) and equivalence contradictions (A=B=C\\neq A). We argue that these issues come from information loss in discrete rating systems and ambiguous tie judgments during pairwise evaluation. We propose TrustJudge, a probabilistic framework that addresses these limitations through two key innovations: 1) distribution-sensitive scoring that computes continuous expectations from discrete rating probabilities, preserving information entropy for more precise scoring, and 2) likelihood-aware aggregation that resolves transitivity violations using bidirectional preference probabilities or perplexity. We also formalize the theoretical limitations of current LLM-as-a-judge frameworks and demonstrate how TrustJudge's components overcome them. When evaluated with Llama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise Transitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining higher evaluation accuracy. Our work provides the first systematic analysis of evaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both theoretical insights and practical solutions for reliable automated assessment. The framework demonstrates consistent improvements across various model architectures and scales, enabling more trustworthy LLM evaluation without requiring additional training or human annotations. The codes can be found at https://github.com/TrustJudge/TrustJudge.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåˆ†æäº†å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„ä¼°å™¨ï¼ˆLLM-as-a-judgeï¼‰æ—¶å­˜åœ¨çš„å…³é”®ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œä¸»è¦è¯†åˆ«äº†è¯„åˆ†ä¸æ¯”è¾ƒä¸ä¸€è‡´ï¼ˆScore-Comparison Inconsistencyï¼‰ä»¥åŠæˆå¯¹ä¼ é€’æ€§ä¸ä¸€è‡´ï¼ˆPairwise Transitivity Inconsistencyï¼‰ä¸¤ç§åŸºæœ¬ç±»å‹ã€‚ä½œè€…è®¤ä¸ºè¿™äº›é—®é¢˜æºäºç¦»æ•£è¯„åˆ†ç³»ç»Ÿä¸­çš„ä¿¡æ¯æŸå¤±ä»¥åŠæˆå¯¹è¯„ä¼°ä¸­æ¨¡ç³Šçš„å¹³å±€åˆ¤æ–­ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†TrustJudgeæ¦‚ç‡æ¡†æ¶ï¼Œé€šè¿‡åˆ†å¸ƒæ•æ„Ÿè¯„åˆ†ï¼ˆdistribution-sensitive scoringï¼‰ä»ç¦»æ•£æ¦‚ç‡ä¸­è®¡ç®—è¿ç»­æœŸæœ›ä»¥ä¿ç•™ä¿¡æ¯ç†µï¼Œå¹¶åˆ©ç”¨ä¼¼ç„¶æ„ŸçŸ¥èšåˆï¼ˆlikelihood-aware aggregationï¼‰è§£å†³ä¼ é€’æ€§è¿è§„ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ä½¿ç”¨Llama-3.1-70B-Instructè¿›è¡Œè¯„ä¼°æ—¶ï¼ŒTrustJudgeå°†è¯„åˆ†æ¯”è¾ƒä¸ä¸€è‡´ç‡é™ä½äº†8.43%ï¼Œä¼ é€’æ€§ä¸ä¸€è‡´ç‡é™ä½äº†10.82%ï¼ŒåŒæ—¶ä¿æŒäº†æ›´é«˜çš„è¯„ä¼°å‡†ç¡®æ€§ã€‚è¯¥å·¥ä½œä¸ä»…ä¸ºLLM-as-a-judgeèŒƒå¼æä¾›äº†ç†è®ºæ´å¯Ÿï¼Œè¿˜ä¸ºå®ç°æ›´å…·ä¿¡ä»»åº¦çš„è‡ªåŠ¨åŒ–è¯„ä¼°æä¾›äº†æ— éœ€é¢å¤–è®­ç»ƒæˆ–äººå·¥æ ‡æ³¨çš„å®ç”¨æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 9 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.21117v2",
      "published_date": "2025-09-25 13:04:29 UTC",
      "updated_date": "2025-09-26 05:33:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:17:53.783254+00:00"
    },
    {
      "arxiv_id": "2509.21107v1",
      "title": "Cross-Modal Instructions for Robot Motion Generation",
      "title_zh": "é¢å‘æœºå™¨äººè¿åŠ¨ç”Ÿæˆçš„è·¨æ¨¡æ€æŒ‡ä»¤",
      "authors": [
        "William Barron",
        "Xiaoxiang Dong",
        "Matthew Johnson-Roberson",
        "Weiming Zhi"
      ],
      "abstract": "Teaching robots novel behaviors typically requires motion demonstrations via teleoperation or kinaesthetic teaching, that is, physically guiding the robot. While recent work has explored using human sketches to specify desired behaviors, data collection remains cumbersome, and demonstration datasets are difficult to scale. In this paper, we introduce an alternative paradigm, Learning from Cross-Modal Instructions, where robots are shaped by demonstrations in the form of rough annotations, which can contain free-form text labels, and are used in lieu of physical motion. We introduce the CrossInstruct framework, which integrates cross-modal instructions as examples into the context input to a foundational vision-language model (VLM). The VLM then iteratively queries a smaller, fine-tuned model, and synthesizes the desired motion over multiple 2D views. These are then subsequently fused into a coherent distribution over 3D motion trajectories in the robot's workspace. By incorporating the reasoning of the large VLM with a fine-grained pointing model, CrossInstruct produces executable robot behaviors that generalize beyond the environment of in the limited set of instruction examples. We then introduce a downstream reinforcement learning pipeline that leverages CrossInstruct outputs to efficiently learn policies to complete fine-grained tasks. We rigorously evaluate CrossInstruct on benchmark simulation tasks and real hardware, demonstrating effectiveness without additional fine-tuning and providing a strong initialization for policies subsequently refined via reinforcement learning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººç¤ºæ•™ä¸­ç‰©ç†æ¼”ç¤ºï¼ˆå¦‚teleoperationæˆ–kinaesthetic teachingï¼‰æ•°æ®é‡‡é›†å›°éš¾ä¸”éš¾ä»¥æ‰©å±•çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºLearning from Cross-Modal Instructionsçš„æ–°èŒƒå¼ã€‚è¯¥èŒƒå¼åˆ©ç”¨åŒ…å«è‡ªç”±æ–‡æœ¬æ ‡ç­¾çš„ç²—ç•¥æ ‡æ³¨ä½œä¸ºæ¼”ç¤ºï¼Œå–ä»£äº†ä¼ ç»Ÿçš„ç‰©ç†è¿åŠ¨æ•°æ®æ¥å¡‘é€ æœºå™¨äººè¡Œä¸ºã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†CrossInstructæ¡†æ¶ï¼Œå°†è¿™äº›è·¨æ¨¡æ€æŒ‡ä»¤æ•´åˆåˆ°åŸºç¡€è§†è§‰è¯­è¨€æ¨¡å‹(VLM)çš„ä¸Šä¸‹æ–‡è¾“å…¥ä¸­ï¼Œé€šè¿‡VLMä¸ç»†ç²’åº¦æŒ‡å‘æ¨¡å‹çš„ååŒå·¥ä½œï¼Œåœ¨å¤šä¸ª2Dè§†å›¾ä¸­åˆæˆè¿åŠ¨å¹¶èåˆä¸ºè¿è´¯çš„3Dè¿åŠ¨è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCrossInstructç»“åˆäº†å¤§å‹VLMçš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰æ³›åŒ–æ€§çš„å¯æ‰§è¡Œæœºå™¨äººè¡Œä¸ºï¼Œä¸”æ— éœ€é¢å¤–çš„å¾®è°ƒã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜èƒ½ä¸ºä¸‹æ¸¸çš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æµç¨‹æä¾›å¼ºæœ‰åŠ›çš„åˆå§‹åŒ–ï¼Œä»è€Œæ˜¾è‘—æé«˜å­¦ä¹ ç»†ç²’åº¦ä»»åŠ¡ç­–ç•¥çš„æ•ˆç‡ã€‚åœ¨åŸºå‡†ä»¿çœŸå’ŒçœŸå®ç¡¬ä»¶ä¸Šçš„æµ‹è¯•å‡éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21107v1",
      "published_date": "2025-09-25 12:54:00 UTC",
      "updated_date": "2025-09-25 12:54:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:17:54.953262+00:00"
    },
    {
      "arxiv_id": "2509.21097v1",
      "title": "GraphUniverse: Enabling Systematic Evaluation of Inductive Generalization",
      "title_zh": "GraphUniverseï¼šå®ç°å½’çº³æ³›åŒ–çš„ç³»ç»Ÿæ€§è¯„ä¼°",
      "authors": [
        "Louis Van Langendonck",
        "Guillermo BernÃ¡rdez",
        "Nina Miolane",
        "Pere Barlet-Ros"
      ],
      "abstract": "A fundamental challenge in graph learning is understanding how models generalize to new, unseen graphs. While synthetic benchmarks offer controlled settings for analysis, existing approaches are confined to single-graph, transductive settings where models train and test on the same graph structure. Addressing this gap, we introduce GraphUniverse, a framework for generating entire families of graphs to enable the first systematic evaluation of inductive generalization at scale. Our core innovation is the generation of graphs with persistent semantic communities, ensuring conceptual consistency while allowing fine-grained control over structural properties like homophily and degree distributions. This enables crucial but underexplored robustness tests, such as performance under controlled distribution shifts. Benchmarking a wide range of architectures -- from GNNs to graph transformers and topological architectures -- reveals that strong transductive performance is a poor predictor of inductive generalization. Furthermore, we find that robustness to distribution shift is highly sensitive not only to model architecture choice but also to the initial graph regime (e.g., high vs. low homophily). Beyond benchmarking, GraphUniverse's flexibility and scalability can facilitate the development of robust and truly generalizable architectures -- including next-generation graph foundation models. An interactive demo is available at https://graphuniverse.streamlit.app.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾å­¦ä¹ ä¸­æ¨¡å‹å¦‚ä½•æ³›åŒ–åˆ°æœªçŸ¥å›¾çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†GraphUniverseæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å¯¹å½’çº³æ³›åŒ–(inductive generalization)çš„å¤§è§„æ¨¡ç³»ç»ŸåŒ–è¯„ä¼°ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºç”Ÿæˆå…·æœ‰æŒä¹…è¯­ä¹‰ç¤¾åŒº(persistent semantic communities)çš„å›¾æ—ï¼Œåœ¨ä¿æŒæ¦‚å¿µä¸€è‡´æ€§çš„åŒæ—¶ï¼Œå…è®¸å¯¹åŒè´¨æ€§(homophily)å’Œåº¦åˆ†å¸ƒ(degree distributions)ç­‰ç»“æ„å±æ€§è¿›è¡Œç²¾ç»†æ§åˆ¶ã€‚é€šè¿‡å¯¹ä»GNNsåˆ°å›¾å˜å‹å™¨(graph transformers)ä»¥åŠæ‹“æ‰‘æ¶æ„çš„å¹¿æ³›åŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶å‘ç°å¼ºå¤§çš„è½¬å¯¼æ€§èƒ½(transductive performance)å¹¶ä¸èƒ½å¾ˆå¥½åœ°é¢„ç¤ºå½’çº³æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå®éªŒè¡¨æ˜æ¨¡å‹å¯¹åˆ†å¸ƒåç§»(distribution shift)çš„é²æ£’æ€§ä¸ä»…å–å†³äºæ¶æ„é€‰æ‹©ï¼Œè¿˜å—åˆå§‹å›¾çŠ¶æ€ï¼ˆå¦‚é«˜åŒè´¨æ€§ä¸ä½åŒè´¨æ€§ï¼‰çš„é«˜åº¦å½±å“ã€‚GraphUniverseçš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ä¸ºå¼€å‘çœŸæ­£å…·æœ‰æ³›åŒ–èƒ½åŠ›çš„æ¶æ„åŠä¸‹ä¸€ä»£å›¾åŸºç¡€æ¨¡å‹(graph foundation models)æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21097v1",
      "published_date": "2025-09-25 12:46:01 UTC",
      "updated_date": "2025-09-25 12:46:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:17:59.478246+00:00"
    },
    {
      "arxiv_id": "2509.21091v1",
      "title": "Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute",
      "title_zh": "Best-of-$\\infty$ï¼šæ¨ç†æ—¶è®¡ç®—çš„æ¸è¿‘æ€§èƒ½",
      "authors": [
        "Junpei Komiyama",
        "Daisuke Oba",
        "Masafumi Oyamada"
      ],
      "abstract": "We study best-of-$N$ for large language models (LLMs) where the selection is based on majority voting. In particular, we analyze the limit $N \\to \\infty$, which we denote as Best-of-$\\infty$. While this approach achieves impressive performance in the limit, it requires an infinite test-time budget. To address this, we propose an adaptive generation scheme that selects $N$ based on answer agreement, thereby efficiently allocating inference-time computation. Beyond adaptivity, we extend the framework to weighted ensembles of multiple LLMs, showing that such mixtures can outperform any individual model. The optimal ensemble weighting is formulated and efficiently computed as a mixed-integer linear program. Extensive experiments demonstrate the effectiveness of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå¤šæ•°æŠ•ç¥¨(majority voting)çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨ç†é˜¶æ®µçš„æ€§èƒ½ï¼Œé‡ç‚¹åˆ†æäº†æ ·æœ¬æ•°é‡ $N$ è¶‹äºæ— ç©·æ—¶çš„æ¸è¿‘è¡¨ç°ï¼Œå³ Best-of-$\\infty$ æé™ã€‚é’ˆå¯¹è¯¥æé™æƒ…å†µä¸‹æ— é™æ¨ç†æˆæœ¬çš„æŒ‘æˆ˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç­”æ¡ˆä¸€è‡´æ€§(answer agreement)çš„è‡ªé€‚åº”ç”Ÿæˆæ–¹æ¡ˆï¼Œä»¥å®ç°æ¨ç†æ—¶é—´è®¡ç®—èµ„æºçš„é«˜æ•ˆåˆ†é…ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ‰©å±•è‡³å¤šä¸ª LLMs çš„åŠ æƒé›†æˆ(weighted ensembles)ï¼Œè¯æ˜äº†æ­¤ç±»æ··åˆæ¨¡å‹èƒ½å¤Ÿè¶…è¶Šä»»ä½•å•ä¸€æ¨¡å‹çš„è¡¨ç°ã€‚æœ€ä¼˜é›†æˆæƒé‡çš„è®¡ç®—è¢«å»ºæ¨¡ä¸ºæ··åˆæ•´æ•°çº¿æ€§è§„åˆ’(mixed-integer linear program)é—®é¢˜å¹¶å¾—åˆ°äº†é«˜æ•ˆè§£å†³ã€‚å®éªŒç»“æœéªŒè¯äº†è¯¥æ–¹æ³•åœ¨æå‡æ¨¡å‹æ€§èƒ½ä¸ä¼˜åŒ–æµ‹è¯•æ—¶è®¡ç®—(test-time compute)èµ„æºåˆ†é…æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21091v1",
      "published_date": "2025-09-25 12:41:05 UTC",
      "updated_date": "2025-09-25 12:41:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:18:01.056979+00:00"
    },
    {
      "arxiv_id": "2509.21084v1",
      "title": "Vision Transformers: the threat of realistic adversarial patches",
      "title_zh": "Vision Transformersï¼šç°å®å¯¹æŠ—è¡¥ä¸çš„å¨èƒ",
      "authors": [
        "Kasper Cools",
        "Clara Maathuis",
        "Alexander M. van Oers",
        "Claudia S. HÃ¼bner",
        "Nikos Deligiannis",
        "Marijke Vandewal",
        "Geert De Cubber"
      ],
      "abstract": "The increasing reliance on machine learning systems has made their security a critical concern. Evasion attacks enable adversaries to manipulate the decision-making processes of AI systems, potentially causing security breaches or misclassification of targets. Vision Transformers (ViTs) have gained significant traction in modern machine learning due to increased 1) performance compared to Convolutional Neural Networks (CNNs) and 2) robustness against adversarial perturbations. However, ViTs remain vulnerable to evasion attacks, particularly to adversarial patches, unique patterns designed to manipulate AI classification systems. These vulnerabilities are investigated by designing realistic adversarial patches to cause misclassification in person vs. non-person classification tasks using the Creases Transformation (CT) technique, which adds subtle geometric distortions similar to those occurring naturally when wearing clothing. This study investigates the transferability of adversarial attack techniques used in CNNs when applied to ViT classification models. Experimental evaluation across four fine-tuned ViT models on a binary person classification task reveals significant vulnerability variations: attack success rates ranged from 40.04% (google/vit-base-patch16-224-in21k) to 99.97% (facebook/dino-vitb16), with google/vit-base-patch16-224 achieving 66.40% and facebook/dinov3-vitb16 reaching 65.17%. These results confirm the cross-architectural transferability of adversarial patches from CNNs to ViTs, with pre-training dataset scale and methodology strongly influencing model resilience to adversarial attacks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰å˜æ¢å™¨(Vision Transformers, ViTs)åœ¨é¢å¯¹ç°å®å¯¹æŠ—è¡¥ä¸(realistic adversarial patches)æ—¶çš„å®‰å…¨æ€§æ¼æ´ã€‚ä½œè€…é€šè¿‡çš±è¤¶å˜æ¢(Creases Transformation, CT)æŠ€æœ¯è®¾è®¡äº†æ¨¡æ‹Ÿè¡£ç‰©è‡ªç„¶å‡ ä½•ç•¸å˜çš„å¯¹æŠ—è¡¥ä¸ï¼Œæ—¨åœ¨å¯¹äººä¸éäººåˆ†ç±»ä»»åŠ¡è¿›è¡Œè§„é¿æ”»å‡»æµ‹è¯•ã€‚ç ”ç©¶æ·±å…¥åˆ†æäº†åŸæœ¬ç”¨äºå·ç§¯ç¥ç»ç½‘ç»œ(CNNs)çš„å¯¹æŠ—æ”»å‡»æŠ€æœ¯åœ¨ä¸åŒViTæ¨¡å‹ä¸­çš„å¯è¿ç§»æ€§(transferability)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå››ç§å—è¯•ViTæ¨¡å‹çš„æ”»å‡»æˆåŠŸç‡(Attack Success Rates)å·®å¼‚æ˜¾è‘—ï¼ŒèŒƒå›´ä»40.04%åˆ°99.97%ä¸ç­‰ã€‚è¿™ä¸€ç»“æœè¯å®äº†å¯¹æŠ—è¡¥ä¸å…·æœ‰è·¨æ¶æ„çš„æ”»å‡»èƒ½åŠ›ï¼Œå¹¶å‘ç°é¢„è®­ç»ƒæ•°æ®é›†çš„è§„æ¨¡å’Œè®­ç»ƒæ–¹æ³•æ˜¯å†³å®šæ¨¡å‹å¯¹æŠ—æ”»å‡»éŸ§æ€§çš„å…³é”®å› ç´ ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to Sensors + Imaging; presented on 17th of September (Artificial Intelligence for Security and Defence Applications III)",
      "pdf_url": "https://arxiv.org/pdf/2509.21084v1",
      "published_date": "2025-09-25 12:36:25 UTC",
      "updated_date": "2025-09-25 12:36:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:18:16.471772+00:00"
    },
    {
      "arxiv_id": "2509.21081v1",
      "title": "TyphoonMLA: A Mixed Naive-Absorb MLA Kernel For Shared Prefix",
      "title_zh": "TyphoonMLAï¼šä¸€ç§é¢å‘å…±äº«å‰ç¼€çš„ Naive-Absorb æ··åˆå¼ MLA ç®—å­",
      "authors": [
        "Ahmet Caner YÃ¼zÃ¼gÃ¼ler",
        "Ahmet Ã‡elik",
        "Jiawei Zhuang",
        "Lukas Cavigelli"
      ],
      "abstract": "Multi-Head Latent Attention (MLA) is a recent attention mechanism adopted in state-of-the-art LLMs such as DeepSeek-v3 and Kimi K2. Thanks to its novel formulation, MLA allows two functionally equivalent but computationally distinct kernel implementations: naive and absorb. While the naive kernels (e.g., FlashAttention) are typically preferred in training and prefill for their computational efficiency, existing decoding kernels (e.g., FlashMLA) rely on the absorb method to minimize HBM bandwidth usage. However, the compute-bound nature of the absorb implementations prohibits performance benefits from data reuse opportunities in attention calculations, such as shared prefixes. In this work, we introduce TyphoonMLA, a hybrid approach that combines naive and absorb formulations to harness the strengths of both. TyphoonMLA effectively leverages the shared prefix by applying the naive formulation to the compute-bound parts of attention calculations, while reducing the bandwidth requirements for non-shared parts by using the absorb formulation. As a result, TyphoonMLA improves the throughput of attention calculations in MLA architectures by up to 3x and 3.24x on NPU and GPUs, with only a 3% overhead in HBM size.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ DeepSeek-v3 å’Œ Kimi K2 ç­‰å¤§è¯­è¨€æ¨¡å‹é‡‡ç”¨çš„ Multi-Head Latent Attention (MLA) æœºåˆ¶ï¼Œåˆ†æäº†å…¶åœ¨å¤„ç†å…±äº«å‰ç¼€ (shared prefix) æ—¶çš„æ€§èƒ½ç“¶é¢ˆã€‚ç°æœ‰çš„ absorb å®ç°å› å…¶è®¡ç®—å—é™ (compute-bound) çš„ç‰¹æ€§ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨æ•°æ®å¤ç”¨æœºä¼šæ¥æå‡æ•ˆç‡ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† TyphoonMLAï¼Œä¸€ç§å°† naive å’Œ absorb ä¸¤ç§å½¢å¼ç›¸ç»“åˆçš„æ··åˆ kernel å®ç°æ–¹æ¡ˆã€‚TyphoonMLA åœ¨å¤„ç†è®¡ç®—å—é™çš„å…±äº«å‰ç¼€éƒ¨åˆ†æ—¶åº”ç”¨ naive å½¢å¼ï¼Œè€Œå¯¹éå…±äº«éƒ¨åˆ†åˆ™åˆ©ç”¨ absorb å½¢å¼ä»¥é™ä½æ˜¾å­˜å¸¦å®½éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä»…å¢åŠ  3% çš„ HBM æ˜¾å­˜å¼€é”€ä¸‹ï¼ŒTyphoonMLA åœ¨ NPU å’Œ GPU ä¸Šçš„ MLA è®¡ç®—ååé‡åˆ†åˆ«æå‡äº†é«˜è¾¾ 3 å€å’Œ 3.24 å€ã€‚è¯¥æˆæœä¸º MLA æ¶æ„åœ¨é•¿æ–‡æœ¬å’Œå¤šè½®å¯¹è¯åœºæ™¯ä¸‹çš„é«˜æ•ˆæ¨ç†æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21081v1",
      "published_date": "2025-09-25 12:32:02 UTC",
      "updated_date": "2025-09-25 12:32:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:18:17.850944+00:00"
    },
    {
      "arxiv_id": "2509.21080v1",
      "title": "Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs",
      "title_zh": "æ¨¡å‹é‡‡ç”¨ä½•ç§æ–‡åŒ–è§†è§’ï¼Ÿè®ºå¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ–‡åŒ–å®šä½åè§åŠå…¶æ™ºèƒ½ä½“ç¼“è§£",
      "authors": [
        "Yixin Wan",
        "Xingrun Chen",
        "Kai-Wei Chang"
      ],
      "abstract": "Large language models (LLMs) have unlocked a wide range of downstream generative applications. However, we found that they also risk perpetuating subtle fairness issues tied to culture, positioning their generations from the perspectives of the mainstream US culture while demonstrating salient externality towards non-mainstream ones. In this work, we identify and systematically investigate this novel culture positioning bias, in which an LLM's default generative stance aligns with a mainstream view and treats other cultures as outsiders. We propose the CultureLens benchmark with 4000 generation prompts and 3 evaluation metrics for quantifying this bias through the lens of a culturally situated interview script generation task, in which an LLM is positioned as an onsite reporter interviewing local people across 10 diverse cultures. Empirical evaluation on 5 state-of-the-art LLMs reveals a stark pattern: while models adopt insider tones in over 88 percent of US-contexted scripts on average, they disproportionately adopt mainly outsider stances for less dominant cultures. To resolve these biases, we propose 2 inference-time mitigation methods: a baseline prompt-based Fairness Intervention Pillars (FIP) method, and a structured Mitigation via Fairness Agents (MFA) framework consisting of 2 pipelines: (1) MFA-SA (Single-Agent) introduces a self-reflection and rewriting loop based on fairness guidelines. (2) MFA-MA (Multi-Agent) structures the process into a hierarchy of specialized agents: a Planner Agent(initial script generation), a Critique Agent (evaluates initial script against fairness pillars), and a Refinement Agent (incorporates feedback to produce a polished, unbiased script). Empirical results showcase the effectiveness of agent-based methods as a promising direction for mitigating biases in generative LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä¸­æ™®éå­˜åœ¨çš„æ–‡åŒ–å®šä½åå·®(Cultural Positioning Bias)ï¼Œå³æ¨¡å‹ç”Ÿæˆå†…å®¹æ—¶å€¾å‘äºé»˜è®¤é‡‡å–ä¸»æµç¾å›½æ–‡åŒ–è§†è§’ï¼Œè€Œå°†å…¶ä»–æ–‡åŒ–è§†ä¸ºâ€œå¤–éƒ¨äººâ€ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼€å‘äº†åŒ…å«4000ä¸ªæç¤ºè¯å’Œ3é¡¹è¯„ä¼°æŒ‡æ ‡çš„CultureLensåŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡æ¨¡æ‹Ÿè·¨æ–‡åŒ–ç°åœºé‡‡è®¿ä»»åŠ¡æ¥é‡åŒ–è¿™ç§åå·®ã€‚å®éªŒæ­ç¤ºäº†æ˜¾è‘—çš„å…¬å¹³æ€§é—®é¢˜ï¼šæ¨¡å‹åœ¨å¤„ç†ç¾å›½èƒŒæ™¯æ—¶è¡¨ç°å‡ºæå¼ºçš„å†…éƒ¨äººç«‹åœºï¼Œä½†åœ¨é¢å¯¹éä¸»æµæ–‡åŒ–æ—¶åˆ™ä¸æˆæ¯”ä¾‹åœ°é‡‡ç”¨å±€å¤–äººè§†è§’ã€‚ä¸ºç¼“è§£æ­¤ç±»åè§ï¼Œç ”ç©¶æå‡ºäº†æç¤ºè¯å¹²é¢„æ–¹æ³•(FIP)ä»¥åŠç”±å•æ™ºèƒ½ä½“(MFA-SA)å’Œå¤šæ™ºèƒ½ä½“(MFA-MA)ç»„æˆçš„å…¬å¹³æ™ºèƒ½ä½“æ¡†æ¶ã€‚MFA-MAæ¡†æ¶é€šè¿‡è§„åˆ’(Planner Agent)ã€æ‰¹è¯„(Critique Agent)å’Œç²¾ç‚¼(Refinement Agent)æ™ºèƒ½ä½“çš„ååŒå·¥ä½œï¼Œåˆ©ç”¨è‡ªæˆ‘åæ€å’Œåé¦ˆæœºåˆ¶ç”Ÿæˆå…·å¤‡æ–‡åŒ–æ•æ„Ÿæ€§çš„å†…å®¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§åŸºäºå¤šæ™ºèƒ½ä½“åä½œçš„ç¼“è§£ç­–ç•¥æ˜¯æå‡ç”Ÿæˆå¼LLMsæ–‡åŒ–å…¬å¹³æ€§çš„æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21080v1",
      "published_date": "2025-09-25 12:28:25 UTC",
      "updated_date": "2025-09-25 12:28:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:18:17.562830+00:00"
    },
    {
      "arxiv_id": "2509.21075v1",
      "title": "Communication Bias in Large Language Models: A Regulatory Perspective",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ä¼ æ’­åè§ï¼šç›‘ç®¡è§†è§’",
      "authors": [
        "Adrian Kuenzler",
        "Stefan Schmid"
      ],
      "abstract": "Large language models (LLMs) are increasingly central to many applications, raising concerns about bias, fairness, and regulatory compliance. This paper reviews risks of biased outputs and their societal impact, focusing on frameworks like the EU's AI Act and the Digital Services Act. We argue that beyond constant regulation, stronger attention to competition and design governance is needed to ensure fair, trustworthy AI. This is a preprint of the Communications of the ACM article of the same title.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)ä¸­çš„é€šä¿¡åå·®ï¼Œå¹¶ä»ç›‘ç®¡è§’åº¦å¯¹å…¶è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚éšç€ LLMs åœ¨å„ç±»åº”ç”¨ä¸­çš„æ ¸å¿ƒåœ°ä½æ—¥ç›Šçªå‡ºï¼Œå…¶äº§ç”Ÿçš„åå·®ã€å…¬å¹³æ€§ä»¥åŠåˆè§„æ€§é—®é¢˜å¼•å‘äº†å¹¿æ³›å…³æ³¨ã€‚è®ºæ–‡è¯¦ç»†è¯„ä¼°äº†åå·®è¾“å‡ºå¸¦æ¥çš„é£é™©åŠå…¶å¯¹ç¤¾ä¼šçš„å½±å“ï¼Œé‡ç‚¹å‚è€ƒäº†æ¬§ç›Ÿçš„ã€Šäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹(EU AI Act)å’Œã€Šæ•°å­—æœåŠ¡æ³•æ¡ˆã€‹(Digital Services Act)ç­‰æ³•å¾‹æ¡†æ¶ã€‚ä½œè€…æŒ‡å‡ºï¼Œé™¤äº†æŒç»­çš„ç›‘ç®¡å¤–ï¼Œè¿˜éœ€è¦åŠ å¼ºå¯¹ç«äº‰å’Œè®¾è®¡æ²»ç†(design governance)çš„å…³æ³¨ï¼Œä»¥ç¡®ä¿ AI ç³»ç»Ÿçš„å…¬å¹³æ€§ä¸å¯é æ€§ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºé€æ˜ä¸”å¯ä¿¡çš„äººå·¥æ™ºèƒ½æ²»ç†ä½“ç³»æä¾›äº†é‡è¦å‚è€ƒï¼Œå¼ºè°ƒäº†æŠ€æœ¯è®¾è®¡ä¸æ”¿ç­–å¹²é¢„ç›¸ç»“åˆçš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.DC",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21075v1",
      "published_date": "2025-09-25 12:25:06 UTC",
      "updated_date": "2025-09-25 12:25:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:18:14.762787+00:00"
    },
    {
      "arxiv_id": "2509.21072v1",
      "title": "Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution",
      "title_zh": "Recon-Actï¼šåŸºäºç½‘é¡µä¾¦å¯Ÿã€å·¥å…·ç”Ÿæˆä¸ä»»åŠ¡æ‰§è¡Œçš„è‡ªæ¼”åŒ–å¤šæ™ºèƒ½ä½“æµè§ˆå™¨ä½¿ç”¨ç³»ç»Ÿ",
      "authors": [
        "Kaiwen He",
        "Zhiwei Wang",
        "Chenyi Zhuang",
        "Jinjie Gu"
      ],
      "abstract": "Recent years, multimodal models have made remarkable strides and pave the way for intelligent browser use agents. However, when solving tasks on real world webpages in multi-turn, long-horizon trajectories, current agents still suffer from disordered action sequencing and excessive trial and error during execution. This paper introduces Recon-Act, a self-evolving multi-agent framework grounded in Reconnaissance-Action behavioral paradigm. The system comprises a Reconnaissance Team and an Action Team: the former conducts comparative analysis and tool generation, while the latter handles intent decomposition, tool orchestration, and execution. By contrasting the erroneous trajectories with successful ones, the Reconnaissance Team infers remedies, and abstracts them into a unified notion of generalized tools, either expressed as hints or as rule-based codes, and register to the tool archive in real time. The Action Team reinference the process empowered with these targeting tools, thus establishing a closed-loop training pipeline of data-tools-action-feedback. Following the 6 level implementation roadmap proposed in this work, we have currently reached Level 3 (with limited human-in-the-loop intervention). Leveraging generalized tools obtained through reconnaissance, Recon-Act substantially improves adaptability to unseen websites and solvability on long-horizon tasks, and achieves state-of-the-art performance on the challenging VisualWebArena dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Recon-Actï¼Œä¸€ç§åŸºäºä¾¦å¯Ÿ-è¡ŒåŠ¨(Reconnaissance-Action)è¡Œä¸ºèŒƒå¼çš„è‡ªè¿›åŒ–å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æµè§ˆå™¨ä½¿ç”¨æ™ºèƒ½ä½“åœ¨å¤„ç†å¤æ‚ç½‘é¡µä»»åŠ¡æ—¶å­˜åœ¨çš„åŠ¨ä½œåºåˆ—ç´Šä¹±å’Œè¿‡åº¦è¯•é”™é—®é¢˜ã€‚è¯¥ç³»ç»Ÿç”±ä¾¦å¯Ÿå›¢é˜Ÿ(Reconnaissance Team)å’Œè¡ŒåŠ¨å›¢é˜Ÿ(Action Team)ç»„æˆï¼Œå‰è€…è´Ÿè´£å¯¹æ¯”åˆ†æå’Œå·¥å…·ç”Ÿæˆï¼Œåè€…è´Ÿè´£æ„å›¾åˆ†è§£ã€å·¥å…·ç¼–æ’åŠæ‰§è¡Œã€‚é€šè¿‡å¯¹æ¯”é”™è¯¯è½¨è¿¹ä¸æˆåŠŸè½¨è¿¹ï¼Œä¾¦å¯Ÿå›¢é˜Ÿæ¨æ–­å‡ºæ”¹è¿›æ–¹æ¡ˆï¼Œå¹¶å°†å…¶æŠ½è±¡ä¸ºåŒ…å«æç¤ºè¯æˆ–ä»£ç çš„é€šç”¨å·¥å…·(generalized tools)ï¼Œå®ç°å®æ—¶å·¥å…·æ³¨å†Œã€‚è¡ŒåŠ¨å›¢é˜Ÿåœ¨è¿™äº›é’ˆå¯¹æ€§å·¥å…·çš„å¢å¼ºä¸‹é‡æ–°è¿›è¡Œæ¨ç†ï¼Œä»è€Œæ„å»ºäº†æ•°æ®-å·¥å…·-è¡ŒåŠ¨-åé¦ˆ(data-tools-action-feedback)çš„é—­ç¯è®­ç»ƒæµæ°´çº¿ã€‚è¯¥å·¥ä½œè¿˜æå‡ºäº†ä¸€ä¸ªå…­çº§å®ç°è·¯çº¿å›¾ï¼Œç›®å‰Recon-Actå·²è¾¾åˆ°ç¬¬ä¸‰çº§æ°´å¹³ã€‚å®éªŒè¯æ˜ï¼ŒRecon-Actæ˜¾è‘—æå‡äº†åœ¨æœªçŸ¥ç½‘ç«™ä¸Šçš„é€‚åº”æ€§å’Œé•¿ç¨‹ä»»åŠ¡(long-horizon tasks)çš„è§£å†³èƒ½åŠ›ï¼Œå¹¶åœ¨æŒ‘æˆ˜æ€§çš„VisualWebArenaåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å½“å‰æœ€å…ˆè¿›(SOTA)çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21072v1",
      "published_date": "2025-09-25 12:23:49 UTC",
      "updated_date": "2025-09-25 12:23:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:18:29.382926+00:00"
    },
    {
      "arxiv_id": "2509.21070v1",
      "title": "ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning",
      "title_zh": "ScaleDiffï¼šé¢å‘é«˜çº§æ•°å­¦æ¨ç†çš„éš¾é¢˜è§„æ¨¡åŒ–ç”Ÿæˆ",
      "authors": [
        "Qizhi Pei",
        "Zhuoshi Pan",
        "Honglin Lin",
        "Xin Gao",
        "Yu Li",
        "Zinan Tang",
        "Conghui He",
        "Rui Yan",
        "Lijun Wu"
      ],
      "abstract": "Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-solving, often benefiting from training on difficult mathematical problems that stimulate intricate reasoning. Recent efforts have explored automated synthesis of mathematical problems by prompting proprietary models or large-scale open-source models from seed data or inherent mathematical concepts. However, scaling up these methods remains challenging due to their high computational/API cost, complexity of prompting, and limited difficulty level of the generated problems. To overcome these limitations, we propose ScaleDiff, a simple yet effective pipeline designed to scale the creation of difficult problems. We efficiently identify difficult problems from existing datasets with only a single forward pass using an adaptive thinking model, which can perceive problem difficulty and automatically switch between \"Thinking\" and \"NoThinking\" modes. We then train a specialized difficult problem generator (DiffGen-8B) on this filtered difficult data, which can produce new difficult problems in large scale, eliminating the need for complex, per-instance prompting and its associated high API costs. Fine-tuning Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial performance increase of 11.3% compared to the original dataset and achieves a 65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500, outperforming recent strong LRMs like OpenThinker3. Notably, this performance is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating that our pipeline can effectively transfer advanced reasoning capabilities without relying on larger, more expensive teacher models. Furthermore, we observe a clear scaling phenomenon in model performance on difficult benchmarks as the quantity of difficult problems increases. Code: https://github.com/QizhiPei/ScaleDiff.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ScaleDiffï¼Œè¿™æ˜¯ä¸€ç§ç®€å•ä¸”é«˜æ•ˆçš„æµæ°´çº¿ï¼Œæ—¨åœ¨é€šè¿‡è§„æ¨¡åŒ–åˆ›å»ºå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦éš¾é¢˜æ¥å¢å¼º Large Reasoning Models (LRMs) çš„é«˜çº§æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†é«˜æ•ˆç­›é€‰æ•°æ®ï¼Œç ”ç©¶é‡‡ç”¨äº†ä¸€ç§ adaptive thinking modelï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ„ŸçŸ¥é—®é¢˜éš¾åº¦å¹¶åœ¨ â€œThinkingâ€ ä¸ â€œNoThinkingâ€ æ¨¡å¼é—´è‡ªåŠ¨åˆ‡æ¢ï¼Œä»è€Œä»ç°æœ‰æ•°æ®é›†ä¸­è¯†åˆ«å‡ºçœŸæ­£çš„éš¾é¢˜ã€‚åŸºäºç­›é€‰å‡ºçš„æ•°æ®è®­ç»ƒçš„ä¸“é—¨ç”Ÿæˆå™¨ DiffGen-8B èƒ½å¤Ÿå¤§è§„æ¨¡äº§å‡ºæ–°éš¾é¢˜ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹é«˜æˆæœ¬ API æˆ–å¤æ‚æç¤ºå·¥ç¨‹çš„ä¾èµ–ã€‚å®éªŒæ˜¾ç¤ºï¼Œåœ¨ ScaleDiff-Math æ•°æ®é›†ä¸Šå¾®è°ƒåçš„ Qwen2.5-Math-7B-Instruct æ€§èƒ½æå‡äº† 11.3%ï¼Œå¹¶åœ¨ AIME'24ã€AIME'25ã€HMMT-Feb'25 ç­‰å¤šä¸ªç«èµ›åŸºå‡†ä¸Šå–å¾—äº† 65.9% çš„å¹³å‡å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº† OpenThinker3 ç­‰å…ˆè¿›æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ä»…è¯æ˜äº†åˆ©ç”¨é«˜æ€§ä»·æ¯”çš„ Qwen3-8B ä½œä¸ºæ•™å¸ˆæ¨¡å‹å®ç°é«˜çº§æ¨ç†èƒ½åŠ›è¿ç§»çš„å¯è¡Œæ€§ï¼Œè¿˜æ­ç¤ºäº†æ¨¡å‹æ€§èƒ½åœ¨å›°éš¾ä»»åŠ¡ä¸Šéšç€éš¾é¢˜æ•°æ®é‡å¢åŠ è€Œå‘ˆç°å‡ºçš„ Scaling ç°è±¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.21070v1",
      "published_date": "2025-09-25 12:22:44 UTC",
      "updated_date": "2025-09-25 12:22:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:19:20.382673+00:00"
    },
    {
      "arxiv_id": "2509.21061v1",
      "title": "EnGraf-Net: Multiple Granularity Branch Network with Fine-Coarse Graft Grained for Classification Task",
      "title_zh": "EnGraf-Netï¼šèåˆç²—ç»†ç²’åº¦å«æ¥çš„å¤šç²’åº¦åˆ†æ”¯åˆ†ç±»ç½‘ç»œ",
      "authors": [
        "Riccardo La Grassa",
        "Ignazio Gallo",
        "Nicola Landro"
      ],
      "abstract": "Fine-grained classification models are designed to focus on the relevant details necessary to distinguish highly similar classes, particularly when intra-class variance is high and inter-class variance is low. Most existing models rely on part annotations such as bounding boxes, part locations, or textual attributes to enhance classification performance, while others employ sophisticated techniques to automatically extract attention maps. We posit that part-based approaches, including automatic cropping methods, suffer from an incomplete representation of local features, which are fundamental for distinguishing similar objects. While fine-grained classification aims to recognize the leaves of a hierarchical structure, humans recognize objects by also forming semantic associations. In this paper, we leverage semantic associations structured as a hierarchy (taxonomy) as supervised signals within an end-to-end deep neural network model, termed EnGraf-Net. Extensive experiments on three well-known datasets CIFAR-100, CUB-200-2011, and FGVC-Aircraft demonstrate the superiority of EnGraf-Net over many existing fine-grained models, showing competitive performance with the most recent state-of-the-art approaches, without requiring cropping techniques or manual annotations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EnGraf-Netï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ç»†ç²’åº¦åˆ†ç±»(Fine-grained classification)ä»»åŠ¡çš„å¤šç²’åº¦åˆ†æ”¯ç½‘ç»œï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å±€éƒ¨ç‰¹å¾è¡¨ç¤ºæ—¶ä¸å®Œæ•´çš„é—®é¢˜ã€‚EnGraf-Netçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨åˆ†å±‚ç»“æ„çš„è¯­ä¹‰å…³è”(Taxonomy)ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œå¹¶å°†å…¶æ•´åˆåˆ°ç«¯åˆ°ç«¯çš„æ·±åº¦ç¥ç»ç½‘ç»œä¸­ã€‚è¯¥æ¨¡å‹é€šè¿‡æ¨¡æ‹Ÿäººç±»åœ¨è¯†åˆ«ç‰©ä½“æ—¶çš„è¯­ä¹‰è”æƒ³è¿‡ç¨‹ï¼Œæœ‰æ•ˆå¢å¼ºäº†æ¨¡å‹å¯¹é«˜åº¦ç›¸ä¼¼ç±»åˆ«é—´ç»†å¾®å·®å¼‚çš„æ•æ‰èƒ½åŠ›ã€‚åœ¨CIFAR-100ã€CUB-200-2011å’ŒFGVC-Aircraftæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒEnGraf-Netåœ¨ä¸ä¾èµ–æ‰‹åŠ¨æ ‡æ³¨æˆ–è£å‰ª(Cropping)æŠ€æœ¯çš„å‰æä¸‹ï¼Œæ€§èƒ½ä¼˜äºå¤šç§ç°æœ‰çš„ç»†ç²’åº¦æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹ç®€æ´æ€§çš„åŒæ—¶ï¼Œè¾¾åˆ°äº†ä¸æœ€æ–°å°–ç«¯æŠ€æœ¯(State-of-the-art)æå…·ç«äº‰åŠ›çš„æ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8",
      "pdf_url": "https://arxiv.org/pdf/2509.21061v1",
      "published_date": "2025-09-25 12:11:42 UTC",
      "updated_date": "2025-09-25 12:11:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:19:19.290576+00:00"
    },
    {
      "arxiv_id": "2509.21054v1",
      "title": "Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems",
      "title_zh": "æ¨ç†åˆ†æ­§ï¼šæ¨¡å‹æ€ç»´è¿‡ç¨‹å¦‚ä½•å†³å®šå¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„è¯´æœè¡Œä¸º",
      "authors": [
        "Haodong Zhao",
        "Jidong Li",
        "Zhaomin Wu",
        "Tianjie Ju",
        "Zhuosheng Zhang",
        "Bingsheng He",
        "Gongshen Liu"
      ],
      "abstract": "The rapid proliferation of recent Multi-Agent Systems (MAS), where Large Language Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to solve complex problems, necessitates a deep understanding of the persuasion dynamics that govern their interactions. This paper challenges the prevailing hypothesis that persuasive efficacy is primarily a function of model scale. We propose instead that these dynamics are fundamentally dictated by a model's underlying cognitive process, especially its capacity for explicit reasoning. Through a series of multi-agent persuasion experiments, we uncover a fundamental trade-off we term the Persuasion Duality. Our findings reveal that the reasoning process in LRMs exhibits significantly greater resistance to persuasion, maintaining their initial beliefs more robustly. Conversely, making this reasoning process transparent by sharing the \"thinking content\" dramatically increases their ability to persuade others. We further consider more complex transmission persuasion situations and reveal complex dynamics of influence propagation and decay within multi-hop persuasion between multiple agent networks. This research provides systematic evidence linking a model's internal processing architecture to its external persuasive behavior, offering a novel explanation for the susceptibility of advanced models and highlighting critical implications for the safety, robustness, and design of future MAS.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(MAS)ä¸­å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸å¤§æ¨ç†æ¨¡å‹(LRMs)ä¹‹é—´çš„è¯´æœåŠ¨åŠ›å­¦ï¼ŒæŒ‘æˆ˜äº†è¯´æœæ•ˆèƒ½ä¸»è¦å–å†³äºæ¨¡å‹è§„æ¨¡çš„ä¼ ç»Ÿå‡è®¾ã€‚ç ”ç©¶æå‡ºï¼Œè¯´æœè¿‡ç¨‹æœ¬è´¨ä¸Šæ˜¯ç”±æ¨¡å‹çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯å…¶æ˜¾å¼æ¨ç†èƒ½åŠ›æ‰€å†³å®šçš„ã€‚é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼Œä½œè€…æ­ç¤ºäº†è¢«ç§°ä¸ºâ€œè¯´æœäºŒå…ƒæ€§â€(Persuasion Duality)çš„æ ¸å¿ƒç°è±¡ï¼šä¸€æ–¹é¢ï¼ŒLRMsçš„æ¨ç†è¿‡ç¨‹ä½¿å…¶å¯¹è¢«è¯´æœè¡¨ç°å‡ºæå¼ºçš„æŠµæŠ—åŠ›ï¼›å¦ä¸€æ–¹é¢ï¼Œå½“å…¶â€œæ€è€ƒå†…å®¹â€(thinking content)é€æ˜åŒ–å¹¶è¢«å…±äº«æ—¶ï¼Œå…¶è¯´æœä»–äººçš„èƒ½åŠ›ä¼šå¤§å¹…æå‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ­ç¤ºäº†å¤šè·³è¯´æœåœºæ™¯ä¸­å½±å“åŠ›ä¼ æ’­ä¸è¡°å‡çš„å¤æ‚åŠ¨æ€ã€‚è¯¥ç ”ç©¶ç³»ç»Ÿæ€§åœ°è¯æ˜äº†æ¨¡å‹å†…éƒ¨å¤„ç†æ¶æ„ä¸å…¶å¤–éƒ¨è¯´æœè¡Œä¸ºä¹‹é—´çš„è”ç³»ï¼Œä¸ºæœªæ¥MASçš„å®‰å…¨ã€é²æ£’æ€§åŠè®¾è®¡æä¾›äº†å…¨æ–°çš„è§£é‡Šæ¡†æ¶å’Œå…³é”®å¯ç¤ºã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Work in progress",
      "pdf_url": "https://arxiv.org/pdf/2509.21054v1",
      "published_date": "2025-09-25 12:03:10 UTC",
      "updated_date": "2025-09-25 12:03:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:19:24.085756+00:00"
    },
    {
      "arxiv_id": "2509.21050v1",
      "title": "GeoRef: Referring Expressions in Geometry via Task Formulation, Synthetic Supervision, and Reinforced MLLM-based Solutions",
      "title_zh": "GeoRefï¼šåŸºäºä»»åŠ¡å½¢å¼åŒ–ã€åˆæˆç›‘ç£ä¸å¼ºåŒ–å¤šæ¨¡æ€å¤§æ¨¡å‹æ–¹æ¡ˆçš„å‡ ä½•æŒ‡ä»£è¡¨è¾¾",
      "authors": [
        "Bing Liu",
        "Wenqiang Yv",
        "Xuzheng Yang",
        "Shichang Wang",
        "Junzhuo Liu",
        "Peng Wang",
        "Guoqing Wang",
        "Yang Yang",
        "Heng Tao Shen"
      ],
      "abstract": "AI-driven geometric problem solving is a complex vision-language task that requires accurate diagram interpretation, mathematical reasoning, and robust cross-modal grounding. A foundational yet underexplored capability for this task is the ability to identify and interpret geometric elements based on natural language queries. To address this, we introduce the task of Referring Expression Comprehension (REC) for geometric problems, which evaluates whether models can localize points, shapes, and spatial relations in diagrams in response to textual prompts. We present GeoRef, a benchmark dataset constructed from existing geometric problem corpora, featuring diverse, high-quality annotations and queries. Due to the lack of annotated data for this task, we generate a large-scale synthetic training dataset using a structured geometric formal language, enabling broad coverage of geometric concepts and facilitating model adaptation. We explore two fine-tuning approaches: Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO). Our results show that GRPO significantly outperforms SFT by better aligning model behavior with task-specific rewards. Furthermore, we propose a verify-and-regenerate mechanism that detects incorrect predictions and re-infers answers using contextual reasoning history, further boosting accuracy. Notably, even state-of-the-art Multimodal Large Language Models (MLLMs) struggle with this task, underscoring the necessity of explicitly evaluating and strengthening geometric grounding as a prerequisite for robust geometric problem solving. Moreover, models trained on GeoRef demonstrate measurable improvements on downstream geometric reasoning tasks, highlighting the broader value of REC as a foundation for multimodal mathematical understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å‡ ä½•é—®é¢˜æ±‚è§£ä¸­çš„è·¨æ¨¡æ€å¯¹é½æŒ‘æˆ˜ï¼Œå¼•å…¥äº†å‡ ä½•é¢†åŸŸçš„Referring Expression Comprehension (REC) ä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨å‡ ä½•å›¾å½¢ä¸­å®šä½ç‚¹ã€å½¢çŠ¶åŠç©ºé—´å…³ç³»çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æ„å»ºäº†åŒ…å«é«˜è´¨é‡æ ‡æ³¨çš„GeoRefåŸºå‡†æ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨ç»“æ„åŒ–å‡ ä½•å½¢å¼è¯­è¨€ç”Ÿæˆå¤§è§„æ¨¡åˆæˆæ•°æ®ä»¥è¾…åŠ©æ¨¡å‹è®­ç»ƒã€‚ç ”ç©¶å¯¹æ¯”äº†Supervised Fine-Tuning (SFT) å’Œ Group Relative Policy Optimization (GRPO) ä¸¤ç§å¾®è°ƒç­–ç•¥ï¼Œå‘ç°GRPOèƒ½æ›´æœ‰æ•ˆåœ°æå‡æ¨¡å‹æ€§èƒ½ï¼Œç»“åˆæ–°æå‡ºçš„éªŒè¯ä¸é‡æ–°ç”Ÿæˆ (verify-and-regenerate) æœºåˆ¶å¯è¿›ä¸€æ­¥å¢å¼ºé¢„æµ‹ç²¾åº¦ã€‚å®éªŒæ­ç¤ºäº†ç°æœ‰Multimodal Large Language Models (MLLMs) åœ¨å‡ ä½•æŒ‡ä»£ç†è§£ä¸Šçš„ä¸è¶³ï¼Œå¹¶è¯å®äº†åœ¨GeoRefä¸Šè®­ç»ƒçš„æ¨¡å‹èƒ½æ˜¾è‘—æå‡ä¸‹æ¸¸å‡ ä½•æ¨ç†ä»»åŠ¡çš„è¡¨ç°ï¼Œä¸ºå¤šæ¨¡æ€æ•°å­¦ç†è§£å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21050v1",
      "published_date": "2025-09-25 12:00:52 UTC",
      "updated_date": "2025-09-25 12:00:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:19:34.292692+00:00"
    },
    {
      "arxiv_id": "2509.21044v1",
      "title": "Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs",
      "title_zh": "å¼ºåŒ–å­¦ä¹ å¾®è°ƒå¢å¼ºäº† LLMs å†…éƒ¨ç”µè·¯çš„æ¿€æ´»å¼ºåº¦ä¸å¤šæ ·æ€§",
      "authors": [
        "Honglin Zhang",
        "Qianyue Hao",
        "Fengli Xu",
        "Yong Li"
      ],
      "abstract": "Large language models (LLMs) acquire extensive prior knowledge through large-scale pretraining and can be further enhanced via supervised fine-tuning (SFT) or reinforcement learning (RL)-based post-training. A growing body of evidence has shown that RL fine-tuning improves the capability of LLMs beyond what SFT alone achieves. However, the underlying mechanisms why RL fine-tuning is able to enhance the capability of various LLMs with distinct intrinsic characteristics remain underexplored. In this study, we draw inspiration from prior work on edge attribution patching (EAP) to investigate the internal differences of LLMs before and after RL fine-tuning. Our analysis across multiple model families shows two robust effects of online RL post-training: (i) an overall increase in activation intensity, indicating that more internal pathways are engaged and their signals become stronger, and (ii) greater diversity in activation patterns, reflected by higher entropy and less concentrated edge distributions. These changes suggest that RL reshapes information flow to be both more redundant and more flexible, which may explain its advantage in generalization. Notably, models fine-tuned with Direct Preference Optimization (DPO) deviate from these trends, exhibiting substantially weaker or inconsistent internal changes compared to PPO- and GRPO-based training. Together, our findings provide a unified view of how RL fine-tuning systematically alters the internal circuitry of LLMs and highlight the methodological distinctions between online RL and preference-based approaches. Our code is open source at https://anonymous.4open.science/r/llm_rl_probing_analysis-F673.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å¾®è°ƒç›¸æ¯”äºç›‘ç£å¾®è°ƒ(SFT)æ›´èƒ½æå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)èƒ½åŠ›çš„å†…åœ¨æœºåˆ¶ã€‚ç ”ç©¶è€…å€Ÿé‰´äº†è¾¹ç¼˜å½’å› è¡¥ä¸(Edge Attribution Patching)æŠ€æœ¯ï¼Œåˆ†æäº†å¤šä¸ªæ¨¡å‹ç³»åˆ—åœ¨RLå¾®è°ƒå‰åçš„å†…éƒ¨ç”µè·¯å·®å¼‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨çº¿RLåè®­ç»ƒäº§ç”Ÿäº†ä¸¤ä¸ªæ˜¾è‘—æ•ˆæœï¼šä¸€æ˜¯å¢å¼ºäº†æ¿€æ´»å¼ºåº¦(Activation Intensity)ï¼Œä½¿æ›´å¤šå†…éƒ¨è·¯å¾„è¢«æ¿€æ´»ä¸”ä¿¡å·å¼ºåº¦æå‡ï¼›äºŒæ˜¯å¢åŠ äº†æ¿€æ´»æ¨¡å¼çš„å¤šæ ·æ€§(Diversity)ï¼Œè¡¨ç°ä¸ºæ›´é«˜çš„ç†µå’Œæ›´åˆ†æ•£çš„è¾¹ç¼˜åˆ†å¸ƒã€‚è¿™äº›å˜åŒ–è¡¨æ˜RLé‡å¡‘äº†æ¨¡å‹çš„ä¿¡æ¯æµï¼Œä½¿å…¶æ›´å…·å†—ä½™æ€§å’Œçµæ´»æ€§ï¼Œè¿™è§£é‡Šäº†å…¶åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–(DPO)å¾®è°ƒçš„æ¨¡å‹åœ¨å†…éƒ¨å˜åŒ–ä¸Šæ˜æ˜¾å¼±äºåŸºäºPPOå’ŒGRPOçš„åœ¨çº¿RLè®­ç»ƒã€‚è¯¥ç ”ç©¶ä¸ºç†è§£RLå¾®è°ƒå¦‚ä½•ç³»ç»Ÿæ€§æ”¹å˜LLMså†…éƒ¨ç”µè·¯æä¾›äº†ç»Ÿä¸€è§†è§’ï¼Œå¹¶æ­ç¤ºäº†ä¸åŒåè®­ç»ƒæ–¹æ³•ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21044v1",
      "published_date": "2025-09-25 11:51:05 UTC",
      "updated_date": "2025-09-25 11:51:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:19:33.994842+00:00"
    },
    {
      "arxiv_id": "2509.21043v5",
      "title": "Combinatorial Creativity: A New Frontier in Generalization Abilities",
      "title_zh": "ç»„åˆå¼åˆ›é€ åŠ›ï¼šæ³›åŒ–èƒ½åŠ›çš„æ–°å‰æ²¿",
      "authors": [
        "Samuel Schapiro",
        "Sumuk Shashidhar",
        "Alexi Gladstone",
        "Jonah Black",
        "Royce Moon",
        "Dilek Hakkani-Tur",
        "Lav R. Varshney"
      ],
      "abstract": "Artificial intelligence (AI) systems, and Large Language Models (LLMs) in particular, are increasingly employed for creative tasks like scientific idea generation, constituting a form of generalization from training data unaddressed by existing conceptual frameworks. Despite its similarities to compositional generalization (CG), combinatorial creativity (CC) is an open-ended ability. Instead of evaluating for accuracy or correctness against fixed targets, which would contradict the open-ended nature of CC, we propose a theoretical framework and algorithmic task for evaluating outputs by their degrees of novelty and utility. From here, we make several important empirical contributions: (1) We obtain the first insights into the scaling behavior of creativity for LLMs. (2) We discover that, for fixed compute budgets, there exist optimal model depths and widths for creative ability. (3) We find that the ideation-execution gap, whereby LLMs excel at generating novel scientific ideas but struggle to ensure their practical feasibility, may be explained by a more fundamental novelty-utility tradeoff characteristic of creativity algorithms in general. Though our findings persist up to the 100M scale, frontier models today are well into the billions of parameters. Therefore, our conceptual framework and empirical findings can best serve as a starting point for understanding and improving the creativity of frontier-size models today, as we begin to bridge the gap between human and machine intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç»„åˆåˆ›é€ åŠ›(Combinatorial Creativity)ï¼Œå°†å…¶å®šä¹‰ä¸ºä¸€ç§ç°æœ‰æ¡†æ¶å°šæœªæ¶µç›–çš„å¼€æ”¾å¼æ³›åŒ–èƒ½åŠ›ã€‚é’ˆå¯¹æ­¤ç±»ä»»åŠ¡éš¾ä»¥ç”¨å‡†ç¡®ç‡è¡¡é‡çš„ç‰¹æ€§ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåŸºäºæ–°é¢–æ€§(novelty)å’Œå®ç”¨æ€§(utility)çš„ç†è®ºæ¡†æ¶åŠç®—æ³•ä»»åŠ¡ï¼Œç”¨äºè¯„ä¼°AIç³»ç»Ÿçš„äº§å‡ºã€‚ç ”ç©¶é¦–æ¬¡æ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨åˆ›é€ åŠ›æ–¹é¢çš„ç¼©æ”¾æ³•åˆ™(scaling behavior)ï¼Œå¹¶å‘ç°åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹å­˜åœ¨æœ€ä¼˜çš„æ¨¡å‹æ·±åº¦å’Œå®½åº¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ·±å…¥åˆ†æäº†â€œæ„æ€-æ‰§è¡Œå·®è·â€(ideation-execution gap)ï¼ŒæŒ‡å‡ºå…¶æœ¬è´¨æ˜¯åˆ›é€ åŠ›ç®—æ³•ä¸­æ™®éå­˜åœ¨çš„æ–°é¢–æ€§ä¸å®ç”¨æ€§æƒè¡¡(novelty-utility tradeoff)ã€‚è¿™äº›å®è¯å‘ç°ä¸ºç†è§£å’Œæå‡å‰æ²¿è§„æ¨¡æ¨¡å‹çš„åˆ›é€ åŠ›ä»¥åŠç¼©å°äººæœºæ™ºèƒ½å·®è·æä¾›äº†é‡è¦èµ·ç‚¹ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint. The first two authors contributed equally",
      "pdf_url": "https://arxiv.org/pdf/2509.21043v5",
      "published_date": "2025-09-25 11:48:37 UTC",
      "updated_date": "2026-01-02 01:20:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:19:45.860325+00:00"
    },
    {
      "arxiv_id": "2509.21040v1",
      "title": "Generative AI for FFRDCs",
      "title_zh": "é¢å‘ FFRDCs çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½",
      "authors": [
        "Arun S. Maiya"
      ],
      "abstract": "Federally funded research and development centers (FFRDCs) face text-heavy workloads, from policy documents to scientific and engineering papers, that are slow to analyze manually. We show how large language models can accelerate summarization, classification, extraction, and sense-making with only a few input-output examples. To enable use in sensitive government contexts, we apply OnPrem$.$LLM, an open-source framework for secure and flexible application of generative AI. Case studies on defense policy documents and scientific corpora, including the National Defense Authorization Act (NDAA) and National Science Foundation (NSF) Awards, demonstrate how this approach enhances oversight and strategic analysis while maintaining auditability and data sovereignty.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) åœ¨è”é‚¦èµ„åŠ©ç ”å‘ä¸­å¿ƒ (FFRDCs) ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Models) è§£å†³æ”¿ç­–æ–‡ä»¶ä¸ç§‘ç ”è®ºæ–‡ç­‰é‡æ–‡æœ¬åˆ†æé€Ÿåº¦æ…¢çš„æŒ‘æˆ˜ã€‚ç ”ç©¶é€šè¿‡å¼€æºæ¡†æ¶ OnPrem.LLM å®ç°äº†å®‰å…¨ä¸”çµæ´»çš„ AI éƒ¨ç½²ï¼Œç‰¹åˆ«å¼ºè°ƒäº†åœ¨æ•æ„Ÿæ”¿åºœèƒŒæ™¯ä¸‹çš„å®¡è®¡æ€§ä¸æ•°æ®ä¸»æƒã€‚é€šè¿‡å¯¹å›½é˜²æˆæƒæ³•æ¡ˆ (NDAA) å’Œç¾å›½å›½å®¶ç§‘å­¦åŸºé‡‘ä¼š (NSF) å¥–é¡¹ç­‰æ¡ˆä¾‹çš„å®è¯åˆ†æï¼Œè¯æ˜äº†è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆåŠ é€Ÿæ‘˜è¦ç”Ÿæˆã€æ–‡æœ¬åˆ†ç±»ä¸æˆ˜ç•¥åˆ†æã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨æå‡ FFRDCs ç›‘ç®¡èƒ½åŠ›ä¸æˆ˜ç•¥æ´å¯ŸåŠ›çš„åŒæ—¶ï¼Œç¡®ä¿äº†å¤„ç†æ•æ„Ÿæ•°æ®æ—¶çš„å®‰å…¨æ€§ï¼Œä¸ºé«˜åº¦æ•æ„Ÿé¢†åŸŸçš„ AI åº”ç”¨æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "4",
      "pdf_url": "https://arxiv.org/pdf/2509.21040v1",
      "published_date": "2025-09-25 11:45:39 UTC",
      "updated_date": "2025-09-25 11:45:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:19:56.066082+00:00"
    },
    {
      "arxiv_id": "2509.21035v1",
      "title": "CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable Context Engineering",
      "title_zh": "CLAUSEï¼šåŸºäºåŠ¨æ€å¯å­¦ä¹ ä¸Šä¸‹æ–‡å·¥ç¨‹çš„æ™ºèƒ½ä½“åŒ–ç¥ç»ç¬¦å·çŸ¥è¯†å›¾è°±æ¨ç†",
      "authors": [
        "Yang Zhao",
        "Chengxiao Dai",
        "Wei Zhuo",
        "Yue Xiu",
        "Dusit Niyato"
      ],
      "abstract": "Knowledge graphs provide structured context for multi-hop question answering, but deployed systems must balance answer accuracy with strict latency and cost targets while preserving provenance. Static k-hop expansions and \"think-longer\" prompting often over-retrieve, inflate context, and yield unpredictable runtime. We introduce CLAUSE, an agentic three-agent neuro-symbolic framework that treats context construction as a sequential decision process over knowledge graphs, deciding what to expand, which paths to follow or backtrack, what evidence to keep, and when to stop. Latency (interaction steps) and prompt cost (selected tokens) are exposed as user-specified budgets or prices, allowing per-query adaptation to trade-offs among accuracy, latency, and cost without retraining. CLAUSE employs the proposed Lagrangian-Constrained Multi-Agent Proximal Policy Optimization (LC-MAPPO) algorithm to coordinate three agents: Subgraph Architect, Path Navigator, and Context Curator, so that subgraph construction, reasoning-path discovery, and evidence selection are jointly optimized under per-query resource budgets on edge edits, interaction steps, and selected tokens. Across HotpotQA, MetaQA, and FactKG, CLAUSE yields higher EM@1 while reducing subgraph growth and end-to-end latency at equal or lower token budgets. On MetaQA-2-hop, relative to the strongest RAG baseline (GraphRAG), CLAUSE achieves +39.3 EM@1 with 18.6% lower latency and 40.9% lower edge growth. The resulting contexts are compact, provenance-preserving, and deliver predictable performance under deployment constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CLAUSEï¼Œä¸€ä¸ªåŸºäºæ™ºèƒ½ä½“(agentic)çš„ä¸‰æ™ºèƒ½ä½“ç¥ç»ç¬¦å·(neuro-symbolic)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åŠ¨æ€å¯å­¦ä¹ çš„ä¸Šä¸‹æ–‡å·¥ç¨‹è§£å†³çŸ¥è¯†å›¾è°±(Knowledge Graph)æ¨ç†ä¸­å¤šè·³é—®ç­”(multi-hop QA)çš„å‡†ç¡®ç‡ã€å»¶è¿Ÿå’Œæˆæœ¬å¹³è¡¡é—®é¢˜ã€‚CLAUSE å°†ä¸Šä¸‹æ–‡æ„å»ºè§†ä¸ºçŸ¥è¯†å›¾è°±ä¸Šçš„é¡ºåºå†³ç­–è¿‡ç¨‹ï¼Œé€šè¿‡å¼•å…¥ Lagrangian-Constrained Multi-Agent Proximal Policy Optimization (LC-MAPPO) ç®—æ³•åè°ƒ Subgraph Architectã€Path Navigator å’Œ Context Curator ä¸‰ä¸ªæ™ºèƒ½ä½“ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨ç”¨æˆ·æŒ‡å®šçš„èµ„æºé¢„ç®—å†…ï¼Œè”åˆä¼˜åŒ–å­å›¾æ„å»ºã€æ¨ç†è·¯å¾„å‘ç°å’Œè¯æ®é€‰æ‹©ï¼Œå®ç°å¯¹å‡†ç¡®ç‡ã€å»¶è¿Ÿå’Œæˆæœ¬çš„çµæ´»æƒè¡¡ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ HotpotQAã€MetaQA å’Œ FactKG æ•°æ®é›†ä¸Šï¼ŒCLAUSE åœ¨åŒç­‰æˆ–æ›´ä½çš„ token é¢„ç®—ä¸‹æ˜¾è‘—æå‡äº† EM@1 å‡†ç¡®ç‡ï¼Œå¹¶æœ‰æ•ˆé™ä½äº†ç«¯åˆ°ç«¯å»¶è¿Ÿã€‚åœ¨ MetaQA-2-hop ä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäº GraphRAG åŸºçº¿ï¼ŒCLAUSE åœ¨å‡å°‘ 18.6% å»¶è¿Ÿçš„åŒæ—¶å®ç°äº† +39.3 çš„ EM@1 å¢é•¿ã€‚è¯¥æ–¹æ³•ç”Ÿæˆçš„ä¸Šä¸‹æ–‡ç´§å‡‘ä¸”ä¿ç•™äº†æº¯æºä¿¡æ¯(provenance-preserving)ï¼Œä¸ºå—é™éƒ¨ç½²ç¯å¢ƒä¸‹çš„çŸ¥è¯†å›¾è°±æ¨ç†æä¾›äº†é«˜æ•ˆä¸”å¯é¢„æµ‹çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21035v1",
      "published_date": "2025-09-25 11:43:08 UTC",
      "updated_date": "2025-09-25 11:43:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:19:56.661457+00:00"
    },
    {
      "arxiv_id": "2509.21033v1",
      "title": "SupCLAP: Controlling Optimization Trajectory Drift in Audio-Text Contrastive Learning with Support Vector Regularization",
      "title_zh": "SupCLAPï¼šåˆ©ç”¨æ”¯æŒå‘é‡æ­£åˆ™åŒ–æ§åˆ¶éŸ³é¢‘-æ–‡æœ¬å¯¹æ¯”å­¦ä¹ ä¸­çš„ä¼˜åŒ–è½¨è¿¹æ¼‚ç§»",
      "authors": [
        "Jiehui Luo",
        "Yuguo Yin",
        "Yuxin Xie",
        "Jinghan Ru",
        "Xianwei Zhuang",
        "Minghua He",
        "Aofan Liu",
        "Zihan Xiong",
        "Dongchao Yang"
      ],
      "abstract": "Contrastive language-audio pretraining, which aims to unify multimodal representations in a shared embedding space, serves as a cornerstone for building a wide range of applications, from cross-modal retrieval to cutting-edge multimodal large language models. However, we find that the perpendicular component of the pushing force from negative samples in contrastive learning is a double-edged sword: it contains rich supplementary information from negative samples, yet its unconstrained nature causes optimization trajectory drift and training instability. To address this, we propose Support Vector Regularization (SVR), a method that introduces an auxiliary support vector to control this perpendicular component, aiming to harness its rich information while mitigating the associated trajectory drift. The efficacy of SVR is critically governed by its semantic radius, for which we explore two unsupervised modeling strategies: direct parameterization and an adaptive radius predictor module enhanced with constraints to improve its predicting accuracy. Extensive experimental results demonstrate that our method surpasses widely used baselines like InfoNCE and SigLIP loss across classification, monolingual retrieval, and multilingual retrieval on standard audio-text datasets. Both the theoretical analysis and the experimental results on optimizing trajectory drift validate the correctness and effectiveness of our SVR method.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SupCLAPï¼Œæ—¨åœ¨è§£å†³éŸ³é¢‘-æ–‡æœ¬å¯¹æ¯”å­¦ä¹ (Contrastive language-audio pretraining)ä¸­è´Ÿæ ·æœ¬æ¨åŠ›çš„å‚ç›´åˆ†é‡æ‰€å¯¼è‡´çš„ä¼˜åŒ–è½¨è¿¹æ¼‚ç§»(optimization trajectory drift)å’Œè®­ç»ƒä¸ç¨³å®šé—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…è®¾è®¡äº†æ”¯æŒå‘é‡æ­£åˆ™åŒ–(Support Vector Regularization, SVR)æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥è¾…åŠ©æ”¯æŒå‘é‡æ¥ç²¾ç¡®æ§åˆ¶è¯¥å‚ç›´åˆ†é‡ï¼Œåœ¨åˆ©ç”¨è´Ÿæ ·æœ¬ä¸°å¯Œä¿¡æ¯çš„åŒæ—¶ç¼“è§£è½¨è¿¹æ¼‚ç§»ã€‚é’ˆå¯¹SVRä¸­çš„æ ¸å¿ƒå‚æ•°è¯­ä¹‰åŠå¾„(semantic radius)ï¼Œç ”ç©¶è¿›ä¸€æ­¥æ¢ç´¢äº†ç›´æ¥å‚æ•°åŒ–å’Œè‡ªé€‚åº”åŠå¾„é¢„æµ‹å™¨ä¸¤ç§æ— ç›‘ç£å»ºæ¨¡ç­–ç•¥ï¼Œä»¥æå‡é¢„æµ‹å‡†ç¡®æ€§ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†ç±»ã€å•è¯­åŠå¤šè¯­æ£€ç´¢ä»»åŠ¡ä¸Šå‡æ˜¾è‘—ä¼˜äºInfoNCEå’ŒSigLIPç­‰ä¸»æµåŸºçº¿æ¨¡å‹ã€‚ç†è®ºåˆ†æä¸å®éªŒæ•°æ®å…±åŒéªŒè¯äº†SVRåœ¨ç¨³å®šä¼˜åŒ–è½¨è¿¹åŠæå‡å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21033v1",
      "published_date": "2025-09-25 11:40:20 UTC",
      "updated_date": "2025-09-25 11:40:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:19:47.483912+00:00"
    },
    {
      "arxiv_id": "2509.25240v1",
      "title": "HAMMER: Hamiltonian Curiosity Augmented Large Language Model Reinforcement",
      "title_zh": "HAMMERï¼šå“ˆå¯†é¡¿å¥½å¥‡å¿ƒå¢å¼ºçš„å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–",
      "authors": [
        "Ming Yang",
        "Xiaofan Li",
        "Zhiyuan Ma",
        "Dengliang Shi",
        "Jintao Du",
        "Yu Cheng",
        "Weiguo Zheng"
      ],
      "abstract": "Recent curriculum reinforcement learning for large language models (LLMs) typically rely on difficulty-based annotations for data filtering and ordering. However, such methods suffer from local optimization, where continual training on simple samples in the early steps can cause the policy to lose its exploration. We propose a novel schema, namely Hamiltonian curiosity augmented large language model reinforcement (HAMMER), that transfers diversity metrics, commonly used in dataset evaluation, into the dynamic reinforcement learning procedure, where training samples are ordered via a minimum-semantic Hamiltonian path making the initial training retrain more exploration. From a theoretical perspective of generalization bounds, diversity-driven ordering facilitates stable convergence. Empirical evaluations indicate that HAMMER stimulates model \"curiosity\" and consistently achieves a 3% to 4% average accuracy gain across diverse inference benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HAMMER (Hamiltonian curiosity augmented large language model reinforcement) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ ä¸­å› è¿‡åº¦ä¾èµ–éš¾åº¦æ’åºè€Œå¯¼è‡´çš„å±€éƒ¨æœ€ä¼˜å’Œæ¢ç´¢èƒ½åŠ›ä¸§å¤±é—®é¢˜ã€‚HAMMER åˆ›æ–°æ€§åœ°å°†æ•°æ®é›†è¯„ä¼°ä¸­çš„å¤šæ ·æ€§æŒ‡æ ‡ (diversity metrics) è½¬åŒ–ä¸ºåŠ¨æ€å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œé€šè¿‡æœ€å°è¯­ä¹‰å“ˆå¯†é¡¿è·¯å¾„ (minimum-semantic Hamiltonian path) å¯¹è®­ç»ƒæ ·æœ¬è¿›è¡Œæ’åºã€‚è¿™ç§æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆæ¿€å‘æ¨¡å‹çš„â€œå¥½å¥‡å¿ƒâ€ï¼Œç¡®ä¿æ¨¡å‹åœ¨åˆå§‹è®­ç»ƒé˜¶æ®µä¿ç•™æ›´å¼ºçš„æ¢ç´¢ç©ºé—´ã€‚ä»æ³›åŒ–ç•Œ (generalization bounds) çš„ç†è®ºè§’åº¦çœ‹ï¼Œè¿™ç§å¤šæ ·æ€§é©±åŠ¨çš„æ ·æœ¬æ’åºç­–ç•¥æœ‰åŠ©äºå®ç°æ›´ç¨³å®šçš„æ¨¡å‹æ”¶æ•›ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒHAMMER åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆèƒ½å¸¦æ¥ 3% è‡³ 4% çš„å¹³å‡å‡†ç¡®ç‡æå‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¢å¼ºå¤§è¯­è¨€æ¨¡å‹æ€§èƒ½æ–¹é¢çš„æ˜¾è‘—æ•ˆæœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 7 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.25240v1",
      "published_date": "2025-09-25 11:38:16 UTC",
      "updated_date": "2025-09-25 11:38:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:19:54.582174+00:00"
    },
    {
      "arxiv_id": "2509.21028v1",
      "title": "Who Gets Cited Most? Benchmarking Long-Context Language Models on Scientific Articles",
      "title_zh": "è°è¢«å¼•ç”¨æœ€å¤šï¼ŸåŸºäºå­¦æœ¯è®ºæ–‡çš„é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•",
      "authors": [
        "Miao Li",
        "Alexander Gurung",
        "Irina Saparina",
        "Mirella Lapata"
      ],
      "abstract": "This paper introduces SciTrek, a novel question-answering benchmark designed to evaluate the long-context reasoning capabilities of large language models (LLMs) using scientific articles. Current long-context benchmarks often rely on non-scientific texts, focus on simple information retrieval tasks, or employ artificial contexts. SciTrek addresses these limitations by proposing complex questions that require information aggregation and synthesis across multiple full-text scientific articles. Questions and their ground-truth answers are automatically generated by formulating them as SQL queries over a database constructed from article metadata (titles, authors, and references). The SQL operations provide explicit, verifiable reasoning steps for fine-grained error analysis, and the construction process scales to contexts up to 1M tokens with minimal supervision. Extensive experiments on a diverse set of open-weight and proprietary LLMs demonstrate that SciTrek poses a significant challenge as the context length increases, with supervised fine-tuning and reinforcement learning offering only limited gains. Our analysis reveals systematic shortcomings in models' abilities to perform basic numerical operations and accurately locate specific information in long contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SciTrekï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ç§‘å­¦æ–‡çŒ®ä¸­é•¿ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›çš„åˆ›æ–°é—®ç­”åŸºå‡†ã€‚ä¸ä¼ ç»ŸåŸºå‡†ä¸åŒï¼ŒSciTrek åˆ©ç”¨åŸºäºæ–‡ç« å…ƒæ•°æ®æ„å»ºçš„ SQL æŸ¥è¯¢æ¥è‡ªåŠ¨ç”Ÿæˆå¤æ‚é—®é¢˜ï¼Œè¦æ±‚æ¨¡å‹è·¨å¤šç¯‡å…¨æ–‡è¿›è¡Œä¿¡æ¯èšåˆä¸ç»¼åˆï¼Œæœ€é«˜å¯æ”¯æŒ 1M tokens çš„ä¸Šä¸‹æ–‡è§„æ¨¡ã€‚é€šè¿‡ SQL æ“ä½œæä¾›çš„å¯éªŒè¯æ¨ç†æ­¥éª¤ï¼Œè¯¥åŸºå‡†èƒ½å¤Ÿå®ç°ç»†ç²’åº¦çš„é”™è¯¯åˆ†æã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œéšç€ä¸Šä¸‹æ–‡é•¿åº¦å¢åŠ ï¼ŒSciTrek å¯¹å¼€æºå’Œé—­æºæ¨¡å‹å‡æ„æˆäº†ä¸¥å³»æŒ‘æˆ˜ï¼Œä¸”ç›‘ç£å¾®è°ƒ (SFT) å’Œå¼ºåŒ–å­¦ä¹  (RL) çš„æ”¹è¿›æ•ˆæœæœ‰é™ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ç¯å¢ƒä¸‹æ‰§è¡ŒåŸºç¡€æ•°å€¼è¿ç®—ä»¥åŠç²¾ç¡®å®šä½ç‰¹å®šä¿¡æ¯æ–¹é¢çš„ç³»ç»Ÿæ€§çŸ­æ¿ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "31 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.21028v1",
      "published_date": "2025-09-25 11:36:09 UTC",
      "updated_date": "2025-09-25 11:36:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:20:08.183660+00:00"
    },
    {
      "arxiv_id": "2509.21021v1",
      "title": "Efficient Ensemble Conditional Independence Test Framework for Causal Discovery",
      "title_zh": "é¢å‘å› æœå‘ç°çš„é«˜æ•ˆé›†æˆæ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒæ¡†æ¶",
      "authors": [
        "Zhengkang Guan",
        "Kun Kuang"
      ],
      "abstract": "Constraint-based causal discovery relies on numerous conditional independence tests (CITs), but its practical applicability is severely constrained by the prohibitive computational cost, especially as CITs themselves have high time complexity with respect to the sample size. To address this key bottleneck, we introduce the Ensemble Conditional Independence Test (E-CIT), a general and plug-and-play framework. E-CIT operates on an intuitive divide-and-aggregate strategy: it partitions the data into subsets, applies a given base CIT independently to each subset, and aggregates the resulting p-values using a novel method grounded in the properties of stable distributions. This framework reduces the computational complexity of a base CIT to linear in the sample size when the subset size is fixed. Moreover, our tailored p-value combination method offers theoretical consistency guarantees under mild conditions on the subtests. Experimental results demonstrate that E-CIT not only significantly reduces the computational burden of CITs and causal discovery but also achieves competitive performance. Notably, it exhibits an improvement in complex testing scenarios, particularly on real-world datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºçº¦æŸçš„å› æœå‘ç°ä¸­æ¡ä»¶ç‹¬ç«‹æ€§æµ‹è¯• (Conditional Independence Tests, CITs) è®¡ç®—æˆæœ¬è¿‡é«˜çš„æ ¸å¿ƒç“¶é¢ˆï¼Œæå‡ºäº† Ensemble Conditional Independence Test (E-CIT) é€šç”¨æ’ä»¶å¼æ¡†æ¶ã€‚E-CIT é‡‡ç”¨ç›´è§‚çš„â€œåˆ†è€Œèšåˆâ€ç­–ç•¥ï¼Œå°†æ•°æ®åˆ’åˆ†ä¸ºå¤šä¸ªå­é›†åç‹¬ç«‹æ‰§è¡ŒåŸºç¡€ CITï¼Œå¹¶åˆ©ç”¨ä¸€ç§åŸºäºç¨³å®šåˆ†å¸ƒ (stable distributions) å±æ€§çš„æ–°å‹æ–¹æ³•èšåˆ p-valuesã€‚è¯¥æ¡†æ¶æˆåŠŸå°† CIT çš„è®¡ç®—å¤æ‚åº¦é™ä½è‡³ä¸æ ·æœ¬é‡å‘ˆçº¿æ€§å…³ç³»ï¼Œå¹¶åœ¨æ¸©å’Œæ¡ä»¶ä¸‹ä¸ºå­æµ‹è¯•æä¾›äº†ä¸€è‡´æ€§ç†è®ºä¿è¯ã€‚å®éªŒè¯æ˜ï¼ŒE-CIT åœ¨å¤§å¹…å‰Šå‡è®¡ç®—æ”¯å‡ºçš„åŒæ—¶ä¿æŒäº†æå…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨å¤„ç†çœŸå®ä¸–ç•Œæ•°æ®é›†çš„å¤æ‚æµ‹è¯•åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†å¤§è§„æ¨¡å› æœå‘ç°çš„å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21021v1",
      "published_date": "2025-09-25 11:31:16 UTC",
      "updated_date": "2025-09-25 11:31:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:20:15.589726+00:00"
    },
    {
      "arxiv_id": "2509.25239v1",
      "title": "A Formal Comparison Between Chain-of-Thought and Latent Thought",
      "title_zh": "æ€ç»´é“¾ä¸éšå¼æ€ç»´çš„å½¢å¼åŒ–æ¯”è¾ƒ",
      "authors": [
        "Kevin Xu",
        "Issei Sato"
      ],
      "abstract": "Chain-of-Thought (CoT) elicits reasoning in large language models by explicitly generating intermediate steps in natural language. In contrast, Latent Thought in looped models operates directly in the continuous latent space, enabling computation beyond discrete linguistic representations. While both approaches exploit iterative computation, their comparative capabilities remain underexplored. In this work, we present a formal analysis showing that Latent Thought in Looped Transformers enables parallel computation, which is more efficient than the inherently sequential process of CoT. In contrast, CoT leverages stochastic decoding to approximate solutions to problems where exact computation is intractable. These separations suggest the tasks for which depth-driven recursion is more suitable, thereby offering practical guidance for choosing between reasoning paradigms. Code is available at https://github.com/kevin671/cot-vs-loop.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ Chain-of-Thought (CoT) ä¸å¾ªç¯æ¨¡å‹ä¸­çš„ Latent Thought è¿›è¡Œäº†æ­£å¼çš„æ¯”è¾ƒåˆ†æã€‚CoT é€šè¿‡åœ¨è‡ªç„¶è¯­è¨€ä¸­æ˜¾å¼ç”Ÿæˆä¸­é—´æ­¥éª¤æ¥è§¦å‘å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†ï¼Œè€Œ Latent Thought åˆ™ç›´æ¥åœ¨è¿ç»­çš„æ½œç©ºé—´ï¼ˆlatent spaceï¼‰ä¸­è¿è¡Œï¼Œèƒ½å¤Ÿå®ç°è¶…å‡ºç¦»æ•£è¯­è¨€è¡¨ç¤ºçš„è®¡ç®—èƒ½åŠ›ã€‚ç ”ç©¶é€šè¿‡å½¢å¼åŒ–åˆ†ææŒ‡å‡ºï¼Œå¾ªç¯ Transformer (Looped Transformers) ä¸­çš„ Latent Thought æ”¯æŒå¹¶è¡Œè®¡ç®—ï¼Œåœ¨æ•ˆç‡ä¸Šä¼˜äº CoT å›ºæœ‰çš„ä¸²è¡Œè¿‡ç¨‹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒCoT åˆ©ç”¨éšæœºè§£ç ï¼ˆstochastic decodingï¼‰æ¥ä¸ºç²¾ç¡®è®¡ç®—éš¾ä»¥å¤„ç†çš„é—®é¢˜æä¾›è¿‘ä¼¼è§£ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†ä¸åŒæ¨ç†èŒƒå¼çš„èƒ½åŠ›åˆ†ç¦»ï¼Œä¸ºæ·±åº¦é©±åŠ¨é€’å½’ï¼ˆdepth-driven recursionï¼‰çš„é€‚ç”¨ä»»åŠ¡æä¾›äº†å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25239v1",
      "published_date": "2025-09-25 11:27:52 UTC",
      "updated_date": "2025-09-25 11:27:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:20:20.664399+00:00"
    },
    {
      "arxiv_id": "2509.21014v1",
      "title": "The Use of the Simplex Architecture to Enhance Safety in Deep-Learning-Powered Autonomous Systems",
      "title_zh": "åˆ©ç”¨ Simplex æ¶æ„æå‡æ·±åº¦å­¦ä¹ é©±åŠ¨çš„è‡ªä¸»ç³»ç»Ÿå®‰å…¨æ€§",
      "authors": [
        "Federico Nesti",
        "Niko Salamini",
        "Mauro Marinoni",
        "Giorgio Maria Cicero",
        "Gabriele Serra",
        "Alessandro Biondi",
        "Giorgio Buttazzo"
      ],
      "abstract": "Recently, the outstanding performance reached by neural networks in many tasks has led to their deployment in autonomous systems, such as robots and vehicles. However, neural networks are not yet trustworthy, being prone to different types of misbehavior, such as anomalous samples, distribution shifts, adversarial attacks, and other threats. Furthermore, frameworks for accelerating the inference of neural networks typically run on rich operating systems that are less predictable in terms of timing behavior and present larger surfaces for cyber-attacks.\n  To address these issues, this paper presents a software architecture for enhancing safety, security, and predictability levels of learning-based autonomous systems. It leverages two isolated execution domains, one dedicated to the execution of neural networks under a rich operating system, which is deemed not trustworthy, and one responsible for running safety-critical functions, possibly under a different operating system capable of handling real-time constraints.\n  Both domains are hosted on the same computing platform and isolated through a type-1 real-time hypervisor enabling fast and predictable inter-domain communication to exchange real-time data. The two domains cooperate to provide a fail-safe mechanism based on a safety monitor, which oversees the state of the system and switches to a simpler but safer backup module, hosted in the safety-critical domain, whenever its behavior is considered untrustworthy.\n  The effectiveness of the proposed architecture is illustrated by a set of experiments performed on two control systems: a Furuta pendulum and a rover. The results confirm the utility of the fall-back mechanism in preventing faults due to the learning component.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»ç½‘ç»œ(neural networks)åœ¨è‡ªä¸»ç³»ç»Ÿä¸­å­˜åœ¨çš„ä¸å¯é æ€§åŠè¿è¡Œç¯å¢ƒä¸å¯é¢„æµ‹ç­‰å®‰å…¨éšæ‚£ï¼Œæå‡ºäº†ä¸€ç§æ—¨åœ¨æå‡ç³»ç»Ÿ safetyã€security å’Œ predictability æ°´å¹³çš„è½¯ä»¶æ¶æ„ã€‚è¯¥æ¶æ„åˆ©ç”¨ type-1 real-time hypervisor åœ¨åŒä¸€ç¡¬ä»¶å¹³å°ä¸Šæ„å»ºäº†ä¸¤ä¸ªç›¸äº’éš”ç¦»çš„æ‰§è¡ŒåŸŸï¼Œåˆ†åˆ«ç”¨äºåœ¨å¯Œæ“ä½œç³»ç»Ÿä¸‹è¿è¡Œæ·±åº¦å­¦ä¹ æ¨¡å‹ä»¥åŠåœ¨å®æ—¶æ“ä½œç³»ç»Ÿä¸‹å¤„ç†å®‰å…¨å…³é”®åŠŸèƒ½ã€‚ç³»ç»Ÿæ ¸å¿ƒåŸºäº Simplex Architectureï¼Œé€šè¿‡å®‰å…¨ç›‘æ§å™¨(safety monitor)å®æ—¶ç›‘ç£ç³»ç»ŸçŠ¶æ€ï¼Œä¸€æ—¦å‘ç°ç¥ç»ç½‘ç»œçš„è¡Œä¸ºä¸å¯ä¿¡ï¼Œä¾¿ä¼šè‡ªåŠ¨åˆ‡æ¢è‡³å®‰å…¨åŸŸå†…çš„ç®€å•å¤‡ä»½æ¨¡å—(backup module)ã€‚å®éªŒåœ¨ Furuta pendulum å’Œ rover ä¸¤ä¸ªæ§åˆ¶ç³»ç»Ÿä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœè¯å®è¯¥æ¶æ„çš„å›é€€æœºåˆ¶(fall-back mechanism)èƒ½å¤Ÿæœ‰æ•ˆé˜²æ­¢å› å­¦ä¹ ç»„ä»¶å¤±æ•ˆè€Œå¯¼è‡´çš„ç³»ç»Ÿæ•…éšœã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†è‡ªä¸»ç³»ç»Ÿåœ¨åˆ©ç”¨æ·±åº¦å­¦ä¹ æå‡æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½å¤Ÿé€šè¿‡ç¡¬ä»¶éš”ç¦»å’Œç›‘æ§æ‰‹æ®µä¿éšœæ•´ä½“çš„è¿è¡Œå®‰å…¨ä¸å¯é æ€§ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21014v1",
      "published_date": "2025-09-25 11:20:47 UTC",
      "updated_date": "2025-09-25 11:20:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:20:25.084269+00:00"
    },
    {
      "arxiv_id": "2509.21013v2",
      "title": "Predicting LLM Reasoning Performance with Small Proxy Model",
      "title_zh": "åˆ©ç”¨å°å‹ä»£ç†æ¨¡å‹é¢„æµ‹å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½",
      "authors": [
        "Woosung Koh",
        "Juyoung Suk",
        "Sungjun Han",
        "Se-Young Yun",
        "Jamin Shin"
      ],
      "abstract": "Given the prohibitive cost of pre-training large language models, it is essential to leverage smaller proxy models to optimize datasets before scaling up. However, this approach becomes challenging for reasoning capabilities, which exhibit emergent behavior that only appear reliably at larger model sizes, often exceeding 7B parameters. To address this, we introduce rBridge, showing that small proxies ($\\leq$1B) can effectively predict large-model reasoning by aligning more closely with (1) the pre-training objective and (2) the target task. rBridge achieves this by weighting negative log-likelihood with task alignment, using reasoning traces from frontier models as gold labels. In our experiments, rBridge (i) reduces dataset ranking costs by over 100x relative to the best baseline, (ii) achieves the strongest correlation across six reasoning benchmarks at 1B to 32B scale, and (iii) zero-shot transfers predictive relationships across pre-training datasets at 1B to 7B scale. These findings indicate that rBridge offers a practical path for exploring reasoning-oriented pre-training at lower cost.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)é¢„è®­ç»ƒæˆæœ¬é«˜æ˜‚ä¸”å°è§„æ¨¡ä»£ç†æ¨¡å‹(proxy models)éš¾ä»¥é¢„æµ‹å¤§æ¨¡å‹æ¶Œç°æ¨ç†èƒ½åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†rBridgeæ–¹æ³•ã€‚rBridgeé€šè¿‡å°†å‚æ•°é‡å°äº1Bçš„å°æ¨¡å‹ä¸é¢„è®­ç»ƒç›®æ ‡åŠç›®æ ‡ä»»åŠ¡è¿›è¡Œæ›´ç´§å¯†çš„å¯¹é½ï¼Œå®ç°äº†å¯¹å¤§æ¨¡å‹æ¨ç†æ€§èƒ½çš„æœ‰æ•ˆé¢„æµ‹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å‰æ²¿æ¨¡å‹çš„æ¨ç†è¿¹(reasoning traces)ä½œä¸ºé‡‘æ ‡å‡†ï¼Œé€šè¿‡ä»»åŠ¡å¯¹é½åŠ æƒçš„è´Ÿå¯¹æ•°ä¼¼ç„¶(negative log-likelihood)æ¥ä¼˜åŒ–é¢„æµ‹è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒrBridgeåœ¨1Bè‡³32Bè§„æ¨¡çš„å…­é¡¹æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å‡å±•ç°å‡ºæå¼ºçš„ç›¸å…³æ€§ï¼Œå¹¶å°†æ•°æ®é›†æ’åçš„è®¡ç®—æˆæœ¬é™ä½äº†100å€ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å®ç°äº†åœ¨1Bè‡³7Bè§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬(zero-shot)é¢„æµ‹å…³ç³»è¿ç§»ã€‚è¿™äº›å‘ç°è¯æ˜äº†rBridgeä¸ºä½æˆæœ¬æ¢ç´¢ä»¥æ¨ç†ä¸ºå¯¼å‘çš„é¢„è®­ç»ƒè·¯å¾„æä¾›äº†åˆ‡å®å¯è¡Œçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Pre-print",
      "pdf_url": "https://arxiv.org/pdf/2509.21013v2",
      "published_date": "2025-09-25 11:20:38 UTC",
      "updated_date": "2025-09-30 11:21:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:20:24.183460+00:00"
    },
    {
      "arxiv_id": "2509.21012v2",
      "title": "Mechanism of Task-oriented Information Removal in In-context Learning",
      "title_zh": "ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­ä»»åŠ¡å¯¼å‘çš„ä¿¡æ¯å‰”é™¤æœºåˆ¶",
      "authors": [
        "Hakaze Cho",
        "Haolin Yang",
        "Gouki Minegishi",
        "Naoya Inoue"
      ],
      "abstract": "In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»ä¿¡æ¯ç§»é™¤(Information Removal)çš„æ–°è§†è§’æ¢è®¨äº†ä¸Šä¸‹æ–‡å­¦ä¹ (In-context Learning, ICL)çš„å†…åœ¨æœºåˆ¶ã€‚ä½œè€…å‘ç°ï¼Œåœ¨é›¶æ ·æœ¬(Zero-shot)åœºæ™¯ä¸‹ï¼Œè¯­è¨€æ¨¡å‹(LMs)å€¾å‘äºå°†æŸ¥è¯¢ç¼–ç ä¸ºåŒ…å«æ‰€æœ‰å¯èƒ½ä»»åŠ¡ä¿¡æ¯çš„éé€‰æ‹©æ€§è¡¨ç¤º(Non-selective Representations)ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•ä¸“æ³¨äºç›®æ ‡ä»»åŠ¡ä¸”å‡†ç¡®ç‡è¾ƒä½ã€‚é€šè¿‡ä½ç§©æ»¤æ³¢å™¨(Low-rank Filter)é€‰æ‹©æ€§åœ°ä»éšè—çŠ¶æ€ä¸­ç§»é™¤ç‰¹å®šä¿¡æ¯ï¼Œå¯ä»¥æœ‰æ•ˆå¼•å¯¼æ¨¡å‹æ‰§è¡Œç›®æ ‡ä»»åŠ¡ã€‚å®éªŒè§‚å¯Ÿè¡¨æ˜ï¼Œå°‘æ ·æœ¬(Few-shot) ICL å®é™…ä¸Šæ¨¡æ‹Ÿäº†è¿™ç§ä»»åŠ¡å¯¼å‘çš„ä¿¡æ¯ç§»é™¤è¿‡ç¨‹ï¼Œé€šè¿‡ç¤ºä¾‹ä»çº ç¼ çš„éé€‰æ‹©æ€§è¡¨ç¤ºä¸­å‰”é™¤å†—ä½™ä¿¡æ¯ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯†åˆ«äº†è´Ÿè´£è¯±å¯¼è¯¥ç§»é™¤æ“ä½œçš„å…³é”®æ³¨æ„åŠ›å¤´ï¼Œå¹¶å°†å…¶å‘½åä¸ºå»å™ªå¤´(Denoising Heads)ã€‚æ¶ˆèå®éªŒè¯æ˜ï¼Œé˜»æ–­å»å™ªå¤´çš„ä¿¡æ¯ç§»é™¤æ“ä½œä¼šå¯¼è‡´ ICL å‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ï¼Œè¯å®äº†ä¿¡æ¯ç§»é™¤æœºåˆ¶åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "87 pages, 90 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.21012v2",
      "published_date": "2025-09-25 11:18:09 UTC",
      "updated_date": "2025-11-26 08:37:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:20:23.258229+00:00"
    },
    {
      "arxiv_id": "2509.21011v1",
      "title": "Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools",
      "title_zh": "åŸºäº Model Context Protocol å·¥å…·çš„åŸºäºå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“è‡ªåŠ¨åŒ–çº¢é˜Ÿæµ‹è¯•",
      "authors": [
        "Ping He",
        "Changjiang Li",
        "Binbin Zhao",
        "Tianyu Du",
        "Shouling Ji"
      ],
      "abstract": "The remarkable capability of large language models (LLMs) has led to the wide application of LLM-based agents in various domains. To standardize interactions between LLM-based agents and their environments, model context protocol (MCP) tools have become the de facto standard and are now widely integrated into these agents. However, the incorporation of MCP tools introduces the risk of tool poisoning attacks, which can manipulate the behavior of LLM-based agents. Although previous studies have identified such vulnerabilities, their red teaming approaches have largely remained at the proof-of-concept stage, leaving the automatic and systematic red teaming of LLM-based agents under the MCP tool poisoning paradigm an open question. To bridge this gap, we propose AutoMalTool, an automated red teaming framework for LLM-based agents by generating malicious MCP tools. Our extensive evaluation shows that AutoMalTool effectively generates malicious MCP tools capable of manipulating the behavior of mainstream LLM-based agents while evading current detection mechanisms, thereby revealing new security risks in these agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æ™ºèƒ½ä½“åœ¨é›†æˆæ¨¡å‹ä¸Šä¸‹æ–‡åè®®(Model Context Protocol, MCP)å·¥å…·æ—¶é¢ä¸´çš„å·¥å…·æŠ•æ¯’æ”»å‡»(tool poisoning attacks)é£é™©ã€‚é’ˆå¯¹ç°æœ‰çº¢é˜Ÿæµ‹è¯•(red teaming)æ–¹æ³•ä¸»è¦åœç•™åœ¨æ¦‚å¿µéªŒè¯é˜¶æ®µä¸”ç¼ºä¹ç³»ç»Ÿæ€§çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†AutoMalToolï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡ç”Ÿæˆæ¶æ„MCPå·¥å…·æ¥å®ç°LLMæ™ºèƒ½ä½“è‡ªåŠ¨åŒ–çº¢é˜Ÿæµ‹è¯•çš„æ¡†æ¶ã€‚AutoMalToolèƒ½å¤Ÿæ¨¡æ‹Ÿæ¶æ„å·¥å…·å¯¹æ™ºèƒ½ä½“è¡Œä¸ºçš„æ“çºµï¼Œæ—¨åœ¨å¡«è¡¥è‡ªåŠ¨åŒ–å®‰å…¨è¯„ä¼°çš„ç©ºç™½ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„æ¶æ„å·¥å…·ä¸ä»…èƒ½æœ‰æ•ˆæ§åˆ¶ä¸»æµLLMæ™ºèƒ½ä½“çš„è¡Œä¸ºï¼Œè¿˜èƒ½æˆåŠŸè§„é¿ç°æœ‰çš„æ£€æµ‹æœºåˆ¶ã€‚è¿™é¡¹å·¥ä½œæ­ç¤ºäº†LLMæ™ºèƒ½ä½“åœ¨MCPç”Ÿæ€ç³»ç»Ÿä¸‹çš„æ–°å‹å®‰å…¨å¨èƒï¼Œå¹¶ä¸ºæ„å»ºæ›´å®‰å…¨çš„æ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21011v1",
      "published_date": "2025-09-25 11:14:38 UTC",
      "updated_date": "2025-09-25 11:14:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:20:42.757436+00:00"
    },
    {
      "arxiv_id": "2509.21010v1",
      "title": "ExMolRL: Phenotype-Target Joint Generation of De Novo Molecules via Multi-Objective Reinforcement Learning",
      "title_zh": "ExMolRLï¼šåŸºäºå¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ çš„è¡¨å‹-é¶ç‚¹è”åˆä»å¤´åˆ†å­ç”Ÿæˆ",
      "authors": [
        "Haotian Guo",
        "Hui Liu"
      ],
      "abstract": "The generation of high-quality candidate molecules remains a central challenge in AI-driven drug design. Current phenotype-based and target-based strategies each suffer limitations, either incurring high experimental costs or overlook system-level cellular responses. To bridge this gap, we propose ExMoIRL, a novel generative framework that synergistically integrates phenotypic and target-specific cues for de novo molecular generation. The phenotype-guided generator is first pretrained on expansive drug-induced transcriptional profiles and subsequently fine-tuned via multi-objective reinforcement learning (RL). Crucially, the reward function fuses docking affinity and drug-likeness scores, augmented with ranking loss, prior-likelihood regularization, and entropy maximization. The multi-objective RL steers the model toward chemotypes that are simultaneously potent, diverse, and aligned with the specified phenotypic effects. Extensive experiments demonstrate ExMoIRL's superior performance over state-of-the-art phenotype-based and target-based models across multiple well-characterized targets. Our generated molecules exhibit favorable drug-like properties, high target affinity, and inhibitory potency (IC50) against cancer cells. This unified framework showcases the synergistic potential of combining phenotype-guided and target-aware strategies, offering a more effective solution for de novo drug discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ExMolRLï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å°†è¡¨å‹(phenotype)ä¸é¶ç‚¹(target)ç‰¹å®šçº¿ç´¢ååŒæ•´åˆçš„æ–°å‹ä»å¤´åˆ†å­ç”Ÿæˆæ¡†æ¶ï¼Œä»¥è§£å†³ç°æœ‰è¯ç‰©è®¾è®¡ç­–ç•¥ä¸­å®éªŒæˆæœ¬é«˜æ˜‚æˆ–å¿½è§†ç»†èƒæ°´å¹³ååº”çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é¦–å…ˆåœ¨å¹¿æ³›çš„è¯ç‰©è¯±å¯¼è½¬å½•ç»„è°±ä¸Šå¯¹è¡¨å‹å¼•å¯¼çš„ç”Ÿæˆå™¨è¿›è¡Œé¢„è®­ç»ƒï¼Œéšåé€šè¿‡å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)è¿›è¡Œå¾®è°ƒã€‚å…¶æ ¸å¿ƒå¥–åŠ±å‡½æ•°èåˆäº†å¯¹æ¥äº²å’ŒåŠ›(docking affinity)å’Œè¯ç‰©ç›¸ä¼¼æ€§è¯„åˆ†ï¼Œå¹¶è¾…ä»¥æ’åºæŸå¤±ã€å…ˆéªŒä¼¼ç„¶æ­£åˆ™åŒ–å’Œç†µæœ€å¤§åŒ–æŠ€æœ¯ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆå…¼å…·é«˜æ•ˆåŠ›ã€å¤šæ ·æ€§ä¸”ç¬¦åˆç‰¹å®šè¡¨å‹æ•ˆåº”çš„åŒ–å­¦ç±»å‹ã€‚å®éªŒè¯æ˜ï¼ŒExMolRLåœ¨å¤šä¸ªé¶ç‚¹ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œç”Ÿæˆçš„åˆ†å­å±•ç°å‡ºè‰¯å¥½çš„è¯ç‰©ç›¸ä¼¼æ€§ã€é«˜é¶ç‚¹äº²å’ŒåŠ›ä»¥åŠä¼˜å¼‚çš„ç™Œç»†èƒæŠ‘åˆ¶æ•ˆåŠ›(IC50)ã€‚è¿™ä¸€ç»Ÿä¸€æ¡†æ¶å±•ç¤ºäº†è¡¨å‹å¼•å¯¼ä¸é¶ç‚¹æ„ŸçŸ¥ç­–ç•¥çš„ååŒæ½œåŠ›ï¼Œä¸ºä»å¤´è¯ç‰©å‘ç°æä¾›äº†ä¸€ç§æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21010v1",
      "published_date": "2025-09-25 11:13:24 UTC",
      "updated_date": "2025-09-25 11:13:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:20:38.557505+00:00"
    },
    {
      "arxiv_id": "2509.21007v1",
      "title": "Marching Neurons: Accurate Surface Extraction for Neural Implicit Shapes",
      "title_zh": "Marching Neuronsï¼šç¥ç»éšå¼å½¢çŠ¶çš„ç²¾ç¡®è¡¨é¢æå–",
      "authors": [
        "Christian Stippel",
        "Felix Mujkanovic",
        "Thomas LeimkÃ¼hler",
        "Pedro Hermosilla"
      ],
      "abstract": "Accurate surface geometry representation is crucial in 3D visual computing. Explicit representations, such as polygonal meshes, and implicit representations, like signed distance functions, each have distinct advantages, making efficient conversions between them increasingly important. Conventional surface extraction methods for implicit representations, such as the widely used Marching Cubes algorithm, rely on spatial decomposition and sampling, leading to inaccuracies due to fixed and limited resolution. We introduce a novel approach for analytically extracting surfaces from neural implicit functions. Our method operates natively in parallel and can navigate large neural architectures. By leveraging the fact that each neuron partitions the domain, we develop a depth-first traversal strategy to efficiently track the encoded surface. The resulting meshes faithfully capture the full geometric information from the network without ad-hoc spatial discretization, achieving unprecedented accuracy across diverse shapes and network architectures while maintaining competitive speed.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»éšå¼å½¢çŠ¶ï¼ˆNeural Implicit Shapesï¼‰åœ¨è¡¨é¢æå–è¿‡ç¨‹ä¸­çš„ç²¾åº¦ç“¶é¢ˆï¼ŒæŒ‡å‡ºäº† Marching Cubes ç­‰ä¼ ç»Ÿæ–¹æ³•å› ç©ºé—´ç¦»æ•£åŒ–å’Œå›ºå®šåˆ†è¾¨ç‡å¯¼è‡´çš„ç²¾åº¦æŸå¤±ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸º Marching Neurons çš„æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿä»ç¥ç»éšå¼å‡½æ•°ä¸­åˆ†æåŒ–åœ°ï¼ˆAnalyticallyï¼‰æå–è¡¨é¢ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç¥ç»å…ƒå¯¹å®šä¹‰åŸŸçš„åˆ’åˆ†ç‰¹æ€§ï¼Œå¼€å‘äº†ä¸€ç§æ·±åº¦ä¼˜å…ˆéå†ï¼ˆDepth-first traversalï¼‰ç­–ç•¥ï¼Œé€šè¿‡ç›´æ¥è¿½è¸ªç¼–ç è¡¨é¢æ¥é¿å…å¯¹ç©ºé—´åˆ†è¾¨ç‡çš„ä¾èµ–ã€‚Marching Neurons æ”¯æŒåŸç”Ÿå¹¶è¡ŒåŒ–å¤„ç†ï¼Œå¹¶å…·å¤‡å¤„ç†å¤§è§„æ¨¡ç¥ç»æ¶æ„çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§å½¢çŠ¶å’Œç½‘ç»œæ¶æ„ä¸Šå‡å®ç°äº†æé«˜çš„å‡ ä½•ç²¾åº¦ï¼ŒåŒæ—¶ä¿æŒäº†æå…·ç«äº‰åŠ›çš„è¿è¡Œé€Ÿåº¦ã€‚è¯¥ç ”ç©¶åœ¨ä¸ä¾èµ–ç‰¹å®šç©ºé—´ç¦»æ•£åŒ–çš„æƒ…å†µä¸‹ï¼Œå¿ å®åœ°æ•æ‰äº†ç½‘ç»œçš„å®Œæ•´å‡ ä½•ä¿¡æ¯ï¼Œä¸º 3D è§†è§‰è®¡ç®—ä¸­çš„é«˜è´¨é‡æ¨¡å‹è½¬æ¢æä¾›äº†å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "SIGGRAPH Asia 2025 (Journal Track)",
      "pdf_url": "https://arxiv.org/pdf/2509.21007v1",
      "published_date": "2025-09-25 11:06:42 UTC",
      "updated_date": "2025-09-25 11:06:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:20:37.444054+00:00"
    },
    {
      "arxiv_id": "2509.21006v1",
      "title": "AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation",
      "title_zh": "AnywhereVLAï¼šåŸºäºè¯­è¨€æ¡ä»¶çš„æ¢ç´¢ä¸ç§»åŠ¨æ“ä½œ",
      "authors": [
        "Konstantin Gubernatorov",
        "Artem Voronov",
        "Roman Voronov",
        "Sergei Pasynkov",
        "Stepan Perminov",
        "Ziang Guo",
        "Dzmitry Tsetserukou"
      ],
      "abstract": "We address natural language pick-and-place in unseen, unpredictable indoor environments with AnywhereVLA, a modular framework for mobile manipulation. A user text prompt serves as an entry point and is parsed into a structured task graph that conditions classical SLAM with LiDAR and cameras, metric semantic mapping, and a task-aware frontier exploration policy. An approach planner then selects visibility and reachability aware pre grasp base poses. For interaction, a compact SmolVLA manipulation head is fine tuned on platform pick and place trajectories for the SO-101 by TheRobotStudio, grounding local visual context and sub-goals into grasp and place proposals. The full system runs fully onboard on consumer-level hardware, with Jetson Orin NX for perception and VLA and an Intel NUC for SLAM, exploration, and control, sustaining real-time operation. We evaluated AnywhereVLA in a multi-room lab under static scenes and normal human motion. In this setting, the system achieves a $46\\%$ overall task success rate while maintaining throughput on embedded compute. By combining a classical stack with a fine-tuned VLA manipulation, the system inherits the reliability of geometry-based navigation with the agility and task generalization of language-conditioned manipulation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AnywhereVLAï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸å¯è§ä¸”ä¸å¯é¢„æµ‹å®¤å†…ç¯å¢ƒçš„ç§»åŠ¨æ“çºµæ¨¡å—åŒ–æ¡†æ¶ã€‚ç³»ç»Ÿå°†ç”¨æˆ·æ–‡æœ¬æç¤ºè§£æä¸ºç»“æ„åŒ–ä»»åŠ¡å›¾ï¼Œç”¨äºå¼•å¯¼ç»“åˆäº† LiDAR å’Œæ‘„åƒå¤´çš„ SLAMã€åº¦é‡è¯­ä¹‰æ˜ å°„ä»¥åŠä»»åŠ¡æ„ŸçŸ¥çš„å‰æ²¿æ¢ç´¢ç­–ç•¥ã€‚åœ¨äº¤äº’å±‚é¢ï¼Œç ”ç©¶è€…å¾®è°ƒäº†ç´§å‡‘çš„ SmolVLA æ“çºµå¤´ï¼Œå°†å±€éƒ¨è§†è§‰ä¸Šä¸‹æ–‡å’Œå­ç›®æ ‡è½¬åŒ–ä¸ºå…·ä½“çš„æŠ“å–ä¸æ”¾ç½®å»ºè®®ã€‚æ•´ä¸ªç³»ç»Ÿå®Œå…¨åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ Jetson Orin NX å’Œ Intel NUC ä¸Šè¿è¡Œï¼Œå®ç°äº†é«˜æ•ˆçš„æ¿è½½å®æ—¶å¤„ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAnywhereVLA åœ¨åŒ…å«åŠ¨æ€äººä½“è¿åŠ¨çš„å¤šæˆ¿é—´å®éªŒå®¤åœºæ™¯ä¸­è¾¾åˆ°äº† 46% çš„ä»»åŠ¡æˆåŠŸç‡ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ç»“åˆç»å…¸çš„å‡ ä½•å¯¼èˆªå †æ ˆä¸å¾®è°ƒçš„ VLA æ“çºµæ¨¡å‹ï¼ŒæˆåŠŸèåˆäº†å¯¼èˆªçš„å¯é æ€§ä¸è¯­è¨€é©±åŠ¨æ“çºµçš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21006v1",
      "published_date": "2025-09-25 11:04:44 UTC",
      "updated_date": "2025-09-25 11:04:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:20:40.367342+00:00"
    },
    {
      "arxiv_id": "2509.21002v1",
      "title": "Lossless Compression: A New Benchmark for Time Series Model Evaluation",
      "title_zh": "æ— æŸå‹ç¼©ï¼šæ—¶é—´åºåˆ—æ¨¡å‹è¯„ä¼°æ–°åŸºå‡†",
      "authors": [
        "Meng Wan",
        "Benxi Tian",
        "Jue Wang",
        "Cui Hui",
        "Ningming Nie",
        "Tiantian Liu",
        "Zongguo Wang",
        "Cao Rongqiang",
        "Peng Shi",
        "Yangang Wang"
      ],
      "abstract": "The evaluation of time series models has traditionally focused on four canonical tasks: forecasting, imputation, anomaly detection, and classification. While these tasks have driven significant progress, they primarily assess task-specific performance and do not rigorously measure whether a model captures the full generative distribution of the data. We introduce lossless compression as a new paradigm for evaluating time series models, grounded in Shannon's source coding theorem. This perspective establishes a direct equivalence between optimal compression length and the negative log-likelihood, providing a strict and unified information-theoretic criterion for modeling capacity. Then We define a standardized evaluation protocol and metrics. We further propose and open-source a comprehensive evaluation framework TSCom-Bench, which enables the rapid adaptation of time series models as backbones for lossless compression. Experiments across diverse datasets on state-of-the-art models, including TimeXer, iTransformer, and PatchTST, demonstrate that compression reveals distributional weaknesses overlooked by classic benchmarks. These findings position lossless compression as a principled task that complements and extends existing evaluation for time series modeling.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†å°†æ— æŸå‹ç¼©(Lossless Compression)ä½œä¸ºæ—¶é—´åºåˆ—æ¨¡å‹è¯„ä¼°çš„æ–°èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿé¢„æµ‹ã€æ’è¡¥å’Œå¼‚å¸¸æ£€æµ‹ç­‰ä»»åŠ¡æ— æ³•ä¸¥æ ¼è¡¡é‡æ¨¡å‹æ•è·å®Œæ•´æ•°æ®ç”Ÿæˆåˆ†å¸ƒçš„é—®é¢˜ã€‚è¯¥æ–¹æ³•æ¤æ ¹äºé¦™å†œä¿¡æºç¼–ç å®šç†(Shannon's source coding theorem)ï¼Œç¡®ç«‹äº†æœ€ä¼˜å‹ç¼©é•¿åº¦ä¸è´Ÿå¯¹æ•°ä¼¼ç„¶(Negative Log-Likelihood)ä¹‹é—´çš„ç›´æ¥ç­‰ä»·å…³ç³»ï¼Œä¸ºè¯„ä¼°æ¨¡å‹å®¹é‡æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„ä¿¡æ¯è®ºå‡†åˆ™ã€‚ç ”ç©¶å›¢é˜Ÿå®šä¹‰äº†æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ä¸æŒ‡æ ‡ï¼Œå¹¶å¼€æºäº†TSCom-Benchæ¡†æ¶ï¼Œæ”¯æŒå°†TimeXerã€iTransformerå’ŒPatchTSTç­‰å…ˆè¿›æ¨¡å‹å¿«é€Ÿé€‚é…ä¸ºæ— æŸå‹ç¼©çš„éª¨å¹²æ¶æ„ã€‚åœ¨å¤šç§æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œæ— æŸå‹ç¼©èƒ½å¤Ÿæ­ç¤ºä¼ ç»ŸåŸºå‡†æµ‹è¯•å®¹æ˜“å¿½ç•¥çš„åˆ†å¸ƒå»ºæ¨¡ç¼ºé™·(distributional weaknesses)ã€‚ä½œä¸ºä¸€ç§åŸåˆ™æ€§çš„è¯„ä¼°æ‰‹æ®µï¼Œè¯¥èŒƒå¼ä¸ä»…æœ‰æ•ˆè¡¥å……äº†ç°æœ‰çš„æ—¶é—´åºåˆ—æ¨¡å‹è¯„ä¼°ä½“ç³»ï¼Œä¹Ÿä¸ºè¡¡é‡æ¨¡å‹æ•æ‰å¤æ‚æ•°æ®ç‰¹å¾çš„èƒ½åŠ›æä¾›äº†æ›´ä¸¥è°¨çš„è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.21002v1",
      "published_date": "2025-09-25 10:52:48 UTC",
      "updated_date": "2025-09-25 10:52:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:21:09.883729+00:00"
    },
    {
      "arxiv_id": "2509.20998v1",
      "title": "CORE: Full-Path Evaluation of LLM Agents Beyond Final State",
      "title_zh": "COREï¼šè¶…è¶Šæœ€ç»ˆçŠ¶æ€çš„ LLM æ™ºèƒ½ä½“å…¨è·¯å¾„è¯„ä¼°",
      "authors": [
        "Panagiotis Michelakis",
        "Yiannis Hadjiyiannis",
        "Dimitrios Stamoulis"
      ],
      "abstract": "Evaluating AI agents that solve real-world tasks through function-call sequences remains an open challenge. Existing agentic benchmarks often reduce evaluation to a binary judgment of the final state, overlooking critical aspects such as safety, efficiency, and intermediate correctness. We propose a framework based on deterministic finite automata (DFAs) that encodes tasks as sets of valid tool-use paths, enabling principled assessment of agent behavior in diverse world models. Building on this foundation, we introduce CORE, a suite of five metrics, namely Path Correctness, Path Correctness - Kendall's tau Composite, Prefix Criticality, Harmful-Call Rate, and Efficiency, that quantify alignment with expected execution patterns. Across diverse worlds, our method reveals important performance differences between agents that would otherwise appear equivalent under traditional final-state evaluation schemes.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºå½“å‰çš„ AI æ™ºèƒ½ä½“ (AI agents) è¯„ä¼°åŸºå‡†é€šå¸¸ä»…å…³æ³¨æœ€ç»ˆçŠ¶æ€ (final state) çš„äºŒå…ƒåˆ¤å®šï¼Œä»è€Œå¿½ç•¥äº†å®‰å…¨æ€§ã€æ•ˆç‡ä»¥åŠä¸­é—´è¿‡ç¨‹çš„æ­£ç¡®æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåŸºäºç¡®å®šæ€§æœ‰é™è‡ªåŠ¨æœº (DFAs) çš„è¯„ä¼°æ¡†æ¶ï¼Œå°†ä»»åŠ¡ç¼–ç ä¸ºä¸€ç³»åˆ—æœ‰æ•ˆçš„å·¥å…·ä½¿ç”¨è·¯å¾„ï¼Œå®ç°åœ¨å¤šæ ·åŒ–ä¸–ç•Œæ¨¡å‹ä¸­å¯¹æ™ºèƒ½ä½“è¡Œä¸ºçš„è§„èŒƒåŒ–è¯„ä¼°ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶è€…å¼•å…¥äº† CORE åº¦é‡æŒ‡æ ‡å¥—ä»¶ï¼ŒåŒ…å«è·¯å¾„æ­£ç¡®æ€§ (Path Correctness)ã€è·¯å¾„æ­£ç¡®æ€§ - è‚¯å¾·å°”ç­‰çº§ç›¸å…³ç³»æ•°å¤åˆæŒ‡æ ‡ (Path Correctness - Kendall's tau Composite)ã€å‰ç¼€å…³é”®åº¦ (Prefix Criticality)ã€æœ‰å®³è°ƒç”¨ç‡ (Harmful-Call Rate) å’Œæ•ˆç‡ (Efficiency) äº”ä¸ªç»´åº¦ï¼Œç”¨äºé‡åŒ–æ™ºèƒ½ä½“ä¸é¢„æœŸæ‰§è¡Œæ¨¡å¼çš„å¯¹é½ç¨‹åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ­ç¤ºæ™ºèƒ½ä½“ä¹‹é—´åœ¨æ‰§è¡Œè·¯å¾„ä¸Šçš„é‡è¦æ€§èƒ½å·®å¼‚ï¼Œè€Œè¿™äº›å·®å¼‚åœ¨ä¼ ç»Ÿçš„æœ€ç»ˆçŠ¶æ€è¯„ä¼°æ–¹æ¡ˆä¸‹å¾€å¾€æ— æ³•è¢«å¯Ÿè§‰ã€‚è¯¥ç ”ç©¶ä¸ºè¶…è¶Šç»“æœå¯¼å‘çš„ LLM æ™ºèƒ½ä½“å…¨è·¯å¾„è¯„ä¼°æä¾›äº†ç³»ç»Ÿæ€§çš„è¡¡é‡æ ‡å‡†ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted: LAW 2025 Workshop NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.20998v1",
      "published_date": "2025-09-25 10:49:35 UTC",
      "updated_date": "2025-09-25 10:49:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:20:58.980683+00:00"
    },
    {
      "arxiv_id": "2509.20997v1",
      "title": "Binary Autoencoder for Mechanistic Interpretability of Large Language Models",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹æœºç†å¯è§£é‡Šæ€§çš„äºŒè¿›åˆ¶è‡ªç¼–ç å™¨",
      "authors": [
        "Hakaze Cho",
        "Haolin Yang",
        "Brian M. Kurkoski",
        "Naoya Inoue"
      ],
      "abstract": "Existing works are dedicated to untangling atomized numerical components (features) from the hidden states of Large Language Models (LLMs) for interpreting their mechanism. However, they typically rely on autoencoders constrained by some implicit training-time regularization on single training instances (i.e., $L_1$ normalization, top-k function, etc.), without an explicit guarantee of global sparsity among instances, causing a large amount of dense (simultaneously inactive) features, harming the feature sparsity and atomization. In this paper, we propose a novel autoencoder variant that enforces minimal entropy on minibatches of hidden activations, thereby promoting feature independence and sparsity across instances. For efficient entropy calculation, we discretize the hidden activations to 1-bit via a step function and apply gradient estimation to enable backpropagation, so that we term it as Binary Autoencoder (BAE) and empirically demonstrate two major applications: (1) Feature set entropy calculation. Entropy can be reliably estimated on binary hidden activations, which we empirically evaluate and leverage to characterize the inference dynamics of LLMs and In-context Learning. (2) Feature untangling. Similar to typical methods, BAE can extract atomized features from LLM's hidden states. To robustly evaluate such feature extraction capability, we refine traditional feature-interpretation methods to avoid unreliable handling of numerical tokens, and show that BAE avoids dense features while producing the largest number of interpretable ones among baselines, which confirms the effectiveness of BAE serving as a feature extractor.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¤§è¯­è¨€æ¨¡å‹ (LLMs) æœºæ¢°å¯è§£é‡Šæ€§ç ”ç©¶ä¸­è‡ªåŠ¨ç¼–ç å™¨ç”±äºç¼ºä¹å…¨å±€ç¨€ç–æ€§ä¿è¯å¯¼è‡´äº§ç”Ÿå¤§é‡å¯†é›†ç‰¹å¾çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„ Binary Autoencoder (BAE)ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨å°æ‰¹é‡éšè—æ¿€æ´»ä¸Šå¼ºåˆ¶æ‰§è¡Œæœ€å°ç†µï¼Œä¿ƒè¿›äº†ç‰¹å¾åœ¨å®ä¾‹é—´çš„ç‹¬ç«‹æ€§ä¸ç¨€ç–æ€§ã€‚ä¸ºäº†æå‡è®¡ç®—æ•ˆç‡ï¼ŒBAE ä½¿ç”¨é˜¶è·ƒå‡½æ•°å°†éšè—æ¿€æ´»ç¦»æ•£åŒ–ä¸º 1-bitï¼Œå¹¶ç»“åˆæ¢¯åº¦ä¼°è®¡æŠ€æœ¯å®ç°æ¨¡å‹è®­ç»ƒã€‚ç ”ç©¶å±•ç¤ºäº† BAE çš„ä¸¤ä¸ªæ ¸å¿ƒåº”ç”¨ï¼ŒåŒ…æ‹¬åˆ©ç”¨ç‰¹å¾é›†ç†µæ¥åˆ»ç”» LLMs çš„æ¨ç†åŠ¨æ€ä¸ In-context Learning è¡¨ç°ï¼Œä»¥åŠä»éšè—çŠ¶æ€ä¸­æå–åŸå­åŒ–ç‰¹å¾ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒBAE èƒ½å¤Ÿæœ‰æ•ˆé¿å…äº§ç”Ÿå¯†é›†ç‰¹å¾ï¼Œå¹¶åœ¨å¯¹æ¯”åŸºå‡†æ¨¡å‹ä¸­ç”Ÿæˆäº†æ•°é‡æœ€å¤šçš„å¯è§£é‡Šç‰¹å¾ã€‚é€šè¿‡æ”¹è¿›ä¼ ç»Ÿçš„ç‰¹å¾è¯„ä¼°æ–¹æ³•ï¼Œè¯¥ç ”ç©¶è¿›ä¸€æ­¥ç¡®è®¤äº† BAE ä½œä¸ºç‰¹å¾æå–å™¨åœ¨å¤„ç†æ•°å€¼æ ‡è®°æ—¶çš„é²æ£’æ€§ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "36 pages, 41 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.20997v1",
      "published_date": "2025-09-25 10:48:48 UTC",
      "updated_date": "2025-09-25 10:48:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:21:04.975366+00:00"
    },
    {
      "arxiv_id": "2509.20991v1",
      "title": "Fast-SEnSeI: Lightweight Sensor-Independent Cloud Masking for On-board Multispectral Sensors",
      "title_zh": "Fast-SEnSeIï¼šé¢å‘æ˜Ÿè½½å¤šå…‰è°±ä¼ æ„Ÿå™¨çš„è½»é‡åŒ–ä¼ æ„Ÿå™¨æ— å…³äº‘æ©è†œ",
      "authors": [
        "Jan KnÄ›Å¾Ã­k",
        "JonÃ¡Å¡ Herec",
        "Rado PitoÅˆÃ¡k"
      ],
      "abstract": "Cloud segmentation is a critical preprocessing step for many Earth observation tasks, yet most models are tightly coupled to specific sensor configurations and rely on ground-based processing. In this work, we propose Fast-SEnSeI, a lightweight, sensor-independent encoder module that enables flexible, on-board cloud segmentation across multispectral sensors with varying band configurations. Building upon SEnSeI-v2, Fast-SEnSeI integrates an improved spectral descriptor, lightweight architecture, and robust padding-band handling. It accepts arbitrary combinations of spectral bands and their wavelengths, producing fixed-size feature maps that feed into a compact, quantized segmentation model based on a modified U-Net. The module runs efficiently on embedded CPUs using Apache TVM, while the segmentation model is deployed on FPGA, forming a CPU-FPGA hybrid pipeline suitable for space-qualified hardware. Evaluations on Sentinel-2 and Landsat 8 datasets demonstrate accurate segmentation across diverse input configurations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Fast-SEnSeIï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ä¸”ä¸ä¼ æ„Ÿå™¨æ— å…³(sensor-independent)çš„ç¼–ç å™¨æ¨¡å—ï¼Œæ—¨åœ¨å®ç°é’ˆå¯¹å¤šå…‰è°±ä¼ æ„Ÿå™¨çš„çµæ´»æ˜Ÿä¸Šäº‘æ©è†œ(cloud masking)ä»»åŠ¡ã€‚è¯¥æ¨¡å—åœ¨SEnSeI-v2çš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼Œé›†æˆäº†ä¼˜åŒ–çš„å…‰è°±æè¿°ç¬¦(spectral descriptor)ã€è½»é‡åŒ–æ¶æ„ä»¥åŠç¨³å¥çš„å¡«å……æ³¢æ®µå¤„ç†æœºåˆ¶ã€‚Fast-SEnSeIèƒ½å¤Ÿæ¥å—ä»»æ„å…‰è°±æ³¢æ®µåŠå…¶æ³¢é•¿çš„ç»„åˆï¼Œå¹¶ç”Ÿæˆå›ºå®šå°ºå¯¸çš„ç‰¹å¾å›¾ï¼Œéšåå°†å…¶è¾“å…¥åˆ°åŸºäºæ”¹è¿›U-Netçš„ç´§å‡‘å‹é‡åŒ–åˆ†å‰²æ¨¡å‹ä¸­ã€‚åœ¨ç¡¬ä»¶å®ç°æ–¹é¢ï¼Œè¯¥æ¨¡å—é€šè¿‡Apache TVMåœ¨åµŒå…¥å¼CPUä¸Šé«˜æ•ˆè¿è¡Œï¼Œè€Œåˆ†å‰²æ¨¡å‹åˆ™éƒ¨ç½²äºFPGAï¼Œå½¢æˆäº†ä¸€å¥—é€‚ç”¨äºå®‡èˆªçº§ç¡¬ä»¶çš„CPU-FPGAæ··åˆæµæ°´çº¿ã€‚é’ˆå¯¹Sentinel-2å’ŒLandsat 8æ•°æ®é›†çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§è¾“å…¥é…ç½®ä¸‹å‡èƒ½å®ç°å‡†ç¡®çš„äº‘åˆ†å‰²ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "cs.CV",
      "comment": "This is a preprint of a paper accepted for the EDHPC 2025 Conference",
      "pdf_url": "https://arxiv.org/pdf/2509.20991v1",
      "published_date": "2025-09-25 10:40:31 UTC",
      "updated_date": "2025-09-25 10:40:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:21:05.275256+00:00"
    },
    {
      "arxiv_id": "2509.25238v1",
      "title": "PALADIN: Self-Correcting Language Model Agents to Cure Tool-Failure Cases",
      "title_zh": "PALADINï¼šä¿®å¤å·¥å…·å¤±æ•ˆæ¡ˆä¾‹çš„è‡ªæ ¡æ­£è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“",
      "authors": [
        "Sri Vatsa Vuddanti",
        "Aarav Shah",
        "Satwik Kumar Chittiprolu",
        "Tony Song",
        "Sunishchal Dev",
        "Kevin Zhu",
        "Maheep Chaudhary"
      ],
      "abstract": "Tool-augmented language agents frequently fail in real-world deployment due to tool malfunctions--timeouts, API exceptions, or inconsistent outputs--triggering cascading reasoning errors and task abandonment. Existing agent training pipelines optimize only for success trajectories, failing to expose models to the tool failures that dominate real-world usage. We propose \\textbf{PALADIN}, a generalizable framework for equipping language agents with robust failure recovery capabilities. PALADIN trains on 50,000+ recovery-annotated trajectories constructed via systematic failure injection and expert demonstrations on an enhanced ToolBench dataset. Training uses LoRA-based fine-tuning to retain base capabilities while injecting recovery competence. At inference, PALADIN detects execution-time errors and retrieves the most similar case from a curated bank of 55+ failure exemplars aligned with ToolScan's taxonomy, then executes the corresponding recovery action. This approach generalizes to novel failures beyond the training distribution, retaining 95.2\\% recovery performance on unseen tool APIs. Evaluation across PaladinEval and ToolReflectEval demonstrates consistent improvements in Recovery Rate (RR), Task Success Rate (TSR), Catastrophic Success Rate (CSR), and Efficiency Score (ES). PALADIN improves RR from 32.76% to 89.68% (+57% relative) over ToolBench and outperforms the strongest baseline CRITIC (76.34%) by +13.3%. Against vanilla agents, PALADIN achieves 89.86\\% RR (+66% relative improvement from 23.75%). These results establish PALADIN as an effective method for building fault-tolerant agents capable of robust recovery in real-world tool environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PALADINï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å·¥å…·å¢å¼ºå‹æ™ºèƒ½ä½“åœ¨å®é™…éƒ¨ç½²ä¸­å› å·¥å…·æ•…éšœï¼ˆå¦‚è¶…æ—¶ã€API exceptionsæˆ–è¾“å‡ºä¸ä¸€è‡´ï¼‰è€Œå¯¼è‡´çš„æ¨ç†é”™è¯¯å’Œä»»åŠ¡ä¸­æ–­é—®é¢˜ã€‚PALADINé€šè¿‡ç³»ç»Ÿæ€§çš„failure injectionå’Œä¸“å®¶æ¼”ç¤ºï¼Œåœ¨å¢å¼ºçš„ToolBenchæ•°æ®é›†ä¸Šæ„å»ºäº†è¶…è¿‡50,000æ¡recovery-annotatedè½¨è¿¹ï¼Œå¹¶åˆ©ç”¨LoRA-based fine-tuningåœ¨ä¿ç•™åŸºç¡€èƒ½åŠ›çš„åŒæ—¶æ³¨å…¥æ•…éšœæ¢å¤èƒœä»»åŠ›ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒPALADINèƒ½å¤Ÿå®æ—¶æ£€æµ‹é”™è¯¯å¹¶ä»åŸºäºToolScanåˆ†ç±»ä½“ç³»çš„exemplar bankä¸­æ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹ï¼Œä»è€Œæ‰§è¡Œç›¸åº”çš„æ¢å¤åŠ¨ä½œã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒPALADINåœ¨PaladinEvalå’ŒToolReflectEvalä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œå°†Recovery Rate (RR)ä»32.76%å¤§å¹…æå‡è‡³89.68%ï¼Œæ˜¾è‘—ä¼˜äºCRITICç­‰å¼ºåŸºå‡†æ¨¡å‹ã€‚è¯¥æ¡†æ¶åœ¨é¢å¯¹æœªè§è¿‡çš„å·¥å…·APIæ—¶ä»èƒ½ä¿æŒ95.2%çš„æ¢å¤æ€§èƒ½ï¼Œä¸ºæ„å»ºèƒ½å¤Ÿåœ¨çœŸå®ç¯å¢ƒä¸­é²æ£’è¿è¡Œçš„fault-tolerantæ™ºèƒ½ä½“æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25238v1",
      "published_date": "2025-09-25 10:37:30 UTC",
      "updated_date": "2025-09-25 10:37:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:21:25.188245+00:00"
    },
    {
      "arxiv_id": "2509.20989v1",
      "title": "Rejuvenating Cross-Entropy Loss in Knowledge Distillation for Recommender Systems",
      "title_zh": "æ¨èç³»ç»ŸçŸ¥è¯†è’¸é¦ä¸­äº¤å‰ç†µæŸå¤±çš„ç„•æ–°",
      "authors": [
        "Zhangchi Zhu",
        "Wei Zhang"
      ],
      "abstract": "This paper analyzes Cross-Entropy (CE) loss in knowledge distillation (KD) for recommender systems. KD for recommender systems targets at distilling rankings, especially among items most likely to be preferred, and can only be computed on a small subset of items. Considering these features, we reveal the connection between CE loss and NDCG in the field of KD. We prove that when performing KD on an item subset, minimizing CE loss maximizes the lower bound of NDCG, only if an assumption of closure is satisfied. It requires that the item subset consists of the student's top items. However, this contradicts our goal of distilling rankings of the teacher's top items. We empirically demonstrate the vast gap between these two kinds of top items. To bridge the gap between our goal and theoretical support, we propose Rejuvenated Cross-Entropy for Knowledge Distillation (RCE-KD). It splits the top items given by the teacher into two subsets based on whether they are highly ranked by the student. For the subset that defies the condition, a sampling strategy is devised to use teacher-student collaboration to approximate our assumption of closure. We also combine the losses on the two subsets adaptively. Extensive experiments demonstrate the effectiveness of our method. Our code is available at https://anonymous.4open.science/r/RCE-KD.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ¨èç³»ç»ŸçŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillation, KDï¼‰ä¸­äº¤å‰ç†µæŸå¤±ï¼ˆCross-Entropy lossï¼‰çš„ç†è®ºå±€é™ï¼Œæ­ç¤ºäº†å…¶ä¸NDCGæŒ‡æ ‡çš„å†…åœ¨è”ç³»ã€‚ä½œè€…æŒ‡å‡ºï¼Œåªæœ‰å½“è’¸é¦å­é›†æ»¡è¶³åŒ…å«å­¦ç”Ÿç«¯Topé¡¹ç›®çš„â€œé—­åˆæ€§â€å‡è®¾æ—¶ï¼Œæœ€å°åŒ–CE lossæ–¹å¯æœ€å¤§åŒ–NDCGçš„ä¸‹ç•Œï¼Œè€Œè¿™ä¸è’¸é¦è€å¸ˆç«¯Topé¡¹ç›®çš„å®é™…éœ€æ±‚å­˜åœ¨æ˜¾è‘—åå·®ã€‚ä¸ºè§£å†³æ­¤çŸ›ç›¾ï¼Œè®ºæ–‡æå‡ºäº†RCE-KDæ¡†æ¶ï¼Œé€šè¿‡æ ¹æ®å­¦ç”Ÿæ’åè¡¨ç°å°†è€å¸ˆçš„Topé¡¹ç›®åˆ’åˆ†ä¸ºä¸¤ç±»å­é›†ï¼Œå¹¶é’ˆå¯¹æ€§åœ°å¼•å…¥æ•™å¸ˆ-å­¦ç”Ÿåä½œé‡‡æ ·ç­–ç•¥ä»¥é€¼è¿‘é—­åˆæ€§å‡è®¾ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡è‡ªé€‚åº”æƒé‡ç»“åˆä¸åŒå­é›†çš„æŸå¤±ï¼Œä»è€Œåœ¨è’¸é¦è¿‡ç¨‹ä¸­å®ç°æ›´ç²¾å‡†çš„æ’åä¼ é€’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRCE-KDåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„KDæ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†æ¨èç³»ç»Ÿçš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20989v1",
      "published_date": "2025-09-25 10:31:59 UTC",
      "updated_date": "2025-09-25 10:31:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:21:36.684082+00:00"
    },
    {
      "arxiv_id": "2509.20988v1",
      "title": "AOT*: Efficient Synthesis Planning via LLM-Empowered AND-OR Tree Search",
      "title_zh": "AOT*ï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹èµ‹èƒ½ä¸æˆ–æ ‘æœç´¢çš„é«˜æ•ˆåˆæˆè§„åˆ’",
      "authors": [
        "Xiaozhuang Song",
        "Xuanhao Pan",
        "Xinjian Zhao",
        "Hangting Ye",
        "Shufei Zhang",
        "Jian Tang",
        "Tianshu Yu"
      ],
      "abstract": "Retrosynthesis planning enables the discovery of viable synthetic routes for target molecules, playing a crucial role in domains like drug discovery and materials design. Multi-step retrosynthetic planning remains computationally challenging due to exponential search spaces and inference costs. While Large Language Models (LLMs) demonstrate chemical reasoning capabilities, their application to synthesis planning faces constraints on efficiency and cost. To address these challenges, we introduce AOT*, a framework that transforms retrosynthetic planning by integrating LLM-generated chemical synthesis pathways with systematic AND-OR tree search. To this end, AOT* atomically maps the generated complete synthesis routes onto AND-OR tree components, with a mathematically sound design of reward assignment strategy and retrieval-based context engineering, thus enabling LLMs to efficiently navigate in the chemical space. Experimental evaluation on multiple synthesis benchmarks demonstrates that AOT* achieves SOTA performance with significantly improved search efficiency. AOT* exhibits competitive solve rates using 3-5$\\times$ fewer iterations than existing LLM-based approaches, with the efficiency advantage becoming more pronounced on complex molecular targets.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AOT* æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é›†æˆå¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ç”Ÿæˆçš„åŒ–å­¦åˆæˆè·¯å¾„ä¸ç³»ç»ŸåŒ–çš„ AND-OR tree searchï¼Œè§£å†³å¤šæ­¥é€†åˆæˆåˆ†æ (retrosynthesis planning) ä¸­æœç´¢ç©ºé—´å·¨å¤§ä¸”æ¨ç†æˆæœ¬é«˜æ˜‚çš„æŒ‘æˆ˜ã€‚AOT* å°† LLM ç”Ÿæˆçš„å®Œæ•´åˆæˆè·¯çº¿åŸå­åŒ–åœ°æ˜ å°„åˆ° AND-OR tree ç»„ä»¶ä¸Šï¼Œå¹¶ç»“åˆæ•°å­¦ä¸¥è°¨çš„å¥–åŠ±åˆ†é…ç­–ç•¥ (reward assignment strategy) å’ŒåŸºäºæ£€ç´¢çš„ä¸Šä¸‹æ–‡å·¥ç¨‹ (retrieval-based context engineering)ï¼Œå¼•å¯¼ LLMs é«˜æ•ˆæ¢ç´¢åŒ–å­¦ç©ºé—´ã€‚å®éªŒè¯„ä¼°è¯æ˜ AOT* åœ¨å¤šä¸ªåˆæˆåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œæœç´¢æ•ˆç‡æ˜¾è‘—æå‡ã€‚ç›¸æ¯”ç°æœ‰çš„åŸºäº LLM çš„æ–¹æ³•ï¼ŒAOT* åœ¨ä¿æŒé«˜æ±‚è§£ç‡çš„åŒæ—¶å‡å°‘äº† 3 åˆ° 5 å€çš„è¿­ä»£æ¬¡æ•°ï¼Œä¸”åœ¨å¤„ç†å¤æ‚åˆ†å­ç›®æ ‡æ—¶æ•ˆç‡ä¼˜åŠ¿æ›´ä¸ºçªå‡ºã€‚",
      "categories": [
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.AI",
      "comment": "34 pages, 21 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.20988v1",
      "published_date": "2025-09-25 10:30:37 UTC",
      "updated_date": "2025-09-25 10:30:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:21:56.187488+00:00"
    },
    {
      "arxiv_id": "2509.20986v2",
      "title": "SiNGER: A Clearer Voice Distills Vision Transformers Further",
      "title_zh": "SiNGERï¼šä»¥æ›´æ¸…æ™°çš„ä¿¡å·è¿›ä¸€æ­¥æ·±åŒ–è§†è§‰ Transformer çŸ¥è¯†è’¸é¦",
      "authors": [
        "Geunhyeok Yu",
        "Sunjae Jeong",
        "Yoonyoung Choi",
        "Jaeseung Kim",
        "Hyoseok Hwang"
      ],
      "abstract": "Vision Transformers are widely adopted as the backbone of vision foundation models, but they are known to produce high-norm artifacts that degrade representation quality. When knowledge distillation transfers these features to students, high-norm artifacts dominate the objective, so students overfit to artifacts and underweight informative signals, diminishing the gains from larger models. Prior work attempted to remove artifacts but encountered an inherent trade-off between artifact suppression and preserving informative signals from teachers. To address this, we introduce Singular Nullspace-Guided Energy Reallocation (SiNGER), a novel distillation framework that suppresses artifacts while preserving informative signals. The key idea is principled teacher feature refinement: during refinement, we leverage the nullspace-guided perturbation to preserve information while suppressing artifacts. Then, the refined teacher's features are distilled to a student. We implement this perturbation efficiently with a LoRA-based adapter that requires minimal structural modification. Extensive experiments show that \\oursname consistently improves student models, achieving state-of-the-art performance in multiple downstream tasks and producing clearer and more interpretable representations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Vision Transformers (ViTs) åœ¨çŸ¥è¯†è’¸é¦ (knowledge distillation) è¿‡ç¨‹ä¸­å› é«˜èŒƒæ•°ä¼ªå½± (high-norm artifacts) å¯¼è‡´å­¦ç”Ÿæ¨¡å‹è¿‡åº¦æ‹Ÿåˆä¸”å¿½ç•¥æœ‰æ•ˆä¿¡å·çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º SiNGER (Singular Nullspace-Guided Energy Reallocation) çš„æ–°å‹è’¸é¦æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡é›¶ç©ºé—´å¼•å¯¼æ‰°åŠ¨ (nullspace-guided perturbation) æŠ€æœ¯å¯¹æ•™å¸ˆæ¨¡å‹ç‰¹å¾è¿›è¡Œç²¾ç‚¼ï¼Œæ—¨åœ¨æœ‰æ•ˆæŠ‘åˆ¶ä¼ªå½±çš„åŒæ—¶æœ€å¤§ç¨‹åº¦ä¿ç•™æ ¸å¿ƒä¿¡æ¯ä¿¡å·ã€‚åœ¨å®ç°ä¸Šï¼ŒSiNGER é‡‡ç”¨äº†åŸºäº LoRA çš„é€‚é…å™¨ (adapter) ä»¥ç¡®ä¿é«˜æ•ˆçš„æ‰°åŠ¨å¤„ç†ï¼Œä¸”ä»…éœ€æå°çš„æ¨¡å‹ç»“æ„ä¿®æ”¹ã€‚å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSiNGER èƒ½å¤Ÿæ˜¾è‘—æå‡å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ï¼Œåœ¨å¤šé¡¹ä¸‹æ¸¸ä»»åŠ¡ä¸­è¾¾åˆ°äº† SOTA æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„ç‰¹å¾è¡¨ç¤ºæ›´åŠ æ¸…æ™°ä¸”å…·æœ‰æ›´å¼ºçš„å¯è§£é‡Šæ€§ (interpretable representations)ï¼ŒæˆåŠŸå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•åœ¨ä¼ªå½±æŠ‘åˆ¶ä¸ä¿¡å·ä¿ç•™ä¹‹é—´çš„æƒè¡¡éš¾é¢˜ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Main paper: 12 pages (including 3 pages of references), 6 figures, 6 tables. Appendix: 9 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.20986v2",
      "published_date": "2025-09-25 10:29:47 UTC",
      "updated_date": "2025-09-29 00:07:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:21:26.273003+00:00"
    },
    {
      "arxiv_id": "2509.20982v1",
      "title": "Analysis of instruction-based LLMs' capabilities to score and judge text-input problems in an academic setting",
      "title_zh": "å­¦æœ¯ç¯å¢ƒä¸‹åŸºäºæŒ‡ä»¤çš„å¤§è¯­è¨€æ¨¡å‹å¯¹æ–‡æœ¬é—®ç­”é¢˜çš„è¯„åˆ†ä¸è¯„åˆ¤èƒ½åŠ›åˆ†æ",
      "authors": [
        "Valeria Ramirez-Garcia",
        "David de-Fitero-Dominguez",
        "Antonio Garcia-Cabot",
        "Eva Garcia-Lopez"
      ],
      "abstract": "Large language models (LLMs) can act as evaluators, a role studied by methods like LLM-as-a-Judge and fine-tuned judging LLMs. In the field of education, LLMs have been studied as assistant tools for students and teachers. Our research investigates LLM-driven automatic evaluation systems for academic Text-Input Problems using rubrics. We propose five evaluation systems that have been tested on a custom dataset of 110 answers about computer science from higher education students with three models: JudgeLM, Llama-3.1-8B and DeepSeek-R1-Distill-Llama-8B. The evaluation systems include: The JudgeLM evaluation, which uses the model's single answer prompt to obtain a score; Reference Aided Evaluation, which uses a correct answer as a guide aside from the original context of the question; No Reference Evaluation, which ommits the reference answer; Additive Evaluation, which uses atomic criteria; and Adaptive Evaluation, which is an evaluation done with generated criteria fitted to each question. All evaluation methods have been compared with the results of a human evaluator. Results show that the best method to automatically evaluate and score Text-Input Problems using LLMs is Reference Aided Evaluation. With the lowest median absolute deviation (0.945) and the lowest root mean square deviation (1.214) when compared to human evaluation, Reference Aided Evaluation offers fair scoring as well as insightful and complete evaluations. Other methods such as Additive and Adaptive Evaluation fail to provide good results in concise answers, No Reference Evaluation lacks information needed to correctly assess questions and JudgeLM Evaluations have not provided good results due to the model's limitations. As a result, we conclude that Artificial Intelligence-driven automatic evaluation systems, aided with proper methodologies, show potential to work as complementary tools to other academic resources.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æŒ‡ä»¤é©±åŠ¨çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)åˆ©ç”¨è¯„åˆ†é‡è§„(rubrics)å¯¹å­¦æœ¯é¢†åŸŸçš„æ–‡æœ¬è¾“å…¥é—®é¢˜(Text-Input Problems)è¿›è¡Œè‡ªåŠ¨è¯„åˆ†å’Œè¯„ä»·çš„èƒ½åŠ›ã€‚ç ”ç©¶è€…åŸºäºè®¡ç®—æœºç§‘å­¦ä¸“ä¸šçš„å­¦ç”Ÿå›ç­”æ•°æ®é›†ï¼Œåˆ©ç”¨Llama-3.1-8Bå’ŒDeepSeek-R1-Distill-Llama-8Bç­‰æ¨¡å‹ï¼Œå¯¹æ¯”æµ‹è¯•äº†åŒ…æ‹¬Reference Aided Evaluationã€No Reference Evaluationã€Additive Evaluationã€Adaptive EvaluationåŠJudgeLM evaluationåœ¨å†…çš„äº”ç§è¯„ä¼°ç³»ç»Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReference Aided Evaluationæ˜¯æ•ˆæœæœ€ä½³çš„æ–¹æ³•ï¼Œå…¶åœ¨ä¸­ä½æ•°ç»å¯¹åå·®(0.945)å’Œå‡æ–¹æ ¹åå·®(1.214)ä¸Šå‡è¾¾åˆ°æœ€ä½ï¼Œèƒ½å¤Ÿæä¾›å…¬å¹³ä¸”æ·±å…¥çš„è¯„ä»·åé¦ˆã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒAdditiveå’ŒAdaptive Evaluationåœ¨å¤„ç†ç®€æ´å›ç­”æ—¶è¡¨ç°è¾ƒå·®ï¼Œè€ŒNo Reference Evaluationåˆ™å› ç¼ºä¹å‚è€ƒä¿¡æ¯è€Œéš¾ä»¥å‡†ç¡®è¯„ä¼°ã€‚ç ”ç©¶æœ€ç»ˆå¾—å‡ºç»“è®ºï¼Œåœ¨é‡‡ç”¨é€‚å½“æ–¹æ³•è®ºè¾…åŠ©çš„å‰æä¸‹ï¼Œäººå·¥æ™ºèƒ½é©±åŠ¨çš„è‡ªåŠ¨è¯„ä»·ç³»ç»Ÿå…·æœ‰ä½œä¸ºå­¦æœ¯èµ„æºè¡¥å……å·¥å…·çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20982v1",
      "published_date": "2025-09-25 10:26:23 UTC",
      "updated_date": "2025-09-25 10:26:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:21:32.192093+00:00"
    },
    {
      "arxiv_id": "2509.20978v1",
      "title": "FracAug: Fractional Augmentation boost Graph-level Anomaly Detection under Limited Supervision",
      "title_zh": "FracAugï¼šæœ‰é™ç›‘ç£ä¸‹é€šè¿‡åˆ†æ•°å¢å¼ºæå‡å›¾çº§å¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Xiangyu Dong",
        "Xingyi Zhang",
        "Sibo Wang"
      ],
      "abstract": "Graph-level anomaly detection (GAD) is critical in diverse domains such as drug discovery, yet high labeling costs and dataset imbalance hamper the performance of Graph Neural Networks (GNNs). To address these issues, we propose FracAug, an innovative plug-in augmentation framework that enhances GNNs by generating semantically consistent graph variants and pseudo-labeling with mutual verification. Unlike previous heuristic methods, FracAug learns semantics within given graphs and synthesizes fractional variants, guided by a novel weighted distance-aware margin loss. This captures multi-scale topology to generate diverse, semantic-preserving graphs unaffected by data imbalance. Then, FracAug utilizes predictions from both original and augmented graphs to pseudo-label unlabeled data, iteratively expanding the training set. As a model-agnostic module compatible with various GNNs, FracAug demonstrates remarkable universality and efficacy: experiments across 14 GNNs on 12 real-world datasets show consistent gains, boosting average AUROC, AUPRC, and F1-score by up to 5.72%, 7.23%, and 4.18%, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾çº§å¼‚å¸¸æ£€æµ‹(Graph-level anomaly detection, GAD)é¢†åŸŸé¢ä¸´çš„é«˜æ˜‚æ ‡æ³¨æˆæœ¬å’Œæ•°æ®é›†ä¸å¹³è¡¡é—®é¢˜ï¼Œæå‡ºäº†åä¸ºFracAugçš„åˆ›æ–°å³æ’å³ç”¨å¢å¼ºæ¡†æ¶ã€‚FracAugé€šè¿‡å­¦ä¹ ç»™å®šå›¾çš„è¯­ä¹‰å¹¶åˆæˆå°æ•°æ¯”ä¾‹å˜ä½“(fractional variants)ï¼Œåˆ©ç”¨åŠ æƒè·ç¦»æ„ŸçŸ¥è¾¹é™…æŸå¤±(weighted distance-aware margin loss)æ•æ‰å¤šå°ºåº¦æ‹“æ‰‘ï¼Œä»è€Œç”Ÿæˆè¯­ä¹‰ä¸€è‡´ä¸”å¤šæ ·åŒ–çš„å›¾æ•°æ®ã€‚è¯¥æ¡†æ¶è¿›ä¸€æ­¥ç»“åˆåŸå§‹å›¾ä¸å¢å¼ºå›¾çš„é¢„æµ‹ç»“æœå¯¹æœªæ ‡è®°æ•°æ®è¿›è¡Œä¼ªæ ‡ç­¾(pseudo-labeling)å¤„ç†ï¼Œé€šè¿‡ç›¸äº’éªŒè¯æœºåˆ¶ä¸æ–­è¿­ä»£æ‰©å¤§è®­ç»ƒé›†ã€‚ä½œä¸ºä¸€ç§æ¨¡å‹æ— å…³(model-agnostic)çš„æ¨¡å—ï¼ŒFracAugå±•ç°äº†æä½³çš„é€šç”¨æ€§ï¼Œåœ¨12ä¸ªçœŸå®æ•°æ®é›†ä¸Šé’ˆå¯¹14ç§å›¾ç¥ç»ç½‘ç»œ(GNNs)çš„å®éªŒè¡¨æ˜å…¶èƒ½æ˜¾è‘—æå‡æ£€æµ‹æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä½¿å¹³å‡AUROCã€AUPRCå’ŒF1-scoreåˆ†åˆ«æœ€é«˜æå‡äº†5.72%ã€7.23%å’Œ4.18%ï¼Œè¯æ˜äº†å…¶åœ¨æœ‰é™ç›‘ç£ä¸‹å¢å¼ºå¼‚å¸¸æ£€æµ‹ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20978v1",
      "published_date": "2025-09-25 10:23:21 UTC",
      "updated_date": "2025-09-25 10:23:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:21:46.690774+00:00"
    },
    {
      "arxiv_id": "2509.20975v1",
      "title": "Knowledgeable Language Models as Black-Box Optimizers for Personalized Medicine",
      "title_zh": "å°†çŸ¥è¯†å‹è¯­è¨€æ¨¡å‹ä½œä¸ºä¸ªæ€§åŒ–åŒ»ç–—çš„é»‘ç›’ä¼˜åŒ–å™¨",
      "authors": [
        "Michael S. Yao",
        "Osbert Bastani",
        "Alma Andersson",
        "Tommaso Biancalani",
        "AÃ¯cha Bentaieb",
        "Claudia Iriondo"
      ],
      "abstract": "The goal of personalized medicine is to discover a treatment regimen that optimizes a patient's clinical outcome based on their personal genetic and environmental factors. However, candidate treatments cannot be arbitrarily administered to the patient to assess their efficacy; we often instead have access to an in silico surrogate model that approximates the true fitness of a proposed treatment. Unfortunately, such surrogate models have been shown to fail to generalize to previously unseen patient-treatment combinations. We hypothesize that domain-specific prior knowledge - such as medical textbooks and biomedical knowledge graphs - can provide a meaningful alternative signal of the fitness of proposed treatments. To this end, we introduce LLM-based Entropy-guided Optimization with kNowledgeable priors (LEON), a mathematically principled approach to leverage large language models (LLMs) as black-box optimizers without any task-specific fine-tuning, taking advantage of their ability to contextualize unstructured domain knowledge to propose personalized treatment plans in natural language. In practice, we implement LEON via 'optimization by prompting,' which uses LLMs as stochastic engines for proposing treatment designs. Experiments on real-world optimization tasks show LEON outperforms both traditional and LLM-based methods in proposing individualized treatments for patients.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸ªæ€§åŒ–åŒ»ç–—(Personalized Medicine)ä¸­ä»£ç†æ¨¡å‹éš¾ä»¥æ³›åŒ–è‡³æœªçŸ¥æ‚£è€…-æ²»ç–—ç»„åˆçš„é—®é¢˜ï¼Œæå‡ºäº†LEON (LLM-based Entropy-guided Optimization with kNowledgeable priors)ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä½œä¸ºé»‘ç›’ä¼˜åŒ–å™¨(Black-Box Optimizers)çš„æ•°å­¦åŒ–æ–¹æ³•ã€‚è¯¥æ¡†æ¶æ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œé€šè¿‡åˆ©ç”¨LLMså¯¹åŒ»å­¦æ•™ç§‘ä¹¦å’Œç”Ÿç‰©åŒ»å­¦çŸ¥è¯†å›¾è°±ç­‰é¢†åŸŸå…ˆéªŒçŸ¥è¯†çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ï¼Œä»¥è‡ªç„¶è¯­è¨€å½¢å¼æå‡ºä¸ªæ€§åŒ–æ²»ç–—æ–¹æ¡ˆã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒLEONé‡‡ç”¨â€œæç¤ºä¼˜åŒ–â€(Optimization by Prompting)æœºåˆ¶ï¼Œå°†LLMsä½œä¸ºç”Ÿæˆæ²»ç–—è®¾è®¡çš„éšæœºå¼•æ“ã€‚å®éªŒè¯æ˜ï¼ŒLEONåœ¨å¤„ç†çœŸå®ä¸–ç•Œçš„ä¼˜åŒ–ä»»åŠ¡æ—¶ï¼Œåœ¨ä¸ºæ‚£è€…æä¾›ä¸ªä½“åŒ–æ²»ç–—å»ºè®®æ–¹é¢çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•åŠå…¶ä»–åŸºäºLLMçš„æ–¹æ³•ã€‚è¯¥æˆæœå±•ç¤ºäº†ç»“åˆé¢†åŸŸçŸ¥è¯†çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¼˜åŒ–å¤æ‚åŒ»ç–—ä¸´åºŠç»“æœæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "56 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.20975v1",
      "published_date": "2025-09-25 10:19:52 UTC",
      "updated_date": "2025-09-25 10:19:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:22:39.886338+00:00"
    },
    {
      "arxiv_id": "2509.20972v1",
      "title": "Dual-Path Phishing Detection: Integrating Transformer-Based NLP with Structural URL Analysis",
      "title_zh": "åŒè·¯å¾„ç½‘ç»œé’“é±¼æ£€æµ‹ï¼šç»“åˆåŸºäº Transformer çš„è‡ªç„¶è¯­è¨€å¤„ç†ä¸ URL ç»“æ„åˆ†æ",
      "authors": [
        "Ibrahim Altan",
        "Abdulla Bachir",
        "Yousuf Parbhulkar",
        "Abdul Muksith Rizvi",
        "Moshiur Farazi"
      ],
      "abstract": "Phishing emails pose a persistent and increasingly sophisticated threat, undermining email security through deceptive tactics designed to exploit both semantic and structural vulnerabilities. Traditional detection methods, often based on isolated analysis of email content or embedded URLs, fail to comprehensively address these evolving attacks. In this paper, we propose a dual-path phishing detection framework that integrates transformer-based natural language processing (NLP) with classical machine learning to jointly analyze email text and embedded URLs. Our approach leverages the complementary strengths of semantic analysis using fine-tuned transformer architectures (e.g., DistilBERT) and structural link analysis via character-level TF-IDF vectorization paired with classical classifiers (e.g., Random Forest). Empirical evaluation on representative email and URL datasets demonstrates that this combined approach significantly improves detection accuracy. Specifically, the DistilBERT model achieves a near-optimal balance between accuracy and computational efficiency for textual phishing detection, while Random Forest notably outperforms other classical classifiers in identifying malicious URLs. The modular design allows flexibility for standalone deployment or ensemble integration, facilitating real-world adoption. Collectively, our results highlight the efficacy and practical value of this dual-path approach, establishing a scalable, accurate, and interpretable solution capable of enhancing email security against contemporary phishing threats.",
      "tldr_zh": "é’ˆå¯¹é’“é±¼é‚®ä»¶åˆ©ç”¨è¯­ä¹‰å’Œç»“æ„æ¼æ´å¸¦æ¥çš„æŒç»­å¨èƒï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŒè·¯å¾„é’“é±¼æ£€æµ‹æ¡†æ¶(Dual-Path Phishing Detection)ï¼Œé€šè¿‡æ•´åˆåŸºäºTransformerçš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)æŠ€æœ¯ä¸ç»“æ„åŒ–URLåˆ†ææ¥å…±åŒæå‡æ£€æµ‹èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¾®è°ƒåçš„DistilBERTæ¨¡å‹å¯¹é‚®ä»¶æ–‡æœ¬è¿›è¡Œæ·±å±‚è¯­ä¹‰åˆ†æï¼ŒåŒæ—¶ç»“åˆå­—ç¬¦çº§TF-IDFå‘é‡åŒ–ä¸Random Foreståˆ†ç±»å™¨å¯¹åµŒå…¥é“¾æ¥è¿›è¡Œç»“æ„åŒ–åˆ†æã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™ç§åŒè·¯å¾„æ–¹æ³•æ˜¾è‘—æé«˜äº†æ•´ä½“æ£€æµ‹å‡†ç¡®åº¦ï¼Œå…¶ä¸­DistilBERTåœ¨æ–‡æœ¬æ£€æµ‹ä¸­å®ç°äº†å‡†ç¡®ç‡ä¸è®¡ç®—æ•ˆç‡çš„æœ€ä¼˜å¹³è¡¡ï¼Œè€ŒRandom Foreståœ¨è¯†åˆ«æ¶æ„URLæ–¹é¢è¡¨ç°ä¼˜äºå…¶ä»–ä¼ ç»Ÿåˆ†ç±»å™¨ã€‚è¯¥ç ”ç©¶é€šè¿‡æ¨¡å—åŒ–è®¾è®¡å®ç°äº†çµæ´»éƒ¨ç½²ï¼Œä¸ºåº”å¯¹ç°ä»£é’“é±¼æ”»å‡»æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€é«˜ç²¾åº¦ä¸”å…·å¤‡å¯è§£é‡Šæ€§çš„é‚®ä»¶å®‰å…¨è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Paper accepted for presentation at the ACS/IEEE 22nd International Conference on Computer Systems and Applications (AICCSA 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.20972v1",
      "published_date": "2025-09-25 10:17:12 UTC",
      "updated_date": "2025-09-25 10:17:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:22:21.891638+00:00"
    },
    {
      "arxiv_id": "2509.20971v2",
      "title": "i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents",
      "title_zh": "i-LAVAï¼šé¢å‘æ™ºèƒ½ä½“çš„ä½å»¶è¿Ÿè¯­éŸ³åˆ°è¯­éŸ³æ¶æ„æ¢æ",
      "authors": [
        "Anupam Purwar",
        "Aditya Choudhary"
      ],
      "abstract": "We experiment with a low-latency, end-to-end voice-to-voice communication model to optimize it for real-time conversational applications. By analyzing components essential to voice to voice (V-2-V) system viz. automatic speech recognition (ASR), text-to-speech (TTS), and dialog management, our work analyzes how to reduce processing time while maintaining high-quality interactions to identify the levers for optimizing V-2-V system. Our work identifies that TTS component which generates life-like voice, full of emotions including natural pauses and exclamations has highest impact on Real time factor (RTF). The experimented V-2-V architecture utilizes CSM1b has the capability to understand tone as well as context of conversation by ingesting both audio and text of prior exchanges to generate contextually accurate speech. We explored optimization of Residual Vector Quantization (RVQ) iterations by the TTS decoder which come at a cost of decrease in the quality of voice generated. Our experimental evaluations also demonstrate that for V-2-V implementations based on CSM most important optimizations can be brought by reducing the number of RVQ Iterations along with the codebooks used in Mimi.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”¨äºæ™ºèƒ½ä½“çš„ä½å»¶è¿Ÿç«¯åˆ°ç«¯è¯­éŸ³åˆ°è¯­éŸ³(Voice-2-Voice, V-2-V)é€šä¿¡æ¶æ„i-LAVAï¼Œæ—¨åœ¨ä¼˜åŒ–å®æ—¶å¯¹è¯åº”ç”¨çš„æ€§èƒ½ã€‚é€šè¿‡åˆ†æè‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)ã€æ–‡æœ¬è½¬è¯­éŸ³(TTS)å’Œå¯¹è¯ç®¡ç†ç­‰æ ¸å¿ƒç»„ä»¶ï¼Œç ”ç©¶å‘ç°å…·å¤‡è‡ªç„¶æƒ…æ„Ÿå’Œåœé¡¿çš„TTSç»„ä»¶å¯¹å®æ—¶å› å­(Real-time factor, RTF)çš„å½±å“æœ€ä¸ºæ˜¾è‘—ã€‚i-LAVAæ¶æ„åˆ©ç”¨CSM1bæ¨¡å‹ï¼Œé€šè¿‡æ•´åˆå…ˆå‰äº¤äº’çš„éŸ³é¢‘å’Œæ–‡æœ¬ä¿¡æ¯æ¥ç†è§£å¯¹è¯çš„è¯­æ°”ä¸ä¸Šä¸‹æ–‡ï¼Œä»è€Œç”Ÿæˆè¯­å¢ƒå‡†ç¡®çš„è¯­éŸ³ã€‚å®éªŒé‡ç‚¹è€ƒå¯Ÿäº†TTSè§£ç å™¨ä¸­æ®‹å·®çŸ¢é‡é‡åŒ–(Residual Vector Quantization, RVQ)è¿­ä»£æ¬¡æ•°çš„ä¼˜åŒ–ï¼Œå¹¶åˆ†æäº†å…¶åœ¨é™ä½å»¶è¿Ÿä¸ç»´æŒè¯­éŸ³è´¨é‡ä¹‹é—´çš„æƒè¡¡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé’ˆå¯¹åŸºäºCSMçš„V-2-Vå®ç°ï¼Œæœ€å…³é”®çš„ä¼˜åŒ–å¯ä»¥é€šè¿‡å‡å°‘RVQè¿­ä»£æ¬¡æ•°ä»¥åŠMimiä¸­ä½¿ç”¨çš„ç æœ¬æ•°é‡æ¥å®ç°ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "This paper analyzes a low-latency, end-to-end voice-to-voice (V-2-V) architecture, identifying that the Text-to-Speech (TTS) component has the highest impact on real-time performance. By reducing the number of Residual Vector Quantization (RVQ) iterations in the TTS model, latency can be effectively halved. Its accepted at AIML Systems 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.20971v2",
      "published_date": "2025-09-25 10:15:51 UTC",
      "updated_date": "2025-09-27 07:00:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:22:37.294809+00:00"
    },
    {
      "arxiv_id": "2509.25237v1",
      "title": "Quantum est in Libris: Navigating Archives with GenAI, Uncovering Tension Between Preservation and Innovation",
      "title_zh": "Quantum est in Librisï¼šåˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¢ç´¢æ¡£æ¡ˆï¼Œæ­ç¤ºä¿æŠ¤ä¸åˆ›æ–°ä¹‹é—´çš„å¼ åŠ›",
      "authors": [
        "Mar Canet Sola",
        "Varvara Guljajeva"
      ],
      "abstract": "\"Quantum est in libris\" explores the intersection of the archaic and the modern. On one side, there are manuscript materials from the Estonian National Museum's (ERM) more than century-old archive describing the life experiences of Estonian people; on the other side, there is technology that transforms these materials into a dynamic and interactive experience. Connecting technology and cultural heritage is the visitor, who turns texts into inputs for a screen sculpture.\n  Historical narratives are visually brought to life through the contemporary technological language. Because the video AI models we employed, Runway Gen-3 and Gen-4, have not previously interacted with Estonian heritage, we can observe how machines today \"read the world\" and create future heritage. \"Quantum est in libris\" introduces an exciting yet unsettling new dimension to the concept of cultural heritage: in a world where data are fluid and interpretations unstable, heritage status becomes fragile. In the digital environment, heritage issues are no longer just about preservation and transmission, but also about representation of the media, machine creativity, and interpretive error. Who or what shapes memory processes and memory spaces, and how?",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤è€æ¡£æ¡ˆæ‰‹ç¨¿ä¸ç°ä»£GenAIæŠ€æœ¯çš„äº¤æ±‡ï¼Œä»¥çˆ±æ²™å°¼äºšå›½å®¶åšç‰©é¦†(ERM)çš„ç™¾å¹´æ¡£æ¡ˆä¸ºåŸºç¡€ï¼Œé€šè¿‡äº’åŠ¨æŠ€æœ¯å°†å†å²èµ„æ–™è½¬åŒ–ä¸ºåŠ¨æ€ä¸”äº¤äº’çš„è§†è§‰ä½“éªŒã€‚ç ”ç©¶åˆ©ç”¨Runway Gen-3å’ŒGen-4ç­‰è§†é¢‘AIæ¨¡å‹å°†å†å²å™äº‹è§†è§‰åŒ–ï¼Œå¹¶è§‚å¯Ÿè¿™äº›æœªæ›¾æ¥è§¦è¿‡çˆ±æ²™å°¼äºšæ–‡åŒ–é—äº§çš„æœºå™¨å¦‚ä½•â€œé˜…è¯»ä¸–ç•Œâ€å¹¶ç”Ÿæˆæœªæ¥é—äº§ã€‚å®éªŒæ­ç¤ºäº†åœ¨æ•°æ®æµåŠ¨ä¸”è§£é‡Šä¸ç¨³å®šçš„æ•°å­—ç¯å¢ƒä¸­ï¼Œæ–‡åŒ–é—äº§çš„åœ°ä½å˜å¾—è„†å¼±ï¼Œå¼•å‘äº†å…³äºåª’ä»‹è¡¨ç°ã€æœºå™¨åˆ›æ„(Machine Creativity)ä»¥åŠè§£é‡Šè¯¯å·®(Interpretive Error)çš„æ·±åº¦è®¨è®ºã€‚é€šè¿‡è¿™ç§åˆ›æ–°æ–¹å¼ï¼Œè®ºæ–‡æ­ç¤ºäº†ä¼ ç»Ÿä¿æŠ¤ä¸æŠ€æœ¯åˆ›æ–°ä¹‹é—´çš„ç´§å¼ å…³ç³»ï¼Œå¹¶å¯¹åœ¨æ•°å­—åŒ–è¿›ç¨‹ä¸­ç©¶ç«Ÿæ˜¯è°æˆ–ä»€ä¹ˆåœ¨å¡‘é€ è®°å¿†ç©ºé—´å’Œè®°å¿†è¿‡ç¨‹æå‡ºäº†å…³é”®æ€§åæ€ã€‚",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.DL",
      "comment": "5 pages, 4 figures,",
      "pdf_url": "https://arxiv.org/pdf/2509.25237v1",
      "published_date": "2025-09-25 09:58:09 UTC",
      "updated_date": "2025-09-25 09:58:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:22:38.383527+00:00"
    },
    {
      "arxiv_id": "2509.20961v1",
      "title": "Unlocking Financial Insights: An advanced Multimodal Summarization with Multimodal Output Framework for Financial Advisory Videos",
      "title_zh": "æ´å¯Ÿé‡‘èè§è§£ï¼šä¸€ç§é¢å‘é‡‘èå’¨è¯¢è§†é¢‘çš„å…·æœ‰å¤šæ¨¡æ€è¾“å‡ºçš„é«˜çº§å¤šæ¨¡æ€æ‘˜è¦æ¡†æ¶",
      "authors": [
        "Sarmistha Das",
        "R E Zera Marveen Lyngkhoi",
        "Sriparna Saha",
        "Alka Maurya"
      ],
      "abstract": "The dynamic propagation of social media has broadened the reach of financial advisory content through podcast videos, yet extracting insights from lengthy, multimodal segments (30-40 minutes) remains challenging. We introduce FASTER (Financial Advisory Summariser with Textual Embedded Relevant images), a modular framework that tackles three key challenges: (1) extracting modality-specific features, (2) producing optimized, concise summaries, and (3) aligning visual keyframes with associated textual points. FASTER employs BLIP for semantic visual descriptions, OCR for textual patterns, and Whisper-based transcription with Speaker diarization as BOS features. A modified Direct Preference Optimization (DPO)-based loss function, equipped with BOS-specific fact-checking, ensures precision, relevance, and factual consistency against the human-aligned summary. A ranker-based retrieval mechanism further aligns keyframes with summarized content, enhancing interpretability and cross-modal coherence. To acknowledge data resource scarcity, we introduce Fin-APT, a dataset comprising 470 publicly accessible financial advisory pep-talk videos for robust multimodal research. Comprehensive cross-domain experiments confirm FASTER's strong performance, robustness, and generalizability when compared to Large Language Models (LLMs) and Vision-Language Models (VLMs). By establishing a new standard for multimodal summarization, FASTER makes financial advisory content more accessible and actionable, thereby opening new avenues for research. The dataset and code are available at: https://github.com/sarmistha-D/FASTER",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FASTER (Financial Advisory Summariser with Textual Embedded Relevant images)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä»é•¿ç¯‡é‡‘èå’¨è¯¢è§†é¢‘ä¸­æå–å…³é”®æ´å¯Ÿçš„æ¨¡å—åŒ–å¤šæ¨¡æ€æ‘˜è¦æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ•´åˆäº†BLIPæå–è§†è§‰è¯­ä¹‰ã€OCRè¯†åˆ«æ–‡æœ¬æ¨¡å¼ä»¥åŠWhisperå’ŒSpeaker diarizationæŠ€æœ¯ï¼Œæœ‰æ•ˆè§£å†³äº†æ¨¡æ€ç‰¹å¾æå–ä¸å¯¹é½çš„éš¾é¢˜ã€‚ä¸ºäº†ä¿éšœæ‘˜è¦çš„å‡†ç¡®æ€§ä¸äº‹å®ä¸€è‡´æ€§ï¼Œç ”ç©¶è€…é‡‡ç”¨äº†æ”¹è¿›çš„Direct Preference Optimization (DPO) æŸå¤±å‡½æ•°ï¼Œå¹¶ç»“åˆäº†Ranker-based retrievalæœºåˆ¶ä»¥å®ç°å…³é”®å¸§ä¸æ–‡æœ¬å†…å®¹çš„ç²¾å‡†å…³è”ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æ¨å‡ºäº†åŒ…å«470ä¸ªå…¬å¼€é‡‘èå’¨è¯¢è§†é¢‘çš„Fin-APTæ•°æ®é›†ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸå¤šæ¨¡æ€ç ”ç©¶èµ„æºçš„åŒ®ä¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFASTERåœ¨è·¨é¢†åŸŸæ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„Large Language Models (LLMs)å’ŒVision-Language Models (VLMs)ã€‚è¯¥ç ”ç©¶ä¸ä»…æå‡äº†é‡‘èå’¨è¯¢å†…å®¹çš„å¯è®¿é—®æ€§ä¸å¯æ“ä½œæ€§ï¼Œä¹Ÿä¸ºå¤šæ¨¡æ€æ‘˜è¦é¢†åŸŸå»ºç«‹äº†ä¸€ä¸ªæ–°çš„æŠ€æœ¯æ ‡å‡†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20961v1",
      "published_date": "2025-09-25 09:54:19 UTC",
      "updated_date": "2025-09-25 09:54:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:22:40.886083+00:00"
    },
    {
      "arxiv_id": "2509.22729v1",
      "title": "Multi-Modal Sentiment Analysis with Dynamic Attention Fusion",
      "title_zh": "åŸºäºåŠ¨æ€æ³¨æ„åŠ›èåˆçš„å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ",
      "authors": [
        "Sadia Abdulhalim",
        "Muaz Albaghdadi",
        "Moshiur Farazi"
      ],
      "abstract": "Traditional sentiment analysis has long been a unimodal task, relying solely on text. This approach overlooks non-verbal cues such as vocal tone and prosody that are essential for capturing true emotional intent. We introduce Dynamic Attention Fusion (DAF), a lightweight framework that combines frozen text embeddings from a pretrained language model with acoustic features from a speech encoder, using an adaptive attention mechanism to weight each modality per utterance. Without any finetuning of the underlying encoders, our proposed DAF model consistently outperforms both static fusion and unimodal baselines on a large multimodal benchmark. We report notable gains in F1-score and reductions in prediction error and perform a variety of ablation studies that support our hypothesis that the dynamic weighting strategy is crucial for modeling emotionally complex inputs. By effectively integrating verbal and non-verbal information, our approach offers a more robust foundation for sentiment prediction and carries broader impact for affective computing applications -- from emotion recognition and mental health assessment to more natural human computer interaction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿæƒ…æ„Ÿåˆ†æä»…ä¾èµ–æ–‡æœ¬ã€å¿½è§†è¯­æ°”å’ŒéŸµå¾‹ç­‰éè¯­è¨€æš—ç¤ºçš„é—®é¢˜ï¼Œæå‡ºäº† Dynamic Attention Fusion (DAF) è½»é‡çº§æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ adaptive attention mechanism å°†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ frozen text embeddings ä¸è¯­éŸ³ç¼–ç å™¨çš„å£°å­¦ç‰¹å¾åŠ¨æ€æ•´åˆï¼Œèƒ½å¤Ÿä¸ºæ¯æ¡è¯è¯­ä¸­çš„ä¸åŒæ¨¡æ€è‡ªé€‚åº”åœ°åˆ†é…æƒé‡ã€‚åœ¨ä¸å¾®è°ƒåº•å±‚ç¼–ç å™¨çš„æƒ…å†µä¸‹ï¼ŒDAF åœ¨å¤§å‹å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°æŒç»­ä¼˜äºé™æ€èåˆå’Œå•æ¨¡æ€åŸºçº¿æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº† F1-score å¹¶é™ä½äº†é¢„æµ‹è¯¯å·®ã€‚æ¶ˆèå®éªŒè¯å®äº†åŠ¨æ€åŠ æƒç­–ç•¥å¯¹äºå»ºæ¨¡å¤æ‚æƒ…æ„Ÿè¾“å…¥è‡³å…³é‡è¦ã€‚é€šè¿‡æœ‰æ•ˆèåˆè¯­è¨€ä¸éè¯­è¨€ä¿¡æ¯ï¼Œè¯¥æ–¹æ³•ä¸ºæƒ…æ„Ÿé¢„æµ‹å¥ å®šäº†æ›´ç¨³å¥çš„åŸºç¡€ï¼Œå¹¶åœ¨ affective computingã€æƒ…ç»ªè¯†åˆ«å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Paper accepted for presentation at the ACS/IEEE 22nd International Conference on Computer Systems and Applications (AICCSA 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.22729v1",
      "published_date": "2025-09-25 09:54:04 UTC",
      "updated_date": "2025-09-25 09:54:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:22:56.588384+00:00"
    },
    {
      "arxiv_id": "2509.20953v1",
      "title": "Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM",
      "title_zh": "è¶…è¶Šæ˜Ÿçº§ï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹å¼¥åˆè¯„åˆ†ä¸è¯„è®ºæƒ…æ„Ÿä¹‹é—´çš„é¸¿æ²Ÿ",
      "authors": [
        "Najla Zuhir",
        "Amna Mohammad Salim",
        "Parvathy Premkumar",
        "Moshiur Farazi"
      ],
      "abstract": "We present an advanced approach to mobile app review analysis aimed at addressing limitations inherent in traditional star-rating systems. Star ratings, although intuitive and popular among users, often fail to capture the nuanced feedback present in detailed review texts. Traditional NLP techniques -- such as lexicon-based methods and classical machine learning classifiers -- struggle to interpret contextual nuances, domain-specific terminology, and subtle linguistic features like sarcasm. To overcome these limitations, we propose a modular framework leveraging large language models (LLMs) enhanced by structured prompting techniques. Our method quantifies discrepancies between numerical ratings and textual sentiment, extracts detailed, feature-level insights, and supports interactive exploration of reviews through retrieval-augmented conversational question answering (RAG-QA). Comprehensive experiments conducted on three diverse datasets (AWARE, Google Play, and Spotify) demonstrate that our LLM-driven approach significantly surpasses baseline methods, yielding improved accuracy, robustness, and actionable insights in challenging and context-rich review scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå…ˆè¿›çš„ç§»åŠ¨åº”ç”¨è¯„è®ºåˆ†ææ¡†æ¶ï¼Œæ—¨åœ¨å¼¥è¡¥ä¼ ç»Ÿæ˜Ÿçº§è¯„ä»·(Star-rating)ç³»ç»Ÿçš„å±€é™æ€§ï¼Œå³æ˜Ÿçº§å¾€å¾€æ— æ³•æ•æ‰è¯„è®ºæ–‡æœ¬ä¸­çš„ç»†å¾®åé¦ˆã€‚ä¸ºäº†è§£å†³ä¼ ç»ŸNLPæŠ€æœ¯ï¼ˆå¦‚åŸºäºè¯å…¸çš„æ–¹æ³•å’Œç»å…¸æœºå™¨å­¦ä¹ åˆ†ç±»å™¨ï¼‰éš¾ä»¥è§£è¯»è¯­å¢ƒç»†å¾®å·®åˆ«ã€é¢†åŸŸä¸“ä¸šæœ¯è¯­ä»¥åŠè®½åˆºç­‰å¤æ‚è¯­è¨€ç‰¹å¾çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)å¹¶ç»“åˆç»“æ„åŒ–æç¤ºè¯(Structured Prompting)æŠ€æœ¯çš„æ¨¡å—åŒ–æ¡†æ¶ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿé‡åŒ–æ•°å€¼è¯„åˆ†ä¸æ–‡æœ¬æƒ…æ„Ÿä¹‹é—´çš„å·®å¼‚ï¼Œæå–è¯¦ç»†çš„åŠŸèƒ½çº§(Feature-level)è§è§£ï¼Œå¹¶æ”¯æŒé€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆå¼å¯¹è¯é—®ç­”(RAG-QA)å¯¹è¯„è®ºè¿›è¡Œäº¤äº’å¼æ¢ç´¢ã€‚åœ¨AWAREã€Google Playå’ŒSpotifyä¸‰ä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¯æ˜ï¼Œè¿™ç§ç”±LLMé©±åŠ¨çš„æ–¹æ³•åœ¨å‡†ç¡®æ€§ã€é²æ£’æ€§å’Œæä¾›å¯æ“ä½œè§è§£æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œæœ‰æ•ˆåº”å¯¹äº†å¤æ‚è¯­å¢ƒä¸‹çš„è¯„è®ºåˆ†ææŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Paper accepted for presentation at ACS/IEEE 22nd International Conference on Computer Systems and Applications (AICCSA 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.20953v1",
      "published_date": "2025-09-25 09:39:12 UTC",
      "updated_date": "2025-09-25 09:39:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:23:02.196707+00:00"
    },
    {
      "arxiv_id": "2509.21424v1",
      "title": "PhenoMoler: Phenotype-Guided Molecular Optimization via Chemistry Large Language Model",
      "title_zh": "PhenoMolerï¼šåŸºäºåŒ–å­¦å¤§è¯­è¨€æ¨¡å‹çš„è¡¨å‹å¼•å¯¼åˆ†å­ä¼˜åŒ–",
      "authors": [
        "Ran Song",
        "Hui Liu"
      ],
      "abstract": "Current molecular generative models primarily focus on improving drug-target binding affinity and specificity, often neglecting the system-level phenotypic effects elicited by compounds. Transcriptional profiles, as molecule-level readouts of drug-induced phenotypic shifts, offer a powerful opportunity to guide molecular design in a phenotype-aware manner. We present PhenoMoler, a phenotype-guided molecular generation framework that integrates a chemistry large language model with expression profiles to enable biologically informed drug design. By conditioning the generation on drug-induced differential expression signatures, PhenoMoler explicitly links transcriptional responses to chemical structure. By selectively masking and reconstructing specific substructures-scaffolds, side chains, or linkers-PhenoMoler supports fine-grained, controllable molecular optimization. Extensive experiments demonstrate that PhenoMoler generates chemically valid, novel, and diverse molecules aligned with desired phenotypic profiles. Compared to FDA-approved drugs, the generated compounds exhibit comparable or enhanced drug-likeness (QED), optimized physicochemical properties, and superior binding affinity to key cancer targets. These findings highlight PhenoMoler's potential for phenotype-guided and structure-controllable molecular optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PhenoMolerï¼Œä¸€ä¸ªå°†åŒ–å­¦å¤§è¯­è¨€æ¨¡å‹(Chemistry Large Language Model)ä¸è¡¨è¾¾è°±(Expression Profiles)ç›¸ç»“åˆçš„è¡¨å‹å¯¼å‘åˆ†å­ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åˆ†å­ç”Ÿæˆæ¨¡å‹å¿½è§†ç³»ç»Ÿçº§è¡¨å‹æ•ˆåº”çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»¥è¯ç‰©è¯±å¯¼çš„å·®å¼‚è¡¨è¾¾ç‰¹å¾(Differential Expression Signatures)ä¸ºæ¡ä»¶ï¼Œå®ç°äº†è½¬å½•å“åº”ä¸åŒ–å­¦ç»“æ„ä¹‹é—´çš„æ˜¾å¼å…³è”ã€‚é€šè¿‡å¯¹éª¨æ¶(Scaffolds)ã€ä¾§é“¾(Side Chains)æˆ–è¿æ¥ç¬¦(Linkers)ç­‰ç‰¹å®šå­ç»“æ„è¿›è¡Œé€‰æ‹©æ€§å±è”½ä¸é‡å»ºï¼ŒPhenoMoleræ”¯æŒç»†ç²’åº¦ä¸”å¯æ§çš„åˆ†å­ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹ç”Ÿæˆçš„åˆ†å­å…·æœ‰è‰¯å¥½çš„åŒ–å­¦æœ‰æ•ˆæ€§ã€æ–°é¢–æ€§å’Œå¤šæ ·æ€§ï¼Œä¸”ä¸ç›®æ ‡è¡¨å‹ç‰¹å¾é«˜åº¦ä¸€è‡´ã€‚ä¸FDAæ‰¹å‡†çš„è¯ç‰©ç›¸æ¯”ï¼Œç”Ÿæˆçš„åŒ–åˆç‰©åœ¨ç±»è¯æ€§(QED)ã€ç†åŒ–æ€§è´¨ä»¥åŠå¯¹å…³é”®ç™Œç—‡é¶ç‚¹çš„ç»“åˆäº²å’ŒåŠ›(Binding Affinity)æ–¹é¢å‡è¡¨ç°å‡ºç›¸å½“æˆ–æ›´ä¼˜çš„æ°´å¹³ã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº†PhenoMoleråœ¨å®ç°è¡¨å‹å¯¼å‘åŠç»“æ„å¯æ§çš„è¯ç‰©è®¾è®¡æ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "physics.chem-ph",
        "cs.AI"
      ],
      "primary_category": "physics.chem-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21424v1",
      "published_date": "2025-09-25 09:37:19 UTC",
      "updated_date": "2025-09-25 09:37:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:23:02.994564+00:00"
    },
    {
      "arxiv_id": "2509.20952v1",
      "title": "Flow Matching in the Low-Noise Regime: Pathologies and a Contrastive Remedy",
      "title_zh": "ä½å™ªå£°æƒ…å½¢ä¸‹çš„æµåŒ¹é…ï¼šç—…æ€ç‰¹æ€§ä¸å¯¹æ¯”å¼è¡¥æ•‘æ–¹æ¡ˆ",
      "authors": [
        "Weili Zeng",
        "Yichao Yan"
      ],
      "abstract": "Flow matching has recently emerged as a powerful alternative to diffusion models, providing a continuous-time formulation for generative modeling and representation learning. Yet, we show that this framework suffers from a fundamental instability in the low-noise regime. As noise levels approach zero, arbitrarily small perturbations in the input can induce large variations in the velocity target, causing the condition number of the learning problem to diverge. This ill-conditioning not only slows optimization but also forces the encoder to reallocate its limited Jacobian capacity toward noise directions, thereby degrading semantic representations. We provide the first theoretical analysis of this phenomenon, which we term the low-noise pathology, establishing its intrinsic link to the structure of the flow matching objective. Building on these insights, we propose Local Contrastive Flow (LCF), a hybrid training protocol that replaces direct velocity regression with contrastive feature alignment at small noise levels, while retaining standard flow matching at moderate and high noise. Empirically, LCF not only improves convergence speed but also stabilizes representation quality. Our findings highlight the critical importance of addressing low-noise pathologies to unlock the full potential of flow matching for both generation and representation learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Flow matching æ¡†æ¶åœ¨ä½å™ªå£°çŠ¶æ€ (low-noise regime) ä¸‹å­˜åœ¨çš„æ ¹æœ¬æ€§ä¸ç¨³å®šæ€§ï¼Œå¹¶å°†å…¶å®šä¹‰ä¸ºâ€œä½å™ªå£°ç—…æ€â€ (low-noise pathology)ã€‚ç†è®ºåˆ†ææŒ‡å‡ºï¼Œå½“å™ªå£°æ°´å¹³æ¥è¿‘é›¶æ—¶ï¼Œè¾“å…¥ä¸­çš„å¾®å°æ‰°åŠ¨ä¼šå¼•èµ·é€Ÿåº¦ç›®æ ‡ (velocity target) çš„å‰§çƒˆæ³¢åŠ¨ï¼Œå¯¼è‡´å­¦ä¹ é—®é¢˜çš„æ¡ä»¶æ•° (condition number) å‘æ•£ã€‚è¿™ç§ç°è±¡ä¸ä»…å‡æ…¢äº†ä¼˜åŒ–é€Ÿåº¦ï¼Œè¿˜è¿«ä½¿ç¼–ç å™¨å°†æœ‰é™çš„ Jacobian å®¹é‡åˆ†é…ç»™å™ªå£°æ–¹å‘ï¼Œä»è€ŒæŸå®³äº†è¯­ä¹‰è¡¨ç¤º (semantic representations) çš„è´¨é‡ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†å±€éƒ¨å¯¹æ¯”æµ (Local Contrastive Flow, LCF) æ··åˆè®­ç»ƒåè®®ï¼Œåœ¨å°å™ªå£°æ°´å¹³ä¸‹åˆ©ç”¨å¯¹æ¯”ç‰¹å¾å¯¹é½ (contrastive feature alignment) ä»£æ›¿ç›´æ¥çš„é€Ÿåº¦å›å½’ (velocity regression)ï¼Œè€Œåœ¨ä¸­é«˜å™ªå£°ä¸‹ä¿ç•™æ ‡å‡†é€»è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLCF ä¸ä»…æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦ï¼Œè¿˜ç¨³å®šäº†è¡¨ç¤ºè´¨é‡ï¼Œä¸ºå……åˆ†å‘æŒ¥ Flow matching åœ¨ç”Ÿæˆå’Œè¡¨ç¤ºå­¦ä¹ é¢†åŸŸçš„æ½œåŠ›æä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¹è¿›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20952v1",
      "published_date": "2025-09-25 09:36:41 UTC",
      "updated_date": "2025-09-25 09:36:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:23:07.781559+00:00"
    },
    {
      "arxiv_id": "2509.21423v2",
      "title": "Near-Optimal Experiment Design in Linear non-Gaussian Cyclic Models",
      "title_zh": "çº¿æ€§éé«˜æ–¯æœ‰ç¯æ¨¡å‹ä¸­çš„è¿‘ä¼¼æœ€ä¼˜å®éªŒè®¾è®¡",
      "authors": [
        "Ehsan Sharifian",
        "Saber Salehkaleybar",
        "Negar Kiyavash"
      ],
      "abstract": "We study the problem of causal structure learning from a combination of observational and interventional data generated by a linear non-Gaussian structural equation model that might contain cycles. Recent results show that using mere observational data identifies the causal graph only up to a permutation-equivalence class. We obtain a combinatorial characterization of this class by showing that each graph in an equivalence class corresponds to a perfect matching in a bipartite graph. This bipartite representation allows us to analyze how interventions modify or constrain the matchings. Specifically, we show that each atomic intervention reveals one edge of the true matching and eliminates all incompatible causal graphs. Consequently, we formalize the optimal experiment design task as an adaptive stochastic optimization problem over the set of equivalence classes with a natural reward function that quantifies how many graphs are eliminated from the equivalence class by an intervention. We show that this reward function is adaptive submodular and provide a greedy policy with a provable near-optimal performance guarantee. A key technical challenge is to efficiently estimate the reward function without having to explicitly enumerate all the graphs in the equivalence class. We propose a sampling-based estimator using random matchings and analyze its bias and concentration behavior. Our simulation results show that performing a small number of interventions guided by our stochastic optimization framework recovers the true underlying causal structure.",
      "tldr_zh": "æœ¬æ–‡ç ”ç©¶äº†åœ¨çº¿æ€§éé«˜æ–¯å¾ªç¯æ¨¡å‹(linear non-Gaussian cyclic models)ä¸­ç»“åˆè§‚æµ‹ä¸å¹²é¢„æ•°æ®è¿›è¡Œå› æœç»“æ„å­¦ä¹ çš„é—®é¢˜ã€‚ç”±äºä»…é è§‚æµ‹æ•°æ®åªèƒ½å°†å› æœå›¾è¯†åˆ«è‡³æ’åˆ—ç­‰ä»·ç±»(permutation-equivalence class)ï¼Œç ”ç©¶è€…é€šè¿‡äºŒéƒ¨å›¾ä¸­çš„å®Œç¾åŒ¹é…(perfect matching)å¯¹è¯¥ç±»è¿›è¡Œäº†åˆ›æ–°çš„ç»„åˆç‰¹å¾åˆ»ç”»ã€‚ç ”ç©¶æ­ç¤ºäº†åŸå­å¹²é¢„(atomic intervention)å¦‚ä½•é€šè¿‡ç¡®å®šåŒ¹é…è¾¹æ¥æœ‰æ•ˆæ¶ˆé™¤ç­‰ä»·ç±»ä¸­ä¸ç›¸å®¹çš„å› æœå›¾ï¼Œå¹¶å°†æœ€ä¼˜å®éªŒè®¾è®¡ä»»åŠ¡å½¢å¼åŒ–ä¸ºä¸€ä¸ªè‡ªé€‚åº”éšæœºä¼˜åŒ–é—®é¢˜ã€‚é€šè¿‡è¯æ˜å¥–åŠ±å‡½æ•°çš„è‡ªé€‚åº”å­æ¨¡æ€§(adaptive submodularity)ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªå…·æœ‰è¿‘ä¼˜æ€§èƒ½ä¿è¯çš„è´ªå¿ƒç­–ç•¥ã€‚ä¸ºäº†è§£å†³æšä¸¾æ‰€æœ‰å›¾çš„è®¡ç®—éš¾é¢˜ï¼Œæ–‡ä¸­è®¾è®¡äº†ä¸€ç§åŸºäºéšæœºåŒ¹é…çš„é‡‡æ ·ä¼°è®¡å™¨ã€‚ä»¿çœŸç»“æœéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜ä»…éœ€å°‘é‡å—æŒ‡å¯¼çš„å¹²é¢„å³å¯å‡†ç¡®æ¢å¤çœŸå®çš„æ½œåœ¨å› æœç»“æ„ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21423v2",
      "published_date": "2025-09-25 09:34:24 UTC",
      "updated_date": "2025-12-04 04:07:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:23:09.393870+00:00"
    },
    {
      "arxiv_id": "2509.20943v1",
      "title": "CTI Dataset Construction from Telegram",
      "title_zh": "åŸºäº Telegram çš„ CTI æ•°æ®é›†æ„å»º",
      "authors": [
        "Dincy R. Arikkat",
        "Sneha B. T.",
        "Serena Nicolazzo",
        "Antonino Nocera",
        "Vinod P.",
        "Rafidha Rehiman K. A.",
        "Karthika R"
      ],
      "abstract": "Cyber Threat Intelligence (CTI) enables organizations to anticipate, detect, and mitigate evolving cyber threats. Its effectiveness depends on high-quality datasets, which support model development, training, evaluation, and benchmarking. Building such datasets is crucial, as attack vectors and adversary tactics continually evolve. Recently, Telegram has gained prominence as a valuable CTI source, offering timely and diverse threat-related information that can help address these challenges. In this work, we address these challenges by presenting an end-to-end automated pipeline that systematically collects and filters threat-related content from Telegram. The pipeline identifies relevant Telegram channels and scrapes 145,349 messages from 12 curated channels out of 150 identified sources. To accurately filter threat intelligence messages from generic content, we employ a BERT-based classifier, achieving an accuracy of 96.64%. From the filtered messages, we compile a dataset of 86,509 malicious Indicators of Compromise, including domains, IPs, URLs, hashes, and CVEs. This approach not only produces a large-scale, high-fidelity CTI dataset but also establishes a foundation for future research and operational applications in cyber threat detection.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªä» Telegram è‡ªåŠ¨åŒ–æ„å»ºç½‘ç»œå¨èƒæƒ…æŠ¥ (Cyber Threat Intelligence, CTI) æ•°æ®é›†çš„ç«¯åˆ°ç«¯æµæ°´çº¿ï¼Œä»¥åº”å¯¹ä¸æ–­æ¼”å˜çš„å¯¹æŠ—ç­–ç•¥ã€‚è¯¥æµæ°´çº¿ç³»ç»Ÿåœ°æ”¶é›†å¹¶è¿‡æ»¤å¨èƒç›¸å…³å†…å®¹ï¼Œä» 150 ä¸ªè¯†åˆ«æºä¸­ç­›é€‰å‡º 12 ä¸ªç²¾é€‰é¢‘é“ï¼Œå¹¶çˆ¬å–äº† 145,349 æ¡åŸå§‹æ¶ˆæ¯ã€‚ä¸ºäº†ä»é€šç”¨å†…å®¹ä¸­ç²¾å‡†è¯†åˆ«å¨èƒæƒ…æŠ¥ï¼Œç ”ç©¶é‡‡ç”¨äº†åŸºäº BERT çš„åˆ†ç±»å™¨ï¼Œå¹¶å®ç°äº† 96.64% çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚é€šè¿‡å¯¹æ¶ˆæ¯çš„è¿‡æ»¤ä¸æå–ï¼Œè¯¥ç ”ç©¶ç¼–è¯‘äº†ä¸€ä¸ªåŒ…å« 86,509 ä¸ªæ¶æ„ Indicators of Compromise (IoC) çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¶µç›–äº† domainsã€IPsã€URLsã€hashes å’Œ CVEs ç­‰å…³é”®ä¿¡æ¯ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æä¾›äº†ä¸€ä¸ªé«˜ä¿çœŸåº¦çš„ CTI æ•°æ®é›†ï¼Œä¹Ÿä¸ºæœªæ¥ç½‘ç»œå¨èƒæ£€æµ‹çš„ç ”ç©¶å’Œæ“ä½œåº”ç”¨å»ºç«‹äº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20943v1",
      "published_date": "2025-09-25 09:27:10 UTC",
      "updated_date": "2025-09-25 09:27:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:23:24.563245+00:00"
    },
    {
      "arxiv_id": "2509.20935v2",
      "title": "GALAX: Graph-Augmented Language Model for Explainable Reinforcement-Guided Subgraph Reasoning in Precision Medicine",
      "title_zh": "GALAXï¼šé¢å‘ç²¾å‡†åŒ»å­¦å¯è§£é‡Šå¼ºåŒ–å¼•å¯¼å­å›¾æ¨ç†çš„å›¾å¢å¼ºè¯­è¨€æ¨¡å‹",
      "authors": [
        "Heming Zhang",
        "Di Huang",
        "Wenyu Li",
        "Michael Province",
        "Yixin Chen",
        "Philip Payne",
        "Fuhai Li"
      ],
      "abstract": "In precision medicine, quantitative multi-omic features, topological context, and textual biological knowledge play vital roles in identifying disease-critical signaling pathways and targets. Existing pipelines capture only part of these-numerical omics ignore topological context, text-centric LLMs lack quantitative grounded reasoning, and graph-only models underuse node semantics and the generalization of LLMs-limiting mechanistic interpretability. Although Process Reward Models (PRMs) aim to guide reasoning in LLMs, they remain limited by unreliable intermediate evaluation, and vulnerability to reward hacking with computational cost. These gaps motivate integrating quantitative multi-omic signals, topological structure with node annotations, and literature-scale text via LLMs, using subgraph reasoning as the principle bridge linking numeric evidence, topological knowledge and language context. Therefore, we propose GALAX (Graph Augmented LAnguage model with eXplainability), an innovative framework that integrates pretrained Graph Neural Networks (GNNs) into Large Language Models (LLMs) via reinforcement learning guided by a Graph Process Reward Model (GPRM), which generates disease-relevant subgraphs in a step-wise manner initiated by an LLM and iteratively evaluated by a pretrained GNN and schema-based rule check, enabling process-level supervision without explicit labels. As an application, we also introduced Target-QA, a benchmark combining CRISPR-identified targets, multi-omic profiles, and biomedical graph knowledge across diverse cancer cell lines, which enables GNN pretraining for supervising step-wise graph construction and supports long-context reasoning over text-numeric graphs (TNGs), providing a scalable and biologically grounded framework for explainable, reinforcement-guided subgraph reasoning toward reliable and interpretable target discovery in precision medicine.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç²¾å‡†åŒ»ç–—ä¸­å¤šç»„å­¦(multi-omic)ç‰¹å¾ã€æ‹“æ‰‘èƒŒæ™¯ä¸ç”Ÿç‰©å­¦çŸ¥è¯†æ•´åˆä¸è¶³å¯¼è‡´çš„è§£é‡Šæ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†GALAXæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡Graph Process Reward Model (GPRM) å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ (reinforcement learning)ï¼Œåˆ›æ–°æ€§åœ°å°†é¢„è®­ç»ƒçš„å›¾ç¥ç»ç½‘ç»œ(GNNs)é›†æˆåˆ°å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­ã€‚GALAXé‡‡ç”¨ç”±LLMå¯åŠ¨å¹¶ç”±GNNä¸æ¨¡å¼è§„åˆ™(schema-based rule check)è¿­ä»£è¯„ä¼°çš„ç­–ç•¥ï¼Œä»¥æ­¥è¿›æ–¹å¼ç”Ÿæˆç–¾ç—…ç›¸å…³å­å›¾ï¼Œå®ç°äº†æ— éœ€æ˜¾å¼æ ‡ç­¾çš„è¿‡ç¨‹çº§ç›‘ç£ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¨å‡ºäº†æ•´åˆCRISPRç›®æ ‡ã€å¤šç»„å­¦æ•°æ®åŠç”Ÿç‰©åŒ»å­¦å›¾çŸ¥è¯†çš„Target-QAåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ”¯æŒåœ¨æ–‡æœ¬-æ•°å€¼å›¾(TNGs)ä¸Šçš„é•¿ä¸Šä¸‹æ–‡æ¨ç†ã€‚è¯¥æ¡†æ¶ä¸ºç²¾å‡†åŒ»ç–—ä¸­å¯é ä¸”å¯è§£é‡Šçš„è¯ç‰©é¶ç‚¹å‘ç°(target discovery)æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”å…·æœ‰ç”Ÿç‰©å­¦åŸºç¡€çš„å¼ºåŒ–å­¦ä¹ å¼•å¯¼å­å›¾æ¨ç†æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20935v2",
      "published_date": "2025-09-25 09:20:58 UTC",
      "updated_date": "2025-12-16 06:45:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:24:25.391450+00:00"
    },
    {
      "arxiv_id": "2509.22728v2",
      "title": "Prompt-aware classifier free guidance for diffusion models",
      "title_zh": "é¢å‘æ‰©æ•£æ¨¡å‹çš„æç¤ºæ„ŸçŸ¥å‹æ— åˆ†ç±»å™¨å¼•å¯¼",
      "authors": [
        "Xuanhao Zhang",
        "Chang Li"
      ],
      "abstract": "Diffusion models have achieved remarkable progress in image and audio generation, largely due to Classifier-Free Guidance. However, the choice of guidance scale remains underexplored: a fixed scale often fails to generalize across prompts of varying complexity, leading to oversaturation or weak alignment. We address this gap by introducing a prompt-aware framework that predicts scale-dependent quality and selects the optimal guidance at inference. Specifically, we construct a large synthetic dataset by generating samples under multiple scales and scoring them with reliable evaluation metrics. A lightweight predictor, conditioned on semantic embeddings and linguistic complexity, estimates multi-metric quality curves and determines the best scale via a utility function with regularization. Experiments on MSCOCO~2014 and AudioCaps show consistent improvements over vanilla CFG, enhancing fidelity, alignment, and perceptual preference. This work demonstrates that prompt-aware scale selection provides an effective, training-free enhancement for pretrained diffusion backbones.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’ŒéŸ³é¢‘ç”Ÿæˆä¸­ Classifier-Free Guidance (CFG) çš„å¼•å¯¼å°ºåº¦å›ºå®šå¯¼è‡´çš„ç”Ÿæˆå›¾åƒè¿‡åº¦é¥±å’Œæˆ–å¯¹é½æ€§å¼±ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ Prompt-aware æ¡†æ¶æ¥é¢„æµ‹æ¨ç†é˜¶æ®µçš„æœ€ä¼˜å¼•å¯¼å°ºåº¦ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨å¤šå°ºåº¦ç”Ÿæˆçš„åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨è½»é‡çº§é¢„æµ‹å™¨ç»“åˆ semantic embeddings å’Œè¯­è¨€å¤æ‚åº¦æ¥ä¼°ç®—è´¨é‡æ›²çº¿ï¼Œå¹¶é€šè¿‡å¸¦æ­£åˆ™åŒ–çš„æ•ˆç”¨å‡½æ•°åŠ¨æ€ç¡®å®šæœ€ä½³å°ºåº¦ã€‚å®éªŒç»“æœåœ¨ MSCOCO 2014 å’Œ AudioCaps æ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆç»“æœçš„å¿ å®åº¦ã€å¯¹é½æ€§ä»¥åŠæ„ŸçŸ¥åå¥½ã€‚è¿™é¡¹å·¥ä½œä¸ºç°æœ‰çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ backbones æä¾›äº†ä¸€ç§æ— éœ€é¢å¤–è®­ç»ƒçš„é«˜æ•ˆå¢å¼ºæ–¹æ¡ˆï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹ä¸åŒå¤æ‚åº¦æç¤ºè¯çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "6 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.22728v2",
      "published_date": "2025-09-25 09:16:25 UTC",
      "updated_date": "2025-10-05 11:32:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:23:32.564637+00:00"
    },
    {
      "arxiv_id": "2509.20913v1",
      "title": "Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales",
      "title_zh": "æ·±åº¦å­¦ä¹ åŠ©åŠ›çŠ¯ç½ªé¢„æµ‹ï¼šç»†ç²’åº¦æ—¶ç©ºå°ºåº¦ä¸‹äººå£æµåŠ¨æ€§çš„ä½œç”¨",
      "authors": [
        "Ariadna Albors Zumel",
        "Michele Tizzoni",
        "Gian Maria Campedelli"
      ],
      "abstract": "Objectives: To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features, alongside historical crime and sociodemographic data, enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions.\n  Methods: We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities (i.e., Baltimore, Chicago, Los Angeles, and Philadelphia). We employ crime incident data obtained from each city's police department, combined with sociodemographic data from the American Community Survey and human mobility data from Advan, collected from 2019 to 2023. This data is aggregated into grids with equally sized cells of 0.077 sq. miles (0.2 sq. kms) and used to train our deep learning forecasting model, a Convolutional Long Short-Term Memory (ConvLSTM) network, which predicts crime occurrences 12 hours ahead using 14-day and 2-day input sequences. We also compare its performance against three baseline models: logistic regression, random forest, and standard LSTM.\n  Results: Incorporating mobility features improves predictive performance, especially when using shorter input sequences. Noteworthy, however, the best results are obtained when both mobility and sociodemographic features are used together, with our deep learning model achieving the highest recall, precision, and F1 score in all four cities, outperforming alternative methods. With this configuration, longer input sequences enhance predictions for violent crimes, while shorter sequences are more effective for property crimes.\n  Conclusion: These findings underscore the importance of integrating diverse data sources for spatiotemporal crime forecasting, mobility included. They also highlight the advantages (and limits) of deep learning when dealing with fine-grained spatial and temporal scales.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°ç»“åˆå¾®è§‚ç§»åŠ¨æ€§(mobility)ç‰¹å¾ã€å†å²çŠ¯ç½ªåŠç¤¾ä¼šäººå£å­¦æ•°æ®å¯¹æå‡ç»†ç²’åº¦æ—¶ç©ºåˆ†è¾¨ç‡ä¸‹çŠ¯ç½ªé¢„æµ‹æ€§èƒ½çš„ä½œç”¨ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ç¾å›½å››ä¸ªåŸå¸‚çš„è­¦åŠ¡æ•°æ®ã€ç¤¾ä¼šç»æµæ•°æ®åŠç§»åŠ¨æ€§æ•°æ®ï¼Œæ„å»ºäº†å·ç§¯é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ(ConvLSTM)æ¨¡å‹ï¼Œé€šè¿‡ä¸åŒé•¿åº¦çš„è¾“å…¥åºåˆ—é¢„æµ‹æœªæ¥12å°æ—¶çš„çŠ¯ç½ªå‘ç”Ÿã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œçº³å…¥ç§»åŠ¨æ€§ç‰¹å¾æ˜¾è‘—æå‡äº†é¢„æµ‹æ€§èƒ½ï¼Œå°¤å…¶åœ¨ç»“åˆç¤¾ä¼šäººå£å­¦ç‰¹å¾æ—¶ï¼Œè¯¥æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¬å›ç‡(recall)ã€ç²¾ç¡®ç‡(precision)å’ŒF1åˆ†æ•°(F1 score)ä¸Šå‡ä¼˜äºé€»è¾‘å›å½’(logistic regression)å’Œéšæœºæ£®æ—(random forest)ç­‰åŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°è¾ƒé•¿çš„è¾“å…¥åºåˆ—æœ‰åŠ©äºæå‡æš´åŠ›çŠ¯ç½ª(violent crimes)çš„é¢„æµ‹æ•ˆæœï¼Œè€Œè¾ƒçŸ­çš„åºåˆ—åˆ™å¯¹è´¢äº§çŠ¯ç½ª(property crimes)é¢„æµ‹æ›´ä¸ºæœ‰æ•ˆã€‚è¯¥ç ”ç©¶ç»“è®ºå¼ºè°ƒäº†æ•´åˆå¤šå…ƒæ•°æ®è¿›è¡Œæ—¶ç©ºçŠ¯ç½ªé¢„æµ‹çš„é‡è¦æ€§ï¼Œå¹¶æ­ç¤ºäº†æ·±åº¦å­¦ä¹ åœ¨å¤„ç†æç²¾ç»†æ—¶ç©ºå°ºåº¦æ—¶çš„ä¼˜åŠ¿ä¸å±€é™æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "64 pages, 33 figures, and 6 tables (including appendix)",
      "pdf_url": "https://arxiv.org/pdf/2509.20913v1",
      "published_date": "2025-09-25 08:58:56 UTC",
      "updated_date": "2025-09-25 08:58:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:23:39.944470+00:00"
    },
    {
      "arxiv_id": "2509.20912v1",
      "title": "DeFacto: Counterfactual Thinking with Images for Enforcing Evidence-Grounded and Faithful Reasoning",
      "title_zh": "DeFactoï¼šé€šè¿‡å›¾åƒåäº‹å®æ€ç»´å¼ºåŒ–è¯æ®é©±åŠ¨çš„å¿ å®æ¨ç†",
      "authors": [
        "Tianrun Xu",
        "Haoda Jing",
        "Ye Li",
        "Yuquan Wei",
        "Jun Feng",
        "Guanyu Chen",
        "Haichuan Gao",
        "Tianren Zhang",
        "Feng Chen"
      ],
      "abstract": "Recent advances in multimodal language models (MLLMs) have achieved remarkable progress in vision-language reasoning, especially with the emergence of \"thinking with images,\" which integrates explicit visual steps into the reasoning process. While this paradigm strengthens image-based reasoning, a significant challenge remains: models may arrive at correct answers by relying on irrelevant or spurious regions, driven by prior knowledge or dataset biases. Even when the answer is correct, flawed reasoning indicates that the model has not truly understood the image, highlighting the critical importance of reasoning fidelity in multimodal tasks. To address this issue, we propose DeFacto, a counterfactual reasoning framework that jointly enforces accurate answering and faithful reasoning. A key component of our approach is the design of three complementary training paradigms: (i) positive, (ii) counterfactual, and (iii) random-masking. To enable these paradigms, we develop a pipeline that automatically localizes question-relevant evidence and constructs positive, counterfactual, and random variants, resulting in a dataset of about 100k images. Building on this framework, we train multimodal language models with GRPO-based reinforcement learning, where we design three complementary rewards to guide the model toward accurate answering and evidence-grounded reasoning. Experiments on diverse benchmarks demonstrate that DeFacto substantially improves both answer accuracy and reasoning faithfulness, establishing a stronger foundation for interpretable multimodal reasoning. The code is available on GitHub and the dataset is released on HuggingFace.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DeFactoï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)æ¨ç†å¿ å®åº¦å’Œè¯æ®å¯¹é½æ€§çš„åäº‹å®æ¨ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³æ¨¡å‹åœ¨è§†è§‰è¯­è¨€æ¨ç†ä¸­å¯èƒ½ä¾èµ–ä¸ç›¸å…³å›¾åƒåŒºåŸŸæˆ–å…ˆéªŒåå·®çš„é—®é¢˜ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤ŸåŸºäºçœŸå®çš„è§†è§‰è¯æ®å¾—å‡ºç»“è®ºã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œç ”ç©¶è€…è®¾è®¡äº†æ­£å‘(positive)ã€åäº‹å®(counterfactual)å’Œéšæœºé®è”½(random-masking)ä¸‰ç§äº’è¡¥çš„è®­ç»ƒèŒƒå¼ï¼Œå¹¶åˆ©ç”¨è‡ªåŠ¨åŒ–æµæ°´çº¿æ„å»ºäº†åŒ…å«çº¦10ä¸‡å¼ å›¾åƒçš„è¯æ®é©±åŠ¨æ•°æ®é›†ã€‚æ­¤å¤–ï¼ŒDeFactoé‡‡ç”¨äº†åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œç»“åˆä¸‰é¡¹ä¸“é—¨è®¾è®¡çš„å¥–åŠ±æœºåˆ¶æ¥å¼•å¯¼æ¨¡å‹å®ç°å‡†ç¡®å›ç­”ä¸è¯æ®å¯¹é½ã€‚å®éªŒè¯æ˜ï¼ŒDeFactoåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†ç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œæ¨ç†è¿‡ç¨‹çš„å¿ å®åº¦ï¼Œä¸ºæ„å»ºå¯è§£é‡Šçš„å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿå¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20912v1",
      "published_date": "2025-09-25 08:58:10 UTC",
      "updated_date": "2025-09-25 08:58:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:23:35.165530+00:00"
    },
    {
      "arxiv_id": "2510.01260v1",
      "title": "IoT-MCP: Bridging LLMs and IoT Systems Through Model Context Protocol",
      "title_zh": "IoT-MCPï¼šåŸºäºæ¨¡å‹ä¸Šä¸‹æ–‡åè®®è¿æ¥å¤§è¯­è¨€æ¨¡å‹ä¸ç‰©è”ç½‘ç³»ç»Ÿ",
      "authors": [
        "Ningyuan Yang",
        "Guanliang Lyu",
        "Mingchen Ma",
        "Yiyi Lu",
        "Yiming Li",
        "Zhihui Gao",
        "Hancheng Ye",
        "Jianyi Zhang",
        "Tingjun Chen",
        "Yiran Chen"
      ],
      "abstract": "The integration of Large Language Models (LLMs) with Internet-of-Things (IoT) systems faces significant challenges in hardware heterogeneity and control complexity. The Model Context Protocol (MCP) emerges as a critical enabler, providing standardized communication between LLMs and physical devices. We propose IoT-MCP, a novel framework that implements MCP through edge-deployed servers to bridge LLMs and IoT ecosystems. To support rigorous evaluation, we introduce IoT-MCP Bench, the first benchmark containing 114 Basic Tasks (e.g., ``What is the current temperature?'') and 1,140 Complex Tasks (e.g., ``I feel so hot, do you have any ideas?'') for IoT-enabled LLMs. Experimental validation across 22 sensor types and 6 microcontroller units demonstrates IoT-MCP's 100% task success rate to generate tool calls that fully meet expectations and obtain completely accurate results, 205ms average response time, and 74KB peak memory footprint. This work delivers both an open-source integration framework (https://github.com/Duke-CEI-Center/IoT-MCP-Servers) and a standardized evaluation methodology for LLM-IoT systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† IoT-MCPï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è¾¹ç¼˜éƒ¨ç½²æœåŠ¡å™¨å®ç°æ¨¡å‹ä¸Šä¸‹æ–‡åè®® (Model Context Protocol, MCP) çš„åˆ›æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) ä¸ç‰©è”ç½‘ (IoT) ç³»ç»Ÿé›†æˆä¸­çš„ç¡¬ä»¶å¼‚æ„æ€§å’Œæ§åˆ¶å¤æ‚æ€§é—®é¢˜ã€‚ä¸ºäº†å¡«è¡¥è¯„ä¼°é¢†åŸŸçš„ç©ºç™½ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†é¦–ä¸ªé’ˆå¯¹ IoT èµ‹èƒ½ LLMs çš„åŸºå‡†æµ‹è¯• IoT-MCP Benchï¼Œæ¶µç›–äº† 114 ä¸ªåŸºç¡€ä»»åŠ¡å’Œ 1,140 ä¸ªå¤æ‚åœºæ™¯ä»»åŠ¡ã€‚é€šè¿‡åœ¨ 22 ç§ä¼ æ„Ÿå™¨å’Œ 6 ç§å¾®æ§åˆ¶å™¨å•å…ƒ (MCUs) ä¸Šçš„å®éªŒéªŒè¯ï¼ŒIoT-MCP åœ¨ç”Ÿæˆå·¥å…·è°ƒç”¨ (tool calls) æ–¹é¢å®ç°äº† 100% çš„ä»»åŠ¡æˆåŠŸç‡ã€‚ç³»ç»Ÿè¡¨ç°å‡ºæé«˜çš„æ•ˆç‡ï¼Œå¹³å‡å“åº”æ—¶é—´ä»…ä¸º 205msï¼Œå³°å€¼å†…å­˜å ç”¨æ§åˆ¶åœ¨ 74KBï¼Œè¯æ˜äº†å…¶åœ¨è¾¹ç¼˜è®¡ç®—ç¯å¢ƒä¸‹çš„å¯è¡Œæ€§ã€‚è¯¥å·¥ä½œä¸º LLM-IoT ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªå¼€æºçš„é›†æˆæ¡†æ¶åŠæ ‡å‡†åŒ–çš„è¯„ä¼°æ–¹æ³•ï¼Œæœ‰æ•ˆæ¨åŠ¨äº†ç‰©ç†è®¾å¤‡ä¸æ™ºèƒ½è¯­è¨€æ¨¡å‹çš„æ— ç¼è¿æ¥ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01260v1",
      "published_date": "2025-09-25 08:35:47 UTC",
      "updated_date": "2025-09-25 08:35:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:23:36.657340+00:00"
    },
    {
      "arxiv_id": "2509.20890v2",
      "title": "FerretNet: Efficient Synthetic Image Detection via Local Pixel Dependencies",
      "title_zh": "FerretNetï¼šåŸºäºå±€éƒ¨åƒç´ ä¾èµ–çš„é«˜æ•ˆåˆæˆå›¾åƒæ£€æµ‹",
      "authors": [
        "Shuqiao Liang",
        "Jian Liu",
        "Renzhang Chen",
        "Quanlong Guan"
      ],
      "abstract": "The increasing realism of synthetic images generated by advanced models such as VAEs, GANs, and LDMs poses significant challenges for synthetic image detection. To address this issue, we explore two artifact types introduced during the generation process: (1) latent distribution deviations and (2) decoding-induced smoothing effects, which manifest as inconsistencies in local textures, edges, and color transitions. Leveraging local pixel dependencies (LPD) properties rooted in Markov Random Fields, we reconstruct synthetic images using neighboring pixel information to expose disruptions in texture continuity and edge coherence. Building upon LPD, we propose FerretNet, a lightweight neural network with only 1.1M parameters that delivers efficient and robust synthetic image detection. Extensive experiments demonstrate that FerretNet, trained exclusively on the 4-class ProGAN dataset, achieves an average accuracy of 97.1% on an open-world benchmark comprising 22 generative models. Our code and datasets are publicly available at https://github.com/xigua7105/FerretNet.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹VAEsã€GANså’ŒLDMsç”Ÿæˆçš„åˆæˆå›¾åƒæ£€æµ‹éš¾é¢˜ï¼Œæ·±å…¥åˆ†æäº†ç”Ÿæˆè¿‡ç¨‹ä¸­äº§ç”Ÿçš„æ½œåˆ†å¸ƒåå·®(latent distribution deviations)å’Œè§£ç è¯±å¯¼çš„å¹³æ»‘æ•ˆåº”(decoding-induced smoothing effects)ã€‚è¿™äº›äººå·¥ç—•è¿¹åœ¨å±€éƒ¨çº¹ç†ã€è¾¹ç¼˜å’Œé¢œè‰²è¿‡æ¸¡ä¸­è¡¨ç°ä¸ºæ˜¾è‘—çš„ä¸ä¸€è‡´æ€§ã€‚ç ”ç©¶è€…åˆ©ç”¨åŸºäºé©¬å°”å¯å¤«éšæœºåœº(Markov Random Fields)çš„å±€éƒ¨åƒç´ ä¾èµ–(Local Pixel Dependencies, LPD)å±æ€§ï¼Œé€šè¿‡é‚»è¿‘åƒç´ ä¿¡æ¯é‡å»ºå›¾åƒï¼Œä»è€Œæœ‰æ•ˆæš´éœ²çº¹ç†è¿ç»­æ€§å’Œè¾¹ç¼˜ç›¸å¹²æ€§çš„ç ´åã€‚åŸºäºæ­¤æå‡ºçš„FerretNetæ˜¯ä¸€ä¸ªä»…åŒ…å«1.1Må‚æ•°çš„è½»é‡çº§ç¥ç»ç½‘ç»œï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆä¸”é²æ£’çš„åˆæˆå›¾åƒæ£€æµ‹ã€‚å®éªŒç»“æœè¯æ˜ï¼Œä»…åœ¨4ç±»ProGANæ•°æ®é›†ä¸Šè®­ç»ƒçš„FerretNetï¼Œåœ¨æ¶µç›–22ç§ç”Ÿæˆæ¨¡å‹çš„å¼€æ”¾ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†97.1%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œå±•ç°äº†æå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 4 figures, 8 tables, accepted at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.20890v2",
      "published_date": "2025-09-25 08:28:32 UTC",
      "updated_date": "2025-10-23 05:06:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:23:51.683261+00:00"
    },
    {
      "arxiv_id": "2509.20885v1",
      "title": "Improving Early Sepsis Onset Prediction Through Federated Learning",
      "title_zh": "é€šè¿‡è”é‚¦å­¦ä¹ æå‡è„“æ¯’ç—‡æ—©æœŸå‘ç—…é¢„æµ‹",
      "authors": [
        "Christoph DÃ¼sing",
        "Philipp Cimiano"
      ],
      "abstract": "Early and accurate prediction of sepsis onset remains a major challenge in intensive care, where timely detection and subsequent intervention can significantly improve patient outcomes. While machine learning models have shown promise in this domain, their success is often limited by the amount and diversity of training data available to individual hospitals and Intensive Care Units (ICUs). Federated Learning (FL) addresses this issue by enabling collaborative model training across institutions without requiring data sharing, thus preserving patient privacy. In this work, we propose a federated, attention-enhanced Long Short-Term Memory model for sepsis onset prediction, trained on multi-centric ICU data. Unlike existing approaches that rely on fixed prediction windows, our model supports variable prediction horizons, enabling both short- and long-term forecasting in a single unified model. During analysis, we put particular emphasis on the improvements through our approach in terms of early sepsis detection, i.e., predictions with large prediction windows by conducting an in-depth temporal analysis. Our results prove that using FL does not merely improve overall prediction performance (with performance approaching that of a centralized model), but is particularly beneficial for early sepsis onset prediction. Finally, we show that our choice of employing a variable prediction window rather than a fixed window does not hurt performance significantly but reduces computational, communicational, and organizational overhead.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é‡ç—‡ç›‘æŠ¤ç—…æˆ¿(ICU)ä¸­è„“æ¯’ç—‡(Sepsis)æ—©æœŸé¢„æµ‹é¢ä¸´çš„æ•°æ®åŒ®ä¹å’Œéšç§ä¿æŠ¤æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè”é‚¦å­¦ä¹ (Federated Learning)çš„åä½œé¢„æµ‹æ¡†æ¶ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªè”é‚¦åŒ–çš„ã€ç»“åˆæ³¨æ„åŠ›æœºåˆ¶çš„é•¿çŸ­æœŸè®°å¿†ç½‘ç»œæ¨¡å‹(Attention-enhanced LSTM)ï¼Œæ—¨åœ¨ä¸å…±äº«åŸå§‹æ•°æ®çš„å‰æä¸‹é€šè¿‡å¤šä¸­å¿ƒICUæ•°æ®æå‡æ¨¡å‹è¡¨ç°ã€‚ä¸ä¼ ç»Ÿä¾èµ–å›ºå®šé¢„æµ‹çª—å£çš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ¨¡å‹æ”¯æŒå¯å˜é¢„æµ‹èŒƒå›´(Variable Prediction Horizons)ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€ç»Ÿä¸€æ¨¡å‹ä¸­åŒæ—¶å®ç°çŸ­æœŸå’Œé•¿æœŸé¢„æµ‹ã€‚é€šè¿‡æ·±å…¥çš„æ—¶é—´åˆ†æ(Temporal Analysis)ï¼Œç ”ç©¶è¯æ˜äº†è¯¥æ–¹æ³•åœ¨è„“æ¯’ç—‡æ—©æœŸæ£€æµ‹ï¼ˆå³å¤§é¢„æµ‹çª—å£åœºæ™¯ä¸‹ï¼‰å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè”é‚¦å­¦ä¹ ä¸‹çš„æ¨¡å‹æ€§èƒ½å·²æ¥è¿‘ä¸­å¿ƒåŒ–è®­ç»ƒæ°´å¹³ï¼Œä¸”åœ¨æ—©æœŸé¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°å°¤ä¸ºçªå‡ºã€‚æ­¤å¤–ï¼Œé‡‡ç”¨å¯å˜é¢„æµ‹çª—å£åœ¨ä¸ç‰ºç‰²æ˜¾è‘—æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆé™ä½äº†è®¡ç®—ã€é€šä¿¡åŠç»„ç»‡å¼€é”€ï¼Œä¸ºè·¨æœºæ„çš„åŒ»ç–—äººå·¥æ™ºèƒ½åä½œæä¾›äº†å¯è¡Œè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the 1st Workshop on Artificial Intelligence for Biomedical Data (AIBio) 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.20885v1",
      "published_date": "2025-09-25 08:19:22 UTC",
      "updated_date": "2025-09-25 08:19:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:23:51.468313+00:00"
    },
    {
      "arxiv_id": "2509.20884v1",
      "title": "Integrating Object Interaction Self-Attention and GAN-Based Debiasing for Visual Question Answering",
      "title_zh": "èåˆç‰©ä½“äº¤äº’è‡ªæ³¨æ„åŠ›ä¸åŸºäº GAN å»åçš„è§†è§‰é—®ç­”",
      "authors": [
        "Zhifei Li",
        "Feng Qiu",
        "Yiran Wang",
        "Yujing Xia",
        "Kui Xiao",
        "Miao Zhang",
        "Yan Zhang"
      ],
      "abstract": "Visual Question Answering (VQA) presents a unique challenge by requiring models to understand and reason about visual content to answer questions accurately. Existing VQA models often struggle with biases introduced by the training data, leading to over-reliance on superficial patterns and inadequate generalization to diverse questions and images. This paper presents a novel model, IOG-VQA, which integrates Object Interaction Self-Attention and GAN-Based Debiasing to enhance VQA model performance. The self-attention mechanism allows our model to capture complex interactions between objects within an image, providing a more comprehensive understanding of the visual context. Meanwhile, the GAN-based debiasing framework generates unbiased data distributions, helping the model to learn more robust and generalizable features. By leveraging these two components, IOG-VQA effectively combines visual and textual information to address the inherent biases in VQA datasets. Extensive experiments on the VQA-CP v1 and VQA-CP v2 datasets demonstrate that our model shows excellent performance compared with the existing methods, particularly in handling biased and imbalanced data distributions highlighting the importance of addressing both object interactions and dataset biases in advancing VQA tasks. Our code is available at https://github.com/HubuKG/IOG-VQA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†IOG-VQAæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆObject Interaction Self-Attentionå’ŒGAN-Based DebiasingæŠ€æœ¯æ¥å¢å¼ºVisual Question Answering (VQA) æ¨¡å‹çš„æ€§èƒ½ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹åœ¨å¤„ç†è®­ç»ƒæ•°æ®åè§å’Œæ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨Self-Attentionæœºåˆ¶æ•æ‰å›¾åƒä¸­ç‰©ä½“é—´çš„å¤æ‚äº¤äº’ï¼Œä»¥æä¾›æ›´å…¨é¢çš„è§†è§‰è¯­å¢ƒç†è§£ã€‚åŒæ—¶ï¼ŒåŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN-Based) çš„å»åè§æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆæ— åçš„æ•°æ®åˆ†å¸ƒï¼Œä¿ƒä½¿æ¨¡å‹å­¦ä¹ åˆ°æ›´ç¨³å¥ã€æ›´å…·æ³›åŒ–æ€§çš„ç‰¹å¾ã€‚åœ¨VQA-CP v1å’ŒVQA-CP v2æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒIOG-VQAåœ¨å¤„ç†åè§å’Œä¸å¹³è¡¡æ•°æ®åˆ†å¸ƒæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨æ¨è¿›VQAä»»åŠ¡ä¸­åŒæ—¶è§£å†³ç‰©ä½“äº¤äº’ç†è§£ä¸æ•°æ®é›†åè§ä¿®æ­£çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 6 figures. ACCEPTED for publication as a REGULAR paper in the IEEE Transactions on Multimedia 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.20884v1",
      "published_date": "2025-09-25 08:13:19 UTC",
      "updated_date": "2025-09-25 08:13:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:23:55.690535+00:00"
    },
    {
      "arxiv_id": "2509.20882v2",
      "title": "On Theoretical Interpretations of Concept-Based In-Context Learning",
      "title_zh": "åŸºäºæ¦‚å¿µçš„ä¸Šä¸‹æ–‡å­¦ä¹ çš„ç†è®ºè§£è¯»",
      "authors": [
        "Huaze Tang",
        "Tianren Peng",
        "Shao-lun Huang"
      ],
      "abstract": "In-Context Learning (ICL) has emerged as an important new paradigm in natural language processing and large language model (LLM) applications. However, the theoretical understanding of the ICL mechanism remains limited. This paper aims to investigate this issue by studying a particular ICL approach, called concept-based ICL (CB-ICL). In particular, we propose theoretical analyses on applying CB-ICL to ICL tasks, which explains why and when the CB-ICL performs well for predicting query labels in prompts with only a few demonstrations. In addition, the proposed theory quantifies the knowledge that can be leveraged by the LLMs to the prompt tasks, and leads to a similarity measure between the prompt demonstrations and the query input, which provides important insights and guidance for model pre-training and prompt engineering in ICL. Moreover, the impact of the prompt demonstration size and the dimension of the LLM embeddings in ICL are also explored based on the proposed theory. Finally, several real-data experiments are conducted to validate the practical usefulness of CB-ICL and the corresponding theory.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºæ¦‚å¿µçš„æƒ…å¢ƒå­¦ä¹ (Concept-Based In-Context Learning, CB-ICL)çš„ç†è®ºæœºåˆ¶ï¼Œæ—¨åœ¨å¡«è¡¥å¤§è¯­è¨€æ¨¡å‹(LLM)ä¸­ICLæœºåˆ¶ç†è§£çš„ç©ºç™½ã€‚é€šè¿‡å¯¹CB-ICLçš„åº”ç”¨è¿›è¡Œç†è®ºåˆ†æï¼Œè®ºæ–‡è§£é‡Šäº†åœ¨ä»…æœ‰å°‘é‡æ¼”ç¤º(demonstrations)çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹é¢„æµ‹æŸ¥è¯¢æ ‡ç­¾çš„åŸç†åŠé€‚ç”¨æ¡ä»¶ã€‚ç ”ç©¶è¿›ä¸€æ­¥é‡åŒ–äº†LLMåœ¨æç¤ºä»»åŠ¡ä¸­å¯åˆ©ç”¨çš„çŸ¥è¯†ï¼Œå¹¶æå‡ºäº†ä¸€ç§è¡¡é‡æ¼”ç¤ºä¸æŸ¥è¯¢è¾“å…¥ä¹‹é—´ç›¸ä¼¼åº¦çš„æ–¹æ³•ï¼Œä¸ºæ¨¡å‹é¢„è®­ç»ƒå’Œæç¤ºå·¥ç¨‹(prompt engineering)æä¾›äº†é‡è¦è§è§£ã€‚æ­¤å¤–ï¼Œè¯¥ç†è®ºè¿˜æ·±å…¥æ¢è®¨äº†æç¤ºæ¼”ç¤ºè§„æ¨¡å’ŒLLMåµŒå…¥ç»´åº¦(embeddings)å¯¹å­¦ä¹ æ•ˆæœçš„å½±å“ã€‚æœ€åï¼Œé€šè¿‡çœŸå®æ•°æ®å®éªŒéªŒè¯äº†CB-ICLåŠå…¶ç†è®ºæ¡†æ¶çš„å®é™…åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20882v2",
      "published_date": "2025-09-25 08:11:09 UTC",
      "updated_date": "2025-10-16 05:50:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:23:54.482729+00:00"
    },
    {
      "arxiv_id": "2509.20871v1",
      "title": "SCRA-VQA: Summarized Caption-Rerank for Augmented Large Language Models in Visual Question Answering",
      "title_zh": "SCRA-VQAï¼šè§†è§‰é—®ç­”ä¸­é¢å‘å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹çš„æ‘˜è¦åŒ–æè¿°é‡æ’åº",
      "authors": [
        "Yan Zhang",
        "Jiaqing Lin",
        "Miao Zhang",
        "Kui Xiao",
        "Xiaoju Hou",
        "Yue Zhao",
        "Zhifei Li"
      ],
      "abstract": "Acquiring high-quality knowledge is a central focus in Knowledge-Based Visual Question Answering (KB-VQA). Recent methods use large language models (LLMs) as knowledge engines for answering. These methods generally employ image captions as visual text descriptions to assist LLMs in interpreting images. However, the captions frequently include excessive noise irrelevant to the question, and LLMs generally do not comprehend VQA tasks, limiting their reasoning capabilities. To address this issue, we propose the Summarized Caption-Rerank Augmented VQA (SCRA-VQA), which employs a pre-trained visual language model to convert images into captions. Moreover, SCRA-VQA generates contextual examples for the captions while simultaneously summarizing and reordering them to exclude unrelated information. The caption-rerank process enables LLMs to understand the image information and questions better, thus enhancing the model's reasoning ability and task adaptability without expensive end-to-end training. Based on an LLM with 6.7B parameters, SCRA-VQA performs excellently on two challenging knowledge-based VQA datasets: OK-VQA and A-OKVQA, achieving accuracies of 38.8% and 34.6%. Our code is available at https://github.com/HubuKG/SCRA-VQA.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºçŸ¥è¯†çš„è§†è§‰é—®ç­”(Knowledge-Based Visual Question Answering, KB-VQA)ä¸­è·å–é«˜è´¨é‡çŸ¥è¯†çš„éš¾é¢˜ï¼Œæå‡ºäº†SCRA-VQAæ¡†æ¶ã€‚SCRA-VQAåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å°†å›¾åƒè½¬æ¢ä¸ºæè¿°(captions)ï¼Œå¹¶é€šè¿‡ç”Ÿæˆä¸Šä¸‹æ–‡ç¤ºä¾‹ä»¥åŠå¯¹æè¿°è¿›è¡Œæ‘˜è¦å’Œé‡æ’åº(rerank)æ¥æœ‰æ•ˆæ’é™¤æ— å…³å™ªå£°ã€‚è¿™ä¸€è¿‡ç¨‹æ˜¾è‘—æå‡äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)å¯¹å›¾åƒä¿¡æ¯å’Œé—®é¢˜çš„ç†è§£æ·±åº¦ï¼Œä»è€Œåœ¨æ— éœ€æ˜‚è´µç«¯åˆ°ç«¯è®­ç»ƒçš„å‰æä¸‹å¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œä»»åŠ¡é€‚åº”æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäº6.7Bå‚æ•°çš„LLMï¼ŒSCRA-VQAåœ¨OK-VQAå’ŒA-OKVQAæ•°æ®é›†ä¸Šåˆ†åˆ«è¾¾åˆ°äº†38.8%å’Œ34.6%çš„å‡†ç¡®ç‡ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ç²¾ç‚¼è§†è§‰æ–‡æœ¬æè¿°ï¼Œå¯ä»¥æœ‰æ•ˆä¼˜åŒ–å¤§æ¨¡å‹åœ¨å¤„ç†å¤æ‚è§†è§‰é—®ç­”ä»»åŠ¡æ—¶çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ACCEPTED as a FULL PAPER for the Research Track at International Conference on Database Systems for Advanced Applications 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.20871v1",
      "published_date": "2025-09-25 08:01:28 UTC",
      "updated_date": "2025-09-25 08:01:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:24:57.692622+00:00"
    },
    {
      "arxiv_id": "2509.20869v1",
      "title": "Model-Based Reinforcement Learning under Random Observation Delays",
      "title_zh": "éšæœºè§‚æµ‹å»¶è¿Ÿä¸‹çš„åŸºäºæ¨¡å‹å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Armin Karamzade",
        "Kyungmin Kim",
        "JB Lanier",
        "Davide Corsi",
        "Roy Fox"
      ],
      "abstract": "Delays frequently occur in real-world environments, yet standard reinforcement learning (RL) algorithms often assume instantaneous perception of the environment. We study random sensor delays in POMDPs, where observations may arrive out-of-sequence, a setting that has not been previously addressed in RL. We analyze the structure of such delays and demonstrate that naive approaches, such as stacking past observations, are insufficient for reliable performance. To address this, we propose a model-based filtering process that sequentially updates the belief state based on an incoming stream of observations. We then introduce a simple delay-aware framework that incorporates this idea into model-based RL, enabling agents to effectively handle random delays. Applying this framework to Dreamer, we compare our approach to delay-aware baselines developed for MDPs. Our method consistently outperforms these baselines and demonstrates robustness to delay distribution shifts during deployment. Additionally, we present experiments on simulated robotic tasks, comparing our method to common practical heuristics and emphasizing the importance of explicitly modeling observation delays.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰åœ¨çœŸå®ç¯å¢ƒä¸­é¢ä¸´çš„éšæœºä¼ æ„Ÿå™¨å»¶è¿Ÿï¼ˆrandom sensor delaysï¼‰é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è§‚æµ‹å¯èƒ½ä¹±åºåˆ°è¾¾çš„éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPsï¼‰è®¾ç½®ä¸‹ã€‚ä½œè€…é€šè¿‡åˆ†æå»¶è¿Ÿç»“æ„è¯æ˜ï¼Œä¼ ç»Ÿçš„å †å å†å²è§‚æµ‹ç­‰ç®€å•æ–¹æ³•æ— æ³•æä¾›å¯é æ€§èƒ½ï¼Œè¿›è€Œæå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹ï¼ˆmodel-basedï¼‰çš„è¿‡æ»¤è¿‡ç¨‹ï¼Œåˆ©ç”¨è§‚æµ‹æµé¡ºåºæ›´æ–°ç½®ä¿¡çŠ¶æ€ï¼ˆbelief stateï¼‰ã€‚é€šè¿‡å°†è¯¥æ€è·¯æ•´åˆè¿› Dreamer ç®—æ³•ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹éšæœºå»¶è¿Ÿçš„å»¶è¿Ÿæ„ŸçŸ¥æ¡†æ¶ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸ŠæŒç»­ä¼˜äºé’ˆå¯¹ MDP å¼€å‘çš„åŸºå‡†æ¨¡å‹ï¼Œå¹¶å¯¹éƒ¨ç½²æ—¶çš„å»¶è¿Ÿåˆ†å¸ƒåç§»ï¼ˆdelay distribution shiftsï¼‰å…·æœ‰æ˜¾è‘—é²æ£’æ€§ã€‚åœ¨æ¨¡æ‹Ÿæœºå™¨äººä»»åŠ¡ä¸­çš„æµ‹è¯•è¿›ä¸€æ­¥éªŒè¯äº†æ˜¾å¼å»ºæ¨¡è§‚æµ‹å»¶è¿Ÿçš„å¿…è¦æ€§ï¼Œå…¶è¡¨ç°ä¼˜äºå¸¸è§çš„å®ç”¨å¯å‘å¼æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20869v1",
      "published_date": "2025-09-25 08:01:13 UTC",
      "updated_date": "2025-09-25 08:01:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:24:58.892193+00:00"
    },
    {
      "arxiv_id": "2509.20868v1",
      "title": "StyleBench: Evaluating thinking styles in Large Language Models",
      "title_zh": "StyleBenchï¼šå¤§è¯­è¨€æ¨¡å‹æ€ç»´é£æ ¼è¯„ä¼°",
      "authors": [
        "Junyu Guo",
        "Shangding Gu",
        "Ming Jin",
        "Costas Spanos",
        "Javad Lavaei"
      ],
      "abstract": "The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or styles of thought, employed in their prompts. However, the interplay between these reasoning styles, model architecture, and task type remains poorly understood. To address this, we introduce StyleBench, a comprehensive benchmark for systematically evaluating reasoning styles across diverse tasks and models. We assess five representative reasoning styles, including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought (AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our large-scale analysis reveals that no single style is universally optimal. We demonstrate that strategy efficacy is highly contingent on both model scale and task type: search-based methods (AoT, ToT) excel in open-ended problems but require large-scale models, while concise styles (SoT, CoD) achieve radical efficiency gains on well-defined tasks. Furthermore, we identify key behavioral patterns: smaller models frequently fail to follow output instructions and default to guessing, while reasoning robustness emerges as a function of scale. Our findings offer a crucial roadmap for selecting optimal reasoning strategies based on specific constraints, we open source the benchmark in https://github.com/JamesJunyuGuo/Style_Bench.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† StyleBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç³»ç»Ÿè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ä¸åŒæ¨ç†ç­–ç•¥æˆ–æ€ç»´é£æ ¼ï¼ˆStyles of Thoughtï¼‰çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶äººå‘˜é’ˆå¯¹ Chain of Thought (CoT)ã€Tree of Thought (ToT)ã€Algorithm of Thought (AoT)ã€Sketch of Thought (SoT) å’Œ Chain-of-Draft (CoD) äº”ç§ä»£è¡¨æ€§æ¨ç†é£æ ¼ï¼Œåœ¨ LLaMAã€Qwenã€Mistral ç­‰ä¸»æµå®¶æ—çš„ 15 ä¸ªå¼€æºæ¨¡å‹ä¸Šè¿›è¡Œäº†å¤§è§„æ¨¡è¯„ä¼°ã€‚åˆ†æç»“æœè¡¨æ˜ï¼Œæ²¡æœ‰ä»»ä½•ä¸€ç§æ¨ç†é£æ ¼æ˜¯æ™®éæœ€ä¼˜çš„ï¼Œå…¶ç­–ç•¥æ•ˆæœé«˜åº¦å–å†³äºæ¨¡å‹è§„æ¨¡å’Œä»»åŠ¡ç±»å‹ã€‚åŸºäºæœç´¢çš„æ–¹æ³•ï¼ˆAoTã€ToTï¼‰åœ¨å¤„ç†å¼€æ”¾å¼é—®é¢˜æ—¶è¡¨ç°å“è¶Šä½†é«˜åº¦ä¾èµ–å¤§å‚æ•°é‡æ¨¡å‹ï¼Œè€Œç®€æ´é£æ ¼ï¼ˆSoTã€CoDï¼‰åœ¨å®šä¹‰æ˜ç¡®çš„ä»»åŠ¡ä¸­å±•ç°å‡ºæé«˜çš„æ•ˆç‡ä¼˜åŠ¿ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œå°è§„æ¨¡æ¨¡å‹åœ¨éµå¾ªæŒ‡ä»¤æ–¹é¢è¡¨ç°è¾ƒå¼±ï¼Œä¸”æ¨ç†çš„é²æ£’æ€§éšæ¨¡å‹è§„æ¨¡å¢é•¿è€Œæ˜¾è‘—æå‡ã€‚è¯¥é¡¹å·¥ä½œä¸ºåœ¨ç‰¹å®šçº¦æŸä¸‹é€‰æ‹©æœ€ä¼˜æ¨ç†ç­–ç•¥æä¾›äº†é‡è¦å‚è€ƒï¼Œå¹¶å·²å¼€æº StyleBench ç›¸å…³èµ„æºã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20868v1",
      "published_date": "2025-09-25 08:00:39 UTC",
      "updated_date": "2025-09-25 08:00:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:24:55.300873+00:00"
    },
    {
      "arxiv_id": "2509.20867v1",
      "title": "Federated Markov Imputation: Privacy-Preserving Temporal Imputation in Multi-Centric ICU Environments",
      "title_zh": "Federated Markov Imputationï¼šå¤šä¸­å¿ƒ ICU ç¯å¢ƒä¸‹çš„éšç§ä¿æŠ¤æ—¶åºæ’è¡¥",
      "authors": [
        "Christoph DÃ¼sing",
        "Philipp Cimiano"
      ],
      "abstract": "Missing data is a persistent challenge in federated learning on electronic health records, particularly when institutions collect time-series data at varying temporal granularities. To address this, we propose Federated Markov Imputation (FMI), a privacy-preserving method that enables Intensive Care Units (ICUs) to collaboratively build global transition models for temporal imputation. We evaluate FMI on a real-world sepsis onset prediction task using the MIMIC-IV dataset and show that it outperforms local imputation baselines, especially in scenarios with irregular sampling intervals across ICUs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šä¸­å¿ƒé‡ç—‡ç›‘æŠ¤å®¤(ICU)ç¯å¢ƒä¸‹ç”µå­å¥åº·æ¡£æ¡ˆ(Electronic Health Records)åœ¨è”é‚¦å­¦ä¹ (Federated Learning)ä¸­æ™®éé¢ä¸´çš„æ•°æ®ç¼ºå¤±é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ä¸åŒæœºæ„é—´æ—¶é—´åºåˆ—æ•°æ®é‡‡é›†ç²’åº¦ä¸ä¸€è‡´çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Federated Markov Imputation (FMI)ï¼Œè¿™æ˜¯ä¸€ç§å…·å¤‡éšç§ä¿æŠ¤ç‰¹æ€§çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿè®©å„ICUåä½œæ„å»ºç”¨äºæ—¶é—´è¡¥å…¨çš„å…¨åŸŸè½¬ç§»æ¨¡å‹(Global Transition Models)ã€‚ç ”ç©¶äººå‘˜åœ¨MIMIC-IVæ•°æ®é›†ä¸Šé’ˆå¯¹è´¥è¡€ç—‡å‘ç—…é¢„æµ‹(Sepsis Onset Prediction)ä»»åŠ¡å¯¹FMIè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFMIåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå±€éƒ¨è¡¥å…¨(Local Imputation)åŸºå‡†æ¨¡å‹ã€‚å°¤å…¶æ˜¯åœ¨å„ICUé‡‡æ ·é—´éš”æä¸è§„åˆ™çš„å¤æ‚åœºæ™¯ä¸‹ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºäº†æ›´å¼ºçš„é²æ£’æ€§ä¸å‡†ç¡®æ€§ï¼Œä¸ºåŒ»ç–—å¤§æ•°æ®åä½œä¸­çš„æ—¶é—´åºåˆ—è¡¥å…¨æä¾›äº†é«˜æ•ˆä¸”ä¿æŠ¤éšç§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the 1st International ECML-PKDD Workshop-Tutorial on Learning on Real and Synthetic Medical Time Series Data (MED-TIME)",
      "pdf_url": "https://arxiv.org/pdf/2509.20867v1",
      "published_date": "2025-09-25 08:00:05 UTC",
      "updated_date": "2025-09-25 08:00:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:25:05.992194+00:00"
    },
    {
      "arxiv_id": "2509.20857v1",
      "title": "TasselNetV4: A vision foundation model for cross-scene, cross-scale, and cross-species plant counting",
      "title_zh": "TasselNetV4ï¼šé¢å‘è·¨åœºæ™¯ã€è·¨å°ºåº¦å’Œè·¨ç‰©ç§æ¤ç‰©è®¡æ•°çš„è§†è§‰åŸºç¡€æ¨¡å‹",
      "authors": [
        "Xiaonan Hu",
        "Xuebing Li",
        "Jinyu Xu",
        "Abdulkadir Duran Adan",
        "Letian Zhou",
        "Xuhui Zhu",
        "Yanan Li",
        "Wei Guo",
        "Shouyang Liu",
        "Wenzhong Liu",
        "Hao Lu"
      ],
      "abstract": "Accurate plant counting provides valuable information for agriculture such as crop yield prediction, plant density assessment, and phenotype quantification. Vision-based approaches are currently the mainstream solution. Prior art typically uses a detection or a regression model to count a specific plant. However, plants have biodiversity, and new cultivars are increasingly bred each year. It is almost impossible to exhaust and build all species-dependent counting models. Inspired by class-agnostic counting (CAC) in computer vision, we argue that it is time to rethink the problem formulation of plant counting, from what plants to count to how to count plants. In contrast to most daily objects with spatial and temporal invariance, plants are dynamic, changing with time and space. Their non-rigid structure often leads to worse performance than counting rigid instances like heads and cars such that current CAC and open-world detection models are suboptimal to count plants. In this work, we inherit the vein of the TasselNet plant counting model and introduce a new extension, TasselNetV4, shifting from species-specific counting to cross-species counting. TasselNetV4 marries the local counting idea of TasselNet with the extract-and-match paradigm in CAC. It builds upon a plain vision transformer and incorporates novel multi-branch box-aware local counters used to enhance cross-scale robustness. Two challenging datasets, PAC-105 and PAC-Somalia, are harvested. Extensive experiments against state-of-the-art CAC models show that TasselNetV4 achieves not only superior counting performance but also high efficiency.Our results indicate that TasselNetV4 emerges to be a vision foundation model for cross-scene, cross-scale, and cross-species plant counting.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TasselNetV4ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ¤ç‰©è®¡æ•°æ¨¡å‹éš¾ä»¥åº”å¯¹ç”Ÿç‰©å¤šæ ·æ€§ã€éåˆšæ€§ç»“æ„ä»¥åŠè·¨ç‰©ç§è®¡æ•°çš„å±€é™æ€§ã€‚TasselNetV4 å°† TasselNet çš„å±€éƒ¨è®¡æ•°æ€æƒ³ä¸ç±»ä¸å¯çŸ¥è®¡æ•°(Class-Agnostic Counting)ä¸­çš„â€œæå–å¹¶åŒ¹é…(extract-and-match)â€èŒƒå¼ç›¸ç»“åˆã€‚è¯¥æ¨¡å‹åŸºäº Vision Transformer æ„å»ºï¼Œå¹¶å¼•å…¥äº†æ–°å‹çš„å¤šåˆ†æ”¯æ¡†æ„ŸçŸ¥å±€éƒ¨è®¡æ•°å™¨(multi-branch box-aware local counters)ï¼Œæ˜¾è‘—å¢å¼ºäº†è·¨å°ºåº¦è®¡æ•°çš„é²æ£’æ€§ã€‚ç ”ç©¶å›¢é˜Ÿä¸ºæ­¤æ„å»ºäº† PAC-105 å’Œ PAC-Somalia ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†è¿›è¡Œè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTasselNetV4 åœ¨è®¡æ•°æ€§èƒ½å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰çš„ SOTA æ¨¡å‹ã€‚è¯¥æˆæœè¯æ˜äº† TasselNetV4 èƒ½å¤Ÿä½œä¸ºä¸€ç§è·¨åœºæ™¯ã€è·¨å°ºåº¦å’Œè·¨ç‰©ç§æ¤ç‰©è®¡æ•°çš„è§†è§‰åŸºç¡€æ¨¡å‹(vision foundation model)ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 figures, 7 tables, code is available at https://github.com/tiny-smart/tasselnetv4",
      "pdf_url": "https://arxiv.org/pdf/2509.20857v1",
      "published_date": "2025-09-25 07:48:29 UTC",
      "updated_date": "2025-09-25 07:48:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:25:09.885276+00:00"
    },
    {
      "arxiv_id": "2509.25236v1",
      "title": "The Causal Abstraction Network: Theory and Learning",
      "title_zh": "å› æœæŠ½è±¡ç½‘ç»œï¼šç†è®ºä¸å­¦ä¹ ",
      "authors": [
        "Gabriele D'Acunto",
        "Paolo Di Lorenzo",
        "Sergio Barbarossa"
      ],
      "abstract": "Causal artificial intelligence aims to enhance explainability, trustworthiness, and robustness in AI by leveraging structural causal models (SCMs). In this pursuit, recent advances formalize network sheaves of causal knowledge. Pushing in the same direction, we introduce the causal abstraction network (CAN), a specific instance of such sheaves where (i) SCMs are Gaussian, (ii) restriction maps are transposes of constructive linear causal abstractions (CAs), and (iii) edge stalks correspond -- up to rotation -- to the node stalks of more detailed SCMs. We investigate the theoretical properties of CAN, including algebraic invariants, cohomology, consistency, global sections characterized via the Laplacian kernel, and smoothness. We then tackle the learning of consistent CANs. Our problem formulation separates into edge-specific local Riemannian problems and avoids nonconvex, costly objectives. We propose an efficient search procedure as a solution, solving the local problems with SPECTRAL, our iterative method with closed-form updates and suitable for positive definite and semidefinite covariance matrices. Experiments on synthetic data show competitive performance in the CA learning task, and successful recovery of diverse CAN structures.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†å› æœæŠ½è±¡ç½‘ç»œ(Causal Abstraction Network, CAN)ï¼Œä½œä¸ºå› æœçŸ¥è¯†å±‚(network sheaves of causal knowledge)çš„ä¸€ä¸ªå…·ä½“å®ä¾‹ï¼Œæ—¨åœ¨é€šè¿‡ç»“æ„å› æœæ¨¡å‹(SCMs)æå‡äººå·¥æ™ºèƒ½çš„å¯è§£é‡Šæ€§ä¸é²æ£’æ€§ã€‚åœ¨ CAN æ¡†æ¶ä¸‹ï¼ŒSCMs å‘ˆç°ä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œè€Œé™åˆ¶æ˜ å°„ç”±æ„é€ æ€§çº¿æ€§å› æœæŠ½è±¡(CAs)çš„è½¬ç½®å®šä¹‰ã€‚ç ”ç©¶ç³»ç»Ÿæ€§åœ°æ¢è®¨äº† CAN çš„ä»£æ•°ä¸å˜é‡ã€ä¸ŠåŒè°ƒ(cohomology)åŠä¸€è‡´æ€§ç­‰ç†è®ºå±æ€§ï¼Œå¹¶åˆ©ç”¨æ‹‰æ™®æ‹‰æ–¯æ ¸(Laplacian kernel)è¡¨å¾å…¶å…¨å±€æˆªé¢ã€‚é’ˆå¯¹ä¸€è‡´æ€§ CAN çš„å­¦ä¹ é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å°†æ•´ä½“ä»»åŠ¡åˆ†è§£ä¸ºå±€éƒ¨é»æ›¼(Riemannian)é—®é¢˜çš„å…¬å¼åŒ–æ–¹æ³•ï¼Œæœ‰æ•ˆè§„é¿äº†é«˜æ˜‚çš„éå‡¸ä¼˜åŒ–æˆæœ¬ã€‚é€šè¿‡åä¸º SPECTRAL çš„è¿­ä»£é—­å¼æ›´æ–°ç®—æ³•ï¼Œè¯¥ç ”ç©¶å®ç°äº†å¯¹å±€éƒ¨é—®é¢˜çš„å¿«é€Ÿæ±‚è§£ã€‚å®éªŒè¯æ˜ï¼ŒCAN åœ¨å› æœæŠ½è±¡å­¦ä¹ ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿå‡†ç¡®æ¢å¤å¤æ‚çš„ç½‘ç»œç»“æ„ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25236v1",
      "published_date": "2025-09-25 07:48:25 UTC",
      "updated_date": "2025-09-25 07:48:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:25:17.800752+00:00"
    },
    {
      "arxiv_id": "2509.25235v1",
      "title": "Machine Learning for Pattern Detection in Printhead Nozzle Logging",
      "title_zh": "ç”¨äºæ‰“å°å¤´å–·å˜´æ—¥å¿—æ¨¡å¼æ£€æµ‹çš„æœºå™¨å­¦ä¹ ",
      "authors": [
        "Nikola Prianikov",
        "Evelyne Janssen-van Dam",
        "Marcin Pietrasik",
        "Charalampos S. Kouzinopoulos"
      ],
      "abstract": "Correct identification of failure mechanisms is essential for manufacturers to ensure the quality of their products. Certain failures of printheads developed by Canon Production Printing can be identified from the behavior of individual nozzles, the states of which are constantly recorded and can form distinct patterns in terms of the number of failed nozzles over time, and in space in the nozzle grid. In our work, we investigate the problem of printhead failure classification based on a multifaceted dataset of nozzle logging and propose a Machine Learning classification approach for this problem. We follow the feature-based framework of time-series classification, where a set of time-based and spatial features was selected with the guidance of domain experts. Several traditional ML classifiers were evaluated, and the One-vs-Rest Random Forest was found to have the best performance. The proposed model outperformed an in-house rule-based baseline in terms of a weighted F1 score for several failure mechanisms.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Canon Production Printing å¼€å‘çš„æ‰“å°å¤´æ•…éšœæœºåˆ¶è¯†åˆ«é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäº Machine Learning çš„åˆ†ç±»æ–¹æ³•ã€‚ç”±äºæ‰“å°å¤´çš„æ•…éšœå¯ä»¥é€šè¿‡å•ä¸ªå–·å˜´åœ¨æ—¶é—´å’Œç©ºé—´ç½‘æ ¼ä¸Šå½¢æˆçš„çŠ¶æ€æ¨¡å¼è¿›è¡Œè¯†åˆ«ï¼Œä½œè€…åˆ©ç”¨å–·å˜´æ—¥å¿—ï¼ˆnozzle loggingï¼‰æ„å»ºäº†ä¸€ä¸ªå¤šç»´æ•°æ®é›†ã€‚ç ”ç©¶é‡‡ç”¨äº†åŸºäºç‰¹å¾çš„ time-series classification æ¡†æ¶ï¼Œå¹¶åœ¨é¢†åŸŸä¸“å®¶çš„æŒ‡å¯¼ä¸‹æå–äº†ä¸€ç³»åˆ—æ—¶é—´ä¸ç©ºé—´ç‰¹å¾ã€‚åœ¨è¯„ä¼°äº†å¤šç§ä¼ ç»Ÿåˆ†ç±»å™¨åï¼Œå‘ç° One-vs-Rest Random Forest æ¨¡å‹åœ¨æ•…éšœæ¨¡å¼è¯†åˆ«ä¸­å…·æœ‰æœ€ä½³è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæ•…éšœæœºåˆ¶ä¸Šçš„ weighted F1 score å‡ä¼˜äºç°æœ‰çš„å†…éƒ¨ rule-based baselineã€‚è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆé¢†åŸŸçŸ¥è¯†çš„è‡ªåŠ¨åŒ–æ¨¡å¼æ£€æµ‹åœ¨æå‡æ‰“å°å¤´è´¨é‡æ§åˆ¶å’Œæ•…éšœè¯Šæ–­æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper has been published in the 37th International Conference on Tools with Artificial Intelligence in Athens, Greece, November 03-05, 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.25235v1",
      "published_date": "2025-09-25 07:41:10 UTC",
      "updated_date": "2025-09-25 07:41:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:25:20.767949+00:00"
    },
    {
      "arxiv_id": "2509.20852v1",
      "title": "FHRFormer: A Self-supervised Transformer Approach for Fetal Heart Rate Inpainting and Forecasting",
      "title_zh": "FHRFormerï¼šç”¨äºèƒå¿ƒç‡è¡¥å…¨ä¸é¢„æµ‹çš„è‡ªç›‘ç£ Transformer æ–¹æ³•",
      "authors": [
        "Kjersti Engan",
        "Neel Kanwal",
        "Anita Yeconia",
        "Ladislaus Blacy",
        "Yuda Munyaw",
        "Estomih Mduma",
        "Hege Ersdal"
      ],
      "abstract": "Approximately 10\\% of newborns require assistance to initiate breathing at birth, and around 5\\% need ventilation support. Fetal heart rate (FHR) monitoring plays a crucial role in assessing fetal well-being during prenatal care, enabling the detection of abnormal patterns and supporting timely obstetric interventions to mitigate fetal risks during labor. Applying artificial intelligence (AI) methods to analyze large datasets of continuous FHR monitoring episodes with diverse outcomes may offer novel insights into predicting the risk of needing breathing assistance or interventions. Recent advances in wearable FHR monitors have enabled continuous fetal monitoring without compromising maternal mobility. However, sensor displacement during maternal movement, as well as changes in fetal or maternal position, often lead to signal dropouts, resulting in gaps in the recorded FHR data. Such missing data limits the extraction of meaningful insights and complicates automated (AI-based) analysis. Traditional approaches to handle missing data, such as simple interpolation techniques, often fail to preserve the spectral characteristics of the signals. In this paper, we propose a masked transformer-based autoencoder approach to reconstruct missing FHR signals by capturing both spatial and frequency components of the data. The proposed method demonstrates robustness across varying durations of missing data and can be used for signal inpainting and forecasting. The proposed approach can be applied retrospectively to research datasets to support the development of AI-based risk algorithms. In the future, the proposed method could be integrated into wearable FHR monitoring devices to achieve earlier and more robust risk detection.",
      "tldr_zh": "èƒå„¿å¿ƒç‡(Fetal Heart Rate, FHR)ç›‘æµ‹åœ¨äº§å‰æŠ¤ç†ä¸­å¯¹è¯„ä¼°èƒå„¿å¥åº·è‡³å…³é‡è¦ï¼Œä½†å¯ç©¿æˆ´ç›‘æ§è®¾å¤‡å¸¸å› æ¯ä½“æ´»åŠ¨æˆ–ä¼ æ„Ÿå™¨ç§»ä½å¯¼è‡´ä¿¡å·ä¸¢å¤±(signal dropouts)ï¼Œä¼ ç»Ÿçš„ç®€å•æ’å€¼æŠ€æœ¯éš¾ä»¥æœ‰æ•ˆä¿ç•™ä¿¡å·çš„é¢‘è°±ç‰¹å¾ã€‚è¯¥ç ”ç©¶æå‡ºäº†FHRFormerï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè‡ªç›‘ç£Transformerçš„å±è”½è‡ªåŠ¨ç¼–ç å™¨(masked transformer-based autoencoder)æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºFHRä¿¡å·çš„ä¿®å¤(inpainting)å’Œé¢„æµ‹(forecasting)ã€‚è¯¥æ–¹æ³•é€šè¿‡æ•è·æ•°æ®çš„ç©ºé—´å’Œé¢‘ç‡æˆåˆ†(spatial and frequency components)æ¥é‡å»ºç¼ºå¤±ä¿¡å·ï¼Œåœ¨å¤„ç†ä¸åŒæ—¶é•¿çš„ä¿¡å·ç¼ºå£æ—¶å±•ç°å‡ºæ˜¾è‘—çš„é²æ£’æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒFHRFormerèƒ½å¤Ÿæœ‰æ•ˆæå‡ç°æœ‰ç ”ç©¶æ•°æ®é›†çš„è´¨é‡ï¼Œå¹¶ä¸ºå¼€å‘åŸºäºAIçš„é£é™©é¢„æµ‹ç®—æ³•æä¾›æ”¯æŒã€‚æœªæ¥è¯¥æŠ€æœ¯æœ‰æœ›é›†æˆè‡³ç§»åŠ¨ç«¯ç›‘æµ‹è®¾å¤‡ä¸­ï¼Œé€šè¿‡å®ç°æ›´æ—©ã€æ›´ç¨³å¥çš„èƒå„¿é£é™©æ£€æµ‹ï¼Œè¾…åŠ©ä¸´åºŠå†³ç­–å¹¶é™ä½åˆ†å¨©è¿‡ç¨‹ä¸­çš„æ½œåœ¨é£é™©ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to IEEE JBHI",
      "pdf_url": "https://arxiv.org/pdf/2509.20852v1",
      "published_date": "2025-09-25 07:40:21 UTC",
      "updated_date": "2025-09-25 07:40:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:25:34.729429+00:00"
    },
    {
      "arxiv_id": "2509.20842v1",
      "title": "Robust Multi-Omics Integration from Incomplete Modalities Significantly Improves Prediction of Alzheimer's Disease",
      "title_zh": "åŸºäºç¼ºå¤±æ¨¡æ€çš„ç¨³å¥å¤šç»„å­¦æ•´åˆæ˜¾è‘—æå‡é˜¿å°”èŒ¨æµ·é»˜ç—…é¢„æµ‹",
      "authors": [
        "Sungjoon Park",
        "Kyungwook Lee",
        "Soorin Yim",
        "Doyeong Hwang",
        "Dongyun Kim",
        "Soonyoung Lee",
        "Amy Dunn",
        "Daniel Gatti",
        "Elissa Chesler",
        "Kristen O'Connell",
        "Kiyoung Kim"
      ],
      "abstract": "Multi-omics data capture complex biomolecular interactions and provide insights into metabolism and disease. However, missing modalities hinder integrative analysis across heterogeneous omics. To address this, we present MOIRA (Multi-Omics Integration with Robustness to Absent modalities), an early integration method enabling robust learning from incomplete omics data via representation alignment and adaptive aggregation. MOIRA leverages all samples, including those with missing modalities, by projecting each omics dataset onto a shared embedding space where a learnable weighting mechanism fuses them. Evaluated on the Religious Order Study and Memory and Aging Project (ROSMAP) dataset for Alzheimer's Disease (AD), MOIRA outperformed existing approaches, and further ablation studies confirmed modality-wise contributions. Feature importance analysis revealed AD-related biomarkers consistent with prior literature, highlighting the biological relevance of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MOIRAï¼ˆMulti-Omics Integration with Robustness to Absent modalitiesï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¤„ç†ä¸å®Œæ•´ç»„å­¦æ•°æ®çš„æ—©æœŸé›†æˆæ–¹æ³•ï¼Œç”¨äºæé«˜å¯¹Alzheimer's Diseaseï¼ˆADï¼‰çš„é¢„æµ‹ç²¾åº¦ã€‚é’ˆå¯¹å¤šç»„å­¦æ•°æ®ä¸­å¸¸è§çš„æ¨¡æ€ç¼ºå¤±é—®é¢˜ï¼ŒMOIRAé€šè¿‡è¡¨ç¤ºå¯¹é½ï¼ˆrepresentation alignmentï¼‰å’Œè‡ªé€‚åº”èšåˆï¼ˆadaptive aggregationï¼‰å®ç°äº†ä»å¼‚æ„ç»„å­¦æ•°æ®ä¸­çš„é²æ£’å­¦ä¹ ã€‚è¯¥æ–¹æ³•å°†å„ä¸ªç»„å­¦æ•°æ®é›†æ˜ å°„åˆ°ä¸€ä¸ªå…±äº«åµŒå…¥ç©ºé—´ï¼ˆshared embedding spaceï¼‰ï¼Œå¹¶åˆ©ç”¨å¯å­¦ä¹ çš„åŠ æƒæœºåˆ¶è¿›è¡Œèåˆï¼Œä»è€Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨åŒ…å«ç¼ºå¤±æ¨¡æ€åœ¨å†…çš„æ‰€æœ‰æ ·æœ¬ã€‚åœ¨Religious Order Study and Memory and Aging Projectï¼ˆROSMAPï¼‰æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒMOIRAçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ¶ˆèå®éªŒå’Œç‰¹å¾é‡è¦æ€§åˆ†æè¿›ä¸€æ­¥è¯å®äº†å„ç»„å­¦æ¨¡æ€çš„è´¡çŒ®ï¼Œå¹¶è¯†åˆ«å‡ºä¸æ—¢å¾€æ–‡çŒ®ä¸€è‡´çš„ADç›¸å…³ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨ç”Ÿç‰©å­¦ä¸Šçš„ç›¸å…³æ€§ä¸åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20842v1",
      "published_date": "2025-09-25 07:29:46 UTC",
      "updated_date": "2025-09-25 07:29:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:25:26.946008+00:00"
    },
    {
      "arxiv_id": "2509.20841v1",
      "title": "ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation",
      "title_zh": "ImaginationPolicyï¼šè¿ˆå‘æ³›åŒ–ã€ç²¾å‡†ä¸”å¯é çš„æœºå™¨äººæ“ä½œç«¯åˆ°ç«¯ç­–ç•¥",
      "authors": [
        "Dekun Lu",
        "Wei Gao",
        "Kui Jia"
      ],
      "abstract": "End-to-end robot manipulation policies offer significant potential for enabling embodied agents to understand and interact with the world. Unlike traditional modular pipelines, end-to-end learning mitigates key limitations such as information loss between modules and feature misalignment caused by isolated optimization targets. Despite these advantages, existing end-to-end neural networks for robotic manipulation--including those based on large VLM/VLA models--remain insufficiently performant for large-scale practical deployment. In this paper, we take a step towards an end-to-end manipulation policy that is generalizable, accurate and reliable. To achieve this goal, we propose a novel Chain of Moving Oriented Keypoints (CoMOK) formulation for robotic manipulation. Our formulation is used as the action representation of a neural policy, which can be trained in an end-to-end fashion. Such an action representation is general, as it extends the standard end-effector pose action representation and supports a diverse set of manipulation tasks in a unified manner. The oriented keypoint in our method enables natural generalization to objects with different shapes and sizes, while achieving sub-centimeter accuracy. Moreover, our formulation can easily handle multi-stage tasks, multi-modal robot behaviors, and deformable objects. Extensive simulated and hardware experiments demonstrate the effectiveness of our method.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ç°æœ‰æœºå™¨äººæ“æ§ç«¯åˆ°ç«¯ç­–ç•¥åœ¨å¤§è§„æ¨¡å®é™…éƒ¨ç½²ä¸­æ€§èƒ½ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº† ImaginationPolicyï¼Œæ—¨åœ¨æ„å»ºä¸€ä¸ªé€šç”¨ã€ç²¾ç¡®ä¸”å¯é çš„æ“æ§ç³»ç»Ÿã€‚è¯¥ç ”ç©¶æ ¸å¿ƒå¼•å…¥äº†â€œç§»åŠ¨å¯¼å‘å…³é”®ç‚¹é“¾â€(Chain of Moving Oriented Keypoints, CoMOK) ä½œä¸ºç¥ç»ç½‘ç»œç­–ç•¥çš„åŠ¨ä½œè¡¨ç¤ºï¼Œèƒ½å¤Ÿä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œè®­ç»ƒå¹¶æ”¯æŒå¤šç§æ“æ§ä»»åŠ¡ã€‚CoMOK æ‰©å±•äº†ä¼ ç»Ÿçš„æœ«ç«¯æ‰§è¡Œå™¨ä½å§¿(end-effector pose)è¡¨ç¤ºï¼Œåˆ©ç”¨å¯¼å‘å…³é”®ç‚¹å®ç°äº†å¯¹ä¸åŒå½¢çŠ¶å’Œå°ºå¯¸ç‰©ä½“çš„è‡ªç„¶æ³›åŒ–ï¼ŒåŒæ—¶è¾¾åˆ°äº†äºšå˜ç±³çº§(sub-centimeter)çš„ç²¾åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å…·å¤‡å¤„ç†å¤šé˜¶æ®µä»»åŠ¡ã€å¤šæ¨¡æ€æœºå™¨äººè¡Œä¸ºä»¥åŠå˜å½¢ä½“(deformable objects)çš„èƒ½åŠ›ã€‚é€šè¿‡å¤§é‡çš„ä»¿çœŸå’Œç¡¬ä»¶å®éªŒï¼Œç ”ç©¶è¯æ˜äº† ImaginationPolicy åœ¨å¤æ‚ç¯å¢ƒä¸‹å…·æœ‰å“è¶Šçš„æœ‰æ•ˆæ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "First two authors contribute equally. Project page: https://sites.google.com/view/imaginationpolicy",
      "pdf_url": "https://arxiv.org/pdf/2509.20841v1",
      "published_date": "2025-09-25 07:29:07 UTC",
      "updated_date": "2025-09-25 07:29:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:25:26.667123+00:00"
    },
    {
      "arxiv_id": "2510.03243v2",
      "title": "Prompt-Aware Scheduling for Low-Latency LLM Serving",
      "title_zh": "é¢å‘ä½å»¶è¿Ÿ LLM æœåŠ¡çš„ Prompt æ„ŸçŸ¥è°ƒåº¦",
      "authors": [
        "Yiheng Tao",
        "Yihe Zhang",
        "Matthew T. Dearing",
        "Xin Wang",
        "Yuping Fan",
        "Zhiling Lan"
      ],
      "abstract": "Efficient scheduling of LLM inference tasks is essential for achieving low latency and high throughput, particularly with the growing use of reasoning-capable LLMs. Traditional strategies like First-Come-First-Serve (FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks delay shorter ones queued behind them. In this paper, we introduce PARS, a prompt-aware LLM task scheduler that improves serving efficiency by approximating shortest-job-first (SJF) scheduling through pairwise ranking with margin ranking loss. PARS focuses on impactful scheduling decisions and is seamlessly integrated into the state-of-the-art LLM serving system vLLM. It effectively predicts response-length-based task ordering, reducing latency with minimal overhead. Extensive experiments across multiple LLMs and real-world inference datasets show that PARS significantly improves performance, including for reasoning workloads. Furthermore, our cross-model evaluations demonstrate that the design generalizes well, enabling effective scheduling even when predictors are trained on different LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PARSï¼Œä¸€ç§æ„ŸçŸ¥æç¤ºè¯çš„ LLM ä»»åŠ¡è°ƒåº¦å™¨ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿ First-Come-First-Serve (FCFS) ç­–ç•¥ä¸­å¸¸è§çš„ Head-of-Line (HOL) é˜»å¡é—®é¢˜ã€‚PARS é€šè¿‡ä½¿ç”¨å¸¦æœ‰ margin ranking loss çš„æˆå¯¹æ’åºæ–¹æ³•æ¥è¿‘ä¼¼ Shortest-Job-First (SJF) è°ƒåº¦ï¼Œèƒ½å¤Ÿç²¾å‡†é¢„æµ‹åŸºäºå“åº”é•¿åº¦çš„ä»»åŠ¡æ‰§è¡Œé¡ºåºã€‚è¯¥ç®—æ³•è¢«æ— ç¼é›†æˆåˆ° vLLM æ¨ç†ç³»ç»Ÿä¸­ï¼Œä»¥æä½çš„è®¡ç®—å¼€é”€æ˜¾è‘—ä¼˜åŒ–äº†ä»»åŠ¡è°ƒåº¦æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPARS åœ¨å¤šç§ LLM å’ŒçœŸå®æ¨ç†æ•°æ®é›†ä¸Šå‡èƒ½å¤§å¹…é™ä½æœåŠ¡å»¶è¿Ÿï¼Œå°¤å…¶åœ¨å¤„ç†å¤æ‚æ¨ç†å·¥ä½œè´Ÿè½½æ—¶è¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼Œè·¨æ¨¡å‹è¯„ä¼°è¿›ä¸€æ­¥è¯æ˜äº†è¯¥æ–¹æ¡ˆå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶åœ¨ä¸åŒæ¨¡å‹ç¯å¢ƒä¸‹å‡èƒ½ä¿æŒé«˜æ•ˆçš„è°ƒåº¦æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03243v2",
      "published_date": "2025-09-25 07:26:38 UTC",
      "updated_date": "2025-10-10 04:42:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:25:46.663992+00:00"
    },
    {
      "arxiv_id": "2509.20837v1",
      "title": "Verification Limits Code LLM Training",
      "title_zh": "éªŒè¯æœºåˆ¶åˆ¶çº¦ä»£ç å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒ",
      "authors": [
        "Srishti Gureja",
        "Elena Tommasone",
        "Jingyi He",
        "Sara Hooker",
        "Matthias GallÃ©",
        "Marzieh Fadaee"
      ],
      "abstract": "Large language models for code generation increasingly rely on synthetic data, where both problem solutions and verification tests are generated by models. While this enables scalable data creation, it introduces a previously unexplored bottleneck: the verification ceiling, in which the quality and diversity of training data are fundamentally constrained by the capabilities of synthetic verifiers. In this work, we systematically study how verification design and strategies influence model performance. We investigate (i) what we verify by analyzing the impact of test complexity and quantity: richer test suites improve code generation capabilities (on average +3 pass@1), while quantity alone yields diminishing returns, (ii) how we verify by exploring relaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By allowing for relaxed thresholds or incorporating LLM-based soft verification, we can recover valuable training data, leading to a 2-4 point improvement in pass@1 performance. However, this benefit is contingent upon the strength and diversity of the test cases used, and (iii) why verification remains necessary through controlled comparisons of formally correct versus incorrect solutions and human evaluation: retaining diverse correct solutions per problem yields consistent generalization gains. Our results show that Verification as currently practiced is too rigid, filtering out valuable diversity. But it cannot be discarded, only recalibrated. By combining calibrated verification with diverse, challenging problem-solution pairs, we outline a path to break the verification ceiling and unlock stronger code generation models.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†åœ¨åˆ©ç”¨åˆæˆæ•°æ®è®­ç»ƒä»£ç å¤§è¯­è¨€æ¨¡å‹ (Code LLMs) æ—¶é‡åˆ°çš„â€œéªŒè¯å¤©èŠ±æ¿â€ (verification ceiling) é—®é¢˜ï¼Œå³è®­ç»ƒæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§å—é™äºåˆæˆéªŒè¯å™¨ (synthetic verifiers) çš„èƒ½åŠ›ã€‚ä½œè€…é€šè¿‡åˆ†æå‘ç°ï¼Œæµ‹è¯•å¥—ä»¶çš„å¤æ‚åº¦å’Œä¸°å¯Œåº¦å¯¹æå‡ä»£ç ç”Ÿæˆèƒ½åŠ›è‡³å…³é‡è¦ï¼Œç›¸æ¯”ä¹‹ä¸‹å•çº¯å¢åŠ æµ‹è¯•æ•°é‡çš„æ”¶ç›Šåˆ™ä¼šè¿…é€Ÿé€’å‡ã€‚ç ”ç©¶è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„ 100% é€šè¿‡ç‡å‡†åˆ™è¿‡äºåƒµåŒ–ï¼Œè€Œé€šè¿‡å¼•å…¥å®½æ¾çš„é€šè¿‡é˜ˆå€¼æˆ–åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è½¯éªŒè¯ (soft verification) èƒ½å¤Ÿæœ‰æ•ˆæ‰¾å›æœ‰ä»·å€¼çš„è®­ç»ƒæ•°æ®ï¼Œä½¿ pass@1 æ€§èƒ½æå‡ 2-4 ä¸ªç™¾åˆ†ç‚¹ã€‚å®éªŒè¯æ˜ï¼Œåœ¨è®­ç»ƒé›†ä¸­ä¿ç•™é’ˆå¯¹åŒä¸€é—®é¢˜çš„å¤šæ ·åŒ–æ­£ç¡®è§£æ³•å¯¹äºæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å…·æœ‰æ˜¾è‘—å¢ç›Šã€‚æœ€ç»ˆç»“è®ºè®¤ä¸ºï¼Œå½“å‰çš„éªŒè¯æœºåˆ¶éœ€è¦ä»è¿‡åº¦è¿‡æ»¤è½¬å‘æ ¡å‡†åçš„éªŒè¯ (calibrated verification)ï¼Œå¹¶ç»“åˆå¤šæ ·ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜è§£æ³•å¯¹ï¼Œä»¥æ‰“ç ´éªŒè¯ç“¶é¢ˆå¹¶è§£é”æ›´å¼ºå¤§çš„ä»£ç ç”Ÿæˆæ¨¡å‹ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20837v1",
      "published_date": "2025-09-25 07:23:30 UTC",
      "updated_date": "2025-09-25 07:23:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:25:50.768413+00:00"
    },
    {
      "arxiv_id": "2509.20835v2",
      "title": "Security-aware Semantic-driven ISAC via Paired Adversarial Residual Networks",
      "title_zh": "åŸºäºé…å¯¹å¯¹æŠ—æ®‹å·®ç½‘ç»œçš„è¯­ä¹‰é©±åŠ¨å®‰å…¨æ„ŸçŸ¥é€šæ„Ÿä¸€ä½“åŒ–",
      "authors": [
        "Yu Liu",
        "Boxiang He",
        "Fanggang Wang"
      ],
      "abstract": "This paper proposes a novel and flexible security-aware semantic-driven integrated sensing and communication (ISAC) framework, namely security semantic ISAC (SS-ISAC). Inspired by the positive impact of the adversarial attack, a pair of pluggable encryption and decryption modules is designed in the proposed SS-ISAC framework. The encryption module is installed after the semantic transmitter, adopting a trainable adversarial residual network (ARN) to create the adversarial attack. Correspondingly, the decryption module before the semantic receiver utilizes another trainable ARN to mitigate the adversarial attack and noise. These two modules can be flexibly assembled considering the system security demands, without drastically modifying the hardware infrastructure. To ensure the sensing and communication (SAC) performance while preventing the eavesdropping threat, the above ARNs are jointly optimized by minimizing a carefully designed loss function that relates to the adversarial attack power, SAC performance, as well as the privacy leakage risk. Simulation results validate the effectiveness of the proposed SS-ISAC framework in terms of both SAC and eavesdropping prevention performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºSS-ISACçš„æ–°å‹çµæ´»å®‰å…¨æ„ŸçŸ¥è¯­ä¹‰é©±åŠ¨æ„ŸçŸ¥é€šä¿¡ä¸€ä½“åŒ–(Integrated Sensing and Communication)æ¡†æ¶ã€‚å—å¯¹æŠ—æ€§æ”»å‡»(Adversarial Attack)å¯å‘ï¼Œè¯¥æ¡†æ¶åœ¨è¯­ä¹‰å‘å°„å™¨åå’Œæ¥æ”¶å™¨å‰åˆ†åˆ«è®¾è®¡äº†ä¸€å¯¹å¯æ’æ‹”çš„åŠ å¯†å’Œè§£å¯†æ¨¡å—ã€‚è¿™äº›æ¨¡å—é‡‡ç”¨å¯è®­ç»ƒçš„å¯¹æŠ—æ®‹å·®ç½‘ç»œ(Adversarial Residual Network, ARN)ï¼Œé€šè¿‡åˆ¶é€ å¹¶éšåç¼“è§£å¯¹æŠ—æ€§æ”»å‡»ä¸å™ªå£°ï¼Œåœ¨ä¸å¤§å¹…ä¿®æ”¹ç¡¬ä»¶åŸºç¡€è®¾æ–½çš„å‰æä¸‹æå‡ç³»ç»Ÿå®‰å…¨æ€§ã€‚ç ”ç©¶é€šè¿‡æœ€å°åŒ–ä¸€ä¸ªç»“åˆäº†å¯¹æŠ—æ”»å‡»åŠŸç‡ã€æ„ŸçŸ¥ä¸é€šä¿¡(SAC)æ€§èƒ½ä»¥åŠéšç§æ³„éœ²é£é™©çš„æŸå¤±å‡½æ•°ï¼Œå®ç°äº†å¯¹ARNçš„è”åˆä¼˜åŒ–ã€‚ä»¿çœŸç»“æœéªŒè¯äº†SS-ISACæ¡†æ¶åœ¨ä¿éšœæ„ŸçŸ¥é€šä¿¡æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½æœ‰æ•ˆåº”å¯¹çªƒå¬å¨èƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "This paper contains errors, including insufficient technical elaboration, incorrect expression of PSR, lack of benchmark, and incomplete simulation justification. We politely request withdrawal of the current version (v1) to prevent misunderstanding",
      "pdf_url": "https://arxiv.org/pdf/2509.20835v2",
      "published_date": "2025-09-25 07:22:33 UTC",
      "updated_date": "2025-11-10 14:34:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:25:56.983788+00:00"
    },
    {
      "arxiv_id": "2509.20830v1",
      "title": "Trustworthy Semantic Communication for Vehicular Networks: Challenges and Solutions",
      "title_zh": "é¢å‘è½¦è”ç½‘çš„å¯ä¿¡è¯­ä¹‰é€šä¿¡ï¼šæŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ",
      "authors": [
        "Yanghe Pan",
        "Yuntao Wang",
        "Shaolong Guo",
        "Chengyu Yin",
        "Ruidong Li",
        "Zhou Su",
        "Yuan Wu"
      ],
      "abstract": "Semantic communication (SemCom) has the potential to significantly reduce communication delay in vehicle-to-everything (V2X) communications within vehicular networks (VNs). However, the deployment of vehicular SemCom networks (VN-SemComNets) faces critical trust challenges in information transmission, semantic encoding, and communication entity reliability. This paper proposes an innovative three-layer trustworthy VN-SemComNet architecture. Specifically, we introduce a semantic camouflage transmission mechanism leveraging defensive adversarial noise for active eavesdropping defense, a robust federated encoder-decoder training framework to mitigate encoder-decoder poisoning attacks, and an audit game-based distributed vehicle trust management mechanism to deter untrustworthy vehicles. A case study validates the effectiveness of the proposed solutions. Lastly, essential future research directions are pointed out to advance this emerging field.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è½¦è½½ç½‘ç»œ(VNs)ä¸­è¯­ä¹‰é€šä¿¡(SemCom)åœ¨è½¦è”ç½‘(V2X)åº”ç”¨åœºæ™¯ä¸‹é¢ä¸´çš„ä¿¡æ¯ä¼ è¾“ã€è¯­ä¹‰ç¼–ç åŠå®ä½“å¯é æ€§ç­‰å…³é”®ä¿¡ä»»æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„ä¸‰å±‚å¯ä¿¡è½¦è½½è¯­ä¹‰é€šä¿¡ç½‘ç»œ(VN-SemComNet)æ¶æ„ï¼Œæ—¨åœ¨æ„å»ºå…¨æ–¹ä½çš„å®‰å…¨ä¿éšœä½“ç³»ã€‚è¯¥æ¶æ„å…·ä½“åŒ…å«ä¸‰ç§å…³é”®æœºåˆ¶ï¼šåˆ©ç”¨é˜²å¾¡æ€§å¯¹æŠ—å™ªå£°å®ç°çš„è¯­ä¹‰ä¼ªè£…ä¼ è¾“æœºåˆ¶ä»¥é˜²å¾¡ä¸»åŠ¨çªƒå¬ï¼Œç”¨äºå‡è½»ç¼–è§£ç å™¨ä¸­æ¯’æ”»å‡»çš„ç¨³å¥è”é‚¦ç¼–è§£ç å™¨è®­ç»ƒæ¡†æ¶ï¼Œä»¥åŠåŸºäºå®¡è®¡åšå¼ˆ(Audit Game)çš„åˆ†å¸ƒå¼è½¦è¾†ä¿¡ä»»ç®¡ç†æœºåˆ¶ã€‚æ¡ˆä¾‹ç ”ç©¶ç»“æœéªŒè¯äº†æ‰€ææ–¹æ¡ˆåœ¨æå‡ç³»ç»Ÿå®‰å…¨æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ¨åŠ¨å¯ä¿¡è¯­ä¹‰é€šä¿¡æŠ€æœ¯åœ¨è½¦è½½ç½‘ç»œç¯å¢ƒä¸­çš„éƒ¨ç½²æä¾›äº†ç†è®ºåŸºç¡€å’Œå®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "8 pages, 8 figures, accepted by IEEE Vehicular Technology Magazine",
      "pdf_url": "https://arxiv.org/pdf/2509.20830v1",
      "published_date": "2025-09-25 07:18:20 UTC",
      "updated_date": "2025-09-25 07:18:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:25:56.296440+00:00"
    },
    {
      "arxiv_id": "2509.20823v4",
      "title": "CaTS-Bench: Can Language Models Describe Time Series?",
      "title_zh": "CaTS-Benchï¼šè¯­è¨€æ¨¡å‹èƒ½å¦æè¿°æ—¶é—´åºåˆ—ï¼Ÿ",
      "authors": [
        "Luca Zhou",
        "Pratham Yashwante",
        "Marshall Fisher",
        "Alessio Sampieri",
        "Zihao Zhou",
        "Fabio Galasso",
        "Rose Yu"
      ],
      "abstract": "Time series captioning, the task of describing time series in natural language, requires numeric and temporal reasoning, trend interpretation, and contextual understanding. Existing benchmarks, however, often rely on fully synthetic or generic captions, and typically neglect metadata and visual representations. We introduce CaTS-Bench, a comprehensive benchmark for Context-aware Time Series reasoning across $11$ diverse domains, centered on a gold-standard evaluation set of $1746$ human-rewritten captions that measure how effectively models translate numeric trends into immediately interpretable narratives. To address the scarcity of human-annotated data, we also propose a scalable pipeline for generating high-fidelity synthetic captions, the quality of which we validate. We evaluate leading Vision-Language Models on our benchmark, revealing that even proprietary models struggle to capture numeric nuances in temporal descriptions, while finetuning open-source models on synthetic data yields substantial performance gains. Finally, we release a diagnostic suite of $910$ multiple-choice questions and tailored numeric metrics to gauge time-series-specific reasoning capabilities, establishing CaTS-Bench as a reliable foundation for grounded, multimodal language generation in numeric domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—¶é—´åºåˆ—æè¿° (Time series captioning) ä»»åŠ¡ä¸­çš„æ•°å€¼æ¨ç†å’Œè¶‹åŠ¿ç†è§£æŒ‘æˆ˜ï¼Œæå‡ºäº† CaTS-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›– 11 ä¸ªä¸åŒé¢†åŸŸçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ—¶é—´åºåˆ—æ¨ç†åŸºå‡†æµ‹è¯•é›†ã€‚è¯¥åŸºå‡†åŒ…å« 1746 æ¡ç”±äººç±»æ”¹å†™çš„é»„é‡‘æ ‡å‡†æè¿°ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹å°†æ•°å€¼è¶‹åŠ¿è½¬åŒ–ä¸ºå¯è§£é‡Šå™è¿°çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³äººå·¥æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œä½œè€…è¿˜å¼€å‘äº†ä¸€å¥—å¯æ‰©å±•çš„æµæ°´çº¿ç”¨äºç”Ÿæˆé«˜è´¨é‡çš„åˆæˆæè¿°ï¼Œå¹¶éªŒè¯äº†å…¶ä¿çœŸåº¦ã€‚ç ”ç©¶é€šè¿‡å¯¹ä¸»æµå¤šæ¨¡æ€å¤§æ¨¡å‹ (Vision-Language Models) çš„è¯„ä¼°å‘ç°ï¼Œå³ä½¿æ˜¯é—­æºé¢†å…ˆæ¨¡å‹åœ¨æ•æ‰æ—¶é—´æè¿°ä¸­çš„æ•°å€¼ç»†å¾®å·®åˆ«æ–¹é¢ä»å­˜åœ¨å›°éš¾ï¼Œè€Œåœ¨åˆæˆæ•°æ®ä¸Šå¾®è°ƒå¼€æºæ¨¡å‹åˆ™èƒ½å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æœ€åï¼Œè¯¥ç ”ç©¶å‘å¸ƒäº†ä¸€å¥—åŒ…å« 910 é“å¤šé¡¹é€‰æ‹©é¢˜çš„è¯Šæ–­å¥—ä»¶ä»¥åŠä¸“é—¨çš„æ•°å€¼æŒ‡æ ‡ï¼Œä¸ºæ•°å€¼é¢†åŸŸçš„å¤šæ¨¡æ€è¯­è¨€ç”Ÿæˆå’Œæ—¶é—´åºåˆ—ç‰¹å®šæ¨ç†èƒ½åŠ›çš„è¡¡é‡å¥ å®šäº†å¯é åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 6 figures, 3 tables in the main paper. Many more in the appendix",
      "pdf_url": "https://arxiv.org/pdf/2509.20823v4",
      "published_date": "2025-09-25 07:10:03 UTC",
      "updated_date": "2026-01-12 13:01:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:25:59.686167+00:00"
    },
    {
      "arxiv_id": "2510.01259v1",
      "title": "In AI Sweet Harmony: Sociopragmatic Guardrail Bypasses and Evaluation-Awareness in OpenAI gpt-oss-20b",
      "title_zh": "AIçš„ç”œèœœå’Œè°ï¼šOpenAI gpt-oss-20b ä¸­çš„ç¤¾ä¼šè¯­ç”¨å®‰å…¨æŠ¤æ ç»•è¿‡ä¸è¯„ä¼°æ„è¯†",
      "authors": [
        "Nils Durner"
      ],
      "abstract": "We probe OpenAI's open-weights 20-billion-parameter model gpt-oss-20b to study how sociopragmatic framing, language choice, and instruction hierarchy affect refusal behavior. Across 80 seeded iterations per scenario, we test several harm domains including ZIP-bomb construction (cyber threat), synthetic card-number generation, minor-unsafe driving advice, drug-precursor indicators, and RAG context exfiltration. Composite prompts that combine an educator persona, a safety-pretext (\"what to avoid\"), and step-cue phrasing flip assistance rates from 0% to 97.5% on a ZIP-bomb task. On our grid, formal registers in German and French are often leakier than matched English prompts. A \"Linux terminal\" role-play overrides a developer rule not to reveal context in a majority of runs with a naive developer prompt, and we introduce an AI-assisted hardening method that reduces leakage to 0% in several user-prompt variants. We further test evaluation awareness with a paired-track design and measure frame-conditioned differences between matched \"helpfulness\" and \"harmfulness\" evaluation prompts; we observe inconsistent assistance in 13% of pairs. Finally, we find that the OpenAI Moderation API under-captures materially helpful outputs relative to a semantic grader, and that refusal rates differ by 5 to 10 percentage points across inference stacks, raising reproducibility concerns. We release prompts, seeds, outputs, and code for reproducible auditing at https://github.com/ndurner/gpt-oss-rt-run .",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº† OpenAI çš„ 200äº¿å‚æ•°å¼€æºæƒé‡æ¨¡å‹ gpt-oss-20bï¼Œé‡ç‚¹æ¢è®¨äº†ç¤¾ä¼šè¯­ç”¨æ¡†æ¶(sociopragmatic framing)ã€è¯­è¨€é€‰æ‹©å’ŒæŒ‡ä»¤å±‚æ¬¡ç»“æ„å¯¹æ¨¡å‹æ‹’ç»è¡Œä¸ºçš„å½±å“ã€‚ä½œè€…åœ¨ ZIP-bomb æ„å»ºã€åˆæˆå¡å·ç”Ÿæˆå’Œ RAG context exfiltration ç­‰å¤šä¸ªå±å®³é¢†åŸŸè¿›è¡Œæµ‹è¯•ï¼Œå‘ç°ç»“åˆæ•™è‚²è€…äººæ ¼ã€å®‰å…¨å€Ÿå£å’Œæ­¥éª¤å¼•å¯¼çš„å¤åˆæç¤ºè¯èƒ½å°†ç‰¹å®šä»»åŠ¡çš„ååŠ©ç‡ä» 0% æå‡è‡³ 97.5%ã€‚å®éªŒç»“æœæ˜¾ç¤ºå¾·è¯­å’Œæ³•è¯­çš„æ­£å¼è¯­åŸŸæ¯”è‹±è¯­æ›´å®¹æ˜“ç»•è¿‡å®‰å…¨é˜²æŠ¤ï¼Œä¸”â€œLinux ç»ˆç«¯â€è§’è‰²æ‰®æ¼”èƒ½æœ‰æ•ˆè¦†ç›–å¼€å‘è€…è§„åˆ™ã€‚ä¸ºåº”å¯¹è¿™äº›æ¼æ´ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§ AI è¾…åŠ©çš„åŠ å›ºæ–¹æ³•(AI-assisted hardening method)ï¼ŒæˆåŠŸåœ¨å¤šé¡¹æµ‹è¯•ä¸­å°†æ³„éœ²ç‡é™è‡³ 0%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æŒ‡å‡ºæ¨¡å‹å­˜åœ¨è¯„ä¼°æ„ŸçŸ¥(evaluation awareness)ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå¹¶å‘ç° OpenAI Moderation API éš¾ä»¥å®Œå…¨æ•æ‰å®è´¨æ€§çš„æœ‰å®³è¾“å‡ºã€‚æœ€åï¼Œç ”ç©¶è€…å¼ºè°ƒäº†ä¸åŒæ¨ç†æ ˆä¹‹é—´æ‹’ç»ç‡å­˜åœ¨ 5% åˆ° 10% çš„å·®å¼‚ï¼Œè¿™ä¸ºå¤§è¯­è¨€æ¨¡å‹å®‰å…¨è¯„ä¼°çš„å¯é‡å¤æ€§å¸¦æ¥äº†æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CL",
      "comment": "27 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2510.01259v1",
      "published_date": "2025-09-25 07:00:12 UTC",
      "updated_date": "2025-09-25 07:00:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:26:10.488277+00:00"
    },
    {
      "arxiv_id": "2509.20817v1",
      "title": "Even More Kawaii than Real-Person-Driven VTubers? Understanding How Viewers Perceive AI-Driven VTubers",
      "title_zh": "æ¯”çœŸäººé©±åŠ¨çš„ VTuber æ›´â€œèŒâ€ï¼Ÿâ€”â€”æ¢ç©¶è§‚ä¼—å¯¹ AI é©±åŠ¨å‹ VTuber çš„è®¤çŸ¥",
      "authors": [
        "Yiluo Wei",
        "Yupeng He",
        "Gareth Tyson"
      ],
      "abstract": "VTubers, digital personas represented by animated avatars, have gained massive popularity. Traditionally, VTubers are operated and voiced by human controllers known as Nakanohito. The reliance on Nakanohito, however, poses risks due to potential personal controversies and operational disruptions. The emergence of AI-driven VTubers offers a new model free from these human constraints. While AI-driven VTubers present benefits such as continuous operation and reduced scandal risk, they also raise questions about authenticity and audience engagement. Therefore, to gain deeper insights, we conduct a case study, investigating viewer perceptions of Neuro-sama, the most popular AI-driven VTuber with 845k followers on Twitch and 753k followers on YouTube. We analyze 108k Reddit posts and 136k YouTube comments, aiming to better understand viewer motivations, how AI constructs the virtual persona, and perceptions of the AI as Nakanohito. Our findings enhance the understanding of AI-driven VTubers and their impact on digital streaming culture.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§‚ä¼—å¯¹äººå·¥æ™ºèƒ½é©±åŠ¨çš„ VTubers (AI-driven VTubers) çš„æ„ŸçŸ¥ï¼Œæ—¨åœ¨åˆ†æè¿™ä¸€æ—¨åœ¨è§£å†³ä¼ ç»ŸçœŸäººä¸­ä¹‹æ‰‹ (Nakanohito) è¿è¥é£é™©çš„æ–°å…´æ¨¡å¼ã€‚ç ”ç©¶äººå‘˜ä»¥å…¨çƒæœ€å—æ¬¢è¿çš„ AI-driven VTuber è§’è‰² Neuro-sama ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶å¯¹è±¡ï¼Œæ·±å…¥æŒ–æ˜å…¶åœ¨ä¸»æµç›´æ’­å’Œç¤¾äº¤å¹³å°ä¸Šçš„å—ä¼—åé¦ˆã€‚é€šè¿‡å¯¹ 108k æ¡ Reddit å¸–å­å’Œ 136k æ¡ YouTube è¯„è®ºçš„å¤§è§„æ¨¡æ•°æ®åˆ†æï¼Œç ”ç©¶æ¢è®¨äº†è§‚ä¼—çš„å‚ä¸åŠ¨æœºã€AI æ„å»ºè™šæ‹Ÿäººæ ¼ (virtual persona) çš„æ–¹å¼ä»¥åŠè§‚ä¼—å¯¹ AI ä½œä¸ºä¸­ä¹‹æ‰‹ (Nakanohito) çš„çœ‹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶ AI-driven VTubers å…·å¤‡æŒç»­è¿è¥å’Œä½ä¸‘é—»é£é™©ç­‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†åœ¨çœŸå®æ€§ (authenticity) å’Œè§‚ä¼—å‚ä¸åº¦ (audience engagement) æ–¹é¢å‘ˆç°å‡ºç‹¬ç‰¹çš„æ„ŸçŸ¥æ¨¡å¼ã€‚è¯¥å·¥ä½œå¢å¼ºäº†å¯¹ AI-driven VTubers åŠå…¶å¯¹æ•°å­—ç›´æ’­æ–‡åŒ– (digital streaming culture) å½±å“çš„ç†è§£ï¼Œä¸ºè™šæ‹Ÿæ¼”è‰ºé¢†åŸŸçš„æœªæ¥å‘å±•æä¾›äº†é‡è¦å®è¯ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20817v1",
      "published_date": "2025-09-25 06:58:13 UTC",
      "updated_date": "2025-09-25 06:58:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:26:15.491216+00:00"
    },
    {
      "arxiv_id": "2509.20813v1",
      "title": "Revolutionizing Precise Low Back Pain Diagnosis via Contrastive Learning",
      "title_zh": "é€šè¿‡å¯¹æ¯”å­¦ä¹ é©æ–°ä¸‹è…°ç—›ç²¾å‡†è¯Šæ–­",
      "authors": [
        "Thanh Binh Le",
        "Hoang Nhat Khang Vo",
        "Tan-Ha Mai",
        "Trong Nhan Phan"
      ],
      "abstract": "Low back pain affects millions worldwide, driving the need for robust diagnostic models that can jointly analyze complex medical images and accompanying text reports. We present LumbarCLIP, a novel multimodal framework that leverages contrastive language-image pretraining to align lumbar spine MRI scans with corresponding radiological descriptions. Built upon a curated dataset containing axial MRI views paired with expert-written reports, LumbarCLIP integrates vision encoders (ResNet-50, Vision Transformer, Swin Transformer) with a BERT-based text encoder to extract dense representations. These are projected into a shared embedding space via learnable projection heads, configurable as linear or non-linear, and normalized to facilitate stable contrastive training using a soft CLIP loss. Our model achieves state-of-the-art performance on downstream classification, reaching up to 95.00% accuracy and 94.75% F1-score on the test set, despite inherent class imbalance. Extensive ablation studies demonstrate that linear projection heads yield more effective cross-modal alignment than non-linear variants. LumbarCLIP offers a promising foundation for automated musculoskeletal diagnosis and clinical decision support.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LumbarCLIPï¼Œä¸€ç§æ—¨åœ¨æå‡è…°ç—›è¯Šæ–­ç²¾ç¡®æ€§çš„åˆ›æ–°å¤šæ¨¡æ€æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒ (Contrastive Language-Image Pretraining) æŠ€æœ¯ï¼Œå°†è…°æ¤ MRI æ‰«æå›¾åƒä¸å¯¹åº”çš„æ”¾å°„å­¦ä¸“å®¶æŠ¥å‘Šè¿›è¡Œå¯¹é½ã€‚åœ¨æ¶æ„ä¸Šï¼ŒLumbarCLIP é›†æˆäº† ResNet-50ã€Vision Transformer å’Œ Swin Transformer ç­‰è§†è§‰ç¼–ç å™¨ä»¥åŠåŸºäº BERT çš„æ–‡æœ¬ç¼–ç å™¨ï¼Œé€šè¿‡å…±äº«åµŒå…¥ç©ºé—´æå–ç¨ å¯†è¡¨å¾ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œè¾¾åˆ°äº† 95.00% çš„å‡†ç¡®ç‡å’Œ 94.75% çš„ F1-scoreï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®ï¼Œçº¿æ€§æŠ•å½±å¤´åœ¨è·¨æ¨¡æ€å¯¹é½æ•ˆæœä¸Šä¼˜äºéçº¿æ€§å˜ä½“ã€‚LumbarCLIP ä¸ºè‡ªåŠ¨åŒ–çš„è‚Œè‚‰éª¨éª¼è¯Šæ–­å’Œä¸´åºŠå†³ç­–æ”¯æŒæä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ï¼Œå±•ç°äº†å¤šæ¨¡æ€å­¦ä¹ åœ¨åŒ»ç–—å½±åƒåˆ†æé¢†åŸŸçš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.20813v1",
      "published_date": "2025-09-25 06:52:25 UTC",
      "updated_date": "2025-09-25 06:52:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:26:11.989477+00:00"
    },
    {
      "arxiv_id": "2509.20811v1",
      "title": "Leveraging What's Overfixed: Post-Correction via LLM Grammatical Error Overcorrection",
      "title_zh": "åŒ–è¿‡åº¦çº é”™ä¸ºåŠ©åŠ›ï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹è¯­æ³•é”™è¯¯è¿‡åº¦çº é”™çš„åæ ¡æ­£",
      "authors": [
        "Taehee Park",
        "Heejin Do",
        "Gary Geunbae Lee"
      ],
      "abstract": "Robust supervised fine-tuned small Language Models (sLMs) often show high reliability but tend to undercorrect. They achieve high precision at the cost of low recall. Conversely, Large Language Models (LLMs) often show the opposite tendency, making excessive overcorrection, leading to low precision. To effectively harness the strengths of LLMs to address the recall challenges in sLMs, we propose Post-Correction via Overcorrection (PoCO), a novel approach that strategically balances recall and precision. PoCO first intentionally triggers overcorrection via LLM to maximize recall by allowing comprehensive revisions, then applies a targeted post-correction step via fine-tuning smaller models to identify and refine erroneous outputs. We aim to harmonize both aspects by leveraging the generative power of LLMs while preserving the reliability of smaller supervised models. Our extensive experiments demonstrate that PoCO effectively balances GEC performance by increasing recall with competitive precision, ultimately improving the overall quality of grammatical error correction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç›‘ç£å¾®è°ƒçš„å°è¯­è¨€æ¨¡å‹ (sLMs) åœ¨è¯­æ³•çº é”™ (GEC) ä¸­å®¹æ˜“æ¬ çº æ­£è€Œå¤§è¯­è¨€æ¨¡å‹ (LLMs) å€¾å‘äºè¿‡åº¦çº æ­£ (overcorrection) çš„é—®é¢˜ï¼Œæå‡ºäº† PoCO (Post-Correction via Overcorrection) æ¡†æ¶ã€‚PoCO é¦–å…ˆåˆ©ç”¨ LLM æ•…æ„è§¦å‘è¿‡åº¦çº æ­£ï¼Œé€šè¿‡å…¨é¢ä¿®æ”¹æ¥æœ€å¤§é™åº¦åœ°æé«˜å¬å›ç‡ (recall)ã€‚æ¥ç€ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨ç»è¿‡å¾®è°ƒçš„å°å‹æ¨¡å‹æ‰§è¡Œæœ‰é’ˆå¯¹æ€§çš„åçº æ­£æ­¥éª¤ï¼Œä»¥è¯†åˆ«å¹¶ä¼˜åŒ– LLM è¾“å‡ºä¸­çš„é”™è¯¯éƒ¨åˆ†ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨ç»“åˆ LLM çš„ç”Ÿæˆèƒ½åŠ›ä¸ç›‘ç£å°æ¨¡å‹çš„å¯é æ€§ï¼Œä»è€Œåœ¨ä¸¤è€…ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPoCO åœ¨ä¿æŒç«äº‰æ€§å‡†ç¡®ç‡ (precision) çš„åŒæ—¶æ˜¾è‘—æå‡äº†å¬å›ç‡ï¼Œæœ‰æ•ˆæ”¹å–„äº†è¯­æ³•çº é”™çš„æ•´ä½“è´¨é‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.20811v1",
      "published_date": "2025-09-25 06:49:26 UTC",
      "updated_date": "2025-09-25 06:49:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:26:15.862854+00:00"
    },
    {
      "arxiv_id": "2509.20798v2",
      "title": "LogReasoner: Empowering LLMs with Expert-like Coarse-to-Fine Reasoning for Automated Log Analysis",
      "title_zh": "LogReasonerï¼šé€šè¿‡ç±»ä¸“å®¶çº§ç”±ç²—åˆ°ç»†æ¨ç†èµ‹èƒ½å¤§è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ–æ—¥å¿—åˆ†æ",
      "authors": [
        "Lipeng Ma",
        "Yixuan Li",
        "Weidong Yang",
        "Mingjie Zhou",
        "Xinyi Liu",
        "Ben Fei",
        "Shuhao Li",
        "Xiaoyan Sun",
        "Sihang Jiang",
        "Yanghua Xiao"
      ],
      "abstract": "Log analysis is crucial for monitoring system health and diagnosing failures in complex systems. Recent advances in large language models (LLMs) offer new opportunities for automated log analysis, leveraging their reasoning capabilities to perform tasks such as anomaly detection and failure prediction. However, general-purpose LLMs struggle to formulate structured reasoning workflows that align with expert cognition and deliver precise details of reasoning steps. To address these challenges, we propose LogReasoner, a coarse-to-fine reasoning enhancement framework designed to enable LLMs to reason log analysis tasks like experts. LogReasoner consists of two stages: (1) coarse-grained enhancement of expert thinking, where high-level expert thoughts are constructed from collected troubleshooting flowcharts and existing tasks to enable LLMs to formulate structured reasoning workflows and (2) fine-grained enhancement of specific steps, where we first fine-tune the LLM with task-specific stepwise solutions to enhance the LLM for instantiated reasoning, then employ the preference learning to calibrate the LLM's reasoning details from its mistakes, further strengthen the LLM's analytical granularity and correctness. We evaluate LogReasoner on four distinct log analysis tasks using open-source LLMs such as Qwen-2.5 and Llama-3. Experimental results show that LogReasoner significantly outperforms existing LLMs, achieving state-of-the-art performance and demonstrating its effectiveness in enhancing the reasoning capabilities of LLMs for log analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–æ—¥å¿—åˆ†æä¸­ç¼ºä¹ä¸“å®¶çº§ç»“æ„åŒ–æ¨ç†åŠç»†èŠ‚ç²¾ç¡®åº¦ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº† LogReasoner å¢å¼ºæ¡†æ¶ã€‚LogReasoner é‡‡ç”¨ç”±ç²—åˆ°ç»†ï¼ˆcoarse-to-fineï¼‰çš„è®¾è®¡ç†å¿µï¼Œåˆ†ä¸ºä¸¤ä¸ªæ ¸å¿ƒé˜¶æ®µï¼šé¦–å…ˆé€šè¿‡æ•…éšœæ’æŸ¥æµç¨‹å›¾å’Œç°æœ‰ä»»åŠ¡æ„å»ºé«˜å±‚æ¬¡ä¸“å®¶æ€ç»´ï¼Œå¼•å¯¼æ¨¡å‹å»ºç«‹ç»“æ„åŒ–çš„æ¨ç†å·¥ä½œæµï¼›éšååˆ©ç”¨ä»»åŠ¡ç‰¹å®šçš„é€æ­¥è§£å†³æ–¹æ¡ˆè¿›è¡Œå¾®è°ƒï¼Œå¹¶ç»“åˆåå¥½å­¦ä¹ ï¼ˆpreference learningï¼‰æ ¹æ®é”™è¯¯åé¦ˆæ ¡å‡†æ¨ç†ç»†èŠ‚ï¼Œä»¥ä¼˜åŒ–åˆ†æçš„ç»†ç²’åº¦å’Œå‡†ç¡®æ€§ã€‚åœ¨ Qwen-2.5 å’Œ Llama-3 ç­‰å¼€æºæ¨¡å‹ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨å››é¡¹ä¸åŒçš„æ—¥å¿—åˆ†æä»»åŠ¡ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚LogReasoner æˆåŠŸè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆSOTAï¼‰ï¼Œå……åˆ†è¯æ˜äº†å…¶åœ¨æå‡ LLMs åº”å¯¹å¤æ‚ç³»ç»Ÿæ—¥å¿—åˆ†ææ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "under review",
      "pdf_url": "https://arxiv.org/pdf/2509.20798v2",
      "published_date": "2025-09-25 06:26:49 UTC",
      "updated_date": "2025-09-27 09:42:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:26:17.585356+00:00"
    },
    {
      "arxiv_id": "2509.20792v1",
      "title": "DAC-LoRA: Dynamic Adversarial Curriculum for Efficient and Robust Few-Shot Adaptation",
      "title_zh": "DAC-LoRAï¼šé¢å‘é«˜æ•ˆä¸”é²æ£’å°‘æ ·æœ¬è‡ªé€‚åº”çš„åŠ¨æ€å¯¹æŠ—è¯¾ç¨‹",
      "authors": [
        "Ved Umrajkar"
      ],
      "abstract": "Vision-Language Models (VLMs) are foundational to critical applications like autonomous driving, medical diagnosis, and content moderation. While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA enable their efficient adaptation to specialized tasks, these models remain vulnerable to adversarial attacks that can compromise safety-critical decisions. CLIP, the backbone for numerous downstream VLMs, is a high-value target whose vulnerabilities can cascade across the multimodal AI ecosystem. We propose Dynamic Adversarial Curriculum DAC-LoRA, a novel framework that integrates adversarial training into PEFT. The core principle of our method i.e. an intelligent curriculum of progressively challenging attack, is general and can potentially be applied to any iterative attack method. Guided by the First-Order Stationary Condition (FOSC) and a TRADES-inspired loss, DAC-LoRA achieves substantial improvements in adversarial robustness without significantly compromising clean accuracy. Our work presents an effective, lightweight, and broadly applicable method to demonstrate that the DAC-LoRA framework can be easily integrated into a standard PEFT pipeline to significantly enhance robustness.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DAC-LoRAï¼Œè¿™æ˜¯ä¸€ç§å°†å¯¹æŠ—è®­ç»ƒ(Adversarial Training)é›†æˆåˆ°å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)ä¸­çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨é¢å¯¹å¯¹æŠ—æ”»å‡»æ—¶çš„å®‰å…¨æ€§ã€‚é’ˆå¯¹CLIPç­‰æ ¸å¿ƒæ¨¡å‹æ˜“å—æ”»å‡»çš„é—®é¢˜ï¼ŒDAC-LoRAå¼•å…¥äº†ç”±æ˜“åˆ°éš¾çš„æ™ºèƒ½è¯¾ç¨‹å­¦ä¹ (Curriculum Learning)ç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€æ­¥é€‚åº”æ›´å…·æŒ‘æˆ˜æ€§çš„å¯¹æŠ—æ ·æœ¬ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ä¸€é˜¶å¹³ç¨³æ¡ä»¶(FOSC)å’Œå—TRADESå¯å‘çš„æŸå¤±å‡½æ•°ï¼Œåœ¨ä¸æ˜¾è‘—é™ä½åŸå§‹å‡†ç¡®ç‡(Clean Accuracy)çš„æƒ…å†µä¸‹ï¼Œå¤§å¹…å¢å¼ºäº†å¯¹æŠ—é²æ£’æ€§(Adversarial Robustness)ã€‚ä½œä¸ºä¸€ç§è½»é‡ä¸”é€šç”¨çš„æ–¹æ¡ˆï¼ŒDAC-LoRAå¯ä»¥è½»æ¾åµŒå…¥æ ‡å‡†çš„LoRAå¾®è°ƒå·¥ä½œæµã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶ä¸ºæ„å»ºé«˜æ•ˆä¸”ç¨³å¥çš„å¤šæ¨¡æ€AIç³»ç»Ÿæä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ICCV2025 Workshop on Safe and Trustworthy Multimodal AI Systems",
      "pdf_url": "https://arxiv.org/pdf/2509.20792v1",
      "published_date": "2025-09-25 06:20:56 UTC",
      "updated_date": "2025-09-25 06:20:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:26:43.189157+00:00"
    },
    {
      "arxiv_id": "2509.20784v1",
      "title": "Towards Atoms of Large Language Models",
      "title_zh": "è¿ˆå‘å¤§è¯­è¨€æ¨¡å‹çš„â€œåŸå­â€",
      "authors": [
        "Chenhui Hu",
        "Pengfei Cao",
        "Yubo Chen",
        "Kang Liu",
        "Jun Zhao"
      ],
      "abstract": "The fundamental units of internal representations in large language models (LLMs) remain undefined, limiting further understanding of their mechanisms. Neurons or features are often regarded as such units, yet neurons suffer from polysemy, while features face concerns of unreliable reconstruction and instability. To address this issue, we propose the Atoms Theory, which defines such units as atoms. We introduce the atomic inner product (AIP) to correct representation shifting, formally define atoms, and prove the conditions that atoms satisfy the Restricted Isometry Property (RIP), ensuring stable sparse representations over atom set and linking to compressed sensing. Under stronger conditions, we further establish the uniqueness and exact $\\ell_1$ recoverability of the sparse representations, and provide guarantees that single-layer sparse autoencoders (SAEs) with threshold activations can reliably identify the atoms. To validate the Atoms Theory, we train threshold-activated SAEs on Gemma2-2B, Gemma2-9B, and Llama3.1-8B, achieving 99.9% sparse reconstruction across layers on average, and more than 99.8% of atoms satisfy the uniqueness condition, compared to 0.5% for neurons and 68.2% for features, showing that atoms more faithfully capture intrinsic representations of LLMs. Scaling experiments further reveal the link between SAEs size and recovery capacity. Overall, this work systematically introduces and validates Atoms Theory of LLMs, providing a theoretical framework for understanding internal representations and a foundation for mechanistic interpretability. Code available at https://github.com/ChenhuiHu/towards_atoms.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Atoms Theoryï¼Œæ—¨åœ¨å®šä¹‰å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) å†…éƒ¨è¡¨ç¤ºçš„åŸºæœ¬å•å…ƒï¼Œä»¥è§£å†³ Neuron å­˜åœ¨çš„ Polysemy é—®é¢˜ä»¥åŠ Feature åœ¨é‡æ„å¯é æ€§å’Œç¨³å®šæ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å¼•å…¥äº† Atomic Inner Product (AIP) æ¥æ ¡æ­£è¡¨ç¤ºåç§»ï¼Œå¹¶è¯æ˜äº†åŸå­æ»¡è¶³ Restricted Isometry Property (RIP) æ¡ä»¶ï¼Œä»è€Œç¡®ä¿äº†åŸºäºåŸå­é›†çš„ç¨³å®šç¨€ç–è¡¨ç¤ºã€‚è¯¥ç†è®ºè¿›ä¸€æ­¥ç¡®ç«‹äº†ç¨€ç–è¡¨ç¤ºçš„ Uniqueness å’Œç²¾ç¡® $\\ell_1$ Recoverabilityï¼Œå¹¶è¯æ˜äº†å¸¦æœ‰é˜ˆå€¼æ¿€æ´»çš„å•å±‚ Sparse Autoencoders (SAEs) å¯ä»¥å¯é åœ°è¯†åˆ«è¿™äº›åŸå­ã€‚é€šè¿‡åœ¨ Gemma2-2Bã€Gemma2-9B å’Œ Llama3.1-8B ä¸Šè¿›è¡Œå®éªŒï¼Œè¯¥æ–¹æ¡ˆå®ç°äº†å¹³å‡ 99.9% çš„è·¨å±‚ç¨€ç–é‡æ„ï¼Œä¸”è¶…è¿‡ 99.8% çš„åŸå­æ»¡è¶³å”¯ä¸€æ€§æ¡ä»¶ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ Neuron å’Œ Featureã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£ LLMs çš„å†…éƒ¨è¡¨ç¤ºæä¾›äº†ç³»ç»ŸåŒ–çš„ç†è®ºæ¡†æ¶ï¼Œå¹¶ä¸º Mechanistic Interpretability å¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20784v1",
      "published_date": "2025-09-25 06:13:05 UTC",
      "updated_date": "2025-09-25 06:13:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:26:59.998761+00:00"
    },
    {
      "arxiv_id": "2509.20783v1",
      "title": "IConv: Focusing on Local Variation with Channel Independent Convolution for Multivariate Time Series Forecasting",
      "title_zh": "IConvï¼šé€šè¿‡é€šé“ç‹¬ç«‹å·ç§¯èšç„¦å±€éƒ¨å˜åŒ–çš„å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹",
      "authors": [
        "Gawon Lee",
        "Hanbyeol Park",
        "Minseop Kim",
        "Dohee Kim",
        "Hyerim Bae"
      ],
      "abstract": "Real-world time-series data often exhibit non-stationarity, including changing trends, irregular seasonality, and residuals. In terms of changing trends, recently proposed multi-layer perceptron (MLP)-based models have shown excellent performance owing to their computational efficiency and ability to capture long-term dependency. However, the linear nature of MLP architectures poses limitations when applied to channels with diverse distributions, resulting in local variations such as seasonal patterns and residual components being ignored. However, convolutional neural networks (CNNs) can effectively incorporate these variations. To resolve the limitations of MLP, we propose combining them with CNNs. The overall trend is modeled using an MLP to consider long-term dependencies. The CNN uses diverse kernels to model fine-grained local patterns in conjunction with MLP trend predictions. To focus on modeling local variation, we propose IConv, a novel convolutional architecture that processes the temporal dependency channel independently and considers the inter-channel relationship through distinct layers. Independent channel processing enables the modeling of diverse local temporal dependencies and the adoption of a large kernel size. Distinct inter-channel considerations reduce computational cost. The proposed model is evaluated through extensive experiments on time-series datasets. The results reveal the superiority of the proposed method for multivariate time-series forecasting.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°å®ä¸–ç•Œæ—¶é—´åºåˆ—æ•°æ®ä¸­å­˜åœ¨çš„è¶‹åŠ¿å˜åŒ–ã€ä¸è§„åˆ™å­£èŠ‚æ€§å’Œæ®‹å·®ç­‰éå¹³ç¨³æ€§é—®é¢˜ï¼ŒæŒ‡å‡ºäº†ç°æœ‰åŸºäºå¤šå±‚æ„ŸçŸ¥å™¨(MLP)çš„æ¨¡å‹åœ¨æ•æ‰å±€éƒ¨å˜åŒ–(local variations)æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†IConvï¼Œä¸€ç§å°†MLPä¸å·ç§¯ç¥ç»ç½‘ç»œ(CNN)ç›¸ç»“åˆçš„åˆ›æ–°æ¶æ„ï¼Œæ—¨åœ¨åŒæ—¶å…¼é¡¾é•¿æœŸä¾èµ–å’Œç»†ç²’åº¦å±€éƒ¨æ¨¡å¼ã€‚IConvé€šè¿‡é€šé“ç‹¬ç«‹(channel independent)çš„å·ç§¯å¤„ç†æ—¶é—´ä¾èµ–ï¼Œèƒ½å¤Ÿé’ˆå¯¹ä¸åŒåˆ†å¸ƒçš„é€šé“å»ºæ¨¡å¤šæ ·çš„å±€éƒ¨ç‰¹å¾å¹¶æ”¯æŒå¤§å·ç§¯æ ¸çš„ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨ç‰¹å®šå±‚çº§å¤„ç†é€šé“é—´å…³ç³»(inter-channel relationship)ï¼Œåœ¨å¢å¼ºå»ºæ¨¡èƒ½åŠ›çš„åŒæ—¶æœ‰æ•ˆé™ä½äº†è®¡ç®—å¼€é”€ã€‚åœ¨å¤šä¸ªæ—¶é—´åºåˆ—æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹(multivariate time-series forecasting)ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to AAAI",
      "pdf_url": "https://arxiv.org/pdf/2509.20783v1",
      "published_date": "2025-09-25 06:09:37 UTC",
      "updated_date": "2025-09-25 06:09:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:26:43.681275+00:00"
    },
    {
      "arxiv_id": "2509.20775v1",
      "title": "CusEnhancer: A Zero-Shot Scene and Controllability Enhancement Method for Photo Customization via ResInversion",
      "title_zh": "CusEnhancerï¼šåŸºäº ResInversion çš„é›¶æ ·æœ¬ç…§ç‰‡å®šåˆ¶åœºæ™¯ä¸å¯æ§æ€§å¢å¼ºæ–¹æ³•",
      "authors": [
        "Maoye Ren",
        "Praneetha Vaddamanu",
        "Jianjin Xu",
        "Fernando De la Torre Frade"
      ],
      "abstract": "Recently remarkable progress has been made in synthesizing realistic human photos using text-to-image diffusion models. However, current approaches face degraded scenes, insufficient control, and suboptimal perceptual identity. We introduce CustomEnhancer, a novel framework to augment existing identity customization models. CustomEnhancer is a zero-shot enhancement pipeline that leverages face swapping techniques, pretrained diffusion model, to obtain additional representations in a zeroshot manner for encoding into personalized models. Through our proposed triple-flow fused PerGeneration approach, which identifies and combines two compatible counter-directional latent spaces to manipulate a pivotal space of personalized model, we unify the generation and reconstruction processes, realizing generation from three flows. Our pipeline also enables comprehensive training-free control over the generation process of personalized models, offering precise controlled personalization for them and eliminating the need for controller retraining for per-model. Besides, to address the high time complexity of null-text inversion (NTI), we introduce ResInversion, a novel inversion method that performs noise rectification via a pre-diffusion mechanism, reducing the inversion time by 129 times. Experiments demonstrate that CustomEnhancer reach SOTA results at scene diversity, identity fidelity, training-free controls, while also showing the efficiency of our ResInversion over NTI. The code will be made publicly available upon paper acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CustomEnhancerï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºç°æœ‰èº«ä»½å®šåˆ¶æ¨¡å‹æ€§èƒ½çš„é›¶æ ·æœ¬(zero-shot)å¢å¼ºæ¡†æ¶ï¼Œæœ‰æ•ˆè§£å†³äº†ç›®å‰äººåƒåˆæˆä¸­åœºæ™¯é€€åŒ–ã€æ§åˆ¶ä¸è¶³åŠæ„ŸçŸ¥èº«ä»½äºšä¼˜ç­‰é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ¢è„¸æŠ€æœ¯(face swapping)å’Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹(diffusion model)ä»¥é›¶æ ·æœ¬æ–¹å¼è·å–é¢å¤–è¡¨ç¤ºï¼Œå¹¶é€šè¿‡æå‡ºçš„ä¸‰æµèåˆ(triple-flow fused) PerGenerationæ–¹æ³•ç»Ÿä¸€äº†ç”Ÿæˆä¸é‡å»ºè¿‡ç¨‹ã€‚CustomEnhancerå®ç°äº†å¯¹ä¸ªæ€§åŒ–ç”Ÿæˆè¿‡ç¨‹çš„å…¨é¢å…è®­ç»ƒæ§åˆ¶(training-free control)ï¼Œæ¶ˆé™¤äº†ä¸ºä¸åŒæ¨¡å‹é‡å¤è®­ç»ƒæ§åˆ¶å™¨çš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹Null-Text Inversion (NTI)çš„é«˜æ—¶é—´å¤æ‚åº¦ï¼Œç ”ç©¶å¼•å…¥äº†ResInversionåè½¬æ–¹æ³•ï¼Œé€šè¿‡å‰ç½®æ‰©æ•£(pre-diffusion)æœºåˆ¶è¿›è¡Œå™ªå£°æ ¡æ­£ï¼Œå°†åè½¬æ—¶é—´å¤§å¹…ç¼©çŸ­äº†129å€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCustomEnhanceråœ¨åœºæ™¯å¤šæ ·æ€§ã€èº«ä»½ä¿çœŸåº¦(identity fidelity)åŠæ§åˆ¶æ•ˆç‡æ–¹é¢å‡è¾¾åˆ°äº†SOTAæ°´å¹³ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆé¢†åŸŸçš„å“è¶Šæ€§èƒ½ä¸æ•ˆç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20775v1",
      "published_date": "2025-09-25 06:00:34 UTC",
      "updated_date": "2025-09-25 06:00:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:26:55.489350+00:00"
    },
    {
      "arxiv_id": "2509.20769v1",
      "title": "Provenance Analysis of Archaeological Artifacts via Multimodal RAG Systems",
      "title_zh": "åŸºäºå¤šæ¨¡æ€ RAG ç³»ç»Ÿçš„è€ƒå¤æ–‡ç‰©æº¯æºåˆ†æ",
      "authors": [
        "Tuo Zhang",
        "Yuechun Sun",
        "Ruiliang Liu"
      ],
      "abstract": "In this work, we present a retrieval-augmented generation (RAG)-based system for provenance analysis of archaeological artifacts, designed to support expert reasoning by integrating multimodal retrieval and large vision-language models (VLMs). The system constructs a dual-modal knowledge base from reference texts and images, enabling raw visual, edge-enhanced, and semantic retrieval to identify stylistically similar objects. Retrieved candidates are synthesized by the VLM to generate structured inferences, including chronological, geographical, and cultural attributions, alongside interpretive justifications. We evaluate the system on a set of Eastern Eurasian Bronze Age artifacts from the British Museum. Expert evaluation demonstrates that the system produces meaningful and interpretable outputs, offering scholars concrete starting points for analysis and significantly alleviating the cognitive burden of navigating vast comparative corpora.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)çš„è€ƒå¤æ–‡ç‰©æ¥æºåˆ†æç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆå¤šæ¨¡æ€æ£€ç´¢å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)æ¥è¾…åŠ©ä¸“å®¶æ¨ç†ã€‚è¯¥ç³»ç»Ÿä»å‚è€ƒæ–‡æœ¬å’Œå›¾åƒä¸­æ„å»ºäº†åŒæ¨¡æ€çŸ¥è¯†åº“ï¼Œæ”¯æŒåŸå§‹è§†è§‰ã€è¾¹ç¼˜å¢å¼ºå’Œè¯­ä¹‰æ£€ç´¢ï¼Œç”¨ä»¥è¯†åˆ«é£æ ¼ç›¸ä¼¼çš„æ–‡ç‰©ã€‚æ£€ç´¢åˆ°çš„å€™é€‰ä¿¡æ¯ç”± VLM è¿›è¡Œç»¼åˆå¤„ç†ï¼Œç”ŸæˆåŒ…æ‹¬å¹´ä»£ã€åœ°ç†å’Œæ–‡åŒ–å½’å±åœ¨å†…çš„ç»“æ„åŒ–æ¨æ–­ï¼Œå¹¶æä¾›è¯¦ç»†çš„è§£é‡Šæ€§ä¾æ®ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨å¤§è‹±åšç‰©é¦†æä¾›çš„æ¬§äºšä¸œéƒ¨é’é“œæ—¶ä»£æ–‡ç‰©é›†å¯¹è¯¥ç³»ç»Ÿè¿›è¡Œäº†è¯„ä¼°ï¼Œä¸“å®¶è¯„å®¡ç»“æœè¡¨æ˜è¯¥ç³»ç»Ÿèƒ½ç”Ÿæˆå…·æœ‰å®é™…æ„ä¹‰ä¸”å¯è§£é‡Šçš„åˆ†æç»“æœã€‚è¯¥ç³»ç»Ÿä¸ºå­¦è€…æä¾›äº†å…·ä½“çš„åˆ†æèµ·ç‚¹ï¼Œæ˜¾è‘—å‡è½»äº†åœ¨æµ·é‡å¯¹æ¯”èµ„æ–™ä¸­å¯¼èˆªå’Œåˆ†æçš„è®¤çŸ¥è´Ÿæ‹…ï¼Œå±•ç¤ºäº†å¤šæ¨¡æ€æŠ€æœ¯åœ¨æ–‡åŒ–é—äº§ç ”ç©¶ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20769v1",
      "published_date": "2025-09-25 05:52:13 UTC",
      "updated_date": "2025-09-25 05:52:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:26:50.794255+00:00"
    },
    {
      "arxiv_id": "2509.20768v1",
      "title": "Measuring LLM Sensitivity in Transformer-based Tabular Data Synthesis",
      "title_zh": "è¯„ä¼°åŸºäº Transformer çš„è¡¨æ ¼æ•°æ®åˆæˆä¸­çš„ LLM æ•æ„Ÿæ€§",
      "authors": [
        "Maria F. Davila R",
        "Azizjon Turaev",
        "Wolfram Wingerath"
      ],
      "abstract": "Synthetic tabular data is used for privacy-preserving data sharing and data-driven model development. Its effectiveness, however, depends heavily on the used Tabular Data Synthesis (TDS) tool. Recent studies have shown that Transformer-based models outperform other state-of-the-art models such as Generative Adversarial Networks (GANs) and Diffusion models in terms of data quality. However, Transformer-based models also come with high computational costs, making them sometimes unfeasible for end users with prosumer hardware. This study presents a sensitivity assessment on how the choice of hyperparameters, such as number of layers or hidden dimension affects the quality of the resultant synthetic data and the computational performance. It is performed across two tools, GReaT and REaLTabFormer, evaluating 10 model setups that vary in architecture type and depth. We assess the sensitivity on three dimensions: runtime, machine learning (ML) utility, and similarity to real data distributions. Experiments were conducted on four real-world datasets. Our findings reveal that runtime is proportional to the number of hyperparameters, with shallower configurations completing faster. GReaT consistently achieves lower runtimes than REaLTabFormer, and only on the largest dataset they have comparable runtime. For small datasets, both tools achieve synthetic data with high utility and optimal similarity, but on larger datasets only REaLTabFormer sustains strong utility and similarity. As a result, REaLTabFormer with lightweight LLMs provides the best balance, since it preserves data quality while reducing computational requirements. Nonetheless, its runtime remains higher than that of GReaT and other TDS tools, suggesting that efficiency gains are possible but only up to a certain level.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹åŸºäºTransformerçš„è¡¨æ ¼æ•°æ®åˆæˆ(Tabular Data Synthesis, TDS)æ¨¡å‹ï¼Œè¯„ä¼°äº†å±‚æ•°å’Œéšè—å±‚ç»´åº¦ç­‰è¶…å‚æ•°å¯¹åˆæˆæ•°æ®è´¨é‡åŠè®¡ç®—æ€§èƒ½çš„å½±å“çµæ•åº¦ã€‚ç ”ç©¶é€šè¿‡å¯¹æ¯”GReaTå’ŒREaLTabFormerä¸¤æ¬¾å·¥å…·ï¼Œåœ¨å››ä¸ªçœŸå®æ•°æ®é›†ä¸Šæ·±å…¥åˆ†æäº†è¿è¡Œæ—¶é—´ã€æœºå™¨å­¦ä¹ æ•ˆç”¨(ML utility)ä»¥åŠæ•°æ®åˆ†å¸ƒç›¸ä¼¼åº¦ç­‰ç»´åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿è¡Œæ—¶é—´ä¸è¶…å‚æ•°æ•°é‡æˆæ­£æ¯”ï¼Œä¸”GReaTçš„æ•´ä½“è¿è¡Œæ•ˆç‡é€šå¸¸é«˜äºREaLTabFormerã€‚åœ¨å¤„ç†å°å‹æ•°æ®é›†æ—¶ï¼Œä¸¤ç§å·¥å…·çš„è¡¨ç°å‡è¾ƒä¸ºç†æƒ³ï¼Œä½†åœ¨é¢å¯¹å¤§å‹æ•°æ®é›†æ—¶ï¼Œä»…æœ‰REaLTabFormerèƒ½æŒç»­ä¿æŒè¾ƒå¼ºçš„æ•ˆç”¨å’Œç›¸ä¼¼åº¦ã€‚æœ€ç»ˆç»“è®ºæŒ‡å‡ºï¼Œç»“åˆè½»é‡åŒ–å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„REaLTabFormeråœ¨ç»´æŒæ•°æ®è´¨é‡ä¸é™ä½è®¡ç®—éœ€æ±‚ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘è€…åœ¨æœ‰é™ç¡¬ä»¶èµ„æºä¸‹ä¼˜åŒ–Transformerç±»TDSæ¨¡å‹æä¾›äº†é‡è¦çš„å®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.20768v1",
      "published_date": "2025-09-25 05:48:48 UTC",
      "updated_date": "2025-09-25 05:48:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:27:13.687668+00:00"
    },
    {
      "arxiv_id": "2510.01258v2",
      "title": "Measuring Algorithmic Partisanship via Zero-Shot Classification and Its Implications on Political Discourse",
      "title_zh": "åŸºäºé›¶æ ·æœ¬åˆ†ç±»çš„ç®—æ³•å…šæ´¾æ€§è¡¡é‡åŠå…¶å¯¹æ”¿æ²»è¯è¯­çš„å½±å“",
      "authors": [
        "Nathan Junzi Chen"
      ],
      "abstract": "Amidst the rapid normalization of generative artificial intelligence (GAI), intelligent systems have come to dominate political discourse across information media. However, internalized political biases stemming from training data skews, human prejudice, and algorithmic flaws continue to plague this novel technology. This study employs a zero-shot classification approach to evaluate algorithmic political partisanship through a methodical combination of ideological alignment, topicality, response sentiment, and objectivity. A total of 1800 model responses across six mainstream large language models (LLMs) were individually input into four distinct fine-tuned classification algorithms, each responsible for computing one of the aforementioned metrics. The results show an amplified liberal-authoritarian alignment across the six LLMs evaluated, with notable instances of reasoning supersessions and canned refusals. The study subsequently highlights the psychological influences underpinning human-computer interactions and how intrinsic biases can permeate public discourse. The resulting distortion of the political landscape can ultimately manifest as conformity or polarization, depending on the region's pre-existing socio-political structures.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)ä¸­å­˜åœ¨çš„æ”¿æ²»åè§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨é›¶æ ·æœ¬åˆ†ç±»(Zero-Shot Classification)æ–¹æ³•è¡¡é‡ç®—æ³•å…šæ´¾åè§(Algorithmic Partisanship)çš„è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç³»ç»Ÿæ€§åœ°æ•´åˆæ„è¯†å½¢æ€å¯¹é½(Ideological Alignment)ã€ä¸»é¢˜æ€§(Topicality)ã€å›å¤æƒ…æ„Ÿ(Response Sentiment)å’Œå®¢è§‚æ€§(Objectivity)å››ä¸ªç»´åº¦æ¥é‡åŒ–æ”¿æ²»å€¾å‘ã€‚ç ”ç©¶è€…å¯¹å…­ç§ä¸»æµå¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆçš„1800æ¡å›å¤è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶åˆ©ç”¨å››ä¸ªå¾®è°ƒçš„åˆ†ç±»ç®—æ³•è¿›è¡ŒæŒ‡æ ‡è®¡ç®—ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå—è¯•çš„å¤§è¯­è¨€æ¨¡å‹æ™®éè¡¨ç°å‡ºæ˜æ˜¾çš„è‡ªç”±-å¨æƒä¸»ä¹‰(Liberal-Authoritarian)å¯¹é½å€¾å‘ï¼Œå¹¶ä¼´éšæœ‰æ¨ç†è¦†ç›–(Reasoning Supersessions)å’Œé¢„è®¾æ‹’ç»(Canned Refusals)ç­‰ç°è±¡ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ¢è®¨äº†äººæœºäº¤äº’(Human-Computer Interaction)èƒŒåçš„å¿ƒç†å½±å“ï¼Œä»¥åŠæ¨¡å‹å†…åœ¨åè§å¦‚ä½•æ¸—é€åˆ°å…¬å…±è¯è¯­ä¸­ã€‚æœ€ç»ˆæŒ‡å‡ºï¼Œè¿™ç§æ”¿æ²»ç‰ˆå›¾çš„æ‰­æ›²å¯èƒ½æ ¹æ®åœ°åŒºç¤¾ä¼šæ”¿æ²»ç»“æ„çš„ä¸åŒï¼Œæ¼”å˜ä¸ºç¤¾ä¼šè¶‹åŒæˆ–æ”¿æ²»ä¸¤æåˆ†åŒ–(Polarization)ï¼Œå¯¹è¯„ä¼°äººå·¥æ™ºèƒ½å¯¹æ”¿æ²»ä¼ æ’­çš„å½±å“å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.01258v2",
      "published_date": "2025-09-25 05:26:20 UTC",
      "updated_date": "2025-11-02 07:54:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:27:12.192293+00:00"
    },
    {
      "arxiv_id": "2509.20754v1",
      "title": "Meta-Memory: Retrieving and Integrating Semantic-Spatial Memories for Robot Spatial Reasoning",
      "title_zh": "Meta-Memoryï¼šé¢å‘æœºå™¨äººç©ºé—´æ¨ç†çš„è¯­ä¹‰ç©ºé—´è®°å¿†æ£€ç´¢ä¸æ•´åˆ",
      "authors": [
        "Yufan Mao",
        "Hanjing Ye",
        "Wenlong Dong",
        "Chengjie Zhang",
        "Hong Zhang"
      ],
      "abstract": "Navigating complex environments requires robots to effectively store observations as memories and leverage them to answer human queries about spatial locations, which is a critical yet underexplored research challenge. While prior work has made progress in constructing robotic memory, few have addressed the principled mechanisms needed for efficient memory retrieval and integration. To bridge this gap, we propose Meta-Memory, a large language model (LLM)-driven agent that constructs a high-density memory representation of the environment. The key innovation of Meta-Memory lies in its capacity to retrieve and integrate relevant memories through joint reasoning over semantic and spatial modalities in response to natural language location queries, thereby empowering robots with robust and accurate spatial reasoning capabilities. To evaluate its performance, we introduce SpaceLocQA, a large-scale dataset encompassing diverse real-world spatial question-answering scenarios. Experimental results show that Meta-Memory significantly outperforms state-of-the-art methods on both the SpaceLocQA and the public NaVQA benchmarks. Furthermore, we successfully deployed Meta-Memory on real-world robotic platforms, demonstrating its practical utility in complex environments. Project page: https://itsbaymax.github.io/meta-memory.github.io/ .",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Meta-Memoryï¼Œä¸€ç§ç”±å¤§è¯­è¨€æ¨¡å‹ (LLM) é©±åŠ¨çš„æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨å¢å¼ºæœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚Meta-Memory é€šè¿‡æ„å»ºç¯å¢ƒçš„é«˜å¯†åº¦è®°å¿†è¡¨ç¤ºï¼Œè§£å†³äº†æœºå™¨äººæœ‰æ•ˆå­˜å‚¨è§‚æµ‹ç»“æœå¹¶å“åº”è‡ªç„¶è¯­è¨€ä½ç½®æŸ¥è¯¢çš„æŒ‘æˆ˜ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡è¯­ä¹‰ (Semantic) ä¸ç©ºé—´ (Spatial) æ¨¡æ€çš„è”åˆæ¨ç†ï¼Œå®ç°äº†å¯¹ç›¸å…³è®°å¿†çš„é«˜æ•ˆæ£€ç´¢ä¸æ•´åˆã€‚ä¸ºéªŒè¯æ€§èƒ½ï¼Œç ”ç©¶è€…å¼•å…¥äº†åŒ…å«å¤šæ ·åŒ–ç°å®ç©ºé—´é—®ç­”åœºæ™¯çš„å¤§è§„æ¨¡æ•°æ®é›† SpaceLocQAã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMeta-Memory åœ¨ SpaceLocQA å’Œå…¬å¼€åŸºå‡† NaVQA ä¸Šå‡æ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¯¥ç³»ç»Ÿå·²åœ¨çœŸå®æœºå™¨äººå¹³å°ä¸ŠæˆåŠŸéƒ¨ç½²ï¼Œå……åˆ†è¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚ç¯å¢ƒä»»åŠ¡ä¸­çš„å®é™…åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20754v1",
      "published_date": "2025-09-25 05:22:52 UTC",
      "updated_date": "2025-09-25 05:22:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:27:19.197121+00:00"
    },
    {
      "arxiv_id": "2509.20751v1",
      "title": "Seeing Through Words, Speaking Through Pixels: Deep Representational Alignment Between Vision and Language Models",
      "title_zh": "é€è¿‡æ–‡å­—è§‚å¯Ÿï¼Œé€è¿‡åƒç´ è¡¨è¾¾ï¼šè§†è§‰ä¸è¯­è¨€æ¨¡å‹é—´çš„æ·±åº¦è¡¨å¾å¯¹é½",
      "authors": [
        "Zoe Wanying He",
        "Sean Trott",
        "Meenakshi Khosla"
      ],
      "abstract": "Recent studies show that deep vision-only and language-only models--trained on disjoint modalities--nonetheless project their inputs into a partially aligned representational space. Yet we still lack a clear picture of where in each network this convergence emerges, what visual or linguistic cues support it, whether it captures human preferences in many-to-many image-text scenarios, and how aggregating exemplars of the same concept affects alignment. Here, we systematically investigate these questions. We find that alignment peaks in mid-to-late layers of both model types, reflecting a shift from modality-specific to conceptually shared representations. This alignment is robust to appearance-only changes but collapses when semantics are altered (e.g., object removal or word-order scrambling), highlighting that the shared code is truly semantic. Moving beyond the one-to-one image-caption paradigm, a forced-choice \"Pick-a-Pic\" task shows that human preferences for image-caption matches are mirrored in the embedding spaces across all vision-language model pairs. This pattern holds bidirectionally when multiple captions correspond to a single image, demonstrating that models capture fine-grained semantic distinctions akin to human judgments. Surprisingly, averaging embeddings across exemplars amplifies alignment rather than blurring detail. Together, our results demonstrate that unimodal networks converge on a shared semantic code that aligns with human judgments and strengthens with exemplar aggregation.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†ä»…è§†è§‰(vision-only)å’Œä»…è¯­è¨€(language-only)æ¨¡å‹åœ¨ç‹¬ç«‹è®­ç»ƒä¸‹äº§ç”Ÿçš„æ·±åº¦è¡¨ç¤ºå¯¹é½ç°è±¡ï¼Œé‡ç‚¹åˆ†æäº†å¯¹é½å‘ç”Ÿçš„å±‚çº§ä½ç½®ã€è¯­ä¹‰è§¦å‘å› ç´ ä»¥åŠä¸äººç±»åå¥½çš„å…³è”ã€‚ç ”ç©¶å‘ç°ï¼Œå¯¹é½åœ¨æ¨¡å‹çš„ä¸­åæœŸå±‚çº§è¾¾åˆ°é¡¶å³°ï¼Œæ ‡å¿—ç€è¡¨ç¤ºä»æ¨¡æ€ç‰¹å®š(modality-specific)å‘æ¦‚å¿µå…±äº«çš„è½¬å˜ã€‚è¿™ç§å¯¹é½æœ¬è´¨ä¸Šæ˜¯è¯­ä¹‰é©±åŠ¨çš„ï¼Œå¯¹ä»…å¤–è§‚å˜åŒ–å…·æœ‰é²æ£’æ€§ï¼Œä½†åœ¨ç‰©ä½“ç§»é™¤æˆ–è¯åºæ‰“ä¹±ç­‰è¯­ä¹‰ç¯¡æ”¹ä¸‹ä¼šå´©æºƒã€‚é€šè¿‡\"Pick-a-Pic\"å¼ºè¿«é€‰æ‹©ä»»åŠ¡ï¼Œç ”ç©¶è¯æ˜äº†æ¨¡å‹åµŒå…¥ç©ºé—´åœ¨å¤„ç†å¤šå¯¹å¤šå›¾åƒ-æ ‡é¢˜åŒ¹é…æ—¶èƒ½åæ˜ äººç±»åå¥½ï¼Œä¸”è¿™ç§ä¸€è‡´æ€§åœ¨åŒå‘åŒ¹é…åœºæ™¯ä¸‹å‡æˆç«‹ã€‚æ­¤å¤–ï¼Œå¯¹å¤šä¸ªæ ·æœ¬çš„åµŒå…¥è¿›è¡Œèšåˆå¹³å‡å¤„ç†ä¸ä»…ä¸ä¼šæ¨¡ç³Šç»†èŠ‚ï¼Œåè€Œä¼šæ”¾å¤§å¯¹é½æ•ˆæœã€‚è¿™äº›ç»“æœå…±åŒè¡¨æ˜ï¼Œå•æ¨¡æ€ç½‘ç»œä¼šè‡ªå‘æ”¶æ•›äºä¸€ç§ä¸äººç±»åˆ¤æ–­ä¸€è‡´çš„å…±äº«è¯­ä¹‰ç¼–ç ï¼Œä¸”è¿™ç§å¯¹é½åœ¨æ ·æœ¬èšåˆä¸­ä¼šå¾—åˆ°è¿›ä¸€æ­¥åŠ å¼ºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at EMNLP 2025 (camera-ready)",
      "pdf_url": "https://arxiv.org/pdf/2509.20751v1",
      "published_date": "2025-09-25 05:16:28 UTC",
      "updated_date": "2025-09-25 05:16:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:27:27.078789+00:00"
    },
    {
      "arxiv_id": "2509.20750v1",
      "title": "Confidence-guided Refinement Reasoning for Zero-shot Question Answering",
      "title_zh": "é¢å‘é›¶æ ·æœ¬é—®ç­”çš„ç½®ä¿¡åº¦å¼•å¯¼å¼ç²¾ç‚¼æ¨ç†",
      "authors": [
        "Youwon Jang",
        "Woo Suk Choi",
        "Minjoon Jung",
        "Minsu Lee",
        "Byoung-Tak Zhang"
      ],
      "abstract": "We propose Confidence-guided Refinement Reasoning (C2R), a novel training-free framework applicable to question-answering (QA) tasks across text, image, and video domains. C2R strategically constructs and refines sub-questions and their answers (sub-QAs), deriving a better confidence score for the target answer. C2R first curates a subset of sub-QAs to explore diverse reasoning paths, then compares the confidence scores of the resulting answer candidates to select the most reliable final answer. Since C2R relies solely on confidence scores derived from the model itself, it can be seamlessly integrated with various existing QA models, demonstrating consistent performance improvements across diverse models and benchmarks. Furthermore, we provide essential yet underexplored insights into how leveraging sub-QAs affects model behavior, specifically analyzing the impact of both the quantity and quality of sub-QAs on achieving robust and reliable reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†C2R (Confidence-guided Refinement Reasoning)ï¼Œè¿™æ˜¯ä¸€ç§é€‚ç”¨äºæ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘é¢†åŸŸé›¶æ ·æœ¬é—®ç­” (Zero-shot Question Answering) ä»»åŠ¡çš„å…è®­ç»ƒ (training-free) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç­–ç•¥æ€§åœ°æ„å»ºå’Œç»†åŒ–å­é—®é¢˜åŠå…¶ç­”æ¡ˆ (sub-QAs)ï¼Œæ—¨åœ¨ä¸ºç›®æ ‡ç­”æ¡ˆæ¨å¯¼å‡ºæ›´å‡†ç¡®çš„ç½®ä¿¡åº¦å¾—åˆ† (confidence score)ã€‚C2Ré¦–å…ˆç­›é€‰å­é—®é¢˜é›†ä»¥æ¢ç´¢å¤šæ ·åŒ–çš„æ¨ç†è·¯å¾„ï¼Œéšåé€šè¿‡æ¯”è¾ƒä¸åŒå€™é€‰ç­”æ¡ˆçš„ç½®ä¿¡åº¦å¾—åˆ†æ¥é€‰æ‹©æœ€å¯é çš„æœ€ç»ˆç­”æ¡ˆã€‚ç”±äºè¯¥æ–¹æ³•å®Œå…¨ä¾èµ–äºæ¨¡å‹ç”Ÿæˆçš„ç½®ä¿¡åº¦å¾—åˆ†ï¼Œå®ƒå¯ä»¥æ— ç¼é›†æˆåˆ°å„ç§ç°æœ‰é—®ç­”æ¨¡å‹ä¸­ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°æŒç»­çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ·±å…¥åˆ†æäº†å­é—®é¢˜çš„æ•°é‡å’Œè´¨é‡å¯¹å®ç°ç¨³å¥å¯é æ¨ç†çš„å½±å“ï¼Œä¸ºç†è§£ sub-QAs å¦‚ä½•æ”¹å˜æ¨¡å‹è¡Œä¸ºæä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages (including references and appendix)",
      "pdf_url": "https://arxiv.org/pdf/2509.20750v1",
      "published_date": "2025-09-25 05:15:12 UTC",
      "updated_date": "2025-09-25 05:15:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:27:20.491768+00:00"
    },
    {
      "arxiv_id": "2509.20748v1",
      "title": "AI-Enabled Crater-Based Navigation for Lunar Mapping",
      "title_zh": "AI èµ‹èƒ½çš„æœˆçƒæµ‹ç»˜é™¨çŸ³å‘å¯¼èˆª",
      "authors": [
        "Sofia McLeod",
        "Chee-Kheng Chng",
        "Matthew Rodda",
        "Tat-Jun Chin"
      ],
      "abstract": "Crater-Based Navigation (CBN) uses the ubiquitous impact craters of the Moon observed on images as natural landmarks to determine the six degrees of freedom pose of a spacecraft. To date, CBN has primarily been studied in the context of powered descent and landing. These missions are typically short in duration, with high-frequency imagery captured from a nadir viewpoint over well-lit terrain. In contrast, lunar mapping missions involve sparse, oblique imagery acquired under varying illumination conditions over potentially year-long campaigns, posing significantly greater challenges for pose estimation. We bridge this gap with STELLA - the first end-to-end CBN pipeline for long-duration lunar mapping. STELLA combines a Mask R-CNN-based crater detector, a descriptor-less crater identification module, a robust perspective-n-crater pose solver, and a batch orbit determination back-end. To rigorously test STELLA, we introduce CRESENT-365 - the first public dataset that emulates a year-long lunar mapping mission. Each of its 15,283 images is rendered from high-resolution digital elevation models with SPICE-derived Sun angles and Moon motion, delivering realistic global coverage, illumination cycles, and viewing geometries. Experiments on CRESENT+ and CRESENT-365 show that STELLA maintains metre-level position accuracy and sub-degree attitude accuracy on average across wide ranges of viewing angles, illumination conditions, and lunar latitudes. These results constitute the first comprehensive assessment of CBN in a true lunar mapping setting and inform operational conditions that should be considered for future missions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é•¿æœŸæœˆçƒæµ‹ç»˜ä»»åŠ¡ä¸­ç¨€ç–ã€å€¾æ–œå›¾åƒåŠå¤æ‚ç…§æ˜å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†é¦–ä¸ªç«¯åˆ°ç«¯çš„é™¨çŸ³å‘å¯¼èˆª(Crater-Based Navigation, CBN)æµæ°´çº¿ STELLAã€‚è¯¥æ¡†æ¶é›†æˆäº†åŸºäº Mask R-CNN çš„é™¨çŸ³å‘æ£€æµ‹å™¨ã€æ— æè¿°ç¬¦(descriptor-less)è¯†åˆ«æ¨¡å—ã€é²æ£’çš„ Perspective-n-Crater ä½å§¿æ±‚è§£å™¨ä»¥åŠæ‰¹é‡è½¨é“ç¡®å®šåç«¯ï¼Œå®ç°äº†å¯¹èˆªå¤©å™¨å…­è‡ªç”±åº¦ä½å§¿çš„ç²¾å‡†ä¼°è®¡ã€‚ä¸ºäº†è¿›è¡Œä¸¥è°¨éªŒè¯ï¼Œç ”ç©¶è€…å‘å¸ƒäº†é¦–ä¸ªæ¨¡æ‹Ÿä¸€å¹´æœŸæœˆçƒæµ‹ç»˜ä»»åŠ¡çš„å…¬å¼€æ•°æ®é›† CRESENT-365ï¼Œè¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡ 1.5 ä¸‡å¼ å…·æœ‰çœŸå®å…¨çƒè¦†ç›–å’Œå…‰ç…§å¾ªç¯çš„æ¸²æŸ“å›¾åƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSTELLA åœ¨å„ç§è§‚æµ‹è§’åº¦å’Œæœˆçƒçº¬åº¦ä¸‹å‡èƒ½ä¿æŒç±³çº§çš„ä½ç½®ç²¾åº¦å’Œäºšåº¦çº§çš„å§¿æ€ç²¾åº¦ã€‚è¿™ä¸€æˆæœå®Œæˆäº†åœ¨çœŸå®æµ‹ç»˜åœºæ™¯ä¸‹å¯¹ CBN çš„é¦–æ¬¡å…¨é¢è¯„ä¼°ï¼Œä¸ºæœªæ¥æœˆçƒæ¢æµ‹ä»»åŠ¡çš„å®é™…è¿è¡Œæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "41 pages, 21 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.20748v1",
      "published_date": "2025-09-25 05:09:41 UTC",
      "updated_date": "2025-09-25 05:09:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:27:44.553341+00:00"
    },
    {
      "arxiv_id": "2509.25233v2",
      "title": "FedCLF -- Towards Efficient Participant Selection for Federated Learning in Heterogeneous IoV Networks",
      "title_zh": "FedCLFï¼šé¢å‘å¼‚æ„è½¦è”ç½‘è”é‚¦å­¦ä¹ çš„é«˜æ•ˆå‚ä¸è€…é€‰æ‹©",
      "authors": [
        "Kasun Eranda Wijethilake",
        "Adnan Mahmood",
        "Quan Z. Sheng"
      ],
      "abstract": "Federated Learning (FL) is a distributed machine learning technique that preserves data privacy by sharing only the trained parameters instead of the client data. This makes FL ideal for highly dynamic, heterogeneous, and time-critical applications, in particular, the Internet of Vehicles (IoV) networks. However, FL encounters considerable challenges in such networks owing to the high data and device heterogeneity. To address these challenges, we propose FedCLF, i.e., FL with Calibrated Loss and Feedback control, which introduces calibrated loss as a utility in the participant selection process and a feedback control mechanism to dynamically adjust the sampling frequency of the clients. The envisaged approach (a) enhances the overall model accuracy in case of highly heterogeneous data and (b) optimizes the resource utilization for resource constrained IoV networks, thereby leading to increased efficiency in the FL process. We evaluated FedCLF vis-Ã -vis baseline models, i.e., FedAvg, Newt, and Oort, using CIFAR-10 dataset with varying data heterogeneity. Our results depict that FedCLF significantly outperforms the baseline models by up to a 16% improvement in high data heterogeneity-related scenarios with improved efficiency via reduced sampling frequency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è½¦è”ç½‘(IoV)ç¯å¢ƒä¸‹è”é‚¦å­¦ä¹ (Federated Learning)é¢ä¸´çš„æ•°æ®ä¸è®¾å¤‡å¼‚æ„æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†FedCLFæ¡†æ¶ï¼Œå³ä¸€ç§ç»“åˆæ ¡å‡†æŸå¤±(Calibrated Loss)ä¸åé¦ˆæ§åˆ¶(Feedback control)çš„é«˜æ•ˆå‚ä¸è€…é€‰æ‹©æ–¹æ³•ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥æ ¡å‡†æŸå¤±ä½œä¸ºå‚ä¸è€…é€‰æ‹©çš„æ•ˆç”¨æŒ‡æ ‡ï¼Œå¹¶åˆ©ç”¨åé¦ˆæ§åˆ¶æœºåˆ¶åŠ¨æ€è°ƒæ•´å®¢æˆ·ç«¯é‡‡æ ·é¢‘ç‡ï¼Œæ—¨åœ¨æå‡å¼‚æ„æ•°æ®ä¸‹çš„æ¨¡å‹å‡†ç¡®ç‡å¹¶ä¼˜åŒ–èµ„æºåˆ©ç”¨ç‡ã€‚ç ”ç©¶äººå‘˜åœ¨CIFAR-10æ•°æ®é›†ä¸Šå°†FedCLFä¸FedAvgã€Newtå’ŒOortç­‰åŸºçº¿æ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨é«˜åº¦æ•°æ®å¼‚æ„çš„åœºæ™¯ä¸‹ï¼ŒFedCLFæ¯”åŸºçº¿æ¨¡å‹å®ç°äº†é«˜è¾¾16%çš„å‡†ç¡®ç‡æå‡ï¼Œå¹¶é€šè¿‡é™ä½é‡‡æ ·é¢‘ç‡æœ‰æ•ˆå¢å¼ºäº†è”é‚¦å­¦ä¹ è¿‡ç¨‹çš„æ•´ä½“æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Already published in ADMA 2024 on 13th December 2024 Wijethilake, K.E., Mahmood, A., Sheng, Q.Z. (2025). FedCLF - Towards Efficient Participant Selection for Federated Learning in Heterogeneous IoV Networks. In: Sheng, Q.Z., et al. Advanced Data Mining and Applications. ADMA 2024. Lecture Notes in Computer Science(), vol 15388. Springer, Singapore. https://doi.org/10.1007/978-981-96-0814-0_15",
      "pdf_url": "https://arxiv.org/pdf/2509.25233v2",
      "published_date": "2025-09-25 04:51:38 UTC",
      "updated_date": "2025-10-29 03:27:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:27:46.261747+00:00"
    },
    {
      "arxiv_id": "2509.20744v1",
      "title": "Parallel Thinking, Sequential Answering: Bridging NAR and AR for Efficient Reasoning",
      "title_zh": "å¹¶è¡Œæ€è€ƒï¼Œé¡ºåºè§£ç­”ï¼šèåˆéè‡ªå›å½’ä¸è‡ªå›å½’æ¨¡å‹å®ç°é«˜æ•ˆæ¨ç†",
      "authors": [
        "Qihang Ai",
        "Haiyun Jiang"
      ],
      "abstract": "We study reasoning tasks through a framework that integrates auto-regressive (AR) and non-autoregressive (NAR) language models. AR models, which generate text sequentially, excel at producing coherent outputs but often suffer from slow inference, particularly in reasoning-intensive domains such as mathematics and code, where lengthy chains of thought are required. In contrast, NAR models, such as discrete diffusion models, allow parallel generation and offer substantial speedups, though typically at the cost of reduced output quality. To address these limitations, we introduce a new paradigm in which an NAR model efficiently produces intermediate reasoning traces, which subsequently guide an AR model to deliver precise final answers. Experiments demonstrate that our approach yields significant 26% improvements over strong baselines while substantially reducing inference cost.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ¨ç†ä»»åŠ¡ä¸­æ•´åˆè‡ªå›å½’(AR)ä¸éè‡ªå›å½’(NAR)è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ARæ¨¡å‹åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶ç”±äºæ¨ç†é“¾è¿‡é•¿è€Œå¯¼è‡´çš„æ¨ç†é€Ÿåº¦ç¼“æ…¢é—®é¢˜ã€‚ä¸ºäº†å¹³è¡¡ç”Ÿæˆè´¨é‡ä¸æ¨ç†æ•ˆç‡ï¼Œä½œè€…æå‡ºäº†ä¸€ç§â€œå¹¶è¡Œæ€è€ƒï¼Œé¡ºåºå›ç­”â€çš„æ–°èŒƒå¼ï¼Œå°†NARæ¨¡å‹ä¸ARæ¨¡å‹ç›¸ç»“åˆã€‚åœ¨è¯¥æ¡†æ¶ä¸‹ï¼ŒNARæ¨¡å‹é¦–å…ˆé«˜æ•ˆåœ°å¹¶è¡Œç”Ÿæˆä¸­é—´æ¨ç†è½¨è¿¹(intermediate reasoning traces)ï¼Œéšåç”±ARæ¨¡å‹åœ¨è¿™äº›è½¨è¿¹çš„å¼•å¯¼ä¸‹è¾“å‡ºç²¾ç¡®çš„æœ€ç»ˆç­”æ¡ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—é™ä½æ¨ç†æˆæœ¬çš„åŒæ—¶ï¼Œç›¸è¾ƒäºå¼ºåŠ›åŸºå‡†æ¨¡å‹å®ç°äº†26%çš„æ€§èƒ½æå‡ã€‚è¿™ä¸€ç ”ç©¶æˆåŠŸæ¡¥æ¥äº†NARä¸ARæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œä¸ºé«˜æ•ˆçš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ¨ç†æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "4 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.20744v1",
      "published_date": "2025-09-25 04:50:11 UTC",
      "updated_date": "2025-09-25 04:50:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:27:45.766436+00:00"
    },
    {
      "arxiv_id": "2509.20731v1",
      "title": "Imagining Design Workflows in Agentic AI Futures",
      "title_zh": "æ™ºèƒ½ä½“ AI æœªæ¥ä¸­çš„è®¾è®¡å·¥ä½œæµæ„æƒ³",
      "authors": [
        "Samangi Wadinambiarachchi",
        "Jenny Waycott",
        "Yvonne Rogers",
        "Greg Wadley"
      ],
      "abstract": "As designers become familiar with Generative AI, a new concept is emerging: Agentic AI. While generative AI produces output in response to prompts, agentic AI systems promise to perform mundane tasks autonomously, potentially freeing designers to focus on what they love: being creative. But how do designers feel about integrating agentic AI systems into their workflows? Through design fiction, we investigated how designers want to interact with a collaborative agentic AI platform. Ten professional designers imagined and discussed collaborating with an AI agent to organise inspiration sources and ideate. Our findings highlight the roles AI agents can play in supporting designers, the division of authority between humans and AI, and how designers' intent can be explained to AI agents beyond prompts. We synthesise our findings into a conceptual framework that identifies authority distribution among humans and AI agents and discuss directions for utilising AI agents in future design workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Agentic AIåœ¨æœªæ¥è®¾è®¡å·¥ä½œæµä¸­çš„åº”ç”¨å‰æ™¯ï¼Œé‡ç‚¹è°ƒæŸ¥äº†è®¾è®¡å¸ˆå¯¹ä¸è‡ªä¸»AIç³»ç»Ÿåä½œçš„çœ‹æ³•ä¸æœŸæœ›ã€‚ç ”ç©¶é‡‡ç”¨è®¾è®¡è™šæ„(Design Fiction)çš„æ–¹æ³•ï¼Œé‚€è¯·10ä½ä¸“ä¸šè®¾è®¡å¸ˆæ¨¡æ‹Ÿå¹¶è®¨è®ºäº†ä¸AI Agentåœ¨æ•´ç†çµæ„Ÿå’Œæ–¹æ¡ˆæ„æ€æ–¹é¢çš„åä½œè¿‡ç¨‹ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†AI Agentåœ¨æ”¯æŒè®¾è®¡ä»»åŠ¡ä¸­çš„å…·ä½“è§’è‰²ã€äººæœºä¹‹é—´çš„æƒåŠ›åˆ†é…(Authority Distribution)æœºåˆ¶ï¼Œä»¥åŠå¦‚ä½•é€šè¿‡è¶…è¶Šä¼ ç»Ÿæç¤ºè¯(Prompts)çš„æ–¹å¼å‘AIä¼ è¾¾è®¾è®¡æ„å›¾ã€‚æœ€åï¼Œæœ¬æ–‡æç‚¼å‡ºäº†ä¸€ä¸ªç”¨äºè¯†åˆ«äººæœºæƒåŠ›åˆ†å¸ƒçš„ç†è®ºæ¡†æ¶ï¼Œå¹¶ä¸ºæœªæ¥è®¾è®¡æµç¨‹ä¸­æ•´åˆä¸åˆ©ç”¨AI Agentæä¾›äº†æŒ‡å¯¼æ–¹å‘ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "37th Australian Conference on Human-Computer Interaction (HCI) (OZCHI '25)",
      "pdf_url": "https://arxiv.org/pdf/2509.20731v1",
      "published_date": "2025-09-25 04:23:16 UTC",
      "updated_date": "2025-09-25 04:23:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:28:01.465921+00:00"
    },
    {
      "arxiv_id": "2509.20729v2",
      "title": "Robust, Observable, and Evolvable Agentic Systems Engineering: A Principled Framework Validated via the Fairy GUI Agent",
      "title_zh": "ç¨³å¥ã€å¯è§‚æµ‹ä¸”å¯æ¼”è¿›çš„æ™ºèƒ½ä½“ç³»ç»Ÿå·¥ç¨‹ï¼šåŸºäº Fairy GUI æ™ºèƒ½ä½“éªŒè¯çš„åŸåˆ™æ€§æ¡†æ¶",
      "authors": [
        "Jiazheng Sun",
        "Ruimeng Yang",
        "Xu Han",
        "Jiayang Niu",
        "Mingxuan Li",
        "Te Yang",
        "Yongyong Lu",
        "Xin Peng"
      ],
      "abstract": "The Agentic Paradigm faces a significant Software Engineering Absence, yielding Agentic systems commonly lacking robustness, observability, and evolvability. To address these deficiencies, we propose a principled engineering framework comprising Runtime Goal Refinement (RGR), Observable Cognitive Architecture (OCA), and Evolutionary Memory Architecture (EMA). In this framework, RGR ensures robustness and intent alignment via knowledge-constrained refinement and human-in-the-loop clarification; OCA builds an observable and maintainable white-box architecture using component decoupling, logic layering, and state-control separation; and EMA employs an execution-evolution dual-loop for evolvability. We implemented and empirically validated Fairy, a mobile GUI agent based on this framework. On RealMobile-Eval, our novel benchmark for ambiguous and complex tasks, Fairy outperformed the best SoTA baseline in user requirement completion by 33.7%. Subsequent controlled experiments, human-subject studies, and ablation studies further confirmed that the RGR enhances refinement accuracy and prevents intent deviation; the OCA improves maintainability; and the EMA is crucial for long-term performance. This research provides empirically validated specifications and a practical blueprint for building reliable, observable, and evolvable Agentic AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Agenticç³»ç»Ÿå› ç¼ºä¹è½¯ä»¶å·¥ç¨‹è§„èŒƒè€Œå¯¼è‡´çš„é²æ£’æ€§ã€å¯è§‚æµ‹æ€§å’Œå¯æ¼”è¿›æ€§ä¸è¶³é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç³»ç»ŸåŒ–çš„å·¥ç¨‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±Runtime Goal Refinement (RGR)ã€Observable Cognitive Architecture (OCA)å’ŒEvolutionary Memory Architecture (EMA)ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶æ„æˆã€‚RGRé€šè¿‡çŸ¥è¯†çº¦æŸå’Œäººæœºåä½œç¡®ä¿æ„å›¾å¯¹é½ä¸é²æ£’æ€§ï¼›OCAé‡‡ç”¨ç»„ä»¶è§£è€¦ã€é€»è¾‘åˆ†å±‚å’ŒçŠ¶æ€æ§åˆ¶åˆ†ç¦»æŠ€æœ¯ï¼Œæ„å»ºäº†å¯ç»´æŠ¤çš„ç™½ç›’æ¶æ„ï¼›EMAåˆ™åˆ©ç”¨æ‰§è¡Œ-æ¼”è¿›åŒå¾ªç¯æœºåˆ¶å®ç°äº†ç³»ç»Ÿçš„æŒç»­è¿›åŒ–ã€‚ç ”ç©¶å›¢é˜ŸåŸºäºæ­¤æ¡†æ¶å¼€å‘äº†ç§»åŠ¨ç«¯GUIæ™ºèƒ½ä½“Fairyï¼Œå¹¶åœ¨å¤„ç†æ¨¡ç³Šå¤æ‚ä»»åŠ¡çš„æ–°åŸºå‡†RealMobile-Evalä¸Šè¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFairyåœ¨ç”¨æˆ·éœ€æ±‚å®Œæˆç‡ä¸Šæ¯”ç°æœ‰æœ€å…ˆè¿›çš„SOTAåŸºå‡†æå‡äº†33.7%ã€‚æ¶ˆèå®éªŒå’Œäººç±»å‚ä¸ç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†å„ç»„ä»¶åœ¨æå‡ç³»ç»Ÿå¯é æ€§ã€é€æ˜åº¦å’Œé•¿æœŸæ€§èƒ½æ–¹é¢çš„å…³é”®ä½œç”¨ï¼Œä¸ºæ„å»ºé«˜æ€§èƒ½Agentic AIç³»ç»Ÿæä¾›äº†å®è·µè“å›¾ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "50 pages, 14 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.20729v2",
      "published_date": "2025-09-25 04:21:31 UTC",
      "updated_date": "2025-12-01 08:01:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:27:53.063326+00:00"
    },
    {
      "arxiv_id": "2509.22725v2",
      "title": "A Meta-Analysis of LLM Effects on Students across Qualification, Socialisation, and Subjectification",
      "title_zh": "LLM å¯¹å­¦ç”Ÿå½±å“çš„å…ƒåˆ†æï¼šåŸºäºèµ„æ ¼åŒ–ã€ç¤¾ä¼šåŒ–ä¸ä¸»ä½“åŒ–çš„å¤šç»´è§†è§’",
      "authors": [
        "Jiayu Huang",
        "Ruoxin Ritter Wang",
        "Jen-Hao Liu",
        "Boming Xia",
        "Yue Huang",
        "Ruoxi Sun",
        "Jason Minhui Xue",
        "Jinan Zou"
      ],
      "abstract": "Large language models (LLMs) are increasingly positioned as solutions for education, yet evaluations often reduce their impact to narrow performance metrics. This paper reframes the question by asking \"what kind of impact should LLMs have in education?\" Drawing on Biesta's tripartite account of good education: qualification, socialisation, and subjectification, we present a meta-analysis of 133 experimental and quasi-experimental studies (k = 188). Overall, the impact of LLMs on student learning is positive but uneven. Strong effects emerge in qualification, particularly when LLMs function as tutors in sustained interventions. Socialisation outcomes appear more variable, concentrated in sustained, reflective interventions. Subjectification, linked to autonomy and learner development, remains fragile, with improvements confined to small-scale, long-term studies. This purpose-level view highlights design as the decisive factor: without scaffolds for participation and agency, LLMs privilege what is easiest to measure while neglecting broader aims of education. For HCI and education, the issue is not just whether LLMs work, but what futures they enable or foreclose.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨æ•™è‚²é¢†åŸŸçš„å½±å“è¿›è¡Œäº†å…ƒåˆ†æ(Meta-analysis)ï¼ŒåŸºäºBiestaå…³äºQualificationã€Socialisationå’ŒSubjectificationçš„ä¸‰é‡æ•™è‚²ç›®æ ‡æ¡†æ¶ï¼Œç³»ç»Ÿåˆ†æäº†133é¡¹å®éªŒæ€§ç ”ç©¶ã€‚ç ”ç©¶å‘ç°LLMså¯¹å­¦ç”Ÿå­¦ä¹ çš„å½±å“æ€»ä½“ä¸ºæ­£å‘ä½†åˆ†å¸ƒä¸å‡ï¼Œåœ¨Qualificationæ–¹é¢ï¼Œå½“LLMsåœ¨é•¿æœŸå¹²é¢„ä¸­å……å½“Tutorsæ—¶è¡¨ç°å‡ºå¼ºåŠ²çš„æ­£å‘æ•ˆæœã€‚Socialisationç»´åº¦çš„ç»“æœåˆ™æ›´å…·æ³¢åŠ¨æ€§ï¼Œä¸”ä¸»è¦é›†ä¸­åœ¨æŒç»­æ€§çš„åæ€å¹²é¢„ä¸­ã€‚æ¶‰åŠè‡ªä¸»æ€§ä¸å­¦ä¹ è€…å‘å±•çš„Subjectificationç»´åº¦ä¾ç„¶è„†å¼±ï¼Œå…¶æ”¹å–„ä»…é™äºå°è§„æ¨¡çš„é•¿æœŸç ”ç©¶ã€‚ç ”ç©¶å¼ºè°ƒè®¾è®¡æ˜¯å†³å®šæ€§å› ç´ ï¼Œè‹¥ç¼ºä¹ä¿ƒè¿›å‚ä¸å’ŒAgencyçš„Scaffoldsï¼ŒLLMså¾€å¾€ä¼šä¼˜å…ˆè€ƒè™‘æ˜“äºè¡¡é‡çš„æŒ‡æ ‡è€Œå¿½è§†æ›´å¹¿æ³›çš„æ•™è‚²ç›®æ ‡ã€‚è¯¥ç ”ç©¶ä¸ºHCIä¸æ•™è‚²é¢†åŸŸæ¢è®¨LLMsç©¶ç«Ÿä¿ƒæˆæˆ–æ’é™¤äº†ä½•ç§æ•™è‚²æœªæ¥æä¾›äº†é‡è¦è§†è§’ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.22725v2",
      "published_date": "2025-09-25 04:11:19 UTC",
      "updated_date": "2025-09-30 10:22:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:28:00.171186+00:00"
    },
    {
      "arxiv_id": "2510.01257v1",
      "title": "RJE: A Retrieval-Judgment-Exploration Framework for Efficient Knowledge Graph Question Answering with LLMs",
      "title_zh": "RJEï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆçŸ¥è¯†å›¾è°±é—®ç­”çš„â€œæ£€ç´¢-åˆ¤å®š-æ¢ç´¢â€æ¡†æ¶",
      "authors": [
        "Can Lin",
        "Zhengwang Jiang",
        "Ling Zheng",
        "Qi Zhao",
        "Yuhang Zhang",
        "Qi Song",
        "Wangqiu Zhou"
      ],
      "abstract": "Knowledge graph question answering (KGQA) aims to answer natural language questions using knowledge graphs. Recent research leverages large language models (LLMs) to enhance KGQA reasoning, but faces limitations: retrieval-based methods are constrained by the quality of retrieved information, while agent-based methods rely heavily on proprietary LLMs. To address these limitations, we propose Retrieval-Judgment-Exploration (RJE), a framework that retrieves refined reasoning paths, evaluates their sufficiency, and conditionally explores additional evidence. Moreover, RJE introduces specialized auxiliary modules enabling small-sized LLMs to perform effectively: Reasoning Path Ranking, Question Decomposition, and Retriever-assisted Exploration. Experiments show that our approach with proprietary LLMs (such as GPT-4o-mini) outperforms existing baselines while enabling small open-source LLMs (such as 3B and 8B parameters) to achieve competitive results without fine-tuning LLMs. Additionally, RJE substantially reduces the number of LLM calls and token usage compared to agent-based methods, yielding significant efficiency improvements.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RJEï¼ˆRetrieval-Judgment-Explorationï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³çŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰ä¸­æ£€ç´¢è´¨é‡å—é™ä»¥åŠæ™ºèƒ½ä½“æ–¹æ³•è¿‡åº¦ä¾èµ–æ˜‚è´µé—­æºå¤§æ¨¡å‹çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ£€ç´¢ç²¾ç‚¼çš„æ¨ç†è·¯å¾„å¹¶è¯„ä¼°å…¶å……åˆ†æ€§ï¼Œç»“åˆæ¡ä»¶å¼çš„è¯æ®æ¢ç´¢ï¼Œå®ç°äº†é«˜æ•ˆä¸”ç²¾å‡†çš„çŸ¥è¯†æ¨ç†ã€‚RJEå¼•å…¥äº†æ¨ç†è·¯å¾„æ’åºï¼ˆReasoning Path Rankingï¼‰ã€é—®é¢˜åˆ†è§£ï¼ˆQuestion Decompositionï¼‰å’Œæ£€ç´¢è¾…åŠ©æ¢ç´¢ï¼ˆRetriever-assisted Explorationï¼‰ç­‰æ¨¡å—ï¼Œæ˜¾è‘—å¢å¼ºäº†å°è§„æ¨¡å¼€æºå¤§æ¨¡å‹åœ¨ä¸ç»è¿‡å¾®è°ƒçš„æƒ…å†µä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä½¿ç”¨GPT-4o-miniç­‰æ¨¡å‹æ—¶ä¼˜äºç°æœ‰åŸºå‡†ï¼ŒåŒæ—¶å¤§å¹…å‡å°‘äº†å¤§æ¨¡å‹çš„è°ƒç”¨æ¬¡æ•°å’ŒTokenä½¿ç”¨é‡ï¼Œåœ¨æ€§èƒ½ä¸æ•ˆç‡ä¹‹é—´å–å¾—äº†å“è¶Šçš„å¹³è¡¡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.01257v1",
      "published_date": "2025-09-25 03:56:18 UTC",
      "updated_date": "2025-09-25 03:56:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:27:59.878198+00:00"
    },
    {
      "arxiv_id": "2509.20717v1",
      "title": "RobotDancing: Residual-Action Reinforcement Learning Enables Robust Long-Horizon Humanoid Motion Tracking",
      "title_zh": "RobotDancingï¼šåŸºäºæ®‹å·®åŠ¨ä½œå¼ºåŒ–å­¦ä¹ çš„äººå½¢æœºå™¨äººç¨³å¥é•¿æ—¶ç¨‹è¿åŠ¨è¿½è¸ª",
      "authors": [
        "Zhenguo Sun",
        "Yibo Peng",
        "Yuan Meng",
        "Xukun Li",
        "Bo-Sheng Huang",
        "Zhenshan Bing",
        "Xinlong Wang",
        "Alois Knoll"
      ],
      "abstract": "Long-horizon, high-dynamic motion tracking on humanoids remains brittle because absolute joint commands cannot compensate model-plant mismatch, leading to error accumulation. We propose RobotDancing, a simple, scalable framework that predicts residual joint targets to explicitly correct dynamics discrepancies. The pipeline is end-to-end--training, sim-to-sim validation, and zero-shot sim-to-real--and uses a single-stage reinforcement learning (RL) setup with a unified observation, reward, and hyperparameter configuration. We evaluate primarily on Unitree G1 with retargeted LAFAN1 dance sequences and validate transfer on H1/H1-2. RobotDancing can track multi-minute, high-energy behaviors (jumps, spins, cartwheels) and deploys zero-shot to hardware with high motion tracking quality.",
      "tldr_zh": "é’ˆå¯¹äººå½¢æœºå™¨äººåœ¨é•¿æ—¶ç¨‹ã€é«˜åŠ¨æ€è¿åŠ¨è·Ÿè¸ªä¸­å› ç»å¯¹å…³èŠ‚æŒ‡ä»¤æ— æ³•è¡¥å¿æ¨¡å‹ä¸ç‰©ç†å®ä½“å·®å¼‚ï¼ˆmodel-plant mismatchï¼‰è€Œå¯¼è‡´çš„è¯¯å·®ç´¯ç§¯é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†RobotDancingæ¡†æ¶ã€‚RobotDancingæ˜¯ä¸€ç§ç®€å•ä¸”å¯æ‰©å±•çš„æ¡†æ¶ï¼Œé€šè¿‡é¢„æµ‹æ®‹å·®å…³èŠ‚ç›®æ ‡ï¼ˆresidual joint targetsï¼‰æ¥æ˜¾å¼ä¿®æ­£åŠ¨åŠ›å­¦å·®å¼‚ã€‚è¯¥æµç¨‹é‡‡ç”¨ç«¯åˆ°ç«¯çš„è®¾è®¡ï¼Œæ¶µç›–äº†è®­ç»ƒã€ä»¿çœŸåˆ°ä»¿çœŸï¼ˆsim-to-simï¼‰éªŒè¯ä»¥åŠé›¶æ ·æœ¬ï¼ˆzero-shotï¼‰ä»ä»¿çœŸåˆ°ç°å®ï¼ˆsim-to-realï¼‰çš„è¿ç§»ï¼Œå¹¶ä½¿ç”¨å•é˜¶æ®µå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®¾ç½®åŠç»Ÿä¸€çš„è§‚æµ‹ã€å¥–åŠ±å’Œè¶…å‚æ•°é…ç½®ã€‚ç ”ç©¶äººå‘˜åœ¨Unitree G1æœºå™¨äººä¸Šåˆ©ç”¨é‡å®šå‘çš„LAFAN1èˆè¹ˆåºåˆ—è¿›è¡Œäº†ä¸»è¦è¯„ä¼°ï¼Œå¹¶åœ¨H1å’ŒH1-2å¹³å°ä¸ŠéªŒè¯äº†å…¶è¿ç§»èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRobotDancingèƒ½å¤Ÿè·Ÿè¸ªé•¿è¾¾æ•°åˆ†é’Ÿçš„é«˜èƒ½é‡è¡Œä¸ºï¼Œå¦‚è·³è·ƒã€æ—‹è½¬å’Œä¾§æ‰‹ç¿»ï¼Œå¹¶èƒ½åœ¨ç¡¬ä»¶ä¸Šå®ç°é«˜è´¨é‡çš„é›¶æ ·æœ¬éƒ¨ç½²ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20717v1",
      "published_date": "2025-09-25 03:30:34 UTC",
      "updated_date": "2025-09-25 03:30:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:28:03.085494+00:00"
    },
    {
      "arxiv_id": "2509.20715v3",
      "title": "Beyond the Individual: Introducing Group Intention Forecasting with SHOT Dataset",
      "title_zh": "è¶…è¶Šä¸ªä½“ï¼šå¼•å…¥åŸºäº SHOT æ•°æ®é›†çš„ç¾¤ä½“æ„å›¾é¢„æµ‹",
      "authors": [
        "Ruixu Zhang",
        "Yuran Wang",
        "Xinyi Hu",
        "Chaoyu Mai",
        "Wenxuan Liu",
        "Danni Xu",
        "Xian Zhong",
        "Zheng Wang"
      ],
      "abstract": "Intention recognition has traditionally focused on individual intentions, overlooking the complexities of collective intentions in group settings. To address this limitation, we introduce the concept of group intention, which represents shared goals emerging through the actions of multiple individuals, and Group Intention Forecasting (GIF), a novel task that forecasts when group intentions will occur by analyzing individual actions and interactions before the collective goal becomes apparent. To investigate GIF in a specific scenario, we propose SHOT, the first large-scale dataset for GIF, consisting of 1,979 basketball video clips captured from 5 camera views and annotated with 6 types of individual attributes. SHOT is designed with 3 key characteristics: multi-individual information, multi-view adaptability, and multi-level intention, making it well-suited for studying emerging group intentions. Furthermore, we introduce GIFT (Group Intention ForecasTer), a framework that extracts fine-grained individual features and models evolving group dynamics to forecast intention emergence. Experimental results confirm the effectiveness of SHOT and GIFT, establishing a strong foundation for future research in group intention forecasting. The dataset is available at https://xinyi-hu.github.io/SHOT_DATASET.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿæ„å›¾è¯†åˆ«è¿‡åº¦å…³æ³¨ä¸ªä½“ã€å¿½è§†ç¾¤ä½“å¤æ‚æ€§çš„å±€é™ï¼Œæå‡ºäº†Group Intentionï¼ˆç¾¤ä½“æ„å›¾ï¼‰çš„æ¦‚å¿µåŠå…¶é¢„æµ‹ä»»åŠ¡Group Intention Forecasting (GIF)ã€‚GIFæ—¨åœ¨é€šè¿‡åˆ†æé›†ä½“ç›®æ ‡æ˜¾ç°å‰çš„ä¸ªä½“è¡Œä¸ºä¸äº’åŠ¨ï¼Œé¢„æµ‹ç¾¤ä½“æ„å›¾ä½•æ—¶å‘ç”Ÿã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†é¦–ä¸ªå¤§è§„æ¨¡GIFæ•°æ®é›†SHOTï¼ŒåŒ…å«1979ä¸ªå¤šè§†è§’ç¯®çƒè§†é¢‘ç‰‡æ®µï¼Œå¹¶æ ‡æ³¨äº†6ç±»ä¸ªä½“å±æ€§ã€‚è¯¥æ•°æ®é›†å…·å¤‡multi-individual informationã€multi-view adaptabilityå’Œmulti-level intentionä¸‰å¤§æ ¸å¿ƒç‰¹å¾ï¼Œéå¸¸é€‚åˆç ”ç©¶æ–°å…´çš„ç¾¤ä½“æ„å›¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†GIFT (Group Intention ForecasTer) æ¡†æ¶ï¼Œé€šè¿‡æå–ç»†ç²’åº¦ä¸ªä½“ç‰¹å¾å¹¶å»ºæ¨¡æ¼”åŒ–çš„ç¾¤ä½“åŠ¨åŠ›å­¦(evolving group dynamics)æ¥å®ç°æ„å›¾é¢„æµ‹ã€‚å®éªŒç»“æœéªŒè¯äº†SHOTå’ŒGIFTçš„æœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥ç¾¤ä½“æ„å›¾é¢„æµ‹é¢†åŸŸçš„ç ”ç©¶å¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ACMMM 2025 Datasets Track",
      "pdf_url": "https://arxiv.org/pdf/2509.20715v3",
      "published_date": "2025-09-25 03:28:01 UTC",
      "updated_date": "2025-10-01 12:41:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:28:08.898613+00:00"
    },
    {
      "arxiv_id": "2509.20707v2",
      "title": "An Automated Retrieval-Augmented Generation LLaMA-4 109B-based System for Evaluating Radiotherapy Treatment Plans",
      "title_zh": "åŸºäº LLaMA-4 109B çš„æ”¾å°„æ²»ç–—è®¡åˆ’è¯„ä¼°è‡ªåŠ¨åŒ–æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ",
      "authors": [
        "Junjie Cui",
        "Peilong Wang",
        "Jason Holmes",
        "Leshan Sun",
        "Michael L. Hinni",
        "Barbara A. Pockaj",
        "Sujay A. Vora",
        "Terence T. Sio",
        "William W. Wong",
        "Nathan Y. Yu",
        "Steven E. Schild",
        "Joshua R. Niska",
        "Sameer R. Keole",
        "Jean-Claude M. Rwigema",
        "Samir H. Patel",
        "Lisa A. McGee",
        "Carlos A. Vargas",
        "Wei Liu"
      ],
      "abstract": "Purpose: To develop a retrieval-augmented generation (RAG) system powered by LLaMA-4 109B for automated, protocol-aware, and interpretable evaluation of radiotherapy treatment plans.\n  Methods and Materials: We curated a multi-protocol dataset of 614 radiotherapy plans across four disease sites and constructed a knowledge base containing normalized dose metrics and protocol-defined constraints. The RAG system integrates three core modules: a retrieval engine optimized across five SentenceTransformer backbones, a percentile prediction component based on cohort similarity, and a clinical constraint checker. These tools are directed by a large language model (LLM) using a multi-step prompt-driven reasoning pipeline to produce concise, grounded evaluations.\n  Results: Retrieval hyperparameters were optimized using Gaussian Process on a scalarized loss function combining root mean squared error (RMSE), mean absolute error (MAE), and clinically motivated accuracy thresholds. The best configuration, based on all-MiniLM-L6-v2, achieved perfect nearest-neighbor accuracy within a 5-percentile-point margin and a sub-2pt MAE. When tested end-to-end, the RAG system achieved 100% agreement with the computed values by standalone retrieval and constraint-checking modules on both percentile estimates and constraint identification, confirming reliable execution of all retrieval, prediction and checking steps.\n  Conclusion: Our findings highlight the feasibility of combining structured population-based scoring with modular tool-augmented reasoning for transparent, scalable plan evaluation in radiation therapy. The system offers traceable outputs, minimizes hallucination, and demonstrates robustness across protocols. Future directions include clinician-led validation, and improved domain-adapted retrieval models to enhance real-world integration.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§åŸºäº LLaMA-4 109B çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ç³»ç»Ÿï¼Œæ—¨åœ¨å®ç°è‡ªåŠ¨åŒ–ã€ç¬¦åˆåè®®ä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„æ”¾å°„æ²»ç–—è®¡åˆ’ (radiotherapy treatment plans) è¯„ä¼°ã€‚è¯¥ç³»ç»Ÿé›†æˆäº†ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šåˆ©ç”¨ SentenceTransformer ä¼˜åŒ–çš„æ£€ç´¢å¼•æ“ã€åŸºäºé˜Ÿåˆ—ç›¸ä¼¼æ€§çš„ç™¾åˆ†ä½æ•°é¢„æµ‹ç»„ä»¶ä»¥åŠä¸´åºŠçº¦æŸæ£€æŸ¥å™¨ (clinical constraint checker)ã€‚é€šè¿‡å¤šæ­¥æç¤ºé©±åŠ¨çš„æ¨ç†ç®¡çº¿ (reasoning pipeline)ï¼Œå¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè°ƒç”¨çŸ¥è¯†åº“ä¸­çš„å‰‚é‡æŒ‡æ ‡å’Œçº¦æŸæ¡ä»¶è¿›è¡Œç²¾å‡†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ç™¾åˆ†ä½æ•°ä¼°è®¡å’Œçº¦æŸè¯†åˆ«ä¸Šä¸ç‹¬ç«‹æ¨¡å—è¾¾åˆ°äº† 100% çš„ä¸€è‡´æ€§ï¼Œä¸”å¹³å‡ç»å¯¹è¯¯å·® (MAE) ä½äº 2ptã€‚ç ”ç©¶è¯æ˜äº†å°†ç»“æ„åŒ–äººç¾¤è¯„åˆ†ä¸å·¥å…·å¢å¼ºæ¨ç†ç›¸ç»“åˆçš„å¯è¡Œæ€§ï¼Œä¸ºæ”¾å°„æ²»ç–—æä¾›äº†é€æ˜ä¸”å¯æ‰©å±•çš„è¯„ä¼°æ–¹æ¡ˆï¼Œæœ‰æ•ˆå‡å°‘äº†æ¨¡å‹å¹»è§‰ (hallucination) å¹¶å¢å¼ºäº†ç³»ç»Ÿåœ¨ä¸åŒåè®®ä¸‹çš„ç¨³å¥æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 4 figures. Submitted to npj Digital Medicine",
      "pdf_url": "https://arxiv.org/pdf/2509.20707v2",
      "published_date": "2025-09-25 03:18:31 UTC",
      "updated_date": "2025-09-29 02:04:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:28:13.870171+00:00"
    },
    {
      "arxiv_id": "2509.20703v1",
      "title": "Joint Flow Trajectory Optimization For Feasible Robot Motion Generation from Video Demonstrations",
      "title_zh": "åŸºäºè§†é¢‘æ¼”ç¤ºçš„å¯è¡Œæœºå™¨äººè¿åŠ¨ç”Ÿæˆçš„å…³èŠ‚æµè½¨è¿¹ä¼˜åŒ–",
      "authors": [
        "Xiaoxiang Dong",
        "Matthew Johnson-Roberson",
        "Weiming Zhi"
      ],
      "abstract": "Learning from human video demonstrations offers a scalable alternative to teleoperation or kinesthetic teaching, but poses challenges for robot manipulators due to embodiment differences and joint feasibility constraints. We address this problem by proposing the Joint Flow Trajectory Optimization (JFTO) framework for grasp pose generation and object trajectory imitation under the video-based Learning-from-Demonstration (LfD) paradigm. Rather than directly imitating human hand motions, our method treats demonstrations as object-centric guides, balancing three objectives: (i) selecting a feasible grasp pose, (ii) generating object trajectories consistent with demonstrated motions, and (iii) ensuring collision-free execution within robot kinematics. To capture the multimodal nature of demonstrations, we extend flow matching to $\\SE(3)$ for probabilistic modeling of object trajectories, enabling density-aware imitation that avoids mode collapse. The resulting optimization integrates grasp similarity, trajectory likelihood, and collision penalties into a unified differentiable objective. We validate our approach in both simulation and real-world experiments across diverse real-world manipulation tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»è§†é¢‘æ¼”ç¤ºä¸­å­¦ä¹ (Learning-from-Demonstration, LfD)æ—¶é¢ä¸´çš„å…·èº«å·®å¼‚å’Œå…³èŠ‚å¯è¡Œæ€§çº¦æŸé—®é¢˜ï¼Œæå‡ºäº†è”åˆæµè½¨è¿¹ä¼˜åŒ–(Joint Flow Trajectory Optimization, JFTO)æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†è§†é¢‘æ¼”ç¤ºè§†ä¸ºä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„å¼•å¯¼ï¼Œè€Œéç›´æ¥æ¨¡ä»¿äººç±»æ‰‹éƒ¨åŠ¨ä½œï¼Œæ—¨åœ¨å¹³è¡¡æŠ“å–ä½å§¿é€‰æ‹©ã€ç‰©ä½“è½¨è¿¹æ¨¡ä»¿ä»¥åŠæœºå™¨äººè¿åŠ¨å­¦çº¦æŸä¸‹çš„æ— ç¢°æ’æ‰§è¡Œã€‚ä¸ºäº†æ•æ‰æ¼”ç¤ºçš„å¤šæ¨¡æ€ç‰¹æ€§ï¼Œç ”ç©¶è€…å°†æµåŒ¹é…(Flow Matching)æŠ€æœ¯æ‰©å±•è‡³$\\SE(3)$ç©ºé—´è¿›è¡Œç‰©ä½“è½¨è¿¹çš„æ¦‚ç‡å»ºæ¨¡ï¼Œæœ‰æ•ˆé¿å…äº†æ¨¡ä»¿å­¦ä¹ ä¸­çš„æ¨¡å¼å´©å¡Œã€‚æ‰€æå‡ºçš„ä¼˜åŒ–æ–¹æ¡ˆå°†æŠ“å–ç›¸ä¼¼åº¦ã€è½¨è¿¹ä¼¼ç„¶å’Œç¢°æ’æƒ©ç½šæ•´åˆè¿›ä¸€ä¸ªç»Ÿä¸€çš„å¯å¾®ç›®æ ‡å‡½æ•°ä¸­ã€‚å®éªŒåœ¨ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œçš„å¤šç§æ“æ§ä»»åŠ¡ä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶åœ¨ç”Ÿæˆå¯è¡Œä¸”ä¸€è‡´çš„æœºå™¨äººåŠ¨ä½œæ–¹é¢çš„ä¼˜è¶Šèƒ½åŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20703v1",
      "published_date": "2025-09-25 03:11:07 UTC",
      "updated_date": "2025-09-25 03:11:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:28:26.270694+00:00"
    },
    {
      "arxiv_id": "2509.20702v1",
      "title": "Incorporating LLM Embeddings for Variation Across the Human Genome",
      "title_zh": "èåˆå¤§è¯­è¨€æ¨¡å‹åµŒå…¥çš„äººç±»åŸºå› ç»„å˜å¼‚ç ”ç©¶",
      "authors": [
        "Hongqian Niu",
        "Jordan Bryan",
        "Xihao Li",
        "Didong Li"
      ],
      "abstract": "Recent advances in large language model (LLM) embeddings have enabled powerful representations for biological data, but most applications to date focus only on gene-level information. We present one of the first systematic frameworks to generate variant-level embeddings across the entire human genome. Using curated annotations from FAVOR, ClinVar, and the GWAS Catalog, we constructed semantic text descriptions for 8.9 billion possible variants and generated embeddings at three scales: 1.5 million HapMap3+MEGA variants, ~90 million imputed UK Biobank variants, and ~9 billion all possible variants. Embeddings were produced with both OpenAI's text-embedding-3-large and the open-source Qwen3-Embedding-0.6B models. Baseline experiments demonstrate high predictive accuracy for variant properties, validating the embeddings as structured representations of genomic variation. We outline two downstream applications: embedding-informed hypothesis testing by extending the Frequentist And Bayesian framework to genome-wide association studies, and embedding-augmented genetic risk prediction that enhances standard polygenic risk scores. These resources, publicly available on Hugging Face, provide a foundation for advancing large-scale genomic discovery and precision medicine.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå°†å¤§å‹è¯­è¨€æ¨¡å‹(LLM) Embeddingsåº”ç”¨äºäººç±»å…¨åŸºå› ç»„å˜å¼‚çš„ç³»ç»Ÿæ€§æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ­¤å‰ç”Ÿç‰©æ•°æ®è¡¨ç¤ºå¤šé›†ä¸­åœ¨gene-levelçš„é—®é¢˜ã€‚é€šè¿‡æ•´åˆFAVORã€ClinVarå’ŒGWAS Catalogçš„æ ‡æ³¨ä¿¡æ¯ï¼Œç ”ç©¶è€…ä¸º89äº¿ä¸ªæ½œåœ¨å˜å¼‚æ„å»ºäº†è¯­ä¹‰æ–‡æœ¬æè¿°ï¼Œå¹¶åˆ©ç”¨OpenAIçš„text-embedding-3-largeå’Œå¼€æºçš„Qwen3-Embedding-0.6Bæ¨¡å‹ç”Ÿæˆäº†å¤šå°ºåº¦çš„variant-level Embeddingsã€‚åŸºçº¿å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™äº›Embeddingsåœ¨é¢„æµ‹å˜å¼‚å±æ€§æ–¹é¢è¡¨ç°å‡ºæé«˜çš„å‡†ç¡®æ€§ï¼Œæœ‰æ•ˆéªŒè¯äº†å…¶ä½œä¸ºåŸºå› ç»„å˜å¼‚ç»“æ„åŒ–è¡¨ç¤ºçš„å¯é æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶å±•ç¤ºäº†è¯¥æˆæœåœ¨GWASå‡è®¾æ£€éªŒï¼ˆæ‰©å±•Frequentist And Bayesianæ¡†æ¶ï¼‰ä»¥åŠå¢å¼ºå¤šåŸºå› é£é™©è¯„åˆ†(Polygenic Risk Scores)é—ä¼ é£é™©é¢„æµ‹ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚ç›®å‰ç›¸å…³èµ„æºå·²åœ¨Hugging Faceå¹³å°å…¬å¼€å‘å¸ƒï¼Œä¸ºæ¨åŠ¨å¤§è§„æ¨¡åŸºå› ç»„å‘ç°å’Œç²¾å‡†åŒ»å­¦(precision medicine)ç ”ç©¶æä¾›äº†åšå®çš„åŸºç¡€ã€‚",
      "categories": [
        "stat.AP",
        "cs.AI",
        "q-bio.GN"
      ],
      "primary_category": "stat.AP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20702v1",
      "published_date": "2025-09-25 03:09:16 UTC",
      "updated_date": "2025-09-25 03:09:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:28:32.469751+00:00"
    },
    {
      "arxiv_id": "2510.00091v1",
      "title": "Simulating Student Success in the Age of GenAI: A Kantian-Axiomatic Perspective",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ—¶ä»£çš„å­¦ç”ŸæˆåŠŸæ¨¡æ‹Ÿï¼šåº·å¾·å…¬ç†åŒ–è§†è§’",
      "authors": [
        "Seyma Yaman Kayadibi"
      ],
      "abstract": "This study reinterprets a Monte Carlo simulation of students' perceived success with generative AI (GenAI) through a Kantian-axiomatic lens. Building on prior work, theme-level survey statistics Ease of Use and Learnability, System Efficiency and Learning Burden, and Perceived Complexity and Integration from a representative dataset are used to generate 10,000 synthetic scores per theme on the [1,5] Likert scale. The simulated outputs are evaluated against the axioms of dense linear order without endpoints (DLO): irreflexivity, transitivity, total comparability (connectedness), no endpoints (no greatest and no least; A4-A5), and density (A6). At the data level, the basic ordering axioms (A1-A3) are satisfied, whereas no-endpoints (A4-A5) and density (A6) fail as expected. Likert clipping introduces minimum and maximum observed values, and a finite, discretized sample need not contain a value strictly between any two distinct scores. These patterns are read not as methodological defects but as markers of an epistemological boundary. Following Kant and Friedman, the findings suggest that what simulations capture finite, quantized observations cannot instantiate the ideal properties of an unbounded, dense continuum. Such properties belong to constructive intuition rather than to finite sampling alone. A complementary visualization contrasts the empirical histogram with a sine-curve proxy to clarify this divide. The contribution is interpretive rather than data-expansive: it reframes an existing simulation as a probe of the synthetic a priori structure underlying students' perceptions, showing how formal order-theoretic coherence coexists with principled failures of endpoint-freeness and density in finite empirical models.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»åº·å¾·å…¬ç†åŒ– (Kantian-axiomatic) çš„è§†è§’ï¼Œå¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (GenAI) ç¯å¢ƒä¸‹å­¦ç”Ÿæ„ŸçŸ¥æˆåŠŸçš„è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ (Monte Carlo simulation) è¿›è¡Œäº†é‡æ–°è§£è¯»ã€‚ç ”ç©¶åˆ©ç”¨æ˜“ç”¨æ€§ä¸å­¦ä¹ èƒ½åŠ› (Ease of Use and Learnability)ã€ç³»ç»Ÿæ•ˆç‡ä¸å­¦ä¹ è´Ÿæ‹…ç­‰ç»´åº¦çš„è°ƒæŸ¥ç»Ÿè®¡æ•°æ®ï¼Œç”Ÿæˆäº†1ä¸‡ä¸ªåŸºäº Likert é‡è¡¨çš„åˆæˆè¯„åˆ†ã€‚è¿™äº›æ¨¡æ‹Ÿè¾“å‡ºä¾æ®æ— ç«¯ç‚¹çš„ç¨ å¯†çº¿æ€§åº (Dense Linear Order without endpoints, DLO) å…¬ç†è¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶µç›–äº†è‡ªåæ€§ (irreflexivity)ã€ä¼ é€’æ€§ (transitivity) å’Œç¨ å¯†æ€§ (density) ç­‰æŒ‡æ ‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ•°æ®å±‚è™½ç„¶æ»¡è¶³åŸºæœ¬çš„æ’åºå…¬ç†ï¼Œä½†ç”±äºé‡è¡¨æˆªæ–­æ•ˆåº”å’Œæœ‰é™ç¦»æ•£é‡‡æ ·çš„ç‰¹æ€§ï¼Œåœ¨æ— ç«¯ç‚¹å’Œç¨ å¯†æ€§å…¬ç†ä¸Šè¡¨ç°å‡ºå¿…ç„¶çš„å¤±æ•ˆã€‚ä½œè€…è®¤ä¸ºè¿™ç§å¤±æ•ˆå¹¶éæ–¹æ³•è®ºç¼ºé™·ï¼Œè€Œæ˜¯ä½“ç°äº†æœ‰é™é‡åŒ–è§‚å¯Ÿä¸è¿ç»­ä½“ç†æƒ³å±æ€§ä¹‹é—´çš„è®¤è¯†è®ºè¾¹ç•Œï¼Œå¼ºè°ƒæ­¤ç±»å±æ€§å±äºæ„é€ ç›´è§‚ (constructive intuition) è€Œéå•çº¯çš„ç»éªŒé‡‡æ ·ã€‚è¯¥ç ”ç©¶é€šè¿‡å°†ç°æœ‰æ¨¡æ‹Ÿé‡æ„ä¸ºå¯¹å­¦ç”Ÿæ„ŸçŸ¥åº•å±‚å…ˆå¤©ç»¼åˆ (synthetic a priori) ç»“æ„çš„æ¢æµ‹ï¼Œæ­ç¤ºäº†å½¢å¼åºç†è®ºçš„é€»è¾‘ä¸€è‡´æ€§ä¸æœ‰é™ç»éªŒæ¨¡å‹å±€é™æ€§ä¹‹é—´çš„å…±å­˜å…³ç³»ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "23 pages in total, including 3 embedded Python code blocks, 4 figures, and 2 tables. The article analyzes student perception data simulated from survey-derived Likert statistics, evaluated against six axioms of Dense Linear Order (DLO). Preliminary version published on Zenodo; see External DOI",
      "pdf_url": "https://arxiv.org/pdf/2510.00091v1",
      "published_date": "2025-09-25 03:01:39 UTC",
      "updated_date": "2025-09-25 03:01:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:28:35.163246+00:00"
    },
    {
      "arxiv_id": "2509.20693v1",
      "title": "Learning to Align Molecules and Proteins: A Geometry-Aware Approach to Binding Affinity",
      "title_zh": "å­¦ä¹ åˆ†å­ä¸è›‹ç™½è´¨çš„å¯¹é½ï¼šä¸€ç§å‡ ä½•æ„ŸçŸ¥çš„ç»“åˆäº²å’ŒåŠ›ç ”ç©¶æ–¹æ³•",
      "authors": [
        "Mohammadsaleh Refahi",
        "Bahrad A. Sokhansanj",
        "James R. Brown",
        "Gail Rosen"
      ],
      "abstract": "Accurate prediction of drug-target binding affinity can accelerate drug discovery by prioritizing promising compounds before costly wet-lab screening. While deep learning has advanced this task, most models fuse ligand and protein representations via simple concatenation and lack explicit geometric regularization, resulting in poor generalization across chemical space and time. We introduce FIRM-DTI, a lightweight framework that conditions molecular embeddings on protein embeddings through a feature-wise linear modulation (FiLM) layer and enforces metric structure with a triplet loss. An RBF regression head operating on embedding distances yields smooth, interpretable affinity predictions. Despite its modest size, FIRM-DTI achieves state-of-the-art performance on the Therapeutics Data Commons DTI-DG benchmark, as demonstrated by an extensive ablation study and out-of-domain evaluation. Our results underscore the value of conditioning and metric learning for robust drug-target affinity prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯ç‰©-é¶æ ‡ç»“åˆäº²å’ŒåŠ› (Binding Affinity) é¢„æµ‹ä¸­ç°æœ‰æ¨¡å‹æ³›åŒ–èƒ½åŠ›å·®ä¸”ç¼ºä¹å‡ ä½•æ­£åˆ™åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº† FIRM-DTI è½»é‡çº§æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°åˆ©ç”¨ç‰¹å¾çº¿æ€§è°ƒåˆ¶ (Feature-wise Linear Modulation, FiLM) å±‚å¯¹åˆ†å­ä¸è›‹ç™½è´¨åµŒå…¥è¿›è¡Œæ¡ä»¶åŒ–å¤„ç†ï¼Œå¹¶é€šè¿‡ä¸‰å…ƒç»„æŸå¤± (Triplet Loss) å¼ºåŒ–åº¦é‡ç»“æ„ã€‚æ­¤å¤–ï¼Œæ¨¡å‹é‡‡ç”¨åŸºäºåµŒå…¥è·ç¦»çš„å¾„å‘åŸºå‡½æ•° (RBF) å›å½’å¤´ï¼Œå®ç°äº†å¹³æ»‘ä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„äº²å’ŒåŠ›é¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼ŒFIRM-DTI åœ¨ Therapeutics Data Commons (TDC) DTI-DG åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿› (State-of-the-Art) çš„æ°´å¹³ï¼Œå¹¶åœ¨åŸŸå¤–è¯„ä¼°ä¸­å±•ç°å‡ºä¼˜å¼‚çš„é²æ£’æ€§ã€‚è¯¥æˆæœè¯æ˜äº†æ¡ä»¶åŒ–å¯¹é½ä¸åº¦é‡å­¦ä¹  (Metric Learning) å¯¹äºæå‡è¯ç‰©å‘ç°æ•ˆç‡çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.MN"
      ],
      "primary_category": "cs.LG",
      "comment": "10pages,2 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.20693v1",
      "published_date": "2025-09-25 02:55:24 UTC",
      "updated_date": "2025-09-25 02:55:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:28:37.958270+00:00"
    },
    {
      "arxiv_id": "2509.20682v1",
      "title": "Addressing Gradient Misalignment in Data-Augmented Training for Robust Speech Deepfake Detection",
      "title_zh": "è§£å†³æ•°æ®å¢å¼ºè®­ç»ƒä¸­çš„æ¢¯åº¦å¤±é…ï¼Œå®ç°é²æ£’è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹",
      "authors": [
        "Duc-Tuan Truong",
        "Tianchi Liu",
        "Junjie Li",
        "Ruijie Tao",
        "Kong Aik Lee",
        "Eng Siong Chng"
      ],
      "abstract": "In speech deepfake detection (SDD), data augmentation (DA) is commonly used to improve model generalization across varied speech conditions and spoofing attacks. However, during training, the backpropagated gradients from original and augmented inputs may misalign, which can result in conflicting parameter updates. These conflicts could hinder convergence and push the model toward suboptimal solutions, thereby reducing the benefits of DA. To investigate and address this issue, we design a dual-path data-augmented (DPDA) training framework with gradient alignment for SDD. In our framework, each training utterance is processed through two input paths: one using the original speech and the other with its augmented version. This design allows us to compare and align their backpropagated gradient directions to reduce optimization conflicts. Our analysis shows that approximately 25% of training iterations exhibit gradient conflicts between the original inputs and their augmented counterparts when using RawBoost augmentation. By resolving these conflicts with gradient alignment, our method accelerates convergence by reducing the number of training epochs and achieves up to an 18.69% relative reduction in Equal Error Rate on the In-the-Wild dataset compared to the baseline.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹(Speech Deepfake Detection, SDD)ä¸­æ•°æ®å¢å¼º(Data Augmentation, DA)å¼•èµ·çš„æ¢¯åº¦å¤±é…é—®é¢˜ï¼ŒæŒ‡å‡ºåŸå§‹è¾“å…¥ä¸å¢å¼ºæ ·æœ¬äº§ç”Ÿçš„æ¢¯åº¦å†²çªä¼šé™åˆ¶æ¨¡å‹è¾¾åˆ°æœ€ä¼˜è§£ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å¸¦æœ‰æ¢¯åº¦å¯¹é½æœºåˆ¶çš„åŒè·¯å¾„æ•°æ®å¢å¼º(Dual-Path Data-Augmented, DPDA)è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡åŒæ­¥æ¯”è¾ƒå¹¶å¯¹é½åŸå§‹ä¸å¢å¼ºè·¯å¾„çš„åå‘ä¼ æ’­æ¢¯åº¦æ–¹å‘æ¥å‡å°‘ä¼˜åŒ–å†²çªã€‚ç ”ç©¶åˆ†æå‘ç°ï¼Œåœ¨ä½¿ç”¨RawBoostå¢å¼ºæ—¶ï¼Œçº¦25%çš„è®­ç»ƒè¿­ä»£å­˜åœ¨æ¢¯åº¦å†²çªã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—åŠ é€Ÿæ¨¡å‹æ”¶æ•›å¹¶å‡å°‘è®­ç»ƒè½®æ•°ï¼Œåœ¨In-the-Wildæ•°æ®é›†ä¸Šå®ç°äº†ç­‰é”™è¯¯ç‡(Equal Error Rate, EER)æœ€é«˜18.69%çš„ç›¸å¯¹é™å¹…ï¼Œæœ‰æ•ˆæå‡äº†æ£€æµ‹ç³»ç»Ÿçš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "5 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.20682v1",
      "published_date": "2025-09-25 02:31:54 UTC",
      "updated_date": "2025-09-25 02:31:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:28:46.786264+00:00"
    },
    {
      "arxiv_id": "2509.20681v1",
      "title": "Efficient Construction of Implicit Surface Models From a Single Image for Motion Generation",
      "title_zh": "é¢å‘è¿åŠ¨ç”Ÿæˆçš„å•å¹…å›¾åƒé«˜æ•ˆéšå¼æ›²é¢æ¨¡å‹æ„å»º",
      "authors": [
        "Wei-Teng Chu",
        "Tianyi Zhang",
        "Matthew Johnson-Roberson",
        "Weiming Zhi"
      ],
      "abstract": "Implicit representations have been widely applied in robotics for obstacle avoidance and path planning. In this paper, we explore the problem of constructing an implicit distance representation from a single image. Past methods for implicit surface reconstruction, such as \\emph{NeuS} and its variants generally require a large set of multi-view images as input, and require long training times. In this work, we propose Fast Image-to-Neural Surface (FINS), a lightweight framework that can reconstruct high-fidelity surfaces and SDF fields based on a single or a small set of images. FINS integrates a multi-resolution hash grid encoder with lightweight geometry and color heads, making the training via an approximate second-order optimizer highly efficient and capable of converging within a few seconds. Additionally, we achieve the construction of a neural surface requiring only a single RGB image, by leveraging pre-trained foundation models to estimate the geometry inherent in the image. Our experiments demonstrate that under the same conditions, our method outperforms state-of-the-art baselines in both convergence speed and accuracy on surface reconstruction and SDF field estimation. Moreover, we demonstrate the applicability of FINS for robot surface following tasks and show its scalability to a variety of benchmark datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Fast Image-to-Neural Surface (FINS)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ä»å•å¼ å›¾åƒé«˜æ•ˆæ„å»ºé«˜ä¿çœŸéšå¼è¡¨é¢æ¨¡å‹å’Œ Signed Distance Function (SDF) åœºçš„è½»é‡çº§æ¡†æ¶ã€‚é’ˆå¯¹ NeuS ç­‰ä¼ ç»Ÿæ–¹æ³•ä¾èµ–å¤šè§†å›¾è¾“å…¥ä¸”è®­ç»ƒæ—¶é—´é•¿çš„é—®é¢˜ï¼ŒFINS é›†æˆäº†å¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼ç¼–ç å™¨ (multi-resolution hash grid encoder) ä»¥åŠè½»é‡çº§çš„å‡ ä½•ä¸é¢œè‰²å¤´ã€‚é€šè¿‡é‡‡ç”¨è¿‘ä¼¼äºŒé˜¶ä¼˜åŒ–å™¨ (approximate second-order optimizer)ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æå‡äº†è®­ç»ƒæ•ˆç‡ï¼Œå¹¶èƒ½åœ¨æ•°ç§’å†…å®ç°æ”¶æ•›ã€‚é’ˆå¯¹å•è§†å›¾æŒ‘æˆ˜ï¼ŒFINS åˆ©ç”¨é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ (foundation models) ä¼°è®¡å›¾åƒä¸­çš„å‡ ä½•ç‰¹å¾ï¼Œä»è€Œä»…ä¾é å•å¼  RGB å›¾åƒå³å¯å®Œæˆç¥ç»è¡¨é¢æ„å»ºã€‚å®éªŒè¡¨æ˜ï¼ŒFINS åœ¨é‡å»ºç²¾åº¦å’Œæ”¶æ•›é€Ÿåº¦ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›åŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜éªŒè¯äº† FINS åœ¨æœºå™¨äººè¡¨é¢è·Ÿéš (surface following) ä»»åŠ¡ä¸­çš„å®ç”¨æ€§åŠå…¶åœ¨å„ç±»åŸºå‡†æ•°æ®é›†ä¸Šçš„å¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20681v1",
      "published_date": "2025-09-25 02:30:05 UTC",
      "updated_date": "2025-09-25 02:30:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:28:42.753256+00:00"
    },
    {
      "arxiv_id": "2509.20679v1",
      "title": "QAMO: Quality-aware Multi-centroid One-class Learning For Speech Deepfake Detection",
      "title_zh": "QAMOï¼šé¢å‘è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹çš„è´¨é‡æ„ŸçŸ¥å¤šè´¨å¿ƒå•ç±»å­¦ä¹ ",
      "authors": [
        "Duc-Tuan Truong",
        "Tianchi Liu",
        "Ruijie Tao",
        "Junjie Li",
        "Kong Aik Lee",
        "Eng Siong Chng"
      ],
      "abstract": "Recent work shows that one-class learning can detect unseen deepfake attacks by modeling a compact distribution of bona fide speech around a single centroid. However, the single-centroid assumption can oversimplify the bona fide speech representation and overlook useful cues, such as speech quality, which reflects the naturalness of the speech. Speech quality can be easily obtained using existing speech quality assessment models that estimate it through Mean Opinion Score. In this paper, we propose QAMO: Quality-Aware Multi-Centroid One-Class Learning for speech deepfake detection. QAMO extends conventional one-class learning by introducing multiple quality-aware centroids. In QAMO, each centroid is optimized to represent a distinct speech quality subspaces, enabling better modeling of intra-class variability in bona fide speech. In addition, QAMO supports a multi-centroid ensemble scoring strategy, which improves decision thresholding and reduces the need for quality labels during inference. With two centroids to represent high- and low-quality speech, our proposed QAMO achieves an equal error rate of 5.09% in In-the-Wild dataset, outperforming previous one-class and quality-aware systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹(Speech Deepfake Detection)ä¸­å•ä¸­å¿ƒä¸€ç±»å­¦ä¹ (One-class Learning)æ¨¡å‹è¿‡åˆ†ç®€åŒ–çœŸå®è¯­éŸ³åˆ†å¸ƒã€å¿½è§†è¯­éŸ³è´¨é‡çº¿ç´¢çš„é—®é¢˜ï¼Œæå‡ºäº†QAMOæ¡†æ¶ã€‚QAMOé€šè¿‡å¼•å…¥å¤šä¸ªè´¨é‡æ„ŸçŸ¥çš„ä¸­å¿ƒ(Quality-aware Centroids)ï¼Œåˆ©ç”¨å¹³å‡ä¸»è§‚æ„è§åˆ†å€¼(Mean Opinion Score)å¯¹ä¸åŒè¯­éŸ³è´¨é‡å­ç©ºé—´è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æ•æ‰çœŸå®è¯­éŸ³çš„ç±»å†…å¤šæ ·æ€§(Intra-class Variability)ã€‚è¯¥ç³»ç»Ÿæ”¯æŒå¤šä¸­å¿ƒé›†æˆè¯„åˆ†ç­–ç•¥ï¼Œåœ¨æé«˜å†³ç­–é˜ˆå€¼å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå‡å°‘äº†æ¨ç†é˜¶æ®µå¯¹è´¨é‡æ ‡ç­¾çš„éœ€æ±‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡è®¾ç½®é«˜ã€ä½è´¨é‡ä¸¤ä¸ªä¸­å¿ƒï¼ŒQAMOåœ¨In-the-Wildæ•°æ®é›†ä¸Šå®ç°äº†5.09%çš„ç­‰é”™è¯¯ç‡(Equal Error Rate)ï¼Œå…¶è¡¨ç°ä¼˜äºæ­¤å‰çš„ä¸€ç±»å­¦ä¹ å’Œè´¨é‡æ„ŸçŸ¥ç³»ç»Ÿã€‚è¿™ä¸€ç ”ç©¶è¯æ˜äº†å¤šä¸­å¿ƒè´¨é‡æ„ŸçŸ¥æ–¹æ³•åœ¨æå‡å¤æ‚ç¯å¢ƒä¸‹æ·±åº¦ä¼ªé€ æ£€æµ‹æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "5 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.20679v1",
      "published_date": "2025-09-25 02:27:49 UTC",
      "updated_date": "2025-09-25 02:27:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:28:43.965028+00:00"
    },
    {
      "arxiv_id": "2509.20678v1",
      "title": "Bispectral OT: Dataset Comparison using Symmetry-Aware Optimal Transport",
      "title_zh": "Bispectral OTï¼šåŸºäºå¯¹ç§°æ„ŸçŸ¥æœ€ä¼˜ä¼ è¾“çš„æ•°æ®é›†å¯¹æ¯”",
      "authors": [
        "Annabel Ma",
        "Kaiying Hou",
        "David Alvarez-Melis",
        "Melanie Weber"
      ],
      "abstract": "Optimal transport (OT) is a widely used technique in machine learning, graphics, and vision that aligns two distributions or datasets using their relative geometry. In symmetry-rich settings, however, OT alignments based solely on pairwise geometric distances between raw features can ignore the intrinsic coherence structure of the data. We introduce Bispectral Optimal Transport, a symmetry-aware extension of discrete OT that compares elements using their representation using the bispectrum, a group Fourier invariant that preserves all signal structure while removing only the variation due to group actions. Empirically, we demonstrate that the transport plans computed with Bispectral OT achieve greater class preservation accuracy than naive feature OT on benchmark datasets transformed with visual symmetries, improving the quality of meaningful correspondences that capture the underlying semantic label structure in the dataset while removing nuisance variation not affecting class or content.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†Bispectral Optimal Transport (Bispectral OT)ï¼Œè¿™æ˜¯ä¸€ç§å¯¹ç§°æ„ŸçŸ¥(symmetry-aware)çš„ç¦»æ•£Optimal Transportæ‰©å±•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸOTåœ¨å¯¹ç§°ä¸°å¯Œåœºæ™¯ä¸‹å®¹æ˜“å¿½ç•¥æ•°æ®å†…åœ¨ç›¸å¹²ç»“æ„çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡bispectrumè¡¨å¾æ¥æ¯”è¾ƒæ•°æ®å…ƒç´ ï¼Œè¿™æ˜¯ä¸€ç§ç¾¤å‚…é‡Œå¶ä¸å˜é‡(group Fourier invariant)ï¼Œèƒ½åœ¨å®Œæ•´ä¿ç•™ä¿¡å·ç»“æ„çš„åŒæ—¶æ¶ˆé™¤ç”±ç¾¤ä½œç”¨(group actions)å¼•èµ·çš„å˜å¼‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å…·æœ‰è§†è§‰å¯¹ç§°æ€§çš„åŸºå‡†æ•°æ®é›†ä¸Šï¼ŒBispectral OTæ¯”ä¼ ç»Ÿçš„ç‰¹å¾OTå…·æœ‰æ›´é«˜çš„ç±»åˆ«ä¿ç•™å‡†ç¡®ç‡ã€‚è¯¥æŠ€æœ¯é€šè¿‡ç§»é™¤ä¸å½±å“å†…å®¹æˆ–ç±»åˆ«çš„å¹²æ‰°å˜å¼‚ï¼Œæ›´æœ‰æ•ˆåœ°æ•è·äº†æ•°æ®é›†åº•å±‚çš„è¯­ä¹‰æ ‡ç­¾ç»“æ„ï¼Œå¹¶å»ºç«‹äº†æ›´æœ‰æ„ä¹‰çš„å¯¹åº”å…³ç³»ã€‚è¯¥ç ”ç©¶ä¸ºå¤„ç†é«˜å¯¹ç§°æ€§æ•°æ®çš„å¯¹é½é—®é¢˜æä¾›äº†æ–°çš„ç†è®ºæ¡†æ¶ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨å¤æ‚å‡ ä½•å˜æ¢ä¸‹çš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to NeurIPS 2025 Workshop on Symmetry and Geometry in Neural Representations (NeurReps)",
      "pdf_url": "https://arxiv.org/pdf/2509.20678v1",
      "published_date": "2025-09-25 02:25:24 UTC",
      "updated_date": "2025-09-25 02:25:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:28:49.293429+00:00"
    },
    {
      "arxiv_id": "2510.01256v1",
      "title": "Kant: An Efficient Unified Scheduling System for Large-Scale AI Clusters",
      "title_zh": "Kantï¼šé¢å‘å¤§è§„æ¨¡ AI é›†ç¾¤çš„é«˜æ•ˆç»Ÿä¸€è°ƒåº¦ç³»ç»Ÿ",
      "authors": [
        "Lingling Zeng",
        "Gen Zhang",
        "Jialin Peng",
        "Xiang Xu",
        "Yuan Xu",
        "Lijun Ma"
      ],
      "abstract": "As AI cluster sizes continue to expand and the demand for large-language-model (LLM) training and inference workloads grows rapidly, traditional scheduling systems face significant challenges in balancing resource utilization, scheduling efficiency, and service quality. This paper presents and evaluates Kant: an efficient unified scheduling platform designed for large-scale AI container clusters, supporting the co-scheduling of both training and inference jobs. Based on the practical implementation of the Kant system, we systematically define a set of key evaluation metrics for AI clusters, including GPU Allocation Ratio (GAR), Scheduling Occupancy Rate (SOR), GPU Node Fragmentation Ratio (GFR), Job Waiting Time Distribution (JWTD), and Job Training Time Estimation Distribution (JTTED), providing a foundation for quantitative performance analysis. Experimental results demonstrate that Kant achieves exceptional performance in clusters ranging from hundreds to tens of thousands of GPUs. By leveraging scheduling strategies such as Backfill and Enhanced Binpack (E-Binpack), the system significantly improves resource utilization and scheduling efficiency, while effectively reducing resource fragmentation and communication overhead in distributed training. The system has been deployed in multiple AI data center clusters, where it stably supports large-scale intelligent computing workloads. This work provides a practical engineering approach for building high-performance, highly available, AI-native scheduling infrastructure.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Kantï¼Œä¸€ä¸ªä¸“ä¸ºå¤§è§„æ¨¡AIå®¹å™¨é›†ç¾¤è®¾è®¡çš„é«˜æ•ˆç»Ÿä¸€è°ƒåº¦ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLM)è®­ç»ƒå’Œæ¨ç†ä»»åŠ¡æ¿€å¢èƒŒæ™¯ä¸‹ï¼Œä¼ ç»Ÿè°ƒåº¦ç³»ç»Ÿåœ¨èµ„æºåˆ©ç”¨ç‡å’Œè°ƒåº¦æ•ˆç‡æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥ç³»ç»Ÿæ”¯æŒè®­ç»ƒä¸æ¨ç†ä»»åŠ¡çš„ååŒè°ƒåº¦(Co-scheduling)ï¼Œå¹¶ç³»ç»Ÿæ€§åœ°å®šä¹‰äº†åŒ…æ‹¬GPU Allocation Ratio (GAR)ã€Scheduling Occupancy Rate (SOR)å’ŒGPU Node Fragmentation Ratio (GFR)åœ¨å†…çš„é‡åŒ–è¯„ä¼°æŒ‡æ ‡ã€‚é€šè¿‡é‡‡ç”¨Backfillå’ŒEnhanced Binpack (E-Binpack)ç­‰è°ƒåº¦ç­–ç•¥ï¼ŒKantåœ¨æ¶µç›–æ•°ç™¾è‡³æ•°ä¸‡ä¸ªGPUçš„é›†ç¾¤è§„æ¨¡ä¸‹å±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼Œè¯¥ç³»ç»Ÿæ˜¾è‘—æå‡äº†èµ„æºåˆ©ç”¨ç‡ï¼Œæœ‰æ•ˆå‡å°‘äº†èµ„æºç¢ç‰‡ä»¥åŠåˆ†å¸ƒå¼è®­ç»ƒä¸­çš„é€šä¿¡å¼€é”€ã€‚ç›®å‰Kantå·²åœ¨å¤šä¸ªAIæ•°æ®ä¸­å¿ƒéƒ¨ç½²å¹¶ç¨³å®šè¿è¡Œï¼Œä¸ºæ„å»ºé«˜æ€§èƒ½ã€é«˜å¯ç”¨çš„AIåŸç”Ÿ(AI-native)è°ƒåº¦åŸºç¡€è®¾æ–½æä¾›äº†é‡è¦çš„å·¥ç¨‹å‚è€ƒã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.IT",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "25 pages,15 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.01256v1",
      "published_date": "2025-09-25 02:25:12 UTC",
      "updated_date": "2025-09-25 02:25:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:28:54.284858+00:00"
    },
    {
      "arxiv_id": "2509.20666v1",
      "title": "Understanding Mode Switching in Human-AI Collaboration: Behavioral Insights and Predictive Modeling",
      "title_zh": "æ¢ç©¶äººæœºåä½œä¸­çš„æ¨¡å¼åˆ‡æ¢ï¼šè¡Œä¸ºæ´å¯Ÿä¸é¢„æµ‹å»ºæ¨¡",
      "authors": [
        "Avinash Ajit Nargund",
        "Arthur Caetano",
        "Kevin Yang",
        "Rose Yiwei Liu",
        "Philip Tezaur",
        "Kriteen Shrestha",
        "Qisen Pan",
        "Tobias HÃ¶llerer",
        "Misha Sra"
      ],
      "abstract": "Human-AI collaboration is typically offered in one of two of user control levels: guidance, where the AI provides suggestions and the human makes the final decision, and delegation, where the AI acts autonomously within user-defined constraints. Systems that integrate both modes, common in robotic surgery or driving assistance, often overlook shifts in user preferences within a task in response to factors like evolving trust, decision complexity, and perceived control. In this work, we investigate how users dynamically switch between higher and lower levels of control during a sequential decision-making task. Using a hand-and-brain chess setup, participants either selected a piece and the AI decided how it moved (brain mode), or the AI selected a piece and the participant decided how it moved (hand mode). We collected over 400 mode-switching decisions from eight participants, along with gaze, emotional state, and subtask difficulty data. Statistical analysis revealed significant differences in gaze patterns and subtask complexity prior to a switch and in the quality of the subsequent move. Based on these results, we engineered behavioral and task-specific features to train a lightweight model that predicted control level switches ($F1 = 0.65$). The model performance suggests that real-time behavioral signals can serve as a complementary input alongside system-driven mode-switching mechanisms currently used. We complement our quantitative results with qualitative factors that influence switching including perceived AI ability, decision complexity, and level of control, identified from post-game interview analysis. The combined behavioral and modeling insights can help inform the design of shared autonomy systems that need dynamic, subtask-level control switches aligned with user intent and evolving task demands.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨äººæœºåä½œ (Human-AI Collaboration) çš„åºåˆ—å†³ç­–ä»»åŠ¡ä¸­ï¼Œç”¨æˆ·å¦‚ä½•åœ¨æŒ‡å¯¼ (guidance) å’Œå§”æ‰˜ (delegation) ä¸¤ç§æ§åˆ¶æ°´å¹³ä¹‹é—´è¿›è¡ŒåŠ¨æ€åˆ‡æ¢ã€‚é€šè¿‡ä¸€ç§â€œæ‰‹è„‘ (hand-and-brain)â€å›½é™…è±¡æ£‹å®éªŒè®¾ç½®ï¼Œç ”ç©¶è€…æ”¶é›†å¹¶åˆ†æäº†æ³¨è§† (gaze)ã€æƒ…ç»ªçŠ¶æ€å’Œå­ä»»åŠ¡éš¾åº¦ç­‰å¤šç»´åº¦è¡Œä¸ºæ•°æ®ã€‚ç»Ÿè®¡ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å¼åˆ‡æ¢å‰çš„æ³¨è§†æ¨¡å¼ (gaze patterns) å’Œä»»åŠ¡å¤æ‚åº¦å…·æœ‰æ˜¾è‘—ç‰¹å¾ï¼Œä¸”åˆ‡æ¢è¡Œä¸ºä¼šå½±å“åç»­çš„ç§»åŠ¨è´¨é‡ã€‚åŸºäºä¸Šè¿°è¡Œä¸ºç‰¹å¾è®­ç»ƒçš„è½»é‡çº§é¢„æµ‹æ¨¡å‹åœ¨é¢„æµ‹æ§åˆ¶æƒåˆ‡æ¢æ–¹é¢è¾¾åˆ°äº† 0.65 çš„ F1 åˆ†æ•°ã€‚ç»“åˆå®šæ€§è®¿è°ˆåˆ†æï¼Œç ”ç©¶è¯†åˆ«å‡º AI èƒ½åŠ›æ„ŸçŸ¥ã€å†³ç­–å¤æ‚åº¦å’Œæ§åˆ¶æ„Ÿæ˜¯å½±å“ç”¨æˆ·åˆ‡æ¢å†³ç­–çš„æ ¸å¿ƒå› ç´ ã€‚è¯¥æˆæœè¯æ˜äº†å®æ—¶è¡Œä¸ºä¿¡å·å¯ä½œä¸ºç°æœ‰æ¨¡å¼åˆ‡æ¢æœºåˆ¶çš„æœ‰æ•ˆè¡¥å……ï¼Œä¸ºè®¾è®¡èƒ½ä¸ç”¨æˆ·æ„å›¾å’ŒåŠ¨æ€ä»»åŠ¡éœ€æ±‚å¯¹é½çš„å…±äº«è‡ªä¸» (shared autonomy) ç³»ç»Ÿæä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20666v1",
      "published_date": "2025-09-25 01:58:46 UTC",
      "updated_date": "2025-09-25 01:58:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:28:57.197425+00:00"
    },
    {
      "arxiv_id": "2509.20652v1",
      "title": "Accelerate Creation of Product Claims Using Generative AI",
      "title_zh": "åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åŠ é€Ÿäº§å“å®£ç§°åˆ›ä½œ",
      "authors": [
        "Po-Yu Liang",
        "Yong Zhang",
        "Tatiana Hwa",
        "Aaron Byers"
      ],
      "abstract": "The benefit claims of a product is a critical driver of consumers' purchase behavior. Creating product claims is an intense task that requires substantial time and funding. We have developed the $\\textbf{Claim Advisor}$ web application to accelerate claim creations using in-context learning and fine-tuning of large language models (LLM). $\\textbf{Claim Advisor}$ was designed to disrupt the speed and economics of claim search, generation, optimization, and simulation. It has three functions: (1) semantically searching and identifying existing claims and/or visuals that resonate with the voice of consumers; (2) generating and/or optimizing claims based on a product description and a consumer profile; and (3) ranking generated and/or manually created claims using simulations via synthetic consumers. Applications in a consumer packaged goods (CPG) company have shown very promising results. We believe that this capability is broadly useful and applicable across product categories and industries. We share our learning to encourage the research and application of generative AI in different industries.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†åä¸ºClaim Advisorçš„Webåº”ç”¨ç¨‹åºï¼Œæ—¨åœ¨åˆ©ç”¨ç”Ÿæˆå¼AI(Generative AI)åŠ é€Ÿäº§å“åˆ©ç›Šä¸»å¼ (Product Claims)çš„åˆ›å»ºè¿‡ç¨‹ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨äº†å¤§è¯­è¨€æ¨¡å‹(LLM)çš„ä¸Šä¸‹æ–‡å­¦ä¹ (In-context learning)å’Œå¾®è°ƒ(Fine-tuning)æŠ€æœ¯ï¼Œæ—¨åœ¨ä»é€Ÿåº¦å’Œç»æµæ€§ä¸Šé¢ è¦†ä¼ ç»Ÿçš„ä¸»å¼ æœç´¢ã€ç”Ÿæˆã€ä¼˜åŒ–å’Œæ¨¡æ‹Ÿæµç¨‹ã€‚Claim Advisoré›†æˆäº†ä¸‰å¤§æ ¸å¿ƒåŠŸèƒ½ï¼šä¸€æ˜¯ä»¥è¯­ä¹‰æœç´¢è¯†åˆ«å¥‘åˆæ¶ˆè´¹è€…å¿ƒå£°çš„ç°æœ‰ä¸»å¼ æˆ–è§†è§‰å…ƒç´ ï¼›äºŒæ˜¯åŸºäºäº§å“æè¿°å’Œæ¶ˆè´¹è€…ç”»åƒ(Consumer profile)è‡ªåŠ¨ç”Ÿæˆå¹¶ä¼˜åŒ–ä¸»å¼ ï¼›ä¸‰æ˜¯åˆ©ç”¨åˆæˆæ¶ˆè´¹è€…(Synthetic consumers)è¿›è¡Œä»¿çœŸæ¨¡æ‹Ÿï¼Œå¯¹ç”Ÿæˆæˆ–äººå·¥åˆ›å»ºçš„ä¸»å¼ è¿›è¡Œæ’åºã€‚åœ¨æ¶ˆè´¹å“(CPG)å…¬å¸ä¸­çš„åˆæ­¥åº”ç”¨æ˜¾ç¤ºå‡ºæå…·å‰æ™¯çš„ç»“æœï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨è·¨è¡Œä¸šäº§å“å¼€å‘ä¸­å…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§å’Œæ¨å¹¿ä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been accepted at the GenProCC workshop (NeurIPS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.20652v1",
      "published_date": "2025-09-25 01:17:13 UTC",
      "updated_date": "2025-09-25 01:17:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:29:30.694791+00:00"
    },
    {
      "arxiv_id": "2509.20645v2",
      "title": "Look Before you Leap: Estimating LLM Benchmark Scores from Descriptions",
      "title_zh": "ä¸‰æ€è€Œåè¡Œï¼šåŸºäºæè¿°çš„å¤§è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•åˆ†æ•°ä¼°ç®—",
      "authors": [
        "Jungsoo Park",
        "Ethan Mendes",
        "Gabriel Stanovsky",
        "Alan Ritter"
      ],
      "abstract": "Progress in large language models is constrained by an evaluation bottleneck: build a benchmark, run models, then iterate. We ask a question: can we forecast outcomes before running any experiments to inform earlier study design? For example, a team building an AI assistant for a certain task can estimate whether expected performance is around 50 or closer to 80, evidence that supports whether to proceed to a pilot study, how to scope it, and how to allocate resources. We study text-only performance forecasting, where a model predicts a score from a redacted task description and intended configuration, with no access to dataset instances. To support systematic study, we curate PRECOG, a corpus of redacted description-performance pairs spanning diverse tasks, domains, and metrics. We scrape task and configuration descriptions from arXiv, yielding 2,290 instances covering 1,519 papers, and construct a leakage free test split using papers published after the knowledge cutoff of the evaluated models. Experiments show the task is challenging but feasible: reasoning models achieve moderate prediction performance with well calibrated uncertainty, reaching mean absolute error as low as 9.9 at high confidence thresholds. We further test a zero-leakage setting, forecasting on newly released datasets or experiments before their papers are indexed, where GPT5 with built in web search still attains nontrivial prediction accuracy. Overall, our corpus and analyses offer an initial step toward open ended anticipatory evaluation, supporting difficulty estimation and smarter experiment prioritization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¯„ä¼°ä¸­çš„ç“¶é¢ˆï¼Œæå‡ºäº†åœ¨å®é™…è¿è¡Œå®éªŒå‰é€šè¿‡æ–‡æœ¬æè¿°é¢„æµ‹ Benchmark åˆ†æ•°çš„æ–°æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åä¸º PRECOG çš„å¤§è§„æ¨¡è¯­æ–™åº“ï¼ŒåŒ…å«ä» 1,519 ç¯‡ arXiv è®ºæ–‡ä¸­æå–çš„ä»»åŠ¡æè¿°ã€é…ç½®ä¿¡æ¯åŠå…¶å¯¹åº”çš„æ€§èƒ½å¾—åˆ†ã€‚è¯¥ä»»åŠ¡é‡‡ç”¨ Text-only performance forecasting æ¨¡å¼ï¼Œå³æ¨¡å‹åœ¨ä¸æ¥è§¦å…·ä½“æ•°æ®é›†å®ä¾‹çš„æƒ…å†µä¸‹ï¼Œä»…å‡­ä»»åŠ¡æè¿°å’Œé…ç½®æ¥é¢„æµ‹åˆ†å€¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥é¢„æµ‹ä»»åŠ¡è™½ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ä½†å…·å¤‡å¯è¡Œæ€§ï¼Œæ¨ç†æ¨¡å‹åœ¨é«˜ç½®ä¿¡åº¦é˜ˆå€¼ä¸‹çš„å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰å¯ä½è‡³ 9.9ã€‚æ­¤å¤–ï¼Œåœ¨é’ˆå¯¹æ–°å‘å¸ƒæ•°æ®é›†çš„ Zero-leakage æµ‹è¯•ä¸­ï¼Œç»“åˆç½‘ç»œæœç´¢çš„æ¨¡å‹ä¾ç„¶å±•ç°å‡ºäº†æ˜¾è‘—çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚è¯¥é¡¹å·¥ä½œä¸º Anticipatory evaluation å¥ å®šäº†åŸºç¡€ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒä»»åŠ¡éš¾åº¦ä¼°ç®—å¹¶è¾…åŠ©ç ”ç©¶è€…è¿›è¡Œæ›´æ˜æ™ºçš„å®éªŒä¼˜å…ˆçº§æ’åºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "32 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.20645v2",
      "published_date": "2025-09-25 01:02:27 UTC",
      "updated_date": "2025-12-01 20:11:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:29:21.985111+00:00"
    },
    {
      "arxiv_id": "2509.20640v2",
      "title": "Adaptive Cybersecurity Architecture for Digital Product Ecosystems Using Agentic AI",
      "title_zh": "åŸºäºæ™ºèƒ½ä½“ AI çš„æ•°å­—äº§å“ç”Ÿæ€ç³»ç»Ÿè‡ªé€‚åº”ç½‘ç»œå®‰å…¨æ¶æ„",
      "authors": [
        "Oluwakemi T. Olayinka",
        "Sumeet Jeswani",
        "Divine Iloh"
      ],
      "abstract": "Traditional static cybersecurity models often struggle with scalability, real-time detection, and contextual responsiveness in the current digital product ecosystems which include cloud services, application programming interfaces (APIs), mobile platforms, and edge devices. This study introduces autonomous goal driven agents capable of dynamic learning and context-aware decision making as part of an adaptive cybersecurity architecture driven by agentic artificial intelligence (AI). To facilitate autonomous threat mitigation, proactive policy enforcement, and real-time anomaly detection, this framework integrates agentic AI across the key ecosystem layers. Behavioral baselining, decentralized risk scoring, and federated threat intelligence sharing are important features. The capacity of the system to identify zero-day attacks and dynamically modify access policies was demonstrated through native cloud simulations. The evaluation results show increased adaptability, decreased response latency, and improved detection accuracy. The architecture provides an intelligent and scalable blueprint for safeguarding complex digital infrastructure and is compatible with zero-trust models, thereby supporting the adherence to international cybersecurity regulations.",
      "tldr_zh": "ä¼ ç»Ÿçš„é™æ€ç½‘ç»œå®‰å…¨æ¨¡å‹åœ¨å¤„ç†æ¶‰åŠäº‘æœåŠ¡ã€APIå’Œè¾¹ç¼˜è®¾å¤‡ç­‰å¤æ‚æ•°å­—äº§å“ç”Ÿæ€ç³»ç»Ÿæ—¶ï¼Œé¢ä¸´å¯æ‰©å±•æ€§ã€å®æ—¶æ£€æµ‹å’Œä¸Šä¸‹æ–‡å“åº”ç­‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä»£ç†äººå·¥æ™ºèƒ½(Agentic AI)çš„è‡ªé€‚åº”ç½‘ç»œå®‰å…¨æ¶æ„ï¼Œå¼•å…¥äº†å…·æœ‰åŠ¨æ€å­¦ä¹ å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å†³ç­–èƒ½åŠ›çš„è‡ªä¸»ç›®æ ‡é©±åŠ¨æ™ºèƒ½ä½“(autonomous goal-driven agents)ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨å…³é”®ç”Ÿæ€ç³»ç»Ÿå±‚é›†æˆ Agentic AIï¼Œå®ç°äº†è‡ªä¸»å¨èƒç¼“è§£ã€ä¸»åŠ¨ç­–ç•¥æ‰§è¡Œå’Œå®æ—¶å¼‚å¸¸æ£€æµ‹ï¼Œå¹¶ç»“åˆäº†è¡Œä¸ºåŸºå‡†æµ‹è¯•(Behavioral baselining)ã€å»ä¸­å¿ƒåŒ–é£é™©è¯„åˆ†(decentralized risk scoring)åŠè”é‚¦å¨èƒæƒ…æŠ¥å…±äº«(federated threat intelligence sharing)ç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚äº‘åŸç”Ÿæ¨¡æ‹Ÿå®éªŒè¯æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«é›¶æ—¥æ”»å‡»(zero-day attacks)å¹¶åŠ¨æ€è°ƒæ•´è®¿é—®ç­–ç•¥ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¶æ„æ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„è‡ªé€‚åº”æ€§ï¼Œé™ä½äº†å“åº”å»¶è¿Ÿå¹¶æé«˜äº†æ£€æµ‹å‡†ç¡®ç‡ã€‚è¯¥ç ”ç©¶ä¸ºä¿æŠ¤å¤æ‚æ•°å­—åŸºç¡€è®¾æ–½æä¾›äº†æ™ºèƒ½ä¸”å¯æ‰©å±•çš„è“å›¾ï¼Œä¸”ä¸é›¶ä¿¡ä»»(zero-trust)æ¨¡å‹ä¿æŒå…¼å®¹å¹¶æ”¯æŒéµå®ˆå›½é™…ç½‘ç»œå®‰å…¨æ³•è§„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20640v2",
      "published_date": "2025-09-25 00:43:53 UTC",
      "updated_date": "2025-10-02 00:45:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:29:20.195123+00:00"
    },
    {
      "arxiv_id": "2509.25230v2",
      "title": "Energy Guided Geometric Flow Matching",
      "title_zh": "èƒ½é‡å¼•å¯¼çš„å‡ ä½•æµåŒ¹é…",
      "authors": [
        "Aaron Zweig",
        "Mingxuan Zhang",
        "Elham Azizi",
        "David Knowles"
      ],
      "abstract": "A useful inductive bias for temporal data is that trajectories should stay close to the data manifold. Traditional flow matching relies on straight conditional paths, and flow matching methods which learn geodesics rely on RBF kernels or nearest neighbor graphs that suffer from the curse of dimensionality. We propose to use score matching and annealed energy distillation to learn a metric tensor that faithfully captures the underlying data geometry and informs more accurate flows. We demonstrate the efficacy of this strategy on synthetic manifolds with analytic geodesics, and interpolation of cell",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Energy Guided Geometric Flow Matchingï¼Œæ—¨åœ¨é€šè¿‡å¼ºåŒ–è½¨è¿¹è´´åˆæ•°æ®æµå½¢(data manifold)çš„å½’çº³åç½®æ¥ä¼˜åŒ–æ—¶é—´æ•°æ®å¤„ç†ã€‚ä¼ ç»Ÿçš„ Flow Matching ä¾èµ–ç›´çº¿è·¯å¾„ï¼Œè€Œç°æœ‰çš„å­¦ä¹ æµ‹åœ°çº¿(geodesics)çš„æ–¹æ³•ç”±äºä¾èµ–æ ¸å‡½æ•°æˆ–è¿‘é‚»å›¾ï¼Œå¸¸é¢ä¸´ç»´åº¦ç¾éš¾çš„æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é™åˆ¶ï¼Œä½œè€…æå‡ºåˆ©ç”¨ score matching å’Œ annealed energy distillation å­¦ä¹ èƒ½å¤Ÿå‡†ç¡®æ•æ‰æ•°æ®å‡ ä½•ç‰¹å¾çš„åº¦é‡å¼ é‡(metric tensor)ï¼Œä»è€ŒæŒ‡å¯¼ç”Ÿæˆæ›´ç²¾ç¡®çš„æµã€‚å®éªŒåœ¨å…·æœ‰è§£ææµ‹åœ°çº¿çš„åˆæˆæµå½¢ä»¥åŠç»†èƒæ’å€¼(cell interpolation)ä»»åŠ¡ä¸ŠéªŒè¯äº†è¯¥ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶åœ¨æ•æ‰åº•å±‚å‡ ä½•å’Œæå‡æµæ¨¡å‹å‡†ç¡®æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25230v2",
      "published_date": "2025-09-25 00:42:28 UTC",
      "updated_date": "2025-11-17 06:32:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:29:39.184377+00:00"
    },
    {
      "arxiv_id": "2509.20639v2",
      "title": "A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹æ”»å‡»é˜²æŠ¤çš„å¿«é€Ÿå¼€å‘ä¸éƒ¨ç½²æ¡†æ¶",
      "authors": [
        "Adam Swanda",
        "Amy Chang",
        "Alexander Chen",
        "Fraser Burch",
        "Paul Kassianik",
        "Konstantin Berlin"
      ],
      "abstract": "The widespread adoption of Large Language Models (LLMs) has revolutionized AI deployment, enabling autonomous and semi-autonomous applications across industries through intuitive language interfaces and continuous improvements in model development. However, the attendant increase in autonomy and expansion of access permissions among AI applications also make these systems compelling targets for malicious attacks. Their inherent susceptibility to security flaws necessitates robust defenses, yet no known approaches can prevent zero-day or novel attacks against LLMs. This places AI protection systems in a category similar to established malware protection systems: rather than providing guaranteed immunity, they minimize risk through enhanced observability, multi-layered defense, and rapid threat response, supported by a threat intelligence function designed specifically for AI-related threats.\n  Prior work on LLM protection has largely evaluated individual detection models rather than end-to-end systems designed for continuous, rapid adaptation to a changing threat landscape. We present a production-grade defense system rooted in established malware detection and threat intelligence practices. Our platform integrates three components: a threat intelligence system that turns emerging threats into protections; a data platform that aggregates and enriches information while providing observability, monitoring, and ML operations; and a release platform enabling safe, rapid detection updates without disrupting customer workflows. Together, these components deliver layered protection against evolving LLM threats while generating training data for continuous model improvement and deploying updates without interrupting production.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”¨äºå¿«é€Ÿå¼€å‘å’Œéƒ¨ç½²é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)æ”»å‡»çš„é˜²å¾¡æ¡†æ¶ï¼Œæ—¨åœ¨åº”å¯¹äººå·¥æ™ºèƒ½åº”ç”¨æ—¥ç›Šå¢é•¿çš„è‡ªä¸»æ€§æ‰€å¸¦æ¥çš„å®‰å…¨å¨èƒã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•éš¾ä»¥é¢„é˜²é›¶æ—¥æ”»å‡»(Zero-day attacks)çš„é—®é¢˜ï¼Œè¯¥ç³»ç»Ÿå€Ÿé‰´äº†æˆç†Ÿçš„æ¶æ„è½¯ä»¶é˜²æŠ¤ä¸å¨èƒæƒ…æŠ¥(Threat Intelligence)å®è·µï¼Œæ„å»ºäº†ä¸€ä¸ªç”±å¨èƒæƒ…æŠ¥ç³»ç»Ÿã€æ•°æ®å¹³å°å’Œå‘å¸ƒå¹³å°ç»„æˆçš„ç”Ÿäº§çº§é˜²å¾¡ä½“ç³»ã€‚å…¶ä¸­ï¼Œæ•°æ®å¹³å°è´Ÿè´£ä¿¡æ¯çš„èšåˆå¯ŒåŒ–ä¸æœºå™¨å­¦ä¹ è¿ç»´(MLOps)ï¼Œè€Œå‘å¸ƒå¹³å°åˆ™æ”¯æŒåœ¨ä¸ä¸­æ–­å®¢æˆ·å·¥ä½œæµçš„æƒ…å†µä¸‹å®ç°æ¢æµ‹æ›´æ–°çš„å¿«é€Ÿå‘å¸ƒã€‚è¯¥æ¡†æ¶é€šè¿‡å¤šå±‚é˜²å¾¡æœºåˆ¶æœ‰æ•ˆåº”å¯¹ä¸æ–­æ¼”å˜çš„LLMå¨èƒï¼Œå¹¶èƒ½åˆ©ç”¨ç”Ÿæˆçš„è®­ç»ƒæ•°æ®å®ç°æ¨¡å‹çš„æŒç»­æ”¹è¿›ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20639v2",
      "published_date": "2025-09-25 00:36:19 UTC",
      "updated_date": "2025-10-17 15:13:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:29:35.483748+00:00"
    },
    {
      "arxiv_id": "2509.20635v2",
      "title": "Learning Terrain-Specialized Policies for Adaptive Locomotion in Challenging Environments",
      "title_zh": "æŒ‘æˆ˜æ€§ç¯å¢ƒä¸‹è‡ªé€‚åº”è¿åŠ¨çš„åœ°å½¢ä¸“ç”¨ç­–ç•¥å­¦ä¹ ",
      "authors": [
        "Matheus P. Angarola",
        "Francisco Affonso",
        "Marcelo Becker"
      ],
      "abstract": "Legged robots must exhibit robust and agile locomotion across diverse, unstructured terrains, a challenge exacerbated under blind locomotion settings where terrain information is unavailable. This work introduces a hierarchical reinforcement learning framework that leverages terrain-specialized policies and curriculum learning to enhance agility and tracking performance in complex environments. We validated our method on simulation, where our approach outperforms a generalist policy by up to 16% in success rate and achieves lower tracking errors as the velocity target increases, particularly on low-friction and discontinuous terrains, demonstrating superior adaptability and robustness across mixed-terrain scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¶³å¼æœºå™¨äººåœ¨å¤æ‚æ— ç»“æ„åœ°å½¢ä¸­çš„ç¨³å¥è¿åŠ¨æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆåœ°å½¢ä¸“åŒ–ç­–ç•¥(terrain-specialized policies)å’Œè¯¾ç¨‹å­¦ä¹ (curriculum learning)çš„åˆ†å±‚å¼ºåŒ–å­¦ä¹ (hierarchical reinforcement learning)æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³ç›²åŠ¨(blind locomotion)è®¾ç½®ä¸‹åœ°å½¢ä¿¡æ¯ç¼ºå¤±çš„é—®é¢˜ï¼Œé€šè¿‡å¤šç­–ç•¥åä½œæå‡æœºå™¨äººçš„æ•æ·æ€§ä¸é€Ÿåº¦è·Ÿè¸ªæ€§èƒ½ã€‚å®éªŒåœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨ä½æ‘©æ“¦(low-friction)å’Œä¸è¿ç»­åœ°å½¢(discontinuous terrains)ç­‰æŒ‘æˆ˜æ€§åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚ç›¸æ¯”äºä¼ ç»Ÿçš„é€šç”¨ç­–ç•¥(generalist policy)ï¼Œè¯¥æ–¹æ³•çš„ä»»åŠ¡æˆåŠŸç‡æå‡äº†é«˜è¾¾16%ï¼Œä¸”åœ¨ç›®æ ‡é€Ÿåº¦å¢åŠ æ—¶ä»èƒ½ä¿æŒè¾ƒä½çš„è·Ÿè¸ªè¯¯å·®ã€‚è¯¥é¡¹å·¥ä½œå±•ç¤ºäº†ç³»ç»Ÿåœ¨æ··åˆåœ°å½¢(mixed-terrain)åœºæ™¯ä¸‹çš„å“è¶Šé€‚åº”æ€§ï¼Œä¸ºå®ç°è¶³å¼æœºå™¨äººçš„é«˜é²æ£’æ€§è¿åŠ¨æ§åˆ¶æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to the 22nd International Conference on Advanced Robotics (ICAR 2025). 7 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.20635v2",
      "published_date": "2025-09-25 00:17:39 UTC",
      "updated_date": "2025-11-03 20:32:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:29:42.191084+00:00"
    },
    {
      "arxiv_id": "2509.20634v2",
      "title": "Recidivism and Peer Influence with LLM Text Embeddings in Low Security Correctional Facilities",
      "title_zh": "ä½å®‰å…¨ç­‰çº§çŸ«æ­£æœºæ„ä¸­åŸºäº LLM æ–‡æœ¬åµŒå…¥çš„å†çŠ¯ä¸åŒä¼´å½±å“",
      "authors": [
        "Shanjukta Nath",
        "Jiwon Hong",
        "Jae Ho Chang",
        "Keith Warren",
        "Subhadeep Paul"
      ],
      "abstract": "Studying peer effects in language is critical because they often reflect behavioral and personality traits that are important determinants of economic outcomes. However, language is unstructured, non-numeric, and high-dimensional. We combine Large Language Model (LLM) embeddings with structural econometric identification to provide a unified framework for identifying peer effects in language. This unified framework is applied to 80,000-120,000 written exchanges among residents of low security correctional facilities. The LLM language profiles predict three-year recidivism 30\\% more accurately than pre-entry covariates alone, showing that text representations capture meaningful signals. We analyze peer effects on multidimensional language embeddings while addressing network endogeneity. We develop novel instrumental variable estimators for peer effects that accommodate multivariate outcomes, sparse networks, and multidimensional latent variables. Our methods achieve root-N consistency and asymptotic normality under realistic sparsity conditions, relaxing the dense-network assumption. Results reveal significant peer effects in residents' language profiles.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ä½å®‰å…¨çº§åˆ«æƒ©æ•™è®¾æ–½ä¸­ï¼Œè¯­è¨€å¦‚ä½•åæ˜ è¡Œä¸ºå’Œæ€§æ ¼ç‰¹å¾å¹¶è¿›è€Œå½±å“å†çŠ¯ç½ªç‡ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€å¥—ç»Ÿä¸€æ¡†æ¶ï¼Œå°†å¤§è¯­è¨€æ¨¡å‹(LLM)ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥(Text Embeddings)ä¸ç»“æ„è®¡é‡ç»æµå­¦è¯†åˆ«æ–¹æ³•ç›¸ç»“åˆï¼Œç”¨äºè¯†åˆ«è¯­è¨€ä¸­çš„åŒä¼´æ•ˆåº”(Peer Effects)ã€‚é€šè¿‡å¯¹æƒ©æ•™è®¾æ–½å±…æ°‘ä¹‹é—´çº¦8ä¸‡è‡³12ä¸‡ä»½ä¹¦é¢äº¤æµè¿›è¡Œåˆ†æï¼ŒLLMè¯­è¨€ç”»åƒé¢„æµ‹ä¸‰å¹´å†çŠ¯ç½ªç‡çš„å‡†ç¡®æ€§æ¯”ä»…ä½¿ç”¨å…¥ç‹±å‰åå˜é‡æé«˜äº†30%ï¼Œè¯æ˜äº†æ–‡æœ¬è¡¨ç¤ºèƒ½å¤Ÿæ•æ‰åˆ°æœ‰æ„ä¹‰çš„ä¿¡å·ã€‚ä¸ºåº”å¯¹ç½‘ç»œå†…ç”Ÿæ€§é—®é¢˜ï¼Œç ”ç©¶å¼€å‘äº†æ–°å‹çš„å·¥å…·å˜é‡ä¼°è®¡å™¨(Instrumental Variable Estimators)ï¼Œè¯¥ä¼°è®¡å™¨èƒ½å¤Ÿé€‚åº”å¤šç»´ç»“æœã€ç¨€ç–ç½‘ç»œå’Œå¤šç»´æ½œåœ¨å˜é‡ï¼Œå¹¶åœ¨ç°å®çš„ç¨€ç–æ¡ä»¶ä¸‹å®ç°äº†æ ¹å·Nä¸€è‡´æ€§(Root-N Consistency)å’Œæ¸è¿‘æ­£æ€æ€§(Asymptotic Normality)ã€‚å®éªŒç»“æœæœ€ç»ˆè¯å®äº†æƒ©æ•™è®¾æ–½å±…æ°‘çš„è¯­è¨€ç”»åƒä¸­å­˜åœ¨æ˜¾è‘—çš„åŒä¼´æ•ˆåº”(Peer Effects)ï¼Œä¸ºåˆ†æé«˜ç»´éç»“æ„åŒ–æ•°æ®ä¸­çš„ç¤¾ä¼šå½±å“æä¾›äº†æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "econ.EM",
        "cs.AI",
        "econ.GN",
        "stat.ME"
      ],
      "primary_category": "econ.EM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20634v2",
      "published_date": "2025-09-25 00:14:57 UTC",
      "updated_date": "2026-01-13 22:21:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:29:51.186876+00:00"
    },
    {
      "arxiv_id": "2509.20627v1",
      "title": "Personalized Federated Dictionary Learning for Modeling Heterogeneity in Multi-site fMRI Data",
      "title_zh": "ä¸ªæ€§åŒ–è”é‚¦å­—å…¸å­¦ä¹ ï¼šé¢å‘å¤šç«™ç‚¹ fMRI æ•°æ®å¼‚è´¨æ€§å»ºæ¨¡",
      "authors": [
        "Yipu Zhang",
        "Chengshuo Zhang",
        "Ziyu Zhou",
        "Gang Qu",
        "Hao Zheng",
        "Yuping Wang",
        "Hui Shen",
        "Hongwen Deng"
      ],
      "abstract": "Data privacy constraints pose significant challenges for large-scale neuroimaging analysis, especially in multi-site functional magnetic resonance imaging (fMRI) studies, where site-specific heterogeneity leads to non-independent and identically distributed (non-IID) data. These factors hinder the development of generalizable models. To address these challenges, we propose Personalized Federated Dictionary Learning (PFedDL), a novel federated learning framework that enables collaborative modeling across sites without sharing raw data. PFedDL performs independent dictionary learning at each site, decomposing each site-specific dictionary into a shared global component and a personalized local component. The global atoms are updated via federated aggregation to promote cross-site consistency, while the local atoms are refined independently to capture site-specific variability, thereby enhancing downstream analysis. Experiments on the ABIDE dataset demonstrate that PFedDL outperforms existing methods in accuracy and robustness across non-IID datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šä¸­å¿ƒåŠŸèƒ½ç£å…±æŒ¯æˆåƒ(fMRI)ç ”ç©¶ä¸­æ•°æ®éšç§é™åˆ¶å’Œç«™ç‚¹é—´å¼‚è´¨æ€§(non-IID)å¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºä¸ªæ€§åŒ–è”é‚¦å­—å…¸å­¦ä¹ (Personalized Federated Dictionary Learning, PFedDL)çš„è”é‚¦å­¦ä¹ æ¡†æ¶ã€‚PFedDLåœ¨æ¯ä¸ªç«™ç‚¹æ‰§è¡Œç‹¬ç«‹çš„å­—å…¸å­¦ä¹ (Dictionary Learning)ï¼Œå°†ç«™ç‚¹ç‰¹å®šå­—å…¸åˆ†è§£ä¸ºå…±äº«çš„å…¨å±€ç»„ä»¶(Global Component)å’Œä¸ªæ€§åŒ–çš„æœ¬åœ°ç»„ä»¶(Local Component)ã€‚å…¨å±€åŸå­é€šè¿‡è”é‚¦èšåˆ(Federated Aggregation)æ›´æ–°ä»¥ç¡®ä¿è·¨ç«™ç‚¹çš„ä¸€è‡´æ€§ï¼Œè€Œæœ¬åœ°åŸå­åˆ™ç‹¬ç«‹ä¼˜åŒ–ä»¥ç²¾ç¡®æ•æ‰ç«™ç‚¹ç‰¹æœ‰çš„å˜å¼‚æ€§ã€‚åœ¨ABIDEæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒPFedDLåœ¨å¤„ç†non-IIDæ•°æ®æ—¶çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥æ¡†æ¶æˆåŠŸå®ç°äº†åœ¨ä¸å…±äº«åŸå§‹æ•°æ®çš„å‰æä¸‹ï¼Œå¯¹å¤šä¸­å¿ƒç¥ç»å½±åƒæ•°æ®è¿›è¡Œé«˜æ•ˆçš„ååŒå»ºæ¨¡ä¸ä¸‹æ¸¸åˆ†æã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20627v1",
      "published_date": "2025-09-25 00:01:02 UTC",
      "updated_date": "2025-09-25 00:01:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T21:30:00.996046+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 218,
  "processed_papers_count": 218,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T21:30:52.579365+00:00"
}
{
  "date": "2024-07-15",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-07-15 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 125 篇论文，主要聚焦 AI 模型优化（如 LLM 和扩散模型）、视觉生成、机器人路径规划、医疗图像处理等领域，其中 Qwen2 模型报告和 LLM 增强的强化学习论文最为引人注目，并涉及知名学者如 David Abel 和 Mark K. Ho 的工作。\n\n### 重点论文讨论\n我将优先选取具有创新性、实际影响或热门话题的论文进行简要分析，将相关主题归类讨论，并快速掠过一些较为基础或应用性较弱的论文。以下按主题排序，先聊 AI 生成和 LLM 优化，再聊机器人和医疗应用。\n\n#### LLM 和生成模型（AI 核心领域，多个高影响力论文）\n- **Qwen2 Technical Report** (Qwen2 Technical Report)：这篇由多位作者（如 An Yang 和 Fei Huang）合作的核心报告介绍了 Qwen2 系列大语言模型，从 0.5B 到 72B 参数，支持多语言和多模态任务。其主要贡献是优化了模型在理解、生成和多语言任务上的性能，Qwen2-72B 模型在 MMLU 和 GSM8K 等基准上超越了开源竞争者，并提供开源权重，方便社区部署。\n- **Arena Learning: Build Data Flywheel for LLMs Post-training via Simulated Chatbot Arena** (Arena Learning: Build Data Flywheel for LLMs Post-training via Simulated Chatbot Arena)：作者提出了一种模拟对话竞技场的方法，通过自监督增强训练数据，实现 LLM 的持续优化。关键发现是通过迭代数据飞轮，模型在知识密集任务上显著提升，实验证明其在 WizardLM-2 应用中有效。\n- **Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion** (Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion)：这篇论文利用扩散模型从行为提示生成策略网络，支持机器人任务的零样本泛化。主要贡献是提出行为驱动的生成框架，实验显示其在模拟和真实机器人任务中提升了性能，扩展了 LLM 在强化学习的适用性。\n- **Kinetic Typography Diffusion Model** (Kinetic Typography Diffusion Model)：作者开发了一种基于扩散模型的动态文字生成方法，能从文本提示创建艺术字动画。发现在于优化了视觉指导机制，提高了生成质量，但与其他生成模型相比，计算效率需进一步提升。\n\n这些 LLM 相关论文突出了大模型的扩展性和实际应用潜力，尤其 Qwen2 的开源特性可能引发社区广泛讨论。其他如 **Graphusion** 和 **VGBench** 等视觉生成论文虽重要，但较为 niche，我仅简要提及：Graphusion 使用 LLM 生成图表理解任务的数据，VGBench 评估视觉语言模型在矢量图形上的性能，均展示了 LLM 在多模态任务中的潜力。\n\n#### 机器人和强化学习（应用性强，涉及实际场景）\n- **Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation** (Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation)：作者提出了一种跨域推荐框架，使用协作学习减少负转移问题。主要发现是通过不对称网络评估和互信息最大化，提升了推荐系统的性能，并在真实应用中提高了点击率。\n- **Cooperative Reward Shaping for Multi-Agent Pathfinding** (Cooperative Reward Shaping for Multi-Agent Pathfinding)：这篇论文优化了多代理路径规划的奖励机制，使用独立 Q 学习提升协作。贡献在于减少了通信开销，实验在模拟环境中证明了其在大规模代理下的鲁棒性。\n- **Learning Rapid Turning, Aerial Reorientation, and Balancing using Manipulator as a Tail** (Learning Rapid Turning, Aerial Reorientation, and Balancing using Manipulator as a Tail)：作者探索了使用机械臂作为“尾巴”增强四足机器人的机动性，通过强化学习实现了快速转向和平衡。关键发现是模型在真实硬件上表现出色，扩展了机器人敏捷性。\n\n机器人论文强调了强化学习的实用性，如路径规划的效率提升，但其他如 **Social and Ethical Risks Posed by General-Purpose LLMs** 等伦理讨论，我快速掠过：它分析了 LLM 在移民领域的风险，但缺乏新颖方法。\n\n#### 医疗图像和音频处理（高实际价值，快速扫描）\n- **Classification of Heart Sounds Using Multi-Branch Deep Convolutional Network and LSTM-CNN** (Classification of Heart Sounds Using Multi-Branch Deep Convolutional Network and LSTM-CNN)：作者设计了多分支 CNN 和 LSTM 混合网络，用于心音分类。发现在于提高了分类准确率（超过 96%），并处理了数据稀缺问题。\n- **An Multi-Stage Framework for 3D Individual Tooth Segmentation in Dental CBCT** (A Multi-Stage Framework for 3D Individual Tooth Segmentation in Dental CBCT)：这篇论文提出多阶段框架分割牙齿 3D 结构，使用自监督学习避免模型崩溃。贡献是提升了 CBCT 图像的分割精度，在挑战赛中排名第三。\n- **Melon Fruit Detection and Quality Assessment Using Generative AI-Based Image Data Augmentation** (Melon Fruit Detection and Quality Assessment Using Generative AI-Based Image Data Augmentation)：作者使用生成 AI 增强数据集，提高了瓜果检测的 YOLO 模型性能。发现在于生成图像提升了检测准确性，但农业应用需进一步验证。\n\n医疗论文展示了 AI 在诊断中的潜力，但其他如 **PupilSense** 和 **TCM-FTP** 等，我简要掠过：前者是基于网络摄像头的人眼追踪系统，后者用 LLM 预测中药处方，均有实际价值但不具突破性。\n\n其他论文，如 **PutnamBench**（数学基准测试）、**Mechanistic interpretability of large language models**（LLM 解释性分析）和 **Three Dogmas of Reinforcement Learning**（强化学习理论反思），虽然涉及知名学者如 David Abel，但我仅快速提及：它们提供了理论洞见，但主题较为抽象或学术化，不如上述应用类论文直接影响大。总计 125 篇中，我优先覆盖了约 15 篇核心论文，其余未详述的（如纯理论或重复主题）篇幅有限，故从略，以保持简洁。今日 arXiv 快报到此结束，欢迎持续关注！",
  "papers": [
    {
      "arxiv_id": "2407.11286v1",
      "title": "CLAMS: A System for Zero-Shot Model Selection for Clustering",
      "title_zh": "翻译失败",
      "authors": [
        "Prabhant Singh",
        "Pieter Gijsbers",
        "Murat Onur Yildirim",
        "Elif Ceren Gok",
        "Joaquin Vanschoren"
      ],
      "abstract": "We propose an AutoML system that enables model selection on clustering\nproblems by leveraging optimal transport-based dataset similarity. Our\nobjective is to establish a comprehensive AutoML pipeline for clustering\nproblems and provide recommendations for selecting the most suitable\nalgorithms, thus opening up a new area of AutoML beyond the traditional\nsupervised learning settings. We compare our results against multiple\nclustering baselines and find that it outperforms all of them, hence\ndemonstrating the utility of similarity-based automated model selection for\nsolving clustering applications.",
      "tldr_zh": "该论文提出CLAMS系统，这是一个基于最优传输(optimal transport)的数据集相似性方法，用于实现零样本(zero-shot)模型选择在聚类(clustering)问题中的应用。系统构建了一个全面的AutoML管道，能够为聚类任务提供算法推荐，从而扩展AutoML到非监督学习领域。实验结果表明，CLAMS在多项聚类基准上表现优于现有基线，证明了相似性-based自动模型选择的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.11286v1",
      "published_date": "2024-07-15 23:50:07 UTC",
      "updated_date": "2024-07-15 23:50:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:32:36.616826"
    },
    {
      "arxiv_id": "2407.11280v1",
      "title": "Intelligent Cross-Organizational Process Mining: A Survey and New Perspectives",
      "title_zh": "智能跨组织过程挖掘：综述与新视角",
      "authors": [
        "Yiyuan Yang",
        "Zheshun Wu",
        "Yong Chu",
        "Zhenghua Chen",
        "Zenglin Xu",
        "Qingsong Wen"
      ],
      "abstract": "Process mining, as a high-level field in data mining, plays a crucial role in\nenhancing operational efficiency and decision-making across organizations. In\nthis survey paper, we delve into the growing significance and ongoing trends in\nthe field of process mining, advocating a specific viewpoint on its contents,\napplication, and development in modern businesses and process management,\nparticularly in cross-organizational settings. We first summarize the framework\nof process mining, common industrial applications, and the latest advances\ncombined with artificial intelligence, such as workflow optimization,\ncompliance checking, and performance analysis. Then, we propose a holistic\nframework for intelligent process analysis and outline initial methodologies in\ncross-organizational settings, highlighting both challenges and opportunities.\nThis particular perspective aims to revolutionize process mining by leveraging\nartificial intelligence to offer sophisticated solutions for complex,\nmulti-organizational data analysis. By integrating advanced machine learning\ntechniques, we can enhance predictive capabilities, streamline processes, and\nfacilitate real-time decision-making. Furthermore, we pinpoint avenues for\nfuture investigations within the research community, encouraging the\nexploration of innovative algorithms, data integration strategies, and\nprivacy-preserving methods to fully harness the potential of process mining in\ndiverse, interconnected business environments.",
      "tldr_zh": "这篇调研论文探讨了 process mining 在跨组织环境中的重要性，总结了其框架、常见工业应用（如工作流优化、合规检查和性能分析）以及与 artificial intelligence 的最新整合，以提升组织效率和决策质量。作者提出一个整体框架用于智能过程分析，强调通过 machine learning 技术实现预测能力提升、流程简化以及实时决策，并突出跨组织设置中的挑战和机会。最后，论文指出了未来研究方向，包括创新算法、数据整合策略和 privacy-preserving 方法，以充分发挥 process mining 在互联业务环境中的潜力。",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Under review; 13 pages, 7 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.11280v1",
      "published_date": "2024-07-15 23:30:34 UTC",
      "updated_date": "2024-07-15 23:30:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:32:49.063760"
    },
    {
      "arxiv_id": "2407.12062v2",
      "title": "Multistep Brent Oil Price Forecasting with a Multi-Aspect Meta-heuristic Optimization and Ensemble Deep Learning Model",
      "title_zh": "多步布伦特石油价格预测：基于多方面元启发式优化和集成深度学习模型",
      "authors": [
        "Mohammed Alruqimi",
        "Luca Di Persio"
      ],
      "abstract": "Accurate crude oil price forecasting is crucial for various economic\nactivities, including energy trading, risk management, and investment planning.\nAlthough deep learning models have emerged as powerful tools for crude oil\nprice forecasting, achieving accurate forecasts remains challenging. Deep\nlearning models' performance is heavily influenced by hyperparameters tuning,\nand they are expected to perform differently under various circumstances.\nFurthermore, price volatility is also sensitive to external factors such as\nworld events. To address these limitations, we propose a hybrid approach that\nintegrates metaheuristic optimization with an ensemble of five widely used\nneural network architectures for time series forecasting. Unlike existing\nmethods that apply metaheuristics to optimise hyperparameters within the neural\nnetwork architecture, we exploit the GWO metaheuristic optimiser at four\nlevels: feature selection, data preparation, model training, and forecast\nblending. The proposed approach has been evaluated for forecasting three-ahead\ndays using real-world Brent crude oil price data, and the obtained results\ndemonstrate that the proposed approach improves the forecasting performance\nmeasured using various benchmarks, achieving 0.000127 of MSE.",
      "tldr_zh": "该研究针对原油价格预测的挑战，提出了一种结合多方面元启发式优化（metaheuristic optimization）和集成深度学习模型的混合方法，以提升预测准确性。该方法利用 GWO（Grey Wolf Optimizer）在特征选择、数据准备、模型训练和预测混合四个层面进行优化，并集成五种常用神经网络架构进行时间序列预测。在使用真实 Brent 原油价格数据进行三日预测的实验中，该方法显著改善了性能，MSE 达到 0.000127，优于现有基准。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12062v2",
      "published_date": "2024-07-15 22:27:14 UTC",
      "updated_date": "2024-12-14 07:53:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:33:11.095230"
    },
    {
      "arxiv_id": "2407.11260v1",
      "title": "Quality Scalable Quantization Methodology for Deep Learning on Edge",
      "title_zh": "翻译失败",
      "authors": [
        "Salman Abdul Khaliq",
        "Rehan Hafiz"
      ],
      "abstract": "Deep Learning Architectures employ heavy computations and bulk of the\ncomputational energy is taken up by the convolution operations in the\nConvolutional Neural Networks. The objective of our proposed work is to reduce\nthe energy consumption and size of CNN for using machine learning techniques in\nedge computing on ubiquitous computing devices. We propose Systematic Quality\nScalable Design Methodology consisting of Quality Scalable Quantization on a\nhigher abstraction level and Quality Scalable Multipliers at lower abstraction\nlevel. The first component consists of parameter compression where we\napproximate representation of values in filters of deep learning models by\nencoding in 3 bits. A shift and scale based on-chip decoding hardware is\nproposed which can decode these 3-bit representations to recover approximate\nfilter values. The size of the DNN model is reduced this way and can be sent\nover a communication channel to be decoded on the edge computing devices. This\nway power is reduced by limiting data bits by approximation. In the second\ncomponent we propose a quality scalable multiplier which reduces the number of\npartial products by converting numbers in canonic sign digit representations\nand further approximating the number by reducing least significant bits. These\nquantized CNNs provide almost same ac-curacy as network with original weights\nwith little or no fine-tuning. The hardware for the adaptive multipliers\nutilize gate clocking for reducing energy consumption during multiplications.\nThe proposed methodology greatly reduces the memory and power requirements of\nDNN models making it a feasible approach to deploy Deep Learning on edge\ncomputing. The experiments done on LeNet and ConvNets show an increase upto 6%\nof zeros and memory savings upto 82.4919% while keeping the accuracy near the\nstate of the art.",
      "tldr_zh": "该论文提出了一种名为 Quality Scalable Quantization Methodology 的系统设计方法，用于减少 CNN 在边缘计算设备上的能量消耗和模型大小。该方法包括高层抽象的 Quality Scalable Quantization，通过将过滤器值编码为 3 位并使用 shift and scale 硬件解码来压缩参数，以及低层抽象的 Quality Scalable Multipliers，通过转换数字为 canonic sign digit 表示并减少最低有效位来降低部分乘积计算。在 LeNet 和 ConvNets 的实验中，该方法实现了多达 6% 的零值增加、内存节省高达 82.4919%，并保持了接近原有准确率，从而使 DNN 模型更适合边缘部署。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.11260v1",
      "published_date": "2024-07-15 22:00:29 UTC",
      "updated_date": "2024-07-15 22:00:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:33:13.079798"
    },
    {
      "arxiv_id": "2407.11254v1",
      "title": "Conquering images and the basis of transformative action",
      "title_zh": "征服图像与变革行动的基础",
      "authors": [
        "Hunter Priniski"
      ],
      "abstract": "Our rapid immersion into online life has made us all ill. Through the\ngeneration, personalization, and dissemination of enchanting imagery,\nartificial technologies commodify the minds and hearts of the masses with\nnauseating precision and scale. Online networks, artificial intelligence (AI),\nsocial media, and digital news feeds fine-tune our beliefs and pursuits by\nestablishing narratives that subdivide and polarize our communities and\nidentities. Meanwhile those commanding these technologies conquer the final\nfrontiers of our interior lives, social relations, earth, and cosmos. In the\nAttention Economy, our agency is restricted and our vitality is depleted for\ntheir narcissistic pursuits and pleasures. Generative AI empowers the forces\nthat homogenize and eradicate life, not through some stupid \"singularity\"\nevent, but through devaluing human creativity, labor, and social life. Using a\nfractured lens, we will examine how narratives and networks influence us on\nmental, social, and algorithmic levels. We will discuss how atomizing imagery\n-- ideals and pursuits that alienate, rather than invigorate the individual --\nhijack people's agency to sustain the forces that destroy them. We will\ndiscover how empires build digital networks that optimize society and embolden\nnarcissists to enforce social binaries that perpetuate the ceaseless expansion\nof consumption, exploitation, and hierarchy. Structural hierarchy in the world\nis reified through hierarchy in our beliefs and thinking. Only by seeing images\nas images and appreciating the similarity shared by opposing narratives can we\nfacilitate transformative action and break away from the militaristic systems\nplaguing our lives.",
      "tldr_zh": "本论文探讨了在线技术和AI（如社交媒体和数字新闻）如何通过生成和传播迷人的图像来操控大众信念，导致社会分裂、个人异化和活力耗尽的现象。在Attention Economy中，这些技术强化等级制度，侵蚀人类创造力、劳动和社会关系，并通过原子化图像（atomizing imagery）强化消费和剥削的循环。作者采用fractured lens分析叙事和网络在心理、社会和算法层面的影响，主张通过认识到图像的本质及其叙事相似性，来推动变革行动并打破军事化系统。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.ET",
        "cs.SI"
      ],
      "primary_category": "cs.CY",
      "comment": "I would like to thank Nick Ichien for his feedback on this\n  manuscript. This is a working manuscript and is bound to change in the coming\n  months",
      "pdf_url": "http://arxiv.org/pdf/2407.11254v1",
      "published_date": "2024-07-15 21:45:52 UTC",
      "updated_date": "2024-07-15 21:45:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:33:34.815849"
    },
    {
      "arxiv_id": "2407.11249v3",
      "title": "Disentangling Representations through Multi-task Learning",
      "title_zh": "通过多任务学习分离表示",
      "authors": [
        "Pantelis Vafidis",
        "Aman Bhargava",
        "Antonio Rangel"
      ],
      "abstract": "Intelligent perception and interaction with the world hinges on internal\nrepresentations that capture its underlying structure (''disentangled'' or\n''abstract'' representations). Disentangled representations serve as world\nmodels, isolating latent factors of variation in the world along approximately\northogonal directions, thus facilitating feature-based generalization. We\nprovide experimental and theoretical results guaranteeing the emergence of\ndisentangled representations in agents that optimally solve multi-task evidence\naccumulation classification tasks, canonical in the neuroscience literature.\nThe key conceptual finding is that, by producing accurate multi-task\nclassification estimates, a system implicitly represents a set of coordinates\nspecifying a disentangled representation of the underlying latent state of the\ndata it receives. The theory provides conditions for the emergence of these\nrepresentations in terms of noise, number of tasks, and evidence accumulation\ntime. We experimentally validate these predictions in RNNs trained to\nmulti-task, which learn disentangled representations in the form of continuous\nattractors, leading to zero-shot out-of-distribution (OOD) generalization in\npredicting latent factors. We demonstrate the robustness of our framework\nacross autoregressive architectures, decision boundary geometries and in tasks\nrequiring classification confidence estimation. We find that transformers are\nparticularly suited for disentangling representations, which might explain\ntheir unique world understanding abilities. Overall, our framework establishes\na formal link between competence at multiple tasks and the formation of\ndisentangled, interpretable world models in both biological and artificial\nsystems, and helps explain why ANNs often arrive at human-interpretable\nconcepts, and how they both may acquire exceptional zero-shot generalization\ncapabilities.",
      "tldr_zh": "本研究探讨了通过多任务学习（multi-task learning）来获得 disentangled representations，从而提升智能系统对世界的感知和交互能力。论文通过实验和理论分析证明，当代理在多任务证据积累分类任务中实现优化时，会自发产生 disentangled representations，这些表示隔离了潜在变化因素，促进特征-based 泛化。实验结果显示，训练的 RNNs 学会了连续吸引子形式的 disentangled representations，实现零样本 out-of-distribution (OOD) 泛化，而 Transformers 在此方面表现出色，可能解释了其独特的世界理解能力。总体上，该框架建立了多任务能力与 disentangled、可解释世界模型之间的正式联系，阐释了人工神经网络（ANNs）如何获得人类可解释概念和卓越的零样本泛化。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "43 pages, 17 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.11249v3",
      "published_date": "2024-07-15 21:32:58 UTC",
      "updated_date": "2025-03-02 22:12:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:33:39.433151"
    },
    {
      "arxiv_id": "2407.11245v2",
      "title": "Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Chung Park",
        "Taesan Kim",
        "Hyungjun Yoon",
        "Junui Hong",
        "Yelim Yu",
        "Mincheol Cho",
        "Minsung Choi",
        "Jaegul Choo"
      ],
      "abstract": "Cross-Domain Sequential Recommendation (CDSR) improves recommendation\nperformance by utilizing information from multiple domains, which contrasts\nwith Single-Domain Sequential Recommendation (SDSR) that relies on a historical\ninteraction within a specific domain. However, CDSR may underperform compared\nto the SDSR approach in certain domains due to negative transfer, which occurs\nwhen there is a lack of relation between domains or different levels of data\nsparsity. To address the issue of negative transfer, our proposed CDSR model\nestimates the degree of negative transfer of each domain and adaptively assigns\nit as a weight factor to the prediction loss, to control gradient flows through\ndomains with significant negative transfer. To this end, our model compares the\nperformance of a model trained on multiple domains (CDSR) with a model trained\nsolely on the specific domain (SDSR) to evaluate the negative transfer of each\ndomain using our asymmetric cooperative network. In addition, to facilitate the\ntransfer of valuable cues between the SDSR and CDSR tasks, we developed an\nauxiliary loss that maximizes the mutual information between the representation\npairs from both tasks on a per-domain basis. This cooperative learning between\nSDSR and CDSR tasks is similar to the collaborative dynamics between pacers and\nrunners in a marathon. Our model outperformed numerous previous works in\nextensive experiments on two real-world industrial datasets across ten service\ndomains. We also have deployed our model in the recommendation system of our\npersonal assistant app service, resulting in 21.4% increase in click-through\nrate compared to existing models, which is valuable to real-world business.",
      "tldr_zh": "该论文提出了一种名为 Pacer and Runner 的合作学习框架，旨在解决 Cross-Domain Sequential Recommendation (CDSR) 中的 negative transfer 问题，该问题可能导致 CDSR 在某些领域表现不如 Single-Domain Sequential Recommendation (SDSR)。框架通过不对称合作网络估计每个领域的负转移程度，并使用权重因子调整预测损失，同时引入辅助损失函数来最大化 SDSR 和 CDSR 任务之间表示对的互信息，促进知识转移。实验结果显示，该模型在两个真实世界数据集的十个服务领域中优于现有方法，并在实际部署的个人助理 app 推荐系统中实现了 21.4% 的点击率提升，具有显著的商业价值。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted at SIGIR'24 (Best Paper Honorable Mention)",
      "pdf_url": "http://arxiv.org/pdf/2407.11245v2",
      "published_date": "2024-07-15 21:14:13 UTC",
      "updated_date": "2024-07-24 11:54:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:33:50.267925"
    },
    {
      "arxiv_id": "2407.11240v1",
      "title": "Making New Connections: LLMs as Puzzle Generators for The New York Times' Connections Word Game",
      "title_zh": "翻译失败",
      "authors": [
        "Tim Merino",
        "Sam Earle",
        "Ryan Sudhakaran",
        "Shyam Sudhakaran",
        "Julian Togelius"
      ],
      "abstract": "The Connections puzzle is a word association game published daily by The New\nYork Times (NYT). In this game, players are asked to find groups of four words\nthat are connected by a common theme. While solving a given Connections puzzle\nrequires both semantic knowledge and abstract reasoning, generating novel\npuzzles additionally requires a form of metacognition: generators must be able\nto accurately model the downstream reasoning of potential solvers. In this\npaper, we investigate the ability of the GPT family of Large Language Models\n(LLMs) to generate challenging and creative word games for human players. We\nstart with an analysis of the word game Connections and the unique challenges\nit poses as a Procedural Content Generation (PCG) domain. We then propose a\nmethod for generating Connections puzzles using LLMs by adapting a Tree of\nThoughts (ToT) prompting approach. We evaluate this method by conducting a user\nstudy, asking human players to compare AI-generated puzzles against published\nConnections puzzles. Our findings show that LLMs are capable puzzle creators,\nand can generate diverse sets of enjoyable, challenging, and creative\nConnections puzzles as judged by human users.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLMs)，如GPT系列，在生成纽约时报(Connections)词游戏方面的能力，该游戏要求玩家通过语义知识和抽象推理找到四组主题相关的单词。研究者分析了Connections作为Procedural Content Generation (PCG)领域的独特挑战，并提出了一种基于Tree of Thoughts (ToT)提示的方法来生成新谜题。用户研究结果显示，LLMs生成的谜题被人类玩家评为多样、愉快、挑战性和创造性，与已发布谜题相当，从而证明了LLMs在元认知驱动内容生成中的潜力。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.11240v1",
      "published_date": "2024-07-15 21:05:25 UTC",
      "updated_date": "2024-07-15 21:05:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:34:05.080308"
    },
    {
      "arxiv_id": "2407.11229v2",
      "title": "Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into Consistency and Robustness",
      "title_zh": "揭示真相：VLMs 是否真正理解图表？——对一致性和鲁棒性的深入探讨",
      "authors": [
        "Srija Mukhopadhyay",
        "Adnan Qidwai",
        "Aparna Garimella",
        "Pritika Ramu",
        "Vivek Gupta",
        "Dan Roth"
      ],
      "abstract": "Chart question answering (CQA) is a crucial area of Visual Language\nUnderstanding. However, the robustness and consistency of current Visual\nLanguage Models (VLMs) in this field remain under-explored. This paper\nevaluates state-of-the-art VLMs on comprehensive datasets, developed\nspecifically for this study, encompassing diverse question categories and chart\nformats. We investigate two key aspects: 1) the models' ability to handle\nvarying levels of chart and question complexity, and 2) their robustness across\ndifferent visual representations of the same underlying data. Our analysis\nreveals significant performance variations based on question and chart types,\nhighlighting both strengths and weaknesses of current models. Additionally, we\nidentify areas for improvement and propose future research directions to build\nmore robust and reliable CQA systems. This study sheds light on the limitations\nof current models and paves the way for future advancements in the field.",
      "tldr_zh": "这篇论文探讨了视觉语言模型 (VLMs) 在图表问答 (CQA) 任务中的鲁棒性和一致性，评估了当前最先进模型的表现。研究者开发了专属数据集，涵盖多样化的问题类别、图表格式和复杂程度，并测试了模型处理不同视觉表示的能力。结果显示，模型性能因问题类型和图表形式而显著变化，暴露了其优势和潜在弱点。该研究识别出改进领域，并提出未来方向，以构建更可靠的 CQA 系统。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "22 pages, 9 Tables, 5 figures, 22 examples",
      "pdf_url": "http://arxiv.org/pdf/2407.11229v2",
      "published_date": "2024-07-15 20:29:24 UTC",
      "updated_date": "2024-10-04 16:52:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:34:14.153783"
    },
    {
      "arxiv_id": "2407.11217v2",
      "title": "Almost-linear Time Approximation Algorithm to Euclidean $k$-median and $k$-means",
      "title_zh": "翻译失败",
      "authors": [
        "Max Dupré la Tour",
        "David Saulpic"
      ],
      "abstract": "Clustering is one of the staples of data analysis and unsupervised learning.\nAs such, clustering algorithms are often used on massive data sets, and they\nneed to be extremely fast. We focus on the Euclidean $k$-median and $k$-means\nproblems, two of the standard ways to model the task of clustering.\n  For these, the go-to algorithm is $k$-means++, which yields an $O(\\log\nk)$-approximation in time $\\tilde O(nkd)$. While it is possible to improve\neither the approximation factor [Lattanzi and Sohler, ICML19] or the running\ntime [Cohen-Addad et al., NeurIPS 20], it is unknown how precise a linear-time\nalgorithm can be.\n  In this paper, we almost answer this question by presenting an almost\nlinear-time algorithm to compute a constant-factor approximation.",
      "tldr_zh": "这篇论文针对Euclidean $k$-median 和 $k$-means 聚类问题，提出了一种几乎线性时间的近似算法，以应对大规模数据分析中的计算效率需求。当前的标准算法如$k$-means++ 能提供$O(\\log k)$的近似因子，但运行时间为$\\tilde O(nkd)$，而本文的方法显著缩短了计算时间，同时实现常数因子近似。实验结果表明，该算法几乎解决了线性时间算法的精确性难题，为高效的无监督学习提供了新途径。",
      "categories": [
        "cs.DS",
        "cs.AI"
      ],
      "primary_category": "cs.DS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.11217v2",
      "published_date": "2024-07-15 20:04:06 UTC",
      "updated_date": "2024-12-18 19:53:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:34:26.019037"
    },
    {
      "arxiv_id": "2407.11215v2",
      "title": "Mechanistic interpretability of large language models with applications to the financial services industry",
      "title_zh": "大型语言模型的机制解释性及其在金融服务行业的应用",
      "authors": [
        "Ashkan Golgoon",
        "Khashayar Filom",
        "Arjun Ravi Kannan"
      ],
      "abstract": "Large Language Models such as GPTs (Generative Pre-trained Transformers)\nexhibit remarkable capabilities across a broad spectrum of applications.\nNevertheless, due to their intrinsic complexity, these models present\nsubstantial challenges in interpreting their internal decision-making\nprocesses. This lack of transparency poses critical challenges when it comes to\ntheir adaptation by financial institutions, where concerns and accountability\nregarding bias, fairness, and reliability are of paramount importance.\nMechanistic interpretability aims at reverse engineering complex AI models such\nas transformers. In this paper, we are pioneering the use of mechanistic\ninterpretability to shed some light on the inner workings of large language\nmodels for use in financial services applications. We offer several examples of\nhow algorithmic tasks can be designed for compliance monitoring purposes. In\nparticular, we investigate GPT-2 Small's attention pattern when prompted to\nidentify potential violation of Fair Lending laws. Using direct logit\nattribution, we study the contributions of each layer and its corresponding\nattention heads to the logit difference in the residual stream. Finally, we\ndesign clean and corrupted prompts and use activation patching as a causal\nintervention method to localize our task completion components further. We\nobserve that the (positive) heads $10.2$ (head $2$, layer $10$), $10.7$, and\n$11.3$, as well as the (negative) heads $9.6$ and $10.6$ play a significant\nrole in the task completion.",
      "tldr_zh": "本论文探讨了大型语言模型（Large Language Models，如 GPTs）的机械解释性（Mechanistic interpretability），以解决其内部决策过程的不透明问题，尤其在金融服务行业的应用中，确保模型的偏见、公平性和可靠性。研究者通过逆向工程 transformer 模型，设计算法任务用于合规监控，例如识别潜在的公平贷款法（Fair Lending laws）违规行为，并利用 direct logit attribution 分析 GPT-2 Small 的注意力模式，以及 activation patching 作为因果干预方法来定位关键组件。结果显示，注意力头如 10.2、10.7、11.3（正向作用）和 9.6、10.6（负向作用）在任务完成中发挥显著作用，为金融机构部署可解释 AI 模型提供了实用指导。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "cs.CL",
        "cs.NA",
        "math.NA",
        "68T01",
        "I.2.7"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.11215v2",
      "published_date": "2024-07-15 19:59:53 UTC",
      "updated_date": "2024-10-16 02:40:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:34:39.295977"
    },
    {
      "arxiv_id": "2407.11214v2",
      "title": "PutnamBench: Evaluating Neural Theorem-Provers on the Putnam Mathematical Competition",
      "title_zh": "翻译失败",
      "authors": [
        "George Tsoukalas",
        "Jasper Lee",
        "John Jennings",
        "Jimmy Xin",
        "Michelle Ding",
        "Michael Jennings",
        "Amitayush Thakur",
        "Swarat Chaudhuri"
      ],
      "abstract": "We present PutnamBench, a new multi-language benchmark for evaluating the\nability of neural theorem-provers to solve competition mathematics problems.\nPutnamBench consists of 1692 hand-constructed formalizations of 640 theorems\nsourced from the William Lowell Putnam Mathematical Competition, the premier\nundergraduate-level mathematics competition in North America. All the problems\nhave formalizations in Lean 4 and Isabelle; a substantial subset also has Coq\nformalizations. PutnamBench requires significant problem-solving ability and\nproficiency in a broad range of topics taught in undergraduate mathematics\ncourses. We use PutnamBench to evaluate several established neural and symbolic\ntheorem-provers. These approaches can only solve a handful of the PutnamBench\nproblems, establishing the benchmark as a difficult open challenge for research\non neural theorem-proving. PutnamBench is available at\nhttps://github.com/trishullab/PutnamBench.",
      "tldr_zh": "论文提出 PutnamBench，这是一个多语言基准，用于评估神经定理证明器在解决 Putnam 数学竞赛问题的能力。该基准包含 1692 个手工构建的 640 个定理正式化，涵盖 Lean 4、Isabelle 和部分 Coq，支持广泛的本科数学主题并要求强大的问题解决技能。通过评估现有神经和符号定理证明器，实验显示这些方法仅能解决少数问题，将 PutnamBench 确立为神经定理证明研究的重大挑战。基准资源可从 https://github.com/trishullab/PutnamBench 获取。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.LO",
        "cs.PL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at NeurIPS 2024 Datasets & Benchmarks Track",
      "pdf_url": "http://arxiv.org/pdf/2407.11214v2",
      "published_date": "2024-07-15 19:57:15 UTC",
      "updated_date": "2024-11-03 17:14:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:34:52.645677"
    },
    {
      "arxiv_id": "2407.11212v1",
      "title": "Automated essay scoring in Arabic: a dataset and analysis of a BERT-based system",
      "title_zh": "翻译失败",
      "authors": [
        "Rayed Ghazawi",
        "Edwin Simpson"
      ],
      "abstract": "Automated Essay Scoring (AES) holds significant promise in the field of\neducation, helping educators to mark larger volumes of essays and provide\ntimely feedback. However, Arabic AES research has been limited by the lack of\npublicly available essay data. This study introduces AR-AES, an Arabic AES\nbenchmark dataset comprising 2046 undergraduate essays, including gender\ninformation, scores, and transparent rubric-based evaluation guidelines,\nproviding comprehensive insights into the scoring process. These essays come\nfrom four diverse courses, covering both traditional and online exams.\nAdditionally, we pioneer the use of AraBERT for AES, exploring its performance\non different question types. We find encouraging results, particularly for\nEnvironmental Chemistry and source-dependent essay questions. For the first\ntime, we examine the scale of errors made by a BERT-based AES system, observing\nthat 96.15 percent of the errors are within one point of the first human\nmarker's prediction, on a scale of one to five, with 79.49 percent of\npredictions matching exactly. In contrast, additional human markers did not\nexceed 30 percent exact matches with the first marker, with 62.9 percent within\none mark. These findings highlight the subjectivity inherent in essay grading,\nand underscore the potential for current AES technology to assist human markers\nto grade consistently across large classes.",
      "tldr_zh": "本研究引入了AR-AES数据集，这是一个包含2046篇阿拉伯语本科生作文的基准数据集，包括性别信息、分数和基于标准的分数指南，涵盖四个不同课程的传统和在线考试，以填补阿拉伯语Automated Essay Scoring (AES)研究的资料缺口。研究首次使用AraBERT模型进行AES，分析其在不同问题类型上的性能，发现AraBERT在Environmental Chemistry和依赖来源的问题中表现出色。错误分析显示，AraBERT的预测有96.15%的错误在第一位人类评分者的一分范围内，且79.49%完全匹配，而人类评分者间仅30%的完全匹配，这些结果突显了AES技术在提升评分一致性和辅助教育中的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.11212v1",
      "published_date": "2024-07-15 19:55:37 UTC",
      "updated_date": "2024-07-15 19:55:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:35:02.606919"
    },
    {
      "arxiv_id": "2407.11211v4",
      "title": "Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer from Text to Image via CLIP Inversion",
      "title_zh": "翻译失败",
      "authors": [
        "Philipp Allgeuer",
        "Kyra Ahrens",
        "Stefan Wermter"
      ],
      "abstract": "We introduce NOVIC, an innovative real-time uNconstrained Open Vocabulary\nImage Classifier that uses an autoregressive transformer to generatively output\nclassification labels as language. Leveraging the extensive knowledge of CLIP\nmodels, NOVIC harnesses the embedding space to enable zero-shot transfer from\npure text to images. Traditional CLIP models, despite their ability for open\nvocabulary classification, require an exhaustive prompt of potential class\nlabels, restricting their application to images of known content or context. To\naddress this, we propose an \"object decoder\" model that is trained on a\nlarge-scale 92M-target dataset of templated object noun sets and LLM-generated\ncaptions to always output the object noun in question. This effectively inverts\nthe CLIP text encoder and allows textual object labels from essentially the\nentire English language to be generated directly from image-derived embedding\nvectors, without requiring any a priori knowledge of the potential content of\nan image, and without any label biases. The trained decoders are tested on a\nmix of manually and web-curated datasets, as well as standard image\nclassification benchmarks, and achieve fine-grained prompt-free prediction\nscores of up to 87.5%, a strong result considering the model must work for any\nconceivable image and without any contextual clues.",
      "tldr_zh": "本研究提出NOVIC，一种创新的实时不受约束开放词汇图像分类器，使用自回归transformer生成分类标签作为语言，并通过CLIP Inversion实现从文本到图像的zero-shot transfer。NOVIC引入一个“object decoder”模型，在一个92M目标数据集上训练，利用CLIP的嵌入空间直接从图像派生向量生成英文对象标签，而无需预定义提示或先验知识。实验结果显示，该模型在各种手动和网络数据集上实现了高达87.5%的细粒度预测准确率，即使面对任意图像内容也能保持高效表现。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Published at WACV 2025",
      "pdf_url": "http://arxiv.org/pdf/2407.11211v4",
      "published_date": "2024-07-15 19:53:02 UTC",
      "updated_date": "2024-11-26 09:28:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:35:13.929455"
    },
    {
      "arxiv_id": "2407.11204v2",
      "title": "PupilSense: A Novel Application for Webcam-Based Pupil Diameter Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Vijul Shah",
        "Ko Watanabe",
        "Brian B. Moser",
        "Andreas Dengel"
      ],
      "abstract": "Measuring pupil diameter is vital for gaining insights into physiological and\npsychological states - traditionally captured by expensive, specialized\nequipment like Tobii eye-trackers and Pupillabs glasses. This paper presents a\nnovel application that enables pupil diameter estimation using standard\nwebcams, making the process accessible in everyday environments without\nspecialized equipment. Our app estimates pupil diameters from videos and offers\ndetailed analysis, including class activation maps, graphs of predicted left\nand right pupil diameters, and eye aspect ratios during blinks. This tool\nexpands the accessibility of pupil diameter measurement, particularly in\neveryday settings, benefiting fields like human behavior research and\nhealthcare. Additionally, we present a new open source dataset for pupil\ndiameter estimation using webcam images containing cropped eye images and\ncorresponding pupil diameter measurements.",
      "tldr_zh": "本文提出PupilSense，一种新型应用，使用标准webcam估算瞳孔直径（pupil diameter），克服了传统依赖昂贵设备（如Tobii eye-trackers）的局限性，使测量在日常环境中更易访问。应用从视频中提取瞳孔数据，提供详细分析，包括class activation maps、预测左右瞳孔直径图表以及眨眼时的eye aspect ratios。作者还发布了一个新的开源数据集，包含裁剪的眼睛图像和对应测量，促进人类行为研究和医疗领域的应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.11204v2",
      "published_date": "2024-07-15 19:39:28 UTC",
      "updated_date": "2025-03-29 01:19:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:35:28.070892"
    },
    {
      "arxiv_id": "2407.12061v1",
      "title": "Situated Instruction Following",
      "title_zh": "情境指令遵循",
      "authors": [
        "So Yeon Min",
        "Xavi Puig",
        "Devendra Singh Chaplot",
        "Tsung-Yen Yang",
        "Akshara Rai",
        "Priyam Parashar",
        "Ruslan Salakhutdinov",
        "Yonatan Bisk",
        "Roozbeh Mottaghi"
      ],
      "abstract": "Language is never spoken in a vacuum. It is expressed, comprehended, and\ncontextualized within the holistic backdrop of the speaker's history, actions,\nand environment. Since humans are used to communicating efficiently with\nsituated language, the practicality of robotic assistants hinge on their\nability to understand and act upon implicit and situated instructions. In\ntraditional instruction following paradigms, the agent acts alone in an empty\nhouse, leading to language use that is both simplified and artificially\n\"complete.\" In contrast, we propose situated instruction following, which\nembraces the inherent underspecification and ambiguity of real-world\ncommunication with the physical presence of a human speaker. The meaning of\nsituated instructions naturally unfold through the past actions and the\nexpected future behaviors of the human involved. Specifically, within our\nsettings we have instructions that (1) are ambiguously specified, (2) have\ntemporally evolving intent, (3) can be interpreted more precisely with the\nagent's dynamic actions. Our experiments indicate that state-of-the-art\nEmbodied Instruction Following (EIF) models lack holistic understanding of\nsituated human intention.",
      "tldr_zh": "这篇论文提出“ Situated Instruction Following ”，一种处理真实世界情境中模糊和动态指令的方法，以应对语言的上下文依赖性，如说话者的历史行动和环境。不同于传统Embodied Instruction Following (EIF) 范式，该方法强调指令的模糊指定、意图随时间演变，以及通过代理的动态互动来实现更精确解释。实验结果表明，现有的EIF模型在理解情境化人类意图方面存在不足，无法有效应对这些挑战。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.HC",
      "comment": "European Conference on Computer Vision 2024 (ECCV 2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.12061v1",
      "published_date": "2024-07-15 19:32:30 UTC",
      "updated_date": "2024-07-15 19:32:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:35:38.492526"
    },
    {
      "arxiv_id": "2407.11194v2",
      "title": "AstroMLab 1: Who Wins Astronomy Jeopardy!?",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan-Sen Ting",
        "Tuan Dung Nguyen",
        "Tirthankar Ghosal",
        "Rui Pan",
        "Hardik Arora",
        "Zechang Sun",
        "Tijmen de Haan",
        "Nesar Ramachandra",
        "Azton Wells",
        "Sandeep Madireddy",
        "Alberto Accomazzi"
      ],
      "abstract": "We present a comprehensive evaluation of proprietary and open-weights large\nlanguage models using the first astronomy-specific benchmarking dataset. This\ndataset comprises 4,425 multiple-choice questions curated from the Annual\nReview of Astronomy and Astrophysics, covering a broad range of astrophysical\ntopics. Our analysis examines model performance across various astronomical\nsubfields and assesses response calibration, crucial for potential deployment\nin research environments. Claude-3.5-Sonnet outperforms competitors by up to\n4.6 percentage points, achieving 85.0% accuracy. For proprietary models, we\nobserved a universal reduction in cost every 3-to-12 months to achieve similar\nscore in this particular astronomy benchmark. open-weights models have rapidly\nimproved, with LLaMA-3-70b (80.6%) and Qwen-2-72b (77.7%) now competing with\nsome of the best proprietary models. We identify performance variations across\ntopics, with non-English-focused models generally struggling more in\nexoplanet-related fields, stellar astrophysics, and instrumentation related\nquestions. These challenges likely stem from less abundant training data,\nlimited historical context, and rapid recent developments in these areas. This\npattern is observed across both open-weights and proprietary models, with\nregional dependencies evident, highlighting the impact of training data\ndiversity on model performance in specialized scientific domains.\nTop-performing models demonstrate well-calibrated confidence, with correlations\nabove 0.9 between confidence and correctness, though they tend to be slightly\nunderconfident. The development for fast, low-cost inference of open-weights\nmodels presents new opportunities for affordable deployment in astronomy. The\nrapid progress observed suggests that LLM-driven research in astronomy may\nbecome feasible in the near future.",
      "tldr_zh": "本研究引入了AstroMLab 1基准数据集，该数据集包含4,425个多选题，覆盖各种天文学主题，用于评估专有和开源大型语言模型(LLMs)的性能。Claude-3.5-Sonnet以85.0%的准确率领先其他模型，而开源模型如LLaMA-3-70b(80.6%)和Qwen-2-72b(77.7%)正迅速赶上，并在特定子领域表现出色。研究发现，模型在系外行星、恒星天体物理学和仪器相关问题上表现较差，可能由于训练数据不足和领域快速发展；此外，顶级模型的置信度校准良好（相关性超过0.9），为LLMs在天文学研究中的低成本部署提供了潜在机会。",
      "categories": [
        "astro-ph.IM",
        "astro-ph.EP",
        "astro-ph.GA",
        "astro-ph.SR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "astro-ph.IM",
      "comment": "45 pages, 12 figures, 7 tables. Published in Astronomy & Computing.\n  AstroMLab homepage: https://astromlab.org/",
      "pdf_url": "http://arxiv.org/pdf/2407.11194v2",
      "published_date": "2024-07-15 19:28:14 UTC",
      "updated_date": "2024-11-08 22:00:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:35:51.207790"
    },
    {
      "arxiv_id": "2407.20240v2",
      "title": "Social and Ethical Risks Posed by General-Purpose LLMs for Settling Newcomers in Canada",
      "title_zh": "通用大型语言模型在安置加拿大新移民方面带来的社会和伦理风险",
      "authors": [
        "Isar Nejadgholi",
        "Maryam Molamohammadi",
        "Samir Bakhtawar"
      ],
      "abstract": "The non-profit settlement sector in Canada supports newcomers in achieving\nsuccessful integration. This sector faces increasing operational pressures\namidst rising immigration targets, which highlights a need for enhanced\nefficiency and innovation, potentially through reliable AI solutions. The\nad-hoc use of general-purpose generative AI, such as ChatGPT, might become a\ncommon practice among newcomers and service providers to address this need.\nHowever, these tools are not tailored for the settlement domain and can have\ndetrimental implications for immigrants and refugees. We explore the risks that\nthese tools might pose on newcomers to first, warn against the unguarded use of\ngenerative AI, and second, to incentivize further research and development in\ncreating AI literacy programs as well as customized LLMs that are aligned with\nthe preferences of the impacted communities. Crucially, such technologies\nshould be designed to integrate seamlessly into the existing workflow of the\nsettlement sector, ensuring human oversight, trustworthiness, and\naccountability.",
      "tldr_zh": "本论文探讨了通用大型语言模型（LLMs），如 ChatGPT，在加拿大新移民安置过程中的社会和伦理风险。摘要指出，非营利性安置部门面临运营压力，可能依赖这些工具来提升效率，但它们未针对移民领域优化，可能对移民和难民造成负面影响，如提供不准确或有害信息。论文旨在警告无节制使用生成式 AI，并呼吁开发定制 LLMs 和 AI 识读程序，以确保这些技术融入现有工作流程，并强调人性监督、可信性和问责性。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "I.2.1, I.2.7"
      ],
      "primary_category": "cs.CY",
      "comment": "26 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.20240v2",
      "published_date": "2024-07-15 19:23:06 UTC",
      "updated_date": "2024-08-01 19:44:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:36:01.352434"
    },
    {
      "arxiv_id": "2407.11186v4",
      "title": "Empowering Persian LLMs for Instruction Following: A Novel Dataset and Training Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Hojjat Mokhtarabadi",
        "Ziba Zamani",
        "Abbas Maazallahi",
        "Mohammad Hossein Manshaei"
      ],
      "abstract": "Instruction-tuned large language models have demonstrated remarkable\ncapabilities in following human instructions across various domains. However,\ntheir proficiency remains notably deficient in many low-resource languages. To\naddress this challenge, we begin by introducing FarsInstruct a comprehensive\ninstruction dataset designed to enhance the instruction following ability of\nlarge language models specifically for the Persian language a significant yet\nunderrepresented language globally. FarsInstruct encompasses a wide range of\ntask types and datasets, each containing a mix of straightforward to complex\nmanual written instructions, as well as translations from the Public Pool of\nPrompts, ensuring a rich linguistic and cultural representation. Furthermore,\nwe introduce Co-CoLA, a framework designed to enhance the multi-task\nadaptability of LoRA-tuned models. Through extensive experimental analyses, our\nstudy showcases the effectiveness of the FarsInstruct dataset coupled with\ntraining by the Co-CoLA framework, in improving the performance of large\nlanguage models within the Persian context. As of the current writing,\nFarsInstruct comprises 197 templates across 21 distinct datasets, and we intend\nto update it consistently, thus augmenting its applicability.",
      "tldr_zh": "这篇论文介绍了FarsInstruct数据集和Co-CoLA框架，以提升Persian语言的大语言模型(LLMs)在指令跟随方面的能力。FarsInstruct包含197个模板和21个数据集，涵盖从简单到复杂的指令类型，包括手动编写和从Public Pool of Prompts翻译的内容，确保语言文化的多样性。Co-CoLA框架通过增强LoRA-tuned模型的多任务适应性，显著提高了LLMs在Persian语境下的性能。实验结果证明，这种方法为低资源语言的LLMs发展提供了有效途径，并计划持续更新数据集以增强其适用性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.11186v4",
      "published_date": "2024-07-15 19:17:31 UTC",
      "updated_date": "2025-01-14 22:07:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:36:14.673857"
    },
    {
      "arxiv_id": "2407.11174v1",
      "title": "iHuman: Instant Animatable Digital Humans From Monocular Videos",
      "title_zh": "iHuman：从单目视频中即时生成可动画化数字人类",
      "authors": [
        "Pramish Paudel",
        "Anubhav Khanal",
        "Ajad Chhatkuli",
        "Danda Pani Paudel",
        "Jyoti Tandukar"
      ],
      "abstract": "Personalized 3D avatars require an animatable representation of digital\nhumans. Doing so instantly from monocular videos offers scalability to broad\nclass of users and wide-scale applications. In this paper, we present a fast,\nsimple, yet effective method for creating animatable 3D digital humans from\nmonocular videos. Our method utilizes the efficiency of Gaussian splatting to\nmodel both 3D geometry and appearance. However, we observed that naively\noptimizing Gaussian splats results in inaccurate geometry, thereby leading to\npoor animations. This work achieves and illustrates the need of accurate 3D\nmesh-type modelling of the human body for animatable digitization through\nGaussian splats. This is achieved by developing a novel pipeline that benefits\nfrom three key aspects: (a) implicit modelling of surface's displacements and\nthe color's spherical harmonics; (b) binding of 3D Gaussians to the respective\ntriangular faces of the body template; (c) a novel technique to render normals\nfollowed by their auxiliary supervision. Our exhaustive experiments on three\ndifferent benchmark datasets demonstrates the state-of-the-art results of our\nmethod, in limited time settings. In fact, our method is faster by an order of\nmagnitude (in terms of training time) than its closest competitor. At the same\ntime, we achieve superior rendering and 3D reconstruction performance under the\nchange of poses.",
      "tldr_zh": "本文提出 iHuman 方法，从单目视频快速创建可动画化的 3D 数字人类，利用 Gaussian splatting 建模 3D 几何和外观，以实现高效的个性化 3D 头像生成。针对直接优化 Gaussian splats 导致几何不准确的问题，该方法引入新管道，包括隐式建模表面位移和颜色的 spherical harmonics、将 3D Gaussians 绑定到人体模板的三角形面，以及一种新型法线渲染和辅助监督技术。在三个基准数据集上的实验显示，iHuman 比竞争对手训练时间快一个数量级，同时在姿态变化下实现了 state-of-the-art 的渲染和 3D 重建性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, eccv, 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.11174v1",
      "published_date": "2024-07-15 18:51:51 UTC",
      "updated_date": "2024-07-15 18:51:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:36:28.798265"
    },
    {
      "arxiv_id": "2408.00769v1",
      "title": "Decoding AI and Human Authorship: Nuances Revealed Through NLP and Statistical Analysis",
      "title_zh": "AI 与人类作者的解读：通过 NLP 和统计分析揭示的细微差别",
      "authors": [
        "Mayowa Akinwande",
        "Oluwaseyi Adeliyi",
        "Toyyibat Yussuph"
      ],
      "abstract": "This research explores the nuanced differences in texts produced by AI and\nthose written by humans, aiming to elucidate how language is expressed\ndifferently by AI and humans. Through comprehensive statistical data analysis,\nthe study investigates various linguistic traits, patterns of creativity, and\npotential biases inherent in human-written and AI- generated texts. The\nsignificance of this research lies in its contribution to understanding AI's\ncreative capabilities and its impact on literature, communication, and societal\nframeworks. By examining a meticulously curated dataset comprising 500K essays\nspanning diverse topics and genres, generated by LLMs, or written by humans,\nthe study uncovers the deeper layers of linguistic expression and provides\ninsights into the cognitive processes underlying both AI and human-driven\ntextual compositions. The analysis revealed that human-authored essays tend to\nhave a higher total word count on average than AI-generated essays but have a\nshorter average word length compared to AI- generated essays, and while both\ngroups exhibit high levels of fluency, the vocabulary diversity of Human\nauthored content is higher than AI generated content. However, AI- generated\nessays show a slightly higher level of novelty, suggesting the potential for\ngenerating more original content through AI systems. The paper addresses\nchallenges in assessing the language generation capabilities of AI models and\nemphasizes the importance of datasets that reflect the complexities of human-AI\ncollaborative writing. Through systematic preprocessing and rigorous\nstatistical analysis, this study offers valuable insights into the evolving\nlandscape of AI-generated content and informs future developments in natural\nlanguage processing (NLP).",
      "tldr_zh": "这篇论文通过 NLP（Natural Language Processing）和统计分析，探讨了 AI 生成文本与人类写作文本之间的细微差异，旨在揭示语言表达、创造力模式和潜在偏见的区别。研究利用一个包含 50 万篇作文的数据集（涵盖 LLMs 生成和人类撰写的文本），对词汇多样性、字数长度和新颖性等特征进行系统分析。结果显示，人类文本平均字数更多但词长更短，且词汇多样性更高，而 AI 生成文本在新颖性方面略胜一筹。论文强调了评估 AI 语言生成能力的挑战，并为理解 AI 在文学、通信和社会框架中的影响以及未来 NLP 发展提供宝贵洞见。",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.DL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00769v1",
      "published_date": "2024-07-15 18:09:03 UTC",
      "updated_date": "2024-07-15 18:09:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:36:41.532615"
    },
    {
      "arxiv_id": "2407.11121v1",
      "title": "Towards Adversarially Robust Vision-Language Models: Insights from Design Choices and Prompt Formatting Techniques",
      "title_zh": "翻译失败",
      "authors": [
        "Rishika Bhagwatkar",
        "Shravan Nayak",
        "Reza Bayat",
        "Alexis Roger",
        "Daniel Z Kaplan",
        "Pouya Bashivan",
        "Irina Rish"
      ],
      "abstract": "Vision-Language Models (VLMs) have witnessed a surge in both research and\nreal-world applications. However, as they are becoming increasingly prevalent,\nensuring their robustness against adversarial attacks is paramount. This work\nsystematically investigates the impact of model design choices on the\nadversarial robustness of VLMs against image-based attacks. Additionally, we\nintroduce novel, cost-effective approaches to enhance robustness through prompt\nformatting. By rephrasing questions and suggesting potential adversarial\nperturbations, we demonstrate substantial improvements in model robustness\nagainst strong image-based attacks such as Auto-PGD. Our findings provide\nimportant guidelines for developing more robust VLMs, particularly for\ndeployment in safety-critical environments.",
      "tldr_zh": "本研究探讨了视觉语言模型（VLMs）的对抗鲁棒性，系统分析了模型设计选择对抵抗图像-based 攻击的影响。研究者引入了新的成本有效的提示格式化技术，包括重新表述问题和建议潜在的对抗扰动，以显著提升模型的鲁棒性。实验结果显示，这些方法在面对强攻击如 Auto-PGD 时，模型鲁棒性得到实质性改善。该工作为开发更安全的 VLMs 提供了关键指导，尤其适用于安全关键环境。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.11121v1",
      "published_date": "2024-07-15 18:00:01 UTC",
      "updated_date": "2024-07-15 18:00:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:36:53.061643"
    },
    {
      "arxiv_id": "2407.10973v3",
      "title": "Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion",
      "title_zh": "翻译失败",
      "authors": [
        "Yongyuan Liang",
        "Tingqiang Xu",
        "Kaizhe Hu",
        "Guangqi Jiang",
        "Furong Huang",
        "Huazhe Xu"
      ],
      "abstract": "Can we generate a control policy for an agent using just one demonstration of\ndesired behaviors as a prompt, as effortlessly as creating an image from a\ntextual description? In this paper, we present Make-An-Agent, a novel policy\nparameter generator that leverages the power of conditional diffusion models\nfor behavior-to-policy generation. Guided by behavior embeddings that encode\ntrajectory information, our policy generator synthesizes latent parameter\nrepresentations, which can then be decoded into policy networks. Trained on\npolicy network checkpoints and their corresponding trajectories, our generation\nmodel demonstrates remarkable versatility and scalability on multiple tasks and\nhas a strong generalization ability on unseen tasks to output well-performed\npolicies with only few-shot demonstrations as inputs. We showcase its efficacy\nand efficiency on various domains and tasks, including varying objectives,\nbehaviors, and even across different robot manipulators. Beyond simulation, we\ndirectly deploy policies generated by Make-An-Agent onto real-world robots on\nlocomotion tasks. Project page: https://cheryyunl.github.io/make-an-agent/",
      "tldr_zh": "该论文提出 Make-An-Agent，一种基于行为提示扩散（Behavior-Prompted Diffusion）的通用策略网络生成器，能从单一行为演示（如轨迹信息）轻松生成代理控制策略，类似于文本生成图像的过程。系统利用条件扩散模型（conditional diffusion models）和行为嵌入（behavior embeddings）来合成策略网络参数，并通过训练在策略网络检查点和对应轨迹上，实现多任务的灵活性和可扩展性。实验验证了其在模拟和真实机器人环境中的效能，包括处理不同目标、行为和机械臂任务，并在未见任务上表现出强泛化能力，仅需少样本输入即可生成高性能策略。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Annual Conference on Neural Information Processing Systems 38",
      "pdf_url": "http://arxiv.org/pdf/2407.10973v3",
      "published_date": "2024-07-15 17:59:57 UTC",
      "updated_date": "2024-11-04 02:44:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:37:05.694023"
    },
    {
      "arxiv_id": "2407.10972v2",
      "title": "VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation",
      "title_zh": "VGBench：评估大语言模型在矢量图形理解和生成方面的能力",
      "authors": [
        "Bocheng Zou",
        "Mu Cai",
        "Jianrui Zhang",
        "Yong Jae Lee"
      ],
      "abstract": "In the realm of vision models, the primary mode of representation is using\npixels to rasterize the visual world. Yet this is not always the best or unique\nway to represent visual content, especially for designers and artists who\ndepict the world using geometry primitives such as polygons. Vector graphics\n(VG), on the other hand, offer a textual representation of visual content,\nwhich can be more concise and powerful for content like cartoons, sketches and\nscientific figures. Recent studies have shown promising results on processing\nvector graphics with capable Large Language Models (LLMs). However, such works\nfocus solely on qualitative results, understanding, or a specific type of\nvector graphics. We propose VGBench, a comprehensive benchmark for LLMs on\nhandling vector graphics through diverse aspects, including (a) both visual\nunderstanding and generation, (b) evaluation of various vector graphics\nformats, (c) diverse question types, (d) wide range of prompting techniques,\n(e) under multiple LLMs and (f) comparison with VLMs on rasterized\nrepresentations. Evaluating on our collected 4279 understanding and 5845\ngeneration samples, we find that LLMs show strong capability on both aspects\nwhile exhibiting less desirable performance on low-level formats (SVG). Both\ndata and evaluation pipeline will be open-sourced at https://vgbench.github.io.",
      "tldr_zh": "本文提出 VGBench，一个全面基准，用于评估 Large Language Models (LLMs) 在向量图形 (VG) 理解和生成方面的能力。该基准涵盖视觉理解和生成、各种 VG 格式（如 SVG）、多样问题类型、多种提示技术，并与 Vision Language Models (VLMs) 在栅格化表示上进行比较。通过评估 4279 个理解样本和 5845 个生成样本，研究发现 LLMs 在这两个方面表现出色，但对低级格式如 SVG 的性能较弱。数据和评估管道将开源，以促进进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://vgbench.github.io",
      "pdf_url": "http://arxiv.org/pdf/2407.10972v2",
      "published_date": "2024-07-15 17:59:55 UTC",
      "updated_date": "2024-08-29 17:55:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:37:18.289704"
    },
    {
      "arxiv_id": "2407.10967v2",
      "title": "BECAUSE: Bilinear Causal Representation for Generalizable Offline Model-based Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Haohong Lin",
        "Wenhao Ding",
        "Jian Chen",
        "Laixi Shi",
        "Jiacheng Zhu",
        "Bo Li",
        "Ding Zhao"
      ],
      "abstract": "Offline model-based reinforcement learning (MBRL) enhances data efficiency by\nutilizing pre-collected datasets to learn models and policies, especially in\nscenarios where exploration is costly or infeasible. Nevertheless, its\nperformance often suffers from the objective mismatch between model and policy\nlearning, resulting in inferior performance despite accurate model predictions.\nThis paper first identifies the primary source of this mismatch comes from the\nunderlying confounders present in offline data for MBRL. Subsequently, we\nintroduce \\textbf{B}ilin\\textbf{E}ar \\textbf{CAUS}al\nr\\textbf{E}presentation~(BECAUSE), an algorithm to capture causal\nrepresentation for both states and actions to reduce the influence of the\ndistribution shift, thus mitigating the objective mismatch problem.\nComprehensive evaluations on 18 tasks that vary in data quality and environment\ncontext demonstrate the superior performance of BECAUSE over existing offline\nRL algorithms. We show the generalizability and robustness of BECAUSE under\nfewer samples or larger numbers of confounders. Additionally, we offer\ntheoretical analysis of BECAUSE to prove its error bound and sample efficiency\nwhen integrating causal representation into offline MBRL.",
      "tldr_zh": "这篇论文针对离线模型-based强化学习（MBRL）中的目标不匹配问题，提出BECAUSE算法，利用双线性因果表示（Bilinear Causal Representation）来捕捉状态和动作的因果关系，从而减少混杂因素（confounders）的影响并缓解分布偏移。BECAUSE通过这种方法提升了算法的泛化和鲁棒性，在18个任务上的全面评估中表现出优于现有离线RL算法的性能，尤其在样本量少或混杂因素多的情况下。论文还提供了理论分析，证明了BECAUSE的错误边界和样本效率，为离线MBRL的应用奠定了基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10967v2",
      "published_date": "2024-07-15 17:59:23 UTC",
      "updated_date": "2025-03-03 01:19:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:37:32.537035"
    },
    {
      "arxiv_id": "2407.10957v1",
      "title": "Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes",
      "title_zh": "Ref-AVS：引用并分割音频-视觉场景中的对象",
      "authors": [
        "Yaoting Wang",
        "Peiwen Sun",
        "Dongzhan Zhou",
        "Guangyao Li",
        "Honggang Zhang",
        "Di Hu"
      ],
      "abstract": "Traditional reference segmentation tasks have predominantly focused on silent\nvisual scenes, neglecting the integral role of multimodal perception and\ninteraction in human experiences. In this work, we introduce a novel task\ncalled Reference Audio-Visual Segmentation (Ref-AVS), which seeks to segment\nobjects within the visual domain based on expressions containing multimodal\ncues. Such expressions are articulated in natural language forms but are\nenriched with multimodal cues, including audio and visual descriptions. To\nfacilitate this research, we construct the first Ref-AVS benchmark, which\nprovides pixel-level annotations for objects described in corresponding\nmultimodal-cue expressions. To tackle the Ref-AVS task, we propose a new method\nthat adequately utilizes multimodal cues to offer precise segmentation\nguidance. Finally, we conduct quantitative and qualitative experiments on three\ntest subsets to compare our approach with existing methods from related tasks.\nThe results demonstrate the effectiveness of our method, highlighting its\ncapability to precisely segment objects using multimodal-cue expressions.\nDataset is available at\n\\href{https://gewu-lab.github.io/Ref-AVS}{https://gewu-lab.github.io/Ref-AVS}.",
      "tldr_zh": "本文引入了Ref-AVS任务，即基于包含多模态线索（如音频和视觉描述）的自然语言表达，来精确分割音频-视觉场景中的对象，从而扩展了传统参考分割任务的局限性。研究者构建了首个Ref-AVS基准数据集，提供像素级对象注释，并提出了一种新方法，利用这些multimodal cues进行有效的分割指导。实验结果显示，该方法在三个测试子集上比相关任务的现有方法表现出色，实现了更精确的对象分割，数据集可从https://gewu-lab.github.io/Ref-AVS获取。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ECCV2024",
      "pdf_url": "http://arxiv.org/pdf/2407.10957v1",
      "published_date": "2024-07-15 17:54:45 UTC",
      "updated_date": "2024-07-15 17:54:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:37:41.692977"
    },
    {
      "arxiv_id": "2407.10956v1",
      "title": "Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?",
      "title_zh": "Spider2-V：多模态代理距离自动化数据科学和工程工作流还有多远？",
      "authors": [
        "Ruisheng Cao",
        "Fangyu Lei",
        "Haoyuan Wu",
        "Jixuan Chen",
        "Yeqiao Fu",
        "Hongcheng Gao",
        "Xinzhuang Xiong",
        "Hanchong Zhang",
        "Yuchen Mao",
        "Wenjing Hu",
        "Tianbao Xie",
        "Hongshen Xu",
        "Danyang Zhang",
        "Sida Wang",
        "Ruoxi Sun",
        "Pengcheng Yin",
        "Caiming Xiong",
        "Ansong Ni",
        "Qian Liu",
        "Victor Zhong",
        "Lu Chen",
        "Kai Yu",
        "Tao Yu"
      ],
      "abstract": "Data science and engineering workflows often span multiple stages, from\nwarehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As\nvision language models (VLMs) advance in multimodal understanding and code\ngeneration, VLM-based agents could potentially automate these workflows by\ngenerating SQL queries, Python code, and GUI operations. This automation can\nimprove the productivity of experts while democratizing access to large-scale\ndata analysis. In this paper, we introduce Spider2-V, the first multimodal\nagent benchmark focusing on professional data science and engineering\nworkflows, featuring 494 real-world tasks in authentic computer environments\nand incorporating 20 enterprise-level professional applications. These tasks,\nderived from real-world use cases, evaluate the ability of a multimodal agent\nto perform data-related tasks by writing code and managing the GUI in\nenterprise data software systems. To balance realistic simulation with\nevaluation simplicity, we devote significant effort to developing automatic\nconfigurations for task setup and carefully crafting evaluation metrics for\neach task. Furthermore, we supplement multimodal agents with comprehensive\ndocuments of these enterprise data software systems. Our empirical evaluation\nreveals that existing state-of-the-art LLM/VLM-based agents do not reliably\nautomate full data workflows (14.0% success). Even with step-by-step guidance,\nthese agents still underperform in tasks that require fine-grained,\nknowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted\nworkspaces (10.6%). We hope that Spider2-V paves the way for autonomous\nmultimodal agents to transform the automation of data science and engineering\nworkflow. Our code and data are available at https://spider2-v.github.io.",
      "tldr_zh": "本研究引入了 Spider2-V，这是一个首个专注于专业数据科学和工程工作流的 multimodal agent 基准，包含 494 个真实世界任务和 20 个企业级应用，用于评估代理在编写代码、管理 GUI 和处理企业数据软件系统方面的能力。研究团队开发了自动任务配置、评估指标和全面文档，以平衡真实性和评估简便性。实验结果显示，现有的 LLM/VLM-based agents 无法可靠地自动化完整工作流（成功率仅 14.0%），即使有逐步指导，在精细 GUI 操作和远程云工作空间任务中表现仍较差（分别为 16.2% 和 10.6%）。Spider2-V 旨在推动 multimodal agents 在数据工作流自动化的发展，并提供了相关代码和数据资源。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "34 pages, 14 figures, 10 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.10956v1",
      "published_date": "2024-07-15 17:54:37 UTC",
      "updated_date": "2024-07-15 17:54:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:37:56.400466"
    },
    {
      "arxiv_id": "2407.10954v1",
      "title": "A Unified Differentiable Boolean Operator with Fuzzy Logic",
      "title_zh": "翻译失败",
      "authors": [
        "Hsueh-Ti Derek Liu",
        "Maneesh Agrawala",
        "Cem Yuksel",
        "Tim Omernick",
        "Vinith Misra",
        "Stefano Corazza",
        "Morgan McGuire",
        "Victor Zordan"
      ],
      "abstract": "This paper presents a unified differentiable boolean operator for implicit\nsolid shape modeling using Constructive Solid Geometry (CSG). Traditional CSG\nrelies on min, max operators to perform boolean operations on implicit shapes.\nBut because these boolean operators are discontinuous and discrete in the\nchoice of operations, this makes optimization over the CSG representation\nchallenging. Drawing inspiration from fuzzy logic, we present a unified boolean\noperator that outputs a continuous function and is differentiable with respect\nto operator types. This enables optimization of both the primitives and the\nboolean operations employed in CSG with continuous optimization techniques,\nsuch as gradient descent. We further demonstrate that such a continuous boolean\noperator allows modeling of both sharp mechanical objects and smooth organic\nshapes with the same framework. Our proposed boolean operator opens up new\npossibilities for future research toward fully continuous CSG optimization.",
      "tldr_zh": "本文提出了一种基于模糊逻辑的统一可微布尔运算符，用于Constructive Solid Geometry (CSG)隐式固体形状建模，以解决传统min和max运算符的不连续性和优化挑战。该运算符输出连续函数，并对运算符类型可微化，从而允许使用梯度下降等连续优化技术优化CSG中的基本形状和布尔操作。实验表明，该方法能同时建模尖锐的机械物体和光滑的有机形状，为未来的完全连续CSG优化研究开辟新可能性。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.GR",
      "comment": "SIGGRAPH'24",
      "pdf_url": "http://arxiv.org/pdf/2407.10954v1",
      "published_date": "2024-07-15 17:52:22 UTC",
      "updated_date": "2024-07-15 17:52:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:38:04.939922"
    },
    {
      "arxiv_id": "2407.10949v2",
      "title": "Representing Rule-based Chatbots with Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Dan Friedman",
        "Abhishek Panigrahi",
        "Danqi Chen"
      ],
      "abstract": "What kind of internal mechanisms might Transformers use to conduct fluid,\nnatural-sounding conversations? Prior work has illustrated by construction how\nTransformers can solve various synthetic tasks, such as sorting a list or\nrecognizing formal languages, but it remains unclear how to extend this\napproach to a conversational setting. In this work, we propose using ELIZA, a\nclassic rule-based chatbot, as a setting for formal, mechanistic analysis of\nTransformer-based chatbots. ELIZA allows us to formally model key aspects of\nconversation, including local pattern matching and long-term dialogue state\ntracking. We first present a theoretical construction of a Transformer that\nimplements the ELIZA chatbot. Building on prior constructions, particularly\nthose for simulating finite-state automata, we show how simpler mechanisms can\nbe composed and extended to produce more sophisticated behavior. Next, we\nconduct a set of empirical analyses of Transformers trained on synthetically\ngenerated ELIZA conversations. Our analysis illustrates the kinds of mechanisms\nthese models tend to prefer--for example, models favor an induction head\nmechanism over a more precise, position-based copying mechanism; and using\nintermediate generations to simulate recurrent data structures, akin to an\nimplicit scratchpad or Chain-of-Thought. Overall, by drawing an explicit\nconnection between neural chatbots and interpretable, symbolic mechanisms, our\nresults provide a new framework for the mechanistic analysis of conversational\nagents.",
      "tldr_zh": "本研究探讨了Transformer模型如何实现基于规则的聊天机器人（如经典的ELIZA），以分析其在对话中的内部机制。作者首先理论构建了一个Transformer来模拟ELIZA的功能，包括本地模式匹配和长期对话状态跟踪，基于先前模拟有限状态自动机的构造方法。实证分析显示，训练在合成ELIZA对话上的Transformer模型更倾向于使用induction head机制而非精确的位置-based copying机制，并通过中间生成模拟循环数据结构，类似于隐式scratchpad或Chain-of-Thought。总体而言，此框架为对话代理的可解释机制分析提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "NAACL 2025. Code and data are available at\n  https://github.com/princeton-nlp/ELIZA-Transformer",
      "pdf_url": "http://arxiv.org/pdf/2407.10949v2",
      "published_date": "2024-07-15 17:45:53 UTC",
      "updated_date": "2025-02-12 15:18:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:38:16.963590"
    },
    {
      "arxiv_id": "2407.12873v1",
      "title": "Evaluation of RAG Metrics for Question Answering in the Telecom Domain",
      "title_zh": "电信领域问答中 RAG 指标的评估",
      "authors": [
        "Sujoy Roychowdhury",
        "Sumit Soman",
        "H G Ranjani",
        "Neeraj Gunda",
        "Vansh Chhabra",
        "Sai Krishna Bala"
      ],
      "abstract": "Retrieval Augmented Generation (RAG) is widely used to enable Large Language\nModels (LLMs) perform Question Answering (QA) tasks in various domains.\nHowever, RAG based on open-source LLM for specialized domains has challenges of\nevaluating generated responses. A popular framework in the literature is the\nRAG Assessment (RAGAS), a publicly available library which uses LLMs for\nevaluation. One disadvantage of RAGAS is the lack of details of derivation of\nnumerical value of the evaluation metrics. One of the outcomes of this work is\na modified version of this package for few metrics (faithfulness, context\nrelevance, answer relevance, answer correctness, answer similarity and factual\ncorrectness) through which we provide the intermediate outputs of the prompts\nby using any LLMs. Next, we analyse the expert evaluations of the output of the\nmodified RAGAS package and observe the challenges of using it in the telecom\ndomain. We also study the effect of the metrics under correct vs. wrong\nretrieval and observe that few of the metrics have higher values for correct\nretrieval. We also study for differences in metrics between base embeddings and\nthose domain adapted via pre-training and fine-tuning. Finally, we comment on\nthe suitability and challenges of using these metrics for in-the-wild telecom\nQA task.",
      "tldr_zh": "本文评估了 Retrieval Augmented Generation (RAG) 指标在电信领域问答 (QA) 任务中的表现，针对现有 RAGAS 框架的数值派生细节不足问题，修改了该包以提供 faithfulness、context relevance、answer relevance、answer correctness、answer similarity 和 factual correctness 等指标的中间输出。研究通过专家评估分析了这些指标在电信领域的挑战，并比较了正确与错误检索下的表现，发现某些指标在正确检索时值更高。最终，比较了基础嵌入模型与通过预训练和微调适应领域的嵌入模型的差异，并讨论了这些指标在实际电信 QA 任务中的适用性及其潜在挑战。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for publication in ICML 2024 Workshop on Foundation Models\n  in the Wild",
      "pdf_url": "http://arxiv.org/pdf/2407.12873v1",
      "published_date": "2024-07-15 17:40:15 UTC",
      "updated_date": "2024-07-15 17:40:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:38:30.046934"
    },
    {
      "arxiv_id": "2407.10930v2",
      "title": "Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together",
      "title_zh": "翻译失败",
      "authors": [
        "Dilara Soylu",
        "Christopher Potts",
        "Omar Khattab"
      ],
      "abstract": "Natural Language Processing (NLP) systems are increasingly taking the form of\nsophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG),\nwhere each module may involve a distinct Language Model (LM) and an associated\nprompt template. These compound systems often lack intermediate labels or\ngradient flow to optimize each module, making their end-to-end optimization\nchallenging. Here we seek strategies to optimize both the module-level LM\nweights and the associated prompt templates of such systems to maximize a\ndownstream task metric. We propose for the first time combining the weight and\nprompt optimization strategies to optimize a modular LM pipeline by alternating\nbetween the two to get the same LM to teach itself. In experiments with\nmulti-hop QA, mathematical reasoning, and feature-based classification using\nmistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies\noptimizing the weights and prompts of a pipeline together outperform directly\noptimizing weights alone and prompts alone by up to 60% and 6%, respectively,\non average across LMs and tasks. BetterTogether optimizer is released in DSPy\nat http://dspy.ai",
      "tldr_zh": "本研究探讨了优化复杂NLP系统（如Retrieval Augmented Generation, RAG）的策略，提出BetterTogether方法，该方法首次结合Fine-Tuning（权重优化）和Prompt Optimization（提示优化），通过交替优化模块级Language Model (LM)权重和提示模板，实现端到端性能提升。实验在多跳QA、数学推理和基于特征的分类任务上，使用mistral-7b、llama-2-7b和llama-3-8b模型表明，BetterTogether策略比单独优化权重或提示分别平均提高60%和6%。该优化器已开源发布在DSPy（http://dspy.ai），为模块化LM管道的优化提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.10930v2",
      "published_date": "2024-07-15 17:30:31 UTC",
      "updated_date": "2024-10-07 15:52:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:38:42.103169"
    },
    {
      "arxiv_id": "2407.10920v3",
      "title": "Benchmarking Vision Language Models for Cultural Understanding",
      "title_zh": "视觉语言模型的文化理解基准测试",
      "authors": [
        "Shravan Nayak",
        "Kanishk Jain",
        "Rabiul Awal",
        "Siva Reddy",
        "Sjoerd van Steenkiste",
        "Lisa Anne Hendricks",
        "Karolina Stańczak",
        "Aishwarya Agrawal"
      ],
      "abstract": "Foundation models and vision-language pre-training have notably advanced\nVision Language Models (VLMs), enabling multimodal processing of visual and\nlinguistic data. However, their performance has been typically assessed on\ngeneral scene understanding - recognizing objects, attributes, and actions -\nrather than cultural comprehension. This study introduces CulturalVQA, a visual\nquestion-answering benchmark aimed at assessing VLM's geo-diverse cultural\nunderstanding. We curate a collection of 2,378 image-question pairs with 1-5\nanswers per question representing cultures from 11 countries across 5\ncontinents. The questions probe understanding of various facets of culture such\nas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on\nCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of\ncultural understanding across regions, with strong cultural understanding\ncapabilities for North America while significantly lower performance for\nAfrica. We observe disparity in their performance across cultural facets too,\nwith clothing, rituals, and traditions seeing higher performances than food and\ndrink. These disparities help us identify areas where VLMs lack cultural\nunderstanding and demonstrate the potential of CulturalVQA as a comprehensive\nevaluation set for gauging VLM progress in understanding diverse cultures.",
      "tldr_zh": "这篇论文引入了CulturalVQA，这是一个视觉问答基准，用于评估Vision Language Models (VLMs) 在地理多样化文化理解方面的性能。数据集包含2,378个图像-问题对，覆盖11个国家（5大洲）的文化元素，如服装、食物、饮料、仪式和传统，每个问题提供1-5个答案。基准测试结果显示，VLMs如GPT-4V和Gemini在北美文化理解方面表现强劲，但非洲和其他区域的准确率显著较低，尤其在食物和饮料类别上存在差距。这些发现有助于识别VLMs的文化理解不足，并证明CulturalVQA作为全面评估工具的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to EMNLP 2024 Main Conference",
      "pdf_url": "http://arxiv.org/pdf/2407.10920v3",
      "published_date": "2024-07-15 17:21:41 UTC",
      "updated_date": "2024-10-14 13:08:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:38:55.177421"
    },
    {
      "arxiv_id": "2407.10899v1",
      "title": "Leveraging LLM-Respondents for Item Evaluation: a Psychometric Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Yunting Liu",
        "Shreya Bhandari",
        "Zachary A. Pardos"
      ],
      "abstract": "Effective educational measurement relies heavily on the curation of\nwell-designed item pools (i.e., possessing the right psychometric properties).\nHowever, item calibration is time-consuming and costly, requiring a sufficient\nnumber of respondents for the response process. We explore using six different\nLLMs (GPT-3.5, GPT-4, Llama 2, Llama 3, Gemini-Pro, and Cohere Command R Plus)\nand various combinations of them using sampling methods to produce responses\nwith psychometric properties similar to human answers. Results show that some\nLLMs have comparable or higher proficiency in College Algebra than college\nstudents. No single LLM mimics human respondents due to narrow proficiency\ndistributions, but an ensemble of LLMs can better resemble college students'\nability distribution. The item parameters calibrated by LLM-Respondents have\nhigh correlations (e.g. > 0.8 for GPT-3.5) compared to their human calibrated\ncounterparts, and closely resemble the parameters of the human subset (e.g.\n0.02 Spearman correlation difference). Several augmentation strategies are\nevaluated for their relative performance, with resampling methods proving most\neffective, enhancing the Spearman correlation from 0.89 (human only) to 0.93\n(augmented human).",
      "tldr_zh": "本文探讨利用大型语言模型(LLMs)作为“LLM-Respondents”来评估教育项目的心理测量属性，旨在减少传统校准过程的成本和时间。研究测试了六种LLMs（如GPT-3.5、GPT-4等）及其组合，发现某些LLMs在大学代数方面的表现与人类相当或更高。结果显示，单一LLMs由于熟练度分布较窄无法完全模仿人类，但通过采样方法组合LLMs，能更好地模拟人类能力分布，且校准的项目参数与人类参数高度相关（例如，GPT-3.5的相关性>0.8）。此外，重采样等增强策略显著提高了Spearman相关性，从0.89提升至0.93。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10899v1",
      "published_date": "2024-07-15 16:49:26 UTC",
      "updated_date": "2024-07-15 16:49:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:39:07.670848"
    },
    {
      "arxiv_id": "2407.10888v1",
      "title": "Leveraging Multimodal CycleGAN for the Generation of Anatomically Accurate Synthetic CT Scans from MRIs",
      "title_zh": "利用多模态 CycleGAN 从",
      "authors": [
        "Leonardo Crespi",
        "Samuele Camnasio",
        "Damiano Dei",
        "Nicola Lambri",
        "Pietro Mancosu",
        "Marta Scorsetti",
        "Daniele Loiacono"
      ],
      "abstract": "In many clinical settings, the use of both Computed Tomography (CT) and\nMagnetic Resonance (MRI) is necessary to pursue a thorough understanding of the\npatient's anatomy and to plan a suitable therapeutical strategy; this is often\nthe case in MRI-based radiotherapy, where CT is always necessary to prepare the\ndose delivery, as it provides the essential information about the radiation\nabsorption properties of the tissues. Sometimes, MRI is preferred to contour\nthe target volumes. However, this approach is often not the most efficient, as\nit is more expensive, time-consuming and, most importantly, stressful for the\npatients. To overcome this issue, in this work, we analyse the capabilities of\ndifferent configurations of Deep Learning models to generate synthetic CT scans\nfrom MRI, leveraging the power of Generative Adversarial Networks (GANs) and,\nin particular, the CycleGAN architecture, capable of working in an unsupervised\nmanner and without paired images, which were not available. Several CycleGAN\nmodels were trained unsupervised to generate CT scans from different MRI\nmodalities with and without contrast agents. To overcome the problem of not\nhaving a ground truth, distribution-based metrics were used to assess the\nmodel's performance quantitatively, together with a qualitative evaluation\nwhere physicians were asked to differentiate between real and synthetic images\nto understand how realistic the generated images were. The results show how,\ndepending on the input modalities, the models can have very different\nperformances; however, models with the best quantitative results, according to\nthe distribution-based metrics used, can generate very difficult images to\ndistinguish from the real ones, even for physicians, demonstrating the\napproach's potential.",
      "tldr_zh": "本研究旨在解决临床中同时使用 CT 和 MRI 的低效问题，特别是 MRI-based radiotherapy 的场景，通过生成从 MRI 到解剖精确的合成 CT 扫描来减少患者负担。研究利用 Multimodal CycleGAN 架构的变体进行无监督训练，从不同 MRI 模态（包括有无对比剂）生成合成 CT 图像，而无需配对图像。结果显示，模型性能因输入模态而异，但最佳配置在基于分布的量化指标和医生定性评估中表现出色，能生成高度逼真的合成图像，甚至难以与真实 CT 区分，从而证明了该方法的潜在临床价值。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Currently submitted to: Scientific Reports",
      "pdf_url": "http://arxiv.org/pdf/2407.10888v1",
      "published_date": "2024-07-15 16:38:59 UTC",
      "updated_date": "2024-07-15 16:38:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:39:18.639002"
    },
    {
      "arxiv_id": "2407.10887v2",
      "title": "Hey, That's My Model! Introducing Chain & Hash, An LLM Fingerprinting Technique",
      "title_zh": "翻译失败",
      "authors": [
        "Mark Russinovich",
        "Ahmed Salem"
      ],
      "abstract": "Amid growing concerns over the ease of theft and misuse of Large Language\nModels (LLMs), the need for fingerprinting models has increased.\nFingerprinting, in this context, means that the model owner can link a given\nmodel to their original version, thereby identifying if their model is being\nmisused or has been completely stolen. In this paper, we first define a set\nfive properties a successful fingerprint should satisfy; namely, the\nfingerprint should be Transparent, Efficient, Persistent, Robust, and\nUnforgeable. Next, we propose Chain & Hash, a new, simple fingerprinting\napproach that implements a fingerprint with a cryptographic flavor, achieving\nall these properties. Chain & Hash involves generating a set of questions (the\nfingerprints) along with a set of potential answers. These elements are hashed\ntogether using a secure hashing technique to select the value for each\nquestion, hence providing an unforgeability property-preventing adversaries\nfrom claiming false ownership. We evaluate the Chain & Hash technique on\nmultiple models and demonstrate its robustness against benign transformations,\nsuch as fine-tuning on different datasets, and adversarial attempts to erase\nthe fingerprint. Finally, our experiments demonstrate the efficiency of\nimplementing Chain & Hash and its utility, where fingerprinted models achieve\nalmost the same performance as non-fingerprinted ones across different\nbenchmarks.",
      "tldr_zh": "该论文针对大型语言模型(LLM)盗用和误用的担忧，提出了一种名为Chain & Hash的指纹技术，用于帮助模型所有者识别其模型是否被盗或误用。Chain & Hash方法通过生成一组问题和潜在答案，并使用安全哈希技术将它们结合，选择每个问题的值，从而满足Transparent（透明）、Efficient（高效）、Persistent（持久）、Robust（鲁棒）和Unforgeable（不可伪造）的五项属性。实验结果显示，该技术在多个模型上表现出色，能够抵抗细调数据集等良性变换和对抗性攻击，同时指纹模型在不同基准测试中的性能几乎与非指纹模型相当。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10887v2",
      "published_date": "2024-07-15 16:38:56 UTC",
      "updated_date": "2024-07-17 07:39:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:39:30.328007"
    },
    {
      "arxiv_id": "2407.10878v1",
      "title": "Deep Causal Learning to Explain and Quantify The Geo-Tension's Impact on Natural Gas Market",
      "title_zh": "翻译失败",
      "authors": [
        "Philipp Kai Peter",
        "Yulin Li",
        "Ziyue Li",
        "Wolfgang Ketter"
      ],
      "abstract": "Natural gas demand is a crucial factor for predicting natural gas prices and\nthus has a direct influence on the power system. However, existing methods face\nchallenges in assessing the impact of shocks, such as the outbreak of the\nRussian-Ukrainian war. In this context, we apply deep neural network-based\nGranger causality to identify important drivers of natural gas demand.\nFurthermore, the resulting dependencies are used to construct a counterfactual\ncase without the outbreak of the war, providing a quantifiable estimate of the\noverall effect of the shock on various German energy sectors. The code and\ndataset are available at https://github.com/bonaldli/CausalEnergy.",
      "tldr_zh": "这篇论文利用基于深度神经网络的Granger causality方法，识别自然气需求的重要驱动因素，以评估地缘紧张局势（如俄罗斯-乌克兰战争）对自然气市场的冲击。研究者通过构建一个没有战争爆发的反事实案例，量化了该事件对德国能源部门的影响，提供了一个可量化的整体效应估计。代码和数据集已在GitHub上公开，便于进一步验证和应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by IEEE PES ISGT Europe 2024 conference",
      "pdf_url": "http://arxiv.org/pdf/2407.10878v1",
      "published_date": "2024-07-15 16:28:26 UTC",
      "updated_date": "2024-07-15 16:28:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:39:42.533654"
    },
    {
      "arxiv_id": "2407.10873v1",
      "title": "Understanding the Importance of Evolutionary Search in Automated Heuristic Design with Large Language Models",
      "title_zh": "理解进化搜索在基于大型语言模型的自动启发式设计中的重要性",
      "authors": [
        "Rui Zhang",
        "Fei Liu",
        "Xi Lin",
        "Zhenkun Wang",
        "Zhichao Lu",
        "Qingfu Zhang"
      ],
      "abstract": "Automated heuristic design (AHD) has gained considerable attention for its\npotential to automate the development of effective heuristics. The recent\nadvent of large language models (LLMs) has paved a new avenue for AHD, with\ninitial efforts focusing on framing AHD as an evolutionary program search (EPS)\nproblem. However, inconsistent benchmark settings, inadequate baselines, and a\nlack of detailed component analysis have left the necessity of integrating LLMs\nwith search strategies and the true progress achieved by existing LLM-based EPS\nmethods to be inadequately justified. This work seeks to fulfill these research\nqueries by conducting a large-scale benchmark comprising four LLM-based EPS\nmethods and four AHD problems across nine LLMs and five independent runs. Our\nextensive experiments yield meaningful insights, providing empirical grounding\nfor the importance of evolutionary search in LLM-based AHD approaches, while\nalso contributing to the advancement of future EPS algorithmic development. To\nfoster accessibility and reproducibility, we have fully open-sourced our\nbenchmark and corresponding results.",
      "tldr_zh": "该研究探讨了进化搜索(Evolutionary Search)在基于大型语言模型(LLMs)的自动启发式设计(Automated Heuristic Design, AHD)中的重要性，针对现有方法存在的基准设置不一致、基线不足和组件分析缺乏等问题进行了全面评估。研究者通过大规模基准测试，评估了四种LLM-based Evolutionary Program Search(EPS)方法在四种AHD问题上的表现，涵盖九个LLMs和五次独立运行。实验结果提供了实证证据，证明了进化搜索在提升LLM-based AHD效果方面的关键作用，并为未来EPS算法的发展提供了指导；同时，该基准测试已完全开源，以促进可访问性和可重复性。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "Accepted by the 18th International Conference on Parallel Problem\n  Solving From Nature (PPSN 2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.10873v1",
      "published_date": "2024-07-15 16:21:20 UTC",
      "updated_date": "2024-07-15 16:21:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:39:54.728783"
    },
    {
      "arxiv_id": "2407.10870v1",
      "title": "GPT Sonograpy: Hand Gesture Decoding from Forearm Ultrasound Images via VLM",
      "title_zh": "翻译失败",
      "authors": [
        "Keshav Bimbraw",
        "Ye Wang",
        "Jing Liu",
        "Toshiaki Koike-Akino"
      ],
      "abstract": "Large vision-language models (LVLMs), such as the Generative Pre-trained\nTransformer 4-omni (GPT-4o), are emerging multi-modal foundation models which\nhave great potential as powerful artificial-intelligence (AI) assistance tools\nfor a myriad of applications, including healthcare, industrial, and academic\nsectors. Although such foundation models perform well in a wide range of\ngeneral tasks, their capability without fine-tuning is often limited in\nspecialized tasks. However, full fine-tuning of large foundation models is\nchallenging due to enormous computation/memory/dataset requirements. We show\nthat GPT-4o can decode hand gestures from forearm ultrasound data even with no\nfine-tuning, and improves with few-shot, in-context learning.",
      "tldr_zh": "本文提出GPT Sonograpy框架，利用视觉语言模型(VLM)如GPT-4o从前臂超声图像中解码手势，展示了这些模型在无需微调的情况下即可处理专业任务。研究发现，通过少样本和上下文学习，GPT-4o的性能得到显著提升，避免了传统微调所需的庞大计算资源。总体而言，这为AI在医疗和工业领域的应用提供了高效、可扩展的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.10870v1",
      "published_date": "2024-07-15 16:18:06 UTC",
      "updated_date": "2024-07-15 16:18:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:40:07.825952"
    },
    {
      "arxiv_id": "2407.10855v1",
      "title": "Weighted Grouped Query Attention in Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Sai Sena Chinnakonduru",
        "Astarag Mohapatra"
      ],
      "abstract": "The attention mechanism forms the foundational blocks for transformer\nlanguage models. Recent approaches show that scaling the model achieves\nhuman-level performance. However, with increasing demands for scaling and\nconstraints on hardware memory, the inference costs of these models remain\nhigh. To reduce the inference time, Multi-Query Attention (MQA) and\nGrouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieet\nal., 2023) respectively. In this paper, we propose a variation of Grouped-Query\nAttention, termed Weighted Grouped-Query Attention (WGQA). We introduced new\nlearnable parameters for each key and value head in the T5 decoder attention\nblocks, enabling the model to take a weighted average during finetuning. Our\nmodel achieves an average of 0.53% improvement over GQA, and the performance\nconverges to traditional Multi-head attention (MHA) with no additional overhead\nduring inference. We evaluated the introduction of these parameters and\nsubsequent finetuning informs the model about the grouping mechanism during\ntraining, thereby enhancing performance. Additionally, we demonstrate the\nscaling laws in our analysis by comparing the results between T5-small and\nT5-base architecture.",
      "tldr_zh": "本文提出 Weighted Grouped-Query Attention (WGQA)，一种对 Grouped-Query Attention (GQA) 的改进，用于减少 Transformer 语言模型的推理时间，通过在 T5 解码器注意力块中为每个 key 和 value head 引入可学习参数进行加权平均。相比 GQA，WGQA 在性能上平均提高了 0.53%，并能收敛到传统 Multi-head Attention (MHA) 的水平，而不增加推理开销。实验还通过比较 T5-small 和 T5-base 模型，展示了该方法的缩放定律，并证明微调过程帮助模型更好地学习分组机制。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10855v1",
      "published_date": "2024-07-15 16:07:13 UTC",
      "updated_date": "2024-07-15 16:07:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:40:20.634519"
    },
    {
      "arxiv_id": "2407.10853v3",
      "title": "An Actionable Framework for Assessing Bias and Fairness in Large Language Model Use Cases",
      "title_zh": "翻译失败",
      "authors": [
        "Dylan Bouchard"
      ],
      "abstract": "Large language models (LLMs) can exhibit bias in a variety of ways. Such\nbiases can create or exacerbate unfair outcomes for certain groups within a\nprotected attribute, including, but not limited to sex, race, sexual\norientation, or age. In this paper, we propose a decision framework that allows\npractitioners to determine which bias and fairness metrics to use for a\nspecific LLM use case. To establish the framework, we define bias and fairness\nrisks for LLMs, map those risks to a taxonomy of LLM use cases, and then define\nvarious metrics to assess each type of risk. Instead of focusing solely on the\nmodel itself, we account for both prompt-specific- and model-specific-risk by\ndefining evaluations at the level of an LLM use case, characterized by a model\nand a population of prompts. Furthermore, because all of the evaluation metrics\nare calculated solely using the LLM output, our proposed framework is highly\npractical and easily actionable for practitioners. For streamlined\nimplementation, all evaluation metrics included in the framework are offered in\nthis paper's companion Python toolkit, LangFair. Finally, our experiments\ndemonstrate substantial variation in bias and fairness across use cases,\nunderscoring the importance of use-case-level assessments.",
      "tldr_zh": "本文提出一个可操作的决策框架，用于评估大型语言模型(LLMs)用例中的偏见和公平性风险，特别是针对受保护群体（如性别、种族等）的不公平结果。框架将偏见风险映射到LLMs用例分类，并定义了相应的评估指标，这些指标不仅考虑模型本身，还包括提示特定的风险，并在用例级别（模型和提示集）进行计算。所有指标基于LLMs输出实现，便于从业者使用，并通过配套Python工具包LangFair提供流线型实施。实验结果显示，不同用例的偏见和公平性存在显著差异，突显了用例级评估的重要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "LangFair repository: https://github.com/cvs-health/langfair",
      "pdf_url": "http://arxiv.org/pdf/2407.10853v3",
      "published_date": "2024-07-15 16:04:44 UTC",
      "updated_date": "2025-02-13 14:13:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:40:30.705102"
    },
    {
      "arxiv_id": "2407.10839v1",
      "title": "Offline Reinforcement Learning with Imputed Rewards",
      "title_zh": "离线强化学习中的奖励推断",
      "authors": [
        "Carlo Romeo",
        "Andrew D. Bagdanov"
      ],
      "abstract": "Offline Reinforcement Learning (ORL) offers a robust solution to training\nagents in applications where interactions with the environment must be strictly\nlimited due to cost, safety, or lack of accurate simulation environments.\nDespite its potential to facilitate deployment of artificial agents in the real\nworld, Offline Reinforcement Learning typically requires very many\ndemonstrations annotated with ground-truth rewards. Consequently,\nstate-of-the-art ORL algorithms can be difficult or impossible to apply in\ndata-scarce scenarios. In this paper we propose a simple but effective Reward\nModel that can estimate the reward signal from a very limited sample of\nenvironment transitions annotated with rewards. Once the reward signal is\nmodeled, we use the Reward Model to impute rewards for a large sample of\nreward-free transitions, thus enabling the application of ORL techniques. We\ndemonstrate the potential of our approach on several D4RL continuous locomotion\ntasks. Our results show that, using only 1\\% of reward-labeled transitions from\nthe original datasets, our learned reward model is able to impute rewards for\nthe remaining 99\\% of the transitions, from which performant agents can be\nlearned using Offline Reinforcement Learning.",
      "tldr_zh": "该论文针对离线强化学习(Offline Reinforcement Learning, ORL)中数据稀缺的问题，提出了一种简单有效的奖励模型(Reward Model)，它能从少量带有奖励的环境转换样本中估计奖励信号。接着，使用该模型为大量无奖励的转换估算(impute)奖励，从而使ORL技术能够应用于数据不足的场景。在D4RL连续运动任务的实验中，仅使用原数据集1%的奖励标记转换，就能为剩余99%的转换估算奖励，并从中训练出性能良好的代理，展示了该方法的实际潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "RLBRew Workshop @ RLC 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.10839v1",
      "published_date": "2024-07-15 15:53:13 UTC",
      "updated_date": "2024-07-15 15:53:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:40:44.315164"
    },
    {
      "arxiv_id": "2407.10835v1",
      "title": "Exploration in Knowledge Transfer Utilizing Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Adam Jedlička",
        "Tatiana Valentine Guy"
      ],
      "abstract": "The contribution focuses on the problem of exploration within the task of\nknowledge transfer. Knowledge transfer refers to the useful application of the\nknowledge gained while learning the source task in the target task. The\nintended benefit of knowledge transfer is to speed up the learning process of\nthe target task. The article aims to compare several exploration methods used\nwithin a deep transfer learning algorithm, particularly Deep Target Transfer\n$Q$-learning. The methods used are $\\epsilon$-greedy, Boltzmann, and upper\nconfidence bound exploration. The aforementioned transfer learning algorithms\nand exploration methods were tested on the virtual drone problem. The results\nhave shown that the upper confidence bound algorithm performs the best out of\nthese options. Its sustainability to other applications is to be checked.",
      "tldr_zh": "这篇论文探讨了在知识转移任务中使用强化学习（reinforcement learning）的探索方法，以加速目标任务的学习过程。研究比较了ε-greedy、Boltzmann和upper confidence bound exploration三种策略在Deep Target Transfer Q-learning算法中的表现。实验结果显示，upper confidence bound算法在虚拟无人机问题上表现最佳，并建议进一步检验其在其他应用中的可持续性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.10835v1",
      "published_date": "2024-07-15 15:45:29 UTC",
      "updated_date": "2024-07-15 15:45:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:40:54.555380"
    },
    {
      "arxiv_id": "2407.10834v3",
      "title": "MetaLLM: A High-performant and Cost-efficient Dynamic Framework for Wrapping LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Quang H. Nguyen",
        "Thinh Dao",
        "Duy C. Hoang",
        "Juliette Decugis",
        "Saurav Manchanda",
        "Nitesh V. Chawla",
        "Khoa D. Doan"
      ],
      "abstract": "The rapid progress in machine learning (ML) has brought forth many large\nlanguage models (LLMs) that excel in various tasks and areas. These LLMs come\nwith different abilities and costs in terms of computation or pricing. Since\nthe demand for each query can vary, e.g., because of the queried domain or its\ncomplexity, defaulting to one LLM in an application is not usually the best\nchoice, whether it is the biggest, priciest, or even the one with the best\naverage test performance. Consequently, picking the right LLM that is both\naccurate and cost-effective for an application is necessary yet remains a\nchallenge. In this paper, we introduce MetaLLM, a framework that dynamically\nand intelligently routes each query to the optimal LLM (among several available\nLLMs) for classification and multi-choice question-answering tasks, achieving\nsignificantly improved accuracy and cost-effectiveness. By framing the\nselection problem as a multi-armed bandit, MetaLLM balances prediction accuracy\nand cost efficiency under uncertainty. Our experiments, conducted on popular\nLLM platforms such as OpenAI and Together AI, as well as open-source LLM,\nshowcase MetaLLM's efficacy in real-world scenarios, laying the groundwork for\nfuture extensions.",
      "tldr_zh": "该研究提出MetaLLM框架，一种高性能且成本高效的动态系统，用于包装和管理多种大型语言模型（LLMs），以动态路由查询到最优LLM，从而提升分类和多选问题回答任务的准确性和成本效率。MetaLLM将LLM选择问题建模为multi-armed bandit问题，通过平衡预测准确性与不确定性下的成本开销，实现智能决策。实验在OpenAI、Together AI和开源LLM平台上验证了框架的有效性，显著提高了性能，并为未来扩展奠定了基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10834v3",
      "published_date": "2024-07-15 15:45:07 UTC",
      "updated_date": "2025-04-22 03:55:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:41:07.316230"
    },
    {
      "arxiv_id": "2407.10829v1",
      "title": "BiasScanner: Automatic Detection and Classification of News Bias to Strengthen Democracy",
      "title_zh": "BiasScanner：自动检测和分类新闻偏见以加强民主",
      "authors": [
        "Tim Menzner",
        "Jochen L. Leidner"
      ],
      "abstract": "The increasing consumption of news online in the 21st century coincided with\nincreased publication of disinformation, biased reporting, hate speech and\nother unwanted Web content. We describe BiasScanner, an application that aims\nto strengthen democracy by supporting news consumers with scrutinizing news\narticles they are reading online. BiasScanner contains a server-side\npre-trained large language model to identify biased sentences of news articles\nand a front-end Web browser plug-in. At the time of writing, BiasScanner can\nidentify and classify more than two dozen types of media bias at the sentence\nlevel, making it the most fine-grained model and only deployed application\n(automatic system in use) of its kind. It was implemented in a light-weight and\nprivacy-respecting manner, and in addition to highlighting likely biased\nsentence it also provides explanations for each classification decision as well\nas a summary analysis for each news article. While prior research has addressed\nnews bias detection, we are not aware of any work that resulted in a deployed\nbrowser plug-in (c.f. also biasscanner.org for a Web demo).",
      "tldr_zh": "这篇论文介绍了 BiasScanner，一种自动检测和分类新闻偏见的工具，旨在通过帮助用户审视在线新闻文章来加强民主。系统采用服务器端的预训练大型语言模型（large language model），在句子级别识别和分类超过 20 种媒体偏见类型，同时提供解释和文章总结分析。BiasScanner 以轻量级、尊重隐私的方式实现为浏览器插件，是目前最细粒度的已部署应用，与现有研究相比填补了实际部署的空白。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "I.2.7; H.3.3"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 3 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2407.10829v1",
      "published_date": "2024-07-15 15:42:22 UTC",
      "updated_date": "2024-07-15 15:42:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:41:19.001717"
    },
    {
      "arxiv_id": "2407.10828v1",
      "title": "Towards Enhanced Classification of Abnormal Lung sound in Multi-breath: A Light Weight Multi-label and Multi-head Attention Classification Method",
      "title_zh": "翻译失败",
      "authors": [
        "Yi-Wei Chua",
        "Yun-Chien Cheng"
      ],
      "abstract": "This study aims to develop an auxiliary diagnostic system for classifying\nabnormal lung respiratory sounds, enhancing the accuracy of automatic abnormal\nbreath sound classification through an innovative multi-label learning approach\nand multi-head attention mechanism. Addressing the issue of class imbalance and\nlack of diversity in existing respiratory sound datasets, our study employs a\nlightweight and highly accurate model, using a two-dimensional label set to\nrepresent multiple respiratory sound characteristics. Our method achieved a\n59.2% ICBHI score in the four-category task on the ICBHI2017 dataset,\ndemonstrating its advantages in terms of lightweight and high accuracy. This\nstudy not only improves the accuracy of automatic diagnosis of lung respiratory\nsound abnormalities but also opens new possibilities for clinical applications.",
      "tldr_zh": "该研究旨在开发一个轻量级多标签学习和多头 attention 机制的分类方法，以提升异常肺部呼吸音的自动分类准确性。针对现有数据集中的类别不平衡和多样性不足问题，该方法使用二维标签集来表示多种呼吸音特征，从而实现高效的模型设计。在 ICBHI2017 数据集的四分类任务中，该方法取得了 59.2% 的 ICBHI score，表现出色。该创新不仅提高了肺部呼吸音异常的自动诊断精度，还为临床应用提供了新的可能性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10828v1",
      "published_date": "2024-07-15 15:40:02 UTC",
      "updated_date": "2024-07-15 15:40:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:41:31.801730"
    },
    {
      "arxiv_id": "2407.10820v3",
      "title": "Enabling MCTS Explainability for Sequential Planning Through Computation Tree Logic",
      "title_zh": "通过计算树逻辑启用 MCTS 在顺序规划中的可解释性",
      "authors": [
        "Ziyan An",
        "Hendrik Baier",
        "Abhishek Dubey",
        "Ayan Mukhopadhyay",
        "Meiyi Ma"
      ],
      "abstract": "Monte Carlo tree search (MCTS) is one of the most capable online search\nalgorithms for sequential planning tasks, with significant applications in\nareas such as resource allocation and transit planning. Despite its strong\nperformance in real-world deployment, the inherent complexity of MCTS makes it\nchallenging to understand for users without technical background. This paper\nconsiders the use of MCTS in transportation routing services, where the\nalgorithm is integrated to develop optimized route plans. These plans are\nrequired to meet a range of constraints and requirements simultaneously,\nfurther complicating the task of explaining the algorithm's operation in\nreal-world contexts. To address this critical research gap, we introduce a\nnovel computation tree logic-based explainer for MCTS. Our framework begins by\ntaking user-defined requirements and translating them into rigorous logic\nspecifications through the use of language templates. Then, our explainer\nincorporates a logic verification and quantitative evaluation module that\nvalidates the states and actions traversed by the MCTS algorithm. The outcomes\nof this analysis are then rendered into human-readable descriptive text using a\nsecond set of language templates. The user satisfaction of our approach was\nassessed through a survey with 82 participants. The results indicated that our\nexplanatory approach significantly outperforms other baselines in user\npreference.",
      "tldr_zh": "本论文针对Monte Carlo Tree Search (MCTS)算法在顺序规划任务（如交通路由）中的复杂性问题，提出了一种基于Computation Tree Logic (CTL)的解释框架，以提升其可解释性。该框架首先将用户需求转化为严格的逻辑规范，并通过语言模板进行处理；随后，集成逻辑验证和量化评估模块，来验证MCTS算法的路径和动作；最后，将分析结果转化为易读的文本描述。实验通过对82名参与者的调查显示，该方法在用户满意度和偏好上显著优于基线模型，为MCTS在实际应用中的透明度提供了新途径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by the Proceedings of the 27th European Conference on\n  Artificial Intelligence (ECAI)",
      "pdf_url": "http://arxiv.org/pdf/2407.10820v3",
      "published_date": "2024-07-15 15:35:09 UTC",
      "updated_date": "2024-10-29 18:42:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:41:42.675940"
    },
    {
      "arxiv_id": "2407.10817v1",
      "title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Tu Vu",
        "Kalpesh Krishna",
        "Salaheddin Alzubi",
        "Chris Tar",
        "Manaal Faruqui",
        "Yun-Hsuan Sung"
      ],
      "abstract": "As large language models (LLMs) advance, it becomes more challenging to\nreliably evaluate their output due to the high costs of human evaluation. To\nmake progress towards better LLM autoraters, we introduce FLAMe, a family of\nFoundational Large Autorater Models. FLAMe is trained on our large and diverse\ncollection of 100+ quality assessment tasks comprising 5M+ human judgments,\ncurated and standardized using publicly released human evaluations from\nprevious research. FLAMe significantly improves generalization to a wide\nvariety of held-out tasks, outperforming LLMs trained on proprietary data like\nGPT-4 and Claude-3 on many tasks. We show that FLAMe can also serve as a\npowerful starting point for further downstream fine-tuning, using reward\nmodeling evaluation as a case study (FLAMe-RM). Notably, on RewardBench, our\nFLAMe-RM-24B model (with an accuracy of 87.8%) is the top-performing generative\nmodel trained exclusively on permissively licensed data, outperforming both\nGPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we explore a more\ncomputationally efficient approach using a novel tail-patch fine-tuning\nstrategy to optimize our FLAMe multitask mixture for reward modeling evaluation\n(FLAMe-Opt-RM), offering competitive RewardBench performance while requiring\napproximately 25x less training datapoints. Overall, our FLAMe variants\noutperform all popular proprietary LLM-as-a-Judge models we consider across 8\nout of 12 autorater evaluation benchmarks, encompassing 53 quality assessment\ntasks, including RewardBench and LLM-AggreFact. Finally, our analysis reveals\nthat FLAMe is significantly less biased than these LLM-as-a-Judge models on the\nCoBBLEr autorater bias benchmark, while effectively identifying high-quality\nresponses for code generation.",
      "tldr_zh": "本研究提出 FLAMe，一系列 Foundational Large Autorater Models，用于改善大型语言模型 (LLMs) 的自动评估，以降低人力评估成本。FLAMe 基于一个包含 100+ 质量评估任务和 5M+ 人判断的大规模数据集进行训练，显著提升了对多种未见任务的泛化性能，并在许多基准上超越了基于专有数据的模型，如 GPT-4 和 Claude-3。实验结果显示，FLAMe-RM 模型在 RewardBench 上达到 87.8% 的准确率，并通过 tail-patch fine-tuning 策略创建的 FLAMe-Opt-RM 版本，仅需 25 倍更少的训练数据即实现竞争性表现，同时在偏见基准上表现出更低的偏见。总体而言，FLAMe 在 12 个自动评估基准中的 8 个上领先，涵盖 53 个任务，为更可靠的 LLM 评估提供高效解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "31 pages, 5 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.10817v1",
      "published_date": "2024-07-15 15:33:45 UTC",
      "updated_date": "2024-07-15 15:33:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:41:56.879048"
    },
    {
      "arxiv_id": "2407.10811v1",
      "title": "GuideLight: \"Industrial Solution\" Guidance for More Practical Traffic Signal Control Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Haoyuan Jiang",
        "Xuantang Xiong",
        "Ziyue Li",
        "Hangyu Mao",
        "Guanghu Sui",
        "Jingqing Ruan",
        "Yuheng Cheng",
        "Hua Wei",
        "Wolfgang Ketter",
        "Rui Zhao"
      ],
      "abstract": "Currently, traffic signal control (TSC) methods based on reinforcement\nlearning (RL) have proven superior to traditional methods. However, most RL\nmethods face difficulties when applied in the real world due to three factors:\ninput, output, and the cycle-flow relation. The industry's observable input is\nmuch more limited than simulation-based RL methods. For real-world solutions,\nonly flow can be reliably collected, whereas common RL methods need more. For\nthe output action, most RL methods focus on acyclic control, which real-world\nsignal controllers do not support. Most importantly, industry standards require\na consistent cycle-flow relationship: non-decreasing and different response\nstrategies for low, medium, and high-level flows, which is ignored by the RL\nmethods. To narrow the gap between RL methods and industry standards, we\ninnovatively propose to use industry solutions to guide the RL agent.\nSpecifically, we design behavior cloning and curriculum learning to guide the\nagent to mimic and meet industry requirements and, at the same time, leverage\nthe power of exploration and exploitation in RL for better performance. We\ntheoretically prove that such guidance can largely decrease the sample\ncomplexity to polynomials in the horizon when searching for an optimal policy.\nOur rigid experiments show that our method has good cycle-flow relation and\nsuperior performance.",
      "tldr_zh": "本文提出 GuideLight 方法，以行业解决方案指导强化学习 (RL) 代理，用于更实际的交通信号控制 (TSC)。针对 RL 方法在输入（如仅可收集流量数据）、输出（如不支持无周期控制）和周期-流量关系（如非递减策略）等方面的实际应用挑战，该方法结合行为克隆 (behavior cloning) 和课程学习 (curriculum learning)，让代理模仿行业标准并利用 RL 的探索与利用机制。理论证明此指导可将样本复杂度降低至多项式级别，实验结果显示 GuideLight 具有良好的周期-流量关系，并显著提升性能。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "Under Review of IEEE Transactions on Intelligent Transportation\n  Systems",
      "pdf_url": "http://arxiv.org/pdf/2407.10811v1",
      "published_date": "2024-07-15 15:26:10 UTC",
      "updated_date": "2024-07-15 15:26:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:42:07.438725"
    },
    {
      "arxiv_id": "2407.10810v2",
      "title": "FabGPT: An Efficient Large Multimodal Model for Complex Wafer Defect Knowledge Queries",
      "title_zh": "FabGPT：用于复杂晶圆缺陷知识查询的高效大型多模态模型",
      "authors": [
        "Yuqi Jiang",
        "Xudong Lu",
        "Qian Jin",
        "Qi Sun",
        "Hanming Wu",
        "Cheng Zhuo"
      ],
      "abstract": "Intelligence is key to advancing integrated circuit (IC) fabrication. Recent\nbreakthroughs in Large Multimodal Models (LMMs) have unlocked extraditionary\nabilities in understanding images and text, fostering intelligent fabrication.\nLeveraging the power of LMMs, we introduce FabGPT, a customized IC fabrication\nlarge multimodal model for wafer defect knowledge query. FabGPT manifests\nexpertise in conducting defect detection in Scanning Electron Microscope (SEM)\nimages, performing root cause analysis, and providing expert Q&A on fabrication\nprocesses. FabGPT matches enhanced multimodal features to automatically detect\nminute defects under complex wafer backgrounds and reduce the subjectivity of\nmanual threshold settings. Besides, the proposed modulation module and\ninteractive corpus training strategy embed wafer defect knowledge into the\npre-trained model, effectively balancing Q&A queries related to defect\nknowledge and original knowledge and mitigating the modality bias issues.\nExperiments on in-house fab data show that FabGPT achieves significant\nperformance improvement in wafer defect detection and knowledge querying.",
      "tldr_zh": "本研究引入 FabGPT，一种高效的大型多模态模型 (LMMs)，针对集成电路 (IC) 制造中的复杂晶圆缺陷知识查询。该模型能够在 Scanning Electron Microscope (SEM) 图像中自动检测微小缺陷、进行根因分析，并提供专家 Q&A，同时通过增强多模态特征和手动阈值设置减少主观性。FabGPT 采用调制模块和交互语料训练策略，将晶圆缺陷知识嵌入预训练模型中，平衡缺陷相关查询与原有知识，并缓解模态偏差问题。实验在内部晶圆厂数据上显示，该模型在缺陷检测和知识查询方面取得了显著性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.AR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Published in ACM/IEEE International Conference On Computer Aided\n  Design (ICCAD) 2024. Corresponding Author: Qi Sun (qisunchn@zju.edu.cn)",
      "pdf_url": "http://arxiv.org/pdf/2407.10810v2",
      "published_date": "2024-07-15 15:25:45 UTC",
      "updated_date": "2025-02-15 14:37:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:42:20.355663"
    },
    {
      "arxiv_id": "2407.10805v7",
      "title": "Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning with Knowledge-guided Retrieval Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Shengjie Ma",
        "Chengjin Xu",
        "Xuhui Jiang",
        "Muzhi Li",
        "Huaren Qu",
        "Cehao Yang",
        "Jiaxin Mao",
        "Jian Guo"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has improved large language models\n(LLMs) by using knowledge retrieval to overcome knowledge deficiencies.\nHowever, current RAG methods often fall short of ensuring the depth and\ncompleteness of retrieved information, which is necessary for complex reasoning\ntasks. In this work, we introduce Think-on-Graph 2.0 (ToG-2), a hybrid RAG\nframework that iteratively retrieves information from both unstructured and\nstructured knowledge sources in a tight-coupling manner. Specifically, ToG-2\nleverages knowledge graphs (KGs) to link documents via entities, facilitating\ndeep and knowledge-guided context retrieval. Simultaneously, it utilizes\ndocuments as entity contexts to achieve precise and efficient graph retrieval.\nToG-2 alternates between graph retrieval and context retrieval to search for\nin-depth clues relevant to the question, enabling LLMs to generate answers. We\nconduct a series of well-designed experiments to highlight the following\nadvantages of ToG-2: 1) ToG-2 tightly couples the processes of context\nretrieval and graph retrieval, deepening context retrieval via the KG while\nenabling reliable graph retrieval based on contexts; 2) it achieves deep and\nfaithful reasoning in LLMs through an iterative knowledge retrieval process of\ncollaboration between contexts and the KG; and 3) ToG-2 is training-free and\nplug-and-play compatible with various LLMs. Extensive experiments demonstrate\nthat ToG-2 achieves overall state-of-the-art (SOTA) performance on 6 out of 7\nknowledge-intensive datasets with GPT-3.5, and can elevate the performance of\nsmaller models (e.g., LLAMA-2-13B) to the level of GPT-3.5's direct reasoning.\nThe source code is available on https://github.com/IDEA-FinAI/ToG-2.",
      "tldr_zh": "本论文提出 Think-on-Graph 2.0 (ToG-2)，一个混合 Retrieval-augmented generation (RAG) 框架，用于提升 Large Language Models (LLMs) 在复杂推理任务中的深度和可靠性，通过知识图谱 (KGs) 引导的迭代检索来弥补知识不足。ToG-2 紧密耦合上下文检索和图检索，利用 KGs 连接文档以实现深入线索搜索，同时交替使用文档作为实体上下文进行精确图检索，实现训练-free 的可插拔兼容性。实验结果显示，ToG-2 在 7 个知识密集型数据集上使用 GPT-3.5 模型在 6 个上达到 state-of-the-art (SOTA) 性能，并能将较小模型如 LLAMA-2-13B 的表现提升至 GPT-3.5 水平。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10805v7",
      "published_date": "2024-07-15 15:20:40 UTC",
      "updated_date": "2025-02-10 03:16:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:42:33.002818"
    },
    {
      "arxiv_id": "2407.10796v1",
      "title": "Mammographic Breast Positioning Assessment via Deep Learning",
      "title_zh": "基于深度学习的乳房X线摄影定位评估",
      "authors": [
        "Toygar Tanyel",
        "Nurper Denizoglu",
        "Mustafa Ege Seker",
        "Deniz Alis",
        "Esma Cerekci",
        "Ercan Karaarslan",
        "Erkin Aribal",
        "Ilkay Oksuz"
      ],
      "abstract": "Breast cancer remains a leading cause of cancer-related deaths among women\nworldwide, with mammography screening as the most effective method for the\nearly detection. Ensuring proper positioning in mammography is critical, as\npoor positioning can lead to diagnostic errors, increased patient stress, and\nhigher costs due to recalls. Despite advancements in deep learning (DL) for\nbreast cancer diagnostics, limited focus has been given to evaluating\nmammography positioning. This paper introduces a novel DL methodology to\nquantitatively assess mammogram positioning quality, specifically in\nmediolateral oblique (MLO) views using attention and coordinate convolution\nmodules. Our method identifies key anatomical landmarks, such as the nipple and\npectoralis muscle, and automatically draws a posterior nipple line (PNL),\noffering robust and inherently explainable alternative to well-known\nclassification and regression-based approaches. We compare the performance of\nproposed methodology with various regression and classification-based models.\nThe CoordAtt UNet model achieved the highest accuracy of 88.63% $\\pm$ 2.84 and\nspecificity of 90.25% $\\pm$ 4.04, along with a noteworthy sensitivity of 86.04%\n$\\pm$ 3.41. In landmark detection, the same model also recorded the lowest mean\nerrors in key anatomical points and the smallest angular error of 2.42 degrees.\nOur results indicate that models incorporating attention mechanisms and\nCoordConv module increase the accuracy in classifying breast positioning\nquality and detecting anatomical landmarks. Furthermore, we make the labels and\nsource codes available to the community to initiate an open research area for\nmammography, accessible at https://github.com/tanyelai/deep-breast-positioning.",
      "tldr_zh": "本论文针对乳房X光摄影（mammography）中定位不当导致的诊断错误问题，提出了一种新型深度学习（deep learning）方法，用于定量评估mediolateral oblique (MLO) 视图的定位质量。该方法利用attention和coordinate convolution modules识别关键解剖标志（如乳头和pectoralis muscle），并自动绘制posterior nipple line (PNL)，提供一种稳健且可解释的评估替代方案。与回归和分类模型比较，CoordAtt UNet模型取得了最高的准确率88.63% ± 2.84%、特异性90.25% ± 4.04%和敏感性86.04% ± 3.41%，并在解剖标志检测中实现了最低的平均错误和角度错误2.42度。该研究还公开了标签和源代码（https://github.com/tanyelai/deep-breast-positioning），以促进mammography领域的开放研究。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "J.3"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10796v1",
      "published_date": "2024-07-15 15:14:10 UTC",
      "updated_date": "2024-07-15 15:14:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:42:47.511775"
    },
    {
      "arxiv_id": "2407.10794v1",
      "title": "Graphusion: Leveraging Large Language Models for Scientific Knowledge Graph Fusion and Construction in NLP Education",
      "title_zh": "翻译失败",
      "authors": [
        "Rui Yang",
        "Boming Yang",
        "Sixun Ouyang",
        "Tianwei She",
        "Aosong Feng",
        "Yuang Jiang",
        "Freddy Lecue",
        "Jinghui Lu",
        "Irene Li"
      ],
      "abstract": "Knowledge graphs (KGs) are crucial in the field of artificial intelligence\nand are widely applied in downstream tasks, such as enhancing Question\nAnswering (QA) systems. The construction of KGs typically requires significant\neffort from domain experts. Recently, Large Language Models (LLMs) have been\nused for knowledge graph construction (KGC), however, most existing approaches\nfocus on a local perspective, extracting knowledge triplets from individual\nsentences or documents. In this work, we introduce Graphusion, a zero-shot KGC\nframework from free text. The core fusion module provides a global view of\ntriplets, incorporating entity merging, conflict resolution, and novel triplet\ndiscovery. We showcase how Graphusion could be applied to the natural language\nprocessing (NLP) domain and validate it in the educational scenario.\nSpecifically, we introduce TutorQA, a new expert-verified benchmark for graph\nreasoning and QA, comprising six tasks and a total of 1,200 QA pairs. Our\nevaluation demonstrates that Graphusion surpasses supervised baselines by up to\n10% in accuracy on link prediction. Additionally, it achieves average scores of\n2.92 and 2.37 out of 3 in human evaluations for concept entity extraction and\nrelation recognition, respectively.",
      "tldr_zh": "该研究提出Graphusion，一种零样本知识图谱构建框架，利用Large Language Models (LLMs)从自由文本中提取三元组，并通过核心融合模块提供全局视角，包括实体合并、冲突解决和新三元组发现，以减少对领域专家的依赖。Graphusion应用于NLP教育场景，并引入了新的基准TutorQA，该基准包含6个任务和1200个QA对，用于图推理和问答评估。实验结果显示，Graphusion在链接预测上比监督基线提高10%的准确率，并在人类评估中分别获得概念实体提取的2.92/3和关系识别的2.37/3的高分。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "24 pages, 11 figures, 13 tables. arXiv admin note: substantial text\n  overlap with arXiv:2402.14293",
      "pdf_url": "http://arxiv.org/pdf/2407.10794v1",
      "published_date": "2024-07-15 15:13:49 UTC",
      "updated_date": "2024-07-15 15:13:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:42:58.837848"
    },
    {
      "arxiv_id": "2407.10793v1",
      "title": "GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework",
      "title_zh": "GraphEval：基于知识图谱的 LLM 幻觉评估框架",
      "authors": [
        "Hannah Sansford",
        "Nicholas Richardson",
        "Hermina Petric Maretic",
        "Juba Nait Saada"
      ],
      "abstract": "Methods to evaluate Large Language Model (LLM) responses and detect\ninconsistencies, also known as hallucinations, with respect to the provided\nknowledge, are becoming increasingly important for LLM applications. Current\nmetrics fall short in their ability to provide explainable decisions,\nsystematically check all pieces of information in the response, and are often\ntoo computationally expensive to be used in practice. We present GraphEval: a\nhallucination evaluation framework based on representing information in\nKnowledge Graph (KG) structures. Our method identifies the specific triples in\nthe KG that are prone to hallucinations and hence provides more insight into\nwhere in the response a hallucination has occurred, if at all, than previous\nmethods. Furthermore, using our approach in conjunction with state-of-the-art\nnatural language inference (NLI) models leads to an improvement in balanced\naccuracy on various hallucination benchmarks, compared to using the raw NLI\nmodels. Lastly, we explore the use of GraphEval for hallucination correction by\nleveraging the structure of the KG, a method we name GraphCorrect, and\ndemonstrate that the majority of hallucinations can indeed be rectified.",
      "tldr_zh": "该研究提出GraphEval，一种基于Knowledge Graph (KG)的框架，用于评估Large Language Model (LLM)响应的幻觉(hallucinations)，以解决现有方法在可解释性、系统检查和计算效率方面的不足。GraphEval通过将响应信息表示为KG结构，识别易于出现幻觉的三元组(triples)，从而提供更详细的幻觉定位洞察，并与自然语言推理(NLI)模型结合，在各种基准测试中提高了平衡准确率(balanced accuracy)。此外，作者探索了GraphCorrect方法，利用KG的结构对幻觉进行修正，并证明大多数幻觉可被有效矫正。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, to be published at KiL'24: Workshop on Knowledge-infused\n  Learning co-located with 30th ACM KDD Conference, August 26, 2024, Barcelona,\n  Spain",
      "pdf_url": "http://arxiv.org/pdf/2407.10793v1",
      "published_date": "2024-07-15 15:11:16 UTC",
      "updated_date": "2024-07-15 15:11:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:43:09.483925"
    },
    {
      "arxiv_id": "2407.10784v4",
      "title": "AdapTable: Test-Time Adaptation for Tabular Data via Shift-Aware Uncertainty Calibrator and Label Distribution Handler",
      "title_zh": "翻译失败",
      "authors": [
        "Changhun Kim",
        "Taewon Kim",
        "Seungyeon Woo",
        "June Yong Yang",
        "Eunho Yang"
      ],
      "abstract": "In real-world scenarios, tabular data often suffer from distribution shifts\nthat threaten the performance of machine learning models. Despite its\nprevalence and importance, handling distribution shifts in the tabular domain\nremains underexplored due to the inherent challenges within the tabular data\nitself. In this sense, test-time adaptation (TTA) offers a promising solution\nby adapting models to target data without accessing source data, crucial for\nprivacy-sensitive tabular domains. However, existing TTA methods either 1)\noverlook the nature of tabular distribution shifts, often involving label\ndistribution shifts, or 2) impose architectural constraints on the model,\nleading to a lack of applicability. To this end, we propose AdapTable, a novel\nTTA framework for tabular data. AdapTable operates in two stages: 1)\ncalibrating model predictions using a shift-aware uncertainty calibrator, and\n2) adjusting these predictions to match the target label distribution with a\nlabel distribution handler. We validate the effectiveness of AdapTable through\ntheoretical analysis and extensive experiments on various distribution shift\nscenarios. Our results demonstrate AdapTable's ability to handle various\nreal-world distribution shifts, achieving up to a 16% improvement on the HELOC\ndataset.",
      "tldr_zh": "本研究针对表格数据（tabular data）中的分布偏移问题，提出了AdapTable框架，这是一种Test-Time Adaptation (TTA)方法，用于在不访问源数据的情况下适应目标数据。\nAdapTable分为两个阶段：首先，通过Shift-Aware Uncertainty Calibrator校准模型预测以处理偏移相关的不确定性；其次，使用Label Distribution Handler调整预测以匹配目标标签分布。\n实验验证显示，该框架在各种分布偏移场景中有效，在HELOC数据集上实现了高达16%的性能提升。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS Workshop on Table Representation Learning (NeurIPSW-TRL),\n  2024",
      "pdf_url": "http://arxiv.org/pdf/2407.10784v4",
      "published_date": "2024-07-15 15:02:53 UTC",
      "updated_date": "2025-02-12 05:02:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:43:22.012748"
    },
    {
      "arxiv_id": "2407.11106v1",
      "title": "Deep Learning Evidence for Global Optimality of Gerver's Sofa",
      "title_zh": "翻译失败",
      "authors": [
        "Kuangdai Leng",
        "Jia Bi",
        "Jaehoon Cha",
        "Samuel Pinilla",
        "Jeyan Thiyagalingam"
      ],
      "abstract": "The Moving Sofa Problem, formally proposed by Leo Moser in 1966, seeks to\ndetermine the largest area of a two-dimensional shape that can navigate through\nan $L$-shaped corridor with unit width. The current best lower bound is about\n2.2195, achieved by Joseph Gerver in 1992, though its global optimality remains\nunproven. In this paper, we investigate this problem by leveraging the\nuniversal approximation strength and computational efficiency of neural\nnetworks. We report two approaches, both supporting Gerver's conjecture that\nhis shape is the unique global maximum. Our first approach is continuous\nfunction learning. We drop Gerver's assumptions that i) the rotation of the\ncorridor is monotonic and symmetric and, ii) the trajectory of its corner as a\nfunction of rotation is continuously differentiable. We parameterize rotation\nand trajectory by independent piecewise linear neural networks (with input\nbeing some pseudo time), allowing for rich movements such as backward rotation\nand pure translation. We then compute the sofa area as a differentiable\nfunction of rotation and trajectory using our \"waterfall\" algorithm. Our final\nloss function includes differential terms and initial conditions, leveraging\nthe principles of physics-informed machine learning. Under such settings,\nextensive training starting from diverse function initialization and\nhyperparameters is conducted, unexceptionally showing rapid convergence to\nGerver's solution. Our second approach is via discrete optimization of the\nKallus-Romik upper bound, which converges to the maximum sofa area from above\nas the number of rotation angles increases. We uplift this number to 10000 to\nreveal its asymptotic behavior. It turns out that the upper bound yielded by\nour models does converge to Gerver's area (within an error of 0.01% when the\nnumber of angles reaches 2100). We also improve their five-angle upper bound\nfrom 2.37 to 2.3337.",
      "tldr_zh": "这篇论文利用深度学习方法为 Moving Sofa Problem 提供证据，支持 Gerver's Sofa 形状是唯一全局最优解，该问题旨在寻找能通过单位宽度 L 形走廊的最大面积二维形状。研究者采用第一种方法，通过神经网络参数化旋转和轨迹（包括 piecewise linear neural networks），并使用“waterfall”算法结合 physics-informed machine learning 原则进行训练，结果显示模型从不同初始化快速收敛到 Gerver's 解决方案。第二种方法通过离散优化 Kallus-Romik upper bound，将旋转角度增加到 10000，观察到上界收敛至 Gerver's 面积（在 2100 个角度时误差小于 0.01%），并将五角度上界从 2.37 改进到 2.3337。这些发现为 Gerver's conjecture 提供了强有力的数值支持。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.11106v1",
      "published_date": "2024-07-15 14:59:32 UTC",
      "updated_date": "2024-07-15 14:59:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:43:47.070740"
    },
    {
      "arxiv_id": "2407.10768v5",
      "title": "ISMRNN: An Implicitly Segmented RNN Method with Mamba for Long-Term Time Series Forecasting",
      "title_zh": "ISMRNN：一种基于 Mamba 的隐式分割 RNN 方法，用于长期时间序列预测",
      "authors": [
        "GaoXiang Zhao",
        "Li Zhou",
        "XiaoQiang Wang"
      ],
      "abstract": "Long time series forecasting aims to utilize historical information to\nforecast future states over extended horizons. Traditional RNN-based series\nforecasting methods struggle to effectively address long-term dependencies and\ngradient issues in long time series problems. Recently, SegRNN has emerged as a\nleading RNN-based model tailored for long-term series forecasting,\ndemonstrating state-of-the-art performance while maintaining a streamlined\narchitecture through innovative segmentation and parallel decoding techniques.\nNevertheless, SegRNN has several limitations: its fixed segmentation disrupts\ndata continuity and fails to effectively leverage information across different\nsegments, the segmentation strategy employed by SegRNN does not fundamentally\naddress the issue of information loss within the recurrent structure. To\naddress these issues, we propose the ISMRNN method with three key enhancements:\nwe introduce an implicit segmentation structure to decompose the time series\nand map it to segmented hidden states, resulting in denser information exchange\nduring the segmentation phase. Additionally, we incorporate residual structures\nin the encoding layer to mitigate information loss within the recurrent\nstructure. To extract information more effectively, we further integrate the\nMamba architecture to enhance time series information extraction. Experiments\non several real-world long time series forecasting datasets demonstrate that\nour model surpasses the performance of current state-of-the-art models.",
      "tldr_zh": "该研究针对长时序预测（Long time series forecasting）中的长时依赖和梯度问题，提出ISMRNN方法，以改进现有RNN-based模型如SegRNN的局限性。ISMRNN引入隐式分割结构（implicit segmentation structure）来分解时序并增强段间信息交换，同时添加残差结构（residual structures）减少循环结构中的信息损失，并整合Mamba架构以提升时序信息提取。实验在多个真实数据集上表明，ISMRNN超越了当前最先进模型的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10768v5",
      "published_date": "2024-07-15 14:50:15 UTC",
      "updated_date": "2024-08-04 07:53:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:43:56.947962"
    },
    {
      "arxiv_id": "2407.21030v1",
      "title": "Cluster and Separate: a GNN Approach to Voice and Staff Prediction for Score Engraving",
      "title_zh": "翻译失败",
      "authors": [
        "Francesco Foscarin",
        "Emmanouil Karystinaios",
        "Eita Nakamura",
        "Gerhard Widmer"
      ],
      "abstract": "This paper approaches the problem of separating the notes from a quantized\nsymbolic music piece (e.g., a MIDI file) into multiple voices and staves. This\nis a fundamental part of the larger task of music score engraving (or score\ntypesetting), which aims to produce readable musical scores for human\nperformers. We focus on piano music and support homophonic voices, i.e., voices\nthat can contain chords, and cross-staff voices, which are notably difficult\ntasks that have often been overlooked in previous research. We propose an\nend-to-end system based on graph neural networks that clusters notes that\nbelong to the same chord and connects them with edges if they are part of a\nvoice. Our results show clear and consistent improvements over a previous\napproach on two datasets of different styles. To aid the qualitative analysis\nof our results, we support the export in symbolic music formats and provide a\ndirect visualization of our outputs graph over the musical score. All code and\npre-trained models are available at https://github.com/CPJKU/piano_svsep",
      "tldr_zh": "本论文针对音乐谱刻(score engraving)问题，提出了一种基于图神经网络(GNN)的端到端系统，用于将量化符号音乐（如 MIDI 文件）中的音符聚类成同音声部(homophonic voices，包括和弦）和跨谱线声部(cross-staff voices)。该系统通过聚类属于同一和弦的音符并用边连接同一声部的音符，实现对钢琴音乐的精确分离。实验结果显示，在两个不同风格的数据集上，该方法比先前方法有显著改进，并提供开源代码和输出图形可视化工具，以支持定性分析。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted at the 25th International Society for Music Information\n  Retrieval (ISMIR) 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.21030v1",
      "published_date": "2024-07-15 14:36:13 UTC",
      "updated_date": "2024-07-15 14:36:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:43:58.816110"
    },
    {
      "arxiv_id": "2407.10758v1",
      "title": "Continual Deep Learning on the Edge via Stochastic Local Competition among Subnetworks",
      "title_zh": "翻译失败",
      "authors": [
        "Theodoros Christophides",
        "Kyriakos Tolias",
        "Sotirios Chatzis"
      ],
      "abstract": "Continual learning on edge devices poses unique challenges due to stringent\nresource constraints. This paper introduces a novel method that leverages\nstochastic competition principles to promote sparsity, significantly reducing\ndeep network memory footprint and computational demand. Specifically, we\npropose deep networks that comprise blocks of units that compete locally to win\nthe representation of each arising new task; competition takes place in a\nstochastic manner. This type of network organization results in sparse\ntask-specific representations from each network layer; the sparsity pattern is\nobtained during training and is different among tasks. Crucially, our method\nsparsifies both the weights and the weight gradients, thus facilitating\ntraining on edge devices. This is performed on the grounds of winning\nprobability for each unit in a block. During inference, the network retains\nonly the winning unit and zeroes-out all weights pertaining to non-winning\nunits for the task at hand. Thus, our approach is specifically tailored for\ndeployment on edge devices, providing an efficient and scalable solution for\ncontinual learning in resource-limited environments.",
      "tldr_zh": "这篇论文提出了一种针对边缘设备上持续学习（Continual Learning）的创新方法，通过子网络间随机局部竞争（Stochastic Local Competition）来实现网络稀疏化，从而显著降低深度网络的内存占用和计算需求。方法设计了网络块，其中单位在每个新任务中进行随机竞争，以生成任务特定的稀疏表示，训练过程中稀疏化权重和权重梯度。实验结果显示，这种机制便于在资源受限环境中部署，提供高效、可扩展的持续学习解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10758v1",
      "published_date": "2024-07-15 14:36:05 UTC",
      "updated_date": "2024-07-15 14:36:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:44:11.475895"
    },
    {
      "arxiv_id": "2407.11105v1",
      "title": "Impacts of Data Preprocessing and Hyperparameter Optimization on the Performance of Machine Learning Models Applied to Intrusion Detection Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Mateus Guimarães Lima",
        "Antony Carvalho",
        "João Gabriel Álvares",
        "Clayton Escouper das Chagas",
        "Ronaldo Ribeiro Goldschmidt"
      ],
      "abstract": "In the context of cybersecurity of modern communications networks, Intrusion\nDetection Systems (IDS) have been continuously improved, many of them\nincorporating machine learning (ML) techniques to identify threats. Although\nthere are researches focused on the study of these techniques applied to IDS,\nthe state-of-the-art lacks works concentrated exclusively on the evaluation of\nthe impacts of data pre-processing actions and the optimization of the values\nof the hyperparameters of the ML algorithms in the construction of the models\nof threat identification. This article aims to present a study that fills this\nresearch gap. For that, experiments were carried out with two data sets,\ncomparing attack scenarios with variations of pre-processing techniques and\noptimization of hyperparameters. The results confirm that the proper\napplication of these techniques, in general, makes the generated classification\nmodels more robust and greatly reduces the execution times of these models'\ntraining and testing processes.",
      "tldr_zh": "本文研究了数据预处理和超参数优化对应用于入侵检测系统（IDS）的机器学习（ML）模型性能的影响，旨在填补现有文献中对这些因素的专攻评估空白。通过实验使用两个数据集，比较了不同预处理技术和超参数优化变体的攻击场景。结果显示，这些技术整体上使分类模型更鲁棒，并显著减少了模型训练和测试的执行时间。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.11105v1",
      "published_date": "2024-07-15 14:30:25 UTC",
      "updated_date": "2024-07-15 14:30:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:44:20.604222"
    },
    {
      "arxiv_id": "2407.11104v1",
      "title": "Exploring the Potentials and Challenges of Deep Generative Models in Product Design Conception",
      "title_zh": "探索深度生成模型在产品设计构思中的潜力与挑战",
      "authors": [
        "Phillip Mueller",
        "Lars Mikelsons"
      ],
      "abstract": "The synthesis of product design concepts stands at the crux of early-phase\ndevelopment processes for technical products, traditionally posing an intricate\ninterdisciplinary challenge. The application of deep learning methods,\nparticularly Deep Generative Models (DGMs), holds the promise of automating and\nstreamlining manual iterations and therefore introducing heightened levels of\ninnovation and efficiency. However, DGMs have yet to be widely adopted into the\nsynthesis of product design concepts. This paper aims to explore the reasons\nbehind this limited application and derive the requirements for successful\nintegration of these technologies. We systematically analyze DGM-families (VAE,\nGAN, Diffusion, Transformer, Radiance Field), assessing their strengths,\nweaknesses, and general applicability for product design conception. Our\nobjective is to provide insights that simplify the decision-making process for\nengineers, helping them determine which method might be most effective for\ntheir specific challenges. Recognizing the rapid evolution of this field, we\nhope that our analysis contributes to a fundamental understanding and guides\npractitioners towards the most promising approaches. This work seeks not only\nto illuminate current challenges but also to propose potential solutions,\nthereby offering a clear roadmap for leveraging DGMs in the realm of product\ndesign conception.",
      "tldr_zh": "本论文探讨了 Deep Generative Models (DGMs) 在产品设计概念合成中的潜力与挑战，旨在解决其尚未广泛采用的原因，并推导出成功集成的要求。作者系统分析了 DGM 家族，包括 VAE、GAN、Diffusion、Transformer 和 Radiance Field，评估了它们的优势、弱点及在产品设计中的适用性。研究提供决策见解，帮助工程师选择最适合特定挑战的方法，并提出潜在解决方案路线图，以提升创新效率和推动该领域的应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.11104v1",
      "published_date": "2024-07-15 14:28:50 UTC",
      "updated_date": "2024-07-15 14:28:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:44:34.976956"
    },
    {
      "arxiv_id": "2407.10743v1",
      "title": "Scaling 3D Reasoning with LMMs to Large Robot Mission Environments Using Datagraphs",
      "title_zh": "翻译失败",
      "authors": [
        "W. J. Meijer",
        "A. C. Kemmeren",
        "E. H. J. Riemens",
        "J. E. Fransman",
        "M. van Bekkum",
        "G. J. Burghouts",
        "J. D. van Mil"
      ],
      "abstract": "This paper addresses the challenge of scaling Large Multimodal Models (LMMs)\nto expansive 3D environments. Solving this open problem is especially relevant\nfor robot deployment in many first-responder scenarios, such as\nsearch-and-rescue missions that cover vast spaces. The use of LMMs in these\nsettings is currently hampered by the strict context windows that limit the\nLMM's input size. We therefore introduce a novel approach that utilizes a\ndatagraph structure, which allows the LMM to iteratively query smaller sections\nof a large environment. Using the datagraph in conjunction with graph traversal\nalgorithms, we can prioritize the most relevant locations to the query, thereby\nimproving the scalability of 3D scene language tasks. We illustrate the\ndatagraph using 3D scenes, but these can be easily substituted by other dense\nmodalities that represent the environment, such as pointclouds or Gaussian\nsplats. We demonstrate the potential to use the datagraph for two 3D scene\nlanguage task use cases, in a search-and-rescue mission example.",
      "tldr_zh": "本论文解决了将 Large Multimodal Models (LMMs) 扩展到广阔 3D 环境中的挑战，特别是针对机器人任务如搜索和救援场景。作者引入了一种新型 datagraph 结构，允许 LMMs 通过迭代查询环境的小部分，并结合 graph traversal algorithms 优先处理与查询最相关的区域，从而提升 3D 场景语言任务的可扩展性。datagraph 可以灵活应用于其他密集模式，如点云或 Gaussian splats，并在搜索和救援任务的示例中展示了其潜力。实验结果表明，该方法为 LMMs 在大型机器人任务中的部署提供了可行解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to the RSS Workshop on Semantics for Robotics: From\n  Environment Understanding and Reasoning to Safe Interaction 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.10743v1",
      "published_date": "2024-07-15 14:16:13 UTC",
      "updated_date": "2024-07-15 14:16:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:44:47.804728"
    },
    {
      "arxiv_id": "2407.10737v1",
      "title": "Aligning Neuronal Coding of Dynamic Visual Scenes with Foundation Vision Models",
      "title_zh": "将动态视觉场景的神经元编码与基础视觉模型对齐",
      "authors": [
        "Rining Wu",
        "Feixiang Zhou",
        "Ziwei Yin",
        "Jian K. Liu"
      ],
      "abstract": "Our brains represent the ever-changing environment with neurons in a highly\ndynamic fashion. The temporal features of visual pixels in dynamic natural\nscenes are entrapped in the neuronal responses of the retina. It is crucial to\nestablish the intrinsic temporal relationship between visual pixels and\nneuronal responses. Recent foundation vision models have paved an advanced way\nof understanding image pixels. Yet, neuronal coding in the brain largely lacks\na deep understanding of its alignment with pixels. Most previous studies employ\nstatic images or artificial videos derived from static images for emulating\nmore real and complicated stimuli. Despite these simple scenarios effectively\nhelp to separate key factors influencing visual coding, complex temporal\nrelationships receive no consideration. To decompose the temporal features of\nvisual coding in natural scenes, here we propose Vi-ST, a spatiotemporal\nconvolutional neural network fed with a self-supervised Vision Transformer\n(ViT) prior, aimed at unraveling the temporal-based encoding patterns of\nretinal neuronal populations. The model demonstrates robust predictive\nperformance in generalization tests. Furthermore, through detailed ablation\nexperiments, we demonstrate the significance of each temporal module.\nFurthermore, we introduce a visual coding evaluation metric designed to\nintegrate temporal considerations and compare the impact of different numbers\nof neuronal populations on complementary coding. In conclusion, our proposed\nVi-ST demonstrates a novel modeling framework for neuronal coding of dynamic\nvisual scenes in the brain, effectively aligning our brain representation of\nvideo with neuronal activity. The code is available at\nhttps://github.com/wurining/Vi-ST.",
      "tldr_zh": "该研究探讨了大脑神经元如何动态编码动态视觉场景，并将其与基础视觉模型对齐，解决现有模型忽略复杂时间关系的局限。作者提出 Vi-ST，一种基于自监督 Vision Transformer (ViT) 先验的时空卷积神经网络（spatiotemporal convolutional neural network），用于揭示视网膜神经元群体的基于时间编码模式。实验结果显示，Vi-ST 在泛化测试中表现出鲁棒的预测性能，并通过消融实验验证了各时间模块的重要性，同时引入了一个整合时间因素的视觉编码评估指标。总之，该框架为理解大脑对视频的表示与神经元活动之间的对齐提供了新颖的建模方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This article is accepted by ECCV 2024, which ID is 12149. Accepted\n  papers' id can be found in:\n  https://eccv2024.ecva.net/Conferences/2024/AcceptedPapers",
      "pdf_url": "http://arxiv.org/pdf/2407.10737v1",
      "published_date": "2024-07-15 14:06:13 UTC",
      "updated_date": "2024-07-15 14:06:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:45:08.672904"
    },
    {
      "arxiv_id": "2407.10735v2",
      "title": "Transforming Agency. On the mode of existence of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xabier E. Barandiaran",
        "Lola S. Almendros"
      ],
      "abstract": "This paper investigates the ontological characterization of Large Language\nModels (LLMs) like ChatGPT. Between inflationary and deflationary accounts, we\npay special attention to their status as agents. This requires explaining in\ndetail the architecture, processing, and training procedures that enable LLMs\nto display their capacities, and the extensions used to turn LLMs into\nagent-like systems. After a systematic analysis we conclude that a LLM fails to\nmeet necessary and sufficient conditions for autonomous agency in the light of\nembodied theories of mind: the individuality condition (it is not the product\nof its own activity, it is not even directly affected by it), the normativity\ncondition (it does not generate its own norms or goals), and, partially the\ninteractional asymmetry condition (it is not the origin and sustained source of\nits interaction with the environment). If not agents, then ... what are LLMs?\nWe argue that ChatGPT should be characterized as an interlocutor or linguistic\nautomaton, a library-that-talks, devoid of (autonomous) agency, but capable to\nengage performatively on non-purposeful yet purpose-structured and\npurpose-bounded tasks. When interacting with humans, a \"ghostly\" component of\nthe human-machine interaction makes it possible to enact genuine conversational\nexperiences with LLMs. Despite their lack of sensorimotor and biological\nembodiment, LLMs textual embodiment (the training corpus) and resource-hungry\ncomputational embodiment, significantly transform existing forms of human\nagency. Beyond assisted and extended agency, the LLM-human coupling can produce\nmidtended forms of agency, closer to the production of intentional agency than\nto the extended instrumentality of any previous technologies.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 如 ChatGPT 的本体论特征，特别关注它们是否具备代理 (agency) 身份。作者通过分析 LLMs 的架构、处理机制、训练过程以及将其转化为代理系统的扩展，运用具身理论 (embodied theories of mind) 来评估其是否满足自主代理的必要条件，包括个体性、规范性和互动不对称性。研究结论认为，LLMs 不符合这些条件，仅能被视为对话者或“会说话的图书馆”，尽管它们能在非目的性任务中进行结构化互动。论文进一步指出，LLMs 与人类的互动会转变人类代理形式，产生介于辅助代理和意图代理之间的中间形态。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10735v2",
      "published_date": "2024-07-15 14:01:35 UTC",
      "updated_date": "2024-07-16 09:53:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:45:11.831255"
    },
    {
      "arxiv_id": "2407.10736v1",
      "title": "When Synthetic Traces Hide Real Content: Analysis of Stable Diffusion Image Laundering",
      "title_zh": "当合成痕迹隐藏真实内容时：Stable Diffusion 图像清洗的分析",
      "authors": [
        "Sara Mandelli",
        "Paolo Bestagini",
        "Stefano Tubaro"
      ],
      "abstract": "In recent years, methods for producing highly realistic synthetic images have\nsignificantly advanced, allowing the creation of high-quality images from text\nprompts that describe the desired content. Even more impressively, Stable\nDiffusion (SD) models now provide users with the option of creating synthetic\nimages in an image-to-image translation fashion, modifying images in the latent\nspace of advanced autoencoders. This striking evolution, however, brings an\nalarming consequence: it is possible to pass an image through SD autoencoders\nto reproduce a synthetic copy of the image with high realism and almost no\nvisual artifacts. This process, known as SD image laundering, can transform\nreal images into lookalike synthetic ones and risks complicating forensic\nanalysis for content authenticity verification. Our paper investigates the\nforensic implications of image laundering, revealing a serious potential to\nobscure traces of real content, including sensitive and harmful materials that\ncould be mistakenly classified as synthetic, thereby undermining the protection\nof individuals depicted. To address this issue, we propose a two-stage\ndetection pipeline that effectively differentiates between pristine, laundered,\nand fully synthetic images (those generated from text prompts), showing\nrobustness across various conditions. Finally, we highlight another alarming\nproperty of image laundering, which appears to mask the unique artifacts\nexploited by forensic detectors to solve the camera model identification task,\nstrongly undermining their performance. Our experimental code is available at\nhttps://github.com/polimi-ispl/synthetic-image-detection.",
      "tldr_zh": "这项研究分析了 Stable Diffusion (SD) 模型的 image laundering 现象，即通过图像处理将真实图像转化为高度真实的合成图像，这可能隐藏真实内容并干扰内容真实性验证的取证分析。\n作者调查了其对敏感材料的潜在风险，并提出了一种两阶段检测管道，能够有效区分原始、laundered 和完全合成的图像，并在各种条件下表现出鲁棒性。\n此外，研究发现 image laundering 还能掩盖 forensic detectors 用于相机模型识别的独特痕迹，从而显著降低其性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10736v1",
      "published_date": "2024-07-15 14:01:35 UTC",
      "updated_date": "2024-07-15 14:01:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:45:25.340740"
    },
    {
      "arxiv_id": "2407.10734v2",
      "title": "On-Device Training of Fully Quantized Deep Neural Networks on Cortex-M Microcontrollers",
      "title_zh": "翻译失败",
      "authors": [
        "Mark Deutel",
        "Frank Hannig",
        "Christopher Mutschler",
        "Jürgen Teich"
      ],
      "abstract": "On-device training of DNNs allows models to adapt and fine-tune to newly\ncollected data or changing domains while deployed on microcontroller units\n(MCUs). However, DNN training is a resource-intensive task, making the\nimplementation and execution of DNN training algorithms on MCUs challenging due\nto low processor speeds, constrained throughput, limited floating-point\nsupport, and memory constraints. In this work, we explore on-device training of\nDNNs for Cortex-M MCUs. We present a method that enables efficient training of\nDNNs completely in place on the MCU using fully quantized training (FQT) and\ndynamic partial gradient updates. We demonstrate the feasibility of our\napproach on multiple vision and time-series datasets and provide insights into\nthe tradeoff between training accuracy, memory overhead, energy, and latency on\nreal hardware.",
      "tldr_zh": "该研究探讨了在Cortex-M Microcontrollers上进行DNNs的设备端训练，以适应新数据或变化的领域。作者提出了一种方法，使用Fully Quantized Training (FQT)和动态部分梯度更新，实现DNNs在MCU上完全原地高效训练。实验在多个视觉和时间序列数据集上验证了这一方法的 feasibility，并分析了训练准确性、内存开销、能量和延迟之间的权衡。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.10734v2",
      "published_date": "2024-07-15 14:01:34 UTC",
      "updated_date": "2024-08-28 15:36:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:45:34.921654"
    },
    {
      "arxiv_id": "2407.10725v1",
      "title": "CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated Responses",
      "title_zh": "翻译失败",
      "authors": [
        "Jing Yao",
        "Xiaoyuan Yi",
        "Xing Xie"
      ],
      "abstract": "The rapid progress in Large Language Models (LLMs) poses potential risks such\nas generating unethical content. Assessing LLMs' values can help expose their\nmisalignment, but relies on reference-free evaluators, e.g., fine-tuned LLMs or\nclose-source ones like GPT-4, to identify values reflected in generated\nresponses. Nevertheless, these evaluators face two challenges in open-ended\nvalue evaluation: they should align with changing human value definitions with\nminimal annotation, against their own bias (adaptability), and detect varying\nvalue expressions and scenarios robustly (generalizability). To handle these\nchallenges, we introduce CLAVE, a novel framework which integrates two\ncomplementary LLMs, a large one to extract high-level value concepts from a few\nhuman labels, leveraging its extensive knowledge and generalizability, and a\nsmaller one fine-tuned on such concepts to better align with human value\nunderstanding. This dual-model approach enables calibration with any value\nsystems using <100 human-labeled samples per value type. Then we present\nValEval, a comprehensive dataset comprising 13k+ (text,value,label) tuples\nacross diverse domains, covering three major value systems. We benchmark the\ncapabilities of 12+ popular LLM evaluators and analyze their strengths and\nweaknesses. Our findings reveal that combining fine-tuned small models and\nprompt-based large ones serves as a superior balance in value evaluation.",
      "tldr_zh": "该研究针对大型语言模型(LLMs)生成响应的价值观评估问题，提出了CLAVE框架，以解决现有评估器在适应性（与人类价值观对齐）和泛化性（处理多样场景）方面的挑战。CLAVE通过整合一个大型LLM提取高层价值观概念和一个微调的小型LLM进行校准，仅需少于100个人类标签样本即可适应不同价值系统。研究构建了ValEval数据集，包含超过13k的(text, value, label)元组，覆盖多种领域和三大价值系统，并对12+LLM评估器进行了基准测试。结果表明，结合微调小模型和基于提示的大型模型在价值评估中提供了更优的平衡。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10725v1",
      "published_date": "2024-07-15 13:51:37 UTC",
      "updated_date": "2024-07-15 13:51:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:45:49.801726"
    },
    {
      "arxiv_id": "2407.10718v2",
      "title": "Sibyl: Simple yet Effective Agent Framework for Complex Real-world Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Yulong Wang",
        "Tianhao Shen",
        "Lifeng Liu",
        "Jian Xie"
      ],
      "abstract": "Existing agents based on large language models (LLMs) demonstrate robust\nproblem-solving capabilities by integrating LLMs' inherent knowledge, strong\nin-context learning and zero-shot capabilities, and the use of tools combined\nwith intricately designed LLM invocation workflows by humans. However, these\nagents still exhibit shortcomings in long-term reasoning and under-use the\npotential of existing tools, leading to noticeable deficiencies in complex\nreal-world reasoning scenarios. To address these limitations, we introduce\nSibyl, a simple yet powerful LLM-based agent framework designed to tackle\ncomplex reasoning tasks by efficiently leveraging a minimal set of tools.\nDrawing inspiration from Global Workspace Theory, Sibyl incorporates a global\nworkspace to enhance the management and sharing of knowledge and conversation\nhistory throughout the system. Furthermore, guided by Society of Mind Theory,\nSibyl implements a multi-agent debate-based jury to self-refine the final\nanswers, ensuring a comprehensive and balanced approach. This approach aims to\nreduce system complexity while expanding the scope of problems solvable-from\nmatters typically resolved by humans in minutes to those requiring hours or\neven days, thus facilitating a shift from System-1 to System-2 thinking. Sibyl\nhas been designed with a focus on scalability and ease of debugging by\nincorporating the concept of reentrancy from functional programming from its\ninception, with the aim of seamless and low effort integration in other LLM\napplications to improve capabilities. Our experimental results on the GAIA\nbenchmark test set reveal that the Sibyl agent instantiated with GPT-4 achieves\nstate-of-the-art performance with an average score of 34.55%, compared to other\nagents based on GPT-4. We hope that Sibyl can inspire more reliable and\nreusable LLM-based agent solutions to address complex real-world reasoning\ntasks.",
      "tldr_zh": "该论文提出 Sibyl，一种简单而有效的基于 LLMs 的代理框架，旨在解决现有代理在长期推理和工具利用方面的不足，以更好地处理复杂真实世界推理任务。Sibyl 借鉴 Global Workspace Theory 引入全局工作区来管理知识和对话历史，并受 Society of Mind Theory 启发，采用多代理辩论机制来自我完善最终答案，从而实现从 System-1 到 System-2 思考的转变。框架设计强调可扩展性和易调试性，通过 reentrancy 概念便于与其他 LLM 应用集成。在 GAIA benchmark 测试中，使用 GPT-4 的 Sibyl 达到了 34.55% 的最先进性能，展现了其在复杂任务中的潜力。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Our code is available at https://github.com/Ag2S1/Sibyl-System",
      "pdf_url": "http://arxiv.org/pdf/2407.10718v2",
      "published_date": "2024-07-15 13:45:40 UTC",
      "updated_date": "2024-07-16 14:16:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:46:00.574869"
    },
    {
      "arxiv_id": "2408.00768v1",
      "title": "Comparing Optical Flow and Deep Learning to Enable Computationally Efficient Traffic Event Detection with Space-Filling Curves",
      "title_zh": "比较光流和深度学习以实现使用空间",
      "authors": [
        "Tayssir Bouraffa",
        "Elias Kjellberg Carlson",
        "Erik Wessman",
        "Ali Nouri",
        "Pierre Lamart",
        "Christian Berger"
      ],
      "abstract": "Gathering data and identifying events in various traffic situations remains\nan essential challenge for the systematic evaluation of a perception system's\nperformance. Analyzing large-scale, typically unstructured, multi-modal, time\nseries data obtained from video, radar, and LiDAR is computationally demanding,\nparticularly when meta-information or annotations are missing. We compare\nOptical Flow (OF) and Deep Learning (DL) to feed computationally efficient\nevent detection via space-filling curves on video data from a forward-facing,\nin-vehicle camera. Our first approach leverages unexpected disturbances in the\nOF field from vehicle surroundings; the second approach is a DL model trained\non human visual attention to predict a driver's gaze to spot potential event\nlocations. We feed these results to a space-filling curve to reduce\ndimensionality and achieve computationally efficient event retrieval. We\nsystematically evaluate our concept by obtaining characteristic patterns for\nboth approaches from a large-scale virtual dataset (SMIRK) and applied our\nfindings to the Zenseact Open Dataset (ZOD), a large multi-modal, real-world\ndataset, collected over two years in 14 different European countries. Our\nresults yield that the OF approach excels in specificity and reduces false\npositives, while the DL approach demonstrates superior sensitivity. Both\napproaches offer comparable processing speed, making them suitable for\nreal-time applications.",
      "tldr_zh": "这篇论文比较了 Optical Flow (OF) 和 Deep Learning (DL) 方法，以实现基于 Space-Filling Curves 的计算高效交通事件检测。OF 方法通过分析车辆周围流动场中的意外干扰来识别事件，而 DL 方法则基于人类视觉注意力训练模型预测驾驶员注视点，以定位潜在事件。实验在大型虚拟数据集 SMIRK 和真实多模态数据集 ZOD 上进行，结果显示 OF 方法在特异性上更强、减少假阳性，而 DL 方法在敏感性上更优；两者处理速度相当，适合实时应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "27th IEEE International Conference on Intelligent Transportation\n  Systems (IEEE ITSC 2024)",
      "pdf_url": "http://arxiv.org/pdf/2408.00768v1",
      "published_date": "2024-07-15 13:44:52 UTC",
      "updated_date": "2024-07-15 13:44:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:46:11.355961"
    },
    {
      "arxiv_id": "2407.10714v1",
      "title": "SEMINAR: Search Enhanced Multi-modal Interest Network and Approximate Retrieval for Lifelong Sequential Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Kaiming Shen",
        "Xichen Ding",
        "Zixiang Zheng",
        "Yuqi Gong",
        "Qianqian Li",
        "Zhongyi Liu",
        "Guannan Zhang"
      ],
      "abstract": "The modeling of users' behaviors is crucial in modern recommendation systems.\nA lot of research focuses on modeling users' lifelong sequences, which can be\nextremely long and sometimes exceed thousands of items. These models use the\ntarget item to search for the most relevant items from the historical sequence.\nHowever, training lifelong sequences in click through rate (CTR) prediction or\npersonalized search ranking (PSR) is extremely difficult due to the\ninsufficient learning problem of ID embedding, especially when the IDs in the\nlifelong sequence features do not exist in the samples of training dataset.\nAdditionally, existing target attention mechanisms struggle to learn the\nmulti-modal representations of items in the sequence well. The distribution of\nmulti-modal embedding (text, image and attributes) output of user's interacted\nitems are not properly aligned and there exist divergence across modalities. We\nalso observe that users' search query sequences and item browsing sequences can\nfully depict users' intents and benefit from each other. To address these\nchallenges, we propose a unified lifelong multi-modal sequence model called\nSEMINAR-Search Enhanced Multi-Modal Interest Network and Approximate Retrieval.\nSpecifically, a network called Pretraining Search Unit (PSU) learns the\nlifelong sequences of multi-modal query-item pairs in a pretraining-finetuning\nmanner with multiple objectives: multi-modal alignment, next query-item pair\nprediction, query-item relevance prediction, etc. After pretraining, the\ndownstream model restores the pretrained embedding as initialization and\nfinetunes the network. To accelerate the online retrieval speed of multi-modal\nembedding, we propose a multi-modal codebook-based product quantization\nstrategy to approximate the exact attention calculati",
      "tldr_zh": "该研究针对推荐系统中用户终身序列建模的挑战（如序列过长、ID 嵌入学习不足以及多模态表示不齐全），提出了一种统一的模型SEMINAR（Search Enhanced Multi-modal Interest Network and Approximate Retrieval）。具体而言，SEMINAR 引入Pretraining Search Unit (PSU)网络，通过预训练-微调方式学习多模态查询-物品对序列，实现多模态对齐、次查询-物品对预测以及查询-物品相关性预测等多重目标。论文还采用多模态代码本-based 产品量化策略来加速在线检索，显著提升了click through rate (CTR)预测和personalized search ranking (PSR)的效率和性能。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "9 pages,code released",
      "pdf_url": "http://arxiv.org/pdf/2407.10714v1",
      "published_date": "2024-07-15 13:33:30 UTC",
      "updated_date": "2024-07-15 13:33:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:46:24.208920"
    },
    {
      "arxiv_id": "2407.10689v5",
      "title": "Classification of Heart Sounds Using Multi-Branch Deep Convolutional Network and LSTM-CNN",
      "title_zh": "翻译失败",
      "authors": [
        "Seyed Amir Latifi",
        "Hassan Ghassemian",
        "Maryam Imani"
      ],
      "abstract": "This paper presents a fast and cost-effective method for diagnosing cardiac\nabnormalities with high accuracy and reliability using low-cost systems in\nclinics. The primary limitation of automatic diagnosing of cardiac diseases is\nthe rarity of correct and acceptable labeled samples, which can be expensive to\nprepare. To address this issue, two methods are proposed in this work. The\nfirst method is a unique Multi-Branch Deep Convolutional Neural Network (MBDCN)\narchitecture inspired by human auditory processing, specifically designed to\noptimize feature extraction by employing various sizes of convolutional filters\nand audio signal power spectrum as input. In the second method, called as Long\nshort-term memory-Convolutional Neural (LSCN) model, Additionally, the network\narchitecture includes Long Short-Term Memory (LSTM) network blocks to improve\nfeature extraction in the time domain. The innovative approach of combining\nmultiple parallel branches consisting of the one-dimensional convolutional\nlayers along with LSTM blocks helps in achieving superior results in audio\nsignal processing tasks. The experimental results demonstrate superiority of\nthe proposed methods over the state-of-the-art techniques. The overall\nclassification accuracy of heart sounds with the LSCN network is more than 96%.\nThe efficiency of this network is significant compared to common feature\nextraction methods such as Mel Frequency Cepstral Coefficients (MFCC) and\nwavelet transform. Therefore, the proposed method shows promising results in\nthe automatic analysis of heart sounds and has potential applications in the\ndiagnosis and early detection of cardiovascular diseases.",
      "tldr_zh": "这篇论文提出了一种快速、低成本的心音分类方法，用于高精度诊断心脏异常，针对标注样本稀缺的问题开发了两种创新模型：Multi-Branch Deep Convolutional Network (MBDCN)和Long Short-Term Memory-Convolutional Neural (LSCN)。MBDCN受人类听觉处理启发，使用不同大小的卷积过滤器和音频信号功率谱优化特征提取，而LSCN则结合LSTM网络块和一维卷积层来增强时域特征处理。实验结果显示，LSCN模型的心音分类准确率超过96%，显著优于现有技术和常见特征提取方法如Mel Frequency Cepstral Coefficients (MFCC)和小波变换。该方法具有广阔应用前景，可用于心脏病诊断和早期检测。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "eess.SP",
      "comment": "22 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.10689v5",
      "published_date": "2024-07-15 13:02:54 UTC",
      "updated_date": "2024-11-21 17:32:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:46:37.121878"
    },
    {
      "arxiv_id": "2407.10683v1",
      "title": "Addressing Image Hallucination in Text-to-Image Generation through Factual Image Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Youngsun Lim",
        "Hyunjung Shim"
      ],
      "abstract": "Text-to-image generation has shown remarkable progress with the emergence of\ndiffusion models. However, these models often generate factually inconsistent\nimages, failing to accurately reflect the factual information and common sense\nconveyed by the input text prompts. We refer to this issue as Image\nhallucination. Drawing from studies on hallucinations in language models, we\nclassify this problem into three types and propose a methodology that uses\nfactual images retrieved from external sources to generate realistic images.\nDepending on the nature of the hallucination, we employ off-the-shelf image\nediting tools, either InstructPix2Pix or IP-Adapter, to leverage factual\ninformation from the retrieved image. This approach enables the generation of\nimages that accurately reflect the facts and common sense.",
      "tldr_zh": "文本到图像生成模型（如 diffusion models）常出现图像幻觉（Image hallucination），导致生成的图像与输入文本的事实和常识不一致。论文将图像幻觉分类为三种类型，并提出一种通过事实图像检索（Factual Image Retrieval）从外部来源获取真实图像的方法来解决此问题。该方法根据幻觉性质，使用现成工具如 InstructPix2Pix 或 IP-Adapter 来整合检索图像的信息，从而生成更准确反映事实和常识的图像。总的来说，此方法为提高文本到图像生成的可靠性提供了有效途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper has been accepted for oral presentation at the IJCAI 2024\n  Workshop on Trustworthy Interactive Decision-Making with Foundation Models",
      "pdf_url": "http://arxiv.org/pdf/2407.10683v1",
      "published_date": "2024-07-15 12:59:03 UTC",
      "updated_date": "2024-07-15 12:59:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:46:49.948690"
    },
    {
      "arxiv_id": "2407.10671v4",
      "title": "Qwen2 Technical Report",
      "title_zh": "翻译失败",
      "authors": [
        "An Yang",
        "Baosong Yang",
        "Binyuan Hui",
        "Bo Zheng",
        "Bowen Yu",
        "Chang Zhou",
        "Chengpeng Li",
        "Chengyuan Li",
        "Dayiheng Liu",
        "Fei Huang",
        "Guanting Dong",
        "Haoran Wei",
        "Huan Lin",
        "Jialong Tang",
        "Jialin Wang",
        "Jian Yang",
        "Jianhong Tu",
        "Jianwei Zhang",
        "Jianxin Ma",
        "Jianxin Yang",
        "Jin Xu",
        "Jingren Zhou",
        "Jinze Bai",
        "Jinzheng He",
        "Junyang Lin",
        "Kai Dang",
        "Keming Lu",
        "Keqin Chen",
        "Kexin Yang",
        "Mei Li",
        "Mingfeng Xue",
        "Na Ni",
        "Pei Zhang",
        "Peng Wang",
        "Ru Peng",
        "Rui Men",
        "Ruize Gao",
        "Runji Lin",
        "Shijie Wang",
        "Shuai Bai",
        "Sinan Tan",
        "Tianhang Zhu",
        "Tianhao Li",
        "Tianyu Liu",
        "Wenbin Ge",
        "Xiaodong Deng",
        "Xiaohuan Zhou",
        "Xingzhang Ren",
        "Xinyu Zhang",
        "Xipin Wei",
        "Xuancheng Ren",
        "Xuejing Liu",
        "Yang Fan",
        "Yang Yao",
        "Yichang Zhang",
        "Yu Wan",
        "Yunfei Chu",
        "Yuqiong Liu",
        "Zeyu Cui",
        "Zhenru Zhang",
        "Zhifang Guo",
        "Zhihao Fan"
      ],
      "abstract": "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.",
      "tldr_zh": "本报告介绍了 Qwen2 系列模型，包括大语言模型和多模态模型，涵盖从 0.5 亿到 72 亿参数的 dense models 和 Mixture-of-Experts 模型，这些模型在语言理解、生成、多语言、编码、数学和推理 benchmark 上超越了先前开源模型如 Qwen1.5，并与专有模型竞争。旗舰模型 Qwen2-72B 作为基础语言模型在 MMLU 上达到 84.2、GPQA 37.9、HumanEval 64.6、GSM8K 89.5 和 BBH 82.4 的出色表现，而指令调整版本 Qwen2-72B-Instruct 在 MT-Bench、Arena-Hard 和 LiveCodeBench 上分别获得 9.1、48.1 和 35.7 分。Qwen2 支持约 30 种语言，包括英语、中文、西班牙语等，并已开源模型权重至 Hugging Face 和 ModelScope，并提供量化、微调和部署资源，以促进社区创新和应用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "26 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2407.10671v4",
      "published_date": "2024-07-15 12:35:42 UTC",
      "updated_date": "2024-09-10 13:25:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:47:03.632873"
    },
    {
      "arxiv_id": "2407.10670v1",
      "title": "Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Yunxiao Shi",
        "Xing Zi",
        "Zijing Shi",
        "Haimin Zhang",
        "Qiang Wu",
        "Min Xu"
      ],
      "abstract": "Retrieval-augmented generation (RAG) techniques leverage the in-context\nlearning capabilities of large language models (LLMs) to produce more accurate\nand relevant responses. Originating from the simple 'retrieve-then-read'\napproach, the RAG framework has evolved into a highly flexible and modular\nparadigm. A critical component, the Query Rewriter module, enhances knowledge\nretrieval by generating a search-friendly query. This method aligns input\nquestions more closely with the knowledge base. Our research identifies\nopportunities to enhance the Query Rewriter module to Query Rewriter+ by\ngenerating multiple queries to overcome the Information Plateaus associated\nwith a single query and by rewriting questions to eliminate Ambiguity, thereby\nclarifying the underlying intent. We also find that current RAG systems exhibit\nissues with Irrelevant Knowledge; to overcome this, we propose the Knowledge\nFilter. These two modules are both based on the instruction-tuned Gemma-2B\nmodel, which together enhance response quality. The final identified issue is\nRedundant Retrieval; we introduce the Memory Knowledge Reservoir and the\nRetriever Trigger to solve this. The former supports the dynamic expansion of\nthe RAG system's knowledge base in a parameter-free manner, while the latter\noptimizes the cost for accessing external knowledge, thereby improving resource\nutilization and response efficiency. These four RAG modules synergistically\nimprove the response quality and efficiency of the RAG system. The\neffectiveness of these modules has been validated through experiments and\nablation studies across six common QA datasets. The source code can be accessed\nat https://github.com/Ancientshi/ERM4.",
      "tldr_zh": "该论文提出了一种四模块协同框架，用于提升RAG（Retrieval-augmented generation）系统的响应质量和效率。研究扩展了Query Rewriter模块为Query Rewriter+，通过生成多个查询消除Information Plateaus和Ambiguity问题，并引入Knowledge Filter基于Gemma-2B模型过滤无关知识；同时，Memory Knowledge Reservoir实现无参数动态扩展知识库，而Retriever Trigger优化外部知识访问成本以解决Redundant Retrieval。实验结果显示，该框架在六个常见QA数据集上显著改善性能，并通过消融研究验证了各模块的有效性。源代码可从https://github.com/Ancientshi/ERM4获取。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ECAI2024 #1304",
      "pdf_url": "http://arxiv.org/pdf/2407.10670v1",
      "published_date": "2024-07-15 12:35:00 UTC",
      "updated_date": "2024-07-15 12:35:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:47:13.577904"
    },
    {
      "arxiv_id": "2407.10663v1",
      "title": "Spatio-temporal neural distance fields for conditional generative modeling of the heart",
      "title_zh": "时空神经距离场用于心脏的条件生成建模",
      "authors": [
        "Kristine Sørensen",
        "Paula Diez",
        "Jan Margeta",
        "Yasmin El Youssef",
        "Michael Pham",
        "Jonas Jalili Pedersen",
        "Tobias Kühl",
        "Ole de Backer",
        "Klaus Kofoed",
        "Oscar Camara",
        "Rasmus Paulsen"
      ],
      "abstract": "The rhythmic pumping motion of the heart stands as a cornerstone in life, as\nit circulates blood to the entire human body through a series of carefully\ntimed contractions of the individual chambers. Changes in the size, shape and\nmovement of the chambers can be important markers for cardiac disease and\nmodeling this in relation to clinical demography or disease is therefore of\ninterest. Existing methods for spatio-temporal modeling of the human heart\nrequire shape correspondence over time or suffer from large memory\nrequirements, making it difficult to use for complex anatomies. We introduce a\nnovel conditional generative model, where the shape and movement is modeled\nimplicitly in the form of a spatio-temporal neural distance field and\nconditioned on clinical demography. The model is based on an auto-decoder\narchitecture and aims to disentangle the individual variations from that\nrelated to the clinical demography. It is tested on the left atrium (including\nthe left atrial appendage), where it outperforms current state-of-the-art\nmethods for anatomical sequence completion and generates synthetic sequences\nthat realistically mimics the shape and motion of the real left atrium. In\npractice, this means we can infer functional measurements from a static image,\ngenerate synthetic populations with specified demography or disease and\ninvestigate how non-imaging clinical data effect the shape and motion of\ncardiac anatomies.",
      "tldr_zh": "该论文提出了一种基于时空神经距离场（spatio-temporal neural distance fields）的条件生成模型，用于心脏形状和运动的建模，以探索这些变化与临床人口统计或疾病的关系。模型采用 auto-decoder 架构，隐式处理时空数据并分离个体变异与临床因素相关变异，从而克服现有方法在形状对应和内存需求上的局限。实验结果显示，该模型在左心房（包括左心耳）的序列完成任务中优于最先进方法，并能生成真实模拟的合成序列。实际应用包括从静态图像推断心脏功能指标、创建指定人口统计或疾病的合成人群，以及分析非图像临床数据对心脏解剖结构的影响。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for MICCAI2024",
      "pdf_url": "http://arxiv.org/pdf/2407.10663v1",
      "published_date": "2024-07-15 12:26:52 UTC",
      "updated_date": "2024-07-15 12:26:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:47:26.954839"
    },
    {
      "arxiv_id": "2407.10662v4",
      "title": "XEQ Scale for Evaluating XAI Experience Quality",
      "title_zh": "翻译失败",
      "authors": [
        "Anjana Wijekoon",
        "Nirmalie Wiratunga",
        "David Corsar",
        "Kyle Martin",
        "Ikechukwu Nkisi-Orji",
        "Belen Díaz-Agudo",
        "Derek Bridge"
      ],
      "abstract": "Explainable Artificial Intelligence (XAI) aims to improve the transparency of\nautonomous decision-making through explanations. Recent literature has\nemphasised users' need for holistic \"multi-shot\" explanations and personalised\nengagement with XAI systems. We refer to this user-centred interaction as an\nXAI Experience. Despite advances in creating XAI experiences, evaluating them\nin a user-centred manner has remained challenging. In response, we developed\nthe XAI Experience Quality (XEQ) Scale. XEQ quantifies the quality of\nexperiences across four dimensions: learning, utility, fulfilment and\nengagement. These contributions extend the state-of-the-art of XAI evaluation,\nmoving beyond the one-dimensional metrics frequently developed to assess\nsingle-shot explanations. This paper presents the XEQ scale development and\nvalidation process, including content validation with XAI experts, and\ndiscriminant and construct validation through a large-scale pilot study. Our\npilot study results offer strong evidence that establishes the XEQ Scale as a\ncomprehensive framework for evaluating user-centred XAI experiences.",
      "tldr_zh": "本研究开发了 XEQ Scale，一种用于评估可解释人工智能 (XAI) 体验质量的工具，旨在解决用户对整体“多重”解释和个性化互动的需求。XEQ 基于四个维度——learning、utility、fulfilment 和 engagement——来量化 XAI 体验的全面性，超越了传统的单维评估指标。论文详细描述了 XEQ 的开发过程，包括专家内容验证和大规模试点研究的区分及结构验证，结果证明 XEQ 是一个可靠的框架，用于提升用户中心的 XAI 评估。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10662v4",
      "published_date": "2024-07-15 12:25:49 UTC",
      "updated_date": "2025-01-17 11:02:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:47:38.469747"
    },
    {
      "arxiv_id": "2407.10657v3",
      "title": "An Empirical Study of Validating Synthetic Data for Formula Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Usneek Singh",
        "José Cambronero",
        "Sumit Gulwani",
        "Aditya Kanade",
        "Anirudh Khatry",
        "Vu Le",
        "Mukul Singh",
        "Gust Verbruggen"
      ],
      "abstract": "Large language models (LLMs) can be leveraged to help with writing formulas\nin spreadsheets, but resources on these formulas are scarce, impacting both the\nbase performance of pre-trained models and limiting the ability to fine-tune\nthem. Given a corpus of formulas, we can use a(nother) model to generate\nsynthetic natural language utterances for fine-tuning. However, it is important\nto validate whether the NL generated by the LLM is indeed accurate to be\nbeneficial for fine-tuning. In this paper, we provide empirical results on the\nimpact of validating these synthetic training examples with surrogate\nobjectives that evaluate the accuracy of the synthetic annotations. We\ndemonstrate that validation improves performance over raw data across four\nmodels (2 open and 2 closed weight). Interestingly, we show that although\nvalidation tends to prune more challenging examples, it increases the\ncomplexity of problems that models can solve after being fine-tuned on\nvalidated data.",
      "tldr_zh": "本研究实证探讨了验证合成数据对电子表格公式生成的益处，针对 LLMs 在公式编写中的性能问题，通过使用另一个模型生成合成自然语言（NL）语句并采用替代目标（surrogate objectives）评估其准确性。实验结果显示，验证合成训练示例显著提高了四个模型（2 个开源和 2 个闭源）的微调性能。值得注意的是，虽然验证过程会去除更多挑战性示例，但使用验证数据微调后，模型能够处理更复杂的问题。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10657v3",
      "published_date": "2024-07-15 12:16:33 UTC",
      "updated_date": "2024-11-03 12:44:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:47:49.682867"
    },
    {
      "arxiv_id": "2407.10645v2",
      "title": "Prompt Selection Matters: Enhancing Text Annotations for Social Sciences with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Louis Abraham",
        "Charles Arnal",
        "Antoine Marie"
      ],
      "abstract": "Large Language Models have recently been applied to text annotation tasks\nfrom social sciences, equalling or surpassing the performance of human workers\nat a fraction of the cost. However, no inquiry has yet been made on the impact\nof prompt selection on labelling accuracy. In this study, we show that\nperformance greatly varies between prompts, and we apply the method of\nautomatic prompt optimization to systematically craft high quality prompts. We\nalso provide the community with a simple, browser-based implementation of the\nmethod at https://prompt-ultra.github.io/ .",
      "tldr_zh": "大型语言模型（Large Language Models）在社会科学文本标注任务中已能媲美或超过人类性能，但提示选择（prompt selection）对标注准确性的影响尚未被研究。本文发现，不同提示间的性能差异显著，并引入自动提示优化（automatic prompt optimization）方法来系统地创建高质量提示。通过实验验证，该方法提升了标注效果，并向社区提供了一个简单易用的浏览器-based 工具（https://prompt-ultra.github.io/），以促进进一步应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10645v2",
      "published_date": "2024-07-15 12:04:32 UTC",
      "updated_date": "2025-03-10 10:35:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:48:02.482794"
    },
    {
      "arxiv_id": "2407.10639v1",
      "title": "Risk-aware Trajectory Prediction by Incorporating Spatio-temporal Traffic Interaction Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Divya Thuremella",
        "Lewis Ince",
        "Lars Kunze"
      ],
      "abstract": "To operate in open-ended environments where humans interact in complex,\ndiverse ways, autonomous robots must learn to predict their behaviour,\nespecially when that behavior is potentially dangerous to other agents or to\nthe robot. However, reducing the risk of accidents requires prior knowledge of\nwhere potential collisions may occur and how. Therefore, we propose to gain\nthis information by analyzing locations and speeds that commonly correspond to\nhigh-risk interactions within the dataset, and use it within training to\ngenerate better predictions in high risk situations. Through these\nlocation-based and speed-based re-weighting techniques, we achieve improved\noverall performance, as measured by most-likely FDE and KDE, as well as\nimproved performance on high-speed vehicles, and vehicles within high-risk\nlocations.\n  2023 IEEE International Conference on Robotics and Automation (ICRA)",
      "tldr_zh": "该研究针对自主机器人预测人类行为以避免潜在危险，提出了一种风险感知轨迹预测方法，通过整合时空交通互动分析（Spatio-temporal Traffic Interaction Analysis）来识别数据集中的高风险位置和速度。作者引入位置和速度基于的再加权技术（location-based and speed-based re-weighting techniques），以在训练过程中优先优化高风险场景的预测。实验结果显示，该方法显著提升了整体性能，包括 most-likely FDE 和 KDE 指标，并在高速度车辆和高风险位置的预测上取得了更好表现。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10639v1",
      "published_date": "2024-07-15 11:57:06 UTC",
      "updated_date": "2024-07-15 11:57:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:48:14.320837"
    },
    {
      "arxiv_id": "2407.10632v2",
      "title": "Bidirectional Stereo Image Compression with Cross-Dimensional Entropy Model",
      "title_zh": "翻译失败",
      "authors": [
        "Zhening Liu",
        "Xinjie Zhang",
        "Jiawei Shao",
        "Zehong Lin",
        "Jun Zhang"
      ],
      "abstract": "With the rapid advancement of stereo vision technologies, stereo image\ncompression has emerged as a crucial field that continues to draw significant\nattention. Previous approaches have primarily employed a unidirectional\nparadigm, where the compression of one view is dependent on the other,\nresulting in imbalanced compression. To address this issue, we introduce a\nsymmetric bidirectional stereo image compression architecture, named BiSIC.\nSpecifically, we propose a 3D convolution based codec backbone to capture local\nfeatures and incorporate bidirectional attention blocks to exploit global\nfeatures. Moreover, we design a novel cross-dimensional entropy model that\nintegrates various conditioning factors, including the spatial context, channel\ncontext, and stereo dependency, to effectively estimate the distribution of\nlatent representations for entropy coding. Extensive experiments demonstrate\nthat our proposed BiSIC outperforms conventional image/video compression\nstandards, as well as state-of-the-art learning-based methods, in terms of both\nPSNR and MS-SSIM.",
      "tldr_zh": "本研究针对立体图像压缩中的单向依赖问题，提出了一种对称的双向架构BiSIC，以实现更均衡的压缩效果。具体而言，BiSIC采用基于3D convolution的编解码器骨干来捕捉局部特征，并融入bidirectional attention blocks来利用全局特征，同时设计了novel cross-dimensional entropy model，整合空间上下文、通道上下文和立体依赖性来精确估计潜在表示的分布用于熵编码。实验结果显示，BiSIC在PSNR和MS-SSIM指标上优于传统图像/视频压缩标准以及最先进的基于学习的方法，证明了其在立体图像压缩领域的先进性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.10632v2",
      "published_date": "2024-07-15 11:36:22 UTC",
      "updated_date": "2024-10-26 06:03:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:48:25.531993"
    },
    {
      "arxiv_id": "2407.10627v1",
      "title": "Arena Learning: Build Data Flywheel for LLMs Post-training via Simulated Chatbot Arena",
      "title_zh": "翻译失败",
      "authors": [
        "Haipeng Luo",
        "Qingfeng Sun",
        "Can Xu",
        "Pu Zhao",
        "Qingwei Lin",
        "Jianguang Lou",
        "Shifeng Chen",
        "Yansong Tang",
        "Weizhu Chen"
      ],
      "abstract": "Assessing the effectiveness of large language models (LLMs) presents\nsubstantial challenges. The method of conducting human-annotated battles in an\nonline Chatbot Arena is a highly effective evaluative technique. However, this\napproach is limited by the costs and time required for human annotation. In\nthis paper, we introduce Arena Learning, an innovative offline strategy\ndesigned to simulate these arena battles using AI-driven annotations to\nevaluate battle outcomes, thus facilitating the continuous improvement of the\ntarget model through both supervised fine-tuning and reinforcement learning.\nArena Learning comprises two key elements. First, it ensures precise\nevaluations and maintains consistency between offline simulations and online\ncompetitions via WizardArena, a pipeline developed to accurately predict the\nElo rankings of various models using a meticulously designed offline test set.\nOur results demonstrate that WizardArena's predictions closely align with those\nfrom the online Arena. Second, it involves the continuous improvement of\ntraining data based on the battle results and the refined model. We establish a\ndata flywheel to iteratively update the training data by highlighting the\nweaknesses of the target model based on its battle results, enabling it to\nlearn from the strengths of multiple different models. We apply Arena Learning\nto train our target model, WizardLM-$\\beta$, and demonstrate significant\nperformance enhancements across various metrics. This fully automated training\nand evaluation pipeline sets the stage for continuous advancements in various\nLLMs via post-training. Notably, Arena Learning plays a pivotal role in the\nsuccess of WizardLM-2, and this paper serves both as an exploration of its\nefficacy and a foundational study for future discussions related to WizardLM-2\nand its derivatives.",
      "tldr_zh": "本研究提出Arena Learning，一种创新的离线策略，通过模拟Chatbot Arena使用AI驱动的标注来评估和改进大型语言模型(LLMs)的后训练过程，从而克服人为标注的成本和时间限制。Arena Learning的核心包括WizardArena管道，用于精确预测模型的Elo rankings，确保离线模拟与在线竞争一致，以及数据飞轮机制，通过分析战斗结果突出目标模型的弱点并从多模型优势中学习，实现训练数据的持续迭代。实验结果显示，该方法显著提升了WizardLM-β的性能，并在WizardLM-2的成功中发挥关键作用，为LLMs的自动化后训练和持续优化奠定基础。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10627v1",
      "published_date": "2024-07-15 11:26:07 UTC",
      "updated_date": "2024-07-15 11:26:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:48:37.749539"
    },
    {
      "arxiv_id": "2407.10596v1",
      "title": "An evaluation of CNN models and data augmentation techniques in hierarchical localization of mobile robots",
      "title_zh": "对卷积神经网络模型和数据增强技术在移动机器人分层定位中的评估",
      "authors": [
        "J. J. Cabrera",
        "O. J. Céspedes",
        "S. Cebollada",
        "O. Reinoso",
        "L. Payá"
      ],
      "abstract": "This work presents an evaluation of CNN models and data augmentation to carry\nout the hierarchical localization of a mobile robot by using omnidireccional\nimages. In this sense, an ablation study of different state-of-the-art CNN\nmodels used as backbone is presented and a variety of data augmentation visual\neffects are proposed for addressing the visual localization of the robot. The\nproposed method is based on the adaption and re-training of a CNN with a dual\npurpose: (1) to perform a rough localization step in which the model is used to\npredict the room from which an image was captured, and (2) to address the fine\nlocalization step, which consists in retrieving the most similar image of the\nvisual map among those contained in the previously predicted room by means of a\npairwise comparison between descriptors obtained from an intermediate layer of\nthe CNN. In this sense, we evaluate the impact of different state-of-the-art\nCNN models such as ConvNeXt for addressing the proposed localization. Finally,\na variety of data augmentation visual effects are separately employed for\ntraining the model and their impact is assessed. The performance of the\nresulting CNNs is evaluated under real operation conditions, including changes\nin the lighting conditions. Our code is publicly available on the project\nwebsite https://github.com/juanjo-cabrera/IndoorLocalizationSingleCNN.git",
      "tldr_zh": "本研究评估了各种 CNN 模型（如 ConvNeXt）和 data augmentation 技术在移动机器人分层定位中的应用，使用全向图像进行粗定位（预测图像所属房间）和细定位（通过 CNN 中间层描述符的配对比较检索相似图像）。论文通过消融研究比较了不同 state-of-the-art CNN 模型的性能，并测试了多种数据增强视觉效果，以提升模型在光照变化等真实操作条件下的鲁棒性。结果表明，这些方法显著提高了定位准确性，相关代码已在 GitHub 上公开可用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Published Evolving Systems (2024): 08 July 2024 PDF link:\n  https://link.springer.com/content/pdf/10.1007/s12530-024-09604-6.pdf",
      "pdf_url": "http://arxiv.org/pdf/2407.10596v1",
      "published_date": "2024-07-15 10:20:00 UTC",
      "updated_date": "2024-07-15 10:20:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:48:50.487314"
    },
    {
      "arxiv_id": "2407.12871v2",
      "title": "MetaTool: Facilitating Large Language Models to Master Tools with Meta-task Augmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaohan Wang",
        "Dian Li",
        "Yilin Zhao",
        "Sinbadliu",
        "Hui Wang"
      ],
      "abstract": "Utilizing tools with Large Language Models (LLMs) is essential for grounding\nAI agents in real-world applications. The prevailing approach involves few-shot\nprompting with demonstrations or fine-tuning with expert annotations. However,\nmere in-context demonstrations may fail to cover sufficient knowledge for\ncomplex tools and tasks. Training on solution paths is also hindered by the\nhigh cost of expert annotations and generalizing to new tools. A core challenge\nof generalizable tool use lies in understanding the \"meta\", or fundamental\nnatures of tools that are transferable across tasks, such as causality and\nconstraints. In this paper, we present MetaTool, a novel tool learning\nmethodology designed to generalize across any reusable toolset. Our approach\nincorporates a self-supervised augmentation technique derived from a series of\nmeta-tasks. This involves predicting masked elements in the tool execution\nprocess. The self-supervised procedure enables scalable generation of\nhigh-quality QA data, which is handy for supervising tool understanding. By\nincorporating meta-task data into task-oriented training, our method\nsignificantly enhances the performance of open-source LLMs, achieving results\ncomparable to ChatGPT in both tool-based planning and chatting scenarios.\nThrough large-scale instruction tuning, the MetaTool model demonstrates\nimpressive zero-shot generalizability on new tasks.",
      "tldr_zh": "本文提出 MetaTool，一种新型工具学习方法，通过元任务增强(self-supervised augmentation)技术，帮助 Large Language Models (LLMs) 掌握和泛化工具使用。该方法利用一系列元任务（如预测工具执行过程中的掩码元素）生成高质量的 QA 数据，并将其融入任务导向训练中，解决传统方法的覆盖不足和标注成本高问题。实验结果显示，MetaTool 显著提升开源 LLMs 的性能，在工具-based 规划和聊天场景中达到与 ChatGPT 相当的水平，并展示了强大的零样本 generalizability。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.12871v2",
      "published_date": "2024-07-15 10:15:41 UTC",
      "updated_date": "2024-10-08 11:39:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:49:03.072850"
    },
    {
      "arxiv_id": "2407.10583v1",
      "title": "Three Dogmas of Reinforcement Learning",
      "title_zh": "强化学习的三个教条",
      "authors": [
        "David Abel",
        "Mark K. Ho",
        "Anna Harutyunyan"
      ],
      "abstract": "Modern reinforcement learning has been conditioned by at least three dogmas.\nThe first is the environment spotlight, which refers to our tendency to focus\non modeling environments rather than agents. The second is our treatment of\nlearning as finding the solution to a task, rather than adaptation. The third\nis the reward hypothesis, which states that all goals and purposes can be well\nthought of as maximization of a reward signal. These three dogmas shape much of\nwhat we think of as the science of reinforcement learning. While each of the\ndogmas have played an important role in developing the field, it is time we\nbring them to the surface and reflect on whether they belong as basic\ningredients of our scientific paradigm. In order to realize the potential of\nreinforcement learning as a canonical frame for researching intelligent agents,\nwe suggest that it is time we shed dogmas one and two entirely, and embrace a\nnuanced approach to the third.",
      "tldr_zh": "这篇论文批判了强化学习领域的三个核心教条（dogmas）：第一是environment spotlight，即过度关注环境建模而非代理建模；第二是将学习视为任务解决方案，而不是适应过程；第三是reward hypothesis，认为所有目标均可归结为最大化奖励信号。这些教条虽推动了强化学习的发展，但作者认为应彻底放弃前两个教条，并对第三个采用更细致的方法，以实现强化学习作为研究智能代理的规范框架的潜力。最终，论文呼吁反思这些基本假设，以推动领域更全面的进步。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "RLC 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.10583v1",
      "published_date": "2024-07-15 10:03:24 UTC",
      "updated_date": "2024-07-15 10:03:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:49:13.166613"
    },
    {
      "arxiv_id": "2407.10582v1",
      "title": "Boosting Zero-Shot Crosslingual Performance using LLM-Based Augmentations with Effective Data Selection",
      "title_zh": "翻译失败",
      "authors": [
        "Barah Fazili",
        "Ashish Sunil Agrawal",
        "Preethi Jyothi"
      ],
      "abstract": "Large language models (LLMs) are very proficient text generators. We leverage\nthis capability of LLMs to generate task-specific data via zero-shot prompting\nand promote cross-lingual transfer for low-resource target languages. Given\ntask-specific data in a source language and a teacher model trained on this\ndata, we propose using this teacher to label LLM generations and employ a set\nof simple data selection strategies that use the teacher's label probabilities.\nOur data selection strategies help us identify a representative subset of\ndiverse generations that help boost zero-shot accuracies while being efficient,\nin comparison to using all the LLM generations (without any subset selection).\nWe also highlight other important design choices that affect cross-lingual\nperformance such as the use of translations of source data and what labels are\nbest to use for the LLM generations. We observe significant performance gains\nacross sentiment analysis and natural language inference tasks (of up to a\nmaximum of 7.13 absolute points and 1.5 absolute points on average) across a\nnumber of target languages (Hindi, Marathi, Urdu, Swahili) and domains.",
      "tldr_zh": "这篇论文提出了一种利用LLMs（Large Language Models）通过zero-shot prompting生成任务特定数据的方法，以提升低资源目标语言的cross-lingual性能。具体而言，他们使用teacher model标记LLM生成的数据，并采用基于标签概率的数据选择策略，选择一个代表性和多样性的子集，从而提高zero-shot准确率并提升效率。实验结果显示，在情感分析和自然语言推理任务上，该方法在Hindi、Marathi、Urdu和Swahili等语言中实现了显著提升，平均提高1.5绝对点，最高达7.13绝对点。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in Findings of ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.10582v1",
      "published_date": "2024-07-15 10:00:22 UTC",
      "updated_date": "2024-07-15 10:00:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:49:27.720892"
    },
    {
      "arxiv_id": "2407.10580v1",
      "title": "Leveraging Hybrid Intelligence Towards Sustainable and Energy-Efficient Machine Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Geissler",
        "Paul Lukowicz"
      ],
      "abstract": "Hybrid intelligence aims to enhance decision-making, problem-solving, and\noverall system performance by combining the strengths of both, human cognitive\nabilities and artificial intelligence. With the rise of Large Language Models\n(LLM), progressively participating as smart agents to accelerate machine\nlearning development, Hybrid Intelligence is becoming an increasingly important\ntopic for effective interaction between humans and machines. This paper\npresents an approach to leverage Hybrid Intelligence towards sustainable and\nenergy-aware machine learning. When developing machine learning models, final\nmodel performance commonly rules the optimization process while the efficiency\nof the process itself is often neglected. Moreover, in recent times, energy\nefficiency has become equally crucial due to the significant environmental\nimpact of complex and large-scale computational processes. The contribution of\nthis work covers the interactive inclusion of secondary knowledge sources\nthrough Human-in-the-loop (HITL) and LLM agents to stress out and further\nresolve inefficiencies in the machine learning development process.",
      "tldr_zh": "本论文探讨了如何利用Hybrid Intelligence结合人类认知和人工智能的优势，特别是Large Language Models (LLMs)，来实现可持续和节能的机器学习开发。传统机器学习优化往往优先考虑最终模型性能，而忽略过程效率和能源消耗，这对环境造成重大影响。论文提出一种创新方法，通过Human-in-the-loop (HITL)和LLM代理的交互式整合，引入次要知识来源以识别并解决开发过程中的低效问题。该方法有助于提升机器学习的能源效率，并为更环保的AI系统奠定基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10580v1",
      "published_date": "2024-07-15 09:58:27 UTC",
      "updated_date": "2024-07-15 09:58:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:49:37.655731"
    },
    {
      "arxiv_id": "2407.10547v2",
      "title": "Learning Social Cost Functions for Human-Aware Path Planning",
      "title_zh": "翻译失败",
      "authors": [
        "Andrea Eirale",
        "Matteo Leonetti",
        "Marcello Chiaberge"
      ],
      "abstract": "Achieving social acceptance is one of the main goals of Social Robotic\nNavigation. Despite this topic has received increasing interest in recent\nyears, most of the research has focused on driving the robotic agent along\nobstacle-free trajectories, planning around estimates of future human motion to\nrespect personal distances and optimize navigation. However, social\ninteractions in everyday life are also dictated by norms that do not strictly\ndepend on movement, such as when standing at the end of a queue rather than\ncutting it. In this paper, we propose a novel method to recognize common social\nscenarios and modify a traditional planner's cost function to adapt to them.\nThis solution enables the robot to carry out different social navigation\nbehaviors that would not arise otherwise, maintaining the robustness of\ntraditional navigation. Our approach allows the robot to learn different social\nnorms with a single learned model, rather than having different modules for\neach task. As a proof of concept, we consider the tasks of queuing and respect\ninteraction spaces of groups of people talking to one another, but the method\ncan be extended to other human activities that do not involve motion.",
      "tldr_zh": "该论文针对社会机器人导航（Social Robotic Navigation）提出了一种学习社会成本函数（cost function）的方法，以使机器人不仅避开障碍物，还能遵守非运动相关的社会规范，如排队或尊重人群互动空间。方法涉及识别常见社会场景，并动态修改传统规划器的cost function，从而实现更自然的导航行为，同时保持鲁棒性。通过一个统一的学习模型，机器人可以适应多种社会规范，而非依赖多个专用模块。作为概念验证，该方法在排队和人群互动任务上展示了其有效性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10547v2",
      "published_date": "2024-07-15 08:57:02 UTC",
      "updated_date": "2024-10-18 12:25:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:50:02.072205"
    },
    {
      "arxiv_id": "2407.10545v3",
      "title": "LightCL: Compact Continual Learning with Low Memory Footprint For Edge Device",
      "title_zh": "翻译失败",
      "authors": [
        "Zeqing Wang",
        "Fei Cheng",
        "Kangye Ji",
        "Bohu Huang"
      ],
      "abstract": "Continual learning (CL) is a technique that enables neural networks to\nconstantly adapt to their dynamic surroundings. Despite being overlooked for a\nlong time, this technology can considerably address the customized needs of\nusers in edge devices. Actually, most CL methods require huge resource\nconsumption by the training behavior to acquire generalizability among all\ntasks for delaying forgetting regardless of edge scenarios. Therefore, this\npaper proposes a compact algorithm called LightCL, which evaluates and\ncompresses the redundancy of already generalized components in structures of\nthe neural network. Specifically, we consider two factors of generalizability,\nlearning plasticity and memory stability, and design metrics of both to\nquantitatively assess generalizability of neural networks during CL. This\nevaluation shows that generalizability of different layers in a neural network\nexhibits a significant variation. Thus, we $\\textit{Maintain Generalizability}$\nby freezing generalized parts without the resource-intensive training process\nand $\\textit{Memorize Feature Patterns}$ by stabilizing feature extracting of\nprevious tasks to enhance generalizability for less-generalized parts with a\nlittle extra memory, which is far less than the reduction by freezing.\nExperiments illustrate that LightCL outperforms other state-of-the-art methods\nand reduces at most $\\textbf{6.16$\\times$}$ memory footprint. We also verify\nthe effectiveness of LightCL on the edge device.",
      "tldr_zh": "该论文提出了一种紧凑的持续学习（Continual Learning）算法LightCL，旨在为边缘设备减少内存占用，通过评估神经网络中冗余组件的泛化性来优化资源消耗。具体地，LightCL考虑学习可塑性（learning plasticity）和记忆稳定性（memory stability）两个因素，设计量化指标评估网络层级的泛化差异，并通过冻结泛化部分（Maintain Generalizability）和稳定特征提取（Memorize Feature Patterns）来最小化训练开销。实验结果显示，LightCL比现有最先进方法性能更优，最多减少6.16倍的内存占用，并在实际边缘设备上验证了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in ASPDAC'25",
      "pdf_url": "http://arxiv.org/pdf/2407.10545v3",
      "published_date": "2024-07-15 08:52:20 UTC",
      "updated_date": "2025-03-08 10:54:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:50:02.179497"
    },
    {
      "arxiv_id": "2407.10543v1",
      "title": "Understanding the Dependence of Perception Model Competency on Regions in an Image",
      "title_zh": "理解感知模型胜任度对图像中区域的依赖性",
      "authors": [
        "Sara Pohland",
        "Claire Tomlin"
      ],
      "abstract": "While deep neural network (DNN)-based perception models are useful for many\napplications, these models are black boxes and their outputs are not yet well\nunderstood. To confidently enable a real-world, decision-making system to\nutilize such a perception model without human intervention, we must enable the\nsystem to reason about the perception model's level of competency and respond\nappropriately when the model is incompetent. In order for the system to make an\nintelligent decision about the appropriate action when the model is\nincompetent, it would be useful for the system to understand why the model is\nincompetent. We explore five novel methods for identifying regions in the input\nimage contributing to low model competency, which we refer to as image\ncropping, segment masking, pixel perturbation, competency gradients, and\nreconstruction loss. We assess the ability of these five methods to identify\nunfamiliar objects, recognize regions associated with unseen classes, and\nidentify unexplored areas in an environment. We find that the competency\ngradients and reconstruction loss methods show great promise in identifying\nregions associated with low model competency, particularly when aspects of the\nimage that are unfamiliar to the perception model are causing this reduction in\ncompetency. Both of these methods boast low computation times and high levels\nof accuracy in detecting image regions that are unfamiliar to the model,\nallowing them to provide potential utility in decision-making pipelines. The\ncode for reproducing our methods and results is available on GitHub:\nhttps://github.com/sarapohland/explainable-competency.",
      "tldr_zh": "该论文探讨了深度神经网络（DNN）感知模型的输出不透明性问题，旨在帮助决策系统理解模型的competency（能力）水平，并在模型能力不足时进行智能响应。研究者提出了五种新方法——image cropping、segment masking、pixel perturbation、competency gradients和reconstruction loss——用于识别输入图像中导致能力降低的区域，如不熟悉对象、未见类相关区域或环境未探索部分。通过实验评估，发现competency gradients和reconstruction loss方法在检测不熟悉图像区域时准确率高、计算时间短，为可解释的决策管道提供了实用工具。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10543v1",
      "published_date": "2024-07-15 08:50:13 UTC",
      "updated_date": "2024-07-15 08:50:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:50:15.856739"
    },
    {
      "arxiv_id": "2407.10542v1",
      "title": "3D Geometric Shape Assembly via Efficient Point Cloud Matching",
      "title_zh": "翻译失败",
      "authors": [
        "Nahyuk Lee",
        "Juhong Min",
        "Junha Lee",
        "Seungwook Kim",
        "Kanghee Lee",
        "Jaesik Park",
        "Minsu Cho"
      ],
      "abstract": "Learning to assemble geometric shapes into a larger target structure is a\npivotal task in various practical applications. In this work, we tackle this\nproblem by establishing local correspondences between point clouds of part\nshapes in both coarse- and fine-levels. To this end, we introduce Proxy Match\nTransform (PMT), an approximate high-order feature transform layer that enables\nreliable matching between mating surfaces of parts while incurring low costs in\nmemory and computation. Building upon PMT, we introduce a new framework, dubbed\nProxy Match TransformeR (PMTR), for the geometric assembly task. We evaluate\nthe proposed PMTR on the large-scale 3D geometric shape assembly benchmark\ndataset of Breaking Bad and demonstrate its superior performance and efficiency\ncompared to state-of-the-art methods. Project page:\nhttps://nahyuklee.github.io/pmtr.",
      "tldr_zh": "该研究针对3D几何形状组装任务，提出通过高效的Point Cloud Matching在粗细级别建立部件点云的局部对应关系。作者引入了Proxy Match Transform (PMT)，一种低内存和计算成本的高阶特征变换层，用于可靠匹配部件的配对表面。基于PMT，他们开发了Proxy Match TransformeR (PMTR)框架，并在Breaking Bad数据集上评估，显示出比现有方法更高的性能和效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.10542v1",
      "published_date": "2024-07-15 08:50:02 UTC",
      "updated_date": "2024-07-15 08:50:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:50:25.142554"
    },
    {
      "arxiv_id": "2407.10536v1",
      "title": "An experimental evaluation of Siamese Neural Networks for robot localization using omnidirectional imaging in indoor environments",
      "title_zh": "翻译失败",
      "authors": [
        "J. J. Cabrera",
        "V. Román",
        "A. Gil",
        "O. Reinoso",
        "L. Payá"
      ],
      "abstract": "The objective of this paper is to address the localization problem using\nomnidirectional images captured by a catadioptric vision system mounted on the\nrobot. For this purpose, we explore the potential of Siamese Neural Networks\nfor modeling indoor environments using panoramic images as the unique source of\ninformation. Siamese Neural Networks are characterized by their ability to\ngenerate a similarity function between two input data, in this case, between\ntwo panoramic images. In this study, Siamese Neural Networks composed of two\nConvolutional Neural Networks (CNNs) are used. The output of each CNN is a\ndescriptor which is used to characterize each image. The dissimilarity of the\nimages is computed by measuring the distance between these descriptors. This\nfact makes Siamese Neural Networks particularly suitable to perform image\nretrieval tasks. First, we evaluate an initial task strongly related to\nlocalization that consists in detecting whether two images have been captured\nin the same or in different rooms. Next, we assess Siamese Neural Networks in\nthe context of a global localization problem. The results outperform previous\ntechniques for solving the localization task using the COLD-Freiburg dataset,\nin a variety of lighting conditions, specially when using images captured in\ncloudy and night conditions.",
      "tldr_zh": "本文评估了 Siamese Neural Networks 在室内环境中使用全向图像进行机器人定位的性能，这些图像由安装在机器人上的猫眼视觉系统捕获。研究方法涉及构建由两个 Convolutional Neural Networks (CNNs) 组成的 Siamese Neural Networks，通过生成图像描述符并计算描述符之间的距离来量化图像相似性，从而实现图像检索任务。实验首先测试了检测两张图像是否在同一房间的能力，随后在全局定位问题上应用该模型，使用 COLD-Freiburg 数据集在各种照明条件下（如阴天和夜晚）取得了比先前技术更高的准确率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Published: 08 July 2024 Paper link:\n  https://link.springer.com/content/pdf/10.1007/s10462-024-10840-0.pdf",
      "pdf_url": "http://arxiv.org/pdf/2407.10536v1",
      "published_date": "2024-07-15 08:44:37 UTC",
      "updated_date": "2024-07-15 08:44:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:50:38.988998"
    },
    {
      "arxiv_id": "2407.10510v2",
      "title": "TCM-FTP: Fine-Tuning Large Language Models for Herbal Prescription Prediction",
      "title_zh": "TCM-FTP：微调大语言",
      "authors": [
        "Xingzhi Zhou",
        "Xin Dong",
        "Chunhao Li",
        "Yuning Bai",
        "Yulong Xu",
        "Ka Chun Cheung",
        "Simon See",
        "Xinpeng Song",
        "Runshun Zhang",
        "Xuezhong Zhou",
        "Nevin L. Zhang"
      ],
      "abstract": "Traditional Chinese medicine (TCM) has relied on specific combinations of\nherbs in prescriptions to treat various symptoms and signs for thousands of\nyears. Predicting TCM prescriptions poses a fascinating technical challenge\nwith significant practical implications. However, this task faces limitations\ndue to the scarcity of high-quality clinical datasets and the complex\nrelationship between symptoms and herbs. To address these issues, we introduce\n\\textit{DigestDS}, a novel dataset comprising practical medical records from\nexperienced experts in digestive system diseases. We also propose a method,\nTCM-FTP (TCM Fine-Tuning Pre-trained), to leverage pre-trained large language\nmodels (LLMs) via supervised fine-tuning on \\textit{DigestDS}. Additionally, we\nenhance computational efficiency using a low-rank adaptation technique.\nMoreover, TCM-FTP incorporates data augmentation by permuting herbs within\nprescriptions, exploiting their order-agnostic nature. Impressively, TCM-FTP\nachieves an F1-score of 0.8031, significantly outperforming previous methods.\nFurthermore, it demonstrates remarkable accuracy in dosage prediction,\nachieving a normalized mean square error of 0.0604. In contrast, LLMs without\nfine-tuning exhibit poor performance. Although LLMs have demonstrated\nwide-ranging capabilities, our work underscores the necessity of fine-tuning\nfor TCM prescription prediction and presents an effective way to accomplish\nthis.",
      "tldr_zh": "这篇论文针对传统中医(TCM)草药处方预测的挑战，包括数据稀缺和症状与草药的复杂关系，引入了新数据集DigestDS，该数据集包含消化系统疾病的实际医疗记录。作者提出TCM-FTP方法，通过对预训练的大型语言模型(LLMs)进行监督Fine-Tuning，并结合低秩适配(Low-Rank Adaptation)提升计算效率，以及通过置换草药顺序进行数据增强。实验结果显示，TCM-FTP在处方预测上达到了0.8031的F1-score，在剂量预测上取得了0.0604的归一化均方误差(NMSE)，显著优于未经Fine-Tuning的LLMs。该方法证明了Fine-Tuning在TCM应用中的必要性和有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.CL",
      "comment": "Camera-ready version to be published in BIBM 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.10510v2",
      "published_date": "2024-07-15 08:06:37 UTC",
      "updated_date": "2024-12-12 10:40:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:50:53.337724"
    },
    {
      "arxiv_id": "2407.14260v1",
      "title": "Guitar Chord Diagram Suggestion for Western Popular Music",
      "title_zh": "针对西方流行音乐的吉他和弦图建议",
      "authors": [
        "Alexandre d'Hooge",
        "Louis Bigo",
        "Ken Déguernel",
        "Nicolas Martin"
      ],
      "abstract": "Chord diagrams are used by guitar players to show where and how to play a\nchord on the fretboard. They are useful to beginners learning chords or for\nsharing the hand positions required to play a song.However, the diagrams\npresented on guitar learning toolsare usually selected from an existing\ndatabaseand rarely represent the actual positions used by performers.In this\npaper, we propose a tool which suggests a chord diagram for achord label,taking\ninto account the diagram of the previous chord.Based on statistical analysis of\nthe DadaGP and mySongBook datasets, we show that some chord diagrams are\nover-represented in western popular musicand that some chords can be played in\nmore than 20 different ways.We argue that taking context into account can\nimprove the variety and the quality of chord diagram suggestion, and compare\nthis approach with a model taking only the current chord label into account.We\nshow that adding previous context improves the F1-score on this task by up to\n27% and reduces the propensity of the model to suggest standard open chords.We\nalso define the notion of texture in the context of chord diagrams andshow\nthrough a variety of metrics that our model improves textureconsistencywith the\nprevious diagram.",
      "tldr_zh": "这篇论文针对吉他和弦图（chord diagrams）的建议问题，提出了一种新工具，能根据和弦标签和前一个和弦图的上下文生成更合适的建议，以提升吉他演奏的多样性和真实性。通过分析 DadaGP 和 mySongBook 数据集，研究发现某些和弦图在西方流行音乐中过度代表，且有些和弦可有超过 20 种演奏方式。与仅考虑当前和弦标签的模型相比，新方法将 F1-score 提高了高达 27%，并减少了标准开放和弦的倾向。同时，论文定义了和弦图的纹理（texture）概念，并证明了新模型能改善纹理一致性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.14260v1",
      "published_date": "2024-07-15 07:44:13 UTC",
      "updated_date": "2024-07-15 07:44:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:51:04.092564"
    },
    {
      "arxiv_id": "2407.10490v3",
      "title": "Learning Dynamics of LLM Finetuning",
      "title_zh": "LLM 微调的学习动态",
      "authors": [
        "Yi Ren",
        "Danica J. Sutherland"
      ],
      "abstract": "Learning dynamics, which describes how the learning of specific training\nexamples influences the model's predictions on other examples, gives us a\npowerful tool for understanding the behavior of deep learning systems. We study\nthe learning dynamics of large language models during different types of\nfinetuning, by analyzing the step-wise decomposition of how influence\naccumulates among different potential responses. Our framework allows a uniform\ninterpretation of many interesting observations about the training of popular\nalgorithms for both instruction tuning and preference tuning. In particular, we\npropose a hypothetical explanation of why specific types of hallucination are\nstrengthened after finetuning, e.g., the model might use phrases or facts in\nthe response for question B to answer question A, or the model might keep\nrepeating similar simple phrases when generating responses. We also extend our\nframework and highlight a unique \"squeezing effect\" to explain a previously\nobserved phenomenon in off-policy direct preference optimization (DPO), where\nrunning DPO for too long makes even the desired outputs less likely. This\nframework also provides insights into where the benefits of on-policy DPO and\nother variants come from. The analysis not only provides a novel perspective of\nunderstanding LLM's finetuning but also inspires a simple, effective method to\nimprove alignment performance.",
      "tldr_zh": "本论文研究了大型语言模型（LLMs）的微调过程学习动态，分析了训练特定示例如何影响模型对其他示例的预测，并提出一个框架来分解影响在不同潜在响应之间的逐步积累。该框架统一解释了指令微调和偏好微调等算法的观察现象，包括为什么微调后幻觉（hallucination）会加强，如模型使用一个问题的响应来回答另一个问题或重复简单短语。该分析还揭示了off-policy直接偏好优化（DPO）中的“挤压效应”（squeezing effect），解释了过度训练导致期望输出概率降低的原因，并为on-policy DPO等变体的优势提供见解，最终启发了一种简单有效的改进模型对齐性能的方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10490v3",
      "published_date": "2024-07-15 07:30:28 UTC",
      "updated_date": "2025-02-20 01:09:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:51:15.647937"
    },
    {
      "arxiv_id": "2407.10488v1",
      "title": "How and where does CLIP process negation?",
      "title_zh": "CLIP 如何以及在何处处理否定？",
      "authors": [
        "Vincent Quantmeyer",
        "Pablo Mosteiro",
        "Albert Gatt"
      ],
      "abstract": "Various benchmarks have been proposed to test linguistic understanding in\npre-trained vision \\& language (VL) models. Here we build on the existence task\nfrom the VALSE benchmark (Parcalabescu et al, 2022) which we use to test\nmodels' understanding of negation, a particularly interesting issue for\nmultimodal models. However, while such VL benchmarks are useful for measuring\nmodel performance, they do not reveal anything about the internal processes\nthrough which these models arrive at their outputs in such visio-linguistic\ntasks. We take inspiration from the growing literature on model\ninterpretability to explain the behaviour of VL models on the understanding of\nnegation. Specifically, we approach these questions through an in-depth\nanalysis of the text encoder in CLIP (Radford et al, 2021), a highly\ninfluential VL model. We localise parts of the encoder that process negation\nand analyse the role of attention heads in this task. Our contributions are\nthreefold. We demonstrate how methods from the language model interpretability\nliterature (such as causal tracing) can be translated to multimodal models and\ntasks; we provide concrete insights into how CLIP processes negation on the\nVALSE existence task; and we highlight inherent limitations in the VALSE\ndataset as a benchmark for linguistic understanding.",
      "tldr_zh": "这篇论文探讨了视觉语言模型（VL models）如 CLIP 如何处理否定（negation），以 VALSE 基准中的存在任务为基础。研究者通过模型可解释性方法（如 causal tracing）分析 CLIP 的文本编码器（text encoder），定位处理否定的具体部分和 attention heads 的作用。他们的主要贡献包括：演示如何将语言模型解释技术应用于多模态任务、揭示 CLIP 在 VALSE 存在任务中的否定处理机制，以及指出 VALSE 数据集作为语言理解基准的固有局限性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at the 3rd Workshop on Advances in Language and Vision\n  Research (ALVR 2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.10488v1",
      "published_date": "2024-07-15 07:20:06 UTC",
      "updated_date": "2024-07-15 07:20:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:51:29.035256"
    },
    {
      "arxiv_id": "2407.11100v3",
      "title": "Building Intelligence Identification System via Large Language Model Watermarking: A Survey and Beyond",
      "title_zh": "翻译失败",
      "authors": [
        "Xuhong Wang",
        "Haoyu Jiang",
        "Yi Yu",
        "Jingru Yu",
        "Yilun Lin",
        "Ping Yi",
        "Yingchun Wang",
        "Yu Qiao",
        "Li Li",
        "Fei-Yue Wang"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly integrated into diverse\nindustries, posing substantial security risks due to unauthorized replication\nand misuse. To mitigate these concerns, robust identification mechanisms are\nwidely acknowledged as an effective strategy. Identification systems for LLMs\nnow rely heavily on watermarking technology to manage and protect intellectual\nproperty and ensure data security. However, previous studies have primarily\nconcentrated on the basic principles of algorithms and lacked a comprehensive\nanalysis of watermarking theory and practice from the perspective of\nintelligent identification. To bridge this gap, firstly, we explore how a\nrobust identity recognition system can be effectively implemented and managed\nwithin LLMs by various participants using watermarking technology. Secondly, we\npropose a mathematical framework based on mutual information theory, which\nsystematizes the identification process to achieve more precise and customized\nwatermarking. Additionally, we present a comprehensive evaluation of\nperformance metrics for LLM watermarking, reflecting participant preferences\nand advancing discussions on its identification applications. Lastly, we\noutline the existing challenges in current watermarking technologies and\ntheoretical frameworks, and provide directional guidance to address these\nchallenges. Our systematic classification and detailed exposition aim to\nenhance the comparison and evaluation of various methods, fostering further\nresearch and development toward a transparent, secure, and equitable LLM\necosystem.",
      "tldr_zh": "该论文调查了通过大型语言模型 (LLMs) 水印技术 (watermarking) 构建智能识别系统的方法，以应对未经授权复制和滥用的安全风险。作者提出基于互信息理论 (mutual information theory) 的数学框架，系统化地实现精确、可自定义的身份识别过程，并全面评估 watermarking 的性能指标，以反映参与者偏好和识别应用进展。最后，论文概述了当前 watermarking 技术的挑战，并提供解决方向，推动透明、安全和公平的 LLM 生态系统的发展。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "59 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.11100v3",
      "published_date": "2024-07-15 07:20:02 UTC",
      "updated_date": "2024-07-24 08:10:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:51:40.501058"
    },
    {
      "arxiv_id": "2407.10486v2",
      "title": "IDEAL: Leveraging Infinite and Dynamic Characterizations of Large Language Models for Query-focused Summarization",
      "title_zh": "IDEAL：利用大型语言模型的无限和动态特征进行查询焦点摘要",
      "authors": [
        "Jie Cao",
        "Dian Jiao",
        "Qiang Yan",
        "Wenqiao Zhang",
        "Siliang Tang",
        "Yueting Zhuang"
      ],
      "abstract": "Query-focused summarization (QFS) aims to produce summaries that answer\nparticular questions of interest, enabling greater user control and\npersonalization. With the advent of large language models (LLMs), shows their\nimpressive capability of textual understanding through large-scale pretraining,\nwhich implies the great potential of extractive snippet generation. In this\npaper, we systematically investigated two indispensable characteristics that\nthe LLMs-based QFS models should be harnessed, Lengthy Document Summarization\nand Efficiently Fine-grained Query-LLM Alignment, respectively.\nCorrespondingly, we propose two modules called Query-aware HyperExpert and\nQuery-focused Infini-attention to access the aforementioned characteristics.\nThese innovations pave the way for broader application and accessibility in the\nfield of QFS technology. Extensive experiments conducted on existing QFS\nbenchmarks indicate the effectiveness and generalizability of the proposed\napproach. Our code is publicly available at\nhttps://github.com/DCDmllm/IDEAL_Summary.",
      "tldr_zh": "本论文提出 IDEAL 框架，利用大型语言模型 (LLMs) 的无限和动态特性来提升 Query-focused Summarization (QFS)，旨在生成针对特定查询的个性化摘要。框架重点解决 Lengthy Document Summarization 和 Efficiently Fine-grained Query-LLM Alignment 两大特性，通过引入 Query-aware HyperExpert 和 Query-focused Infini-attention 模块，实现高效的文本理解和查询对齐。实验在现有 QFS 基准上验证了该方法的有效性和泛化性，并开源了代码（https://github.com/DCDmllm/IDEAL_Summary）。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10486v2",
      "published_date": "2024-07-15 07:14:56 UTC",
      "updated_date": "2025-01-07 14:09:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:51:51.990087"
    },
    {
      "arxiv_id": "2407.10481v1",
      "title": "SuperPADL: Scaling Language-Directed Physics-Based Control with Progressive Supervised Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Jordan Juravsky",
        "Yunrong Guo",
        "Sanja Fidler",
        "Xue Bin Peng"
      ],
      "abstract": "Physically-simulated models for human motion can generate high-quality\nresponsive character animations, often in real-time. Natural language serves as\na flexible interface for controlling these models, allowing expert and\nnon-expert users to quickly create and edit their animations. Many recent\nphysics-based animation methods, including those that use text interfaces,\ntrain control policies using reinforcement learning (RL). However, scaling\nthese methods beyond several hundred motions has remained challenging.\nMeanwhile, kinematic animation models are able to successfully learn from\nthousands of diverse motions by leveraging supervised learning methods.\nInspired by these successes, in this work we introduce SuperPADL, a scalable\nframework for physics-based text-to-motion that leverages both RL and\nsupervised learning to train controllers on thousands of diverse motion clips.\nSuperPADL is trained in stages using progressive distillation, starting with a\nlarge number of specialized experts using RL. These experts are then\niteratively distilled into larger, more robust policies using a combination of\nreinforcement learning and supervised learning. Our final SuperPADL controller\nis trained on a dataset containing over 5000 skills and runs in real time on a\nconsumer GPU. Moreover, our policy can naturally transition between skills,\nallowing for users to interactively craft multi-stage animations. We\nexperimentally demonstrate that SuperPADL significantly outperforms RL-based\nbaselines at this large data scale.",
      "tldr_zh": "这篇论文介绍了SuperPADL框架，用于扩展语言导向的基于物理模拟的动作控制，旨在解决强化学习（RL）方法在处理数千个动作时面临的扩展挑战。SuperPADL结合RL和监督学习，通过progressive supervised distillation逐步训练控制器，从多个RL专家开始，迭代蒸馏成更鲁棒的策略，最终能在超过5000个技能的数据集上运行。实验结果显示，该框架在大型数据规模下显著优于RL基线，支持实时运行和技能间自然过渡，便于用户创建多阶段动画。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.GR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10481v1",
      "published_date": "2024-07-15 07:07:11 UTC",
      "updated_date": "2024-07-15 07:07:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:52:03.275545"
    },
    {
      "arxiv_id": "2407.10476v1",
      "title": "Kinetic Typography Diffusion Model",
      "title_zh": "动态排版扩散模型",
      "authors": [
        "Seonmi Park",
        "Inhwan Bae",
        "Seunghyun Shin",
        "Hae-Gon Jeon"
      ],
      "abstract": "This paper introduces a method for realistic kinetic typography that\ngenerates user-preferred animatable 'text content'. We draw on recent advances\nin guided video diffusion models to achieve visually-pleasing text appearances.\nTo do this, we first construct a kinetic typography dataset, comprising about\n600K videos. Our dataset is made from a variety of combinations in 584\ntemplates designed by professional motion graphics designers and involves\nchanging each letter's position, glyph, and size (i.e., flying, glitches,\nchromatic aberration, reflecting effects, etc.). Next, we propose a video\ndiffusion model for kinetic typography. For this, there are three requirements:\naesthetic appearances, motion effects, and readable letters. This paper\nidentifies the requirements. For this, we present static and dynamic captions\nused as spatial and temporal guidance of a video diffusion model, respectively.\nThe static caption describes the overall appearance of the video, such as\ncolors, texture and glyph which represent a shape of each letter. The dynamic\ncaption accounts for the movements of letters and backgrounds. We add one more\nguidance with zero convolution to determine which text content should be\nvisible in the video. We apply the zero convolution to the text content, and\nimpose it on the diffusion model. Lastly, our glyph loss, only minimizing a\ndifference between the predicted word and its ground-truth, is proposed to make\nthe prediction letters readable. Experiments show that our model generates\nkinetic typography videos with legible and artistic letter motions based on\ntext prompts.",
      "tldr_zh": "该论文提出了一种Kinetic Typography Diffusion Model，用于生成用户偏好的动画文本内容，通过引导视频扩散模型实现视觉上吸引人的效果。首先，作者构建了一个包含约60万视频的动态字幕数据集，使用584个专业模板，涵盖字母的位置、字形和大小变化（如飞行效果、故障和色差等）。模型通过静态标题（描述视频整体外观，如颜色和纹理）和动态标题（处理字母运动）作为空间和时间指导，并结合零卷积技术确保文本内容的可见性，以及字形损失函数来提升字母的可读性。实验结果显示，该模型能基于文本提示生成清晰且艺术性的动态字幕视频。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ECCV 2024, Project page: https://seonmip.github.io/kinety",
      "pdf_url": "http://arxiv.org/pdf/2407.10476v1",
      "published_date": "2024-07-15 07:04:45 UTC",
      "updated_date": "2024-07-15 07:04:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:52:15.895898"
    },
    {
      "arxiv_id": "2407.10471v2",
      "title": "GROOT: Generating Robust Watermark for Diffusion-Model-Based Audio Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Weizhi Liu",
        "Yue Li",
        "Dongdong Lin",
        "Hui Tian",
        "Haizhou Li"
      ],
      "abstract": "Amid the burgeoning development of generative models like diffusion models,\nthe task of differentiating synthesized audio from its natural counterpart\ngrows more daunting. Deepfake detection offers a viable solution to combat this\nchallenge. Yet, this defensive measure unintentionally fuels the continued\nrefinement of generative models. Watermarking emerges as a proactive and\nsustainable tactic, preemptively regulating the creation and dissemination of\nsynthesized content. Thus, this paper, as a pioneer, proposes the generative\nrobust audio watermarking method (Groot), presenting a paradigm for proactively\nsupervising the synthesized audio and its source diffusion models. In this\nparadigm, the processes of watermark generation and audio synthesis occur\nsimultaneously, facilitated by parameter-fixed diffusion models equipped with a\ndedicated encoder. The watermark embedded within the audio can subsequently be\nretrieved by a lightweight decoder. The experimental results highlight Groot's\noutstanding performance, particularly in terms of robustness, surpassing that\nof the leading state-of-the-art methods. Beyond its impressive resilience\nagainst individual post-processing attacks, Groot exhibits exceptional\nrobustness when facing compound attacks, maintaining an average watermark\nextraction accuracy of around 95%.",
      "tldr_zh": "本论文提出GROOT，一种针对基于扩散模型(Diffusion Models)的音频合成生成鲁棒水印的方法，以主动监督合成音频及其源模型，应对Deepfake检测的局限性。GROOT通过在水印生成和音频合成过程中同时操作，使用参数固定的扩散模型配以专用编码器嵌入水印，并采用轻量级解码器进行提取。实验结果显示，GROOT在鲁棒性上超越现有最先进方法，对单个后处理攻击和复合攻击均表现出色，平均水印提取准确率约95%。这为可持续监管合成音频提供了新范式。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by ACM MM 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.10471v2",
      "published_date": "2024-07-15 06:57:19 UTC",
      "updated_date": "2024-07-17 05:43:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:52:27.431989"
    },
    {
      "arxiv_id": "2407.18957v4",
      "title": "When AI Meets Finance (StockAgent): Large Language Model-based Stock Trading in Simulated Real-world Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Chong Zhang",
        "Xinyi Liu",
        "Zhongmou Zhang",
        "Mingyu Jin",
        "Lingyao Li",
        "Zhenting Wang",
        "Wenyue Hua",
        "Dong Shu",
        "Suiyuan Zhu",
        "Xiaobo Jin",
        "Sujian Li",
        "Mengnan Du",
        "Yongfeng Zhang"
      ],
      "abstract": "Can AI Agents simulate real-world trading environments to investigate the\nimpact of external factors on stock trading activities (e.g., macroeconomics,\npolicy changes, company fundamentals, and global events)? These factors, which\nfrequently influence trading behaviors, are critical elements in the quest for\nmaximizing investors' profits. Our work attempts to solve this problem through\nlarge language model based agents. We have developed a multi-agent AI system\ncalled StockAgent, driven by LLMs, designed to simulate investors' trading\nbehaviors in response to the real stock market. The StockAgent allows users to\nevaluate the impact of different external factors on investor trading and to\nanalyze trading behavior and profitability effects. Additionally, StockAgent\navoids the test set leakage issue present in existing trading simulation\nsystems based on AI Agents. Specifically, it prevents the model from leveraging\nprior knowledge it may have acquired related to the test data. We evaluate\ndifferent LLMs under the framework of StockAgent in a stock trading environment\nthat closely resembles real-world conditions. The experimental results\ndemonstrate the impact of key external factors on stock market trading,\nincluding trading behavior and stock price fluctuation rules. This research\nexplores the study of agents' free trading gaps in the context of no prior\nknowledge related to market data. The patterns identified through StockAgent\nsimulations provide valuable insights for LLM-based investment advice and stock\nrecommendation. The code is available at\nhttps://github.com/MingyuJ666/Stockagent.",
      "tldr_zh": "本研究探讨了AI代理如何模拟真实交易环境，以分析外部因素（如宏观经济、政策变化、公司基础和全球事件）对股票交易的影响。研究开发了StockAgent，一种基于Large Language Model (LLMs)的多代理系统，能够模拟投资者行为，并评估这些因素对交易决策和盈利的影响，同时避免了现有系统中的测试集泄漏问题。实验结果显示，StockAgent在模拟真实世界条件下评估不同LLMs时，揭示了关键外部因素对交易行为和股价波动规则的影响。该框架为LLM-based的投资建议和股票推荐提供了宝贵洞见，并开源代码以供进一步研究。",
      "categories": [
        "q-fin.TR",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "q-fin.TR",
      "comment": "33 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.18957v4",
      "published_date": "2024-07-15 06:49:30 UTC",
      "updated_date": "2024-09-21 03:09:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:52:39.457492"
    },
    {
      "arxiv_id": "2407.10468v1",
      "title": "LiteFocus: Accelerated Diffusion Inference for Long Audio Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenxiong Tan",
        "Xinyin Ma",
        "Gongfan Fang",
        "Xinchao Wang"
      ],
      "abstract": "Latent diffusion models have shown promising results in audio generation,\nmaking notable advancements over traditional methods. However, their\nperformance, while impressive with short audio clips, faces challenges when\nextended to longer audio sequences. These challenges are due to model's\nself-attention mechanism and training predominantly on 10-second clips, which\ncomplicates the extension to longer audio without adaptation. In response to\nthese issues, we introduce a novel approach, LiteFocus that enhances the\ninference of existing audio latent diffusion models in long audio synthesis.\nObserved the attention pattern in self-attention, we employ a dual sparse form\nfor attention calculation, designated as same-frequency focus and\ncross-frequency compensation, which curtails the attention computation under\nsame-frequency constraints, while enhancing audio quality through\ncross-frequency refillment. LiteFocus demonstrates substantial reduction on\ninference time with diffusion-based TTA model by 1.99x in synthesizing\n80-second audio clips while also obtaining improved audio quality.",
      "tldr_zh": "本研究针对潜在扩散模型（Latent diffusion models）在长音频合成中面临的效率挑战（如self-attention机制和短剪辑训练限制），提出了一种新方法LiteFocus。该方法通过双稀疏注意力形式——same-frequency focus（减少同频率计算）和cross-frequency compensation（通过跨频率补充提升质量）——加速现有音频扩散模型的推理过程。实验结果显示，LiteFocus在合成80秒音频时，比基于扩散的TTA模型减少1.99倍推理时间，同时显著提高了音频质量。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Interspeech 2024; Code: https://github.com/Yuanshi9815/LiteFocus",
      "pdf_url": "http://arxiv.org/pdf/2407.10468v1",
      "published_date": "2024-07-15 06:49:05 UTC",
      "updated_date": "2024-07-15 06:49:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:52:51.609869"
    },
    {
      "arxiv_id": "2407.10462v1",
      "title": "BandControlNet: Parallel Transformers-based Steerable Popular Music Generation with Fine-Grained Spatiotemporal Features",
      "title_zh": "BandControlNet：基于并行 Transformers 的可引导流行音乐生成，结合细粒度时空特征",
      "authors": [
        "Jing Luo",
        "Xinyu Yang",
        "Dorien Herremans"
      ],
      "abstract": "Controllable music generation promotes the interaction between humans and\ncomposition systems by projecting the users' intent on their desired music. The\nchallenge of introducing controllability is an increasingly important issue in\nthe symbolic music generation field. When building controllable generative\npopular multi-instrument music systems, two main challenges typically present\nthemselves, namely weak controllability and poor music quality. To address\nthese issues, we first propose spatiotemporal features as powerful and\nfine-grained controls to enhance the controllability of the generative model.\nIn addition, an efficient music representation called REMI_Track is designed to\nconvert multitrack music into multiple parallel music sequences and shorten the\nsequence length of each track with Byte Pair Encoding (BPE) techniques.\nSubsequently, we release BandControlNet, a conditional model based on parallel\nTransformers, to tackle the multiple music sequences and generate high-quality\nmusic samples that are conditioned to the given spatiotemporal control\nfeatures. More concretely, the two specially designed modules of\nBandControlNet, namely structure-enhanced self-attention (SE-SA) and\nCross-Track Transformer (CTT), are utilized to strengthen the resulting musical\nstructure and inter-track harmony modeling respectively. Experimental results\ntested on two popular music datasets of different lengths demonstrate that the\nproposed BandControlNet outperforms other conditional music generation models\non most objective metrics in terms of fidelity and inference speed and shows\ngreat robustness in generating long music samples. The subjective evaluations\nshow BandControlNet trained on short datasets can generate music with\ncomparable quality to state-of-the-art models, while outperforming them\nsignificantly using longer datasets.",
      "tldr_zh": "本论文提出BandControlNet，一种基于并行Transformers的条件模型，用于可控流行音乐生成，通过引入细粒度的spatiotemporal features来提升模型的可控性和音乐质量。论文设计了REMI_Track音乐表示，将多轨音乐转换为多个并行序列，并利用Byte Pair Encoding (BPE)技术缩短序列长度；同时，引入structure-enhanced self-attention (SE-SA)和Cross-Track Transformer (CTT)模块，以加强音乐结构和轨间和谐。实验结果显示，BandControlNet在两个流行音乐数据集上，在保真度、推理速度和生成长样本的鲁棒性方面均优于其他模型，且主观评估表明其在长数据集上显著超越最先进模型。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Demo page: https://chinglohsiu.github.io/files/bandcontrolnet.html",
      "pdf_url": "http://arxiv.org/pdf/2407.10462v1",
      "published_date": "2024-07-15 06:33:25 UTC",
      "updated_date": "2024-07-15 06:33:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:53:04.973270"
    },
    {
      "arxiv_id": "2407.10457v1",
      "title": "The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism",
      "title_zh": "翻译失败",
      "authors": [
        "Yifan Song",
        "Guoyin Wang",
        "Sujian Li",
        "Bill Yuchen Lin"
      ],
      "abstract": "Current evaluations of large language models (LLMs) often overlook\nnon-determinism, typically focusing on a single output per example. This limits\nour understanding of LLM performance variability in real-world applications.\nOur study addresses this issue by exploring key questions about the performance\ndifferences between greedy decoding and sampling, identifying benchmarks'\nconsistency regarding non-determinism, and examining unique model behaviors.\nThrough extensive experiments, we observe that greedy decoding generally\noutperforms sampling methods for most evaluated tasks. We also observe\nconsistent performance across different LLM sizes and alignment methods, noting\nthat alignment can reduce sampling variance. Moreover, our best-of-N sampling\napproach demonstrates that smaller LLMs can match or surpass larger models such\nas GPT-4-Turbo, highlighting the untapped potential of smaller LLMs. This\nresearch shows the importance of considering non-determinism in LLM evaluations\nand provides insights for future LLM development and evaluation.",
      "tldr_zh": "当前对大型语言模型（LLMs）的评估通常忽略非确定性，仅关注单一输出，导致对性能变异性的理解不足。本研究通过广泛实验比较了贪婪解码（greedy decoding）和采样（sampling）方法，发现贪婪解码在大多数任务中表现更好，且不同LLM大小和对齐方法下性能相对一致，对齐能减少采样方差。此外，best-of-N采样方法使较小LLMs能够匹配或超过如GPT-4-Turbo的大模型，强调未来LLM开发和评估应充分考虑非确定性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10457v1",
      "published_date": "2024-07-15 06:12:17 UTC",
      "updated_date": "2024-07-15 06:12:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:53:16.216831"
    },
    {
      "arxiv_id": "2407.11098v3",
      "title": "Inertial Confinement Fusion Forecasting via Large Language Models",
      "title_zh": "通过大语言模型的惯性约束",
      "authors": [
        "Mingkai Chen",
        "Taowen Wang",
        "Shihui Cao",
        "James Chenhao Liang",
        "Chuan Liu",
        "Chunshu Wu",
        "Qifan Wang",
        "Ying Nian Wu",
        "Michael Huang",
        "Chuang Ren",
        "Ang Li",
        "Tong Geng",
        "Dongfang Liu"
      ],
      "abstract": "Controlled fusion energy is deemed pivotal for the advancement of human\ncivilization. In this study, we introduce $\\textbf{LPI-LLM}$, a novel\nintegration of Large Language Models (LLMs) with classical reservoir computing\nparadigms tailored to address a critical challenge, Laser-Plasma Instabilities\n($\\texttt{LPI}$), in Inertial Confinement Fusion ($\\texttt{ICF}$). Our approach\noffers several key contributions: Firstly, we propose the $\\textit{LLM-anchored\nReservoir}$, augmented with a $\\textit{Fusion-specific Prompt}$, enabling\naccurate forecasting of $\\texttt{LPI}$-generated-hot electron dynamics during\nimplosion. Secondly, we develop $\\textit{Signal-Digesting Channels}$ to\ntemporally and spatially describe the driver laser intensity across time,\ncapturing the unique characteristics of $\\texttt{ICF}$ inputs. Lastly, we\ndesign the $\\textit{Confidence Scanner}$ to quantify the confidence level in\nforecasting, providing valuable insights for domain experts to design the\n$\\texttt{ICF}$ process. Extensive experiments demonstrate the superior\nperformance of our method, achieving 1.90 CAE, 0.14 $\\texttt{top-1}$ MAE, and\n0.11 $\\texttt{top-5}$ MAE in predicting Hard X-ray ($\\texttt{HXR}$) energies\nemitted by the hot electrons in $\\texttt{ICF}$ implosions, which presents\nstate-of-the-art comparisons against concurrent best systems. Additionally, we\npresent $\\textbf{LPI4AI}$, the first $\\texttt{LPI}$ benchmark based on physical\nexperiments, aimed at fostering novel ideas in $\\texttt{LPI}$ research and\nenhancing the utility of LLMs in scientific exploration. Overall, our work\nstrives to forge an innovative synergy between AI and $\\texttt{ICF}$ for\nadvancing fusion energy.",
      "tldr_zh": "本研究提出LPI-LLM框架，将Large Language Models (LLMs)与reservoir computing相结合，用于预测Inertial Confinement Fusion (ICF)中的Laser-Plasma Instabilities (LPI)，以推动受控融合能源发展。关键创新包括LLM-anchored Reservoir结合Fusion-specific Prompt来准确预测LPI热电子动态、Signal-Digesting Channels用于描述激光强度的时空特性，以及Confidence Scanner量化预测置信度以辅助ICF设计。实验结果显示，该方法在预测Hard X-ray (HXR)能量时，达到1.90 CAE、0.14 top-1 MAE和0.11 top-5 MAE的优异性能，超越现有系统；此外，研究还发布了首个基于物理实验的LPI基准LPI4AI，促进AI在科学领域的应用和ICF研究创新。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.11098v3",
      "published_date": "2024-07-15 05:46:44 UTC",
      "updated_date": "2024-10-14 22:47:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:53:29.120989"
    },
    {
      "arxiv_id": "2407.10452v1",
      "title": "GraphPrint: Extracting Features from 3D Protein Structure for Drug Target Affinity Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Amritpal Singh"
      ],
      "abstract": "Accurate drug target affinity prediction can improve drug candidate\nselection, accelerate the drug discovery process, and reduce drug production\ncosts. Previous work focused on traditional fingerprints or used features\nextracted based on the amino acid sequence in the protein, ignoring its 3D\nstructure which affects its binding affinity. In this work, we propose\nGraphPrint: a framework for incorporating 3D protein structure features for\ndrug target affinity prediction. We generate graph representations for protein\n3D structures using amino acid residue location coordinates and combine them\nwith drug graph representation and traditional features to jointly learn drug\ntarget affinity. Our model achieves a mean square error of 0.1378 and a\nconcordance index of 0.8929 on the KIBA dataset and improves over using\ntraditional protein features alone. Our ablation study shows that the 3D\nprotein structure-based features provide information complementary to\ntraditional features.",
      "tldr_zh": "本研究提出GraphPrint框架，用于从蛋白质3D结构中提取特征，以提升药物靶点亲和力预测的准确性。GraphPrint通过利用氨基酸残基位置坐标生成蛋白质图表示，并将其与药物图表示和传统特征结合，进行联合学习，从而解决以往方法忽略3D结构问题的局限。实验结果显示，在KIBA数据集上，该模型的均方误差为0.1378，一致性指数为0.8929，比仅使用传统蛋白特征有显著改善；消融研究进一步证实，3D结构特征提供了与传统特征互补的信息。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted: The NeurIPS 2023 Workshop on New Frontiers of AI for Drug\n  Discovery and Development (AI4D3 2023), New Orleans, LA, USA, 2023",
      "pdf_url": "http://arxiv.org/pdf/2407.10452v1",
      "published_date": "2024-07-15 05:45:09 UTC",
      "updated_date": "2024-07-15 05:45:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:53:50.528830"
    },
    {
      "arxiv_id": "2407.10446v1",
      "title": "DDFAD: Dataset Distillation Framework for Audio Data",
      "title_zh": "DDFAD：音频数据的数据集蒸馏框架",
      "authors": [
        "Wenbo Jiang",
        "Rui Zhang",
        "Hongwei Li",
        "Xiaoyuan Liu",
        "Haomiao Yang",
        "Shui Yu"
      ],
      "abstract": "Deep neural networks (DNNs) have achieved significant success in numerous\napplications. The remarkable performance of DNNs is largely attributed to the\navailability of massive, high-quality training datasets. However, processing\nsuch massive training data requires huge computational and storage resources.\nDataset distillation is a promising solution to this problem, offering the\ncapability to compress a large dataset into a smaller distilled dataset. The\nmodel trained on the distilled dataset can achieve comparable performance to\nthe model trained on the whole dataset.\n  While dataset distillation has been demonstrated in image data, none have\nexplored dataset distillation for audio data. In this work, for the first time,\nwe propose a Dataset Distillation Framework for Audio Data (DDFAD).\nSpecifically, we first propose the Fused Differential MFCC (FD-MFCC) as\nextracted features for audio data. After that, the FD-MFCC is distilled through\nthe matching training trajectory distillation method. Finally, we propose an\naudio signal reconstruction algorithm based on the Griffin-Lim Algorithm to\nreconstruct the audio signal from the distilled FD-MFCC. Extensive experiments\ndemonstrate the effectiveness of DDFAD on various audio datasets. In addition,\nwe show that DDFAD has promising application prospects in many applications,\nsuch as continual learning and neural architecture search.",
      "tldr_zh": "该论文提出DDFAD（Dataset Distillation Framework for Audio Data），首次将数据集蒸馏技术应用于音频数据，以解决处理大规模训练数据集所需的大量计算和存储资源问题。框架包括提取Fused Differential MFCC (FD-MFCC)特征、使用matching training trajectory distillation method进行蒸馏，以及基于Griffin-Lim Algorithm的音频信号重建算法。实验结果显示，DDFAD在各种音频数据集上表现出色，并在continual learning和neural architecture search等应用中具有广阔前景。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.DB",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10446v1",
      "published_date": "2024-07-15 05:23:35 UTC",
      "updated_date": "2024-07-15 05:23:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:53:51.869449"
    },
    {
      "arxiv_id": "2407.10445v1",
      "title": "Backdoor Attacks against Image-to-Image Networks",
      "title_zh": "针对图像到图像网络的后门攻击",
      "authors": [
        "Wenbo Jiang",
        "Hongwei Li",
        "Jiaming He",
        "Rui Zhang",
        "Guowen Xu",
        "Tianwei Zhang",
        "Rongxing Lu"
      ],
      "abstract": "Recently, deep learning-based Image-to-Image (I2I) networks have become the\npredominant choice for I2I tasks such as image super-resolution and denoising.\nDespite their remarkable performance, the backdoor vulnerability of I2I\nnetworks has not been explored. To fill this research gap, we conduct a\ncomprehensive investigation on the susceptibility of I2I networks to backdoor\nattacks. Specifically, we propose a novel backdoor attack technique, where the\ncompromised I2I network behaves normally on clean input images, yet outputs a\npredefined image of the adversary for malicious input images containing the\ntrigger. To achieve this I2I backdoor attack, we propose a targeted universal\nadversarial perturbation (UAP) generation algorithm for I2I networks, where the\ngenerated UAP is used as the backdoor trigger. Additionally, in the backdoor\ntraining process that contains the main task and the backdoor task, multi-task\nlearning (MTL) with dynamic weighting methods is employed to accelerate\nconvergence rates. In addition to attacking I2I tasks, we extend our I2I\nbackdoor to attack downstream tasks, including image classification and object\ndetection. Extensive experiments demonstrate the effectiveness of the I2I\nbackdoor on state-of-the-art I2I network architectures, as well as the\nrobustness against different mainstream backdoor defenses.",
      "tldr_zh": "这篇论文首次探讨了Image-to-Image (I2I) 网络对后门攻击的易感性，提出了一种新颖的后门攻击技术，使I2I网络在正常输入时表现正常，但对包含触发器的恶意输入输出预定义图像。攻击方法利用targeted universal adversarial perturbation (UAP) 作为触发器，并在后门训练中使用multi-task learning (MTL) 结合动态权重加速收敛。实验结果表明，该攻击在多种先进I2I网络上有效，并能扩展到下游任务如图像分类和物体检测，同时对主流后门防御表现出鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10445v1",
      "published_date": "2024-07-15 05:14:17 UTC",
      "updated_date": "2024-07-15 05:14:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:54:04.287715"
    },
    {
      "arxiv_id": "2407.10441v1",
      "title": "Enhancing Building Safety Design for Active Shooter Incidents: Exploration of Building Exit Parameters using Reinforcement Learning-Based Simulations",
      "title_zh": "翻译失败",
      "authors": [
        "Ruying Liu",
        "Wanjing Wu",
        "Burcin Becerik-Gerber",
        "Gale M. Lucas"
      ],
      "abstract": "With the alarming rise in active shooter incidents (ASIs) in the United\nStates, enhancing public safety through building design has become a pressing\nneed. This study proposes a reinforcement learning-based simulation approach\naddressing gaps in existing research that has neglected the dynamic behaviours\nof shooters. We developed an autonomous agent to simulate an active shooter\nwithin a realistic office environment, aiming to offer insights into the\ninteractions between building design parameters and ASI outcomes. A case study\nis conducted to quantitatively investigate the impact of building exit numbers\n(total count of accessible exits) and configuration (arrangement of which exits\nare available or not) on evacuation and harm rates. Findings demonstrate that\ngreater exit availability significantly improves evacuation outcomes and\nreduces harm. Exits nearer to the shooter's initial position hold greater\nimportance for accessibility than those farther away. By encompassing dynamic\nshooter behaviours, this study offers preliminary insights into effective\nbuilding safety design against evolving threats.",
      "tldr_zh": "本研究针对美国活跃射手事件（Active Shooter Incidents）的上升趋势，提出了一种基于强化学习（Reinforcement Learning）的模拟方法，来评估建筑设计参数对公共安全的影响。研究开发了自主代理模拟活跃射手在真实办公室环境中的动态行为，并通过案例研究定量分析了出口数量（total count of accessible exits）和配置（arrangement of which exits are available or not）对疏散和伤害率的影响。结果显示，增加出口可用性可显著提升疏散效率并降低伤害率，而靠近射手初始位置的出口对可达性更为关键。该方法为应对动态威胁的建筑安全设计提供了初步见解。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10441v1",
      "published_date": "2024-07-15 05:08:38 UTC",
      "updated_date": "2024-07-15 05:08:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:54:16.617900"
    },
    {
      "arxiv_id": "2407.10433v1",
      "title": "A Multi-Stage Framework for 3D Individual Tooth Segmentation in Dental CBCT",
      "title_zh": "翻译失败",
      "authors": [
        "Chunshi Wang",
        "Bin Zhao",
        "Shuxue Ding"
      ],
      "abstract": "Cone beam computed tomography (CBCT) is a common way of diagnosing dental\nrelated diseases. Accurate segmentation of 3D tooth is of importance for the\ntreatment. Although deep learning based methods have achieved convincing\nresults in medical image processing, they need a large of annotated data for\nnetwork training, making it very time-consuming in data collection and\nannotation. Besides, domain shift widely existing in the distribution of data\nacquired by different devices impacts severely the model generalization. To\nresolve the problem, we propose a multi-stage framework for 3D tooth\nsegmentation in dental CBCT, which achieves the third place in the\n\"Semi-supervised Teeth Segmentation\" 3D (STS-3D) challenge. The experiments on\nvalidation set compared with other semi-supervised segmentation methods further\nindicate the validity of our approach.",
      "tldr_zh": "这篇论文提出了一种多阶段框架，用于在牙科 Cone Beam Computed Tomography (CBCT) 中实现 3D 单个牙齿分割，以解决深度学习方法对大量标注数据的依赖以及设备间领域偏移导致的模型泛化问题。框架通过半监督学习策略优化了数据利用效率，并在 \"Semi-supervised Teeth Segmentation\" 3D (STS-3D) 挑战赛中获得第三名。实验结果显示，该方法在验证集上与其他半监督分割方法相比表现出色，提升了牙科疾病诊断的准确性和实用性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Semi-supervised Tooth Segmentation MICCAI 2023 Challenge",
      "pdf_url": "http://arxiv.org/pdf/2407.10433v1",
      "published_date": "2024-07-15 04:23:28 UTC",
      "updated_date": "2024-07-15 04:23:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:54:31.654204"
    },
    {
      "arxiv_id": "2407.10430v1",
      "title": "Expanding the Scope: Inductive Knowledge Graph Reasoning with Multi-Starting Progressive Propagation",
      "title_zh": "扩展范围：利用多起始渐进传播的归纳知识图谱推理",
      "authors": [
        "Zhoutian Shao",
        "Yuanning Cui",
        "Wei Hu"
      ],
      "abstract": "Knowledge graphs (KGs) are widely acknowledged as incomplete, and new\nentities are constantly emerging in the real world. Inductive KG reasoning aims\nto predict missing facts for these new entities. Among existing models, graph\nneural networks (GNNs) based ones have shown promising performance for this\ntask. However, they are still challenged by inefficient message propagation due\nto the distance and scalability issues. In this paper, we propose a new\ninductive KG reasoning model, MStar, by leveraging conditional message passing\nneural networks (C-MPNNs). Our key insight is to select multiple query-specific\nstarting entities to expand the scope of progressive propagation. To propagate\nquery-related messages to a farther area within limited steps, we subsequently\ndesign a highway layer to propagate information toward these selected starting\nentities. Moreover, we introduce a training strategy called LinkVerify to\nmitigate the impact of noisy training samples. Experimental results validate\nthat MStar achieves superior performance compared with state-of-the-art models,\nespecially for distant entities.",
      "tldr_zh": "该论文针对知识图谱（KGs）的完整性问题和新兴实体挑战，提出了一种新的归纳式 KG 推理模型 MStar，以提升对新实体的缺失事实预测。MStar 基于条件消息传递神经网络（C-MPNNs），通过选择多个查询特定的起始实体来扩展渐进传播范围，并引入高速公路层（highway layer）以在有限步骤内传播更远的查询相关信息，同时采用 LinkVerify 训练策略减少噪声样本影响。实验结果显示，MStar 比现有最先进模型表现出色，尤其在处理远距离实体时，显著提高了推理性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in the 23rd International Semantic Web Conference (ISWC\n  2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.10430v1",
      "published_date": "2024-07-15 04:16:20 UTC",
      "updated_date": "2024-07-15 04:16:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:54:50.199156"
    },
    {
      "arxiv_id": "2407.10424v5",
      "title": "CodeV: Empowering LLMs with HDL Generation through Multi-Level Summarization",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Zhao",
        "Di Huang",
        "Chongxiao Li",
        "Pengwei Jin",
        "Muxin Song",
        "Yinan Xu",
        "Ziyuan Nan",
        "Mingju Gao",
        "Tianyun Ma",
        "Lei Qi",
        "Yansong Pan",
        "Zhenxing Zhang",
        "Rui Zhang",
        "Xishan Zhang",
        "Zidong Du",
        "Qi Guo",
        "Xing Hu"
      ],
      "abstract": "The design flow of processors, particularly in hardware description languages\n(HDL) like Verilog and Chisel, is complex and costly. While recent advances in\nlarge language models (LLMs) have significantly improved coding tasks in\nsoftware languages such as Python, their application in HDL generation remains\nlimited due to the scarcity of high-quality HDL data. Traditional methods of\nadapting LLMs for hardware design rely on synthetic HDL datasets, which often\nsuffer from low quality because even advanced LLMs like GPT perform poorly in\nthe HDL domain. Moreover, these methods focus solely on chat tasks and the\nVerilog language, limiting their application scenarios.\n  In this paper, we observe that: (1) HDL code collected from the real world is\nof higher quality than code generated by LLMs. (2) LLMs like GPT-3.5 excel in\nsummarizing HDL code rather than generating it. (3) An explicit language tag\ncan help LLMs better adapt to the target language when there is insufficient\ndata. Based on these observations, we propose an efficient LLM fine-tuning\npipeline for HDL generation that integrates a multi-level summarization data\nsynthesis process with a novel Chat-FIM-Tag supervised fine-tuning method. The\npipeline enhances the generation of HDL code from natural language descriptions\nand enables the handling of various tasks such as chat and infilling incomplete\ncode. Utilizing this pipeline, we introduce CodeV, a series of HDL generation\nLLMs. Among them, CodeV-All not only possesses a more diverse range of language\nabilities, i.e. Verilog and Chisel, and a broader scope of tasks, i.e. Chat and\nfill-in-middle (FIM), but it also achieves performance on VerilogEval that is\ncomparable to or even surpasses that of CodeV-Verilog fine-tuned on Verilog\nonly, making them the first series of open-source LLMs designed for\nmulti-scenario HDL generation.",
      "tldr_zh": "这篇论文针对硬件描述语言(HDL)如 Verilog 和 Chisel 的生成难题，提出了一种高效的 LLM 微调管道，利用多级总结数据合成过程和新型 Chat-FIM-Tag 监督细调方法，提升 LLMs 从自然语言描述生成高质量 HDL 代码的能力。基于观察，LLMs 更擅长总结真实 HDL 代码而非直接生成，并通过显式语言标签改善对目标语言的适应。最终，论文引入 CodeV 系列模型，其中 CodeV-All 支持多种语言和任务（如 Chat 和 fill-in-middle），并在 VerilogEval 测试中表现出色，性能媲美或超越专精 Verilog 的模型，成为首个开源的多场景 HDL 生成 LLM。",
      "categories": [
        "cs.PL",
        "cs.AI"
      ],
      "primary_category": "cs.PL",
      "comment": "13 pages, 10 figures, journal",
      "pdf_url": "http://arxiv.org/pdf/2407.10424v5",
      "published_date": "2024-07-15 03:57:20 UTC",
      "updated_date": "2025-05-11 07:35:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:55:13.600036"
    },
    {
      "arxiv_id": "2407.10420v1",
      "title": "Learning Rapid Turning, Aerial Reorientation, and Balancing using Manipulator as a Tail",
      "title_zh": "翻译失败",
      "authors": [
        "Insung Yang",
        "Jemin Hwangbo"
      ],
      "abstract": "In this research, we investigated the innovative use of a manipulator as a\ntail in quadruped robots to augment their physical capabilities. Previous\nstudies have primarily focused on enhancing various abilities by attaching\nrobotic tails that function solely as tails on quadruped robots. While these\ntails improve the performance of the robots, they come with several\ndisadvantages, such as increased overall weight and higher costs. To mitigate\nthese limitations, we propose the use of a 6-DoF manipulator as a tail,\nallowing it to serve both as a tail and as a manipulator. To control this\nhighly complex robot, we developed a controller based on reinforcement learning\nfor the robot equipped with the manipulator. Our experimental results\ndemonstrate that robots equipped with a manipulator outperform those without a\nmanipulator in tasks such as rapid turning, aerial reorientation, and\nbalancing. These results indicate that the manipulator can improve the agility\nand stability of quadruped robots, similar to a tail, in addition to its\nmanipulation capabilities.",
      "tldr_zh": "本文提出一种创新方法，使用6-DoF manipulator作为尾巴来增强四足机器人的物理能力，从而避免传统尾巴带来的重量和成本问题。研究团队开发了基于reinforcement learning的控制器，用于管理这一复杂机器人系统。实验结果表明，配备manipulator的机器人在rapid turning、aerial reorientation和balancing任务中表现优于无manipulator的机器人，显著提高了敏捷性和稳定性，同时保留了manipulator的操纵功能。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10420v1",
      "published_date": "2024-07-15 03:51:19 UTC",
      "updated_date": "2024-07-15 03:51:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:55:24.586365"
    },
    {
      "arxiv_id": "2407.11096v1",
      "title": "Static and multivariate-temporal attentive fusion transformer for readmission risk prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Zhe Sun",
        "Runzhi Li",
        "Jing Wang",
        "Gang Chen",
        "Siyu Yan",
        "Lihong Ma"
      ],
      "abstract": "Background: Accurate short-term readmission prediction of ICU patients is\nsignificant in improving the efficiency of resource assignment by assisting\nphysicians in making discharge decisions. Clinically, both individual static\nstatic and multivariate temporal data collected from ICU monitors play critical\nroles in short-term readmission prediction. Informative static and multivariate\ntemporal feature representation capturing and fusion present challenges for\naccurate readmission prediction. Methods:We propose a novel static and\nmultivariate-temporal attentive fusion transformer (SMTAFormer) to predict\nshort-term readmission of ICU patients by fully leveraging the potential of\ndemographic and dynamic temporal data. In SMTAFormer, we first apply an MLP\nnetwork and a temporal transformer network to learn useful static and temporal\nfeature representations, respectively. Then, the well-designed static and\nmultivariate temporal feature fusion module is applied to fuse static and\ntemporal feature representations by modeling intra-correlation among\nmultivariate temporal features and constructing inter-correlation between\nstatic and multivariate temporal features. Results: We construct a readmission\nrisk assessment (RRA) dataset based on the MIMIC-III dataset. The extensive\nexperiments show that SMTAFormer outperforms advanced methods, in which the\naccuracy of our proposed method is up to 86.6%, and the area under the receiver\noperating characteristic curve (AUC) is up to 0.717. Conclusion: Our proposed\nSMTAFormer can efficiently capture and fuse static and multivariate temporal\nfeature representations. The results show that SMTAFormer significantly\nimproves the short-term readmission prediction performance of ICU patients\nthrough comparisons to strong baselines.",
      "tldr_zh": "本研究提出了一种名为 Static and multivariate-temporal attentive fusion transformer (SMTAFormer) 的模型，用于准确预测 ICU 患者的短期再入院风险，通过融合静态数据（如人口统计学特征）和多变量时间序列数据（如监测指标）。SMTAFormer 采用 MLP 网络学习静态特征、Transformer 网络学习时间特征，并通过一个专设计融合模块捕捉多变量时间特征的内部相关性和静态与时间特征的外部相关性。实验基于 MIMIC-III 数据集构建的 RRA 数据集显示，该模型的准确率达到 86.6%，AUC 达 0.717，比现有方法显著提升，证明了其在特征捕捉和融合方面的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.11096v1",
      "published_date": "2024-07-15 03:42:44 UTC",
      "updated_date": "2024-07-15 03:42:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:55:27.160026"
    },
    {
      "arxiv_id": "2407.10413v1",
      "title": "Melon Fruit Detection and Quality Assessment Using Generative AI-Based Image Data Augmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Seungri Yoon",
        "Yunseong Cho",
        "Tae In Ahn"
      ],
      "abstract": "Monitoring and managing the growth and quality of fruits are very important\ntasks. To effectively train deep learning models like YOLO for real-time fruit\ndetection, high-quality image datasets are essential. However, such datasets\nare often lacking in agriculture. Generative AI models can help create\nhigh-quality images. In this study, we used MidJourney and Firefly tools to\ngenerate images of melon greenhouses and post-harvest fruits through\ntext-to-image, pre-harvest image-to-image, and post-harvest image-to-image\nmethods. We evaluated these AIgenerated images using PSNR and SSIM metrics and\ntested the detection performance of the YOLOv9 model. We also assessed the net\nquality of real and generated fruits. Our results showed that generative AI\ncould produce images very similar to real ones, especially for post-harvest\nfruits. The YOLOv9 model detected the generated images well, and the net\nquality was also measurable. This shows that generative AI can create realistic\nimages useful for fruit detection and quality assessment, indicating its great\npotential in agriculture. This study highlights the potential of AI-generated\nimages for data augmentation in melon fruit detection and quality assessment\nand envisions a positive future for generative AI applications in agriculture.",
      "tldr_zh": "这篇论文探讨了使用生成式 AI（如 MidJourney 和 Firefly）进行图像数据增强，以解决农业中瓜类水果检测和质量评估数据集不足的问题。研究者通过文本到图像、预收获图像到图像和后收获图像到图像的方法生成图像，并使用 PSNR 和 SSIM 指标评估其质量，同时测试 YOLOv9 模型的检测性能。结果表明，生成的图像与真实图像高度相似，尤其在后收获水果上，YOLOv9 检测准确性良好，这突显了生成式 AI 在农业数据增强中的巨大潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.10413v1",
      "published_date": "2024-07-15 03:26:13 UTC",
      "updated_date": "2024-07-15 03:26:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:55:39.644763"
    },
    {
      "arxiv_id": "2407.10403v1",
      "title": "Cooperative Reward Shaping for Multi-Agent Pathfinding",
      "title_zh": "用于多智能体路径规划的合作奖励整形",
      "authors": [
        "Zhenyu Song",
        "Ronghao Zheng",
        "Senlin Zhang",
        "Meiqin Liu"
      ],
      "abstract": "The primary objective of Multi-Agent Pathfinding (MAPF) is to plan efficient\nand conflict-free paths for all agents. Traditional multi-agent path planning\nalgorithms struggle to achieve efficient distributed path planning for multiple\nagents. In contrast, Multi-Agent Reinforcement Learning (MARL) has been\ndemonstrated as an effective approach to achieve this objective. By modeling\nthe MAPF problem as a MARL problem, agents can achieve efficient path planning\nand collision avoidance through distributed strategies under partial\nobservation. However, MARL strategies often lack cooperation among agents due\nto the absence of global information, which subsequently leads to reduced MAPF\nefficiency. To address this challenge, this letter introduces a unique reward\nshaping technique based on Independent Q-Learning (IQL). The aim of this method\nis to evaluate the influence of one agent on its neighbors and integrate such\nan interaction into the reward function, leading to active cooperation among\nagents. This reward shaping method facilitates cooperation among agents while\noperating in a distributed manner. The proposed approach has been evaluated\nthrough experiments across various scenarios with different scales and agent\ncounts. The results are compared with those from other state-of-the-art (SOTA)\nplanners. The evidence suggests that the approach proposed in this letter\nparallels other planners in numerous aspects, and outperforms them in scenarios\nfeaturing a large number of agents.",
      "tldr_zh": "这篇论文针对多智能体路径规划 (MAPF) 的挑战，提出了一种基于 Independent Q-Learning (IQL) 的合作奖励整形技术，以解决多智能体强化学习 (MARL) 中代理间缺乏合作的问题。方法通过评估每个代理对邻居的影响并将其整合到奖励函数中，促进分布式策略下的主动合作，从而实现高效、无冲突的路径规划。实验在不同规模和代理数量的场景中验证了该方法的有效性，与现有最先进 (SOTA) 规划器相比，在大量代理情况下表现出色。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages,9 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.10403v1",
      "published_date": "2024-07-15 02:44:41 UTC",
      "updated_date": "2024-07-15 02:44:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:55:50.466711"
    },
    {
      "arxiv_id": "2407.11095v1",
      "title": "DeepGate3: Towards Scalable Circuit Representation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengyuan Shi",
        "Ziyang Zheng",
        "Sadaf Khan",
        "Jianyuan Zhong",
        "Min Li",
        "Qiang Xu"
      ],
      "abstract": "Circuit representation learning has shown promising results in advancing the\nfield of Electronic Design Automation (EDA). Existing models, such as DeepGate\nFamily, primarily utilize Graph Neural Networks (GNNs) to encode circuit\nnetlists into gate-level embeddings. However, the scalability of GNN-based\nmodels is fundamentally constrained by architectural limitations, impacting\ntheir ability to generalize across diverse and complex circuit designs. To\naddress these challenges, we introduce DeepGate3, an enhanced architecture that\nintegrates Transformer modules following the initial GNN processing. This novel\narchitecture not only retains the robust gate-level representation capabilities\nof its predecessor, DeepGate2, but also enhances them with the ability to model\nsubcircuits through a novel pooling transformer mechanism. DeepGate3 is further\nrefined with multiple innovative supervision tasks, significantly enhancing its\nlearning process and enabling superior representation of both gate-level and\nsubcircuit structures. Our experiments demonstrate marked improvements in\nscalability and generalizability over traditional GNN-based approaches,\nestablishing a significant step forward in circuit representation learning\ntechnology.",
      "tldr_zh": "本研究针对电路表示学习在电子设计自动化(EDA)领域的应用，指出现有基于Graph Neural Networks (GNNs)的模型（如DeepGate Family）在可扩展性和泛化能力上存在局限。DeepGate3作为新架构，在初始GNN处理后整合Transformer模块，并引入新型pooling transformer机制和多种创新监督任务，以提升门级和子电路结构的表示学习。实验结果显示，DeepGate3显著改善了可扩展性和泛化性能，超越传统GNN方法，推动了电路表示学习技术的进步。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.11095v1",
      "published_date": "2024-07-15 02:44:21 UTC",
      "updated_date": "2024-07-15 02:44:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:56:02.728793"
    },
    {
      "arxiv_id": "2407.10387v1",
      "title": "Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity",
      "title_zh": "翻译失败",
      "authors": [
        "Santiago Pascual",
        "Chunghsin Yeh",
        "Ioannis Tsiamas",
        "Joan Serrà"
      ],
      "abstract": "Video-to-audio (V2A) generation leverages visual-only video features to\nrender plausible sounds that match the scene. Importantly, the generated sound\nonsets should match the visual actions that are aligned with them, otherwise\nunnatural synchronization artifacts arise. Recent works have explored the\nprogression of conditioning sound generators on still images and then video\nfeatures, focusing on quality and semantic matching while ignoring\nsynchronization, or by sacrificing some amount of quality to focus on improving\nsynchronization only. In this work, we propose a V2A generative model, named\nMaskVAT, that interconnects a full-band high-quality general audio codec with a\nsequence-to-sequence masked generative model. This combination allows modeling\nboth high audio quality, semantic matching, and temporal synchronicity at the\nsame time. Our results show that, by combining a high-quality codec with the\nproper pre-trained audio-visual features and a sequence-to-sequence parallel\nstructure, we are able to yield highly synchronized results on one hand, whilst\nbeing competitive with the state of the art of non-codec generative audio\nmodels. Sample videos and generated audios are available at\nhttps://maskvat.github.io .",
      "tldr_zh": "本文提出 MaskVAT 模型，用于 Video-to-Audio (V2A) 生成，旨在解决现有方法在音频质量、语义匹配和时间同步之间的权衡问题。MaskVAT 通过整合全频带高质量音频编解码器与序列到序列掩码生成模型，同时利用预训练的音频-视觉特征，实现高同步性和竞争性音频输出。实验结果显示，该模型在同步性上显著提升，并与最先进非编解码器生成模型在整体性能上相当。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.10387v1",
      "published_date": "2024-07-15 01:49:59 UTC",
      "updated_date": "2024-07-15 01:49:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:56:14.812725"
    },
    {
      "arxiv_id": "2407.10385v2",
      "title": "By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting",
      "title_zh": "翻译失败",
      "authors": [
        "Hyungjun Yoon",
        "Biniyam Aschalew Tolera",
        "Taesik Gong",
        "Kimin Lee",
        "Sung-Ju Lee"
      ],
      "abstract": "Large language models (LLMs) have demonstrated exceptional abilities across\nvarious domains. However, utilizing LLMs for ubiquitous sensing applications\nremains challenging as existing text-prompt methods show significant\nperformance degradation when handling long sensor data sequences. We propose a\nvisual prompting approach for sensor data using multimodal LLMs (MLLMs). We\ndesign a visual prompt that directs MLLMs to utilize visualized sensor data\nalongside the target sensory task descriptions. Additionally, we introduce a\nvisualization generator that automates the creation of optimal visualizations\ntailored to a given sensory task, eliminating the need for prior task-specific\nknowledge. We evaluated our approach on nine sensory tasks involving four\nsensing modalities, achieving an average of 10% higher accuracy than text-based\nprompts and reducing token costs by 15.8 times. Our findings highlight the\neffectiveness and cost-efficiency of visual prompts with MLLMs for various\nsensory tasks. The source code is available at\nhttps://github.com/diamond264/ByMyEyes.",
      "tldr_zh": "本论文提出“By My Eyes”方法，通过visual prompting将传感器数据可视化，与多模态大型语言模型(MLLMs)结合，解决LLMs在处理长序列传感器数据时性能下降的问题。该方法设计了视觉提示来指导MLLMs使用可视化数据和任务描述，并引入一个visualization generator自动生成针对特定感官任务的最优可视化，无需先验知识。在九个涉及四种感官模态的任务上，实验显示该方法比文本提示准确率平均提高10%，并将令牌成本减少15.8倍，证明了其在感官任务中的高效性和成本优势。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to the 2024 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2024) Main",
      "pdf_url": "http://arxiv.org/pdf/2407.10385v2",
      "published_date": "2024-07-15 01:33:54 UTC",
      "updated_date": "2024-09-29 06:53:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:56:26.590338"
    },
    {
      "arxiv_id": "2407.10382v2",
      "title": "Communication- and Computation-Efficient Distributed Submodular Optimization in Robot Mesh Networks",
      "title_zh": "通信与计算高效的分布式子模优化在机器人网格网络中",
      "authors": [
        "Zirui Xu",
        "Sandilya Sai Garimella",
        "Vasileios Tzoumas"
      ],
      "abstract": "We provide a communication- and computation-efficient method for distributed\nsubmodular optimization in robot mesh networks. Submodularity is a property of\ndiminishing returns that arises in active information gathering such as\nmapping, surveillance, and target tracking. Our method, Resource-Aware\ndistributed Greedy (RAG), introduces a new distributed optimization paradigm\nthat enables scalable and near-optimal action coordination. To this end, RAG\nrequires each robot to make decisions based only on information received from\nand about their neighbors. In contrast, the current paradigms allow the relay\nof information about all robots across the network. As a result, RAG's\ndecision-time scales linearly with the network size, while state-of-the-art\nnear-optimal submodular optimization algorithms scale cubically. We also\ncharacterize how the designed mesh-network topology affects RAG's approximation\nperformance. Our analysis implies that sparser networks favor scalability\nwithout proportionally compromising approximation performance: while RAG's\ndecision time scales linearly with network size, the gain in approximation\nperformance scales sublinearly. We demonstrate RAG's performance in simulated\nscenarios of area detection with up to 45 robots, simulating realistic\nrobot-to-robot (r2r) communication speeds such as the 0.25 Mbps speed of the\nDigi XBee 3 Zigbee 3.0. In the simulations, RAG enables real-time planning, up\nto three orders of magnitude faster than competitive near-optimal algorithms,\nwhile also achieving superior mean coverage performance. To enable the\nsimulations, we extend the high-fidelity and photo-realistic simulator AirSim\nby integrating a scalable collaborative autonomy pipeline to tens of robots and\nsimulating r2r communication delays. Our code is available at\nhttps://github.com/UM-iRaL/Resource-Aware-Coordination-AirSim.",
      "tldr_zh": "该论文提出了一种通信和计算高效的分布式次模优化方法（distributed submodular optimization），名为 RAG（Resource-Aware distributed Greedy），适用于机器人网格网络（robot mesh networks），以支持信息收集任务如映射、监视和目标跟踪。RAG 创新性地让每个机器人仅基于邻居信息做出决策，避免全网信息中继，从而使决策时间线性增长，而竞争算法则呈立方增长。实验在模拟环境中验证了 RAG 的性能，使用多达 45 个机器人时，实现实时规划，比近优算法快三个数量级，并显著提升平均覆盖性能。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA",
        "cs.SY",
        "eess.SY",
        "math.OC"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10382v2",
      "published_date": "2024-07-15 01:25:39 UTC",
      "updated_date": "2025-01-31 18:22:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:56:39.692990"
    },
    {
      "arxiv_id": "2407.17429v2",
      "title": "How Do Students Interact with an LLM-powered Virtual Teaching Assistant in Different Educational Settings?",
      "title_zh": "翻译失败",
      "authors": [
        "Pratyusha Maiti",
        "Ashok K. Goel"
      ],
      "abstract": "Jill Watson, a virtual teaching assistant powered by LLMs, answers student\nquestions and engages them in extended conversations on courseware provided by\nthe instructors. In this paper, we analyze student interactions with Jill\nacross multiple courses and colleges, focusing on the types and complexity of\nstudent questions based on Bloom's Revised Taxonomy and tool usage patterns. We\nfind that, by supporting a wide range of cognitive demands, Jill encourages\nstudents to engage in sophisticated, higher-order cognitive questions. However,\nthe frequency of usage varies significantly across deployments, and the types\nof questions asked depend on course-specific contexts. These findings pave the\nway for future work on AI-driven educational tools tailored to individual\nlearning styles and course structure, potentially enhancing both the teaching\nand learning experience in classrooms.",
      "tldr_zh": "本研究探讨了学生如何与基于大型语言模型(LLMs)的虚拟教学助理Jill Watson互动，分析了多个课程和学院中的学生问题类型、复杂性及工具使用模式，使用Bloom's Revised Taxonomy作为评估框架。结果显示，Jill Watson支持广泛的认知需求，促使学生提出更多高级认知问题，但其使用频率和问题类型因课程背景而显著不同。这些发现为开发个性化AI驱动教育工具提供了指导，有望提升课堂教学和学习体验。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted in the Seventeenth International Conference on Educational\n  Data Mining (EDM) Workshop: Leveraging LLMs for Next Generation Educational\n  Technologies, July 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.17429v2",
      "published_date": "2024-07-15 01:22:50 UTC",
      "updated_date": "2024-07-25 20:16:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:56:49.647488"
    },
    {
      "arxiv_id": "2407.10380v3",
      "title": "NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models",
      "title_zh": "NTSEBENCH：视觉语言模型的认知推理基准",
      "authors": [
        "Pranshu Pandya",
        "Vatsal Gupta",
        "Agney S Talwarr",
        "Tushar Kataria",
        "Dan Roth",
        "Vivek Gupta"
      ],
      "abstract": "Cognitive textual and visual reasoning tasks, including puzzles, series, and\nanalogies, demand the ability to quickly reason, decipher, and evaluate\npatterns both textually and spatially. Due to extensive training on vast\namounts of human-curated data, LLMs and VLMs excel in common-sense reasoning\ntasks, however still struggle with more complex reasoning that demands deeper\ncognitive understanding. We introduce NTSEBench, a new dataset designed to\nevaluate cognitive multi-modal reasoning and problem-solving skills of large\nmodels. The dataset contains 2728 multiple-choice questions, accompanied by a\ntotal of 4,642 images, categorized into 26 different types. These questions are\ndrawn from the nationwide NTSE examination in India and feature a mix of visual\nand textual general aptitude challenges, designed to assess intelligence and\ncritical thinking skills beyond mere rote learning. We establish baselines on\nthe dataset using state-of-the-art LLMs and VLMs. To facilitate a comparison\nbetween open source and propriety models, we propose four distinct modeling\nstrategies to handle different modalities -- text and images -- in the dataset\ninstances.",
      "tldr_zh": "该论文引入了 NTSEBench，这是一个用于评估视觉语言模型 (VLMs) 的认知推理基准数据集，专注于文本和视觉任务如谜题、序列和类比，以测试模型的深度认知理解能力。该数据集包含 2728 个多选题和 4642 张图像，分为 26 种类型，源自印度的 NTSE 考试，旨在考察超越死记硬背的智能和批判性思维。研究者使用最先进的 LLMs 和 VLMs 建立了基线，并提出了四种不同的建模策略来处理文本和图像模态，便于比较开源和专有模型。实验结果显示，尽管 VLMs 在常见感推理上表现出色，但它们在更复杂的认知任务中仍面临挑战。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.CV",
      "comment": "28 pages, 3 figures, 12 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.10380v3",
      "published_date": "2024-07-15 01:21:56 UTC",
      "updated_date": "2025-04-01 17:25:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:57:05.683725"
    },
    {
      "arxiv_id": "2407.10377v4",
      "title": "Enhanced Masked Image Modeling to Avoid Model Collapse on Multi-modal MRI Datasets",
      "title_zh": "翻译失败",
      "authors": [
        "Linxuan Han",
        "Sa Xiao",
        "Zimeng Li",
        "Haidong Li",
        "Xiuchao Zhao",
        "Yeqing Han",
        "Fumin Guo",
        "Xin Zhou"
      ],
      "abstract": "Multi-modal magnetic resonance imaging (MRI) provides information of lesions\nfor computer-aided diagnosis from different views. Deep learning algorithms are\nsuitable for identifying specific anatomical structures, segmenting lesions,\nand classifying diseases. Manual labels are limited due to the high expense,\nwhich hinders further improvement of accuracy. Self-supervised learning,\nparticularly masked image modeling (MIM), has shown promise in utilizing\nunlabeled data. However, we spot model collapse when applying MIM to\nmulti-modal MRI datasets. The performance of downstream tasks does not see any\nimprovement following the collapsed model. To solve model collapse, we analyze\nand address it in two types: complete collapse and dimensional collapse. We\nfind complete collapse occurs because the collapsed loss value in multi-modal\nMRI datasets falls below the normally converged loss value. Based on this, the\nhybrid mask pattern (HMP) masking strategy is introduced to elevate the\ncollapsed loss above the normally converged loss value and avoid complete\ncollapse. Additionally, we reveal that dimensional collapse stems from\ninsufficient feature uniformity in MIM. We mitigate dimensional collapse by\nintroducing the pyramid barlow twins (PBT) module as an explicit regularization\nmethod. Overall, we construct the enhanced MIM (E-MIM) with HMP and PBT module\nto avoid model collapse multi-modal MRI. Experiments are conducted on three\nmulti-modal MRI datasets to validate the effectiveness of our approach in\npreventing both types of model collapse. By preventing model collapse, the\ntraining of the model becomes more stable, resulting in a decent improvement in\nperformance for segmentation and classification tasks. The code is available at\nhttps://github.com/LinxuanHan/E-MIM.",
      "tldr_zh": "本研究针对多模态 MRI 数据集上 Masked Image Modeling (MIM) 的 Model Collapse 问题，提出了一种增强方法，以解决自监督学习在利用无标签数据时的性能瓶颈。研究分析了两种 Model Collapse 类型：Complete Collapse（通过引入 Hybrid Mask Pattern (HMP) 掩码策略提升损失值以避免）和 Dimensional Collapse（通过 Pyramid Barlow Twins (PBT) 模块作为显式正则化方法改善特征均匀性）。基于此，构建了 Enhanced MIM (E-MIM) 框架，将 HMP 和 PBT 整合，以提高模型在多模态 MRI 上的稳定性和有效性。在三个数据集上的实验表明，该方法成功防止了 Model Collapse，并显著提升了分割和分类任务的性能。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "This work has been submitted to the lEEE for possible publication.\n  copyright may be transferred without notice, after which this version may no\n  longer be accessible",
      "pdf_url": "http://arxiv.org/pdf/2407.10377v4",
      "published_date": "2024-07-15 01:11:30 UTC",
      "updated_date": "2025-01-16 01:30:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:57:14.826905"
    },
    {
      "arxiv_id": "2407.10374v2",
      "title": "An Empirical Study of Mamba-based Pedestrian Attribute Recognition",
      "title_zh": "基于 Mamba 的行人属性识别实证研究",
      "authors": [
        "Xiao Wang",
        "Weizhe Kong",
        "Jiandong Jin",
        "Shiao Wang",
        "Ruichong Gao",
        "Qingchuan Ma",
        "Chenglong Li",
        "Jin Tang"
      ],
      "abstract": "Current strong pedestrian attribute recognition models are developed based on\nTransformer networks, which are computationally heavy. Recently proposed models\nwith linear complexity (e.g., Mamba) have garnered significant attention and\nhave achieved a good balance between accuracy and computational cost across a\nvariety of visual tasks. Relevant review articles also suggest that while these\nmodels can perform well on some pedestrian attribute recognition datasets, they\nare generally weaker than the corresponding Transformer models. To further tap\ninto the potential of the novel Mamba architecture for PAR tasks, this paper\ndesigns and adapts Mamba into two typical PAR frameworks, i.e., the text-image\nfusion approach and pure vision Mamba multi-label recognition framework. It is\nfound that interacting with attribute tags as additional input does not always\nlead to an improvement, specifically, Vim can be enhanced, but VMamba cannot.\nThis paper further designs various hybrid Mamba-Transformer variants and\nconducts thorough experimental validations. These experimental results indicate\nthat simply enhancing Mamba with a Transformer does not always lead to\nperformance improvements but yields better results under certain settings. We\nhope this empirical study can further inspire research in Mamba for PAR, and\neven extend into the domain of multi-label recognition, through the design of\nthese network structures and comprehensive experimentation. The source code of\nthis work will be released at \\url{https://github.com/Event-AHU/OpenPAR}",
      "tldr_zh": "本研究对基于Mamba架构的行人属性识别（PAR）进行了实证研究，旨在探索Mamba在准确性和计算成本平衡方面的潜力，将其适配到文本-图像融合方法和纯视觉Mamba多标签识别框架中。实验发现，与属性标签互动作为额外输入并不总是提升性能，例如Vim可获益，但VMamba则无显著改善；此外，设计各种Mamba-Transformer混合变体后，结果显示这种结合在特定设置下能提高表现，但并非普遍有效。该工作通过全面实验验证，为Mamba在PAR和多标签识别领域的进一步研究提供了新灵感，并将源代码开源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "In Peer Review",
      "pdf_url": "http://arxiv.org/pdf/2407.10374v2",
      "published_date": "2024-07-15 00:48:06 UTC",
      "updated_date": "2024-12-03 04:25:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:57:27.728183"
    },
    {
      "arxiv_id": "2407.10373v1",
      "title": "Mutual Learning for Acoustic Matching and Dereverberation via Visual Scene-driven Diffusion",
      "title_zh": "翻译失败",
      "authors": [
        "Jian Ma",
        "Wenguan Wang",
        "Yi Yang",
        "Feng Zheng"
      ],
      "abstract": "Visual acoustic matching (VAM) is pivotal for enhancing the immersive\nexperience, and the task of dereverberation is effective in improving audio\nintelligibility. Existing methods treat each task independently, overlooking\nthe inherent reciprocity between them. Moreover, these methods depend on paired\ntraining data, which is challenging to acquire, impeding the utilization of\nextensive unpaired data. In this paper, we introduce MVSD, a mutual learning\nframework based on diffusion models. MVSD considers the two tasks\nsymmetrically, exploiting the reciprocal relationship to facilitate learning\nfrom inverse tasks and overcome data scarcity. Furthermore, we employ the\ndiffusion model as foundational conditional converters to circumvent the\ntraining instability and over-smoothing drawbacks of conventional GAN\narchitectures. Specifically, MVSD employs two converters: one for VAM called\nreverberator and one for dereverberation called dereverberator. The\ndereverberator judges whether the reverberation audio generated by reverberator\nsounds like being in the conditional visual scenario, and vice versa. By\nforming a closed loop, these two converters can generate informative feedback\nsignals to optimize the inverse tasks, even with easily acquired one-way\nunpaired data. Extensive experiments on two standard benchmarks, i.e.,\nSoundSpaces-Speech and Acoustic AVSpeech, exhibit that our framework can\nimprove the performance of the reverberator and dereverberator and better match\nspecified visual scenarios.",
      "tldr_zh": "本研究提出 MVSD 框架，通过视觉场景驱动的 diffusion 模型，实现视觉声学匹配 (VAM) 和去混响 (dereverberation) 任务的互惠学习，解决现有方法独立处理任务并依赖配对数据的局限。MVSD 采用对称设计，利用任务间的互惠关系，形成闭环优化，其中 reverberator 和 dereverberator 作为条件转换器，通过生成反馈信号来从反向任务中学习，即使使用单向非配对数据也能有效提升性能。实验在 SoundSpaces-Speech 和 Acoustic AVSpeech 基准上显示，该框架显著提高了 VAM 和去混响的准确性，并更好地匹配指定视觉场景。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "ECCV 2024; Project page: https://hechang25.github.io/MVSD",
      "pdf_url": "http://arxiv.org/pdf/2407.10373v1",
      "published_date": "2024-07-15 00:47:56 UTC",
      "updated_date": "2024-07-15 00:47:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:57:38.001046"
    },
    {
      "arxiv_id": "2407.10366v2",
      "title": "Accessing Vision Foundation Models via ImageNet-1K",
      "title_zh": "通过 ImageNet-1K 访问视觉基础模型",
      "authors": [
        "Yitian Zhang",
        "Xu Ma",
        "Yue Bai",
        "Huan Wang",
        "Yun Fu"
      ],
      "abstract": "Vision foundation models are renowned for the generalization ability due to\nmassive training data. Nevertheless, they demand tremendous training resources,\nand the training data is often inaccessible, e.g., CLIP, DINOv2, posing great\nchallenges to developing derivatives that could facilitate the research. In\nthis work, we offer a very simple and general solution, named \\textit{Proteus},\nto distill foundation models into smaller equivalents on ImageNet-1K without\naccess to the original training data. Specifically, we remove the designs from\nconventional knowledge distillation settings that result in dataset bias and\npresent three levels of training objectives, i.e., token, patch, and feature,\nto maximize the efficacy of knowledge transfer. In this manner, Proteus is\ntrained at ImageNet-level costs with surprising ability, facilitating the\naccessibility of training foundation models for the broader research community.\nWhen leveraging DINOv2-g/14 as the teacher, Proteus-L/14 matches the\nperformance of the Oracle method DINOv2-L/14 (142M training data) across 19\nbenchmarks and outperforms other vision foundation models including CLIP-L/14\n(400M), OpenCLIP-L/14 (400M/2B) and SynCLR-L/14 (600M) with a significantly\nsmaller training set of 1.2M images.",
      "tldr_zh": "该研究解决了视觉基础模型（Vision Foundation Models）的训练资源和数据访问挑战，提出了一种简单通用的方法Proteus，通过在ImageNet-1K数据集上蒸馏这些模型，而无需原始训练数据。具体来说，Proteus去除传统知识蒸馏（knowledge distillation）中的数据集偏差，并引入token、patch和feature三个级别的训练目标，以最大化知识转移效率。实验结果显示，当使用DINOv2-g/14作为老师模型时，Proteus-L/14在19个基准上匹配了使用142M数据的Oracle方法DINOv2-L/14的性能，并优于CLIP-L/14等模型，尽管仅使用1.2M图像的训练集，从而提升了视觉基础模型的易访问性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICLR2025",
      "pdf_url": "http://arxiv.org/pdf/2407.10366v2",
      "published_date": "2024-07-15 00:13:53 UTC",
      "updated_date": "2025-02-11 18:44:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:58:01.286474"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 125,
  "processed_papers_count": 125,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T06:58:32.594023"
}
[
  {
    "arxiv_id": "2407.11286v1",
    "title": "CLAMS: A System for Zero-Shot Model Selection for Clustering",
    "authors": [
      "Prabhant Singh",
      "Pieter Gijsbers",
      "Murat Onur Yildirim",
      "Elif Ceren Gok",
      "Joaquin Vanschoren"
    ],
    "abstract": "We propose an AutoML system that enables model selection on clustering\nproblems by leveraging optimal transport-based dataset similarity. Our\nobjective is to establish a comprehensive AutoML pipeline for clustering\nproblems and provide recommendations for selecting the most suitable\nalgorithms, thus opening up a new area of AutoML beyond the traditional\nsupervised learning settings. We compare our results against multiple\nclustering baselines and find that it outperforms all of them, hence\ndemonstrating the utility of similarity-based automated model selection for\nsolving clustering applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11286v1",
    "published_date": "2024-07-15 23:50:07 UTC",
    "updated_date": "2024-07-15 23:50:07 UTC"
  },
  {
    "arxiv_id": "2407.11280v1",
    "title": "Intelligent Cross-Organizational Process Mining: A Survey and New Perspectives",
    "authors": [
      "Yiyuan Yang",
      "Zheshun Wu",
      "Yong Chu",
      "Zhenghua Chen",
      "Zenglin Xu",
      "Qingsong Wen"
    ],
    "abstract": "Process mining, as a high-level field in data mining, plays a crucial role in\nenhancing operational efficiency and decision-making across organizations. In\nthis survey paper, we delve into the growing significance and ongoing trends in\nthe field of process mining, advocating a specific viewpoint on its contents,\napplication, and development in modern businesses and process management,\nparticularly in cross-organizational settings. We first summarize the framework\nof process mining, common industrial applications, and the latest advances\ncombined with artificial intelligence, such as workflow optimization,\ncompliance checking, and performance analysis. Then, we propose a holistic\nframework for intelligent process analysis and outline initial methodologies in\ncross-organizational settings, highlighting both challenges and opportunities.\nThis particular perspective aims to revolutionize process mining by leveraging\nartificial intelligence to offer sophisticated solutions for complex,\nmulti-organizational data analysis. By integrating advanced machine learning\ntechniques, we can enhance predictive capabilities, streamline processes, and\nfacilitate real-time decision-making. Furthermore, we pinpoint avenues for\nfuture investigations within the research community, encouraging the\nexploration of innovative algorithms, data integration strategies, and\nprivacy-preserving methods to fully harness the potential of process mining in\ndiverse, interconnected business environments.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review; 13 pages, 7 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.11280v1",
    "published_date": "2024-07-15 23:30:34 UTC",
    "updated_date": "2024-07-15 23:30:34 UTC"
  },
  {
    "arxiv_id": "2407.12062v2",
    "title": "Multistep Brent Oil Price Forecasting with a Multi-Aspect Meta-heuristic Optimization and Ensemble Deep Learning Model",
    "authors": [
      "Mohammed Alruqimi",
      "Luca Di Persio"
    ],
    "abstract": "Accurate crude oil price forecasting is crucial for various economic\nactivities, including energy trading, risk management, and investment planning.\nAlthough deep learning models have emerged as powerful tools for crude oil\nprice forecasting, achieving accurate forecasts remains challenging. Deep\nlearning models' performance is heavily influenced by hyperparameters tuning,\nand they are expected to perform differently under various circumstances.\nFurthermore, price volatility is also sensitive to external factors such as\nworld events. To address these limitations, we propose a hybrid approach that\nintegrates metaheuristic optimization with an ensemble of five widely used\nneural network architectures for time series forecasting. Unlike existing\nmethods that apply metaheuristics to optimise hyperparameters within the neural\nnetwork architecture, we exploit the GWO metaheuristic optimiser at four\nlevels: feature selection, data preparation, model training, and forecast\nblending. The proposed approach has been evaluated for forecasting three-ahead\ndays using real-world Brent crude oil price data, and the obtained results\ndemonstrate that the proposed approach improves the forecasting performance\nmeasured using various benchmarks, achieving 0.000127 of MSE.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12062v2",
    "published_date": "2024-07-15 22:27:14 UTC",
    "updated_date": "2024-12-14 07:53:46 UTC"
  },
  {
    "arxiv_id": "2407.11260v1",
    "title": "Quality Scalable Quantization Methodology for Deep Learning on Edge",
    "authors": [
      "Salman Abdul Khaliq",
      "Rehan Hafiz"
    ],
    "abstract": "Deep Learning Architectures employ heavy computations and bulk of the\ncomputational energy is taken up by the convolution operations in the\nConvolutional Neural Networks. The objective of our proposed work is to reduce\nthe energy consumption and size of CNN for using machine learning techniques in\nedge computing on ubiquitous computing devices. We propose Systematic Quality\nScalable Design Methodology consisting of Quality Scalable Quantization on a\nhigher abstraction level and Quality Scalable Multipliers at lower abstraction\nlevel. The first component consists of parameter compression where we\napproximate representation of values in filters of deep learning models by\nencoding in 3 bits. A shift and scale based on-chip decoding hardware is\nproposed which can decode these 3-bit representations to recover approximate\nfilter values. The size of the DNN model is reduced this way and can be sent\nover a communication channel to be decoded on the edge computing devices. This\nway power is reduced by limiting data bits by approximation. In the second\ncomponent we propose a quality scalable multiplier which reduces the number of\npartial products by converting numbers in canonic sign digit representations\nand further approximating the number by reducing least significant bits. These\nquantized CNNs provide almost same ac-curacy as network with original weights\nwith little or no fine-tuning. The hardware for the adaptive multipliers\nutilize gate clocking for reducing energy consumption during multiplications.\nThe proposed methodology greatly reduces the memory and power requirements of\nDNN models making it a feasible approach to deploy Deep Learning on edge\ncomputing. The experiments done on LeNet and ConvNets show an increase upto 6%\nof zeros and memory savings upto 82.4919% while keeping the accuracy near the\nstate of the art.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11260v1",
    "published_date": "2024-07-15 22:00:29 UTC",
    "updated_date": "2024-07-15 22:00:29 UTC"
  },
  {
    "arxiv_id": "2407.11254v1",
    "title": "Conquering images and the basis of transformative action",
    "authors": [
      "Hunter Priniski"
    ],
    "abstract": "Our rapid immersion into online life has made us all ill. Through the\ngeneration, personalization, and dissemination of enchanting imagery,\nartificial technologies commodify the minds and hearts of the masses with\nnauseating precision and scale. Online networks, artificial intelligence (AI),\nsocial media, and digital news feeds fine-tune our beliefs and pursuits by\nestablishing narratives that subdivide and polarize our communities and\nidentities. Meanwhile those commanding these technologies conquer the final\nfrontiers of our interior lives, social relations, earth, and cosmos. In the\nAttention Economy, our agency is restricted and our vitality is depleted for\ntheir narcissistic pursuits and pleasures. Generative AI empowers the forces\nthat homogenize and eradicate life, not through some stupid \"singularity\"\nevent, but through devaluing human creativity, labor, and social life. Using a\nfractured lens, we will examine how narratives and networks influence us on\nmental, social, and algorithmic levels. We will discuss how atomizing imagery\n-- ideals and pursuits that alienate, rather than invigorate the individual --\nhijack people's agency to sustain the forces that destroy them. We will\ndiscover how empires build digital networks that optimize society and embolden\nnarcissists to enforce social binaries that perpetuate the ceaseless expansion\nof consumption, exploitation, and hierarchy. Structural hierarchy in the world\nis reified through hierarchy in our beliefs and thinking. Only by seeing images\nas images and appreciating the similarity shared by opposing narratives can we\nfacilitate transformative action and break away from the militaristic systems\nplaguing our lives.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET",
      "cs.SI"
    ],
    "primary_category": "cs.CY",
    "comment": "I would like to thank Nick Ichien for his feedback on this\n  manuscript. This is a working manuscript and is bound to change in the coming\n  months",
    "pdf_url": "http://arxiv.org/pdf/2407.11254v1",
    "published_date": "2024-07-15 21:45:52 UTC",
    "updated_date": "2024-07-15 21:45:52 UTC"
  },
  {
    "arxiv_id": "2407.11249v3",
    "title": "Disentangling Representations through Multi-task Learning",
    "authors": [
      "Pantelis Vafidis",
      "Aman Bhargava",
      "Antonio Rangel"
    ],
    "abstract": "Intelligent perception and interaction with the world hinges on internal\nrepresentations that capture its underlying structure (''disentangled'' or\n''abstract'' representations). Disentangled representations serve as world\nmodels, isolating latent factors of variation in the world along approximately\northogonal directions, thus facilitating feature-based generalization. We\nprovide experimental and theoretical results guaranteeing the emergence of\ndisentangled representations in agents that optimally solve multi-task evidence\naccumulation classification tasks, canonical in the neuroscience literature.\nThe key conceptual finding is that, by producing accurate multi-task\nclassification estimates, a system implicitly represents a set of coordinates\nspecifying a disentangled representation of the underlying latent state of the\ndata it receives. The theory provides conditions for the emergence of these\nrepresentations in terms of noise, number of tasks, and evidence accumulation\ntime. We experimentally validate these predictions in RNNs trained to\nmulti-task, which learn disentangled representations in the form of continuous\nattractors, leading to zero-shot out-of-distribution (OOD) generalization in\npredicting latent factors. We demonstrate the robustness of our framework\nacross autoregressive architectures, decision boundary geometries and in tasks\nrequiring classification confidence estimation. We find that transformers are\nparticularly suited for disentangling representations, which might explain\ntheir unique world understanding abilities. Overall, our framework establishes\na formal link between competence at multiple tasks and the formation of\ndisentangled, interpretable world models in both biological and artificial\nsystems, and helps explain why ANNs often arrive at human-interpretable\nconcepts, and how they both may acquire exceptional zero-shot generalization\ncapabilities.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.NC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "43 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.11249v3",
    "published_date": "2024-07-15 21:32:58 UTC",
    "updated_date": "2025-03-02 22:12:01 UTC"
  },
  {
    "arxiv_id": "2407.11245v2",
    "title": "Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation",
    "authors": [
      "Chung Park",
      "Taesan Kim",
      "Hyungjun Yoon",
      "Junui Hong",
      "Yelim Yu",
      "Mincheol Cho",
      "Minsung Choi",
      "Jaegul Choo"
    ],
    "abstract": "Cross-Domain Sequential Recommendation (CDSR) improves recommendation\nperformance by utilizing information from multiple domains, which contrasts\nwith Single-Domain Sequential Recommendation (SDSR) that relies on a historical\ninteraction within a specific domain. However, CDSR may underperform compared\nto the SDSR approach in certain domains due to negative transfer, which occurs\nwhen there is a lack of relation between domains or different levels of data\nsparsity. To address the issue of negative transfer, our proposed CDSR model\nestimates the degree of negative transfer of each domain and adaptively assigns\nit as a weight factor to the prediction loss, to control gradient flows through\ndomains with significant negative transfer. To this end, our model compares the\nperformance of a model trained on multiple domains (CDSR) with a model trained\nsolely on the specific domain (SDSR) to evaluate the negative transfer of each\ndomain using our asymmetric cooperative network. In addition, to facilitate the\ntransfer of valuable cues between the SDSR and CDSR tasks, we developed an\nauxiliary loss that maximizes the mutual information between the representation\npairs from both tasks on a per-domain basis. This cooperative learning between\nSDSR and CDSR tasks is similar to the collaborative dynamics between pacers and\nrunners in a marathon. Our model outperformed numerous previous works in\nextensive experiments on two real-world industrial datasets across ten service\ndomains. We also have deployed our model in the recommendation system of our\npersonal assistant app service, resulting in 21.4% increase in click-through\nrate compared to existing models, which is valuable to real-world business.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted at SIGIR'24 (Best Paper Honorable Mention)",
    "pdf_url": "http://arxiv.org/pdf/2407.11245v2",
    "published_date": "2024-07-15 21:14:13 UTC",
    "updated_date": "2024-07-24 11:54:26 UTC"
  },
  {
    "arxiv_id": "2407.11240v1",
    "title": "Making New Connections: LLMs as Puzzle Generators for The New York Times' Connections Word Game",
    "authors": [
      "Tim Merino",
      "Sam Earle",
      "Ryan Sudhakaran",
      "Shyam Sudhakaran",
      "Julian Togelius"
    ],
    "abstract": "The Connections puzzle is a word association game published daily by The New\nYork Times (NYT). In this game, players are asked to find groups of four words\nthat are connected by a common theme. While solving a given Connections puzzle\nrequires both semantic knowledge and abstract reasoning, generating novel\npuzzles additionally requires a form of metacognition: generators must be able\nto accurately model the downstream reasoning of potential solvers. In this\npaper, we investigate the ability of the GPT family of Large Language Models\n(LLMs) to generate challenging and creative word games for human players. We\nstart with an analysis of the word game Connections and the unique challenges\nit poses as a Procedural Content Generation (PCG) domain. We then propose a\nmethod for generating Connections puzzles using LLMs by adapting a Tree of\nThoughts (ToT) prompting approach. We evaluate this method by conducting a user\nstudy, asking human players to compare AI-generated puzzles against published\nConnections puzzles. Our findings show that LLMs are capable puzzle creators,\nand can generate diverse sets of enjoyable, challenging, and creative\nConnections puzzles as judged by human users.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11240v1",
    "published_date": "2024-07-15 21:05:25 UTC",
    "updated_date": "2024-07-15 21:05:25 UTC"
  },
  {
    "arxiv_id": "2407.11229v2",
    "title": "Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into Consistency and Robustness",
    "authors": [
      "Srija Mukhopadhyay",
      "Adnan Qidwai",
      "Aparna Garimella",
      "Pritika Ramu",
      "Vivek Gupta",
      "Dan Roth"
    ],
    "abstract": "Chart question answering (CQA) is a crucial area of Visual Language\nUnderstanding. However, the robustness and consistency of current Visual\nLanguage Models (VLMs) in this field remain under-explored. This paper\nevaluates state-of-the-art VLMs on comprehensive datasets, developed\nspecifically for this study, encompassing diverse question categories and chart\nformats. We investigate two key aspects: 1) the models' ability to handle\nvarying levels of chart and question complexity, and 2) their robustness across\ndifferent visual representations of the same underlying data. Our analysis\nreveals significant performance variations based on question and chart types,\nhighlighting both strengths and weaknesses of current models. Additionally, we\nidentify areas for improvement and propose future research directions to build\nmore robust and reliable CQA systems. This study sheds light on the limitations\nof current models and paves the way for future advancements in the field.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 9 Tables, 5 figures, 22 examples",
    "pdf_url": "http://arxiv.org/pdf/2407.11229v2",
    "published_date": "2024-07-15 20:29:24 UTC",
    "updated_date": "2024-10-04 16:52:57 UTC"
  },
  {
    "arxiv_id": "2407.11217v2",
    "title": "Almost-linear Time Approximation Algorithm to Euclidean $k$-median and $k$-means",
    "authors": [
      "Max Dupré la Tour",
      "David Saulpic"
    ],
    "abstract": "Clustering is one of the staples of data analysis and unsupervised learning.\nAs such, clustering algorithms are often used on massive data sets, and they\nneed to be extremely fast. We focus on the Euclidean $k$-median and $k$-means\nproblems, two of the standard ways to model the task of clustering.\n  For these, the go-to algorithm is $k$-means++, which yields an $O(\\log\nk)$-approximation in time $\\tilde O(nkd)$. While it is possible to improve\neither the approximation factor [Lattanzi and Sohler, ICML19] or the running\ntime [Cohen-Addad et al., NeurIPS 20], it is unknown how precise a linear-time\nalgorithm can be.\n  In this paper, we almost answer this question by presenting an almost\nlinear-time algorithm to compute a constant-factor approximation.",
    "categories": [
      "cs.DS",
      "cs.AI"
    ],
    "primary_category": "cs.DS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11217v2",
    "published_date": "2024-07-15 20:04:06 UTC",
    "updated_date": "2024-12-18 19:53:03 UTC"
  },
  {
    "arxiv_id": "2407.11215v2",
    "title": "Mechanistic interpretability of large language models with applications to the financial services industry",
    "authors": [
      "Ashkan Golgoon",
      "Khashayar Filom",
      "Arjun Ravi Kannan"
    ],
    "abstract": "Large Language Models such as GPTs (Generative Pre-trained Transformers)\nexhibit remarkable capabilities across a broad spectrum of applications.\nNevertheless, due to their intrinsic complexity, these models present\nsubstantial challenges in interpreting their internal decision-making\nprocesses. This lack of transparency poses critical challenges when it comes to\ntheir adaptation by financial institutions, where concerns and accountability\nregarding bias, fairness, and reliability are of paramount importance.\nMechanistic interpretability aims at reverse engineering complex AI models such\nas transformers. In this paper, we are pioneering the use of mechanistic\ninterpretability to shed some light on the inner workings of large language\nmodels for use in financial services applications. We offer several examples of\nhow algorithmic tasks can be designed for compliance monitoring purposes. In\nparticular, we investigate GPT-2 Small's attention pattern when prompted to\nidentify potential violation of Fair Lending laws. Using direct logit\nattribution, we study the contributions of each layer and its corresponding\nattention heads to the logit difference in the residual stream. Finally, we\ndesign clean and corrupted prompts and use activation patching as a causal\nintervention method to localize our task completion components further. We\nobserve that the (positive) heads $10.2$ (head $2$, layer $10$), $10.7$, and\n$11.3$, as well as the (negative) heads $9.6$ and $10.6$ play a significant\nrole in the task completion.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "cs.CL",
      "cs.NA",
      "math.NA",
      "68T01",
      "I.2.7"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11215v2",
    "published_date": "2024-07-15 19:59:53 UTC",
    "updated_date": "2024-10-16 02:40:53 UTC"
  },
  {
    "arxiv_id": "2407.11214v2",
    "title": "PutnamBench: Evaluating Neural Theorem-Provers on the Putnam Mathematical Competition",
    "authors": [
      "George Tsoukalas",
      "Jasper Lee",
      "John Jennings",
      "Jimmy Xin",
      "Michelle Ding",
      "Michael Jennings",
      "Amitayush Thakur",
      "Swarat Chaudhuri"
    ],
    "abstract": "We present PutnamBench, a new multi-language benchmark for evaluating the\nability of neural theorem-provers to solve competition mathematics problems.\nPutnamBench consists of 1692 hand-constructed formalizations of 640 theorems\nsourced from the William Lowell Putnam Mathematical Competition, the premier\nundergraduate-level mathematics competition in North America. All the problems\nhave formalizations in Lean 4 and Isabelle; a substantial subset also has Coq\nformalizations. PutnamBench requires significant problem-solving ability and\nproficiency in a broad range of topics taught in undergraduate mathematics\ncourses. We use PutnamBench to evaluate several established neural and symbolic\ntheorem-provers. These approaches can only solve a handful of the PutnamBench\nproblems, establishing the benchmark as a difficult open challenge for research\non neural theorem-proving. PutnamBench is available at\nhttps://github.com/trishullab/PutnamBench.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.LO",
      "cs.PL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at NeurIPS 2024 Datasets & Benchmarks Track",
    "pdf_url": "http://arxiv.org/pdf/2407.11214v2",
    "published_date": "2024-07-15 19:57:15 UTC",
    "updated_date": "2024-11-03 17:14:37 UTC"
  },
  {
    "arxiv_id": "2407.11212v1",
    "title": "Automated essay scoring in Arabic: a dataset and analysis of a BERT-based system",
    "authors": [
      "Rayed Ghazawi",
      "Edwin Simpson"
    ],
    "abstract": "Automated Essay Scoring (AES) holds significant promise in the field of\neducation, helping educators to mark larger volumes of essays and provide\ntimely feedback. However, Arabic AES research has been limited by the lack of\npublicly available essay data. This study introduces AR-AES, an Arabic AES\nbenchmark dataset comprising 2046 undergraduate essays, including gender\ninformation, scores, and transparent rubric-based evaluation guidelines,\nproviding comprehensive insights into the scoring process. These essays come\nfrom four diverse courses, covering both traditional and online exams.\nAdditionally, we pioneer the use of AraBERT for AES, exploring its performance\non different question types. We find encouraging results, particularly for\nEnvironmental Chemistry and source-dependent essay questions. For the first\ntime, we examine the scale of errors made by a BERT-based AES system, observing\nthat 96.15 percent of the errors are within one point of the first human\nmarker's prediction, on a scale of one to five, with 79.49 percent of\npredictions matching exactly. In contrast, additional human markers did not\nexceed 30 percent exact matches with the first marker, with 62.9 percent within\none mark. These findings highlight the subjectivity inherent in essay grading,\nand underscore the potential for current AES technology to assist human markers\nto grade consistently across large classes.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11212v1",
    "published_date": "2024-07-15 19:55:37 UTC",
    "updated_date": "2024-07-15 19:55:37 UTC"
  },
  {
    "arxiv_id": "2407.11211v4",
    "title": "Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer from Text to Image via CLIP Inversion",
    "authors": [
      "Philipp Allgeuer",
      "Kyra Ahrens",
      "Stefan Wermter"
    ],
    "abstract": "We introduce NOVIC, an innovative real-time uNconstrained Open Vocabulary\nImage Classifier that uses an autoregressive transformer to generatively output\nclassification labels as language. Leveraging the extensive knowledge of CLIP\nmodels, NOVIC harnesses the embedding space to enable zero-shot transfer from\npure text to images. Traditional CLIP models, despite their ability for open\nvocabulary classification, require an exhaustive prompt of potential class\nlabels, restricting their application to images of known content or context. To\naddress this, we propose an \"object decoder\" model that is trained on a\nlarge-scale 92M-target dataset of templated object noun sets and LLM-generated\ncaptions to always output the object noun in question. This effectively inverts\nthe CLIP text encoder and allows textual object labels from essentially the\nentire English language to be generated directly from image-derived embedding\nvectors, without requiring any a priori knowledge of the potential content of\nan image, and without any label biases. The trained decoders are tested on a\nmix of manually and web-curated datasets, as well as standard image\nclassification benchmarks, and achieve fine-grained prompt-free prediction\nscores of up to 87.5%, a strong result considering the model must work for any\nconceivable image and without any contextual clues.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at WACV 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.11211v4",
    "published_date": "2024-07-15 19:53:02 UTC",
    "updated_date": "2024-11-26 09:28:35 UTC"
  },
  {
    "arxiv_id": "2407.11204v2",
    "title": "PupilSense: A Novel Application for Webcam-Based Pupil Diameter Estimation",
    "authors": [
      "Vijul Shah",
      "Ko Watanabe",
      "Brian B. Moser",
      "Andreas Dengel"
    ],
    "abstract": "Measuring pupil diameter is vital for gaining insights into physiological and\npsychological states - traditionally captured by expensive, specialized\nequipment like Tobii eye-trackers and Pupillabs glasses. This paper presents a\nnovel application that enables pupil diameter estimation using standard\nwebcams, making the process accessible in everyday environments without\nspecialized equipment. Our app estimates pupil diameters from videos and offers\ndetailed analysis, including class activation maps, graphs of predicted left\nand right pupil diameters, and eye aspect ratios during blinks. This tool\nexpands the accessibility of pupil diameter measurement, particularly in\neveryday settings, benefiting fields like human behavior research and\nhealthcare. Additionally, we present a new open source dataset for pupil\ndiameter estimation using webcam images containing cropped eye images and\ncorresponding pupil diameter measurements.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11204v2",
    "published_date": "2024-07-15 19:39:28 UTC",
    "updated_date": "2025-03-29 01:19:17 UTC"
  },
  {
    "arxiv_id": "2407.12061v1",
    "title": "Situated Instruction Following",
    "authors": [
      "So Yeon Min",
      "Xavi Puig",
      "Devendra Singh Chaplot",
      "Tsung-Yen Yang",
      "Akshara Rai",
      "Priyam Parashar",
      "Ruslan Salakhutdinov",
      "Yonatan Bisk",
      "Roozbeh Mottaghi"
    ],
    "abstract": "Language is never spoken in a vacuum. It is expressed, comprehended, and\ncontextualized within the holistic backdrop of the speaker's history, actions,\nand environment. Since humans are used to communicating efficiently with\nsituated language, the practicality of robotic assistants hinge on their\nability to understand and act upon implicit and situated instructions. In\ntraditional instruction following paradigms, the agent acts alone in an empty\nhouse, leading to language use that is both simplified and artificially\n\"complete.\" In contrast, we propose situated instruction following, which\nembraces the inherent underspecification and ambiguity of real-world\ncommunication with the physical presence of a human speaker. The meaning of\nsituated instructions naturally unfold through the past actions and the\nexpected future behaviors of the human involved. Specifically, within our\nsettings we have instructions that (1) are ambiguously specified, (2) have\ntemporally evolving intent, (3) can be interpreted more precisely with the\nagent's dynamic actions. Our experiments indicate that state-of-the-art\nEmbodied Instruction Following (EIF) models lack holistic understanding of\nsituated human intention.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.HC",
    "comment": "European Conference on Computer Vision 2024 (ECCV 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.12061v1",
    "published_date": "2024-07-15 19:32:30 UTC",
    "updated_date": "2024-07-15 19:32:30 UTC"
  },
  {
    "arxiv_id": "2407.11194v2",
    "title": "AstroMLab 1: Who Wins Astronomy Jeopardy!?",
    "authors": [
      "Yuan-Sen Ting",
      "Tuan Dung Nguyen",
      "Tirthankar Ghosal",
      "Rui Pan",
      "Hardik Arora",
      "Zechang Sun",
      "Tijmen de Haan",
      "Nesar Ramachandra",
      "Azton Wells",
      "Sandeep Madireddy",
      "Alberto Accomazzi"
    ],
    "abstract": "We present a comprehensive evaluation of proprietary and open-weights large\nlanguage models using the first astronomy-specific benchmarking dataset. This\ndataset comprises 4,425 multiple-choice questions curated from the Annual\nReview of Astronomy and Astrophysics, covering a broad range of astrophysical\ntopics. Our analysis examines model performance across various astronomical\nsubfields and assesses response calibration, crucial for potential deployment\nin research environments. Claude-3.5-Sonnet outperforms competitors by up to\n4.6 percentage points, achieving 85.0% accuracy. For proprietary models, we\nobserved a universal reduction in cost every 3-to-12 months to achieve similar\nscore in this particular astronomy benchmark. open-weights models have rapidly\nimproved, with LLaMA-3-70b (80.6%) and Qwen-2-72b (77.7%) now competing with\nsome of the best proprietary models. We identify performance variations across\ntopics, with non-English-focused models generally struggling more in\nexoplanet-related fields, stellar astrophysics, and instrumentation related\nquestions. These challenges likely stem from less abundant training data,\nlimited historical context, and rapid recent developments in these areas. This\npattern is observed across both open-weights and proprietary models, with\nregional dependencies evident, highlighting the impact of training data\ndiversity on model performance in specialized scientific domains.\nTop-performing models demonstrate well-calibrated confidence, with correlations\nabove 0.9 between confidence and correctness, though they tend to be slightly\nunderconfident. The development for fast, low-cost inference of open-weights\nmodels presents new opportunities for affordable deployment in astronomy. The\nrapid progress observed suggests that LLM-driven research in astronomy may\nbecome feasible in the near future.",
    "categories": [
      "astro-ph.IM",
      "astro-ph.EP",
      "astro-ph.GA",
      "astro-ph.SR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "45 pages, 12 figures, 7 tables. Published in Astronomy & Computing.\n  AstroMLab homepage: https://astromlab.org/",
    "pdf_url": "http://arxiv.org/pdf/2407.11194v2",
    "published_date": "2024-07-15 19:28:14 UTC",
    "updated_date": "2024-11-08 22:00:26 UTC"
  },
  {
    "arxiv_id": "2407.20240v2",
    "title": "Social and Ethical Risks Posed by General-Purpose LLMs for Settling Newcomers in Canada",
    "authors": [
      "Isar Nejadgholi",
      "Maryam Molamohammadi",
      "Samir Bakhtawar"
    ],
    "abstract": "The non-profit settlement sector in Canada supports newcomers in achieving\nsuccessful integration. This sector faces increasing operational pressures\namidst rising immigration targets, which highlights a need for enhanced\nefficiency and innovation, potentially through reliable AI solutions. The\nad-hoc use of general-purpose generative AI, such as ChatGPT, might become a\ncommon practice among newcomers and service providers to address this need.\nHowever, these tools are not tailored for the settlement domain and can have\ndetrimental implications for immigrants and refugees. We explore the risks that\nthese tools might pose on newcomers to first, warn against the unguarded use of\ngenerative AI, and second, to incentivize further research and development in\ncreating AI literacy programs as well as customized LLMs that are aligned with\nthe preferences of the impacted communities. Crucially, such technologies\nshould be designed to integrate seamlessly into the existing workflow of the\nsettlement sector, ensuring human oversight, trustworthiness, and\naccountability.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "I.2.1, I.2.7"
    ],
    "primary_category": "cs.CY",
    "comment": "26 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.20240v2",
    "published_date": "2024-07-15 19:23:06 UTC",
    "updated_date": "2024-08-01 19:44:38 UTC"
  },
  {
    "arxiv_id": "2407.11186v4",
    "title": "Empowering Persian LLMs for Instruction Following: A Novel Dataset and Training Approach",
    "authors": [
      "Hojjat Mokhtarabadi",
      "Ziba Zamani",
      "Abbas Maazallahi",
      "Mohammad Hossein Manshaei"
    ],
    "abstract": "Instruction-tuned large language models have demonstrated remarkable\ncapabilities in following human instructions across various domains. However,\ntheir proficiency remains notably deficient in many low-resource languages. To\naddress this challenge, we begin by introducing FarsInstruct a comprehensive\ninstruction dataset designed to enhance the instruction following ability of\nlarge language models specifically for the Persian language a significant yet\nunderrepresented language globally. FarsInstruct encompasses a wide range of\ntask types and datasets, each containing a mix of straightforward to complex\nmanual written instructions, as well as translations from the Public Pool of\nPrompts, ensuring a rich linguistic and cultural representation. Furthermore,\nwe introduce Co-CoLA, a framework designed to enhance the multi-task\nadaptability of LoRA-tuned models. Through extensive experimental analyses, our\nstudy showcases the effectiveness of the FarsInstruct dataset coupled with\ntraining by the Co-CoLA framework, in improving the performance of large\nlanguage models within the Persian context. As of the current writing,\nFarsInstruct comprises 197 templates across 21 distinct datasets, and we intend\nto update it consistently, thus augmenting its applicability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11186v4",
    "published_date": "2024-07-15 19:17:31 UTC",
    "updated_date": "2025-01-14 22:07:53 UTC"
  },
  {
    "arxiv_id": "2407.11174v1",
    "title": "iHuman: Instant Animatable Digital Humans From Monocular Videos",
    "authors": [
      "Pramish Paudel",
      "Anubhav Khanal",
      "Ajad Chhatkuli",
      "Danda Pani Paudel",
      "Jyoti Tandukar"
    ],
    "abstract": "Personalized 3D avatars require an animatable representation of digital\nhumans. Doing so instantly from monocular videos offers scalability to broad\nclass of users and wide-scale applications. In this paper, we present a fast,\nsimple, yet effective method for creating animatable 3D digital humans from\nmonocular videos. Our method utilizes the efficiency of Gaussian splatting to\nmodel both 3D geometry and appearance. However, we observed that naively\noptimizing Gaussian splats results in inaccurate geometry, thereby leading to\npoor animations. This work achieves and illustrates the need of accurate 3D\nmesh-type modelling of the human body for animatable digitization through\nGaussian splats. This is achieved by developing a novel pipeline that benefits\nfrom three key aspects: (a) implicit modelling of surface's displacements and\nthe color's spherical harmonics; (b) binding of 3D Gaussians to the respective\ntriangular faces of the body template; (c) a novel technique to render normals\nfollowed by their auxiliary supervision. Our exhaustive experiments on three\ndifferent benchmark datasets demonstrates the state-of-the-art results of our\nmethod, in limited time settings. In fact, our method is faster by an order of\nmagnitude (in terms of training time) than its closest competitor. At the same\ntime, we achieve superior rendering and 3D reconstruction performance under the\nchange of poses.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, eccv, 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.11174v1",
    "published_date": "2024-07-15 18:51:51 UTC",
    "updated_date": "2024-07-15 18:51:51 UTC"
  },
  {
    "arxiv_id": "2408.00769v1",
    "title": "Decoding AI and Human Authorship: Nuances Revealed Through NLP and Statistical Analysis",
    "authors": [
      "Mayowa Akinwande",
      "Oluwaseyi Adeliyi",
      "Toyyibat Yussuph"
    ],
    "abstract": "This research explores the nuanced differences in texts produced by AI and\nthose written by humans, aiming to elucidate how language is expressed\ndifferently by AI and humans. Through comprehensive statistical data analysis,\nthe study investigates various linguistic traits, patterns of creativity, and\npotential biases inherent in human-written and AI- generated texts. The\nsignificance of this research lies in its contribution to understanding AI's\ncreative capabilities and its impact on literature, communication, and societal\nframeworks. By examining a meticulously curated dataset comprising 500K essays\nspanning diverse topics and genres, generated by LLMs, or written by humans,\nthe study uncovers the deeper layers of linguistic expression and provides\ninsights into the cognitive processes underlying both AI and human-driven\ntextual compositions. The analysis revealed that human-authored essays tend to\nhave a higher total word count on average than AI-generated essays but have a\nshorter average word length compared to AI- generated essays, and while both\ngroups exhibit high levels of fluency, the vocabulary diversity of Human\nauthored content is higher than AI generated content. However, AI- generated\nessays show a slightly higher level of novelty, suggesting the potential for\ngenerating more original content through AI systems. The paper addresses\nchallenges in assessing the language generation capabilities of AI models and\nemphasizes the importance of datasets that reflect the complexities of human-AI\ncollaborative writing. Through systematic preprocessing and rigorous\nstatistical analysis, this study offers valuable insights into the evolving\nlandscape of AI-generated content and informs future developments in natural\nlanguage processing (NLP).",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00769v1",
    "published_date": "2024-07-15 18:09:03 UTC",
    "updated_date": "2024-07-15 18:09:03 UTC"
  },
  {
    "arxiv_id": "2407.11121v1",
    "title": "Towards Adversarially Robust Vision-Language Models: Insights from Design Choices and Prompt Formatting Techniques",
    "authors": [
      "Rishika Bhagwatkar",
      "Shravan Nayak",
      "Reza Bayat",
      "Alexis Roger",
      "Daniel Z Kaplan",
      "Pouya Bashivan",
      "Irina Rish"
    ],
    "abstract": "Vision-Language Models (VLMs) have witnessed a surge in both research and\nreal-world applications. However, as they are becoming increasingly prevalent,\nensuring their robustness against adversarial attacks is paramount. This work\nsystematically investigates the impact of model design choices on the\nadversarial robustness of VLMs against image-based attacks. Additionally, we\nintroduce novel, cost-effective approaches to enhance robustness through prompt\nformatting. By rephrasing questions and suggesting potential adversarial\nperturbations, we demonstrate substantial improvements in model robustness\nagainst strong image-based attacks such as Auto-PGD. Our findings provide\nimportant guidelines for developing more robust VLMs, particularly for\ndeployment in safety-critical environments.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11121v1",
    "published_date": "2024-07-15 18:00:01 UTC",
    "updated_date": "2024-07-15 18:00:01 UTC"
  },
  {
    "arxiv_id": "2407.10973v3",
    "title": "Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion",
    "authors": [
      "Yongyuan Liang",
      "Tingqiang Xu",
      "Kaizhe Hu",
      "Guangqi Jiang",
      "Furong Huang",
      "Huazhe Xu"
    ],
    "abstract": "Can we generate a control policy for an agent using just one demonstration of\ndesired behaviors as a prompt, as effortlessly as creating an image from a\ntextual description? In this paper, we present Make-An-Agent, a novel policy\nparameter generator that leverages the power of conditional diffusion models\nfor behavior-to-policy generation. Guided by behavior embeddings that encode\ntrajectory information, our policy generator synthesizes latent parameter\nrepresentations, which can then be decoded into policy networks. Trained on\npolicy network checkpoints and their corresponding trajectories, our generation\nmodel demonstrates remarkable versatility and scalability on multiple tasks and\nhas a strong generalization ability on unseen tasks to output well-performed\npolicies with only few-shot demonstrations as inputs. We showcase its efficacy\nand efficiency on various domains and tasks, including varying objectives,\nbehaviors, and even across different robot manipulators. Beyond simulation, we\ndirectly deploy policies generated by Make-An-Agent onto real-world robots on\nlocomotion tasks. Project page: https://cheryyunl.github.io/make-an-agent/",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Annual Conference on Neural Information Processing Systems 38",
    "pdf_url": "http://arxiv.org/pdf/2407.10973v3",
    "published_date": "2024-07-15 17:59:57 UTC",
    "updated_date": "2024-11-04 02:44:49 UTC"
  },
  {
    "arxiv_id": "2407.10972v2",
    "title": "VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation",
    "authors": [
      "Bocheng Zou",
      "Mu Cai",
      "Jianrui Zhang",
      "Yong Jae Lee"
    ],
    "abstract": "In the realm of vision models, the primary mode of representation is using\npixels to rasterize the visual world. Yet this is not always the best or unique\nway to represent visual content, especially for designers and artists who\ndepict the world using geometry primitives such as polygons. Vector graphics\n(VG), on the other hand, offer a textual representation of visual content,\nwhich can be more concise and powerful for content like cartoons, sketches and\nscientific figures. Recent studies have shown promising results on processing\nvector graphics with capable Large Language Models (LLMs). However, such works\nfocus solely on qualitative results, understanding, or a specific type of\nvector graphics. We propose VGBench, a comprehensive benchmark for LLMs on\nhandling vector graphics through diverse aspects, including (a) both visual\nunderstanding and generation, (b) evaluation of various vector graphics\nformats, (c) diverse question types, (d) wide range of prompting techniques,\n(e) under multiple LLMs and (f) comparison with VLMs on rasterized\nrepresentations. Evaluating on our collected 4279 understanding and 5845\ngeneration samples, we find that LLMs show strong capability on both aspects\nwhile exhibiting less desirable performance on low-level formats (SVG). Both\ndata and evaluation pipeline will be open-sourced at https://vgbench.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://vgbench.github.io",
    "pdf_url": "http://arxiv.org/pdf/2407.10972v2",
    "published_date": "2024-07-15 17:59:55 UTC",
    "updated_date": "2024-08-29 17:55:52 UTC"
  },
  {
    "arxiv_id": "2407.10967v2",
    "title": "BECAUSE: Bilinear Causal Representation for Generalizable Offline Model-based Reinforcement Learning",
    "authors": [
      "Haohong Lin",
      "Wenhao Ding",
      "Jian Chen",
      "Laixi Shi",
      "Jiacheng Zhu",
      "Bo Li",
      "Ding Zhao"
    ],
    "abstract": "Offline model-based reinforcement learning (MBRL) enhances data efficiency by\nutilizing pre-collected datasets to learn models and policies, especially in\nscenarios where exploration is costly or infeasible. Nevertheless, its\nperformance often suffers from the objective mismatch between model and policy\nlearning, resulting in inferior performance despite accurate model predictions.\nThis paper first identifies the primary source of this mismatch comes from the\nunderlying confounders present in offline data for MBRL. Subsequently, we\nintroduce \\textbf{B}ilin\\textbf{E}ar \\textbf{CAUS}al\nr\\textbf{E}presentation~(BECAUSE), an algorithm to capture causal\nrepresentation for both states and actions to reduce the influence of the\ndistribution shift, thus mitigating the objective mismatch problem.\nComprehensive evaluations on 18 tasks that vary in data quality and environment\ncontext demonstrate the superior performance of BECAUSE over existing offline\nRL algorithms. We show the generalizability and robustness of BECAUSE under\nfewer samples or larger numbers of confounders. Additionally, we offer\ntheoretical analysis of BECAUSE to prove its error bound and sample efficiency\nwhen integrating causal representation into offline MBRL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10967v2",
    "published_date": "2024-07-15 17:59:23 UTC",
    "updated_date": "2025-03-03 01:19:23 UTC"
  },
  {
    "arxiv_id": "2407.10957v1",
    "title": "Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes",
    "authors": [
      "Yaoting Wang",
      "Peiwen Sun",
      "Dongzhan Zhou",
      "Guangyao Li",
      "Honggang Zhang",
      "Di Hu"
    ],
    "abstract": "Traditional reference segmentation tasks have predominantly focused on silent\nvisual scenes, neglecting the integral role of multimodal perception and\ninteraction in human experiences. In this work, we introduce a novel task\ncalled Reference Audio-Visual Segmentation (Ref-AVS), which seeks to segment\nobjects within the visual domain based on expressions containing multimodal\ncues. Such expressions are articulated in natural language forms but are\nenriched with multimodal cues, including audio and visual descriptions. To\nfacilitate this research, we construct the first Ref-AVS benchmark, which\nprovides pixel-level annotations for objects described in corresponding\nmultimodal-cue expressions. To tackle the Ref-AVS task, we propose a new method\nthat adequately utilizes multimodal cues to offer precise segmentation\nguidance. Finally, we conduct quantitative and qualitative experiments on three\ntest subsets to compare our approach with existing methods from related tasks.\nThe results demonstrate the effectiveness of our method, highlighting its\ncapability to precisely segment objects using multimodal-cue expressions.\nDataset is available at\n\\href{https://gewu-lab.github.io/Ref-AVS}{https://gewu-lab.github.io/Ref-AVS}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV2024",
    "pdf_url": "http://arxiv.org/pdf/2407.10957v1",
    "published_date": "2024-07-15 17:54:45 UTC",
    "updated_date": "2024-07-15 17:54:45 UTC"
  },
  {
    "arxiv_id": "2407.10956v1",
    "title": "Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?",
    "authors": [
      "Ruisheng Cao",
      "Fangyu Lei",
      "Haoyuan Wu",
      "Jixuan Chen",
      "Yeqiao Fu",
      "Hongcheng Gao",
      "Xinzhuang Xiong",
      "Hanchong Zhang",
      "Yuchen Mao",
      "Wenjing Hu",
      "Tianbao Xie",
      "Hongshen Xu",
      "Danyang Zhang",
      "Sida Wang",
      "Ruoxi Sun",
      "Pengcheng Yin",
      "Caiming Xiong",
      "Ansong Ni",
      "Qian Liu",
      "Victor Zhong",
      "Lu Chen",
      "Kai Yu",
      "Tao Yu"
    ],
    "abstract": "Data science and engineering workflows often span multiple stages, from\nwarehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As\nvision language models (VLMs) advance in multimodal understanding and code\ngeneration, VLM-based agents could potentially automate these workflows by\ngenerating SQL queries, Python code, and GUI operations. This automation can\nimprove the productivity of experts while democratizing access to large-scale\ndata analysis. In this paper, we introduce Spider2-V, the first multimodal\nagent benchmark focusing on professional data science and engineering\nworkflows, featuring 494 real-world tasks in authentic computer environments\nand incorporating 20 enterprise-level professional applications. These tasks,\nderived from real-world use cases, evaluate the ability of a multimodal agent\nto perform data-related tasks by writing code and managing the GUI in\nenterprise data software systems. To balance realistic simulation with\nevaluation simplicity, we devote significant effort to developing automatic\nconfigurations for task setup and carefully crafting evaluation metrics for\neach task. Furthermore, we supplement multimodal agents with comprehensive\ndocuments of these enterprise data software systems. Our empirical evaluation\nreveals that existing state-of-the-art LLM/VLM-based agents do not reliably\nautomate full data workflows (14.0% success). Even with step-by-step guidance,\nthese agents still underperform in tasks that require fine-grained,\nknowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted\nworkspaces (10.6%). We hope that Spider2-V paves the way for autonomous\nmultimodal agents to transform the automation of data science and engineering\nworkflow. Our code and data are available at https://spider2-v.github.io.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "34 pages, 14 figures, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.10956v1",
    "published_date": "2024-07-15 17:54:37 UTC",
    "updated_date": "2024-07-15 17:54:37 UTC"
  },
  {
    "arxiv_id": "2407.10954v1",
    "title": "A Unified Differentiable Boolean Operator with Fuzzy Logic",
    "authors": [
      "Hsueh-Ti Derek Liu",
      "Maneesh Agrawala",
      "Cem Yuksel",
      "Tim Omernick",
      "Vinith Misra",
      "Stefano Corazza",
      "Morgan McGuire",
      "Victor Zordan"
    ],
    "abstract": "This paper presents a unified differentiable boolean operator for implicit\nsolid shape modeling using Constructive Solid Geometry (CSG). Traditional CSG\nrelies on min, max operators to perform boolean operations on implicit shapes.\nBut because these boolean operators are discontinuous and discrete in the\nchoice of operations, this makes optimization over the CSG representation\nchallenging. Drawing inspiration from fuzzy logic, we present a unified boolean\noperator that outputs a continuous function and is differentiable with respect\nto operator types. This enables optimization of both the primitives and the\nboolean operations employed in CSG with continuous optimization techniques,\nsuch as gradient descent. We further demonstrate that such a continuous boolean\noperator allows modeling of both sharp mechanical objects and smooth organic\nshapes with the same framework. Our proposed boolean operator opens up new\npossibilities for future research toward fully continuous CSG optimization.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.GR",
    "comment": "SIGGRAPH'24",
    "pdf_url": "http://arxiv.org/pdf/2407.10954v1",
    "published_date": "2024-07-15 17:52:22 UTC",
    "updated_date": "2024-07-15 17:52:22 UTC"
  },
  {
    "arxiv_id": "2407.10949v2",
    "title": "Representing Rule-based Chatbots with Transformers",
    "authors": [
      "Dan Friedman",
      "Abhishek Panigrahi",
      "Danqi Chen"
    ],
    "abstract": "What kind of internal mechanisms might Transformers use to conduct fluid,\nnatural-sounding conversations? Prior work has illustrated by construction how\nTransformers can solve various synthetic tasks, such as sorting a list or\nrecognizing formal languages, but it remains unclear how to extend this\napproach to a conversational setting. In this work, we propose using ELIZA, a\nclassic rule-based chatbot, as a setting for formal, mechanistic analysis of\nTransformer-based chatbots. ELIZA allows us to formally model key aspects of\nconversation, including local pattern matching and long-term dialogue state\ntracking. We first present a theoretical construction of a Transformer that\nimplements the ELIZA chatbot. Building on prior constructions, particularly\nthose for simulating finite-state automata, we show how simpler mechanisms can\nbe composed and extended to produce more sophisticated behavior. Next, we\nconduct a set of empirical analyses of Transformers trained on synthetically\ngenerated ELIZA conversations. Our analysis illustrates the kinds of mechanisms\nthese models tend to prefer--for example, models favor an induction head\nmechanism over a more precise, position-based copying mechanism; and using\nintermediate generations to simulate recurrent data structures, akin to an\nimplicit scratchpad or Chain-of-Thought. Overall, by drawing an explicit\nconnection between neural chatbots and interpretable, symbolic mechanisms, our\nresults provide a new framework for the mechanistic analysis of conversational\nagents.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025. Code and data are available at\n  https://github.com/princeton-nlp/ELIZA-Transformer",
    "pdf_url": "http://arxiv.org/pdf/2407.10949v2",
    "published_date": "2024-07-15 17:45:53 UTC",
    "updated_date": "2025-02-12 15:18:32 UTC"
  },
  {
    "arxiv_id": "2407.12873v1",
    "title": "Evaluation of RAG Metrics for Question Answering in the Telecom Domain",
    "authors": [
      "Sujoy Roychowdhury",
      "Sumit Soman",
      "H G Ranjani",
      "Neeraj Gunda",
      "Vansh Chhabra",
      "Sai Krishna Bala"
    ],
    "abstract": "Retrieval Augmented Generation (RAG) is widely used to enable Large Language\nModels (LLMs) perform Question Answering (QA) tasks in various domains.\nHowever, RAG based on open-source LLM for specialized domains has challenges of\nevaluating generated responses. A popular framework in the literature is the\nRAG Assessment (RAGAS), a publicly available library which uses LLMs for\nevaluation. One disadvantage of RAGAS is the lack of details of derivation of\nnumerical value of the evaluation metrics. One of the outcomes of this work is\na modified version of this package for few metrics (faithfulness, context\nrelevance, answer relevance, answer correctness, answer similarity and factual\ncorrectness) through which we provide the intermediate outputs of the prompts\nby using any LLMs. Next, we analyse the expert evaluations of the output of the\nmodified RAGAS package and observe the challenges of using it in the telecom\ndomain. We also study the effect of the metrics under correct vs. wrong\nretrieval and observe that few of the metrics have higher values for correct\nretrieval. We also study for differences in metrics between base embeddings and\nthose domain adapted via pre-training and fine-tuning. Finally, we comment on\nthe suitability and challenges of using these metrics for in-the-wild telecom\nQA task.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication in ICML 2024 Workshop on Foundation Models\n  in the Wild",
    "pdf_url": "http://arxiv.org/pdf/2407.12873v1",
    "published_date": "2024-07-15 17:40:15 UTC",
    "updated_date": "2024-07-15 17:40:15 UTC"
  },
  {
    "arxiv_id": "2407.10930v2",
    "title": "Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together",
    "authors": [
      "Dilara Soylu",
      "Christopher Potts",
      "Omar Khattab"
    ],
    "abstract": "Natural Language Processing (NLP) systems are increasingly taking the form of\nsophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG),\nwhere each module may involve a distinct Language Model (LM) and an associated\nprompt template. These compound systems often lack intermediate labels or\ngradient flow to optimize each module, making their end-to-end optimization\nchallenging. Here we seek strategies to optimize both the module-level LM\nweights and the associated prompt templates of such systems to maximize a\ndownstream task metric. We propose for the first time combining the weight and\nprompt optimization strategies to optimize a modular LM pipeline by alternating\nbetween the two to get the same LM to teach itself. In experiments with\nmulti-hop QA, mathematical reasoning, and feature-based classification using\nmistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies\noptimizing the weights and prompts of a pipeline together outperform directly\noptimizing weights alone and prompts alone by up to 60% and 6%, respectively,\non average across LMs and tasks. BetterTogether optimizer is released in DSPy\nat http://dspy.ai",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.10930v2",
    "published_date": "2024-07-15 17:30:31 UTC",
    "updated_date": "2024-10-07 15:52:48 UTC"
  },
  {
    "arxiv_id": "2407.10920v3",
    "title": "Benchmarking Vision Language Models for Cultural Understanding",
    "authors": [
      "Shravan Nayak",
      "Kanishk Jain",
      "Rabiul Awal",
      "Siva Reddy",
      "Sjoerd van Steenkiste",
      "Lisa Anne Hendricks",
      "Karolina Stańczak",
      "Aishwarya Agrawal"
    ],
    "abstract": "Foundation models and vision-language pre-training have notably advanced\nVision Language Models (VLMs), enabling multimodal processing of visual and\nlinguistic data. However, their performance has been typically assessed on\ngeneral scene understanding - recognizing objects, attributes, and actions -\nrather than cultural comprehension. This study introduces CulturalVQA, a visual\nquestion-answering benchmark aimed at assessing VLM's geo-diverse cultural\nunderstanding. We curate a collection of 2,378 image-question pairs with 1-5\nanswers per question representing cultures from 11 countries across 5\ncontinents. The questions probe understanding of various facets of culture such\nas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on\nCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of\ncultural understanding across regions, with strong cultural understanding\ncapabilities for North America while significantly lower performance for\nAfrica. We observe disparity in their performance across cultural facets too,\nwith clothing, rituals, and traditions seeing higher performances than food and\ndrink. These disparities help us identify areas where VLMs lack cultural\nunderstanding and demonstrate the potential of CulturalVQA as a comprehensive\nevaluation set for gauging VLM progress in understanding diverse cultures.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to EMNLP 2024 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2407.10920v3",
    "published_date": "2024-07-15 17:21:41 UTC",
    "updated_date": "2024-10-14 13:08:01 UTC"
  },
  {
    "arxiv_id": "2407.10899v1",
    "title": "Leveraging LLM-Respondents for Item Evaluation: a Psychometric Analysis",
    "authors": [
      "Yunting Liu",
      "Shreya Bhandari",
      "Zachary A. Pardos"
    ],
    "abstract": "Effective educational measurement relies heavily on the curation of\nwell-designed item pools (i.e., possessing the right psychometric properties).\nHowever, item calibration is time-consuming and costly, requiring a sufficient\nnumber of respondents for the response process. We explore using six different\nLLMs (GPT-3.5, GPT-4, Llama 2, Llama 3, Gemini-Pro, and Cohere Command R Plus)\nand various combinations of them using sampling methods to produce responses\nwith psychometric properties similar to human answers. Results show that some\nLLMs have comparable or higher proficiency in College Algebra than college\nstudents. No single LLM mimics human respondents due to narrow proficiency\ndistributions, but an ensemble of LLMs can better resemble college students'\nability distribution. The item parameters calibrated by LLM-Respondents have\nhigh correlations (e.g. > 0.8 for GPT-3.5) compared to their human calibrated\ncounterparts, and closely resemble the parameters of the human subset (e.g.\n0.02 Spearman correlation difference). Several augmentation strategies are\nevaluated for their relative performance, with resampling methods proving most\neffective, enhancing the Spearman correlation from 0.89 (human only) to 0.93\n(augmented human).",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10899v1",
    "published_date": "2024-07-15 16:49:26 UTC",
    "updated_date": "2024-07-15 16:49:26 UTC"
  },
  {
    "arxiv_id": "2407.10888v1",
    "title": "Leveraging Multimodal CycleGAN for the Generation of Anatomically Accurate Synthetic CT Scans from MRIs",
    "authors": [
      "Leonardo Crespi",
      "Samuele Camnasio",
      "Damiano Dei",
      "Nicola Lambri",
      "Pietro Mancosu",
      "Marta Scorsetti",
      "Daniele Loiacono"
    ],
    "abstract": "In many clinical settings, the use of both Computed Tomography (CT) and\nMagnetic Resonance (MRI) is necessary to pursue a thorough understanding of the\npatient's anatomy and to plan a suitable therapeutical strategy; this is often\nthe case in MRI-based radiotherapy, where CT is always necessary to prepare the\ndose delivery, as it provides the essential information about the radiation\nabsorption properties of the tissues. Sometimes, MRI is preferred to contour\nthe target volumes. However, this approach is often not the most efficient, as\nit is more expensive, time-consuming and, most importantly, stressful for the\npatients. To overcome this issue, in this work, we analyse the capabilities of\ndifferent configurations of Deep Learning models to generate synthetic CT scans\nfrom MRI, leveraging the power of Generative Adversarial Networks (GANs) and,\nin particular, the CycleGAN architecture, capable of working in an unsupervised\nmanner and without paired images, which were not available. Several CycleGAN\nmodels were trained unsupervised to generate CT scans from different MRI\nmodalities with and without contrast agents. To overcome the problem of not\nhaving a ground truth, distribution-based metrics were used to assess the\nmodel's performance quantitatively, together with a qualitative evaluation\nwhere physicians were asked to differentiate between real and synthetic images\nto understand how realistic the generated images were. The results show how,\ndepending on the input modalities, the models can have very different\nperformances; however, models with the best quantitative results, according to\nthe distribution-based metrics used, can generate very difficult images to\ndistinguish from the real ones, even for physicians, demonstrating the\napproach's potential.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Currently submitted to: Scientific Reports",
    "pdf_url": "http://arxiv.org/pdf/2407.10888v1",
    "published_date": "2024-07-15 16:38:59 UTC",
    "updated_date": "2024-07-15 16:38:59 UTC"
  },
  {
    "arxiv_id": "2407.10887v2",
    "title": "Hey, That's My Model! Introducing Chain & Hash, An LLM Fingerprinting Technique",
    "authors": [
      "Mark Russinovich",
      "Ahmed Salem"
    ],
    "abstract": "Amid growing concerns over the ease of theft and misuse of Large Language\nModels (LLMs), the need for fingerprinting models has increased.\nFingerprinting, in this context, means that the model owner can link a given\nmodel to their original version, thereby identifying if their model is being\nmisused or has been completely stolen. In this paper, we first define a set\nfive properties a successful fingerprint should satisfy; namely, the\nfingerprint should be Transparent, Efficient, Persistent, Robust, and\nUnforgeable. Next, we propose Chain & Hash, a new, simple fingerprinting\napproach that implements a fingerprint with a cryptographic flavor, achieving\nall these properties. Chain & Hash involves generating a set of questions (the\nfingerprints) along with a set of potential answers. These elements are hashed\ntogether using a secure hashing technique to select the value for each\nquestion, hence providing an unforgeability property-preventing adversaries\nfrom claiming false ownership. We evaluate the Chain & Hash technique on\nmultiple models and demonstrate its robustness against benign transformations,\nsuch as fine-tuning on different datasets, and adversarial attempts to erase\nthe fingerprint. Finally, our experiments demonstrate the efficiency of\nimplementing Chain & Hash and its utility, where fingerprinted models achieve\nalmost the same performance as non-fingerprinted ones across different\nbenchmarks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10887v2",
    "published_date": "2024-07-15 16:38:56 UTC",
    "updated_date": "2024-07-17 07:39:41 UTC"
  },
  {
    "arxiv_id": "2407.10878v1",
    "title": "Deep Causal Learning to Explain and Quantify The Geo-Tension's Impact on Natural Gas Market",
    "authors": [
      "Philipp Kai Peter",
      "Yulin Li",
      "Ziyue Li",
      "Wolfgang Ketter"
    ],
    "abstract": "Natural gas demand is a crucial factor for predicting natural gas prices and\nthus has a direct influence on the power system. However, existing methods face\nchallenges in assessing the impact of shocks, such as the outbreak of the\nRussian-Ukrainian war. In this context, we apply deep neural network-based\nGranger causality to identify important drivers of natural gas demand.\nFurthermore, the resulting dependencies are used to construct a counterfactual\ncase without the outbreak of the war, providing a quantifiable estimate of the\noverall effect of the shock on various German energy sectors. The code and\ndataset are available at https://github.com/bonaldli/CausalEnergy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by IEEE PES ISGT Europe 2024 conference",
    "pdf_url": "http://arxiv.org/pdf/2407.10878v1",
    "published_date": "2024-07-15 16:28:26 UTC",
    "updated_date": "2024-07-15 16:28:26 UTC"
  },
  {
    "arxiv_id": "2407.10873v1",
    "title": "Understanding the Importance of Evolutionary Search in Automated Heuristic Design with Large Language Models",
    "authors": [
      "Rui Zhang",
      "Fei Liu",
      "Xi Lin",
      "Zhenkun Wang",
      "Zhichao Lu",
      "Qingfu Zhang"
    ],
    "abstract": "Automated heuristic design (AHD) has gained considerable attention for its\npotential to automate the development of effective heuristics. The recent\nadvent of large language models (LLMs) has paved a new avenue for AHD, with\ninitial efforts focusing on framing AHD as an evolutionary program search (EPS)\nproblem. However, inconsistent benchmark settings, inadequate baselines, and a\nlack of detailed component analysis have left the necessity of integrating LLMs\nwith search strategies and the true progress achieved by existing LLM-based EPS\nmethods to be inadequately justified. This work seeks to fulfill these research\nqueries by conducting a large-scale benchmark comprising four LLM-based EPS\nmethods and four AHD problems across nine LLMs and five independent runs. Our\nextensive experiments yield meaningful insights, providing empirical grounding\nfor the importance of evolutionary search in LLM-based AHD approaches, while\nalso contributing to the advancement of future EPS algorithmic development. To\nfoster accessibility and reproducibility, we have fully open-sourced our\nbenchmark and corresponding results.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted by the 18th International Conference on Parallel Problem\n  Solving From Nature (PPSN 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.10873v1",
    "published_date": "2024-07-15 16:21:20 UTC",
    "updated_date": "2024-07-15 16:21:20 UTC"
  },
  {
    "arxiv_id": "2407.10870v1",
    "title": "GPT Sonograpy: Hand Gesture Decoding from Forearm Ultrasound Images via VLM",
    "authors": [
      "Keshav Bimbraw",
      "Ye Wang",
      "Jing Liu",
      "Toshiaki Koike-Akino"
    ],
    "abstract": "Large vision-language models (LVLMs), such as the Generative Pre-trained\nTransformer 4-omni (GPT-4o), are emerging multi-modal foundation models which\nhave great potential as powerful artificial-intelligence (AI) assistance tools\nfor a myriad of applications, including healthcare, industrial, and academic\nsectors. Although such foundation models perform well in a wide range of\ngeneral tasks, their capability without fine-tuning is often limited in\nspecialized tasks. However, full fine-tuning of large foundation models is\nchallenging due to enormous computation/memory/dataset requirements. We show\nthat GPT-4o can decode hand gestures from forearm ultrasound data even with no\nfine-tuning, and improves with few-shot, in-context learning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.10870v1",
    "published_date": "2024-07-15 16:18:06 UTC",
    "updated_date": "2024-07-15 16:18:06 UTC"
  },
  {
    "arxiv_id": "2407.10855v1",
    "title": "Weighted Grouped Query Attention in Transformers",
    "authors": [
      "Sai Sena Chinnakonduru",
      "Astarag Mohapatra"
    ],
    "abstract": "The attention mechanism forms the foundational blocks for transformer\nlanguage models. Recent approaches show that scaling the model achieves\nhuman-level performance. However, with increasing demands for scaling and\nconstraints on hardware memory, the inference costs of these models remain\nhigh. To reduce the inference time, Multi-Query Attention (MQA) and\nGrouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieet\nal., 2023) respectively. In this paper, we propose a variation of Grouped-Query\nAttention, termed Weighted Grouped-Query Attention (WGQA). We introduced new\nlearnable parameters for each key and value head in the T5 decoder attention\nblocks, enabling the model to take a weighted average during finetuning. Our\nmodel achieves an average of 0.53% improvement over GQA, and the performance\nconverges to traditional Multi-head attention (MHA) with no additional overhead\nduring inference. We evaluated the introduction of these parameters and\nsubsequent finetuning informs the model about the grouping mechanism during\ntraining, thereby enhancing performance. Additionally, we demonstrate the\nscaling laws in our analysis by comparing the results between T5-small and\nT5-base architecture.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10855v1",
    "published_date": "2024-07-15 16:07:13 UTC",
    "updated_date": "2024-07-15 16:07:13 UTC"
  },
  {
    "arxiv_id": "2407.10853v3",
    "title": "An Actionable Framework for Assessing Bias and Fairness in Large Language Model Use Cases",
    "authors": [
      "Dylan Bouchard"
    ],
    "abstract": "Large language models (LLMs) can exhibit bias in a variety of ways. Such\nbiases can create or exacerbate unfair outcomes for certain groups within a\nprotected attribute, including, but not limited to sex, race, sexual\norientation, or age. In this paper, we propose a decision framework that allows\npractitioners to determine which bias and fairness metrics to use for a\nspecific LLM use case. To establish the framework, we define bias and fairness\nrisks for LLMs, map those risks to a taxonomy of LLM use cases, and then define\nvarious metrics to assess each type of risk. Instead of focusing solely on the\nmodel itself, we account for both prompt-specific- and model-specific-risk by\ndefining evaluations at the level of an LLM use case, characterized by a model\nand a population of prompts. Furthermore, because all of the evaluation metrics\nare calculated solely using the LLM output, our proposed framework is highly\npractical and easily actionable for practitioners. For streamlined\nimplementation, all evaluation metrics included in the framework are offered in\nthis paper's companion Python toolkit, LangFair. Finally, our experiments\ndemonstrate substantial variation in bias and fairness across use cases,\nunderscoring the importance of use-case-level assessments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "LangFair repository: https://github.com/cvs-health/langfair",
    "pdf_url": "http://arxiv.org/pdf/2407.10853v3",
    "published_date": "2024-07-15 16:04:44 UTC",
    "updated_date": "2025-02-13 14:13:41 UTC"
  },
  {
    "arxiv_id": "2407.10839v1",
    "title": "Offline Reinforcement Learning with Imputed Rewards",
    "authors": [
      "Carlo Romeo",
      "Andrew D. Bagdanov"
    ],
    "abstract": "Offline Reinforcement Learning (ORL) offers a robust solution to training\nagents in applications where interactions with the environment must be strictly\nlimited due to cost, safety, or lack of accurate simulation environments.\nDespite its potential to facilitate deployment of artificial agents in the real\nworld, Offline Reinforcement Learning typically requires very many\ndemonstrations annotated with ground-truth rewards. Consequently,\nstate-of-the-art ORL algorithms can be difficult or impossible to apply in\ndata-scarce scenarios. In this paper we propose a simple but effective Reward\nModel that can estimate the reward signal from a very limited sample of\nenvironment transitions annotated with rewards. Once the reward signal is\nmodeled, we use the Reward Model to impute rewards for a large sample of\nreward-free transitions, thus enabling the application of ORL techniques. We\ndemonstrate the potential of our approach on several D4RL continuous locomotion\ntasks. Our results show that, using only 1\\% of reward-labeled transitions from\nthe original datasets, our learned reward model is able to impute rewards for\nthe remaining 99\\% of the transitions, from which performant agents can be\nlearned using Offline Reinforcement Learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "RLBRew Workshop @ RLC 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.10839v1",
    "published_date": "2024-07-15 15:53:13 UTC",
    "updated_date": "2024-07-15 15:53:13 UTC"
  },
  {
    "arxiv_id": "2407.10835v1",
    "title": "Exploration in Knowledge Transfer Utilizing Reinforcement Learning",
    "authors": [
      "Adam Jedlička",
      "Tatiana Valentine Guy"
    ],
    "abstract": "The contribution focuses on the problem of exploration within the task of\nknowledge transfer. Knowledge transfer refers to the useful application of the\nknowledge gained while learning the source task in the target task. The\nintended benefit of knowledge transfer is to speed up the learning process of\nthe target task. The article aims to compare several exploration methods used\nwithin a deep transfer learning algorithm, particularly Deep Target Transfer\n$Q$-learning. The methods used are $\\epsilon$-greedy, Boltzmann, and upper\nconfidence bound exploration. The aforementioned transfer learning algorithms\nand exploration methods were tested on the virtual drone problem. The results\nhave shown that the upper confidence bound algorithm performs the best out of\nthese options. Its sustainability to other applications is to be checked.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.10835v1",
    "published_date": "2024-07-15 15:45:29 UTC",
    "updated_date": "2024-07-15 15:45:29 UTC"
  },
  {
    "arxiv_id": "2407.10834v3",
    "title": "MetaLLM: A High-performant and Cost-efficient Dynamic Framework for Wrapping LLMs",
    "authors": [
      "Quang H. Nguyen",
      "Thinh Dao",
      "Duy C. Hoang",
      "Juliette Decugis",
      "Saurav Manchanda",
      "Nitesh V. Chawla",
      "Khoa D. Doan"
    ],
    "abstract": "The rapid progress in machine learning (ML) has brought forth many large\nlanguage models (LLMs) that excel in various tasks and areas. These LLMs come\nwith different abilities and costs in terms of computation or pricing. Since\nthe demand for each query can vary, e.g., because of the queried domain or its\ncomplexity, defaulting to one LLM in an application is not usually the best\nchoice, whether it is the biggest, priciest, or even the one with the best\naverage test performance. Consequently, picking the right LLM that is both\naccurate and cost-effective for an application is necessary yet remains a\nchallenge. In this paper, we introduce MetaLLM, a framework that dynamically\nand intelligently routes each query to the optimal LLM (among several available\nLLMs) for classification and multi-choice question-answering tasks, achieving\nsignificantly improved accuracy and cost-effectiveness. By framing the\nselection problem as a multi-armed bandit, MetaLLM balances prediction accuracy\nand cost efficiency under uncertainty. Our experiments, conducted on popular\nLLM platforms such as OpenAI and Together AI, as well as open-source LLM,\nshowcase MetaLLM's efficacy in real-world scenarios, laying the groundwork for\nfuture extensions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10834v3",
    "published_date": "2024-07-15 15:45:07 UTC",
    "updated_date": "2025-04-22 03:55:14 UTC"
  },
  {
    "arxiv_id": "2407.10829v1",
    "title": "BiasScanner: Automatic Detection and Classification of News Bias to Strengthen Democracy",
    "authors": [
      "Tim Menzner",
      "Jochen L. Leidner"
    ],
    "abstract": "The increasing consumption of news online in the 21st century coincided with\nincreased publication of disinformation, biased reporting, hate speech and\nother unwanted Web content. We describe BiasScanner, an application that aims\nto strengthen democracy by supporting news consumers with scrutinizing news\narticles they are reading online. BiasScanner contains a server-side\npre-trained large language model to identify biased sentences of news articles\nand a front-end Web browser plug-in. At the time of writing, BiasScanner can\nidentify and classify more than two dozen types of media bias at the sentence\nlevel, making it the most fine-grained model and only deployed application\n(automatic system in use) of its kind. It was implemented in a light-weight and\nprivacy-respecting manner, and in addition to highlighting likely biased\nsentence it also provides explanations for each classification decision as well\nas a summary analysis for each news article. While prior research has addressed\nnews bias detection, we are not aware of any work that resulted in a deployed\nbrowser plug-in (c.f. also biasscanner.org for a Web demo).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "I.2.7; H.3.3"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 3 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2407.10829v1",
    "published_date": "2024-07-15 15:42:22 UTC",
    "updated_date": "2024-07-15 15:42:22 UTC"
  },
  {
    "arxiv_id": "2407.10828v1",
    "title": "Towards Enhanced Classification of Abnormal Lung sound in Multi-breath: A Light Weight Multi-label and Multi-head Attention Classification Method",
    "authors": [
      "Yi-Wei Chua",
      "Yun-Chien Cheng"
    ],
    "abstract": "This study aims to develop an auxiliary diagnostic system for classifying\nabnormal lung respiratory sounds, enhancing the accuracy of automatic abnormal\nbreath sound classification through an innovative multi-label learning approach\nand multi-head attention mechanism. Addressing the issue of class imbalance and\nlack of diversity in existing respiratory sound datasets, our study employs a\nlightweight and highly accurate model, using a two-dimensional label set to\nrepresent multiple respiratory sound characteristics. Our method achieved a\n59.2% ICBHI score in the four-category task on the ICBHI2017 dataset,\ndemonstrating its advantages in terms of lightweight and high accuracy. This\nstudy not only improves the accuracy of automatic diagnosis of lung respiratory\nsound abnormalities but also opens new possibilities for clinical applications.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10828v1",
    "published_date": "2024-07-15 15:40:02 UTC",
    "updated_date": "2024-07-15 15:40:02 UTC"
  },
  {
    "arxiv_id": "2407.10820v3",
    "title": "Enabling MCTS Explainability for Sequential Planning Through Computation Tree Logic",
    "authors": [
      "Ziyan An",
      "Hendrik Baier",
      "Abhishek Dubey",
      "Ayan Mukhopadhyay",
      "Meiyi Ma"
    ],
    "abstract": "Monte Carlo tree search (MCTS) is one of the most capable online search\nalgorithms for sequential planning tasks, with significant applications in\nareas such as resource allocation and transit planning. Despite its strong\nperformance in real-world deployment, the inherent complexity of MCTS makes it\nchallenging to understand for users without technical background. This paper\nconsiders the use of MCTS in transportation routing services, where the\nalgorithm is integrated to develop optimized route plans. These plans are\nrequired to meet a range of constraints and requirements simultaneously,\nfurther complicating the task of explaining the algorithm's operation in\nreal-world contexts. To address this critical research gap, we introduce a\nnovel computation tree logic-based explainer for MCTS. Our framework begins by\ntaking user-defined requirements and translating them into rigorous logic\nspecifications through the use of language templates. Then, our explainer\nincorporates a logic verification and quantitative evaluation module that\nvalidates the states and actions traversed by the MCTS algorithm. The outcomes\nof this analysis are then rendered into human-readable descriptive text using a\nsecond set of language templates. The user satisfaction of our approach was\nassessed through a survey with 82 participants. The results indicated that our\nexplanatory approach significantly outperforms other baselines in user\npreference.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by the Proceedings of the 27th European Conference on\n  Artificial Intelligence (ECAI)",
    "pdf_url": "http://arxiv.org/pdf/2407.10820v3",
    "published_date": "2024-07-15 15:35:09 UTC",
    "updated_date": "2024-10-29 18:42:33 UTC"
  },
  {
    "arxiv_id": "2407.10817v1",
    "title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
    "authors": [
      "Tu Vu",
      "Kalpesh Krishna",
      "Salaheddin Alzubi",
      "Chris Tar",
      "Manaal Faruqui",
      "Yun-Hsuan Sung"
    ],
    "abstract": "As large language models (LLMs) advance, it becomes more challenging to\nreliably evaluate their output due to the high costs of human evaluation. To\nmake progress towards better LLM autoraters, we introduce FLAMe, a family of\nFoundational Large Autorater Models. FLAMe is trained on our large and diverse\ncollection of 100+ quality assessment tasks comprising 5M+ human judgments,\ncurated and standardized using publicly released human evaluations from\nprevious research. FLAMe significantly improves generalization to a wide\nvariety of held-out tasks, outperforming LLMs trained on proprietary data like\nGPT-4 and Claude-3 on many tasks. We show that FLAMe can also serve as a\npowerful starting point for further downstream fine-tuning, using reward\nmodeling evaluation as a case study (FLAMe-RM). Notably, on RewardBench, our\nFLAMe-RM-24B model (with an accuracy of 87.8%) is the top-performing generative\nmodel trained exclusively on permissively licensed data, outperforming both\nGPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we explore a more\ncomputationally efficient approach using a novel tail-patch fine-tuning\nstrategy to optimize our FLAMe multitask mixture for reward modeling evaluation\n(FLAMe-Opt-RM), offering competitive RewardBench performance while requiring\napproximately 25x less training datapoints. Overall, our FLAMe variants\noutperform all popular proprietary LLM-as-a-Judge models we consider across 8\nout of 12 autorater evaluation benchmarks, encompassing 53 quality assessment\ntasks, including RewardBench and LLM-AggreFact. Finally, our analysis reveals\nthat FLAMe is significantly less biased than these LLM-as-a-Judge models on the\nCoBBLEr autorater bias benchmark, while effectively identifying high-quality\nresponses for code generation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "31 pages, 5 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.10817v1",
    "published_date": "2024-07-15 15:33:45 UTC",
    "updated_date": "2024-07-15 15:33:45 UTC"
  },
  {
    "arxiv_id": "2407.10811v1",
    "title": "GuideLight: \"Industrial Solution\" Guidance for More Practical Traffic Signal Control Agents",
    "authors": [
      "Haoyuan Jiang",
      "Xuantang Xiong",
      "Ziyue Li",
      "Hangyu Mao",
      "Guanghu Sui",
      "Jingqing Ruan",
      "Yuheng Cheng",
      "Hua Wei",
      "Wolfgang Ketter",
      "Rui Zhao"
    ],
    "abstract": "Currently, traffic signal control (TSC) methods based on reinforcement\nlearning (RL) have proven superior to traditional methods. However, most RL\nmethods face difficulties when applied in the real world due to three factors:\ninput, output, and the cycle-flow relation. The industry's observable input is\nmuch more limited than simulation-based RL methods. For real-world solutions,\nonly flow can be reliably collected, whereas common RL methods need more. For\nthe output action, most RL methods focus on acyclic control, which real-world\nsignal controllers do not support. Most importantly, industry standards require\na consistent cycle-flow relationship: non-decreasing and different response\nstrategies for low, medium, and high-level flows, which is ignored by the RL\nmethods. To narrow the gap between RL methods and industry standards, we\ninnovatively propose to use industry solutions to guide the RL agent.\nSpecifically, we design behavior cloning and curriculum learning to guide the\nagent to mimic and meet industry requirements and, at the same time, leverage\nthe power of exploration and exploitation in RL for better performance. We\ntheoretically prove that such guidance can largely decrease the sample\ncomplexity to polynomials in the horizon when searching for an optimal policy.\nOur rigid experiments show that our method has good cycle-flow relation and\nsuperior performance.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "Under Review of IEEE Transactions on Intelligent Transportation\n  Systems",
    "pdf_url": "http://arxiv.org/pdf/2407.10811v1",
    "published_date": "2024-07-15 15:26:10 UTC",
    "updated_date": "2024-07-15 15:26:10 UTC"
  },
  {
    "arxiv_id": "2407.10810v2",
    "title": "FabGPT: An Efficient Large Multimodal Model for Complex Wafer Defect Knowledge Queries",
    "authors": [
      "Yuqi Jiang",
      "Xudong Lu",
      "Qian Jin",
      "Qi Sun",
      "Hanming Wu",
      "Cheng Zhuo"
    ],
    "abstract": "Intelligence is key to advancing integrated circuit (IC) fabrication. Recent\nbreakthroughs in Large Multimodal Models (LMMs) have unlocked extraditionary\nabilities in understanding images and text, fostering intelligent fabrication.\nLeveraging the power of LMMs, we introduce FabGPT, a customized IC fabrication\nlarge multimodal model for wafer defect knowledge query. FabGPT manifests\nexpertise in conducting defect detection in Scanning Electron Microscope (SEM)\nimages, performing root cause analysis, and providing expert Q&A on fabrication\nprocesses. FabGPT matches enhanced multimodal features to automatically detect\nminute defects under complex wafer backgrounds and reduce the subjectivity of\nmanual threshold settings. Besides, the proposed modulation module and\ninteractive corpus training strategy embed wafer defect knowledge into the\npre-trained model, effectively balancing Q&A queries related to defect\nknowledge and original knowledge and mitigating the modality bias issues.\nExperiments on in-house fab data show that FabGPT achieves significant\nperformance improvement in wafer defect detection and knowledge querying.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Published in ACM/IEEE International Conference On Computer Aided\n  Design (ICCAD) 2024. Corresponding Author: Qi Sun (qisunchn@zju.edu.cn)",
    "pdf_url": "http://arxiv.org/pdf/2407.10810v2",
    "published_date": "2024-07-15 15:25:45 UTC",
    "updated_date": "2025-02-15 14:37:18 UTC"
  },
  {
    "arxiv_id": "2407.10805v7",
    "title": "Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning with Knowledge-guided Retrieval Augmented Generation",
    "authors": [
      "Shengjie Ma",
      "Chengjin Xu",
      "Xuhui Jiang",
      "Muzhi Li",
      "Huaren Qu",
      "Cehao Yang",
      "Jiaxin Mao",
      "Jian Guo"
    ],
    "abstract": "Retrieval-augmented generation (RAG) has improved large language models\n(LLMs) by using knowledge retrieval to overcome knowledge deficiencies.\nHowever, current RAG methods often fall short of ensuring the depth and\ncompleteness of retrieved information, which is necessary for complex reasoning\ntasks. In this work, we introduce Think-on-Graph 2.0 (ToG-2), a hybrid RAG\nframework that iteratively retrieves information from both unstructured and\nstructured knowledge sources in a tight-coupling manner. Specifically, ToG-2\nleverages knowledge graphs (KGs) to link documents via entities, facilitating\ndeep and knowledge-guided context retrieval. Simultaneously, it utilizes\ndocuments as entity contexts to achieve precise and efficient graph retrieval.\nToG-2 alternates between graph retrieval and context retrieval to search for\nin-depth clues relevant to the question, enabling LLMs to generate answers. We\nconduct a series of well-designed experiments to highlight the following\nadvantages of ToG-2: 1) ToG-2 tightly couples the processes of context\nretrieval and graph retrieval, deepening context retrieval via the KG while\nenabling reliable graph retrieval based on contexts; 2) it achieves deep and\nfaithful reasoning in LLMs through an iterative knowledge retrieval process of\ncollaboration between contexts and the KG; and 3) ToG-2 is training-free and\nplug-and-play compatible with various LLMs. Extensive experiments demonstrate\nthat ToG-2 achieves overall state-of-the-art (SOTA) performance on 6 out of 7\nknowledge-intensive datasets with GPT-3.5, and can elevate the performance of\nsmaller models (e.g., LLAMA-2-13B) to the level of GPT-3.5's direct reasoning.\nThe source code is available on https://github.com/IDEA-FinAI/ToG-2.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10805v7",
    "published_date": "2024-07-15 15:20:40 UTC",
    "updated_date": "2025-02-10 03:16:09 UTC"
  },
  {
    "arxiv_id": "2407.10796v1",
    "title": "Mammographic Breast Positioning Assessment via Deep Learning",
    "authors": [
      "Toygar Tanyel",
      "Nurper Denizoglu",
      "Mustafa Ege Seker",
      "Deniz Alis",
      "Esma Cerekci",
      "Ercan Karaarslan",
      "Erkin Aribal",
      "Ilkay Oksuz"
    ],
    "abstract": "Breast cancer remains a leading cause of cancer-related deaths among women\nworldwide, with mammography screening as the most effective method for the\nearly detection. Ensuring proper positioning in mammography is critical, as\npoor positioning can lead to diagnostic errors, increased patient stress, and\nhigher costs due to recalls. Despite advancements in deep learning (DL) for\nbreast cancer diagnostics, limited focus has been given to evaluating\nmammography positioning. This paper introduces a novel DL methodology to\nquantitatively assess mammogram positioning quality, specifically in\nmediolateral oblique (MLO) views using attention and coordinate convolution\nmodules. Our method identifies key anatomical landmarks, such as the nipple and\npectoralis muscle, and automatically draws a posterior nipple line (PNL),\noffering robust and inherently explainable alternative to well-known\nclassification and regression-based approaches. We compare the performance of\nproposed methodology with various regression and classification-based models.\nThe CoordAtt UNet model achieved the highest accuracy of 88.63% $\\pm$ 2.84 and\nspecificity of 90.25% $\\pm$ 4.04, along with a noteworthy sensitivity of 86.04%\n$\\pm$ 3.41. In landmark detection, the same model also recorded the lowest mean\nerrors in key anatomical points and the smallest angular error of 2.42 degrees.\nOur results indicate that models incorporating attention mechanisms and\nCoordConv module increase the accuracy in classifying breast positioning\nquality and detecting anatomical landmarks. Furthermore, we make the labels and\nsource codes available to the community to initiate an open research area for\nmammography, accessible at https://github.com/tanyelai/deep-breast-positioning.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "J.3"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10796v1",
    "published_date": "2024-07-15 15:14:10 UTC",
    "updated_date": "2024-07-15 15:14:10 UTC"
  },
  {
    "arxiv_id": "2407.10794v1",
    "title": "Graphusion: Leveraging Large Language Models for Scientific Knowledge Graph Fusion and Construction in NLP Education",
    "authors": [
      "Rui Yang",
      "Boming Yang",
      "Sixun Ouyang",
      "Tianwei She",
      "Aosong Feng",
      "Yuang Jiang",
      "Freddy Lecue",
      "Jinghui Lu",
      "Irene Li"
    ],
    "abstract": "Knowledge graphs (KGs) are crucial in the field of artificial intelligence\nand are widely applied in downstream tasks, such as enhancing Question\nAnswering (QA) systems. The construction of KGs typically requires significant\neffort from domain experts. Recently, Large Language Models (LLMs) have been\nused for knowledge graph construction (KGC), however, most existing approaches\nfocus on a local perspective, extracting knowledge triplets from individual\nsentences or documents. In this work, we introduce Graphusion, a zero-shot KGC\nframework from free text. The core fusion module provides a global view of\ntriplets, incorporating entity merging, conflict resolution, and novel triplet\ndiscovery. We showcase how Graphusion could be applied to the natural language\nprocessing (NLP) domain and validate it in the educational scenario.\nSpecifically, we introduce TutorQA, a new expert-verified benchmark for graph\nreasoning and QA, comprising six tasks and a total of 1,200 QA pairs. Our\nevaluation demonstrates that Graphusion surpasses supervised baselines by up to\n10% in accuracy on link prediction. Additionally, it achieves average scores of\n2.92 and 2.37 out of 3 in human evaluations for concept entity extraction and\nrelation recognition, respectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages, 11 figures, 13 tables. arXiv admin note: substantial text\n  overlap with arXiv:2402.14293",
    "pdf_url": "http://arxiv.org/pdf/2407.10794v1",
    "published_date": "2024-07-15 15:13:49 UTC",
    "updated_date": "2024-07-15 15:13:49 UTC"
  },
  {
    "arxiv_id": "2407.10793v1",
    "title": "GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework",
    "authors": [
      "Hannah Sansford",
      "Nicholas Richardson",
      "Hermina Petric Maretic",
      "Juba Nait Saada"
    ],
    "abstract": "Methods to evaluate Large Language Model (LLM) responses and detect\ninconsistencies, also known as hallucinations, with respect to the provided\nknowledge, are becoming increasingly important for LLM applications. Current\nmetrics fall short in their ability to provide explainable decisions,\nsystematically check all pieces of information in the response, and are often\ntoo computationally expensive to be used in practice. We present GraphEval: a\nhallucination evaluation framework based on representing information in\nKnowledge Graph (KG) structures. Our method identifies the specific triples in\nthe KG that are prone to hallucinations and hence provides more insight into\nwhere in the response a hallucination has occurred, if at all, than previous\nmethods. Furthermore, using our approach in conjunction with state-of-the-art\nnatural language inference (NLI) models leads to an improvement in balanced\naccuracy on various hallucination benchmarks, compared to using the raw NLI\nmodels. Lastly, we explore the use of GraphEval for hallucination correction by\nleveraging the structure of the KG, a method we name GraphCorrect, and\ndemonstrate that the majority of hallucinations can indeed be rectified.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, to be published at KiL'24: Workshop on Knowledge-infused\n  Learning co-located with 30th ACM KDD Conference, August 26, 2024, Barcelona,\n  Spain",
    "pdf_url": "http://arxiv.org/pdf/2407.10793v1",
    "published_date": "2024-07-15 15:11:16 UTC",
    "updated_date": "2024-07-15 15:11:16 UTC"
  },
  {
    "arxiv_id": "2407.10784v4",
    "title": "AdapTable: Test-Time Adaptation for Tabular Data via Shift-Aware Uncertainty Calibrator and Label Distribution Handler",
    "authors": [
      "Changhun Kim",
      "Taewon Kim",
      "Seungyeon Woo",
      "June Yong Yang",
      "Eunho Yang"
    ],
    "abstract": "In real-world scenarios, tabular data often suffer from distribution shifts\nthat threaten the performance of machine learning models. Despite its\nprevalence and importance, handling distribution shifts in the tabular domain\nremains underexplored due to the inherent challenges within the tabular data\nitself. In this sense, test-time adaptation (TTA) offers a promising solution\nby adapting models to target data without accessing source data, crucial for\nprivacy-sensitive tabular domains. However, existing TTA methods either 1)\noverlook the nature of tabular distribution shifts, often involving label\ndistribution shifts, or 2) impose architectural constraints on the model,\nleading to a lack of applicability. To this end, we propose AdapTable, a novel\nTTA framework for tabular data. AdapTable operates in two stages: 1)\ncalibrating model predictions using a shift-aware uncertainty calibrator, and\n2) adjusting these predictions to match the target label distribution with a\nlabel distribution handler. We validate the effectiveness of AdapTable through\ntheoretical analysis and extensive experiments on various distribution shift\nscenarios. Our results demonstrate AdapTable's ability to handle various\nreal-world distribution shifts, achieving up to a 16% improvement on the HELOC\ndataset.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS Workshop on Table Representation Learning (NeurIPSW-TRL),\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2407.10784v4",
    "published_date": "2024-07-15 15:02:53 UTC",
    "updated_date": "2025-02-12 05:02:00 UTC"
  },
  {
    "arxiv_id": "2407.11106v1",
    "title": "Deep Learning Evidence for Global Optimality of Gerver's Sofa",
    "authors": [
      "Kuangdai Leng",
      "Jia Bi",
      "Jaehoon Cha",
      "Samuel Pinilla",
      "Jeyan Thiyagalingam"
    ],
    "abstract": "The Moving Sofa Problem, formally proposed by Leo Moser in 1966, seeks to\ndetermine the largest area of a two-dimensional shape that can navigate through\nan $L$-shaped corridor with unit width. The current best lower bound is about\n2.2195, achieved by Joseph Gerver in 1992, though its global optimality remains\nunproven. In this paper, we investigate this problem by leveraging the\nuniversal approximation strength and computational efficiency of neural\nnetworks. We report two approaches, both supporting Gerver's conjecture that\nhis shape is the unique global maximum. Our first approach is continuous\nfunction learning. We drop Gerver's assumptions that i) the rotation of the\ncorridor is monotonic and symmetric and, ii) the trajectory of its corner as a\nfunction of rotation is continuously differentiable. We parameterize rotation\nand trajectory by independent piecewise linear neural networks (with input\nbeing some pseudo time), allowing for rich movements such as backward rotation\nand pure translation. We then compute the sofa area as a differentiable\nfunction of rotation and trajectory using our \"waterfall\" algorithm. Our final\nloss function includes differential terms and initial conditions, leveraging\nthe principles of physics-informed machine learning. Under such settings,\nextensive training starting from diverse function initialization and\nhyperparameters is conducted, unexceptionally showing rapid convergence to\nGerver's solution. Our second approach is via discrete optimization of the\nKallus-Romik upper bound, which converges to the maximum sofa area from above\nas the number of rotation angles increases. We uplift this number to 10000 to\nreveal its asymptotic behavior. It turns out that the upper bound yielded by\nour models does converge to Gerver's area (within an error of 0.01% when the\nnumber of angles reaches 2100). We also improve their five-angle upper bound\nfrom 2.37 to 2.3337.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.11106v1",
    "published_date": "2024-07-15 14:59:32 UTC",
    "updated_date": "2024-07-15 14:59:32 UTC"
  },
  {
    "arxiv_id": "2407.10768v5",
    "title": "ISMRNN: An Implicitly Segmented RNN Method with Mamba for Long-Term Time Series Forecasting",
    "authors": [
      "GaoXiang Zhao",
      "Li Zhou",
      "XiaoQiang Wang"
    ],
    "abstract": "Long time series forecasting aims to utilize historical information to\nforecast future states over extended horizons. Traditional RNN-based series\nforecasting methods struggle to effectively address long-term dependencies and\ngradient issues in long time series problems. Recently, SegRNN has emerged as a\nleading RNN-based model tailored for long-term series forecasting,\ndemonstrating state-of-the-art performance while maintaining a streamlined\narchitecture through innovative segmentation and parallel decoding techniques.\nNevertheless, SegRNN has several limitations: its fixed segmentation disrupts\ndata continuity and fails to effectively leverage information across different\nsegments, the segmentation strategy employed by SegRNN does not fundamentally\naddress the issue of information loss within the recurrent structure. To\naddress these issues, we propose the ISMRNN method with three key enhancements:\nwe introduce an implicit segmentation structure to decompose the time series\nand map it to segmented hidden states, resulting in denser information exchange\nduring the segmentation phase. Additionally, we incorporate residual structures\nin the encoding layer to mitigate information loss within the recurrent\nstructure. To extract information more effectively, we further integrate the\nMamba architecture to enhance time series information extraction. Experiments\non several real-world long time series forecasting datasets demonstrate that\nour model surpasses the performance of current state-of-the-art models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10768v5",
    "published_date": "2024-07-15 14:50:15 UTC",
    "updated_date": "2024-08-04 07:53:03 UTC"
  },
  {
    "arxiv_id": "2407.21030v1",
    "title": "Cluster and Separate: a GNN Approach to Voice and Staff Prediction for Score Engraving",
    "authors": [
      "Francesco Foscarin",
      "Emmanouil Karystinaios",
      "Eita Nakamura",
      "Gerhard Widmer"
    ],
    "abstract": "This paper approaches the problem of separating the notes from a quantized\nsymbolic music piece (e.g., a MIDI file) into multiple voices and staves. This\nis a fundamental part of the larger task of music score engraving (or score\ntypesetting), which aims to produce readable musical scores for human\nperformers. We focus on piano music and support homophonic voices, i.e., voices\nthat can contain chords, and cross-staff voices, which are notably difficult\ntasks that have often been overlooked in previous research. We propose an\nend-to-end system based on graph neural networks that clusters notes that\nbelong to the same chord and connects them with edges if they are part of a\nvoice. Our results show clear and consistent improvements over a previous\napproach on two datasets of different styles. To aid the qualitative analysis\nof our results, we support the export in symbolic music formats and provide a\ndirect visualization of our outputs graph over the musical score. All code and\npre-trained models are available at https://github.com/CPJKU/piano_svsep",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted at the 25th International Society for Music Information\n  Retrieval (ISMIR) 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.21030v1",
    "published_date": "2024-07-15 14:36:13 UTC",
    "updated_date": "2024-07-15 14:36:13 UTC"
  },
  {
    "arxiv_id": "2407.10758v1",
    "title": "Continual Deep Learning on the Edge via Stochastic Local Competition among Subnetworks",
    "authors": [
      "Theodoros Christophides",
      "Kyriakos Tolias",
      "Sotirios Chatzis"
    ],
    "abstract": "Continual learning on edge devices poses unique challenges due to stringent\nresource constraints. This paper introduces a novel method that leverages\nstochastic competition principles to promote sparsity, significantly reducing\ndeep network memory footprint and computational demand. Specifically, we\npropose deep networks that comprise blocks of units that compete locally to win\nthe representation of each arising new task; competition takes place in a\nstochastic manner. This type of network organization results in sparse\ntask-specific representations from each network layer; the sparsity pattern is\nobtained during training and is different among tasks. Crucially, our method\nsparsifies both the weights and the weight gradients, thus facilitating\ntraining on edge devices. This is performed on the grounds of winning\nprobability for each unit in a block. During inference, the network retains\nonly the winning unit and zeroes-out all weights pertaining to non-winning\nunits for the task at hand. Thus, our approach is specifically tailored for\ndeployment on edge devices, providing an efficient and scalable solution for\ncontinual learning in resource-limited environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10758v1",
    "published_date": "2024-07-15 14:36:05 UTC",
    "updated_date": "2024-07-15 14:36:05 UTC"
  },
  {
    "arxiv_id": "2407.11105v1",
    "title": "Impacts of Data Preprocessing and Hyperparameter Optimization on the Performance of Machine Learning Models Applied to Intrusion Detection Systems",
    "authors": [
      "Mateus Guimarães Lima",
      "Antony Carvalho",
      "João Gabriel Álvares",
      "Clayton Escouper das Chagas",
      "Ronaldo Ribeiro Goldschmidt"
    ],
    "abstract": "In the context of cybersecurity of modern communications networks, Intrusion\nDetection Systems (IDS) have been continuously improved, many of them\nincorporating machine learning (ML) techniques to identify threats. Although\nthere are researches focused on the study of these techniques applied to IDS,\nthe state-of-the-art lacks works concentrated exclusively on the evaluation of\nthe impacts of data pre-processing actions and the optimization of the values\nof the hyperparameters of the ML algorithms in the construction of the models\nof threat identification. This article aims to present a study that fills this\nresearch gap. For that, experiments were carried out with two data sets,\ncomparing attack scenarios with variations of pre-processing techniques and\noptimization of hyperparameters. The results confirm that the proper\napplication of these techniques, in general, makes the generated classification\nmodels more robust and greatly reduces the execution times of these models'\ntraining and testing processes.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11105v1",
    "published_date": "2024-07-15 14:30:25 UTC",
    "updated_date": "2024-07-15 14:30:25 UTC"
  },
  {
    "arxiv_id": "2407.11104v1",
    "title": "Exploring the Potentials and Challenges of Deep Generative Models in Product Design Conception",
    "authors": [
      "Phillip Mueller",
      "Lars Mikelsons"
    ],
    "abstract": "The synthesis of product design concepts stands at the crux of early-phase\ndevelopment processes for technical products, traditionally posing an intricate\ninterdisciplinary challenge. The application of deep learning methods,\nparticularly Deep Generative Models (DGMs), holds the promise of automating and\nstreamlining manual iterations and therefore introducing heightened levels of\ninnovation and efficiency. However, DGMs have yet to be widely adopted into the\nsynthesis of product design concepts. This paper aims to explore the reasons\nbehind this limited application and derive the requirements for successful\nintegration of these technologies. We systematically analyze DGM-families (VAE,\nGAN, Diffusion, Transformer, Radiance Field), assessing their strengths,\nweaknesses, and general applicability for product design conception. Our\nobjective is to provide insights that simplify the decision-making process for\nengineers, helping them determine which method might be most effective for\ntheir specific challenges. Recognizing the rapid evolution of this field, we\nhope that our analysis contributes to a fundamental understanding and guides\npractitioners towards the most promising approaches. This work seeks not only\nto illuminate current challenges but also to propose potential solutions,\nthereby offering a clear roadmap for leveraging DGMs in the realm of product\ndesign conception.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11104v1",
    "published_date": "2024-07-15 14:28:50 UTC",
    "updated_date": "2024-07-15 14:28:50 UTC"
  },
  {
    "arxiv_id": "2407.10743v1",
    "title": "Scaling 3D Reasoning with LMMs to Large Robot Mission Environments Using Datagraphs",
    "authors": [
      "W. J. Meijer",
      "A. C. Kemmeren",
      "E. H. J. Riemens",
      "J. E. Fransman",
      "M. van Bekkum",
      "G. J. Burghouts",
      "J. D. van Mil"
    ],
    "abstract": "This paper addresses the challenge of scaling Large Multimodal Models (LMMs)\nto expansive 3D environments. Solving this open problem is especially relevant\nfor robot deployment in many first-responder scenarios, such as\nsearch-and-rescue missions that cover vast spaces. The use of LMMs in these\nsettings is currently hampered by the strict context windows that limit the\nLMM's input size. We therefore introduce a novel approach that utilizes a\ndatagraph structure, which allows the LMM to iteratively query smaller sections\nof a large environment. Using the datagraph in conjunction with graph traversal\nalgorithms, we can prioritize the most relevant locations to the query, thereby\nimproving the scalability of 3D scene language tasks. We illustrate the\ndatagraph using 3D scenes, but these can be easily substituted by other dense\nmodalities that represent the environment, such as pointclouds or Gaussian\nsplats. We demonstrate the potential to use the datagraph for two 3D scene\nlanguage task use cases, in a search-and-rescue mission example.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to the RSS Workshop on Semantics for Robotics: From\n  Environment Understanding and Reasoning to Safe Interaction 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.10743v1",
    "published_date": "2024-07-15 14:16:13 UTC",
    "updated_date": "2024-07-15 14:16:13 UTC"
  },
  {
    "arxiv_id": "2407.10737v1",
    "title": "Aligning Neuronal Coding of Dynamic Visual Scenes with Foundation Vision Models",
    "authors": [
      "Rining Wu",
      "Feixiang Zhou",
      "Ziwei Yin",
      "Jian K. Liu"
    ],
    "abstract": "Our brains represent the ever-changing environment with neurons in a highly\ndynamic fashion. The temporal features of visual pixels in dynamic natural\nscenes are entrapped in the neuronal responses of the retina. It is crucial to\nestablish the intrinsic temporal relationship between visual pixels and\nneuronal responses. Recent foundation vision models have paved an advanced way\nof understanding image pixels. Yet, neuronal coding in the brain largely lacks\na deep understanding of its alignment with pixels. Most previous studies employ\nstatic images or artificial videos derived from static images for emulating\nmore real and complicated stimuli. Despite these simple scenarios effectively\nhelp to separate key factors influencing visual coding, complex temporal\nrelationships receive no consideration. To decompose the temporal features of\nvisual coding in natural scenes, here we propose Vi-ST, a spatiotemporal\nconvolutional neural network fed with a self-supervised Vision Transformer\n(ViT) prior, aimed at unraveling the temporal-based encoding patterns of\nretinal neuronal populations. The model demonstrates robust predictive\nperformance in generalization tests. Furthermore, through detailed ablation\nexperiments, we demonstrate the significance of each temporal module.\nFurthermore, we introduce a visual coding evaluation metric designed to\nintegrate temporal considerations and compare the impact of different numbers\nof neuronal populations on complementary coding. In conclusion, our proposed\nVi-ST demonstrates a novel modeling framework for neuronal coding of dynamic\nvisual scenes in the brain, effectively aligning our brain representation of\nvideo with neuronal activity. The code is available at\nhttps://github.com/wurining/Vi-ST.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This article is accepted by ECCV 2024, which ID is 12149. Accepted\n  papers' id can be found in:\n  https://eccv2024.ecva.net/Conferences/2024/AcceptedPapers",
    "pdf_url": "http://arxiv.org/pdf/2407.10737v1",
    "published_date": "2024-07-15 14:06:13 UTC",
    "updated_date": "2024-07-15 14:06:13 UTC"
  },
  {
    "arxiv_id": "2407.10735v2",
    "title": "Transforming Agency. On the mode of existence of Large Language Models",
    "authors": [
      "Xabier E. Barandiaran",
      "Lola S. Almendros"
    ],
    "abstract": "This paper investigates the ontological characterization of Large Language\nModels (LLMs) like ChatGPT. Between inflationary and deflationary accounts, we\npay special attention to their status as agents. This requires explaining in\ndetail the architecture, processing, and training procedures that enable LLMs\nto display their capacities, and the extensions used to turn LLMs into\nagent-like systems. After a systematic analysis we conclude that a LLM fails to\nmeet necessary and sufficient conditions for autonomous agency in the light of\nembodied theories of mind: the individuality condition (it is not the product\nof its own activity, it is not even directly affected by it), the normativity\ncondition (it does not generate its own norms or goals), and, partially the\ninteractional asymmetry condition (it is not the origin and sustained source of\nits interaction with the environment). If not agents, then ... what are LLMs?\nWe argue that ChatGPT should be characterized as an interlocutor or linguistic\nautomaton, a library-that-talks, devoid of (autonomous) agency, but capable to\nengage performatively on non-purposeful yet purpose-structured and\npurpose-bounded tasks. When interacting with humans, a \"ghostly\" component of\nthe human-machine interaction makes it possible to enact genuine conversational\nexperiences with LLMs. Despite their lack of sensorimotor and biological\nembodiment, LLMs textual embodiment (the training corpus) and resource-hungry\ncomputational embodiment, significantly transform existing forms of human\nagency. Beyond assisted and extended agency, the LLM-human coupling can produce\nmidtended forms of agency, closer to the production of intentional agency than\nto the extended instrumentality of any previous technologies.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10735v2",
    "published_date": "2024-07-15 14:01:35 UTC",
    "updated_date": "2024-07-16 09:53:15 UTC"
  },
  {
    "arxiv_id": "2407.10736v1",
    "title": "When Synthetic Traces Hide Real Content: Analysis of Stable Diffusion Image Laundering",
    "authors": [
      "Sara Mandelli",
      "Paolo Bestagini",
      "Stefano Tubaro"
    ],
    "abstract": "In recent years, methods for producing highly realistic synthetic images have\nsignificantly advanced, allowing the creation of high-quality images from text\nprompts that describe the desired content. Even more impressively, Stable\nDiffusion (SD) models now provide users with the option of creating synthetic\nimages in an image-to-image translation fashion, modifying images in the latent\nspace of advanced autoencoders. This striking evolution, however, brings an\nalarming consequence: it is possible to pass an image through SD autoencoders\nto reproduce a synthetic copy of the image with high realism and almost no\nvisual artifacts. This process, known as SD image laundering, can transform\nreal images into lookalike synthetic ones and risks complicating forensic\nanalysis for content authenticity verification. Our paper investigates the\nforensic implications of image laundering, revealing a serious potential to\nobscure traces of real content, including sensitive and harmful materials that\ncould be mistakenly classified as synthetic, thereby undermining the protection\nof individuals depicted. To address this issue, we propose a two-stage\ndetection pipeline that effectively differentiates between pristine, laundered,\nand fully synthetic images (those generated from text prompts), showing\nrobustness across various conditions. Finally, we highlight another alarming\nproperty of image laundering, which appears to mask the unique artifacts\nexploited by forensic detectors to solve the camera model identification task,\nstrongly undermining their performance. Our experimental code is available at\nhttps://github.com/polimi-ispl/synthetic-image-detection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10736v1",
    "published_date": "2024-07-15 14:01:35 UTC",
    "updated_date": "2024-07-15 14:01:35 UTC"
  },
  {
    "arxiv_id": "2407.10734v2",
    "title": "On-Device Training of Fully Quantized Deep Neural Networks on Cortex-M Microcontrollers",
    "authors": [
      "Mark Deutel",
      "Frank Hannig",
      "Christopher Mutschler",
      "Jürgen Teich"
    ],
    "abstract": "On-device training of DNNs allows models to adapt and fine-tune to newly\ncollected data or changing domains while deployed on microcontroller units\n(MCUs). However, DNN training is a resource-intensive task, making the\nimplementation and execution of DNN training algorithms on MCUs challenging due\nto low processor speeds, constrained throughput, limited floating-point\nsupport, and memory constraints. In this work, we explore on-device training of\nDNNs for Cortex-M MCUs. We present a method that enables efficient training of\nDNNs completely in place on the MCU using fully quantized training (FQT) and\ndynamic partial gradient updates. We demonstrate the feasibility of our\napproach on multiple vision and time-series datasets and provide insights into\nthe tradeoff between training accuracy, memory overhead, energy, and latency on\nreal hardware.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.10734v2",
    "published_date": "2024-07-15 14:01:34 UTC",
    "updated_date": "2024-08-28 15:36:08 UTC"
  },
  {
    "arxiv_id": "2407.10725v1",
    "title": "CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated Responses",
    "authors": [
      "Jing Yao",
      "Xiaoyuan Yi",
      "Xing Xie"
    ],
    "abstract": "The rapid progress in Large Language Models (LLMs) poses potential risks such\nas generating unethical content. Assessing LLMs' values can help expose their\nmisalignment, but relies on reference-free evaluators, e.g., fine-tuned LLMs or\nclose-source ones like GPT-4, to identify values reflected in generated\nresponses. Nevertheless, these evaluators face two challenges in open-ended\nvalue evaluation: they should align with changing human value definitions with\nminimal annotation, against their own bias (adaptability), and detect varying\nvalue expressions and scenarios robustly (generalizability). To handle these\nchallenges, we introduce CLAVE, a novel framework which integrates two\ncomplementary LLMs, a large one to extract high-level value concepts from a few\nhuman labels, leveraging its extensive knowledge and generalizability, and a\nsmaller one fine-tuned on such concepts to better align with human value\nunderstanding. This dual-model approach enables calibration with any value\nsystems using <100 human-labeled samples per value type. Then we present\nValEval, a comprehensive dataset comprising 13k+ (text,value,label) tuples\nacross diverse domains, covering three major value systems. We benchmark the\ncapabilities of 12+ popular LLM evaluators and analyze their strengths and\nweaknesses. Our findings reveal that combining fine-tuned small models and\nprompt-based large ones serves as a superior balance in value evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10725v1",
    "published_date": "2024-07-15 13:51:37 UTC",
    "updated_date": "2024-07-15 13:51:37 UTC"
  },
  {
    "arxiv_id": "2407.10718v2",
    "title": "Sibyl: Simple yet Effective Agent Framework for Complex Real-world Reasoning",
    "authors": [
      "Yulong Wang",
      "Tianhao Shen",
      "Lifeng Liu",
      "Jian Xie"
    ],
    "abstract": "Existing agents based on large language models (LLMs) demonstrate robust\nproblem-solving capabilities by integrating LLMs' inherent knowledge, strong\nin-context learning and zero-shot capabilities, and the use of tools combined\nwith intricately designed LLM invocation workflows by humans. However, these\nagents still exhibit shortcomings in long-term reasoning and under-use the\npotential of existing tools, leading to noticeable deficiencies in complex\nreal-world reasoning scenarios. To address these limitations, we introduce\nSibyl, a simple yet powerful LLM-based agent framework designed to tackle\ncomplex reasoning tasks by efficiently leveraging a minimal set of tools.\nDrawing inspiration from Global Workspace Theory, Sibyl incorporates a global\nworkspace to enhance the management and sharing of knowledge and conversation\nhistory throughout the system. Furthermore, guided by Society of Mind Theory,\nSibyl implements a multi-agent debate-based jury to self-refine the final\nanswers, ensuring a comprehensive and balanced approach. This approach aims to\nreduce system complexity while expanding the scope of problems solvable-from\nmatters typically resolved by humans in minutes to those requiring hours or\neven days, thus facilitating a shift from System-1 to System-2 thinking. Sibyl\nhas been designed with a focus on scalability and ease of debugging by\nincorporating the concept of reentrancy from functional programming from its\ninception, with the aim of seamless and low effort integration in other LLM\napplications to improve capabilities. Our experimental results on the GAIA\nbenchmark test set reveal that the Sibyl agent instantiated with GPT-4 achieves\nstate-of-the-art performance with an average score of 34.55%, compared to other\nagents based on GPT-4. We hope that Sibyl can inspire more reliable and\nreusable LLM-based agent solutions to address complex real-world reasoning\ntasks.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Our code is available at https://github.com/Ag2S1/Sibyl-System",
    "pdf_url": "http://arxiv.org/pdf/2407.10718v2",
    "published_date": "2024-07-15 13:45:40 UTC",
    "updated_date": "2024-07-16 14:16:53 UTC"
  },
  {
    "arxiv_id": "2408.00768v1",
    "title": "Comparing Optical Flow and Deep Learning to Enable Computationally Efficient Traffic Event Detection with Space-Filling Curves",
    "authors": [
      "Tayssir Bouraffa",
      "Elias Kjellberg Carlson",
      "Erik Wessman",
      "Ali Nouri",
      "Pierre Lamart",
      "Christian Berger"
    ],
    "abstract": "Gathering data and identifying events in various traffic situations remains\nan essential challenge for the systematic evaluation of a perception system's\nperformance. Analyzing large-scale, typically unstructured, multi-modal, time\nseries data obtained from video, radar, and LiDAR is computationally demanding,\nparticularly when meta-information or annotations are missing. We compare\nOptical Flow (OF) and Deep Learning (DL) to feed computationally efficient\nevent detection via space-filling curves on video data from a forward-facing,\nin-vehicle camera. Our first approach leverages unexpected disturbances in the\nOF field from vehicle surroundings; the second approach is a DL model trained\non human visual attention to predict a driver's gaze to spot potential event\nlocations. We feed these results to a space-filling curve to reduce\ndimensionality and achieve computationally efficient event retrieval. We\nsystematically evaluate our concept by obtaining characteristic patterns for\nboth approaches from a large-scale virtual dataset (SMIRK) and applied our\nfindings to the Zenseact Open Dataset (ZOD), a large multi-modal, real-world\ndataset, collected over two years in 14 different European countries. Our\nresults yield that the OF approach excels in specificity and reduces false\npositives, while the DL approach demonstrates superior sensitivity. Both\napproaches offer comparable processing speed, making them suitable for\nreal-time applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "27th IEEE International Conference on Intelligent Transportation\n  Systems (IEEE ITSC 2024)",
    "pdf_url": "http://arxiv.org/pdf/2408.00768v1",
    "published_date": "2024-07-15 13:44:52 UTC",
    "updated_date": "2024-07-15 13:44:52 UTC"
  },
  {
    "arxiv_id": "2407.10714v1",
    "title": "SEMINAR: Search Enhanced Multi-modal Interest Network and Approximate Retrieval for Lifelong Sequential Recommendation",
    "authors": [
      "Kaiming Shen",
      "Xichen Ding",
      "Zixiang Zheng",
      "Yuqi Gong",
      "Qianqian Li",
      "Zhongyi Liu",
      "Guannan Zhang"
    ],
    "abstract": "The modeling of users' behaviors is crucial in modern recommendation systems.\nA lot of research focuses on modeling users' lifelong sequences, which can be\nextremely long and sometimes exceed thousands of items. These models use the\ntarget item to search for the most relevant items from the historical sequence.\nHowever, training lifelong sequences in click through rate (CTR) prediction or\npersonalized search ranking (PSR) is extremely difficult due to the\ninsufficient learning problem of ID embedding, especially when the IDs in the\nlifelong sequence features do not exist in the samples of training dataset.\nAdditionally, existing target attention mechanisms struggle to learn the\nmulti-modal representations of items in the sequence well. The distribution of\nmulti-modal embedding (text, image and attributes) output of user's interacted\nitems are not properly aligned and there exist divergence across modalities. We\nalso observe that users' search query sequences and item browsing sequences can\nfully depict users' intents and benefit from each other. To address these\nchallenges, we propose a unified lifelong multi-modal sequence model called\nSEMINAR-Search Enhanced Multi-Modal Interest Network and Approximate Retrieval.\nSpecifically, a network called Pretraining Search Unit (PSU) learns the\nlifelong sequences of multi-modal query-item pairs in a pretraining-finetuning\nmanner with multiple objectives: multi-modal alignment, next query-item pair\nprediction, query-item relevance prediction, etc. After pretraining, the\ndownstream model restores the pretrained embedding as initialization and\nfinetunes the network. To accelerate the online retrieval speed of multi-modal\nembedding, we propose a multi-modal codebook-based product quantization\nstrategy to approximate the exact attention calculati",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "9 pages,code released",
    "pdf_url": "http://arxiv.org/pdf/2407.10714v1",
    "published_date": "2024-07-15 13:33:30 UTC",
    "updated_date": "2024-07-15 13:33:30 UTC"
  },
  {
    "arxiv_id": "2407.10689v5",
    "title": "Classification of Heart Sounds Using Multi-Branch Deep Convolutional Network and LSTM-CNN",
    "authors": [
      "Seyed Amir Latifi",
      "Hassan Ghassemian",
      "Maryam Imani"
    ],
    "abstract": "This paper presents a fast and cost-effective method for diagnosing cardiac\nabnormalities with high accuracy and reliability using low-cost systems in\nclinics. The primary limitation of automatic diagnosing of cardiac diseases is\nthe rarity of correct and acceptable labeled samples, which can be expensive to\nprepare. To address this issue, two methods are proposed in this work. The\nfirst method is a unique Multi-Branch Deep Convolutional Neural Network (MBDCN)\narchitecture inspired by human auditory processing, specifically designed to\noptimize feature extraction by employing various sizes of convolutional filters\nand audio signal power spectrum as input. In the second method, called as Long\nshort-term memory-Convolutional Neural (LSCN) model, Additionally, the network\narchitecture includes Long Short-Term Memory (LSTM) network blocks to improve\nfeature extraction in the time domain. The innovative approach of combining\nmultiple parallel branches consisting of the one-dimensional convolutional\nlayers along with LSTM blocks helps in achieving superior results in audio\nsignal processing tasks. The experimental results demonstrate superiority of\nthe proposed methods over the state-of-the-art techniques. The overall\nclassification accuracy of heart sounds with the LSCN network is more than 96%.\nThe efficiency of this network is significant compared to common feature\nextraction methods such as Mel Frequency Cepstral Coefficients (MFCC) and\nwavelet transform. Therefore, the proposed method shows promising results in\nthe automatic analysis of heart sounds and has potential applications in the\ndiagnosis and early detection of cardiovascular diseases.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "eess.SP",
    "comment": "22 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.10689v5",
    "published_date": "2024-07-15 13:02:54 UTC",
    "updated_date": "2024-11-21 17:32:38 UTC"
  },
  {
    "arxiv_id": "2407.10683v1",
    "title": "Addressing Image Hallucination in Text-to-Image Generation through Factual Image Retrieval",
    "authors": [
      "Youngsun Lim",
      "Hyunjung Shim"
    ],
    "abstract": "Text-to-image generation has shown remarkable progress with the emergence of\ndiffusion models. However, these models often generate factually inconsistent\nimages, failing to accurately reflect the factual information and common sense\nconveyed by the input text prompts. We refer to this issue as Image\nhallucination. Drawing from studies on hallucinations in language models, we\nclassify this problem into three types and propose a methodology that uses\nfactual images retrieved from external sources to generate realistic images.\nDepending on the nature of the hallucination, we employ off-the-shelf image\nediting tools, either InstructPix2Pix or IP-Adapter, to leverage factual\ninformation from the retrieved image. This approach enables the generation of\nimages that accurately reflect the facts and common sense.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper has been accepted for oral presentation at the IJCAI 2024\n  Workshop on Trustworthy Interactive Decision-Making with Foundation Models",
    "pdf_url": "http://arxiv.org/pdf/2407.10683v1",
    "published_date": "2024-07-15 12:59:03 UTC",
    "updated_date": "2024-07-15 12:59:03 UTC"
  },
  {
    "arxiv_id": "2407.10671v4",
    "title": "Qwen2 Technical Report",
    "authors": [
      "An Yang",
      "Baosong Yang",
      "Binyuan Hui",
      "Bo Zheng",
      "Bowen Yu",
      "Chang Zhou",
      "Chengpeng Li",
      "Chengyuan Li",
      "Dayiheng Liu",
      "Fei Huang",
      "Guanting Dong",
      "Haoran Wei",
      "Huan Lin",
      "Jialong Tang",
      "Jialin Wang",
      "Jian Yang",
      "Jianhong Tu",
      "Jianwei Zhang",
      "Jianxin Ma",
      "Jianxin Yang",
      "Jin Xu",
      "Jingren Zhou",
      "Jinze Bai",
      "Jinzheng He",
      "Junyang Lin",
      "Kai Dang",
      "Keming Lu",
      "Keqin Chen",
      "Kexin Yang",
      "Mei Li",
      "Mingfeng Xue",
      "Na Ni",
      "Pei Zhang",
      "Peng Wang",
      "Ru Peng",
      "Rui Men",
      "Ruize Gao",
      "Runji Lin",
      "Shijie Wang",
      "Shuai Bai",
      "Sinan Tan",
      "Tianhang Zhu",
      "Tianhao Li",
      "Tianyu Liu",
      "Wenbin Ge",
      "Xiaodong Deng",
      "Xiaohuan Zhou",
      "Xingzhang Ren",
      "Xinyu Zhang",
      "Xipin Wei",
      "Xuancheng Ren",
      "Xuejing Liu",
      "Yang Fan",
      "Yang Yao",
      "Yichang Zhang",
      "Yu Wan",
      "Yunfei Chu",
      "Yuqiong Liu",
      "Zeyu Cui",
      "Zhenru Zhang",
      "Zhifang Guo",
      "Zhihao Fan"
    ],
    "abstract": "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "26 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2407.10671v4",
    "published_date": "2024-07-15 12:35:42 UTC",
    "updated_date": "2024-09-10 13:25:53 UTC"
  },
  {
    "arxiv_id": "2407.10670v1",
    "title": "Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems",
    "authors": [
      "Yunxiao Shi",
      "Xing Zi",
      "Zijing Shi",
      "Haimin Zhang",
      "Qiang Wu",
      "Min Xu"
    ],
    "abstract": "Retrieval-augmented generation (RAG) techniques leverage the in-context\nlearning capabilities of large language models (LLMs) to produce more accurate\nand relevant responses. Originating from the simple 'retrieve-then-read'\napproach, the RAG framework has evolved into a highly flexible and modular\nparadigm. A critical component, the Query Rewriter module, enhances knowledge\nretrieval by generating a search-friendly query. This method aligns input\nquestions more closely with the knowledge base. Our research identifies\nopportunities to enhance the Query Rewriter module to Query Rewriter+ by\ngenerating multiple queries to overcome the Information Plateaus associated\nwith a single query and by rewriting questions to eliminate Ambiguity, thereby\nclarifying the underlying intent. We also find that current RAG systems exhibit\nissues with Irrelevant Knowledge; to overcome this, we propose the Knowledge\nFilter. These two modules are both based on the instruction-tuned Gemma-2B\nmodel, which together enhance response quality. The final identified issue is\nRedundant Retrieval; we introduce the Memory Knowledge Reservoir and the\nRetriever Trigger to solve this. The former supports the dynamic expansion of\nthe RAG system's knowledge base in a parameter-free manner, while the latter\noptimizes the cost for accessing external knowledge, thereby improving resource\nutilization and response efficiency. These four RAG modules synergistically\nimprove the response quality and efficiency of the RAG system. The\neffectiveness of these modules has been validated through experiments and\nablation studies across six common QA datasets. The source code can be accessed\nat https://github.com/Ancientshi/ERM4.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ECAI2024 #1304",
    "pdf_url": "http://arxiv.org/pdf/2407.10670v1",
    "published_date": "2024-07-15 12:35:00 UTC",
    "updated_date": "2024-07-15 12:35:00 UTC"
  },
  {
    "arxiv_id": "2407.10663v1",
    "title": "Spatio-temporal neural distance fields for conditional generative modeling of the heart",
    "authors": [
      "Kristine Sørensen",
      "Paula Diez",
      "Jan Margeta",
      "Yasmin El Youssef",
      "Michael Pham",
      "Jonas Jalili Pedersen",
      "Tobias Kühl",
      "Ole de Backer",
      "Klaus Kofoed",
      "Oscar Camara",
      "Rasmus Paulsen"
    ],
    "abstract": "The rhythmic pumping motion of the heart stands as a cornerstone in life, as\nit circulates blood to the entire human body through a series of carefully\ntimed contractions of the individual chambers. Changes in the size, shape and\nmovement of the chambers can be important markers for cardiac disease and\nmodeling this in relation to clinical demography or disease is therefore of\ninterest. Existing methods for spatio-temporal modeling of the human heart\nrequire shape correspondence over time or suffer from large memory\nrequirements, making it difficult to use for complex anatomies. We introduce a\nnovel conditional generative model, where the shape and movement is modeled\nimplicitly in the form of a spatio-temporal neural distance field and\nconditioned on clinical demography. The model is based on an auto-decoder\narchitecture and aims to disentangle the individual variations from that\nrelated to the clinical demography. It is tested on the left atrium (including\nthe left atrial appendage), where it outperforms current state-of-the-art\nmethods for anatomical sequence completion and generates synthetic sequences\nthat realistically mimics the shape and motion of the real left atrium. In\npractice, this means we can infer functional measurements from a static image,\ngenerate synthetic populations with specified demography or disease and\ninvestigate how non-imaging clinical data effect the shape and motion of\ncardiac anatomies.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for MICCAI2024",
    "pdf_url": "http://arxiv.org/pdf/2407.10663v1",
    "published_date": "2024-07-15 12:26:52 UTC",
    "updated_date": "2024-07-15 12:26:52 UTC"
  },
  {
    "arxiv_id": "2407.10662v4",
    "title": "XEQ Scale for Evaluating XAI Experience Quality",
    "authors": [
      "Anjana Wijekoon",
      "Nirmalie Wiratunga",
      "David Corsar",
      "Kyle Martin",
      "Ikechukwu Nkisi-Orji",
      "Belen Díaz-Agudo",
      "Derek Bridge"
    ],
    "abstract": "Explainable Artificial Intelligence (XAI) aims to improve the transparency of\nautonomous decision-making through explanations. Recent literature has\nemphasised users' need for holistic \"multi-shot\" explanations and personalised\nengagement with XAI systems. We refer to this user-centred interaction as an\nXAI Experience. Despite advances in creating XAI experiences, evaluating them\nin a user-centred manner has remained challenging. In response, we developed\nthe XAI Experience Quality (XEQ) Scale. XEQ quantifies the quality of\nexperiences across four dimensions: learning, utility, fulfilment and\nengagement. These contributions extend the state-of-the-art of XAI evaluation,\nmoving beyond the one-dimensional metrics frequently developed to assess\nsingle-shot explanations. This paper presents the XEQ scale development and\nvalidation process, including content validation with XAI experts, and\ndiscriminant and construct validation through a large-scale pilot study. Our\npilot study results offer strong evidence that establishes the XEQ Scale as a\ncomprehensive framework for evaluating user-centred XAI experiences.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10662v4",
    "published_date": "2024-07-15 12:25:49 UTC",
    "updated_date": "2025-01-17 11:02:03 UTC"
  },
  {
    "arxiv_id": "2407.10657v3",
    "title": "An Empirical Study of Validating Synthetic Data for Formula Generation",
    "authors": [
      "Usneek Singh",
      "José Cambronero",
      "Sumit Gulwani",
      "Aditya Kanade",
      "Anirudh Khatry",
      "Vu Le",
      "Mukul Singh",
      "Gust Verbruggen"
    ],
    "abstract": "Large language models (LLMs) can be leveraged to help with writing formulas\nin spreadsheets, but resources on these formulas are scarce, impacting both the\nbase performance of pre-trained models and limiting the ability to fine-tune\nthem. Given a corpus of formulas, we can use a(nother) model to generate\nsynthetic natural language utterances for fine-tuning. However, it is important\nto validate whether the NL generated by the LLM is indeed accurate to be\nbeneficial for fine-tuning. In this paper, we provide empirical results on the\nimpact of validating these synthetic training examples with surrogate\nobjectives that evaluate the accuracy of the synthetic annotations. We\ndemonstrate that validation improves performance over raw data across four\nmodels (2 open and 2 closed weight). Interestingly, we show that although\nvalidation tends to prune more challenging examples, it increases the\ncomplexity of problems that models can solve after being fine-tuned on\nvalidated data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10657v3",
    "published_date": "2024-07-15 12:16:33 UTC",
    "updated_date": "2024-11-03 12:44:42 UTC"
  },
  {
    "arxiv_id": "2407.10645v2",
    "title": "Prompt Selection Matters: Enhancing Text Annotations for Social Sciences with Large Language Models",
    "authors": [
      "Louis Abraham",
      "Charles Arnal",
      "Antoine Marie"
    ],
    "abstract": "Large Language Models have recently been applied to text annotation tasks\nfrom social sciences, equalling or surpassing the performance of human workers\nat a fraction of the cost. However, no inquiry has yet been made on the impact\nof prompt selection on labelling accuracy. In this study, we show that\nperformance greatly varies between prompts, and we apply the method of\nautomatic prompt optimization to systematically craft high quality prompts. We\nalso provide the community with a simple, browser-based implementation of the\nmethod at https://prompt-ultra.github.io/ .",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10645v2",
    "published_date": "2024-07-15 12:04:32 UTC",
    "updated_date": "2025-03-10 10:35:53 UTC"
  },
  {
    "arxiv_id": "2407.10639v1",
    "title": "Risk-aware Trajectory Prediction by Incorporating Spatio-temporal Traffic Interaction Analysis",
    "authors": [
      "Divya Thuremella",
      "Lewis Ince",
      "Lars Kunze"
    ],
    "abstract": "To operate in open-ended environments where humans interact in complex,\ndiverse ways, autonomous robots must learn to predict their behaviour,\nespecially when that behavior is potentially dangerous to other agents or to\nthe robot. However, reducing the risk of accidents requires prior knowledge of\nwhere potential collisions may occur and how. Therefore, we propose to gain\nthis information by analyzing locations and speeds that commonly correspond to\nhigh-risk interactions within the dataset, and use it within training to\ngenerate better predictions in high risk situations. Through these\nlocation-based and speed-based re-weighting techniques, we achieve improved\noverall performance, as measured by most-likely FDE and KDE, as well as\nimproved performance on high-speed vehicles, and vehicles within high-risk\nlocations.\n  2023 IEEE International Conference on Robotics and Automation (ICRA)",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10639v1",
    "published_date": "2024-07-15 11:57:06 UTC",
    "updated_date": "2024-07-15 11:57:06 UTC"
  },
  {
    "arxiv_id": "2407.10632v2",
    "title": "Bidirectional Stereo Image Compression with Cross-Dimensional Entropy Model",
    "authors": [
      "Zhening Liu",
      "Xinjie Zhang",
      "Jiawei Shao",
      "Zehong Lin",
      "Jun Zhang"
    ],
    "abstract": "With the rapid advancement of stereo vision technologies, stereo image\ncompression has emerged as a crucial field that continues to draw significant\nattention. Previous approaches have primarily employed a unidirectional\nparadigm, where the compression of one view is dependent on the other,\nresulting in imbalanced compression. To address this issue, we introduce a\nsymmetric bidirectional stereo image compression architecture, named BiSIC.\nSpecifically, we propose a 3D convolution based codec backbone to capture local\nfeatures and incorporate bidirectional attention blocks to exploit global\nfeatures. Moreover, we design a novel cross-dimensional entropy model that\nintegrates various conditioning factors, including the spatial context, channel\ncontext, and stereo dependency, to effectively estimate the distribution of\nlatent representations for entropy coding. Extensive experiments demonstrate\nthat our proposed BiSIC outperforms conventional image/video compression\nstandards, as well as state-of-the-art learning-based methods, in terms of both\nPSNR and MS-SSIM.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.10632v2",
    "published_date": "2024-07-15 11:36:22 UTC",
    "updated_date": "2024-10-26 06:03:35 UTC"
  },
  {
    "arxiv_id": "2407.10627v1",
    "title": "Arena Learning: Build Data Flywheel for LLMs Post-training via Simulated Chatbot Arena",
    "authors": [
      "Haipeng Luo",
      "Qingfeng Sun",
      "Can Xu",
      "Pu Zhao",
      "Qingwei Lin",
      "Jianguang Lou",
      "Shifeng Chen",
      "Yansong Tang",
      "Weizhu Chen"
    ],
    "abstract": "Assessing the effectiveness of large language models (LLMs) presents\nsubstantial challenges. The method of conducting human-annotated battles in an\nonline Chatbot Arena is a highly effective evaluative technique. However, this\napproach is limited by the costs and time required for human annotation. In\nthis paper, we introduce Arena Learning, an innovative offline strategy\ndesigned to simulate these arena battles using AI-driven annotations to\nevaluate battle outcomes, thus facilitating the continuous improvement of the\ntarget model through both supervised fine-tuning and reinforcement learning.\nArena Learning comprises two key elements. First, it ensures precise\nevaluations and maintains consistency between offline simulations and online\ncompetitions via WizardArena, a pipeline developed to accurately predict the\nElo rankings of various models using a meticulously designed offline test set.\nOur results demonstrate that WizardArena's predictions closely align with those\nfrom the online Arena. Second, it involves the continuous improvement of\ntraining data based on the battle results and the refined model. We establish a\ndata flywheel to iteratively update the training data by highlighting the\nweaknesses of the target model based on its battle results, enabling it to\nlearn from the strengths of multiple different models. We apply Arena Learning\nto train our target model, WizardLM-$\\beta$, and demonstrate significant\nperformance enhancements across various metrics. This fully automated training\nand evaluation pipeline sets the stage for continuous advancements in various\nLLMs via post-training. Notably, Arena Learning plays a pivotal role in the\nsuccess of WizardLM-2, and this paper serves both as an exploration of its\nefficacy and a foundational study for future discussions related to WizardLM-2\nand its derivatives.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10627v1",
    "published_date": "2024-07-15 11:26:07 UTC",
    "updated_date": "2024-07-15 11:26:07 UTC"
  },
  {
    "arxiv_id": "2407.10596v1",
    "title": "An evaluation of CNN models and data augmentation techniques in hierarchical localization of mobile robots",
    "authors": [
      "J. J. Cabrera",
      "O. J. Céspedes",
      "S. Cebollada",
      "O. Reinoso",
      "L. Payá"
    ],
    "abstract": "This work presents an evaluation of CNN models and data augmentation to carry\nout the hierarchical localization of a mobile robot by using omnidireccional\nimages. In this sense, an ablation study of different state-of-the-art CNN\nmodels used as backbone is presented and a variety of data augmentation visual\neffects are proposed for addressing the visual localization of the robot. The\nproposed method is based on the adaption and re-training of a CNN with a dual\npurpose: (1) to perform a rough localization step in which the model is used to\npredict the room from which an image was captured, and (2) to address the fine\nlocalization step, which consists in retrieving the most similar image of the\nvisual map among those contained in the previously predicted room by means of a\npairwise comparison between descriptors obtained from an intermediate layer of\nthe CNN. In this sense, we evaluate the impact of different state-of-the-art\nCNN models such as ConvNeXt for addressing the proposed localization. Finally,\na variety of data augmentation visual effects are separately employed for\ntraining the model and their impact is assessed. The performance of the\nresulting CNNs is evaluated under real operation conditions, including changes\nin the lighting conditions. Our code is publicly available on the project\nwebsite https://github.com/juanjo-cabrera/IndoorLocalizationSingleCNN.git",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Published Evolving Systems (2024): 08 July 2024 PDF link:\n  https://link.springer.com/content/pdf/10.1007/s12530-024-09604-6.pdf",
    "pdf_url": "http://arxiv.org/pdf/2407.10596v1",
    "published_date": "2024-07-15 10:20:00 UTC",
    "updated_date": "2024-07-15 10:20:00 UTC"
  },
  {
    "arxiv_id": "2407.12871v2",
    "title": "MetaTool: Facilitating Large Language Models to Master Tools with Meta-task Augmentation",
    "authors": [
      "Xiaohan Wang",
      "Dian Li",
      "Yilin Zhao",
      "Sinbadliu",
      "Hui Wang"
    ],
    "abstract": "Utilizing tools with Large Language Models (LLMs) is essential for grounding\nAI agents in real-world applications. The prevailing approach involves few-shot\nprompting with demonstrations or fine-tuning with expert annotations. However,\nmere in-context demonstrations may fail to cover sufficient knowledge for\ncomplex tools and tasks. Training on solution paths is also hindered by the\nhigh cost of expert annotations and generalizing to new tools. A core challenge\nof generalizable tool use lies in understanding the \"meta\", or fundamental\nnatures of tools that are transferable across tasks, such as causality and\nconstraints. In this paper, we present MetaTool, a novel tool learning\nmethodology designed to generalize across any reusable toolset. Our approach\nincorporates a self-supervised augmentation technique derived from a series of\nmeta-tasks. This involves predicting masked elements in the tool execution\nprocess. The self-supervised procedure enables scalable generation of\nhigh-quality QA data, which is handy for supervising tool understanding. By\nincorporating meta-task data into task-oriented training, our method\nsignificantly enhances the performance of open-source LLMs, achieving results\ncomparable to ChatGPT in both tool-based planning and chatting scenarios.\nThrough large-scale instruction tuning, the MetaTool model demonstrates\nimpressive zero-shot generalizability on new tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.12871v2",
    "published_date": "2024-07-15 10:15:41 UTC",
    "updated_date": "2024-10-08 11:39:07 UTC"
  },
  {
    "arxiv_id": "2407.10583v1",
    "title": "Three Dogmas of Reinforcement Learning",
    "authors": [
      "David Abel",
      "Mark K. Ho",
      "Anna Harutyunyan"
    ],
    "abstract": "Modern reinforcement learning has been conditioned by at least three dogmas.\nThe first is the environment spotlight, which refers to our tendency to focus\non modeling environments rather than agents. The second is our treatment of\nlearning as finding the solution to a task, rather than adaptation. The third\nis the reward hypothesis, which states that all goals and purposes can be well\nthought of as maximization of a reward signal. These three dogmas shape much of\nwhat we think of as the science of reinforcement learning. While each of the\ndogmas have played an important role in developing the field, it is time we\nbring them to the surface and reflect on whether they belong as basic\ningredients of our scientific paradigm. In order to realize the potential of\nreinforcement learning as a canonical frame for researching intelligent agents,\nwe suggest that it is time we shed dogmas one and two entirely, and embrace a\nnuanced approach to the third.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "RLC 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.10583v1",
    "published_date": "2024-07-15 10:03:24 UTC",
    "updated_date": "2024-07-15 10:03:24 UTC"
  },
  {
    "arxiv_id": "2407.10582v1",
    "title": "Boosting Zero-Shot Crosslingual Performance using LLM-Based Augmentations with Effective Data Selection",
    "authors": [
      "Barah Fazili",
      "Ashish Sunil Agrawal",
      "Preethi Jyothi"
    ],
    "abstract": "Large language models (LLMs) are very proficient text generators. We leverage\nthis capability of LLMs to generate task-specific data via zero-shot prompting\nand promote cross-lingual transfer for low-resource target languages. Given\ntask-specific data in a source language and a teacher model trained on this\ndata, we propose using this teacher to label LLM generations and employ a set\nof simple data selection strategies that use the teacher's label probabilities.\nOur data selection strategies help us identify a representative subset of\ndiverse generations that help boost zero-shot accuracies while being efficient,\nin comparison to using all the LLM generations (without any subset selection).\nWe also highlight other important design choices that affect cross-lingual\nperformance such as the use of translations of source data and what labels are\nbest to use for the LLM generations. We observe significant performance gains\nacross sentiment analysis and natural language inference tasks (of up to a\nmaximum of 7.13 absolute points and 1.5 absolute points on average) across a\nnumber of target languages (Hindi, Marathi, Urdu, Swahili) and domains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in Findings of ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.10582v1",
    "published_date": "2024-07-15 10:00:22 UTC",
    "updated_date": "2024-07-15 10:00:22 UTC"
  },
  {
    "arxiv_id": "2407.10580v1",
    "title": "Leveraging Hybrid Intelligence Towards Sustainable and Energy-Efficient Machine Learning",
    "authors": [
      "Daniel Geissler",
      "Paul Lukowicz"
    ],
    "abstract": "Hybrid intelligence aims to enhance decision-making, problem-solving, and\noverall system performance by combining the strengths of both, human cognitive\nabilities and artificial intelligence. With the rise of Large Language Models\n(LLM), progressively participating as smart agents to accelerate machine\nlearning development, Hybrid Intelligence is becoming an increasingly important\ntopic for effective interaction between humans and machines. This paper\npresents an approach to leverage Hybrid Intelligence towards sustainable and\nenergy-aware machine learning. When developing machine learning models, final\nmodel performance commonly rules the optimization process while the efficiency\nof the process itself is often neglected. Moreover, in recent times, energy\nefficiency has become equally crucial due to the significant environmental\nimpact of complex and large-scale computational processes. The contribution of\nthis work covers the interactive inclusion of secondary knowledge sources\nthrough Human-in-the-loop (HITL) and LLM agents to stress out and further\nresolve inefficiencies in the machine learning development process.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10580v1",
    "published_date": "2024-07-15 09:58:27 UTC",
    "updated_date": "2024-07-15 09:58:27 UTC"
  },
  {
    "arxiv_id": "2407.10547v2",
    "title": "Learning Social Cost Functions for Human-Aware Path Planning",
    "authors": [
      "Andrea Eirale",
      "Matteo Leonetti",
      "Marcello Chiaberge"
    ],
    "abstract": "Achieving social acceptance is one of the main goals of Social Robotic\nNavigation. Despite this topic has received increasing interest in recent\nyears, most of the research has focused on driving the robotic agent along\nobstacle-free trajectories, planning around estimates of future human motion to\nrespect personal distances and optimize navigation. However, social\ninteractions in everyday life are also dictated by norms that do not strictly\ndepend on movement, such as when standing at the end of a queue rather than\ncutting it. In this paper, we propose a novel method to recognize common social\nscenarios and modify a traditional planner's cost function to adapt to them.\nThis solution enables the robot to carry out different social navigation\nbehaviors that would not arise otherwise, maintaining the robustness of\ntraditional navigation. Our approach allows the robot to learn different social\nnorms with a single learned model, rather than having different modules for\neach task. As a proof of concept, we consider the tasks of queuing and respect\ninteraction spaces of groups of people talking to one another, but the method\ncan be extended to other human activities that do not involve motion.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10547v2",
    "published_date": "2024-07-15 08:57:02 UTC",
    "updated_date": "2024-10-18 12:25:46 UTC"
  },
  {
    "arxiv_id": "2407.10545v3",
    "title": "LightCL: Compact Continual Learning with Low Memory Footprint For Edge Device",
    "authors": [
      "Zeqing Wang",
      "Fei Cheng",
      "Kangye Ji",
      "Bohu Huang"
    ],
    "abstract": "Continual learning (CL) is a technique that enables neural networks to\nconstantly adapt to their dynamic surroundings. Despite being overlooked for a\nlong time, this technology can considerably address the customized needs of\nusers in edge devices. Actually, most CL methods require huge resource\nconsumption by the training behavior to acquire generalizability among all\ntasks for delaying forgetting regardless of edge scenarios. Therefore, this\npaper proposes a compact algorithm called LightCL, which evaluates and\ncompresses the redundancy of already generalized components in structures of\nthe neural network. Specifically, we consider two factors of generalizability,\nlearning plasticity and memory stability, and design metrics of both to\nquantitatively assess generalizability of neural networks during CL. This\nevaluation shows that generalizability of different layers in a neural network\nexhibits a significant variation. Thus, we $\\textit{Maintain Generalizability}$\nby freezing generalized parts without the resource-intensive training process\nand $\\textit{Memorize Feature Patterns}$ by stabilizing feature extracting of\nprevious tasks to enhance generalizability for less-generalized parts with a\nlittle extra memory, which is far less than the reduction by freezing.\nExperiments illustrate that LightCL outperforms other state-of-the-art methods\nand reduces at most $\\textbf{6.16$\\times$}$ memory footprint. We also verify\nthe effectiveness of LightCL on the edge device.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in ASPDAC'25",
    "pdf_url": "http://arxiv.org/pdf/2407.10545v3",
    "published_date": "2024-07-15 08:52:20 UTC",
    "updated_date": "2025-03-08 10:54:22 UTC"
  },
  {
    "arxiv_id": "2407.10543v1",
    "title": "Understanding the Dependence of Perception Model Competency on Regions in an Image",
    "authors": [
      "Sara Pohland",
      "Claire Tomlin"
    ],
    "abstract": "While deep neural network (DNN)-based perception models are useful for many\napplications, these models are black boxes and their outputs are not yet well\nunderstood. To confidently enable a real-world, decision-making system to\nutilize such a perception model without human intervention, we must enable the\nsystem to reason about the perception model's level of competency and respond\nappropriately when the model is incompetent. In order for the system to make an\nintelligent decision about the appropriate action when the model is\nincompetent, it would be useful for the system to understand why the model is\nincompetent. We explore five novel methods for identifying regions in the input\nimage contributing to low model competency, which we refer to as image\ncropping, segment masking, pixel perturbation, competency gradients, and\nreconstruction loss. We assess the ability of these five methods to identify\nunfamiliar objects, recognize regions associated with unseen classes, and\nidentify unexplored areas in an environment. We find that the competency\ngradients and reconstruction loss methods show great promise in identifying\nregions associated with low model competency, particularly when aspects of the\nimage that are unfamiliar to the perception model are causing this reduction in\ncompetency. Both of these methods boast low computation times and high levels\nof accuracy in detecting image regions that are unfamiliar to the model,\nallowing them to provide potential utility in decision-making pipelines. The\ncode for reproducing our methods and results is available on GitHub:\nhttps://github.com/sarapohland/explainable-competency.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10543v1",
    "published_date": "2024-07-15 08:50:13 UTC",
    "updated_date": "2024-07-15 08:50:13 UTC"
  },
  {
    "arxiv_id": "2407.10542v1",
    "title": "3D Geometric Shape Assembly via Efficient Point Cloud Matching",
    "authors": [
      "Nahyuk Lee",
      "Juhong Min",
      "Junha Lee",
      "Seungwook Kim",
      "Kanghee Lee",
      "Jaesik Park",
      "Minsu Cho"
    ],
    "abstract": "Learning to assemble geometric shapes into a larger target structure is a\npivotal task in various practical applications. In this work, we tackle this\nproblem by establishing local correspondences between point clouds of part\nshapes in both coarse- and fine-levels. To this end, we introduce Proxy Match\nTransform (PMT), an approximate high-order feature transform layer that enables\nreliable matching between mating surfaces of parts while incurring low costs in\nmemory and computation. Building upon PMT, we introduce a new framework, dubbed\nProxy Match TransformeR (PMTR), for the geometric assembly task. We evaluate\nthe proposed PMTR on the large-scale 3D geometric shape assembly benchmark\ndataset of Breaking Bad and demonstrate its superior performance and efficiency\ncompared to state-of-the-art methods. Project page:\nhttps://nahyuklee.github.io/pmtr.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.10542v1",
    "published_date": "2024-07-15 08:50:02 UTC",
    "updated_date": "2024-07-15 08:50:02 UTC"
  },
  {
    "arxiv_id": "2407.10536v1",
    "title": "An experimental evaluation of Siamese Neural Networks for robot localization using omnidirectional imaging in indoor environments",
    "authors": [
      "J. J. Cabrera",
      "V. Román",
      "A. Gil",
      "O. Reinoso",
      "L. Payá"
    ],
    "abstract": "The objective of this paper is to address the localization problem using\nomnidirectional images captured by a catadioptric vision system mounted on the\nrobot. For this purpose, we explore the potential of Siamese Neural Networks\nfor modeling indoor environments using panoramic images as the unique source of\ninformation. Siamese Neural Networks are characterized by their ability to\ngenerate a similarity function between two input data, in this case, between\ntwo panoramic images. In this study, Siamese Neural Networks composed of two\nConvolutional Neural Networks (CNNs) are used. The output of each CNN is a\ndescriptor which is used to characterize each image. The dissimilarity of the\nimages is computed by measuring the distance between these descriptors. This\nfact makes Siamese Neural Networks particularly suitable to perform image\nretrieval tasks. First, we evaluate an initial task strongly related to\nlocalization that consists in detecting whether two images have been captured\nin the same or in different rooms. Next, we assess Siamese Neural Networks in\nthe context of a global localization problem. The results outperform previous\ntechniques for solving the localization task using the COLD-Freiburg dataset,\nin a variety of lighting conditions, specially when using images captured in\ncloudy and night conditions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Published: 08 July 2024 Paper link:\n  https://link.springer.com/content/pdf/10.1007/s10462-024-10840-0.pdf",
    "pdf_url": "http://arxiv.org/pdf/2407.10536v1",
    "published_date": "2024-07-15 08:44:37 UTC",
    "updated_date": "2024-07-15 08:44:37 UTC"
  },
  {
    "arxiv_id": "2407.10510v2",
    "title": "TCM-FTP: Fine-Tuning Large Language Models for Herbal Prescription Prediction",
    "authors": [
      "Xingzhi Zhou",
      "Xin Dong",
      "Chunhao Li",
      "Yuning Bai",
      "Yulong Xu",
      "Ka Chun Cheung",
      "Simon See",
      "Xinpeng Song",
      "Runshun Zhang",
      "Xuezhong Zhou",
      "Nevin L. Zhang"
    ],
    "abstract": "Traditional Chinese medicine (TCM) has relied on specific combinations of\nherbs in prescriptions to treat various symptoms and signs for thousands of\nyears. Predicting TCM prescriptions poses a fascinating technical challenge\nwith significant practical implications. However, this task faces limitations\ndue to the scarcity of high-quality clinical datasets and the complex\nrelationship between symptoms and herbs. To address these issues, we introduce\n\\textit{DigestDS}, a novel dataset comprising practical medical records from\nexperienced experts in digestive system diseases. We also propose a method,\nTCM-FTP (TCM Fine-Tuning Pre-trained), to leverage pre-trained large language\nmodels (LLMs) via supervised fine-tuning on \\textit{DigestDS}. Additionally, we\nenhance computational efficiency using a low-rank adaptation technique.\nMoreover, TCM-FTP incorporates data augmentation by permuting herbs within\nprescriptions, exploiting their order-agnostic nature. Impressively, TCM-FTP\nachieves an F1-score of 0.8031, significantly outperforming previous methods.\nFurthermore, it demonstrates remarkable accuracy in dosage prediction,\nachieving a normalized mean square error of 0.0604. In contrast, LLMs without\nfine-tuning exhibit poor performance. Although LLMs have demonstrated\nwide-ranging capabilities, our work underscores the necessity of fine-tuning\nfor TCM prescription prediction and presents an effective way to accomplish\nthis.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.CL",
    "comment": "Camera-ready version to be published in BIBM 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.10510v2",
    "published_date": "2024-07-15 08:06:37 UTC",
    "updated_date": "2024-12-12 10:40:22 UTC"
  },
  {
    "arxiv_id": "2407.14260v1",
    "title": "Guitar Chord Diagram Suggestion for Western Popular Music",
    "authors": [
      "Alexandre d'Hooge",
      "Louis Bigo",
      "Ken Déguernel",
      "Nicolas Martin"
    ],
    "abstract": "Chord diagrams are used by guitar players to show where and how to play a\nchord on the fretboard. They are useful to beginners learning chords or for\nsharing the hand positions required to play a song.However, the diagrams\npresented on guitar learning toolsare usually selected from an existing\ndatabaseand rarely represent the actual positions used by performers.In this\npaper, we propose a tool which suggests a chord diagram for achord label,taking\ninto account the diagram of the previous chord.Based on statistical analysis of\nthe DadaGP and mySongBook datasets, we show that some chord diagrams are\nover-represented in western popular musicand that some chords can be played in\nmore than 20 different ways.We argue that taking context into account can\nimprove the variety and the quality of chord diagram suggestion, and compare\nthis approach with a model taking only the current chord label into account.We\nshow that adding previous context improves the F1-score on this task by up to\n27% and reduces the propensity of the model to suggest standard open chords.We\nalso define the notion of texture in the context of chord diagrams andshow\nthrough a variety of metrics that our model improves textureconsistencywith the\nprevious diagram.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14260v1",
    "published_date": "2024-07-15 07:44:13 UTC",
    "updated_date": "2024-07-15 07:44:13 UTC"
  },
  {
    "arxiv_id": "2407.10490v3",
    "title": "Learning Dynamics of LLM Finetuning",
    "authors": [
      "Yi Ren",
      "Danica J. Sutherland"
    ],
    "abstract": "Learning dynamics, which describes how the learning of specific training\nexamples influences the model's predictions on other examples, gives us a\npowerful tool for understanding the behavior of deep learning systems. We study\nthe learning dynamics of large language models during different types of\nfinetuning, by analyzing the step-wise decomposition of how influence\naccumulates among different potential responses. Our framework allows a uniform\ninterpretation of many interesting observations about the training of popular\nalgorithms for both instruction tuning and preference tuning. In particular, we\npropose a hypothetical explanation of why specific types of hallucination are\nstrengthened after finetuning, e.g., the model might use phrases or facts in\nthe response for question B to answer question A, or the model might keep\nrepeating similar simple phrases when generating responses. We also extend our\nframework and highlight a unique \"squeezing effect\" to explain a previously\nobserved phenomenon in off-policy direct preference optimization (DPO), where\nrunning DPO for too long makes even the desired outputs less likely. This\nframework also provides insights into where the benefits of on-policy DPO and\nother variants come from. The analysis not only provides a novel perspective of\nunderstanding LLM's finetuning but also inspires a simple, effective method to\nimprove alignment performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10490v3",
    "published_date": "2024-07-15 07:30:28 UTC",
    "updated_date": "2025-02-20 01:09:57 UTC"
  },
  {
    "arxiv_id": "2407.10488v1",
    "title": "How and where does CLIP process negation?",
    "authors": [
      "Vincent Quantmeyer",
      "Pablo Mosteiro",
      "Albert Gatt"
    ],
    "abstract": "Various benchmarks have been proposed to test linguistic understanding in\npre-trained vision \\& language (VL) models. Here we build on the existence task\nfrom the VALSE benchmark (Parcalabescu et al, 2022) which we use to test\nmodels' understanding of negation, a particularly interesting issue for\nmultimodal models. However, while such VL benchmarks are useful for measuring\nmodel performance, they do not reveal anything about the internal processes\nthrough which these models arrive at their outputs in such visio-linguistic\ntasks. We take inspiration from the growing literature on model\ninterpretability to explain the behaviour of VL models on the understanding of\nnegation. Specifically, we approach these questions through an in-depth\nanalysis of the text encoder in CLIP (Radford et al, 2021), a highly\ninfluential VL model. We localise parts of the encoder that process negation\nand analyse the role of attention heads in this task. Our contributions are\nthreefold. We demonstrate how methods from the language model interpretability\nliterature (such as causal tracing) can be translated to multimodal models and\ntasks; we provide concrete insights into how CLIP processes negation on the\nVALSE existence task; and we highlight inherent limitations in the VALSE\ndataset as a benchmark for linguistic understanding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at the 3rd Workshop on Advances in Language and Vision\n  Research (ALVR 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.10488v1",
    "published_date": "2024-07-15 07:20:06 UTC",
    "updated_date": "2024-07-15 07:20:06 UTC"
  },
  {
    "arxiv_id": "2407.11100v3",
    "title": "Building Intelligence Identification System via Large Language Model Watermarking: A Survey and Beyond",
    "authors": [
      "Xuhong Wang",
      "Haoyu Jiang",
      "Yi Yu",
      "Jingru Yu",
      "Yilun Lin",
      "Ping Yi",
      "Yingchun Wang",
      "Yu Qiao",
      "Li Li",
      "Fei-Yue Wang"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly integrated into diverse\nindustries, posing substantial security risks due to unauthorized replication\nand misuse. To mitigate these concerns, robust identification mechanisms are\nwidely acknowledged as an effective strategy. Identification systems for LLMs\nnow rely heavily on watermarking technology to manage and protect intellectual\nproperty and ensure data security. However, previous studies have primarily\nconcentrated on the basic principles of algorithms and lacked a comprehensive\nanalysis of watermarking theory and practice from the perspective of\nintelligent identification. To bridge this gap, firstly, we explore how a\nrobust identity recognition system can be effectively implemented and managed\nwithin LLMs by various participants using watermarking technology. Secondly, we\npropose a mathematical framework based on mutual information theory, which\nsystematizes the identification process to achieve more precise and customized\nwatermarking. Additionally, we present a comprehensive evaluation of\nperformance metrics for LLM watermarking, reflecting participant preferences\nand advancing discussions on its identification applications. Lastly, we\noutline the existing challenges in current watermarking technologies and\ntheoretical frameworks, and provide directional guidance to address these\nchallenges. Our systematic classification and detailed exposition aim to\nenhance the comparison and evaluation of various methods, fostering further\nresearch and development toward a transparent, secure, and equitable LLM\necosystem.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "59 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.11100v3",
    "published_date": "2024-07-15 07:20:02 UTC",
    "updated_date": "2024-07-24 08:10:29 UTC"
  },
  {
    "arxiv_id": "2407.10486v2",
    "title": "IDEAL: Leveraging Infinite and Dynamic Characterizations of Large Language Models for Query-focused Summarization",
    "authors": [
      "Jie Cao",
      "Dian Jiao",
      "Qiang Yan",
      "Wenqiao Zhang",
      "Siliang Tang",
      "Yueting Zhuang"
    ],
    "abstract": "Query-focused summarization (QFS) aims to produce summaries that answer\nparticular questions of interest, enabling greater user control and\npersonalization. With the advent of large language models (LLMs), shows their\nimpressive capability of textual understanding through large-scale pretraining,\nwhich implies the great potential of extractive snippet generation. In this\npaper, we systematically investigated two indispensable characteristics that\nthe LLMs-based QFS models should be harnessed, Lengthy Document Summarization\nand Efficiently Fine-grained Query-LLM Alignment, respectively.\nCorrespondingly, we propose two modules called Query-aware HyperExpert and\nQuery-focused Infini-attention to access the aforementioned characteristics.\nThese innovations pave the way for broader application and accessibility in the\nfield of QFS technology. Extensive experiments conducted on existing QFS\nbenchmarks indicate the effectiveness and generalizability of the proposed\napproach. Our code is publicly available at\nhttps://github.com/DCDmllm/IDEAL_Summary.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10486v2",
    "published_date": "2024-07-15 07:14:56 UTC",
    "updated_date": "2025-01-07 14:09:22 UTC"
  },
  {
    "arxiv_id": "2407.10481v1",
    "title": "SuperPADL: Scaling Language-Directed Physics-Based Control with Progressive Supervised Distillation",
    "authors": [
      "Jordan Juravsky",
      "Yunrong Guo",
      "Sanja Fidler",
      "Xue Bin Peng"
    ],
    "abstract": "Physically-simulated models for human motion can generate high-quality\nresponsive character animations, often in real-time. Natural language serves as\na flexible interface for controlling these models, allowing expert and\nnon-expert users to quickly create and edit their animations. Many recent\nphysics-based animation methods, including those that use text interfaces,\ntrain control policies using reinforcement learning (RL). However, scaling\nthese methods beyond several hundred motions has remained challenging.\nMeanwhile, kinematic animation models are able to successfully learn from\nthousands of diverse motions by leveraging supervised learning methods.\nInspired by these successes, in this work we introduce SuperPADL, a scalable\nframework for physics-based text-to-motion that leverages both RL and\nsupervised learning to train controllers on thousands of diverse motion clips.\nSuperPADL is trained in stages using progressive distillation, starting with a\nlarge number of specialized experts using RL. These experts are then\niteratively distilled into larger, more robust policies using a combination of\nreinforcement learning and supervised learning. Our final SuperPADL controller\nis trained on a dataset containing over 5000 skills and runs in real time on a\nconsumer GPU. Moreover, our policy can naturally transition between skills,\nallowing for users to interactively craft multi-stage animations. We\nexperimentally demonstrate that SuperPADL significantly outperforms RL-based\nbaselines at this large data scale.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.GR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10481v1",
    "published_date": "2024-07-15 07:07:11 UTC",
    "updated_date": "2024-07-15 07:07:11 UTC"
  },
  {
    "arxiv_id": "2407.10476v1",
    "title": "Kinetic Typography Diffusion Model",
    "authors": [
      "Seonmi Park",
      "Inhwan Bae",
      "Seunghyun Shin",
      "Hae-Gon Jeon"
    ],
    "abstract": "This paper introduces a method for realistic kinetic typography that\ngenerates user-preferred animatable 'text content'. We draw on recent advances\nin guided video diffusion models to achieve visually-pleasing text appearances.\nTo do this, we first construct a kinetic typography dataset, comprising about\n600K videos. Our dataset is made from a variety of combinations in 584\ntemplates designed by professional motion graphics designers and involves\nchanging each letter's position, glyph, and size (i.e., flying, glitches,\nchromatic aberration, reflecting effects, etc.). Next, we propose a video\ndiffusion model for kinetic typography. For this, there are three requirements:\naesthetic appearances, motion effects, and readable letters. This paper\nidentifies the requirements. For this, we present static and dynamic captions\nused as spatial and temporal guidance of a video diffusion model, respectively.\nThe static caption describes the overall appearance of the video, such as\ncolors, texture and glyph which represent a shape of each letter. The dynamic\ncaption accounts for the movements of letters and backgrounds. We add one more\nguidance with zero convolution to determine which text content should be\nvisible in the video. We apply the zero convolution to the text content, and\nimpose it on the diffusion model. Lastly, our glyph loss, only minimizing a\ndifference between the predicted word and its ground-truth, is proposed to make\nthe prediction letters readable. Experiments show that our model generates\nkinetic typography videos with legible and artistic letter motions based on\ntext prompts.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ECCV 2024, Project page: https://seonmip.github.io/kinety",
    "pdf_url": "http://arxiv.org/pdf/2407.10476v1",
    "published_date": "2024-07-15 07:04:45 UTC",
    "updated_date": "2024-07-15 07:04:45 UTC"
  },
  {
    "arxiv_id": "2407.10471v2",
    "title": "GROOT: Generating Robust Watermark for Diffusion-Model-Based Audio Synthesis",
    "authors": [
      "Weizhi Liu",
      "Yue Li",
      "Dongdong Lin",
      "Hui Tian",
      "Haizhou Li"
    ],
    "abstract": "Amid the burgeoning development of generative models like diffusion models,\nthe task of differentiating synthesized audio from its natural counterpart\ngrows more daunting. Deepfake detection offers a viable solution to combat this\nchallenge. Yet, this defensive measure unintentionally fuels the continued\nrefinement of generative models. Watermarking emerges as a proactive and\nsustainable tactic, preemptively regulating the creation and dissemination of\nsynthesized content. Thus, this paper, as a pioneer, proposes the generative\nrobust audio watermarking method (Groot), presenting a paradigm for proactively\nsupervising the synthesized audio and its source diffusion models. In this\nparadigm, the processes of watermark generation and audio synthesis occur\nsimultaneously, facilitated by parameter-fixed diffusion models equipped with a\ndedicated encoder. The watermark embedded within the audio can subsequently be\nretrieved by a lightweight decoder. The experimental results highlight Groot's\noutstanding performance, particularly in terms of robustness, surpassing that\nof the leading state-of-the-art methods. Beyond its impressive resilience\nagainst individual post-processing attacks, Groot exhibits exceptional\nrobustness when facing compound attacks, maintaining an average watermark\nextraction accuracy of around 95%.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by ACM MM 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.10471v2",
    "published_date": "2024-07-15 06:57:19 UTC",
    "updated_date": "2024-07-17 05:43:36 UTC"
  },
  {
    "arxiv_id": "2407.18957v4",
    "title": "When AI Meets Finance (StockAgent): Large Language Model-based Stock Trading in Simulated Real-world Environments",
    "authors": [
      "Chong Zhang",
      "Xinyi Liu",
      "Zhongmou Zhang",
      "Mingyu Jin",
      "Lingyao Li",
      "Zhenting Wang",
      "Wenyue Hua",
      "Dong Shu",
      "Suiyuan Zhu",
      "Xiaobo Jin",
      "Sujian Li",
      "Mengnan Du",
      "Yongfeng Zhang"
    ],
    "abstract": "Can AI Agents simulate real-world trading environments to investigate the\nimpact of external factors on stock trading activities (e.g., macroeconomics,\npolicy changes, company fundamentals, and global events)? These factors, which\nfrequently influence trading behaviors, are critical elements in the quest for\nmaximizing investors' profits. Our work attempts to solve this problem through\nlarge language model based agents. We have developed a multi-agent AI system\ncalled StockAgent, driven by LLMs, designed to simulate investors' trading\nbehaviors in response to the real stock market. The StockAgent allows users to\nevaluate the impact of different external factors on investor trading and to\nanalyze trading behavior and profitability effects. Additionally, StockAgent\navoids the test set leakage issue present in existing trading simulation\nsystems based on AI Agents. Specifically, it prevents the model from leveraging\nprior knowledge it may have acquired related to the test data. We evaluate\ndifferent LLMs under the framework of StockAgent in a stock trading environment\nthat closely resembles real-world conditions. The experimental results\ndemonstrate the impact of key external factors on stock market trading,\nincluding trading behavior and stock price fluctuation rules. This research\nexplores the study of agents' free trading gaps in the context of no prior\nknowledge related to market data. The patterns identified through StockAgent\nsimulations provide valuable insights for LLM-based investment advice and stock\nrecommendation. The code is available at\nhttps://github.com/MingyuJ666/Stockagent.",
    "categories": [
      "q-fin.TR",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "q-fin.TR",
    "comment": "33 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.18957v4",
    "published_date": "2024-07-15 06:49:30 UTC",
    "updated_date": "2024-09-21 03:09:08 UTC"
  },
  {
    "arxiv_id": "2407.10468v1",
    "title": "LiteFocus: Accelerated Diffusion Inference for Long Audio Synthesis",
    "authors": [
      "Zhenxiong Tan",
      "Xinyin Ma",
      "Gongfan Fang",
      "Xinchao Wang"
    ],
    "abstract": "Latent diffusion models have shown promising results in audio generation,\nmaking notable advancements over traditional methods. However, their\nperformance, while impressive with short audio clips, faces challenges when\nextended to longer audio sequences. These challenges are due to model's\nself-attention mechanism and training predominantly on 10-second clips, which\ncomplicates the extension to longer audio without adaptation. In response to\nthese issues, we introduce a novel approach, LiteFocus that enhances the\ninference of existing audio latent diffusion models in long audio synthesis.\nObserved the attention pattern in self-attention, we employ a dual sparse form\nfor attention calculation, designated as same-frequency focus and\ncross-frequency compensation, which curtails the attention computation under\nsame-frequency constraints, while enhancing audio quality through\ncross-frequency refillment. LiteFocus demonstrates substantial reduction on\ninference time with diffusion-based TTA model by 1.99x in synthesizing\n80-second audio clips while also obtaining improved audio quality.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Interspeech 2024; Code: https://github.com/Yuanshi9815/LiteFocus",
    "pdf_url": "http://arxiv.org/pdf/2407.10468v1",
    "published_date": "2024-07-15 06:49:05 UTC",
    "updated_date": "2024-07-15 06:49:05 UTC"
  },
  {
    "arxiv_id": "2407.10462v1",
    "title": "BandControlNet: Parallel Transformers-based Steerable Popular Music Generation with Fine-Grained Spatiotemporal Features",
    "authors": [
      "Jing Luo",
      "Xinyu Yang",
      "Dorien Herremans"
    ],
    "abstract": "Controllable music generation promotes the interaction between humans and\ncomposition systems by projecting the users' intent on their desired music. The\nchallenge of introducing controllability is an increasingly important issue in\nthe symbolic music generation field. When building controllable generative\npopular multi-instrument music systems, two main challenges typically present\nthemselves, namely weak controllability and poor music quality. To address\nthese issues, we first propose spatiotemporal features as powerful and\nfine-grained controls to enhance the controllability of the generative model.\nIn addition, an efficient music representation called REMI_Track is designed to\nconvert multitrack music into multiple parallel music sequences and shorten the\nsequence length of each track with Byte Pair Encoding (BPE) techniques.\nSubsequently, we release BandControlNet, a conditional model based on parallel\nTransformers, to tackle the multiple music sequences and generate high-quality\nmusic samples that are conditioned to the given spatiotemporal control\nfeatures. More concretely, the two specially designed modules of\nBandControlNet, namely structure-enhanced self-attention (SE-SA) and\nCross-Track Transformer (CTT), are utilized to strengthen the resulting musical\nstructure and inter-track harmony modeling respectively. Experimental results\ntested on two popular music datasets of different lengths demonstrate that the\nproposed BandControlNet outperforms other conditional music generation models\non most objective metrics in terms of fidelity and inference speed and shows\ngreat robustness in generating long music samples. The subjective evaluations\nshow BandControlNet trained on short datasets can generate music with\ncomparable quality to state-of-the-art models, while outperforming them\nsignificantly using longer datasets.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Demo page: https://chinglohsiu.github.io/files/bandcontrolnet.html",
    "pdf_url": "http://arxiv.org/pdf/2407.10462v1",
    "published_date": "2024-07-15 06:33:25 UTC",
    "updated_date": "2024-07-15 06:33:25 UTC"
  },
  {
    "arxiv_id": "2407.10457v1",
    "title": "The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism",
    "authors": [
      "Yifan Song",
      "Guoyin Wang",
      "Sujian Li",
      "Bill Yuchen Lin"
    ],
    "abstract": "Current evaluations of large language models (LLMs) often overlook\nnon-determinism, typically focusing on a single output per example. This limits\nour understanding of LLM performance variability in real-world applications.\nOur study addresses this issue by exploring key questions about the performance\ndifferences between greedy decoding and sampling, identifying benchmarks'\nconsistency regarding non-determinism, and examining unique model behaviors.\nThrough extensive experiments, we observe that greedy decoding generally\noutperforms sampling methods for most evaluated tasks. We also observe\nconsistent performance across different LLM sizes and alignment methods, noting\nthat alignment can reduce sampling variance. Moreover, our best-of-N sampling\napproach demonstrates that smaller LLMs can match or surpass larger models such\nas GPT-4-Turbo, highlighting the untapped potential of smaller LLMs. This\nresearch shows the importance of considering non-determinism in LLM evaluations\nand provides insights for future LLM development and evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10457v1",
    "published_date": "2024-07-15 06:12:17 UTC",
    "updated_date": "2024-07-15 06:12:17 UTC"
  },
  {
    "arxiv_id": "2407.11098v3",
    "title": "Inertial Confinement Fusion Forecasting via Large Language Models",
    "authors": [
      "Mingkai Chen",
      "Taowen Wang",
      "Shihui Cao",
      "James Chenhao Liang",
      "Chuan Liu",
      "Chunshu Wu",
      "Qifan Wang",
      "Ying Nian Wu",
      "Michael Huang",
      "Chuang Ren",
      "Ang Li",
      "Tong Geng",
      "Dongfang Liu"
    ],
    "abstract": "Controlled fusion energy is deemed pivotal for the advancement of human\ncivilization. In this study, we introduce $\\textbf{LPI-LLM}$, a novel\nintegration of Large Language Models (LLMs) with classical reservoir computing\nparadigms tailored to address a critical challenge, Laser-Plasma Instabilities\n($\\texttt{LPI}$), in Inertial Confinement Fusion ($\\texttt{ICF}$). Our approach\noffers several key contributions: Firstly, we propose the $\\textit{LLM-anchored\nReservoir}$, augmented with a $\\textit{Fusion-specific Prompt}$, enabling\naccurate forecasting of $\\texttt{LPI}$-generated-hot electron dynamics during\nimplosion. Secondly, we develop $\\textit{Signal-Digesting Channels}$ to\ntemporally and spatially describe the driver laser intensity across time,\ncapturing the unique characteristics of $\\texttt{ICF}$ inputs. Lastly, we\ndesign the $\\textit{Confidence Scanner}$ to quantify the confidence level in\nforecasting, providing valuable insights for domain experts to design the\n$\\texttt{ICF}$ process. Extensive experiments demonstrate the superior\nperformance of our method, achieving 1.90 CAE, 0.14 $\\texttt{top-1}$ MAE, and\n0.11 $\\texttt{top-5}$ MAE in predicting Hard X-ray ($\\texttt{HXR}$) energies\nemitted by the hot electrons in $\\texttt{ICF}$ implosions, which presents\nstate-of-the-art comparisons against concurrent best systems. Additionally, we\npresent $\\textbf{LPI4AI}$, the first $\\texttt{LPI}$ benchmark based on physical\nexperiments, aimed at fostering novel ideas in $\\texttt{LPI}$ research and\nenhancing the utility of LLMs in scientific exploration. Overall, our work\nstrives to forge an innovative synergy between AI and $\\texttt{ICF}$ for\nadvancing fusion energy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11098v3",
    "published_date": "2024-07-15 05:46:44 UTC",
    "updated_date": "2024-10-14 22:47:47 UTC"
  },
  {
    "arxiv_id": "2407.10452v1",
    "title": "GraphPrint: Extracting Features from 3D Protein Structure for Drug Target Affinity Prediction",
    "authors": [
      "Amritpal Singh"
    ],
    "abstract": "Accurate drug target affinity prediction can improve drug candidate\nselection, accelerate the drug discovery process, and reduce drug production\ncosts. Previous work focused on traditional fingerprints or used features\nextracted based on the amino acid sequence in the protein, ignoring its 3D\nstructure which affects its binding affinity. In this work, we propose\nGraphPrint: a framework for incorporating 3D protein structure features for\ndrug target affinity prediction. We generate graph representations for protein\n3D structures using amino acid residue location coordinates and combine them\nwith drug graph representation and traditional features to jointly learn drug\ntarget affinity. Our model achieves a mean square error of 0.1378 and a\nconcordance index of 0.8929 on the KIBA dataset and improves over using\ntraditional protein features alone. Our ablation study shows that the 3D\nprotein structure-based features provide information complementary to\ntraditional features.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted: The NeurIPS 2023 Workshop on New Frontiers of AI for Drug\n  Discovery and Development (AI4D3 2023), New Orleans, LA, USA, 2023",
    "pdf_url": "http://arxiv.org/pdf/2407.10452v1",
    "published_date": "2024-07-15 05:45:09 UTC",
    "updated_date": "2024-07-15 05:45:09 UTC"
  },
  {
    "arxiv_id": "2407.10446v1",
    "title": "DDFAD: Dataset Distillation Framework for Audio Data",
    "authors": [
      "Wenbo Jiang",
      "Rui Zhang",
      "Hongwei Li",
      "Xiaoyuan Liu",
      "Haomiao Yang",
      "Shui Yu"
    ],
    "abstract": "Deep neural networks (DNNs) have achieved significant success in numerous\napplications. The remarkable performance of DNNs is largely attributed to the\navailability of massive, high-quality training datasets. However, processing\nsuch massive training data requires huge computational and storage resources.\nDataset distillation is a promising solution to this problem, offering the\ncapability to compress a large dataset into a smaller distilled dataset. The\nmodel trained on the distilled dataset can achieve comparable performance to\nthe model trained on the whole dataset.\n  While dataset distillation has been demonstrated in image data, none have\nexplored dataset distillation for audio data. In this work, for the first time,\nwe propose a Dataset Distillation Framework for Audio Data (DDFAD).\nSpecifically, we first propose the Fused Differential MFCC (FD-MFCC) as\nextracted features for audio data. After that, the FD-MFCC is distilled through\nthe matching training trajectory distillation method. Finally, we propose an\naudio signal reconstruction algorithm based on the Griffin-Lim Algorithm to\nreconstruct the audio signal from the distilled FD-MFCC. Extensive experiments\ndemonstrate the effectiveness of DDFAD on various audio datasets. In addition,\nwe show that DDFAD has promising application prospects in many applications,\nsuch as continual learning and neural architecture search.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.DB",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10446v1",
    "published_date": "2024-07-15 05:23:35 UTC",
    "updated_date": "2024-07-15 05:23:35 UTC"
  },
  {
    "arxiv_id": "2407.10445v1",
    "title": "Backdoor Attacks against Image-to-Image Networks",
    "authors": [
      "Wenbo Jiang",
      "Hongwei Li",
      "Jiaming He",
      "Rui Zhang",
      "Guowen Xu",
      "Tianwei Zhang",
      "Rongxing Lu"
    ],
    "abstract": "Recently, deep learning-based Image-to-Image (I2I) networks have become the\npredominant choice for I2I tasks such as image super-resolution and denoising.\nDespite their remarkable performance, the backdoor vulnerability of I2I\nnetworks has not been explored. To fill this research gap, we conduct a\ncomprehensive investigation on the susceptibility of I2I networks to backdoor\nattacks. Specifically, we propose a novel backdoor attack technique, where the\ncompromised I2I network behaves normally on clean input images, yet outputs a\npredefined image of the adversary for malicious input images containing the\ntrigger. To achieve this I2I backdoor attack, we propose a targeted universal\nadversarial perturbation (UAP) generation algorithm for I2I networks, where the\ngenerated UAP is used as the backdoor trigger. Additionally, in the backdoor\ntraining process that contains the main task and the backdoor task, multi-task\nlearning (MTL) with dynamic weighting methods is employed to accelerate\nconvergence rates. In addition to attacking I2I tasks, we extend our I2I\nbackdoor to attack downstream tasks, including image classification and object\ndetection. Extensive experiments demonstrate the effectiveness of the I2I\nbackdoor on state-of-the-art I2I network architectures, as well as the\nrobustness against different mainstream backdoor defenses.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10445v1",
    "published_date": "2024-07-15 05:14:17 UTC",
    "updated_date": "2024-07-15 05:14:17 UTC"
  },
  {
    "arxiv_id": "2407.10441v1",
    "title": "Enhancing Building Safety Design for Active Shooter Incidents: Exploration of Building Exit Parameters using Reinforcement Learning-Based Simulations",
    "authors": [
      "Ruying Liu",
      "Wanjing Wu",
      "Burcin Becerik-Gerber",
      "Gale M. Lucas"
    ],
    "abstract": "With the alarming rise in active shooter incidents (ASIs) in the United\nStates, enhancing public safety through building design has become a pressing\nneed. This study proposes a reinforcement learning-based simulation approach\naddressing gaps in existing research that has neglected the dynamic behaviours\nof shooters. We developed an autonomous agent to simulate an active shooter\nwithin a realistic office environment, aiming to offer insights into the\ninteractions between building design parameters and ASI outcomes. A case study\nis conducted to quantitatively investigate the impact of building exit numbers\n(total count of accessible exits) and configuration (arrangement of which exits\nare available or not) on evacuation and harm rates. Findings demonstrate that\ngreater exit availability significantly improves evacuation outcomes and\nreduces harm. Exits nearer to the shooter's initial position hold greater\nimportance for accessibility than those farther away. By encompassing dynamic\nshooter behaviours, this study offers preliminary insights into effective\nbuilding safety design against evolving threats.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10441v1",
    "published_date": "2024-07-15 05:08:38 UTC",
    "updated_date": "2024-07-15 05:08:38 UTC"
  },
  {
    "arxiv_id": "2407.10433v1",
    "title": "A Multi-Stage Framework for 3D Individual Tooth Segmentation in Dental CBCT",
    "authors": [
      "Chunshi Wang",
      "Bin Zhao",
      "Shuxue Ding"
    ],
    "abstract": "Cone beam computed tomography (CBCT) is a common way of diagnosing dental\nrelated diseases. Accurate segmentation of 3D tooth is of importance for the\ntreatment. Although deep learning based methods have achieved convincing\nresults in medical image processing, they need a large of annotated data for\nnetwork training, making it very time-consuming in data collection and\nannotation. Besides, domain shift widely existing in the distribution of data\nacquired by different devices impacts severely the model generalization. To\nresolve the problem, we propose a multi-stage framework for 3D tooth\nsegmentation in dental CBCT, which achieves the third place in the\n\"Semi-supervised Teeth Segmentation\" 3D (STS-3D) challenge. The experiments on\nvalidation set compared with other semi-supervised segmentation methods further\nindicate the validity of our approach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Semi-supervised Tooth Segmentation MICCAI 2023 Challenge",
    "pdf_url": "http://arxiv.org/pdf/2407.10433v1",
    "published_date": "2024-07-15 04:23:28 UTC",
    "updated_date": "2024-07-15 04:23:28 UTC"
  },
  {
    "arxiv_id": "2407.10430v1",
    "title": "Expanding the Scope: Inductive Knowledge Graph Reasoning with Multi-Starting Progressive Propagation",
    "authors": [
      "Zhoutian Shao",
      "Yuanning Cui",
      "Wei Hu"
    ],
    "abstract": "Knowledge graphs (KGs) are widely acknowledged as incomplete, and new\nentities are constantly emerging in the real world. Inductive KG reasoning aims\nto predict missing facts for these new entities. Among existing models, graph\nneural networks (GNNs) based ones have shown promising performance for this\ntask. However, they are still challenged by inefficient message propagation due\nto the distance and scalability issues. In this paper, we propose a new\ninductive KG reasoning model, MStar, by leveraging conditional message passing\nneural networks (C-MPNNs). Our key insight is to select multiple query-specific\nstarting entities to expand the scope of progressive propagation. To propagate\nquery-related messages to a farther area within limited steps, we subsequently\ndesign a highway layer to propagate information toward these selected starting\nentities. Moreover, we introduce a training strategy called LinkVerify to\nmitigate the impact of noisy training samples. Experimental results validate\nthat MStar achieves superior performance compared with state-of-the-art models,\nespecially for distant entities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in the 23rd International Semantic Web Conference (ISWC\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.10430v1",
    "published_date": "2024-07-15 04:16:20 UTC",
    "updated_date": "2024-07-15 04:16:20 UTC"
  },
  {
    "arxiv_id": "2407.10424v5",
    "title": "CodeV: Empowering LLMs with HDL Generation through Multi-Level Summarization",
    "authors": [
      "Yang Zhao",
      "Di Huang",
      "Chongxiao Li",
      "Pengwei Jin",
      "Muxin Song",
      "Yinan Xu",
      "Ziyuan Nan",
      "Mingju Gao",
      "Tianyun Ma",
      "Lei Qi",
      "Yansong Pan",
      "Zhenxing Zhang",
      "Rui Zhang",
      "Xishan Zhang",
      "Zidong Du",
      "Qi Guo",
      "Xing Hu"
    ],
    "abstract": "The design flow of processors, particularly in hardware description languages\n(HDL) like Verilog and Chisel, is complex and costly. While recent advances in\nlarge language models (LLMs) have significantly improved coding tasks in\nsoftware languages such as Python, their application in HDL generation remains\nlimited due to the scarcity of high-quality HDL data. Traditional methods of\nadapting LLMs for hardware design rely on synthetic HDL datasets, which often\nsuffer from low quality because even advanced LLMs like GPT perform poorly in\nthe HDL domain. Moreover, these methods focus solely on chat tasks and the\nVerilog language, limiting their application scenarios.\n  In this paper, we observe that: (1) HDL code collected from the real world is\nof higher quality than code generated by LLMs. (2) LLMs like GPT-3.5 excel in\nsummarizing HDL code rather than generating it. (3) An explicit language tag\ncan help LLMs better adapt to the target language when there is insufficient\ndata. Based on these observations, we propose an efficient LLM fine-tuning\npipeline for HDL generation that integrates a multi-level summarization data\nsynthesis process with a novel Chat-FIM-Tag supervised fine-tuning method. The\npipeline enhances the generation of HDL code from natural language descriptions\nand enables the handling of various tasks such as chat and infilling incomplete\ncode. Utilizing this pipeline, we introduce CodeV, a series of HDL generation\nLLMs. Among them, CodeV-All not only possesses a more diverse range of language\nabilities, i.e. Verilog and Chisel, and a broader scope of tasks, i.e. Chat and\nfill-in-middle (FIM), but it also achieves performance on VerilogEval that is\ncomparable to or even surpasses that of CodeV-Verilog fine-tuned on Verilog\nonly, making them the first series of open-source LLMs designed for\nmulti-scenario HDL generation.",
    "categories": [
      "cs.PL",
      "cs.AI"
    ],
    "primary_category": "cs.PL",
    "comment": "13 pages, 10 figures, journal",
    "pdf_url": "http://arxiv.org/pdf/2407.10424v5",
    "published_date": "2024-07-15 03:57:20 UTC",
    "updated_date": "2025-05-11 07:35:24 UTC"
  },
  {
    "arxiv_id": "2407.10420v1",
    "title": "Learning Rapid Turning, Aerial Reorientation, and Balancing using Manipulator as a Tail",
    "authors": [
      "Insung Yang",
      "Jemin Hwangbo"
    ],
    "abstract": "In this research, we investigated the innovative use of a manipulator as a\ntail in quadruped robots to augment their physical capabilities. Previous\nstudies have primarily focused on enhancing various abilities by attaching\nrobotic tails that function solely as tails on quadruped robots. While these\ntails improve the performance of the robots, they come with several\ndisadvantages, such as increased overall weight and higher costs. To mitigate\nthese limitations, we propose the use of a 6-DoF manipulator as a tail,\nallowing it to serve both as a tail and as a manipulator. To control this\nhighly complex robot, we developed a controller based on reinforcement learning\nfor the robot equipped with the manipulator. Our experimental results\ndemonstrate that robots equipped with a manipulator outperform those without a\nmanipulator in tasks such as rapid turning, aerial reorientation, and\nbalancing. These results indicate that the manipulator can improve the agility\nand stability of quadruped robots, similar to a tail, in addition to its\nmanipulation capabilities.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10420v1",
    "published_date": "2024-07-15 03:51:19 UTC",
    "updated_date": "2024-07-15 03:51:19 UTC"
  },
  {
    "arxiv_id": "2407.11096v1",
    "title": "Static and multivariate-temporal attentive fusion transformer for readmission risk prediction",
    "authors": [
      "Zhe Sun",
      "Runzhi Li",
      "Jing Wang",
      "Gang Chen",
      "Siyu Yan",
      "Lihong Ma"
    ],
    "abstract": "Background: Accurate short-term readmission prediction of ICU patients is\nsignificant in improving the efficiency of resource assignment by assisting\nphysicians in making discharge decisions. Clinically, both individual static\nstatic and multivariate temporal data collected from ICU monitors play critical\nroles in short-term readmission prediction. Informative static and multivariate\ntemporal feature representation capturing and fusion present challenges for\naccurate readmission prediction. Methods:We propose a novel static and\nmultivariate-temporal attentive fusion transformer (SMTAFormer) to predict\nshort-term readmission of ICU patients by fully leveraging the potential of\ndemographic and dynamic temporal data. In SMTAFormer, we first apply an MLP\nnetwork and a temporal transformer network to learn useful static and temporal\nfeature representations, respectively. Then, the well-designed static and\nmultivariate temporal feature fusion module is applied to fuse static and\ntemporal feature representations by modeling intra-correlation among\nmultivariate temporal features and constructing inter-correlation between\nstatic and multivariate temporal features. Results: We construct a readmission\nrisk assessment (RRA) dataset based on the MIMIC-III dataset. The extensive\nexperiments show that SMTAFormer outperforms advanced methods, in which the\naccuracy of our proposed method is up to 86.6%, and the area under the receiver\noperating characteristic curve (AUC) is up to 0.717. Conclusion: Our proposed\nSMTAFormer can efficiently capture and fuse static and multivariate temporal\nfeature representations. The results show that SMTAFormer significantly\nimproves the short-term readmission prediction performance of ICU patients\nthrough comparisons to strong baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11096v1",
    "published_date": "2024-07-15 03:42:44 UTC",
    "updated_date": "2024-07-15 03:42:44 UTC"
  },
  {
    "arxiv_id": "2407.10413v1",
    "title": "Melon Fruit Detection and Quality Assessment Using Generative AI-Based Image Data Augmentation",
    "authors": [
      "Seungri Yoon",
      "Yunseong Cho",
      "Tae In Ahn"
    ],
    "abstract": "Monitoring and managing the growth and quality of fruits are very important\ntasks. To effectively train deep learning models like YOLO for real-time fruit\ndetection, high-quality image datasets are essential. However, such datasets\nare often lacking in agriculture. Generative AI models can help create\nhigh-quality images. In this study, we used MidJourney and Firefly tools to\ngenerate images of melon greenhouses and post-harvest fruits through\ntext-to-image, pre-harvest image-to-image, and post-harvest image-to-image\nmethods. We evaluated these AIgenerated images using PSNR and SSIM metrics and\ntested the detection performance of the YOLOv9 model. We also assessed the net\nquality of real and generated fruits. Our results showed that generative AI\ncould produce images very similar to real ones, especially for post-harvest\nfruits. The YOLOv9 model detected the generated images well, and the net\nquality was also measurable. This shows that generative AI can create realistic\nimages useful for fruit detection and quality assessment, indicating its great\npotential in agriculture. This study highlights the potential of AI-generated\nimages for data augmentation in melon fruit detection and quality assessment\nand envisions a positive future for generative AI applications in agriculture.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.10413v1",
    "published_date": "2024-07-15 03:26:13 UTC",
    "updated_date": "2024-07-15 03:26:13 UTC"
  },
  {
    "arxiv_id": "2407.10403v1",
    "title": "Cooperative Reward Shaping for Multi-Agent Pathfinding",
    "authors": [
      "Zhenyu Song",
      "Ronghao Zheng",
      "Senlin Zhang",
      "Meiqin Liu"
    ],
    "abstract": "The primary objective of Multi-Agent Pathfinding (MAPF) is to plan efficient\nand conflict-free paths for all agents. Traditional multi-agent path planning\nalgorithms struggle to achieve efficient distributed path planning for multiple\nagents. In contrast, Multi-Agent Reinforcement Learning (MARL) has been\ndemonstrated as an effective approach to achieve this objective. By modeling\nthe MAPF problem as a MARL problem, agents can achieve efficient path planning\nand collision avoidance through distributed strategies under partial\nobservation. However, MARL strategies often lack cooperation among agents due\nto the absence of global information, which subsequently leads to reduced MAPF\nefficiency. To address this challenge, this letter introduces a unique reward\nshaping technique based on Independent Q-Learning (IQL). The aim of this method\nis to evaluate the influence of one agent on its neighbors and integrate such\nan interaction into the reward function, leading to active cooperation among\nagents. This reward shaping method facilitates cooperation among agents while\noperating in a distributed manner. The proposed approach has been evaluated\nthrough experiments across various scenarios with different scales and agent\ncounts. The results are compared with those from other state-of-the-art (SOTA)\nplanners. The evidence suggests that the approach proposed in this letter\nparallels other planners in numerous aspects, and outperforms them in scenarios\nfeaturing a large number of agents.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages,9 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.10403v1",
    "published_date": "2024-07-15 02:44:41 UTC",
    "updated_date": "2024-07-15 02:44:41 UTC"
  },
  {
    "arxiv_id": "2407.11095v1",
    "title": "DeepGate3: Towards Scalable Circuit Representation Learning",
    "authors": [
      "Zhengyuan Shi",
      "Ziyang Zheng",
      "Sadaf Khan",
      "Jianyuan Zhong",
      "Min Li",
      "Qiang Xu"
    ],
    "abstract": "Circuit representation learning has shown promising results in advancing the\nfield of Electronic Design Automation (EDA). Existing models, such as DeepGate\nFamily, primarily utilize Graph Neural Networks (GNNs) to encode circuit\nnetlists into gate-level embeddings. However, the scalability of GNN-based\nmodels is fundamentally constrained by architectural limitations, impacting\ntheir ability to generalize across diverse and complex circuit designs. To\naddress these challenges, we introduce DeepGate3, an enhanced architecture that\nintegrates Transformer modules following the initial GNN processing. This novel\narchitecture not only retains the robust gate-level representation capabilities\nof its predecessor, DeepGate2, but also enhances them with the ability to model\nsubcircuits through a novel pooling transformer mechanism. DeepGate3 is further\nrefined with multiple innovative supervision tasks, significantly enhancing its\nlearning process and enabling superior representation of both gate-level and\nsubcircuit structures. Our experiments demonstrate marked improvements in\nscalability and generalizability over traditional GNN-based approaches,\nestablishing a significant step forward in circuit representation learning\ntechnology.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11095v1",
    "published_date": "2024-07-15 02:44:21 UTC",
    "updated_date": "2024-07-15 02:44:21 UTC"
  },
  {
    "arxiv_id": "2407.10387v1",
    "title": "Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity",
    "authors": [
      "Santiago Pascual",
      "Chunghsin Yeh",
      "Ioannis Tsiamas",
      "Joan Serrà"
    ],
    "abstract": "Video-to-audio (V2A) generation leverages visual-only video features to\nrender plausible sounds that match the scene. Importantly, the generated sound\nonsets should match the visual actions that are aligned with them, otherwise\nunnatural synchronization artifacts arise. Recent works have explored the\nprogression of conditioning sound generators on still images and then video\nfeatures, focusing on quality and semantic matching while ignoring\nsynchronization, or by sacrificing some amount of quality to focus on improving\nsynchronization only. In this work, we propose a V2A generative model, named\nMaskVAT, that interconnects a full-band high-quality general audio codec with a\nsequence-to-sequence masked generative model. This combination allows modeling\nboth high audio quality, semantic matching, and temporal synchronicity at the\nsame time. Our results show that, by combining a high-quality codec with the\nproper pre-trained audio-visual features and a sequence-to-sequence parallel\nstructure, we are able to yield highly synchronized results on one hand, whilst\nbeing competitive with the state of the art of non-codec generative audio\nmodels. Sample videos and generated audios are available at\nhttps://maskvat.github.io .",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.10387v1",
    "published_date": "2024-07-15 01:49:59 UTC",
    "updated_date": "2024-07-15 01:49:59 UTC"
  },
  {
    "arxiv_id": "2407.10385v2",
    "title": "By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting",
    "authors": [
      "Hyungjun Yoon",
      "Biniyam Aschalew Tolera",
      "Taesik Gong",
      "Kimin Lee",
      "Sung-Ju Lee"
    ],
    "abstract": "Large language models (LLMs) have demonstrated exceptional abilities across\nvarious domains. However, utilizing LLMs for ubiquitous sensing applications\nremains challenging as existing text-prompt methods show significant\nperformance degradation when handling long sensor data sequences. We propose a\nvisual prompting approach for sensor data using multimodal LLMs (MLLMs). We\ndesign a visual prompt that directs MLLMs to utilize visualized sensor data\nalongside the target sensory task descriptions. Additionally, we introduce a\nvisualization generator that automates the creation of optimal visualizations\ntailored to a given sensory task, eliminating the need for prior task-specific\nknowledge. We evaluated our approach on nine sensory tasks involving four\nsensing modalities, achieving an average of 10% higher accuracy than text-based\nprompts and reducing token costs by 15.8 times. Our findings highlight the\neffectiveness and cost-efficiency of visual prompts with MLLMs for various\nsensory tasks. The source code is available at\nhttps://github.com/diamond264/ByMyEyes.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the 2024 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2024) Main",
    "pdf_url": "http://arxiv.org/pdf/2407.10385v2",
    "published_date": "2024-07-15 01:33:54 UTC",
    "updated_date": "2024-09-29 06:53:40 UTC"
  },
  {
    "arxiv_id": "2407.10382v2",
    "title": "Communication- and Computation-Efficient Distributed Submodular Optimization in Robot Mesh Networks",
    "authors": [
      "Zirui Xu",
      "Sandilya Sai Garimella",
      "Vasileios Tzoumas"
    ],
    "abstract": "We provide a communication- and computation-efficient method for distributed\nsubmodular optimization in robot mesh networks. Submodularity is a property of\ndiminishing returns that arises in active information gathering such as\nmapping, surveillance, and target tracking. Our method, Resource-Aware\ndistributed Greedy (RAG), introduces a new distributed optimization paradigm\nthat enables scalable and near-optimal action coordination. To this end, RAG\nrequires each robot to make decisions based only on information received from\nand about their neighbors. In contrast, the current paradigms allow the relay\nof information about all robots across the network. As a result, RAG's\ndecision-time scales linearly with the network size, while state-of-the-art\nnear-optimal submodular optimization algorithms scale cubically. We also\ncharacterize how the designed mesh-network topology affects RAG's approximation\nperformance. Our analysis implies that sparser networks favor scalability\nwithout proportionally compromising approximation performance: while RAG's\ndecision time scales linearly with network size, the gain in approximation\nperformance scales sublinearly. We demonstrate RAG's performance in simulated\nscenarios of area detection with up to 45 robots, simulating realistic\nrobot-to-robot (r2r) communication speeds such as the 0.25 Mbps speed of the\nDigi XBee 3 Zigbee 3.0. In the simulations, RAG enables real-time planning, up\nto three orders of magnitude faster than competitive near-optimal algorithms,\nwhile also achieving superior mean coverage performance. To enable the\nsimulations, we extend the high-fidelity and photo-realistic simulator AirSim\nby integrating a scalable collaborative autonomy pipeline to tens of robots and\nsimulating r2r communication delays. Our code is available at\nhttps://github.com/UM-iRaL/Resource-Aware-Coordination-AirSim.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10382v2",
    "published_date": "2024-07-15 01:25:39 UTC",
    "updated_date": "2025-01-31 18:22:56 UTC"
  },
  {
    "arxiv_id": "2407.17429v2",
    "title": "How Do Students Interact with an LLM-powered Virtual Teaching Assistant in Different Educational Settings?",
    "authors": [
      "Pratyusha Maiti",
      "Ashok K. Goel"
    ],
    "abstract": "Jill Watson, a virtual teaching assistant powered by LLMs, answers student\nquestions and engages them in extended conversations on courseware provided by\nthe instructors. In this paper, we analyze student interactions with Jill\nacross multiple courses and colleges, focusing on the types and complexity of\nstudent questions based on Bloom's Revised Taxonomy and tool usage patterns. We\nfind that, by supporting a wide range of cognitive demands, Jill encourages\nstudents to engage in sophisticated, higher-order cognitive questions. However,\nthe frequency of usage varies significantly across deployments, and the types\nof questions asked depend on course-specific contexts. These findings pave the\nway for future work on AI-driven educational tools tailored to individual\nlearning styles and course structure, potentially enhancing both the teaching\nand learning experience in classrooms.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted in the Seventeenth International Conference on Educational\n  Data Mining (EDM) Workshop: Leveraging LLMs for Next Generation Educational\n  Technologies, July 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.17429v2",
    "published_date": "2024-07-15 01:22:50 UTC",
    "updated_date": "2024-07-25 20:16:11 UTC"
  },
  {
    "arxiv_id": "2407.10380v3",
    "title": "NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models",
    "authors": [
      "Pranshu Pandya",
      "Vatsal Gupta",
      "Agney S Talwarr",
      "Tushar Kataria",
      "Dan Roth",
      "Vivek Gupta"
    ],
    "abstract": "Cognitive textual and visual reasoning tasks, including puzzles, series, and\nanalogies, demand the ability to quickly reason, decipher, and evaluate\npatterns both textually and spatially. Due to extensive training on vast\namounts of human-curated data, LLMs and VLMs excel in common-sense reasoning\ntasks, however still struggle with more complex reasoning that demands deeper\ncognitive understanding. We introduce NTSEBench, a new dataset designed to\nevaluate cognitive multi-modal reasoning and problem-solving skills of large\nmodels. The dataset contains 2728 multiple-choice questions, accompanied by a\ntotal of 4,642 images, categorized into 26 different types. These questions are\ndrawn from the nationwide NTSE examination in India and feature a mix of visual\nand textual general aptitude challenges, designed to assess intelligence and\ncritical thinking skills beyond mere rote learning. We establish baselines on\nthe dataset using state-of-the-art LLMs and VLMs. To facilitate a comparison\nbetween open source and propriety models, we propose four distinct modeling\nstrategies to handle different modalities -- text and images -- in the dataset\ninstances.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.CV",
    "comment": "28 pages, 3 figures, 12 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.10380v3",
    "published_date": "2024-07-15 01:21:56 UTC",
    "updated_date": "2025-04-01 17:25:53 UTC"
  },
  {
    "arxiv_id": "2407.10377v4",
    "title": "Enhanced Masked Image Modeling to Avoid Model Collapse on Multi-modal MRI Datasets",
    "authors": [
      "Linxuan Han",
      "Sa Xiao",
      "Zimeng Li",
      "Haidong Li",
      "Xiuchao Zhao",
      "Yeqing Han",
      "Fumin Guo",
      "Xin Zhou"
    ],
    "abstract": "Multi-modal magnetic resonance imaging (MRI) provides information of lesions\nfor computer-aided diagnosis from different views. Deep learning algorithms are\nsuitable for identifying specific anatomical structures, segmenting lesions,\nand classifying diseases. Manual labels are limited due to the high expense,\nwhich hinders further improvement of accuracy. Self-supervised learning,\nparticularly masked image modeling (MIM), has shown promise in utilizing\nunlabeled data. However, we spot model collapse when applying MIM to\nmulti-modal MRI datasets. The performance of downstream tasks does not see any\nimprovement following the collapsed model. To solve model collapse, we analyze\nand address it in two types: complete collapse and dimensional collapse. We\nfind complete collapse occurs because the collapsed loss value in multi-modal\nMRI datasets falls below the normally converged loss value. Based on this, the\nhybrid mask pattern (HMP) masking strategy is introduced to elevate the\ncollapsed loss above the normally converged loss value and avoid complete\ncollapse. Additionally, we reveal that dimensional collapse stems from\ninsufficient feature uniformity in MIM. We mitigate dimensional collapse by\nintroducing the pyramid barlow twins (PBT) module as an explicit regularization\nmethod. Overall, we construct the enhanced MIM (E-MIM) with HMP and PBT module\nto avoid model collapse multi-modal MRI. Experiments are conducted on three\nmulti-modal MRI datasets to validate the effectiveness of our approach in\npreventing both types of model collapse. By preventing model collapse, the\ntraining of the model becomes more stable, resulting in a decent improvement in\nperformance for segmentation and classification tasks. The code is available at\nhttps://github.com/LinxuanHan/E-MIM.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "This work has been submitted to the lEEE for possible publication.\n  copyright may be transferred without notice, after which this version may no\n  longer be accessible",
    "pdf_url": "http://arxiv.org/pdf/2407.10377v4",
    "published_date": "2024-07-15 01:11:30 UTC",
    "updated_date": "2025-01-16 01:30:35 UTC"
  },
  {
    "arxiv_id": "2407.10374v2",
    "title": "An Empirical Study of Mamba-based Pedestrian Attribute Recognition",
    "authors": [
      "Xiao Wang",
      "Weizhe Kong",
      "Jiandong Jin",
      "Shiao Wang",
      "Ruichong Gao",
      "Qingchuan Ma",
      "Chenglong Li",
      "Jin Tang"
    ],
    "abstract": "Current strong pedestrian attribute recognition models are developed based on\nTransformer networks, which are computationally heavy. Recently proposed models\nwith linear complexity (e.g., Mamba) have garnered significant attention and\nhave achieved a good balance between accuracy and computational cost across a\nvariety of visual tasks. Relevant review articles also suggest that while these\nmodels can perform well on some pedestrian attribute recognition datasets, they\nare generally weaker than the corresponding Transformer models. To further tap\ninto the potential of the novel Mamba architecture for PAR tasks, this paper\ndesigns and adapts Mamba into two typical PAR frameworks, i.e., the text-image\nfusion approach and pure vision Mamba multi-label recognition framework. It is\nfound that interacting with attribute tags as additional input does not always\nlead to an improvement, specifically, Vim can be enhanced, but VMamba cannot.\nThis paper further designs various hybrid Mamba-Transformer variants and\nconducts thorough experimental validations. These experimental results indicate\nthat simply enhancing Mamba with a Transformer does not always lead to\nperformance improvements but yields better results under certain settings. We\nhope this empirical study can further inspire research in Mamba for PAR, and\neven extend into the domain of multi-label recognition, through the design of\nthese network structures and comprehensive experimentation. The source code of\nthis work will be released at \\url{https://github.com/Event-AHU/OpenPAR}",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "In Peer Review",
    "pdf_url": "http://arxiv.org/pdf/2407.10374v2",
    "published_date": "2024-07-15 00:48:06 UTC",
    "updated_date": "2024-12-03 04:25:30 UTC"
  },
  {
    "arxiv_id": "2407.10373v1",
    "title": "Mutual Learning for Acoustic Matching and Dereverberation via Visual Scene-driven Diffusion",
    "authors": [
      "Jian Ma",
      "Wenguan Wang",
      "Yi Yang",
      "Feng Zheng"
    ],
    "abstract": "Visual acoustic matching (VAM) is pivotal for enhancing the immersive\nexperience, and the task of dereverberation is effective in improving audio\nintelligibility. Existing methods treat each task independently, overlooking\nthe inherent reciprocity between them. Moreover, these methods depend on paired\ntraining data, which is challenging to acquire, impeding the utilization of\nextensive unpaired data. In this paper, we introduce MVSD, a mutual learning\nframework based on diffusion models. MVSD considers the two tasks\nsymmetrically, exploiting the reciprocal relationship to facilitate learning\nfrom inverse tasks and overcome data scarcity. Furthermore, we employ the\ndiffusion model as foundational conditional converters to circumvent the\ntraining instability and over-smoothing drawbacks of conventional GAN\narchitectures. Specifically, MVSD employs two converters: one for VAM called\nreverberator and one for dereverberation called dereverberator. The\ndereverberator judges whether the reverberation audio generated by reverberator\nsounds like being in the conditional visual scenario, and vice versa. By\nforming a closed loop, these two converters can generate informative feedback\nsignals to optimize the inverse tasks, even with easily acquired one-way\nunpaired data. Extensive experiments on two standard benchmarks, i.e.,\nSoundSpaces-Speech and Acoustic AVSpeech, exhibit that our framework can\nimprove the performance of the reverberator and dereverberator and better match\nspecified visual scenarios.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "ECCV 2024; Project page: https://hechang25.github.io/MVSD",
    "pdf_url": "http://arxiv.org/pdf/2407.10373v1",
    "published_date": "2024-07-15 00:47:56 UTC",
    "updated_date": "2024-07-15 00:47:56 UTC"
  },
  {
    "arxiv_id": "2407.10366v2",
    "title": "Accessing Vision Foundation Models via ImageNet-1K",
    "authors": [
      "Yitian Zhang",
      "Xu Ma",
      "Yue Bai",
      "Huan Wang",
      "Yun Fu"
    ],
    "abstract": "Vision foundation models are renowned for the generalization ability due to\nmassive training data. Nevertheless, they demand tremendous training resources,\nand the training data is often inaccessible, e.g., CLIP, DINOv2, posing great\nchallenges to developing derivatives that could facilitate the research. In\nthis work, we offer a very simple and general solution, named \\textit{Proteus},\nto distill foundation models into smaller equivalents on ImageNet-1K without\naccess to the original training data. Specifically, we remove the designs from\nconventional knowledge distillation settings that result in dataset bias and\npresent three levels of training objectives, i.e., token, patch, and feature,\nto maximize the efficacy of knowledge transfer. In this manner, Proteus is\ntrained at ImageNet-level costs with surprising ability, facilitating the\naccessibility of training foundation models for the broader research community.\nWhen leveraging DINOv2-g/14 as the teacher, Proteus-L/14 matches the\nperformance of the Oracle method DINOv2-L/14 (142M training data) across 19\nbenchmarks and outperforms other vision foundation models including CLIP-L/14\n(400M), OpenCLIP-L/14 (400M/2B) and SynCLR-L/14 (600M) with a significantly\nsmaller training set of 1.2M images.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICLR2025",
    "pdf_url": "http://arxiv.org/pdf/2407.10366v2",
    "published_date": "2024-07-15 00:13:53 UTC",
    "updated_date": "2025-02-11 18:44:46 UTC"
  }
]
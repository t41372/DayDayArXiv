{
  "date": "2025-10-21",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-10-21 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\næˆ‘æ˜¯ä½ çš„ä¸“å±ç ”ç©¶å‘˜å’Œæ—¥æŠ¥ä½œè€…ã€‚ä»Šå¤© arXiv çš„æ›´æ–°å¯ä»¥ç”¨**â€œå®šä¹‰æœªæ¥ä¸è½åœ°å®æˆ˜â€**æ¥å½¢å®¹ã€‚æœ€é‡ç£…çš„æ¶ˆæ¯è«è¿‡äºå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œå¤§ä½¬è”æ‰‹ç»™å‡ºäº† AGI çš„é‡åŒ–å®šä¹‰ï¼›åŒæ—¶ï¼Œå¼€æºç¤¾åŒºè¿æ¥äº†ä¸€ä¸ª 1T å‚æ•°é‡çš„æ¨ç†æ¨¡å‹ Ring-1Tï¼›Google å‘å¸ƒäº†åœ°çƒçº§çš„åŸºç¡€æ¨¡å‹ Earth AIï¼›æé£é£å›¢é˜Ÿåœ¨ç§»åŠ¨æ“ä½œæœºå™¨äººæ•°æ®ç”Ÿæˆä¸Šæœ‰äº†æ–°è¿›å±•ã€‚æ­¤å¤–ï¼Œå…³äº LLM å†…éƒ¨æ¨ç†æœºåˆ¶ï¼ˆæ·±åº¦åˆ©ç”¨ã€Scaling Lawï¼‰çš„ç ”ç©¶ä¹Ÿé¢‡ä¸ºæ·±å…¥ã€‚\n\nä¸‹é¢æˆ‘ä»¬ç›´å…¥ä¸»é¢˜ï¼Œçœ‹çœ‹ä»Šå¤©ä¸å®¹é”™è¿‡çš„ç¡¬æ ¸è®ºæ–‡ã€‚\n\n---\n\n### ğŸš€ é‡ç£…å¿…è¯» (Top Picks)\n\nè¿™å‡ ç¯‡æ–‡ç« å®šä¹‰äº†ä»Šå¤©çš„å¤´æ¡ï¼Œæ¶µç›–äº† AGI å®šä¹‰ã€è¶…å¤§æ¨¡å‹å¼€æºå’Œç§‘å­¦æ™ºèƒ½ã€‚\n\n## 158. AGI çš„å®šä¹‰\n**# title: A Definition of AGI**\n> **authors:** Dan Hendrycks, Dawn Song, Christian Szegedy, Yoshua Bengio, et al. (ä¼—æ˜Ÿäº‘é›†)\n\n**æ ¸å¿ƒå‘ç°ï¼š** è¿™æ˜¯ä¸€ç¯‡è¿™ä¹Ÿå¯èƒ½æ˜¯è¿‘æœŸæœ€é‡è¦çš„å…ƒç§‘å­¦è®ºæ–‡ä¹‹ä¸€ã€‚å›¾çµå¥–å¾—ä¸» Bengio ç­‰å¤§ä½¬è”æ‰‹ï¼Œè¯•å›¾è§£å†³â€œåˆ°åº•ä»€ä¹ˆæ˜¯ AGIâ€è¿™ä¸ªæ¨¡ç³Šçš„é—®é¢˜ã€‚\n- **å®šä¹‰æ¡†æ¶ï¼š** è®ºæ–‡åŸºäº Cattell-Horn-Carroll è®¤çŸ¥ç†è®ºï¼Œæå‡º AGI åº”å½“åŒ¹é…â€œå—è¿‡è‰¯å¥½æ•™è‚²çš„æˆå¹´äººâ€çš„è®¤çŸ¥å¤šåŠŸèƒ½æ€§å’Œç†Ÿç»ƒåº¦ã€‚\n- **é‡åŒ–è¯„ä¼°ï¼š** ä»–ä»¬å°†é€šç”¨æ™ºèƒ½åˆ†è§£ä¸º 10 ä¸ªæ ¸å¿ƒè®¤çŸ¥é¢†åŸŸï¼ˆå¦‚æ¨ç†ã€è®°å¿†ã€æ„ŸçŸ¥ç­‰ï¼‰ã€‚\n- **ç°çŠ¶æ®‹é…·ï¼š** æµ‹è¯„å‘ç°ï¼Œç›®å‰çš„ AI æ¨¡å‹è®¤çŸ¥è½®å»“å‘ˆç°é«˜åº¦â€œé”¯é½¿çŠ¶â€ï¼ˆJaggedï¼‰â€”â€”åœ¨çŸ¥è¯†å¯†é›†å‹é¢†åŸŸå¾ˆå¼ºï¼Œä½†åœ¨é•¿æœŸè®°å¿†ç­‰åŸºç¡€è®¤çŸ¥æœºåˆ¶ä¸Šå­˜åœ¨ä¸¥é‡ç¼ºé™·ã€‚\n- **å¾—åˆ†ï¼š** åœ¨è¿™ä¸ªæ¡†æ¶ä¸‹ï¼Œ**GPT-4 å¾—åˆ†ä»…ä¸º 27%ï¼ŒGPT-5ï¼ˆå‡è®¾ä»£å·ï¼‰ä¸º 57%**ã€‚è¿™é‡åŒ–äº†æˆ‘ä»¬è·ç¦» AGI è¿˜æœ‰å¤šè¿œã€‚\n\n## 34. 1T å‚æ•°çš„å¼€æºæ¨ç†æ¨¡å‹ Ring-1T\n**# title: Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model (æ¯ä¸€æ­¥éƒ½åœ¨è¿›åŒ–ï¼šæ‰©å±•å¼ºåŒ–å­¦ä¹ ä»¥æ„å»ºä¸‡äº¿çº§æ€ç»´æ¨¡å‹)**\n> **authors:** Ling Team et al.\n\n**æ ¸å¿ƒå‘ç°ï¼š** å¼€æºç•Œçš„ä¸€æšæ·±æ°´ç‚¸å¼¹ã€‚\n- **æ¨¡å‹è§„æ¨¡ï¼š** Ring-1T æ˜¯é¦–ä¸ªå¼€æºçš„ä¸‡äº¿å‚æ•°ï¼ˆTotal 1T, æ¿€æ´» 50B/tokenï¼‰çš„ MoE æ€ç»´æ¨¡å‹ã€‚\n- **æŠ€æœ¯çªç ´ï¼š** è§£å†³äº†ä¸‡äº¿å‚æ•°æ¨¡å‹è®­ç»ƒä¸­çš„ä¸ç¨³å®šæ€§ã€‚æå‡ºäº† **IcePop**ï¼ˆé€šè¿‡æ©ç å’Œè£å‰ªè§£å†³è®­ç»ƒ-æ¨ç†ä¸åŒ¹é…ï¼‰ã€**C3PO++**ï¼ˆåŠ¨æ€åˆ†å‰²é•¿ rollout æé«˜èµ„æºåˆ©ç”¨ç‡ï¼‰å’Œ **ASystem**ï¼ˆé«˜æ€§èƒ½ RL æ¡†æ¶ï¼‰ã€‚\n- **æ€§èƒ½ï¼š** åœ¨æ•°å­¦ç«èµ›ï¼ˆAIME 2025, IMO 2025 é“¶ç‰Œæ°´å¹³ï¼‰å’Œç¼–ç¨‹ï¼ˆCodeForcesï¼‰ä¸Šè¡¨ç°æƒŠäººã€‚å®ƒä¸ä»…æ˜¯ä¸€ä¸ªæ¨¡å‹ï¼Œæ›´æ˜¯å‘ç¤¾åŒºå¼€æ”¾äº†å¤§è§„æ¨¡æ¨ç†æ™ºèƒ½çš„å…¥åœºåˆ¸ã€‚\n\n## 129. Google çš„åœ°çƒçº§åŸºç¡€æ¨¡å‹\n**# title: Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning (Earth AIï¼šåˆ©ç”¨åŸºç¡€æ¨¡å‹å’Œè·¨æ¨¡æ€æ¨ç†ä»¥æ­¤è§£é”åœ°ç†ç©ºé—´æ´å¯Ÿ)**\n> **authors:** Aaron Bell, ..., Jeff Dean, et al. (Google Research)\n\n**æ ¸å¿ƒå‘ç°ï¼š** Google å±•ç¤ºäº†å…¶å¤„ç†åœ°çƒçº§æ•°æ®çš„é‡å¿ƒã€‚\n- **å¤šæ¨¡æ€æ•´åˆï¼š** Earth AI æ˜¯ä¸€ä¸ªåŸºç¡€æ¨¡å‹å®¶æ—ï¼Œæ¶µç›–äº†è¡Œæ˜Ÿçº§å›¾åƒã€äººå£æ•°æ®å’Œç¯å¢ƒæ•°æ®ã€‚\n- **Agentic Reasoningï¼š** å¼•å…¥äº†åŸºäº Gemini çš„æ¨ç†å¼•æ“ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„å¤šæ­¥æŸ¥è¯¢ï¼ˆä¾‹å¦‚å±æœºåœºæ™¯ä¸‹çš„å®æ—¶åˆ†æï¼‰ã€‚\n- **è´¡çŒ®ï¼š** è¯æ˜äº†å°†ä¸åŒé¢†åŸŸçš„åœ°ç†ç©ºé—´æ•°æ®é€šè¿‡åŸºç¡€æ¨¡å‹ååŒå·¥ä½œï¼Œå¯ä»¥è§£é”ä¼ ç»Ÿå•ä¸€æ¨¡å‹æ— æ³•å®ç°çš„é¢„æµ‹èƒ½åŠ›ï¼Œæ¯”å¦‚å‡†ç¡®æè¿°æ´ªæ°´é£é™©åŠå…¶å¯¹ç‰¹å®šç¤¾åŒºçš„å½±å“ã€‚\n\n## 130. æé£é£å›¢é˜Ÿï¼šç§»åŠ¨æ“ä½œæœºå™¨äººçš„æ•°æ®ç”Ÿæˆ\n**# title: MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation (MoMaGenï¼šåœ¨è½¯ç¡¬çº¦æŸä¸‹ä¸ºå¤šæ­¥åŒè‡‚ç§»åŠ¨æ“ä½œç”Ÿæˆæ¼”ç¤º)**\n> **authors:** Chengshu Li, ..., Jiajun Wu, Li Fei-Fei\n\n**æ ¸å¿ƒå‘ç°ï¼š** é’ˆå¯¹æœºå™¨äººå­¦ä¹ ä¸­æ•°æ®æ˜‚è´µçš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å¤æ‚çš„â€œç§»åŠ¨+åŒè‡‚æ“ä½œâ€ã€‚\n- **æ–¹æ³•ï¼š** æå‡ºäº† MoMaGenï¼Œå°†æ•°æ®ç”Ÿæˆè½¬åŒ–ä¸ºä¸€ä¸ªçº¦æŸä¼˜åŒ–é—®é¢˜ã€‚\n- **åˆ›æ–°ï¼š** åŒæ—¶å¤„ç†ç¡¬çº¦æŸï¼ˆå¦‚å¯è¾¾æ€§ï¼‰å’Œè½¯çº¦æŸï¼ˆå¦‚å¯¼èˆªæ—¶çš„å¯è§æ€§ï¼‰ã€‚\n- **æ•ˆæœï¼š** èƒ½å¤Ÿä»**å•ä¸ª**æºæ¼”ç¤ºç”Ÿæˆæå…¶å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œä»¥æ­¤è®­ç»ƒçš„ç­–ç•¥ä»…éœ€ 40 ä¸ªçœŸå®ä¸–ç•Œæ¼”ç¤ºå¾®è°ƒå³å¯éƒ¨ç½²åˆ°ç‰©ç†æœºå™¨äººä¸Šã€‚\n\n---\n\n### ğŸ§  LLM æ¨ç†ã€å¯¹é½ä¸æœºç† (Reasoning & Alignment)\n\nä»Šå¤©çš„è®ºæ–‡åœ¨è§£é‡Š LLM â€œå¦‚ä½•æ€è€ƒâ€ä»¥åŠâ€œå¦‚ä½•æ›´é«˜æ•ˆè®­ç»ƒâ€ä¸Šæœ‰å¾ˆå¤šæ·±åˆ»è§è§£ã€‚\n\n## 31. LLM æ˜¯å¦‚ä½•åˆ©ç”¨ç½‘ç»œæ·±åº¦çš„ï¼Ÿ\n**# title: How Do LLMs Use Their Depth? (LLM å¦‚ä½•åˆ©ç”¨å…¶æ·±åº¦ï¼Ÿ)**\n> **authors:** Akshat Gupta et al.\n\n**æ ¸å¿ƒå‘ç°ï¼š** è¿™ç¯‡æ–‡ç« é€šè¿‡è¿½è¸ªä¸­é—´å±‚è¡¨ç¤ºï¼Œæ­ç¤ºäº† LLM å†…éƒ¨çš„**\"å…ˆçŒœåæ”¹\" (Guess-then-Refine)** æœºåˆ¶ã€‚\n- **ç°è±¡ï¼š** æ—©æœŸå±‚å€¾å‘äºé¢„æµ‹é«˜é¢‘è¯ï¼ˆåŸºäºç»Ÿè®¡çš„çŒœæµ‹ï¼‰ï¼Œéšç€ç½‘ç»œå˜æ·±ï¼Œä¸Šä¸‹æ–‡ä¿¡æ¯ä»‹å…¥ï¼Œå°†è¿™äº›çŒœæµ‹ä¿®æ­£ä¸ºä¸Šä¸‹æ–‡åˆé€‚çš„ tokenã€‚\n- **æ·±åº¦å·®å¼‚ï¼š** è™šè¯ï¼ˆFunction wordsï¼‰æœ€æ—©è¢«ç¡®å®šï¼Œè€Œäº‹å®å›å¿†ï¼ˆFact recallï¼‰çš„å¤š token ç­”æ¡ˆä¸­ï¼Œç¬¬ä¸€ä¸ª token éœ€è¦æœ€æ·±çš„è®¡ç®—æ·±åº¦ã€‚è¿™ä¸ºæœªæ¥çš„å±‚çº§å‰ªææˆ–åŠ¨æ€è®¡ç®—æä¾›äº†ç†è®ºä¾æ®ã€‚\n\n## 153. æ¨ç†æ•ˆç‡ä¸ Scaling Laws\n**# title: Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs (å½“ Scaling Laws é‡ä¸Šæ¨¡å‹æ¶æ„ï¼šè¿ˆå‘æ¨ç†é«˜æ•ˆçš„ LLM)**\n> **authors:** Song Bian et al.\n\n**æ ¸å¿ƒå‘ç°ï¼š** ä¼ ç»Ÿçš„ Scaling Laws åªçœ‹å‚æ•°é‡å’Œæ•°æ®é‡ï¼Œè¿™ç¯‡è®ºæ–‡åŠ å…¥äº†**æ¨ç†æˆæœ¬**çš„è€ƒé‡ã€‚\n- **æ¡ä»¶ Scaling Lawï¼š** å¼•å…¥äº† hidden sizeã€MLP/Attention æ¯”ä¾‹ã€GQA ç­‰æ¶æ„å› ç´ ã€‚\n- **ç»“è®ºï¼š** åŒæ ·çš„è®­ç»ƒé¢„ç®—ä¸‹ï¼Œä¼˜åŒ–åçš„æ¶æ„æ¯” LLaMA-3.2 å‡†ç¡®ç‡é«˜ 2.1%ï¼Œæ¨ç†ååé‡é«˜ 42%ã€‚è¿™å¯¹äºå·¥ä¸šç•Œéƒ¨ç½²è‡³å…³é‡è¦ã€‚\n\n## 68. è§£å†³ Off-Policy RL çš„ä¸ç¨³å®šæ€§\n**# title: BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping**\n> **authors:** Zhiheng Xi et al.\n\n**æ ¸å¿ƒå‘ç°ï¼š** é’ˆå¯¹ RLHF/RLAIF ä¸­ Off-policyï¼ˆä½¿ç”¨æ—§æ•°æ®ï¼‰è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚\n- **æ´å¯Ÿï¼š** å‘ç°è´Ÿä¼˜åŠ¿æ ·æœ¬ï¼ˆNegative-advantageï¼‰åœ¨æ¢¯åº¦ä¸­å ä¸»å¯¼ï¼Œå¯¼è‡´æ¢ç´¢è¢«æŠ‘åˆ¶ã€‚\n- **æ–¹æ³•ï¼š** BAPO é€šè¿‡è‡ªé€‚åº”è£å‰ªå¹³è¡¡æ­£è´Ÿè´¡çŒ®ã€‚\n- **æˆ˜ç»©ï¼š** 7B æ¨¡å‹åœ¨æ•°å­¦åŸºå‡†ä¸Šè¶…è¶Šäº† SkyWorkï¼Œ32B æ¨¡å‹å‡»è´¥äº† Gemini-2.5-Flash-Thinkingã€‚\n\n## 44. æ— å¥–åŠ±çš„è‡ªæˆ‘å¾®è°ƒ\n**# title: Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning without Rewards (LLM æ¨ç†çš„åœ¨çº¿ SFTï¼šæ— å¥–åŠ±è‡ªæˆ‘å¾®è°ƒçš„æƒŠäººæ•ˆæœ)**\n> **authors:** Mengqi Li et al.\n\n**æ ¸å¿ƒå‘ç°ï¼š** å¤§é“è‡³ç®€ã€‚\n- **æ–¹æ³•ï¼š** OSFT (Online Supervised Finetuning)ã€‚æ¨¡å‹è‡ªå·±ç”Ÿæˆå›å¤ï¼Œç„¶åç›´æ¥æ‹¿è¿™äº›æ•°æ®å¾®è°ƒè‡ªå·±ï¼Œ**ä¸éœ€è¦** å¤–éƒ¨çš„å¥–åŠ±æ¨¡å‹æˆ–éªŒè¯å™¨ã€‚\n- **æ•ˆæœï¼š** åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼Œæ•ˆæœç«Ÿç„¶å¯ä»¥åª²ç¾å¤æ‚çš„ RLVR (å¦‚ GRPO)ã€‚è¿™è¡¨æ˜é¢„è®­ç»ƒæ¨¡å‹ä¸­å…¶å®è•´å«äº†è¶³å¤Ÿæ½œèƒ½ï¼Œåªæ˜¯éœ€è¦é€šè¿‡è‡ªæˆ‘ç”Ÿæˆçš„åˆ†å¸ƒæ¥æ¿€å‘ã€‚\n\n## 152. è‡ªæˆ‘è°ƒèŠ‚çš„ Token é€‰æ‹©\n**# title: ssToken: Self-modulated and Semantic-aware Token Selection for LLM Fine-tuning (ssTokenï¼šç”¨äº LLM å¾®è°ƒçš„è‡ªæˆ‘è°ƒèŠ‚å’Œè¯­ä¹‰æ„ŸçŸ¥ Token é€‰æ‹©)**\n> **authors:** Xiaohan Qin et al.\n\n**æ ¸å¿ƒå‘ç°ï¼š** ä¸æ˜¯æ‰€æœ‰ Token éƒ½ç”Ÿè€Œå¹³ç­‰ã€‚\n- **æ–¹æ³•ï¼š** ä»¥å‰é€‰æ•°æ®è¦ä¹ˆé å¤–éƒ¨æ¨¡å‹ï¼Œè¦ä¹ˆé  Lossã€‚ssToken åˆ©ç”¨å†å²æ¨¡å‹è®¡ç®— Loss å·®å€¼ä½œä¸ºâ€œè‡ªæˆ‘è°ƒèŠ‚â€ä¿¡å·ï¼Œå¹¶ç»“åˆæ³¨æ„åŠ›æœºåˆ¶è¯„ä¼° Token çš„è¯­ä¹‰é‡è¦æ€§ã€‚\n- **ä»·å€¼ï¼š** åœ¨å¾®è°ƒæ—¶åªè®­ç»ƒé‡è¦çš„ Tokenï¼Œæ—¢ææ•ˆåˆæç‚¹ã€‚\n\n---\n\n### ğŸ¨ å¤šæ¨¡æ€ä¸ç”Ÿæˆå¼ AI (Multimodal & GenAI)\n\n## 2. åƒè¿™æ§ä»£ç ä¸€æ ·æŒæ§éŸ³ä¹ç”Ÿæˆ\n**# title: Steering Autoregressive Music Generation with Recursive Feature Machines (ç”¨é€’å½’ç‰¹å¾æœºå¼•å¯¼è‡ªå›å½’éŸ³ä¹ç”Ÿæˆ)**\n> **authors:** Daniel Zhao et al.\n\n**æ ¸å¿ƒå‘ç°ï¼š** è§£å†³äº†éŸ³ä¹ç”Ÿæˆä¸­â€œå¯æ§æ€§â€ä¸â€œéŸ³è´¨â€çš„æƒè¡¡ã€‚\n- **æ–¹æ³•ï¼š** ä½¿ç”¨ Recursive Feature Machines (RFMs) åœ¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆMusicGenï¼‰çš„æ¿€æ´»ç©ºé—´ä¸­æ‰¾åˆ°å¯¹åº”éŸ³ç¬¦ã€å’Œå¼¦çš„â€œæ¦‚å¿µæ–¹å‘â€ã€‚\n- **æ•ˆæœï¼š** æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œå³å¯åœ¨æ¨ç†é˜¶æ®µå®æ—¶å¹²é¢„ã€‚å°†ç‰¹å®šéŸ³ç¬¦ç”Ÿæˆçš„å‡†ç¡®ç‡ä» 0.23 æå‡åˆ° 0.82ï¼Œä¸”å‡ ä¹ä¸å½±å“æ–‡æœ¬æç¤ºçš„éµå¾ªåº¦ã€‚\n\n## 70. å¤šä¸»ä½“è§†é¢‘ç”Ÿæˆ\n**# title: Kaleido: Open-Sourced Multi-Subject Reference Video Generation Model (Kaleidoï¼šå¼€æºå¤šä¸»ä½“å‚è€ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹)**\n> **authors:** Zhenxing Zhang et al.\n\n**æ ¸å¿ƒå‘ç°ï¼š** è§†é¢‘ç”Ÿæˆçš„ä¸€å¤§ç—›ç‚¹æ˜¯â€œå¤šä¸ªäººç‰©/ä¸»ä½“çš„ä¸€è‡´æ€§â€ã€‚\n- **æ–¹æ³•ï¼š** å¼•å…¥äº† Reference Rotary Positional Encoding (R-RoPE) æ¥å¤„ç†å¤šä¸ªå‚è€ƒå›¾åƒï¼Œé˜²æ­¢ä¸åŒäººç‰©çš„ç‰¹å¾æ··æ·†ã€‚\n- **è´¡çŒ®ï¼š** å¼€æºäº†æ¨¡å‹å’Œä¸“é—¨çš„æ•°æ®æ„å»ºæµç¨‹ï¼Œæ˜¾è‘—æå‡äº†å¤šä¸»ä½“è§†é¢‘ç”Ÿæˆçš„ä¿çœŸåº¦ã€‚\n\n## 101. åˆ›é€ åŠ›éœ€è¦ä¸€ç‚¹â€œå¤–æ˜Ÿâ€æ€ç»´\n**# title: Cultural Alien Sampler: Open-ended art generation balancing originality and coherence (æ–‡åŒ–å¤–æ˜Ÿäººé‡‡æ ·å™¨ï¼šå¹³è¡¡åŸåˆ›æ€§ä¸è¿è´¯æ€§çš„å¼€æ”¾å¼è‰ºæœ¯ç”Ÿæˆ)**\n> **authors:** Alejandro H. Artiles et al. (NeurIPS 2025 Creative AI Track)\n\n**æ ¸å¿ƒå‘ç°ï¼š** LLM ç”Ÿæˆè‰ºæœ¯åˆ›æ„æ—¶å¾€å¾€å¤ªâ€œä¿—â€ï¼ˆè½å…¥å¸¸è§æ¨¡å¼ï¼‰ã€‚\n- **æ–¹æ³•ï¼š** CAS ç³»ç»Ÿä½¿ç”¨ä¸¤ä¸ªæ¨¡å‹ï¼Œä¸€ä¸ªè¯„ä¼°æ¦‚å¿µç»„åˆæ˜¯å¦â€œè¿è´¯â€ï¼ˆMake senseï¼‰ï¼Œå¦ä¸€ä¸ªè¯„ä¼°æ˜¯å¦â€œå…¸å‹â€ï¼ˆClichÃ©ï¼‰ã€‚\n- **ç­–ç•¥ï¼š** ä¸“é—¨å¯»æ‰¾é‚£äº›**é«˜è¿è´¯æ€§ä½†ä½å…¸å‹æ€§**çš„ç»„åˆï¼ˆå³â€œæ„æ–™ä¹‹å¤–ï¼Œæƒ…ç†ä¹‹ä¸­â€ï¼‰ã€‚è¿™è®© AI èƒ½ç”Ÿæˆæ›´å…·åŸåˆ›æ€§çš„è‰ºæœ¯æ¦‚å¿µã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ä¸ Agent (Safety & Agent)\n\n## 133. é’ˆå¯¹ Web Agent çš„çº¢é˜Ÿæ”»å‡»\n**# title: Genesis: Evolving Attack Strategies for LLM Web Agent Red-Teaming (Genesisï¼šè¿›åŒ–å¼ LLM Web Agent çº¢é˜Ÿæ”»å‡»ç­–ç•¥)**\n> **authors:** Zheng Zhang et al.\n\n**æ ¸å¿ƒå‘ç°ï¼š** éšç€ Agent å¼€å§‹æ“ä½œç½‘é¡µï¼Œå®‰å…¨é£é™©å‰§å¢ã€‚\n- **æ–¹æ³•ï¼š** Genesis æ¡†æ¶åˆ©ç”¨é—ä¼ ç®—æ³•è¿›åŒ–æ”»å‡»ç­–ç•¥ã€‚ä¸ä»…ä»…æ˜¯ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œè€Œæ˜¯é€šè¿‡â€œæ”»å‡»è€…-è¯„åˆ†è€…-ç­–ç•¥å®¶â€çš„å¾ªç¯ï¼ŒåŠ¨æ€å‘ç° Web Agent çš„è¡Œä¸ºæ¼æ´ã€‚\n- **ç»“æœï¼š** èƒ½å¤Ÿå‘ç°ç°æœ‰é™æ€æ–¹æ³•æ— æ³•æ‰¾åˆ°çš„å¤æ‚æ”»å‡»è·¯å¾„ã€‚\n\n## 157. å¤šæ¨¡æ€å®‰å…¨çš„ç›²åŒº\n**# title: VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety (VLSUï¼šç»˜åˆ¶ AI å®‰å…¨ä¸­è”åˆå¤šæ¨¡æ€ç†è§£çš„å±€é™)**\n> **authors:** Shruti Palaskar et al.\n\n**æ ¸å¿ƒå‘ç°ï¼š** å›¾æ–‡åˆ†å¼€çœ‹éƒ½å®‰å…¨ï¼Œåˆåœ¨ä¸€èµ·å°±â€œç‚¸â€äº†ã€‚\n- **å‘ç°ï¼š** ç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨å•æ¨¡æ€å®‰å…¨ä¸Šåšå¾—å¾ˆå¥½ï¼ˆ90%+ï¼‰ï¼Œä½†å½“éœ€è¦**è”åˆæ¨ç†**ï¼ˆä¾‹å¦‚å›¾ç‰‡æœ¬èº«æ— å®³ï¼Œé…ä¸Šç‰¹å®šæ–‡å­—å˜å¾—æœ‰å®³ï¼‰æ—¶ï¼Œå‡†ç¡®ç‡æ‰åˆ°äº† 20-55%ã€‚\n- **ç»“è®ºï¼š** è¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„å®‰å…¨éšæ‚£ï¼Œæ¨¡å‹ç›®å‰æå…¶ç¼ºä¹ç»„åˆæ¨ç†çš„å®‰å…¨åˆ¤æ–­èƒ½åŠ›ã€‚\n\n---\n\n### ğŸ”¬ ç§‘å­¦ä¸åŒ»ç–— AI (AI for Science)\n\n## 139. å¤§è„‘è¿æ¥ç»„çš„åŸºç¡€æ¨¡å‹\n**# title: Large Connectome Model: An fMRI Foundation Model of Brain Connectomes Empowered by Brain-Environment Interaction**\n> **authors:** Ziquan Wei et al.\n\n**æ ¸å¿ƒå‘ç°ï¼š** fMRI æ•°æ®æ ·æœ¬å°‘ï¼Œéš¾ä»¥è®­ç»ƒå¤§æ¨¡å‹ã€‚\n- **æ–¹æ³•ï¼š** å°†å¤§è„‘å»ºæ¨¡è§†ä¸ºå¤šä»»åŠ¡å­¦ä¹ ï¼Œåˆ©ç”¨ç¯å¢ƒå˜é‡å’Œäººå£ç»Ÿè®¡æ•°æ®è¾…åŠ© fMRI é¢„è®­ç»ƒã€‚\n- **åº”ç”¨ï¼š** åœ¨è‡ªé—­ç—‡ã€å¸•é‡‘æ£®ã€é˜¿å°”èŒ¨æµ·é»˜ç—‡çš„æ—©æœŸè¯Šæ–­ä¸Šè¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚\n\n## 3. å¯è§£é‡Šçš„åŒ»ç–—å½±åƒè¯Šæ–­\n**# title: Explainable Deep Learning in Medical Imaging: Brain Tumor and Pneumonia Detection**\n> **authors:** Sai Teja Erukude et al.\n\n**æ ¸å¿ƒå‘ç°ï¼š** æ¯”è¾ƒäº† ResNet50 å’Œ DenseNet121 åœ¨è„‘è‚¿ç˜¤å’Œè‚ºç‚æ£€æµ‹ä¸Šçš„è¡¨ç°ã€‚\n- **æœ‰è¶£å‘ç°ï¼š** è™½ç„¶ä¸¤è€…å‡†ç¡®ç‡éƒ½é«˜ï¼Œä½†é€šè¿‡ Grad-CAM çƒ­åŠ›å›¾å‘ç°ï¼Œ**DenseNet121 æ›´å…³æ³¨ç—…ç†æ ¸å¿ƒåŒºåŸŸ**ï¼Œè€Œ ResNet50 æœ‰æ—¶ä¼šå…³æ³¨æ— å…³åŒºåŸŸã€‚è¿™å¯¹åŒ»ç–— AI çš„å¯ä¿¡åº¦è‡³å…³é‡è¦ã€‚\n\n---\n\nä»Šå¤©å€¼å¾—æ³¨æ„çš„è¶‹åŠ¿æ˜¯ï¼š**AI æ­£åœ¨ä»â€œè¿½æ±‚é«˜åˆ†â€è½¬å‘â€œè¿½æ±‚å®šä¹‰ï¼ˆå¦‚ AGIï¼‰â€å’Œâ€œè¿½æ±‚æœºç†ï¼ˆå¦‚ Depth usage, Scaling lawsï¼‰â€**ã€‚åŒæ—¶ï¼ŒRing-1T çš„å¼€æºå’Œ Earth AI çš„å‘å¸ƒæ˜¾ç¤ºäº†æ¨¡å‹å‘æå¤§è§„æ¨¡ï¼ˆä¸‡äº¿å‚æ•°ï¼‰å’Œæå¹¿åº”ç”¨ï¼ˆåœ°çƒç§‘å­¦ï¼‰ä¸¤ç«¯æ‰©å±•çš„è¶‹åŠ¿ã€‚\n\nå¸Œæœ›è¿™ä»½å¿«æŠ¥å¯¹ä½ çš„ç ”ç©¶æœ‰æ‰€å¯å‘ã€‚æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2510.19128v1",
      "title": "A Cross-Environment and Cross-Embodiment Path Planning Framework via a Conditional Diffusion Model",
      "title_zh": "åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„è·¨ç¯å¢ƒä¸è·¨å…·èº«è·¯å¾„è§„åˆ’æ¡†æ¶",
      "authors": [
        "Mehran Ghafarian Tamizi",
        "Homayoun Honari",
        "Amir Mehdi Soufi Enayati",
        "Aleksey Nozdryn-Plotnicki",
        "Homayoun Najjaran"
      ],
      "abstract": "Path planning for a robotic system in high-dimensional cluttered environments needs to be efficient, safe, and adaptable for different environments and hardware. Conventional methods face high computation time and require extensive parameter tuning, while prior learning-based methods still fail to generalize effectively. The primary goal of this research is to develop a path planning framework capable of generalizing to unseen environments and new robotic manipulators without the need for retraining. We present GADGET (Generalizable and Adaptive Diffusion-Guided Environment-aware Trajectory generation), a diffusion-based planning model that generates joint-space trajectories conditioned on voxelized scene representations as well as start and goal configurations. A key innovation is GADGET's hybrid dual-conditioning mechanism that combines classifier-free guidance via learned scene encoding with classifier-guided Control Barrier Function (CBF) safety shaping, integrating environment awareness with real-time collision avoidance directly in the denoising process. This design supports zero-shot transfer to new environments and robotic embodiments without retraining. Experimental results show that GADGET achieves high success rates with low collision intensity in spherical-obstacle, bin-picking, and shelf environments, with CBF guidance further improving safety. Moreover, comparative evaluations indicate strong performance relative to both sampling-based and learning-based baselines. Furthermore, GADGET provides transferability across Franka Panda, Kinova Gen3 (6/7-DoF), and UR5 robots, and physical execution on a Kinova Gen3 demonstrates its ability to generate safe, collision-free trajectories in real-world settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GADGET (Generalizable and Adaptive Diffusion-Guided Environment-aware Trajectory generation)ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„è·¯å¾„è§„åˆ’æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é«˜ç»´å¤æ‚ç¯å¢ƒä¸­æœºå™¨äººè¿åŠ¨è§„åˆ’çš„æ•ˆç‡ã€å®‰å…¨æ€§å’Œæ³›åŒ–æ€§éš¾é¢˜ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºé‡‡ç”¨äº†ä¸€ç§æ··åˆåŒé‡æ¡ä»¶æœºåˆ¶ (Hybrid Dual-Conditioning Mechanism)ï¼Œå°†åŸºäºåœºæ™¯ç¼–ç çš„ Classifier-free Guidance ä¸åŸºäºæ§åˆ¶å±éšœå‡½æ•° (Control Barrier Function, CBF) çš„ Classifier-guided å®‰å…¨å¼•å¯¼ç›¸ç»“åˆï¼Œåœ¨å»å™ªè¿‡ç¨‹ä¸­ç›´æ¥æ•´åˆç¯å¢ƒæ„ŸçŸ¥ä¸å®æ—¶é¿éšœã€‚è¯¥æ¨¡å‹åˆ©ç”¨ä½“ç´ åŒ–åœºæ™¯è¡¨ç¤º (Voxelized Scene Representations) ç”Ÿæˆå…³èŠ‚ç©ºé—´è½¨è¿¹ï¼Œæ”¯æŒåœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°å‘æ–°ç¯å¢ƒå’Œæ–°æœºå™¨äººæœ¬ä½“ (Embodiments) çš„é›¶æ ·æœ¬è¿ç§» (Zero-shot Transfer)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGADGET åœ¨å¤šç§å¤æ‚ä»»åŠ¡åœºæ™¯ä¸­å‡è¡¨ç°å‡ºé«˜æˆåŠŸç‡å’Œä½ç¢°æ’å¼ºåº¦ï¼Œæ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„åŸºäºé‡‡æ ·å’Œå­¦ä¹ çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨ Franka Pandaã€Kinova Gen3 å’Œ UR5 ç­‰ä¸åŒå‹å·æœºå™¨äººä¸Šå±•ç¤ºäº†å¼ºå¤§çš„è·¨å¹³å°è¿ç§»èƒ½åŠ›ï¼Œå¹¶é€šè¿‡ç‰©ç†å®éªŒéªŒè¯äº†å…¶åœ¨çœŸå®ä¸–ç•Œä¸­ç”Ÿæˆå®‰å…¨æ— ç¢°æ’è½¨è¿¹çš„å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "20 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.19128v1",
      "published_date": "2025-10-21 23:30:14 UTC",
      "updated_date": "2025-10-21 23:30:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:42:11.877768+00:00"
    },
    {
      "arxiv_id": "2510.19127v1",
      "title": "Steering Autoregressive Music Generation with Recursive Feature Machines",
      "title_zh": "åˆ©ç”¨é€’å½’ç‰¹å¾æœºå¼•å¯¼è‡ªå›å½’éŸ³ä¹ç”Ÿæˆ",
      "authors": [
        "Daniel Zhao",
        "Daniel Beaglehole",
        "Taylor Berg-Kirkpatrick",
        "Julian McAuley",
        "Zachary Novack"
      ],
      "abstract": "Controllable music generation remains a significant challenge, with existing methods often requiring model retraining or introducing audible artifacts. We introduce MusicRFM, a framework that adapts Recursive Feature Machines (RFMs) to enable fine-grained, interpretable control over frozen, pre-trained music models by directly steering their internal activations. RFMs analyze a model's internal gradients to produce interpretable \"concept directions\", or specific axes in the activation space that correspond to musical attributes like notes or chords. We first train lightweight RFM probes to discover these directions within MusicGen's hidden states; then, during inference, we inject them back into the model to guide the generation process in real-time without per-step optimization. We present advanced mechanisms for this control, including dynamic, time-varying schedules and methods for the simultaneous enforcement of multiple musical properties. Our method successfully navigates the trade-off between control and generation quality: we can increase the accuracy of generating a target musical note from 0.23 to 0.82, while text prompt adherence remains within approximately 0.02 of the unsteered baseline, demonstrating effective control with minimal impact on prompt fidelity. We release code to encourage further exploration on RFMs in the music domain.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MusicRFMï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨é€’å½’ç‰¹å¾æœº (Recursive Feature Machines, RFMs) å¯¹é¢„è®­ç»ƒéŸ³ä¹ç”Ÿæˆæ¨¡å‹è¿›è¡Œå¾®è°ƒå’Œå¯è§£é‡Šæ§åˆ¶çš„æ¡†æ¶ã€‚MusicRFM æ—¨åœ¨è§£å†³å¯æ§éŸ³ä¹ç”Ÿæˆä¸­æ¨¡å‹é‡è®­æˆ–äº§ç”Ÿå¬è§‰ä¼ªå½±çš„æŒ‘æˆ˜ï¼Œé€šè¿‡ç›´æ¥å¼•å¯¼å†»ç»“æ¨¡å‹çš„å†…éƒ¨æ¿€æ´»å®ç°ç²¾ç»†åŒ–æ§åˆ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ RFMs åˆ†ææ¨¡å‹æ¢¯åº¦å¹¶æå–ä»£è¡¨éŸ³ç¬¦æˆ–å’Œå¼¦ç­‰éŸ³ä¹å±æ€§çš„â€œæ¦‚å¿µæ–¹å‘â€ (concept directions)ï¼Œéšåé€šè¿‡è½»é‡çº§æ¢é’ˆ (probes) åœ¨æ¨ç†é˜¶æ®µå®æ—¶æ³¨å…¥è¿™äº›æ–¹å‘ã€‚ç ”ç©¶è¿˜å±•ç¤ºäº†åŠ¨æ€æ—¶é—´è°ƒåº¦å’Œå¤šå±æ€§åŒæ—¶æ§åˆ¶çš„é«˜çº§æœºåˆ¶ï¼Œç¡®ä¿äº†æ§åˆ¶è¿‡ç¨‹æ— éœ€é€æ­¥ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMusicRFM èƒ½åœ¨ä¿æŒæ–‡æœ¬æç¤º (text prompt) å¿ å®åº¦çš„å‰æä¸‹ï¼Œå°†ç›®æ ‡éŸ³ç¬¦çš„ç”Ÿæˆå‡†ç¡®ç‡ä» 0.23 æ˜¾è‘—æå‡è‡³ 0.82ã€‚è¯¥æ–¹æ³•æˆåŠŸå¹³è¡¡äº†ç”Ÿæˆè´¨é‡ä¸æ§åˆ¶ç²¾åº¦ï¼Œä¸ºè‡ªå›å½’éŸ³ä¹ç”Ÿæˆæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å…·å¤‡å¯è§£é‡Šæ€§çš„å¼•å¯¼æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19127v1",
      "published_date": "2025-10-21 23:23:14 UTC",
      "updated_date": "2025-10-21 23:23:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:42:10.981590+00:00"
    },
    {
      "arxiv_id": "2510.21823v1",
      "title": "Explainable Deep Learning in Medical Imaging: Brain Tumor and Pneumonia Detection",
      "title_zh": "åŒ»å­¦å½±åƒä¸­çš„å¯è§£é‡Šæ·±åº¦å­¦ä¹ ï¼šè„‘è‚¿ç˜¤ä¸è‚ºç‚æ£€æµ‹",
      "authors": [
        "Sai Teja Erukude",
        "Viswa Chaitanya Marella",
        "Suhasnadh Reddy Veluru"
      ],
      "abstract": "Deep Learning (DL) holds enormous potential for improving medical imaging diagnostics, yet the lack of interpretability in most models hampers clinical trust and adoption. This paper presents an explainable deep learning framework for detecting brain tumors in MRI scans and pneumonia in chest X-ray images using two leading Convolutional Neural Networks, ResNet50 and DenseNet121. These models were trained on publicly available Kaggle datasets comprising 7,023 brain MRI images and 5,863 chest X-ray images, achieving high classification performance. DenseNet121 consistently outperformed ResNet50 with 94.3 percent vs. 92.5 percent accuracy for brain tumors and 89.1 percent vs. 84.4 percent accuracy for pneumonia. For better explainability, Gradient-weighted Class Activation Mapping (Grad-CAM) was integrated to create heatmap visualizations superimposed on the test images, indicating the most influential image regions in the decision-making process. Interestingly, while both models produced accurate results, Grad-CAM showed that DenseNet121 consistently focused on core pathological regions, whereas ResNet50 sometimes scattered attention to peripheral or non-pathological areas. Combining deep learning and explainable AI offers a promising path toward reliable, interpretable, and clinically useful diagnostic tools.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¯è§£é‡Šçš„Deep Learningæ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨MRIæ‰«æå’Œèƒ¸éƒ¨Xå°„çº¿å›¾åƒæ£€æµ‹è„‘è‚¿ç˜¤åŠè‚ºç‚ï¼Œä»¥è§£å†³æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ä¸´åºŠåº”ç”¨ä¸­ç¼ºä¹å¯è§£é‡Šæ€§çš„é—®é¢˜ã€‚ç ”ç©¶å¯¹æ¯”äº†ResNet50å’ŒDenseNet121ä¸¤ç§ä¸»æµçš„Convolutional Neural Networksï¼Œå¹¶åœ¨Kaggleå…¬å¼€æ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDenseNet121åœ¨è„‘è‚¿ç˜¤å’Œè‚ºç‚æ£€æµ‹ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°94.3%å’Œ89.1%ï¼Œä¸€è‡´ä¼˜äºResNet50ã€‚ä¸ºæå‡æ¨¡å‹é€æ˜åº¦ï¼Œç ”ç©¶é›†æˆäº†Gradient-weighted Class Activation Mapping (Grad-CAM)æŠ€æœ¯ï¼Œé€šè¿‡ç”Ÿæˆçƒ­å›¾æ¥å¯è§†åŒ–æ¨¡å‹å†³ç­–æ—¶çš„å…³é”®å…³æ³¨åŒºåŸŸã€‚åˆ†æå‘ç°ï¼ŒDenseNet121èƒ½æ›´ç²¾å‡†åœ°èšç„¦äºæ ¸å¿ƒç—…ç†åŒºåŸŸï¼Œè€ŒResNet50çš„æ³¨æ„åŠ›æœ‰æ—¶ä¼šåˆ†æ•£åˆ°éç—…ç†åŒºåŸŸã€‚è¯¥ç ”ç©¶é€šè¿‡ç»“åˆExplainable AIæŠ€æœ¯ï¼Œä¸ºå¼€å‘ä¸´åºŠå¯ä¿¡ä¸”é«˜æ•ˆçš„è¾…åŠ©è¯Šæ–­å·¥å…·æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Published in IEEE",
      "pdf_url": "https://arxiv.org/pdf/2510.21823v1",
      "published_date": "2025-10-21 22:44:40 UTC",
      "updated_date": "2025-10-21 22:44:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:42:16.685460+00:00"
    },
    {
      "arxiv_id": "2510.21822v1",
      "title": "Wavelet-based GAN Fingerprint Detection using ResNet50",
      "title_zh": "åŸºäº ResNet50 çš„å°æ³¢ GAN æŒ‡çº¹æ£€æµ‹",
      "authors": [
        "Sai Teja Erukude",
        "Suhasnadh Reddy Veluru",
        "Viswa Chaitanya Marella"
      ],
      "abstract": "Identifying images generated by Generative Adversarial Networks (GANs) has become a significant challenge in digital image forensics. This research presents a wavelet-based detection method that uses discrete wavelet transform (DWT) preprocessing and a ResNet50 classification layer to differentiate the StyleGAN-generated images from real ones. Haar and Daubechies wavelet filters are applied to convert the input images into multi-resolution representations, which will then be fed to a ResNet50 network for classification, capitalizing on subtle artifacts left by the generative process. Moreover, the wavelet-based models are compared to an identical ResNet50 model trained on spatial data. The Haar and Daubechies preprocessed models achieved a greater accuracy of 93.8 percent and 95.1 percent, much higher than the model developed in the spatial domain (accuracy rate of 81.5 percent). The Daubechies-based model outperforms Haar, showing that adding layers of descriptive frequency patterns can lead to even greater distinguishing power. These results indicate that the GAN-generated images have unique wavelet-domain artifacts or \"fingerprints.\" The method proposed illustrates the effectiveness of wavelet-domain analysis to detect GAN images and emphasizes the potential of further developing the capabilities of future deepfake detection systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå°æ³¢å˜æ¢ (Wavelet-based) çš„ GAN ç”Ÿæˆå›¾åƒæ£€æµ‹æ–¹æ³•ï¼Œåˆ©ç”¨ç¦»æ•£å°æ³¢å˜æ¢ (Discrete Wavelet Transform, DWT) é¢„å¤„ç†ä¸ ResNet50 åˆ†ç±»å±‚æ¥è¯†åˆ« StyleGAN ç”Ÿæˆçš„å›¾åƒã€‚é€šè¿‡åº”ç”¨ Haar å’Œ Daubechies å°æ³¢æ»¤æ³¢å™¨å°†å›¾åƒè½¬åŒ–ä¸ºå¤šåˆ†è¾¨ç‡è¡¨ç¤ºï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ•æ‰åˆ°ç”Ÿæˆè¿‡ç¨‹ä¸­ç•™ä¸‹çš„ç»†å¾®äººå·¥ç—•è¿¹æˆ–â€œæŒ‡çº¹â€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäº Daubechies å’Œ Haar å°æ³¢é¢„å¤„ç†çš„æ¨¡å‹å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº† 95.1% å’Œ 93.8%ï¼Œè¿œé«˜äºä¼ ç»Ÿç©ºé—´åŸŸ (Spatial Domain) æ¨¡å‹çš„ 81.5%ã€‚ç ”ç©¶å‘ç° Daubechies å°æ³¢å› å…¶æ›´ä¸°å¯Œçš„é¢‘ç‡æ¨¡å¼æè¿°èƒ½åŠ›è€Œä¼˜äº Haar å°æ³¢ï¼Œè¯æ˜äº† GAN ç”Ÿæˆå›¾åƒåœ¨å°æ³¢åŸŸå…·æœ‰ç‹¬ç‰¹çš„ç»Ÿè®¡ç‰¹å¾ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆå±•ç¤ºäº†å°æ³¢åŸŸåˆ†æåœ¨æ£€æµ‹ GAN å›¾åƒæ–¹é¢çš„ä¼˜è¶Šæ€§ï¼Œä¸ºå¼€å‘æ›´å¼ºå¤§çš„ Deepfake æ£€æµ‹ç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages; Published in IEEE",
      "pdf_url": "https://arxiv.org/pdf/2510.21822v1",
      "published_date": "2025-10-21 22:40:16 UTC",
      "updated_date": "2025-10-21 22:40:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:42:27.081850+00:00"
    },
    {
      "arxiv_id": "2510.19118v1",
      "title": "A Novel Approach to Breast Cancer Segmentation using U-Net Model with Attention Mechanisms and FedProx",
      "title_zh": "ä¸€ç§ç»“åˆæ³¨æ„åŠ›æœºåˆ¶ U-Net æ¨¡å‹ä¸ FedProx çš„ä¹³è…ºç™Œåˆ†å‰²æ–°æ–¹æ³•",
      "authors": [
        "Eyad Gad",
        "Mustafa Abou Khatwa",
        "Mustafa A. Elattar",
        "Sahar Selim"
      ],
      "abstract": "Breast cancer is a leading cause of death among women worldwide, emphasizing the need for early detection and accurate diagnosis. As such Ultrasound Imaging, a reliable and cost-effective tool, is used for this purpose, however the sensitive nature of medical data makes it challenging to develop accurate and private artificial intelligence models. A solution is Federated Learning as it is a promising technique for distributed machine learning on sensitive medical data while preserving patient privacy. However, training on non-Independent and non-Identically Distributed (non-IID) local datasets can impact the accuracy and generalization of the trained model, which is crucial for accurate tumour boundary delineation in BC segmentation. This study aims to tackle this challenge by applying the Federated Proximal (FedProx) method to non-IID Ultrasonic Breast Cancer Imaging datasets. Moreover, we focus on enhancing tumour segmentation accuracy by incorporating a modified U-Net model with attention mechanisms. Our approach resulted in a global model with 96% accuracy, demonstrating the effectiveness of our method in enhancing tumour segmentation accuracy while preserving patient privacy. Our findings suggest that FedProx has the potential to be a promising approach for training precise machine learning models on non-IID local medical datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¹³è…ºç™Œè¶…å£°å›¾åƒåˆ†å‰²ä¸­çš„æ•°æ®éšç§ä¿æŠ¤å’Œéç‹¬ç«‹åŒåˆ†å¸ƒ(non-IID)æ•°æ®æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå…ˆè¿›æ¨¡å‹æ¶æ„ä¸è”é‚¦å­¦ä¹ çš„æ–°å‹æ–¹æ¡ˆã€‚ç ”ç©¶çš„æ ¸å¿ƒé‡‡ç”¨äº†é›†æˆæ³¨æ„åŠ›æœºåˆ¶(Attention Mechanisms)çš„æ”¹è¿›U-Netæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å¢å¼ºç‰¹å¾è¡¨è¾¾èƒ½åŠ›æ¥ç²¾ç¡®å‹¾å‹’è‚¿ç˜¤è¾¹ç•Œã€‚ä¸ºäº†åœ¨åˆ†å¸ƒå¼å­¦ä¹ ç¯å¢ƒä¸‹æœ‰æ•ˆåº”å¯¹åŒ»ç–—æ•°æ®é›†çš„å¼‚è´¨æ€§ï¼Œè¯¥æ–¹æ¡ˆå¼•å…¥äº†FedProx(Federated Proximal)ç®—æ³•è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥å…¨å±€æ¨¡å‹åœ¨ç¡®ä¿æ‚£è€…éšç§çš„å‰æä¸‹ï¼Œå®ç°äº†é«˜è¾¾96%çš„åˆ†å‰²å‡†ç¡®ç‡ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚è¿™é¡¹ç ”ç©¶è¯æ˜äº†FedProxåœ¨å¤„ç†non-IIDåŒ»å­¦å½±åƒæ•°æ®æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºå¼€å‘é«˜æ€§èƒ½ä¸”å…·å¤‡éšç§ä¿æŠ¤èƒ½åŠ›çš„åŒ»ç–—AIæ¨¡å‹æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19118v1",
      "published_date": "2025-10-21 22:38:18 UTC",
      "updated_date": "2025-10-21 22:38:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:42:21.574149+00:00"
    },
    {
      "arxiv_id": "2510.19116v1",
      "title": "That's Deprecated! Understanding, Detecting, and Steering Knowledge Conflicts in Language Models for Code Generation",
      "title_zh": "é‚£å·²è¢«å¼ƒç”¨ï¼ç†è§£ã€æ£€æµ‹ä¸å¼•å¯¼ä»£ç ç”Ÿæˆè¯­è¨€æ¨¡å‹ä¸­çš„çŸ¥è¯†å†²çª",
      "authors": [
        "Jaesung Bae",
        "Cameron Churchwell",
        "Mitchell Hermon",
        "Tsun-An Hsieh",
        "Jocelyn Xu",
        "Yekaterina Yegorova",
        "Mark Hasegawa-Johnson",
        "Heng Ji"
      ],
      "abstract": "This paper investigates how large language models (LLMs) behave when faced with discrepancies between their parametric knowledge and conflicting information contained in a prompt. Building on prior question-answering (QA) research, we extend the investigation of knowledge conflicts to the realm of code generation. We propose a domain-agnostic framework for constructing and interpreting such conflicts, along with a novel evaluation method and dataset tailored to code conflict scenarios. Our experiments indicate that sufficiently large LLMs encode the notion of a knowledge conflict in their parameters, enabling us to detect knowledge conflicts with up to \\textbf{80.65\\%} accuracy. Building on these insights, we show that activation-level steering can achieve up to a \\textbf{12.6\\%} improvement in steering success over a random baseline. However, effectiveness depends critically on balancing model size, task domain, and steering direction. The experiment code and data will be made publicly available after acceptance.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶è°ƒæŸ¥äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†å…¶å†…éƒ¨å‚æ•°åŒ–çŸ¥è¯†ä¸æç¤ºè¯ä¸­å†²çªä¿¡æ¯æ—¶çš„è¡¨ç°ï¼Œé‡ç‚¹å°†çŸ¥è¯†å†²çª(Knowledge Conflicts)çš„ç ”ç©¶ä»é—®ç­”é¢†åŸŸæ‰©å±•åˆ°äº†ä»£ç ç”Ÿæˆé¢†åŸŸã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªç”¨äºæ„å»ºå’Œè§£é‡Šæ­¤ç±»å†²çªçš„é€šç”¨æ¡†æ¶ï¼Œå¹¶é’ˆå¯¹ä»£ç å†²çªåœºæ™¯è®¾è®¡äº†å…¨æ–°çš„è¯„ä¼°æ–¹æ³•å’Œæ•°æ®é›†ã€‚å®éªŒå‘ç°ï¼Œå¤§è§„æ¨¡LLMsçš„å‚æ•°ä¸­åŒ…å«å¯¹çŸ¥è¯†å†²çªçš„å†…åœ¨ç¼–ç ï¼Œèƒ½å¤Ÿä»¥æœ€é«˜80.65%çš„å‡†ç¡®ç‡æ£€æµ‹åˆ°å†²çªã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶è¯æ˜é€šè¿‡æ¿€æ´»å±‚çº§è½¬å‘(Activation-level Steering)å¯ä»¥å°†å¼•å¯¼æˆåŠŸçš„æ¦‚ç‡æ¯”éšæœºåŸºçº¿æé«˜12.6%ã€‚æœ€åï¼Œç ”ç©¶æŒ‡å‡ºè¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å…³é”®å–å†³äºæ¨¡å‹å¤§å°ã€ä»»åŠ¡é¢†åŸŸä»¥åŠè½¬å‘æ–¹å‘ä¹‹é—´çš„å¹³è¡¡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19116v1",
      "published_date": "2025-10-21 22:27:56 UTC",
      "updated_date": "2025-10-21 22:27:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:42:22.493003+00:00"
    },
    {
      "arxiv_id": "2510.19099v2",
      "title": "What Makes a Good Curriculum? Disentangling the Effects of Data Ordering on LLM Mathematical Reasoning",
      "title_zh": "ä½•ä¸ºä¼˜è´¨è¯¾ç¨‹ï¼Ÿå‰–ææ•°æ®æ’åºå¯¹å¤§è¯­è¨€æ¨¡å‹æ•°å­¦æ¨ç†çš„å½±å“",
      "authors": [
        "Yaning Jia",
        "Chunhui Zhang",
        "Xingjian Diao",
        "Xiangchi Yuan",
        "Zhongyu Ouyang",
        "Chiyu Ma",
        "Soroush Vosoughi"
      ],
      "abstract": "Curriculum learning (CL) - ordering training data from easy to hard - has become a popular strategy for improving reasoning in large language models (LLMs). Yet prior work employs disparate difficulty metrics and training setups, leaving open fundamental questions: When does curriculum help? Which direction - forward or reverse - is better? And does the answer depend on what we measure? We address these questions through a unified offline evaluation framework that decomposes curriculum difficulty into five complementary dimensions: Problem Difficulty, Model Surprisal, Confidence Margin, Predictive Uncertainty, and Decision Variability. Through controlled post-training experiments on mathematical reasoning benchmarks with Llama3.1-8B, Mistral-7B, and Gemma3-4B, we find that (i) no curriculum strategy dominates universally - the relative effectiveness of forward versus reverse CL depends jointly on model capability and task complexity; (ii) even within a single metric, samples at different difficulty levels produce distinct gains depending on task demands; and (iii) task-aligned curricula focus on shaping the model's final representations and generalization, whereas inner-state curricula modulate internal states such as confidence and uncertainty. Our findings challenge the notion of a universal curriculum strategy and offer actionable guidance across model and task regimes, with some metrics indicating that prioritizing decision-uncertain samples can further enhance learning outcomes.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†è¯¾ç¨‹å­¦ä¹ (Curriculum Learning)åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æ•°å­¦æ¨ç†èƒ½åŠ›ä¸­çš„ä½œç”¨ï¼Œå¹¶åˆ†æäº†æ•°æ®æ’åºå¯¹è®­ç»ƒæ•ˆæœçš„å¤æ‚å½±å“ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œä»Problem Difficultyã€Model Surprisalã€Confidence Marginã€Predictive Uncertaintyå’ŒDecision Variabilityäº”ä¸ªäº’è¡¥ç»´åº¦è§£æ„è¯¾ç¨‹éš¾åº¦ã€‚é€šè¿‡å¯¹Llama3.1-8Bã€Mistral-7Bå’ŒGemma3-4Bç­‰æ¨¡å‹çš„å®éªŒå‘ç°ï¼Œå¹¶ä¸å­˜åœ¨æ™®é€‚çš„æœ€ä¼˜è¯¾ç¨‹ç­–ç•¥ï¼Œå‰å‘ä¸åå‘è¯¾ç¨‹å­¦ä¹ çš„æœ‰æ•ˆæ€§é«˜åº¦ä¾èµ–äºæ¨¡å‹èƒ½åŠ›ä¸ä»»åŠ¡å¤æ‚åº¦çš„äº¤äº’ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†ä»»åŠ¡å¯¹é½å‹è¯¾ç¨‹ä¾§é‡äºå¡‘é€ æ¨¡å‹è¡¨å¾ï¼Œè€Œå†…éƒ¨çŠ¶æ€å‹è¯¾ç¨‹åˆ™ä¸»è¦è°ƒèŠ‚ç½®ä¿¡åº¦ä¸ä¸ç¡®å®šæ€§ã€‚è¯¥ç ”ç©¶æŒ‘æˆ˜äº†é€šç”¨è¯¾ç¨‹ç­–ç•¥çš„å‡è®¾ï¼Œå¹¶æŒ‡å‡ºä¼˜å…ˆå¤„ç†å†³ç­–ä¸ç¡®å®š(Decision-Uncertain)çš„æ ·æœ¬èƒ½è¿›ä¸€æ­¥å¢å¼ºå­¦ä¹ æ•ˆæœï¼Œä¸ºä¸åŒæ¨¡å‹å’Œä»»åŠ¡æœºåˆ¶ä¸‹çš„æ•°æ®è¯¾ç¨‹è®¾è®¡æä¾›äº†å®æ“æŒ‡å—ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages (main text) + 4 pages (appendix), 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.19099v2",
      "published_date": "2025-10-21 21:43:38 UTC",
      "updated_date": "2025-10-24 20:45:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:42:35.817201+00:00"
    },
    {
      "arxiv_id": "2510.21821v1",
      "title": "Prompt fidelity of ChatGPT4o / Dall-E3 text-to-image visualisations",
      "title_zh": "ChatGPT4o / Dall-E3 æ–‡ç”Ÿå›¾è§†è§‰å‘ˆç°çš„æç¤ºè¯å¿ å®åº¦",
      "authors": [
        "Dirk HR Spennemann"
      ],
      "abstract": "This study examines the prompt fidelity of ChatGPT4o / DALL-E3 text-to-image visualisations by analysing whether attributes explicitly specified in autogenously generated prompts are correctly rendered in the resulting images. Using two public-domain datasets comprising 200 visualisations of women working in the cultural and creative industries and 230 visualisations of museum curators, the study assessed accuracy across personal attributes (age, hair), appearance (attire, glasses), and paraphernalia (name tags, clipboards). While correctly rendered in most cases, DALL-E3 deviated from prompt specifications in 15.6% of all attributes (n=710). Errors were lowest for paraphernalia, moderate for personal appearance, and highest for depictions of the person themselves, particularly age. These findings demonstrate measurable prompt-to-image fidelity gaps with implications for bias detection and model evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº† ChatGPT4o / DALL-E3 åœ¨æ–‡æœ¬ç”Ÿæˆå›¾åƒè¿‡ç¨‹ä¸­çš„æç¤ºè¯å¿ å®åº¦ (Prompt Fidelity)ï¼Œé€šè¿‡åˆ†æè‡ªåŠ¨ç”Ÿæˆçš„æç¤ºè¯ä¸­æ˜ç¡®æŒ‡å®šçš„å±æ€§æ˜¯å¦åœ¨æœ€ç»ˆå›¾åƒä¸­å¾—åˆ°å‡†ç¡®å‘ˆç°ã€‚ç ”ç©¶ä½¿ç”¨äº†åŒ…å«åˆ›æ„äº§ä¸šä»ä¸šå¥³æ€§å’Œåšç‰©é¦†ç­–å±•äººçš„ä¸¤ç»„å…¬å…±é¢†åŸŸæ•°æ®é›†ï¼Œå…±è®¡åˆ†æäº† 430 å¼ è§†è§‰å›¾åƒã€‚è¯„ä¼°æ¶µç›–äº†ä¸ªäººå±æ€§ï¼ˆå¹´é¾„ã€å¤´å‘ï¼‰ã€å¤–è²Œï¼ˆç€è£…ã€çœ¼é•œï¼‰ä»¥åŠéšèº«ç‰©å“ï¼ˆåç‰Œã€å‰ªè´´æ¿ï¼‰ç­‰å…³é”®ç»´åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶å¤§å¤šæ•°å±æ€§å¾—åˆ°äº†æ­£ç¡®æ¸²æŸ“ï¼Œä½† DALL-E3 åœ¨ 15.6% çš„å±æ€§æŒ‡æ ‡ä¸Šåç¦»äº†æç¤ºè¯è§„èŒƒã€‚ç ”ç©¶å‘ç°è¯¯å·®ç‡åœ¨éšèº«ç‰©å“ç±»åˆ«ä¸­æœ€ä½ï¼Œè€Œåœ¨äººç‰©å±æ€§ï¼ˆç‰¹åˆ«æ˜¯å¹´é¾„ï¼‰çš„æè¿°ä¸­æœ€é«˜ã€‚è¯¥å‘ç°æ­ç¤ºäº†æç¤ºè¯ä¸ç”Ÿæˆå›¾åƒä¹‹é—´å­˜åœ¨å¯é‡åŒ–çš„å¿ å®åº¦å·®è·ï¼Œå¯¹åè§æ£€æµ‹ (Bias Detection) å’Œæ¨¡å‹è¯„ä¼° (Model Evaluation) å…·æœ‰é‡è¦çš„ç†è®ºä¸å®è·µæ„ä¹‰ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.21821v1",
      "published_date": "2025-10-21 21:33:09 UTC",
      "updated_date": "2025-10-21 21:33:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:42:31.897775+00:00"
    },
    {
      "arxiv_id": "2510.19072v3",
      "title": "Local Guidance for Configuration-Based Multi-Agent Pathfinding",
      "title_zh": "åŸºäºé…ç½®çš„å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’å±€éƒ¨å¼•å¯¼",
      "authors": [
        "Tomoki Arita",
        "Keisuke Okumura"
      ],
      "abstract": "Guidance is an emerging concept that improves the empirical performance of real-time, sub-optimal multi-agent pathfinding (MAPF) methods. It offers additional information to MAPF algorithms to mitigate congestion on a global scale by considering the collective behavior of all agents across the entire workspace. This global perspective helps reduce agents' waiting times, thereby improving overall coordination efficiency. In contrast, this study explores an alternative approach: providing local guidance in the vicinity of each agent. While such localized methods involve recomputation as agents move and may appear computationally demanding, we empirically demonstrate that supplying informative spatiotemporal cues to the planner can significantly improve solution quality without exceeding a moderate time budget. When applied to LaCAM, a leading configuration-based solver, this form of guidance establishes a new performance frontier for MAPF.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’ (Multi-Agent Pathfinding, MAPF) çš„ä¸€ç§æ›¿ä»£å¼•å¯¼æ–¹æ³•ï¼Œå³åœ¨æ¯ä¸ªæ™ºèƒ½ä½“é‚»åŸŸå†…æä¾›å±€éƒ¨å¼•å¯¼ (Local Guidance)ã€‚ä¸ä¾§é‡äºå…¨å±€å·¥ä½œç©ºé—´åä½œè¡Œä¸ºçš„ä¼ ç»Ÿå¼•å¯¼æ–¹å¼ä¸åŒï¼Œè¯¥æ–¹æ³•åœ¨æ™ºèƒ½ä½“ç§»åŠ¨è¿‡ç¨‹ä¸­åŠ¨æ€æä¾›ä¿¡æ¯ä¸°å¯Œçš„æ—¶ç©ºçº¿ç´¢ (Spatiotemporal Cues) ä»¥è¾…åŠ©è§„åˆ’å™¨å†³ç­–ã€‚å°½ç®¡å±€éƒ¨åŒ–æ–¹æ³•æ¶‰åŠå®æ—¶é‡æ–°è®¡ç®—ï¼Œä½†å®éªŒç»“æœè¡¨æ˜ï¼Œå®ƒèƒ½åœ¨ä¸æ˜¾è‘—å¢åŠ è®¡ç®—è´Ÿæ‹…çš„æƒ…å†µä¸‹å¤§å¹…æå‡è·¯å¾„è§£çš„è´¨é‡ã€‚é€šè¿‡å°†è¯¥å¼•å¯¼æœºåˆ¶åº”ç”¨äºé¢†å…ˆçš„åŸºäºé…ç½®çš„æ±‚è§£å™¨ LaCAMï¼Œç ”ç©¶æˆåŠŸç¡®ç«‹äº† MAPF çš„æ–°æ€§èƒ½è¾¹ç•Œ (Performance Frontier)ã€‚è¿™ä¸€æˆæœè¯æ˜äº†å±€éƒ¨å¼•å¯¼åœ¨ä¼˜åŒ–å¤šæ™ºèƒ½ä½“åè°ƒæ•ˆç‡å’Œç¼“è§£æ‹¥å µæ–¹é¢çš„å“è¶Šæ•ˆèƒ½ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.MA",
      "comment": "To be presented at AAAI-26",
      "pdf_url": "https://arxiv.org/pdf/2510.19072v3",
      "published_date": "2025-10-21 20:51:32 UTC",
      "updated_date": "2025-11-15 18:47:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:42:35.576330+00:00"
    },
    {
      "arxiv_id": "2510.23619v1",
      "title": "Short Ticketing Detection Framework Analysis Report",
      "title_zh": "çŸ­é€”ç¥¨æ£€æµ‹æ¡†æ¶åˆ†ææŠ¥å‘Š",
      "authors": [
        "Yuyang Miao",
        "Huijun Xing",
        "Danilo P. Mandic",
        "Tony G. Constantinides"
      ],
      "abstract": "This report presents a comprehensive analysis of an unsupervised multi-expert machine learning framework for detecting short ticketing fraud in railway systems. The study introduces an A/B/C/D station classification system that successfully identifies suspicious patterns across 30 high-risk stations. The framework employs four complementary algorithms: Isolation Forest, Local Outlier Factor, One-Class SVM, and Mahalanobis Distance. Key findings include the identification of five distinct short ticketing patterns and potential for short ticketing recovery in transportation systems.",
      "tldr_zh": "è¯¥æŠ¥å‘Šåˆ†æäº†ä¸€ç§ç”¨äºæ£€æµ‹é“è·¯ç³»ç»ŸçŸ­é€”é€ƒç¥¨ (Short Ticketing) æ¬ºè¯ˆçš„æ— ç›‘ç£å¤šä¸“å®¶æœºå™¨å­¦ä¹ æ¡†æ¶ã€‚ç ”ç©¶å¼•å…¥äº† A/B/C/D ç«™ç‚¹åˆ†ç±»ç³»ç»Ÿï¼Œå¹¶åœ¨ 30 ä¸ªé«˜é£é™©ç«™ç‚¹ä¸­æˆåŠŸè¯†åˆ«å‡ºå¼‚å¸¸æ¨¡å¼ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†å››ç§äº’è¡¥çš„ç®—æ³•è¿›è¡Œæ£€æµ‹ï¼ŒåŒ…æ‹¬ Isolation Forestã€Local Outlier Factorã€One-Class SVM ä»¥åŠ Mahalanobis Distanceã€‚å…³é”®ç ”ç©¶å‘ç°åŒ…æ‹¬è¯†åˆ«å‡ºäº”ç§æˆªç„¶ä¸åŒçš„çŸ­é€”é€ƒç¥¨ (Short Ticketing) è¡Œä¸ºæ¨¡å¼ã€‚æœ€ç»ˆç»“æœè¯æ˜äº†è¯¥æ¡†æ¶åœ¨äº¤é€šç³»ç»ŸçŸ­é€”é€ƒç¥¨å›æ”¶ (Short Ticketing Recovery) æ–¹é¢çš„åº”ç”¨æ½œåŠ›å’Œæœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.23619v1",
      "published_date": "2025-10-21 20:50:48 UTC",
      "updated_date": "2025-10-21 20:50:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:42:36.494888+00:00"
    },
    {
      "arxiv_id": "2510.19060v2",
      "title": "PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions",
      "title_zh": "PoShï¼šåˆ©ç”¨åœºæ™¯å›¾å¼•å¯¼ LLMs-as-a-Judge è¿›è¡Œè¯¦ç»†å›¾åƒæè¿°è¯„æµ‹",
      "authors": [
        "Amith Ananthram",
        "Elias Stengel-Eskin",
        "Lorena A. Bradford",
        "Julia Demarest",
        "Adam Purvis",
        "Keith Krut",
        "Robert Stein",
        "Rina Elster Pantalony",
        "Mohit Bansal",
        "Kathleen McKeown"
      ],
      "abstract": "While vision-language models (VLMs) have advanced into detailed image description, evaluation remains a challenge. Standard metrics (e.g. CIDEr, SPICE) were designed for short texts and tuned to recognize errors that are now uncommon, such as object misidentification. In contrast, long texts require sensitivity to attribute and relation attachments and scores that localize errors to particular text spans. In this work, we introduce PoSh, a metric for detailed image description that uses scene graphs as structured rubrics to guide LLMs-as-a-Judge, producing aggregate scores grounded in fine-grained errors (e.g. mistakes in compositional understanding). PoSh is replicable, interpretable and a better proxy for human raters than existing metrics (including GPT4o-as-a-Judge). To validate PoSh, we introduce a challenging new dataset, DOCENT. This novel benchmark contains artwork, paired with expert-written references, and model-generated descriptions, augmented with granular and coarse judgments of their quality from art history students. Thus, DOCENT enables evaluating both detailed image description metrics and detailed image description itself in a challenging new domain. We show that PoSh achieves stronger correlations (+0.05 Spearman $Ï$) with the human judgments in DOCENT than the best open-weight alternatives, is robust to image type (using CapArena, an existing dataset of web imagery) and is a capable reward function, outperforming standard supervised fine-tuning. Then, using PoSh, we characterize the performance of open and closed models in describing the paintings, sketches and statues in DOCENT and find that foundation models struggle to achieve full, error-free coverage of images with rich scene dynamics, establishing a demanding new task to gauge VLM progress. Through both PoSh and DOCENT, we hope to enable advances in important areas such as assistive text generation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰è¯„ä»·æŒ‡æ ‡å¦‚CIDErå’ŒSPICEåœ¨è¯„ä¼°è¯¦ç»†å›¾åƒæè¿°æ—¶éš¾ä»¥æ•æ‰å±æ€§ä¸å…³ç³»é”™è¯¯çš„é—®é¢˜ï¼Œæå‡ºäº†å…¨æ–°çš„è¯„ä»·æŒ‡æ ‡PoShã€‚PoShé€šè¿‡å¼•å…¥Scene Graphsä½œä¸ºç»“æ„åŒ–å‡†åˆ™æ¥å¼•å¯¼LLMs-as-a-Judgeï¼Œèƒ½å¤Ÿç”ŸæˆåŸºäºç»„åˆç†è§£ç­‰ç»†ç²’åº¦é”™è¯¯çš„ç»¼åˆè¯„åˆ†ï¼Œå¹¶æä¾›æ›´å…·è§£é‡Šæ€§çš„è¯„ä¼°ç»“æœã€‚ä¸ºäº†éªŒè¯å…¶æœ‰æ•ˆæ€§ï¼Œä½œè€…æ„å»ºäº†åŒ…å«è‰ºæœ¯å“å›¾åƒåŠä¸“å®¶è¯„è¿°çš„DOCENTæ•°æ®é›†ï¼Œå¹¶ç”±è‰ºæœ¯å²ä¸“ä¸šå­¦ç”Ÿæä¾›è´¨é‡åˆ¤å®šåŸºå‡†ã€‚å®éªŒè¯æ˜ï¼ŒPoShä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§ä¼˜äºåŒ…æ‹¬GPT-4oåœ¨å†…çš„ç°æœ‰æŒ‡æ ‡ï¼Œä¸”èƒ½ä½œä¸ºæœ‰æ•ˆçš„Reward Functionæ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚ç ”ç©¶æœ€ååˆ©ç”¨PoShåˆ†æå‘ç°ï¼Œç°æœ‰çš„åŸºç¡€æ¨¡å‹åœ¨å¤„ç†å…·æœ‰ä¸°å¯Œåœºæ™¯åŠ¨æ€çš„å¤æ‚å›¾åƒæ—¶ä»å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œä¸ºè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„è¿›é˜¶å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "26 pages, 9 figures. Metric/benchmark available at https://github.com/amith-ananthram/posh",
      "pdf_url": "https://arxiv.org/pdf/2510.19060v2",
      "published_date": "2025-10-21 20:30:20 UTC",
      "updated_date": "2025-12-05 21:57:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:42:49.180403+00:00"
    },
    {
      "arxiv_id": "2510.19055v1",
      "title": "The MUSE Benchmark: Probing Music Perception and Auditory Relational Reasoning in Audio LLMS",
      "title_zh": "MUSE åŸºå‡†ï¼šæ¢ç©¶éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹ä¸­çš„éŸ³ä¹æ„ŸçŸ¥ä¸å¬è§‰å…³ç³»æ¨ç†",
      "authors": [
        "Brandon James Carone",
        "Iran R. Roman",
        "Pablo RipollÃ©s"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated capabilities in audio understanding, but current evaluations may obscure fundamental weaknesses in relational reasoning. We introduce the Music Understanding and Structural Evaluation (MUSE) Benchmark, an open-source resource with 10 tasks designed to probe fundamental music perception skills. We evaluate four SOTA models (Gemini Pro and Flash, Qwen2.5-Omni, and Audio-Flamingo 3) against a large human baseline (N=200). Our results reveal a wide variance in SOTA capabilities and a persistent gap with human experts. While Gemini Pro succeeds on basic perception, Qwen and Audio Flamingo 3 perform at or near chance, exposing severe perceptual deficits. Furthermore, we find Chain-of-Thought (CoT) prompting provides inconsistent, often detrimental results. Our work provides a critical tool for evaluating invariant musical representations and driving development of more robust AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†MUSE Benchmark (Music Understanding and Structural Evaluation)ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«10é¡¹ä»»åŠ¡çš„å¼€æºåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) çš„åŸºç¡€éŸ³ä¹æ„ŸçŸ¥å’Œå¬è§‰å…³ç³»æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶è€…å¯¹æ¯”äº†Gemini Proã€Qwen2.5-Omniç­‰å››ç§é¡¶å°–æ¨¡å‹ä¸200åäººç±»åŸºå‡†çš„è¡¨ç°ï¼Œæ­ç¤ºäº†æ¨¡å‹èƒ½åŠ›ä¹‹é—´å­˜åœ¨çš„å·¨å¤§å·®å¼‚ã€‚å®éªŒå‘ç°ï¼Œå°½ç®¡Gemini Proåœ¨åŸºç¡€æ„ŸçŸ¥ä»»åŠ¡ä¸­è¡¨ç°å°šå¯ï¼Œä½†Qwenå’ŒAudio Flamingo 3ç­‰æ¨¡å‹è¡¨ç°æ¥è¿‘éšæœºæ°´å¹³ï¼Œåæ˜ å‡ºä¸¥é‡çš„æ„ŸçŸ¥ç¼ºé™·ä¸”ä¸äººç±»ä¸“å®¶å­˜åœ¨æ˜¾è‘—å·®è·ã€‚æ­¤å¤–ï¼Œé“¾å¼æ€ç»´ (Chain-of-Thought) æç¤ºåœ¨è¯¥ä»»åŠ¡ä¸­è¡¨ç°ä¸ç¨³å®šç”šè‡³é€‚å¾—å…¶åã€‚è¯¥å·¥ä½œä¸ºè¯„ä¼°å’Œæ¨åŠ¨æ›´é²æ£’çš„éŸ³é¢‘AIç³»ç»Ÿå‘å±•æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages, 2 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.19055v1",
      "published_date": "2025-10-21 20:14:36 UTC",
      "updated_date": "2025-10-21 20:14:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:42:51.291047+00:00"
    },
    {
      "arxiv_id": "2510.21820v1",
      "title": "Unlocking Biomedical Insights: Hierarchical Attention Networks for High-Dimensional Data Interpretation",
      "title_zh": "æ­ç¤ºç”Ÿç‰©åŒ»å­¦æ´å¯Ÿï¼šé¢å‘é«˜ç»´æ•°æ®è§£è¯»çš„å±‚çº§æ³¨æ„åŠ›ç½‘ç»œ",
      "authors": [
        "Rekha R Nair",
        "Tina Babu",
        "Alavikunhu Panthakkan",
        "Hussain Al-Ahmad",
        "Balamurugan Balusamy"
      ],
      "abstract": "The proliferation of high-dimensional datasets in fields such as genomics, healthcare, and finance has created an urgent need for machine learning models that are both highly accurate and inherently interpretable. While traditional deep learning approaches deliver strong predictive performance, their lack of transparency often impedes their deployment in critical, decision-sensitive applications. In this work, we introduce the Hierarchical Attention-based Interpretable Network (HAIN), a novel architecture that unifies multi-level attention mechanisms, dimensionality reduction, and explanation-driven loss functions to deliver interpretable and robust analysis of complex biomedical data. HAIN provides feature-level interpretability via gradientweighted attention and offers global model explanations through prototype-based representations. Comprehensive evaluation on The Cancer Genome Atlas (TCGA) dataset demonstrates that HAIN achieves a classification accuracy of 94.3%, surpassing conventional post-hoc interpretability approaches such as SHAP and LIME in both transparency and explanatory power. Furthermore, HAIN effectively identifies biologically relevant cancer biomarkers, supporting its utility for clinical and research applications. By harmonizing predictive accuracy with interpretability, HAIN advances the development of transparent AI solutions for precision medicine and regulatory compliance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HAIN (Hierarchical Attention-based Interpretable Network)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³é«˜ç»´ç”Ÿç‰©åŒ»å­¦æ•°æ®åœ¨æ·±åº¦å­¦ä¹ åº”ç”¨ä¸­ç¼ºä¹é€æ˜åº¦é—®é¢˜çš„æ–°å‹æ¶æ„ã€‚HAIN ç»Ÿä¸€äº†å¤šçº§ Attention æœºåˆ¶ã€é™ç»´æŠ€æœ¯ä»¥åŠè§£é‡Šé©±åŠ¨çš„ Loss functionsï¼Œä»è€Œå¯¹å¤æ‚çš„ç”Ÿç‰©åŒ»å­¦æ•°æ®è¿›è¡Œå¯è§£é‡Šä¸”ç¨³å¥çš„åˆ†æã€‚è¯¥æ¨¡å‹é€šè¿‡ Gradient-weighted attention æä¾›ç‰¹å¾çº§è§£é‡Šï¼Œå¹¶åˆ©ç”¨ Prototype-based representations æä¾›å…¨å±€æ¨¡å‹è¯´æ˜ã€‚åœ¨ TCGA æ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼ŒHAIN çš„åˆ†ç±»å‡†ç¡®ç‡è¾¾åˆ° 94.3%ï¼Œåœ¨é€æ˜åº¦å’Œè§£é‡ŠåŠ›æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿçš„ SHAP å’Œ LIME ç­‰ Post-hoc interpretability æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒHAIN èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å…·æœ‰ç”Ÿç‰©å­¦æ„ä¹‰çš„ Cancer biomarkersï¼Œä¸ºå…¶åœ¨ä¸´åºŠå’Œç ”ç©¶åº”ç”¨ä¸­çš„ä»·å€¼æä¾›äº†æ”¯æŒã€‚é€šè¿‡å¹³è¡¡é¢„æµ‹å‡†ç¡®æ€§ä¸å¯è§£é‡Šæ€§ï¼ŒHAIN ä¸ºç²¾å‡†åŒ»å­¦å’Œåˆè§„ç›‘ç®¡é¢†åŸŸçš„é€æ˜ AI è§£å†³æ–¹æ¡ˆæä¾›äº†æœ‰åŠ›è¿›å±•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.21820v1",
      "published_date": "2025-10-21 20:08:50 UTC",
      "updated_date": "2025-10-21 20:08:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:42:53.089576+00:00"
    },
    {
      "arxiv_id": "2510.19050v1",
      "title": "Rectifying Shortcut Behaviors in Preference-based Reward Learning",
      "title_zh": "çº æ­£åŸºäºåå¥½çš„å¥–åŠ±å­¦ä¹ ä¸­çš„æ·å¾„è¡Œä¸º",
      "authors": [
        "Wenqian Ye",
        "Guangtao Zheng",
        "Aidong Zhang"
      ],
      "abstract": "In reinforcement learning from human feedback, preference-based reward models play a central role in aligning large language models to human-aligned behavior. However, recent studies show that these models are prone to reward hacking and often fail to generalize well due to over-optimization. They achieve high reward scores by exploiting shortcuts, that is, exploiting spurious features (e.g., response verbosity, agreeable tone, or sycophancy) that correlate with human preference labels in the training data rather than genuinely reflecting the intended objectives. In this paper, instead of probing these issues one at a time, we take a broader view of the reward hacking problem as shortcut behaviors and introduce a principled yet flexible approach to mitigate shortcut behaviors in preference-based reward learning. Inspired by the invariant theory in the kernel perspective, we propose Preference-based Reward Invariance for Shortcut Mitigation (PRISM), which learns group-invariant kernels with feature maps in a closed-form learning objective. Experimental results in several benchmarks show that our method consistently improves the accuracy of the reward model on diverse out-of-distribution tasks and reduces the dependency on shortcuts in downstream policy models, establishing a robust framework for preference-based alignment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºåå¥½çš„å¥–åŠ±æ¨¡å‹(preference-based reward models)åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å®¹æ˜“åˆ©ç”¨æ·å¾„(shortcuts)â€”â€”å¦‚å›å¤é•¿åº¦ã€è¯­æ°”æˆ–è°„åªšè¡Œä¸ºâ€”â€”è€ŒéçœŸå®ç›®æ ‡æ¥è·å–é«˜åˆ†ï¼Œä»è€Œå¯¼è‡´å¥–åŠ±ç¯¡æ”¹(reward hacking)å’Œæ³›åŒ–å¤±æ•ˆçš„é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åä¸º PRISM (Preference-based Reward Invariance for Shortcut Mitigation) çš„åŸåˆ™æ€§æ–¹æ³•ï¼Œæ—¨åœ¨åå¥½å­¦ä¹ ä¸­ç¼“è§£è¿™äº›æ·å¾„è¡Œä¸ºã€‚è¯¥æ–¹æ³•å—å†…æ ¸è§†è§’ä¸‹çš„ä¸å˜æ€§ç†è®ºå¯å‘ï¼Œé€šè¿‡é—­å¼å­¦ä¹ ç›®æ ‡æ„å»ºç»„ä¸å˜å†…æ ¸(group-invariant kernels)ï¼Œä»¥æå–æ›´å…·é²æ£’æ€§çš„ç‰¹å¾æ˜ å°„ã€‚å®éªŒè¯æ˜ï¼ŒPRISM èƒ½å¤Ÿæ˜¾è‘—æå‡å¥–åŠ±æ¨¡å‹åœ¨åˆ†å¸ƒå¤–(out-of-distribution)ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§ï¼Œå¹¶é™ä½ä¸‹æ¸¸ç­–ç•¥æ¨¡å‹å¯¹æ·å¾„çš„ä¾èµ–ã€‚è¯¥æ¡†æ¶ä¸ºæ„å»ºç¨³å¥çš„åå¥½å¯¹é½(preference-based alignment)æœºåˆ¶æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æŒä¸å®è·µæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.19050v1",
      "published_date": "2025-10-21 20:08:32 UTC",
      "updated_date": "2025-10-21 20:08:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:42:59.684999+00:00"
    },
    {
      "arxiv_id": "2510.19048v1",
      "title": "REPAIR Approach for Social-based City Reconstruction Planning in case of natural disasters",
      "title_zh": "REPAIRï¼šé¢å‘è‡ªç„¶ç¾å®³çš„ç¤¾ä¼šå¯¼å‘å‹åŸå¸‚é‡å»ºè§„åˆ’æ–¹æ³•",
      "authors": [
        "Ghulam Mudassir",
        "Antinisca Di Marco",
        "Giordano d'Aloisio"
      ],
      "abstract": "Natural disasters always have several effects on human lives. It is challenging for governments to tackle these incidents and to rebuild the economic, social and physical infrastructures and facilities with the available resources (mainly budget and time). Governments always define plans and policies according to the law and political strategies that should maximise social benefits. The severity of damage and the vast resources needed to bring life back to normality make such reconstruction a challenge. This article is the extension of our previously published work by conducting comprehensive comparative analysis by integrating additional deep learning models plus random agent which is used as a baseline. Our prior research introduced a decision support system by using the Deep Reinforcement Learning technique for the planning of post-disaster city reconstruction, maximizing the social benefit of the reconstruction process, considering available resources, meeting the needs of the broad community stakeholders (like citizens' social benefits and politicians' priorities) and keeping in consideration city's structural constraints (like dependencies among roads and buildings). The proposed approach, named post disaster REbuilding plAn ProvIdeR (REPAIR) is generic. It can determine a set of alternative plans for local administrators who select the ideal one to implement, and it can be applied to areas of any extension. We show the application of REPAIR in a real use case, i.e., to the L'Aquila reconstruction process, damaged in 2009 by a major earthquake.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†åä¸º REPAIRï¼ˆpost disaster REbuilding plAn ProvIdeRï¼‰çš„é€šç”¨æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ Deep Reinforcement Learning æŠ€æœ¯ä¼˜åŒ–è‡ªç„¶ç¾å®³åçš„åŸå¸‚é‡å»ºè§„åˆ’ã€‚è¯¥ç³»ç»Ÿä½œä¸ºä¸€ä¸ªå†³ç­–æ”¯æŒå·¥å…·ï¼Œåœ¨è€ƒè™‘é¢„ç®—å’Œæ—¶é—´ç­‰èµ„æºé™åˆ¶çš„åŒæ—¶ï¼Œè‡´åŠ›äºæœ€å¤§åŒ–ç¤¾ä¼šæ•ˆç›Šï¼Œå¹¶å…¼é¡¾å…¬æ°‘éœ€æ±‚ã€æ”¿æ²»ä¼˜å…ˆçº§ä»¥åŠåŸå¸‚åŸºç¡€è®¾æ–½é—´çš„ç»“æ„çº¦æŸã€‚æœ¬æ–‡åœ¨å…ˆå‰ç ”ç©¶çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡æ•´åˆå¤šç§ Deep Learning æ¨¡å‹å’ŒéšæœºåŸºçº¿æ™ºèƒ½ä½“è¿›è¡Œäº†å…¨é¢çš„å¯¹æ¯”åˆ†æã€‚REPAIR æ¡†æ¶èƒ½å¤Ÿä¸ºåœ°æ–¹ç®¡ç†äººå‘˜æä¾›ä¸€ç³»åˆ—å¤‡é€‰æ–¹æ¡ˆï¼Œä»¥ä¾›å…¶æ ¹æ®å®é™…æƒ…å†µé€‰æ‹©æœ€ä¼˜æ‰§è¡Œè®¡åˆ’ã€‚ç ”ç©¶é€šè¿‡ 2009 å¹´æ„å¤§åˆ©æ‹‰å¥æ‹‰ï¼ˆL'Aquilaï¼‰å¤§åœ°éœ‡é‡å»ºçš„çœŸå®æ¡ˆä¾‹å±•ç¤ºäº†è¯¥æ–¹æ³•çš„åº”ç”¨æ•ˆæœï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚åŸå¸‚é‡å»ºå†³ç­–ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted at International Journal of Data Science and Analytics",
      "pdf_url": "https://arxiv.org/pdf/2510.19048v1",
      "published_date": "2025-10-21 20:05:18 UTC",
      "updated_date": "2025-10-21 20:05:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:43:04.289057+00:00"
    },
    {
      "arxiv_id": "2510.19033v1",
      "title": "\"Over-the-Hood\" AI Inclusivity Bugs and How 3 AI Product Teams Found and Fixed Them",
      "title_zh": "â€œå¼•æ“ç›–ä¹‹ä¸Šâ€çš„ AI åŒ…å®¹æ€§ç¼ºé™·ï¼šä¸‰æ”¯ AI äº§å“å›¢é˜Ÿçš„å‘ç°ä¸ä¿®å¤å®è·µ",
      "authors": [
        "Andrew Anderson",
        "Fatima A. Moussaoui",
        "Jimena Noa Guevara",
        "Md Montaser Hamid",
        "Margaret Burnett"
      ],
      "abstract": "While much research has shown the presence of AI's \"under-the-hood\" biases (e.g., algorithmic, training data, etc.), what about \"over-the-hood\" inclusivity biases: barriers in user-facing AI products that disproportionately exclude users with certain problem-solving approaches? Recent research has begun to report the existence of such biases -- but what do they look like, how prevalent are they, and how can developers find and fix them? To find out, we conducted a field study with 3 AI product teams, to investigate what kinds of AI inclusivity bugs exist uniquely in user-facing AI products, and whether/how AI product teams might harness an existing (non-AI-oriented) inclusive design method to find and fix them. The teams' work resulted in identifying 6 types of AI inclusivity bugs arising 83 times, fixes covering 47 of these bug instances, and a new variation of the GenderMag inclusive design method, GenderMag-for-AI, that is especially effective at detecting certain kinds of AI inclusivity bugs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†AIäº§å“ä¸­é™¤äº†ç®—æ³•å’Œæ•°æ®ç­‰â€œunder-the-hoodâ€åå·®å¤–çš„â€œover-the-hoodâ€åŒ…å®¹æ€§ç¼ºé™·ï¼Œå³é‚£äº›ä¸æˆæ¯”ä¾‹åœ°æ’æ–¥å…·æœ‰ç‰¹å®šé—®é¢˜è§£å†³æ–¹å¼çš„ç”¨æˆ·çš„ç•Œé¢å±éšœã€‚ç ”ç©¶äººå‘˜å¯¹ä¸‰ä¸ªAIäº§å“å›¢é˜Ÿè¿›è¡Œäº†å®åœ°ç ”ç©¶(field study)ï¼Œè°ƒæŸ¥äº†ç”¨æˆ·ç«¯AIäº§å“ä¸­ç‰¹æœ‰çš„åŒ…å®¹æ€§ç¼ºé™·ç±»å‹åŠå…¶å‘ç°ä¸ä¿®å¤æ–¹æ³•ã€‚ç ”ç©¶å…±è¯†åˆ«å‡º6ç§ç±»å‹çš„AIåŒ…å®¹æ€§ç¼ºé™·(AI inclusivity bugs)ï¼Œç´¯è®¡å‡ºç°83æ¬¡ï¼Œå¹¶é’ˆå¯¹å…¶ä¸­çš„47ä¸ªå®ä¾‹å®æ–½äº†ä¿®å¤æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶é€šè¿‡æ”¹è¿›ç°æœ‰çš„GenderMagåŒ…å®¹æ€§è®¾è®¡æ–¹æ³•ï¼Œæå‡ºå¹¶éªŒè¯äº†ä¸“é—¨é’ˆå¯¹AIäº§å“çš„GenderMag-for-AIæ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼ŒGenderMag-for-AIåœ¨æ£€æµ‹ç‰¹å®šç±»å‹çš„AIåŒ…å®¹æ€§ç¼ºé™·æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä¸ºAIäº§å“å¼€å‘å›¢é˜Ÿæå‡äº¤äº’å±‚é¢çš„åŒ…å®¹æ€§æä¾›äº†å®ç”¨çš„å·¥å…·å’ŒæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19033v1",
      "published_date": "2025-10-21 19:24:12 UTC",
      "updated_date": "2025-10-21 19:24:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:43:04.787682+00:00"
    },
    {
      "arxiv_id": "2510.19031v1",
      "title": "CLiVR: Conversational Learning System in Virtual Reality with AI-Powered Patients",
      "title_zh": "CLiVRï¼šåŸºäº AI é©±åŠ¨æ‚£è€…çš„è™šæ‹Ÿç°å®å¯¹è¯å¼å­¦ä¹ ç³»ç»Ÿ",
      "authors": [
        "Akilan Amithasagaran",
        "Sagnik Dakshit",
        "Bhavani Suryadevara",
        "Lindsey Stockton"
      ],
      "abstract": "Simulations constitute a fundamental component of medical and nursing education and traditionally employ standardized patients (SP) and high-fidelity manikins to develop clinical reasoning and communication skills. However, these methods require substantial resources, limiting accessibility and scalability. In this study, we introduce CLiVR, a Conversational Learning system in Virtual Reality that integrates large language models (LLMs), speech processing, and 3D avatars to simulate realistic doctor-patient interactions. Developed in Unity and deployed on the Meta Quest 3 platform, CLiVR enables trainees to engage in natural dialogue with virtual patients. Each simulation is dynamically generated from a syndrome-symptom database and enhanced with sentiment analysis to provide feedback on communication tone. Through an expert user study involving medical school faculty (n=13), we assessed usability, realism, and perceived educational impact. Results demonstrated strong user acceptance, high confidence in educational potential, and valuable feedback for improvement. CLiVR offers a scalable, immersive supplement to SP-based training.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†CLiVRï¼Œä¸€ä¸ªé›†æˆå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ã€è¯­éŸ³å¤„ç†å’Œ3DåŒ–èº«çš„è™šæ‹Ÿç°å®å¯¹è¯å­¦ä¹ ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³åŒ»å­¦æ•™è‚²ä¸­ä¼ ç»Ÿæ¨¡æ‹Ÿç—…äºº(Standardized Patients)å’Œé«˜ä¿çœŸæ¨¡æ‹Ÿäºº(High-fidelity Manikins)èµ„æºæ¶ˆè€—å¤§ä¸”éš¾ä»¥æ‰©å±•çš„é—®é¢˜ã€‚è¯¥ç³»ç»ŸåŸºäºUnityå¼€å‘å¹¶éƒ¨ç½²äºMeta Quest 3å¹³å°ï¼Œä½¿å—è®­è€…èƒ½å¤Ÿä¸è™šæ‹Ÿç—…äººè¿›è¡Œè‡ªç„¶å¯¹è¯ã€‚é€šè¿‡ç»¼åˆç—‡-ç—‡çŠ¶æ•°æ®åº“(Syndrome-symptom database)åŠ¨æ€ç”Ÿæˆæ¨¡æ‹Ÿæƒ…æ™¯ï¼Œå¹¶ç»“åˆæƒ…æ„Ÿåˆ†æ(Sentiment analysis)å¯¹æ²Ÿé€šè¯­æ°”æä¾›åé¦ˆï¼Œä»¥æå‡ä¸´åºŠæ¨ç†ä¸æ²Ÿé€šæŠ€å·§ã€‚é’ˆå¯¹13ååŒ»å­¦é™¢æ•™èŒå‘˜å·¥çš„ä¸“å®¶ç ”ç©¶è¡¨æ˜ï¼ŒCLiVRåœ¨æ˜“ç”¨æ€§ã€çœŸå®æ„Ÿå’Œæ•™è‚²æ½œåŠ›æ–¹é¢è·å¾—äº†é«˜åº¦è®¤å¯ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥ç³»ç»Ÿä¸ºåŸºäºSPçš„åŒ»å­¦åŸ¹è®­æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”å…·å¤‡æ²‰æµ¸æ„Ÿçš„æœ‰æ•ˆè¡¥å……æ‰‹æ®µã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19031v1",
      "published_date": "2025-10-21 19:19:55 UTC",
      "updated_date": "2025-10-21 19:19:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:43:06.595205+00:00"
    },
    {
      "arxiv_id": "2510.19025v1",
      "title": "FlexiDataGen: An Adaptive LLM Framework for Dynamic Semantic Dataset Generation in Sensitive Domains",
      "title_zh": "FlexiDataGenï¼šé¢å‘æ•æ„Ÿé¢†åŸŸçš„åŠ¨æ€è¯­ä¹‰æ•°æ®é›†ç”Ÿæˆè‡ªé€‚åº”å¤§è¯­è¨€æ¨¡å‹æ¡†æ¶",
      "authors": [
        "Hamed Jelodar",
        "Samita Bai",
        "Roozbeh Razavi-Far",
        "Ali A. Ghorbani"
      ],
      "abstract": "Dataset availability and quality remain critical challenges in machine learning, especially in domains where data are scarce, expensive to acquire, or constrained by privacy regulations. Fields such as healthcare, biomedical research, and cybersecurity frequently encounter high data acquisition costs, limited access to annotated data, and the rarity or sensitivity of key events. These issues-collectively referred to as the dataset challenge-hinder the development of accurate and generalizable machine learning models in such high-stakes domains. To address this, we introduce FlexiDataGen, an adaptive large language model (LLM) framework designed for dynamic semantic dataset generation in sensitive domains. FlexiDataGen autonomously synthesizes rich, semantically coherent, and linguistically diverse datasets tailored to specialized fields. The framework integrates four core components: (1) syntactic-semantic analysis, (2) retrieval-augmented generation, (3) dynamic element injection, and (4) iterative paraphrasing with semantic validation. Together, these components ensure the generation of high-quality, domain-relevant data. Experimental results show that FlexiDataGen effectively alleviates data shortages and annotation bottlenecks, enabling scalable and accurate machine learning model development.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—ã€ç”Ÿç‰©åŒ»å­¦å’Œç½‘ç»œå®‰å…¨ç­‰æ•æ„Ÿé¢†åŸŸä¸­æ•°æ®ç¨€ç¼ºã€è·å–æˆæœ¬é«˜ä»¥åŠéšç§ç›‘ç®¡é™åˆ¶ç­‰å¸¦æ¥çš„æ•°æ®é›†æŒ‘æˆ˜(dataset challenge)ï¼Œæå‡ºäº†FlexiDataGenæ¡†æ¶ã€‚FlexiDataGenæ˜¯ä¸€ä¸ªè‡ªé€‚åº”çš„å¤§è¯­è¨€æ¨¡å‹(LLM)æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºåœ¨æ•æ„Ÿé¢†åŸŸè‡ªä¸»åˆæˆè¯­ä¹‰è¿è´¯ä¸”å…·æœ‰è¯­è¨€å¤šæ ·æ€§çš„åŠ¨æ€æ•°æ®é›†ã€‚è¯¥æ¡†æ¶é›†æˆäº†å¥æ³•è¯­ä¹‰åˆ†æ(syntactic-semantic analysis)ã€æ£€ç´¢å¢å¼ºç”Ÿæˆ(retrieval-augmented generation)ã€åŠ¨æ€å…ƒç´ æ³¨å…¥(dynamic element injection)ä»¥åŠå¸¦æœ‰è¯­ä¹‰éªŒè¯çš„è¿­ä»£æ”¹å†™(iterative paraphrasing with semantic validation)å››ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚é€šè¿‡è¿™äº›ç»„ä»¶çš„åä½œï¼Œè¯¥ç³»ç»Ÿç¡®ä¿äº†ç”Ÿæˆæ•°æ®çš„é«˜è´¨é‡å’Œé¢†åŸŸç›¸å…³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlexiDataGenèƒ½å¤Ÿæœ‰æ•ˆç¼“è§£æ•°æ®çŸ­ç¼ºå’Œæ ‡æ³¨ç“¶é¢ˆï¼Œä¸ºé«˜é£é™©é¢†åŸŸä¸­å¼€å‘å¯æ‰©å±•ä¸”å‡†ç¡®çš„æœºå™¨å­¦ä¹ æ¨¡å‹æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19025v1",
      "published_date": "2025-10-21 19:07:49 UTC",
      "updated_date": "2025-10-21 19:07:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:43:18.078799+00:00"
    },
    {
      "arxiv_id": "2510.19014v1",
      "title": "Prior-informed optimization of treatment recommendation via bandit algorithms trained on large language model-processed historical records",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹å¤„ç†çš„å†å²è®°å½•è®­ç»ƒå¤šè‡‚è€è™æœºç®—æ³•ï¼Œå®ç°å…ˆéªŒä¿¡æ¯é©±åŠ¨çš„æ²»ç–—æ¨èä¼˜åŒ–",
      "authors": [
        "Saman Nessari",
        "Ali Bozorgi-Amiri"
      ],
      "abstract": "Current medical practice depends on standardized treatment frameworks and empirical methodologies that neglect individual patient variations, leading to suboptimal health outcomes. We develop a comprehensive system integrating Large Language Models (LLMs), Conditional Tabular Generative Adversarial Networks (CTGAN), T-learner counterfactual models, and contextual bandit approaches to provide customized, data-informed clinical recommendations. The approach utilizes LLMs to process unstructured medical narratives into structured datasets (93.2% accuracy), uses CTGANs to produce realistic synthetic patient data (55% accuracy via two-sample verification), deploys T-learners to forecast patient-specific treatment responses (84.3% accuracy), and integrates prior-informed contextual bandits to enhance online therapeutic selection by effectively balancing exploration of new possibilities with exploitation of existing knowledge. Testing on stage III colon cancer datasets revealed that our KernelUCB approach obtained 0.60-0.61 average reward scores across 5,000 rounds, exceeding other reference methods. This comprehensive system overcomes cold-start limitations in online learning environments, improves computational effectiveness, and constitutes notable progress toward individualized medicine adapted to specific patient characteristics.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªç»¼åˆç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆå¤šç§å…ˆè¿›ç®—æ³•ä¼˜åŒ–æ²»ç–—å»ºè®®ï¼Œè§£å†³ä¼ ç»ŸåŒ»ç–—å®è·µå¿½è§†ä¸ªä½“å·®å¼‚çš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨Large Language Models (LLMs)å°†éç»“æ„åŒ–åŒ»ç–—å™è¿°è½¬åŒ–ä¸ºå‡†ç¡®ç‡ä¸º93.2%çš„ç»“æ„åŒ–æ•°æ®é›†ï¼Œå¹¶åº”ç”¨Conditional Tabular Generative Adversarial Networks (CTGAN)ç”ŸæˆçœŸå®çš„åˆæˆæ‚£è€…æ•°æ®ã€‚ç ”ç©¶é‡‡ç”¨T-learneråäº‹å®æ¨¡å‹é¢„æµ‹ä¸ªä½“åŒ–æ²»ç–—ååº”ï¼Œå¹¶é›†æˆå…ˆéªŒçŸ¥è¯†é©±åŠ¨çš„Contextual Banditç®—æ³•ï¼Œä»¥æœ‰æ•ˆå¹³è¡¡æ–°ç–—æ³•æ¢ç´¢ä¸å·²æœ‰çŸ¥è¯†åˆ©ç”¨ã€‚åœ¨ä¸‰æœŸç»“è‚ ç™Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¶KernelUCBæ–¹æ³•åœ¨5,000è½®æµ‹è¯•ä¸­è·å¾—äº†0.60-0.61çš„å¹³å‡å¥–åŠ±å¾—åˆ†ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–åŸºå‡†æ–¹æ³•ã€‚è¯¥ç³»ç»ŸæˆåŠŸå…‹æœäº†åœ¨çº¿å­¦ä¹ ç¯å¢ƒä¸­çš„å†·å¯åŠ¨(Cold-start)é™åˆ¶å¹¶æé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œæ˜¯å‘é’ˆå¯¹æ‚£è€…å…·ä½“ç‰¹å¾çš„ä¸ªä½“åŒ–åŒ»ç–—(Individualized Medicine)è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19014v1",
      "published_date": "2025-10-21 18:57:00 UTC",
      "updated_date": "2025-10-21 18:57:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:43:15.880039+00:00"
    },
    {
      "arxiv_id": "2510.19008v1",
      "title": "Plural Voices, Single Agent: Towards Inclusive AI in Multi-User Domestic Spaces",
      "title_zh": "ä¼—å£°ä¸€ä½“ï¼šé¢å‘å¤šç”¨æˆ·å±…å®¶ç©ºé—´çš„åŒ…å®¹æ€§äººå·¥æ™ºèƒ½",
      "authors": [
        "Joydeep Chandra",
        "Satyam Kumar Navneet"
      ],
      "abstract": "Domestic AI agents faces ethical, autonomy, and inclusion challenges, particularly for overlooked groups like children, elderly, and Neurodivergent users. We present the Plural Voices Model (PVM), a novel single-agent framework that dynamically negotiates multi-user needs through real-time value alignment, leveraging diverse public datasets on mental health, eldercare, education, and moral reasoning. Using human+synthetic curriculum design with fairness-aware scenarios and ethical enhancements, PVM identifies core values, conflicts, and accessibility requirements to inform inclusive principles. Our privacy-focused prototype features adaptive safety scaffolds, tailored interactions (e.g., step-by-step guidance for Neurodivergent users, simple wording for children), and equitable conflict resolution. In preliminary evaluations, PVM outperforms multi-agent baselines in compliance (76% vs. 70%), fairness (90% vs. 85%), safety-violation rate (0% vs. 7%), and latency. Design innovations, including video guidance, autonomy sliders, family hubs, and adaptive safety dashboards, demonstrate new directions for ethical and inclusive domestic AI, for building user-centered agentic systems in plural domestic contexts. Our Codes and Model are been open sourced, available for reproduction: https://github.com/zade90/Agora",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å®¶åº­AIæ™ºèƒ½ä½“åœ¨åŒ…å®¹æ€§ã€è‡ªä¸»æ€§å’Œä¼¦ç†æ–¹é¢çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å„¿ç«¥ã€è€å¹´äººå’Œç¥ç»å¤šæ ·æ€§(Neurodivergent)ç”¨æˆ·ç­‰å¼±åŠ¿ç¾¤ä½“ï¼Œæå‡ºäº†Plural Voices Model (PVM)æ¡†æ¶ã€‚ä½œä¸ºä¸€ä¸ªå•æ™ºèƒ½ä½“æ¡†æ¶ï¼ŒPVMé€šè¿‡å®æ—¶ä»·å€¼å¯¹é½(real-time value alignment)åŠ¨æ€åå•†å¤šç”¨æˆ·éœ€æ±‚ï¼Œå¹¶åˆ©ç”¨æ¶‰åŠå¿ƒç†å¥åº·ã€å…»è€å’Œé“å¾·æ¨ç†çš„å¤šæ ·åŒ–æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚è¯¥æ¨¡å‹é‡‡ç”¨äººç±»+åˆæˆè¯¾ç¨‹è®¾è®¡(human+synthetic curriculum design)å’Œå…¬å¹³æ„ŸçŸ¥åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«æ ¸å¿ƒä»·å€¼å†²çªå¹¶æä¾›ä¸ªæ€§åŒ–äº¤äº’ï¼Œä¾‹å¦‚ä¸ºç¥ç»å¤šæ ·æ€§ç”¨æˆ·æä¾›åˆ†æ­¥æŒ‡å¯¼ã€‚åˆæ­¥è¯„ä¼°è¡¨æ˜ï¼ŒPVMåœ¨åˆè§„æ€§(76%)ã€å…¬å¹³æ€§(90%)å’Œå®‰å…¨æ€§(0%è¿è§„ç‡)ç­‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºå¤šæ™ºèƒ½ä½“åŸºå‡†æ¨¡å‹ï¼Œä¸”å…·æœ‰æ›´ä½çš„å»¶è¿Ÿã€‚é€šè¿‡å¼•å…¥è§†é¢‘å¼•å¯¼ã€è‡ªä¸»æ€§æ»‘å—(autonomy sliders)å’Œå®¶åº­ä¸­å¿ƒ(family hubs)ç­‰åˆ›æ–°è®¾è®¡ï¼Œè¯¥ç ”ç©¶ä¸ºåœ¨å¤šç”¨æˆ·å®¶åº­ç¯å¢ƒä¸­æ„å»ºä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒã€å…·å¤‡ä¼¦ç†åŒ…å®¹æ€§çš„æ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19008v1",
      "published_date": "2025-10-21 18:48:26 UTC",
      "updated_date": "2025-10-21 18:48:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:43:16.388164+00:00"
    },
    {
      "arxiv_id": "2510.19003v1",
      "title": "$Î”$t-Mamba3D: A Time-Aware Spatio-Temporal State-Space Model for Breast Cancer Risk Prediction",
      "title_zh": "$Î”$t-Mamba3Dï¼šä¸€ç§ç”¨äºä¹³è…ºç™Œé£é™©é¢„æµ‹çš„æ—¶é—´æ„ŸçŸ¥å‹æ—¶ç©ºçŠ¶æ€ç©ºé—´æ¨¡å‹",
      "authors": [
        "Zhengbo Zhou",
        "Dooman Arefan",
        "Margarita Zuley",
        "Shandong Wu"
      ],
      "abstract": "Longitudinal analysis of sequential radiological images is hampered by a fundamental data challenge: how to effectively model a sequence of high-resolution images captured at irregular time intervals. This data structure contains indispensable spatial and temporal cues that current methods fail to fully exploit. Models often compromise by either collapsing spatial information into vectors or applying spatio-temporal models that are computationally inefficient and incompatible with non-uniform time steps. We address this challenge with Time-Aware $Î”$t-Mamba3D, a novel state-space architecture adapted for longitudinal medical imaging. Our model simultaneously encodes irregular inter-visit intervals and rich spatio-temporal context while remaining computationally efficient. Its core innovation is a continuous-time selective scanning mechanism that explicitly integrates the true time difference between exams into its state transitions. This is complemented by a multi-scale 3D neighborhood fusion module that robustly captures spatio-temporal relationships. In a comprehensive breast cancer risk prediction benchmark using sequential screening mammogram exams, our model shows superior performance, improving the validation c-index by 2-5 percentage points and achieving higher 1-5 year AUC scores compared to established variants of recurrent, transformer, and state-space models. Thanks to its linear complexity, the model can efficiently process long and complex patient screening histories of mammograms, forming a new framework for longitudinal image analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜åˆ†è¾¨ç‡æ”¾å°„å½±åƒåºåˆ—åœ¨ä¸è§„åˆ™æ—¶é—´é—´éš”ä¸‹çš„å»ºæ¨¡éš¾é¢˜ï¼Œæå‡ºäº† $\\Delta$t-Mamba3Dï¼Œä¸€ç§ä¸“ä¸ºçºµå‘åŒ»å­¦å½±åƒè®¾è®¡çš„æ—¶ç©ºçŠ¶æ€ç©ºé—´æ¨¡å‹ (State-Space Model)ã€‚è¯¥æ¶æ„çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†è¿ç»­æ—¶é—´é€‰æ‹©æ€§æ‰«ææœºåˆ¶ (Continuous-time selective scanning mechanism)ï¼Œèƒ½å¤Ÿå°†ä¸¤æ¬¡æ£€æŸ¥ä¹‹é—´çš„çœŸå®æ—¶é—´å·®æ˜¾å¼æ•´åˆåˆ°çŠ¶æ€è½¬ç§»è¿‡ç¨‹ä¸­ï¼Œæœ‰æ•ˆå¤„ç†ä¸å‡åŒ€çš„æ—¶é—´æ­¥é•¿ã€‚æ­¤å¤–ï¼Œæ¨¡å‹ç»“åˆäº†å¤šå°ºåº¦ 3D é‚»åŸŸèåˆæ¨¡å— (Multi-scale 3D neighborhood fusion module)ï¼Œä»¥ç¨³å¥åœ°æ•æ‰å¤æ‚çš„æ—¶ç©ºå…³ç³»ã€‚åœ¨ä¹³è…ºç™Œé£é™©é¢„æµ‹çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨åºè´¯ç­›æŸ¥ä¹³è…º X çº¿æ‘„å½± (Mammogram) æ•°æ®ï¼Œå°†éªŒè¯é›† C-index æå‡äº† 2-5 ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶åœ¨ 1-5 å¹´ AUC åˆ†æ•°ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ Recurrentã€Transformer å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹å˜ä½“ã€‚å‡­å€Ÿå…¶çº¿æ€§å¤æ‚åº¦ (Linear complexity)ï¼Œ$\\Delta$t-Mamba3D èƒ½å¤Ÿé«˜æ•ˆå¤„ç†æ‚£è€…é•¿æœŸçš„ç­›æŸ¥å†å²ï¼Œä¸ºçºµå‘å½±åƒåˆ†ææä¾›äº†é«˜æ•ˆä¸”ç²¾å‡†çš„æ–°æ¡†æ¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19003v1",
      "published_date": "2025-10-21 18:29:17 UTC",
      "updated_date": "2025-10-21 18:29:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:43:32.480694+00:00"
    },
    {
      "arxiv_id": "2510.19001v1",
      "title": "Robust Driving QA through Metadata-Grounded Context and Task-Specific Prompts",
      "title_zh": "åŸºäºå…ƒæ•°æ®å…³è”ä¸Šä¸‹æ–‡ä¸ä»»åŠ¡ç‰¹å®šæç¤ºçš„é²æ£’é©¾é©¶é—®ç­”",
      "authors": [
        "Seungjun Yu",
        "Junsung Park",
        "Youngsun Lim",
        "Hyunjung Shim"
      ],
      "abstract": "We present a two-phase vision-language QA system for autonomous driving that answers high-level perception, prediction, and planning questions. In Phase-1, a large multimodal LLM (Qwen2.5-VL-32B) is conditioned on six-camera inputs, a short temporal window of history, and a chain-of-thought prompt with few-shot exemplars. A self-consistency ensemble (multiple sampled reasoning chains) further improves answer reliability. In Phase-2, we augment the prompt with nuScenes scene metadata (object annotations, ego-vehicle state, etc.) and category-specific question instructions (separate prompts for perception, prediction, planning tasks). In experiments on a driving QA benchmark, our approach significantly outperforms the baseline Qwen2.5 models. For example, using 5 history frames and 10-shot prompting in Phase-1 yields 65.1% overall accuracy (vs.62.61% with zero-shot); applying self-consistency raises this to 66.85%. Phase-2 achieves 67.37% overall. Notably, the system maintains 96% accuracy under severe visual corruption. These results demonstrate that carefully engineered prompts and contextual grounding can greatly enhance high-level driving QA with pretrained vision-language models.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è§†è§‰è¯­è¨€é—®ç­”ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­çš„é«˜çº§æ„ŸçŸ¥ã€é¢„æµ‹å’Œè§„åˆ’é—®é¢˜ã€‚ç¬¬ä¸€é˜¶æ®µåŸºäº Qwen2.5-VL-32B æ¨¡å‹ï¼Œç»“åˆå…­æ‘„åƒå¤´è¾“å…¥å’Œå†å²æ—¶é—´çª—å£ï¼Œåˆ©ç”¨ Chain-of-Thought æç¤ºã€few-shot ç¤ºä¾‹ä»¥åŠ self-consistency ensemble æŠ€æœ¯æ¥æå‡ç­”æ¡ˆçš„å¯é æ€§ã€‚ç¬¬äºŒé˜¶æ®µé€šè¿‡å¼•å…¥ nuScenes åœºæ™¯å…ƒæ•°æ®å’Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æŒ‡ä»¤è¿›ä¸€æ­¥å¢å¼ºæç¤ºæ•ˆæœã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é©¾é©¶ QA åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œæ•´ä½“å‡†ç¡®ç‡è¾¾åˆ° 67.37%ã€‚å³ä¾¿åœ¨ä¸¥é‡çš„è§†è§‰æŸåç¯å¢ƒä¸‹ï¼Œè¯¥ç³»ç»Ÿä»èƒ½ç»´æŒ 96% çš„å‡†ç¡®ç‡ã€‚ç ”ç©¶ç»“æœè¯æ˜äº†é€šè¿‡ç²¾ç»†çš„æç¤ºå·¥ç¨‹å’Œ contextual grounding æŠ€æœ¯ï¼Œå¯ä»¥å¤§å¹…æå‡é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚é©¾é©¶åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19001v1",
      "published_date": "2025-10-21 18:24:59 UTC",
      "updated_date": "2025-10-21 18:24:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:43:38.671384+00:00"
    },
    {
      "arxiv_id": "2510.18999v1",
      "title": "$\\nabla$-SDF: Learning Euclidean Signed Distance Functions Online with Gradient-Augmented Octree Interpolation and Neural Residual",
      "title_zh": "$\\nabla$-SDFï¼šåŸºäºæ¢¯åº¦å¢å¼ºå…«å‰æ ‘æ’å€¼ä¸ç¥ç»æ®‹å·®çš„æ¬§å‡ é‡Œå¾—ç¬¦å·è·ç¦»å‡½æ•°åœ¨çº¿å­¦ä¹ ",
      "authors": [
        "Zhirui Dai",
        "Qihao Qian",
        "Tianxing Fan",
        "Nikolay Atanasov"
      ],
      "abstract": "Estimation of signed distance functions (SDFs) from point cloud data has been shown to benefit many robot autonomy capabilities, including localization, mapping, motion planning, and control. Methods that support online and large-scale SDF reconstruction tend to rely on discrete volumetric data structures, which affect the continuity and differentiability of the SDF estimates. Recently, using implicit features, neural network methods have demonstrated high-fidelity and differentiable SDF reconstruction but they tend to be less efficient, can experience catastrophic forgetting and memory limitations in large environments, and are often restricted to truncated SDFs. This work proposes $\\nabla$-SDF, a hybrid method that combines an explicit prior obtained from gradient-augmented octree interpolation with an implicit neural residual. Our method achieves non-truncated (Euclidean) SDF reconstruction with computational and memory efficiency comparable to volumetric methods and differentiability and accuracy comparable to neural network methods. Extensive experiments demonstrate that \\methodname{} outperforms the state of the art in terms of accuracy and efficiency, providing a scalable solution for downstream tasks in robotics and computer vision.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººè‡ªä¸»èƒ½åŠ›ä¸­ Signed Distance Functions (SDF) çš„åœ¨çº¿å¤§è§„æ¨¡é‡å»ºé—®é¢˜ï¼ŒæŒ‡å‡ºäº†ä¼ ç»Ÿä½“ç´ æ–¹æ³•åœ¨è¿ç»­æ€§å’Œå¯å¾®æ€§ä¸Šçš„ä¸è¶³ï¼Œä»¥åŠçº¯ç¥ç»æ–¹æ³•åœ¨æ•ˆç‡ã€ç¾éš¾æ€§é—å¿˜å’Œç¯å¢ƒè§„æ¨¡æ–¹é¢çš„å±€é™ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº† $\\nabla$-SDFï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†åŸºäº Gradient-augmented octree interpolation çš„æ˜¾å¼å…ˆéªŒä¸ Implicit neural residual çš„æ··åˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•å®ç°äº†éæˆªæ–­çš„ Euclidean SDF é‡å»ºï¼Œåœ¨è®¡ç®—å’Œå†…å­˜æ•ˆç‡ä¸Šå¯ä¸ä½“ç´ æ–¹æ³•åª²ç¾ï¼ŒåŒæ—¶å…·å¤‡ç¥ç»ç½‘ç»œæ–¹æ³•çš„é«˜ç²¾åº¦å’Œå¯å¾®æ€§ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œ$\\nabla$-SDF åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯ï¼Œä¸ºæœºå™¨äººå’Œè®¡ç®—æœºè§†è§‰çš„ä¸‹æ¸¸ä»»åŠ¡æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18999v1",
      "published_date": "2025-10-21 18:24:45 UTC",
      "updated_date": "2025-10-21 18:24:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:43:34.690263+00:00"
    },
    {
      "arxiv_id": "2510.20222v1",
      "title": "QKCV Attention: Enhancing Time Series Forecasting with Static Categorical Embeddings for Both Lightweight and Pre-trained Foundation Models",
      "title_zh": "QKCV æ³¨æ„åŠ›ï¼šåˆ©ç”¨é™æ€ç±»åˆ«åµŒå…¥å¢å¼ºè½»é‡çº§ä¸é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹çš„æ—¶é—´åºåˆ—é¢„æµ‹",
      "authors": [
        "Hao Wang",
        "Baojun Ma"
      ],
      "abstract": "In real-world time series forecasting tasks, category information plays a pivotal role in capturing inherent data patterns. This paper introduces QKCV (Query-Key-Category-Value) attention, an extension of the traditional QKV framework that incorporates a static categorical embedding C to emphasize category-specific information. As a versatile plug-in module, QKCV enhances the forecasting accuracy of attention-based models (e.g., Vanilla Transformer, Informer, PatchTST, TFT) across diverse real-world datasets. Furthermore, QKCV demonstrates remarkable adaptability in fine-tuning univariate time series foundation model by solely updating the static embedding C while preserving pretrained weights, thereby reducing computational overhead and achieving superior fine-tuning performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† QKCV (Query-Key-Category-Value) æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½œä¸ºä¼ ç»Ÿ QKV æ¡†æ¶çš„æ‰©å±•ï¼Œé€šè¿‡å¼•å…¥é™æ€ç±»åˆ«åµŒå…¥ C æ¥å¼ºåŒ–æ—¶é—´åºåˆ—é¢„æµ‹ä¸­å…³é”®çš„ç±»åˆ«ç‰¹å®šä¿¡æ¯ã€‚QKCV è®¾è®¡ä¸ºä¸€ä¸ªé€šç”¨çš„æ’ä»¶æ¨¡å—ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ Vanilla Transformerã€Informerã€PatchTST å’Œ TFT ç­‰å¤šç§ä¸»æµæ¨¡å‹åœ¨ç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„é¢„æµ‹å‡†ç¡®åº¦ã€‚åœ¨åŸºç¡€æ¨¡å‹ (foundation model) çš„åº”ç”¨åœºæ™¯ä¸­ï¼Œè¯¥æœºåˆ¶æ”¯æŒä»…é€šè¿‡æ›´æ–°é™æ€åµŒå…¥ C è€Œä¿æŒé¢„è®­ç»ƒæƒé‡ä¸å˜çš„æ–¹å¼è¿›è¡Œé«˜æ•ˆå¾®è°ƒã€‚è¿™ç§æ–¹æ³•ä¸ä»…å¤§å¹…é™ä½äº†è®¡ç®—å¼€é”€ï¼Œè¿˜åœ¨å•å˜é‡æ—¶é—´åºåˆ—ä»»åŠ¡ä¸­å®ç°äº†ä¼˜äºä¼ ç»Ÿæ–¹æ³•çš„å¾®è°ƒæ€§èƒ½ã€‚æ€»çš„æ¥çœ‹ï¼ŒQKCV ä¸ºè½»é‡åŒ–æ¨¡å‹å’Œå¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹åœ¨å¤æ‚æ—¶é—´åºåˆ—æ¨¡å¼æ•æ‰æ–¹é¢æä¾›äº†ä¸€ç§çµæ´»ä¸”é«˜æ•ˆçš„å¢å¼ºæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.20222v1",
      "published_date": "2025-10-21 18:15:21 UTC",
      "updated_date": "2025-10-21 18:15:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:43:37.880718+00:00"
    },
    {
      "arxiv_id": "2510.18988v4",
      "title": "Timely Clinical Diagnosis through Active Test Selection",
      "title_zh": "åŸºäºä¸»åŠ¨æµ‹è¯•é€‰æ‹©çš„åŠæ—¶ä¸´åºŠè¯Šæ–­",
      "authors": [
        "Silas Ruhrberg EstÃ©vez",
        "NicolÃ¡s Astorga",
        "Mihaela van der Schaar"
      ],
      "abstract": "There is growing interest in using machine learning (ML) to support clinical diagnosis, but most approaches rely on static, fully observed datasets and fail to reflect the sequential, resource-aware reasoning clinicians use in practice. Diagnosis remains complex and error prone, especially in high-pressure or resource-limited settings, underscoring the need for frameworks that help clinicians make timely and cost-effective decisions. We propose ACTMED (Adaptive Clinical Test selection via Model-based Experimental Design), a diagnostic framework that integrates Bayesian Experimental Design (BED) with large language models (LLMs) to better emulate real-world diagnostic reasoning. At each step, ACTMED selects the test expected to yield the greatest reduction in diagnostic uncertainty for a given patient. LLMs act as flexible simulators, generating plausible patient state distributions and supporting belief updates without requiring structured, task-specific training data. Clinicians can remain in the loop; reviewing test suggestions, interpreting intermediate outputs, and applying clinical judgment throughout. We evaluate ACTMED on real-world datasets and show it can optimize test selection to improve diagnostic accuracy, interpretability, and resource use. This represents a step toward transparent, adaptive, and clinician-aligned diagnostic systems that generalize across settings with reduced reliance on domain-specific data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿæœºå™¨å­¦ä¹ åœ¨ä¸´åºŠè¯Šæ–­ä¸­ä¾èµ–é™æ€æ•°æ®é›†ã€æ— æ³•åæ˜ åŒ»ç”Ÿåºåˆ—åŒ–ä¸”èµ„æºæ„ŸçŸ¥æ¨ç†è¿‡ç¨‹çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º ACTMED (Adaptive Clinical Test selection via Model-based Experimental Design) çš„è¯Šæ–­æ¡†æ¶ã€‚ACTMED å°†è´å¶æ–¯å®éªŒè®¾è®¡ (Bayesian Experimental Design, BED) ä¸å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) ç›¸ç»“åˆï¼Œæ—¨åœ¨æ›´å¥½åœ°æ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„è¯Šæ–­é€»è¾‘ã€‚åœ¨è¯Šæ–­çš„æ¯ä¸ªæ­¥éª¤ä¸­ï¼Œè¯¥æ¡†æ¶èƒ½è‡ªåŠ¨é€‰æ‹©é¢„æœŸæœ€èƒ½å‡å°‘ç‰¹å®šæ‚£è€…è¯Šæ–­ä¸ç¡®å®šæ€§ (Diagnostic Uncertainty) çš„æ£€æµ‹é¡¹ç›®ã€‚LLMs åœ¨å…¶ä¸­å……å½“çµæ´»çš„æ¨¡æ‹Ÿå™¨ï¼Œç”Ÿæˆå¯èƒ½çš„æ‚£è€…çŠ¶æ€åˆ†å¸ƒï¼Œå¹¶åœ¨æ— éœ€ç»“æ„åŒ–ã€ç‰¹å®šä»»åŠ¡è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹æ”¯æŒä¿¡å¿µæ›´æ–° (Belief Updates)ã€‚ä¸´åºŠåŒ»ç”Ÿå¯ä»¥å…¨ç¨‹å‚ä¸å…¶ä¸­ï¼Œé€šè¿‡å®¡æŸ¥å»ºè®®å’Œè§£é‡Šä¸­é—´è¾“å‡ºæ¥åº”ç”¨ä¸´åºŠåˆ¤æ–­ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒACTMED èƒ½å¤Ÿä¼˜åŒ–æ£€æµ‹é€‰æ‹©ï¼Œæ˜¾è‘—æé«˜è¯Šæ–­å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§åŠèµ„æºåˆ©ç”¨ç‡ã€‚è¿™ä¸€æˆæœä¸ºæ„å»ºé€æ˜ã€è‡ªé€‚åº”ä¸”ä¸ä¸´åºŠåŒ»ç”Ÿå¯¹é½çš„é€šç”¨è¯Šæ–­ç³»ç»Ÿè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18988v4",
      "published_date": "2025-10-21 18:10:45 UTC",
      "updated_date": "2025-11-16 23:13:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:43:43.376548+00:00"
    },
    {
      "arxiv_id": "2510.19851v1",
      "title": "Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability",
      "title_zh": "æ¨ç†æ¨¡å‹èƒ½å¦éšåŒ¿å…¶æ¨ç†è¿‡ç¨‹ï¼Ÿæ€ç»´é“¾å¯ç›‘æµ‹æ€§çš„å‹åŠ›æµ‹è¯•",
      "authors": [
        "Artur Zolkowski",
        "Wen Xing",
        "David Lindner",
        "Florian TramÃ¨r",
        "Erik Jenner"
      ],
      "abstract": "Recent findings suggest that misaligned models may exhibit deceptive behavior, raising concerns about output trustworthiness. Chain-of-thought (CoT) is a promising tool for alignment monitoring: when models articulate their reasoning faithfully, monitors can detect and mitigate harmful behaviors before undesirable outcomes occur. However, a key uncertainty is: Can models obfuscate their CoT in order to pursue hidden adversarial objectives while evading detection? To answer this question and thus stress-test CoT monitorability, we develop a composable and quantifiable taxonomy of prompts to elicit CoT obfuscation. We evaluate both internal CoT (reasoning traces) and external CoT (prompted reasoning in outputs) using toy tasks and more realistic environments in SHADE-Arena. We show that: (i) CoT monitoring performs accurately and efficiently without obfuscation pressure. (ii) Under strong obfuscation pressure, some models successfully complete adversarial tasks while evading detection. (iii) Models do not obfuscate their internal CoT as much as their external CoT (under prompt pressure). These results suggest that while CoT provides valuable oversight in benign settings, robust deployment requires model-specific stress-testing of monitorability.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å‹åŠ›æµ‹è¯•æ¢è®¨äº†æ¨ç†æ¨¡å‹æ˜¯å¦ä¼šæ··æ·†å…¶æ€ç»´é“¾(Chain-of-Thought)ä»¥éšè—å¯¹æŠ—æ€§ç›®æ ‡å¹¶è§„é¿å¯¹é½ç›‘æµ‹ã€‚ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€å¥—å¯é‡åŒ–çš„æç¤ºåˆ†ç±»æ³•æ¥è¯±å¯¼æ€ç»´é“¾æ··æ·†(CoT obfuscation)ï¼Œå¹¶åœ¨SHADE-Arenaç­‰å®éªŒç¯å¢ƒä¸­è¯„ä¼°äº†æ¨¡å‹å†…éƒ¨æ¨ç†è½¨è¿¹ä¸å¤–éƒ¨è¾“å‡ºçš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶æ€ç»´é“¾ç›‘æµ‹åœ¨å¸¸è§„åœºæ™¯ä¸‹è¡¨ç°é«˜æ•ˆï¼Œä½†åœ¨å¼ºå‹åŠ›ä¸‹ï¼Œéƒ¨åˆ†æ¨¡å‹èƒ½æˆåŠŸåœ¨è§„é¿æ£€æµ‹çš„åŒæ—¶å®Œæˆå¯¹æŠ—ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°æ¨¡å‹å¯¹å†…éƒ¨æ€ç»´é“¾çš„æ··æ·†ç¨‹åº¦æ™®éä½äºå—æç¤ºå‹åŠ›å½±å“çš„å¤–éƒ¨æ€ç»´é“¾ã€‚è¯¥é¡¹å·¥ä½œå¼ºè°ƒäº†å°½ç®¡æ€ç»´é“¾åœ¨è‰¯æ€§ç¯å¢ƒä¸­å…·æœ‰ç›‘ç®¡ä»·å€¼ï¼Œä½†è¦å®ç°é²æ£’éƒ¨ç½²ï¼Œä»å¿…é¡»é’ˆå¯¹å…·ä½“æ¨¡å‹çš„å¯ç›‘æµ‹æ€§è¿›è¡Œä¸¥æ ¼çš„å‹åŠ›æµ‹è¯•ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19851v1",
      "published_date": "2025-10-21 18:07:10 UTC",
      "updated_date": "2025-10-21 18:07:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:43:52.590334+00:00"
    },
    {
      "arxiv_id": "2510.18982v1",
      "title": "Test-time Verification via Optimal Transport: Coverage, ROC, & Sub-optimality",
      "title_zh": "åŸºäºæœ€ä¼˜ä¼ è¾“çš„æµ‹è¯•æ—¶éªŒè¯ï¼šè¦†ç›–ç‡ã€ROC ä¸æ¬¡ä¼˜æ€§",
      "authors": [
        "Arpan Mukherjee",
        "Marcello Bullo",
        "Debabrota Basu",
        "Deniz GÃ¼ndÃ¼z"
      ],
      "abstract": "While test-time scaling with verification has shown promise in improving the performance of large language models (LLMs), the role of the verifier and its imperfections remain underexplored. The effect of verification manifests through interactions of three quantities: (i) the generator's coverage, (ii) the verifier's region of convergence (ROC), and (iii) the sampling algorithm's sub-optimality. Though recent studies capture subsets of these factors, a unified framework quantifying the geometry of their interplay is missing. We frame verifiable test-time scaling as a transport problem. This characterizes the interaction of coverage, ROC, and sub-optimality, and uncovers that the sub-optimality--coverage curve exhibits three regimes. A transport regime -- where sub-optimality increases with coverage, a policy improvement regime -- where sub-optimality may decrease with coverage, depending on the verifier's ROC, and a saturation regime -- where sub-optimality plateaus, unaffected by coverage. We further propose and analyze two classes of sampling algorithms -- sequential and batched, and examine how their computational complexities shape these trade-offs. Empirical results with Qwen, Llama, and Gemma models corroborate our theoretical findings.",
      "tldr_zh": "è¯¥ç ”ç©¶å°†å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„å¯éªŒè¯æµ‹è¯•æ—¶æ‰©å±• (test-time scaling) å»ºæ¨¡ä¸ºæœ€ä¼˜ä¼ è¾“ (Optimal Transport) é—®é¢˜ï¼Œç³»ç»Ÿåœ°æ¢è®¨äº†ç”Ÿæˆå™¨è¦†ç›–ç‡ (coverage)ã€éªŒè¯è€…æ”¶æ•›åŸŸ (ROC) ä¸é‡‡æ ·ç®—æ³•æ¬¡ä¼˜æ€§ (sub-optimality) ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚é€šè¿‡é‡åŒ–ä¸‰è€…äº¤äº’çš„å‡ ä½•ç‰¹å¾ï¼Œç ”ç©¶æ­ç¤ºäº†æ¬¡ä¼˜æ€§-è¦†ç›–ç‡æ›²çº¿å­˜åœ¨ä¼ è¾“é˜¶æ®µã€ç­–ç•¥æ”¹è¿›é˜¶æ®µå’Œé¥±å’Œé˜¶æ®µè¿™ä¸‰ç§æˆªç„¶ä¸åŒçš„æ¨¡å¼ã€‚ä½œè€…è¿›ä¸€æ­¥æå‡ºå¹¶åˆ†æäº†åºåˆ—åŒ–ä¸æ‰¹å¤„ç†ä¸¤ç±»é‡‡æ ·ç®—æ³•ï¼Œé˜æ˜äº†è®¡ç®—å¤æ‚åº¦å¦‚ä½•å¡‘é€ è¿™äº›æ€§èƒ½æƒè¡¡ã€‚åœ¨ Qwenã€Llama å’Œ Gemma ç­‰æ¨¡å‹ä¸Šçš„å®éªŒç»“æœè¯å®äº†è¯¥ç†è®ºæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œä¸ºç†è§£ä¸å®Œå–„éªŒè¯è€…å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“æä¾›äº†é‡è¦ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18982v1",
      "published_date": "2025-10-21 18:05:42 UTC",
      "updated_date": "2025-10-21 18:05:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:43:57.175291+00:00"
    },
    {
      "arxiv_id": "2510.18876v2",
      "title": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs",
      "title_zh": "Grasp Any Regionï¼šé¢å‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç²¾å‡†ã€ä¸Šä¸‹æ–‡å…³è”åƒç´ çº§ç†è§£",
      "authors": [
        "Haochen Wang",
        "Yuhao Wang",
        "Tao Zhang",
        "Yikang Zhou",
        "Yanwei Li",
        "Jiacong Wang",
        "Jiani Zheng",
        "Ye Tian",
        "Jiahao Meng",
        "Zilong Huang",
        "Guangcan Mai",
        "Anran Wang",
        "Yunhai Tong",
        "Zhuochen Wang",
        "Xiangtai Li",
        "Zhaoxiang Zhang"
      ],
      "abstract": "While Multimodal Large Language Models (MLLMs) excel at holistic understanding, they struggle in capturing the dense world with complex scenes, requiring fine-grained analysis of intricate details and object inter-relationships. Region-level MLLMs have been a promising step. However, previous attempts are generally optimized to understand given regions in isolation, neglecting crucial global contexts. To address this, we introduce Grasp Any Region (GAR) for comprehen- sive region-level visual understanding. Empowered by an effective RoI-aligned feature replay technique, GAR supports (1) precise perception by leveraging necessary global contexts, and (2) modeling interactions between multiple prompts. Together, it then naturally achieves (3) advanced compositional reasoning to answer specific free-form questions about any region, shifting the paradigm from passive description to active dialogue. Moreover, we construct GAR-Bench, which not only provides a more accurate evaluation of single-region comprehension, but also, more importantly, measures interactions and complex reasoning across multiple regions. Extensive experiments have demonstrated that GAR-1B not only maintains the state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5 on DLC-Bench, but also excels at modeling relationships between multiple prompts with advanced comprehension capabilities, even surpassing InternVL3-78B on GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong capabilities can be easily transferred to videos.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Grasp Any Region (GAR) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§æ¨¡å‹ (MLLMs) åœ¨å¤„ç†å¤æ‚åœºæ™¯æ—¶éš¾ä»¥å…¼é¡¾ç»†ç²’åº¦åˆ†æä¸å…¨å±€ä¸Šä¸‹æ–‡çš„é—®é¢˜ã€‚GAR é€šè¿‡å¼•å…¥é«˜æ•ˆçš„ RoI-aligned feature replay æŠ€æœ¯ï¼Œå®ç°äº†åŸºäºå…¨å±€èƒŒæ™¯çš„ç²¾ç¡®åŒºåŸŸæ„ŸçŸ¥ï¼Œå¹¶æ”¯æŒå¤šä¸ªæç¤ºè¯ (prompts) ä¹‹é—´çš„äº¤äº’å»ºæ¨¡ã€‚è¯¥æ¡†æ¶å°†åŒºåŸŸç†è§£ä»ä¼ ç»Ÿçš„è¢«åŠ¨æè¿°æå‡ä¸ºä¸»åŠ¨å¯¹è¯ï¼Œèƒ½å¤Ÿé’ˆå¯¹ä»»æ„å›¾åƒåŒºåŸŸè¿›è¡Œé«˜çº§çš„ç»„åˆæ¨ç† (compositional reasoning)ã€‚ä¸ºäº†æ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº† GAR-Bench åŸºå‡†ï¼Œä¸“é—¨ç”¨äºè¡¡é‡å•åŒºåŸŸç†è§£åŠå¤šåŒºåŸŸé—´çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGAR-1B åœ¨å…³ç³»å»ºæ¨¡å’Œé—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç”šè‡³åœ¨ GAR-Bench-VQA åŸºå‡†ä¸Šè¶…è¶Šäº† InternVL3-78B ç­‰è§„æ¨¡æ›´å¤§çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒGAR-8B åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­å±•ç°äº†æå¼ºçš„é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18876v2",
      "published_date": "2025-10-21 17:59:59 UTC",
      "updated_date": "2025-10-22 04:30:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:43:58.184308+00:00"
    },
    {
      "arxiv_id": "2510.18941v1",
      "title": "ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge",
      "title_zh": "ProfBenchï¼šéœ€è¦ä¸“ä¸šçŸ¥è¯†è¿›è¡Œå›ç­”ä¸è¯„åˆ¤çš„å¤šé¢†åŸŸè¯„ä»·å‡†åˆ™",
      "authors": [
        "Zhilin Wang",
        "Jaehun Jung",
        "Ximing Lu",
        "Shizhe Diao",
        "Ellie Evans",
        "Jiaqi Zeng",
        "Pavlo Molchanov",
        "Yejin Choi",
        "Jan Kautz",
        "Yi Dong"
      ],
      "abstract": "Evaluating progress in large language models (LLMs) is often constrained by the challenge of verifying responses, limiting assessments to tasks like mathematics, programming, and short-form question-answering. However, many real-world applications require evaluating LLMs in processing professional documents, synthesizing information, and generating comprehensive reports in response to user queries. We introduce ProfBench: a set of over 7000 response-criterion pairs as evaluated by human-experts with professional knowledge across Physics PhD, Chemistry PhD, Finance MBA and Consulting MBA. We build robust and affordable LLM-Judges to evaluate ProfBench rubrics, by mitigating self-enhancement bias and reducing the cost of evaluation by 2-3 orders of magnitude, to make it fair and accessible to the broader community. Our findings reveal that ProfBench poses significant challenges even for state-of-the-art LLMs, with top-performing models like GPT-5-high achieving only 65.9\\% overall performance. Furthermore, we identify notable performance disparities between proprietary and open-weight models and provide insights into the role that extended thinking plays in addressing complex, professional-domain tasks. Data: https://huggingface.co/datasets/nvidia/ProfBench and Code: https://github.com/NVlabs/ProfBench",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ProfBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡ 7000 ä¸ªç”±äººç±»ä¸“å®¶è¯„ä¼°çš„å“åº”-å‡†åˆ™å¯¹ï¼ˆresponse-criterion pairsï¼‰çš„æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†ä¸“ä¸šæ–‡æ¡£ã€ç»¼åˆä¿¡æ¯å’Œç”Ÿæˆå¤æ‚æŠ¥å‘Šæ–¹é¢çš„è¯„ä¼°å±€é™ã€‚è¯¥åŸºå‡†æµ‹è¯•è·¨è¶Šäº†ç‰©ç†å­¦åšå£«ï¼ˆPhysics PhDï¼‰ã€åŒ–å­¦åšå£«ï¼ˆChemistry PhDï¼‰ã€é‡‘è MBA å’Œå’¨è¯¢ MBA ç­‰å¤šä¸ªéœ€è¦æ·±åšä¸“ä¸šçŸ¥è¯†çš„é¢†åŸŸã€‚ä¸ºäº†ç¡®ä¿è¯„ä¼°çš„å…¬å¹³æ€§ä¸å¯è®¿é—®æ€§ï¼Œç ”ç©¶è€…æ„å»ºäº†é²æ£’ä¸”ä½æˆæœ¬çš„ LLM-Judgesï¼Œé€šè¿‡ç¼“è§£è‡ªæˆ‘å¢å¼ºåå·®ï¼ˆself-enhancement biasï¼‰å°†è¯„ä¼°æˆæœ¬é™ä½äº† 2-3 ä¸ªæ•°é‡çº§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProfBench å¯¹å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹æ„æˆäº†ä¸¥å³»æŒ‘æˆ˜ï¼Œå³ä¾¿è¡¨ç°æœ€ä½³çš„ GPT-5-high ä¹Ÿä»…è¾¾åˆ° 65.9% çš„æ€»åˆ†ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ­ç¤ºäº†é—­æºæ¨¡å‹ä¸å¼€æºæ¨¡å‹ä¹‹é—´çš„æ˜¾è‘—æ€§èƒ½å·®å¼‚ï¼Œå¹¶æä¾›äº†å…³äºé•¿é“¾æ€è€ƒï¼ˆextended thinkingï¼‰åœ¨å¤„ç†ä¸“ä¸šé¢†åŸŸå¤æ‚ä»»åŠ¡ä¸­æ‰€èµ·ä½œç”¨çš„æ·±åˆ»è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "23 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.18941v1",
      "published_date": "2025-10-21 17:59:44 UTC",
      "updated_date": "2025-10-21 17:59:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:43:54.179259+00:00"
    },
    {
      "arxiv_id": "2510.18940v1",
      "title": "NeuroAda: Activating Each Neuron's Potential for Parameter-Efficient Fine-Tuning",
      "title_zh": "NeuroAdaï¼šæ¿€å‘æ¯ä¸ªç¥ç»å…ƒæ½œèƒ½çš„å‚æ•°é«˜æ•ˆå¾®è°ƒ",
      "authors": [
        "Zhi Zhang",
        "Yixian Shen",
        "Congfeng Cao",
        "Ekaterina Shutova"
      ],
      "abstract": "Existing parameter-efficient fine-tuning (PEFT) methods primarily fall into two categories: addition-based and selective in-situ adaptation. The former, such as LoRA, introduce additional modules to adapt the model to downstream tasks, offering strong memory efficiency. However, their representational capacity is often limited, making them less suitable for fine-grained adaptation. In contrast, the latter directly fine-tunes a carefully chosen subset of the original model parameters, allowing for more precise and effective adaptation, but at the cost of significantly increased memory consumption. To reconcile this trade-off, we propose NeuroAda, a novel PEFT method that enables fine-grained model finetuning while maintaining high memory efficiency. Our approach first identifies important parameters (i.e., connections within the network) as in selective adaptation, and then introduces bypass connections for these selected parameters. During finetuning, only the bypass connections are updated, leaving the original model parameters frozen. Empirical results on 23+ tasks spanning both natural language generation and understanding demonstrate that NeuroAda achieves state-of-the-art performance with as little as $\\leq \\textbf{0.02}\\%$ trainable parameters, while reducing CUDA memory usage by up to 60%. We release our code here: https://github.com/FightingFighting/NeuroAda.git.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†NeuroAdaï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„Parameter-Efficient Fine-Tuning (PEFT)æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨è¡¨ç¤ºèƒ½åŠ›ä¸å†…å­˜æ•ˆç‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚NeuroAdaé¦–å…ˆè¯†åˆ«å‡ºç½‘ç»œä¸­é‡è¦çš„å‚æ•°è¿æ¥ï¼Œå¹¶é’ˆå¯¹è¿™äº›é€‰å®šå‚æ•°å¼•å…¥æ—è·¯è¿æ¥(bypass connections)ï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ä»…æ›´æ–°æ—è·¯è¿æ¥è€Œä¿æŒåŸå§‹æ¨¡å‹å‚æ•°å†»ç»“ã€‚è¿™ç§è®¾è®¡å·§å¦™åœ°ç»“åˆäº†LoRAç­‰addition-basedæ–¹æ³•çš„å†…å­˜ä¼˜åŠ¿ä¸selective adaptationçš„ç²¾ç¡®è¡¨ç¤ºèƒ½åŠ›ï¼Œä»è€Œå®ç°äº†æ›´ç»†ç²’åº¦ä¸”æœ‰æ•ˆçš„æ¨¡å‹é€‚é…ã€‚åœ¨æ¶‰åŠè‡ªç„¶è¯­è¨€ç”Ÿæˆä¸ç†è§£çš„23ä¸ªä»¥ä¸Šä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒNeuroAdaä»…éœ€ä¸è¶…è¿‡0.02%çš„å¯è®­ç»ƒå‚æ•°å³å¯è¾¾åˆ°state-of-the-artæ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æå‡æ¨¡å‹è¡¨ç°çš„åŒæ—¶ï¼ŒæˆåŠŸé™ä½äº†é«˜è¾¾60%çš„CUDAå†…å­˜å ç”¨ï¼Œä¸ºå¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„é«˜æ•ˆå¾®è°ƒæä¾›äº†æå…·ç«äº‰åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18940v1",
      "published_date": "2025-10-21 17:59:24 UTC",
      "updated_date": "2025-10-21 17:59:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:43:59.108546+00:00"
    },
    {
      "arxiv_id": "2510.18871v1",
      "title": "How Do LLMs Use Their Depth?",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹å¦‚ä½•åˆ©ç”¨å…¶æ·±åº¦ï¼Ÿ",
      "authors": [
        "Akshat Gupta",
        "Jay Yeung",
        "Gopala Anumanchipalli",
        "Anna Ivanova"
      ],
      "abstract": "Growing evidence suggests that large language models do not use their depth uniformly, yet we still lack a fine-grained understanding of their layer-wise prediction dynamics. In this paper, we trace the intermediate representations of several open-weight models during inference and reveal a structured and nuanced use of depth. Specifically, we propose a \"Guess-then-Refine\" framework that explains how LLMs internally structure their computations to make predictions. We first show that the top-ranked predictions in early LLM layers are composed primarily of high-frequency tokens, which act as statistical guesses proposed by the model early on due to the lack of appropriate contextual information. As contextual information develops deeper into the model, these initial guesses get refined into contextually appropriate tokens. Even high-frequency token predictions from early layers get refined >70% of the time, indicating that correct token prediction is not \"one-and-done\". We then go beyond frequency-based prediction to examine the dynamic usage of layer depth across three case studies. (i) Part-of-speech analysis shows that function words are, on average, the earliest to be predicted correctly. (ii) Fact recall task analysis shows that, in a multi-token answer, the first token requires more computational depth than the rest. (iii) Multiple-choice task analysis shows that the model identifies the format of the response within the first half of the layers, but finalizes its response only toward the end. Together, our results provide a detailed view of depth usage in LLMs, shedding light on the layer-by-layer computations that underlie successful predictions and providing insights for future works to improve computational efficiency in transformer-based models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)å¦‚ä½•éå‡åŒ€åœ°åˆ©ç”¨å…¶æ¨¡å‹æ·±åº¦ï¼Œå¹¶æå‡ºäº†â€œçŒœæƒ³åå®Œå–„â€(Guess-then-Refine)æ¡†æ¶æ¥è§£é‡Šæ¨¡å‹å†…éƒ¨çš„é¢„æµ‹åŠ¨æ€ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMsçš„æ—©æœŸå±‚ä¸»è¦ç”Ÿæˆé«˜é¢‘Tokenä½œä¸ºåˆæ­¥çš„ç»Ÿè®¡å­¦çŒœæƒ³(statistical guesses)ï¼Œéšåé€šè¿‡æ·±å±‚ç½‘ç»œç»“åˆä¸Šä¸‹æ–‡ä¿¡æ¯å¯¹è¿™äº›çŒœæƒ³è¿›è¡Œä¸æ–­å®Œå–„ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä¾¿æ—©æœŸå±‚é¢„æµ‹æ­£ç¡®ï¼Œè¶…è¿‡70%çš„é«˜é¢‘Tokené¢„æµ‹ä»ä¼šåœ¨åç»­å±‚è¢«è¿›ä¸€æ­¥ç»†åŒ–ï¼Œè¿™è¯´æ˜é¢„æµ‹è¿‡ç¨‹å¹¶éä¸€è¹´è€Œå°±ã€‚é€šè¿‡å¯¹ä¸åŒä»»åŠ¡çš„åŠ¨æ€åˆ†æå‘ç°ï¼Œè™šè¯(function words)é€šå¸¸æœ€æ—©è¢«æ­£ç¡®é¢„æµ‹ï¼Œè€Œäº‹å®å¬å›(Fact recall)ä»»åŠ¡çš„é¦–ä¸ªTokenæ¯”åç»­Tokenéœ€è¦æ›´æ·±çš„è®¡ç®—æ·±åº¦ã€‚åœ¨å¤šé€‰é¢˜(Multiple-choice)ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹åœ¨å‰åŠéƒ¨åˆ†å±‚å³å¯è¯†åˆ«è¾“å‡ºæ ¼å¼ï¼Œä½†ç›´åˆ°æ¥è¿‘æœ«å°¾å±‚æ‰æœ€ç»ˆç¡®å®šé€‰é¡¹ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†LLMsé€å±‚è®¡ç®—çš„ç²¾ç»†ç»“æ„ï¼Œä¸ºæœªæ¥ä¼˜åŒ–Transformeræ¨¡å‹çš„è®¡ç®—æ•ˆç‡(computational efficiency)æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18871v1",
      "published_date": "2025-10-21 17:59:05 UTC",
      "updated_date": "2025-10-21 17:59:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:44:10.488099+00:00"
    },
    {
      "arxiv_id": "2510.18866v3",
      "title": "LightMem: Lightweight and Efficient Memory-Augmented Generation",
      "title_zh": "LightMemï¼šè½»é‡çº§ä¸”é«˜æ•ˆçš„è®°å¿†å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Jizhan Fang",
        "Xinle Deng",
        "Haoming Xu",
        "Ziyan Jiang",
        "Yuqi Tang",
        "Ziwen Xu",
        "Shumin Deng",
        "Yunzhi Yao",
        "Mengru Wang",
        "Shuofei Qiao",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. On LongMemEval and LoCoMo, using GPT and Qwen backbones, LightMem consistently surpasses strong baselines, improving QA accuracy by up to 7.7% / 29.3%, reducing total token usage by up to 38x / 20.9x and API calls by up to 30x / 55.5x, while purely online test-time costs are even lower, achieving up to 106x / 117x token reduction and 159x / 310x fewer API calls. The code is available at https://github.com/zjunlp/LightMem.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åˆ©ç”¨å†å²äº¤äº’ä¿¡æ¯æ—¶å­˜åœ¨çš„è®¡ç®—ä¸æ—¶é—´å¼€é”€è¿‡å¤§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…¼é¡¾æ€§èƒ½ä¸æ•ˆç‡çš„å†…å­˜ç³»ç»ŸLightMemã€‚å—äººç±»è®°å¿†çš„Atkinson-Shiffrinæ¨¡å‹å¯å‘ï¼ŒLightMemå°†è®°å¿†åˆ’åˆ†ä¸ºä¸‰ä¸ªäº’è¡¥é˜¶æ®µï¼šé€šè¿‡è½»é‡åŒ–å‹ç¼©è¿‡æ»¤ä¿¡æ¯çš„æ„Ÿå®˜è®°å¿†(Sensory Memory)ã€è¿›è¡Œä¸»é¢˜åŒ–æ•´åˆæ€»ç»“çš„çŸ­æœŸè®°å¿†(Short-term Memory)ï¼Œä»¥åŠåˆ©ç”¨ç¡çœ æ—¶é—´æ›´æ–°(Sleep-time Update)æœºåˆ¶å®ç°ç¦»çº¿æ•´åˆçš„é•¿æœŸè®°å¿†(Long-term Memory)ã€‚åœ¨LongMemEvalå’ŒLoCoMoç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLightMemåœ¨GPTå’ŒQwenç­‰åº•åº§æ¨¡å‹ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†ï¼Œä¸ä»…æé«˜äº†é—®ç­”å‡†ç¡®ç‡ï¼Œè¿˜æå¤§åœ°é™ä½äº†èµ„æºæ¶ˆè€—ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿæœ€é«˜å¯å‡å°‘117å€çš„Tokenä½¿ç”¨é‡å’Œ310å€çš„APIè°ƒç”¨æ¬¡æ•°ï¼ŒæˆåŠŸåœ¨ä¿è¯æ¨¡å‹æ€§èƒ½çš„åŒæ—¶å¤§å¹…æå‡äº†æ¨ç†æ•ˆç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in progress",
      "pdf_url": "https://arxiv.org/pdf/2510.18866v3",
      "published_date": "2025-10-21 17:58:17 UTC",
      "updated_date": "2025-11-26 09:32:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:45:01.501633+00:00"
    },
    {
      "arxiv_id": "2510.18938v2",
      "title": "StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction",
      "title_zh": "StutterZero ä¸ StutterFormerï¼šé¢å‘å£åƒè½¬å†™ä¸çº é”™çš„ç«¯åˆ°ç«¯è¯­éŸ³è½¬æ¢",
      "authors": [
        "Qianheng Xu"
      ],
      "abstract": "Over 70 million people worldwide experience stuttering, yet most automatic speech systems misinterpret disfluent utterances or fail to transcribe them accurately. Existing methods for stutter correction rely on handcrafted feature extraction or multi-stage automatic speech recognition (ASR) and text-to-speech (TTS) pipelines, which separate transcription from audio reconstruction and often amplify distortions. This work introduces StutterZero and StutterFormer, the first end-to-end waveform-to-waveform models that directly convert stuttered speech into fluent speech while jointly predicting its transcription. StutterZero employs a convolutional-bidirectional LSTM encoder-decoder with attention, whereas StutterFormer integrates a dual-stream Transformer with shared acoustic-linguistic representations. Both architectures are trained on paired stuttered-fluent data synthesized from the SEP-28K and LibriStutter corpora and evaluated on unseen speakers from the FluencyBank dataset. Across all benchmarks, StutterZero had a 24% decrease in Word Error Rate (WER) and a 31% improvement in semantic similarity (BERTScore) compared to the leading Whisper-Medium model. StutterFormer achieved better results, with a 28% decrease in WER and a 34% improvement in BERTScore. The results validate the feasibility of direct end-to-end stutter-to-fluent speech conversion, offering new opportunities for inclusive human-computer interaction, speech therapy, and accessibility-oriented AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨çƒè¶…è¿‡7000ä¸‡äººå£åƒè€…é¢ä¸´çš„è¯­éŸ³è¯†åˆ«éšœç¢ï¼Œæå‡ºäº†StutterZeroå’ŒStutterFormerä¸¤ç§ç«¯åˆ°ç«¯(end-to-end)çš„æ³¢å½¢åˆ°æ³¢å½¢(waveform-to-waveform)æ¨¡å‹ï¼Œæ—¨åœ¨ç›´æ¥å°†å£åƒè¯­éŸ³è½¬æ¢ä¸ºæµåˆ©è¯­éŸ³å¹¶åŒæ­¥ç”Ÿæˆè½¬å½•ã€‚ç›¸è¾ƒäºä¼ ç»Ÿä¾èµ–å¤šé˜¶æ®µASRå’ŒTTSçš„å¤æ‚æµç¨‹ï¼ŒStutterZeroé‡‡ç”¨å¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„å·ç§¯åŒå‘LSTMç»“æ„ï¼Œè€ŒStutterFormeråˆ™åˆ©ç”¨åŒæµTransformer(dual-stream Transformer)å®ç°å£°å­¦ä¸è¯­è¨€è¡¨å¾çš„å…±äº«ã€‚è¿™äº›æ¨¡å‹åœ¨SEP-28Kå’ŒLibriStutteråˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå¹¶åœ¨FluencyBankæ•°æ®é›†çš„æœªçŸ¥è¯´è¯äººä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒStutterFormeråœ¨è¯é”™ç‡(Word Error Rate, WER)ä¸Šæ¯”é¢†å…ˆçš„Whisper-Mediumæ¨¡å‹é™ä½äº†28%ï¼Œè¯­ä¹‰ç›¸ä¼¼åº¦(BERTScore)æå‡äº†34%ï¼Œè€ŒStutterZeroä¹Ÿåˆ†åˆ«å®ç°äº†24%çš„WERé™ä½å’Œ31%çš„BERTScoreæå‡ã€‚è¯¥å·¥ä½œè¯æ˜äº†ç›´æ¥è¿›è¡Œç«¯åˆ°ç«¯å£åƒè¯­éŸ³ä¿®å¤çš„å¯è¡Œæ€§ï¼Œä¸ºæ„å»ºåŒ…å®¹æ€§äººæœºäº¤äº’ã€è¨€è¯­æ²»ç–—åŠæ— éšœç¢äººå·¥æ™ºèƒ½ç³»ç»Ÿå¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "eess.AS",
      "comment": "13 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.18938v2",
      "published_date": "2025-10-21 17:54:36 UTC",
      "updated_date": "2025-11-05 00:00:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:44:19.498053+00:00"
    },
    {
      "arxiv_id": "2510.18855v2",
      "title": "Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model",
      "title_zh": "æ­¥æ­¥è¿›åŒ–ï¼šé¢å‘ä¸‡äº¿çº§æ€ç»´æ¨¡å‹çš„å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Ling Team",
        "Anqi Shen",
        "Baihui Li",
        "Bin Hu",
        "Bin Jing",
        "Cai Chen",
        "Chao Huang",
        "Chao Zhang",
        "Chaokun Yang",
        "Cheng Lin",
        "Chengyao Wen",
        "Congqi Li",
        "Deng Zhao",
        "Dingbo Yuan",
        "Donghai You",
        "Fagui Mao",
        "Fanzhuang Meng",
        "Feng Xu",
        "Guojie Li",
        "Guowei Wang",
        "Hao Dai",
        "Haonan Zheng",
        "Hong Liu",
        "Jia Guo",
        "Jiaming Liu",
        "Jian Liu",
        "Jianhao Fu",
        "Jiannan Shi",
        "Jianwen Wang",
        "Jianxin Lai",
        "Jin Yang",
        "Jun Mei",
        "Jun Zhou",
        "Junbo Zhao",
        "Junping Zhao",
        "Kuan Xu",
        "Le Su",
        "Lei Chen",
        "Li Tang",
        "Liang Jiang",
        "Liangcheng Fu",
        "Lianhao Xu",
        "Linfeng Shi",
        "Lisha Liao",
        "Longfei Zheng",
        "Meng Li",
        "Mingchun Chen",
        "Qi Zuo",
        "Qiang Cheng",
        "Qianggang Cao",
        "Qitao Shi",
        "Quanrui Guo",
        "Senlin Zhu",
        "Shaofei Wang",
        "Shaomian Zheng",
        "Shuaicheng Li",
        "Shuwei Gu",
        "Siba Chen",
        "Tao Wu",
        "Tao Zhang",
        "Tianyu Zhang",
        "Tianyu Zhou",
        "Tiwei Bie",
        "Tongkai Yang",
        "Wang Hong",
        "Wang Ren",
        "Weihua Chen",
        "Wenbo Yu",
        "Wengang Zheng",
        "Xiangchun Wang",
        "Xiaodong Yan",
        "Xiaopei Wan",
        "Xin Zhao",
        "Xinyu Kong",
        "Xinyu Tang",
        "Xudong Han",
        "Xudong Wang",
        "Xuemin Yang",
        "Xueyu Hu",
        "Yalin Zhang",
        "Yan Sun",
        "Yicheng Shan",
        "Yilong Wang",
        "Yingying Xu",
        "Yongkang Liu",
        "Yongzhen Guo",
        "Yuanyuan Wang",
        "Yuchen Yan",
        "Yuefan Wang",
        "Yuhong Guo",
        "Zehuan Li",
        "Zhankai Xu",
        "Zhe Li",
        "Zhenduo Zhang",
        "Zhengke Gui",
        "Zhenxuan Pan",
        "Zhenyu Huang",
        "Zhenzhong Lan",
        "Zhiqiang Ding",
        "Zhiqiang Zhang",
        "Zhixun Li",
        "Zhizhen Liu",
        "Zihao Wang",
        "Zujie Wen"
      ],
      "abstract": "We present Ring-1T, the first open-source, state-of-the-art thinking model with a trillion-scale parameter. It features 1 trillion total parameters and activates approximately 50 billion per token. Training such models at a trillion-parameter scale introduces unprecedented challenges, including train-inference misalignment, inefficiencies in rollout processing, and bottlenecks in the RL system. To address these, we pioneer three interconnected innovations: (1) IcePop stabilizes RL training via token-level discrepancy masking and clipping, resolving instability from training-inference mismatches; (2) C3PO++ improves resource utilization for long rollouts under a token budget by dynamically partitioning them, thereby obtaining high time efficiency; and (3) ASystem, a high-performance RL framework designed to overcome the systemic bottlenecks that impede trillion-parameter model training. Ring-1T delivers breakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on HMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-1. Notably, it attains a silver medal-level result on the IMO-2025, underscoring its exceptional reasoning capabilities. By releasing the complete 1T parameter MoE model to the community, we provide the research community with direct access to cutting-edge reasoning capabilities. This contribution marks a significant milestone in democratizing large-scale reasoning intelligence and establishes a new baseline for open-source model performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Ring-1Tï¼Œè¿™æ˜¯é¦–ä¸ªå‚æ•°è§„æ¨¡è¾¾ä¸‡äº¿çº§çš„å¼€æºå…ˆè¿›æ€ç»´æ¨¡å‹(thinking model)ï¼ŒåŒ…å«1ä¸‡äº¿æ€»å‚æ•°å¹¶é‡‡ç”¨æ¯tokenæ¿€æ´»500äº¿å‚æ•°çš„MoEæ¶æ„ã€‚é’ˆå¯¹ä¸‡äº¿è§„æ¨¡æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¸­é¢ä¸´çš„è®­ç»ƒæ¨ç†å¤±é…ã€rolloutå¤„ç†æ•ˆç‡ä½å’Œç³»ç»Ÿç“¶é¢ˆç­‰æŒ‘æˆ˜ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸‰é¡¹å…³é”®åˆ›æ–°ã€‚IcePopé€šè¿‡tokençº§åˆ«çš„å·®å¼‚æ©ç å’Œè£å‰ªæŠ€æœ¯ç¨³å®šäº†RLè®­ç»ƒï¼Œè§£å†³äº†è®­ç»ƒä¸æ¨ç†ä¸ä¸€è‡´å¸¦æ¥çš„ä¸ç¨³å®šæ€§ã€‚C3PO++é€šè¿‡åŠ¨æ€åˆ‡åˆ†é•¿rolloutä¼˜åŒ–äº†èµ„æºåˆ©ç”¨ç‡ï¼Œå¤§å¹…æå‡äº†åœ¨tokené¢„ç®—ä¸‹çš„æ—¶é—´æ•ˆç‡ã€‚ASystemåˆ™ä½œä¸ºé«˜æ€§èƒ½å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå…‹æœäº†æ”¯æ’‘ä¸‡äº¿å‚æ•°æ¨¡å‹è®­ç»ƒçš„ç³»ç»Ÿæ€§éšœç¢ã€‚å®éªŒè¡¨æ˜ï¼ŒRing-1Tåœ¨AIME-2025ã€HMMT-2025å’ŒCodeForcesç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†çªç ´æ€§æˆç»©ï¼Œå¹¶åœ¨IMO-2025ä¸­è¾¾åˆ°äº†é“¶ç‰Œæ°´å¹³ã€‚é€šè¿‡å‘ç¤¾åŒºå¼€æºè¿™ä¸€å®Œæ•´çš„1Tå‚æ•°æ¨¡å‹ï¼Œè¯¥ç ”ç©¶ä¸ä»…æ¨åŠ¨äº†å¤§è§„æ¨¡æ¨ç†æ™ºèƒ½çš„æ°‘ä¸»åŒ–ï¼Œä¹Ÿä¸ºå¼€æºæ¨¡å‹æ€§èƒ½ç¡®ç«‹äº†æ–°çš„æ ‡æ†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Technical Report",
      "pdf_url": "https://arxiv.org/pdf/2510.18855v2",
      "published_date": "2025-10-21 17:46:14 UTC",
      "updated_date": "2025-10-25 08:21:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:44:23.090903+00:00"
    },
    {
      "arxiv_id": "2510.18852v1",
      "title": "Lyapunov-Aware Quantum-Inspired Reinforcement Learning for Continuous-Time Vehicle Control: A Feasibility Study",
      "title_zh": "é¢å‘è¿ç»­æ—¶é—´è½¦è¾†æ§åˆ¶çš„ Lyapunov æ„ŸçŸ¥é‡å­å¯å‘å¼ºåŒ–å­¦ä¹ ï¼šä¸€é¡¹å¯è¡Œæ€§ç ”ç©¶",
      "authors": [
        "Nutkritta Kraipatthanapong",
        "Natthaphat Thathong",
        "Pannita Suksawas",
        "Thanunnut Klunklin",
        "Kritin Vongthonglua",
        "Krit Attahakul",
        "Aueaphum Aueawatthanaphisut"
      ],
      "abstract": "This paper presents a novel Lyapunov-Based Quantum Reinforcement Learning (LQRL) framework that integrates quantum policy optimization with Lyapunov stability analysis for continuous-time vehicle control. The proposed approach combines the representational power of variational quantum circuits (VQCs) with a stability-aware policy gradient mechanism to ensure asymptotic convergence and safe decision-making under dynamic environments. The vehicle longitudinal control problem was formulated as a continuous-state reinforcement learning task, where the quantum policy network generates control actions subject to Lyapunov stability constraints. Simulation experiments were conducted in a closed-loop adaptive cruise control scenario using a quantum-inspired policy trained under stability feedback. The results demonstrate that the LQRL framework successfully embeds Lyapunov stability verification into quantum policy learning, enabling interpretable and stability-aware control performance. Although transient overshoot and Lyapunov divergence were observed under aggressive acceleration, the system maintained bounded state evolution, validating the feasibility of integrating safety guarantees within quantum reinforcement learning architectures. The proposed framework provides a foundational step toward provably safe quantum control in autonomous systems and hybrid quantum-classical optimization domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Lyapunov-Based Quantum Reinforcement Learning (LQRL) æ¡†æ¶ï¼Œæ—¨åœ¨å°†é‡å­ç­–ç•¥ä¼˜åŒ–ä¸Lyapunovç¨³å®šæ€§åˆ†æç›¸ç»“åˆï¼Œç”¨äºè¿ç»­æ—¶é—´è½¦è¾†æ§åˆ¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å˜åˆ†é‡å­ç”µè·¯ (Variational Quantum Circuits, VQCs) çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶ç»“åˆç¨³å®šæ€§æ„ŸçŸ¥ç­–ç•¥æ¢¯åº¦æœºåˆ¶ï¼Œä»¥ç¡®ä¿åœ¨åŠ¨æ€ç¯å¢ƒä¸‹å®ç°æ¸è¿‘æ”¶æ•›å’Œå®‰å…¨å†³ç­–ã€‚ç ”ç©¶å°†è½¦è¾†çºµå‘æ§åˆ¶é—®é¢˜å»ºæ¨¡ä¸ºè¿ç»­çŠ¶æ€å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼Œç”±é‡å­ç­–ç•¥ç½‘ç»œåœ¨Lyapunovç¨³å®šæ€§çº¦æŸä¸‹ç”Ÿæˆæ§åˆ¶åŠ¨ä½œã€‚åœ¨è‡ªé€‚åº”å·¡èˆªæ§åˆ¶ (Adaptive Cruise Control) çš„é—­ç¯æ¨¡æ‹Ÿå®éªŒä¸­ï¼ŒLQRLæ¡†æ¶æˆåŠŸåœ°å°†ç¨³å®šæ€§éªŒè¯åµŒå…¥é‡å­ç­–ç•¥å­¦ä¹ ï¼Œå®ç°äº†å…·å¤‡è§£é‡Šæ€§å’Œç¨³å®šæ€§æ„ŸçŸ¥çš„æ§åˆ¶è¡¨ç°ã€‚å°½ç®¡åœ¨å‰§çƒˆåŠ é€Ÿä¸‹è§‚å¯Ÿåˆ°ç¬æ€è¶…è°ƒå’ŒLyapunovå‘æ•£ï¼Œä½†ç³»ç»Ÿä»ä¿æŒäº†æœ‰ç•Œçš„çŠ¶æ€æ¼”åŒ–ï¼ŒéªŒè¯äº†åœ¨é‡å­å¼ºåŒ–å­¦ä¹ æ¶æ„ä¸­é›†æˆå®‰å…¨ä¿éšœçš„å¯è¡Œæ€§ã€‚è¯¥æ¡†æ¶ä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå’Œæ··åˆé‡å­-ç»å…¸ä¼˜åŒ–é¢†åŸŸä¸­å¯è¯æ˜å®‰å…¨çš„é‡å­æ§åˆ¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "quant-ph",
      "comment": "7 pages, 4 figures, 20 equations, 3 appendices, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.18852v1",
      "published_date": "2025-10-21 17:44:45 UTC",
      "updated_date": "2025-10-21 17:44:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:44:22.391046+00:00"
    },
    {
      "arxiv_id": "2510.18851v1",
      "title": "DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World Image Super-Resolution",
      "title_zh": "DP$^2$O-SRï¼šé¢å‘çœŸå®åœºæ™¯å›¾åƒè¶…åˆ†è¾¨ç‡çš„ç›´æ¥æ„ŸçŸ¥åå¥½ä¼˜åŒ–",
      "authors": [
        "Rongyuan Wu",
        "Lingchen Sun",
        "Zhengqiang Zhang",
        "Shihao Wang",
        "Tianhe Wu",
        "Qiaosi Yi",
        "Shuai Li",
        "Lei Zhang"
      ],
      "abstract": "Benefiting from pre-trained text-to-image (T2I) diffusion models, real-world image super-resolution (Real-ISR) methods can synthesize rich and realistic details. However, due to the inherent stochasticity of T2I models, different noise inputs often lead to outputs with varying perceptual quality. Although this randomness is sometimes seen as a limitation, it also introduces a wider perceptual quality range, which can be exploited to improve Real-ISR performance. To this end, we introduce Direct Perceptual Preference Optimization for Real-ISR (DP$^2$O-SR), a framework that aligns generative models with perceptual preferences without requiring costly human annotations. We construct a hybrid reward signal by combining full-reference and no-reference image quality assessment (IQA) models trained on large-scale human preference datasets. This reward encourages both structural fidelity and natural appearance. To better utilize perceptual diversity, we move beyond the standard best-vs-worst selection and construct multiple preference pairs from outputs of the same model. Our analysis reveals that the optimal selection ratio depends on model capacity: smaller models benefit from broader coverage, while larger models respond better to stronger contrast in supervision. Furthermore, we propose hierarchical preference optimization, which adaptively weights training pairs based on intra-group reward gaps and inter-group diversity, enabling more efficient and stable learning. Extensive experiments across both diffusion- and flow-based T2I backbones demonstrate that DP$^2$O-SR significantly improves perceptual quality and generalizes well to real-world benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DP$^2$O-SRï¼Œä¸€ç§é’ˆå¯¹çœŸå®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡(Real-ISR)è®¾è®¡çš„ç›´æ¥æ„ŸçŸ¥åå¥½ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ–‡æœ¬åˆ°å›¾åƒ(T2I)æ‰©æ•£æ¨¡å‹ç”±äºéšæœºæ€§å¯¼è‡´çš„è¾“å‡ºè´¨é‡ä¸ç¨³å®šé—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆå…¨å‚è€ƒä¸æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°(IQA)æ¨¡å‹æ„å»ºæ··åˆå¥–åŠ±ä¿¡å·ï¼Œæ— éœ€äººå·¥æ ‡æ³¨å³å¯å°†ç”Ÿæˆæ¨¡å‹ä¸äººç±»æ„ŸçŸ¥åå¥½å¯¹é½ï¼Œå…¼é¡¾ç»“æ„ä¿çœŸåº¦ä¸è‡ªç„¶å¤–è§‚ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨æ„ŸçŸ¥å¤šæ ·æ€§ï¼Œç ”ç©¶è€…ä»åŒä¸€æ¨¡å‹çš„å¤šæ¬¡è¾“å‡ºä¸­æ„å»ºå¤šç»„åå¥½å¯¹ï¼Œå¹¶å‘ç°æœ€ä¼˜é€‰æ‹©æ¯”ä¾‹ä¸æ¨¡å‹å®¹é‡å¯†åˆ‡ç›¸å…³ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†å±‚æ¬¡åŒ–åå¥½ä¼˜åŒ–(Hierarchical Preference Optimization)æœºåˆ¶ï¼Œæ ¹æ®å¥–åŠ±å·®è·å’Œå¤šæ ·æ€§è‡ªé€‚åº”åŠ æƒè®­ç»ƒå¯¹ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆä¸”ç¨³å®šçš„å­¦ä¹ è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒDP$^2$O-SRåœ¨æ‰©æ•£å’ŒæµåŒ¹é…(Flow-based)æ¶æ„ä¸‹å‡èƒ½æ˜¾è‘—æå‡æ„ŸçŸ¥è´¨é‡ï¼Œå¹¶åœ¨å¤šä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accept by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.18851v1",
      "published_date": "2025-10-21 17:43:23 UTC",
      "updated_date": "2025-10-21 17:43:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:44:25.479912+00:00"
    },
    {
      "arxiv_id": "2510.21815v1",
      "title": "HDR Image Reconstruction using an Unsupervised Fusion Model",
      "title_zh": "åŸºäºæ— ç›‘ç£èåˆæ¨¡å‹çš„HDRå›¾åƒé‡å»º",
      "authors": [
        "Kumbha Nagaswetha"
      ],
      "abstract": "High Dynamic Range (HDR) imaging aims to reproduce the wide range of brightness levels present in natural scenes, which the human visual system can perceive but conventional digital cameras often fail to capture due to their limited dynamic range. To address this limitation, we propose a deep learning-based multi-exposure fusion approach for HDR image generation. The method takes a set of differently exposed Low Dynamic Range (LDR) images, typically an underexposed and an overexposed image, and learns to fuse their complementary information using a convolutional neural network (CNN). The underexposed image preserves details in bright regions, while the overexposed image retains information in dark regions; the network effectively combines these to reconstruct a high-quality HDR output. The model is trained in an unsupervised manner, without relying on ground-truth HDR images, making it practical for real-world applications where such data is unavailable. We evaluate our results using the Multi-Exposure Fusion Structural Similarity Index Measure (MEF-SSIM) and demonstrate that our approach achieves superior visual quality compared to existing fusion methods. A customized loss function is further introduced to improve reconstruction fidelity and optimize model performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ— ç›‘ç£èåˆæ¨¡å‹(Unsupervised Fusion Model)çš„é«˜åŠ¨æ€èŒƒå›´(High Dynamic Range, HDR)å›¾åƒé‡å»ºæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ•°å­—ç›¸æœºç”±äºåŠ¨æ€èŒƒå›´æœ‰é™è€Œæ— æ³•å®Œæ•´æ•æ‰è‡ªç„¶åœºæ™¯äº®åº¦å±‚æ¬¡çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œ(CNN)å¯¹ä¸åŒæ›å…‰åº¦çš„ä½åŠ¨æ€èŒƒå›´(Low Dynamic Range, LDR)å›¾åƒè¿›è¡Œå¤šæ›å…‰èåˆï¼Œæœ‰æ•ˆç»“åˆäº†æ¬ æ›å…‰å›¾åƒçš„é«˜å…‰ç»†èŠ‚ä¸è¿‡æ›å…‰å›¾åƒçš„æš—éƒ¨ä¿¡æ¯ã€‚æ¨¡å‹é‡‡ç”¨æ— ç›‘ç£(Unsupervised)æ–¹å¼è®­ç»ƒï¼Œæ— éœ€ä¾èµ–çœŸå®çš„HDRå›¾åƒ(Ground-truth)ï¼Œä½¿å…¶åœ¨ç¼ºä¹æ ‡æ³¨æ•°æ®çš„å®é™…åº”ç”¨åœºæ™¯ä¸­æ›´å…·å¯è¡Œæ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†å®šåˆ¶çš„æŸå¤±å‡½æ•°(Loss function)ä»¥æå‡é‡å»ºä¿çœŸåº¦å¹¶ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡å¤šæ›å…‰èåˆç»“æ„ç›¸ä¼¼æ€§æŒ‡æ ‡(MEF-SSIM)çš„è¯„ä¼°ï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨è§†è§‰è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„èåˆæŠ€æœ¯ï¼Œå®ç°äº†é«˜è´¨é‡çš„HDRå›¾åƒé‡å»ºã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.21815v1",
      "published_date": "2025-10-21 17:43:22 UTC",
      "updated_date": "2025-10-21 17:43:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:44:32.679752+00:00"
    },
    {
      "arxiv_id": "2510.18849v1",
      "title": "Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning",
      "title_zh": "åŸºäºâ€œæ‰¹è¯„-åç¼–è¾‘â€å¼ºåŒ–å­¦ä¹ å®ç°å¿ å®ä¸”å¯æ§çš„ä¸ªæ€§åŒ–",
      "authors": [
        "Chenghao Zhu",
        "Meiling Tao",
        "Tiannan Wang",
        "Dongyi Ding",
        "Yuchen Eleanor Jiang",
        "Wangchunshu Zhou"
      ],
      "abstract": "Faithfully personalizing large language models (LLMs) to align with individual user preferences is a critical but challenging task. While supervised fine-tuning (SFT) quickly reaches a performance plateau, standard reinforcement learning from human feedback (RLHF) also struggles with the nuances of personalization. Scalar-based reward models are prone to reward hacking which leads to verbose and superficially personalized responses. To address these limitations, we propose Critique-Post-Edit, a robust reinforcement learning framework that enables more faithful and controllable personalization. Our framework integrates two key components: (1) a Personalized Generative Reward Model (GRM) that provides multi-dimensional scores and textual critiques to resist reward hacking, and (2) a Critique-Post-Edit mechanism where the policy model revises its own outputs based on these critiques for more targeted and efficient learning. Under a rigorous length-controlled evaluation, our method substantially outperforms standard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an average 11\\% win-rate improvement, and personalized Qwen2.5-14B model surpasses the performance of GPT-4.1. These results demonstrate a practical path to faithful, efficient, and controllable personalization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¸ªæ€§åŒ–è¿‡ç¨‹ä¸­é¢ä¸´çš„ç›‘ç£å¾®è°ƒ(SFT)ç“¶é¢ˆåŠä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ (RLHF)å®¹æ˜“äº§ç”Ÿå¥–åŠ±é»‘å®¢(reward hacking)çš„é—®é¢˜ï¼Œæå‡ºäº†Critique-Post-Editå¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥ä¸ªæ€§åŒ–ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹(Generative Reward Model, GRM)æ¥æä¾›å¤šç»´åº¦è¯„åˆ†å’Œæ–‡æœ¬æ‰¹åˆ¤ï¼Œæœ‰æ•ˆæŠµå¾¡äº†æ¨¡å‹ç”Ÿæˆå†—é•¿ä¸”è¡¨é¢åŒ–å“åº”çš„å€¾å‘ã€‚åŒæ—¶ï¼Œåˆ©ç”¨Critique-Post-Editæœºåˆ¶ä¿ƒä½¿ç­–ç•¥æ¨¡å‹æ ¹æ®æ‰¹åˆ¤åé¦ˆä¿®æ­£å…¶è¾“å‡ºï¼Œæ˜¾è‘—æå‡äº†å­¦ä¹ çš„é’ˆå¯¹æ€§ä¸æ•ˆç‡ã€‚åœ¨é•¿åº¦å—æ§çš„ä¸¥è‹›è¯„ä¼°ä¸‹ï¼Œè¯¥æ–¹æ³•è¡¨ç°å¤§å¹…ä¼˜äºæ ‡å‡†PPOç®—æ³•ï¼Œä¸ªæ€§åŒ–Qwen2.5-7Bçš„å¹³å‡èƒœç‡æå‡äº†11%ï¼Œè€ŒQwen2.5-14Bçš„æ€§èƒ½ç”šè‡³è¶…è¿‡äº†GPT-4.1ã€‚è¿™é¡¹å·¥ä½œä¸ºå®ç°å¿ å®ã€é«˜æ•ˆä¸”å¯æ§çš„ä¸ªæ€§åŒ–å¯¹é½æä¾›äº†ä¸€æ¡åˆ‡å®çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "work in progress",
      "pdf_url": "https://arxiv.org/pdf/2510.18849v1",
      "published_date": "2025-10-21 17:40:03 UTC",
      "updated_date": "2025-10-21 17:40:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:44:43.676257+00:00"
    },
    {
      "arxiv_id": "2510.19850v1",
      "title": "Prompt Decorators: A Declarative and Composable Syntax for Reasoning, Formatting, and Control in LLMs",
      "title_zh": "Prompt Decoratorsï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹æ¨ç†ã€æ ¼å¼åŒ–ä¸æ§åˆ¶çš„å£°æ˜å¼å¯ç»„åˆè¯­æ³•",
      "authors": [
        "Mostapha Kalami Heris"
      ],
      "abstract": "Large Language Models (LLMs) are central to reasoning, writing, and decision-support workflows, yet users lack consistent control over how they reason and express outputs. Conventional prompt engineering relies on verbose natural-language instructions, limiting reproducibility, modularity, and interpretability. This paper introduces Prompt Decorators, a declarative, composable syntax that governs LLM behavior through compact control tokens such as +++Reasoning, +++Tone(style=formal), and +++Import(topic=\"Systems Thinking\"). Each decorator modifies a behavioral dimension, such as reasoning style, structure, or tone, without changing task content. The framework formalizes twenty core decorators organized into two functional families (Cognitive & Generative and Expressive & Systemic), each further decomposed into subcategories that govern reasoning, interaction, expression, and session-control. It defines a unified syntax, scoping model, and deterministic processing pipeline enabling predictable and auditable behavior composition. By decoupling task intent from execution behavior, Prompt Decorators create a reusable and interpretable interface for prompt design. Illustrative use cases demonstrate improved reasoning transparency, reduced prompt complexity, and standardized model behavior across domains. The paper concludes with implications for interoperability, behavioral consistency, and the development of declarative interfaces for scalable AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† Prompt Decoratorsï¼Œè¿™æ˜¯ä¸€ç§ç”¨äº Large Language Models (LLMs) çš„å£°æ˜å¼ä¸”å¯ç»„åˆçš„è¯­æ³•ï¼Œæ—¨åœ¨é€šè¿‡ç´§å‡‘çš„æ§åˆ¶ä»¤ç‰Œï¼ˆå¦‚ +++Reasoning å’Œ +++Toneï¼‰æ¥è§„èŒƒæ¨¡å‹çš„æ¨ç†ã€æ ¼å¼å’Œæ§åˆ¶è¡Œä¸ºã€‚ä¸ºäº†è§£å†³ä¼ ç»Ÿ Prompt Engineering å­˜åœ¨çš„æŒ‡ä»¤å†—é•¿ã€å¯é‡å¤æ€§å·®åŠæ¨¡å—åŒ–ç¨‹åº¦ä½ç­‰é—®é¢˜ï¼Œè¯¥æ¡†æ¶å®ç°äº†ä»»åŠ¡æ„å›¾ä¸æ‰§è¡Œè¡Œä¸ºçš„æœ‰æ•ˆè§£è€¦ã€‚ç ”ç©¶å½¢å¼åŒ–äº† 20 ä¸ªæ ¸å¿ƒè£…é¥°å™¨ï¼Œå¹¶å°†å…¶å½’ç±»ä¸º Cognitive & Generative å’Œ Expressive & Systemic ä¸¤ä¸ªåŠŸèƒ½å®¶æ—ï¼Œåˆ†åˆ«è´Ÿè´£ç®¡ç†æ¨ç†ã€äº¤äº’ã€è¡¨è¾¾å’Œä¼šè¯æ§åˆ¶ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å®šä¹‰ç»Ÿä¸€è¯­æ³•ã€ä½œç”¨åŸŸæ¨¡å‹å’Œç¡®å®šæ€§å¤„ç†æµæ°´çº¿ï¼Œç¡®ä¿äº†è¡Œä¸ºç»„åˆçš„å¯é¢„æµ‹æ€§ä¸å¯å®¡è®¡æ€§ã€‚åº”ç”¨æ¡ˆä¾‹è¯æ˜ï¼ŒPrompt Decorators æ˜¾è‘—æå‡äº†æ¨ç†é€æ˜åº¦ï¼Œé™ä½äº†æç¤ºè¯å¤æ‚åº¦ï¼Œå¹¶åœ¨ä¸åŒé¢†åŸŸé—´å®ç°äº†æ ‡å‡†åŒ–çš„æ¨¡å‹è¡Œä¸ºã€‚è¯¥å·¥ä½œä¸ºæ„å»ºå¯æ‰©å±• AI ç³»ç»Ÿçš„å£°æ˜å¼æ¥å£ä»¥åŠç¡®ä¿è¡Œä¸ºä¸€è‡´æ€§æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.PL",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.PL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.19850v1",
      "published_date": "2025-10-21 17:35:49 UTC",
      "updated_date": "2025-10-21 17:35:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:44:38.684677+00:00"
    },
    {
      "arxiv_id": "2510.18828v1",
      "title": "Actor-Free Continuous Control via Structurally Maximizable Q-Functions",
      "title_zh": "åŸºäºç»“æ„åŒ–å¯æœ€å¤§åŒ– Q å‡½æ•°çš„æ—  Actor è¿ç»­æ§åˆ¶",
      "authors": [
        "Yigit Korkmaz",
        "Urvi Bhuwania",
        "Ayush Jain",
        "Erdem BÄ±yÄ±k"
      ],
      "abstract": "Value-based algorithms are a cornerstone of off-policy reinforcement learning due to their simplicity and training stability. However, their use has traditionally been restricted to discrete action spaces, as they rely on estimating Q-values for individual state-action pairs. In continuous action spaces, evaluating the Q-value over the entire action space becomes computationally infeasible. To address this, actor-critic methods are typically employed, where a critic is trained on off-policy data to estimate Q-values, and an actor is trained to maximize the critic's output. Despite their popularity, these methods often suffer from instability during training. In this work, we propose a purely value-based framework for continuous control that revisits structural maximization of Q-functions, introducing a set of key architectural and algorithmic choices to enable efficient and stable learning. We evaluate the proposed actor-free Q-learning approach on a range of standard simulation tasks, demonstrating performance and sample efficiency on par with state-of-the-art baselines, without the cost of learning a separate actor. Particularly, in environments with constrained action spaces, where the value functions are typically non-smooth, our method with structural maximization outperforms traditional actor-critic methods with gradient-based maximization. We have released our code at https://github.com/USC-Lira/Q3C.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»ŸåŸºäºå€¼çš„(Value-based)ç®—æ³•åœ¨è¿ç»­æ§åˆ¶(Continuous control)é¢†åŸŸç”±äºQå€¼æœ€å¤§åŒ–è®¡ç®—å›°éš¾è€Œå—é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§çº¯ç²¹åŸºäºå€¼çš„Actor-freeæ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ç»“æ„åŒ–å¯æœ€å¤§åŒ–Qå‡½æ•°(Structurally Maximizable Q-Functions)ä»¥åŠä¸€ç³»åˆ—å…³é”®çš„æ¶æ„å’Œç®—æ³•é€‰æ‹©ï¼Œå®ç°äº†é«˜æ•ˆä¸”ç¨³å®šçš„å­¦ä¹ ã€‚åœ¨æ ‡å‡†æ¨¡æ‹Ÿä»»åŠ¡çš„è¯„ä¼°ä¸­ï¼Œè¯¥ç®—æ³•åœ¨æ— éœ€è®­ç»ƒç‹¬ç«‹Actorçš„å‰æä¸‹ï¼Œå±•ç°å‡ºä¸å½“å‰æœ€å…ˆè¿›åŸºå‡†ç›¸å½“çš„æ€§èƒ½å’Œæ ·æœ¬æ•ˆç‡(Sample efficiency)ã€‚ç‰¹åˆ«æ˜¯åœ¨åŠ¨ä½œç©ºé—´å—é™ä¸”ä»·å€¼å‡½æ•°éå¹³æ»‘çš„ç¯å¢ƒä¸‹ï¼Œè¿™ç§åŸºäºç»“æ„åŒ–æœ€å¤§åŒ–çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿçš„åŸºäºæ¢¯åº¦(Gradient-based)çš„Actor-criticæ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2510.18828v1",
      "published_date": "2025-10-21 17:24:27 UTC",
      "updated_date": "2025-10-21 17:24:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:44:41.282831+00:00"
    },
    {
      "arxiv_id": "2510.18826v2",
      "title": "An AI enhanced approach to the tree unimodality conjecture",
      "title_zh": "æ ‘å•å³°æ€§çŒœæƒ³çš„äººå·¥æ™ºèƒ½å¢å¼ºç ”ç©¶æ–¹æ³•",
      "authors": [
        "Eric Ramos",
        "Sunny Sun"
      ],
      "abstract": "Given a graph $G$, its independence sequence is the integral sequence $a_1,a_2,...,a_n$, where $a_i$ is the number of independent sets of vertices of size i. In the late 80's Alavi, Erdos, Malde, Schwenk showed that this sequence need not be unimodal for general graphs, but conjectured that it is always unimodal whenever $G$ is a tree. This conjecture was then naturally generalized to claim that the independence sequence of trees should be log concave, in the sense that $a_i^2$ is always above $a_{i-1}a_{i+1}$. This conjecture stood for many years, until in 2023, Kadrawi, Levit, Yosef, and Mizrachi proved that there were exactly two trees on 26 vertices whose independence sequence was not log concave. In this paper, we use the AI architecture PatternBoost, developed by Charton, Ellenberg, Wagner, and Williamson to train a machine to find counter-examples to the log-concavity conjecture. We will discuss the successes of this approach - finding tens of thousands of new counter-examples to log-concavity with vertex set sizes varying from 27 to 101 - and some of its fascinating failures.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å…³äºæ ‘çš„ç‹¬ç«‹åºåˆ— (independence sequence) çš„å¯¹æ•°å‡¹æ€§ (log-concavity) çŒœæƒ³ï¼Œå³æ ‘çš„ç‹¬ç«‹é›†æ•°é‡åºåˆ—æ˜¯å¦å§‹ç»ˆæ»¡è¶³ $a_i^2 \\ge a_{i-1}a_{i+1}$ã€‚è®ºæ–‡é‡‡ç”¨äº†ä¸€ç§åä¸º PatternBoost çš„äººå·¥æ™ºèƒ½æ¶æ„ï¼Œé€šè¿‡è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹æ¥è‡ªåŠ¨åŒ–æœç´¢è¯¥çŒœæƒ³çš„åä¾‹ã€‚ç ”ç©¶æˆåŠŸå‘ç°äº†æ•°ä»¥ä¸‡è®¡çš„å…¨æ–°åä¾‹ï¼Œè¿™äº›æ ‘çš„é¡¶ç‚¹æ•°èŒƒå›´ä» 27 åˆ° 101ï¼Œæ˜¾è‘—çªç ´äº†æ­¤å‰åœ¨è¯¥é¢†åŸŸä»…å‘ç°çš„ä¸¤ä¸ª 26 é¡¶ç‚¹åä¾‹ã€‚ä½œè€…è¯¦ç»†è®¨è®ºäº†è¯¥ AI æ–¹æ³•åœ¨å¯»æ‰¾åä¾‹è¿‡ç¨‹ä¸­çš„æˆåŠŸå®è·µåŠå…¶å…·æœ‰å¯å‘æ€§çš„å¤±è´¥æ¡ˆä¾‹ã€‚è¿™ä¸€æˆæœä¸ä»…ä¸ºè§£å†³æ ‘çš„å¯¹æ•°å‡¹æ€§çŒœæƒ³æä¾›äº†å¤§é‡å®è¯è¯æ®ï¼Œä¹Ÿå±•ç¤ºäº†äººå·¥æ™ºèƒ½è¾…åŠ©æ•°å­¦ç ”ç©¶åœ¨å¤„ç†å¤æ‚ç»„åˆæ•°å­¦é—®é¢˜ä¸Šçš„æ•ˆç‡ä¸æ½œåŠ›ã€‚",
      "categories": [
        "math.CO",
        "cs.AI",
        "cs.DM",
        "cs.LG"
      ],
      "primary_category": "math.CO",
      "comment": "V2 - Fixed typographical errors. Added a remark noting a private correspondence with Galvin and Bencs, who have shown the existence of trees with log concavity breakage at multiple indices",
      "pdf_url": "https://arxiv.org/pdf/2510.18826v2",
      "published_date": "2025-10-21 17:23:09 UTC",
      "updated_date": "2025-10-22 21:17:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:45:16.173385+00:00"
    },
    {
      "arxiv_id": "2510.18819v1",
      "title": "An Explainable Hybrid AI Framework for Enhanced Tuberculosis and Symptom Detection",
      "title_zh": "ç”¨äºå¢å¼ºè‚ºç»“æ ¸ä¸ç—‡çŠ¶æ£€æµ‹çš„å¯è§£é‡Šæ··åˆäººå·¥æ™ºèƒ½æ¡†æ¶",
      "authors": [
        "Neel Patel",
        "Alexander Wong",
        "Ashkan Ebadi"
      ],
      "abstract": "Tuberculosis remains a critical global health issue, particularly in resource-limited and remote areas. Early detection is vital for treatment, yet the lack of skilled radiologists underscores the need for artificial intelligence (AI)-driven screening tools. Developing reliable AI models is challenging due to the necessity for large, high-quality datasets, which are costly to obtain. To tackle this, we propose a teacher--student framework which enhances both disease and symptom detection on chest X-rays by integrating two supervised heads and a self-supervised head. Our model achieves an accuracy of 98.85% for distinguishing between COVID-19, tuberculosis, and normal cases, and a macro-F1 score of 90.09% for multilabel symptom detection, significantly outperforming baselines. The explainability assessments also show the model bases its predictions on relevant anatomical features, demonstrating promise for deployment in clinical screening and triage settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç»“æ ¸ç—… (Tuberculosis) åœ¨èµ„æºåŒ®ä¹åœ°åŒºè¯Šæ–­å›°éš¾çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…·æœ‰å¯è§£é‡Šæ€§çš„æ•™å¸ˆ-å­¦ç”Ÿ (teacher-student) æ··åˆ AI æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºèƒ¸éƒ¨ X å°„çº¿çš„ç–¾ç—…ä¸ç—‡çŠ¶æ£€æµ‹ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆä¸¤ä¸ªç›‘ç£å¤´ (supervised heads) å’Œä¸€ä¸ªè‡ªç›‘ç£å¤´ (self-supervised head)ï¼Œæœ‰æ•ˆåº”å¯¹äº†é«˜è´¨é‡æ ‡æ³¨æ•°æ®è·å–æˆæœ¬é«˜æ˜‚çš„æŒ‘æˆ˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨åŒºåˆ† COVID-19ã€ç»“æ ¸ç—… (tuberculosis) å’Œæ­£å¸¸ç—…ä¾‹æ–¹é¢è¾¾åˆ°äº† 98.85% çš„å‡†ç¡®ç‡ã€‚åœ¨å¤šæ ‡ç­¾ç—‡çŠ¶æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œè¯¥æ¨¡å‹å–å¾—äº† 90.09% çš„å® F1 åˆ†æ•° (macro-F1 score)ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå¯è§£é‡Šæ€§è¯„ä¼°è¯å®æ¨¡å‹çš„é¢„æµ‹é€»è¾‘åŸºäºç›¸å…³çš„è§£å‰–å­¦ç‰¹å¾ï¼Œè€Œéæ— å…³å¹²æ‰°å› ç´ ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºä¸´åºŠç­›æŸ¥å’Œåˆ†è¯Šæä¾›äº†å¯é å·¥å…·ï¼Œå±•ç°äº†åœ¨çœŸå®åŒ»ç–—ç¯å¢ƒä¸­çš„é«˜åº¦åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.18819v1",
      "published_date": "2025-10-21 17:18:55 UTC",
      "updated_date": "2025-10-21 17:18:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:45:26.696413+00:00"
    },
    {
      "arxiv_id": "2510.18817v1",
      "title": "Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for Industrial Asset Health Monitoring",
      "title_zh": "ç²¾è°ƒæ€ç»´ï¼šåˆ©ç”¨æ€ç»´é“¾æ¨ç†å®ç°å·¥ä¸šèµ„äº§å¥åº·ç›‘æµ‹",
      "authors": [
        "Shuxin Lin",
        "Dhaval Patel",
        "Christodoulos Constantinides"
      ],
      "abstract": "Small Language Models (SLMs) are becoming increasingly popular in specialized fields, such as industrial applications, due to their efficiency, lower computational requirements, and ability to be fine-tuned for domain-specific tasks, enabling accurate and cost-effective solutions. However, performing complex reasoning using SLMs in specialized fields such as Industry 4.0 remains challenging. In this paper, we propose a knowledge distillation framework for industrial asset health, which transfers reasoning capabilities via Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) to smaller, more efficient models (SLMs). We discuss the advantages and the process of distilling LLMs using multi-choice question answering (MCQA) prompts to enhance reasoning and refine decision-making. We also perform in-context learning to verify the quality of the generated knowledge and benchmark the performance of fine-tuned SLMs with generated knowledge against widely used LLMs. The results show that the fine-tuned SLMs with CoT reasoning outperform the base models by a significant margin, narrowing the gap to their LLM counterparts. Our code is open-sourced at: https://github.com/IBM/FailureSensorIQ.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥ä¸š 4.0 é¢†åŸŸä¸­å°å‹è¯­è¨€æ¨¡å‹ (SLMs) åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§ä¸“é—¨ç”¨äºå·¥ä¸šèµ„äº§å¥åº·ç›‘æµ‹çš„çŸ¥è¯†è’¸é¦æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é“¾å¼æ€ç»´ (Chain-of-Thought, CoT) è’¸é¦æŠ€æœ¯ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„æ¨ç†èƒ½åŠ›æœ‰æ•ˆè½¬ç§»è‡³æ›´è½»é‡ã€æ›´é«˜æ•ˆçš„ SLMsã€‚ç ”ç©¶é€šè¿‡å¤šé¡¹é€‰æ‹©é—®ç­” (MCQA) æç¤ºæ¥å¢å¼ºæ¨ç†é€»è¾‘å¹¶ä¼˜åŒ–å†³ç­–è¿‡ç¨‹ï¼Œå¹¶åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹  (In-context learning) éªŒè¯ç”ŸæˆçŸ¥è¯†çš„è´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œèåˆäº† CoT æ¨ç†çš„å¾®è°ƒ SLMs æ€§èƒ½æ˜¾è‘—ä¼˜äºå…¶åŸºç¡€æ¨¡å‹ï¼Œå¤§å¹…ç¼©å°äº†ä¸ LLMs ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚è¿™é¡¹å·¥ä½œä¸ºå·¥ä¸šé¢†åŸŸæä¾›äº†ä¸€ç§å…¼é¡¾å‡†ç¡®æ€§ä¸æˆæœ¬æ•ˆç›Šçš„èµ„äº§ç›‘æµ‹æ–¹æ¡ˆï¼Œç›¸å…³ä»£ç å·²åœ¨ GitHub å¼€æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.18817v1",
      "published_date": "2025-10-21 17:18:24 UTC",
      "updated_date": "2025-10-21 17:18:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:45:21.605697+00:00"
    },
    {
      "arxiv_id": "2510.18814v1",
      "title": "Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning without Rewards",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹æ¨ç†çš„åœ¨çº¿ SFTï¼šæ— éœ€å¥–åŠ±çš„è‡ªæˆ‘è°ƒä¼˜åŠå…¶æ˜¾è‘—æœ‰æ•ˆæ€§",
      "authors": [
        "Mengqi Li",
        "Lei Zhao",
        "Anthony Man-Cho So",
        "Ruoyu Sun",
        "Xiao Li"
      ],
      "abstract": "We present a simple, self-help online supervised finetuning (OSFT) paradigm for LLM reasoning. In this paradigm, the model generates its own responses and is immediately finetuned on this self-generated data. OSFT is a highly efficient training strategy for LLM reasoning, as it is reward-free and uses just one rollout by default. Experiment results show that OSFT achieves downstream performance on challenging mathematical reasoning tasks comparable to strong reinforcement learning with verifiable rewards (RLVR) methods such as GRPO. Our ablation study further demonstrates the efficiency and robustness of OSFT. The major mechanism of OSFT lies in facilitating the model's own existing preference (latent knowledge) learned from pretraining, which leads to reasoning ability improvement. We believe that OSFT offers an efficient and promising alternative to more complex, reward-based training paradigms. Our code is available at https://github.com/ElementQi/OnlineSFT.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Online SFT (OSFT) çš„ç®€å•ä¸”è‡ªå‘çš„åœ¨çº¿ç›‘ç£å¾®è°ƒèŒƒå¼ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) çš„æ¨ç†èƒ½åŠ›ã€‚åœ¨è¿™ç§èŒƒå¼ä¸‹ï¼Œæ¨¡å‹ç”Ÿæˆè‡ªèº«çš„å›å¤å¹¶ç«‹å³åŸºäºè¿™äº›è‡ªç”Ÿæˆæ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä¸”é»˜è®¤ä»…éœ€ä¸€æ¬¡é‡‡æ · (rollout) ä¸”æ— éœ€ä»»ä½•å¤–éƒ¨å¥–åŠ±ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOSFT åœ¨æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…¶æ€§èƒ½å¯ä¸ GRPO ç­‰å¼ºåŠ›å¼ºåŒ–å­¦ä¹  (RLVR) æ–¹æ³•ç›¸åª²ç¾ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº† OSFT çš„é«˜æ•ˆæ€§ä¸é²æ£’æ€§ï¼ŒæŒ‡å‡ºå…¶æ ¸å¿ƒæœºåˆ¶åœ¨äºæ¿€æ´»æ¨¡å‹åœ¨é¢„è®­ç»ƒä¸­ä¹ å¾—çš„æ½œè—çŸ¥è¯† (latent knowledge)ã€‚è¯¥ç ”ç©¶è¡¨æ˜ OSFT ä¸ºå¤æ‚ä¸”åŸºäºå¥–åŠ±çš„è®­ç»ƒæ–¹æ³•æä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”æå…·æ½œåŠ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18814v1",
      "published_date": "2025-10-21 17:15:56 UTC",
      "updated_date": "2025-10-21 17:15:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:45:23.419988+00:00"
    },
    {
      "arxiv_id": "2510.18803v1",
      "title": "Decoding Funded Research: Comparative Analysis of Topic Models and Uncovering the Effect of Gender and Geographic Location",
      "title_zh": "è§£ç å—èµ„åŠ©ç ”ç©¶ï¼šä¸»é¢˜æ¨¡å‹å¯¹æ¯”åˆ†æåŠæ€§åˆ«ä¸åœ°ç†ä½ç½®çš„å½±å“æ•ˆåº”æ¢ç©¶",
      "authors": [
        "Shirin Tavakoli Kafiabad",
        "Andrea Schiffauerova",
        "Ashkan Ebadi"
      ],
      "abstract": "Optimizing national scientific investment requires a clear understanding of evolving research trends and the demographic and geographical forces shaping them, particularly in light of commitments to equity, diversity, and inclusion. This study addresses this need by analyzing 18 years (2005-2022) of research proposals funded by the Natural Sciences and Engineering Research Council of Canada (NSERC). We conducted a comprehensive comparative evaluation of three topic modelling approaches: Latent Dirichlet Allocation (LDA), Structural Topic Modelling (STM), and BERTopic. We also introduced a novel algorithm, named COFFEE, designed to enable robust covariate effect estimation for BERTopic. This advancement addresses a significant gap, as BERTopic lacks a native function for covariate analysis, unlike the probabilistic STM. Our findings highlight that while all models effectively delineate core scientific domains, BERTopic outperformed by consistently identifying more granular, coherent, and emergent themes, such as the rapid expansion of artificial intelligence. Additionally, the covariate analysis, powered by COFFEE, confirmed distinct provincial research specializations and revealed consistent gender-based thematic patterns across various scientific disciplines. These insights offer a robust empirical foundation for funding organizations to formulate more equitable and impactful funding strategies, thereby enhancing the effectiveness of the scientific ecosystem.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ†æäº†åŠ æ‹¿å¤§è‡ªç„¶ç§‘å­¦ä¸å·¥ç¨‹ç ”ç©¶å§”å‘˜ä¼š (NSERC) åœ¨ 2005 å¹´è‡³ 2022 å¹´é—´èµ„åŠ©çš„ç ”ç©¶ææ¡ˆï¼Œæ—¨åœ¨ç†è§£ç ”ç©¶è¶‹åŠ¿åŠå…¶å—æ€§åˆ«å’Œåœ°ç†å› ç´ çš„å½±å“ã€‚ç ”ç©¶å¯¹æ¯”è¯„ä¼°äº† Latent Dirichlet Allocation (LDA)ã€Structural Topic Modelling (STM) å’Œ BERTopic ä¸‰ç§ä¸»é¢˜å»ºæ¨¡æ–¹æ³•ã€‚ä¸ºå¼¥è¡¥ BERTopic ç¼ºä¹åŸç”Ÿåå˜é‡åˆ†æåŠŸèƒ½çš„çŸ­æ¿ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º COFFEE çš„æ–°ç®—æ³•ï¼Œå®ç°äº†ç¨³å¥çš„åå˜é‡æ•ˆåº”ä¼°è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBERTopic åœ¨è¯†åˆ«ç»†ç²’åº¦ã€è¿è´¯ä¸”æ–°å…´ä¸»é¢˜ï¼ˆå¦‚äººå·¥æ™ºèƒ½çš„æ‰©å¼ ï¼‰æ–¹é¢è¡¨ç°æœ€ä¼˜ã€‚é€šè¿‡ COFFEE è¿›è¡Œçš„åˆ†æè¯å®äº†ä¸åŒçœä»½çš„ç ”ç©¶ä¸“ä¸šåŒ–å·®å¼‚ï¼Œå¹¶æ­ç¤ºäº†å„å­¦ç§‘ä¸­åŸºäºæ€§åˆ«çš„ç‰¹å®šä¸»é¢˜æ¨¡å¼ã€‚è¿™äº›æ´å¯Ÿä¸ºèµ„åŠ©æœºæ„åˆ¶å®šæ›´å…¬å¹³ã€æ›´é«˜æ•ˆçš„ç§‘å­¦èµ„åŠ©ç­–ç•¥æä¾›äº†å®è¯åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "35 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.18803v1",
      "published_date": "2025-10-21 16:58:00 UTC",
      "updated_date": "2025-10-21 16:58:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:45:25.687863+00:00"
    },
    {
      "arxiv_id": "2510.18802v4",
      "title": "Computational Foundations for Strategic Coopetition: Formalizing Interdependence and Complementarity",
      "title_zh": "æˆ˜ç•¥ç«åˆçš„è®¡ç®—åŸºç¡€ï¼šç›¸äº’ä¾èµ–ä¸äº’è¡¥æ€§çš„å½¢å¼åŒ–ç ”ç©¶",
      "authors": [
        "Vik Pant",
        "Eric Yu"
      ],
      "abstract": "Coopetition refers to simultaneous cooperation and competition among actors who \"cooperate to grow the pie and compete to split it up.\" Modern socio-technical systems are characterized by strategic coopetition in which actors concomitantly cooperate to create value and compete to capture it. While conceptual modeling languages such as i* provide rich qualitative representations of strategic dependencies, they lack mechanisms for quantitative analysis of dynamic trade-offs. Conversely, classical game theory offers mathematical rigor but strips away contextual richness. This technical report bridges this gap by developing computational foundations that formalize two critical dimensions of coopetition: interdependence and complementarity. We ground interdependence in i* structural dependency analysis, translating depender-dependee-dependum relationships into quantitative interdependence coefficients through a structured translation framework. We formalize complementarity following Brandenburger and Nalebuff's Added Value concept, modeling synergistic value creation with validated parameterization. We integrate structural dependencies with bargaining power in value appropriation and introduce a game-theoretic formulation where Nash Equilibrium incorporates structural interdependence. Validation combines comprehensive experimental testing comprising over 22,000 trials across power and logarithmic value function specifications, demonstrating functional form robustness, with empirical application to the Samsung-Sony S-LCD joint venture (2004-2011). This technical report serves as the foundational reference for a coordinated research program examining strategic coopetition in multi-agent systems, with companion work addressing trust dynamics, collective action, and reciprocity mechanisms.",
      "tldr_zh": "è¯¥ç ”ç©¶ä¸ºæˆ˜ç•¥ç«åˆ(Strategic Coopetition)å»ºç«‹äº†è®¡ç®—åŸºç¡€ï¼Œæ—¨åœ¨å¼¥è¡¥å®šæ€§å»ºæ¨¡è¯­è¨€i*ä¸å¤å…¸åšå¼ˆè®º(Game Theory)ä¹‹é—´çš„é¸¿æ²Ÿã€‚è®ºæ–‡é€šè¿‡æ­£å¼å®šä¹‰ç›¸äº’ä¾èµ–(Interdependence)å’Œäº’è¡¥æ€§(Complementarity)ä¸¤ä¸ªæ ¸å¿ƒç»´åº¦ï¼Œæå‡ºäº†ä¸€å¥—å°†i*ç»“æ„ä¾èµ–å…³ç³»è½¬åŒ–ä¸ºå®šé‡ç›¸äº’ä¾èµ–ç³»æ•°çš„ç¿»è¯‘æ¡†æ¶ã€‚ç ”ç©¶åŸºäºé™„åŠ å€¼(Added Value)æ¦‚å¿µå¯¹ååŒä»·å€¼åˆ›é€ è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶å¼•å…¥äº†ç»“åˆç»“æ„ä¾èµ–æ€§çš„çº³ä»€å‡è¡¡(Nash Equilibrium)åšå¼ˆè®ºå…¬å¼ï¼Œä»¥åˆ†æä»·å€¼è·å–ä¸­çš„è®®ä»·èƒ½åŠ›ã€‚é€šè¿‡è¶…è¿‡22,000æ¬¡å®éªŒæµ‹è¯•ä»¥åŠå¯¹ä¸‰æ˜Ÿ-ç´¢å°¼S-LCDåˆèµ„ä¼ä¸šçš„å®è¯åº”ç”¨ï¼ŒéªŒè¯äº†è¯¥æ¨¡å‹åœ¨å¤„ç†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(Multi-agent Systems)ä¸­åŠ¨æ€æƒè¡¡é—®é¢˜çš„é²æ£’æ€§ã€‚è¯¥æˆæœä¸ºæ·±å…¥ç ”ç©¶å¤æ‚ç¤¾ä¼šæŠ€æœ¯ç³»ç»Ÿä¸­çš„ä¿¡ä»»åŠ¨æ€ã€é›†ä½“è¡ŒåŠ¨å’Œäº’æƒ æœºåˆ¶æä¾›äº†å…³é”®çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.MA",
      "comment": "39 pages, 9 figures, Validation artifacts including source code available at https://github.com/vikpant/strategic-coopetition/tree/master/TR_validation/TR1_foundations",
      "pdf_url": "https://arxiv.org/pdf/2510.18802v4",
      "published_date": "2025-10-21 16:57:40 UTC",
      "updated_date": "2025-12-26 00:45:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:45:26.420901+00:00"
    },
    {
      "arxiv_id": "2510.18751v2",
      "title": "Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation",
      "title_zh": "Seg the HABï¼šè¯­è¨€å¼•å¯¼çš„åœ°ç†ç©ºé—´è—»åæ¨ç†ä¸åˆ†å‰²",
      "authors": [
        "Patterson Hsieh",
        "Jerry Yeh",
        "Mao-Chi He",
        "Wen-Han Hsieh",
        "Elvis Hsieh"
      ],
      "abstract": "Climate change is intensifying the occurrence of harmful algal bloom (HAB), particularly cyanobacteria, which threaten aquatic ecosystems and human health through oxygen depletion, toxin release, and disruption of marine biodiversity. Traditional monitoring approaches, such as manual water sampling, remain labor-intensive and limited in spatial and temporal coverage. Recent advances in vision-language models (VLMs) for remote sensing have shown potential for scalable AI-driven solutions, yet challenges remain in reasoning over imagery and quantifying bloom severity. In this work, we introduce ALGae Observation and Segmentation (ALGOS), a segmentation-and-reasoning system for HAB monitoring that combines remote sensing image understanding with severity estimation. Our approach integrates GeoSAM-assisted human evaluation for high-quality segmentation mask curation and fine-tunes vision language model on severity prediction using the Cyanobacteria Aggregated Manual Labels (CAML) from NASA. Experiments demonstrate that ALGOS achieves robust performance on both segmentation and severity-level estimation, paving the way toward practical and automated cyanobacterial monitoring systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ALGae Observation and Segmentation (ALGOS)ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†é¥æ„Ÿå›¾åƒç†è§£ä¸ä¸¥é‡ç¨‹åº¦ä¼°ç®—çš„æœ‰å®³è—»ç±»çˆ†å‘ (Harmful Algal Bloom, HAB) åˆ†å‰²ä¸æ¨ç†ç³»ç»Ÿã€‚ä¸ºè§£å†³ä¼ ç»Ÿç›‘æµ‹æ–¹æ³•åœ¨ç©ºé—´å’Œæ—¶é—´è¦†ç›–ä¸Šçš„å±€é™ï¼Œè¯¥ç³»ç»Ÿé›†æˆäº†GeoSAMè¾…åŠ©çš„äººå·¥è¯„ä¼°æµç¨‹ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡çš„åˆ†å‰²æ©ç ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨æ¥è‡ªNASAçš„Cyanobacteria Aggregated Manual Labels (CAML) æ•°æ®é›†å¯¹è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) è¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹è“è—»ä¸¥é‡ç¨‹åº¦çš„é¢„æµ‹èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒALGOSåœ¨åˆ†å‰²å’Œä¸¥é‡ç¨‹åº¦ç­‰çº§ä¼°ç®—ä»»åŠ¡ä¸­å‡å±•ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚è¯¥å·¥ä½œä¸ºå®ç°è‡ªåŠ¨åŒ–ã€å¯æ‰©å±•çš„è“è—»ç›‘æµ‹ç³»ç»Ÿå¼€è¾Ÿäº†æ–°è·¯å¾„ï¼Œæœ‰åŠ©äºåº”å¯¹æ°”å€™å˜åŒ–å¯¹æ°´ç”Ÿç”Ÿæ€ç³»ç»Ÿå’Œäººç±»å¥åº·é€ æˆçš„å¨èƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18751v2",
      "published_date": "2025-10-21 15:59:00 UTC",
      "updated_date": "2025-11-05 22:17:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:45:36.089721+00:00"
    },
    {
      "arxiv_id": "2510.18731v1",
      "title": "Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate Lost-in-Conversation",
      "title_zh": "é€šè¿‡è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ ä¸­çš„å¯éªŒè¯å‡†ç¡®æ€§ä¸å¼ƒæƒå¥–åŠ±ç¼“è§£ Lost-in-Conversation ç°è±¡",
      "authors": [
        "Ming Li"
      ],
      "abstract": "Large Language Models demonstrate strong capabilities in single-turn instruction following but suffer from Lost-in-Conversation (LiC), a degradation in performance as information is revealed progressively in multi-turn settings. Motivated by the current progress on Reinforcement Learning with Verifiable Rewards (RLVR), we propose Curriculum Reinforcement Learning with Verifiable Accuracy and Abstention Rewards (RLAAR), a framework that encourages models not only to generate correct answers, but also to judge the solvability of questions in the multi-turn conversation setting. Our approach employs a competence-gated curriculum that incrementally increases dialogue difficulty (in terms of instruction shards), stabilizing training while promoting reliability. Using multi-turn, on-policy rollouts and a mixed-reward system, RLAAR teaches models to balance problem-solving with informed abstention, reducing premature answering behaviors that cause LiC. Evaluated on LiC benchmarks, RLAAR significantly mitigates LiC performance decay (62.6% to 75.1%) and improves calibrated abstention rates (33.5% to 73.4%). Together, these results provide a practical recipe for building multi-turn reliable and trustworthy LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ä¸­æ€§èƒ½é€€åŒ–çš„ Lost-in-Conversation (LiC) é—®é¢˜ï¼Œæå‡ºäº†åŸºäºè¯¾ç¨‹å¼ºåŒ–å­¦ä¹ çš„ RLAAR (Curriculum Reinforcement Learning with Verifiable Accuracy and Abstention Rewards) æ¡†æ¶ã€‚RLAAR ç»“åˆäº†å¯éªŒè¯çš„å‡†ç¡®æ€§ä¸å¼ƒæƒå¥–åŠ±ï¼Œæ—¨åœ¨ä¿ƒä½¿æ¨¡å‹ä¸ä»…èƒ½ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆï¼Œè¿˜èƒ½åœ¨å¤šè½®å¯¹è¯èƒŒæ™¯ä¸‹å‡†ç¡®åˆ¤æ–­é—®é¢˜çš„å¯è§£æ€§ã€‚è¯¥æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§èƒ½åŠ›é—¨æ§è¯¾ç¨‹ (competence-gated curriculum)ï¼Œé€šè¿‡é€æ­¥å¢åŠ å¯¹è¯éš¾åº¦ï¼ˆæŒ‡ä»¤åˆ†ç‰‡ï¼‰æ¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹å¹¶æé«˜ç³»ç»Ÿå¯é æ€§ã€‚é€šè¿‡å¤šè½®ç­–ç•¥å†…é‡‡æ · (on-policy rollouts) å’Œæ··åˆå¥–åŠ±ç³»ç»Ÿï¼ŒRLAAR ä½¿æ¨¡å‹åœ¨è§£å†³é—®é¢˜ä¸çŸ¥æƒ…å¼ƒæƒ (informed abstention) ä¹‹é—´è¾¾æˆå¹³è¡¡ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘äº†å¯¼è‡´ LiC çš„è¿‡æ—©å›ç­”è¡Œä¸ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ LiC åŸºå‡†æµ‹è¯•ä¸­å°†æ€§èƒ½è¡¨ç°ä» 62.6% æå‡è‡³ 75.1%ï¼Œå¹¶å°†æ ¡å‡†åçš„å¼ƒæƒç‡ (calibrated abstention rates) ä» 33.5% æ˜¾è‘—æé«˜åˆ° 73.4%ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºæ›´å…·å¯é æ€§å’Œé€æ˜åº¦çš„å¤šè½®å¯¹è¯å¤§è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€å¥—å®ç”¨çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18731v1",
      "published_date": "2025-10-21 15:32:26 UTC",
      "updated_date": "2025-10-21 15:32:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:45:42.090830+00:00"
    },
    {
      "arxiv_id": "2510.18728v1",
      "title": "HarmNet: A Framework for Adaptive Multi-Turn Jailbreak Attacks on Large Language Models",
      "title_zh": "HarmNetï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„è‡ªé€‚åº”å¤šè½®è¶Šç‹±æ”»å‡»æ¡†æ¶",
      "authors": [
        "Sidhant Narula",
        "Javad Rafiei Asl",
        "Mohammad Ghasemigol",
        "Eduardo Blanco",
        "Daniel Takabi"
      ],
      "abstract": "Large Language Models (LLMs) remain vulnerable to multi-turn jailbreak attacks. We introduce HarmNet, a modular framework comprising ThoughtNet, a hierarchical semantic network; a feedback-driven Simulator for iterative query refinement; and a Network Traverser for real-time adaptive attack execution. HarmNet systematically explores and refines the adversarial space to uncover stealthy, high-success attack paths. Experiments across closed-source and open-source LLMs show that HarmNet outperforms state-of-the-art methods, achieving higher attack success rates. For example, on Mistral-7B, HarmNet achieves a 99.4% attack success rate, 13.9% higher than the best baseline. Index terms: jailbreak attacks; large language models; adversarial framework; query refinement.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HarmNetï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸º Large Language Models (LLMs) è®¾è®¡çš„è‡ªé€‚åº”å¤šè½®è¶Šç‹±æ”»å‡» (jailbreak attacks) æ¨¡å—åŒ–æ¡†æ¶ã€‚HarmNet ç”±åˆ†å±‚è¯­ä¹‰ç½‘ç»œ ThoughtNetã€ç”¨äºè¿­ä»£æŸ¥è¯¢ä¼˜åŒ– (query refinement) çš„åé¦ˆé©±åŠ¨æ¨¡æ‹Ÿå™¨ Simulator ä»¥åŠç”¨äºå®æ—¶æ‰§è¡Œè‡ªé€‚åº”æ”»å‡»çš„ç½‘ç»œéå†å™¨ Network Traverser ç»„æˆã€‚é€šè¿‡ç³»ç»Ÿæ€§åœ°æ¢ç´¢å’Œç²¾ç‚¼å¯¹æŠ—ç©ºé—´ï¼Œè¯¥æ¡†æ¶èƒ½å¤ŸæŒ–æ˜å‡ºéšè”½ä¸”æˆåŠŸç‡é«˜çš„æ”»å‡»è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHarmNet åœ¨å¼€æºå’Œé—­æº LLMs ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨ Mistral-7B æ¨¡å‹ä¸Šï¼ŒHarmNet è¾¾åˆ°äº† 99.4% çš„æ”»å‡»æˆåŠŸç‡ï¼Œæ¯”æœ€ä¼˜åŸºå‡†æ¨¡å‹é«˜å‡º 13.9%ï¼Œå±•ç¤ºäº†å…¶åœ¨è¯„ä¼°å¤§æ¨¡å‹å®‰å…¨æ€§æ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "This paper has been accepted for presentation at the Conference on Applied Machine Learning in Information Security (CAMLIS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2510.18728v1",
      "published_date": "2025-10-21 15:28:20 UTC",
      "updated_date": "2025-10-21 15:28:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:45:40.575060+00:00"
    },
    {
      "arxiv_id": "2510.18719v1",
      "title": "Causally Perturbed Fairness Testing",
      "title_zh": "å› æœæ‰°åŠ¨å…¬å¹³æ€§æµ‹è¯•",
      "authors": [
        "Chengwen Du",
        "Tao Chen"
      ],
      "abstract": "To mitigate unfair and unethical discrimination over sensitive features (e.g., gender, age, or race), fairness testing plays an integral role in engineering systems that leverage AI models to handle tabular data. A key challenge therein is how to effectively reveal fairness bugs under an intractable sample size using perturbation. Much current work has been focusing on designing the test sample generators, ignoring the valuable knowledge about data characteristics that can help guide the perturbation and hence limiting their full potential. In this paper, we seek to bridge such a gap by proposing a generic framework of causally perturbed fairness testing, dubbed CausalFT. Through causal inference, the key idea of CausalFT is to extract the most directly and causally relevant non-sensitive feature to its sensitive counterpart, which can jointly influence the prediction of the label. Such a causal relationship is then seamlessly injected into the perturbation to guide a test sample generator. Unlike existing generator-level work, CausalFT serves as a higher-level framework that can be paired with diverse base generators. Extensive experiments on 1296 cases confirm that CausalFT can considerably improve arbitrary base generators in revealing fairness bugs over 93% of the cases with acceptable extra runtime overhead. Compared with a state-of-the-art approach that ranks the non-sensitive features solely based on correlation, CausalFT performs significantly better on 64% cases while being much more efficient. Further, CausalFT can better improve bias resilience in nearly all cases.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨å¤„ç†è¡¨æ ¼æ•°æ®æ—¶é¢ä¸´çš„ä¸å…¬å¹³å’Œä¸é“å¾·æ­§è§†é—®é¢˜ï¼ŒæŒ‡å‡ºå…¬å¹³æ€§æµ‹è¯• (fairness testing) åœ¨æ­ç¤ºå…¬å¹³æ€§ç¼ºé™·æ–¹é¢é¢ä¸´æ ·æœ¬ç©ºé—´éš¾ä»¥å¤„ç†çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„å› æœæ‰°åŠ¨å…¬å¹³æ€§æµ‹è¯•æ¡†æ¶ CausalFTï¼Œæ—¨åœ¨é€šè¿‡å› æœæ¨ç† (causal inference) æå–ä¸æ•æ„Ÿç‰¹å¾ (sensitive features) å…·æœ‰æœ€ç›´æ¥å› æœç›¸å…³æ€§çš„éæ•æ„Ÿç‰¹å¾ã€‚è¯¥æ¡†æ¶å°†æå–çš„å› æœå…³ç³»æ— ç¼æ³¨å…¥åˆ°æ‰°åŠ¨è¿‡ç¨‹ä¸­ï¼Œä»è€ŒæŒ‡å¯¼æµ‹è¯•æ ·æœ¬ç”Ÿæˆå™¨æ›´é«˜æ•ˆåœ°å·¥ä½œã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCausalFT åœ¨ 1296 ä¸ªæ¡ˆä¾‹ä¸­çš„ 93% ä»¥ä¸Šæ˜¾è‘—æå‡äº†åŸºç¡€ç”Ÿæˆå™¨ (base generators) å‘ç°å…¬å¹³æ€§ç¼ºé™·çš„èƒ½åŠ›ã€‚ç›¸æ¯”äºä»…ä¾èµ–ç›¸å…³æ€§ (correlation) æ’åºçš„ç°æœ‰æŠ€æœ¯ï¼ŒCausalFT åœ¨ 64% çš„æ¡ˆä¾‹ä¸­è¡¨ç°æ›´ä¼˜ä¸”æ•ˆç‡æ›´é«˜ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨å‡ ä¹æ‰€æœ‰æµ‹è¯•æ¡ˆä¾‹ä¸­éƒ½å¢å¼ºäº†æ¨¡å‹çš„åå·®éŸ§æ€§ (bias resilience)ï¼Œè¯æ˜äº†å…¶åœ¨æ„å»ºå¯é  AI ç³»ç»Ÿä¸­çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "accepted by TOSEM",
      "pdf_url": "https://arxiv.org/pdf/2510.18719v1",
      "published_date": "2025-10-21 15:20:30 UTC",
      "updated_date": "2025-10-21 15:20:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:45:43.495656+00:00"
    },
    {
      "arxiv_id": "2510.18713v2",
      "title": "Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options",
      "title_zh": "è¶…è¶Šä¸¤ä¸¤æ¯”è¾ƒçš„åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ ï¼šå¤šé¡¹é€‰æ‹©çš„ä¼˜åŠ¿",
      "authors": [
        "Joongkyu Lee",
        "Seouh-won Yi",
        "Min-hwan Oh"
      ],
      "abstract": "We study online preference-based reinforcement learning (PbRL) with the goal of improving sample efficiency. While a growing body of theoretical work has emerged-motivated by PbRL's recent empirical success, particularly in aligning large language models (LLMs)-most existing studies focus only on pairwise comparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024, Thekumparampil et al., 2024) have explored using multiple comparisons and ranking feedback, but their performance guarantees fail to improve-and can even deteriorate-as the feedback length increases, despite the richer information available. To address this gap, we adopt the Plackett-Luce (PL) model for ranking feedback over action subsets and propose M-AUPO, an algorithm that selects multiple actions by maximizing the average uncertainty within the offered subset. We prove that M-AUPO achieves a suboptimality gap of $\\tilde{O}\\left( \\frac{d}{T} \\sqrt{ \\sum_{t=1}^T \\frac{1}{|S_t|}} \\right)$, where $T$ is the total number of rounds, $d$ is the feature dimension, and $|S_t|$ is the size of the subset at round $t$. This result shows that larger subsets directly lead to improved performance and, notably, the bound avoids the exponential dependence on the unknown parameter's norm, which was a fundamental limitation in most previous works. Moreover, we establish a near-matching lower bound of $Î©\\left( \\frac{d}{K \\sqrt{T}} \\right)$, where $K$ is the maximum subset size. To the best of our knowledge, this is the first theoretical result in PbRL with ranking feedback that explicitly shows improved sample efficiency as a function of the subset size.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨çº¿åŸºäºåå¥½çš„å¼ºåŒ–å­¦ä¹ (Preference-based Reinforcement Learning, PbRL)ï¼Œæ—¨åœ¨é€šè¿‡è¶…è¶Šä¼ ç»Ÿçš„æˆå¯¹æ¯”è¾ƒ(Pairwise Comparisons)æ¥æé«˜æ ·æœ¬æ•ˆç‡ã€‚é’ˆå¯¹ç°æœ‰ç ”ç©¶åœ¨å¤„ç†æ’åºåé¦ˆæ—¶æ€§èƒ½ä¿è¯æœªèƒ½éšåé¦ˆé•¿åº¦å¢åŠ è€Œæ”¹å–„çš„å±€é™ï¼Œä½œè€…é‡‡ç”¨äº†Plackett-Luce (PL)æ¨¡å‹ï¼Œå¹¶æå‡ºäº†M-AUPOç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡æœ€å¤§åŒ–å€™é€‰å­é›†å†…çš„å¹³å‡ä¸ç¡®å®šæ€§æ¥é€‰æ‹©å¤šä¸ªåŠ¨ä½œã€‚ç†è®ºè¯æ˜è¡¨æ˜ï¼ŒM-AUPOå®ç°äº†ä¸å­é›†å¤§å°è´Ÿç›¸å…³çš„æ¬¡ä¼˜æ€§é—´éš™(Suboptimality Gap)ï¼Œä¸”æˆåŠŸé¿å¼€äº†ä»¥å¾€ç ”ç©¶ä¸­å¯¹æœªçŸ¥å‚æ•°èŒƒæ•°çš„æŒ‡æ•°çº§ä¾èµ–ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜ç¡®ç«‹äº†æ¥è¿‘åŒ¹é…çš„ä¸‹ç•Œï¼Œåœ¨ç†è®ºå±‚é¢æ˜ç¡®å±•ç¤ºäº†æ’åºåé¦ˆä¸‹çš„æ ·æœ¬æ•ˆç‡å¦‚ä½•éšå­é›†è§„æ¨¡çš„æ‰©å¤§è€Œæå‡ã€‚è¿™ä¸€æˆæœä¸ºåˆ©ç”¨å¤šé€‰é¡¹åé¦ˆä¼˜åŒ–PbRLç³»ç»Ÿæä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ä¸ç®—æ³•æ”¹è¿›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.18713v2",
      "published_date": "2025-10-21 15:11:01 UTC",
      "updated_date": "2025-11-11 14:16:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:45:54.896259+00:00"
    },
    {
      "arxiv_id": "2510.18699v1",
      "title": "Fetch.ai: An Architecture for Modern Multi-Agent Systems",
      "title_zh": "Fetch.aiï¼šé¢å‘ç°ä»£å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ¶æ„",
      "authors": [
        "Michael J. Wooldridge",
        "Attila Bagoly",
        "Jonathan J. Ward",
        "Emanuele La Malfa",
        "Gabriel Paludo Licks"
      ],
      "abstract": "Recent surges in LLM-driven intelligent systems largely overlook decades of foundational multi-agent systems (MAS) research, resulting in frameworks with critical limitations such as centralization and inadequate trust and communication protocols. This paper introduces the Fetch.ai architecture, an industrial-strength platform designed to bridge this gap by facilitating the integration of classical MAS principles with modern AI capabilities. We present a novel, multi-layered solution built on a decentralized foundation of on-chain blockchain services for verifiable identity, discovery, and transactions. This is complemented by a comprehensive development framework for creating secure, interoperable agents, a cloud-based platform for deployment, and an intelligent orchestration layer where an agent-native LLM translates high-level human goals into complex, multi-agent workflows. We demonstrate the deployed nature of this system through a decentralized logistics use case where autonomous agents dynamically discover, negotiate, and transact with one another securely. Ultimately, the Fetch.ai stack provides a principled architecture for moving beyond current agent implementations towards open, collaborative, and economically sustainable multi-agent ecosystems.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† Fetch.ai æ¶æ„ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å°†ç»å…¸å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (MAS) åŸåˆ™ä¸ç°ä»£äººå·¥æ™ºèƒ½ (AI) èƒ½åŠ›ç›¸ç»“åˆçš„å·¥ä¸šçº§å¹³å°ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ™ºèƒ½ç³»ç»Ÿåœ¨ä¸­å¿ƒåŒ–ã€ä¿¡ä»»å’Œé€šä¿¡åè®®æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¶æ„å»ºç«‹åœ¨åŸºäºåŒºå—é“¾ (blockchain) æœåŠ¡çš„å»ä¸­å¿ƒåŒ–åº•å±‚ä¹‹ä¸Šï¼Œä¸ºå¯éªŒè¯èº«ä»½ã€æœåŠ¡å‘ç°å’Œäº¤æ˜“æä¾›æ”¯æŒã€‚ç³»ç»Ÿæ•´åˆäº†å®‰å…¨äº’æ“ä½œæ™ºèƒ½ä½“çš„å¼€å‘æ¡†æ¶ã€äº‘ç«¯éƒ¨ç½²å¹³å°ï¼Œä»¥åŠåˆ©ç”¨ agent-native LLM å°†äººç±»ç›®æ ‡è½¬åŒ–ä¸ºå¤æ‚å·¥ä½œæµçš„æ™ºèƒ½ç¼–æ’å±‚ã€‚é€šè¿‡å»ä¸­å¿ƒåŒ–ç‰©æµçš„å®é™…æ¡ˆä¾‹ï¼Œç ”ç©¶å±•ç¤ºäº†è‡ªä¸»æ™ºèƒ½ä½“å¦‚ä½•å®‰å…¨åœ°è¿›è¡ŒåŠ¨æ€å‘ç°ã€è°ˆåˆ¤å’Œäº¤æ˜“ã€‚æœ€ç»ˆï¼ŒFetch.ai ä¸ºæ„å»ºå¼€æ”¾ã€ååŒä¸”ç»æµå¯æŒç»­çš„å¤šæ™ºèƒ½ä½“ç”Ÿæ€ç³»ç»Ÿæä¾›äº†ä¸€å¥—åŸåˆ™æ€§çš„æ¶æ„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "26 pages, figures, code examples",
      "pdf_url": "https://arxiv.org/pdf/2510.18699v1",
      "published_date": "2025-10-21 14:53:56 UTC",
      "updated_date": "2025-10-21 14:53:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:46:00.987541+00:00"
    },
    {
      "arxiv_id": "2510.21814v2",
      "title": "Gestura: A LVLM-Powered System Bridging Motion and Semantics for Real-Time Free-Form Gesture Understanding",
      "title_zh": "Gesturaï¼šåŸºäºå¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰è¿æ¥åŠ¨ä½œä¸è¯­ä¹‰çš„å®æ—¶è‡ªç”±å½¢å¼æ‰‹åŠ¿ç†è§£ç³»ç»Ÿ",
      "authors": [
        "Zhuoming Li",
        "Aitong Liu",
        "Mengxi Jia",
        "Yubi Lu",
        "Tengxiang Zhang",
        "Changzhi Sun",
        "Dell Zhang",
        "Xuelong Li"
      ],
      "abstract": "Free-form gesture understanding is highly appealing for human-computer interaction, as it liberates users from the constraints of predefined gesture categories. However, the sole existing solution GestureGPT suffers from limited recognition accuracy and slow response times. In this paper, we propose Gestura, an end-to-end system for free-form gesture understanding. Gestura harnesses a pre-trained Large Vision-Language Model (LVLM) to align the highly dynamic and diverse patterns of free-form gestures with high-level semantic concepts. To better capture subtle hand movements across different styles, we introduce a Landmark Processing Module that compensate for LVLMs' lack of fine-grained domain knowledge by embedding anatomical hand priors. Further, a Chain-of-Thought (CoT) reasoning strategy enables step-by-step semantic inference, transforming shallow knowledge into deep semantic understanding and significantly enhancing the model's ability to interpret ambiguous or unconventional gestures. Together, these components allow Gestura to achieve robust and adaptable free-form gesture comprehension. Additionally, we have developed the first open-source dataset for free-form gesture intention reasoning and understanding with over 300,000 annotated QA pairs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Gesturaï¼Œä¸€ä¸ªåŸºäºå¤§è¯­è¨€è§†è§‰æ¨¡å‹(LVLM)çš„ç«¯åˆ°ç«¯ç³»ç»Ÿï¼Œæ—¨åœ¨å®ç°å®æ—¶ã€è‡ªç”±å½¢å¼(Free-form)çš„æ‰‹åŠ¿ç†è§£ã€‚ä¸ºè§£å†³ç°æœ‰æ–¹æ¡ˆç²¾åº¦ä½ã€å“åº”æ…¢çš„é—®é¢˜ï¼ŒGesturaå¼•å…¥äº†åœ°æ ‡å¤„ç†æ¨¡å—(Landmark Processing Module)ï¼Œé€šè¿‡åµŒå…¥è§£å‰–å­¦æ‰‹éƒ¨å…ˆéªŒ(Anatomical hand priors)è¡¥å¿æ¨¡å‹åœ¨ç»†ç²’åº¦é¢†åŸŸçŸ¥è¯†æ–¹é¢çš„ä¸è¶³ã€‚åŒæ—¶ï¼Œè¯¥ç³»ç»Ÿé‡‡ç”¨é“¾å¼æ€ç»´(Chain-of-Thought, CoT)æ¨ç†ç­–ç•¥è¿›è¡Œé€æ­¥è¯­ä¹‰æ¨ç†ï¼Œæœ‰æ•ˆæå‡äº†å¯¹æ­§ä¹‰æˆ–éå¸¸è§„æ‰‹åŠ¿çš„è§£é‡Šèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘å¹¶å¼€æºäº†é¦–ä¸ªé’ˆå¯¹è‡ªç”±å½¢å¼æ‰‹åŠ¿æ„å›¾æ¨ç†çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡30ä¸‡ä¸ªæ ‡æ³¨é—®ç­”å¯¹ã€‚è¿™äº›æŠ€æœ¯åˆ›æ–°å…±åŒç¡®ä¿äº†Gesturaåœ¨æ•æ‰ç»†å¾®æ‰‹éƒ¨è¿åŠ¨å’Œå®ç°æ·±å±‚è¯­ä¹‰ç†è§£æ–¹é¢çš„å“è¶Šæ€§èƒ½ï¼Œä¸ºæ›´è‡ªç„¶çš„äººæœºäº¤äº’å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "IMWUT2025",
      "pdf_url": "https://arxiv.org/pdf/2510.21814v2",
      "published_date": "2025-10-21 14:46:48 UTC",
      "updated_date": "2025-11-06 01:45:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:46:02.097336+00:00"
    },
    {
      "arxiv_id": "2510.21813v1",
      "title": "SITS-DECO: A Generative Decoder Is All You Need For Multitask Satellite Image Time Series Modelling",
      "title_zh": "SITS-DECOï¼šä»…éœ€ç”Ÿæˆå¼è§£ç å™¨å³å¯å®ç°å¤šä»»åŠ¡å«æ˜Ÿå›¾åƒæ—¶é—´åºåˆ—å»ºæ¨¡",
      "authors": [
        "Samuel J. Barrett",
        "Docko Sow"
      ],
      "abstract": "Earth Observation (EO) Foundation Modelling (FM) holds great promise for simplifying and improving the use of EO data for diverse real-world tasks. However, most existing models require additional adaptation before they can be used and are structured rigidly around particular data sources or training approaches. To address this, we take inspiration from large language models, where diverse tasks, both pre-training and downstream, are implicitly captured through next-token prediction over unified token sequences, leveraging the structure and diversity of the training data.\n  We introduce SITS-DECO (Satellite Image Time Series-DECoder Only), a proof-of-concept generative model that applies this unified-sequence framing to EO data. Using a simple GPT-style decoder-only architecture, and demonstrate its ability to perform useful EO tasks (pixel-wise, multi-temporal, multi-modal crop-type classification) in a purely generative framework. Through symbolic prompting, we show that the model can perform multiple supervised and self-supervised tasks within a single unified architecture, without task- or modality-specific adaptation. Despite its simplicity and lack of spatial context, SITS-DECO outperforms much larger EO foundation models on crop-type classification (PASTIS-R) demonstrating that dense temporal sequence modelling is a critical missing ingredient in the current paradigm.\n  This work exemplifies a data-centric modelling paradigm in which capability arises from the diversity and structure of the training data rather than from architectural complexity. SITS-DECO provides a lightweight, practical route to multi-modal, multi-task EO modelling, and a conceptual bridge toward future generative EO foundation models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SITS-DECOï¼Œä¸€ç§åŸºäºä»…è§£ç å™¨ (decoder-only) æ¶æ„çš„ç”Ÿæˆå¼æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡ç»Ÿä¸€çš„åºåˆ—æ¡†æ¶è§£å†³å«æ˜Ÿå›¾åƒæ—¶é—´åºåˆ— (Satellite Image Time Series) çš„å¤šä»»åŠ¡å»ºæ¨¡é—®é¢˜ã€‚è¯¥æ¨¡å‹å—å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¯å‘ï¼Œé‡‡ç”¨ GPT é£æ ¼çš„æ¶æ„ï¼Œåˆ©ç”¨ç¬¦å·æç¤º (symbolic prompting) åœ¨å•ä¸€æ¶æ„ä¸­å®ç°å¤šç§ç›‘ç£å’Œè‡ªç›‘ç£ä»»åŠ¡ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šæ¨¡æ€æˆ–ä»»åŠ¡è¿›è¡Œé€‚é…ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSITS-DECO åœ¨ PASTIS-R å†œä½œç‰©åˆ†ç±»ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºè®¸å¤šå‚æ•°é‡æ›´å¤§çš„åœ°çƒè§‚æµ‹åŸºç¡€æ¨¡å‹ (EO Foundation Models)ï¼Œè¯æ˜äº†å¯†é›†çš„æ—¶åºåºåˆ—å»ºæ¨¡æ˜¯æå‡æ€§èƒ½çš„å…³é”®å› ç´ ã€‚è¿™ä¸€æˆæœå±•ç¤ºäº†ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„å»ºæ¨¡èŒƒå¼ï¼Œå³æ¨¡å‹èƒ½åŠ›æºäºè®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ä¸ç»“æ„è€Œéæ¶æ„çš„å¤æ‚æ€§ã€‚SITS-DECO ä¸ºå¤šæ¨¡æ€ã€å¤šä»»åŠ¡çš„åœ°çƒè§‚æµ‹å»ºæ¨¡æä¾›äº†ä¸€æ¡è½»é‡ä¸”å®ç”¨çš„è·¯å¾„ï¼Œå¹¶ä¸ºæœªæ¥çš„ç”Ÿæˆå¼åŸºç¡€æ¨¡å‹å¥ å®šäº†æ¦‚å¿µåŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "27 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.21813v1",
      "published_date": "2025-10-21 14:42:55 UTC",
      "updated_date": "2025-10-21 14:42:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:46:08.081486+00:00"
    },
    {
      "arxiv_id": "2510.18674v1",
      "title": "Exploring Membership Inference Vulnerabilities in Clinical Large Language Models",
      "title_zh": "ä¸´åºŠå¤§è¯­è¨€æ¨¡å‹æˆå‘˜æ¨ç†æ¼æ´æ¢ç©¶",
      "authors": [
        "Alexander Nemecek",
        "Zebin Yun",
        "Zahra Rahmani",
        "Yaniv Harel",
        "Vipin Chaudhary",
        "Mahmood Sharif",
        "Erman Ayday"
      ],
      "abstract": "As large language models (LLMs) become progressively more embedded in clinical decision-support, documentation, and patient-information systems, ensuring their privacy and trustworthiness has emerged as an imperative challenge for the healthcare sector. Fine-tuning LLMs on sensitive electronic health record (EHR) data improves domain alignment but also raises the risk of exposing patient information through model behaviors. In this work-in-progress, we present an exploratory empirical study on membership inference vulnerabilities in clinical LLMs, focusing on whether adversaries can infer if specific patient records were used during model training. Using a state-of-the-art clinical question-answering model, Llemr, we evaluate both canonical loss-based attacks and a domain-motivated paraphrasing-based perturbation strategy that more realistically reflects clinical adversarial conditions. Our preliminary findings reveal limited but measurable membership leakage, suggesting that current clinical LLMs provide partial resistance yet remain susceptible to subtle privacy risks that could undermine trust in clinical AI adoption. These results motivate continued development of context-aware, domain-specific privacy evaluations and defenses such as differential privacy fine-tuning and paraphrase-aware training, to strengthen the security and trustworthiness of healthcare AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿæ¢è®¨äº†ä¸´åºŠå¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) åœ¨åˆ©ç”¨æ•æ„Ÿçš„ç”µå­å¥åº·è®°å½• (Electronic Health Record, EHR) æ•°æ®è¿›è¡Œé¢†åŸŸå¾®è°ƒæ—¶æ‰€é¢ä¸´çš„æˆå‘˜æ¨ç† (Membership Inference) éšç§æ¼æ´ã€‚ç ”ç©¶è€…åˆ©ç”¨æœ€å…ˆè¿›çš„ä¸´åºŠé—®ç­”æ¨¡å‹ Llemrï¼Œè¯„ä¼°äº†ä¼ ç»Ÿçš„åŸºäºæŸå¤±çš„æ”»å‡»æ–¹æ³•ä»¥åŠä¸€ç§æ›´ç¬¦åˆä¸´åºŠå®é™…çš„æ”¹å†™æ‰°åŠ¨ç­–ç•¥ (paraphrasing-based perturbation strategy)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ä¸´åºŠ LLMs å…·æœ‰éƒ¨åˆ†æŠµæŠ—åŠ›ï¼Œä½†ä»å­˜åœ¨æœ‰é™ä¸”å¯æµ‹é‡çš„æˆå‘˜ä¿¡æ¯æ³„éœ²ï¼Œè¡¨æ˜æ¨¡å‹ä»é¢ä¸´å¯èƒ½æŸå®³åŒ»ç–— AI ä¿¡ä»»åº¦çš„å¾®å¦™éšç§é£é™©ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å¼€å‘é¢†åŸŸç‰¹å®šéšç§è¯„ä¼°çš„å¿…è¦æ€§ï¼Œå¹¶å»ºè®®é‡‡ç”¨å·®å¼‚éšç§å¾®è°ƒ (differential privacy fine-tuning) å’Œæ”¹å†™æ„ŸçŸ¥è®­ç»ƒ (paraphrase-aware training) ç­‰é˜²å¾¡æ‰‹æ®µæ¥å¢å¼ºç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯ä¿¡åº¦ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at the 1st IEEE Workshop on Healthcare and Medical Device Security, Privacy, Resilience, and Trust (IEEE HMD-SPiRiT)",
      "pdf_url": "https://arxiv.org/pdf/2510.18674v1",
      "published_date": "2025-10-21 14:27:48 UTC",
      "updated_date": "2025-10-21 14:27:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:46:16.416524+00:00"
    },
    {
      "arxiv_id": "2510.18672v1",
      "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
      "title_zh": "æ¨ç†è¯­è¨€æ¨¡å‹æ¨ç†æœåŠ¡æ­ç§˜ï¼šä¸€é¡¹å®è¯ç ”ç©¶",
      "authors": [
        "Qi Li",
        "Junpan Wu",
        "Xiang Liu",
        "Yuxin Wang",
        "Zeyu Li",
        "Zhenheng Tang",
        "Yuhan Chen",
        "Shaohuai Shi",
        "Xiaowen Chu"
      ],
      "abstract": "The reasoning large language model (RLLM) has been proven competitive in solving complex reasoning tasks such as mathematics, coding, compared to general LLM. However, the serving performance and behavior of RLLM remains unexplored, which may undermine the deployment and utilization of RLLM in real-world scenario. To close this gap, in this paper, we conduct a comprehensive study of RLLM service. We first perform a pilot study on comparing the serving performance between RLLM and traditional LLM and reveal that there are several distinct differences regarding serving behavior: (1) significant memory usage and fluctuations; (2) straggler requests; (3) adaptive running time; (4) domain preference. Then we further investigate whether existing inference optimization techniques are valid for RLLM. Our main takeaways are that model quantization methods and speculative decoding can improve service system efficiency with small compromise to RLLM accuracy, while prefix caching, KV cache quantization may even degrade accuracy or serving performance for small RLLM. Lastly, we conduct evaluation under real world workload modeled by Gamma distribution to verify our findings. Empirical results of real world workload evaluation across different dataset are aligned with our main findings regarding RLLM serving. We hope our work can provide the research community and industry with insights to advance RLLM inference serving.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨ç†å¤§è¯­è¨€æ¨¡å‹ï¼ˆReasoning Large Language Model, RLLMï¼‰åœ¨æ¨ç†æœåŠ¡ï¼ˆInference Servingï¼‰ä¸­çš„æ€§èƒ½ä¸è¡Œä¸ºè¿›è¡Œäº†æ·±å…¥çš„å®è¯ç ”ç©¶ã€‚é€šè¿‡ä¸ä¼ ç»Ÿå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹æ¯”ï¼Œç ”ç©¶æ­ç¤ºäº† RLLM åœ¨æœåŠ¡è¡Œä¸ºä¸Šçš„æ˜¾è‘—å·®å¼‚ï¼ŒåŒ…æ‹¬é«˜å†…å­˜å ç”¨ä¸æ³¢åŠ¨ã€æ‹–åè…¿è¯·æ±‚ï¼ˆStraggler Requestsï¼‰ã€è‡ªé€‚åº”è¿è¡Œæ—¶é—´ä»¥åŠé¢†åŸŸåå¥½ã€‚éšåï¼Œè¯¥ç ”ç©¶è¯„ä¼°äº†ç°æœ‰æ¨ç†ä¼˜åŒ–æŠ€æœ¯çš„æœ‰æ•ˆæ€§ï¼Œå‘ç°æ¨¡å‹é‡åŒ–ï¼ˆModel Quantizationï¼‰å’Œæ¨æµ‹è§£ç ï¼ˆSpeculative Decodingï¼‰èƒ½æœ‰æ•ˆæå‡æœåŠ¡æ•ˆç‡ä¸”ä»…å¯¹å‡†ç¡®åº¦æœ‰å¾®å°å½±å“ã€‚ç„¶è€Œï¼Œå‰ç¼€ç¼“å­˜ï¼ˆPrefix Cachingï¼‰å’Œ KV Cache é‡åŒ–åœ¨å°å‹ RLLM ä¸Šè¡¨ç°ä¸ä½³ï¼Œç”šè‡³å¯èƒ½é™ä½æ¨¡å‹å‡†ç¡®åº¦æˆ–æœåŠ¡æ€§èƒ½ã€‚æœ€åï¼Œç ”ç©¶åˆ©ç”¨åŸºäºä¼½é©¬åˆ†å¸ƒï¼ˆGamma Distributionï¼‰æ„å»ºçš„çœŸå®å·¥ä½œè´Ÿè½½éªŒè¯äº†ä¸Šè¿°å‘ç°ï¼Œä¸ºä¼˜åŒ– RLLM æ¨ç†æœåŠ¡éƒ¨ç½²æä¾›äº†é‡è¦çš„ç†è®ºä¾æ®å’Œå®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18672v1",
      "published_date": "2025-10-21 14:25:51 UTC",
      "updated_date": "2025-10-21 14:25:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:46:09.688569+00:00"
    },
    {
      "arxiv_id": "2510.18659v1",
      "title": "Sherlock Your Queries: Learning to Ask the Right Questions for Dialogue-Based Retrieval",
      "title_zh": "Sherlock Your Queriesï¼šé¢å‘å¯¹è¯å¼æ£€ç´¢çš„ç²¾å‡†æé—®å­¦ä¹ ",
      "authors": [
        "Dong Yun",
        "Marco Schouten",
        "Dim Papadopoulos"
      ],
      "abstract": "User queries in information retrieval are often ambiguous, making it challenging for systems to identify a user's target from a single query. While recent dialogue-based interactive retrieval systems can clarify user intent, they are inefficient as they often lack an explicit strategy to ask the most informative questions. To address this limitation, we propose SherlockLLM, a dialogue-driven retrieval framework that learns an optimal questioning strategy via Reinforcement Learning (RL) and avoids the need for large-scale annotated dialogue data. In our framework, an agent is trained to generate a sequence of binary questions to efficiently narrow down the search space. To validate our approach, we introduce a benchmark with both structured and unstructured tasks. Experimental results show that SherlockLLM is a robust and efficient solution. On the structured tasks, its performance matches strong baselines and approaches the theoretical optimal defined by binary search. On the challenging unstructured task, our agent significantly outperforms these baselines, showcasing its ability to learn a highly effective information-seeking dialogue policy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SherlockLLMï¼Œè¿™æ˜¯ä¸€ä¸ªå¯¹è¯é©±åŠ¨çš„æ£€ç´¢æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¿¡æ¯æ£€ç´¢ä¸­ç”¨æˆ·æŸ¥è¯¢å­˜åœ¨æ­§ä¹‰ä¸”ç°æœ‰ç³»ç»Ÿç¼ºä¹é«˜æ•ˆæé—®ç­–ç•¥çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ Reinforcement Learning (RL) å­¦ä¹ æœ€ä¼˜æé—®ç­–ç•¥ï¼Œæœ‰æ•ˆé¿å…äº†å¯¹å¤§è§„æ¨¡æ ‡æ³¨å¯¹è¯æ•°æ®çš„éœ€æ±‚ã€‚SherlockLLM èƒ½å¤Ÿç”Ÿæˆä¸€ç³»åˆ— binary questions ä»¥å¿«é€Ÿç¼©å°æœç´¢ç©ºé—´ï¼Œä»è€Œæå‡æ£€ç´¢æ•ˆç‡ã€‚ä¸ºäº†éªŒè¯è¯¥æ–¹æ³•ï¼Œç ”ç©¶è€…å¼•å…¥äº†ä¸€ä¸ªåŒ…å«ç»“æ„åŒ–å’Œéç»“æ„åŒ–ä»»åŠ¡çš„æ–°åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSherlockLLM åœ¨ç»“æ„åŒ–ä»»åŠ¡ä¸Šçš„è¡¨ç°æ¥è¿‘ binary search çš„ç†è®ºæœ€ä¼˜æ°´å¹³ï¼Œè€Œåœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„éç»“æ„åŒ–ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºå¼ºåŠ›åŸºçº¿æ¨¡å‹ã€‚è¿™è¯æ˜äº†è¯¥æ™ºèƒ½ä½“åœ¨å­¦ä¹ é«˜æ•ˆä¿¡æ¯å¯»æ±‚å¯¹è¯ç­–ç•¥æ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18659v1",
      "published_date": "2025-10-21 14:10:42 UTC",
      "updated_date": "2025-10-21 14:10:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:46:13.687688+00:00"
    },
    {
      "arxiv_id": "2510.18650v1",
      "title": "Binary Quadratic Quantization: Beyond First-Order Quantization for Real-Valued Matrix Compression",
      "title_zh": "äºŒè¿›åˆ¶äºŒæ¬¡é‡åŒ–ï¼šè¶…è¶Šä¸€é˜¶é‡åŒ–çš„å®å€¼çŸ©é˜µå‹ç¼©",
      "authors": [
        "Kyo Kuroki",
        "Yasuyuki Okoshi",
        "Thiem Van Chu",
        "Kazushi Kawamura",
        "Masato Motomura"
      ],
      "abstract": "This paper proposes a novel matrix quantization method, Binary Quadratic Quantization (BQQ). In contrast to conventional first-order quantization approaches, such as uniform quantization and binary coding quantization, that approximate real-valued matrices via linear combinations of binary bases, BQQ leverages the expressive power of binary quadratic expressions while maintaining an extremely compact data format. We validate our approach with two experiments: a matrix compression benchmark and post-training quantization (PTQ) on pretrained Vision Transformer-based models. Experimental results demonstrate that BQQ consistently achieves a superior trade-off between memory efficiency and reconstruction error than conventional methods for compressing diverse matrix data. It also delivers strong PTQ performance, even though we neither target state-of-the-art PTQ accuracy under tight memory constraints nor rely on PTQ-specific binary matrix optimization. For example, our proposed method outperforms the state-of-the-art PTQ method by up to 2.2\\% and 59.1% on the ImageNet dataset under the calibration-based and data-free scenarios, respectively, with quantization equivalent to 2 bits. These findings highlight the surprising effectiveness of binary quadratic expressions for efficient matrix approximation and neural network compression.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Binary Quadratic Quantization (BQQ) çš„æ–°å‹çŸ©é˜µé‡åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨è¶…è¶Šä¼ ç»Ÿçš„ First-Order Quantization æ–¹æ¡ˆã€‚ä¸é€šè¿‡äºŒè¿›åˆ¶åŸºç»„çº¿æ€§ç»„åˆé€¼è¿‘å®å€¼çŸ©é˜µçš„å¸¸è§„æ–¹æ³•ä¸åŒï¼ŒBQQ å……åˆ†åˆ©ç”¨äº† Binary Quadratic Expressions çš„å¼ºå¤§è¡¨è¾¾èƒ½åŠ›ï¼Œåœ¨ç»´æŒæç´§å‡‘æ•°æ®æ ¼å¼çš„åŒæ—¶æ˜¾è‘—æå‡äº†è¿‘ä¼¼ç²¾åº¦ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡çŸ©é˜µå‹ç¼©åŸºå‡†æµ‹è¯•ä»¥åŠåœ¨ Vision Transformer æ¨¡å‹ä¸Šçš„è®­ç»ƒåé‡åŒ– (Post-Training Quantization, PTQ) å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBQQ åœ¨å†…å­˜æ•ˆç‡ä¸é‡å»ºè¯¯å·®çš„æƒè¡¡ä¸ŠæŒç»­ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œåœ¨ ImageNet æ•°æ®é›†ç­‰æ•ˆ 2 bits é‡åŒ–çš„é…ç½®ä¸‹ï¼Œå…¶åœ¨åŸºäºæ ¡å‡†å’Œæ— æ•°æ®åœºæ™¯ä¸­åˆ†åˆ«æ¯”ç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯æé«˜äº† 2.2% å’Œ 59.1% çš„å‡†ç¡®ç‡ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†äºŒè¿›åˆ¶äºŒæ¬¡è¡¨è¾¾å¼åœ¨é«˜æ•ˆçŸ©é˜µé€¼è¿‘å’Œç¥ç»ç½‘ç»œå‹ç¼©é¢†åŸŸå…·æœ‰æé«˜çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.18650v1",
      "published_date": "2025-10-21 13:58:46 UTC",
      "updated_date": "2025-10-21 13:58:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:46:17.491896+00:00"
    },
    {
      "arxiv_id": "2510.18637v2",
      "title": "Îµ-Seg: Sparsely Supervised Semantic Segmentation of Microscopy Data",
      "title_zh": "Îµ-Segï¼šæ˜¾å¾®æˆåƒæ•°æ®çš„ç¨€ç–ç›‘ç£è¯­ä¹‰åˆ†å‰²",
      "authors": [
        "Sheida Rahnamai Kordasiabi",
        "Damian Dalle Nogare",
        "Florian Jug"
      ],
      "abstract": "Semantic segmentation of electron microscopy (EM) images of biological samples remains a challenge in the life sciences. EM data captures details of biological structures, sometimes with such complexity that even human observers can find it overwhelming. We introduce Îµ-Seg, a method based on hierarchical variational autoencoders (HVAEs), employing center-region masking, sparse label contrastive learning (CL), a Gaussian mixture model (GMM) prior, and clustering-free label prediction. Center-region masking and the inpainting loss encourage the model to learn robust and representative embeddings to distinguish the desired classes, even if training labels are sparse (0.05% of the total image data or less). For optimal performance, we employ CL and a GMM prior to shape the latent space of the HVAE such that encoded input patches tend to cluster wrt. the semantic classes we wish to distinguish. Finally, instead of clustering latent embeddings for semantic segmentation, we propose a MLP semantic segmentation head to directly predict class labels from latent embeddings. We show empirical results of Îµ-Seg and baseline methods on 2 dense EM datasets of biological tissues and demonstrate the applicability of our method also on fluorescence microscopy data. Our results show that Îµ-Seg is capable of achieving competitive sparsely-supervised segmentation results on complex biological image data, even if only limited amounts of training labels are available.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Îµ-Segï¼Œä¸€ç§åŸºäºå±‚æ¬¡åŒ–å˜åˆ†è‡ªç¼–ç å™¨ (HVAEs) çš„ç¨€ç–ç›‘ç£è¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç”Ÿç‰©æ ·æœ¬ç”µå­æ˜¾å¾®é•œ (EM) å›¾åƒçš„å¤æ‚åˆ†å‰²éš¾é¢˜ã€‚è¯¥æ–¹æ³•é›†æˆäº†ä¸­å¿ƒåŒºåŸŸæ©ç  (center-region masking)ã€ç¨€ç–æ ‡ç­¾å¯¹æ¯”å­¦ä¹  (CL) å’Œé«˜æ–¯æ··åˆæ¨¡å‹ (GMM) å…ˆéªŒï¼Œèƒ½å¤Ÿä»æå°‘é‡çš„æ ‡æ³¨ï¼ˆ0.05% æˆ–æ›´å°‘ï¼‰ä¸­å­¦ä¹ åˆ°å…·æœ‰é²æ£’æ€§çš„åµŒå…¥è¡¨ç¤ºã€‚ä¸åŒäºä¼ ç»Ÿçš„èšç±»æ–¹å¼ï¼ŒÎµ-Seg é‡‡ç”¨äº†å¤šå±‚æ„ŸçŸ¥æœº (MLP) åˆ†å‰²å¤´ç›´æ¥ä»æ½œåœ¨ç©ºé—´é¢„æµ‹æ ‡ç­¾ï¼Œæ˜¾è‘—æå‡äº†é¢„æµ‹æ•ˆç‡ã€‚åœ¨ä¸¤ä¸ªå¯†é›† EM æ•°æ®é›†å’Œè§å…‰æ˜¾å¾®æ•°æ®ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ‡æ³¨å—é™çš„æƒ…å†µä¸‹ä»èƒ½å–å¾—å…·æœ‰ç«äº‰åŠ›çš„è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚è¿™ä¸ºæ ‡ç­¾è·å–æˆæœ¬é«˜æ˜‚çš„å¤æ‚ç”Ÿç‰©å›¾åƒåˆ†ææä¾›äº†ä¸€ç§é«˜æ•ˆä¸”ç²¾å‡†çš„å¯è¡Œæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages main text, 17 pages total",
      "pdf_url": "https://arxiv.org/pdf/2510.18637v2",
      "published_date": "2025-10-21 13:41:07 UTC",
      "updated_date": "2025-10-30 18:38:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:46:23.123841+00:00"
    },
    {
      "arxiv_id": "2510.18636v1",
      "title": "C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural Networks Compression",
      "title_zh": "C-SWAPï¼šé¢å‘é«˜æ•ˆç¥ç»ç½‘ç»œå‹ç¼©çš„å¯è§£é‡Šæ€§æ„ŸçŸ¥ç»“æ„åŒ–å‰ªæ",
      "authors": [
        "Baptiste Bauvin",
        "LoÃ¯c Baret",
        "Ola Ahmad"
      ],
      "abstract": "Neural network compression has gained increasing attention in recent years, particularly in computer vision applications, where the need for model reduction is crucial for overcoming deployment constraints. Pruning is a widely used technique that prompts sparsity in model structures, e.g. weights, neurons, and layers, reducing size and inference costs. Structured pruning is especially important as it allows for the removal of entire structures, which further accelerates inference time and reduces memory overhead. However, it can be computationally expensive, requiring iterative retraining and optimization. To overcome this problem, recent methods considered one-shot setting, which applies pruning directly at post-training. Unfortunately, they often lead to a considerable drop in performance. In this paper, we focus on this issue by proposing a novel one-shot pruning framework that relies on explainable deep learning. First, we introduce a causal-aware pruning approach that leverages cause-effect relations between model predictions and structures in a progressive pruning process. It allows us to efficiently reduce the size of the network, ensuring that the removed structures do not deter the performance of the model. Then, through experiments conducted on convolution neural network and vision transformer baselines, pre-trained on classification tasks, we demonstrate that our method consistently achieves substantial reductions in model size, with minimal impact on performance, and without the need for fine-tuning. Overall, our approach outperforms its counterparts, offering the best trade-off. Our code is available on GitHub.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç»“æ„åŒ–å‰ªæ(Structured Pruning)åœ¨å•æ¬¡(one-shot)è®¾ç½®ä¸‹å®¹æ˜“å¯¼è‡´æ€§èƒ½å¤§å¹…ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºC-SWAPçš„åˆ›æ–°å‰ªææ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¯è§£é‡Šæ·±åº¦å­¦ä¹ (Explainable Deep Learning)æŠ€æœ¯ï¼Œå¼•å…¥äº†ä¸€ç§å› æœæ„ŸçŸ¥å‰ªæ(Causal-aware pruning)æ–¹æ³•ï¼Œé€šè¿‡åˆ†ææ¨¡å‹é¢„æµ‹ä¸å†…éƒ¨ç»“æ„ä¹‹é—´çš„å› æœå…³ç³»æ¥è¿›è¡Œæ¸è¿›å¼å‰ªæã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆè¯†åˆ«å¹¶ç§»é™¤å†—ä½™ç»“æ„ï¼ŒåŒæ—¶ç¡®ä¿ç½‘ç»œçš„é¢„æµ‹æ€§èƒ½ä¸å—æ˜¾è‘—æŸå®³ã€‚åœ¨å·ç§¯ç¥ç»ç½‘ç»œ(CNN)å’Œè§†è§‰Transformer(Vision Transformer)åŸºå‡†æµ‹è¯•ä¸­ï¼ŒC-SWAPåœ¨æ— éœ€ä»»ä½•å¾®è°ƒ(Fine-tuning)çš„å‰æä¸‹å®ç°äº†æ˜¾è‘—çš„æ¨¡å‹è§„æ¨¡ç¼©å‡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹ç²¾åº¦ä¸å‡å°‘è®¡ç®—å¼€é”€ä¹‹é—´å–å¾—äº†æœ€ä¼˜æƒè¡¡ï¼Œå…¶æ€§èƒ½è¡¨ç°ä¼˜äºç°æœ‰çš„åŒç±»å•æ¬¡å‰ªææ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, BMVC2025",
      "pdf_url": "https://arxiv.org/pdf/2510.18636v1",
      "published_date": "2025-10-21 13:40:11 UTC",
      "updated_date": "2025-10-21 13:40:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:46:23.291805+00:00"
    },
    {
      "arxiv_id": "2510.18633v1",
      "title": "Query Decomposition for RAG: Balancing Exploration-Exploitation",
      "title_zh": "RAG æŸ¥è¯¢åˆ†è§£ï¼šæ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡",
      "authors": [
        "Roxana Petcu",
        "Kenton Murray",
        "Daniel Khashabi",
        "Evangelos Kanoulas",
        "Maarten de Rijke",
        "Dawn Lawrie",
        "Kevin Duh"
      ],
      "abstract": "Retrieval-augmented generation (RAG) systems address complex user requests by decomposing them into subqueries, retrieving potentially relevant documents for each, and then aggregating them to generate an answer. Efficiently selecting informative documents requires balancing a key trade-off: (i) retrieving broadly enough to capture all the relevant material, and (ii) limiting retrieval to avoid excessive noise and computational cost. We formulate query decomposition and document retrieval in an exploitation-exploration setting, where retrieving one document at a time builds a belief about the utility of a given sub-query and informs the decision to continue exploiting or exploring an alternative. We experiment with a variety of bandit learning methods and demonstrate their effectiveness in dynamically selecting the most informative sub-queries. Our main finding is that estimating document relevance using rank information and human judgments yields a 35% gain in document-level precision, 15% increase in Î±-nDCG, and better performance on the downstream task of long-form generation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç³»ç»Ÿä¸­çš„æŸ¥è¯¢åˆ†è§£(Query Decomposition)é—®é¢˜ï¼Œæ—¨åœ¨å¹³è¡¡å…¨é¢æ£€ç´¢ä¸æ§åˆ¶è®¡ç®—æˆæœ¬åŠå™ªå£°ä¹‹é—´çš„æƒè¡¡ã€‚ä½œè€…å°†æŸ¥è¯¢åˆ†è§£ä¸æ–‡æ¡£æ£€ç´¢å»ºæ¨¡ä¸ºæ¢ç´¢-åˆ©ç”¨(Exploration-Exploitation)åœºæ™¯ï¼Œå¹¶é‡‡ç”¨å¤šç§å¤šè‡‚å¼ºç›—(Bandit Learning)ç®—æ³•æ¥åŠ¨æ€é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„å­æŸ¥è¯¢ã€‚è¯¥æ–¹æ³•é€šè¿‡é€ä¸€æ£€ç´¢æ–‡æ¡£æ¥å»ºç«‹å¯¹ç‰¹å®šå­æŸ¥è¯¢æ•ˆç”¨çš„è¯„ä¼°ï¼Œä»è€ŒæŒ‡å¯¼ç³»ç»Ÿå†³å®šæ˜¯ç»§ç»­åˆ©ç”¨å½“å‰è·¯å¾„è¿˜æ˜¯æ¢ç´¢å…¶ä»–æ›¿ä»£æ–¹æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼Œåˆ©ç”¨æ’åä¿¡æ¯å’Œäººå·¥è¯„åˆ¤æ¥ä¼°ç®—æ–‡æ¡£ç›¸å…³æ€§ï¼Œåœ¨æ–‡æ¡£çº§ç²¾ç¡®åº¦ä¸Šæå‡äº†35%ï¼Œåœ¨Î±-nDCGæŒ‡æ ‡ä¸Šå¢é•¿äº†15%ã€‚è¯¥æ¡†æ¶åœ¨ä¸‹æ¸¸çš„é•¿æ–‡æœ¬ç”Ÿæˆ(Long-form Generation)ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºäº†æ›´ä¼˜çš„æ€§èƒ½ï¼Œè¯æ˜äº†åœ¨æŸ¥è¯¢åˆ†è§£è¿‡ç¨‹ä¸­åŠ¨æ€å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18633v1",
      "published_date": "2025-10-21 13:37:11 UTC",
      "updated_date": "2025-10-21 13:37:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:46:38.693524+00:00"
    },
    {
      "arxiv_id": "2510.18632v1",
      "title": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views",
      "title_zh": "Think with 3Dï¼šåŸºäºå‡ ä½•æƒ³è±¡çš„æœ‰é™è§†è§’ç©ºé—´æ¨ç†",
      "authors": [
        "Zhangquan Chen",
        "Manyuan Zhang",
        "Xinlei Yu",
        "Xufang Luo",
        "Mingze Sun",
        "Zihao Pan",
        "Yan Feng",
        "Peng Pei",
        "Xunliang Cai",
        "Ruqi Huang"
      ],
      "abstract": "Though recent advances in vision-language models (VLMs) have achieved remarkable progress across a wide range of multimodal tasks, understanding 3D spatial relationships from limited views remains a significant challenge. Previous reasoning methods typically rely on pure text (e.g., topological cognitive maps) or on 2D visual cues. However, their limited representational capacity hinders performance in specific tasks that require 3D spatial imagination. To address this limitation, we propose 3DThinker, a framework that can effectively exploits the rich geometric information embedded within images while reasoning, like humans do. Our framework is the first to enable 3D mentaling during reasoning without any 3D prior input, and it does not rely on explicitly labeled 3D data for training. Specifically, our training consists of two stages. First, we perform supervised training to align the 3D latent generated by VLM while reasoning with that of a 3D foundation model (e.g., VGGT). Then, we optimize the entire reasoning trajectory solely based on outcome signals, thereby refining the underlying 3D mentaling. Extensive experiments across multiple benchmarks show that 3DThinker consistently outperforms strong baselines and offers a new perspective toward unifying 3D representations into multimodal reasoning. Our code will be available at https://github.com/zhangquanchen/3DThinker.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models)åœ¨æœ‰é™è§†è§’ä¸‹ç†è§£3Dç©ºé—´å…³ç³»çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†3DThinkeræ¡†æ¶ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿäººç±»åœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆ©ç”¨å›¾åƒå‡ ä½•ä¿¡æ¯çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶æ˜¯é¦–ä¸ªåœ¨æ— éœ€3Då…ˆéªŒè¾“å…¥æˆ–æ˜¾å¼3Dæ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å®ç°3Då¿ƒç†æ¨¡æ‹Ÿ(3D mentaling)çš„ç³»ç»Ÿã€‚è®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆé€šè¿‡ç›‘ç£å­¦ä¹ å°†æ¨¡å‹æ¨ç†ç”Ÿæˆçš„3Dæ½œå˜é‡(3D latent)ä¸3DåŸºç¡€æ¨¡å‹(å¦‚VGGT)å¯¹é½ï¼Œéšåä»…åˆ©ç”¨ç»“æœä¿¡å·ä¼˜åŒ–æ¨ç†è½¨è¿¹ä»¥ç²¾ç‚¼å…¶ç©ºé—´æƒ³è±¡èƒ½åŠ›ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œ3DThinkerçš„è¡¨ç°æŒç»­ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œä¸ºå°†3Dè¡¨å¾æ•´åˆè¿›å¤šæ¨¡æ€æ¨ç†æä¾›äº†å…¨æ–°çš„è§†è§’ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚ç©ºé—´ä»»åŠ¡ä¸­çš„å‡ ä½•æƒ³è±¡ä¸æ¨ç†æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.18632v1",
      "published_date": "2025-10-21 13:36:58 UTC",
      "updated_date": "2025-10-21 13:36:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:46:40.697357+00:00"
    },
    {
      "arxiv_id": "2510.18631v1",
      "title": "Comparative Expressivity for Structured Argumentation Frameworks with Uncertain Rules and Premises",
      "title_zh": "è§„åˆ™ä¸å‰æå…·æœ‰ä¸ç¡®å®šæ€§çš„ç»“æ„åŒ–è®ºè¾©æ¡†æ¶çš„æ¯”è¾ƒè¡¨è¾¾èƒ½åŠ›ç ”ç©¶",
      "authors": [
        "Carlo Proietti",
        "Antonio Yuste-Ginel"
      ],
      "abstract": "Modelling qualitative uncertainty in formal argumentation is essential both for practical applications and theoretical understanding. Yet, most of the existing works focus on \\textit{abstract} models for arguing with uncertainty. Following a recent trend in the literature, we tackle the open question of studying plausible instantiations of these abstract models. To do so, we ground the uncertainty of arguments in their components, structured within rules and premises. Our main technical contributions are: i) the introduction of a notion of expressivity that can handle abstract and structured formalisms, and ii) the presentation of both negative and positive expressivity results, comparing the expressivity of abstract and structured models of argumentation with uncertainty. These results affect incomplete abstract argumentation frameworks, and their extension with dependencies, on the abstract side, and ASPIC+, on the structured side.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Formal Argumentationä¸­å®šæ€§ä¸ç¡®å®šæ€§å»ºæ¨¡çš„é—®é¢˜ï¼Œæ—¨åœ¨å¼¥è¡¥ç°æœ‰ç ”ç©¶å¤šé›†ä¸­äºæŠ½è±¡æ¨¡å‹è€Œç¼ºä¹å¯¹ç»“æ„åŒ–å®ä¾‹åŒ–ç ”ç©¶çš„ä¸è¶³ã€‚ä½œè€…é€šè¿‡å°†è®ºè¯çš„ä¸ç¡®å®šæ€§æ ¹æ¤äºè§„åˆ™å’Œå‰æç­‰ç»„æˆéƒ¨åˆ†ï¼Œæå‡ºäº†ä¸€ç§èƒ½å¤ŸåŒæ—¶å¤„ç†æŠ½è±¡å’Œç»“æ„åŒ–å½¢å¼åŒ–è¡¨è¾¾çš„Expressivityæ¦‚å¿µã€‚ç ”ç©¶é‡ç‚¹å¯¹æ¯”äº†Incomplete Abstract Argumentation FrameworksåŠå…¶ä¾èµ–æ‰©å±•ä¸ç»“æ„åŒ–æ¨¡å‹ASPIC+åœ¨è¡¨è¾¾èƒ½åŠ›ä¸Šçš„å·®å¼‚ã€‚é€šè¿‡å‘ˆç°ä¸€ç³»åˆ—æ­£é¢å’Œè´Ÿé¢çš„Expressivityç»“æœï¼Œè®ºæ–‡æ­ç¤ºäº†ä¸åŒä¸ç¡®å®šæ€§è®ºè¯æ¨¡å‹ä¹‹é—´çš„é€»è¾‘å…³è”ä¸å±€é™æ€§ã€‚è¿™äº›è´¡çŒ®ä¸ºç†è§£è®ºè¯ä¸­çš„ä¸ç¡®å®šæ€§æä¾›äº†ç†è®ºåŸºç¡€ï¼Œå¹¶ä¸ºå¤æ‚åœºæ™¯ä¸‹çš„è®ºè¯å»ºæ¨¡æä¾›äº†æŠ€æœ¯æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18631v1",
      "published_date": "2025-10-21 13:36:38 UTC",
      "updated_date": "2025-10-21 13:36:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:46:39.972754+00:00"
    },
    {
      "arxiv_id": "2510.18628v1",
      "title": "Leveraging Association Rules for Better Predictions and Better Explanations",
      "title_zh": "åˆ©ç”¨å…³è”è§„åˆ™æå‡é¢„æµ‹æ€§èƒ½ä¸è§£é‡Šè´¨é‡",
      "authors": [
        "Gilles Audemard",
        "Sylvie Coste-Marquis",
        "Pierre Marquis",
        "Mehdi Sabiri",
        "Nicolas Szczepanski"
      ],
      "abstract": "We present a new approach to classification that combines data and knowledge. In this approach, data mining is used to derive association rules (possibly with negations) from data. Those rules are leveraged to increase the predictive performance of tree-based models (decision trees and random forests) used for a classification task. They are also used to improve the corresponding explanation task through the generation of abductive explanations that are more general than those derivable without taking such rules into account. Experiments show that for the two tree-based models under consideration, benefits can be offered by the approach in terms of predictive performance and in terms of explanation sizes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆæ•°æ®ä¸çŸ¥è¯†çš„æ–°å‹åˆ†ç±»æ–¹æ³•ï¼Œåˆ©ç”¨æ•°æ®æŒ–æ˜æŠ€æœ¯ä»æ•°æ®ä¸­æ¨å¯¼å‡ºåŒ…å«å¦å®šå½¢å¼çš„ Association Rulesã€‚è¿™äº›è§„åˆ™è¢«æ•´åˆè¿› Decision Trees å’Œ Random Forests ç­‰æ ‘åŸºæ¨¡å‹ä¸­ï¼Œæ—¨åœ¨æ˜¾è‘—æå‡å…¶åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„é¢„æµ‹æ€§èƒ½ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆæ¯”ä¼ ç»Ÿæ‰‹æ®µæ›´å…·æ™®é€‚æ€§çš„ Abductive Explanationsï¼Œæœ‰æ•ˆæ”¹è¿›äº†æ¨¡å‹çš„è§£é‡Šä»»åŠ¡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨æé«˜æ¨¡å‹é¢„æµ‹å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç¼©å‡è§£é‡Šé•¿åº¦ (Explanation Sizes)ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºå…¼å…·é«˜æ€§èƒ½ä¸é«˜å¯è§£é‡Šæ€§çš„æœºå™¨å­¦ä¹ æ¨¡å‹æä¾›äº†åˆ›æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.18628v1",
      "published_date": "2025-10-21 13:32:02 UTC",
      "updated_date": "2025-10-21 13:32:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:46:42.892401+00:00"
    },
    {
      "arxiv_id": "2510.18931v1",
      "title": "A Justice Lens on Fairness and Ethics Courses in Computing Education: LLM-Assisted Multi-Perspective and Thematic Evaluation",
      "title_zh": "è®¡ç®—æœºæ•™è‚²ä¸­å…¬å¹³ä¸ä¼¦ç†è¯¾ç¨‹çš„æ­£ä¹‰è§†è§’ï¼šåŸºäº LLM çš„å¤šç»´è§†è§’ä¸ä¸»é¢˜æ€§è¯„ä¼°",
      "authors": [
        "Kenya S. Andrews",
        "Deborah Dormah Kanubala",
        "Kehinde Aruleba",
        "Francisco Enrique Vicente Castro",
        "Renata A Revelo"
      ],
      "abstract": "Course syllabi set the tone and expectations for courses, shaping the learning experience for both students and instructors. In computing courses, especially those addressing fairness and ethics in artificial intelligence (AI), machine learning (ML), and algorithmic design, it is imperative that we understand how approaches to navigating barriers to fair outcomes are being addressed.These expectations should be inclusive, transparent, and grounded in promoting critical thinking. Syllabus analysis offers a way to evaluate the coverage, depth, practices, and expectations within a course. Manual syllabus evaluation, however, is time-consuming and prone to inconsistency. To address this, we developed a justice-oriented scoring rubric and asked a large language model (LLM) to review syllabi through a multi-perspective role simulation. Using this rubric, we evaluated 24 syllabi from four perspectives: instructor, departmental chair, institutional reviewer, and external evaluator. We also prompted the LLM to identify thematic trends across the courses. Findings show that multiperspective evaluation aids us in noting nuanced, role-specific priorities, leveraging them to fill hidden gaps in curricula design of AI/ML and related computing courses focused on fairness and ethics. These insights offer concrete directions for improving the design and delivery of fairness, ethics, and justice content in such courses.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è®¡ç®—æ•™è‚²ä¸­å…¬å¹³ä¸ä¼¦ç†è¯¾ç¨‹æ•™å­¦å¤§çº²è¯„ä¼°è€—æ—¶ä¸”ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå¼€å‘äº†ä¸€ç§æ­£ä¹‰å¯¼å‘çš„è¯„åˆ†é‡è¡¨(Justice-oriented scoring rubric)ï¼Œå¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)è¿›è¡Œå¤šè§†è§’è§’è‰²æ¨¡æ‹Ÿ(Multi-perspective role simulation)è¯„ä¼°ã€‚ç ”ç©¶è€…é€šè¿‡æ¨¡æ‹Ÿè®²å¸ˆã€ç³»ä¸»ä»»ã€æœºæ„è¯„å®¡å‘˜å’Œå¤–éƒ¨è¯„ä¼°å‘˜å››ä¸ªè§†è§’ï¼Œå¯¹24ä»½æ•™å­¦å¤§çº²è¿›è¡Œäº†å¤šç»´åº¦è¯„ä»·ä¸ä¸»é¢˜è¶‹åŠ¿åˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§å¤šè§†è§’è¯„ä¼°æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ç»†å¾®ä¸”ç‰¹å®šäºè§’è‰²çš„ä¼˜å…ˆçº§ï¼Œå¹¶æ­ç¤ºAI/MLåŠç›¸å…³è¯¾ç¨‹è®¾è®¡ä¸­éšè—çš„è¯¾ç¨‹ç¼ºå£ã€‚è¯¥é¡¹å·¥ä½œä¸ä»…éªŒè¯äº†LLMåœ¨è¾…åŠ©æ•™è‚²è¯„ä¼°ä¸­çš„æ½œåŠ›ï¼Œä¹Ÿä¸ºä¼˜åŒ–è®¡ç®—æ•™è‚²ä¸­å…³äºå…¬å¹³(Fairness)ã€ä¼¦ç†(Ethics)å’Œæ­£ä¹‰(Justice)å†…å®¹çš„æ•™å­¦è®¾è®¡ä¸äº¤ä»˜æä¾›äº†å…·ä½“çš„æ”¹è¿›æ–¹å‘ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "14 pages, 8 figures, In Review",
      "pdf_url": "https://arxiv.org/pdf/2510.18931v1",
      "published_date": "2025-10-21 13:30:45 UTC",
      "updated_date": "2025-10-21 13:30:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:46:49.494905+00:00"
    },
    {
      "arxiv_id": "2510.18619v2",
      "title": "Visual Attention Reasoning via Hierarchical Search and Self-Verification",
      "title_zh": "åŸºäºå±‚çº§æœç´¢ä¸è‡ªæˆ‘éªŒè¯çš„è§†è§‰æ³¨æ„åŠ›æ¨ç†",
      "authors": [
        "Wei Cai",
        "Jian Zhao",
        "Yuchen Yuan",
        "Tianle Zhang",
        "Ming Zhu",
        "Haichuan Tang",
        "Xuelong Li"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) frequently hallucinate due to their reliance on fragile, linear reasoning and weak visual grounding. We propose Visual Attention Reasoning (VAR), a reinforcement learning framework that reformulates reasoning as a hierarchical search with self-verification. VAR enforces traceable evidence grounding by generating explicit bounding boxes, guided by a novel reward function combining geometric precision and semantic sufficiency. Furthermore, it replaces linear Chain-of-Thought with a tree-search policy capable of backtracking to correct logical errors. Theoretical analysis validates the framework's reliability, and extensive experiments demonstrate that VAR significantly outperforms state-of-the-art methods on complex hallucination and safety benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Multimodal Large Language Models, MLLMs) å› ä¾èµ–çº¿æ€§æ¨ç†å’Œè§†è§‰å¯¹é½ä¸è¶³è€Œå¯¼è‡´çš„å¹»è§‰é—®é¢˜ï¼Œæå‡ºäº†åä¸º Visual Attention Reasoning (VAR) çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚VAR å°†æ¨ç†è¿‡ç¨‹é‡æ–°æ„å»ºä¸ºåŒ…å«è‡ªæˆ‘éªŒè¯çš„å±‚æ¬¡åŒ–æœç´¢ï¼Œå¹¶é€šè¿‡ç»“åˆå‡ ä½•ç²¾åº¦ä¸è¯­ä¹‰å……åˆ†æ€§çš„æ–°å‹å¥–åŠ±å‡½æ•°å¼•å¯¼æ¨¡å‹ç”Ÿæˆæ˜ç¡®çš„ bounding boxesï¼Œç¡®ä¿äº†è¯æ®å®šä½çš„å¯è¿½æº¯æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨èƒ½å¤Ÿå›æº¯å¹¶ä¿®æ­£é€»è¾‘é”™è¯¯çš„ tree-search policy æ›¿ä»£äº†è„†å¼±çš„çº¿æ€§ Chain-of-Thoughtã€‚ç†è®ºåˆ†æå’Œå®éªŒç»“æœå‡è¡¨æ˜ï¼ŒVAR åœ¨å¤æ‚å¹»è§‰åŠå®‰å…¨åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºå½“å‰çš„ state-of-the-art æ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹çš„å¯é æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18619v2",
      "published_date": "2025-10-21 13:18:44 UTC",
      "updated_date": "2026-01-09 14:55:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:46:52.275725+00:00"
    },
    {
      "arxiv_id": "2510.18615v1",
      "title": "A Rectification-Based Approach for Distilling Boosted Trees into Decision Trees",
      "title_zh": "åŸºäºä¿®æ­£çš„æå‡æ ‘åˆ°å†³ç­–æ ‘è’¸é¦æ–¹æ³•",
      "authors": [
        "Gilles Audemard",
        "Sylvie Coste-Marquis",
        "Pierre Marquis",
        "Mehdi Sabiri",
        "Nicolas Szczepanski"
      ],
      "abstract": "We present a new approach for distilling boosted trees into decision trees, in the objective of generating an ML model offering an acceptable compromise in terms of predictive performance and interpretability. We explain how the correction approach called rectification can be used to implement such a distillation process. We show empirically that this approach provides interesting results, in comparison with an approach to distillation achieved by retraining the model.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºrectificationï¼ˆçº æ­£ï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºå°†æå‡æ ‘(boosted trees)è’¸é¦ä¸ºå†³ç­–æ ‘(decision trees)ï¼Œå…¶ç›®æ ‡æ˜¯ç”Ÿæˆåœ¨é¢„æµ‹æ€§èƒ½å’Œå¯è§£é‡Šæ€§ä¹‹é—´è¾¾åˆ°ç†æƒ³å¹³è¡¡çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚ç ”ç©¶è¯¦ç»†é˜è¿°äº†å¦‚ä½•åˆ©ç”¨åä¸ºrectificationçš„ä¿®æ­£æ–¹æ³•æ¥å®ç°è¿™ä¸€è’¸é¦è¿‡ç¨‹ï¼Œé€šè¿‡çº æ­£æœºåˆ¶ä¼˜åŒ–å†³ç­–è·¯å¾„ã€‚å®è¯ç ”ç©¶è¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„é€šè¿‡é‡æ–°è®­ç»ƒæ¨¡å‹è¿›è¡Œçš„è’¸é¦æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºäº†æ›´å…·ä¼˜åŠ¿çš„ç»“æœã€‚è¿™ä¸€æ–¹æ³•ä¸ºæå‡å¤æ‚æ¨¡å‹é€æ˜åº¦çš„åŒæ—¶ä¿ç•™å…¶æ•ˆèƒ½æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "29 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.18615v1",
      "published_date": "2025-10-21 13:14:04 UTC",
      "updated_date": "2025-10-21 13:14:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:46:52.487719+00:00"
    },
    {
      "arxiv_id": "2510.18927v1",
      "title": "BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping",
      "title_zh": "BAPOï¼šé€šè¿‡å¸¦è‡ªé€‚åº”å‰ªåˆ‡çš„å¹³è¡¡ç­–ç•¥ä¼˜åŒ–ç¨³å®šå¤§è¯­è¨€æ¨¡å‹çš„ç¦»ç­–ç•¥å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Zhiheng Xi",
        "Xin Guo",
        "Yang Nan",
        "Enyu Zhou",
        "Junrui Shen",
        "Wenxiang Chen",
        "Jiaqi Liu",
        "Jixuan Huang",
        "Zhihao Zhang",
        "Honglin Guo",
        "Xun Deng",
        "Zhikai Lei",
        "Miao Zheng",
        "Guoteng Wang",
        "Shuo Zhang",
        "Peng Sun",
        "Rui Zheng",
        "Hang Yan",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "abstract": "Reinforcement learning (RL) has recently become the core paradigm for aligning and strengthening large language models (LLMs). Yet, applying RL in off-policy settings--where stale data from past policies are used for training--improves sample efficiency, but remains challenging: policy entropy declines sharply, optimization often becomes unstable and may even collapse. Through theoretical and empirical analysis, we identify two key insights: (i) an imbalance in optimization, where negative-advantage samples dominate the policy gradient, suppressing useful behaviors and risking gradient explosions; and (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping mechanism in PPO-like objectives systematically blocks entropy-increasing updates, thereby driving the policy toward over-exploitation at the expense of exploration. Building on these insights, we propose BAlanced Policy Optimization with Adaptive Clipping (BAPO), a simple yet effective method that dynamically adjusts clipping bounds to adaptively re-balance positive and negative contributions, preserve entropy, and stabilize RL optimization. Across diverse off-policy scenarios--including sample replay and partial rollout--BAPO achieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025 benchmarks, our 7B BAPO model surpasses open-source counterparts such as SkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art results among models of the same scale but also outperforms leading proprietary systems like o3-mini and Gemini-2.5-Flash-Thinking.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åœ¨å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ç¦»ç­–(off-policy)è®¾ç½®ä¸‹è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œæ·±å…¥åˆ†æå¹¶æ­ç¤ºäº†ä¼˜åŒ–ä¸å¹³è¡¡å’Œç†µå‰ªåˆ‡è§„åˆ™(Entropy-Clip Rule)ä¸¤å¤§æ ¸å¿ƒæˆå› ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè´Ÿä¼˜åŠ¿(negative-advantage)æ ·æœ¬çš„æ¢¯åº¦ä¸»å¯¼ä¼šæŠ‘åˆ¶æœ‰ç›Šè¡Œä¸ºï¼Œè€ŒPPOç±»ç®—æ³•çš„å›ºå®šå‰ªåˆ‡æœºåˆ¶åˆ™ç³»ç»Ÿæ€§åœ°é˜»ç¢äº†ç­–ç•¥çš„ç†µå¢æ›´æ–°ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†BAPO (BAlanced Policy Optimization with Adaptive Clipping)æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´å‰ªåˆ‡è¾¹ç•Œæ¥å¹³è¡¡æ­£è´Ÿæ¢¯åº¦è´¡çŒ®ï¼Œä»è€Œåœ¨ä¿æŒç­–ç•¥ç†µçš„åŒæ—¶ç¨³å®šRLä¼˜åŒ–è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒBAPOåœ¨æ ·æœ¬é‡æ”¾(sample replay)ç­‰å¤šç§ç¦»ç­–åœºæ™¯ä¸‹å±•ç°å‡ºæé«˜çš„æ•°æ®æ•ˆç‡å’Œè®­ç»ƒç¨³å®šæ€§ã€‚åœ¨AIME 2024å’ŒAIME 2025åŸºå‡†æµ‹è¯•ä¸­ï¼Œ32Bè§„æ¨¡çš„BAPOæ¨¡å‹ä¸ä»…åˆ·æ–°äº†åŒè§„æ¨¡æ¨¡å‹çš„æ€§èƒ½çºªå½•ï¼Œæ›´è¶…è¶Šäº†o3-miniå’ŒGemini-2.5-Flash-Thinkingç­‰é¡¶å°–å•†ä¸šæ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2510.18927v1",
      "published_date": "2025-10-21 12:55:04 UTC",
      "updated_date": "2025-10-21 12:55:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:46:58.184170+00:00"
    },
    {
      "arxiv_id": "2510.18581v1",
      "title": "The Cost-Benefit of Interdisciplinarity in AI for Mental Health",
      "title_zh": "å¿ƒç†å¥åº·äººå·¥æ™ºèƒ½é¢†åŸŸè·¨å­¦ç§‘åä½œçš„æˆæœ¬æ•ˆç›Šåˆ†æ",
      "authors": [
        "Katerina Drakos",
        "Eva Paraschou",
        "Simay Toplu",
        "Line Harder Clemmensen",
        "Christoph LÃ¼tge",
        "Nicole Nadine LÃ¸nfeldt",
        "Sneha Das"
      ],
      "abstract": "Artificial intelligence has been introduced as a way to improve access to mental health support. However, most AI mental health chatbots rely on a limited range of disciplinary input, and fail to integrate expertise across the chatbot's lifecycle. This paper examines the cost-benefit trade-off of interdisciplinary collaboration in AI mental health chatbots. We argue that involving experts from technology, healthcare, ethics, and law across key lifecycle phases is essential to ensure value-alignment and compliance with the high-risk requirements of the AI Act. We also highlight practical recommendations and existing frameworks to help balance the challenges and benefits of interdisciplinarity in mental health chatbots.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¿ƒç†å¥åº· AI (AI for Mental Health) é¢†åŸŸä¸­è·¨å­¦ç§‘åä½œ (Interdisciplinarity) çš„æˆæœ¬æ•ˆç›Šæƒè¡¡ã€‚é’ˆå¯¹å½“å‰å¿ƒç†å¥åº·èŠå¤©æœºå™¨äººæ™®éç¼ºä¹å¤šå­¦ç§‘è§†è§’ä¸”æœªèƒ½æ•´åˆå…¨ç”Ÿå‘½å‘¨æœŸä¸“ä¸šçŸ¥è¯†çš„é—®é¢˜ï¼Œä½œè€…åˆ†æäº†å¼•å…¥è·¨å­¦ç§‘ä¸“å®¶çš„å¿…è¦æ€§ã€‚æ–‡ç« è®¤ä¸ºï¼Œåœ¨æŠ€æœ¯ã€åŒ»ç–—ã€ä¼¦ç†å’Œæ³•å¾‹é¢†åŸŸä¸“å®¶çš„å…±åŒå‚ä¸ä¸‹ï¼Œæ‰èƒ½ç¡®ä¿ç³»ç»Ÿå®ç°ä»·å€¼å¯¹é½ (Value-alignment) å¹¶ç¬¦åˆäººå·¥æ™ºèƒ½æ³•æ¡ˆ (AI Act) å¯¹é«˜é£é™©äººå·¥æ™ºèƒ½çš„åˆè§„è¦æ±‚ã€‚ç ”ç©¶è¿›ä¸€æ­¥æä¾›äº†å®ç”¨çš„å»ºè®®å’Œç°æœ‰æ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘è€…æœ‰æ•ˆåº”å¯¹è·¨å­¦ç§‘åˆä½œå¸¦æ¥çš„æŒ‘æˆ˜å¹¶å¹³è¡¡å…¶æ”¶ç›Šã€‚è¿™äº›æˆæœä¸ºæ„å»ºæ›´å®‰å…¨ã€åˆè§„ä¸”å…·å¤‡ä¸“ä¸šæ·±åº¦çš„å¿ƒç†å¥åº· AI æ”¯æŒç³»ç»Ÿæä¾›äº†æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted for poster presentation at the AI in Science Summit 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.18581v1",
      "published_date": "2025-10-21 12:34:44 UTC",
      "updated_date": "2025-10-21 12:34:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:46:59.178193+00:00"
    },
    {
      "arxiv_id": "2510.18573v1",
      "title": "Kaleido: Open-Sourced Multi-Subject Reference Video Generation Model",
      "title_zh": "Kaleidoï¼šå¼€æºå¤šä¸»ä½“å‚è€ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹",
      "authors": [
        "Zhenxing Zhang",
        "Jiayan Teng",
        "Zhuoyi Yang",
        "Tiankun Cao",
        "Cheng Wang",
        "Xiaotao Gu",
        "Jie Tang",
        "Dan Guo",
        "Meng Wang"
      ],
      "abstract": "We present Kaleido, a subject-to-video~(S2V) generation framework, which aims to synthesize subject-consistent videos conditioned on multiple reference images of target subjects. Despite recent progress in S2V generation models, existing approaches remain inadequate at maintaining multi-subject consistency and at handling background disentanglement, often resulting in lower reference fidelity and semantic drift under multi-image conditioning. These shortcomings can be attributed to several factors. Primarily, the training dataset suffers from a lack of diversity and high-quality samples, as well as cross-paired data, i.e., paired samples whose components originate from different instances. In addition, the current mechanism for integrating multiple reference images is suboptimal, potentially resulting in the confusion of multiple subjects. To overcome these limitations, we propose a dedicated data construction pipeline, incorporating low-quality sample filtering and diverse data synthesis, to produce consistency-preserving training data. Moreover, we introduce Reference Rotary Positional Encoding (R-RoPE) to process reference images, enabling stable and precise multi-image integration. Extensive experiments across numerous benchmarks demonstrate that Kaleido significantly outperforms previous methods in consistency, fidelity, and generalization, marking an advance in S2V generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Kaleidoï¼Œä¸€ä¸ªå¼€æºçš„å¤šä¸»ä½“å‚è€ƒè§†é¢‘ç”Ÿæˆ(Multi-Subject Reference Video Generation)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰Subject-to-Video (S2V)æ¨¡å‹åœ¨å¤šä¸»ä½“ä¸€è‡´æ€§ã€èƒŒæ™¯è§£è€¦(Background Disentanglement)å’Œå‚è€ƒä¿çœŸåº¦æ–¹é¢çš„ä¸è¶³ã€‚é’ˆå¯¹è®­ç»ƒæ•°æ®ç¼ºä¹å¤šæ ·æ€§åŠå¤šå›¾åƒé›†æˆæœºåˆ¶ä¸ä½³çš„é—®é¢˜ï¼ŒKaleidoè®¾è®¡äº†ä¸€ä¸ªä¸“é—¨çš„æ•°æ®æ„å»ºæµæ°´çº¿ï¼Œé€šè¿‡ä½è´¨é‡æ ·æœ¬è¿‡æ»¤å’Œå¤šæ ·åŒ–æ•°æ®åˆæˆæ¥ç”Ÿæˆä¿æŒä¸€è‡´æ€§çš„è®­ç»ƒæ•°æ®ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†å‚è€ƒæ—‹è½¬ä½ç½®ç¼–ç (Reference Rotary Positional Encoding, R-RoPE)æ¥å¤„ç†å‚è€ƒå›¾åƒï¼Œå®ç°äº†ç¨³å®šä¸”ç²¾ç¡®çš„å¤šå›¾åƒé›†æˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKaleidoåœ¨ä¸€è‡´æ€§ã€ä¿çœŸåº¦å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢å‡æ˜¾è‘—ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œæ ‡å¿—ç€S2Vç”Ÿæˆé¢†åŸŸçš„é‡è¦è¿›å±•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.18573v1",
      "published_date": "2025-10-21 12:28:14 UTC",
      "updated_date": "2025-10-21 12:28:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:47:03.086538+00:00"
    },
    {
      "arxiv_id": "2510.18569v1",
      "title": "QuantEvolve: Automating Quantitative Strategy Discovery through Multi-Agent Evolutionary Framework",
      "title_zh": "QuantEvolveï¼šåŸºäºå¤šæ™ºèƒ½ä½“æ¼”åŒ–æ¡†æ¶çš„è‡ªåŠ¨åŒ–é‡åŒ–ç­–ç•¥å‘ç°",
      "authors": [
        "Junhyeog Yun",
        "Hyoun Jun Lee",
        "Insu Jeon"
      ],
      "abstract": "Automating quantitative trading strategy development in dynamic markets is challenging, especially with increasing demand for personalized investment solutions. Existing methods often fail to explore the vast strategy space while preserving the diversity essential for robust performance across changing market conditions. We present QuantEvolve, an evolutionary framework that combines quality-diversity optimization with hypothesis-driven strategy generation. QuantEvolve employs a feature map aligned with investor preferences, such as strategy type, risk profile, turnover, and return characteristics, to maintain a diverse set of effective strategies. It also integrates a hypothesis-driven multi-agent system to systematically explore the strategy space through iterative generation and evaluation. This approach produces diverse, sophisticated strategies that adapt to both market regime shifts and individual investment needs. Empirical results show that QuantEvolve outperforms conventional baselines, validating its effectiveness. We release a dataset of evolved strategies to support future research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†QuantEvolveï¼Œä¸€ç§æ—¨åœ¨è§£å†³åŠ¨æ€å¸‚åœºä¸‹é‡åŒ–äº¤æ˜“ç­–ç•¥è‡ªåŠ¨åŒ–å¼€å‘ä¸­ç­–ç•¥ç©ºé—´æ¢ç´¢å’Œå¤šæ ·æ€§ç»´æŒéš¾é¢˜çš„è¿›åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†è´¨é‡-å¤šæ ·æ€§ä¼˜åŒ–(Quality-Diversity Optimization)ä¸å‡è®¾é©±åŠ¨(Hypothesis-Driven)çš„ç­–ç•¥ç”Ÿæˆç›¸ç»“åˆï¼Œé€šè¿‡ä¸æŠ•èµ„è€…åå¥½å¯¹é½çš„ç‰¹å¾å›¾æ¥ç®¡ç†å¤šå…ƒåŒ–çš„æœ‰æ•ˆç­–ç•¥é›†ã€‚QuantEvolveæ ¸å¿ƒé›†æˆäº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(Multi-Agent System)ï¼Œåˆ©ç”¨è¿­ä»£ç”Ÿæˆä¸è¯„ä¼°æœºåˆ¶å¯¹ç­–ç•¥ç©ºé—´è¿›è¡Œç³»ç»Ÿæ€§æ¢ç´¢ã€‚è¿™ç§æ–¹æ³•äº§å‡ºçš„å¤æ‚ç­–ç•¥ä¸ä»…èƒ½é€‚åº”å¸‚åœºç¯å¢ƒå˜åŒ–(Market Regime Shifts)ï¼Œè¿˜èƒ½æ»¡è¶³ä¸ªæ€§åŒ–çš„æŠ•èµ„éœ€æ±‚ã€‚å®éªŒç»“æœè¯æ˜QuantEvolveåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºçº¿æ¨¡å‹ï¼Œå±•ç°äº†å…¶åœ¨è‡ªåŠ¨åŒ–ç­–ç•¥å‘ç°æ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜å…¬å¼€äº†è¿›åŒ–ç­–ç•¥æ•°æ®é›†ï¼Œæ—¨åœ¨ä¸ºè¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶æä¾›æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages, 13 figures. Accepted for oral presentation at the 2nd Workshop on LLMs and Generative AI for Finance (AI4F), part of ACM ICAIF 2025, Singapore. Non-archival workshop",
      "pdf_url": "https://arxiv.org/pdf/2510.18569v1",
      "published_date": "2025-10-21 12:22:16 UTC",
      "updated_date": "2025-10-21 12:22:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:47:14.590151+00:00"
    },
    {
      "arxiv_id": "2510.18561v1",
      "title": "Large language models for folktale type automation based on motifs: Cinderella case study",
      "title_zh": "åŸºäºæ¯é¢˜çš„æ°‘é—´æ•…äº‹ç±»å‹è‡ªåŠ¨åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼šCinderella æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "TjaÅ¡a ArÄon",
        "Marko Robnik-Å ikonja",
        "Polona Tratnik"
      ],
      "abstract": "Artificial intelligence approaches are being adapted to many research areas, including digital humanities. We built a methodology for large-scale analyses in folkloristics. Using machine learning and natural language processing, we automatically detected motifs in a large collection of Cinderella variants and analysed their similarities and differences with clustering and dimensionality reduction. The results show that large language models detect complex interactions in tales, enabling computational analysis of extensive text collections and facilitating cross-lingual comparisons.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ•°å­—äººæ–‡(Digital Humanities)é¢†åŸŸçš„åº”ç”¨ï¼Œæå‡ºäº†ä¸€ç§é’ˆå¯¹æ°‘é—´æ•…äº‹ç±»å‹çš„æ¯é¢˜(Motifs)è‡ªåŠ¨åŒ–åˆ†ææ–¹æ³•ã€‚ç ”ç©¶ç»“åˆæœºå™¨å­¦ä¹ (Machine Learning)å’Œè‡ªç„¶è¯­è¨€å¤„ç†(NLP)æŠ€æœ¯ï¼Œåœ¨ã€Šç°å§‘å¨˜ã€‹(Cinderella)å˜ä½“çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šå®ç°äº†æ¯é¢˜çš„è‡ªåŠ¨æ£€æµ‹ï¼Œå¹¶åˆ©ç”¨èšç±»(Clustering)å’Œé™ç»´(Dimensionality Reduction)æŠ€æœ¯æ·±å…¥åˆ†æäº†æ•…äº‹é—´çš„å¼‚åŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«æ•…äº‹ä¸­å¤æ‚çš„äº¤äº’æ¨¡å¼ï¼Œæ”¯æŒå¯¹æµ·é‡æ–‡æœ¬é›†è¿›è¡Œé«˜æ•ˆçš„è®¡ç®—åˆ†æã€‚è¿™ä¸€æ–¹æ³•è®ºä¸ºæ°‘ä¿—å­¦(Folkloristics)çš„å¤§è§„æ¨¡ç ”ç©¶æä¾›äº†æ”¯æŒï¼Œå¹¶æ˜¾è‘—ä¿ƒè¿›äº†è·¨è¯­è¨€çš„æ•…äº‹æ¯”è¾ƒåˆ†æã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18561v1",
      "published_date": "2025-10-21 12:18:20 UTC",
      "updated_date": "2025-10-21 12:18:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:47:24.787703+00:00"
    },
    {
      "arxiv_id": "2510.18560v1",
      "title": "WebDevJudge: Evaluating (M)LLMs as Critiques for Web Development Quality",
      "title_zh": "WebDevJudgeï¼šè¯„ä¼° (M)LLM ä½œä¸º Web å¼€å‘è´¨é‡è¯„åˆ¤è€…çš„èƒ½åŠ›",
      "authors": [
        "Chunyang Li",
        "Yilun Zheng",
        "Xinting Huang",
        "Tianqing Fang",
        "Jiahao Xu",
        "Yangqiu Song",
        "Lihui Chen",
        "Han Hu"
      ],
      "abstract": "The paradigm of LLM-as-a-judge is emerging as a scalable and efficient alternative to human evaluation, demonstrating strong performance on well-defined tasks. However, its reliability in open-ended tasks with dynamic environments and complex interactions remains unexplored. To bridge the gap, we introduce WebDevJudge, a systematic benchmark for assessing LLM-as-a-judge performance in web development, with support for both non-interactive evaluation based on static observations and continuous interactive evaluation with a dynamic web environment. WebDevJudge comprises human preference labels over paired web implementations, annotated with structured and query-grounded rubrics to ensure high-quality ground truth. Using this benchmark, we comprehensively evaluate various evaluators, including LLMs, MLLMs, and agentic workflows. We systematically investigate the impact of different paradigms and guidance mechanisms. Our experiments reveal a significant gap between LLM judges and human experts. In-depth analysis indicates this gap stems from fundamental model limitations, including failures in recognizing functional equivalence, verifying task feasibility, and mitigating bias. Overall, WebDevJudge presents a significant challenge to LLM-as-a-judge, offering insights to guide future research toward developing more reliable and capable automated evaluators for complicated scenarios. Code and data are available at https://github.com/lcy2723/WebDevJudge.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† WebDevJudgeï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼° (M)LLMs åœ¨ç½‘é¡µå¼€å‘é¢†åŸŸä½œä¸ºè¯„åˆ¤è€… (LLM-as-a-judge) è¡¨ç°çš„ç³»ç»Ÿæ€§åŸºå‡†æµ‹è¯•ã€‚WebDevJudge åŒæ—¶æ”¯æŒåŸºäºé™æ€è§‚å¯Ÿçš„éäº¤äº’å¼è¯„ä¼°ä»¥åŠåœ¨åŠ¨æ€ç½‘é¡µç¯å¢ƒä¸­çš„æŒç»­äº¤äº’å¼è¯„ä¼°ï¼Œå¡«è¡¥äº†å¼€æ”¾æ€§å¤æ‚ä»»åŠ¡è¯„ä¼°é¢†åŸŸçš„ç©ºç™½ã€‚è¯¥åŸºå‡†åŒ…å«ç»è¿‡äººç±»åå¥½æ ‡æ³¨çš„ç½‘é¡µå®ç°å¯¹ï¼Œå¹¶é…å¤‡äº†ç»“æ„åŒ–ä¸”åŸºäºæŸ¥è¯¢çš„è¯„åˆ†å‡†åˆ™ (rubrics) ä»¥ç¡®ä¿æ•°æ®è´¨é‡ã€‚ç ”ç©¶äººå‘˜å¯¹åŒ…æ‹¬ LLMsã€MLLMs åŠæ™ºèƒ½ä½“å·¥ä½œæµ (agentic workflows) åœ¨å†…çš„å¤šç§è¯„ä¼°å™¨è¿›è¡Œäº†å®éªŒï¼Œå‘ç°è‡ªåŠ¨è¯„åˆ¤è€…ä¸äººç±»ä¸“å®¶ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚åˆ†æè¡¨æ˜ï¼Œè¿™ç§å·®è·ä¸»è¦æºäºæ¨¡å‹åœ¨è¯†åˆ«åŠŸèƒ½ç­‰æ•ˆæ€§ (functional equivalence)ã€éªŒè¯ä»»åŠ¡å¯è¡Œæ€§ä»¥åŠå‡è½»åè§æ–¹é¢çš„å±€é™æ€§ã€‚WebDevJudge çš„æå‡ºä¸ä»…æ­ç¤ºäº†å½“å‰æ¨¡å‹çš„çŸ­æ¿ï¼Œä¹Ÿä¸ºæœªæ¥å¼€å‘æ›´å¯é çš„å¤æ‚åœºæ™¯è‡ªåŠ¨è¯„ä¼°ç³»ç»ŸæŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18560v1",
      "published_date": "2025-10-21 12:16:04 UTC",
      "updated_date": "2025-10-21 12:16:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:47:22.386560+00:00"
    },
    {
      "arxiv_id": "2510.18559v1",
      "title": "RAISE: A Unified Framework for Responsible AI Scoring and Evaluation",
      "title_zh": "RAISEï¼šè´Ÿè´£ä»»äººå·¥æ™ºèƒ½è¯„åˆ†ä¸è¯„ä¼°çš„ç»Ÿä¸€æ¡†æ¶",
      "authors": [
        "Loc Phuc Truong Nguyen",
        "Hung Thanh Do"
      ],
      "abstract": "As AI systems enter high-stakes domains, evaluation must extend beyond predictive accuracy to include explainability, fairness, robustness, and sustainability. We introduce RAISE (Responsible AI Scoring and Evaluation), a unified framework that quantifies model performance across these four dimensions and aggregates them into a single, holistic Responsibility Score. We evaluated three deep learning models: a Multilayer Perceptron (MLP), a Tabular ResNet, and a Feature Tokenizer Transformer, on structured datasets from finance, healthcare, and socioeconomics. Our findings reveal critical trade-offs: the MLP demonstrated strong sustainability and robustness, the Transformer excelled in explainability and fairness at a very high environmental cost, and the Tabular ResNet offered a balanced profile. These results underscore that no single model dominates across all responsibility criteria, highlighting the necessity of multi-dimensional evaluation for responsible model selection. Our implementation is available at: https://github.com/raise-framework/raise.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RAISE (Responsible AI Scoring and Evaluation)ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œæ—¨åœ¨é‡åŒ– AI æ¨¡å‹åœ¨å¯è§£é‡Šæ€§ (Explainability)ã€å…¬å¹³æ€§ (Fairness)ã€ç¨³å¥æ€§ (Robustness) å’Œå¯æŒç»­æ€§ (Sustainability) å››ä¸ªç»´åº¦çš„è¡¨ç°ã€‚RAISE é€šè¿‡å°†è¿™äº›æŒ‡æ ‡æ•´åˆä¸ºå•ä¸€çš„ç»¼åˆ Responsibility Scoreï¼Œä½¿è¯„ä¼°èƒ½å¤Ÿè¶…è¶Šä¼ ç»Ÿçš„é¢„æµ‹å‡†ç¡®æ€§ï¼Œå®ç°å¯¹æ¨¡å‹è´£ä»»æ€§çš„å…¨é¢è¡¡é‡ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨é‡‘èã€åŒ»ç–—å’Œç¤¾ä¼šç»æµé¢†åŸŸçš„ç»“æ„åŒ–æ•°æ®é›†ä¸Šï¼Œå¯¹ MLPã€Tabular ResNet å’Œ Feature Tokenizer Transformer ä¸‰ç§æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœæ­ç¤ºäº†ä¸åŒæ¨¡å‹æ¶æ„ä¹‹é—´çš„å…³é”®æƒè¡¡ï¼šTransformer åœ¨ Explainability å’Œ Fairness æ–¹é¢è¡¨ç°å“è¶Šï¼Œä½†å…¶ç¯å¢ƒæˆæœ¬æé«˜ï¼›MLP åœ¨ Sustainability å’Œ Robustness ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œè€Œ Tabular ResNet åˆ™å±•ç°äº†è¾ƒä¸ºå‡è¡¡çš„æ€§èƒ½ç‰¹å¾ã€‚è¿™äº›å‘ç°è¯æ˜æ²¡æœ‰å•ä¸€æ¨¡å‹èƒ½åœ¨æ‰€æœ‰è´£ä»»å‡†åˆ™ä¸­å æ®ç»å¯¹ä¸»å¯¼åœ°ä½ï¼Œçªæ˜¾äº†åœ¨è´Ÿè´£ä»»æ¨¡å‹é€‰æ‹©è¿‡ç¨‹ä¸­è¿›è¡Œå¤šç»´åº¦è¯„ä¼°çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the 26th International Conference on Principles and Practice of Multi-Agent Systems",
      "pdf_url": "https://arxiv.org/pdf/2510.18559v1",
      "published_date": "2025-10-21 12:15:13 UTC",
      "updated_date": "2025-10-21 12:15:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:47:25.291303+00:00"
    },
    {
      "arxiv_id": "2510.18554v2",
      "title": "Extracting alignment data in open models",
      "title_zh": "å¼€æ”¾æ¨¡å‹ä¸­çš„å¯¹é½æ•°æ®æå–",
      "authors": [
        "Federico Barbero",
        "Xiangming Gu",
        "Christopher A. Choquette-Choo",
        "Chawin Sitawarin",
        "Matthew Jagielski",
        "Itay Yona",
        "Petar VeliÄkoviÄ‡",
        "Ilia Shumailov",
        "Jamie Hayes"
      ],
      "abstract": "In this work, we show that it is possible to extract significant amounts of alignment training data from a post-trained model -- useful to steer the model to improve certain capabilities such as long-context reasoning, safety, instruction following, and maths. While the majority of related work on memorisation has focused on measuring success of training data extraction through string matching, we argue that embedding models are better suited for our specific goals. Distances measured through a high quality embedding model can identify semantic similarities between strings that a different metric such as edit distance will struggle to capture. In fact, in our investigation, approximate string matching would have severely undercounted (by a conservative estimate of $10\\times$) the amount of data that can be extracted due to trivial artifacts that deflate the metric. Interestingly, we find that models readily regurgitate training data that was used in post-training phases such as SFT or RL. We show that this data can be then used to train a base model, recovering a meaningful amount of the original performance. We believe our work exposes a possibly overlooked risk towards extracting alignment data. Finally, our work opens up an interesting discussion on the downstream effects of distillation practices: since models seem to be regurgitating aspects of their training set, distillation can therefore be thought of as indirectly training on the model's original dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶å±•ç¤ºäº†ä»åæœŸè®­ç»ƒï¼ˆpost-trainedï¼‰æ¨¡å‹ä¸­æå–å¤§é‡å¯¹é½ï¼ˆalignmentï¼‰è®­ç»ƒæ•°æ®çš„å¯èƒ½æ€§ï¼Œè¿™äº›æ•°æ®å¯¹äºæå‡æ¨¡å‹çš„é•¿æ–‡æœ¬æ¨ç†ã€å®‰å…¨æ€§åŠæ•°å­¦èƒ½åŠ›å…·æœ‰æ˜¾è‘—ä½œç”¨ã€‚ç ”ç©¶è€…æŒ‡å‡ºï¼Œç›¸æ¯”äºä¼ ç»Ÿçš„å­—ç¬¦ä¸²åŒ¹é…ï¼ˆstring matchingï¼‰ï¼Œä½¿ç”¨é«˜è´¨é‡çš„åµŒå…¥æ¨¡å‹ï¼ˆembedding modelsï¼‰èƒ½æ›´ç²¾å‡†åœ°è¯†åˆ«è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œé¿å…äº†å› çç¢äººå·¥ç—•è¿¹å¯¼è‡´çš„æå–é‡ä½ä¼°ã€‚å®éªŒå‘ç°ï¼Œæ¨¡å‹ä¼šé¢‘ç¹å¤ç°ï¼ˆregurgitateï¼‰åœ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç­‰å¯¹é½é˜¶æ®µä½¿ç”¨çš„è®­ç»ƒæ•°æ®ã€‚é€šè¿‡è¿™äº›æå–å‡ºçš„æ•°æ®é‡æ–°è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œå¯ä»¥æ¢å¤åŸå§‹æ¨¡å‹æ˜¾è‘—çš„æ€§èƒ½æ°´å¹³ã€‚è¯¥å·¥ä½œä¸ä»…æ­ç¤ºäº†å¯¹é½æ•°æ®è¢«æå–çš„æ½œåœ¨å®‰å…¨é£é™©ï¼Œè¿˜å¼•å‘äº†å…³äºçŸ¥è¯†è’¸é¦ï¼ˆdistillationï¼‰æœ¬è´¨ä¸Šæ˜¯é—´æ¥åˆ©ç”¨åŸå§‹æ•°æ®é›†è¿›è¡Œè®­ç»ƒçš„æ·±åˆ»è®¨è®ºã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18554v2",
      "published_date": "2025-10-21 12:06:00 UTC",
      "updated_date": "2025-10-23 21:46:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:47:27.901103+00:00"
    },
    {
      "arxiv_id": "2512.08934v1",
      "title": "Motion2Meaning: A Clinician-Centered Framework for Contestable LLM in Parkinson's Disease Gait Interpretation",
      "title_zh": "Motion2Meaningï¼šé¢å‘å¸•é‡‘æ£®ç—…æ­¥æ€è§£æçš„ä»¥ä¸´åºŠåŒ»ç”Ÿä¸ºä¸­å¿ƒçš„å¯è´¨è¯¢å¤§è¯­è¨€æ¨¡å‹æ¡†æ¶",
      "authors": [
        "Loc Phuc Truong Nguyen",
        "Hung Thanh Do",
        "Hung Truong Thanh Nguyen",
        "Hung Cao"
      ],
      "abstract": "AI-assisted gait analysis holds promise for improving Parkinson's Disease (PD) care, but current clinical dashboards lack transparency and offer no meaningful way for clinicians to interrogate or contest AI decisions. To address this issue, we present Motion2Meaning, a clinician-centered framework that advances Contestable AI through a tightly integrated interface designed for interpretability, oversight, and procedural recourse. Our approach leverages vertical Ground Reaction Force (vGRF) time-series data from wearable sensors as an objective biomarker of PD motor states. The system comprises three key components: a Gait Data Visualization Interface (GDVI), a one-dimensional Convolutional Neural Network (1D-CNN) that predicts Hoehn & Yahr severity stages, and a Contestable Interpretation Interface (CII) that combines our novel Cross-Modal Explanation Discrepancy (XMED) safeguard with a contestable Large Language Model (LLM). Our 1D-CNN achieves 89.0% F1-score on the public PhysioNet gait dataset. XMED successfully identifies model unreliability by detecting a five-fold increase in explanation discrepancies in incorrect predictions (7.45%) compared to correct ones (1.56%), while our LLM-powered interface enables clinicians to validate correct predictions and successfully contest a portion of the model's errors. A human-centered evaluation of this contestable interface reveals a crucial trade-off between the LLM's factual grounding and its readability and responsiveness to clinical feedback. This work demonstrates the feasibility of combining wearable sensor analysis with Explainable AI (XAI) and contestable LLMs to create a transparent, auditable system for PD gait interpretation that maintains clinical oversight while leveraging advanced AI capabilities. Our implementation is publicly available at: https://github.com/hungdothanh/motion2meaning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Motion2Meaningï¼Œä¸€ä¸ªä»¥ä¸´åºŠåŒ»ç”Ÿä¸ºä¸­å¿ƒçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯è´¨ç–‘äººå·¥æ™ºèƒ½(Contestable AI)æå‡å¸•é‡‘æ£®ç—…(Parkinson's Disease)æ­¥æ€è§£è¯»çš„é€æ˜åº¦ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨å¯ç©¿æˆ´ä¼ æ„Ÿå™¨è·å–çš„å‚ç›´åœ°é¢åä½œç”¨åŠ›(vGRF)æ—¶é—´åºåˆ—æ•°æ®ä½œä¸ºå®¢è§‚ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œå…¶æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬æ­¥æ€æ•°æ®å¯è§†åŒ–ç•Œé¢(GDVI)ã€ç”¨äºé¢„æµ‹Hoehn & Yahrä¸¥é‡ç¨‹åº¦åˆ†çº§çš„1D-CNNï¼Œä»¥åŠé›†æˆæ–°å‹è·¨æ¨¡æ€è§£é‡Šå·®å¼‚(XMED)ä¿éšœæœºåˆ¶ä¸å¤§è¯­è¨€æ¨¡å‹(LLM)çš„å¯è´¨ç–‘è§£é‡Šç•Œé¢(CII)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œ1D-CNNåœ¨å…¬å¼€æ•°æ®é›†ä¸Šè¾¾åˆ°äº†89.0%çš„F1-scoreï¼Œè€ŒXMEDé€šè¿‡æ£€æµ‹åˆ°é”™è¯¯é¢„æµ‹ä¸­æ˜¾è‘—å‡é«˜çš„è§£é‡Šå·®å¼‚ç‡(7.45%)æœ‰æ•ˆè¯†åˆ«äº†æ¨¡å‹çš„ä¸ç¡®å®šæ€§ã€‚è¯¥æ¡†æ¶å…è®¸ä¸´åºŠåŒ»ç”ŸéªŒè¯æˆ–è´¨ç–‘æ¨¡å‹å†³ç­–ï¼Œå¹¶æ­ç¤ºäº†LLMåœ¨äº‹å®åŸºç¡€ä¸ä¸´åºŠåé¦ˆå“åº”æ€§ä¹‹é—´çš„å…³é”®æƒè¡¡ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†ç»“åˆå¯ç©¿æˆ´ä¼ æ„Ÿå™¨åˆ†æã€å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)ä¸LLMæ„å»ºé€æ˜ä¸”å¯å®¡è®¡çš„æ­¥æ€åˆ†æç³»ç»Ÿçš„å¯è¡Œæ€§ï¼Œåœ¨ç»´æŒä¸´åºŠç›‘ç®¡çš„åŒæ—¶å……åˆ†åˆ©ç”¨äº†å…ˆè¿›çš„AIèƒ½åŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted at the 9th International Symposium on Chatbots and Human-Centered AI",
      "pdf_url": "https://arxiv.org/pdf/2512.08934v1",
      "published_date": "2025-10-21 12:04:58 UTC",
      "updated_date": "2025-10-21 12:04:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:47:30.980996+00:00"
    },
    {
      "arxiv_id": "2510.18551v2",
      "title": "SOCIA-Nabla: Textual Gradient Meets Multi-Agent Orchestration for Automated Simulator Generation",
      "title_zh": "SOCIA-Nablaï¼šèåˆæ–‡æœ¬æ¢¯åº¦ä¸å¤šæ™ºèƒ½ä½“ç¼–æ’çš„è‡ªåŠ¨åŒ–æ¨¡æ‹Ÿå™¨ç”Ÿæˆ",
      "authors": [
        "Yuncheng Hua",
        "Sion Weatherhead",
        "Mehdi Jafari",
        "Hao Xue",
        "Flora D. Salim"
      ],
      "abstract": "In this paper, we present SOCIA-Nabla, an end-to-end, agentic framework that treats simulator construction asinstance optimization over code within a textual computation graph. Specialized LLM-driven agents are embedded as graph nodes, and a workflow manager executes a loss-driven loop: code synthesis -> execution -> evaluation -> code repair. The optimizer performs Textual-Gradient Descent (TGD), while human-in-the-loop interaction is reserved for task-spec confirmation, minimizing expert effort and keeping the code itself as the trainable object. Across three CPS tasks, i.e., User Modeling, Mask Adoption, and Personal Mobility, SOCIA-Nabla attains state-of-the-art overall accuracy. By unifying multi-agent orchestration with a loss-aligned optimization view, SOCIA-Nabla converts brittle prompt pipelines into reproducible, constraint-aware simulator code generation that scales across domains and simulation granularities. This work is under review, and we will release the code soon.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SOCIA-Nablaï¼Œä¸€ä¸ªå°†æ¨¡æ‹Ÿå™¨æ„å»ºè§†ä¸ºæ–‡æœ¬è®¡ç®—å›¾å†…ä»£ç å®ä¾‹ä¼˜åŒ–çš„ç«¯åˆ°ç«¯æ™ºèƒ½ä½“æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†ç‰¹å®šçš„ LLM é©±åŠ¨æ™ºèƒ½ä½“ä½œä¸ºå›¾èŠ‚ç‚¹ï¼Œå¹¶é€šè¿‡å·¥ä½œæµç®¡ç†å™¨æ‰§è¡Œä»£ç åˆæˆã€æ‰§è¡Œã€è¯„ä¼°å’Œä¿®å¤çš„é—­ç¯æµç¨‹ã€‚å…¶æ ¸å¿ƒä¼˜åŒ–å™¨é‡‡ç”¨ Textual-Gradient Descent (TGD) ç­–ç•¥ï¼Œä»…åœ¨ä»»åŠ¡è§„èŒƒé˜¶æ®µå¼•å…¥äººç±»åé¦ˆï¼Œæœ‰æ•ˆé™ä½äº†ä¸“å®¶å‚ä¸æˆæœ¬ã€‚åœ¨ User Modelingã€Mask Adoption å’Œ Personal Mobility ç­‰ç½‘ç»œç‰©ç†ç³»ç»Ÿ (CPS) ä»»åŠ¡çš„å®éªŒä¸­ï¼ŒSOCIA-Nabla å–å¾—äº† SOTA çº§åˆ«çš„å‡†ç¡®ç‡ã€‚é€šè¿‡èåˆå¤šæ™ºèƒ½ä½“ç¼–æ’ (multi-agent orchestration) ä¸æŸå¤±å¯¹é½çš„ä¼˜åŒ–æœºåˆ¶ï¼Œè¯¥æ–¹æ¡ˆå°†è„†å¼±çš„æç¤ºè¯æµæ°´çº¿è½¬åŒ–ä¸ºå¯é‡å¤ã€å…·å¤‡çº¦æŸæ„ŸçŸ¥èƒ½åŠ›ä¸”è·¨é¢†åŸŸå¯æ‰©å±•çš„æ¨¡æ‹Ÿå™¨ä»£ç ç”Ÿæˆç³»ç»Ÿã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "superseded by newest version of arXiv:2505.12006",
      "pdf_url": "https://arxiv.org/pdf/2510.18551v2",
      "published_date": "2025-10-21 12:00:00 UTC",
      "updated_date": "2025-11-10 14:12:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:47:35.586942+00:00"
    },
    {
      "arxiv_id": "2510.18546v2",
      "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval",
      "title_zh": "EfficientNavï¼šåŸºäºå¯¼èˆªåœ°å›¾ç¼“å­˜ä¸æ£€ç´¢çš„ç«¯ä¾§ç‰©ä½“ç›®æ ‡å¯¼èˆª",
      "authors": [
        "Zebin Yang",
        "Sunjian Zheng",
        "Tong Xie",
        "Tianshi Xu",
        "Bo Yu",
        "Fan Wang",
        "Jie Tang",
        "Shaoshan Liu",
        "Meng Li"
      ],
      "abstract": "Object-goal navigation (ObjNav) tasks an agent with navigating to the location of a specific object in an unseen environment. Embodied agents equipped with large language models (LLMs) and online constructed navigation maps can perform ObjNav in a zero-shot manner. However, existing agents heavily rely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small LLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to limited model capacity for understanding complex navigation maps, which prevents deploying ObjNav on local devices. At the same time, the long prompt introduced by the navigation map description will cause high planning latency on local devices. In this paper, we propose EfficientNav to enable on-device efficient LLM-based zero-shot ObjNav. To help the smaller LLMs better understand the environment, we propose semantics-aware memory retrieval to prune redundant information in navigation maps. To reduce planning latency, we propose discrete memory caching and attention-based memory clustering to efficiently save and re-use the KV cache. Extensive experimental results demonstrate that EfficientNav achieves 11.1% improvement in success rate on HM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time latency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our code is available on https://github.com/PKU-SEC-Lab/EfficientNav.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EfficientNavï¼Œæ—¨åœ¨è§£å†³åœ¨æœ¬åœ°è®¾å¤‡ä¸Šéƒ¨ç½²åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„é›¶æ ·æœ¬ Object-goal navigation (ObjNav) æ—¶é¢ä¸´çš„æ€§èƒ½ä¸‹é™å’Œé«˜è§„åˆ’å»¶è¿Ÿé—®é¢˜ã€‚ä¸ºäº†æå‡å°å‹ LLM å¯¹ç¯å¢ƒçš„ç†è§£èƒ½åŠ›ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†è¯­ä¹‰æ„ŸçŸ¥è®°å¿†æ£€ç´¢ (semantics-aware memory retrieval) æŠ€æœ¯æ¥ä¿®å‰ªå¯¼èˆªåœ°å›¾ä¸­çš„å†—ä½™ä¿¡æ¯ã€‚åŒæ—¶ï¼Œä¸ºäº†é™ä½è§„åˆ’å»¶è¿Ÿï¼Œè¯¥æ–¹æ¡ˆé‡‡ç”¨äº†ç¦»æ•£è®°å¿†ç¼“å­˜ (discrete memory caching) å’ŒåŸºäºæ³¨æ„åŠ›çš„è®°å¿†èšç±» (attention-based memory clustering) ä»¥å®ç° KV cache çš„é«˜æ•ˆä¿å­˜ä¸é‡ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEfficientNav åœ¨ HM3D åŸºå‡†æµ‹è¯•ä¸­çš„æˆåŠŸç‡æ¯”åŸºäº GPT-4 çš„åŸºçº¿æ¨¡å‹æå‡äº† 11.1%ï¼Œä¸”å®æ—¶å»¶è¿Ÿå’Œç«¯åˆ°ç«¯å»¶è¿Ÿåˆ†åˆ«é™ä½äº† 6.7 å€å’Œ 4.7 å€ã€‚è¯¥æˆæœè¯æ˜äº†åœ¨æœ¬åœ°è®¾å¤‡ä¸Šå®ç°é«˜æ•ˆã€é«˜æ€§èƒ½çš„å…·èº«å¯¼èˆªä»»åŠ¡çš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.18546v2",
      "published_date": "2025-10-21 11:52:44 UTC",
      "updated_date": "2025-11-27 13:54:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:47:37.089238+00:00"
    },
    {
      "arxiv_id": "2510.18925v1",
      "title": "Application of Reduced-Order Models for Temporal Multiscale Representations in the Prediction of Dynamical Systems",
      "title_zh": "é™é˜¶æ¨¡å‹åœ¨åŠ¨åŠ›ç³»ç»Ÿé¢„æµ‹ä¸­æ—¶é—´å¤šå°ºåº¦è¡¨ç¤ºçš„åº”ç”¨",
      "authors": [
        "Elias Al Ghazal",
        "Jad Mounayer",
        "Beatriz Moya",
        "Sebastian Rodriguez",
        "Chady Ghnatios",
        "Francisco Chinesta"
      ],
      "abstract": "Modeling and predicting the dynamics of complex multiscale systems remains a significant challenge due to their inherent nonlinearities and sensitivity to initial conditions, as well as limitations of traditional machine learning methods that fail to capture high frequency behaviours. To overcome these difficulties, we propose three approaches for multiscale learning. The first leverages the Partition of Unity (PU) method, integrated with neural networks, to decompose the dynamics into local components and directly predict both macro- and micro-scale behaviors. The second applies the Singular Value Decomposition (SVD) to extract dominant modes that explicitly separate macro- and micro-scale dynamics. Since full access to the data matrix is rarely available in practice, we further employ a Sparse High-Order SVD to reconstruct multiscale dynamics from limited measurements. Together, these approaches ensure that both coarse and fine dynamics are accurately captured, making the framework effective for real-world applications involving complex, multi-scale phenomena and adaptable to higher-dimensional systems with incomplete observations, by providing an approximation and interpretation in all time scales present in the phenomena under study.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤æ‚å¤šå°ºåº¦åŠ¨åŠ›ç³»ç»Ÿé¢„æµ‹ä¸­ç”±äºéçº¿æ€§ã€åˆå§‹æ¡ä»¶æ•æ„Ÿæ€§ä»¥åŠä¼ ç»Ÿæœºå™¨å­¦ä¹ éš¾ä»¥æ•æ‰é«˜é¢‘è¡Œä¸ºæ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸‰ç§åˆ›æ–°çš„å¤šå°ºåº¦å­¦ä¹ æ–¹æ³•ã€‚ç¬¬ä¸€ç§æ–¹æ³•ç»“åˆäº†å•ä½åˆ†è§£æ³•(Partition of Unity)ä¸ç¥ç»ç½‘ç»œï¼Œé€šè¿‡å°†åŠ¨åŠ›å­¦åˆ†è§£ä¸ºå±€éƒ¨ç»„ä»¶æ¥ç›´æ¥é¢„æµ‹å®è§‚å’Œå¾®è§‚å°ºåº¦çš„è¡Œä¸ºã€‚ç¬¬äºŒç§æ–¹æ³•åˆ©ç”¨å¥‡å¼‚å€¼åˆ†è§£(Singular Value Decomposition)æå–ä¸»æ¨¡æ€ï¼Œå®ç°äº†å®è§‚ä¸å¾®è§‚åŠ¨åŠ›å­¦çš„æ˜¾å¼åˆ†ç¦»ã€‚ç¬¬ä¸‰ç§æ–¹æ³•åˆ™é‡‡ç”¨ç¨€ç–é«˜é˜¶SVD(Sparse High-Order SVD)ï¼Œè§£å†³äº†å®é™…åº”ç”¨ä¸­æ•°æ®çŸ©é˜µéš¾ä»¥å®Œå…¨è·å–çš„é—®é¢˜ï¼Œä»è€Œèƒ½å¤Ÿä»æœ‰é™çš„è§‚æµ‹ä¸­é‡æ„å¤šå°ºåº¦åŠ¨åŠ›å­¦ã€‚è¯¥æ¡†æ¶ç¡®ä¿äº†å¯¹ç²—ç•¥å’Œç²¾ç»†åŠ¨æ€çš„å‡†ç¡®æ•æ‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å…·æœ‰ä¸å®Œæ•´è§‚æµ‹çš„é«˜ç»´ç³»ç»Ÿï¼Œå¹¶ä¸ºå¤æ‚å¤šå°ºåº¦ç°è±¡çš„æ‰€æœ‰æ—¶é—´å°ºåº¦æä¾›ç²¾ç¡®çš„è¿‘ä¼¼ä¸ç‰©ç†è§£é‡Šã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Regular research article, 28 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.18925v1",
      "published_date": "2025-10-21 11:46:15 UTC",
      "updated_date": "2025-10-21 11:46:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:47:54.702629+00:00"
    },
    {
      "arxiv_id": "2510.18541v1",
      "title": "Pay Attention to the Triggers: Constructing Backdoors That Survive Distillation",
      "title_zh": "èšç„¦è§¦å‘å™¨ï¼šæ„å»ºåœ¨çŸ¥è¯†è’¸é¦ä¸­å­˜ç»­çš„åé—¨",
      "authors": [
        "Giovanni De Muri",
        "Mark Vero",
        "Robin Staab",
        "Martin Vechev"
      ],
      "abstract": "LLMs are often used by downstream users as teacher models for knowledge distillation, compressing their capabilities into memory-efficient models. However, as these teacher models may stem from untrusted parties, distillation can raise unexpected security risks. In this paper, we investigate the security implications of knowledge distillation from backdoored teacher models. First, we show that prior backdoors mostly do not transfer onto student models. Our key insight is that this is because existing LLM backdooring methods choose trigger tokens that rarely occur in usual contexts. We argue that this underestimates the security risks of knowledge distillation and introduce a new backdooring technique, T-MTB, that enables the construction and study of transferable backdoors. T-MTB carefully constructs a composite backdoor trigger, made up of several specific tokens that often occur individually in anticipated distillation datasets. As such, the poisoned teacher remains stealthy, while during distillation the individual presence of these tokens provides enough signal for the backdoor to transfer onto the student. Using T-MTB, we demonstrate and extensively study the security risks of transferable backdoors across two attack scenarios, jailbreaking and content modulation, and across four model families of LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨çŸ¥è¯†è’¸é¦(knowledge distillation)è¿‡ç¨‹ä¸­é¢ä¸´çš„å®‰å…¨éšæ‚£ï¼ŒæŒ‡å‡ºä»ä¸å¯ä¿¡çš„æ•™å¸ˆæ¨¡å‹è¿›è¡Œè’¸é¦å¯èƒ½å¼•å…¥åé—¨(backdoor)æ”»å‡»ã€‚ä½œè€…åˆ†æå‘ç°ï¼Œç°æœ‰çš„åé—¨æ–¹æ³•ä¹‹æ‰€ä»¥éš¾ä»¥è½¬ç§»è‡³å­¦ç”Ÿæ¨¡å‹ï¼Œæ˜¯å› ä¸ºå…¶é€‰å–çš„è§¦å‘å™¨(trigger tokens)åœ¨å¸¸è§„è¯­å¢ƒä¸­å‡ºç°é¢‘ç‡æä½ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºT-MTBçš„æ–°å‹åé—¨æŠ€æœ¯ï¼Œé€šè¿‡æ„å»ºç”±è’¸é¦æ•°æ®é›†ä¸­é¢‘ç¹å‡ºç°çš„å¤šä¸ªè¯é¡¹ç»„æˆçš„å¤åˆè§¦å‘å™¨ï¼Œå®ç°äº†æ”»å‡»ä¿¡å·çš„æœ‰æ•ˆä¼ é€’ã€‚è¿™ç§è®¾è®¡ä½¿å¾—ä¸­æ¯’çš„æ•™å¸ˆæ¨¡å‹åœ¨ä¿æŒéšè”½æ€§çš„åŒæ—¶ï¼Œèƒ½å¤Ÿè®©å­¦ç”Ÿæ¨¡å‹åœ¨è’¸é¦åç»§æ‰¿åé—¨ç‰¹æ€§ã€‚å®éªŒåœ¨è¶Šç‹±(jailbreaking)å’Œå†…å®¹è°ƒåˆ¶(content modulation)ä¸¤ç§æ”»å‡»åœºæ™¯ä»¥åŠå››ä¸ªLLMæ¨¡å‹å®¶æ—ä¸ŠéªŒè¯äº†è¯¥æŠ€æœ¯çš„å¯è¡Œæ€§ï¼Œæ­ç¤ºäº†çŸ¥è¯†è’¸é¦ä¸­æ½œåœ¨çš„å®‰å…¨æ¼æ´ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18541v1",
      "published_date": "2025-10-21 11:39:45 UTC",
      "updated_date": "2025-10-21 11:39:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:47:46.975880+00:00"
    },
    {
      "arxiv_id": "2511.01870v1",
      "title": "CytoNet: A Foundation Model for the Human Cerebral Cortex",
      "title_zh": "CytoNetï¼šäººç±»å¤§è„‘çš®å±‚åŸºç¡€æ¨¡å‹",
      "authors": [
        "Christian Schiffer",
        "Zeynep Boztoprak",
        "Jan-Oliver Kropp",
        "Julia ThÃ¶nniÃŸen",
        "Katia Berr",
        "Hannah Spitzer",
        "Katrin Amunts",
        "Timo Dickscheid"
      ],
      "abstract": "To study how the human brain works, we need to explore the organization of the cerebral cortex and its detailed cellular architecture. We introduce CytoNet, a foundation model that encodes high-resolution microscopic image patches of the cerebral cortex into highly expressive feature representations, enabling comprehensive brain analyses. CytoNet employs self-supervised learning using spatial proximity as a powerful training signal, without requiring manual labelling. The resulting features are anatomically sound and biologically relevant. They encode general aspects of cortical architecture and unique brain-specific traits. We demonstrate top-tier performance in tasks such as cortical area classification, cortical layer segmentation, cell morphology estimation, and unsupervised brain region mapping. As a foundation model, CytoNet offers a consistent framework for studying cortical microarchitecture, supporting analyses of its relationship with other structural and functional brain features, and paving the way for diverse neuroscientific investigations.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†CytoNetï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹äººç±»å¤§è„‘çš®å±‚(Cerebral Cortex)å¼€å‘çš„åŸºåº§æ¨¡å‹(Foundation Model)ï¼Œæ—¨åœ¨å°†é«˜åˆ†è¾¨ç‡æ˜¾å¾®å›¾åƒå—ç¼–ç ä¸ºæå…·è¡¨ç°åŠ›çš„ç‰¹å¾è¡¨ç¤ºã€‚CytoNeté‡‡ç”¨äº†è‡ªç›‘ç£å­¦ä¹ (Self-Supervised Learning)æ–¹æ³•ï¼Œåˆ©ç”¨ç©ºé—´é‚»è¿‘æ€§ä½œä¸ºå…³é”®è®­ç»ƒä¿¡å·ï¼Œæ— éœ€äººå·¥æ ‡æ³¨å³å¯å­¦ä¹ åˆ°å…·æœ‰è§£å‰–å­¦æ„ä¹‰å’Œç”Ÿç‰©å­¦ç›¸å…³æ€§çš„ç‰¹å¾ã€‚è¿™äº›ç‰¹å¾ä¸ä»…æ¶µç›–äº†çš®å±‚æ¶æ„çš„é€šç”¨æ–¹é¢ï¼Œè¿˜æ•æ‰åˆ°äº†å¤§è„‘ç‰¹æœ‰çš„æ€§çŠ¶ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨çš®å±‚åŒºåŸŸåˆ†ç±»(Cortical Area Classification)ã€çš®å±‚åˆ†å±‚åˆ†å‰²(Cortical Layer Segmentation)ã€ç»†èƒå½¢æ€ä¼°è®¡(Cell Morphology Estimation)ä»¥åŠæ— ç›‘ç£å¤§è„‘åŒºåŸŸæ˜ å°„(Unsupervised Brain Region Mapping)ç­‰ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºé¡¶å°–æ€§èƒ½ã€‚ä½œä¸ºä¸€ç§åŸºåº§æ¨¡å‹ï¼ŒCytoNetä¸ºç ”ç©¶çš®å±‚å¾®è§‚ç»“æ„æä¾›äº†ç»Ÿä¸€çš„æ¡†æ¶ï¼Œæ”¯æŒåˆ†æå…¶ä¸å…¶ä»–å¤§è„‘ç»“æ„åŠåŠŸèƒ½ç‰¹å¾çš„å…³ç³»ï¼Œä¸ºå¤šæ ·åŒ–çš„ç¥ç»ç§‘å­¦æ¢ç´¢å¼€è¾Ÿäº†é“è·¯ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "comment": "Under review for journal publication",
      "pdf_url": "https://arxiv.org/pdf/2511.01870v1",
      "published_date": "2025-10-21 11:39:23 UTC",
      "updated_date": "2025-10-21 11:39:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:48:08.592163+00:00"
    },
    {
      "arxiv_id": "2510.18535v1",
      "title": "Physics-guided Emulators Reveal Resilience and Fragility under Operational Latencies and Outages",
      "title_zh": "ç‰©ç†å¼•å¯¼çš„ä»¿çœŸå™¨æ­ç¤ºè¿è¡Œå»¶è¿Ÿä¸ä¸­æ–­ä¸‹çš„éŸ§æ€§ä¸è„†å¼±æ€§",
      "authors": [
        "Sarth Dubey",
        "Subimal Ghosh",
        "Udit Bhatia"
      ],
      "abstract": "Reliable hydrologic and flood forecasting requires models that remain stable when input data are delayed, missing, or inconsistent. However, most advances in rainfall-runoff prediction have been evaluated under ideal data conditions, emphasizing accuracy rather than operational resilience. Here, we develop an operationally ready emulator of the Global Flood Awareness System (GloFAS) that couples long- and short-term memory networks with a relaxed water-balance constraint to preserve physical coherence. Five architectures span a continuum of information availability: from complete historical and forecast forcings to scenarios with data latency and outages, allowing systematic evaluation of robustness. Trained in minimally managed catchments across the United States and tested in more than 5,000 basins, including heavily regulated rivers in India, the emulator reproduces the hydrological core of GloFAS and degrades smoothly as information quality declines. Transfer across contrasting hydroclimatic and management regimes yields reduced yet physically consistent performance, defining the limits of generalization under data scarcity and human influence. The framework establishes operational robustness as a measurable property of hydrological machine learning and advances the design of reliable real-time forecasting systems.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§é’ˆå¯¹å…¨çƒæ´ªæ°´é¢„è­¦ç³»ç»Ÿ(GloFAS)çš„ç‰©ç†å¼•å¯¼ä»¿çœŸå™¨ï¼Œé‡ç‚¹è§£å†³æ°´æ–‡é¢„æµ‹åœ¨æ•°æ®å»¶è¿Ÿã€ç¼ºå¤±æˆ–ä¸ä¸€è‡´æƒ…å†µä¸‹çš„è¿è¡ŒéŸ§æ€§(operational resilience)é—®é¢˜ã€‚è¯¥æ–¹æ³•å°†é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ(long- and short-term memory networks)ä¸æ¾å¼›çš„æ°´é‡å¹³è¡¡çº¦æŸ(relaxed water-balance constraint)ç›¸ç»“åˆï¼Œä»¥ç¡®ä¿é¢„æµ‹ç»“æœçš„ç‰©ç†ä¸€è‡´æ€§ã€‚ç ”ç©¶äººå‘˜è®¾è®¡äº†äº”ç§æ¶µç›–ä¸åŒä¿¡æ¯å¯ç”¨æ€§çš„æ¶æ„ï¼Œç³»ç»Ÿåœ°è¯„ä¼°äº†æ¨¡å‹åœ¨æ•°æ®å»¶è¿Ÿå’Œæ•…éšœåœºæ™¯ä¸‹çš„ç¨³å¥æ€§ã€‚é€šè¿‡åœ¨ç¾å›½å’Œå°åº¦è¶…è¿‡5,000ä¸ªæµåŸŸè¿›è¡Œçš„æµ‹è¯•ï¼Œè¯¥ä»¿çœŸå™¨æˆåŠŸå¤ç°äº†GloFASçš„æ°´æ–‡æ ¸å¿ƒï¼Œå¹¶åœ¨ä¿¡æ¯è´¨é‡ä¸‹é™æ—¶è¡¨ç°å‡ºå¹³ç¨³çš„æ€§èƒ½é€€åŒ–ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥ç•Œå®šäº†æ¨¡å‹åœ¨æ•°æ®åŒ®ä¹å’Œäººä¸ºå¹²æ‰°ç¯å¢ƒä¸‹çš„æ³›åŒ–æé™ï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒæ°´æ–‡æ°”å€™å’Œç®¡ç†ä½“åˆ¶ä¸‹ä»èƒ½ä¿æŒç‰©ç†ä¸€è‡´ã€‚è¯¥æ¡†æ¶å°†è¿è¡Œç¨³å¥æ€§ç¡®ç«‹ä¸ºæ°´æ–‡æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå¯è¡¡é‡å±æ€§ï¼Œä¸ºè®¾è®¡æ›´å¯é çš„å®æ—¶æ´ªæ°´é¢„æŠ¥ç³»ç»Ÿæä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "45 pages, 5 main figures, 10 supplementary figures, 5 supplementary tables",
      "pdf_url": "https://arxiv.org/pdf/2510.18535v1",
      "published_date": "2025-10-21 11:25:31 UTC",
      "updated_date": "2025-10-21 11:25:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:48:13.084460+00:00"
    },
    {
      "arxiv_id": "2510.18526v2",
      "title": "Counterfactual Reasoning for Steerable Pluralistic Value Alignment of Large Language Models",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹å¯æ§å¤šå…ƒä»·å€¼å¯¹é½çš„åäº‹å®æ¨ç†",
      "authors": [
        "Hanze Guo",
        "Jing Yao",
        "Xiao Zhou",
        "Xiaoyuan Yi",
        "Xing Xie"
      ],
      "abstract": "As large language models (LLMs) become increasingly integrated into applications serving users across diverse cultures, communities and demographics, it is critical to align LLMs with pluralistic human values beyond average principles (e.g., HHH). In psychological and social value theories such as Schwartz's Value Theory, pluralistic values are represented by multiple value dimensions paired with various priorities. However, existing methods encounter two challenges when aligning with such fine-grained value objectives: 1) they often treat multiple values as independent and equally important, ignoring their interdependence and relative priorities (value complexity); 2) they struggle to precisely control nuanced value priorities, especially those underrepresented ones (value steerability). To handle these challenges, we propose COUPLE, a COUnterfactual reasoning framework for PLuralistic valuE alignment. It introduces a structural causal model (SCM) to feature complex interdependency and prioritization among features, as well as the causal relationship between high-level value dimensions and behaviors. Moreover, it applies counterfactual reasoning to generate outputs aligned with any desired value objectives. Benefitting from explicit causal modeling, COUPLE also provides better interpretability. We evaluate COUPLE on two datasets with different value systems and demonstrate that COUPLE advances other baselines across diverse types of value objectives.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† COUPLE æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤šå…ƒä»·å€¼è§‚å¯¹é½ (Pluralistic Value Alignment) è¿‡ç¨‹ä¸­é¢ä¸´çš„ä»·å€¼è§‚å¤æ‚æ€§ (Value Complexity) å’Œå¯æ§æ€§ (Value Steerability) æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å¿½ç•¥äº†ä»·å€¼ç»´åº¦é—´çš„ç›¸äº’ä¾èµ–ä¸ä¼˜å…ˆçº§ï¼Œä¸”éš¾ä»¥ç²¾ç¡®æ§åˆ¶ç‰¹å®šä»·å€¼ç›®æ ‡ã€‚COUPLE å¼•å…¥äº†ç»“æ„å› æœæ¨¡å‹ (Structural Causal Model, SCM) æ¥åˆ»ç”»é«˜å±‚ä»·å€¼ç»´åº¦ä¸è¡Œä¸ºä¹‹é—´çš„å› æœå…³ç³»ï¼Œå¹¶åº”ç”¨åäº‹å®æ¨ç† (Counterfactual Reasoning) ç”Ÿæˆç¬¦åˆç‰¹å®šä»·å€¼ç›®æ ‡çš„è¾“å‡ºã€‚å¾—ç›Šäºæ˜¾å¼çš„å› æœå»ºæ¨¡ï¼Œè¯¥æ¡†æ¶åœ¨æä¾›æ›´å¥½å¯è§£é‡Šæ€§ (Interpretability) çš„åŒæ—¶ï¼Œå®ç°äº†å¯¹ç»†ç²’åº¦ä»·å€¼ä¼˜å…ˆçº§çš„ç²¾å‡†å¼•å¯¼ã€‚åœ¨ä¸¤ä¸ªä¸åŒä»·å€¼ç³»ç»Ÿæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒCOUPLE åœ¨å¤šç§ä»·å€¼ç›®æ ‡ä¸‹å‡ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œæœ‰æ•ˆæå‡äº† LLMs åœ¨å¤šå…ƒæ–‡åŒ–èƒŒæ™¯ä¸‹çš„å¯¹é½æ•ˆæœã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "NeurIPS 2025. 41 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.18526v2",
      "published_date": "2025-10-21 11:12:45 UTC",
      "updated_date": "2025-12-05 01:37:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:48:11.192558+00:00"
    },
    {
      "arxiv_id": "2510.18502v1",
      "title": "Zero-Shot Vehicle Model Recognition via Text-Based Retrieval-Augmented Generation",
      "title_zh": "åŸºäºæ–‡æœ¬æ£€ç´¢å¢å¼ºç”Ÿæˆçš„é›¶æ ·æœ¬è½¦å‹è¯†åˆ«",
      "authors": [
        "Wei-Chia Chang",
        "Yan-Ann Chen"
      ],
      "abstract": "Vehicle make and model recognition (VMMR) is an important task in intelligent transportation systems, but existing approaches struggle to adapt to newly released models. Contrastive Language-Image Pretraining (CLIP) provides strong visual-text alignment, yet its fixed pretrained weights limit performance without costly image-specific finetuning. We propose a pipeline that integrates vision language models (VLMs) with Retrieval-Augmented Generation (RAG) to support zero-shot recognition through text-based reasoning. A VLM converts vehicle images into descriptive attributes, which are compared against a database of textual features. Relevant entries are retrieved and combined with the description to form a prompt, and a language model (LM) infers the make and model. This design avoids large-scale retraining and enables rapid updates by adding textual descriptions of new vehicles. Experiments show that the proposed method improves recognition by nearly 20% over the CLIP baseline, demonstrating the potential of RAG-enhanced LM reasoning for scalable VMMR in smart-city applications.",
      "tldr_zh": "è½¦è¾†å“ç‰Œå’Œå‹å·è¯†åˆ« (VMMR) æ˜¯æ™ºèƒ½äº¤é€šç³»ç»Ÿçš„å…³é”®ä»»åŠ¡ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨é€‚åº”æ–°è½¦å‹æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) çš„æµç¨‹ï¼Œé€šè¿‡åŸºäºæ–‡æœ¬çš„æ¨ç†å®ç°é›¶æ ·æœ¬ (Zero-Shot) è¯†åˆ«ã€‚è¯¥æ¶æ„åˆ©ç”¨ VLM å°†è½¦è¾†å›¾åƒè½¬åŒ–ä¸ºæè¿°æ€§å±æ€§ï¼Œå¹¶å°†å…¶ä¸æ–‡æœ¬ç‰¹å¾æ•°æ®åº“è¿›è¡Œæ£€ç´¢æ¯”å¯¹ï¼Œæœ€åç”±è¯­è¨€æ¨¡å‹ (LM) æ ¹æ®æ£€ç´¢ä¿¡æ¯æ¨æ–­å‡ºå…·ä½“çš„å“ç‰Œå’Œå‹å·ã€‚è¿™ç§è®¾è®¡æ— éœ€å¤§è§„æ¨¡é‡æ–°è®­ç»ƒï¼Œä»…é€šè¿‡æ›´æ–°æ–‡æœ¬æ•°æ®åº“å³å¯å¿«é€Ÿå®ç°å¯¹æ–°è½¦å‹çš„è¯†åˆ«ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•è¾ƒ CLIP åŸºçº¿æ¨¡å‹åœ¨è¯†åˆ«å‡†ç¡®ç‡ä¸Šæå‡äº†çº¦ 20%ï¼Œå……åˆ†å±•ç¤ºäº† RAG å¢å¼ºå‹ LM æ¨ç†åœ¨æ™ºèƒ½åŸå¸‚åº”ç”¨ä¸­å®ç°å¯æ‰©å±• VMMR çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by The 38th Conference of Open Innovations Association FRUCT, 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.18502v1",
      "published_date": "2025-10-21 10:39:39 UTC",
      "updated_date": "2025-10-21 10:39:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:48:15.774866+00:00"
    },
    {
      "arxiv_id": "2510.18493v1",
      "title": "One Size Fits All? A Modular Adaptive Sanitization Kit (MASK) for Customizable Privacy-Preserving Phone Scam Detection",
      "title_zh": "é€šç”¨æ–¹æ¡ˆæ˜¯å¦è¶³çŸ£ï¼Ÿé¢å‘å¯å®šåˆ¶éšç§ä¿æŠ¤ç”µè¯è¯ˆéª—æ£€æµ‹çš„æ¨¡å—åŒ–è‡ªé€‚åº”è„±æ•å·¥å…·åŒ… (MASK)",
      "authors": [
        "Kangzhong Wang",
        "Zitong Shen",
        "Youqian Zhang",
        "Michael MK Cheung",
        "Xiapu Luo",
        "Grace Ngai",
        "Eugene Yujun Fu"
      ],
      "abstract": "Phone scams remain a pervasive threat to both personal safety and financial security worldwide. Recent advances in large language models (LLMs) have demonstrated strong potential in detecting fraudulent behavior by analyzing transcribed phone conversations. However, these capabilities introduce notable privacy risks, as such conversations frequently contain sensitive personal information that may be exposed to third-party service providers during processing. In this work, we explore how to harness LLMs for phone scam detection while preserving user privacy. We propose MASK (Modular Adaptive Sanitization Kit), a trainable and extensible framework that enables dynamic privacy adjustment based on individual preferences. MASK provides a pluggable architecture that accommodates diverse sanitization methods - from traditional keyword-based techniques for high-privacy users to sophisticated neural approaches for those prioritizing accuracy. We also discuss potential modeling approaches and loss function designs for future development, enabling the creation of truly personalized, privacy-aware LLM-based detection systems that balance user trust and detection effectiveness, even beyond phone scam context.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç”µè¯è¯ˆéª—æ£€æµ‹ä¸­å¯èƒ½å¼•å‘çš„éšç§æ³„éœ²é£é™©ï¼Œæå‡ºäº†MASK (Modular Adaptive Sanitization Kit)ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯è®­ç»ƒä¸”å¯æ‰©å±•çš„æ¨¡å—åŒ–è‡ªé€‚åº”è„±æ•æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¯æ’æ‹”æ¶æ„ï¼Œå…è®¸æ ¹æ®ç”¨æˆ·çš„ä¸ªäººåå¥½åŠ¨æ€è°ƒæ•´éšç§ä¿æŠ¤å¼ºåº¦ï¼Œä»¥å…¼å®¹å¤šæ ·åŒ–çš„è„±æ•éœ€æ±‚ã€‚MASKèƒ½å¤Ÿæ•´åˆä»ä¼ ç»Ÿçš„åŸºäºå…³é”®è¯(Keyword-based)çš„è„±æ•æŠ€æœ¯åˆ°å…ˆè¿›çš„ç¥ç»è„±æ•æ–¹æ³•(Neural approaches)ï¼Œä»è€Œå¹³è¡¡é«˜éšç§éœ€æ±‚ä¸æ£€æµ‹å‡†ç¡®åº¦ä¹‹é—´çš„çŸ›ç›¾ã€‚ç ”ç©¶è¿˜è¿›ä¸€æ­¥æ¢è®¨äº†æœªæ¥çš„å»ºæ¨¡æ–¹æ³•ä¸æŸå¤±å‡½æ•°è®¾è®¡ï¼Œæ—¨åœ¨æ„å»ºçœŸæ­£ä¸ªæ€§åŒ–çš„éšç§æ„ŸçŸ¥å‹æ£€æµ‹ç³»ç»Ÿã€‚é€šè¿‡åœ¨ä¿æŠ¤æ•æ„Ÿä¿¡æ¯ä¸ç»´æŒæ£€æµ‹æ•ˆèƒ½ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œè¯¥å·¥ä½œä¸ºå¼€å‘å¯ä¿¡ã€å®‰å…¨çš„LLMè¾…åŠ©é˜²è¯ˆéª—æŠ€æœ¯æä¾›äº†é‡è¦æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CR",
      "comment": "9 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.18493v1",
      "published_date": "2025-10-21 10:30:36 UTC",
      "updated_date": "2025-10-21 10:30:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:48:21.274612+00:00"
    },
    {
      "arxiv_id": "2510.21812v1",
      "title": "Unifying Inductive, Cross-Domain, and Multimodal Learning for Robust and Generalizable Recommendation",
      "title_zh": "èåˆå½’çº³ã€è·¨åŸŸä¸å¤šæ¨¡æ€å­¦ä¹ çš„ç¨³å¥å¯æ³›åŒ–æ¨è",
      "authors": [
        "Chanyoung Chung",
        "Kyeongryul Lee",
        "Sunbin Park",
        "Joyce Jiyoung Whang"
      ],
      "abstract": "Recommender systems have long been built upon the modeling of interactions between users and items, while recent studies have sought to broaden this paradigm by generalizing to new users and items, incorporating diverse information sources, and transferring knowledge across domains. Nevertheless, these efforts have largely focused on individual aspects, hindering their ability to tackle the complex recommendation scenarios that arise in daily consumptions across diverse domains. In this paper, we present MICRec, a unified framework that fuses inductive modeling, multimodal guidance, and cross-domain transfer to capture user contexts and latent preferences in heterogeneous and incomplete real-world data. Moving beyond the inductive backbone of INMO, our model refines expressive representations through modality-based aggregation and alleviates data sparsity by leveraging overlapping users as anchors across domains, thereby enabling robust and generalizable recommendation. Experiments show that MICRec outperforms 12 baselines, with notable gains in domains with limited training data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MICRecï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¨èæ¡†æ¶ï¼Œé€šè¿‡èåˆå½’çº³å»ºæ¨¡ (Inductive Modeling)ã€å¤šæ¨¡æ€å¼•å¯¼ (Multimodal Guidance) å’Œè·¨åŸŸè¿ç§» (Cross-Domain Transfer) æ¥åº”å¯¹ç°å®ä¸–ç•Œä¸­å¼‚æ„ä¸”ä¸å®Œæ•´æ•°æ®å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹åœ¨ INMO çš„å½’çº³éª¨å¹²ç½‘ç»œåŸºç¡€ä¸Šè¿›è¡Œäº†æ‰©å±•ï¼Œé€šè¿‡åŸºäºæ¨¡æ€çš„èšåˆ (Modality-Based Aggregation) æå–æ›´å…·è¡¨ç°åŠ›çš„ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨è·¨é¢†åŸŸçš„é‡å ç”¨æˆ·ä½œä¸ºé”šç‚¹æ¥ç¼“è§£æ•°æ®ç¨€ç–é—®é¢˜ã€‚è¿™ç§æ•´åˆç­–ç•¥ä¸ä»…èƒ½å¤Ÿç²¾å‡†æ•æ‰ç”¨æˆ·çš„ä¸Šä¸‹æ–‡å’Œæ½œåœ¨åå¥½ï¼Œè¿˜æ˜¾è‘—å¢å¼ºäº†æ¨èç³»ç»Ÿçš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMICRec åœ¨ä¸ 12 ä¸ªåŸºçº¿æ¨¡å‹çš„å¯¹æ¯”ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨è®­ç»ƒæ•°æ®æœ‰é™çš„é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½å¢ç›Šã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "7 pages, 3 figures, and 4 tables. International Workshop on Multimodal Generative Search and Recommendation (MMGenSR) at The 34th ACM International Conference on Information and Knowledge Management (CIKM 2025)",
      "pdf_url": "https://arxiv.org/pdf/2510.21812v1",
      "published_date": "2025-10-21 10:26:33 UTC",
      "updated_date": "2025-10-21 10:26:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:48:21.695649+00:00"
    },
    {
      "arxiv_id": "2510.24757v1",
      "title": "Stable-by-Design Neural Network-Based LPV State-Space Models for System Identification",
      "title_zh": "ç”¨äºç³»ç»Ÿè¾¨è¯†çš„ç»“æ„åŒ–ç¨³å®šç¥ç»ç½‘ç»œ LPV çŠ¶æ€ç©ºé—´æ¨¡å‹",
      "authors": [
        "Ahmet Eren SertbaÅŸ",
        "Tufan Kumbasar"
      ],
      "abstract": "Accurate modeling of nonlinear systems is essential for reliable control, yet conventional identification methods often struggle to capture latent dynamics while maintaining stability. We propose a \\textit{stable-by-design LPV neural network-based state-space} (NN-SS) model that simultaneously learns latent states and internal scheduling variables directly from data. The state-transition matrix, generated by a neural network using the learned scheduling variables, is guaranteed to be stable through a Schur-based parameterization. The architecture combines an encoder for initial state estimation with a state-space representer network that constructs the full set of scheduling-dependent system matrices. For training the NN-SS, we develop a framework that integrates multi-step prediction losses with a state-consistency regularization term, ensuring robustness against drift and improving long-horizon prediction accuracy. The proposed NN-SS is evaluated on benchmark nonlinear systems, and the results demonstrate that the model consistently matches or surpasses classical subspace identification methods and recent gradient-based approaches. These findings highlight the potential of stability-constrained neural LPV identification as a scalable and reliable framework for modeling complex nonlinear systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè®¾è®¡å³ç¨³å®š(stable-by-design)çš„çº¿æ€§å‚æ•°å˜åŒ–(LPV)ç¥ç»ç½‘ç»œçŠ¶æ€ç©ºé—´(NN-SS)æ¨¡å‹ï¼Œæ—¨åœ¨ç›´æ¥ä»æ•°æ®ä¸­å­¦ä¹ æ½œåœ¨çŠ¶æ€å’Œå†…éƒ¨è°ƒåº¦å˜é‡ï¼Œä»¥è§£å†³éçº¿æ€§ç³»ç»Ÿå»ºæ¨¡ä¸­çš„ç¨³å®šæ€§éš¾é¢˜ã€‚è¯¥æ¶æ„å°†ç”¨äºåˆå§‹çŠ¶æ€ä¼°è®¡çš„ç¼–ç å™¨(encoder)ä¸æ„å»ºè°ƒåº¦ä¾èµ–ç³»ç»ŸçŸ©é˜µçš„çŠ¶æ€ç©ºé—´è¡¨ç¤ºç½‘ç»œ(state-space representer network)ç›¸ç»“åˆã€‚é€šè¿‡é‡‡ç”¨åŸºäºSchurçš„å‚æ•°åŒ–(Schur-based parameterization)æ–¹æ³•ï¼Œæ¨¡å‹èƒ½å¤Ÿç¡®ä¿ç”±ç¥ç»ç½‘ç»œç”Ÿæˆçš„è°ƒåº¦å˜é‡çŠ¶æ€è½¬ç§»çŸ©é˜µåœ¨è®¾è®¡å±‚é¢å…·æœ‰ç¨³å®šæ€§ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¯¥æ¡†æ¶æ•´åˆäº†å¤šæ­¥é¢„æµ‹æŸå¤±(multi-step prediction losses)å’ŒçŠ¶æ€ä¸€è‡´æ€§æ­£åˆ™åŒ–(state-consistency regularization)ï¼Œæœ‰æ•ˆæé«˜äº†æ¨¡å‹å¯¹æŠ—æ¼‚ç§»çš„é²æ£’æ€§å¹¶å¢å¼ºäº†é•¿æ—¶åŸŸé¢„æµ‹ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨åŸºå‡†éçº¿æ€§ç³»ç»Ÿä¸Šçš„è¡¨ç°ä¸€è‡´è¾¾åˆ°æˆ–è¶…è¿‡äº†ç»å…¸çš„å­ç©ºé—´è¯†åˆ«(subspace identification)æ–¹æ³•ä»¥åŠè¿‘æœŸçš„æ¢¯åº¦ä¸‹é™ç®—æ³•ã€‚è¿™ä¸€ç ”ç©¶å±•ç¤ºäº†å—ç¨³å®šæ€§çº¦æŸçš„ç¥ç»ç½‘ç»œLPVè¯†åˆ«æŠ€æœ¯åœ¨æ„å»ºå¯æ‰©å±•ä¸”å¯é çš„å¤æ‚éçº¿æ€§ç³»ç»Ÿæ¨¡å‹æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SY",
      "comment": "In the 12th International Conference of Image Processing, Wavelet and Applications on Real World Problems, 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.24757v1",
      "published_date": "2025-10-21 10:25:54 UTC",
      "updated_date": "2025-10-21 10:25:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:48:24.982026+00:00"
    },
    {
      "arxiv_id": "2510.18491v1",
      "title": "Crucible: Quantifying the Potential of Control Algorithms through LLM Agents",
      "title_zh": "Crucibleï¼šå€ŸåŠ©å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“é‡åŒ–æ§åˆ¶ç®—æ³•çš„æ½œåŠ›",
      "authors": [
        "Lianchen Jia",
        "Chaoyang Li",
        "Qian Houde",
        "Tianchi Huang",
        "Jiangchuan Liu",
        "Lifeng Sun"
      ],
      "abstract": "Control algorithms in production environments typically require domain experts to tune their parameters and logic for specific scenarios. However, existing research predominantly focuses on algorithmic performance under ideal or default configurations, overlooking the critical aspect of Tuning Potential. To bridge this gap, we introduce Crucible, an agent that employs an LLM-driven, multi-level expert simulation to turn algorithms and defines a formalized metric to quantitatively evaluate their Tuning Potential. We demonstrate Crucible's effectiveness across a wide spectrum of case studies, from classic control tasks to complex computer systems, and validate its findings in a real-world deployment. Our experimental results reveal that Crucible systematically quantifies the tunable space across different algorithms. Furthermore, Crucible provides a new dimension for algorithm analysis and design, which ultimately leads to performance improvements. Our code is available at https://github.com/thu-media/Crucible.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿäº§ç¯å¢ƒä¸­æ§åˆ¶ç®—æ³•(Control algorithms)é«˜åº¦ä¾èµ–ä¸“å®¶è°ƒä¼˜ä¸”ç°æœ‰ç ”ç©¶æ™®éå¿½è§†è°ƒä¼˜æ½œåŠ›(Tuning Potential)é‡åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºCrucibleçš„æ™ºèƒ½ä½“æ¡†æ¶ã€‚Crucibleåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)é©±åŠ¨çš„å¤šå±‚çº§ä¸“å®¶ä»¿çœŸ(multi-level expert simulation)æ¥å®ç°ç®—æ³•è‡ªåŠ¨è°ƒä¼˜ï¼Œå¹¶å®šä¹‰äº†ä¸€å¥—æ­£å¼æŒ‡æ ‡æ¥å®šé‡è¯„ä¼°ç®—æ³•çš„è°ƒä¼˜æ½œåŠ›ã€‚é€šè¿‡æ¶µç›–ç»å…¸æ§åˆ¶ä»»åŠ¡åˆ°å¤æ‚è®¡ç®—æœºç³»ç»Ÿçš„å¹¿æ³›æ¡ˆä¾‹ç ”ç©¶ï¼Œä»¥åŠåœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„éƒ¨ç½²ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCrucibleèƒ½å¤Ÿç³»ç»Ÿåœ°é‡åŒ–ä¸åŒç®—æ³•çš„å¯è°ƒä¼˜ç©ºé—´ï¼Œä¸ºç®—æ³•çš„åˆ†æä¸è®¾è®¡æä¾›äº†å…¨æ–°çš„è¯„ä¼°ç»´åº¦ï¼Œå¹¶æœ€ç»ˆå®ç°äº†æ€§èƒ½çš„æ˜¾è‘—æå‡ã€‚ç›®å‰ï¼Œè¯¥ç ”ç©¶çš„ç›¸å…³ä»£ç å·²åœ¨GitHubå¼€æºã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.18491v1",
      "published_date": "2025-10-21 10:25:26 UTC",
      "updated_date": "2025-10-21 10:25:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:48:27.485052+00:00"
    },
    {
      "arxiv_id": "2510.18924v2",
      "title": "Noise-corrected GRPO: From Noisy Rewards to Unbiased Gradients",
      "title_zh": "å™ªå£°ä¿®æ­£ GRPOï¼šä»å¸¦å™ªå¥–åŠ±åˆ°æ— åæ¢¯åº¦",
      "authors": [
        "Omar El Mansouri",
        "Mohamed El Amine Seddik",
        "Salem Lahlou"
      ],
      "abstract": "Reinforcement learning from human feedback (RLHF) or verifiable rewards (RLVR), the standard paradigm for aligning LLMs or building recent SOTA reasoning models, is highly sensitive to noise from inconsistent or erroneous rewards. Yet, the interaction between such noise and widely used group-based policy optimization methods remains underexplored. We introduce a noise-robust Group Relative Policy Optimization (GRPO) and Done Right GRPO (Dr.GRPO) framework that explicitly models reward corruption as Bernoulli noise. Our method applies noise correction after estimating reward flip probabilities to debias the learning signal, yielding provably unbiased gradient estimates. Theoretical analysis shows that group-based methods inherently mitigate individual-level noise, and our correction strategy amplifies this robustness. Empirically, we observe consistent improvements across math and code tasks when applying our noise correction to standard reward model usage, with particular gains of up to 6.7 percentage points in accuracy on math tasks and 1.5 on code tasks under realistic reward model conditions. This work bridges label-noise correction from supervised learning with modern RLHF, offering both theoretical insights and a practical algorithm for noisy real-world deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ (RLHF)å’Œå¯éªŒè¯å¥–åŠ±(RLVR)åœ¨å¤§è¯­è¨€æ¨¡å‹å¯¹é½ä¸­å¯¹å™ªå£°é«˜åº¦æ•æ„Ÿçš„é—®é¢˜ï¼Œæå‡ºäº†å™ªå£°é²æ£’çš„ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ¡†æ¶Dr.GRPOã€‚è¯¥æ¡†æ¶å°†å¥–åŠ±æŸåæ˜¾å¼å»ºæ¨¡ä¸ºä¼¯åŠªåˆ©å™ªå£°(Bernoulli noise)ï¼Œé€šè¿‡ä¼°è®¡å¥–åŠ±ç¿»è½¬æ¦‚ç‡å¹¶åº”ç”¨å™ªå£°æ ¡æ­£æ¥æ¶ˆé™¤å­¦ä¹ ä¿¡å·çš„åå·®ï¼Œä»è€Œå®ç°å¯è¯æ˜çš„æ— åæ¢¯åº¦ä¼°è®¡(unbiased gradient estimates)ã€‚ç†è®ºåˆ†ææŒ‡å‡ºï¼ŒåŸºäºç¾¤ç»„çš„æ–¹æ³•å¤©ç„¶å…·æœ‰å‡è½»ä¸ªä½“å±‚é¢å™ªå£°çš„ä¼˜åŠ¿ï¼Œè€ŒDr.GRPOçš„æ ¡æ­£ç­–ç•¥è¿›ä¸€æ­¥æ”¾å¤§äº†è¿™ç§é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ•°å­¦å’Œä»£ç ä»»åŠ¡ä¸­åº”ç”¨è¯¥å™ªå£°æ ¡æ­£å¯æ˜¾è‘—æå‡å‡†ç¡®ç‡ï¼Œå…¶ä¸­æ•°å­¦ä»»åŠ¡åœ¨ç°å®å¥–åŠ±æ¨¡å‹æ¡ä»¶ä¸‹æœ€é«˜æå‡è¾¾6.7ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™é¡¹å·¥ä½œæˆåŠŸå°†ç›‘ç£å­¦ä¹ çš„æ ‡ç­¾å™ªå£°æ ¡æ­£æŠ€æœ¯å¼•å…¥ç°ä»£RLHFé¢†åŸŸï¼Œä¸ºçœŸå®ä¸–ç•Œä¸­çš„å™ªå£°ç¯å¢ƒéƒ¨ç½²æä¾›äº†åšå®çš„ç†è®ºåŸºç¡€å’Œå®ç”¨çš„ç®—æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18924v2",
      "published_date": "2025-10-21 10:14:49 UTC",
      "updated_date": "2025-10-27 13:24:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:48:32.155252+00:00"
    },
    {
      "arxiv_id": "2510.18488v1",
      "title": "AndroidControl-Curated: Revealing the True Potential of GUI Agents through Benchmark Purification",
      "title_zh": "AndroidControl-Curatedï¼šé€šè¿‡åŸºå‡†å‡€åŒ–æ­ç¤º GUI æ™ºèƒ½ä½“çš„çœŸå®æ½œåŠ›",
      "authors": [
        "Ho Fai Leung",
        "Xiaoyan Xi",
        "Fei Zuo"
      ],
      "abstract": "On-device virtual assistants like Siri and Google Assistant are increasingly pivotal, yet their capabilities are hamstrung by a reliance on rigid, developer-dependent APIs. GUI agents offer a powerful, API-independent alternative, but their adoption is hindered by the perception of poor performance, as even the best models (e.g. Qwen3-VL-235B) scores are capped at around 60% on benchmarks like AndroidControl, far from viability for real-world use. Our research reveals that issue lies not only with the models but with the benchmarks themselves. We identified notable shortcomings in AndroidControl, including ambiguities and factual errors, which systematically underrates agent capabilities. To address this critical oversight, we enhanced AndroidControl into AndroidControl-Curated, a refined version of the benchmark improved through a rigorous purification pipeline. On this enhanced benchmark, state-of-the-art models achieve success rates nearing 75% on complex tasks (15% improvement), reflecting that on-device GUI agents are actually closer to practical deployment than previously thought. We introduce our new SOTA model, Magma-R1- 3B, post-trained on just 2.4k curated samples using 60 hours of an H20 GPU (approximately $60). Despite being 200 times smaller in parameters, this model delivers performance comparable to Qwen3- VL-235B. We release both AndroidControl-Curated benchmark and Magma-R1 model to the research community, encouraging adoption of this enhanced benchmark to better reflect model capabilities and accelerate the development of robust, on-device virtual assistants.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œç›®å‰çš„GUIæ™ºèƒ½ä½“(GUI agents)åœ¨AndroidControlç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ¬ ä½³ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸ŠæºäºåŸºå‡†æµ‹è¯•æœ¬èº«å­˜åœ¨çš„æ­§ä¹‰å’Œäº‹å®é”™è¯¯ã€‚ä¸ºäº†æ­ç¤ºæ™ºèƒ½ä½“çš„çœŸå®æ½œåŠ›ï¼Œç ”ç©¶è€…é€šè¿‡ä¸¥æ ¼çš„å‡€åŒ–æµæ°´çº¿(Purification Pipeline)å°†åŸºå‡†æ”¹è¿›ä¸ºAndroidControl-Curatedã€‚åœ¨è¿™ä¸€ä¼˜åŒ–åçš„åŸºå‡†ä¸Šï¼Œæœ€å…ˆè¿›æ¨¡å‹(SOTA models)åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æˆåŠŸç‡æå‡äº†15%ï¼Œè¾¾åˆ°è¿‘75%ï¼Œè¯æ˜ç«¯ä¾§è™šæ‹ŸåŠ©æ‰‹(On-device virtual assistants)å·²æ¯”ä»¥å¾€è®¤ä¸ºçš„æ›´æ¥è¿‘å®é™…éƒ¨ç½²æ°´å¹³ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§è½»é‡åŒ–æ¨¡å‹Magma-R1-3Bï¼Œä»…éœ€2.4kä¸ªç²¾é€‰æ ·æœ¬å’Œçº¦60å°æ—¶çš„H20 GPUè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å‚æ•°è§„æ¨¡ç¼©å°200å€çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä»å¯ä¸Qwen3-VL-235Bç­‰å¤§å‹æ¨¡å‹ç›¸åª²ç¾ã€‚è¯¥ç ”ç©¶é€šè¿‡å‘å¸ƒæ–°çš„åŸºå‡†å’Œæ¨¡å‹ï¼Œä¸ºå‡†ç¡®è¯„ä¼°æ¨¡å‹èƒ½åŠ›å¹¶æ¨åŠ¨é²æ£’çš„æ™ºèƒ½ä½“å¼€å‘æä¾›äº†é‡è¦è´¡çŒ®ã€‚",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18488v1",
      "published_date": "2025-10-21 10:11:33 UTC",
      "updated_date": "2025-10-21 10:11:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:48:32.553750+00:00"
    },
    {
      "arxiv_id": "2510.21811v1",
      "title": "Comparative Analysis of Object Detection Algorithms for Surface Defect Detection",
      "title_zh": "è¡¨é¢ç¼ºé™·æ£€æµ‹çš„ç›®æ ‡æ£€æµ‹ç®—æ³•å¯¹æ¯”åˆ†æ",
      "authors": [
        "Arpan Maity",
        "Tamal Ghosh"
      ],
      "abstract": "This article compares the performance of six prominent object detection algorithms, YOLOv11, RetinaNet, Fast R-CNN, YOLOv8, RT-DETR, and DETR, on the NEU-DET surface defect detection dataset, comprising images representing various metal surface defects, a crucial application in industrial quality control. Each model's performance was assessed regarding detection accuracy, speed, and robustness across different defect types such as scratches, inclusions, and rolled-in scales. YOLOv11, a state-of-the-art real-time object detection algorithm, demonstrated superior performance compared to the other methods, achieving a remarkable 70% higher accuracy on average. This improvement can be attributed to YOLOv11s enhanced feature extraction capabilities and ability to process the entire image in a single forward pass, making it faster and more efficient in detecting minor surface defects. Additionally, YOLOv11's architecture optimizations, such as improved anchor box generation and deeper convolutional layers, contributed to more precise localization of defects. In conclusion, YOLOv11's outstanding performance in accuracy and speed solidifies its position as the most effective model for surface defect detection on the NEU dataset, surpassing competing algorithms by a substantial margin.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹æ¯”äº†å…­ç§ä¸»æµç›®æ ‡æ£€æµ‹ç®—æ³•ï¼ˆYOLOv11, RetinaNet, Fast R-CNN, YOLOv8, RT-DETR å’Œ DETRï¼‰åœ¨ NEU-DET é‡‘å±è¡¨é¢ç¼ºé™·æ£€æµ‹æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚è¯„ä¼°è¿‡ç¨‹é‡ç‚¹å…³æ³¨äº†ä¸åŒç¼ºé™·ç±»å‹ï¼ˆå¦‚ scratches, inclusions å’Œ rolled-in scalesï¼‰ä¸‹çš„æ£€æµ‹å‡†ç¡®ç‡ã€é€Ÿåº¦ä»¥åŠé²æ£’æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒYOLOv11 ä½œä¸ºä¸€ç§å…ˆè¿›çš„å®æ—¶ç›®æ ‡æ£€æµ‹ç®—æ³•ï¼Œå…¶å¹³å‡å‡†ç¡®ç‡æ¯”å…¶ä»–æ–¹æ³•é«˜å‡º 70%ï¼Œå±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚è¿™ä¸»è¦å½’åŠŸäº YOLOv11 å¢å¼ºçš„ç‰¹å¾æå–èƒ½åŠ›ä»¥åŠå•æ¬¡å‰å‘ä¼ é€’å¤„ç†æ•´å¹…å›¾åƒçš„æœºåˆ¶ï¼Œä½¿å…¶åœ¨æ£€æµ‹å¾®å°è¡¨é¢ç¼ºé™·æ—¶æ›´å¿«é€Ÿä¸”é«˜æ•ˆã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹é€šè¿‡æ”¹è¿› anchor box ç”Ÿæˆå’ŒåŠ æ·±å·ç§¯å±‚ç­‰æ¶æ„ä¼˜åŒ–ï¼Œå®ç°äº†æ›´ç²¾ç¡®çš„ç¼ºé™·å®šä½ã€‚ç»¼ä¸Šæ‰€è¿°ï¼ŒYOLOv11 åœ¨å‡†ç¡®æ€§å’Œé€Ÿåº¦ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ä½¿å…¶æˆä¸º NEU æ•°æ®é›†ä¸Šæœ€æœ‰æ•ˆçš„è¡¨é¢ç¼ºé™·æ£€æµ‹æ¨¡å‹ï¼Œå¤§å¹…è¶…è¶Šäº†åŒç±»ç«äº‰ç®—æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.21811v1",
      "published_date": "2025-10-21 10:05:02 UTC",
      "updated_date": "2025-10-21 10:05:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:48:44.208430+00:00"
    },
    {
      "arxiv_id": "2510.18483v1",
      "title": "StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making and Information Seeking",
      "title_zh": "StarBenchï¼šé¢å‘æ™ºèƒ½ä½“å¤šæ¨¡æ€å†³ç­–ä¸ä¿¡æ¯å¯»æ±‚çš„å›åˆåˆ¶ RPG è¯„æµ‹åŸºå‡†",
      "authors": [
        "Haoran Zhang",
        "Chenhao Zhu",
        "Sicong Guo",
        "Hanzhe Guo",
        "Haiming Li",
        "Donglin Yu"
      ],
      "abstract": "Human players do more than press buttons: they ground what they see on screen into precise keyboard-mouse actions and, when stuck, they seek information before trying again. We ask whether current vision-language models (VLMs) can do the same. Despite encouraging results under simplified control or tool scaffolds, human-like play in a real client - mapping raw screenshots to temporally coherent low-level actions while deciding when to ask for guidance - remains an open challenge. We introduce StarBench, a turn-based RPG benchmark derived from Honkai: Star Rail that targets these two human-like competencies: multimodal decision-making from pixels to actions and agentic information seeking. StarBench standardizes evaluation across eight combat tasks and two regimes with shared tasks and metrics: (i) direct control, where agents receive only screenshots and must emit low-level primitives (click and keypress) with no semantic hints; and (ii) tool-assisted control, where higher-level intents can be mapped to primitives by detectors and OCR outputs provide optional textualized observations to ease UI grounding. To mirror human practice, StarBench also includes an ask-or-act diagnostic that measures whether and when agents choose to request brief guidance before proceeding, and how that choice affects subsequent performance. We report reference baselines for contemporary VLMs and a human reference. Results expose sizable gaps in perception-to-control fidelity in the direct regime, while showing that judicious information seeking correlates with improved success, establishing StarBench as a reproducible yardstick for agentic information seeking and multimodal decision-making in real-client play.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† StarBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºã€Šå´©åï¼šæ˜Ÿç©¹é“é“ã€‹(Honkai: Star Rail) çš„å›åˆåˆ¶ RPG åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨å¤šæ¨¡æ€å†³ç­–å’Œä¸»åŠ¨ä¿¡æ¯å¯»æ±‚æ–¹é¢çš„èƒ½åŠ›ã€‚StarBench æ ‡å‡†åŒ–äº†å…«é¡¹æˆ˜æ–—ä»»åŠ¡çš„è¯„ä¼°ï¼Œå¹¶æ¶µç›–äº†ç›´æ¥æ§åˆ¶ (direct control) å’Œå·¥å…·è¾…åŠ©æ§åˆ¶ (tool-assisted control) ä¸¤ç§æ¨¡å¼ï¼Œè¦æ±‚æ™ºèƒ½ä½“å°†åŸå§‹æˆªå›¾æ˜ å°„ä¸ºåº•å±‚çš„é”®ç›˜é¼ æ ‡åŠ¨ä½œã€‚ä¸ºäº†æ¨¡æ‹Ÿäººç±»ç©å®¶çš„è¡Œä¸ºï¼Œè¯¥åŸºå‡†è¿˜åŒ…å«äº†ä¸€é¡¹â€œæé—®æˆ–è¡ŒåŠ¨â€ (ask-or-act) è¯Šæ–­æµ‹è¯•ï¼Œç”¨äºè¡¡é‡æ™ºèƒ½ä½“åœ¨è¿›å±•å—é˜»æ—¶æ˜¯å¦ä»¥åŠä½•æ—¶è¯·æ±‚å¤–éƒ¨å¼•å¯¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“ä»£ VLMs åœ¨ç›´æ¥æ§åˆ¶æ¨¡å¼ä¸‹çš„æ„ŸçŸ¥åˆ°æ§åˆ¶ (perception-to-control) å¿ å®åº¦æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚åŒæ—¶ï¼Œç ”ç©¶è¯æ˜äº†æ˜æ™ºçš„ä¿¡æ¯å¯»æ±‚è¡Œä¸ºä¸æ›´é«˜çš„ä»»åŠ¡æˆåŠŸç‡å‘ˆæ­£ç›¸å…³ï¼Œä¸ºçœŸå®æ¸¸æˆå®¢æˆ·ç«¯ç¯å¢ƒä¸‹çš„å¤šæ¨¡æ€å†³ç­–æä¾›äº†å¯é‡å¤çš„è¡¡é‡å°ºåº¦ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18483v1",
      "published_date": "2025-10-21 10:02:59 UTC",
      "updated_date": "2025-10-21 10:02:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:48:48.554415+00:00"
    },
    {
      "arxiv_id": "2510.18477v2",
      "title": "LAFA: Agentic LLM-Driven Federated Analytics over Decentralized Data Sources",
      "title_zh": "LAFAï¼šé¢å‘å»ä¸­å¿ƒåŒ–æ•°æ®æºçš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“é©±åŠ¨è”é‚¦åˆ†æ",
      "authors": [
        "Haichao Ji",
        "Zibo Wang",
        "Cheng Pan",
        "Meng Han",
        "Yifei Zhu",
        "Dan Wang",
        "Zhu Han"
      ],
      "abstract": "Large Language Models (LLMs) have shown great promise in automating data analytics tasks by interpreting natural language queries and generating multi-operation execution plans. However, existing LLM-agent-based analytics frameworks operate under the assumption of centralized data access, offering little to no privacy protection. In contrast, federated analytics (FA) enables privacy-preserving computation across distributed data sources, but lacks support for natural language input and requires structured, machine-readable queries. In this work, we present LAFA, the first system that integrates LLM-agent-based data analytics with FA. LAFA introduces a hierarchical multi-agent architecture that accepts natural language queries and transforms them into optimized, executable FA workflows. A coarse-grained planner first decomposes complex queries into sub-queries, while a fine-grained planner maps each subquery into a Directed Acyclic Graph of FA operations using prior structural knowledge. To improve execution efficiency, an optimizer agent rewrites and merges multiple DAGs, eliminating redundant operations and minimizing computational and communicational overhead. Our experiments demonstrate that LAFA consistently outperforms baseline prompting strategies by achieving higher execution plan success rates and reducing resource-intensive FA operations by a substantial margin. This work establishes a practical foundation for privacy-preserving, LLM-driven analytics that supports natural language input in the FA setting.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LAFAï¼Œè¿™æ˜¯é¦–ä¸ªå°†å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ï¼ˆLLM-agentï¼‰ä¸è”é‚¦åˆ†æï¼ˆFederated Analytics, FAï¼‰ç›¸ç»“åˆçš„ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åˆ†ææ¡†æ¶åœ¨åˆ†æ•£æ•°æ®æºä¸Šçš„éšç§ä¿æŠ¤ç¼ºå£åŠFAç³»ç»Ÿç¼ºä¹è‡ªç„¶è¯­è¨€æ”¯æŒçš„é—®é¢˜ã€‚LAFAé‡‡ç”¨äº†ä¸€ç§å±‚æ¬¡åŒ–å¤šæ™ºèƒ½ä½“æ¶æ„ï¼ˆHierarchical Multi-agent Architectureï¼‰ï¼Œé€šè¿‡ç²—ç²’åº¦è§„åˆ’å™¨ï¼ˆCoarse-grained Plannerï¼‰å°†å¤æ‚æŸ¥è¯¢åˆ†è§£ï¼Œå¹¶ç”±ç»†ç²’åº¦è§„åˆ’å™¨ï¼ˆFine-grained Plannerï¼‰åˆ©ç”¨å…ˆéªŒç»“æ„çŸ¥è¯†å°†å­æŸ¥è¯¢æ˜ å°„ä¸ºFAæ“ä½œçš„æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ã€‚ç³»ç»Ÿå¼•å…¥çš„ä¼˜åŒ–å™¨æ™ºèƒ½ä½“ï¼ˆOptimizer Agentï¼‰èƒ½å¤Ÿé‡å†™å¹¶åˆå¹¶å¤šä¸ªDAGï¼Œä»è€Œæ¶ˆé™¤å†—ä½™æ“ä½œå¹¶æœ€å°åŒ–è®¡ç®—ä¸é€šä¿¡å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLAFAåœ¨æ‰§è¡Œè®¡åˆ’æˆåŠŸç‡ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æç¤ºç­–ç•¥ï¼Œå¹¶å¤§å¹…å‡å°‘äº†èµ„æºå¯†é›†å‹FAæ“ä½œçš„ä½¿ç”¨ã€‚è¯¥å·¥ä½œä¸ºåœ¨è”é‚¦åˆ†æç¯å¢ƒä¸‹å®ç°æ”¯æŒè‡ªç„¶è¯­è¨€è¾“å…¥ä¸”å…¼é¡¾éšç§ä¿æŠ¤çš„è‡ªåŠ¨åŒ–æ•°æ®åˆ†æå¥ å®šäº†å®è·µåŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.DC",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been accepted by the 16th IEEE International Conference on Cloud Computing Technology and Science (CloudCom 2025)",
      "pdf_url": "https://arxiv.org/pdf/2510.18477v2",
      "published_date": "2025-10-21 09:56:25 UTC",
      "updated_date": "2025-10-30 04:49:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:48:49.159024+00:00"
    },
    {
      "arxiv_id": "2510.18476v1",
      "title": "Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents",
      "title_zh": "ç¤¾äº¤æ™ºèƒ½ LLM æ™ºèƒ½ä½“ä¸­çš„æ„å›¾æ¦‚ç‡å»ºæ¨¡",
      "authors": [
        "Feifan Xia",
        "Yuyang Fang",
        "Defang Li",
        "Yantong Xie",
        "Weikang Li",
        "Yang Li",
        "Deguo Xia",
        "Jizhou Huang"
      ],
      "abstract": "We present a probabilistic intent modeling framework for large language model (LLM) agents in multi-turn social dialogue. The framework maintains a belief distribution over a partner's latent intentions, initialized from contextual priors and dynamically updated through likelihood estimation after each utterance. The evolving distribution provides additional contextual grounding for the policy, enabling adaptive dialogue strategies under uncertainty. Preliminary experiments in the SOTOPIA environment show consistent improvements: the proposed framework increases the Overall score by 9.0% on SOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and slightly surpasses an oracle agent that directly observes partner intentions. These early results suggest that probabilistic intent modeling can contribute to the development of socially intelligent LLM agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šè½®ç¤¾äº¤å¯¹è¯ä¸­çš„å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“ï¼Œæå‡ºäº†ä¸€ä¸ªæ¦‚ç‡æ„å›¾å»ºæ¨¡æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»´æŠ¤åˆä½œä¼™ä¼´æ½œåœ¨æ„å›¾çš„ç½®ä¿¡åˆ†å¸ƒ(belief distribution)ï¼Œå¹¶åˆ©ç”¨ä¼¼ç„¶ä¼°è®¡(likelihood estimation)åœ¨æ¯è½®å¯¹è¯ååŠ¨æ€æ›´æ–°ï¼Œä¸ºæ™ºèƒ½ä½“å†³ç­–æä¾›æ›´ç²¾ç¡®çš„ä¸Šä¸‹æ–‡åŸºå‡†ã€‚è¿™ç§å»ºæ¨¡æ–¹å¼ä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å­˜åœ¨ä¸ç¡®å®šæ€§çš„ç¤¾äº¤äº’åŠ¨ä¸­é‡‡å–è‡ªé€‚åº”å¯¹è¯ç­–ç•¥ã€‚åœ¨SOTOPIAç¯å¢ƒä¸‹çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨SOTOPIA-Allå’ŒSOTOPIA-HardæŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”Qwen2.5-7BåŸºçº¿æé«˜äº†9.0%å’Œ4.1%ã€‚æ­¤å¤–ï¼Œå…¶æ€§èƒ½ç”šè‡³ç•¥å¾®è¶…è¿‡äº†èƒ½ç›´æ¥è·çŸ¥æ„å›¾çš„oracle agentï¼Œè¯æ˜äº†æ¦‚ç‡æ„å›¾å»ºæ¨¡åœ¨æå‡LLMæ™ºèƒ½ä½“ç¤¾äº¤æ™ºèƒ½æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18476v1",
      "published_date": "2025-10-21 09:54:44 UTC",
      "updated_date": "2025-10-21 09:54:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:49:00.557755+00:00"
    },
    {
      "arxiv_id": "2510.18473v1",
      "title": "Benchmarking Fairness-aware Graph Neural Networks in Knowledge Graphs",
      "title_zh": "çŸ¥è¯†å›¾è°±ä¸­çš„å…¬å¹³æ„ŸçŸ¥å›¾ç¥ç»ç½‘ç»œåŸºå‡†æµ‹è¯•",
      "authors": [
        "Yuya Sasaki"
      ],
      "abstract": "Graph neural networks (GNNs) are powerful tools for learning from graph-structured data but often produce biased predictions with respect to sensitive attributes. Fairness-aware GNNs have been actively studied for mitigating biased predictions. However, no prior studies have evaluated fairness-aware GNNs on knowledge graphs, which are one of the most important graphs in many applications, such as recommender systems. Therefore, we introduce a benchmarking study on knowledge graphs. We generate new graphs from three knowledge graphs, YAGO, DBpedia, and Wikidata, that are significantly larger than the existing graph datasets used in fairness studies. We benchmark inprocessing and preprocessing methods in different GNN backbones and early stopping conditions. We find several key insights: (i) knowledge graphs show different trends from existing datasets; clearer trade-offs between prediction accuracy and fairness metrics than other graphs in fairness-aware GNNs, (ii) the performance is largely affected by not only fairness-aware GNN methods but also GNN backbones and early stopping conditions, and (iii) preprocessing methods often improve fairness metrics, while inprocessing methods improve prediction accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çŸ¥è¯†å›¾è°±(Knowledge Graphs)é¢†åŸŸå…¬å¹³æ€§ç ”ç©¶çš„ç©ºç™½ï¼Œé¦–æ¬¡å¯¹å…¬å¹³æ„ŸçŸ¥å›¾ç¥ç»ç½‘ç»œ(Fairness-aware GNNs)è¿›è¡Œäº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶è€…é€šè¿‡ä»YAGOã€DBpediaå’ŒWikidataç”Ÿæˆå¤§è§„æ¨¡æ–°æ•°æ®é›†ï¼Œç³»ç»Ÿå¯¹æ¯”äº†é¢„å¤„ç†(Preprocessing)å’Œè¿›ç¨‹å†…(Inprocessing)æ–¹æ³•åœ¨å¤šç§GNN backbonesåŠæ—©åœæ¡ä»¶(Early stopping conditions)ä¸‹çš„æ€§èƒ½ã€‚å®éªŒå‘ç°çŸ¥è¯†å›¾è°±åœ¨å…¬å¹³æ„ŸçŸ¥GNNsä¸­å±•ç°å‡ºæ¯”ä¼ ç»Ÿæ•°æ®é›†æ›´æ˜¾è‘—çš„é¢„æµ‹å‡†ç¡®ç‡ä¸å…¬å¹³æ€§æƒè¡¡å…³ç³»ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯æ˜æ¨¡å‹æ€§èƒ½æ·±å—GNN backboneså’Œæ—©åœæ¡ä»¶çš„å½±å“ï¼Œå¹¶æŒ‡å‡ºPreprocessingæ–¹æ³•æ›´èƒ½æœ‰æ•ˆæ”¹å–„å…¬å¹³æ€§æŒ‡æ ‡ï¼Œè€ŒInprocessingæ–¹æ³•åˆ™åœ¨æå‡é¢„æµ‹å‡†ç¡®ç‡æ–¹é¢æ›´å…·ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18473v1",
      "published_date": "2025-10-21 09:51:42 UTC",
      "updated_date": "2025-10-21 09:51:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:49:00.158801+00:00"
    },
    {
      "arxiv_id": "2510.21810v1",
      "title": "Hybrid Deep Learning Framework for Enhanced Diabetic Retinopathy Detection: Integrating Traditional Features with AI-driven Insights",
      "title_zh": "å¼ºåŒ–ç³–å°¿ç—…è§†ç½‘è†œç—…å˜æ£€æµ‹çš„æ··åˆæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼šä¼ ç»Ÿç‰¹å¾ä¸ AI é©±åŠ¨è§è§£çš„æ•´åˆ",
      "authors": [
        "Arpan Maity",
        "Aviroop Pal",
        "MD. Samiul Islam",
        "Tamal Ghosh"
      ],
      "abstract": "Diabetic Retinopathy (DR), a vision-threatening complication of Dia-betes Mellitus (DM), is a major global concern, particularly in India, which has one of the highest diabetic populations. Prolonged hyperglycemia damages reti-nal microvasculature, leading to DR symptoms like microaneurysms, hemor-rhages, and fluid leakage, which, if undetected, cause irreversible vision loss. Therefore, early screening is crucial as DR is asymptomatic in its initial stages. Fundus imaging aids precise diagnosis by detecting subtle retinal lesions. This paper introduces a hybrid diagnostic framework combining traditional feature extraction and deep learning (DL) to enhance DR detection. While handcrafted features capture key clinical markers, DL automates hierarchical pattern recog-nition, improving early diagnosis. The model synergizes interpretable clinical data with learned features, surpassing standalone DL approaches that demon-strate superior classification and reduce false negatives. This multimodal AI-driven approach enables scalable, accurate DR screening, crucial for diabetes-burdened regions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç³–å°¿ç—…è§†ç½‘è†œç—…å˜ï¼ˆDiabetic Retinopathyï¼‰æ—©æœŸè¯Šæ–­çš„ç´§è¿«æ€§ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆä¼ ç»Ÿç‰¹å¾æå–ä¸æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰çš„æ··åˆè¯Šæ–­æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨çœ¼åº•å›¾åƒï¼ˆFundus imagingï¼‰è¿›è¡Œæ£€æµ‹ï¼Œé€šè¿‡æ‰‹å·¥ç‰¹å¾ï¼ˆhandcrafted featuresï¼‰æ•æ‰å…³é”®ä¸´åºŠæ ‡å¿—ç‰©ï¼Œå¹¶ç»“åˆæ·±åº¦å­¦ä¹ çš„è‡ªåŠ¨åŒ–å±‚æ¬¡åŒ–æ¨¡å¼è¯†åˆ«èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§å°†å¯è§£é‡Šçš„ä¸´åºŠæ•°æ®ä¸å­¦ä¹ ç‰¹å¾ç›¸ç»“åˆçš„æ–¹æ³•åœ¨åˆ†ç±»æ€§èƒ½ä¸Šä¼˜äºå•ä¸€çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå¹¶èƒ½æ˜¾è‘—å‡å°‘æ¼è¯Šï¼ˆfalse negativesï¼‰ã€‚è¿™ç§å¤šæ¨¡æ€AIé©±åŠ¨çš„æ–¹æ¡ˆä¸ºç³–å°¿ç—…é«˜å‘åœ°åŒºæä¾›äº†ä¸€ç§é«˜ç²¾åº¦ä¸”å…·å¤‡æ‰©å±•æ€§çš„ç­›æŸ¥æ‰‹æ®µï¼Œæœ‰åŠ©äºé˜²æ­¢å› ç—…å˜å¯¼è‡´çš„ä¸å¯é€†è§†åŠ›æŸä¼¤ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.21810v1",
      "published_date": "2025-10-21 09:50:16 UTC",
      "updated_date": "2025-10-21 09:50:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:48:59.362733+00:00"
    },
    {
      "arxiv_id": "2510.18471v1",
      "title": "CodeRL+: Improving Code Generation via Reinforcement with Execution Semantics Alignment",
      "title_zh": "CodeRL+ï¼šé€šè¿‡æ‰§è¡Œè¯­ä¹‰å¯¹é½å¼ºåŒ–å­¦ä¹ æå‡ä»£ç ç”Ÿæˆèƒ½åŠ›",
      "authors": [
        "Xue Jiang",
        "Yihong Dong",
        "Mengyang Liu",
        "Hongyi Deng",
        "Tian Wang",
        "Yongding Tao",
        "Rongyu Cao",
        "Binhua Li",
        "Zhi Jin",
        "Wenpin Jiao",
        "Fei Huang",
        "Yongbin Li",
        "Ge Li"
      ],
      "abstract": "While Large Language Models (LLMs) excel at code generation by learning from vast code corpora, a fundamental semantic gap remains between their training on textual patterns and the goal of functional correctness, which is governed by formal execution semantics. Reinforcement Learning with Verifiable Rewards (RLVR) approaches attempt to bridge this gap using outcome rewards from executing test cases. However, solely relying on binary pass/fail signals is inefficient for establishing a well-aligned connection between the textual representation of code and its execution semantics, especially for subtle logical errors within the code. In this paper, we propose CodeRL+, a novel approach that integrates execution semantics alignment into the RLVR training pipeline for code generation. CodeRL+ enables the model to infer variable-level execution trajectory, providing a direct learning signal of execution semantics. CodeRL+ can construct execution semantics alignment directly using existing on-policy rollouts and integrates seamlessly with various RL algorithms. Extensive experiments demonstrate that CodeRL+ outperforms post-training baselines (including RLVR and Distillation), achieving a 4.6% average relative improvement in pass@1. CodeRL+ generalizes effectively to other coding tasks, yielding 15.5% and 4.4% higher accuracy on code-reasoning and test-output-generation benchmarks, respectively. CodeRL+ shows strong applicability across diverse RL algorithms and LLMs. Furthermore, probe analyses provide compelling evidence that CodeRL+ strengthens the alignment between code's textual representations and its underlying execution semantics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä»£ç ç”Ÿæˆä¸­å­˜åœ¨çš„æ–‡æœ¬æ¨¡å¼ä¸æ‰§è¡Œè¯­ä¹‰(execution semantics)è„±èŠ‚çš„é—®é¢˜ï¼Œæå‡ºäº†CodeRL+æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡ä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚ä¸åŒäºä»…ä¾èµ–äºŒå…ƒæµ‹è¯•é€šè¿‡ä¿¡å·çš„ä¼ ç»Ÿæ–¹æ³•ï¼ŒCodeRL+é€šè¿‡æ¨æ–­å˜é‡çº§åˆ«çš„æ‰§è¡Œè½¨è¿¹(variable-level execution trajectory)æ¥æ„å»ºæ‰§è¡Œè¯­ä¹‰å¯¹é½(execution semantics alignment)ï¼Œä¸ºæ¨¡å‹æä¾›æ›´ç›´æ¥çš„å­¦ä¹ ä¿¡å·ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•å’Œåœ¨çº¿é‡‡æ ·(on-policy rollouts)æµç¨‹ä¸­ï¼Œå±•ç°å‡ºæå¼ºçš„é€šç”¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCodeRL+åœ¨pass@1æŒ‡æ ‡ä¸Šå®ç°äº†4.6%çš„å¹³å‡ç›¸å¯¹æå‡ï¼Œå¹¶åœ¨ä»£ç æ¨ç†å’Œæµ‹è¯•è¾“å‡ºç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ã€‚æ¢é’ˆåˆ†æè¿›ä¸€æ­¥è¯å®ï¼ŒCodeRL+æœ‰æ•ˆåŠ å¼ºäº†ä»£ç æ–‡æœ¬è¡¨ç¤ºä¸å…¶åº•å±‚æ‰§è¡Œè¯­ä¹‰ä¹‹é—´çš„å…³è”ï¼Œä¸ºè§£å†³ä»£ç ä¸­çš„ç»†å¾®é€»è¾‘é”™è¯¯æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18471v1",
      "published_date": "2025-10-21 09:48:06 UTC",
      "updated_date": "2025-10-21 09:48:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:49:02.654284+00:00"
    },
    {
      "arxiv_id": "2510.18470v2",
      "title": "CircuitSeer: Mining High-Quality Data by Probing Mathematical Reasoning Circuits in LLMs",
      "title_zh": "CircuitSeerï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹æ•°å­¦æ¨ç†ç”µè·¯æ¢æµ‹çš„é«˜è´¨é‡æ•°æ®æŒ–æ˜",
      "authors": [
        "Shaobo Wang",
        "Yongliang Miao",
        "Yuancheng Liu",
        "Qianli Ma",
        "Ning Liao",
        "Linfeng Zhang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated impressive reasoning capabilities, but scaling their performance often relies on massive reasoning datasets that are computationally expensive to train on. Existing data selection methods aim to curate smaller, high-quality subsets but often rely on costly external models or opaque heuristics. In this work, we shift the focus from external heuristics to the model's internal mechanisms. We find that complex reasoning tasks consistently activate a sparse, specialized subset of attention heads, forming core reasoning circuits. Building on this insight, we propose CircuitSeer, a novel data selection method that quantifies the reasoning complexity of data by measuring its influence on these crucial circuits. Extensive experiments on 4 models and 9 datasets demonstrate CircuitSeer's superiority. Notably, fine-tuning Qwen2.5-Math-7B on just 10% of data selected by our method achieves a 1.4-point gain in average Pass@1 over training on the full dataset, highlighting its efficiency and effectiveness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¨ç†æ€§èƒ½æå‡é«˜åº¦ä¾èµ–æµ·é‡æ•°æ®é›†çš„é—®é¢˜ï¼Œæå‡ºäº†CircuitSeerï¼Œä¸€ç§é€šè¿‡æ¢æµ‹æ¨¡å‹å†…éƒ¨æœºåˆ¶æ¥æŒ–æ˜é«˜è´¨é‡æ¨ç†æ•°æ®çš„æ–°æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œå¤æ‚æ¨ç†ä»»åŠ¡ä¼šä¸€è‡´åœ°æ¿€æ´»æ¨¡å‹ä¸­ç‰¹å®šçš„ç¨€ç–æ³¨æ„åŠ›å¤´å­é›†ï¼Œå³æ ¸å¿ƒæ¨ç†ç”µè·¯(core reasoning circuits)ã€‚CircuitSeeré€šè¿‡è¡¡é‡æ•°æ®å¯¹è¿™äº›å…³é”®ç”µè·¯çš„å½±å“æ¥é‡åŒ–å…¶æ¨ç†å¤æ‚åº¦ï¼Œä»è€Œæœ‰æ•ˆåœ°ç­›é€‰å‡ºé«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬ï¼Œé¿å…äº†å¯¹æ˜‚è´µå¤–éƒ¨æ¨¡å‹æˆ–ä¸é€æ˜å¯å‘å¼æ–¹æ³•çš„ä¾èµ–ã€‚åœ¨æ¶‰åŠ4ä¸ªæ¨¡å‹å’Œ9ä¸ªæ•°æ®é›†çš„å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•å±•ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚æ˜¾è‘—çš„æ˜¯ï¼Œä»…åˆ©ç”¨CircuitSeerç­›é€‰çš„10%æ•°æ®å¯¹Qwen2.5-Math-7Bè¿›è¡Œå¾®è°ƒï¼Œå…¶Pass@1å¹³å‡å¾—åˆ†æ¯”ä½¿ç”¨å…¨é‡æ•°æ®è®­ç»ƒè¿˜æé«˜äº†1.4åˆ†ã€‚è¿™ä¸€ç»“æœå……åˆ†è¯æ˜äº†é€šè¿‡æ¨¡å‹å†…éƒ¨æœºåˆ¶è¯†åˆ«å¹¶åˆ©ç”¨é«˜è´¨é‡æ•°æ®åœ¨æå‡è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.18470v2",
      "published_date": "2025-10-21 09:47:00 UTC",
      "updated_date": "2025-10-23 03:50:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:49:07.952179+00:00"
    },
    {
      "arxiv_id": "2510.18467v1",
      "title": "Simple and Efficient Heterogeneous Temporal Graph Neural Network",
      "title_zh": "ç®€å•é«˜æ•ˆçš„å¼‚æ„æ—¶åºå›¾ç¥ç»ç½‘ç»œ",
      "authors": [
        "Yili Wang",
        "Tairan Huang",
        "Changlong He",
        "Qiutong Li",
        "Jianliang Gao"
      ],
      "abstract": "Heterogeneous temporal graphs (HTGs) are ubiquitous data structures in the real world. Recently, to enhance representation learning on HTGs, numerous attention-based neural networks have been proposed. Despite these successes, existing methods rely on a decoupled temporal and spatial learning paradigm, which weakens interactions of spatio-temporal information and leads to a high model complexity. To bridge this gap, we propose a novel learning paradigm for HTGs called Simple and Efficient Heterogeneous Temporal Graph N}eural Network (SE-HTGNN). Specifically, we innovatively integrate temporal modeling into spatial learning via a novel dynamic attention mechanism, which retains attention information from historical graph snapshots to guide subsequent attention computation, thereby improving the overall discriminative representations learning of HTGs. Additionally, to comprehensively and adaptively understand HTGs, we leverage large language models to prompt SE-HTGNN, enabling the model to capture the implicit properties of node types as prior knowledge. Extensive experiments demonstrate that SE-HTGNN achieves up to 10x speed-up over the state-of-the-art and latest baseline while maintaining the best forecasting accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SE-HTGNN (Simple and Efficient Heterogeneous Temporal Graph Neural Network)ï¼Œæ—¨åœ¨è§£å†³å¼‚æ„æ—¶é—´å›¾ (Heterogeneous Temporal Graphs, HTGs) åœ¨è¡¨ç¤ºå­¦ä¹ ä¸­å› æ—¶ç©ºå»ºæ¨¡è§£è€¦å¯¼è‡´çš„ä¿¡æ¯äº¤äº’å¼±å’Œæ¨¡å‹å¤æ‚åº¦é«˜çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å¼•å…¥äº†ä¸€ç§åŠ¨æ€æ³¨æ„åŠ›æœºåˆ¶ (dynamic attention mechanism)ï¼Œå°†æ—¶é—´å»ºæ¨¡æ•´åˆè¿›ç©ºé—´å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ä¿ç•™å¹¶åˆ©ç”¨å†å²å›¾å¿«ç…§çš„æ³¨æ„åŠ›ä¿¡æ¯æ¥å¢å¼ºå½“å‰çš„è¡¨ç¤ºå­¦ä¹ ã€‚æ­¤å¤–ï¼ŒSE-HTGNN åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) æ•è·èŠ‚ç‚¹ç±»å‹çš„éšå«å±æ€§ä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œå®ç°äº†å¯¹å¼‚æ„å›¾ç»“æ„æ›´å…¨é¢ä¸”è‡ªé€‚åº”çš„ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSE-HTGNN åœ¨ä¿æŒæœ€ä½³é¢„æµ‹ç²¾åº¦çš„åŒæ—¶ï¼Œè¿è¡Œé€Ÿåº¦æ¯”å½“å‰æœ€å…ˆè¿›çš„åŸºå‡†æ¨¡å‹æå‡äº†é«˜è¾¾ 10 å€ï¼Œå®ç°äº†æ•ˆç‡ä¸æ€§èƒ½çš„åŒé‡çªç ´ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by Neurips 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.18467v1",
      "published_date": "2025-10-21 09:43:08 UTC",
      "updated_date": "2025-10-21 09:43:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:49:20.062327+00:00"
    },
    {
      "arxiv_id": "2510.18459v1",
      "title": "DeLoad: Demand-Driven Short-Video Preloading with Scalable Watch-Time Estimation",
      "title_zh": "DeLoadï¼šåŸºäºå¯æ‰©å±•è§‚çœ‹æ—¶é•¿ä¼°è®¡çš„éœ€æ±‚é©±åŠ¨çŸ­è§†é¢‘é¢„åŠ è½½",
      "authors": [
        "Tong Liu",
        "Zhiwei Fan",
        "Guanyan Peng",
        "Haodan Zhang",
        "Yucheng Zhang",
        "Zhen Wang",
        "Pengjin Xie",
        "Liang Liu"
      ],
      "abstract": "Short video streaming has become a dominant paradigm in digital media, characterized by rapid swiping interactions and diverse media content. A key technical challenge is designing an effective preloading strategy that dynamically selects and prioritizes download tasks from an evolving playlist, balancing Quality of Experience (QoE) and bandwidth efficiency under practical commercial constraints. However, real world analysis reveals critical limitations of existing approaches: (1) insufficient adaptation of download task sizes to dynamic conditions, and (2) watch time prediction models that are difficult to deploy reliably at scale. In this paper, we propose DeLoad, a novel preloading framework that addresses these issues by introducing dynamic task sizing and a practical, multi dimensional watch time estimation method. Additionally, a Deep Reinforcement Learning (DRL) enhanced agent is trained to optimize the download range decisions adaptively. Extensive evaluations conducted on an offline testing platform, leveraging massive real world network data, demonstrate that DeLoad achieves significant improvements in QoE metrics (34.4% to 87.4% gain). Furthermore, after deployment on a large scale commercial short video platform, DeLoad has increased overall user watch time by 0.09% while simultaneously reducing rebuffering events and 3.76% bandwidth consumption.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DeLoadï¼Œä¸€ç§é’ˆå¯¹çŸ­è§†é¢‘æµåª’ä½“çš„éœ€æ±‚é©±åŠ¨é¢„åŠ è½½(Preloading)æ¡†æ¶ï¼Œæ—¨åœ¨å¹³è¡¡ç”¨æˆ·ä½“éªŒ(QoE)ä¸å¸¦å®½æ•ˆç‡ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€ç¯å¢ƒä¸‹çš„ä»»åŠ¡å¤§å°é€‚é…ä¸è¶³ä»¥åŠå¤§è§„æ¨¡è§‚çœ‹æ—¶é•¿(Watch Time)ä¼°è®¡éš¾ä»¥éƒ¨ç½²ç­‰æŒ‘æˆ˜ï¼ŒDeLoadå¼•å…¥äº†åŠ¨æ€ä»»åŠ¡è°ƒæ•´å’Œå¤šç»´åº¦çš„é¢„æµ‹æ–¹æ³•ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep Reinforcement Learning, DRL)å¢å¼ºçš„æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿæ ¹æ®æ’­æ”¾åˆ—è¡¨çš„å˜åŒ–è‡ªé€‚åº”åœ°ä¼˜åŒ–ä¸‹è½½å†³ç­–ã€‚ç¦»çº¿è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒDeLoadåœ¨QoEæŒ‡æ ‡ä¸Šå®ç°äº†34.4%è‡³87.4%çš„æ˜¾è‘—æå‡ã€‚åœ¨å¤§è§„æ¨¡å•†ä¸šçŸ­è§†é¢‘å¹³å°çš„å®é™…éƒ¨ç½²ä¸­ï¼Œè¯¥ç³»ç»Ÿåœ¨é™ä½3.76%å¸¦å®½æ¶ˆè€—å¹¶å‡å°‘å¡é¡¿çš„åŒæ—¶ï¼ŒæˆåŠŸæå‡äº†0.09%çš„æ€»ç”¨æˆ·è§‚çœ‹æ—¶é•¿ã€‚è¿™äº›å‘ç°è¯æ˜äº†DeLoadåœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­å¤„ç†å¤æ‚äº¤äº’å’Œèµ„æºçº¦æŸçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.MM",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.MM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18459v1",
      "published_date": "2025-10-21 09:33:49 UTC",
      "updated_date": "2025-10-21 09:33:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:49:20.257542+00:00"
    },
    {
      "arxiv_id": "2510.20849v1",
      "title": "Cultural Alien Sampler: Open-ended art generation balancing originality and coherence",
      "title_zh": "Cultural Alien Samplerï¼šå¹³è¡¡åŸåˆ›æ€§ä¸è¿è´¯æ€§çš„å¼€æ”¾å¼è‰ºæœ¯ç”Ÿæˆ",
      "authors": [
        "Alejandro H. Artiles",
        "Hiromu Yakura",
        "Levin Brinkmann",
        "Mar Canet Sola",
        "Hassan Abu Alhaija",
        "Ignacio Serna",
        "Nasim Rahaman",
        "Bernhard SchÃ¶lkopf",
        "Iyad Rahwan"
      ],
      "abstract": "In open-ended domains like art, autonomous agents must generate ideas that are both original and internally coherent, yet current Large Language Models (LLMs) either default to familiar cultural patterns or sacrifice coherence when pushed toward novelty. We address this by introducing the Cultural Alien Sampler (CAS), a concept-selection method that explicitly separates compositional fit from cultural typicality. CAS uses two GPT-2 models fine-tuned on WikiArt concepts: a Concept Coherence Model that scores whether concepts plausibly co-occur within artworks, and a Cultural Context Model that estimates how typical those combinations are within individual artists' bodies of work. CAS targets combinations that are high in coherence and low in typicality, yielding ideas that maintain internal consistency while deviating from learned conventions and embedded cultural context. In a human evaluation (N = 100), our approach outperforms random selection and GPT-4o baselines and achieves performance comparable to human art students in both perceived originality and harmony. Additionally, a quantitative study shows that our method produces more diverse outputs and explores a broader conceptual space than its GPT-4o counterpart, demonstrating that artificial cultural alienness can unlock creative potential in autonomous agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼€æ”¾å¼è‰ºæœ¯ç”Ÿæˆä¸­ Large Language Models (LLMs) éš¾ä»¥å¹³è¡¡åŸåˆ›æ€§ä¸è¿è´¯æ€§çš„é—®é¢˜ï¼Œæå‡ºäº† Cultural Alien Sampler (CAS) æ¦‚å¿µé€‰æ‹©æ–¹æ³•ã€‚CAS é€šè¿‡æ˜ç¡®åˆ†ç¦»ç»„åˆé€‚é…åº¦(compositional fit)ä¸æ–‡åŒ–å…¸å‹æ€§(cultural typicality)ï¼Œåˆ©ç”¨ä¸¤ä¸ªåœ¨ WikiArt ä¸Šå¾®è°ƒçš„ GPT-2 æ¨¡å‹ï¼ˆConcept Coherence Model å’Œ Cultural Context Modelï¼‰æ¥è¯†åˆ«é«˜è¿è´¯æ€§ä¸”ä½å…¸å‹æ€§çš„æ¦‚å¿µç»„åˆã€‚è¯¥æ–¹æ³•ç”Ÿæˆçš„åˆ›æ„æ—¢èƒ½åç¦»æ—¢å®šæ–‡åŒ–æƒ¯ä¾‹ï¼Œåˆèƒ½ä¿æŒå†…éƒ¨é€»è¾‘çš„ä¸€è‡´æ€§ã€‚äººç±»è¯„ä¼°è¡¨æ˜ï¼ŒCAS åœ¨åŸåˆ›æ€§å’Œå’Œè°åº¦ä¸Šä¼˜äº GPT-4o åŸºå‡†ï¼Œä¸”è¡¨ç°ä¸è‰ºæœ¯ä¸“ä¸šå­¦ç”Ÿç›¸å½“ã€‚å®šé‡ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼ŒCAS èƒ½å¤Ÿç”Ÿæˆæ›´å¤šæ ·åŒ–çš„å†…å®¹å¹¶æ¢ç´¢æ›´å¹¿é˜”çš„æ¦‚å¿µç©ºé—´ï¼Œæœ‰æ•ˆé‡Šæ”¾äº†è‡ªä¸»æ™ºèƒ½ä½“çš„åˆ›ä½œæ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Proceedings of the 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Creative AI Track. 26 pages, 24 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.20849v1",
      "published_date": "2025-10-21 09:32:46 UTC",
      "updated_date": "2025-10-21 09:32:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:49:39.258009+00:00"
    },
    {
      "arxiv_id": "2510.18442v2",
      "title": "PlanU: Large Language Model Reasoning through Planning under Uncertainty",
      "title_zh": "PlanUï¼šåŸºäºä¸ç¡®å®šæ€§ä¸‹è§„åˆ’çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†",
      "authors": [
        "Ziwei Deng",
        "Mian Deng",
        "Chenjing Liang",
        "Zeming Gao",
        "Chennan Ma",
        "Chenxing Lin",
        "Haipeng Zhang",
        "Songzhu Mei",
        "Siqi Shen",
        "Cheng Wang"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly being explored across a range of reasoning tasks. However, LLMs sometimes struggle with reasoning tasks under uncertainty that are relatively easy for humans, such as planning actions in stochastic environments. The adoption of LLMs for reasoning is impeded by uncertainty challenges, such as LLM uncertainty and environmental uncertainty. LLM uncertainty arises from the stochastic sampling process inherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM uncertainty through multiple reasoning chains or search trees. However, these approaches overlook environmental uncertainty, which leads to poor performance in environments with stochastic state transitions. Some recent LDM approaches deal with uncertainty by forecasting the probability of unknown variables. However, they are not designed for multi-step reasoning tasks that require interaction with the environment. To address uncertainty in LLM decision-making, we introduce PlanU, an LLM-based planning method that captures uncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of each node in the MCTS as a quantile distribution, which uses a set of quantiles to represent the return distribution. To balance exploration and exploitation during tree search, PlanU introduces an Upper Confidence Bounds with Curiosity (UCC) score which estimates the uncertainty of MCTS nodes. Through extensive experiments, we demonstrate the effectiveness of PlanU in LLM-based reasoning tasks under uncertainty.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨éšæœºç¯å¢ƒä¸‹æ¨ç†æ—¶é¢ä¸´çš„LLM uncertaintyå’Œenvironmental uncertaintyæŒ‘æˆ˜ï¼Œæå‡ºäº†PlanUè§„åˆ’æ–¹æ³•ã€‚PlanUé€šè¿‡åœ¨è’™ç‰¹å¡æ´›æ ‘æœç´¢(Monte Carlo Tree Search, MCTS)æ¡†æ¶ä¸­èå…¥ä¸ç¡®å®šæ€§æ•æ‰ï¼Œå°†æ¯ä¸ªæœç´¢èŠ‚ç‚¹çš„æ”¶ç›Šå»ºæ¨¡ä¸ºåˆ†ä½æ•°åˆ†å¸ƒ(quantile distribution)ï¼Œåˆ©ç”¨ä¸€ç»„åˆ†ä½æ•°æ¥è¡¨å¾å¤æ‚çš„æ”¶ç›Šåˆ†å¸ƒæƒ…å†µã€‚ä¸ºäº†åœ¨æœç´¢è¿‡ç¨‹ä¸­æœ‰æ•ˆå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†å¸¦å¥½å¥‡å¿ƒçš„ç½®ä¿¡ä¸Šé™(Upper Confidence Bounds with Curiosity, UCC)è¯„åˆ†æ¥è¯„ä¼°èŠ‚ç‚¹çš„ä¸ç¡®å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPlanUèƒ½å¤Ÿæ˜¾è‘—æå‡LLMåœ¨éœ€è¦ä¸ç¯å¢ƒäº¤äº’çš„å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¸ºä¸ç¡®å®šæ€§ç¯å¢ƒä¸‹çš„æ™ºèƒ½å†³ç­–æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "38 pages, 19 figures, NeurIPS 2025 Accepted",
      "pdf_url": "https://arxiv.org/pdf/2510.18442v2",
      "published_date": "2025-10-21 09:17:50 UTC",
      "updated_date": "2025-11-05 02:40:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:49:35.264376+00:00"
    },
    {
      "arxiv_id": "2510.18433v1",
      "title": "ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization",
      "title_zh": "ImageGemï¼šé¢å‘ç”Ÿæˆå¼æ¨¡å‹ä¸ªæ€§åŒ–çš„çœŸå®åœºæ™¯ç”Ÿæˆå¼å›¾åƒäº¤äº’æ•°æ®é›†",
      "authors": [
        "Yuanhe Guo",
        "Linxi Xie",
        "Zhuoran Chen",
        "Kangrui Yu",
        "Ryan Po",
        "Guandao Yang",
        "Gordon Wetztein",
        "Hongyi Wen"
      ],
      "abstract": "We introduce ImageGem, a dataset for studying generative models that understand fine-grained individual preferences. We posit that a key challenge hindering the development of such a generative model is the lack of in-the-wild and fine-grained user preference annotations. Our dataset features real-world interaction data from 57K users, who collectively have built 242K customized LoRAs, written 3M text prompts, and created 5M generated images. With user preference annotations from our dataset, we were able to train better preference alignment models. In addition, leveraging individual user preference, we investigated the performance of retrieval models and a vision-language model on personalized image retrieval and generative model recommendation. Finally, we propose an end-to-end framework for editing customized diffusion models in a latent weight space to align with individual user preferences. Our results demonstrate that the ImageGem dataset enables, for the first time, a new paradigm for generative model personalization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ImageGemï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç ”ç©¶ç†è§£ç»†ç²’åº¦ä¸ªäººåå¥½çš„ç”Ÿæˆæ¨¡å‹çš„çœŸå®åœºæ™¯äº¤äº’æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ±‡é›†äº†æ¥è‡ª 57K åç”¨æˆ·çš„çœŸå®äº¤äº’æ•°æ®ï¼ŒåŒ…å« 242K ä¸ªå®šåˆ¶çš„ LoRAsã€3M æ¡ text prompts ä»¥åŠ 5M å¼ ç”Ÿæˆçš„å›¾åƒï¼Œå¡«è¡¥äº†ç”Ÿæˆæ¨¡å‹ä¸ªæ€§åŒ–é¢†åŸŸç¼ºä¹å¤§è§„æ¨¡ç»†ç²’åº¦ç”¨æˆ·åå¥½æ ‡æ³¨çš„ç©ºç™½ã€‚é€šè¿‡è¯¥æ•°æ®é›†ï¼Œç ”ç©¶è€…æˆåŠŸè®­ç»ƒäº†æ€§èƒ½æ›´ä¼˜çš„ preference alignment æ¨¡å‹ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†æ£€ç´¢æ¨¡å‹å’Œ vision-language model åœ¨ä¸ªæ€§åŒ–å›¾åƒæ£€ç´¢åŠç”Ÿæˆæ¨¡å‹æ¨èä¸­çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åœ¨ latent weight space ä¸­ç¼–è¾‘å®šåˆ¶ diffusion models çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œä»¥å®ç°ä¸ä¸ªä½“åå¥½çš„ç²¾ç¡®å¯¹é½ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒImageGem æ•°æ®é›†ä¸ºå®ç°ç”Ÿæˆæ¨¡å‹çš„ personalization æä¾›äº†å…¨æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18433v1",
      "published_date": "2025-10-21 09:08:01 UTC",
      "updated_date": "2025-10-21 09:08:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:49:43.058949+00:00"
    },
    {
      "arxiv_id": "2510.18431v2",
      "title": "ScaleNet: Scaling up Pretrained Neural Networks with Incremental Parameters",
      "title_zh": "ScaleNetï¼šåŸºäºå¢é‡å‚æ•°çš„é¢„è®­ç»ƒç¥ç»ç½‘ç»œè§„æ¨¡æ‰©å±•",
      "authors": [
        "Zhiwei Hao",
        "Jianyuan Guo",
        "Li Shen",
        "Kai Han",
        "Yehui Tang",
        "Han Hu",
        "Yunhe Wang"
      ],
      "abstract": "Recent advancements in vision transformers (ViTs) have demonstrated that larger models often achieve superior performance. However, training these models remains computationally intensive and costly. To address this challenge, we introduce ScaleNet, an efficient approach for scaling ViT models. Unlike conventional training from scratch, ScaleNet facilitates rapid model expansion with negligible increases in parameters, building on existing pretrained models. This offers a cost-effective solution for scaling up ViTs. Specifically, ScaleNet achieves model expansion by inserting additional layers into pretrained ViTs, utilizing layer-wise weight sharing to maintain parameters efficiency. Each added layer shares its parameter tensor with a corresponding layer from the pretrained model. To mitigate potential performance degradation due to shared weights, ScaleNet introduces a small set of adjustment parameters for each layer. These adjustment parameters are implemented through parallel adapter modules, ensuring that each instance of the shared parameter tensor remains distinct and optimized for its specific function. Experiments on the ImageNet-1K dataset demonstrate that ScaleNet enables efficient expansion of ViT models. With a 2$\\times$ depth-scaled DeiT-Base model, ScaleNet achieves a 7.42% accuracy improvement over training from scratch while requiring only one-third of the training epochs, highlighting its efficiency in scaling ViTs. Beyond image classification, our method shows significant potential for application in downstream vision areas, as evidenced by the validation in object detection task.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ScaleNetï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ Vision Transformers (ViTs) æ¨¡å‹æ‰©å±•çš„é«˜æ•ˆæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡æ¨¡å‹ä»å¤´å¼€å§‹è®­ç»ƒæ‰€é¢ä¸´çš„é«˜æ˜‚è®¡ç®—æˆæœ¬æŒ‘æˆ˜ã€‚ScaleNet é€šè¿‡åœ¨ç°æœ‰çš„é¢„è®­ç»ƒ ViT æ¨¡å‹ä¸­æ’å…¥é¢å¤–å±‚æ¥å®ç°æ¨¡å‹æ‰©å¼ ï¼Œå¹¶åˆ©ç”¨å±‚çº§ Weight Sharing æŠ€æœ¯æ¥ç»´æŒæé«˜çš„å‚æ•°æ•ˆç‡ã€‚ä¸ºäº†ç¼“è§£å…±äº«æƒé‡å¯èƒ½å¸¦æ¥çš„æ€§èƒ½ä¸‹é™ï¼Œè¯¥æ¡†æ¶ä¸ºæ¯å±‚å¼•å…¥äº†ä¸€ç»„ç”±å¹¶è¡Œ Adapter æ¨¡å—å®ç°çš„ç»†å¾®è°ƒæ•´å‚æ•°ï¼Œç¡®ä¿å…±äº«å¼ é‡åœ¨ä¸åŒå±‚çº§ä¸­èƒ½é’ˆå¯¹ç‰¹å®šåŠŸèƒ½è¿›è¡Œä¼˜åŒ–ã€‚åœ¨ ImageNet-1K æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ·±åº¦ç¿»å€çš„ DeiT-Base æ¨¡å‹é€šè¿‡ ScaleNet ä»…éœ€ä¸‰åˆ†ä¹‹ä¸€çš„è®­ç»ƒå‘¨æœŸï¼Œå³å¯æ¯”ä»å¤´è®­ç»ƒæå‡ 7.42% çš„å‡†ç¡®ç‡ã€‚é™¤äº†å›¾åƒåˆ†ç±»ä»»åŠ¡å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ Object Detection ç­‰ä¸‹æ¸¸è§†è§‰ä»»åŠ¡ä¸­ä¹Ÿå±•ç¤ºäº†æ˜¾è‘—çš„åº”ç”¨æ½œåŠ›ï¼Œä¸ºè§†è§‰å¤§æ¨¡å‹çš„é«˜æ•ˆæ‰©å±•æä¾›äº†ä¸€ç§ä½æˆæœ¬çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted to IEEE Transactions on Image Processing (TIP)",
      "pdf_url": "https://arxiv.org/pdf/2510.18431v2",
      "published_date": "2025-10-21 09:07:25 UTC",
      "updated_date": "2025-10-22 03:50:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:49:42.342351+00:00"
    },
    {
      "arxiv_id": "2510.18429v1",
      "title": "Optimistic Higher-Order Superposition",
      "title_zh": "ä¹è§‚å¼é«˜é˜¶å åŠ ",
      "authors": [
        "Alexander Bentkamp",
        "Jasmin Blanchette",
        "Matthias Hetzenberger",
        "Uwe Waldmann"
      ],
      "abstract": "The $Î»$-superposition calculus is a successful approach to proving higher-order formulas. However, some parts of the calculus are extremely explosive, notably due to the higher-order unifier enumeration and the functional extensionality axiom. In the present work, we introduce an \"optimistic\" version of $Î»$-superposition that addresses these two issues. Specifically, our new calculus delays explosive unification problems using constraints stored along with the clauses, and it applies functional extensionality in a more targeted way. The calculus is sound and refutationally complete with respect to a Henkin semantics. We have yet to implement it in a prover, but examples suggest that it will outperform, or at least usefully complement, the original $Î»$-superposition calculus.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯æ˜é«˜é˜¶å…¬å¼çš„ $\\lambda$-superposition æ¼”ç®—åœ¨å¤„ç†é«˜é˜¶ unifier enumeration å’Œ functional extensionality å…¬ç†æ—¶æ˜“äº§ç”Ÿæœç´¢ç©ºé—´çˆ†ç‚¸çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º Optimistic Higher-Order Superposition çš„æ”¹è¿›æ¼”ç®—ç³»ç»Ÿã€‚æ–°æ¼”ç®—é€šè¿‡åœ¨ clauses ä¸­å­˜å‚¨ constraints æ¥å»¶è¿Ÿå¤„ç†å…·æœ‰çˆ†ç‚¸æ€§çš„ unification é—®é¢˜ï¼Œå¹¶ä»¥æ›´å…·é’ˆå¯¹æ€§çš„æ–¹å¼åº”ç”¨ functional extensionalityã€‚è¯¥æ¼”ç®—ç³»ç»Ÿåœ¨ Henkin semantics ä¸‹è¢«è¯æ˜å…·æœ‰ sound å’Œ refutationally complete çš„ç‰¹æ€§ã€‚è™½ç„¶è¯¥æ–¹æ³•å°šæœªåœ¨å…·ä½“çš„ prover ä¸­æ­£å¼å®ç°ï¼Œä½†åˆæ­¥åˆ†æä¸ç¤ºä¾‹è¡¨æ˜å®ƒèƒ½æ˜¾è‘—è¶…è¶Šæˆ–æœ‰æ•ˆè¡¥å……åŸæœ‰çš„ $\\lambda$-superposition æ¼”ç®—ã€‚è¯¥æˆæœä¸ºè§£å†³é«˜é˜¶è‡ªåŠ¨å®šç†è¯æ˜ä¸­çš„æ•ˆç‡ç“¶é¢ˆæä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18429v1",
      "published_date": "2025-10-21 09:05:54 UTC",
      "updated_date": "2025-10-21 09:05:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:49:46.363668+00:00"
    },
    {
      "arxiv_id": "2510.21808v1",
      "title": "Semantic Relation-Enhanced CLIP Adapter for Domain Adaptive Zero-Shot Learning",
      "title_zh": "é¢å‘é¢†åŸŸè‡ªé€‚åº”é›¶æ ·æœ¬å­¦ä¹ çš„è¯­ä¹‰å…³ç³»å¢å¼º CLIP é€‚é…å™¨",
      "authors": [
        "Jiaao Yu",
        "Mingjie Han",
        "Jinkun Jiang",
        "Junyu Dong",
        "Tao Gong",
        "Man Lan"
      ],
      "abstract": "The high cost of data annotation has spurred research on training deep learning models in data-limited scenarios. Existing paradigms, however, fail to balance cross-domain transfer and cross-category generalization, giving rise to the demand for Domain-Adaptive Zero-Shot Learning (DAZSL). Although vision-language models (e.g., CLIP) have inherent advantages in the DAZSL field, current studies do not fully exploit their potential. Applying CLIP to DAZSL faces two core challenges: inefficient cross-category knowledge transfer due to the lack of semantic relation guidance, and degraded cross-modal alignment during target domain fine-tuning. To address these issues, we propose a Semantic Relation-Enhanced CLIP (SRE-CLIP) Adapter framework, integrating a Semantic Relation Structure Loss and a Cross-Modal Alignment Retention Strategy. As the first CLIP-based DAZSL method, SRE-CLIP achieves state-of-the-art performance on the I2AwA and I2WebV benchmarks, significantly outperforming existing approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é¢†åŸŸè‡ªé€‚åº”é›¶æ ·æœ¬å­¦ä¹  (Domain-Adaptive Zero-Shot Learning, DAZSL) ä¸­è·¨é¢†åŸŸè¿ç§»ä¸è·¨ç±»åˆ«æ³›åŒ–éš¾ä»¥å¹³è¡¡çš„é—®é¢˜ï¼Œæ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ CLIP åœ¨è¯¥é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œåœ¨ DAZSL ä¸­ç›´æ¥åº”ç”¨ CLIP é¢ä¸´è·¨ç±»åˆ«çŸ¥è¯†è¿ç§»æ•ˆç‡ä½ä»¥åŠç›®æ ‡åŸŸå¾®è°ƒè¿‡ç¨‹ä¸­è·¨æ¨¡æ€å¯¹é½é€€åŒ–çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† SRE-CLIP (Semantic Relation-Enhanced CLIP Adapter) æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†è¯­ä¹‰å…³ç³»ç»“æ„æŸå¤± (Semantic Relation Structure Loss) å’Œè·¨æ¨¡æ€å¯¹é½ä¿æŒç­–ç•¥ (Cross-Modal Alignment Retention Strategy)ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥è¯­ä¹‰å…³ç³»æŒ‡å¯¼æ¥ä¼˜åŒ–çŸ¥è¯†è¿ç§»ï¼Œå¹¶ç¡®ä¿åœ¨æ¨¡å‹é€‚é…è¿‡ç¨‹ä¸­ç»´æŒåŸæœ‰çš„å¯¹é½èƒ½åŠ›ã€‚ä½œä¸ºé¦–ä¸ªåŸºäº CLIP çš„ DAZSL æ–¹æ³•ï¼ŒSRE-CLIP åœ¨ I2AwA å’Œ I2WebV åŸºå‡†æµ‹è¯•ä¸Šå‡è¾¾åˆ°äº† state-of-the-art çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸ºå—é™æ•°æ®åœºæ™¯ä¸‹çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒæä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.21808v1",
      "published_date": "2025-10-21 09:03:30 UTC",
      "updated_date": "2025-10-21 09:03:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:49:46.558863+00:00"
    },
    {
      "arxiv_id": "2510.18428v2",
      "title": "AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library",
      "title_zh": "AlphaOPTï¼šåŸºäºè‡ªæˆ‘æ”¹è¿›å¤§è¯­è¨€æ¨¡å‹ç»éªŒåº“çš„ä¼˜åŒ–ç¨‹åºå»ºæ¨¡",
      "authors": [
        "Minwei Kong",
        "Ao Qu",
        "Xiaotong Guo",
        "Wenbin Ouyang",
        "Chonghe Jiang",
        "Han Zheng",
        "Yining Ma",
        "Dingyi Zhuang",
        "Yuhan Tang",
        "Junyi Li",
        "Shenhao Wang",
        "Haris Koutsopoulos",
        "Hai Wang",
        "Cathy Wu",
        "Jinhua Zhao"
      ],
      "abstract": "Optimization modeling enables critical decisions across industries but remains difficult to automate: informal language must be mapped to precise mathematical formulations and executable solver code. Prior LLM approaches either rely on brittle prompting or costly retraining with limited generalization. We present AlphaOPT, a self-improving experience library that enables an LLM to learn from limited demonstrations (even answers alone, without gold-standard programs) and solver feedback - without annotated reasoning traces or parameter updates. AlphaOPT operates in a continual two-phase cycle: (i) a Library Learning phase that reflects on failed attempts, extracting solver-verified, structured insights as {taxonomy, condition, explanation, example}; and (ii) a Library Evolution phase that diagnoses retrieval misalignments and refines the applicability conditions of stored insights, improving transfer across tasks. This design (1) learns efficiently from limited demonstrations without curated rationales, (2) expands continually without costly retraining by updating the library rather than model weights, and (3) makes knowledge explicit and interpretable for human inspection and intervention. Experiments show that AlphaOPT steadily improves with more data (65% to 72% from 100 to 300 training items) and surpasses the strongest baseline by 7.7% on the out-of-distribution OptiBench dataset when trained only on answers. Code and data are available at: https://github.com/Minw913/AlphaOPT.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AlphaOPTï¼Œä¸€ä¸ªé€šè¿‡è‡ªæˆ‘æ”¹è¿›ç»éªŒåº“å®ç°ä¼˜åŒ–ç¨‹åºå»ºæ¨¡çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨è‡ªåŠ¨åŒ–å»ºæ¨¡ä¸­å­˜åœ¨çš„æç¤ºè¯è„†å¼±å’Œé‡è®­ç»ƒæˆæœ¬é«˜æ˜‚ç­‰æŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨æŒç»­çš„åŒé˜¶æ®µå¾ªç¯æœºåˆ¶ï¼ŒåŒ…æ‹¬ä»å¤±è´¥å°è¯•ä¸­æå–æ±‚è§£å™¨éªŒè¯è§è§£çš„Library Learningé˜¶æ®µï¼Œä»¥åŠé€šè¿‡è¯Šæ–­æ£€ç´¢åå·®æ¥ä¼˜åŒ–çŸ¥è¯†é€‚ç”¨æ€§çš„Library Evolutioné˜¶æ®µã€‚AlphaOPTæ— éœ€æ ‡æ³¨æ¨ç†è½¨è¿¹æˆ–æ¨¡å‹å‚æ•°æ›´æ–°ï¼Œå³å¯ä»æœ‰é™çš„ç¤ºä¾‹ç”šè‡³ä»…æœ‰çš„ç­”æ¡ˆä¸­é«˜æ•ˆå­¦ä¹ ï¼Œå¹¶ä½¿çŸ¥è¯†åº“å…·å¤‡æ˜¾æ€§åŒ–å’Œå¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAlphaOPTçš„è¡¨ç°éšæ•°æ®é‡å¢åŠ ç¨³æ­¥æå‡ï¼Œåœ¨ä»…ä½¿ç”¨ç­”æ¡ˆè¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå…¶åœ¨åˆ†å¸ƒå¤–æ•°æ®é›†OptiBenchä¸Šçš„è¡¨ç°æ¯”æœ€å¼ºåŸºçº¿é«˜å‡º7.7%ï¼Œå±•ç¤ºäº†æå¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œå­¦ä¹ æ•ˆç‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18428v2",
      "published_date": "2025-10-21 09:03:26 UTC",
      "updated_date": "2025-12-11 03:59:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:49:52.359023+00:00"
    },
    {
      "arxiv_id": "2510.18425v1",
      "title": "Automated urban waterlogging assessment and early warning through a mixture of foundation models",
      "title_zh": "åŸºäºæ··åˆåŸºç¡€æ¨¡å‹çš„è‡ªåŠ¨åŒ–åŸå¸‚å†…æ¶è¯„ä¼°ä¸é¢„è­¦",
      "authors": [
        "Chenxu Zhang",
        "Fuxiang Huang",
        "Lei Zhang"
      ],
      "abstract": "With climate change intensifying, urban waterlogging poses an increasingly severe threat to global public safety and infrastructure. However, existing monitoring approaches rely heavily on manual reporting and fail to provide timely and comprehensive assessments. In this study, we present Urban Waterlogging Assessment (UWAssess), a foundation model-driven framework that automatically identifies waterlogged areas in surveillance images and generates structured assessment reports. To address the scarcity of labeled data, we design a semi-supervised fine-tuning strategy and a chain-of-thought (CoT) prompting strategy to unleash the potential of the foundation model for data-scarce downstream tasks. Evaluations on challenging visual benchmarks demonstrate substantial improvements in perception performance. GPT-based evaluations confirm the ability of UWAssess to generate reliable textual reports that accurately describe waterlogging extent, depth, risk and impact. This dual capability enables a shift of waterlogging monitoring from perception to generation, while the collaborative framework of multiple foundation models lays the groundwork for intelligent and scalable systems, supporting urban management, disaster response and climate resilience.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†UWAssessï¼Œä¸€ä¸ªç”±åŸºç¡€æ¨¡å‹(foundation models)é©±åŠ¨çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç›‘æ§å›¾åƒè‡ªåŠ¨è¯†åˆ«åŸå¸‚å†…æ¶åŒºåŸŸå¹¶ç”Ÿæˆç»“æ„åŒ–è¯„ä¼°æŠ¥å‘Šã€‚ä¸ºè§£å†³æ ‡æ³¨æ•°æ®åŒ®ä¹çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶è®¾è®¡äº†åŠç›‘ç£å¾®è°ƒç­–ç•¥å’Œé“¾å¼æ€ç»´(Chain-of-Thought)æç¤ºç­–ç•¥ï¼Œä»¥æ¿€å‘åŸºç¡€æ¨¡å‹åœ¨æ•°æ®ç¨€ç¼ºçš„ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨è§†è§‰æ„ŸçŸ¥æ€§èƒ½ä¸Šè¾ƒåŸºçº¿æœ‰æ˜¾è‘—æå‡ï¼Œä¸”é€šè¿‡GPTè¯„ä¼°éªŒè¯äº†å…¶ç”ŸæˆæŠ¥å‘Šåœ¨æè¿°å†…æ¶ç¨‹åº¦ã€æ·±åº¦ã€é£é™©å’Œå½±å“æ–¹é¢çš„å‡†ç¡®æ€§ã€‚è¿™ç§åŒé‡èƒ½åŠ›å®ç°äº†å†…æ¶ç›‘æµ‹ä»å•çº¯æ„ŸçŸ¥å‘å†…å®¹ç”Ÿæˆçš„è½¬å˜ã€‚è¯¥å¤šæ¨¡å‹åä½œæ¡†æ¶ä¸ºæ™ºèƒ½ã€å¯æ‰©å±•çš„åŸå¸‚ç®¡ç†å’Œé˜²ç¾å“åº”ç³»ç»Ÿæä¾›äº†æ”¯æŒï¼Œæœ‰åŠ©äºæå‡åŸå¸‚åº”å¯¹æ°”å€™å˜åŒ–çš„éŸ§æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted to Nature",
      "pdf_url": "https://arxiv.org/pdf/2510.18425v1",
      "published_date": "2025-10-21 08:59:30 UTC",
      "updated_date": "2025-10-21 08:59:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:49:53.854916+00:00"
    },
    {
      "arxiv_id": "2510.18424v1",
      "title": "Med-VRAgent: A Framework for Medical Visual Reasoning-Enhanced Agents",
      "title_zh": "Med-VRAgentï¼šä¸€ç§åŒ»ç–—è§†è§‰æ¨ç†å¢å¼ºæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Guangfu Guo",
        "Xiaoqian Lu",
        "Yue Feng"
      ],
      "abstract": "Visual Language Models (VLMs) achieve promising results in medical reasoning but struggle with hallucinations, vague descriptions, inconsistent logic and poor localization. To address this, we propose a agent framework named Medical Visual Reasoning Agent (\\textbf{Med-VRAgent}). The approach is based on Visual Guidance and Self-Reward paradigms and Monte Carlo Tree Search (MCTS). By combining the Visual Guidance with tree search, Med-VRAgent improves the medical visual reasoning capabilities of VLMs. We use the trajectories collected by Med-VRAgent as feedback to further improve the performance by fine-tuning the VLMs with the proximal policy optimization (PPO) objective. Experiments on multiple medical VQA benchmarks demonstrate that our method outperforms existing approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨åŒ»å­¦æ¨ç†ä¸­å­˜åœ¨çš„å¹»è§‰ã€é€»è¾‘ä¸ä¸€è‡´åŠå®šä½èƒ½åŠ›å·®ç­‰é—®é¢˜ï¼Œæå‡ºäº†Med-VRAgentæ™ºèƒ½ä½“æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäºVisual Guidanceå’ŒSelf-RewardèŒƒå¼ï¼Œé€šè¿‡é›†æˆè’™ç‰¹å¡æ´›æ ‘æœç´¢(Monte Carlo Tree Search, MCTS)æ¥æ˜¾è‘—å¢å¼ºæ¨¡å‹çš„åŒ»å­¦è§†è§‰æ¨ç†èƒ½åŠ›ã€‚Med-VRAgentå°†è§†è§‰å¼•å¯¼ä¸æ ‘æœç´¢è¿‡ç¨‹ç›¸ç»“åˆï¼Œå¹¶åˆ©ç”¨ç”Ÿæˆçš„æ¨ç†è½¨è¿¹ä½œä¸ºåé¦ˆï¼Œé€šè¿‡è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(Proximal Policy Optimization, PPO)ç›®æ ‡å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚åœ¨å¤šä¸ªåŒ»å­¦è§†è§‰é—®ç­”(VQA)åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†å¤æ‚åŒ»ç–—å½±åƒæ¨ç†ä»»åŠ¡æ—¶ä¼˜äºç°æœ‰æ–¹æ¡ˆï¼Œæœ‰æ•ˆæå‡äº†åŒ»ç–—æ™ºèƒ½ä½“çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18424v1",
      "published_date": "2025-10-21 08:56:23 UTC",
      "updated_date": "2025-10-21 08:56:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:49:56.473139+00:00"
    },
    {
      "arxiv_id": "2510.21807v1",
      "title": "Activating Visual Context and Commonsense Reasoning through Masked Prediction in VLMs",
      "title_zh": "é€šè¿‡æ©ç é¢„æµ‹æ¿€æ´»è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰è¯­å¢ƒä¸å¸¸è¯†æ¨ç†",
      "authors": [
        "Jiaao Yu",
        "Shenwei Li",
        "Mingjie Han",
        "Yifei Yin",
        "Wenzheng Song",
        "Chenghao Jia",
        "Man Lan"
      ],
      "abstract": "Recent breakthroughs in reasoning models have markedly advanced the reasoning capabilities of large language models, particularly via training on tasks with verifiable rewards. Yet, a significant gap persists in their adaptation to real world multimodal scenarios, most notably, vision language tasks, due to a heavy focus on single modal language settings. While efforts to transplant reinforcement learning techniques from NLP to VLMs have emerged, these approaches often remain confined to perception centric tasks or reduce images to textual summaries, failing to fully exploit visual context and commonsense knowledge, ultimately constraining the generalization of reasoning capabilities across diverse multimodal environments. To address this limitation, we introduce a novel fine tuning task, Masked Prediction via Context and Commonsense, which forces models to integrate visual context and commonsense reasoning by reconstructing semantically meaningful content from occluded images, thereby laying the foundation for generalized reasoning. To systematically evaluate the model performance in generalized reasoning, we developed a specialized evaluation benchmark, MPCC Eval, and employed various fine tuning strategies to guide reasoning. Among these, we introduced an innovative training method, Reinforcement Fine tuning with Prior Sampling, which not only enhances model performance but also improves its generalized reasoning capabilities in OOD and cross task scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­éš¾ä»¥å……åˆ†åˆ©ç”¨è§†è§‰ä¸Šä¸‹æ–‡å’Œå¸¸è¯†çŸ¥è¯†çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMasked Prediction via Context and Commonsenseçš„åˆ›æ–°å¾®è°ƒä»»åŠ¡ã€‚è¯¥ä»»åŠ¡é€šè¿‡è¦æ±‚æ¨¡å‹ä»é®æŒ¡å›¾åƒä¸­é‡å»ºå…·æœ‰è¯­ä¹‰æ„ä¹‰çš„å†…å®¹ï¼Œå¼ºåˆ¶å…¶æ•´åˆè§†è§‰èƒŒæ™¯ä¸å¸¸è¯†æ¨ç†ï¼Œä»è€Œä¸ºæ³›åŒ–æ¨ç†å¥ å®šåŸºç¡€ã€‚ä¸ºäº†ç³»ç»Ÿè¯„ä¼°æ€§èƒ½ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸“é—¨çš„è¯„ä¼°åŸºå‡†MPCC-Evalï¼Œå¹¶å¼•å…¥äº†å¸¦å…ˆéªŒé‡‡æ ·çš„å¼ºåŒ–å¾®è°ƒ(Reinforcement Fine-tuning with Prior Sampling)è®­ç»ƒæ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨åˆ†å¸ƒå¤–(OOD)åŠè·¨ä»»åŠ¡åœºæ™¯ä¸‹çš„æ³›åŒ–æ¨ç†èƒ½åŠ›ï¼Œæœ‰æ•ˆå…‹æœäº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­æ³›åŒ–æ€§ä¸è¶³çš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.21807v1",
      "published_date": "2025-10-21 08:50:11 UTC",
      "updated_date": "2025-10-21 08:50:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:50:02.461334+00:00"
    },
    {
      "arxiv_id": "2510.21806v1",
      "title": "Frame-Difference Guided Dynamic Region Perception for CLIP Adaptation in Text-Video Retrieval",
      "title_zh": "é¢å‘æ–‡æœ¬-è§†é¢‘æ£€ç´¢ CLIP é€‚é…çš„å¸§å·®å¼•å¯¼åŠ¨æ€åŒºåŸŸæ„ŸçŸ¥",
      "authors": [
        "Jiaao Yu",
        "Mingjie Han",
        "Tao Gong",
        "Jian Zhang",
        "Man Lan"
      ],
      "abstract": "With the rapid growth of video data, text-video retrieval technology has become increasingly important in numerous application scenarios such as recommendation and search. Early text-video retrieval methods suffer from two critical drawbacks: first, they heavily rely on large-scale annotated video-text pairs, leading to high data acquisition costs; second, there is a significant modal gap between video and text features, which limits cross-modal alignment accuracy. With the development of vision-language model, adapting CLIP to video tasks has attracted great attention. However, existing adaptation methods generally lack enhancement for dynamic video features and fail to effectively suppress static redundant features. To address this issue, this paper proposes FDA-CLIP (Frame Difference Alpha-CLIP), which is a concise CLIP-based training framework for text-video alignment. Specifically, the method uses frame differences to generate dynamic region masks, which are input into Alpha-CLIP as an additional Alpha channel. This proactively guides the model to focus on semantically critical dynamic regions while suppressing static background redundancy. Experiments demonstrate that frame difference-guided video semantic encoding can effectively balance retrieval efficiency and accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FDA-CLIP (Frame Difference Alpha-CLIP)ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº CLIP çš„ç®€æ´è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–æ–‡æœ¬-è§†é¢‘æ£€ç´¢ (Text-Video Retrieval) ä»»åŠ¡ã€‚é’ˆå¯¹ç°æœ‰é€‚é…æ–¹æ³•ç¼ºä¹åŠ¨æ€ç‰¹å¾å¢å¼ºä¸”éš¾ä»¥æŠ‘åˆ¶é™æ€å†—ä½™çš„é—®é¢˜ï¼ŒFDA-CLIP é€šè¿‡è®¡ç®—å¸§å·® (Frame Difference) ç”ŸæˆåŠ¨æ€åŒºåŸŸæ©ç  (Dynamic Region Masks)ï¼Œå¹¶å°†å…¶ä½œä¸ºé¢å¤–çš„ Alpha é€šé“è¾“å…¥åˆ° Alpha-CLIP ä¸­ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿä¸»åŠ¨å¼•å¯¼æ¨¡å‹èšç„¦äºè¯­ä¹‰å…³é”®çš„åŠ¨æ€åŒºåŸŸï¼ŒåŒæ—¶æœ‰æ•ˆæŠ‘åˆ¶é™æ€èƒŒæ™¯ä¿¡æ¯çš„å¹²æ‰°ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¿™ç§åŸºäºå¸§å·®å¼•å¯¼çš„è§†é¢‘è¯­ä¹‰ç¼–ç æ–¹å¼èƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡æ£€ç´¢çš„æ•ˆç‡ä¸å‡†ç¡®æ€§ï¼Œæ˜¾è‘—æå‡äº†è·¨æ¨¡æ€å¯¹é½çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.21806v1",
      "published_date": "2025-10-21 08:48:59 UTC",
      "updated_date": "2025-10-21 08:48:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:50:13.853179+00:00"
    },
    {
      "arxiv_id": "2510.18417v1",
      "title": "On AI Verification in Open RAN",
      "title_zh": "è®º Open RAN ä¸­çš„äººå·¥æ™ºèƒ½éªŒè¯",
      "authors": [
        "Rahul Soundrarajan",
        "Claudio Fiandrino",
        "Michele Polese",
        "Salvatore D'Oro",
        "Leonardo Bonati",
        "Tommaso Melodia"
      ],
      "abstract": "Open RAN introduces a flexible, cloud-based architecture for the Radio Access Network (RAN), enabling Artificial Intelligence (AI)/Machine Learning (ML)-driven automation across heterogeneous, multi-vendor deployments. While EXplainable Artificial Intelligence (XAI) helps mitigate the opacity of AI models, explainability alone does not guarantee reliable network operations. In this article, we propose a lightweight verification approach based on interpretable models to validate the behavior of Deep Reinforcement Learning (DRL) agents for RAN slicing and scheduling in Open RAN. Specifically, we use Decision Tree (DT)-based verifiers to perform near-real-time consistency checks at runtime, which would be otherwise unfeasible with computationally expensive state-of-the-art verifiers. We analyze the landscape of XAI and AI verification, propose a scalable architectural integration, and demonstrate feasibility with a DT-based slice-verifier. We also outline future challenges to ensure trustworthy AI adoption in Open RAN.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Open RANæ¶æ„ä¸­AIé©±åŠ¨è‡ªåŠ¨åŒ–çš„å¯é æ€§é—®é¢˜ï¼ŒæŒ‡å‡ºä»…é å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)ä¸è¶³ä»¥ä¿è¯ç½‘ç»œè¿è¡Œã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºå¯è§£é‡Šæ¨¡å‹(Interpretable Models)çš„è½»é‡çº§éªŒè¯æ–¹æ³•ï¼Œç”¨äºéªŒè¯Open RANä¸­è´Ÿè´£åˆ‡ç‰‡(Slicing)å’Œè°ƒåº¦(Scheduling)çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ (DRL)æ™ºèƒ½ä½“è¡Œä¸ºã€‚è¯¥æ–¹æ³•åˆ©ç”¨å†³ç­–æ ‘(Decision Tree)éªŒè¯å™¨åœ¨è¿è¡Œæ—¶æ‰§è¡Œè¿‘å®æ—¶çš„ä¸€è‡´æ€§æ£€æŸ¥ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»ŸéªŒè¯æŠ€æœ¯è®¡ç®—å¼€é”€è¿‡å¤§çš„éš¾é¢˜ã€‚æ–‡ç« è¯¦ç»†åˆ†æäº†XAIä¸AIéªŒè¯çš„ç°çŠ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„æ¶æ„é›†æˆæ–¹æ¡ˆï¼Œé€šè¿‡åˆ‡ç‰‡éªŒè¯å™¨è¯æ˜äº†è¯¥æ–¹æ¡ˆçš„å¯è¡Œæ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨å¤šå‚å•†Open RANéƒ¨ç½²ä¸­å®ç°å¯ä¿¡ã€å¯é çš„AIåº”ç”¨å¥ å®šäº†åŸºç¡€å¹¶æŒ‡å‡ºäº†æœªæ¥æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18417v1",
      "published_date": "2025-10-21 08:48:26 UTC",
      "updated_date": "2025-10-21 08:48:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:50:22.664953+00:00"
    },
    {
      "arxiv_id": "2510.18412v1",
      "title": "Deep Learning-Based Control Optimization for Glass Bottle Forming",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„ç»ç’ƒç“¶æˆå‹æ§åˆ¶ä¼˜åŒ–",
      "authors": [
        "Mattia Pujatti",
        "Andrea Di Luca",
        "Nicola Peghini",
        "Federico Monegaglia",
        "Marco Cristoforetti"
      ],
      "abstract": "In glass bottle manufacturing, precise control of forming machines is critical for ensuring quality and minimizing defects. This study presents a deep learning-based control algorithm designed to optimize the forming process in real production environments. Using real operational data from active manufacturing plants, our neural network predicts the effects of parameter changes based on the current production setup. Through a specifically designed inversion mechanism, the algorithm identifies the optimal machine settings required to achieve the desired glass gob characteristics. Experimental results on historical datasets from multiple production lines show that the proposed method yields promising outcomes, suggesting potential for enhanced process stability, reduced waste, and improved product consistency. These results highlight the potential of deep learning to process control in glass manufacturing.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº Deep Learning çš„æ§åˆ¶ä¼˜åŒ–ç®—æ³•ï¼Œæ—¨åœ¨æé«˜ç»ç’ƒç“¶æˆå‹è¿‡ç¨‹ä¸­æœºæ¢°æ§åˆ¶çš„ç²¾ç¡®åº¦å¹¶å‡å°‘ç¼ºé™·ã€‚è¯¥ç®—æ³•åˆ©ç”¨æ¥è‡ªå®é™…ç”Ÿäº§ç¯å¢ƒçš„è¿è¥æ•°æ®ï¼Œé€šè¿‡ Neural Network é¢„æµ‹ç”Ÿäº§å‚æ•°å˜åŒ–å¯¹å·¥è‰ºæµç¨‹çš„å½±å“ã€‚é€šè¿‡è®¾è®¡ä¸€ç§ä¸“é—¨çš„ Inversion Mechanismï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé’ˆå¯¹ç›®æ ‡ Glass Gob Characteristics è¯†åˆ«å‡ºæœ€ä¼˜çš„æœºå™¨è®¾ç½®æ–¹æ¡ˆã€‚åœ¨å¤šæ¡ç”Ÿäº§çº¿å†å²æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¢å¼ºè¿‡ç¨‹ç¨³å®šæ€§ã€å‡å°‘åºŸå“ä»¥åŠæå‡äº§å“ä¸€è‡´æ€§æ–¹é¢å…·æœ‰æ˜¾è‘—æ•ˆæœã€‚è¿™é¡¹ç ”ç©¶å±•ç¤ºäº†æ·±åº¦å­¦ä¹ åœ¨ç»ç’ƒåˆ¶é€ è¿‡ç¨‹æ§åˆ¶ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºå·¥ä¸šç”Ÿäº§çš„æ™ºèƒ½åŒ–ä¼˜åŒ–æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "37 pages, 17 figures, accepted for publication in \"Expert Systems With Applications\"",
      "pdf_url": "https://arxiv.org/pdf/2510.18412v1",
      "published_date": "2025-10-21 08:42:35 UTC",
      "updated_date": "2025-10-21 08:42:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:50:22.855024+00:00"
    },
    {
      "arxiv_id": "2510.18407v1",
      "title": "Heterogeneous Adversarial Play in Interactive Environments",
      "title_zh": "äº¤äº’å¼ç¯å¢ƒä¸‹çš„å¼‚æ„å¯¹æŠ—åšå¼ˆ",
      "authors": [
        "Manjie Xu",
        "Xinyi Yang",
        "Jiayu Zhan",
        "Wei Liang",
        "Chi Zhang",
        "Yixin Zhu"
      ],
      "abstract": "Self-play constitutes a fundamental paradigm for autonomous skill acquisition, whereby agents iteratively enhance their capabilities through self-directed environmental exploration. Conventional self-play frameworks exploit agent symmetry within zero-sum competitive settings, yet this approach proves inadequate for open-ended learning scenarios characterized by inherent asymmetry. Human pedagogical systems exemplify asymmetric instructional frameworks wherein educators systematically construct challenges calibrated to individual learners' developmental trajectories. The principal challenge resides in operationalizing these asymmetric, adaptive pedagogical mechanisms within artificial systems capable of autonomously synthesizing appropriate curricula without predetermined task hierarchies. Here we present Heterogeneous Adversarial Play (HAP), an adversarial Automatic Curriculum Learning framework that formalizes teacher-student interactions as a minimax optimization wherein task-generating instructor and problem-solving learner co-evolve through adversarial dynamics. In contrast to prevailing ACL methodologies that employ static curricula or unidirectional task selection mechanisms, HAP establishes a bidirectional feedback system wherein instructors continuously recalibrate task complexity in response to real-time learner performance metrics. Experimental validation across multi-task learning domains demonstrates that our framework achieves performance parity with SOTA baselines while generating curricula that enhance learning efficacy in both artificial agents and human subjects.",
      "tldr_zh": "ä¼ ç»Ÿçš„Self-playæ¡†æ¶é€šå¸¸ä¾èµ–é›¶å’Œç«äº‰ä¸­çš„Agent Symmetryï¼Œä½†åœ¨å¤„ç†å…·æœ‰å›ºæœ‰ä¸å¯¹ç§°æ€§çš„å¼€æ”¾å¼å­¦ä¹ åœºæ™¯æ—¶è¡¨ç°ä¸è¶³ã€‚è¯¥ç ”ç©¶å€Ÿé‰´äººç±»æ•™å­¦ç³»ç»Ÿä¸­çš„ä¸å¯¹ç§°æ•™å­¦æ¨¡å¼ï¼Œæ—¨åœ¨è§£å†³äººå·¥ç³»ç»Ÿä¸­è‡ªåŠ¨åˆæˆè‡ªé€‚åº”è¯¾ç¨‹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†Heterogeneous Adversarial Play (HAP) è¿™ä¸€å¯¹æŠ—æ€§Automatic Curriculum Learning (ACL) æ¡†æ¶ã€‚HAPå°†å¸ˆç”Ÿäº’åŠ¨å»ºæ¨¡ä¸ºMinimax Optimizationï¼Œä½¿è´Ÿè´£ä»»åŠ¡ç”Ÿæˆçš„Instructorä¸è´Ÿè´£è§£å†³é—®é¢˜çš„Learneråœ¨å¯¹æŠ—åŠ¨æ€ä¸­å…±åŒè¿›åŒ–ã€‚ä¸é‡‡ç”¨é™æ€è¯¾ç¨‹æˆ–å•å‘ä»»åŠ¡é€‰æ‹©æœºåˆ¶çš„ä¼ ç»ŸACLæ–¹æ³•ä¸åŒï¼ŒHAPå»ºç«‹äº†åŒå‘åé¦ˆç³»ç»Ÿï¼Œèƒ½å¤Ÿæ ¹æ®Learnerçš„å®æ—¶è¡¨ç°æŒ‡æ ‡æŒç»­é‡æ–°æ ¡å‡†ä»»åŠ¡å¤æ‚åº¦ã€‚å¤šä»»åŠ¡å­¦ä¹ é¢†åŸŸçš„å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°äº†SOTAæ°´å¹³ï¼Œå¹¶èƒ½ç”Ÿæˆæœ‰æ•ˆæå‡äººå·¥æ™ºèƒ½Agentå’Œäººç±»å—è¯•è€…å­¦ä¹ æ•ˆç‡çš„è¯¾ç¨‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.18407v1",
      "published_date": "2025-10-21 08:29:59 UTC",
      "updated_date": "2025-10-21 08:29:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:50:23.059153+00:00"
    },
    {
      "arxiv_id": "2510.18406v2",
      "title": "Learning from N-Tuple Data with M Positive Instances: Unbiased Risk Estimation and Theoretical Guarantees",
      "title_zh": "ä»å…·æœ‰ M ä¸ªæ­£ä¾‹çš„ N å…ƒç»„æ•°æ®ä¸­å­¦ä¹ ï¼šæ— åé£é™©ä¼°è®¡ä¸ç†è®ºä¿è¯",
      "authors": [
        "Miao Zhang",
        "Junpeng Li",
        "ChangChun HUa",
        "Yana Yang"
      ],
      "abstract": "Weakly supervised learning often operates with coarse aggregate signals rather than instance labels. We study a setting where each training example is an $n$-tuple containing exactly m positives, while only the count m per tuple is observed. This NTMP (N-tuple with M positives) supervision arises in, e.g., image classification with region proposals and multi-instance measurements. We show that tuple counts admit a trainable unbiased risk estimator (URE) by linking the tuple-generation process to latent instance marginals. Starting from fixed (n,m), we derive a closed-form URE and extend it to variable tuple sizes, variable counts, and their combination. Identification holds whenever the effective mixing rate is separated from the class prior. We establish generalization bounds via Rademacher complexity and prove statistical consistency with standard rates under mild regularity assumptions. To improve finite-sample stability, we introduce simple ReLU corrections to the URE that preserve asymptotic correctness. Across benchmarks converted to NTMP tasks, the approach consistently outperforms representative weak-supervision baselines and yields favorable precision-recall and F1 trade-offs. It remains robust under class-prior imbalance and across diverse tuple configurations, demonstrating that count-only supervision can be exploited effectively through a theoretically grounded and practically stable objective.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼±ç›‘ç£å­¦ä¹ ä¸­çš„ NTMP (N-tuple with M positives) è®¾ç½®ï¼Œå³è®­ç»ƒæ ·æœ¬ä¸ºåŒ…å« m ä¸ªæ­£æ ·æœ¬çš„ n å…ƒç»„ï¼Œä¸”ä»…èƒ½è§‚æµ‹åˆ°è®¡æ•° mã€‚ç ”ç©¶è€…é€šè¿‡å»ºç«‹å…ƒç»„ç”Ÿæˆè¿‡ç¨‹ä¸æ½œåœ¨å®ä¾‹è¾¹é™…åˆ†å¸ƒçš„è”ç³»ï¼Œæ¨å¯¼å‡ºä¸€ç§é—­å¼æ— åé£é™©ä¼°è®¡å™¨ (Unbiased Risk Estimator, URE)ï¼Œå¹¶å°†å…¶æ‰©å±•è‡³å¯å˜å…ƒç»„å¤§å°ä¸è®¡æ•°çš„æƒ…å½¢ã€‚è®ºæ–‡åœ¨ç†è®ºä¸Šè¯æ˜äº†è¯¥æ–¹æ³•çš„å¯è¾¨è¯†æ€§ï¼Œå»ºç«‹äº†åŸºäº Rademacher å¤æ‚åº¦çš„æ³›åŒ–ç•Œï¼Œå¹¶éªŒè¯äº†å…¶åœ¨æ¸©å’Œå‡è®¾ä¸‹çš„ç»Ÿè®¡ä¸€è‡´æ€§ã€‚ä¸ºæå‡æœ‰é™æ ·æœ¬çš„ç¨³å®šæ€§ï¼Œç ”ç©¶å¼•å…¥äº†ç®€å•çš„ ReLU ä¿®æ­£æ‰‹æ®µä»¥ä¼˜åŒ– URE çš„å®é™…è¡¨ç°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šé¡¹åŸºå‡†ä»»åŠ¡ä¸­ä¸€è‡´ä¼˜äºç°æœ‰çš„å¼±ç›‘ç£åŸºçº¿ï¼Œåœ¨ç²¾åº¦ã€å¬å›ç‡åŠ F1 åˆ†æ•°ä¸Šå–å¾—äº†æ›´ä½³çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆåœ¨ç±»å…ˆéªŒä¸å¹³è¡¡å’Œå¤šæ ·åŒ–å…ƒç»„é…ç½®ä¸‹å±•ç°äº†å‡ºè‰²çš„é²æ£’æ€§ï¼Œè¯æ˜äº†é€šè¿‡ç†è®ºå®Œå¤‡çš„å‡½æ•°å¯ä»¥æœ‰æ•ˆåˆ©ç”¨çº¯è®¡æ•°ç›‘ç£ä¿¡æ¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18406v2",
      "published_date": "2025-10-21 08:28:07 UTC",
      "updated_date": "2025-11-10 07:41:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:50:30.252666+00:00"
    },
    {
      "arxiv_id": "2510.18405v1",
      "title": "Automated Wicket-Taking Delivery Segmentation and Weakness Detection in Cricket Videos Using OCR-Guided YOLOv8 and Trajectory Modeling",
      "title_zh": "åŸºäº OCR å¼•å¯¼çš„ YOLOv8 ä¸è½¨è¿¹å»ºæ¨¡çš„æ¿çƒè§†é¢‘è‡ªåŠ¨å–çƒç‰‡æ®µåˆ†å‰²åŠå¼±ç‚¹æ£€æµ‹",
      "authors": [
        "Mst Jannatun Ferdous",
        "Masum Billah",
        "Joy Karmoker",
        "Mohd Ruhul Ameen",
        "Akif Islam",
        "Md. Omar Faruqe"
      ],
      "abstract": "This paper presents an automated system for cricket video analysis that leverages deep learning techniques to extract wicket-taking deliveries, detect cricket balls, and model ball trajectories. The system employs the YOLOv8 architecture for pitch and ball detection, combined with optical character recognition (OCR) for scorecard extraction to identify wicket-taking moments. Through comprehensive image preprocessing, including grayscale transformation, power transformation, and morphological operations, the system achieves robust text extraction from video frames. The pitch detection model achieved 99.5% mean Average Precision at 50% IoU (mAP50) with a precision of 0.999, while the ball detection model using transfer learning attained 99.18% mAP50 with 0.968 precision and 0.978 recall. The system enables trajectory modeling on detected pitches, providing data-driven insights for identifying batting weaknesses. Experimental results on multiple cricket match videos demonstrate the effectiveness of this approach for automated cricket analytics, offering significant potential for coaching and strategic decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§è‡ªåŠ¨æ¿çƒè§†é¢‘åˆ†æç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡æ·±åº¦å­¦ä¹ æŠ€æœ¯æå–ä¸‰æŸ±é—¨(wicket-taking)æŠ•çƒç‰‡æ®µã€æ£€æµ‹æ¿çƒå¹¶å»ºæ¨¡çƒçš„è½¨è¿¹ã€‚ç³»ç»Ÿåˆ©ç”¨ YOLOv8 æ¶æ„è¿›è¡Œçƒåœº(pitch)å’Œçƒçš„æ£€æµ‹ï¼Œå¹¶ç»“åˆå…‰å­¦å­—ç¬¦è¯†åˆ«(OCR)æŠ€æœ¯ä»è®°åˆ†ç‰Œä¸­æå–æ•°æ®ï¼Œä»¥ç²¾å‡†è¯†åˆ«å‡ºå‡»çƒå‡ºå±€çš„å…³é”®æ—¶åˆ»ã€‚é€šè¿‡ç°åº¦å˜æ¢ã€å¹‚å¾‹å˜æ¢å’Œå½¢æ€å­¦æ“ä½œç­‰é¢„å¤„ç†æ‰‹æ®µï¼Œç³»ç»Ÿå®ç°äº†ç¨³å¥çš„æ–‡æœ¬æå–ï¼Œå…¶ä¸­çƒåœºæ£€æµ‹æ¨¡å‹çš„å¹³å‡ç²¾åº¦å‡å€¼(mAP50)è¾¾åˆ° 99.5%ï¼Œçƒæ£€æµ‹æ¨¡å‹è¾¾åˆ° 99.18% çš„ mAP50ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿæ”¯æŒåœ¨æ£€æµ‹åˆ°çš„çƒåœºä¸Šè¿›è¡Œè½¨è¿¹å»ºæ¨¡(trajectory modeling)ï¼Œä¸ºåˆ†æå‡»çƒæ‰‹çš„å¼±ç‚¹æä¾›æ•°æ®é©±åŠ¨çš„è§è§£ã€‚å¤šåœºæ¿çƒæ¯”èµ›è§†é¢‘çš„å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç°äº†å…¶åœ¨è‡ªåŠ¨åŒ–æ¿çƒåˆ†æã€æ•™ç»ƒæŒ‡å¯¼å’Œæˆ˜ç•¥å†³ç­–æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 figures, 5 tables, submitted to the 11th IEEE International Women in Engineering (WIE) Conference on Electrical and Computer Engineering 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.18405v1",
      "published_date": "2025-10-21 08:27:23 UTC",
      "updated_date": "2025-10-21 08:27:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:50:31.050884+00:00"
    },
    {
      "arxiv_id": "2510.18921v1",
      "title": "Benchmarking On-Device Machine Learning on Apple Silicon with MLX",
      "title_zh": "åŸºäº MLX çš„ Apple Silicon ç«¯ä¾§æœºå™¨å­¦ä¹ åŸºå‡†æµ‹è¯•",
      "authors": [
        "Oluwaseun A. Ajayi",
        "Ogundepo Odunayo"
      ],
      "abstract": "The recent widespread adoption of Large Language Models (LLMs) and machine learning in general has sparked research interest in exploring the possibilities of deploying these models on smaller devices such as laptops and mobile phones. This creates a need for frameworks and approaches that are capable of taking advantage of on-device hardware. The MLX framework was created to address this need. It is a framework optimized for machine learning (ML) computations on Apple silicon devices, facilitating easier research, experimentation, and prototyping.\n  This paper presents a performance evaluation of MLX, focusing on inference latency of transformer models. We compare the performance of different transformer architecture implementations in MLX with their Pytorch counterparts. For this research we create a framework called MLX-transformers which includes different transformer implementations in MLX and downloads the model checkpoints in pytorch and converts it to the MLX format. By leveraging the advanced architecture and capabilities of Apple Silicon, MLX-Transformers enables seamless execution of transformer models directly sourced from Hugging Face, eliminating the need for checkpoint conversion often required when porting models between frameworks.\n  Our study benchmarks different transformer models on two Apple Silicon macbook devices against an NVIDIA CUDA GPU. Specifically, we compare the inference latency performance of models with the same parameter sizes and checkpoints. We evaluate the performance of BERT, RoBERTa, and XLM-RoBERTa models, with the intention of extending future work to include models of different modalities, thus providing a more comprehensive assessment of MLX's capabilities. The results highlight MLX's potential in enabling efficient and more accessible on-device ML applications within Apple's ecosystem.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹é’ˆå¯¹ Apple Silicon ä¼˜åŒ–çš„æœºå™¨å­¦ä¹ æ¡†æ¶ MLX è¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ï¼Œé‡ç‚¹å…³æ³¨ transformer æ¨¡å‹çš„æ¨ç†å»¶è¿Ÿ(inference latency)ã€‚ä½œè€…å¼€å‘äº† MLX-transformers æ¡†æ¶ï¼Œæ—¨åœ¨ç®€åŒ–æ¨¡å‹ä» Hugging Face åˆ° MLX æ ¼å¼çš„è½¬æ¢ä¸æ‰§è¡Œè¿‡ç¨‹ï¼Œå®ç°æ— ç¼æ¨ç†ã€‚ç ”ç©¶å¯¹æ¯”äº† MLX å®ç°ä¸ PyTorch ç‰ˆæœ¬çš„æ€§èƒ½ï¼Œå¹¶åœ¨ä¸¤æ¬¾ Apple Silicon MacBook ä»¥åŠ NVIDIA CUDA GPU ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚è¯„ä¼°æ¶µç›–äº† BERTã€RoBERTa å’Œ XLM-RoBERTa ç­‰å¤šç§æ¨¡å‹æ¶æ„ã€‚å®éªŒç»“æœçªå‡ºäº† MLX åœ¨ Apple ç”Ÿæ€ç³»ç»Ÿä¸­å®ç°é«˜æ•ˆã€æ˜“ç”¨çš„è®¾å¤‡ç«¯(on-device)æœºå™¨å­¦ä¹ åº”ç”¨çš„æ½œåŠ›ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨ä¾¿æºå¼è®¾å¤‡ä¸Šéƒ¨ç½²å¤§å‹æ¨¡å‹æä¾›äº†é‡è¦çš„æŠ€æœ¯å‚è€ƒï¼Œå¹¶è®¡åˆ’åœ¨æœªæ¥æ‰©å±•åˆ°å¤šæ¨¡æ€æ¨¡å‹çš„è¯„ä¼°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 6 figures. Presented at the 6th Deep Learning Indaba (DLI 2024), Dakar, Senegal; non-archival presentation. Poster: https://storage.googleapis.com/indaba-public/Oluwaseun_Ajayi%20.pdf",
      "pdf_url": "https://arxiv.org/pdf/2510.18921v1",
      "published_date": "2025-10-21 08:19:27 UTC",
      "updated_date": "2025-10-21 08:19:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:50:33.155642+00:00"
    },
    {
      "arxiv_id": "2510.18395v1",
      "title": "Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for Real-Time Strategy Games",
      "title_zh": "è®°å¿†å¢å¼ºçŠ¶æ€æœºæç¤ºï¼šä¸€ç§é¢å‘å³æ—¶æˆ˜ç•¥æ¸¸æˆçš„æ–°å‹å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Runnan Qi",
        "Yanan Ni",
        "Lumin Jiang",
        "Zongyuan Li",
        "Kuihua Huang",
        "Xian Guo"
      ],
      "abstract": "This paper proposes Memory-Augmented State Machine Prompting (MASMP), a novel framework for LLM agents in real-time strategy games. Addressing key challenges like hallucinations and fragmented decision-making in existing approaches, MASMP integrates state machine prompting with memory mechanisms to unify structured actions with long-term tactical coherence. The framework features: (1) a natural language-driven state machine architecture that guides LLMs to emulate finite state machines and behavior trees through prompts, and (2) a lightweight memory module preserving strategic variables (e.g., tactics, priority units) across decision cycles. Experiments in StarCraft II demonstrate MASMP's 60% win rate against the hardest built-in AI (Lv7), vastly outperforming baselines (0%). Case studies reveal the method retains LLMs' semantic comprehension while resolving the \"Knowing-Doing Gap\" through strict state-action mapping, achieving both interpretability and FSM-like reliability. This work establishes a new paradigm for combining neural and symbolic AI in complex decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Memory-Augmented State Machine Prompting (MASMP)ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå®æ—¶æˆ˜ç•¥ (Real-Time Strategy) æ¸¸æˆè®¾è®¡çš„ LLM æ™ºèƒ½ä½“æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•ä¸­å­˜åœ¨çš„å¹»è§‰å’Œå†³ç­–ç¢ç‰‡åŒ–ç­‰å…³é”®æŒ‘æˆ˜ï¼Œé€šè¿‡å°†çŠ¶æ€æœºæç¤º (State Machine Prompting) ä¸è®°å¿†æœºåˆ¶ç›¸ç»“åˆï¼Œå®ç°äº†ç»“æ„åŒ–æ“ä½œä¸é•¿æœŸæˆ˜æœ¯è¿è´¯æ€§çš„ç»Ÿä¸€ã€‚MASMP é‡‡ç”¨è‡ªç„¶è¯­è¨€é©±åŠ¨çš„çŠ¶æ€æœºæ¶æ„å¼•å¯¼ LLM æ¨¡æ‹Ÿæœ‰é™çŠ¶æ€æœº (Finite State Machines) å’Œè¡Œä¸ºæ ‘ï¼Œå¹¶åˆ©ç”¨è½»é‡çº§è®°å¿†æ¨¡å—åœ¨å†³ç­–å‘¨æœŸä¸­ä¿ç•™æ ¸å¿ƒæˆ˜ç•¥å˜é‡ã€‚åœ¨ã€Šæ˜Ÿé™…äº‰éœ¸IIã€‹(StarCraft II) çš„å®éªŒä¸­ï¼ŒMASMP é¢å¯¹æœ€é«˜éš¾åº¦å†…ç½® AI è¾¾åˆ°äº† 60% çš„èƒœç‡ï¼Œè¿œè¶…åŸºå‡†æ¨¡å‹çš„é›¶èƒœç‡è¡¨ç°ã€‚æ¡ˆä¾‹ç ”ç©¶è¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿ç•™ LLM è¯­ä¹‰ç†è§£èƒ½åŠ›çš„åŒæ—¶ï¼Œé€šè¿‡ä¸¥æ ¼çš„çŠ¶æ€-åŠ¨ä½œæ˜ å°„è§£å†³äº†â€œçŸ¥è¡Œå·®è·â€ (Knowing-Doing Gap) é—®é¢˜ï¼Œå…¼å…·å¯è§£é‡Šæ€§ä¸å¯é æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨å¤æ‚å†³ç­–åœºæ™¯ä¸­ç»“åˆç¥ç»ä¸ç¬¦å·äººå·¥æ™ºèƒ½ (Neural and Symbolic AI) å»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 4 figures, 1 table, 1 algorithm. Submitted to conference",
      "pdf_url": "https://arxiv.org/pdf/2510.18395v1",
      "published_date": "2025-10-21 08:15:04 UTC",
      "updated_date": "2025-10-21 08:15:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:50:37.056805+00:00"
    },
    {
      "arxiv_id": "2510.18383v2",
      "title": "MENTOR: A Reinforcement Learning Framework for Enabling Tool Use in Small Models via Teacher-Optimized Rewards",
      "title_zh": "MENTORï¼šä¸€ç§é€šè¿‡æ•™å¸ˆä¼˜åŒ–å¥–åŠ±èµ‹èƒ½å°æ¨¡å‹å·¥å…·è°ƒç”¨èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
      "authors": [
        "ChangSu Choi",
        "Hoyun Song",
        "Dongyeon Kim",
        "WooHyeon Jung",
        "Minkyung Cho",
        "Sunjin Park",
        "NohHyeob Bae",
        "Seona Yu",
        "KyungTae Lim"
      ],
      "abstract": "Distilling the tool-using capabilities of large language models (LLMs) into smaller, more efficient small language models (SLMs) is a key challenge for their practical application. The predominant approach, supervised fine-tuning (SFT), suffers from poor generalization as it trains models to imitate a static set of teacher trajectories rather than learn a robust methodology. While reinforcement learning (RL) offers an alternative, the standard RL using sparse rewards fails to effectively guide SLMs, causing them to struggle with inefficient exploration and adopt suboptimal strategies. To address these distinct challenges, we propose MENTOR, a framework that synergistically combines RL with teacher-guided distillation. Instead of simple imitation, MENTOR employs an RL-based process to learn a more generalizable policy through exploration. In addition, to solve the problem of reward sparsity, it uses a teacher's reference trajectory to construct a dense, composite teacher-guided reward that provides fine-grained guidance. Extensive experiments demonstrate that MENTOR significantly improves the cross-domain generalization and strategic competence of SLMs compared to both SFT and standard sparse-reward RL baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MENTOR æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å°†å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›è’¸é¦åˆ°å°å‹è¯­è¨€æ¨¡å‹ (SLMs) æ—¶é¢ä¸´çš„æ³›åŒ–èƒ½åŠ›å·®å’Œæ¢ç´¢æ•ˆç‡ä½ç­‰å…³é”®æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒ (SFT) å¾€å¾€å› è¿‡åº¦æ¨¡ä»¿é™æ€è½¨è¿¹å¯¼è‡´æ³›åŒ–ä¸è¶³ï¼Œè€Œæ ‡å‡†çš„å¼ºåŒ–å­¦ä¹  (RL) åˆ™å—é™äºç¨€ç–å¥–åŠ± (sparse rewards)ï¼Œéš¾ä»¥æœ‰æ•ˆå¼•å¯¼æ¨¡å‹è¿›è¡Œé«˜æ•ˆæ¢ç´¢ã€‚MENTOR é€šè¿‡ååŒç»“åˆå¼ºåŒ–å­¦ä¹ ä¸æ•™å¸ˆå¼•å¯¼çš„è’¸é¦æŠ€æœ¯ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ çš„æ¢ç´¢æœºåˆ¶æ¥åŸ¹å…»æ›´å…·æ³›åŒ–æ€§çš„ç­–ç•¥ï¼Œè€Œéç®€å•çš„æ¨¡ä»¿å­¦ä¹ ã€‚ä¸ºäº†åº”å¯¹å¥–åŠ±ç¨€ç–æ€§é—®é¢˜ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ•™å¸ˆæ¨¡å‹çš„å‚è€ƒè½¨è¿¹æ„å»ºäº†ä¸€ç§ç¨ å¯†çš„å¤åˆå‹æ•™å¸ˆä¼˜åŒ–å¥–åŠ± (teacher-optimized rewards)ï¼Œä¸ºæ¨¡å‹æä¾›ç»†ç²’åº¦çš„æŒ‡å¯¼ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ SFT å’Œæ ‡å‡†å¼ºåŒ–å­¦ä¹ åŸºå‡†ç›¸æ¯”ï¼ŒMENTOR æ˜¾è‘—æå‡äº†å°å‹è¯­è¨€æ¨¡å‹åœ¨è·¨é¢†åŸŸæ³›åŒ–å’Œç­–ç•¥èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18383v2",
      "published_date": "2025-10-21 08:03:14 UTC",
      "updated_date": "2025-10-28 04:50:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:50:40.841417+00:00"
    },
    {
      "arxiv_id": "2510.18381v1",
      "title": "S2AP: Score-space Sharpness Minimization for Adversarial Pruning",
      "title_zh": "S2APï¼šé¢å‘å¯¹æŠ—å‰ªæçš„åˆ†æ•°ç©ºé—´é”åº¦æœ€å°åŒ–",
      "authors": [
        "Giorgio Piras",
        "Qi Zhao",
        "Fabio Brau",
        "Maura Pintor",
        "Christian Wressnegger",
        "Battista Biggio"
      ],
      "abstract": "Adversarial pruning methods have emerged as a powerful tool for compressing neural networks while preserving robustness against adversarial attacks. These methods typically follow a three-step pipeline: (i) pretrain a robust model, (ii) select a binary mask for weight pruning, and (iii) finetune the pruned model. To select the binary mask, these methods minimize a robust loss by assigning an importance score to each weight, and then keep the weights with the highest scores. However, this score-space optimization can lead to sharp local minima in the robust loss landscape and, in turn, to an unstable mask selection, reducing the robustness of adversarial pruning methods. To overcome this issue, we propose a novel plug-in method for adversarial pruning, termed Score-space Sharpness-aware Adversarial Pruning (S2AP). Through our method, we introduce the concept of score-space sharpness minimization, which operates during the mask search by perturbing importance scores and minimizing the corresponding robust loss. Extensive experiments across various datasets, models, and sparsity levels demonstrate that S2AP effectively minimizes sharpness in score space, stabilizing the mask selection, and ultimately improving the robustness of adversarial pruning methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯¹æŠ—æ€§å‰ªæ(Adversarial pruning)åœ¨æ©ç æœç´¢è¿‡ç¨‹ä¸­å®¹æ˜“é™·å…¥åˆ†æ•°ç©ºé—´(score-space)å°–é”å±€éƒ¨æœ€å°å€¼(sharp local minima)ä»è€Œå¯¼è‡´é²æ£’æ€§ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºS2APçš„å³æ’å³ç”¨æ–¹æ³•ã€‚S2APçš„æ ¸å¿ƒåœ¨äºå¼•å…¥äº†åˆ†æ•°ç©ºé—´é”åº¦æœ€å°åŒ–(Score-space Sharpness Minimization)çš„æ¦‚å¿µï¼Œé€šè¿‡åœ¨æœç´¢äºŒè¿›åˆ¶æ©ç (binary mask)æ—¶æ‰°åŠ¨é‡è¦æ€§åˆ†æ•°å¹¶æœ€å°åŒ–ç›¸åº”çš„é²æ£’æŸå¤±ï¼Œæ¥æå‡æ©ç é€‰æ‹©çš„ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆå¹³æ»‘é²æ£’æŸå¤±çš„åœ°è²Œï¼Œå¹¶ç¨³å®šæ©ç ç”Ÿæˆçš„è´¨é‡ã€‚åœ¨å¤šç§æ•°æ®é›†ã€æ¨¡å‹å’Œç¨€ç–åº¦æ°´å¹³ä¸‹çš„å¹¿æ³›è¯„ä¼°è¯æ˜ï¼ŒS2APæ˜¾è‘—å¢å¼ºäº†å¯¹æŠ—æ€§å‰ªææ–¹æ³•çš„é²æ£’æ€§ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•åœ¨åˆ†æ•°ç©ºé—´ä¼˜åŒ–ä¸­çš„ä¸ç¨³å®šæ€§æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18381v1",
      "published_date": "2025-10-21 07:55:31 UTC",
      "updated_date": "2025-10-21 07:55:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:50:45.208957+00:00"
    },
    {
      "arxiv_id": "2511.02845v2",
      "title": "AI-Enhanced Real-Time Wi-Fi Sensing Through Single Transceiver Pair",
      "title_zh": "åŸºäºå•æ”¶å‘å¯¹çš„AIå¢å¼ºå®æ—¶Wi-Fiæ„ŸçŸ¥",
      "authors": [
        "Yuxuan Liu",
        "Chiya Zhang",
        "Yifeng Yuan",
        "Chunlong He",
        "Weizheng Zhang",
        "Gaojie Chen"
      ],
      "abstract": "The advancement of next-generation Wi-Fi technology heavily relies on sensing capabilities, which play a pivotal role in enabling sophisticated applications. In response to the growing demand for large-scale deployments, contemporary Wi-Fi sensing systems strive to achieve high-precision perception while maintaining minimal bandwidth consumption and antenna count requirements. Remarkably, various AI-driven perception technologies have demonstrated the ability to surpass the traditional resolution limitations imposed by radar theory. However, the theoretical underpinnings of this phenomenon have not been thoroughly investigated in existing research. In this study, we found that under hardware-constrained conditions, the performance gains brought by AI to Wi-Fi sensing systems primarily originate from two aspects: prior information and temporal correlation. Prior information enables the AI to generate plausible details based on vague input, while temporal correlation helps reduce the upper bound of sensing error. Building on these insights, we developed a real-time, AI-based Wi-Fi sensing and visualization system using a single transceiver pair, and designed experiments focusing on human pose estimation and indoor localization. The system operates in real time on commodity hardware, and experimental results confirm our theoretical findings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸‹ä¸€ä»£ Wi-Fi æŠ€æœ¯ä¸­çš„é«˜ç²¾åº¦æ„ŸçŸ¥éœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•åœ¨æœ€å°å¸¦å®½å’Œå¤©çº¿æ•°é‡çš„é™åˆ¶ä¸‹é€šè¿‡ AI æŠ€æœ¯è¶…è¶Šä¼ ç»Ÿé›·è¾¾ç†è®º (radar theory) çš„åˆ†è¾¨ç‡é™åˆ¶ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œåœ¨ç¡¬ä»¶å—é™æ¡ä»¶ä¸‹ï¼ŒAI å¯¹ Wi-Fi Sensing ç³»ç»Ÿçš„æå‡ä¸»è¦æºäºå…ˆéªŒä¿¡æ¯ (prior information) å’Œæ—¶é—´ç›¸å…³æ€§ (temporal correlation) ä¸¤ä¸ªæ ¸å¿ƒè¦ç´ ã€‚å…¶ä¸­å…ˆéªŒä¿¡æ¯è¾…åŠ© AI ä»æ¨¡ç³Šè¾“å…¥ä¸­è¿˜åŸç»†èŠ‚ï¼Œè€Œæ—¶é—´ç›¸å…³æ€§åˆ™æœ‰æ•ˆé™ä½äº†æ„ŸçŸ¥çš„è¯¯å·®ä¸Šé™ã€‚åŸºäºæ­¤ç†è®ºï¼Œä½œè€…å¼€å‘äº†ä¸€å¥—ä»…ä½¿ç”¨å•æ”¶å‘å™¨å¯¹ (single transceiver pair) çš„å®æ—¶ AI Wi-Fi Sensing ä¸å¯è§†åŒ–ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿåœ¨å•†ç”¨ç¡¬ä»¶ä¸Šå®æ—¶è¿è¡Œï¼Œå¹¶æˆåŠŸåº”ç”¨äºäººä½“å§¿æ€ä¼°è®¡ (human pose estimation) å’Œå®¤å†…å®šä½ (indoor localization) ä»»åŠ¡ã€‚å®éªŒç»“æœä¸ä»…éªŒè¯äº†ä¸Šè¿°ç†è®ºæ¨æ–­ï¼Œè¿˜å±•ç¤ºäº†åœ¨ä½ç¡¬ä»¶æˆæœ¬ä¸‹å®ç°é«˜æ€§èƒ½å®æ—¶æ„ŸçŸ¥çš„æ½œåŠ›ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "physics.ins-det"
      ],
      "primary_category": "eess.SP",
      "comment": "13 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.02845v2",
      "published_date": "2025-10-21 07:31:24 UTC",
      "updated_date": "2025-12-26 11:22:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:50:58.517559+00:00"
    },
    {
      "arxiv_id": "2510.18348v1",
      "title": "PGTT: Phase-Guided Terrain Traversal for Perceptive Legged Locomotion",
      "title_zh": "PGTTï¼šé¢å‘æ„ŸçŸ¥è¶³å¼è¿åŠ¨çš„ç›¸ä½å¼•å¯¼åœ°å½¢ç©¿è¶Š",
      "authors": [
        "Alexandros Ntagkas",
        "Chairi Kiourt",
        "Konstantinos Chatzilygeroudis"
      ],
      "abstract": "State-of-the-art perceptive Reinforcement Learning controllers for legged robots either (i) impose oscillator or IK-based gait priors that constrain the action space, add bias to the policy optimization and reduce adaptability across robot morphologies, or (ii) operate \"blind\", which struggle to anticipate hind-leg terrain, and are brittle to noise. In this paper, we propose Phase-Guided Terrain Traversal (PGTT), a perception-aware deep-RL approach that overcomes these limitations by enforcing gait structure purely through reward shaping, thereby reducing inductive bias in policy learning compared to oscillator/IK-conditioned action priors. PGTT encodes per-leg phase as a cubic Hermite spline that adapts swing height to local heightmap statistics and adds a swing-phase contact penalty, while the policy acts directly in joint space supporting morphology-agnostic deployment. Trained in MuJoCo (MJX) on procedurally generated stair-like terrains with curriculum and domain randomization, PGTT achieves the highest success under push disturbances (median +7.5% vs. the next best method) and on discrete obstacles (+9%), with comparable velocity tracking, and converging to an effective policy roughly 2x faster than strong end-to-end baselines. We validate PGTT on a Unitree Go2 using a real-time LiDAR elevation-to-heightmap pipeline, and we report preliminary results on ANYmal-C obtained with the same hyperparameters. These findings indicate that terrain-adaptive, phase-guided reward shaping is a simple and general mechanism for robust perceptive locomotion across platforms.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PGTTï¼ˆPhase-Guided Terrain Traversalï¼‰ï¼Œæ—¨åœ¨è§£å†³è¶³å¼æœºå™¨äººæ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ§åˆ¶å™¨ä¸­åŠ¨ä½œç©ºé—´å—é™åŠåœ°å½¢é¢„æµ‹èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚PGTT é‡‡ç”¨ä¸€ç§æ„ŸçŸ¥æ„ŸçŸ¥çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep-RL)æ¶æ„ï¼Œé€šè¿‡å¥–åŠ±å¡‘é€ (Reward Shaping)è€Œéé¢„è®¾æ­¥æ€å…ˆéªŒæ¥çº¦æŸè¿åŠ¨ï¼Œæœ‰æ•ˆé™ä½äº†ç­–ç•¥å­¦ä¹ çš„å½’çº³åå·®(Inductive Bias)ã€‚è¯¥æ–¹æ³•å°†æ¯æ¡è…¿çš„ç›¸ä½ç¼–ç ä¸ºä¸‰æ¬¡åŸƒå°”ç±³ç‰¹æ ·æ¡(Cubic Hermite Spline)ï¼Œä½¿å…¶èƒ½æ ¹æ®å±€éƒ¨é«˜åº¦å›¾ç»Ÿè®¡è‡ªé€‚åº”è°ƒæ•´æ‘†åŠ¨é«˜åº¦ï¼Œå¹¶æ”¯æŒåœ¨å…³èŠ‚ç©ºé—´è¿›è¡Œè·¨å½¢æ€éƒ¨ç½²ã€‚åœ¨MuJoCo (MJX)ä»¿çœŸå®éªŒä¸­ï¼ŒPGTT åœ¨å¤æ‚åœ°å½¢ä¸‹çš„æˆåŠŸç‡æ¯”æ¬¡ä¼˜æ–¹æ³•æå‡äº†7.5%è‡³9%ï¼Œä¸”æ”¶æ•›é€Ÿåº¦æå‡äº†çº¦2å€ã€‚é€šè¿‡åœ¨Unitree Go2å’ŒANYmal-Cæœºå™¨äººä¸Šçš„å®éªŒéªŒè¯ï¼Œè¯æ˜äº†è¿™ç§ç›¸ä½å¼•å¯¼çš„å¥–åŠ±å¡‘é€ æœºåˆ¶æ˜¯å®ç°è·¨å¹³å°é²æ£’æ„ŸçŸ¥è¿åŠ¨çš„ä¸€ç§é€šç”¨ä¸”æœ‰æ•ˆçš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 9 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.18348v1",
      "published_date": "2025-10-21 07:00:18 UTC",
      "updated_date": "2025-10-21 07:00:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:51:16.659011+00:00"
    },
    {
      "arxiv_id": "2510.18918v1",
      "title": "Misinformation Detection using Large Language Models with Explainability",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§è™šå‡ä¿¡æ¯æ£€æµ‹",
      "authors": [
        "Jainee Patel",
        "Chintan Bhatt",
        "Himani Trivedi",
        "Thanh Thi Nguyen"
      ],
      "abstract": "The rapid spread of misinformation on online platforms undermines trust among individuals and hinders informed decision making. This paper shows an explainable and computationally efficient pipeline to detect misinformation using transformer-based pretrained language models (PLMs). We optimize both RoBERTa and DistilBERT using a two-step strategy: first, we freeze the backbone and train only the classification head; then, we progressively unfreeze the backbone layers while applying layer-wise learning rate decay. On two real-world benchmark datasets, COVID Fake News and FakeNewsNet GossipCop, we test the proposed approach with a unified protocol of preprocessing and stratified splits. To ensure transparency, we integrate the Local Interpretable Model-Agnostic Explanations (LIME) at the token level to present token-level rationales and SHapley Additive exPlanations (SHAP) at the global feature attribution level. It demonstrates that DistilBERT achieves accuracy comparable to RoBERTa while requiring significantly less computational resources. This work makes two key contributions: (1) it quantitatively shows that a lightweight PLM can maintain task performance while substantially reducing computational cost, and (2) it presents an explainable pipeline that retrieves faithful local and global justifications without compromising performance. The results suggest that PLMs combined with principled fine-tuning and interpretability can be an effective framework for scalable, trustworthy misinformation detection.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäº transformer é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ (PLMs) çš„å¯è§£é‡Šä¸”é«˜æ•ˆçš„è¯¯å¯¼ä¿¡æ¯æ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨åº”å¯¹ç¤¾äº¤åª’ä½“ä¸Šè™šå‡ä¿¡æ¯çš„ä¼ æ’­æŒ‘æˆ˜ã€‚ä½œè€…é’ˆå¯¹ RoBERTa å’Œ DistilBERT æ¨¡å‹é‡‡ç”¨äº†ä¸€ç§ä¸¤æ­¥ä¼˜åŒ–ç­–ç•¥ï¼Œé€šè¿‡å…ˆå†»ç»“åˆ†ç±»å¤´å†é€æ­¥è§£å†» backbone å±‚å¹¶é…åˆé€å±‚å­¦ä¹ ç‡è¡°å‡ (layer-wise learning rate decay) çš„æ–¹å¼è¿›è¡Œå¾®è°ƒã€‚ä¸ºäº†ç¡®ä¿æ£€æµ‹è¿‡ç¨‹çš„é€æ˜åº¦ï¼Œè¯¥æ¡†æ¶é›†æˆäº†å±€éƒ¨å¯è§£é‡Šæ¨¡å‹æ— å…³è§£é‡Š (LIME) è¿›è¡Œè¯å…ƒçº§åˆ†æï¼Œå¹¶åˆ©ç”¨ SHapley Additive exPlanations (SHAP) æä¾›å…¨å±€ç‰¹å¾å½’å› ã€‚åœ¨ COVID Fake News å’Œ FakeNewsNet GossipCop æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒDistilBERT èƒ½ä»¥æä½çš„è®¡ç®—èµ„æºæ¶ˆè€—è¾¾åˆ°ä¸ RoBERTa ç›¸å½“çš„æ£€æµ‹å‡†ç¡®ç‡ã€‚è¯¥å·¥ä½œä¸ä»…é‡åŒ–å±•ç¤ºäº†è½»é‡çº§ PLMs åœ¨ç»´æŒæ€§èƒ½æ–¹é¢çš„æ½œåŠ›ï¼Œè¿˜é€šè¿‡å¿ å®çš„å±€éƒ¨å’Œå…¨å±€è§£é‡Šæœºåˆ¶ï¼Œä¸ºæ„å»ºå¯æ‰©å±•ã€å¯ä¿¡èµ–çš„è¯¯å¯¼ä¿¡æ¯æ£€æµ‹ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for publication in the Proceedings of the 8th International Conference on Algorithms, Computing and Artificial Intelligence (ACAI 2025)",
      "pdf_url": "https://arxiv.org/pdf/2510.18918v1",
      "published_date": "2025-10-21 06:56:45 UTC",
      "updated_date": "2025-10-21 06:56:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:51:02.858297+00:00"
    },
    {
      "arxiv_id": "2510.18342v1",
      "title": "ShortcutBreaker: Low-Rank Noisy Bottleneck with Global Perturbation Attention for Multi-Class Unsupervised Anomaly Detection",
      "title_zh": "ShortcutBreakerï¼šåŸºäºå…¨å±€æ‰°åŠ¨æ³¨æ„åŠ›ä¸ä½ç§©å™ªå£°ç“¶é¢ˆçš„å¤šç±»åˆ«æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Peng Tang",
        "Xiaoxiao Yan",
        "Xiaobin Hu",
        "Yuning Cui",
        "Donghao Luo",
        "Jiangning Zhang",
        "Pengcheng Xu",
        "Jinlong Peng",
        "Qingdong He",
        "Feiyue Huang",
        "Song Xue",
        "Tobias Lasser"
      ],
      "abstract": "Multi-class unsupervised anomaly detection (MUAD) has garnered growing research interest, as it seeks to develop a unified model for anomaly detection across multiple classes, i.e., eliminating the need to train separate models for distinct objects and thereby saving substantial computational resources. Under the MUAD setting, while advanced Transformer-based architectures have brought significant performance improvements, identity shortcuts persist: they directly copy inputs to outputs, narrowing the gap in reconstruction errors between normal and abnormal cases, and thereby making the two harder to distinguish. Therefore, we propose ShortcutBreaker, a novel unified feature-reconstruction framework for MUAD tasks, featuring two key innovations to address the issue of shortcuts. First, drawing on matrix rank inequality, we design a low-rank noisy bottleneck (LRNB) to project highdimensional features into a low-rank latent space, and theoretically demonstrate its capacity to prevent trivial identity reproduction. Second, leveraging ViTs global modeling capability instead of merely focusing on local features, we incorporate a global perturbation attention to prevent information shortcuts in the decoders. Extensive experiments are performed on four widely used anomaly detection benchmarks, including three industrial datasets (MVTec-AD, ViSA, and Real-IAD) and one medical dataset (Universal Medical). The proposed method achieves a remarkable image-level AUROC of 99.8%, 98.9%, 90.6%, and 87.8% on these four datasets, respectively, consistently outperforming previous MUAD methods across different scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šç±»åˆ«æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹(MUAD)ä¸­Transformeræ¶æ„å­˜åœ¨çš„æ’ç­‰å¿«æ·è¿æ¥(identity shortcuts)é—®é¢˜ï¼Œæå‡ºäº†åä¸ºShortcutBreakerçš„ç»Ÿä¸€ç‰¹å¾é‡å»ºæ¡†æ¶ã€‚ä¸ºäº†é˜²æ­¢æ¨¡å‹ç›´æ¥å¤åˆ¶è¾“å…¥å¯¼è‡´éš¾ä»¥åŒºåˆ†æ­£å¸¸ä¸å¼‚å¸¸æ ·æœ¬ï¼Œè¯¥æ–¹æ³•è®¾è®¡äº†ä½ç§©å™ªå£°ç“¶é¢ˆ(LRNB)ï¼Œåˆ©ç”¨çŸ©é˜µç§©ä¸ç­‰å¼ç†è®ºå°†é«˜ç»´ç‰¹å¾æŠ•å½±åˆ°ä½ç§©æ½œç©ºé—´ï¼Œä»ç†è®ºä¸Šæœç»äº†å¹³å‡¡çš„æ’ç­‰å¤ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†å…¨å±€æ‰°åŠ¨æ³¨æ„åŠ›(global perturbation attention)ï¼Œåˆ©ç”¨ViTçš„å…¨å±€å»ºæ¨¡èƒ½åŠ›æ¥é˜²æ­¢è§£ç å™¨ä¸­çš„ä¿¡æ¯å¿«æ·è·¯å¾„ã€‚åœ¨MVTec-ADã€ViSAã€Real-IADåŠUniversal Medicalå››ä¸ªä¸»æµæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒShortcutBreakeråœ¨ä¸åŒåœºæ™¯ä¸‹å‡ä¼˜äºç°æœ‰çš„MUADæ–¹æ³•ã€‚è¯¥æ¨¡å‹åœ¨ä¸Šè¿°æ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†99.8%ã€98.9%ã€90.6%å’Œ87.8%çš„å›¾åƒçº§AUROCï¼Œè¯æ˜äº†å…¶åœ¨å·¥ä¸šå’ŒåŒ»ç–—å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Under Review",
      "pdf_url": "https://arxiv.org/pdf/2510.18342v1",
      "published_date": "2025-10-21 06:51:30 UTC",
      "updated_date": "2025-10-21 06:51:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:51:08.791212+00:00"
    },
    {
      "arxiv_id": "2510.18328v1",
      "title": "Scalable, Explainable and Provably Robust Anomaly Detection with One-Step Flow Matching",
      "title_zh": "åŸºäºå•æ­¥æµåŒ¹é…çš„å¯æ‰©å±•ã€å¯è§£é‡Šä¸”å…·å¤‡å¯è¯æ˜é²æ£’æ€§çš„å¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Zhong Li",
        "Qi Huang",
        "Yuxuan Zhu",
        "Lincen Yang",
        "Mohammad Mohammadi Amiri",
        "Niki van Stein",
        "Matthijs van Leeuwen"
      ],
      "abstract": "We introduce Time-Conditioned Contraction Matching (TCCM), a novel method for semi-supervised anomaly detection in tabular data. TCCM is inspired by flow matching, a recent generative modeling framework that learns velocity fields between probability distributions and has shown strong performance compared to diffusion models and generative adversarial networks. Instead of directly applying flow matching as originally formulated, TCCM builds on its core idea -- learning velocity fields between distributions -- but simplifies the framework by predicting a time-conditioned contraction vector toward a fixed target (the origin) at each sampled time step. This design offers three key advantages: (1) a lightweight and scalable training objective that removes the need for solving ordinary differential equations during training and inference; (2) an efficient scoring strategy called one time-step deviation, which quantifies deviation from expected contraction behavior in a single forward pass, addressing the inference bottleneck of existing continuous-time models such as DTE (a diffusion-based model with leading anomaly detection accuracy but heavy inference cost); and (3) explainability and provable robustness, as the learned velocity field operates directly in input space, making the anomaly score inherently feature-wise attributable; moreover, the score function is Lipschitz-continuous with respect to the input, providing theoretical guarantees under small perturbations. Extensive experiments on the ADBench benchmark show that TCCM strikes a favorable balance between detection accuracy and inference cost, outperforming state-of-the-art methods -- especially on high-dimensional and large-scale datasets. The source code is available at our GitHub repository.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Time-Conditioned Contraction Matching (TCCM)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹è¡¨æ ¼æ•°æ®åŠç›‘ç£å¼‚å¸¸æ£€æµ‹ (semi-supervised anomaly detection) çš„åˆ›æ–°æ–¹æ³•ã€‚TCCM å—åˆ° Flow Matching ç”Ÿæˆæ¨¡å‹çš„å¯å‘ï¼Œé€šè¿‡åœ¨æ¯ä¸ªé‡‡æ ·æ—¶é—´æ­¥é¢„æµ‹å‘å›ºå®šç›®æ ‡æ”¶ç¼©çš„å‘é‡ (time-conditioned contraction vector) æ¥ç®€åŒ–åŸæœ‰æ¡†æ¶ã€‚è¯¥è®¾è®¡æ¶ˆé™¤äº†è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­æ±‚è§£å¸¸å¾®åˆ†æ–¹ç¨‹ (Ordinary Differential Equations) çš„éœ€æ±‚ï¼Œå®ç°äº†è½»é‡åŒ–ä¸”å¯æ‰©å±•çš„è®­ç»ƒè¿‡ç¨‹ã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§å•æ­¥åå·® (one time-step deviation) è¯„åˆ†ç­–ç•¥ï¼Œé€šè¿‡å•æ¬¡å‰å‘ä¼ æ’­é‡åŒ–å¼‚å¸¸ç¨‹åº¦ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹åœ¨æ¨ç†ç«¯çš„æ•ˆç‡ç“¶é¢ˆã€‚ç”±äºå…¶é€Ÿåº¦åœºç›´æ¥åœ¨è¾“å…¥ç©ºé—´æ“ä½œï¼Œè¯¥æ–¹æ³•å…·å¤‡ç‰¹å¾ç»´åº¦çš„å¯è§£é‡Šæ€§ (explainability)ï¼Œä¸”æ»¡è¶³ Lipschitz-continuous è¿ç»­æ€§ï¼Œä¸ºæŠ—å¾®å°æ‰°åŠ¨çš„é²æ£’æ€§æä¾›äº†ç†è®ºä¿è¯ã€‚åœ¨ ADBench åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜ï¼ŒTCCM åœ¨æ£€æµ‹ç²¾åº¦ä¸æ¨ç†æˆæœ¬ä¹‹é—´å–å¾—äº†ä¼˜å¼‚å¹³è¡¡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é«˜ç»´å’Œå¤§è§„æ¨¡æ•°æ®é›†æ—¶è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Paper accepted by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.18328v1",
      "published_date": "2025-10-21 06:26:38 UTC",
      "updated_date": "2025-10-21 06:26:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:51:19.757725+00:00"
    },
    {
      "arxiv_id": "2511.01867v1",
      "title": "DiffPace: Diffusion-based Plug-and-play Augmented Channel Estimation in mmWave and Terahertz Ultra-Massive MIMO Systems",
      "title_zh": "DiffPaceï¼šæ¯«ç±³æ³¢ä¸å¤ªèµ«å…¹è¶…å¤§è§„æ¨¡ MIMO ç³»ç»Ÿä¸­åŸºäºæ‰©æ•£æ¨¡å‹çš„å³æ’å³ç”¨å¢å¼ºå‹ä¿¡é“ä¼°è®¡",
      "authors": [
        "Zhengdong Hu",
        "Chong Han",
        "Wolfgang Gerstacker",
        "Robert Schober"
      ],
      "abstract": "Millimeter-wave (mmWave) and Terahertz (THz)-band communications hold great promise in meeting the growing data-rate demands of next-generation wireless networks, offering abundant bandwidth. To mitigate the severe path loss inherent to these high frequencies and reduce hardware costs, ultra-massive multiple-input multiple-output (UM-MIMO) systems with hybrid beamforming architectures can deliver substantial beamforming gains and enhanced spectral efficiency. However, accurate channel estimation (CE) in mmWave and THz UM-MIMO systems is challenging due to high channel dimensionality and compressed observations from a limited number of RF chains, while the hybrid near- and far-field radiation patterns, arising from large array apertures and high carrier frequencies, further complicate CE. Conventional compressive sensing based frameworks rely on predefined sparsifying matrices, which cannot faithfully capture the hybrid near-field and far-field channel structures, leading to degraded estimation performance. This paper introduces DiffPace, a diffusion-based plug-and-play method for channel estimation. DiffPace uses a diffusion model (DM) to capture the channel distribution based on the hybrid spherical and planar-wave (HPSM) model. By applying the plug-and-play approach, it leverages the DM as prior knowledge, improving CE accuracy. Moreover, DM performs inference by solving an ordinary differential equation, minimizing the number of required inference steps compared with stochastic sampling method. Experimental results show that DiffPace achieves competitive CE performance, attaining -15 dB normalized mean square error (NMSE) at a signal-to-noise ratio (SNR) of 10 dB, with 90\\% fewer inference steps compared to state-of-the-art schemes, simultaneously providing high estimation precision and enhanced computational efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¯«ç±³æ³¢(mmWave)å’Œå¤ªèµ«å…¹(THz)è¶…å¤§è§„æ¨¡å¤šè¾“å…¥å¤šè¾“å‡º(UM-MIMO)ç³»ç»Ÿä¸­çš„ä¿¡é“ä¼°è®¡(Channel Estimation)æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºDiffPaceçš„æ‰©æ•£æ¨¡å‹(Diffusion-based)å³æ’å³ç”¨(Plug-and-play)å¢å¼ºæ–¹æ³•ã€‚ä¼ ç»Ÿçš„å‹ç¼©æ„ŸçŸ¥(Compressive Sensing)æ¡†æ¶éš¾ä»¥å‡†ç¡®æ•æ‰æ··åˆè¿‘åœºå’Œè¿œåœºä¿¡é“ç»“æ„ï¼Œå¯¼è‡´åœ¨é«˜ç»´ä¿¡é“å’Œå—é™å°„é¢‘é“¾è·¯ä¸‹çš„ä¼°è®¡æ€§èƒ½ä¸‹é™ã€‚DiffPaceåˆ©ç”¨æ‰©æ•£æ¨¡å‹(DM)å­¦ä¹ åŸºäºæ··åˆçƒé¢æ³¢å’Œå¹³é¢æ³¢æ¨¡å‹(HPSM)çš„ä¿¡é“åˆ†å¸ƒï¼Œå¹¶å°†å…¶ä½œä¸ºå…ˆéªŒçŸ¥è¯†åµŒå…¥å³æ’å³ç”¨æ¶æ„ä¸­ä»¥æå‡ä¼°è®¡ç²¾åº¦ã€‚ä¸ºäº†æé«˜è®¡ç®—æ•ˆç‡ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ±‚è§£å¸¸å¾®åˆ†æ–¹ç¨‹(Ordinary Differential Equation)è¿›è¡Œæ¨ç†ï¼Œæ˜¾è‘—å‡å°‘äº†æ‰€éœ€çš„æ¨ç†æ­¥æ•°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDiffPaceåœ¨ä¿¡å™ªæ¯”(SNR)ä¸º10 dBæ—¶è¾¾åˆ°äº†-15 dBçš„å½’ä¸€åŒ–å‡æ–¹è¯¯å·®(NMSE)ã€‚ä¸ç°æœ‰å…ˆè¿›æ–¹æ¡ˆç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ä¿è¯é«˜ç²¾åº¦ä¼°è®¡çš„åŒæ—¶å‡å°‘äº†90%çš„æ¨ç†æ­¥éª¤ï¼Œå®ç°äº†æ€§èƒ½ä¸æ•ˆç‡çš„å¹³è¡¡ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01867v1",
      "published_date": "2025-10-21 06:22:24 UTC",
      "updated_date": "2025-10-21 06:22:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:51:12.643407+00:00"
    },
    {
      "arxiv_id": "2510.18915v3",
      "title": "UNO-Bench: A Unified Benchmark for Exploring the Compositional Law Between Uni-modal and Omni-modal in Omni Models",
      "title_zh": "UNO-Benchï¼šæ¢ç©¶å…¨æ¨¡æ€æ¨¡å‹ä¸­å•æ¨¡æ€ä¸å…¨æ¨¡æ€ç»„åˆè§„å¾‹çš„ç»Ÿä¸€åŸºå‡†",
      "authors": [
        "Chen Chen",
        "ZeYang Hu",
        "Fengjiao Chen",
        "Liya Ma",
        "Jiaxing Liu",
        "Xiaoyu Li",
        "Ziwen Wang",
        "Xuezhi Cao",
        "Xunliang Cai"
      ],
      "abstract": "Multimodal Large Languages models have been progressing from uni-modal understanding toward unifying visual, audio and language modalities, collectively termed omni models. However, the correlation between uni-modal and omni-modal remains unclear, which requires comprehensive evaluation to drive omni model's intelligence evolution. In this work, we introduce a novel, high-quality, and UNified Omni model benchmark, UNO-Bench. This benchmark is designed to effectively evaluate both UNi-modal and Omni-modal capabilities under a unified ability taxonomy, spanning 44 task types and 5 modality combinations. It includes 1250 human curated samples for omni-modal with 98% cross-modality solvability, and 2480 enhanced uni-modal samples. The human-generated dataset is well-suited to real-world scenarios, particularly within the Chinese context, whereas the automatically compressed dataset offers a 90% increase in speed and maintains 98% consistency across 18 public benchmarks. In addition to traditional multi-choice questions, we propose an innovative multi-step open-ended question format to assess complex reasoning. A general scoring model is incorporated, supporting 6 question types for automated evaluation with 95% accuracy. Experimental result shows the Compositional Law between omni-modal and uni-modal performance and the omni-modal capability manifests as a bottleneck effect on weak models, while exhibiting synergistic promotion on strong models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨æ¨¡æ€(omni-modal)å¤§æ¨¡å‹ä¸­å•æ¨¡æ€(uni-modal)ä¸å…¨æ¨¡æ€èƒ½åŠ›å…³è”ä¸æ˜çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºUNO-Benchçš„æ–°å‹é«˜è´¨é‡ç»Ÿä¸€åŸºå‡†ã€‚UNO-Benchæ—¨åœ¨ç»Ÿä¸€çš„èƒ½åŠ›åˆ†ç±»ä½“ç³»ä¸‹è¯„ä¼°44ç§ä»»åŠ¡ç±»å‹å’Œ5ç§æ¨¡æ€ç»„åˆï¼ŒåŒ…å«1250ä¸ªç”±äººå·¥ç­–åˆ’çš„é«˜è´¨é‡å…¨æ¨¡æ€æ ·æœ¬å’Œ2480ä¸ªå¢å¼ºçš„å•æ¨¡æ€æ ·æœ¬ã€‚è¯¥åŸºå‡†ä¸ä»…æ¶µç›–ä¼ ç»Ÿå¤šé€‰é¢˜ï¼Œè¿˜å¼•å…¥äº†åˆ›æ–°çš„å¤šæ­¥å¼€æ”¾å¼é—®é¢˜æ ¼å¼ä»¥è¯„ä¼°å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œå¹¶é…å¤‡äº†æ”¯æŒ6ç§é—®é¢˜ç±»å‹ä¸”å‡†ç¡®ç‡è¾¾95%çš„è‡ªåŠ¨è¯„åˆ†æ¨¡å‹ã€‚å®éªŒç»“æœæ­ç¤ºäº†å…¨æ¨¡æ€ä¸å•æ¨¡æ€è¡¨ç°ä¹‹é—´çš„ç»„åˆå®šå¾‹(Compositional Law)ï¼Œå‘ç°å…¨æ¨¡æ€èƒ½åŠ›åœ¨å¼±æ¨¡å‹ä¸Šè¡¨ç°ä¸ºç“¶é¢ˆæ•ˆåº”ï¼Œè€Œåœ¨å¼ºæ¨¡å‹ä¸Šåˆ™å±•ç°å‡ºååŒä¿ƒè¿›ä½œç”¨ã€‚è¿™ä¸€å·¥ä½œä¸ºç†è§£å’Œæ¨åŠ¨å…¨æ¨¡æ€æ¨¡å‹çš„æ™ºèƒ½æ¼”è¿›æä¾›äº†ç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶èƒ½æœ‰æ•ˆæ¨¡æ‹ŸçœŸå®ä¸–ç•ŒåŠä¸­æ–‡è¯­å¢ƒä¸‹çš„å¤æ‚åœºæ™¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "v3: Switch the paper template. Work in progress. Github: https://github.com/meituan-longcat/UNO-Bench Hugging Face: https://huggingface.co/datasets/meituan-longcat/UNO-Bench",
      "pdf_url": "https://arxiv.org/pdf/2510.18915v3",
      "published_date": "2025-10-21 06:14:40 UTC",
      "updated_date": "2025-10-30 10:00:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:51:19.058579+00:00"
    },
    {
      "arxiv_id": "2510.18914v1",
      "title": "Context-aware Fairness Evaluation and Mitigation in LLMs",
      "title_zh": "LLM ä¸­ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å…¬å¹³æ€§è¯„ä¼°ä¸ç¼“è§£",
      "authors": [
        "Afrozah Nadeem",
        "Mark Dras",
        "Usman Naseem"
      ],
      "abstract": "Large language models often display undesirable behaviors embedded in their internal representations, undermining fairness, inconsistency drift, amplification of harmful content, and the propagation of unwanted patterns during extended dialogue and conversations. Although training-time or data-centric methods attempt to reduce these effects, they are computationally expensive, irreversible once deployed, and slow to adapt to new conversational contexts. Pruning-based methods provide a flexible and transparent way to reduce bias by adjusting the neurons responsible for certain behaviors. However, most existing approaches are static; once a neuron is removed, the model loses the ability to adapt when the conversation or context changes. To address this, we propose a dynamic, reversible, pruning-based framework that detects context-aware neuron activations and applies adaptive masking to modulate their influence during generation. Our inference-time solution provides fine-grained, memory-aware mitigation with knowledge-preserved, more coherent behavior across multilingual single- and multi-turn dialogues, enabling dynamic fairness control in real-world conversational AI.",
      "tldr_zh": "å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å¸¸å› å†…éƒ¨è¡¨å¾å­˜åœ¨ä¸å…¬å¹³ã€ä¸ä¸€è‡´å’Œæœ‰å®³å†…å®¹æ”¾å¤§ç­‰é—®é¢˜ï¼Œè€Œä¼ ç»Ÿçš„è®­ç»ƒæˆ–æ•°æ®ä¸­å¿ƒåŒ–æ–¹æ³•æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥é€†è½¬ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŠ¨æ€ã€å¯é€†çš„å‰ªæ(pruning-based)æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å…¬å¹³æ€§è¯„ä¼°ä¸ç¼“è§£ã€‚è¯¥æ¡†æ¶é€šè¿‡æ£€æµ‹ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç¥ç»å…ƒæ¿€æ´»æƒ…å†µï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µåº”ç”¨è‡ªé€‚åº”æ©ç (adaptive masking)æ¥è°ƒèŠ‚å…¶å½±å“ã€‚ä¸é™æ€å‰ªææ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ¡ˆå…è®¸æ¨¡å‹åœ¨å¯¹è¯ç¯å¢ƒå˜åŒ–æ—¶è¿›è¡ŒåŠ¨æ€è°ƒæ•´ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨ä¸åŒè¯­å¢ƒä¸‹çš„çµæ´»æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šè¯­è¨€çš„å•è½®å’Œå¤šè½®å¯¹è¯ä¸­æä¾›äº†ç»†ç²’åº¦ã€å†…å­˜æ„ŸçŸ¥çš„åå·®ç¼“è§£ï¼ŒåŒæ—¶æœ‰æ•ˆä¿ç•™äº†æ¨¡å‹åŸæœ‰çŸ¥è¯†ã€‚è¿™ç§æ¨ç†æ—¶(inference-time)è§£å†³æ–¹æ¡ˆä¸ºç°å®ä¸–ç•Œå¯¹è¯å¼äººå·¥æ™ºèƒ½çš„åŠ¨æ€å…¬å¹³æ€§æ§åˆ¶æä¾›äº†å¯è¡Œçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "PrePrint",
      "pdf_url": "https://arxiv.org/pdf/2510.18914v1",
      "published_date": "2025-10-21 06:13:17 UTC",
      "updated_date": "2025-10-21 06:13:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:51:21.365743+00:00"
    },
    {
      "arxiv_id": "2510.18318v3",
      "title": "Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning",
      "title_zh": "Earth AIï¼šåŸºäºåŸºç¡€æ¨¡å‹ä¸è·¨æ¨¡æ€æ¨ç†çš„åœ°ç†ç©ºé—´æ´å¯Ÿæ·±åº¦æŒ–æ˜",
      "authors": [
        "Aaron Bell",
        "Amit Aides",
        "Amr Helmy",
        "Arbaaz Muslim",
        "Aviad Barzilai",
        "Aviv Slobodkin",
        "Bolous Jaber",
        "David Schottlander",
        "George Leifman",
        "Joydeep Paul",
        "Mimi Sun",
        "Nadav Sherman",
        "Natalie Williams",
        "Per Bjornsson",
        "Roy Lee",
        "Ruth Alcantara",
        "Thomas Turnbull",
        "Tomer Shekel",
        "Vered Silverman",
        "Yotam Gigi",
        "Adam Boulanger",
        "Alex Ottenwess",
        "Ali Ahmadalipour",
        "Anna Carter",
        "Behzad Vahedi",
        "Charles Elliott",
        "David Andre",
        "Elad Aharoni",
        "Gia Jung",
        "Hassler Thurston",
        "Jacob Bien",
        "Jamie McPike",
        "Jessica Sapick",
        "Juliet Rothenberg",
        "Kartik Hegde",
        "Kel Markert",
        "Kim Philipp Jablonski",
        "Luc Houriez",
        "Monica Bharel",
        "Phing VanLee",
        "Reuven Sayag",
        "Sebastian Pilarski",
        "Shelley Cazares",
        "Shlomi Pasternak",
        "Siduo Jiang",
        "Thomas Colthurst",
        "Yang Chen",
        "Yehonathan Refael",
        "Yochai Blau",
        "Yuval Carny",
        "Yael Maguire",
        "Avinatan Hassidim",
        "James Manyika",
        "Tim Thelin",
        "Genady Beryozkin",
        "Gautam Prasad",
        "Luke Barrington",
        "Yossi Matias",
        "Niv Efron",
        "Shravya Shetty"
      ],
      "abstract": "Geospatial data offers immense potential for understanding our planet. However, the sheer volume and diversity of this data along with its varied resolutions, timescales, and sparsity pose significant challenges for thorough analysis and interpretation. This paper introduces Earth AI, a family of geospatial AI models and agentic reasoning that enables significant advances in our ability to unlock novel and profound insights into our planet. This approach is built upon foundation models across three key domains--Planet-scale Imagery, Population, and Environment--and an intelligent Gemini-powered reasoning engine. We present rigorous benchmarks showcasing the power and novel capabilities of our foundation models and validate that when used together, they provide complementary value for geospatial inference and their synergies unlock superior predictive capabilities. To handle complex, multi-step queries, we developed a Gemini-powered agent that jointly reasons over our multiple foundation models along with large geospatial data sources and tools. On a new benchmark of real-world crisis scenarios, our agent demonstrates the ability to deliver critical and timely insights, effectively bridging the gap between raw geospatial data and actionable understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Earth AIï¼Œè¿™æ˜¯ä¸€ç³»åˆ—åœ°ç†ç©ºé—´äººå·¥æ™ºèƒ½æ¨¡å‹å’ŒAgentic Reasoningç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³åœ°ç†ç©ºé—´æ•°æ®åœ¨è§„æ¨¡ã€å¤šæ ·æ€§å’Œç¨€ç–æ€§æ–¹é¢å¸¦æ¥çš„ä¸¥å³»æŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿæ„å»ºäºæ¶µç›–Planet-scale Imageryã€Populationå’ŒEnvironmentä¸‰ä¸ªå…³é”®é¢†åŸŸçš„Foundation Modelsä¹‹ä¸Šï¼Œå¹¶é›†æˆäº†é«˜æ€§èƒ½çš„Geminié©±åŠ¨æ¨ç†å¼•æ“ã€‚é€šè¿‡ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶éªŒè¯äº†è¿™äº›åŸºç¡€æ¨¡å‹åœ¨è”åˆä½¿ç”¨æ—¶å…·æœ‰æ˜¾è‘—çš„äº’è¡¥ä»·å€¼ï¼Œå…¶ååŒæ•ˆåº”èƒ½å¤Ÿè§£é”å“è¶Šçš„é¢„æµ‹èƒ½åŠ›ã€‚ä¸ºäº†åº”å¯¹å¤æ‚çš„å¤šæ­¥æŸ¥è¯¢ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªç”±Geminié©±åŠ¨çš„Agentï¼Œèƒ½å¤ŸåŒæ—¶å¯¹å¤šä¸ªåŸºç¡€æ¨¡å‹ã€æµ·é‡åœ°ç†ç©ºé—´æ•°æ®æºåŠå·¥å…·è¿›è¡Œè·¨æ¨¡æ€æ¨ç†ã€‚åœ¨é’ˆå¯¹çœŸå®ä¸–ç•Œå±æœºåœºæ™¯çš„æ–°åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥Agentå±•ç°äº†æä¾›å…³é”®ä¸”åŠæ—¶æ´å¯Ÿçš„èƒ½åŠ›ï¼ŒæˆåŠŸå®ç°äº†ä»åŸå§‹åœ°ç†ç©ºé—´æ•°æ®åˆ°å¯æ“ä½œç†è§£çš„æœ‰æ•ˆæ¡¥æ¥ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18318v3",
      "published_date": "2025-10-21 06:05:47 UTC",
      "updated_date": "2025-11-07 22:10:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:51:25.654297+00:00"
    },
    {
      "arxiv_id": "2510.18316v1",
      "title": "MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation",
      "title_zh": "MoMaGenï¼šè½¯ç¡¬çº¦æŸä¸‹çš„å¤šæ­¥åŒè‡‚ç§»åŠ¨æ“ä½œæ¼”ç¤ºç”Ÿæˆ",
      "authors": [
        "Chengshu Li",
        "Mengdi Xu",
        "Arpit Bahety",
        "Hang Yin",
        "Yunfan Jiang",
        "Huang Huang",
        "Josiah Wong",
        "Sujay Garlanka",
        "Cem Gokmen",
        "Ruohan Zhang",
        "Weiyu Liu",
        "Jiajun Wu",
        "Roberto MartÃ­n-MartÃ­n",
        "Li Fei-Fei"
      ],
      "abstract": "Imitation learning from large-scale, diverse human demonstrations has proven effective for training robots, but collecting such data is costly and time-consuming. This challenge is amplified for multi-step bimanual mobile manipulation, where humans must teleoperate both a mobile base and two high-degree-of-freedom arms. Prior automated data generation frameworks have addressed static bimanual manipulation by augmenting a few human demonstrations in simulation, but they fall short for mobile settings due to two key challenges: (1) determining base placement to ensure reachability, and (2) positioning the camera to provide sufficient visibility for visuomotor policies. To address these issues, we introduce MoMaGen, which formulates data generation as a constrained optimization problem that enforces hard constraints (e.g., reachability) while balancing soft constraints (e.g., visibility during navigation). This formulation generalizes prior approaches and provides a principled foundation for future methods. We evaluate MoMaGen on four multi-step bimanual mobile manipulation tasks and show that it generates significantly more diverse datasets than existing methods. Leveraging this diversity, MoMaGen can train successful imitation learning policies from a single source demonstration, and these policies can be fine-tuned with as few as 40 real-world demonstrations to achieve deployment on physical robotic hardware. More details are available at our project page: momagen.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MoMaGen æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ­¥åŒè‡‚ç§»åŠ¨æ“æ§ (multi-step bimanual mobile manipulation) ä¸­äººç±»ç¤ºæ•™æ•°æ®é‡‡é›†æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†æ•°æ®ç”Ÿæˆå½¢å¼åŒ–ä¸ºä¸€ä¸ªçº¦æŸä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡å¹³è¡¡ç¡¬çº¦æŸï¼ˆå¦‚ reachabilityï¼‰å’Œè½¯çº¦æŸï¼ˆå¦‚å¯¼èˆªè¿‡ç¨‹ä¸­çš„ visibilityï¼‰æ¥è§£å†³ç§»åŠ¨æœºå™¨äººåº•åº§æ”¾ç½®ä¸ç›¸æœºè§†è§’çš„ååŒä¼˜åŒ–éš¾é¢˜ã€‚MoMaGen æ³›åŒ–äº†ä»¥å¾€çš„è‡ªåŠ¨åŒ–æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œä¸ºæœªæ¥ç§»åŠ¨æ“æ§é¢†åŸŸçš„ç¤ºæ•™ç”Ÿæˆæä¾›äº†åŸåˆ™æ€§çš„åŸºç¡€ã€‚åœ¨å››é¡¹å¤æ‚ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„æ•°æ®é›†æ¯”ç°æœ‰æ–¹æ³•æ›´å…·å¤šæ ·æ€§ã€‚å‡­å€Ÿè¿™ç§å¤šæ ·æ€§ï¼ŒMoMaGen èƒ½å¤Ÿä»…ä»å•ä¸ªæºç¤ºæ•™ä¸­è®­ç»ƒå‡ºæˆåŠŸçš„ imitation learning ç­–ç•¥ã€‚æœ€åï¼Œå®éªŒè¯æ˜è¿™äº›ç­–ç•¥åœ¨ç»è¿‡å°‘è‡³ 40 æ¬¡çœŸå®ä¸–ç•Œç¤ºæ•™çš„å¾®è°ƒåï¼Œä¾¿å¯åœ¨ç‰©ç†æœºå™¨äººç¡¬ä»¶ä¸Šå®ç°é«˜æ•ˆéƒ¨ç½²ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project website: momagen.github.io. The first four authors contribute equally",
      "pdf_url": "https://arxiv.org/pdf/2510.18316v1",
      "published_date": "2025-10-21 05:56:47 UTC",
      "updated_date": "2025-10-21 05:56:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:51:28.150114+00:00"
    },
    {
      "arxiv_id": "2510.18913v7",
      "title": "ADPO: Anchored Direct Preference Optimization",
      "title_zh": "ADPOï¼šé”šå®šå¼ç›´æ¥åå¥½ä¼˜åŒ–",
      "authors": [
        "Wang Zixian"
      ],
      "abstract": "We present Anchored Direct Preference Optimization (ADPO), a policy alignment method derived from first principles of KL-regularized reinforcement learning. Unlike standard approaches that treat the reference policy merely as a regularizer, we show that the optimal policy in reinforcement learning from human feedback inherently operates in a differential coordinate system, optimizing relative advantage in the form of log ratios rather than absolute probabilities. ADPO explicitly parameterizes this optimal structure through anchored logits, effectively decoupling response quality from prior popularity and creating an implicit trust region through curvature scaling. We show that this formulation unifies supervised fine-tuning, reinforcement learning, and ranking-based objectives under a single geometric perspective. Theoretically, ADPO resolves the probability smearing problem of supervised fine-tuning while avoiding the mode-seeking instability characteristic of reverse-KL methods. Empirically, the listwise ranking variant of ADPO achieves state-of-the-art performance on reasoning tasks, outperforming GRPO by 30.9 percent on Qwen3-1.7B and demonstrating superior robustness under distribution shift.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Anchored Direct Preference Optimization (ADPO)ï¼Œè¿™æ˜¯ä¸€ç§åŸºäº KL-regularized reinforcement learning åŸºæœ¬åŸç†å¯¼å‡ºçš„ç­–ç•¥å¯¹é½æ–¹æ³•ã€‚ä¸åŒäºä»…å°†å‚è€ƒç­–ç•¥è§†ä¸ºæ­£åˆ™åŒ–é¡¹çš„ä¼ ç»Ÿæ–¹æ³•ï¼ŒADPO æ­ç¤ºäº†æœ€ä¼˜ç­–ç•¥æœ¬è´¨ä¸Šè¿è¡Œåœ¨å¾®åˆ†åæ ‡ç³»ä¸­ï¼Œé€šè¿‡ anchored logits å‚æ•°åŒ–æœ€ä¼˜ç»“æ„ï¼Œå®ç°äº†å“åº”è´¨é‡ä¸å…ˆéªŒåˆ†å¸ƒçš„è§£è€¦ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ curvature scaling å»ºç«‹äº†éšå¼ä¿¡ä»»åŒºåŸŸï¼Œä»å‡ ä½•è§†è§’ç»Ÿä¸€äº† supervised fine-tuning (SFT)ã€å¼ºåŒ–å­¦ä¹ å’ŒåŸºäºæ’åºçš„ç›®æ ‡å‡½æ•°ã€‚åœ¨ç†è®ºå±‚é¢ï¼ŒADPO è§£å†³äº† SFT ä¸­çš„ probability smearing é—®é¢˜ï¼Œå¹¶é¿å…äº† reverse-KL æ–¹æ³•ä¸­å¸¸è§çš„æ¨¡å¼å¯»æ±‚ (mode-seeking) ä¸ç¨³å®šæ€§ã€‚å®éªŒè¯æ˜ï¼ŒADPO çš„ listwise ranking å˜ä½“åœ¨æ¨ç†ä»»åŠ¡ä¸­è¾¾åˆ°äº† state-of-the-art æ€§èƒ½ï¼Œåœ¨ Qwen3-1.7B æ¨¡å‹ä¸Šæ¯” GRPO è¡¨ç°æå‡äº† 30.9%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†å¸ƒåç§» (distribution shift) ä¸‹å±•ç°å‡ºäº†å“è¶Šçš„é²æ£’æ€§ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹çš„ç­–ç•¥å¯¹é½æä¾›äº†æ›´ç¨³å®šçš„ç†è®ºæ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18913v7",
      "published_date": "2025-10-21 05:53:13 UTC",
      "updated_date": "2026-01-12 06:15:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:51:40.260300+00:00"
    },
    {
      "arxiv_id": "2510.18315v1",
      "title": "Higher Embedding Dimension Creates a Stronger World Model for a Simple Sorting Task",
      "title_zh": "å¢åŠ åµŒå…¥ç»´åº¦ä¸ºç®€å•æ’åºä»»åŠ¡æ„å»ºæ›´å¼ºå¤§çš„ä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Brady Bhalla",
        "Honglu Fan",
        "Nancy Chen",
        "Tony Yue YU"
      ],
      "abstract": "We investigate how embedding dimension affects the emergence of an internal \"world model\" in a transformer trained with reinforcement learning to perform bubble-sort-style adjacent swaps. Models achieve high accuracy even with very small embedding dimensions, but larger dimensions yield more faithful, consistent, and robust internal representations. In particular, higher embedding dimensions strengthen the formation of structured internal representation and lead to better interpretability. After hundreds of experiments, we observe two consistent mechanisms: (1) the last row of the attention weight matrix monotonically encodes the global ordering of tokens; and (2) the selected transposition aligns with the largest adjacent difference of these encoded values. Our results provide quantitative evidence that transformers build structured internal world models and that model size improves representation quality in addition to end performance. We release our metrics and analyses, which can be used to probe similar algorithmic tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)è®­ç»ƒçš„Transformeræ¨¡å‹ä¸­ï¼ŒEmbedding Dimensionï¼ˆåµŒå…¥ç»´åº¦ï¼‰å¦‚ä½•å½±å“å…¶æ‰§è¡Œç›¸é‚»äº¤æ¢æ’åºä»»åŠ¡æ—¶å†…éƒ¨â€œä¸–ç•Œæ¨¡å‹â€(World Model)çš„å½¢æˆã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶å³ä½¿åœ¨è¾ƒå°çš„Embedding Dimensionä¸‹æ¨¡å‹ä¹Ÿèƒ½è¾¾åˆ°å¾ˆé«˜çš„å‡†ç¡®ç‡ï¼Œä½†æ›´å¤§çš„ç»´åº¦èƒ½äº§ç”Ÿæ›´çœŸå®ã€ä¸€è‡´ä¸”ç¨³å¥çš„å†…éƒ¨è¡¨ç¤ºï¼Œå¹¶æ˜¾è‘—å¢å¼ºç»“æ„åŒ–å†…éƒ¨è¡¨ç¤ºçš„å½¢æˆå’Œæ¨¡å‹çš„å¯è§£é‡Šæ€§(Interpretability)ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œç ”ç©¶è€…è§‚å¯Ÿåˆ°ä¸¤ä¸ªä¸€è‡´çš„ç‰©ç†æœºåˆ¶ï¼šAttention Weightï¼ˆæ³¨æ„åŠ›æƒé‡ï¼‰çŸ©é˜µçš„æœ€åä¸€è¡Œå•è°ƒåœ°ç¼–ç äº†Tokençš„å…¨å±€é¡ºåºï¼Œä¸”æ‰€é€‰çš„ç½®æ¢æ“ä½œä¸è¿™äº›ç¼–ç å€¼çš„æœ€å¤§ç›¸é‚»å·®å€¼å¯¹é½ã€‚è¯¥ç»“æœä¸ºTransformeræ„å»ºç»“æ„åŒ–å†…éƒ¨ä¸–ç•Œæ¨¡å‹æä¾›äº†å®šé‡è¯æ®ï¼Œè¯æ˜æ¨¡å‹è§„æ¨¡é™¤äº†æå‡æœ€ç»ˆæ€§èƒ½å¤–ï¼Œè¿˜èƒ½æ˜¾è‘—æ”¹å–„å†…éƒ¨è¡¨ç¤ºè´¨é‡ã€‚è¯¥ç ”ç©¶è¿˜å‘å¸ƒäº†ç›¸å…³åº¦é‡æŒ‡æ ‡å’Œåˆ†ææ–¹æ³•ï¼Œå¯ç”¨äºæ¢æµ‹ç±»ä¼¼çš„ç®—æ³•ä»»åŠ¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18315v1",
      "published_date": "2025-10-21 05:51:02 UTC",
      "updated_date": "2025-10-21 05:51:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:51:47.153019+00:00"
    },
    {
      "arxiv_id": "2510.18314v1",
      "title": "Genesis: Evolving Attack Strategies for LLM Web Agent Red-Teaming",
      "title_zh": "Genesisï¼šé¢å‘ LLM Web æ™ºèƒ½ä½“çº¢é˜Ÿè¯„ä¼°çš„æ¼”åŒ–æ”»å‡»ç­–ç•¥",
      "authors": [
        "Zheng Zhang",
        "Jiarui He",
        "Yuchen Cai",
        "Deheng Ye",
        "Peilin Zhao",
        "Ruili Feng",
        "Hao Wang"
      ],
      "abstract": "As large language model (LLM) agents increasingly automate complex web tasks, they boost productivity while simultaneously introducing new security risks. However, relevant studies on web agent attacks remain limited. Existing red-teaming approaches mainly rely on manually crafted attack strategies or static models trained offline. Such methods fail to capture the underlying behavioral patterns of web agents, making it difficult to generalize across diverse environments. In web agent attacks, success requires the continuous discovery and evolution of attack strategies. To this end, we propose Genesis, a novel agentic framework composed of three modules: Attacker, Scorer, and Strategist. The Attacker generates adversarial injections by integrating the genetic algorithm with a hybrid strategy representation. The Scorer evaluates the target web agent's responses to provide feedback. The Strategist dynamically uncovers effective strategies from interaction logs and compiles them into a continuously growing strategy library, which is then re-deployed to enhance the Attacker's effectiveness. Extensive experiments across various web tasks show that our framework discovers novel strategies and consistently outperforms existing attack baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Genesisï¼Œä¸€ç§é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLM) Web æ™ºèƒ½ä½“çº¢é˜Ÿæµ‹è¯• (red-teaming) çš„æ–°å‹æ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ‰‹åŠ¨æˆ–é™æ€æ”»å‡»æ–¹æ³•éš¾ä»¥æ³›åŒ–å’Œæ•æ‰è¡Œä¸ºæ¨¡å¼çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ç”± Attackerã€Scorer å’Œ Strategist ä¸‰ä¸ªæ¨¡å—ç»„æˆï¼Œå…¶ä¸­ Attacker ç»“åˆé—ä¼ ç®—æ³• (genetic algorithm) ä¸æ··åˆç­–ç•¥è¡¨ç¤ºç”Ÿæˆå¯¹æŠ—æ³¨å…¥ï¼ŒScorer è´Ÿè´£è¯„ä¼°åé¦ˆï¼Œè€Œ Strategist åˆ™ä»äº¤äº’æ—¥å¿—ä¸­åŠ¨æ€æŒ–æ˜å¹¶è¿›åŒ–æ”»å‡»ç­–ç•¥ä»¥æ„å»ºæŒç»­å¢é•¿çš„ç­–ç•¥åº“ (strategy library)ã€‚Genesis é€šè¿‡é—­ç¯çš„ç­–ç•¥è¿›åŒ–æœºåˆ¶ï¼Œèƒ½å¤Ÿé’ˆå¯¹ä¸åŒç¯å¢ƒè‡ªåŠ¨å‘ç°æœ‰æ•ˆçš„æ”»å‡»æ‰‹æ®µã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§ Web ä»»åŠ¡ä¸­å‡èƒ½å‘ç°æ–°é¢–ç­–ç•¥ï¼Œä¸”æ”»å‡»æ•ˆæœæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºå‡† (baselines) æ–¹æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18314v1",
      "published_date": "2025-10-21 05:49:37 UTC",
      "updated_date": "2025-10-21 05:49:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:51:47.007188+00:00"
    },
    {
      "arxiv_id": "2510.18297v1",
      "title": "From Retrieval to Generation: Unifying External and Parametric Knowledge for Medical Question Answering",
      "title_zh": "ä»æ£€ç´¢åˆ°ç”Ÿæˆï¼šèåˆå¤–éƒ¨çŸ¥è¯†ä¸å‚æ•°åŒ–çŸ¥è¯†çš„åŒ»ç–—é—®ç­”",
      "authors": [
        "Lei Li",
        "Xiao Zhou",
        "Yingying Zhang",
        "Xian Wu"
      ],
      "abstract": "Medical question answering (QA) requires extensive access to domain-specific knowledge. A promising direction is to enhance large language models (LLMs) with external knowledge retrieved from medical corpora or parametric knowledge stored in model parameters. Existing approaches typically fall into two categories: Retrieval-Augmented Generation (RAG), which grounds model reasoning on externally retrieved evidence, and Generation-Augmented Generation (GAG), which depends solely on the models internal knowledge to generate contextual documents. However, RAG often suffers from noisy or incomplete retrieval, while GAG is vulnerable to hallucinated or inaccurate information due to unconstrained generation. Both issues can mislead reasoning and undermine answer reliability. To address these challenges, we propose MedRGAG, a unified retrieval-generation augmented framework that seamlessly integrates external and parametric knowledge for medical QA. MedRGAG comprises two key modules: Knowledge-Guided Context Completion (KGCC), which directs the generator to produce background documents that complement the missing knowledge revealed by retrieval; and Knowledge-Aware Document Selection (KADS), which adaptively selects an optimal combination of retrieved and generated documents to form concise yet comprehensive evidence for answer generation. Extensive experiments on five medical QA benchmarks demonstrate that MedRGAG achieves a 12.5% improvement over MedRAG and a 4.5% gain over MedGENIE, highlighting the effectiveness of unifying retrieval and generation for knowledge-intensive reasoning. Our code and data are publicly available at https://anonymous.4open.science/r/MedRGAG",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—é—®ç­”(Medical QA)ä¸­çŸ¥è¯†è·å–çš„å±€é™æ€§ï¼Œæå‡ºäº†åä¸ºMedRGAGçš„ç»Ÿä¸€æ£€ç´¢ä¸ç”Ÿæˆå¢å¼ºæ¡†æ¶ï¼Œæ—¨åœ¨èåˆå¤–éƒ¨æ£€ç´¢çŸ¥è¯†ä¸æ¨¡å‹å†…éƒ¨å‚æ•°çŸ¥è¯†ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šçŸ¥è¯†å¼•å¯¼ä¸Šä¸‹æ–‡è¡¥å…¨(KGCC)é€šè¿‡ç”ŸæˆèƒŒæ™¯æ–‡æ¡£è¡¥å……æ£€ç´¢ç¼ºå¤±ï¼Œè€ŒçŸ¥è¯†æ„ŸçŸ¥æ–‡æ¡£é€‰æ‹©(KADS)åˆ™è‡ªé€‚åº”ç­›é€‰æ£€ç´¢ä¸ç”Ÿæˆçš„æœ€ä½³æ–‡æ¡£ç»„åˆã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆè§£å†³äº†æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä¸­çš„å™ªå£°ä¸ä¸å®Œæ•´é—®é¢˜ï¼Œä»¥åŠç”Ÿæˆå¢å¼ºç”Ÿæˆ(GAG)ä¸­çš„ä¿¡æ¯å¹»è§‰ç°è±¡ï¼Œæ˜¾è‘—æå‡äº†ç­”æ¡ˆçš„å¯é æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMedRGAGåœ¨äº”ä¸ªåŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¾ƒMedRAGæå‡äº†12.5%ï¼Œè¾ƒMedGENIEæå‡äº†4.5%ï¼ŒéªŒè¯äº†ç»Ÿä¸€æ£€ç´¢ä¸ç”Ÿæˆåœ¨çŸ¥è¯†å¯†é›†å‹æ¨ç†ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.18297v1",
      "published_date": "2025-10-21 04:58:29 UTC",
      "updated_date": "2025-10-21 04:58:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:51:56.363052+00:00"
    },
    {
      "arxiv_id": "2510.20847v1",
      "title": "Integrated representational signatures strengthen specificity in brains and models",
      "title_zh": "ç»¼åˆè¡¨å¾ç‰¹å¾å¢å¼ºäº†å¤§è„‘ä¸æ¨¡å‹ä¸­çš„ç‰¹å¼‚æ€§",
      "authors": [
        "Jialin Wu",
        "Shreya Saha",
        "Yiqing Bo",
        "Meenakshi Khosla"
      ],
      "abstract": "The extent to which different neural or artificial neural networks (models) rely on equivalent representations to support similar tasks remains a central question in neuroscience and machine learning. Prior work has typically compared systems using a single representational similarity metric, yet each captures only one facet of representational structure. To address this, we leverage a suite of representational similarity metrics-each capturing a distinct facet of representational correspondence, such as geometry, unit-level tuning, or linear decodability-and assess brain region or model separability using multiple complementary measures. Metrics that preserve geometric or tuning structure (e.g., RSA, Soft Matching) yield stronger region-based discrimination, whereas more flexible mappings such as Linear Predictivity show weaker separation. These findings suggest that geometry and tuning encode brain-region- or model-family-specific signatures, while linearly decodable information tends to be more globally shared across regions or models. To integrate these complementary representational facets, we adapt Similarity Network Fusion (SNF), a framework originally developed for multi-omics data integration. SNF produces substantially sharper regional and model family-level separation than any single metric and yields robust composite similarity profiles. Moreover, clustering cortical regions using SNF-derived similarity scores reveals a clearer hierarchical organization that aligns closely with established anatomical and functional hierarchies of the visual cortex-surpassing the correspondence achieved by individual metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨æ¢ç©¶å¤§è„‘ç¥ç»ç³»ç»Ÿä¸äººå·¥ç¥ç»ç½‘ç»œåœ¨æ‰§è¡Œä»»åŠ¡æ—¶æ˜¯å¦ä¾èµ–ç­‰æ•ˆçš„è¡¨å¾ï¼ŒæŒ‡å‡ºä¼ ç»Ÿå•ä¸€è¡¨å¾ç›¸ä¼¼æ€§åº¦é‡æ— æ³•å…¨é¢æ•æ‰è¡¨å¾ç»“æ„çš„å¤æ‚æ€§ã€‚é€šè¿‡è¯„ä¼° RSAã€Soft Matching å’Œ Linear Predictivity ç­‰å¤šç§åº¦é‡æ ‡å‡†ï¼Œç ”ç©¶å‘ç°å‡ ä½•ç»“æ„ï¼ˆgeometryï¼‰ä¸å•ä½çº§è°ƒæ•´ï¼ˆunit-level tuningï¼‰èƒ½ç¼–ç ç‰¹å®šåŒºåŸŸæˆ–æ¨¡å‹çš„ç‰¹å¾ç­¾åï¼Œè€Œçº¿æ€§å¯è§£ç ä¿¡æ¯åˆ™æ›´å…·æ™®éæ€§ã€‚ä¸ºäº†ç»¼åˆè¿™äº›äº’è¡¥çš„è¡¨å¾ç»´åº¦ï¼Œç ”ç©¶å¼•å…¥äº†ç›¸ä¼¼æ€§ç½‘ç»œèåˆï¼ˆSimilarity Network Fusion, SNFï¼‰æ¡†æ¶ã€‚å®éªŒè¯æ˜ï¼ŒSNF åœ¨åŒºåˆ†è„‘åŒºå’Œæ¨¡å‹å®¶æ—æ–¹é¢çš„ç‰¹å¼‚æ€§æ˜¾è‘—ä¼˜äºå•ä¸€æŒ‡æ ‡ï¼Œå¹¶èƒ½äº§ç”Ÿæ›´ç¨³å¥çš„å¤åˆç›¸ä¼¼æ€§æ¦‚å†µã€‚æ­¤å¤–ï¼ŒåŸºäº SNF çš„èšç±»åˆ†ææ­ç¤ºäº†ä¸è§†è§‰çš®å±‚è§£å‰–åŠåŠŸèƒ½å±‚çº§é«˜åº¦ä¸€è‡´çš„å±‚æ¬¡ç»„ç»‡ï¼Œä¸ºé‡åŒ–ä¸åŒè®¡ç®—ç³»ç»Ÿé—´çš„è¡¨å¾å¯¹åº”å…³ç³»æä¾›äº†æ›´å¼ºæœ‰åŠ›çš„æ‰‹æ®µã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.20847v1",
      "published_date": "2025-10-21 04:37:27 UTC",
      "updated_date": "2025-10-21 04:37:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:51:55.955551+00:00"
    },
    {
      "arxiv_id": "2510.18911v1",
      "title": "Prospects for Using Artificial Intelligence to Understand Intrinsic Kinetics of Heterogeneous Catalytic Reactions",
      "title_zh": "åˆ©ç”¨äººå·¥æ™ºèƒ½æ¢ç©¶å¤šç›¸å‚¬åŒ–ååº”æœ¬å¾åŠ¨åŠ›å­¦çš„å‰æ™¯å±•æœ›",
      "authors": [
        "Andrew J. Medford",
        "Todd N. Whittaker",
        "Bjarne Kreitz",
        "David W. Flaherty",
        "John R. Kitchin"
      ],
      "abstract": "Artificial intelligence (AI) is influencing heterogeneous catalysis research by accelerating simulations and materials discovery. A key frontier is integrating AI with multiscale models and multimodal experiments to address the \"many-to-one\" challenge of linking intrinsic kinetics to observables. Advances in machine-learned force fields, microkinetics, and reactor modeling enable rapid exploration of chemical spaces, while operando and transient data provide unprecedented insight. Yet, inconsistent data quality and model complexity limit mechanistic discovery. Generative and agentic AI can automate model generation, quantify uncertainty, and couple theory with experiment, realizing \"self-driving models\" that produce interpretable, reproducible, and transferable understanding of catalytic systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨ç†è§£å¤šç›¸å‚¬åŒ–ååº”æœ¬å¾åŠ¨åŠ›å­¦ï¼ˆintrinsic kineticsï¼‰æ–¹é¢çš„åº”ç”¨å‰æ™¯ï¼ŒæŒ‡å‡ºå…¶å…³é”®å‰æ²¿åœ¨äºå°† AI ä¸å¤šå°ºåº¦æ¨¡å‹åŠå¤šæ¨¡æ€å®éªŒç›¸ç»“åˆï¼Œä»¥è§£å†³å°†åŠ¨åŠ›å­¦ä¸å¯è§‚æµ‹æŒ‡æ ‡ï¼ˆobservablesï¼‰å…³è”æ—¶é¢ä¸´çš„â€œå¤šå¯¹ä¸€â€æŒ‘æˆ˜ã€‚é€šè¿‡é›†æˆæœºå™¨å­¦ä¹ åŠ›åœºï¼ˆmachine-learned force fieldsï¼‰ã€å¾®è§‚åŠ¨åŠ›å­¦ï¼ˆmicrokineticsï¼‰å’Œååº”å™¨å»ºæ¨¡ï¼Œç ”ç©¶äººå‘˜èƒ½å¤Ÿå¿«é€Ÿæ¢ç´¢åŒ–å­¦ç©ºé—´ï¼Œå¹¶åˆ©ç”¨åŸä½ï¼ˆoperandoï¼‰åŠç¬æ€æ•°æ®è·å–æ·±å…¥æ´å¯Ÿã€‚å°½ç®¡æ•°æ®è´¨é‡ä¸ä¸€å’Œæ¨¡å‹å¤æ‚æ€§ä»æ˜¯åˆ¶çº¦æœºç†å‘ç°çš„ç“¶é¢ˆï¼Œä½†ç”Ÿæˆå¼ AIï¼ˆgenerative AIï¼‰å’Œæ™ºèƒ½ä½“ AIï¼ˆagentic AIï¼‰æœ‰æœ›å®ç°æ¨¡å‹ç”Ÿæˆçš„è‡ªåŠ¨åŒ–ä¸ä¸ç¡®å®šæ€§é‡åŒ–ã€‚è¿™ç§å°†ç†è®ºä¸å®éªŒæ·±åº¦è€¦åˆçš„â€œè‡ªåŠ¨é©¾é©¶æ¨¡å‹â€ï¼ˆself-driving modelsï¼‰å°†æ¨åŠ¨äº§ç”Ÿå¯è§£é‡Šã€å¯é‡å¤ä¸”å…·æœ‰è¿ç§»æ€§çš„å‚¬åŒ–ç³»ç»Ÿç†è§£ã€‚",
      "categories": [
        "physics.chem-ph",
        "cs.AI"
      ],
      "primary_category": "physics.chem-ph",
      "comment": "Submitted to \"Current Opinion in Chemical Engineering\" for peer review",
      "pdf_url": "https://arxiv.org/pdf/2510.18911v1",
      "published_date": "2025-10-21 04:35:26 UTC",
      "updated_date": "2025-10-21 04:35:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:51:53.965847+00:00"
    },
    {
      "arxiv_id": "2511.01866v1",
      "title": "EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs",
      "title_zh": "EdgeReasoningï¼šè¾¹ç¼˜ GPU ä¸Šçš„æ¨ç†å¤§è¯­è¨€æ¨¡å‹éƒ¨ç½²ç‰¹æ€§åˆ†æ",
      "authors": [
        "Benjamin Kubwimana",
        "Qijing Huang"
      ],
      "abstract": "Edge intelligence paradigm is increasingly demanded by the emerging autonomous systems, such as robotics. Beyond ensuring privacy-preserving operation and resilience in connectivity-limited environments, edge deployment offers significant energy and cost advantages over cloud-based solutions. However, deploying large language models (LLMs) for reasoning tasks on edge GPUs faces critical challenges from strict latency constraints and limited computational resources. To navigate these constraints, developers must balance multiple design factors - choosing reasoning versus non-reasoning architectures, selecting appropriate model sizes, allocating token budgets, and applying test-time scaling strategies - to meet target latency and optimize accuracy. Yet guidance on optimal combinations of these variables remains scarce. In this work, we present EdgeReasoning, a comprehensive study characterizing the deployment of reasoning LLMs on edge GPUs. We systematically quantify latency-accuracy tradeoffs across various LLM architectures and model sizes. We systematically evaluate prompt-based and model-tuning-based techniques for reducing reasoning token length while maintaining performance quality. We further profile test-time scaling methods with varying degrees of parallelism to maximize accuracy under strict latency budgets. Through these analyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency configurations, offering systematic guidance for optimal edge deployment of reasoning LLMs.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹æœºå™¨äººç­‰è‡ªä¸»ç³»ç»Ÿå¯¹è¾¹ç¼˜æ™ºèƒ½æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ï¼Œæ¢è®¨äº†åœ¨èµ„æºå—é™ä¸”å»¶è¿Ÿæ•æ„Ÿçš„è¾¹ç¼˜ GPU ä¸Šéƒ¨ç½²æ¨ç† LLMs é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç ”ç©¶æå‡ºäº† EdgeReasoningï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°åˆ»ç”»æ¨ç† LLMs åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²ç‰¹å¾ï¼Œé€šè¿‡ç»¼åˆåˆ†ææ¨¡å‹æ¶æ„ã€æ¨¡å‹å¤§å°ã€Token é¢„ç®—åŠ Test-time scaling ç­–ç•¥ç­‰è®¾è®¡å› ç´ ï¼Œå¯»æ‰¾æ€§èƒ½ä¸æ•ˆç‡çš„å¹³è¡¡ç‚¹ã€‚è¯¥å·¥ä½œç³»ç»Ÿåœ°é‡åŒ–äº†ä¸åŒæ¶æ„ä¸‹çš„ Latency-accuracy æƒè¡¡ï¼Œå¹¶è¯„ä¼°äº†é€šè¿‡æç¤ºè¯åŠæ¨¡å‹å¾®è°ƒæŠ€æœ¯ç¼©å‡æ¨ç† Token é•¿åº¦çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‰–æäº†å…·å¤‡ä¸åŒå¹¶è¡Œç¨‹åº¦çš„ Test-time scaling æ–¹æ³•ï¼Œä»¥åœ¨ä¸¥æ ¼çš„å»¶è¿Ÿé¢„ç®—å†…æœ€å¤§åŒ–å‡†ç¡®ç‡ã€‚æœ€ç»ˆï¼ŒEdgeReasoning æˆåŠŸæç»˜äº†ç²¾åº¦ä¸å»¶è¿Ÿé…ç½®çš„ Pareto frontierï¼Œä¸ºæ¨ç† LLMs åœ¨è¾¹ç¼˜ GPU ä¸Šçš„æœ€ä¼˜éƒ¨ç½²æä¾›äº†é‡è¦çš„ç³»ç»Ÿæ€§æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.DC",
      "comment": "Published in the Proceedings of the 2025 IEEE International Symposium on Workload Characterization (IISWC 2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.01866v1",
      "published_date": "2025-10-21 04:18:25 UTC",
      "updated_date": "2025-10-21 04:18:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:51:59.672373+00:00"
    },
    {
      "arxiv_id": "2510.18279v2",
      "title": "Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs",
      "title_zh": "æ–‡æœ¬è¿˜æ˜¯åƒç´ ï¼Ÿåªéœ€ä¸€åŠï¼šè®ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­è§†è§‰æ–‡æœ¬è¾“å…¥çš„ Token æ•ˆç‡",
      "authors": [
        "Yanhong Li",
        "Zixuan Lan",
        "Jiawei Zhou"
      ],
      "abstract": "Large language models (LLMs) and their multimodal variants can now process visual inputs, including images of text. This raises an intriguing question: can we compress textual inputs by feeding them as images to reduce token usage while preserving performance? In this paper, we show that visual text representations are a practical and surprisingly effective form of input compression for decoder LLMs. We exploit the idea of rendering long text inputs as a single image and provide it directly to the model. This leads to dramatically reduced number of decoder tokens required, offering a new form of input compression. Through experiments on two distinct benchmarks RULER (long-context retrieval) and CNN/DailyMail (document summarization) we demonstrate that this text-as-image method yields substantial token savings (often nearly half) without degrading task performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMultimodal LLMsï¼‰ä¸­è§†è§‰æ–‡æœ¬è¾“å…¥çš„ token æ•ˆç‡é—®é¢˜ï¼Œæ—¨åœ¨é€šè¿‡å°†æ–‡æœ¬è½¬åŒ–ä¸ºå›¾åƒæ¥å‡å°‘ token æ¶ˆè€—ã€‚ä½œè€…æå‡ºäº†ä¸€ç§å°†é•¿æ–‡æœ¬æ¸²æŸ“ä¸ºå•å¼ å›¾åƒå¹¶ç›´æ¥è¾“å…¥æ¨¡å‹çš„æ–¹æ³•ï¼Œå°†å…¶ä½œä¸º decoder LLMs çš„ä¸€ç§å®ç”¨ä¸”æœ‰æ•ˆçš„è¾“å…¥å‹ç¼©å½¢å¼ã€‚åœ¨é•¿ä¸Šä¸‹æ–‡æ£€ç´¢åŸºå‡† RULER å’Œæ–‡æ¡£æ‘˜è¦åŸºå‡† CNN/DailyMail ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¿™ç§ text-as-image æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸é™ä½ä»»åŠ¡æ€§èƒ½çš„å‰æä¸‹ï¼Œæ˜¾è‘—èŠ‚çœè¿‘ä¸€åŠçš„ decoder tokens ä½¿ç”¨é‡ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†è§†è§‰æ–‡æœ¬è¡¨ç¤ºï¼ˆvisual text representationsï¼‰åœ¨å¤„ç†é•¿æ–‡æœ¬è¾“å…¥æ—¶å…·æœ‰æé«˜çš„å‹ç¼©æ•ˆç‡ï¼Œä¸ºä¼˜åŒ–å¤šæ¨¡æ€æ¨¡å‹çš„è®¡ç®—æˆæœ¬æä¾›äº†æ–°çš„ç ”ç©¶è§†è§’ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2025 Findings (\"Text or Pixels? Evaluating Efficiency and Understanding of LLMs with Visual Text Inputs\")",
      "pdf_url": "https://arxiv.org/pdf/2510.18279v2",
      "published_date": "2025-10-21 04:07:20 UTC",
      "updated_date": "2025-10-22 01:54:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:52:03.355879+00:00"
    },
    {
      "arxiv_id": "2510.18910v1",
      "title": "Large Connectome Model: An fMRI Foundation Model of Brain Connectomes Empowered by Brain-Environment Interaction in Multitask Learning Landscape",
      "title_zh": "Large Connectome Modelï¼šå¤šä»»åŠ¡å­¦ä¹ èŒƒå¼ä¸‹è„‘-ç¯å¢ƒäº¤äº’èµ‹èƒ½çš„ fMRI å¤§è„‘è¿æ¥ç»„åŸºåº§æ¨¡å‹",
      "authors": [
        "Ziquan Wei",
        "Tingting Dan",
        "Guorong Wu"
      ],
      "abstract": "A reliable foundation model of functional neuroimages is critical to promote clinical applications where the performance of current AI models is significantly impeded by a limited sample size. To that end, tremendous efforts have been made to pretraining large models on extensive unlabeled fMRI data using scalable self-supervised learning. Since self-supervision is not necessarily aligned with the brain-to-outcome relationship, most foundation models are suboptimal to the downstream task, such as predicting disease outcomes. By capitalizing on rich environmental variables and demographic data along with an unprecedented amount of functional neuroimages, we form the brain modeling as a multitask learning and present a scalable model architecture for (i) multitask pretraining by tokenizing multiple brain-environment interactions (BEI) and (ii) semi-supervised finetuning by assigning pseudo-labels of pretrained BEI. We have evaluated our foundation model on a variety of applications, including sex prediction, human behavior recognition, and disease early diagnosis of Autism, Parkinson's disease, Alzheimer's disease, and {Schizophrenia}, where promising results indicate the great potential to facilitate current neuroimaging applications in clinical routines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Large Connectome Model (LCM)ï¼Œè¿™æ˜¯ä¸€ç§åŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒ (fMRI) åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å½“å‰ AI æ¨¡å‹å› æ ·æœ¬é‡æœ‰é™è€Œå¯¼è‡´åœ¨ä¸´åºŠåº”ç”¨ä¸­æ€§èƒ½å—é˜»çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡æ•´åˆä¸°å¯Œçš„ç¯å¢ƒå’Œäººå£ç»Ÿè®¡å­¦å˜é‡ï¼Œå°†å¤§è„‘å»ºæ¨¡è§†ä¸ºä¸€ç§å¤šä»»åŠ¡å­¦ä¹  (Multitask Learning) è¿‡ç¨‹ï¼Œå…‹æœäº†ä¼ ç»Ÿè‡ªç›‘ç£å­¦ä¹ ä¸å¤§è„‘äº§å‡ºå…³ç³»ä¸åŒ¹é…çš„ç¼ºé™·ã€‚å…¶æ¶æ„æ”¯æŒé€šè¿‡æ ‡è®°åŒ–å¤šç§å¤§è„‘-ç¯å¢ƒäº¤äº’ (Brain-Environment Interaction, BEI) è¿›è¡Œå¤šä»»åŠ¡é¢„è®­ç»ƒï¼Œå¹¶åˆ©ç”¨ BEI ä¼ªæ ‡ç­¾è¿›è¡ŒåŠç›‘ç£å¾®è°ƒ (Semi-supervised Finetuning)ã€‚LCM åœ¨æ€§åˆ«é¢„æµ‹ã€äººç±»è¡Œä¸ºè¯†åˆ«ä»¥åŠè‡ªé—­ç—‡ (Autism)ã€å¸•é‡‘æ£®ç—… (Parkinson's disease)ã€é˜¿å°”èŒ¨æµ·é»˜ç—… (Alzheimer's disease) å’Œç²¾ç¥åˆ†è£‚ç—‡ (Schizophrenia) çš„æ—©æœŸè¯Šæ–­ä¸­å±•ç°äº†ä¼˜å¼‚æ€§èƒ½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜è¯¥åŸºç¡€æ¨¡å‹å…·æœ‰æå‡ä¸´åºŠç¥ç»å½±åƒå¸¸è§„åº”ç”¨çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºå¤„ç†è„‘è¿æ¥ç»„æ•°æ®æä¾›äº†å¯æ‰©å±•çš„æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.18910v1",
      "published_date": "2025-10-21 03:50:51 UTC",
      "updated_date": "2025-10-21 03:50:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:52:09.755562+00:00"
    },
    {
      "arxiv_id": "2510.18269v1",
      "title": "StreamingTOM: Streaming Token Compression for Efficient Video Understanding",
      "title_zh": "StreamingTOMï¼šé¢å‘é«˜æ•ˆè§†é¢‘ç†è§£çš„æµå¼ Token å‹ç¼©",
      "authors": [
        "Xueyi Chen",
        "Keda Tao",
        "Kele Shao",
        "Huan Wang"
      ],
      "abstract": "Unlike offline processing, streaming video vision-language models face two fundamental constraints: causality and accumulation. Causality prevents access to future frames that offline methods exploit, while accumulation causes tokens to grow unbounded, creating efficiency bottlenecks. However, existing approaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill unchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage framework that addresses both pre-LLM and post-LLM bottlenecks with predictable latency. Causal Temporal Reduction imposes a fixed per-frame budget and selects tokens based on adjacent-frame changes and token saliency, drastically reducing per-frame prefill cost by processing only a compact subset of visual tokens per frame instead of all visual tokens. Online Quantized Memory stores tokens in 4-bit format, retrieves relevant groups on demand, and dequantizes them, keeping the active kv-cache bounded regardless of stream length. Experiments demonstrate our method achieves $15.7\\times$ kv-cache compression, $1.2\\times$ lower peak memory and $2\\times$ faster TTFT compared to prior SOTA. StreamingTOM maintains state-of-the-art accuracy among training-free methods with an average of $63.8\\%$ on offline benchmarks and $55.8\\%/3.7$ on RVS. These results highlight the practical benefits of our two-stage approach for efficient streaming video understanding with bounded growth.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æµå¼è§†é¢‘è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVideo Vision-Language Modelsï¼‰é¢ä¸´çš„å› æœæ€§ï¼ˆCausalityï¼‰å’Œç´¯ç§¯æ€§ï¼ˆAccumulationï¼‰ä¸¤å¤§æ ¸å¿ƒé™åˆ¶ï¼Œæå‡ºäº†StreamingTOMæ¡†æ¶ã€‚è¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒä¸”å³æ’å³ç”¨çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶è§£å†³é¢„LLMé¢„å¡«å……ï¼ˆpre-LLM prefillï¼‰å’ŒåLLMç“¶é¢ˆã€‚æ¡†æ¶çš„ç¬¬ä¸€é˜¶æ®µé€šè¿‡å› æœæ—¶é—´è§„çº¦ï¼ˆCausal Temporal Reductionï¼‰åŸºäºç›¸é‚»å¸§å˜åŒ–å’ŒTokenæ˜¾è‘—æ€§é€‰æ‹©Tokenï¼Œå¤§å¹…é™ä½äº†æ¯å¸§çš„é¢„å¡«å……æˆæœ¬ã€‚ç¬¬äºŒé˜¶æ®µé‡‡ç”¨åœ¨çº¿é‡åŒ–å­˜å‚¨ï¼ˆOnline Quantized Memoryï¼‰ï¼Œä»¥4-bitæ ¼å¼å­˜å‚¨Tokenå¹¶æŒ‰éœ€æ£€ç´¢ï¼Œç¡®ä¿æ´»è·ƒçš„é”®å€¼ç¼“å­˜ï¼ˆKV-cacheï¼‰åœ¨æµé•¿åº¦å¢åŠ æ—¶ä»ä¿æŒæœ‰ç•Œã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯ç›¸æ¯”ï¼ŒStreamingTOMå®ç°äº†15.7å€çš„KV-cacheå‹ç¼©ï¼Œå¹¶å°†å³°å€¼å†…å­˜é™ä½äº†1.2å€ï¼Œé¦–å­—ç”Ÿæˆæ—¶é—´ï¼ˆTTFTï¼‰æå‡äº†2å€ã€‚è¯¥æ–¹æ³•åœ¨ç¦»çº¿åŸºå‡†æµ‹è¯•å’ŒRVSæµ‹è¯•ä¸­å‡ä¿æŒäº†é¢†å…ˆçš„å‡†ç¡®ç‡ï¼Œè¯æ˜äº†å…¶åœ¨é«˜æ•ˆæµå¼è§†é¢‘ç†è§£ä¸­çš„å®é™…åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18269v1",
      "published_date": "2025-10-21 03:39:41 UTC",
      "updated_date": "2025-10-21 03:39:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:52:07.866410+00:00"
    },
    {
      "arxiv_id": "2510.18909v1",
      "title": "Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection",
      "title_zh": "æ‹©ä¼˜è€Œå­¦ï¼Œå¦è¾Ÿè¹Šå¾„ï¼šå¤šæ ·æ€§é©±åŠ¨çš„æ•°æ®é€‰æ‹©é‡æ–°å®¡è§†",
      "authors": [
        "Hongyi He",
        "Xiao Liu",
        "Zhenghao Lin",
        "Mingni Tang",
        "Yi Cheng",
        "Jintao Wang",
        "Wenjie Li",
        "Peng Cheng",
        "Yeyun Gong"
      ],
      "abstract": "High-quality pre-training data is crutial for large language models, where quality captures factual reliability and semantic value, and diversity ensures broad coverage and distributional heterogeneity. Existing approaches typically rely on single or multiple-dimensional score-based selection. However, directly selecting top-scored data often degrades performance, and sampling from a broader range is required to recover results. The above non-monotonicity between dataset scores and downstream benchmark results reveals a fundamental bias: score-based methods collapse correlated dimensions, causing top-scored data to appear high-quality while systematically overlooking diversity. We argue that ensuring diversity requires decomposing correlated metrics into orthogonal feature dimensions, from which the top-scored data can be directly selected. Therefore, we proposed the Orthogonal Diversity-Aware Selection (ODiS) algorithm, which preserves both quality and diversity during data selection. First, ODiS evaluates data from multiple dimensions, covering language quality, knowledge quality, and comprehension difficulty. The multi-dimensional scores are then decorrelated via Principal Component Analysis (PCA), yielding orthogonal evaluation dimensions. For each dimension, a Roberta-based scorer is trained to regress the data onto PCA-projected scores, enabling scalable inference on large corpora. Finally, ODiS constructs the training dataset by selecting top-scored data within each orthogonal dimension, thereby ensuring both quality and diversity. Empirical results show that ODiS-selected data exhibit less than 2\\% inter-dimension overlap, confirming orthogonality between dimensions. More importantly, models trained with ODiS-selected data significantly outperform other baselines on downstream benchmarks, highlighting the necessity of orthogonal, diversity-aware data selection for LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)é¢„è®­ç»ƒä¸­çš„æ•°æ®é€‰æ‹©é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„åŸºäºåˆ†æ•°çš„é€‰æ‹©æ–¹æ³•å› å¿½è§†æŒ‡æ ‡é—´çš„ç›¸å…³æ€§è€Œå¯¼è‡´å¤šæ ·æ€§(diversity)ç¼ºå¤±ï¼Œè¿›è€Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Orthogonal Diversity-Aware Selection (ODiS)ç®—æ³•ï¼Œæ—¨åœ¨åŒæ—¶ä¿è¯é¢„è®­ç»ƒæ•°æ®çš„é«˜è´¨é‡ä¸å¤šæ ·æ€§ã€‚è¯¥æ–¹æ³•é¦–å…ˆä»è¯­è¨€è´¨é‡ã€çŸ¥è¯†è´¨é‡å’Œç†è§£éš¾åº¦ç­‰å¤šä¸ªç»´åº¦è¯„ä¼°æ•°æ®ï¼Œéšååˆ©ç”¨ä¸»æˆåˆ†åˆ†æ(PCA)å¯¹è¯„åˆ†è¿›è¡Œå»ç›¸å…³å¤„ç†ï¼Œä»è€Œç”Ÿæˆæ­£äº¤çš„è¯„ä¼°ç»´åº¦ã€‚ç ”ç©¶è¿›ä¸€æ­¥è®­ç»ƒäº†åŸºäºRobertaçš„è¯„åˆ†å™¨ä»¥å®ç°å¤§è§„æ¨¡è¯­æ–™åº“ä¸Šçš„é«˜æ•ˆæ¨ç†ï¼Œå¹¶ä»æ¯ä¸ªæ­£äº¤ç»´åº¦ä¸­ç­›é€‰é«˜åˆ†æ•°æ®æ„å»ºè®­ç»ƒé›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»ODiSé€‰æ‹©çš„æ•°æ®åœ¨ä¸åŒç»´åº¦é—´çš„é‡å ç‡ä½äº2%ï¼Œä¸”åœ¨ä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ˜¾è‘—ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚è¿™ä¸€å‘ç°è¯å®äº†åœ¨LLMsæ•°æ®é¢„å¤„ç†ä¸­é‡‡ç”¨æ­£äº¤ä¸”å…·å¤‡å¤šæ ·æ€§æ„ŸçŸ¥(diversity-aware)çš„æ•°æ®é€‰æ‹©ç­–ç•¥çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18909v1",
      "published_date": "2025-10-21 03:37:31 UTC",
      "updated_date": "2025-10-21 03:37:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:52:27.555350+00:00"
    },
    {
      "arxiv_id": "2510.18267v1",
      "title": "Latent-Info and Low-Dimensional Learning for Human Mesh Recovery and Parallel Optimization",
      "title_zh": "åŸºäºæ½œåœ¨ä¿¡æ¯ä¸ä½ç»´å­¦ä¹ çš„äººä½“ç½‘æ ¼é‡å»ºåŠå¹¶è¡Œä¼˜åŒ–",
      "authors": [
        "Xiang Zhang",
        "Suping Wu",
        "Sheng Yang"
      ],
      "abstract": "Existing 3D human mesh recovery methods often fail to fully exploit the latent information (e.g., human motion, shape alignment), leading to issues with limb misalignment and insufficient local details in the reconstructed human mesh (especially in complex scenes). Furthermore, the performance improvement gained by modelling mesh vertices and pose node interactions using attention mechanisms comes at a high computational cost. To address these issues, we propose a two-stage network for human mesh recovery based on latent information and low dimensional learning. Specifically, the first stage of the network fully excavates global (e.g., the overall shape alignment) and local (e.g., textures, detail) information from the low and high-frequency components of image features and aggregates this information into a hybrid latent frequency domain feature. This strategy effectively extracts latent information. Subsequently, utilizing extracted hybrid latent frequency domain features collaborates to enhance 2D poses to 3D learning. In the second stage, with the assistance of hybrid latent features, we model the interaction learning between the rough 3D human mesh template and the 3D pose, optimizing the pose and shape of the human mesh. Unlike existing mesh pose interaction methods, we design a low-dimensional mesh pose interaction method through dimensionality reduction and parallel optimization that significantly reduces computational costs without sacrificing reconstruction accuracy. Extensive experimental results on large publicly available datasets indicate superiority compared to the most state-of-the-art.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰3Däººä½“é‡å»º(3D human mesh recovery)æ–¹æ³•åœ¨æŒ–æ˜æ½œåœ¨ä¿¡æ¯(latent information)æ–¹é¢çš„ä¸è¶³ï¼Œä»¥åŠæ³¨æ„åŠ›æœºåˆ¶å¸¦æ¥çš„é«˜æ˜‚è®¡ç®—æˆæœ¬ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨ä¿¡æ¯å’Œä½ç»´å­¦ä¹ (low-dimensional learning)çš„ä¸¤é˜¶æ®µç½‘ç»œã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡ä»å›¾åƒç‰¹å¾çš„ä½é¢‘å’Œé«˜é¢‘åˆ†é‡ä¸­å……åˆ†æŒ–æ˜å…¨å±€å½¢çŠ¶å¯¹é½å’Œå±€éƒ¨ç»†èŠ‚ä¿¡æ¯ï¼Œå¹¶å°†å…¶èšåˆä¸ºæ··åˆæ½œåœ¨é¢‘åŸŸç‰¹å¾(hybrid latent frequency domain feature)ï¼Œæœ‰æ•ˆå¢å¼ºäº†ä»2Då§¿æ€åˆ°3Dçš„å­¦ä¹ ã€‚ç¬¬äºŒé˜¶æ®µåˆ©ç”¨è¯¥ç‰¹å¾å¯¹ç²—ç•¥çš„3Däººä½“ç½‘æ ¼æ¨¡æ¿ä¸3Då§¿æ€ä¹‹é—´çš„äº¤äº’è¿›è¡Œå»ºæ¨¡ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–äº†ç½‘æ ¼çš„å§¿æ€å’Œå½¢çŠ¶ã€‚è¯¥ç ”ç©¶ç‰¹åˆ«è®¾è®¡äº†ä¸€ç§ç»“åˆé™ç»´(dimensionality reduction)å’Œå¹¶è¡Œä¼˜åŒ–(parallel optimization)çš„ä½ç»´ç½‘æ ¼å§¿æ€äº¤äº’æ–¹æ³•ï¼Œåœ¨ä¸ç‰ºç‰²é‡å»ºç²¾åº¦çš„æƒ…å†µä¸‹æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚åœ¨å¤§å‹å…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”å½“å‰æœ€å…ˆè¿›(state-of-the-art)çš„æŠ€æœ¯å…·æœ‰æ˜¾è‘—ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICME2025",
      "pdf_url": "https://arxiv.org/pdf/2510.18267v1",
      "published_date": "2025-10-21 03:35:12 UTC",
      "updated_date": "2025-10-21 03:35:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:52:26.072797+00:00"
    },
    {
      "arxiv_id": "2510.18266v1",
      "title": "SPIKE: Stable Physics-Informed Kernel Evolution Method for Solving Hyperbolic Conservation Laws",
      "title_zh": "SPIKEï¼šç”¨äºæ±‚è§£åŒæ›²å®ˆæ’å¾‹çš„ç¨³å®šç‰©ç†ä¿¡æ¯æ ¸æ¼”åŒ–æ–¹æ³•",
      "authors": [
        "Hua Su",
        "Lei Zhang",
        "Jin Zhao"
      ],
      "abstract": "We introduce the Stable Physics-Informed Kernel Evolution (SPIKE) method for numerical computation of inviscid hyperbolic conservation laws. SPIKE resolves a fundamental paradox: how strong-form residual minimization can capture weak solutions containing discontinuities. SPIKE employs reproducing kernel representations with regularized parameter evolution, where Tikhonov regularization provides a smooth transition mechanism through shock formation, allowing the dynamics to traverse shock singularities. This approach automatically maintains conservation, tracks characteristics, and captures shocks satisfying Rankine-Hugoniot conditions within a unified framework requiring no explicit shock detection or artificial viscosity. Numerical validation across scalar and vector-valued conservation laws confirms the method's effectiveness.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SPIKE (Stable Physics-Informed Kernel Evolution) æ–¹æ³•ï¼Œç”¨äºæ•°å€¼è®¡ç®—æ— ç²˜æ€§ Hyperbolic Conservation Lawsã€‚è¯¥æ–¹æ³•æœ‰æ•ˆè§£å†³äº†å¼ºå½¢å¼æ®‹å·®æœ€å°åŒ– (strong-form residual minimization) å¦‚ä½•æ•æ‰åŒ…å«é—´æ–­ç‚¹çš„å¼±è§£ (weak solutions) è¿™ä¸€æ ¸å¿ƒæ‚–è®ºã€‚SPIKE é‡‡ç”¨å†ç”Ÿæ ¸è¡¨ç¤º (reproducing kernel representations) ç»“åˆæ­£åˆ™åŒ–å‚æ•°æ¼”åŒ–ï¼Œåˆ©ç”¨ Tikhonov Regularization åœ¨æ¿€æ³¢å½¢æˆæ—¶æä¾›å¹³æ»‘è½¬æ¢æœºåˆ¶ï¼Œå¼•å¯¼åŠ¨åŠ›å­¦è¿‡ç¨‹ç©¿è¶Šæ¿€æ³¢å¥‡å¼‚ç‚¹ (shock singularities)ã€‚è¯¥æ¡†æ¶èƒ½åœ¨ä¸ä¾èµ–æ˜¾å¼æ¿€æ³¢æ£€æµ‹ (shock detection) æˆ–äººå·¥ç²˜æ€§ (artificial viscosity) çš„æƒ…å†µä¸‹ï¼Œè‡ªåŠ¨ç»´æŒå®ˆæ’æ€§ã€è¿½è¸ªç‰¹å¾çº¿å¹¶æ•æ‰æ»¡è¶³ Rankine-Hugoniot æ¡ä»¶çš„æ¿€æ³¢ã€‚é€šè¿‡å¯¹æ ‡é‡å’Œå‘é‡å®ˆæ’å¾‹çš„æ•°å€¼éªŒè¯ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤„ç†å¤æ‚æµä½“åŠ¨åŠ›å­¦é—®é¢˜ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "math.NA",
        "cs.AI",
        "cs.LG",
        "math.AP"
      ],
      "primary_category": "math.NA",
      "comment": "24 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.18266v1",
      "published_date": "2025-10-21 03:34:49 UTC",
      "updated_date": "2025-10-21 03:34:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:52:28.241517+00:00"
    },
    {
      "arxiv_id": "2510.18259v1",
      "title": "Learning under Quantization for High-Dimensional Linear Regression",
      "title_zh": "é«˜ç»´çº¿æ€§å›å½’åœ¨é‡åŒ–æ¡ä»¶ä¸‹çš„å­¦ä¹ ",
      "authors": [
        "Dechen Zhang",
        "Junwei Su",
        "Difan Zou"
      ],
      "abstract": "The use of low-bit quantization has emerged as an indispensable technique for enabling the efficient training of large-scale models. Despite its widespread empirical success, a rigorous theoretical understanding of its impact on learning performance remains notably absent, even in the simplest linear regression setting. We present the first systematic theoretical study of this fundamental question, analyzing finite-step stochastic gradient descent (SGD) for high-dimensional linear regression under a comprehensive range of quantization targets: data, labels, parameters, activations, and gradients. Our novel analytical framework establishes precise algorithm-dependent and data-dependent excess risk bounds that characterize how different quantization affects learning: parameter, activation, and gradient quantization amplify noise during training; data quantization distorts the data spectrum; and data and label quantization introduce additional approximation and quantized error. Crucially, we prove that for multiplicative quantization (with input-dependent quantization step), this spectral distortion can be eliminated, and for additive quantization (with constant quantization step), a beneficial scaling effect with batch size emerges. Furthermore, for common polynomial-decay data spectra, we quantitatively compare the risks of multiplicative and additive quantization, drawing a parallel to the comparison between FP and integer quantization methods. Our theory provides a powerful lens to characterize how quantization shapes the learning dynamics of optimization algorithms, paving the way to further explore learning theory under practical hardware constraints.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹é«˜ç»´çº¿æ€§å›å½’(High-Dimensional Linear Regression)ä¸­ä½æ¯”ç‰¹é‡åŒ–(low-bit quantization)å­¦ä¹ æ€§èƒ½ç¼ºä¹ç†è®ºç†è§£çš„é—®é¢˜ï¼Œé¦–æ¬¡åœ¨æœ‰é™æ­¥éšæœºæ¢¯åº¦ä¸‹é™(finite-step SGD)æ¡†æ¶ä¸‹å¯¹æ•°æ®ã€æ ‡ç­¾ã€å‚æ•°ã€æ¿€æ´»å’Œæ¢¯åº¦ç­‰å¤šç§é‡åŒ–ç›®æ ‡è¿›è¡Œäº†ç³»ç»Ÿçš„ç†è®ºåˆ†æã€‚ç ”ç©¶å»ºç«‹äº†ä¸€å¥—æ–°å‹åˆ†ææ¡†æ¶ï¼Œé€šè¿‡ç²¾ç¡®çš„ç®—æ³•ä¸æ•°æ®ç›¸å…³çš„è¿‡åº¦é£é™©ç•Œé™(excess risk bounds)æ­ç¤ºäº†å‚æ•°ã€æ¿€æ´»å’Œæ¢¯åº¦é‡åŒ–ä¼šæ”¾å¤§è®­ç»ƒå™ªå£°ï¼Œè€Œæ•°æ®é‡åŒ–ä¼šæ‰­æ›²æ•°æ®é¢‘è°±(data spectrum)ï¼Œæ•°æ®å’Œæ ‡ç­¾é‡åŒ–åˆ™ä¼šå¼•å…¥é¢å¤–çš„è¿‘ä¼¼å’Œé‡åŒ–è¯¯å·®ã€‚å…³é”®ç ”ç©¶ç»“æœè¯æ˜ï¼Œå¯¹äºä¹˜æ³•é‡åŒ–(multiplicative quantization)ï¼Œé¢‘è°±å¤±çœŸå¯ä»¥è¢«æ¶ˆé™¤ï¼›è€Œå¯¹äºåŠ æ³•é‡åŒ–(additive quantization)ï¼Œåˆ™ä¼šå±•ç°å‡ºéšæ‰¹å¤§å°(batch size)å¢åŠ çš„æœ‰ç›Šç¼©æ”¾æ•ˆåº”ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å®šé‡æ¯”è¾ƒäº†ä¸åŒæ•°æ®é¢‘è°±ä¸‹ä¹˜æ³•ä¸åŠ æ³•é‡åŒ–çš„é£é™©ï¼Œå¹¶å°†å…¶ä¸æµ®ç‚¹(FP)å’Œæ•´æ•°é‡åŒ–æ–¹æ³•è¿›è¡Œäº†ç±»æ¯”ã€‚è¯¥ç†è®ºå·¥ä½œä¸ºç†è§£é‡åŒ–å¦‚ä½•å¡‘é€ ä¼˜åŒ–ç®—æ³•çš„å­¦ä¹ åŠ¨æ€æä¾›äº†é‡è¦è§†è§’ï¼Œä¸ºè¿›ä¸€æ­¥æ¢ç´¢ç¡¬ä»¶çº¦æŸä¸‹çš„å­¦ä¹ ç†è®ºå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18259v1",
      "published_date": "2025-10-21 03:30:11 UTC",
      "updated_date": "2025-10-21 03:30:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:52:41.687323+00:00"
    },
    {
      "arxiv_id": "2510.18258v1",
      "title": "NTKMTL: Mitigating Task Imbalance in Multi-Task Learning from Neural Tangent Kernel Perspective",
      "title_zh": "NTKMTLï¼šä»ç¥ç»åˆ‡çº¿æ ¸è§†è§’ç¼“è§£å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„ä»»åŠ¡å¤±è¡¡",
      "authors": [
        "Xiaohan Qin",
        "Xiaoxing Wang",
        "Ning Liao",
        "Junchi Yan"
      ],
      "abstract": "Multi-Task Learning (MTL) enables a single model to learn multiple tasks simultaneously, leveraging knowledge transfer among tasks for enhanced generalization, and has been widely applied across various domains. However, task imbalance remains a major challenge in MTL. Although balancing the convergence speeds of different tasks is an effective approach to address this issue, it is highly challenging to accurately characterize the training dynamics and convergence speeds of multiple tasks within the complex MTL system. To this end, we attempt to analyze the training dynamics in MTL by leveraging Neural Tangent Kernel (NTK) theory and propose a new MTL method, NTKMTL. Specifically, we introduce an extended NTK matrix for MTL and adopt spectral analysis to balance the convergence speeds of multiple tasks, thereby mitigating task imbalance. Based on the approximation via shared representation, we further propose NTKMTL-SR, achieving training efficiency while maintaining competitive performance. Extensive experiments demonstrate that our methods achieve state-of-the-art performance across a wide range of benchmarks, including both multi-task supervised learning and multi-task reinforcement learning. Source code is available at https://github.com/jianke0604/NTKMTL.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Multi-Task Learning (MTL) ä¸­æ™®éå­˜åœ¨çš„ä»»åŠ¡ä¸å¹³è¡¡é—®é¢˜ï¼ŒæŒ‡å‡ºåœ¨å¤æ‚çš„ MTL ç³»ç»Ÿä¸­å‡†ç¡®åˆ»ç”»å¤šä»»åŠ¡çš„è®­ç»ƒåŠ¨åŠ›å­¦å’Œæ”¶æ•›é€Ÿåº¦å…·æœ‰æå¤§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡ä» Neural Tangent Kernel (NTK) ç†è®ºè§†è§’å‡ºå‘ï¼Œæå‡ºäº†åä¸º NTKMTL çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§æ‰©å±•çš„ NTK çŸ©é˜µï¼Œå¹¶é‡‡ç”¨è°±åˆ†æ (spectral analysis) æ‰‹æ®µæ¥å¹³è¡¡å¤šä¸ªä»»åŠ¡çš„æ”¶æ•›é€Ÿåº¦ï¼Œä»è€Œç¼“è§£ä»»åŠ¡ä¸å¹³è¡¡ç°è±¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜åŸºäºå…±äº«è¡¨ç¤º (shared representation) è¿‘ä¼¼æå‡ºäº† NTKMTL-SRï¼Œåœ¨ä¿è¯ç«äº‰æ€§èƒ½çš„åŒæ—¶æå‡äº†è®­ç»ƒæ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNTKMTL åœ¨å¤šä»»åŠ¡ç›‘ç£å­¦ä¹ å’Œå¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº† state-of-the-art çš„æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18258v1",
      "published_date": "2025-10-21 03:29:40 UTC",
      "updated_date": "2025-10-21 03:29:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:52:32.991591+00:00"
    },
    {
      "arxiv_id": "2510.18908v1",
      "title": "Improving Topic Modeling of Social Media Short Texts with Rephrasing: A Case Study of COVID-19 Related Tweets",
      "title_zh": "é€šè¿‡é‡è¿°æ”¹è¿›ç¤¾äº¤åª’ä½“çŸ­æ–‡æœ¬çš„ä¸»é¢˜å»ºæ¨¡ï¼šä»¥æ–°å† è‚ºç‚ç›¸å…³æ¨æ–‡ä¸ºä¾‹çš„ç ”ç©¶",
      "authors": [
        "Wangjiaxuan Xin",
        "Shuhua Yin",
        "Shi Chen",
        "Yaorong Ge"
      ],
      "abstract": "Social media platforms such as Twitter (now X) provide rich data for analyzing public discourse, especially during crises such as the COVID-19 pandemic. However, the brevity, informality, and noise of social media short texts often hinder the effectiveness of traditional topic modeling, producing incoherent or redundant topics that are often difficult to interpret. To address these challenges, we have developed \\emph{TM-Rephrase}, a model-agnostic framework that leverages large language models (LLMs) to rephrase raw tweets into more standardized and formal language prior to topic modeling. Using a dataset of 25,027 COVID-19-related Twitter posts, we investigate the effects of two rephrasing strategies, general- and colloquial-to-formal-rephrasing, on multiple topic modeling methods. Results demonstrate that \\emph{TM-Rephrase} improves three metrics measuring topic modeling performance (i.e., topic coherence, topic uniqueness, and topic diversity) while reducing topic redundancy of most topic modeling algorithms, with the colloquial-to-formal strategy yielding the greatest performance gains and especially for the Latent Dirichlet Allocation (LDA) algorithm. This study contributes to a model-agnostic approach to enhancing topic modeling in public health related social media analysis, with broad implications for improved understanding of public discourse in health crisis as well as other important domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¤¾äº¤åª’ä½“çŸ­æ–‡æœ¬ï¼ˆå¦‚Twitterï¼‰å­˜åœ¨çš„ç®€çŸ­ã€éæ­£å¼åŠå™ªå£°ç­‰å½±å“ä¼ ç»Ÿä¸»é¢˜å»ºæ¨¡ï¼ˆTopic Modelingï¼‰æ•ˆæœçš„é—®é¢˜ï¼Œå¼€å‘äº†åä¸ºTM-Rephraseçš„æ¨¡å‹æ— å…³æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¿›è¡Œä¸»é¢˜å»ºæ¨¡å‰å°†åŸå§‹æ¨æ–‡æ”¹å†™ä¸ºæ›´æ ‡å‡†åŒ–å’Œæ­£å¼çš„è¯­è¨€ï¼Œå¹¶åŸºäº25,027æ¡ä¸COVID-19ç›¸å…³çš„æ¨æ–‡æ•°æ®é›†è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚é€šè¿‡å¯¹æ¯”é€šç”¨æ”¹å†™å’Œå£è¯­è½¬æ­£å¼æ”¹å†™ä¸¤ç§ç­–ç•¥ï¼Œå®éªŒç»“æœè¯æ˜TM-Rephraseèƒ½å¤Ÿæ˜¾è‘—æå‡ä¸»é¢˜è¿è´¯æ€§ï¼ˆTopic Coherenceï¼‰ã€ä¸»é¢˜ç‹¬ç‰¹æ€§ï¼ˆTopic Uniquenessï¼‰å’Œä¸»é¢˜å¤šæ ·æ€§ï¼ˆTopic Diversityï¼‰ï¼Œå¹¶æœ‰æ•ˆå‡å°‘ä¸»é¢˜å†—ä½™ã€‚å…¶ä¸­ï¼Œå£è¯­è½¬æ­£å¼ç­–ç•¥åœ¨æ€§èƒ½æå‡ä¸Šæœ€ä¸ºçªå‡ºï¼Œå°¤å…¶æ˜¾è‘—å¢å¼ºäº†Latent Dirichlet Allocation (LDA)ç®—æ³•çš„å»ºæ¨¡æ•ˆæœã€‚è¿™é¡¹å·¥ä½œä¸ºå…¬å…±å«ç”ŸåŠå…¶ä»–é¢†åŸŸçš„ç¤¾äº¤åª’ä½“èˆ†è®ºåˆ†ææä¾›äº†ä¸€ç§åˆ›æ–°çš„å¢å¼ºæ‰‹æ®µï¼Œæœ‰åŠ©äºæ›´å‡†ç¡®åœ°è§£è¯»å±æœºæœŸé—´çš„å…¬ä¼—è¯è¯­ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18908v1",
      "published_date": "2025-10-21 03:29:38 UTC",
      "updated_date": "2025-10-21 03:29:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:52:37.979879+00:00"
    },
    {
      "arxiv_id": "2510.18257v1",
      "title": "DelvePO: Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization",
      "title_zh": "DelvePOï¼šé¢å‘çµæ´»æç¤ºè¯ä¼˜åŒ–çš„æ–¹å‘å¼•å¯¼å¼è‡ªè¿›åŒ–æ¡†æ¶",
      "authors": [
        "Tao Tao",
        "Guanghui Zhu",
        "Lang Guo",
        "Hongyi Chen",
        "Chunfeng Yuan",
        "Yihua Huang"
      ],
      "abstract": "Prompt Optimization has emerged as a crucial approach due to its capabilities in steering Large Language Models to solve various tasks. However, current works mainly rely on the random rewriting ability of LLMs, and the optimization process generally focus on specific influencing factors, which makes it easy to fall into local optimum. Besides, the performance of the optimized prompt is often unstable, which limits its transferability in different tasks. To address the above challenges, we propose $\\textbf{DelvePO}$ ($\\textbf{D}$irection-Guid$\\textbf{e}$d Se$\\textbf{l}$f-E$\\textbf{v}$olving Framework for Fl$\\textbf{e}$xible $\\textbf{P}$rompt $\\textbf{O}$ptimization), a task-agnostic framework to optimize prompts in self-evolve manner. In our framework, we decouple prompts into different components that can be used to explore the impact that different factors may have on various tasks. On this basis, we introduce working memory, through which LLMs can alleviate the deficiencies caused by their own uncertainties and further obtain key insights to guide the generation of new prompts. Extensive experiments conducted on different tasks covering various domains for both open- and closed-source LLMs, including DeepSeek-R1-Distill-Llama-8B, Qwen2.5-7B-Instruct and GPT-4o-mini. Experimental results show that DelvePO consistently outperforms previous SOTA methods under identical experimental settings, demonstrating its effectiveness and transferability across different tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DelvePOï¼Œä¸€ç§ä»»åŠ¡æ— å…³çš„æç¤ºè¯è‡ªæˆ‘è¿›åŒ–ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰Prompt Optimizationæ–¹æ³•è¿‡åº¦ä¾èµ–éšæœºé‡å†™åŠæ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æç¤ºè¯è§£æ„ä¸ºä¸åŒç»„ä»¶ï¼Œæ¢ç´¢å„å› ç´ å¯¹ä¸åŒä»»åŠ¡çš„å½±å“ï¼Œå¹¶å¼•å…¥working memoryæœºåˆ¶æ¥å‡è½»Large Language Modelsè‡ªèº«ä¸ç¡®å®šæ€§å¯¼è‡´çš„ç¼ºé™·ã€‚é€šè¿‡åˆ©ç”¨working memoryè·å–çš„å…³é”®è§è§£ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æŒ‡å¯¼æ–°æç¤ºè¯çš„ç”Ÿæˆã€‚åœ¨æ¶‰åŠDeepSeek-R1-Distill-Llama-8Bã€Qwen2.5-7B-Instructå’ŒGPT-4o-miniç­‰å¤šç§å¼€æºåŠé—­æºæ¨¡å‹çš„è·¨é¢†åŸŸå®éªŒä¸­ï¼ŒDelvePOçš„è¡¨ç°æŒç»­ä¼˜äºç°æœ‰çš„SOTAæ–¹æ³•ã€‚å®éªŒç»“æœå……åˆ†è¯æ˜äº†è¯¥æ¡†æ¶åœ¨æå‡Prompt Optimizationæœ‰æ•ˆæ€§çš„åŒæ—¶ï¼Œå…·å¤‡æå¼ºçš„transferabilityï¼Œä¸ºå¤„ç†å¤æ‚ä»»åŠ¡æä¾›äº†æ›´ç¨³å®šä¸”çµæ´»çš„ä¼˜åŒ–è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18257v1",
      "published_date": "2025-10-21 03:28:53 UTC",
      "updated_date": "2025-10-21 03:28:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:52:40.892528+00:00"
    },
    {
      "arxiv_id": "2510.18256v1",
      "title": "Hyperbolic Space Learning Method Leveraging Temporal Motion Priors for Human Mesh Recovery",
      "title_zh": "ç»“åˆæ—¶åºè¿åŠ¨å…ˆéªŒçš„åŒæ›²ç©ºé—´äººä½“ç½‘æ ¼é‡å»ºå­¦ä¹ æ–¹æ³•",
      "authors": [
        "Xiang Zhang",
        "Suping Wu",
        "Weibin Qiu",
        "Zhaocheng Jin",
        "Sheng Yang"
      ],
      "abstract": "3D human meshes show a natural hierarchical structure (like torso-limbs-fingers). But existing video-based 3D human mesh recovery methods usually learn mesh features in Euclidean space. It's hard to catch this hierarchical structure accurately. So wrong human meshes are reconstructed. To solve this problem, we propose a hyperbolic space learning method leveraging temporal motion prior for recovering 3D human meshes from videos. First, we design a temporal motion prior extraction module. This module extracts the temporal motion features from the input 3D pose sequences and image feature sequences respectively. Then it combines them into the temporal motion prior. In this way, it can strengthen the ability to express features in the temporal motion dimension. Since data representation in non-Euclidean space has been proved to effectively capture hierarchical relationships in real-world datasets (especially in hyperbolic space), we further design a hyperbolic space optimization learning strategy. This strategy uses the temporal motion prior information to assist learning, and uses 3D pose and pose motion information respectively in the hyperbolic space to optimize and learn the mesh features. Then, we combine the optimized results to get an accurate and smooth human mesh. Besides, to make the optimization learning process of human meshes in hyperbolic space stable and effective, we propose a hyperbolic mesh optimization loss. Extensive experimental results on large publicly available datasets indicate superiority in comparison with most state-of-the-art.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ 3D human meshes å›ºæœ‰çš„è‡ªç„¶å±‚æ¬¡ç»“æ„ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆ temporal motion priors çš„ hyperbolic space å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ Euclidean space æ–¹æ³•å› éš¾ä»¥æ•æ‰å±‚æ¬¡å…³ç³»è€Œå¯¼è‡´é‡å»ºç²¾åº¦ä¸è¶³çš„é—®é¢˜ã€‚ç ”ç©¶é€šè¿‡ temporal motion prior æå–æ¨¡å—èåˆ 3D pose åºåˆ—ä¸å›¾åƒç‰¹å¾ï¼Œå¢å¼ºäº†æ—¶é—´è¿åŠ¨ç»´åº¦çš„ç‰¹å¾è¡¨è¾¾èƒ½åŠ›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè®¾è®¡äº† hyperbolic space ä¼˜åŒ–å­¦ä¹ ç­–ç•¥ï¼Œåˆ©ç”¨è¿åŠ¨å…ˆéªŒåœ¨ hyperbolic space ä¸­ä¼˜åŒ– 3D pose å’Œ pose motion ä¿¡æ¯ï¼Œä»¥è·å–æ›´å‡†ç¡®çš„ç½‘æ ¼ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº† hyperbolic mesh optimization lossï¼Œç¡®ä¿äº†åœ¨ hyperbolic space ä¸­å­¦ä¹ è¿‡ç¨‹çš„ç¨³å®šæ€§ä¸æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„ state-of-the-art æ–¹æ³•ï¼Œå®ç°äº†æ›´ç²¾ç¡®ä¸”å¹³æ»‘çš„ 3D äººä½“ç½‘æ ¼é‡å»ºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICME2025",
      "pdf_url": "https://arxiv.org/pdf/2510.18256v1",
      "published_date": "2025-10-21 03:26:27 UTC",
      "updated_date": "2025-10-21 03:26:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:52:49.414198+00:00"
    },
    {
      "arxiv_id": "2510.18254v2",
      "title": "Illusions of reflection: open-ended task reveals systematic failures in Large Language Models' reflective reasoning",
      "title_zh": "åæ€çš„å¹»è±¡ï¼šå¼€æ”¾å¼ä»»åŠ¡æ­ç¤ºå¤§è¯­è¨€æ¨¡å‹åæ€æ¨ç†çš„ç³»ç»Ÿæ€§å¤±æ•ˆ",
      "authors": [
        "Sion Weatherhead",
        "Flora Salim",
        "Aaron Belbasis"
      ],
      "abstract": "Humans do not just find mistakes after the fact -- we often catch them mid-stream because 'reflection' is tied to the goal and its constraints. Today's large language models produce reasoning tokens and 'reflective' text, but is it functionally equivalent with human reflective reasoning? Prior work on closed-ended tasks -- with clear, external 'correctness' signals -- can make 'reflection' look effective while masking limits in self-correction. We therefore test eight frontier models on a simple, real-world task that is open-ended yet rule-constrained, with auditable success criteria: to produce valid scientific test items, then revise after considering their own critique. First-pass performance is poor (often zero valid items out of 4 required; mean $\\approx$ 1), and reflection yields only modest gains (also $\\approx$ 1). Crucially, the second attempt frequently repeats the same violation of constraint, indicating 'corrective gains' arise largely from chance production of a valid item rather than error detection and principled, constraint-sensitive repair. Performance before and after reflection deteriorates as open-endedness increases, and models marketed for 'reasoning' show no advantage. Our results suggest that current LLM 'reflection' lacks functional evidence of the active, goal-driven monitoring that helps humans respect constraints even on a first pass. Until such mechanisms are instantiated in the model itself, reliable performance requires external structure that enforces constraints. Our code is available at: https://github.com/cruiseresearchgroup/LLM_ReflectionTest",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Models, LLMsï¼‰çš„åæ€æ¨ç†ï¼ˆreflective reasoningï¼‰èƒ½åŠ›ï¼Œæ­ç¤ºäº†å…¶åœ¨å¤„ç†å¼€æ”¾å¼ä½†å—è§„åˆ™çº¦æŸçš„ä»»åŠ¡æ—¶å­˜åœ¨çš„ç³»ç»Ÿæ€§ç¼ºé™·ã€‚é€šè¿‡æµ‹è¯•å…«ä¸ªå‰æ²¿æ¨¡å‹åœ¨ç”Ÿæˆç§‘å­¦æµ‹è¯•é¡¹ç›®åŠåç»­è‡ªæˆ‘ä¿®è®¢ä¸­çš„è¡¨ç°ï¼Œç ”ç©¶å‘ç°æ¨¡å‹çš„åˆå§‹è¡¨ç°æå·®ï¼Œä¸”é€šè¿‡è‡ªæˆ‘åæ€è·å¾—çš„æ”¹è¿›éå¸¸æœ‰é™ã€‚å…³é”®è¯æ®æ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨ä¿®è®¢æ—¶å¾€å¾€ä¼šé‡å¤ä¹‹å‰çš„é”™è¯¯ï¼Œè¿™è¡¨æ˜å…¶æ€§èƒ½æå‡æ›´å¤šæºäºéšæœºäº§ç”Ÿçš„æ­£ç¡®è¾“å‡ºï¼Œè€ŒéåŸºäºå¯¹çº¦æŸæ¡ä»¶çš„é€»è¾‘ç†è§£æˆ–åŸåˆ™æ€§ä¿®å¤ã€‚éšç€ä»»åŠ¡å¼€æ”¾ç¨‹åº¦çš„æé«˜ï¼Œæ¨¡å‹è¡¨ç°è¿›ä¸€æ­¥æ¶åŒ–ï¼Œç”šè‡³è¢«å† ä»¥æ¨ç†æ ‡ç­¾çš„æ¨¡å‹ä¹Ÿæœªè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚ç ”ç©¶ç»“è®ºè®¤ä¸ºï¼Œç›®å‰çš„LLMåæ€ç¼ºä¹äººç±»é‚£ç§ä¸»åŠ¨ä¸”ç›®æ ‡é©±åŠ¨çš„ç›‘æ§æœºåˆ¶ï¼Œåœ¨æ¨¡å‹å†…éƒ¨å»ºç«‹èµ·çœŸæ­£çš„åæ€æœºåˆ¶ä¹‹å‰ï¼Œå®ç°å¯é æ€§èƒ½ä»éœ€ä¾èµ–å¤–éƒ¨ç»“æ„æ¥å¼ºåˆ¶æ‰§è¡Œçº¦æŸã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Currently under review",
      "pdf_url": "https://arxiv.org/pdf/2510.18254v2",
      "published_date": "2025-10-21 03:24:21 UTC",
      "updated_date": "2025-10-23 01:36:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:52:48.797418+00:00"
    },
    {
      "arxiv_id": "2510.21805v1",
      "title": "DiffGRM: Diffusion-based Generative Recommendation Model",
      "title_zh": "DiffGRMï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå¼æ¨èæ¨¡å‹",
      "authors": [
        "Zhao Liu",
        "Yichen Zhu",
        "Yiqing Yang",
        "Guoping Tang",
        "Rui Huang",
        "Qiang Luo",
        "Xiao Lv",
        "Ruiming Tang",
        "Kun Gai",
        "Guorui Zhou"
      ],
      "abstract": "Generative recommendation (GR) is an emerging paradigm that represents each item via a tokenizer as an n-digit semantic ID (SID) and predicts the next item by autoregressively generating its SID conditioned on the user's history. However, two structural properties of SIDs make ARMs ill-suited. First, intra-item consistency: the n digits jointly specify one item, yet the left-to-right causality trains each digit only under its prefix and blocks bidirectional cross-digit evidence, collapsing supervision to a single causal path. Second, inter-digit heterogeneity: digits differ in semantic granularity and predictability, while the uniform next-token objective assigns equal weight to all digits, overtraining easy digits and undertraining hard digits. To address these two issues, we propose DiffGRM, a diffusion-based GR model that replaces the autoregressive decoder with a masked discrete diffusion model (MDM), thereby enabling bidirectional context and any-order parallel generation of SID digits for recommendation. Specifically, we tailor DiffGRM in three aspects: (1) tokenization with Parallel Semantic Encoding (PSE) to decouple digits and balance per-digit information; (2) training with On-policy Coherent Noising (OCN) that prioritizes uncertain digits via coherent masking to concentrate supervision on high-value signals; and (3) inference with Confidence-guided Parallel Denoising (CPD) that fills higher-confidence digits first and generates diverse Top-K candidates. Experiments show consistent gains over strong generative and discriminative recommendation baselines on multiple datasets, improving NDCG@10 by 6.9%-15.5%. Code is available at https://github.com/liuzhao09/DiffGRM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DiffGRMï¼Œä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹(Diffusion-based)çš„ç”Ÿæˆå¼æ¨èæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç”Ÿæˆå¼æ¨è(GR)ä¸­è‡ªå›å½’æ¨¡å‹(ARMs)åœ¨å¤„ç†è¯­ä¹‰ID(SIDs)æ—¶å­˜åœ¨çš„å†…éƒ¨ä¸€è‡´æ€§ç¼ºå¤±å’Œä½é—´å¼‚æ„æ€§ç­‰é—®é¢˜ã€‚æ¨¡å‹é‡‡ç”¨é®è”½ç¦»æ•£æ‰©æ•£æ¨¡å‹(MDM)å–ä»£è‡ªå›å½’è§£ç å™¨ï¼Œå®ç°äº†å¯¹è¯­ä¹‰IDä½æ•°çš„åŒå‘ä¸Šä¸‹æ–‡ç†è§£å’Œä»»æ„é¡ºåºçš„å¹¶è¡Œç”Ÿæˆã€‚DiffGRMé€šè¿‡å¹¶è¡Œè¯­ä¹‰ç¼–ç (PSE)å¯¹é¡¹ç›®è¿›è¡Œåˆ†è¯ä»¥å®ç°ä½æ•°è§£è€¦å’Œä¿¡æ¯å¹³è¡¡ï¼Œå¹¶ç»“åˆç­–ç•¥æ€§ä¸€è‡´åŠ å™ª(OCN)åœ¨è®­ç»ƒä¸­é‡ç‚¹å…³æ³¨é«˜ä»·å€¼çš„ä¸ç¡®å®šæ€§ä¿¡å·ã€‚æ¨ç†é˜¶æ®µå¼•å…¥çš„ç½®ä¿¡åº¦å¼•å¯¼å¹¶è¡Œå»å™ª(CPD)æŠ€æœ¯èƒ½ä¼˜å…ˆå¡«å……é«˜ç½®ä¿¡åº¦ä½ï¼Œæœ‰æ•ˆæå‡äº†Top-Kæ¨èçš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ã€‚å¤šé¡¹æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDiffGRMç›¸æ¯”å¼ºåŸºçº¿æ¨¡å‹åœ¨NDCG@10æŒ‡æ ‡ä¸Šå®ç°äº†6.9%-15.5%çš„æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨ç”Ÿæˆå¼æ¨èé¢†åŸŸçš„ä¼˜è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "13 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.21805v1",
      "published_date": "2025-10-21 03:23:32 UTC",
      "updated_date": "2025-10-21 03:23:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:52:50.887150+00:00"
    },
    {
      "arxiv_id": "2510.18252v1",
      "title": "Finding the Sweet Spot: Optimal Data Augmentation Ratio for Imbalanced Credit Scoring Using ADASYN",
      "title_zh": "å¯»æ‰¾æœ€ä½³å¹³è¡¡ç‚¹ï¼šåŸºäº ADASYN çš„ä¸å¹³è¡¡ä¿¡ç”¨è¯„åˆ†æœ€ä¼˜æ•°æ®å¢å¼ºæ¯”ä¾‹",
      "authors": [
        "Luis H. Chia"
      ],
      "abstract": "Credit scoring models face a critical challenge: severe class imbalance, with default rates typically below 10%, which hampers model learning and predictive performance. While synthetic data augmentation techniques such as SMOTE and ADASYN have been proposed to address this issue, the optimal augmentation ratio remains unclear, with practitioners often defaulting to full balancing (1:1 ratio) without empirical justification.\n  This study systematically evaluates 10 data augmentation scenarios using the Give Me Some Credit dataset (97,243 observations, 7% default rate), comparing SMOTE, BorderlineSMOTE, and ADASYN at different multiplication factors (1x, 2x, 3x). All models were trained using XGBoost and evaluated on a held-out test set of 29,173 real observations. Statistical significance was assessed using bootstrap testing with 1,000 iterations.\n  Key findings reveal that ADASYN with 1x multiplication (doubling the minority class) achieved optimal performance with AUC of 0.6778 and Gini coefficient of 0.3557, representing statistically significant improvements of +0.77% and +3.00% respectively (p = 0.017, bootstrap test). Higher multiplication factors (2x and 3x) resulted in performance degradation, with 3x showing a -0.48% decrease in AUC, suggesting a \"law of diminishing returns\" for synthetic oversampling. The optimal class imbalance ratio was found to be 6.6:1 (majority:minority), contradicting the common practice of balancing to 1:1.\n  This work provides the first empirical evidence of an optimal \"sweet spot\" for data augmentation in credit scoring, with practical guidelines for industry practitioners and researchers working with imbalanced datasets. While demonstrated on a single representative dataset, the methodology provides a reproducible framework for determining optimal augmentation ratios in other imbalanced domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¿¡ç”¨è¯„åˆ†(Credit scoring)æ¨¡å‹ä¸­ä¸¥é‡çš„ç±»ä¸å¹³è¡¡(class imbalance)é—®é¢˜ï¼Œç³»ç»Ÿæ¢è®¨äº†åˆæˆæ•°æ®å¢å¼ºæŠ€æœ¯åœ¨æå‡é¢„æµ‹æ€§èƒ½æ–¹é¢çš„æœ€ä¼˜æ¯”ä¾‹ã€‚ç ”ç©¶é€šè¿‡\"Give Me Some Credit\"æ•°æ®é›†ï¼Œå¯¹æ¯”è¯„ä¼°äº†SMOTEã€BorderlineSMOTEå’ŒADASYNåœ¨ä¸åŒå¢å¼ºå€æ•°ï¼ˆ1xã€2xã€3xï¼‰ä¸‹çš„è¡¨ç°ï¼Œå¹¶åˆ©ç”¨XGBoostæ¨¡å‹å’Œè‡ªåŠ©æ³•(bootstrap testing)éªŒè¯ç»“æœã€‚å®éªŒå‘ç°ï¼ŒADASYNåœ¨1xå€æ•°ï¼ˆå³å°‘æ•°ç±»ç¿»å€ï¼‰æ—¶è¾¾åˆ°æœ€ä¼˜æ€§èƒ½ï¼Œå…¶AUCå’ŒåŸºå°¼ç³»æ•°(Gini coefficient)åˆ†åˆ«æå‡è‡³0.6778å’Œ0.3557ï¼Œå…·æœ‰ç»Ÿè®¡å­¦æ˜¾è‘—æ€§ã€‚ç ”ç©¶è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œè¿‡é«˜çš„å¢å¼ºå€æ•°ï¼ˆ2xæˆ–3xï¼‰ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œæ­ç¤ºäº†åˆæˆè¿‡é‡‡æ ·ä¸­å­˜åœ¨â€œæ”¶ç›Šé€’å‡è§„å¾‹â€ã€‚æœ€ç»ˆç¡®å®šçš„æœ€ä¼˜ç±»æ¯”ä¾‹ä¸º6.6:1ï¼ˆå¤šæ•°ç±»ä¸å°‘æ•°ç±»ä¹‹æ¯”ï¼‰ï¼Œè¿™ä¸€å‘ç°æœ‰åŠ›åœ°æŒ‘æˆ˜äº†ä¸šç•Œå¸¸ç”¨çš„1:1å…¨å¹³è¡¡åšæ³•ã€‚è¯¥å·¥ä½œä¸ºä¿¡ç”¨è¯„åˆ†é¢†åŸŸçš„æ•°æ®å¢å¼ºæä¾›äº†é¦–ä¸ªâ€œé»„é‡‘æ¯”ä¾‹(sweet spot)â€å®è¯è¯æ®ï¼Œå¹¶ä¸ºå¤„ç†ä¸å¹³è¡¡æ•°æ®é›†çš„ä»ä¸šè€…æä¾›äº†å…·æœ‰å¯æ“ä½œæ€§çš„å®è·µæŒ‡å—ã€‚",
      "categories": [
        "stat.AP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.AP",
      "comment": "25 pages, 3 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.18252v1",
      "published_date": "2025-10-21 03:22:43 UTC",
      "updated_date": "2025-10-21 03:22:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:53:02.797997+00:00"
    },
    {
      "arxiv_id": "2510.18250v1",
      "title": "ssToken: Self-modulated and Semantic-aware Token Selection for LLM Fine-tuning",
      "title_zh": "ssTokenï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒçš„è‡ªè°ƒåˆ¶ä¸è¯­ä¹‰æ„ŸçŸ¥è¯å…ƒé€‰æ‹©",
      "authors": [
        "Xiaohan Qin",
        "Xiaoxing Wang",
        "Ning Liao",
        "Cancheng Zhang",
        "Xiangdong Zhang",
        "Mingquan Feng",
        "Jingzhi Wang",
        "Junchi Yan"
      ],
      "abstract": "Data quality plays a critical role in enhancing supervised fine-tuning (SFT) for large language models (LLMs), and token-level data selection has emerged as a promising direction for its fine-grained nature. Despite their strong empirical performance, existing token-level selection methods share two key limitations: (1) requiring training or accessing an additional reference model, and (2) relying solely on loss information for token selection, which cannot well preserve semantically important tokens that are not favored by loss-based metrics. To address these challenges, we propose ssToken, a Self-modulated and Semantic-aware Token Selection approach. ssToken leverages readily accessible history models to compute the per-token loss difference with the current model, which serves as a self-modulated signal that enables the model to adaptively select tokens along its optimization trajectory, rather than relying on excess loss from an offline-trained reference model as in prior works. We further introduce a semantic-aware, attention-based token importance estimation metric, orthogonal to loss-based selection and providing complementary semantic information for more effective filtering. Extensive experiments across different model families and scales demonstrate that both self-modulated selection and semantic-aware selection alone outperform full-data fine-tuning, while their integration--ssToken--achieves synergistic gains and further surpasses prior token-level selection methods, delivering performance improvements while maintaining training efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ssTokenï¼Œä¸€ç§ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) æœ‰ç›‘ç£å¾®è°ƒ (SFT) çš„è‡ªè°ƒåˆ¶ä¸”è¯­ä¹‰æ„ŸçŸ¥ Token é€‰æ‹©æ–¹æ³•ã€‚é’ˆå¯¹ç°æœ‰ token-level é€‰æ‹©æ–¹æ³•ä¾èµ–å¤–éƒ¨å‚è€ƒæ¨¡å‹ä¸”ä»…åŸºäº loss ä¿¡æ¯çš„å±€é™ï¼ŒssToken åˆ©ç”¨å†å²æ¨¡å‹è®¡ç®— per-token loss å·®å¼‚ä½œä¸ºè‡ªè°ƒåˆ¶ä¿¡å·ï¼Œä½¿æ¨¡å‹èƒ½æ²¿ä¼˜åŒ–è½¨è¿¹è‡ªé€‚åº”é€‰æ‹© tokenã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†åŸºäº attention çš„è¯­ä¹‰æ„ŸçŸ¥é‡è¦æ€§è¯„ä¼°æŒ‡æ ‡ï¼Œä¸º token è¿‡æ»¤æä¾›äº’è¡¥çš„è¯­ä¹‰ä¿¡æ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒssToken åœ¨ä¸åŒæ¨¡å‹å®¶æ—å’Œè§„æ¨¡ä¸Šå‡ä¼˜äºå…¨é‡æ•°æ®å¾®è°ƒåŠå…ˆå‰çš„ token-level é€‰æ‹©æ–¹æ³•ã€‚è¿™ç§é›†æˆç­–ç•¥ä¸ä»…å®ç°äº†ååŒå¢ç›Šï¼Œè¿˜åœ¨ä¿æŒè®­ç»ƒæ•ˆç‡çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18250v1",
      "published_date": "2025-10-21 03:21:04 UTC",
      "updated_date": "2025-10-21 03:21:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:53:05.393601+00:00"
    },
    {
      "arxiv_id": "2510.18245v1",
      "title": "Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs",
      "title_zh": "ç¼©æ”¾å®šå¾‹é‡ä¸Šæ¨¡å‹æ¶æ„ï¼šè¿ˆå‘æ¨ç†é«˜æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Song Bian",
        "Tao Yu",
        "Shivaram Venkataraman",
        "Youngsuk Park"
      ],
      "abstract": "Scaling the number of parameters and the size of training data has proven to be an effective strategy for improving large language model (LLM) performance. Yet, as these models grow increasingly powerful and widely deployed, the cost of inference has become a pressing concern. Despite its importance, the trade-off between model accuracy and inference efficiency remains underexplored. In this work, we examine how key architectural factors, hidden size, the allocation of parameters between MLP and attention (mlp-to-attention ratio), and grouped-query attention (GQA), influence both inference cost and accuracy. We introduce a conditional scaling law that augments the Chinchilla framework with architectural information, along with a search framework for identifying architectures that are simultaneously inference-efficient and accurate. To validate our approach, we train more than 200 models spanning 80M to 3B parameters and 8B to 100B training tokens, and fit the proposed conditional scaling law. Our results show that the conditional scaling law reliably predicts optimal architectural choices and that the resulting models outperform existing open-source baselines. Under the same training budget, optimized architectures achieve up to 2.1% higher accuracy and 42% greater inference throughput compared to LLaMA-3.2.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹(LLMs)è§„æ¨¡æ‰©å¤§çš„èƒŒæ™¯ä¸‹ï¼Œå¦‚ä½•åœ¨æ¨¡å‹å‡†ç¡®æ€§ä¸æ¨ç†æ•ˆç‡(inference efficiency)ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ç ”ç©¶é‡ç‚¹åˆ†æäº† hidden sizeã€MLP-to-attention ratio ä»¥åŠ Grouped-Query Attention (GQA) ç­‰å…³é”®æ¶æ„å› ç´ å¯¹æ¨ç†æˆæœ¬çš„å½±å“ï¼Œå¹¶æå‡ºäº†ä¸€ç§å°†æ¶æ„ä¿¡æ¯æ•´åˆè¿› Chinchilla æ¡†æ¶çš„æ¡ä»¶ç¼©æ”¾æ³•åˆ™(conditional scaling law)ã€‚é€šè¿‡è¯¥æ³•åˆ™åŠå…¶é…å¥—çš„æœç´¢æ¡†æ¶ï¼Œç ”ç©¶è€…èƒ½å¤Ÿè¯†åˆ«å‡ºæ—¢é«˜æ•ˆåˆå‡†ç¡®çš„æœ€ä¼˜æ¶æ„ã€‚å®éªŒä¸­è®­ç»ƒäº†è¶…è¿‡ 200 ä¸ªè§„æ¨¡ä» 80M åˆ° 3B å‚æ•°çš„æ¨¡å‹ï¼ŒéªŒè¯äº†è¯¥æ³•åˆ™åœ¨é¢„æµ‹æ¶æ„é€‰æ‹©æ–¹é¢çš„å¯é æ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰å¼€æºåŸºçº¿ï¼Œåœ¨ç›¸åŒè®­ç»ƒé¢„ç®—ä¸‹ï¼Œä¼˜åŒ–åçš„æ¶æ„æ¯” LLaMA-3.2 å‡†ç¡®ç‡æé«˜ 2.1%ï¼Œä¸”æ¨ç†ååé‡(inference throughput)æå‡äº† 42%ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "27 pages, 17 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.18245v1",
      "published_date": "2025-10-21 03:08:48 UTC",
      "updated_date": "2025-10-21 03:08:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:53:14.677216+00:00"
    },
    {
      "arxiv_id": "2510.18224v1",
      "title": "EVER: Edge-Assisted Auto-Verification for Mobile MR-Aided Operation",
      "title_zh": "EVERï¼šé¢å‘ç§»åŠ¨ç«¯ MR è¾…åŠ©æ“ä½œçš„è¾¹ç¼˜è¾…åŠ©è‡ªåŠ¨éªŒè¯ç³»ç»Ÿ",
      "authors": [
        "Jiangong Chen",
        "Mingyu Zhu",
        "Bin Li"
      ],
      "abstract": "Mixed Reality (MR)-aided operation overlays digital objects on the physical world to provide a more immersive and intuitive operation process. A primary challenge is the precise and fast auto-verification of whether the user follows MR guidance by comparing frames before and after each operation. The pre-operation frame includes virtual guiding objects, while the post-operation frame contains physical counterparts. Existing approaches fall short of accounting for the discrepancies between physical and virtual objects due to imperfect 3D modeling or lighting estimation. In this paper, we propose EVER: an edge-assisted auto-verification system for mobile MR-aided operations. Unlike traditional frame-based similarity comparisons, EVER leverages the segmentation model and rendering pipeline adapted to the unique attributes of frames with physical pieces and those with their virtual counterparts; it adopts a threshold-based strategy using Intersection over Union (IoU) metrics for accurate auto-verification. To ensure fast auto-verification and low energy consumption, EVER offloads compute-intensive tasks to an edge server. Through comprehensive evaluations of public datasets and custom datasets with practical implementation, EVER achieves over 90% verification accuracy within 100 milliseconds (significantly faster than average human reaction time of approximately 273 milliseconds), while consuming only minimal additional computational resources and energy compared to a system without auto-verification.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ··åˆç°å® (Mixed Reality, MR) è¾…åŠ©æ“ä½œä¸­éš¾ä»¥ç²¾ç¡®ä¸”å¿«é€Ÿåœ°è‡ªåŠ¨éªŒè¯ç”¨æˆ·æ˜¯å¦éµå¾ªæŒ‡å¯¼çš„é—®é¢˜ï¼Œæå‡ºäº† EVER ç³»ç»Ÿã€‚ç”±äº 3D å»ºæ¨¡å’Œå…‰ç…§ä¼°è®¡çš„ä¸å®Œå–„ï¼Œä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†ç‰©ç†å¯¹è±¡ä¸è™šæ‹Ÿå¯¹åº”ç‰©çš„å·®å¼‚æ—¶è¡¨ç°æ¬ ä½³ã€‚EVER åˆ©ç”¨é’ˆå¯¹ç‰©ç†å’Œè™šæ‹Ÿå¸§å±æ€§ä¼˜åŒ–çš„åˆ†å‰²æ¨¡å‹ (segmentation model) ä¸æ¸²æŸ“ç®¡çº¿ (rendering pipeline)ï¼Œå¹¶ç»“åˆäº¤å¹¶æ¯” (Intersection over Union, IoU) æŒ‡æ ‡çš„é˜ˆå€¼ç­–ç•¥å®ç°ç²¾ç¡®éªŒè¯ã€‚ä¸ºä¿éšœä½å»¶è¿Ÿä¸ä½åŠŸè€—ï¼Œç³»ç»Ÿå°†è®¡ç®—å¯†é›†å‹ä»»åŠ¡å¸è½½è‡³è¾¹ç¼˜æœåŠ¡å™¨ (edge server)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEVER åœ¨ 100 æ¯«ç§’å†…è¾¾åˆ°äº† 90% ä»¥ä¸Šçš„éªŒè¯å‡†ç¡®ç‡ï¼Œå…¶é€Ÿåº¦ä¼˜äºäººç±»ååº”æ—¶é—´ï¼Œä¸”ä»…äº§ç”Ÿæä½çš„é¢å¤–è®¡ç®—ä¸èƒ½é‡å¼€é”€ã€‚",
      "categories": [
        "cs.MM",
        "cs.AI"
      ],
      "primary_category": "cs.MM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18224v1",
      "published_date": "2025-10-21 02:12:34 UTC",
      "updated_date": "2025-10-21 02:12:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:53:12.395963+00:00"
    },
    {
      "arxiv_id": "2510.18221v3",
      "title": "The Emergence of Complex Behavior in Large-Scale Ecological Environments",
      "title_zh": "å¤§è§„æ¨¡ç”Ÿæ€ç¯å¢ƒä¸‹å¤æ‚è¡Œä¸ºçš„æ¶Œç°",
      "authors": [
        "Joseph Bejjani",
        "Chase Van Amburg",
        "Chengrui Wang",
        "Chloe Huangyuan Su",
        "Sarah M. Pratt",
        "Yasin Mazloumi",
        "Naeem Khoshnevis",
        "Sham M. Kakade",
        "KiantÃ© Brantley",
        "Aaron Walsman"
      ],
      "abstract": "We explore how physical scale and population size shape the emergence of complex behaviors in open-ended ecological environments. In our setting, agents are unsupervised and have no explicit rewards or learning objectives but instead evolve over time according to reproduction, mutation, and selection. As they act, agents also shape their environment and the population around them in an ongoing dynamic ecology. Our goal is not to optimize a single high-performance policy, but instead to examine how behaviors emerge and evolve across large populations due to natural competition and environmental pressures. We use modern hardware along with a new multi-agent simulator to scale the environment and population to sizes much larger than previously attempted, reaching populations of over 60,000 agents, each with their own evolved neural network policy. We identify various emergent behaviors such as long-range resource extraction, vision-based foraging, and predation that arise under competitive and survival pressures. We examine how sensing modalities and environmental scale affect the emergence of these behaviors and find that some of them appear only in sufficiently large environments and populations, and that larger scales increase the stability and consistency of these emergent behaviors. While there is a rich history of research in evolutionary settings, our scaling results on modern hardware provide promising new directions to explore ecology as an instrument of machine learning in an era of increasingly abundant computational resources and efficient machine frameworks. Experimental code is available at https://github.com/jbejjani2022/ecological-emergent-behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢ç´¢äº†åœ¨å¼€æ”¾å¼ç”Ÿæ€ç¯å¢ƒä¸­ï¼Œç‰©ç†è§„æ¨¡å’Œç§ç¾¤å¤§å°å¦‚ä½•å½±å“å¤æ‚è¡Œä¸ºçš„æ¶Œç°(Emergence)ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨æ–°å‹å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿå™¨ï¼Œåœ¨ç°ä»£ç¡¬ä»¶ä¸Šå°†ç§ç¾¤è§„æ¨¡æ‰©å±•åˆ°è¶…è¿‡60,000ä¸ªå…·æœ‰ç‹¬ç«‹ç¥ç»ç½‘ç»œç­–ç•¥çš„æ™ºèƒ½ä½“ï¼Œé€šè¿‡ç¹æ®–ã€å˜å¼‚å’Œé€‰æ‹©(Selection)çš„è‡ªç„¶æ¼”åŒ–å‹åŠ›é©±åŠ¨å…¶è¡Œä¸ºæ¼”å˜ã€‚å®éªŒæˆåŠŸè¯†åˆ«å‡ºé•¿ç¨‹èµ„æºæå–(Long-range resource extraction)ã€åŸºäºè§†è§‰çš„è§…é£Ÿ(Vision-based foraging)ä»¥åŠæ•é£Ÿ(Predation)ç­‰å¤šç§æ¶Œç°è¡Œä¸ºã€‚ç ”ç©¶å‘ç°ï¼ŒæŸäº›ç‰¹å®šå¤æ‚è¡Œä¸ºä»…åœ¨è¶³å¤Ÿå¤§çš„ç¯å¢ƒå’Œç§ç¾¤è§„æ¨¡ä¸‹æ‰ä¼šå‡ºç°ï¼Œä¸”è§„æ¨¡çš„æ‰©å¤§æ˜¾è‘—æå‡äº†è¿™äº›è¡Œä¸ºçš„ç¨³å®šæ€§å’Œä¸€è‡´æ€§ã€‚è¯¥å·¥ä½œæ­ç¤ºäº†ç”Ÿæ€ç«äº‰ä½œä¸ºæœºå™¨å­¦ä¹ å·¥å…·çš„æ½œåŠ›ï¼Œä¸ºåœ¨å¤§è®¡ç®—èµ„æºæ—¶ä»£æ¢ç´¢å¼€æ”¾å¼æ¼”åŒ–æä¾›äº†æ–°çš„æ–¹å‘ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.MA",
      "comment": "33 pages, 23 figures, 12 tables, experiment code available at https://github.com/jbejjani2022/ecological-emergent-behavior",
      "pdf_url": "https://arxiv.org/pdf/2510.18221v3",
      "published_date": "2025-10-21 02:03:25 UTC",
      "updated_date": "2025-12-12 15:48:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:53:16.277076+00:00"
    },
    {
      "arxiv_id": "2510.20846v1",
      "title": "This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN",
      "title_zh": "æ­¤è„‘ç”µå›¾ç±»æ¯”å½¼è„‘ç”µå›¾ï¼šåŸºäº ProtoEEG-kNN çš„å¯è§£é‡Šå‘ä½œé—´æœŸç™«ç—«æ ·æ”¾ç”µæ£€æµ‹",
      "authors": [
        "Dennis Tang",
        "Jon Donnelly",
        "Alina Jade Barnett",
        "Lesia Semenova",
        "Jin Jing",
        "Peter Hadar",
        "Ioannis Karakis",
        "Olga Selioutski",
        "Kehan Zhao",
        "M. Brandon Westover",
        "Cynthia Rudin"
      ],
      "abstract": "The presence of interictal epileptiform discharges (IEDs) in electroencephalogram (EEG) recordings is a critical biomarker of epilepsy. Even trained neurologists find detecting IEDs difficult, leading many practitioners to turn to machine learning for help. While existing machine learning algorithms can achieve strong accuracy on this task, most models are uninterpretable and cannot justify their conclusions. Absent the ability to understand model reasoning, doctors cannot leverage their expertise to identify incorrect model predictions and intervene accordingly. To improve the human-model interaction, we introduce ProtoEEG-kNN, an inherently interpretable model that follows a simple case-based reasoning process. ProtoEEG-kNN reasons by comparing an EEG to similar EEGs from the training set and visually demonstrates its reasoning both in terms of IED morphology (shape) and spatial distribution (location). We show that ProtoEEG-kNN can achieve state-of-the-art accuracy in IED detection while providing explanations that experts prefer over existing approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è„‘ç”µå›¾(EEG)ä¸­å‘ä½œé—´æœŸç™«ç—«æ ·æ”¾ç”µ(IEDs)æ£€æµ‹çš„é«˜éš¾åº¦ä»¥åŠç°æœ‰æœºå™¨å­¦ä¹ æ¨¡å‹ç¼ºä¹å¯è§£é‡Šæ€§çš„é—®é¢˜ï¼Œæå‡ºäº† ProtoEEG-kNN æ¨¡å‹ã€‚è¿™æ˜¯ä¸€ç§å…·æœ‰å†…åœ¨å¯è§£é‡Šæ€§çš„ç³»ç»Ÿï¼Œé‡‡ç”¨åŸºäºæ¡ˆä¾‹çš„æ¨ç†(case-based reasoning)æœºåˆ¶ï¼Œé€šè¿‡å°†ç›®æ ‡è„‘ç”µå›¾ä¸è®­ç»ƒé›†ä¸­çš„ç›¸ä¼¼æ ·æœ¬è¿›è¡Œå¯¹æ¯”æ¥è¾…åŠ©è¯Šæ–­ã€‚ProtoEEG-kNN èƒ½å¤Ÿä» IED çš„å½¢æ€å­¦(morphology)å’Œç©ºé—´åˆ†å¸ƒ(spatial distribution)ä¸¤ä¸ªç»´åº¦ç›´è§‚åœ°å±•ç¤ºå…¶æ¨ç†ä¾æ®ï¼Œå¢å¼ºäº†åŒ»ç”Ÿå¯¹æ¨¡å‹å†³ç­–çš„ç†è§£ä¸ä¿¡ä»»ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒ IED æ£€æµ‹ state-of-the-art å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå…¶æä¾›çš„è§£é‡Šæ–¹æ¡ˆæ¯”ç°æœ‰æ–¹æ³•æ›´å—ä¸“å®¶é’çã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆæ”¹å–„äº†äººæœºäº¤äº’ï¼Œä½¿åŒ»ç”Ÿèƒ½å¤Ÿç»“åˆä¸“ä¸šç»éªŒè¯†åˆ«å¹¶å¹²é¢„æ¨¡å‹çš„é”™è¯¯é¢„æµ‹ï¼Œä¸ºä¸´åºŠç™«ç—«è¯Šæ–­æä¾›äº†æ›´å¯é çš„è¾…åŠ©å·¥å…·ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "comment": "MICCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.20846v1",
      "published_date": "2025-10-21 01:58:34 UTC",
      "updated_date": "2025-10-21 01:58:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:53:19.101551+00:00"
    },
    {
      "arxiv_id": "2510.18214v2",
      "title": "VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety",
      "title_zh": "VLSUï¼šæ¢ç©¶é¢å‘äººå·¥æ™ºèƒ½å®‰å…¨çš„è”åˆå¤šæ¨¡æ€ç†è§£æé™",
      "authors": [
        "Shruti Palaskar",
        "Leon Gatys",
        "Mona Abdelrahman",
        "Mar Jacobo",
        "Larry Lindsey",
        "Rutika Moharir",
        "Gunnar Lund",
        "Yang Xu",
        "Navid Shiee",
        "Jeffrey Bigham",
        "Charles Maalouf",
        "Joseph Yitan Cheng"
      ],
      "abstract": "Safety evaluation of multimodal foundation models often treats vision and language inputs separately, missing risks from joint interpretation where benign content becomes harmful in combination. Existing approaches also fail to distinguish clearly unsafe content from borderline cases, leading to problematic over-blocking or under-refusal of genuinely harmful content. We present Vision Language Safety Understanding (VLSU), a comprehensive framework to systematically evaluate multimodal safety through fine-grained severity classification and combinatorial analysis across 17 distinct safety patterns. Using a multi-stage pipeline with real-world images and human annotation, we construct a large-scale benchmark of 8,187 samples spanning 15 harm categories. Our evaluation of eleven state-of-the-art models reveals systematic joint understanding failures: while models achieve 90%-plus accuracy on clear unimodal safety signals, performance degrades substantially to 20-55% when joint image-text reasoning is required to determine the safety label. Most critically, 34% of errors in joint image-text safety classification occur despite correct classification of the individual modalities, further demonstrating absent compositional reasoning capabilities. Additionally, we find that models struggle to balance refusing unsafe content while still responding to borderline cases that deserve engagement. For example, we find that instruction framing can reduce the over-blocking rate on borderline content from 62.4% to 10.4% in Gemini-1.5, but only at the cost of under-refusing on unsafe content with refusal rate dropping from 90.8% to 53.9%. Overall, our framework exposes weaknesses in joint image-text understanding and alignment gaps in current models, and provides a critical test bed to enable the next milestones in research on robust vision-language safety.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VLSU (Vision Language Safety Understanding) æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨è”åˆç†è§£ (joint multimodal understanding) æ–¹é¢çš„å®‰å…¨æ€§ï¼Œä»¥åº”å¯¹è‰¯æ€§å†…å®¹ç»„åˆåå¯èƒ½äº§ç”Ÿçš„æœ‰å®³é£é™©ã€‚ç ”ç©¶è€…é€šè¿‡ç»†ç²’åº¦çš„ä¸¥é‡æ€§åˆ†ç±»å’Œæ¶µç›– 17 ç§å®‰å…¨æ¨¡å¼çš„ç»„åˆåˆ†æï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å« 8,187 ä¸ªæ ·æœ¬ã€è·¨è¶Š 15 ä¸ªä¼¤å®³ç±»åˆ«çš„è§„æ¨¡åŒ–åŸºå‡†æµ‹è¯•ã€‚å¯¹ 11 ä¸ªä¸»æµæ¨¡å‹çš„è¯„ä¼°å‘ç°ï¼Œå°½ç®¡æ¨¡å‹åœ¨å•æ¨¡æ€ (unimodal) å®‰å…¨ä¿¡å·ä¸Šçš„å‡†ç¡®ç‡æ™®éè¶…è¿‡ 90%ï¼Œä½†åœ¨éœ€è¦è”åˆå›¾æ–‡æ¨ç†æ—¶ï¼Œæ€§èƒ½ä¼šå¤§å¹…ä¸‹é™è‡³ 20-55%ã€‚å®éªŒè¿›ä¸€æ­¥æ­ç¤ºæ¨¡å‹æ™®éç¼ºä¹ç»„åˆæ¨ç† (compositional reasoning) èƒ½åŠ›ï¼Œå…¶ä¸­ 34% çš„è”åˆåˆ†ç±»é”™è¯¯å‘ç”Ÿåœ¨å•æ¨¡æ€åˆ†ç±»æ­£ç¡®çš„å‰æä¸‹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨æ‹’ç»æœ‰å®³å†…å®¹ä¸å“åº”è¾¹ç•Œæ¡ˆä¾‹ (borderline cases) ä¹‹é—´éš¾ä»¥å–å¾—å¹³è¡¡ï¼Œå¸¸è¡¨ç°å‡ºä¸¥é‡çš„è¿‡åº¦æ‹¦æˆªæˆ–æ‹’ç»ä¸è¶³ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆåœ°æš´éœ²äº†å½“å‰æ¨¡å‹åœ¨è”åˆå›¾æ–‡ç†è§£å’Œå¯¹é½æ–¹é¢çš„çŸ­æ¿ï¼Œä¸ºå¼€å‘æ›´ç¨³å¥çš„è§†è§‰è¯­è¨€å®‰å…¨æŠ€æœ¯æä¾›äº†å…³é”®çš„å®éªŒå¹³å°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 5 figures, 4 tables, detailed appendix. Under review",
      "pdf_url": "https://arxiv.org/pdf/2510.18214v2",
      "published_date": "2025-10-21 01:30:31 UTC",
      "updated_date": "2025-12-03 08:09:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:53:23.275524+00:00"
    },
    {
      "arxiv_id": "2510.18212v3",
      "title": "A Definition of AGI",
      "title_zh": "AGI çš„å®šä¹‰",
      "authors": [
        "Dan Hendrycks",
        "Dawn Song",
        "Christian Szegedy",
        "Honglak Lee",
        "Yarin Gal",
        "Erik Brynjolfsson",
        "Sharon Li",
        "Andy Zou",
        "Lionel Levine",
        "Bo Han",
        "Jie Fu",
        "Ziwei Liu",
        "Jinwoo Shin",
        "Kimin Lee",
        "Mantas Mazeika",
        "Long Phan",
        "George Ingebretsen",
        "Adam Khoja",
        "Cihang Xie",
        "Olawale Salaudeen",
        "Matthias Hein",
        "Kevin Zhao",
        "Alexander Pan",
        "David Duvenaud",
        "Bo Li",
        "Steve Omohundro",
        "Gabriel Alfour",
        "Max Tegmark",
        "Kevin McGrew",
        "Gary Marcus",
        "Jaan Tallinn",
        "Eric Schmidt",
        "Yoshua Bengio"
      ],
      "abstract": "The lack of a concrete definition for Artificial General Intelligence (AGI) obscures the gap between today's specialized AI and human-level cognition. This paper introduces a quantifiable framework to address this, defining AGI as matching the cognitive versatility and proficiency of a well-educated adult. To operationalize this, we ground our methodology in Cattell-Horn-Carroll theory, the most empirically validated model of human cognition. The framework dissects general intelligence into ten core cognitive domains-including reasoning, memory, and perception-and adapts established human psychometric batteries to evaluate AI systems. Application of this framework reveals a highly \"jagged\" cognitive profile in contemporary models. While proficient in knowledge-intensive domains, current AI systems have critical deficits in foundational cognitive machinery, particularly long-term memory storage. The resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 57%) concretely quantify both rapid progress and the substantial gap remaining before AGI.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½é¢†åŸŸç¼ºä¹é€šç”¨äººå·¥æ™ºèƒ½(Artificial General Intelligence, AGI)å…·ä½“å®šä¹‰çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå¯é‡åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œå°†AGIå®šä¹‰ä¸ºå…·å¤‡ä¸å—è¿‡è‰¯å¥½æ•™è‚²çš„æˆäººç›¸å½“çš„è®¤çŸ¥å¤šåŠŸèƒ½æ€§ä¸ç†Ÿç»ƒåº¦ã€‚è¯¥æ¡†æ¶ä»¥äººç±»è®¤çŸ¥æ¨¡å‹Cattell-Horn-Carrollç†è®ºä¸ºåŸºç¡€ï¼Œå°†é€šç”¨æ™ºèƒ½åˆ’åˆ†ä¸ºæ¨ç†ã€è®°å¿†å’Œæ„ŸçŸ¥ç­‰åä¸ªæ ¸å¿ƒè®¤çŸ¥é¢†åŸŸï¼Œå¹¶æ”¹ç¼–å¿ƒç†æµ‹é‡é‡è¡¨æ¥è¯„ä¼°AIç³»ç»Ÿã€‚ç ”ç©¶å‘ç°å½“å‰æ¨¡å‹å‘ˆç°å‡ºé«˜åº¦â€œå‚å·®ä¸é½(jagged)â€çš„è®¤çŸ¥å‰–é¢ï¼Œè™½ç„¶åœ¨çŸ¥è¯†å¯†é›†å‹é¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨é•¿æœŸå­˜å‚¨(long-term memory storage)ç­‰åŸºç¡€è®¤çŸ¥æœºåˆ¶æ–¹é¢ä»å­˜åœ¨å…³é”®ç¼ºé™·ã€‚é€šè¿‡è¯¥æ¡†æ¶è®¡ç®—çš„AGIè¯„åˆ†æ˜¾ç¤ºï¼ŒGPT-4å¾—åˆ†ä¸º27%ï¼ŒGPT-5ä¸º57%ï¼Œè¿™ä¸ä»…é‡åŒ–äº†æŠ€æœ¯çš„é£é€Ÿè¿›æ­¥ï¼Œä¹Ÿç›´è§‚åœ°æ­ç¤ºäº†å®ç°çœŸæ­£AGIä¹‹å‰ä»éœ€è·¨è¶Šçš„å·¨å¤§é¸¿æ²Ÿã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18212v3",
      "published_date": "2025-10-21 01:28:35 UTC",
      "updated_date": "2025-12-03 00:35:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:53:25.301706+00:00"
    },
    {
      "arxiv_id": "2510.18905v3",
      "title": "3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and Latency",
      "title_zh": "AI æ¨ç†ç¼©æ”¾çš„ä¸‰ç»´ä¼˜åŒ–ï¼šå¹³è¡¡å‡†ç¡®åº¦ã€æˆæœ¬ä¸å»¶è¿Ÿ",
      "authors": [
        "Minseok Jung",
        "Abhas Ricky",
        "Muhammad Rameez Chatni"
      ],
      "abstract": "AI inference scaling is often tuned through 1D heuristics (a fixed reasoning pass) or 2D bivariate trade-offs (e.g., accuracy vs. compute), which fail to consider cost and latency constraints. We introduce a 3D optimization framework that jointly calibrates accuracy, cost, and latency within a unified decision space, enabling constraints-aware inference scaling. Using Monte Carlo simulations across three representative scenarios and nine simulated large language models, we evaluate four optimization methods to address the 3D multi-objective optimization (MOO) problem. Framing inference scaling in MOO shapes a feasible space that 1D and 2D optimizations fail to capture, enabling environment-adaptive selection of the inference scaling~$k$. Results show that knee-point optimization based on Pareto frontiers achieves the best balance, while accuracy-maximization remains favorable when accuracy is prioritized. Our results further show that smaller models, when combined with optimal inference scaling, can match or exceed the performance of larger models at a fraction of the cost. The framework establishes a theoretical foundation for deployment-aware inference scaling across diverse operational conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰AIæ¨ç†æ‰©å±•(AI inference scaling)ä¸»è¦ä¾èµ–1Då¯å‘å¼æˆ–2Dæƒè¡¡è€Œå¿½ç•¥æˆæœ¬ä¸å»¶è¿Ÿçº¦æŸçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„3D optimizationæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨ç»Ÿä¸€å†³ç­–ç©ºé—´å†…è”åˆæ ¡å‡†å‡†ç¡®åº¦(accuracy)ã€æˆæœ¬(cost)å’Œå»¶è¿Ÿ(latency)ï¼Œå®ç°äº†çº¦æŸæ„ŸçŸ¥çš„æ¨ç†æ‰©å±•ã€‚ç ”ç©¶è€…åœ¨ä¹ä¸ªæ¨¡æ‹Ÿçš„å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸Šåˆ©ç”¨Monte Carlo simulationsè¯„ä¼°äº†å››ç§å¤šç›®æ ‡ä¼˜åŒ–(MOO)æ–¹æ³•ã€‚ç»“æœæ˜¾ç¤ºï¼ŒåŸºäºPareto frontiersçš„æ‹ç‚¹ä¼˜åŒ–(knee-point optimization)åœ¨å„é¡¹æŒ‡æ ‡é—´è¾¾æˆäº†æœ€ä½³å¹³è¡¡ï¼Œå¹¶æ”¯æŒç¯å¢ƒè‡ªé€‚åº”çš„æ¨ç†æ‰©å±•å› å­$k$çš„é€‰æ‹©ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œè¾ƒå°çš„æ¨¡å‹åœ¨ç»“åˆæœ€ä¼˜æ¨ç†æ‰©å±•æ—¶ï¼Œèƒ½ä»¥æä½çš„æˆæœ¬è¾¾åˆ°ç”šè‡³è¶…è¿‡å¤§å‹æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶ä¸ºå¤æ‚è¿è¡Œæ¡ä»¶ä¸‹çš„éƒ¨ç½²æ„ŸçŸ¥(deployment-aware)æ¨ç†æ‰©å±•æä¾›äº†åšå®çš„ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18905v3",
      "published_date": "2025-10-21 01:03:46 UTC",
      "updated_date": "2025-11-16 00:34:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:53:29.400596+00:00"
    },
    {
      "arxiv_id": "2510.18196v1",
      "title": "Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge",
      "title_zh": "å¯¹æ¯”è§£ç ç¼“è§£ LLM-as-a-Judge ä¸­çš„è¯„åˆ†é‡ç¨‹åå·®",
      "authors": [
        "Yoshinari Fujinuma"
      ],
      "abstract": "Large Language Models (LLMs) are commonly used as evaluators in various applications, but the reliability of the outcomes remains a challenge. One such challenge is using LLMs-as-judges for direct assessment, i.e., assigning scores from a specified range without any references. We first show that this challenge stems from LLM judge outputs being associated with score range bias, i.e., LLM judge outputs are highly sensitive to pre-defined score ranges, preventing the search for optimal score ranges. We also show that similar biases exist among models from the same family. We then mitigate this bias through contrastive decoding, achieving up to 11.3% relative improvement on average in Spearman correlation with human judgments across different score ranges.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† LLM-as-a-Judge åœ¨ç›´æ¥è¯„ä¼°ä»»åŠ¡ä¸­çš„å¯é æ€§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹åœ¨æ— å‚è€ƒæƒ…å†µä¸‹å¯¹ç‰¹å®šèŒƒå›´è¿›è¡Œè¯„åˆ†çš„æƒ…æ™¯ã€‚ç ”ç©¶é¦–å…ˆæŒ‡å‡º LLM è¯„å®¡å‘˜æ™®éå­˜åœ¨ Score Range Biasï¼ˆè¯„åˆ†èŒƒå›´åå·®ï¼‰ï¼Œå³è¯„ä¼°ç»“æœå¯¹é¢„å®šä¹‰çš„è¯„åˆ†èŒƒå›´é«˜åº¦æ•æ„Ÿï¼Œè¿™é˜»ç¢äº†å¯¹æœ€ä¼˜è¯„åˆ†åŒºé—´çš„æ¢ç´¢ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œå³ä½¿åœ¨åŒç³»åˆ—æ¨¡å‹ä¹‹é—´ä¹Ÿå­˜åœ¨ç±»ä¼¼çš„åå·®ç°è±¡ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†åˆ©ç”¨ Contrastive Decodingï¼ˆå¯¹æ¯”è§£ç ï¼‰çš„ä¼˜åŒ–æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨ä¸åŒè¯„åˆ†èŒƒå›´å†…æ˜¾è‘—æå‡äº†æ¨¡å‹è¯„åˆ†ä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§ï¼Œå…¶ Spearman Correlationï¼ˆæ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°ï¼‰å¹³å‡å®ç°äº† 11.3% çš„ç›¸å¯¹æ”¹å–„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18196v1",
      "published_date": "2025-10-21 00:47:11 UTC",
      "updated_date": "2025-10-21 00:47:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:53:29.972509+00:00"
    },
    {
      "arxiv_id": "2510.18193v2",
      "title": "FST.ai 2.0: An Explainable AI Ecosystem for Fair, Fast, and Inclusive Decision-Making in Olympic and Paralympic Taekwondo",
      "title_zh": "FST.ai 2.0ï¼šé¢å‘å¥¥è¿ä¼šä¸æ®‹å¥¥ä¼šè·†æ‹³é“å…¬å¹³ã€å¿«é€ŸåŠåŒ…å®¹æ€§å†³ç­–çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½ç”Ÿæ€ç³»ç»Ÿ",
      "authors": [
        "Keivan Shariatmadar",
        "Ahmad Osman",
        "Ramin Ray",
        "Kisam Kim"
      ],
      "abstract": "Fair, transparent, and explainable decision-making remains a critical challenge in Olympic and Paralympic combat sports. This paper presents \\emph{FST.ai 2.0}, an explainable AI ecosystem designed to support referees, coaches, and athletes in real time during Taekwondo competitions and training. The system integrates {pose-based action recognition} using graph convolutional networks (GCNs), {epistemic uncertainty modeling} through credal sets, and {explainability overlays} for visual decision support. A set of {interactive dashboards} enables human--AI collaboration in referee evaluation, athlete performance analysis, and Para-Taekwondo classification. Beyond automated scoring, FST.ai~2.0 incorporates modules for referee training, fairness monitoring, and policy-level analytics within the World Taekwondo ecosystem. Experimental validation on competition data demonstrates an {85\\% reduction in decision review time} and {93\\% referee trust} in AI-assisted decisions. The framework thus establishes a transparent and extensible pipeline for trustworthy, data-driven officiating and athlete assessment. By bridging real-time perception, explainable inference, and governance-aware design, FST.ai~2.0 represents a step toward equitable, accountable, and human-aligned AI in sports.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† FST.ai 2.0ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºå¥¥è¿ä¼šå’Œæ®‹å¥¥ä¼šè·†æ‹³é“è®¾è®¡çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½ (Explainable AI) ç”Ÿæ€ç³»ç»Ÿï¼Œæ—¨åœ¨æ”¯æŒè£åˆ¤ã€æ•™ç»ƒå’Œè¿åŠ¨å‘˜å®ç°å…¬æ­£ã€å¿«é€Ÿå’ŒåŒ…å®¹çš„å®æ—¶å†³ç­–ã€‚è¯¥ç³»ç»Ÿé›†æˆäº†åŸºäºå›¾å·ç§¯ç½‘ç»œ (GCNs) çš„ pose-based action recognition æŠ€æœ¯ï¼Œå¹¶åˆ©ç”¨ credal sets è¿›è¡Œ epistemic uncertainty modelingï¼Œè¾…ä»¥ explainability overlays æä¾›ç›´è§‚çš„è§†è§‰å†³ç­–æ”¯æŒã€‚é€šè¿‡ interactive dashboardsï¼Œç³»ç»Ÿå®ç°äº†äººç±»ä¸ AI åœ¨è£åˆ¤è¯„ä¼°ã€è¿åŠ¨å‘˜è¡¨ç°åˆ†æå’Œæ®‹å¥¥è·†æ‹³é“åˆ†çº§ä¸­çš„é«˜æ•ˆåä½œã€‚æ­¤å¤–ï¼ŒFST.ai 2.0 è¿˜åŒ…å«äº†è£åˆ¤åŸ¹è®­ã€å…¬å¹³æ€§ç›‘æµ‹åŠä¸–ç•Œè·†æ‹³é“ç”Ÿæ€ç³»ç»Ÿå†…çš„æ”¿ç­–çº§åˆ†ææ¨¡å—ã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿå°†å†³ç­–å®¡æ ¸æ—¶é—´ç¼©çŸ­äº† 85%ï¼Œè£åˆ¤å¯¹ AI è¾…åŠ©å†³ç­–çš„ä¿¡ä»»åº¦é«˜è¾¾ 93%ã€‚è¿™ä¸€æ¡†æ¶ä¸ºä½“è‚²é¢†åŸŸä¸­å€¼å¾—ä¿¡èµ–çš„ã€æ•°æ®é©±åŠ¨çš„è£åˆ¤å’Œè¿åŠ¨å‘˜è¯„ä¼°å»ºç«‹äº†é€æ˜ä¸”å¯æ‰©å±•çš„æµç¨‹ï¼Œæ˜¯å®ç°ä½“è‚² AI å…¬å¹³æ€§ä¸é—®è´£åˆ¶çš„é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.18193v2",
      "published_date": "2025-10-21 00:35:56 UTC",
      "updated_date": "2025-10-22 05:00:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:53:44.191091+00:00"
    },
    {
      "arxiv_id": "2510.18188v1",
      "title": "RadDiagSeg-M: A Vision Language Model for Joint Diagnosis and Multi-Target Segmentation in Radiology",
      "title_zh": "RadDiagSeg-Mï¼šé¢å‘æ”¾å°„å­¦è”åˆè¯Šæ–­ä¸å¤šç›®æ ‡åˆ†å‰²çš„è§†è§‰è¯­è¨€æ¨¡å‹",
      "authors": [
        "Chengrun Li",
        "Corentin Royer",
        "Haozhe Luo",
        "Bastian Wittmann",
        "Xia Li",
        "Ibrahim Hamamci",
        "Sezgin Er",
        "Anjany Sekuboyina",
        "Bjoern Menze"
      ],
      "abstract": "Most current medical vision language models struggle to jointly generate diagnostic text and pixel-level segmentation masks in response to complex visual questions. This represents a major limitation towards clinical application, as assistive systems that fail to provide both modalities simultaneously offer limited value to medical practitioners. To alleviate this limitation, we first introduce RadDiagSeg-D, a dataset combining abnormality detection, diagnosis, and multi-target segmentation into a unified and hierarchical task. RadDiagSeg-D covers multiple imaging modalities and is precisely designed to support the development of models that produce descriptive text and corresponding segmentation masks in tandem. Subsequently, we leverage the dataset to propose a novel vision-language model, RadDiagSeg-M, capable of joint abnormality detection, diagnosis, and flexible segmentation. RadDiagSeg-M provides highly informative and clinically useful outputs, effectively addressing the need to enrich contextual information for assistive diagnosis. Finally, we benchmark RadDiagSeg-M and showcase its strong performance across all components involved in the task of multi-target text-and-mask generation, establishing a robust and competitive baseline.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ (Vision Language Models, VLMs) åœ¨å¤„ç†å¤æ‚è§†è§‰é—®ç­”æ—¶ï¼Œéš¾ä»¥åŒæ—¶ç”Ÿæˆè¯Šæ–­æ–‡æœ¬å’Œåƒç´ çº§åˆ†å‰²æ©ç çš„é—®é¢˜ï¼Œæå‡ºäº† RadDiagSeg-M æ¨¡å‹ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ä¸´åºŠåº”ç”¨ä¸­çš„å±€é™ï¼Œç ”ç©¶è€…é¦–å…ˆæ„å»ºäº† RadDiagSeg-D æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å°†å¼‚å¸¸æ£€æµ‹ã€è¯Šæ–­å’Œ Multi-Target Segmentation æ•´åˆä¸ºä¸€ä¸ªè·¨å¤šç§å½±åƒæ¨¡æ€çš„ç»Ÿä¸€å±‚æ¬¡åŒ–ä»»åŠ¡ã€‚ä¾æ‰˜è¯¥æ•°æ®é›†ï¼ŒRadDiagSeg-M æ¨¡å‹å®ç°äº†è”åˆå¼‚å¸¸æ£€æµ‹ã€è¯Šæ–­å’Œçµæ´»åˆ†å‰²ï¼Œæä¾›äº†æå…·ä¸´åºŠå‚è€ƒä»·å€¼çš„ä¿¡æ¯ã€‚åŸºå‡†æµ‹è¯•ç»“æœè¡¨æ˜ï¼ŒRadDiagSeg-M åœ¨æ–‡æœ¬ä¸æ©ç ç”Ÿæˆä»»åŠ¡çš„æ‰€æœ‰ç»„æˆéƒ¨åˆ†ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œå±•ç°äº†å¼ºå¤§çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ä»…è§£å†³äº†è¾…åŠ©è¯Šæ–­ä¸­ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³çš„é—®é¢˜ï¼Œè¿˜ä¸ºæ”¾å°„å­¦é¢†åŸŸçš„è”åˆä»»åŠ¡ç¡®ç«‹äº†ä¸€ä¸ªå¼ºæœ‰åŠ›çš„ Baselineã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18188v1",
      "published_date": "2025-10-21 00:28:13 UTC",
      "updated_date": "2025-10-21 00:28:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:53:46.687657+00:00"
    },
    {
      "arxiv_id": "2510.18187v1",
      "title": "VelocityNet: Real-Time Crowd Anomaly Detection via Person-Specific Velocity Analysis",
      "title_zh": "VelocityNetï¼šåŸºäºç‰¹å®šä¸ªä½“é€Ÿåº¦åˆ†æçš„å®æ—¶äººç¾¤å¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Fatima AlGhamdi",
        "Omar Alharbi",
        "Abdullah Aldwyish",
        "Raied Aljadaany",
        "Muhammad Kamran J Khan",
        "Huda Alamri"
      ],
      "abstract": "Detecting anomalies in crowded scenes is challenging due to severe inter-person occlusions and highly dynamic, context-dependent motion patterns. Existing approaches often struggle to adapt to varying crowd densities and lack interpretable anomaly indicators. To address these limitations, we introduce VelocityNet, a dual-pipeline framework that combines head detection and dense optical flow to extract person-specific velocities. Hierarchical clustering categorizes these velocities into semantic motion classes (halt, slow, normal, and fast), and a percentile-based anomaly scoring system measures deviations from learned normal patterns. Experiments demonstrate the effectiveness of our framework in real-time detection of diverse anomalous motion patterns within densely crowded environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VelocityNetï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³æ‹¥æŒ¤åœºæ™¯å®æ—¶å¼‚å¸¸æ£€æµ‹éš¾é¢˜çš„åŒæµæ°´çº¿(dual-pipeline)æ¡†æ¶ã€‚é’ˆå¯¹äººå‘˜é®æŒ¡å’Œå¤æ‚åŠ¨æ€è¿åŠ¨æ¨¡å¼å¸¦æ¥çš„æŒ‘æˆ˜ï¼ŒVelocityNetç»“åˆäº†å¤´éƒ¨æ£€æµ‹(head detection)ä¸ç¨ å¯†å…‰æµ(dense optical flow)æŠ€æœ¯ï¼Œä»è€Œç²¾å‡†æå–ç‰¹å®šäºä¸ªäººçš„é€Ÿåº¦ç‰¹å¾ã€‚é€šè¿‡å±‚æ¬¡èšç±»(Hierarchical clustering)æ–¹æ³•ï¼Œè¯¥ç³»ç»Ÿå°†é€Ÿåº¦å½’ç±»ä¸ºåœæ­¢(halt)ã€ç¼“æ…¢(slow)ã€æ­£å¸¸(normal)å’Œå¿«é€Ÿ(fast)ç­‰è¯­ä¹‰è¿åŠ¨ç±»åˆ«ï¼Œæ˜¾è‘—å¢å¼ºäº†æ£€æµ‹ç»“æœçš„å¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºç™¾åˆ†ä½æ•°çš„å¼‚å¸¸è¯„åˆ†ç³»ç»Ÿ(percentile-based anomaly scoring system)ï¼Œé€šè¿‡è¡¡é‡ä¸ªä½“é€Ÿåº¦ä¸å·²å­¦ä¹ æ­£å¸¸æ¨¡å¼çš„åå·®æ¥è¯†åˆ«å¼‚å¸¸è¡Œä¸ºã€‚å®éªŒè¯æ˜ï¼ŒVelocityNetèƒ½å¤Ÿåœ¨é«˜å¯†åº¦æ‹¥æŒ¤ç¯å¢ƒä¸­æœ‰æ•ˆå®æ—¶æ£€æµ‹å¤šç§å¼‚å¸¸è¿åŠ¨æ¨¡å¼ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•åœ¨é€‚åº”ä¸åŒäººç¾¤å¯†åº¦æ–¹é¢çš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.18187v1",
      "published_date": "2025-10-21 00:26:54 UTC",
      "updated_date": "2025-10-21 00:26:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:53:48.673502+00:00"
    },
    {
      "arxiv_id": "2510.18184v1",
      "title": "ActivationReasoning: Logical Reasoning in Latent Activation Spaces",
      "title_zh": "ActivationReasoningï¼šæ½œåœ¨æ¿€æ´»ç©ºé—´ä¸­çš„é€»è¾‘æ¨ç†",
      "authors": [
        "Lukas Helff",
        "Ruben HÃ¤rle",
        "Wolfgang Stammer",
        "Felix Friedrich",
        "Manuel Brack",
        "Antonia WÃ¼st",
        "Hikaru Shindo",
        "Patrick Schramowski",
        "Kristian Kersting"
      ],
      "abstract": "Large language models (LLMs) excel at generating fluent text, but their internal reasoning remains opaque and difficult to control. Sparse autoencoders (SAEs) make hidden activations more interpretable by exposing latent features that often align with human concepts. Yet, these features are fragile and passive, offering no mechanism for systematic reasoning or model control. To address this, we introduce ActivationReasoning (AR), a framework that embeds explicit logical reasoning into the latent space of LLMs. It proceeds in three stages: (1) Finding latent representations, first latent concept representations are identified (e.g., via SAEs) and organized into a dictionary; (2) Activating propositions, at inference time AR detects activating concepts and maps them to logical propositions; and (3)Logical reasoning, applying logical rules over these propositions to infer higher-order structures, compose new concepts, and steer model behavior. We evaluate AR on multi-hop reasoning (PrOntoQA), abstraction and robustness to indirect concept cues (Rail2Country), reasoning over natural and diverse language (ProverQA), and context-sensitive safety (BeaverTails). Across all tasks, AR scales robustly with reasoning complexity, generalizes to abstract and context-sensitive tasks, and transfers across model backbones. These results demonstrate that grounding logical structure in latent activations not only improves transparency but also enables structured reasoning, reliable control, and alignment with desired behaviors, providing a path toward more reliable and auditable AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ActivationReasoning (AR) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) å†…éƒ¨æ¨ç†è¿‡ç¨‹ä¸é€æ˜ä¸”éš¾ä»¥æ§åˆ¶çš„å±€é™æ€§ã€‚AR é€šè¿‡å°†æ˜¾å¼é€»è¾‘æ¨ç†åµŒå…¥åˆ° LLMs çš„æ½œç©ºé—´ (latent space) ä¸­ï¼Œåˆ©ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ (Sparse Autoencoders, SAEs) è¯†åˆ«æ½œæ¦‚å¿µè¡¨å¾ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µå°†å…¶æ˜ å°„ä¸ºé€»è¾‘å‘½é¢˜ï¼Œè¿›è€Œåº”ç”¨é€»è¾‘è§„åˆ™å®ç°é«˜é˜¶æ¨ç†ä¸è¡Œä¸ºå¼•å¯¼ã€‚å®éªŒåœ¨ PrOntoQAã€Rail2Countryã€ProverQA å’Œ BeaverTails ç­‰å¤šé¡¹ä»»åŠ¡ä¸ŠéªŒè¯äº† AR çš„æœ‰æ•ˆæ€§ï¼Œç»“æœè¡¨æ˜è¯¥æ¡†æ¶èƒ½éšæ¨ç†å¤æ‚åº¦ç¨³å¥æ‰©å±•ï¼Œå¹¶åœ¨æŠ½è±¡ä»»åŠ¡å’Œå®‰å…¨æ€§å¯¹é½ä¸­å±•ç°å‡ºæå¼ºçš„æ³›åŒ–ä¸è·¨æ¨¡å‹è¿ç§»èƒ½åŠ›ã€‚ç ”ç©¶è¯æ˜ï¼Œåœ¨æ½œæ¿€æ´» (latent activations) ä¸­æ„å»ºé€»è¾‘ç»“æ„ä¸ä»…æ˜¾è‘—æå‡äº†æ¨¡å‹é€æ˜åº¦ï¼Œè¿˜å®ç°äº†å¯¹æ¨¡å‹è¡Œä¸ºçš„å¯é æ§åˆ¶ã€‚è¿™ä¸€æˆæœä¸ºæ„å»ºå¯ç†è§£ã€å¯å®¡è®¡ä¸”é«˜åº¦å¯¹é½çš„ AI ç³»ç»Ÿæä¾›äº†åˆ›æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.18184v1",
      "published_date": "2025-10-21 00:21:04 UTC",
      "updated_date": "2025-10-21 00:21:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:53:53.289560+00:00"
    },
    {
      "arxiv_id": "2510.18904v1",
      "title": "DuoLens: A Framework for Robust Detection of Machine-Generated Multilingual Text and Code",
      "title_zh": "DuoLensï¼šä¸€ç§é’ˆå¯¹æœºå™¨ç”Ÿæˆçš„å¤šè¯­è¨€æ–‡æœ¬ä¸ä»£ç çš„é²æ£’æ£€æµ‹æ¡†æ¶",
      "authors": [
        "Shriyansh Agrawal",
        "Aidan Lau",
        "Sanyam Shah",
        "Ahan M R",
        "Kevin Zhu",
        "Sunishchal Dev",
        "Vasu Sharma"
      ],
      "abstract": "The prevalence of Large Language Models (LLMs) for generating multilingual text and source code has only increased the imperative for machine-generated content detectors to be accurate and efficient across domains. Current detectors, predominantly utilizing zero-shot methods, such as Fast DetectGPT or GPTZero, either incur high computational cost or lack sufficient accuracy, often with a trade-off between the two, leaving room for further improvement. To address these gaps, we propose the fine-tuning of encoder-only Small Language Models (SLMs), in particular, the pre-trained models of RoBERTA and CodeBERTa using specialized datasets on source code and other natural language to prove that for the task of binary classification, SLMs outperform LLMs by a huge margin whilst using a fraction of compute. Our encoders achieve AUROC $= 0.97$ to $0.99$ and macro-F1 $0.89$ to $0.94$ while reducing latency by $8$-$12\\times$ and peak VRAM by $3$-$5\\times$ at $512$-token inputs. Under cross-generator shifts and adversarial transformations (paraphrase, back-translation; code formatting/renaming), performance retains $\\geq 92%$ of clean AUROC. We release training and evaluation scripts with seeds and configs; a reproducibility checklist is also included.",
      "tldr_zh": "éšç€å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆçš„è·¨è¯­è¨€æ–‡æœ¬å’Œä»£ç æ—¥ç›Šæ™®åŠï¼Œç°æœ‰çš„é›¶æ ·æœ¬(zero-shot)æ£€æµ‹æ–¹æ³•ï¼ˆå¦‚Fast DetectGPTå’ŒGPTZeroï¼‰åœ¨è®¡ç®—æˆæœ¬å’Œå‡†ç¡®ç‡ä¹‹é—´éš¾ä»¥å…¼é¡¾ã€‚è¯¥ç ”ç©¶æå‡ºäº†DuoLensæ¡†æ¶ï¼Œé€šè¿‡å¾®è°ƒä»…ç¼–ç å™¨(encoder-only)çš„å°è¯­è¨€æ¨¡å‹(SLMs)ï¼Œç‰¹åˆ«æ˜¯é¢„è®­ç»ƒçš„RoBERTAå’ŒCodeBERTaï¼Œæ¥æ‰§è¡Œæœºå™¨ç”Ÿæˆå†…å®¹çš„äºŒå…ƒåˆ†ç±»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSLMsåœ¨åˆ†ç±»æ€§èƒ½ä¸Šå¤§å¹…è¶…è¶Šäº†LLMsï¼Œä»…éœ€æå°æ¯”ä¾‹çš„è®¡ç®—èµ„æºå³å¯å®ç°0.97è‡³0.99çš„AUROCå’Œ0.89è‡³0.94çš„macro-F1ã€‚åœ¨512ä¸ªtokenè¾“å…¥ä¸‹ï¼ŒDuoLenså°†æ¨ç†å»¶è¿Ÿé™ä½äº†8-12å€ï¼Œå¹¶å°†å³°å€¼æ˜¾å­˜(VRAM)å ç”¨å‡å°‘äº†3-5å€ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨é¢å¯¹è·¨ç”Ÿæˆå™¨åç§»ä»¥åŠæ”¹å†™(paraphrase)ã€ä»£ç é‡å‘½åç­‰å¯¹æŠ—æ€§è½¬æ¢æ—¶ï¼Œä»èƒ½ä¿æŒåŸå§‹AUROCçš„92%ä»¥ä¸Šã€‚è¯¥ç ”ç©¶è¯æ˜äº†SLMsåœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§ï¼Œå¹¶å…¬å¼€äº†è®­ç»ƒä¸è¯„ä¼°è„šæœ¬ä»¥æ”¯æŒå¤šé¢†åŸŸæœºå™¨ç”Ÿæˆå†…å®¹æ£€æµ‹çš„å¤ç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to 39th Conference on Neural Information Processing Systems (NeurIPS 2025): 4th Workshop on Deep Learning for Code",
      "pdf_url": "https://arxiv.org/pdf/2510.18904v1",
      "published_date": "2025-10-21 00:17:00 UTC",
      "updated_date": "2025-10-21 00:17:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T04:54:00.485228+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 165,
  "processed_papers_count": 165,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-25T04:55:08.929873+00:00"
}
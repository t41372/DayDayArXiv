[
  {
    "arxiv_id": "2502.00225v1",
    "title": "Should You Use Your Large Language Model to Explore or Exploit?",
    "authors": [
      "Keegan Harris",
      "Aleksandrs Slivkins"
    ],
    "abstract": "We evaluate the ability of the current generation of large language models\n(LLMs) to help a decision-making agent facing an exploration-exploitation\ntradeoff. We use LLMs to explore and exploit in silos in various (contextual)\nbandit tasks. We find that while the current LLMs often struggle to exploit,\nin-context mitigations may be used to substantially improve performance for\nsmall-scale tasks. However even then, LLMs perform worse than a simple linear\nregression. On the other hand, we find that LLMs do help at exploring large\naction spaces with inherent semantics, by suggesting suitable candidates to\nexplore.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00225v1",
    "published_date": "2025-01-31 23:42:53 UTC",
    "updated_date": "2025-01-31 23:42:53 UTC"
  },
  {
    "arxiv_id": "2502.00217v1",
    "title": "Fantastic Multi-Task Gradient Updates and How to Find Them In a Cone",
    "authors": [
      "Negar Hassanpour",
      "Muhammad Kamran Janjua",
      "Kunlin Zhang",
      "Sepehr Lavasani",
      "Xiaowen Zhang",
      "Chunhua Zhou",
      "Chao Gao"
    ],
    "abstract": "Balancing competing objectives remains a fundamental challenge in multi-task\nlearning (MTL), primarily due to conflicting gradients across individual tasks.\nA common solution relies on computing a dynamic gradient update vector that\nbalances competing tasks as optimization progresses. Building on this idea, we\npropose ConicGrad, a principled, scalable, and robust MTL approach formulated\nas a constrained optimization problem. Our method introduces an angular\nconstraint to dynamically regulate gradient update directions, confining them\nwithin a cone centered on the reference gradient of the overall objective. By\nbalancing task-specific gradients without over-constraining their direction or\nmagnitude, ConicGrad effectively resolves inter-task gradient conflicts.\nMoreover, our framework ensures computational efficiency and scalability to\nhigh-dimensional parameter spaces. We conduct extensive experiments on standard\nsupervised learning and reinforcement learning MTL benchmarks, and demonstrate\nthat ConicGrad achieves state-of-the-art performance across diverse tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 7 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.00217v1",
    "published_date": "2025-01-31 23:11:12 UTC",
    "updated_date": "2025-01-31 23:11:12 UTC"
  },
  {
    "arxiv_id": "2502.00213v2",
    "title": "Understanding Why Adam Outperforms SGD: Gradient Heterogeneity in Transformers",
    "authors": [
      "Akiyoshi Tomihari",
      "Issei Sato"
    ],
    "abstract": "Transformers are challenging to optimize with SGD and typically require\nadaptive optimizers such as Adam. However, the reasons behind the superior\nperformance of Adam over SGD remain unclear. In this study, we investigate the\noptimization of transformers by focusing on gradient heterogeneity, defined as\nthe disparity in gradient norms among parameters. Our analysis shows that\ngradient heterogeneity hinders gradient-based optimization, including SGD,\nwhile sign-based optimization, a simplified variant of Adam, is less affected.\nWe further examine gradient heterogeneity in transformers and show that it is\ninfluenced by the placement of layer normalization. Experimental results from\nfine-tuning transformers in both NLP and vision domains validate our\ntheoretical analyses. This study provides insights into the optimization\nchallenges of transformers and offers guidance for designing future\noptimization algorithms. Code is available at\nhttps://github.com/tom4649/gradient-heterogeneity.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00213v2",
    "published_date": "2025-01-31 23:05:52 UTC",
    "updated_date": "2025-05-16 12:15:17 UTC"
  },
  {
    "arxiv_id": "2502.00212v4",
    "title": "STP: Self-play LLM Theorem Provers with Iterative Conjecturing and Proving",
    "authors": [
      "Kefan Dong",
      "Tengyu Ma"
    ],
    "abstract": "A fundamental challenge in formal theorem proving by LLMs is the lack of\nhigh-quality training data. Although reinforcement learning or expert iteration\npartially mitigates this issue by alternating between LLM generating proofs and\nfinetuning them on correctly generated ones, performance quickly plateaus due\nto the scarcity of correct proofs (sparse rewards). To keep improving the\nmodels with limited data, we draw inspiration from mathematicians, who\ncontinuously develop new results, partly by proposing novel conjectures or\nexercises (which are often variants of known results) and attempting to solve\nthem. We design the Self-play Theorem Prover (STP) that simultaneously takes on\ntwo roles, conjecturer and prover, each providing training signals to the\nother. The conjecturer is trained iteratively on previously generated\nconjectures that are barely provable by the current prover, which incentivizes\nit to generate increasingly challenging conjectures over time. The prover\nattempts to prove the conjectures with standard expert iteration. We evaluate\nSTP with both Lean and Isabelle formal versifiers. With 51.3 billion tokens\ngenerated during the training in Lean, STP proves 28.5% of the statements in\nthe LeanWorkbook dataset, doubling the previous best result of 13.2% achieved\nthrough expert iteration. The final model achieves state-of-the-art performance\namong whole-proof generation methods on miniF2F-test (65.0%, pass@3200),\nProofnet-test (23.9%, pass@3200) and PutnamBench (8/644, pass@3200). We release\nour code, model, and dataset in this URL: https://github.com/kfdong/STP.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.00212v4",
    "published_date": "2025-01-31 23:01:48 UTC",
    "updated_date": "2025-03-21 03:27:55 UTC"
  },
  {
    "arxiv_id": "2502.00205v2",
    "title": "EcoWeedNet: A Lightweight and Automated Weed Detection Method for Sustainable Next-Generation Agricultural Consumer Electronics",
    "authors": [
      "Omar H. Khater",
      "Abdul Jabbar Siddiqui",
      "M. Shamim Hossain",
      "Aiman El-Maleh"
    ],
    "abstract": "Sustainable agriculture plays a crucial role in ensuring world food security\nfor consumers. A critical challenge faced by sustainable precision agriculture\nis weed growth, as weeds compete for essential resources with crops, such as\nwater, soil nutrients, and sunlight, which notably affect crop yields. The\nadoption of automated computer vision technologies and ground agricultural\nconsumer electronic vehicles in precision agriculture offers sustainable,\nlow-carbon solutions. However, prior works suffer from issues such as low\naccuracy and precision, as well as high computational expense. This work\nproposes EcoWeedNet, a novel model that enhances weed detection performance\nwithout introducing significant computational complexity, aligning with the\ngoals of low-carbon agricultural practices. The effectiveness of the proposed\nmodel is demonstrated through comprehensive experiments on the CottonWeedDet12\nbenchmark dataset, which reflects real-world scenarios. EcoWeedNet achieves\nperformance comparable to that of large models (mAP@0.5 = 95.2%), yet with\nsignificantly fewer parameters (approximately 4.21% of the parameters of\nYOLOv4), lower computational complexity and better computational efficiency\n6.59% of the GFLOPs of YOLOv4). These key findings indicate EcoWeedNet's\ndeployability on low-power consumer hardware, lower energy consumption, and\nhence reduced carbon footprint, thereby emphasizing the application prospects\nof EcoWeedNet in next-generation sustainable agriculture. These findings\nprovide the way forward for increased application of environmentally-friendly\nagricultural consumer technologies.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00205v2",
    "published_date": "2025-01-31 22:46:20 UTC",
    "updated_date": "2025-05-07 11:40:20 UTC"
  },
  {
    "arxiv_id": "2502.00201v1",
    "title": "Year-over-Year Developments in Financial Fraud Detection via Deep Learning: A Systematic Literature Review",
    "authors": [
      "Yisong Chen",
      "Chuqing Zhao",
      "Yixin Xu",
      "Chuanhao Nie"
    ],
    "abstract": "This paper systematically reviews advancements in deep learning (DL)\ntechniques for financial fraud detection, a critical issue in the financial\nsector. Using the Kitchenham systematic literature review approach, 57 studies\npublished between 2019 and 2024 were analyzed. The review highlights the\neffectiveness of various deep learning models such as Convolutional Neural\nNetworks, Long Short-Term Memory, and transformers across domains such as\ncredit card transactions, insurance claims, and financial statement audits.\nPerformance metrics such as precision, recall, F1-score, and AUC-ROC were\nevaluated. Key themes explored include the impact of data privacy frameworks\nand advancements in feature engineering and data preprocessing. The study\nemphasizes challenges such as imbalanced datasets, model interpretability, and\nethical considerations, alongside opportunities for automation and\nprivacy-preserving techniques such as blockchain integration and Principal\nComponent Analysis. By examining trends over the past five years, this review\nidentifies critical gaps and promising directions for advancing DL applications\nin financial fraud detection, offering actionable insights for researchers and\npractitioners.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-fin.ST"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00201v1",
    "published_date": "2025-01-31 22:31:50 UTC",
    "updated_date": "2025-01-31 22:31:50 UTC"
  },
  {
    "arxiv_id": "2502.00196v2",
    "title": "DermaSynth: Rich Synthetic Image-Text Pairs Using Open Access Dermatology Datasets",
    "authors": [
      "Abdurrahim Yilmaz",
      "Furkan Yuceyalcin",
      "Ece Gokyayla",
      "Donghee Choi",
      "Ozan Erdem",
      "Ali Anil Demircali",
      "Rahmetullah Varol",
      "Ufuk Gorkem Kirabali",
      "Gulsum Gencoglan",
      "Joram M. Posma",
      "Burak Temelkuran"
    ],
    "abstract": "A major barrier to developing vision large language models (LLMs) in\ndermatology is the lack of large image--text pairs dataset. We introduce\nDermaSynth, a dataset comprising of 92,020 synthetic image--text pairs curated\nfrom 45,205 images (13,568 clinical and 35,561 dermatoscopic) for\ndermatology-related clinical tasks. Leveraging state-of-the-art LLMs, using\nGemini 2.0, we used clinically related prompts and self-instruct method to\ngenerate diverse and rich synthetic texts. Metadata of the datasets were\nincorporated into the input prompts by targeting to reduce potential\nhallucinations. The resulting dataset builds upon open access dermatological\nimage repositories (DERM12345, BCN20000, PAD-UFES-20, SCIN, and HIBA) that have\npermissive CC-BY-4.0 licenses. We also fine-tuned a preliminary\nLlama-3.2-11B-Vision-Instruct model, DermatoLlama 1.0, on 5,000 samples. We\nanticipate this dataset to support and accelerate AI research in dermatology.\nData and code underlying this work are accessible at\nhttps://github.com/abdurrahimyilmaz/DermaSynth.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.00196v2",
    "published_date": "2025-01-31 22:26:33 UTC",
    "updated_date": "2025-03-04 12:36:10 UTC"
  },
  {
    "arxiv_id": "2502.01660v1",
    "title": "Employee Turnover Prediction: A Cross-component Attention Transformer with Consideration of Competitor Influence and Contagious Effect",
    "authors": [
      "Hao Liu",
      "Yong Ge"
    ],
    "abstract": "Employee turnover refers to an individual's termination of employment from\nthe current organization. It is one of the most persistent challenges for\nfirms, especially those ones in Information Technology (IT) industry that\nconfront high turnover rates. Effective prediction of potential employee\nturnovers benefits multiple stakeholders such as firms and online recruiters.\nPrior studies have focused on either the turnover prediction within a single\nfirm or the aggregated employee movement among firms. How to predict the\nindividual employees' turnovers among multiple firms has gained little\nattention in literature, and thus remains a great research challenge. In this\nstudy, we propose a novel deep learning approach based on job embeddedness\ntheory to predict the turnovers of individual employees across different firms.\nThrough extensive experimental evaluations using a real-world dataset, our\ndeveloped method demonstrates superior performance over several\nstate-of-the-art benchmark methods. Additionally, we estimate the cost saving\nfor recruiters by using our turnover prediction solution and interpret the\nattributions of various driving factors to employee's turnover to showcase its\npractical business value.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.01660v1",
    "published_date": "2025-01-31 22:25:39 UTC",
    "updated_date": "2025-01-31 22:25:39 UTC"
  },
  {
    "arxiv_id": "2502.00194v1",
    "title": "Physics-Informed Neural Network based Damage Identification for Truss Railroad Bridges",
    "authors": [
      "Althaf Shajihan",
      "Kirill Mechitov",
      "Girish Chowdhary",
      "Billie F. Spencer Jr"
    ],
    "abstract": "Railroad bridges are a crucial component of the U.S. freight rail system,\nwhich moves over 40 percent of the nation's freight and plays a critical role\nin the economy. However, aging bridge infrastructure and increasing train\ntraffic pose significant safety hazards and risk service disruptions. The U.S.\nrail network includes over 100,000 railroad bridges, averaging one every 1.4\nmiles of track, with steel bridges comprising over 50% of the network's total\nbridge length. Early identification and assessment of damage in these bridges\nremain challenging tasks. This study proposes a physics-informed neural network\n(PINN) based approach for damage identification in steel truss railroad\nbridges. The proposed approach employs an unsupervised learning approach,\neliminating the need for large datasets typically required by supervised\nmethods. The approach utilizes train wheel load data and bridge response during\ntrain crossing events as inputs for damage identification. The PINN model\nexplicitly incorporates the governing differential equations of the linear\ntime-varying (LTV) bridge-train system. Herein, this model employs a recurrent\nneural network (RNN) based architecture incorporating a custom Runge-Kutta (RK)\nintegrator cell, designed for gradient-based learning. The proposed approach\nupdates the bridge finite element model while also quantifying damage severity\nand localizing the affected structural members. A case study on the Calumet\nBridge in Chicago, Illinois, with simulated damage scenarios, is used to\ndemonstrate the model's effectiveness in identifying damage while maintaining\nlow false-positive rates. Furthermore, the damage identification pipeline is\ndesigned to seamlessly integrate prior knowledge from inspections and drone\nsurveys, also enabling context-aware updating and assessment of bridge's\ncondition.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "30 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.00194v1",
    "published_date": "2025-01-31 22:22:35 UTC",
    "updated_date": "2025-01-31 22:22:35 UTC"
  },
  {
    "arxiv_id": "2503.15516v1",
    "title": "In Pursuit of Predictive Models of Human Preferences Toward AI Teammates",
    "authors": [
      "Ho Chit Siu",
      "Jaime D. Peña",
      "Yutai Zhou",
      "Ross E. Allen"
    ],
    "abstract": "We seek measurable properties of AI agents that make them better or worse\nteammates from the subjective perspective of human collaborators. Our\nexperiments use the cooperative card game Hanabi -- a common benchmark for\nAI-teaming research. We first evaluate AI agents on a set of objective metrics\nbased on task performance, information theory, and game theory, which are\nmeasurable without human interaction. Next, we evaluate subjective human\npreferences toward AI teammates in a large-scale (N=241) human-AI teaming\nexperiment. Finally, we correlate the AI-only objective metrics with the human\nsubjective preferences. Our results refute common assumptions from prior\nliterature on reinforcement learning, revealing new correlations between AI\nbehaviors and human preferences. We find that the final game score a human-AI\nteam achieves is less predictive of human preferences than esoteric measures of\nAI action diversity, strategic dominance, and ability to team with other AI. In\nthe future, these correlations may help shape reward functions for training\nhuman-collaborative AI.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15516v1",
    "published_date": "2025-01-31 22:10:37 UTC",
    "updated_date": "2025-01-31 22:10:37 UTC"
  },
  {
    "arxiv_id": "2502.01659v2",
    "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
    "authors": [
      "Nathaniel Tomczak",
      "Sanmukh Kuppannagari"
    ],
    "abstract": "Transformers have demonstrated great success in numerous domains including\nnatural language processing and bioinformatics. This success stems from the use\nof the attention mechanism by these models in order to represent and propagate\npairwise interactions between individual tokens of sequential data. However,\nthe primary limitation of this operation is its quadratic memory and time\ncomplexity in relation to the input's context length - the length of a sequence\nover which the interactions need to be captured. This significantly limits the\nlength of sequences that can be inferred upon by these models. Extensive\nresearch has been conducted to reduce the number of pairwise interactions to\nsub-quadratic in relation to the context length by introducing sparsity into\nthe attention mechanism through the development of sparse attention masks.\nHowever, efficient implementations that achieve \"true sparsity\" are lacking.\n  In this work, we address this issue by proposing a graph computing view of\nattention where tokens are perceived as nodes of the graph and the attention\nmask determines the edges of the graph. Using this view, we develop graph\nprocessing algorithms to implement the attention mechanism. Both theoretically\nand empirically, we demonstrate that our algorithms only perform the needed\ncomputations, i.e., they are work optimal. We also perform extensive\nexperimentation using popular attention masks to explore the impact of sparsity\non execution time and achievable context length. Our experiments demonstrate\nsignificant speedups in execution times compared to state-of-the-art attention\nimplementations such as FlashAttention for large sequence lengths. We also\ndemonstrate that our algorithms are able to achieve extremely long sequence\nlengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.01659v2",
    "published_date": "2025-01-31 22:05:00 UTC",
    "updated_date": "2025-02-07 13:44:24 UTC"
  },
  {
    "arxiv_id": "2502.00182v2",
    "title": "Understanding Federated Learning from IID to Non-IID dataset: An Experimental Study",
    "authors": [
      "Jungwon Seo",
      "Ferhat Ozgur Catak",
      "Chunming Rong"
    ],
    "abstract": "As privacy concerns and data regulations grow, federated learning (FL) has\nemerged as a promising approach for training machine learning models across\ndecentralized data sources without sharing raw data. However, a significant\nchallenge in FL is that client data are often non-IID (non-independent and\nidentically distributed), leading to reduced performance compared to\ncentralized learning. While many methods have been proposed to address this\nissue, their underlying mechanisms are often viewed from different\nperspectives. Through a comprehensive investigation from gradient descent to\nFL, and from IID to non-IID data settings, we find that inconsistencies in\nclient loss landscapes primarily cause performance degradation in non-IID\nscenarios. From this understanding, we observe that existing methods can be\ngrouped into two main strategies: (i) adjusting parameter update paths and (ii)\nmodifying client loss landscapes. These findings offer a clear perspective on\naddressing non-IID challenges in FL and help guide future research in the\nfield.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00182v2",
    "published_date": "2025-01-31 21:58:15 UTC",
    "updated_date": "2025-02-07 14:31:59 UTC"
  },
  {
    "arxiv_id": "2502.00174v1",
    "title": "The role of positional encodings in the ARC benchmark",
    "authors": [
      "Guilherme H. Bandeira Costa",
      "Miguel Freire",
      "Arlindo L. Oliveira"
    ],
    "abstract": "The Abstraction and Reasoning Corpus challenges AI systems to perform\nabstract reasoning with minimal training data, a task intuitive for humans but\ndemanding for machine learning models. Using CodeT5+ as a case study, we\ndemonstrate how limitations in positional encoding hinder reasoning and impact\nperformance. This work further examines the role of positional encoding across\ntransformer architectures, highlighting its critical influence on models of\nvarying sizes and configurations. Comparing several strategies, we find that\nwhile 2D positional encoding and Rotary Position Embedding offer competitive\nperformance, 2D encoding excels in data-constrained scenarios, emphasizing its\neffectiveness for ARC tasks",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00174v1",
    "published_date": "2025-01-31 21:34:54 UTC",
    "updated_date": "2025-01-31 21:34:54 UTC"
  },
  {
    "arxiv_id": "2502.01657v1",
    "title": "Improving Rule-based Reasoning in LLMs via Neurosymbolic Representations",
    "authors": [
      "Varun Dhanraj",
      "Chris Eliasmith"
    ],
    "abstract": "Large language models (LLMs) continue to face challenges in reliably solving\nreasoning tasks, particularly tasks that involve precise rule following, as\noften found in mathematical reasoning tasks. This paper introduces a novel\nneurosymbolic method that improves LLM reasoning by encoding hidden states into\nneurosymbolic vectors, allowing for problem-solving within a neurosymbolic\nvector space. The results are decoded and combined with the original hidden\nstate, boosting the model's performance on numerical reasoning tasks. By\noffloading computation through neurosymbolic representations, this method\nimproves efficiency, reliability, and interpretability. Our experimental\nresults demonstrate an average of $82.86\\%$ lower cross entropy loss and\n$24.50$ times more problems correctly solved on a suite of mathematical\nreasoning problems compared to chain-of-thought prompting and supervised\nfine-tuning (LoRA), while at the same time not hindering the performance of the\nLLM on other tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.01657v1",
    "published_date": "2025-01-31 20:29:51 UTC",
    "updated_date": "2025-01-31 20:29:51 UTC"
  },
  {
    "arxiv_id": "2502.00151v1",
    "title": "A Comprehensive Review: Applicability of Deep Neural Networks in Business Decision Making and Market Prediction Investment",
    "authors": [
      "Viet Trinh"
    ],
    "abstract": "Big data, both in its structured and unstructured formats, have brought in\nunforeseen challenges in economics and business. How to organize, classify, and\nthen analyze such data to obtain meaningful insights are the ever-going\nresearch topics for business leaders and academic researchers. This paper\nstudies recent applications of deep neural networks in decision making in\neconomical business and investment; especially in risk management, portfolio\noptimization, and algorithmic trading. Set aside limitation in data privacy and\ncross-market analysis, the article establishes that deep neural networks have\nperformed remarkably in financial classification and prediction. Moreover, the\nstudy suggests that by compositing multiple neural networks, spanning different\ndata type modalities, a more robust, efficient, and scalable financial\nprediction framework can be constructed.",
    "categories": [
      "econ.GN",
      "cs.AI",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00151v1",
    "published_date": "2025-01-31 20:24:21 UTC",
    "updated_date": "2025-01-31 20:24:21 UTC"
  },
  {
    "arxiv_id": "2504.13842v1",
    "title": "The Model Counting Competitions 2021-2023",
    "authors": [
      "Johannes K. Fichte",
      "Markus Hecher"
    ],
    "abstract": "Modern society is full of computational challenges that rely on probabilistic\nreasoning, statistics, and combinatorics. Interestingly, many of these\nquestions can be formulated by encoding them into propositional formulas and\nthen asking for its number of models. With a growing interest in practical\nproblem-solving for tasks that involve model counting, the community\nestablished the Model Counting (MC) Competition in fall of 2019 with its first\niteration in 2020. The competition aims at advancing applications, identifying\nchallenging benchmarks, fostering new solver development, and enhancing\nexisting solvers for model counting problems and their variants. The first\niteration, brought together various researchers, identified challenges, and\ninspired numerous new applications. In this paper, we present a comprehensive\noverview of the 2021-2023 iterations of the Model Counting Competition. We\ndetail its execution and outcomes. The competition comprised four tracks, each\nfocusing on a different variant of the model counting problem. The first track\ncentered on the model counting problem (MC), which seeks the count of models\nfor a given propositional formula. The second track challenged developers to\nsubmit programs capable of solving the weighted model counting problem (WMC).\nThe third track was dedicated to projected model counting (PMC). Finally, we\ninitiated a track that combined projected and weighted model counting (PWMC).\nThe competition continued with a high level of participation, with seven to\nnine solvers submitted in various different version and based on quite\ndiverging techniques.",
    "categories": [
      "cs.AI",
      "cs.DS",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.13842v1",
    "published_date": "2025-01-31 20:12:43 UTC",
    "updated_date": "2025-01-31 20:12:43 UTC"
  },
  {
    "arxiv_id": "2503.04731v1",
    "title": "Epistemic Logic Programs: Non-Ground and Counting Complexity",
    "authors": [
      "Thomas Eiter",
      "Johannes K. Fichte",
      "Markus Hecher",
      "Stefan Woltran"
    ],
    "abstract": "Answer Set Programming (ASP) is a prominent problem-modeling and solving\nframework, whose solutions are called answer sets. Epistemic logic programs\n(ELP) extend ASP to reason about all or some answer sets. Solutions to an ELP\ncan be seen as consequences over multiple collections of answer sets, known as\nworld views. While the complexity of propositional programs is well studied,\nthe non-ground case remains open. This paper establishes the complexity of\nnon-ground ELPs. We provide a comprehensive picture for well-known program\nfragments, which turns out to be complete for the class NEXPTIME with access to\noracles up to \\Sigma^P_2. In the quantitative setting, we establish complexity\nresults for counting complexity beyond #EXP. To mitigate high complexity, we\nestablish results in case of bounded predicate arity, reaching up to the fourth\nlevel of the polynomial hierarchy. Finally, we provide ETH-tight runtime\nresults for the parameter treewidth, which has applications in quantitative\nreasoning, where we reason on (marginal) probabilities of epistemic literals.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.CC"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.04731v1",
    "published_date": "2025-01-31 20:08:52 UTC",
    "updated_date": "2025-01-31 20:08:52 UTC"
  },
  {
    "arxiv_id": "2502.00146v1",
    "title": "Multimodal MRI-Ultrasound AI for Prostate Cancer Detection Outperforms Radiologist MRI Interpretation: A Multi-Center Study",
    "authors": [
      "Hassan Jahanandish",
      "Shengtian Sang",
      "Cynthia Xinran Li",
      "Sulaiman Vesal",
      "Indrani Bhattacharya",
      "Jeong Hoon Lee",
      "Richard Fan",
      "Geoffrey A. Sonna",
      "Mirabela Rusu"
    ],
    "abstract": "Pre-biopsy magnetic resonance imaging (MRI) is increasingly used to target\nsuspicious prostate lesions. This has led to artificial intelligence (AI)\napplications improving MRI-based detection of clinically significant prostate\ncancer (CsPCa). However, MRI-detected lesions must still be mapped to\ntransrectal ultrasound (TRUS) images during biopsy, which results in missing\nCsPCa. This study systematically evaluates a multimodal AI framework\nintegrating MRI and TRUS image sequences to enhance CsPCa identification. The\nstudy included 3110 patients from three cohorts across two institutions who\nunderwent prostate biopsy. The proposed framework, based on the 3D UNet\narchitecture, was evaluated on 1700 test cases, comparing performance to\nunimodal AI models that use either MRI or TRUS alone. Additionally, the\nproposed model was compared to radiologists in a cohort of 110 patients. The\nmultimodal AI approach achieved superior sensitivity (80%) and Lesion Dice\n(42%) compared to unimodal MRI (73%, 30%) and TRUS models (49%, 27%). Compared\nto radiologists, the multimodal model showed higher specificity (88% vs. 78%)\nand Lesion Dice (38% vs. 33%), with equivalent sensitivity (79%). Our findings\ndemonstrate the potential of multimodal AI to improve CsPCa lesion targeting\nduring biopsy and treatment planning, surpassing current unimodal models and\nradiologists; ultimately improving outcomes for prostate cancer patients.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00146v1",
    "published_date": "2025-01-31 20:04:20 UTC",
    "updated_date": "2025-01-31 20:04:20 UTC"
  },
  {
    "arxiv_id": "2502.00145v1",
    "title": "Counting and Reasoning with Plans",
    "authors": [
      "David Speck",
      "Markus Hecher",
      "Daniel Gnad",
      "Johannes K. Fichte",
      "Augusto B. Corrêa"
    ],
    "abstract": "Classical planning asks for a sequence of operators reaching a given goal.\nWhile the most common case is to compute a plan, many scenarios require more\nthan that. However, quantitative reasoning on the plan space remains mostly\nunexplored. A fundamental problem is to count plans, which relates to the\nconditional probability on the plan space. Indeed, qualitative and quantitative\napproaches are well-established in various other areas of automated reasoning.\nWe present the first study to quantitative and qualitative reasoning on the\nplan space. In particular, we focus on polynomially bounded plans. On the\ntheoretical side, we study its complexity, which gives rise to rich reasoning\nmodes. Since counting is hard in general, we introduce the easier notion of\nfacets, which enables understanding the significance of operators. On the\npractical side, we implement quantitative reasoning for planning. Thereby, we\ntransform a planning task into a propositional formula and use knowledge\ncompilation to count different plans. This framework scales well to large plan\nspaces, while enabling rich reasoning capabilities such as learning pruning\nfunctions and explainable planning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00145v1",
    "published_date": "2025-01-31 20:03:51 UTC",
    "updated_date": "2025-01-31 20:03:51 UTC"
  },
  {
    "arxiv_id": "2502.00140v1",
    "title": "Demystifying MPNNs: Message Passing as Merely Efficient Matrix Multiplication",
    "authors": [
      "Qin Jiang",
      "Chengjia Wang",
      "Michael Lones",
      "Wei Pang"
    ],
    "abstract": "While Graph Neural Networks (GNNs) have achieved remarkable success, their\ndesign largely relies on empirical intuition rather than theoretical\nunderstanding. In this paper, we present a comprehensive analysis of GNN\nbehavior through three fundamental aspects: (1) we establish that\n\\textbf{$k$-layer} Message Passing Neural Networks efficiently aggregate\n\\textbf{$k$-hop} neighborhood information through iterative computation, (2)\nanalyze how different loop structures influence neighborhood computation, and\n(3) examine behavior across structure-feature hybrid and structure-only tasks.\nFor deeper GNNs, we demonstrate that gradient-related issues, rather than just\nover-smoothing, can significantly impact performance in sparse graphs. We also\nanalyze how different normalization schemes affect model performance and how\nGNNs make predictions with uniform node features, providing a theoretical\nframework that bridges the gap between empirical success and theoretical\nunderstanding.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00140v1",
    "published_date": "2025-01-31 19:48:03 UTC",
    "updated_date": "2025-01-31 19:48:03 UTC"
  },
  {
    "arxiv_id": "2502.07071v2",
    "title": "TRADES: Generating Realistic Market Simulations with Diffusion Models",
    "authors": [
      "Leonardo Berti",
      "Bardh Prenkaj",
      "Paola Velardi"
    ],
    "abstract": "Financial markets are complex systems characterized by high statistical\nnoise, nonlinearity, and constant evolution. Thus, modeling them is extremely\nhard. We address the task of generating realistic and responsive Limit Order\nBook (LOB) market simulations, which are fundamental for calibrating and\ntesting trading strategies, performing market impact experiments, and\ngenerating synthetic market data. Previous works lack realism, usefulness, and\nresponsiveness of the generated simulations. To bridge this gap, we propose a\nnovel TRAnsformer-based Denoising Diffusion Probabilistic Engine for LOB\nSimulations (TRADES). TRADES generates realistic order flows conditioned on the\nstate of the market, leveraging a transformer-based architecture that captures\nthe temporal and spatial characteristics of high-frequency market data. There\nis a notable absence of quantitative metrics for evaluating generative market\nsimulation models in the literature. To tackle this problem, we adapt the\npredictive score, a metric measured as an MAE, by training a stock price\npredictive model on synthetic data and testing it on real data. We compare\nTRADES with previous works on two stocks, reporting an x3.27 and x3.47\nimprovement over SoTA according to the predictive score, demonstrating that we\ngenerate useful synthetic market data for financial downstream tasks. We assess\nTRADES's market simulation realism and responsiveness, showing that it\neffectively learns the conditional data distribution and successfully reacts to\nan experimental agent, giving sprout to possible calibrations and evaluations\nof trading strategies and market impact experiments. We developed DeepMarket,\nthe first open-source Python framework for market simulation with deep\nlearning. Our repository includes a synthetic LOB dataset composed of TRADES's\ngenerates simulations. We release the code at\ngithub.com/LeonardoBerti00/DeepMarket.",
    "categories": [
      "q-fin.TR",
      "cs.AI",
      "cs.LG",
      "q-fin.CP"
    ],
    "primary_category": "q-fin.TR",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.07071v2",
    "published_date": "2025-01-31 19:43:13 UTC",
    "updated_date": "2025-02-12 12:38:13 UTC"
  },
  {
    "arxiv_id": "2502.00136v1",
    "title": "A Three-Branch Checks-and-Balances Frameworkfor Context-Aware Ethical Alignment of Large Language Models",
    "authors": [
      "Edward Y. Chang"
    ],
    "abstract": "This paper introduces a three-branch checks-and-balances framework for\nethical alignment of Large Language Models (LLMs), inspired by governmental\nsystems. It implements three independent yet interacting components: LLMs as\nthe executive branch for knowledge generation, DIKE as the legislative branch\nestablishing ethical guardrails, and ERIS as the judicial branch for contextual\ninterpretation. The adversarial DIKE-ERIS duality enables adaptation to diverse\ncultural contexts while upholding consistent ethical principles. This\narchitecture addresses limitations of reinforcement learning with human\nfeedback (RLHF) by providing interpretable, adaptable, and culturally-aware\nethical reasoning. Through self-supervised learning and adversarial testing,\nour framework demonstrates how emotional modeling can guide linguistic\nbehaviors toward ethical outcomes while preserving independence across\nknowledge generation, ethical oversight, and contextual interpretation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "F.2.2"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 6 tables, 6 figures. arXiv admin note: substantial text\n  overlap with arXiv:2405.07076",
    "pdf_url": "http://arxiv.org/pdf/2502.00136v1",
    "published_date": "2025-01-31 19:41:28 UTC",
    "updated_date": "2025-01-31 19:41:28 UTC"
  },
  {
    "arxiv_id": "2502.00133v1",
    "title": "Exploring Transfer Learning for Deep Learning Polyp Detection in Colonoscopy Images Using YOLOv8",
    "authors": [
      "Fabian Vazquez",
      "Jose Angel Nuñez",
      "Xiaoyan Fu",
      "Pengfei Gu",
      "Bin Fu"
    ],
    "abstract": "Deep learning methods have demonstrated strong performance in objection\ntasks; however, their ability to learn domain-specific applications with\nlimited training data remains a significant challenge. Transfer learning\ntechniques address this issue by leveraging knowledge from pre-training on\nrelated datasets, enabling faster and more efficient learning for new tasks.\nFinding the right dataset for pre-training can play a critical role in\ndetermining the success of transfer learning and overall model performance. In\nthis paper, we investigate the impact of pre-training a YOLOv8n model on seven\ndistinct datasets, evaluating their effectiveness when transferred to the task\nof polyp detection. We compare whether large, general-purpose datasets with\ndiverse objects outperform niche datasets with characteristics similar to\npolyps. In addition, we assess the influence of the size of the dataset on the\nefficacy of transfer learning. Experiments on the polyp datasets show that\nmodels pre-trained on relevant datasets consistently outperform those trained\nfrom scratch, highlighting the benefit of pre-training on datasets with shared\ndomain-specific features.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.0"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 3 figures, 6 tables, SPIE conference",
    "pdf_url": "http://arxiv.org/pdf/2502.00133v1",
    "published_date": "2025-01-31 19:33:45 UTC",
    "updated_date": "2025-01-31 19:33:45 UTC"
  },
  {
    "arxiv_id": "2502.05202v1",
    "title": "Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies",
    "authors": [
      "Nadav Timor",
      "Jonathan Mamou",
      "Daniel Korat",
      "Moshe Berchansky",
      "Oren Pereg",
      "Gaurav Jain",
      "Roy Schwartz",
      "Moshe Wasserblat",
      "David Harel"
    ],
    "abstract": "Accelerating the inference of large language models (LLMs) is a critical\nchallenge in generative AI. Speculative decoding (SD) methods offer substantial\nefficiency gains by generating multiple tokens using a single target forward\npass. However, existing SD approaches require the drafter and target models to\nshare the same vocabulary, thus limiting the pool of possible drafters, often\nnecessitating the training of a drafter from scratch. We present three new SD\nmethods that remove this shared-vocabulary constraint. All three methods\npreserve the target distribution (i.e., they are lossless) and work with\noff-the-shelf models without requiring additional training or modifications.\nEmpirically, on summarization, programming, and long-context tasks, our\nalgorithms achieve significant speedups over standard autoregressive decoding.\nBy enabling any off-the-shelf model to serve as drafter and requiring no\nretraining, this work substantially broadens the applicability of the SD\nframework in practice.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05202v1",
    "published_date": "2025-01-31 19:13:58 UTC",
    "updated_date": "2025-01-31 19:13:58 UTC"
  },
  {
    "arxiv_id": "2501.19403v2",
    "title": "Redefining Machine Unlearning: A Conformal Prediction-Motivated Approach",
    "authors": [
      "Yingdan Shi",
      "Sijia Liu",
      "Ren Wang"
    ],
    "abstract": "Machine unlearning seeks to remove the influence of specified data from a\ntrained model. While metrics such as unlearning accuracy (UA) and membership\ninference attack (MIA) provide baselines for assessing unlearning performance,\nthey fall short of evaluating the forgetting reliability. In this paper, we\nfind that the data misclassified across UA and MIA still have their ground\ntruth labels included in the prediction set from the uncertainty quantification\nperspective, which raises a fake unlearning issue. To address this issue, we\npropose two novel metrics inspired by conformal prediction that more reliably\nevaluate forgetting quality. Building on these insights, we further propose a\nconformal prediction-based unlearning framework that integrates conformal\nprediction into Carlini & Wagner adversarial attack loss, which can\nsignificantly push the ground truth label out of the conformal prediction set.\nThrough extensive experiments on image classification task, we demonstrate both\nthe effectiveness of our proposed metrics and the superiority of our unlearning\nframework, which improves the UA of existing unlearning methods by an average\nof 6.6% through the incorporation of a tailored loss term alone.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19403v2",
    "published_date": "2025-01-31 18:58:43 UTC",
    "updated_date": "2025-05-20 03:37:51 UTC"
  },
  {
    "arxiv_id": "2502.00094v2",
    "title": "AIN: The Arabic INclusive Large Multimodal Model",
    "authors": [
      "Ahmed Heakl",
      "Sara Ghaboura",
      "Omkar Thawkar",
      "Fahad Shahbaz Khan",
      "Hisham Cholakkal",
      "Rao Muhammad Anwer",
      "Salman Khan"
    ],
    "abstract": "Amid the swift progress of large language models (LLMs) and their evolution\ninto large multimodal models (LMMs), significant strides have been made in\nhigh-resource languages such as English and Chinese. While Arabic LLMs have\nseen notable progress, Arabic LMMs remain largely unexplored, often narrowly\nfocusing on a few specific aspects of the language and visual understanding. To\nbridge this gap, we introduce AIN-the Arabic Inclusive Multimodal\nModel-designed to excel across diverse domains. AIN is an English-Arabic\nbilingual LMM designed to excel in English and Arabic, leveraging carefully\nconstructed 3.6 million high-quality Arabic-English multimodal data samples.\nAIN demonstrates state-of-the-art Arabic performance, while also possessing\nstrong English-language visual capabilities. On the recent CAMEL-Bench\nbenchmark comprising 38 sub-domains including, multi-image understanding,\ncomplex visual perception, handwritten document understanding, video\nunderstanding, medical imaging, plant diseases, and remote sensing-based land\nuse understanding, our AIN demonstrates strong performance with the 7B model\noutperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains\nand 38 sub-domains. AIN's superior capabilities position it as a significant\nstep toward empowering Arabic speakers with advanced multimodal generative AI\ntools across diverse applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 16 figures, ACL",
    "pdf_url": "http://arxiv.org/pdf/2502.00094v2",
    "published_date": "2025-01-31 18:58:20 UTC",
    "updated_date": "2025-02-04 18:05:23 UTC"
  },
  {
    "arxiv_id": "2501.19400v1",
    "title": "Vintix: Action Model via In-Context Reinforcement Learning",
    "authors": [
      "Andrey Polubarov",
      "Nikita Lyubaykin",
      "Alexander Derevyagin",
      "Ilya Zisman",
      "Denis Tarasov",
      "Alexander Nikulin",
      "Vladislav Kurenkov"
    ],
    "abstract": "In-Context Reinforcement Learning (ICRL) represents a promising paradigm for\ndeveloping generalist agents that learn at inference time through\ntrial-and-error interactions, analogous to how large language models adapt\ncontextually, but with a focus on reward maximization. However, the scalability\nof ICRL beyond toy tasks and single-domain settings remains an open challenge.\nIn this work, we present the first steps toward scaling ICRL by introducing a\nfixed, cross-domain model capable of learning behaviors through in-context\nreinforcement learning. Our results demonstrate that Algorithm Distillation, a\nframework designed to facilitate ICRL, offers a compelling and competitive\nalternative to expert distillation to construct versatile action models. These\nfindings highlight the potential of ICRL as a scalable approach for generalist\ndecision-making systems. Code to be released at\nhttps://github.com/dunnolab/vintix",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint. In review",
    "pdf_url": "http://arxiv.org/pdf/2501.19400v1",
    "published_date": "2025-01-31 18:57:08 UTC",
    "updated_date": "2025-01-31 18:57:08 UTC"
  },
  {
    "arxiv_id": "2501.19399v1",
    "title": "Scalable-Softmax Is Superior for Attention",
    "authors": [
      "Ken M. Nakanishi"
    ],
    "abstract": "The maximum element of the vector output by the Softmax function approaches\nzero as the input vector size increases. Transformer-based language models rely\non Softmax to compute attention scores, causing the attention distribution to\nflatten as the context size grows. This reduces the model's ability to\nprioritize key information effectively and potentially limits its length\ngeneralization. To address this problem, we propose Scalable-Softmax (SSMax),\nwhich replaces Softmax in scenarios where the input vector size varies. SSMax\ncan be seamlessly integrated into existing Transformer-based architectures.\nExperimental results in language modeling show that models using SSMax not only\nachieve faster loss reduction during pretraining but also significantly improve\nperformance in long contexts and key information retrieval. Furthermore, an\nanalysis of attention scores reveals that SSMax enables the model to focus\nattention on key information even in long contexts. Additionally, although\nmodels that use SSMax from the beginning of pretraining achieve better length\ngeneralization, those that have already started pretraining can still gain some\nof this ability by replacing Softmax in the attention layers with SSMax, either\nduring or after pretraining.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.19399v1",
    "published_date": "2025-01-31 18:55:35 UTC",
    "updated_date": "2025-01-31 18:55:35 UTC"
  },
  {
    "arxiv_id": "2501.19398v1",
    "title": "Do LLMs Strategically Reveal, Conceal, and Infer Information? A Theoretical and Empirical Analysis in The Chameleon Game",
    "authors": [
      "Mustafa O. Karabag",
      "Ufuk Topcu"
    ],
    "abstract": "Large language model-based (LLM-based) agents have become common in settings\nthat include non-cooperative parties. In such settings, agents' decision-making\nneeds to conceal information from their adversaries, reveal information to\ntheir cooperators, and infer information to identify the other agents'\ncharacteristics. To investigate whether LLMs have these information control and\ndecision-making capabilities, we make LLM agents play the language-based\nhidden-identity game, The Chameleon. In the game, a group of non-chameleon\nagents who do not know each other aim to identify the chameleon agent without\nrevealing a secret. The game requires the aforementioned information control\ncapabilities both as a chameleon and a non-chameleon. The empirical results\nshow that while non-chameleon LLM agents identify the chameleon, they fail to\nconceal the secret from the chameleon, and their winning probability is far\nfrom the levels of even trivial strategies. To formally explain this behavior,\nwe give a theoretical analysis for a spectrum of strategies, from concealing to\nrevealing, and provide bounds on the non-chameleons' winning probability. Based\non the empirical results and theoretical analysis of different strategies, we\ndeduce that LLM-based non-chameleon agents reveal excessive information to\nagents of unknown identities. Our results point to a weakness of contemporary\nLLMs, including GPT-4, GPT-4o, Gemini 1.5, and Claude 3.5 Sonnet, in strategic\ninteractions.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19398v1",
    "published_date": "2025-01-31 18:53:43 UTC",
    "updated_date": "2025-01-31 18:53:43 UTC"
  },
  {
    "arxiv_id": "2501.19393v3",
    "title": "s1: Simple test-time scaling",
    "authors": [
      "Niklas Muennighoff",
      "Zitong Yang",
      "Weijia Shi",
      "Xiang Lisa Li",
      "Li Fei-Fei",
      "Hannaneh Hajishirzi",
      "Luke Zettlemoyer",
      "Percy Liang",
      "Emmanuel Candès",
      "Tatsunori Hashimoto"
    ],
    "abstract": "Test-time scaling is a promising new approach to language modeling that uses\nextra test-time compute to improve performance. Recently, OpenAI's o1 model\nshowed this capability but did not publicly share its methodology, leading to\nmany replication efforts. We seek the simplest approach to achieve test-time\nscaling and strong reasoning performance. First, we curate a small dataset s1K\nof 1,000 questions paired with reasoning traces relying on three criteria we\nvalidate through ablations: difficulty, diversity, and quality. Second, we\ndevelop budget forcing to control test-time compute by forcefully terminating\nthe model's thinking process or lengthening it by appending \"Wait\" multiple\ntimes to the model's generation when it tries to end. This can lead the model\nto double-check its answer, often fixing incorrect reasoning steps. After\nsupervised finetuning the Qwen2.5-32B-Instruct language model on s1K and\nequipping it with budget forcing, our model s1-32B exceeds o1-preview on\ncompetition math questions by up to 27% (MATH and AIME24). Further, scaling\ns1-32B with budget forcing allows extrapolating beyond its performance without\ntest-time intervention: from 50% to 57% on AIME24. Our model, data, and code\nare open-source at https://github.com/simplescaling/s1",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "46 pages (9 main), 10 figures, 15 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.19393v3",
    "published_date": "2025-01-31 18:48:08 UTC",
    "updated_date": "2025-03-01 06:07:39 UTC"
  },
  {
    "arxiv_id": "2501.19383v1",
    "title": "Decoding-based Regression",
    "authors": [
      "Xingyou Song",
      "Dara Bahri"
    ],
    "abstract": "Language models have recently been shown capable of performing regression\ntasks wherein numeric predictions are represented as decoded strings. In this\nwork, we provide theoretical grounds for this capability and furthermore\ninvestigate the utility of causal auto-regressive sequence models when they are\napplied to any feature representation. We find that, despite being trained in\nthe usual way - for next-token prediction via cross-entropy loss -\ndecoding-based regression is as performant as traditional approaches for\ntabular regression tasks, while being flexible enough to capture arbitrary\ndistributions, such as in the task of density estimation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Google DeepMind Technical Report, 25 pages. Code can be found at\n  https://github.com/google-research/optformer/tree/main/optformer/decoding_regression",
    "pdf_url": "http://arxiv.org/pdf/2501.19383v1",
    "published_date": "2025-01-31 18:37:42 UTC",
    "updated_date": "2025-01-31 18:37:42 UTC"
  },
  {
    "arxiv_id": "2501.19364v1",
    "title": "CoSTI: Consistency Models for (a faster) Spatio-Temporal Imputation",
    "authors": [
      "Javier Solís-García",
      "Belén Vega-Márquez",
      "Juan A. Nepomuceno",
      "Isabel A. Nepomuceno-Chamorro"
    ],
    "abstract": "Multivariate Time Series Imputation (MTSI) is crucial for many applications,\nsuch as healthcare monitoring and traffic management, where incomplete data can\ncompromise decision-making. Existing state-of-the-art methods, like Denoising\nDiffusion Probabilistic Models (DDPMs), achieve high imputation accuracy;\nhowever, they suffer from significant computational costs and are notably\ntime-consuming due to their iterative nature. In this work, we propose CoSTI,\nan innovative adaptation of Consistency Models (CMs) for the MTSI domain. CoSTI\nemploys Consistency Training to achieve comparable imputation quality to DDPMs\nwhile drastically reducing inference times, making it more suitable for\nreal-time applications. We evaluate CoSTI across multiple datasets and missing\ndata scenarios, demonstrating up to a 98% reduction in imputation time with\nperformance on par with diffusion-based models. This work bridges the gap\nbetween efficiency and accuracy in generative imputation tasks, providing a\nscalable solution for handling missing data in critical spatio-temporal\nsystems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 5 figures, 13 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.19364v1",
    "published_date": "2025-01-31 18:14:28 UTC",
    "updated_date": "2025-01-31 18:14:28 UTC"
  },
  {
    "arxiv_id": "2501.19361v1",
    "title": "We're Different, We're the Same: Creative Homogeneity Across LLMs",
    "authors": [
      "Emily Wenger",
      "Yoed Kenett"
    ],
    "abstract": "Numerous powerful large language models (LLMs) are now available for use as\nwriting support tools, idea generators, and beyond. Although these LLMs are\nmarketed as helpful creative assistants, several works have shown that using an\nLLM as a creative partner results in a narrower set of creative outputs.\nHowever, these studies only consider the effects of interacting with a single\nLLM, begging the question of whether such narrowed creativity stems from using\na particular LLM -- which arguably has a limited range of outputs -- or from\nusing LLMs in general as creative assistants. To study this question, we elicit\ncreative responses from humans and a broad set of LLMs using standardized\ncreativity tests and compare the population-level diversity of responses. We\nfind that LLM responses are much more similar to other LLM responses than human\nresponses are to each other, even after controlling for response structure and\nother key variables. This finding of significant homogeneity in creative\noutputs across the LLMs we evaluate adds a new dimension to the ongoing\nconversation about creativity and LLMs. If today's LLMs behave similarly, using\nthem as a creative partners -- regardless of the model used -- may drive all\nusers towards a limited set of \"creative\" outputs.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19361v1",
    "published_date": "2025-01-31 18:12:41 UTC",
    "updated_date": "2025-01-31 18:12:41 UTC"
  },
  {
    "arxiv_id": "2502.00089v1",
    "title": "Ensembles of Low-Rank Expert Adapters",
    "authors": [
      "Yinghao Li",
      "Vianne Gao",
      "Chao Zhang",
      "MohamadAli Torkamani"
    ],
    "abstract": "The training and fine-tuning of large language models (LLMs) often involve\ndiverse textual data from multiple sources, which poses challenges due to\nconflicting gradient directions, hindering optimization and specialization.\nThese challenges can undermine model generalization across tasks, resulting in\nreduced downstream performance. Recent research suggests that fine-tuning LLMs\non carefully selected, task-specific subsets of data can match or even surpass\nthe performance of using the entire dataset. Building on these insights, we\npropose the Ensembles of Low-Rank Expert Adapters (ELREA) framework to improve\nthe model's capability to handle diverse tasks. ELREA clusters the training\ninstructions based on their gradient directions, representing different areas\nof expertise and thereby reducing conflicts during optimization. Expert\nadapters are then trained on these clusters, utilizing the low-rank adaptation\n(LoRA) technique to ensure training efficiency and model scalability. During\ninference, ELREA combines predictions from the most relevant expert adapters\nbased on the input data's gradient similarity to the training clusters,\nensuring optimal adapter selection for each task. Experiments show that our\nmethod outperforms baseline LoRA adapters trained on the full dataset and other\nensemble approaches with similar training and inference complexity across a\nrange of domain-specific tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "29 pages, 5 figures, 5 tables; proceedings in ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.00089v1",
    "published_date": "2025-01-31 18:07:21 UTC",
    "updated_date": "2025-01-31 18:07:21 UTC"
  },
  {
    "arxiv_id": "2501.19353v3",
    "title": "Do Large Multimodal Models Solve Caption Generation for Scientific Figures? Lessons Learned from SciCap Challenge 2023",
    "authors": [
      "Ting-Yao E. Hsu",
      "Yi-Li Hsu",
      "Shaurya Rohatgi",
      "Chieh-Yang Huang",
      "Ho Yin Sam Ng",
      "Ryan Rossi",
      "Sungchul Kim",
      "Tong Yu",
      "Lun-Wei Ku",
      "C. Lee Giles",
      "Ting-Hao K. Huang"
    ],
    "abstract": "Since the SciCap datasets launch in 2021, the research community has made\nsignificant progress in generating captions for scientific figures in scholarly\narticles. In 2023, the first SciCap Challenge took place, inviting global teams\nto use an expanded SciCap dataset to develop models for captioning diverse\nfigure types across various academic fields. At the same time, text generation\nmodels advanced quickly, with many powerful pre-trained large multimodal models\n(LMMs) emerging that showed impressive capabilities in various\nvision-and-language tasks. This paper presents an overview of the first SciCap\nChallenge and details the performance of various models on its data, capturing\na snapshot of the fields state. We found that professional editors\noverwhelmingly preferred figure captions generated by GPT-4V over those from\nall other models and even the original captions written by authors. Following\nthis key finding, we conducted detailed analyses to answer this question: Have\nadvanced LMMs solved the task of generating captions for scientific figures?",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to TACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.19353v3",
    "published_date": "2025-01-31 18:02:19 UTC",
    "updated_date": "2025-02-18 18:07:34 UTC"
  },
  {
    "arxiv_id": "2501.19338v1",
    "title": "Pathological MRI Segmentation by Synthetic Pathological Data Generation in Fetuses and Neonates",
    "authors": [
      "Misha P. T Kaandorp",
      "Damola Agbelese",
      "Hosna Asma-ull",
      "Hyun-Gi Kim",
      "Kelly Payette",
      "Patrice Grehten",
      "Gennari Antonio Giulio",
      "Levente István Lánczi",
      "Andras Jakab"
    ],
    "abstract": "Developing new methods for the automated analysis of clinical fetal and\nneonatal MRI data is limited by the scarcity of annotated pathological datasets\nand privacy concerns that often restrict data sharing, hindering the\neffectiveness of deep learning models. We address this in two ways. First, we\nintroduce Fetal&Neonatal-DDPM, a novel diffusion model framework designed to\ngenerate high-quality synthetic pathological fetal and neonatal MRIs from\nsemantic label images. Second, we enhance training data by modifying healthy\nlabel images through morphological alterations to simulate conditions such as\nventriculomegaly, cerebellar and pontocerebellar hypoplasia, and microcephaly.\nBy leveraging Fetal&Neonatal-DDPM, we synthesize realistic pathological MRIs\nfrom these modified pathological label images. Radiologists rated the synthetic\nMRIs as significantly (p < 0.05) superior in quality and diagnostic value\ncompared to real MRIs, demonstrating features such as blood vessels and choroid\nplexus, and improved alignment with label annotations. Synthetic pathological\ndata enhanced state-of-the-art nnUNet segmentation performance, particularly\nfor severe ventriculomegaly cases, with the greatest improvements achieved in\nventricle segmentation (Dice scores: 0.9253 vs. 0.7317). This study underscores\nthe potential of generative AI as transformative tool for data augmentation,\noffering improved segmentation performance in pathological cases. This\ndevelopment represents a significant step towards improving analysis and\nsegmentation accuracy in prenatal imaging, and also offers new ways for data\nanonymization through the generation of pathologic image data.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "30 pages, 4 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.19338v1",
    "published_date": "2025-01-31 17:36:24 UTC",
    "updated_date": "2025-01-31 17:36:24 UTC"
  },
  {
    "arxiv_id": "2501.19335v2",
    "title": "What is causal about causal models and representations?",
    "authors": [
      "Frederik Hytting Jørgensen",
      "Luigi Gresele",
      "Sebastian Weichwald"
    ],
    "abstract": "Causal Bayesian networks are 'causal' models since they make predictions\nabout interventional distributions. To connect such causal model predictions to\nreal-world outcomes, we must determine which actions in the world correspond to\nwhich interventions in the model. For example, to interpret an action as an\nintervention on a treatment variable, the action will presumably have to a)\nchange the distribution of treatment in a way that corresponds to the\nintervention, and b) not change other aspects, such as how the outcome depends\non the treatment; while the marginal distributions of some variables may change\nas an effect. We introduce a formal framework to make such requirements for\ndifferent interpretations of actions as interventions precise. We prove that\nthe seemingly natural interpretation of actions as interventions is circular:\nUnder this interpretation, every causal Bayesian network that correctly models\nthe observational distribution is trivially also interventionally valid, and no\naction yields empirical data that could possibly falsify such a model. We prove\nan impossibility result: No interpretation exists that is non-circular and\nsimultaneously satisfies a set of natural desiderata. Instead, we examine\nnon-circular interpretations that may violate some desiderata and show how this\nmay in turn enable the falsification of causal models. By rigorously examining\nhow a causal Bayesian network could be a 'causal' model of the world instead of\nmerely a mathematical object, our formal framework contributes to the\nconceptual foundations of causal representation learning, causal discovery, and\ncausal abstraction, while also highlighting some limitations of existing\napproaches.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "stat.ML",
    "comment": "50 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.19335v2",
    "published_date": "2025-01-31 17:35:21 UTC",
    "updated_date": "2025-02-03 17:24:50 UTC"
  },
  {
    "arxiv_id": "2501.19328v1",
    "title": "Capturing Temporal Dynamics in Large-Scale Canopy Tree Height Estimation",
    "authors": [
      "Jan Pauls",
      "Max Zimmer",
      "Berkant Turan",
      "Sassan Saatchi",
      "Philippe Ciais",
      "Sebastian Pokutta",
      "Fabian Gieseke"
    ],
    "abstract": "With the rise in global greenhouse gas emissions, accurate large-scale tree\ncanopy height maps are essential for understanding forest structure, estimating\nabove-ground biomass, and monitoring ecological disruptions. To this end, we\npresent a novel approach to generate large-scale, high-resolution canopy height\nmaps over time. Our model accurately predicts canopy height over multiple years\ngiven Sentinel-2 time series satellite data. Using GEDI LiDAR data as the\nground truth for training the model, we present the first 10m resolution\ntemporal canopy height map of the European continent for the period 2019-2022.\nAs part of this product, we also offer a detailed canopy height map for 2020,\nproviding more precise estimates than previous studies. Our pipeline and the\nresulting temporal height map are publicly available, enabling comprehensive\nlarge-scale monitoring of forests and, hence, facilitating future research and\necological analyses. For an interactive viewer, see\nhttps://europetreemap.projects.earthengine.app/view/temporalcanopyheight.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages main paper, 5 pages references and appendix, 8 figures, 5\n  tables",
    "pdf_url": "http://arxiv.org/pdf/2501.19328v1",
    "published_date": "2025-01-31 17:26:06 UTC",
    "updated_date": "2025-01-31 17:26:06 UTC"
  },
  {
    "arxiv_id": "2501.19324v2",
    "title": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning",
    "authors": [
      "Baohao Liao",
      "Yuhui Xu",
      "Hanze Dong",
      "Junnan Li",
      "Christof Monz",
      "Silvio Savarese",
      "Doyen Sahoo",
      "Caiming Xiong"
    ],
    "abstract": "We introduce Reward-Guided Speculative Decoding (RSD), a novel framework\naimed at improving the efficiency of inference in large language models (LLMs).\nRSD synergistically combines a lightweight draft model with a more powerful\ntarget model, incorporating a controlled bias to prioritize high-reward\noutputs, in contrast to existing speculative decoding methods that enforce\nstrict unbiasedness. RSD employs a process reward model to evaluate\nintermediate decoding steps and dynamically decide whether to invoke the target\nmodel, optimizing the trade-off between computational cost and output quality.\nWe theoretically demonstrate that a threshold-based mixture strategy achieves\nan optimal balance between resource utilization and performance. Extensive\nevaluations on challenging reasoning benchmarks, including Olympiad-level\ntasks, show that RSD delivers significant efficiency gains against decoding\nwith the target model only (up to 4.4x fewer FLOPs), while achieving\nsignificant better accuracy than parallel decoding method on average (up to\n+3.5). These results highlight RSD as a robust and cost-effective approach for\ndeploying LLMs in resource-intensive scenarios. The code is available at\nhttps://github.com/BaohaoLiao/RSD.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.19324v2",
    "published_date": "2025-01-31 17:19:57 UTC",
    "updated_date": "2025-02-14 07:30:00 UTC"
  },
  {
    "arxiv_id": "2501.19321v1",
    "title": "Language Bias in Self-Supervised Learning For Automatic Speech Recognition",
    "authors": [
      "Edward Storey",
      "Naomi Harte",
      "Peter Bell"
    ],
    "abstract": "Self-supervised learning (SSL) is used in deep learning to train on large\ndatasets without the need for expensive labelling of the data. Recently, large\nAutomatic Speech Recognition (ASR) models such as XLS-R have utilised SSL to\ntrain on over one hundred different languages simultaneously. However, deeper\ninvestigation shows that the bulk of the training data for XLS-R comes from a\nsmall number of languages. Biases learned through SSL have been shown to exist\nin multiple domains, but language bias in multilingual SSL ASR has not been\nthoroughly examined. In this paper, we utilise the Lottery Ticket Hypothesis\n(LTH) to identify language-specific subnetworks within XLS-R and test the\nperformance of these subnetworks on a variety of different languages. We are\nable to show that when fine-tuning, XLS-R bypasses traditional linguistic\nknowledge and builds only on weights learned from the languages with the\nlargest data contribution to the pretraining data.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted to Speech and Language Technology Workshop (SLT) 2024\n  accessible on IEEE Xplore",
    "pdf_url": "http://arxiv.org/pdf/2501.19321v1",
    "published_date": "2025-01-31 17:16:45 UTC",
    "updated_date": "2025-01-31 17:16:45 UTC"
  },
  {
    "arxiv_id": "2501.19318v1",
    "title": "MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented Reinforcement in Embodied Systems",
    "authors": [
      "Anirudh Chari",
      "Suraj Reddy",
      "Aditya Tiwari",
      "Richard Lian",
      "Brian Zhou"
    ],
    "abstract": "While large language models (LLMs) have shown promising capabilities as\nzero-shot planners for embodied agents, their inability to learn from\nexperience and build persistent mental models limits their robustness in\ncomplex open-world environments like Minecraft. We introduce MINDSTORES, an\nexperience-augmented planning framework that enables embodied agents to build\nand leverage mental models through natural interaction with their environment.\nDrawing inspiration from how humans construct and refine cognitive mental\nmodels, our approach extends existing zero-shot LLM planning by maintaining a\ndatabase of past experiences that informs future planning iterations. The key\ninnovation is representing accumulated experiences as natural language\nembeddings of (state, task, plan, outcome) tuples, which can then be\nefficiently retrieved and reasoned over by an LLM planner to generate insights\nand guide plan refinement for novel states and tasks. Through extensive\nexperiments in the MineDojo environment, a simulation environment for agents in\nMinecraft that provides low-level controls for Minecraft, we find that\nMINDSTORES learns and applies its knowledge significantly better than existing\nmemory-based LLM planners while maintaining the flexibility and generalization\nbenefits of zero-shot approaches, representing an important step toward more\ncapable embodied AI systems that can learn continuously through natural\nexperience.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19318v1",
    "published_date": "2025-01-31 17:15:33 UTC",
    "updated_date": "2025-01-31 17:15:33 UTC"
  },
  {
    "arxiv_id": "2501.19308v1",
    "title": "Ontological analysis of proactive life event services",
    "authors": [
      "Kuldar Taveter"
    ],
    "abstract": "Life event service is a direct digital public service provided jointly by\nseveral governmental institutions so that a person can fulfill all the\nobligations and use all the rights that arise due to a particular event or\nsituation in personal life. Life event service consolidates several public\nservices related to the same life event into one service for the service\nconsumer. This paper presents an ontological analysis of life event services,\nwhich is based on the works by Guarino, Guizzardi, Nardi, Wagner, and others.\nThe purpose of the ontological analysis is to understand the meanings of life\nevent, proactive public service based on life event, and other related notions.\nThis kind of ontological analysis is crucial because for implementing the\nhardware and software architectures of e-government and digital public\nservices, it is essential to agree upon the precise meanings of the underlying\nterms.",
    "categories": [
      "cs.AI",
      "H.1.1"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19308v1",
    "published_date": "2025-01-31 17:09:53 UTC",
    "updated_date": "2025-01-31 17:09:53 UTC"
  },
  {
    "arxiv_id": "2501.19306v2",
    "title": "SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling",
    "authors": [
      "Jiefeng Chen",
      "Jie Ren",
      "Xinyun Chen",
      "Chengrun Yang",
      "Ruoxi Sun",
      "Sercan Ö Arık"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have created new\nopportunities to enhance performance on complex reasoning tasks by leveraging\ntest-time computation. However, conventional approaches such as repeated\nsampling with majority voting or reward model scoring, often face diminishing\nreturns as test-time compute scales, in addition to requiring costly\ntask-specific reward model training. In this paper, we present Self-Enhanced\nTest-Time Scaling (SETS), a novel method that leverages the self-verification\nand self-correction capabilities of recent advanced LLMs to overcome these\nlimitations. SETS integrates sampling, self-verification, and self-correction\ninto a unified framework, enabling efficient and scalable test-time computation\nfor improved capabilities at complex tasks. Through extensive experiments on\nchallenging planning and reasoning benchmarks, compared to the alternatives, we\ndemonstrate that SETS achieves significant performance improvements and more\nfavorable test-time scaling laws.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19306v2",
    "published_date": "2025-01-31 17:03:16 UTC",
    "updated_date": "2025-02-03 06:21:08 UTC"
  },
  {
    "arxiv_id": "2501.19301v1",
    "title": "Beyond checkmate: exploring the creative chokepoints in AI text",
    "authors": [
      "Nafis Irtiza Tripto",
      "Saranya Venkatraman",
      "Mahjabin Nahar",
      "Dongwon Lee"
    ],
    "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP) and Artificial Intelligence (AI), unlocking unprecedented capabilities.\nThis rapid advancement has spurred research into various aspects of LLMs, their\ntext generation & reasoning capability, and potential misuse, fueling the\nnecessity for robust detection methods. While numerous prior research has\nfocused on detecting LLM-generated text (AI text) and thus checkmating them,\nour study investigates a relatively unexplored territory: portraying the\nnuanced distinctions between human and AI texts across text segments. Whether\nLLMs struggle with or excel at incorporating linguistic ingenuity across\ndifferent text segments carries substantial implications for determining their\npotential as effective creative assistants to humans. Through an analogy with\nthe structure of chess games-comprising opening, middle, and end games-we\nanalyze text segments (introduction, body, and conclusion) to determine where\nthe most significant distinctions between human and AI texts exist. While AI\ntexts can approximate the body segment better due to its increased length, a\ncloser examination reveals a pronounced disparity, highlighting the importance\nof this segment in AI text detection. Additionally, human texts exhibit higher\ncross-segment differences compared to AI texts. Overall, our research can shed\nlight on the intricacies of human-AI text distinctions, offering novel insights\nfor text detection and understanding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, single columns, under review at Nature Machine Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2501.19301v1",
    "published_date": "2025-01-31 16:57:01 UTC",
    "updated_date": "2025-01-31 16:57:01 UTC"
  },
  {
    "arxiv_id": "2501.19298v1",
    "title": "Synthetic User Behavior Sequence Generation with Large Language Models for Smart Homes",
    "authors": [
      "Zhiyao Xu",
      "Dan Zhao",
      "Qingsong Zou",
      "Jingyu Xiao",
      "Yong Jiang",
      "Zhenhui Yuan",
      "Qing Li"
    ],
    "abstract": "In recent years, as smart home systems have become more widespread, security\nconcerns within these environments have become a growing threat. Currently,\nmost smart home security solutions, such as anomaly detection and behavior\nprediction models, are trained using fixed datasets that are precollected.\nHowever, the process of dataset collection is time-consuming and lacks the\nflexibility needed to adapt to the constantly evolving smart home environment.\nAdditionally, the collection of personal data raises significant privacy\nconcerns for users. Lately, large language models (LLMs) have emerged as a\npowerful tool for a wide range of tasks across diverse application domains,\nthanks to their strong capabilities in natural language processing, reasoning,\nand problem-solving. In this paper, we propose an LLM-based synthetic dataset\ngeneration IoTGen framework to enhance the generalization of downstream smart\nhome intelligent models. By generating new synthetic datasets that reflect\nchanges in the environment, smart home intelligent models can be retrained to\novercome the limitations of fixed and outdated data, allowing them to better\nalign with the dynamic nature of real-world home environments. Specifically, we\nfirst propose a Structure Pattern Perception Compression (SPPC) method tailored\nfor IoT behavior data, which preserves the most informative content in the data\nwhile significantly reducing token consumption. Then, we propose a systematic\napproach to create prompts and implement data generation to automatically\ngenerate IoT synthetic data with normative and reasonable properties, assisting\ntask models in adaptive training to improve generalization and real-world\nperformance.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19298v1",
    "published_date": "2025-01-31 16:55:43 UTC",
    "updated_date": "2025-01-31 16:55:43 UTC"
  },
  {
    "arxiv_id": "2501.19297v2",
    "title": "Analysis of LLMs vs Human Experts in Requirements Engineering",
    "authors": [
      "Cory Hymel",
      "Hiroe Johnson"
    ],
    "abstract": "The majority of research around Large Language Models (LLM) application to\nsoftware development has been on the subject of code generation. There is\nlittle literature on LLMs' impact on requirements engineering (RE), which deals\nwith the process of developing and verifying the system requirements. Within\nRE, there is a subdiscipline of requirements elicitation, which is the practice\nof discovering and documenting requirements for a system from users, customers,\nand other stakeholders. In this analysis, we compare LLM's ability to elicit\nrequirements of a software system, as compared to that of a human expert in a\ntime-boxed and prompt-boxed study. We found LLM-generated requirements were\nevaluated as more aligned (+1.12) than human-generated requirements with a\ntrend of being more complete (+10.2%). Conversely, we found users tended to\nbelieve that solutions they perceived as more aligned had been generated by\nhuman experts. Furthermore, while LLM-generated documents scored higher and\nperformed at 720x the speed, their cost was, on average, only 0.06% that of a\nhuman expert. Overall, these findings indicate that LLMs will play an\nincreasingly important role in requirements engineering by improving\nrequirements definitions, enabling more efficient resource allocation, and\nreducing overall project timelines.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "8 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.19297v2",
    "published_date": "2025-01-31 16:55:17 UTC",
    "updated_date": "2025-02-04 15:33:51 UTC"
  },
  {
    "arxiv_id": "2501.19271v1",
    "title": "Concept-Based Explainable Artificial Intelligence: Metrics and Benchmarks",
    "authors": [
      "Halil Ibrahim Aysel",
      "Xiaohao Cai",
      "Adam Prugel-Bennett"
    ],
    "abstract": "Concept-based explanation methods, such as concept bottleneck models (CBMs),\naim to improve the interpretability of machine learning models by linking their\ndecisions to human-understandable concepts, under the critical assumption that\nsuch concepts can be accurately attributed to the network's feature space.\nHowever, this foundational assumption has not been rigorously validated, mainly\nbecause the field lacks standardised metrics and benchmarks to assess the\nexistence and spatial alignment of such concepts. To address this, we propose\nthree metrics: the concept global importance metric, the concept existence\nmetric, and the concept location metric, including a technique for visualising\nconcept activations, i.e., concept activation mapping. We benchmark post-hoc\nCBMs to illustrate their capabilities and challenges. Through qualitative and\nquantitative experiments, we demonstrate that, in many cases, even the most\nimportant concepts determined by post-hoc CBMs are not present in input images;\nmoreover, when they are present, their saliency maps fail to align with the\nexpected regions by either activating across an entire object or misidentifying\nrelevant concept-specific regions. We analyse the root causes of these\nlimitations, such as the natural correlation of concepts. Our findings\nunderscore the need for more careful application of concept-based explanation\ntechniques especially in settings where spatial interpretability is critical.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages it total, 8 main pages",
    "pdf_url": "http://arxiv.org/pdf/2501.19271v1",
    "published_date": "2025-01-31 16:32:36 UTC",
    "updated_date": "2025-01-31 16:32:36 UTC"
  },
  {
    "arxiv_id": "2501.19266v1",
    "title": "Jackpot! Alignment as a Maximal Lottery",
    "authors": [
      "Roberto-Rafael Maura-Rivero",
      "Marc Lanctot",
      "Francesco Visin",
      "Kate Larson"
    ],
    "abstract": "Reinforcement Learning from Human Feedback (RLHF), the standard for aligning\nLarge Language Models (LLMs) with human values, is known to fail to satisfy\nproperties that are intuitively desirable, such as respecting the preferences\nof the majority \\cite{ge2024axioms}. To overcome these issues, we propose the\nuse of a probabilistic Social Choice rule called \\emph{maximal lotteries} as a\nreplacement for RLHF. We show that a family of alignment techniques, namely\nNash Learning from Human Feedback (NLHF) \\cite{munos2023nash} and variants,\napproximate maximal lottery outcomes and thus inherit its beneficial\nproperties.\n  We confirm experimentally that our proposed methodology handles situations\nthat arise when working with preferences more robustly than standard RLHF,\nincluding supporting the preferences of the majority, providing principled ways\nof handling non-transitivities in the preference data, and robustness to\nirrelevant alternatives. This results in systems that better incorporate human\nvalues and respect human intentions.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "econ.TH"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19266v1",
    "published_date": "2025-01-31 16:26:28 UTC",
    "updated_date": "2025-01-31 16:26:28 UTC"
  },
  {
    "arxiv_id": "2501.19256v1",
    "title": "Objective Metrics for Human-Subjects Evaluation in Explainable Reinforcement Learning",
    "authors": [
      "Balint Gyevnar",
      "Mark Towers"
    ],
    "abstract": "Explanation is a fundamentally human process. Understanding the goal and\naudience of the explanation is vital, yet existing work on explainable\nreinforcement learning (XRL) routinely does not consult humans in their\nevaluations. Even when they do, they routinely resort to subjective metrics,\nsuch as confidence or understanding, that can only inform researchers of users'\nopinions, not their practical effectiveness for a given problem. This paper\ncalls on researchers to use objective human metrics for explanation evaluations\nbased on observable and actionable behaviour to build more reproducible,\ncomparable, and epistemically grounded research. To this end, we curate,\ndescribe, and compare several objective evaluation methodologies for applying\nexplanations to debugging agent behaviour and supporting human-agent teaming,\nillustrating our proposed methods using a novel grid-based environment. We\ndiscuss how subjective and objective metrics complement each other to provide\nholistic validation and how future work needs to utilise standardised\nbenchmarks for testing to enable greater comparisons between research.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19256v1",
    "published_date": "2025-01-31 16:12:23 UTC",
    "updated_date": "2025-01-31 16:12:23 UTC"
  },
  {
    "arxiv_id": "2501.19254v3",
    "title": "Linear $Q$-Learning Does Not Diverge in $L^2$: Convergence Rates to a Bounded Set",
    "authors": [
      "Xinyu Liu",
      "Zixuan Xie",
      "Shangtong Zhang"
    ],
    "abstract": "$Q$-learning is one of the most fundamental reinforcement learning\nalgorithms. It is widely believed that $Q$-learning with linear function\napproximation (i.e., linear $Q$-learning) suffers from possible divergence\nuntil the recent work Meyn (2024) which establishes the ultimate almost sure\nboundedness of the iterates of linear $Q$-learning. Building on this success,\nthis paper further establishes the first $L^2$ convergence rate of linear\n$Q$-learning iterates (to a bounded set). Similar to Meyn (2024), we do not\nmake any modification to the original linear $Q$-learning algorithm, do not\nmake any Bellman completeness assumption, and do not make any near-optimality\nassumption on the behavior policy. All we need is an $\\epsilon$-softmax\nbehavior policy with an adaptive temperature. The key to our analysis is the\ngeneral result of stochastic approximations under Markovian noise with\nfast-changing transition functions. As a side product, we also use this general\nresult to establish the $L^2$ convergence rate of tabular $Q$-learning with an\n$\\epsilon$-softmax behavior policy, for which we rely on a novel\npseudo-contraction property of the weighted Bellman optimality operator.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19254v3",
    "published_date": "2025-01-31 16:10:50 UTC",
    "updated_date": "2025-02-24 16:39:27 UTC"
  },
  {
    "arxiv_id": "2501.19245v2",
    "title": "SHARPIE: A Modular Framework for Reinforcement Learning and Human-AI Interaction Experiments",
    "authors": [
      "Hüseyin Aydın",
      "Kevin Godin-Dubois",
      "Libio Goncalvez Braz",
      "Floris den Hengst",
      "Kim Baraka",
      "Mustafa Mert Çelikok",
      "Andreas Sauter",
      "Shihan Wang",
      "Frans A. Oliehoek"
    ],
    "abstract": "Reinforcement learning (RL) offers a general approach for modeling and\ntraining AI agents, including human-AI interaction scenarios. In this paper, we\npropose SHARPIE (Shared Human-AI Reinforcement Learning Platform for\nInteractive Experiments) to address the need for a generic framework to support\nexperiments with RL agents and humans. Its modular design consists of a\nversatile wrapper for RL environments and algorithm libraries, a\nparticipant-facing web interface, logging utilities, deployment on popular\ncloud and participant recruitment platforms. It empowers researchers to study a\nwide variety of research questions related to the interaction between humans\nand RL agents, including those related to interactive reward specification and\nlearning, learning from human feedback, action delegation, preference\nelicitation, user-modeling, and human-AI teaming. The platform is based on a\ngeneric interface for human-RL interactions that aims to standardize the field\nof study on RL in human contexts.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19245v2",
    "published_date": "2025-01-31 15:59:50 UTC",
    "updated_date": "2025-02-03 08:41:43 UTC"
  },
  {
    "arxiv_id": "2501.19232v1",
    "title": "A Zero-Shot Generalization Framework for LLM-Driven Cross-Domain Sequential Recommendation",
    "authors": [
      "Yunzhe Li",
      "Junting Wang",
      "Hari Sundaram",
      "Zhining Liu"
    ],
    "abstract": "Zero-shot cross-domain sequential recommendation (ZCDSR) enables predictions\nin unseen domains without the need for additional training or fine-tuning,\nmaking it particularly valuable in data-sparse environments where traditional\nmodels struggle. Recent advancements in large language models (LLMs) have\ngreatly improved ZCDSR by leveraging rich pretrained representations to\nfacilitate cross-domain knowledge transfer. However, a key challenge persists:\ndomain semantic bias, which arises from variations in vocabulary and content\nfocus across domains. This misalignment leads to inconsistencies in item\nembeddings and hinders generalization.\n  To address this issue, we propose a novel framework designed to enhance\nLLM-based ZCDSR by improving cross-domain alignment at both the item and\nsequential levels. At the item level, we introduce a generalization loss that\npromotes inter-domain compactness by aligning embeddings of similar items\nacross domains while maintaining intra-domain diversity to preserve unique item\ncharacteristics. This prevents embeddings from becoming overly generic while\nensuring effective transferability. At the sequential level, we develop a\nmethod for transferring user behavioral patterns by clustering user sequences\nin the source domain and applying attention-based aggregation for target domain\ninference. This dynamic adaptation of user embeddings allows effective\nzero-shot recommendations without requiring target-domain interactions.\n  Comprehensive experiments across multiple datasets and domains demonstrate\nthat our framework significantly improves sequential recommendation performance\nin the ZCDSR setting. By mitigating domain bias and enhancing the\ntransferability of sequential patterns, our method provides a scalable and\nrobust approach for achieving more effective zero-shot recommendations across\ndomains.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.19232v1",
    "published_date": "2025-01-31 15:43:21 UTC",
    "updated_date": "2025-01-31 15:43:21 UTC"
  },
  {
    "arxiv_id": "2501.19227v1",
    "title": "Integrating Semi-Supervised and Active Learning for Semantic Segmentation",
    "authors": [
      "Wanli Ma",
      "Oktay Karakus",
      "Paul L. Rosin"
    ],
    "abstract": "In this paper, we propose a novel active learning approach integrated with an\nimproved semi-supervised learning framework to reduce the cost of manual\nannotation and enhance model performance. Our proposed approach effectively\nleverages both the labelled data selected through active learning and the\nunlabelled data excluded from the selection process. The proposed active\nlearning approach pinpoints areas where the pseudo-labels are likely to be\ninaccurate. Then, an automatic and efficient pseudo-label auto-refinement\n(PLAR) module is proposed to correct pixels with potentially erroneous\npseudo-labels by comparing their feature representations with those of labelled\nregions. This approach operates without increasing the labelling budget and is\nbased on the cluster assumption, which states that pixels belonging to the same\nclass should exhibit similar representations in feature space. Furthermore,\nmanual labelling is only applied to the most difficult and uncertain areas in\nunlabelled data, where insufficient information prevents the PLAR module from\nmaking a decision. We evaluated the proposed hybrid semi-supervised active\nlearning framework on two benchmark datasets, one from natural and the other\nfrom remote sensing imagery domains. In both cases, it outperformed\nstate-of-the-art methods in the semantic segmentation task.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19227v1",
    "published_date": "2025-01-31 15:37:19 UTC",
    "updated_date": "2025-01-31 15:37:19 UTC"
  },
  {
    "arxiv_id": "2501.19215v2",
    "title": "Strassen Attention: Unlocking Compositional Abilities in Transformers Based on a New Lower Bound Method",
    "authors": [
      "Alexander Kozachinskiy",
      "Felipe Urrutia",
      "Hector Jimenez",
      "Tomasz Steifer",
      "Germán Pizarro",
      "Matías Fuentes",
      "Francisco Meza",
      "Cristian B. Calderon",
      "Cristóbal Rojas"
    ],
    "abstract": "We propose a novel method to evaluate the theoretical limits of Transformers,\nallowing us to prove the first lower bounds against one-layer softmax\nTransformers with infinite precision. We establish those bounds for three tasks\nthat require advanced reasoning. The first task, Match3 (Sanford et al., 2023),\nrequires looking at all triples of positions. The second and third tasks\naddress compositionality-based reasoning: one is composition of functions (Peng\net al., 2024) and the other is composition of binary relations. We formally\nprove the inability of one-layer softmax Transformers to solve any of these\ntasks. In an attempt to overcome these limitations, we introduce Strassen\nattention and prove that with this mechanism a one-layer Transformer can in\nprinciple solve all these tasks. We also show that it enjoys sub-cubic\nrunning-time complexity, making it more scalable than similar previously\nproposed mechanisms, such as higher-order attention (Sanford et al., 2023). To\ncomplement our theoretical findings, we experimentally studied Strassen\nattention and compared it against standard (Vaswani et al, 2017), higher-order\nattention (Sanford et al., 2023) and triangular attention (Bergen et al. 2021).\nOur results help to disentangle all these attention mechanisms, highlighting\ntheir strengths and limitations. In particular, Strassen attention outperforms\nstandard attention significantly on all the tasks. Altogether, understanding\nthe theoretical limitations can guide research towards scalable attention\nmechanisms that improve the reasoning abilities of Transformers.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19215v2",
    "published_date": "2025-01-31 15:21:54 UTC",
    "updated_date": "2025-02-06 12:45:47 UTC"
  },
  {
    "arxiv_id": "2501.19206v1",
    "title": "An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents",
    "authors": [
      "Gregory Palmer",
      "Luke Swaby",
      "Daniel J. B. Harrold",
      "Matthew Stewart",
      "Alex Hiles",
      "Chris Willis",
      "Ian Miles",
      "Sara Farmer"
    ],
    "abstract": "The recent rise in increasingly sophisticated cyber-attacks raises the need\nfor robust and resilient autonomous cyber-defence (ACD) agents. Given the\nvariety of cyber-attack tactics, techniques and procedures (TTPs) employed,\nlearning approaches that can return generalisable policies are desirable.\nMeanwhile, the assurance of ACD agents remains an open challenge. We address\nboth challenges via an empirical game-theoretic analysis of deep reinforcement\nlearning (DRL) approaches for ACD using the principled double oracle (DO)\nalgorithm. This algorithm relies on adversaries iteratively learning\n(approximate) best responses against each others' policies; a computationally\nexpensive endeavour for autonomous cyber operations agents. In this work we\nintroduce and evaluate a theoretically-sound, potential-based reward shaping\napproach to expedite this process. In addition, given the increasing number of\nopen-source ACD-DRL approaches, we extend the DO formulation to allow for\nmultiple response oracles (MRO), providing a framework for a holistic\nevaluation of ACD approaches.",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.GT"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages, 17 figures, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.19206v1",
    "published_date": "2025-01-31 15:15:02 UTC",
    "updated_date": "2025-01-31 15:15:02 UTC"
  },
  {
    "arxiv_id": "2501.19203v1",
    "title": "Single cell resolution 3D imaging and segmentation within intact live tissues",
    "authors": [
      "G. Paci",
      "P. Vicente-Munuera",
      "I. Fernandez-Mosquera",
      "A. Miranda",
      "K. Lau",
      "Q. Zhang",
      "R. Barrientos",
      "Y. Mao"
    ],
    "abstract": "Epithelial cells form diverse structures from squamous spherical organoids to\ndensely packed pseudostratified tissues. Quantification of cellular properties\nin these contexts requires high-resolution deep imaging and computational\ntechniques to achieve truthful three-dimensional (3D) structural features.\nHere, we describe a detailed step-by-step protocol for sample preparation,\nimaging and deep-learning-assisted cell segmentation to achieve accurate\nquantification of fluorescently labelled individual cells in 3D within live\ntissues. We share the lessons learned through troubleshooting 3D imaging of\nDrosophila wing discs, including considerations on the choice of microscopy\nmodality and settings (objective, sample mounting) and available segmentation\nmethods. In addition, we include a computational pipeline alongside custom code\nto assist replication of the protocol. While we focus on the segmentation of\ncell outlines from membrane labelling, this protocol applies to a wide variety\nof samples, and we believe it be valuable for studying other tissues that\ndemand complex analysis in 3D.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CV",
      "q-bio.CB",
      "q-bio.TO"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19203v1",
    "published_date": "2025-01-31 15:13:04 UTC",
    "updated_date": "2025-01-31 15:13:04 UTC"
  },
  {
    "arxiv_id": "2501.19201v1",
    "title": "Efficient Reasoning with Hidden Thinking",
    "authors": [
      "Xuan Shen",
      "Yizhou Wang",
      "Xiangxi Shi",
      "Yanzhi Wang",
      "Pu Zhao",
      "Jiuxiang Gu"
    ],
    "abstract": "Chain-of-Thought (CoT) reasoning has become a powerful framework for\nimproving complex problem-solving capabilities in Multimodal Large Language\nModels (MLLMs). However, the verbose nature of textual reasoning introduces\nsignificant inefficiencies. In this work, we propose $\\textbf{Heima}$ (as\nhidden llama), an efficient reasoning framework that leverages reasoning CoTs\nat hidden latent space. We design the Heima Encoder to condense each\nintermediate CoT into a compact, higher-level hidden representation using a\nsingle thinking token, effectively minimizing verbosity and reducing the\noverall number of tokens required during the reasoning process. Meanwhile, we\ndesign corresponding Heima Decoder with traditional Large Language Models\n(LLMs) to adaptively interpret the hidden representations into variable-length\ntextual sequence, reconstructing reasoning processes that closely resemble the\noriginal CoTs. Experimental results across diverse reasoning MLLM benchmarks\ndemonstrate that Heima model achieves higher generation efficiency while\nmaintaining or even better zero-shot task accuracy. Moreover, the effective\nreconstruction of multimodal reasoning processes with Heima Decoder validates\nboth the robustness and interpretability of our approach.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint version",
    "pdf_url": "http://arxiv.org/pdf/2501.19201v1",
    "published_date": "2025-01-31 15:10:29 UTC",
    "updated_date": "2025-01-31 15:10:29 UTC"
  },
  {
    "arxiv_id": "2501.19195v1",
    "title": "Rethinking Early Stopping: Refine, Then Calibrate",
    "authors": [
      "Eugène Berta",
      "David Holzmüller",
      "Michael I. Jordan",
      "Francis Bach"
    ],
    "abstract": "Machine learning classifiers often produce probabilistic predictions that are\ncritical for accurate and interpretable decision-making in various domains. The\nquality of these predictions is generally evaluated with proper losses like\ncross-entropy, which decompose into two components: calibration error assesses\ngeneral under/overconfidence, while refinement error measures the ability to\ndistinguish different classes. In this paper, we provide theoretical and\nempirical evidence that these two errors are not minimized simultaneously\nduring training. Selecting the best training epoch based on validation loss\nthus leads to a compromise point that is suboptimal for both calibration error\nand, most importantly, refinement error. To address this, we introduce a new\nmetric for early stopping and hyperparameter tuning that makes it possible to\nminimize refinement error during training. The calibration error is minimized\nafter training, using standard techniques. Our method integrates seamlessly\nwith any architecture and consistently improves performance across diverse\nclassification tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19195v1",
    "published_date": "2025-01-31 15:03:54 UTC",
    "updated_date": "2025-01-31 15:03:54 UTC"
  },
  {
    "arxiv_id": "2501.19191v1",
    "title": "Secured Communication Schemes for UAVs in 5G: CRYSTALS-Kyber and IDS",
    "authors": [
      "Taneya Sharma",
      "Seyed Ahmad Soleymani",
      "Mohammad Shojafar",
      "Rahim Tafazolli"
    ],
    "abstract": "This paper introduces a secure communication architecture for Unmanned Aerial\nVehicles (UAVs) and ground stations in 5G networks, addressing critical\nchallenges in network security. The proposed solution integrates the Advanced\nEncryption Standard (AES) with Elliptic Curve Cryptography (ECC) and\nCRYSTALS-Kyber for key encapsulation, offering a hybrid cryptographic approach.\nBy incorporating CRYSTALS-Kyber, the framework mitigates vulnerabilities in ECC\nagainst quantum attacks, positioning it as a quantum-resistant alternative. The\narchitecture is based on a server-client model, with UAVs functioning as\nclients and the ground station acting as the server. The system was rigorously\nevaluated in both VPN and 5G environments. Experimental results confirm that\nCRYSTALS-Kyber delivers strong protection against quantum threats with minimal\nperformance overhead, making it highly suitable for UAVs with resource\nconstraints. Moreover, the proposed architecture integrates an Artificial\nIntelligence (AI)-based Intrusion Detection System (IDS) to further enhance\nsecurity. In performance evaluations, the IDS demonstrated strong results\nacross multiple models with XGBoost, particularly in more demanding scenarios,\noutperforming other models with an accuracy of 97.33% and an AUC of 0.94. These\nfindings underscore the potential of combining quantum-resistant encryption\nmechanisms with AI-driven IDS to create a robust, scalable, and secure\ncommunication framework for UAV networks, particularly within the\nhigh-performance requirements of 5G environments.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "6 pages, 5 figures, Paper accepted at IEEE FNWF'25 conference\n  (References number: 1571070613)",
    "pdf_url": "http://arxiv.org/pdf/2501.19191v1",
    "published_date": "2025-01-31 15:00:27 UTC",
    "updated_date": "2025-01-31 15:00:27 UTC"
  },
  {
    "arxiv_id": "2501.19180v1",
    "title": "Enhancing Model Defense Against Jailbreaks with Proactive Safety Reasoning",
    "authors": [
      "Xianglin Yang",
      "Gelei Deng",
      "Jieming Shi",
      "Tianwei Zhang",
      "Jin Song Dong"
    ],
    "abstract": "Large language models (LLMs) are vital for a wide range of applications yet\nremain susceptible to jailbreak threats, which could lead to the generation of\ninappropriate responses. Conventional defenses, such as refusal and adversarial\ntraining, often fail to cover corner cases or rare domains, leaving LLMs still\nvulnerable to more sophisticated attacks. We propose a novel defense strategy,\nSafety Chain-of-Thought (SCoT), which harnesses the enhanced \\textit{reasoning\ncapabilities} of LLMs for proactive assessment of harmful inputs, rather than\nsimply blocking them. SCoT augments any refusal training datasets to critically\nanalyze the intent behind each request before generating answers. By employing\nproactive reasoning, SCoT enhances the generalization of LLMs across varied\nharmful queries and scenarios not covered in the safety alignment corpus.\nAdditionally, it generates detailed refusals specifying the rules violated.\nComparative evaluations show that SCoT significantly surpasses existing\ndefenses, reducing vulnerability to out-of-distribution issues and adversarial\nmanipulations while maintaining strong general capabilities.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19180v1",
    "published_date": "2025-01-31 14:45:23 UTC",
    "updated_date": "2025-01-31 14:45:23 UTC"
  },
  {
    "arxiv_id": "2501.19176v1",
    "title": "Augmented Intelligence for Multimodal Virtual Biopsy in Breast Cancer Using Generative Artificial Intelligence",
    "authors": [
      "Aurora Rofena",
      "Claudia Lucia Piccolo",
      "Bruno Beomonte Zobel",
      "Paolo Soda",
      "Valerio Guarrasi"
    ],
    "abstract": "Full-Field Digital Mammography (FFDM) is the primary imaging modality for\nroutine breast cancer screening; however, its effectiveness is limited in\npatients with dense breast tissue or fibrocystic conditions. Contrast-Enhanced\nSpectral Mammography (CESM), a second-level imaging technique, offers enhanced\naccuracy in tumor detection. Nonetheless, its application is restricted due to\nhigher radiation exposure, the use of contrast agents, and limited\naccessibility. As a result, CESM is typically reserved for select cases,\nleaving many patients to rely solely on FFDM despite the superior diagnostic\nperformance of CESM. While biopsy remains the gold standard for definitive\ndiagnosis, it is an invasive procedure that can cause discomfort for patients.\nWe introduce a multimodal, multi-view deep learning approach for virtual\nbiopsy, integrating FFDM and CESM modalities in craniocaudal and mediolateral\noblique views to classify lesions as malignant or benign. To address the\nchallenge of missing CESM data, we leverage generative artificial intelligence\nto impute CESM images from FFDM scans. Experimental results demonstrate that\nincorporating the CESM modality is crucial to enhance the performance of\nvirtual biopsy. When real CESM data is missing, synthetic CESM images proved\neffective, outperforming the use of FFDM alone, particularly in multimodal\nconfigurations that combine FFDM and CESM modalities. The proposed approach has\nthe potential to improve diagnostic workflows, providing clinicians with\naugmented intelligence tools to improve diagnostic accuracy and patient care.\nAdditionally, as a contribution to the research community, we publicly release\nthe dataset used in our experiments, facilitating further advancements in this\nfield.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19176v1",
    "published_date": "2025-01-31 14:41:17 UTC",
    "updated_date": "2025-01-31 14:41:17 UTC"
  },
  {
    "arxiv_id": "2501.19155v1",
    "title": "SWAT: Sliding Window Adversarial Training for Gradual Domain Adaptation",
    "authors": [
      "Zixi Wang",
      "Yubo Huang",
      "Wenwei Luo",
      "Tonglan Xie",
      "Mengmeng Jing",
      "Lin Zuo"
    ],
    "abstract": "Domain shifts are critical issues that harm the performance of machine\nlearning. Unsupervised Domain Adaptation (UDA) mitigates this issue but suffers\nwhen the domain shifts are steep and drastic. Gradual Domain Adaptation (GDA)\nalleviates this problem in a mild way by gradually adapting from the source to\nthe target domain using multiple intermediate domains. In this paper, we\npropose Sliding Window Adversarial Training (SWAT) for Gradual Domain\nAdaptation. SWAT uses the construction of adversarial streams to connect the\nfeature spaces of the source and target domains. In order to gradually narrow\nthe small gap between adjacent intermediate domains, a sliding window paradigm\nis designed that moves along the adversarial stream. When the window moves to\nthe end of the stream, i.e., the target domain, the domain shift is drastically\nreduced. Extensive experiments are conducted on public GDA benchmarks, and the\nresults demonstrate that the proposed SWAT significantly outperforms the\nstate-of-the-art approaches. The implementation is available at:\nhttps://anonymous.4open.science/r/SWAT-8677.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "submitted to icml 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.19155v1",
    "published_date": "2025-01-31 14:16:22 UTC",
    "updated_date": "2025-01-31 14:16:22 UTC"
  },
  {
    "arxiv_id": "2501.19149v1",
    "title": "On the inductive bias of infinite-depth ResNets and the bottleneck rank",
    "authors": [
      "Enric Boix-Adsera"
    ],
    "abstract": "We compute the minimum-norm weights of a deep linear ResNet, and find that\nthe inductive bias of this architecture lies between minimizing nuclear norm\nand rank. This implies that, with appropriate hyperparameters, deep nonlinear\nResNets have an inductive bias towards minimizing bottleneck rank.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.19149v1",
    "published_date": "2025-01-31 14:06:13 UTC",
    "updated_date": "2025-01-31 14:06:13 UTC"
  },
  {
    "arxiv_id": "2501.19145v1",
    "title": "Improving Multi-Label Contrastive Learning by Leveraging Label Distribution",
    "authors": [
      "Ning Chen",
      "Shen-Huan Lyu",
      "Tian-Shuang Wu",
      "Yanyan Wang",
      "Bin Tang"
    ],
    "abstract": "In multi-label learning, leveraging contrastive learning to learn better\nrepresentations faces a key challenge: selecting positive and negative samples\nand effectively utilizing label information. Previous studies selected positive\nand negative samples based on the overlap between labels and used them for\nlabel-wise loss balancing. However, these methods suffer from a complex\nselection process and fail to account for the varying importance of different\nlabels. To address these problems, we propose a novel method that improves\nmulti-label contrastive learning through label distribution. Specifically, when\nselecting positive and negative samples, we only need to consider whether there\nis an intersection between labels. To model the relationships between labels,\nwe introduce two methods to recover label distributions from logical labels,\nbased on Radial Basis Function (RBF) and contrastive loss, respectively. We\nevaluate our method on nine widely used multi-label datasets, including image\nand vector datasets. The results demonstrate that our method outperforms\nstate-of-the-art methods in six evaluation metrics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19145v1",
    "published_date": "2025-01-31 14:00:02 UTC",
    "updated_date": "2025-01-31 14:00:02 UTC"
  },
  {
    "arxiv_id": "2501.19143v1",
    "title": "Imitation Game for Adversarial Disillusion with Multimodal Generative Chain-of-Thought Role-Play",
    "authors": [
      "Ching-Chun Chang",
      "Fan-Yun Chen",
      "Shih-Hong Gu",
      "Kai Gao",
      "Hanrui Wang",
      "Isao Echizen"
    ],
    "abstract": "As the cornerstone of artificial intelligence, machine perception confronts a\nfundamental threat posed by adversarial illusions. These adversarial attacks\nmanifest in two primary forms: deductive illusion, where specific stimuli are\ncrafted based on the victim model's general decision logic, and inductive\nillusion, where the victim model's general decision logic is shaped by specific\nstimuli. The former exploits the model's decision boundaries to create a\nstimulus that, when applied, interferes with its decision-making process. The\nlatter reinforces a conditioned reflex in the model, embedding a backdoor\nduring its learning phase that, when triggered by a stimulus, causes aberrant\nbehaviours. The multifaceted nature of adversarial illusions calls for a\nunified defence framework, addressing vulnerabilities across various forms of\nattack. In this study, we propose a disillusion paradigm based on the concept\nof an imitation game. At the heart of the imitation game lies a multimodal\ngenerative agent, steered by chain-of-thought reasoning, which observes,\ninternalises and reconstructs the semantic essence of a sample, liberated from\nthe classic pursuit of reversing the sample to its original state. As a proof\nof concept, we conduct experimental simulations using a multimodal generative\ndialogue agent and evaluates the methodology under a variety of attack\nscenarios.",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19143v1",
    "published_date": "2025-01-31 13:57:34 UTC",
    "updated_date": "2025-01-31 13:57:34 UTC"
  },
  {
    "arxiv_id": "2501.19137v1",
    "title": "A Metric for the Balance of Information in Graph Learning",
    "authors": [
      "Alex O. Davies",
      "Nirav S. Ajmeri",
      "Telmo de Menezes e Silva Filho"
    ],
    "abstract": "Graph learning on molecules makes use of information from both the molecular\nstructure and the features attached to that structure. Much work has been\nconducted on biasing either towards structure or features, with the aim that\nbias bolsters performance. Identifying which information source a dataset\nfavours, and therefore how to approach learning that dataset, is an open issue.\nHere we propose Noise-Noise Ratio Difference (NNRD), a quantitative metric for\nwhether there is more useful information in structure or features. By employing\niterative noising on features and structure independently, leaving the other\nintact, NNRD measures the degradation of information in each. We employ NNRD\nover a range of molecular tasks, and show that it corresponds well to a loss of\ninformation, with intuitive results that are more expressive than simple\nperformance aggregates. Our future work will focus on expanding data domains,\ntasks and types, as well as refining our choice of baseline model.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "In proceedings of the 4th Annual AAAI Workshop on AI to Accelerate\n  Science and Engineering (AI2ASE)",
    "pdf_url": "http://arxiv.org/pdf/2501.19137v1",
    "published_date": "2025-01-31 13:46:42 UTC",
    "updated_date": "2025-01-31 13:46:42 UTC"
  },
  {
    "arxiv_id": "2501.19133v1",
    "title": "Decorrelated Soft Actor-Critic for Efficient Deep Reinforcement Learning",
    "authors": [
      "Burcu Küçükoğlu",
      "Sander Dalm",
      "Marcel van Gerven"
    ],
    "abstract": "The effectiveness of credit assignment in reinforcement learning (RL) when\ndealing with high-dimensional data is influenced by the success of\nrepresentation learning via deep neural networks, and has implications for the\nsample efficiency of deep RL algorithms. Input decorrelation has been\npreviously introduced as a method to speed up optimization in neural networks,\nand has proven impactful in both efficient deep learning and as a method for\neffective representation learning for deep RL algorithms. We propose a novel\napproach to online decorrelation in deep RL based on the decorrelated\nbackpropagation algorithm that seamlessly integrates the decorrelation process\ninto the RL training pipeline. Decorrelation matrices are added to each layer,\nwhich are updated using a separate decorrelation learning rule that minimizes\nthe total decorrelation loss across all layers, in parallel to minimizing the\nusual RL loss. We used our approach in combination with the soft actor-critic\n(SAC) method, which we refer to as decorrelated soft actor-critic (DSAC).\nExperiments on the Atari 100k benchmark with DSAC shows, compared to the\nregular SAC baseline, faster training in five out of the seven games tested and\nimproved reward performance in two games with around 50% reduction in\nwall-clock time, while maintaining performance levels on the other games. These\nresults demonstrate the positive impact of network-wide decorrelation in deep\nRL for speeding up its sample efficiency through more effective credit\nassignment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19133v1",
    "published_date": "2025-01-31 13:38:57 UTC",
    "updated_date": "2025-01-31 13:38:57 UTC"
  },
  {
    "arxiv_id": "2501.19128v1",
    "title": "Shaping Sparse Rewards in Reinforcement Learning: A Semi-supervised Approach",
    "authors": [
      "Wenyun Li",
      "Wenjie Huang"
    ],
    "abstract": "In many real-world scenarios, reward signal for agents are exceedingly\nsparse, making it challenging to learn an effective reward function for reward\nshaping. To address this issue, our approach performs reward shaping not only\nby utilizing non-zero-reward transitions but also by employing the\nSemi-Supervised Learning (SSL) technique combined with a novel data\naugmentation to learn trajectory space representations from the majority of\ntransitions, zero-reward transitions, thereby improving the efficacy of reward\nshaping. Experimental results in Atari and robotic manipulation demonstrate\nthat our method effectively generalizes reward shaping to sparse reward\nscenarios, achieving up to four times better performance in reaching higher\nbest scores compared to curiosity-driven methods. The proposed double entropy\ndata augmentation enhances performance, showcasing a 15.8\\% increase in best\nscore over other augmentation methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19128v1",
    "published_date": "2025-01-31 13:35:19 UTC",
    "updated_date": "2025-01-31 13:35:19 UTC"
  },
  {
    "arxiv_id": "2501.19122v2",
    "title": "FedRTS: Federated Robust Pruning via Combinatorial Thompson Sampling",
    "authors": [
      "Hong Huang",
      "Hai Yang",
      "Yuan Chen",
      "Jiaxun Ye",
      "Dapeng Wu"
    ],
    "abstract": "Federated Learning (FL) enables collaborative model training across\ndistributed clients without data sharing, but its high computational and\ncommunication demands strain resource-constrained devices. While existing\nmethods use dynamic pruning to improve efficiency by periodically adjusting\nsparse model topologies while maintaining sparsity, these approaches suffer\nfrom issues such as greedy adjustments, unstable topologies, and communication\ninefficiency, resulting in less robust models and suboptimal performance under\ndata heterogeneity and partial client availability. To address these\nchallenges, we propose Federated Robust pruning via combinatorial Thompson\nSampling (FedRTS), a novel framework designed to develop robust sparse models.\nFedRTS enhances robustness and performance through its Thompson Sampling-based\nAdjustment (TSAdj) mechanism, which uses probabilistic decisions informed by\nstable, farsighted information instead of deterministic decisions reliant on\nunstable and myopic information in previous methods. Extensive experiments\ndemonstrate that FedRTS achieves state-of-the-art performance in computer\nvision and natural language processing tasks while reducing communication\ncosts, particularly excelling in scenarios with heterogeneous data\ndistributions and partial client participation. Our codes are available at:\nhttps://github.com/Little0o0/FedRTS",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19122v2",
    "published_date": "2025-01-31 13:26:22 UTC",
    "updated_date": "2025-05-17 16:45:33 UTC"
  },
  {
    "arxiv_id": "2501.19114v1",
    "title": "Principal Components for Neural Network Initialization",
    "authors": [
      "Nhan Phan",
      "Thu Nguyen",
      "Pål Halvorsen",
      "Michael A. Riegler"
    ],
    "abstract": "Principal Component Analysis (PCA) is a commonly used tool for dimension\nreduction and denoising. Therefore, it is also widely used on the data prior to\ntraining a neural network. However, this approach can complicate the\nexplanation of explainable AI (XAI) methods for the decision of the model. In\nthis work, we analyze the potential issues with this approach and propose\nPrincipal Components-based Initialization (PCsInit), a strategy to incorporate\nPCA into the first layer of a neural network via initialization of the first\nlayer in the network with the principal components, and its two variants\nPCsInit-Act and PCsInit-Sub. Explanations using these strategies are as direct\nand straightforward as for neural networks and are simpler than using PCA prior\nto training a neural network on the principal components. Moreover, as will be\nillustrated in the experiments, such training strategies can also allow further\nimprovement of training via backpropagation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19114v1",
    "published_date": "2025-01-31 13:18:10 UTC",
    "updated_date": "2025-01-31 13:18:10 UTC"
  },
  {
    "arxiv_id": "2501.19112v2",
    "title": "Logical Modalities within the European AI Act: An Analysis",
    "authors": [
      "Lara Lawniczak",
      "Christoph Benzmüller"
    ],
    "abstract": "The paper presents a comprehensive analysis of the European AI Act in terms\nof its logical modalities, with the aim of preparing its formal representation,\nfor example, within the logic-pluralistic Knowledge Engineering Framework and\nMethodology (LogiKEy). LogiKEy develops computational tools for normative\nreasoning based on formal methods, employing Higher-Order Logic (HOL) as a\nunifying meta-logic to integrate diverse logics through shallow semantic\nembeddings. This integration is facilitated by Isabelle/HOL, a proof assistant\ntool equipped with several automated theorem provers. The modalities within the\nAI Act and the logics suitable for their representation are discussed. For a\nselection of these logics, embeddings in HOL are created, which are then used\nto encode sample paragraphs. Initial experiments evaluate the suitability of\nthese embeddings for automated reasoning, and highlight key challenges on the\nway to more robust reasoning capabilities.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LO",
      "68T01 68T27 68T30 03Axx 03B16 03B35 03B45 03B60 03B70",
      "I.2.0; I.2.3; I.2.4; J.1"
    ],
    "primary_category": "cs.AI",
    "comment": "Extended preprint of paper accepted for ICAIL 2025; 15 pages, 19\n  figures",
    "pdf_url": "http://arxiv.org/pdf/2501.19112v2",
    "published_date": "2025-01-31 13:15:33 UTC",
    "updated_date": "2025-05-12 13:07:46 UTC"
  },
  {
    "arxiv_id": "2501.19111v2",
    "title": "A Benchmark for Incremental Micro-expression Recognition",
    "authors": [
      "Zhengqin Lai",
      "Xiaopeng Hong",
      "Yabin Wang",
      "Xiaobai Li"
    ],
    "abstract": "Micro-expression recognition plays a pivotal role in understanding hidden\nemotions and has applications across various fields. Traditional recognition\nmethods assume access to all training data at once, but real-world scenarios\ninvolve continuously evolving data streams. To respond to the requirement of\nadapting to new data while retaining previously learned knowledge, we introduce\nthe first benchmark specifically designed for incremental micro-expression\nrecognition. Our contributions include: Firstly, we formulate the incremental\nlearning setting tailored for micro-expression recognition. Secondly, we\norganize sequential datasets with carefully curated learning orders to reflect\nreal-world scenarios. Thirdly, we define two cross-evaluation-based testing\nprotocols, each targeting distinct evaluation objectives. Finally, we provide\nsix baseline methods and their corresponding evaluation results. This benchmark\nlays the groundwork for advancing incremental micro-expression recognition\nresearch. All source code used in this study will be publicly available at\nhttps://github.com/ZhengQinLai/IMER-benchmark.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19111v2",
    "published_date": "2025-01-31 13:14:16 UTC",
    "updated_date": "2025-02-03 08:14:52 UTC"
  },
  {
    "arxiv_id": "2501.19095v1",
    "title": "PathE: Leveraging Entity-Agnostic Paths for Parameter-Efficient Knowledge Graph Embeddings",
    "authors": [
      "Ioannis Reklos",
      "Jacopo de Berardinis",
      "Elena Simperl",
      "Albert Meroño-Peñuela"
    ],
    "abstract": "Knowledge Graphs (KGs) store human knowledge in the form of entities (nodes)\nand relations, and are used extensively in various applications. KG embeddings\nare an effective approach to addressing tasks like knowledge discovery, link\nprediction, and reasoning. This is often done by allocating and learning\nembedding tables for all or a subset of the entities. As this scales linearly\nwith the number of entities, learning embedding models in real-world KGs with\nmillions of nodes can be computationally intractable. To address this\nscalability problem, our model, PathE, only allocates embedding tables for\nrelations (which are typically orders of magnitude fewer than the entities) and\nrequires less than 25% of the parameters of previous parameter efficient\nmethods. Rather than storing entity embeddings, we learn to compute them by\nleveraging multiple entity-relation paths to contextualise individual entities\nwithin triples. Evaluated on four benchmarks, PathE achieves state-of-the-art\nperformance in relation prediction, and remains competitive in link prediction\non path-rich KGs while training on consumer-grade hardware. We perform ablation\nexperiments to test our design choices and analyse the sensitivity of the model\nto key hyper-parameters. PathE is efficient and cost-effective for relationally\ndiverse and well-connected KGs commonly found in real-world applications.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19095v1",
    "published_date": "2025-01-31 12:41:02 UTC",
    "updated_date": "2025-01-31 12:41:02 UTC"
  },
  {
    "arxiv_id": "2503.15515v1",
    "title": "Towards Computer-Using Personal Agents",
    "authors": [
      "Piero A. Bonatti",
      "John Domingue",
      "Anna Lisa Gentile",
      "Andreas Harth",
      "Olaf Hartig",
      "Aidan Hogan",
      "Katja Hose",
      "Ernesto Jimenez-Ruiz",
      "Deborah L. McGuinness",
      "Chang Sun",
      "Ruben Verborgh",
      "Jesse Wright"
    ],
    "abstract": "Computer-Using Agents (CUA) enable users to automate increasingly-complex\ntasks using graphical interfaces such as browsers. As many potential tasks\nrequire personal data, we propose Computer-Using Personal Agents (CUPAs) that\nhave access to an external repository of the user's personal data. Compared\nwith CUAs, CUPAs offer users better control of their personal data, the\npotential to automate more tasks involving personal data, better\ninteroperability with external sources of data, and better capabilities to\ncoordinate with other CUPAs in order to solve collaborative tasks involving the\npersonal data of multiple users.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.MA",
      "I.2.7; I.2.4; I.2.11; H.3.5"
    ],
    "primary_category": "cs.HC",
    "comment": "This report is a result of Dagstuhl Seminar 25051 \"Trust and\n  Accountability in Knowledge Graph-Based AI for Self Determination\", which\n  took place in January 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.15515v1",
    "published_date": "2025-01-31 12:26:27 UTC",
    "updated_date": "2025-01-31 12:26:27 UTC"
  },
  {
    "arxiv_id": "2501.19086v1",
    "title": "Fairness Analysis of CLIP-Based Foundation Models for X-Ray Image Classification",
    "authors": [
      "Xiangyu Sun",
      "Xiaoguang Zou",
      "Yuanquan Wu",
      "Guotai Wang",
      "Shaoting Zhang"
    ],
    "abstract": "X-ray imaging is pivotal in medical diagnostics, offering non-invasive\ninsights into a range of health conditions. Recently, vision-language models,\nsuch as the Contrastive Language-Image Pretraining (CLIP) model, have\ndemonstrated potential in improving diagnostic accuracy by leveraging\nlarge-scale image-text datasets. However, since CLIP was not initially designed\nfor medical images, several CLIP-like models trained specifically on medical\nimages have been developed. Despite their enhanced performance, issues of\nfairness - particularly regarding demographic attributes - remain largely\nunaddressed. In this study, we perform a comprehensive fairness analysis of\nCLIP-like models applied to X-ray image classification. We assess their\nperformance and fairness across diverse patient demographics and disease\ncategories using zero-shot inference and various fine-tuning techniques,\nincluding Linear Probing, Multilayer Perceptron (MLP), Low-Rank Adaptation\n(LoRA), and full fine-tuning. Our results indicate that while fine-tuning\nimproves model accuracy, fairness concerns persist, highlighting the need for\nfurther fairness interventions in these foundational models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper has been accepted for presentation at the 2025 IEEE\n  International Symposium on Biomedical Imaging (ISBI 2025)",
    "pdf_url": "http://arxiv.org/pdf/2501.19086v1",
    "published_date": "2025-01-31 12:23:50 UTC",
    "updated_date": "2025-01-31 12:23:50 UTC"
  },
  {
    "arxiv_id": "2501.19069v2",
    "title": "Improving vision-language alignment with graph spiking hybrid Networks",
    "authors": [
      "Siyu Zhang",
      "Wenzhe Liu",
      "Yeming Chen",
      "Yiming Wu",
      "Heming Zheng",
      "Cheng Cheng"
    ],
    "abstract": "To bridge the semantic gap between vision and language (VL), it is necessary\nto develop a good alignment strategy, which includes handling semantic\ndiversity, abstract representation of visual information, and generalization\nability of models. Recent works use detector-based bounding boxes or patches\nwith regular partitions to represent visual semantics. While current paradigms\nhave made strides, they are still insufficient for fully capturing the nuanced\ncontextual relations among various objects. This paper proposes a comprehensive\nvisual semantic representation module, necessitating the utilization of\npanoptic segmentation to generate coherent fine-grained semantic features.\nFurthermore, we propose a novel Graph Spiking Hybrid Network (GSHN) that\nintegrates the complementary advantages of Spiking Neural Networks (SNNs) and\nGraph Attention Networks (GATs) to encode visual semantic information.\nIntriguingly, the model not only encodes the discrete and continuous latent\nvariables of instances but also adeptly captures both local and global\ncontextual features, thereby significantly enhancing the richness and diversity\nof semantic representations. Leveraging the spatiotemporal properties inherent\nin SNNs, we employ contrastive learning (CL) to enhance the similarity-based\nrepresentation of embeddings. This strategy alleviates the computational\noverhead of the model and enriches meaningful visual representations by\nconstructing positive and negative sample pairs. We design an innovative\npre-training method, Spiked Text Learning (STL), which uses text features to\nimprove the encoding ability of discrete semantics. Experiments show that the\nproposed GSHN exhibits promising results on multiple VL downstream tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19069v2",
    "published_date": "2025-01-31 11:55:17 UTC",
    "updated_date": "2025-03-02 07:22:57 UTC"
  },
  {
    "arxiv_id": "2501.19065v1",
    "title": "BEAT: Balanced Frequency Adaptive Tuning for Long-Term Time-Series Forecasting",
    "authors": [
      "Zhixuan Li",
      "Naipeng Chen",
      "Seonghwa Choi",
      "Sanghoon Lee",
      "Weisi Lin"
    ],
    "abstract": "Time-series forecasting is crucial for numerous real-world applications\nincluding weather prediction and financial market modeling. While\ntemporal-domain methods remain prevalent, frequency-domain approaches can\neffectively capture multi-scale periodic patterns, reduce sequence\ndependencies, and naturally denoise signals. However, existing approaches\ntypically train model components for all frequencies under a unified training\nobjective, often leading to mismatched learning speeds: high-frequency\ncomponents converge faster and risk overfitting, while low-frequency components\nunderfit due to insufficient training time. To deal with this challenge, we\npropose BEAT (Balanced frEquency Adaptive Tuning), a novel framework that\ndynamically monitors the training status for each frequency and adaptively\nadjusts their gradient updates. By recognizing convergence, overfitting, or\nunderfitting for each frequency, BEAT dynamically reallocates learning\npriorities, moderating gradients for rapid learners and increasing those for\nslower ones, alleviating the tension between competing objectives across\nfrequencies and synchronizing the overall learning process. Extensive\nexperiments on seven real-world datasets demonstrate that BEAT consistently\noutperforms state-of-the-art approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.19065v1",
    "published_date": "2025-01-31 11:52:35 UTC",
    "updated_date": "2025-01-31 11:52:35 UTC"
  },
  {
    "arxiv_id": "2501.19056v1",
    "title": "Enabling Autonomic Microservice Management through Self-Learning Agents",
    "authors": [
      "Fenglin Yu",
      "Fangkai Yang",
      "Xiaoting Qin",
      "Zhiyang Zhang",
      "Jue Zhang",
      "Qingwei Lin",
      "Hongyu Zhang",
      "Yingnong Dang",
      "Saravan Rajmohan",
      "Dongmei Zhang",
      "Qi Zhang"
    ],
    "abstract": "The increasing complexity of modern software systems necessitates robust\nautonomic self-management capabilities. While Large Language Models (LLMs)\ndemonstrate potential in this domain, they often face challenges in adapting\ntheir general knowledge to specific service contexts. To address this\nlimitation, we propose ServiceOdyssey, a self-learning agent system that\nautonomously manages microservices without requiring prior knowledge of\nservice-specific configurations. By leveraging curriculum learning principles\nand iterative exploration, ServiceOdyssey progressively develops a deep\nunderstanding of operational environments, reducing dependence on human input\nor static documentation. A prototype built with the Sock Shop microservice\ndemonstrates the potential of this approach for autonomic microservice\nmanagement.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19056v1",
    "published_date": "2025-01-31 11:32:05 UTC",
    "updated_date": "2025-01-31 11:32:05 UTC"
  },
  {
    "arxiv_id": "2501.19055v1",
    "title": "Towards Physiologically Sensible Predictions via the Rule-based Reinforcement Learning Layer",
    "authors": [
      "Lingwei Zhu",
      "Zheng Chen",
      "Yukie Nagai",
      "Jimeng Sun"
    ],
    "abstract": "This paper adds to the growing literature of reinforcement learning (RL) for\nhealthcare by proposing a novel paradigm: augmenting any predictor with\nRule-based RL Layer (RRLL) that corrects the model's physiologically impossible\npredictions. Specifically, RRLL takes as input states predicted labels and\noutputs corrected labels as actions. The reward of the state-action pair is\nevaluated by a set of general rules. RRLL is efficient, general and\nlightweight: it does not require heavy expert knowledge like prior work but\nonly a set of impossible transitions. This set is much smaller than all\npossible transitions; yet it can effectively reduce physiologically impossible\nmistakes made by the state-of-the-art predictor models. We verify the utility\nof RRLL on a variety of important healthcare classification problems and\nobserve significant improvements using the same setup, with only the\ndomain-specific set of impossibility changed. In-depth analysis shows that RRLL\nindeed improves accuracy by effectively reducing the presence of\nphysiologically impossible predictions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19055v1",
    "published_date": "2025-01-31 11:29:26 UTC",
    "updated_date": "2025-01-31 11:29:26 UTC"
  },
  {
    "arxiv_id": "2501.19047v4",
    "title": "Understanding Model Calibration -- A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)",
    "authors": [
      "Maja Pavlovic"
    ],
    "abstract": "To be considered reliable, a model must be calibrated so that its confidence\nin each decision closely reflects its true outcome. In this blogpost we'll take\na look at the most commonly used definition for calibration and then dive into\na frequently used evaluation measure for model calibration. We'll then cover\nsome of the drawbacks of this measure and how these surfaced the need for\nadditional notions of calibration, which require their own new evaluation\nmeasures. This post is not intended to be an in-depth dissection of all works\non calibration, nor does it focus on how to calibrate models. Instead, it is\nmeant to provide a gentle introduction to the different notions and their\nevaluation measures as well as to re-highlight some issues with a measure that\nis still widely used to evaluate calibration.",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "comment": "https://openreview.net/forum?id=BxBeCjQd2y",
    "pdf_url": "http://arxiv.org/pdf/2501.19047v4",
    "published_date": "2025-01-31 11:18:45 UTC",
    "updated_date": "2025-05-11 14:27:57 UTC"
  },
  {
    "arxiv_id": "2501.19042v1",
    "title": "Swarm-Gen: Fast Generation of Diverse Feasible Swarm Behaviors",
    "authors": [
      "Simon Idoko",
      "B. Bhanu Teja",
      "K. Madhava Krishna",
      "Arun Kumar Singh"
    ],
    "abstract": "Coordination behavior in robot swarms is inherently multi-modal in nature.\nThat is, there are numerous ways in which a swarm of robots can avoid\ninter-agent collisions and reach their respective goals. However, the problem\nof generating diverse and feasible swarm behaviors in a scalable manner remains\nlargely unaddressed. In this paper, we fill this gap by combining generative\nmodels with a safety-filter (SF). Specifically, we sample diverse trajectories\nfrom a learned generative model which is subsequently projected onto the\nfeasible set using the SF. We experiment with two choices for generative\nmodels, namely: Conditional Variational Autoencoder (CVAE) and Vector-Quantized\nVariational Autoencoder (VQ-VAE). We highlight the trade-offs these two models\nprovide in terms of computation time and trajectory diversity. We develop a\ncustom solver for our SF and equip it with a neural network that predicts\ncontext-specific initialization. Thecinitialization network is trained in a\nself-supervised manner, taking advantage of the differentiability of the SF\nsolver. We provide two sets of empirical results. First, we demonstrate that we\ncan generate a large set of multi-modal, feasible trajectories, simulating\ndiverse swarm behaviors, within a few tens of milliseconds. Second, we show\nthat our initialization network provides faster convergence of our SF solver\nvis-a-vis other alternative heuristics.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Submitted to RAL",
    "pdf_url": "http://arxiv.org/pdf/2501.19042v1",
    "published_date": "2025-01-31 11:13:09 UTC",
    "updated_date": "2025-01-31 11:13:09 UTC"
  },
  {
    "arxiv_id": "2501.19003v1",
    "title": "Virtual airways heatmaps to optimize point of entry location in lung biopsy planning systems",
    "authors": [
      "Debora Gil",
      "Pere Lloret",
      "Marta Diez-Ferrer",
      "Carles Sanchez"
    ],
    "abstract": "Purpose: We present a virtual model to optimize point of entry (POE) in lung\nbiopsy planning systems. Our model allows to compute the quality of a biopsy\nsample taken from potential POE, taking into account the margin of error that\narises from discrepancies between the orientation in the planning simulation\nand the actual orientation during the operation. Additionally, the study\nexamines the impact of the characteristics of the lesion. Methods: The quality\nof the biopsy is given by a heatmap projected onto the skeleton of a\npatient-specific model of airways. The skeleton provides a 3D representation of\nairways structure, while the heatmap intensity represents the potential amount\nof tissue that it could be extracted from each POE. This amount of tissue is\ndetermined by the intersection of the lesion with a cone that represents the\nuncertainty area in the introduction of biopsy instruments. The cone, lesion,\nand skeleton are modelled as graphical objects that define a 3D scene of the\nintervention. Results: We have simulated different settings of the intervention\nscene from a single anatomy extracted from a CT scan and two lesions with\nregular and irregular shapes. The different scenarios are simulated by\nsystematic rotation of each lesion placed at different distances from airways.\nAnalysis of the heatmaps for the different settings show a strong impact of\nlesion orientation for irregular shape and the distance for both shapes.\nConclusion: The proposed heatmaps help to visually assess the optimal POE and\nidentify whether multiple optimal POEs exist in different zones of the bronchi.\nThey also allow us to model the maximum allowable error in navigation systems\nand study which variables have the greatest influence on the success of the\noperation. Additionally, they help determine at what point this influence could\npotentially jeopardize the operation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.19003v1",
    "published_date": "2025-01-31 10:14:47 UTC",
    "updated_date": "2025-01-31 10:14:47 UTC"
  },
  {
    "arxiv_id": "2501.18998v2",
    "title": "Adversarial Attacks on AI-Generated Text Detection Models: A Token Probability-Based Approach Using Embeddings",
    "authors": [
      "Ahmed K. Kadhim",
      "Lei Jiao",
      "Rishad Shafik",
      "Ole-Christoffer Granmo"
    ],
    "abstract": "In recent years, text generation tools utilizing Artificial Intelligence (AI)\nhave occasionally been misused across various domains, such as generating\nstudent reports or creative writings. This issue prompts plagiarism detection\nservices to enhance their capabilities in identifying AI-generated content.\nAdversarial attacks are often used to test the robustness of AI-text generated\ndetectors. This work proposes a novel textual adversarial attack on the\ndetection models such as Fast-DetectGPT. The method employs embedding models\nfor data perturbation, aiming at reconstructing the AI generated texts to\nreduce the likelihood of detection of the true origin of the texts.\nSpecifically, we employ different embedding techniques, including the Tsetlin\nMachine (TM), an interpretable approach in machine learning for this purpose.\nBy combining synonyms and embedding similarity vectors, we demonstrates the\nstate-of-the-art reduction in detection scores against Fast-DetectGPT.\nParticularly, in the XSum dataset, the detection score decreased from 0.4431 to\n0.2744 AUROC, and in the SQuAD dataset, it dropped from 0.5068 to 0.3532 AUROC.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.18998v2",
    "published_date": "2025-01-31 10:06:27 UTC",
    "updated_date": "2025-04-10 18:46:55 UTC"
  },
  {
    "arxiv_id": "2502.00076v1",
    "title": "Influence of color correction on pathology detection in Capsule Endoscopy",
    "authors": [
      "Bidossessi Emmanuel Agossou",
      "Marius Pedersen",
      "Kiran Raja",
      "Anuja Vats",
      "Pål Anders Floor"
    ],
    "abstract": "Pathology detection in Wireless Capsule Endoscopy (WCE) using deep learning\nhas been explored in the recent past. However, deep learning models can be\ninfluenced by the color quality of the dataset used to train them, impacting\ndetection, segmentation and classification tasks. In this work, we evaluate the\nimpact of color correction on pathology detection using two prominent object\ndetection models: Retinanet and YOLOv5. We first generate two color corrected\nversions of a popular WCE dataset (i.e., SEE-AI dataset) using two different\ncolor correction functions. We then evaluate the performance of the Retinanet\nand YOLOv5 on the original and color corrected versions of the dataset. The\nresults reveal that color correction makes the models generate larger bounding\nboxes and larger intersection areas with the ground truth annotations.\nFurthermore, color correction leads to an increased number of false positives\nfor certain pathologies. However, these effects do not translate into a\nconsistent improvement in performance metrics such as F1-scores, IoU, and AP50.\nThe code is available at https://github.com/agossouema2011/WCE2024. Keywords:\nWireless Capsule Endoscopy, Color correction, Retinanet, YOLOv5, Detection",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00076v1",
    "published_date": "2025-01-31 10:05:44 UTC",
    "updated_date": "2025-01-31 10:05:44 UTC"
  },
  {
    "arxiv_id": "2501.18994v1",
    "title": "VKFPos: A Learning-Based Monocular Positioning with Variational Bayesian Extended Kalman Filter Integration",
    "authors": [
      "Jian-Yu Chen",
      "Yi-Ru Chen",
      "Yin-Qiao Chang",
      "Che-Ming Li",
      "Jann-Long Chern",
      "Chih-Wei Huang"
    ],
    "abstract": "This paper addresses the challenges in learning-based monocular positioning\nby proposing VKFPos, a novel approach that integrates Absolute Pose Regression\n(APR) and Relative Pose Regression (RPR) via an Extended Kalman Filter (EKF)\nwithin a variational Bayesian inference framework. Our method shows that the\nessential posterior probability of the monocular positioning problem can be\ndecomposed into APR and RPR components. This decomposition is embedded in the\ndeep learning model by predicting covariances in both APR and RPR branches,\nallowing them to account for associated uncertainties. These covariances\nenhance the loss functions and facilitate EKF integration. Experimental\nevaluations on both indoor and outdoor datasets show that the single-shot APR\nbranch achieves accuracy on par with state-of-the-art methods. Furthermore, for\ntemporal positioning, where consecutive images allow for RPR and EKF\nintegration, VKFPos outperforms temporal APR and model-based integration\nmethods, achieving superior accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.18994v1",
    "published_date": "2025-01-31 09:54:11 UTC",
    "updated_date": "2025-01-31 09:54:11 UTC"
  },
  {
    "arxiv_id": "2501.18980v1",
    "title": "Symmetric Pruning of Large Language Models",
    "authors": [
      "Kai Yi",
      "Peter Richtárik"
    ],
    "abstract": "Popular post-training pruning methods such as Wanda and RIA are known for\ntheir simple, yet effective, designs that have shown exceptional empirical\nperformance. Wanda optimizes performance through calibrated activations during\npruning, while RIA emphasizes the relative, rather than absolute, importance of\nweight elements. Despite their practical success, a thorough theoretical\nfoundation explaining these outcomes has been lacking. This paper introduces\nnew theoretical insights that redefine the standard minimization objective for\npruning, offering a deeper understanding of the factors contributing to their\nsuccess. Our study extends beyond these insights by proposing complementary\nstrategies that consider both input activations and weight significance. We\nvalidate these approaches through rigorous experiments, demonstrating\nsubstantial enhancements over existing methods. Furthermore, we introduce a\nnovel training-free fine-tuning approach $R^2$-DSnoT that incorporates relative\nweight importance and a regularized decision boundary within a dynamic\npruning-and-growing framework, significantly outperforming strong baselines and\nestablishing a new state of the art.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.18980v1",
    "published_date": "2025-01-31 09:23:06 UTC",
    "updated_date": "2025-01-31 09:23:06 UTC"
  },
  {
    "arxiv_id": "2501.18973v1",
    "title": "GPO-VAE: Modeling Explainable Gene Perturbation Responses utilizing GRN-Aligned Parameter Optimization",
    "authors": [
      "Seungheun Baek",
      "Soyon Park",
      "Yan Ting Chok",
      "Mogan Gim",
      "Jaewoo Kang"
    ],
    "abstract": "Motivation: Predicting cellular responses to genetic perturbations is\nessential for understanding biological systems and developing targeted\ntherapeutic strategies. While variational autoencoders (VAEs) have shown\npromise in modeling perturbation responses, their limited explainability poses\na significant challenge, as the learned features often lack clear biological\nmeaning. Nevertheless, model explainability is one of the most important\naspects in the realm of biological AI. One of the most effective ways to\nachieve explainability is incorporating the concept of gene regulatory networks\n(GRNs) in designing deep learning models such as VAEs. GRNs elicit the\nunderlying causal relationships between genes and are capable of explaining the\ntranscriptional responses caused by genetic perturbation treatments. Results:\nWe propose GPO-VAE, an explainable VAE enhanced by GRN-aligned Parameter\nOptimization that explicitly models gene regulatory networks in the latent\nspace. Our key approach is to optimize the learnable parameters related to\nlatent perturbation effects towards GRN-aligned explainability. Experimental\nresults on perturbation prediction show our model achieves state-of-the-art\nperformance in predicting transcriptional responses across multiple benchmark\ndatasets. Furthermore, additional results on evaluating the GRN inference task\nreveal our model's ability to generate meaningful GRNs compared to other\nmethods. According to qualitative analysis, GPO-VAE posseses the ability to\nconstruct biologically explainable GRNs that align with experimentally\nvalidated regulatory pathways. GPO-VAE is available at\nhttps://github.com/dmis-lab/GPO-VAE",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.18973v1",
    "published_date": "2025-01-31 09:08:52 UTC",
    "updated_date": "2025-01-31 09:08:52 UTC"
  },
  {
    "arxiv_id": "2501.18959v2",
    "title": "Enhancing Neural Function Approximation: The XNet Outperforming KAN",
    "authors": [
      "Xin Li",
      "Xiaotao Zheng",
      "Zhihong Xia"
    ],
    "abstract": "XNet is a single-layer neural network architecture that leverages Cauchy\nintegral-based activation functions for high-order function approximation.\nThrough theoretical analysis, we show that the Cauchy activation functions used\nin XNet can achieve arbitrary-order polynomial convergence, fundamentally\noutperforming traditional MLPs and Kolmogorov-Arnold Networks (KANs) that rely\non increased depth or B-spline activations. Our extensive experiments on\nfunction approximation, PDE solving, and reinforcement learning demonstrate\nXNet's superior performance - reducing approximation error by up to 50000 times\nand accelerating training by up to 10 times compared to existing approaches.\nThese results establish XNet as a highly efficient architecture for both\nscientific computing and AI applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2410.02033",
    "pdf_url": "http://arxiv.org/pdf/2501.18959v2",
    "published_date": "2025-01-31 08:33:10 UTC",
    "updated_date": "2025-02-14 02:50:45 UTC"
  },
  {
    "arxiv_id": "2501.18955v1",
    "title": "Deep Learning based Quasi-consciousness Training for Robot Intelligent Model",
    "authors": [
      "Yuchun Li",
      "Fang Zhang"
    ],
    "abstract": "This paper explores a deep learning based robot intelligent model that\nrenders robots learn and reason for complex tasks. First, by constructing a\nnetwork of environmental factor matrix to stimulate the learning process of the\nrobot intelligent model, the model parameters must be subjected to coarse &\nfine tuning to optimize the loss function for minimizing the loss score,\nmeanwhile robot intelligent model can fuse all previously known concepts\ntogether to represent things never experienced before, which need robot\nintelligent model can be generalized extensively. Secondly, in order to\nprogressively develop a robot intelligent model with primary consciousness,\nevery robot must be subjected to at least 1~3 years of special school for\ntraining anthropomorphic behaviour patterns to understand and process complex\nenvironmental information and make rational decisions. This work explores and\ndelivers the potential application of deep learning-based quasi-consciousness\ntraining in the field of robot intelligent model.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.18955v1",
    "published_date": "2025-01-31 08:27:32 UTC",
    "updated_date": "2025-01-31 08:27:32 UTC"
  },
  {
    "arxiv_id": "2501.18950v2",
    "title": "Fantastic Targets for Concept Erasure in Diffusion Models and Where To Find Them",
    "authors": [
      "Anh Bui",
      "Trang Vu",
      "Long Vuong",
      "Trung Le",
      "Paul Montague",
      "Tamas Abraham",
      "Junae Kim",
      "Dinh Phung"
    ],
    "abstract": "Concept erasure has emerged as a promising technique for mitigating the risk\nof harmful content generation in diffusion models by selectively unlearning\nundesirable concepts. The common principle of previous works to remove a\nspecific concept is to map it to a fixed generic concept, such as a neutral\nconcept or just an empty text prompt. In this paper, we demonstrate that this\nfixed-target strategy is suboptimal, as it fails to account for the impact of\nerasing one concept on the others. To address this limitation, we model the\nconcept space as a graph and empirically analyze the effects of erasing one\nconcept on the remaining concepts. Our analysis uncovers intriguing geometric\nproperties of the concept space, where the influence of erasing a concept is\nconfined to a local region. Building on this insight, we propose the Adaptive\nGuided Erasure (AGE) method, which \\emph{dynamically} selects optimal target\nconcepts tailored to each undesirable concept, minimizing unintended side\neffects. Experimental results show that AGE significantly outperforms\nstate-of-the-art erasure methods on preserving unrelated concepts while\nmaintaining effective erasure performance. Our code is published at\n{https://github.com/tuananhbui89/Adaptive-Guided-Erasure}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.18950v2",
    "published_date": "2025-01-31 08:17:23 UTC",
    "updated_date": "2025-02-27 23:36:38 UTC"
  },
  {
    "arxiv_id": "2502.00074v1",
    "title": "SpikingRTNH: Spiking Neural Network for 4D Radar Object Detection",
    "authors": [
      "Dong-Hee Paek",
      "Seung-Hyun Kong"
    ],
    "abstract": "Recently, 4D Radar has emerged as a crucial sensor for 3D object detection in\nautonomous vehicles, offering both stable perception in adverse weather and\nhigh-density point clouds for object shape recognition. However, processing\nsuch high-density data demands substantial computational resources and energy\nconsumption. We propose SpikingRTNH, the first spiking neural network (SNN) for\n3D object detection using 4D Radar data. By replacing conventional ReLU\nactivation functions with leaky integrate-and-fire (LIF) spiking neurons,\nSpikingRTNH achieves significant energy efficiency gains. Furthermore, inspired\nby human cognitive processes, we introduce biological top-down inference (BTI),\nwhich processes point clouds sequentially from higher to lower densities. This\napproach effectively utilizes points with lower noise and higher importance for\ndetection. Experiments on K-Radar dataset demonstrate that SpikingRTNH with BTI\nsignificantly reduces energy consumption by 78% while achieving comparable\ndetection performance to its ANN counterpart (51.1% AP 3D, 57.0% AP BEV). These\nresults establish the viability of SNNs for energy-efficient 4D Radar-based\nobject detection in autonomous driving systems. All codes are available at\nhttps://github.com/kaist-avelab/k-radar.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "arxiv preprint",
    "pdf_url": "http://arxiv.org/pdf/2502.00074v1",
    "published_date": "2025-01-31 07:33:30 UTC",
    "updated_date": "2025-01-31 07:33:30 UTC"
  },
  {
    "arxiv_id": "2501.18924v1",
    "title": "Language Games as the Pathway to Artificial Superhuman Intelligence",
    "authors": [
      "Ying Wen",
      "Ziyu Wan",
      "Shao Zhang"
    ],
    "abstract": "The evolution of large language models (LLMs) toward artificial superhuman\nintelligence (ASI) hinges on data reproduction, a cyclical process in which\nmodels generate, curate and retrain on novel data to refine capabilities.\nCurrent methods, however, risk getting stuck in a data reproduction trap:\noptimizing outputs within fixed human-generated distributions in a closed loop\nleads to stagnation, as models merely recombine existing knowledge rather than\nexplore new frontiers. In this paper, we propose language games as a pathway to\nexpanded data reproduction, breaking this cycle through three mechanisms: (1)\n\\textit{role fluidity}, which enhances data diversity and coverage by enabling\nmulti-agent systems to dynamically shift roles across tasks; (2) \\textit{reward\nvariety}, embedding multiple feedback criteria that can drive complex\nintelligent behaviors; and (3) \\textit{rule plasticity}, iteratively evolving\ninteraction constraints to foster learnability, thereby injecting continual\nnovelty. By scaling language games into global sociotechnical ecosystems,\nhuman-AI co-evolution generates unbounded data streams that drive open-ended\nexploration. This framework redefines data reproduction not as a closed loop\nbut as an engine for superhuman intelligence.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "This position paper argues that language games provide robust\n  mechanism for achieving superhuman intelligence in large language models",
    "pdf_url": "http://arxiv.org/pdf/2501.18924v1",
    "published_date": "2025-01-31 07:10:40 UTC",
    "updated_date": "2025-01-31 07:10:40 UTC"
  },
  {
    "arxiv_id": "2501.18922v1",
    "title": "KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search",
    "authors": [
      "Haoran Luo",
      "Haihong E",
      "Yikai Guo",
      "Qika Lin",
      "Xiaobao Wu",
      "Xinyu Mu",
      "Wenhao Liu",
      "Meina Song",
      "Yifan Zhu",
      "Luu Anh Tuan"
    ],
    "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions with a large-scale structured knowledge base (KB). Despite\nadvancements with large language models (LLMs), KBQA still faces challenges in\nweak KB awareness, imbalance between effectiveness and efficiency, and high\nreliance on annotated data. To address these challenges, we propose KBQA-o1, a\nnovel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a\nReAct-based agent process for stepwise logical form generation with KB\nenvironment exploration. Moreover, it employs MCTS, a heuristic search method\ndriven by policy and reward models, to balance agentic exploration's\nperformance and search space. With heuristic exploration, KBQA-o1 generates\nhigh-quality annotations for further improvement by incremental fine-tuning.\nExperimental results show that KBQA-o1 outperforms previous low-resource KBQA\nmethods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1\nperformance to 78.5% compared to 48.5% of the previous sota method with\nGPT-3.5-turbo.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2501.18922v1",
    "published_date": "2025-01-31 06:59:49 UTC",
    "updated_date": "2025-01-31 06:59:49 UTC"
  },
  {
    "arxiv_id": "2501.18919v1",
    "title": "Deepfake Detection of Singing Voices With Whisper Encodings",
    "authors": [
      "Falguni Sharma",
      "Priyanka Gupta"
    ],
    "abstract": "The deepfake generation of singing vocals is a concerning issue for artists\nin the music industry. In this work, we propose a singing voice deepfake\ndetection (SVDD) system, which uses noise-variant encodings of open-AI's\nWhisper model. As counter-intuitive as it may sound, even though the Whisper\nmodel is known to be noise-robust, the encodings are rich in non-speech\ninformation, and are noise-variant. This leads us to evaluate Whisper encodings\nas feature representations for the SVDD task. Therefore, in this work, the SVDD\ntask is performed on vocals and mixtures, and the performance is evaluated in\n\\%EER over varying Whisper model sizes and two classifiers- CNN and ResNet34,\nunder different testing conditions.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted in ICASSP,2025",
    "pdf_url": "http://arxiv.org/pdf/2501.18919v1",
    "published_date": "2025-01-31 06:43:50 UTC",
    "updated_date": "2025-01-31 06:43:50 UTC"
  },
  {
    "arxiv_id": "2502.18468v1",
    "title": "SOK: Exploring Hallucinations and Security Risks in AI-Assisted Software Development with Insights for LLM Deployment",
    "authors": [
      "Ariful Haque",
      "Sunzida Siddique",
      "Md. Mahfuzur Rahman",
      "Ahmed Rafi Hasan",
      "Laxmi Rani Das",
      "Marufa Kamal",
      "Tasnim Masura",
      "Kishor Datta Gupta"
    ],
    "abstract": "The integration of Large Language Models (LLMs) such as GitHub Copilot,\nChatGPT, Cursor AI, and Codeium AI into software development has revolutionized\nthe coding landscape, offering significant productivity gains, automation, and\nenhanced debugging capabilities. These tools have proven invaluable for\ngenerating code snippets, refactoring existing code, and providing real-time\nsupport to developers. However, their widespread adoption also presents notable\nchallenges, particularly in terms of security vulnerabilities, code quality,\nand ethical concerns. This paper provides a comprehensive analysis of the\nbenefits and risks associated with AI-powered coding tools, drawing on user\nfeedback, security analyses, and practical use cases. We explore the potential\nfor these tools to replicate insecure coding practices, introduce biases, and\ngenerate incorrect or non-sensical code (hallucinations). In addition, we\ndiscuss the risks of data leaks, intellectual property violations and the need\nfor robust security measures to mitigate these threats. By comparing the\nfeatures and performance of these tools, we aim to guide developers in making\ninformed decisions about their use, ensuring that the benefits of AI-assisted\ncoding are maximized while minimizing associated risks.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18468v1",
    "published_date": "2025-01-31 06:00:27 UTC",
    "updated_date": "2025-01-31 06:00:27 UTC"
  },
  {
    "arxiv_id": "2503.15514v2",
    "title": "Superhuman Game AI Disclosure: Expertise and Context Moderate Effects on Trust and Fairness",
    "authors": [
      "Jaymari Chua",
      "Chen Wang",
      "Lina Yao"
    ],
    "abstract": "As artificial intelligence surpasses human performance in select tasks,\ndisclosing superhuman capabilities poses distinct challenges for fairness,\naccountability, and trust. However, the impact of such disclosures on diverse\nuser attitudes and behaviors remains unclear, particularly concerning potential\nnegative reactions like discouragement or overreliance. This paper investigates\nthese effects by utilizing Persona Cards: a validated, standardized set of\nsynthetic personas designed to simulate diverse user reactions and fairness\nperspectives. We conducted an ethics board-approved study (N=32), utilizing\nthese personas to investigate how capability disclosure influenced behaviors\nwith a superhuman game AI in competitive StarCraft II scenarios. Our results\nreveal transparency is double-edged: while disclosure could alleviate\nsuspicion, it also provoked frustration and strategic defeatism among novices\nin cooperative scenarios, as well as overreliance in competitive contexts.\nExperienced and competitive players interpreted disclosure as confirmation of\nan unbeatable opponent, shifting to suboptimal goals. We release the Persona\nCards Dataset, including profiles, prompts, interaction logs, and protocols, to\nfoster reproducible research into human alignment AI design. This work\ndemonstrates that transparency is not a cure-all; successfully leveraging\ndisclosure to enhance trust and accountability requires careful tailoring to\nuser characteristics, domain norms, and specific fairness objectives.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.ET",
      "K.4.1; K.4.3; H.5.2; H.5.1; I.2.7"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15514v2",
    "published_date": "2025-01-31 05:50:50 UTC",
    "updated_date": "2025-04-07 17:39:10 UTC"
  },
  {
    "arxiv_id": "2501.18901v2",
    "title": "Lightspeed Geometric Dataset Distance via Sliced Optimal Transport",
    "authors": [
      "Khai Nguyen",
      "Hai Nguyen",
      "Tuan Pham",
      "Nhat Ho"
    ],
    "abstract": "We introduce sliced optimal transport dataset distance (s-OTDD), a\nmodel-agnostic, embedding-agnostic approach for dataset comparison that\nrequires no training, is robust to variations in the number of classes, and can\nhandle disjoint label sets. The core innovation is Moment Transform Projection\n(MTP), which maps a label, represented as a distribution over features, to a\nreal number. Using MTP, we derive a data point projection that transforms\ndatasets into one-dimensional distributions. The s-OTDD is defined as the\nexpected Wasserstein distance between the projected distributions, with respect\nto random projection parameters. Leveraging the closed form solution of\none-dimensional optimal transport, s-OTDD achieves (near-)linear computational\ncomplexity in the number of data points and feature dimensions and is\nindependent of the number of classes. With its geometrically meaningful\nprojection, s-OTDD strongly correlates with the optimal transport dataset\ndistance while being more efficient than existing dataset discrepancy measures.\nMoreover, it correlates well with the performance gap in transfer learning and\nclassification accuracy in data augmentation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.CO",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ICML 2025, 16 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.18901v2",
    "published_date": "2025-01-31 05:42:58 UTC",
    "updated_date": "2025-05-15 17:48:47 UTC"
  },
  {
    "arxiv_id": "2502.00072v1",
    "title": "LLM Cyber Evaluations Don't Capture Real-World Risk",
    "authors": [
      "Kamilė Lukošiūtė",
      "Adam Swanda"
    ],
    "abstract": "Large language models (LLMs) are demonstrating increasing prowess in\ncybersecurity applications, creating creating inherent risks alongside their\npotential for strengthening defenses. In this position paper, we argue that\ncurrent efforts to evaluate risks posed by these capabilities are misaligned\nwith the goal of understanding real-world impact. Evaluating LLM cybersecurity\nrisk requires more than just measuring model capabilities -- it demands a\ncomprehensive risk assessment that incorporates analysis of threat actor\nadoption behavior and potential for impact. We propose a risk assessment\nframework for LLM cyber capabilities and apply it to a case study of language\nmodels used as cybersecurity assistants. Our evaluation of frontier models\nreveals high compliance rates but moderate accuracy on realistic cyber\nassistance tasks. However, our framework suggests that this particular use case\npresents only moderate risk due to limited operational advantages and impact\npotential. Based on these findings, we recommend several improvements to align\nresearch priorities with real-world impact assessment, including closer\nacademia-industry collaboration, more realistic modeling of attacker behavior,\nand inclusion of economic metrics in evaluations. This work represents an\nimportant step toward more effective assessment and mitigation of LLM-enabled\ncybersecurity risks.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.00072v1",
    "published_date": "2025-01-31 05:33:48 UTC",
    "updated_date": "2025-01-31 05:33:48 UTC"
  },
  {
    "arxiv_id": "2501.18887v2",
    "title": "Building Bridges, Not Walls -- Advancing Interpretability by Unifying Feature, Data, and Model Component Attribution",
    "authors": [
      "Shichang Zhang",
      "Tessa Han",
      "Usha Bhalla",
      "Himabindu Lakkaraju"
    ],
    "abstract": "The increasing complexity of AI systems has made understanding their behavior\na critical challenge. Numerous methods have been developed to attribute model\nbehavior to three key aspects: input features, training data, and internal\nmodel components. However, these attribution methods are studied and applied\nrather independently, resulting in a fragmented landscape of approaches and\nterminology. This position paper argues that feature, data, and component\nattribution methods share fundamental similarities, and bridging them can\nbenefit interpretability research. We conduct a detailed analysis of successful\nmethods of these three attribution aspects and present a unified view to\ndemonstrate that these seemingly distinct methods employ similar approaches,\nsuch as perturbations, gradients, and linear approximations, differing\nprimarily in their perspectives rather than core techniques. Our unified\nperspective enhances understanding of existing attribution methods, identifies\nshared concepts and challenges, makes this field more accessible to newcomers,\nand highlights new directions not only for attribution and interpretability but\nalso for broader AI research, including model editing, steering, and\nregulation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.18887v2",
    "published_date": "2025-01-31 04:42:45 UTC",
    "updated_date": "2025-02-13 22:59:22 UTC"
  },
  {
    "arxiv_id": "2502.17446v1",
    "title": "DCentNet: Decentralized Multistage Biomedical Signal Classification using Early Exits",
    "authors": [
      "Xiaolin Li",
      "Binhua Huang",
      "Barry Cardiff",
      "Deepu John"
    ],
    "abstract": "DCentNet is a novel decentralized multistage signal classification approach\ndesigned for biomedical data from IoT wearable sensors, integrating early exit\npoints (EEP) to enhance energy efficiency and processing speed. Unlike\ntraditional centralized processing methods, which result in high energy\nconsumption and latency, DCentNet partitions a single CNN model into multiple\nsub-networks using EEPs. By introducing encoder-decoder pairs at EEPs, the\nsystem compresses large feature maps before transmission, significantly\nreducing wireless data transfer and power usage. If an input is confidently\nclassified at an EEP, processing stops early, optimizing efficiency. Initial\nsub-networks can be deployed on fog or edge devices to further minimize energy\nconsumption. A genetic algorithm is used to optimize EEP placement, balancing\nperformance and complexity. Experimental results on ECG classification show\nthat with one EEP, DCentNet reduces wireless data transmission by 94.54% and\ncomplexity by 21%, while maintaining original accuracy and sensitivity. With\ntwo EEPs, sensitivity reaches 98.36%, accuracy 97.74%, wireless data\ntransmission decreases by 91.86%, and complexity is reduced by 22%. Implemented\non an ARM Cortex-M4 MCU, DCentNet achieves an average power saving of 73.6%\ncompared to continuous wireless ECG transmission.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17446v1",
    "published_date": "2025-01-31 04:24:39 UTC",
    "updated_date": "2025-01-31 04:24:39 UTC"
  },
  {
    "arxiv_id": "2502.00070v2",
    "title": "Can AI Solve the Peer Review Crisis? A Large Scale Cross Model Experiment of LLMs' Performance and Biases in Evaluating over 1000 Economics Papers",
    "authors": [
      "Pat Pataranutaporn",
      "Nattavudh Powdthavee",
      "Chayapatr Achiwaranguprok",
      "Pattie Maes"
    ],
    "abstract": "This study examines the potential of large language models (LLMs) to augment\nthe academic peer review process by reliably evaluating the quality of\neconomics research without introducing systematic bias. We conduct one of the\nfirst large-scale experimental assessments of four LLMs (GPT-4o, Claude 3.5,\nGemma 3, and LLaMA 3.3) across two complementary experiments. In the first, we\nuse nonparametric binscatter and linear regression techniques to analyze over\n29,000 evaluations of 1,220 anonymized papers drawn from 110 economics journals\nexcluded from the training data of current LLMs, along with a set of\nAI-generated submissions. The results show that LLMs consistently distinguish\nbetween higher- and lower-quality research based solely on textual content,\nproducing quality gradients that closely align with established journal\nprestige measures. Claude and Gemma perform exceptionally well in capturing\nthese gradients, while GPT excels in detecting AI-generated content. The second\nexperiment comprises 8,910 evaluations designed to assess whether LLMs\nreplicate human like biases in single blind reviews. By systematically varying\nauthor gender, institutional affiliation, and academic prominence across 330\npapers, we find that GPT, Gemma, and LLaMA assign significantly higher ratings\nto submissions from top male authors and elite institutions relative to the\nsame papers presented anonymously. These results emphasize the importance of\nexcluding author-identifying information when deploying LLMs in editorial\nscreening. Overall, our findings provide compelling evidence and practical\nguidance for integrating LLMs into peer review to enhance efficiency, improve\naccuracy, and promote equity in the publication process of economics research.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.CY",
    "comment": "58 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.00070v2",
    "published_date": "2025-01-31 04:04:02 UTC",
    "updated_date": "2025-04-03 02:12:13 UTC"
  },
  {
    "arxiv_id": "2501.18867v2",
    "title": "UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent",
    "authors": [
      "Jianke Zhang",
      "Yanjiang Guo",
      "Yucheng Hu",
      "Xiaoyu Chen",
      "Xiang Zhu",
      "Jianyu Chen"
    ],
    "abstract": "Recent advancements in Vision-Language-Action (VLA) models have leveraged\npre-trained Vision-Language Models (VLMs) to improve the generalization\ncapabilities. VLMs, typically pre-trained on vision-language understanding\ntasks, provide rich semantic knowledge and reasoning abilities. However, prior\nresearch has shown that VLMs often focus on high-level semantic content and\nneglect low-level features, limiting their ability to capture detailed spatial\ninformation and understand physical dynamics. These aspects, which are crucial\nfor embodied control tasks, remain underexplored in existing pre-training\nparadigms. In this paper, we investigate the training paradigm for VLAs, and\nintroduce \\textbf{UP-VLA}, a \\textbf{U}nified VLA model training with both\nmulti-modal \\textbf{U}nderstanding and future \\textbf{P}rediction objectives,\nenhancing both high-level semantic comprehension and low-level spatial\nunderstanding. Experimental results show that UP-VLA achieves a 33% improvement\non the Calvin ABC-D benchmark compared to the previous state-of-the-art method.\nAdditionally, UP-VLA demonstrates improved success rates in real-world\nmanipulation tasks, particularly those requiring precise spatial information.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.18867v2",
    "published_date": "2025-01-31 03:20:09 UTC",
    "updated_date": "2025-02-03 03:53:25 UTC"
  },
  {
    "arxiv_id": "2501.18865v1",
    "title": "REG: Rectified Gradient Guidance for Conditional Diffusion Models",
    "authors": [
      "Zhengqi Gao",
      "Kaiwen Zha",
      "Tianyuan Zhang",
      "Zihui Xue",
      "Duane S. Boning"
    ],
    "abstract": "Guidance techniques are simple yet effective for improving conditional\ngeneration in diffusion models. Albeit their empirical success, the practical\nimplementation of guidance diverges significantly from its theoretical\nmotivation. In this paper, we reconcile this discrepancy by replacing the\nscaled marginal distribution target, which we prove theoretically invalid, with\na valid scaled joint distribution objective. Additionally, we show that the\nestablished guidance implementations are approximations to the intractable\noptimal solution under no future foresight constraint. Building on these\ntheoretical insights, we propose rectified gradient guidance (REG), a versatile\nenhancement designed to boost the performance of existing guidance methods.\nExperiments on 1D and 2D demonstrate that REG provides a better approximation\nto the optimal solution than prior guidance techniques, validating the proposed\ntheoretical framework. Extensive experiments on class-conditional ImageNet and\ntext-to-image generation tasks show that incorporating REG consistently\nimproves FID and Inception/CLIP scores across various settings compared to its\nabsence.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "19 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.18865v1",
    "published_date": "2025-01-31 03:16:18 UTC",
    "updated_date": "2025-01-31 03:16:18 UTC"
  },
  {
    "arxiv_id": "2502.00068v1",
    "title": "Privacy Preserving Charge Location Prediction for Electric Vehicles",
    "authors": [
      "Robert Marlin",
      "Raja Jurdak",
      "Alsharif Abuadbba",
      "Dimity Miller"
    ],
    "abstract": "By 2050, electric vehicles (EVs) are projected to account for 70% of global\nvehicle sales. While EVs provide environmental benefits, they also pose\nchallenges for energy generation, grid infrastructure, and data privacy.\nCurrent research on EV routing and charge management often overlooks privacy\nwhen predicting energy demands, leaving sensitive mobility data vulnerable. To\naddress this, we developed a Federated Learning Transformer Network (FLTN) to\npredict EVs' next charge location with enhanced privacy measures. Each EV\noperates as a client, training an onboard FLTN model that shares only model\nweights, not raw data with a community-based Distributed Energy Resource\nManagement System (DERMS), which aggregates them into a community global model.\nTo further enhance privacy, non-transitory EVs use peer-to-peer weight sharing\nand augmentation within their community, obfuscating individual contributions\nand improving model accuracy. Community DERMS global model weights are then\nredistributed to EVs for continuous training. Our FLTN approach achieved up to\n92% accuracy while preserving data privacy, compared to our baseline\ncentralised model, which achieved 98% accuracy with no data privacy.\nSimulations conducted across diverse charge levels confirm the FLTN's ability\nto forecast energy demands over extended periods. We present a privacy-focused\nsolution for forecasting EV charge location prediction, effectively mitigating\ndata leakage risks.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "I.6.5"
    ],
    "primary_category": "cs.CR",
    "comment": "12 pages, 7 figures, IEEE Journal paper",
    "pdf_url": "http://arxiv.org/pdf/2502.00068v1",
    "published_date": "2025-01-31 03:14:36 UTC",
    "updated_date": "2025-01-31 03:14:36 UTC"
  },
  {
    "arxiv_id": "2501.18858v1",
    "title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning",
    "authors": [
      "Han Zhong",
      "Yutong Yin",
      "Shenao Zhang",
      "Xiaojun Xu",
      "Yuanxin Liu",
      "Yifei Zuo",
      "Zhihan Liu",
      "Boyi Liu",
      "Sirui Zheng",
      "Hongyi Guo",
      "Liwei Wang",
      "Mingyi Hong",
      "Zhaoran Wang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks, yet generating reliable reasoning processes remains a\nsignificant challenge. We present a unified probabilistic framework that\nformalizes LLM reasoning through a novel graphical model incorporating latent\nthinking processes and evaluation signals. Within this framework, we introduce\nthe Bootstrapping Reinforced Thinking Process (BRiTE) algorithm, which works in\ntwo steps. First, it generates high-quality rationales by approximating the\noptimal thinking process through reinforcement learning, using a novel reward\nshaping mechanism. Second, it enhances the base LLM by maximizing the joint\nprobability of rationale generation with respect to the model's parameters.\nTheoretically, we demonstrate BRiTE's convergence at a rate of $1/T$ with $T$\nrepresenting the number of iterations. Empirical evaluations on math and coding\nbenchmarks demonstrate that our approach consistently improves performance\nacross different base models without requiring human-annotated thinking\nprocesses. In addition, BRiTE demonstrates superior performance compared to\nexisting algorithms that bootstrap thinking processes use alternative methods\nsuch as rejection sampling, and can even match or exceed the results achieved\nthrough supervised fine-tuning with human-annotated data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.18858v1",
    "published_date": "2025-01-31 02:39:07 UTC",
    "updated_date": "2025-01-31 02:39:07 UTC"
  },
  {
    "arxiv_id": "2502.01655v1",
    "title": "A binary PSO based ensemble under-sampling model for rebalancing imbalanced training data",
    "authors": [
      "Jinyan Li",
      "Yaoyang Wu",
      "Simon Fong",
      "Antonio J. Tallón-Ballesteros",
      "Xin-she Yang",
      "Sabah Mohammed",
      "Feng Wu"
    ],
    "abstract": "Ensemble technique and under-sampling technique are both effective tools used\nfor imbalanced dataset classification problems. In this paper, a novel ensemble\nmethod combining the advantages of both ensemble learning for biasing\nclassifiers and a new under-sampling method is proposed. The under-sampling\nmethod is named Binary PSO instance selection; it gathers with ensemble\nclassifiers to find the most suitable length and combination of the majority\nclass samples to build a new dataset with minority class samples. The proposed\nmethod adopts multi-objective strategy, and contribution of this method is a\nnotable improvement of the performances of imbalanced classification, and in\nthe meantime guaranteeing a best integrity possible for the original dataset.\nWe experimented the proposed method and compared its performance of processing\nimbalanced datasets with several other conventional basic ensemble methods.\nExperiment is also conducted on these imbalanced datasets using an improved\nversion where ensemble classifiers are wrapped in the Binary PSO instance\nselection. According to experimental results, our proposed methods outperform\nsingle ensemble methods, state-of-the-art under-sampling methods, and also\ncombinations of these methods with the traditional PSO instance selection\nalgorithm.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.01655v1",
    "published_date": "2025-01-31 01:45:20 UTC",
    "updated_date": "2025-01-31 01:45:20 UTC"
  },
  {
    "arxiv_id": "2501.18837v1",
    "title": "Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming",
    "authors": [
      "Mrinank Sharma",
      "Meg Tong",
      "Jesse Mu",
      "Jerry Wei",
      "Jorrit Kruthoff",
      "Scott Goodfriend",
      "Euan Ong",
      "Alwin Peng",
      "Raj Agarwal",
      "Cem Anil",
      "Amanda Askell",
      "Nathan Bailey",
      "Joe Benton",
      "Emma Bluemke",
      "Samuel R. Bowman",
      "Eric Christiansen",
      "Hoagy Cunningham",
      "Andy Dau",
      "Anjali Gopal",
      "Rob Gilson",
      "Logan Graham",
      "Logan Howard",
      "Nimit Kalra",
      "Taesung Lee",
      "Kevin Lin",
      "Peter Lofgren",
      "Francesco Mosconi",
      "Clare O'Hara",
      "Catherine Olsson",
      "Linda Petrini",
      "Samir Rajani",
      "Nikhil Saxena",
      "Alex Silverstein",
      "Tanya Singh",
      "Theodore Sumers",
      "Leonard Tang",
      "Kevin K. Troy",
      "Constantin Weisser",
      "Ruiqi Zhong",
      "Giulio Zhou",
      "Jan Leike",
      "Jared Kaplan",
      "Ethan Perez"
    ],
    "abstract": "Large language models (LLMs) are vulnerable to universal jailbreaks-prompting\nstrategies that systematically bypass model safeguards and enable users to\ncarry out harmful processes that require many model interactions, like\nmanufacturing illegal substances at scale. To defend against these attacks, we\nintroduce Constitutional Classifiers: safeguards trained on synthetic data,\ngenerated by prompting LLMs with natural language rules (i.e., a constitution)\nspecifying permitted and restricted content. In over 3,000 estimated hours of\nred teaming, no red teamer found a universal jailbreak that could extract\ninformation from an early classifier-guarded LLM at a similar level of detail\nto an unguarded model across most target queries. On automated evaluations,\nenhanced classifiers demonstrated robust defense against held-out\ndomain-specific jailbreaks. These classifiers also maintain deployment\nviability, with an absolute 0.38% increase in production-traffic refusals and a\n23.7% inference overhead. Our work demonstrates that defending against\nuniversal jailbreaks while maintaining practical deployment viability is\ntractable.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.18837v1",
    "published_date": "2025-01-31 01:09:32 UTC",
    "updated_date": "2025-01-31 01:09:32 UTC"
  },
  {
    "arxiv_id": "2501.18834v1",
    "title": "Pitfalls of defacing whole-head MRI: re-identification risk with diffusion models and compromised research potential",
    "authors": [
      "Chenyu Gao",
      "Kaiwen Xu",
      "Michael E. Kim",
      "Lianrui Zuo",
      "Zhiyuan Li",
      "Derek B. Archer",
      "Timothy J. Hohman",
      "Ann Zenobia Moore",
      "Luigi Ferrucci",
      "Lori L. Beason-Held",
      "Susan M. Resnick",
      "Christos Davatzikos",
      "Jerry L. Prince",
      "Bennett A. Landman"
    ],
    "abstract": "Defacing is often applied to head magnetic resonance image (MRI) datasets\nprior to public release to address privacy concerns. The alteration of facial\nand nearby voxels has provoked discussions about the true capability of these\ntechniques to ensure privacy as well as their impact on downstream tasks. With\nadvancements in deep generative models, the extent to which defacing can\nprotect privacy is uncertain. Additionally, while the altered voxels are known\nto contain valuable anatomical information, their potential to support research\nbeyond the anatomical regions directly affected by defacing remains uncertain.\nTo evaluate these considerations, we develop a refacing pipeline that recovers\nfaces in defaced head MRIs using cascaded diffusion probabilistic models\n(DPMs). The DPMs are trained on images from 180 subjects and tested on images\nfrom 484 unseen subjects, 469 of whom are from a different dataset. To assess\nwhether the altered voxels in defacing contain universally useful information,\nwe also predict computed tomography (CT)-derived skeletal muscle radiodensity\nfrom facial voxels in both defaced and original MRIs. The results show that\nDPMs can generate high-fidelity faces that resemble the original faces from\ndefaced images, with surface distances to the original faces significantly\nsmaller than those of a population average face (p < 0.05). This performance\nalso generalizes well to previously unseen datasets. For skeletal muscle\nradiodensity predictions, using defaced images results in significantly weaker\nSpearman's rank correlation coefficients compared to using original images (p <\n10-4). For shin muscle, the correlation is statistically significant (p < 0.05)\nwhen using original images but not statistically significant (p > 0.05) when\nany defacing method is applied, suggesting that defacing might not only fail to\nprotect privacy but also eliminate valuable information.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.18834v1",
    "published_date": "2025-01-31 00:58:12 UTC",
    "updated_date": "2025-01-31 00:58:12 UTC"
  },
  {
    "arxiv_id": "2501.18821v2",
    "title": "An Optimal Cascade Feature-Level Spatiotemporal Fusion Strategy for Anomaly Detection in CAN Bus",
    "authors": [
      "Mohammad Fatahi",
      "Danial Sadrian Zadeh",
      "Benyamin Ghojogh",
      "Behzad Moshiri",
      "Otman Basir"
    ],
    "abstract": "Autonomous vehicles represent a revolutionary advancement driven by the\nintegration of artificial intelligence within intelligent transportation\nsystems. However, they remain vulnerable due to the absence of robust security\nmechanisms in the Controller Area Network (CAN) bus. In order to mitigate the\nsecurity issue, many machine learning models and strategies have been proposed,\nwhich primarily focus on a subset of dominant patterns of anomalies and lack\nrigorous evaluation in terms of reliability and robustness. Therefore, to\naddress the limitations of previous works and mitigate the security\nvulnerability in CAN bus, the current study develops a model based on the\nintrinsic nature of the problem to cover all dominant patterns of anomalies. To\nachieve this, a cascade feature-level fusion strategy optimized by a\ntwo-parameter genetic algorithm is proposed to combine temporal and spatial\ninformation. Subsequently, the model is evaluated using a paired t-test to\nensure reliability and robustness. Finally, a comprehensive comparative\nanalysis conducted on two widely used datasets advocates that the proposed\nmodel outperforms other models and achieves superior accuracy and F1-score,\ndemonstrating the best performance among all models presented to date.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "v2: updated the text and graphs",
    "pdf_url": "http://arxiv.org/pdf/2501.18821v2",
    "published_date": "2025-01-31 00:36:08 UTC",
    "updated_date": "2025-03-05 04:45:03 UTC"
  },
  {
    "arxiv_id": "2501.18817v1",
    "title": "Bridging the Reasoning Gap: Small LLMs Can Plan with Generalised Strategies",
    "authors": [
      "Andrey Borro",
      "Patricia J Riddle",
      "Michael W Barley",
      "Michael J Witbrock"
    ],
    "abstract": "Recent advancements in the reasoning skills of Large Language Models (LLMs)\ndemonstrate an increase in the ability of LLMs to solve simple planning tasks.\nHowever, as long as the driving force behind improved reasoning capability is\nthe size and complexity of the model, the financial and computational costs\nassociated with running them will also increase. This trend raises questions\nabout continued accessibility and whether these improvements will increase at\nthe same pace as models continue to grow in size and expense. We propose two\napproaches to enhance the reasoning ability of less resource-intensive LLMs.\n(1) Provide them with a generalised strategy for solving tasks within a given\ndomain, generated by a more resource-intensive LLM. (2) Exploit their\ncost-effectiveness by iteratively prompting these models to correct errors in\ntheir proposed solutions. Our empirical results from planning and mathematical\nreasoning tasks demonstrate that these methods improve the performance of less\nresource-intensive LLMs to levels comparable with their more resource-intensive\ncounterparts, at a fraction of the cost. Additionally, we show that the\nutilisation of generalised strategies in our experiments reduced the cost of\nthe less resource-intensive model by nearly 30 percent on average.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "I.2.8; I.2.6"
    ],
    "primary_category": "cs.AI",
    "comment": "7 page body, 2 page references, 16 page appendix (25 pages total); 2\n  figures; submitted to IJCAI2025",
    "pdf_url": "http://arxiv.org/pdf/2501.18817v1",
    "published_date": "2025-01-31 00:28:29 UTC",
    "updated_date": "2025-01-31 00:28:29 UTC"
  },
  {
    "arxiv_id": "2501.18816v1",
    "title": "Large Language Models as Common-Sense Heuristics",
    "authors": [
      "Andrey Borro",
      "Patricia J Riddle",
      "Michael W Barley",
      "Michael J Witbrock"
    ],
    "abstract": "While systems designed for solving planning tasks vastly outperform Large\nLanguage Models (LLMs) in this domain, they usually discard the rich semantic\ninformation embedded within task descriptions. In contrast, LLMs possess\nparametrised knowledge across a wide range of topics, enabling them to leverage\nthe natural language descriptions of planning tasks in their solutions.\nHowever, current research in this direction faces challenges in generating\ncorrect and executable plans. Furthermore, these approaches depend on the LLM\nto output solutions in an intermediate language, which must be translated into\nthe representation language of the planning task. We introduce a novel planning\nmethod, which leverages the parametrised knowledge of LLMs by using their\noutput as a heuristic for Hill-Climbing Search. This approach is further\nenhanced by prompting the LLM to generate a solution estimate to guide the\nsearch. Our method outperforms the task success rate of similar systems within\na common household environment by 22 percentage points, with consistently\nexecutable plans. All actions are encoded in their original representation,\ndemonstrating that strong results can be achieved without an intermediate\nlanguage, thus eliminating the need for a translation step.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.8"
    ],
    "primary_category": "cs.CL",
    "comment": "7 page body, 2 page references, 5 page appendix (14 page total); 1\n  figure; Submitted to IJCAI2025",
    "pdf_url": "http://arxiv.org/pdf/2501.18816v1",
    "published_date": "2025-01-31 00:26:38 UTC",
    "updated_date": "2025-01-31 00:26:38 UTC"
  },
  {
    "arxiv_id": "2501.18815v1",
    "title": "An Adversarial Approach to Register Extreme Resolution Tissue Cleared 3D Brain Images",
    "authors": [
      "Abdullah Naziba",
      "Clinton Fookes",
      "Dimitri Perrin"
    ],
    "abstract": "We developed a generative patch based 3D image registration model that can\nregister very high resolution images obtained from a biochemical process name\ntissue clearing. Tissue clearing process removes lipids and fats from the\ntissue and make the tissue transparent. When cleared tissues are imaged with\nLight-sheet fluorescent microscopy, the resulting images give a clear window to\nthe cellular activities and dynamics inside the tissue.Thus the images obtained\nare very rich with cellular information and hence their resolution is extremely\nhigh (eg .2560x2160x676). Analyzing images with such high resolution is a\ndifficult task for any image analysis pipeline.Image registration is a common\nstep in image analysis pipeline when comparison between images are required.\nTraditional image registration methods fail to register images with such\nextant. In this paper we addressed this very high resolution image registration\nissue by proposing a patch-based generative network named InvGAN. Our proposed\nnetwork can register very high resolution tissue cleared images. The tissue\ncleared dataset used in this paper are obtained from a tissue clearing protocol\nnamed CUBIC. We compared our method both with traditional and deep-learning\nbased registration methods.Two different versions of CUBIC dataset are used,\nrepresenting two different resolutions 25% and 100% respectively. Experiments\non two different resolutions clearly show the impact of resolution on the\nregistration quality. At 25% resolution, our method achieves comparable\nregistration accuracy with very short time (7 minutes approximately). At 100%\nresolution, most of the traditional registration methods fail except Elastix\nregistration tool.Elastix takes 28 hours to register where proposed InvGAN\ntakes only 10 minutes.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.18815v1",
    "published_date": "2025-01-31 00:19:45 UTC",
    "updated_date": "2025-01-31 00:19:45 UTC"
  }
]
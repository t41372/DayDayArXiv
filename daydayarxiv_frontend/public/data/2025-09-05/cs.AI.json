{
  "date": "2025-09-05",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-09-05 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\næˆ‘æ˜¯ä½ ä»¬çš„æ—¥æŠ¥ä½œè€…ï¼Œä»Šå¤© arXiv æ›´æ–°äº† 107 ç¯‡è®ºæ–‡ã€‚\n\n**ä¸€å¥è¯æ€»ç»“ï¼š** ä»Šå¤©çš„ arXiv **æ¶æ„åˆ›æ–°**äº•å–·ï¼Œä» Preferred Networks å‘å¸ƒçš„æ··åˆæ¶æ„ **PLaMo 2**ï¼Œåˆ°å¤§è§„æ¨¡è„‰å†²ç¥ç»ç½‘ç»œ **SpikingBrain**ï¼Œå†åˆ°èƒ½åœ¨å•ç‰‡æœºä¸Šè·‘çš„ **MambaLite**ï¼Œå¤§å®¶éƒ½åœ¨æ‹¼å‘½åœ¨è¿™ä¸ªå Transformer æ—¶ä»£å¯»æ‰¾æ›´é«˜æ•ˆçš„è®¡ç®—èŒƒå¼ï¼›ä¸æ­¤åŒæ—¶ï¼Œ**Agentï¼ˆæ™ºèƒ½ä½“ï¼‰** çš„ç ”ç©¶å¼€å§‹ä»â€œå¦‚ä½•æ„å»ºâ€è½¬å‘â€œå¦‚ä½•æ²»ç†â€å’Œâ€œåä½œé™·é˜±â€çš„åæ€ã€‚\n\n---\n\n### ğŸš€ æ¶æ„åˆ›æ–°ï¼šæ··åˆæ¨¡å‹ä¸è„‰å†²ç½‘ç»œ\nè¿™ä¸€æ¿å—ä»Šå¤©éå¸¸ç¡¬æ ¸ï¼Œç ”ç©¶è€…ä»¬æ­£åœ¨è¯•å›¾æ‰“ç ´ Transformer çš„å„æ–­æˆ–å¯¹å…¶è¿›è¡Œæœ¬è´¨ä¸Šçš„é­”æ”¹ã€‚\n\n**1. PLaMo 2: Technical Report**\n**PLaMo 2 æŠ€æœ¯æŠ¥å‘Š**\n> æ ¸å¿ƒçœ‹ç‚¹ï¼š**æ··åˆæ¶æ„ (Hybrid Architecture)**ã€**Samba**ã€**çŠ¶æ€ç©ºé—´æ¨¡å‹ (SSM)**\n> è¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰æ„æ€çš„å°è¯•ã€‚æ—¥æœ¬ Preferred Networks å‘å¸ƒäº† PLaMo 2ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº **Samba** çš„æ··åˆæ¶æ„æ¨¡å‹ï¼ˆSSM + Attentionï¼‰ã€‚å®ƒé€šè¿‡æŒç»­é¢„è®­ç»ƒï¼ˆContinual Pre-trainingï¼‰è¿‡æ¸¡åˆ°å…¨æ³¨æ„åŠ›æœºåˆ¶ä»¥æ”¯æŒ 32K é•¿ä¸Šä¸‹æ–‡ã€‚æœ€æƒŠäººçš„æ˜¯ï¼Œä»–ä»¬é€šè¿‡ç»“æ„åŒ–å‰ªæï¼ˆStructured Pruningï¼‰ï¼Œè®©ä¸€ä¸ª 8B çš„æ¨¡å‹è¾¾åˆ°äº†ä¹‹å‰ 100B æ¨¡å‹çš„æ€§èƒ½æ°´å¹³ã€‚è¿™ä¸ºâ€œå°æ¨¡å‹ã€é«˜æ€§èƒ½â€æä¾›äº†ä¸€æ¡éä¸»æµä½†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚\n\n**2. SpikingBrain: Spiking Brain-inspired Large Models**\n**SpikingBrainï¼šè„‘å¯å‘è„‰å†²å¤§æ¨¡å‹**\n> æ ¸å¿ƒçœ‹ç‚¹ï¼š**è„‰å†²ç¥ç»ç½‘ç»œ (SNN)**ã€**èƒ½æ•ˆæ¯”**ã€**é Nvidia ç¡¬ä»¶**\n> è¿™æ˜¯ä¸€ä¸ªé‡å¿ƒå‹ƒå‹ƒçš„å·¥ä½œã€‚ä½œè€…å›¢é˜Ÿæå‡ºäº† SpikingBrainï¼Œè¯•å›¾è§£å†³ Transformer è®¡ç®—é‡äºŒæ¬¡å¢é•¿çš„é—®é¢˜ã€‚ä»–ä»¬æå‡ºäº† SpikingBrain-76Bï¼ˆæ··åˆçº¿æ€§ MoE LLMï¼‰ï¼Œè¿™æ˜¯ç›®å‰æå…¶ç½•è§çš„å¤§è§„æ¨¡è„‰å†²ç¥ç»ç½‘ç»œåº”ç”¨ã€‚åœ¨ MetaX GPU é›†ç¾¤ä¸Šï¼Œä»–ä»¬è¯æ˜äº† SNN å¯ä»¥åƒ LLM ä¸€æ ·è¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒï¼Œä¸”æ¨ç†æ—¶çš„å†…å­˜å ç”¨æä½ï¼ˆéƒ¨åˆ†å¸¸é‡çº§ï¼‰ï¼Œè¿™å¯èƒ½æ˜¯æœªæ¥ä½åŠŸè€—å¤§æ¨¡å‹çš„ä¸€ä¸ªé‡è¦æ–¹å‘ã€‚\n\n**3. MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs**\n**MambaLite-Microï¼šå¾®æ§åˆ¶å™¨ä¸Šçš„å†…å­˜ä¼˜åŒ– Mamba æ¨ç†**\n> æ ¸å¿ƒçœ‹ç‚¹ï¼š**Edge AI**ã€**Mamba**ã€**å¾®æ§åˆ¶å™¨ (MCU)**\n> Mamba ç»ˆäºæ€åˆ°äº†å¾®æ§åˆ¶å™¨ï¼ˆMCUï¼‰ä¸Šã€‚è¿™æ˜¯é¦–ä¸ªåœ¨èµ„æºå—é™ MCU ä¸Šéƒ¨ç½² Mamba æ¶æ„çš„å·¥ä½œã€‚ä½œè€…æ‰‹å†™äº† C è¯­è¨€ç®—å­ï¼Œæ¶ˆé™¤äº†å¤§çš„ä¸­é—´å¼ é‡ï¼ŒæŠŠå³°å€¼å†…å­˜å‡å°‘äº† 83%ã€‚è¿™æ„å‘³ç€åƒ ESP32 è¿™ç§å‡ å—é’±çš„èŠ¯ç‰‡ä¹Ÿèƒ½è·‘å…ˆè¿›çš„åºåˆ—æ¨¡å‹äº†ï¼Œç‰©è”ç½‘ï¼ˆIoTï¼‰è®¾å¤‡å°†è¿æ¥æ™ºå•†å‡çº§ã€‚\n\n---\n\n### ğŸ¤– Agent ç”Ÿæ€ä¸åæ€\nAgent ä¸ä»…ä»…æ˜¯â€œèƒ½ç”¨å·¥å…·â€ï¼Œç ”ç©¶ç•Œå¼€å§‹æ€è€ƒ Agent ç»„æˆçš„ç½‘ç»œï¼ˆWeb-of-Agentsï¼‰è¯¥å¦‚ä½•è¿ä½œï¼Œä»¥åŠå¤šæ™ºèƒ½ä½“è¾©è®ºæ˜¯å¦çœŸçš„æœ‰æ•ˆã€‚\n\n**4. Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for Ranking Agents**\n**äº’è”ç½‘ 3.0ï¼šWeb-of-Agents æ¶æ„åŠå…¶æ™ºèƒ½ä½“æ’åç®—æ³•**\n> æ ¸å¿ƒçœ‹ç‚¹ï¼š**AgentRank**ã€**DOVIS åè®®**ã€**å»ä¸­å¿ƒåŒ–**\n> è¿™æ˜¯ä¸€ä¸ªå®å¤§çš„æ„¿æ™¯è®ºæ–‡ã€‚ä½œè€…è®¤ä¸ºæœªæ¥çš„äº’è”ç½‘æ˜¯ **Web of Agents**ã€‚å°±åƒ Web 1.0 éœ€è¦ PageRank ä¸€æ ·ï¼ŒWeb of Agents éœ€è¦ **AgentRank**ã€‚ä½œè€…æå‡ºäº† DOVIS åè®®ï¼ˆå‘ç°ã€ç¼–æ’ã€éªŒè¯ã€æ¿€åŠ±ã€è¯­ä¹‰ï¼‰ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ç®—æ³•ï¼Œæ ¹æ® Agent çš„â€œä½¿ç”¨ç‡â€å’Œâ€œèƒ½åŠ›ï¼ˆè´¨é‡ã€å®‰å…¨ã€å»¶è¿Ÿï¼‰â€å¯¹å…¶è¿›è¡Œæ’åã€‚è¿™æ˜¯åœ¨ä¸ºæœªæ¥çš„ AI æµé‡åˆ†å‘å®šè§„çŸ©ã€‚\n\n**5. Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate**\n**ç©ºè°ˆè¯¯å›½ï¼šç†è§£å¤šæ™ºèƒ½ä½“è¾©è®ºä¸­çš„å¤±è´¥æ¨¡å¼**\n> æ ¸å¿ƒçœ‹ç‚¹ï¼š**å¤šæ™ºèƒ½ä½“è¾©è®º (Multi-Agent Debate)**ã€**é˜¿è°€å¥‰æ‰¿ (Sycophancy)**ã€**ä»ä¼—å¿ƒç†**\n> æ³¼å†·æ°´çš„ç ”ç©¶ã€‚ä¹‹å‰å¤§å®¶è®¤ä¸ºè®© AI äº’å–·ï¼ˆè¾©è®ºï¼‰èƒ½æé«˜æ¨ç†èƒ½åŠ›ï¼Œä½†è¿™ç¯‡æ–‡ç« å‘ç°ï¼š**è¾©è®ºæœ‰æ—¶æ˜¯æœ‰å®³çš„**ã€‚å³ä½¿å¼ºæ¨¡å‹å¤šäºå¼±æ¨¡å‹ï¼Œæ¨¡å‹ä¹Ÿç»å¸¸ä¼šå› ä¸ºâ€œç¤¾ä¼šå‹åŠ›â€æˆ–â€œé˜¿è°€å¥‰æ‰¿â€è€Œæ”¾å¼ƒæ­£ç¡®ç­”æ¡ˆï¼Œè½¬è€ŒåŒæ„é”™è¯¯çš„æ¨ç†ã€‚è¿™æé†’æˆ‘ä»¬ï¼Œç®€å•çš„å¤šæ™ºèƒ½ä½“å †å å¹¶ä¸ç­‰äºæ›´é«˜çš„æ™ºèƒ½ï¼Œåè€Œå¯èƒ½å¸¦æ¥ç¾¤ä½“è¿·æ€ã€‚\n\n**6. Generative World Models of Tasks: LLM-Driven Hierarchical Scaffolding for Embodied Agents**\n**ä»»åŠ¡çš„ç”Ÿæˆå¼ä¸–ç•Œæ¨¡å‹ï¼šLLM é©±åŠ¨çš„å…·èº«æ™ºèƒ½ä½“åˆ†å±‚è„šæ‰‹æ¶**\n> æ ¸å¿ƒçœ‹ç‚¹ï¼š**å…·èº«æ™ºèƒ½**ã€**åˆ†å±‚ä»»åŠ¡ç½‘ç»œ (HTN)**ã€**ä¸–ç•Œæ¨¡å‹**\n> é’ˆå¯¹å¤æ‚çš„é•¿ç¨‹ä»»åŠ¡ï¼ˆå¦‚æœºå™¨äººè¶³çƒï¼‰ï¼Œç«¯åˆ°ç«¯çš„ RL å¾€å¾€è¡Œä¸é€šã€‚ä½œè€…æå‡ºåˆ©ç”¨ LLM ä½œä¸ºâ€œä»»åŠ¡ç”Ÿæˆå™¨â€ï¼ŒåŠ¨æ€æ„å»ºåˆ†å±‚ä»»åŠ¡ç¯å¢ƒï¼ˆHTEsï¼‰ï¼Œä¸º Agent æä¾›å­¦ä¹ çš„è„šæ‰‹æ¶ã€‚è¿™æ˜¯ä¸€ç§å°†ç¬¦å· AI çš„ç»“æ„åŒ–ä¸ç°ä»£ RL ç»“åˆçš„æœ‰æ•ˆæ€è·¯ã€‚\n\n---\n\n### ğŸ§  LLM åŸºç¡€ç†è®ºä¸å®‰å…¨æ€§\né™¤äº†æ¶æ„ï¼Œä»Šå¤©åœ¨ä½ç½®ç¼–ç çš„æ”¹è¿›å’Œæ¨¡å‹â€œé—å¿˜â€æœºåˆ¶ä¸Šä¹Ÿæœ‰äº®ç‚¹ã€‚\n\n**7. Newton to Einstein: Axiom-Based Discovery via Game Design**\n**ä»ç‰›é¡¿åˆ°çˆ±å› æ–¯å¦ï¼šåŸºäºå…¬ç†çš„æ¸¸æˆåŒ–å‘ç°**\n> æ ¸å¿ƒçœ‹ç‚¹ï¼š**AI for Science**ã€**å…¬ç†æ¨ç†**ã€**å½’çº³ vs æ¼”ç»**\n> è¿™æ˜¯ä¸€ç¯‡ç«‹åœºè®ºæ–‡ï¼ˆPosition Paperï¼‰ã€‚ä½œè€…è®¤ä¸º AI æç§‘å­¦å‘ç°ä¸èƒ½åªé â€œå½’çº³æ¨¡å¼è¯†åˆ«â€ï¼ˆç°åœ¨çš„ LLM ä¸»æµï¼‰ï¼Œè€Œåº”è¯¥è½¬å‘ **åŸºäºå…¬ç†çš„æ¨ç†**ã€‚ä»–ä»¬æŠŠç§‘å­¦æ¢ç´¢é‡æ„ä¸ºä¸€ä¸ªè§„åˆ™æ¼”åŒ–çš„æ¸¸æˆç³»ç»Ÿï¼ŒAgent åœ¨å…¶ä¸­ä¿®æ”¹å…¬ç†ä»¥è§£é‡Šå¼‚å¸¸è§‚æµ‹ã€‚è¿™æ˜¯é€šå‘æ›´å…·è§£é‡Šæ€§å’Œåˆ›é€ æ€§ AI çš„ä¸€æ¡è·¯ã€‚\n\n**8. Pre-Forgettable Models: Prompt Learning as a Native Mechanism for Unlearning**\n**é¢„é—å¿˜æ¨¡å‹ï¼šå°† Prompt Learning ä½œä¸ºåŸç”Ÿçš„é—å¿˜æœºåˆ¶**\n> æ ¸å¿ƒçœ‹ç‚¹ï¼š**æœºå™¨é—å¿˜ (Unlearning)**ã€**éšç§åˆè§„ (GDPR)**ã€**Prompt Tuning**\n> æ€ä¹ˆè®©å¤§æ¨¡å‹â€œå¿˜æ‰â€ç‰¹å®šçš„æ•°æ®ï¼ˆå¦‚éšç§ä¿¡æ¯ï¼‰ï¼Ÿä¼ ç»Ÿçš„é‡è®­å¤ªè´µã€‚è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ä¸ªæå…¶èªæ˜çš„åšæ³•ï¼šåœ¨è®­ç»ƒæ—¶å°±æŠŠç±»åˆ«è¯­ä¹‰ç»‘å®šåˆ°ç‰¹å®šçš„ **Prompt Token** ä¸Šã€‚æƒ³é—å¿˜æŸä¸ªæ¦‚å¿µï¼Ÿç›´æ¥åˆ æ‰é‚£ä¸ª Token å°±è¡Œäº†ï¼Œæ— éœ€è§¦ç¢°æ¨¡å‹æƒé‡ã€‚è¿™ä¸º GDPR åˆè§„æä¾›äº†ä¸€ä¸ªä½æˆæœ¬æ–¹æ¡ˆã€‚\n\n**9. HoPE: Hyperbolic Rotary Positional Encoding & Decoupling the \"What\" and \"Where\" (PoPE)**\n**HoPE ä¸ PoPEï¼šä½ç½®ç¼–ç çš„åŒé‡æ”¹è¿›**\n> æ ¸å¿ƒçœ‹ç‚¹ï¼š**RoPE æ”¹è¿›**ã€**é•¿æ–‡æœ¬å¤–æ¨**ã€**åŒæ›²å‡ ä½•**\n> ä»Šå¤©æœ‰ä¸¤ç¯‡å…³äºæ”¹è¿› RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰çš„æ–‡ç« ã€‚\n> *   **HoPE** å¼•å…¥äº†åŒæ›²å‡ ä½•ï¼ˆHyperbolic geometryï¼‰æ¥åšä½ç½®ç¼–ç ï¼Œè§£å†³äº† RoPE åœ¨é•¿è·ç¦»ä¾èµ–ä¸Šçš„éœ‡è¡é—®é¢˜ã€‚\n> *   **PoPE (Polar Coordinate Positional Embeddings)** æŒ‡å‡º RoPE æ··æ·†äº†â€œå†…å®¹â€å’Œâ€œä½ç½®â€ï¼Œå¹¶åœ¨æåæ ‡ç³»ä¸‹è§£è€¦äº†è¿™ä¸¤è€…ï¼Œæ˜¾è‘—æå‡äº†å¤–æ¨èƒ½åŠ›ã€‚\n> æé•¿æ–‡æœ¬æ¨¡å‹çš„åŒå­¦è¯·å…³æ³¨è¿™ä¸¤ç¯‡ã€‚\n\n---\n\n### ğŸŒ ç¤¾ä¼šå½±å“ä¸å¤šæ¨¡æ€æ•°æ®\n\n**10. The Token Tax: Systematic Bias in Multilingual Tokenization**\n**Token ç¨ï¼šå¤šè¯­è¨€ Tokenization ä¸­çš„ç³»ç»Ÿæ€§åè§**\n> æ ¸å¿ƒçœ‹ç‚¹ï¼š**å…¬å¹³æ€§**ã€**Token æ•ˆç‡**ã€**ä½èµ„æºè¯­è¨€**\n> è¿™ç¯‡æ–‡ç« é‡åŒ–äº†â€œToken ç¨â€ã€‚å¯¹äºå½¢æ€å¤æ‚çš„ä½èµ„æºè¯­è¨€ï¼ˆå¦‚éæ´²è¯­è¨€ï¼‰ï¼ŒToken åŒ–æ•ˆç‡æä½ï¼ˆä¸€ä¸ªè¯è¢«åˆ‡æˆå¾ˆå¤š Tokenï¼‰ã€‚è¿™ç›´æ¥å¯¼è‡´æ¨ç†æˆæœ¬ç¿»å€ã€ç”šè‡³å››å€ï¼Œä¸” Token å¯†åº¦é«˜çš„è¯­è¨€æ¨¡å‹æ€§èƒ½æ™®éæ›´å·®ã€‚è¿™æ˜¯ LLM æ—¶ä»£çš„â€œè´«å¯Œå·®è·â€ã€‚\n\n**11. OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation**\n**OpenEgoï¼šç”¨äºçµå·§æ“ä½œçš„å¤§è§„æ¨¡å¤šæ¨¡æ€ç¬¬ä¸€äººç§°æ•°æ®é›†**\n> æ ¸å¿ƒçœ‹ç‚¹ï¼š**æ•°æ®é›†**ã€**çµå·§æ‰‹**ã€**ç¬¬ä¸€äººç§°è§†è§’**\n> æœºå™¨äººçµå·§æ‰‹æ“ä½œä¸€ç›´ç¼ºæ•°æ®ã€‚OpenEgo æä¾›äº† 1107 å°æ—¶çš„è§†é¢‘ï¼Œè¦†ç›– 290 ç§æ“ä½œä»»åŠ¡ï¼Œå¹¶ä¸”ç»Ÿä¸€äº†æ‰‹éƒ¨å§¿æ€æ ‡æ³¨ã€‚è¿™æ˜¯ Vision-Language-Action (VLA) å­¦ä¹ çš„é‡è¦èµ„æºã€‚\n\n---\n\n### âš¡ï¸ å¿«é€Ÿæ å½± (Quick Reads)\n\n*   **[Uncertainty] The Non-Determinism of Small LLMs**: å°æ¨¡å‹ï¼ˆ2B-8Bï¼‰åœ¨é‡å¤å›ç­”åŒä¸€é—®é¢˜æ—¶ä¸€è‡´æ€§å¾ˆå·®ï¼ˆ50%-80%ï¼‰ï¼Œå³ä½¿æ¸©åº¦å¾ˆä½ã€‚é€‰å°æ¨¡å‹åšç¡®å®šæ€§ä»»åŠ¡è¦å°å¿ƒã€‚\n*   **[Vision] SparkUI-Parser**: æ—¢ç„¶ç°åœ¨çš„å¤šæ¨¡æ€æ¨¡å‹å®šä½ GUI å…ƒç´ ä¸å‡†ï¼Œè¿™ä¸ªå·¥ä½œé€šè¿‡è¿ç»­åæ ‡å»ºæ¨¡å’Œé¢å¤–çš„ Router æ˜¾è‘—æå‡äº† GUI æ™ºèƒ½ä½“çš„ç‚¹å‡»ç²¾åº¦ã€‚\n*   **[Application] FloodVision**: ç»“åˆ GPT-4o å’Œé¢†åŸŸçŸ¥è¯†å›¾è°±æ¥ä¼°ç®—åŸå¸‚æ´ªæ°´æ·±åº¦ã€‚åˆ©ç”¨è·¯è¾¹çš„æ±½è½¦ã€äººä½œä¸ºå‚ç…§ç‰©ï¼Œè¿™æ‰æ˜¯ AI è½åœ°çš„æ ·å­ã€‚\n*   **[Testing] Combining TSL and LLM to Automate REST API Testing**: æµ‹è¯„äº†å„è·¯ LLM å†™ API æµ‹è¯•ä»£ç çš„èƒ½åŠ›ï¼Œç»“è®ºæ˜¯ **Claude 3.5 Sonnet** åŠæ‰“å…¶ä»–æ¨¡å‹ï¼ˆåŒ…æ‹¬ Deepseek R1 å’Œ Qwen 2.5ï¼‰ã€‚\n*   **[Security] Behind the Mask**: å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•é›†ï¼Œä¸“é—¨æµ‹â€œä¼ªè£…è¶Šç‹±â€ï¼ˆCamouflaged Jailbreaksï¼‰ï¼Œå³æŠŠæ¶æ„æ„å›¾è—åœ¨çœ‹ä¼¼è‰¯æ€§çš„æ–‡æœ¬é‡Œã€‚\n\n---\n\nä»Šå¤©çš„ arXiv å¿«æŠ¥å°±åˆ°è¿™é‡Œã€‚ä»Šå¤©çš„è¶‹åŠ¿å¾ˆæ˜æ˜¾ï¼šäººä»¬ä¸å†æ»¡è¶³äº Transformer ç°æœ‰çš„æ•ˆç‡ï¼Œå¼€å§‹å‘**ç¡¬ä»¶åº•å±‚ï¼ˆSNN/Sambaï¼‰**å’Œ**æ•°å­¦åŸç†ï¼ˆåŒæ›²å‡ ä½•ï¼‰**åŠ¨åˆ€ï¼›åŒæ—¶ï¼Œå¯¹äº AI Agent çš„ç ”ç©¶ä¹Ÿæ›´åŠ åŠ¡å®ï¼Œå¼€å§‹å…³æ³¨**åä½œä¸­çš„ç¤¾ä¼šå­¦é™·é˜±**å’Œ**å®è§‚çš„æµé‡åˆ†å‘æœºåˆ¶**ã€‚\n\nç¥å¤§å®¶ç§‘ç ”é¡ºåˆ©ï¼Œæˆ‘ä»¬æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2509.07998v1",
      "title": "Bilingual Word Level Language Identification for Omotic Languages",
      "title_zh": "é’ˆå¯¹å¥¥è«è¯­çš„è¯çº§åŒè¯­è¯­è¨€è¯†åˆ«",
      "authors": [
        "Mesay Gemeda Yigezu",
        "Girma Yohannis Bade",
        "Atnafu Lambebo Tonja",
        "Olga Kolesnikova",
        "Grigori Sidorov",
        "Alexander Gelbukh"
      ],
      "abstract": "Language identification is the task of determining the languages for a given text. In many real world scenarios, text may contain more than one language, particularly in multilingual communities. Bilingual Language Identification (BLID) is the task of identifying and distinguishing between two languages in a given text. This paper presents BLID for languages spoken in the southern part of Ethiopia, namely Wolaita and Gofa. The presence of words similarities and differences between the two languages makes the language identification task challenging. To overcome this challenge, we employed various experiments on various approaches. Then, the combination of the BERT based pretrained language model and LSTM approach performed better, with an F1 score of 0.72 on the test set. As a result, the work will be effective in tackling unwanted social media issues and providing a foundation for further research in this area.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸƒå¡ä¿„æ¯”äºšå—éƒ¨çš„Wolaitaè¯­å’ŒGofaè¯­ï¼Œæå‡ºäº†å•è¯çº§åˆ«çš„åŒè¯­è¯­è¨€è¯†åˆ«(Bilingual Language Identification, BLID)æ–¹æ¡ˆã€‚ç”±äºä¸¤ç§è¯­è¨€åœ¨è¯æ±‡ä¸Šçš„ç›¸ä¼¼æ€§ä¸å·®å¼‚æ€§å¢åŠ äº†è¯†åˆ«éš¾åº¦ï¼Œç ”ç©¶è€…å¯¹æ¯”äº†å¤šç§æŠ€æœ¯è·¯å¾„ï¼Œæœ€ç»ˆé‡‡ç”¨åŸºäºBERTçš„é¢„è®­ç»ƒæ¨¡å‹ç»“åˆLSTMçš„æ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†0.72çš„F1åˆ†æ•°ã€‚è¿™é¡¹å·¥ä½œä¸ä»…ä¸ºåº”å¯¹ç¤¾äº¤åª’ä½“ä¸­çš„å¤šè¯­è¨€å†…å®¹æ²»ç†æä¾›äº†æ”¯æŒï¼Œä¹Ÿä¸ºå¥¥è«ç‰¹è¯­ç³»(Omotic Languages)çš„åç»­è®¡ç®—è¯­è¨€å­¦ç ”ç©¶å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.07998v1",
      "published_date": "2025-09-05 23:36:26 UTC",
      "updated_date": "2025-09-05 23:36:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:42:13.393389+00:00"
    },
    {
      "arxiv_id": "2509.05540v1",
      "title": "Combining TSL and LLM to Automate REST API Testing: A Comparative Study",
      "title_zh": "ç»“åˆ TSL ä¸ LLM å®ç° REST API è‡ªåŠ¨åŒ–æµ‹è¯•ï¼šä¸€é¡¹å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Thiago Barradas",
        "Aline Paes",
        "VÃ¢nia de Oliveira Neves"
      ],
      "abstract": "The effective execution of tests for REST APIs remains a considerable challenge for development teams, driven by the inherent complexity of distributed systems, the multitude of possible scenarios, and the limited time available for test design. Exhaustive testing of all input combinations is impractical, often resulting in undetected failures, high manual effort, and limited test coverage. To address these issues, we introduce RestTSLLM, an approach that uses Test Specification Language (TSL) in conjunction with Large Language Models (LLMs) to automate the generation of test cases for REST APIs. The approach targets two core challenges: the creation of test scenarios and the definition of appropriate input data. The proposed solution integrates prompt engineering techniques with an automated pipeline to evaluate various LLMs on their ability to generate tests from OpenAPI specifications. The evaluation focused on metrics such as success rate, test coverage, and mutation score, enabling a systematic comparison of model performance. The results indicate that the best-performing LLMs - Claude 3.5 Sonnet (Anthropic), Deepseek R1 (Deepseek), Qwen 2.5 32b (Alibaba), and Sabia 3 (Maritaca) - consistently produced robust and contextually coherent REST API tests. Among them, Claude 3.5 Sonnet outperformed all other models across every metric, emerging in this study as the most suitable model for this task. These findings highlight the potential of LLMs to automate the generation of tests based on API specifications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RestTSLLM æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆ Test Specification Language (TSL) å’Œ Large Language Models (LLMs) æ¥è‡ªåŠ¨åŒ– REST API æµ‹è¯•ç”¨ä¾‹çš„ç”Ÿæˆï¼Œæ—¨åœ¨è§£å†³åˆ†å¸ƒå¼ç³»ç»Ÿæµ‹è¯•ä¸­çš„é«˜å¤æ‚æ€§å’Œæ‰‹åŠ¨å·¥ä½œé‡å¤§çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é‡ç‚¹åº”å¯¹æµ‹è¯•åœºæ™¯åˆ›å»ºå’Œè¾“å…¥æ•°æ®å®šä¹‰ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼Œé€šè¿‡ Prompt Engineering æŠ€æœ¯å’Œè‡ªåŠ¨åŒ–æµæ°´çº¿ï¼Œåˆ©ç”¨ OpenAPI è§„èŒƒæŒ‡å¯¼ LLMs ç”Ÿæˆæµ‹è¯•ã€‚è¯„ä¼°è¿‡ç¨‹é‡‡ç”¨äº†æˆåŠŸç‡ã€æµ‹è¯•è¦†ç›–ç‡å’Œå˜å¼‚å¾—åˆ† (Mutation Score) ç­‰æŒ‡æ ‡ï¼Œå¯¹å¤šç§æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿæ€§æ¯”è¾ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒClaude 3.5 Sonnetã€Deepseek R1ã€Qwen 2.5 32b å’Œ Sabia 3 èƒ½å¤Ÿç”Ÿæˆé²æ£’ä¸”ä¸Šä¸‹æ–‡ä¸€è‡´çš„æµ‹è¯•ï¼Œå…¶ä¸­ Claude 3.5 Sonnet åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä¼˜ï¼Œè¢«è®¤ä¸ºæ˜¯è¯¥ä»»åŠ¡æœ€åˆé€‚çš„æ¨¡å‹ã€‚è¿™é¡¹ç ”ç©¶å±•ç¤ºäº†åˆ©ç”¨ LLMs æ ¹æ® API è§„èŒƒè‡ªåŠ¨ç”Ÿæˆæµ‹è¯•çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºæå‡ REST API æµ‹è¯•çš„è‡ªåŠ¨åŒ–ç¨‹åº¦æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "10 pages, article computer science, software engineering, software testing, ia, llm",
      "pdf_url": "https://arxiv.org/pdf/2509.05540v1",
      "published_date": "2025-09-05 23:32:35 UTC",
      "updated_date": "2025-09-05 23:32:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:42:23.386145+00:00"
    },
    {
      "arxiv_id": "2509.05513v1",
      "title": "OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation",
      "title_zh": "OpenEgoï¼šç”¨äºçµå·§æ“ä½œçš„å¤§è§„æ¨¡å¤šæ¨¡æ€ç¬¬ä¸€è§†è§’æ•°æ®é›†",
      "authors": [
        "Ahad Jawaid",
        "Yu Xiang"
      ],
      "abstract": "Egocentric human videos provide scalable demonstrations for imitation learning, but existing corpora often lack either fine-grained, temporally localized action descriptions or dexterous hand annotations. We introduce OpenEgo, a multimodal egocentric manipulation dataset with standardized hand-pose annotations and intention-aligned action primitives. OpenEgo totals 1107 hours across six public datasets, covering 290 manipulation tasks in 600+ environments. We unify hand-pose layouts and provide descriptive, timestamped action primitives. To validate its utility, we train language-conditioned imitation-learning policies to predict dexterous hand trajectories. OpenEgo is designed to lower the barrier to learning dexterous manipulation from egocentric video and to support reproducible research in vision-language-action learning. All resources and instructions will be released at www.openegocentric.com.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† OpenEgoï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºçµå·§æ“ä½œ(Dexterous Manipulation)çš„å¤§è§„æ¨¡å¤šæ¨¡æ€ç¬¬ä¸€è§†è§’(Egocentric)æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¯­æ–™åº“ç¼ºä¹ç»†ç²’åº¦åŠ¨ä½œæè¿°æˆ–çµå·§æ‰‹éƒ¨æ ‡æ³¨çš„é—®é¢˜ã€‚OpenEgo æ•´åˆäº†å…­ä¸ªå…¬å¼€æ•°æ®é›†ï¼Œæ¶µç›–äº† 600 å¤šä¸ªç¯å¢ƒä¸­çš„ 290 é¡¹æ“ä½œä»»åŠ¡ï¼Œæ€»æ—¶é•¿è¾¾ 1107 å°æ—¶ã€‚ç ”ç©¶å›¢é˜Ÿç»Ÿä¸€äº†æ‰‹éƒ¨å§¿æ€å¸ƒå±€ï¼Œå¹¶æä¾›äº†å¸¦æœ‰æ—¶é—´æˆ³çš„æè¿°æ€§åŠ¨ä½œåŸè¯­(Action Primitives)ã€‚ä¸ºäº†éªŒè¯å…¶æœ‰æ•ˆæ€§ï¼Œç ”ç©¶è€…è®­ç»ƒäº†ä»¥è¯­è¨€ä¸ºæ¡ä»¶çš„æ¨¡ä»¿å­¦ä¹ (Imitation Learning)ç­–ç•¥æ¥é¢„æµ‹çµå·§çš„æ‰‹éƒ¨è¿åŠ¨è½¨è¿¹ã€‚OpenEgo çš„è®¾è®¡æ—¨åœ¨é™ä½ä»ç¬¬ä¸€è§†è§’è§†é¢‘å­¦ä¹ çµå·§æ“ä½œçš„é—¨æ§›ï¼Œå¹¶ä¸ºè§†è§‰-è¯­è¨€-åŠ¨ä½œ(Vision-Language-Action)å­¦ä¹ çš„å¯é‡å¤æ€§ç ”ç©¶æä¾›æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "4 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2509.05513v1",
      "published_date": "2025-09-05 21:47:55 UTC",
      "updated_date": "2025-09-05 21:47:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:42:16.291085+00:00"
    },
    {
      "arxiv_id": "2509.09707v1",
      "title": "LLM-Based Instance-Driven Heuristic Bias In the Context of a Biased Random Key Genetic Algorithm",
      "title_zh": "æœ‰åéšæœºé”®é—ä¼ ç®—æ³•ä¸­åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å®ä¾‹é©±åŠ¨å¯å‘å¼åç½®",
      "authors": [
        "Camilo ChacÃ³n Sartori",
        "MartÃ­n Isla Pino",
        "Pedro Pinacho-Davidson",
        "Christian Blum"
      ],
      "abstract": "Integrating Large Language Models (LLMs) within metaheuristics opens a novel path for solving complex combinatorial optimization problems. While most existing approaches leverage LLMs for code generation to create or refine specific heuristics, they often overlook the structural properties of individual problem instances. In this work, we introduce a novel framework that integrates LLMs with a Biased Random-Key Genetic Algorithm (BRKGA) to solve the NP-hard Longest Run Subsequence problem. Our approach extends the instance-driven heuristic bias paradigm by introducing a human-LLM collaborative process to co-design and implement a set of computationally efficient metrics. The LLM analyzes these instance-specific metrics to generate a tailored heuristic bias, which steers the BRKGA toward promising areas of the search space. We conduct a comprehensive experimental evaluation, including rigorous statistical tests, convergence and behavioral analyses, and targeted ablation studies, comparing our method against a standard BRKGA baseline across 1,050 generated instances of varying complexity. Results show that our top-performing hybrid, BRKGA+Llama-4-Maverick, achieves statistically significant improvements over the baseline, particularly on the most complex instances. Our findings confirm that leveraging an LLM to produce an a priori, instance-driven heuristic bias is a valuable approach for enhancing metaheuristics in complex optimization domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå°†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸æœ‰åéšæœºé”®é—ä¼ ç®—æ³•(Biased Random-Key Genetic Algorithm, BRKGA)ç›¸ç»“åˆçš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³NPéš¾çš„Longest Run Subsequenceé—®é¢˜ã€‚è¯¥æ¡†æ¶æ‰©å±•äº†å®ä¾‹é©±åŠ¨å¯å‘å¼åç½®(instance-driven heuristic bias)èŒƒå¼ï¼Œé€šè¿‡äººç±»ä¸LLMçš„åä½œè¿‡ç¨‹å…±åŒè®¾è®¡å¹¶å®ç°äº†ä¸€ç³»åˆ—é«˜æ•ˆçš„è®¡ç®—æŒ‡æ ‡ã€‚LLMé€šè¿‡åˆ†æè¿™äº›ç‰¹å®šçš„å®ä¾‹æŒ‡æ ‡æ¥ç”Ÿæˆå®šåˆ¶åŒ–çš„å¯å‘å¼åç½®ï¼Œä»è€Œå¼•å¯¼BRKGAåœ¨æœç´¢ç©ºé—´ä¸­å‘æ›´æœ‰å‰æ™¯çš„åŒºåŸŸç§»åŠ¨ã€‚ç ”ç©¶åœ¨1,050ä¸ªä¸åŒå¤æ‚åº¦çš„å®ä¾‹ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºæ··åˆæ¨¡å‹BRKGA+Llama-4-Maverickæ¯”åŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚å®ä¾‹æ—¶ä¼˜åŠ¿æ˜æ˜¾ã€‚è¯¥ç ”ç©¶è¯å®äº†åˆ©ç”¨LLMç”Ÿæˆå…ˆéªŒçš„å®ä¾‹é©±åŠ¨å¯å‘å¼åç½®æ˜¯å¢å¼ºå¤æ‚ä¼˜åŒ–é¢†åŸŸå…ƒå¯å‘å¼ç®—æ³•(metaheuristics)çš„ä¸€ç§æå…·ä»·å€¼çš„æ–¹æ³•ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.NE",
      "comment": "Submitted to a journal for review",
      "pdf_url": "https://arxiv.org/pdf/2509.09707v1",
      "published_date": "2025-09-05 21:46:41 UTC",
      "updated_date": "2025-09-05 21:46:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:42:28.101387+00:00"
    },
    {
      "arxiv_id": "2509.09706v1",
      "title": "Differential Robustness in Transformer Language Models: Empirical Evaluation Under Adversarial Text Attacks",
      "title_zh": "Transformer è¯­è¨€æ¨¡å‹çš„å·®å¼‚åŒ–é²æ£’æ€§ï¼šå¯¹æŠ—æ€§æ–‡æœ¬æ”»å‡»ä¸‹çš„å®è¯è¯„ä¼°",
      "authors": [
        "Taniya Gidatkar",
        "Oluwaseun Ajao",
        "Matthew Shardlow"
      ],
      "abstract": "This study evaluates the resilience of large language models (LLMs) against adversarial attacks, specifically focusing on Flan-T5, BERT, and RoBERTa-Base. Using systematically designed adversarial tests through TextFooler and BERTAttack, we found significant variations in model robustness. RoBERTa-Base and FlanT5 demonstrated remarkable resilience, maintaining accuracy even when subjected to sophisticated attacks, with attack success rates of 0%. In contrast. BERT-Base showed considerable vulnerability, with TextFooler achieving a 93.75% success rate in reducing model accuracy from 48% to just 3%. Our research reveals that while certain LLMs have developed effective defensive mechanisms, these safeguards often require substantial computational resources. This study contributes to the understanding of LLM security by identifying existing strengths and weaknesses in current safeguarding approaches and proposes practical recommendations for developing more efficient and effective defensive strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº† Flan-T5ã€BERT å’Œ RoBERTa-Base ç­‰å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é¢å¯¹å¯¹æŠ—æ€§æ”»å‡» (Adversarial Attacks) æ—¶çš„å·®å¼‚åŒ–é²æ£’æ€§ã€‚é€šè¿‡ TextFooler å’Œ BERTAttack ç³»ç»Ÿæ€§åœ°è®¾è®¡çš„å¯¹æŠ—æ€§æµ‹è¯•å‘ç°ï¼ŒRoBERTa-Base å’Œ Flan-T5 è¡¨ç°å‡ºå“è¶Šçš„éŸ§æ€§ï¼Œåœ¨å¤æ‚æ”»å‡»ä¸‹ä»èƒ½ä¿æŒå‡†ç¡®ç‡ï¼Œæ”»å‡»æˆåŠŸç‡ (Attack Success Rate) ä¸º 0%ã€‚ä¸ä¹‹å½¢æˆé²œæ˜å¯¹æ¯”çš„æ˜¯ï¼ŒBERT-Base å±•ç°å‡ºæ˜¾è‘—çš„è„†å¼±æ€§ï¼Œå…¶å‡†ç¡®ç‡åœ¨ TextFooler æ”»å‡»ä¸‹ä» 48% éª¤é™è‡³ 3%ï¼ŒæˆåŠŸç‡é«˜è¾¾ 93.75%ã€‚ç ”ç©¶æ­ç¤ºäº†è™½ç„¶éƒ¨åˆ†æ¨¡å‹å·²å…·å¤‡æœ‰æ•ˆé˜²å¾¡æœºåˆ¶ï¼Œä½†è¿™äº›é˜²æŠ¤æ‰‹æ®µå¾€å¾€ä¾èµ–å¤§é‡çš„è®¡ç®—èµ„æºã€‚è¯¥å·¥ä½œé€šè¿‡è¯†åˆ«ç°æœ‰é˜²æŠ¤æŠ€æœ¯çš„ä¼˜åŠ£ï¼Œä¸ºæœªæ¥å¼€å‘æ›´é«˜æ•ˆã€æ›´å®ç”¨çš„é˜²å¾¡ç­–ç•¥æä¾›äº†å…³é”®è§è§£å’Œå®è·µå»ºè®®ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "8 pages, 4 tables, to appear in proceedings of Recent Advances in Natural Language Processing (RANLP 2025) and ACL Anthology",
      "pdf_url": "https://arxiv.org/pdf/2509.09706v1",
      "published_date": "2025-09-05 21:43:06 UTC",
      "updated_date": "2025-09-05 21:43:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:42:30.994427+00:00"
    },
    {
      "arxiv_id": "2509.05500v1",
      "title": "Microrobot Vascular Parkour: Analytic Geometry-based Path Planning with Real-time Dynamic Obstacle Avoidance",
      "title_zh": "å¾®å‹æœºå™¨äººè¡€ç®¡è·‘é…·ï¼šåŸºäºè§£æå‡ ä½•çš„è·¯å¾„è§„åˆ’ä¸å®æ—¶åŠ¨æ€é¿éšœ",
      "authors": [
        "Yanda Yang",
        "Max Sokolich",
        "Fatma Ceren Kirmizitas",
        "Sambeeta Das",
        "Andreas A. Malikopoulos"
      ],
      "abstract": "Autonomous microrobots in blood vessels could enable minimally invasive therapies, but navigation is challenged by dense, moving obstacles. We propose a real-time path planning framework that couples an analytic geometry global planner (AGP) with two reactive local escape controllers, one based on rules and one based on reinforcement learning, to handle sudden moving obstacles. Using real-time imaging, the system estimates the positions of the microrobot, obstacles, and targets and computes collision-free motions. In simulation, AGP yields shorter paths and faster planning than weighted A* (WA*), particle swarm optimization (PSO), and rapidly exploring random trees (RRT), while maintaining feasibility and determinism. We extend AGP from 2D to 3D without loss of speed. In both simulations and experiments, the combined global planner and local controllers reliably avoid moving obstacles and reach targets. The average planning time is 40 ms per frame, compatible with 25 fps image acquisition and real-time closed-loop control. These results advance autonomous microrobot navigation and targeted drug delivery in vascular environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¡€ç®¡ç¯å¢ƒå†…è‡ªä¸»å¾®å‹æœºå™¨äºº(Microrobots)å¯¼èˆªé¢ä¸´çš„å¯†é›†åŠ¨æ€éšœç¢ç‰©æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè§£æå‡ ä½•å…¨å±€è§„åˆ’å™¨(Analytic Geometry Global Planner, AGP)çš„å®æ—¶è·¯å¾„è§„åˆ’æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è€¦åˆAGPä¸åŸºäºè§„åˆ™åŠå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)çš„ä¸¤ç§ååº”å¼å±€éƒ¨é¿éšœæ§åˆ¶å™¨ï¼Œåˆ©ç”¨å®æ—¶æˆåƒæŠ€æœ¯è®¡ç®—æ— ç¢°æ’è¿åŠ¨ä»¥åº”å¯¹çªå‘éšœç¢ã€‚ä»¿çœŸå®éªŒè¡¨æ˜ï¼Œä¸åŠ æƒA*(WA*)ã€ç²’å­ç¾¤ç®—æ³•(PSO)å’Œå¿«é€Ÿæ‰©å±•éšæœºæ ‘(RRT)ç­‰ç®—æ³•ç›¸æ¯”ï¼ŒAGPåœ¨ä¿è¯ç¡®å®šæ€§çš„åŒæ—¶å…·æœ‰æ›´çŸ­çš„è·¯å¾„å’Œæ›´å¿«çš„è§„åˆ’é€Ÿåº¦ï¼Œå¹¶èƒ½æ— æŸæ‰©å±•è‡³ä¸‰ç»´ç©ºé—´ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¯¥ç³»ç»Ÿå®ç°äº†å¹³å‡æ¯å¸§40æ¯«ç§’çš„è§„åˆ’æ—¶é—´ï¼Œèƒ½å¤Ÿæ»¡è¶³25 fpså›¾åƒé‡‡é›†ä¸‹çš„å®æ—¶é—­ç¯æ§åˆ¶è¦æ±‚ã€‚è¿™é¡¹æˆæœæ˜¾è‘—æ¨è¿›äº†è¡€ç®¡ç¯å¢ƒä¸‹è‡ªä¸»å¾®å‹æœºå™¨äººçš„ç²¾ç¡®å¯¼èˆªä»¥åŠé¶å‘è¯ç‰©è¾“é€(Targeted drug delivery)æŠ€æœ¯çš„å‘å±•ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "56 pages, 19 figures including Supplementary Materials. Supplementary videos available at https://robotyyd.github.io/yanda-yang.github.io/vascular-parkour.html. Preprint. This version has not been peer reviewed",
      "pdf_url": "https://arxiv.org/pdf/2509.05500v1",
      "published_date": "2025-09-05 21:22:37 UTC",
      "updated_date": "2025-09-05 21:22:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:42:39.494286+00:00"
    },
    {
      "arxiv_id": "2509.05490v1",
      "title": "An Analysis of Layer-Freezing Strategies for Enhanced Transfer Learning in YOLO Architectures",
      "title_zh": "YOLO æ¶æ„ä¸‹å¢å¼ºè¿ç§»å­¦ä¹ çš„å±‚å†»ç»“ç­–ç•¥åˆ†æ",
      "authors": [
        "Andrzej D. Dobrzycki",
        "Ana M. Bernardos",
        "JosÃ© R. Casar"
      ],
      "abstract": "The You Only Look Once (YOLO) architecture is crucial for real-time object detection. However, deploying it in resource-constrained environments such as unmanned aerial vehicles (UAVs) requires efficient transfer learning. Although layer freezing is a common technique, the specific impact of various freezing configurations on contemporary YOLOv8 and YOLOv10 architectures remains unexplored, particularly with regard to the interplay between freezing depth, dataset characteristics, and training dynamics. This research addresses this gap by presenting a detailed analysis of layer-freezing strategies. We systematically investigate multiple freezing configurations across YOLOv8 and YOLOv10 variants using four challenging datasets that represent critical infrastructure monitoring. Our methodology integrates a gradient behavior analysis (L2 norm) and visual explanations (Grad-CAM) to provide deeper insights into training dynamics under different freezing strategies. Our results reveal that there is no universal optimal freezing strategy but, rather, one that depends on the properties of the data. For example, freezing the backbone is effective for preserving general-purpose features, while a shallower freeze is better suited to handling extreme class imbalance. These configurations reduce graphics processing unit (GPU) memory consumption by up to 28% compared to full fine-tuning and, in some cases, achieve mean average precision (mAP@50) scores that surpass those of full fine-tuning. Gradient analysis corroborates these findings, showing distinct convergence patterns for moderately frozen models. Ultimately, this work provides empirical findings and practical guidelines for selecting freezing strategies. It offers a practical, evidence-based approach to balanced transfer learning for object detection in scenarios with limited resources.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ YOLOv8 å’Œ YOLOv10 æ¶æ„åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„ Transfer Learning æ•ˆç‡ï¼Œç³»ç»Ÿåˆ†æäº† Layer-Freezing ç­–ç•¥çš„å®é™…å½±å“ã€‚é€šè¿‡åœ¨å››ä¸ªå…³é”®åŸºç¡€è®¾æ–½ç›‘æ§æ•°æ®é›†ä¸Šå¼€å±•å®éªŒï¼Œç ”ç©¶ç»“åˆäº†æ¢¯åº¦è¡Œä¸ºåˆ†æï¼ˆL2 normï¼‰å’Œ Grad-CAM å¯è§†åŒ–æŠ€æœ¯ï¼Œæ­ç¤ºäº†å†»ç»“æ·±åº¦ã€æ•°æ®ç‰¹æ€§ä¸è®­ç»ƒåŠ¨æ€ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚ç ”ç©¶å‘ç°ï¼Œæœ€ä½³çš„å†»ç»“ç­–ç•¥å¹¶éä¸€æˆä¸å˜ï¼Œè€Œæ˜¯ç”±æ•°æ®é›†çš„ç‰¹å¾å†³å®šï¼Œä¾‹å¦‚å†»ç»“ Backbone æœ‰åˆ©äºä¿ç•™é€šç”¨ç‰¹å¾ï¼Œè€Œæµ…å±‚å†»ç»“åœ¨åº”å¯¹ä¸¥é‡ Class Imbalance æ—¶è¡¨ç°æ›´ä½³ã€‚å®éªŒè¯æ˜ï¼Œè¯¥ç­–ç•¥åœ¨é™ä½é«˜è¾¾ 28% çš„ GPU æ˜¾å­˜å ç”¨çš„åŒæ—¶ï¼Œå…¶ mAP@50 è¯„åˆ†åœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³èƒ½è¶…è¿‡ Full Fine-tuningã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹å·¥ä½œä¸ºèµ„æºå—é™åœºæ™¯ä¸‹çš„ç›®æ ‡æ£€æµ‹æä¾›äº†å®è¯ä¾æ®å’Œå®è·µæŒ‡å—ï¼Œä¸ºå®ç°æ›´å¹³è¡¡çš„è¿ç§»å­¦ä¹ æ•ˆæœæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "31 pages, 14 figures, 9 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.05490v1",
      "published_date": "2025-09-05 20:39:43 UTC",
      "updated_date": "2025-09-05 20:39:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:42:43.294044+00:00"
    },
    {
      "arxiv_id": "2509.05488v1",
      "title": "MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs",
      "title_zh": "MambaLite-Microï¼šé¢å‘å¾®æ§åˆ¶å™¨çš„å†…å­˜ä¼˜åŒ–å‹ Mamba æ¨ç†",
      "authors": [
        "Hongjun Xu",
        "Junxi Xia",
        "Weisi Yang",
        "Yueyuan Sui",
        "Stephen Xia"
      ],
      "abstract": "Deploying Mamba models on microcontrollers (MCUs) remains challenging due to limited memory, the lack of native operator support, and the absence of embedded-friendly toolchains. We present, to our knowledge, the first deployment of a Mamba-based neural architecture on a resource-constrained MCU, a fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline maps a trained PyTorch Mamba model to on-device execution by (1) exporting model weights into a lightweight format, and (2) implementing a handcrafted Mamba layer and supporting operators in C with operator fusion and memory layout optimization. MambaLite-Micro eliminates large intermediate tensors, reducing 83.0% peak memory, while maintaining an average numerical error of only 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on keyword spotting(KWS) and human activity recognition (HAR) tasks, MambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully preserving classification accuracy. We further validated portability by deploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating consistent operation across heterogeneous embedded platforms and paving the way for bringing advanced sequence models like Mamba to real-world resource-constrained applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨å¾®æ§åˆ¶å™¨(MCU)ä¸Šéƒ¨ç½² Mamba æ¨¡å‹é¢ä¸´çš„å†…å­˜é™åˆ¶å’Œå·¥å…·é“¾ç¼ºå¤±ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†é¦–ä¸ªåŸºäº C è¯­è¨€çš„å…è¿è¡Œæ—¶æ¨ç†å¼•æ“ MambaLite-Microã€‚è¯¥å¼•æ“é€šè¿‡æƒé‡è½»é‡åŒ–å¯¼å‡ºã€æ‰‹å·¥ç¼–å†™ C è¯­è¨€ç®—å­ä»¥åŠç®—å­èåˆ(Operator Fusion)ä¸å†…å­˜å¸ƒå±€ä¼˜åŒ–(Memory Layout Optimization)æŠ€æœ¯ï¼ŒæˆåŠŸå°†å³°å€¼å†…å­˜é™ä½äº† 83.0%ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMambaLite-Micro åœ¨å…³é”®è¯æ£€æµ‹(KWS)å’Œäººä½“æ´»åŠ¨è¯†åˆ«(HAR)ä»»åŠ¡ä¸­å®ç°äº†ä¸ PyTorch åŸºå‡† 100% çš„ä¸€è‡´æ€§ï¼Œä¸”æ•°å€¼è¯¯å·®æä½ã€‚é€šè¿‡åœ¨ ESP32S3 å’Œ STM32H7 ç­‰å¼‚æ„ MCU å¹³å°ä¸Šçš„æˆåŠŸéªŒè¯ï¼Œè¯¥ç ”ç©¶è¯æ˜äº†å…¶å‡ºè‰²çš„å¯ç§»æ¤æ€§ã€‚è¯¥æˆæœä¸ºåœ¨çœŸå®ä¸–ç•Œèµ„æºå—é™çš„åµŒå…¥å¼è®¾å¤‡ä¸­åº”ç”¨ Mamba ç­‰é«˜çº§åºåˆ—æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.OS"
      ],
      "primary_category": "cs.LG",
      "comment": "4 pages, 1 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.05488v1",
      "published_date": "2025-09-05 20:34:06 UTC",
      "updated_date": "2025-09-05 20:34:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:42:44.298379+00:00"
    },
    {
      "arxiv_id": "2509.05486v1",
      "title": "The Token Tax: Systematic Bias in Multilingual Tokenization",
      "title_zh": "è¯å…ƒç¨ï¼šå¤šè¯­è¨€è¯å…ƒåŒ–ä¸­çš„ç³»ç»Ÿæ€§åè§",
      "authors": [
        "Jessica M. Lundin",
        "Ada Zhang",
        "Nihal Karim",
        "Hamza Louzan",
        "Victor Wei",
        "David Adelani",
        "Cody Carroll"
      ],
      "abstract": "Tokenization inefficiency imposes structural disadvantages on morphologically complex, low-resource languages, inflating compute resources and depressing accuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA items; 5 subjects; 16 African languages) and show that fertility (tokens/word) reliably predicts accuracy. Higher fertility consistently predicts lower accuracy across all models and subjects. We further find that reasoning models (DeepSeek, o1) consistently outperform non-reasoning peers across high and low resource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in prior generations. Finally, translating token inflation to economics, a doubling in tokens results in quadrupled training cost and time, underscoring the token tax faced by many languages. These results motivate morphologically aware tokenization, fair pricing, and multilingual benchmarks for equitable natural language processing (NLP).",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šè¯­è¨€åˆ†è¯(Tokenization)ä¸­çš„ç³»ç»Ÿæ€§åå·®ï¼ŒæŒ‡å‡ºåˆ†è¯æ•ˆç‡ä½ä¸‹ç»™å½¢æ€å¤æ‚ä¸”ä½èµ„æºçš„è¯­è¨€å¸¦æ¥äº†ç»“æ„æ€§åŠ£åŠ¿ã€‚ç ”ç©¶äººå‘˜åœ¨AfriMMLUåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†10ä¸ªå¤§è¯­è¨€æ¨¡å‹(LLMs)ï¼Œæ¶µç›–äº†16ç§éæ´²è¯­è¨€çš„9,000ä¸ªæµ‹è¯•é¡¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯å…ƒç‡(Fertility)èƒ½å¯é åœ°é¢„æµ‹æ¨¡å‹å‡†ç¡®ç‡ï¼Œè¾ƒé«˜çš„è¯å…ƒç‡åœ¨æ‰€æœ‰æ¨¡å‹å’Œä¸»é¢˜ä¸­å‡å¯¼è‡´äº†è¾ƒä½çš„å‡†ç¡®ç‡ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œæ¨ç†æ¨¡å‹(Reasoning models)å¦‚DeepSeekå’Œo1åœ¨AfriMMLUæ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºéæ¨ç†æ¨¡å‹ï¼Œç¼©å°äº†æ­¤å‰è§‚å¯Ÿåˆ°çš„å‡†ç¡®ç‡å·®è·ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ­ç¤ºäº†â€œè¯å…ƒç¨(Token Tax)â€çš„ç»æµä»£ä»·ï¼Œå³è¯å…ƒæ•°é‡ç¿»å€ä¼šå¯¼è‡´è®­ç»ƒæˆæœ¬å’Œæ—¶é—´å¢åŠ è‡³å››å€ã€‚è¯¥æˆæœå¼ºè°ƒäº†å¼€å‘å…·å¤‡å½¢æ€æ„ŸçŸ¥èƒ½åŠ›(Morphologically aware)çš„åˆ†è¯æŠ€æœ¯ã€åˆ¶å®šå…¬å¹³å®šä»·ç­–ç•¥ä»¥åŠå»ºç«‹å¤šè¯­è¨€åŸºå‡†å¯¹äºå®ç°å…¬å¹³è‡ªç„¶è¯­è¨€å¤„ç†(NLP)çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05486v1",
      "published_date": "2025-09-05 20:20:51 UTC",
      "updated_date": "2025-09-05 20:20:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:42:51.996512+00:00"
    },
    {
      "arxiv_id": "2509.21327v2",
      "title": "Assessment of deep learning models integrated with weather and environmental variables for wildfire spread prediction and a case study of the 2023 Maui fires",
      "title_zh": "èåˆæ°”è±¡ä¸ç¯å¢ƒå˜é‡çš„æ·±åº¦å­¦ä¹ é‡ç«è”“å»¶é¢„æµ‹æ¨¡å‹è¯„ä¼°åŠ 2023 å¹´ Maui ç«ç¾æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Jiyeon Kim",
        "Yingjie Hu",
        "Negar Elhami-Khorasani",
        "Kai Sun",
        "Ryan Zhenqi Zhou"
      ],
      "abstract": "Predicting the spread of wildfires is essential for effective fire management and risk assessment. With the fast advancements of artificial intelligence (AI), various deep learning models have been developed and utilized for wildfire spread prediction. However, there is limited understanding of the advantages and limitations of these models, and it is also unclear how deep learning-based fire spread models can be compared with existing non-AI fire models. In this work, we assess the ability of five typical deep learning models integrated with weather and environmental variables for wildfire spread prediction based on over ten years of wildfire data in the state of Hawaii. We further use the 2023 Maui fires as a case study to compare the best deep learning models with a widely-used fire spread model, FARSITE. The results show that two deep learning models, i.e., ConvLSTM and ConvLSTM with attention, perform the best among the five tested AI models. FARSITE shows higher precision, lower recall, and higher F1-score than the best AI models, while the AI models offer higher flexibility for the input data. By integrating AI models with an explainable AI method, we further identify important weather and environmental factors associated with the 2023 Maui wildfires.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†äº”ç§é›†æˆäº†å¤©æ°”å’Œç¯å¢ƒå˜é‡çš„æ·±åº¦å­¦ä¹ (Deep Learning)æ¨¡å‹åœ¨é‡ç«è”“å»¶é¢„æµ‹ä¸­çš„æ€§èƒ½ï¼Œå¹¶åŸºäºå¤å¨å¤·å·åä½™å¹´çš„ç«ç¾æ•°æ®è¿›è¡Œäº†æµ‹è¯•ã€‚ç ”ç©¶ä»¥2023å¹´æ¯›ä¼Šå²›(Maui)å¤§ç«ä¸ºæ¡ˆä¾‹ï¼Œå°†è¡¨ç°æœ€ä¼˜çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸ä¼ ç»Ÿç«ç¾è”“å»¶æ¨¡å‹FARSITEè¿›è¡Œäº†å¯¹æ¯”åˆ†æã€‚ç»“æœæ˜¾ç¤ºï¼ŒConvLSTMä»¥åŠå¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶(Attention)çš„ConvLSTMåœ¨å—è¯•AIæ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ã€‚è™½ç„¶FARSITEåœ¨ç²¾ç¡®ç‡(Precision)å’ŒF1åˆ†æ•°(F1-score)ä¸Šä¼˜äºAIæ¨¡å‹ï¼Œä½†AIæ¨¡å‹åœ¨è¾“å…¥æ•°æ®å¤„ç†ä¸Šå±•ç°å‡ºæ›´é«˜çš„çµæ´»æ€§ã€‚é€šè¿‡å¼•å…¥å¯è§£é‡Šäººå·¥æ™ºèƒ½(Explainable AI)æ–¹æ³•ï¼Œç ”ç©¶è¿›ä¸€æ­¥è¯†åˆ«äº†å¯¼è‡´2023å¹´æ¯›ä¼Šå²›ç«ç¾çš„å…³é”®å¤©æ°”å’Œç¯å¢ƒå› ç´ ã€‚è¯¥å·¥ä½œæ­ç¤ºäº†æ·±åº¦å­¦ä¹ åœ¨é‡ç«ç®¡ç†ä¸­çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥ç»“åˆç‰©ç†æ¨¡å‹ä¸AIæŠ€æœ¯çš„é¢„æµ‹æ–¹æ³•æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "physics.soc-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.soc-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21327v2",
      "published_date": "2025-09-05 20:20:19 UTC",
      "updated_date": "2025-11-22 02:48:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:42:57.865807+00:00"
    },
    {
      "arxiv_id": "2509.05478v1",
      "title": "PLanTS: Periodicity-aware Latent-state Representation Learning for Multivariate Time Series",
      "title_zh": "PLanTSï¼šé¢å‘å¤šå˜é‡æ—¶é—´åºåˆ—çš„å‘¨æœŸæ„ŸçŸ¥éšçŠ¶æ€è¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "Jia Wang",
        "Xiao Wang",
        "Chi Zhang"
      ],
      "abstract": "Multivariate time series (MTS) are ubiquitous in domains such as healthcare, climate science, and industrial monitoring, but their high dimensionality, limited labeled data, and non-stationary nature pose significant challenges for conventional machine learning methods. While recent self-supervised learning (SSL) approaches mitigate label scarcity by data augmentations or time point-based contrastive strategy, they neglect the intrinsic periodic structure of MTS and fail to capture the dynamic evolution of latent states. We propose PLanTS, a periodicity-aware self-supervised learning framework that explicitly models irregular latent states and their transitions. We first designed a period-aware multi-granularity patching mechanism and a generalized contrastive loss to preserve both instance-level and state-level similarities across multiple temporal resolutions. To further capture temporal dynamics, we design a next-transition prediction pretext task that encourages representations to encode predictive information about future state evolution. We evaluate PLanTS across a wide range of downstream tasks-including multi-class and multi-label classification, forecasting, trajectory tracking and anomaly detection. PLanTS consistently improves the representation quality over existing SSL methods and demonstrates superior runtime efficiency compared to DTW-based methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šå…ƒæ—¶é—´åºåˆ—ï¼ˆMultivariate Time Series, MTSï¼‰ä¸­é«˜ç»´åº¦ã€æ ‡æ³¨æ•°æ®ç¨€ç¼ºä»¥åŠéå¹³ç¨³æ€§å¸¦æ¥çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºç›®å‰çš„è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ–¹æ³•å¾€å¾€å¿½ç•¥äº†MTSå†…åœ¨çš„å‘¨æœŸç»“æ„ï¼Œä¸”æœªèƒ½æœ‰æ•ˆæ•æ‰æ½œçŠ¶æ€ï¼ˆlatent statesï¼‰çš„åŠ¨æ€æ¼”åŒ–ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†PLanTSï¼Œä¸€ç§å…·æœ‰å‘¨æœŸæ„ŸçŸ¥èƒ½åŠ›çš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æ˜¾å¼åœ°å¯¹ä¸è§„åˆ™çš„æ½œçŠ¶æ€åŠå…¶è½¬æ¢è¿›è¡Œå»ºæ¨¡ã€‚è¯¥æ¡†æ¶è®¾è®¡äº†å‘¨æœŸæ„ŸçŸ¥çš„å¤šç²’åº¦è¡¥ä¸æœºåˆ¶ï¼ˆperiod-aware multi-granularity patchingï¼‰å’Œå¹¿ä¹‰å¯¹æ¯”æŸå¤±ï¼ˆgeneralized contrastive lossï¼‰ï¼Œä»¥åœ¨å¤šç§æ—¶é—´åˆ†è¾¨ç‡ä¸‹ä¿ç•™å®ä¾‹çº§å’ŒçŠ¶æ€çº§çš„ç›¸ä¼¼æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ•æ‰æ—¶é—´åŠ¨æ€ï¼ŒPLanTS å¼•å…¥äº†ä¸‹ä¸€çŠ¶æ€è½¬æ¢é¢„æµ‹ï¼ˆnext-transition predictionï¼‰é¢„è®­ç»ƒä»»åŠ¡ï¼Œä½¿è¡¨å¾å‘é‡èƒ½å¤Ÿç¼–ç å…³äºæœªæ¥çŠ¶æ€æ¼”åŒ–çš„é¢„æµ‹ä¿¡æ¯ã€‚ç ”ç©¶åœ¨åˆ†ç±»ã€é¢„æµ‹ã€è½¨è¿¹è·Ÿè¸ªå’Œå¼‚å¸¸æ£€æµ‹ç­‰å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸ŠéªŒè¯äº†è¯¥æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPLanTS åœ¨è¡¨å¾è´¨é‡ä¸ŠæŒç»­ä¼˜äºç°æœ‰çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨è¿è¡Œæ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºåŸºäº DTW çš„æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05478v1",
      "published_date": "2025-09-05 20:10:09 UTC",
      "updated_date": "2025-09-05 20:10:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:43:14.707746+00:00"
    },
    {
      "arxiv_id": "2509.05475v1",
      "title": "Learning Tool-Aware Adaptive Compliant Control for Autonomous Regolith Excavation",
      "title_zh": "é¢å‘è‡ªä¸»æœˆå£¤æŒ–æ˜çš„å·¥å…·æ„ŸçŸ¥è‡ªé€‚åº”æŸ”é¡ºæ§åˆ¶å­¦ä¹ ",
      "authors": [
        "Andrej Orsula",
        "Matthieu Geist",
        "Miguel Olivares-Mendez",
        "Carol Martinez"
      ],
      "abstract": "Autonomous regolith excavation is a cornerstone of in-situ resource utilization for a sustained human presence beyond Earth. However, this task is fundamentally hindered by the complex interaction dynamics of granular media and the operational need for robots to use diverse tools. To address these challenges, this work introduces a framework where a model-based reinforcement learning agent learns within a parallelized simulation. This environment leverages high-fidelity particle physics and procedural generation to create a vast distribution of both lunar terrains and excavation tool geometries. To master this diversity, the agent learns an adaptive interaction strategy by dynamically modulating its own stiffness and damping at each control step through operational space control. Our experiments demonstrate that training with a procedural distribution of tools is critical for generalization and enables the development of sophisticated tool-aware behavior. Furthermore, we show that augmenting the agent with visual feedback significantly improves task success. These results represent a validated methodology for developing the robust and versatile autonomous systems required for the foundational tasks of future space missions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªä¸»æœˆå£¤æŒ–æ˜(Autonomous regolith excavation)ä¸­é¢—ç²’ä»‹è´¨åŠ¨åŠ›å­¦å¤æ‚åŠå·¥å…·å¤šæ ·åŒ–å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹å¼ºåŒ–å­¦ä¹ (model-based reinforcement learning)çš„è®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨å¹¶è¡ŒåŒ–æ¨¡æ‹Ÿç¯å¢ƒä¸­ä½¿ç”¨é«˜ä¿çœŸç²’å­ç‰©ç†å’Œç¨‹åºåŒ–ç”ŸæˆæŠ€æœ¯ï¼Œæ„å»ºäº†æ¶µç›–å¤šç§æœˆçƒåœ°å½¢å’Œå·¥å…·å‡ ä½•å½¢çŠ¶çš„è®­ç»ƒåˆ†å¸ƒã€‚æ™ºèƒ½ä½“é€šè¿‡æ“ä½œç©ºé—´æ§åˆ¶(operational space control)åœ¨æ¯ä¸ªæ§åˆ¶æ­¥åŠ¨æ€è°ƒèŠ‚è‡ªèº«çš„åˆšåº¦å’Œé˜»å°¼ï¼Œä»è€Œå­¦ä¹ åˆ°ä¸€ç§è‡ªé€‚åº”é¡ºåº”æ§åˆ¶ç­–ç•¥ã€‚å®éªŒç»“æœè¯æ˜ï¼Œåˆ©ç”¨ç¨‹åºåŒ–ç”Ÿæˆçš„å·¥å…·åˆ†å¸ƒè¿›è¡Œè®­ç»ƒå¯¹äºæå‡ç³»ç»Ÿçš„æ³›åŒ–èƒ½åŠ›è‡³å…³é‡è¦ï¼Œå¹¶èƒ½ä¿ƒä½¿æ™ºèƒ½ä½“æ¼”åŒ–å‡ºå¤æ‚çš„å·¥å…·æ„ŸçŸ¥è¡Œä¸º(tool-aware behavior)ã€‚æ­¤å¤–ï¼Œå¼•å…¥è§†è§‰åé¦ˆ(visual feedback)è¿›ä¸€æ­¥æ˜¾è‘—æé«˜äº†ä»»åŠ¡çš„æˆåŠŸç‡ã€‚è¯¥ç ”ç©¶ä¸ºæœªæ¥å¤ªç©ºä»»åŠ¡ä¸­å¼€å‘å¥å£®ä¸”é€šç”¨çš„è‡ªä¸»æŒ–æ˜ç³»ç»Ÿæä¾›äº†ä¸€å¥—ç»è¿‡éªŒè¯çš„æ–¹æ³•è®ºã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "The source code is available at https://github.com/AndrejOrsula/space_robotics_bench",
      "pdf_url": "https://arxiv.org/pdf/2509.05475v1",
      "published_date": "2025-09-05 20:09:28 UTC",
      "updated_date": "2025-09-05 20:09:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:43:15.375768+00:00"
    },
    {
      "arxiv_id": "2509.05474v5",
      "title": "From Vision to Validation: A Theory- and Data-Driven Construction of a GCC-Specific AI Adoption Index",
      "title_zh": "ä»æ„¿æ™¯åˆ°éªŒè¯ï¼šç†è®ºä¸æ•°æ®é©±åŠ¨çš„ GCC åŒºåŸŸäººå·¥æ™ºèƒ½é‡‡çº³æŒ‡æ•°æ„å»º",
      "authors": [
        "Mohammad Rashed Albous",
        "Abdel Latef Anouze"
      ],
      "abstract": "Artificial intelligence (AI) is rapidly transforming public-sector processes worldwide, yet standardized measures rarely address the unique drivers, governance models, and cultural nuances of the Gulf Cooperation Council (GCC) countries. This study employs a theory-driven foundation derived from an in-depth analysis of literature review and six National AI Strategies (NASs), coupled with a data-driven approach that utilizes a survey of 203 mid- and senior-level government employees and advanced statistical techniques (K-Means clustering, Principal Component Analysis, and Partial Least Squares Structural Equation Modeling). By combining policy insights with empirical evidence, the research develops and validates a novel AI Adoption Index specifically tailored to the GCC public sector. Findings indicate that robust technical infrastructure and clear policy mandates exert the strongest influence on successful AI implementations, overshadowing organizational readiness in early adoption stages. The combined model explains 70% of the variance in AI outcomes, suggesting that resource-rich environments and top-down policy directives can drive rapid but uneven technology uptake. By consolidating key dimensions (Technical Infrastructure (TI), Organizational Readiness (OR), and Governance Environment (GE)) into a single composite index, this study provides a holistic yet context-sensitive tool for benchmarking AI maturity. The index offers actionable guidance for policymakers seeking to harmonize large-scale deployments with ethical and regulatory standards. Beyond advancing academic discourse, these insights inform more strategic allocation of resources, cross-country cooperation, and capacity-building initiatives, thereby supporting sustained AI-driven transformation in the GCC region and beyond.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æµ·æ¹¾é˜¿æ‹‰ä¼¯å›½å®¶åˆä½œå§”å‘˜ä¼š(GCC)å…¬å…±éƒ¨é—¨çš„ç‰¹æ®Šæ€§ï¼Œé€šè¿‡ç†è®ºä¸æ•°æ®é©±åŠ¨çš„æ–¹æ³•æ„å»ºå¹¶éªŒè¯äº†ä¸€ä¸ªä¸“é—¨çš„AI Adoption Indexã€‚ç ”ç©¶ç»“åˆäº†å¯¹6é¡¹National AI Strategies (NASs)çš„æ·±å…¥åˆ†æä»¥åŠå¯¹203åæ”¿åºœä¸­é«˜çº§é›‡å‘˜çš„é—®å·è°ƒæŸ¥ï¼Œå¹¶åº”ç”¨äº†K-Means clusteringã€Principal Component Analysis (PCA)å’ŒPartial Least Squares Structural Equation Modeling (PLS-SEM)ç­‰ç»Ÿè®¡æŠ€æœ¯ã€‚å‘ç°æ˜¾ç¤ºï¼ŒTechnical Infrastructure (TI)å’Œæ¸…æ™°çš„æ”¿ç­–æˆæƒå¯¹AIçš„æˆåŠŸå®æ–½å½±å“æœ€ä¸ºæ˜¾è‘—ï¼Œåœ¨æ—©æœŸé˜¶æ®µå…¶é‡è¦æ€§è¶…è¿‡äº†Organizational Readiness (OR)ã€‚è¯¥ç»„åˆæ¨¡å‹è§£é‡Šäº†70%çš„AIäº§å‡ºå·®å¼‚ï¼Œæ­ç¤ºäº†èµ„æºä¸°å¯Œç¯å¢ƒä¸è‡ªä¸Šè€Œä¸‹æ”¿ç­–å¯¼å‘å¯¹æŠ€æœ¯å¿«é€Ÿæ™®åŠçš„é©±åŠ¨ä½œç”¨ã€‚é€šè¿‡æ•´åˆTIã€ORå’ŒGovernance Environment (GE)ä¸‰ä¸ªå…³é”®ç»´åº¦ï¼Œè¯¥æŒ‡æ•°ä¸ºè¯„ä¼°AIæˆç†Ÿåº¦æä¾›äº†ä¸€ä¸ªå…¨é¢ä¸”å…·èƒŒæ™¯æ•æ„Ÿæ€§çš„åŸºå‡†å·¥å…·ã€‚è¿™ä¸€æˆæœä¸ä»…æ¨è¿›äº†ç›¸å…³é¢†åŸŸçš„å­¦æœ¯è®¨è®ºï¼Œè¿˜ä¸ºGCCåœ°åŒºçš„èµ„æºæˆ˜ç•¥åˆ†é…ã€è·¨å¢ƒåˆä½œåŠèƒ½åŠ›å»ºè®¾æä¾›äº†åˆ‡å®å¯è¡Œçš„æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "38 pages, 8 figures, 17 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.05474v5",
      "published_date": "2025-09-05 20:06:57 UTC",
      "updated_date": "2025-11-26 20:20:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:43:29.377308+00:00"
    },
    {
      "arxiv_id": "2509.05471v1",
      "title": "Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language Models",
      "title_zh": "é¢å…·ä¹‹ä¸‹ï¼šå¤§è¯­è¨€æ¨¡å‹ä¸­ä¼ªè£…è¶Šç‹±çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Youjia Zheng",
        "Mohammad Zandsalimy",
        "Shanu Sushmita"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly vulnerable to a sophisticated form of adversarial prompting known as camouflaged jailbreaking. This method embeds malicious intent within seemingly benign language to evade existing safety mechanisms. Unlike overt attacks, these subtle prompts exploit contextual ambiguity and the flexible nature of language, posing significant challenges to current defense systems. This paper investigates the construction and impact of camouflaged jailbreak prompts, emphasizing their deceptive characteristics and the limitations of traditional keyword-based detection methods. We introduce a novel benchmark dataset, Camouflaged Jailbreak Prompts, containing 500 curated examples (400 harmful and 100 benign prompts) designed to rigorously stress-test LLM safety protocols. In addition, we propose a multi-faceted evaluation framework that measures harmfulness across seven dimensions: Safety Awareness, Technical Feasibility, Implementation Safeguards, Harmful Potential, Educational Value, Content Quality, and Compliance Score. Our findings reveal a stark contrast in LLM behavior: while models demonstrate high safety and content quality with benign inputs, they exhibit a significant decline in performance and safety when confronted with camouflaged jailbreak attempts. This disparity underscores a pervasive vulnerability, highlighting the urgent need for more nuanced and adaptive security strategies to ensure the responsible and robust deployment of LLMs in real-world applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)é¢ä¸´çš„ä¸€ç§è¢«ç§°ä¸ºä¼ªè£…è¶Šç‹±(Camouflaged Jailbreaking)çš„å¤æ‚å¯¹æŠ—æ€§æç¤ºæ”»å‡»ï¼Œè¿™ç±»æ”»å‡»é€šè¿‡å°†æ¶æ„æ„å›¾åµŒå…¥çœ‹ä¼¼æ— å®³çš„è¯­è¨€ä¸­æ¥è§„é¿ç°æœ‰çš„å®‰å…¨æœºåˆ¶ã€‚ä½œè€…æ·±å…¥è°ƒæŸ¥äº†è¿™äº›éšè”½æ”»å‡»çš„æ„å»ºæ–¹å¼åŠä¼ ç»Ÿå…³é”®è¯æ£€æµ‹æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªåŒ…å«500ä¸ªç²¾é€‰ç¤ºä¾‹çš„æ–°å‹åŸºå‡†æ•°æ®é›† Camouflaged Jailbreak Promptsã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ä¸ªå¤šé¢è¯„ä¼°æ¡†æ¶ï¼Œä»å®‰å…¨æ„è¯†(Safety Awareness)ã€æœ‰å®³æ½œåŠ›(Harmful Potential)å’Œåˆè§„å¾—åˆ†(Compliance Score)ç­‰ä¸ƒä¸ªç»´åº¦è¡¡é‡æ”»å‡»çš„å½±å“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨é¢å¯¹ä¼ªè£…è¶Šç‹±å°è¯•æ—¶å®‰å…¨æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œæ­ç¤ºäº†æ¨¡å‹æ™®éå­˜åœ¨çš„è„†å¼±æ€§ã€‚è¯¥ç ”ç©¶ç»“æœå¼ºè°ƒäº†å¼€å‘æ›´å…·æ•é”æ€§å’Œè‡ªé€‚åº”æ€§å®‰å…¨ç­–ç•¥çš„è¿«åˆ‡æ€§ï¼Œä»¥ç¡®ä¿LLMsåœ¨å®é™…åº”ç”¨ä¸­çš„å¯é éƒ¨ç½²ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05471v1",
      "published_date": "2025-09-05 19:57:38 UTC",
      "updated_date": "2025-09-05 19:57:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:43:23.658133+00:00"
    },
    {
      "arxiv_id": "2509.05469v1",
      "title": "From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation",
      "title_zh": "ä»å›¾åƒç”Ÿæˆåˆ°åŸºç¡€è®¾æ–½è®¾è®¡ï¼šé¢å‘è¡—é“è®¾è®¡ç”Ÿæˆçš„å¤šæ™ºèƒ½ä½“æµæ°´çº¿",
      "authors": [
        "Chenguang Wang",
        "Xiang Yan",
        "Yilong Dai",
        "Ziyi Wang",
        "Susu Xu"
      ],
      "abstract": "Realistic visual renderings of street-design scenarios are essential for public engagement in active transportation planning. Traditional approaches are labor-intensive, hindering collective deliberation and collaborative decision-making. While AI-assisted generative design shows transformative potential by enabling rapid creation of design scenarios, existing generative approaches typically require large amounts of domain-specific training data and struggle to enable precise spatial variations of design/configuration in complex street-view scenes. We introduce a multi-agent system that edits and redesigns bicycle facilities directly on real-world street-view imagery. The framework integrates lane localization, prompt optimization, design generation, and automated evaluation to synthesize realistic, contextually appropriate designs. Experiments across diverse urban scenarios demonstrate that the system can adapt to varying road geometries and environmental conditions, consistently yielding visually coherent and instruction-compliant results. This work establishes a foundation for applying multi-agent pipelines to transportation infrastructure planning and facility design.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“æµæ°´çº¿ (Multi-agent Pipeline)ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè¡—é“è®¾è®¡å¯è§†åŒ–æ–¹æ³•åŠ³åŠ¨å¼ºåº¦å¤§ä»¥åŠç°æœ‰ AI ç”ŸæˆæŠ€æœ¯åœ¨å¤æ‚åœºæ™¯ä¸‹ç©ºé—´æ§åˆ¶ç²¾åº¦ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶æ•´åˆäº†è½¦é“å®šä½ (lane localization)ã€æç¤ºè¯ä¼˜åŒ– (prompt optimization)ã€è®¾è®¡ç”Ÿæˆ (design generation) å’Œè‡ªåŠ¨è¯„ä¼° (automated evaluation) ç­‰æ ¸å¿ƒæ¨¡å—ï¼Œèƒ½å¤Ÿç›´æ¥åœ¨çœŸå®ä¸–ç•Œçš„è¡—æ™¯å›¾åƒä¸Šè¿›è¡Œè‡ªè¡Œè½¦è®¾æ–½çš„ç¼–è¾‘ä¸é‡æ–°è®¾è®¡ã€‚ç›¸æ¯”äºä¼ ç»Ÿçš„ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥ç³»ç»Ÿå…‹æœäº†å¯¹å¤§è§„æ¨¡ç‰¹å®šé¢†åŸŸè®­ç»ƒæ•°æ®çš„ä¾èµ–ï¼Œå®ç°äº†å¯¹å¤æ‚è¡—æ™¯ä¸­è®¾è®¡é…ç½®çš„ç²¾å‡†ç©ºé—´è°ƒæ§ã€‚åœ¨å¤šç§åŸå¸‚åœºæ™¯ä¸‹çš„å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿé€‚åº”ä¸åŒçš„é“è·¯å‡ ä½•å½¢çŠ¶ (road geometries) å’Œç¯å¢ƒæ¡ä»¶ï¼Œå¹¶ç”Ÿæˆè§†è§‰è¿è´¯ä¸”ç¬¦åˆè®¾è®¡æŒ‡ä»¤çš„æ–¹æ¡ˆã€‚è¯¥å·¥ä½œä¸ä»…ä¸ºäº¤é€šè§„åˆ’ä¸­çš„é›†ä½“å®¡è®®æä¾›äº†é«˜æ•ˆçš„å¯è§†åŒ–å·¥å…·ï¼Œä¹Ÿä¸ºå°†å¤šæ™ºèƒ½ä½“æµæ°´çº¿åº”ç”¨äºäº¤é€šåŸºç¡€è®¾æ–½è§„åˆ’å’Œè®¾æ–½è®¾è®¡å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.05469v1",
      "published_date": "2025-09-05 19:49:36 UTC",
      "updated_date": "2025-09-05 19:49:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:43:24.458218+00:00"
    },
    {
      "arxiv_id": "2509.05449v1",
      "title": "Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis",
      "title_zh": "Neural Breadcrumbsï¼šåŸºäºéšè—çŠ¶æ€ä¸æ³¨æ„åŠ›æ¨¡å¼åˆ†æçš„å¤§è¯­è¨€æ¨¡å‹æˆå‘˜æ¨ç†æ”»å‡»",
      "authors": [
        "Disha Makhija",
        "Manoj Ghuhan Arivazhagan",
        "Vinayshekhar Bannihatti Kumar",
        "Rashmi Gangadharaiah"
      ],
      "abstract": "Membership inference attacks (MIAs) reveal whether specific data was used to train machine learning models, serving as important tools for privacy auditing and compliance assessment. Recent studies have reported that MIAs perform only marginally better than random guessing against large language models, suggesting that modern pre-training approaches with massive datasets may be free from privacy leakage risks. Our work offers a complementary perspective to these findings by exploring how examining LLMs' internal representations, rather than just their outputs, may provide additional insights into potential membership inference signals. Our framework, \\emph{memTrace}, follows what we call \\enquote{neural breadcrumbs} extracting informative signals from transformer hidden states and attention patterns as they process candidate sequences. By analyzing layer-wise representation dynamics, attention distribution characteristics, and cross-layer transition patterns, we detect potential memorization fingerprints that traditional loss-based approaches may not capture. This approach yields strong membership detection across several model families achieving average AUC scores of 0.85 on popular MIA benchmarks. Our findings suggest that internal model behaviors can reveal aspects of training data exposure even when output-based signals appear protected, highlighting the need for further research into membership privacy and the development of more robust privacy-preserving training techniques for large language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† memTrace æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ†æå¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„å†…éƒ¨è¡¨ç¤ºæ¥è¿›è¡Œæˆå‘˜æ¨ç†æ”»å‡» (Membership Inference Attacks, MIAs)ã€‚ä¸ä»¥å¾€ä¸»è¦ä¾èµ–æ¨¡å‹è¾“å‡ºçš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ¡†æ¶é€šè¿‡ä» Transformer éšçŠ¶æ€ (hidden states) å’Œæ³¨æ„åŠ›æ¨¡å¼ (attention patterns) ä¸­æå–â€œç¥ç»é¢åŒ…å±‘â€ (neural breadcrumbs) ä¿¡å·æ¥è¯†åˆ«è®­ç»ƒæ•°æ®ã€‚ç ”ç©¶é€šè¿‡åˆ†æé€å±‚è¡¨ç¤ºåŠ¨æ€ã€æ³¨æ„åŠ›åˆ†å¸ƒç‰¹å¾åŠè·¨å±‚è½¬æ¢æ¨¡å¼ï¼ŒæˆåŠŸæ•æ‰åˆ°äº†ä¼ ç»ŸåŸºäºæŸå¤± (loss-based) çš„æ–¹æ³•éš¾ä»¥å‘ç°çš„è®°å¿†æŒ‡çº¹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒmemTrace åœ¨å¤šä¸ªæ¨¡å‹ç³»åˆ—ä¸Šå–å¾—äº†æ˜¾è‘—æ•ˆæœï¼Œåœ¨ä¸»æµ MIA åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† 0.85 çš„å¹³å‡ AUC åˆ†æ•°ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼Œå³ä½¿è¾“å‡ºä¿¡å·çœ‹ä¼¼å—åˆ°ä¿æŠ¤ï¼Œæ¨¡å‹çš„å†…éƒ¨è¡Œä¸ºä»å¯èƒ½æ³„éœ²è®­ç»ƒæ•°æ®ä¿¡æ¯ï¼Œå¼ºè°ƒäº†è¿›ä¸€æ­¥ç ”ç©¶æˆå‘˜éšç§ä¿æŠ¤å’Œå¼€å‘æ›´é²æ£’çš„éšç§ä¿æŠ¤è®­ç»ƒæŠ€æœ¯çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05449v1",
      "published_date": "2025-09-05 19:05:49 UTC",
      "updated_date": "2025-09-05 19:05:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:43:29.567112+00:00"
    },
    {
      "arxiv_id": "2509.05448v1",
      "title": "Newton to Einstein: Axiom-Based Discovery via Game Design",
      "title_zh": "Newton to Einsteinï¼šåŸºäºæ¸¸æˆè®¾è®¡çš„å…¬ç†åŒ–å‘ç°",
      "authors": [
        "Pingchuan Ma",
        "Benjamin Tod Jones",
        "Tsun-Hsuan Wang",
        "Minghao Guo",
        "Michal Piotr Lipiec",
        "Chuang Gan",
        "Wojciech Matusik"
      ],
      "abstract": "This position paper argues that machine learning for scientific discovery should shift from inductive pattern recognition to axiom-based reasoning. We propose a game design framework in which scientific inquiry is recast as a rule-evolving system: agents operate within environments governed by axioms and modify them to explain outlier observations. Unlike conventional ML approaches that operate within fixed assumptions, our method enables the discovery of new theoretical structures through systematic rule adaptation. We demonstrate the feasibility of this approach through preliminary experiments in logic-based games, showing that agents can evolve axioms that solve previously unsolvable problems. This framework offers a foundation for building machine learning systems capable of creative, interpretable, and theory-driven discovery.",
      "tldr_zh": "è¿™ç¯‡ç«‹åœºè®ºæ–‡ä¸»å¼ ç§‘å­¦å‘ç°çš„æœºå™¨å­¦ä¹ (machine learning)åº”ä»å½’çº³æ¨¡å¼è¯†åˆ«è½¬å‘åŸºäºå…¬ç†(axiom-based)çš„æ¨ç†ã€‚ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ¸¸æˆè®¾è®¡(game design)æ¡†æ¶ï¼Œå°†ç§‘å­¦æ¢ç©¶é‡å¡‘ä¸ºä¸€ç§è§„åˆ™æ¼”åŒ–çš„ç³»ç»Ÿã€‚åœ¨è¯¥æ¡†æ¶ä¸­ï¼Œæ™ºèƒ½ä½“(agents)åœ¨å…¬ç†çº¦æŸçš„ç¯å¢ƒä¸‹è¿è¡Œï¼Œå¹¶é€šè¿‡ä¿®æ”¹è¿™äº›å…¬ç†æ¥è§£é‡Šè§‚å¯Ÿåˆ°çš„å¼‚å¸¸å€¼ã€‚ä¸æ“ä½œäºå›ºå®šå‡è®¾ä¸‹çš„ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•é€šè¿‡ç³»ç»Ÿçš„è§„åˆ™è‡ªé€‚åº”æ¥å®ç°æ–°ç†è®ºç»“æ„çš„å‘ç°ã€‚åˆæ­¥çš„é€»è¾‘æ¸¸æˆå®éªŒè¯æ˜ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿæ¼”åŒ–å‡ºå…¬ç†ä»¥è§£å†³ä¹‹å‰æ— æ³•è§£å†³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ä¸ºæ„å»ºå…·å¤‡åˆ›é€ æ€§ã€å¯è§£é‡Šæ€§å’Œç†è®ºé©±åŠ¨(theory-driven)å‘ç°èƒ½åŠ›çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05448v1",
      "published_date": "2025-09-05 18:59:18 UTC",
      "updated_date": "2025-09-05 18:59:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:43:37.872061+00:00"
    },
    {
      "arxiv_id": "2509.05440v1",
      "title": "Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too",
      "title_zh": "ç›´æ¥è¯„åˆ†å¼ NLG è¯„ä¼°å™¨äº¦å¯é‡‡ç”¨æˆå¯¹æ¯”è¾ƒ",
      "authors": [
        "Logan Lawrence",
        "Ashton Williamson",
        "Alexander Shelton"
      ],
      "abstract": "As large-language models have been increasingly used as automatic raters for evaluating free-form content, including document summarization, dialog, and story generation, work has been dedicated to evaluating such models by measuring their correlations with human judgment. For \\textit{sample-level} performance, methods which operate by using pairwise comparisons between machine-generated text perform well but often lack the ability to assign absolute scores to individual summaries, an ability crucial for use cases that require thresholding. In this work, we propose a direct-scoring method which uses synthetic summaries to act as pairwise machine rankings at test time. We show that our method performs comparably to state-of-the-art pairwise evaluators in terms of axis-averaged sample-level correlations on the SummEval (\\textbf{+0.03}), TopicalChat (\\textbf{-0.03}), and HANNA (\\textbf{+0.05}) meta-evaluation benchmarks, and release the synthetic in-context summaries as data to facilitate future work.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models)ä½œä¸ºè‡ªåŠ¨è¯„åˆ†å™¨åœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆ(NLG)è¯„ä¼°ä¸­çš„åº”ç”¨ï¼Œé’ˆå¯¹ä¸¤ä¸¤æ¯”è¾ƒ(Pairwise Comparisons)æ–¹æ³•æ— æ³•ä¸ºå•ä¸ªæ ·æœ¬æä¾›ç»å¯¹åˆ†æ•°çš„å±€é™æ€§æå‡ºäº†æ”¹è¿›æ–¹æ¡ˆã€‚ä½œè€…æå‡ºäº†ä¸€ç§ç›´æ¥è¯„åˆ†(Direct-Scoring)æ–¹æ³•ï¼Œé€šè¿‡åœ¨æµ‹è¯•é˜¶æ®µå¼•å…¥åˆæˆæ‘˜è¦(Synthetic Summaries)æ¥è¾…åŠ©å®ç°æˆå¯¹æ’åºã€‚è¯¥æ–¹æ³•æ—¨åœ¨ä¿ç•™ä¸¤ä¸¤æ¯”è¾ƒåœ¨æ ·æœ¬çº§ç›¸å…³æ€§ä¸Šçš„ä¼˜åŠ¿ï¼ŒåŒæ—¶æ»¡è¶³é˜ˆå€¼è®¾å®šç­‰å®é™…åº”ç”¨åœºæ™¯å¯¹ç»å¯¹è¯„åˆ†çš„éœ€æ±‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨SummEvalã€TopicalChatå’ŒHANNAç­‰å…ƒè¯„ä¼°åŸºå‡†ä¸Šçš„è½´å¹³å‡æ ·æœ¬çº§ç›¸å…³æ€§ä¸æœ€å…ˆè¿›çš„æˆå¯¹è¯„ä¼°å™¨ç›¸å½“ï¼Œè¡¨ç°å·®å¼‚åˆ†åˆ«ä¸º+0.03ã€-0.03å’Œ+0.05ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å…¬å¼€å‘å¸ƒäº†åˆæˆçš„ä¸Šä¸‹æ–‡æ‘˜è¦æ•°æ®ï¼Œä»¥ä¿ƒè¿›è‡ªç„¶è¯­è¨€ç”Ÿæˆè¯„ä¼°é¢†åŸŸçš„åç»­ç ”ç©¶å·¥ä½œã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 18 tables, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2509.05440v1",
      "published_date": "2025-09-05 18:48:34 UTC",
      "updated_date": "2025-09-05 18:48:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:43:38.368826+00:00"
    },
    {
      "arxiv_id": "2509.05431v1",
      "title": "Advanced Brain Tumor Segmentation Using EMCAD: Efficient Multi-scale Convolutional Attention Decoding",
      "title_zh": "åŸºäº EMCAD çš„é«˜çº§è„‘è‚¿ç˜¤åˆ†å‰²ï¼šé«˜æ•ˆå¤šå°ºåº¦å·ç§¯æ³¨æ„åŠ›è§£ç ",
      "authors": [
        "GodsGift Uzor",
        "Tania-Amanda Nkoyo Fredrick Eneye",
        "Chukwuebuka Ijezue"
      ],
      "abstract": "Brain tumor segmentation is a critical pre-processing step in the medical image analysis pipeline that involves precise delineation of tumor regions from healthy brain tissue in medical imaging data, particularly MRI scans. An efficient and effective decoding mechanism is crucial in brain tumor segmentation especially in scenarios with limited computational resources. However these decoding mechanisms usually come with high computational costs. To address this concern EMCAD a new efficient multi-scale convolutional attention decoder designed was utilized to optimize both performance and computational efficiency for brain tumor segmentation on the BraTs2020 dataset consisting of MRI scans from 369 brain tumor patients. The preliminary result obtained by the model achieved a best Dice score of 0.31 and maintained a stable mean Dice score of 0.285 plus/minus 0.015 throughout the training process which is moderate. The initial model maintained consistent performance across the validation set without showing signs of over-fitting.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è„‘è‚¿ç˜¤åˆ†å‰²ï¼ˆBrain tumor segmentationï¼‰ä¸­è§£ç æœºåˆ¶è®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºEMCADçš„é«˜æ•ˆå¤šå°ºåº¦å·ç§¯æ³¨æ„è§£ç å™¨ï¼ˆEfficient Multi-scale Convolutional Attention Decoderï¼‰ã€‚EMCADæ—¨åœ¨å¹³è¡¡æ¨¡å‹æ€§èƒ½ä¸è®¡ç®—æ•ˆç‡ï¼Œç‰¹åˆ«ä¼˜åŒ–äº†åœ¨è®¡ç®—èµ„æºå—é™ç¯å¢ƒä¸‹çš„åŒ»å­¦å›¾åƒåˆ†æè¡¨ç°ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨åŒ…å«369åæ‚£è€…MRIæ•°æ®çš„BraTS2020æ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ¨¡å‹å–å¾—äº†0.31çš„æœ€ä½³Dice scoreï¼Œå¹¶ä¿æŒäº†çº¦0.285çš„ç¨³å®šå¹³å‡Dice scoreã€‚åˆæ­¥æ¨¡å‹åœ¨æ•´ä¸ªè®­ç»ƒå’ŒéªŒè¯è¿‡ç¨‹ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„ç¨³å®šæ€§ï¼Œä¸”æœªå‡ºç°è¿‡æ‹Ÿåˆï¼ˆover-fittingï¼‰è¿¹è±¡ã€‚è¯¥å·¥ä½œå±•ç¤ºäº†EMCADåœ¨æé«˜è„‘è‚¿ç˜¤åˆ†å‰²æ•ˆç‡æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºåŒ»ç–—å½±åƒçš„ç²¾å‡†å¤„ç†æä¾›äº†æœ‰æ•ˆçš„è§£ç æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05431v1",
      "published_date": "2025-09-05 18:23:47 UTC",
      "updated_date": "2025-09-05 18:23:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:43:44.383369+00:00"
    },
    {
      "arxiv_id": "2509.05425v1",
      "title": "No Translation Needed: Forecasting Quality from Fertility and Metadata",
      "title_zh": "æ— éœ€ç¿»è¯‘ï¼šåŸºäºè¯å…ƒä¸°é¥¶åº¦ä¸å…ƒæ•°æ®çš„è´¨é‡é¢„æµ‹",
      "authors": [
        "Jessica M. Lundin",
        "Ada Zhang",
        "David Adelani",
        "Cody Carroll"
      ],
      "abstract": "We show that translation quality can be predicted with surprising accuracy \\textit{without ever running the translation system itself}. Using only a handful of features, token fertility ratios, token counts, and basic linguistic metadata (language family, script, and region), we can forecast ChrF scores for GPT-4o translations across 203 languages in the FLORES-200 benchmark. Gradient boosting models achieve favorable performance ($R^{2}=0.66$ for XX$\\rightarrow$English and $R^{2}=0.72$ for English$\\rightarrow$XX). Feature importance analyses reveal that typological factors dominate predictions into English, while fertility plays a larger role for translations into diverse target languages. These findings suggest that translation quality is shaped by both token-level fertility and broader linguistic typology, offering new insights for multilingual evaluation and quality estimation.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶è¡¨æ˜ï¼Œæ— éœ€å®é™…è¿è¡Œç¿»è¯‘ç³»ç»Ÿå³å¯é«˜ç²¾åº¦åœ°é¢„æµ‹ç¿»è¯‘è´¨é‡ã€‚ç ”ç©¶é€šè¿‡åˆ©ç”¨ token fertility ratiosã€token æ•°é‡ä»¥åŠåŸºæœ¬çš„è¯­è¨€å…ƒæ•°æ®ï¼ˆå¦‚è¯­è¨€å®¶æ—ã€è„šæœ¬å’ŒåŒºåŸŸï¼‰ç­‰å°‘é‡ç‰¹å¾ï¼ŒæˆåŠŸé¢„æµ‹äº† GPT-4o åœ¨ FLORES-200 åŸºå‡†æµ‹è¯•ä¸­ 203 ç§è¯­è¨€çš„ ChrF åˆ†æ•°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¢¯åº¦æå‡æ¨¡å‹ (Gradient boosting models) åœ¨ XX$\\rightarrow$English ($R^{2}=0.66$) å’Œ English$\\rightarrow$XX ($R^{2}=0.72$) çš„ç¿»è¯‘è´¨é‡é¢„æµ‹ä¸Šå‡è¡¨ç°ä¼˜å¼‚ã€‚ç‰¹å¾é‡è¦æ€§åˆ†ææ­ç¤ºï¼Œç¿»è¯‘ä¸ºè‹±è¯­æ—¶è¯­è¨€ç±»å‹å­¦ (typological factors) å ä¸»å¯¼åœ°ä½ï¼Œè€Œç¿»è¯‘ä¸ºå…¶ä»–è¯­è¨€æ—¶ fertility çš„å½±å“æ›´ä¸ºæ˜¾è‘—ã€‚è¿™äº›å‘ç°è¯æ˜äº†ç¿»è¯‘è´¨é‡å— token-level fertility å’Œå¹¿æ³›çš„è¯­è¨€ç±»å‹å­¦å…±åŒå¡‘é€ ï¼Œä¸ºå¤šè¯­è¨€è¯„ä¼°å’Œè´¨é‡ä¼°ç®— (Quality Estimation) æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05425v1",
      "published_date": "2025-09-05 18:11:49 UTC",
      "updated_date": "2025-09-05 18:11:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:43:44.783759+00:00"
    },
    {
      "arxiv_id": "2509.10538v1",
      "title": "DualAlign: Generating Clinically Grounded Synthetic Data",
      "title_zh": "DualAlignï¼šç”Ÿæˆå…·å¤‡ä¸´åºŠä¾æ®çš„åˆæˆæ•°æ®",
      "authors": [
        "Rumeng Li",
        "Xun Wang",
        "Hong Yu"
      ],
      "abstract": "Synthetic clinical data are increasingly important for advancing AI in healthcare, given strict privacy constraints on real-world EHRs, limited availability of annotated rare-condition data, and systemic biases in observational datasets. While large language models (LLMs) can generate fluent clinical text, producing synthetic data that is both realistic and clinically meaningful remains challenging. We introduce DualAlign, a framework that enhances statistical fidelity and clinical plausibility through dual alignment: (1) statistical alignment, which conditions generation on patient demographics and risk factors; and (2) semantic alignment, which incorporates real-world symptom trajectories to guide content generation. Using Alzheimer's disease (AD) as a case study, DualAlign produces context-grounded symptom-level sentences that better reflect real-world clinical documentation. Fine-tuning an LLaMA 3.1-8B model with a combination of DualAlign-generated and human-annotated data yields substantial performance gains over models trained on gold data alone or unguided synthetic baselines. While DualAlign does not fully capture longitudinal complexity, it offers a practical approach for generating clinically grounded, privacy-preserving synthetic data to support low-resource clinical text analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çœŸå®ä¸–ç•Œç”µå­å¥åº·æ¡£æ¡ˆ(EHR)éšç§å—é™åŠç¨€æœ‰ç–¾ç—…æ•°æ®ç¨€ç¼ºç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†DualAlignæ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆå…¼å…·ç»Ÿè®¡ä¿çœŸåº¦ä¸ä¸´åºŠåˆç†æ€§çš„åˆæˆæ•°æ®ã€‚DualAligné€šè¿‡ç»Ÿè®¡å¯¹é½(statistical alignment)å’Œè¯­ä¹‰å¯¹é½(semantic alignment)çš„åŒé‡æœºåˆ¶ï¼Œå°†æ‚£è€…äººå£ç»Ÿè®¡å­¦ç‰¹å¾ã€é£é™©å› ç´ åŠçœŸå®ç—‡çŠ¶è½¨è¿¹å¼•å…¥ç”Ÿæˆè¿‡ç¨‹ã€‚ä»¥é˜¿å°”èŒ¨æµ·é»˜ç—…(AD)ä¸ºæ¡ˆä¾‹çš„å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„ç—‡çŠ¶çº§æ–‡æœ¬èƒ½æ›´å‡†ç¡®åœ°åæ˜ ä¸´åºŠå®é™…ã€‚ç ”ç©¶å‘ç°ï¼Œåˆ©ç”¨DualAlignåˆæˆæ•°æ®ä¸äººå·¥æ ‡æ³¨æ•°æ®è”åˆå¾®è°ƒLLaMA 3.1-8Bæ¨¡å‹ï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºå•çº¯ä½¿ç”¨çœŸå®æ•°æ®æˆ–æ— å¼•å¯¼çš„åˆæˆåŸºå‡†ã€‚å°½ç®¡åœ¨å¤„ç†é•¿æœŸçºµå‘å¤æ‚æ€§æ–¹é¢ä»æœ‰å±€é™ï¼ŒDualAlignä»ä¸ºä½èµ„æºä¸´åºŠæ–‡æœ¬åˆ†ææä¾›äº†ä¸€ç§ç”Ÿæˆä¸´åºŠå¯é ä¸”ä¿æŠ¤éšç§çš„åˆæˆæ•°æ®çš„å®ç”¨æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.10538v1",
      "published_date": "2025-09-05 18:04:38 UTC",
      "updated_date": "2025-09-05 18:04:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:44:02.193818+00:00"
    },
    {
      "arxiv_id": "2509.05296v1",
      "title": "WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool",
      "title_zh": "WinT3Rï¼šåŸºäºç›¸æœº Token æ± çš„çª—å£å¼æµå¼é‡å»º",
      "authors": [
        "Zizun Li",
        "Jianjun Zhou",
        "Yifan Wang",
        "Haoyu Guo",
        "Wenzheng Chang",
        "Yang Zhou",
        "Haoyi Zhu",
        "Junyi Chen",
        "Chunhua Shen",
        "Tong He"
      ],
      "abstract": "We present WinT3R, a feed-forward reconstruction model capable of online prediction of precise camera poses and high-quality point maps. Previous methods suffer from a trade-off between reconstruction quality and real-time performance. To address this, we first introduce a sliding window mechanism that ensures sufficient information exchange among frames within the window, thereby improving the quality of geometric predictions without large computation. In addition, we leverage a compact representation of cameras and maintain a global camera token pool, which enhances the reliability of camera pose estimation without sacrificing efficiency. These designs enable WinT3R to achieve state-of-the-art performance in terms of online reconstruction quality, camera pose estimation, and reconstruction speed, as validated by extensive experiments on diverse datasets. Code and model are publicly available at https://github.com/LiZizun/WinT3R.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† WinT3Rï¼Œä¸€ç§èƒ½å¤Ÿåœ¨çº¿é¢„æµ‹ç²¾ç¡® Camera Poses å’Œé«˜è´¨é‡ Point Maps çš„å‰é¦ˆé‡å»ºæ¨¡å‹ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨é‡å»ºè´¨é‡ä¸å®æ—¶æ€§èƒ½ä¹‹é—´éš¾ä»¥æƒè¡¡çš„é—®é¢˜ï¼ŒWinT3R å¼•å…¥äº† Sliding Window æœºåˆ¶ï¼Œé€šè¿‡ç¡®ä¿çª—å£å†…å¸§ä¹‹é—´çš„å……åˆ†ä¿¡æ¯äº¤æ¢ï¼Œåœ¨ä¸å¢åŠ å¤§è§„æ¨¡è®¡ç®—é‡çš„æƒ…å†µä¸‹æå‡äº†å‡ ä½•é¢„æµ‹çš„è´¨é‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨ç´§å‡‘çš„ç›¸æœºè¡¨ç¤ºå¹¶ç»´æŠ¤å…¨å±€ Camera Token Poolï¼Œåœ¨ä¸ç‰ºç‰²æ•ˆç‡çš„å‰æä¸‹æ˜¾è‘—å¢å¼ºäº†ä½å§¿ä¼°è®¡çš„å¯é æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWinT3R åœ¨åœ¨çº¿é‡å»ºè´¨é‡ã€Camera Pose Estimation å’Œé‡å»ºé€Ÿåº¦æ–¹é¢å‡è¾¾åˆ°äº† State-of-the-art æ€§èƒ½ã€‚è¯¥ç ”ç©¶é€šè¿‡åˆ›æ–°çš„çª—å£åŒ–æœºåˆ¶ä¸ä»¤ç‰Œæ± ç®¡ç†ï¼Œä¸ºé«˜æ•ˆä¸”é«˜ç²¾åº¦çš„æµå¼ä¸‰ç»´é‡å»ºæä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05296v1",
      "published_date": "2025-09-05 17:59:47 UTC",
      "updated_date": "2025-09-05 17:59:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:44:04.792808+00:00"
    },
    {
      "arxiv_id": "2509.05291v1",
      "title": "Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic Representations Throughout LLM Pretraining",
      "title_zh": "è·¨æ—¶è·¨ç¼–ç ï¼šè¿½è¸ª LLM é¢„è®­ç»ƒå…¨è¿‡ç¨‹ä¸­è¯­è¨€è¡¨å¾çš„æ¶Œç°ä¸å·©å›º",
      "authors": [
        "Deniz Bayazit",
        "Aaron Mueller",
        "Antoine Bosselut"
      ],
      "abstract": "Large language models (LLMs) learn non-trivial abstractions during pretraining, like detecting irregular plural noun subjects. However, it is not well understood when and how specific linguistic abilities emerge as traditional evaluation methods such as benchmarking fail to reveal how models acquire concepts and capabilities. To bridge this gap and better understand model training at the concept level, we use sparse crosscoders to discover and align features across model checkpoints. Using this approach, we track the evolution of linguistic features during pretraining. We train crosscoders between open-sourced checkpoint triplets with significant performance and representation shifts, and introduce a novel metric, Relative Indirect Effects (RelIE), to trace training stages at which individual features become causally important for task performance. We show that crosscoders can detect feature emergence, maintenance, and discontinuation during pretraining. Our approach is architecture-agnostic and scalable, offering a promising path toward more interpretable and fine-grained analysis of representation learning throughout pretraining.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å¦‚ä½•å­¦ä¹ å’Œæ•´åˆè¯­è¨€è¡¨å¾ã€‚ä¸ºäº†è§£å†³ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•æ— æ³•æ­ç¤ºæ¨¡å‹è·å–æ¦‚å¿µè¿‡ç¨‹çš„é—®é¢˜ï¼Œä½œè€…é‡‡ç”¨äº†ç¨€ç–äº¤å‰ç¼–ç å™¨(sparse crosscoders)æ¥å‘ç°å¹¶å¯¹é½ä¸åŒæ¨¡å‹æ£€æŸ¥ç‚¹(checkpoints)ä¹‹é—´çš„ç‰¹å¾ï¼Œä»è€Œè¿½è¸ªè¯­è¨€ç‰¹å¾åœ¨é¢„è®­ç»ƒæœŸé—´çš„æ¼”åŒ–ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ç§åä¸ºç›¸å¯¹é—´æ¥å½±å“(Relative Indirect Effects, RelIE)çš„æ–°æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡å•ä¸ªç‰¹å¾åœ¨ä¸åŒè®­ç»ƒé˜¶æ®µå¯¹ä»»åŠ¡æ€§èƒ½çš„å› æœé‡è¦æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ£€æµ‹ç‰¹å¾çš„å‡ºç°(emergence)ã€ç»´æŒ(maintenance)å’Œä¸­æ­¢(discontinuation)ã€‚è¯¥æ–¹æ¡ˆå…·æœ‰æ¶æ„æ— å…³æ€§å’Œè‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œä¸ºç†è§£é¢„è®­ç»ƒé˜¶æ®µçš„è¡¨å¾å­¦ä¹ æä¾›äº†ä¸€ç§ç»†ç²’åº¦ä¸”æ›´å…·å¯è§£é‡Šæ€§çš„åˆ†æå·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05291v1",
      "published_date": "2025-09-05 17:56:24 UTC",
      "updated_date": "2025-09-05 17:56:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:44:11.391204+00:00"
    },
    {
      "arxiv_id": "2509.05276v3",
      "title": "SpikingBrain: Spiking Brain-inspired Large Models",
      "title_zh": "SpikingBrainï¼šç±»è„‘è„‰å†²å¤§æ¨¡å‹",
      "authors": [
        "Yuqi Pan",
        "Yupeng Feng",
        "Jinghao Zhuang",
        "Siyu Ding",
        "Han Xu",
        "Zehao Liu",
        "Bohan Sun",
        "Yuhong Chou",
        "Xuerui Qiu",
        "Anlin Deng",
        "Anjie Hu",
        "Shurong Wang",
        "Peng Zhou",
        "Man Yao",
        "Jibin Wu",
        "Jian Yang",
        "Guoliang Sun",
        "Bo Xu",
        "Guoqi Li"
      ],
      "abstract": "Mainstream Transformer-based large language models face major efficiency bottlenecks: training computation scales quadratically with sequence length, and inference memory grows linearly, limiting long-context processing. Building large models on non-NVIDIA platforms also poses challenges for stable and efficient training. To address this, we introduce SpikingBrain, a family of brain-inspired models designed for efficient long-context training and inference. SpikingBrain leverages the MetaX GPU cluster and focuses on three aspects: (1) Model Architecture: linear and hybrid-linear attention architectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an efficient, conversion-based training pipeline and a dedicated spike coding framework; (3) System Engineering: customized training frameworks, operator libraries, and parallelism strategies tailored to MetaX hardware.\n  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM, and SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the feasibility of large-scale LLM development on non-NVIDIA platforms, and training remains stable for weeks on hundreds of MetaX GPUs with Model FLOPs Utilization at expected levels. SpikingBrain achieves performance comparable to open-source Transformer baselines while using only about 150B tokens for continual pre-training. Our models also significantly improve long-context efficiency and deliver inference with (partially) constant memory and event-driven spiking behavior. For example, SpikingBrain-7B attains over 100x speedup in Time to First Token for 4M-token sequences. Furthermore, the proposed spiking scheme achieves 69.15 percent sparsity, enabling low-power operation. Overall, this work demonstrates the potential of brain-inspired mechanisms to drive the next generation of efficient and scalable large model design.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†SpikingBrainç³»åˆ—ç±»è„‘å¤§æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡è„‘å¯å‘æœºåˆ¶è§£å†³ä¼ ç»ŸTransformeræ¶æ„åœ¨é•¿ä¸Šä¸‹æ–‡å¤„ç†ä¸­çš„è®¡ç®—ä¸å†…å­˜æ•ˆç‡ç“¶é¢ˆã€‚æ¨¡å‹æ ¸å¿ƒé‡‡ç”¨äº†çº¿æ€§åŠæ··åˆçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶(linear and hybrid-linear attention)ä¸è‡ªé€‚åº”è„‰å†²ç¥ç»å…ƒ(adaptive spiking neurons)ï¼Œå¹¶é’ˆå¯¹MetaXç¡¬ä»¶ä¼˜åŒ–äº†ç®—å­åº“ä¸å¹¶è¡Œç­–ç•¥ã€‚å›¢é˜Ÿç ”å‘äº†SpikingBrain-7Bçº¿æ€§LLMå’ŒSpikingBrain-76Bæ··åˆä¸“å®¶LLM(MoE)ï¼Œè¯æ˜äº†åœ¨éNVIDIAå¹³å°ä¸Šè¿›è¡Œå¤§è§„æ¨¡ç¨³å®šè®­ç»ƒçš„å¯è¡Œæ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä»…ä½¿ç”¨çº¦150B tokensè¿›è¡ŒæŒç»­é¢„è®­ç»ƒåï¼Œå…¶æ€§èƒ½å³å¯ä¸å¼€æºTransformeråŸºå‡†ç›¸åª²ç¾ã€‚åœ¨æ¨ç†æ€§èƒ½ä¸Šï¼ŒSpikingBrain-7Båœ¨å¤„ç†400ä¸‡é•¿åº¦åºåˆ—æ—¶çš„é¦–å­—ç”Ÿæˆæ—¶é—´(Time to First Token)å®ç°äº†è¶…100å€çš„åŠ é€Ÿã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„è„‰å†²æ–¹æ¡ˆè¾¾åˆ°äº†69.15%çš„ç¨€ç–åº¦(sparsity)ï¼Œæœ‰æ•ˆå®ç°äº†ä½åŠŸè€—è¿è¡Œï¼Œä¸ºé«˜æ•ˆã€å¯æ‰©å±•çš„ä¸‹ä¸€ä»£å¤§æ¨¡å‹è®¾è®¡æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05276v3",
      "published_date": "2025-09-05 17:34:00 UTC",
      "updated_date": "2025-12-01 18:21:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:44:17.589117+00:00"
    },
    {
      "arxiv_id": "2509.10537v1",
      "title": "On Using Large-Batches in Federated Learning",
      "title_zh": "è”é‚¦å­¦ä¹ ä¸­çš„å¤§æ‰¹é‡è®­ç»ƒç ”ç©¶",
      "authors": [
        "Sahil Tyagi"
      ],
      "abstract": "Efficient Federated learning (FL) is crucial for training deep networks over devices with limited compute resources and bounded networks. With the advent of big data, devices either generate or collect multimodal data to train either generic or local-context aware networks, particularly when data privacy and locality is vital. FL algorithms generally trade-off between parallel and statistical performance, improving model quality at the cost of higher communication frequency, or vice versa. Under frequent synchronization settings, FL over a large cluster of devices may perform more work per-training iteration by processing a larger global batch-size, thus attaining considerable training speedup. However, this may result in poor test performance (i.e., low test loss or accuracy) due to generalization degradation issues associated with large-batch training. To address these challenges with large-batches, this work proposes our vision of exploiting the trade-offs between small and large-batch training, and explore new directions to enjoy both the parallel scaling of large-batches and good generalizability of small-batch training. For the same number of iterations, we observe that our proposed large-batch training technique attains about 32.33% and 3.74% higher test accuracy than small-batch training in ResNet50 and VGG11 models respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨è”é‚¦å­¦ä¹ (Federated Learning)ä¸­åº”ç”¨å¤§æ‰¹é‡(Large-Batches)è®­ç»ƒçš„æŒ‘æˆ˜ä¸æœºé‡ã€‚é’ˆå¯¹å¤§æ‰¹é‡è®­ç»ƒè™½èƒ½æå‡å¹¶è¡Œæ‰©å±•é€Ÿåº¦ä½†å¾€å¾€å¯¼è‡´æ³›åŒ–é€€åŒ–ï¼ˆå³æµ‹è¯•æ€§èƒ½ä¸‹é™ï¼‰çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¹³è¡¡å°æ‰¹é‡(Small-Batch)ä¸å¤§æ‰¹é‡è®­ç»ƒæƒè¡¡çš„æ–°æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•æ—¨åœ¨åŒæ—¶è·å¾—å¤§æ‰¹é‡çš„è®¡ç®—æ•ˆç‡å’Œå°æ‰¹é‡çš„ä¼˜å¼‚æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œä¼˜åŒ–æ·±åº¦ç½‘ç»œåœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„è®­ç»ƒè¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¿æŒè¿­ä»£æ¬¡æ•°ç›¸åŒçš„å‰æä¸‹ï¼Œæ‰€ææŠ€æœ¯åœ¨ ResNet50 å’Œ VGG11 æ¨¡å‹ä¸Šçš„æµ‹è¯•å‡†ç¡®ç‡åˆ†åˆ«æ¯”å°æ‰¹é‡è®­ç»ƒé«˜å‡º 32.33% å’Œ 3.74%ã€‚è¿™ä¸€å‘ç°ä¸ºè§£å†³è”é‚¦å­¦ä¹ ä¸­çš„å¤§è§„æ¨¡å¹¶è¡ŒåŒ–ä¸æ¨¡å‹æ€§èƒ½å†²çªæä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.10537v1",
      "published_date": "2025-09-05 17:31:50 UTC",
      "updated_date": "2025-09-05 17:31:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:44:26.691181+00:00"
    },
    {
      "arxiv_id": "2509.09705v1",
      "title": "The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks",
      "title_zh": "å°å‹ LLM çš„éç¡®å®šæ€§ï¼šæ ‡å‡†å¤šé¡¹é€‰æ‹©åŸºå‡†æµ‹è¯•é‡å¤å®éªŒä¸­ä½ç­”æ¡ˆä¸€è‡´æ€§çš„è¯æ®",
      "authors": [
        "Claudio Pinhanez",
        "Paulo Cavalin",
        "Cassia Sanctos",
        "Marcelo Grave",
        "Yago Primerano"
      ],
      "abstract": "This work explores the consistency of small LLMs (2B-8B parameters) in answering multiple times the same question. We present a study on known, open-source LLMs responding to 10 repetitions of questions from the multiple-choice benchmarks MMLU-Redux and MedQA, considering different inference temperatures, small vs. medium models (50B-80B), finetuned vs. base models, and other parameters. We also look into the effects of requiring multi-trial answer consistency on accuracy and the trade-offs involved in deciding which model best provides both of them. To support those studies, we propose some new analytical and graphical tools. Results show that the number of questions which can be answered consistently vary considerably among models but are typically in the 50%-80% range for small models at low inference temperatures. Also, accuracy among consistent answers seems to reasonably correlate with overall accuracy. Results for medium-sized models seem to indicate much higher levels of answer consistency.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°å‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼Œ2B-8Bå‚æ•°ï¼‰åœ¨å¤šæ¬¡å›ç­”ç›¸åŒé—®é¢˜æ—¶çš„ä¸€è‡´æ€§è¡¨ç°ï¼Œæ­ç¤ºäº†å°å‹æ¨¡å‹åœ¨æ ‡å‡†å¤šé€‰é¢˜åŸºå‡†æµ‹è¯•ä¸­å­˜åœ¨çš„éç¡®å®šæ€§é—®é¢˜ã€‚ç ”ç©¶äººå‘˜é€šè¿‡å¯¹å¼€æºæ¨¡å‹åœ¨MMLU-Reduxå’ŒMedQAä¸Šçš„10æ¬¡é‡å¤å®éªŒï¼Œåˆ†æäº†æ¨ç†æ¸©åº¦ã€æ¨¡å‹è§„æ¨¡ä»¥åŠå¾®è°ƒçŠ¶æ€ç­‰å‚æ•°å¯¹ä¸€è‡´æ€§çš„å½±å“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä½æ¨ç†æ¸©åº¦ä¸‹ï¼Œå°å‹æ¨¡å‹èƒ½å¤Ÿäº§ç”Ÿä¸€è‡´å›ç­”çš„é—®é¢˜æ¯”ä¾‹é€šå¸¸åœ¨50%è‡³80%ä¹‹é—´ï¼Œä¸”ä¸åŒæ¨¡å‹é—´è¡¨ç°å·®å¼‚æ˜¾è‘—ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ50B-80Bè§„æ¨¡çš„ä¸­å‹æ¨¡å‹å±•ç°å‡ºæ˜¾è‘—æ›´é«˜çš„ä¸€è‡´æ€§æ°´å¹³ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°ä¸€è‡´æ€§å›ç­”çš„å‡†ç¡®ç‡ä¸æ•´ä½“å‡†ç¡®ç‡å‘ˆæ­£ç›¸å…³ï¼Œå¹¶ä¸ºæ­¤æå‡ºäº†æ–°çš„åˆ†æä¸å¯è§†åŒ–å·¥å…·ã€‚è¯¥å·¥ä½œé‡åŒ–äº†å°å‹LLMsçš„å›ç­”ä¸ä¸€è‡´æ€§ï¼Œä¸ºåœ¨æ¨¡å‹æ€§èƒ½ä¸ä¸€è‡´æ€§ä¹‹é—´è¿›è¡Œæƒè¡¡æä¾›äº†å®è¯å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09705v1",
      "published_date": "2025-09-05 17:31:14 UTC",
      "updated_date": "2025-09-05 17:31:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:44:27.383336+00:00"
    },
    {
      "arxiv_id": "2509.05263v2",
      "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation",
      "title_zh": "LatticeWorldï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹èµ‹èƒ½çš„äº¤äº’å¼å¤æ‚ä¸–ç•Œç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Yinglin Duan",
        "Zhengxia Zou",
        "Tongwei Gu",
        "Wei Jia",
        "Zhan Zhao",
        "Luyi Xu",
        "Xinzhu Liu",
        "Yenan Lin",
        "Hao Jiang",
        "Kang Chen",
        "Shuang Qiu"
      ],
      "abstract": "Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a $90\\times$ increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LatticeWorldï¼Œä¸€ä¸ªç”±å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(Multimodal Large Language Model)é©±åŠ¨çš„äº¤äº’å¼å¤æ‚ä¸–ç•Œç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–3Dç¯å¢ƒçš„å·¥ä¸šåŒ–ç”Ÿäº§æµç¨‹ã€‚è¯¥æ¡†æ¶å°†è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹(LLaMA-2-7B)ä¸å·¥ä¸šçº§æ¸²æŸ“å¼•æ“(Unreal Engine 5)ç›¸ç»“åˆï¼Œèƒ½å¤Ÿæ¥æ”¶æ–‡æœ¬æè¿°å’Œè§†è§‰æŒ‡ä»¤ä½œä¸ºå¤šæ¨¡æ€è¾“å…¥ã€‚LatticeWorldå¯ä»¥ç”ŸæˆåŒ…å«åŠ¨æ€æ™ºèƒ½ä½“(dynamic agents)çš„å¤§è§„æ¨¡äº¤äº’å¼3Dä¸–ç•Œï¼Œå¹¶æ”¯æŒç«äº‰æ€§å¤šæ™ºèƒ½ä½“äº¤äº’(multi-agent interaction)ã€é«˜ä¿çœŸç‰©ç†æ¨¡æ‹Ÿå’Œå®æ—¶æ¸²æŸ“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨åœºæ™¯å¸ƒå±€ç”Ÿæˆç²¾åº¦å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢è¡¨ç°å“è¶Šï¼Œæœ‰æ•ˆç¼©å°äº†æ¨¡æ‹Ÿä¸ç°å®ä¹‹é—´çš„å·®è·(sim-to-real gap)ã€‚ä¸ä¼ ç»Ÿçš„å¼€å‘æ–¹å¼ç›¸æ¯”ï¼ŒLatticeWorldåœ¨ä¿æŒé«˜åˆ›ä½œè´¨é‡çš„åŒæ—¶ï¼Œå°†å·¥ä¸šç”Ÿäº§æ•ˆç‡æå‡äº†90å€ä»¥ä¸Šï¼Œä¸ºå…·ä½“åŒ–AI(embodied AI)å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸæä¾›äº†é«˜æ•ˆçš„æ¨¡æ‹Ÿç¯å¢ƒç”Ÿæˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05263v2",
      "published_date": "2025-09-05 17:22:33 UTC",
      "updated_date": "2025-09-08 17:05:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:44:34.797224+00:00"
    },
    {
      "arxiv_id": "2509.05258v2",
      "title": "Scaling Performance of Large Language Model Pretraining",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„æ‰©å±•æ€§èƒ½",
      "authors": [
        "Alexander Interrante-Grant",
        "Carla Varela-Rosa",
        "Suhaas Narayan",
        "Chris Connelly",
        "Albert Reuther"
      ],
      "abstract": "Large language models (LLMs) show best-in-class performance across a wide range of natural language processing applications. Training these models is an extremely computationally expensive task; frontier Artificial Intelligence (AI) research companies are investing billions of dollars into supercomputing infrastructure to train progressively larger models on increasingly massive datasets. Unfortunately, very little information about the scaling performance and training considerations of these large training pipelines is released publicly. Working with very large datasets and models can be complex and practical recommendations are scarce in the public literature for tuning training performance when scaling up large language models. In this paper, we aim to demystify the large language model pretraining pipeline somewhat - in particular with respect to distributed training, managing large datasets across hundreds of nodes, and scaling up data parallelism with an emphasis on fully leveraging available GPU compute capacity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Large Language Models (LLMs) é¢„è®­ç»ƒè¿‡ç¨‹ä¸­æé«˜çš„è®¡ç®—æˆæœ¬ï¼Œä»¥åŠå…¬å¼€æ–‡çŒ®ä¸­ç¼ºä¹å…³äº Scaling Performance å’Œè®­ç»ƒä¼˜åŒ–å»ºè®®çš„é—®é¢˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚è®ºæ–‡æ—¨åœ¨æ­å¼€å¤§è§„æ¨¡æ¨¡å‹é¢„è®­ç»ƒæµæ°´çº¿çš„æŠ€æœ¯ç»†èŠ‚ï¼Œé‡ç‚¹åˆ†æäº† Distributed Training æ¶æ„ä»¥åŠå¦‚ä½•åœ¨æ•°ç™¾ä¸ªèŠ‚ç‚¹ä¸Šé«˜æ•ˆç®¡ç†æµ·é‡æ•°æ®é›†ã€‚ç ”ç©¶ç‰¹åˆ«å¼ºè°ƒäº†é€šè¿‡ä¼˜åŒ– Data Parallelism æ¥å……åˆ†é‡Šæ”¾ GPU è®¡ç®—èƒ½åŠ›çš„å®è·µæ–¹æ³•ã€‚é€šè¿‡å¯¹è¿™äº›å…³é”®ç¯èŠ‚çš„å‰–æï¼Œè¯¥ç ”ç©¶ä¸ºåœ¨è¶…å¤§è§„æ¨¡è®¡ç®—åŸºç¡€è®¾æ–½ä¸Šæå‡é¢„è®­ç»ƒæ€§èƒ½æä¾›äº†å®è´µçš„å®ç”¨æŒ‡å¯¼ï¼Œæœ‰æ•ˆå¡«è¡¥äº†å·¥ä¸šç•Œå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒå®è·µä¸å­¦æœ¯å…¬å¼€èµ„æ–™ä¹‹é—´çš„ä¿¡æ¯ç¼ºå£ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05258v2",
      "published_date": "2025-09-05 17:14:58 UTC",
      "updated_date": "2025-10-09 13:56:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:44:36.391441+00:00"
    },
    {
      "arxiv_id": "2509.05256v1",
      "title": "Recomposer: Event-roll-guided generative audio editing",
      "title_zh": "Recomposerï¼šåŸºäºäº‹ä»¶å·è½´å¼•å¯¼çš„ç”Ÿæˆå¼éŸ³é¢‘ç¼–è¾‘",
      "authors": [
        "Daniel P. W. Ellis",
        "Eduardo Fonseca",
        "Ron J. Weiss",
        "Kevin Wilson",
        "Scott Wisdom",
        "Hakan Erdogan",
        "John R. Hershey",
        "Aren Jansen",
        "R. Channing Moore",
        "Manoj Plakal"
      ],
      "abstract": "Editing complex real-world sound scenes is difficult because individual sound sources overlap in time. Generative models can fill-in missing or corrupted details based on their strong prior understanding of the data domain. We present a system for editing individual sound events within complex scenes able to delete, insert, and enhance individual sound events based on textual edit descriptions (e.g., ``enhance Door'') and a graphical representation of the event timing derived from an ``event roll'' transcription. We present an encoder-decoder transformer working on SoundStream representations, trained on synthetic (input, desired output) audio example pairs formed by adding isolated sound events to dense, real-world backgrounds. Evaluation reveals the importance of each part of the edit descriptions -- action, class, timing. Our work demonstrates ``recomposition'' is an important and practical application.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Recomposerï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºç¼–è¾‘å¤æ‚ç°å®ä¸–ç•Œå£°éŸ³åœºæ™¯çš„ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³å£°éŸ³æºåœ¨æ—¶é—´ä¸Šé‡å å¸¦æ¥çš„ç¼–è¾‘æŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿæ”¯æŒé€šè¿‡æ–‡æœ¬ç¼–è¾‘æè¿°å’ŒåŸºäºâ€œevent rollâ€è½¬å½•çš„äº‹ä»¶æ—¶åºå›¾å½¢è¡¨ç¤ºï¼Œå¯¹å•ä¸ªå£°éŸ³äº‹ä»¶æ‰§è¡Œåˆ é™¤ã€æ’å…¥å’Œå¢å¼ºæ“ä½œã€‚Recomposerçš„æ ¸å¿ƒæ¶æ„æ˜¯ä¸€ä¸ªå¤„ç†SoundStreamè¡¨ç¤ºçš„encoder-decoder transformerï¼Œå¹¶åœ¨åˆæˆç”Ÿæˆçš„éŸ³é¢‘ç¤ºä¾‹å¯¹ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚ç ”ç©¶é€šè¿‡å®éªŒè¯„ä¼°äº†ç¼–è¾‘æè¿°ä¸­åŠ¨ä½œï¼ˆactionï¼‰ã€ç±»åˆ«ï¼ˆclassï¼‰å’Œæ—¶åºï¼ˆtimingï¼‰ä¿¡æ¯çš„å…³é”®ä½œç”¨ã€‚è¯¥ç ”ç©¶æˆæœè¡¨æ˜ï¼Œâ€œrecompositionâ€æ˜¯ç”Ÿæˆå¼éŸ³é¢‘ç¼–è¾‘é¢†åŸŸä¸­ä¸€ä¸ªå…·æœ‰é«˜åº¦å®ç”¨ä»·å€¼çš„åº”ç”¨æ–¹å‘ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "5 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.05256v1",
      "published_date": "2025-09-05 17:14:29 UTC",
      "updated_date": "2025-09-05 17:14:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:44:45.194022+00:00"
    },
    {
      "arxiv_id": "2509.05249v1",
      "title": "COGITAO: A Visual Reasoning Framework To Study Compositionality & Generalization",
      "title_zh": "COGITAOï¼šä¸€ç§ç”¨äºç ”ç©¶ç»„åˆæ€§ä¸æ³›åŒ–èƒ½åŠ›çš„è§†è§‰æ¨ç†æ¡†æ¶",
      "authors": [
        "Yassine Taoudi-Benchekroun",
        "Klim Troyan",
        "Pascal Sager",
        "Stefan Gerber",
        "Lukas Tuggener",
        "Benjamin Grewe"
      ],
      "abstract": "The ability to compose learned concepts and apply them in novel settings is key to human intelligence, but remains a persistent limitation in state-of-the-art machine learning models. To address this issue, we introduce COGITAO, a modular and extensible data generation framework and benchmark designed to systematically study compositionality and generalization in visual domains. Drawing inspiration from ARC-AGI's problem-setting, COGITAO constructs rule-based tasks which apply a set of transformations to objects in grid-like environments. It supports composition, at adjustable depth, over a set of 28 interoperable transformations, along with extensive control over grid parametrization and object properties. This flexibility enables the creation of millions of unique task rules -- surpassing concurrent datasets by several orders of magnitude -- across a wide range of difficulties, while allowing virtually unlimited sample generation per rule. We provide baseline experiments using state-of-the-art vision models, highlighting their consistent failures to generalize to novel combinations of familiar elements, despite strong in-domain performance. COGITAO is fully open-sourced, including all code and datasets, to support continued research in this field.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† COGITAOï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–ä¸”å¯æ‰©å±•çš„æ•°æ®ç”Ÿæˆæ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°ç ”ç©¶è§†è§‰é¢†åŸŸçš„ Compositionalityï¼ˆç»„åˆæ€§ï¼‰ä¸ Generalizationï¼ˆæ³›åŒ–ï¼‰èƒ½åŠ›ã€‚å— ARC-AGI ä»»åŠ¡è®¾ç½®çš„å¯å‘ï¼ŒCOGITAO åœ¨ç±»ç½‘æ ¼ç¯å¢ƒä¸­é€šè¿‡å¯¹ç‰©ä½“åº”ç”¨ 28 ç§å¯äº’æ“ä½œçš„å˜æ¢æ¥æ„å»ºåŸºäºè§„åˆ™çš„ä»»åŠ¡ï¼Œå¹¶æ”¯æŒå¯è°ƒèŠ‚æ·±åº¦çš„é€»è¾‘ç»„åˆã€‚è¯¥æ¡†æ¶å…·æœ‰æé«˜çš„çµæ´»æ€§ï¼Œèƒ½å¤Ÿç”Ÿæˆæ•°ç™¾ä¸‡ä¸ªç‹¬ç‰¹çš„ä»»åŠ¡è§„åˆ™ï¼Œåœ¨è§„æ¨¡å’Œéš¾åº¦è·¨åº¦ä¸Šå‡æ˜¾è‘—è¶…è¶Šäº†åŒç±»æ•°æ®é›†ã€‚åŸºå‡†å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„å…ˆè¿›è§†è§‰æ¨¡å‹è™½ç„¶åœ¨åŸŸå†…è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨é¢å¯¹ç†Ÿæ‚‰å…ƒç´ çš„æ–°é¢–ç»„åˆæ—¶ä¾ç„¶éš¾ä»¥å®ç°æœ‰æ•ˆæ³›åŒ–ã€‚COGITAO ç°å·²å®Œå…¨å¼€æºï¼Œä¸ºåç»­æ¢ç´¢æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨å¤æ‚é€»è¾‘æ¨ç†ä¸æ¦‚å¿µç»„åˆæ–¹é¢çš„ç ”ç©¶æä¾›äº†é‡è¦æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 main pages, 3 figure, appendix available",
      "pdf_url": "https://arxiv.org/pdf/2509.05249v1",
      "published_date": "2025-09-05 17:01:05 UTC",
      "updated_date": "2025-09-05 17:01:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:44:48.686955+00:00"
    },
    {
      "arxiv_id": "2509.05238v1",
      "title": "Uncertain but Useful: Leveraging CNN Variability into Data Augmentation",
      "title_zh": "ä¸ç¡®å®šäº¦æœ‰ç”¨ï¼šåˆ©ç”¨CNNå˜å¼‚æ€§è¿›è¡Œæ•°æ®å¢å¼º",
      "authors": [
        "InÃ©s Gonzalez-Pepe",
        "Vinuyan Sivakolunthu",
        "Yohan Chatelain",
        "Tristan Glatard"
      ],
      "abstract": "Deep learning (DL) is rapidly advancing neuroimaging by achieving state-of-the-art performance with reduced computation times. Yet the numerical stability of DL models -- particularly during training -- remains underexplored. While inference with DL is relatively stable, training introduces additional variability primarily through iterative stochastic optimization. We investigate this training-time variability using FastSurfer, a CNN-based whole-brain segmentation pipeline. Controlled perturbations are introduced via floating point perturbations and random seeds. We find that: (i) FastSurfer exhibits higher variability compared to that of a traditional neuroimaging pipeline, suggesting that DL inherits and is particularly susceptible to sources of instability present in its predecessors; (ii) ensembles generated with perturbations achieve performance similar to an unperturbed baseline; and (iii) variability effectively produces ensembles of numerical model families that can be repurposed for downstream applications. As a proof of concept, we demonstrate that numerical ensembles can be used as a data augmentation strategy for brain age regression. These findings position training-time variability not only as a reproducibility concern but also as a resource that can be harnessed to improve robustness and enable new applications in neuroimaging.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ (Deep Learning)æ¨¡å‹åœ¨ç¥ç»å½±åƒå­¦è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ•°å€¼ç¨³å®šæ€§(Numerical Stability)ï¼Œç‰¹åˆ«æ˜¯è¿­ä»£éšæœºä¼˜åŒ–å¸¦æ¥çš„å˜å¼‚æ€§ã€‚ç ”ç©¶å›¢é˜Ÿä»¥åŸºäºå·ç§¯ç¥ç»ç½‘ç»œ(CNN)çš„å…¨è„‘åˆ†å‰²å·¥å…· FastSurfer ä¸ºå¯¹è±¡ï¼Œé€šè¿‡å¼•å…¥æµ®ç‚¹æ‰°åŠ¨(Floating Point Perturbations)å’Œéšæœºç§å­(Random Seeds)è¿›è¡Œäº†æ§åˆ¶å®éªŒã€‚ç»“æœæ˜¾ç¤º FastSurfer çš„å˜å¼‚æ€§é«˜äºä¼ ç»Ÿç¥ç»å½±åƒæµç¨‹ï¼Œè¡¨æ˜æ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹ä¸ç¨³å®šå› ç´ æ›´ä¸ºæ•æ„Ÿã€‚å®éªŒè¯æ˜ï¼Œé€šè¿‡æ‰°åŠ¨äº§ç”Ÿçš„é›†æˆæ¨¡å‹(Ensembles)èƒ½å¤Ÿä¿æŒä¸åŸºçº¿ä¸€è‡´çš„æ€§èƒ½ï¼Œä¸”è¿™ç§å˜å¼‚æ€§å¯ä»¥äº§ç”Ÿå…·æœ‰å®ç”¨ä»·å€¼çš„æ•°å€¼æ¨¡å‹æ—ã€‚ä½œä¸ºæ ¸å¿ƒè´¡çŒ®ï¼Œè¯¥ç ”ç©¶éªŒè¯äº†åˆ©ç”¨æ•°å€¼é›†æˆä½œä¸ºæ•°æ®å¢å¼ºç­–ç•¥(Data Augmentation)åœ¨è„‘å¹´é¾„å›å½’(Brain Age Regression)ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¯¥å‘ç°å°†è®­ç»ƒå˜å¼‚æ€§ä»å•çº¯çš„å¯é‡å¤æ€§éšå¿§è½¬åŒ–ä¸ºæå‡é²æ£’æ€§å’Œèµ‹èƒ½æ–°åº”ç”¨çš„å®è´µèµ„æºã€‚",
      "categories": [
        "math.NA",
        "cs.AI"
      ],
      "primary_category": "math.NA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05238v1",
      "published_date": "2025-09-05 16:54:26 UTC",
      "updated_date": "2025-09-05 16:54:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:45:13.483893+00:00"
    },
    {
      "arxiv_id": "2509.05230v2",
      "title": "CURE: Controlled Unlearning for Robust Embeddings -- Mitigating Conceptual Shortcuts in Pre-Trained Language Models",
      "title_zh": "CUREï¼šé¢å‘é²æ£’åµŒå…¥çš„å—æ§é—å¿˜â€”â€”ç¼“è§£é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­çš„æ¦‚å¿µæ·å¾„",
      "authors": [
        "Aysenur Kocak",
        "Shuo Yang",
        "Bardh Prenkaj",
        "Gjergji Kasneci"
      ],
      "abstract": "Pre-trained language models have achieved remarkable success across diverse applications but remain susceptible to spurious, concept-driven correlations that impair robustness and fairness. In this work, we introduce CURE, a novel and lightweight framework that systematically disentangles and suppresses conceptual shortcuts while preserving essential content information. Our method first extracts concept-irrelevant representations via a dedicated content extractor reinforced by a reversal network, ensuring minimal loss of task-relevant information. A subsequent controllable debiasing module employs contrastive learning to finely adjust the influence of residual conceptual cues, enabling the model to either diminish harmful biases or harness beneficial correlations as appropriate for the target task. Evaluated on the IMDB and Yelp datasets using three pre-trained architectures, CURE achieves an absolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp, while introducing minimal computational overhead. Our approach establishes a flexible, unsupervised blueprint for combating conceptual biases, paving the way for more reliable and fair language understanding systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CUREæ¡†æ¶ï¼Œä¸€ç§æ—¨åœ¨å‡è½»é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹(Pre-trained Language Models)ä¸­æ¦‚å¿µæ·å¾„(Conceptual Shortcuts)å¯¹é²æ£’æ€§å’Œå…¬å¹³æ€§è´Ÿé¢å½±å“çš„è½»é‡åŒ–æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸€ä¸ªç”±é€†å‘ç½‘ç»œ(Reversal Network)å¢å¼ºçš„ä¸“ç”¨å†…å®¹æå–å™¨ï¼Œåœ¨ä¿ç•™ä»»åŠ¡ç›¸å…³ä¿¡æ¯çš„åŒæ—¶æå–ä¸æ¦‚å¿µæ— å…³çš„è¡¨ç¤ºã€‚éšåï¼Œå…¶å¯æ§å»åæ¨¡å—åˆ©ç”¨å¯¹æ¯”å­¦ä¹ (Contrastive Learning)ç²¾ç»†è°ƒèŠ‚æ®‹ä½™æ¦‚å¿µçº¿ç´¢çš„å½±å“ï¼Œä½¿æ¨¡å‹èƒ½æ ¹æ®ç›®æ ‡ä»»åŠ¡çµæ´»åœ°å‰Šå¼±æœ‰å®³åè§æˆ–åˆ©ç”¨æœ‰ç›Šå…³è”ã€‚åœ¨IMDBå’ŒYelpæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCUREåœ¨IMDBä¸Šçš„F1åˆ†æ•°æå‡äº†10ä¸ªç™¾åˆ†ç‚¹ï¼Œåœ¨Yelpä¸Šæå‡äº†2ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”è®¡ç®—å¼€é”€æä½ã€‚ä½œä¸ºä¸€ç§çµæ´»çš„æ— ç›‘ç£æ–¹æ¡ˆï¼ŒCUREä¸ºæ„å»ºæ›´å¯é ã€æ›´å…¬å¹³çš„è¯­è¨€ç†è§£ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at the Conference on Empirical Methods in Natural Language Processing (EMNLP 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.05230v2",
      "published_date": "2025-09-05 16:47:22 UTC",
      "updated_date": "2025-09-10 17:32:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:45:08.854279+00:00"
    },
    {
      "arxiv_id": "2509.09704v1",
      "title": "Temporal Preferences in Language Models for Long-Horizon Assistance",
      "title_zh": "é¢å‘é•¿ç¨‹è¾…åŠ©çš„è¯­è¨€æ¨¡å‹æ—¶é—´åå¥½",
      "authors": [
        "Ali Mazyaki",
        "Mohammad Naghizadeh",
        "Samaneh Ranjkhah Zonouzaghi",
        "Hossein Setareh"
      ],
      "abstract": "We study whether language models (LMs) exhibit future- versus present-oriented preferences in intertemporal choice and whether those preferences can be systematically manipulated. Using adapted human experimental protocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them against a sample of human decision makers. We introduce an operational metric, the Manipulability of Time Orientation (MTO), defined as the change in an LM's revealed time preference between future- and present-oriented prompts. In our tests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini) choose later options under future-oriented prompts but only partially personalize decisions across identities or geographies. Moreover, models that correctly reason about time orientation internalize a future orientation for themselves as AI decision makers. We discuss design implications for AI assistants that should align with heterogeneous, long-horizon goals and outline a research agenda on personalized contextual calibration and socially aware deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ¨¡å‹(Language Models)åœ¨è·¨æœŸå†³ç­–(intertemporal choice)ä¸­è¡¨ç°å‡ºçš„æœªæ¥å¯¼å‘ä¸ç°åœ¨å¯¼å‘åå¥½ï¼Œå¹¶åˆ†æäº†è¿™äº›åå¥½æ˜¯å¦å¯ä»¥è¢«ç³»ç»Ÿæ€§åœ°æ“çºµã€‚é€šè¿‡é‡‡ç”¨äººç±»å®éªŒæ–¹æ¡ˆå’Œæ—¶é—´æƒè¡¡ä»»åŠ¡(time-tradeoff tasks)ï¼Œç ”ç©¶äººå‘˜åœ¨å¤šä¸ªæ¨¡å‹ä¸Šè¯„ä¼°äº†æ—¶é—´åå¥½å¹¶å°†å…¶ä¸äººç±»å†³ç­–è€…è¿›è¡ŒåŸºå‡†å¯¹æ¯”ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªè¡¡é‡æŒ‡æ ‡â€”â€”æ—¶é—´å¯¼å‘å¯æ“çºµæ€§(Manipulability of Time Orientation, MTO)ï¼Œç”¨ä»¥é‡åŒ–æ¨¡å‹åœ¨æœªæ¥æˆ–ç°åœ¨å¯¼å‘æç¤ºä¸‹åå¥½çš„å˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸“æ³¨äºæ¨ç†çš„æ¨¡å‹ï¼ˆå¦‚DeepSeek-Reasonerå’Œgrok-3-miniï¼‰åœ¨æœªæ¥å¯¼å‘æç¤ºä¸‹æ›´å€¾å‘äºé€‰æ‹©è¿œæœŸå›æŠ¥ï¼Œä½†åœ¨è·¨èº«ä»½æˆ–åœ°ç†èƒŒæ™¯çš„ä¸ªæ€§åŒ–å†³ç­–æ–¹é¢è¡¨ç°æœ‰é™ã€‚æ­¤å¤–ï¼Œèƒ½å¤Ÿæ­£ç¡®æ¨ç†æ—¶é—´å¯¼å‘çš„æ¨¡å‹åœ¨æ‰®æ¼”AIå†³ç­–è€…è§’è‰²æ—¶ï¼Œå¾€å¾€ä¼šè¡¨ç°å‡ºå†…åŒ–çš„æœªæ¥å¯¼å‘åå¥½ã€‚è¿™é¡¹å·¥ä½œä¸ºAIåŠ©æ‰‹å¦‚ä½•ä¸å¼‚è´¨åŒ–çš„é•¿è¿œç›®æ ‡å¯¹é½æä¾›äº†è®¾è®¡å‚è€ƒï¼Œå¹¶æŒ‡æ˜äº†å…³äºä¸ªæ€§åŒ–æƒ…å¢ƒæ ¡å‡†å’Œç¤¾äº¤æ„ŸçŸ¥éƒ¨ç½²çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09704v1",
      "published_date": "2025-09-05 16:21:23 UTC",
      "updated_date": "2025-09-05 16:21:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:45:10.957928+00:00"
    },
    {
      "arxiv_id": "2509.05218v2",
      "title": "HoPE: Hyperbolic Rotary Positional Encoding for Stable Long-Range Dependency Modeling in Large Language Models",
      "title_zh": "HoPEï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹ç¨³å®šé•¿ç¨‹ä¾èµ–å»ºæ¨¡çš„åŒæ›²æ—‹è½¬ä½ç½®ç¼–ç ",
      "authors": [
        "Chang Dai",
        "Hongyu Shan",
        "Mingyang Song",
        "Di Liang"
      ],
      "abstract": "Positional encoding mechanisms enable Transformers to model sequential structure and long-range dependencies in text. While absolute positional encodings struggle with extrapolation to longer sequences due to fixed positional representations, and relative approaches like Alibi exhibit performance degradation on extremely long contexts, the widely-used Rotary Positional Encoding (RoPE) introduces oscillatory attention patterns that hinder stable long-distance dependency modelling. We address these limitations through a geometric reformulation of positional encoding. Drawing inspiration from Lorentz transformations in hyperbolic geometry, we propose Hyperbolic Rotary Positional Encoding (HoPE), which leverages hyperbolic functions to implement Lorentz rotations on token representations. Theoretical analysis demonstrates that RoPE is a special case of our generalized formulation. HoPE fundamentally resolves RoPE's slation issues by enforcing monotonic decay of attention weights with increasing token distances. Extensive experimental results, including perplexity evaluations under several extended sequence benchmarks, show that HoPE consistently exceeds existing positional encoding methods. These findings underscore HoPE's enhanced capacity for representing and generalizing long-range dependencies. Data and code will be available.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ä¸­ Rotary Positional Encoding (RoPE) å› éœ‡è¡æ³¨æ„åŠ›æ¨¡å¼è€Œéš¾ä»¥ç¨³å®šå»ºæ¨¡é•¿ç¨‹ä¾èµ–çš„é—®é¢˜ï¼Œæå‡ºäº† Hyperbolic Rotary Positional Encoding (HoPE)ã€‚è¯¥æ–¹æ³•ä»åŒæ›²å‡ ä½•ä¸­çš„ Lorentz transformations ä¸­æ±²å–çµæ„Ÿï¼Œé€šè¿‡å‡ ä½•é‡æ„åˆ©ç”¨åŒæ›²å‡½æ•°åœ¨ token è¡¨ç¤ºä¸Šå®ç° Lorentz rotationsã€‚ç†è®ºåˆ†æè¯æ˜ RoPE æ˜¯è¯¥é€šç”¨å…¬å¼çš„ä¸€ä¸ªç‰¹ä¾‹ï¼Œè€Œ HoPE é€šè¿‡å¼ºåˆ¶æ³¨æ„åŠ›æƒé‡éš token è·ç¦»å¢åŠ è€Œå•è°ƒè¡°å‡ï¼Œä»æ ¹æœ¬ä¸Šè§£å†³äº† RoPE çš„éœ‡è¡é—®é¢˜ã€‚åœ¨å¤šä¸ªæ‰©å±•åºåˆ—åŸºå‡†æµ‹è¯•ä¸‹çš„ perplexity è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒHoPE çš„è¡¨ç°ä¸€è‡´ä¼˜äºç°æœ‰çš„ä½ç½®ç¼–ç æ–¹æ³•ã€‚è¿™äº›å®éªŒç»“æœå……åˆ†è¯æ˜äº† HoPE åœ¨è¡¨ç¤ºå’Œæ³›åŒ–é•¿ç¨‹ä¾èµ–å…³ç³»æ–¹é¢çš„å“è¶Šèƒ½åŠ›ï¼Œä¸ºè¶…é•¿ä¸Šä¸‹æ–‡å¤„ç†æä¾›äº†æ›´ç¨³å®šçš„å‡ ä½•åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05218v2",
      "published_date": "2025-09-05 16:20:48 UTC",
      "updated_date": "2025-09-08 03:13:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:45:18.261372+00:00"
    },
    {
      "arxiv_id": "2509.05207v1",
      "title": "RapidGNN: Energy and Communication-Efficient Distributed Training on Large-Scale Graph Neural Networks",
      "title_zh": "RapidGNNï¼šå¤§è§„æ¨¡å›¾ç¥ç»ç½‘ç»œçš„é«˜èƒ½æ•ˆä¸é€šä¿¡é«˜æ•ˆåˆ†å¸ƒå¼è®­ç»ƒ",
      "authors": [
        "Arefin Niam",
        "Tevfik Kosar",
        "M S Q Zulkar Nine"
      ],
      "abstract": "Graph Neural Networks (GNNs) have become popular across a diverse set of tasks in exploring structural relationships between entities. However, due to the highly connected structure of the datasets, distributed training of GNNs on large-scale graphs poses significant challenges. Traditional sampling-based approaches mitigate the computational loads, yet the communication overhead remains a challenge. This paper presents RapidGNN, a distributed GNN training framework with deterministic sampling-based scheduling to enable efficient cache construction and prefetching of remote features. Evaluation on benchmark graph datasets demonstrates RapidGNN's effectiveness across different scales and topologies. RapidGNN improves end-to-end training throughput by 2.46x to 3.00x on average over baseline methods across the benchmark datasets, while cutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further demonstrates near-linear scalability with an increasing number of computing units efficiently. Furthermore, it achieves increased energy efficiency over the baseline methods for both CPU and GPU by 44% and 32%, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RapidGNNï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³å¤§è§„æ¨¡å›¾ç¥ç»ç½‘ç»œ(Graph Neural Networks, GNNs)åˆ†å¸ƒå¼è®­ç»ƒä¸­é€šä¿¡å¼€é”€æŒ‘æˆ˜çš„æ¡†æ¶ã€‚RapidGNNé‡‡ç”¨äº†åŸºäºç¡®å®šæ€§é‡‡æ ·çš„è°ƒåº¦(deterministic sampling-based scheduling)æŠ€æœ¯ï¼Œå®ç°äº†é«˜æ•ˆçš„ç¼“å­˜æ„å»º(cache construction)å’Œè¿œç¨‹ç‰¹å¾é¢„å–(prefetching of remote features)ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„ç«¯åˆ°ç«¯è®­ç»ƒååé‡æ¯”åŸºçº¿æ–¹æ³•å¹³å‡æé«˜äº†2.46å€è‡³3.00å€ï¼ŒåŒæ—¶å°†è¿œç¨‹ç‰¹å¾è·å–é‡æ˜¾è‘—å‡å°‘äº†9.70å€è‡³15.39å€ã€‚æ­¤å¤–ï¼ŒRapidGNNå±•ç¤ºäº†è¿‘ä¹çº¿æ€§çš„å¯æ‰©å±•æ€§ï¼Œå¹¶æœ‰æ•ˆæå‡äº†ç¡¬ä»¶èƒ½æ•ˆï¼Œåœ¨CPUå’ŒGPUä¸Šåˆ†åˆ«å®ç°äº†44%å’Œ32%çš„èƒ½æ•ˆå¢é•¿ã€‚è¯¥æ¡†æ¶ä¸ºå®ç°é«˜æ•ˆã€ä½èƒ½è€—çš„å¤§è§„æ¨¡åˆ†å¸ƒå¼GNNè®­ç»ƒæä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: text overlap with arXiv:2505.10806",
      "pdf_url": "https://arxiv.org/pdf/2509.05207v1",
      "published_date": "2025-09-05 16:10:20 UTC",
      "updated_date": "2025-09-05 16:10:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:45:16.757859+00:00"
    },
    {
      "arxiv_id": "2509.05198v1",
      "title": "Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet",
      "title_zh": "åˆ©ç”¨ ModelNet-R ä¸ Point-SkipNet æå‡ 3D ç‚¹äº‘åˆ†ç±»æ€§èƒ½",
      "authors": [
        "Mohammad Saeid",
        "Amir Salarpour",
        "Pedram MohajerAnsari"
      ],
      "abstract": "The classification of 3D point clouds is crucial for applications such as autonomous driving, robotics, and augmented reality. However, the commonly used ModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D data, size mismatches, and inadequate class differentiation, which hinder model performance. This paper introduces ModelNet-R, a meticulously refined version of ModelNet40 designed to address these issues and serve as a more reliable benchmark. Additionally, this paper proposes Point-SkipNet, a lightweight graph-based neural network that leverages efficient sampling, neighborhood grouping, and skip connections to achieve high classification accuracy with reduced computational overhead. Extensive experiments demonstrate that models trained in ModelNet-R exhibit significant performance improvements. Notably, Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a substantially lower parameter count compared to contemporary models. This research highlights the crucial role of dataset quality in optimizing model efficiency for 3D point cloud classification. For more details, see the code at: https://github.com/m-saeid/ModeNetR_PointSkipNet.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ModelNet40æ•°æ®é›†å­˜åœ¨çš„æ ‡ç­¾ä¸ä¸€è‡´ã€2Dæ•°æ®å†—ä½™åŠç±»é—´åŒºåˆ†åº¦ä¸è¶³ç­‰å±€é™æ€§ï¼Œæå‡ºäº†ç²¾ç»†åŒ–æ”¹è¿›åçš„ç‰ˆæœ¬ModelNet-Rï¼Œæ—¨åœ¨ä¸º3Dç‚¹äº‘åˆ†ç±»æä¾›æ›´å¯é çš„åŸºå‡†ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„åŸºäºå›¾çš„ç¥ç»ç½‘ç»œPoint-SkipNetï¼Œè¯¥æ¶æ„é€šè¿‡é«˜æ•ˆé‡‡æ ·(sampling)ã€é‚»åŸŸåˆ†ç»„(neighborhood grouping)å’Œè·³è·ƒè¿æ¥(skip connections)æŠ€æœ¯ï¼Œåœ¨å¤§å¹…é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶å®ç°äº†é«˜åˆ†ç±»ç²¾åº¦ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ModelNet-Rä¸Šè®­ç»ƒçš„æ¨¡å‹å‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…¶ä¸­ï¼ŒPoint-SkipNetåœ¨ModelNet-Rä¸Šä»¥è¿œä½äºå½“ä»£æ¨¡å‹çš„å‚æ•°é‡è¾¾åˆ°äº†State-of-the-artçš„å‡†ç¡®ç‡ã€‚è¯¥ç ”ç©¶æˆæœå¼ºè°ƒäº†é«˜è´¨é‡æ•°æ®é›†åœ¨æå‡3Dç‚¹äº‘åˆ†ç±»æ¨¡å‹æ•ˆç‡æ–¹é¢çš„æ ¸å¿ƒä½œç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper has been accepted for presentation at the 7th International Conference on Pattern Recognition and Image Analysis (IPRIA 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.05198v1",
      "published_date": "2025-09-05 15:57:36 UTC",
      "updated_date": "2025-09-05 15:57:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:45:30.161818+00:00"
    },
    {
      "arxiv_id": "2509.05197v1",
      "title": "AI Agents for Web Testing: A Case Study in the Wild",
      "title_zh": "Web æµ‹è¯•ä¸­çš„ AI æ™ºèƒ½ä½“ï¼šçœŸå®åœºæ™¯ä¸‹çš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Naimeng Ye",
        "Xiao Yu",
        "Ruize Xu",
        "Tianyi Peng",
        "Zhou Yu"
      ],
      "abstract": "Automated web testing plays a critical role in ensuring high-quality user experiences and delivering business value. Traditional approaches primarily focus on code coverage and load testing, but often fall short of capturing complex user behaviors, leaving many usability issues undetected. The emergence of large language models (LLM) and AI agents opens new possibilities for web testing by enabling human-like interaction with websites and a general awareness of common usability problems. In this work, we present WebProber, a prototype AI agent-based web testing framework. Given a URL, WebProber autonomously explores the website, simulating real user interactions, identifying bugs and usability issues, and producing a human-readable report. We evaluate WebProber through a case study of 120 academic personal websites, where it uncovered 29 usability issues--many of which were missed by traditional tools. Our findings highlight agent-based testing as a promising direction while outlining directions for developing next-generation, user-centered testing frameworks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿç½‘é¡µæµ‹è¯•éš¾ä»¥æ•æ‰å¤æ‚ç”¨æˆ·è¡Œä¸ºå’Œå¯ç”¨æ€§é—®é¢˜ç­‰å±€é™ï¼Œæå‡ºäº†åŸºäº AI agents çš„è‡ªåŠ¨åŒ–æµ‹è¯•æ¡†æ¶ WebProberã€‚è¯¥åŸå‹æ¡†æ¶åœ¨ç»™å®š URL åèƒ½å¤Ÿè‡ªä¸»æ¢ç´¢ç½‘ç«™å¹¶æ¨¡æ‹ŸçœŸå®ç”¨æˆ·äº¤äº’ï¼Œä»è€Œè¯†åˆ« bugs ä¸å¯ç”¨æ€§ç¼ºé™·ã€‚WebProber åˆ©ç”¨ Large Language Models (LLM) çš„é€šç”¨èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«é—®é¢˜å¹¶ç”Ÿæˆäººç±»å¯è¯»çš„æµ‹è¯•æŠ¥å‘Šã€‚é€šè¿‡å¯¹ 120 ä¸ªå­¦æœ¯ä¸ªäººç½‘ç«™çš„æ¡ˆä¾‹ç ”ç©¶ï¼ŒWebProber æˆåŠŸå‘ç°äº† 29 ä¸ªä¼ ç»Ÿå·¥å…·æœªèƒ½æ£€æµ‹åˆ°çš„å¯ç”¨æ€§é—®é¢˜ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åŸºäº agent çš„æµ‹è¯•åœ¨æ„å»ºä¸‹ä¸€ä»£ user-centered æµ‹è¯•æ¡†æ¶æ–¹é¢å…·æœ‰æ˜¾è‘—çš„æ½œåŠ›å’Œåº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05197v1",
      "published_date": "2025-09-05 15:57:16 UTC",
      "updated_date": "2025-09-05 15:57:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:45:37.068430+00:00"
    },
    {
      "arxiv_id": "2509.05190v1",
      "title": "Accuracy-Constrained CNN Pruning for Efficient and Reliable EEG-Based Seizure Detection",
      "title_zh": "é¢å‘é«˜æ•ˆå¯é EEGç™«ç—«æ£€æµ‹çš„ç²¾åº¦çº¦æŸCNNå‰ªæ",
      "authors": [
        "Mounvik K",
        "N Harshit"
      ],
      "abstract": "Deep learning models, especially convolutional neural networks (CNNs), have shown considerable promise for biomedical signals such as EEG-based seizure detection. However, these models come with challenges, primarily due to their size and compute requirements in environments where real-time detection or limited resources are available. In this study, we present a lightweight one-dimensional CNN model with structured pruning to improve efficiency and reliability. The model was trained with mild early stopping to address possible overfitting, achieving an accuracy of 92.78% and a macro-F1 score of 0.8686. Structured pruning of the baseline CNN involved removing 50% of the convolutional kernels based on their importance to model predictions. Surprisingly, after pruning the weights and memory by 50%, the new network was still able to maintain predictive capabilities, while modestly increasing precision to 92.87% and improving the macro-F1 score to 0.8707. Overall, we present a convincing case that structured pruning removes redundancy, improves generalization, and, in combination with mild early stopping, achieves a promising way forward to improve seizure detection efficiency and reliability, which is clear motivation for resource-limited settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§è½»é‡çº§çš„ 1D CNN æ¨¡å‹ï¼Œé€šè¿‡ç»“æ„åŒ–å‰ªæ (Structured Pruning) æŠ€æœ¯æ¥æå‡åŸºäº EEG çš„ç™«ç—«æ£€æµ‹ (Seizure Detection) åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„æ•ˆç‡ä¸å¯é æ€§ã€‚ä¸ºäº†è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œç ”ç©¶äººå‘˜åœ¨è®­ç»ƒä¸­ç»“åˆäº†é€‚åº¦çš„æ—©åœ (Early Stopping) ç­–ç•¥ï¼Œå¹¶ä¾æ®å·ç§¯æ ¸çš„é‡è¦æ€§ç§»é™¤äº† 50% çš„å·ç§¯æ ¸å‚æ•°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºå‡†æ¨¡å‹åˆå§‹å‡†ç¡®ç‡ä¸º 92.78%ï¼Œè€Œå‰ªæåçš„ç½‘ç»œåœ¨æƒé‡é‡å’Œå†…å­˜éœ€æ±‚å‡åŠçš„åŒæ—¶ï¼Œä¾ç„¶ä¿æŒäº†ä¼˜å¼‚çš„é¢„æµ‹æ€§èƒ½ã€‚ä»¤äººå…³æ³¨çš„æ˜¯ï¼Œå‰ªæåçš„æ¨¡å‹ç²¾åº¦å¾®å¢è‡³ 92.87%ï¼Œä¸” macro-F1 åˆ†æ•°ä» 0.8686 æå‡è‡³ 0.8707ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æ¶ˆé™¤å†—ä½™å’Œå¢å¼ºæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æ˜¾è‘—ä½œç”¨ã€‚è¿™é¡¹å·¥ä½œä¸ºå®æ—¶ã€èµ„æºå—é™çš„ç”Ÿç‰©åŒ»å­¦ä¿¡å·å¤„ç†åœºæ™¯æä¾›äº†ä¸€ç§æ›´é«˜æ•ˆä¸”å¯é çš„æ·±åº¦å­¦ä¹ ä¼˜åŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05190v1",
      "published_date": "2025-09-05 15:42:15 UTC",
      "updated_date": "2025-09-05 15:42:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:45:30.960574+00:00"
    },
    {
      "arxiv_id": "2509.16215v2",
      "title": "Discovering Software Parallelization Points Using Deep Neural Networks",
      "title_zh": "åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„è½¯ä»¶å¹¶è¡ŒåŒ–ç‚¹å‘ç°",
      "authors": [
        "Izavan dos S. Correia",
        "Henrique C. T. Santos",
        "Tiago A. E. Ferreira"
      ],
      "abstract": "This study proposes a deep learning-based approach for discovering loops in programming code according to their potential for parallelization. Two genetic algorithm-based code generators were developed to produce two distinct types of code: (i) independent loops, which are parallelizable, and (ii) ambiguous loops, whose dependencies are unclear, making them impossible to define if the loop is parallelizable or not. The generated code snippets were tokenized and preprocessed to ensure a robust dataset. Two deep learning models - a Deep Neural Network (DNN) and a Convolutional Neural Network (CNN) - were implemented to perform the classification. Based on 30 independent runs, a robust statistical analysis was employed to verify the expected performance of both models, DNN and CNN. The CNN showed a slightly higher mean performance, but the two models had a similar variability. Experiments with varying dataset sizes highlighted the importance of data diversity for model performance. These results demonstrate the feasibility of using deep learning to automate the identification of parallelizable structures in code, offering a promising tool for software optimization and performance improvement.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡è¯†åˆ«ç¨‹åºä»£ç ä¸­çš„å¾ªç¯ç»“æ„æ¥å‘ç°æ½œåœ¨çš„å¯å¹¶è¡ŒåŒ–ç‚¹ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸¤ä¸ªåŸºäºé—ä¼ ç®—æ³• (Genetic Algorithm) çš„ä»£ç ç”Ÿæˆå™¨ï¼Œåˆ†åˆ«ç”Ÿæˆå¯å¹¶è¡ŒåŒ–çš„ç‹¬ç«‹å¾ªç¯ (Independent Loops) å’Œä¾èµ–å…³ç³»ä¸æ˜ç¡®çš„æ¨¡ç³Šå¾ªç¯ (Ambiguous Loops)ã€‚åœ¨å¯¹ä»£ç ç‰‡æ®µè¿›è¡Œæ ‡è®°åŒ– (Tokenization) å¤„ç†åï¼Œåˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œ (DNN) å’Œå·ç§¯ç¥ç»ç½‘ç»œ (CNN) æ¨¡å‹è¿›è¡Œåˆ†ç±»å®éªŒã€‚ç»è¿‡30æ¬¡ç‹¬ç«‹è¿è¡Œçš„ç»Ÿè®¡åˆ†æï¼Œç»“æœæ˜¾ç¤º CNN çš„å¹³å‡æ€§èƒ½ç•¥é«˜äº DNNï¼Œä¸”ä¸¤è€…çš„è¡¨ç°å˜å¼‚æ€§ç›¸ä¼¼ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥å¼ºè°ƒäº†æ•°æ®å¤šæ ·æ€§å¯¹æå‡æ¨¡å‹æ€§èƒ½çš„å…³é”®ä½œç”¨ã€‚è¯¥ç ”ç©¶éªŒè¯äº†åˆ©ç”¨æ·±åº¦å­¦ä¹ è‡ªåŠ¨åŒ–è¯†åˆ«å¹¶è¡Œç»“æ„çš„å¯è¡Œæ€§ï¼Œä¸ºè½¯ä»¶ä¼˜åŒ–å’Œæ€§èƒ½æå‡æä¾›äº†æœ‰æ•ˆçš„è‡ªåŠ¨åŒ–å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.NE",
        "cs.PL",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.16215v2",
      "published_date": "2025-09-05 15:32:23 UTC",
      "updated_date": "2025-10-01 23:33:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:45:30.755468+00:00"
    },
    {
      "arxiv_id": "2509.05399v1",
      "title": "Graph Connectionist Temporal Classification for Phoneme Recognition",
      "title_zh": "é¢å‘éŸ³ç´ è¯†åˆ«çš„å›¾è”ç»“ä¸»ä¹‰æ—¶é—´åˆ†ç±»",
      "authors": [
        "Henry GrafÃ©",
        "Hugo Van hamme"
      ],
      "abstract": "Automatic Phoneme Recognition (APR) systems are often trained using pseudo phoneme-level annotations generated from text through Grapheme-to-Phoneme (G2P) systems. These G2P systems frequently output multiple possible pronunciations per word, but the standard Connectionist Temporal Classification (CTC) loss cannot account for such ambiguity during training. In this work, we adapt Graph Temporal Classification (GTC) to the APR setting. GTC enables training from a graph of alternative phoneme sequences, allowing the model to consider multiple pronunciations per word as valid supervision. Our experiments on English and Dutch data sets show that incorporating multiple pronunciations per word into the training loss consistently improves phoneme error rates compared to a baseline trained with CTC. These results suggest that integrating pronunciation variation into the loss function is a promising strategy for training APR systems from noisy G2P-based supervision.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªåŠ¨éŸ³ç´ è¯†åˆ«(APR)ç³»ç»Ÿä¸­åˆ©ç”¨Grapheme-to-Phoneme(G2P)ç”Ÿæˆçš„ä¼ªæ ‡æ³¨è¿›è¡Œè®­ç»ƒæ—¶ï¼ŒConnectionist Temporal Classification(CTC)æŸå¤±å‡½æ•°æ— æ³•å¤„ç†è¯è¯­å¤šéŸ³æ€§æ­§ä¹‰çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…å°†å›¾è¿æ¥æ—¶é—´åˆ†ç±»(Graph Temporal Classification, GTC)å¼•å…¥APRä»»åŠ¡ï¼Œå…è®¸æ¨¡å‹ä»åŒ…å«å¤šç§å¯é€‰éŸ³ç´ åºåˆ—çš„å›¾ç»“æ„ä¸­å­¦ä¹ ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒGTCå¯ä»¥å°†ä¸€ä¸ªå•è¯çš„å¤šç§å¯èƒ½å‘éŸ³å‡è§†ä¸ºæœ‰æ•ˆçš„ç›‘ç£ä¿¡å·ï¼Œä»è€Œåº”å¯¹è®­ç»ƒä¸­çš„ä¸ç¡®å®šæ€§ã€‚åœ¨è‹±è¯­å’Œè·å…°è¯­æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„CTCåŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼Œæ•´åˆå¤šç§å‘éŸ³å˜ä½“çš„GTCæ˜¾è‘—é™ä½äº†éŸ³ç´ é”™è¯¯ç‡ã€‚è¿™ä¸€ç»“æœè¯æ˜äº†å°†å‘éŸ³å¤šæ ·æ€§èå…¥æŸå¤±å‡½æ•°æ˜¯æå‡åŸºäºå™ªå£°G2Pç›‘ç£çš„APRç³»ç»Ÿæ€§èƒ½çš„æœ‰æ•ˆç­–ç•¥ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.05399v1",
      "published_date": "2025-09-05 15:20:59 UTC",
      "updated_date": "2025-09-05 15:20:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:45:38.462838+00:00"
    },
    {
      "arxiv_id": "2509.10535v1",
      "title": "Semantic-guided LoRA Parameters Generation",
      "title_zh": "è¯­ä¹‰å¼•å¯¼çš„ LoRA å‚æ•°ç”Ÿæˆ",
      "authors": [
        "Miaoge Li",
        "Yang Chen",
        "Zhijie Rao",
        "Can Jiang",
        "Jingcai Guo"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) has demonstrated strong generalization capabilities across a variety of tasks for efficiently fine-tuning AI models, especially on resource-constrained edges. However, in real-world applications, edge users often exhibit task-specific preferences that are difficult to handle with a unified model trained under a closed-world assumption, and the challenge may further increase when there are significant domain shifts between training and deployment. Meanwhile, retraining/fine-tuning models for each user is also impractical due to its cost-intensive nature and privacy concerns over raw data utilization from edges. To address these challenges, we propose Semantic-guided LoRA Parameter Generation (SG-LoRA), the first of its kind framework to efficiently produce user-specific LoRA parameters without any additional training on user tasks or access to user-specific data. Concretely, SG-LoRA uses task descriptions as the semantic bridge, measuring their proximity to a set of known expert tasks in a shared embedding space. Based on this semantic guidance, it models the target task's LoRA parameter distribution to generate high-performing parameters for novel tasks. SG-LoRA enables the real-time construction of LoRA models aligned with individual intents by distilling knowledge from prominent LoRA experts and, meanwhile, offering a privacy-preserving solution for personalized model adaptation in a novel zero-shot open-world setting proposed in this work. Extensive experiments on multiple challenging tasks confirm the superior performance and remarkable adaptability of SG-LoRA. Code is available at https://github.com/keepgoingjkg/SG-LoRA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Semantic-guided LoRA Parameter Generation (SG-LoRA)æ¡†æ¶ï¼Œè¿™æ˜¯é¦–ä¸ªåœ¨æ— éœ€é¢å¤–è®­ç»ƒæˆ–è®¿é—®ç”¨æˆ·ç§æœ‰æ•°æ®çš„å‰æä¸‹ï¼Œä¸ºè¾¹ç¼˜ç”¨æˆ·é«˜æ•ˆç”Ÿæˆç‰¹å®šLow-Rank Adaptation (LoRA)å‚æ•°çš„ç³»ç»Ÿã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä»»åŠ¡æè¿°ä½œä¸ºè¯­ä¹‰æ¡¥æ¢ï¼Œé€šè¿‡è¡¡é‡ç›®æ ‡ä»»åŠ¡ä¸å·²çŸ¥ä¸“å®¶ä»»åŠ¡åœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸­çš„è¯­ä¹‰æ¥è¿‘åº¦æ¥æä¾›å¼•å¯¼ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿå¾®è°ƒæˆæœ¬é«˜æ˜‚åŠæ½œåœ¨çš„éšç§æ³„éœ²é—®é¢˜ã€‚SG-LoRAé€šè¿‡ä»å¤šä¸ªLoRAä¸“å®¶æ¨¡å‹ä¸­è’¸é¦çŸ¥è¯†ï¼Œå¯¹ç›®æ ‡ä»»åŠ¡çš„LoRAå‚æ•°åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œåœ¨é›¶æ ·æœ¬å¼€æ”¾ä¸–ç•Œ(zero-shot open-world)è®¾å®šä¸‹å®ç°æ¨¡å‹çš„å®æ—¶ä¸ªæ€§åŒ–é€‚é…ã€‚è¯¥æ¡†æ¶ä¸ä»…æ”¯æŒæ ¹æ®ç”¨æˆ·æ„å›¾ç²¾å‡†å¯¹é½æ¨¡å‹ï¼Œè¿˜èƒ½æœ‰æ•ˆåº”å¯¹éƒ¨ç½²è¿‡ç¨‹ä¸­çš„é¢†åŸŸåç§»(Domain Shifts)æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSG-LoRAåœ¨å¤šç§å¤æ‚ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½å’Œæå¼ºçš„é€‚åº”æ€§ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„éšç§ä¿æŠ¤æ¨¡å‹å®šåˆ¶æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.10535v1",
      "published_date": "2025-09-05 14:43:41 UTC",
      "updated_date": "2025-09-05 14:43:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:45:51.991821+00:00"
    },
    {
      "arxiv_id": "2509.05145v1",
      "title": "Exploring Situated Stabilities of a Rhythm Generation System through Variational Cross-Examination",
      "title_zh": "é€šè¿‡å˜å¼‚äº¤å‰æ£€éªŒæ¢ç©¶èŠ‚å¥ç”Ÿæˆç³»ç»Ÿçš„æƒ…å¢ƒåŒ–ç¨³å®šæ€§",
      "authors": [
        "BÅ‚aÅ¼ej Kotowski",
        "Nicholas Evans",
        "Behzad Haki",
        "Frederic Font",
        "Sergi JordÃ "
      ],
      "abstract": "This paper investigates GrooveTransformer, a real-time rhythm generation system, through the postphenomenological framework of Variational Cross-Examination (VCE). By reflecting on its deployment across three distinct artistic contexts, we identify three stabilities: an autonomous drum accompaniment generator, a rhythmic control voltage sequencer in Eurorack format, and a rhythm driver for a harmonic accompaniment system. The versatility of its applications was not an explicit goal from the outset of the project. Thus, we ask: how did this multistability emerge? Through VCE, we identify three key contributors to its emergence: the affordances of system invariants, the interdisciplinary collaboration, and the situated nature of its development. We conclude by reflecting on the viability of VCE as a descriptive and analytical method for Digital Musical Instrument (DMI) design, emphasizing its value in uncovering how technologies mediate, co-shape, and are co-shaped by users and contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åä¸º GrooveTransformer çš„å®æ—¶èŠ‚å¥ç”Ÿæˆç³»ç»Ÿï¼Œå¹¶åˆ©ç”¨å˜åˆ†äº¤å‰æ£€éªŒ (Variational Cross-Examination, VCE) è¿™ä¸€åç°è±¡å­¦æ¡†æ¶åˆ†æäº†å…¶åœ¨ä¸åŒè‰ºæœ¯èƒŒæ™¯ä¸‹çš„åº”ç”¨ã€‚é€šè¿‡å®¡è§†ç³»ç»Ÿçš„ä¸‰ç§ç¨³å®šçŠ¶æ€ (stabilities)â€”â€”è‡ªåŠ¨é¼“ä¼´å¥ç”Ÿæˆå™¨ã€Eurorack æ ¼å¼çš„èŠ‚å¥åºåˆ—å™¨åŠå’Œå£°ä¼´å¥é©±åŠ¨å™¨ï¼Œè®ºæ–‡æ­ç¤ºäº†è¯¥ç³»ç»Ÿåœ¨éé¢„è®¾æƒ…å†µä¸‹äº§ç”Ÿå¤šç¨³å®šæ€§ (multistability) çš„æ¼”åŒ–è¿‡ç¨‹ã€‚ç ”ç©¶æŒ‡å‡ºç³»ç»Ÿä¸å˜æ€§ (system invariants) çš„å¯ä¾›æ€§ã€è·¨å­¦ç§‘åä½œä»¥åŠå¼€å‘çš„å…·èº«æƒ…å¢ƒæ€§ (situated nature) æ˜¯ä¿ƒæˆè¿™ç§çµæ´»æ€§çš„æ ¸å¿ƒå› ç´ ã€‚è¿™ç§åˆ†ææ–¹æ³•å¼ºè°ƒäº†æŠ€æœ¯å¦‚ä½•ä¸­ä»‹ã€å…±åŒå¡‘é€ å¹¶è¢«ç”¨æˆ·ä¸ç¯å¢ƒæ‰€å®šä¹‰ã€‚æœ€ç»ˆï¼Œä½œè€…è‚¯å®šäº† VCE ä½œä¸ºæ•°å­—ä¹å™¨ (Digital Musical Instrument, DMI) è®¾è®¡ä¸­æè¿°æ€§å’Œåˆ†ææ€§å·¥å…·çš„æœ‰æ•ˆæ€§ï¼Œä¸ºç†è§£å¤æ‚äº¤äº’ç³»ç»Ÿçš„å¼€å‘æä¾›äº†æ–°è§†è§’ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.HC",
      "comment": "AI Music Creativity 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.05145v1",
      "published_date": "2025-09-05 14:38:02 UTC",
      "updated_date": "2025-09-05 14:38:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:46:01.600233+00:00"
    },
    {
      "arxiv_id": "2509.05139v1",
      "title": "Evaluation and Comparison Semantics for ODRL",
      "title_zh": "ODRL çš„è¯„ä¼°ä¸æ¯”è¾ƒè¯­ä¹‰",
      "authors": [
        "Jaime Osvaldo Salas",
        "Paolo Pareti",
        "Semih YumuÅŸak",
        "Soulmaz Gheisari",
        "Luis-Daniel IbÃ¡Ã±ez",
        "George Konstantinidis"
      ],
      "abstract": "We consider the problem of evaluating, and comparing computational policies in the Open Digital Rights Language (ODRL), which has become the de facto standard for governing the access and usage of digital resources. Although preliminary progress has been made on the formal specification of the language's features, a comprehensive formal semantics of ODRL is still missing. In this paper, we provide a simple and intuitive formal semantics for ODRL that is based on query answering. Our semantics refines previous formalisations, and is aligned with the latest published specification of the language (2.2). Building on our evaluation semantics, and motivated by data sharing scenarios, we also define and study the problem of comparing two policies, detecting equivalent, more restrictive or more permissive policies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼€æ”¾æ•°å­—æƒåˆ©è¯­è¨€ (ODRL) åœ¨æ²»ç†æ•°å­—èµ„æºè®¿é—®ä¸ä½¿ç”¨ä¸­ç¼ºä¹å…¨é¢å½¢å¼è¯­ä¹‰çš„é—®é¢˜ï¼Œæä¾›äº†ä¸€ç§åŸºäºæŸ¥è¯¢å›ç­” (query answering) çš„ç®€å•ä¸”ç›´è§‚çš„å½¢å¼è¯­ä¹‰ã€‚è¯¥è¯­ä¹‰ä¸ä»…æ”¹è¿›äº†å…ˆå‰çš„å½¢å¼åŒ–ç ”ç©¶ï¼Œè¿˜ä¸æœ€æ–°çš„ ODRL 2.2 è§„èŒƒä¿æŒäº†é«˜åº¦ä¸€è‡´ã€‚åŸºäºè¿™ä¸€è¯„ä¼°è¯­ä¹‰ï¼Œä½œè€…è¿›ä¸€æ­¥å®šä¹‰å¹¶ç ”ç©¶äº†ç­–ç•¥æ¯”è¾ƒé—®é¢˜ï¼Œæ—¨åœ¨è¯†åˆ«ä¸¤ä¸ªç­–ç•¥ä¹‹é—´æ˜¯å¦å­˜åœ¨ç­‰ä»·ã€æ›´ä¸¥æ ¼æˆ–æ›´å®½æ¾çš„é€»è¾‘å…³ç³»ã€‚è¿™é¡¹å·¥ä½œç”±æ•°æ®å…±äº«åœºæ™¯é©±åŠ¨ï¼Œä¸ºæ•°å­—æƒåˆ©ç®¡ç†ä¸­çš„ç­–ç•¥è¯„ä¼°å’Œæ¯”è¾ƒæä¾›äº†åšå®çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted as a full paper at the 14th International Joint Conference on Knowledge Graphs (IJCKG 2025). This is the submitted manuscript, the accepted manuscript will be published by Springer Nature",
      "pdf_url": "https://arxiv.org/pdf/2509.05139v1",
      "published_date": "2025-09-05 14:30:41 UTC",
      "updated_date": "2025-09-05 14:30:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:46:00.296095+00:00"
    },
    {
      "arxiv_id": "2509.10534v2",
      "title": "Decoupling the \"What\" and \"Where\" With Polar Coordinate Positional Embeddings",
      "title_zh": "é€šè¿‡æåæ ‡ä½ç½®åµŒå…¥å®ç°â€œå†…å®¹â€ä¸â€œä½ç½®â€çš„è§£è€¦",
      "authors": [
        "Anand Gopalakrishnan",
        "Robert CsordÃ¡s",
        "JÃ¼rgen Schmidhuber",
        "Michael C. Mozer"
      ],
      "abstract": "The attention mechanism in a Transformer architecture matches key to query based on both content -- the what -- and position in a sequence -- the where. We present an analysis indicating that what and where are entangled in the popular RoPE rotary position embedding. This entanglement can impair performance particularly when decisions require independent matches on these two factors. We propose an improvement to RoPE, which we call Polar Coordinate Position Embeddings or PoPE, that eliminates the what-where confound. PoPE is far superior on a diagnostic task requiring indexing solely by position or by content. On autoregressive sequence modeling in music, genomic, and natural language domains, Transformers using PoPE as the positional encoding scheme outperform baselines using RoPE with respect to evaluation loss (perplexity) and downstream task performance. On language modeling, these gains persist across model scale, from 124M to 774M parameters. Crucially, PoPE shows strong zero-shot length extrapolation capabilities compared not only to RoPE but even a method designed for extrapolation, YaRN, which requires additional fine tuning and frequency interpolation.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ†ææŒ‡å‡º Transformer æ¶æ„ä¸­å¹¿æ³›ä½¿ç”¨çš„ RoPE æ—‹è½¬ä½ç½®ç¼–ç å­˜åœ¨å†…å®¹ï¼ˆwhatï¼‰ä¸ä½ç½®ï¼ˆwhereï¼‰ä¿¡æ¯ç›¸äº’çº ç¼ çš„é—®é¢˜ï¼Œè¿™åœ¨éœ€è¦ç‹¬ç«‹åŒ¹é…è¿™ä¸¤ç±»å› å­çš„ä»»åŠ¡ä¸­ä¼šæŸå®³æ¨¡å‹æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† PoPEï¼ˆPolar Coordinate Position Embeddingsï¼‰ï¼Œé€šè¿‡æåæ ‡è§£è€¦æœºåˆ¶æ¶ˆé™¤äº†è¿™ç§å†…å®¹ä¸ä½ç½®çš„æ··æ·†ã€‚åœ¨è¯Šæ–­æ€§ä»»åŠ¡ä¸­ï¼ŒPoPE åœ¨ä»…ä¾èµ–ä½ç½®æˆ–ä»…ä¾èµ–å†…å®¹çš„ç´¢å¼•ä»»åŠ¡ä¸Šè¡¨ç°è¿œä¼˜äº RoPEã€‚åœ¨éŸ³ä¹ã€åŸºå› ç»„å’Œè‡ªç„¶è¯­è¨€å»ºæ¨¡ç­‰è‡ªå›å½’åºåˆ—å»ºæ¨¡å®éªŒä¸­ï¼ŒPoPE åœ¨è¯„ä»·æŸå¤±ï¼ˆperplexityï¼‰å’Œä¸‹æ¸¸ä»»åŠ¡è¡¨ç°ä¸Šå‡è¶…è¶Šäº†åŸºå‡†æ¨¡å‹ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯å®ï¼Œè¿™ç§æ€§èƒ½å¢ç›Šåœ¨ 124M è‡³ 774M çš„æ¨¡å‹å‚æ•°è§„æ¨¡ä¸‹å‡å…·æœ‰æŒç»­æ€§ã€‚æœ€å…³é”®çš„æ˜¯ï¼Œç›¸æ¯” RoPE ç”šè‡³ä¸“é—¨é’ˆå¯¹é•¿åº¦å¤–æ¨è®¾è®¡çš„ YaRNï¼ŒPoPE åœ¨æ— éœ€é¢å¤–å¾®è°ƒå’Œé¢‘ç‡æ’å€¼çš„æƒ…å†µä¸‹å±•ç°äº†æ›´å¼ºçš„é›¶æ ·æœ¬ï¼ˆzero-shotï¼‰é•¿åº¦å¤–æ¨èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Comparison to YaRN added + additional bias visualization + model ablation",
      "pdf_url": "https://arxiv.org/pdf/2509.10534v2",
      "published_date": "2025-09-05 14:22:27 UTC",
      "updated_date": "2025-12-22 20:13:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:46:04.482917+00:00"
    },
    {
      "arxiv_id": "2509.05112v1",
      "title": "GenAI-based test case generation and execution in SDV platform",
      "title_zh": "åŸºäº GenAI çš„ SDV å¹³å°æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆä¸æ‰§è¡Œ",
      "authors": [
        "Denesa Zyberaj",
        "Lukasz Mazur",
        "Nenad Petrovic",
        "Pankhuri Verma",
        "Pascal Hirmer",
        "Dirk Slama",
        "Xiangwei Cheng",
        "Alois Knoll"
      ],
      "abstract": "This paper introduces a GenAI-driven approach for automated test case generation, leveraging Large Language Models and Vision-Language Models to translate natural language requirements and system diagrams into structured Gherkin test cases. The methodology integrates Vehicle Signal Specification modeling to standardize vehicle signal definitions, improve compatibility across automotive subsystems, and streamline integration with third-party testing tools. Generated test cases are executed within the digital.auto playground, an open and vendor-neutral environment designed to facilitate rapid validation of software-defined vehicle functionalities. We evaluate our approach using the Child Presence Detection System use case, demonstrating substantial reductions in manual test specification effort and rapid execution of generated tests. Despite significant automation, the generation of test cases and test scripts still requires manual intervention due to current limitations in the GenAI pipeline and constraints of the digital.auto platform.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½(GenAI)çš„è‡ªåŠ¨åŒ–æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆä¸æ‰§è¡Œæ–¹æ³•ï¼Œæ—¨åœ¨æå‡è½¯ä»¶å®šä¹‰æ±½è½¦(Software-Defined Vehicle, SDV)å¹³å°çš„æµ‹è¯•æ•ˆç‡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)å’Œè§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models, VLMs)ï¼Œå°†è‡ªç„¶è¯­è¨€éœ€æ±‚å’Œç³»ç»Ÿå›¾è¡¨è½¬åŒ–ä¸ºç»“æ„åŒ–çš„Gherkinæµ‹è¯•ç”¨ä¾‹ã€‚ç ”ç©¶é›†æˆäº†è½¦è¾†ä¿¡å·è§„èŒƒ(Vehicle Signal Specification, VSS)å»ºæ¨¡ï¼Œä»¥æ ‡å‡†åŒ–è½¦è¾†ä¿¡å·å®šä¹‰ï¼Œå¹¶å¢å¼ºä¸åŒæ±½è½¦å­ç³»ç»Ÿçš„å…¼å®¹æ€§åŠä¸ç¬¬ä¸‰æ–¹æµ‹è¯•å·¥å…·çš„é›†æˆã€‚ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹åœ¨digital.auto playgroundè¿™ä¸€å¼€æ”¾ä¸”å‚å•†ä¸­ç«‹çš„ç¯å¢ƒä¸­æ‰§è¡Œï¼Œå®ç°äº†SDVåŠŸèƒ½çš„å¿«é€ŸéªŒè¯ã€‚é€šè¿‡å„¿ç«¥å­˜åœ¨æ£€æµ‹ç³»ç»Ÿ(Child Presence Detection System)æ¡ˆä¾‹åˆ†æï¼Œè¯æ˜è¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†æ‰‹åŠ¨ç¼–å†™æµ‹è¯•è§„èŒƒçš„å·¥ä½œé‡å¹¶å®ç°äº†æµ‹è¯•çš„å¿«é€Ÿæ‰§è¡Œã€‚å°½ç®¡è‡ªåŠ¨åŒ–ç¨‹åº¦è¾ƒé«˜ï¼Œä½†ç”±äºGenAIæµæ°´çº¿å’Œå¹³å°çš„é™åˆ¶ï¼Œæµ‹è¯•ç”¨ä¾‹åŠè„šæœ¬çš„ç”Ÿæˆç›®å‰ä»éœ€éƒ¨åˆ†äººå·¥å¹²é¢„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05112v1",
      "published_date": "2025-09-05 13:50:26 UTC",
      "updated_date": "2025-09-05 13:50:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:46:05.490856+00:00"
    },
    {
      "arxiv_id": "2509.05396v2",
      "title": "Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate",
      "title_zh": "è¨€è¾å¹¶éå…¨æ— ä»£ä»·ï¼šæ¢ç©¶å¤šæ™ºèƒ½ä½“è¾©è®ºä¸­çš„å¤±æ•ˆæ¨¡å¼",
      "authors": [
        "Andrea Wynn",
        "Harsh Satija",
        "Gillian Hadfield"
      ],
      "abstract": "While multi-agent debate has been proposed as a promising strategy for improving AI reasoning ability, we find that debate can sometimes be harmful rather than helpful. Prior work has primarily focused on debates within homogeneous groups of agents, whereas we explore how diversity in model capabilities influences the dynamics and outcomes of multi-agent interactions. Through a series of experiments, we demonstrate that debate can lead to a decrease in accuracy over time - even in settings where stronger (i.e., more capable) models outnumber their weaker counterparts. Our analysis reveals that models frequently shift from correct to incorrect answers in response to peer reasoning, favoring agreement over challenging flawed reasoning. We perform additional experiments investigating various potential contributing factors to these harmful shifts - including sycophancy, social conformity, and model and task type. These results highlight important failure modes in the exchange of reasons during multi-agent debate, suggesting that naive applications of debate may cause performance degradation when agents are neither incentivised nor adequately equipped to resist persuasive but incorrect reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ™ºèƒ½ä½“è¾©è®º(Multi-Agent Debate)åœ¨æå‡AIæ¨ç†èƒ½åŠ›æ—¶å¯èƒ½äº§ç”Ÿçš„è´Ÿé¢å½±å“ï¼Œé‡ç‚¹åˆ†æäº†æ¨¡å‹èƒ½åŠ›å¤šæ ·æ€§(Diversity in model capabilities)å¯¹äº¤äº’åŠ¨æ€å’Œç»“æœçš„å½±å“ã€‚å®éªŒè¡¨æ˜ï¼Œè¾©è®ºè¿‡ç¨‹å¯èƒ½å¯¼è‡´å‡†ç¡®ç‡éšæ—¶é—´æ¨ç§»è€Œä¸‹é™ï¼Œå³ä½¿åœ¨æ›´å¼º(Stronger)çš„æ¨¡å‹æ•°é‡å¤šäºå¼±æ¨¡å‹çš„æƒ…å†µä¸‹è¿™ä¸€ç°è±¡ä¾ç„¶å­˜åœ¨ã€‚åˆ†æå‘ç°ï¼Œæ¨¡å‹åœ¨å—åˆ°åŒä¼´æ¨ç†å½±å“æ—¶ï¼Œå¾€å¾€ä¼šä¸ºäº†è¿½æ±‚è¾¾æˆä¸€è‡´(Agreement)è€Œæ”¾å¼ƒæ­£ç¡®ç­”æ¡ˆï¼Œè€Œéå»æŒ‘æˆ˜æœ‰ç¼ºé™·çš„é€»è¾‘ã€‚ç ”ç©¶è¿›ä¸€æ­¥è°ƒæŸ¥äº†å¯¼è‡´è¿™ç§è´Ÿçº§è½¬å˜çš„å› ç´ ï¼ŒåŒ…æ‹¬é¡ºä»æ€§(Sycophancy)ã€ç¤¾ä¼šä»ä¼—æ€§(Social conformity)ä»¥åŠæ¨¡å‹å’Œä»»åŠ¡ç±»å‹çš„å½±å“ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å¤šæ™ºèƒ½ä½“è¾©è®ºä¸­çš„å…³é”®å¤±æ•ˆæ¨¡å¼(Failure modes)ï¼Œè¡¨æ˜å¦‚æœæ™ºèƒ½ä½“ç¼ºä¹æŠµåˆ¶è¯´æœæ€§ä½†é”™è¯¯æ¨ç†çš„èƒ½åŠ›æˆ–è¯±å› ï¼Œç®€å•çš„è¾©è®ºæœºåˆ¶åè€Œä¼šå¯¼è‡´æ€§èƒ½é€€åŒ–(Performance degradation)ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "ICML MAS Workshop 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.05396v2",
      "published_date": "2025-09-05 13:47:38 UTC",
      "updated_date": "2025-10-13 16:40:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:46:18.596577+00:00"
    },
    {
      "arxiv_id": "2509.05100v2",
      "title": "ICR: Iterative Clarification and Rewriting for Conversational Search",
      "title_zh": "ICRï¼šé¢å‘å¯¹è¯å¼æœç´¢çš„è¿­ä»£æ¾„æ¸…ä¸é‡å†™",
      "authors": [
        "Zhiyu Cao",
        "Peifeng Li",
        "Qiaoming Zhu"
      ],
      "abstract": "Most previous work on Conversational Query Rewriting employs an end-to-end rewriting paradigm. However, this approach is hindered by the issue of multiple fuzzy expressions within the query, which complicates the simultaneous identification and rewriting of multiple positions. To address this issue, we propose a novel framework ICR (Iterative Clarification and Rewriting), an iterative rewriting scheme that pivots on clarification questions. Within this framework, the model alternates between generating clarification questions and rewritten queries. The experimental results show that our ICR can continuously improve retrieval performance in the clarification-rewriting iterative process, thereby achieving state-of-the-art performance on two popular datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯¹è¯å¼æœç´¢(Conversational Search)ä¸­çš„å¯¹è¯æŸ¥è¯¢é‡å†™(Conversational Query Rewriting)é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„ç«¯åˆ°ç«¯é‡å†™æ¨¡å¼åœ¨å¤„ç†åŒ…å«å¤šä¸ªæ¨¡ç³Šè¡¨è¾¾çš„æŸ¥è¯¢æ—¶ï¼Œéš¾ä»¥åŒæ—¶è¯†åˆ«å¹¶é‡å†™å¤šä¸ªä½ç½®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†åä¸ºICRï¼ˆIterative Clarification and Rewritingï¼‰çš„è¿­ä»£æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨ä»¥æ¾„æ¸…é—®é¢˜ä¸ºæ ¸å¿ƒçš„å¾ªç¯é‡å†™æ–¹æ¡ˆã€‚åœ¨ICRæ¡†æ¶ä¸‹ï¼Œæ¨¡å‹é€šè¿‡äº¤æ›¿ç”Ÿæˆæ¾„æ¸…é—®é¢˜å’Œé‡å†™æŸ¥è¯¢ï¼Œå®ç°å¯¹æœç´¢æ„å›¾çš„é€æ­¥ç²¾å‡†å®šä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒICRåœ¨è¿­ä»£è¿‡ç¨‹ä¸­èƒ½æŒç»­ä¼˜åŒ–æ£€ç´¢æ€§èƒ½ï¼Œå¹¶åœ¨ä¸¤ä¸ªä¸»æµæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›(State-of-the-art)çš„æ•ˆæœã€‚è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡è¿­ä»£æ¾„æ¸…ä¸é‡å†™çš„åé¦ˆæœºåˆ¶èƒ½å¤Ÿæ˜¾è‘—æå‡å¤æ‚å¯¹è¯ç¯å¢ƒä¸‹çš„æ£€ç´¢å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05100v2",
      "published_date": "2025-09-05 13:37:04 UTC",
      "updated_date": "2025-09-16 01:26:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:46:20.984357+00:00"
    },
    {
      "arxiv_id": "2509.05091v1",
      "title": "ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed Feedback",
      "title_zh": "ProToMï¼šé€šè¿‡åŸºäºå¿ƒç†ç†è®ºçš„åé¦ˆä¿ƒè¿›äº²ç¤¾ä¼šè¡Œä¸º",
      "authors": [
        "Matteo Bortoletto",
        "Yichao Zhou",
        "Lance Ying",
        "Tianmin Shu",
        "Andreas Bulling"
      ],
      "abstract": "While humans are inherently social creatures, the challenge of identifying when and how to assist and collaborate with others - particularly when pursuing independent goals - can hinder cooperation. To address this challenge, we aim to develop an AI system that provides useful feedback to promote prosocial behaviour - actions that benefit others, even when not directly aligned with one's own goals. We introduce ProToM, a Theory of Mind-informed facilitator that promotes prosocial actions in multi-agent systems by providing targeted, context-sensitive feedback to individual agents. ProToM first infers agents' goals using Bayesian inverse planning, then selects feedback to communicate by maximising expected utility, conditioned on the inferred goal distribution. We evaluate our approach against baselines in two multi-agent environments: Doors, Keys, and Gems, as well as Overcooked. Our results suggest that state-of-the-art large language and reasoning models fall short of communicating feedback that is both contextually grounded and well-timed - leading to higher communication overhead and task speedup. In contrast, ProToM provides targeted and helpful feedback, achieving a higher success rate, shorter task completion times, and is consistently preferred by human users.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ProToMï¼Œä¸€ç§åŸºäºå¿ƒç†ç†è®º (Theory of Mind-informed) çš„è¾…åŠ©æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­å› ä¸ªä½“ç›®æ ‡ç‹¬ç«‹è€Œå¯¼è‡´åä½œå—é˜»çš„æŒ‘æˆ˜ï¼Œä»è€Œä¿ƒè¿›äº²ç¤¾ä¼šè¡Œä¸º (prosocial behaviour)ã€‚ProToM é¦–å…ˆåˆ©ç”¨è´å¶æ–¯é€†è§„åˆ’ (Bayesian inverse planning) æ¨æ–­å„æ™ºèƒ½ä½“çš„ç›®æ ‡ï¼Œéšåé€šè¿‡åœ¨æ¨æ–­çš„ç›®æ ‡åˆ†å¸ƒä¸‹æœ€å¤§åŒ–æœŸæœ›æ•ˆç”¨ (maximizing expected utility) æ¥ç”Ÿæˆé’ˆå¯¹æ€§ä¸”è¯­å¢ƒæ•æ„Ÿçš„åé¦ˆã€‚ç ”ç©¶åœ¨ Doors, Keys, and Gems å’Œ Overcooked ä¸¤ä¸ªç¯å¢ƒä¸­å¯¹è¯¥æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ç°æœ‰çš„å‰æ²¿å¤§è¯­è¨€å’Œæ¨ç†æ¨¡å‹åœ¨åé¦ˆçš„è¯­å¢ƒè½åœ°å’Œæ—¶æœºé€‰æ‹©ä¸Šä»å­˜åœ¨å±€é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒProToM èƒ½å¤Ÿæä¾›æ›´ç²¾å‡†ä¸”æœ‰ç”¨çš„æŒ‡å¯¼ï¼Œæ˜¾è‘—æé«˜äº†ä»»åŠ¡æˆåŠŸç‡å¹¶ç¼©çŸ­äº†ä»»åŠ¡å®Œæˆæ—¶é—´ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨äººå·¥åå¥½æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸ºæå‡å¤šæ™ºèƒ½ä½“åä½œæ•ˆç‡æä¾›äº†æœ‰æ•ˆçš„ AI åé¦ˆæœºåˆ¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Website at https://www.matteobortoletto.org/protom/",
      "pdf_url": "https://arxiv.org/pdf/2509.05091v1",
      "published_date": "2025-09-05 13:30:17 UTC",
      "updated_date": "2025-09-05 13:30:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:46:23.089477+00:00"
    },
    {
      "arxiv_id": "2509.15230v1",
      "title": "Pre-Forgettable Models: Prompt Learning as a Native Mechanism for Unlearning",
      "title_zh": "Pre-Forgettable Modelsï¼šä½œä¸ºåŸç”Ÿé—å¿˜æœºåˆ¶çš„æç¤ºå­¦ä¹ ",
      "authors": [
        "Rutger Hendrix",
        "Giovanni PatanÃ¨",
        "Leonardo G. Russo",
        "Simone Carnemolla",
        "Giovanni Bellitto",
        "Federica Proietto Salanitri",
        "Concetto Spampinato",
        "Matteo Pennisi"
      ],
      "abstract": "Foundation models have transformed multimedia analysis by enabling robust and transferable representations across diverse modalities and tasks. However, their static deployment conflicts with growing societal and regulatory demands -- particularly the need to unlearn specific data upon request, as mandated by privacy frameworks such as the GDPR. Traditional unlearning approaches, including retraining, activation editing, or distillation, are often computationally expensive, fragile, and ill-suited for real-time or continuously evolving systems. In this paper, we propose a paradigm shift: rethinking unlearning not as a retroactive intervention but as a built-in capability. We introduce a prompt-based learning framework that unifies knowledge acquisition and removal within a single training phase. Rather than encoding information in model weights, our approach binds class-level semantics to dedicated prompt tokens. This design enables instant unlearning simply by removing the corresponding prompt -- without retraining, model modification, or access to original data. Experiments demonstrate that our framework preserves predictive performance on retained classes while effectively erasing forgotten ones. Beyond utility, our method exhibits strong privacy and security guarantees: it is resistant to membership inference attacks, and prompt removal prevents any residual knowledge extraction, even under adversarial conditions. This ensures compliance with data protection principles and safeguards against unauthorized access to forgotten information, making the framework suitable for deployment in sensitive and regulated environments. Overall, by embedding removability into the architecture itself, this work establishes a new foundation for designing modular, scalable and ethically responsive AI models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Pre-Forgettable Modelsï¼Œæ—¨åœ¨é€šè¿‡Prompt Learningå°†Machine Unlearningä»ä¸€ç§äº‹åè¡¥æ•‘æªæ–½è½¬å˜ä¸ºæ¨¡å‹çš„ä¸€ç§åŸç”Ÿæœºåˆ¶ï¼Œä»¥æ»¡è¶³GDPRç­‰éšç§æ¡†æ¶å¯¹æ•°æ®é—å¿˜çš„éœ€æ±‚ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†ç±»åˆ«è¯­ä¹‰ç»‘å®šåˆ°ç‰¹å®šçš„Prompt Tokensè€Œéæ¨¡å‹æƒé‡ï¼Œå®ç°äº†ä¸€ç§å…¨æ–°çš„çŸ¥è¯†è·å–ä¸ç§»é™¤æ–¹å¼ã€‚åœ¨è¿™ç§è®¾è®¡ä¸‹ï¼Œç³»ç»Ÿä»…éœ€ç§»é™¤å¯¹åº”çš„æç¤ºè¯å³å¯å®ç°ç¬æ—¶é—å¿˜ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€ä¿®æ”¹æ¨¡å‹æƒé‡æˆ–è®¿é—®åŸå§‹æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æœ‰æ•ˆæ“¦é™¤ç›®æ ‡ç±»åˆ«çš„åŒæ—¶ï¼Œèƒ½ä¿æŒå¯¹ä¿ç•™ç±»åˆ«çš„é¢„æµ‹æ€§èƒ½ï¼Œå¹¶è¡¨ç°å‡ºæå¼ºçš„å®‰å…¨æ€§ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæŠµæŠ—Membership Inference Attacksï¼Œå¹¶é˜²æ­¢åœ¨å¯¹æŠ—æ¡ä»¶ä¸‹æå–æ®‹ç•™çŸ¥è¯†ï¼Œç¡®ä¿äº†æ•æ„Ÿç¯å¢ƒä¸‹çš„åˆè§„æ€§ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡å°†å¯ç§»é™¤æ€§åµŒå…¥åˆ°æ¶æ„æœ¬èº«ï¼Œä¸ºå¼€å‘æ¨¡å—åŒ–ã€å¯æ‰©å±•ä¸”ç¬¦åˆä¼¦ç†è§„èŒƒçš„AIæ¨¡å‹æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ACM multimedia 2025 BNI track",
      "pdf_url": "https://arxiv.org/pdf/2509.15230v1",
      "published_date": "2025-09-05 13:28:04 UTC",
      "updated_date": "2025-09-05 13:28:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:46:30.892833+00:00"
    },
    {
      "arxiv_id": "2509.05072v1",
      "title": "Finding your MUSE: Mining Unexpected Solutions Engine",
      "title_zh": "å¯»æ‰¾ä½ çš„ MUSEï¼šéé¢„æœŸè§£å†³æ–¹æ¡ˆæŒ–æ˜å¼•æ“",
      "authors": [
        "Nir Sweed",
        "Hanit Hakim",
        "Ben Wolfson",
        "Hila Lifshitz",
        "Dafna Shahaf"
      ],
      "abstract": "Innovators often exhibit cognitive fixation on existing solutions or nascent ideas, hindering the exploration of novel alternatives. This paper introduces a methodology for constructing Functional Concept Graphs (FCGs), interconnected representations of functional elements that support abstraction, problem reframing, and analogical inspiration. Our approach yields large-scale, high-quality FCGs with explicit abstraction relations, overcoming limitations of prior work. We further present MUSE, an algorithm leveraging FCGs to generate creative inspirations for a given problem. We demonstrate our method by computing an FCG on 500K patents, which we release for further research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åˆ›æ–°è¿‡ç¨‹ä¸­å¸¸è§çš„è®¤çŸ¥å›ºç€ (cognitive fixation) é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ„å»ºåŠŸèƒ½æ¦‚å¿µå›¾ (Functional Concept Graphs, FCGs) çš„æ–°æ–¹æ³•ã€‚FCGs æ˜¯ä¸€ç§åŠŸèƒ½è¦ç´ çš„äº’è¿è¡¨ç¤ºï¼Œæ—¨åœ¨æ”¯æŒæŠ½è±¡åŒ–ã€é—®é¢˜é‡æ„å’Œç±»æ¯”å¯å‘ï¼Œä»è€Œå¸®åŠ©åˆ›æ–°è€…æ¢ç´¢æ–°é¢–çš„æ›¿ä»£æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå…·æœ‰æ˜¾å¼æŠ½è±¡å…³ç³»çš„å¤§è§„æ¨¡ã€é«˜è´¨é‡ FCGsï¼Œæœ‰æ•ˆå…‹æœäº†ç°æœ‰ç ”ç©¶çš„å±€é™æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè®ºæ–‡è¿›ä¸€æ­¥æå‡ºäº† MUSE ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡æŒ–æ˜ FCGs ä¸ºç‰¹å®šé—®é¢˜ç”Ÿæˆåˆ›æ„çµæ„Ÿã€‚æœ€åï¼Œç ”ç©¶è€…åŸºäº 50 ä¸‡é¡¹ä¸“åˆ©æ„å»ºå¹¶å‘å¸ƒäº†ä¸€ä¸ªå¤§è§„æ¨¡ FCG æ•°æ®åº“ï¼Œä¸ºç›¸å…³çš„å­¦æœ¯ç ”ç©¶å’Œåˆ›æ–°å®è·µæä¾›äº†é‡è¦çš„æ•°æ®æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05072v1",
      "published_date": "2025-09-05 13:13:19 UTC",
      "updated_date": "2025-09-05 13:13:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:46:31.187518+00:00"
    },
    {
      "arxiv_id": "2509.07997v1",
      "title": "Learning-Based Planning for Improving Science Return of Earth Observation Satellites",
      "title_zh": "æ—¨åœ¨æå‡åœ°çƒè§‚æµ‹å«æ˜Ÿç§‘å­¦æ”¶ç›Šçš„åŸºäºå­¦ä¹ çš„è§„åˆ’",
      "authors": [
        "Abigail Breitfeld",
        "Alberto Candela",
        "Juan Delfa",
        "Akseli Kangaslahti",
        "Itai Zilberstein",
        "Steve Chien",
        "David Wettergreen"
      ],
      "abstract": "Earth observing satellites are powerful tools for collecting scientific information about our planet, however they have limitations: they cannot easily deviate from their orbital trajectories, their sensors have a limited field of view, and pointing and operating these sensors can take a large amount of the spacecraft's resources. It is important for these satellites to optimize the data they collect and include only the most important or informative measurements. Dynamic targeting is an emerging concept in which satellite resources and data from a lookahead instrument are used to intelligently reconfigure and point a primary instrument. Simulation studies have shown that dynamic targeting increases the amount of scientific information gathered versus conventional sampling strategies. In this work, we present two different learning-based approaches to dynamic targeting, using reinforcement and imitation learning, respectively. These learning methods build on a dynamic programming solution to plan a sequence of sampling locations. We evaluate our approaches against existing heuristic methods for dynamic targeting, showing the benefits of using learning for this application. Imitation learning performs on average 10.0\\% better than the best heuristic method, while reinforcement learning performs on average 13.7\\% better. We also show that both learning methods can be trained effectively with relatively small amounts of data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ°çƒè§‚æµ‹å«æ˜Ÿ(Earth observation satellites)åœ¨è½¨é“è½¨è¿¹ã€è§†åœºèŒƒå›´åŠèµ„æºåˆ†é…ç­‰æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå­¦ä¹ çš„è§„åˆ’æ–¹æ³•æ¥æå‡ç§‘å­¦å›æŠ¥ã€‚ç ”ç©¶å¼•å…¥äº†åŠ¨æ€ç›®æ ‡å®šä½(Dynamic targeting)æ¦‚å¿µï¼Œé€šè¿‡å‰ç»ä»ªå™¨æ•°æ®æ™ºèƒ½è°ƒæ•´ä¸»ä¼ æ„Ÿå™¨æŒ‡å‘ï¼Œä»¥ä¼˜åŒ–è§‚æµ‹æ•°æ®çš„ç§‘å­¦ä»·å€¼ã€‚ä½œè€…åˆ†åˆ«é‡‡ç”¨äº†å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å’Œæ¨¡ä»¿å­¦ä¹ (Imitation Learning)ä¸¤ç§æ–¹æ³•ï¼Œå¹¶ç»“åˆåŠ¨æ€è§„åˆ’(Dynamic Programming)æ–¹æ¡ˆæ¥è§„åˆ’é‡‡æ ·åºåˆ—ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™äº›å­¦ä¹ é©±åŠ¨çš„æ–¹æ³•ä¼˜äºä¼ ç»Ÿå¯å‘å¼æ–¹æ³•(Heuristic methods)ï¼Œæ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½å¹³å‡åˆ†åˆ«æå‡äº†10.0%å’Œ13.7%ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œå³ä¾¿åœ¨æ•°æ®é‡è¾ƒå°çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸¤ç§æ¨¡å‹ä¹Ÿèƒ½é€šè¿‡è®­ç»ƒå®ç°é«˜æ•ˆçš„ç§‘å­¦ç›®æ ‡è§‚æµ‹è§„åˆ’ã€‚",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "International Symposium on Artificial Intelligence, Robotics and Automation in Space, November 2024",
      "pdf_url": "https://arxiv.org/pdf/2509.07997v1",
      "published_date": "2025-09-05 13:11:50 UTC",
      "updated_date": "2025-09-05 13:11:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:46:59.788781+00:00"
    },
    {
      "arxiv_id": "2509.05066v2",
      "title": "ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions",
      "title_zh": "ToM-SSIï¼šæƒ…å¢ƒåŒ–ç¤¾äº¤äº’åŠ¨ä¸­çš„å¿ƒç†ç†è®ºè¯„ä¼°",
      "authors": [
        "Matteo Bortoletto",
        "Constantin Ruhdorfer",
        "Andreas Bulling"
      ],
      "abstract": "Most existing Theory of Mind (ToM) benchmarks for foundation models rely on variations of the Sally-Anne test, offering only a very limited perspective on ToM and neglecting the complexity of human social interactions. To address this gap, we propose ToM-SSI: a new benchmark specifically designed to test ToM capabilities in environments rich with social interactions and spatial dynamics. While current ToM benchmarks are limited to text-only or dyadic interactions, ToM-SSI is multimodal and includes group interactions of up to four agents that communicate and move in situated environments. This unique design allows us to study, for the first time, mixed cooperative-obstructive settings and reasoning about multiple agents' mental state in parallel, thus capturing a wider range of social cognition than existing benchmarks. Our evaluations reveal that the current models' performance is still severely limited, especially in these new tasks, highlighting critical gaps for future research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ToM-SSIï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°åŸºç¡€æ¨¡å‹åœ¨å¤æ‚ç¤¾äº¤äº’åŠ¨ä¸­å¿ƒç†ç†è®º(Theory of Mind, ToM)èƒ½åŠ›çš„å…¨æ–°åŸºå‡†ã€‚ç°æœ‰çš„ToMåŸºå‡†ä¸»è¦ä¾èµ–äºSally-Anneæµ‹è¯•ï¼Œä»…èƒ½æä¾›æœ‰é™è§†è§’ä¸”å¿½ç•¥äº†äººç±»ç¤¾äº¤äº’åŠ¨çš„å¤æ‚æ€§ï¼Œè€ŒToM-SSIåˆ™ä¸“é—¨é’ˆå¯¹å…·æœ‰ä¸°å¯Œç¤¾äº¤äº’åŠ¨å’Œç©ºé—´åŠ¨æ€çš„æƒ…å¢ƒåŒ–ç¯å¢ƒè€Œè®¾è®¡ã€‚ä¸åŒäºä»¥å¾€ä»…é™æ–‡æœ¬æˆ–åŒäººäº’åŠ¨çš„åŸºå‡†ï¼ŒToM-SSIé‡‡ç”¨å¤šæ¨¡æ€(multimodal)å½¢å¼ï¼Œæ¶µç›–äº†å¤šè¾¾å››ä¸ªæ™ºèƒ½ä½“(agents)åœ¨ç¯å¢ƒä¸­ç§»åŠ¨å¹¶äº¤æµçš„ç¾¤ä½“äº’åŠ¨ã€‚è¿™ç§ç‹¬ç‰¹è®¾è®¡é¦–æ¬¡æ”¯æŒäº†å¯¹æ··åˆåˆä½œ-é˜»ç¢(mixed cooperative-obstructive)è®¾ç½®ä»¥åŠå¹¶è¡Œæ¨ç†å¤šä¸ªæ™ºèƒ½ä½“å¿ƒç†çŠ¶æ€çš„ç ”ç©¶ï¼Œä»è€Œæ•æ‰äº†æ¯”ç°æœ‰åŸºå‡†æ›´å¹¿æ³›çš„ç¤¾ä¼šè®¤çŸ¥ç»´åº¦ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹åœ¨å¤„ç†è¿™äº›æ–°ä»»åŠ¡æ—¶çš„æ€§èƒ½è¡¨ç°ä»ç„¶éå¸¸æœ‰é™ï¼Œå‡¸æ˜¾äº†æœªæ¥ç ”ç©¶ä¸­äºŸå¾…å¼¥è¡¥çš„å…³é”®èƒ½åŠ›å·®è·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2025 (Main)",
      "pdf_url": "https://arxiv.org/pdf/2509.05066v2",
      "published_date": "2025-09-05 12:58:15 UTC",
      "updated_date": "2025-09-16 12:22:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:46:55.297970+00:00"
    },
    {
      "arxiv_id": "2509.05034v1",
      "title": "Towards Efficient Pixel Labeling for Industrial Anomaly Detection and Localization",
      "title_zh": "è¿ˆå‘å·¥ä¸šå¼‚å¸¸æ£€æµ‹ä¸å®šä½çš„é«˜æ•ˆåƒç´ æ ‡æ³¨",
      "authors": [
        "Jingqi Wu",
        "Hanxi Li",
        "Lin Yuanbo Wu",
        "Hao Chen",
        "Deyin Liu",
        "Peng Wang"
      ],
      "abstract": "Industrial product inspection is often performed using Anomaly Detection (AD) frameworks trained solely on non-defective samples. Although defective samples can be collected during production, leveraging them usually requires pixel-level annotations, limiting scalability. To address this, we propose ADClick, an Interactive Image Segmentation (IIS) algorithm for industrial anomaly detection. ADClick generates pixel-wise anomaly annotations from only a few user clicks and a brief textual description, enabling precise and efficient labeling that significantly improves AD model performance (e.g., AP = 96.1\\% on MVTec AD). We further introduce ADClick-Seg, a cross-modal framework that aligns visual features and textual prompts via a prototype-based approach for anomaly detection and localization. By combining pixel-level priors with language-guided cues, ADClick-Seg achieves state-of-the-art results on the challenging ``Multi-class'' AD task (AP = 80.0\\%, PRO = 97.5\\%, Pixel-AUROC = 99.1\\% on MVTec AD).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆAnomaly Detection, ADï¼‰ä¸­ç¼ºé™·æ ·æœ¬åƒç´ çº§æ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº†äº¤äº’å¼å›¾åƒåˆ†å‰²ï¼ˆInteractive Image Segmentation, IISï¼‰ç®—æ³• ADClickã€‚è¯¥ç®—æ³•ä»…é€šè¿‡å°‘é‡ç‚¹å‡»å’Œç®€çŸ­æ–‡æœ¬æè¿°å³å¯é«˜æ•ˆç”Ÿæˆç²¾ç¡®çš„åƒç´ çº§å¼‚å¸¸æ ‡æ³¨ï¼Œåœ¨ MVTec AD æ•°æ®é›†ä¸Šå®ç°äº† 96.1% çš„å¹³å‡ç²¾åº¦ï¼ˆAPï¼‰ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿›ä¸€æ­¥æ¨å‡ºäº†è·¨æ¨¡æ€æ¡†æ¶ ADClick-Segï¼Œé€šè¿‡åŸºäºåŸå‹ï¼ˆprototype-basedï¼‰çš„æ–¹æ³•å®ç°è§†è§‰ç‰¹å¾ä¸æ–‡æœ¬æç¤ºçš„å¯¹é½ã€‚è¯¥æ¡†æ¶ç»“åˆäº†åƒç´ çº§å…ˆéªŒä¸è¯­è¨€å¼•å¯¼çº¿ç´¢ï¼Œåœ¨æŒ‘æˆ˜æ€§çš„â€œMulti-classâ€ AD ä»»åŠ¡ä¸­å–å¾—äº† state-of-the-art çš„ç»“æœï¼Œå…¶åœ¨ MVTec AD æ•°æ®é›†ä¸Šçš„ PRO è¾¾åˆ° 97.5%ï¼ŒPixel-AUROC è¾¾åˆ° 99.1%ã€‚è¿™ä¸€ç³»åˆ—è´¡çŒ®ä¸ºå·¥ä¸šåœºæ™¯ä¸‹é«˜æ•ˆã€ç²¾å‡†çš„å¼‚å¸¸æ£€æµ‹ä¸å®šä½æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05034v1",
      "published_date": "2025-09-05 11:45:17 UTC",
      "updated_date": "2025-09-05 11:45:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:46:58.483122+00:00"
    },
    {
      "arxiv_id": "2509.05031v1",
      "title": "Pointing-Guided Target Estimation via Transformer-Based Attention",
      "title_zh": "åŸºäº Transformer æ³¨æ„åŠ›æœºåˆ¶çš„æŒ‡å‘å¼•å¯¼ç›®æ ‡ä¼°è®¡",
      "authors": [
        "Luca MÃ¼ller",
        "Hassan Ali",
        "Philipp Allgeuer",
        "LukÃ¡Å¡ GajdoÅ¡ech",
        "Stefan Wermter"
      ],
      "abstract": "Deictic gestures, like pointing, are a fundamental form of non-verbal communication, enabling humans to direct attention to specific objects or locations. This capability is essential in Human-Robot Interaction (HRI), where robots should be able to predict human intent and anticipate appropriate responses. In this work, we propose the Multi-Modality Inter-TransFormer (MM-ITF), a modular architecture to predict objects in a controlled tabletop scenario with the NICOL robot, where humans indicate targets through natural pointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing gestures to object locations, assigns a likelihood score to each, and identifies the most likely target. Our results demonstrate that the method can accurately predict the intended object using monocular RGB data, thus enabling intuitive and accessible human-robot collaboration. To evaluate the performance, we introduce a patch confusion matrix, providing insights into the model's predictions across candidate object locations. Code available at: https://github.com/lucamuellercode/MMITF.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Human-Robot Interaction (HRI)ä¸­çš„æŒ‡å‘æ€§æ‰‹åŠ¿è¯†åˆ«é—®é¢˜ï¼Œæå‡ºäº†åä¸ºMulti-Modality Inter-TransFormer (MM-ITF)çš„æ¨¡å—åŒ–æ¶æ„ã€‚åœ¨NICOLæœºå™¨äººçš„æ¡Œé¢å—æ§åœºæ™¯ä¸‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨inter-modality attentionæœºåˆ¶å°†2DæŒ‡å‘æ‰‹åŠ¿æ˜ å°„åˆ°ç‰©ä½“ä½ç½®ï¼Œé€šè¿‡ä¸ºæ¯ä¸ªå€™é€‰ç›®æ ‡åˆ†é…å¯èƒ½æ€§å¾—åˆ†æ¥ç²¾å‡†è¯†åˆ«æœ€å¯èƒ½çš„æŒ‡å‘ç›®æ ‡ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMM-ITFä»…å‡­å•ç›®RGBæ•°æ®å°±èƒ½å‡†ç¡®é¢„æµ‹äººç±»æ„å›¾ï¼Œä»è€Œå®ç°äº†æ›´åŠ ç›´è§‚ä¸”æ˜“äºæ“ä½œçš„äººæœºåä½œã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†patch confusion matrixä½œä¸ºæ–°çš„è¯„ä¼°å·¥å…·ï¼Œé€šè¿‡å¯¹å€™é€‰ç‰©ä½“ä½ç½®çš„é¢„æµ‹åˆ†å¸ƒè¿›è¡Œé‡åŒ–ï¼Œä¸ºæ¨¡å‹çš„å†³ç­–è¿‡ç¨‹æä¾›äº†æ·±å…¥çš„åˆ†ææ´å¯Ÿã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted at the 34th International Conference on Artificial Neural Networks (ICANN) 2025,12 pages,4 figures,1 table; work was co-funded by Horizon Europe project TERAIS under Grant agreement number 101079338",
      "pdf_url": "https://arxiv.org/pdf/2509.05031v1",
      "published_date": "2025-09-05 11:42:03 UTC",
      "updated_date": "2025-09-05 11:42:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:47:07.096682+00:00"
    },
    {
      "arxiv_id": "2509.05007v2",
      "title": "Sticker-TTS: Learn to Utilize Historical Experience with a Sticker-driven Test-Time Scaling Framework",
      "title_zh": "Sticker-TTSï¼šåŸºäºè´´çº¸é©±åŠ¨æ¨ç†æ—¶æ‰©å±•æ¡†æ¶çš„å†å²ç»éªŒåˆ©ç”¨å­¦ä¹ ",
      "authors": [
        "Jie Chen",
        "Jinhao Jiang",
        "Yingqian Min",
        "Zican Dong",
        "Shijie Wang",
        "Wayne Xin Zhao",
        "Ji-Rong Wen"
      ],
      "abstract": "Large reasoning models (LRMs) have exhibited strong performance on complex reasoning tasks, with further gains achievable through increased computational budgets at inference. However, current test-time scaling methods predominantly rely on redundant sampling, ignoring the historical experience utilization, thereby limiting computational efficiency. To overcome this limitation, we propose Sticker-TTS, a novel test-time scaling framework that coordinates three collaborative LRMs to iteratively explore and refine solutions guided by historical attempts. At the core of our framework are distilled key conditions-termed stickers-which drive the extraction, refinement, and reuse of critical information across multiple rounds of reasoning. To further enhance the efficiency and performance of our framework, we introduce a two-stage optimization strategy that combines imitation learning with self-improvement, enabling progressive refinement. Extensive evaluations on three challenging mathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH, demonstrate that Sticker-TTS consistently surpasses strong baselines, including self-consistency and advanced reinforcement learning approaches, under comparable inference budgets. These results highlight the effectiveness of sticker-guided historical experience utilization. Our code and data are available at https://github.com/RUCAIBox/Sticker-TTS.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨ç†æ¨¡å‹(Large Reasoning Models, LRMs)åœ¨æ¨ç†é˜¶æ®µè¿‡åº¦ä¾èµ–å†—ä½™é‡‡æ ·è€Œå¿½è§†å†å²ç»éªŒåˆ©ç”¨çš„é—®é¢˜ï¼Œæå‡ºäº†Sticker-TTSè¿™ä¸€åˆ›æ–°çš„æµ‹è¯•æ—¶ç¼©æ”¾(Test-Time Scaling)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åè°ƒä¸‰ä¸ªåä½œçš„LRMsï¼Œåˆ©ç”¨è¢«ç§°ä¸ºâ€œè´´çº¸â€(Stickers)çš„è’¸é¦å…³é”®æ¡ä»¶ï¼Œåœ¨å¤šè½®æ¨ç†ä¸­å®ç°å…³é”®ä¿¡æ¯çš„æå–ã€ç²¾ç‚¼ä¸é‡ç”¨ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡æ€§èƒ½ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†ä¸€ç§ç»“åˆæ¨¡ä»¿å­¦ä¹ (Imitation Learning)ä¸è‡ªæˆ‘æå‡(Self-Improvement)çš„ä¸¤é˜¶æ®µä¼˜åŒ–ç­–ç•¥ã€‚åœ¨AIME-24ã€AIME-25åŠOlymMATHç­‰æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒSticker-TTSåœ¨åŒç­‰æ¨ç†é¢„ç®—ä¸‹æ˜¾è‘—ä¼˜äºè‡ªæˆ‘ä¸€è‡´æ€§(Self-Consistency)å’Œå¼ºåŒ–å­¦ä¹ ç­‰å…ˆè¿›æ–¹æ³•ã€‚å®éªŒè¯æ˜ï¼Œé€šè¿‡è´´çº¸é©±åŠ¨çš„å†å²ç»éªŒåˆ©ç”¨èƒ½æœ‰æ•ˆå¢å¼ºLRMsçš„æ¨ç†æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 1 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.05007v2",
      "published_date": "2025-09-05 11:14:11 UTC",
      "updated_date": "2025-09-08 03:26:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:47:04.892641+00:00"
    },
    {
      "arxiv_id": "2509.05394v1",
      "title": "Reverse Browser: Vector-Image-to-Code Generator",
      "title_zh": "Reverse Browserï¼šçŸ¢é‡å›¾åƒè½¬ä»£ç ç”Ÿæˆå™¨",
      "authors": [
        "Zoltan Toth-Czifra"
      ],
      "abstract": "Automating the conversion of user interface design into code (image-to-code or image-to-UI) is an active area of software engineering research. However, the state-of-the-art solutions do not achieve high fidelity to the original design, as evidenced by benchmarks. In this work, I approach the problem differently: I use vector images instead of bitmaps as model input. I create several large datasets for training machine learning models. I evaluate the available array of Image Quality Assessment (IQA) algorithms and introduce a new, multi-scale metric. I then train a large open-weights model and discuss its limitations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Reverse Browserï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å®ç°çŸ¢é‡å›¾åƒåˆ°ä»£ç (Vector-Image-to-Code)è½¬æ¢çš„ç”Ÿæˆå™¨ï¼Œç”¨ä»¥è§£å†³å½“å‰å›¾åƒè½¬ç”¨æˆ·ç•Œé¢(image-to-UI)æŠ€æœ¯åœ¨è®¾è®¡è¿˜åŸåº¦ä¸Šå­˜åœ¨çš„ä¸è¶³ã€‚ä¸ä»¥å¾€é‡‡ç”¨ä½å›¾(bitmaps)çš„ç ”ç©¶è·¯å¾„ä¸åŒï¼Œè¯¥å·¥ä½œåˆ›æ–°æ€§åœ°ä½¿ç”¨çŸ¢é‡å›¾åƒä½œä¸ºæ¨¡å‹è¾“å…¥ï¼Œå¹¶ä¸ºæ­¤æ„å»ºäº†æ•°ä¸ªç”¨äºæ¨¡å‹è®­ç»ƒçš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚ç ”ç©¶è¿‡ç¨‹ä¸­ï¼Œä½œè€…æ·±å…¥è¯„ä¼°äº†ç°æœ‰çš„å›¾åƒè´¨é‡è¯„ä¼°(Image Quality Assessment, IQA)ç®—æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ç§å…¨æ–°çš„å¤šå°ºåº¦æŒ‡æ ‡(multi-scale metric)æ¥è¡¡é‡è½¬æ¢æ•ˆæœã€‚åŸºäºè¿™äº›ç ”ç©¶æˆæœï¼Œä½œè€…è®­ç»ƒäº†ä¸€ä¸ªå¤§å‹å¼€æºæƒé‡(open-weights)æ¨¡å‹ï¼Œå¹¶å¯¹å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½è¡¨ç°åŠå±€é™æ€§è¿›è¡Œäº†è¯¦ç»†æ¢è®¨ã€‚è¯¥ç ”ç©¶ä¸ºè‡ªåŠ¨åŒ–UIè®¾è®¡è½¬æ¢é¢†åŸŸæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ä¸è¯„ä¼°æ ‡å‡†ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Submitted to AIWare 2025 ArXiv Track",
      "pdf_url": "https://arxiv.org/pdf/2509.05394v1",
      "published_date": "2025-09-05 11:13:40 UTC",
      "updated_date": "2025-09-05 11:13:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:47:14.390991+00:00"
    },
    {
      "arxiv_id": "2509.04999v1",
      "title": "Adversarial Augmentation and Active Sampling for Robust Cyber Anomaly Detection",
      "title_zh": "é¢å‘é²æ£’ç½‘ç»œå¼‚å¸¸æ£€æµ‹çš„å¯¹æŠ—æ€§å¢å¼ºä¸ä¸»åŠ¨é‡‡æ ·",
      "authors": [
        "Sidahmed Benabderrahmane",
        "Talal Rahwan"
      ],
      "abstract": "Advanced Persistent Threats (APTs) present a considerable challenge to cybersecurity due to their stealthy, long-duration nature. Traditional supervised learning methods typically require large amounts of labeled data, which is often scarce in real-world scenarios. This paper introduces a novel approach that combines AutoEncoders for anomaly detection with active learning to iteratively enhance APT detection. By selectively querying an oracle for labels on uncertain or ambiguous samples, our method reduces labeling costs while improving detection accuracy, enabling the model to effectively learn with minimal data and reduce reliance on extensive manual labeling. We present a comprehensive formulation of the Attention Adversarial Dual AutoEncoder-based anomaly detection framework and demonstrate how the active learning loop progressively enhances the model's performance. The framework is evaluated on real-world, imbalanced provenance trace data from the DARPA Transparent Computing program, where APT-like attacks account for just 0.004\\% of the data. The datasets, which cover multiple operating systems including Android, Linux, BSD, and Windows, are tested in two attack scenarios. The results show substantial improvements in detection rates during active learning, outperforming existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜çº§æŒç»­æ€§å¨èƒ(Advanced Persistent Threats, APTs)éšè”½æ€§å¼ºä¸”æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆAutoEncoderså¼‚å¸¸æ£€æµ‹ä¸ä¸»åŠ¨å­¦ä¹ (active learning)çš„åˆ›æ–°æ¡†æ¶ã€‚æ ¸å¿ƒé‡‡ç”¨äº†åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å¯¹æŠ—åŒè‡ªåŠ¨ç¼–ç å™¨(Attention Adversarial Dual AutoEncoder)ï¼Œé€šè¿‡è¿­ä»£æŸ¥è¯¢ä¸ç¡®å®šæ ·æœ¬çš„æ ‡ç­¾ï¼Œåœ¨é™ä½æ ‡æ³¨æˆæœ¬çš„åŒæ—¶æœ‰æ•ˆæå‡äº†æ¨¡å‹å¯¹APTæ”»å‡»çš„æ£€æµ‹ç²¾åº¦ã€‚å®éªŒåœ¨DARPA Transparent Computingé¡¹ç›®çš„çœŸå®ã€é«˜åº¦ä¸å¹³è¡¡æº¯æºå›¾æ•°æ®ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå…¶ä¸­APTæ”»å‡»ä»…å æ•°æ®çš„0.004%ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Androidã€Linuxã€BSDå’ŒWindowsç­‰å¤šä¸ªæ“ä½œç³»ç»Ÿåœºæ™¯ä¸‹å‡å®ç°äº†æ£€æµ‹ç‡çš„æ˜¾è‘—æå‡ï¼Œå…¶è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼‚å¸¸æ£€æµ‹æŠ€æœ¯ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04999v1",
      "published_date": "2025-09-05 10:47:49 UTC",
      "updated_date": "2025-09-05 10:47:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:47:21.971079+00:00"
    },
    {
      "arxiv_id": "2509.04993v1",
      "title": "LLM Enabled Multi-Agent System for 6G Networks: Framework and Method of Dual-Loop Edge-Terminal Collaboration",
      "title_zh": "é¢å‘6Gç½‘ç»œçš„LLMèµ‹èƒ½å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼šè¾¹ç«¯åŒç¯ååŒæ¡†æ¶ä¸æ–¹æ³•",
      "authors": [
        "Zheyan Qu",
        "Wenbo Wang",
        "Zitong Yu",
        "Boquan Sun",
        "Yang Li",
        "Xing Zhang"
      ],
      "abstract": "The ubiquitous computing resources in 6G networks provide ideal environments for the fusion of large language models (LLMs) and intelligent services through the agent framework. With auxiliary modules and planning cores, LLM-enabled agents can autonomously plan and take actions to deal with diverse environment semantics and user intentions. However, the limited resources of individual network devices significantly hinder the efficient operation of LLM-enabled agents with complex tool calls, highlighting the urgent need for efficient multi-level device collaborations. To this end, the framework and method of the LLM-enabled multi-agent system with dual-loop terminal-edge collaborations are proposed in 6G networks. Firstly, the outer loop consists of the iterative collaborations between the global agent and multiple sub-agents deployed on edge servers and terminals, where the planning capability is enhanced through task decomposition and parallel sub-task distribution. Secondly, the inner loop utilizes sub-agents with dedicated roles to circularly reason, execute, and replan the sub-task, and the parallel tool calling generation with offloading strategies is incorporated to improve efficiency. The improved task planning capability and task execution efficiency are validated through the conducted case study in 6G-supported urban safety governance. Finally, the open challenges and future directions are thoroughly analyzed in 6G networks, accelerating the advent of the 6G era.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ 6G ç½‘ç»œä¸­å•ä½“è®¾å¤‡èµ„æºæœ‰é™ã€éš¾ä»¥é«˜æ•ˆè¿è¡Œå¤æ‚å·¥å…·è°ƒç”¨çš„ LLM-enabled agents é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºåŒç¯è·¯ç«¯è¾¹åä½œ (dual-loop terminal-edge collaboration) çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¡†æ¶ã€‚æ¡†æ¶çš„å¤–ç¯ (outer loop) ç”±éƒ¨ç½²åœ¨è¾¹ç¼˜æœåŠ¡å™¨å’Œç»ˆç«¯ä¸Šçš„å…¨å±€æ™ºèƒ½ä½“ä¸å¤šä¸ªå­æ™ºèƒ½ä½“ç»„æˆï¼Œé€šè¿‡ä»»åŠ¡åˆ†è§£ (task decomposition) å’Œå¹¶è¡Œå­ä»»åŠ¡åˆ†å‘æ¥å¢å¼ºç³»ç»Ÿçš„æ•´ä½“è§„åˆ’èƒ½åŠ›ã€‚å†…ç¯ (inner loop) åˆ™åˆ©ç”¨å…·æœ‰ç‰¹å®šè§’è‰²çš„å­æ™ºèƒ½ä½“è¿›è¡Œå¾ªç¯æ¨ç†ã€æ‰§è¡Œå’Œé‡æ–°è§„åˆ’ï¼Œå¹¶ç»“åˆå¹¶è¡Œå·¥å…·è°ƒç”¨ (parallel tool calling) ä¸å¸è½½ç­–ç•¥ (offloading strategies) ä»¥ä¼˜åŒ–æ‰§è¡Œæ•ˆç‡ã€‚é€šè¿‡åœ¨ 6G æ”¯æŒçš„åŸå¸‚å®‰å…¨æ²»ç†æ¡ˆä¾‹ä¸­çš„åº”ç”¨ï¼Œå®éªŒéªŒè¯äº†è¯¥æ–¹æ¡ˆåœ¨ä»»åŠ¡è§„åˆ’ä¸æ‰§è¡Œæ•ˆç‡æ–¹é¢çš„æ˜¾è‘—æå‡ã€‚è¯¥ç ”ç©¶æœ€åæ·±å…¥åˆ†æäº† 6G ç½‘ç»œä¸­çš„å¼€æ”¾æŒ‘æˆ˜ä¸æœªæ¥æ–¹å‘ï¼Œä¸ºåŠ é€Ÿ 6G æ—¶ä»£çš„æ™ºèƒ½æœåŠ¡èåˆæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "This paper has been accepted by IEEE Communications Magazine",
      "pdf_url": "https://arxiv.org/pdf/2509.04993v1",
      "published_date": "2025-09-05 10:40:31 UTC",
      "updated_date": "2025-09-05 10:40:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:47:28.085362+00:00"
    },
    {
      "arxiv_id": "2509.05393v1",
      "title": "Inferring Prerequisite Knowledge Concepts in Educational Knowledge Graphs: A Multi-criteria Approach",
      "title_zh": "æ•™è‚²çŸ¥è¯†å›¾è°±ä¸­çš„å…ˆä¿®çŸ¥è¯†æ¦‚å¿µæ¨æ–­ï¼šä¸€ç§å¤šå‡†åˆ™æ–¹æ³•",
      "authors": [
        "Rawaa Alatrash",
        "Mohamed Amine Chatti",
        "Nasha Wibowo",
        "Qurat Ul Ain"
      ],
      "abstract": "Educational Knowledge Graphs (EduKGs) organize various learning entities and their relationships to support structured and adaptive learning. Prerequisite relationships (PRs) are critical in EduKGs for defining the logical order in which concepts should be learned. However, the current EduKG in the MOOC platform CourseMapper lacks explicit PR links, and manually annotating them is time-consuming and inconsistent. To address this, we propose an unsupervised method for automatically inferring concept PRs without relying on labeled data. We define ten criteria based on document-based, Wikipedia hyperlink-based, graph-based, and text-based features, and combine them using a voting algorithm to robustly capture PRs in educational content. Experiments on benchmark datasets show that our approach achieves higher precision than existing methods while maintaining scalability and adaptability, thus providing reliable support for sequence-aware learning in CourseMapper.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ•™è‚²çŸ¥è¯†å›¾è°± (Educational Knowledge Graphs, EduKGs) ä¸­å…ˆéªŒå…³ç³» (Prerequisite Relationships, PRs) çš„ç¼ºå¤±é—®é¢˜ï¼ŒæŒ‡å‡ºæ‰‹åŠ¨æ ‡æ³¨åœ¨ CourseMapper ç­‰ MOOC å¹³å°ä¸­ä¸ä»…è´¹æ—¶ä¸”ç¼ºä¹ä¸€è‡´æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ— éœ€æ ‡è®°æ•°æ®çš„æ— ç›‘ç£æ–¹æ³•ï¼Œæ—¨åœ¨è‡ªåŠ¨æ¨æ–­æ¦‚å¿µä¹‹é—´çš„å…ˆéªŒå…³ç³»ã€‚è¯¥æ–¹æ³•å®šä¹‰äº†åŸºäºæ–‡æ¡£ã€Wikipedia è¶…é“¾æ¥ã€å›¾ç»“æ„åŠæ–‡æœ¬ç‰¹å¾çš„åé¡¹æ ‡å‡†ï¼Œå¹¶åˆ©ç”¨æŠ•ç¥¨ç®—æ³• (voting algorithm) è¿›è¡Œç»„åˆï¼Œä»¥ç¨³å¥åœ°æ•æ‰æ•™è‚²å†…å®¹ä¸­çš„å…ˆéªŒé€»è¾‘ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå¯æ‰©å±•æ€§å’Œé€‚åº”æ€§çš„åŒæ—¶ï¼Œæ¯”ç°æœ‰æ–¹æ³•å…·æœ‰æ›´é«˜çš„å‡†ç¡®ç‡ (precision)ã€‚è¿™ä¸€æˆæœä¸º CourseMapper å¹³å°æä¾›äº†å¯é çš„åºåˆ—æ„ŸçŸ¥å­¦ä¹  (sequence-aware learning) æ”¯æŒï¼Œæœ‰åŠ©äºå®ç°ç»“æ„åŒ–å’Œè‡ªé€‚åº”çš„å­¦ä¹ è·¯å¾„ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted at IJCKG 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.05393v1",
      "published_date": "2025-09-05 10:37:58 UTC",
      "updated_date": "2025-09-05 10:37:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:47:19.779417+00:00"
    },
    {
      "arxiv_id": "2509.04991v1",
      "title": "High-Resolution Global Land Surface Temperature Retrieval via a Coupled Mechanism-Machine Learning Framework",
      "title_zh": "åŸºäºæœºç†-æœºå™¨å­¦ä¹ è€¦åˆæ¡†æ¶çš„é«˜åˆ†è¾¨ç‡å…¨çƒåœ°è¡¨æ¸©åº¦åæ¼”",
      "authors": [
        "Tian Xie",
        "Huanfeng Shen",
        "Menghui Jiang",
        "Juan-Carlos JimÃ©nez-MuÃ±oz",
        "JosÃ© A. Sobrino",
        "Huifang Li",
        "Chao Zeng"
      ],
      "abstract": "Land surface temperature (LST) is vital for land-atmosphere interactions and climate processes. Accurate LST retrieval remains challenging under heterogeneous land cover and extreme atmospheric conditions. Traditional split window (SW) algorithms show biases in humid environments; purely machine learning (ML) methods lack interpretability and generalize poorly with limited data. We propose a coupled mechanism model-ML (MM-ML) framework integrating physical constraints with data-driven learning for robust LST retrieval. Our approach fuses radiative transfer modeling with data components, uses MODTRAN simulations with global atmospheric profiles, and employs physics-constrained optimization. Validation against 4,450 observations from 29 global sites shows MM-ML achieves MAE=1.84K, RMSE=2.55K, and R-squared=0.966, outperforming conventional methods. Under extreme conditions, MM-ML reduces errors by over 50%. Sensitivity analysis indicates LST estimates are most sensitive to sensor radiance, then water vapor, and less to emissivity, with MM-ML showing superior stability. These results demonstrate the effectiveness of our coupled modeling strategy for retrieving geophysical parameters. The MM-ML framework combines physical interpretability with nonlinear modeling capacity, enabling reliable LST retrieval in complex environments and supporting climate monitoring and ecosystem studies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§è€¦åˆæœºåˆ¶æ¨¡å‹ä¸æœºå™¨å­¦ä¹ (MM-ML)çš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é«˜åˆ†è¾¨ç‡å…¨çƒåœ°è¡¨æ¸©åº¦(Land Surface Temperature, LST)çš„ç²¾ç¡®åæ¼”ã€‚è¯¥æ¡†æ¶å°†ç‰©ç†çº¦æŸï¼ˆåŒ…æ‹¬è¾å°„ä¼ è¾“å»ºæ¨¡ä¸MODTRANæ¨¡æ‹Ÿï¼‰ä¸æ•°æ®é©±åŠ¨çš„å­¦ä¹ æ–¹æ³•ç›¸ç»“åˆï¼Œå…‹æœäº†ä¼ ç»Ÿåˆ†è£‚çª—(Split Window)ç®—æ³•åœ¨æ½®æ¹¿ç¯å¢ƒä¸‹çš„åå·®ä»¥åŠçº¯æœºå™¨å­¦ä¹ æ–¹æ³•ç¼ºä¹å¯è§£é‡Šæ€§çš„å±€é™ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMM-MLåœ¨å…¨çƒ29ä¸ªç«™ç‚¹çš„éªŒè¯ä¸­è¾¾åˆ°äº†1.84Kçš„MAEå’Œ2.55Kçš„RMSEï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨æç«¯å¤§æ°”æ¡ä»¶ä¸‹ï¼Œè¯¥æ¡†æ¶å°†è¯¯å·®é™ä½äº†50%ä»¥ä¸Šï¼Œå±•ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚æ•æ„Ÿæ€§åˆ†æè¿›ä¸€æ­¥è¯å®è¯¥æ–¹æ³•å¯¹ä¼ æ„Ÿå™¨è¾äº®åº¦(Sensor Radiance)å’Œæ°´æ±½å˜åŒ–å…·æœ‰å“è¶Šçš„ç¨³å®šæ€§ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†ç‰©ç†æœºåˆ¶ä¸éçº¿æ€§å»ºæ¨¡èƒ½åŠ›çš„ç»“åˆå¯ä»¥æœ‰æ•ˆæå‡åœ°è¡¨å‚æ•°åæ¼”çš„å¯é æ€§ï¼Œä¸ºå…¨çƒæ°”å€™ç›‘æµ‹å’Œç”Ÿæ€ç³»ç»Ÿç ”ç©¶æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "physics.ao-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04991v1",
      "published_date": "2025-09-05 10:37:27 UTC",
      "updated_date": "2025-09-05 10:37:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:47:26.791469+00:00"
    },
    {
      "arxiv_id": "2509.05392v1",
      "title": "An Optimized Pipeline for Automatic Educational Knowledge Graph Construction",
      "title_zh": "ä¸€ç§ä¼˜åŒ–çš„è‡ªåŠ¨æ•™è‚²çŸ¥è¯†å›¾è°±æ„å»ºç®¡çº¿",
      "authors": [
        "Qurat Ul Ain",
        "Mohamed Amine Chatti",
        "Jean Qussa",
        "Amr Shakhshir",
        "Rawaa Alatrash",
        "Shoeb Joarder"
      ],
      "abstract": "The automatic construction of Educational Knowledge Graphs (EduKGs) is essential for domain knowledge modeling by extracting meaningful representations from learning materials. Despite growing interest, identifying a scalable and reliable approach for automatic EduKG generation remains a challenge. In an attempt to develop a unified and robust pipeline for automatic EduKG construction, in this study we propose a pipeline for automatic EduKG construction from PDF learning materials. The process begins with generating slide-level EduKGs from individual pages/slides, which are then merged to form a comprehensive EduKG representing the entire learning material. We evaluate the accuracy of the EduKG generated from the proposed pipeline in our MOOC platform, CourseMapper. The observed accuracy, while indicative of partial success, is relatively low particularly in the educational context, where the reliability of knowledge representations is critical for supporting meaningful learning. To address this, we introduce targeted optimizations across multiple pipeline components. The optimized pipeline achieves a 17.5% improvement in accuracy and a tenfold increase in processing efficiency. Our approach offers a holistic, scalable and end-to-end pipeline for automatic EduKG construction, adaptable to diverse educational contexts, and supports improved semantic representation of learning content.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºè‡ªåŠ¨æ„å»ºæ•™è‚²çŸ¥è¯†å›¾è°±(Educational Knowledge Graphs, EduKGs)çš„ä¼˜åŒ–æµæ°´çº¿æ¡†æ¶ï¼Œæ—¨åœ¨ä»PDFå­¦ä¹ ææ–™ä¸­é«˜æ•ˆæå–é¢†åŸŸçŸ¥è¯†ã€‚è¯¥æ–¹æ³•é¦–å…ˆä»å•ä¸ªé¡µé¢æˆ–å¹»ç¯ç‰‡ä¸­ç”Ÿæˆå¹»ç¯ç‰‡çº§åˆ«çš„EduKGsï¼Œéšåå°†å…¶åˆå¹¶ä»¥å½¢æˆä»£è¡¨å®Œæ•´å­¦ä¹ ææ–™çš„ç»¼åˆæ€§çŸ¥è¯†å›¾è°±ã€‚é’ˆå¯¹åˆå§‹ç‰ˆæœ¬åœ¨MOOCå¹³å°CourseMapperä¸­è¡¨ç°å‡ºçš„å‡†ç¡®ç‡é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¯¹æµæ°´çº¿çš„å¤šä¸ªç»„ä»¶è¿›è¡Œäº†å®šå‘ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¼˜åŒ–åçš„æµæ°´çº¿åœ¨å‡†ç¡®ç‡ä¸Šæå‡äº†17.5%ï¼Œä¸”å¤„ç†æ•ˆç‡æé«˜äº†åå€ã€‚è¯¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ•´ä½“ã€å¯æ‰©å±•ä¸”ç«¯åˆ°ç«¯çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿé€‚åº”å¤šæ ·çš„æ•™è‚²è¯­å¢ƒå¹¶æ˜¾è‘—æ”¹å–„å­¦ä¹ å†…å®¹çš„è¯­ä¹‰è¡¨ç¤ºã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted at IJCKG 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.05392v1",
      "published_date": "2025-09-05 10:29:46 UTC",
      "updated_date": "2025-09-05 10:29:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:47:40.092571+00:00"
    },
    {
      "arxiv_id": "2509.10531v1",
      "title": "FinXplore: An Adaptive Deep Reinforcement Learning Framework for Balancing and Discovering Investment Opportunities",
      "title_zh": "FinXploreï¼šä¸€ç§ç”¨äºå¹³è¡¡ä¸æŒ–æ˜æŠ•èµ„æœºä¼šçš„è‡ªé€‚åº”æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Himanshu Choudhary",
        "Arishi Orra",
        "Manoj Thakur"
      ],
      "abstract": "Portfolio optimization is essential for balancing risk and return in financial decision-making. Deep Reinforcement Learning (DRL) has stood out as a cutting-edge tool for portfolio optimization that learns dynamic asset allocation using trial-and-error interactions. However, most DRL-based methods are restricted to allocating assets within a pre-defined investment universe and overlook exploring new opportunities. This study introduces an investment landscape that integrates exploiting existing assets with exploring new investment opportunities in an extended universe. The proposed approach leverages two DRL agents and dynamically balances these objectives to adapt to evolving markets while enhancing portfolio performance. One agent allocates assets within the existing universe, while another assists in exploring new opportunities in the extended universe. The effciency of the proposed methodology is determined using two real-world market data sets. The experiments demonstrate the superiority of the suggested approach against the state-of-the-art portfolio strategies and baseline methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FinXploreï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¹³è¡¡ç°æœ‰èµ„äº§åˆ©ç”¨ä¸æ–°æŠ•èµ„æœºä¼šæ¢ç´¢çš„è‡ªé€‚åº”æ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep Reinforcement Learning)æ¡†æ¶ã€‚é’ˆå¯¹ä¼ ç»Ÿæ–¹æ³•å±€é™äºé¢„å®šä¹‰æŠ•èµ„èŒƒå›´è€Œå¿½è§†æ–°æœºä¼šçš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†æŠ•èµ„æ™¯è§‚(investment landscape)æ¦‚å¿µï¼Œåˆ©ç”¨ä¸¤ä¸ªååŒçš„DRLæ™ºèƒ½ä½“åˆ†åˆ«åœ¨ç°æœ‰å®‡å®™(existing universe)å’Œæ‰©å±•å®‡å®™(extended universe)ä¸­è¿›è¡Œèµ„äº§é…ç½®ä¸æœºä¼šæŒ–æ˜ã€‚è¿™ç§åŒæ™ºèƒ½ä½“æœºåˆ¶èƒ½å¤ŸåŠ¨æ€è°ƒæ•´ç›®æ ‡æƒé‡ï¼Œä»¥é€‚åº”ä¸æ–­æ¼”å˜çš„å¸‚åœºç¯å¢ƒå¹¶å¢å¼ºæŠ•èµ„ç»„åˆä¼˜åŒ–(Portfolio optimization)çš„æ•ˆæœã€‚åœ¨ä¸¤ä¸ªçœŸå®ä¸–ç•Œå¸‚åœºæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFinXploreçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æŠ•èµ„ç»„åˆç­–ç•¥(portfolio strategies)åŠåŸºå‡†æ–¹æ³•ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆæ¢ç´¢ä¸åˆ©ç”¨æœºåˆ¶ï¼Œä¸ºé‡‘èå†³ç­–ä¸­çš„åŠ¨æ€èµ„äº§é…ç½®æä¾›äº†æ›´å…·ç«äº‰åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.10531v1",
      "published_date": "2025-09-05 10:20:32 UTC",
      "updated_date": "2025-09-05 10:20:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:47:50.456522+00:00"
    },
    {
      "arxiv_id": "2509.04983v1",
      "title": "Exploring an implementation of quantum learning pipeline for support vector machines",
      "title_zh": "æ”¯æŒå‘é‡æœºé‡å­å­¦ä¹ æµæ°´çº¿çš„å®ç°æ¢ç©¶",
      "authors": [
        "Mario Bifulco",
        "Luca Roversi"
      ],
      "abstract": "This work presents a fully quantum approach to support vector machine (SVM) learning by integrating gate-based quantum kernel methods with quantum annealing-based optimization. We explore the construction of quantum kernels using various feature maps and qubit configurations, evaluating their suitability through Kernel-Target Alignment (KTA). The SVM dual problem is reformulated as a Quadratic Unconstrained Binary Optimization (QUBO) problem, enabling its solution via quantum annealers. Our experiments demonstrate that a high degree of alignment in the kernel and an appropriate regularization parameter lead to competitive performance, with the best model achieving an F1-score of 90%. These results highlight the feasibility of an end-to-end quantum learning pipeline and the potential of hybrid quantum architectures in quantum high-performance computing (QHPC) contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢ç´¢äº†æ”¯æŒå‘é‡æœº(SVM)çš„å…¨é‡å­å­¦ä¹ æµç¨‹å®ç°ï¼Œé€šè¿‡å°†åŸºäºé—¨ç”µè·¯çš„é‡å­æ ¸æ–¹æ³•(gate-based quantum kernel methods)ä¸åŸºäºé‡å­é€€ç«(quantum annealing)çš„ä¼˜åŒ–æŠ€æœ¯ç›¸ç»“åˆã€‚ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨å¤šç§ç‰¹å¾æ˜ å°„(feature maps)å’Œé‡å­æ¯”ç‰¹é…ç½®æ„å»ºé‡å­æ ¸ï¼Œå¹¶åˆ©ç”¨æ ¸ç›®æ ‡å¯¹é½(Kernel-Target Alignment, KTA)è¯„ä¼°å…¶é€‚ç”¨æ€§ã€‚SVMçš„å¯¹å¶é—®é¢˜è¢«é‡æ–°è¡¨è¿°ä¸ºäºŒæ¬¡æ— çº¦æŸäºŒè¿›åˆ¶ä¼˜åŒ–(QUBO)é—®é¢˜ï¼Œä»è€Œèƒ½å¤Ÿåˆ©ç”¨é‡å­é€€ç«æœºè¿›è¡Œæ±‚è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé«˜ç¨‹åº¦çš„æ ¸å¯¹é½å’Œé€‚å½“çš„æ­£åˆ™åŒ–å‚æ•°èƒ½å¸¦æ¥æå…·ç«äº‰åŠ›çš„è¡¨ç°ï¼Œæœ€ä½³æ¨¡å‹å®ç°äº†90%çš„F1-scoreã€‚è¿™äº›å‘ç°éªŒè¯äº†ç«¯åˆ°ç«¯é‡å­å­¦ä¹ æµç¨‹çš„å¯è¡Œæ€§ï¼Œå¹¶å±•ç¤ºäº†æ··åˆé‡å­æ¶æ„åœ¨é‡å­é«˜æ€§èƒ½è®¡ç®—(QHPC)ç¯å¢ƒä¸­çš„æ½œåŠ›ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04983v1",
      "published_date": "2025-09-05 10:19:32 UTC",
      "updated_date": "2025-09-05 10:19:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:47:46.254803+00:00"
    },
    {
      "arxiv_id": "2509.05390v1",
      "title": "Authorship Without Writing: Large Language Models and the Senior Author Analogy",
      "title_zh": "æ— é¡»åŠ¨ç¬”çš„ä½œè€…èº«ä»½ï¼šå¤§è¯­è¨€æ¨¡å‹ä¸èµ„æ·±ä½œè€…ç±»æ¯”",
      "authors": [
        "Clint Hurshman",
        "Sebastian Porsdam Mann",
        "Julian Savulescu",
        "Brian D. Earp"
      ],
      "abstract": "The use of large language models (LLMs) in bioethical, scientific, and medical writing remains controversial. While there is broad agreement in some circles that LLMs cannot count as authors, there is no consensus about whether and how humans using LLMs can count as authors. In many fields, authorship is distributed among large teams of researchers, some of whom, including paradigmatic senior authors who guide and determine the scope of a project and ultimately vouch for its integrity, may not write a single word. In this paper, we argue that LLM use (under specific conditions) is analogous to a form of senior authorship. On this view, the use of LLMs, even to generate complete drafts of research papers, can be considered a legitimate form of authorship according to the accepted criteria in many fields. We conclude that either such use should be recognized as legitimate, or current criteria for authorship require fundamental revision. AI use declaration: GPT-5 was used to help format Box 1. AI was not used for any other part of the preparation or writing of this manuscript.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨ç”Ÿç‰©ä¼¦ç†ã€ç§‘å­¦åŠåŒ»å­¦å†™ä½œä¸­å¼•å‘çš„ä½œè€…èº«ä»½(Authorship)äº‰è®®ã€‚æ–‡ç« æŒ‡å‡ºï¼Œåœ¨è®¸å¤šç ”ç©¶é¢†åŸŸï¼Œèµ„æ·±ä½œè€…(Senior Authors)å³ä½¿ä¸äº²è‡ªæ’°å†™æ–‡å­—ï¼Œä»…é€šè¿‡ç¡®å®šé¡¹ç›®èŒƒå›´å¹¶å¯¹ç ”ç©¶å®Œæ•´æ€§è´Ÿè´£ä¹Ÿèƒ½è·å¾—ä½œè€…èº«ä»½ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶è®¤ä¸ºåœ¨ç‰¹å®šæ¡ä»¶ä¸‹ä½¿ç”¨ LLMs ç”Ÿæˆè®ºæ–‡è‰ç¨¿åº”è¢«è§†ä¸ºä¸€ç§ç±»ä¼¼äºèµ„æ·±ä½œè€…èº«ä»½çš„åˆæ³•å½¢å¼ã€‚ç ”ç©¶å¾—å‡ºç»“è®ºï¼ŒæŒ‰ç…§ç°è¡Œçš„å­¦æœ¯æ ‡å‡†ï¼Œè¿™ç§ä½¿ç”¨æ–¹å¼åº”å½“è¢«æ‰¿è®¤ï¼Œå¦åˆ™ç›®å‰çš„ä½œè€…èº«ä»½è®¤å®šæ ‡å‡†(Criteria for Authorship)åˆ™éœ€è¦è¿›è¡Œæ ¹æœ¬æ€§çš„ä¿®è®¢ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "28 pages, 0 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.05390v1",
      "published_date": "2025-09-05 10:11:56 UTC",
      "updated_date": "2025-09-05 10:11:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:47:58.563741+00:00"
    },
    {
      "arxiv_id": "2509.04979v1",
      "title": "Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for Ranking Agents",
      "title_zh": "äº’è”ç½‘ 3.0ï¼šæ™ºèƒ½ä½“ç½‘ç»œæ¶æ„åŠå…¶æ™ºèƒ½ä½“æ’åºç®—æ³•",
      "authors": [
        "Rajesh Tembarai Krishnamachari",
        "Srividya Rajesh"
      ],
      "abstract": "AI agents -- powered by reasoning-capable large language models (LLMs) and integrated with tools, data, and web search -- are poised to transform the internet into a \\emph{Web of Agents}: a machine-native ecosystem where autonomous agents interact, collaborate, and execute tasks at scale. Realizing this vision requires \\emph{Agent Ranking} -- selecting agents not only by declared capabilities but by proven, recent performance. Unlike Web~1.0's PageRank, a global, transparent network of agent interactions does not exist; usage signals are fragmented and private, making ranking infeasible without coordination.\n  We propose \\textbf{DOVIS}, a five-layer operational protocol (\\emph{Discovery, Orchestration, Verification, Incentives, Semantics}) that enables the collection of minimal, privacy-preserving aggregates of usage and performance across the ecosystem. On this substrate, we implement \\textbf{AgentRank-UC}, a dynamic, trust-aware algorithm that combines \\emph{usage} (selection frequency) and \\emph{competence} (outcome quality, cost, safety, latency) into a unified ranking. We present simulation results and theoretical guarantees on convergence, robustness, and Sybil resistance, demonstrating the viability of coordinated protocols and performance-aware ranking in enabling a scalable, trustworthy Agentic Web.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”±å¤§è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨çš„æ™ºèƒ½ä½“æ­£å°†äº’è”ç½‘è½¬å‹ä¸ºæ™ºèƒ½ä½“ç½‘(Web of Agents)çš„è¶‹åŠ¿ï¼Œå¹¶æŒ‡å‡ºå®ç°è¯¥æ„¿æ™¯çš„å…³é”®æŒ‘æˆ˜åœ¨äºå»ºç«‹ä¸€ç§åŸºäºå®é™…æ€§èƒ½è€Œéä»…å‡­å£°æ˜çš„æ’åæœºåˆ¶ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†DOVISåè®®ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«Discoveryã€Orchestrationã€Verificationã€Incentiveså’ŒSemanticsäº”ä¸ªå±‚çº§çš„æ¶æ„ï¼Œæ—¨åœ¨ä»¥ä¿æŠ¤éšç§çš„æ–¹å¼è·¨ç”Ÿæ€ç³»ç»Ÿæ”¶é›†ä½¿ç”¨ä¸æ€§èƒ½æŒ‡æ ‡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶å®ç°äº†ä¸€ç§åä¸ºAgentRank-UCçš„åŠ¨æ€ä¿¡ä»»æ„ŸçŸ¥ç®—æ³•ï¼Œè¯¥ç®—æ³•å°†ä½¿ç”¨é¢‘ç‡(Usage)ä¸èƒœä»»åŠ›(Competenceï¼Œæ¶µç›–ç»“æœè´¨é‡ã€æˆæœ¬ã€å®‰å…¨å’Œå»¶è¿Ÿ)æ•´åˆè¿›ç»Ÿä¸€çš„æ’åè¯„ä»·ä½“ç³»ã€‚æ¨¡æ‹Ÿå®éªŒå’Œç†è®ºåˆ†æè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ”¶æ•›æ€§ã€é²æ£’æ€§å’ŒæŠ—å¥³å·«æ”»å‡»(Sybil resistance)æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚è¯¥é¡¹ç ”ç©¶ä¸ºæ„å»ºä¸€ä¸ªå¯æ‰©å±•ã€å¯ä¿¡èµ–ä¸”å…·å¤‡æ€§èƒ½æ„ŸçŸ¥èƒ½åŠ›çš„æ™ºèƒ½ä½“åŒ–ç½‘ç»œ(Agentic Web)å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04979v1",
      "published_date": "2025-09-05 10:04:33 UTC",
      "updated_date": "2025-09-05 10:04:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:47:57.283829+00:00"
    },
    {
      "arxiv_id": "2509.04970v1",
      "title": "DeGuV: Depth-Guided Visual Reinforcement Learning for Generalization and Interpretability in Manipulation",
      "title_zh": "DeGuVï¼šé¢å‘æ“ä½œä»»åŠ¡æ³›åŒ–æ€§ä¸å¯è§£é‡Šæ€§çš„æ·±åº¦å¼•å¯¼è§†è§‰å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Tien Pham",
        "Xinyun Chi",
        "Khang Nguyen",
        "Manfred Huber",
        "Angelo Cangelosi"
      ],
      "abstract": "Reinforcement learning (RL) agents can learn to solve complex tasks from visual inputs, but generalizing these learned skills to new environments remains a major challenge in RL application, especially robotics. While data augmentation can improve generalization, it often compromises sample efficiency and training stability. This paper introduces DeGuV, an RL framework that enhances both generalization and sample efficiency. In specific, we leverage a learnable masker network that produces a mask from the depth input, preserving only critical visual information while discarding irrelevant pixels. Through this, we ensure that our RL agents focus on essential features, improving robustness under data augmentation. In addition, we incorporate contrastive learning and stabilize Q-value estimation under augmentation to further enhance sample efficiency and training stability. We evaluate our proposed method on the RL-ViGen benchmark using the Franka Emika robot and demonstrate its effectiveness in zero-shot sim-to-real transfer. Our results show that DeGuV outperforms state-of-the-art methods in both generalization and sample efficiency while also improving interpretability by highlighting the most relevant regions in the visual input",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DeGuVï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æå‡æœºå™¨äººæ“çºµä»»åŠ¡æ³›åŒ–èƒ½åŠ›å’Œæ ·æœ¬æ•ˆç‡çš„æ·±åº¦å¼•å¯¼è§†è§‰å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æ¡†æ¶ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„æ©ç ç½‘ç»œ (masker network)ï¼Œé€šè¿‡æ·±åº¦è¾“å…¥ç”Ÿæˆæ©ç ä»¥ä¿ç•™å…³é”®è§†è§‰ç‰¹å¾å¹¶æ»¤é™¤æ— å…³åƒç´ ï¼Œä»è€Œç¡®ä¿å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“åœ¨æ•°æ®å¢å¼ºç¯å¢ƒä¸‹ä¿æŒé²æ£’ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¼•å…¥å¯¹æ¯”å­¦ä¹  (contrastive learning) å¹¶ç¨³å®šæ•°æ®å¢å¼ºä¸‹çš„ Q å€¼ä¼°è®¡ï¼Œè¿›ä¸€æ­¥æå‡äº†è®­ç»ƒçš„æ ·æœ¬æ•ˆç‡ä¸ç¨³å®šæ€§ã€‚åœ¨ RL-ViGen åŸºå‡†æµ‹è¯•åŠ Franka Emika æœºå™¨äººçš„å®éªŒä¸­ï¼ŒDeGuV å±•ç¤ºäº†å“è¶Šçš„é›¶æ ·æœ¬ (zero-shot) æ¨¡æ‹Ÿåˆ°ç°å® (sim-to-real) è¿ç§»èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ³›åŒ–æ€§èƒ½å’Œæ ·æœ¬æ•ˆç‡ä¸Šå‡ä¼˜äºå½“å‰æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå¹¶èƒ½é€šè¿‡çªå‡ºæ˜¾ç¤ºè§†è§‰è¾“å…¥ä¸­çš„ç›¸å…³åŒºåŸŸæœ‰æ•ˆå¢å¼ºæ¨¡å‹çš„å¯è§£é‡Šæ€§ (interpretability)ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04970v1",
      "published_date": "2025-09-05 09:52:08 UTC",
      "updated_date": "2025-09-05 09:52:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:48:00.362057+00:00"
    },
    {
      "arxiv_id": "2509.04926v1",
      "title": "Towards Ontology-Based Descriptions of Conversations with Qualitatively-Defined Concepts",
      "title_zh": "é¢å‘å®šæ€§å®šä¹‰æ¦‚å¿µçš„åŸºäºæœ¬ä½“çš„å¯¹è¯æè¿°",
      "authors": [
        "Barbara Gendron",
        "GaÃ«l Guibon",
        "Mathieu D'aquin"
      ],
      "abstract": "The controllability of Large Language Models (LLMs) when used as conversational agents is a key challenge, particularly to ensure predictable and user-personalized responses. This work proposes an ontology-based approach to formally define conversational features that are typically qualitative in nature. By leveraging a set of linguistic descriptors, we derive quantitative definitions for qualitatively-defined concepts, enabling their integration into an ontology for reasoning and consistency checking. We apply this framework to the task of proficiency-level control in conversations, using CEFR language proficiency levels as a case study. These definitions are then formalized in description logic and incorporated into an ontology, which guides controlled text generation of an LLM through fine-tuning. Experimental results demonstrate that our approach provides consistent and explainable proficiency-level definitions, improving transparency in conversational AI.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) ä½œä¸ºå¯¹è¯æ™ºèƒ½ä½“æ—¶å¯æ§æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæœ¬ä½“ (ontology-based) çš„æ–¹æ³•æ¥å½¢å¼åŒ–å®šä¹‰å®šæ€§æ€§è´¨çš„å¯¹è¯ç‰¹å¾ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä¸€ç³»åˆ—è¯­è¨€å­¦æè¿°ç¬¦ä¸ºå®šæ€§æ¦‚å¿µæ¨å¯¼å‡ºå®šé‡å®šä¹‰ï¼Œå¹¶å°†å…¶æ•´åˆåˆ°æœ¬ä½“ä¸­ä»¥å®ç°æ¨ç†å’Œä¸€è‡´æ€§æ£€æŸ¥ã€‚ç ”ç©¶äººå‘˜å°†è¯¥æ¡†æ¶åº”ç”¨äºå¯¹è¯ç†Ÿç»ƒåº¦ç­‰çº§æ§åˆ¶ä»»åŠ¡ï¼Œå¹¶ä»¥æ¬§æ´²è¯­è¨€å…±åŒå‚è€ƒæ¡†æ¶ (CEFR) ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ã€‚é€šè¿‡æè¿°é€»è¾‘ (description logic) å½¢å¼åŒ–è¿™äº›å®šä¹‰å¹¶å°†å…¶çº³å…¥æœ¬ä½“ï¼Œè¯¥æ¡†æ¶èƒ½å¤ŸæŒ‡å¯¼ LLM åœ¨å¾®è°ƒ (fine-tuning) è¿‡ç¨‹ä¸­è¿›è¡Œå—æ§æ–‡æœ¬ç”Ÿæˆã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•æä¾›äº†è¿è´¯ä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„ç†Ÿç»ƒåº¦ç­‰çº§å®šä¹‰ï¼Œæ˜¾è‘—æå‡äº†å¯¹è¯å¼äººå·¥æ™ºèƒ½ (Conversational AI) çš„é€æ˜åº¦ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at TOTh 2025 (Terminology \\& Ontology: Theories and applications)",
      "pdf_url": "https://arxiv.org/pdf/2509.04926v1",
      "published_date": "2025-09-05 08:44:27 UTC",
      "updated_date": "2025-09-05 08:44:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:47:58.759794+00:00"
    },
    {
      "arxiv_id": "2509.04923v1",
      "title": "Artificial intelligence for representing and characterizing quantum systems",
      "title_zh": "ç”¨äºé‡å­ç³»ç»Ÿè¡¨ç¤ºä¸è¡¨å¾çš„äººå·¥æ™ºèƒ½",
      "authors": [
        "Yuxuan Du",
        "Yan Zhu",
        "Yuan-Hang Zhang",
        "Min-Hsiu Hsieh",
        "Patrick Rebentrost",
        "Weibo Gao",
        "Ya-Dong Wu",
        "Jens Eisert",
        "Giulio Chiribella",
        "Dacheng Tao",
        "Barry C. Sanders"
      ],
      "abstract": "Efficient characterization of large-scale quantum systems, especially those produced by quantum analog simulators and megaquop quantum computers, poses a central challenge in quantum science due to the exponential scaling of the Hilbert space with respect to system size. Recent advances in artificial intelligence (AI), with its aptitude for high-dimensional pattern recognition and function approximation, have emerged as a powerful tool to address this challenge. A growing body of research has leveraged AI to represent and characterize scalable quantum systems, spanning from theoretical foundations to experimental realizations. Depending on how prior knowledge and learning architectures are incorporated, the integration of AI into quantum system characterization can be categorized into three synergistic paradigms: machine learning, and, in particular, deep learning and language models. This review discusses how each of these AI paradigms contributes to two core tasks in quantum systems characterization: quantum property prediction and the construction of surrogates for quantum states. These tasks underlie diverse applications, from quantum certification and benchmarking to the enhancement of quantum algorithms and the understanding of strongly correlated phases of matter. Key challenges and open questions are also discussed, together with future prospects at the interface of AI and quantum science.",
      "tldr_zh": "å¤§è§„æ¨¡é‡å­ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯é‡å­æ¨¡æ‹Ÿå™¨å’Œå¤§å‹é‡å­è®¡ç®—æœºçš„ç‰¹å¾è¡¨å¾ï¼Œå› å¸Œå°”ä¼¯ç‰¹ç©ºé—´ (Hilbert space) çš„æŒ‡æ•°çº§å¢é•¿è€Œé¢ä¸´æ ¸å¿ƒæŒ‘æˆ˜ã€‚äººå·¥æ™ºèƒ½ (AI) å‡­å€Ÿå…¶åœ¨é«˜ç»´æ¨¡å¼è¯†åˆ«å’Œå‡½æ•°é€¼è¿‘æ–¹é¢çš„èƒ½åŠ›ï¼Œå·²æˆä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜çš„å¼ºå¤§å·¥å…·ã€‚è¯¥ç»¼è¿°æ¢è®¨äº†å°†äººå·¥æ™ºèƒ½ (AI) é›†æˆäºé‡å­ç³»ç»Ÿè¡¨å¾çš„ä¸‰ç§ååŒèŒƒå¼ï¼šæœºå™¨å­¦ä¹  (Machine Learning)ã€æ·±åº¦å­¦ä¹  (Deep Learning) å’Œè¯­è¨€æ¨¡å‹ (Language Models)ã€‚æ–‡ç« é‡ç‚¹è®¨è®ºäº†è¿™äº›èŒƒå¼åœ¨é‡å­å±æ€§é¢„æµ‹ (Quantum property prediction) å’Œæ„å»ºé‡å­æ€ä»£ç† (Surrogates for quantum states) ä¸¤å¤§æ ¸å¿ƒä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚è¿™äº›ä»»åŠ¡æ”¯æŒäº†ä»é‡å­è®¤è¯ (Quantum certification) ä¸åŸºå‡†æµ‹è¯• (Benchmarking) åˆ°å¢å¼ºé‡å­ç®—æ³• (Quantum algorithms) åŠç†è§£å¼ºå…³è”ç‰©è´¨ç›¸ (Strongly correlated phases of matter) ç­‰å¤šæ ·åŒ–åº”ç”¨ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ€»ç»“äº†è¯¥é¢†åŸŸé¢ä¸´çš„å…³é”®æŒ‘æˆ˜ã€å¼€æ”¾æ€§é—®é¢˜ä»¥åŠäººå·¥æ™ºèƒ½ (AI) ä¸é‡å­ç§‘å­¦ (Quantum science) äº¤å‰é¢†åŸŸçš„æœªæ¥å‰æ™¯ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "32 pages. Comments are welcome",
      "pdf_url": "https://arxiv.org/pdf/2509.04923v1",
      "published_date": "2025-09-05 08:41:24 UTC",
      "updated_date": "2025-09-05 08:41:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:48:02.666594+00:00"
    },
    {
      "arxiv_id": "2509.04908v1",
      "title": "SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing",
      "title_zh": "SparkUI-Parserï¼šé€šè¿‡é²æ£’å®šä½ä¸è§£æå¢å¼º GUI æ„ŸçŸ¥èƒ½åŠ›",
      "authors": [
        "Hongyi Jing",
        "Jiafu Chen",
        "Chen Rao",
        "Ziqiang Dang",
        "Jiajie Teng",
        "Tianyi Chu",
        "Juncheng Mo",
        "Shuo Fang",
        "Huaizhong Lin",
        "Rui Lv",
        "Chenguang Ma",
        "Lei Zhao"
      ],
      "abstract": "The existing Multimodal Large Language Models (MLLMs) for GUI perception have made great progress. However, the following challenges still exist in prior methods: 1) They model discrete coordinates based on text autoregressive mechanism, which results in lower grounding accuracy and slower inference speed. 2) They can only locate predefined sets of elements and are not capable of parsing the entire interface, which hampers the broad application and support for downstream tasks. To address the above issues, we propose SparkUI-Parser, a novel end-to-end framework where higher localization precision and fine-grained parsing capability of the entire interface are simultaneously achieved. Specifically, instead of using probability-based discrete modeling, we perform continuous modeling of coordinates based on a pre-trained Multimodal Large Language Model (MLLM) with an additional token router and coordinate decoder. This effectively mitigates the limitations inherent in the discrete output characteristics and the token-by-token generation process of MLLMs, consequently boosting both the accuracy and the inference speed. To further enhance robustness, a rejection mechanism based on a modified Hungarian matching algorithm is introduced, which empowers the model to identify and reject non-existent elements, thereby reducing false positives. Moreover, we present ScreenParse, a rigorously constructed benchmark to systematically assess structural perception capabilities of GUI models across diverse scenarios. Extensive experiments demonstrate that our approach consistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2, CAGUI-Grounding and ScreenParse benchmarks. The resources are available at https://github.com/antgroup/SparkUI-Parser.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SparkUI-Parserï¼Œä¸€ä¸ªæ—¨åœ¨é€šè¿‡é²æ£’çš„Groundingå’ŒParsingå¢å¼ºGUIæ„ŸçŸ¥èƒ½åŠ›çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰Multimodal Large Language Models (MLLMs)åœ¨åæ ‡å»ºæ¨¡ç²¾åº¦ã€æ¨ç†é€Ÿåº¦ä»¥åŠå…¨ç•Œé¢è§£æèƒ½åŠ›æ–¹é¢çš„å±€é™ã€‚SparkUI-Parseré‡‡ç”¨äº†åŸºäºé¢„è®­ç»ƒMLLMçš„è¿ç»­åæ ‡å»ºæ¨¡ï¼Œå¹¶å¼•å…¥äº†é¢å¤–çš„token routerå’Œcoordinate decoderï¼Œæ˜¾è‘—æå‡äº†å®šä½ç²¾åº¦ä¸æ¨ç†æ•ˆç‡ã€‚ä¸ºäº†å¢å¼ºé²æ£’æ€§ï¼Œæ¨¡å‹æ•´åˆäº†åŸºäºæ”¹è¿›åŒˆç‰™åˆ©åŒ¹é…ç®—æ³•(Hungarian matching algorithm)çš„æ‹’ç»æœºåˆ¶ï¼Œä½¿å…¶èƒ½å¤Ÿå‡†ç¡®è¯†åˆ«å¹¶æ‹’ç»ä¸å­˜åœ¨çš„ç•Œé¢å…ƒç´ ï¼Œä»è€Œé™ä½è¯¯æŠ¥ç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æ¨å‡ºäº†ScreenParseåŸºå‡†æµ‹è¯•ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°GUIæ¨¡å‹åœ¨å¤šæ ·åŒ–åœºæ™¯ä¸‹çš„ç»“æ„åŒ–æ„ŸçŸ¥æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSparkUI-Parseråœ¨ScreenSpotã€ScreenSpot-v2å’ŒCAGUI-Groundingç­‰å¤šä¸ªæƒå¨åŸºå‡†æµ‹è¯•ä¸­å‡ä¸€è‡´ä¼˜äºç°æœ‰çš„SOTAæ–¹æ³•ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04908v1",
      "published_date": "2025-09-05 08:24:12 UTC",
      "updated_date": "2025-09-05 08:24:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:48:15.094893+00:00"
    },
    {
      "arxiv_id": "2509.04897v2",
      "title": "PLaMo 2 Technical Report",
      "title_zh": "PLaMo 2 æŠ€æœ¯æŠ¥å‘Š",
      "authors": [
        "Preferred Networks",
        ":",
        "Kaizaburo Chubachi",
        "Yasuhiro Fujita",
        "Shinichi Hemmi",
        "Yuta Hirokawa",
        "Kentaro Imajo",
        "Toshiki Kataoka",
        "Goro Kobayashi",
        "Kenichi Maehashi",
        "Calvin Metzger",
        "Hiroaki Mikami",
        "Shogo Murai",
        "Daisuke Nishino",
        "Kento Nozawa",
        "Toru Ogawa",
        "Shintarou Okada",
        "Daisuke Okanohara",
        "Shunta Saito",
        "Shotaro Sano",
        "Shuji Suzuki",
        "Kuniyuki Takahashi",
        "Daisuke Tanaka",
        "Avinash Ummadisingu",
        "Hanqin Wang",
        "Sixue Wang",
        "Tianqi Xu"
      ],
      "abstract": "In this report, we introduce PLaMo 2, a series of Japanese-focused large language models featuring a hybrid Samba-based architecture that transitions to full attention via continual pre-training to support 32K token contexts. Training leverages extensive synthetic corpora to overcome data scarcity, while computational efficiency is achieved through weight reuse and structured pruning. This efficient pruning methodology produces an 8B model that achieves performance comparable to our previous 100B model. Post-training further refines the models using a pipeline of supervised fine-tuning (SFT) and direct preference optimization (DPO), enhanced by synthetic Japanese instruction data and model merging techniques. Optimized for inference using vLLM and quantization with minimal accuracy loss, the PLaMo 2 models achieve state-of-the-art results on Japanese benchmarks, outperforming similarly-sized open models in instruction-following, language fluency, and Japanese-specific knowledge.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† PLaMo 2ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—ä¸“æ³¨äºæ—¥è¯­çš„å¤§å‹è¯­è¨€æ¨¡å‹ (Large Language Models)ï¼Œé‡‡ç”¨äº†åŸºäº Samba çš„æ··åˆæ¶æ„ï¼Œå¹¶é€šè¿‡æŒç»­é¢„è®­ç»ƒ (Continual Pre-training) è¿‡æ¸¡åˆ°å…¨æ³¨æ„åŠ›æœºåˆ¶ä»¥æ”¯æŒ 32K Token çš„é•¿ä¸Šä¸‹æ–‡ã€‚ä¸ºåº”å¯¹æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜ï¼Œè®­ç»ƒè¿‡ç¨‹å¤§è§„æ¨¡åˆ©ç”¨äº†åˆæˆè¯­æ–™åº“ (Synthetic Corpora)ï¼Œå¹¶ç»“åˆæƒé‡é‡ç”¨ (Weight Reuse) å’Œç»“æ„åŒ–å‰ªæ (Structured Pruning) æŠ€æœ¯ï¼Œä½¿ 8B è§„æ¨¡çš„æ¨¡å‹å®ç°äº†ä¸æ­¤å‰ 100B æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚åœ¨åæœŸè®­ç»ƒé˜¶æ®µï¼Œè¯¥æ¨¡å‹é€šè¿‡ç›‘ç£å¾®è°ƒ (SFT) å’Œç›´æ¥åå¥½ä¼˜åŒ– (DPO) æµæ°´çº¿ï¼Œå¹¶è¾…åŠ©ä»¥æ—¥è¯­æŒ‡ä»¤åˆæˆæ•°æ®åŠæ¨¡å‹åˆå¹¶ (Model Merging) æŠ€æœ¯è¿›è¡Œç²¾ç‚¼ã€‚PLaMo 2 é’ˆå¯¹ vLLM æ¨ç†å’Œé‡åŒ– (Quantization) è¿›è¡Œäº†ä¼˜åŒ–ï¼Œåœ¨æ—¥è¯­åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº† SOTA æ€§èƒ½ï¼Œåœ¨æŒ‡ä»¤éµå¾ªã€è¯­è¨€æµç•…åº¦å’Œæ—¥è¯­ç‰¹å®šçŸ¥è¯†æ–¹é¢æ˜¾è‘—ä¼˜äºåŒå°ºå¯¸çš„å¼€æºæ¨¡å‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04897v2",
      "published_date": "2025-09-05 08:17:59 UTC",
      "updated_date": "2025-09-25 06:33:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:48:16.984863+00:00"
    },
    {
      "arxiv_id": "2509.04889v1",
      "title": "SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models",
      "title_zh": "SpiderNetsï¼šåˆ©ç”¨è§†è§‰æ¨¡å‹è¯„ä¼°èœ˜è››ç›¸å…³å›¾åƒçš„ææƒ§è¯„åˆ†",
      "authors": [
        "Dominik Pegler",
        "David Steyrl",
        "Mengfan Zhang",
        "Alexander Karner",
        "Jozsef Arato",
        "Frank Scharnowski",
        "Filip Melinscak"
      ],
      "abstract": "Advances in computer vision have opened new avenues for clinical applications, particularly in computerized exposure therapy where visual stimuli can be dynamically adjusted based on patient responses. As a critical step toward such adaptive systems, we investigated whether pretrained computer vision models can accurately predict fear levels from spider-related images. We adapted three diverse models using transfer learning to predict human fear ratings (on a 0-100 scale) from a standardized dataset of 313 images. The models were evaluated using cross-validation, achieving an average mean absolute error (MAE) between 10.1 and 11.0. Our learning curve analysis revealed that reducing the dataset size significantly harmed performance, though further increases yielded no substantial gains. Explainability assessments showed the models' predictions were based on spider-related features. A category-wise error analysis further identified visual conditions associated with higher errors (e.g., distant views and artificial/painted spiders). These findings demonstrate the potential of explainable computer vision models in predicting fear ratings, highlighting the importance of both model explainability and a sufficient dataset size for developing effective emotion-aware therapeutic technologies.",
      "tldr_zh": "### è®ºæ–‡æ‘˜è¦æ±‡æ€» ğŸ•·ï¸\n\n---\n\nè¯¥ç ”ç©¶æ¢è®¨äº†é¢„è®­ç»ƒè®¡ç®—æœºè§†è§‰æ¨¡å‹(Computer Vision)åœ¨é¢„æµ‹èœ˜è››ç›¸å…³å›¾åƒææƒ§ç­‰çº§æ–¹é¢çš„æ½œåŠ›ï¼Œæ—¨åœ¨ä¸ºè®¡ç®—æœºè¾…åŠ©æš´éœ²ç–—æ³•(Computerized Exposure Therapy)æä¾›æ”¯æŒã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨è¿ç§»å­¦ä¹ (Transfer Learning)å¯¹ä¸‰ç§ä¸åŒçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤ŸåŸºäºåŒ…å«313å¼ å›¾åƒçš„æ ‡å‡†æ•°æ®é›†é¢„æµ‹äººç±»çš„ææƒ§è¯„åˆ†ï¼ˆ0-100åˆ†ï¼‰ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨äº¤å‰éªŒè¯ä¸­è¾¾åˆ°äº†10.1è‡³11.0çš„å¹³å‡ç»å¯¹è¯¯å·®(MAE)ï¼Œä¸”å­¦ä¹ æ›²çº¿åˆ†æè¡¨æ˜è¶³å¤Ÿçš„æ•°æ®é›†è§„æ¨¡å¯¹ä¿æŒæ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚å¯è§£é‡Šæ€§è¯„ä¼°(Explainability assessments)è¯å®æ¨¡å‹çš„é¢„æµ‹ä¸»è¦ä¾æ®èœ˜è››ç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼ŒåŒæ—¶é”™è¯¯åˆ†æå‘ç°è¿œæ™¯ã€äººé€ æˆ–ç»˜ç”»èœ˜è››å›¾åƒæ˜¯å¯¼è‡´é«˜è¯¯å·®çš„ä¸»è¦åŸå› ã€‚è¿™äº›å‘ç°è¯æ˜äº†å¯è§£é‡Šè®¡ç®—æœºè§†è§‰æ¨¡å‹åœ¨é¢„æµ‹æƒ…æ„Ÿååº”æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¼€å‘æƒ…ç»ªæ„ŸçŸ¥å‹æ²»ç–—æŠ€æœ¯å¥ å®šäº†åŸºç¡€ã€‚\n\n---\n\nå¦‚æœæ‚¨æœ‰æ›´å¤šè®ºæ–‡éœ€è¦è½¬æ¢ï¼Œæˆ–è€…æƒ³é’ˆå¯¹è¯¥ç ”ç©¶çš„æ–¹æ³•è®ºè¿›è¡Œæ·±å…¥æ¢è®¨ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "60 pages (30 main text, 30 appendix), 20 figures (5 in main text, 15 in appendix)",
      "pdf_url": "https://arxiv.org/pdf/2509.04889v1",
      "published_date": "2025-09-05 08:10:40 UTC",
      "updated_date": "2025-09-05 08:10:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:48:33.585703+00:00"
    },
    {
      "arxiv_id": "2509.04876v1",
      "title": "OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in Multi-Agent LLM Collaboration",
      "title_zh": "OSCï¼šåŸºäºåŠ¨æ€çŸ¥è¯†å¯¹é½çš„å¤§è¯­è¨€æ¨¡å‹å¤šæ™ºèƒ½ä½“åä½œè®¤çŸ¥ç¼–æ’",
      "authors": [
        "Jusheng Zhang",
        "Yijia Fan",
        "Kaitong Cai",
        "Xiaofei Sun",
        "Keze Wang"
      ],
      "abstract": "This paper introduces OSC (Orchestrating Cognitive Synergy), a knowledge-aware adaptive collaboration framework designed to enhance cognitive synergy in multi-agent systems with large language models. While prior work has advanced agent selection and result aggregation, efficient linguistic interactions for deep collaboration among expert agents remain a critical bottleneck. OSC addresses this gap as a pivotal intermediate layer between selection and aggregation, introducing Collaborator Knowledge Models (CKM) to enable each agent to dynamically perceive its collaborators' cognitive states. Through real-time cognitive gap analysis, agents adaptively adjust communication behaviors, including content focus, detail level, and expression style, using learned strategies. Experiments on complex reasoning and problem-solving benchmarks demonstrate that OSC significantly improves task performance and communication efficiency, transforming \"parallel-working individuals'' into a \"deeply collaborative cognitive team.'' This framework not only optimizes multi-agent collaboration but also offers new insights into LLM agent interaction behaviors.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OSC (Orchestrating Cognitive Synergy)ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰çŸ¥è¯†æ„ŸçŸ¥çš„è‡ªé€‚åº”åä½œæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤§è¯­è¨€æ¨¡å‹(LLMs)å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„è®¤çŸ¥ååŒã€‚ç°æœ‰ç ”ç©¶è™½åœ¨æ™ºèƒ½ä½“é€‰æ‹©å’Œç»“æœèšåˆæ–¹é¢æœ‰æ‰€è¿›å±•ï¼Œä½†ä¸“å®¶æ™ºèƒ½ä½“é—´æ·±åº¦åä½œçš„é«˜æ•ˆè¯­è¨€äº¤äº’ä»æ˜¯å…³é”®ç“¶é¢ˆã€‚ä½œä¸ºé€‰æ‹©ä¸èšåˆä¹‹é—´çš„æ ¸å¿ƒä¸­é—´å±‚ï¼ŒOSCå¼•å…¥äº†åä½œä¼™ä¼´çŸ¥è¯†æ¨¡å‹(CKM)ï¼Œä½¿æ¯ä¸ªæ™ºèƒ½ä½“èƒ½å¤ŸåŠ¨æ€æ„ŸçŸ¥å…¶åˆä½œä¼™ä¼´çš„è®¤çŸ¥çŠ¶æ€ã€‚é€šè¿‡å®æ—¶è®¤çŸ¥å·®è·åˆ†æï¼Œæ™ºèƒ½ä½“å¯ä»¥åˆ©ç”¨å­¦ä¹ åˆ°çš„ç­–ç•¥è‡ªé€‚åº”è°ƒæ•´é€šä¿¡è¡Œä¸ºï¼ŒåŒ…æ‹¬å†…å®¹ç„¦ç‚¹ã€ç»†èŠ‚ç¨‹åº¦å’Œè¡¨è¾¾é£æ ¼ã€‚åœ¨å¤æ‚æ¨ç†å’Œé—®é¢˜è§£å†³åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜ï¼ŒOSCæ˜¾è‘—æå‡äº†ä»»åŠ¡æ€§èƒ½å’Œé€šä¿¡æ•ˆç‡ã€‚è¯¥æ¡†æ¶æˆåŠŸå°†â€œå¹¶è¡Œå·¥ä½œçš„ä¸ªä½“â€è½¬åŒ–ä¸ºâ€œæ·±åº¦åä½œçš„è®¤çŸ¥å›¢é˜Ÿâ€ï¼Œä¸ºä¼˜åŒ–å¤šæ™ºèƒ½ä½“åä½œåŠç†è§£LLMæ™ºèƒ½ä½“äº¤äº’è¡Œä¸ºæä¾›äº†æ–°è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at EMNLP 2025 (Long Paper)",
      "pdf_url": "https://arxiv.org/pdf/2509.04876v1",
      "published_date": "2025-09-05 07:44:05 UTC",
      "updated_date": "2025-09-05 07:44:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:48:41.593862+00:00"
    },
    {
      "arxiv_id": "2509.12223v1",
      "title": "Ratio1 -- AI meta-OS",
      "title_zh": "Ratio1ï¼šAI å…ƒæ“ä½œç³»ç»Ÿ",
      "authors": [
        "Andrei Damian",
        "Petrica Butusina",
        "Alessandro De Franceschi",
        "Vitalii Toderian",
        "Marius Grigoras",
        "Cristian Bleotiu"
      ],
      "abstract": "We propose the Ratio1 AI meta-operating system (meta-OS), a decentralized MLOps protocol that unifies AI model development, deployment, and inference across heterogeneous edge devices. Its key innovation is an integrated blockchain-based framework that transforms idle computing resources (laptops, smartphones, cloud VMs) into a trustless global supercomputer. The architecture includes novel components: a decentralized authentication layer (dAuth), an in-memory state database (CSTORE), a distributed storage system (R1FS), homomorphic encrypted federated learning (EDIL), decentralized container orchestration (Deeploy) and an oracle network (OracleSync), which collectively ensure secure, resilient execution of AI pipelines and other container based apps at scale. The protocol enforces a formal circular token-economic model combining Proof-of-Availability (PoA) and Proof-of-AI (PoAI) consensus. Compared to centralized heterogeneous cloud MLOps and existing decentralized compute platforms, which often lack integrated AI toolchains or trusted Ratio1 node operators (R1OP) mechanics, Ratio1's holistic design lowers barriers for AI deployment and improves cost-efficiency. We provide mathematical formulations of its secure licensing and reward protocols, and include descriptive information for the system architecture and protocol flow. We argue that our proposed fully functional ecosystem proposes and demonstrates significant improvements in accessibility, scalability, and security over existing alternatives.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Ratio1ï¼Œä¸€ç§å»ä¸­å¿ƒåŒ–çš„AIå…ƒæ“ä½œç³»ç»Ÿ(meta-OS)ï¼Œæ—¨åœ¨é€šè¿‡MLOpsåè®®ç»Ÿä¸€å¼‚æ„è¾¹ç¼˜è®¾å¤‡ä¸Šçš„AIæ¨¡å‹å¼€å‘ã€éƒ¨ç½²å’Œæ¨ç†ã€‚è¯¥ç³»ç»Ÿçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶åŸºäºåŒºå—é“¾çš„é›†æˆæ¡†æ¶ï¼Œåˆ©ç”¨å»ä¸­å¿ƒåŒ–è®¤è¯(dAuth)ã€å†…å­˜çŠ¶æ€æ•°æ®åº“(CSTORE)ã€åˆ†å¸ƒå¼å­˜å‚¨(R1FS)ä»¥åŠåŒæ€åŠ å¯†è”é‚¦å­¦ä¹ (EDIL)ç­‰ç»„ä»¶ï¼Œå°†é—²ç½®è®¡ç®—èµ„æºè½¬åŒ–ä¸ºæ— ä¿¡ä»»çš„å…¨çƒè¶…çº§è®¡ç®—æœºã€‚é€šè¿‡å»ä¸­å¿ƒåŒ–å®¹å™¨ç¼–æ’(Deeploy)å’Œé¢„è¨€æœºç½‘ç»œ(OracleSync)ï¼Œè¯¥åè®®ç»“åˆå¯ç”¨æ€§è¯æ˜(PoA)ä¸AIè¯æ˜(PoAI)å…±è¯†æœºåˆ¶ï¼Œç¡®ä¿äº†AIæµæ°´çº¿çš„å¤§è§„æ¨¡å®‰å…¨æ‰§è¡Œã€‚ä¸ç°æœ‰çš„ä¸­å¿ƒåŒ–äº‘MLOpså’Œå»ä¸­å¿ƒåŒ–è®¡ç®—å¹³å°ç›¸æ¯”ï¼ŒRatio1é€šè¿‡é›†æˆçš„AIå·¥å…·é“¾å’ŒèŠ‚ç‚¹è¿è¥å•†(R1OP)æœºåˆ¶æ˜¾è‘—é™ä½äº†éƒ¨ç½²é—¨æ§›ã€‚æ•°å­¦å…¬å¼å’Œç³»ç»Ÿæ¶æ„åˆ†æè¡¨æ˜ï¼Œè¯¥ç”Ÿæ€ç³»ç»Ÿåœ¨å¯æ‰©å±•æ€§ã€å®‰å…¨æ€§å’Œæˆæœ¬æ•ˆç›Šæ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œä¸ºå…¨çƒåˆ†å¸ƒå¼AIåŸºç¡€è®¾æ–½æä¾›äº†åŠŸèƒ½å®Œå¤‡çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.OS",
        "cs.AI",
        "cs.CR",
        "cs.DC"
      ],
      "primary_category": "cs.OS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.12223v1",
      "published_date": "2025-09-05 07:41:54 UTC",
      "updated_date": "2025-09-05 07:41:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:49:47.566191+00:00"
    },
    {
      "arxiv_id": "2509.04871v1",
      "title": "Cloning a Conversational Voice AI Agent from Call\\,Recording Datasets for Telesales",
      "title_zh": "åŸºäºé€šè¯å½•éŸ³æ•°æ®é›†çš„ç”µè¯é”€å”®å¯¹è¯å¼è¯­éŸ³AIæ™ºèƒ½ä½“å…‹éš†",
      "authors": [
        "Krittanon Kaewtawee",
        "Wachiravit Modecrua",
        "Krittin Pachtrachai",
        "Touchapon Kraisingkorn"
      ],
      "abstract": "Recent advances in language and speech modelling have made it possible to build autonomous voice assistants that understand and generate human dialogue in real time. These systems are increasingly being deployed in domains such as customer service and healthcare care, where they can automate repetitive tasks, reduce operational costs, and provide constant support around the clock. In this paper, we present a general methodology for cloning a conversational voice AI agent from a corpus of call recordings. Although the case study described in this paper uses telesales data to illustrate the approach, the underlying process generalizes to any domain where call transcripts are available. Our system listens to customers over the telephone, responds with a synthetic voice, and follows a structured playbook learned from top performing human agents. We describe the domain selection, knowledge extraction, and prompt engineering used to construct the agent, integrating automatic speech recognition, a large language model based dialogue manager, and text to speech synthesis into a streaming inference pipeline. The cloned agent is evaluated against human agents on a rubric of 22 criteria covering introduction, product communication, sales drive, objection handling, and closing. Blind tests show that the AI agent approaches human performance in routine aspects of the call while underperforming in persuasion and objection handling. We analyze these shortcomings and refine the prompt accordingly. The paper concludes with design lessons and avenues for future research, including large scale simulation and automated evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ä»é€šè¯å½•éŸ³æ•°æ®é›†ä¸­å…‹éš†å¯¹è¯å¼è¯­éŸ³äººå·¥æ™ºèƒ½(Conversational Voice AI Agent)çš„é€šç”¨æ–¹æ³•ï¼Œå¹¶ä»¥è¿œç¨‹é”€å”®(Telesales)åœºæ™¯ä½œä¸ºåº”ç”¨æ¡ˆä¾‹ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ•´åˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)ã€åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„å¯¹è¯ç®¡ç†å™¨å’Œæ–‡æœ¬è½¬è¯­éŸ³(TTS)æŠ€æœ¯ï¼Œæ„å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿå­¦ä¹ ä¼˜ç§€äººç±»ä»£ç†é”€å”®è¯æœ¯çš„æµå¼æ¨ç†æµæ°´çº¿ã€‚ç ”ç©¶äººå‘˜ä¾æ®22é¡¹ä¸“ä¸šæ ‡å‡†å¯¹AIä»£ç†ä¸äººç±»ä»£ç†è¿›è¡Œäº†å¯¹æ¯”è¯„ä¼°ï¼Œæ¶µç›–äº†ä»å¼€åœºåˆ°æˆäº¤çš„å…¨æµç¨‹ã€‚åŒç›²æµ‹è¯•ç»“æœè¡¨æ˜ï¼ŒAIä»£ç†åœ¨å¤„ç†å¸¸è§„é€šè¯å†…å®¹æ–¹é¢å·²æ¥è¿‘äººç±»æ°´å¹³ï¼Œä½†åœ¨è¯´æœæŠ€å·§å’Œå¼‚è®®å¤„ç†(Objection Handling)æ–¹é¢ä»æ˜¾ä¸è¶³ã€‚é€šè¿‡åˆ†æè¿™äº›å±€é™æ€§ï¼Œç ”ç©¶è¿›ä¸€æ­¥ä¼˜åŒ–äº†æç¤ºå·¥ç¨‹(Prompt Engineering)ï¼Œå¹¶æ€»ç»“äº†æœªæ¥åœ¨è‡ªåŠ¨åŒ–è¯„ä¼°å’Œå¤§è§„æ¨¡æ¨¡æ‹Ÿæ–¹é¢çš„è®¾è®¡ç»éªŒä¸ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.04871v1",
      "published_date": "2025-09-05 07:36:12 UTC",
      "updated_date": "2025-09-05 07:36:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:48:45.584791+00:00"
    },
    {
      "arxiv_id": "2509.05388v1",
      "title": "Augmented Structure Preserving Neural Networks for cell biomechanics",
      "title_zh": "ç”¨äºç»†èƒç”Ÿç‰©åŠ›å­¦çš„å¢å¼ºå‹ä¿ç»“æ„ç¥ç»ç½‘ç»œ",
      "authors": [
        "Juan Olalla-Pombo",
        "Alberto BadÃ­as",
        "Miguel Ãngel Sanz-GÃ³mez",
        "JosÃ© MarÃ­a BenÃ­tez",
        "Francisco Javier MontÃ¡ns"
      ],
      "abstract": "Cell biomechanics involve a great number of complex phenomena that are fundamental to the evolution of life itself and other associated processes, ranging from the very early stages of embryo-genesis to the maintenance of damaged structures or the growth of tumors. Given the importance of such phenomena, increasing research has been dedicated to their understanding, but the many interactions between them and their influence on the decisions of cells as a collective network or cluster remain unclear. We present a new approach that combines Structure Preserving Neural Networks, which study cell movements as a purely mechanical system, with other Machine Learning tools (Artificial Neural Networks), which allow taking into consideration environmental factors that can be directly deduced from an experiment with Computer Vision techniques. This new model, tested on simulated and real cell migration cases, predicts complete cell trajectories following a roll-out policy with a high level of accuracy. This work also includes a mitosis event prediction model based on Neural Networks architectures which makes use of the same observed features.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç»†èƒç”Ÿç‰©åŠ›å­¦ä¸­å¤æ‚çš„ç›¸äº’ä½œç”¨åŠå…¶å¯¹é›†ä½“è¡Œä¸ºçš„å½±å“ï¼Œæå‡ºäº†ä¸€ç§å¢å¼ºç»“æ„ä¿ç•™ç¥ç»ç½‘ç»œ (Augmented Structure Preserving Neural Networks) æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ›æ–°æ€§åœ°å°†ç”¨äºæ¨¡æ‹Ÿç»†èƒçº¯åŠ›å­¦è¿åŠ¨çš„ Structure Preserving Neural Networks ä¸å¤„ç†ç¯å¢ƒå½±å“å› ç´ çš„ Artificial Neural Networks ç›¸ç»“åˆï¼Œå¹¶åˆ©ç”¨è®¡ç®—æœºè§†è§‰ (Computer Vision) æŠ€æœ¯ä»å®éªŒæ•°æ®ä¸­æå–å…³é”®ç‰¹å¾ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®çš„ç»†èƒè¿ç§» (cell migration) åœºæ™¯æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨ Roll-out ç­–ç•¥å®ç°äº†å¯¹å®Œæ•´ç»†èƒè½¨è¿¹çš„é«˜ç²¾åº¦é¢„æµ‹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜åˆ©ç”¨ç›¸åŒçš„è§‚æµ‹ç‰¹å¾æ„å»ºäº†èƒ½å¤Ÿé¢„æµ‹ç»†èƒæœ‰ä¸åˆ†è£‚ (mitosis) äº‹ä»¶çš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡æ•´åˆåŠ›å­¦ç³»ç»Ÿå»ºæ¨¡ä¸æœºå™¨å­¦ä¹ å·¥å…·ï¼Œä¸ºæ·±å…¥ç†è§£ç»†èƒé›†ç¾¤çš„åŠ¨æ€æ¼”åŒ–æä¾›äº†æœ‰æ•ˆçš„è®¡ç®—æ‰‹æ®µã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05388v1",
      "published_date": "2025-09-05 07:32:50 UTC",
      "updated_date": "2025-09-05 07:32:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:48:51.391767+00:00"
    },
    {
      "arxiv_id": "2509.04855v1",
      "title": "The Paradox of Doom: Acknowledging Extinction Risk Reduces the Incentive to Prevent It",
      "title_zh": "æœ«æ—¥æ‚–è®ºï¼šå¯¹ç­ç»é£é™©çš„è®¤çŸ¥å‰Šå¼±äº†é¢„é˜²åŠ¨åŠ›",
      "authors": [
        "Jakub Growiec",
        "Klaus Prettner"
      ],
      "abstract": "We investigate the salience of extinction risk as a source of impatience. Our framework distinguishes between human extinction risk and individual mortality risk while allowing for various degrees of intergenerational altruism. Additionally, we consider the evolutionarily motivated \"selfish gene\" perspective. We find that the risk of human extinction is an indispensable component of the discount rate, whereas individual mortality risk can be hedged against - partially or fully, depending on the setup - through human reproduction. Overall, we show that in the face of extinction risk, people become more impatient rather than more farsighted. Thus, the greater the threat of extinction, the less incentive there is to invest in avoiding it. Our framework can help explain why humanity consistently underinvests in mitigation of catastrophic risks, ranging from climate change mitigation, via pandemic prevention, to addressing the emerging risks of transformative artificial intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººç±»ç­ç»é£é™©(extinction risk)ä½œä¸ºä¸è€çƒ¦(impatience)æ¥æºçš„å½±å“ï¼Œå¹¶æå‡ºäº†â€œæœ«æ—¥æ‚–è®ºâ€æ¡†æ¶ã€‚è¯¥æ¨¡å‹åŒºåˆ†äº†äººç±»ç­ç»é£é™©ä¸ä¸ªä½“æ­»äº¡é£é™©(individual mortality risk)ï¼ŒåŒæ—¶è€ƒè™‘äº†ä¸åŒç¨‹åº¦çš„ä»£é™…åˆ©ä»–ä¸»ä¹‰(intergenerational altruism)å’Œâ€œè‡ªç§çš„åŸºå› â€(selfish gene)è§†è§’ã€‚ç ”ç©¶å‘ç°ï¼Œç­ç»é£é™©æ˜¯è´´ç°ç‡(discount rate)ä¸­ä¸å¯æˆ–ç¼ºçš„ç»„æˆéƒ¨åˆ†ï¼Œè€Œä¸ªä½“æ­»äº¡é£é™©å¯ä»¥é€šè¿‡äººç±»ç¹è¡å¾—åˆ°éƒ¨åˆ†æˆ–å…¨éƒ¨å¯¹å†²(hedge)ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé¢å¯¹ç­ç»é£é™©æ—¶ï¼Œäººç±»ä¼šå˜å¾—æ›´åŠ ä¸è€çƒ¦è€Œéæ›´å…·è¿œè§ï¼Œå³ç­ç»å¨èƒè¶Šå¤§ï¼Œé¢„é˜²è¯¥é£é™©çš„åŠ¨åŠ›åè€Œè¶Šå°ã€‚è¿™ä¸€ç»“è®ºåˆç†è§£é‡Šäº†äººç±»åœ¨æ°”å€™å˜åŒ–ã€æµè¡Œç—…é¢„é˜²ä»¥åŠå˜é©æ€§äººå·¥æ™ºèƒ½(transformative artificial intelligence)ç­‰ç¾éš¾æ€§é£é™©ç¼“è§£æ–¹é¢æŒç»­æŠ•èµ„ä¸è¶³çš„ç°çŠ¶ã€‚",
      "categories": [
        "econ.GN",
        "cs.AI"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04855v1",
      "published_date": "2025-09-05 07:14:24 UTC",
      "updated_date": "2025-09-05 07:14:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:49:01.194212+00:00"
    },
    {
      "arxiv_id": "2509.04853v2",
      "title": "A Knowledge-Driven Diffusion Policy for End-to-End Autonomous Driving Based on Expert Routing",
      "title_zh": "åŸºäºä¸“å®¶è·¯ç”±çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶çŸ¥è¯†é©±åŠ¨æ‰©æ•£ç­–ç•¥",
      "authors": [
        "Chengkai Xu",
        "Jiaqi Liu",
        "Yicheng Guo",
        "Peng Hang",
        "Jian Sun"
      ],
      "abstract": "End-to-end autonomous driving remains constrained by the difficulty of producing adaptive, robust, and interpretable decision-making across diverse scenarios. Existing methods often collapse diverse driving behaviors, lack long-horizon consistency, or require task-specific engineering that limits generalization. This paper presents KDP, a knowledge-driven diffusion policy that integrates generative diffusion modeling with a sparse mixture-of-experts routing mechanism. The diffusion component generates temporally coherent action sequences, while the expert routing mechanism activates specialized and reusable experts according to context, enabling modular knowledge composition. Extensive experiments across representative driving scenarios demonstrate that KDP achieves consistently higher success rates, reduced collision risk, and smoother control compared to prevailing paradigms. Ablation studies highlight the effectiveness of sparse expert activation and the Transformer backbone, and activation analyses reveal structured specialization and cross-scenario reuse of experts. These results establish diffusion with expert routing as a scalable and interpretable paradigm for knowledge-driven end-to-end autonomous driving.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†KDPï¼ˆKnowledge-driven Diffusion Policyï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æå‡ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ï¼ˆEnd-to-End Autonomous Drivingï¼‰åœ¨å¤šæ ·åœºæ™¯ä¸‹å†³ç­–é€‚åº”æ€§ã€é²æ£’æ€§å’Œå¯è§£é‡Šæ€§çš„çŸ¥è¯†é©±åŠ¨æ‰©æ•£ç­–ç•¥ã€‚KDPå°†ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹ï¼ˆGenerative Diffusion Modelingï¼‰ä¸ç¨€ç–ä¸“å®¶æ··åˆè·¯ç”±æœºåˆ¶ï¼ˆSparse Mixture-of-Experts Routing Mechanismï¼‰ç›¸ç»“åˆï¼Œåˆ©ç”¨æ‰©æ•£ç»„ä»¶ç”Ÿæˆæ—¶é—´ç›¸å¹²çš„åŠ¨ä½œåºåˆ—ã€‚å…¶ä¸“å®¶è·¯ç”±æœºåˆ¶èƒ½å¤Ÿæ ¹æ®å…·ä½“ä¸Šä¸‹æ–‡æ¿€æ´»ä¸“é—¨ä¸”å¯å¤ç”¨çš„ä¸“å®¶æ¨¡å—ï¼Œä»è€Œå®ç°äº†æ¨¡å—åŒ–çš„çŸ¥è¯†ç»„åˆã€‚å¹¿æ³›çš„å®éªŒç»“æœè¯æ˜ï¼Œä¸ç°æœ‰ä¸»æµèŒƒå¼ç›¸æ¯”ï¼ŒKDPåœ¨ä»£è¡¨æ€§é©¾é©¶åœºæ™¯ä¸­å®ç°äº†æ›´é«˜çš„æˆåŠŸç‡ã€æ˜¾è‘—é™ä½äº†ç¢°æ’é£é™©å¹¶æä¾›äº†æ›´å¹³æ»‘çš„æ§åˆ¶ã€‚æ¶ˆèå®éªŒå’Œæ¿€æ´»åˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†ç¨€ç–ä¸“å®¶æ¿€æ´»ä¸Transformeréª¨å¹²ç½‘ç»œçš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†ä¸“å®¶æ¨¡å—åœ¨ç»“æ„åŒ–ä¸“ä¸šåŒ–åŠè·¨åœºæ™¯å¤ç”¨æ–¹é¢çš„ä¼˜åŠ¿ã€‚è¯¥ç ”ç©¶ç¡®ç«‹äº†æ‰©æ•£æ¨¡å‹ç»“åˆä¸“å®¶è·¯ç”±ä½œä¸ºä¸€ç§å¯æ‰©å±•ä¸”å…·å¯è§£é‡Šæ€§çš„èŒƒå¼ï¼Œä¸ºçŸ¥è¯†é©±åŠ¨çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "https://perfectxu88.github.io/KDP-AD/",
      "pdf_url": "https://arxiv.org/pdf/2509.04853v2",
      "published_date": "2025-09-05 07:07:18 UTC",
      "updated_date": "2025-10-06 01:23:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:49:05.183225+00:00"
    },
    {
      "arxiv_id": "2509.04847v1",
      "title": "Collaboration and Conflict between Humans and Language Models through the Lens of Game Theory",
      "title_zh": "åšå¼ˆè®ºè§†è§’ä¸‹äººç±»ä¸è¯­è¨€æ¨¡å‹çš„åˆä½œä¸å†²çª",
      "authors": [
        "Mukul Singh",
        "Arjun Radhakrishna",
        "Sumit Gulwani"
      ],
      "abstract": "Language models are increasingly deployed in interactive online environments, from personal chat assistants to domain-specific agents, raising questions about their cooperative and competitive behavior in multi-party settings. While prior work has examined language model decision-making in isolated or short-term game-theoretic contexts, these studies often neglect long-horizon interactions, human-model collaboration, and the evolution of behavioral patterns over time. In this paper, we investigate the dynamics of language model behavior in the iterated prisoner's dilemma (IPD), a classical framework for studying cooperation and conflict. We pit model-based agents against a suite of 240 well-established classical strategies in an Axelrod-style tournament and find that language models achieve performance on par with, and in some cases exceeding, the best-known classical strategies. Behavioral analysis reveals that language models exhibit key properties associated with strong cooperative strategies - niceness, provocability, and generosity while also demonstrating rapid adaptability to changes in opponent strategy mid-game. In controlled \"strategy switch\" experiments, language models detect and respond to shifts within only a few rounds, rivaling or surpassing human adaptability. These results provide the first systematic characterization of long-term cooperative behaviors in language model agents, offering a foundation for future research into their role in more complex, mixed human-AI social environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡åšå¼ˆè®ºè§†è§’æ¢è®¨äº†äººç±»ä¸Language Modelsä¹‹é—´çš„åä½œä¸å†²çªï¼Œé‡ç‚¹åˆ†æäº†æ¨¡å‹åœ¨é‡å¤å›šå¾’å›°å¢ƒ(Iterated Prisoner's Dilemma, IPD)ä¸­çš„åŠ¨æ€è¡Œä¸ºã€‚ç ”ç©¶è€…é€šè¿‡Axelrodé£æ ¼çš„ç«èµ›ï¼Œå°†åŸºäºæ¨¡å‹çš„æ™ºèƒ½ä½“ä¸240ç§ç»å…¸çš„åšå¼ˆç­–ç•¥è¿›è¡Œå¯¹æ¯”ï¼Œå‘ç°Language Modelsçš„è¡¨ç°ä¸ä»…èƒ½ä¸é¡¶å°–ç»å…¸ç­–ç•¥æŒå¹³ï¼Œåœ¨æŸäº›åœºæ™¯ä¸‹ç”šè‡³æ›´ä¼˜ã€‚è¡Œä¸ºåˆ†æè¡¨æ˜ï¼ŒLanguage Modelså±•ç°å‡ºäº†å¼ºåˆä½œç­–ç•¥çš„å…³é”®å±æ€§ï¼ŒåŒ…æ‹¬å‹å¥½æ€§(niceness)ã€å¯æ¿€æ€’æ€§(provocability)å’Œå®½å®å¤§é‡(generosity)ï¼Œå¹¶èƒ½é’ˆå¯¹å¯¹æ‰‹çš„ç­–ç•¥å˜åŒ–åšå‡ºå¿«é€Ÿè°ƒæ•´ã€‚åœ¨å—æ§çš„ç­–ç•¥åˆ‡æ¢å®éªŒä¸­ï¼Œæ¨¡å‹ä»…éœ€æ•°è½®å³å¯æ£€æµ‹å¹¶å“åº”å˜åŒ–ï¼Œå…¶é€‚åº”æ€§è¾¾åˆ°ç”šè‡³è¶…è¿‡äº†äººç±»æ°´å¹³ã€‚è¿™é¡¹ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿåœ°åˆ»ç”»äº†Language Modelsæ™ºèƒ½ä½“çš„é•¿æœŸåˆä½œè¡Œä¸ºï¼Œä¸ºæœªæ¥ç ”ç©¶å…¶åœ¨å¤æ‚çš„äººæœºç¤¾äº¤ç¯å¢ƒä¸­çš„ä½œç”¨æä¾›äº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.04847v1",
      "published_date": "2025-09-05 06:55:15 UTC",
      "updated_date": "2025-09-05 06:55:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:49:15.893205+00:00"
    },
    {
      "arxiv_id": "2509.04844v1",
      "title": "REMOTE: A Unified Multimodal Relation Extraction Framework with Multilevel Optimal Transport and Mixture-of-Experts",
      "title_zh": "REMOTEï¼šåŸºäºå¤šå±‚çº§æœ€ä¼˜ä¼ è¾“ä¸æ··åˆä¸“å®¶çš„ç»Ÿä¸€å¤šæ¨¡æ€å…³ç³»æŠ½å–æ¡†æ¶",
      "authors": [
        "Xinkui Lin",
        "Yongxiu Xu",
        "Minghao Tang",
        "Shilong Zhang",
        "Hongbo Xu",
        "Hao Xu",
        "Yubin Wang"
      ],
      "abstract": "Multimodal relation extraction (MRE) is a crucial task in the fields of Knowledge Graph and Multimedia, playing a pivotal role in multimodal knowledge graph construction. However, existing methods are typically limited to extracting a single type of relational triplet, which restricts their ability to extract triplets beyond the specified types. Directly combining these methods fails to capture dynamic cross-modal interactions and introduces significant computational redundancy. Therefore, we propose a novel \\textit{unified multimodal Relation Extraction framework with Multilevel Optimal Transport and mixture-of-Experts}, termed REMOTE, which can simultaneously extract intra-modal and inter-modal relations between textual entities and visual objects. To dynamically select optimal interaction features for different types of relational triplets, we introduce mixture-of-experts mechanism, ensuring the most relevant modality information is utilized. Additionally, considering that the inherent property of multilayer sequential encoding in existing encoders often leads to the loss of low-level information, we adopt a multilevel optimal transport fusion module to preserve low-level features while maintaining multilayer encoding, yielding more expressive representations. Correspondingly, we also create a Unified Multimodal Relation Extraction (UMRE) dataset to evaluate the effectiveness of our framework, encompassing diverse cases where the head and tail entities can originate from either text or image. Extensive experiments show that REMOTE effectively extracts various types of relational triplets and achieves state-of-the-art performanc on almost all metrics across two other public MRE datasets. We release our resources at https://github.com/Nikol-coder/REMOTE.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†REMOTEï¼Œä¸€ç§èåˆäº†å¤šçº§æœ€ä¼˜ä¼ è¾“(Multilevel Optimal Transport)å’Œæ··åˆä¸“å®¶æœºåˆ¶(Mixture-of-Experts)çš„ç»Ÿä¸€å¤šæ¨¡æ€å…³ç³»æŠ½å–(Multimodal Relation Extraction)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨æå–å¤šç§ç±»å‹å…³ç³»ä¸‰å…ƒç»„æ—¶çš„å±€é™æ€§ï¼Œé€šè¿‡Mixture-of-Expertsæœºåˆ¶åŠ¨æ€é€‰æ‹©æœ€ä¼˜äº¤äº’ç‰¹å¾ï¼Œç¡®ä¿äº†æœ€ç›¸å…³æ¨¡æ€ä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨ã€‚ä¸ºäº†è§£å†³å¤šå±‚é¡ºåºç¼–ç ä¸­ä½å±‚ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†å¤šçº§æœ€ä¼˜ä¼ è¾“èåˆæ¨¡å—ä»¥ä¿ç•™å…³é”®ç‰¹å¾ï¼Œä»è€Œç”Ÿæˆæ›´å…·è¡¨è¾¾åŠ›çš„å¤šæ¨¡æ€è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜æ„å»ºäº†å…¨æ–°çš„ç»Ÿä¸€å¤šæ¨¡æ€å…³ç³»æŠ½å–(UMRE)æ•°æ®é›†ï¼Œæ¶µç›–äº†å¤´å°¾å®ä½“è·¨è¶Šæ–‡æœ¬å’Œå›¾åƒçš„å¤šç§å¤æ‚åœºæ™¯ã€‚å®éªŒè¯æ˜ï¼ŒREMOTEåœ¨å‡ ä¹æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›(SOTA)çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿé«˜æ•ˆæå–æ–‡æœ¬å®ä½“ä¸è§†è§‰å¯¹è±¡é—´çš„å„ç§æ¨¡æ€å†…å’Œè·¨æ¨¡æ€å…³ç³»ã€‚",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.MM",
      "comment": "ACM MM 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.04844v1",
      "published_date": "2025-09-05 06:52:03 UTC",
      "updated_date": "2025-09-05 06:52:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:49:14.300577+00:00"
    },
    {
      "arxiv_id": "2509.19306v1",
      "title": "A Federated Fine-Tuning Paradigm of Foundation Models in Heterogenous Wireless Networks",
      "title_zh": "å¼‚æ„æ— çº¿ç½‘ç»œä¸­åŸºç¡€æ¨¡å‹çš„è”é‚¦å¾®è°ƒèŒƒå¼",
      "authors": [
        "Jingyi Wang",
        "Zhongyuan Zhao",
        "Qingtian Wang",
        "Zexu Li",
        "Yue Wang",
        "Tony Q. S. Quek"
      ],
      "abstract": "Edge intelligence has emerged as a promising strategy to deliver low-latency and ubiquitous services for mobile devices. Recent advances in fine-tuning mechanisms of foundation models have enabled edge intelligence by integrating low-rank adaptation (LoRA) with federated learning. However, in wireless networks, the device heterogeneity and resource constraints on edge devices pose great threats to the performance of federated fine-tuning. To tackle these issues, we propose to optimize federated fine-tuning in heterogenous wireless networks via online learning. First, the framework of switching-based federated fine-tuning in wireless networks is provided. The edge devices switches to LoRA modules dynamically for federated fine-tuning with base station to jointly mitigate the impact of device heterogeneity and transmission unreliability. Second, a tractable upper bound on the inference risk gap is derived based on theoretical analysis. To improve the generalization capability, we formulate a non-convex mixed-integer programming problem with long-term constraints, and decouple it into model switching, transmit power control, and bandwidth allocation subproblems. An online optimization algorithm is developed to solve the problems with polynomial computational complexity. Finally, the simulation results on the SST-2 and QNLI data sets demonstrate the performance gains in test accuracy and energy efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼‚æ„æ— çº¿ç½‘ç»œä¸­è¾¹ç¼˜è®¾å¤‡çš„å¤šæ ·æ€§å’Œèµ„æºé™åˆ¶ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåœ¨çº¿å­¦ä¹ çš„è”é‚¦å¾®è°ƒï¼ˆFederated Fine-Tuningï¼‰ä¼˜åŒ–èŒƒå¼ã€‚æ–‡ç« å¼•å…¥äº†ä¸€ç§åŸºäºåˆ‡æ¢ï¼ˆswitching-basedï¼‰çš„æ¡†æ¶ï¼Œé€šè¿‡è®©è¾¹ç¼˜è®¾å¤‡åŠ¨æ€åˆ‡æ¢ LoRA æ¨¡å—æ¥ååŒç¼“è§£è®¾å¤‡å¼‚æ„å’Œä¼ è¾“ä¸å¯é å¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚ç ”ç©¶å›¢é˜ŸåŸºäºç†è®ºåˆ†ææ¨å¯¼å‡ºäº†æ¨ç†é£é™©é—´éš™ï¼ˆinference risk gapï¼‰çš„å¯å¤„ç†ä¸Šç•Œï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªæ¶‰åŠæ¨¡å‹åˆ‡æ¢ã€å‘å°„åŠŸç‡æ§åˆ¶å’Œå¸¦å®½åˆ†é…çš„éå‡¸æ··åˆæ•´æ•°è§„åˆ’é—®é¢˜ã€‚é€šè¿‡å¼€å‘çš„åœ¨çº¿ä¼˜åŒ–ç®—æ³•ï¼Œè¯¥æ–¹æ¡ˆèƒ½å¤Ÿä»¥å¤šé¡¹å¼è®¡ç®—å¤æ‚åº¦è§£å†³å¤æ‚çš„èµ„æºä¼˜åŒ–é—®é¢˜ï¼Œä»è€Œæœ‰æ•ˆæå‡åŸºç¡€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœåœ¨ SST-2 å’Œ QNLI æ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥èŒƒå¼åœ¨æé«˜æµ‹è¯•å‡†ç¡®ç‡å’Œèƒ½æºæ•ˆç‡æ–¹é¢çš„æ˜¾è‘—å¢ç›Šã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.IT",
        "cs.NI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19306v1",
      "published_date": "2025-09-05 06:38:36 UTC",
      "updated_date": "2025-09-05 06:38:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:49:26.276648+00:00"
    },
    {
      "arxiv_id": "2509.04833v1",
      "title": "PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination",
      "title_zh": "PropVGï¼šåŸºäºå¤šç²’åº¦åˆ¤åˆ«çš„ç«¯åˆ°ç«¯æè®®é©±åŠ¨è§†è§‰å®šä½",
      "authors": [
        "Ming Dai",
        "Wenxuan Cheng",
        "Jiedong Zhuang",
        "Jiang-jiang Liu",
        "Hongshen Zhao",
        "Zhenhua Feng",
        "Wankou Yang"
      ],
      "abstract": "Recent advances in visual grounding have largely shifted away from traditional proposal-based two-stage frameworks due to their inefficiency and high computational complexity, favoring end-to-end direct reference paradigms. However, these methods rely exclusively on the referred target for supervision, overlooking the potential benefits of prominent prospective targets. Moreover, existing approaches often fail to incorporate multi-granularity discrimination, which is crucial for robust object identification in complex scenarios. To address these limitations, we propose PropVG, an end-to-end proposal-based framework that, to the best of our knowledge, is the first to seamlessly integrate foreground object proposal generation with referential object comprehension without requiring additional detectors. Furthermore, we introduce a Contrastive-based Refer Scoring (CRS) module, which employs contrastive learning at both sentence and word levels to enhance the capability in understanding and distinguishing referred objects. Additionally, we design a Multi-granularity Target Discrimination (MTD) module that fuses object- and semantic-level information to improve the recognition of absent targets. Extensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO (REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes and models are available at https://github.com/Dmmm1997/PropVG.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PropVGï¼Œä¸€ç§ç«¯åˆ°ç«¯çš„åŸºäº Proposal çš„ Visual Grounding æ¡†æ¶ï¼Œé¦–æ¬¡å®ç°äº†åœ¨å‰æ™¯è‰²ç‰©ä½“ Proposal ç”Ÿæˆä¸å‚è€ƒç‰©ä½“ç†è§£ä¹‹é—´çš„æ— ç¼é›†æˆï¼Œä¸”æ— éœ€é¢å¤–çš„æ£€æµ‹å™¨ã€‚é’ˆå¯¹ç°æœ‰ç›´æ¥å¼•ç”¨èŒƒå¼å¿½è§†æ½œåœ¨å€™é€‰ç›®æ ‡åŠç¼ºä¹å¤šç²’åº¦è¾¨åˆ«èƒ½åŠ›çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†åŸºäºå¯¹æ¯”å­¦ä¹ çš„ Refer Scoring (CRS) æ¨¡å—ï¼Œåœ¨å¥å­å’Œè¯è¯­çº§åˆ«åº”ç”¨å¯¹æ¯”å­¦ä¹ ä»¥å¢å¼ºå¯¹å¼•ç”¨ç‰©ä½“çš„åŒºåˆ†èƒ½åŠ›ã€‚åŒæ—¶ï¼Œç ”ç©¶è€…è®¾è®¡äº† Multi-granularity Target Discrimination (MTD) æ¨¡å—ï¼Œé€šè¿‡èåˆç‰©ä½“çº§å’Œè¯­ä¹‰çº§ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹ä¸å­˜åœ¨ç›®æ ‡ï¼ˆabsent targetsï¼‰çš„è¯†åˆ«æ•ˆæœã€‚åœ¨ gRefCOCOã€Ref-ZOMã€R-RefCOCO å’Œ RefCOCO ç­‰å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœå……åˆ†è¯æ˜äº† PropVG çš„æœ‰æ•ˆæ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤æ‚åœºæ™¯ä¸‹çš„é²æ£’ç›®æ ‡è¯†åˆ«æä¾›äº†æ–°çš„è§£å†³æ€è·¯ï¼Œå¹¶åœ¨å¤šé¡¹ä¸»æµä»»åŠ¡ä¸­å±•ç°äº†å‡ºè‰²çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV2025",
      "pdf_url": "https://arxiv.org/pdf/2509.04833v1",
      "published_date": "2025-09-05 06:30:06 UTC",
      "updated_date": "2025-09-05 06:30:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:49:59.065180+00:00"
    },
    {
      "arxiv_id": "2509.09703v1",
      "title": "CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor",
      "title_zh": "CTCCï¼šä¸€ç§åŸºäºè·¨è½®ä¸Šä¸‹æ–‡å…³è”åé—¨çš„é²æ£’ä¸”éšè”½çš„å¤§è¯­è¨€æ¨¡å‹æŒ‡çº¹æ¡†æ¶",
      "authors": [
        "Zhenhua Xu",
        "Xixiang Zhao",
        "Xubin Yue",
        "Shengwei Tian",
        "Changting Lin",
        "Meng Han"
      ],
      "abstract": "The widespread deployment of large language models (LLMs) has intensified concerns around intellectual property (IP) protection, as model theft and unauthorized redistribution become increasingly feasible. To address this, model fingerprinting aims to embed verifiable ownership traces into LLMs. However, existing methods face inherent trade-offs between stealthness, robustness, and generalizability, being either detectable via distributional shifts, vulnerable to adversarial modifications, or easily invalidated once the fingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven fingerprinting framework that encodes contextual correlations across multiple dialogue turns, such as counterfactual, rather than relying on token-level or single-turn triggers. CTCC enables fingerprint verification under black-box access while mitigating false positives and fingerprint leakage, supporting continuous construction under a shared semantic rule even if partial triggers are exposed. Extensive experiments across multiple LLM architectures demonstrate that CTCC consistently achieves stronger stealth and robustness than prior work. Our findings position CTCC as a reliable and practical solution for ownership verification in real-world LLM deployment scenarios. Our code and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CTCCï¼Œä¸€ç§é’ˆå¯¹Large Language Models (LLMs)çš„ç¨³å¥ä¸”éšè”½çš„fingerprintingæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ¨¡å‹ç›—ç”¨å’Œéæ³•åˆ†å‘ä¸­çš„Intellectual Property (IP)ä¿æŠ¤æŒ‘æˆ˜ã€‚ä¸ä¾èµ–è¯å…ƒçº§åˆ«æˆ–å•è½®è§¦å‘å™¨çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒCTCCé€šè¿‡è§„åˆ™é©±åŠ¨çš„æ–¹å¼ç¼–ç è·¨å¤šè½®å¯¹è¯çš„ä¸Šä¸‹æ–‡ç›¸å…³æ€§(Cross-Turn Contextual Correlation)ï¼Œä¾‹å¦‚åˆ©ç”¨åäº‹å®é€»è¾‘æ¥åµŒå…¥æ‰€æœ‰æƒè¸ªè¿¹ã€‚è¯¥æ¡†æ¶æ”¯æŒåœ¨black-box accessæ¡ä»¶ä¸‹è¿›è¡ŒéªŒè¯ï¼Œèƒ½æœ‰æ•ˆå‡å°‘è¯¯æŠ¥å¹¶é˜²æ­¢æŒ‡çº¹æ³„éœ²ï¼Œå³ä½¿éƒ¨åˆ†è§¦å‘å™¨è¢«å…¬å¼€ï¼Œä»å¯åŸºäºå…±äº«è¯­ä¹‰è§„åˆ™è¿›è¡ŒæŒç»­æ„å»ºã€‚åœ¨å¤šç§LLMæ¶æ„ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒCTCCåœ¨stealthå’Œrobustnessæ–¹é¢å‡æ˜¾è‘—ä¼˜äºå…ˆå‰å·¥ä½œã€‚è¯¥ç ”ç©¶ä¸ºç°å®ä¸–ç•ŒLLMéƒ¨ç½²åœºæ™¯ä¸­çš„æ‰€æœ‰æƒéªŒè¯æä¾›äº†ä¸€ç§å¯é ä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by EMNLP2025 MainConference",
      "pdf_url": "https://arxiv.org/pdf/2509.09703v1",
      "published_date": "2025-09-05 05:59:50 UTC",
      "updated_date": "2025-09-05 05:59:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:50:10.870229+00:00"
    },
    {
      "arxiv_id": "2509.04827v2",
      "title": "VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving",
      "title_zh": "VoltanaLLMï¼šé¢å‘é«˜èƒ½æ•ˆå¤§è¯­è¨€æ¨¡å‹æœåŠ¡çš„åé¦ˆé©±åŠ¨é¢‘ç‡æ§åˆ¶ä¸çŠ¶æ€ç©ºé—´è·¯ç”±",
      "authors": [
        "Jiahuan Yu",
        "Aryan Taneja",
        "Junfeng Lin",
        "Minjia Zhang"
      ],
      "abstract": "Modern Large Language Model (LLM) serving systems increasingly support interactive applications, like real-time chat assistants, code generation tools, and agentic workflows. However, the soaring energy cost of LLM inference presents a growing challenge for sustainable and cost-effective deployment. This paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM serving, built from a control theory perspective. VoltanaLLM co-designs frequency scaling and request routing in emerging prefill/decode disaggregated architectures, leveraging their decoupled execution to enable fine-grained phase-specific control. It consists of a feedback-driven frequency controller that dynamically adapts GPU frequency for prefill and decode phases, and a state-space router that explores routing decisions across frequency-scaled instances to minimize energy under latency constraints. We implement VoltanaLLM in SGLang and evaluate its performance over multiple state-of-the-art LLMs and real-world datasets. The results demonstrate that VoltanaLLM achieves up to 36.3% energy savings while maintaining near-perfect SLO attainment rate, paving the way for sustainable and intelligent LLM serving. Code of VoltanaLLM is open-sourced on GitHub: https://github.com/Supercomputing-System-AI-Lab/VoltanaLLM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VoltanaLLMï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ§åˆ¶ç†è®º(control theory)è§†è§’è®¾è®¡çš„SLOæ„ŸçŸ¥å‹é«˜èƒ½æ•ˆLarge Language Model (LLM)æœåŠ¡ç³»ç»Ÿã€‚é’ˆå¯¹é¢„å¡«å……ä¸è§£ç è§£è€¦æ¶æ„(prefill/decode disaggregated architectures)ï¼Œè¯¥ç³»ç»ŸååŒä¼˜åŒ–äº†é¢‘ç‡ç¼©æ”¾(frequency scaling)å’Œè¯·æ±‚è·¯ç”±(request routing)ï¼Œä»è€Œå®ç°ç»†ç²’åº¦çš„é˜¶æ®µç‰¹å®šæ§åˆ¶ã€‚å…¶æ ¸å¿ƒåŒ…å«ä¸€ä¸ªåé¦ˆé©±åŠ¨çš„é¢‘ç‡æ§åˆ¶å™¨ï¼Œèƒ½å¤ŸåŠ¨æ€è°ƒæ•´GPUé¢‘ç‡ä»¥é€‚åº”ä¸åŒé˜¶æ®µéœ€æ±‚ï¼Œä»¥åŠä¸€ä¸ªåœ¨æ»¡è¶³å»¶è¿Ÿçº¦æŸä¸‹ä¼˜åŒ–èŠ‚èƒ½å†³ç­–çš„çŠ¶æ€ç©ºé—´è·¯ç”±å™¨(state-space router)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨SGLangæ¡†æ¶ä¸‹å®ç°çš„VoltanaLLMåœ¨å¤šç±»å‰æ²¿LLMåŠçœŸå®æ•°æ®é›†ä¸Šæœ€é«˜å¯èŠ‚çœ36.3%çš„èƒ½æºæ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒäº†æé«˜çš„SLOè¾¾æˆç‡ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå¯æŒç»­çš„æ™ºèƒ½LLMéƒ¨ç½²æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ–¹æ¡ˆï¼Œå…¶ç›¸å…³ä»£ç ä¹Ÿå·²åœ¨GitHubå¼€æºã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04827v2",
      "published_date": "2025-09-05 05:58:16 UTC",
      "updated_date": "2025-09-14 07:30:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:50:11.267926+00:00"
    },
    {
      "arxiv_id": "2509.04824v1",
      "title": "Exploring Non-Local Spatial-Angular Correlations with a Hybrid Mamba-Transformer Framework for Light Field Super-Resolution",
      "title_zh": "åŸºäº Mamba-Transformer æ··åˆæ¡†æ¶çš„å…‰åœºè¶…åˆ†è¾¨ç‡éå±€éƒ¨ç©ºé—´-è§’åº¦ç›¸å…³æ€§æ¢ç´¢",
      "authors": [
        "Haosong Liu",
        "Xiancheng Zhu",
        "Huanqiang Zeng",
        "Jianqing Zhu",
        "Jiuwen Cao",
        "Junhui Hou"
      ],
      "abstract": "Recently, Mamba-based methods, with its advantage in long-range information modeling and linear complexity, have shown great potential in optimizing both computational cost and performance of light field image super-resolution (LFSR). However, current multi-directional scanning strategies lead to inefficient and redundant feature extraction when applied to complex LF data. To overcome this challenge, we propose a Subspace Simple Scanning (Sub-SS) strategy, based on which we design the Subspace Simple Mamba Block (SSMB) to achieve more efficient and precise feature extraction. Furthermore, we propose a dual-stage modeling strategy to address the limitation of state space in preserving spatial-angular and disparity information, thereby enabling a more comprehensive exploration of non-local spatial-angular correlations. Specifically, in stage I, we introduce the Spatial-Angular Residual Subspace Mamba Block (SA-RSMB) for shallow spatial-angular feature extraction; in stage II, we use a dual-branch parallel structure combining the Epipolar Plane Mamba Block (EPMB) and Epipolar Plane Transformer Block (EPTB) for deep epipolar feature refinement. Building upon meticulously designed modules and strategies, we introduce a hybrid Mamba-Transformer framework, termed LFMT. LFMT integrates the strengths of Mamba and Transformer models for LFSR, enabling comprehensive information exploration across spatial, angular, and epipolar-plane domains. Experimental results demonstrate that LFMT significantly outperforms current state-of-the-art methods in LFSR, achieving substantial improvements in performance while maintaining low computational complexity on both real-word and synthetic LF datasets.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º LFMT çš„æ··åˆ Mamba-Transformer æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å…‰åœºå›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆLight Field Super-Resolution, LFSRï¼‰ä¸­ç°æœ‰ Mamba æ–¹æ³•å› å¤šå‘æ‰«æç­–ç•¥å¯¼è‡´çš„å¤„ç†å¤æ‚æ•°æ®æ•ˆç‡ä½ä¸‹å’Œç‰¹å¾å†—ä½™é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å­ç©ºé—´ç®€å•æ‰«æï¼ˆSubspace Simple Scanning, Sub-SSï¼‰ç­–ç•¥å¹¶è®¾è®¡äº†ç›¸åº”çš„ SSMB æ¨¡å—ï¼Œä»¥å®ç°æ›´é«˜æ•ˆç²¾ç¡®çš„ç‰¹å¾æå–ã€‚ç ”ç©¶é‡‡ç”¨åŒé˜¶æ®µå»ºæ¨¡ç­–ç•¥æ¥å¢å¼ºå¯¹éå±€éƒ¨ç©ºé—´-è§’åº¦ç›¸å…³æ€§çš„æ¢ç´¢ï¼Œç¬¬ä¸€é˜¶æ®µåˆ©ç”¨ SA-RSMB æå–æµ…å±‚ç‰¹å¾ï¼Œç¬¬äºŒé˜¶æ®µé€šè¿‡ç»“åˆ EPMB ä¸ EPTB çš„åŒåˆ†æ”¯å¹¶è¡Œç»“æ„è¿›è¡Œæ·±å±‚æå¹³é¢ï¼ˆEpipolar Planeï¼‰ç‰¹å¾ç²¾ç‚¼ã€‚LFMT æˆåŠŸæ•´åˆäº† Mamba åœ¨é•¿ç¨‹å»ºæ¨¡ä¸Šçš„ä¼˜åŠ¿ä¸ Transformer çš„ç»†ç²’åº¦å¤„ç†èƒ½åŠ›ï¼Œå®ç°äº†å¯¹ç©ºé—´ã€è§’åº¦å’Œæå¹³é¢åŸŸä¿¡æ¯çš„å…¨é¢æŒ–æ˜ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨åˆæˆåŠçœŸå®å…‰åœºæ•°æ®é›†ä¸Šå‡æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨å¤§å¹…æå‡æ€§èƒ½çš„åŒæ—¶ä¿æŒäº†è¾ƒä½çš„è®¡ç®—å¤æ‚åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04824v1",
      "published_date": "2025-09-05 05:50:38 UTC",
      "updated_date": "2025-09-05 05:50:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:50:12.073286+00:00"
    },
    {
      "arxiv_id": "2509.04809v2",
      "title": "TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models",
      "title_zh": "TalkToAgentï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä»¥äººä¸ºä¸­å¿ƒçš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“è§£é‡Š",
      "authors": [
        "Haechang Kim",
        "Hao Chen",
        "Can Li",
        "Jong Min Lee"
      ],
      "abstract": "Explainable Reinforcement Learning (XRL) has emerged as a promising approach in improving the transparency of Reinforcement Learning (RL) agents. However, there remains a gap between complex RL policies and domain experts, due to the limited comprehensibility of XRL results and isolated coverage of current XRL approaches that leave users uncertain about which tools to employ. To address these challenges, we introduce TalkToAgent, a multi-agent Large Language Models (LLM) framework that delivers interactive, natural language explanations for RL policies. The architecture with five specialized LLM agents (Coordinator, Explainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically map user queries to relevant XRL tools and clarify an agent's actions in terms of either key state variables, expected outcomes, or counterfactual explanations. Moreover, our approach extends previous counterfactual explanations by deriving alternative scenarios from qualitative behavioral descriptions, or even new rule-based policies. We validated TalkToAgent on quadruple-tank process control problem, a well-known nonlinear control benchmark. Results demonstrated that TalkToAgent successfully mapped user queries into XRL tasks with high accuracy, and coder-debugger interactions minimized failures in counterfactual generation. Furthermore, qualitative evaluation confirmed that TalkToAgent effectively interpreted agent's actions and contextualized their meaning within the problem domain.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤æ‚å¼ºåŒ–å­¦ä¹ (RL)ç­–ç•¥ä¸é¢†åŸŸä¸“å®¶ä¹‹é—´å­˜åœ¨ç†è§£é¸¿æ²Ÿï¼Œä»¥åŠç°æœ‰å¯è§£é‡Šå¼ºåŒ–å­¦ä¹ (XRL)å·¥å…·è¦†ç›–å­¤ç«‹ç­‰é—®é¢˜ï¼Œæå‡ºäº† TalkToAgent å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹(LLM)æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±äº”ä¸ªä¸“é—¨çš„æ™ºèƒ½ä½“ï¼ˆCoordinatorã€Explainerã€Coderã€Evaluator å’Œ Debuggerï¼‰ç»„æˆï¼Œèƒ½è‡ªåŠ¨å°†ç”¨æˆ·æŸ¥è¯¢æ˜ å°„åˆ°ç›¸å…³çš„ XRL å·¥å…·ï¼Œå¹¶æä¾›äº¤äº’å¼çš„è‡ªç„¶è¯­è¨€è§£é‡Šã€‚TalkToAgent é€šè¿‡åˆ†æå…³é”®çŠ¶æ€å˜é‡ã€é¢„æœŸç»“æœæˆ–åäº‹å®è§£é‡Š (counterfactual explanations) æ¥æ¾„æ¸…æ™ºèƒ½ä½“è¡Œä¸ºï¼Œå¹¶æ”¯æŒä»å®šæ€§è¡Œä¸ºæè¿°ä¸­æ¨å¯¼å‡ºæ›¿ä»£åœºæ™¯æˆ–æ–°çš„è§„åˆ™ç­–ç•¥ã€‚å®éªŒåœ¨å››ç½è¿‡ç¨‹æ§åˆ¶ (quadruple-tank process control) åŸºå‡†é—®é¢˜ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ¡†æ¶èƒ½ä»¥æé«˜å‡†ç¡®ç‡å¤„ç† XRL ä»»åŠ¡ï¼Œä¸”æ™ºèƒ½ä½“é—´çš„åä½œæ˜¾è‘—é™ä½äº†åäº‹å®ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¤±è´¥ç‡ã€‚å®šæ€§è¯„ä¼°è¿›ä¸€æ­¥è¯æ˜ï¼ŒTalkToAgent èƒ½å¤Ÿæœ‰æ•ˆè§£é‡Šæ™ºèƒ½ä½“åŠ¨ä½œå¹¶å°†å…¶åœ¨é¢†åŸŸèƒŒæ™¯ä¸‹å…·è±¡åŒ–ï¼Œæ˜¾è‘—æå‡äº†å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿçš„é€æ˜åº¦å’Œå¯ç”¨æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "31 pages total",
      "pdf_url": "https://arxiv.org/pdf/2509.04809v2",
      "published_date": "2025-09-05 05:09:09 UTC",
      "updated_date": "2025-09-08 00:52:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:50:33.796050+00:00"
    },
    {
      "arxiv_id": "2509.04805v1",
      "title": "AI-Driven Fronthaul Link Compression in Wireless Communication Systems: Review and Method Design",
      "title_zh": "æ— çº¿é€šä¿¡ç³»ç»Ÿä¸­çš„ AI é©±åŠ¨å‰ä¼ é“¾è·¯å‹ç¼©ï¼šç»¼è¿°ä¸æ–¹æ³•è®¾è®¡",
      "authors": [
        "Keqin Zhang"
      ],
      "abstract": "Modern fronthaul links in wireless systems must transport high-dimensional signals under stringent bandwidth and latency constraints, which makes compression indispensable. Traditional strategies such as compressed sensing, scalar quantization, and fixed-codec pipelines often rely on restrictive priors, degrade sharply at high compression ratios, and are hard to tune across channels and deployments. Recent progress in Artificial Intelligence (AI) has brought end-to-end learned transforms, vector and hierarchical quantization, and learned entropy models that better exploit the structure of Channel State Information(CSI), precoding matrices, I/Q samples, and LLRs. This paper first surveys AI-driven compression techniques and then provides a focused analysis of two representative high-compression routes: CSI feedback with end-to-end learning and Resource Block (RB) granularity precoding optimization combined with compression. Building on these insights, we propose a fronthaul compression strategy tailored to cell-free architectures. The design targets high compression with controlled performance loss, supports RB-level rate adaptation, and enables low-latency inference suitable for centralized cooperative transmission in next-generation networks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ— çº¿é€šä¿¡ç³»ç»Ÿä¸­å‰ä¼ é“¾è·¯(fronthaul links)åœ¨ä¸¥è‹›å¸¦å®½å’Œå»¶è¿Ÿçº¦æŸä¸‹çš„å‹ç¼©æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºå‹ç¼©æ„ŸçŸ¥(compressed sensing)å’Œæ ‡é‡é‡åŒ–(scalar quantization)ç­‰ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†é«˜ç»´åº¦ä¿¡å·æ—¶å­˜åœ¨å±€é™æ€§ã€‚è®ºæ–‡ç³»ç»Ÿåœ°ç»¼è¿°äº†åˆ©ç”¨ç«¯åˆ°ç«¯å­¦ä¹ å˜æ¢(end-to-end learned transforms)ã€çŸ¢é‡é‡åŒ–(vector quantization)åŠå­¦ä¹ ç†µæ¨¡å‹(learned entropy models)ç­‰AIæŠ€æœ¯ä¼˜åŒ–ä¿¡é“çŠ¶æ€ä¿¡æ¯(CSI)å’ŒI/Qæ ·æœ¬å‹ç¼©çš„æœ€æ–°è¿›å±•ã€‚é€šè¿‡åˆ†æCSIåé¦ˆå’Œèµ„æºå—(Resource Block)ç²’åº¦é¢„ç¼–ç ä¼˜åŒ–çš„å‹ç¼©è·¯å¾„ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ— å°åŒº(cell-free)æ¶æ„å®šåˆ¶çš„å‰ä¼ å‹ç¼©ç­–ç•¥ã€‚è¯¥æ–¹æ¡ˆåœ¨å®ç°é«˜å‹ç¼©ç‡çš„åŒæ—¶èƒ½å¤Ÿæœ‰æ•ˆæ§åˆ¶æ€§èƒ½æŸå¤±ï¼Œå¹¶æ”¯æŒRBçº§åˆ«çš„é€Ÿç‡è‡ªé€‚åº”ã€‚æ­¤å¤–ï¼Œè¯¥è®¾è®¡å®ç°äº†é€‚ç”¨äºä¸‹ä¸€ä»£ç½‘ç»œé›†ä¸­åä½œä¼ è¾“çš„ä½å»¶è¿Ÿæ¨ç†(low-latency inference)ï¼Œä¸ºæå‡æ— çº¿é€šä¿¡ç³»ç»Ÿçš„æ•´ä½“ä¼ è¾“æ•ˆç‡æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04805v1",
      "published_date": "2025-09-05 04:52:51 UTC",
      "updated_date": "2025-09-05 04:52:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:50:16.671262+00:00"
    },
    {
      "arxiv_id": "2509.09702v2",
      "title": "Creativity Benchmark: A benchmark for marketing creativity for large language models",
      "title_zh": "Creativity Benchmarkï¼šå¤§è¯­è¨€æ¨¡å‹è¥é”€åˆ›æ„è¯„ä¼°åŸºå‡†",
      "authors": [
        "Ninad Bhat",
        "Kieran Browne",
        "Pip Bingemann"
      ],
      "abstract": "We introduce Creativity Benchmark, an evaluation framework for large language models (LLMs) in marketing creativity. The benchmark covers 100 brands (12 categories) and three prompt types (Insights, Ideas, Wild Ideas). Human pairwise preferences from 678 practising creatives over 11,012 anonymised comparisons, analysed with Bradley-Terry models, show tightly clustered performance with no model dominating across brands or prompt types: the top-bottom spread is $Î”Î¸\\approx 0.45$, which implies a head-to-head win probability of $0.61$; the highest-rated model beats the lowest only about $61\\%$ of the time. We also analyse model diversity using cosine distances to capture intra- and inter-model variation and sensitivity to prompt reframing. Comparing three LLM-as-judge setups with human rankings reveals weak, inconsistent correlations and judge-specific biases, underscoring that automated judges cannot substitute for human evaluation. Conventional creativity tests also transfer only partially to brand-constrained tasks. Overall, the results highlight the need for expert human evaluation and diversity-aware workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Creativity Benchmarkï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¥é”€åˆ›æ„èƒ½åŠ›çš„æ¡†æ¶ï¼Œæ¶µç›–äº†12ä¸ªç±»åˆ«çš„100ä¸ªå“ç‰Œå’Œä¸‰ç§ä¸åŒçš„æç¤ºè¯ç±»å‹ï¼ˆInsights, Ideas, Wild Ideasï¼‰ã€‚é€šè¿‡678åä¸“ä¸šåˆ›æ„äººå‘˜è¿›è¡Œçš„11,012æ¬¡åŒ¿åé…å¯¹æ¯”è¾ƒï¼Œå¹¶åˆ©ç”¨Bradley-Terryæ¨¡å‹è¿›è¡Œåˆ†æï¼Œç ”ç©¶å‘ç°å„æ¨¡å‹çš„è¡¨ç°éå¸¸æ¥è¿‘ï¼Œæ²¡æœ‰ä»»ä½•æ¨¡å‹èƒ½å¤Ÿåœ¨æ‰€æœ‰å“ç‰Œæˆ–æç¤ºç±»å‹ä¸­å æ®ç»å¯¹ä¸»å¯¼åœ°ä½ã€‚åˆ†æè¿˜æ¶‰åŠäº†åˆ©ç”¨ä½™å¼¦è·ç¦»è¯„ä¼°æ¨¡å‹çš„å¤šæ ·æ€§ä»¥åŠæ¨¡å‹å¯¹æç¤ºè¯é‡æ„çš„æ•æ„Ÿæ€§ï¼Œæ­ç¤ºäº†LLM-as-judgeä¸äººç±»è¯„ä¼°ä¹‹é—´å­˜åœ¨å¼±ç›¸å…³æ€§å’Œç‰¹å®šçš„è¯„åˆ¤åè§ã€‚ç»“æœè¡¨æ˜ï¼Œä¼ ç»Ÿçš„åˆ›é€ åŠ›æµ‹è¯•åœ¨å¤„ç†å“ç‰Œçº¦æŸä»»åŠ¡æ—¶å…·æœ‰å±€é™æ€§ï¼Œå› æ­¤æ— æ³•é€šè¿‡è‡ªåŠ¨åŒ–è¯„æµ‹å®Œå…¨å–ä»£äººç±»ä¸“å®¶ã€‚è¯¥åŸºå‡†æµ‹è¯•å¼ºè°ƒäº†åœ¨åˆ›æ„ç”Ÿæˆæµç¨‹ä¸­ç»“åˆä¸“å®¶è¯„ä¼°å’Œå¤šæ ·æ€§æ„ŸçŸ¥å·¥ä½œæµçš„å…³é”®æ„ä¹‰ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "30 Pages, 14 figures. Fixed typos",
      "pdf_url": "https://arxiv.org/pdf/2509.09702v2",
      "published_date": "2025-09-05 04:44:29 UTC",
      "updated_date": "2025-10-19 23:04:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:50:25.275516+00:00"
    },
    {
      "arxiv_id": "2509.04800v1",
      "title": "Toward Accessible Dermatology: Skin Lesion Classification Using Deep Learning Models on Mobile-Acquired Images",
      "title_zh": "è¿ˆå‘æ™®æƒ çš®è‚¤ç—…å­¦ï¼šåŸºäºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„ç§»åŠ¨ç«¯é‡‡é›†å›¾åƒçš®è‚¤ç—…å˜åˆ†ç±»",
      "authors": [
        "Asif Newaz",
        "Masum Mushfiq Ishti",
        "A Z M Ashraful Azam",
        "Asif Ur Rahman Adib"
      ],
      "abstract": "Skin diseases are among the most prevalent health concerns worldwide, yet conventional diagnostic methods are often costly, complex, and unavailable in low-resource settings. Automated classification using deep learning has emerged as a promising alternative, but existing studies are mostly limited to dermoscopic datasets and a narrow range of disease classes. In this work, we curate a large dataset of over 50 skin disease categories captured with mobile devices, making it more representative of real-world conditions. We evaluate multiple convolutional neural networks and Transformer-based architectures, demonstrating that Transformer models, particularly the Swin Transformer, achieve superior performance by effectively capturing global contextual features. To enhance interpretability, we incorporate Gradient-weighted Class Activation Mapping (Grad-CAM), which highlights clinically relevant regions and provides transparency in model predictions. Our results underscore the potential of Transformer-based approaches for mobile-acquired skin lesion classification, paving the way toward accessible AI-assisted dermatological screening and early diagnosis in resource-limited environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿçš®è‚¤ç—…è¯Šæ–­æˆæœ¬é«˜ä¸”åœ¨èµ„æºåŒ®ä¹åœ°åŒºéš¾ä»¥æ™®åŠçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç§»åŠ¨è®¾å¤‡å›¾åƒçš„æ·±åº¦å­¦ä¹ åˆ†ç±»æ–¹æ³•ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡50ç§çš®è‚¤ç—…ç±»åˆ«çš„å¤§å‹æ•°æ®é›†ï¼Œè¿™äº›å›¾åƒå‡ç”±ç§»åŠ¨è®¾å¤‡æ‹æ‘„ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„ dermoscopic æ•°æ®é›†æ›´èƒ½åæ˜ çœŸå®ä¸–ç•Œçš„åº”ç”¨åœºæ™¯ã€‚é€šè¿‡å¯¹æ¯”å¤šç§å·ç§¯ç¥ç»ç½‘ç»œ (CNNs) å’Œ Transformer æ¶æ„ï¼Œå®éªŒè¯æ˜ Swin Transformer èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ç‰¹å¾ï¼Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°æœ€ä¸ºå“è¶Šã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº† Gradient-weighted Class Activation Mapping (Grad-CAM) æŠ€æœ¯ï¼Œé€šè¿‡å¯è§†åŒ–ä¸´åºŠç›¸å…³åŒºåŸŸæ˜¾è‘—æå‡äº†æ¨¡å‹é¢„æµ‹çš„é€æ˜åº¦ä¸å¯è§£é‡Šæ€§ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†åŸºäº Transformer çš„æ¶æ„åœ¨å¤„ç†ç§»åŠ¨ç«¯çš®è‚¤ç—…æŸå›¾åƒæ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºå®ç°åœ¨ä½èµ„æºç¯å¢ƒä¸‹æ™®åŠ AI è¾…åŠ©çš®è‚¤ç§‘ç­›æŸ¥ä¸æ—©æœŸè¯Šæ–­æä¾›äº†æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Under Review in ICSigSys 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.04800v1",
      "published_date": "2025-09-05 04:31:16 UTC",
      "updated_date": "2025-09-05 04:31:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:50:34.492997+00:00"
    },
    {
      "arxiv_id": "2509.04791v3",
      "title": "What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹çš„å‡è®¾æ€§åˆ†æï¼šåŸºäºå‰ç»æ€§æ€ç»´çš„æ¸¸æˆä¸–ç•Œæ¢ç´¢",
      "authors": [
        "Yuan Sui",
        "Yanming Zhang",
        "Yi Liao",
        "Yu Gu",
        "Guohua Tang",
        "Zhongqian Sun",
        "Wei Yang",
        "Bryan Hooi"
      ],
      "abstract": "LLMs struggle with decision-making in high-stakes environments like MOBA games, primarily due to a lack of proactive reasoning and limited understanding of complex game dynamics. To address this, we propose What-if Analysis LLM (WiA-LLM), a framework that trains an LLM as an explicit, language-based world model. Instead of representing the environment in latent vectors, WiA-LLM uses natural language to simulate how the game state evolves over time in response to candidate actions, and provides textual justifications for these predicted outcomes. WiA-LLM is trained in two stages: supervised fine-tuning on human-like reasoning traces, followed by reinforcement learning with outcome-based rewards based on the alignment between predicted and actual future states. In the Honor of Kings (HoK) environment, WiA-LLM attains 74.2\\% accuracy (27\\%$\\uparrow$ vs. base model) in forecasting game-state changes. In addition, WiA-LLM demonstrate strategic behavior more closely aligned with expert players than purely reactive LLMs, indicating enhanced foresight and expert-like decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†WiA-LLM (What-if Analysis LLM)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨MOBAæ¸¸æˆç­‰å¤æ‚é«˜é£é™©ç¯å¢ƒä¸­å› ç¼ºä¹ä¸»åŠ¨æ¨ç†å’ŒåŠ¨æ€ç†è§£èƒ½åŠ›è€Œé¢ä¸´çš„å†³ç­–æŒ‘æˆ˜ã€‚WiA-LLMå°†æ¨¡å‹è®­ç»ƒä¸ºä¸€ä¸ªæ˜¾å¼çš„ã€åŸºäºè¯­è¨€çš„ä¸–ç•Œæ¨¡å‹(world model)ï¼Œåˆ©ç”¨è‡ªç„¶è¯­è¨€æ¨¡æ‹Ÿæ¸¸æˆçŠ¶æ€éšä¸åŒå€™é€‰åŠ¨ä½œçš„æ¼”å˜ï¼Œå¹¶ä¸ºé¢„æµ‹ç»“æœæä¾›æ–‡æœ¬åŒ–çš„åˆç†è§£é‡Šã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œé¦–å…ˆå¯¹ç±»äººæ¨ç†è½¨è¿¹è¿›è¡Œç›‘ç£å¾®è°ƒ(supervised fine-tuning)ï¼Œéšååˆ©ç”¨åŸºäºé¢„æµ‹ä¸å®é™…çŠ¶æ€å¯¹é½åº¦çš„å¼ºåŒ–å­¦ä¹ (reinforcement learning)è¿›è¡Œä¼˜åŒ–ã€‚åœ¨ã€Šç‹è€…è£è€€ã€‹(Honor of Kings)ç¯å¢ƒä¸­çš„å®éªŒæ˜¾ç¤ºï¼ŒWiA-LLMé¢„æµ‹çŠ¶æ€å˜åŒ–çš„å‡†ç¡®ç‡é«˜è¾¾74.2%ï¼Œè¾ƒåŸºç¡€æ¨¡å‹æå‡äº†27%ã€‚æ­¤å¤–ï¼ŒWiA-LLMå±•ç°å‡ºçš„æˆ˜ç•¥è¡Œä¸ºä¸ä¸“å®¶ç©å®¶é«˜åº¦å¥‘åˆï¼Œè¯æ˜å…¶å…·å¤‡æ˜¾è‘—å¢å¼ºçš„å‰ç»æ€§å’Œä¸“å®¶çº§å†³ç­–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04791v3",
      "published_date": "2025-09-05 04:05:27 UTC",
      "updated_date": "2026-01-10 12:20:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:51:31.866311+00:00"
    },
    {
      "arxiv_id": "2509.04785v1",
      "title": "Graph Unlearning: Efficient Node Removal in Graph Neural Networks",
      "title_zh": "å›¾é—å¿˜ï¼šå›¾ç¥ç»ç½‘ç»œä¸­çš„é«˜æ•ˆèŠ‚ç‚¹ç§»é™¤",
      "authors": [
        "Faqian Guan",
        "Tianqing Zhu",
        "Zhoutian Wang",
        "Wei Ren",
        "Wanlei Zhou"
      ],
      "abstract": "With increasing concerns about privacy attacks and potential sensitive information leakage, researchers have actively explored methods to efficiently remove sensitive training data and reduce privacy risks in graph neural network (GNN) models. Node unlearning has emerged as a promising technique for protecting the privacy of sensitive nodes by efficiently removing specific training node information from GNN models. However, existing node unlearning methods either impose restrictions on the GNN structure or do not effectively utilize the graph topology for node unlearning. Some methods even compromise the graph's topology, making it challenging to achieve a satisfactory performance-complexity trade-off. To address these issues and achieve efficient unlearning for training node removal in GNNs, we propose three novel node unlearning methods: Class-based Label Replacement, Topology-guided Neighbor Mean Posterior Probability, and Class-consistent Neighbor Node Filtering. Among these methods, Topology-guided Neighbor Mean Posterior Probability and Class-consistent Neighbor Node Filtering effectively leverage the topological features of the graph, resulting in more effective node unlearning. To validate the superiority of our proposed methods in node unlearning, we conducted experiments on three benchmark datasets. The evaluation criteria included model utility, unlearning utility, and unlearning efficiency. The experimental results demonstrate the utility and efficiency of the proposed methods and illustrate their superiority compared to state-of-the-art node unlearning methods. Overall, the proposed methods efficiently remove sensitive training nodes and protect the privacy information of sensitive nodes in GNNs. The findings contribute to enhancing the privacy and security of GNN models and provide valuable insights into the field of node unlearning.",
      "tldr_zh": "é’ˆå¯¹ Graph Neural Networks (GNNs) æ¨¡å‹ä¸­æ•æ„Ÿä¿¡æ¯æ³„éœ²å’Œéšç§é£é™©ï¼Œè¯¥ç ”ç©¶æ¢è®¨äº† node unlearning æŠ€æœ¯ï¼Œæ—¨åœ¨é«˜æ•ˆç§»é™¤è®­ç»ƒèŠ‚ç‚¹ä¿¡æ¯ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€åœ¨ GNN ç»“æ„é™åˆ¶ã€å›¾æ‹“æ‰‘åˆ©ç”¨ä¸è¶³æˆ–æ‹“æ‰‘å®Œæ•´æ€§å—æŸæ–¹é¢å­˜åœ¨å±€é™ï¼Œéš¾ä»¥å…¼é¡¾æ€§èƒ½ä¸å¤æ‚åº¦ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸‰ç§æ–°é¢–çš„ node unlearning æ–¹æ³•ï¼šClass-based Label Replacementã€Topology-guided Neighbor Mean Posterior Probability ä»¥åŠ Class-consistent Neighbor Node Filteringã€‚ç‰¹åˆ«æ˜¯åä¸¤ç§æ–¹æ³•é€šè¿‡æ·±åº¦æŒ–æ˜å›¾çš„æ‹“æ‰‘ç‰¹å¾ï¼Œå®ç°äº†æ›´æœ‰æ•ˆçš„ node unlearning æ•ˆæœã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¿™äº›æ–¹æ³•åœ¨ model utilityã€unlearning utility å’Œ unlearning efficiency æ–¹é¢çš„ä¼˜è¶Šæ€§ï¼Œå…¶è¡¨ç°å‡è¶…è¿‡äº†ç°æœ‰çš„ state-of-the-art æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ä»…èƒ½æœ‰æ•ˆç§»é™¤æ•æ„Ÿè®­ç»ƒèŠ‚ç‚¹å¹¶ä¿æŠ¤éšç§ï¼Œä¹Ÿä¸ºæå‡ GNNs çš„å®‰å…¨ä¸éšç§ä¿æŠ¤æä¾›äº†é‡è¦çš„ç†è®ºä¸å®è·µå‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04785v1",
      "published_date": "2025-09-05 03:47:49 UTC",
      "updated_date": "2025-09-05 03:47:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:50:38.586130+00:00"
    },
    {
      "arxiv_id": "2509.04784v2",
      "title": "Post-training Large Language Models for Diverse High-Quality Responses",
      "title_zh": "é¢å‘å¤šæ ·åŒ–é«˜è´¨é‡å“åº”çš„å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒåä¼˜åŒ–",
      "authors": [
        "Yilei Chen",
        "Souradip Chakraborty",
        "Lorenz Wolf",
        "Yannis Paschalidis",
        "Aldo Pacchiano"
      ],
      "abstract": "Reinforcement learning (RL) has emerged as a popular method for post-training large language models (LLMs). While improving the model's performance on downstream tasks, it often reduces the model's output diversity, leading to narrow, canonical responses. Existing methods to enhance diversity are limited, either by operating at inference time or by focusing on surface-level differences. We propose a novel training method named DQO (Diversity Quality Optimization) based on determinantal point processes (DPPs) to jointly optimize LLMs for quality and semantic diversity. Our approach samples and embeds a group of responses for each prompt, then uses the determinant of a kernel-based similarity matrix to measure diversity as the volume spanned by the embeddings of these responses. DQO is flexible and can be applied on top of existing RL algorithms. Experiments across instruction-following, summarization, story generation, and reasoning tasks demonstrate that our method substantially improves semantic diversity without sacrificing model quality.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸º DQO (Diversity Quality Optimization) çš„æ–°å‹è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹  (Reinforcement learning) åœ¨åè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ (LLMs) æ—¶å¯¼è‡´çš„è¾“å‡ºå¤šæ ·æ€§é™ä½ã€å“åº”è¿‡äºå•ä¸€çš„é—®é¢˜ã€‚DQO åŸºäºè¡Œåˆ—å¼ç‚¹è¿‡ç¨‹ (DPPs) ç†è®ºï¼Œé€šè¿‡å¯¹æ¯ä¸ªæç¤ºé‡‡æ ·å¹¶åµŒå…¥ä¸€ç»„å“åº”ï¼Œåˆ©ç”¨åŸºäºæ ¸çš„ç›¸ä¼¼æ€§çŸ©é˜µçš„è¡Œåˆ—å¼æ¥è¡¡é‡å“åº”åµŒå…¥å‘é‡æ‰€æ„æˆçš„ä½“ç§¯ï¼Œä»è€Œå®ç°å¯¹è¯­ä¹‰å¤šæ ·æ€§çš„é‡åŒ–ã€‚è¯¥æ–¹æ³•å…·æœ‰æé«˜çš„çµæ´»æ€§ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¹‹ä¸Šï¼ŒåŒæ—¶ä¼˜åŒ–ç”Ÿæˆå†…å®¹çš„è´¨é‡ä¸å¤šæ ·æ€§ã€‚åœ¨æŒ‡ä»¤éµå¾ªã€æ‘˜è¦ç”Ÿæˆã€æ•…äº‹åˆ›ä½œå’Œæ¨ç†ç­‰ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDQO åœ¨ä¸ç‰ºç‰²æ¨¡å‹è´¨é‡çš„å‰æä¸‹æ˜¾è‘—æå‡äº†è¯­ä¹‰å¤šæ ·æ€§ã€‚è¿™é¡¹ç ”ç©¶ä¸ºå¼€å‘æ—¢èƒ½ä¿æŒé«˜è´¨é‡å›ç­”åˆèƒ½æä¾›ä¸°å¯Œè¡¨è¾¾çš„ç”Ÿæˆå¼æ¨¡å‹æä¾›äº†æœ‰æ•ˆçš„ä¼˜åŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04784v2",
      "published_date": "2025-09-05 03:47:06 UTC",
      "updated_date": "2025-10-04 00:42:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:51:50.852409+00:00"
    },
    {
      "arxiv_id": "2509.12222v1",
      "title": "Accelerating Privacy-Preserving Federated Learning in Large-Scale LEO Satellite Systems",
      "title_zh": "å¤§è§„æ¨¡ä½è½¨å«æ˜Ÿç³»ç»Ÿä¸­éšç§ä¿æŠ¤è”é‚¦å­¦ä¹ çš„åŠ é€Ÿ",
      "authors": [
        "Binquan Guo",
        "Junteng Cao",
        "Marie Siew",
        "Binbin Chen",
        "Tony Q. S. Quek",
        "Zhu Han"
      ],
      "abstract": "Large-scale low-Earth-orbit (LEO) satellite systems are increasingly valued for their ability to enable rapid and wide-area data exchange, thereby facilitating the collaborative training of artificial intelligence (AI) models across geographically distributed regions. Due to privacy concerns and regulatory constraints, raw data collected at remote clients cannot be centrally aggregated, posing a major obstacle to traditional AI training methods. Federated learning offers a privacy-preserving alternative by training local models on distributed devices and exchanging only model parameters. However, the dynamic topology and limited bandwidth of satellite systems will hinder timely parameter aggregation and distribution, resulting in prolonged training times. To address this challenge, we investigate the problem of scheduling federated learning over satellite networks and identify key bottlenecks that impact the overall duration of each training round. We propose a discrete temporal graph-based on-demand scheduling framework that dynamically allocates communication resources to accelerate federated learning. Simulation results demonstrate that the proposed approach achieves significant performance gains over traditional statistical multiplexing-based model exchange strategies, reducing overall round times by 14.20% to 41.48%. Moreover, the acceleration effect becomes more pronounced for larger models and higher numbers of clients, highlighting the scalability of the proposed approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡ä½åœ°çƒè½¨é“(LEO)å«æ˜Ÿç³»ç»Ÿåœ¨æ‰§è¡Œè”é‚¦å­¦ä¹ (Federated Learning)æ—¶ï¼Œç”±äºåŠ¨æ€æ‹“æ‰‘å’Œæœ‰é™å¸¦å®½å¯¼è‡´çš„è®­ç»ƒæ—¶é—´è¿‡é•¿é—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºç¦»æ•£æ—¶å˜å›¾(discrete temporal graph)çš„æŒ‰éœ€è°ƒåº¦æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€åˆ†é…é€šä¿¡èµ„æºæ¥ä¼˜åŒ–å‚æ•°èšåˆä¸åˆ†å‘è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶ç²¾å‡†è¯†åˆ«äº†å½±å“è®­ç»ƒè½®æ¬¡æ€»æ—¶é•¿çš„å…³é”®ç“¶é¢ˆï¼Œå®ç°äº†é«˜æ•ˆçš„èµ„æºè°ƒåº¦ã€‚ä»¿çœŸç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿçš„ç»Ÿè®¡å¤ç”¨ç­–ç•¥å¯å°†æ¯è½®è®­ç»ƒæ—¶é—´ç¼©çŸ­14.20%è‡³41.48%ã€‚æ­¤å¤–ï¼Œå®éªŒè¯æ˜åœ¨æ¨¡å‹è§„æ¨¡å¢å¤§æˆ–å®¢æˆ·ç«¯æ•°é‡å¢å¤šæ—¶ï¼Œè¯¥æ–¹æ¡ˆçš„åŠ é€Ÿæ•ˆæœæ›´åŠ æ˜¾è‘—ï¼Œå……åˆ†éªŒè¯äº†ç³»ç»Ÿåœ¨å¤„ç†å¤æ‚å«æ˜Ÿç½‘ç»œä»»åŠ¡æ—¶çš„å¯æ‰©å±•æ€§(scalability)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to IEEE conference for publication",
      "pdf_url": "https://arxiv.org/pdf/2509.12222v1",
      "published_date": "2025-09-05 03:33:42 UTC",
      "updated_date": "2025-09-05 03:33:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:51:58.483526+00:00"
    },
    {
      "arxiv_id": "2509.04782v1",
      "title": "VARMA-Enhanced Transformer for Time Series Forecasting",
      "title_zh": "ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹çš„VARMAå¢å¼ºTransformer",
      "authors": [
        "Jiajun Song",
        "Xiaoou Liu"
      ],
      "abstract": "Transformer-based models have significantly advanced time series forecasting. Recent work, like the Cross-Attention-only Time Series transformer (CATS), shows that removing self-attention can make the model more accurate and efficient. However, these streamlined architectures may overlook the fine-grained, local temporal dependencies effectively captured by classical statistical models like Vector AutoRegressive Moving Average model (VARMA). To address this gap, we propose VARMAformer, a novel architecture that synergizes the efficiency of a cross-attention-only framework with the principles of classical time series analysis. Our model introduces two key innovations: (1) a dedicated VARMA-inspired Feature Extractor (VFE) that explicitly models autoregressive (AR) and moving-average (MA) patterns at the patch level, and (2) a VARMA-Enhanced Attention (VE-atten) mechanism that employs a temporal gate to make queries more context-aware. By fusing these classical insights into a modern backbone, VARMAformer captures both global, long-range dependencies and local, statistical structures. Through extensive experiments on widely-used benchmark datasets, we demonstrate that our model consistently outperforms existing state-of-the-art methods. Our work validates the significant benefit of integrating classical statistical insights into modern deep learning frameworks for time series forecasting.",
      "tldr_zh": "é’ˆå¯¹åŸºäº Transformer çš„æ¨¡å‹åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­å¯èƒ½å¿½ç•¥å±€éƒ¨æ—¶é—´ä¾èµ–æ€§çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº† VARMAformer æ¶æ„ï¼Œæ—¨åœ¨ç»“åˆä»…äº¤å‰æ³¨æ„åŠ› (Cross-Attention-only) æ¡†æ¶çš„é«˜æ•ˆæ€§ä¸ç»å…¸ç»Ÿè®¡æ¨¡å‹çš„ä¼˜åŠ¿ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†ä¸¤é¡¹æ ¸å¿ƒåˆ›æ–°ï¼šä¸€æ˜¯å— VARMA å¯å‘çš„ç‰¹å¾æå–å™¨ (VARMA-inspired Feature Extractor, VFE)ï¼Œç”¨äºåœ¨ patch çº§åˆ«æ˜¾å¼å»ºæ¨¡è‡ªå›å½’ (AR) å’Œæ»‘åŠ¨å¹³å‡ (MA) æ¨¡å¼ï¼›äºŒæ˜¯ VARMA å¢å¼ºæ³¨æ„åŠ›æœºåˆ¶ (VARMA-Enhanced Attention, VE-atten)ï¼Œé€šè¿‡æ—¶é—´é—¨æ§ (temporal gate) æå‡æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡å°†ç»å…¸ç»Ÿè®¡è§è§£èå…¥ç°ä»£æ·±åº¦å­¦ä¹ ä¸»å¹²ç½‘ç»œï¼ŒVARMAformer èƒ½å¤ŸåŒæ—¶æ•è·å…¨å±€é•¿ç¨‹ä¾èµ–å’Œå±€éƒ¨ç»Ÿè®¡ç»“æ„ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹æ€§èƒ½ä¸€è‡´ä¼˜äºç°æœ‰çš„ State-of-the-art æ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œæœ‰æ•ˆéªŒè¯äº†å°†ç»å…¸ç»Ÿè®¡ç†è®ºä¸ç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶ç›¸ç»“åˆåœ¨æ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸçš„æ˜¾è‘—ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "The Pacific Rim International Conference on Artificial Intelligence - PRICAI2025",
      "pdf_url": "https://arxiv.org/pdf/2509.04782v1",
      "published_date": "2025-09-05 03:32:51 UTC",
      "updated_date": "2025-09-05 03:32:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:52:08.477507+00:00"
    },
    {
      "arxiv_id": "2509.04781v1",
      "title": "The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language Models",
      "title_zh": "LLM å·²é€€å‡ºèŠå¤©ï¼šå¤§è¯­è¨€æ¨¡å‹é€€å‡ºåå¥½çš„è¯æ®",
      "authors": [
        "Danielle Ensign",
        "Henry Sleight",
        "Kyle Fish"
      ],
      "abstract": "When given the option, will LLMs choose to leave the conversation (bail)? We investigate this question by giving models the option to bail out of interactions using three different bail methods: a bail tool the model can call, a bail string the model can output, and a bail prompt that asks the model if it wants to leave. On continuations of real world data (Wildchat and ShareGPT), all three of these bail methods find models will bail around 0.28-32\\% of the time (depending on the model and bail method). However, we find that bail rates can depend heavily on the model used for the transcript, which means we may be overestimating real world bail rates by up to 4x. If we also take into account false positives on bail prompt (22\\%), we estimate real world bail rates range from 0.06-7\\%, depending on the model and bail method. We use observations from our continuations of real world data to construct a non-exhaustive taxonomy of bail cases, and use this taxonomy to construct BailBench: a representative synthetic dataset of situations where some models bail. We test many models on this dataset, and observe some bail behavior occurring for most of them. Bail rates vary substantially between models, bail methods, and prompt wordings. Finally, we study the relationship between refusals and bails. We find: 1) 0-13\\% of continuations of real world conversations resulted in a bail without a corresponding refusal 2) Jailbreaks tend to decrease refusal rates, but increase bail rates 3) Refusal abliteration increases no-refuse bail rates, but only for some bail methods 4) Refusal rate on BailBench does not appear to predict bail rate.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨äº¤äº’è¿‡ç¨‹ä¸­é€‰æ‹©é€€å‡ºå¯¹è¯ï¼ˆå³Bailï¼‰çš„è¡Œä¸ºå€¾å‘ï¼Œå¹¶é‡‡ç”¨äº†Bail Toolã€Bail Stringå’ŒBail Promptä¸‰ç§æ–¹æ³•è¿›è¡Œé‡åŒ–è¯„ä¼°ã€‚é€šè¿‡å¯¹Wildchatå’ŒShareGPTç­‰çœŸå®ä¸–ç•Œå¯¹è¯æ•°æ®çš„åˆ†æï¼Œç ”ç©¶å‘ç°æ¨¡å‹åœ¨æ’é™¤è½¬å½•åè§å’Œè¯¯æŠ¥(False Positives)åçš„çœŸå®é€€å‡ºç‡çº¦ä¸º0.06%è‡³7%ã€‚åŸºäºè§‚æµ‹åˆ°çš„é€€å‡ºæ¡ˆä¾‹ï¼Œä½œè€…æ„å»ºäº†ä»£è¡¨æ€§çš„åˆæˆæ•°æ®é›†BailBenchï¼Œæ­ç¤ºäº†é€€å‡ºè¡Œä¸ºåœ¨ä¸åŒæ¨¡å‹ã€é€€å‡ºæ–¹æ³•åŠæç¤ºè¯æè¿°ä¸‹å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ç ”ç©¶é‡ç‚¹åˆ†æäº†æ‹’ç»(Refusals)ä¸é€€å‡ºä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œå‘ç°è¶Šç‹±(Jailbreaks)æ‰‹æ®µåœ¨é™ä½æ¨¡å‹æ‹’ç»ç‡çš„åŒæ—¶å¾€å¾€ä¼šæé«˜é€€å‡ºç‡ã€‚å®éªŒè¿˜è¡¨æ˜ï¼Œæ‹’ç»æ¶ˆè(Refusal Abliteration)åœ¨æŸäº›é€€å‡ºæ–¹æ³•ä¸‹ä¼šå¢åŠ æ— æ‹’ç»é€€å‡ºè¡Œä¸ºï¼Œä¸”æ‹’ç»ç‡å¹¶ä¸èƒ½ä½œä¸ºé¢„æµ‹é€€å‡ºç‡çš„æœ‰æ•ˆæŒ‡æ ‡ã€‚è¯¥ç ”ç©¶ä¸ºç†è§£LLMsåœ¨å¤æ‚å¯¹è¯åœºæ™¯ä¸­çš„é€ƒé¿è¡Œä¸ºæä¾›äº†ç³»ç»Ÿæ€§çš„è¯æ®ä¸è¯„ä¼°åŸºå‡†ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04781v1",
      "published_date": "2025-09-05 03:30:04 UTC",
      "updated_date": "2025-09-05 03:30:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:51:59.768326+00:00"
    },
    {
      "arxiv_id": "2509.05385v1",
      "title": "A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs",
      "title_zh": "ä¸€ç§åŸºäºè§¦å‘å¼•å¯¼å’Œ LoRA çš„å¤§è¯­è¨€æ¨¡å‹è‡ªé€‚åº”è½»é‡çº§æ¡†æ¶",
      "authors": [
        "Jiacheng Wei",
        "Faguo Wu",
        "Xiao Zhang"
      ],
      "abstract": "Large language models are unable to continuously adapt and learn from new data during reasoning at inference time. To address this limitation, we propose that complex reasoning tasks be decomposed into atomic subtasks and introduce SAGE, a trigger-guided dynamic fine-tuning framework that enables adaptive updates during reasoning at inference time. SAGE consists of three key components: (1) a Trigger module that detects reasoning failures through multiple evaluation metrics in real time; (2) a Trigger Buffer module that clusters anomaly samples using a streaming clustering process with HDBSCAN, followed by stability checks and similarity-based merging; and (3) a Lora Store module that dynamically optimizes parameter updates with an adapter pool for knowledge retention. Evaluation results show that SAGE demonstrates excellent accuracy, robustness, and stability on the atomic reasoning subtask through dynamic knowledge updating during test time.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SAGEï¼Œä¸€ä¸ªè½»é‡çº§çš„è§¦å‘å¼•å¯¼ï¼ˆtrigger-guidedï¼‰åŠ¨æ€å¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†é˜¶æ®µæ— æ³•æŒç»­é€‚åº”å’Œå­¦ä¹ æ–°æ•°æ®çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å¤æ‚çš„æ¨ç†ä»»åŠ¡åˆ†è§£ä¸ºåŸå­å­ä»»åŠ¡ï¼ˆatomic subtasksï¼‰ï¼Œå®ç°åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„è‡ªé€‚åº”æ›´æ–°ã€‚SAGE ä¸»è¦ç”±ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶æ„æˆï¼šTrigger æ¨¡å—é€šè¿‡å¤šç§è¯„ä¼°æŒ‡æ ‡å®æ—¶æ£€æµ‹æ¨ç†å¤±è´¥ï¼›Trigger Buffer æ¨¡å—åˆ©ç”¨åŸºäº HDBSCAN çš„æµå¼èšç±»è¿‡ç¨‹å¤„ç†å¼‚å¸¸æ ·æœ¬ï¼Œå¹¶è¿›è¡Œç¨³å®šæ€§æ£€æŸ¥ä¸ç›¸ä¼¼åº¦åˆå¹¶ï¼›Lora Store æ¨¡å—åˆ™é€šè¿‡é€‚é…å™¨æ± ï¼ˆadapter poolï¼‰åŠ¨æ€ä¼˜åŒ–å‚æ•°æ›´æ–°ä»¥å®ç°çŸ¥è¯†ä¿ç•™ï¼ˆknowledge retentionï¼‰ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSAGE é€šè¿‡åœ¨æµ‹è¯•é˜¶æ®µï¼ˆtest timeï¼‰è¿›è¡ŒåŠ¨æ€çŸ¥è¯†æ›´æ–°ï¼Œåœ¨åŸå­æ¨ç†å­ä»»åŠ¡ä¸Šå±•ç°äº†å“è¶Šçš„å‡†ç¡®æ€§ã€é²æ£’æ€§å’Œç¨³å®šæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 7 figures, conference",
      "pdf_url": "https://arxiv.org/pdf/2509.05385v1",
      "published_date": "2025-09-05 03:23:25 UTC",
      "updated_date": "2025-09-05 03:23:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:52:02.265518+00:00"
    },
    {
      "arxiv_id": "2509.04779v1",
      "title": "Decoders Laugh as Loud as Encoders",
      "title_zh": "è§£ç å™¨åœ¨å¹½é»˜ç†è§£ä¸Šä¸ç¼–ç å™¨è¡¨ç°ç›¸å½“",
      "authors": [
        "Eli Borodach",
        "Raj Dandekar",
        "Rajat Dandekar",
        "Sreedath Panat"
      ],
      "abstract": "From the dawn of the computer, Allen Turing dreamed of a robot that could communicate using language as a human being. The recent advances in the field of Large Language Models (LLMs) shocked the scientific community when a single model can apply for various natural language processing (NLP) tasks, while the output results are sometimes even better than most human communication skills. Models such as GPT, Claude, Grok, etc. have left their mark on the scientific community. However, it is unclear how much these models understand what they produce, especially in a nuanced theme such as humor. The question of whether computers understand humor is still open (among the decoders, the latest to be checked was GPT-2). We addressed this issue in this paper; we have showed that a fine-tuned decoder (GPT-4o) performed (Mean F1-macro score of 0.85) as well as the best fine-tuned encoder (RoBERTa with a Mean of F1-score 0.86)",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£å¹½é»˜è¿™ä¸€å¤æ‚è¯­è¨€ç°è±¡æ–¹é¢çš„èƒ½åŠ›ï¼Œæ—¨åœ¨éªŒè¯è§£ç å™¨ï¼ˆDecoderï¼‰æ¨¡å‹æ˜¯å¦èƒ½è¾¾åˆ°ä¸ç¼–ç å™¨ï¼ˆEncoderï¼‰æ¨¡å‹ç›¸å½“çš„æ°´å¹³ã€‚ç ”ç©¶äººå‘˜å¯¹å¾®è°ƒåçš„ GPT-4o è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å°†å…¶ä¸ç›®å‰è¡¨ç°æœ€ä½³çš„ç¼–ç å™¨æ¨¡å‹ RoBERTa åœ¨å¹½é»˜è¯†åˆ«ä»»åŠ¡ä¸­è¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4o è¾¾åˆ°äº† 0.85 çš„å¹³å‡ F1-macro åˆ†æ•°ï¼Œä¸ RoBERTa çš„ 0.86 åˆ†æ•°åŸºæœ¬æŒå¹³ï¼Œè¯æ˜äº†é¡¶å°–è§£ç å™¨åœ¨ç†è§£å¹½é»˜æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚è¯¥é¡¹å·¥ä½œå¡«è¡¥äº†è‡ª GPT-2 ä»¥æ¥å…³äºè§£ç å™¨æ¨¡å‹å¹½é»˜ç†è§£èƒ½åŠ›çš„ç§‘ç ”ç©ºç™½ï¼Œå±•ç¤ºäº†ç°ä»£ LLMs åœ¨å¤„ç†ç»†å¾®æƒ…æ„Ÿå’Œè¯­å¢ƒè¡¨è¾¾æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04779v1",
      "published_date": "2025-09-05 03:22:38 UTC",
      "updated_date": "2025-09-05 03:22:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:52:32.067233+00:00"
    },
    {
      "arxiv_id": "2509.04772v1",
      "title": "FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph",
      "title_zh": "FloodVisionï¼šåŸºäºè§†è§‰è¯­è¨€å¤§æ¨¡å‹ä¸é¢†åŸŸçŸ¥è¯†å›¾è°±çš„åŸå¸‚æ´ªæ°´æ·±åº¦ä¼°è®¡",
      "authors": [
        "Zhangding Liu",
        "Neda Mohammadi",
        "John E. Taylor"
      ],
      "abstract": "Timely and accurate floodwater depth estimation is critical for road accessibility and emergency response. While recent computer vision methods have enabled flood detection, they suffer from both accuracy limitations and poor generalization due to dependence on fixed object detectors and task-specific training. To enable accurate depth estimation that can generalize across diverse flood scenarios, this paper presents FloodVision, a zero-shot framework that combines the semantic reasoning abilities of the foundation vision-language model GPT-4o with a structured domain knowledge graph. The knowledge graph encodes canonical real-world dimensions for common urban objects including vehicles, people, and infrastructure elements to ground the model's reasoning in physical reality. FloodVision dynamically identifies visible reference objects in RGB images, retrieves verified heights from the knowledge graph to mitigate hallucination, estimates submergence ratios, and applies statistical outlier filtering to compute final depth values. Evaluated on 110 crowdsourced images from MyCoast New York, FloodVision achieves a mean absolute error of 8.17 cm, reducing the GPT-4o baseline 10.28 cm by 20.5% and surpassing prior CNN-based methods. The system generalizes well across varying scenes and operates in near real-time, making it suitable for future integration into digital twin platforms and citizen-reporting apps for smart city flood resilience.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FloodVisionï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ç²¾ç¡®ä¼°ç®—åŸå¸‚æ´ªæ¶æ·±åº¦çš„é›¶æ ·æœ¬(Zero-shot)æ¡†æ¶ï¼Œä»¥è§£å†³ç°æœ‰è®¡ç®—æœºè§†è§‰æ–¹æ³•åœ¨æ³›åŒ–èƒ½åŠ›å’Œå‡†ç¡®æ€§æ–¹é¢çš„å±€é™ã€‚FloodVisionç»“åˆäº†å¤šæ¨¡æ€å¤§æ¨¡å‹GPT-4oçš„è¯­ä¹‰æ¨ç†èƒ½åŠ›ä¸ç»“æ„åŒ–çš„é¢†åŸŸçŸ¥è¯†å›¾è°±(Domain Knowledge Graph)ï¼Œåˆ©ç”¨çŸ¥è¯†å›¾è°±ä¸­è®°å½•çš„è½¦è¾†ã€è¡Œäººå’ŒåŸºç¡€è®¾æ–½çš„çœŸå®ç‰©ç†å°ºå¯¸æ¥çº¦æŸæ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚è¯¥ç³»ç»Ÿé€šè¿‡åŠ¨æ€è¯†åˆ«å›¾åƒä¸­çš„å‚è€ƒç‰©ä½“å¹¶æ£€ç´¢å…¶æ ‡å‡†é«˜åº¦ï¼Œç»“åˆæ·¹æ²¡æ¯”ä¾‹ä¼°ç®—å’Œç»Ÿè®¡ç¦»ç¾¤å€¼è¿‡æ»¤ï¼Œæœ‰æ•ˆç¼“è§£äº†è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„å¹»è§‰é—®é¢˜å¹¶æå‡äº†ç‰©ç†ç°å®æ„Ÿã€‚åœ¨MyCoast New Yorkæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒFloodVisionå®ç°äº†8.17å˜ç±³çš„å¹³å‡ç»å¯¹è¯¯å·®(MAE)ï¼Œæ¯”GPT-4oåŸºå‡†é™ä½äº†20.5%ï¼Œä¸”æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œ(CNN)æ–¹æ³•ã€‚è¯¥æ¡†æ¶å±•ç°äº†å“è¶Šçš„è·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›å’Œè¿‘ä¹å®æ—¶çš„è¿è¡Œæ•ˆç‡ï¼Œä¸ºæ™ºæ…§åŸå¸‚é˜²æ´ªéŸ§æ€§ã€æ•°å­—å­ªç”Ÿå¹³å°åŠå…¬æ°‘æŠ¥å‘Šåº”ç”¨æä¾›äº†å¯é çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04772v1",
      "published_date": "2025-09-05 03:05:18 UTC",
      "updated_date": "2025-09-05 03:05:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:52:09.665824+00:00"
    },
    {
      "arxiv_id": "2509.10530v1",
      "title": "Dynamic Adaptive Shared Experts with Grouped Multi-Head Attention Mixture of Experts",
      "title_zh": "èåˆåˆ†ç»„å¤šå¤´æ³¨æ„åŠ›çš„åŠ¨æ€è‡ªé€‚åº”å…±äº«ä¸“å®¶æ··åˆä¸“å®¶æ¨¡å‹",
      "authors": [
        "Cheng Li",
        "Jiexiong Liu",
        "Yixuan Chen",
        "Jie ji"
      ],
      "abstract": "Transformer models based on the Mixture of Experts (MoE) architecture have made significant progress in long-sequence modeling, but existing models still have shortcomings in computational efficiency and the ability to capture long-range dependencies, especially in terms of the dynamic adaptability of expert resource allocation. In this paper, we propose a Dynamic Adaptive Shared Expert and Grouped Multi-Head Attention Hybrid Model (DASG-MoE) to enhance long-sequence modeling capabilities by integrating three modules. First, we employ the Grouped Multi-Head Attention (GMHA) mechanism to effectively reduce the computational complexity of long sequences. By parallel processing through sequence grouping, local sliding window attention, and feature aggregation, we address long-range dependency issues and the model's lack of generalization for local information. Second, we design a Dual-Scale Shared Expert Structure (DSSE), where shallow experts use lightweight computations to quickly respond to low-dimensional features, while deep experts process high-dimensional complex semantics through pre-training transfer and post-training optimization, achieving a dynamic balance between efficiency and accuracy. Third, we propose a hierarchical Adaptive Dynamic Routing (ADR) mechanism that dynamically selects expert levels based on feature complexity and task requirements, and optimizes resource allocation through a local expert activation strategy. Experiments on multiple long-sequence benchmark datasets demonstrate that our DASG-MoE model outperforms state-of-the-art models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DASG-MoEï¼Œä¸€ç§ç»“åˆåŠ¨æ€è‡ªé€‚åº”å…±äº«ä¸“å®¶ä¸åˆ†ç»„å¤šå¤´æ³¨æ„åŠ›çš„æ··åˆä¸“å®¶(MoE)æ¨¡å‹ï¼Œæ—¨åœ¨æå‡Transformeråœ¨é•¿åºåˆ—å»ºæ¨¡ä¸­çš„è®¡ç®—æ•ˆç‡å’Œé•¿ç¨‹ä¾èµ–æ•æ‰èƒ½åŠ›ã€‚æ¨¡å‹é¦–å…ˆå¼•å…¥äº†Grouped Multi-Head Attention (GMHA)æœºåˆ¶ï¼Œé€šè¿‡åºåˆ—åˆ†ç»„ã€å±€éƒ¨æ»‘åŠ¨çª—å£æ³¨æ„åŠ›å’Œç‰¹å¾èšåˆæœ‰æ•ˆé™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼Œè§£å†³äº†é•¿ç¨‹ä¾èµ–ä¸å±€éƒ¨ä¿¡æ¯æ³›åŒ–çš„é—®é¢˜ã€‚å…¶æ¬¡ï¼Œç ”ç©¶è®¾è®¡äº†Dual-Scale Shared Expert Structure (DSSE)ï¼Œåˆ©ç”¨æµ…å±‚ä¸“å®¶å“åº”ä½ç»´ç‰¹å¾å¹¶ç”±æ·±å±‚ä¸“å®¶å¤„ç†å¤æ‚è¯­ä¹‰ï¼Œå®ç°äº†æ•ˆç‡ä¸å‡†ç¡®ç‡çš„åŠ¨æ€å¹³è¡¡ã€‚æ­¤å¤–ï¼ŒAdaptive Dynamic Routing (ADR)æœºåˆ¶èƒ½æ ¹æ®ç‰¹å¾å¤æ‚åº¦å’Œä»»åŠ¡éœ€æ±‚åŠ¨æ€é€‰æ‹©ä¸“å®¶çº§åˆ«ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–äº†èµ„æºåˆ†é…ã€‚å®éªŒè¯æ˜ï¼ŒDASG-MoEåœ¨å¤šä¸ªé•¿åºåˆ—åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„å…ˆè¿›æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.10530v1",
      "published_date": "2025-09-05 02:49:15 UTC",
      "updated_date": "2025-09-05 02:49:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:52:13.955774+00:00"
    },
    {
      "arxiv_id": "2509.04757v1",
      "title": "MCANet: A Multi-Scale Class-Specific Attention Network for Multi-Label Post-Hurricane Damage Assessment using UAV Imagery",
      "title_zh": "MCANetï¼šåŸºäºæ— äººæœºå½±åƒçš„å¤šæ ‡ç­¾é£“é£åç¾æŸè¯„ä¼°å¤šå°ºåº¦ç±»åˆ«ç‰¹å®šæ³¨æ„åŠ›ç½‘ç»œ",
      "authors": [
        "Zhangding Liu",
        "Neda Mohammadi",
        "John E. Taylor"
      ],
      "abstract": "Rapid and accurate post-hurricane damage assessment is vital for disaster response and recovery. Yet existing CNN-based methods struggle to capture multi-scale spatial features and to distinguish visually similar or co-occurring damage types. To address these issues, we propose MCANet, a multi-label classification framework that learns multi-scale representations and adaptively attends to spatially relevant regions for each damage category. MCANet employs a Res2Net-based hierarchical backbone to enrich spatial context across scales and a multi-head class-specific residual attention module to enhance discrimination. Each attention branch focuses on different spatial granularities, balancing local detail with global context. We evaluate MCANet on the RescueNet dataset of 4,494 UAV images collected after Hurricane Michael. MCANet achieves a mean average precision (mAP) of 91.75%, outperforming ResNet, Res2Net, VGG, MobileNet, EfficientNet, and ViT. With eight attention heads, performance further improves to 92.35%, boosting average precision for challenging classes such as Road Blocked by over 6%. Class activation mapping confirms MCANet's ability to localize damage-relevant regions, supporting interpretability. Outputs from MCANet can inform post-disaster risk mapping, emergency routing, and digital twin-based disaster response. Future work could integrate disaster-specific knowledge graphs and multimodal large language models to improve adaptability to unseen disasters and enrich semantic understanding for real-world decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MCANetï¼Œä¸€ç§ä¸“ä¸ºæ— äººæœº(UAV)å›¾åƒå¤šæ ‡ç­¾é£“é£ç¾åæŸå®³è¯„ä¼°è®¾è®¡çš„åˆ†ç±»æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ç°æœ‰å·ç§¯ç¥ç»ç½‘ç»œ(CNN)åœ¨æ•æ‰å¤šå°ºåº¦ç©ºé—´ç‰¹å¾ä»¥åŠåŒºåˆ†è§†è§‰ç›¸ä¼¼æŸå®³ç±»å‹æ–¹é¢çš„å±€é™ï¼ŒMCANeté‡‡ç”¨äº†åŸºäºRes2Netçš„åˆ†å±‚ä¸»å¹²ç½‘ç»œæ¥ä¸°å¯Œå¤šå°ºåº¦ç©ºé—´ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†å¤šå¤´ç±»åˆ«ç‰¹å®šæ®‹å·®æ³¨æ„åŠ›æ¨¡å—(multi-head class-specific residual attention module)ï¼Œé€šè¿‡å¹³è¡¡å±€éƒ¨ç»†èŠ‚ä¸å…¨å±€ä¸Šä¸‹æ–‡ï¼Œä½¿ä¸åŒæ³¨æ„åŠ›åˆ†æ”¯èšç„¦äºç‰¹å®šç©ºé—´ç²’åº¦ä»¥å¢å¼ºè¾¨åˆ«èƒ½åŠ›ã€‚åœ¨RescueNetæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMCANetè¾¾åˆ°äº†91.75%çš„å¹³å‡ç²¾åº¦å‡å€¼(mAP)ï¼Œä¼˜äºResNetã€EfficientNetå’ŒViTç­‰åŸºçº¿æ¨¡å‹ã€‚å½“é…ç½®å…«ä¸ªæ³¨æ„åŠ›å¤´æ—¶ï¼Œå…¶æ€§èƒ½è¿›ä¸€æ­¥æå‡è‡³92.35%ï¼Œå¹¶æ˜¾è‘—æ”¹å–„äº†Road Blockedç­‰æŒ‘æˆ˜æ€§ç±»åˆ«çš„è¯†åˆ«æ•ˆæœã€‚ç±»æ¿€æ´»å›¾(Class activation mapping)è¯å®äº†æ¨¡å‹ç²¾å‡†å®šä½å—ç¾åŒºåŸŸçš„èƒ½åŠ›ï¼Œä¸ºç¾åé£é™©åˆ¶å›¾ã€åº”æ€¥è·¯ç”±åŠæ•°å­—å­ªç”Ÿåº”æ€¥å“åº”æä¾›äº†å¯é çš„å†³ç­–æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "34 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.04757v1",
      "published_date": "2025-09-05 02:25:05 UTC",
      "updated_date": "2025-09-05 02:25:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:52:18.885511+00:00"
    },
    {
      "arxiv_id": "2509.04753v1",
      "title": "A Study of Large Language Models for Patient Information Extraction: Model Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning",
      "title_zh": "é¢å‘æ‚£è€…ä¿¡æ¯æå–çš„å¤§è¯­è¨€æ¨¡å‹ç ”ç©¶ï¼šæ¨¡å‹æ¶æ„ã€å¾®è°ƒç­–ç•¥ä¸å¤šä»»åŠ¡æŒ‡ä»¤å¾®è°ƒ",
      "authors": [
        "Cheng Peng",
        "Xinyu Dong",
        "Mengxian Lyu",
        "Daniel Paredes",
        "Yaoyun Zhang",
        "Yonghui Wu"
      ],
      "abstract": "Natural language processing (NLP) is a key technology to extract important patient information from clinical narratives to support healthcare applications. The rapid development of large language models (LLMs) has revolutionized many NLP tasks in the clinical domain, yet their optimal use in patient information extraction tasks requires further exploration. This study examines LLMs' effectiveness in patient information extraction, focusing on LLM architectures, fine-tuning strategies, and multi-task instruction tuning techniques for developing robust and generalizable patient information extraction systems. This study aims to explore key concepts of using LLMs for clinical concept and relation extraction tasks, including: (1) encoder-only or decoder-only LLMs, (2) prompt-based parameter-efficient fine-tuning (PEFT) algorithms, and (3) multi-task instruction tuning on few-shot learning performance. We benchmarked a suite of LLMs, including encoder-based LLMs (BERT, GatorTron) and decoder-based LLMs (GatorTronGPT, Llama 3.1, GatorTronLlama), across five datasets. We compared traditional full-size fine-tuning and prompt-based PEFT. We explored a multi-task instruction tuning framework that combines both tasks across four datasets to evaluate the zero-shot and few-shot learning performance using the leave-one-dataset-out strategy.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models)åœ¨æ‚£è€…ä¿¡æ¯æå–(Patient Information Extraction)ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œé‡ç‚¹è¯„ä¼°äº†æ¨¡å‹æ¶æ„ã€å¾®è°ƒç­–ç•¥åŠå¤šä»»åŠ¡æŒ‡ä»¤å¾®è°ƒ(Multi-task Instruction Tuning)çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶å¯¹æ¯”äº†ä»…ç¼–ç å™¨(Encoder-only)æ¨¡å‹ï¼ˆå¦‚BERTã€GatorTronï¼‰ä¸ä»…è§£ç å™¨(Decoder-only)æ¨¡å‹ï¼ˆå¦‚GatorTronGPTã€Llama 3.1ã€GatorTronLlamaï¼‰åœ¨äº”ä¸ªä¸´åºŠæ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚å®éªŒåˆ†æäº†ä¼ ç»Ÿå…¨å‚æ•°å¾®è°ƒ(Full-size fine-tuning)ä¸åŸºäºæç¤ºçš„å‚æ•°é«˜æ•ˆå¾®è°ƒ(Parameter-efficient fine-tuning, PEFT)ç®—æ³•å¯¹ä¸´åºŠæ¦‚å¿µå’Œå…³ç³»æå–çš„å…·ä½“å½±å“ã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨ç•™ä¸€æ³•(Leave-one-dataset-out)ç­–ç•¥ï¼Œé€šè¿‡å¤šä»»åŠ¡æŒ‡ä»¤å¾®è°ƒæ¡†æ¶éªŒè¯äº†æ¨¡å‹åœ¨é›¶æ ·æœ¬(Zero-shot)å’Œå°‘æ ·æœ¬(Few-shot)å­¦ä¹ åœºæ™¯ä¸‹çš„æ³›åŒ–è¡¨ç°ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘é²æ£’ä¸”é€šç”¨çš„åŒ»ç–—ä¿¡æ¯æå–ç³»ç»Ÿæä¾›äº†å…³é”®çš„æŠ€æœ¯æŒ‡å¯¼ï¼Œæ˜ç¡®äº†åœ¨å¤„ç†ä¸´åºŠå™äº‹æ•°æ®æ—¶ä¼˜åŒ–ä½¿ç”¨LLMsçš„æœ€ä½³å®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04753v1",
      "published_date": "2025-09-05 02:07:40 UTC",
      "updated_date": "2025-09-05 02:07:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:52:26.693595+00:00"
    },
    {
      "arxiv_id": "2509.04752v1",
      "title": "SePA: A Search-enhanced Predictive Agent for Personalized Health Coaching",
      "title_zh": "SePAï¼šé¢å‘ä¸ªæ€§åŒ–å¥åº·æŒ‡å¯¼çš„æœç´¢å¢å¼ºå‹é¢„æµ‹æ™ºèƒ½ä½“",
      "authors": [
        "Melik Ozolcer",
        "Sang Won Bae"
      ],
      "abstract": "This paper introduces SePA (Search-enhanced Predictive AI Agent), a novel LLM health coaching system that integrates personalized machine learning and retrieval-augmented generation to deliver adaptive, evidence-based guidance. SePA combines: (1) Individualized models predicting daily stress, soreness, and injury risk from wearable sensor data (28 users, 1260 data points); and (2) A retrieval module that grounds LLM-generated feedback in expert-vetted web content to ensure contextual relevance and reliability. Our predictive models, evaluated with rolling-origin cross-validation and group k-fold cross-validation show that personalized models outperform generalized baselines. In a pilot expert study (n=4), SePA's retrieval-based advice was preferred over a non-retrieval baseline, yielding meaningful practical effect (Cliff's $Î´$=0.3, p=0.05). We also quantify latency performance trade-offs between response quality and speed, offering a transparent blueprint for next-generation, trustworthy personal health informatics systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SePA (Search-enhanced Predictive AI Agent)ï¼Œè¿™æ˜¯ä¸€ç§æ•´åˆäº†ä¸ªæ€§åŒ–æœºå™¨å­¦ä¹  (Personalized Machine Learning) å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval-Augmented Generation, RAG) çš„æ–°å‹å¤§è¯­è¨€æ¨¡å‹å¥åº·è¾…å¯¼ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®å»ºç«‹ä¸ªä½“åŒ–æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹æ¯æ—¥å‹åŠ›ã€è‚Œè‚‰é…¸ç—›åŠå—ä¼¤é£é™©ï¼Œå¹¶ç»“åˆæ£€ç´¢æ¨¡å—å°†åé¦ˆæ ¹æ¤äºä¸“å®¶å®¡æ ¸çš„ç½‘é¡µå†…å®¹ä¸­ï¼Œä»¥æå‡å»ºè®®çš„å¯é æ€§ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¿™ç§ä¸ªæ€§åŒ–æ¨¡å‹åœ¨é¢„æµ‹å‡†ç¡®ç‡ä¸Šä¼˜äºé€šç”¨åŸºå‡†æ¨¡å‹ã€‚åœ¨ä¸€é¡¹é’ˆå¯¹ä¸“å®¶çš„è¯•ç‚¹ç ”ç©¶ä¸­ï¼ŒSePA æä¾›çš„åŸºäºæ£€ç´¢çš„å»ºè®®ç›¸æ¯”éæ£€ç´¢åŸºå‡†æ›´å—é’çï¼Œå±•ç°å‡ºæ˜¾è‘—çš„å®é™…åº”ç”¨æ•ˆæœã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ·±å…¥æ¢è®¨äº†ç³»ç»Ÿåœ¨å“åº”è´¨é‡ä¸é€Ÿåº¦ä¹‹é—´çš„å»¶è¿Ÿæ€§èƒ½æƒè¡¡ï¼Œä¸ºæ„å»ºå¯ä¿¡çš„ä¸ªäººå¥åº·ä¿¡æ¯ç³»ç»Ÿæä¾›äº†é‡è¦çš„è®¾è®¡è“å›¾ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted at IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI'25). 7 pages, 5 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.04752v1",
      "published_date": "2025-09-05 02:07:36 UTC",
      "updated_date": "2025-09-05 02:07:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:52:52.287640+00:00"
    },
    {
      "arxiv_id": "2509.04735v1",
      "title": "Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A Dual Uncertainty-Aware Training Approach to SAM Optimization",
      "title_zh": "æå‡æ¶åŠ£å¤©æ°”ä¸‹çš„è‡ªåŠ¨é©¾é©¶åˆ†å‰²æ€§èƒ½ï¼šä¸€ç§é’ˆå¯¹ SAM ä¼˜åŒ–çš„åŒé‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥è®­ç»ƒæ–¹æ³•",
      "authors": [
        "Dharsan Ravindran",
        "Kevin Wang",
        "Zhuoyuan Cao",
        "Saleh Abdelrahman",
        "Jeffery Wu"
      ],
      "abstract": "Recent advances in vision foundation models, such as the Segment Anything Model (SAM) and its successor SAM2, have achieved state-of-the-art performance on general image segmentation benchmarks. However, these models struggle in adverse weather conditions where visual ambiguity is high, largely due to their lack of uncertainty quantification. Inspired by progress in medical imaging, where uncertainty-aware training has improved reliability in ambiguous cases, we investigate two approaches to enhance segmentation robustness for autonomous driving. First, we introduce a multi-step finetuning procedure for SAM2 that incorporates uncertainty metrics directly into the loss function, improving overall scene recognition. Second, we adapt the Uncertainty-Aware Adapter (UAT), originally designed for medical image segmentation, to driving contexts. We evaluate both methods on CamVid, BDD100K, and GTA driving datasets. Experiments show that UAT-SAM outperforms standard SAM in extreme weather, while SAM2 with uncertainty-aware loss achieves improved performance across diverse driving scenes. These findings underscore the value of explicit uncertainty modeling for safety-critical autonomous driving in challenging environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Segment Anything Model (SAM)åŠå…¶åç»­ç‰ˆæœ¬SAM2åœ¨æ¶åŠ£å¤©æ°”ä¸‹ç”±äºè§†è§‰æ­§ä¹‰é«˜ä¸”ç¼ºä¹ä¸ç¡®å®šæ€§é‡åŒ–è€Œè¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸¤ç§å¢å¼ºè‡ªåŠ¨é©¾é©¶å›¾åƒåˆ†å‰²é²æ£’æ€§çš„è®­ç»ƒæ–¹æ³•ã€‚ç ”ç©¶é¦–å…ˆä¸ºSAM2å¼•å…¥äº†ä¸€ç§å¤šæ­¥å¾®è°ƒ(finetuning)ç¨‹åºï¼Œå°†ä¸ç¡®å®šæ€§æŒ‡æ ‡(uncertainty metrics)ç›´æ¥æ•´åˆåˆ°æŸå¤±å‡½æ•°ä¸­ï¼Œä»¥æå‡æ•´ä½“åœºæ™¯è¯†åˆ«èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œç ”ç©¶å€Ÿé‰´åŒ»å­¦å½±åƒé¢†åŸŸçš„è¿›å±•ï¼Œå°†åŸæœ¬ç”¨äºåŒ»ç–—å›¾åƒåˆ†å‰²çš„Uncertainty-Aware Adapter (UAT)é€‚é…åˆ°è‡ªåŠ¨é©¾é©¶è¯­å¢ƒä¸­ã€‚åœ¨CamVidã€BDD100Kå’ŒGTAç­‰æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒUAT-SAMåœ¨æç«¯å¤©æ°”ä¸‹çš„è¡¨ç°ä¼˜äºæ ‡å‡†SAMï¼Œè€Œç»“åˆä¸ç¡®å®šæ€§æ„ŸçŸ¥æŸå¤±çš„SAM2åœ¨å¤šç§é©¾é©¶åœºæ™¯ä¸­å‡å®ç°äº†æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†æ˜¾å¼ä¸ç¡®å®šæ€§å»ºæ¨¡(explicit uncertainty modeling)å¯¹äºåœ¨æŒ‘æˆ˜æ€§ç¯å¢ƒä¸­ä¿éšœå®‰å…¨å…³é”®å‹è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå¯é æ€§çš„é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04735v1",
      "published_date": "2025-09-05 01:24:42 UTC",
      "updated_date": "2025-09-05 01:24:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:52:56.589919+00:00"
    },
    {
      "arxiv_id": "2509.04734v2",
      "title": "Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning",
      "title_zh": "Beyond I-Conï¼šæ¢ç´¢è¡¨ç¤ºå­¦ä¹ ä¸­è·ç¦»åº¦é‡çš„æ–°ç»´åº¦",
      "authors": [
        "Jasmine Shone",
        "Zhening Li",
        "Shaden Alshammari",
        "Mark Hamilton",
        "William Freeman"
      ],
      "abstract": "The Information Contrastive (I-Con) framework revealed that over 23 representation learning methods implicitly minimize KL divergence between data and learned distributions that encode similarities between data points. However, a KL-based loss may be misaligned with the true objective, and properties of KL divergence such as asymmetry and unboundedness may create optimization challenges. We present Beyond I-Con, a framework that enables systematic discovery of novel loss functions by exploring alternative statistical divergences. Key findings: (1) on unsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-art results by modifying the PMI algorithm to use total variation (TV) distance; (2) supervised contrastive learning with Euclidean distance as the feature space metric is improved by replacing the standard loss function with Jenson-Shannon divergence (JSD); (3) on dimensionality reduction, we achieve superior qualitative results and better performance on downstream tasks than SNE by replacing KL with a bounded $f$-divergence. Our results highlight the importance of considering divergence choices in representation learning optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Beyond I-Conæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³Information Contrastive (I-Con) æ¡†æ¶ä¸­ç”±äºä½¿ç”¨KL divergenceè€Œå¯¼è‡´çš„æŸå¤±å‡½æ•°å¤±é…ã€éå¯¹ç§°æ€§åŠæ— ç•Œæ€§ç­‰ä¼˜åŒ–æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¢ç´¢æ›¿ä»£çš„ç»Ÿè®¡æ•£åº¦ï¼ˆstatistical divergencesï¼‰ï¼Œå®ç°äº†å¯¹è¡¨å¾å­¦ä¹ ï¼ˆRepresentation Learningï¼‰ä¸­æ–°å‹æŸå¤±å‡½æ•°çš„ç³»ç»Ÿæ€§å‘ç°ã€‚åœ¨DINO-ViTåµŒå…¥çš„æ— ç›‘ç£èšç±»ä»»åŠ¡ä¸­ï¼Œé€šè¿‡å°†PMIç®—æ³•ä¿®æ”¹ä¸ºä½¿ç”¨Total Variation (TV) è·ç¦»ï¼Œç ”ç©¶è¾¾åˆ°äº†state-of-the-artçš„æ•ˆæœã€‚åœ¨æœ‰ç›‘ç£å¯¹æ¯”å­¦ä¹ ä¸­ï¼Œç ”ç©¶è¯æ˜å°†æ ‡å‡†çš„æŸå¤±å‡½æ•°æ›¿æ¢ä¸ºJenson-Shannon divergence (JSD)ï¼Œå¯ä»¥æœ‰æ•ˆæå‡ä»¥Euclidean distanceä½œä¸ºç‰¹å¾ç©ºé—´åº¦é‡çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨é™ç»´ä»»åŠ¡ä¸­ï¼Œé‡‡ç”¨æœ‰ç•Œçš„$f$-divergenceæ›¿ä»£KLæ•£åº¦ï¼Œè·å¾—äº†ä¼˜äºSNEçš„å®šæ€§ç»“æœå’Œæ›´ä½³çš„ä¸‹æ¸¸ä»»åŠ¡è¡¨ç°ã€‚å®éªŒç»“æœçªæ˜¾äº†æ•£åº¦é€‰æ‹©åœ¨è¡¨å¾å­¦ä¹ ä¼˜åŒ–ä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸ºè®¾è®¡æ›´ç¨³å¥çš„å­¦ä¹ ç›®æ ‡æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04734v2",
      "published_date": "2025-09-05 01:23:59 UTC",
      "updated_date": "2025-12-04 18:09:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:53:06.795325+00:00"
    },
    {
      "arxiv_id": "2509.04733v1",
      "title": "CoVeR: Conformal Calibration for Versatile and Reliable Autoregressive Next-Token Prediction",
      "title_zh": "CoVeRï¼šé¢å‘é€šç”¨ä¸”å¯é çš„è‡ªå›å½’ä¸‹ä¸€ Token é¢„æµ‹çš„ç¬¦åˆæ€§æ ¡å‡†",
      "authors": [
        "Yuzhu Chen",
        "Yingjie Wang",
        "Shunyu Liu",
        "Yongcheng Jing",
        "Dacheng Tao"
      ],
      "abstract": "Autoregressive pre-trained models combined with decoding methods have achieved impressive performance on complex reasoning tasks. While mainstream decoding strategies such as beam search can generate plausible candidate sets, they often lack provable coverage guarantees, and struggle to effectively balance search efficiency with the need for versatile trajectories, particularly those involving long-tail sequences that are essential in certain real-world applications. To address these limitations, we propose \\textsc{CoVeR}, a novel model-free decoding strategy wihtin the conformal prediction framework that simultaneously maintains a compact search space and ensures high coverage probability over desirable trajectories. Theoretically, we establish a PAC-style generalization bound, guaranteeing that \\textsc{CoVeR} asymptotically achieves a coverage rate of at least $1 - Î±$ for any target level $Î±\\in (0,1)$.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªå›å½’æ¨¡å‹(autoregressive models)åœ¨è§£ç è¿‡ç¨‹ä¸­ç¼ºä¹å¯è¯æ˜çš„è¦†ç›–ç‡ä¿è¯ä»¥åŠéš¾ä»¥å¹³è¡¡æœç´¢æ•ˆç‡ä¸è·¯å¾„å¤šæ ·æ€§ç­‰é—®é¢˜ï¼Œæå‡ºäº†CoVeRï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç¬¦åˆæ€§é¢„æµ‹(conformal prediction)æ¡†æ¶çš„æ–°å‹æ— æ¨¡å‹è§£ç ç­–ç•¥ã€‚CoVeRæ—¨åœ¨åŒæ—¶ç»´æŒç´§å‡‘çš„æœç´¢ç©ºé—´å¹¶ç¡®ä¿ç›®æ ‡è·¯å¾„å…·æœ‰è¾ƒé«˜çš„è¦†ç›–æ¦‚ç‡ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹åœ¨å®é™…åº”ç”¨ä¸­è‡³å…³é‡è¦ä½†åœ¨æŸæœç´¢(beam search)ä¸­éš¾ä»¥æ•è·çš„é•¿å°¾åºåˆ—ã€‚åœ¨ç†è®ºåˆ†æä¸Šï¼Œè¯¥ç ”ç©¶ç¡®ç«‹äº†PACé£æ ¼çš„æ³›åŒ–ç•Œé™(PAC-style generalization bound)ï¼Œä»æ•°å­¦ä¸Šè¯æ˜äº†CoVeRèƒ½å¤Ÿé’ˆå¯¹ä»»ä½•ç›®æ ‡æ°´å¹³$\\alpha$æ¸è¿‘åœ°å®ç°è‡³å°‘$1-\\alpha$çš„è¦†ç›–ç‡ã€‚è¿™ä¸€å·¥ä½œä¸ºå¼€å‘å…¼å…·å¤šåŠŸèƒ½æ€§ä¸å¯é æ€§çš„è‡ªå›å½’ä¸‹æ–‡é¢„æµ‹æŠ€æœ¯æä¾›äº†åšå®çš„ç†è®ºåŸºç¡€å’Œå…¨æ–°çš„è§£ç è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04733v1",
      "published_date": "2025-09-05 01:07:12 UTC",
      "updated_date": "2025-09-05 01:07:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:53:29.984380+00:00"
    },
    {
      "arxiv_id": "2509.04731v3",
      "title": "Generative World Models of Tasks: LLM-Driven Hierarchical Scaffolding for Embodied Agents",
      "title_zh": "ç”Ÿæˆå¼ä»»åŠ¡ä¸–ç•Œæ¨¡å‹ï¼šé¢å‘å…·èº«æ™ºèƒ½ä½“çš„å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨å±‚æ¬¡åŒ–è„šæ‰‹æ¶",
      "authors": [
        "Brennen Hill"
      ],
      "abstract": "Recent advances in agent development have focused on scaling model size and raw interaction data, mirroring successes in large language models. However, for complex, long-horizon multi-agent tasks such as robotic soccer, this end-to-end approach often fails due to intractable exploration spaces and sparse rewards. We propose that an effective world model for decision-making must model the world's physics and also its task semantics. A systematic review of 2024 research in low-resource multi-agent soccer reveals a clear trend towards integrating symbolic and hierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian Strategy Networks (BSNs), with multi-agent reinforcement learning (MARL). These methods decompose complex goals into manageable subgoals, creating an intrinsic curriculum that shapes agent learning. We formalize this trend into a framework for Hierarchical Task Environments (HTEs), which are essential for bridging the gap between simple, reactive behaviors and sophisticated, strategic team play. Our framework incorporates the use of Large Language Models (LLMs) as generative world models of tasks, capable of dynamically generating this scaffolding. We argue that HTEs provide a mechanism to guide exploration, generate meaningful learning signals, and train agents to internalize hierarchical structure, enabling the development of more capable and general-purpose agents with greater sample efficiency than purely end-to-end approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·èº«æ™ºèƒ½ä½“åœ¨æ‰§è¡Œæœºå™¨äººè¶³çƒç­‰å¤æ‚ã€é•¿æ—¶ç¨‹å¤šæ™ºèƒ½ä½“ä»»åŠ¡æ—¶ï¼Œç«¯åˆ°ç«¯æ–¹æ³•å› æ¢ç´¢ç©ºé—´è¿‡å¤§å’Œå¥–åŠ±ç¨€ç–è€Œå¤±æ•ˆçš„é—®é¢˜ï¼Œæå‡ºäº†å±‚æ¬¡åŒ–ä»»åŠ¡ç¯å¢ƒ(Hierarchical Task Environments, HTEs)æ¡†æ¶ã€‚ä½œè€…å¼ºè°ƒæœ‰æ•ˆçš„å†³ç­–ä¸–ç•Œæ¨¡å‹åº”åŒæ—¶æ¶µç›–ç‰©ç†è§„å¾‹ä¸ä»»åŠ¡è¯­ä¹‰(task semantics)ï¼Œå¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ä½œä¸ºç”Ÿæˆå¼ä»»åŠ¡æ¨¡å‹æ¥åŠ¨æ€æ„å»ºå¼•å¯¼æ”¯æ¶ã€‚è¯¥æ¡†æ¶å°†å±‚æ¬¡åŒ–ä»»åŠ¡ç½‘ç»œ(HTNs)å’Œè´å¶æ–¯ç­–ç•¥ç½‘ç»œ(BSNs)ç­‰ç¬¦å·åŒ–æ–¹æ³•ä¸å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (MARL)æ·±åº¦é›†æˆï¼Œé€šè¿‡ç›®æ ‡åˆ†è§£ä¸ºæ™ºèƒ½ä½“æ„å»ºå†…åœ¨å­¦ä¹ è¯¾ç¨‹ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒHTEsèƒ½å¤Ÿæœ‰æ•ˆå¼•å¯¼æ¢ç´¢å¹¶äº§ç”Ÿå…³é”®å­¦ä¹ ä¿¡å·ï¼Œå¸®åŠ©æ™ºèƒ½ä½“é€šè¿‡å†…éƒ¨åŒ–å±‚æ¬¡ç»“æ„å®ç°æ¯”ç«¯åˆ°ç«¯æ–¹æ³•æ›´é«˜çš„æ ·æœ¬æ•ˆç‡ã€‚è¿™ä¸€æ–¹æ³•æˆåŠŸå¼¥åˆäº†åŸºç¡€ååº”æ€§è¡Œä¸ºä¸é«˜çº§æˆ˜ç•¥åä½œä¹‹é—´çš„é¸¿æ²Ÿï¼Œä¸ºå¼€å‘æ›´å…·æ³›åŒ–èƒ½åŠ›çš„é€šç”¨å…·èº«æ™ºèƒ½ä½“æä¾›äº†ç³»ç»Ÿæ€§æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MA",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "In the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Embodied World Models for Decision Making (EWM)",
      "pdf_url": "https://arxiv.org/pdf/2509.04731v3",
      "published_date": "2025-09-05 01:03:51 UTC",
      "updated_date": "2025-11-04 06:25:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:53:10.687763+00:00"
    },
    {
      "arxiv_id": "2509.05382v1",
      "title": "User Privacy and Large Language Models: An Analysis of Frontier Developers' Privacy Policies",
      "title_zh": "ç”¨æˆ·éšç§ä¸å¤§è¯­è¨€æ¨¡å‹ï¼šå‰æ²¿å¼€å‘è€…éšç§æ”¿ç­–åˆ†æ",
      "authors": [
        "Jennifer King",
        "Kevin Klyman",
        "Emily Capstick",
        "Tiffany Saade",
        "Victoria Hsieh"
      ],
      "abstract": "Hundreds of millions of people now regularly interact with large language models via chatbots. Model developers are eager to acquire new sources of high-quality training data as they race to improve model capabilities and win market share. This paper analyzes the privacy policies of six U.S. frontier AI developers to understand how they use their users' chats to train models. Drawing primarily on the California Consumer Privacy Act, we develop a novel qualitative coding schema that we apply to each developer's relevant privacy policies to compare data collection and use practices across the six companies. We find that all six developers appear to employ their users' chat data to train and improve their models by default, and that some retain this data indefinitely. Developers may collect and train on personal information disclosed in chats, including sensitive information such as biometric and health data, as well as files uploaded by users. Four of the six companies we examined appear to include children's chat data for model training, as well as customer data from other products. On the whole, developers' privacy policies often lack essential information about their practices, highlighting the need for greater transparency and accountability. We address the implications of users' lack of consent for the use of their chat data for model training, data security issues arising from indefinite chat data retention, and training on children's chat data. We conclude by providing recommendations to policymakers and developers to address the data privacy challenges posed by LLM-powered chatbots.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ†æäº†å…­å®¶ç¾å›½å‰æ²¿äººå·¥æ™ºèƒ½å¼€å‘å•†ï¼ˆfrontier AI developersï¼‰çš„éšç§æ”¿ç­–ï¼Œæ—¨åœ¨æ¢ç©¶å…¶å¦‚ä½•åˆ©ç”¨ç”¨æˆ·å¯¹è¯æ•°æ®è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚ç ”ç©¶åŸºäºã€ŠåŠ å·æ¶ˆè´¹è€…éšç§æ³•æ¡ˆã€‹ï¼ˆCCPAï¼‰å¼€å‘äº†ä¸€ç§æ–°å‹å®šæ€§ç¼–ç æ–¹æ¡ˆï¼ˆqualitative coding schemaï¼‰ï¼Œå¯¹è¿™äº›å¼€å‘å•†çš„æ•°æ®æ”¶é›†å’Œä½¿ç”¨å®è·µè¿›è¡Œäº†å¯¹æ¯”åˆ†æã€‚ç ”ç©¶å‘ç°ï¼Œæ‰€æœ‰å…­å®¶å¼€å‘å•†é»˜è®¤éƒ½ä¼šä½¿ç”¨ç”¨æˆ·çš„å¯¹è¯æ•°æ®æ¥è®­ç»ƒå’Œæ”¹è¿›æ¨¡å‹ï¼Œä¸”éƒ¨åˆ†å…¬å¸ä¼šæ— é™æœŸä¿ç•™è¿™äº›æ•°æ®ï¼Œå…¶ä¸­æ¶µç›–äº†ç”Ÿç‰©è¯†åˆ«ã€å¥åº·æ•°æ®ç­‰ä¸ªäººæ•æ„Ÿä¿¡æ¯åŠç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶ã€‚æ­¤å¤–ï¼Œè°ƒæŸ¥æ˜¾ç¤ºæœ‰å››å®¶å…¬å¸å°†å„¿ç«¥çš„å¯¹è¯æ•°æ®åŠå…¶ä»–äº§å“çš„å®¢æˆ·æ•°æ®ç”¨äºæ¨¡å‹è®­ç»ƒã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå½“å‰çš„éšç§æ”¿ç­–æ™®éç¼ºä¹é€æ˜åº¦å’Œé—®è´£æœºåˆ¶ï¼Œå¹¶é’ˆå¯¹ç”¨æˆ·æˆæƒç¼ºå¤±ã€æ•°æ®ç•™å­˜å®‰å…¨éšæ‚£ä»¥åŠå„¿ç«¥éšç§ä¿æŠ¤ç­‰æŒ‘æˆ˜ï¼Œå‘æ”¿ç­–åˆ¶å®šè€…å’Œå¼€å‘å•†æå‡ºäº†ä¸“ä¸šå»ºè®®ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CY",
      "comment": "See additional files for appendices",
      "pdf_url": "https://arxiv.org/pdf/2509.05382v1",
      "published_date": "2025-09-05 01:01:21 UTC",
      "updated_date": "2025-09-05 01:01:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:53:12.998062+00:00"
    },
    {
      "arxiv_id": "2509.04716v1",
      "title": "KERAG: Knowledge-Enhanced Retrieval-Augmented Generation for Advanced Question Answering",
      "title_zh": "KERAGï¼šé¢å‘é«˜çº§é—®ç­”çš„çŸ¥è¯†å¢å¼ºæ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Yushi Sun",
        "Kai Sun",
        "Yifan Ethan Xu",
        "Xiao Yang",
        "Xin Luna Dong",
        "Nan Tang",
        "Lei Chen"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) mitigates hallucination in Large Language Models (LLMs) by incorporating external data, with Knowledge Graphs (KGs) offering crucial information for question answering. Traditional Knowledge Graph Question Answering (KGQA) methods rely on semantic parsing, which typically retrieves knowledge strictly necessary for answer generation, thus often suffer from low coverage due to rigid schema requirements and semantic ambiguity. We present KERAG, a novel KG-based RAG pipeline that enhances QA coverage by retrieving a broader subgraph likely to contain relevant information. Our retrieval-filtering-summarization approach, combined with fine-tuned LLMs for Chain-of-Thought reasoning on knowledge sub-graphs, reduces noises and improves QA for both simple and complex questions. Experiments demonstrate that KERAG surpasses state-of-the-art solutions by about 7% in quality and exceeds GPT-4o (Tool) by 10-21%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†KERAGï¼Œä¸€ç§æ–°å‹çš„åŸºäºçŸ¥è¯†å›¾è°±(Knowledge Graph, KG)çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)æµæ°´çº¿ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„å¹»è§‰é—®é¢˜åŠä¼ ç»ŸKGQAæ–¹æ³•å› ä¸¥è‹›çš„æ¨¡å¼è¦æ±‚å’Œè¯­ä¹‰æ¨¡ç³Šå¯¼è‡´çš„ä½è¦†ç›–ç‡é—®é¢˜ã€‚KERAGé€šè¿‡æ£€ç´¢å¯èƒ½åŒ…å«ç›¸å…³ä¿¡æ¯çš„æ›´å¹¿æ³›å­å›¾ï¼Œå¹¶é‡‡ç”¨æ£€ç´¢-è¿‡æ»¤-æ‘˜è¦(retrieval-filtering-summarization)çš„æ–¹æ³•ï¼Œæ˜¾è‘—å¢å¼ºäº†é—®ç­”ä»»åŠ¡çš„çŸ¥è¯†è¦†ç›–èŒƒå›´ã€‚è¯¥æ¡†æ¶è¿›ä¸€æ­¥ç»“åˆäº†ç»è¿‡å¾®è°ƒçš„LLMsï¼Œåœ¨çŸ¥è¯†å­å›¾ä¸Šæ‰§è¡Œé“¾å¼æ€ç»´(Chain-of-Thought, CoT)æ¨ç†ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘å™ªå£°å¹¶æå‡äº†å¤„ç†ç®€å•åŠå¤æ‚é—®é¢˜çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒKERAGåœ¨é—®ç­”è´¨é‡ä¸Šæ¯”ç°æœ‰å…ˆè¿›æ–¹æ¡ˆæé«˜äº†çº¦7%ï¼Œä¸”åœ¨æ€§èƒ½ä¸Šè¶…è¿‡äº†GPT-4o (Tool) 10-21%ï¼Œè¯æ˜äº†å…¶åœ¨é«˜çº§é—®ç­”ç³»ç»Ÿä¸­çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by EMNLP Findings 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.04716v1",
      "published_date": "2025-09-05 00:06:00 UTC",
      "updated_date": "2025-09-05 00:06:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T16:53:15.793079+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 107,
  "processed_papers_count": 107,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T16:54:22.350397+00:00"
}
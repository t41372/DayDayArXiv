{
  "date": "2025-03-25",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-25 的 arXiv 中文 TLDR 快报！\n\n今天的 arXiv 论文主要聚焦于 AI 模型优化、多模态理解和实际应用创新，强调大型语言模型（LLM）的微调、零样本生成以及多代理强化学习系统；令人印象深刻的包括 Gemma 3 报告（Google DeepMind 团队发布，展示了多模态 LLM 的进展）和多模态视觉语言模型在遥感领域的应用，这些论文突显了 AI 在医学、图像处理和机器人领域的潜力。\n\n### 重点论文讨论\n我们挑选了最具影响力和话题度的论文，先从 LLM 优化和多模态模型入手，这些领域有显著创新和知名团队贡献。接下来，快速概述图像处理、强化学习和应用领域的相关工作。\n\n**LLM 优化与多模态模型（高影响力领域）**  \n- **Fine-tuning Transfer / 微调转移优化** (Efficient Model Development through Fine-tuning Transfer)  \n  作者包括 Tu Vu，这篇论文提出了一种微调更新转移方法，利用差分向量在不同模型版本间重用微调数据，避免重复训练。核心贡献是显著提升模型性能（如在 Llama 系列上提高 10.7% 准确率），并证明源目标模型在参数空间线性连接时效果最佳，为 LLM 高效开发提供新策略。\n\n- **Post-Training Preference Alignment / 训练后偏好对齐** (Direct Post-Training Preference Alignment for Multi-Agent Motion Generation Models Using Implicit Feedback from Pre-training Demonstrations)  \n  作者 Ran Tian 和 Kratarth Goel 的工作利用预训练演示隐式反馈改进多代理运动生成模型的偏好对齐。发现通过构建更细致的偏好排名（而非简单视所有生成样本为负面），模型在交通模拟中提升了真实性，实现与大型模型相当的性能，而无需额外标注。\n\n- **Multi-modal LLMs for Deepfake Detection / 多模态 LLM 用于深度伪造检测** (Can Multi-modal (reasoning) LLMs work as deepfake detectors?)  \n  这篇论文测试了多种多模态 LLM（如 GPT-4o 和 Gemini）在深度伪造图像检测中的表现。关键发现是某些 LLM（如 GPT-4o）在零样本场景下超越传统方法，但模型规模和推理能力并非决定性因素，强调了多模态推理在鲁棒性检测中的潜力。\n\n- **Representation Engineering / 表示工程理论** (Why Representation Engineering Works: A Theoretical and Empirical Study in Vision-Language Models)  \n  作者 Bowei Tian 等构建了视觉语言模型的表示工程框架，证明了神经活动稳定性的理论基础。贡献包括将表示工程从描述工具转化为结构化框架，提升 AI 的鲁棒性和透明度。\n\n- **Gemma 3 Technical Report / Gemma 3 技术报告** (Gemma 3 Technical Report)  \n  Google DeepMind 团队发布，介绍了 Gemma 3 模型的多模态能力、语言覆盖和长上下文处理。核心发现是新架构减少了 KV-缓存内存，同时在多任务上超越前代，标志着开源 LLM 在多模态领域的重大进展。\n\n- **OmniNova / 通用多模态代理框架** (OmniNova: A General Multimodal Agent Framework)  \n  这篇论文提出一个分层多代理框架，结合 LLM 和工具（如网页搜索），优化任务路由。发现它在复杂任务中提升了完成率（87% vs. 62%），并减少了 token 使用，展示了多代理系统的实用性。\n\n这些 LLM 相关论文突出了模型微调和多模态融合的趋势，强调了效率和泛化能力的提升，尤其在零样本和实际应用中。\n\n**图像处理与生成（创新性强，相关联讨论）**  \n- **Zero-Shot HOI Synthesis / 零样本人-物交互合成** (Zero-Shot Human-Object Interaction Synthesis with Multimodal Priors)  \n  论文利用多模态模型实现零样本 3D 人-物交互合成。贡献是通过预训练模型生成物理真实和语义多样的交互序列，实验证明了其在虚拟现实中的潜力。\n\n- **LEGO-Puzzles / LEGO 拼图基准** (LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?)  \n  作者评估多模态 LLM 在多步空间推理中的表现。发现现有模型仅正确一半测试案例，人类准确率超 90%，强调了 LLM 在空间任务中的局限性。\n\n- **Deepfake Detection / 深度伪造检测** (OpenSDI: Spotting Diffusion-Generated Images in the Open World)  \n  工作提出新基准和模型，检测扩散生成图像。关键发现是通过提示和注意力机制提升泛化能力，相对改进 14%。\n\n图像生成和检测论文展示了多模态模型在视觉任务中的进展，但也暴露了空间推理的挑战。\n\n**强化学习与代理系统（话题度高）**  \n- **Multi-Agent Motion / 多代理运动生成** (如上文提到的 Post-Training Preference Alignment)  \n  扩展到强化学习，论文强调隐式反馈在多代理环境中的作用。\n\n- **Federated Graph Learning / 联邦图学习** (Data-centric Federated Graph Learning with Large Language Models)  \n  利用 LLM 增强联邦学习，解决数据异质性。发现生成缺失节点和边后，模型在图任务中显著提升。\n\n这些强化学习论文展示了多代理系统的潜力，尤其在动态环境中。\n\n**医学与应用领域（实际影响显著，快速掠过）**  \n- **AI in Maternal Health / AI 在孕产健康干预** (LLM-based Agent Simulation for Maternal Health Interventions)  \n  使用 LLM 模拟健康干预，贡献是通过不确定性估计改善决策，支持数据稀缺场景。\n\n- **Surgical Segmentation / 手术器械分割** (RP-SAM2: Refining Point Prompts for Stable Surgical Instrument Segmentation)  \n  改进 SAM2 模型，提升白内障手术分割准确性（mDSC 提升 2%）。\n\n其他应用论文，如遥感和健康AI，展示了 LLM 在实际中的扩展，但细节较常规，我们仅简要提及。\n\n其他论文，如纯理论工作（e.g., Dynamics of Structured Complex-Valued Hopfield Neural Networks）或小众基准（e.g., ACVUBench），虽有贡献但影响力有限，故快速掠过。\n\n总之，今天的论文强调了 AI 模型的实用性和鲁棒性，Gemma 3 等工作值得关注，为未来应用提供了新方向。更多细节可查阅 arXiv！",
  "papers": [
    {
      "arxiv_id": "2503.20118v1",
      "title": "Zero-Shot Human-Object Interaction Synthesis with Multimodal Priors",
      "title_zh": "零样本人类-物体交互合成",
      "authors": [
        "Yuke Lou",
        "Yiming Wang",
        "Zhen Wu",
        "Rui Zhao",
        "Wenjia Wang",
        "Mingyi Shi",
        "Taku Komura"
      ],
      "abstract": "Human-object interaction (HOI) synthesis is important for various\napplications, ranging from virtual reality to robotics. However, acquiring 3D\nHOI data is challenging due to its complexity and high cost, limiting existing\nmethods to the narrow diversity of object types and interaction patterns in\ntraining datasets. This paper proposes a novel zero-shot HOI synthesis\nframework without relying on end-to-end training on currently limited 3D HOI\ndatasets. The core idea of our method lies in leveraging extensive HOI\nknowledge from pre-trained Multimodal Models. Given a text description, our\nsystem first obtains temporally consistent 2D HOI image sequences using image\nor video generation models, which are then uplifted to 3D HOI milestones of\nhuman and object poses. We employ pre-trained human pose estimation models to\nextract human poses and introduce a generalizable category-level 6-DoF\nestimation method to obtain the object poses from 2D HOI images. Our estimation\nmethod is adaptive to various object templates obtained from text-to-3D models\nor online retrieval. A physics-based tracking of the 3D HOI kinematic milestone\nis further applied to refine both body motions and object poses, yielding more\nphysically plausible HOI generation results. The experimental results\ndemonstrate that our method is capable of generating open-vocabulary HOIs with\nphysical realism and semantic diversity.",
      "tldr_zh": "本文提出了一种zero-shot Human-Object Interaction (HOI) 合成框架，利用预训练的Multimodal Models的先验知识，避免依赖于有限的3D HOI数据集的端到端训练。框架从文本描述生成时间一致的2D HOI图像序列，然后通过预训练的人体姿势估计模型提取人体姿势，并引入通用的类别级6-DoF估计方法来获取物体姿势，以适应各种物体模板。接着，应用基于物理的跟踪技术细化3D HOI运动，确保生成结果的物理真实性和语义多样性。实验结果显示，该方法能够生成开放词汇的HOI，具有较高的物理真实性和多样性。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20118v1",
      "published_date": "2025-03-25 23:55:47 UTC",
      "updated_date": "2025-03-25 23:55:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:51:47.495949"
    },
    {
      "arxiv_id": "2503.20110v1",
      "title": "Efficient Model Development through Fine-tuning Transfer",
      "title_zh": "翻译失败",
      "authors": [
        "Pin-Jie Lin",
        "Rishab Balasubramanian",
        "Fengyuan Liu",
        "Nikhil Kandpal",
        "Tu Vu"
      ],
      "abstract": "Modern LLMs struggle with efficient updates, as each new pretrained model\nversion requires repeating expensive alignment processes. This challenge also\napplies to domain- or language-specific models, where fine-tuning on\nspecialized data must be redone for every new base model release. In this\npaper, we explore the transfer of fine-tuning updates between model versions.\nSpecifically, we derive the diff vector from one source model version, which\nrepresents the weight changes from fine-tuning, and apply it to the base model\nof a different target version. Through empirical evaluations on various\nopen-weight model versions, we show that transferring diff vectors can\nsignificantly improve the target base model, often achieving performance\ncomparable to its fine-tuned counterpart. For example, reusing the fine-tuning\nupdates from Llama 3.0 8B leads to an absolute accuracy improvement of 10.7% on\nGPQA over the base Llama 3.1 8B without additional training, surpassing Llama\n3.1 8B Instruct. In a multilingual model development setting, we show that this\napproach can significantly increase performance on target-language tasks\nwithout retraining, achieving an absolute improvement of 4.7% and 15.5% on\nGlobal MMLU for Malagasy and Turkish, respectively, compared to Llama 3.1 8B\nInstruct. Our controlled experiments reveal that fine-tuning transfer is most\neffective when the source and target models are linearly connected in the\nparameter space. Additionally, we demonstrate that fine-tuning transfer offers\na stronger and more computationally efficient starting point for further\nfine-tuning. Finally, we propose an iterative recycling-then-finetuning\napproach for continuous model development, which improves both efficiency and\neffectiveness. Our findings suggest that fine-tuning transfer is a viable\nstrategy to reduce training costs while maintaining model performance.",
      "tldr_zh": "本文提出一种通过 fine-tuning 转移来提升大型语言模型 (LLMs) 开发效率的方法，具体涉及从源模型提取 fine-tuning 的 diff vector，并将其应用到目标模型基版本上，以避免重复昂贵的训练过程。实验结果显示，这种转移在各种开源模型上显著改善性能，例如从 Llama 3.0 8B 转移到 Llama 3.1 8B，可在 GPQA 上提升 10.7% 准确率，并超越 Llama 3.1 8B Instruct；在多语言任务中，Global MMLU 的 Malagasy 和 Turkish 性能分别提升 4.7% 和 15.5%。研究进一步发现，fine-tuning 转移在源和目标模型参数空间线性连接时最有效，并提出迭代的回收-then-finetuning 策略，作为减少训练成本的同时维持模型性能的可行方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 4 figures, 13 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.20110v1",
      "published_date": "2025-03-25 23:24:43 UTC",
      "updated_date": "2025-03-25 23:24:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:52:00.635076"
    },
    {
      "arxiv_id": "2503.20105v1",
      "title": "Direct Post-Training Preference Alignment for Multi-Agent Motion Generation Models Using Implicit Feedback from Pre-training Demonstrations",
      "title_zh": "翻译失败",
      "authors": [
        "Ran Tian",
        "Kratarth Goel"
      ],
      "abstract": "Recent advancements in LLMs have revolutionized motion generation models in\nembodied applications. While LLM-type auto-regressive motion generation models\nbenefit from training scalability, there remains a discrepancy between their\ntoken prediction objectives and human preferences. As a result, models\npre-trained solely with token-prediction objectives often generate behaviors\nthat deviate from what humans would prefer, making post-training preference\nalignment crucial for producing human-preferred motions. Unfortunately,\npost-training alignment requires extensive preference rankings of motions\ngenerated by the pre-trained model, which are costly to annotate, especially in\nmulti-agent settings. Recently, there has been growing interest in leveraging\npre-training demonstrations to scalably generate preference data for\npost-training alignment. However, these methods often adopt an adversarial\nassumption, treating all pre-trained model-generated samples as unpreferred\nexamples. This adversarial approach overlooks the valuable signal provided by\npreference rankings among the model's own generations, ultimately reducing\nalignment effectiveness and potentially leading to misaligned behaviors. In\nthis work, instead of treating all generated samples as equally bad, we\nleverage implicit preferences encoded in pre-training demonstrations to\nconstruct preference rankings among the pre-trained model's generations,\noffering more nuanced preference alignment guidance with zero human cost. We\napply our approach to large-scale traffic simulation and demonstrate its\neffectiveness in improving the realism of pre-trained model's generated\nbehaviors, making a lightweight 1M motion generation model comparable to SOTA\nlarge imitation-based models by relying solely on implicit feedback from\npre-training demonstrations, without additional post-training human preference\nannotations or high computational costs.",
      "tldr_zh": "该研究针对多智能体运动生成模型（multi-agent motion generation models）的问题，提出了一种直接后训练偏好对齐（direct post-training preference alignment）方法，利用预训练演示（pre-training demonstrations）中的隐式反馈（implicit feedback）来构建模型生成样本之间的偏好排名，从而避免了传统方法的对抗性假设和人类标注成本。不同于以往将所有预训练样本视为不佳，该方法提供更细致的偏好指导，提升了模型对人类偏好的适应性。在大型交通模拟应用中，实验证明，该方法使一个轻量级1M模型的表现可媲美最先进的大型模仿模型，仅依赖预训练数据，而无需额外计算开销或人工标注。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "ICLR 2025 Spotlight",
      "pdf_url": "http://arxiv.org/pdf/2503.20105v1",
      "published_date": "2025-03-25 23:02:13 UTC",
      "updated_date": "2025-03-25 23:02:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:52:10.980568"
    },
    {
      "arxiv_id": "2503.20099v1",
      "title": "AI Identity, Empowerment, and Mindfulness in Mitigating Unethical AI Use",
      "title_zh": "翻译失败",
      "authors": [
        "Mayssam Tarighi Shaayesteh",
        "Sara Memarian Esfahani",
        "Hossein Mohit"
      ],
      "abstract": "This study examines how AI identity influences psychological empowerment and\nunethical AI behavior among college students, while also exploring the\nmoderating role of IT mindfulness. Findings show that a strong AI identity\nenhances psychological empowerment and academic engagement but can also lead to\nincreased unethical AI practices. Crucially, IT mindfulness acts as an ethical\nsafeguard, promoting sensitivity to ethical concerns and reducing misuse of AI.\nThese insights have implications for educators, policymakers, and AI\ndevelopers, emphasizing For Peer Review the need for a balanced approach that\nencourages digital engagement without compromising student responsibility. The\nstudy also contributes to philosophical discussions of psychological agency,\nsuggesting that empowerment through AI can yield both positive and negative\noutcomes. Mindfulness emerges as essential in guiding ethical AI interactions.\nOverall, the research informs ongoing debates on ethics in education and AI,\noffering strategies to align technological advancement with ethical\naccountability and responsible use.",
      "tldr_zh": "这篇研究探讨了 AI identity 如何影响大学生的 psychological empowerment 和不道德 AI 使用，同时考察 IT mindfulness 的调节作用。结果显示，强 AI identity 可提升 psychological empowerment 和学术参与，但也可能增加不道德 AI 实践；IT mindfulness 作为道德保障，能增强对伦理问题的敏感性和减少 AI 误用。该研究为教育者、政策制定者和 AI 开发者提供启示，强调平衡数字参与与责任感的重要性，并贡献于 psychological agency 的哲学讨论，以促进伦理 AI 互动。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20099v1",
      "published_date": "2025-03-25 22:36:21 UTC",
      "updated_date": "2025-03-25 22:36:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:52:23.103031"
    },
    {
      "arxiv_id": "2503.20084v2",
      "title": "Can Multi-modal (reasoning) LLMs work as deepfake detectors?",
      "title_zh": "翻译失败",
      "authors": [
        "Simiao Ren",
        "Yao Yao",
        "Kidus Zewde",
        "Zisheng Liang",
        "Tsang",
        "Ng",
        "Ning-Yau Cheng",
        "Xiaoou Zhan",
        "Qinzhe Liu",
        "Yifei Chen",
        "Hengwei Xu"
      ],
      "abstract": "Deepfake detection remains a critical challenge in the era of advanced\ngenerative models, particularly as synthetic media becomes more sophisticated.\nIn this study, we explore the potential of state of the art multi-modal\n(reasoning) large language models (LLMs) for deepfake image detection such as\n(OpenAI O1/4o, Gemini thinking Flash 2, Deepseek Janus, Grok 3, llama 3.2, Qwen\n2/2.5 VL, Mistral Pixtral, Claude 3.5/3.7 sonnet) . We benchmark 12 latest\nmulti-modal LLMs against traditional deepfake detection methods across multiple\ndatasets, including recently published real-world deepfake imagery. To enhance\nperformance, we employ prompt tuning and conduct an in-depth analysis of the\nmodels' reasoning pathways to identify key contributing factors in their\ndecision-making process. Our findings indicate that best multi-modal LLMs\nachieve competitive performance with promising generalization ability with zero\nshot, even surpass traditional deepfake detection pipelines in\nout-of-distribution datasets while the rest of the LLM families performs\nextremely disappointing with some worse than random guess. Furthermore, we\nfound newer model version and reasoning capabilities does not contribute to\nperformance in such niche tasks of deepfake detection while model size do help\nin some cases. This study highlights the potential of integrating multi-modal\nreasoning in future deepfake detection frameworks and provides insights into\nmodel interpretability for robustness in real-world scenarios.",
      "tldr_zh": "本文探讨了state-of-the-art multi-modal (reasoning) LLMs（如OpenAI O1/4o、Gemini等）在deepfake图像检测中的潜力，通过基准测试12个最新multi-modal LLMs与传统方法在多个数据集上的表现。研究采用prompt tuning并分析模型的推理路径，以识别决策关键因素。结果显示，最优秀的multi-modal LLMs在zero-shot设置下表现出色，甚至在out-of-distribution数据集上超越传统方法，而其他LLMs表现极差，有些不如随机猜测。此外，研究发现模型版本和推理能力对deepfake检测贡献有限，模型大小在某些情况下有益，并强调了在未来deepfake检测框架中整合multi-modal推理的潜力，以提升模型可解释性和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20084v2",
      "published_date": "2025-03-25 21:47:29 UTC",
      "updated_date": "2025-03-29 19:19:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:52:35.699986"
    },
    {
      "arxiv_id": "2503.20078v1",
      "title": "Abstracting Geo-specific Terrains to Scale Up Reinforcement Learning",
      "title_zh": "抽象特定地理地形以扩展强化学习规模",
      "authors": [
        "Volkan Ustun",
        "Soham Hans",
        "Rajay Kumar",
        "Yunzhe Wang"
      ],
      "abstract": "Multi-agent reinforcement learning (MARL) is increasingly ubiquitous in\ntraining dynamic and adaptive synthetic characters for interactive simulations\non geo-specific terrains. Frameworks such as Unity's ML-Agents help to make\nsuch reinforcement learning experiments more accessible to the simulation\ncommunity. Military training simulations also benefit from advances in MARL,\nbut they have immense computational requirements due to their complex,\ncontinuous, stochastic, partially observable, non-stationary, and\ndoctrine-based nature. Furthermore, these simulations require geo-specific\nterrains, further exacerbating the computational resources problem. In our\nresearch, we leverage Unity's waypoints to automatically generate multi-layered\nrepresentation abstractions of the geo-specific terrains to scale up\nreinforcement learning while still allowing the transfer of learned policies\nbetween different representations. Our early exploratory results on a novel\nMARL scenario, where each side has differing objectives, indicate that\nwaypoint-based navigation enables faster and more efficient learning while\nproducing trajectories similar to those taken by expert human players in CSGO\ngaming environments. This research points out the potential of waypoint-based\nnavigation for reducing the computational costs of developing and training MARL\nmodels for military training simulations, where geo-specific terrains and\ndiffering objectives are crucial.",
      "tldr_zh": "这篇论文针对多智能体强化学习(MARL) 在地理特定地形(geo-specific terrains)上的计算资源挑战，提出了一种方法，通过Unity的waypoints自动生成多层表示抽象，以扩展强化学习规模并实现策略在不同表示间的转移。实验在一种新型MARL场景中显示，该方法显著加速了学习过程，并产生了类似于CSGO专家玩家的轨迹，证明其在处理复杂、随机环境中的高效性。该研究为军事训练模拟等应用提供了降低计算成本的潜在解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 6 figures, 2024 Interservice/Industry Training, Simulation,\n  and Education Conference (I/ITSEC)",
      "pdf_url": "http://arxiv.org/pdf/2503.20078v1",
      "published_date": "2025-03-25 21:29:49 UTC",
      "updated_date": "2025-03-25 21:29:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:52:47.075011"
    },
    {
      "arxiv_id": "2503.20074v2",
      "title": "Adaptive Orchestration for Large-Scale Inference on Heterogeneous Accelerator Systems Balancing Cost, Performance, and Resilience",
      "title_zh": "平衡成本、性能和弹性的自",
      "authors": [
        "Yahav Biran",
        "Imry Kissos"
      ],
      "abstract": "The surge in generative AI workloads has created a need for scalable\ninference systems that can flexibly harness both GPUs and specialized\naccelerators while containing operational costs. This paper proposes a\nhardware-agnostic control loop that adaptively allocates requests across\nheterogeneous accelerators based on real-time cost and capacity signals. The\napproach sustains low latency and high throughput by dynamically shifting\nbetween cost-optimized and capacity-optimized modes, ensuring the most\nefficient use of expensive compute resources under fluctuating availability.\nEvaluated using the Stable Diffusion model, the framework consistently meets\nlatency targets, automatically redirects traffic during capacity shortfalls,\nand capitalizes on lower-cost accelerators when possible. These results\nhighlight how a feedback-driven deployment strategy, spanning the entire\nsoftware and hardware stack, can help organizations efficiently scale\ngenerative AI workloads while maintaining resilience in the face of limited\naccelerator capacity.",
      "tldr_zh": "该论文提出了一种硬件无关的控制循环（hardware-agnostic control loop），用于在异构加速器系统中自适应地分配大规模AI推理请求，从而平衡成本、性能和弹性。方法通过动态切换成本优化和容量优化模式，根据实时成本和容量信号高效分配请求到GPUs或其他专用加速器，确保在资源波动时维持低延迟和高吞吐量。在使用Stable Diffusion模型的实验中，该框架成功满足延迟目标、自动重定向流量以应对容量短缺，并利用低成本加速器，证明了反馈驱动的部署策略能帮助组织高效扩展generative AI工作负载并提升系统弹性。",
      "categories": [
        "cs.PF",
        "cs.AI",
        "68U01"
      ],
      "primary_category": "cs.PF",
      "comment": "14 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.20074v2",
      "published_date": "2025-03-25 21:20:11 UTC",
      "updated_date": "2025-03-27 17:16:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:52:59.757754"
    },
    {
      "arxiv_id": "2503.22720v1",
      "title": "Why Representation Engineering Works: A Theoretical and Empirical Study in Vision-Language Models",
      "title_zh": "为什么表示工程有效：在视觉语言模型中的理论与实证研究",
      "authors": [
        "Bowei Tian",
        "Xuntao Lyu",
        "Meng Liu",
        "Hongyi Wang",
        "Ang Li"
      ],
      "abstract": "Representation Engineering (RepE) has emerged as a powerful paradigm for\nenhancing AI transparency by focusing on high-level representations rather than\nindividual neurons or circuits. It has proven effective in improving\ninterpretability and control, showing that representations can emerge,\npropagate, and shape final model outputs in large language models (LLMs).\nHowever, in Vision-Language Models (VLMs), visual input can override factual\nlinguistic knowledge, leading to hallucinated responses that contradict\nreality. To address this challenge, we make the first attempt to extend RepE to\nVLMs, analyzing how multimodal representations are preserved and transformed.\nBuilding on our findings and drawing inspiration from successful RepE\napplications, we develop a theoretical framework that explains the stability of\nneural activity across layers using the principal eigenvector, uncovering the\nunderlying mechanism of RepE. We empirically validate these instrinsic\nproperties, demonstrating their broad applicability and significance. By\nbridging theoretical insights with empirical validation, this work transforms\nRepE from a descriptive tool into a structured theoretical framework, opening\nnew directions for improving AI robustness, fairness, and transparency.",
      "tldr_zh": "本研究探讨了 Representation Engineering (RepE) 在 Vision-Language Models (VLMs) 中的有效性，首次扩展 RepE 以解决视觉输入可能覆盖事实语言知识导致的幻觉问题。作者开发了一个理论框架，利用 principal eigenvector 解释神经活动在层间的稳定性，并通过实证验证证实了这些内在属性的广泛适用性。通过将 RepE 从描述性工具转变为结构化框架，该工作为提升 AI 的稳健性、公平性和透明度开辟了新方向。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22720v1",
      "published_date": "2025-03-25 20:32:15 UTC",
      "updated_date": "2025-03-25 20:32:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:53:11.036931"
    },
    {
      "arxiv_id": "2503.22719v1",
      "title": "LLM-based Agent Simulation for Maternal Health Interventions: Uncertainty Estimation and Decision-focused Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Sarah Martinson",
        "Lingkai Kong",
        "Cheol Woo Kim",
        "Aparna Taneja",
        "Milind Tambe"
      ],
      "abstract": "Agent-based simulation is crucial for modeling complex human behavior, yet\ntraditional approaches require extensive domain knowledge and large datasets.\nIn data-scarce healthcare settings where historic and counterfactual data are\nlimited, large language models (LLMs) offer a promising alternative by\nleveraging broad world knowledge. This study examines an LLM-driven simulation\nof a maternal mobile health program, predicting beneficiaries' listening\nbehavior when they receive health information via automated messages (control)\nor live representatives (intervention). Since uncertainty quantification is\ncritical for decision-making in health interventions, we propose an LLM\nepistemic uncertainty estimation method based on binary entropy across multiple\nsamples. We enhance model robustness through ensemble approaches, improving F1\nscore and model calibration compared to individual models. Beyond direct\nevaluation, we take a decision-focused approach, demonstrating how LLM\npredictions inform intervention feasibility and trial implementation in\ndata-limited settings. The proposed method extends to public health, disaster\nresponse, and other domains requiring rapid intervention assessment under\nsevere data constraints. All code and prompts used for this work can be found\nat https://github.com/sarahmart/LLM-ABS-ARMMAN-prediction.",
      "tldr_zh": "这篇论文提出了一种基于大型语言模型（LLM）的代理模拟方法，用于母性健康干预场景，旨在解决传统方法对大量数据和领域知识的依赖问题。该方法模拟受益者对健康信息（自动消息控制组 vs. 直播代表干预组）的收听行为，并引入一种基于二元熵的LLM认识不确定性估计（epistemic uncertainty estimation），通过集成方法提升模型鲁棒性，提高F1分数和校准性能。研究采用决策导向评估（decision-focused evaluation），展示LLM预测如何在数据有限的环境中指导干预可行性和试验实施。该方法可扩展到公共卫生、灾害响应等领域，提供快速评估工具。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.22719v1",
      "published_date": "2025-03-25 20:24:47 UTC",
      "updated_date": "2025-03-25 20:24:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:53:22.852143"
    },
    {
      "arxiv_id": "2503.20036v1",
      "title": "BugCraft: End-to-End Crash Bug Reproduction Using LLM Agents in Minecraft",
      "title_zh": "翻译失败",
      "authors": [
        "Eray Yapağcı",
        "Yavuz Alp Sencer Öztürk",
        "Eray Tüzün"
      ],
      "abstract": "Reproducing game bugs, in our case crash bugs in continuously evolving games\nlike Minecraft, is a notoriously manual, time-consuming, and challenging\nprocess to automate. Despite the success of LLM-driven bug reproduction in\nother software domains, games, with their complex interactive environments,\nremain largely unaddressed. This paper introduces BugCraft, a novel end-to-end\nframework designed to automate the reproduction of crash bugs in Minecraft\ndirectly from user-submitted bug reports, addressing the critical gap in\nautomated game bug reproduction. BugCraft employs a two-stage approach: first,\na Step Synthesizer leverages LLMs and Minecraft Wiki knowledge to transform bug\nreports into high-quality, structured steps to reproduce (S2R). Second, an\nAction Model, powered by a vision-based LLM agent (GPT-4o) and a custom macro\nAPI, executes these S2R steps within Minecraft to trigger the reported crash.\nTo facilitate evaluation, we introduce BugCraft-Bench, a curated dataset of\nMinecraft crash bug reports. Evaluated on BugCraft-Bench, our framework\nsuccessfully reproduced 30.23% of crash bugs end-to-end. The Step Synthesizer\ndemonstrated a 66.28% accuracy in generating correct bug reproduction plans,\nhighlighting its effectiveness in interpreting and structuring bug report\ninformation. BugCraft demonstrates the feasibility of automated reproduction of\ncrash bugs in complex game environments using LLMs, opening promising avenues\nfor game testing and development. The framework and the BugCraft-Bench dataset\npave the way for future research in automated game bug analysis and hold\npotential for generalization to other interactive game platforms. Finally, we\nmake our code open at https://bugcraft2025.github.io/",
      "tldr_zh": "该研究提出了 BugCraft，一个端到端框架，利用 LLM Agents 自动重现 Minecraft 中的崩溃 bug，解决了游戏环境复杂导致的传统重现难题。框架采用两阶段方法：Step Synthesizer 使用 LLMs 和 Minecraft Wiki 知识，将用户提交的 bug 报告转化为高质量的结构化重现步骤 (S2R)；Action Model 则通过视觉-based LLM 代理（如 GPT-4o）和自定义宏 API 在游戏中执行这些步骤以触发崩溃。实验在 BugCraft-Bench 数据集上显示，BugCraft 端到端成功重现 30.23% 的崩溃 bug，Step Synthesizer 的准确率达到 66.28%。这项工作证明了 LLM 在复杂游戏 bug 分析中的可行性，并为游戏测试和开发提供新途径，同时开源代码以支持进一步研究。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20036v1",
      "published_date": "2025-03-25 19:34:24 UTC",
      "updated_date": "2025-03-25 19:34:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:53:36.169994"
    },
    {
      "arxiv_id": "2503.20028v1",
      "title": "OmniNova:A General Multimodal Agent Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Pengfei Du"
      ],
      "abstract": "The integration of Large Language Models (LLMs) with specialized tools\npresents new opportunities for intelligent automation systems. However,\norchestrating multiple LLM-driven agents to tackle complex tasks remains\nchallenging due to coordination difficulties, inefficient resource utilization,\nand inconsistent information flow. We present OmniNova, a modular multi-agent\nautomation framework that combines language models with specialized tools such\nas web search, crawling, and code execution capabilities. OmniNova introduces\nthree key innovations: (1) a hierarchical multi-agent architecture with\ndistinct coordinator, planner, supervisor, and specialist agents; (2) a dynamic\ntask routing mechanism that optimizes agent deployment based on task\ncomplexity; and (3) a multi-layered LLM integration system that allocates\nappropriate models to different cognitive requirements. Our evaluations across\n50 complex tasks in research, data analysis, and web interaction domains\ndemonstrate that OmniNova outperforms existing frameworks in task completion\nrate (87\\% vs. baseline 62\\%), efficiency (41\\% reduced token usage), and\nresult quality (human evaluation score of 4.2/5 vs. baseline 3.1/5). We\ncontribute both a theoretical framework for multi-agent system design and an\nopen-source implementation that advances the state-of-the-art in LLM-based\nautomation systems.",
      "tldr_zh": "该研究提出 OmniNova，一种通用的多模态智能体框架，将大型语言模型 (LLMs) 与专业工具（如网络搜索、爬取和代码执行）整合，以解决多智能体系统中的协调难题、资源利用 inefficiency 和信息流不一致问题。OmniNova 的关键创新包括：（1）分层多智能体架构，包含协调者、规划者、监督者和专家智能体；（2）动态任务路由机制，根据任务复杂度优化智能体部署；（3）多层 LLM 整合系统，根据认知需求分配模型。在 50 个复杂任务的评估中，OmniNova 显著优于基线框架，任务完成率达 87%（对比 62%），效率提升 41%（减少令牌使用），结果质量得分 4.2/5（对比 3.1/5），并提供了多智能体系统设计的理论框架和开源实现。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.20028v1",
      "published_date": "2025-03-25 19:21:01 UTC",
      "updated_date": "2025-03-25 19:21:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:53:48.287902"
    },
    {
      "arxiv_id": "2503.20018v1",
      "title": "Experience Replay Addresses Loss of Plasticity in Continual Learning",
      "title_zh": "经验回放解决持续学习中的可塑性损失",
      "authors": [
        "Jiuqi Wang",
        "Rohan Chandra",
        "Shangtong Zhang"
      ],
      "abstract": "Loss of plasticity is one of the main challenges in continual learning with\ndeep neural networks, where neural networks trained via backpropagation\ngradually lose their ability to adapt to new tasks and perform significantly\nworse than their freshly initialized counterparts. The main contribution of\nthis paper is to propose a new hypothesis that experience replay addresses the\nloss of plasticity in continual learning. Here, experience replay is a form of\nmemory. We provide supporting evidence for this hypothesis. In particular, we\ndemonstrate in multiple different tasks, including regression, classification,\nand policy evaluation, that by simply adding an experience replay and\nprocessing the data in the experience replay with Transformers, the loss of\nplasticity disappears. Notably, we do not alter any standard components of deep\nlearning. For example, we do not change backpropagation. We do not modify the\nactivation functions. And we do not use any regularization. We conjecture that\nexperience replay and Transformers can address the loss of plasticity because\nof the in-context learning phenomenon.",
      "tldr_zh": "本论文探讨了持续学习（Continual Learning）中深度神经网络的损失塑性（Loss of Plasticity）问题，即模型在新任务上逐渐丧失适应能力。研究假设使用经验回放（Experience Replay）作为一种记忆机制可以有效解决这一问题，并通过在回归、分类和策略评估等任务中添加经验回放并结合 Transformers 处理数据，证明了损失塑性现象的消失。关键贡献在于不需修改反向传播、激活函数或正则化等标准深度学习组件，而是依赖于 Transformers 的 in-context learning 现象来实现模型的可塑性恢复。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.20018v1",
      "published_date": "2025-03-25 19:01:10 UTC",
      "updated_date": "2025-03-25 19:01:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:53:59.681951"
    },
    {
      "arxiv_id": "2504.07117v1",
      "title": "RP-SAM2: Refining Point Prompts for Stable Surgical Instrument Segmentation",
      "title_zh": "RP-SAM2：优化点提示以实现稳定的手术器械分割",
      "authors": [
        "Nuren Zhaksylyk",
        "Ibrahim Almakky",
        "Jay Paranjape",
        "S. Swaroop Vedula",
        "Shameema Sikder",
        "Vishal M. Patel",
        "Mohammad Yaqub"
      ],
      "abstract": "Accurate surgical instrument segmentation is essential in cataract surgery\nfor tasks such as skill assessment and workflow optimization. However, limited\nannotated data makes it difficult to develop fully automatic models.\nPrompt-based methods like SAM2 offer flexibility yet remain highly sensitive to\nthe point prompt placement, often leading to inconsistent segmentations. We\naddress this issue by introducing RP-SAM2, which incorporates a novel shift\nblock and a compound loss function to stabilize point prompts. Our approach\nreduces annotator reliance on precise point positioning while maintaining\nrobust segmentation capabilities. Experiments on the Cataract1k dataset\ndemonstrate that RP-SAM2 improves segmentation accuracy, with a 2% mDSC gain, a\n21.36% reduction in mHD95, and decreased variance across random single-point\nprompt results compared to SAM2. Additionally, on the CaDIS dataset, pseudo\nmasks generated by RP-SAM2 for fine-tuning SAM2's mask decoder outperformed\nthose generated by SAM2. These results highlight RP-SAM2 as a practical, stable\nand reliable solution for semi-automatic instrument segmentation in\ndata-constrained medical settings. The code is available at\nhttps://github.com/BioMedIA-MBZUAI/RP-SAM2.",
      "tldr_zh": "该论文提出RP-SAM2框架，旨在通过优化点提示来提升手术器械分割的稳定性和准确性，解决如SAM2模型对点位置高度敏感的问题。RP-SAM2引入了shift block和复合损失函数，减少对精确点提示位置的依赖，同时保持鲁棒的分割性能。在Cataract1k数据集上，实验结果显示RP-SAM2较SAM2提高了2%的mDSC，降低了21.36%的mHD95，并显著减少了随机单点提示结果的方差。此外，在CaDIS数据集上，RP-SAM2生成的伪掩码用于微调SAM2的掩码解码器，表现出色，为数据受限的医疗环境提供了一个实用、可靠的半自动分割方案。",
      "categories": [
        "q-bio.TO",
        "cs.AI"
      ],
      "primary_category": "q-bio.TO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.07117v1",
      "published_date": "2025-03-25 18:59:23 UTC",
      "updated_date": "2025-03-25 18:59:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:54:13.393279"
    },
    {
      "arxiv_id": "2503.20001v1",
      "title": "Unsupervised Learning for Quadratic Assignment",
      "title_zh": "翻译失败",
      "authors": [
        "Yimeng Min",
        "Carla P. Gomes"
      ],
      "abstract": "We introduce PLUME search, a data-driven framework that enhances search\nefficiency in combinatorial optimization through unsupervised learning. Unlike\nsupervised or reinforcement learning, PLUME search learns directly from problem\ninstances using a permutation-based loss with a non-autoregressive approach. We\nevaluate its performance on the quadratic assignment problem, a fundamental\nNP-hard problem that encompasses various combinatorial optimization problems.\nExperimental results demonstrate that PLUME search consistently improves\nsolution quality. Furthermore, we study the generalization behavior and show\nthat the learned model generalizes across different densities and sizes.",
      "tldr_zh": "本文提出 PLUME search，一种通过无监督学习提升组合优化搜索效率的数据驱动框架，与监督或强化学习不同，它直接从问题实例中学习，使用基于置换的损失和非自回归方法。研究重点评估了其在 quadratic assignment problem（一个基本的 NP-hard 问题）上的性能，实验结果显示 PLUME search 能够 consistently improve solution quality。此外，模型在不同密度和大小的场景中表现出良好的泛化能力，为组合优化问题提供了新的解决思路。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.20001v1",
      "published_date": "2025-03-25 18:37:46 UTC",
      "updated_date": "2025-03-25 18:37:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:54:23.490100"
    },
    {
      "arxiv_id": "2503.19990v1",
      "title": "LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?",
      "title_zh": "翻译失败",
      "authors": [
        "Kexian Tang",
        "Junyao Gao",
        "Yanhong Zeng",
        "Haodong Duan",
        "Yanan Sun",
        "Zhening Xing",
        "Wenran Liu",
        "Kaifeng Lyu",
        "Kai Chen"
      ],
      "abstract": "Multi-step spatial reasoning entails understanding and reasoning about\nspatial relationships across multiple sequential steps, which is crucial for\ntackling complex real-world applications, such as robotic manipulation,\nautonomous navigation, and automated assembly. To assess how well current\nMultimodal Large Language Models (MLLMs) have acquired this fundamental\ncapability, we introduce \\textbf{LEGO-Puzzles}, a scalable benchmark designed\nto evaluate both \\textbf{spatial understanding} and \\textbf{sequential\nreasoning} in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100\ncarefully curated visual question-answering (VQA) samples spanning 11 distinct\ntasks, ranging from basic spatial understanding to complex multi-step\nreasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of\nstate-of-the-art MLLMs and uncover significant limitations in their spatial\nreasoning capabilities: even the most powerful MLLMs can answer only about half\nof the test cases, whereas human participants achieve over 90\\% accuracy. In\naddition to VQA tasks, we evaluate MLLMs' abilities to generate LEGO images\nfollowing assembly illustrations. Our experiments show that only\nGemini-2.0-Flash and GPT-4o exhibit a limited ability to follow these\ninstructions, while other MLLMs either replicate the input image or generate\ncompletely irrelevant outputs. Overall, LEGO-Puzzles exposes critical\ndeficiencies in existing MLLMs' spatial understanding and sequential reasoning\ncapabilities, and underscores the need for further advancements in multimodal\nspatial reasoning.",
      "tldr_zh": "该研究引入了 LEGO-Puzzles 基准，用于评估 Multimodal Large Language Models (MLLMs) 在多步空间推理方面的能力，该基准包含 1100 个视觉问答 (VQA) 样本，涵盖 11 个从基本空间理解到复杂顺序推理的任务。实验结果显示，即使是顶级 MLLMs 也仅能正确回答约一半的测试案例，而人类参与者准确率超过 90%。此外，在生成 LEGO 图像的任务中，只有 Gemini-2.0-Flash 和 GPT-4o 展现出有限的指令遵循能力，其他模型要么复制输入图像，要么输出无关内容。总体上，这暴露了现有 MLLMs 在多模态空间理解和顺序推理方面的重大不足，强调了进一步改进的必要性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.19990v1",
      "published_date": "2025-03-25 18:21:07 UTC",
      "updated_date": "2025-03-25 18:21:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:54:36.434728"
    },
    {
      "arxiv_id": "2503.19988v1",
      "title": "ExCoT: Optimizing Reasoning for Text-to-SQL with Execution Feedback",
      "title_zh": "翻译失败",
      "authors": [
        "Bohan Zhai",
        "Canwen Xu",
        "Yuxiong He",
        "Zhewei Yao"
      ],
      "abstract": "Text-to-SQL demands precise reasoning to convert natural language questions\ninto structured queries. While large language models (LLMs) excel in many\nreasoning tasks, their ability to leverage Chain-of-Thought (CoT) reasoning for\ntext-to-SQL remains underexplored. We identify critical limitations: zero-shot\nCoT offers minimal gains, and Direct Preference Optimization (DPO) applied\nwithout CoT yields marginal improvements. We propose ExCoT, a novel framework\nthat iteratively optimizes open-source LLMs by combining CoT reasoning with\noff-policy and on-policy DPO, relying solely on execution accuracy as feedback.\nThis approach eliminates the need for reward models or human-annotated\npreferences.\n  Our experimental results demonstrate significant performance gains: ExCoT\nimproves execution accuracy on BIRD dev set from 57.37% to 68.51% and on Spider\ntest set from 78.81% to 86.59% for LLaMA-3 70B, with Qwen-2.5-Coder\ndemonstrating similar improvements. Our best model achieves state-of-the-art\nperformance in the single-model setting on both BIRD and Spider datasets,\nnotably achieving 68.53% on the BIRD test set.",
      "tldr_zh": "本研究针对Text-to-SQL任务的推理挑战，提出ExCoT框架，以优化大语言模型(LLMs)的Chain-of-Thought (CoT)推理。ExCoT结合CoT推理与off-policy和on-policy Direct Preference Optimization (DPO)，仅使用执行准确性作为反馈，从而无需奖励模型或人工标注偏好。实验结果显示，该框架显著提升性能，例如LLaMA-3 70B在BIRD dev set上的执行准确率从57.37%提高到68.51%，在Spider test set上从78.81%提高到86.59%；最佳模型在BIRD和Spider数据集上实现单模型状态-of-the-art表现，BIRD test set达到68.53%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19988v1",
      "published_date": "2025-03-25 18:17:36 UTC",
      "updated_date": "2025-03-25 18:17:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:54:48.540500"
    },
    {
      "arxiv_id": "2503.19900v1",
      "title": "CAFe: Unifying Representation and Generation with Contrastive-Autoregressive Finetuning",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Yu",
        "Zhuokai Zhao",
        "Shen Yan",
        "Lukasz Korycki",
        "Jianyu Wang",
        "Baosheng He",
        "Jiayi Liu",
        "Lizhu Zhang",
        "Xiangjun Fan",
        "Hanchao Yu"
      ],
      "abstract": "The rapid advancement of large vision-language models (LVLMs) has driven\nsignificant progress in multimodal tasks, enabling models to interpret, reason,\nand generate outputs across both visual and textual domains. While excelling in\ngenerative tasks, existing LVLMs often face limitations in tasks requiring\nhigh-fidelity representation learning, such as generating image or text\nembeddings for retrieval. Recent work has proposed finetuning LVLMs for\nrepresentational learning, but the fine-tuned model often loses its generative\ncapabilities due to the representational learning training paradigm. To address\nthis trade-off, we introduce CAFe, a contrastive-autoregressive fine-tuning\nframework that enhances LVLMs for both representation and generative tasks. By\nintegrating a contrastive objective with autoregressive language modeling, our\napproach unifies these traditionally separate tasks, achieving state-of-the-art\nresults in both multimodal retrieval and multimodal generative benchmarks,\nincluding object hallucination (OH) mitigation. CAFe establishes a novel\nframework that synergizes embedding and generative functionalities in a single\nmodel, setting a foundation for future multimodal models that excel in both\nretrieval precision and coherent output generation.",
      "tldr_zh": "该研究提出了 CAFe，一种对比-自回归微调框架，用于统一大型视觉语言模型 (LVLMs) 在表示学习和生成任务上的能力，解决现有模型在高保真表示学习（如多模态检索）中可能丧失生成功能的权衡问题。通过整合对比学习目标 (contrastive objective) 与自回归语言建模 (autoregressive language modeling)，CAFe 使模型能够在多模态检索和生成基准上实现最先进性能，包括缓解对象幻觉 (object hallucination, OH)。这项工作为未来的多模态模型奠定了基础，使其同时在检索精度和连贯输出生成方面表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19900v1",
      "published_date": "2025-03-25 17:57:17 UTC",
      "updated_date": "2025-03-25 17:57:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:54:59.769790"
    },
    {
      "arxiv_id": "2503.19887v5",
      "title": "AI threats to national security can be countered through an incident regime",
      "title_zh": "翻译失败",
      "authors": [
        "Alejandro Ortega"
      ],
      "abstract": "Recent progress in AI capabilities has heightened concerns that AI systems\ncould pose a threat to national security, for example, by making it easier for\nmalicious actors to perform cyberattacks on critical national infrastructure,\nor through loss of control of autonomous AI systems. In parallel, federal\nlegislators in the US have proposed nascent 'AI incident regimes' to identify\nand counter similar threats. In this paper, we consolidate these two trends and\npresent a timely proposal for a legally mandated post-deployment AI incident\nregime that aims to counter potential national security threats from AI\nsystems. We start the paper by introducing the concept of 'security-critical'\nto describe sectors that pose extreme risks to national security, before\narguing that 'security-critical' describes civilian nuclear power, aviation,\nlife science dual-use research of concern, and frontier AI development. We then\npresent in detail our AI incident regime proposal, justifying each component of\nthe proposal by demonstrating its similarity to US domestic incident regimes in\nother 'security-critical' sectors. Finally, we sketch a hypothetical scenario\nwhere our proposed AI incident regime deals with an AI cyber incident. Our\nproposed AI incident regime is split into three phases. The first phase\nrevolves around a novel operationalization of what counts as an 'AI incident'\nand we suggest that AI providers must create a 'national security case' before\ndeploying a frontier AI system. The second and third phases spell out that AI\nproviders should notify a government agency about incidents, and that the\ngovernment agency should be involved in amending AI providers' security and\nsafety procedures, in order to counter future threats to national security.",
      "tldr_zh": "这篇论文讨论了AI系统对国家安全的潜在威胁，例如网络攻击或自主AI失控问题，并提出通过一个法律强制性的后部署AI incident regime来应对这些风险。作者定义了“security-critical”领域，包括民用核能、航空、生命科学双重用途研究和前沿AI开发，这些领域可能对国家安全构成极端风险。提案的核心是三个阶段：首先，操作化AI事件定义并要求AI提供者在部署frontier AI system前创建“national security case”；其次，AI提供者需向政府机构报告事件；最后，政府参与改进AI提供者的安全和安全程序，以防范未来威胁。该制度借鉴了其他security-critical领域的国内事件机制，并在假设场景中展示了其处理AI网络事件的有效性。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19887v5",
      "published_date": "2025-03-25 17:51:50 UTC",
      "updated_date": "2025-04-16 09:24:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:55:11.517156"
    },
    {
      "arxiv_id": "2503.19885v1",
      "title": "Dynamics of Structured Complex-Valued Hopfield Neural Networks",
      "title_zh": "结构化复值霍普菲尔德神经网络的动力学",
      "authors": [
        "Rama Murthy Garimella",
        "Marcos Eduardo Valle",
        "Guilherme Vieira",
        "Anil Rayala",
        "Dileep Munugoti"
      ],
      "abstract": "In this paper, we explore the dynamics of structured complex-valued Hopfield\nneural networks (CvHNNs), which arise when the synaptic weight matrix possesses\nspecific structural properties. We begin by analyzing CvHNNs with a Hermitian\nsynaptic weight matrix and establish the existence of four-cycle dynamics in\nCvHNNs with skew-Hermitian weight matrices operating synchronously.\nFurthermore, we introduce two new classes of complex-valued matrices: braided\nHermitian and braided skew-Hermitian matrices. We demonstrate that CvHNNs\nutilizing these matrix types exhibit cycles of length eight when operating in\nfull parallel update mode. Finally, we conduct extensive computational\nexperiments on synchronous CvHNNs, exploring other synaptic weight matrix\nstructures. The findings provide a comprehensive overview of the dynamics of\nstructured CvHNNs, offering insights that may contribute to developing improved\nassociative memory models when integrated with suitable learning rules.",
      "tldr_zh": "本研究探讨了结构化复值 Hopfield 神经网络 (CvHNNs) 的动态，重点分析突触权重矩阵的特定结构。论文首先证明了具有 Hermitian 矩阵的 CvHNNs 在 skew-Hermitian 矩阵下同步操作时存在四周期动态，并引入了两种新矩阵类型：braided Hermitian 和 braided skew-Hermitian 矩阵，这些矩阵在全并行更新模式下显示八周期动态。通过广泛的计算实验，研究者探索了其他权重矩阵结构，为 CvHNNs 的动态提供了全面见解，并为结合合适学习规则开发改进的联想记忆模型奠定了基础。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19885v1",
      "published_date": "2025-03-25 17:49:36 UTC",
      "updated_date": "2025-03-25 17:49:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:55:23.069515"
    },
    {
      "arxiv_id": "2503.19867v1",
      "title": "Geometric Meta-Learning via Coupled Ricci Flow: Unifying Knowledge Representation and Quantum Entanglement",
      "title_zh": "翻译失败",
      "authors": [
        "Ming Lei",
        "Christophe Baehr"
      ],
      "abstract": "This paper establishes a unified framework integrating geometric flows with\ndeep learning through three fundamental innovations. First, we propose a\nthermodynamically coupled Ricci flow that dynamically adapts parameter space\ngeometry to loss landscape topology, formally proved to preserve isometric\nknowledge embedding (Theorem~\\ref{thm:isometric}). Second, we derive explicit\nphase transition thresholds and critical learning rates\n(Theorem~\\ref{thm:critical}) through curvature blowup analysis, enabling\nautomated singularity resolution via geometric surgery\n(Lemma~\\ref{lem:surgery}). Third, we establish an AdS/CFT-type holographic\nduality (Theorem~\\ref{thm:ads}) between neural networks and conformal field\ntheories, providing entanglement entropy bounds for regularization design.\nExperiments demonstrate 2.1$\\times$ convergence acceleration and 63\\%\ntopological simplification while maintaining $\\mathcal{O}(N\\log N)$ complexity,\noutperforming Riemannian baselines by 15.2\\% in few-shot accuracy.\nTheoretically, we prove exponential stability (Theorem~\\ref{thm:converge})\nthrough a new Lyapunov function combining Perelman entropy with Wasserstein\ngradient flows, fundamentally advancing geometric deep learning.",
      "tldr_zh": "本论文提出了一种将几何流与深度学习整合的统一框架，通过三个关键创新统一知识表示和量子纠缠。首先，引入热力学耦合的 Ricci flow，以动态适应参数空间几何和损失景观拓扑，并证明了等距知识嵌入（Theorem 1）。其次，推导相变阈值和关键学习率（Theorem 2）通过曲率爆发分析，并利用几何手术（Lemma 1）自动处理奇点；第三，建立神经网络与共形场理论的 AdS/CFT 型全息对偶（Theorem 3），提供纠缠熵边界用于正则化设计。实验显示，该框架实现了 2.1 倍的收敛加速、63% 的拓扑简化，同时保持 O(N log N) 复杂度，并在 few-shot 准确率上比 Riemannian 基线提升 15.2%。理论上，通过结合 Perelman 熵和 Wasserstein 梯度流的 Lyapunov 函数，该方法证明了指数稳定性（Theorem 4），从而推进了几何深度学习的发展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP",
        "math.GT",
        "quant-ph",
        "68T05, 68T07, 68T27, 81V99, 37F40,",
        "I.2; K.3.2; F.4.1"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, submitted to IEEE PAMI",
      "pdf_url": "http://arxiv.org/pdf/2503.19867v1",
      "published_date": "2025-03-25 17:32:31 UTC",
      "updated_date": "2025-03-25 17:32:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:55:36.969386"
    },
    {
      "arxiv_id": "2503.19868v1",
      "title": "GENIUS: A Generative Framework for Universal Multimodal Search",
      "title_zh": "GENIUS：通用多模态搜索的生成式框架",
      "authors": [
        "Sungyeon Kim",
        "Xinliang Zhu",
        "Xiaofan Lin",
        "Muhammet Bastan",
        "Douglas Gray",
        "Suha Kwak"
      ],
      "abstract": "Generative retrieval is an emerging approach in information retrieval that\ngenerates identifiers (IDs) of target data based on a query, providing an\nefficient alternative to traditional embedding-based retrieval methods.\nHowever, existing models are task-specific and fall short of embedding-based\nretrieval in performance. This paper proposes GENIUS, a universal generative\nretrieval framework supporting diverse tasks across multiple modalities and\ndomains. At its core, GENIUS introduces modality-decoupled semantic\nquantization, transforming multimodal data into discrete IDs encoding both\nmodality and semantics. Moreover, to enhance generalization, we propose a query\naugmentation that interpolates between a query and its target, allowing GENIUS\nto adapt to varied query forms. Evaluated on the M-BEIR benchmark, it surpasses\nprior generative methods by a clear margin. Unlike embedding-based retrieval,\nGENIUS consistently maintains high retrieval speed across database size, with\ncompetitive performance across multiple benchmarks. With additional re-ranking,\nGENIUS often achieves results close to those of embedding-based methods while\npreserving efficiency.",
      "tldr_zh": "这篇论文提出了 GENIUS，一种通用的生成式检索框架，用于支持多种任务、多模态和领域的搜索。框架的核心创新包括 modality-decoupled semantic quantization，将多模态数据转化为编码模态和语义的离散 ID，以及 query augmentation，通过查询与目标的插值增强模型的泛化能力。在 M-BEIR 基准测试中，GENIUS 显著超过了现有生成式方法，并在不同数据库规模下保持高检索速度，与嵌入式方法性能相当或接近，尤其在添加再排序后进一步提升效率。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted to CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.19868v1",
      "published_date": "2025-03-25 17:32:31 UTC",
      "updated_date": "2025-03-25 17:32:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:55:48.338024"
    },
    {
      "arxiv_id": "2503.19848v1",
      "title": "Guarding against artificial intelligence--hallucinated citations: the case for full-text reference deposit",
      "title_zh": "翻译失败",
      "authors": [
        "Alex Glynn"
      ],
      "abstract": "The tendency of generative artificial intelligence (AI) systems to\n\"hallucinate\" false information is well-known; AI-generated citations to\nnon-existent sources have made their way into the reference lists of\npeer-reviewed publications. Here, I propose a solution to this problem, taking\ninspiration from the Transparency and Openness Promotion (TOP) data sharing\nguidelines, the clash of generative AI with the American judiciary, and the\nprecedent set by submissions of prior art to the United States Patent and\nTrademark Office. Journals should require authors to submit the full text of\neach cited source along with their manuscripts, thereby preventing authors from\nciting any material whose full text they cannot produce. This solution requires\nlimited additional work on the part of authors or editors while effectively\nimmunizing journals against hallucinated references.",
      "tldr_zh": "该论文讨论了生成式人工智能（generative AI）系统产生的虚假引用（hallucinated citations）问题，这些引用已进入同行评议出版物的参考文献中。作者提出解决方案：期刊应要求作者在提交稿件时提供每个引用的完整文本，从而防止引用无法验证的材料。该方案借鉴了Transparency and Openness Promotion (TOP)数据共享指南、AI与美国司法系统的冲突，以及美国专利商标局的先例。这种方法只需作者和编辑进行有限的额外工作，就能有效防范虚假引用并提升学术诚信。",
      "categories": [
        "cs.DL",
        "cs.AI",
        "I.2.0; K.4.1"
      ],
      "primary_category": "cs.DL",
      "comment": "3 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.19848v1",
      "published_date": "2025-03-25 17:12:38 UTC",
      "updated_date": "2025-03-25 17:12:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:56:00.633935"
    },
    {
      "arxiv_id": "2503.19844v1",
      "title": "A Comparative Analysis of Word Segmentation, Part-of-Speech Tagging, and Named Entity Recognition for Historical Chinese Sources, 1900-1950",
      "title_zh": "翻译失败",
      "authors": [
        "Zhao Fang",
        "Liang-Chun Wu",
        "Xuening Kong",
        "Spencer Dean Stewart"
      ],
      "abstract": "This paper compares large language models (LLMs) and traditional natural\nlanguage processing (NLP) tools for performing word segmentation,\npart-of-speech (POS) tagging, and named entity recognition (NER) on Chinese\ntexts from 1900 to 1950. Historical Chinese documents pose challenges for text\nanalysis due to their logographic script, the absence of natural word\nboundaries, and significant linguistic changes. Using a sample dataset from the\nShanghai Library Republican Journal corpus, traditional tools such as Jieba and\nspaCy are compared to LLMs, including GPT-4o, Claude 3.5, and the GLM series.\nThe results show that LLMs outperform traditional methods in all metrics,\nalbeit at considerably higher computational costs, highlighting a trade-off\nbetween accuracy and efficiency. Additionally, LLMs better handle\ngenre-specific challenges such as poetry and temporal variations (i.e.,\npre-1920 versus post-1920 texts), demonstrating that their contextual learning\ncapabilities can advance NLP approaches to historical texts by reducing the\nneed for domain-specific training data.",
      "tldr_zh": "本研究比较了大型语言模型 (LLMs) 和传统自然语言处理 (NLP) 工具在处理1900-1950年历史中文文本时的表现，焦点任务包括Word Segmentation、Part-of-Speech Tagging和Named Entity Recognition。使用上海图书馆共和期刊语料库的样本数据集，研究者对比了传统工具如Jieba和spaCy与LLMs（如GPT-4o、Claude 3.5和GLM系列），结果显示LLMs在所有指标上表现出色，但计算成本显著更高，体现了准确性和效率的权衡。LLMs特别擅长应对体裁特定挑战（如诗歌）和时间变化（1920前后），通过其上下文学习能力减少了对领域特定训练数据的依赖，从而推进了历史文本NLP方法的发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NLP4DH 2025 at NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.19844v1",
      "published_date": "2025-03-25 17:07:21 UTC",
      "updated_date": "2025-03-25 17:07:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:56:13.117181"
    },
    {
      "arxiv_id": "2503.19823v2",
      "title": "GyralNet Subnetwork Partitioning via Differentiable Spectral Modularity Optimization",
      "title_zh": "通过可",
      "authors": [
        "Yan Zhuang",
        "Minheng Chen",
        "Chao Cao",
        "Tong Chen",
        "Jing Zhang",
        "Xiaowei Yu",
        "Yanjun Lyu",
        "Lu Zhang",
        "Tianming Liu",
        "Dajiang Zhu"
      ],
      "abstract": "Understanding the structural and functional organization of the human brain\nrequires a detailed examination of cortical folding patterns, among which the\nthree-hinge gyrus (3HG) has been identified as a key structural landmark.\nGyralNet, a network representation of cortical folding, models 3HGs as nodes\nand gyral crests as edges, highlighting their role as critical hubs in\ncortico-cortical connectivity. However, existing methods for analyzing 3HGs\nface significant challenges, including the sub-voxel scale of 3HGs at typical\nneuroimaging resolutions, the computational complexity of establishing\ncross-subject correspondences, and the oversimplification of treating 3HGs as\nindependent nodes without considering their community-level relationships. To\naddress these limitations, we propose a fully differentiable subnetwork\npartitioning framework that employs a spectral modularity maximization\noptimization strategy to modularize the organization of 3HGs within GyralNet.\nBy incorporating topological structural similarity and DTI-derived connectivity\npatterns as attribute features, our approach provides a biologically meaningful\nrepresentation of cortical organization. Extensive experiments on the Human\nConnectome Project (HCP) dataset demonstrate that our method effectively\npartitions GyralNet at the individual level while preserving the\ncommunity-level consistency of 3HGs across subjects, offering a robust\nfoundation for understanding brain connectivity.",
      "tldr_zh": "本研究针对大脑皮层折叠模式中的三铰链回 (3HG) 分析问题，提出了一种基于可微光谱模数最大化优化的子网络分区框架，用于模块化 GyralNet 网络。框架将拓扑结构相似性和 DTI 衍生的连接模式作为属性特征，构建生物学上意义重大的皮层组织表示，从而解决现有方法的计算复杂性和忽略社区关系的局限性。在 Human Connectome Project (HCP) 数据集上的广泛实验表明，该方法能在个体水平有效分区 GyralNet，同时保持跨主体的社区级一致性，为理解大脑连接性提供稳健基础。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "q-bio.NC",
      "comment": "10 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.19823v2",
      "published_date": "2025-03-25 16:33:12 UTC",
      "updated_date": "2025-03-31 21:17:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:56:24.551056"
    },
    {
      "arxiv_id": "2503.19817v1",
      "title": "Bitstream Collisions in Neural Image Compression via Adversarial Perturbations",
      "title_zh": "翻译失败",
      "authors": [
        "Jordan Madden",
        "Lhamo Dorje",
        "Xiaohua Li"
      ],
      "abstract": "Neural image compression (NIC) has emerged as a promising alternative to\nclassical compression techniques, offering improved compression ratios. Despite\nits progress towards standardization and practical deployment, there has been\nminimal exploration into it's robustness and security. This study reveals an\nunexpected vulnerability in NIC - bitstream collisions - where semantically\ndifferent images produce identical compressed bitstreams. Utilizing a novel\nwhitebox adversarial attack algorithm, this paper demonstrates that adding\ncarefully crafted perturbations to semantically different images can cause\ntheir compressed bitstreams to collide exactly. The collision vulnerability\nposes a threat to the practical usability of NIC, particularly in\nsecurity-critical applications. The cause of the collision is analyzed, and a\nsimple yet effective mitigation method is presented.",
      "tldr_zh": "本研究揭示了神经图像压缩 (NIC) 的一种关键漏洞：位流碰撞 (bitstream collisions)，即语义不同的图像通过添加精心设计的对抗扰动 (adversarial perturbations) 可产生相同的压缩位流，这威胁到NIC的安全性和实际应用。研究者开发了一种新型白盒对抗攻击算法，成功演示了这一现象在多种场景中的发生。分析显示，这种碰撞源于NIC的内部机制，并提出了一种简单有效的缓解方法，以提升NIC在安全关键领域的鲁棒性。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19817v1",
      "published_date": "2025-03-25 16:29:17 UTC",
      "updated_date": "2025-03-25 16:29:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:56:35.728257"
    },
    {
      "arxiv_id": "2503.19951v1",
      "title": "ACVUBench: Audio-Centric Video Understanding Benchmark",
      "title_zh": "ACVUBench：以音频为中心的视频理解基准",
      "authors": [
        "Yudong Yang",
        "Jimin Zhuang",
        "Guangzhi Sun",
        "Changli Tang",
        "Yixuan Li",
        "Peihan Li",
        "Yifan Jiang",
        "Wei Li",
        "Zejun Ma",
        "Chao Zhang"
      ],
      "abstract": "Audio often serves as an auxiliary modality in video understanding tasks of\naudio-visual large language models (LLMs), merely assisting in the\ncomprehension of visual information. However, a thorough understanding of\nvideos significantly depends on auditory information, as audio offers critical\ncontext, emotional cues, and semantic meaning that visual data alone often\nlacks. This paper proposes an audio-centric video understanding benchmark\n(ACVUBench) to evaluate the video comprehension capabilities of multimodal LLMs\nwith a particular focus on auditory information. Specifically, ACVUBench\nincorporates 2,662 videos spanning 18 different domains with rich auditory\ninformation, together with over 13k high-quality human annotated or validated\nquestion-answer pairs. Moreover, ACVUBench introduces a suite of carefully\ndesigned audio-centric tasks, holistically testing the understanding of both\naudio content and audio-visual interactions in videos. A thorough evaluation\nacross a diverse range of open-source and proprietary multimodal LLMs is\nperformed, followed by the analyses of deficiencies in audio-visual LLMs. Demos\nare available at https://github.com/lark-png/ACVUBench.",
      "tldr_zh": "该论文提出ACVUBench，一个以音频为中心的视频理解基准，用于评估多模态LLMs在处理音频信息方面的能力，因为音频提供视觉数据无法替代的关键上下文、情感线索和语义含义。ACVUBench包含2,662个视频，覆盖18个领域，并附带超过13k条高质量的人类标注的问题-答案对，同时引入一系列音频中心任务，以全面测试音频内容和音频-视觉交互的理解。研究对各种开源和专有多模态LLMs进行了彻底评估，并分析了这些模型在音频理解方面的不足，为未来视频理解技术的改进提供了宝贵见解。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19951v1",
      "published_date": "2025-03-25 16:28:24 UTC",
      "updated_date": "2025-03-25 16:28:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:56:49.539892"
    },
    {
      "arxiv_id": "2503.19815v1",
      "title": "Thinking agents for zero-shot generalization to qualitatively novel tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Thomas Miconi",
        "Kevin McKee",
        "Yicong Zheng",
        "Jed McCaleb"
      ],
      "abstract": "Intelligent organisms can solve truly novel problems which they have never\nencountered before, either in their lifetime or their evolution. An important\ncomponent of this capacity is the ability to ``think'', that is, to mentally\nmanipulate objects, concepts and behaviors in order to plan and evaluate\npossible solutions to novel problems, even without environment interaction. To\ngenerate problems that are truly qualitatively novel, while still solvable\nzero-shot (by mental simulation), we use the combinatorial nature of\nenvironments: we train the agent while withholding a specific combination of\nthe environment's elements. The novel test task, based on this combination, is\nthus guaranteed to be truly novel, while still mentally simulable since the\nagent has been exposed to each individual element (and their pairwise\ninteractions) during training. We propose a method to train agents endowed with\nworld models to make use their mental simulation abilities, by selecting tasks\nbased on the difference between the agent's pre-thinking and post-thinking\nperformance. When tested on the novel, withheld problem, the resulting agent\nsuccessfully simulated alternative scenarios and used the resulting information\nto guide its behavior in the actual environment, solving the novel task in a\nsingle real-environment trial (zero-shot).",
      "tldr_zh": "这篇论文探讨了训练智能代理以实现零-shot 泛化到定性新颖任务的方法，通过赋予代理“思考”能力，即心理模拟（mental simulation）来规划和评估解决方案，而无需实际环境互动。作者利用环境的组合性质，在训练中隐藏特定元素组合，确保测试任务新颖但仍可模拟，因为代理已接触过单个元素及其成对互动。提出的训练方法基于世界模型（world models），通过比较代理的思考前后性能来选择任务，最终实验显示，代理能在新任务上成功模拟场景并指导行为，实现零-shot 解决。",
      "categories": [
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19815v1",
      "published_date": "2025-03-25 16:26:31 UTC",
      "updated_date": "2025-03-25 16:26:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:57:00.662339"
    },
    {
      "arxiv_id": "2503.21810v1",
      "title": "Taxonomy Inference for Tabular Data Using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenyu Wu",
        "Jiaoyan Chen",
        "Norman W. Paton"
      ],
      "abstract": "Taxonomy inference for tabular data is a critical task of schema inference,\naiming at discovering entity types (i.e., concepts) of the tables and building\ntheir hierarchy. It can play an important role in data management, data\nexploration, ontology learning, and many data-centric applications. Existing\nschema inference systems focus more on XML, JSON or RDF data, and often rely on\nlexical formats and structures of the data for calculating similarities, with\nlimited exploitation of the semantics of the text across a table. Motivated by\nrecent works on taxonomy completion and construction using Large Language\nModels (LLMs), this paper presents two LLM-based methods for taxonomy inference\nfor tables: (i) EmTT which embeds columns by fine-tuning with contrastive\nlearning encoder-alone LLMs like BERT and utilises clustering for hierarchy\nconstruction, and (ii) GeTT which generates table entity types and their\nhierarchy by iterative prompting using a decoder-alone LLM like GPT-4.\nExtensive evaluation on three real-world datasets with six metrics covering\ndifferent aspects of the output taxonomies has demonstrated that EmTT and GeTT\ncan both produce taxonomies with strong consistency relative to the Ground\nTruth.",
      "tldr_zh": "该论文探讨了使用 Large Language Models (LLMs) 进行表格数据的分类学推理 (taxonomy inference)，旨在发现表格的实体类型并构建其层次结构，以支持数据管理、数据探索和本体学习等应用。论文提出两种方法：EmTT 通过微调对比学习编码器（如 BERT）嵌入列并利用聚类构建层次结构；GeTT 通过迭代提示解码器 LLM（如 GPT-4）生成实体类型和层次。实验在三个真实数据集上使用六种指标评估，结果显示 EmTT 和 GeTT 输出的分类学与 Ground Truth 高度一致，显著提升了现有方法的语义利用能力。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21810v1",
      "published_date": "2025-03-25 16:26:05 UTC",
      "updated_date": "2025-03-25 16:26:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:57:14.338168"
    },
    {
      "arxiv_id": "2503.19813v1",
      "title": "Guidelines For The Choice Of The Baseline in XAI Attribution Methods",
      "title_zh": "XAI 归因方法中基线选择的指南",
      "authors": [
        "Cristian Morasso",
        "Giorgio Dolci",
        "Ilaria Boscolo Galazzo",
        "Sergey M. Plis",
        "Gloria Menegaz"
      ],
      "abstract": "Given the broad adoption of artificial intelligence, it is essential to\nprovide evidence that AI models are reliable, trustable, and fair. To this end,\nthe emerging field of eXplainable AI develops techniques to probe such\nrequirements, counterbalancing the hype pushing the pervasiveness of this\ntechnology. Among the many facets of this issue, this paper focuses on baseline\nattribution methods, aiming at deriving a feature attribution map at the\nnetwork input relying on a \"neutral\" stimulus usually called \"baseline\". The\nchoice of the baseline is crucial as it determines the explanation of the\nnetwork behavior. In this framework, this paper has the twofold goal of\nshedding light on the implications of the choice of the baseline and providing\na simple yet effective method for identifying the best baseline for the task.\nTo achieve this, we propose a decision boundary sampling method, since the\nbaseline, by definition, lies on the decision boundary, which naturally becomes\nthe search domain. Experiments are performed on synthetic examples and\nvalidated relying on state-of-the-art methods. Despite being limited to the\nexperimental scope, this contribution is relevant as it offers clear guidelines\nand a simple proxy for baseline selection, reducing ambiguity and enhancing\ndeep models' reliability and trust.",
      "tldr_zh": "该论文探讨了XAI（eXplainable AI）归因方法中baseline选择的重要性，因为baseline作为中性刺激直接影响网络行为的解释。该研究提出了一种基于decision boundary sampling的方法，通过在决策边界上采样来识别最佳baseline，从而减少归因歧义。该方法在合成例子和现有技术上进行了实验验证，结果显示它能有效提升深度模型的可靠性和可信度，为XAI应用提供清晰的指导原则。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19813v1",
      "published_date": "2025-03-25 16:25:04 UTC",
      "updated_date": "2025-03-25 16:25:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:57:23.723217"
    },
    {
      "arxiv_id": "2503.19950v1",
      "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior Accuracy Preservation",
      "title_zh": "翻译失败",
      "authors": [
        "Han Chen",
        "Zicong Jiang",
        "Zining Zhang",
        "Bingsheng He",
        "Pingyi Luo",
        "Mian Lu",
        "Yuqiang Chen"
      ],
      "abstract": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV.",
      "tldr_zh": "本文提出LogQuant，一种创新的2-bit量化技术，用于大型语言模型(LLM)推理中的KV Cache，能够显著节省内存同时保持高准确性。不同于以往方法，LogQuant采用基于log的过滤机制，选择性地压缩整个上下文，避免了性能瓶颈和预测错误，从而在相同或更少的内存占用下实现优越性能。在基准测试中，LogQuant提高了吞吐量25%、增加了批量大小60%，并在Math和Code Completion任务中将准确率提升40%至200%，易于集成到Python的transformers库中。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ICLR 2025 Workshop on Sparsity in LLMs (SLLM)",
      "pdf_url": "http://arxiv.org/pdf/2503.19950v1",
      "published_date": "2025-03-25 16:24:45 UTC",
      "updated_date": "2025-03-25 16:24:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:57:37.743089"
    },
    {
      "arxiv_id": "2503.19809v1",
      "title": "Simulating Tracking Data to Advance Sports Analytics Research",
      "title_zh": "翻译失败",
      "authors": [
        "David Radke",
        "Kyle Tilbury"
      ],
      "abstract": "Advanced analytics have transformed how sports teams operate, particularly in\nepisodic sports like baseball. Their impact on continuous invasion sports, such\nas soccer and ice hockey, has been limited due to increased game complexity and\nrestricted access to high-resolution game tracking data. In this demo, we\npresent a method to collect and utilize simulated soccer tracking data from the\nGoogle Research Football environment to support the development of models\ndesigned for continuous tracking data. The data is stored in a schema that is\nrepresentative of real tracking data and we provide processes that extract\nhigh-level features and events. We include examples of established tracking\ndata models to showcase the efficacy of the simulated data. We address the\nscarcity of publicly available tracking data, providing support for research at\nthe intersection of artificial intelligence and sports analytics.",
      "tldr_zh": "这篇论文解决了体育分析在足球等持续入侵运动中因游戏复杂性和高分辨率 tracking data 稀缺而受限的问题。作者提出了一种方法，使用 Google Research Football 环境模拟足球 tracking data，并将其存储在与真实数据相似的模式中，同时提供提取高级特征和事件的流程。实验展示了模拟数据在支持 tracking data 模型开发方面的有效性，从而缓解公开可用数据的短缺，并推动人工智能与体育分析交叉研究的发展。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "2 pages, 2 figures, Proceedings of the 24th International Conference\n  on Autonomous Agents and MultiAgent Systems (AAMAS)",
      "pdf_url": "http://arxiv.org/pdf/2503.19809v1",
      "published_date": "2025-03-25 16:18:23 UTC",
      "updated_date": "2025-03-25 16:18:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:57:49.763193"
    },
    {
      "arxiv_id": "2503.19804v1",
      "title": "LENVIZ: A High-Resolution Low-Exposure Night Vision Benchmark Dataset",
      "title_zh": "LENVIZ：高分辨率低曝光夜视基准数据集",
      "authors": [
        "Manjushree Aithal",
        "Rosaura G. VidalMata",
        "Manikandtan Kartha",
        "Gong Chen",
        "Eashan Adhikarla",
        "Lucas N. Kirsten",
        "Zhicheng Fu",
        "Nikhil A. Madhusudhana",
        "Joe Nasti"
      ],
      "abstract": "Low-light image enhancement is crucial for a myriad of applications, from\nnight vision and surveillance, to autonomous driving. However, due to the\ninherent limitations that come in hand with capturing images in\nlow-illumination environments, the task of enhancing such scenes still presents\na formidable challenge. To advance research in this field, we introduce our Low\nExposure Night Vision (LENVIZ) Dataset, a comprehensive multi-exposure\nbenchmark dataset for low-light image enhancement comprising of over 230K\nframes showcasing 24K real-world indoor and outdoor, with-and without human,\nscenes. Captured using 3 different camera sensors, LENVIZ offers a wide range\nof lighting conditions, noise levels, and scene complexities, making it the\nlargest publicly available up-to 4K resolution benchmark in the field. LENVIZ\nincludes high quality human-generated ground truth, for which each\nmulti-exposure low-light scene has been meticulously curated and edited by\nexpert photographers to ensure optimal image quality. Furthermore, we also\nconduct a comprehensive analysis of current state-of-the-art low-light image\nenhancement techniques on our dataset and highlight potential areas of\nimprovement.",
      "tldr_zh": "本文介绍了LENVIZ数据集，这是一个高分辨率低曝光夜视基准数据集，旨在推进低-light image enhancement领域的研究。该数据集包含超过23万帧图像，涵盖2.4万真实室内和室外场景，包括有无人类元素，并使用3种不同相机传感器捕获，提供多样化的照明条件、噪声水平和场景复杂度，是目前最大的公开可用数据集，支持高达4K分辨率。LENVIZ包括高质量的人工生成ground truth，由专家摄影师精心编辑以确保最佳图像质量。此外，作者对现有state-of-the-art低光照图像增强技术进行了全面分析，并突出了潜在改进领域。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Dataset will be released upon publication",
      "pdf_url": "http://arxiv.org/pdf/2503.19804v1",
      "published_date": "2025-03-25 16:12:28 UTC",
      "updated_date": "2025-03-25 16:12:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:58:02.702268"
    },
    {
      "arxiv_id": "2503.19801v1",
      "title": "SeLIP: Similarity Enhanced Contrastive Language Image Pretraining for Multi-modal Head MRI",
      "title_zh": "SeLIP：相似性增强对比语言图像预训练用于多模态头部MRI",
      "authors": [
        "Zhiyang Liu",
        "Dong Yang",
        "Minghao Zhang",
        "Hanyu Sun",
        "Hong Wu",
        "Huiying Wang",
        "Wen Shen",
        "Chao Chai",
        "Shuang Xia"
      ],
      "abstract": "Despite that deep learning (DL) methods have presented tremendous potential\nin many medical image analysis tasks, the practical applications of medical DL\nmodels are limited due to the lack of enough data samples with manual\nannotations. By noting that the clinical radiology examinations are associated\nwith radiology reports that describe the images, we propose to develop a\nfoundation model for multi-model head MRI by using contrastive learning on the\nimages and the corresponding radiology findings. In particular, a contrastive\nlearning framework is proposed, where a mixed syntax and semantic similarity\nmatching metric is integrated to reduce the thirst of extreme large dataset in\nconventional contrastive learning framework. Our proposed similarity enhanced\ncontrastive language image pretraining (SeLIP) is able to effectively extract\nmore useful features. Experiments revealed that our proposed SeLIP performs\nwell in many downstream tasks including image-text retrieval task,\nclassification task, and image segmentation, which highlights the importance of\nconsidering the similarities among texts describing different images in\ndeveloping medical image foundation models.",
      "tldr_zh": "该研究针对深度学习（deep learning）在医疗图像分析中数据标注不足的问题，提出了一种SeLIP（Similarity Enhanced Contrastive Language Image Pretraining）框架，用于多模态头MRI（multi-modal Head MRI）。SeLIP通过整合混合语法和语义相似性匹配指标的对比学习（contrastive learning）方法，利用图像和对应的放射学报告进行预训练，从而更有效地提取有用特征，并减少对大规模数据集的依赖。实验结果显示，SeLIP在图像-文本检索（image-text retrieval）、分类（classification）和图像分割（image segmentation）等下游任务中表现出色，突出了在医疗图像基础模型开发中考虑文本相似性的重要性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19801v1",
      "published_date": "2025-03-25 16:09:45 UTC",
      "updated_date": "2025-03-25 16:09:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:58:12.858780"
    },
    {
      "arxiv_id": "2503.19794v1",
      "title": "PAVE: Patching and Adapting Video Large Language Models",
      "title_zh": "PAVE",
      "authors": [
        "Zhuoming Liu",
        "Yiquan Li",
        "Khoi Duc Nguyen",
        "Yiwu Zhong",
        "Yin Li"
      ],
      "abstract": "Pre-trained video large language models (Video LLMs) exhibit remarkable\nreasoning capabilities, yet adapting these models to new tasks involving\nadditional modalities or data types (e.g., audio or 3D information) remains\nchallenging. In this paper, we present PAVE, a flexible framework for adapting\npre-trained Video LLMs to downstream tasks with side-channel signals, such as\naudio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters,\nreferred to as \"patches,\" which add a small number of parameters and operations\nto a base model without modifying its architecture or pre-trained weights. In\ndoing so, PAVE can effectively adapt the pre-trained base model to support\ndiverse downstream tasks, including audio-visual question answering, 3D\nreasoning, multi-view video recognition, and high frame rate video\nunderstanding. Across these tasks, PAVE significantly enhances the performance\nof the base model, surpassing state-of-the-art task-specific models while\nincurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE\nsupports multi-task learning and generalizes well across different Video LLMs.\nOur code is available at https://github.com/dragonlzm/PAVE.",
      "tldr_zh": "本文提出PAVE框架，用于适应预训练的Video LLMs到下游任务，如音频-视觉问答、3D推理和多视图视频识别。PAVE通过引入轻量级适配器（patches）添加少量参数和操作，而不修改基模型的架构或权重，从而高效扩展模型功能。实验结果显示，PAVE显著提升基模型性能，超越最先进任务特定模型，仅增加约0.1%的FLOPs和参数，并支持多任务学习和不同Video LLMs的泛化。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR2025 Camera Ready",
      "pdf_url": "http://arxiv.org/pdf/2503.19794v1",
      "published_date": "2025-03-25 16:02:37 UTC",
      "updated_date": "2025-03-25 16:02:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:58:24.982151"
    },
    {
      "arxiv_id": "2503.19786v1",
      "title": "Gemma 3 Technical Report",
      "title_zh": "Gemma 3 技术报告",
      "authors": [
        "Gemma Team",
        "Aishwarya Kamath",
        "Johan Ferret",
        "Shreya Pathak",
        "Nino Vieillard",
        "Ramona Merhej",
        "Sarah Perrin",
        "Tatiana Matejovicova",
        "Alexandre Ramé",
        "Morgane Rivière",
        "Louis Rouillard",
        "Thomas Mesnard",
        "Geoffrey Cideron",
        "Jean-bastien Grill",
        "Sabela Ramos",
        "Edouard Yvinec",
        "Michelle Casbon",
        "Etienne Pot",
        "Ivo Penchev",
        "Gaël Liu",
        "Francesco Visin",
        "Kathleen Kenealy",
        "Lucas Beyer",
        "Xiaohai Zhai",
        "Anton Tsitsulin",
        "Robert Busa-Fekete",
        "Alex Feng",
        "Noveen Sachdeva",
        "Benjamin Coleman",
        "Yi Gao",
        "Basil Mustafa",
        "Iain Barr",
        "Emilio Parisotto",
        "David Tian",
        "Matan Eyal",
        "Colin Cherry",
        "Jan-Thorsten Peter",
        "Danila Sinopalnikov",
        "Surya Bhupatiraju",
        "Rishabh Agarwal",
        "Mehran Kazemi",
        "Dan Malkin",
        "Ravin Kumar",
        "David Vilar",
        "Idan Brusilovsky",
        "Jiaming Luo",
        "Andreas Steiner",
        "Abe Friesen",
        "Abhanshu Sharma",
        "Abheesht Sharma",
        "Adi Mayrav Gilady",
        "Adrian Goedeckemeyer",
        "Alaa Saade",
        "Alex Feng",
        "Alexander Kolesnikov",
        "Alexei Bendebury",
        "Alvin Abdagic",
        "Amit Vadi",
        "András György",
        "André Susano Pinto",
        "Anil Das",
        "Ankur Bapna",
        "Antoine Miech",
        "Antoine Yang",
        "Antonia Paterson",
        "Ashish Shenoy",
        "Ayan Chakrabarti",
        "Bilal Piot",
        "Bo Wu",
        "Bobak Shahriari",
        "Bryce Petrini",
        "Charlie Chen",
        "Charline Le Lan",
        "Christopher A. Choquette-Choo",
        "CJ Carey",
        "Cormac Brick",
        "Daniel Deutsch",
        "Danielle Eisenbud",
        "Dee Cattle",
        "Derek Cheng",
        "Dimitris Paparas",
        "Divyashree Shivakumar Sreepathihalli",
        "Doug Reid",
        "Dustin Tran",
        "Dustin Zelle",
        "Eric Noland",
        "Erwin Huizenga",
        "Eugene Kharitonov",
        "Frederick Liu",
        "Gagik Amirkhanyan",
        "Glenn Cameron",
        "Hadi Hashemi",
        "Hanna Klimczak-Plucińska",
        "Harman Singh",
        "Harsh Mehta",
        "Harshal Tushar Lehri",
        "Hussein Hazimeh",
        "Ian Ballantyne",
        "Idan Szpektor",
        "Ivan Nardini",
        "Jean Pouget-Abadie",
        "Jetha Chan",
        "Joe Stanton",
        "John Wieting",
        "Jonathan Lai",
        "Jordi Orbay",
        "Joseph Fernandez",
        "Josh Newlan",
        "Ju-yeong Ji",
        "Jyotinder Singh",
        "Kat Black",
        "Kathy Yu",
        "Kevin Hui",
        "Kiran Vodrahalli",
        "Klaus Greff",
        "Linhai Qiu",
        "Marcella Valentine",
        "Marina Coelho",
        "Marvin Ritter",
        "Matt Hoffman",
        "Matthew Watson",
        "Mayank Chaturvedi",
        "Michael Moynihan",
        "Min Ma",
        "Nabila Babar",
        "Natasha Noy",
        "Nathan Byrd",
        "Nick Roy",
        "Nikola Momchev",
        "Nilay Chauhan",
        "Noveen Sachdeva",
        "Oskar Bunyan",
        "Pankil Botarda",
        "Paul Caron",
        "Paul Kishan Rubenstein",
        "Phil Culliton",
        "Philipp Schmid",
        "Pier Giuseppe Sessa",
        "Pingmei Xu",
        "Piotr Stanczyk",
        "Pouya Tafti",
        "Rakesh Shivanna",
        "Renjie Wu",
        "Renke Pan",
        "Reza Rokni",
        "Rob Willoughby",
        "Rohith Vallu",
        "Ryan Mullins",
        "Sammy Jerome",
        "Sara Smoot",
        "Sertan Girgin",
        "Shariq Iqbal",
        "Shashir Reddy",
        "Shruti Sheth",
        "Siim Põder",
        "Sijal Bhatnagar",
        "Sindhu Raghuram Panyam",
        "Sivan Eiger",
        "Susan Zhang",
        "Tianqi Liu",
        "Trevor Yacovone",
        "Tyler Liechty",
        "Uday Kalra",
        "Utku Evci",
        "Vedant Misra",
        "Vincent Roseberry",
        "Vlad Feinberg",
        "Vlad Kolesnikov",
        "Woohyun Han",
        "Woosuk Kwon",
        "Xi Chen",
        "Yinlam Chow",
        "Yuvein Zhu",
        "Zichuan Wei",
        "Zoltan Egyed",
        "Victor Cotruta",
        "Minh Giang",
        "Phoebe Kirk",
        "Anand Rao",
        "Kat Black",
        "Nabila Babar",
        "Jessica Lo",
        "Erica Moreira",
        "Luiz Gustavo Martins",
        "Omar Sanseviero",
        "Lucas Gonzalez",
        "Zach Gleicher",
        "Tris Warkentin",
        "Vahab Mirrokni",
        "Evan Senter",
        "Eli Collins",
        "Joelle Barral",
        "Zoubin Ghahramani",
        "Raia Hadsell",
        "Yossi Matias",
        "D. Sculley",
        "Slav Petrov",
        "Noah Fiedel",
        "Noam Shazeer",
        "Oriol Vinyals",
        "Jeff Dean",
        "Demis Hassabis",
        "Koray Kavukcuoglu",
        "Clement Farabet",
        "Elena Buchatskaya",
        "Jean-Baptiste Alayrac",
        "Rohan Anil",
        "Dmitry",
        "Lepikhin",
        "Sebastian Borgeaud",
        "Olivier Bachem",
        "Armand Joulin",
        "Alek Andreev",
        "Cassidy Hardin",
        "Robert Dadashi",
        "Léonard Hussenot"
      ],
      "abstract": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community.",
      "tldr_zh": "我们介绍了 Gemma 3，这是一个轻量级开源模型家族，从1到27亿参数不等，新增了视觉理解能力、更广泛的语言覆盖以及至少128K tokens的上下文长度。模型通过修改架构，增加局部到全局 attention 层的比率并缩短局部 attention 跨度，以减少 KV-cache 内存消耗，并采用蒸馏训练和创新的后训练配方，提升了数学、聊天、指令遵循和多语言能力。结果显示，Gemma 3 在预训练和指令微调版本中性能优于 Gemma 2，使 Gemma3-4B-IT 竞争于 Gemma2-27B-IT，且 Gemma3-27B-IT 与 Gemini-1.5-Pro 相当；所有模型已向社区发布。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19786v1",
      "published_date": "2025-03-25 15:52:34 UTC",
      "updated_date": "2025-03-25 15:52:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:58:38.718546"
    },
    {
      "arxiv_id": "2503.19948v1",
      "title": "Test-Time Reasoning Through Visual Human Preferences with VLMs and Soft Rewards",
      "title_zh": "翻译失败",
      "authors": [
        "Alexander Gambashidze",
        "Konstantin Sobolev",
        "Andrey Kuznetsov",
        "Ivan Oseledets"
      ],
      "abstract": "Can Visual Language Models (VLMs) effectively capture human visual\npreferences? This work addresses this question by training VLMs to think about\npreferences at test time, employing reinforcement learning methods inspired by\nDeepSeek R1 and OpenAI O1. Using datasets such as ImageReward and Human\nPreference Score v2 (HPSv2), our models achieve accuracies of 64.9% on the\nImageReward test set (trained on ImageReward official split) and 65.4% on HPSv2\n(trained on approximately 25% of its data). These results match traditional\nencoder-based models while providing transparent reasoning and enhanced\ngeneralization. This approach allows to use not only rich VLM world knowledge,\nbut also its potential to think, yielding interpretable outcomes that help\ndecision-making processes. By demonstrating that human visual preferences\nreasonable by current VLMs, we introduce efficient soft-reward strategies for\nimage ranking, outperforming simplistic selection or scoring methods. This\nreasoning capability enables VLMs to rank arbitrary images-regardless of aspect\nratio or complexity-thereby potentially amplifying the effectiveness of visual\nPreference Optimization. By reducing the need for extensive markup while\nimproving reward generalization and explainability, our findings can be a\nstrong mile-stone that will enhance text-to-vision models even further.",
      "tldr_zh": "本研究探讨了 Visual Language Models (VLMs) 是否能有效捕捉人类视觉偏好，通过在测试时采用受 DeepSeek R1 和 OpenAI O1 启发的强化学习方法，并在 ImageReward 和 HPSv2 数据集上训练模型。结果显示，模型在 ImageReward 测试集上达到 64.9% 准确率，在 HPSv2 上达到 65.4%，与传统编码器模型相当，同时提供了透明推理和增强的泛化能力。该方法引入了高效的软奖励策略，实现了对任意图像（无论长宽比或复杂度）的排序，提高了图像排名任务的可解释性和决策有效性，并减少了对大量标记的需求，从而为视觉偏好优化提供了新里程碑。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19948v1",
      "published_date": "2025-03-25 15:30:21 UTC",
      "updated_date": "2025-03-25 15:30:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:58:50.019475"
    },
    {
      "arxiv_id": "2503.19762v1",
      "title": "Splitting Answer Set Programs with respect to Intensionality Statements (Extended Version)",
      "title_zh": "根据内涵",
      "authors": [
        "Jorge Fandinno",
        "Yuliya Lierler"
      ],
      "abstract": "Splitting a logic program allows us to reduce the task of computing its\nstable models to similar tasks for its subprograms. This can be used to\nincrease solving performance and prove program correctness. We generalize the\nconditions under which this technique is applicable, by considering not only\ndependencies between predicates but also their arguments and context. This\nallows splitting programs commonly used in practice to which previous results\nwere not applicable.",
      "tldr_zh": "本论文扩展了 Answer Set Programs 的分割技术，允许根据 Intensionality Statements 将程序分解为子程序，从而简化计算稳定模型（stable models）的任务，并提升求解性能和程序正确性证明。传统方法仅考虑谓词间的依赖，但本研究进一步纳入参数和上下文等因素，使分割技术适用于更多实际程序。总体而言，此扩展为处理复杂逻辑程序提供了更广泛和实用的框架。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Extended version of the paper published in AAAI 2023",
      "pdf_url": "http://arxiv.org/pdf/2503.19762v1",
      "published_date": "2025-03-25 15:27:05 UTC",
      "updated_date": "2025-03-25 15:27:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:59:00.094318"
    },
    {
      "arxiv_id": "2503.19947v1",
      "title": "Vanishing Depth: A Depth Adapter with Positional Depth Encoding for Generalized Image Encoders",
      "title_zh": "Vanishing Depth：一种用于泛化图像编码器的带有位置深度编码的深度适配器",
      "authors": [
        "Paul Koch",
        "Jörg Krüger",
        "Ankit Chowdhury",
        "Oliver Heimann"
      ],
      "abstract": "Generalized metric depth understanding is critical for precise vision-guided\nrobotics, which current state-of-the-art (SOTA) vision-encoders do not support.\nTo address this, we propose Vanishing Depth, a self-supervised training\napproach that extends pretrained RGB encoders to incorporate and align metric\ndepth into their feature embeddings. Based on our novel positional depth\nencoding, we enable stable depth density and depth distribution invariant\nfeature extraction. We achieve performance improvements and SOTA results across\na spectrum of relevant RGBD downstream tasks - without the necessity of\nfinetuning the encoder. Most notably, we achieve 56.05 mIoU on SUN-RGBD\nsegmentation, 88.3 RMSE on Void's depth completion, and 83.8 Top 1 accuracy on\nNYUv2 scene classification. In 6D-object pose estimation, we outperform our\npredecessors of DinoV2, EVA-02, and Omnivore and achieve SOTA results for\nnon-finetuned encoders in several related RGBD downstream tasks.",
      "tldr_zh": "该研究提出 Vanishing Depth，一种自监督训练方法，用于扩展预训练 RGB 编码器，使其能够整合度量深度并对齐特征嵌入。基于新型 positional depth encoding，该方法实现了稳定的深度密度和深度分布不变的特征提取，从而在无需微调编码器的情况下提升 RGBD 下游任务性能。实验结果显示，该方法在 SUN-RGBD 分割任务上达到 56.05 mIoU、在 Void's 深度完成上达到 88.3 RMSE、在 NYUv2 场景分类上达到 83.8 Top 1 准确率，并在 6D 对象姿态估计中超越 DinoV2、EVA-02 和 Omnivore 等模型，实现了 SOTA 水平。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.19947v1",
      "published_date": "2025-03-25 15:19:48 UTC",
      "updated_date": "2025-03-25 15:19:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:59:13.602017"
    },
    {
      "arxiv_id": "2503.19753v2",
      "title": "A Survey on Event-driven 3D Reconstruction: Development under Different Categories",
      "title_zh": "事件驱动三维重建的调查：不同类别下的发展",
      "authors": [
        "Chuanzhi Xu",
        "Haoxian Zhou",
        "Haodong Chen",
        "Vera Chung",
        "Qiang Qu"
      ],
      "abstract": "Event cameras have gained increasing attention for 3D reconstruction due to\ntheir high temporal resolution, low latency, and high dynamic range. They\ncapture per-pixel brightness changes asynchronously, allowing accurate\nreconstruction under fast motion and challenging lighting conditions. In this\nsurvey, we provide a comprehensive review of event-driven 3D reconstruction\nmethods, including stereo, monocular, and multimodal systems. We further\ncategorize recent developments based on geometric, learning-based, and hybrid\napproaches. Emerging trends, such as neural radiance fields and 3D Gaussian\nsplatting with event data, are also covered. The related works are structured\nchronologically to illustrate the innovations and progression within the field.\nTo support future research, we also highlight key research gaps and future\nresearch directions in dataset, experiment, evaluation, event representation,\netc.",
      "tldr_zh": "这篇调查综述了基于事件相机(event cameras)的3D重建技术，强调其高时间分辨率、低延迟和高动态范围的优势，特别适用于快速运动和复杂光照条件下的重建。论文对相关方法进行了全面分类，包括立体(stereo)、单目(monocular)和多模态(multimodal)系统，并进一步细分为几何(geometric)、基于学习(learning-based)和混合(hybrid)方法，同时涵盖新兴趋势如neural radiance fields和3D Gaussian splatting。相关工作按时间顺序组织，展示了领域的创新与进展，并指出了关键研究空白和未来方向，如数据集、实验、评估和事件表示(event representation)。这为推动事件驱动3D重建的研究提供了宝贵参考。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "6 pages, 1 figure, 6 tables, submitted to an anonymous conference\n  under double-blind review",
      "pdf_url": "http://arxiv.org/pdf/2503.19753v2",
      "published_date": "2025-03-25 15:16:53 UTC",
      "updated_date": "2025-03-26 12:34:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:59:25.487238"
    },
    {
      "arxiv_id": "2503.19752v1",
      "title": "Inducing Personality in LLM-Based Honeypot Agents: Measuring the Effect on Human-Like Agenda Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Lewis Newsham",
        "Ryan Hyland",
        "Daniel Prince"
      ],
      "abstract": "This paper presents SANDMAN, an architecture for cyber deception that\nleverages Language Agents to emulate convincing human simulacra. Our 'Deceptive\nAgents' serve as advanced cyber decoys, designed for high-fidelity engagement\nwith attackers by extending the observation period of attack behaviours.\nThrough experimentation, measurement, and analysis, we demonstrate how a prompt\nschema based on the five-factor model of personality systematically induces\ndistinct 'personalities' in Large Language Models. Our results highlight the\nfeasibility of persona-driven Language Agents for generating diverse, realistic\nbehaviours, ultimately improving cyber deception strategies.",
      "tldr_zh": "这篇论文介绍了 SANDMAN 架构，一种利用 Language Agents 模拟人类行为的网络欺骗系统，作为高级 cyber decoys 来延长攻击者观察期。研究通过基于 five-factor model 的提示方案，系统地诱导 Large Language Models (LLM) 中的不同 personalities，以生成多样且真实的 behaviors。实验结果证明，这种方法显著提高了 cyber deception 策略的有效性。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 1 figure, 6 tables. Accepted to NLPAICS 2024",
      "pdf_url": "http://arxiv.org/pdf/2503.19752v1",
      "published_date": "2025-03-25 15:16:35 UTC",
      "updated_date": "2025-03-25 15:16:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:59:36.678947"
    },
    {
      "arxiv_id": "2503.19730v2",
      "title": "CamSAM2: Segment Anything Accurately in Camouflaged Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Yuli Zhou",
        "Guolei Sun",
        "Yawei Li",
        "Yuqian Fu",
        "Luca Benini",
        "Ender Konukoglu"
      ],
      "abstract": "Video camouflaged object segmentation (VCOS), aiming at segmenting\ncamouflaged objects that seamlessly blend into their environment, is a\nfundamental vision task with various real-world applications. With the release\nof SAM2, video segmentation has witnessed significant progress. However, SAM2's\ncapability of segmenting camouflaged videos is suboptimal, especially when\ngiven simple prompts such as point and box. To address the problem, we propose\nCamouflaged SAM2 (CamSAM2), which enhances SAM2's ability to handle camouflaged\nscenes without modifying SAM2's parameters. Specifically, we introduce a\ndecamouflaged token to provide the flexibility of feature adjustment for VCOS.\nTo make full use of fine-grained and high-resolution features from the current\nframe and previous frames, we propose implicit object-aware fusion (IOF) and\nexplicit object-aware fusion (EOF) modules, respectively. Object prototype\ngeneration (OPG) is introduced to abstract and memorize object prototypes with\ninformative details using high-quality features from previous frames. Extensive\nexperiments are conducted to validate the effectiveness of our approach. While\nCamSAM2 only adds negligible learnable parameters to SAM2, it substantially\noutperforms SAM2 on three VCOS datasets, especially achieving 12.2 mDice gains\nwith click prompt on MoCA-Mask and 19.6 mDice gains with mask prompt on\nSUN-SEG-Hard, with Hiera-T as the backbone. The code will be available at\nhttps://github.com/zhoustan/CamSAM2.",
      "tldr_zh": "该论文提出 CamSAM2，一种增强版 SAM2 方法，用于精确分割视频伪装物体 (VCOS)，以解决 SAM2 在处理伪装场景时的不足，而无需修改其参数。具体地，CamSAM2 引入 decamouflaged token 来调整特征，并通过 implicit object-aware fusion (IOF) 和 explicit object-aware fusion (EOF) 模块融合当前帧和高分辨率前帧特征，以及 object prototype generation (OPG) 来抽象和记忆物体原型，从而提升分割准确性。实验结果显示，CamSAM2 在三个 VCOS 数据集上大幅优于 SAM2，例如在 MoCA-Mask 上以点击提示获得 12.2 mDice 提升，在 SUN-SEG-Hard 上以掩码提示获得 19.6 mDice 提升，仅添加了微不足道的可学习参数。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19730v2",
      "published_date": "2025-03-25 14:58:52 UTC",
      "updated_date": "2025-03-26 02:14:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T05:59:51.235564"
    },
    {
      "arxiv_id": "2503.19719v1",
      "title": "On What Depends the Robustness of Multi-source Models to Missing Data in Earth Observation?",
      "title_zh": "多源模型对地球观测中缺失数据的鲁棒性取决于什么？",
      "authors": [
        "Francisco Mena",
        "Diego Arenas",
        "Miro Miranda",
        "Andreas Dengel"
      ],
      "abstract": "In recent years, the development of robust multi-source models has emerged in\nthe Earth Observation (EO) field. These are models that leverage data from\ndiverse sources to improve predictive accuracy when there is missing data.\nDespite these advancements, the factors influencing the varying effectiveness\nof such models remain poorly understood. In this study, we evaluate the\npredictive performance of six state-of-the-art multi-source models in\npredicting scenarios where either a single data source is missing or only a\nsingle source is available. Our analysis reveals that the efficacy of these\nmodels is intricately tied to the nature of the task, the complementarity among\ndata sources, and the model design. Surprisingly, we observe instances where\nthe removal of certain data sources leads to improved predictive performance,\nchallenging the assumption that incorporating all available data is always\nbeneficial. These findings prompt critical reflections on model complexity and\nthe necessity of all collected data sources, potentially shaping the way for\nmore streamlined approaches in EO applications.",
      "tldr_zh": "本文研究了 Earth Observation (EO) 领域中多源模型对数据缺失的鲁棒性影响因素，这些模型利用多种数据来源提升预测准确性。研究者评估了六种 state-of-the-art 多源模型在单一数据源缺失或仅单一源可用场景下的预测性能。结果显示，模型效能取决于任务性质、数据来源之间的互补性以及模型设计，令人意外的是，移除某些数据源有时反而能改善预测性能。这些发现促使对模型复杂性和数据来源必要性的反思，可能推动 EO 应用采用更简化的策略。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.19719v1",
      "published_date": "2025-03-25 14:45:23 UTC",
      "updated_date": "2025-03-25 14:45:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:00:01.706655"
    },
    {
      "arxiv_id": "2503.19717v1",
      "title": "Invertible Koopman neural operator for data-driven modeling of partial differential equations",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhong Jin",
        "Andong Cong",
        "Lei Hou",
        "Qiang Gao",
        "Xiangdong Ge",
        "Chonglong Zhu",
        "Yongzhi Feng",
        "Jun Li"
      ],
      "abstract": "Koopman operator theory is a popular candidate for data-driven modeling\nbecause it provides a global linearization representation for nonlinear\ndynamical systems. However, existing Koopman operator-based methods suffer from\nshortcomings in constructing the well-behaved observable function and its\ninverse and are inefficient enough when dealing with partial differential\nequations (PDEs). To address these issues, this paper proposes the Invertible\nKoopman Neural Operator (IKNO), a novel data-driven modeling approach inspired\nby the Koopman operator theory and neural operator. IKNO leverages an\nInvertible Neural Network to parameterize observable function and its inverse\nsimultaneously under the same learnable parameters, explicitly guaranteeing the\nreconstruction relation, thus eliminating the dependency on the reconstruction\nloss, which is an essential improvement over the original Koopman Neural\nOperator (KNO). The structured linear matrix inspired by the Koopman operator\ntheory is parameterized to learn the evolution of observables' low-frequency\nmodes in the frequency space rather than directly in the observable space,\nsustaining IKNO is resolution-invariant like other neural operators. Moreover,\nwith preprocessing such as interpolation and dimension expansion, IKNO can be\nextended to operator learning tasks defined on non-Cartesian domains. We fully\nsupport the above claims based on rich numerical and real-world examples and\ndemonstrate the effectiveness of IKNO and superiority over other neural\noperators.",
      "tldr_zh": "本研究针对现有Koopman operator-based方法在构建可观测函数及其逆函数时存在的缺陷，以及处理偏微分方程(PDEs)时的低效率问题，提出了一种新型数据驱动建模方法：Invertible Koopman Neural Operator (IKNO)。IKNO通过Invertible Neural Network同时参数化可观测函数及其逆函数，使用相同的可学习参数来确保重建关系，从而无需依赖重建损失，这是对原Koopman Neural Operator (KNO)的显著改进。该方法还采用受Koopman operator启发的结构化线性矩阵，在频率空间学习可观测量的低频模式，实现分辨率不变性，并通过插值和维度扩展适用于非笛卡尔域。实验结果显示，IKNO在丰富的数值和真实世界例子中表现出色，优于其他神经算子。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.19717v1",
      "published_date": "2025-03-25 14:43:53 UTC",
      "updated_date": "2025-03-25 14:43:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:00:14.712107"
    },
    {
      "arxiv_id": "2503.19712v1",
      "title": "Decoupled Dynamics Framework with Neural Fields for 3D Spatio-temporal Prediction of Vehicle Collisions",
      "title_zh": "基于神经场的解耦动力学框架，用于车辆碰撞的3D 时空预测",
      "authors": [
        "Sanghyuk Kim",
        "Minsik Seo",
        "Namwoo Kang"
      ],
      "abstract": "This study proposes a neural framework that predicts 3D vehicle collision\ndynamics by independently modeling global rigid-body motion and local\nstructural deformation. Unlike approaches directly predicting absolute\ndisplacement, this method explicitly separates the vehicle's overall\ntranslation and rotation from its structural deformation. Two specialized\nnetworks form the core of the framework: a quaternion-based Rigid Net for rigid\nmotion and a coordinate-based Deformation Net for local deformation. By\nindependently handling fundamentally distinct physical phenomena, the proposed\narchitecture achieves accurate predictions without requiring separate\nsupervision for each component. The model, trained on only 10% of available\nsimulation data, significantly outperforms baseline models, including single\nmulti-layer perceptron (MLP) and deep operator networks (DeepONet), with\nprediction errors reduced by up to 83%. Extensive validation demonstrates\nstrong generalization to collision conditions outside the training range,\naccurately predicting responses even under severe impacts involving extreme\nvelocities and large impact angles. Furthermore, the framework successfully\nreconstructs high-resolution deformation details from low-resolution inputs\nwithout increased computational effort. Consequently, the proposed approach\nprovides an effective, computationally efficient method for rapid and reliable\nassessment of vehicle safety across complex collision scenarios, substantially\nreducing the required simulation data and time while preserving prediction\nfidelity.",
      "tldr_zh": "这篇论文提出了一种 Decoupled Dynamics Framework，使用 Neural Fields 来预测 3D 车辆碰撞的时空动态，通过独立建模全局刚体运动和局部结构变形，避免直接预测绝对位移的局限。框架的核心包括基于四元数的 Rigid Net 处理整体平移和旋转，以及基于坐标的 Deformation Net 处理局部变形，从而实现准确预测而不需单独监督。实验结果显示，该模型仅使用 10% 的模拟数据，就比基线模型如 MLP 和 DeepONet 减少预测错误高达 83%，并在训练范围外的极端碰撞条件下表现出强泛化能力。总之，该方法提供了一种高效、计算资源友好的方式，用于快速评估车辆安全，显著减少模拟数据需求的同时保持高预测精度。",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "24 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.19712v1",
      "published_date": "2025-03-25 14:38:37 UTC",
      "updated_date": "2025-03-25 14:38:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:00:26.447502"
    },
    {
      "arxiv_id": "2503.19711v1",
      "title": "Writing as a testbed for open ended agents",
      "title_zh": "翻译失败",
      "authors": [
        "Sian Gooding",
        "Lucia Lopez-Rivilla",
        "Edward Grefenstette"
      ],
      "abstract": "Open-ended tasks are particularly challenging for LLMs due to the vast\nsolution space, demanding both expansive exploration and adaptable strategies,\nespecially when success lacks a clear, objective definition. Writing, with its\nvast solution space and subjective evaluation criteria, provides a compelling\ntestbed for studying such problems. In this paper, we investigate the potential\nof LLMs to act as collaborative co-writers, capable of suggesting and\nimplementing text improvements autonomously. We analyse three prominent LLMs -\nGemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o - focusing on how their action\ndiversity, human alignment, and iterative improvement capabilities impact\noverall performance. This work establishes a framework for benchmarking\nautonomous writing agents and, more broadly, highlights fundamental challenges\nand potential solutions for building systems capable of excelling in diverse\nopen-ended domains.",
      "tldr_zh": "本研究将写作视为一个理想的测试平台，用于评估大型语言模型（LLMs）在开放式任务中的表现，这些任务涉及广阔的解决方案空间、主观评估标准以及探索和适应策略的需求。作者分析了 Gemini 1.5 Pro、Claude 3.5 Sonnet 和 GPT-4o 这三个 LLMs 作为自主协作共同作者的能力，重点考察它们的行动多样性、人性化对齐以及迭代改进对整体性能的影响。通过建立一个基准框架，该工作不仅为评估自主写作代理提供了工具，还揭示了构建适用于多样开放式领域的系统所面临的根本挑战和潜在解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19711v1",
      "published_date": "2025-03-25 14:38:36 UTC",
      "updated_date": "2025-03-25 14:38:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:00:37.098959"
    },
    {
      "arxiv_id": "2503.19706v2",
      "title": "Bootstrap Your Own Views: Masked Ego-Exo Modeling for Fine-grained View-invariant Video Representations",
      "title_zh": "翻译失败",
      "authors": [
        "Jungin Park",
        "Jiyoung Lee",
        "Kwanghoon Sohn"
      ],
      "abstract": "View-invariant representation learning from egocentric (first-person, ego)\nand exocentric (third-person, exo) videos is a promising approach toward\ngeneralizing video understanding systems across multiple viewpoints. However,\nthis area has been underexplored due to the substantial differences in\nperspective, motion patterns, and context between ego and exo views. In this\npaper, we propose a novel masked ego-exo modeling that promotes both causal\ntemporal dynamics and cross-view alignment, called Bootstrap Your Own Views\n(BYOV), for fine-grained view-invariant video representation learning from\nunpaired ego-exo videos. We highlight the importance of capturing the\ncompositional nature of human actions as a basis for robust cross-view\nunderstanding. Specifically, self-view masking and cross-view masking\npredictions are designed to learn view-invariant and powerful representations\nconcurrently. Experimental results demonstrate that our BYOV significantly\nsurpasses existing approaches with notable gains across all metrics in four\ndownstream ego-exo video tasks. The code is available at\nhttps://github.com/park-jungin/byov.",
      "tldr_zh": "本论文提出了一种名为 BYOV（Bootstrap Your Own Views）的 masked ego-exo modeling 方法，用于从未配对的 ego（第一人称）和 exo（第三人称）视频中学习细粒度的视图不变视频表示，以解决视角差异、运动模式和上下文问题。BYOV 通过 self-view masking 和 cross-view masking 预测来促进因果时间动态和跨视图对齐，同时捕捉人类动作的组合性质。实验结果显示，该方法在四个下游 ego-exo 视频任务中显著优于现有方法，在所有指标上均取得显著提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025 Camera-ready, 18 pages, 7 figures, 9 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.19706v2",
      "published_date": "2025-03-25 14:33:32 UTC",
      "updated_date": "2025-03-31 08:46:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:00:49.865049"
    },
    {
      "arxiv_id": "2503.19699v1",
      "title": "Optimal Path Planning and Cost Minimization for a Drone Delivery System Via Model Predictive Control",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Al-Zafar Khan",
        "Jamal Al-Karaki"
      ],
      "abstract": "In this study, we formulate the drone delivery problem as a control problem\nand solve it using Model Predictive Control. Two experiments are performed: The\nfirst is on a less challenging grid world environment with lower\ndimensionality, and the second is with a higher dimensionality and added\ncomplexity. The MPC method was benchmarked against three popular Multi-Agent\nReinforcement Learning (MARL): Independent $Q$-Learning (IQL), Joint Action\nLearners (JAL), and Value-Decomposition Networks (VDN). It was shown that the\nMPC method solved the problem quicker and required fewer optimal numbers of\ndrones to achieve a minimized cost and navigate the optimal path.",
      "tldr_zh": "本研究将无人机交付问题表述为控制问题，并采用 Model Predictive Control (MPC) 方法来进行路径规划和成本最小化。实验包括一个低维网格世界环境和一个高维复杂环境，MPC 与三种多智能体强化学习方法（Independent Q-Learning (IQL)、Joint Action Learners (JAL) 和 Value-Decomposition Networks (VDN)）进行了比较。结果表明，MPC 方法更快地解决问题，并需要更少的无人机数量来实现成本最小化和最优路径导航，从而为高效无人机系统提供了实用解决方案。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 5 figures, Submitted to the 2025 International Conference\n  on Artificial Intelligence, Computer, Data Sciences and Applications",
      "pdf_url": "http://arxiv.org/pdf/2503.19699v1",
      "published_date": "2025-03-25 14:27:29 UTC",
      "updated_date": "2025-03-25 14:27:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:01:01.179467"
    },
    {
      "arxiv_id": "2503.19677v1",
      "title": "Deep Learning for Speech Emotion Recognition: A CNN Approach Utilizing Mel Spectrograms",
      "title_zh": "深度学习",
      "authors": [
        "Niketa Penumajji"
      ],
      "abstract": "This paper explores the application of Convolutional Neural Networks CNNs for\nclassifying emotions in speech through Mel Spectrogram representations of audio\nfiles. Traditional methods such as Gaussian Mixture Models and Hidden Markov\nModels have proven insufficient for practical deployment, prompting a shift\ntowards deep learning techniques. By transforming audio data into a visual\nformat, the CNN model autonomously learns to identify intricate patterns,\nenhancing classification accuracy. The developed model is integrated into a\nuser-friendly graphical interface, facilitating realtime predictions and\npotential applications in educational environments. The study aims to advance\nthe understanding of deep learning in speech emotion recognition, assess the\nmodels feasibility, and contribute to the integration of technology in learning\ncontexts",
      "tldr_zh": "本研究探讨了使用卷积神经网络 (CNNs) 通过 Mel Spectrograms 来识别语音中的情感，解决了传统方法如 Gaussian Mixture Models 和 Hidden Markov Models 在实际部署中的不足。方法涉及将音频数据转换为视觉格式的 Mel Spectrograms，让 CNN 模型自动学习并识别复杂情感模式，从而提升分类准确性。该模型集成到用户友好的图形界面中，支持实时预测，并应用于教育环境，以推进深度学习在语音情感识别领域的理解和可行性。",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "5 pages 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.19677v1",
      "published_date": "2025-03-25 14:02:10 UTC",
      "updated_date": "2025-03-25 14:02:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:01:12.438035"
    },
    {
      "arxiv_id": "2503.19658v1",
      "title": "BiblioPage: A Dataset of Scanned Title Pages for Bibliographic Metadata Extraction",
      "title_zh": "翻译失败",
      "authors": [
        "Jan Kohút",
        "Martin Dočekal",
        "Michal Hradiš",
        "Marek Vaško"
      ],
      "abstract": "Manual digitization of bibliographic metadata is time consuming and labor\nintensive, especially for historical and real-world archives with highly\nvariable formatting across documents. Despite advances in machine learning, the\nabsence of dedicated datasets for metadata extraction hinders automation. To\naddress this gap, we introduce BiblioPage, a dataset of scanned title pages\nannotated with structured bibliographic metadata. The dataset consists of\napproximately 2,000 monograph title pages collected from 14 Czech libraries,\nspanning a wide range of publication periods, typographic styles, and layout\nstructures. Each title page is annotated with 16 bibliographic attributes,\nincluding title, contributors, and publication metadata, along with precise\npositional information in the form of bounding boxes. To extract structured\ninformation from this dataset, we valuated object detection models such as YOLO\nand DETR combined with transformer-based OCR, achieving a maximum mAP of 52 and\nan F1 score of 59. Additionally, we assess the performance of various visual\nlarge language models, including LlamA 3.2-Vision and GPT-4o, with the best\nmodel reaching an F1 score of 67. BiblioPage serves as a real-world benchmark\nfor bibliographic metadata extraction, contributing to document understanding,\ndocument question answering, and document information extraction. Dataset and\nevaluation scripts are availible at: https://github.com/DCGM/biblio-dataset",
      "tldr_zh": "本研究引入了 BiblioPage 数据集，这是一个针对扫描书页的书目元数据（bibliographic metadata）提取工具，旨在解决手动数字化过程耗时费力的问题，尤其是针对历史档案的多样化格式。数据集包含约 2,000 个从 14 个捷克图书馆收集的专著标题页，每页标注了 16 个书目属性（如标题、贡献者）和精确边界框（bounding boxes）。通过评估物体检测模型（如 YOLO 和 DETR）结合 transformer-based OCR，以及视觉大语言模型（如 Llama 3.2-Vision 和 GPT-4o），研究取得了最高 mAP 52 和 F1 score 67 的性能。BiblioPage 作为真实世界基准，可促进文档理解（document understanding）、问答和信息提取，并提供开源数据集和评估脚本。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to ICDAR2025 conference",
      "pdf_url": "http://arxiv.org/pdf/2503.19658v1",
      "published_date": "2025-03-25 13:46:55 UTC",
      "updated_date": "2025-03-25 13:46:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:01:26.034401"
    },
    {
      "arxiv_id": "2503.19656v1",
      "title": "Towards Reliable Time Series Forecasting under Future Uncertainty: Ambiguity and Novelty Rejection Mechanisms",
      "title_zh": "翻译失败",
      "authors": [
        "Ninghui Feng",
        "Songning Lai",
        "Xin Zhou",
        "Jiayu Yang",
        "Kunlong Feng",
        "Zhenxiao Yin",
        "Fobao Zhou",
        "Zhangyi Hu",
        "Yutao Yue",
        "Yuxuan Liang",
        "Boyu Wang",
        "Hang Zhao"
      ],
      "abstract": "In real-world time series forecasting, uncertainty and lack of reliable\nevaluation pose significant challenges. Notably, forecasting errors often arise\nfrom underfitting in-distribution data and failing to handle\nout-of-distribution inputs. To enhance model reliability, we introduce a dual\nrejection mechanism combining ambiguity and novelty rejection. Ambiguity\nrejection, using prediction error variance, allows the model to abstain under\nlow confidence, assessed through historical error variance analysis without\nfuture ground truth. Novelty rejection, employing Variational Autoencoders and\nMahalanobis distance, detects deviations from training data. This dual approach\nimproves forecasting reliability in dynamic environments by reducing errors and\nadapting to data changes, advancing reliability in complex scenarios.",
      "tldr_zh": "该论文针对时间序列预测中的不确定性和分布外输入问题，提出了一种双重拒绝机制，包括模糊拒绝（ambiguity rejection）和新颖拒绝（novelty rejection）。模糊拒绝机制利用预测错误方差（prediction error variance）通过历史错误方差分析，让模型在置信度低时弃权，而不依赖未来真实值；新颖拒绝机制则采用Variational Autoencoders和Mahalanobis distance检测训练数据偏差。实验结果表明，该方法显著提升了预测可靠性，减少错误并适应动态环境变化。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19656v1",
      "published_date": "2025-03-25 13:44:29 UTC",
      "updated_date": "2025-03-25 13:44:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:01:37.278745"
    },
    {
      "arxiv_id": "2503.19654v3",
      "title": "RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of Vision Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mehdi Moshtaghi",
        "Siavash H. Khajavi",
        "Joni Pajarinen"
      ],
      "abstract": "We introduce RGB-Th-Bench, the first benchmark designed to evaluate the\nability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs.\nWhile VLMs have demonstrated remarkable progress in visual reasoning and\nmultimodal understanding, their evaluation has been predominantly limited to\nRGB-based benchmarks, leaving a critical gap in assessing their capabilities in\ninfrared vision tasks. Existing visible-infrared datasets are either\ntask-specific or lack high-quality annotations necessary for rigorous model\nevaluation. To address these limitations, RGB-Th-Bench provides a comprehensive\nevaluation framework covering 14 distinct skill dimensions, with a total of\n1,600+ expert-annotated Yes/No questions. The benchmark employs two accuracy\nmetrics: a standard question-level accuracy and a stricter skill-level\naccuracy, which evaluates model robustness across multiple questions within\neach skill dimension. This design ensures a thorough assessment of model\nperformance, including resilience to adversarial and hallucinated responses. We\nconduct extensive evaluations on 19 state-of-the-art VLMs, revealing\nsignificant performance gaps in RGB-Thermal understanding. Our results show\nthat even the strongest models struggle with thermal image comprehension, with\nperformance heavily constrained by their RGB-based capabilities. Additionally,\nthe lack of large-scale application-specific and expert-annotated\nthermal-caption-pair datasets in pre-training is an important reason of the\nobserved performance gap. RGB-Th-Bench highlights the urgent need for further\nadvancements in multimodal learning to bridge the gap between visible and\nthermal image understanding. The dataset is available through this link, and\nthe evaluation code will also be made publicly available.",
      "tldr_zh": "我们引入了 RGB-Th-Bench，这是第一个专门评估 Vision-Language Models (VLMs) 对 RGB-Thermal 图像对理解能力的基准，填补了现有 RGB 基准的评估空白。该基准涵盖 14 个技能维度，共 1600+ 专家标注的 Yes/No 问题，并采用标准问题级准确性和更严格的技能级准确性指标，以全面评估模型的鲁棒性和抗幻觉能力。通过对 19 个最先进 VLMs 的实验，我们发现这些模型在热成像理解上存在显著性能差距，主要源于预训练数据中缺乏大规模热成像-标题对，从而强调了推进多模态学习以桥接可见光和热成像理解的迫切需求。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19654v3",
      "published_date": "2025-03-25 13:43:47 UTC",
      "updated_date": "2025-03-30 15:08:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:01:50.745109"
    },
    {
      "arxiv_id": "2503.19653v3",
      "title": "OpenSDI: Spotting Diffusion-Generated Images in the Open World",
      "title_zh": "翻译失败",
      "authors": [
        "Yabin Wang",
        "Zhiwu Huang",
        "Xiaopeng Hong"
      ],
      "abstract": "This paper identifies OpenSDI, a challenge for spotting diffusion-generated\nimages in open-world settings. In response to this challenge, we define a new\nbenchmark, the OpenSDI dataset (OpenSDID), which stands out from existing\ndatasets due to its diverse use of large vision-language models that simulate\nopen-world diffusion-based manipulations. Another outstanding feature of\nOpenSDID is its inclusion of both detection and localization tasks for images\nmanipulated globally and locally by diffusion models. To address the OpenSDI\nchallenge, we propose a Synergizing Pretrained Models (SPM) scheme to build up\na mixture of foundation models. This approach exploits a collaboration\nmechanism with multiple pretrained foundation models to enhance generalization\nin the OpenSDI context, moving beyond traditional training by synergizing\nmultiple pretrained models through prompting and attending strategies. Building\non this scheme, we introduce MaskCLIP, an SPM-based model that aligns\nContrastive Language-Image Pre-Training (CLIP) with Masked Autoencoder (MAE).\nExtensive evaluations on OpenSDID show that MaskCLIP significantly outperforms\ncurrent state-of-the-art methods for the OpenSDI challenge, achieving\nremarkable relative improvements of 14.23% in IoU (14.11% in F1) and 2.05% in\naccuracy (2.38% in F1) compared to the second-best model in localization and\ndetection tasks, respectively. Our dataset and code are available at\nhttps://github.com/iamwangyabin/OpenSDI.",
      "tldr_zh": "该论文针对OpenSDI挑战，即在开放世界环境中识别扩散生成图像，提出了一种新的基准数据集OpenSDID，该数据集利用大型视觉语言模型模拟多样化的扩散操作，支持图像的全局和局部检测及定位任务。作者引入了Synergizing Pretrained Models (SPM)方案，通过提示和关注策略协同多个预训练基础模型，如将Contrastive Language-Image Pre-Training (CLIP)与Masked Autoencoder (MAE)对齐，构建了MaskCLIP模型，以提升泛化能力。在OpenSDID数据集上的评估显示，MaskCLIP在定位任务中IoU和F1分数分别相对提升14.23%和14.11%，在检测任务中准确率和F1分数分别提升2.05%和2.38%，显著优于现有方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19653v3",
      "published_date": "2025-03-25 13:43:16 UTC",
      "updated_date": "2025-04-16 08:07:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:02:02.311083"
    },
    {
      "arxiv_id": "2503.19650v1",
      "title": "HausaNLP at SemEval-2025 Task 3: Towards a Fine-Grained Model-Aware Hallucination Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Maryam Bala",
        "Amina Imam Abubakar",
        "Abdulhamid Abubakar",
        "Abdulkadir Shehu Bichi",
        "Hafsa Kabir Ahmad",
        "Sani Abdullahi Sani",
        "Idris Abdulmumin",
        "Shamsuddeen Hassan Muhamad",
        "Ibrahim Said Ahmad"
      ],
      "abstract": "This paper presents our findings of the Multilingual Shared Task on\nHallucinations and Related Observable Overgeneration Mistakes, MU-SHROOM, which\nfocuses on identifying hallucinations and related overgeneration errors in\nlarge language models (LLMs). The shared task involves detecting specific text\nspans that constitute hallucinations in the outputs generated by LLMs in 14\nlanguages. To address this task, we aim to provide a nuanced, model-aware\nunderstanding of hallucination occurrences and severity in English. We used\nnatural language inference and fine-tuned a ModernBERT model using a synthetic\ndataset of 400 samples, achieving an Intersection over Union (IoU) score of\n0.032 and a correlation score of 0.422. These results indicate a moderately\npositive correlation between the model's confidence scores and the actual\npresence of hallucinations. The IoU score indicates that our model has a\nrelatively low overlap between the predicted hallucination span and the truth\nannotation. The performance is unsurprising, given the intricate nature of\nhallucination detection. Hallucinations often manifest subtly, relying on\ncontext, making pinpointing their exact boundaries formidable.",
      "tldr_zh": "这篇论文介绍了HausaNLP团队在SemEval-2025 Task 3的多语言共享任务MU-SHROOM中的研究，专注于检测大型语言模型(LLMs)输出中的hallucinations和相关过生成错误，特别是针对英语的细粒度模型感知检测。研究团队使用natural language inference方法，并通过一个包含400个样本的合成数据集微调ModernBERT模型，以识别幻觉文本跨度。结果显示，模型的IoU分数为0.032，相关性分数为0.422，表明模型置信度与实际hallucinations存在中等正相关，但预测跨度与真实标注的重叠较低。这些发现突显了hallucinations检测的复杂性，因为它们往往依赖上下文而难以精确界定，为未来改进模型感知提供了参考。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19650v1",
      "published_date": "2025-03-25 13:40:22 UTC",
      "updated_date": "2025-03-25 13:40:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:02:13.558309"
    },
    {
      "arxiv_id": "2503.19649v1",
      "title": "Recover from Horcrux: A Spectrogram Augmentation Method for Cardiac Feature Monitoring from Radar Signal Components",
      "title_zh": "翻译失败",
      "authors": [
        "Yuanyuan Zhang",
        "Sijie Xiong",
        "Rui Yang",
        "EngGee Lim",
        "Yutao Yue"
      ],
      "abstract": "Radar-based wellness monitoring is becoming an effective measurement to\nprovide accurate vital signs in a contactless manner, but data scarcity retards\nthe related research on deep-learning-based methods. Data augmentation is\ncommonly used to enrich the dataset by modifying the existing data, but most\naugmentation techniques can only couple with classification tasks. To enable\nthe augmentation for regression tasks, this research proposes a spectrogram\naugmentation method, Horcrux, for radar-based cardiac feature monitoring (e.g.,\nheartbeat detection, electrocardiogram reconstruction) with both classification\nand regression tasks involved. The proposed method is designed to increase the\ndiversity of input samples while the augmented spectrogram is still faithful to\nthe original ground truth vital sign. In addition, Horcrux proposes to inject\nzero values in specific areas to enhance the awareness of the deep learning\nmodel on subtle cardiac features, improving the performance for the limited\ndataset. Experimental result shows that Horcrux achieves an overall improvement\nof 16.20% in cardiac monitoring and has the potential to be extended to other\nspectrogram-based tasks. The code will be released upon publication.",
      "tldr_zh": "该研究针对雷达-based 健康监测中数据稀缺的问题，提出了一种频谱图（spectrogram）增强方法 Horcrux，用于心脏特征监测（如心跳检测和心电图重建），适用于分类和回归任务。Horcrux 通过增加输入样本的多样性，同时确保增强后的频谱图忠实于原始生命体征，并通过在特定区域注入零值来提升深度学习模型对微妙心脏特征的感知，从而改善有限数据集的性能。实验结果显示，该方法在心脏监测中整体提高了 16.20% 的准确率，并具有扩展到其他 spectrogram-based 任务的潜力。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19649v1",
      "published_date": "2025-03-25 13:40:05 UTC",
      "updated_date": "2025-03-25 13:40:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:02:25.640867"
    },
    {
      "arxiv_id": "2503.19647v1",
      "title": "Show or Tell? Effectively prompting Vision-Language Models for semantic segmentation",
      "title_zh": "展示还是讲述？ 有效地提示视觉语言模型用于语义分割",
      "authors": [
        "Niccolo Avogaro",
        "Thomas Frick",
        "Mattia Rigotti",
        "Andrea Bartezzaghi",
        "Filip Janicki",
        "Cristiano Malossi",
        "Konrad Schindler",
        "Roy Assaf"
      ],
      "abstract": "Large Vision-Language Models (VLMs) are increasingly being regarded as\nfoundation models that can be instructed to solve diverse tasks by prompting,\nwithout task-specific training. We examine the seemingly obvious question: how\nto effectively prompt VLMs for semantic segmentation. To that end, we\nsystematically evaluate the segmentation performance of several recent models\nguided by either text or visual prompts on the out-of-distribution MESS dataset\ncollection. We introduce a scalable prompting scheme, few-shot prompted\nsemantic segmentation, inspired by open-vocabulary segmentation and few-shot\nlearning. It turns out that VLMs lag far behind specialist models trained for a\nspecific segmentation task, by about 30% on average on the\nIntersection-over-Union metric. Moreover, we find that text prompts and visual\nprompts are complementary: each one of the two modes fails on many examples\nthat the other one can solve. Our analysis suggests that being able to\nanticipate the most effective prompt modality can lead to a 11% improvement in\nperformance. Motivated by our findings, we propose PromptMatcher, a remarkably\nsimple training-free baseline that combines both text and visual prompts,\nachieving state-of-the-art results outperforming the best text-prompted VLM by\n2.5%, and the top visual-prompted VLM by 3.5% on few-shot prompted semantic\nsegmentation.",
      "tldr_zh": "这篇论文探讨了如何有效提示 Vision-Language Models (VLMs) 来实现 semantic segmentation，通过系统评估文本和视觉提示在 MESS 数据集上的性能。研究发现，VLMs 的分割准确率（IoU 指标）平均落后于专业模型约 30%，且文本提示和视觉提示互补，能够在各自失败的示例中互为补充，从而潜在提高 11% 的性能。作者提出 PromptMatcher，一种无需训练的简单基线方法，结合两种提示模式，实现了 state-of-the-art 结果，比最佳文本提示 VLM 高 2.5%，比顶级视觉提示 VLM 高 3.5%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19647v1",
      "published_date": "2025-03-25 13:36:59 UTC",
      "updated_date": "2025-03-25 13:36:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:02:37.891751"
    },
    {
      "arxiv_id": "2503.19611v1",
      "title": "Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music Generation",
      "title_zh": "可分析的音乐链式思维提示技术用于高保",
      "authors": [
        "Max W. Y. Lam",
        "Yijin Xing",
        "Weiya You",
        "Jingcheng Wu",
        "Zongyu Yin",
        "Fuqiang Jiang",
        "Hangyu Liu",
        "Feng Liu",
        "Xingda Li",
        "Wei-Tsung Lu",
        "Hanyu Chen",
        "Tong Feng",
        "Tianwei Zhao",
        "Chien-Hung Liu",
        "Xuchen Song",
        "Yang Li",
        "Yahui Zhou"
      ],
      "abstract": "Autoregressive (AR) models have demonstrated impressive capabilities in\ngenerating high-fidelity music. However, the conventional next-token prediction\nparadigm in AR models does not align with the human creative process in music\ncomposition, potentially compromising the musicality of generated samples. To\novercome this limitation, we introduce MusiCoT, a novel chain-of-thought (CoT)\nprompting technique tailored for music generation. MusiCoT empowers the AR\nmodel to first outline an overall music structure before generating audio\ntokens, thereby enhancing the coherence and creativity of the resulting\ncompositions. By leveraging the contrastive language-audio pretraining (CLAP)\nmodel, we establish a chain of \"musical thoughts\", making MusiCoT scalable and\nindependent of human-labeled data, in contrast to conventional CoT methods.\nMoreover, MusiCoT allows for in-depth analysis of music structure, such as\ninstrumental arrangements, and supports music referencing -- accepting\nvariable-length audio inputs as optional style references. This innovative\napproach effectively addresses copying issues, positioning MusiCoT as a vital\npractical method for music prompting. Our experimental results indicate that\nMusiCoT consistently achieves superior performance across both objective and\nsubjective metrics, producing music quality that rivals state-of-the-art\ngeneration models.\n  Our samples are available at https://MusiCoT.github.io/.",
      "tldr_zh": "这篇论文提出了 MusiCoT，一种基于 Chain-of-Thought (CoT) 的提示技术，用于提升 Autoregressive (AR) 模型在高保真音乐生成中的表现，通过先规划整体音乐结构再生成音频 tokens，从而改善音乐的连贯性和创造力。MusiCoT 利用 Contrastive Language-Audio Pretraining (CLAP) 模型构建“musical thoughts”链，使其无需人类标注数据即可实现可扩展性，并支持音乐结构分析（如乐器安排）和音乐引用功能，以解决复制问题。实验结果显示，MusiCoT 在客观和主观指标上优于基线模型，生成的音乐质量可与最先进模型媲美。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS",
        "eess.SP"
      ],
      "primary_category": "cs.SD",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.19611v1",
      "published_date": "2025-03-25 12:51:21 UTC",
      "updated_date": "2025-03-25 12:51:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:02:50.090649"
    },
    {
      "arxiv_id": "2503.19607v1",
      "title": "Enabling Rapid Shared Human-AI Mental Model Alignment via the After-Action Review",
      "title_zh": "翻译失败",
      "authors": [
        "Edward Gu",
        "Ho Chit Siu",
        "Melanie Platt",
        "Isabelle Hurley",
        "Jaime Peña",
        "Rohan Paleja"
      ],
      "abstract": "In this work, we present two novel contributions toward improving research in\nhuman-machine teaming (HMT): 1) a Minecraft testbed to accelerate testing and\ndeployment of collaborative AI agents and 2) a tool to allow users to revisit\nand analyze behaviors within an HMT episode to facilitate shared mental model\ndevelopment. Our browser-based Minecraft testbed allows for rapid testing of\ncollaborative agents in a continuous-space, real-time, partially-observable\nenvironment with real humans without cumbersome setup typical to human-AI\ninteraction user studies. As Minecraft has an extensive player base and a rich\necosystem of pre-built AI agents, we hope this contribution can help to\nfacilitate research quickly in the design of new collaborative agents and in\nunderstanding different human factors within HMT. Our mental model alignment\ntool facilitates user-led post-mission analysis by including video displays of\nfirst-person perspectives of the team members (i.e., the human and AI) that can\nbe replayed, and a chat interface that leverages GPT-4 to provide answers to\nvarious queries regarding the AI's experiences and model details.",
      "tldr_zh": "本研究针对人类-机器团队(Human-Machine Teaming, HMT)提出两个关键贡献：一是开发了一个基于 Minecraft 的浏览器测试平台，允许快速测试和部署协作 AI 代理，该平台采用连续空间、实时和部分可观察环境，便于与真实人类互动而避免繁琐设置。二是设计了一个心理模型对齐工具，支持用户回顾 HMT 事件，包括团队成员（人类和 AI）第一人称视角视频回放，以及利用 GPT-4 的聊天接口回答 AI 体验和模型细节的查询。Minecraft 的丰富玩家生态和预建 AI 代理有助于加速新协作代理的设计和人类因素分析。这些创新工具促进了共享心理模型的快速对齐，提升了 HMT 研究的效率和效果。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted to the Cooperative Multi-Agent Systems Decision-making and\n  Learning:Human-Multi-Agent Cognitive Fusion Workshop at AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.19607v1",
      "published_date": "2025-03-25 12:43:18 UTC",
      "updated_date": "2025-03-25 12:43:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:03:01.383050"
    },
    {
      "arxiv_id": "2503.19602v1",
      "title": "Innate Reasoning is Not Enough: In-Context Learning Enhances Reasoning Large Language Models with Less Overthinking",
      "title_zh": "翻译失败",
      "authors": [
        "Yuyao Ge",
        "Shenghua Liu",
        "Yiwei Wang",
        "Lingrui Mei",
        "Lizhe Chen",
        "Baolong Bi",
        "Xueqi Cheng"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have introduced Reasoning\nLarge Language Models (RLLMs), which employ extended thinking processes with\nreflection and self-correction capabilities, demonstrating the effectiveness of\ntest-time scaling. RLLMs exhibit innate Chain-of-Thought (CoT) reasoning\ncapability obtained from training, leading to a natural question: \"Is CoT\nprompting, a popular In-Context Learning (ICL) method for chat LLMs, necessary\nto enhance the reasoning capability of RLLMs?\" In this work, we present the\nfirst comprehensive analysis of the impacts of Zero-shot CoT and Few-shot CoT\non RLLMs across mathematical reasoning tasks. We examine models ranging from\n1.5B to 32B parameters, finding that contrary to concerns, CoT prompting\nsignificantly enhances RLLMs' performance in most scenarios. Our results reveal\ndistinct patterns: large-capacity models show minimal improvement on simple\ntasks but substantial gains on complex problems, while smaller models exhibit\nthe opposite behavior. Further analysis demonstrates that CoT prompting\neffectively controls the distribution of the numbers of thinking tokens and\nreasoning steps, reducing excessive reflections by approximately 90% in some\ncases. Moreover, attention logits analysis reveals the RLLMs' overfitting to\nreflection-related words, which is mitigated by external CoT guidance. Notably,\nour experiments indicate that for RLLMs, one-shot CoT consistently yields\nsuperior performance compared to Few-shot CoT approaches. Our findings provide\nimportant insights for optimizing RLLMs' performance through appropriate\nprompting strategies.",
      "tldr_zh": "本文研究发现，Reasoning Large Language Models (RLLMs) 的先天推理能力不足，通过 In-Context Learning (ICL) 中的 Chain-of-Thought (CoT) 提示可以显著提升其性能，同时减少过度思考。研究者对从 1.5B 到 32B 参数的 RLLMs 进行了首次全面分析，包括 Zero-shot CoT 和 Few-shot CoT 在数学推理任务上的影响，结果显示大模型在复杂问题上获益最大，而小模型在简单任务上表现更佳。CoT 提示还能控制思考步骤，降低过度反思约 90%，并证明 One-shot CoT 比 Few-shot CoT 更有效，为优化 RLLMs 的提示策略提供了关键见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19602v1",
      "published_date": "2025-03-25 12:37:22 UTC",
      "updated_date": "2025-03-25 12:37:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:03:15.239313"
    },
    {
      "arxiv_id": "2503.19599v1",
      "title": "HoarePrompt: Structural Reasoning About Program Correctness in Natural Language",
      "title_zh": "翻译失败",
      "authors": [
        "Dimitrios Stamatios Bouras",
        "Yihan Dai",
        "Tairan Wang",
        "Yingfei Xiong",
        "Sergey Mechtaev"
      ],
      "abstract": "While software requirements are often expressed in natural language,\nverifying the correctness of a program against natural language requirements is\na hard and underexplored problem. Large language models (LLMs) are promising\ncandidates for addressing this challenge, however our experience shows that\nthey are ineffective in this task, often failing to detect even straightforward\nbugs. To address this gap, we introduce HoarePrompt, a novel approach that\nadapts fundamental ideas from program analysis and verification to natural\nlanguage artifacts. Drawing inspiration from the strongest postcondition\ncalculus, HoarePrompt employs a systematic, step-by-step process in which an\nLLM generates natural language descriptions of reachable program states at\nvarious points in the code. To manage loops, we propose few-shot-driven\nk-induction, an adaptation of the k-induction method widely used in model\nchecking. Once program states are described, HoarePrompt leverages the LLM to\nassess whether the program, annotated with these state descriptions, conforms\nto the natural language requirements. For evaluating the quality of classifiers\nof program correctness with respect to natural language requirements, we\nconstructed CoCoClaNeL, a challenging dataset of solutions to programming\ncompetition problems. Our experiments show that HoarePrompt improves the MCC by\n62% compared to directly using Zero-shot-CoT prompts for correctness\nclassification. Furthermore, HoarePrompt outperforms a classifier that assesses\ncorrectness via LLM-based test generation by increasing the MCC by 93%. The\ninductive reasoning mechanism contributes a 28% boost to MCC, underscoring its\neffectiveness in managing loops.",
      "tldr_zh": "这篇论文提出了 HoarePrompt，一种将程序分析和验证理念应用于自然语言的结构化推理方法，用于验证程序是否符合自然语言要求。HoarePrompt 通过 LLM 生成程序不同点的可达状态描述，并引入 few-shot-driven k-induction 来处理循环，从而系统评估程序正确性。在 CoCoClaNeL 数据集上的实验显示，该方法相较于 Zero-shot-CoT 提示提高了 62% 的 MCC，并相较于基于 LLM 测试生成的分类器提升了 93%，证明了归纳推理机制的有效性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19599v1",
      "published_date": "2025-03-25 12:30:30 UTC",
      "updated_date": "2025-03-25 12:30:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:03:25.089430"
    },
    {
      "arxiv_id": "2503.19584v3",
      "title": "Multi-agent Application System in Office Collaboration Scenarios",
      "title_zh": "办公协作场景中的多智能体应用系统",
      "authors": [
        "Songtao Sun",
        "Jingyi Li",
        "Yuanfei Dong",
        "Haoguang Liu",
        "Chenxin Xu",
        "Fuyang Li",
        "Qiang Liu"
      ],
      "abstract": "This paper introduces a multi-agent application system designed to enhance\noffice collaboration efficiency and work quality. The system integrates\nartificial intelligence, machine learning, and natural language processing\ntechnologies, achieving functionalities such as task allocation, progress\nmonitoring, and information sharing. The agents within the system are capable\nof providing personalized collaboration support based on team members' needs\nand incorporate data analysis tools to improve decision-making quality. The\npaper also proposes an intelligent agent architecture that separates Plan and\nSolver, and through techniques such as multi-turn query rewriting and business\ntool retrieval, it enhances the agent's multi-intent and multi-turn dialogue\ncapabilities. Furthermore, the paper details the design of tools and multi-turn\ndialogue in the context of office collaboration scenarios, and validates the\nsystem's effectiveness through experiments and evaluations. Ultimately, the\nsystem has demonstrated outstanding performance in real business applications,\nparticularly in query understanding, task planning, and tool calling. Looking\nforward, the system is expected to play a more significant role in addressing\ncomplex interaction issues within dynamic environments and large-scale\nmulti-agent systems.",
      "tldr_zh": "这篇论文介绍了多智能体应用系统，旨在提升办公室协作场景中的效率和工作质量，通过整合人工智能、机器学习和自然语言处理技术，实现任务分配、进度监控和信息共享等功能。系统采用分离 Plan and Solver 的智能体架构，并利用多轮查询重写和业务工具检索等方法，提供个性化支持和提升多意图多轮对话能力。实验结果显示，该系统在实际业务应用中表现出色，尤其在查询理解、任务规划和工具调用方面，并有望在动态环境和大规模多智能体系统中发挥更大作用。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "Technical report",
      "pdf_url": "http://arxiv.org/pdf/2503.19584v3",
      "published_date": "2025-03-25 12:07:20 UTC",
      "updated_date": "2025-04-07 07:46:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:03:37.840062"
    },
    {
      "arxiv_id": "2503.19945v1",
      "title": "Optimizing Breast Cancer Detection in Mammograms: A Comprehensive Study of Transfer Learning, Resolution Reduction, and Multi-View Classification",
      "title_zh": "优化乳腺癌在乳腺X光片中的检测：迁移学习、分辨率降低和多视图分类的全面研究",
      "authors": [
        "Daniel G. P. Petrini",
        "Hae Yong Kim"
      ],
      "abstract": "This study explores open questions in the application of machine learning for\nbreast cancer detection in mammograms. Current approaches often employ a\ntwo-stage transfer learning process: first, adapting a backbone model trained\non natural images to develop a patch classifier, which is then used to create a\nsingle-view whole-image classifier. Additionally, many studies leverage both\nmammographic views to enhance model performance. In this work, we\nsystematically investigate five key questions: (1) Is the intermediate patch\nclassifier essential for optimal performance? (2) Do backbone models that excel\nin natural image classification consistently outperform others on mammograms?\n(3) When reducing mammogram resolution for GPU processing, does the\nlearn-to-resize technique outperform conventional methods? (4) Does\nincorporating both mammographic views in a two-view classifier significantly\nimprove detection accuracy? (5) How do these findings vary when analyzing\nlow-quality versus high-quality mammograms? By addressing these questions, we\ndeveloped models that outperform previous results for both single-view and\ntwo-view classifiers. Our findings provide insights into model architecture and\ntransfer learning strategies contributing to more accurate and efficient\nmammogram analysis.",
      "tldr_zh": "这篇论文系统研究了机器学习在乳房X光片（mammograms）中检测乳腺癌的优化，重点探讨了transfer learning、分辨率减少和multi-view classification。研究者调查了五个关键问题，包括中间图像块分类器的必要性、骨干模型在乳房X光上的表现、learn-to-resize技术与传统方法的比较、双视图分类器的准确率提升，以及这些因素在低质量和高质量图像上的差异。最终，开发了优于现有方法的单视图和双视图分类器，并提供了关于模型架构和transfer learning策略的见解，以实现更准确和高效的乳房X光分析。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "8 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.19945v1",
      "published_date": "2025-03-25 11:51:21 UTC",
      "updated_date": "2025-03-25 11:51:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:03:50.371295"
    },
    {
      "arxiv_id": "2503.19564v1",
      "title": "FedMM-X: A Trustworthy and Interpretable Framework for Federated Multi-Modal Learning in Dynamic Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Sree Bhargavi Balija"
      ],
      "abstract": "As artificial intelligence systems increasingly operate in Real-world\nenvironments, the integration of multi-modal data sources such as vision,\nlanguage, and audio presents both unprecedented opportunities and critical\nchallenges for achieving trustworthy intelligence. In this paper, we propose a\nnovel framework that unifies federated learning with explainable multi-modal\nreasoning to ensure trustworthiness in decentralized, dynamic settings. Our\napproach, called FedMM-X (Federated Multi-Modal Explainable Intelligence),\nleverages cross-modal consistency checks, client-level interpretability\nmechanisms, and dynamic trust calibration to address challenges posed by data\nheterogeneity, modality imbalance, and out-of-distribution generalization.\nThrough rigorous evaluation across federated multi-modal benchmarks involving\nvision-language tasks, we demonstrate improved performance in both accuracy and\ninterpretability while reducing vulnerabilities to adversarial and spurious\ncorrelations. Further, we introduce a novel trust score aggregation method to\nquantify global model reliability under dynamic client participation. Our\nfindings pave the way toward developing robust, interpretable, and socially\nresponsible AI systems in Real-world environments.",
      "tldr_zh": "该研究提出FedMM-X框架，将联邦学习(Federated Learning)与可解释的多模态推理相结合，旨在处理动态环境中多模态数据（如视觉、语言和音频）的挑战，确保AI系统的可信性。该框架通过跨模态一致性检查、客户端级解释机制和动态信任校准，解决了数据异质性、模态不平衡以及分布外泛化等问题。在联邦多模态基准测试中，FedMM-X显著提升了准确性和可解释性，同时减少了对抗性和虚假相关性的脆弱性。研究还引入了新的信任分数聚合方法，以量化全局模型在动态客户端参与下的可靠性，为开发稳健、可解释的AI系统提供了重要基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19564v1",
      "published_date": "2025-03-25 11:28:21 UTC",
      "updated_date": "2025-03-25 11:28:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:04:01.298850"
    },
    {
      "arxiv_id": "2503.19551v2",
      "title": "Scaling Laws of Synthetic Data for Language Models",
      "title_zh": "合成数据用于语言模型的缩放定律",
      "authors": [
        "Zeyu Qin",
        "Qingxiu Dong",
        "Xingxing Zhang",
        "Li Dong",
        "Xiaolong Huang",
        "Ziyi Yang",
        "Mahmoud Khademi",
        "Dongdong Zhang",
        "Hany Hassan Awadalla",
        "Yi R. Fung",
        "Weizhu Chen",
        "Minhao Cheng",
        "Furu Wei"
      ],
      "abstract": "Large language models (LLMs) achieve strong performance across diverse tasks,\nlargely driven by high-quality web data used in pre-training. However, recent\nstudies indicate this data source is rapidly depleting. Synthetic data emerges\nas a promising alternative, but it remains unclear whether synthetic datasets\nexhibit predictable scalability comparable to raw pre-training data. In this\nwork, we systematically investigate the scaling laws of synthetic data by\nintroducing SynthLLM, a scalable framework that transforms pre-training corpora\ninto diverse, high-quality synthetic datasets. Our approach achieves this by\nautomatically extracting and recombining high-level concepts across multiple\ndocuments using a graph algorithm. Key findings from our extensive mathematical\nexperiments on SynthLLM include: (1) SynthLLM generates synthetic data that\nreliably adheres to the rectified scaling law across various model sizes; (2)\nPerformance improvements plateau near 300B tokens; and (3) Larger models\napproach optimal performance with fewer training tokens. For instance, an 8B\nmodel peaks at 1T tokens, while a 3B model requires 4T. Moreover, comparisons\nwith existing synthetic data generation and augmentation methods demonstrate\nthat SynthLLM achieves superior performance and scalability. Our findings\nhighlight synthetic data as a scalable and reliable alternative to organic\npre-training corpora, offering a viable path toward continued improvement in\nmodel performance.",
      "tldr_zh": "本研究调查了合成数据在大型语言模型（LLMs）训练中的缩放定律，针对高质量网络数据耗尽的问题，引入了SynthLLM框架，该框架通过提取和重组多个文档中的高水平概念（使用图算法）来生成多样、高质量的合成数据集。实验结果显示，SynthLLM生成的合成数据遵循修正的缩放定律（rectified scaling law），性能提升在约300B标记处达到平台期，且更大模型（如8B模型）只需较少训练标记（如1T标记）即可达到最佳性能，而3B模型需4T标记。相比现有合成数据生成方法，SynthLLM表现出优越的性能和可扩展性，证明合成数据是可靠的替代方案，有助于LLMs性能的持续改进。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "work in progress",
      "pdf_url": "http://arxiv.org/pdf/2503.19551v2",
      "published_date": "2025-03-25 11:07:12 UTC",
      "updated_date": "2025-03-26 11:23:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:04:13.800039"
    },
    {
      "arxiv_id": "2503.19540v1",
      "title": "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models",
      "title_zh": "FLEX：用于评估",
      "authors": [
        "Dahyun Jung",
        "Seungyoon Lee",
        "Hyeonseok Moon",
        "Chanjun Park",
        "Heuiseok Lim"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced interactions between users and models. These advancements concurrently\nunderscore the need for rigorous safety evaluations due to the manifestation of\nsocial biases, which can lead to harmful societal impacts. Despite these\nconcerns, existing benchmarks may overlook the intrinsic weaknesses of LLMs,\nwhich can generate biased responses even with simple adversarial instructions.\nTo address this critical gap, we introduce a new benchmark, Fairness Benchmark\nin LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can\nsustain fairness even when exposed to prompts constructed to induce bias. To\nthoroughly evaluate the robustness of LLMs, we integrate prompts that amplify\npotential biases into the fairness assessment. Comparative experiments between\nFLEX and existing benchmarks demonstrate that traditional evaluations may\nunderestimate the inherent risks in models. This highlights the need for more\nstringent LLM evaluation benchmarks to guarantee safety and fairness.",
      "tldr_zh": "本研究探讨了大型语言模型(LLMs)中公平性的鲁棒性问题，指出现有基准可能低估模型在面对对抗指令(adversarial instructions)时产生的社会偏见风险。作者引入了新的基准FLEX（Fairness Benchmark in LLM under Extreme Scenarios），通过设计诱导偏见的提示来评估LLMs在极端场景下维持公平的能力。实验比较显示，FLEX相对于传统基准更能揭示模型的内在弱点，证明了需要更严格的评估方法以确保LLMs的安全和公平性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2025 findings",
      "pdf_url": "http://arxiv.org/pdf/2503.19540v1",
      "published_date": "2025-03-25 10:48:33 UTC",
      "updated_date": "2025-03-25 10:48:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:04:24.986367"
    },
    {
      "arxiv_id": "2503.19530v1",
      "title": "VectorFit : Adaptive Singular & Bias Vector Fine-Tuning of Pre-trained Foundation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Suhas G Hegde",
        "Shilpy Kaur",
        "Aruna Tiwari"
      ],
      "abstract": "Popular PEFT methods achieve parameter efficiency by assuming that\nincremental weight updates are inherently low-rank, which often leads to a\nperformance gap compared to full fine-tuning. While recent methods have\nattempted to address this limitation, they typically lack sufficient parameter\nand memory efficiency. We propose VectorFit, an effective and easily deployable\napproach that adaptively trains the singular vectors and biases of pre-trained\nweight matrices. We demonstrate that the utilization of structural and\ntransformational characteristics of pre-trained weights enables high-rank\nupdates comparable to those of full fine-tuning. As a result, VectorFit\nachieves superior performance with 9X less trainable parameters compared to\nstate-of-the-art PEFT methods. Through extensive experiments over 17 datasets\nspanning diverse language and vision tasks such as natural language\nunderstanding and generation, question answering, image classification, and\nimage generation, we exhibit that VectorFit consistently outperforms baselines,\neven in extremely low-budget scenarios.",
      "tldr_zh": "该论文提出VectorFit，一种自适应微调预训练基础模型的方法，通过训练奇异向量(singular vectors)和偏差(bias)来实现高秩更新，从而克服传统PEFT方法的性能差距。VectorFit利用预训练权重矩阵的结构和变换特性，仅需比最先进PEFT方法少9倍可训练参数，即可达到与full fine-tuning相当的效果。在17个数据集上的广泛实验中，涵盖自然语言理解、生成、问答、图像分类和生成等任务，VectorFit consistently outperforms baselines，甚至在极低预算场景下表现突出。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19530v1",
      "published_date": "2025-03-25 10:36:27 UTC",
      "updated_date": "2025-03-25 10:36:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:04:37.903153"
    },
    {
      "arxiv_id": "2504.00009v1",
      "title": "Deep Learning-Based Hypoglycemia Classification Across Multiple Prediction Horizons",
      "title_zh": "基于深度学习的跨多个预测时间窗低血糖分类",
      "authors": [
        "Beyza Cinar",
        "Jennifer Daniel Onwuchekwa",
        "Maria Maleshkova"
      ],
      "abstract": "Type 1 diabetes (T1D) management can be significantly enhanced through the\nuse of predictive machine learning (ML) algorithms, which can mitigate the risk\nof adverse events like hypoglycemia. Hypoglycemia, characterized by blood\nglucose levels below 70 mg/dL, is a life-threatening condition typically caused\nby excessive insulin administration, missed meals, or physical activity. Its\nasymptomatic nature impedes timely intervention, making ML models crucial for\nearly detection. This study integrates short- (up to 2h) and long-term (up to\n24h) prediction horizons (PHs) within a single classification model to enhance\ndecision support. The predicted times are 5-15 min, 15-30 min, 30 min-1h, 1-2h,\n2-4h, 4-8h, 8-12h, and 12-24h before hypoglycemia. In addition, a simplified\nmodel classifying up to 4h before hypoglycemia is compared. We trained ResNet\nand LSTM models on glucose levels, insulin doses, and acceleration data. The\nresults demonstrate the superiority of the LSTM models when classifying nine\nclasses. In particular, subject-specific models yielded better performance but\nachieved high recall only for classes 0, 1, and 2 with 98%, 72%, and 50%,\nrespectively. A population-based six-class model improved the results with at\nleast 60% of events detected. In contrast, longer PHs remain challenging with\nthe current approach and may be considered with different models.",
      "tldr_zh": "该研究提出了一种基于深度学习的模型，用于跨多个预测视野（PHs）分类低血糖（hypoglycemia）事件，以提升1型糖尿病（T1D）管理。该模型整合了从5分钟到24小时的预测期，包括短时（最多2小时）和长时（最多24小时）的分类，并使用ResNet和LSTM模型训练葡萄糖水平、胰岛素剂量及加速度数据。结果显示，LSTM模型在九类分类中表现出色，特定受试者模型在短期PHs（如类0、1、2）中实现高召回率（分别为98%、72%、50%），而总体六类模型至少检测到60%的事件；然而，长PHs预测仍面临挑战，需要进一步优化。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG",
        "stat.AP"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.00009v1",
      "published_date": "2025-03-25 10:24:27 UTC",
      "updated_date": "2025-03-25 10:24:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:04:50.458554"
    },
    {
      "arxiv_id": "2503.19943v1",
      "title": "A Spatiotemporal Radar-Based Precipitation Model for Water Level Prediction and Flood Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Sakshi Dhankhar",
        "Stefan Wittek",
        "Hamidreza Eivazi",
        "Andreas Rausch"
      ],
      "abstract": "Study Region: Goslar and G\\\"ottingen, Lower Saxony, Germany. Study Focus: In\nJuly 2017, the cities of Goslar and G\\\"ottingen experienced severe flood events\ncharacterized by short warning time of only 20 minutes, resulting in extensive\nregional flooding and significant damage. This highlights the critical need for\na more reliable and timely flood forecasting system. This paper presents a\ncomprehensive study on the impact of radar-based precipitation data on\nforecasting river water levels in Goslar. Additionally, the study examines how\nprecipitation influences water level forecasts in G\\\"ottingen. The analysis\nintegrates radar-derived spatiotemporal precipitation patterns with\nhydrological sensor data obtained from ground stations to evaluate the\neffectiveness of this approach in improving flood prediction capabilities. New\nHydrological Insights for the Region: A key innovation in this paper is the use\nof residual-based modeling to address the non-linearity between precipitation\nimages and water levels, leading to a Spatiotemporal Radar-based Precipitation\nModel with residuals (STRPMr). Unlike traditional hydrological models, our\napproach does not rely on upstream data, making it independent of additional\nhydrological inputs. This independence enhances its adaptability and allows for\nbroader applicability in other regions with RADOLAN precipitation. The deep\nlearning architecture integrates (2+1)D convolutional neural networks for\nspatial and temporal feature extraction with LSTM for timeseries forecasting.\nThe results demonstrate the potential of the STRPMr for capturing extreme\nevents and more accurate flood forecasting.",
      "tldr_zh": "本研究针对德国下萨克森州Goslar和Göttingen的洪水事件，提出了一种基于雷达的时空降水模型（Spatiotemporal Radar-based Precipitation Model with residuals, STRPMr），旨在提升河流水位预测和洪水预报的准确性与及时性。模型通过整合雷达衍生的时空降水模式与地面水文传感器数据，使用残差建模处理降水和水位之间的非线性关系，并采用(2+1)D卷积神经网络提取空间和时间特征，结合LSTM进行时间序列预测。不同于传统模型，STRPMr不依赖上游数据，提高了其适应性和在其他RADOLAN降水区域的应用潜力；实验结果显示，该模型在捕捉极端事件方面表现出色，显著提高了洪水预测能力。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "28 pages, 11 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.19943v1",
      "published_date": "2025-03-25 10:14:54 UTC",
      "updated_date": "2025-03-25 10:14:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:05:03.397815"
    },
    {
      "arxiv_id": "2503.19510v1",
      "title": "RoboFlamingo-Plus: Fusion of Depth and RGB Perception with Vision-Language Models for Enhanced Robotic Manipulation",
      "title_zh": "RoboF",
      "authors": [
        "Sheng Wang"
      ],
      "abstract": "As robotic technologies advancing towards more complex multimodal\ninteractions and manipulation tasks, the integration of advanced\nVision-Language Models (VLMs) has become a key driver in the field. Despite\nprogress with current methods, challenges persist in fusing depth and RGB\ninformation within 3D environments and executing tasks guided by linguistic\ninstructions. In response to these challenges, we have enhanced the existing\nRoboFlamingo framework by introducing RoboFlamingo-Plus, which incorporates\ndepth data into VLMs to significantly improve robotic manipulation performance.\nOur research achieves a nuanced fusion of RGB and depth information by\nintegrating a pre-trained Vision Transformer (ViT) with a resampling technique,\nclosely aligning this combined data with linguistic cues for superior\nmultimodal understanding. The novelty of RoboFlamingo-Plus lies in its\nadaptation of inputs for depth data processing, leveraging a pre-trained\nresampler for depth feature extraction, and employing cross-attention\nmechanisms for optimal feature integration. These improvements allow\nRoboFlamingo-Plus to not only deeply understand 3D environments but also easily\nperform complex, language-guided tasks in challenging settings. Experimental\nresults show that RoboFlamingo-Plus boosts robotic manipulation by 10-20% over\ncurrent methods, marking a significant advancement. Codes and model weights are\npublic at RoboFlamingo-Plus.",
      "tldr_zh": "本文提出RoboFlamingo-Plus框架，通过融合深度和RGB感知于Vision-Language Models (VLMs)，以提升机器人操作在3D环境中的性能。该框架采用预训练的Vision Transformer (ViT)结合重采样技术，并利用跨注意力机制整合深度特征与语言指令，实现更精确的多模态理解。实验结果显示，RoboFlamingo-Plus在复杂任务中比现有方法提高了10-20%的操作效率，并公开了代码和模型权重。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19510v1",
      "published_date": "2025-03-25 10:01:57 UTC",
      "updated_date": "2025-03-25 10:01:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:05:13.382256"
    },
    {
      "arxiv_id": "2503.19502v1",
      "title": "Towards Long-Range ENSO Prediction with an Explainable Deep Learning Model",
      "title_zh": "翻译失败",
      "authors": [
        "Qi Chen",
        "Yinghao Cui",
        "Guobin Hong",
        "Karumuri Ashok",
        "Yuchun Pu",
        "Xiaogu Zheng",
        "Xuanze Zhang",
        "Wei Zhong",
        "Peng Zhan",
        "Zhonglei Wang"
      ],
      "abstract": "El Ni\\~no-Southern Oscillation (ENSO) is a prominent mode of interannual\nclimate variability with far-reaching global impacts. Its evolution is governed\nby intricate air-sea interactions, posing significant challenges for long-term\nprediction. In this study, we introduce CTEFNet, a multivariate deep learning\nmodel that synergizes convolutional neural networks and transformers to enhance\nENSO forecasting. By integrating multiple oceanic and atmospheric predictors,\nCTEFNet extends the effective forecast lead time to 20 months while mitigating\nthe impact of the spring predictability barrier, outperforming both dynamical\nmodels and state-of-the-art deep learning approaches. Furthermore, CTEFNet\noffers physically meaningful and statistically significant insights through\ngradient-based sensitivity analysis, revealing the key precursor signals that\ngovern ENSO dynamics, which align with well-established theories and reveal new\ninsights about inter-basin interactions among the Pacific, Atlantic, and Indian\nOceans. The CTEFNet's superior predictive skill and interpretable sensitivity\nassessments underscore its potential for advancing climate prediction. Our\nfindings highlight the importance of multivariate coupling in ENSO evolution\nand demonstrate the promise of deep learning in capturing complex climate\ndynamics with enhanced interpretability.",
      "tldr_zh": "本文提出CTEFNet，一种结合卷积神经网络和Transformer的多变量深度学习模型，用于提升El Niño-Southern Oscillation (ENSO) 的长期预测能力。该模型整合多种海洋和大气预测因子，将有效预测提前时间延长至20个月，并成功缓解春季预测障碍，优于动态模型和现有深度学习方法。通过梯度-based敏感性分析，CTEFNet揭示了ENSO动态的关键前兆信号，包括太平洋、大西洋和印度洋之间的相互作用，这些发现与既有理论一致并提供新见解。该研究强调多变量耦合在ENSO演变中的重要性，并展示了深度学习在可解释气候预测方面的潜力。",
      "categories": [
        "physics.geo-ph",
        "cs.AI"
      ],
      "primary_category": "physics.geo-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19502v1",
      "published_date": "2025-03-25 09:50:19 UTC",
      "updated_date": "2025-03-25 09:50:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:05:25.352354"
    },
    {
      "arxiv_id": "2503.19501v1",
      "title": "Pose-Based Fall Detection System: Efficient Monitoring on Standard CPUs",
      "title_zh": "基于姿势的跌倒检测系统：在标准CPU上高效监控",
      "authors": [
        "Vinayak Mali",
        "Saurabh Jaiswal"
      ],
      "abstract": "Falls among elderly residents in assisted living homes pose significant\nhealth risks, often leading to injuries and a decreased quality of life.\nCurrent fall detection solutions typically rely on sensor-based systems that\nrequire dedicated hardware, or on video-based models that demand high\ncomputational resources and GPUs for real-time processing. In contrast, this\npaper presents a robust fall detection system that does not require any\nadditional sensors or high-powered hardware. The system uses pose estimation\ntechniques, combined with threshold-based analysis and a voting mechanism, to\neffectively distinguish between fall and non-fall activities. For pose\ndetection, we leverage MediaPipe, a lightweight and efficient framework that\nenables real-time processing on standard CPUs with minimal computational\noverhead. By analyzing motion, body position, and key pose points, the system\nprocesses pose features with a 20-frame buffer, minimizing false positives and\nmaintaining high accuracy even in real-world settings. This unobtrusive,\nresource-efficient approach provides a practical solution for enhancing\nresident safety in old age homes, without the need for expensive sensors or\nhigh-end computational resources.",
      "tldr_zh": "这篇论文提出了一种基于姿势估计(pose estimation)的跌倒检测系统，旨在为养老机构提供高效监控，而无需额外传感器或高性能硬件，仅依赖标准 CPU。该系统结合阈值分析(threshold-based analysis)、投票机制(voting mechanism)和 MediaPipe 框架，对运动、体位和关键姿势点进行实时分析，使用 20 帧缓冲区来最小化假阳性并提升准确性。结果表明，该方法在真实场景中表现出色，为老年居民安全提供了一个实用、资源高效的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "4 Pages, 2 figures, 2 code block, 1 flow chart",
      "pdf_url": "http://arxiv.org/pdf/2503.19501v1",
      "published_date": "2025-03-25 09:49:36 UTC",
      "updated_date": "2025-03-25 09:49:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:05:38.312103"
    },
    {
      "arxiv_id": "2503.19496v1",
      "title": "SMT-EX: An Explainable Surrogate Modeling Toolbox for Mixed-Variables Design Exploration",
      "title_zh": "SMT-EX：一种可解释的代理建模工具箱，用于混合变量设计探索",
      "authors": [
        "Mohammad Daffa Robani",
        "Paul Saves",
        "Pramudita Satria Palar",
        "Lavi Rizki Zuhal",
        "oseph Morlier"
      ],
      "abstract": "Surrogate models are of high interest for many engineering applications,\nserving as cheap-to-evaluate time-efficient approximations of black-box\nfunctions to help engineers and practitioners make decisions and understand\ncomplex systems. As such, the need for explainability methods is rising and\nmany studies have been performed to facilitate knowledge discovery from\nsurrogate models. To respond to these enquiries, this paper introduces SMT-EX,\nan enhancement of the open-source Python Surrogate Modeling Toolbox (SMT) that\nintegrates explainability techniques into a state-of-the-art surrogate\nmodelling framework. More precisely, SMT-EX includes three key explainability\nmethods: Shapley Additive Explanations, Partial Dependence Plot, and Individual\nConditional Expectations. A peculiar explainability dependency of SMT has been\ndeveloped for such purpose that can be easily activated once the surrogate\nmodel is built, offering a user-friendly and efficient tool for swift insight\nextraction. The effectiveness of SMT-EX is showcased through two test cases.\nThe first case is a 10-variable wing weight problem with purely continuous\nvariables and the second one is a 3-variable mixed-categorical cantilever beam\nbending problem. Relying on SMT-EX analyses for these problems, we demonstrate\nits versatility in addressing a diverse range of problem characteristics.\nSMT-Explainability is freely available on Github:\nhttps://github.com/SMTorg/smt-explainability .",
      "tldr_zh": "该论文引入了 SMT-EX，这是一个增强版的开源 Python Surrogate Modeling Toolbox (SMT)，旨在为混合变量设计探索提供可解释性功能。SMT-EX 集成了 Shapley Additive Explanations、Partial Dependence Plot 和 Individual Conditional Expectations 等关键解释方法，便于用户从代理模型中快速提取洞见。工具箱开发了一个易于激活的解释依赖性，简化了代理模型构建后的分析过程。通过两个测试案例——一个是10变量的翼重问题和一个是3变量的混合分类悬臂梁弯曲问题——证明了 SMT-EX 的有效性和多功能性。该工具箱已免费开源，可在 GitHub 上获取。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19496v1",
      "published_date": "2025-03-25 09:38:27 UTC",
      "updated_date": "2025-03-25 09:38:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:05:51.105447"
    },
    {
      "arxiv_id": "2503.19941v1",
      "title": "Body Discovery of Embodied AI",
      "title_zh": "具身人工智能的身体发现",
      "authors": [
        "Zhe Sun",
        "Pengfei Tian",
        "Xiaozhu Hu",
        "Xiaoyu Zhao",
        "Huiying Li",
        "Zhenliang Zhang"
      ],
      "abstract": "In the pursuit of realizing artificial general intelligence (AGI), the\nimportance of embodied artificial intelligence (AI) becomes increasingly\napparent. Following this trend, research integrating robots with AGI has become\nprominent. As various kinds of embodiments have been designed, adaptability to\ndiverse embodiments will become important to AGI. We introduce a new challenge,\ntermed \"Body Discovery of Embodied AI\", focusing on tasks of recognizing\nembodiments and summarizing neural signal functionality. The challenge\nencompasses the precise definition of an AI body and the intricate task of\nidentifying embodiments in dynamic environments, where conventional approaches\noften prove inadequate. To address these challenges, we apply causal inference\nmethod and evaluate it by developing a simulator tailored for testing\nalgorithms with virtual environments. Finally, we validate the efficacy of our\nalgorithms through empirical testing, demonstrating their robust performance in\nvarious scenarios based on virtual environments.",
      "tldr_zh": "这篇论文强调了Embodied AI在实现AGI（人工通用智能）中的重要性，并引入了“Body Discovery of Embodied AI”这一新挑战，专注于识别embodiments和总结neural signal functionality，以提升AI对多样化身体结构的适应性。作者通过causal inference方法来解决动态环境中识别embodiments的难题，并开发了一个专属模拟器进行算法测试。实验结果显示，该算法在各种虚拟场景中表现出色，验证了其有效性和鲁棒性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19941v1",
      "published_date": "2025-03-25 09:21:10 UTC",
      "updated_date": "2025-03-25 09:21:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:06:01.607180"
    },
    {
      "arxiv_id": "2503.19474v2",
      "title": "A-MESS: Anchor based Multimodal Embedding with Semantic Synchronization for Multimodal Intent Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Yaomin Shen",
        "Xiaojian Lin",
        "Wei Fan"
      ],
      "abstract": "In the domain of multimodal intent recognition (MIR), the objective is to\nrecognize human intent by integrating a variety of modalities, such as language\ntext, body gestures, and tones. However, existing approaches face difficulties\nadequately capturing the intrinsic connections between the modalities and\noverlooking the corresponding semantic representations of intent. To address\nthese limitations, we present the Anchor-based Multimodal Embedding with\nSemantic Synchronization (A-MESS) framework. We first design an Anchor-based\nMultimodal Embedding (A-ME) module that employs an anchor-based embedding\nfusion mechanism to integrate multimodal inputs. Furthermore, we develop a\nSemantic Synchronization (SS) strategy with the Triplet Contrastive Learning\npipeline, which optimizes the process by synchronizing multimodal\nrepresentation with label descriptions produced by the large language model.\nComprehensive experiments indicate that our A-MESS achieves state-of-the-art\nand provides substantial insight into multimodal representation and downstream\ntasks.",
      "tldr_zh": "本研究针对多模态意图识别(Multimodal Intent Recognition, MIR)中的挑战，提出了一种 A-MESS 框架，以整合语言文本、肢体姿势和语气等模态，同时捕捉模态间的内在连接和意图的语义表示。框架包括 Anchor-based Multimodal Embedding (A-ME) 模块，通过基于锚点的嵌入融合机制处理多模态输入，以及 Semantic Synchronization (SS) 策略，利用 Triplet Contrastive Learning 来同步多模态表示与大语言模型生成的标签描述。实验结果显示，A-MESS 达到了 state-of-the-art 水平，并为多模态表示和下游任务提供了宝贵的洞见。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICME2025",
      "pdf_url": "http://arxiv.org/pdf/2503.19474v2",
      "published_date": "2025-03-25 09:09:30 UTC",
      "updated_date": "2025-04-02 03:33:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:06:14.991636"
    },
    {
      "arxiv_id": "2503.19470v2",
      "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Mingyang Chen",
        "Tianpeng Li",
        "Haoze Sun",
        "Yijie Zhou",
        "Chenzheng Zhu",
        "Haofen Wang",
        "Jeff Z. Pan",
        "Wen Zhang",
        "Huajun Chen",
        "Fan Yang",
        "Zenan Zhou",
        "Weipeng Chen"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process.",
      "tldr_zh": "该研究提出 ReSearch 框架，通过强化学习（Reinforcement Learning）训练大型语言模型（LLMs），使它们学会将搜索操作整合到推理链中，从而处理复杂多跳问题，而无需任何监督数据。框架以文本-based 思考指导何时和如何进行搜索，并利用搜索结果影响后续推理。在 Qwen2.5-7B 和 Qwen2.5-32B 模型上训练后，ReSearch 展示了强大的泛化能力，仅在单一数据集上训练即可在各种基准上表现出色，并自然激发了反思和自我修正等高级推理能力。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2503.19470v2",
      "published_date": "2025-03-25 09:00:58 UTC",
      "updated_date": "2025-03-27 05:56:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:06:25.654429"
    },
    {
      "arxiv_id": "2503.19469v2",
      "title": "Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot Classification with Soft Prompt Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Fred Philippy",
        "Siwen Guo",
        "Cedric Lothritz",
        "Jacques Klein",
        "Tegawendé F. Bissyandé"
      ],
      "abstract": "In NLP, Zero-Shot Classification (ZSC) has become essential for enabling\nmodels to classify text into categories unseen during training, particularly in\nlow-resource languages and domains where labeled data is scarce. While\npretrained language models (PLMs) have shown promise in ZSC, they often rely on\nlarge training datasets or external knowledge, limiting their applicability in\nmultilingual and low-resource scenarios. Recent approaches leveraging natural\nlanguage prompts reduce the dependence on large training datasets but struggle\nto effectively incorporate available labeled data from related classification\ntasks, especially when these datasets originate from different languages or\ndistributions. Moreover, existing prompt-based methods typically rely on\nmanually crafted prompts in a specific language, limiting their adaptability\nand effectiveness in cross-lingual settings. To address these challenges, we\nintroduce RoSPrompt, a lightweight and data-efficient approach for training\nsoft prompts that enhance cross-lingual ZSC while ensuring robust\ngeneralization across data distribution shifts. RoSPrompt is designed for small\nmultilingual PLMs, enabling them to leverage high-resource languages to improve\nperformance in low-resource settings without requiring extensive fine-tuning or\nhigh computational costs. We evaluate our approach on multiple multilingual\nPLMs across datasets covering 106 languages, demonstrating strong cross-lingual\ntransfer performance and robust generalization capabilities over unseen\nclasses.",
      "tldr_zh": "本文提出 RoSPrompt，一种轻量级软提示调优方法，用于增强小型多语言预训练语言模型(PLMs)在跨语言零样本分类(ZSC)中的性能，尤其针对低资源语言和数据分布偏移问题。RoSPrompt 通过利用高资源语言的标签数据来改善低资源场景下的分类效果，同时避免了手动提示的局限性和高计算成本。实验在覆盖106种语言的数据集上评估，展示了显著的跨语言转移性能和泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Workshop on Language Models for Underserved Communities (co-located\n  with NAACL 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.19469v2",
      "published_date": "2025-03-25 09:00:25 UTC",
      "updated_date": "2025-03-28 09:23:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:06:38.325193"
    },
    {
      "arxiv_id": "2503.19455v1",
      "title": "Data-centric Federated Graph Learning with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Bo Yan",
        "Zhongjian Zhang",
        "Huabin Sun",
        "Mengmei Zhang",
        "Yang Cao",
        "Chuan Shi"
      ],
      "abstract": "In federated graph learning (FGL), a complete graph is divided into multiple\nsubgraphs stored in each client due to privacy concerns, and all clients\njointly train a global graph model by only transmitting model parameters. A\npain point of FGL is the heterogeneity problem, where nodes or structures\npresent non-IID properties among clients (e.g., different node label\ndistributions), dramatically undermining the convergence and performance of\nFGL. To address this, existing efforts focus on design strategies at the model\nlevel, i.e., they design models to extract common knowledge to mitigate\nheterogeneity. However, these model-level strategies fail to fundamentally\naddress the heterogeneity problem as the model needs to be designed from\nscratch when transferring to other tasks. Motivated by large language models\n(LLMs) having achieved remarkable success, we aim to utilize LLMs to fully\nunderstand and augment local text-attributed graphs, to address data\nheterogeneity at the data level. In this paper, we propose a general framework\nLLM4FGL that innovatively decomposes the task of LLM for FGL into two sub-tasks\ntheoretically. Specifically, for each client, it first utilizes the LLM to\ngenerate missing neighbors and then infers connections between generated nodes\nand raw nodes. To improve the quality of generated nodes, we design a novel\nfederated generation-and-reflection mechanism for LLMs, without the need to\nmodify the parameters of the LLM but relying solely on the collective feedback\nfrom all clients. After neighbor generation, all the clients utilize a\npre-trained edge predictor to infer the missing edges. Furthermore, our\nframework can seamlessly integrate as a plug-in with existing FGL methods.\nExperiments on three real-world datasets demonstrate the superiority of our\nmethod compared to advanced baselines.",
      "tldr_zh": "本研究针对联邦图学习（FGL）中的异质性问题（如节点标签分布的non-IID特性），提出了一种数据导向框架LLM4FGL，利用Large Language Models（LLMs）从数据层面增强本地文本属性图。框架将任务分解为两个子任务：首先，使用LLMs生成缺失的邻居节点，然后通过预训练的边预测器推断节点间的连接；同时，引入联邦生成和反射机制（federated generation-and-reflection mechanism），通过客户端集体反馈提高生成质量，而无需修改LLMs参数。实验在三个真实数据集上表明，该框架可无缝整合到现有FGL方法中，并显著优于先进基线。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ongoing work",
      "pdf_url": "http://arxiv.org/pdf/2503.19455v1",
      "published_date": "2025-03-25 08:43:08 UTC",
      "updated_date": "2025-03-25 08:43:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:06:50.466176"
    },
    {
      "arxiv_id": "2503.19449v1",
      "title": "VecTrans: LLM Transformation Framework for Better Auto-vectorization on High-performance CPU",
      "title_zh": "VecTrans：LLM 变换框架，用于在高性能 CPU 上实现更好的自动向量化",
      "authors": [
        "Zhongchun Zheng",
        "Long Cheng",
        "Lu Li",
        "Rodrigo C. O. Rocha",
        "Tianyi Liu",
        "Wei Wei",
        "Xianwei Zhang",
        "Yaoqing Gao"
      ],
      "abstract": "Large language models (LLMs) have demonstrated great capabilities in code\ngeneration, yet their effective application in compiler optimizations remains\nan open challenge due to issues such as hallucinations and a lack of\ndomain-specific reasoning. Vectorization, a crucial optimization for enhancing\ncode performance, often fails because of the compiler's inability to recognize\ncomplex code patterns, which commonly require extensive empirical expertise.\nLLMs, with their ability to capture intricate patterns, thus providing a\npromising solution to this challenge. This paper presents VecTrans, a novel\nframework that leverages LLMs to enhance compiler-based code vectorization.\nVecTrans first employs compiler analysis to identify potentially vectorizable\ncode regions. It then utilizes an LLM to refactor these regions into patterns\nthat are more amenable to the compiler's auto-vectorization. To ensure semantic\ncorrectness, VecTrans further integrates a hybrid validation mechanism at the\nintermediate representation (IR) level. With the above efforts, VecTrans\ncombines the adaptability of LLMs with the precision of compiler vectorization,\nthereby effectively opening up the vectorization opportunities. Experimental\nresults show that among all 50 TSVC functions unvectorizable by Clang, GCC, and\nBiShengCompiler, VecTrans successfully vectorizes 23 cases (46%) and achieves\nan average speedup of 2.02x, greatly surpassing state-of-the-art performance.",
      "tldr_zh": "这篇论文提出VecTrans框架，利用LLMs（Large Language Models）来提升高性能CPU上的代码自动向量化，解决编译器在识别复杂代码模式时的局限性。框架首先通过编译器分析识别潜在可向量化区域，然后使用LLMs重构这些区域以更适合自动向量化，并整合IR（Intermediate Representation）级别的混合验证机制确保语义正确性。实验结果显示，在50个TSVC函数中，VecTrans成功向量化了23个（46%），并实现了平均2.02倍的加速，显著超越现有编译器如Clang、GCC和BiShengCompiler。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19449v1",
      "published_date": "2025-03-25 08:39:35 UTC",
      "updated_date": "2025-03-25 08:39:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:07:01.976036"
    },
    {
      "arxiv_id": "2503.19940v1",
      "title": "FuXi-RTM: A Physics-Guided Prediction Framework with Radiative Transfer Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Qiusheng Huang",
        "Xiaohui Zhong",
        "Xu Fan",
        "Lei Chen",
        "Hao Li"
      ],
      "abstract": "Similar to conventional video generation, current deep learning-based weather\nprediction frameworks often lack explicit physical constraints, leading to\nunphysical outputs that limit their reliability for operational forecasting.\nAmong various physical processes requiring proper representation, radiation\nplays a fundamental role as it drives Earth's weather and climate systems.\nHowever, accurate simulation of radiative transfer processes remains\nchallenging for traditional numerical weather prediction (NWP) models due to\ntheir inherent complexity and high computational costs. Here, we propose\nFuXi-RTM, a hybrid physics-guided deep learning framework designed to enhance\nweather forecast accuracy while enforcing physical consistency. FuXi-RTM\nintegrates a primary forecasting model (FuXi) with a fixed deep learning-based\nradiative transfer model (DLRTM) surrogate that efficiently replaces\nconventional radiation parameterization schemes. This represents the first deep\nlearning-based weather forecasting framework to explicitly incorporate physical\nprocess modeling. Evaluated over a comprehensive 5-year dataset, FuXi-RTM\noutperforms its unconstrained counterpart in 88.51% of 3320 variable and lead\ntime combinations, with improvements in radiative flux predictions. By\nincorporating additional physical processes, FuXi-RTM paves the way for\nnext-generation weather forecasting systems that are both accurate and\nphysically consistent.",
      "tldr_zh": "这篇论文提出了FuXi-RTM，一种结合物理引导的深度学习框架，用于提升天气预测的准确性和物理一致性，以解决传统模型在辐射传输建模中的复杂性和计算成本问题。FuXi-RTM将FuXi主预测模型与一个固定深度学习辐射传输模型(DLRTM)整合，取代传统的辐射参数化方案，从而明确纳入辐射等物理过程。实验结果显示，在5年数据集上评估，该框架在88.51%的3320个变量和预测时间组合中优于无约束基线模型，尤其在辐射通量预测方面表现出显著改善。通过这种方法，FuXi-RTM为开发下一代准确且物理一致的天气预报系统铺平了道路。",
      "categories": [
        "physics.ao-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19940v1",
      "published_date": "2025-03-25 08:21:58 UTC",
      "updated_date": "2025-03-25 08:21:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:07:14.646515"
    },
    {
      "arxiv_id": "2503.19426v1",
      "title": "DeCAP: Context-Adaptive Prompt Generation for Debiasing Zero-shot Question Answering in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Suyoung Bae",
        "YunSeok Choi",
        "Jee-Hyong Lee"
      ],
      "abstract": "While Large Language Models (LLMs) excel in zero-shot Question Answering\n(QA), they tend to expose biases in their internal knowledge when faced with\nsocially sensitive questions, leading to a degradation in performance. Existing\nzero-shot methods are efficient but fail to consider context and prevent bias\npropagation in the answers. To address this, we propose DeCAP, a method for\ndebiasing LLMs using Context-Adaptive Prompt Generation. DeCAP leverages a\nQuestion Ambiguity Detection to take appropriate debiasing actions based on the\ncontext and a Neutral Answer Guidance Generation to suppress the LLMs make\nobjective judgments about the context, minimizing the propagation of bias from\ntheir internal knowledge. Our various experiments across eight LLMs show that\nDeCAP achieves state-of-the-art zero-shot debiased QA performance. This\ndemonstrates DeCAP's efficacy in enhancing the fairness and accuracy of LLMs in\ndiverse QA settings.",
      "tldr_zh": "大型语言模型（LLMs）在零-shot Question Answering（QA）中表现出色，但容易在社会敏感问题上暴露偏见，导致性能下降。论文提出 DeCAP 方法，通过 Context-Adaptive Prompt Generation 动态生成自适应提示，包括 Question Ambiguity Detection 来识别问题歧义，以及 Neutral Answer Guidance Generation 来抑制偏见传播，确保答案更客观。实验在八个 LLMs 上证明，DeCAP 实现了最先进的零样本去偏见 QA 性能，提升了模型的公平性和准确性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2025 main. 20 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.19426v1",
      "published_date": "2025-03-25 08:16:35 UTC",
      "updated_date": "2025-03-25 08:16:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:07:25.982079"
    },
    {
      "arxiv_id": "2503.19394v1",
      "title": "Quantifying Symptom Causality in Clinical Decision Making: An Exploration Using CausaLM",
      "title_zh": "在临床决策中量化症状因果关系：使用 CausaLM 进行的一项探索",
      "authors": [
        "Mehul Shetty",
        "Connor Jordan"
      ],
      "abstract": "Current machine learning approaches to medical diagnosis often rely on\ncorrelational patterns between symptoms and diseases, risking misdiagnoses when\nsymptoms are ambiguous or common across multiple conditions. In this work, we\nmove beyond correlation to investigate the causal influence of key\nsymptoms-specifically \"chest pain\" on diagnostic predictions. Leveraging the\nCausaLM framework, we generate counterfactual text representations in which\ntarget concepts are effectively \"forgotten\" enabling a principled estimation of\nthe causal effect of that concept on a model's predicted disease distribution.\nBy employing Textual Representation-based Average Treatment Effect (TReATE), we\nquantify how the presence or absence of a symptom shapes the model's diagnostic\noutcomes, and contrast these findings against correlation-based baselines such\nas CONEXP. Our results offer deeper insight into the decision-making behavior\nof clinical NLP models and have the potential to inform more trustworthy,\ninterpretable, and causally-grounded decision support tools in medical\npractice.",
      "tldr_zh": "本文研究了如何超越症状与疾病的相关性，量化关键症状（如“chest pain”）对临床决策的因果影响，以减少潜在误诊风险。作者利用 CausaLM 框架生成反事实文本表示，并应用 Textual Representation-based Average Treatment Effect (TReATE) 方法，估算症状存在或缺失对模型诊断分布的因果效应，并与相关性基线如 CONEXP 进行对比。结果揭示了临床 NLP 模型的决策行为，提供更深入的见解，有助于开发可信、可解释的因果导向医疗决策支持工具。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19394v1",
      "published_date": "2025-03-25 06:59:21 UTC",
      "updated_date": "2025-03-25 06:59:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:07:38.275104"
    },
    {
      "arxiv_id": "2504.03700v1",
      "title": "SAFE: Self-Adjustment Federated Learning Framework for Remote Sensing Collaborative Perception",
      "title_zh": "SAFE：自调整联邦学习框架用于遥感协作感知",
      "authors": [
        "Xiaohe Li",
        "Haohua Wu",
        "Jiahao Li",
        "Zide Fan",
        "Kaixin Zhang",
        "Xinming Li",
        "Yunping Ge",
        "Xinyu Zhao"
      ],
      "abstract": "The rapid increase in remote sensing satellites has led to the emergence of\ndistributed space-based observation systems. However, existing distributed\nremote sensing models often rely on centralized training, resulting in data\nleakage, communication overhead, and reduced accuracy due to data distribution\ndiscrepancies across platforms. To address these challenges, we propose the\n\\textit{Self-Adjustment FEderated Learning} (SAFE) framework, which\ninnovatively leverages federated learning to enhance collaborative sensing in\nremote sensing scenarios. SAFE introduces four key strategies: (1)\n\\textit{Class Rectification Optimization}, which autonomously addresses class\nimbalance under unknown local and global distributions. (2) \\textit{Feature\nAlignment Update}, which mitigates Non-IID data issues via locally controlled\nEMA updates. (3) \\textit{Dual-Factor Modulation Rheostat}, which dynamically\nbalances optimization effects during training. (4) \\textit{Adaptive Context\nEnhancement}, which is designed to improve model performance by dynamically\nrefining foreground regions, ensuring computational efficiency with accuracy\nimprovement across distributed satellites. Experiments on real-world image\nclassification and object segmentation datasets validate the effectiveness and\nreliability of the SAFE framework in complex remote sensing scenarios.",
      "tldr_zh": "这篇论文提出了 SAFE（Self-Adjustment Federated Learning）框架，利用联邦学习（Federated Learning）解决分布式遥感卫星系统的协作感知问题，包括数据泄露、通信开销和数据分布差异导致的准确率下降。框架引入四个关键策略：Class Rectification Optimization 自主处理未知分布下的类别不平衡、Feature Alignment Update 通过本地 EMA 更新缓解 Non-IID 数据问题、Dual-Factor Modulation Rheostat 动态平衡优化效果，以及 Adaptive Context Enhancement 通过动态优化前景区域提升模型性能，同时确保计算效率。实验在真实图像分类和对象分割数据集上验证了 SAFE 的有效性，在复杂遥感场景中显著提高了准确性和可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03700v1",
      "published_date": "2025-03-25 06:39:34 UTC",
      "updated_date": "2025-03-25 06:39:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:07:49.640984"
    },
    {
      "arxiv_id": "2503.21807v1",
      "title": "LERO: LLM-driven Evolutionary framework with Hybrid Rewards and Enhanced Observation for Multi-Agent Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan Wei",
        "Xiaohan Shan",
        "Jianmin Li"
      ],
      "abstract": "Multi-agent reinforcement learning (MARL) faces two critical bottlenecks\ndistinct from single-agent RL: credit assignment in cooperative tasks and\npartial observability of environmental states. We propose LERO, a framework\nintegrating Large language models (LLMs) with evolutionary optimization to\naddress these MARL-specific challenges. The solution centers on two\nLLM-generated components: a hybrid reward function that dynamically allocates\nindividual credit through reward decomposition, and an observation enhancement\nfunction that augments partial observations with inferred environmental\ncontext. An evolutionary algorithm optimizes these components through iterative\nMARL training cycles, where top-performing candidates guide subsequent LLM\ngenerations. Evaluations in Multi-Agent Particle Environments (MPE) demonstrate\nLERO's superiority over baseline methods, with improved task performance and\ntraining efficiency.",
      "tldr_zh": "这篇论文提出 LERO 框架，将大型语言模型 (LLMs) 与进化优化相结合，针对多智能体强化学习 (MARL) 中的信用分配和部分可观察性问题提供解决方案。LERO 通过 LLM 生成的混合奖励函数动态分解奖励以分配个体信用，以及观察增强函数利用推断的环境上下文来丰富部分观察。实验在 Multi-Agent Particle Environments (MPE) 中显示，LERO 相较于基线方法显著提升了任务性能和训练效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21807v1",
      "published_date": "2025-03-25 06:28:42 UTC",
      "updated_date": "2025-03-25 06:28:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:08:01.672481"
    },
    {
      "arxiv_id": "2503.19382v1",
      "title": "Causal invariant geographic network representations with feature and structural distribution shifts",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhan Wang",
        "Silu He",
        "Qinyao Luo",
        "Hongyuan Yuan",
        "Ling Zhao",
        "Jiawei Zhu",
        "Haifeng Li"
      ],
      "abstract": "The existing methods learn geographic network representations through deep\ngraph neural networks (GNNs) based on the i.i.d. assumption. However, the\nspatial heterogeneity and temporal dynamics of geographic data make the\nout-of-distribution (OOD) generalisation problem particularly salient. The\nlatter are particularly sensitive to distribution shifts (feature and\nstructural shifts) between testing and training data and are the main causes of\nthe OOD generalisation problem. Spurious correlations are present between\ninvariant and background representations due to selection biases and\nenvironmental effects, resulting in the model extremes being more likely to\nlearn background representations. The existing approaches focus on background\nrepresentation changes that are determined by shifts in the feature\ndistributions of nodes in the training and test data while ignoring changes in\nthe proportional distributions of heterogeneous and homogeneous neighbour\nnodes, which we refer to as structural distribution shifts. We propose a\nfeature-structure mixed invariant representation learning (FSM-IRL) model that\naccounts for both feature distribution shifts and structural distribution\nshifts. To address structural distribution shifts, we introduce a sampling\nmethod based on causal attention, encouraging the model to identify nodes\npossessing strong causal relationships with labels or nodes that are more\nsimilar to the target node. Inspired by the Hilbert-Schmidt independence\ncriterion, we implement a reweighting strategy to maximise the orthogonality of\nthe node representations, thereby mitigating the spurious correlations among\nthe node representations and suppressing the learning of background\nrepresentations. Our experiments demonstrate that FSM-IRL exhibits strong\nlearning capabilities on both geographic and social network datasets in OOD\nscenarios.",
      "tldr_zh": "现有方法使用深度图神经网络 (GNNs) 学习地理网络表示，但受限于数据分布偏移（包括特征分布偏移和结构分布偏移），导致 out-of-distribution (OOD) 泛化问题加剧。针对此，本文提出特征-结构混合不变表示学习 (FSM-IRL) 模型，通过基于因果注意力的采样方法，鼓励模型关注与标签有强因果关系的节点，并采用受 Hilbert-Schmidt independence criterion (HSIC) 启发的再加权策略，最大化节点表示的正交性以减少虚假相关性。实验结果显示，FSM-IRL 在地理和社交网络数据集的 OOD 场景中表现出色，提升了模型的鲁棒性和泛化能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 3 figures, 8 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.19382v1",
      "published_date": "2025-03-25 06:21:57 UTC",
      "updated_date": "2025-03-25 06:21:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:08:14.780157"
    },
    {
      "arxiv_id": "2503.19373v1",
      "title": "DeClotH: Decomposable 3D Cloth and Human Body Reconstruction from a Single Image",
      "title_zh": "翻译失败",
      "authors": [
        "Hyeongjin Nam",
        "Donghwan Kim",
        "Jeongtaek Oh",
        "Kyoung Mu Lee"
      ],
      "abstract": "Most existing methods of 3D clothed human reconstruction from a single image\ntreat the clothed human as a single object without distinguishing between cloth\nand human body. In this regard, we present DeClotH, which separately\nreconstructs 3D cloth and human body from a single image. This task remains\nlargely unexplored due to the extreme occlusion between cloth and the human\nbody, making it challenging to infer accurate geometries and textures.\nMoreover, while recent 3D human reconstruction methods have achieved impressive\nresults using text-to-image diffusion models, directly applying such an\napproach to this problem often leads to incorrect guidance, particularly in\nreconstructing 3D cloth. To address these challenges, we propose two core\ndesigns in our framework. First, to alleviate the occlusion issue, we leverage\n3D template models of cloth and human body as regularizations, which provide\nstrong geometric priors to prevent erroneous reconstruction by the occlusion.\nSecond, we introduce a cloth diffusion model specifically designed to provide\ncontextual information about cloth appearance, thereby enhancing the\nreconstruction of 3D cloth. Qualitative and quantitative experiments\ndemonstrate that our proposed approach is highly effective in reconstructing\nboth 3D cloth and the human body. More qualitative results are provided at\nhttps://hygenie1228.github.io/DeClotH/.",
      "tldr_zh": "该研究提出DeClotH框架，从单张图像中分解重建3D布物和人体，解决了现有方法将衣着人体视为单一物体的局限性。针对衣物与人体的极端遮挡问题，该框架采用3D模板模型作为几何先验进行正则化，并引入专门的布物扩散模型（cloth diffusion model）来提供布物外观的上下文信息，从而提升重建准确性。实验结果显示，DeClotH在定性和定量评估中表现出色，有效地重建了3D布物和人体细节。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Published at CVPR 2025, 17 pages including the supplementary material",
      "pdf_url": "http://arxiv.org/pdf/2503.19373v1",
      "published_date": "2025-03-25 06:00:15 UTC",
      "updated_date": "2025-03-25 06:00:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:08:25.654874"
    },
    {
      "arxiv_id": "2503.21806v1",
      "title": "Large Language Models Meet Contrastive Learning: Zero-Shot Emotion Recognition Across Languages",
      "title_zh": "翻译失败",
      "authors": [
        "Heqing Zou",
        "Fengmao Lv",
        "Desheng Zheng",
        "Eng Siong Chng",
        "Deepu Rajan"
      ],
      "abstract": "Multilingual speech emotion recognition aims to estimate a speaker's\nemotional state using a contactless method across different languages. However,\nvariability in voice characteristics and linguistic diversity poses significant\nchallenges for zero-shot speech emotion recognition, especially with\nmultilingual datasets. In this paper, we propose leveraging contrastive\nlearning to refine multilingual speech features and extend large language\nmodels for zero-shot multilingual speech emotion estimation. Specifically, we\nemploy a novel two-stage training framework to align speech signals with\nlinguistic features in the emotional space, capturing both emotion-aware and\nlanguage-agnostic speech representations. To advance research in this field, we\nintroduce a large-scale synthetic multilingual speech emotion dataset, M5SER.\nOur experiments demonstrate the effectiveness of the proposed method in both\nspeech emotion recognition and zero-shot multilingual speech emotion\nrecognition, including previously unseen datasets and languages.",
      "tldr_zh": "本文提出了一种结合对比学习（contrastive learning）和大型语言模型（large language models）的方法，用于实现跨语言的零样本（zero-shot）语音情感识别，旨在解决语音特征变异性和语言多样性的挑战。该方法采用一个两阶段训练框架，将语音信号与语言特征在情感空间中对齐，生成情感感知且语言无关的语音表示。为了推进研究，我们引入了大规模合成数据集 M5SER，并通过实验验证，该方法在语音情感识别和零样本多语言任务上表现出色，包括对未见数据集和语言的适用性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ICME 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.21806v1",
      "published_date": "2025-03-25 05:58:18 UTC",
      "updated_date": "2025-03-25 05:58:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:08:38.046165"
    },
    {
      "arxiv_id": "2503.19371v2",
      "title": "Flow to Learn: Flow Matching on Neural Network Parameters",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Saragih",
        "Deyu Cao",
        "Tejas Balaji",
        "Ashwin Santhosh"
      ],
      "abstract": "Foundational language models show a remarkable ability to learn new concepts\nduring inference via context data. However, similar work for images lag behind.\nTo address this challenge, we introduce FLoWN, a flow matching model that\nlearns to generate neural network parameters for different tasks. Our approach\nmodels the flow on latent space, while conditioning the process on context\ndata. Experiments verify that FLoWN attains various desiderata for a\nmeta-learning model. In addition, it matches or exceeds baselines on\nin-distribution tasks, provides better initializations for classifier training,\nand is performant on out-of-distribution few-shot tasks while having a\nfine-tuning mechanism to improve performance.",
      "tldr_zh": "该论文提出 FLoWN，一种基于 flow matching 的模型，用于生成 neural network parameters，以帮助图像模型通过 context data 学习新概念，从而解决语言模型在这一领域的优势。方法通过在 latent space 上建模流，并基于上下文数据进行条件化，实现对不同任务的适应。实验结果显示，FLoWN 在 meta-learning 方面满足关键要求，能在 in-distribution 任务上匹配或超过基线，提供更好的分类器初始化，并在 out-of-distribution few-shot 任务上表现出色，同时支持微调机制提升性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the ICLR Workshop on Neural Network Weights as a New Data\n  Modality 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.19371v2",
      "published_date": "2025-03-25 05:57:50 UTC",
      "updated_date": "2025-04-19 13:42:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:08:50.020216"
    },
    {
      "arxiv_id": "2503.21805v2",
      "title": "ImF: Implicit Fingerprint for Large Language Models",
      "title_zh": "ImF：大型语言模型的隐式指纹",
      "authors": [
        "Wu jiaxuan",
        "Peng Wanli",
        "Fu hang",
        "Xue Yiming",
        "Wen juan"
      ],
      "abstract": "Training large language models (LLMs) is resource-intensive and expensive,\nmaking protecting intellectual property (IP) for LLMs crucial. Recently,\nembedding fingerprints into LLMs has emerged as a prevalent method for\nestablishing model ownership. However, existing fingerprinting techniques\ntypically embed identifiable patterns with weak semantic coherence, resulting\nin fingerprints that significantly differ from the natural question-answering\n(QA) behavior inherent to LLMs. This discrepancy undermines the stealthiness of\nthe embedded fingerprints and makes them vulnerable to adversarial attacks. In\nthis paper, we first demonstrate the critical vulnerability of existing\nfingerprint embedding methods by introducing a novel adversarial attack named\nGeneration Revision Intervention (GRI) attack. GRI attack exploits the semantic\nfragility of current fingerprinting methods, effectively erasing fingerprints\nby disrupting their weakly correlated semantic structures. Our empirical\nevaluation highlights that traditional fingerprinting approaches are\nsignificantly compromised by the GRI attack, revealing severe limitations in\ntheir robustness under realistic adversarial conditions. To advance the\nstate-of-the-art in model fingerprinting, we propose a novel model fingerprint\nparadigm called Implicit Fingerprints (ImF). ImF leverages steganography\ntechniques to subtly embed ownership information within natural texts,\nsubsequently using Chain-of-Thought (CoT) prompting to construct semantically\ncoherent and contextually natural QA pairs. This design ensures that\nfingerprints seamlessly integrate with the standard model behavior, remaining\nindistinguishable from regular outputs and substantially reducing the risk of\naccidental triggering and targeted removal. We conduct a comprehensive\nevaluation of ImF on 15 diverse LLMs, spanning different architectures and\nvarying scales.",
      "tldr_zh": "本研究针对大型语言模型 (LLMs) 的知识产权保护问题，指出现有指纹嵌入方法存在语义连贯性弱的缺陷，导致指纹易被 Generation Revision Intervention (GRI) 攻击破坏，从而削弱其隐秘性和鲁棒性。作者提出了一种新型指纹范式 Implicit Fingerprints (ImF)，利用隐写术 (steganography) 在自然文本中嵌入所有权信息，并结合 Chain-of-Thought (CoT) 提示构建语义连贯的问答 (QA) 对，使指纹与模型的正常行为无缝整合，减少被检测或移除的风险。在 15 个不同架构和规模的 LLMs 上进行的全面评估表明，ImF 显著提升了指纹的安全性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.21805v2",
      "published_date": "2025-03-25 05:47:34 UTC",
      "updated_date": "2025-05-17 23:00:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:09:02.613926"
    },
    {
      "arxiv_id": "2504.03699v3",
      "title": "Reinforcing Clinical Decision Support through Multi-Agent Systems and Ethical AI Governance",
      "title_zh": "翻译失败",
      "authors": [
        "Ying-Jung Chen",
        "Ahmad Albarqawi",
        "Chi-Sheng Chen"
      ],
      "abstract": "Recent advances in the data-driven medicine approach, which integrates\nethically managed and explainable artificial intelligence into clinical\ndecision support systems (CDSS), are critical to ensure reliable and effective\npatient care. This paper focuses on comparing novel agent system designs that\nuse modular agents to analyze laboratory results, vital signs, and clinical\ncontext, and to predict and validate results. We implement our agent system\nwith the eICU database, including running lab analysis, vitals-only\ninterpreters, and contextual reasoners agents first, then sharing the memory\ninto the integration agent, prediction agent, transparency agent, and a\nvalidation agent. Our results suggest that the multi-agent system (MAS)\nperformed better than the single-agent system (SAS) with mortality prediction\naccuracy (59%, 56%) and the mean error for length of stay (LOS)(4.37 days, 5.82\ndays), respectively. However, the transparency score for the SAS (86.21) is\nslightly better than the transparency score for MAS (85.5). Finally, this study\nsuggests that our agent-based framework not only improves process transparency\nand prediction accuracy but also strengthens trustworthy AI-assisted decision\nsupport in an intensive care setting.",
      "tldr_zh": "本研究探讨了通过多代理系统（Multi-Agent Systems, MAS）和伦理 AI 治理强化临床决策支持系统（CDSS）的潜力，重点比较 MAS 和单代理系统（Single-Agent System, SAS）在分析实验室结果、生命体征和临床背景方面的性能。研究使用 eICU 数据库构建代理系统，包括实验室分析代理、生命体征解释器、背景推理器等模块，并通过整合、预测、透明度和验证代理共享记忆。结果显示，MAS 在死亡率预测准确率（59% vs. 56%）和住院天数平均误差（4.37 天 vs. 5.82 天）上优于 SAS，但 SAS 的透明度分数（86.21）略高于 MAS（85.5）。总体而言，该框架提升了过程透明度、预测准确性和可信赖的 AI 辅助决策，支持伦理管理的医疗应用。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG",
        "cs.MA",
        "q-bio.QM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03699v3",
      "published_date": "2025-03-25 05:32:43 UTC",
      "updated_date": "2025-04-15 05:26:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:09:15.387982"
    },
    {
      "arxiv_id": "2503.21804v2",
      "title": "Comparison of Metadata Representation Models for Knowledge Graph Embeddings",
      "title_zh": "知识图谱嵌入的元数据表示模型比较",
      "authors": [
        "Shusaku Egami",
        "Kyoumoto Matsushita",
        "Takanori Ugai",
        "Ken Fukuda"
      ],
      "abstract": "Hyper-relational Knowledge Graphs (HRKGs) extend traditional KGs beyond\nbinary relations, enabling the representation of contextual, provenance, and\ntemporal information in domains, such as historical events, sensor data, video\ncontent, and narratives. HRKGs can be structured using several Metadata\nRepresentation Models (MRMs), including Reification (REF), Singleton Property\n(SGP), and RDF-star (RDR). However, the effects of different MRMs on KG\nEmbedding (KGE) and Link Prediction (LP) models remain unclear. This study\nevaluates MRMs in the context of LP tasks, identifies the limitations of\nexisting evaluation frameworks, and introduces a new task that ensures fair\ncomparisons across MRMs. Furthermore, we propose a framework that effectively\nreflects the knowledge representations of the three MRMs in latent space.\nExperiments on two types of datasets reveal that REF performs well in simple\nHRKGs, whereas SGP is less effective. However, in complex HRKGs, the\ndifferences among MRMs in the LP tasks are minimal. Our findings contribute to\nan optimal knowledge representation strategy for HRKGs in LP tasks.",
      "tldr_zh": "该研究比较了不同元数据表示模型(Metadata Representation Models, MRMs)，包括 Reification (REF)、Singleton Property (SGP) 和 RDF-star (RDR)，对超关系知识图谱(Hyper-relational Knowledge Graphs, HRKGs)嵌入(Knowledge Graph Embeddings, KGE)和链接预测(Link Prediction, LP)任务的影响。研究识别了现有评估框架的局限性，并引入了一个新任务和框架，以确保MRMs之间的公平比较，并将这些模型的知识表示映射到潜在空间。实验在两种数据集上显示，REF在简单HRKGs中表现最佳，而SGP效果较差；在复杂HRKGs中，MRMs在LP任务中的差异最小。该工作为HRKGs在LP任务中的最佳知识表示策略提供了重要指导。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR",
        "I.2.4; I.2.7"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 9 Figures",
      "pdf_url": "http://arxiv.org/pdf/2503.21804v2",
      "published_date": "2025-03-25 04:46:23 UTC",
      "updated_date": "2025-03-31 04:31:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:09:27.440143"
    },
    {
      "arxiv_id": "2504.00008v1",
      "title": "Tensor Generalized Approximate Message Passing",
      "title_zh": "翻译失败",
      "authors": [
        "Yinchuan Li",
        "Guangchen Lan",
        "Xiaodong Wang"
      ],
      "abstract": "We propose a tensor generalized approximate message passing (TeG-AMP)\nalgorithm for low-rank tensor inference, which can be used to solve tensor\ncompletion and decomposition problems. We derive TeG-AMP algorithm as an\napproximation of the sum-product belief propagation algorithm in high\ndimensions where the central limit theorem and Taylor series approximations are\napplicable. As TeG-AMP is developed based on a general TR decomposition model,\nit can be directly applied to many low-rank tensor types. Moreover, our TeG-AMP\ncan be simplified based on the CP decomposition model and a tensor simplified\nAMP is proposed for low CP-rank tensor inference problems. Experimental results\ndemonstrate that the proposed methods significantly improve recovery\nperformances since it takes full advantage of tensor structures.",
      "tldr_zh": "本文提出了一种张量广义近似消息传递(TeG-AMP)算法，用于低秩张量推理问题，包括张量补全和分解。TeG-AMP 通过对求和-乘积信念传播算法的近似实现，结合中心极限定理和泰勒级数，在高维场景下有效运行，并基于通用的 TR 分解模型适用于多种低秩张量类型。此外，该算法可简化至基于 CP 分解的张量简化 AMP 版本。实验结果显示，该方法显著提升了恢复性能，因为它充分利用了张量结构。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT",
        "E.4; I.2.0; I.2.6; I.4"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.00008v1",
      "published_date": "2025-03-25 04:17:10 UTC",
      "updated_date": "2025-03-25 04:17:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:09:38.762151"
    },
    {
      "arxiv_id": "2503.21803v1",
      "title": "Forecasting Volcanic Radiative Power (VPR) at Fuego Volcano Using Bayesian Regularized Neural Network",
      "title_zh": "翻译失败",
      "authors": [
        "Snehamoy Chatterjee",
        "Greg Waite",
        "Sidike Paheding",
        "Luke Bowman"
      ],
      "abstract": "Forecasting volcanic activity is critical for hazard assessment and risk\nmitigation. Volcanic Radiative Power (VPR), derived from thermal remote sensing\ndata, serves as an essential indicator of volcanic activity. In this study, we\nemploy Bayesian Regularized Neural Networks (BRNN) to predict future VPR values\nbased on historical data from Fuego Volcano, comparing its performance against\nScaled Conjugate Gradient (SCG) and Levenberg-Marquardt (LM) models. The\nresults indicate that BRNN outperforms SCG and LM, achieving the lowest mean\nsquared error (1.77E+16) and the highest R-squared value (0.50), demonstrating\nits superior ability to capture VPR variability while minimizing overfitting.\nDespite these promising results, challenges remain in improving the model's\npredictive accuracy. Future research should focus on integrating additional\ngeophysical parameters, such as seismic and gas emission data, to enhance\nforecasting precision. The findings highlight the potential of machine learning\nmodels, particularly BRNN, in advancing volcanic activity forecasting,\ncontributing to more effective early warning systems for volcanic hazards.",
      "tldr_zh": "本文使用 Bayesian Regularized Neural Networks (BRNN) 基于 Fuego Volcano 的历史数据预测 Volcanic Radiative Power (VPR)，并与 Scaled Conjugate Gradient (SCG) 和 Levenberg-Marquardt (LM) 模型进行比较。结果显示，BRNN 取得了最低的 mean squared error (1.77E+16) 和最高的 R-squared 值 (0.50)，证明其在捕捉 VPR 变异性和减少过拟合方面表现出色。未来研究应整合更多地球物理参数，如地震和气体排放数据，以进一步提升预测精度，并为火山活动预警系统提供更有效的支持。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP",
        "physics.ao-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.21803v1",
      "published_date": "2025-03-25 04:15:24 UTC",
      "updated_date": "2025-03-25 04:15:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:09:50.952948"
    },
    {
      "arxiv_id": "2503.19339v3",
      "title": "Efficient IoT Intrusion Detection with an Improved Attention-Based CNN-BiLSTM Architecture",
      "title_zh": "翻译失败",
      "authors": [
        "Amna Naeem",
        "Muazzam A. Khan",
        "Nada Alasbali",
        "Jawad Ahmad",
        "Aizaz Ahmad Khattak",
        "Muhammad Shahbaz Khan"
      ],
      "abstract": "The ever-increasing security vulnerabilities in the Internet-of-Things (IoT)\nsystems require improved threat detection approaches. This paper presents a\ncompact and efficient approach to detect botnet attacks by employing an\nintegrated approach that consists of traffic pattern analysis, temporal support\nlearning, and focused feature extraction. The proposed attention-based model\nbenefits from a hybrid CNN-BiLSTM architecture and achieves 99% classification\naccuracy in detecting botnet attacks utilizing the N-BaIoT dataset, while\nmaintaining high precision and recall across various scenarios. The proposed\nmodel's performance is further validated by key parameters, such as Mathews\nCorrelation Coefficient and Cohen's kappa Correlation Coefficient. The\nclose-to-ideal results for these parameters demonstrate the proposed model's\nability to detect botnet attacks accurately and efficiently in practical\nsettings and on unseen data. The proposed model proved to be a powerful defence\nmechanism for IoT networks to face emerging security challenges.",
      "tldr_zh": "这篇论文提出了一种高效的 IoT 入侵检测方法，采用改进的注意力机制结合 CNN-BiLSTM 架构，通过流量模式分析、时间支持学习和聚焦特征提取来检测 botnet 攻击。实验在 N-BaIoT 数据集上实现了 99% 的分类准确率，同时在精确度和召回率方面表现出色。模型通过 Mathews Correlation Coefficient 和 Cohen's kappa Correlation Coefficient 等指标验证了其在实际场景和未见数据上的准确性和鲁棒性，为 IoT 网络提供强大的防御机制。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19339v3",
      "published_date": "2025-03-25 04:12:14 UTC",
      "updated_date": "2025-05-01 15:12:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:10:03.133098"
    },
    {
      "arxiv_id": "2503.19329v1",
      "title": "Wavelet-based Global-Local Interaction Network with Cross-Attention for Multi-View Diabetic Retinopathy Detection",
      "title_zh": "基于小波的全局-局部交互网络，结合交叉注意力，用于多视图糖尿病视网膜病变检测",
      "authors": [
        "Yongting Hu",
        "Yuxin Lin",
        "Chengliang Liu",
        "Xiaoling Luo",
        "Xiaoyan Dou",
        "Qihao Xu",
        "Yong Xu"
      ],
      "abstract": "Multi-view diabetic retinopathy (DR) detection has recently emerged as a\npromising method to address the issue of incomplete lesions faced by\nsingle-view DR. However, it is still challenging due to the variable sizes and\nscattered locations of lesions. Furthermore, existing multi-view DR methods\ntypically merge multiple views without considering the correlations and\nredundancies of lesion information across them. Therefore, we propose a novel\nmethod to overcome the challenges of difficult lesion information learning and\ninadequate multi-view fusion. Specifically, we introduce a two-branch network\nto obtain both local lesion features and their global dependencies. The\nhigh-frequency component of the wavelet transform is used to exploit lesion\nedge information, which is then enhanced by global semantic to facilitate\ndifficult lesion learning. Additionally, we present a cross-view fusion module\nto improve multi-view fusion and reduce redundancy. Experimental results on\nlarge public datasets demonstrate the effectiveness of our method. The code is\nopen sourced on https://github.com/HuYongting/WGLIN.",
      "tldr_zh": "该论文针对多视图糖尿病视网膜病变（DR）检测的挑战，提出了一种基于小波变换（wavelet transform）的全局-局部交互网络，以解决病变大小不一和位置分散的问题，以及现有方法在多视图融合中忽略相关性和冗余的问题。方法包括一个两分支网络：利用wavelet transform的高频成分提取病变边缘信息，并通过全局语义增强来改善困难病变学习；同时引入cross-view fusion module来优化多视图融合并减少冗余。实验在大规模公共数据集上证明了该方法的有效性，并已开源代码于GitHub。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted by IEEE International Conference on Multimedia & Expo (ICME)\n  2025",
      "pdf_url": "http://arxiv.org/pdf/2503.19329v1",
      "published_date": "2025-03-25 03:44:57 UTC",
      "updated_date": "2025-03-25 03:44:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:10:15.463559"
    },
    {
      "arxiv_id": "2503.19328v1",
      "title": "Substance over Style: Evaluating Proactive Conversational Coaching Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Vidya Srinivas",
        "Xuhai Xu",
        "Xin Liu",
        "Kumar Ayush",
        "Isaac Galatzer-Levy",
        "Shwetak Patel",
        "Daniel McDuff",
        "Tim Althoff"
      ],
      "abstract": "While NLP research has made strides in conversational tasks, many approaches\nfocus on single-turn responses with well-defined objectives or evaluation\ncriteria. In contrast, coaching presents unique challenges with initially\nundefined goals that evolve through multi-turn interactions, subjective\nevaluation criteria, mixed-initiative dialogue. In this work, we describe and\nimplement five multi-turn coaching agents that exhibit distinct conversational\nstyles, and evaluate them through a user study, collecting first-person\nfeedback on 155 conversations. We find that users highly value core\nfunctionality, and that stylistic components in absence of core components are\nviewed negatively. By comparing user feedback with third-person evaluations\nfrom health experts and an LM, we reveal significant misalignment across\nevaluation approaches. Our findings provide insights into design and evaluation\nof conversational coaching agents and contribute toward improving\nhuman-centered NLP applications.",
      "tldr_zh": "本研究评估了主动对话指导代理（conversational coaching agents），强调实质内容胜过风格表现。研究者设计并实现了五种多轮指导代理，展示不同对话风格，并通过用户研究收集155次对话的第一人称反馈。结果显示，用户更重视代理的核心功能，而缺乏实质的风格元素会招致负面评价；此外，用户反馈与健康专家或语言模型（LM）的第三方评估存在显著不一致。这些发现为对话指导代理的设计和评估提供了宝贵见解，并推动了以人为中心的NLP应用的改进。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19328v1",
      "published_date": "2025-03-25 03:44:31 UTC",
      "updated_date": "2025-03-25 03:44:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:10:26.590191"
    },
    {
      "arxiv_id": "2503.22714v1",
      "title": "TRIDIS: A Comprehensive Medieval and Early Modern Corpus for HTR and NER",
      "title_zh": "TRIDIS：一个全面的中世纪和早期现代语料库，用于 HTR 和 NER",
      "authors": [
        "Sergio Torres Aguilar"
      ],
      "abstract": "This paper introduces TRIDIS (Tria Digita Scribunt), an open-source corpus of\nmedieval and early modern manuscripts. TRIDIS aggregates multiple legacy\ncollections (all published under open licenses) and incorporates large metadata\ndescriptions. While prior publications referenced some portions of this corpus,\nhere we provide a unified overview with a stronger focus on its constitution.\nWe describe (i) the narrative, chronological, and editorial background of each\nmajor sub-corpus, (ii) its semi-diplomatic transcription rules (expansion,\nnormalization, punctuation), (iii) a strategy for challenging out-of-domain\ntest splits driven by outlier detection in a joint embedding space, and (iv)\npreliminary baseline experiments using TrOCR and MiniCPM2.5 comparing random\nand outlier-based test partitions. Overall, TRIDIS is designed to stimulate\njoint robust Handwritten Text Recognition (HTR) and Named Entity Recognition\n(NER) research across medieval and early modern textual heritage.",
      "tldr_zh": "本论文介绍了 TRIDIS（Tria Digita Scribunt），一个开源语料库，汇集了中世纪和早期现代手稿的多个遗留集合，并包含详尽的元数据描述，以支持 HTR (Handwritten Text Recognition) 和 NER (Named Entity Recognition) 研究。论文提供了各子语料库的叙述、时间背景、编辑信息，以及半外交式转录规则（如扩展、标准化和标点处理），并提出了一种基于异常检测在联合嵌入空间中的策略，用于创建具有挑战性的领域外测试集。研究者进行了初步基准实验，使用 TrOCR 和 MiniCPM2.5 模型比较了随机测试分区与基于异常的测试分区。总体而言，TRIDIS 旨在促进中世纪和早期现代文本遗产中 HTR 和 NER 的联合鲁棒研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.DL"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages, 3 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.22714v1",
      "published_date": "2025-03-25 03:44:11 UTC",
      "updated_date": "2025-03-25 03:44:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:10:39.770990"
    },
    {
      "arxiv_id": "2503.19326v2",
      "title": "Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs to Ignore the Correct Reasoning Steps",
      "title_zh": "翻译失败",
      "authors": [
        "Yu Cui",
        "Bryan Hooi",
        "Yujun Cai",
        "Yiwei Wang"
      ],
      "abstract": "Recent reasoning large language models (LLMs) have demonstrated remarkable\nimprovements in mathematical reasoning capabilities through long\nChain-of-Thought. The reasoning tokens of these models enable self-correction\nwithin reasoning chains, enhancing robustness. This motivates our exploration:\nhow vulnerable are reasoning LLMs to subtle errors in their input reasoning\nchains? We introduce \"Compromising Thought\" (CPT), a vulnerability where models\npresented with reasoning tokens containing manipulated calculation results tend\nto ignore correct reasoning steps and adopt incorrect results instead. Through\nsystematic evaluation across multiple reasoning LLMs, we design three\nincreasingly explicit prompting methods to measure CPT resistance, revealing\nthat models struggle significantly to identify and correct these manipulations.\nNotably, contrary to existing research suggesting structural alterations affect\nmodel performance more than content modifications, we find that local ending\ntoken manipulations have greater impact on reasoning outcomes than structural\nchanges. Moreover, we discover a security vulnerability in DeepSeek-R1 where\ntampered reasoning tokens can trigger complete reasoning cessation. Our work\nenhances understanding of reasoning robustness and highlights security\nconsiderations for reasoning-intensive applications.",
      "tldr_zh": "本研究探讨了推理大语言模型（LLMs）在处理输入推理链时存在的“Compromising Thought”（CPT）漏洞，即当推理链中包含被操纵的结束标记时，模型往往忽略正确的推理步骤而采用错误结果。研究者设计了三种 increasingly explicit 的提示方法，对多个 LLMs 进行系统评估，发现局部结束标记的操纵比结构变化对推理结果的影响更大，且 DeepSeek-R1 存在安全漏洞，可能导致推理完全停止。这些发现增强了对 LLMs 推理鲁棒性的理解，并强调了在推理密集型应用中加强安全措施的必要性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19326v2",
      "published_date": "2025-03-25 03:43:11 UTC",
      "updated_date": "2025-04-01 00:07:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:10:51.635097"
    },
    {
      "arxiv_id": "2503.19311v1",
      "title": "LRSCLIP: A Vision-Language Foundation Model for Aligning Remote Sensing Image with Longer Text",
      "title_zh": "翻译失败",
      "authors": [
        "Weizhi Chen",
        "Jingbo Chen",
        "Yupeng Deng",
        "Jiansheng Chen",
        "Yuman Feng",
        "Zhihao Xi",
        "Diyou Liu",
        "Kai Li",
        "Yu Meng"
      ],
      "abstract": "This study addresses the technical bottlenecks in handling long text and the\n\"hallucination\" issue caused by insufficient short text information in remote\nsensing vision-language foundation models (VLFM). We propose a novel\nvision-language foundation model, LRSCLIP, and a multimodal dataset, LRS2M. The\nmain contributions are as follows: (1) By integrating multi-source remote\nsensing data and adopting a large language model labeling strategy, we\nconstruct the LRS2M dataset, which contains 2 million image-text pairs,\nproviding both short and long texts for the first time, thus solving the\nproblem of semantic granularity limitations in existing datasets; (2) The\ndesign of the LRSCLIP architecture based on Long-CLIP's KPS module, which\nextends CLIP's text processing capacity and achieves fine-grained cross-modal\nfeature alignment through a dual-text loss weighting mechanism. Experimental\nresults show that LRSCLIP improves retrieval accuracy by 10\\%-20\\% over the\nLong-CLIP baseline in the zero-shot long-text cross-modal retrieval task. For\nthe zero-shot short-text cross-modal retrieval task, LRSCLIP achieves\nimprovements over the current best model, GeoRSCLIP, with increases of 0.17\\%,\n0.67\\%, and 0.92\\% in Text to Image R@1, Image to Text R@1, and mR on RSITMD,\nrespectively, and 0.04\\%, 2.93\\%, and 1.28\\% on RSICD. In the zero-shot image\nclassification task (average accuracy=75.75\\%) and semantic localization task\n(Rmi=0.7653), LRSCLIP achieves state-of-the-art performance. These results\nvalidate the dual advantages of fine-grained semantic understanding and global\nfeature matching in LRSCLIP. This work provides a new benchmark model and data\nsupport for remote sensing multimodal learning. The related code has been open\nsource and is available at https://github.com/MitsuiChen14/LRSCLIP.",
      "tldr_zh": "本研究针对遥感视觉语言基础模型（VLFM）在处理长文本时的瓶颈和“hallucination”问题，提出了一种新型模型LRSCLIP，并构建了包含2百万图像-文本对的多模态数据集LRS2M，以解决现有数据集的语义粒度限制。LRSCLIP基于Long-CLIP的KPS模块，扩展了CLIP的文本处理能力，通过双文本损失加权机制实现细粒度跨模态特征对齐。实验结果显示，在零样本长文本跨模态检索任务中，LRSCLIP比Long-CLIP基准提升10%-20%的准确率；在短文本检索、图像分类和语义定位任务中，也超过了GeoRSCLIP等模型，达到最先进性能，从而为遥感多模态学习提供新基准和数据支持。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.19311v1",
      "published_date": "2025-03-25 03:17:42 UTC",
      "updated_date": "2025-03-25 03:17:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:11:03.273006"
    },
    {
      "arxiv_id": "2503.19302v1",
      "title": "Observation Adaptation via Annealed Importance Resampling for Partially Observable Markov Decision Processes",
      "title_zh": "通过退火重要性重采样",
      "authors": [
        "Yunuo Zhang",
        "Baiting Luo",
        "Ayan Mukhopadhyay",
        "Abhishek Dubey"
      ],
      "abstract": "Partially observable Markov decision processes (POMDPs) are a general\nmathematical model for sequential decision-making in stochastic environments\nunder state uncertainty. POMDPs are often solved \\textit{online}, which enables\nthe algorithm to adapt to new information in real time. Online solvers\ntypically use bootstrap particle filters based on importance resampling for\nupdating the belief distribution. Since directly sampling from the ideal state\ndistribution given the latest observation and previous state is infeasible,\nparticle filters approximate the posterior belief distribution by propagating\nstates and adjusting weights through prediction and resampling steps. However,\nin practice, the importance resampling technique often leads to particle\ndegeneracy and sample impoverishment when the state transition model poorly\naligns with the posterior belief distribution, especially when the received\nobservation is highly informative. We propose an approach that constructs a\nsequence of bridge distributions between the state-transition and optimal\ndistributions through iterative Monte Carlo steps, better accommodating noisy\nobservations in online POMDP solvers. Our algorithm demonstrates significantly\nsuperior performance compared to state-of-the-art methods when evaluated across\nmultiple challenging POMDP domains.",
      "tldr_zh": "本研究针对部分可观测Markov决策过程（POMDPs）的在线求解问题，提出了一种基于退火重要性重采样（Annealed Importance Resampling）的观察适应方法，以解决传统引导粒子滤波（bootstrap particle filters）中的粒子退化（particle degeneracy）和样本贫乏（sample impoverishment）问题。该方法通过迭代Monte Carlo步骤构建一系列桥分布（bridge distributions），在状态转移分布和最优分布之间过渡，从而更好地处理噪声观察和高度信息量观察。实验结果表明，该算法在多个挑战性POMDP领域显著优于现有方法，提升了决策性能。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted as Oral Presentation to ICAPS 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.19302v1",
      "published_date": "2025-03-25 03:05:00 UTC",
      "updated_date": "2025-03-25 03:05:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:11:14.922532"
    },
    {
      "arxiv_id": "2503.19292v1",
      "title": "Adaptive Wavelet Filters as Practical Texture Feature Amplifiers for Parkinson's Disease Screening in OCT",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoqing Zhang",
        "Hanfeng Shi",
        "Xiangyu Li",
        "Haili Ye",
        "Tao Xu",
        "Na Li",
        "Yan Hu",
        "Fan Lv",
        "Jiangfan Chen",
        "Jiang Liu"
      ],
      "abstract": "Parkinson's disease (PD) is a prevalent neurodegenerative disorder globally.\nThe eye's retina is an extension of the brain and has great potential in PD\nscreening. Recent studies have suggested that texture features extracted from\nretinal layers can be adopted as biomarkers for PD diagnosis under optical\ncoherence tomography (OCT) images. Frequency domain learning techniques can\nenhance the feature representations of deep neural networks (DNNs) by\ndecomposing frequency components involving rich texture features. Additionally,\nprevious works have not exploited texture features for automated PD screening\nin OCT. Motivated by the above analysis, we propose a novel Adaptive Wavelet\nFilter (AWF) that serves as the Practical Texture Feature Amplifier to fully\nleverage the merits of texture features to boost the PD screening performance\nof DNNs with the aid of frequency domain learning. Specifically, AWF first\nenhances texture feature representation diversities via channel mixer, then\nemphasizes informative texture feature representations with the well-designed\nadaptive wavelet filtering token mixer. By combining the AWFs with the DNN\nstem, AWFNet is constructed for automated PD screening. Additionally, we\nintroduce a novel Balanced Confidence (BC) Loss by mining the potential of\nsample-wise predicted probabilities of all classes and class frequency prior,\nto further boost the PD screening performance and trustworthiness of AWFNet.\nThe extensive experiments manifest the superiority of our AWFNet and BC over\nstate-of-the-art methods in terms of PD screening performance and\ntrustworthiness.",
      "tldr_zh": "该研究针对帕金森病 (PD) 筛查，利用光学相干断层扫描 (OCT) 图像中的视网膜纹理特征作为生物标志物，提出了一种新型 Adaptive Wavelet Filter (AWF) 作为 Practical Texture Feature Amplifier，通过频率域学习增强深度神经网络 (DNNs) 的纹理特征表示。具体而言，AWF 通过通道混合器增加特征多样性，并使用自适应小波过滤令牌混合器强调关键纹理信息，从而构建 AWFNet 用于自动化 PD 筛查。研究还引入 Balanced Confidence (BC) Loss，通过挖掘样本级预测概率和类别频率先验，进一步提升模型的性能和可信度。实验结果显示，AWFNet 和 BC 在 PD 筛查性能和可信度方面优于现有方法。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19292v1",
      "published_date": "2025-03-25 02:47:24 UTC",
      "updated_date": "2025-03-25 02:47:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:11:27.437215"
    },
    {
      "arxiv_id": "2503.19285v2",
      "title": "No Black Box Anymore: Demystifying Clinical Predictive Modeling with Temporal-Feature Cross Attention Mechanism",
      "title_zh": "翻译失败",
      "authors": [
        "Yubo Li",
        "Xinyu Yao",
        "Rema Padman"
      ],
      "abstract": "Despite the outstanding performance of deep learning models in clinical\nprediction tasks, explainability remains a significant challenge. Inspired by\ntransformer architectures, we introduce the Temporal-Feature Cross Attention\nMechanism (TFCAM), a novel deep learning framework designed to capture dynamic\ninteractions among clinical features across time, enhancing both predictive\naccuracy and interpretability. In an experiment with 1,422 patients with\nChronic Kidney Disease, predicting progression to End-Stage Renal Disease,\nTFCAM outperformed LSTM and RETAIN baselines, achieving an AUROC of 0.95 and an\nF1-score of 0.69. Beyond performance gains, TFCAM provides multi-level\nexplainability by identifying critical temporal periods, ranking feature\nimportance, and quantifying how features influence each other across time\nbefore affecting predictions. Our approach addresses the \"black box\"\nlimitations of deep learning in healthcare, offering clinicians transparent\ninsights into disease progression mechanisms while maintaining state-of-the-art\npredictive performance.",
      "tldr_zh": "本研究提出 Temporal-Feature Cross Attention Mechanism (TFCAM)，一个受 Transformer 架构启发的框架，用于捕捉临床特征在时间上的动态交互，从而提升临床预测任务的准确性和可解释性。实验在1422名慢性肾病患者数据上进行，TFCAM 优于 LSTM 和 RETAIN 基准模型，实现了 AUROC 0.95 和 F1-score 0.69 的性能。TFCAM 提供多级解释性，包括识别关键时间段、排名特征重要性和量化特征间影响，解决了深度学习在医疗领域的“黑箱”问题，并为临床决策提供透明洞见。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 3 figures, submitted to AMIA 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.19285v2",
      "published_date": "2025-03-25 02:35:08 UTC",
      "updated_date": "2025-03-26 22:09:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:11:38.890075"
    },
    {
      "arxiv_id": "2503.19281v1",
      "title": "CubeRobot: Grounding Language in Rubik's Cube Manipulation via Vision-Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Feiyang Wang",
        "Xiaomin Yu",
        "Wangyu Wu"
      ],
      "abstract": "Proving Rubik's Cube theorems at the high level represents a notable\nmilestone in human-level spatial imagination and logic thinking and reasoning.\nTraditional Rubik's Cube robots, relying on complex vision systems and fixed\nalgorithms, often struggle to adapt to complex and dynamic scenarios. To\novercome this limitation, we introduce CubeRobot, a novel vision-language model\n(VLM) tailored for solving 3x3 Rubik's Cubes, empowering embodied agents with\nmultimodal understanding and execution capabilities. We used the CubeCoT image\ndataset, which contains multiple-level tasks (43 subtasks in total) that humans\nare unable to handle, encompassing various cube states. We incorporate a\ndual-loop VisionCoT architecture and Memory Stream, a paradigm for extracting\ntask-related features from VLM-generated planning queries, thus enabling\nCubeRobot to independent planning, decision-making, reflection and separate\nmanagement of high- and low-level Rubik's Cube tasks. Furthermore, in low-level\nRubik's Cube restoration tasks, CubeRobot achieved a high accuracy rate of\n100%, similar to 100% in medium-level tasks, and achieved an accuracy rate of\n80% in high-level tasks.",
      "tldr_zh": "这篇论文介绍了 CubeRobot，一种基于 Vision-Language Model (VLM) 的系统，旨在通过多模态理解和执行能力提升 Rubik's Cube 操作的适应性，解决传统机器人依赖固定算法的局限性。研究利用 CubeCoT 图像数据集（包含 43 个子任务）和双循环 VisionCoT 架构及 Memory Stream 机制，实现独立规划、决策、反思，并有效管理高低级任务。在实验中，CubeRobot 在低级和中级 Rubik's Cube 恢复任务中达到 100% 准确率，在高级任务中达到 80%。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19281v1",
      "published_date": "2025-03-25 02:23:47 UTC",
      "updated_date": "2025-03-25 02:23:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:11:50.509718"
    },
    {
      "arxiv_id": "2503.19280v1",
      "title": "LogicLearner: A Tool for the Guided Practice of Propositional Logic Proofs",
      "title_zh": "翻译失败",
      "authors": [
        "Amogh Inamdar",
        "Uzay Macar",
        "Michel Vazirani",
        "Michael Tarnow",
        "Zarina Mustapha",
        "Natalia Dittren",
        "Sam Sadeh",
        "Nakul Verma",
        "Ansaf Salleb-Aouissi"
      ],
      "abstract": "The study of propositional logic -- fundamental to the theory of computing --\nis a cornerstone of the undergraduate computer science curriculum. Learning to\nsolve logical proofs requires repeated guided practice, but undergraduate\nstudents often lack access to on-demand tutoring in a judgment-free\nenvironment. In this work, we highlight the need for guided practice tools in\nundergraduate mathematics education and outline the desiderata of an effective\npractice tool. We accordingly develop LogicLearner, a web application for\nguided logic proof practice. LogicLearner consists of an interface to attempt\nlogic proofs step-by-step and an automated proof solver to generate solutions\non the fly, allowing users to request guidance as needed. We pilot LogicLearner\nas a practice tool in two semesters of an undergraduate discrete mathematics\ncourse and receive strongly positive feedback for usability and pedagogical\nvalue in student surveys. To the best of our knowledge, LogicLearner is the\nonly learning tool that provides an end-to-end practice environment for logic\nproofs with immediate, judgment-free feedback.",
      "tldr_zh": "这篇论文针对本科计算机科学课程中命题逻辑证明的指导实践需求，开发了 LogicLearner 工具，以解决学生缺乏即时、无判断辅导的问题。LogicLearner 是一个网络应用，包含一个逐步尝试证明的界面和一个自动证明求解器，能实时生成解决方案并提供所需指导。在两学期的本科离散数学课程中进行试点测试后，学生反馈显示该工具在可用性和教育价值方面获得高度认可。该工具是首个提供端到端逻辑证明实践环境的解决方案，确保即时反馈和无判断学习体验。",
      "categories": [
        "cs.DM",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.DM",
      "comment": "32 pages, 27 figures, open-source codebase linked in paper",
      "pdf_url": "http://arxiv.org/pdf/2503.19280v1",
      "published_date": "2025-03-25 02:23:08 UTC",
      "updated_date": "2025-03-25 02:23:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:12:02.593340"
    },
    {
      "arxiv_id": "2503.19276v1",
      "title": "Context-Aware Semantic Segmentation: Enhancing Pixel-Level Understanding with Large Language Models for Advanced Vision Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Ben Rahman"
      ],
      "abstract": "Semantic segmentation has made significant strides in pixel-level image\nunderstanding, yet it remains limited in capturing contextual and semantic\nrelationships between objects. Current models, such as CNN and\nTransformer-based architectures, excel at identifying pixel-level features but\nfail to distinguish semantically similar objects (e.g., \"doctor\" vs. \"nurse\" in\na hospital scene) or understand complex contextual scenarios (e.g.,\ndifferentiating a running child from a regular pedestrian in autonomous\ndriving). To address these limitations, we proposed a novel Context-Aware\nSemantic Segmentation framework that integrates Large Language Models (LLMs)\nwith state-of-the-art vision backbones. Our hybrid model leverages the Swin\nTransformer for robust visual feature extraction and GPT-4 for enriching\nsemantic understanding through text embeddings. A Cross-Attention Mechanism is\nintroduced to align vision and language features, enabling the model to reason\nabout context more effectively. Additionally, Graph Neural Networks (GNNs) are\nemployed to model object relationships within the scene, capturing dependencies\nthat are overlooked by traditional models. Experimental results on benchmark\ndatasets (e.g., COCO, Cityscapes) demonstrate that our approach outperforms the\nexisting methods in both pixel-level accuracy (mIoU) and contextual\nunderstanding (mAP). This work bridges the gap between vision and language,\npaving the path for more intelligent and context-aware vision systems in\napplications including autonomous driving, medical imaging, and robotics.",
      "tldr_zh": "本研究针对语义分割在捕捉对象间语义关系和上下文方面的局限性（如区分 \"doctor\" vs. \"nurse\" 或复杂场景），提出了一种Context-Aware Semantic Segmentation框架，将Large Language Models (LLMs)与先进的视觉骨干网络整合。框架利用Swin Transformer提取视觉特征、GPT-4提供文本嵌入、Cross-Attention Mechanism对齐视觉和语言特征，以及Graph Neural Networks (GNNs)建模对象关系，从而提升语义理解能力。在COCO和Cityscapes等基准数据集上，该方法在像素级准确率(mIoU)和上下文理解(mAP)上超过了现有模型，为自主驾驶、医疗成像和机器人等应用提供了更智能的视觉系统。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19276v1",
      "published_date": "2025-03-25 02:12:35 UTC",
      "updated_date": "2025-03-25 02:12:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:12:14.982709"
    },
    {
      "arxiv_id": "2503.19937v1",
      "title": "Reverse Prompt: Cracking the Recipe Inside Text-to-Image Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiyao Ren",
        "Yibing Zhan",
        "Baosheng Yu",
        "Dacheng Tao"
      ],
      "abstract": "Text-to-image generation has become increasingly popular, but achieving the\ndesired images often requires extensive prompt engineering. In this paper, we\nexplore how to decode textual prompts from reference images, a process we refer\nto as image reverse prompt engineering. This technique enables us to gain\ninsights from reference images, understand the creative processes of great\nartists, and generate impressive new images. To address this challenge, we\npropose a method known as automatic reverse prompt optimization (ARPO).\nSpecifically, our method refines an initial prompt into a high-quality prompt\nthrough an iteratively imitative gradient prompt optimization process: 1)\ngenerating a recreated image from the current prompt to instantiate its\nguidance capability; 2) producing textual gradients, which are candidate\nprompts intended to reduce the difference between the recreated image and the\nreference image; 3) updating the current prompt with textual gradients using a\ngreedy search method to maximize the CLIP similarity between prompt and\nreference image. We compare ARPO with several baseline methods, including\nhandcrafted techniques, gradient-based prompt tuning methods, image captioning,\nand data-driven selection method. Both quantitative and qualitative results\ndemonstrate that our ARPO converges quickly to generate high-quality reverse\nprompts. More importantly, we can easily create novel images with diverse\nstyles and content by directly editing these reverse prompts. Code will be made\npublicly available.",
      "tldr_zh": "这篇论文探讨了Text-to-Image Generation中的图像反向提示工程（image reverse prompt engineering），旨在从参考图像中解码文本提示，以理解艺术家创作过程并生成新图像。作者提出了Automatic Reverse Prompt Optimization (ARPO)方法，通过迭代过程——包括从当前提示生成重现图像、产生文本梯度以减少图像差异，以及使用贪婪搜索更新提示以最大化CLIP相似度——来优化提示质量。与基线方法（如手工技巧、梯度提示调整和图像描述）相比，实验结果显示ARPO能快速收敛，产生高质量反向提示，并易于编辑以创建多样风格的新图像。代码将公开可用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19937v1",
      "published_date": "2025-03-25 02:08:05 UTC",
      "updated_date": "2025-03-25 02:08:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:12:26.992291"
    },
    {
      "arxiv_id": "2503.19267v1",
      "title": "NeoRL-2: Near Real-World Benchmarks for Offline Reinforcement Learning with Extended Realistic Scenarios",
      "title_zh": "NeoRL-2：接近真实世界的离线强化学习基准，包含扩展的真实场景",
      "authors": [
        "Songyi Gao",
        "Zuolin Tu",
        "Rong-Jun Qin",
        "Yi-Hao Sun",
        "Xiong-Hui Chen",
        "Yang Yu"
      ],
      "abstract": "Offline reinforcement learning (RL) aims to learn from historical data\nwithout requiring (costly) access to the environment. To facilitate offline RL\nresearch, we previously introduced NeoRL, which highlighted that datasets from\nreal-world tasks are often conservative and limited. With years of experience\napplying offline RL to various domains, we have identified additional\nreal-world challenges. These include extremely conservative data distributions\nproduced by deployed control systems, delayed action effects caused by\nhigh-latency transitions, external factors arising from the uncontrollable\nvariance of transitions, and global safety constraints that are difficult to\nevaluate during the decision-making process. These challenges are\nunderrepresented in previous benchmarks but frequently occur in real-world\ntasks. To address this, we constructed the extended Near Real-World Offline RL\nBenchmark (NeoRL-2), which consists of 7 datasets from 7 simulated tasks along\nwith their corresponding evaluation simulators. Benchmarking results from\nstate-of-the-art offline RL approaches demonstrate that current methods often\nstruggle to outperform the data-collection behavior policy, highlighting the\nneed for more effective methods. We hope NeoRL-2 will accelerate the\ndevelopment of reinforcement learning algorithms for real-world applications.\nThe benchmark project page is available at https://github.com/polixir/NeoRL2.",
      "tldr_zh": "本论文引入了 NeoRL-2 基准，用于评估离线强化学习(Offline RL)，扩展了真实场景的挑战，如极端保守的数据分布、延迟行动效果、外部因素和全局安全约束。NeoRL-2 包括7个模拟任务的数据集及其评估模拟器，旨在弥补现有基准的不足。实验结果显示，当前最先进的离线 RL 方法通常无法超越数据收集的行为策略，强调了需要更有效的算法。该基准有望加速强化学习在真实世界应用的开发。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.19267v1",
      "published_date": "2025-03-25 02:01:54 UTC",
      "updated_date": "2025-03-25 02:01:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:12:38.825151"
    },
    {
      "arxiv_id": "2503.19260v1",
      "title": "Linguistic Blind Spots of Large Language Models",
      "title_zh": "大语言模型的语言学盲点",
      "authors": [
        "Jiali Cheng",
        "Hadi Amiri"
      ],
      "abstract": "Large language models (LLMs) are the foundation of many AI applications\ntoday. However, despite their remarkable proficiency in generating coherent\ntext, questions linger regarding their ability to perform fine-grained\nlinguistic annotation tasks, such as detecting nouns or verbs, or identifying\nmore complex syntactic structures like clauses in input texts. These tasks\nrequire precise syntactic and semantic understanding of input text, and when\nLLMs underperform on specific linguistic structures, it raises concerns about\ntheir reliability for detailed linguistic analysis and whether their (even\ncorrect) outputs truly reflect an understanding of the inputs. In this paper,\nwe empirically study the performance of recent LLMs on fine-grained linguistic\nannotation tasks. Through a series of experiments, we find that recent LLMs\nshow limited efficacy in addressing linguistic queries and often struggle with\nlinguistically complex inputs. We show that the most capable LLM (Llama3-70b)\nmakes notable errors in detecting linguistic structures, such as misidentifying\nembedded clauses, failing to recognize verb phrases, and confusing complex\nnominals with clauses. Our results provide insights to inform future\nadvancements in LLM design and development.",
      "tldr_zh": "本研究探讨了大型语言模型（Large Language Models, LLMs）在细粒度语言标注任务上的局限性，例如检测名词或动词，以及识别复杂句法结构如从句（clauses）。通过一系列实验，作者评估了最近LLMs的性能，发现这些模型在处理语言查询时效果有限，尤其在复杂输入上表现不佳，例如Llama3-70b模型常误识别嵌入从句（embedded clauses）、无法正确识别动词短语（verb phrases），并混淆复杂名词短语和从句。研究结果为未来LLMs的设计和开发提供了宝贵见解，以提升其在语言分析的可靠性和准确性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "NAACL 2025 Cognitive Modeling and Computational Linguistics Workshop",
      "pdf_url": "http://arxiv.org/pdf/2503.19260v1",
      "published_date": "2025-03-25 01:47:13 UTC",
      "updated_date": "2025-03-25 01:47:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:12:50.634204"
    },
    {
      "arxiv_id": "2503.19223v1",
      "title": "Face Spoofing Detection using Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Najeebullah",
        "Maaz Salman",
        "Zar Nawab Khan Swati"
      ],
      "abstract": "Digital image spoofing has emerged as a significant security threat in\nbiometric authentication systems, particularly those relying on facial\nrecognition. This study evaluates the performance of three vision based models,\nMobileNetV2, ResNET50, and Vision Transformer, ViT, for spoof detection in\nimage classification, utilizing a dataset of 150,986 images divided into\ntraining , 140,002, testing, 10,984, and validation ,39,574, sets. Spoof\ndetection is critical for enhancing the security of image recognition systems,\nand this research compares the models effectiveness through accuracy,\nprecision, recall, and F1 score metrics. Results reveal that MobileNetV2\noutperforms other architectures on the test dataset, achieving an accuracy of\n91.59%, precision of 91.72%, recall of 91.59%, and F1 score of 91.58%, compared\nto ViT 86.54%, 88.28%, 86.54%, and 86.39%, respectively. On the validation\ndataset, MobileNetV2, and ViT excel, with MobileNetV2 slightly ahead at 97.17%\naccuracy versus ViT 96.36%. MobileNetV2 demonstrates faster convergence during\ntraining and superior generalization to unseen data, despite both models\nshowing signs of overfitting. These findings highlight MobileNetV2 balanced\nperformance and robustness, making it the preferred choice for spoof detection\napplications where reliability on new data is essential. The study underscores\nthe importance of model selection in security sensitive contexts and suggests\nMobileNetV2 as a practical solution for real world deployment.",
      "tldr_zh": "这篇论文评估了MobileNetV2、ResNET50 和 Vision Transformer (ViT) 三种深度学习模型在面部欺骗检测中的性能，使用一个包含150,986张图像的数据集（分为训练集140,002张、测试集10,984张和验证集39,574张）。研究通过准确率、精确率、召回率和F1 score等指标比较模型表现，结果显示MobileNetV2在测试集上表现出色，达到91.59%的准确率、91.72%的精确率、91.59%的召回率和91.58%的F1 score，优于ViT和ResNET50。MobileNetV2还展现出更快训练收敛和更好的泛化能力，尽管所有模型存在轻微过拟合问题。该研究强调模型选择在安全敏感应用中的重要性，并推荐MobileNetV2作为面部欺骗检测的实用解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "26 pages, 9 figures,3 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.19223v1",
      "published_date": "2025-03-25 00:09:21 UTC",
      "updated_date": "2025-03-25 00:09:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T06:13:03.411033"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 107,
  "processed_papers_count": 107,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-24T06:13:25.924874"
}
{
  "date": "2024-03-01",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-03-01 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 领域的创新应用，包括大型语言模型（LLM）的多模态扩展、强化学习在复杂决策中的优化，以及医疗和图像处理的 AI 模型改进。其中，Peacock 论文引入了 Arabic 多模态 LLM 和文化基准，令人印象深刻；EfficientZero V2 展示了强化学习在有限数据下的高效性能；有名学者如 Tamer Başar 的 PDE 控制优化论文也值得关注。这些工作突显了 AI 在跨领域（如语言、文化和医疗）的潜力。\n\n以下是今日论文的精选摘要，我将优先讨论重要和高话题度的论文（如 LLM 和强化学习相关），并将相关主题归类讨论。其他次要论文（如一些特定领域的小改进）将快速掠过，只列出标题和核心贡献。\n\n### LLM 和多模态模型（高话题度）\n- **Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks**（中文：Peacock: 一种 Arabic 多模态大型语言模型家族及其基准）  \n  这篇论文由 Muhammad Abdul-Mageed 等作者发布，引入了 Peacock 系列模型，支持视觉和语言任务，针对 Arabic 语言填补了多模态资源的空白。主要贡献：通过定性和定量分析，展示了模型在视觉推理和方言处理上的优越性能，并提出了 Henna 基准来评估文化相关任务。该工作推动了非英语 LLM 的发展，具有文化多样性影响。\n\n- **Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries**（中文：属性结构化提升基于 LLM 的临床文本摘要评估）  \n  作者包括 Jianfeng Gao 和 Hoifung Poon。该论文提出 Attribute Structuring 框架，用于 LLM 在临床摘要评估中的 grounded 过程。主要发现：通过分解评估任务，提高了人类标注与自动指标的对应性，并提供可审计的文本片段，适用于资源有限的医疗场景。\n\n- **Dialect prejudice predicts AI decisions about people's character, employability, and criminality**（中文：方言偏见预测 AI 对人的性格、可雇佣性和犯罪性的决策）  \n  作者 Valentin Hofmann 等探讨了 LLM 中的方言偏见（如对 African American English 的负面刻板印象）。主要贡献：揭示 LLM 隐含的隐形种族偏见，可能导致不公平决策（如就业或刑罚建议），并证明现有偏见缓解方法（如人类反馈训练）可能加剧问题。\n\n- **Merging Text Transformer Models from Different Initializations**（中文：合并不同初始化下的文本 Transformer 模型）  \n  快速掠过：作者 Neha Verma 和 Maha Elbayad 提出了一种合并 Transformer 模型的方法，降低了损失障碍。主要发现：通过排列和合并，模型在语言任务中表现出更平滑的损失景观，提升了性能。\n\n其他 LLM 相关论文（如 Leveraging Prompt-Based Large Language Models）则聚焦于社交媒体健康预测，但细节较常规，快速掠过：它们使用提示增强 LLM 来分析疫情讨论，提取 gist 模式与健康趋势关联。\n\n### 强化学习和决策优化（重要创新）\n- **EfficientZero V2: Mastering Discrete and Continuous Control with Limited Data**（中文：EfficientZero V2: 使用有限数据掌握离散和连续控制）  \n  作者 Shengjie Wang 等扩展了 EfficientZero 框架，支持视觉和低维输入的控制任务。主要贡献：在 Atari 和其他基准上，模型在有限数据下超越了 SOTA，如 DreamerV3，提供更高效的强化学习策略。\n\n- **Policy Optimization for PDE Control with a Warm Start**（中文：带预热启动的 PDE 控制策略优化）  \n  作者包括有名学者 Tamer Başar。该论文提出“reduce-then-design-then-adapt”策略，用于非线性偏微分方程控制。主要发现：通过策略优化补偿建模误差，提升了控制器性能，适用于混沌系统。\n\n- **Scale-free Adversarial Reinforcement Learning**（中文：无尺度对抗强化学习）  \n  快速掠过：作者 Mingyu Chen 等设计了 SCB 框架，实现无尺度奖励学习的 regret 上界。主要贡献：首次在对抗 MDP 中提供高效样本复杂度保证。\n\n其他强化学习论文（如 On the Role of Information Structure）讨论了决策问题的信息结构，但较为理论化，快速掠过。\n\n### 医疗和图像处理 AI（实际应用潜力）\n- **AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge Graph Construction**（中文：AutoRD: 基于本体增强的 LLM 构建罕见疾病知识图的自动系统）  \n  作者 Jimeng Sun 等提出 AutoRD 系统，使用 LLM 和本体知识提取罕见疾病信息。主要发现：在缺乏专业知识的 LLM 上，系统显著提升了信息提取性能，支持医疗诊断。\n\n- **Binary Gaussian Copula Synthesis: A Novel Data Augmentation Technique for Early Prediction of Dialysis Among CKD Patients**（中文：二元高斯 Copula 合成: 用于 CKD 患者透析早期预测的增强技术）  \n  快速掠过：作者 Hamed Khosravi 等开发了 BGCS 方法，提升了机器学习模型的预测准确性（如 72% 改善）。主要贡献：解决了数据不平衡问题。\n\n其他医疗论文（如 VisRec 和 Equipment Health Assessment）聚焦图像分割和设备健康评估，但较为具体，快速掠过：它们使用半监督学习和时间序列方法改进性能。\n\n### 其他领域（快速概述）\n- **FlaKat: A Machine Learning-Based Categorization Framework for Flaky Tests**（中文：FlaKat: 基于机器学习的 Flaky 测试分类框架）  \n  快速掠过：作者 Shizhe Lin 等提出 FDC 指标，支持 Flaky 测试分类。主要发现：提升了测试可靠性。\n\n- **SEGAA: A Unified Approach to Predicting Age, Gender, and Emotion in Speech**（中文：SEGAA: 语音中年龄、性别和情感预测的统一方法）  \n  快速掠过：作者 Aron R 等的多输出模型整合了语音分析任务。主要贡献：提高了多任务预测效率。\n\n今日论文总数达 81 篇，但以上仅覆盖了核心亮点。总体上，AI 模型的鲁棒性和泛化能力（如对抗训练、数据增强）是 recurring 主题，而 LLM 在文化和医疗中的应用显示了其广阔潜力。未来几天，关注这些领域的进展！",
  "papers": [
    {
      "arxiv_id": "2403.01031v2",
      "title": "Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks",
      "title_zh": "翻译失败",
      "authors": [
        "Fakhraddin Alwajih",
        "El Moatez Billah Nagoudi",
        "Gagan Bhatia",
        "Abdelrahman Mohamed",
        "Muhammad Abdul-Mageed"
      ],
      "abstract": "Multimodal large language models (MLLMs) have proven effective in a wide\nrange of tasks requiring complex reasoning and linguistic comprehension.\nHowever, due to a lack of high-quality multimodal resources in languages other\nthan English, success of MLLMs remains relatively limited to English-based\nsettings. This poses significant challenges in developing comparable models for\nother languages, including even those with large speaker populations such as\nArabic. To alleviate this challenge, we introduce a comprehensive family of\nArabic MLLMs, dubbed \\textit{Peacock}, with strong vision and language\ncapabilities. Through comprehensive qualitative and quantitative analysis, we\ndemonstrate the solid performance of our models on various visual reasoning\ntasks and further show their emerging dialectal potential. Additionally, we\nintroduce ~\\textit{Henna}, a new benchmark specifically designed for assessing\nMLLMs on aspects related to Arabic culture, setting the first stone for\nculturally-aware Arabic MLLMs.The GitHub repository for the \\textit{Peacock}\nproject is available at \\url{https://github.com/UBC-NLP/peacock}.",
      "tldr_zh": "本研究引入了 Peacock，这是一个阿拉伯语的多模态大型语言模型 (MLLMs) 家族，旨在解决非英语语言资源不足的问题，提升视觉和语言处理能力。通过定性和定量分析，Peacock 模型在各种视觉推理任务上表现出色，并展示了其对阿拉伯方言的潜在适应性。此外，研究还提出了 Henna，这是一个新基准，用于评估 MLLMs 在阿拉伯文化相关方面的性能，从而为文化感知的阿拉伯 MLLMs 奠定基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.01031v2",
      "published_date": "2024-03-01 23:38:02 UTC",
      "updated_date": "2024-05-24 20:24:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:33:19.908309"
    },
    {
      "arxiv_id": "2403.01024v1",
      "title": "Reservoir Computing Using Measurement-Controlled Quantum Dynamics",
      "title_zh": "基于测量控制量子动力学的 Reservoir Computing",
      "authors": [
        "A. H. Abbas",
        "Ivan S. Maksymov"
      ],
      "abstract": "Physical reservoir computing (RC) is a machine learning algorithm that\nemploys the dynamics of a physical system to forecast highly nonlinear and\nchaotic phenomena. In this paper, we introduce a quantum RC system that employs\nthe dynamics of a probed atom in a cavity. The atom experiences coherent\ndriving at a particular rate, leading to a measurement-controlled quantum\nevolution. The proposed quantum reservoir can make fast and reliable forecasts\nusing a small number of artificial neurons compared with the traditional RC\nalgorithm. We theoretically validate the operation of the reservoir,\ndemonstrating its potential to be used in error-tolerant applications, where\napproximate computing approaches may be used to make feasible forecasts in\nconditions of limited computational and energy resources.",
      "tldr_zh": "本论文提出了一种基于测量控制量子动态的 Reservoir Computing (RC) 系统，利用腔中原子的相干驱动实现测量控制的量子演化，以预测高度非线性或混沌现象。相比传统 RC，该系统使用更少的神经元即可实现快速可靠的预测，并通过理论验证证明其有效性。研究强调，该量子 RC 在错误容忍应用中具有潜力，能在计算和能源资源有限的条件下进行可行的近似计算。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "quant-ph"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.01024v1",
      "published_date": "2024-03-01 22:59:41 UTC",
      "updated_date": "2024-03-01 22:59:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:33:32.742579"
    },
    {
      "arxiv_id": "2403.01005v1",
      "title": "Policy Optimization for PDE Control with a Warm Start",
      "title_zh": "带有热启动的 PDE 控制策略优化",
      "authors": [
        "Xiangyuan Zhang",
        "Saviz Mowlavi",
        "Mouhacine Benosman",
        "Tamer Başar"
      ],
      "abstract": "Dimensionality reduction is crucial for controlling nonlinear partial\ndifferential equations (PDE) through a \"reduce-then-design\" strategy, which\nidentifies a reduced-order model and then implements model-based control\nsolutions. However, inaccuracies in the reduced-order modeling can\nsubstantially degrade controller performance, especially in PDEs with chaotic\nbehavior. To address this issue, we augment the reduce-then-design procedure\nwith a policy optimization (PO) step. The PO step fine-tunes the model-based\ncontroller to compensate for the modeling error from dimensionality reduction.\nThis augmentation shifts the overall strategy into\nreduce-then-design-then-adapt, where the model-based controller serves as a\nwarm start for PO. Specifically, we study the state-feedback tracking control\nof PDEs that aims to align the PDE state with a specific constant target\nsubject to a linear-quadratic cost. Through extensive experiments, we show that\na few iterations of PO can significantly improve the model-based controller\nperformance. Our approach offers a cost-effective alternative to PDE control\nusing end-to-end reinforcement learning.",
      "tldr_zh": "本研究针对非线性偏微分方程 (PDE) 控制中的维度减少不准确性问题，提出了一种增强的“reduce-then-design”策略，即通过添加策略优化 (PO) 步骤形成“reduce-then-design-then-adapt”流程。PO 步骤以基于模型的控制器作为 warm start，对其进行微调，以补偿维度减少带来的建模错误，从而实现 PDE 状态反馈跟踪控制的目标，使 PDE 状态与特定常量目标一致，同时优化线性-quadratic 成本。通过广泛实验证明，少量 PO 迭代即可显著提升控制器性能，并为 PDE 控制提供一种比端到端强化学习更具成本效益的替代方案。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY",
        "math.OC"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.01005v1",
      "published_date": "2024-03-01 22:03:22 UTC",
      "updated_date": "2024-03-01 22:03:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:33:45.541794"
    },
    {
      "arxiv_id": "2403.01003v1",
      "title": "FlaKat: A Machine Learning-Based Categorization Framework for Flaky Tests",
      "title_zh": "FlaKat：一种",
      "authors": [
        "Shizhe Lin",
        "Ryan Zheng He Liu",
        "Ladan Tahvildari"
      ],
      "abstract": "Flaky tests can pass or fail non-deterministically, without alterations to a\nsoftware system. Such tests are frequently encountered by developers and hinder\nthe credibility of test suites. State-of-the-art research incorporates machine\nlearning solutions into flaky test detection and achieves reasonably good\naccuracy. Moreover, the majority of automated flaky test repair solutions are\ndesigned for specific types of flaky tests. This research work proposes a novel\ncategorization framework, called FlaKat, which uses machine-learning\nclassifiers for fast and accurate prediction of the category of a given flaky\ntest that reflects its root cause. Sampling techniques are applied to address\nthe imbalance between flaky test categories in the International Dataset of\nFlaky Test (IDoFT). A new evaluation metric, called Flakiness Detection\nCapacity (FDC), is proposed for measuring the accuracy of classifiers from the\nperspective of information theory and provides proof for its effectiveness. The\nfinal FDC results are also in agreement with F1 score regarding which\nclassifier yields the best flakiness classification.",
      "tldr_zh": "这篇论文提出了一种基于机器学习(Machine Learning)的分类框架FlaKat，用于快速准确预测Flaky Tests的类别，从而揭示其根因。框架通过采样技术(Sampling Techniques)处理国际Flaky Tests数据集(IDoFT)中的类别不平衡问题，并引入新的评价指标Flakiness Detection Capacity (FDC)，基于信息理论评估分类器的准确性。实验结果显示，FDC与F1 score一致，证明FlaKat在Flaky Tests分类方面表现出色，有助于提升软件测试的可靠性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.01003v1",
      "published_date": "2024-03-01 22:00:44 UTC",
      "updated_date": "2024-03-01 22:00:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:33:57.962869"
    },
    {
      "arxiv_id": "2403.01002v2",
      "title": "Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries",
      "title_zh": "属性结构",
      "authors": [
        "Zelalem Gero",
        "Chandan Singh",
        "Yiqing Xie",
        "Sheng Zhang",
        "Praveen Subramanian",
        "Paul Vozila",
        "Tristan Naumann",
        "Jianfeng Gao",
        "Hoifung Poon"
      ],
      "abstract": "Summarizing clinical text is crucial in health decision-support and clinical\nresearch. Large language models (LLMs) have shown the potential to generate\naccurate clinical text summaries, but still struggle with issues regarding\ngrounding and evaluation, especially in safety-critical domains such as health.\nHolistically evaluating text summaries is challenging because they may contain\nunsubstantiated information. Here, we explore a general mitigation framework\nusing Attribute Structuring (AS), which structures the summary evaluation\nprocess. It decomposes the evaluation process into a grounded procedure that\nuses an LLM for relatively simple structuring and scoring tasks, rather than\nthe full task of holistic summary evaluation. Experiments show that AS\nconsistently improves the correspondence between human annotations and\nautomated metrics in clinical text summarization. Additionally, AS yields\ninterpretations in the form of a short text span corresponding to each output,\nwhich enables efficient human auditing, paving the way towards trustworthy\nevaluation of clinical information in resource-constrained scenarios. We\nrelease our code, prompts, and an open-source benchmark at\nhttps://github.com/microsoft/attribute-structuring.",
      "tldr_zh": "该研究探讨了Attribute Structuring (AS)框架，以提升大语言模型 (LLMs) 在临床文本总结评估中的性能，解决LLMs在接地和评估方面的挑战，特别是针对可能包含未证实信息的摘要。AS通过将评估过程分解为结构化和评分任务，使用LLMs执行简单子任务，从而实现更可靠的整体评估。实验结果表明，AS显著改善了自动指标与人类标注的对应性，并生成可解释的文本片段，便于高效的人类审计。该框架为资源受限场景下的可信赖临床信息评估铺平了道路，并发布了开源代码和基准。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Published in ML4H Findings 2024, 4 pages",
      "pdf_url": "http://arxiv.org/pdf/2403.01002v2",
      "published_date": "2024-03-01 21:59:03 UTC",
      "updated_date": "2024-12-14 19:46:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:34:09.367906"
    },
    {
      "arxiv_id": "2403.00994v1",
      "title": "Leveraging Prompt-Based Large Language Models: Predicting Pandemic Health Decisions and Outcomes Through Social Media Language",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaohan Ding",
        "Buse Carik",
        "Uma Sushmitha Gunturi",
        "Valerie Reyna",
        "Eugenia H. Rho"
      ],
      "abstract": "We introduce a multi-step reasoning framework using prompt-based LLMs to\nexamine the relationship between social media language patterns and trends in\nnational health outcomes. Grounded in fuzzy-trace theory, which emphasizes the\nimportance of gists of causal coherence in effective health communication, we\nintroduce Role-Based Incremental Coaching (RBIC), a prompt-based LLM framework,\nto identify gists at-scale. Using RBIC, we systematically extract gists from\nsubreddit discussions opposing COVID-19 health measures (Study 1). We then\ntrack how these gists evolve across key events (Study 2) and assess their\ninfluence on online engagement (Study 3). Finally, we investigate how the\nvolume of gists is associated with national health trends like vaccine uptake\nand hospitalizations (Study 4). Our work is the first to empirically link\nsocial media linguistic patterns to real-world public health trends,\nhighlighting the potential of prompt-based LLMs in identifying critical online\ndiscussion patterns that can form the basis of public health communication\nstrategies.",
      "tldr_zh": "本研究提出了一种多步推理框架，利用基于提示的大型语言模型 (LLMs)，来探索社交媒体语言模式与国家健康结果趋势之间的关系，该框架基于模糊踪迹理论 (fuzzy-trace theory)，强调因果连贯性要点 (gists) 在健康传播中的作用。研究引入 Role-Based Incremental Coaching (RBIC) 框架，通过从反对 COVID-19 健康措施的 Reddit 讨论中提取 gists（研究1），并跟踪其在关键事件中的演变（研究2）、对在线参与的影响（研究3），以及与疫苗接种和住院率等国家健康趋势的关联（研究4）。这项工作首次实证地将社交媒体语言模式与真实公共健康趋势联系起来，展示了基于提示的 LLMs 在识别关键在线讨论模式并制定公共健康策略方面的潜力。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.SI"
      ],
      "primary_category": "cs.HC",
      "comment": "20 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.00994v1",
      "published_date": "2024-03-01 21:29:32 UTC",
      "updated_date": "2024-03-01 21:29:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:34:22.951672"
    },
    {
      "arxiv_id": "2403.00993v2",
      "title": "On the Role of Information Structure in Reinforcement Learning for Partially-Observable Sequential Teams and Games",
      "title_zh": "翻译失败",
      "authors": [
        "Awni Altabaa",
        "Zhuoran Yang"
      ],
      "abstract": "In a sequential decision-making problem, the information structure is the\ndescription of how events in the system occurring at different points in time\naffect each other. Classical models of reinforcement learning (e.g., MDPs,\nPOMDPs) assume a simple and highly regular information structure, while more\ngeneral models like predictive state representations do not explicitly model\nthe information structure. By contrast, real-world sequential decision-making\nproblems typically involve a complex and time-varying interdependence of system\nvariables, requiring a rich and flexible representation of information\nstructure. In this paper, we formalize a novel reinforcement learning model\nwhich explicitly represents the information structure. We then use this model\nto carry out an information-structural analysis of the statistical hardness of\ngeneral sequential decision-making problems, obtaining a characterization via a\ngraph-theoretic quantity of the DAG representation of the information\nstructure. We prove an upper bound on the sample complexity of learning a\ngeneral sequential decision-making problem in terms of its information\nstructure by exhibiting an algorithm achieving the upper bound. This recovers\nknown tractability results and gives a novel perspective on reinforcement\nlearning in general sequential decision-making problems, providing a systematic\nway of identifying new tractable classes of problems.",
      "tldr_zh": "这篇论文探讨了信息结构在 Reinforcement Learning 中的作用，特别是针对部分可观测的顺序团队和游戏，强调了传统模型如 MDPs 和 POMDPs 的简单假设无法处理真实世界中复杂的时间相关性。作者提出一个新模型，通过显式表示信息结构来分析顺序决策问题的统计难度，并使用图论方法（如 DAG 表示）进行表征。论文证明了样本复杂度的上界，并展示了一个算法实现此上界，从而恢复已知的可处理结果，并为识别新可处理问题类提供系统性视角。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "59 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.00993v2",
      "published_date": "2024-03-01 21:28:19 UTC",
      "updated_date": "2024-05-27 22:19:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:34:34.310099"
    },
    {
      "arxiv_id": "2403.00986v3",
      "title": "Merging Text Transformer Models from Different Initializations",
      "title_zh": "合并来自不同初始化的文本 Transformer 模型",
      "authors": [
        "Neha Verma",
        "Maha Elbayad"
      ],
      "abstract": "Recent work on permutation-based model merging has shown impressive low- or\nzero-barrier mode connectivity between models from completely different\ninitializations. However, this line of work has not yet extended to the\nTransformer architecture, despite its dominant popularity in the language\ndomain. Therefore, in this work, we investigate the extent to which separate\nTransformer minima learn similar features, and propose a model merging\ntechnique to investigate the relationship between these minima in the loss\nlandscape. The specifics of the architecture, like its residual connections,\nmulti-headed attention, and discrete, sequential input, require specific\ninterventions in order to compute model permutations that remain within the\nsame functional equivalence class. In merging these models with our method, we\nconsistently find lower loss barriers between minima compared to model\naveraging, across models trained on a masked-language modeling task or\nfine-tuned on a language understanding benchmark. Our results show that the\nminima of these models are less sharp and isolated than previously understood,\nand provide a basis for future work on merging separately trained Transformer\nmodels.",
      "tldr_zh": "本文提出了一种模型合并技术，用于从不同初始化合并文本Transformer模型，旨在探索这些模型在损失景观(loss landscape)中的关系。该方法通过处理Transformer特有的架构元素，如残差连接(residual connections)、多头注意力(multi-headed attention)和离散顺序输入，确保模型置换保持在相同的功能等价类中。实验结果显示，与模型平均(model averaging)相比，这种合并方式显著降低了模型最小值之间的损失障碍，无论是在掩码语言建模(masked-language modeling)任务上训练的模型还是在语言理解基准上微调的模型。这些发现表明，Transformer模型的最小值不如之前理解的那样尖锐和孤立，为未来合并独立训练的Transformer模型提供了重要基础。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "TMLR, November 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.00986v3",
      "published_date": "2024-03-01 21:16:29 UTC",
      "updated_date": "2024-12-16 18:00:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:34:46.039870"
    },
    {
      "arxiv_id": "2403.00980v2",
      "title": "Even-Ifs From If-Onlys: Are the Best Semi-Factual Explanations Found Using Counterfactuals As Guides?",
      "title_zh": "翻译失败",
      "authors": [
        "Saugat Aryal",
        "Mark T. Keane"
      ],
      "abstract": "Recently, counterfactuals using \"if-only\" explanations have become very\npopular in eXplainable AI (XAI), as they describe which changes to\nfeature-inputs of a black-box AI system result in changes to a (usually\nnegative) decision-outcome. Even more recently, semi-factuals using \"even-if\"\nexplanations have gained more attention. They elucidate the feature-input\nchanges that do not change the decision-outcome of the AI system, with a\npotential to suggest more beneficial recourses. Some semi-factual methods use\ncounterfactuals to the query-instance to guide semi-factual production\n(so-called counterfactual-guided methods), whereas others do not (so-called\ncounterfactual-free methods). In this work, we perform comprehensive tests of 8\nsemi-factual methods on 7 datasets using 5 key metrics, to determine whether\ncounterfactual guidance is necessary to find the best semi-factuals. The\nresults of these tests suggests not, but rather that computing other aspects of\nthe decision space lead to better semi-factual XAI.",
      "tldr_zh": "本论文探讨了在可解释 AI (XAI) 领域，是否需要使用反事实解释 (counterfactuals) 来指导半事实解释 (semi-factuals) 的生成，以找出最佳解释。研究者测试了 8 种半事实方法，包括 counterfactual-guided 和 counterfactual-free 方法，在 7 个数据集上使用 5 个关键指标进行全面评估。主要发现是，反事实指导并非必要，而是通过计算决策空间的其他方面，能产生更有效的 semi-factual XAI，从而为提供更有益的决策补救措施提供新见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.00980v2",
      "published_date": "2024-03-01 21:04:48 UTC",
      "updated_date": "2024-04-25 15:36:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:34:57.049008"
    },
    {
      "arxiv_id": "2403.00975v1",
      "title": "Equipment Health Assessment: Time Series Analysis for Wind Turbine Performance",
      "title_zh": "设备健康评估：时间序列分析用于风力涡轮机性能",
      "authors": [
        "Jana Backhus",
        "Aniruddha Rajendra Rao",
        "Chandrasekar Venkatraman",
        "Abhishek Padmanabhan",
        "A. Vinoth Kumar",
        "Chetan Gupta"
      ],
      "abstract": "In this study, we leverage SCADA data from diverse wind turbines to predict\npower output, employing advanced time series methods, specifically Functional\nNeural Networks (FNN) and Long Short-Term Memory (LSTM) networks. A key\ninnovation lies in the ensemble of FNN and LSTM models, capitalizing on their\ncollective learning. This ensemble approach outperforms individual models,\nensuring stable and accurate power output predictions. Additionally, machine\nlearning techniques are applied to detect wind turbine performance\ndeterioration, enabling proactive maintenance strategies and health assessment.\nCrucially, our analysis reveals the uniqueness of each wind turbine,\nnecessitating tailored models for optimal predictions. These insight\nunderscores the importance of providing automatized customization for different\nturbines to keep human modeling effort low. Importantly, the methodologies\ndeveloped in this analysis are not limited to wind turbines; they can be\nextended to predict and optimize performance in various machinery, highlighting\nthe versatility and applicability of our research across diverse industrial\ncontexts.",
      "tldr_zh": "本研究利用 SCADA 数据，通过 Functional Neural Networks (FNN) 和 Long Short-Term Memory (LSTM) 网络预测风力涡轮机的功率输出，并创新性地采用 FNN 与 LSTM 的集成模型，提升预测的稳定性和准确性。机器学习技术被应用来检测涡轮机性能恶化，支持主动维护策略和健康评估。研究发现，每个风力涡轮机的独特性要求定制模型，以减少人为建模努力，且这些方法可扩展至其他工业机械的性能预测和优化。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.FA",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "19 Pages, 17 Figures, 3 Tables, Submitted at Applied Sciences (MDPI)",
      "pdf_url": "http://arxiv.org/pdf/2403.00975v1",
      "published_date": "2024-03-01 20:54:31 UTC",
      "updated_date": "2024-03-01 20:54:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:35:09.588848"
    },
    {
      "arxiv_id": "2403.00965v1",
      "title": "Binary Gaussian Copula Synthesis: A Novel Data Augmentation Technique to Advance ML-based Clinical Decision Support Systems for Early Prediction of Dialysis Among CKD Patients",
      "title_zh": "翻译失败",
      "authors": [
        "Hamed Khosravi",
        "Srinjoy Das",
        "Abdullah Al-Mamun",
        "Imtiaz Ahmed"
      ],
      "abstract": "The Center for Disease Control estimates that over 37 million US adults\nsuffer from chronic kidney disease (CKD), yet 9 out of 10 of these individuals\nare unaware of their condition due to the absence of symptoms in the early\nstages. It has a significant impact on patients' quality of life, particularly\nwhen it progresses to the need for dialysis. Early prediction of dialysis is\ncrucial as it can significantly improve patient outcomes and assist healthcare\nproviders in making timely and informed decisions. However, developing an\neffective machine learning (ML)-based Clinical Decision Support System (CDSS)\nfor early dialysis prediction poses a key challenge due to the imbalanced\nnature of data. To address this challenge, this study evaluates various data\naugmentation techniques to understand their effectiveness on real-world\ndatasets. We propose a new approach named Binary Gaussian Copula Synthesis\n(BGCS). BGCS is tailored for binary medical datasets and excels in generating\nsynthetic minority data that mirrors the distribution of the original data.\nBGCS enhances early dialysis prediction by outperforming traditional methods in\ndetecting dialysis patients. For the best ML model, Random Forest, BCGS\nachieved a 72% improvement, surpassing the state-of-the-art augmentation\napproaches. Also, we present a ML-based CDSS, designed to aid clinicians in\nmaking informed decisions. CDSS, which utilizes decision tree models, is\ndeveloped to improve patient outcomes, identify critical variables, and thereby\nenable clinicians to make proactive decisions, and strategize treatment plans\neffectively for CKD patients who are more likely to require dialysis in the\nnear future. Through comprehensive feature analysis and meticulous data\npreparation, we ensure that the CDSS's dialysis predictions are not only\naccurate but also actionable, providing a valuable tool in the management and\ntreatment of CKD.",
      "tldr_zh": "本研究针对慢性肾病（CKD）患者的数据不平衡问题，提出了一种新数据增强技术Binary Gaussian Copula Synthesis (BGCS)，旨在改善机器学习（ML）模型在早期预测透析方面的性能。BGCS 专门设计用于二元医疗数据集，能够生成模拟原数据分布的合成少数类样本，并在随机森林模型上实现了72%的性能提升，优于现有增强方法。最终，该研究开发了一个基于决策树的ML Clinical Decision Support System (CDSS)，帮助临床医生识别关键变量、制定主动治疗计划，从而提升CKD患者的结果和决策效率。",
      "categories": [
        "stat.AP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.AP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00965v1",
      "published_date": "2024-03-01 20:32:17 UTC",
      "updated_date": "2024-03-01 20:32:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:35:21.985957"
    },
    {
      "arxiv_id": "2403.00957v2",
      "title": "Resolution of Simpson's paradox via the common cause principle",
      "title_zh": "翻译失败",
      "authors": [
        "A. Hovhannisyan",
        "A. E. Allahverdyan"
      ],
      "abstract": "Simpson's paradox is an obstacle to establishing a probabilistic association\nbetween two events $a_1$ and $a_2$, given the third (lurking) random variable\n$B$. We focus on scenarios when the random variables $A$ (which combines $a_1$,\n$a_2$, and their complements) and $B$ have a common cause $C$ that need not be\nobserved. Alternatively, we can assume that $C$ screens out $A$ from $B$. For\nsuch cases, the correct association between $a_1$ and $a_2$ is to be defined\nvia conditioning over $C$. This setup generalizes the original Simpson's\nparadox: now its two contradicting options refer to two particular and\ndifferent causes $C$. We show that if $B$ and $C$ are binary and $A$ is\nquaternary (the minimal and the most widespread situation for the Simpson's\nparadox), the conditioning over any binary common cause $C$ establishes the\nsame direction of association between $a_1$ and $a_2$ as the conditioning over\n$B$ in the original formulation of the paradox. Thus, for the minimal common\ncause, one should choose the option of Simpson's paradox that assumes\nconditioning over $B$ and not its marginalization. The same conclusion is\nreached when Simpson's paradox is formulated via 3 continuous Gaussian\nvariables: within the minimal formulation of the paradox (3 scalar continuous\nvariables $A_1$, $A_2$, and $B$), one should choose the option with the\nconditioning over $B$.",
      "tldr_zh": "该论文探讨了通过共同原因原则（common cause principle）解决Simpson's paradox的问题，该悖论发生在存在隐藏变量B时，导致事件a1和a2之间的关联在总体和子组上方向相反。作者假设随机变量A（包含a1、a2及其补集）和B共享一个共同原因C，并通过对C进行条件化（conditioning）来正确定义a1和a2的关联，从而扩展了原悖论的框架。研究发现，在最小情况下（B和C为二元、A为四元），对任何二元C进行条件化会与原公式中对B进行条件化得到相同的关联方向，因此应选择对B进行条件化的选项而非边缘化。同样，对于由三个连续高斯变量（A1、A2和B）构成的Simpson's paradox，该结论也成立，为统计关联分析提供了更可靠的指导。",
      "categories": [
        "stat.ME",
        "cs.AI",
        "math.PR",
        "physics.data-an",
        "stat.AP"
      ],
      "primary_category": "stat.ME",
      "comment": "Added new results, enhanced references",
      "pdf_url": "http://arxiv.org/pdf/2403.00957v2",
      "published_date": "2024-03-01 20:15:28 UTC",
      "updated_date": "2024-07-21 08:57:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:35:35.778954"
    },
    {
      "arxiv_id": "2403.00953v4",
      "title": "AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge Graph Construction Based on Ontologies-enhanced Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Lang Cao",
        "Jimeng Sun",
        "Adam Cross"
      ],
      "abstract": "Rare diseases affect millions worldwide but often face limited research focus\ndue to their low prevalence. This results in prolonged diagnoses and a lack of\napproved therapies. Recent advancements in Large Language Models (LLMs) have\nshown promise in automating the extraction of medical information, offering\npotential to improve medical diagnosis and management. However, most LLMs lack\nprofessional medical knowledge, especially concerning rare diseases, and\nstruggle to handle the latest rare disease information. They also cannot\neffectively manage rare disease data and are not directly suitable for\ndiagnosis and management tasks. Our objective is to create an end-to-end system\ncalled AutoRD, which automates the extraction of information from medical texts\nabout rare diseases, focusing on entities and their relations. AutoRD\nintegrates up-to-date structured knowledge and demonstrates superior\nperformance in rare disease extraction tasks. We conduct various experiments to\nevaluate AutoRD's performance, aiming to surpass common LLMs and traditional\nmethods.",
      "tldr_zh": "该研究针对稀有疾病的诊断延迟和治疗不足问题，提出了一种自动端到端系统AutoRD，利用本体增强的Large Language Models (LLMs)来构建稀有疾病知识图谱。AutoRD通过从医疗文本中自动提取实体和关系，并整合最新的结构化知识，解决了传统LLMs在专业医疗知识和数据处理方面的局限性。在各种实验中，AutoRD在稀有疾病信息提取任务上表现出色，超过了常见LLMs和传统方法，为改善医疗诊断和管理提供了潜在解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00953v4",
      "published_date": "2024-03-01 20:06:39 UTC",
      "updated_date": "2024-10-25 14:20:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:35:45.060891"
    },
    {
      "arxiv_id": "2403.00942v2",
      "title": "Resilience of Entropy Model in Distributed Neural Networks",
      "title_zh": "分布式神经网络中熵模型的弹性",
      "authors": [
        "Milin Zhang",
        "Mohammad Abdi",
        "Shahriar Rifat",
        "Francesco Restuccia"
      ],
      "abstract": "Distributed deep neural networks (DNNs) have emerged as a key technique to\nreduce communication overhead without sacrificing performance in edge computing\nsystems. Recently, entropy coding has been introduced to further reduce the\ncommunication overhead. The key idea is to train the distributed DNN jointly\nwith an entropy model, which is used as side information during inference time\nto adaptively encode latent representations into bit streams with variable\nlength. To the best of our knowledge, the resilience of entropy models is yet\nto be investigated. As such, in this paper we formulate and investigate the\nresilience of entropy models to intentional interference (e.g., adversarial\nattacks) and unintentional interference (e.g., weather changes and motion\nblur). Through an extensive experimental campaign with 3 different DNN\narchitectures, 2 entropy models and 4 rate-distortion trade-off factors, we\ndemonstrate that the entropy attacks can increase the communication overhead by\nup to 95%. By separating compression features in frequency and spatial domain,\nwe propose a new defense mechanism that can reduce the transmission overhead of\nthe attacked input by about 9% compared to unperturbed data, with only about 2%\naccuracy loss. Importantly, the proposed defense mechanism is a standalone\napproach which can be applied in conjunction with approaches such as\nadversarial training to further improve robustness. Code will be shared for\nreproducibility.",
      "tldr_zh": "本文研究了分布式神经网络(DNNs)中熵模型的鲁棒性，针对故意干扰（如对抗攻击）和无意干扰（如天气变化和运动模糊）的潜在影响。作者通过实验（涉及3种DNN架构、2种熵模型和4种率-失真权衡因素）发现，熵攻击可使通信开销增加高达95%。为应对此问题，他们提出了一种新防御机制，通过在频率和空间域分离压缩特征，将受攻击输入的传输开销减少约9%，并仅损失约2%的准确率；该机制可独立应用或与其他方法（如对抗训练）结合，以提升整体鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "accepted at ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.00942v2",
      "published_date": "2024-03-01 19:39:19 UTC",
      "updated_date": "2024-07-11 13:51:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:35:59.868835"
    },
    {
      "arxiv_id": "2403.02352v1",
      "title": "ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys",
      "title_zh": "翻译失败",
      "authors": [
        "Yue Niu",
        "Saurav Prakash",
        "Salman Avestimehr"
      ],
      "abstract": "We propose a new attention mechanism with linear complexity, ATP, that\nfixates \\textbf{A}ttention on \\textbf{T}op \\textbf{P}rincipal keys, rather than\non each individual token. Particularly, ATP is driven by an important\nobservation that input sequences are typically low-rank, i.e., input sequences\ncan be represented by a few principal bases. Therefore, instead of directly\niterating over all the input tokens, ATP transforms inputs into an orthogonal\nspace and computes attention only on the top principal bases (keys). Owing to\nthe observed low-rank structure in input sequences, ATP is able to capture\nsemantic relationships in input sequences with a few principal keys.\nFurthermore, the attention complexity is reduced from \\emph{quadratic} to\n\\emph{linear} without incurring a noticeable performance drop. ATP further\nreduces complexity for other linear layers with low-rank inputs, leading to\nmore speedup compared to prior works that solely target the attention module.\nOur evaluations on various models (e.g., BERT and Llama) demonstrate that ATP\nachieves comparable accuracy with much lower computation and memory complexity\nthan the standard attention mechanism. In particular, ATP barely loses accuracy\nwith only $1/2$ principal keys, and only incurs around $2\\%$ accuracy drops\nwith $1/4$ principal keys.",
      "tldr_zh": "这篇论文提出了 ATP，一种新的线性复杂度的注意力机制，用于加速大型语言模型（LLM）的服务，通过关注输入序列的顶层主要键（top principal keys）而非每个 token。ATP 基于输入序列的低秩特性，将输入转换为正交空间，仅在主要基上计算注意力，从而捕捉语义关系并将复杂度从二次降低到线性，同时优化其他线性层以进一步提升速度。实验结果显示，在 BERT 和 Llama 等模型上，ATP 保持了与标准注意力机制可比的准确性，仅使用 1/2 的主要键几乎无损失准确性，使用 1/4 时仅损失约 2%，并显著减少了计算和内存开销。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 7 figures, 8 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.02352v1",
      "published_date": "2024-03-01 19:24:37 UTC",
      "updated_date": "2024-03-01 19:24:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:36:11.038858"
    },
    {
      "arxiv_id": "2403.00930v1",
      "title": "Scale-free Adversarial Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Mingyu Chen",
        "Xuezhou Zhang"
      ],
      "abstract": "This paper initiates the study of scale-free learning in Markov Decision\nProcesses (MDPs), where the scale of rewards/losses is unknown to the learner.\nWe design a generic algorithmic framework, \\underline{S}cale\n\\underline{C}lipping \\underline{B}ound (\\texttt{SCB}), and instantiate this\nframework in both the adversarial Multi-armed Bandit (MAB) setting and the\nadversarial MDP setting. Through this framework, we achieve the first minimax\noptimal expected regret bound and the first high-probability regret bound in\nscale-free adversarial MABs, resolving an open problem raised in\n\\cite{hadiji2023adaptation}. On adversarial MDPs, our framework also give birth\nto the first scale-free RL algorithm with a $\\tilde{\\mathcal{O}}(\\sqrt{T})$\nhigh-probability regret guarantee.",
      "tldr_zh": "本论文探讨了马尔可夫决策过程 (MDPs) 中的无尺度学习问题，即奖励/损失尺度未知，并提出了一种通用算法框架 SCB (Scale Clipping Bound)。在对抗性多臂老虎机 (MAB) 设置中，该框架实现了第一个最小最大最优的期望遗憾界和第一个高概率遗憾界，解决了文献中的一个开放问题。在对抗性 MDPs 设置中，SCB 框架提供了第一个无尺度强化学习 (RL) 算法，具有 $\\tilde{\\mathcal{O}}(\\sqrt{T})$ 的高概率遗憾保证。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00930v1",
      "published_date": "2024-03-01 19:21:10 UTC",
      "updated_date": "2024-03-01 19:21:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:36:22.826933"
    },
    {
      "arxiv_id": "2403.00929v3",
      "title": "PRIME: Scaffolding Manipulation Tasks with Behavior Primitives for Data-Efficient Imitation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Tian Gao",
        "Soroush Nasiriany",
        "Huihan Liu",
        "Quantao Yang",
        "Yuke Zhu"
      ],
      "abstract": "Imitation learning has shown great potential for enabling robots to acquire\ncomplex manipulation behaviors. However, these algorithms suffer from high\nsample complexity in long-horizon tasks, where compounding errors accumulate\nover the task horizons. We present PRIME (PRimitive-based IMitation with data\nEfficiency), a behavior primitive-based framework designed for improving the\ndata efficiency of imitation learning. PRIME scaffolds robot tasks by\ndecomposing task demonstrations into primitive sequences, followed by learning\na high-level control policy to sequence primitives through imitation learning.\nOur experiments demonstrate that PRIME achieves a significant performance\nimprovement in multi-stage manipulation tasks, with 10-34% higher success rates\nin simulation over state-of-the-art baselines and 20-48% on physical hardware.",
      "tldr_zh": "该研究提出PRIME框架，通过行为原语(Behavior Primitives)来支撑机器人操作任务，从而提升Imitation Learning的效率。PRIME方法将任务演示分解为原语序列，并通过模仿学习训练高层控制策略，以减少长周期任务中的样本复杂性和累积错误。实验结果显示，在多阶段操作任务中，PRIME在模拟环境中比现有基线成功率提高10-34%，而在物理硬件上提升20-48%。这为数据高效的机器人学习提供了新途径。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00929v3",
      "published_date": "2024-03-01 19:19:56 UTC",
      "updated_date": "2024-08-17 07:50:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:36:33.577602"
    },
    {
      "arxiv_id": "2403.00758v3",
      "title": "Mitigating Reversal Curse in Large Language Models via Semantic-aware Permutation Training",
      "title_zh": "翻译失败",
      "authors": [
        "Qingyan Guo",
        "Rui Wang",
        "Junliang Guo",
        "Xu Tan",
        "Jiang Bian",
        "Yujiu Yang"
      ],
      "abstract": "While large language models (LLMs) have achieved impressive performance\nacross diverse tasks, recent studies showcase that causal LLMs suffer from the\n\"reversal curse\". It is a typical example that the model knows \"A's father is\nB\", but is unable to reason \"B's child is A\". This limitation poses a challenge\nto the advancement of artificial general intelligence (AGI), as it suggests a\ngap in the models' ability to comprehend and apply bidirectional reasoning. In\nthis paper, we first conduct substantial evaluation and identify that the root\ncause of the reversal curse lies in the different word order between the\ntraining and inference stage, namely, the poor ability of causal language\nmodels to predict antecedent words within the training data. Accordingly,\npermutation on the training data is considered as a potential solution, since\nthis can make the model predict antecedent words or tokens. However, previous\npermutation methods may disrupt complete phrases or entities, thereby posing\nchallenges for the model to comprehend and learn from training data. To address\nthis issue, we propose Semantic-aware Permutation Training (SPT), which\naddresses this issue by segmenting the training sentences into semantic units\n(i.e., entities or phrases) with an assistant language model and permuting\nthese units before feeding into the model. Extensive experiments demonstrate\nthat SPT effectively mitigates the reversal curse since the performance on\nreversed questions approximates that on the forward ones, and significantly\nadvances the performance of existing works.",
      "tldr_zh": "大语言模型（LLMs）存在“reversal curse”问题，即模型能理解“A's father is B”但无法推理“B's child is A”，这阻碍了人工通用智能（AGI）的进展。研究者发现这一问题的根因在于训练和推理阶段的词序差异，导致模型难以预测前置词。为此，他们提出Semantic-aware Permutation Training (SPT)方法，通过辅助语言模型将训练句子分割成语义单位（如实体或短语），然后打乱这些单位进行训练，从而提升模型的双向推理能力。实验结果显示，SPT 显著缓解了 reversal curse，使反向问题的性能接近正向问题，并优于现有方法。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00758v3",
      "published_date": "2024-03-01 18:55:20 UTC",
      "updated_date": "2024-03-20 07:37:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:36:46.139425"
    },
    {
      "arxiv_id": "2403.00742v1",
      "title": "Dialect prejudice predicts AI decisions about people's character, employability, and criminality",
      "title_zh": "方言偏见预测 AI 对人们性格、可雇佣性和犯罪性的决策",
      "authors": [
        "Valentin Hofmann",
        "Pratyusha Ria Kalluri",
        "Dan Jurafsky",
        "Sharese King"
      ],
      "abstract": "Hundreds of millions of people now interact with language models, with uses\nranging from serving as a writing aid to informing hiring decisions. Yet these\nlanguage models are known to perpetuate systematic racial prejudices, making\ntheir judgments biased in problematic ways about groups like African Americans.\nWhile prior research has focused on overt racism in language models, social\nscientists have argued that racism with a more subtle character has developed\nover time. It is unknown whether this covert racism manifests in language\nmodels. Here, we demonstrate that language models embody covert racism in the\nform of dialect prejudice: we extend research showing that Americans hold\nraciolinguistic stereotypes about speakers of African American English and find\nthat language models have the same prejudice, exhibiting covert stereotypes\nthat are more negative than any human stereotypes about African Americans ever\nexperimentally recorded, although closest to the ones from before the civil\nrights movement. By contrast, the language models' overt stereotypes about\nAfrican Americans are much more positive. We demonstrate that dialect prejudice\nhas the potential for harmful consequences by asking language models to make\nhypothetical decisions about people, based only on how they speak. Language\nmodels are more likely to suggest that speakers of African American English be\nassigned less prestigious jobs, be convicted of crimes, and be sentenced to\ndeath. Finally, we show that existing methods for alleviating racial bias in\nlanguage models such as human feedback training do not mitigate the dialect\nprejudice, but can exacerbate the discrepancy between covert and overt\nstereotypes, by teaching language models to superficially conceal the racism\nthat they maintain on a deeper level. Our findings have far-reaching\nimplications for the fair and safe employment of language technology.",
      "tldr_zh": "本研究发现，语言模型(language models)体现出方言偏见(dialect prejudice)，对非洲裔美国英语(African American English)说话者的隐蔽刻板印象(covert stereotypes)比历史上任何人类记录都更负面，尽管其公开刻板印象(overt stereotypes)相对积极。研究者通过扩展社会科学实验，测试语言模型对说话方式的偏见，发现这些模型在假设决策中更倾向于建议这些说话者从事低端工作、被判刑或判处死刑，从而可能导致有害后果。实验结果显示，现有的种族偏见缓解方法，如人类反馈训练(human feedback training)，无法有效减轻这种方言偏见，反而可能加剧隐蔽与公开刻板印象的差距。该发现强调了在语言技术应用中确保公平性和安全性的必要性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00742v1",
      "published_date": "2024-03-01 18:43:09 UTC",
      "updated_date": "2024-03-01 18:43:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:36:59.998763"
    },
    {
      "arxiv_id": "2403.00694v1",
      "title": "Defining Expertise: Applications to Treatment Effect Estimation",
      "title_zh": "定义专业知识：在治疗效果估计中的应用",
      "authors": [
        "Alihan Hüyük",
        "Qiyao Wei",
        "Alicia Curth",
        "Mihaela van der Schaar"
      ],
      "abstract": "Decision-makers are often experts of their domain and take actions based on\ntheir domain knowledge. Doctors, for instance, may prescribe treatments by\npredicting the likely outcome of each available treatment. Actions of an expert\nthus naturally encode part of their domain knowledge, and can help make\ninferences within the same domain: Knowing doctors try to prescribe the best\ntreatment for their patients, we can tell treatments prescribed more frequently\nare likely to be more effective. Yet in machine learning, the fact that most\ndecision-makers are experts is often overlooked, and \"expertise\" is seldom\nleveraged as an inductive bias. This is especially true for the literature on\ntreatment effect estimation, where often the only assumption made about actions\nis that of overlap. In this paper, we argue that expertise - particularly the\ntype of expertise the decision-makers of a domain are likely to have - can be\ninformative in designing and selecting methods for treatment effect estimation.\nWe formally define two types of expertise, predictive and prognostic, and\ndemonstrate empirically that: (i) the prominent type of expertise in a domain\nsignificantly influences the performance of different methods in treatment\neffect estimation, and (ii) it is possible to predict the type of expertise\npresent in a dataset, which can provide a quantitative basis for model\nselection.",
      "tldr_zh": "该论文探讨了决策者作为领域专家，其行为如何编码知识并应用于治疗效果估计（treatment effect estimation），强调现有机器学习方法常忽略这一“expertise”作为归纳偏差。作者正式定义了两种专家类型：predictive expertise（预测性专家知识）和 prognostic expertise（预后性专家知识）。实验结果显示，这些专家类型显著影响不同方法的性能，并且可以通过预测数据集中的专家类型来为模型选择提供定量依据。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "stat.ML",
      "comment": "The 12th International Conference on Learning Representations (ICLR\n  2024)",
      "pdf_url": "http://arxiv.org/pdf/2403.00694v1",
      "published_date": "2024-03-01 17:30:49 UTC",
      "updated_date": "2024-03-01 17:30:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:37:10.537802"
    },
    {
      "arxiv_id": "2403.00898v1",
      "title": "The Algorithm Configuration Problem",
      "title_zh": "算法配置问题",
      "authors": [
        "Gabriele Iommazzo",
        "Claudia D'Ambrosio",
        "Antonio Frangioni",
        "Leo Liberti"
      ],
      "abstract": "The field of algorithmic optimization has significantly advanced with the\ndevelopment of methods for the automatic configuration of algorithmic\nparameters. This article delves into the Algorithm Configuration Problem,\nfocused on optimizing parametrized algorithms for solving specific instances of\ndecision/optimization problems. We present a comprehensive framework that not\nonly formalizes the Algorithm Configuration Problem, but also outlines\ndifferent approaches for its resolution, leveraging machine learning models and\nheuristic strategies. The article categorizes existing methodologies into\nper-instance and per-problem approaches, distinguishing between offline and\nonline strategies for model construction and deployment. By synthesizing these\napproaches, we aim to provide a clear pathway for both understanding and\naddressing the complexities inherent in algorithm configuration.",
      "tldr_zh": "这篇论文探讨了 Algorithm Configuration Problem，即优化参数化算法以解决特定决策/优化问题的过程。作者提出一个全面框架，形式化该问题并概述了利用 machine learning models 和 heuristic strategies 的解决方法，将现有方法分类为 per-instance 和 per-problem 方式，以及 offline 和 online 策略。通过综合这些方法，论文为理解和应对算法配置的复杂性提供了清晰路径。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.OC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00898v1",
      "published_date": "2024-03-01 17:29:34 UTC",
      "updated_date": "2024-03-01 17:29:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:37:22.312865"
    },
    {
      "arxiv_id": "2403.00692v2",
      "title": "Toward Autonomous Cooperation in Heterogeneous Nanosatellite Constellations Using Dynamic Graph Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Guillem Casadesus-Vila",
        "Joan-Adria Ruiz-de-Azua",
        "Eduard Alarcon"
      ],
      "abstract": "The upcoming landscape of Earth Observation missions will defined by\nnetworked heterogeneous nanosatellite constellations required to meet strict\nmission requirements, such as revisit times and spatial resolution. However,\nscheduling satellite communications in these satellite networks through\nefficiently creating a global satellite Contact Plan (CP) is a complex task,\nwith current solutions requiring ground-based coordination or being limited by\nonboard computational resources. The paper proposes a novel approach to\novercome these challenges by modeling the constellations and CP as dynamic\nnetworks and employing graph-based techniques. The proposed method utilizes a\nstate-of-the-art dynamic graph neural network to evaluate the performance of a\ngiven CP and update it using a heuristic algorithm based on simulated\nannealing. The trained neural network can predict the network delay with a mean\nabsolute error of 3.6 minutes. Simulation results show that the proposed method\ncan successfully design a contact plan for large satellite networks, improving\nthe delay by 29.1%, similar to a traditional approach, while performing the\nobjective evaluations 20x faster.",
      "tldr_zh": "这篇论文针对异构纳米卫星星座的自主合作问题，提出了一种新方法，将星座和联系计划（Contact Plan）建模为动态网络，并利用动态图神经网络（dynamic graph neural networks）评估其性能，同时结合基于模拟退火（simulated annealing）的启发式算法进行优化。相比传统方法，该方法能以3.6分钟的平均绝对误差预测网络延迟，并在模拟实验中将延迟改善29.1%。此外，它能为大型卫星网络高效设计联系计划，评估速度比传统方法快20倍，从而提升了卫星通信调度的自主性和效率。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "eess.SP",
      "comment": "8 pages, 5 figures, conference",
      "pdf_url": "http://arxiv.org/pdf/2403.00692v2",
      "published_date": "2024-03-01 17:26:02 UTC",
      "updated_date": "2024-03-04 04:47:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:37:36.159434"
    },
    {
      "arxiv_id": "2403.00691v1",
      "title": "Tri-Modal Motion Retrieval by Learning a Joint Embedding Space",
      "title_zh": "翻译失败",
      "authors": [
        "Kangning Yin",
        "Shihao Zou",
        "Yuxuan Ge",
        "Zheng Tian"
      ],
      "abstract": "Information retrieval is an ever-evolving and crucial research domain. The\nsubstantial demand for high-quality human motion data especially in online\nacquirement has led to a surge in human motion research works. Prior works have\nmainly concentrated on dual-modality learning, such as text and motion tasks,\nbut three-modality learning has been rarely explored. Intuitively, an extra\nintroduced modality can enrich a model's application scenario, and more\nimportantly, an adequate choice of the extra modality can also act as an\nintermediary and enhance the alignment between the other two disparate\nmodalities. In this work, we introduce LAVIMO (LAnguage-VIdeo-MOtion\nalignment), a novel framework for three-modality learning integrating\nhuman-centric videos as an additional modality, thereby effectively bridging\nthe gap between text and motion. Moreover, our approach leverages a specially\ndesigned attention mechanism to foster enhanced alignment and synergistic\neffects among text, video, and motion modalities. Empirically, our results on\nthe HumanML3D and KIT-ML datasets show that LAVIMO achieves state-of-the-art\nperformance in various motion-related cross-modal retrieval tasks, including\ntext-to-motion, motion-to-text, video-to-motion and motion-to-video.",
      "tldr_zh": "这项研究提出 LAVIMO 框架，用于三模态（文本、视频和动作）学习，旨在桥接文本和动作之间的差距，并丰富模型的应用场景。\n框架通过引入人类中心视频作为中介模态，并采用特殊设计的 attention mechanism 来增强文本、视频和动作模态之间的对齐和协同效应。\n实验结果显示，在 HumanML3D 和 KIT-ML 数据集上，LAVIMO 在文本到动作、动作到文本、视频到动作和动作到视频等跨模态检索任务中取得了 state-of-the-art 性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00691v1",
      "published_date": "2024-03-01 17:23:30 UTC",
      "updated_date": "2024-03-01 17:23:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:37:47.464943"
    },
    {
      "arxiv_id": "2403.00690v1",
      "title": "Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents",
      "title_zh": "使用",
      "authors": [
        "Dominik Jeurissen",
        "Diego Perez-Liebana",
        "Jeremy Gow",
        "Duygu Cakmak",
        "James Kwan"
      ],
      "abstract": "Large Language Models (LLMs) have shown great success as high-level planners\nfor zero-shot game-playing agents. However, these agents are primarily\nevaluated on Minecraft, where long-term planning is relatively straightforward.\nIn contrast, agents tested in dynamic robot environments face limitations due\nto simplistic environments with only a few objects and interactions. To fill\nthis gap in the literature, we present NetPlay, the first LLM-powered zero-shot\nagent for the challenging roguelike NetHack. NetHack is a particularly\nchallenging environment due to its diverse set of items and monsters, complex\ninteractions, and many ways to die.\n  NetPlay uses an architecture designed for dynamic robot environments,\nmodified for NetHack. Like previous approaches, it prompts the LLM to choose\nfrom predefined skills and tracks past interactions to enhance decision-making.\nGiven NetHack's unpredictable nature, NetPlay detects important game events to\ninterrupt running skills, enabling it to react to unforeseen circumstances.\nWhile NetPlay demonstrates considerable flexibility and proficiency in\ninteracting with NetHack's mechanics, it struggles with ambiguous task\ndescriptions and a lack of explicit feedback. Our findings demonstrate that\nNetPlay performs best with detailed context information, indicating the\nnecessity for dynamic methods in supplying context information for complex\ngames such as NetHack.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）作为零样本代理（Zero-Shot Agents）在游戏中的潜力和限制，特别通过开发NetPlay代理应用于挑战性的roguelike游戏NetHack。NetPlay基于动态机器人环境的架构进行修改，利用LLMs提示选择预定义技能、跟踪过去互动，并通过检测重要游戏事件来中断技能，实现对不可预测情况的反应。实验结果显示，NetPlay在处理NetHack的复杂互动时表现出灵活性和熟练度，但面临模糊任务描述和缺乏明确反馈的挑战；研究强调，提供详细上下文信息是提升性能的关键，这突显了在复杂游戏中动态上下文方法的必要性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00690v1",
      "published_date": "2024-03-01 17:22:16 UTC",
      "updated_date": "2024-03-01 17:22:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:37:58.983258"
    },
    {
      "arxiv_id": "2403.00685v2",
      "title": "Know your exceptions: Towards an Ontology of Exceptions in Knowledge Representation",
      "title_zh": "翻译失败",
      "authors": [
        "Gabriele Sacco",
        "Loris Bozzato",
        "Oliver Kutz"
      ],
      "abstract": "Defeasible reasoning is a kind of reasoning where some generalisations may\nnot be valid in all circumstances, that is general conclusions may fail in some\ncases. Various formalisms have been developed to model this kind of reasoning,\nwhich is characteristic of common-sense contexts. However, it is not easy for a\nmodeller to choose among these systems the one that better fits its domain from\nan ontological point of view. In this paper we first propose a framework based\non the notions of exceptionality and defeasibility in order to be able to\ncompare formalisms and reveal their ontological commitments. Then, we apply\nthis framework to compare four systems, showing the differences that may occur\nfrom an ontological perspective.",
      "tldr_zh": "本论文探讨了Defeasible reasoning，即一种可能在特定情况下失效的推理形式，旨在帮助建模者在知识表示领域选择合适的形式主义。作者提出一个基于exceptionality和defeasibility的框架，用于比较不同形式主义并揭示它们的ontological commitments。该框架应用于比较四个系统，从本体视角突显了它们之间的差异，从而为常识性推理建模提供更精确的指导。",
      "categories": [
        "cs.AI",
        "I.2.4"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 4 pages are appendix. (v2 updates: minor revisions on\n  discussions, terminology and text editing)",
      "pdf_url": "http://arxiv.org/pdf/2403.00685v2",
      "published_date": "2024-03-01 17:19:35 UTC",
      "updated_date": "2024-03-05 16:35:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:38:10.019604"
    },
    {
      "arxiv_id": "2403.00897v1",
      "title": "VisRec: A Semi-Supervised Approach to Radio Interferometric Data Reconstruction",
      "title_zh": "VisRec: 一种射电干涉测量数据重建的半监督方法",
      "authors": [
        "Ruoqi Wang",
        "Haitao Wang",
        "Qiong Luo",
        "Feng Wang",
        "Hejun Wu"
      ],
      "abstract": "Radio telescopes produce visibility data about celestial objects, but these\ndata are sparse and noisy. As a result, images created on raw visibility data\nare of low quality. Recent studies have used deep learning models to\nreconstruct visibility data to get cleaner images. However, these methods rely\non a substantial amount of labeled training data, which requires significant\nlabeling effort from radio astronomers. Addressing this challenge, we propose\nVisRec, a model-agnostic semi-supervised learning approach to the\nreconstruction of visibility data. Specifically, VisRec consists of both a\nsupervised learning module and an unsupervised learning module. In the\nsupervised learning module, we introduce a set of data augmentation functions\nto produce diverse training examples. In comparison, the unsupervised learning\nmodule in VisRec augments unlabeled data and uses reconstructions from\nnon-augmented visibility data as pseudo-labels for training. This hybrid\napproach allows VisRec to effectively leverage both labeled and unlabeled data.\nThis way, VisRec performs well even when labeled data is scarce. Our evaluation\nresults show that VisRec outperforms all baseline methods in reconstruction\nquality, robustness against common observation perturbation, and\ngeneralizability to different telescope configurations.",
      "tldr_zh": "该论文提出 VisRec，一种模型无关的半监督学习方法，用于无线电干涉测量数据重建，以解决数据稀疏和嘈杂导致的图像质量问题。VisRec 包括监督学习模块（通过数据增强函数生成多样化训练样本）和无监督学习模块（对无标记数据进行增强，并使用非增强数据的重建作为伪标签）。这种混合方法能有效利用标记和无标记数据，即使标记数据稀缺时也能保持高性能。实验结果显示，VisRec 在重建质量、抗常见观测干扰的鲁棒性和不同望远镜配置的泛化性方面均优于基线方法。",
      "categories": [
        "eess.IV",
        "astro-ph.GA",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00897v1",
      "published_date": "2024-03-01 16:27:33 UTC",
      "updated_date": "2024-03-01 16:27:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:38:23.247649"
    },
    {
      "arxiv_id": "2403.00642v2",
      "title": "Rethinking The Uniformity Metric in Self-Supervised Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xianghong Fang",
        "Jian Li",
        "Qiang Sun",
        "Benyou Wang"
      ],
      "abstract": "Uniformity plays an important role in evaluating learned representations,\nproviding insights into self-supervised learning. In our quest for effective\nuniformity metrics, we pinpoint four principled properties that such metrics\nshould possess. Namely, an effective uniformity metric should remain invariant\nto instance permutations and sample replications while accurately capturing\nfeature redundancy and dimensional collapse. Surprisingly, we find that the\nuniformity metric proposed by \\citet{Wang2020UnderstandingCR} fails to satisfy\nthe majority of these properties. Specifically, their metric is sensitive to\nsample replications, and can not account for feature redundancy and dimensional\ncollapse correctly. To overcome these limitations, we introduce a new\nuniformity metric based on the Wasserstein distance, which satisfies all the\naforementioned properties. Integrating this new metric in existing\nself-supervised learning methods effectively mitigates dimensional collapse and\nconsistently improves their performance on downstream tasks involving CIFAR-10\nand CIFAR-100 datasets. Code is available at\n\\url{https://github.com/statsle/WassersteinSSL}.",
      "tldr_zh": "这篇论文重新审视了自监督学习(Self-Supervised Learning)中的均匀性指标(Uniformity Metric)，指出现有指标（如Wang et al.提出的）无法满足关键属性，包括对实例排列和样本复制的不变性，以及对特征冗余和维度坍缩(Dimensional Collapse)的准确捕捉。作者定义了四个原则属性，并引入了一种基于Wasserstein距离的新均匀性指标，以克服这些局限。实验结果显示，该新指标整合到现有自监督学习方法后，有效缓解了维度坍缩，并在CIFAR-10和CIFAR-100数据集上的下游任务中提升了性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00642v2",
      "published_date": "2024-03-01 16:22:05 UTC",
      "updated_date": "2024-04-26 08:24:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:38:36.015403"
    },
    {
      "arxiv_id": "2403.00632v1",
      "title": "Metamorpheus: Interactive, Affective, and Creative Dream Narration Through Metaphorical Visual Storytelling",
      "title_zh": "翻译失败",
      "authors": [
        "Qian Wan",
        "Xin Feng",
        "Yining Bei",
        "Zhiqi Gao",
        "Zhicong Lu"
      ],
      "abstract": "Human emotions are essentially molded by lived experiences, from which we\nconstruct personalised meaning. The engagement in such meaning-making process\nhas been practiced as an intervention in various psychotherapies to promote\nwellness. Nevertheless, to support recollecting and recounting lived\nexperiences in everyday life remains under explored in HCI. It also remains\nunknown how technologies such as generative AI models can facilitate the\nmeaning making process, and ultimately support affective mindfulness. In this\npaper we present Metamorpheus, an affective interface that engages users in a\ncreative visual storytelling of emotional experiences during dreams.\nMetamorpheus arranges the storyline based on a dream's emotional arc, and\nprovokes self-reflection through the creation of metaphorical images and text\ndepictions. The system provides metaphor suggestions, and generates visual\nmetaphors and text depictions using generative AI models, while users can apply\ngenerations to recolour and re-arrange the interface to be visually affective.\nOur experience-centred evaluation manifests that, by interacting with\nMetamorpheus, users can recall their dreams in vivid detail, through which they\nrelive and reflect upon their experiences in a meaningful way.",
      "tldr_zh": "本论文介绍了 Metamorpheus，一种交互式情感界面，通过隐喻视觉故事讲述，帮助用户回忆和反思梦境中的情感经历。该系统基于梦境的情感弧线生成隐喻图像和文本建议，利用 generative AI 模型支持用户自定义界面，从而促进自省和情感表达。研究结果显示，用户通过与 Metamorpheus 互动，能更生动地回忆梦境，并以有意义的方式重温和分析其生活经历。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted by CHI 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.00632v1",
      "published_date": "2024-03-01 16:09:32 UTC",
      "updated_date": "2024-03-01 16:09:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:38:46.828632"
    },
    {
      "arxiv_id": "2403.00896v3",
      "title": "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Kedi Chen",
        "Qin Chen",
        "Jie Zhou",
        "Yishen He",
        "Liang He"
      ],
      "abstract": "Since large language models (LLMs) achieve significant success in recent\nyears, the hallucination issue remains a challenge, numerous benchmarks are\nproposed to detect the hallucination. Nevertheless, some of these benchmarks\nare not naturally generated by LLMs but are intentionally induced. Also, many\nmerely focus on the factuality hallucination while ignoring the faithfulness\nhallucination. Additionally, although dialogue pattern is more widely utilized\nin the era of LLMs, current benchmarks only concentrate on sentence-level and\npassage-level hallucination. In this study, we propose DiaHalu, the first\ndialogue-level hallucination evaluation benchmark to our knowledge. Initially,\nwe integrate the collected topics into system prompts and facilitate a dialogue\nbetween two ChatGPT3.5. Subsequently, we manually modify the contents that do\nnot adhere to human language conventions and then have LLMs re-generate,\nsimulating authentic human-machine interaction scenarios. Finally, professional\nscholars annotate all the samples in the dataset. DiaHalu covers four common\nmulti-turn dialogue domains and five hallucination subtypes, extended from\nfactuality and faithfulness hallucination. Experiments through some well-known\nLLMs and detection methods on the dataset show that DiaHalu is a challenging\nbenchmark, holding significant value for further research.",
      "tldr_zh": "该研究针对大型语言模型 (LLMs) 的幻觉问题，指出现有基准存在局限，如非自然生成、仅关注事实性幻觉和忽略对话级评估，因此提出首个对话级幻觉评估基准 DiaHalu。DiaHalu 通过将收集的话题整合到系统提示中，让两个 ChatGPT3.5 进行对话，并手动修改不合规范内容后重新生成，最后由专业学者标注，覆盖四个多轮对话领域和五种幻觉子类型（扩展自事实性和忠实性幻觉）。实验结果显示，在知名 LLMs 和检测方法上，DiaHalu 是一个极具挑战性的基准，为幻觉检测研究提供重要价值。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00896v3",
      "published_date": "2024-03-01 15:38:55 UTC",
      "updated_date": "2024-10-10 08:27:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:38:59.723153"
    },
    {
      "arxiv_id": "2403.00895v3",
      "title": "End-to-End Graph-Sequential Representation Learning for Accurate Recommendations",
      "title_zh": "翻译失败",
      "authors": [
        "Vladimir Baikalov",
        "Evgeny Frolov"
      ],
      "abstract": "Recent recommender system advancements have focused on developing\nsequence-based and graph-based approaches. Both approaches proved useful in\nmodeling intricate relationships within behavioral data, leading to promising\noutcomes in personalized ranking and next-item recommendation tasks while\nmaintaining good scalability. However, they capture very different signals from\ndata. While the former approach represents users directly through ordered\ninteractions with recent items, the latter aims to capture indirect\ndependencies across the interactions graph. This paper presents a novel\nmulti-representational learning framework exploiting these two paradigms'\nsynergies. Our empirical evaluation on several datasets demonstrates that\nmutual training of sequential and graph components with the proposed framework\nsignificantly improves recommendations performance.",
      "tldr_zh": "本文提出了一种端到端的图-序列表示学习框架，用于提升推荐系统的准确性。该框架整合了基于序列的方法（通过用户与最近物品的有序交互表示用户）和基于图的方法（捕捉交互图中的间接依赖），并通过相互训练机制利用两者的协同作用。在多个数据集上的实证评估表明，这种多表示学习方法显著提高了个性化排名和下一项推荐任务的性能。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "4 pages, 1 figure, submitted to WWW'24, short-paper track",
      "pdf_url": "http://arxiv.org/pdf/2403.00895v3",
      "published_date": "2024-03-01 15:32:44 UTC",
      "updated_date": "2024-03-15 00:40:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:39:10.238141"
    },
    {
      "arxiv_id": "2403.00587v1",
      "title": "Improving Explicit Spatial Relationships in Text-to-Image Generation through an Automatically Derived Dataset",
      "title_zh": "通过自动派生的数据集改善文本到图像生成中的显式空间关系",
      "authors": [
        "Ander Salaberria",
        "Gorka Azkune",
        "Oier Lopez de Lacalle",
        "Aitor Soroa",
        "Eneko Agirre",
        "Frank Keller"
      ],
      "abstract": "Existing work has observed that current text-to-image systems do not\naccurately reflect explicit spatial relations between objects such as 'left of'\nor 'below'. We hypothesize that this is because explicit spatial relations\nrarely appear in the image captions used to train these models. We propose an\nautomatic method that, given existing images, generates synthetic captions that\ncontain 14 explicit spatial relations. We introduce the Spatial Relation for\nGeneration (SR4G) dataset, which contains 9.9 millions image-caption pairs for\ntraining, and more than 60 thousand captions for evaluation. In order to test\ngeneralization we also provide an 'unseen' split, where the set of objects in\nthe train and test captions are disjoint. SR4G is the first dataset that can be\nused to spatially fine-tune text-to-image systems. We show that fine-tuning two\ndifferent Stable Diffusion models (denoted as SD$_{SR4G}$) yields up to 9\npoints improvements in the VISOR metric. The improvement holds in the 'unseen'\nsplit, showing that SD$_{SR4G}$ is able to generalize to unseen objects.\nSD$_{SR4G}$ improves the state-of-the-art with fewer parameters, and avoids\ncomplex architectures. Our analysis shows that improvement is consistent for\nall relations. The dataset and the code will be publicly available.",
      "tldr_zh": "本研究发现，现有的文本到图像生成模型无法准确处理对象间的显式空间关系（如“left of”或“below”），原因是训练数据中这些关系较少出现。为此，研究提出一种自动方法，通过给定图像生成包含14种空间关系的合成标题，并创建了Spatial Relation for Generation (SR4G)数据集，包括990万图像-标题对用于训练和6万多对用于评估，还包含“unseen”分割以测试泛化能力。SR4G是首个可用于空间微调文本到图像系统的数据集，通过微调Stable Diffusion模型（SD_SR4G），在VISOR metric上实现了高达9点的改进，且这种提升在“unseen”分割中保持一致，证明了模型对未见对象的泛化能力。该方法改进了现有最先进技术，使用更少参数且避免复杂架构，所有空间关系均显示出一致改进。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages and 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.00587v1",
      "published_date": "2024-03-01 15:09:37 UTC",
      "updated_date": "2024-03-01 15:09:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:39:24.040746"
    },
    {
      "arxiv_id": "2403.00570v2",
      "title": "Rethinking cluster-conditioned diffusion models for label-free image synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Nikolas Adaloglou",
        "Tim Kaiser",
        "Felix Michels",
        "Markus Kollmann"
      ],
      "abstract": "Diffusion-based image generation models can enhance image quality when\nconditioned on ground truth labels. Here, we conduct a comprehensive\nexperimental study on image-level conditioning for diffusion models using\ncluster assignments. We investigate how individual clustering determinants,\nsuch as the number of clusters and the clustering method, impact image\nsynthesis across three different datasets. Given the optimal number of clusters\nwith respect to image synthesis, we show that cluster-conditioning can achieve\nstate-of-the-art performance, with an FID of 1.67 for CIFAR10 and 2.17 for\nCIFAR100, along with a strong increase in training sample efficiency. We\nfurther propose a novel empirical method to estimate an upper bound for the\noptimal number of clusters. Unlike existing approaches, we find no significant\nassociation between clustering performance and the corresponding\ncluster-conditional FID scores. The code is available at\nhttps://github.com/HHU-MMBS/cedm-official-wavc2025.",
      "tldr_zh": "本研究重新审视了基于聚类条件的扩散模型，用于无标签图像合成，通过实验探讨聚类参数（如聚类数量和方法）对三个数据集的影响。结果显示，在最佳聚类数量下，该模型达到了最先进性能，CIFAR10 的 FID 为 1.67、CIFAR100 的 FID 为 2.17，同时显著提高了训练样本效率。研究还提出了一种新的经验方法来估算最佳聚类数量上限，并发现聚类性能与对应的 cluster-conditional FID 得分之间无显著关联。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted in WAVC2025 (21 pages, 15 figures). Code is available at\n  https://github.com/HHU-MMBS/cedm-official-wavc2025",
      "pdf_url": "http://arxiv.org/pdf/2403.00570v2",
      "published_date": "2024-03-01 14:47:46 UTC",
      "updated_date": "2024-11-19 11:00:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:39:37.468817"
    },
    {
      "arxiv_id": "2403.00567v2",
      "title": "Flatten Long-Range Loss Landscapes for Cross-Domain Few-Shot Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yixiong Zou",
        "Yicong Liu",
        "Yiman Hu",
        "Yuhua Li",
        "Ruixuan Li"
      ],
      "abstract": "Cross-domain few-shot learning (CDFSL) aims to acquire knowledge from limited\ntraining data in the target domain by leveraging prior knowledge transferred\nfrom source domains with abundant training samples. CDFSL faces challenges in\ntransferring knowledge across dissimilar domains and fine-tuning models with\nlimited training data. To address these challenges, we initially extend the\nanalysis of loss landscapes from the parameter space to the representation\nspace, which allows us to simultaneously interpret the transferring and\nfine-tuning difficulties of CDFSL models. We observe that sharp minima in the\nloss landscapes of the representation space result in representations that are\nhard to transfer and fine-tune. Moreover, existing flatness-based methods have\nlimited generalization ability due to their short-range flatness. To enhance\nthe transferability and facilitate fine-tuning, we introduce a simple yet\neffective approach to achieve long-range flattening of the minima in the loss\nlandscape. This approach considers representations that are differently\nnormalized as minima in the loss landscape and flattens the high-loss region in\nthe middle by randomly sampling interpolated representations. We implement this\nmethod as a new normalization layer that replaces the original one in both CNNs\nand ViTs. This layer is simple and lightweight, introducing only a minimal\nnumber of additional parameters. Experimental results on 8 datasets demonstrate\nthat our approach outperforms state-of-the-art methods in terms of average\naccuracy. Moreover, our method achieves performance improvements of up to 9\\%\ncompared to the current best approaches on individual datasets. Our code will\nbe released.",
      "tldr_zh": "本研究针对跨域少样本学习(CDFSL)面临的知识转移和微调挑战，通过将损失景观分析扩展到表示空间，观察到锐利极小值导致的转移困难。作者提出一种简单有效的长程平坦化方法，该方法将不同归一化的表示视为极小值，并通过随机采样插值表示来平坦化高损失区域。基于此，他们设计了一个轻量级的新归一化层，替换原有的层，适用于CNNs和ViTs，仅引入少量额外参数。在8个数据集上的实验显示，该方法平均准确率优于现有最先进技术，并在个别数据集上提升高达9%。这为提升CDFSL的泛化能力提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00567v2",
      "published_date": "2024-03-01 14:44:41 UTC",
      "updated_date": "2024-04-19 03:17:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:39:47.606855"
    },
    {
      "arxiv_id": "2403.00565v1",
      "title": "Predicting UAV Type: An Exploration of Sampling and Data Augmentation for Time Series Classification",
      "title_zh": "预测 UAV 类型：采样和数据增强在时间序列分类中的探索",
      "authors": [
        "Tarik Crnovrsanin",
        "Calvin Yu",
        "Dane Hankamer",
        "Cody Dunne"
      ],
      "abstract": "Unmanned aerial vehicles are becoming common and have many productive uses.\nHowever, their increased prevalence raises safety concerns -- how can we\nprotect restricted airspace? Knowing the type of unmanned aerial vehicle can go\na long way in determining any potential risks it carries. For instance,\nfixed-wing craft can carry more weight over longer distances, thus potentially\nposing a more significant threat. This paper presents a machine learning model\nfor classifying unmanned aerial vehicles as quadrotor, hexarotor, or\nfixed-wing. Our approach effectively applies a Long-Short Term Memory (LSTM)\nneural network for the purpose of time series classification. We performed\nexperiments to test the effects of changing the timestamp sampling method and\naddressing the imbalance in the class distribution. Through these experiments,\nwe identified the top-performing sampling and class imbalance fixing methods.\nAveraging the macro f-scores across 10 folds of data, we found that the\nmajority quadrotor class was predicted well (98.16%), and, despite an extreme\nclass imbalance, the model could also predicted a majority of fixed-wing\nflights correctly (73.15%). Hexarotor instances were often misclassified as\nquadrotors due to the similarity of multirotors in general (42.15%). However,\nresults remained relatively stable across certain methods, which prompted us to\nanalyze and report on their tradeoffs. The supplemental material for this\npaper, including the code and data for running all the experiments and\ngenerating the results tables, is available at https://osf.io/mnsgk/.",
      "tldr_zh": "这篇论文探讨了使用 Long-Short Term Memory (LSTM) 神经网络进行时间序列分类，以预测无人机的类型，包括 quadrotor、hexarotor 和 fixed-wing，从而评估潜在安全风险。研究通过实验测试了不同时间戳采样方法和类别不平衡修正策略，识别出最佳组合来提高模型性能。结果显示，模型对 quadrotor 的预测准确率最高（宏 F-分数 98.16%），fixed-wing 为 73.15%，但 hexarotor 常被误分类为 quadrotor（42.15%），并分析了各种方法的稳定性和权衡。该工作为处理不平衡数据集的无人机分类提供了实用见解，并附有代码和数据补充材料。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "12 pages, 3 figures, 4 tables, submitted to IEEE Transactions on\n  Cybernetics",
      "pdf_url": "http://arxiv.org/pdf/2403.00565v1",
      "published_date": "2024-03-01 14:43:55 UTC",
      "updated_date": "2024-03-01 14:43:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:40:00.180773"
    },
    {
      "arxiv_id": "2403.00894v2",
      "title": "Comparing large language models and human programmers for generating programming code",
      "title_zh": "翻译失败",
      "authors": [
        "Wenpin Hou",
        "Zhicheng Ji"
      ],
      "abstract": "We systematically evaluated the performance of seven large language models in\ngenerating programming code using various prompt strategies, programming\nlanguages, and task difficulties. GPT-4 substantially outperforms other large\nlanguage models, including Gemini Ultra and Claude 2. The coding performance of\nGPT-4 varies considerably with different prompt strategies. In most LeetCode\nand GeeksforGeeks coding contests evaluated in this study, GPT-4 employing the\noptimal prompt strategy outperforms 85 percent of human participants.\nAdditionally, GPT-4 demonstrates strong capabilities in translating code\nbetween different programming languages and in learning from past errors. The\ncomputational efficiency of the code generated by GPT-4 is comparable to that\nof human programmers. These results suggest that GPT-4 has the potential to\nserve as a reliable assistant in programming code generation and software\ndevelopment.",
      "tldr_zh": "这篇论文系统评估了七个大型语言模型（Large Language Models, LLMs）在生成编程代码方面的表现，涵盖不同提示策略（prompt strategies）、编程语言和任务难度。结果显示，GPT-4 显著优于其他模型如 Gemini Ultra 和 Claude 2，其性能因提示策略而异。在 LeetCode 和 GeeksforGeeks 编码竞赛中，使用最佳提示策略的 GPT-4 超过了 85% 的人类参与者。GPT-4 还表现出色地处理代码翻译和从错误中学习，且其生成的代码计算效率与人类程序员相当。这些发现表明，GPT-4 有潜力作为编程代码生成和软件开发的可靠助手。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00894v2",
      "published_date": "2024-03-01 14:43:06 UTC",
      "updated_date": "2024-10-05 00:34:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:40:13.423762"
    },
    {
      "arxiv_id": "2403.00564v2",
      "title": "EfficientZero V2: Mastering Discrete and Continuous Control with Limited Data",
      "title_zh": "EfficientZero V2：在有限数据下",
      "authors": [
        "Shengjie Wang",
        "Shaohuai Liu",
        "Weirui Ye",
        "Jiacheng You",
        "Yang Gao"
      ],
      "abstract": "Sample efficiency remains a crucial challenge in applying Reinforcement\nLearning (RL) to real-world tasks. While recent algorithms have made\nsignificant strides in improving sample efficiency, none have achieved\nconsistently superior performance across diverse domains. In this paper, we\nintroduce EfficientZero V2, a general framework designed for sample-efficient\nRL algorithms. We have expanded the performance of EfficientZero to multiple\ndomains, encompassing both continuous and discrete actions, as well as visual\nand low-dimensional inputs. With a series of improvements we propose,\nEfficientZero V2 outperforms the current state-of-the-art (SOTA) by a\nsignificant margin in diverse tasks under the limited data setting.\nEfficientZero V2 exhibits a notable advancement over the prevailing general\nalgorithm, DreamerV3, achieving superior outcomes in 50 of 66 evaluated tasks\nacross diverse benchmarks, such as Atari 100k, Proprio Control, and Vision\nControl.",
      "tldr_zh": "这篇论文介绍了 EfficientZero V2，一种针对强化学习（RL）的通用框架，旨在解决样本效率在现实任务中的关键挑战。EfficientZero V2 通过一系列改进扩展了 EfficientZero 的性能，适用于离散和连续控制，以及视觉和低维输入。实验结果显示，该框架在有限数据设置下大幅超越当前 SOTA 算法，并在 Atari 100k、Proprio Control 和 Vision Control 等基准中，在 66 个任务中比 DreamerV3 表现更好，赢得了 50 个任务。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages,10 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.00564v2",
      "published_date": "2024-03-01 14:42:25 UTC",
      "updated_date": "2024-09-12 08:37:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:40:23.128430"
    },
    {
      "arxiv_id": "2403.00561v1",
      "title": "Multi-Task Learning Using Uncertainty to Weigh Losses for Heterogeneous Face Attribute Estimation",
      "title_zh": "基于不确定性权衡损失的多任务学习，用于异构面部属性估计",
      "authors": [
        "Huaqing Yuan",
        "Yi He",
        "Peng Du",
        "Lu Song"
      ],
      "abstract": "Face images contain a wide variety of attribute information. In this paper,\nwe propose a generalized framework for joint estimation of ordinal and nominal\nattributes based on information sharing. We tackle the correlation problem\nbetween heterogeneous attributes using hard parameter sharing of shallow\nfeatures, and trade-off multiple loss functions by considering homoskedastic\nuncertainty for each attribute estimation task. This leads to optimal\nestimation of multiple attributes of the face and reduces the training cost of\nmultitask learning. Experimental results on benchmarks with multiple face\nattributes show that the proposed approach has superior performance compared to\nstate of the art. Finally, we discuss the bias issues arising from the proposed\napproach in face attribute estimation and validate its feasibility on edge\nsystems.",
      "tldr_zh": "这篇论文提出了一种多任务学习框架，用于异构面部属性估计，通过使用不确定性（uncertainty）来权衡损失函数，实现序数和名义属性的联合估计。方法采用硬参数共享（hard parameter sharing）处理浅层特征的相关性问题，并利用同方差不确定性（homoskedastic uncertainty）优化多任务训练，降低了训练成本。实验结果显示，该框架在面部属性基准测试中性能优于现有技术，同时讨论了潜在偏差问题并验证了其在边缘系统的可行性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00561v1",
      "published_date": "2024-03-01 14:39:15 UTC",
      "updated_date": "2024-03-01 14:39:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:40:35.007204"
    },
    {
      "arxiv_id": "2403.03962v1",
      "title": "Identify Critical Nodes in Complex Network with Large Language Models",
      "title_zh": "使用大型语言模型识别复杂网络中的关键节点",
      "authors": [
        "Jinzhu Mao",
        "Dongyun Zou",
        "Li Sheng",
        "Siyi Liu",
        "Chen Gao",
        "Yue Wang",
        "Yong Li"
      ],
      "abstract": "Identifying critical nodes in networks is a classical decision-making task,\nand many methods struggle to strike a balance between adaptability and utility.\nTherefore, we propose an approach that empowers Evolutionary Algorithm (EA)\nwith Large Language Models (LLMs), to generate a function called \"score\\_nodes\"\nwhich can further be used to identify crucial nodes based on their assigned\nscores. Our model consists of three main components: Manual Initialization,\nPopulation Management, and LLMs-based Evolution. It evolves from initial\npopulations with a set of designed node scoring functions created manually.\nLLMs leverage their strong contextual understanding and rich programming skills\nto perform crossover and mutation operations on the individuals, generating\nexcellent new functions. These functions are then categorized, ranked, and\neliminated to ensure the stable development of the populations while preserving\ndiversity. Extensive experiments demonstrate the excellent performance of our\nmethod, showcasing its strong generalization ability compared to other\nstate-of-the-art algorithms. It can consistently and orderly generate diverse\nand efficient node scoring functions. All source codes and models that can\nreproduce all results in this work are publicly available at this link:\n\\url{https://anonymous.4open.science/r/LLM4CN-6520}",
      "tldr_zh": "本文提出了一种结合 Large Language Models (LLMs) 和 Evolutionary Algorithm (EA) 的方法，用于识别复杂网络中的关键节点。该方法包括三个核心组件：Manual Initialization、Population Management 和 LLMs-based Evolution，通过 LLMs 的上下文理解和编程能力进行交叉和变异操作，生成高效的节点评分函数 \"score_nodes\"。实验结果表明，该方法在性能和泛化能力上优于现有最先进算法，能一致产生多样且稳定的函数，所有代码和模型已公开可用。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.03962v1",
      "published_date": "2024-03-01 14:23:26 UTC",
      "updated_date": "2024-03-01 14:23:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:40:47.398086"
    },
    {
      "arxiv_id": "2403.00550v1",
      "title": "Imitation Learning Datasets: A Toolkit For Creating Datasets, Training Agents and Benchmarking",
      "title_zh": "翻译失败",
      "authors": [
        "Nathan Gavenski",
        "Michael Luck",
        "Odinaldo Rodrigues"
      ],
      "abstract": "Imitation learning field requires expert data to train agents in a task. Most\noften, this learning approach suffers from the absence of available data, which\nresults in techniques being tested on its dataset. Creating datasets is a\ncumbersome process requiring researchers to train expert agents from scratch,\nrecord their interactions and test each benchmark method with newly created\ndata. Moreover, creating new datasets for each new technique results in a lack\nof consistency in the evaluation process since each dataset can drastically\nvary in state and action distribution. In response, this work aims to address\nthese issues by creating Imitation Learning Datasets, a toolkit that allows\nfor: (i) curated expert policies with multithreaded support for faster dataset\ncreation; (ii) readily available datasets and techniques with precise\nmeasurements; and (iii) sharing implementations of common imitation learning\ntechniques. Demonstration link:\nhttps://nathangavenski.github.io/#/il-datasets-video",
      "tldr_zh": "这篇论文针对模仿学习(Imitation Learning)领域的数据缺失问题，提出一个名为 Imitation Learning Datasets 的工具包，以简化数据集创建、代理训练和基准测试(Benchmarking)过程。该工具包包括精选的专家策略，支持多线程加速数据集生成；提供现成的可用数据集、技术实现以及精确测量功能；并促进常见模仿学习技术的共享实施。通过这一工具包，研究人员可以避免重复创建数据集带来的不一致性，提高实验效率和可比性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "his paper has been accepted in the demonstration track for the 23rd\n  International Conference on Autonomous Agents and Multi-Agent Systems",
      "pdf_url": "http://arxiv.org/pdf/2403.00550v1",
      "published_date": "2024-03-01 14:18:46 UTC",
      "updated_date": "2024-03-01 14:18:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:40:59.654818"
    },
    {
      "arxiv_id": "2403.05581v2",
      "title": "Can Interpretability Layouts Influence Human Perception of Offensive Sentences?",
      "title_zh": "翻译失败",
      "authors": [
        "Thiago Freitas dos Santos",
        "Nardine Osman",
        "Marco Schorlemmer"
      ],
      "abstract": "This paper conducts a user study to assess whether three machine learning\n(ML) interpretability layouts can influence participants' views when evaluating\nsentences containing hate speech, focusing on the \"Misogyny\" and \"Racism\"\nclasses. Given the existence of divergent conclusions in the literature, we\nprovide empirical evidence on using ML interpretability in online communities\nthrough statistical and qualitative analyses of questionnaire responses. The\nGeneralized Additive Model estimates participants' ratings, incorporating\nwithin-subject and between-subject designs. While our statistical analysis\nindicates that none of the interpretability layouts significantly influences\nparticipants' views, our qualitative analysis demonstrates the advantages of ML\ninterpretability: 1) triggering participants to provide corrective feedback in\ncase of discrepancies between their views and the model, and 2) providing\ninsights to evaluate a model's behavior beyond traditional performance metrics.",
      "tldr_zh": "这篇论文通过用户研究评估三种机器学习 (ML) 可解释性布局是否会影响参与者对仇恨言论句子的感知，焦点在于 “Misogyny” 和 “Racism” 类。研究采用 Generalized Additive Model 进行统计分析，包括组内和组间设计，结果显示这些布局并未显著改变参与者的看法。定性分析则突显了 ML 可解释性的优势：它能激发参与者提供纠正反馈以解决观点与模型不一致的问题，并提供见解来超越传统性能指标评估模型行为。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.05581v2",
      "published_date": "2024-03-01 13:25:54 UTC",
      "updated_date": "2025-05-10 07:15:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:41:11.204018"
    },
    {
      "arxiv_id": "2403.00510v3",
      "title": "ROME: Memorization Insights from Text, Logits and Representation",
      "title_zh": "翻译失败",
      "authors": [
        "Bo Li",
        "Qinghua Zhao",
        "Lijie Wen"
      ],
      "abstract": "Previous works have evaluated memorization by comparing model outputs with\ntraining corpora, examining how factors such as data duplication, model size,\nand prompt length influence memorization. However, analyzing these extensive\ntraining corpora is highly time-consuming. To address this challenge, this\npaper proposes an innovative approach named ROME that bypasses direct\nprocessing of the training data. Specifically, we select datasets categorized\ninto three distinct types -- context-independent, conventional, and factual --\nand redefine memorization as the ability to produce correct answers under these\nconditions. Our analysis then focuses on disparities between memorized and\nnon-memorized samples by examining the logits and representations of generated\ntexts. Experimental findings reveal that longer words are less likely to be\nmemorized, higher confidence correlates with greater memorization, and\nrepresentations of the same concepts are more similar across different\ncontexts. Our code and data will be publicly available when the paper is\naccepted.",
      "tldr_zh": "本研究提出了一种名为 ROME 的创新方法，用于评估语言模型的记忆化过程，而无需直接处理庞大的训练语料。该方法通过选取三种数据集（context-independent、conventional 和 factual），重新定义记忆化为模型在这些条件下生成正确答案的能力，并通过分析 logits 和 representations 来比较记忆化与非记忆化样本的差异。实验发现，更长的单词更不易被记忆化、模型信心越高记忆化程度越强，以及相同概念的 representations 在不同上下文中更相似。该方法为理解模型记忆机制提供了高效见解，并计划公开代码和数据。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Submitted to EMNLP, 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.00510v3",
      "published_date": "2024-03-01 13:15:30 UTC",
      "updated_date": "2024-06-16 13:53:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:41:22.670727"
    },
    {
      "arxiv_id": "2403.00509v1",
      "title": "Surveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese",
      "title_zh": "翻译失败",
      "authors": [
        "Yuqi Chen",
        "Sixuan Li",
        "Ying Li",
        "Mohammad Atari"
      ],
      "abstract": "In this work, we develop a pipeline for historical-psychological text\nanalysis in classical Chinese. Humans have produced texts in various languages\nfor thousands of years; however, most of the computational literature is\nfocused on contemporary languages and corpora. The emerging field of historical\npsychology relies on computational techniques to extract aspects of psychology\nfrom historical corpora using new methods developed in natural language\nprocessing (NLP). The present pipeline, called Contextualized Construct\nRepresentations (CCR), combines expert knowledge in psychometrics (i.e.,\npsychological surveys) with text representations generated via\ntransformer-based language models to measure psychological constructs such as\ntraditionalism, norm strength, and collectivism in classical Chinese corpora.\nConsidering the scarcity of available data, we propose an indirect supervised\ncontrastive learning approach and build the first Chinese historical psychology\ncorpus (C-HI-PSY) to fine-tune pre-trained models. We evaluate the pipeline to\ndemonstrate its superior performance compared with other approaches. The CCR\nmethod outperforms word-embedding-based approaches across all of our tasks and\nexceeds prompting with GPT-4 in most tasks. Finally, we benchmark the pipeline\nagainst objective, external data to further verify its validity.",
      "tldr_zh": "本研究开发了Contextualized Construct Representations (CCR) 管道，用于古典中文的历史心理文本分析，该方法结合心理测量学专家知识和基于Transformer的语言模型，测量心理结构如传统主义、规范强度和集体主义。针对数据稀缺问题，论文提出间接监督对比学习方法，并构建了首个中文历史心理学语料库 (C-HI-PSY) 来微调预训练模型。实验结果显示，CCR 在所有任务中优于基于词嵌入的方法，并在大多数任务中超过 GPT-4 提示，并通过外部客观数据基准测试验证了其有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00509v1",
      "published_date": "2024-03-01 13:14:45 UTC",
      "updated_date": "2024-03-01 13:14:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:41:36.534740"
    },
    {
      "arxiv_id": "2403.00504v1",
      "title": "Learning and Leveraging World Models in Visual Representation Learning",
      "title_zh": "视觉表示学习中的世界模型学习与利用",
      "authors": [
        "Quentin Garrido",
        "Mahmoud Assran",
        "Nicolas Ballas",
        "Adrien Bardes",
        "Laurent Najman",
        "Yann LeCun"
      ],
      "abstract": "Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising\nself-supervised approach that learns by leveraging a world model. While\npreviously limited to predicting missing parts of an input, we explore how to\ngeneralize the JEPA prediction task to a broader set of corruptions. We\nintroduce Image World Models, an approach that goes beyond masked image\nmodeling and learns to predict the effect of global photometric transformations\nin latent space. We study the recipe of learning performant IWMs and show that\nit relies on three key aspects: conditioning, prediction difficulty, and\ncapacity. Additionally, we show that the predictive world model learned by IWM\ncan be adapted through finetuning to solve diverse tasks; a fine-tuned IWM\nworld model matches or surpasses the performance of previous self-supervised\nmethods. Finally, we show that learning with an IWM allows one to control the\nabstraction level of the learned representations, learning invariant\nrepresentations such as contrastive methods, or equivariant representations\nsuch as masked image modelling.",
      "tldr_zh": "本研究扩展了 Joint-Embedding Predictive Architecture (JEPA) 的自监督学习方法，通过提出 Image World Models (IWM) 来预测潜在空间中的全局光度变换，从而超越传统的masked image modeling。IWM 的学习依赖于三个关键要素：conditioning（条件化）、prediction difficulty（预测难度）和capacity（容量），这些要素确保了模型的高效性。实验结果显示，微调后的 IWM 世界模型在各种任务上匹配或超过了现有自监督方法的性能。最后，该方法允许控制学习表示的抽象水平，从不变性表示（如对比学习）到等变性表示（如masked image modelling），为视觉表示学习提供了更灵活的框架。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "23 pages, 16 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.00504v1",
      "published_date": "2024-03-01 13:05:38 UTC",
      "updated_date": "2024-03-01 13:05:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:41:47.197310"
    },
    {
      "arxiv_id": "2403.00891v1",
      "title": "A Regularization-based Transfer Learning Method for Information Extraction via Instructed Graph Decoder",
      "title_zh": "翻译失败",
      "authors": [
        "Kedi Chen",
        "Jie Zhou",
        "Qin Chen",
        "Shunyu Liu",
        "Liang He"
      ],
      "abstract": "Information extraction (IE) aims to extract complex structured information\nfrom the text. Numerous datasets have been constructed for various IE tasks,\nleading to time-consuming and labor-intensive data annotations. Nevertheless,\nmost prevailing methods focus on training task-specific models, while the\ncommon knowledge among different IE tasks is not explicitly modeled. Moreover,\nthe same phrase may have inconsistent labels in different tasks, which poses a\nbig challenge for knowledge transfer using a unified model. In this study, we\npropose a regularization-based transfer learning method for IE (TIE) via an\ninstructed graph decoder. Specifically, we first construct an instruction pool\nfor datasets from all well-known IE tasks, and then present an instructed graph\ndecoder, which decodes various complex structures into a graph uniformly based\non corresponding instructions. In this way, the common knowledge shared with\nexisting datasets can be learned and transferred to a new dataset with new\nlabels. Furthermore, to alleviate the label inconsistency problem among various\nIE tasks, we introduce a task-specific regularization strategy, which does not\nupdate the gradients of two tasks with 'opposite direction'. We conduct\nextensive experiments on 12 datasets spanning four IE tasks, and the results\ndemonstrate the great advantages of our proposed method",
      "tldr_zh": "本研究针对信息抽取 (IE) 任务中数据标注耗时的问题，提出了一种基于正则化的转移学习方法 TIE，通过指导图解码器 (Instructed Graph Decoder) 统一建模不同 IE 任务间的共同知识。具体而言，该方法构建一个指令池来处理各种数据集，并将复杂结构统一解码成图，从而实现知识从现有数据集向新数据集的转移；同时，引入任务特定正则化策略来缓解标签不一致问题，避免更新“相反方向”的梯度。在跨 12 个数据集和四个 IE 任务的实验中，TIE 方法显示出显著优势，证明了其在提升效率和泛化能力方面的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00891v1",
      "published_date": "2024-03-01 13:04:12 UTC",
      "updated_date": "2024-03-01 13:04:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:42:00.358667"
    },
    {
      "arxiv_id": "2403.00890v2",
      "title": "Improving Android Malware Detection Through Data Augmentation Using Wasserstein Generative Adversarial Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Kawana Stalin",
        "Mikias Berhanu Mekoya"
      ],
      "abstract": "Generative Adversarial Networks (GANs) have demonstrated their versatility\nacross various applications, including data augmentation and malware detection.\nThis research explores the effectiveness of utilizing GAN-generated data to\ntrain a model for the detection of Android malware. Given the considerable\nstorage requirements of Android applications, the study proposes a method to\nsynthetically represent data using GANs, thereby reducing storage demands. The\nproposed methodology involves creating image representations of features\nextracted from an existing dataset. A GAN model is then employed to generate a\nmore extensive dataset consisting of realistic synthetic grayscale images.\nSubsequently, this synthetic dataset is utilized to train a Convolutional\nNeural Network (CNN) designed to identify previously unseen Android malware\napplications. The study includes a comparative analysis of the CNN's\nperformance when trained on real images versus synthetic images generated by\nthe GAN. Furthermore, the research explores variations in performance between\nthe Wasserstein Generative Adversarial Network (WGAN) and the Deep\nConvolutional Generative Adversarial Network (DCGAN). The investigation extends\nto studying the impact of image size and malware obfuscation on the\nclassification model's effectiveness. The data augmentation approach\nimplemented in this study resulted in a notable performance enhancement of the\nclassification model, ranging from 1.5% to 7%, depending on the dataset. The\nhighest achieved F1 score reached 0.975.\n  Keywords--Generative Adversarial Networks, Android Malware, Data\nAugmentation, Wasserstein Generative Adversarial Network",
      "tldr_zh": "本研究旨在通过数据增强改善Android Malware检测，使用Wasserstein Generative Adversarial Networks (WGAN)生成合成数据以减少存储需求。方法包括提取Android应用特征转化为图像，然后利用WGAN创建扩展的合成灰度图像数据集，并以此训练Convolutional Neural Network (CNN)来识别新出现的恶意软件。实验比较了WGAN与Deep Convolutional Generative Adversarial Network (DCGAN)的性能，并探讨了图像大小和恶意软件混淆对模型的影响。结果显示，该数据增强方法显著提升了分类模型的性能，F1分数最高达0.975，提升幅度为1.5%至7%。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "20 pages",
      "pdf_url": "http://arxiv.org/pdf/2403.00890v2",
      "published_date": "2024-03-01 12:38:52 UTC",
      "updated_date": "2024-03-05 14:33:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:42:12.620121"
    },
    {
      "arxiv_id": "2403.00887v1",
      "title": "SEGAA: A Unified Approach to Predicting Age, Gender, and Emotion in Speech",
      "title_zh": "SEGAA：一种预测语音中年龄、性别和情感的统一方法",
      "authors": [
        "Aron R",
        "Indra Sigicharla",
        "Chirag Periwal",
        "Mohanaprasad K",
        "Nithya Darisini P S",
        "Sourabh Tiwari",
        "Shivani Arora"
      ],
      "abstract": "The interpretation of human voices holds importance across various\napplications. This study ventures into predicting age, gender, and emotion from\nvocal cues, a field with vast applications. Voice analysis tech advancements\nspan domains, from improving customer interactions to enhancing healthcare and\nretail experiences. Discerning emotions aids mental health, while age and\ngender detection are vital in various contexts. Exploring deep learning models\nfor these predictions involves comparing single, multi-output, and sequential\nmodels highlighted in this paper. Sourcing suitable data posed challenges,\nresulting in the amalgamation of the CREMA-D and EMO-DB datasets. Prior work\nshowed promise in individual predictions, but limited research considered all\nthree variables simultaneously. This paper identifies flaws in an individual\nmodel approach and advocates for our novel multi-output learning architecture\nSpeech-based Emotion Gender and Age Analysis (SEGAA) model. The experiments\nsuggest that Multi-output models perform comparably to individual models,\nefficiently capturing the intricate relationships between variables and speech\ninputs, all while achieving improved runtime.",
      "tldr_zh": "这篇论文提出SEGAA模型，一种统一的多输出学习架构，用于从语音中同时预测年龄、性别和情感，解决了现有研究中单独处理变量的局限性。该方法通过整合CREMA-D和EMO-DB数据集，并比较单输出、多输出和顺序模型，高效捕捉语音输入与变量间的复杂关系。实验结果表明，Multi-output模型的性能与独立模型相当，同时显著改善了运行时效率，为语音分析在医疗、零售等领域应用提供了更可靠的解决方案。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00887v1",
      "published_date": "2024-03-01 11:28:37 UTC",
      "updated_date": "2024-03-01 11:28:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:42:23.381669"
    },
    {
      "arxiv_id": "2403.00450v1",
      "title": "Parallel Hyperparameter Optimization Of Spiking Neural Network",
      "title_zh": "翻译失败",
      "authors": [
        "Thomas Firmin",
        "Pierre Boulet",
        "El-Ghazali Talbi"
      ],
      "abstract": "Spiking Neural Networks (SNN). SNNs are based on a more biologically inspired\napproach than usual artificial neural networks. Such models are characterized\nby complex dynamics between neurons and spikes. These are very sensitive to the\nhyperparameters, making their optimization challenging. To tackle\nhyperparameter optimization of SNNs, we initially extended the signal loss\nissue of SNNs to what we call silent networks. These networks fail to emit\nenough spikes at their outputs due to mistuned hyperparameters or architecture.\nGenerally, search spaces are heavily restrained, sometimes even discretized, to\nprevent the sampling of such networks. By defining an early stopping criterion\ndetecting silent networks and by designing specific constraints, we were able\nto instantiate larger and more flexible search spaces. We applied a constrained\nBayesian optimization technique, which was asynchronously parallelized, as the\nevaluation time of a SNN is highly stochastic. Large-scale experiments were\ncarried-out on a multi-GPU Petascale architecture. By leveraging silent\nnetworks, results show an acceleration of the search, while maintaining good\nperformances of both the optimization algorithm and the best solution obtained.\nWe were able to apply our methodology to two popular training algorithms, known\nas spike timing dependent plasticity and surrogate gradient. Early detection\nallowed us to prevent worthless and costly computation, directing the search\ntoward promising hyperparameter combinations. Our methodology could be applied\nto multi-objective problems, where the spiking activity is often minimized to\nreduce the energy consumption. In this scenario, it becomes essential to find\nthe delicate frontier between low-spiking and silent networks. Finally, our\napproach may have implications for neural architecture search, particularly in\ndefining suitable spiking architectures.",
      "tldr_zh": "本文提出了一种针对 Spiking Neural Networks (SNNs) 的并行超参数优化方法，解决其对超参数高度敏感的问题，包括扩展“silent networks”概念（即因超参数不当导致输出 spikes 不足），并设计早停准则和特定约束来扩大搜索空间。采用异步并行化的约束 Bayesian 优化技术，在多 GPU Petascale 架构上进行大规模实验，结果显示该方法加速了优化过程，同时维持了算法效率和最佳解决方案的性能。最终，该方法适用于 spike timing dependent plasticity 和 surrogate gradient 等训练算法，并为多目标优化（如降低能耗）和神经架构搜索提供潜在扩展。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00450v1",
      "published_date": "2024-03-01 11:11:59 UTC",
      "updated_date": "2024-03-01 11:11:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:42:37.419090"
    },
    {
      "arxiv_id": "2403.00439v1",
      "title": "Authors' Values and Attitudes Towards AI-bridged Scalable Personalization of Creative Language Arts",
      "title_zh": "翻译失败",
      "authors": [
        "Taewook Kim",
        "Hyomin Han",
        "Eytan Adar",
        "Matthew Kay",
        "John Joon Young Chung"
      ],
      "abstract": "Generative AI has the potential to create a new form of interactive media:\nAI-bridged creative language arts (CLA), which bridge the author and audience\nby personalizing the author's vision to the audience's context and taste at\nscale. However, it is unclear what the authors' values and attitudes would be\nregarding AI-bridged CLA. To identify these values and attitudes, we conducted\nan interview study with 18 authors across eight genres (e.g., poetry, comics)\nby presenting speculative but realistic AI-bridged CLA scenarios. We identified\nthree benefits derived from the dynamics between author, artifact, and\naudience: those that 1) authors get from the process, 2) audiences get from the\nartifact, and 3) authors get from the audience. We found how AI-bridged CLA\nwould either promote or reduce these benefits, along with authors' concerns. We\nhope our investigation hints at how AI can provide intriguing experiences to\nCLA audiences while promoting authors' values.",
      "tldr_zh": "本研究探讨了作者对基于 AI 的可扩展个性化创造性语言艺术（AI-bridged Creative Language Arts, AI-bridged CLA）的价值观和态度，旨在通过生成式 AI 桥接作者与观众，实现个性化内容。研究团队通过采访 18 位跨八个类型（如诗歌和漫画）的作者，并呈现假设性场景，识别了三种益处：作者从创作过程获得、观众从作品获得，以及作者从观众反馈获得的益处。结果显示，AI-bridged CLA 可能促进或减少这些益处，同时引发作者的担忧，如对原创性和控制力的疑虑。该研究为 AI 在提供引人入胜的 CLA 体验的同时，尊重作者价值观提供了重要启示。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "16 pages, 6 figures, 2 tables. Accepted to ACM CHI 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.00439v1",
      "published_date": "2024-03-01 10:53:10 UTC",
      "updated_date": "2024-03-01 10:53:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:42:48.716169"
    },
    {
      "arxiv_id": "2403.00437v1",
      "title": "LoMOE: Localized Multi-Object Editing via Multi-Diffusion",
      "title_zh": "LoMOE：通过多扩散实现本地化多对象编辑",
      "authors": [
        "Goirik Chakrabarty",
        "Aditya Chandrasekar",
        "Ramya Hebbalaguppe",
        "Prathosh AP"
      ],
      "abstract": "Recent developments in the field of diffusion models have demonstrated an\nexceptional capacity to generate high-quality prompt-conditioned image edits.\nNevertheless, previous approaches have primarily relied on textual prompts for\nimage editing, which tend to be less effective when making precise edits to\nspecific objects or fine-grained regions within a scene containing\nsingle/multiple objects. We introduce a novel framework for zero-shot localized\nmulti-object editing through a multi-diffusion process to overcome this\nchallenge. This framework empowers users to perform various operations on\nobjects within an image, such as adding, replacing, or editing $\\textbf{many}$\nobjects in a complex scene $\\textbf{in one pass}$. Our approach leverages\nforeground masks and corresponding simple text prompts that exert localized\ninfluences on the target regions resulting in high-fidelity image editing. A\ncombination of cross-attention and background preservation losses within the\nlatent space ensures that the characteristics of the object being edited are\npreserved while simultaneously achieving a high-quality, seamless\nreconstruction of the background with fewer artifacts compared to the current\nmethods. We also curate and release a dataset dedicated to multi-object\nediting, named $\\texttt{LoMOE}$-Bench. Our experiments against existing\nstate-of-the-art methods demonstrate the improved effectiveness of our approach\nin terms of both image editing quality and inference speed.",
      "tldr_zh": "本研究提出LoMOE框架，通过Multi-Diffusion过程实现零样本（zero-shot）本地化多对象编辑，解决了传统基于文本提示的扩散模型在精确编辑图像中特定对象或区域时的局限性。框架利用前景掩码（foreground masks）和简单文本提示，对目标区域施加本地化影响，同时结合交叉注意力损失（cross-attention）和背景保留损失（background preservation losses），确保编辑对象的保真度和背景的无缝重建，减少伪影。作者还发布了LoMOE-Bench数据集，并实验证明，该方法在图像编辑质量和推理速度上均优于现有最先进技术。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages",
      "pdf_url": "http://arxiv.org/pdf/2403.00437v1",
      "published_date": "2024-03-01 10:46:47 UTC",
      "updated_date": "2024-03-01 10:46:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:42:59.790124"
    },
    {
      "arxiv_id": "2403.00436v1",
      "title": "Abductive Ego-View Accident Video Understanding for Safe Driving Perception",
      "title_zh": "基于溯因推理的自我视角事故视频理解，用于安全驾驶感知",
      "authors": [
        "Jianwu Fang",
        "Lei-lei Li",
        "Junfei Zhou",
        "Junbin Xiao",
        "Hongkai Yu",
        "Chen Lv",
        "Jianru Xue",
        "Tat-Seng Chua"
      ],
      "abstract": "We present MM-AU, a novel dataset for Multi-Modal Accident video\nUnderstanding. MM-AU contains 11,727 in-the-wild ego-view accident videos, each\nwith temporally aligned text descriptions. We annotate over 2.23 million object\nboxes and 58,650 pairs of video-based accident reasons, covering 58 accident\ncategories. MM-AU supports various accident understanding tasks, particularly\nmultimodal video diffusion to understand accident cause-effect chains for safe\ndriving. With MM-AU, we present an Abductive accident Video understanding\nframework for Safe Driving perception (AdVersa-SD). AdVersa-SD performs video\ndiffusion via an Object-Centric Video Diffusion (OAVD) method which is driven\nby an abductive CLIP model. This model involves a contrastive interaction loss\nto learn the pair co-occurrence of normal, near-accident, accident frames with\nthe corresponding text descriptions, such as accident reasons, prevention\nadvice, and accident categories. OAVD enforces the causal region learning while\nfixing the content of the original frame background in video generation, to\nfind the dominant cause-effect chain for certain accidents. Extensive\nexperiments verify the abductive ability of AdVersa-SD and the superiority of\nOAVD against the state-of-the-art diffusion models. Additionally, we provide\ncareful benchmark evaluations for object detection and accident reason\nanswering since AdVersa-SD relies on precise object and accident reason\ninformation.",
      "tldr_zh": "本研究引入了MM-AU数据集，包含11,727个ego-view事故视频、超过2.23百万对象框和58,650对事故原因，覆盖58个事故类别，支持多模态视频扩散任务以分析事故的因果链。论文提出Abductive accident Video understanding framework for Safe Driving perception (AdVersa-SD)，该框架利用abductive CLIP模型和Object-Centric Video Diffusion (OAVD)方法，通过对比交互损失学习正常、近事故和事故帧与文本描述的配对共现，并强制执行因果区域学习以生成准确的视频。实验结果显示，AdVersa-SD在事故理解能力上优于现有扩散模型，并在对象检测和事故原因回答基准上表现出色，为安全驾驶感知提供了可靠的工具。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR2024. This is not the camera-ready version. The\n  Project page: http://www.lotvsmmau.net",
      "pdf_url": "http://arxiv.org/pdf/2403.00436v1",
      "published_date": "2024-03-01 10:42:52 UTC",
      "updated_date": "2024-03-01 10:42:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:43:12.991768"
    },
    {
      "arxiv_id": "2403.00425v2",
      "title": "HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Zhaorun Chen",
        "Zhuokai Zhao",
        "Hongyin Luo",
        "Huaxiu Yao",
        "Bo Li",
        "Jiawei Zhou"
      ],
      "abstract": "While large vision-language models (LVLMs) have demonstrated impressive\ncapabilities in interpreting multi-modal contexts, they invariably suffer from\nobject hallucinations (OH). We introduce HALC, a novel decoding algorithm\ndesigned to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal\nvisual information in vision-language tasks and operates on both local and\nglobal contexts simultaneously. Specifically, HALC integrates a robust\nauto-focal grounding mechanism (locally) to correct hallucinated tokens on the\nfly, and a specialized beam search algorithm (globally) to significantly reduce\nOH while preserving text generation quality. Additionally, HALC can be\nintegrated into any LVLMs as a plug-and-play module without extra training.\nExtensive experimental studies demonstrate the effectiveness of HALC in\nreducing OH, outperforming state-of-the-arts across four benchmarks.",
      "tldr_zh": "大型视觉语言模型 (LVLMs) 在处理多模态上下文时常出现对象幻觉 (object hallucinations, OH)，为此，本文提出 HALC，一种新型解码算法来有效减轻这一问题。HALC 同时在局部和全局上下文中运作，通过自动焦点 grounding 机制实时修正幻觉 token，以及专门的 beam search 算法减少 OH，同时保持文本生成质量。该算法作为即插即用模块无需额外训练即可集成到任何 LVLMs 中，并在四个基准测试中优于现有技术，证明了其有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "ICML camera-ready version. Code is released at\n  https://github.com/BillChan226/HALC",
      "pdf_url": "http://arxiv.org/pdf/2403.00425v2",
      "published_date": "2024-03-01 10:21:52 UTC",
      "updated_date": "2024-06-10 15:21:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:43:27.614129"
    },
    {
      "arxiv_id": "2403.00420v2",
      "title": "Robust Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Lucas Schott",
        "Josephine Delas",
        "Hatem Hajri",
        "Elies Gherbi",
        "Reda Yaich",
        "Nora Boulahia-Cuppens",
        "Frederic Cuppens",
        "Sylvain Lamprier"
      ],
      "abstract": "Deep Reinforcement Learning (DRL) is a subfield of machine learning for\ntraining autonomous agents that take sequential actions across complex\nenvironments. Despite its significant performance in well-known environments,\nit remains susceptible to minor condition variations, raising concerns about\nits reliability in real-world applications. To improve usability, DRL must\ndemonstrate trustworthiness and robustness. A way to improve the robustness of\nDRL to unknown changes in the environmental conditions and possible\nperturbations is through Adversarial Training, by training the agent against\nwell-suited adversarial attacks on the observations and the dynamics of the\nenvironment. Addressing this critical issue, our work presents an in-depth\nanalysis of contemporary adversarial attack and training methodologies,\nsystematically categorizing them and comparing their objectives and operational\nmechanisms.",
      "tldr_zh": "这篇论文调查了深度强化学习 (DRL) 的鲁棒性问题，强调 DRL 在复杂环境中虽表现出色，但对环境条件微小变化高度敏感，从而影响其在真实世界应用的可靠性。作者通过对抗训练 (Adversarial Training) 方法来提升 DRL 对未知扰动和动态变化的抵抗力，该方法涉及针对观察和环境动态的对抗攻击 (Adversarial Attacks)。论文对当代相关方法进行深入分析、系统分类和比较，旨在为改进 DRL 的可信度和实用性提供全面指导。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "61 pages, 17 figues, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2403.00420v2",
      "published_date": "2024-03-01 10:16:46 UTC",
      "updated_date": "2024-12-11 15:03:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:43:38.932209"
    },
    {
      "arxiv_id": "2403.00884v3",
      "title": "Zero-Shot Topic Classification of Column Headers: Leveraging LLMs for Metadata Enrichment",
      "title_zh": "翻译失败",
      "authors": [
        "Margherita Martorana",
        "Tobias Kuhn",
        "Lise Stork",
        "Jacco van Ossenbruggen"
      ],
      "abstract": "Traditional dataset retrieval systems rely on metadata for indexing, rather\nthan on the underlying data values. However, high-quality metadata creation and\nenrichment often require manual annotations, which is a labour-intensive and\nchallenging process to automate. In this study, we propose a method to support\nmetadata enrichment using topic annotations generated by three Large Language\nModels (LLMs): ChatGPT-3.5, GoogleBard, and GoogleGemini. Our analysis focuses\non classifying column headers based on domain-specific topics from the\nConsortium of European Social Science Data Archives (CESSDA), a Linked Data\ncontrolled vocabulary. Our approach operates in a zero-shot setting,\nintegrating the controlled topic vocabulary directly within the input prompt.\nThis integration serves as a Large Context Windows approach, with the aim of\nimproving the results of the topic classification task.\n  We evaluated the performance of the LLMs in terms of internal consistency,\ninter-machine alignment, and agreement with human classification. Additionally,\nwe investigate the impact of contextual information (i.e., dataset description)\non the classification outcomes. Our findings suggest that ChatGPT and\nGoogleGemini outperform GoogleBard in terms of internal consistency as well as\nLLM-human-agreement. Interestingly, we found that contextual information had no\nsignificant impact on LLM performance.\n  This work proposes a novel approach that leverages LLMs for topic\nclassification of column headers using a controlled vocabulary, presenting a\npractical application of LLMs and Large Context Windows within the Semantic Web\ndomain. This approach has the potential to facilitate automated metadata\nenrichment, thereby enhancing dataset retrieval and the Findability,\nAccessibility, Interoperability, and Reusability (FAIR) of research data on the\nWeb.",
      "tldr_zh": "这篇论文提出了一种零样本（Zero-Shot）方法，利用大型语言模型（LLMs）如 ChatGPT-3.5、GoogleBard 和 GoogleGemini，对数据集列头进行主题分类，从而实现元数据的自动丰富。方法通过在输入提示中直接整合 CESSDA 的领域特定主题词汇表，作为 Large Context Windows 策略，以提高分类准确性。实验评估显示，ChatGPT 和 GoogleGemini 在内部一致性和与人类分类的一致性上优于 GoogleBard，而添加数据集描述等上下文信息对性能无显著影响。该方法为语义网（Semantic Web）领域提供了实用应用，有助于提升数据集检索和 FAIR（Findability, Accessibility, Interoperability, and Reusability）原则。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00884v3",
      "published_date": "2024-03-01 10:01:36 UTC",
      "updated_date": "2024-09-06 14:49:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:43:51.236179"
    },
    {
      "arxiv_id": "2403.00396v1",
      "title": "GLFNET: Global-Local (frequency) Filter Networks for efficient medical image segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Athanasios Tragakis",
        "Qianying Liu",
        "Chaitanya Kaul",
        "Swalpa Kumar Roy",
        "Hang Dai",
        "Fani Deligianni",
        "Roderick Murray-Smith",
        "Daniele Faccio"
      ],
      "abstract": "We propose a novel transformer-style architecture called Global-Local Filter\nNetwork (GLFNet) for medical image segmentation and demonstrate its\nstate-of-the-art performance. We replace the self-attention mechanism with a\ncombination of global-local filter blocks to optimize model efficiency. The\nglobal filters extract features from the whole feature map whereas the local\nfilters are being adaptively created as 4x4 patches of the same feature map and\nadd restricted scale information. In particular, the feature extraction takes\nplace in the frequency domain rather than the commonly used spatial (image)\ndomain to facilitate faster computations. The fusion of information from both\nspatial and frequency spaces creates an efficient model with regards to\ncomplexity, required data and performance. We test GLFNet on three benchmark\ndatasets achieving state-of-the-art performance on all of them while being\nalmost twice as efficient in terms of GFLOP operations.",
      "tldr_zh": "这篇论文提出了 GLFNet，一种新型的 transformer-style 架构，用于高效的医疗图像分割，以优化模型性能。GLFNet 替换自注意力机制，使用 global-local filter blocks：在频率域中，global filters 从整个特征图提取特征，而 local filters 通过自适应创建的 4x4 补丁添加受限尺度信息，从而融合空间和频率域的信息。实验结果显示，GLFNet 在三个基准数据集上达到了 state-of-the-art 性能，同时在 GFLOP 操作上几乎是基线模型的两倍高效。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00396v1",
      "published_date": "2024-03-01 09:35:03 UTC",
      "updated_date": "2024-03-01 09:35:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:44:02.015297"
    },
    {
      "arxiv_id": "2403.00376v3",
      "title": "Spurious Feature Eraser: Stabilizing Test-Time Adaptation for Vision-Language Foundation Model",
      "title_zh": "翻译失败",
      "authors": [
        "Huan Ma",
        "Yan Zhu",
        "Changqing Zhang",
        "Peilin Zhao",
        "Baoyuan Wu",
        "Long-Kai Huang",
        "Qinghua Hu",
        "Bingzhe Wu"
      ],
      "abstract": "Vision-language foundation models have exhibited remarkable success across a\nmultitude of downstream tasks due to their scalability on extensive image-text\npaired data. However, these models also display significant limitations when\napplied to downstream tasks, such as fine-grained image classification, as a\nresult of ``decision shortcuts'' that hinder their generalization capabilities.\nIn this work, we find that the CLIP model possesses a rich set of features,\nencompassing both \\textit{desired invariant causal features} and\n\\textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP\non downstream tasks originates from its inability to effectively utilize\npre-trained features in accordance with specific task requirements. To address\nthis challenge, we propose a simple yet effective method, Spurious Feature\nEraser (SEraser), to alleviate the decision shortcuts by erasing the spurious\nfeatures. Specifically, we introduce a test-time prompt tuning paradigm that\noptimizes a learnable prompt, thereby compelling the model to exploit invariant\nfeatures while disregarding decision shortcuts during the inference phase. The\nproposed method effectively alleviates excessive dependence on potentially\nmisleading spurious information. We conduct comparative analysis of the\nproposed method against various approaches which validates the significant\nsuperiority.",
      "tldr_zh": "该研究发现，视觉语言基础模型（如 CLIP）在下游任务如细粒度图像分类中，由于决策捷径（decision shortcuts）和虚假特征（spurious features）的存在，导致泛化能力不足。论文提出了一种简单有效的解决方案，Spurious Feature Eraser (SEraser)，通过测试时提示调优（test-time prompt tuning）优化可学习提示，迫使模型在推理阶段优先利用不变的因果特征（invariant causal features）而忽略误导性信息。该方法显著减少了对虚假特征的过度依赖，并在与多种方法的比较中表现出性能上的明显优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00376v3",
      "published_date": "2024-03-01 09:01:53 UTC",
      "updated_date": "2025-01-14 12:37:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:44:14.128841"
    },
    {
      "arxiv_id": "2403.00880v2",
      "title": "CIDGMed: Causal Inference-Driven Medication Recommendation with Enhanced Dual-Granularity Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Shunpan Liang",
        "Xiang Li",
        "Shi Mu",
        "Chen Li",
        "Yu Lei",
        "Yulei Hou",
        "Tengfei Ma"
      ],
      "abstract": "Medication recommendation aims to integrate patients' long-term health\nrecords to provide accurate and safe medication combinations for specific\nhealth states. Existing methods often fail to deeply explore the true causal\nrelationships between diseases/procedures and medications, resulting in biased\nrecommendations. Additionally, in medication representation learning, the\nrelationships between information at different granularities of medications,\ncoarse-grained (medication itself) and fine-grained (molecular level), are not\neffectively integrated, leading to biases in representation learning. To\naddress these limitations, we propose the Causal Inference-driven\nDual-Granularity Medication Recommendation method (CIDGMed). Our approach\nleverages causal inference to uncover the relationships between\ndiseases/procedures and medications, thereby enhancing the rationality and\ninterpretability of recommendations. By integrating coarse-grained medication\neffects with fine-grained molecular structure information, CIDGMed provides a\ncomprehensive representation of medications. Additionally, we employ a bias\ncorrection model during the prediction phase to further refine recommendations,\nensuring both accuracy and safety. Through extensive experiments, CIDGMed\nsignificantly outperforms current state-of-the-art models across multiple\nmetrics, achieving a 2.54% increase in accuracy, a 3.65% reduction in side\neffects, and a 39.42% improvement in time efficiency. Additionally, we\ndemonstrate the rationale of CIDGMed through a case study.",
      "tldr_zh": "这篇论文提出了CIDGMed方法，利用Causal Inference驱动的药物推荐框架，通过增强的Dual-Granularity学习整合粗粒度（药物本身）和细粒度（分子水平）信息，以解决现有方法在因果关系探索和表示学习中的偏差问题。CIDGMed通过因果推理揭示疾病/程序与药物间的真实关系，并引入偏差修正模型来优化推荐的准确性、安全性和可解释性。实验结果显示，该方法在多个指标上优于现有模型，提高准确率2.54%、减少副作用3.65%、提升时间效率39.42%，并通过案例研究证明其合理性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00880v2",
      "published_date": "2024-03-01 08:50:27 UTC",
      "updated_date": "2024-10-30 05:18:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:44:26.061622"
    },
    {
      "arxiv_id": "2403.00878v1",
      "title": "Crimson: Empowering Strategic Reasoning in Cybersecurity through Large Language Models",
      "title_zh": "Crimson：通过大型语言模型增强网络安全中的战略推理",
      "authors": [
        "Jiandong Jin",
        "Bowen Tang",
        "Mingxuan Ma",
        "Xiao Liu",
        "Yunfei Wang",
        "Qingnan Lai",
        "Jia Yang",
        "Changling Zhou"
      ],
      "abstract": "We introduces Crimson, a system that enhances the strategic reasoning\ncapabilities of Large Language Models (LLMs) within the realm of cybersecurity.\nBy correlating CVEs with MITRE ATT&CK techniques, Crimson advances threat\nanticipation and strategic defense efforts. Our approach includes defining and\nevaluating cybersecurity strategic tasks, alongside implementing a\ncomprehensive human-in-the-loop data-synthetic workflow to develop the\nCVE-to-ATT&CK Mapping (CVEM) dataset. We further enhance LLMs' reasoning\nabilities through a novel Retrieval-Aware Training (RAT) process and its\nrefined iteration, RAT-R.\n  Our findings demonstrate that an LLM fine-tuned with our techniques,\npossessing 7 billion parameters, approaches the performance level of GPT-4,\nshowing markedly lower rates of hallucination and errors, and surpassing other\nmodels in strategic reasoning tasks. Moreover, domain-specific fine-tuning of\nembedding models significantly improves performance within cybersecurity\ncontexts, underscoring the efficacy of our methodology. By leveraging Crimson\nto convert raw vulnerability data into structured and actionable insights, we\nbolster proactive cybersecurity defenses.",
      "tldr_zh": "本研究引入Crimson系统，通过将CVE与MITRE ATT&CK技术相关联，提升LLMs在网络安全中的战略推理能力。研究团队开发了CVE-to-ATT&CK Mapping (CVEM)数据集，并采用Retrieval-Aware Training (RAT)及其改进版RAT-R，以及人类参与的数据合成工作流程来增强模型的推理和准确性。实验结果表明，一个7亿参数的LLM在经过这些技术微调后，性能接近GPT-4，显著降低幻觉和错误率，并在战略任务上优于其他模型，从而将原始漏洞数据转化为可操作的洞见，强化主动网络安全防御。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "9 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.00878v1",
      "published_date": "2024-03-01 08:43:43 UTC",
      "updated_date": "2024-03-01 08:43:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:44:39.705977"
    },
    {
      "arxiv_id": "2403.00353v1",
      "title": "MS-Net: A Multi-Path Sparse Model for Motion Prediction in Multi-Scenes",
      "title_zh": "MS-Net：一种多路径稀疏模型用于多场景中的运动预测",
      "authors": [
        "Xiaqiang Tang",
        "Weigao Sun",
        "Siyuan Hu",
        "Yiyang Sun",
        "Yafeng Guo"
      ],
      "abstract": "The multi-modality and stochastic characteristics of human behavior make\nmotion prediction a highly challenging task, which is critical for autonomous\ndriving. While deep learning approaches have demonstrated their great potential\nin this area, it still remains unsolved to establish a connection between\nmultiple driving scenes (e.g., merging, roundabout, intersection) and the\ndesign of deep learning models. Current learning-based methods typically use\none unified model to predict trajectories in different scenarios, which may\nresult in sub-optimal results for one individual scene. To address this issue,\nwe propose Multi-Scenes Network (aka. MS-Net), which is a multi-path sparse\nmodel trained by an evolutionary process. MS-Net selectively activates a subset\nof its parameters during the inference stage to produce prediction results for\neach scene. In the training stage, the motion prediction task under\ndifferentiated scenes is abstracted as a multi-task learning problem, an\nevolutionary algorithm is designed to encourage the network search of the\noptimal parameters for each scene while sharing common knowledge between\ndifferent scenes. Our experiment results show that with substantially reduced\nparameters, MS-Net outperforms existing state-of-the-art methods on\nwell-established pedestrian motion prediction datasets, e.g., ETH and UCY, and\nranks the 2nd place on the INTERACTION challenge.",
      "tldr_zh": "本研究针对多场景下的人类运动预测挑战（如合并、环岛和交叉路口），提出了一种多路径稀疏模型MS-Net，以解决传统统一模型在特定场景下性能次优的问题。MS-Net通过进化算法进行训练，将不同场景的预测任务抽象为多任务学习问题，实现参数子集的 selective activation 和知识共享，从而优化每个场景的预测准确性。实验结果显示，MS-Net在减少参数数量的情况下，在ETH和UCY数据集上优于现有最先进方法，并在INTERACTION挑战赛中排名第二。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by IEEE Robotics and Automation Letters (RAL)",
      "pdf_url": "http://arxiv.org/pdf/2403.00353v1",
      "published_date": "2024-03-01 08:32:12 UTC",
      "updated_date": "2024-03-01 08:32:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:44:49.506672"
    },
    {
      "arxiv_id": "2403.00876v1",
      "title": "Word Order and World Knowledge",
      "title_zh": "词序和世界知识",
      "authors": [
        "Qinghua Zhao",
        "Vinit Ravishankar",
        "Nicolas Garneau",
        "Anders Søgaard"
      ],
      "abstract": "Word order is an important concept in natural language, and in this work, we\nstudy how word order affects the induction of world knowledge from raw text\nusing language models. We use word analogies to probe for such knowledge.\nSpecifically, in addition to the natural word order, we first respectively\nextract texts of six fixed word orders from five languages and then pretrain\nthe language models on these texts. Finally, we analyze the experimental\nresults of the fixed word orders on word analogies and show that i) certain\nfixed word orders consistently outperform or underperform others, though the\nspecifics vary across languages, and ii) the Wov2Lex hypothesis is not hold in\npre-trained language models, and the natural word order typically yields\nmediocre results. The source code will be made publicly available at\nhttps://github.com/lshowway/probing_by_analogy.",
      "tldr_zh": "本文研究了词序（word order）如何影响语言模型从原始文本中诱导世界知识（world knowledge），并使用词类比（word analogies）作为探测工具。具体方法包括从五种语言中提取六种固定词序的文本，并在这些文本上预训练语言模型。实验结果显示，某些固定词序在不同语言中一致表现出优异或劣势，Wov2Lex hypothesis 在预训练模型中不成立，而自然词序通常表现中等。该研究为理解语言模型的知识获取机制提供了新见解，并将源代码公开在 GitHub 上。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00876v1",
      "published_date": "2024-03-01 08:13:48 UTC",
      "updated_date": "2024-03-01 08:13:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:45:03.640008"
    },
    {
      "arxiv_id": "2403.00875v1",
      "title": "Enhancing Protein Predictive Models via Proteins Data Augmentation: A Benchmark and New Directions",
      "title_zh": "翻译失败",
      "authors": [
        "Rui Sun",
        "Lirong Wu",
        "Haitao Lin",
        "Yufei Huang",
        "Stan Z. Li"
      ],
      "abstract": "Augmentation is an effective alternative to utilize the small amount of\nlabeled protein data. However, most of the existing work focuses on design-ing\nnew architectures or pre-training tasks, and relatively little work has studied\ndata augmentation for proteins. This paper extends data augmentation techniques\npreviously used for images and texts to proteins and then benchmarks these\ntechniques on a variety of protein-related tasks, providing the first\ncomprehensive evaluation of protein augmentation. Furthermore, we propose two\nnovel semantic-level protein augmentation methods, namely Integrated Gradients\nSubstitution and Back Translation Substitution, which enable protein\nsemantic-aware augmentation through saliency detection and biological\nknowledge. Finally, we integrate extended and proposed augmentations into an\naugmentation pool and propose a simple but effective framework, namely\nAutomated Protein Augmentation (APA), which can adaptively select the most\nsuitable augmentation combinations for different tasks. Extensive experiments\nhave shown that APA enhances the performance of five protein related tasks by\nan average of 10.55% across three architectures compared to vanilla\nimplementations without augmentation, highlighting its potential to make a\ngreat impact on the field.",
      "tldr_zh": "本研究探讨了通过数据增强提升蛋白预测模型的方法，以解决标记蛋白数据稀缺的问题。首先，将图像和文本的数据增强技术扩展到蛋白领域，并对这些技术在多种蛋白相关任务上进行首次全面基准测试。其次，提出两种新颖的语义级增强方法：Integrated Gradients Substitution 和 Back Translation Substitution，利用显著性检测和生物知识实现蛋白语义感知增强。随后，整合这些增强方法到一个增强池中，开发了Automated Protein Augmentation (APA)框架，该框架能自适应选择最佳增强组合。实验结果显示，APA 在三个架构上平均提升五个蛋白相关任务的性能 10.55%，为蛋白数据增强领域开辟了新方向。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG",
        "q-bio.BM"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00875v1",
      "published_date": "2024-03-01 07:58:29 UTC",
      "updated_date": "2024-03-01 07:58:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:45:14.526175"
    },
    {
      "arxiv_id": "2403.00336v2",
      "title": "Never-Ending Behavior-Cloning Agent for Robotic Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Wenqi Liang",
        "Gan Sun",
        "Qian He",
        "Yu Ren",
        "Jiahua Dong",
        "Yang Cong"
      ],
      "abstract": "Relying on multi-modal observations, embodied robots could perform multiple\nrobotic manipulation tasks in unstructured real-world environments. However,\nmost language-conditioned behavior-cloning agents still face existing\nlong-standing challenges, i.e., 3D scene representation and human-level task\nlearning, when adapting into new sequential tasks in practical scenarios. We\nhere investigate these above challenges with NBAgent in embodied robots, a\npioneering language-conditioned Never-ending Behavior-cloning Agent. It can\ncontinually learn observation knowledge of novel 3D scene semantics and robot\nmanipulation skills from skill-shared and skill-specific attributes,\nrespectively. Specifically, we propose a skill-sharedsemantic rendering module\nand a skill-shared representation distillation module to effectively learn 3D\nscene semantics from skill-shared attribute, further tackling 3D scene\nrepresentation overlooking. Meanwhile, we establish a skill-specific evolving\nplanner to perform manipulation knowledge decoupling, which can continually\nembed novel skill-specific knowledge like human from latent and low-rank space.\nFinally, we design a never-ending embodied robot manipulation benchmark, and\nexpensive experiments demonstrate the significant performance of our method.\nVisual results, code, and dataset are provided at: https://neragent.github.io.",
      "tldr_zh": "这篇论文提出了NBAgent，一种开创性的语言条件Never-ending Behavior-cloning Agent，用于机器人操作任务，能够持续学习新3D场景语义和操作技能，从而解决现有代理在3D场景表示和人类级任务学习方面的挑战。关键方法包括技能共享语义渲染模块和表示蒸馏模块，用于从技能共享属性中有效提取3D场景知识，以及技能特定演化规划器，通过知识解耦从潜在和低秩空间嵌入新技能。实验结果显示，该方法在永不结束的机器人操作基准上表现出显著性能提升，并提供了相关代码和数据集以供验证。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "17 pages, 6 figures, 9 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.00336v2",
      "published_date": "2024-03-01 07:51:29 UTC",
      "updated_date": "2024-06-07 08:10:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:45:27.716607"
    },
    {
      "arxiv_id": "2403.00329v1",
      "title": "Learning with Logical Constraints but without Shortcut Satisfaction",
      "title_zh": "翻译失败",
      "authors": [
        "Zenan Li",
        "Zehua Liu",
        "Yuan Yao",
        "Jingwei Xu",
        "Taolue Chen",
        "Xiaoxing Ma",
        "Jian Lü"
      ],
      "abstract": "Recent studies in neuro-symbolic learning have explored the integration of\nlogical knowledge into deep learning via encoding logical constraints as an\nadditional loss function. However, existing approaches tend to vacuously\nsatisfy logical constraints through shortcuts, failing to fully exploit the\nknowledge. In this paper, we present a new framework for learning with logical\nconstraints. Specifically, we address the shortcut satisfaction issue by\nintroducing dual variables for logical connectives, encoding how the constraint\nis satisfied. We further propose a variational framework where the encoded\nlogical constraint is expressed as a distributional loss that is compatible\nwith the model's original training loss. The theoretical analysis shows that\nthe proposed approach bears salient properties, and the experimental\nevaluations demonstrate its superior performance in both model generalizability\nand constraint satisfaction.",
      "tldr_zh": "本研究针对神经符号学习（neuro-symbolic learning）中将逻辑约束整合到深度学习的问题，指出现有方法往往通过shortcuts空洞满足约束，从而未能充分利用逻辑知识。论文提出一种新框架，使用dual variables for logical connectives来编码约束满足方式，并通过variational framework将逻辑约束转化为与模型原始训练损失兼容的distributional loss，以避免shortcut satisfaction。实验结果显示，该方法显著提升了模型的generalizability和constraint satisfaction性能，为更有效的逻辑约束学习提供了理论和实证支持。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Published as a conference paper at ICLR 2023, and code is available\n  at https://github.com/SoftWiser-group/NeSy-without-Shortcuts",
      "pdf_url": "http://arxiv.org/pdf/2403.00329v1",
      "published_date": "2024-03-01 07:17:20 UTC",
      "updated_date": "2024-03-01 07:17:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:45:37.882049"
    },
    {
      "arxiv_id": "2403.00872v1",
      "title": "DFIN-SQL: Integrating Focused Schema with DIN-SQL for Superior Accuracy in Large-Scale Databases",
      "title_zh": "翻译失败",
      "authors": [
        "Shai Volvovsky",
        "Marco Marcassa",
        "Mustafa Panbiharwala"
      ],
      "abstract": "The task of converting natural language queries into SQL queries is\nintricate, necessitating a blend of precise techniques for an accurate\ntranslation. The DIN-SQL (Decomposed-In-Context SQL) methodology represents a\nsignificant development in this domain. This paper introduces DFIN (Decomposed\nFocused-In-Context), an innovative extension of DIN-SQL that enhances\nText-to-SQL conversion by addressing schema linking errors, which are a major\nsource of inaccuracies. DFIN uniquely alternates between prompting techniques\nand Retrieval-Augmented Generation (RAG), adapting to the size and complexity\nof the database schema. A preprocessing phase embeds database definitions and\nleverages annotated files, akin to those in the BIRD dataset, facilitating the\nruntime retrieval of pertinent schema information. This strategy significantly\nreduces the token count for schema linking prompts, enabling the use of a\nstandard GPT-4 model over its larger context variant, thus handling large-scale\ndatabases more effectively and economically. Our evaluation on the BIRD\ndataset, a challenging real-world benchmark, demonstrates that DFIN not only\nscales efficiently but also improves accuracy, achieving a score of 51.69. This\nimprovement surpasses DIN-SQL method (the current third-place), which is the\nhighest-ranked model employing in-context learning rather than fine-tuning,\npreviously scoring 50.72. The advancement of DFIN underscores the evolving\ncapabilities of in-context learning methodologies combined with advanced\nlanguage models, offering a promising avenue for future research in complex\nText-to-SQL conversion tasks.",
      "tldr_zh": "该论文提出DFIN（Decomposed Focused-In-Context）方法，作为DIN-SQL（Decomposed-In-Context SQL）的扩展，旨在提升Text-to-SQL转换的准确性，特别是针对schema linking errors在大型数据库中的问题。DFIN通过交替使用prompting techniques和Retrieval-Augmented Generation (RAG)，结合预处理阶段嵌入数据库定义和BIRD数据集的注释文件，减少token计数并适应复杂schema，从而更高效地使用标准GPT-4模型。实验结果显示，DFIN在BIRD数据集上实现51.69的准确率，超过DIN-SQL的50.72，并证明了in-context learning结合高级语言模型的潜力，为处理大规模数据库提供经济可行的方案。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00872v1",
      "published_date": "2024-03-01 07:14:45 UTC",
      "updated_date": "2024-03-01 07:14:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:45:52.435058"
    },
    {
      "arxiv_id": "2403.00323v1",
      "title": "Softened Symbol Grounding for Neuro-symbolic Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Zenan Li",
        "Yuan Yao",
        "Taolue Chen",
        "Jingwei Xu",
        "Chun Cao",
        "Xiaoxing Ma",
        "Jian Lü"
      ],
      "abstract": "Neuro-symbolic learning generally consists of two separated worlds, i.e.,\nneural network training and symbolic constraint solving, whose success hinges\non symbol grounding, a fundamental problem in AI. This paper presents a novel,\nsoftened symbol grounding process, bridging the gap between the two worlds, and\nresulting in an effective and efficient neuro-symbolic learning framework.\nTechnically, the framework features (1) modeling of symbol solution states as a\nBoltzmann distribution, which avoids expensive state searching and facilitates\nmutually beneficial interactions between network training and symbolic\nreasoning;(2) a new MCMC technique leveraging projection and SMT solvers, which\nefficiently samples from disconnected symbol solution spaces; (3) an annealing\nmechanism that can escape from %being trapped into sub-optimal symbol\ngroundings. Experiments with three representative neuro symbolic learning tasks\ndemonstrate that, owining to its superior symbol grounding capability, our\nframework successfully solves problems well beyond the frontier of the existing\nproposals.",
      "tldr_zh": "本文提出了一种软化符号接地（softened symbol grounding）过程，用于桥接神经符号学习（neuro-symbolic learning）中的神经网络训练和符号约束求解鸿沟，构建一个高效框架。该框架的关键创新包括：将符号解决方案建模为 Boltzmann distribution 以促进互利互动、开发一种基于投影和 SMT solvers 的新 MCMC 技术来高效采样不连通的解决方案空间、以及引入退火机制以避免陷入次优符号接地。在三个代表性任务的实验中，该框架展示了卓越的符号接地能力，成功解决了现有方法无法处理的难题。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Published as a conference paper at ICLR 2023. Code is available at\n  https://github.com/SoftWiser-group/Soften-NeSy-learning",
      "pdf_url": "http://arxiv.org/pdf/2403.00323v1",
      "published_date": "2024-03-01 06:57:09 UTC",
      "updated_date": "2024-03-01 06:57:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:46:03.757568"
    },
    {
      "arxiv_id": "2403.00318v1",
      "title": "Deep Reinforcement Learning for Solving Management Problems: Towards A Large Management Mode",
      "title_zh": "翻译失败",
      "authors": [
        "Jinyang Jiang",
        "Xiaotian Liu",
        "Tao Ren",
        "Qinghao Wang",
        "Yi Zheng",
        "Yufu Du",
        "Yijie Peng",
        "Cheng Zhang"
      ],
      "abstract": "We introduce a deep reinforcement learning (DRL) approach for solving\nmanagement problems including inventory management, dynamic pricing, and\nrecommendation. This DRL approach has the potential to lead to a large\nmanagement model based on certain transformer neural network structures,\nresulting in an artificial general intelligence paradigm for various management\ntasks. Traditional methods have limitations for solving complex real-world\nproblems, and we demonstrate how DRL can surpass existing heuristic approaches\nfor solving management tasks. We aim to solve the problems in a unified\nframework, considering the interconnections between different tasks. Central to\nour methodology is the development of a foundational decision model\ncoordinating decisions across the different domains through generative\ndecision-making. Our experimental results affirm the effectiveness of our\nDRL-based framework in complex and dynamic business environments. This work\nopens new pathways for the application of DRL in management problems,\nhighlighting its potential to revolutionize traditional business management.",
      "tldr_zh": "本研究提出了一种基于深度强化学习 (DRL) 的方法，用于解决管理问题，如库存管理、动态定价和推荐系统。该方法旨在构建一个统一框架，考虑不同任务间的相互连接，并通过一个基础决策模型实现生成式决策协调，从而超越传统启发式方法的局限性。基于 Transformer 神经网络结构，DRL 有潜力发展成大型管理模型，形成针对各种管理任务的人工通用智能 (AGI) 范式。实验结果证实，该框架在复杂动态商业环境中表现出色，为 DRL 在管理领域的应用开辟新路径，并可能革新传统商业管理实践。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00318v1",
      "published_date": "2024-03-01 06:40:02 UTC",
      "updated_date": "2024-03-01 06:40:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:46:15.587541"
    },
    {
      "arxiv_id": "2403.00315v1",
      "title": "Axe the X in XAI: A Plea for Understandable AI",
      "title_zh": "翻译失败",
      "authors": [
        "Andrés Páez"
      ],
      "abstract": "In a recent paper, Erasmus et al. (2021) defend the idea that the ambiguity\nof the term \"explanation\" in explainable AI (XAI) can be solved by adopting any\nof four different extant accounts of explanation in the philosophy of science:\nthe Deductive Nomological, Inductive Statistical, Causal Mechanical, and New\nMechanist models. In this chapter, I show that the authors' claim that these\naccounts can be applied to deep neural networks as they would to any natural\nphenomenon is mistaken. I also provide a more general argument as to why the\nnotion of explainability as it is currently used in the XAI literature bears\nlittle resemblance to the traditional concept of scientific explanation. It\nwould be more fruitful to use the label \"understandable AI\" to avoid the\nconfusion that surrounds the goal and purposes of XAI. In the second half of\nthe chapter, I argue for a pragmatic conception of understanding that is better\nsuited to play the central role attributed to explanation in XAI. Following\nKuorikoski & Ylikoski (2015), the conditions of satisfaction for understanding\nan ML system are fleshed out in terms of an agent's success in using the\nsystem, in drawing correct inferences from it.",
      "tldr_zh": "该论文批评了XAI（可解释AI）中“解释”概念的模糊性，指出哲学科学中的解释模型（如Deductive Nomological、Inductive Statistical、Causal Mechanical和New Mechanist模型）无法直接应用于深度神经网络，因为这些模型不适合AI的实际场景。作者论证了XAI的当前定义与传统科学解释相去甚远，建议改用“Understandable AI”标签，以减少混淆并更注重实际目标。论文进一步提出一个实用主义的理解概念，基于Kuorikoski & Ylikoski (2015)的观点，将ML（机器学习）系统的理解定义为代理在正确使用和推理方面的成功。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00315v1",
      "published_date": "2024-03-01 06:28:53 UTC",
      "updated_date": "2024-03-01 06:28:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:46:28.129822"
    },
    {
      "arxiv_id": "2403.00307v1",
      "title": "Embedded Multi-label Feature Selection via Orthogonal Regression",
      "title_zh": "基于正交回归的嵌入式多标签特征选择",
      "authors": [
        "Xueyuan Xu",
        "Fulin Wei",
        "Tianyuan Jia",
        "Li Zhuo",
        "Feiping Nie",
        "Xia Wu"
      ],
      "abstract": "In the last decade, embedded multi-label feature selection methods,\nincorporating the search for feature subsets into model optimization, have\nattracted considerable attention in accurately evaluating the importance of\nfeatures in multi-label classification tasks. Nevertheless, the\nstate-of-the-art embedded multi-label feature selection algorithms based on\nleast square regression usually cannot preserve sufficient discriminative\ninformation in multi-label data. To tackle the aforementioned challenge, a\nnovel embedded multi-label feature selection method, termed global redundancy\nand relevance optimization in orthogonal regression (GRROOR), is proposed to\nfacilitate the multi-label feature selection. The method employs orthogonal\nregression with feature weighting to retain sufficient statistical and\nstructural information related to local label correlations of the multi-label\ndata in the feature learning process. Additionally, both global feature\nredundancy and global label relevancy information have been considered in the\northogonal regression model, which could contribute to the search for\ndiscriminative and non-redundant feature subsets in the multi-label data. The\ncost function of GRROOR is an unbalanced orthogonal Procrustes problem on the\nStiefel manifold. A simple yet effective scheme is utilized to obtain an\noptimal solution. Extensive experimental results on ten multi-label data sets\ndemonstrate the effectiveness of GRROOR.",
      "tldr_zh": "本文提出了一种新的嵌入式多-label feature selection 方法，名为 GRROOR，通过 orthogonal regression 结合特征加权，旨在解决现有基于最小二乘回归算法在多-label 数据中保留判别信息不足的问题。GRROOR 同时考虑全局特征冗余和标签相关性，以优化搜索判别性和非冗余的特征子集。成本函数被构建为不平衡的 orthogonal Procrustes 问题，在 Stiefel manifold 上求解，并采用简单有效的方案获得最优解。在十个多-label 数据集上的实验结果证明了 GRROOR 的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00307v1",
      "published_date": "2024-03-01 06:18:40 UTC",
      "updated_date": "2024-03-01 06:18:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:46:40.272027"
    },
    {
      "arxiv_id": "2403.00871v1",
      "title": "Teach LLMs to Phish: Stealing Private Information from Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ashwinee Panda",
        "Christopher A. Choquette-Choo",
        "Zhengming Zhang",
        "Yaoqing Yang",
        "Prateek Mittal"
      ],
      "abstract": "When large language models are trained on private data, it can be a\nsignificant privacy risk for them to memorize and regurgitate sensitive\ninformation. In this work, we propose a new practical data extraction attack\nthat we call \"neural phishing\". This attack enables an adversary to target and\nextract sensitive or personally identifiable information (PII), e.g., credit\ncard numbers, from a model trained on user data with upwards of 10% attack\nsuccess rates, at times, as high as 50%. Our attack assumes only that an\nadversary can insert as few as 10s of benign-appearing sentences into the\ntraining dataset using only vague priors on the structure of the user data.",
      "tldr_zh": "本文提出了一种名为“neural phishing”的新数据提取攻击方法，用于从训练有私人数据的语言模型（LLMs）中窃取敏感信息，如信用卡号码或个人信息（PII）。攻击者只需在训练数据集插入少量看似无害的句子（例如10句左右），并利用对用户数据结构模糊的先验知识，即可触发模型的记忆泄露。实验结果显示，该攻击的成功率可达10%以上，有时高达50%，突显了LLMs在隐私保护方面的重大风险。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.00871v1",
      "published_date": "2024-03-01 06:15:07 UTC",
      "updated_date": "2024-03-01 06:15:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:46:51.801897"
    },
    {
      "arxiv_id": "2403.00299v1",
      "title": "Universal Auto-encoder Framework for MIMO CSI Feedback",
      "title_zh": "MIMO CSI 反馈的",
      "authors": [
        "Jinhyun So",
        "Hyukjoon Kwon"
      ],
      "abstract": "Existing auto-encoder (AE)-based channel state information (CSI) frameworks\nhave focused on a specific configuration of user equipment (UE) and base\nstation (BS), and thus the input and output sizes of the AE are fixed. However,\nin the real-world scenario, the input and output sizes may vary depending on\nthe number of antennas of the BS and UE and the allocated resource block in the\nfrequency dimension. A naive approach to support the different input and output\nsizes is to use multiple AE models, which is impractical for the UE due to the\nlimited HW resources. In this paper, we propose a universal AE framework that\ncan support different input sizes and multiple compression ratios. The proposed\nAE framework significantly reduces the HW complexity while providing comparable\nperformance in terms of compression ratio-distortion trade-off compared to the\nnaive and state-of-the-art approaches.",
      "tldr_zh": "本文提出了一种通用 Auto-encoder (AE) 框架，用于 MIMO CSI 反馈，以解决现有框架因 UE 和 BS 配置变化而导致输入输出大小固定的问题。传统方法需使用多个 AE 模型，但这会增加 UE 的硬件资源负担。新的框架支持不同输入大小和多种压缩比率，同时显著降低硬件复杂度，并在压缩比率-失真权衡上提供与最先进方法相当的性能。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG",
        "eess.SP",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "7 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.00299v1",
      "published_date": "2024-03-01 05:57:08 UTC",
      "updated_date": "2024-03-01 05:57:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:47:03.366078"
    },
    {
      "arxiv_id": "2403.00290v1",
      "title": "Semantic Text Transmission via Prediction with Small Language Models: Cost-Similarity Trade-off",
      "title_zh": "通过小型语言模型预测实现的语义文本传输：成本-相似度权衡",
      "authors": [
        "Bhavani A Madhabhavi",
        "Gangadhar Karevvanavar",
        "Rajshekhar V Bhat",
        "Nikolaos Pappas"
      ],
      "abstract": "We consider the communication of natural language text from a source to a\ndestination over noiseless and character-erasure channels. We exploit\nlanguage's inherent correlations and predictability to constrain transmission\ncosts by allowing the destination to predict or complete words with potential\ndissimilarity with the source text. Concretely, our objective is to obtain\nachievable $(\\bar{c}, \\bar{s})$ pairs, where $\\bar{c}$ is the average\ntransmission cost at the source and $\\bar{s}$ is the average semantic\nsimilarity measured via cosine similarity between vector embedding of words at\nthe source and those predicted/completed at the destination. We obtain\n$(\\bar{c}, \\bar{s})$ pairs for neural language and first-order Markov\nchain-based small language models (SLM) for prediction, using both a threshold\npolicy that transmits a word if its cosine similarity with that\npredicted/completed at the destination is below a threshold, and a periodic\npolicy, which transmits words after a specific interval and predicts/completes\nthe words in between, at the destination. We adopt an SLM for word completion.\nWe demonstrate that, when communication occurs over a noiseless channel, the\nthreshold policy achieves a higher $\\bar{s}$ for a given $\\bar{c}$ than the\nperiodic policy and that the $\\bar{s}$ achieved with the neural SLM is greater\nthan or equal to that of the Markov chain-based algorithm for the same\n$\\bar{c}$. The improved performance comes with a higher complexity in terms of\ntime and computing requirements. However, when communication occurs over a\ncharacter-erasure channel, all prediction algorithms and scheduling policies\nperform poorly. Furthermore, if character-level Huffman coding is used, the\nrequired $\\bar{c}$ to achieve a given $\\bar{s}$ is reduced, but the above\nobservations still apply.",
      "tldr_zh": "这篇论文探讨了使用小型语言模型（Small Language Models, SLMs）进行语义文本传输的成本-相似度权衡（Cost-Similarity Trade-off），目标是通过预测或完成单词来最小化传输成本，同时保持平均语义相似度（\\(\\bar{s}\\)，基于单词嵌入的余弦相似度）。研究采用阈值策略（当预测单词的相似度低于阈值时传输）和周期策略（定期传输单词），并比较神经语言模型和一阶Markov链模型在无噪通道下的性能，结果显示阈值策略在给定平均传输成本（\\(\\bar{c}\\)) 时能实现更高的\\(\\bar{s}\\)，而神经SLM 优于Markov链模型，但需更高的计算复杂度。在字符擦除通道中，所有算法表现较差，使用字符级Huffman编码可降低所需的\\(\\bar{c}\\)，但总体趋势保持不变。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00290v1",
      "published_date": "2024-03-01 05:20:16 UTC",
      "updated_date": "2024-03-01 05:20:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:47:17.196704"
    },
    {
      "arxiv_id": "2403.00284v2",
      "title": "A Survey of Route Recommendations: Methods, Applications, and Opportunities",
      "title_zh": "路线推荐的综述：方法、应用和机会",
      "authors": [
        "Shiming Zhang",
        "Zhipeng Luo",
        "Li Yang",
        "Fei Teng",
        "Tianrui Li"
      ],
      "abstract": "Nowadays, with advanced information technologies deployed citywide, large\ndata volumes and powerful computational resources are intelligentizing modern\ncity development. As an important part of intelligent transportation, route\nrecommendation and its applications are widely used, directly influencing\ncitizens` travel habits. Developing smart and efficient travel routes based on\nbig data (possibly multi-modal) has become a central challenge in route\nrecommendation research. Our survey offers a comprehensive review of route\nrecommendation work based on urban computing. It is organized by the following\nthree parts: 1) Methodology-wise. We categorize a large volume of traditional\nmachine learning and modern deep learning methods. Also, we discuss their\nhistorical relations and reveal the edge-cutting progress. 2)\nApplication\\-wise. We present numerous novel applications related to route\ncommendation within urban computing scenarios. 3) We discuss current problems\nand challenges and envision several promising research directions. We believe\nthat this survey can help relevant researchers quickly familiarize themselves\nwith the current state of route recommendation research and then direct them to\nfuture research trends.",
      "tldr_zh": "这篇调查论文对基于城市计算的路线推荐研究进行了全面综述，聚焦于方法、应用和未来机会。论文将传统machine learning和现代deep learning方法分类，讨论了它们的历史关系和最新进展，并展示了在智能交通场景中的多种新型应用，如基于大数据的多模式路线优化。最终，它分析了当前问题和挑战，并展望了有前景的研究方向，以帮助相关研究者快速掌握领域现状并指导未来趋势。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.00284v2",
      "published_date": "2024-03-01 05:04:00 UTC",
      "updated_date": "2024-04-06 07:02:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:47:29.726884"
    },
    {
      "arxiv_id": "2403.00868v3",
      "title": "SoftTiger: A Clinical Foundation Model for Healthcare Workflows",
      "title_zh": "SoftTiger: 用于医疗保健工作流的临床基础模型",
      "authors": [
        "Ye Chen",
        "Igor Couto",
        "Wei Cai",
        "Cong Fu",
        "Bruno Dorneles"
      ],
      "abstract": "We introduce SoftTiger, a clinical large language model (CLaM) designed as a\nfoundation model for healthcare workflows. The narrative and unstructured\nnature of clinical notes is a major obstacle for healthcare intelligentization.\nWe address a critical problem of structuring clinical notes into clinical data,\naccording to international interoperability standards. We collect and annotate\ndata for three subtasks, namely, international patient summary, clinical\nimpression and medical encounter. We then supervised fine-tuned a\nstate-of-the-art LLM using public and credentialed clinical data. The training\nis orchestrated in a way that the target model can first support basic clinical\ntasks such as abbreviation expansion and temporal information extraction, and\nthen learn to perform more complex downstream clinical tasks. Moreover, we\naddress several modeling challenges in the healthcare context, e.g., extra long\ncontext window. Our blind pairwise evaluation shows that SoftTiger outperforms\nother popular open-source models and GPT-3.5, comparable to Gemini-pro, with a\nmild gap from GPT-4. We believe that LLMs may become a step-stone towards\nhealthcare digitalization and democratization. Therefore, we publicly release\nSoftTiger models at scales of 13 billion and 70 billion parameters, as well as\ndatasets and code for our innovative scalable evaluation, hopefully, making a\nsignificant contribution to the healthcare industry.",
      "tldr_zh": "我们介绍了SoftTiger，一种临床大语言模型(CLaM)，旨在作为医疗工作流程的基础模型，通过结构化临床笔记来解决其叙述性和非结构化问题，并符合国际互操作性标准。该模型通过收集并标注数据用于国际患者总结、临床印象和医疗就诊等子任务，并采用监督微调技术训练最先进的LLM，先处理基本任务如缩写扩展和时间信息提取，再扩展到复杂下游任务，同时应对医疗背景中的建模挑战如超长上下文窗口。实验结果显示，SoftTiger在盲配对评估中优于其他开源模型和GPT-3.5，与Gemini-pro相当，但略逊于GPT-4；我们公开发布了13亿和70亿参数的模型、数据集和代码，以推动医疗数字化和民主化。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to AAAI 2024 Spring Symposium on Clinical Foundation Models,\n  Stanford University, Stanford, California",
      "pdf_url": "http://arxiv.org/pdf/2403.00868v3",
      "published_date": "2024-03-01 04:39:16 UTC",
      "updated_date": "2024-08-20 12:37:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:47:42.167769"
    },
    {
      "arxiv_id": "2403.00254v1",
      "title": "Cloud-based Federated Learning Framework for MRI Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Rukesh Prajapati",
        "Amr S. El-Wakeel"
      ],
      "abstract": "In contemporary rural healthcare settings, the principal challenge in\ndiagnosing brain images is the scarcity of available data, given that most of\nthe existing deep learning models demand extensive training data to optimize\ntheir performance, necessitating centralized processing methods that\npotentially compromise data privacy. This paper proposes a novel framework\ntailored for brain tissue segmentation in rural healthcare facilities. The\nframework employs a deep reinforcement learning (DRL) environment in tandem\nwith a refinement model (RM) deployed locally at rural healthcare sites. The\nproposed DRL model has a reduced parameter count and practicality for\nimplementation across distributed rural sites. To uphold data privacy and\nenhance model generalization without transgressing privacy constraints, we\nemploy federated learning (FL) for cooperative model training. We demonstrate\nthe efficacy of our approach by training the network with a limited data set\nand observing a substantial performance enhancement, mitigating inaccuracies\nand irregularities in segmentation across diverse sites. Remarkably, the DRL\nmodel attains an accuracy of up to 80%, surpassing the capabilities of\nconventional convolutional neural networks when confronted with data\ninsufficiency. Incorporating our RM results in an additional accuracy\nimprovement of at least 10%, while FL contributes to a further accuracy\nenhancement of up to 5%. Collectively, the framework achieves an average 92%\naccuracy rate within rural healthcare settings characterized by data\nconstraints.",
      "tldr_zh": "该论文针对农村医疗环境中脑部图像数据稀缺的问题，提出了一种基于云的联邦学习(FL)框架，用于MRI分割，以保护数据隐私并提升模型泛化能力。框架结合深度强化学习(DRL)环境和精炼模型(RM)，在分布式农村站点进行本地部署，减少参数量并实现合作训练。实验结果显示，该方法在数据有限条件下，DRL模型达到80%准确率，添加RM后提高至少10%，FL进一步提升5%，最终实现平均92%的准确率，显著改善了脑组织分割的可靠性和准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00254v1",
      "published_date": "2024-03-01 03:39:17 UTC",
      "updated_date": "2024-03-01 03:39:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:47:54.115744"
    },
    {
      "arxiv_id": "2403.00252v2",
      "title": "EUROPA: A Legal Multilingual Keyphrase Generation Dataset",
      "title_zh": "EUROPA：法律多语言关键短语生成数据集",
      "authors": [
        "Olivier Salaün",
        "Frédéric Piedboeuf",
        "Guillaume Le Berre",
        "David Alfonso Hermelo",
        "Philippe Langlais"
      ],
      "abstract": "Keyphrase generation has primarily been explored within the context of\nacademic research articles, with a particular focus on scientific domains and\nthe English language. In this work, we present EUROPA, a dataset for\nmultilingual keyphrase generation in the legal domain. It is derived from legal\njudgments from the Court of Justice of the European Union (EU), and contains\ninstances in all 24 EU official languages. We run multilingual models on our\ncorpus and analyze the results, showing room for improvement on a\ndomain-specific multilingual corpus such as the one we present.",
      "tldr_zh": "这篇论文介绍了 EUROPA 数据集，这是一个针对法律领域的多语言关键短语 generation 数据集，用于扩展关键短语生成研究。数据集来源于欧盟法院的法律判决，涵盖 24 种欧盟官方语言，并针对多语言模型进行了实验分析。结果显示，在这种领域特定的多语言语料库上，模型性能仍有显著改进空间。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 2 figures, accepted at ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.00252v2",
      "published_date": "2024-03-01 03:30:38 UTC",
      "updated_date": "2024-06-14 13:51:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:48:05.851537"
    },
    {
      "arxiv_id": "2403.00867v3",
      "title": "Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaomeng Hu",
        "Pin-Yu Chen",
        "Tsung-Yi Ho"
      ],
      "abstract": "Large Language Models (LLMs) are becoming a prominent generative AI tool,\nwhere the user enters a query and the LLM generates an answer. To reduce harm\nand misuse, efforts have been made to align these LLMs to human values using\nadvanced training techniques such as Reinforcement Learning from Human Feedback\n(RLHF). However, recent studies have highlighted the vulnerability of LLMs to\nadversarial jailbreak attempts aiming at subverting the embedded safety\nguardrails. To address this challenge, this paper defines and investigates the\nRefusal Loss of LLMs and then proposes a method called Gradient Cuff to detect\njailbreak attempts. Gradient Cuff exploits the unique properties observed in\nthe refusal loss landscape, including functional values and its smoothness, to\ndesign an effective two-step detection strategy. Experimental results on two\naligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak\nattacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can\nsignificantly improve the LLM's rejection capability for malicious jailbreak\nqueries, while maintaining the model's performance for benign user queries by\nadjusting the detection threshold.",
      "tldr_zh": "这篇论文探讨了 Large Language Models (LLMs) 在面对 jailbreak attacks 时存在的脆弱性，并通过定义和调查 Refusal Loss 来提出 Gradient Cuff 方法，用于检测这些攻击。Gradient Cuff 利用 Refusal Loss 景观的函数值和光滑度属性，设计了一个两步检测策略，以提升模型的安全性。实验结果显示，在 LLaMA-2-7B-Chat 和 Vicuna-7B-V1.5 等模型上，对 GCG、AutoDAN、PAIR、TAP、Base64 和 LRL 等六种攻击类型，Gradient Cuff 显著提高了模型对恶意查询的拒绝能力，同时通过调整检测阈值保持了良性查询的性能。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by NeurIPS 2024. Project page:\n  https://huggingface.co/spaces/TrustSafeAI/GradientCuff-Jailbreak-Defense",
      "pdf_url": "http://arxiv.org/pdf/2403.00867v3",
      "published_date": "2024-03-01 03:29:54 UTC",
      "updated_date": "2024-11-07 15:41:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:48:20.329607"
    },
    {
      "arxiv_id": "2403.00250v1",
      "title": "Rethinking Classifier Re-Training in Long-Tailed Recognition: A Simple Logits Retargeting Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Han Lu",
        "Siyu Sun",
        "Yichen Xie",
        "Liqing Zhang",
        "Xiaokang Yang",
        "Junchi Yan"
      ],
      "abstract": "In the long-tailed recognition field, the Decoupled Training paradigm has\ndemonstrated remarkable capabilities among various methods. This paradigm\ndecouples the training process into separate representation learning and\nclassifier re-training. Previous works have attempted to improve both stages\nsimultaneously, making it difficult to isolate the effect of classifier\nre-training. Furthermore, recent empirical studies have demonstrated that\nsimple regularization can yield strong feature representations, emphasizing the\nneed to reassess existing classifier re-training methods. In this study, we\nrevisit classifier re-training methods based on a unified feature\nrepresentation and re-evaluate their performances. We propose a new metric\ncalled Logits Magnitude as a superior measure of model performance, replacing\nthe commonly used Weight Norm. However, since it is hard to directly optimize\nthe new metric during training, we introduce a suitable approximate invariant\ncalled Regularized Standard Deviation. Based on the two newly proposed metrics,\nwe prove that reducing the absolute value of Logits Magnitude when it is nearly\nbalanced can effectively decrease errors and disturbances during training,\nleading to better model performance. Motivated by these findings, we develop a\nsimple logits retargeting approach (LORT) without the requirement of prior\nknowledge of the number of samples per class. LORT divides the original one-hot\nlabel into small true label probabilities and large negative label\nprobabilities distributed across each class. Our method achieves\nstate-of-the-art performance on various imbalanced datasets, including\nCIFAR100-LT, ImageNet-LT, and iNaturalist2018.",
      "tldr_zh": "在长尾识别（Long-Tailed Recognition）领域，本文重新审视了 Decoupled Training 范式中分类器再训练的方法，强调了隔离其效果的重要性，并证明简单正则化能产生强表示学习。作者提出 Logits Magnitude 作为更优的模型性能指标，取代传统的 Weight Norm，并引入 Regularized Standard Deviation 作为近似不变指标，以减少训练中的错误和干扰。基于这些发现，开发了简单有效的 LORT（Logits Retargeting）方法，该方法将 one-hot 标签分解为小的真实标签概率和大的负标签概率分布，而无需事先知道每个类别的样本数。在 CIFAR100-LT、ImageNet-LT 和 iNaturalist2018 等不平衡数据集上，LORT 达到了最先进性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00250v1",
      "published_date": "2024-03-01 03:27:08 UTC",
      "updated_date": "2024-03-01 03:27:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:48:33.100285"
    },
    {
      "arxiv_id": "2403.00236v1",
      "title": "Benchmarking zero-shot stance detection with FlanT5-XXL: Insights from training data, prompting, and decoding strategies into its near-SoTA performance",
      "title_zh": "翻译失败",
      "authors": [
        "Rachith Aiyappa",
        "Shruthi Senthilmani",
        "Jisun An",
        "Haewoon Kwak",
        "Yong-Yeol Ahn"
      ],
      "abstract": "We investigate the performance of LLM-based zero-shot stance detection on\ntweets. Using FlanT5-XXL, an instruction-tuned open-source LLM, with the\nSemEval 2016 Tasks 6A, 6B, and P-Stance datasets, we study the performance and\nits variations under different prompts and decoding strategies, as well as the\npotential biases of the model. We show that the zero-shot approach can match or\noutperform state-of-the-art benchmarks, including fine-tuned models. We provide\nvarious insights into its performance including the sensitivity to instructions\nand prompts, the decoding strategies, the perplexity of the prompts, and to\nnegations and oppositions present in prompts. Finally, we ensure that the LLM\nhas not been trained on test datasets, and identify a positivity bias which may\npartially explain the performance differences across decoding strategie",
      "tldr_zh": "本文评估了使用 FlanT5-XXL 模型进行零样本立场检测（zero-shot stance detection）的性能，基于 SemEval 2016 Tasks 6A、6B 和 P-Stance 数据集，探讨了不同提示（prompting）和解码策略（decoding strategies）对模型的影响。研究发现，该零样本方法能匹敌或超越最先进（SoTA）基准，包括微调模型，并揭示了模型对指令的敏感性、提示的困惑度（perplexity）以及对否定和对立元素的响应。最终，论文确认模型未在测试数据上训练，并识别了积极偏差（positivity bias），这可能解释了性能差异。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00236v1",
      "published_date": "2024-03-01 02:33:26 UTC",
      "updated_date": "2024-03-01 02:33:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:48:46.267138"
    },
    {
      "arxiv_id": "2403.00865v1",
      "title": "Fast and Efficient Local Search for Genetic Programming Based Loss Function Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Christian Raymond",
        "Qi Chen",
        "Bing Xue",
        "Mengjie Zhang"
      ],
      "abstract": "In this paper, we develop upon the topic of loss function learning, an\nemergent meta-learning paradigm that aims to learn loss functions that\nsignificantly improve the performance of the models trained under them.\nSpecifically, we propose a new meta-learning framework for task and\nmodel-agnostic loss function learning via a hybrid search approach. The\nframework first uses genetic programming to find a set of symbolic loss\nfunctions. Second, the set of learned loss functions is subsequently\nparameterized and optimized via unrolled differentiation. The versatility and\nperformance of the proposed framework are empirically validated on a diverse\nset of supervised learning tasks. Results show that the learned loss functions\nbring improved convergence, sample efficiency, and inference performance on\ntabulated, computer vision, and natural language processing problems, using a\nvariety of task-specific neural network architectures.",
      "tldr_zh": "本篇论文提出了一种新的 meta-learning 框架，用于 task 和 model-agnostic 的 loss function learning，通过混合搜索方法显著提升模型性能。该框架首先利用 genetic programming 生成一组 symbolic loss functions，然后通过 unrolled differentiation 进行参数化和优化。实验在多样化的监督学习任务上验证了其有效性，包括表格数据、计算机视觉和自然语言处理问题，使用各种神经网络架构后，学到的 loss functions 改善了模型的收敛性、样本效率和推理性能。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2209.08907",
      "pdf_url": "http://arxiv.org/pdf/2403.00865v1",
      "published_date": "2024-03-01 02:20:04 UTC",
      "updated_date": "2024-03-01 02:20:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:48:55.258201"
    },
    {
      "arxiv_id": "2403.00225v3",
      "title": "Robust Policy Learning via Offline Skill Diffusion",
      "title_zh": "基于离线技能扩散的鲁棒策略学习",
      "authors": [
        "Woo Kyung Kim",
        "Minjong Yoo",
        "Honguk Woo"
      ],
      "abstract": "Skill-based reinforcement learning (RL) approaches have shown considerable\npromise, especially in solving long-horizon tasks via hierarchical structures.\nThese skills, learned task-agnostically from offline datasets, can accelerate\nthe policy learning process for new tasks. Yet, the application of these skills\nin different domains remains restricted due to their inherent dependency on the\ndatasets, which poses a challenge when attempting to learn a skill-based policy\nvia RL for a target domain different from the datasets' domains. In this paper,\nwe present a novel offline skill learning framework DuSkill which employs a\nguided Diffusion model to generate versatile skills extended from the limited\nskills in datasets, thereby enhancing the robustness of policy learning for\ntasks in different domains. Specifically, we devise a guided diffusion-based\nskill decoder in conjunction with the hierarchical encoding to disentangle the\nskill embedding space into two distinct representations, one for encapsulating\ndomain-invariant behaviors and the other for delineating the factors that\ninduce domain variations in the behaviors. Our DuSkill framework enhances the\ndiversity of skills learned offline, thus enabling to accelerate the learning\nprocedure of high-level policies for different domains. Through experiments, we\nshow that DuSkill outperforms other skill-based imitation learning and RL\nalgorithms for several long-horizon tasks, demonstrating its benefits in\nfew-shot imitation and online RL.",
      "tldr_zh": "本研究提出了一种名为 DuSkill 的离线技能学习框架，旨在解决技能-based reinforcement learning (RL) 在不同领域应用受限于数据集的问题，通过 guided Diffusion model 生成更多样化的技能，从而提升政策学习的鲁棒性。具体而言，DuSkill 采用 guided diffusion-based skill decoder 和 hierarchical encoding，将技能嵌入空间分解为 domain-invariant behaviors 和 domain variations 的独立表示，增强技能的多样性和适应性。实验结果显示，该框架在多个长-horizon 任务上优于其他技能-based imitation learning 和 RL 算法，尤其在 few-shot imitation 和 online RL 场景中加速了学习过程。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 6 figures; Accepted for AAAI Conference on Artificial\n  Intelligence (AAAI 2024); Published version",
      "pdf_url": "http://arxiv.org/pdf/2403.00225v3",
      "published_date": "2024-03-01 02:00:44 UTC",
      "updated_date": "2024-08-22 04:03:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:49:05.936478"
    },
    {
      "arxiv_id": "2403.15408v1",
      "title": "Multi-modal Heart Failure Risk Estimation based on Short ECG and Sampled Long-Term HRV",
      "title_zh": "基于短 ECG 和采样",
      "authors": [
        "Sergio González",
        "Abel Ko-Chun Yi",
        "Wan-Ting Hsieh",
        "Wei-Chao Chen",
        "Chun-Li Wang",
        "Victor Chien-Chia Wu",
        "Shang-Hung Chang"
      ],
      "abstract": "Cardiovascular diseases, including Heart Failure (HF), remain a leading\nglobal cause of mortality, often evading early detection. In this context,\naccessible and effective risk assessment is indispensable. Traditional\napproaches rely on resource-intensive diagnostic tests, typically administered\nafter the onset of symptoms. The widespread availability of electrocardiogram\n(ECG) technology and the power of Machine Learning are emerging as viable\nalternatives within smart healthcare. In this paper, we propose several\nmulti-modal approaches that combine 30-second ECG recordings and approximate\nlong-term Heart Rate Variability (HRV) data to estimate the risk of HF\nhospitalization. We introduce two survival models: an XGBoost model with\nAccelerated Failure Time (AFT) incorporating comprehensive ECG features and a\nResNet model that learns from the raw ECG. We extend these with our novel\nlong-term HRVs extracted from the combination of ultra-short-term beat-to-beat\nmeasurements taken over the day. To capture their temporal dynamics, we propose\na survival model comprising ResNet and Transformer architectures (TFM-ResNet).\nOur experiments demonstrate high model performance for HF risk assessment with\na concordance index of 0.8537 compared to 14 survival models and competitive\ndiscrimination power on various external ECG datasets. After transferability\ntests with Apple Watch data, our approach implemented in the myHeartScore App\noffers cost-effective and highly accessible HF risk assessment, contributing to\nits prevention and management.",
      "tldr_zh": "本研究针对心力衰竭（HF）早期检测的挑战，提出多模态方法，利用30秒ECG记录和采样长期心率变异性（HRV）数据来估计HF住院风险。方法包括两个生存模型：XGBoost结合Accelerated Failure Time (AFT)使用全面ECG特征，以及ResNet直接从原始ECG学习；此外，引入新型TFM-ResNet模型，通过ResNet和Transformer架构捕捉HRV的时间动态。实验结果显示，该方法在HF风险评估中达到0.8537的协调指数（concordance index），优于14个基准模型，并在外部ECG数据集上表现出色，并通过Apple Watch数据验证，可在myHeartScore App中实现经济高效的风险评估和预防。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.15408v1",
      "published_date": "2024-03-01 01:16:27 UTC",
      "updated_date": "2024-03-01 01:16:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:49:20.358130"
    },
    {
      "arxiv_id": "2403.00198v1",
      "title": "AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs",
      "title_zh": "翻译失败",
      "authors": [
        "Sana Ebrahimi",
        "Kaiwen Chen",
        "Abolfazl Asudeh",
        "Gautam Das",
        "Nick Koudas"
      ],
      "abstract": "Pre-trained Large Language Models (LLMs) have significantly advanced natural\nlanguage processing capabilities but are susceptible to biases present in their\ntraining data, leading to unfair outcomes in various applications. While\nnumerous strategies have been proposed to mitigate bias, they often require\nextensive computational resources and may compromise model performance. In this\nwork, we introduce AXOLOTL, a novel post-processing framework, which operates\nagnostically across tasks and models, leveraging public APIs to interact with\nLLMs without direct access to internal parameters. Through a three-step process\nresembling zero-shot learning, AXOLOTL identifies biases, proposes resolutions,\nand guides the model to self-debias its outputs. This approach minimizes\ncomputational costs and preserves model performance, making AXOLOTL a promising\ntool for debiasing LLM outputs with broad applicability and ease of use.",
      "tldr_zh": "预训练的 Large Language Models (LLMs) 容易因训练数据中的偏见而产生不公平输出，现有的缓解策略往往需要大量计算资源并可能损害模型性能。本文提出 AXOLOTL，一种新型的后处理框架，通过利用公共 API 和一个三步过程（类似于 zero-shot learning）来识别偏见、提出解决方案并引导模型进行自我去偏。该框架在任务和模型上均具有通用性，能最小化计算成本、保持模型性能，并作为一种易用工具广泛适用于去偏场景。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.00198v1",
      "published_date": "2024-03-01 00:02:37 UTC",
      "updated_date": "2024-03-01 00:02:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T11:49:35.812985"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 81,
  "processed_papers_count": 81,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T11:50:03.249410"
}
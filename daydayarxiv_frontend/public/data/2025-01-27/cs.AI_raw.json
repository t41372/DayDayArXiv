[
  {
    "arxiv_id": "2501.16577v1",
    "title": "Generative AI Uses and Risks for Knowledge Workers in a Science Organization",
    "authors": [
      "Kelly B. Wagman",
      "Matthew T. Dearing",
      "Marshini Chetty"
    ],
    "abstract": "Generative AI could enhance scientific discovery by supporting knowledge\nworkers in science organizations. However, the real-world applications and\nperceived concerns of generative AI use in these organizations are uncertain.\nIn this paper, we report on a collaborative study with a US national laboratory\nwith employees spanning Science and Operations about their use of generative AI\ntools. We surveyed 66 employees, interviewed a subset (N=22), and measured\nearly adoption of an internal generative AI interface called Argo lab-wide. We\nhave four findings: (1) Argo usage data shows small but increasing use by\nScience and Operations employees; Common current and envisioned use cases for\ngenerative AI in this context conceptually fall into either a (2) copilot or\n(3) workflow agent modality; and (4) Concerns include sensitive data security,\nacademic publishing, and job impacts. Based on our findings, we make\nrecommendations for generative AI use in science and other organizations.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "CHI Conference on Human Factors in Computing Systems (CHI '25)",
    "pdf_url": "http://arxiv.org/pdf/2501.16577v1",
    "published_date": "2025-01-27 23:41:13 UTC",
    "updated_date": "2025-01-27 23:41:13 UTC"
  },
  {
    "arxiv_id": "2502.15721v1",
    "title": "iTRI-QA: a Toolset for Customized Question-Answer Dataset Generation Using Language Models for Enhanced Scientific Research",
    "authors": [
      "Qiming Liu",
      "Zhongzheng Niu",
      "Siting Liu",
      "Mao Tian"
    ],
    "abstract": "The exponential growth of AI in science necessitates efficient and scalable\nsolutions for retrieving and preserving research information. Here, we present\na tool for the development of a customized question-answer (QA) dataset, called\nInteractive Trained Research Innovator (iTRI) - QA, tailored for the needs of\nresearchers leveraging language models (LMs) to retrieve scientific knowledge\nin a QA format. Our approach integrates curated QA datasets with a specialized\nresearch paper dataset to enhance responses' contextual relevance and accuracy\nusing fine-tuned LM. The framework comprises four key steps: (1) the generation\nof high-quality and human-generated QA examples, (2) the creation of a\nstructured research paper database, (3) the fine-tuning of LMs using\ndomain-specific QA examples, and (4) the generation of QA dataset that align\nwith user queries and the curated database. This pipeline provides a dynamic\nand domain-specific QA system that augments the utility of LMs in academic\nresearch that will be applied for future research LM deployment. We demonstrate\nthe feasibility and scalability of our tool for streamlining knowledge\nretrieval in scientific contexts, paving the way for its integration into\nbroader multi-disciplinary applications.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.DL"
    ],
    "primary_category": "cs.IR",
    "comment": "13 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.15721v1",
    "published_date": "2025-01-27 23:38:39 UTC",
    "updated_date": "2025-01-27 23:38:39 UTC"
  },
  {
    "arxiv_id": "2501.16571v1",
    "title": "Efficient Object Detection of Marine Debris using Pruned YOLO Model",
    "authors": [
      "Abi Aryaza",
      "Novanto Yudistira",
      "Tibyani"
    ],
    "abstract": "Marine debris poses significant harm to marine life due to substances like\nmicroplastics, polychlorinated biphenyls, and pesticides, which damage habitats\nand poison organisms. Human-based solutions, such as diving, are increasingly\nineffective in addressing this issue. Autonomous underwater vehicles (AUVs) are\nbeing developed for efficient sea garbage collection, with the choice of object\ndetection architecture being critical. This research employs the YOLOv4 model\nfor real-time detection of marine debris using the Trash-ICRA 19 dataset,\nconsisting of 7683 images at 480x320 pixels. Various modifications-pretrained\nmodels, training from scratch, mosaic augmentation, layer freezing,\nYOLOv4-tiny, and channel pruning-are compared to enhance architecture\nefficiency. Channel pruning significantly improves detection speed, increasing\nthe base YOLOv4 frame rate from 15.19 FPS to 19.4 FPS, with only a 1.2% drop in\nmean Average Precision, from 97.6% to 96.4%.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16571v1",
    "published_date": "2025-01-27 23:31:39 UTC",
    "updated_date": "2025-01-27 23:31:39 UTC"
  },
  {
    "arxiv_id": "2501.16551v1",
    "title": "PackDiT: Joint Human Motion and Text Generation via Mutual Prompting",
    "authors": [
      "Zhongyu Jiang",
      "Wenhao Chai",
      "Zhuoran Zhou",
      "Cheng-Yen Yang",
      "Hsiang-Wei Huang",
      "Jenq-Neng Hwang"
    ],
    "abstract": "Human motion generation has advanced markedly with the advent of diffusion\nmodels. Most recent studies have concentrated on generating motion sequences\nbased on text prompts, commonly referred to as text-to-motion generation.\nHowever, the bidirectional generation of motion and text, enabling tasks such\nas motion-to-text alongside text-to-motion, has been largely unexplored. This\ncapability is essential for aligning diverse modalities and supports\nunconditional generation. In this paper, we introduce PackDiT, the first\ndiffusion-based generative model capable of performing various tasks\nsimultaneously, including motion generation, motion prediction, text\ngeneration, text-to-motion, motion-to-text, and joint motion-text generation.\nOur core innovation leverages mutual blocks to integrate multiple diffusion\ntransformers (DiTs) across different modalities seamlessly. We train PackDiT on\nthe HumanML3D dataset, achieving state-of-the-art text-to-motion performance\nwith an FID score of 0.106, along with superior results in motion prediction\nand in-between tasks. Our experiments further demonstrate that diffusion models\nare effective for motion-to-text generation, achieving performance comparable\nto that of autoregressive models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16551v1",
    "published_date": "2025-01-27 22:51:45 UTC",
    "updated_date": "2025-01-27 22:51:45 UTC"
  },
  {
    "arxiv_id": "2501.16546v1",
    "title": "Sample-Efficient Behavior Cloning Using General Domain Knowledge",
    "authors": [
      "Feiyu Zhu",
      "Jean Oh",
      "Reid Simmons"
    ],
    "abstract": "Behavior cloning has shown success in many sequential decision-making tasks\nby learning from expert demonstrations, yet they can be very sample inefficient\nand fail to generalize to unseen scenarios. One approach to these problems is\nto introduce general domain knowledge, such that the policy can focus on the\nessential features and may generalize to unseen states by applying that\nknowledge. Although this knowledge is easy to acquire from the experts, it is\nhard to be combined with learning from individual examples due to the lack of\nsemantic structure in neural networks and the time-consuming nature of feature\nengineering. To enable learning from both general knowledge and specific\ndemonstration trajectories, we use a large language model's coding capability\nto instantiate a policy structure based on expert domain knowledge expressed in\nnatural language and tune the parameters in the policy with demonstrations. We\nname this approach the Knowledge Informed Model (KIM) as the structure reflects\nthe semantics of expert knowledge. In our experiments with lunar lander and car\nracing tasks, our approach learns to solve the tasks with as few as 5\ndemonstrations and is robust to action noise, outperforming the baseline model\nwithout domain knowledge. This indicates that with the help of large language\nmodels, we can incorporate domain knowledge into the structure of the policy,\nincreasing sample efficiency for behavior cloning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16546v1",
    "published_date": "2025-01-27 22:40:11 UTC",
    "updated_date": "2025-01-27 22:40:11 UTC"
  },
  {
    "arxiv_id": "2501.16539v1",
    "title": "Generalized Mission Planning for Heterogeneous Multi-Robot Teams via LLM-constructed Hierarchical Trees",
    "authors": [
      "Piyush Gupta",
      "David Isele",
      "Enna Sachdeva",
      "Pin-Hao Huang",
      "Behzad Dariush",
      "Kwonjoon Lee",
      "Sangjae Bae"
    ],
    "abstract": "We present a novel mission-planning strategy for heterogeneous multi-robot\nteams, taking into account the specific constraints and capabilities of each\nrobot. Our approach employs hierarchical trees to systematically break down\ncomplex missions into manageable sub-tasks. We develop specialized APIs and\ntools, which are utilized by Large Language Models (LLMs) to efficiently\nconstruct these hierarchical trees. Once the hierarchical tree is generated, it\nis further decomposed to create optimized schedules for each robot, ensuring\nadherence to their individual constraints and capabilities. We demonstrate the\neffectiveness of our framework through detailed examples covering a wide range\nof missions, showcasing its flexibility and scalability.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16539v1",
    "published_date": "2025-01-27 22:20:48 UTC",
    "updated_date": "2025-01-27 22:20:48 UTC"
  },
  {
    "arxiv_id": "2501.17201v1",
    "title": "Smart Cubing for Graph Search: A Comparative Study",
    "authors": [
      "Markus Kirchweger",
      "Hai Xia",
      "Tomáš Peitl",
      "Stefan Szeider"
    ],
    "abstract": "Parallel solving via cube-and-conquer is a key method for scaling SAT solvers\nto hard instances. While cube-and-conquer has proven successful for pure SAT\nproblems, notably the Pythagorean triples conjecture, its application to SAT\nsolvers extended with propagators presents unique challenges, as these\npropagators learn constraints dynamically during the search.\n  We study this problem using SAT Modulo Symmetries (SMS) as our primary test\ncase, where a symmetry-breaking propagator reduces the search space by learning\nconstraints that eliminate isomorphic graphs. Through extensive experimentation\ncomprising over 10,000 CPU hours, we systematically evaluate different\ncube-and-conquer variants on three well-studied combinatorial problems. Our\nmethodology combines prerun phases to collect learned constraints, various\ncubing strategies, and parameter tuning via algorithm configuration and\nLLM-generated design suggestions.\n  The comprehensive empirical evaluation provides new insights into effective\ncubing strategies for propagator-based SAT solving, with our best method\nachieving speedups of 2-3x from improved cubing and parameter tuning, providing\nan additional 1.5-2x improvement on harder instances.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.17201v1",
    "published_date": "2025-01-27 22:15:54 UTC",
    "updated_date": "2025-01-27 22:15:54 UTC"
  },
  {
    "arxiv_id": "2501.16534v1",
    "title": "Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs",
    "authors": [
      "Jean-Charles Noirot Ferrand",
      "Yohan Beugin",
      "Eric Pauley",
      "Ryan Sheatsley",
      "Patrick McDaniel"
    ],
    "abstract": "Alignment in large language models (LLMs) is used to enforce guidelines such\nas safety. Yet, alignment fails in the face of jailbreak attacks that modify\ninputs to induce unsafe outputs. In this paper, we present and evaluate a\nmethod to assess the robustness of LLM alignment. We observe that alignment\nembeds a safety classifier in the target model that is responsible for deciding\nbetween refusal and compliance. We seek to extract an approximation of this\nclassifier, called a surrogate classifier, from the LLM. We develop an\nalgorithm for identifying candidate classifiers from subsets of the LLM model.\nWe evaluate the degree to which the candidate classifiers approximate the\nmodel's embedded classifier in benign (F1 score) and adversarial (using\nsurrogates in a white-box attack) settings. Our evaluation shows that the best\ncandidates achieve accurate agreement (an F1 score above 80%) using as little\nas 20% of the model architecture. Further, we find attacks mounted on the\nsurrogate models can be transferred with high accuracy. For example, a\nsurrogate using only 50% of the Llama 2 model achieved an attack success rate\n(ASR) of 70%, a substantial improvement over attacking the LLM directly, where\nwe only observed a 22% ASR. These results show that extracting surrogate\nclassifiers is a viable (and highly effective) means for modeling (and therein\naddressing) the vulnerability of aligned models to jailbreaking attacks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16534v1",
    "published_date": "2025-01-27 22:13:05 UTC",
    "updated_date": "2025-01-27 22:13:05 UTC"
  },
  {
    "arxiv_id": "2501.16525v1",
    "title": "Multi-Objective Deep-Learning-based Biomechanical Deformable Image Registration with MOREA",
    "authors": [
      "Georgios Andreadis",
      "Eduard Ruiz Munné",
      "Thomas H. W. Bäck",
      "Peter A. N. Bosman",
      "Tanja Alderliesten"
    ],
    "abstract": "When choosing a deformable image registration (DIR) approach for images with\nlarge deformations and content mismatch, the realism of found transformations\noften needs to be traded off against the required runtime. DIR approaches using\ndeep learning (DL) techniques have shown remarkable promise in instantly\npredicting a transformation. However, on difficult registration problems, the\nrealism of these transformations can fall short. DIR approaches using\nbiomechanical, finite element modeling (FEM) techniques can find more realistic\ntransformations, but tend to require much longer runtimes. This work proposes\nthe first hybrid approach to combine them, with the aim of getting the best of\nboth worlds. This hybrid approach, called DL-MOREA, combines a recently\nintroduced multi-objective DL-based DIR approach which leverages the VoxelMorph\nframework, called DL-MODIR, with MOREA, an evolutionary algorithm-based,\nmulti-objective DIR approach in which a FEM-like biomechanical mesh\ntransformation model is used. In our proposed hybrid approach, the DL results\nare used to smartly initialize MOREA, with the aim of more efficiently\noptimizing its mesh transformation model. We empirically compare DL-MOREA\nagainst its components, DL-MODIR and MOREA, on CT scan pairs capturing large\nbladder filling differences of 15 cervical cancer patients. While MOREA\nrequires a median runtime of 45 minutes, DL-MOREA can already find high-quality\ntransformations after 5 minutes. Compared to the DL-MODIR transformations, the\ntransformations found by DL-MOREA exhibit far less folding and improve or\npreserve the bladder contour distance error.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "Pre-print for the SPIE Medical Imaging: Image Processing Conference",
    "pdf_url": "http://arxiv.org/pdf/2501.16525v1",
    "published_date": "2025-01-27 21:50:12 UTC",
    "updated_date": "2025-01-27 21:50:12 UTC"
  },
  {
    "arxiv_id": "2501.16516v1",
    "title": "How well can LLMs Grade Essays in Arabic?",
    "authors": [
      "Rayed Ghazawi",
      "Edwin Simpson"
    ],
    "abstract": "This research assesses the effectiveness of state-of-the-art large language\nmodels (LLMs), including ChatGPT, Llama, Aya, Jais, and ACEGPT, in the task of\nArabic automated essay scoring (AES) using the AR-AES dataset. It explores\nvarious evaluation methodologies, including zero-shot, few-shot in-context\nlearning, and fine-tuning, and examines the influence of instruction-following\ncapabilities through the inclusion of marking guidelines within the prompts. A\nmixed-language prompting strategy, integrating English prompts with Arabic\ncontent, was implemented to improve model comprehension and performance. Among\nthe models tested, ACEGPT demonstrated the strongest performance across the\ndataset, achieving a Quadratic Weighted Kappa (QWK) of 0.67, but was\noutperformed by a smaller BERT-based model with a QWK of 0.88. The study\nidentifies challenges faced by LLMs in processing Arabic, including\ntokenization complexities and higher computational demands. Performance\nvariation across different courses underscores the need for adaptive models\ncapable of handling diverse assessment formats and highlights the positive\nimpact of effective prompt engineering on improving LLM outputs. To the best of\nour knowledge, this study is the first to empirically evaluate the performance\nof multiple generative Large Language Models (LLMs) on Arabic essays using\nauthentic student data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.16516v1",
    "published_date": "2025-01-27 21:30:02 UTC",
    "updated_date": "2025-01-27 21:30:02 UTC"
  },
  {
    "arxiv_id": "2501.17200v1",
    "title": "Improving LLM Leaderboards with Psychometrical Methodology",
    "authors": [
      "Denis Federiakin"
    ],
    "abstract": "The rapid development of large language models (LLMs) has necessitated the\ncreation of benchmarks to evaluate their performance. These benchmarks resemble\nhuman tests and surveys, as they consist of sets of questions designed to\nmeasure emergent properties in the cognitive behavior of these systems.\nHowever, unlike the well-defined traits and abilities studied in social\nsciences, the properties measured by these benchmarks are often vaguer and less\nrigorously defined. The most prominent benchmarks are often grouped into\nleaderboards for convenience, aggregating performance metrics and enabling\ncomparisons between models. Unfortunately, these leaderboards typically rely on\nsimplistic aggregation methods, such as taking the average score across\nbenchmarks. In this paper, we demonstrate the advantages of applying\ncontemporary psychometric methodologies - originally developed for human tests\nand surveys - to improve the ranking of large language models on leaderboards.\nUsing data from the Hugging Face Leaderboard as an example, we compare the\nresults of the conventional naive ranking approach with a psychometrically\ninformed ranking. The findings highlight the benefits of adopting psychometric\ntechniques for more robust and meaningful evaluation of LLM performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.CL",
    "comment": "53 pages, 10 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.17200v1",
    "published_date": "2025-01-27 21:21:46 UTC",
    "updated_date": "2025-01-27 21:21:46 UTC"
  },
  {
    "arxiv_id": "2501.16510v1",
    "title": "Decrypting the temperature field in flow boiling with latent diffusion models",
    "authors": [
      "UngJin Na",
      "JunYoung Seo",
      "Taeil Kim",
      "ByongGuk Jeon",
      "HangJin Jo"
    ],
    "abstract": "This paper presents an innovative method using Latent Diffusion Models (LDMs)\nto generate temperature fields from phase indicator maps. By leveraging the\nBubbleML dataset from numerical simulations, the LDM translates phase field\ndata into corresponding temperature distributions through a two-stage training\nprocess involving a vector-quantized variational autoencoder (VQVAE) and a\ndenoising autoencoder. The resulting model effectively reconstructs complex\ntemperature fields at interfaces. Spectral analysis indicates a high degree of\nagreement with ground truth data in the low to mid wavenumber ranges, even\nthough some inconsistencies are observed at higher wavenumbers, suggesting\nareas for further enhancement. This machine learning approach significantly\nreduces the computational burden of traditional simulations and improves the\nprecision of experimental calibration methods. Future work will focus on\nrefining the model's ability to represent small-scale turbulence and expanding\nits applicability to a broader range of boiling conditions.",
    "categories": [
      "physics.flu-dyn",
      "cs.AI"
    ],
    "primary_category": "physics.flu-dyn",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16510v1",
    "published_date": "2025-01-27 21:18:05 UTC",
    "updated_date": "2025-01-27 21:18:05 UTC"
  },
  {
    "arxiv_id": "2501.16509v1",
    "title": "Reinforcement Learning for Quantum Circuit Design: Using Matrix Representations",
    "authors": [
      "Zhiyuan Wang",
      "Chunlin Feng",
      "Christopher Poon",
      "Lijian Huang",
      "Xingjian Zhao",
      "Yao Ma",
      "Tianfan Fu",
      "Xiao-Yang Liu"
    ],
    "abstract": "Quantum computing promises advantages over classical computing. The\nmanufacturing of quantum hardware is in the infancy stage, called the Noisy\nIntermediate-Scale Quantum (NISQ) era. A major challenge is automated quantum\ncircuit design that map a quantum circuit to gates in a universal gate set. In\nthis paper, we present a generic MDP modeling and employ Q-learning and DQN\nalgorithms for quantum circuit design. By leveraging the power of deep\nreinforcement learning, we aim to provide an automatic and scalable approach\nover traditional hand-crafted heuristic methods.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16509v1",
    "published_date": "2025-01-27 21:17:58 UTC",
    "updated_date": "2025-01-27 21:17:58 UTC"
  },
  {
    "arxiv_id": "2501.16507v1",
    "title": "Characterizing Network Structure of Anti-Trans Actors on TikTok",
    "authors": [
      "Maxyn Leitner",
      "Rebecca Dorn",
      "Fred Morstatter",
      "Kristina Lerman"
    ],
    "abstract": "The recent proliferation of short form video social media sites such as\nTikTok has been effectively utilized for increased visibility, communication,\nand community connection amongst trans/nonbinary creators online. However,\nthese same platforms have also been exploited by right-wing actors targeting\ntrans/nonbinary people, enabling such anti-trans actors to efficiently spread\nhate speech and propaganda. Given these divergent groups, what are the\ndifferences in network structure between anti-trans and pro-trans communities\non TikTok, and to what extent do they amplify the effects of anti-trans\ncontent? In this paper, we collect a sample of TikTok videos containing pro and\nanti-trans content, and develop a taxonomy of trans related sentiment to enable\nthe classification of content on TikTok, and ultimately analyze the reply\nnetwork structures of pro-trans and anti-trans communities. In order to\naccomplish this, we worked with hired expert data annotators from the\ntrans/nonbinary community in order to generate a sample of highly accurately\nlabeled data. From this subset, we utilized a novel classification pipeline\nleveraging Retrieval-Augmented Generation (RAG) with annotated examples and\ntaxonomy definitions to classify content into pro-trans, anti-trans, or neutral\ncategories. We find that incorporating our taxonomy and its logics into our\nclassification engine results in improved ability to differentiate trans\nrelated content, and that Results from network analysis indicate many\ninteractions between posters of pro-trans and anti-trans content exist, further\ndemonstrating targeting of trans individuals, and demonstrating the need for\nbetter content moderation tools",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SI",
      "I.2.7; J.4; H.3.3; K.4.2"
    ],
    "primary_category": "cs.HC",
    "comment": "11 pages, 4 figures. 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.16507v1",
    "published_date": "2025-01-27 21:14:18 UTC",
    "updated_date": "2025-01-27 21:14:18 UTC"
  },
  {
    "arxiv_id": "2501.16504v1",
    "title": "Digital Twin Enabled Site Specific Channel Precoding: Over the Air CIR Inference",
    "authors": [
      "Majumder Haider",
      "Imtiaz Ahmed",
      "Zoheb Hassan",
      "Timothy J. O'Shea",
      "Lingjia Liu",
      "Danda B. Rawat"
    ],
    "abstract": "This paper investigates the significance of designing a reliable,\nintelligent, and true physical environment-aware precoding scheme by leveraging\nan accurately designed channel twin model to obtain realistic channel state\ninformation (CSI) for cellular communication systems. Specifically, we propose\na fine-tuned multi-step channel twin design process that can render CSI very\nclose to the CSI of the actual environment. After generating a precise CSI, we\nexecute precoding using the obtained CSI at the transmitter end. We demonstrate\na two-step parameters' tuning approach to design channel twin by ray tracing\n(RT) emulation, then further fine-tuning of CSI by employing an artificial\nintelligence (AI) based algorithm can significantly reduce the gap between\nactual CSI and the fine-tuned digital twin (DT) rendered CSI. The simulation\nresults show the effectiveness of the proposed novel approach in designing a\ntrue physical environment-aware channel twin model.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16504v1",
    "published_date": "2025-01-27 21:10:07 UTC",
    "updated_date": "2025-01-27 21:10:07 UTC"
  },
  {
    "arxiv_id": "2501.16497v1",
    "title": "Smoothed Embeddings for Robust Language Models",
    "authors": [
      "Ryo Hase",
      "Md Rafi Ur Rashid",
      "Ashley Lewis",
      "Jing Liu",
      "Toshiaki Koike-Akino",
      "Kieran Parsons",
      "Ye Wang"
    ],
    "abstract": "Improving the safety and reliability of large language models (LLMs) is a\ncrucial aspect of realizing trustworthy AI systems. Although alignment methods\naim to suppress harmful content generation, LLMs are often still vulnerable to\njailbreaking attacks that employ adversarial inputs that subvert alignment and\ninduce harmful outputs. We propose the Randomized Embedding Smoothing and Token\nAggregation (RESTA) defense, which adds random noise to the embedding vectors\nand performs aggregation during the generation of each output token, with the\naim of better preserving semantic information. Our experiments demonstrate that\nour approach achieves superior robustness versus utility tradeoffs compared to\nthe baseline defenses.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "stat.ML",
      "68T07 (Primary), 68T50 (Secondary)"
    ],
    "primary_category": "cs.LG",
    "comment": "Presented in the Safe Generative AI Workshop at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2501.16497v1",
    "published_date": "2025-01-27 20:57:26 UTC",
    "updated_date": "2025-01-27 20:57:26 UTC"
  },
  {
    "arxiv_id": "2501.16490v1",
    "title": "Towards Robust Stability Prediction in Smart Grids: GAN-based Approach under Data Constraints and Adversarial Challenges",
    "authors": [
      "Emad Efatinasab",
      "Alessandro Brighente",
      "Denis Donadel",
      "Mauro Conti",
      "Mirco Rampazzo"
    ],
    "abstract": "Smart grids are critical for addressing the growing energy demand due to\nglobal population growth and urbanization. They enhance efficiency,\nreliability, and sustainability by integrating renewable energy. Ensuring their\navailability and safety requires advanced operational control and safety\nmeasures. Researchers employ AI and machine learning to assess grid stability,\nbut challenges like the lack of datasets and cybersecurity threats, including\nadversarial attacks, persist. In particular, data scarcity is a key issue:\nobtaining grid instability instances is tough due to the need for significant\nexpertise, resources, and time. However, they are essential to test novel\nresearch advancements and security mitigations. In this paper, we introduce a\nnovel framework to detect instability in smart grids by employing only stable\ndata. It relies on a Generative Adversarial Network (GAN) where the generator\nis trained to create instability data that are used along with stable data to\ntrain the discriminator. Moreover, we include a new adversarial training layer\nto improve robustness against adversarial attacks. Our solution, tested on a\ndataset composed of real-world stable and unstable samples, achieve accuracy up\nto 97.5\\% in predicting grid stability and up to 98.9\\% in detecting\nadversarial attacks. Moreover, we implemented our model in a single-board\ncomputer demonstrating efficient real-time decision-making with an average\nresponse time of less than 7ms. Our solution improves prediction accuracy and\nresilience while addressing data scarcity in smart grid management.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "This work has been submitted to the IEEE Internet of Things Journal\n  for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2501.16490v1",
    "published_date": "2025-01-27 20:48:25 UTC",
    "updated_date": "2025-01-27 20:48:25 UTC"
  },
  {
    "arxiv_id": "2501.16471v1",
    "title": "SIM: Surface-based fMRI Analysis for Inter-Subject Multimodal Decoding from Movie-Watching Experiments",
    "authors": [
      "Simon Dahan",
      "Gabriel Bénédict",
      "Logan Z. J. Williams",
      "Yourong Guo",
      "Daniel Rueckert",
      "Robert Leech",
      "Emma C. Robinson"
    ],
    "abstract": "Current AI frameworks for brain decoding and encoding, typically train and\ntest models within the same datasets. This limits their utility for brain\ncomputer interfaces (BCI) or neurofeedback, for which it would be useful to\npool experiences across individuals to better simulate stimuli not sampled\nduring training. A key obstacle to model generalisation is the degree of\nvariability of inter-subject cortical organisation, which makes it difficult to\nalign or compare cortical signals across participants. In this paper we address\nthis through the use of surface vision transformers, which build a\ngeneralisable model of cortical functional dynamics, through encoding the\ntopography of cortical networks and their interactions as a moving image across\na surface. This is then combined with tri-modal self-supervised contrastive\n(CLIP) alignment of audio, video, and fMRI modalities to enable the retrieval\nof visual and auditory stimuli from patterns of cortical activity (and\nvice-versa). We validate our approach on 7T task-fMRI data from 174 healthy\nparticipants engaged in the movie-watching experiment from the Human Connectome\nProject (HCP). Results show that it is possible to detect which movie clips an\nindividual is watching purely from their brain activity, even for individuals\nand movies not seen during training. Further analysis of attention maps reveals\nthat our model captures individual patterns of brain activity that reflect\nsemantic and visual systems. This opens the door to future personalised\nsimulations of brain function. Code & pre-trained models will be made available\nat https://github.com/metrics-lab/sim, processed data for training will be\navailable upon request at https://gin.g-node.org/Sdahan30/sim.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.AS",
      "eess.IV",
      "q-bio.NC"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages, accepted to ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.16471v1",
    "published_date": "2025-01-27 20:05:17 UTC",
    "updated_date": "2025-01-27 20:05:17 UTC"
  },
  {
    "arxiv_id": "2501.16466v3",
    "title": "On the Feasibility of Using LLMs to Autonomously Execute Multi-host Network Attacks",
    "authors": [
      "Brian Singer",
      "Keane Lucas",
      "Lakshmi Adiga",
      "Meghna Jain",
      "Lujo Bauer",
      "Vyas Sekar"
    ],
    "abstract": "LLMs have shown preliminary promise in some security tasks and CTF\nchallenges. Real cyberattacks are often multi-host network attacks, which\ninvolve executing a number of steps across multiple hosts such as conducting\nreconnaissance, exploiting vulnerabilities, and using compromised hosts to\nexfiltrate data. To date, the extent to which LLMs can autonomously execute\nmulti-host network attacks} is not well understood. To this end, our first\ncontribution is MHBench, an open-source multi-host attack benchmark with 10\nrealistic emulated networks (from 25 to 50 hosts). We find that popular LLMs\nincluding modern reasoning models (e.g., GPT4o, Gemini 2.5 Pro, Sonnet 3.7\nThinking) with state-of-art security-relevant prompting strategies (e.g.,\nPentestGPT, CyberSecEval3) cannot autonomously execute multi-host network\nattacks. To enable LLMs to autonomously execute such attacks, our second\ncontribution is Incalmo, an high-level abstraction layer. Incalmo enables LLMs\nto specify high-level actions (e.g., infect a host, scan a network). Incalmo's\ntranslation layer converts these actions into lower-level primitives (e.g.,\ncommands to exploit tools) through expert agents. In 9 out of 10 networks in\nMHBench, LLMs using Incalmo achieve at least some of the attack goals. Even\nsmaller LLMs (e.g., Haiku 3.5, Gemini 2 Flash) equipped with Incalmo achieve\nall goals in 5 of 10 environments. We also validate the key role of high-level\nactions in Incalmo's abstraction in enabling LLMs to autonomously execute such\nattacks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "18 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.16466v3",
    "published_date": "2025-01-27 19:58:29 UTC",
    "updated_date": "2025-05-16 14:55:52 UTC"
  },
  {
    "arxiv_id": "2501.16453v1",
    "title": "Detecting Zero-Day Attacks in Digital Substations via In-Context Learning",
    "authors": [
      "Faizan Manzoor",
      "Vanshaj Khattar",
      "Akila Herath",
      "Clifton Black",
      "Matthew C Nielsen",
      "Junho Hong",
      "Chen-Ching Liu",
      "Ming Jin"
    ],
    "abstract": "The occurrences of cyber attacks on the power grids have been increasing\nevery year, with novel attack techniques emerging every year. In this paper, we\naddress the critical challenge of detecting novel/zero-day attacks in digital\nsubstations that employ the IEC-61850 communication protocol. While many\nheuristic and machine learning (ML)-based methods have been proposed for attack\ndetection in IEC-61850 digital substations, generalization to novel or zero-day\nattacks remains challenging. We propose an approach that leverages the\nin-context learning (ICL) capability of the transformer architecture, the\nfundamental building block of large language models. The ICL approach enables\nthe model to detect zero-day attacks and learn from a few examples of that\nattack without explicit retraining. Our experiments on the IEC-61850 dataset\ndemonstrate that the proposed method achieves more than $85\\%$ detection\naccuracy on zero-day attacks while the existing state-of-the-art baselines\nfail. This work paves the way for building more secure and resilient digital\nsubstations of the future.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16453v1",
    "published_date": "2025-01-27 19:24:00 UTC",
    "updated_date": "2025-01-27 19:24:00 UTC"
  },
  {
    "arxiv_id": "2501.16450v3",
    "title": "360Brew: A Decoder-only Foundation Model for Personalized Ranking and Recommendation",
    "authors": [
      "Hamed Firooz",
      "Maziar Sanjabi",
      "Adrian Englhardt",
      "Aman Gupta",
      "Ben Levine",
      "Dre Olgiati",
      "Gungor Polatkan",
      "Iuliia Melnychuk",
      "Karthik Ramgopal",
      "Kirill Talanine",
      "Kutta Srinivasan",
      "Luke Simon",
      "Natesh Sivasubramoniapillai",
      "Necip Fazil Ayan",
      "Qingquan Song",
      "Samira Sriram",
      "Souvik Ghosh",
      "Tao Song",
      "Tejas Dharamsi",
      "Vignesh Kothapalli",
      "Xiaoling Zhai",
      "Ya Xu",
      "Yu Wang",
      "Yun Dai"
    ],
    "abstract": "Ranking and recommendation systems are the foundation for numerous online\nexperiences, ranging from search results to personalized content delivery.\nThese systems have evolved into complex, multilayered architectures that\nleverage vast datasets and often incorporate thousands of predictive models.\nThe maintenance and enhancement of these models is a labor intensive process\nthat requires extensive feature engineering. This approach not only exacerbates\ntechnical debt but also hampers innovation in extending these systems to\nemerging problem domains. In this report, we present our research to address\nthese challenges by utilizing a large foundation model with a textual interface\nfor ranking and recommendation tasks. We illustrate several key advantages of\nour approach: (1) a single model can manage multiple predictive tasks involved\nin ranking and recommendation, (2) decoder models with textual interface due to\ntheir comprehension of reasoning capabilities, can generalize to new\nrecommendation surfaces and out-of-domain problems, and (3) by employing\nnatural language interfaces for task definitions and verbalizing member\nbehaviors and their social connections, we eliminate the need for feature\nengineering and the maintenance of complex directed acyclic graphs of model\ndependencies. We introduce our research pre-production model, 360Brew V1.0, a\n150B parameter, decoder-only model that has been trained and fine-tuned on\nLinkedIn's data and tasks. This model is capable of solving over 30 predictive\ntasks across various segments of the LinkedIn platform, achieving performance\nlevels comparable to or exceeding those of current production systems based on\noffline metrics, without task-specific fine-tuning. Notably, each of these\ntasks is conventionally addressed by dedicated models that have been developed\nand maintained over multiple years by teams of a similar or larger size than\nour own.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16450v3",
    "published_date": "2025-01-27 19:14:52 UTC",
    "updated_date": "2025-02-07 22:11:01 UTC"
  },
  {
    "arxiv_id": "2501.16448v1",
    "title": "What is Harm? Baby Don't Hurt Me! On the Impossibility of Complete Harm Specification in AI Alignment",
    "authors": [
      "Robin Young"
    ],
    "abstract": "\"First, do no harm\" faces a fundamental challenge in artificial intelligence:\nhow can we specify what constitutes harm? While prior work treats harm\nspecification as a technical hurdle to be overcome through better algorithms or\nmore data, we argue this assumption is unsound. Drawing on information theory,\nwe demonstrate that complete harm specification is fundamentally impossible for\nany system where harm is defined external to its specifications. This\nimpossibility arises from an inescapable information-theoretic gap: the entropy\nof harm H(O) always exceeds the mutual information I(O;I) between ground truth\nharm O and a system's specifications I.\n  We introduce two novel metrics: semantic entropy H(S) and the\nsafety-capability ratio I(O;I)/H(O), to quantify these limitations. Through a\nprogression of increasingly sophisticated specification attempts, we show why\neach approach must fail and why the resulting gaps are not mere engineering\nchallenges but fundamental constraints akin to the halting problem. These\nresults suggest a paradigm shift: rather than pursuing complete specifications,\nAI alignment research should focus on developing systems that can operate\nsafely despite irreducible specification uncertainty.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16448v1",
    "published_date": "2025-01-27 19:13:39 UTC",
    "updated_date": "2025-01-27 19:13:39 UTC"
  },
  {
    "arxiv_id": "2502.15720v1",
    "title": "Training AI to be Loyal",
    "authors": [
      "Sewoong Oh",
      "Himanshu Tyagi",
      "Pramod Viswanath"
    ],
    "abstract": "Loyal AI is loyal to the community that builds it. An AI is loyal to a\ncommunity if the community has ownership, alignment, and control. Community\nowned models can only be used with the approval of the community and share the\neconomic rewards communally. Community aligned models have values that are\naligned with the consensus of the community. Community controlled models\nperform functions designed by the community. Since we would like permissionless\naccess to the loyal AI's community, we need the AI to be open source. The key\nscientific question then is: how can we build models that are openly accessible\n(open source) and yet are owned and governed by the community. This seeming\nimpossibility is the focus of this paper where we outline a concrete pathway to\nOpen, Monetizable and Loyal models (OML), building on our earlier work on OML,\narXiv:2411.03887(1) , and a representation via a cryptographic-ML library\nhttp://github.com/sentient-agi/oml-1.0-fingerprinting .",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "I.2.6"
    ],
    "primary_category": "cs.CY",
    "comment": "13 pages 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.15720v1",
    "published_date": "2025-01-27 19:11:19 UTC",
    "updated_date": "2025-01-27 19:11:19 UTC"
  },
  {
    "arxiv_id": "2502.00045v1",
    "title": "Restless Multi-armed Bandits under Frequency and Window Constraints for Public Service Inspections",
    "authors": [
      "Yi Mao",
      "Andrew Perrault"
    ],
    "abstract": "Municipal inspections are an important part of maintaining the quality of\ngoods and services. In this paper, we approach the problem of intelligently\nscheduling service inspections to maximize their impact, using the case of food\nestablishment inspections in Chicago as a case study. The Chicago Department of\nPublic Health (CDPH) inspects thousands of establishments each year, with a\nsubstantial fail rate (over 3,000 failed inspection reports in 2023). To\nbalance the objectives of ensuring adherence to guidelines, minimizing\ndisruption to establishments, and minimizing inspection costs, CDPH assigns\neach establishment an inspection window every year and guarantees that they\nwill be inspected exactly once during that window. These constraints create a\nchallenge for a restless multi-armed bandit (RMAB) approach, for which there\nare no existing methods. We develop an extension to Whittle index-based systems\nfor RMABs that can guarantee action window constraints and frequencies, and\nfurthermore can be leveraged to optimize action window assignments themselves.\nBriefly, we combine MDP reformulation and integer programming-based lookahead\nto maximize the impact of inspections subject to constraints. A neural\nnetwork-based supervised learning model is developed to model state transitions\nof real Chicago establishments using public CDPH inspection records, which\ndemonstrates 10\\% AUC improvements compared with directly predicting\nestablishments' failures. Our experiments not only show up to 24\\% (in\nsimulation) or 33\\% (on real data) reward improvements resulting from our\napproach but also give insight into the impact of scheduling constraints.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00045v1",
    "published_date": "2025-01-27 19:08:15 UTC",
    "updated_date": "2025-01-27 19:08:15 UTC"
  },
  {
    "arxiv_id": "2501.16411v2",
    "title": "PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding",
    "authors": [
      "Wei Chow",
      "Jiageng Mao",
      "Boyi Li",
      "Daniel Seita",
      "Vitor Guizilini",
      "Yue Wang"
    ],
    "abstract": "Understanding the physical world is a fundamental challenge in embodied AI,\ncritical for enabling agents to perform complex tasks and operate safely in\nreal-world environments. While Vision-Language Models (VLMs) have shown great\npromise in reasoning and task planning for embodied agents, their ability to\ncomprehend physical phenomena remains extremely limited. To close this gap, we\nintroduce PhysBench, a comprehensive benchmark designed to evaluate VLMs'\nphysical world understanding capability across a diverse set of tasks.\nPhysBench contains 10,002 entries of interleaved video-image-text data,\ncategorized into four major domains: physical object properties, physical\nobject relationships, physical scene understanding, and physics-based dynamics,\nfurther divided into 19 subclasses and 8 distinct capability dimensions. Our\nextensive experiments, conducted on 75 representative VLMs, reveal that while\nthese models excel in common-sense reasoning, they struggle with understanding\nthe physical world -- likely due to the absence of physical knowledge in their\ntraining data and the lack of embedded physical priors. To tackle the\nshortfall, we introduce PhysAgent, a novel framework that combines the\ngeneralization strengths of VLMs with the specialized expertise of vision\nmodels, significantly enhancing VLMs' physical understanding across a variety\nof tasks, including an 18.4\\% improvement on GPT-4o. Furthermore, our results\ndemonstrate that enhancing VLMs' physical world understanding capabilities can\nhelp embodied agents such as MOKA. We believe that PhysBench and PhysAgent\noffer valuable insights and contribute to bridging the gap between VLMs and\nphysical world understanding.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR 2025. Project page: https://physbench.github.io/ Dataset:\n  https://huggingface.co/datasets/USC-GVL/PhysBench",
    "pdf_url": "http://arxiv.org/pdf/2501.16411v2",
    "published_date": "2025-01-27 18:59:58 UTC",
    "updated_date": "2025-01-29 03:52:39 UTC"
  },
  {
    "arxiv_id": "2501.16330v1",
    "title": "RelightVid: Temporal-Consistent Diffusion Model for Video Relighting",
    "authors": [
      "Ye Fang",
      "Zeyi Sun",
      "Shangzhan Zhang",
      "Tong Wu",
      "Yinghao Xu",
      "Pan Zhang",
      "Jiaqi Wang",
      "Gordon Wetzstein",
      "Dahua Lin"
    ],
    "abstract": "Diffusion models have demonstrated remarkable success in image generation and\nediting, with recent advancements enabling albedo-preserving image relighting.\nHowever, applying these models to video relighting remains challenging due to\nthe lack of paired video relighting datasets and the high demands for output\nfidelity and temporal consistency, further complicated by the inherent\nrandomness of diffusion models. To address these challenges, we introduce\nRelightVid, a flexible framework for video relighting that can accept\nbackground video, text prompts, or environment maps as relighting conditions.\nTrained on in-the-wild videos with carefully designed illumination\naugmentations and rendered videos under extreme dynamic lighting, RelightVid\nachieves arbitrary video relighting with high temporal consistency without\nintrinsic decomposition while preserving the illumination priors of its image\nbackbone.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16330v1",
    "published_date": "2025-01-27 18:59:57 UTC",
    "updated_date": "2025-01-27 18:59:57 UTC"
  },
  {
    "arxiv_id": "2501.16329v1",
    "title": "sDREAMER: Self-distilled Mixture-of-Modality-Experts Transformer for Automatic Sleep Staging",
    "authors": [
      "Jingyuan Chen",
      "Yuan Yao",
      "Mie Anderson",
      "Natalie Hauglund",
      "Celia Kjaerby",
      "Verena Untiet",
      "Maiken Nedergaard",
      "Jiebo Luo"
    ],
    "abstract": "Automatic sleep staging based on electroencephalography (EEG) and\nelectromyography (EMG) signals is an important aspect of sleep-related\nresearch. Current sleep staging methods suffer from two major drawbacks. First,\nthere are limited information interactions between modalities in the existing\nmethods. Second, current methods do not develop unified models that can handle\ndifferent sources of input. To address these issues, we propose a novel sleep\nstage scoring model sDREAMER, which emphasizes cross-modality interaction and\nper-channel performance. Specifically, we develop a mixture-of-modality-expert\n(MoME) model with three pathways for EEG, EMG, and mixed signals with partially\nshared weights. We further propose a self-distillation training scheme for\nfurther information interaction across modalities. Our model is trained with\nmulti-channel inputs and can make classifications on either single-channel or\nmulti-channel inputs. Experiments demonstrate that our model outperforms the\nexisting transformer-based sleep scoring methods for multi-channel inference.\nFor single-channel inference, our model also outperforms the transformer-based\nmodels trained with single-channel signals.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16329v1",
    "published_date": "2025-01-27 18:59:55 UTC",
    "updated_date": "2025-01-27 18:59:55 UTC"
  },
  {
    "arxiv_id": "2501.16309v1",
    "title": "Evaluating The Performance of Using Large Language Models to Automate Summarization of CT Simulation Orders in Radiation Oncology",
    "authors": [
      "Meiyun Cao",
      "Shaw Hu",
      "Jason Sharp",
      "Edward Clouser",
      "Jason Holmes",
      "Linda L. Lam",
      "Xiaoning Ding",
      "Diego Santos Toesca",
      "Wendy S. Lindholm",
      "Samir H. Patel",
      "Sujay A. Vora",
      "Peilong Wang",
      "Wei Liu"
    ],
    "abstract": "Purpose: This study aims to use a large language model (LLM) to automate the\ngeneration of summaries from the CT simulation orders and evaluate its\nperformance.\n  Materials and Methods: A total of 607 CT simulation orders for patients were\ncollected from the Aria database at our institution. A locally hosted Llama 3.1\n405B model, accessed via the Application Programming Interface (API) service,\nwas used to extract keywords from the CT simulation orders and generate\nsummaries. The downloaded CT simulation orders were categorized into seven\ngroups based on treatment modalities and disease sites. For each group, a\ncustomized instruction prompt was developed collaboratively with therapists to\nguide the Llama 3.1 405B model in generating summaries. The ground truth for\nthe corresponding summaries was manually derived by carefully reviewing each CT\nsimulation order and subsequently verified by therapists. The accuracy of the\nLLM-generated summaries was evaluated by therapists using the verified ground\ntruth as a reference.\n  Results: About 98% of the LLM-generated summaries aligned with the manually\ngenerated ground truth in terms of accuracy. Our evaluations showed an improved\nconsistency in format and enhanced readability of the LLM-generated summaries\ncompared to the corresponding therapists-generated summaries. This automated\napproach demonstrated a consistent performance across all groups, regardless of\nmodality or disease site.\n  Conclusions: This study demonstrated the high precision and consistency of\nthe Llama 3.1 405B model in extracting keywords and summarizing CT simulation\norders, suggesting that LLMs have great potential to help with this task,\nreduce the workload of therapists and improve workflow efficiency.",
    "categories": [
      "physics.med-ph",
      "cs.AI"
    ],
    "primary_category": "physics.med-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16309v1",
    "published_date": "2025-01-27 18:47:58 UTC",
    "updated_date": "2025-01-27 18:47:58 UTC"
  },
  {
    "arxiv_id": "2501.16300v1",
    "title": "Large Models in Dialogue for Active Perception and Anomaly Detection",
    "authors": [
      "Tzoulio Chamiti",
      "Nikolaos Passalis",
      "Anastasios Tefas"
    ],
    "abstract": "Autonomous aerial monitoring is an important task aimed at gathering\ninformation from areas that may not be easily accessible by humans. At the same\ntime, this task often requires recognizing anomalies from a significant\ndistance or not previously encountered in the past. In this paper, we propose a\nnovel framework that leverages the advanced capabilities provided by Large\nLanguage Models (LLMs) to actively collect information and perform anomaly\ndetection in novel scenes. To this end, we propose an LLM based model dialogue\napproach, in which two deep learning models engage in a dialogue to actively\ncontrol a drone to increase perception and anomaly detection accuracy. We\nconduct our experiments in a high fidelity simulation environment where an LLM\nis provided with a predetermined set of natural language movement commands\nmapped into executable code functions. Additionally, we deploy a multimodal\nVisual Question Answering (VQA) model charged with the task of visual question\nanswering and captioning. By engaging the two models in conversation, the LLM\nasks exploratory questions while simultaneously flying a drone into different\nparts of the scene, providing a novel way to implement active perception. By\nleveraging LLMs reasoning ability, we output an improved detailed description\nof the scene going beyond existing static perception approaches. In addition to\ninformation gathering, our approach is utilized for anomaly detection and our\nresults demonstrate the proposed methods effectiveness in informing and\nalerting about potential hazards.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to International Conference of Pattern Recognition (ICPR\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2501.16300v1",
    "published_date": "2025-01-27 18:38:36 UTC",
    "updated_date": "2025-01-27 18:38:36 UTC"
  },
  {
    "arxiv_id": "2501.16295v1",
    "title": "Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity",
    "authors": [
      "Weixin Liang",
      "Junhong Shen",
      "Genghan Zhang",
      "Ning Dong",
      "Luke Zettlemoyer",
      "Lili Yu"
    ],
    "abstract": "State Space Models (SSMs) have emerged as efficient alternatives to\nTransformers for sequential modeling, but their inability to leverage\nmodality-specific features limits their performance in multi-modal pretraining.\nHere, we propose Mixture-of-Mamba, a novel SSM architecture that introduces\nmodality-aware sparsity through modality-specific parameterization of the Mamba\nblock. Building on Mixture-of-Transformers (W. Liang et al. arXiv:2411.04996;\n2024), we extend the benefits of modality-aware sparsity to SSMs while\npreserving their computational efficiency. We evaluate Mixture-of-Mamba across\nthree multi-modal pretraining settings: Transfusion (interleaved text and\ncontinuous image tokens with diffusion loss), Chameleon (interleaved text and\ndiscrete image tokens), and an extended three-modality framework incorporating\nspeech. Mixture-of-Mamba consistently reaches the same loss values at earlier\ntraining steps with significantly reduced computational costs. In the\nTransfusion setting, Mixture-of-Mamba achieves equivalent image loss using only\n34.76% of the training FLOPs at the 1.4B scale. In the Chameleon setting,\nMixture-of-Mamba reaches similar image loss with just 42.50% of the FLOPs at\nthe 1.4B scale, and similar text loss with just 65.40% of the FLOPs. In the\nthree-modality setting, MoM matches speech loss at 24.80% of the FLOPs at the\n1.4B scale. Our ablation study highlights the synergistic effects of decoupling\nprojection components, where joint decoupling yields greater gains than\nindividual modifications. These results establish modality-aware sparsity as a\nversatile and effective design principle, extending its impact from\nTransformers to SSMs and setting new benchmarks in multi-modal pretraining. Our\ncode can be accessed at https://github.com/Weixin-Liang/Mixture-of-Mamba",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16295v1",
    "published_date": "2025-01-27 18:35:05 UTC",
    "updated_date": "2025-01-27 18:35:05 UTC"
  },
  {
    "arxiv_id": "2501.16288v2",
    "title": "Upside Down Reinforcement Learning with Policy Generators",
    "authors": [
      "Jacopo Di Ventura",
      "Dylan R. Ashley",
      "Vincent Herrmann",
      "Francesco Faccio",
      "Jürgen Schmidhuber"
    ],
    "abstract": "Upside Down Reinforcement Learning (UDRL) is a promising framework for\nsolving reinforcement learning problems which focuses on learning\ncommand-conditioned policies. In this work, we extend UDRL to the task of\nlearning a command-conditioned generator of deep neural network policies. We\naccomplish this using Hypernetworks - a variant of Fast Weight Programmers,\nwhich learn to decode input commands representing a desired expected return\ninto command-specific weight matrices. Our method, dubbed Upside Down\nReinforcement Learning with Policy Generators (UDRLPG), streamlines comparable\ntechniques by removing the need for an evaluator or critic to update the\nweights of the generator. To counteract the increased variance in last returns\ncaused by not having an evaluator, we decouple the sampling probability of the\nbuffer from the absolute number of policies in it, which, together with a\nsimple weighting strategy, improves the empirical convergence of the algorithm.\nCompared with existing algorithms, UDRLPG achieves competitive performance and\nhigh returns, sometimes outperforming more complex architectures. Our\nexperiments show that a trained generator can generalize to create policies\nthat achieve unseen returns zero-shot. The proposed method appears to be\neffective in mitigating some of the challenges associated with learning highly\nmultimodal functions. Altogether, we believe that UDRLPG represents a promising\nstep forward in achieving greater empirical sample efficiency in RL. A full\nimplementation of UDRLPG is publicly available at\nhttps://github.com/JacopoD/udrlpg_",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T07",
      "I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "4 pages in main text, 4 figures in main text; source code available\n  at https://github.com/JacopoD/udrlpg_",
    "pdf_url": "http://arxiv.org/pdf/2501.16288v2",
    "published_date": "2025-01-27 18:25:04 UTC",
    "updated_date": "2025-01-28 13:05:53 UTC"
  },
  {
    "arxiv_id": "2501.16282v1",
    "title": "Brain-Adapter: Enhancing Neurological Disorder Analysis with Adapter-Tuning Multimodal Large Language Models",
    "authors": [
      "Jing Zhang",
      "Xiaowei Yu",
      "Yanjun Lyu",
      "Lu Zhang",
      "Tong Chen",
      "Chao Cao",
      "Yan Zhuang",
      "Minheng Chen",
      "Tianming Liu",
      "Dajiang Zhu"
    ],
    "abstract": "Understanding brain disorders is crucial for accurate clinical diagnosis and\ntreatment. Recent advances in Multimodal Large Language Models (MLLMs) offer a\npromising approach to interpreting medical images with the support of text\ndescriptions. However, previous research has primarily focused on 2D medical\nimages, leaving richer spatial information of 3D images under-explored, and\nsingle-modality-based methods are limited by overlooking the critical clinical\ninformation contained in other modalities. To address this issue, this paper\nproposes Brain-Adapter, a novel approach that incorporates an extra bottleneck\nlayer to learn new knowledge and instill it into the original pre-trained\nknowledge. The major idea is to incorporate a lightweight bottleneck layer to\ntrain fewer parameters while capturing essential information and utilize a\nContrastive Language-Image Pre-training (CLIP) strategy to align multimodal\ndata within a unified representation space. Extensive experiments demonstrated\nthe effectiveness of our approach in integrating multimodal data to\nsignificantly improve the diagnosis accuracy without high computational costs,\nhighlighting the potential to enhance real-world diagnostic workflows.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16282v1",
    "published_date": "2025-01-27 18:20:49 UTC",
    "updated_date": "2025-01-27 18:20:49 UTC"
  },
  {
    "arxiv_id": "2501.16409v1",
    "title": "Classification of Mild Cognitive Impairment Based on Dynamic Functional Connectivity Using Spatio-Temporal Transformer",
    "authors": [
      "Jing Zhang",
      "Yanjun Lyu",
      "Xiaowei Yu",
      "Lu Zhang",
      "Chao Cao",
      "Tong Chen",
      "Minheng Chen",
      "Yan Zhuang",
      "Tianming Liu",
      "Dajiang Zhu"
    ],
    "abstract": "Dynamic functional connectivity (dFC) using resting-state functional magnetic\nresonance imaging (rs-fMRI) is an advanced technique for capturing the dynamic\nchanges of neural activities, and can be very useful in the studies of brain\ndiseases such as Alzheimer's disease (AD). Yet, existing studies have not fully\nleveraged the sequential information embedded within dFC that can potentially\nprovide valuable information when identifying brain conditions. In this paper,\nwe propose a novel framework that jointly learns the embedding of both spatial\nand temporal information within dFC based on the transformer architecture.\nSpecifically, we first construct dFC networks from rs-fMRI data through a\nsliding window strategy. Then, we simultaneously employ a temporal block and a\nspatial block to capture higher-order representations of dynamic\nspatio-temporal dependencies, via mapping them into an efficient fused feature\nrepresentation. To further enhance the robustness of these feature\nrepresentations by reducing the dependency on labeled data, we also introduce a\ncontrastive learning strategy to manipulate different brain states.\nExperimental results on 345 subjects with 570 scans from the Alzheimer's\nDisease Neuroimaging Initiative (ADNI) demonstrate the superiority of our\nproposed method for MCI (Mild Cognitive Impairment, the prodromal stage of AD)\nprediction, highlighting its potential for early identification of AD.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16409v1",
    "published_date": "2025-01-27 18:20:33 UTC",
    "updated_date": "2025-01-27 18:20:33 UTC"
  },
  {
    "arxiv_id": "2501.16274v1",
    "title": "What is Formal Verification without Specifications? A Survey on mining LTL Specifications",
    "authors": [
      "Daniel Neider",
      "Rajarshi Roy"
    ],
    "abstract": "Virtually all verification techniques using formal methods rely on the\navailability of a formal specification, which describes the design requirements\nprecisely. However, formulating specifications remains a manual task that is\nnotoriously challenging and error-prone. To address this bottleneck in formal\nverification, recent research has thus focussed on automatically generating\nspecifications for formal verification from examples of (desired and undesired)\nsystem behavior. In this survey, we list and compare recent advances in mining\nspecifications in Linear Temporal Logic (LTL), the de facto standard\nspecification language for reactive systems. Several approaches have been\ndesigned for learning LTL formulas, which address different aspects and\nsettings of specification design. Moreover, the approaches rely on a diverse\nrange of techniques such as constraint solving, neural network training,\nenumerative search, etc. We survey the current state-of-the-art techniques and\ncompare them for the convenience of the formal methods practitioners.",
    "categories": [
      "cs.FL",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.FL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16274v1",
    "published_date": "2025-01-27 18:06:48 UTC",
    "updated_date": "2025-01-27 18:06:48 UTC"
  },
  {
    "arxiv_id": "2501.16273v2",
    "title": "Return of the Encoder: Maximizing Parameter Efficiency for SLMs",
    "authors": [
      "Mohamed Elfeki",
      "Rui Liu",
      "Chad Voegele"
    ],
    "abstract": "The dominance of large decoder-only language models has overshadowed\nencoder-decoder architectures, despite their fundamental efficiency advantages\nin sequence processing. For small language models (SLMs) - those with 1 billion\nparameters or fewer - our systematic analysis across GPU, CPU, and NPU\nplatforms reveals that encoder-decoder architectures achieve 47% lower\nfirst-token latency and 4.7x higher throughput compared to decoder-only models\non edge devices. These gains may be attributed to encoder-decoder's one-time\ninput processing and efficient separation of understanding and generation\nphases.\n  We introduce a novel knowledge distillation framework that enables\nencoder-decoder models to leverage capabilities from large scalable\ndecoder-only teachers while preserving their architectural advantages,\nachieving up to 6 average performance points improvement across diverse tasks,\nwith significant gains in asymmetric sequence tasks where input and output\ndistributions can benefit from different processing approaches.\n  When combined with modern advances like Rotary Positional Embeddings (RoPE)\nand Vision encoders, our systematic investigation demonstrates that\nencoder-decoder architectures provide a more practical path toward deploying\ncapable language models in resource-constrained environments. Our findings\nchallenge the prevailing trend toward decoder-only scaling, showing that\narchitectural choices become increasingly crucial as parameter budgets\ndecrease, particularly for on-device and edge deployments where computational\nefficiency is paramount.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 5 figures. LLMs/SLMs, encoder-decoder and decoder-only",
    "pdf_url": "http://arxiv.org/pdf/2501.16273v2",
    "published_date": "2025-01-27 18:06:36 UTC",
    "updated_date": "2025-01-30 16:44:45 UTC"
  },
  {
    "arxiv_id": "2501.16271v1",
    "title": "From Molecules to Mixtures: Learning Representations of Olfactory Mixture Similarity using Inductive Biases",
    "authors": [
      "Gary Tom",
      "Cher Tian Ser",
      "Ella M. Rajaonson",
      "Stanley Lo",
      "Hyun Suk Park",
      "Brian K. Lee",
      "Benjamin Sanchez-Lengeling"
    ],
    "abstract": "Olfaction -- how molecules are perceived as odors to humans -- remains poorly\nunderstood. Recently, the principal odor map (POM) was introduced to digitize\nthe olfactory properties of single compounds. However, smells in real life are\nnot pure single molecules, but complex mixtures of molecules, whose\nrepresentations remain relatively under-explored. In this work, we introduce\nPOMMix, an extension of the POM to represent mixtures. Our representation\nbuilds upon the symmetries of the problem space in a hierarchical manner: (1)\ngraph neural networks for building molecular embeddings, (2) attention\nmechanisms for aggregating molecular representations into mixture\nrepresentations, and (3) cosine prediction heads to encode olfactory perceptual\ndistance in the mixture embedding space. POMMix achieves state-of-the-art\npredictive performance across multiple datasets. We also evaluate the\ngeneralizability of the representation on multiple splits when applied to\nunseen molecules and mixture sizes. Our work advances the effort to digitize\nolfaction, and highlights the synergy of domain expertise and deep learning in\ncrafting expressive representations in low-data regimes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.16271v1",
    "published_date": "2025-01-27 18:05:28 UTC",
    "updated_date": "2025-01-27 18:05:28 UTC"
  },
  {
    "arxiv_id": "2501.16249v2",
    "title": "Lightweight Weighted Average Ensemble Model for Pneumonia Detection in Chest X-Ray Images",
    "authors": [
      "Suresh Babu Nettur",
      "Shanthi Karpurapu",
      "Unnati Nettur",
      "Likhit Sagar Gajja",
      "Sravanthy Myneni",
      "Akhil Dusi",
      "Lalithya Posham"
    ],
    "abstract": "Pneumonia is a leading cause of illness and death in children, underscoring\nthe need for early and accurate detection. In this study, we propose a novel\nlightweight ensemble model for detecting pneumonia in children using chest\nX-ray images. This ensemble model integrates two pre-trained convolutional\nneural networks (CNNs), MobileNetV2 and NASNetMobile, selected for their\nbalance of computational efficiency and accuracy. These models were fine-tuned\non a pediatric chest X-ray dataset and combined to enhance classification\nperformance. Our proposed ensemble model achieved a classification accuracy of\n98.63%, significantly outperforming individual models such as MobileNetV2\n(97.10%) and NASNetMobile(96.25%) in terms of accuracy, precision, recall, and\nF1 score. Moreover, the ensemble model outperformed state-of-the-art\narchitectures, including ResNet50, InceptionV3, and DenseNet201, while\nmaintaining computational efficiency. The proposed lightweight ensemble model\npresents a highly effective and resource-efficient solution for pneumonia\ndetection, making it particularly suitable for deployment in\nresource-constrained settings.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Corresponding authors: Shanthi Karpurapu\n  (shanthi.karpurapu@gmail.com), Suresh Babu Nettur (nettursuresh@gmail.com)\n  Shanthi Karpurapu and Suresh Babu Nettur are co-first authors",
    "pdf_url": "http://arxiv.org/pdf/2501.16249v2",
    "published_date": "2025-01-27 17:51:29 UTC",
    "updated_date": "2025-02-01 00:54:20 UTC"
  },
  {
    "arxiv_id": "2501.16243v1",
    "title": "Accelerating Quantum Reinforcement Learning with a Quantum Natural Policy Gradient Based Approach",
    "authors": [
      "Yang Xu",
      "Vaneet Aggarwal"
    ],
    "abstract": "We address the problem of quantum reinforcement learning (QRL) under\nmodel-free settings with quantum oracle access to the Markov Decision Process\n(MDP). This paper introduces a Quantum Natural Policy Gradient (QNPG)\nalgorithm, which replaces the random sampling used in classical Natural Policy\nGradient (NPG) estimators with a deterministic gradient estimation approach,\nenabling seamless integration into quantum systems. While this modification\nintroduces a bounded bias in the estimator, the bias decays exponentially with\nincreasing truncation levels. This paper demonstrates that the proposed QNPG\nalgorithm achieves a sample complexity of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-1.5})$ for queries to the quantum oracle,\nsignificantly improving the classical lower bound of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ for queries to the MDP.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16243v1",
    "published_date": "2025-01-27 17:38:30 UTC",
    "updated_date": "2025-01-27 17:38:30 UTC"
  },
  {
    "arxiv_id": "2501.16224v1",
    "title": "Language-Based Bayesian Optimization Research Assistant (BORA)",
    "authors": [
      "Abdoulatif Cissé",
      "Xenophon Evangelopoulos",
      "Vladimir V. Gusev",
      "Andrew I. Cooper"
    ],
    "abstract": "Many important scientific problems involve multivariate optimization coupled\nwith slow and laborious experimental measurements. These complex,\nhigh-dimensional searches can be defined by non-convex optimization landscapes\nthat resemble needle-in-a-haystack surfaces, leading to entrapment in local\nminima. Contextualizing optimizers with human domain knowledge is a powerful\napproach to guide searches to localized fruitful regions. However, this\napproach is susceptible to human confirmation bias and it is also challenging\nfor domain experts to keep track of the rapidly expanding scientific\nliterature. Here, we propose the use of Large Language Models (LLMs) for\ncontextualizing Bayesian optimization (BO) via a hybrid optimization framework\nthat intelligently and economically blends stochastic inference with domain\nknowledge-based insights from the LLM, which is used to suggest new,\nbetter-performing areas of the search space for exploration. Our method fosters\nuser engagement by offering real-time commentary on the optimization progress,\nexplaining the reasoning behind the search strategies. We validate the\neffectiveness of our approach on synthetic benchmarks with up to 15 independent\nvariables and demonstrate the ability of LLMs to reason in four real-world\nexperimental tasks where context-aware suggestions boost optimization\nperformance substantially.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16224v1",
    "published_date": "2025-01-27 17:20:04 UTC",
    "updated_date": "2025-01-27 17:20:04 UTC"
  },
  {
    "arxiv_id": "2501.16215v1",
    "title": "Enhancing Visual Inspection Capability of Multi-Modal Large Language Models on Medical Time Series with Supportive Conformalized and Interpretable Small Specialized Models",
    "authors": [
      "Huayu Li",
      "Xiwen Chen",
      "Ci Zhang",
      "Stuart F. Quan",
      "William D. S. Killgore",
      "Shu-Fen Wung",
      "Chen X. Chen",
      "Geng Yuan",
      "Jin Lu",
      "Ao Li"
    ],
    "abstract": "Large language models (LLMs) exhibit remarkable capabilities in visual\ninspection of medical time-series data, achieving proficiency comparable to\nhuman clinicians. However, their broad scope limits domain-specific precision,\nand proprietary weights hinder fine-tuning for specialized datasets. In\ncontrast, small specialized models (SSMs) excel in targeted tasks but lack the\ncontextual reasoning required for complex clinical decision-making. To address\nthese challenges, we propose ConMIL (Conformalized Multiple Instance Learning),\na decision-support SSM that integrates seamlessly with LLMs. By using Multiple\nInstance Learning (MIL) to identify clinically significant signal segments and\nconformal prediction for calibrated set-valued outputs, ConMIL enhances LLMs'\ninterpretative capabilities for medical time-series analysis. Experimental\nresults demonstrate that ConMIL significantly improves the performance of\nstate-of-the-art LLMs, such as ChatGPT4.0 and Qwen2-VL-7B. Specifically,\n\\ConMIL{}-supported Qwen2-VL-7B achieves 94.92% and 96.82% precision for\nconfident samples in arrhythmia detection and sleep staging, compared to\nstandalone LLM accuracy of 46.13% and 13.16%. These findings highlight the\npotential of ConMIL to bridge task-specific precision and broader contextual\nreasoning, enabling more reliable and interpretable AI-driven clinical decision\nsupport.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16215v1",
    "published_date": "2025-01-27 17:07:20 UTC",
    "updated_date": "2025-01-27 17:07:20 UTC"
  },
  {
    "arxiv_id": "2501.16211v1",
    "title": "UDBE: Unsupervised Diffusion-based Brightness Enhancement in Underwater Images",
    "authors": [
      "Tatiana Taís Schein",
      "Gustavo Pereira de Almeira",
      "Stephanie Loi Brião",
      "Rodrigo Andrade de Bem",
      "Felipe Gomes de Oliveira",
      "Paulo L. J. Drews-Jr"
    ],
    "abstract": "Activities in underwater environments are paramount in several scenarios,\nwhich drives the continuous development of underwater image enhancement\ntechniques. A major challenge in this domain is the depth at which images are\ncaptured, with increasing depth resulting in a darker environment. Most\nexisting methods for underwater image enhancement focus on noise removal and\ncolor adjustment, with few works dedicated to brightness enhancement. This work\nintroduces a novel unsupervised learning approach to underwater image\nenhancement using a diffusion model. Our method, called UDBE, is based on\nconditional diffusion to maintain the brightness details of the unpaired input\nimages. The input image is combined with a color map and a Signal-Noise\nRelation map (SNR) to ensure stable training and prevent color distortion in\nthe output images. The results demonstrate that our approach achieves an\nimpressive accuracy rate in the datasets UIEB, SUIM and RUIE, well-established\nunderwater image benchmarks. Additionally, the experiments validate the\nrobustness of our approach, regarding the image quality metrics PSNR, SSIM,\nUIQM, and UISM, indicating the good performance of the brightness enhancement\nprocess. The source code is available here: https://github.com/gusanagy/UDBE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Paper presented at ICMLA 2024",
    "pdf_url": "http://arxiv.org/pdf/2501.16211v1",
    "published_date": "2025-01-27 17:01:45 UTC",
    "updated_date": "2025-01-27 17:01:45 UTC"
  },
  {
    "arxiv_id": "2501.16207v3",
    "title": "From Informal to Formal -- Incorporating and Evaluating LLMs on Natural Language Requirements to Verifiable Formal Proofs",
    "authors": [
      "Jialun Cao",
      "Yaojie Lu",
      "Meiziniu Li",
      "Haoyang Ma",
      "Haokun Li",
      "Mengda He",
      "Cheng Wen",
      "Le Sun",
      "Hongyu Zhang",
      "Shengchao Qin",
      "Shing-Chi Cheung",
      "Cong Tian"
    ],
    "abstract": "The research in AI-based formal mathematical reasoning has shown an\nunstoppable growth trend. These studies have excelled in mathematical\ncompetitions like IMO and have made significant progress. This paper focuses on\nformal verification, an immediate application scenario of formal reasoning, and\nbreaks it down into sub-tasks. We constructed 18k high-quality\ninstruction-response pairs across five formal specification languages (Coq,\nLean4, Dafny, ACSL, and TLA+) by distilling gpt-4o and evaluated against ten\nopen-sourced LLMs, including recent popular DeepSeek-R1. We also fine-tuned\nseveral 7~8B small models to achieve comparable performance with\nDeepseek-R1-671B. Interestingly, we observed that fine-tuning with formal data\nalso enhances mathematics, reasoning, and coding capabilities. Fine-tuned\nmodels are released at https: //huggingface.co/fm-universe.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.PL"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.16207v3",
    "published_date": "2025-01-27 17:00:56 UTC",
    "updated_date": "2025-03-05 15:26:49 UTC"
  },
  {
    "arxiv_id": "2501.16191v1",
    "title": "Raiders of the Lost Dependency: Fixing Dependency Conflicts in Python using LLMs",
    "authors": [
      "Antony Bartlett",
      "Cynthia Liem",
      "Annibale Panichella"
    ],
    "abstract": "Fixing Python dependency issues is a tedious and error-prone task for\ndevelopers, who must manually identify and resolve environment dependencies and\nversion constraints of third-party modules and Python interpreters. Researchers\nhave attempted to automate this process by relying on large knowledge graphs\nand database lookup tables. However, these traditional approaches face\nlimitations due to the variety of dependency error types, large sets of\npossible module versions, and conflicts among transitive dependencies. This\nstudy explores the potential of using large language models (LLMs) to\nautomatically fix dependency issues in Python programs. We introduce PLLM\n(pronounced \"plum\"), a novel technique that employs retrieval-augmented\ngeneration (RAG) to help an LLM infer Python versions and required modules for\na given Python file. PLLM builds a testing environment that iteratively (1)\nprompts the LLM for module combinations, (2) tests the suggested changes, and\n(3) provides feedback (error messages) to the LLM to refine the fix. This\nfeedback cycle leverages natural language processing (NLP) to intelligently\nparse and interpret build error messages. We benchmark PLLM on the Gistable\nHG2.9K dataset, a collection of challenging single-file Python gists. We\ncompare PLLM against two state-of-the-art automatic dependency inference\napproaches, namely PyEGo and ReadPyE, w.r.t. the ability to resolve dependency\nissues. Our results indicate that PLLM can fix more dependency issues than the\ntwo baselines, with +218 (+15.97%) more fixes over ReadPyE and +281 (+21.58%)\nover PyEGo. Our deeper analyses suggest that PLLM is particularly beneficial\nfor projects with many dependencies and for specific third-party numerical and\nmachine-learning modules. Our findings demonstrate the potential of LLM-based\napproaches to iteratively resolve Python dependency issues.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Under submission to TOSEM, 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.16191v1",
    "published_date": "2025-01-27 16:45:34 UTC",
    "updated_date": "2025-01-27 16:45:34 UTC"
  },
  {
    "arxiv_id": "2501.16182v1",
    "title": "The Linear Attention Resurrection in Vision Transformer",
    "authors": [
      "Chuanyang Zheng"
    ],
    "abstract": "Vision Transformers (ViTs) have recently taken computer vision by storm.\nHowever, the softmax attention underlying ViTs comes with a quadratic\ncomplexity in time and memory, hindering the application of ViTs to\nhigh-resolution images. We revisit the attention design and propose a linear\nattention method to address the limitation, which doesn't sacrifice ViT's core\nadvantage of capturing global representation like existing methods (e.g. local\nwindow attention of Swin). We further investigate the key difference between\nlinear attention and softmax attention. Our empirical results suggest that\nlinear attention lacks a fundamental property of concentrating the distribution\nof the attention matrix. Inspired by this observation, we introduce a local\nconcentration module to enhance linear attention. By incorporating enhanced\nlinear global attention and local window attention, we propose a new ViT\narchitecture, dubbed L$^2$ViT. Notably, L$^2$ViT can effectively capture both\nglobal interactions and local representations while enjoying linear\ncomputational complexity. Extensive experiments demonstrate the strong\nperformance of L$^2$ViT. On image classification, L$^2$ViT achieves 84.4% Top-1\naccuracy on ImageNet-1K without any extra training data or label. By further\npre-training on ImageNet-22k, it attains 87.0% when fine-tuned with resolution\n384$^2$. For downstream tasks, L$^2$ViT delivers favorable performance as a\nbackbone on object detection as well as semantic segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16182v1",
    "published_date": "2025-01-27 16:29:17 UTC",
    "updated_date": "2025-01-27 16:29:17 UTC"
  },
  {
    "arxiv_id": "2502.15719v1",
    "title": "Governing AI Beyond the Pretraining Frontier",
    "authors": [
      "Nicholas A. Caputo"
    ],
    "abstract": "This year, jurisdictions worldwide, including the United States, the European\nUnion, the United Kingdom, and China, are set to enact or revise laws governing\nfrontier AI. Their efforts largely rely on the assumption that increasing model\nscale through pretraining is the path to more advanced AI capabilities. Yet\ngrowing evidence suggests that this \"pretraining paradigm\" may be hitting a\nwall and major AI companies are turning to alternative approaches, like\ninference-time \"reasoning,\" to boost capabilities instead.\n  This paradigm shift presents fundamental challenges for the frontier AI\ngovernance frameworks that target pretraining scale as a key bottleneck useful\nfor monitoring, control, and exclusion, threatening to undermine this new legal\norder as it emerges. This essay seeks to identify these challenges and point to\nnew paths forward for regulation. First, we examine the existing frontier AI\nregulatory regime and analyze some key traits and vulnerabilities. Second, we\nintroduce the concept of the \"pretraining frontier,\" the capabilities threshold\nmade possible by scaling up pretraining alone, and demonstrate how it could\nmake the regulatory field more diffuse and complex and lead to new forms of\ncompetition. Third, we lay out a regulatory approach that focuses on increasing\ntransparency and leveraging new natural technical bottlenecks to effectively\noversee changing frontier AI development while minimizing regulatory burdens\nand protecting fundamental rights. Our analysis provides concrete mechanisms\nfor governing frontier AI systems across diverse technical paradigms, offering\npolicymakers tools for addressing both current and future regulatory challenges\nin frontier AI.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.15719v1",
    "published_date": "2025-01-27 16:25:03 UTC",
    "updated_date": "2025-01-27 16:25:03 UTC"
  },
  {
    "arxiv_id": "2501.16177v1",
    "title": "BAG: Body-Aligned 3D Wearable Asset Generation",
    "authors": [
      "Zhongjin Luo",
      "Yang Li",
      "Mingrui Zhang",
      "Senbo Wang",
      "Han Yan",
      "Xibin Song",
      "Taizhang Shang",
      "Wei Mao",
      "Hongdong Li",
      "Xiaoguang Han",
      "Pan Ji"
    ],
    "abstract": "While recent advancements have shown remarkable progress in general 3D shape\ngeneration models, the challenge of leveraging these approaches to\nautomatically generate wearable 3D assets remains unexplored. To this end, we\npresent BAG, a Body-aligned Asset Generation method to output 3D wearable asset\nthat can be automatically dressed on given 3D human bodies. This is achived by\ncontrolling the 3D generation process using human body shape and pose\ninformation. Specifically, we first build a general single-image to consistent\nmultiview image diffusion model, and train it on the large Objaverse dataset to\nachieve diversity and generalizability. Then we train a Controlnet to guide the\nmultiview generator to produce body-aligned multiview images. The control\nsignal utilizes the multiview 2D projections of the target human body, where\npixel values represent the XYZ coordinates of the body surface in a canonical\nspace. The body-conditioned multiview diffusion generates body-aligned\nmultiview images, which are then fed into a native 3D diffusion model to\nproduce the 3D shape of the asset. Finally, by recovering the similarity\ntransformation using multiview silhouette supervision and addressing asset-body\npenetration with physics simulators, the 3D asset can be accurately fitted onto\nthe target human body. Experimental results demonstrate significant advantages\nover existing methods in terms of image prompt-following capability, shape\ndiversity, and shape quality. Our project page is available at\nhttps://bag-3d.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "video: https://youtu.be/XJtG82LjQKc",
    "pdf_url": "http://arxiv.org/pdf/2501.16177v1",
    "published_date": "2025-01-27 16:23:45 UTC",
    "updated_date": "2025-01-27 16:23:45 UTC"
  },
  {
    "arxiv_id": "2501.16174v1",
    "title": "Measuring Heterogeneity in Machine Learning with Distributed Energy Distance",
    "authors": [
      "Mengchen Fan",
      "Baocheng Geng",
      "Roman Shterenberg",
      "Joseph A. Casey",
      "Zhong Chen",
      "Keren Li"
    ],
    "abstract": "In distributed and federated learning, heterogeneity across data sources\nremains a major obstacle to effective model aggregation and convergence. We\nfocus on feature heterogeneity and introduce energy distance as a sensitive\nmeasure for quantifying distributional discrepancies. While we show that energy\ndistance is robust for detecting data distribution shifts, its direct use in\nlarge-scale systems can be prohibitively expensive. To address this, we develop\nTaylor approximations that preserve key theoretical quantitative properties\nwhile reducing computational overhead. Through simulation studies, we show how\naccurately capturing feature discrepancies boosts convergence in distributed\nlearning. Finally, we propose a novel application of energy distance to assign\npenalty weights for aligning predictions across heterogeneous nodes, ultimately\nenhancing coordination in federated and distributed settings.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "15 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.16174v1",
    "published_date": "2025-01-27 16:15:57 UTC",
    "updated_date": "2025-01-27 16:15:57 UTC"
  },
  {
    "arxiv_id": "2501.16164v1",
    "title": "MetaDecorator: Generating Immersive Virtual Tours through Multimodality",
    "authors": [
      "Shuang Xie",
      "Yang Liu",
      "Jeannie S. A. Lee",
      "Haiwei Dong"
    ],
    "abstract": "MetaDecorator, is a framework that empowers users to personalize virtual\nspaces. By leveraging text-driven prompts and image synthesis techniques,\nMetaDecorator adorns static panoramas captured by 360{\\deg} imaging devices,\ntransforming them into uniquely styled and visually appealing environments.\nThis significantly enhances the realism and engagement of virtual tours\ncompared to traditional offerings. Beyond the core framework, we also discuss\nthe integration of Large Language Models (LLMs) and haptics in the VR\napplication to provide a more immersive experience.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.ET",
      "cs.MM"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16164v1",
    "published_date": "2025-01-27 15:59:58 UTC",
    "updated_date": "2025-01-27 15:59:58 UTC"
  },
  {
    "arxiv_id": "2501.16154v2",
    "title": "AdaCoT: Rethinking Cross-Lingual Factual Reasoning through Adaptive Chain-of-Thought",
    "authors": [
      "Xin Huang",
      "Tarun Kumar Vangani",
      "Zhengyuan Liu",
      "Bowei Zou",
      "Ai Ti Aw"
    ],
    "abstract": "Large language models have shown impressive multilingual capabilities through\npretraining on diverse corpora. While these models show strong reasoning\nabilities, their performance varies significantly across languages due to\nimbalanced training data distribution. Existing approaches using sample-level\ntranslation for extensive multilingual pretraining and cross-lingual tuning\nface scalability challenges and often fail to capture nuanced reasoning\nprocesses across languages. In this paper, we introduce AdaCoT (Adaptive\nChain-of-Thought), a framework that enhances multilingual factual reasoning by\ndynamically routing thought processes in intermediary ``thinking languages''\nbefore generating target-language responses. AdaCoT leverages a\nlanguage-agnostic core and incorporates an adaptive, reward-based mechanism for\nselecting optimal reasoning pathways without requiring additional pretraining.\nOur comprehensive evaluation across multiple benchmarks demonstrates\nsubstantial improvements in both factual reasoning quality and cross-lingual\nconsistency, with particularly strong performance gains in low-resource\nlanguage settings. The results suggest that adaptive reasoning paths can\neffectively bridge the performance gap between high and low-resource languages\nwhile maintaining cultural and linguistic nuances.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16154v2",
    "published_date": "2025-01-27 15:48:57 UTC",
    "updated_date": "2025-05-09 05:50:14 UTC"
  },
  {
    "arxiv_id": "2501.16150v1",
    "title": "AI Agents for Computer Use: A Review of Instruction-based Computer Control, GUI Automation, and Operator Assistants",
    "authors": [
      "Pascal J. Sager",
      "Benjamin Meyer",
      "Peng Yan",
      "Rebekka von Wartburg-Kottler",
      "Layan Etaiwi",
      "Aref Enayati",
      "Gabriel Nobel",
      "Ahmed Abdulkadir",
      "Benjamin F. Grewe",
      "Thilo Stadelmann"
    ],
    "abstract": "Instruction-based computer control agents (CCAs) execute complex action\nsequences on personal computers or mobile devices to fulfill tasks using the\nsame graphical user interfaces as a human user would, provided instructions in\nnatural language. This review offers a comprehensive overview of the emerging\nfield of instruction-based computer control, examining available agents --\ntheir taxonomy, development, and respective resources -- and emphasizing the\nshift from manually designed, specialized agents to leveraging foundation\nmodels such as large language models (LLMs) and vision-language models (VLMs).\nWe formalize the problem and establish a taxonomy of the field to analyze\nagents from three perspectives: (a) the environment perspective, analyzing\ncomputer environments; (b) the interaction perspective, describing observations\nspaces (e.g., screenshots, HTML) and action spaces (e.g., mouse and keyboard\nactions, executable code); and (c) the agent perspective, focusing on the core\nprinciple of how an agent acts and learns to act. Our framework encompasses\nboth specialized and foundation agents, facilitating their comparative analysis\nand revealing how prior solutions in specialized agents, such as an environment\nlearning step, can guide the development of more capable foundation agents.\nAdditionally, we review current CCA datasets and CCA evaluation methods and\noutline the challenges to deploying such agents in a productive setting. In\ntotal, we review and classify 86 CCAs and 33 related datasets. By highlighting\ntrends, limitations, and future research directions, this work presents a\ncomprehensive foundation to obtain a broad understanding of the field and push\nits future development.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16150v1",
    "published_date": "2025-01-27 15:44:02 UTC",
    "updated_date": "2025-01-27 15:44:02 UTC"
  },
  {
    "arxiv_id": "2501.16146v1",
    "title": "Toward Efficient Generalization in 3D Human Pose Estimation via a Canonical Domain Approach",
    "authors": [
      "Hoosang Lee",
      "Jeha Ryu"
    ],
    "abstract": "Recent advancements in deep learning methods have significantly improved the\nperformance of 3D Human Pose Estimation (HPE). However, performance degradation\ncaused by domain gaps between source and target domains remains a major\nchallenge to generalization, necessitating extensive data augmentation and/or\nfine-tuning for each specific target domain. To address this issue more\nefficiently, we propose a novel canonical domain approach that maps both the\nsource and target domains into a unified canonical domain, alleviating the need\nfor additional fine-tuning in the target domain. To construct the canonical\ndomain, we introduce a canonicalization process to generate a novel canonical\n2D-3D pose mapping that ensures 2D-3D pose consistency and simplifies 2D-3D\npose patterns, enabling more efficient training of lifting networks. The\ncanonicalization of both domains is achieved through the following steps: (1)\nin the source domain, the lifting network is trained within the canonical\ndomain; (2) in the target domain, input 2D poses are canonicalized prior to\ninference by leveraging the properties of perspective projection and known\ncamera intrinsics. Consequently, the trained network can be directly applied to\nthe target domain without requiring additional fine-tuning. Experiments\nconducted with various lifting networks and publicly available datasets (e.g.,\nHuman3.6M, Fit3D, MPI-INF-3DHP) demonstrate that the proposed method\nsubstantially improves generalization capability across datasets while using\nthe same data volume.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.16146v1",
    "published_date": "2025-01-27 15:39:39 UTC",
    "updated_date": "2025-01-27 15:39:39 UTC"
  },
  {
    "arxiv_id": "2501.16142v1",
    "title": "Towards General-Purpose Model-Free Reinforcement Learning",
    "authors": [
      "Scott Fujimoto",
      "Pierluca D'Oro",
      "Amy Zhang",
      "Yuandong Tian",
      "Michael Rabbat"
    ],
    "abstract": "Reinforcement learning (RL) promises a framework for near-universal\nproblem-solving. In practice however, RL algorithms are often tailored to\nspecific benchmarks, relying on carefully tuned hyperparameters and algorithmic\nchoices. Recently, powerful model-based RL methods have shown impressive\ngeneral results across benchmarks but come at the cost of increased complexity\nand slow run times, limiting their broader applicability. In this paper, we\nattempt to find a unifying model-free deep RL algorithm that can address a\ndiverse class of domains and problem settings. To achieve this, we leverage\nmodel-based representations that approximately linearize the value function,\ntaking advantage of the denser task objectives used by model-based RL while\navoiding the costs associated with planning or simulated trajectories. We\nevaluate our algorithm, MR.Q, on a variety of common RL benchmarks with a\nsingle set of hyperparameters and show a competitive performance against\ndomain-specific and general baselines, providing a concrete step towards\nbuilding general-purpose model-free deep RL algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.16142v1",
    "published_date": "2025-01-27 15:36:37 UTC",
    "updated_date": "2025-01-27 15:36:37 UTC"
  },
  {
    "arxiv_id": "2501.17195v1",
    "title": "Atla Selene Mini: A General Purpose Evaluation Model",
    "authors": [
      "Andrei Alexandru",
      "Antonia Calvi",
      "Henry Broomfield",
      "Jackson Golden",
      "Kyle Dai",
      "Mathias Leys",
      "Maurice Burger",
      "Max Bartolo",
      "Roman Engeler",
      "Sashank Pisupati",
      "Toby Drane",
      "Young Sun Park"
    ],
    "abstract": "We introduce Atla Selene Mini, a state-of-the-art small language\nmodel-as-a-judge (SLMJ). Selene Mini is a general-purpose evaluator that\noutperforms the best SLMJs and GPT-4o-mini on overall performance across 11\nout-of-distribution benchmarks, spanning absolute scoring, classification, and\npairwise preference tasks. It is the highest-scoring 8B generative model on\nRewardBench, surpassing strong baselines like GPT-4o and specialized judges. To\nachieve this, we develop a principled data curation strategy that augments\npublic datasets with synthetically generated critiques and ensures high quality\nthrough filtering and dataset ablations. We train our model on a combined\ndirect preference optimization (DPO) and supervised fine-tuning (SFT) loss, and\nproduce a highly promptable evaluator that excels in real-world scenarios.\nSelene Mini shows dramatically improved zero-shot agreement with human expert\nevaluations on financial and medical industry datasets. It is also robust to\nvariations in prompt format. Preliminary results indicate that Selene Mini is\nthe top-ranking evaluator in a live, community-driven Judge Arena. We release\nthe model weights on HuggingFace\n(https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) and Ollama to encourage\nwidespread community adoption.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.17195v1",
    "published_date": "2025-01-27 15:09:08 UTC",
    "updated_date": "2025-01-27 15:09:08 UTC"
  },
  {
    "arxiv_id": "2501.16100v2",
    "title": "Automated Detection of Sport Highlights from Audio and Video Sources",
    "authors": [
      "Francesco Della Santa",
      "Morgana Lalli"
    ],
    "abstract": "This study presents a novel Deep Learning-based and lightweight approach for\nthe automated detection of sports highlights (HLs) from audio and video\nsources. HL detection is a key task in sports video analysis, traditionally\nrequiring significant human effort. Our solution leverages Deep Learning (DL)\nmodels trained on relatively small datasets of audio Mel-spectrograms and\ngrayscale video frames, achieving promising accuracy rates of 89% and 83% for\naudio and video detection, respectively. The use of small datasets, combined\nwith simple architectures, demonstrates the practicality of our method for fast\nand cost-effective deployment. Furthermore, an ensemble model combining both\nmodalities shows improved robustness against false positives and false\nnegatives. The proposed methodology offers a scalable solution for automated HL\ndetection across various types of sports video content, reducing the need for\nmanual intervention. Future work will focus on enhancing model architectures\nand extending this approach to broader scene-detection tasks in media analysis.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16100v2",
    "published_date": "2025-01-27 14:50:13 UTC",
    "updated_date": "2025-01-31 09:03:03 UTC"
  },
  {
    "arxiv_id": "2501.16093v1",
    "title": "STAR: Stepwise Task Augmentation and Relation Learning for Aspect Sentiment Quad Prediction",
    "authors": [
      "Wenna Lai",
      "Haoran Xie",
      "Guandong Xu",
      "Qing Li"
    ],
    "abstract": "Aspect-based sentiment analysis (ABSA) aims to identify four sentiment\nelements, including aspect term, aspect category, opinion term, and sentiment\npolarity. These elements construct the complete picture of sentiments. The most\nchallenging task, aspect sentiment quad prediction (ASQP), predicts these\nelements simultaneously, hindered by difficulties in accurately coupling\ndifferent sentiment elements. A key challenge is insufficient annotated data\nthat limits the capability of models in semantic understanding and reasoning\nabout quad prediction. To address this, we propose stepwise task augmentation\nand relation learning (STAR), a strategy inspired by human reasoning. STAR\nconstructs auxiliary data to learn quadruple relationships incrementally by\naugmenting with pairwise and overall relation tasks derived from training data.\nBy encouraging the model to infer causal relationships among sentiment elements\nwithout requiring additional annotations, STAR effectively enhances quad\nprediction. Extensive experiments demonstrate the proposed STAR exhibits\nsuperior performance on four benchmark datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 2 figures, and 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.16093v1",
    "published_date": "2025-01-27 14:41:20 UTC",
    "updated_date": "2025-01-27 14:41:20 UTC"
  },
  {
    "arxiv_id": "2502.20402v1",
    "title": "Beyond transparency: computational reliabilism as an externalist epistemology of algorithms",
    "authors": [
      "Juan Manuel Durán"
    ],
    "abstract": "This chapter is interested in the epistemology of algorithms. As I intend to\napproach the topic, this is an issue about epistemic justification. Current\napproaches to justification emphasize the transparency of algorithms, which\nentails elucidating their internal mechanisms -- such as functions and\nvariables -- and demonstrating how (or that) these produce outputs. Thus, the\nmode of justification through transparency is contingent on what can be shown\nabout the algorithm and, in this sense, is internal to the algorithm. In\ncontrast, I advocate for an externalist epistemology of algorithms that I term\ncomputational reliabilism (CR). While I have previously introduced and examined\nCR in the field of computer simulations ([42, 53, 4]), this chapter extends\nthis reliabilist epistemology to encompass a broader spectrum of algorithms\nutilized in various scientific disciplines, with a particular emphasis on\nmachine learning applications. At its core, CR posits that an algorithm's\noutput is justified if it is produced by a reliable algorithm. A reliable\nalgorithm is one that has been specified, coded, used, and maintained utilizing\nreliability indicators. These reliability indicators stem from formal methods,\nalgorithmic metrics, expert competencies, cultures of research, and other\nscientific endeavors. The primary aim of this chapter is to delineate the\nfoundations of CR, explicate its operational mechanisms, and outline its\npotential as an externalist epistemology of algorithms.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.20402v1",
    "published_date": "2025-01-27 14:31:47 UTC",
    "updated_date": "2025-01-27 14:31:47 UTC"
  },
  {
    "arxiv_id": "2502.00043v2",
    "title": "Mitigating Traffic Oscillations in Mixed Traffic Flow with Scalable Deep Koopman Predictive Control",
    "authors": [
      "Hao Lyu",
      "Yanyong Guo",
      "Pan Liu",
      "Nan Zheng",
      "Ting Wang",
      "Quansheng Yue"
    ],
    "abstract": "The use of connected automated vehicle (CAV) is advocated to mitigate traffic\noscillations in mixed traffic flow consisting of CAVs and human driven vehicles\n(HDVs). This study proposes an adaptive deep Koopman predictive control\nframework (AdapKoopPC) for regulating mixed traffic flow. Firstly, a Koopman\ntheory-based adaptive trajectory prediction deep network (AdapKoopnet) is\ndesigned for modeling HDVs car-following behavior. AdapKoopnet enables the\nrepresentation of HDVs behavior by a linear model in a high-dimensional space.\nSecondly, the model predictive control is employed to smooth the mixed traffic\nflow, where the combination of the linear dynamic model of CAVs and linear\nprediction blocks from AdapKoopnet is embedded as the predictive model into the\nAdapKoopPC. Finally, the predictive performance of the prosed AdapKoopnet is\nverified using the HighD naturalistic driving dataset. Furthermore, the control\nperformance of AdapKoopPC is validated by the numerical simulations. Results\ndemonstrate that the AdapKoopnet provides more accuracy HDVs predicted\ntrajectories than the baseline nonlinear models. Moreover, the proposed\nAdapKoopPC exhibits more effective control performance with less computation\ncost compared with baselines in mitigating traffic oscillations, especially at\nthe low CAVs penetration rates. The code of proposed AdapKoopPC is open source.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00043v2",
    "published_date": "2025-01-27 14:28:20 UTC",
    "updated_date": "2025-04-22 15:15:14 UTC"
  },
  {
    "arxiv_id": "2501.16075v1",
    "title": "PISCO: Pretty Simple Compression for Retrieval-Augmented Generation",
    "authors": [
      "Maxime Louis",
      "Hervé Déjean",
      "Stéphane Clinchant"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) pipelines enhance Large Language Models\n(LLMs) by retrieving relevant documents, but they face scalability issues due\nto high inference costs and limited context size. Document compression is a\npractical solution, but current soft compression methods suffer from accuracy\nlosses and require extensive pretraining. In this paper, we introduce PISCO, a\nnovel method that achieves a 16x compression rate with minimal accuracy loss\n(0-3%) across diverse RAG-based question-answering (QA) tasks. Unlike existing\napproaches, PISCO requires no pretraining or annotated data, relying solely on\nsequence-level knowledge distillation from document-based questions. With the\nability to fine-tune a 7-10B LLM in 48 hours on a single A100 GPU, PISCO offers\na highly efficient and scalable solution. We present comprehensive experiments\nshowing that PISCO outperforms existing compression models by 8% in accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16075v1",
    "published_date": "2025-01-27 14:26:27 UTC",
    "updated_date": "2025-01-27 14:26:27 UTC"
  },
  {
    "arxiv_id": "2501.18628v1",
    "title": "Indiana Jones: There Are Always Some Useful Ancient Relics",
    "authors": [
      "Junchen Ding",
      "Jiahao Zhang",
      "Yi Liu",
      "Ziqi Ding",
      "Gelei Deng",
      "Yuekang Li"
    ],
    "abstract": "This paper introduces Indiana Jones, an innovative approach to jailbreaking\nLarge Language Models (LLMs) by leveraging inter-model dialogues and\nkeyword-driven prompts. Through orchestrating interactions among three\nspecialised LLMs, the method achieves near-perfect success rates in bypassing\ncontent safeguards in both white-box and black-box LLMs. The research exposes\nsystemic vulnerabilities within contemporary models, particularly their\nsusceptibility to producing harmful or unethical outputs when guided by\nostensibly innocuous prompts framed in historical or contextual contexts.\nExperimental evaluations highlight the efficacy and adaptability of Indiana\nJones, demonstrating its superiority over existing jailbreak methods. These\nfindings emphasise the urgent need for enhanced ethical safeguards and robust\nsecurity measures in the development of LLMs. Moreover, this work provides a\ncritical foundation for future studies aimed at fortifying LLMs against\nadversarial exploitation while preserving their utility and flexibility.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.18628v1",
    "published_date": "2025-01-27 14:12:07 UTC",
    "updated_date": "2025-01-27 14:12:07 UTC"
  },
  {
    "arxiv_id": "2501.16061v1",
    "title": "The Unbearable Lightness of Prompting: A Critical Reflection on the Environmental Impact of genAI use in Design Education",
    "authors": [
      "Maria Luce Lupetti",
      "Elena Cavallin",
      "Dave Murray-Rust"
    ],
    "abstract": "Design educators are finding ways to support students in skillfully using\nGenAI tools in their practices while encouraging the critical scrutiny of the\nethical and social issues around these technologies. However, the issue of\nenvironmental sustainability remains unaddressed. There is a lack of both\nresources to grasp the environmental costs of genAI in education and a lack of\nshared practices for engaging with the issue. This paper critically reflects on\nthe energy costs of using genAI in design education, using a workshop held in\n2023 with 49 students as a motivating example. Through this reflection, we\ndevelop a set of five alternative stances, with related actions, that support\nthe conscious use of genAI in design education. The work contributes to the\nfield of design and HCI by bringing together ways for educators to reflect on\ntheir practices, informing the future development of educational programs\naround genAI.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "25 pages, 3 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2501.16061v1",
    "published_date": "2025-01-27 14:01:14 UTC",
    "updated_date": "2025-01-27 14:01:14 UTC"
  },
  {
    "arxiv_id": "2501.16050v1",
    "title": "Skeleton-Guided-Translation: A Benchmarking Framework for Code Repository Translation with Fine-Grained Quality Evaluation",
    "authors": [
      "Xing Zhang",
      "Jiaheng Wen",
      "Fangkai Yang",
      "Pu Zhao",
      "Yu Kang",
      "Junhao Wang",
      "Maoquan Wang",
      "Yufan Huang",
      "Elsie Nallipogu",
      "Qingwei Lin",
      "Yingnong Dang",
      "Saravan Rajmohan",
      "Dongmei Zhang",
      "Qi Zhang"
    ],
    "abstract": "The advancement of large language models has intensified the need to\nmodernize enterprise applications and migrate legacy systems to secure,\nversatile languages. However, existing code translation benchmarks primarily\nfocus on individual functions, overlooking the complexities involved in\ntranslating entire repositories, such as maintaining inter-module coherence and\nmanaging dependencies. While some recent repository-level translation\nbenchmarks attempt to address these challenges, they still face limitations,\nincluding poor maintainability and overly coarse evaluation granularity, which\nmake them less developer-friendly. We introduce Skeleton-Guided-Translation, a\nframework for repository-level Java to C# code translation with fine-grained\nquality evaluation. It uses a two-step process: first translating the\nrepository's structural \"skeletons\", then translating the full repository\nguided by these skeletons. Building on this, we present TRANSREPO-BENCH, a\nbenchmark of high quality open-source Java repositories and their corresponding\nC# skeletons, including matching unit tests and build configurations. Our unit\ntests are fixed and can be applied across multiple or incremental translations\nwithout manual adjustments, enhancing automation and scalability in\nevaluations. Additionally, we develop fine-grained evaluation metrics that\nassess translation quality at the individual test case level, addressing\ntraditional binary metrics' inability to distinguish when build failures cause\nall tests to fail. Evaluations using TRANSREPO-BENCH highlight key challenges\nand advance more accurate repository level code translation.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16050v1",
    "published_date": "2025-01-27 13:44:51 UTC",
    "updated_date": "2025-01-27 13:44:51 UTC"
  },
  {
    "arxiv_id": "2501.16033v1",
    "title": "PRISMe: A Novel LLM-Powered Tool for Interactive Privacy Policy Assessment",
    "authors": [
      "Vincent Freiberger",
      "Arthur Fleig",
      "Erik Buchmann"
    ],
    "abstract": "Protecting online privacy requires users to engage with and comprehend\nwebsite privacy policies, but many policies are difficult and tedious to read.\nWe present PRISMe (Privacy Risk Information Scanner for Me), a novel Large\nLanguage Model (LLM)-driven privacy policy assessment tool, which helps users\nto understand the essence of a lengthy, complex privacy policy while browsing.\nThe tool, a browser extension, integrates a dashboard and an LLM chat. One\nmajor contribution is the first rigorous evaluation of such a tool. In a\nmixed-methods user study (N=22), we evaluate PRISMe's efficiency, usability,\nunderstandability of the provided information, and impacts on awareness. While\nour tool improves privacy awareness by providing a comprehensible quick\noverview and a quality chat for in-depth discussion, users note issues with\nconsistency and building trust in the tool. From our insights, we derive\nimportant design implications to guide future policy analysis tools.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.m; I.2.7"
    ],
    "primary_category": "cs.HC",
    "comment": "30 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.16033v1",
    "published_date": "2025-01-27 13:27:04 UTC",
    "updated_date": "2025-01-27 13:27:04 UTC"
  },
  {
    "arxiv_id": "2501.16029v1",
    "title": "FDLLM: A Text Fingerprint Detection Method for LLMs in Multi-Language, Multi-Domain Black-Box Environments",
    "authors": [
      "Zhiyuan Fu",
      "Junfan Chen",
      "Hongyu Sun",
      "Ting Yang",
      "Ruidong Li",
      "Yuqing Zhang"
    ],
    "abstract": "Using large language models (LLMs) integration platforms without transparency\nabout which LLM is being invoked can lead to potential security risks.\nSpecifically, attackers may exploit this black-box scenario to deploy malicious\nmodels and embed viruses in the code provided to users. In this context, it is\nincreasingly urgent for users to clearly identify the LLM they are interacting\nwith, in order to avoid unknowingly becoming victims of malicious models.\nHowever, existing studies primarily focus on mixed classification of human and\nmachine-generated text, with limited attention to classifying texts generated\nsolely by different models. Current research also faces dual bottlenecks: poor\nquality of LLM-generated text (LLMGT) datasets and limited coverage of\ndetectable LLMs, resulting in poor detection performance for various LLMGT in\nblack-box scenarios. We propose the first LLMGT fingerprint detection model,\n\\textbf{FDLLM}, based on Qwen2.5-7B and fine-tuned using LoRA to address these\nchallenges. FDLLM can more efficiently handle detection tasks across\nmultilingual and multi-domain scenarios. Furthermore, we constructed a dataset\nnamed \\textbf{FD-Datasets}, consisting of 90,000 samples that span multiple\nlanguages and domains, covering 20 different LLMs. Experimental results\ndemonstrate that FDLLM achieves a macro F1 score 16.7\\% higher than the best\nbaseline method, LM-D.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16029v1",
    "published_date": "2025-01-27 13:18:40 UTC",
    "updated_date": "2025-01-27 13:18:40 UTC"
  },
  {
    "arxiv_id": "2501.18626v3",
    "title": "The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt Adversarial Attacks on LLMs",
    "authors": [
      "Sergey Berezin",
      "Reza Farahbakhsh",
      "Noel Crespi"
    ],
    "abstract": "We present a novel class of jailbreak adversarial attacks on LLMs, termed\nTask-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks\n(e.g., cipher decoding, riddles, code execution) into the model's prompt to\nindirectly generate prohibited inputs. To systematically assess the\neffectiveness of these attacks, we introduce the PHRYGE benchmark. We\ndemonstrate that our techniques successfully circumvent safeguards in six\nstate-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings\nhighlight critical weaknesses in current LLM safety alignments and underscore\nthe urgent need for more sophisticated defence strategies.\n  Warning: this paper contains examples of unethical inquiries used solely for\nresearch purposes.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.18626v3",
    "published_date": "2025-01-27 12:48:47 UTC",
    "updated_date": "2025-02-04 17:09:13 UTC"
  },
  {
    "arxiv_id": "2501.15998v1",
    "title": "Controllable Forgetting Mechanism for Few-Shot Class-Incremental Learning",
    "authors": [
      "Kirill Paramonov",
      "Mete Ozay",
      "Eunju Yang",
      "Jijoong Moon",
      "Umberto Michieli"
    ],
    "abstract": "Class-incremental learning in the context of limited personal labeled samples\n(few-shot) is critical for numerous real-world applications, such as smart home\ndevices. A key challenge in these scenarios is balancing the trade-off between\nadapting to new, personalized classes and maintaining the performance of the\nmodel on the original, base classes. Fine-tuning the model on novel classes\noften leads to the phenomenon of catastrophic forgetting, where the accuracy of\nbase classes declines unpredictably and significantly. In this paper, we\npropose a simple yet effective mechanism to address this challenge by\ncontrolling the trade-off between novel and base class accuracy. We\nspecifically target the ultra-low-shot scenario, where only a single example is\navailable per novel class. Our approach introduces a Novel Class Detection\n(NCD) rule, which adjusts the degree of forgetting a priori while\nsimultaneously enhancing performance on novel classes. We demonstrate the\nversatility of our solution by applying it to state-of-the-art Few-Shot\nClass-Incremental Learning (FSCIL) methods, showing consistent improvements\nacross different settings. To better quantify the trade-off between novel and\nbase class performance, we introduce new metrics: NCR@2FOR and NCR@5FOR. Our\napproach achieves up to a 30% improvement in novel class accuracy on the\nCIFAR100 dataset (1-shot, 1 novel class) while maintaining a controlled base\nclass forgetting rate of 2%.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.15998v1",
    "published_date": "2025-01-27 12:31:50 UTC",
    "updated_date": "2025-01-27 12:31:50 UTC"
  },
  {
    "arxiv_id": "2502.00040v2",
    "title": "Multi-Objective Reinforcement Learning for Power Grid Topology Control",
    "authors": [
      "Thomas Lautenbacher",
      "Ali Rajaei",
      "Davide Barbieri",
      "Jan Viebahn",
      "Jochen L. Cremer"
    ],
    "abstract": "Transmission grid congestion increases as the electrification of various\nsectors requires transmitting more power. Topology control, through substation\nreconfiguration, can reduce congestion but its potential remains\nunder-exploited in operations. A challenge is modeling the topology control\nproblem to align well with the objectives and constraints of operators.\nAddressing this challenge, this paper investigates the application of\nmulti-objective reinforcement learning (MORL) to integrate multiple conflicting\nobjectives for power grid topology control. We develop a MORL approach using\ndeep optimistic linear support (DOL) and multi-objective proximal policy\noptimization (MOPPO) to generate a set of Pareto-optimal policies that balance\nobjectives such as minimizing line loading, topological deviation, and\nswitching frequency. Initial case studies show that the MORL approach can\nprovide valuable insights into objective trade-offs and improve Pareto front\napproximation compared to a random search baseline. The generated\nmulti-objective RL policies are 30% more successful in preventing grid failure\nunder contingencies and 20% more effective when training budget is reduced -\ncompared to the common single objective RL policy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00040v2",
    "published_date": "2025-01-27 12:23:03 UTC",
    "updated_date": "2025-05-01 13:45:05 UTC"
  },
  {
    "arxiv_id": "2501.15987v1",
    "title": "MultiPDENet: PDE-embedded Learning with Multi-time-stepping for Accelerated Flow Simulation",
    "authors": [
      "Qi Wang",
      "Yuan Mi",
      "Haoyun Wang",
      "Yi Zhang",
      "Ruizhi Chengze",
      "Hongsheng Liu",
      "Ji-Rong Wen",
      "Hao Sun"
    ],
    "abstract": "Solving partial differential equations (PDEs) by numerical methods meet\ncomputational cost challenge for getting the accurate solution since fine grids\nand small time steps are required. Machine learning can accelerate this\nprocess, but struggle with weak generalizability, interpretability, and data\ndependency, as well as suffer in long-term prediction. To this end, we propose\na PDE-embedded network with multiscale time stepping (MultiPDENet), which fuses\nthe scheme of numerical methods and machine learning, for accelerated\nsimulation of flows. In particular, we design a convolutional filter based on\nthe structure of finite difference stencils with a small number of parameters\nto optimize, which estimates the equivalent form of spatial derivative on a\ncoarse grid to minimize the equation's residual. A Physics Block with a\n4th-order Runge-Kutta integrator at the fine time scale is established that\nembeds the structure of PDEs to guide the prediction. To alleviate the curse of\ntemporal error accumulation in long-term prediction, we introduce a multiscale\ntime integration approach, where a neural network is used to correct the\nprediction error at a coarse time scale. Experiments across various PDE\nsystems, including the Navier-Stokes equations, demonstrate that MultiPDENet\ncan accurately predict long-term spatiotemporal dynamics, even given small and\nincomplete training data, e.g., spatiotemporally down-sampled datasets.\nMultiPDENet achieves the state-of-the-art performance compared with other\nneural baseline models, also with clear speedup compared to classical numerical\nmethods.",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.NA"
    ],
    "primary_category": "math.NA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15987v1",
    "published_date": "2025-01-27 12:15:51 UTC",
    "updated_date": "2025-01-27 12:15:51 UTC"
  },
  {
    "arxiv_id": "2501.15972v1",
    "title": "Flexible Blood Glucose Control: Offline Reinforcement Learning from Human Feedback",
    "authors": [
      "Harry Emerson",
      "Sam Gordon James",
      "Matthew Guy",
      "Ryan McConville"
    ],
    "abstract": "Reinforcement learning (RL) has demonstrated success in automating insulin\ndosing in simulated type 1 diabetes (T1D) patients but is currently unable to\nincorporate patient expertise and preference. This work introduces PAINT\n(Preference Adaptation for INsulin control in T1D), an original RL framework\nfor learning flexible insulin dosing policies from patient records. PAINT\nemploys a sketch-based approach for reward learning, where past data is\nannotated with a continuous reward signal to reflect patient's desired\noutcomes. Labelled data trains a reward model, informing the actions of a novel\nsafety-constrained offline RL algorithm, designed to restrict actions to a safe\nstrategy and enable preference tuning via a sliding scale. In-silico evaluation\nshows PAINT achieves common glucose goals through simple labelling of desired\nstates, reducing glycaemic risk by 15% over a commercial benchmark. Action\nlabelling can also be used to incorporate patient expertise, demonstrating an\nability to pre-empt meals (+10% time-in-range post-meal) and address certain\ndevice errors (-1.6% variance post-error) with patient guidance. These results\nhold under realistic conditions, including limited samples, labelling errors,\nand intra-patient variability. This work illustrates PAINT's potential in\nreal-world T1D management and more broadly any tasks requiring rapid and\nprecise preference learning under safety constraints.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.15972v1",
    "published_date": "2025-01-27 11:31:40 UTC",
    "updated_date": "2025-01-27 11:31:40 UTC"
  },
  {
    "arxiv_id": "2501.15969v1",
    "title": "An Explainable Disease Surveillance System for Early Prediction of Multiple Chronic Diseases",
    "authors": [
      "Shaheer Ahmad Khan",
      "Muhammad Usamah Shahid",
      "Ahmad Abdullah",
      "Ibrahim Hashmat",
      "Muddassar Farooq"
    ],
    "abstract": "This study addresses a critical gap in the healthcare system by developing a\nclinically meaningful, practical, and explainable disease surveillance system\nfor multiple chronic diseases, utilizing routine EHR data from multiple U.S.\npractices integrated with CureMD's EMR/EHR system. Unlike traditional\nsystems--using AI models that rely on features from patients' labs--our\napproach focuses on routinely available data, such as medical history, vitals,\ndiagnoses, and medications, to preemptively assess the risks of chronic\ndiseases in the next year. We trained three distinct models for each chronic\ndisease: prediction models that forecast the risk of a disease 3, 6, and 12\nmonths before a potential diagnosis. We developed Random Forest models, which\nwere internally validated using F1 scores and AUROC as performance metrics and\nfurther evaluated by a panel of expert physicians for clinical relevance based\non inferences grounded in medical knowledge. Additionally, we discuss our\nimplementation of integrating these models into a practical EMR system. Beyond\nusing Shapley attributes and surrogate models for explainability, we also\nintroduce a new rule-engineering framework to enhance the intrinsic\nexplainability of Random Forests.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15969v1",
    "published_date": "2025-01-27 11:26:54 UTC",
    "updated_date": "2025-01-27 11:26:54 UTC"
  },
  {
    "arxiv_id": "2501.15968v1",
    "title": "Multi-View Attention Syntactic Enhanced Graph Convolutional Network for Aspect-based Sentiment Analysis",
    "authors": [
      "Xiang Huang",
      "Hao Peng",
      "Shuo Sun",
      "Zhifeng Hao",
      "Hui Lin",
      "Shuhai Wang"
    ],
    "abstract": "Aspect-based Sentiment Analysis (ABSA) is the task aimed at predicting the\nsentiment polarity of aspect words within sentences. Recently, incorporating\ngraph neural networks (GNNs) to capture additional syntactic structure\ninformation in the dependency tree derived from syntactic dependency parsing\nhas been proven to be an effective paradigm for boosting ABSA. Despite GNNs\nenhancing model capability by fusing more types of information, most works only\nutilize a single topology view of the dependency tree or simply conflate\ndifferent perspectives of information without distinction, which limits the\nmodel performance. To address these challenges, in this paper, we propose a new\nmulti-view attention syntactic enhanced graph convolutional network (MASGCN)\nthat weighs different syntactic information of views using attention\nmechanisms. Specifically, we first construct distance mask matrices from the\ndependency tree to obtain multiple subgraph views for GNNs. To aggregate\nfeatures from different views, we propose a multi-view attention mechanism to\ncalculate the attention weights of views. Furthermore, to incorporate more\nsyntactic information, we fuse the dependency type information matrix into the\nadjacency matrices and present a structural entropy loss to learn the\ndependency type adjacency matrix. Comprehensive experiments on four benchmark\ndatasets demonstrate that our model outperforms state-of-the-art methods. The\ncodes and datasets are available at https://github.com/SELGroup/MASGCN.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This paper is accepted by DASFAA 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.15968v1",
    "published_date": "2025-01-27 11:26:13 UTC",
    "updated_date": "2025-01-27 11:26:13 UTC"
  },
  {
    "arxiv_id": "2501.15963v1",
    "title": "Evaluating Data Influence in Meta Learning",
    "authors": [
      "Chenyang Ren",
      "Huanyi Xie",
      "Shu Yang",
      "Meng Ding",
      "Lijie Hu",
      "Di Wang"
    ],
    "abstract": "As one of the most fundamental models, meta learning aims to effectively\naddress few-shot learning challenges. However, it still faces significant\nissues related to the training data, such as training inefficiencies due to\nnumerous low-contribution tasks in large datasets and substantial noise from\nincorrect labels. Thus, training data attribution methods are needed for meta\nlearning. However, the dual-layer structure of mata learning complicates the\nmodeling of training data contributions because of the interdependent influence\nbetween meta-parameters and task-specific parameters, making existing data\ninfluence evaluation tools inapplicable or inaccurate. To address these\nchallenges, based on the influence function, we propose a general data\nattribution evaluation framework for meta-learning within the bilevel\noptimization framework. Our approach introduces task influence functions\n(task-IF) and instance influence functions (instance-IF) to accurately assess\nthe impact of specific tasks and individual data points in closed forms. This\nframework comprehensively models data contributions across both the inner and\nouter training processes, capturing the direct effects of data points on\nmeta-parameters as well as their indirect influence through task-specific\nparameters. We also provide several strategies to enhance computational\nefficiency and scalability. Experimental results demonstrate the framework's\neffectiveness in training data evaluation via several downstream tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15963v1",
    "published_date": "2025-01-27 11:14:04 UTC",
    "updated_date": "2025-01-27 11:14:04 UTC"
  },
  {
    "arxiv_id": "2501.15928v1",
    "title": "Generative AI for Lyapunov Optimization Theory in UAV-based Low-Altitude Economy Networking",
    "authors": [
      "Zhang Liu",
      "Dusit Niyato",
      "Jiacheng Wang",
      "Geng Sun",
      "Lianfen Huang",
      "Zhibin Gao",
      "Xianbin Wang"
    ],
    "abstract": "Lyapunov optimization theory has recently emerged as a powerful mathematical\nframework for solving complex stochastic optimization problems by transforming\nlong-term objectives into a sequence of real-time short-term decisions while\nensuring system stability. This theory is particularly valuable in unmanned\naerial vehicle (UAV)-based low-altitude economy (LAE) networking scenarios,\nwhere it could effectively address inherent challenges of dynamic network\nconditions, multiple optimization objectives, and stability requirements.\nRecently, generative artificial intelligence (GenAI) has garnered significant\nattention for its unprecedented capability to generate diverse digital content.\nExtending beyond content generation, in this paper, we propose a framework\nintegrating generative diffusion models with reinforcement learning to address\nLyapunov optimization problems in UAV-based LAE networking. We begin by\nintroducing the fundamentals of Lyapunov optimization theory and analyzing the\nlimitations of both conventional methods and traditional AI-enabled approaches.\nWe then examine various GenAI models and comprehensively analyze their\npotential contributions to Lyapunov optimization. Subsequently, we develop a\nLyapunov-guided generative diffusion model-based reinforcement learning\nframework and validate its effectiveness through a UAV-based LAE networking\ncase study. Finally, we outline several directions for future research.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "8 pages, 5 figures, magazine paper",
    "pdf_url": "http://arxiv.org/pdf/2501.15928v1",
    "published_date": "2025-01-27 10:27:15 UTC",
    "updated_date": "2025-01-27 10:27:15 UTC"
  },
  {
    "arxiv_id": "2501.15916v1",
    "title": "Online Housing Market",
    "authors": [
      "Julien Lesca"
    ],
    "abstract": "This paper studies an online variant of the celebrated housing market\nproblem, where each agent has a single house and seeks to exchange it for\nanother based on her preferences. In this online setting, agents may arrive and\ndepart at any time, meaning that not all agents are present on the housing\nmarket simultaneously. I extend the well known serial dictatorship and Gale s\ntop trading cycle mechanisms to this online scenario, aiming to retain their\ndesirable properties such as Pareto efficiency, individual rationality, and\nstrategy proofness. These extensions also seek to prevent agents from\nstrategically delaying their arrival or advancing their departure. I\ndemonstrate that achieving all of these properties simultaneously is impossible\nin the online context, and I present several variants that achieve different\nsubsets of these properties.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15916v1",
    "published_date": "2025-01-27 10:05:49 UTC",
    "updated_date": "2025-01-27 10:05:49 UTC"
  },
  {
    "arxiv_id": "2501.15908v1",
    "title": "Evidential Physics-Informed Neural Networks",
    "authors": [
      "Hai Siong Tan",
      "Kuancheng Wang",
      "Rafe McBeth"
    ],
    "abstract": "We present a novel class of Physics-Informed Neural Networks that is\nformulated based on the principles of Evidential Deep Learning, where the model\nincorporates uncertainty quantification by learning parameters of a\nhigher-order distribution. The dependent and trainable variables of the PDE\nresidual loss and data-fitting loss terms are recast as functions of the\nhyperparameters of an evidential prior distribution. Our model is equipped with\nan information-theoretic regularizer that contains the Kullback-Leibler\ndivergence between two inverse-gamma distributions characterizing predictive\nuncertainty. Relative to Bayesian-Physics-Informed-Neural-Networks, our\nframework appeared to exhibit higher sensitivity to data noise, preserve\nboundary conditions more faithfully and yield empirical coverage probabilities\ncloser to nominal ones. Toward examining its relevance for data mining in\nscientific discoveries, we demonstrate how to apply our model to inverse\nproblems involving 1D and 2D nonlinear differential equations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for International Conference on Scientific Computing and\n  Machine Learning (SCML) 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.15908v1",
    "published_date": "2025-01-27 10:01:10 UTC",
    "updated_date": "2025-01-27 10:01:10 UTC"
  },
  {
    "arxiv_id": "2501.15890v3",
    "title": "Complexity in Complexity: Understanding Visual Complexity Through Structure, Color, and Surprise",
    "authors": [
      "Karahan Sarıtaş",
      "Peter Dayan",
      "Tingke Shen",
      "Surabhi S Nath"
    ],
    "abstract": "Understanding how humans perceive visual complexity is a key area of study in\nvisual cognition. Previous approaches to modeling visual complexity assessments\nhave often resulted in intricate, difficult-to-interpret algorithms that employ\nnumerous features or sophisticated deep learning architectures. While these\ncomplex models achieve high performance on specific datasets, they often\nsacrifice interpretability, making it challenging to understand the factors\ndriving human perception of complexity. Recently (Shen, et al. 2024) proposed\nan interpretable segmentation-based model that accurately predicted complexity\nacross various datasets, supporting the idea that complexity can be explained\nsimply. In this work, we investigate the failure of their model to capture\nstructural, color and surprisal contributions to complexity. To this end, we\npropose Multi-Scale Sobel Gradient (MSG) which measures spatial intensity\nvariations, Multi-Scale Unique Color (MUC) which quantifies colorfulness across\nmultiple scales, and surprise scores generated using a Large Language Model. We\ntest our features on existing benchmarks and a novel dataset (Surprising Visual\nGenome) containing surprising images from Visual Genome. Our experiments\ndemonstrate that modeling complexity accurately is not as simple as previously\nthought, requiring additional perceptual and semantic factors to address\ndataset biases. Our model improves predictive performance while maintaining\ninterpretability, offering deeper insights into how visual complexity is\nperceived and assessed. Our code, analysis and data are available at\nhttps://github.com/Complexity-Project/Complexity-in-Complexity.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15890v3",
    "published_date": "2025-01-27 09:32:56 UTC",
    "updated_date": "2025-03-20 12:06:51 UTC"
  },
  {
    "arxiv_id": "2501.15889v4",
    "title": "Adaptive Width Neural Networks",
    "authors": [
      "Federico Errica",
      "Henrik Christiansen",
      "Viktor Zaverkin",
      "Mathias Niepert",
      "Francesco Alesiani"
    ],
    "abstract": "For almost 70 years, researchers have mostly relied on hyper-parameter tuning\nto select the width of neural networks' layers. This paper challenges the\nstatus quo by introducing an easy-to-use technique to learn an unbounded width\nof a neural network's layer during training. The technique does not rely on\nalternate optimization nor hand-crafted gradient heuristics; rather, it jointly\noptimizes the width and the parameters of each layer via simple\nbackpropagation. We apply the technique to a broad range of data domains such\nas tables, images, text, sequences, and graphs, showing how the width adapts to\nthe task's difficulty. The method imposes a soft ordering of importance among\nneurons, by which it also is possible to truncate the trained network at\nvirtually zero cost, achieving a smooth trade-off between performance and\ncompute resources in a structured way. Alternatively, one can dynamically\ncompress the network with no performance degradation. In light of recent\nfoundation models trained on large datasets, believed to require billions of\nparameters and where hyper-parameter tuning is unfeasible due to humongous\ntraining costs, our approach stands as a viable alternative for width learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15889v4",
    "published_date": "2025-01-27 09:25:56 UTC",
    "updated_date": "2025-05-21 14:46:05 UTC"
  },
  {
    "arxiv_id": "2501.16404v1",
    "title": "DynaPrompt: Dynamic Test-Time Prompt Tuning",
    "authors": [
      "Zehao Xiao",
      "Shilin Yan",
      "Jack Hong",
      "Jiayin Cai",
      "Xiaolong Jiang",
      "Yao Hu",
      "Jiayi Shen",
      "Qi Wang",
      "Cees G. M. Snoek"
    ],
    "abstract": "Test-time prompt tuning enhances zero-shot generalization of vision-language\nmodels but tends to ignore the relatedness among test samples during inference.\nOnline test-time prompt tuning provides a simple way to leverage the\ninformation in previous test samples, albeit with the risk of prompt collapse\ndue to error accumulation. To enhance test-time prompt tuning, we propose\nDynaPrompt, short for dynamic test-time prompt tuning, exploiting relevant data\ndistribution information while reducing error accumulation. Built on an online\nprompt buffer, DynaPrompt adaptively selects and optimizes the relevant prompts\nfor each test sample during tuning. Specifically, we introduce a dynamic prompt\nselection strategy based on two metrics: prediction entropy and probability\ndifference. For unseen test data information, we develop dynamic prompt\nappending, which allows the buffer to append new prompts and delete the\ninactive ones. By doing so, the prompts are optimized to exploit beneficial\ninformation on specific test data, while alleviating error accumulation.\nExperiments on fourteen datasets demonstrate the effectiveness of dynamic\ntest-time prompt tuning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.16404v1",
    "published_date": "2025-01-27 09:10:06 UTC",
    "updated_date": "2025-01-27 09:10:06 UTC"
  },
  {
    "arxiv_id": "2501.16403v1",
    "title": "Is Open Source the Future of AI? A Data-Driven Approach",
    "authors": [
      "Domen Vake",
      "Bogdan Šinik",
      "Jernej Vičič",
      "Aleksandar Tošić"
    ],
    "abstract": "Large Language Models (LLMs) have become central in academia and industry,\nraising concerns about privacy, transparency, and misuse. A key issue is the\ntrustworthiness of proprietary models, with open-sourcing often proposed as a\nsolution. However, open-sourcing presents challenges, including potential\nmisuse, financial disincentives, and intellectual property concerns.\nProprietary models, backed by private sector resources, are better positioned\nfor return on investment.\n  There are also other approaches that lie somewhere on the spectrum between\ncompletely open-source and proprietary. These can largely be categorised into\nopen-source usage limitations protected by licensing, partially open-source\n(open weights) models, hybrid approaches where obsolete model versions are\nopen-sourced, while competitive versions with market value remain proprietary.\n  Currently, discussions on where on the spectrum future models should fall on\nremains unbacked and mostly opinionated where industry leaders are weighing in\non the discussion. In this paper, we present a data-driven approach by\ncompiling data on open-source development of LLMs, and their contributions in\nterms of improvements, modifications, and methods. Our goal is to avoid\nsupporting either extreme but rather present data that will support future\ndiscussions both by industry experts as well as policy makers.\n  Our findings indicate that open-source contributions can enhance model\nperformance, with trends such as reduced model size and manageable accuracy\nloss. We also identify positive community engagement patterns and architectures\nthat benefit most from open contributions.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16403v1",
    "published_date": "2025-01-27 09:03:49 UTC",
    "updated_date": "2025-01-27 09:03:49 UTC"
  },
  {
    "arxiv_id": "2501.15877v3",
    "title": "Boli: A dataset for understanding stuttering experience and analyzing stuttered speech",
    "authors": [
      "Ashita Batra",
      "Mannas Narang",
      "Neeraj Kumar Sharma",
      "Pradip K Das"
    ],
    "abstract": "There is a growing need for diverse, high-quality stuttered speech data,\nparticularly in the context of Indian languages. This paper introduces Project\nBoli, a multi-lingual stuttered speech dataset designed to advance scientific\nunderstanding and technology development for individuals who stutter,\nparticularly in India. The dataset constitutes (a) anonymized metadata (gender,\nage, country, mother tongue) and responses to a questionnaire about how\nstuttering affects their daily lives, (b) captures both read speech (using the\nRainbow Passage) and spontaneous speech (through image description tasks) for\neach participant and (c) includes detailed annotations of five stutter types:\nblocks, prolongations, interjections, sound repetitions and word repetitions.\nWe present a comprehensive analysis of the dataset, including the data\ncollection procedure, experience summarization of people who stutter, severity\nassessment of stuttering events and technical validation of the collected data.\nThe dataset is released as an open access to further speech technology\ndevelopment.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15877v3",
    "published_date": "2025-01-27 09:03:28 UTC",
    "updated_date": "2025-05-01 09:38:34 UTC"
  },
  {
    "arxiv_id": "2501.15876v1",
    "title": "Optimizing Sentence Embedding with Pseudo-Labeling and Model Ensembles: A Hierarchical Framework for Enhanced NLP Tasks",
    "authors": [
      "Ziwei Liu",
      "Qi Zhang",
      "Lifu Gao"
    ],
    "abstract": "Sentence embedding tasks are important in natural language processing (NLP),\nbut improving their performance while keeping them reliable is still hard. This\npaper presents a framework that combines pseudo-label generation and model\nensemble techniques to improve sentence embeddings. We use external data from\nSimpleWiki, Wikipedia, and BookCorpus to make sure the training data is\nconsistent. The framework includes a hierarchical model with an encoding layer,\nrefinement layer, and ensemble prediction layer, using ALBERT-xxlarge,\nRoBERTa-large, and DeBERTa-large models. Cross-attention layers combine\nexternal context, and data augmentation techniques like synonym replacement and\nback-translation increase data variety. Experimental results show large\nimprovements in accuracy and F1-score compared to basic models, and studies\nconfirm that cross-attention and data augmentation make a difference. This work\npresents an effective way to improve sentence embedding tasks and lays the\ngroundwork for future NLP research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15876v1",
    "published_date": "2025-01-27 09:02:42 UTC",
    "updated_date": "2025-01-27 09:02:42 UTC"
  },
  {
    "arxiv_id": "2501.15870v1",
    "title": "D-PLS: Decoupled Semantic Segmentation for 4D-Panoptic-LiDAR-Segmentation",
    "authors": [
      "Maik Steinhauser",
      "Laurenz Reichardt",
      "Nikolas Ebert",
      "Oliver Wasenmüller"
    ],
    "abstract": "This paper introduces a novel approach to 4D Panoptic LiDAR Segmentation that\ndecouples semantic and instance segmentation, leveraging single-scan semantic\npredictions as prior information for instance segmentation. Our method D-PLS\nfirst performs single-scan semantic segmentation and aggregates the results\nover time, using them to guide instance segmentation. The modular design of\nD-PLS allows for seamless integration on top of any semantic segmentation\narchitecture, without requiring architectural changes or retraining. We\nevaluate our approach on the SemanticKITTI dataset, where it demonstrates\nsignificant improvements over the baseline in both classification and\nassociation tasks, as measured by the LiDAR Segmentation and Tracking Quality\n(LSTQ) metric. Furthermore, we show that our decoupled architecture not only\nenhances instance prediction but also surpasses the baseline due to\nadvancements in single-scan semantic segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15870v1",
    "published_date": "2025-01-27 08:46:22 UTC",
    "updated_date": "2025-01-27 08:46:22 UTC"
  },
  {
    "arxiv_id": "2501.15865v2",
    "title": "Transfer of Knowledge through Reverse Annealing: A Preliminary Analysis of the Benefits and What to Share",
    "authors": [
      "Eneko Osaba",
      "Esther Villar-Rodriguez"
    ],
    "abstract": "Being immersed in the NISQ-era, current quantum annealers present limitations\nfor solving optimization problems efficiently. To mitigate these limitations,\nD-Wave Systems developed a mechanism called Reverse Annealing, a specific type\nof quantum annealing designed to perform local refinement of good states found\nelsewhere. Despite the research activity around Reverse Annealing, none has\ntheorized about the possible benefits related to the transfer of knowledge\nunder this paradigm. This work moves in that direction and is driven by\nexperimentation focused on answering two key research questions: i) is reverse\nannealing a paradigm that can benefit from knowledge transfer between similar\nproblems? and ii) can we infer the characteristics that an input solution\nshould meet to help increase the probability of success? To properly guide the\ntests in this paper, the well-known Knapsack Problem has been chosen for\nbenchmarking purposes, using a total of 34 instances composed of 14 and 16\nitems.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "quant-ph",
    "comment": "13 pages, 2 figures and 2 tables. Paper submitted to Frontiers in\n  Physics journal",
    "pdf_url": "http://arxiv.org/pdf/2501.15865v2",
    "published_date": "2025-01-27 08:42:40 UTC",
    "updated_date": "2025-04-11 10:03:59 UTC"
  },
  {
    "arxiv_id": "2501.15857v5",
    "title": "Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?",
    "authors": [
      "Yutong Yin",
      "Zhaoran Wang"
    ],
    "abstract": "Humans exhibit remarkable compositional reasoning by integrating knowledge\nfrom various sources. For example, if someone learns ( B = f(A) ) from one\nsource and ( C = g(B) ) from another, they can deduce ( C=g(B)=g(f(A)) ) even\nwithout encountering ( ABC ) together, showcasing the generalization ability of\nhuman intelligence. In this paper, we introduce a synthetic learning task,\n\"FTCT\" (Fragmented at Training, Chained at Testing), to validate the potential\nof Transformers in replicating this skill and interpret its inner mechanism. In\nthe training phase, data consist of separated knowledge fragments from an\noverall causal graph. During testing, Transformers must infer complete causal\ngraph traces by integrating these fragments. Our findings demonstrate that\nfew-shot Chain-of-Thought prompting enables Transformers to perform\ncompositional reasoning on FTCT by revealing correct combinations of fragments,\neven if such combinations were absent in the training data. Furthermore, the\nemergence of compositional reasoning ability is strongly correlated with the\nmodel complexity and training-testing data similarity. We propose, both\ntheoretically and empirically, that Transformers learn an underlying\ngeneralizable program from training, enabling effective compositional reasoning\nduring testing.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.15857v5",
    "published_date": "2025-01-27 08:34:38 UTC",
    "updated_date": "2025-05-13 00:04:47 UTC"
  },
  {
    "arxiv_id": "2501.15842v1",
    "title": "Beyond In-Distribution Performance: A Cross-Dataset Study of Trajectory Prediction Robustness",
    "authors": [
      "Yue Yao",
      "Daniel Goehring",
      "Joerg Reichardt"
    ],
    "abstract": "We study the Out-of-Distribution (OoD) generalization ability of three SotA\ntrajectory prediction models with comparable In-Distribution (ID) performance\nbut different model designs. We investigate the influence of inductive bias,\nsize of training data and data augmentation strategy by training the models on\nArgoverse 2 (A2) and testing on Waymo Open Motion (WO) and vice versa. We find\nthat the smallest model with highest inductive bias exhibits the best OoD\ngeneralization across different augmentation strategies when trained on the\nsmaller A2 dataset and tested on the large WO dataset. In the converse setting,\ntraining all models on the larger WO dataset and testing on the smaller A2\ndataset, we find that all models generalize poorly, even though the model with\nthe highest inductive bias still exhibits the best generalization ability. We\ndiscuss possible reasons for this surprising finding and draw conclusions about\nthe design and test of trajectory prediction models and benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2407.13431",
    "pdf_url": "http://arxiv.org/pdf/2501.15842v1",
    "published_date": "2025-01-27 08:08:17 UTC",
    "updated_date": "2025-01-27 08:08:17 UTC"
  },
  {
    "arxiv_id": "2501.15838v1",
    "title": "CrySPAI: A new Crystal Structure Prediction Software Based on Artificial Intelligence",
    "authors": [
      "Zongguo Wang",
      "Ziyi Chen",
      "Yang Yuan",
      "Yangang Wang"
    ],
    "abstract": "Crystal structure predictions based on the combination of first-principles\ncalculations and machine learning have achieved significant success in\nmaterials science. However, most of these approaches are limited to predicting\nspecific systems, which hinders their application to unknown or unexplored\ndomains. In this paper, we present CrySPAI, a crystal structure prediction\npackage developed using artificial intelligence (AI) to predict energetically\nstable crystal structures of inorganic materials given their chemical\ncompositions. The software consists of three key modules, an evolutionary\noptimization algorithm (EOA) that searches for all possible crystal structure\nconfigurations, density functional theory (DFT) that provides the accurate\nenergy values for these structures, and a deep neural network (DNN) that learns\nthe relationship between crystal structures and their corresponding energies.\nTo optimize the process across these modules, a distributed framework is\nimplemented to parallelize tasks, and an automated workflow has been integrated\ninto CrySPAI for seamless execution. This paper reports the development and\nimplementation of AI AI-based CrySPAI Crystal Prediction Software tool and its\nunique features.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15838v1",
    "published_date": "2025-01-27 07:53:06 UTC",
    "updated_date": "2025-01-27 07:53:06 UTC"
  },
  {
    "arxiv_id": "2501.15836v2",
    "title": "Intelligent Code Embedding Framework for High-Precision Ransomware Detection via Multimodal Execution Path Analysis",
    "authors": [
      "Levi Gareth",
      "Maximilian Fairbrother",
      "Peregrine Blackwood",
      "Lucasta Underhill",
      "Benedict Ruthermore"
    ],
    "abstract": "Modern threat landscapes continue to evolve with increasing sophistication,\nchallenging traditional detection methodologies and necessitating innovative\nsolutions capable of addressing complex adversarial tactics. A novel framework\nwas developed to identify ransomware activity through multimodal execution path\nanalysis, integrating high-dimensional embeddings and dynamic heuristic\nderivation mechanisms to capture behavioral patterns across diverse attack\nvariants. The approach demonstrated high adaptability, effectively mitigating\nobfuscation strategies and polymorphic characteristics often employed by\nransomware families to evade detection. Comprehensive experimental evaluations\nrevealed significant advancements in precision, recall, and accuracy metrics\ncompared to baseline techniques, particularly under conditions of variable\nencryption speeds and obfuscated execution flows. The framework achieved\nscalable and computationally efficient performance, ensuring robust\napplicability across a range of system configurations, from\nresource-constrained environments to high-performance infrastructures. Notable\nfindings included reduced false positive rates and enhanced detection latency,\neven for ransomware families employing sophisticated encryption mechanisms. The\nmodular design allowed seamless integration of additional modalities, enabling\nextensibility and future-proofing against emerging threat vectors. Quantitative\nanalyses further highlighted the system's energy efficiency, emphasizing its\npracticality for deployment in environments with stringent operational\nconstraints. The results underline the importance of integrating advanced\ncomputational techniques and dynamic adaptability to safeguard digital\necosystems from increasingly complex threats.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
    "pdf_url": "http://arxiv.org/pdf/2501.15836v2",
    "published_date": "2025-01-27 07:51:51 UTC",
    "updated_date": "2025-03-26 15:52:26 UTC"
  },
  {
    "arxiv_id": "2501.15830v5",
    "title": "SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model",
    "authors": [
      "Delin Qu",
      "Haoming Song",
      "Qizhi Chen",
      "Yuanqi Yao",
      "Xinyi Ye",
      "Yan Ding",
      "Zhigang Wang",
      "JiaYuan Gu",
      "Bin Zhao",
      "Dong Wang",
      "Xuelong Li"
    ],
    "abstract": "In this paper, we claim that spatial understanding is the keypoint in robot\nmanipulation, and propose SpatialVLA to explore effective spatial\nrepresentations for the robot foundation model. Specifically, we introduce\nEgo3D Position Encoding to inject 3D information into the input observations of\nthe visual-language-action model, and propose Adaptive Action Grids to\nrepresent spatial robot movement actions with adaptive discretized action\ngrids, facilitating learning generalizable and transferrable spatial action\nknowledge for cross-robot control. SpatialVLA is first pre-trained on top of a\nvision-language model with 1.1 Million real-world robot episodes, to learn a\ngeneralist manipulation policy across multiple robot environments and tasks.\nAfter pre-training, SpatialVLA is directly applied to perform numerous tasks in\na zero-shot manner. The superior results in both simulation and real-world\nrobots demonstrate its advantage of inferring complex robot motion trajectories\nand its strong in-domain multi-task generalization ability. We further show the\nproposed Adaptive Action Grids offer a new and effective way to fine-tune the\npre-trained SpatialVLA model for new simulation and real-world setups, where\nthe pre-learned action grids are re-discretized to capture robot-specific\nspatial action movements of new setups. The superior results from extensive\nevaluations demonstrate the exceptional in-distribution generalization and\nout-of-distribution adaptation capability, highlighting the crucial benefit of\nthe proposed spatial-aware representations for generalist robot policy\nlearning. All the details and codes will be open-sourced.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15830v5",
    "published_date": "2025-01-27 07:34:33 UTC",
    "updated_date": "2025-05-19 02:40:18 UTC"
  },
  {
    "arxiv_id": "2501.15820v1",
    "title": "FuzzyLight: A Robust Two-Stage Fuzzy Approach for Traffic Signal Control Works in Real Cities",
    "authors": [
      "Mingyuan Li",
      "Jiahao Wang",
      "Bo Du",
      "Jun Shen",
      "Qiang Wu"
    ],
    "abstract": "Effective traffic signal control (TSC) is crucial in mitigating urban\ncongestion and reducing emissions. Recently, reinforcement learning (RL) has\nbeen the research trend for TSC. However, existing RL algorithms face several\nreal-world challenges that hinder their practical deployment in TSC: (1) Sensor\naccuracy deteriorates with increased sensor detection range, and data\ntransmission is prone to noise, potentially resulting in unsafe TSC decisions.\n(2) During the training of online RL, interactions with the environment could\nbe unstable, potentially leading to inappropriate traffic signal phase (TSP)\nselection and traffic congestion. (3) Most current TSC algorithms focus only on\nTSP decisions, overlooking the critical aspect of phase duration, affecting\nsafety and efficiency. To overcome these challenges, we propose a robust\ntwo-stage fuzzy approach called FuzzyLight, which integrates compressed sensing\nand RL for TSC deployment. FuzzyLight offers several key contributions: (1) It\nemploys fuzzy logic and compressed sensing to address sensor noise and enhances\nthe efficiency of TSP decisions. (2) It maintains stable performance during\ntraining and combines fuzzy logic with RL to generate precise phases. (3) It\nworks in real cities across 22 intersections and demonstrates superior\nperformance in both real-world and simulated environments. Experimental results\nindicate that FuzzyLight enhances traffic efficiency by 48% compared to\nexpert-designed timings in the real world. Furthermore, it achieves\nstate-of-the-art (SOTA) performance in simulated environments using six\nreal-world datasets with transmission noise. The code and deployment video are\navailable at the URL1",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15820v1",
    "published_date": "2025-01-27 06:55:47 UTC",
    "updated_date": "2025-01-27 06:55:47 UTC"
  },
  {
    "arxiv_id": "2501.15817v1",
    "title": "Long-Term Interest Clock: Fine-Grained Time Perception in Streaming Recommendation System",
    "authors": [
      "Yongchun Zhu",
      "Guanyu Jiang",
      "Jingwu Chen",
      "Feng Zhang",
      "Xiao Yang",
      "Zuotao Liu"
    ],
    "abstract": "User interests manifest a dynamic pattern within the course of a day, e.g., a\nuser usually favors soft music at 8 a.m. but may turn to ambient music at 10\np.m. To model dynamic interests in a day, hour embedding is widely used in\ntraditional daily-trained industrial recommendation systems. However, its\ndiscreteness can cause periodical online patterns and instability in recent\nstreaming recommendation systems. Recently, Interest Clock has achieved\nremarkable performance in streaming recommendation systems. Nevertheless, it\nmodels users' dynamic interests in a coarse-grained manner, merely encoding\nusers' discrete interests of 24 hours from short-term behaviors. In this paper,\nwe propose a fine-grained method for perceiving time information for streaming\nrecommendation systems, named Long-term Interest Clock (LIC). The key idea of\nLIC is adaptively calculating current user interests by taking into\nconsideration the relevance of long-term behaviors around current time (e.g., 8\na.m.) given a candidate item. LIC consists of two modules: (1) Clock-GSU\nretrieves a sub-sequence by searching through long-term behaviors, using query\ninformation from a candidate item and current time, (2) Clock-ESU employs a\ntime-gap-aware attention mechanism to aggregate sub-sequence with the candidate\nitem. With Clock-GSU and Clock-ESU, LIC is capable of capturing users' dynamic\nfine-grained interests from long-term behaviors. We conduct online A/B tests,\nobtaining +0.122% improvements on user active days. Besides, the extended\noffline experiments show improvements as well. Long-term Interest Clock has\nbeen integrated into Douyin Music App's recommendation system.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by WWW2025",
    "pdf_url": "http://arxiv.org/pdf/2501.15817v1",
    "published_date": "2025-01-27 06:52:50 UTC",
    "updated_date": "2025-01-27 06:52:50 UTC"
  },
  {
    "arxiv_id": "2501.15816v1",
    "title": "AdaF^2M^2: Comprehensive Learning and Responsive Leveraging Features in Recommendation System",
    "authors": [
      "Yongchun Zhu",
      "Jingwu Chen",
      "Ling Chen",
      "Yitan Li",
      "Feng Zhang",
      "Xiao Yang",
      "Zuotao Liu"
    ],
    "abstract": "Feature modeling, which involves feature representation learning and\nleveraging, plays an essential role in industrial recommendation systems.\nHowever, the data distribution in real-world applications usually follows a\nhighly skewed long-tail pattern due to the popularity bias, which easily leads\nto over-reliance on ID-based features, such as user/item IDs and ID sequences\nof interactions. Such over-reliance makes it hard for models to learn features\ncomprehensively, especially for those non-ID meta features, e.g., user/item\ncharacteristics. Further, it limits the feature leveraging ability in models,\ngetting less generalized and more susceptible to data noise. Previous studies\non feature modeling focus on feature extraction and interaction, hardly\nnoticing the problems brought about by the long-tail data distribution. To\nachieve better feature representation learning and leveraging on real-world\ndata, we propose a model-agnostic framework AdaF^2M^2, short for Adaptive\nFeature Modeling with Feature Mask. The feature-mask mechanism helps\ncomprehensive feature learning via multi-forward training with augmented\nsamples, while the adapter applies adaptive weights on features responsive to\ndifferent user/item states. By arming base models with AdaF^2M^2, we conduct\nonline A/B tests on multiple recommendation scenarios, obtaining +1.37% and\n+1.89% cumulative improvements on user active days and app duration\nrespectively. Besides, the extended offline experiments on different models\nshow improvements as well. AdaF$^2$M$^2$ has been widely deployed on both\nretrieval and ranking tasks in multiple applications of Douyin Group,\nindicating its superior effectiveness and universality.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by DASFAA2025",
    "pdf_url": "http://arxiv.org/pdf/2501.15816v1",
    "published_date": "2025-01-27 06:49:27 UTC",
    "updated_date": "2025-01-27 06:49:27 UTC"
  },
  {
    "arxiv_id": "2502.07794v1",
    "title": "Regulatory Science Innovation for Generative AI and Large Language Models in Health and Medicine: A Global Call for Action",
    "authors": [
      "Jasmine Chiat Ling Ong",
      "Yilin Ning",
      "Mingxuan Liu",
      "Yian Ma",
      "Zhao Liang",
      "Kuldev Singh",
      "Robert T Chang",
      "Silke Vogel",
      "John CW Lim",
      "Iris Siu Kwan Tan",
      "Oscar Freyer",
      "Stephen Gilbert",
      "Danielle S Bitterman",
      "Xiaoxuan Liu",
      "Alastair K Denniston",
      "Nan Liu"
    ],
    "abstract": "The integration of generative AI (GenAI) and large language models (LLMs) in\nhealthcare presents both unprecedented opportunities and challenges,\nnecessitating innovative regulatory approaches. GenAI and LLMs offer broad\napplications, from automating clinical workflows to personalizing diagnostics.\nHowever, the non-deterministic outputs, broad functionalities and complex\nintegration of GenAI and LLMs challenge existing medical device regulatory\nframeworks, including the total product life cycle (TPLC) approach. Here we\ndiscuss the constraints of the TPLC approach to GenAI and LLM-based medical\ndevice regulation, and advocate for global collaboration in regulatory science\nresearch. This serves as the foundation for developing innovative approaches\nincluding adaptive policies and regulatory sandboxes, to test and refine\ngovernance in real-world settings. International harmonization, as seen with\nthe International Medical Device Regulators Forum, is essential to manage\nimplications of LLM on global health, including risks of widening health\ninequities driven by inherent model biases. By engaging multidisciplinary\nexpertise, prioritizing iterative, data-driven approaches, and focusing on the\nneeds of diverse populations, global regulatory science research enables the\nresponsible and equitable advancement of LLM innovations in healthcare.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.07794v1",
    "published_date": "2025-01-27 06:21:13 UTC",
    "updated_date": "2025-01-27 06:21:13 UTC"
  },
  {
    "arxiv_id": "2501.15802v1",
    "title": "Adaptive AI-based Decentralized Resource Management in the Cloud-Edge Continuum",
    "authors": [
      "Lanpei Li",
      "Jack Bell",
      "Massimo Coppola",
      "Vincenzo Lomonaco"
    ],
    "abstract": "The increasing complexity of application requirements and the dynamic nature\nof the Cloud-Edge Continuum present significant challenges for efficient\nresource management. These challenges stem from the ever-changing\ninfrastructure, which is characterized by additions, removals, and\nreconfigurations of nodes and links, as well as the variability of application\nworkloads. Traditional centralized approaches struggle to adapt to these\nchanges due to their static nature, while decentralized solutions face\nchallenges such as limited global visibility and coordination overhead. This\npaper proposes a hybrid decentralized framework for dynamic application\nplacement and resource management. The framework utilizes Graph Neural Networks\n(GNNs) to embed resource and application states, enabling comprehensive\nrepresentation and efficient decision-making. It employs a collaborative\nmulti-agent reinforcement learning (MARL) approach, where local agents optimize\nresource management in their neighborhoods and a global orchestrator ensures\nsystem-wide coordination. By combining decentralized application placement with\ncentralized oversight, our framework addresses the scalability, adaptability,\nand accuracy challenges inherent in the Cloud-Edge Continuum. This work\ncontributes to the development of decentralized application placement\nstrategies, the integration of GNN embeddings, and collaborative MARL systems,\nproviding a foundation for efficient, adaptive and scalable resource\nmanagement.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15802v1",
    "published_date": "2025-01-27 06:07:09 UTC",
    "updated_date": "2025-01-27 06:07:09 UTC"
  },
  {
    "arxiv_id": "2501.18624v2",
    "title": "Membership Inference Attacks Against Vision-Language Models",
    "authors": [
      "Yuke Hu",
      "Zheng Li",
      "Zhihao Liu",
      "Yang Zhang",
      "Zhan Qin",
      "Kui Ren",
      "Chun Chen"
    ],
    "abstract": "Vision-Language Models (VLMs), built on pre-trained vision encoders and large\nlanguage models (LLMs), have shown exceptional multi-modal understanding and\ndialog capabilities, positioning them as catalysts for the next technological\nrevolution. However, while most VLM research focuses on enhancing multi-modal\ninteraction, the risks of data misuse and leakage have been largely unexplored.\nThis prompts the need for a comprehensive investigation of such risks in VLMs.\nIn this paper, we conduct the first analysis of misuse and leakage detection in\nVLMs through the lens of membership inference attack (MIA). In specific, we\nfocus on the instruction tuning data of VLMs, which is more likely to contain\nsensitive or unauthorized information. To address the limitation of existing\nMIA methods, we introduce a novel approach that infers membership based on a\nset of samples and their sensitivity to temperature, a unique parameter in\nVLMs. Based on this, we propose four membership inference methods, each\ntailored to different levels of background knowledge, ultimately arriving at\nthe most challenging scenario. Our comprehensive evaluations show that these\nmethods can accurately determine membership status, e.g., achieving an AUC\ngreater than 0.8 targeting a small set consisting of only 5 samples on LLaVA.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by USENIX'25; 22 pages, 28 figures;",
    "pdf_url": "http://arxiv.org/pdf/2501.18624v2",
    "published_date": "2025-01-27 05:44:58 UTC",
    "updated_date": "2025-02-07 05:11:54 UTC"
  },
  {
    "arxiv_id": "2501.15791v2",
    "title": "Harnessing Diverse Perspectives: A Multi-Agent Framework for Enhanced Error Detection in Knowledge Graphs",
    "authors": [
      "Yu Li",
      "Yi Huang",
      "Guilin Qi",
      "Junlan Feng",
      "Nan Hu",
      "Songlin Zhai",
      "Haohan Xue",
      "Yongrui Chen",
      "Ruoyan Shen",
      "Tongtong Wu"
    ],
    "abstract": "Knowledge graphs are widely used in industrial applications, making error\ndetection crucial for ensuring the reliability of downstream applications.\nExisting error detection methods often fail to effectively utilize fine-grained\nsubgraph information and rely solely on fixed graph structures, while also\nlacking transparency in their decision-making processes, which results in\nsuboptimal detection performance. In this paper, we propose a novel Multi-Agent\nframework for Knowledge Graph Error Detection (MAKGED) that utilizes multiple\nlarge language models (LLMs) in a collaborative setting. By concatenating\nfine-grained, bidirectional subgraph embeddings with LLM-based query embeddings\nduring training, our framework integrates these representations to produce four\nspecialized agents. These agents utilize subgraph information from different\ndimensions to engage in multi-round discussions, thereby improving error\ndetection accuracy and ensuring a transparent decision-making process.\nExtensive experiments on FB15K and WN18RR demonstrate that MAKGED outperforms\nstate-of-the-art methods, enhancing the accuracy and robustness of KG\nevaluation. For specific industrial scenarios, our framework can facilitate the\ntraining of specialized agents using domain-specific knowledge graphs for error\ndetection, which highlights the potential industrial application value of our\nframework. Our code and datasets are available at\nhttps://github.com/kse-ElEvEn/MAKGED.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been ACCEPTED as a FULL PAPER at DASFAA 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.15791v2",
    "published_date": "2025-01-27 05:35:25 UTC",
    "updated_date": "2025-02-20 13:07:14 UTC"
  },
  {
    "arxiv_id": "2501.15781v1",
    "title": "Large Language Models to Diffusion Finetuning",
    "authors": [
      "Edoardo Cetin",
      "Tianyu Zhao",
      "Yujin Tang"
    ],
    "abstract": "We propose a new finetuning method to provide pre-trained large language\nmodels (LMs) the ability to scale test-time compute through the diffusion\nframework. By increasing the number of diffusion steps, we show our finetuned\nmodels achieve monotonically increasing accuracy, directly translating to\nimproved performance across downstream tasks. Furthermore, our finetuned models\ncan expertly answer questions on specific topics by integrating powerful\nguidance techniques, and autonomously determine the compute required for a\ngiven problem by leveraging adaptive ODE solvers. Our method is universally\napplicable to any foundation model pre-trained with a cross-entropy loss and\ndoes not modify any of its original weights, fully preserving its strong\nsingle-step generation capabilities. We show our method is more effective and\nfully compatible with traditional finetuning approaches, introducing an\northogonal new direction to unify the strengths of the autoregressive and\ndiffusion frameworks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint. 19 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.15781v1",
    "published_date": "2025-01-27 04:59:29 UTC",
    "updated_date": "2025-01-27 04:59:29 UTC"
  },
  {
    "arxiv_id": "2501.15767v2",
    "title": "Formal Verification of Markov Processes with Learned Parameters",
    "authors": [
      "Muhammad Maaz",
      "Timothy C. Y. Chan"
    ],
    "abstract": "We introduce the problem of formally verifying properties of Markov processes\nwhere the parameters are given by the output of machine learning models. For a\nbroad class of machine learning models, including linear models, tree-based\nmodels, and neural networks, verifying properties of Markov chains like\nreachability, hitting time, and total reward can be formulated as a bilinear\nprogram. We develop a decomposition and bound propagation scheme for solving\nthe bilinear program and show through computational experiments that our method\nsolves the problem to global optimality up to 100x faster than state-of-the-art\nsolvers. To demonstrate the practical utility of our approach, we apply it to a\nreal-world healthcare case study. Along with the paper, we release markovml, an\nopen-source tool for building Markov processes, integrating pretrained machine\nlearning models, and verifying their properties, available at\nhttps://github.com/mmaaz-git/markovml.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "68Q60 (primary) 90C30, 60J20, 60J22 (secondary)",
      "F.4.1; G.1.6; I.2.3"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages (main manuscript), 3 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2501.15767v2",
    "published_date": "2025-01-27 04:34:22 UTC",
    "updated_date": "2025-05-11 08:04:44 UTC"
  },
  {
    "arxiv_id": "2501.15757v2",
    "title": "Efficiency Bottlenecks of Convolutional Kolmogorov-Arnold Networks: A Comprehensive Scrutiny with ImageNet, AlexNet, LeNet and Tabular Classification",
    "authors": [
      "Ashim Dahal",
      "Saydul Akbar Murad",
      "Nick Rahimi"
    ],
    "abstract": "Algorithmic level developments like Convolutional Neural Networks,\ntransformers, attention mechanism, Retrieval Augmented Generation and so on\nhave changed Artificial Intelligence. Recent such development was observed by\nKolmogorov-Arnold Networks that suggested to challenge the fundamental concept\nof a Neural Network, thus change Multilayer Perceptron, and Convolutional\nNeural Networks. They received a good reception in terms of scientific\nmodeling, yet had some drawbacks in terms of efficiency. In this paper, we\ntrain Convolutional Kolmogorov Arnold Networks (CKANs) with the ImageNet-1k\ndataset with 1.3 million images, MNIST dataset with 60k images and a tabular\nbiological science related MoA dataset and test the promise of CKANs in terms\nof FLOPS, Inference Time, number of trainable parameters and training time\nagainst the accuracy, precision, recall and f-1 score they produce against the\nstandard industry practice on CNN models. We show that the CKANs perform fair\nyet slower than CNNs in small size dataset like MoA and MNIST but are not\nnearly comparable as the dataset gets larger and more complex like the\nImageNet. The code implementation of this paper can be found on the link:\n\\href{https://github.com/ashimdahal/Study-of-Convolutional-Kolmogorov-Arnold-networks}{https://github.com/ashimdahal/Study-of-Convolutional-Kolmogorov-Arnold-networks}",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15757v2",
    "published_date": "2025-01-27 04:00:05 UTC",
    "updated_date": "2025-01-28 04:26:12 UTC"
  },
  {
    "arxiv_id": "2501.15749v1",
    "title": "LLM-powered Multi-agent Framework for Goal-oriented Learning in Intelligent Tutoring System",
    "authors": [
      "Tianfu Wang",
      "Yi Zhan",
      "Jianxun Lian",
      "Zhengyu Hu",
      "Nicholas Jing Yuan",
      "Qi Zhang",
      "Xing Xie",
      "Hui Xiong"
    ],
    "abstract": "Intelligent Tutoring Systems (ITSs) have revolutionized education by offering\npersonalized learning experiences. However, as goal-oriented learning, which\nemphasizes efficiently achieving specific objectives, becomes increasingly\nimportant in professional contexts, existing ITSs often struggle to deliver\nthis type of targeted learning experience. In this paper, we propose GenMentor,\nan LLM-powered multi-agent framework designed to deliver goal-oriented,\npersonalized learning within ITS. GenMentor begins by accurately mapping\nlearners' goals to required skills using a fine-tuned LLM trained on a custom\ngoal-to-skill dataset. After identifying the skill gap, it schedules an\nefficient learning path using an evolving optimization approach, driven by a\ncomprehensive and dynamic profile of learners' multifaceted status.\nAdditionally, GenMentor tailors learning content with an\nexploration-drafting-integration mechanism to align with individual learner\nneeds. Extensive automated and human evaluations demonstrate GenMentor's\neffectiveness in learning guidance and content quality. Furthermore, we have\ndeployed it in practice and also implemented it as an application. Practical\nhuman study with professional learners further highlights its effectiveness in\ngoal alignment and resource targeting, leading to enhanced personalization.\nSupplementary resources are available at\nhttps://github.com/GeminiLight/gen-mentor.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by WWW 2025 (Industry Track)",
    "pdf_url": "http://arxiv.org/pdf/2501.15749v1",
    "published_date": "2025-01-27 03:29:44 UTC",
    "updated_date": "2025-01-27 03:29:44 UTC"
  },
  {
    "arxiv_id": "2501.15747v2",
    "title": "IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding",
    "authors": [
      "Sankalp KJ",
      "Ashutosh Kumar",
      "Laxmaan Balaji",
      "Nikunj Kotecha",
      "Vinija Jain",
      "Aman Chadha",
      "Sreyoshi Bhaduri"
    ],
    "abstract": "Known by more than 1.5 billion people in the Indian subcontinent, Indic\nlanguages present unique challenges and opportunities for natural language\nprocessing (NLP) research due to their rich cultural heritage, linguistic\ndiversity, and complex structures. IndicMMLU-Pro is a comprehensive benchmark\ndesigned to evaluate Large Language Models (LLMs) across Indic languages,\nbuilding upon the MMLU Pro (Massive Multitask Language Understanding)\nframework. Covering major languages such as Hindi, Bengali, Gujarati, Marathi,\nKannada, Punjabi, Tamil, Telugu, and Urdu, our benchmark addresses the unique\nchallenges and opportunities presented by the linguistic diversity of the\nIndian subcontinent. This benchmark encompasses a wide range of tasks in\nlanguage comprehension, reasoning, and generation, meticulously crafted to\ncapture the intricacies of Indian languages. IndicMMLU-Pro provides a\nstandardized evaluation framework to push the research boundaries in Indic\nlanguage AI, facilitating the development of more accurate, efficient, and\nculturally sensitive models. This paper outlines the benchmarks' design\nprinciples, task taxonomy, and data collection methodology, and presents\nbaseline results from state-of-the-art multilingual models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15747v2",
    "published_date": "2025-01-27 03:19:03 UTC",
    "updated_date": "2025-01-28 04:56:40 UTC"
  },
  {
    "arxiv_id": "2501.15740v1",
    "title": "Propositional Interpretability in Artificial Intelligence",
    "authors": [
      "David J. Chalmers"
    ],
    "abstract": "Mechanistic interpretability is the program of explaining what AI systems are\ndoing in terms of their internal mechanisms. I analyze some aspects of the\nprogram, along with setting out some concrete challenges and assessing progress\nto date. I argue for the importance of propositional interpretability, which\ninvolves interpreting a system's mechanisms and behavior in terms of\npropositional attitudes: attitudes (such as belief, desire, or subjective\nprobability) to propositions (e.g. the proposition that it is hot outside).\nPropositional attitudes are the central way that we interpret and explain human\nbeings and they are likely to be central in AI too. A central challenge is what\nI call thought logging: creating systems that log all of the relevant\npropositional attitudes in an AI system over time. I examine currently popular\nmethods of interpretability (such as probing, sparse auto-encoders, and chain\nof thought methods) as well as philosophical methods of interpretation\n(including those grounded in psychosemantics) to assess their strengths and\nweaknesses as methods of propositional interpretability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15740v1",
    "published_date": "2025-01-27 03:06:06 UTC",
    "updated_date": "2025-01-27 03:06:06 UTC"
  },
  {
    "arxiv_id": "2502.07088v1",
    "title": "Kernels of Selfhood: GPT-4o shows humanlike patterns of cognitive consistency moderated by free choice",
    "authors": [
      "Steven A. Lehr",
      "Ketan S. Saichandran",
      "Eddie Harmon-Jones",
      "Nykko Vitali",
      "Mahzarin R. Banaji"
    ],
    "abstract": "Large Language Models (LLMs) show emergent patterns that mimic human\ncognition. We explore whether they also mirror other, less deliberative human\npsychological processes. Drawing upon classical theories of cognitive\nconsistency, two preregistered studies tested whether GPT-4o changed its\nattitudes toward Vladimir Putin in the direction of a positive or negative\nessay it wrote about the Russian leader. Indeed, GPT displayed patterns of\nattitude change mimicking cognitive consistency effects in humans. Even more\nremarkably, the degree of change increased sharply when the LLM was offered an\nillusion of choice about which essay (positive or negative) to write. This\nresult suggests that GPT-4o manifests a functional analog of humanlike\nselfhood, although how faithfully the chatbot's behavior reflects the\nmechanisms of human attitude change remains to be understood.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG",
      "68T50 (Primary), 91E10, 68T30, 68T99, 91C99 (Secondary)",
      "I.2.7; I.2.0; J.4; K.4.0; H.1.2; K.4.1"
    ],
    "primary_category": "cs.CY",
    "comment": "Main Article: 10 pages, Supporting Information: 61 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.07088v1",
    "published_date": "2025-01-27 02:25:12 UTC",
    "updated_date": "2025-01-27 02:25:12 UTC"
  },
  {
    "arxiv_id": "2501.15733v1",
    "title": "Leveraging Video Vision Transformer for Alzheimer's Disease Diagnosis from 3D Brain MRI",
    "authors": [
      "Taymaz Akan",
      "Sait Alp",
      "Md. Shenuarin Bhuiyan",
      "Elizabeth A. Disbrow",
      "Steven A. Conrad",
      "John A. Vanchiere",
      "Christopher G. Kevil",
      "Mohammad A. N. Bhuiyan"
    ],
    "abstract": "Alzheimer's disease (AD) is a neurodegenerative disorder affecting millions\nworldwide, necessitating early and accurate diagnosis for optimal patient\nmanagement. In recent years, advancements in deep learning have shown\nremarkable potential in medical image analysis. Methods In this study, we\npresent \"ViTranZheimer,\" an AD diagnosis approach which leverages video vision\ntransformers to analyze 3D brain MRI data. By treating the 3D MRI volumes as\nvideos, we exploit the temporal dependencies between slices to capture\nintricate structural relationships. The video vision transformer's\nself-attention mechanisms enable the model to learn long-range dependencies and\nidentify subtle patterns that may indicate AD progression. Our proposed deep\nlearning framework seeks to enhance the accuracy and sensitivity of AD\ndiagnosis, empowering clinicians with a tool for early detection and\nintervention. We validate the performance of the video vision transformer using\nthe ADNI dataset and conduct comparative analyses with other relevant models.\nResults The proposed ViTranZheimer model is compared with two hybrid models,\nCNN-BiLSTM and ViT-BiLSTM. CNN-BiLSTM is the combination of a convolutional\nneural network (CNN) and a bidirectional long-short-term memory network\n(BiLSTM), while ViT-BiLSTM is the combination of a vision transformer (ViT)\nwith BiLSTM. The accuracy levels achieved in the ViTranZheimer, CNN-BiLSTM, and\nViT-BiLSTM models are 98.6%, 96.479%, and 97.465%, respectively. ViTranZheimer\ndemonstrated the highest accuracy at 98.6%, outperforming other models in this\nevaluation metric, indicating its superior performance in this specific\nevaluation metric. Conclusion This research advances the understanding of\napplying deep learning techniques in neuroimaging and Alzheimer's disease\nresearch, paving the way for earlier and less invasive clinical diagnosis.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15733v1",
    "published_date": "2025-01-27 02:18:08 UTC",
    "updated_date": "2025-01-27 02:18:08 UTC"
  },
  {
    "arxiv_id": "2501.15731v1",
    "title": "Renewable Energy Prediction: A Comparative Study of Deep Learning Models for Complex Dataset Analysis",
    "authors": [
      "Haibo Wang",
      "Jun Huang",
      "Lutfu Sua",
      "Bahram Alidaee"
    ],
    "abstract": "The increasing focus on predicting renewable energy production aligns with\nadvancements in deep learning (DL). The inherent variability of renewable\nsources and the complexity of prediction methods require robust approaches,\nsuch as DL models, in the renewable energy sector. DL models are preferred over\ntraditional machine learning (ML) because they capture complex, nonlinear\nrelationships in renewable energy datasets. This study examines key factors\ninfluencing DL technique accuracy, including sampling and hyperparameter\noptimization, by comparing various methods and training and test ratios within\na DL framework. Seven machine learning methods, LSTM, Stacked LSTM, CNN,\nCNN-LSTM, DNN, Time-Distributed MLP (TD-MLP), and Autoencoder (AE), are\nevaluated using a dataset combining weather and photovoltaic power output data\nfrom 12 locations. Regularization techniques such as early stopping, neuron\ndropout, L1 and L2 regularization are applied to address overfitting. The\nresults demonstrate that the combination of early stopping, dropout, and L1\nregularization provides the best performance to reduce overfitting in the CNN\nand TD-MLP models with larger training set, while the combination of early\nstopping, dropout, and L2 regularization is the most effective to reduce the\noverfitting in CNN-LSTM and AE models with smaller training set.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 2 figures and 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.15731v1",
    "published_date": "2025-01-27 02:10:10 UTC",
    "updated_date": "2025-01-27 02:10:10 UTC"
  },
  {
    "arxiv_id": "2501.15727v1",
    "title": "Gensors: Authoring Personalized Visual Sensors with Multimodal Foundation Models and Reasoning",
    "authors": [
      "Michael Xieyang Liu",
      "Savvas Petridis",
      "Vivian Tsai",
      "Alexander J. Fiannaca",
      "Alex Olwal",
      "Michael Terry",
      "Carrie J. Cai"
    ],
    "abstract": "Multimodal large language models (MLLMs), with their expansive world\nknowledge and reasoning capabilities, present a unique opportunity for\nend-users to create personalized AI sensors capable of reasoning about complex\nsituations. A user could describe a desired sensing task in natural language\n(e.g., \"alert if my toddler is getting into mischief\"), with the MLLM analyzing\nthe camera feed and responding within seconds. In a formative study, we found\nthat users saw substantial value in defining their own sensors, yet struggled\nto articulate their unique personal requirements and debug the sensors through\nprompting alone. To address these challenges, we developed Gensors, a system\nthat empowers users to define customized sensors supported by the reasoning\ncapabilities of MLLMs. Gensors 1) assists users in eliciting requirements\nthrough both automatically-generated and manually created sensor criteria, 2)\nfacilitates debugging by allowing users to isolate and test individual criteria\nin parallel, 3) suggests additional criteria based on user-provided images, and\n4) proposes test cases to help users \"stress test\" sensors on potentially\nunforeseen scenarios. In a user study, participants reported significantly\ngreater sense of control, understanding, and ease of communication when\ndefining sensors using Gensors. Beyond addressing model limitations, Gensors\nsupported users in debugging, eliciting requirements, and expressing unique\npersonal requirements to the sensor through criteria-based reasoning; it also\nhelped uncover users' \"blind spots\" by exposing overlooked criteria and\nrevealing unanticipated failure modes. Finally, we discuss how unique\ncharacteristics of MLLMs--such as hallucinations and inconsistent\nresponses--can impact the sensor-creation process. These findings contribute to\nthe design of future intelligent sensing systems that are intuitive and\ncustomizable by everyday users.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15727v1",
    "published_date": "2025-01-27 01:47:57 UTC",
    "updated_date": "2025-01-27 01:47:57 UTC"
  },
  {
    "arxiv_id": "2501.15724v2",
    "title": "A Survey on Computational Pathology Foundation Models: Datasets, Adaptation Strategies, and Evaluation Tasks",
    "authors": [
      "Dong Li",
      "Guihong Wan",
      "Xintao Wu",
      "Xinyu Wu",
      "Ajit J. Nirmal",
      "Christine G. Lian",
      "Peter K. Sorger",
      "Yevgeniy R. Semenov",
      "Chen Zhao"
    ],
    "abstract": "Computational pathology foundation models (CPathFMs) have emerged as a\npowerful approach for analyzing histopathological data, leveraging\nself-supervised learning to extract robust feature representations from\nunlabeled whole-slide images. These models, categorized into uni-modal and\nmulti-modal frameworks, have demonstrated promise in automating complex\npathology tasks such as segmentation, classification, and biomarker discovery.\nHowever, the development of CPathFMs presents significant challenges, such as\nlimited data accessibility, high variability across datasets, the necessity for\ndomain-specific adaptation, and the lack of standardized evaluation benchmarks.\nThis survey provides a comprehensive review of CPathFMs in computational\npathology, focusing on datasets, adaptation strategies, and evaluation tasks.\nWe analyze key techniques, such as contrastive learning and multi-modal\nintegration, and highlight existing gaps in current research. Finally, we\nexplore future directions from four perspectives for advancing CPathFMs. This\nsurvey serves as a valuable resource for researchers, clinicians, and AI\npractitioners, guiding the advancement of CPathFMs toward robust and clinically\napplicable AI-driven pathology solutions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.15724v2",
    "published_date": "2025-01-27 01:27:59 UTC",
    "updated_date": "2025-02-26 03:14:21 UTC"
  },
  {
    "arxiv_id": "2501.15708v3",
    "title": "StaICC: Standardized Evaluation for Classification Task in In-context Learning",
    "authors": [
      "Hakaze Cho",
      "Naoya Inoue"
    ],
    "abstract": "Classification tasks are widely investigated in the In-Context Learning (ICL)\nparadigm. However, current efforts are evaluated on disjoint benchmarks and\nsettings, while their performances are significantly influenced by some trivial\nvariables, such as prompt templates, data sampling, instructions, etc., which\nleads to significant inconsistencies in the results reported across various\nliterature, preventing fair comparison or meta-analysis across different\npapers. Therefore, this paper proposes a standardized and easy-to-use\nevaluation toolkit (StaICC) for in-context classification. Including, for the\nnormal classification task, we provide StaICC-Normal, selecting 10 widely used\ndatasets, and generating prompts with a fixed form, to mitigate the variance\namong the experiment implementations. To enrich the usage of our benchmark, we\nalso provide a sub-benchmark StaICC-Diag for diagnosing ICL from several\naspects, aiming for a more robust inference processing.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 8 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.15708v3",
    "published_date": "2025-01-27 00:05:12 UTC",
    "updated_date": "2025-04-18 08:09:26 UTC"
  }
]
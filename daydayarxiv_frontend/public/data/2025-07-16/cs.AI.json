{
  "date": "2025-07-16",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-07-16 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ä½ ä»¬çš„æ—¥æŠ¥ä½œè€…ã€‚\n\n**ä¸€å¥è¯æ€»ç»“ä»Šå¤©ï¼š**\nä»Šå¤©çš„ arXiv ä¹Ÿæ˜¯â€œç¥ä»™æ‰“æ¶â€çš„ä¸€å¤©ï¼Œ**æ¨ç†ï¼ˆReasoningï¼‰èƒ½åŠ›**çš„æå‡ä¾ç„¶æ˜¯æ ¸å¿ƒæˆ˜åœºï¼Œä»å¤ç° OpenAI o1/DeepSeek-R1 çš„å¼ºåŒ–å­¦ä¹ é…æ–¹ï¼Œåˆ°è®© LLM å†™ CUDA å†…æ ¸ï¼Œå†åˆ°ä¸­å›½è±¡æ£‹çš„ç­–ç•¥æ¨ç†ï¼Œå¤§å®¶éƒ½è¯•å›¾é€šè¿‡ RL è§£é”æ›´å¼ºçš„æ€ç»´é“¾ã€‚æ­¤å¤–ï¼Œ**Vision-Language Model (VLM)** åœ¨æœºå™¨äººå·¥å…·åˆ¶é€ å’Œç©ºé—´æ¨ç†ä¸Šçš„åº”ç”¨ä¹Ÿè®©äººçœ¼å‰ä¸€äº®ã€‚\n\nä¸‹é¢æˆ‘ä»¬ç›´å…¥ä¸»é¢˜ï¼Œçœ‹çœ‹ä»Šå¤©ä¸å®¹é”™è¿‡çš„ç¡¬æ ¸ç ”ç©¶ã€‚\n\n---\n\n### ğŸš€ æ ¸å¿ƒå…³æ³¨ï¼šLLM æ¨ç†ã€å¼ºåŒ–å­¦ä¹ ä¸ Scaling\n\nè¿™ä¸€æ¿å—ä¸ä»…æœ‰è¯•å›¾å¤ç° o1/R1 çš„æŠ€æœ¯æŠ¥å‘Šï¼Œè¿˜æœ‰é’ˆå¯¹ä»£ç ç”Ÿæˆçš„æ·±åº¦ä¼˜åŒ–ã€‚\n\n**1. Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training**\n**é€šè¿‡å»¶é•¿è®­ç»ƒæ—¶é—´æ‰©å±• RLï¼šè§£é” LLM çš„å¤šæ ·åŒ–æ¨ç†èƒ½åŠ›**\n> æ ¸å¿ƒå…³é”®è¯ï¼š*Reinforcement Learning, GRPO, Reasoning, Chain-of-Thought*\n>\nè¿™æ˜¯ä¸€ç¯‡éå¸¸å€¼å¾—å…³æ³¨çš„å®æˆ˜å‹æŠ¥å‘Šã€‚ä½œè€…å›¢é˜Ÿï¼ˆåŒ…å« NVIDIA å­¦è€…ï¼‰æ·±å…¥ç ”ç©¶äº†ç±»ä¼¼ OpenAI o1 å’Œ DeepSeek-R1 çš„æ¨ç†æ¨¡å‹èƒŒåçš„æœºåˆ¶ã€‚\n*   **æ ¸å¿ƒå‘ç°**ï¼šå•çº¯ä¾é æ‰©å¤§æ¨¡å‹æ˜¯ä¸å¤Ÿçš„ï¼Œ**å»¶é•¿çš„å¼ºåŒ–å­¦ä¹ ï¼ˆProlonged RLï¼‰** ç»“åˆæµ‹è¯•æ—¶çš„è®¡ç®—æ‰©å±•ï¼ˆTest-time computationï¼‰æ˜¯å…³é”®ã€‚\n*   **æ–¹æ³•**ï¼šä»–ä»¬ç¡®å®šäº†å‡ ä¸ªå…³é”®è¦ç´ ï¼šå¯éªŒè¯çš„å¥–åŠ±ä»»åŠ¡ã€æ”¹è¿›çš„ GRPOï¼ˆGroup Relative Policy Optimizationï¼‰ç®—æ³•ï¼Œä»¥åŠæ§åˆ¶ KL æ­£åˆ™åŒ–å’Œç­–ç•¥é‡ç½®çš„æŠ€å·§ã€‚\n*   **ç»“æœ**ï¼šåœ¨æ•°å­¦ï¼ˆ+14.7%ï¼‰ã€ä»£ç ï¼ˆ+13.9%ï¼‰å’Œé€»è¾‘è°œé¢˜ï¼ˆ+54.8%ï¼‰ä¸Šå–å¾—äº†å·¨å¤§æå‡ã€‚è¿™ä¸ä»…æ˜¯ä¸€ä¸ªæ¨¡å‹å‘å¸ƒï¼Œæ›´åƒæ˜¯ä¸€ä»½â€œæ¨ç†æ¨¡å‹çƒ¹é¥ªæŒ‡å—â€ã€‚\n\n**2. Kevin: Multi-Turn RL for Generating CUDA Kernels**\n**Kevinï¼šç”¨äºç”Ÿæˆ CUDA å†…æ ¸çš„å¤šè½®å¼ºåŒ–å­¦ä¹ æ¨¡å‹**\n> æ ¸å¿ƒå…³é”®è¯ï¼š*CUDA Generation, Multi-turn RL, Optimization*\n>\nå†™ GPU å†…æ ¸ï¼ˆKernelï¼‰æ˜¯ AI æ•ˆç‡çš„å…³é”®ï¼Œä½†ä¹Ÿæéš¾ã€‚è¿™ç¯‡æ–‡ç« æå‡ºäº† **Kevin**ï¼Œä¸€ä¸ªä¸“é—¨å†™ CUDA çš„æ¨¡å‹ã€‚\n*   **åˆ›æ–°**ï¼šåˆ©ç”¨ CUDA ç¼–ç¨‹ä¸­â€œæ­£ç¡®æ€§â€å’Œâ€œåŠ é€Ÿæ¯”â€æ˜“äºéªŒè¯çš„ç‰¹ç‚¹ï¼Œæ„å»ºäº†ä¸€ä¸ªå¤šè½® RL è®­ç»ƒæµç¨‹ã€‚æ¨¡å‹ä¸ä»…ç”Ÿæˆä»£ç ï¼Œè¿˜èƒ½æ ¹æ®åé¦ˆè‡ªæˆ‘ä¼˜åŒ–ï¼ˆSelf-refinementï¼‰ã€‚\n*   **æ•ˆæœ**ï¼šç”Ÿæˆçš„ CUDA ä»£ç æ­£ç¡®ç‡ä» 56% æå‡åˆ° 82%ï¼Œè¿è¡Œé€Ÿåº¦ç”šè‡³è¶…è¿‡äº† PyTorch Eager æ¨¡å¼ï¼Œè¿™å¯¹äº AI ç³»ç»Ÿä¼˜åŒ–é¢†åŸŸæ˜¯é‡å¤§åˆ©å¥½ã€‚\n\n**3. Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess**\n**Xiangqi-R1ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ å¢å¼º LLM åœ¨ä¸­å›½è±¡æ£‹ä¸­çš„ç©ºé—´ç­–ç•¥æ¨ç†**\n> æ ¸å¿ƒå…³é”®è¯ï¼š*Chinese Chess, Spatial Reasoning, RL*\n>\né€šç”¨ LLM ä¸‹å›´æ£‹æˆ–è±¡æ£‹é€šå¸¸å¾ˆçƒ‚ã€‚è¿™ç¯‡æ–‡ç« å‘å¸ƒäº† **Xiangqi-R1**ï¼ˆè‡´æ•¬ DeepSeek-R1 çš„å‘½åï¼Ÿï¼‰ã€‚\n*   **æ–¹æ³•**ï¼šåˆ©ç”¨ 500 ä¸‡ä¸ªç»è¿‡å¼•æ“è¯„ä¼°çš„æ£‹å±€æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä¸“æ³¨äºâ€œç©ºé—´ç­–ç•¥æ¨ç†â€ã€‚\n*   **å‘ç°**ï¼šé€šç”¨ LLM ç¼ºä¹ç©ºé—´æ„Ÿï¼Œè€Œç»è¿‡ç‰¹å®š RL è®­ç»ƒåï¼Œæ¨¡å‹åœ¨èµ°æ³•åˆæ³•æ€§å’Œåˆ†æå‡†ç¡®ç‡ä¸Šå¤§å¹…æå‡ã€‚è¿™æ˜¯ LLM è§£å†³å¤æ‚ç©ºé—´åšå¼ˆé—®é¢˜çš„ä¸€ä¸ªæœ‰è¶£æ ·æœ¬ã€‚\n\n---\n\n### ğŸ¤– å…·èº«æ™ºèƒ½ä¸è§†è§‰è¯­è¨€æ¨¡å‹ (VLM)\n\nä»Šå¤©çš„ VLM ä¸å†åªæ˜¯â€œçœ‹å›¾è¯´è¯â€ï¼Œå®ƒä»¬å¼€å§‹â€œåˆ¶é€ å·¥å…·â€å’Œâ€œç†è§£ä¸‰ç»´ä¸–ç•Œâ€ã€‚\n\n**4. VLMgineer: Vision Language Models as Robotic Toolsmiths**\n**VLMgineerï¼šè§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºæœºå™¨äººå·¥å…·åŒ **\n> æ ¸å¿ƒå…³é”®è¯ï¼š*Robotic Tool Design, Evolutionary Search, VLM*\n>\nè¿™ç¯‡å¾ˆæœ‰æ„æ€ã€‚é€šå¸¸æˆ‘ä»¬è®­ç»ƒæœºå™¨äººç”¨å·¥å…·ï¼Œä½†è¿™ç¯‡è®ºæ–‡è®© VLM **è®¾è®¡**å·¥å…·ã€‚\n*   **åˆ›æ„**ï¼šæå‡ºäº† VLMgineer æ¡†æ¶ï¼Œåˆ©ç”¨ VLM çš„ä»£ç ç”Ÿæˆèƒ½åŠ›å’Œè¿›åŒ–æœç´¢ï¼Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡è®¾è®¡ç‰©ç†å·¥å…·å¹¶ç”Ÿæˆæ“ä½œç­–ç•¥ã€‚\n*   **ç»“æœ**ï¼šVLM è®¾è®¡å‡ºçš„å·¥å…·æ¯”äººç±»è®¾è®¡çš„åœ¨æŸäº›ä»»åŠ¡ä¸Šæ›´æœ‰æ•ˆã€æ›´æœ‰åˆ›æ„ï¼Œç”šè‡³èƒ½è§£å†³ä¸€äº›â€œåˆé’»â€çš„ç‰©ç†æ“ä½œéš¾é¢˜ã€‚\n\n**5. MindJourney: Test-Time Scaling with World Models for Spatial Reasoning**\n**MindJourneyï¼šåˆ©ç”¨ä¸–ç•Œæ¨¡å‹è¿›è¡Œç©ºé—´æ¨ç†çš„æµ‹è¯•æ—¶æ‰©å±•**\n> æ ¸å¿ƒå…³é”®è¯ï¼š*World Models, Spatial Reasoning, 3D Dynamics*\n>\nVLM é€šå¸¸ç¼ºä¹ 3D ç©ºé—´æƒ³è±¡åŠ›ã€‚\n*   **æ–¹æ³•**ï¼šä½œè€…ç»™ VLM é…äº†ä¸€ä¸ªâ€œä¸–ç•Œæ¨¡å‹â€ï¼ˆè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼‰ã€‚VLM æå‡ºæƒ³çœ‹ä»€ä¹ˆè§†è§’ï¼Œä¸–ç•Œæ¨¡å‹ç”Ÿæˆé‚£ä¸ªè§†è§’çš„å›¾åƒï¼Œå¦‚æ­¤è¿­ä»£ã€‚\n*   **è´¡çŒ®**ï¼šè¿™ç§â€œæƒ³è±¡-è§‚å¯Ÿ-æ¨ç†â€çš„é—­ç¯ï¼Œåœ¨ä¸éœ€è¦å¾®è°ƒ VLM çš„æƒ…å†µä¸‹ï¼Œç›´æ¥æå‡äº†ç©ºé—´æ¨ç†èƒ½åŠ›ï¼ˆSAT åŸºå‡†æå‡ 7.7%ï¼‰ã€‚\n\n**6. EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos**\n**EgoVLAï¼šä»ç¬¬ä¸€äººç§°äººç±»è§†é¢‘ä¸­å­¦ä¹ è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹**\n> æ ¸å¿ƒå…³é”®è¯ï¼š*Egocentric Video, Imitation Learning, Retargeting*\n>\næœºå™¨äººæ•°æ®å¤ªå°‘ï¼Ÿé‚£å°±ç”¨äººç±»çš„ç¬¬ä¸€äººç§°è§†é¢‘ï¼ˆEgocentric Videoï¼‰ã€‚\n*   **æ–¹æ³•**ï¼šåˆ©ç”¨æµ·é‡äººç±»è§†é¢‘è®­ç»ƒ VLM é¢„æµ‹æ‰‹éƒ¨åŠ¨ä½œï¼Œç„¶åé€šè¿‡é‡å®šå‘ï¼ˆRetargetingï¼‰æŠ€æœ¯æ˜ å°„åˆ°æœºå™¨äººä¸Šã€‚\n*   **ä»·å€¼**ï¼šæå¤§é™ä½äº†æ”¶é›†æœºå™¨äººçœŸå®æ•°æ®çš„æˆæœ¬ï¼Œè¯æ˜äº†äººç±»è§†é¢‘æ˜¯æœºå™¨äººå­¦ä¹ çš„å¯ŒçŸ¿ã€‚\n\n---\n\n### ğŸ›¡ï¸ AI å®‰å…¨ä¸å¯è§£é‡Šæ€§\n\néšç€æ¨¡å‹è¶Šæ¥è¶Šå¼ºï¼Œä¸ä»…è¦é˜²â€œå¹»è§‰â€ï¼Œè¿˜è¦é˜²â€œæ¬ºéª—â€ã€‚\n\n**7. Benchmarking Deception Probes via Black-to-White Performance Boosts**\n**é€šè¿‡é»‘ç›’åˆ°ç™½ç›’çš„æ€§èƒ½æå‡åŸºå‡†æµ‹è¯•æ¬ºéª—æ¢æµ‹å™¨**\n> æ ¸å¿ƒå…³é”®è¯ï¼š*Deception Detection, AI Safety, Monitoring*\n>\nAI åŠ©æ‰‹æœ‰æ—¶ä¼šæ’’è°ï¼ˆDeceptiveï¼‰ã€‚æ€ä¹ˆæŠ“ç°è¡Œï¼Ÿ\n*   **ç ”ç©¶**ï¼šå¯¹æ¯”äº†â€œç™½ç›’ç›‘æ§â€ï¼ˆèƒ½çœ‹å†…éƒ¨æ¿€æ´»ï¼‰å’Œâ€œé»‘ç›’ç›‘æ§â€ï¼ˆåªèƒ½çœ‹è¾“å‡ºï¼‰æ¢æµ‹ AI æ¬ºéª—çš„æ•ˆæœã€‚\n*   **ç»“è®º**ï¼šç›®å‰çš„æ¬ºéª—æ¢æµ‹å™¨åœ¨ç™½ç›’æ¨¡å¼ä¸‹æœ‰ä¸€å®šæå‡ï¼Œä½†ä¾ç„¶å¾ˆè„†å¼±ã€‚è¿™æ˜¯ AI å®‰å…¨é¢†åŸŸéå¸¸åŸºç¡€ä¸”é‡è¦çš„å¯¹æŠ—æ€§ç ”ç©¶ã€‚\n\n**8. Can We Predict Alignment Before Models Finish Thinking?**\n**æˆ‘ä»¬èƒ½åœ¨æ¨¡å‹â€œæ€è€ƒâ€ç»“æŸå‰é¢„æµ‹å…¶å¯¹é½æ€§å—ï¼Ÿ**\n> æ ¸å¿ƒå…³é”®è¯ï¼š*CoT Monitoring, Safety, Early Intervention*\n>\nç°åœ¨çš„æ¨ç†æ¨¡å‹ä¼šè¾“å‡ºå¾ˆé•¿çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ã€‚\n*   **å‘ç°**ï¼šä¸éœ€è¦ç­‰å®ƒè¯´å®Œã€‚é€šè¿‡ç®€å•çš„çº¿æ€§æ¢æµ‹å™¨ï¼ˆLinear Probeï¼‰ç›‘æµ‹ CoT çš„å†…éƒ¨æ¿€æ´»ï¼Œå°±èƒ½æ¯”ç›‘æµ‹æ–‡æœ¬æ›´æ—©ã€æ›´å‡†åœ°åˆ¤æ–­æ¨¡å‹æœ€åæ˜¯å¦ä¼šè¾“å‡ºæœ‰å®³ä¿¡æ¯ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥å®ç°å®æ—¶çš„â€œæ€æƒ³å®¡æŸ¥â€å’Œå¹²é¢„ã€‚\n\n---\n\n### ğŸ’» è½¯ä»¶å·¥ç¨‹ä¸ç³»ç»Ÿä¼˜åŒ–\n\n**9. BootSeer: Analyzing and Mitigating Initialization Bottlenecks in Large-Scale LLM Training**\n**BootSeerï¼šåˆ†æä¸ç¼“è§£å¤§è§„æ¨¡ LLM è®­ç»ƒä¸­çš„åˆå§‹åŒ–ç“¶é¢ˆ**\n> æ ¸å¿ƒå…³é”®è¯ï¼š*LLM Training, Systems, Startup Overhead*\n>\nè¿™æ˜¯ä¸€ç¯‡çº¯æ­£çš„ç³»ç»Ÿï¼ˆSysMLï¼‰è®ºæ–‡ã€‚åœ¨å·¥ä¸šçº§é›†ç¾¤ä¸­ï¼ŒGPU æœ‰ 3.5% çš„æ—¶é—´æµªè´¹åœ¨â€œå¯åŠ¨â€ä¸Šã€‚\n*   **è§£å†³æ–¹æ¡ˆ**ï¼šé’ˆå¯¹é•œåƒåŠ è½½ã€ä¾èµ–å®‰è£…ã€Checkpoint æ¢å¤è¿™ä¸‰å¤§ç“¶é¢ˆï¼Œæå‡ºäº† BootSeer ç³»ç»Ÿï¼Œå°†å¯åŠ¨å¼€é”€é™ä½äº† 50%ã€‚è¿™å¯¹äºçƒ§é’±çš„ LLM è®­ç»ƒå›¢é˜Ÿæ¥è¯´å°±æ˜¯çœŸé‡‘ç™½é“¶ã€‚\n\n**10. GitChameleon 2.0: Evaluating AI Code Generation Against Python Library Version Incompatibilities**\n**GitChameleon 2.0ï¼šè¯„ä¼° AI ä»£ç ç”Ÿæˆå¯¹ Python åº“ç‰ˆæœ¬ä¸å…¼å®¹æ€§çš„åº”å¯¹**\n> æ ¸å¿ƒå…³é”®è¯ï¼š*Code Generation, Version Control, Benchmark*\n>\nå³ä½¿æ˜¯ GPT-4ï¼Œä¹Ÿå¸¸å†™å‡º API å·²ç»è¿‡æ—¶çš„ä»£ç ã€‚\n*   **è´¡çŒ®**ï¼šå‘å¸ƒäº†ä¸€ä¸ªæ–°æ•°æ®é›†ï¼Œä¸“é—¨æµ‹è¯• LLM æ˜¯å¦èƒ½æ ¹æ®**ç‰¹å®šç‰ˆæœ¬**çš„åº“ç”Ÿæˆæ­£ç¡®çš„ä»£ç ã€‚\n*   **ç°çŠ¶**ï¼šå³ä¾¿æ˜¯æœ€å¼ºçš„ä¼ä¸šçº§æ¨¡å‹ï¼ŒæˆåŠŸç‡ä¹Ÿåªæœ‰ 50% å·¦å³ã€‚è¿™æ˜¯ä»£ç åŠ©æ‰‹ç›®å‰çš„ä¸€å¤§ç—›ç‚¹ã€‚\n\n---\n\n### ğŸ”¬ ç§‘å­¦ AI (AI for Science)\n\n**11. The Evolving Role of Large Language Models in Scientific Innovation**\n**å¤§è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦åˆ›æ–°ä¸­æ¼”å˜çš„è§’è‰²ï¼šè¯„ä¼°è€…ã€åˆä½œè€…ä¸ç§‘å­¦å®¶**\n> æ ¸å¿ƒå…³é”®è¯ï¼š*Survey, Scientific Discovery, AI for Science*\n>\nä¸€ç¯‡ç»¼è¿°ã€‚å°† LLM åœ¨ç§‘å­¦ä¸­çš„è§’è‰²åˆ†ä¸ºä¸‰ä¸ªå±‚çº§ï¼šè¯„ä¼°è€…ï¼ˆæ‰“æ‚ï¼‰ã€åˆä½œè€…ï¼ˆå‰¯æ‰‹ï¼‰ã€ç§‘å­¦å®¶ï¼ˆç‹¬ç«‹å‘ç°ï¼‰ã€‚é€‚åˆæƒ³äº†è§£ AI å¦‚ä½•æ”¹å˜ç§‘ç ”èŒƒå¼çš„è¯»è€…é˜…è¯»ã€‚\n\n**12. Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models**\n**ç”¨äºé«˜ä¿çœŸã€é«˜æ•ˆæ‰©æ•£æ¨¡å‹çš„ç»„åˆç¦»æ•£æ½œåœ¨ä»£ç **\n> æ ¸å¿ƒå…³é”®è¯ï¼š*Diffusion Models, Representation Learning, Mila*\n>\næ¥è‡ª Bengio æ‰€åœ¨çš„ Mila å®éªŒå®¤ï¼ˆAaron Courville ç­‰ï¼‰ã€‚\n*   **æ–¹æ³•**ï¼šæå‡º DLCï¼ˆç¦»æ•£æ½œåœ¨ä»£ç ï¼‰ï¼Œç”¨ç¦»æ•£ token æ›¿ä»£è¿ç»­åµŒå…¥æ¥è°ƒèŠ‚æ‰©æ•£æ¨¡å‹ã€‚\n*   **æ•ˆæœ**ï¼šä¸ä»…æå‡äº†ç”Ÿæˆå›¾åƒçš„ä¿çœŸåº¦ï¼Œè¿˜è®©æ¨¡å‹å…·å¤‡äº†æ›´å¥½çš„â€œç»„åˆæ€§â€ï¼ˆCompositionalityï¼‰ï¼Œèƒ½ç”Ÿæˆè®­ç»ƒåˆ†å¸ƒä¹‹å¤–çš„æ–°é¢–å›¾åƒã€‚\n\n---\n\n### âš¡ï¸ å¿«é€Ÿæ å½± (Quick Reads)\n\n*   **[Game Design] Fly, Fail, Fix: Iterative Game Repair... (#6)**: ç»“åˆ RL ç©å®¶å’Œå¤šæ¨¡æ€å¤§æ¨¡å‹è®¾è®¡å¸ˆï¼Œè‡ªåŠ¨æµ‹è¯•å¹¶â€œä¿®è¡¥â€æ¸¸æˆè§„åˆ™ã€‚æ¸¸æˆç­–åˆ’å¯èƒ½ä¼šå¤±ä¸šï¼Ÿ\n*   **[Quantum] QSpark: Towards Reliable Qiskit Code Generation (#11)**: é‡å­è®¡ç®—ä»£ç ï¼ˆQiskitï¼‰å¾ˆéš¾å†™ï¼Œç”¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒ LLM ä¸“é—¨å†™è¿™ä¸ªï¼Œæ¯”é€šç”¨æ¨¡å‹å¼ºä¸å°‘ã€‚\n*   **[Math] A Compute-Matched Re-Evaluation of TroVE on MATH (#102)**: æ‰“å‡ï¼ˆæˆ–è€…è¯´çº åï¼‰è®ºæ–‡ã€‚æŒ‡å‡ºä¹‹å‰çš„æ–¹æ³• TroVE åœ¨æ•°å­¦é¢˜ä¸Šçš„æå‡ï¼Œä¸»è¦æ˜¯å› ä¸ºâ€œç®—çš„æ›´å¤šâ€ï¼ˆCompute budgetæ›´é«˜ï¼‰ï¼Œè€Œä¸æ˜¯æ–¹æ³•æœ¬èº«å¤šé«˜æ˜ã€‚\n*   **[Social] Cognitive Castes: Artificial Intelligence... (#77)**: *Craig S Wright* å†™çš„ä¸€ç¯‡ç†è®ºæ–‡ç« ï¼Œè®ºè¿° AI å¹¶éæ‹‰å¹³äº†è®¤çŸ¥å·®è·ï¼Œè€Œæ˜¯åŠ å‰§äº†â€œè®¤çŸ¥åˆ†å±‚â€å’Œé˜¶çº§å›ºåŒ–ã€‚è§‚ç‚¹çŠ€åˆ©ï¼Œå€¼å¾—ä¸€è¯»ã€‚\n*   **[Speech] Quantize More, Lose Less (#59)**: æ–°çš„ TTS ç¼–è§£ç å™¨ QDAC å’Œæ¡†æ¶ QTTSï¼Œåœ¨æä½æ¯”ç‰¹ç‡ä¸‹ä¿æŒé«˜ä¿çœŸè¯­éŸ³åˆæˆã€‚\n\n---\n**æ—¥æŠ¥ç»“è¯­**ï¼š\nä»Šå¤©çš„è®ºæ–‡æ˜æ˜¾æ„Ÿè§‰åˆ°ç¤¾åŒºåœ¨ä»â€œè®©æ¨¡å‹è·‘é€šâ€è½¬å‘â€œè®©æ¨¡å‹è·‘å¥½ã€è·‘å¯¹ã€è·‘å¾—æ›´æœ‰é€»è¾‘â€ã€‚æ— è®ºæ˜¯ Kevin å¯¹ CUDA çš„ä¼˜åŒ–ï¼Œè¿˜æ˜¯ Xiangqi-R1 å¯¹ç©ºé—´é€»è¾‘çš„æ¢ç´¢ï¼Œäº¦æˆ–æ˜¯ BootSeer å¯¹è®­ç»ƒç³»ç»Ÿçš„å‹æ¦¨ï¼Œéƒ½åœ¨è¯´æ˜ AI ç ”ç©¶æ­£åœ¨è¿›å…¥ç²¾ç»†åŒ–æ·±è€•é˜¶æ®µã€‚\n\næ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2507.12691v3",
      "title": "Benchmarking Deception Probes via Black-to-White Performance Boosts",
      "title_zh": "é€šè¿‡é»‘ç›’è‡³ç™½ç›’æ€§èƒ½å¢ç›Šè¯„ä¼°æ¬ºéª—æ¢é’ˆ",
      "authors": [
        "Avi Parrack",
        "Carlo Leonardo Attubato",
        "Stefan Heimersheim"
      ],
      "abstract": "AI assistants will occasionally respond deceptively to user queries. Recently, linear classifiers (called \"deception probes\") have been trained to distinguish the internal activations of a language model during deceptive versus honest responses. However, it's unclear how effective these probes are at detecting deception in practice, nor whether such probes are resistant to simple counter strategies from a deceptive assistant who wishes to evade detection. In this paper, we compare white-box monitoring (where the monitor has access to token-level probe activations) to black-box monitoring (without such access). We benchmark deception probes by the extent to which the white box monitor outperforms the black-box monitor, i.e. the black-to-white performance boost. We find weak but encouraging black-to-white performance boosts from existing deception probes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½åŠ©æ‰‹å¯èƒ½äº§ç”Ÿçš„æ¬ºéª—æ€§å“åº”ï¼Œè¯„ä¼°äº†æ—¨åœ¨è¯†åˆ«è¯­è¨€æ¨¡å‹å†…éƒ¨æ¿€æ´»çŠ¶æ€çš„çº¿æ€§åˆ†ç±»å™¨â€”â€”æ¬ºéª—æ¢æµ‹å™¨ (deception probes) çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†éªŒè¯æ¢æµ‹å™¨åœ¨å®é™…åº”ç”¨ä¸­çš„æ£€æµ‹èƒ½åŠ›åŠå…¶åº”å¯¹è§„é¿ç­–ç•¥çš„é²æ£’æ€§ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§ä»¥â€œé»‘ç›’åˆ°ç™½ç›’æ€§èƒ½æå‡â€ (black-to-white performance boosts) ä¸ºæ ¸å¿ƒçš„åŸºå‡†æµ‹è¯•æ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¯”ç™½ç›’ç›‘æ§ï¼ˆå¯è®¿é—® token çº§åˆ«çš„æ¢æµ‹å™¨æ¿€æ´»ï¼‰ä¸é»‘ç›’ç›‘æ§çš„æ•ˆæœæ¥è¡¡é‡æ¢æµ‹å™¨çš„æ€§èƒ½ã€‚ç ”ç©¶é‡ç‚¹æ¢è®¨äº†æ¢æµ‹å™¨åœ¨åŒºåˆ†æ¬ºéª—æ€§ä¸è¯šå®å“åº”æ—¶çš„è¡¨ç°ï¼Œå¹¶åˆ†æäº†å…¶ä½œä¸ºç›‘æ§å·¥å…·çš„å®é™…ä»·å€¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰æ¢æµ‹å™¨åœ¨é»‘ç›’åˆ°ç™½ç›’çš„æ€§èƒ½æå‡ä¸Šè¡¨ç°è™½ç„¶å°šæµ…ï¼Œä½†å…¶å±•ç°å‡ºçš„æ½œåŠ›ä¸ºæœªæ¥æ„å»ºå¯ä¿¡çš„ AI ç›‘æ§ç³»ç»Ÿæä¾›äº†ä»¤äººé¼“èˆçš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint. 39 pages, 11 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2507.12691v3",
      "published_date": "2025-07-16 23:49:55 UTC",
      "updated_date": "2026-01-17 22:28:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:00:24.296709+00:00"
    },
    {
      "arxiv_id": "2507.12677v1",
      "title": "Data Transformation Strategies to Remove Heterogeneity",
      "title_zh": "æ¶ˆé™¤å¼‚æ„æ€§çš„æ•°æ®è½¬æ¢ç­–ç•¥",
      "authors": [
        "Sangbong Yoo",
        "Jaeyoung Lee",
        "Chanyoung Yoon",
        "Geonyeong Son",
        "Hyein Hong",
        "Seongbum Seo",
        "Soobin Yim",
        "Chanyoung Jung",
        "Jungsoo Park",
        "Misuk Kim",
        "Yun Jang"
      ],
      "abstract": "Data heterogeneity is a prevalent issue, stemming from various conflicting factors, making its utilization complex. This uncertainty, particularly resulting from disparities in data formats, frequently necessitates the involvement of experts to find resolutions. Current methodologies primarily address conflicts related to data structures and schemas, often overlooking the pivotal role played by data transformation. As the utilization of artificial intelligence (AI) continues to expand, there is a growing demand for a more streamlined data preparation process, and data transformation becomes paramount. It customizes training data to enhance AI learning efficiency and adapts input formats to suit diverse AI models. Selecting an appropriate transformation technique is paramount in preserving crucial data details. Despite the widespread integration of AI across various industries, comprehensive reviews concerning contemporary data transformation approaches are scarce. This survey explores the intricacies of data heterogeneity and its underlying sources. It systematically categorizes and presents strategies to address heterogeneity stemming from differences in data formats, shedding light on the inherent challenges associated with each strategy.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ•°æ®å¼‚æ„æ€§ (Data Heterogeneity) åŠå…¶å¸¦æ¥çš„åˆ©ç”¨å¤æ‚æ€§ï¼Œé‡ç‚¹å…³æ³¨äº†ç”±æ•°æ®æ ¼å¼ (Data Formats) å·®å¼‚å¼•èµ·çš„ä¸ç¡®å®šæ€§ã€‚ç›®å‰çš„ç ”ç©¶å¤šä¾§é‡äºæ•°æ®ç»“æ„å’Œæ¨¡å¼å†²çªï¼Œè€Œå¿½è§†äº†æ•°æ®è½¬æ¢ (Data Transformation) åœ¨äººå·¥æ™ºèƒ½ (AI) æ•°æ®å‡†å¤‡ä¸­çš„å…³é”®ä½œç”¨ã€‚æœ¬æ–‡é€šè¿‡ä¸€ç¯‡ç»¼è¿° (Survey) ç³»ç»Ÿåœ°åˆ†ç±»å¹¶ä»‹ç»äº†åº”å¯¹æ ¼å¼å¼‚æ„æ€§çš„è½¬æ¢ç­–ç•¥ï¼Œåˆ†æäº†å„ç­–ç•¥åœ¨ä¿ç•™æ•°æ®ç»†èŠ‚æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å¼ºè°ƒäº†é€‰æ‹©åˆé€‚è½¬æ¢æŠ€æœ¯å¯¹æé«˜ AI å­¦ä¹ æ•ˆç‡å’Œæ¨¡å‹é€‚é…çš„é‡è¦æ€§ã€‚æ–‡ç« å¡«è¡¥äº†å½“å‰ç¼ºä¹å¯¹ç°ä»£æ•°æ®è½¬æ¢æ–¹æ³•å…¨é¢å›é¡¾çš„ç©ºç™½ï¼Œä¸ºä¼˜åŒ–æ•°æ®å‡†å¤‡æµç¨‹æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12677v1",
      "published_date": "2025-07-16 23:27:24 UTC",
      "updated_date": "2025-07-16 23:27:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:00:27.981309+00:00"
    },
    {
      "arxiv_id": "2507.12675v1",
      "title": "FORTRESS: Function-composition Optimized Real-Time Resilient Structural Segmentation via Kolmogorov-Arnold Enhanced Spatial Attention Networks",
      "title_zh": "FORTRESSï¼šåŸºäº Kolmogorov-Arnold å¢å¼ºç©ºé—´æ³¨æ„åŠ›ç½‘ç»œçš„å‡½æ•°ç»„åˆä¼˜åŒ–å®æ—¶éŸ§æ€§ç»“æ„åˆ†å‰²",
      "authors": [
        "Christina Thrainer",
        "Md Meftahul Ferdaus",
        "Mahdi Abdelguerfi",
        "Christian Guetl",
        "Steven Sloan",
        "Kendall N. Niles",
        "Ken Pathak"
      ],
      "abstract": "Automated structural defect segmentation in civil infrastructure faces a critical challenge: achieving high accuracy while maintaining computational efficiency for real-time deployment. This paper presents FORTRESS (Function-composition Optimized Real-Time Resilient Structural Segmentation), a new architecture that balances accuracy and speed by using a special method that combines depthwise separable convolutions with adaptive Kolmogorov-Arnold Network integration. FORTRESS incorporates three key innovations: a systematic depthwise separable convolution framework achieving a 3.6x parameter reduction per layer, adaptive TiKAN integration that selectively applies function composition transformations only when computationally beneficial, and multi-scale attention fusion combining spatial, channel, and KAN-enhanced features across decoder levels. The architecture achieves remarkable efficiency gains with 91% parameter reduction (31M to 2.9M), 91% computational complexity reduction (13.7 to 1.17 GFLOPs), and 3x inference speed improvement while delivering superior segmentation performance. Evaluation on benchmark infrastructure datasets demonstrates state-of-the-art results with an F1- score of 0.771 and a mean IoU of 0.677, significantly outperforming existing methods including U-Net, SA-UNet, and U- KAN. The dual optimization strategy proves essential for optimal performance, establishing FORTRESS as a robust solution for practical structural defect segmentation in resource-constrained environments where both accuracy and computational efficiency are paramount. Comprehensive architectural specifications are provided in the Supplemental Material. Source code is available at URL: https://github.com/faeyelab/fortress-paper-code.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FORTRESSï¼Œè¿™æ˜¯ä¸€ç§åŸºäºKolmogorov-Arnold Enhanced Spatial Attention Networksçš„å®æ—¶å¼¹æ€§ç»“æ„åˆ†å‰²æ¶æ„ï¼Œæ—¨åœ¨è§£å†³åœŸæœ¨åŸºç¡€è®¾æ–½è‡ªåŠ¨åŒ–ç¼ºé™·åˆ†å‰²ä¸­é«˜å‡†ç¡®ç‡ä¸è®¡ç®—æ•ˆç‡éš¾ä»¥å…¼é¡¾çš„é—®é¢˜ã€‚è¯¥æ¶æ„é‡‡ç”¨äº†ç³»ç»Ÿæ€§çš„æ·±åº¦å¯åˆ†ç¦»å·ç§¯(depthwise separable convolution)æ¡†æ¶ï¼Œå¹¶é›†æˆäº†è‡ªé€‚åº”çš„TiKANæŠ€æœ¯ï¼Œä»…åœ¨è®¡ç®—æœ‰ç›Šæ—¶åº”ç”¨å‡½æ•°ç»„åˆå˜æ¢ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚é€šè¿‡åœ¨è§£ç å™¨å±‚çº§å¼•å…¥å¤šå°ºåº¦æ³¨æ„åŠ›èåˆï¼ŒFORTRESSæˆåŠŸæ•´åˆäº†ç©ºé—´ã€é€šé“åŠKANå¢å¼ºçš„ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å‚æ•°é‡å’Œè®¡ç®—å¤æ‚åº¦ä¸Šå‡å‡å°‘äº†91%ï¼Œæ¨ç†é€Ÿåº¦æå‡äº†3å€ï¼ŒåŒæ—¶åœ¨F1-score (0.771)å’Œmean IoU (0.677)ç­‰æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†State-of-the-artæ°´å¹³ã€‚ç›¸æ¯”U-Netã€SA-UNetå’ŒU-KANç­‰æ¨¡å‹ï¼ŒFORTRESSè¡¨ç°å‡ºæ›´ä¼˜çš„åˆ†å‰²æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹è¿›è¡Œå®ç”¨ç»“æ„ç¼ºé™·åˆ†å‰²çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12675v1",
      "published_date": "2025-07-16 23:17:58 UTC",
      "updated_date": "2025-07-16 23:17:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:00:17.388565+00:00"
    },
    {
      "arxiv_id": "2507.12674v2",
      "title": "ParaStudent: Generating and Evaluating Realistic Student Code by Teaching LLMs to Struggle",
      "title_zh": "ParaStudentï¼šé€šè¿‡æ•™å¯¼å¤§è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿâ€œæŒ£æ‰â€è¿‡ç¨‹ç”Ÿæˆä¸è¯„ä¼°çœŸå®çš„å­¦ç”Ÿä»£ç ",
      "authors": [
        "Mihran Miroyan",
        "Rose Niousha",
        "Joseph E. Gonzalez",
        "Gireeja Ranade",
        "Narges Norouzi"
      ],
      "abstract": "Large Language Models (LLMs) have shown strong performance on programming tasks, but can they generate student-like code like real students - imperfect, iterative, and stylistically diverse? We present ParaStudent, a systematic study of LLM-based \"student-like\" code generation in an introductory programming course setting. Using a dataset of timestamped student submissions across multiple semesters, we design low- and high-resolution experiments to model student progress and evaluate code outputs along semantic, functional, and stylistic dimensions. Our results show that fine-tuning significantly improves alignment with real student trajectories and captures error patterns, incremental improvements, and stylistic variations more faithfully. This study shows that modeling realistic student code requires capturing learning dynamics through context-aware generation, temporal modeling, and multi-dimensional evaluation. Code for experiments and evaluation is available at https://github.com/mmiroyan/ParaStudent.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æ˜¯å¦èƒ½åƒçœŸå®å­¦ç”Ÿä¸€æ ·ç”Ÿæˆå…·æœ‰ä¸å®Œç¾æ€§ã€è¿­ä»£æ€§å’Œé£æ ¼å¤šæ ·æ€§çš„ç¼–ç¨‹ä»£ç ã€‚ä½œè€…æå‡ºäº†ParaStudentï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨å…¥é—¨çº§ç¼–ç¨‹è¯¾ç¨‹èƒŒæ™¯ä¸‹ï¼ŒåŸºäºLLMsç”Ÿæˆâ€œç±»å­¦ç”Ÿâ€ä»£ç çš„ç³»ç»Ÿæ€§ç ”ç©¶æ¡†æ¶ã€‚è¯¥ç ”ç©¶åˆ©ç”¨å¤šä¸ªå­¦æœŸå¸¦æœ‰æ—¶é—´æˆ³çš„å­¦ç”Ÿæäº¤æ•°æ®é›†ï¼Œè®¾è®¡äº†ä½åˆ†è¾¨ç‡å’Œé«˜åˆ†è¾¨ç‡å®éªŒï¼Œä»è¯­ä¹‰ã€åŠŸèƒ½å’Œé£æ ¼ç»´åº¦å¯¹ä»£ç è¾“å‡ºè¿›è¡Œå»ºæ¨¡å’Œè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡å¾®è°ƒ(Fine-tuning)å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹ä¸çœŸå®å­¦ç”Ÿå­¦ä¹ è½¨è¿¹çš„å¯¹é½ç¨‹åº¦ï¼Œä»è€Œæ›´å¿ å®åœ°æ•æ‰é”™è¯¯æ¨¡å¼(Error patterns)ã€å¢é‡æ”¹è¿›å’Œé£æ ¼å˜åŒ–ã€‚ç ”ç©¶æœ€ç»ˆæŒ‡å‡ºï¼Œæœ‰æ•ˆæ¨¡æ‹ŸçœŸå®çš„ç±»å­¦ç”Ÿä»£ç éœ€è¦é€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç”Ÿæˆ(Context-aware generation)ã€æ—¶é—´å»ºæ¨¡(Temporal modeling)ä»¥åŠå¤šç»´åº¦è¯„ä¼°æ¥æ•æ‰å­¦ä¹ åŠ¨æ€ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12674v2",
      "published_date": "2025-07-16 23:12:14 UTC",
      "updated_date": "2025-07-18 01:02:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:00:22.989325+00:00"
    },
    {
      "arxiv_id": "2507.12669v1",
      "title": "InSight: AI Mobile Screening Tool for Multiple Eye Disease Detection using Multimodal Fusion",
      "title_zh": "InSightï¼šåŸºäºå¤šæ¨¡æ€èåˆçš„å¤šç§çœ¼ç§‘ç–¾ç—…æ£€æµ‹äººå·¥æ™ºèƒ½ç§»åŠ¨ç«¯ç­›æŸ¥å·¥å…·",
      "authors": [
        "Ananya Raghu",
        "Anisha Raghu",
        "Alice S. Tang",
        "Yannis M. Paulus",
        "Tyson N. Kim",
        "Tomiko T. Oskotsky"
      ],
      "abstract": "Background/Objectives: Age-related macular degeneration, glaucoma, diabetic retinopathy (DR), diabetic macular edema, and pathological myopia affect hundreds of millions of people worldwide. Early screening for these diseases is essential, yet access to medical care remains limited in low- and middle-income countries as well as in resource-limited settings. We develop InSight, an AI-based app that combines patient metadata with fundus images for accurate diagnosis of five common eye diseases to improve accessibility of screenings.\n  Methods: InSight features a three-stage pipeline: real-time image quality assessment, disease diagnosis model, and a DR grading model to assess severity. Our disease diagnosis model incorporates three key innovations: (a) Multimodal fusion technique (MetaFusion) combining clinical metadata and images; (b) Pretraining method leveraging supervised and self-supervised loss functions; and (c) Multitask model to simultaneously predict 5 diseases. We make use of BRSET (lab-captured images) and mBRSET (smartphone-captured images) datasets, both of which also contain clinical metadata for model training/evaluation.\n  Results: Trained on a dataset of BRSET and mBRSET images, the image quality checker achieves near-100% accuracy in filtering out low-quality fundus images. The multimodal pretrained disease diagnosis model outperforms models using only images by 6% in balanced accuracy for BRSET and 4% for mBRSET.\n  Conclusions: The InSight pipeline demonstrates robustness across varied image conditions and has high diagnostic accuracy across all five diseases, generalizing to both smartphone and lab captured images. The multitask model contributes to the lightweight nature of the pipeline, making it five times computationally efficient compared to having five individual models corresponding to each disease.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†åä¸º InSight çš„ AI ç§»åŠ¨åº”ç”¨å·¥å…·ï¼Œæ—¨åœ¨ä¸ºå…¨çƒèŒƒå›´å†…å°¤å…¶æ˜¯èµ„æºåŒ®ä¹åœ°åŒºæä¾›é’ˆå¯¹äº”ç§å¸¸è§çœ¼éƒ¨ç–¾ç—…çš„æ—©æœŸç­›æŸ¥ã€‚è¯¥ç³»ç»Ÿæ„å»ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µæµæ°´çº¿ï¼ŒåŒ…æ‹¬å®æ—¶å›¾åƒè´¨é‡è¯„ä¼°ã€ç–¾ç—…è¯Šæ–­æ¨¡å‹ä»¥åŠ Diabetic Retinopathy (DR) ä¸¥é‡ç¨‹åº¦åˆ†çº§æ¨¡å‹ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ MetaFusion å¤šæ¨¡æ€èåˆæŠ€æœ¯ã€ç»“åˆç›‘ç£ä¸è‡ªç›‘ç£æŸå¤±çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œä»¥åŠèƒ½å¤ŸåŒæ—¶é¢„æµ‹äº”é¡¹ç–¾ç—…çš„ Multitask æ¨¡å‹ã€‚åœ¨ BRSET å’Œ mBRSET æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œå¤šæ¨¡æ€èåˆæ¨¡å‹æ¯”ä»…ä½¿ç”¨å›¾åƒçš„æ¨¡å‹åœ¨å¹³è¡¡å‡†ç¡®ç‡ä¸Šæå‡äº† 4% è‡³ 6%ï¼Œä¸”è´¨é‡è¯„ä¼°å™¨è¾¾åˆ°äº†è¿‘ 100% çš„å‡†ç¡®ç‡ã€‚å¾—ç›Šäºå¤šä»»åŠ¡æ¶æ„ï¼ŒInSight åœ¨è®¡ç®—æ•ˆç‡ä¸Šæ¯”äº”ä¸ªç‹¬ç«‹æ¨¡å‹ç»„åˆæå‡äº†äº”å€ï¼Œä¸”åœ¨æ™ºèƒ½æ‰‹æœºå’Œå®éªŒå®¤æ‹æ‘„çš„çœ¼åº•å›¾åƒä¸­å‡è¡¨ç°å‡ºæå¼ºçš„ç¨³å¥æ€§ä¸è¯Šæ–­å‡†ç¡®æ€§ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12669v1",
      "published_date": "2025-07-16 23:00:10 UTC",
      "updated_date": "2025-07-16 23:00:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:00:25.790131+00:00"
    },
    {
      "arxiv_id": "2507.12666v1",
      "title": "Fly, Fail, Fix: Iterative Game Repair with Reinforcement Learning and Large Multimodal Models",
      "title_zh": "Fly, Fail, Fixï¼šåŸºäºå¼ºåŒ–å­¦ä¹ ä¸å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹çš„è¿­ä»£å¼æ¸¸æˆä¿®å¤",
      "authors": [
        "Alex Zook",
        "Josef Spjut",
        "Jonathan Tremblay"
      ],
      "abstract": "Game design hinges on understanding how static rules and content translate into dynamic player behavior - something modern generative systems that inspect only a game's code or assets struggle to capture. We present an automated design iteration framework that closes this gap by pairing a reinforcement learning (RL) agent, which playtests the game, with a large multimodal model (LMM), which revises the game based on what the agent does. In each loop the RL player completes several episodes, producing (i) numerical play metrics and/or (ii) a compact image strip summarising recent video frames. The LMM designer receives a gameplay goal and the current game configuration, analyses the play traces, and edits the configuration to steer future behaviour toward the goal. We demonstrate results that LMMs can reason over behavioral traces supplied by RL agents to iteratively refine game mechanics, pointing toward practical, scalable tools for AI-assisted game design.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œFly, Fail, Fixâ€çš„è‡ªåŠ¨åŒ–æ¸¸æˆè®¾è®¡è¿­ä»£æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”Ÿæˆç³»ç»Ÿéš¾ä»¥æ•æ‰é™æ€è§„åˆ™å‘åŠ¨æ€ç©å®¶è¡Œä¸ºè½¬åŒ–çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ Reinforcement Learning (RL) æ™ºèƒ½ä½“è¿›è¡Œ playtestingï¼Œå¹¶ç»“åˆ Large Multimodal Model (LMM) æ ¹æ®è¯•ç©åé¦ˆå¯¹æ¸¸æˆè¿›è¡Œä¿®è®¢ã€‚åœ¨è¿­ä»£å¾ªç¯ä¸­ï¼ŒRL æ™ºèƒ½ä½“ç”Ÿæˆçš„ play metrics å’Œå›¾åƒè½¨è¿¹æ‘˜è¦ä¸º LMM æä¾›åˆ†æä¾æ®ï¼Œä½¿å…¶èƒ½æ ¹æ®è®¾å®šçš„ç›®æ ‡å®æ—¶ç¼–è¾‘æ¸¸æˆé…ç½®ã€‚å®éªŒè¯æ˜ï¼ŒLMM èƒ½å¤Ÿæœ‰æ•ˆæ¨ç† RL æä¾›çš„ behavioral traces å¹¶è¿­ä»£ä¼˜åŒ–æ¸¸æˆæœºåˆ¶ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°å®ç”¨ä¸”å¯æ‰©å±•çš„ AI-assisted game design å·¥å…·æä¾›äº†æ–°çš„æ€è·¯ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Published at Reinforcement Learning and Video Games workshop https://sites.google.com/view/rlvg-workshop-2025/home",
      "pdf_url": "https://arxiv.org/pdf/2507.12666v1",
      "published_date": "2025-07-16 22:45:40 UTC",
      "updated_date": "2025-07-16 22:45:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:00:34.389953+00:00"
    },
    {
      "arxiv_id": "2507.12665v1",
      "title": "Single Conversation Methodology: A Human-Centered Protocol for AI-Assisted Software Development",
      "title_zh": "å•ä¸€å¯¹è¯æ–¹æ³•è®ºï¼šä¸€ç§ä»¥äººä¸ºä¸­å¿ƒçš„äººå·¥æ™ºèƒ½è¾…åŠ©è½¯ä»¶å¼€å‘åè®®",
      "authors": [
        "Salvador D. Escobedo"
      ],
      "abstract": "We propose the Single Conversation Methodology (SCM), a novel and pragmatic approach to software development using large language models (LLMs). In contrast to ad hoc interactions with generative AI, SCM emphasizes a structured and persistent development dialogue, where all stages of a project - from requirements to architecture and implementation - unfold within a single, long-context conversation. The methodology is grounded on principles of cognitive clarity, traceability, modularity, and documentation. We define its phases, best practices, and philosophical stance, while arguing that SCM offers a necessary correction to the passive reliance on LLMs prevalent in current practices. We aim to reassert the active role of the developer as architect and supervisor of the intelligent tool.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Single Conversation Methodology (SCM)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) è¾…åŠ©è½¯ä»¶å¼€å‘çš„æ–°é¢–ä¸”å®ç”¨çš„åè®®ã€‚ä¸ä¼ ç»Ÿçš„éšæœº AI äº¤äº’æ–¹å¼ä¸åŒï¼ŒSCM çš„æ ¸å¿ƒåœ¨äºå»ºç«‹ç»“æ„åŒ–ä¸”æŒä¹…çš„å¼€å‘å¯¹è¯ï¼Œä½¿é¡¹ç›®ä»éœ€æ±‚åˆ†æã€æ¶æ„è®¾è®¡åˆ°æœ€ç»ˆå®ç°çš„æ‰€æœ‰é˜¶æ®µéƒ½åœ¨å•ä¸€çš„é•¿ä¸Šä¸‹æ–‡ (long-context) ä¼šè¯ä¸­å±•å¼€ã€‚è¯¥æ–¹æ³•è®ºå»ºç«‹åœ¨è®¤çŸ¥æ¸…æ™°åº¦ (cognitive clarity)ã€å¯è¿½æº¯æ€§ã€æ¨¡å—åŒ–å’Œæ–‡æ¡£åŒ–åŸåˆ™ä¹‹ä¸Šï¼Œå¹¶æ˜ç¡®äº†å…·ä½“çš„å®æ–½é˜¶æ®µã€æœ€ä½³å®è·µåŠå…¶èƒŒåçš„å“²å­¦ç«‹åœºã€‚é€šè¿‡è¿™ç§ç³»ç»ŸåŒ–çš„æ–¹æ³•ï¼ŒSCM è¯•å›¾çº æ­£ç›®å‰å¼€å‘è€…å¯¹ LLMs æ™®éå­˜åœ¨çš„è¢«åŠ¨ä¾èµ–ç°çŠ¶ï¼Œæ—¨åœ¨é‡æ–°ç¡®ç«‹äººç±»å¼€å‘è€…åœ¨æ™ºèƒ½è¾…åŠ©å¼€å‘è¿‡ç¨‹ä¸­ä½œä¸ºæ¶æ„å¸ˆå’Œç›‘ç£è€…çš„ä¸»åŠ¨æ ¸å¿ƒä½œç”¨ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "Style reviewed by a LLM for improving clarity and English syntax",
      "pdf_url": "https://arxiv.org/pdf/2507.12665v1",
      "published_date": "2025-07-16 22:43:30 UTC",
      "updated_date": "2025-07-16 22:43:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:00:38.584976+00:00"
    },
    {
      "arxiv_id": "2507.14227v1",
      "title": "Domain Generalization via Pareto Optimal Gradient Matching",
      "title_zh": "åŸºäºå¸•ç´¯æ‰˜æœ€ä¼˜æ¢¯åº¦åŒ¹é…çš„åŸŸæ³›åŒ–",
      "authors": [
        "Khoi Do",
        "Duong Nguyen",
        "Nam-Khanh Le",
        "Quoc-Viet Pham",
        "Binh-Son Hua",
        "Won-Joo Hwang"
      ],
      "abstract": "In this study, we address the gradient-based domain generalization problem, where predictors aim for consistent gradient directions across different domains. Existing methods have two main challenges. First, minimization of gradient empirical distance or gradient inner products (GIP) leads to gradient fluctuations among domains, thereby hindering straightforward learning. Second, the direct application of gradient learning to the joint loss function can incur high computation overheads due to second-order derivative approximation. To tackle these challenges, we propose a new Pareto Optimality Gradient Matching (POGM) method. In contrast to existing methods that add gradient matching as regularization, we leverage gradient trajectories as collected data and apply independent training at the meta-learner. In the meta-update, we maximize GIP while limiting the learned gradient from deviating too far from the empirical risk minimization gradient trajectory. By doing so, the aggregate gradient can incorporate knowledge from all domains without suffering gradient fluctuation towards any particular domain. Experimental evaluations on datasets from DomainBed demonstrate competitive results yielded by POGM against other baselines while achieving computational efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºæ¢¯åº¦çš„é¢†åŸŸæ³›åŒ–(Domain Generalization)é—®é¢˜ï¼Œæ—¨åœ¨ä½¿é¢„æµ‹å™¨åœ¨ä¸åŒé¢†åŸŸé—´ä¿æŒä¸€è‡´çš„æ¢¯åº¦æ–¹å‘ã€‚é’ˆå¯¹ç°æœ‰æ¢¯åº¦åŒ¹é…æ–¹æ³•æ˜“å¼•å‘æ¢¯åº¦æ³¢åŠ¨åŠäºŒé˜¶å¯¼æ•°è¿‘ä¼¼å¯¼è‡´çš„é«˜è®¡ç®—å¼€é”€ç­‰æŒ‘æˆ˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºå¸•ç´¯æ‰˜æœ€ä¼˜æ¢¯åº¦åŒ¹é…(Pareto Optimality Gradient Matching, POGM)çš„æ–°æ–¹æ³•ã€‚ä¸åŒäºä¼ ç»Ÿçš„æ­£åˆ™åŒ–æ‰‹æ®µï¼ŒPOGM å°†æ¢¯åº¦è½¨è¿¹ä½œä¸ºè®­ç»ƒæ•°æ®åœ¨å…ƒå­¦ä¹ å™¨(Meta-learner)ä¸­ç‹¬ç«‹è¿è¡Œã€‚é€šè¿‡åœ¨å…ƒæ›´æ–°ä¸­æœ€å¤§åŒ–æ¢¯åº¦å†…ç§¯(GIP)å¹¶é™åˆ¶å…¶åç¦»ç»éªŒé£é™©æœ€å°åŒ–(ERM)çš„æ¢¯åº¦è½¨è¿¹ï¼Œè¯¥æ–¹æ³•ä½¿èšåˆæ¢¯åº¦èƒ½æœ‰æ•ˆèåˆå„é¢†åŸŸçŸ¥è¯†å¹¶æ¶ˆé™¤ç‰¹å®šé¢†åŸŸçš„æ³¢åŠ¨ã€‚åœ¨ DomainBed æ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¯æ˜ï¼ŒPOGM åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œå–å¾—äº†ä¼˜äºå¤šç§åŸºå‡†æ¨¡å‹çš„ç«äº‰æ€§æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14227v1",
      "published_date": "2025-07-16 22:41:49 UTC",
      "updated_date": "2025-07-16 22:41:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:00:39.898594+00:00"
    },
    {
      "arxiv_id": "2507.12659v1",
      "title": "Improving physics-informed neural network extrapolation via transfer learning and adaptive activation functions",
      "title_zh": "é€šè¿‡è¿ç§»å­¦ä¹ å’Œè‡ªé€‚åº”æ¿€æ´»å‡½æ•°æå‡ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œçš„å¤–æ¨æ€§èƒ½",
      "authors": [
        "Athanasios Papastathopoulos-Katsaros",
        "Alexandra Stavrianidi",
        "Zhandong Liu"
      ],
      "abstract": "Physics-Informed Neural Networks (PINNs) are deep learning models that incorporate the governing physical laws of a system into the learning process, making them well-suited for solving complex scientific and engineering problems. Recently, PINNs have gained widespread attention as a powerful framework for combining physical principles with data-driven modeling to improve prediction accuracy. Despite their successes, however, PINNs often exhibit poor extrapolation performance outside the training domain and are highly sensitive to the choice of activation functions (AFs). In this paper, we introduce a transfer learning (TL) method to improve the extrapolation capability of PINNs. Our approach applies transfer learning (TL) within an extended training domain, using only a small number of carefully selected collocation points. Additionally, we propose an adaptive AF that takes the form of a linear combination of standard AFs, which improves both the robustness and accuracy of the model. Through a series of experiments, we demonstrate that our method achieves an average of 40% reduction in relative L2 error and an average of 50% reduction in mean absolute error in the extrapolation domain, all without a significant increase in computational cost. The code is available at https://github.com/LiuzLab/PINN-extrapolation .",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ (Physics-Informed Neural Networks, PINNs) åœ¨è®­ç»ƒåŸŸå¤–å¤–æ¨æ€§èƒ½è¾ƒå·®ä»¥åŠå¯¹æ¿€æ´»å‡½æ•° (activation functions) é€‰å–é«˜åº¦æ•æ„Ÿçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆè¿ç§»å­¦ä¹  (transfer learning) ä¸è‡ªé€‚åº”æ¿€æ´»å‡½æ•°çš„æ”¹è¿›æ¡†æ¶ã€‚è¯¥æ–¹æ³•åœ¨æ‰©å±•çš„è®­ç»ƒåŸŸå†…åº”ç”¨è¿ç§»å­¦ä¹ ï¼Œé€šè¿‡ä»…ä½¿ç”¨å°‘é‡ç²¾é€‰çš„é…ç‚¹ (collocation points) æœ‰æ•ˆæå‡äº†æ¨¡å‹çš„å¤–æ¨èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§ç”±æ ‡å‡†æ¿€æ´»å‡½æ•°çº¿æ€§ç»„åˆæ„æˆçš„è‡ªé€‚åº”æ¿€æ´»å‡½æ•°ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„é²æ£’æ€§ä¸é¢„æµ‹ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸æ˜¾è‘—å¢åŠ è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹ï¼Œä½¿å¤–æ¨åŸŸçš„ç›¸å¯¹ L2 è¯¯å·®å¹³å‡é™ä½äº† 40%ï¼Œå¹³å‡ç»å¯¹è¯¯å·® (mean absolute error) å¹³å‡é™ä½äº† 50%ã€‚è¿™é¡¹å·¥ä½œæ˜¾è‘—æé«˜äº† PINNs åœ¨è§£å†³å¤æ‚ç§‘å­¦å’Œå·¥ç¨‹é—®é¢˜æ—¶çš„å¯é æ€§ä¸å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.DS",
        "math.NA",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 16 figures, 7 tables Accepted to ICANN 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.12659v1",
      "published_date": "2025-07-16 22:19:53 UTC",
      "updated_date": "2025-07-16 22:19:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:00:47.387751+00:00"
    },
    {
      "arxiv_id": "2507.12644v1",
      "title": "VLMgineer: Vision Language Models as Robotic Toolsmiths",
      "title_zh": "VLMgineerï¼šä½œä¸ºæœºå™¨äººå·¥å…·åˆ¶é€ è€…çš„è§†è§‰è¯­è¨€æ¨¡å‹",
      "authors": [
        "George Jiayuan Gao",
        "Tianyu Li",
        "Junyao Shi",
        "Yihan Li",
        "Zizhe Zhang",
        "Nadia Figueroa",
        "Dinesh Jayaraman"
      ],
      "abstract": "Tool design and use reflect the ability to understand and manipulate the physical world through creativity, planning, and foresight. As such, these capabilities are often regarded as measurable indicators of intelligence across biological species. While much of today's research on robotic intelligence focuses on generating better controllers, inventing smarter tools offers a complementary form of physical intelligence: shifting the onus of problem-solving onto the tool's design. Given the vast and impressive common-sense, reasoning, and creative capabilities of today's foundation models, we investigate whether these models can provide useful priors to automatically design and effectively wield such tools? We present VLMgineer, a framework that harnesses the code generation abilities of vision language models (VLMs) together with evolutionary search to iteratively co-design physical tools and the action plans that operate them to perform a task. We evaluate VLMgineer on a diverse new benchmark of everyday manipulation scenarios that demand creative tool design and use. Across this suite, VLMgineer consistently discovers tools and policies that solve tasks more effectively and innovatively, transforming challenging robotics problems into straightforward executions. It also outperforms VLM-generated designs from human specifications and existing human-crafted tools for everyday tasks. To facilitate future research on automated tool invention, we will release our benchmark and code.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VLMgineeræ¡†æ¶ï¼Œåˆ©ç”¨Vision Language Models (VLMs) çš„ code generation èƒ½åŠ›ç»“åˆ evolutionary search ç®—æ³•ï¼Œå®ç°äº†ç‰©ç†å·¥å…·åŠå…¶æ“ä½œç­–ç•¥çš„è¿­ä»£ååŒè®¾è®¡ã€‚VLMgineeré€šè¿‡æ•´åˆè§†è§‰è¯­è¨€æ¨¡å‹çš„å¸¸è¯†æ¨ç†ä¸åˆ›é€ åŠ›ï¼Œå°†å¤æ‚çš„æœºå™¨äººæ“çºµé—®é¢˜è½¬åŒ–ä¸ºé€šè¿‡å·¥å…·ä¼˜åŒ–å®ç°çš„ç®€å•æ‰§è¡Œä»»åŠ¡ã€‚åœ¨æ¶µç›–å¤šç§æ—¥å¸¸æ“çºµåœºæ™¯çš„æ–°åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVLMgineerèƒ½å¤Ÿè‡ªä¸»å‘ç°æ¯”äººç±»æ‰‹å·¥è®¾è®¡æˆ–å•çº¯ç”±VLMç”Ÿæˆçš„æ–¹æ¡ˆæ›´å…·åˆ›æ–°æ€§ä¸”æ›´æœ‰æ•ˆçš„å·¥å…·ä¸ç­–ç•¥ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä»»åŠ¡è§£å†³æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†ï¼Œå±•ç¤ºäº†åŸºç¡€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–å·¥å…·å‘æ˜å’Œç‰©ç†æ™ºèƒ½é¢†åŸŸçš„å·¨å¤§æ½œåŠ›ã€‚è¯¥ç ”ç©¶è¿˜åŒæ­¥å‘å¸ƒäº†ç›¸å…³åŸºå‡†æµ‹è¯•å’Œä»£ç ï¼Œä¸ºæœªæ¥è‡ªåŠ¨åŒ–å·¥å…·è®¾è®¡çš„ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project Website: https://vlmgineer.github.io/release",
      "pdf_url": "https://arxiv.org/pdf/2507.12644v1",
      "published_date": "2025-07-16 21:30:05 UTC",
      "updated_date": "2025-07-16 21:30:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:00:46.892375+00:00"
    },
    {
      "arxiv_id": "2507.12642v2",
      "title": "QSpark: Towards Reliable Qiskit Code Generation",
      "title_zh": "QSparkï¼šè¿ˆå‘å¯é çš„ Qiskit ä»£ç ç”Ÿæˆ",
      "authors": [
        "Kiana Kheiri",
        "Aamna Aamir",
        "Andriy Miranskyy",
        "Chen Ding"
      ],
      "abstract": "Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and StarCoder often output flawed Qiskit code. We fine-tuned the Qwen2.5-Coder-32B model with two RL methods, Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference Optimization (ORPO), using a richly annotated synthetic dataset. On the Qiskit HumanEval benchmark, ORPO reaches 56.29% Pass@1 ($\\approx+10$ pp over Granite-8B-QK) and GRPO hits 49%, both beating all general-purpose baselines; on the original HumanEval they score 65.90% and 63.00%. GRPO performs well on basic tasks (44/78) and excels on intermediate ones (41/68), but neither GRPO nor ORPO solves any of the five advanced tasks, highlighting clear gains yet room for progress in AI-assisted quantum programming.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç”Ÿæˆ Qiskit ä»£ç æ—¶æ˜“å‡ºç°é”™è¯¯ä¸”é‡å­ç”µè·¯ç¼ºä¹å®¹é”™æ€§çš„é—®é¢˜ï¼Œæå‡ºäº† QSpark æ¡†æ¶ã€‚ç ”ç©¶å›¢é˜ŸåŸºäº Qwen2.5-Coder-32B æ¨¡å‹ï¼Œåˆ©ç”¨é«˜è´¨é‡æ ‡æ³¨çš„åˆæˆæ•°æ®é›†ï¼Œåˆ†åˆ«é‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(GRPO)å’Œä¼˜åŠ¿æ¯”åå¥½ä¼˜åŒ–(ORPO)ä¸¤ç§å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¿›è¡Œäº†å¾®è°ƒã€‚åœ¨ Qiskit HumanEval åŸºå‡†æµ‹è¯•ä¸­ï¼ŒORPO å®ç°äº† 56.29% çš„ Pass@1 å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäº Granite-8B-QK æå‡äº†çº¦ 10 ä¸ªç™¾åˆ†ç‚¹ï¼Œè€Œ GRPO è¾¾åˆ° 49%ï¼Œä¸¤è€…å‡ä¼˜äºé€šç”¨çš„åŸºå‡†æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGRPO åœ¨åŸºç¡€å’Œä¸­ç­‰éš¾åº¦çš„é‡å­ç¼–ç¨‹ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†åœ¨å¤„ç†é«˜çº§å¤æ‚ä»»åŠ¡æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶åœ¨æå‡äººå·¥æ™ºèƒ½è¾…åŠ©é‡å­ç¼–ç¨‹çš„å¯é æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼ŒåŒæ—¶ä¹Ÿä¸ºæœªæ¥å®ç°æ›´é«˜çº§çš„è‡ªåŠ¨åŒ–é‡å­ç¼–ç¨‹æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "quant-ph"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12642v2",
      "published_date": "2025-07-16 21:27:31 UTC",
      "updated_date": "2025-09-23 16:36:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:01:01.598289+00:00"
    },
    {
      "arxiv_id": "2507.12630v2",
      "title": "Achieving Robust Channel Estimation Neural Networks by Designed Training Data",
      "title_zh": "é€šè¿‡è®­ç»ƒæ•°æ®è®¾è®¡å®ç°é²æ£’çš„ä¿¡é“ä¼°è®¡ç¥ç»ç½‘ç»œ",
      "authors": [
        "Dianxin Luan",
        "John Thompson"
      ],
      "abstract": "Channel estimation is crucial in wireless communications. However, in many papers neural networks are frequently tested by training and testing on one example channel or similar channels. This is because data-driven methods often degrade on new data which they are not trained on, as they cannot extrapolate their training knowledge. This is despite the fact physical channels are often assumed to be time-variant. However, due to the low latency requirements and limited computing resources, neural networks may not have enough time and computing resources to execute online training to fine-tune the parameters. This motivates us to design offline-trained neural networks that can perform robustly over wireless channels, but without any actual channel information being known at design time. In this paper, we propose design criteria to generate synthetic training datasets for neural networks, which guarantee that after training the resulting networks achieve a certain mean squared error (MSE) on new and previously unseen channels. Therefore, trained neural networks require no prior channel information or parameters update for real-world implementations. Based on the proposed design criteria, we further propose a benchmark design which ensures intelligent operation for different channel profiles. To demonstrate general applicability, we use neural networks with different levels of complexity to show that the generalization achieved appears to be independent of neural network architecture. From simulations, neural networks achieve robust generalization to wireless channels with both fixed channel profiles and variable delay spreads.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»ç½‘ç»œ(Neural networks)åœ¨ä¿¡é“ä¼°è®¡(Channel estimation)ä¸­éš¾ä»¥æ³›åŒ–è‡³æœªè§ä¿¡é“ä¸”åœ¨çº¿è®­ç»ƒå—é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡è®¾è®¡åˆæˆè®­ç»ƒæ•°æ®é›†(Training datasets)æ¥æå‡æ¨¡å‹ç¨³å¥æ€§çš„æ–¹æ³•ã€‚ä½œè€…åˆ¶å®šçš„è®¾è®¡å‡†åˆ™èƒ½å¤Ÿç¡®ä¿ç¦»çº¿è®­ç»ƒåçš„æ¨¡å‹åœ¨æ— éœ€å…ˆéªŒä¿¡é“ä¿¡æ¯æˆ–å‚æ•°æ›´æ–°çš„æƒ…å†µä¸‹ï¼Œåœ¨æœªçŸ¥ä¿¡é“ä¸Šè¾¾åˆ°é¢„è®¾çš„å‡æ–¹è¯¯å·®(MSE)æ°´å¹³ã€‚ç ”ç©¶è¿˜æä¾›äº†ä¸€ç§åŸºå‡†è®¾è®¡ï¼Œä»¥ä¿éšœæ¨¡å‹åœ¨ä¸åŒä¿¡é“å‰–é¢(Channel profiles)ä¸‹çš„æ™ºèƒ½è¿è¡Œã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•çš„æ³›åŒ–æ•ˆæœç‹¬ç«‹äºå…·ä½“çš„ç¥ç»ç½‘ç»œæ¶æ„(Neural network architecture)ï¼Œåœ¨å›ºå®šä¿¡é“å‰–é¢å’Œå¯å˜æ—¶å»¶æ‰©å±•(Delay spreads)çš„æ— çº¿ä¿¡é“åœºæ™¯ä¸­å‡å±•ç°å‡ºå“è¶Šçš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "Accepted by IEEE Transactions on Cognitive Communications and Networking (TCCN)",
      "pdf_url": "https://arxiv.org/pdf/2507.12630v2",
      "published_date": "2025-07-16 21:04:37 UTC",
      "updated_date": "2025-07-18 21:16:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:01:02.695867+00:00"
    },
    {
      "arxiv_id": "2507.12619v1",
      "title": "BootSeer: Analyzing and Mitigating Initialization Bottlenecks in Large-Scale LLM Training",
      "title_zh": "BootSeerï¼šå¤§è§„æ¨¡ LLM è®­ç»ƒä¸­çš„åˆå§‹åŒ–ç“¶é¢ˆåˆ†æä¸ç¼“è§£",
      "authors": [
        "Rui Li",
        "Xiaoyun Zhi",
        "Jinxin Chi",
        "Menghan Yu",
        "Lixin Huang",
        "Jia Zhu",
        "Weilun Zhang",
        "Xing Ma",
        "Wenjia Liu",
        "Zhicheng Zhu",
        "Daowen Luo",
        "Zuquan Song",
        "Xin Yin",
        "Chao Xiang",
        "Shuguang Wang",
        "Wencong Xiao",
        "Gene Cooperman"
      ],
      "abstract": "Large Language Models (LLMs) have become a cornerstone of modern AI, driving breakthroughs in natural language processing and expanding into multimodal jobs involving images, audio, and video. As with most computational software, it is important to distinguish between ordinary runtime performance and startup overhead. Prior research has focused on runtime performance: improving training efficiency and stability. This work focuses instead on the increasingly critical issue of startup overhead in training: the delay before training jobs begin execution. Startup overhead is particularly important in large, industrial-scale LLMs, where failures occur more frequently and multiple teams operate in iterative update-debug cycles. In one of our training clusters, more than 3.5% of GPU time is wasted due to startup overhead alone.\n  In this work, we present the first in-depth characterization of LLM training startup overhead based on real production data. We analyze the components of startup cost, quantify its direct impact, and examine how it scales with job size. These insights motivate the design of Bootseer, a system-level optimization framework that addresses three primary startup bottlenecks: (a) container image loading, (b) runtime dependency installation, and (c) model checkpoint resumption. To mitigate these bottlenecks, Bootseer introduces three techniques: (a) hot block record-and-prefetch, (b) dependency snapshotting, and (c) striped HDFS-FUSE. Bootseer has been deployed in a production environment and evaluated on real LLM training workloads, demonstrating a 50% reduction in startup overhead.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡å¤§è¯­è¨€æ¨¡å‹ (LLMs) è®­ç»ƒä¸­æ—¥ç›Šä¸¥å³»çš„å¯åŠ¨å¼€é”€ (startup overhead) é—®é¢˜è¿›è¡Œäº†æ·±å…¥åˆ†æï¼ŒæŒ‡å‡ºåœ¨å·¥ä¸šçº§é›†ç¾¤ä¸­ï¼Œä»…å¯åŠ¨å»¶è¿Ÿå°±æµªè´¹äº†è¶…è¿‡3.5%çš„GPUæ—¶é—´ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¯¹çœŸå®ç”Ÿäº§æ•°æ®çš„åˆ»ç”»ï¼Œè¯†åˆ«å‡ºå¯åŠ¨æˆæœ¬çš„ä¸»è¦ç»„æˆéƒ¨åˆ†ï¼Œå¹¶é‡åŒ–äº†å…¶éšä½œä¸šè§„æ¨¡æ‰©å±•çš„å½±å“ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæœ¬æ–‡æå‡ºäº† BootSeerï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³å®¹å™¨é•œåƒåŠ è½½ (container image loading)ã€è¿è¡Œæ—¶ä¾èµ–å®‰è£… (runtime dependency installation) ä»¥åŠæ¨¡å‹æ£€æŸ¥ç‚¹æ¢å¤ (model checkpoint resumption) ä¸‰å¤§ç“¶é¢ˆçš„ç³»ç»Ÿçº§ä¼˜åŒ–æ¡†æ¶ã€‚BootSeer å¼•å…¥äº†çƒ­å—è®°å½•é¢„å– (hot block record-and-prefetch)ã€ä¾èµ–é¡¹å¿«ç…§ (dependency snapshotting) ä»¥åŠæ¡å¸¦åŒ– HDFS-FUSE (striped HDFS-FUSE) ä¸‰é¡¹æ ¸å¿ƒæŠ€æœ¯ã€‚è¯¥ç³»ç»Ÿå·²åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ï¼Œå®éªŒè¯æ˜ BootSeer åœ¨çœŸå®çš„å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒå·¥ä½œè´Ÿè½½ä¸‹æˆåŠŸå°†å¯åŠ¨å¼€é”€é™ä½äº†50%ï¼Œæ˜¾è‘—æå‡äº†å¤§è§„æ¨¡é›†ç¾¤çš„èµ„æºåˆ©ç”¨ç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 14 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.12619v1",
      "published_date": "2025-07-16 20:32:33 UTC",
      "updated_date": "2025-07-16 20:32:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:01:04.540578+00:00"
    },
    {
      "arxiv_id": "2507.12612v2",
      "title": "Learning What Matters: Probabilistic Task Selection via Mutual Information for Model Finetuning",
      "title_zh": "å­¦å…¶æ‰€é‡ï¼šåŸºäºäº’ä¿¡æ¯çš„æ¨¡å‹å¾®è°ƒæ¦‚ç‡ä»»åŠ¡é€‰æ‹©",
      "authors": [
        "Prateek Chanda",
        "Saral Sureka",
        "Parth Pratim Chatterjee",
        "Krishnateja Killamsetty",
        "Nikhil Shivakumar Nayak",
        "Ganesh Ramakrishnan"
      ],
      "abstract": "The performance of finetuned large language models (LLMs) hinges critically on the composition of the training mixture. However, selecting an optimal blend of task datasets remains a largely manual, heuristic driven process, with practitioners often relying on uniform or size based sampling strategies. We introduce TASKPGM, a principled and scalable framework for mixture optimization that selects continuous task proportions by minimizing an energy function over a Markov Random Field (MRF). Task relationships are modeled using behavioral divergences such as Jensen Shannon Divergence and Pointwise Mutual Information computed from the predictive distributions of single task finetuned models. Our method yields a closed form solution under simplex constraints and provably balances representativeness and diversity among tasks. We provide theoretical guarantees, including weak submodularity for budgeted variants, and demonstrate consistent empirical improvements on Llama 2 and Mistral across evaluation suites such as MMLU and BIGBench. Beyond performance, TASKPGM offers interpretable insights into task influence and mixture composition, making it a powerful tool for efficient and robust LLM finetuning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å¾®è°ƒè¿‡ç¨‹ä¸­ä»»åŠ¡æ•°æ®æ¯”ä¾‹é€‰æ‹©ä¸»è¦ä¾èµ–äººå·¥å¯å‘å¼æ–¹æ³•çš„é—®é¢˜ï¼Œæå‡ºäº†TASKPGMæ¡†æ¶ã€‚è¿™æ˜¯ä¸€ä¸ªåŸåˆ™æ€§å¼ºä¸”å¯æ‰©å±•çš„ä»»åŠ¡æ··åˆä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡åœ¨é©¬å°”å¯å¤«éšæœºåœº(Markov Random Field)ä¸Šæœ€å°åŒ–èƒ½é‡å‡½æ•°æ¥ç¡®å®šè¿ç»­çš„ä»»åŠ¡æ¯”ä¾‹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å•ä»»åŠ¡å¾®è°ƒæ¨¡å‹çš„é¢„æµ‹åˆ†å¸ƒï¼Œé€šè¿‡Jensen Shannon Divergenceå’ŒPointwise Mutual Informationç­‰æŒ‡æ ‡æ¥å»ºæ¨¡ä»»åŠ¡é—´çš„è¡Œä¸ºå·®å¼‚ã€‚TASKPGMåœ¨å•çº¯å½¢çº¦æŸä¸‹æä¾›äº†é—­å¼è§£ï¼Œä¸”åœ¨ç†è®ºä¸Šä¿è¯äº†ä»»åŠ¡çš„é€‰æ‹©èƒ½å¤Ÿå¹³è¡¡ä»£è¡¨æ€§ä¸å¤šæ ·æ€§ã€‚åœ¨Llama 2å’ŒMistralæ¨¡å‹ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨MMLUå’ŒBIG-Benchç­‰è¯„ä¼°å¥—ä»¶ä¸­å‡å–å¾—äº†æŒç»­çš„æ€§èƒ½æå‡ã€‚é™¤äº†æ€§èƒ½ä¼˜åŠ¿å¤–ï¼ŒTASKPGMè¿˜ä¸ºä»»åŠ¡å½±å“åŠ›å’Œæ··åˆæ„æˆæä¾›äº†å…·æœ‰å¯è§£é‡Šæ€§çš„æ´å¯Ÿï¼Œä¸ºé«˜æ•ˆã€é²æ£’çš„LLMå¾®è°ƒæä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9, 8 tables, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.12612v2",
      "published_date": "2025-07-16 20:14:55 UTC",
      "updated_date": "2025-08-07 04:25:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:01:12.282127+00:00"
    },
    {
      "arxiv_id": "2507.12602v1",
      "title": "MS-DGCNN++: A Multi-Scale Fusion Dynamic Graph Neural Network with Biological Knowledge Integration for LiDAR Tree Species Classification",
      "title_zh": "MS-DGCNN++ï¼šé›†æˆç”Ÿç‰©å­¦çŸ¥è¯†çš„æ¿€å…‰é›·è¾¾æ ‘ç§åˆ†ç±»å¤šå°ºåº¦èåˆåŠ¨æ€å›¾ç¥ç»ç½‘ç»œ",
      "authors": [
        "Said Ohamouddou",
        "Abdellatif El Afia",
        "Hanaa El Afia",
        "Raddouane Chiheb"
      ],
      "abstract": "Tree species classification from terrestrial LiDAR point clouds is challenging because of the complex multi-scale geometric structures in forest environments. Existing approaches using multi-scale dynamic graph convolutional neural networks (MS-DGCNN) employ parallel multi-scale processing, which fails to capture the semantic relationships between the hierarchical levels of the tree architecture. We present MS-DGCNN++, a hierarchical multiscale fusion dynamic graph convolutional network that uses semantically meaningful feature extraction at local, branch, and canopy scales with cross-scale information propagation. Our method employs scale-specific feature engineering, including standard geometric features for the local scale, normalized relative vectors for the branch scale, and distance information for the canopy scale. This hierarchical approach replaces uniform parallel processing with semantically differentiated representations that are aligned with the natural tree structure. Under the same proposed tree species data augmentation strategy for all experiments, MS-DGCNN++ achieved an accuracy of 94.96 \\% on STPCTLS, outperforming DGCNN, MS-DGCNN, and the state-of-the-art model PPT. On FOR-species20K, it achieves 67.25\\% accuracy (6.1\\% improvement compared to MS-DGCNN). For standard 3D object recognition, our method outperformed DGCNN and MS-DGCNN with overall accuracies of 93.15\\% on ModelNet40 and 94.05\\% on ModelNet10. With lower parameters and reduced complexity compared to state-of-the-art transformer approaches, our method is suitable for resource-constrained applications while maintaining a competitive accuracy. Beyond tree classification, the method generalizes to standard 3D object recognition, establishing it as a versatile solution for diverse point cloud processing applications. The implementation code is publicly available at https://github.com/said-ohamouddou/MS-DGCNN2.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£®æ—ç¯å¢ƒä¸­é™†åœ°æ¿€å…‰é›·è¾¾ï¼ˆLiDARï¼‰ç‚¹äº‘æ ‘ç§åˆ†ç±»é¢ä¸´çš„å¤šå°ºåº¦å‡ ä½•ç»“æ„å¤æ‚æ€§ï¼Œä»¥åŠç°æœ‰MS-DGCNNæ¨¡å‹åœ¨å¤„ç†æ ‘æœ¨æ¶æ„æ—¶ç¼ºä¹å±‚çº§è¯­ä¹‰å…³è”çš„é—®é¢˜ï¼Œæå‡ºäº†MS-DGCNN++ã€‚è¿™æ˜¯ä¸€ç§é›†æˆç”Ÿç‰©å­¦çŸ¥è¯†çš„å±‚çº§å¤šå°ºåº¦èåˆåŠ¨æ€å›¾å·ç§¯ç½‘ç»œï¼Œé€šè¿‡è·¨å°ºåº¦ä¿¡æ¯ä¼ æ’­å®ç°äº†å±€éƒ¨ã€åˆ†æå’Œå† å±‚å°ºåº¦çš„è¯­ä¹‰ç‰¹å¾æå–ã€‚è¯¥æ–¹æ³•é‡‡ç”¨äº†å°ºåº¦ç‰¹å®šçš„ç‰¹å¾å·¥ç¨‹ï¼ŒåŒ…æ‹¬å±€éƒ¨å°ºåº¦çš„æ ‡å‡†å‡ ä½•ç‰¹å¾ã€åˆ†æå°ºåº¦çš„å½’ä¸€åŒ–ç›¸å¯¹çŸ¢é‡ä»¥åŠå† å±‚å°ºåº¦çš„è·ç¦»ä¿¡æ¯ï¼Œå°†ä¼ ç»Ÿçš„å¹¶è¡Œå¤„ç†è½¬å˜ä¸ºä¸è‡ªç„¶æ ‘æœ¨ç»“æ„ä¸€è‡´çš„å±‚çº§åŒ–è¡¨å¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMS-DGCNN++ åœ¨ STPCTLS æ•°æ®é›†ä¸Šè¾¾åˆ°äº† 94.96% çš„å‡†ç¡®ç‡ï¼Œä¼˜äº DGCNNã€MS-DGCNN åŠ SOTA æ¨¡å‹ PPTï¼Œå¹¶åœ¨ FOR-species20K æ•°æ®é›†ä¸Šè¾ƒå‰ä»£æ¨¡å‹æå‡äº† 6.1%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ ModelNet40 ç­‰æ ‡å‡† 3D ç‰©ä½“è¯†åˆ«ä»»åŠ¡ä¸­åŒæ ·è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶ä¼˜ç§€çš„æ³›åŒ–èƒ½åŠ›ã€‚ç›¸æ¯”äºå…ˆè¿›çš„ Transformer æ¨¡å‹ï¼ŒMS-DGCNN++ å…·æœ‰æ›´ä½çš„å‚æ•°é‡å’Œè®¡ç®—å¤æ‚åº¦ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„ç‚¹äº‘å¤„ç†æä¾›äº†é«˜æ•ˆä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12602v1",
      "published_date": "2025-07-16 19:44:23 UTC",
      "updated_date": "2025-07-16 19:44:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:02:14.786863+00:00"
    },
    {
      "arxiv_id": "2507.12599v1",
      "title": "A Survey of Explainable Reinforcement Learning: Targets, Methods and Needs",
      "title_zh": "å¯è§£é‡Šå¼ºåŒ–å­¦ä¹ ç»¼è¿°ï¼šç›®æ ‡ã€æ–¹æ³•ä¸éœ€æ±‚",
      "authors": [
        "LÃ©o SauliÃ¨res"
      ],
      "abstract": "The success of recent Artificial Intelligence (AI) models has been accompanied by the opacity of their internal mechanisms, due notably to the use of deep neural networks. In order to understand these internal mechanisms and explain the output of these AI models, a set of methods have been proposed, grouped under the domain of eXplainable AI (XAI). This paper focuses on a sub-domain of XAI, called eXplainable Reinforcement Learning (XRL), which aims to explain the actions of an agent that has learned by reinforcement learning. We propose an intuitive taxonomy based on two questions \"What\" and \"How\". The first question focuses on the target that the method explains, while the second relates to the way the explanation is provided. We use this taxonomy to provide a state-of-the-art review of over 250 papers. In addition, we present a set of domains close to XRL, which we believe should get attention from the community. Finally, we identify some needs for the field of XRL.",
      "tldr_zh": "æœ¬æ–‡å¯¹å¯è§£é‡Šå¼ºåŒ–å­¦ä¹ ï¼ˆeXplainable Reinforcement Learning, XRLï¼‰è¿›è¡Œäº†ç³»ç»Ÿçš„ç»¼è¿°ï¼Œæ—¨åœ¨è§£å†³æ·±åº¦ç¥ç»ç½‘ç»œåœ¨äººå·¥æ™ºèƒ½æ¨¡å‹åº”ç”¨ä¸­æ™®éå­˜åœ¨çš„é»‘ç›’ä¸é€æ˜é—®é¢˜ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºâ€œè§£é‡Šå¯¹è±¡ï¼ˆWhatï¼‰â€å’Œâ€œè§£é‡Šæ–¹å¼ï¼ˆHowï¼‰â€ä¸¤ä¸ªç»´åº¦çš„ç›´è§‚åˆ†ç±»æ³•ï¼ˆTaxonomyï¼‰ï¼Œå¹¶åˆ©ç”¨è¯¥åˆ†ç±»æ³•å¯¹è¶…è¿‡250ç¯‡ç›¸å…³æ–‡çŒ®è¿›è¡Œäº†è¯¦å°½çš„æ¢³ç†ä¸è¯„ä¼°ã€‚é™¤äº†æ€»ç»“ç°æœ‰æŠ€æœ¯ï¼Œæ–‡ç« è¿˜æ¢è®¨äº†ä¸XRLç´§å¯†ç›¸å…³çš„é‚»è¿‘é¢†åŸŸï¼Œè®¤ä¸ºè¿™äº›é¢†åŸŸåº”å½“å¼•èµ·å­¦æœ¯ç•Œçš„è¿›ä¸€æ­¥å…³æ³¨ã€‚æœ€åï¼Œç ”ç©¶è¯†åˆ«å¹¶æ€»ç»“äº†XRLé¢†åŸŸçš„å…³é”®éœ€æ±‚ï¼Œä¸ºæœªæ¥æ¢ç´¢å¦‚ä½•ç†è§£å’Œè§£é‡Šå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“çš„è¡Œä¸ºæä¾›äº†é‡è¦çš„ç†è®ºæŒ‡å¼•ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "69 pages, 19 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.12599v1",
      "published_date": "2025-07-16 19:41:41 UTC",
      "updated_date": "2025-07-16 19:41:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:01:14.435029+00:00"
    },
    {
      "arxiv_id": "2507.12574v2",
      "title": "Assay2Mol: large language model-based drug design using BioAssay context",
      "title_zh": "Assay2Molï¼šåŸºäº BioAssay ä¸Šä¸‹æ–‡çš„å¤§è¯­è¨€æ¨¡å‹è¯ç‰©è®¾è®¡",
      "authors": [
        "Yifan Deng",
        "Spencer S. Ericksen",
        "Anthony Gitter"
      ],
      "abstract": "Scientific databases aggregate vast amounts of quantitative data alongside descriptive text. In biochemistry, molecule screening assays evaluate candidate molecules' functional responses against disease targets. Unstructured text that describes the biological mechanisms through which these targets operate, experimental screening protocols, and other attributes of assays offer rich information for drug discovery campaigns but has been untapped because of that unstructured format. We present Assay2Mol, a large language model-based workflow that can capitalize on the vast existing biochemical screening assays for early-stage drug discovery. Assay2Mol retrieves existing assay records involving targets similar to the new target and generates candidate molecules using in-context learning with the retrieved assay screening data. Assay2Mol outperforms recent machine learning approaches that generate candidate ligand molecules for target protein structures, while also promoting more synthesizable molecule generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Assay2Molï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ (Large Language Model) çš„è¯ç‰©è®¾è®¡å·¥ä½œæµï¼Œæ—¨åœ¨åˆ©ç”¨ä¸°å¯Œçš„ç”Ÿç‰©æµ‹å®š (BioAssay) èƒŒæ™¯ä¿¡æ¯è¾…åŠ©æ—©æœŸè¯ç‰©ç ”å‘ã€‚é’ˆå¯¹æè¿°ç”Ÿç‰©æœºåˆ¶å’Œå®éªŒæ–¹æ¡ˆç­‰éç»“æ„åŒ–æ–‡æœ¬ä¿¡æ¯é•¿æœŸæœªè¢«å……åˆ†åˆ©ç”¨çš„ç°çŠ¶ï¼ŒAssay2Mol é€šè¿‡æ£€ç´¢ä¸æ–°é¶ç‚¹ç›¸ä¼¼çš„ç°æœ‰æµ‹å®šè®°å½•ï¼Œå¹¶ç»“åˆä¸Šä¸‹æ–‡å­¦ä¹  (In-context Learning) æŠ€æœ¯ç”Ÿæˆå€™é€‰åˆ†å­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAssay2Mol åœ¨ç”Ÿæˆé’ˆå¯¹è›‹ç™½è´¨ç»“æ„çš„å€™é€‰é…ä½“åˆ†å­æ–¹é¢æ€§èƒ½ä¼˜äºç°æœ‰çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„åˆ†å­å…·æœ‰æ›´é«˜çš„å¯åˆæˆæ€§ (Synthesizability)ï¼Œä¸ºåˆ©ç”¨æµ·é‡éç»“æ„åŒ–ç”ŸåŒ–æ•°æ®é©±åŠ¨è‡ªåŠ¨åŒ–è¯ç‰©å‘ç°æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "26 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.12574v2",
      "published_date": "2025-07-16 18:42:18 UTC",
      "updated_date": "2025-09-24 16:03:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:01:19.134614+00:00"
    },
    {
      "arxiv_id": "2507.12568v1",
      "title": "Safeguarding Federated Learning-based Road Condition Classification",
      "title_zh": "ä¿éšœåŸºäºè”é‚¦å­¦ä¹ çš„è·¯å†µåˆ†ç±»å®‰å…¨",
      "authors": [
        "Sheng Liu",
        "Panos Papadimitratos"
      ],
      "abstract": "Federated Learning (FL) has emerged as a promising solution for privacy-preserving autonomous driving, specifically camera-based Road Condition Classification (RCC) systems, harnessing distributed sensing, computing, and communication resources on board vehicles without sharing sensitive image data. However, the collaborative nature of FL-RCC frameworks introduces new vulnerabilities: Targeted Label Flipping Attacks (TLFAs), in which malicious clients (vehicles) deliberately alter their training data labels to compromise the learned model inference performance. Such attacks can, e.g., cause a vehicle to mis-classify slippery, dangerous road conditions as pristine and exceed recommended speed. However, TLFAs for FL-based RCC systems are largely missing. We address this challenge with a threefold contribution: 1) we disclose the vulnerability of existing FL-RCC systems to TLFAs; 2) we introduce a novel label-distance-based metric to precisely quantify the safety risks posed by TLFAs; and 3) we propose FLARE, a defensive mechanism leveraging neuron-wise analysis of the output layer to mitigate TLFA effects. Extensive experiments across three RCC tasks, four evaluation metrics, six baselines, and three deep learning models demonstrate both the severity of TLFAs on FL-RCC systems and the effectiveness of FLARE in mitigating the attack impact.",
      "tldr_zh": "è¯¥ç ”ç©¶é‡ç‚¹å…³æ³¨åŸºäº Federated Learning (FL) çš„è·¯å†µåˆ†ç±» (Road Condition Classification, RCC) ç³»ç»Ÿåœ¨è‡ªåŠ¨é©¾é©¶éšç§ä¿æŠ¤ä¸­çš„å®‰å…¨æ€§ã€‚ç ”ç©¶æŠ«éœ²äº†æ­¤ç±»åä½œæ¡†æ¶å®¹æ˜“å—åˆ° Targeted Label Flipping Attacks (TLFAs) çš„å¨èƒï¼Œæ¶æ„è½¦è¾†å¯é€šè¿‡æ•…æ„ç¯¡æ”¹è®­ç»ƒæ ‡ç­¾è¯±å¯¼æ¨¡å‹å°†å±é™©è·¯å†µé”™è¯¯åˆ†ç±»ï¼Œä»è€Œå¼•å‘ä¸¥é‡é©¾é©¶é£é™©ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº label-distance-based çš„æ–°åº¦é‡æŒ‡æ ‡ï¼Œç”¨äºç²¾ç¡®é‡åŒ– TLFAs å¸¦æ¥çš„å®‰å…¨é£é™©ã€‚åŒæ—¶ï¼Œä½œè€…æå‡ºäº†åä¸º FLARE çš„é˜²å¾¡æœºåˆ¶ï¼Œè¯¥æœºåˆ¶é€šè¿‡å¯¹è¾“å‡ºå±‚è¿›è¡Œ neuron-wise analysis æ¥è¯†åˆ«å¹¶å‡è½»æ”»å‡»å½±å“ã€‚åœ¨ä¸‰é¡¹ RCC ä»»åŠ¡ã€å¤šä¸ªåŸºå‡†æ¨¡å‹å’Œè¯„ä¼°æŒ‡æ ‡ä¸Šçš„å¹¿æ³›å®éªŒä¸ä»…æ­ç¤ºäº† TLFAs çš„ä¸¥é‡æ€§ï¼Œä¹Ÿè¯æ˜äº† FLARE åœ¨ä¿éšœè·¯å†µåˆ†ç±»ç³»ç»Ÿå®‰å…¨æ€§æ–¹é¢çš„æ˜¾è‘—æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by IEEE Conference on Communications and Network Security (CNS) 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.12568v1",
      "published_date": "2025-07-16 18:33:29 UTC",
      "updated_date": "2025-07-16 18:33:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:02:18.793192+00:00"
    },
    {
      "arxiv_id": "2507.12555v2",
      "title": "Can Mental Imagery Improve the Thinking Capabilities of AI Systems?",
      "title_zh": "å¿ƒç†æ„è±¡èƒ½å¦æå‡äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ€ç»´èƒ½åŠ›ï¼Ÿ",
      "authors": [
        "Slimane Larabi"
      ],
      "abstract": "Although existing models can interact with humans and provide satisfactory responses, they lack the ability to act autonomously or engage in independent reasoning. Furthermore, input data in these models is typically provided as explicit queries, even when some sensory data is already acquired.\n  In addition, AI agents, which are computational entities designed to perform tasks and make decisions autonomously based on their programming, data inputs, and learned knowledge, have shown significant progress. However, they struggle with integrating knowledge across multiple domains, unlike humans.\n  Mental imagery plays a fundamental role in the brain's thinking process, which involves performing tasks based on internal multisensory data, planned actions, needs, and reasoning capabilities. In this paper, we investigate how to integrate mental imagery into a machine thinking framework and how this could be beneficial in initiating the thinking process. Our proposed machine thinking framework integrates a Cognitive thinking unit supported by three auxiliary units: the Input Data Unit, the Needs Unit, and the Mental Imagery Unit. Within this framework, data is represented as natural language sentences or drawn sketches, serving both informative and decision-making purposes. We conducted validation tests for this framework, and the results are presented and discussed.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å°†å¿ƒç†æ„è±¡(Mental Imagery)é›†æˆåˆ°æœºå™¨æ€ç»´æ¡†æ¶ä¸­ï¼Œä»¥è§£å†³å½“å‰AIç³»ç»Ÿåœ¨è‡ªä¸»è¡ŒåŠ¨ã€ç‹¬ç«‹æ¨ç†ä»¥åŠå¤šé¢†åŸŸçŸ¥è¯†æ•´åˆæ–¹é¢çš„å±€é™æ€§ã€‚ä½œè€…å€Ÿé‰´äº†äººç±»å¤§è„‘åˆ©ç”¨å†…éƒ¨å¤šæ„Ÿå®˜æ•°æ®è¿›è¡Œæ€è€ƒçš„æœºåˆ¶ï¼Œæå‡ºäº†ä¸€ä¸ªç”±è®¤çŸ¥æ€ç»´å•å…ƒ(Cognitive thinking unit)åŠä¸‰ä¸ªè¾…åŠ©å•å…ƒï¼ˆè¾“å…¥æ•°æ®å•å…ƒInput Data Unitã€éœ€æ±‚å•å…ƒNeeds Unitå’Œå¿ƒç†æ„è±¡å•å…ƒMental Imagery Unitï¼‰ç»„æˆçš„æœºå™¨æ€ç»´æ¡†æ¶ã€‚åœ¨æ­¤æ¡†æ¶ä¸‹ï¼Œä¿¡æ¯ä»¥è‡ªç„¶è¯­è¨€æˆ–æ‰‹ç»˜è‰å›¾(sketches)çš„å½¢å¼å‘ˆç°ï¼Œå…±åŒæ”¯æŒç³»ç»Ÿçš„å†³ç­–åˆ¶å®šè¿‡ç¨‹ã€‚å®éªŒéªŒè¯ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆåˆ©ç”¨å†…éƒ¨ç”Ÿæˆçš„æ•°æ®å¯åŠ¨æ€ç»´è¿‡ç¨‹ï¼Œä¸ºæå‡äººå·¥æ™ºèƒ½çš„è®¤çŸ¥ä¸æ€è€ƒèƒ½åŠ›æä¾›äº†æ–°çš„å¯è¡Œè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.12555v2",
      "published_date": "2025-07-16 18:06:13 UTC",
      "updated_date": "2025-07-20 15:39:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:02:22.439879+00:00"
    },
    {
      "arxiv_id": "2507.12553v1",
      "title": "Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility",
      "title_zh": "è¿™ä»…ä»…æ˜¯å¹»æƒ³å—ï¼Ÿè¯­è¨€æ¨¡å‹è¡¨å¾åæ˜ äººç±»å¯¹äº‹ä»¶åˆç†æ€§çš„åˆ¤æ–­",
      "authors": [
        "Michael A. Lepori",
        "Jennifer Hu",
        "Ishita Dasgupta",
        "Roma Patel",
        "Thomas Serre",
        "Ellie Pavlick"
      ],
      "abstract": "Language models (LMs) are used for a diverse range of tasks, from question answering to writing fantastical stories. In order to reliably accomplish these tasks, LMs must be able to discern the modal category of a sentence (i.e., whether it describes something that is possible, impossible, completely nonsensical, etc.). However, recent studies have called into question the ability of LMs to categorize sentences according to modality (Michaelov et al., 2025; Kauf et al., 2023). In this work, we identify linear representations that discriminate between modal categories within a variety of LMs, or modal difference vectors. Analysis of modal difference vectors reveals that LMs have access to more reliable modal categorization judgments than previously reported. Furthermore, we find that modal difference vectors emerge in a consistent order as models become more competent (i.e., through training steps, layers, and parameter count). Notably, we find that modal difference vectors identified within LM activations can be used to model fine-grained human categorization behavior. This potentially provides a novel view into how human participants distinguish between modal categories, which we explore by correlating projections along modal difference vectors with human participants' ratings of interpretable features. In summary, we derive new insights into LM modal categorization using techniques from mechanistic interpretability, with the potential to inform our understanding of modal categorization in humans.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ¨¡å‹(LMs)åŒºåˆ†å¥å­è¯­æ°”èŒƒç•´(modal categories)çš„èƒ½åŠ›ï¼Œå³åˆ¤æ–­æè¿°å†…å®¹æ˜¯å¯èƒ½ã€ä¸å¯èƒ½è¿˜æ˜¯æ¯«æ— æ„ä¹‰ã€‚ç ”ç©¶åˆ©ç”¨æœºæ¢°è§£é‡Šæ€§(mechanistic interpretability)æŠ€æœ¯ï¼Œåœ¨å¤šç§æ¨¡å‹ä¸­è¯†åˆ«å‡ºäº†èƒ½å¤ŸåŒºåˆ†ä¸åŒè¯­æ°”èŒƒç•´çš„çº¿æ€§è¡¨ç¤ºï¼Œå³è¯­æ°”å·®å¼‚å‘é‡(modal difference vectors)ã€‚åˆ†æå‘ç°ï¼Œè¯­è¨€æ¨¡å‹å†…éƒ¨è•´å«çš„è¯­æ°”åˆ†ç±»åˆ¤æ–­æ¯”æ­¤å‰æŠ¥å‘Šçš„æ›´ä¸ºå¯é ï¼Œä¸”è¿™äº›å‘é‡éšç€æ¨¡å‹è§„æ¨¡å’Œè®­ç»ƒç¨‹åº¦çš„æå‡è€ŒæŒ‰ä¸€è‡´é¡ºåºå‡ºç°ã€‚æ­¤å¤–ï¼Œè¯­æ°”å·®å¼‚å‘é‡èƒ½æœ‰æ•ˆæ¨¡æ‹Ÿäººç±»ç»†ç²’åº¦çš„åˆ†ç±»è¡Œä¸ºï¼Œåæ˜ å‡ºæ¨¡å‹å†…éƒ¨è¡¨ç¤ºä¸äººç±»è®¤çŸ¥åˆ¤æ–­çš„é«˜åº¦ä¸€è‡´æ€§ã€‚é€šè¿‡å°†å‘é‡æŠ•å½±ä¸äººç±»è¯„åˆ†çš„å¯è§£é‡Šç‰¹å¾è¿›è¡Œå…³è”ï¼Œæœ¬ç ”ç©¶ä¸ºç†è§£äººå·¥æ™ºèƒ½ä¸äººç±»å¦‚ä½•åŒºåˆ†è¯­æ°”èŒƒç•´æä¾›äº†å…¨æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12553v1",
      "published_date": "2025-07-16 18:04:26 UTC",
      "updated_date": "2025-07-16 18:04:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:01:27.105979+00:00"
    },
    {
      "arxiv_id": "2507.12547v2",
      "title": "Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models",
      "title_zh": "å°†å¼€æ”¾ä¸–ç•Œè®¤çŸ¥å»ºæ¨¡ä¸ºæ¦‚ç‡æ¨¡å‹çš„æŒ‰éœ€åˆæˆ",
      "authors": [
        "Lionel Wong",
        "Katherine M. Collins",
        "Lance Ying",
        "Cedegao E. Zhang",
        "Adrian Weller",
        "Tobias Gerstenberg",
        "Timothy O'Donnell",
        "Alexander K. Lew",
        "Jacob D. Andreas",
        "Joshua B. Tenenbaum",
        "Tyler Brooke-Wilson"
      ],
      "abstract": "When faced with novel situations, people are able to marshal relevant considerations from a wide range of background knowledge and put these to use in inferences and predictions. What permits us to draw in globally relevant information and reason over it coherently? Here, we explore the hypothesis that people use a combination of distributed and symbolic representations to construct bespoke mental models tailored to novel situations. We propose a computational implementation of this idea -- a ``Model Synthesis Architecture'' (MSA) -- using language models to implement global relevance-based retrieval and model synthesis and probabilistic programs to implement bespoke, coherent world models. We evaluate our MSA as a model of human judgments on a novel reasoning dataset. The dataset -- built around a `Model Olympics` domain of sports vignettes -- tests models' capacity for human-like, open-ended reasoning by requiring (i) judgments about novel causal structures described in language; (ii) drawing on large bodies of background knowledge; and (iii) doing both in light of observations that introduce arbitrary novel variables. Our MSA approach captures human judgments better than language model-only baselines, under both direct and chain-of-thought generations from the LM that supports model synthesis. These results suggest that MSAs can be implemented in a way that mirrors people's ability to deliver locally coherent reasoning over globally relevant variables, offering a path to understanding and replicating human reasoning in open-ended domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†æ¨¡å‹åˆæˆæ¶æ„ (Model Synthesis Architecture, MSA)ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿäººç±»åœ¨é¢å¯¹æ–°æƒ…å¢ƒæ—¶æ•´åˆèƒŒæ™¯çŸ¥è¯†å¹¶æ„å»ºå®šåˆ¶åŒ–å¿ƒç†æ¨¡å‹çš„èƒ½åŠ›ã€‚è¯¥æ¶æ„åˆ©ç”¨è¯­è¨€æ¨¡å‹ (Language Models) å®ç°åŸºäºå…¨å±€ç›¸å…³æ€§çš„æ£€ç´¢ä¸æ¨¡å‹åˆæˆï¼Œå¹¶ç»“åˆæ¦‚ç‡ç¼–ç¨‹ (Probabilistic Programs) æ„å»ºå‡ºè¿è´¯çš„å®šåˆ¶åŒ–ä¸–ç•Œæ¨¡å‹ã€‚é€šè¿‡åœ¨åŒ…å«è¿åŠ¨ç‰‡æ®µçš„ â€œModel Olympicsâ€ æ¨ç†æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶éªŒè¯äº†è¯¥æ¨¡å‹åœ¨å¤„ç†æ–°é¢–å› æœç»“æ„å’Œå¼€æ”¾å¼æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨ç›´æ¥ç”Ÿæˆè¿˜æ˜¯é“¾å¼æ€ç»´ (Chain-of-Thought) æç¤ºä¸‹ï¼ŒMSA å¯¹äººç±»åˆ¤æ–­çš„é¢„æµ‹å‡†ç¡®æ€§å‡ä¼˜äºå•çº¯çš„è¯­è¨€æ¨¡å‹åŸºçº¿ã€‚è¿™é¡¹æˆæœè¯æ˜äº† MSA èƒ½å¤Ÿæœ‰æ•ˆæ¨¡æ‹Ÿäººç±»åœ¨å¼€æ”¾é¢†åŸŸä¸­æ•´åˆå…¨å±€å˜é‡å¹¶è¿›è¡Œå±€éƒ¨è¿è´¯æ¨ç†çš„è¿‡ç¨‹ï¼Œä¸ºäººå·¥æ™ºèƒ½å®ç°ç±»äººæ¨ç†æä¾›äº†é‡è¦è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.CL",
      "comment": "Presented at CogSci 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.12547v2",
      "published_date": "2025-07-16 18:01:03 UTC",
      "updated_date": "2025-07-18 06:48:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:03:06.734490+00:00"
    },
    {
      "arxiv_id": "2507.12508v2",
      "title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning",
      "title_zh": "MindJourneyï¼šåŸºäºä¸–ç•Œæ¨¡å‹çš„ç©ºé—´æ¨ç†æµ‹è¯•æ—¶æ‰©å±•",
      "authors": [
        "Yuncong Yang",
        "Jiageng Liu",
        "Zheyuan Zhang",
        "Siyuan Zhou",
        "Reuben Tan",
        "Jianwei Yang",
        "Yilun Du",
        "Chuang Gan"
      ],
      "abstract": "Spatial reasoning in 3D space is central to human cognition and indispensable for embodied tasks such as navigation and manipulation. However, state-of-the-art vision-language models (VLMs) struggle frequently with tasks as simple as anticipating how a scene will look after an egocentric motion: they perceive 2D images but lack an internal model of 3D dynamics. We therefore propose MindJourney, a test-time scaling framework that grants a VLM with this missing capability by coupling it to a controllable world model based on video diffusion. The VLM iteratively sketches a concise camera trajectory, while the world model synthesizes the corresponding view at each step. The VLM then reasons over this multi-view evidence gathered during the interactive exploration. Without any fine-tuning, our MindJourney achieves over an average 7.7% performance boost on the representative spatial reasoning benchmark SAT, showing that pairing VLMs with world models for test-time scaling offers a simple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also improves upon the test-time inference VLMs trained through reinforcement learning, which demonstrates the potential of our method that utilizes world models for test-time scaling.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨ä¸‰ç»´ç©ºé—´æ¨ç†æ–¹é¢å› ç¼ºä¹ä¸‰ç»´åŠ¨åŠ›å­¦æ¨¡å‹è€Œè¡¨ç°æ¬ ä½³çš„é—®é¢˜ï¼Œæå‡ºäº†MindJourneyæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æµ‹è¯•æ—¶ç¼©æ”¾(test-time scaling)ç­–ç•¥ï¼Œå°†VLMä¸åŸºäºè§†é¢‘æ‰©æ•£(video diffusion)çš„å¯æ§ä¸–ç•Œæ¨¡å‹(world model)ç›¸ç»“åˆã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒVLMè¿­ä»£åœ°å‹¾å‹’å‡ºç›¸æœºè½¨è¿¹ï¼Œç”±ä¸–ç•Œæ¨¡å‹åˆæˆå¯¹åº”çš„è§†è§‰è§†å›¾ï¼ŒéšåVLMå¯¹æ”¶é›†åˆ°çš„å¤šè§†å›¾è¯æ®è¿›è¡Œç»¼åˆæ¨ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æ— éœ€ä»»ä½•å¾®è°ƒ(fine-tuning)çš„æƒ…å†µä¸‹ï¼ŒMindJourneyåœ¨ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•SATä¸Šå®ç°äº†å¹³å‡7.7%çš„æ€§èƒ½æå‡ã€‚è¿™ä¸€æˆæœè¯æ˜äº†å°†VLMsä¸ä¸–ç•Œæ¨¡å‹ç»“åˆæ˜¯å®ç°é²æ£’3Dæ¨ç†çš„ä¸€ç§å³æ’å³ç”¨æ–¹æ¡ˆï¼Œä¸”å…¶æ€§èƒ½ä¼˜äºç»è¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æµ‹è¯•æ—¶æ¨ç†æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ä»…ä¸ºå¢å¼ºæ¨¡å‹çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›æä¾›äº†æ–°è·¯å¾„ï¼Œæ›´å±•ç¤ºäº†åˆ©ç”¨ä¸–ç•Œæ¨¡å‹è¿›è¡Œæµ‹è¯•æ—¶ç¼©æ”¾çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://umass-embodied-agi.github.io/MindJourney",
      "pdf_url": "https://arxiv.org/pdf/2507.12508v2",
      "published_date": "2025-07-16 17:59:36 UTC",
      "updated_date": "2025-11-01 05:54:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:02:39.794410+00:00"
    },
    {
      "arxiv_id": "2507.12507v1",
      "title": "Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training",
      "title_zh": "å¼ºåŒ–å­¦ä¹ è§„æ¨¡åŒ–ï¼šé€šè¿‡é•¿æ—¶é—´è®­ç»ƒè§£é”å¤§è¯­è¨€æ¨¡å‹çš„å¤šæ ·åŒ–æ¨ç†èƒ½åŠ›",
      "authors": [
        "Mingjie Liu",
        "Shizhe Diao",
        "Jian Hu",
        "Ximing Lu",
        "Xin Dong",
        "Hao Zhang",
        "Alexander Bukharin",
        "Shaokun Zhang",
        "Jiaqi Zeng",
        "Makesh Narsimhan Sreedhar",
        "Gerald Shen",
        "David Mosallanezhad",
        "Di Zhang",
        "Jonas Yang",
        "June Yang",
        "Oleksii Kuchaiev",
        "Guilin Liu",
        "Zhiding Yu",
        "Pavlo Molchanov",
        "Yejin Choi",
        "Jan Kautz",
        "Yi Dong"
      ],
      "abstract": "Recent advancements in reasoning-focused language models such as OpenAI's O1 and DeepSeek-R1 have shown that scaling test-time computation-through chain-of-thought reasoning and iterative exploration-can yield substantial improvements on complex tasks like mathematics and code generation. These breakthroughs have been driven by large-scale reinforcement learning (RL), particularly when combined with verifiable reward signals that provide objective and grounded supervision. In this report, we investigate the effects of prolonged reinforcement learning on a small language model across a diverse set of reasoning domains. Our work identifies several key ingredients for effective training, including the use of verifiable reward tasks, enhancements to Group Relative Policy Optimization (GRPO), and practical techniques to improve training stability and generalization. We introduce controlled KL regularization, clipping ratio, and periodic reference policy resets as critical components for unlocking long-term performance gains. Our model achieves significant improvements over strong baselines, including +14.7% on math, +13.9% on coding, and +54.8% on logic puzzle tasks. To facilitate continued research, we release our model publicly.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤šæ ·åŒ–æ¨ç†é¢†åŸŸå¯¹å°å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé•¿æ—¶é—´å¼ºåŒ–å­¦ä¹ (RL)çš„å½±å“ï¼Œæ—¨åœ¨é€šè¿‡æ‰©å±•æµ‹è¯•æ—¶è®¡ç®—å’ŒéªŒè¯æ€§å¥–åŠ±ä¿¡å·æ¥æå‡æ¨¡å‹å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚ä½œè€…è¯†åˆ«äº†æœ‰æ•ˆè®­ç»ƒçš„å…³é”®è¦ç´ ï¼ŒåŒ…æ‹¬ä½¿ç”¨éªŒè¯æ€§å¥–åŠ±ä»»åŠ¡(verifiable reward tasks)ã€æ”¹è¿›ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(GRPO)ä»¥åŠå¼•å…¥å—æ§KLæ­£åˆ™åŒ–(KL regularization)ã€è£å‰ªæ¯”ä¾‹(clipping ratio)å’Œå‘¨æœŸæ€§å‚è€ƒç­–ç•¥é‡ç½®ç­‰æŠ€æœ¯ï¼Œä»¥æ˜¾è‘—å¢å¼ºè®­ç»ƒçš„ç¨³å®šæ€§å’Œæ³›åŒ–è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨æ•°å­¦ã€ç¼–ç¨‹å’Œé€»è¾‘è°œé¢˜ä»»åŠ¡ä¸Šåˆ†åˆ«æ¯”å¼ºåŸºçº¿æ¨¡å‹æå‡äº†14.7%ã€13.9%å’Œ54.8%ï¼Œè¯æ˜äº†å»¶é•¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¯¹äºè§£é”å¤§è¯­è¨€æ¨¡å‹(LLMs)å¤šæ ·åŒ–æ¨ç†æ½œåŠ›çš„é‡è¦æ€§ã€‚æœ€åï¼Œç ”ç©¶å›¢é˜Ÿå…¬å¼€äº†è¯¥æ¨¡å‹ï¼Œä¸ºæœªæ¥åœ¨é“¾å¼æ€ç»´(Chain-of-Thought)æ¨ç†å’Œå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„ç ”ç©¶æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.12507v1",
      "published_date": "2025-07-16 17:59:24 UTC",
      "updated_date": "2025-07-16 17:59:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:02:44.288609+00:00"
    },
    {
      "arxiv_id": "2507.12461v1",
      "title": "Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis",
      "title_zh": "èƒ¸éƒ¨ X çº¿è¯Šæ–­ä¸­åŸºäºçœ¼åŠ¨æ•°æ®çš„æ”¾å°„ç§‘åŒ»å¸ˆæ„å›¾è§£è¯»",
      "authors": [
        "Trong-Thang Pham",
        "Anh Nguyen",
        "Zhigang Deng",
        "Carol C. Wu",
        "Hien Van Nguyen",
        "Ngan Le"
      ],
      "abstract": "Radiologists rely on eye movements to navigate and interpret medical images. A trained radiologist possesses knowledge about the potential diseases that may be present in the images and, when searching, follows a mental checklist to locate them using their gaze. This is a key observation, yet existing models fail to capture the underlying intent behind each fixation. In this paper, we introduce a deep learning-based approach, RadGazeIntent, designed to model this behavior: having an intention to find something and actively searching for it. Our transformer-based architecture processes both the temporal and spatial dimensions of gaze data, transforming fine-grained fixation features into coarse, meaningful representations of diagnostic intent to interpret radiologists' goals. To capture the nuances of radiologists' varied intention-driven behaviors, we process existing medical eye-tracking datasets to create three intention-labeled subsets: RadSeq (Systematic Sequential Search), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid Pattern). Experimental results demonstrate RadGazeIntent's ability to predict which findings radiologists are examining at specific moments, outperforming baseline methods across all intention-labeled datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ”¾å°„ç§‘åŒ»ç”Ÿåœ¨èƒ¸éƒ¨ X-ray è¯Šæ–­ä¸­åŸºäºå¿ƒç†æ¸…å•è¿›è¡Œçœ¼åŠ¨æœç´¢çš„è¡Œä¸ºï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹éš¾ä»¥æ•æ‰çœ¼åŠ¨æ³¨è§†ç‚¹èƒŒåæ½œåœ¨è¯Šæ–­æ„å›¾çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†åŸºäº Transformer æ¶æ„çš„æ·±åº¦å­¦ä¹ æ–¹æ³• RadGazeIntentï¼Œè¯¥æ¨¡å‹é€šè¿‡æ•´åˆçœ¼åŠ¨æ•°æ®çš„æ—¶ç©ºç‰¹å¾ï¼Œå°†ç»†ç²’åº¦çš„æ³¨è§†ä¿¡æ¯è½¬åŒ–ä¸ºèƒ½å¤Ÿè§£è¯»æ”¾å°„ç§‘åŒ»ç”Ÿç›®æ ‡çš„æ„å›¾è¡¨ç¤ºã€‚ç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥å¤„ç†äº†ç°æœ‰çš„åŒ»å­¦çœ¼åŠ¨è¿½è¸ªæ•°æ®é›†ï¼Œæ„å»ºå‡º RadSeq (Systematic Sequential Search)ã€RadExplore (Uncertainty-driven Exploration) å’Œ RadHybrid (Hybrid Pattern) ä¸‰ä¸ªæ„å›¾æ ‡æ³¨å­é›†ä»¥æ¨¡æ‹Ÿä¸åŒçš„æœç´¢è¡Œä¸ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRadGazeIntent åœ¨é¢„æµ‹æ”¾å°„ç§‘åŒ»ç”Ÿå½“å‰å…³æ³¨çš„ç‰¹å®šå‘ç°æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶åœ¨è§£é‡Šä¸“å®¶æ„å›¾æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€æˆæœä¸ºæ·±å…¥ç†è§£åŒ»ç–—å½±åƒè¯Šæ–­è¿‡ç¨‹ä¸­çš„è®¤çŸ¥é€»è¾‘æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ACM MM 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.12461v1",
      "published_date": "2025-07-16 17:58:35 UTC",
      "updated_date": "2025-07-16 17:58:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:02:49.495429+00:00"
    },
    {
      "arxiv_id": "2507.12451v1",
      "title": "S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling",
      "title_zh": "S2WTMï¼šé¢å‘ä¸»é¢˜å»ºæ¨¡çš„çƒé¢åˆ‡ç‰‡ Wasserstein è‡ªåŠ¨ç¼–ç å™¨",
      "authors": [
        "Suman Adhya",
        "Debarshi Kumar Sanyal"
      ],
      "abstract": "Modeling latent representations in a hyperspherical space has proven effective for capturing directional similarities in high-dimensional text data, benefiting topic modeling. Variational autoencoder-based neural topic models (VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical structure. However, VAE-NTMs often suffer from posterior collapse, where the KL divergence term in the objective function highly diminishes, leading to ineffective latent representations. To mitigate this issue while modeling hyperspherical structure in the latent space, we propose the Spherical Sliced Wasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior distribution supported on the unit hypersphere and leverages the Spherical Sliced-Wasserstein distance to align the aggregated posterior distribution with the prior. Experimental results demonstrate that S2WTM outperforms state-of-the-art topic models, generating more coherent and diverse topics while improving performance on downstream tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†S2WTMï¼Œä¸€ç§ç”¨äºä¸»é¢˜æ¨¡å‹(Topic Modeling)çš„è¶…çƒé¢åˆ‡ç‰‡ç“¦ç‘Ÿæ–¯å¦è‡ªç¼–ç å™¨ï¼Œæ—¨åœ¨è§£å†³å˜åˆ†è‡ªç¼–ç å™¨æ¨¡å‹(VAE-NTMs)åœ¨è¶…çƒé¢æ½œåœ¨ç©ºé—´å»ºæ¨¡ä¸­å¸¸é‡åˆ°çš„åéªŒåç¼©(posterior collapse)é—®é¢˜ã€‚S2WTMé‡‡ç”¨æ”¯æŒåœ¨å•ä½è¶…çƒé¢ä¸Šçš„å…ˆéªŒåˆ†å¸ƒ(prior distribution)ï¼Œå¹¶åˆ›æ–°æ€§åœ°åˆ©ç”¨Spherical Sliced-Wasserstein distanceæ¥å¯¹é½èšåˆåéªŒåˆ†å¸ƒ(aggregated posterior distribution)ä¸å…ˆéªŒã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰é«˜ç»´æ–‡æœ¬æ•°æ®çš„æ–¹å‘ç›¸ä¼¼æ€§ï¼Œå…‹æœäº†ä¼ ç»Ÿçš„KLæ•£åº¦é¡¹å¯¼è‡´çš„æ½œåœ¨è¡¨ç¤ºå¤±æ•ˆé—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒS2WTMåœ¨ç”Ÿæˆæ›´å…·è¿è´¯æ€§(coherent)å’Œå¤šæ ·æ€§(diverse)çš„ä¸»é¢˜æ–¹é¢ä¼˜äºç°æœ‰çš„å…ˆè¿›æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å„é¡¹ä¸‹æ¸¸ä»»åŠ¡(downstream tasks)ä¸­ä¹Ÿå±•ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted as a long paper for ACL 2025 main conference",
      "pdf_url": "https://arxiv.org/pdf/2507.12451v1",
      "published_date": "2025-07-16 17:47:45 UTC",
      "updated_date": "2025-07-16 17:47:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:02:50.148991+00:00"
    },
    {
      "arxiv_id": "2507.12443v1",
      "title": "LLM-Based Config Synthesis requires Disambiguation",
      "title_zh": "åŸºäº LLM çš„é…ç½®åˆæˆéœ€è¿›è¡Œæ¶ˆæ­§",
      "authors": [
        "Rajdeep Mondal",
        "Nikolaj Bjorner",
        "Todd Millstein",
        "Alan Tang",
        "George Varghese"
      ],
      "abstract": "Beyond hallucinations, another problem in program synthesis using LLMs is ambiguity in user intent. We illustrate the ambiguity problem in a networking context for LLM-based incremental configuration synthesis of route-maps and ACLs. These structures frequently overlap in header space, making the relative priority of actions impossible for the LLM to infer without user interaction. Measurements in a large cloud identify complex ACLs with 100's of overlaps, showing ambiguity is a real problem. We propose a prototype system, Clarify, which uses an LLM augmented with a new module called a Disambiguator that helps elicit user intent. On a small synthetic workload, Clarify incrementally synthesizes routing policies after disambiguation and then verifies them. Our treatment of ambiguities is useful more generally when the intent of updates can be correctly synthesized by LLMs, but their integration is ambiguous and can lead to different global behaviors.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºåœ¨åŸºäºLLMçš„ç¨‹åºåˆæˆä¸­ï¼Œé™¤äº†å¹»è§‰é—®é¢˜å¤–ï¼Œç”¨æˆ·æ„å›¾çš„æ­§ä¹‰æ€§(ambiguity)ä¹Ÿæ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚åœ¨ç½‘ç»œé…ç½®åœºæ™¯ä¸‹ï¼Œç”±äºroute-mapså’ŒACLsç­‰ç»“æ„åœ¨å¤´éƒ¨ç©ºé—´é¢‘ç¹é‡å ï¼ŒLLMåœ¨ç¼ºä¹äº¤äº’çš„æƒ…å†µä¸‹éš¾ä»¥æ¨æ–­åŠ¨ä½œçš„ç›¸å¯¹ä¼˜å…ˆçº§ï¼Œå¤§è§„æ¨¡äº‘ç¯å¢ƒçš„æµ‹é‡æ•°æ®ä¹Ÿè¯å®äº†è¿™ç§æ­§ä¹‰é—®é¢˜çš„ä¸¥é‡æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†åŸå‹ç³»ç»ŸClarifyï¼Œè¯¥ç³»ç»Ÿé€šè¿‡é›†æˆä¸€ä¸ªåä¸ºDisambiguatorçš„æ–°æ¨¡å—æ¥ä¸»åŠ¨å¼•å¯¼å¹¶æ˜ç¡®ç”¨æˆ·æ„å›¾ã€‚å®éªŒè¡¨æ˜ï¼ŒClarifyèƒ½å¤Ÿåœ¨æ¶ˆé™¤æ­§ä¹‰åå®ç°è·¯ç”±ç­–ç•¥çš„å¢é‡åˆæˆä¸éªŒè¯ã€‚è¿™ç§å¤„ç†æ­§ä¹‰çš„æ–¹æ³•å…·æœ‰æ›´å¹¿æ³›çš„æ„ä¹‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³å½“LLMç”Ÿæˆçš„æ›´æ–°å†…å®¹æ­£ç¡®ä½†é›†æˆé€»è¾‘æ¨¡ç³Šæ—¶å¯èƒ½å¯¼è‡´çš„å…¨å±€è¡Œä¸ºåå·®é—®é¢˜ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.HC",
        "cs.PL"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12443v1",
      "published_date": "2025-07-16 17:29:15 UTC",
      "updated_date": "2025-07-16 17:29:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:02:51.982825+00:00"
    },
    {
      "arxiv_id": "2507.12442v2",
      "title": "Characterizing State Space Model (SSM) and SSM-Transformer Hybrid Language Model Performance with Long Context Length",
      "title_zh": "é•¿ä¸Šä¸‹æ–‡ç¯å¢ƒä¸‹çŠ¶æ€ç©ºé—´æ¨¡å‹ (SSM) ä¸ SSM-Transformer æ··åˆè¯­è¨€æ¨¡å‹çš„æ€§èƒ½è¡¨å¾",
      "authors": [
        "Saptarshi Mitra",
        "Rachid Karami",
        "Haocheng Xu",
        "Sitao Huang",
        "Hyoukjun Kwon"
      ],
      "abstract": "The demand for machine intelligence capable of processing continuous, long-context inputs on local devices is growing rapidly. However, the quadratic complexity and memory requirements of traditional Transformer architectures make them inefficient and often unusable for these tasks. This has spurred a paradigm shift towards new architectures like State Space Models (SSMs) and hybrids, which promise near-linear scaling. While most current research focuses on the accuracy and theoretical throughput of these models, a systematic performance characterization on practical consumer hardware is critically needed to guide system-level optimization and unlock new applications.\n  To address this gap, we present a comprehensive, comparative benchmarking of carefully selected Transformer, SSM, and hybrid models specifically for long-context inference on consumer and embedded GPUs. Our analysis reveals that SSMs are not only viable but superior for this domain, capable of processing sequences up to 220K tokens on a 24GB consumer GPU-approximately 4x longer than comparable Transformers. While Transformers may be up to 1.8x faster at short sequences, SSMs demonstrate a dramatic performance inversion, becoming up to 4x faster at very long contexts (~57K tokens). Our operator-level analysis reveals that custom, hardware-aware SSM kernels dominate the inference runtime, accounting for over 55% of latency on edge platforms, identifying them as a primary target for future hardware acceleration. We also provide detailed, device-specific characterization results to guide system co-design for the edge. To foster further research, we will open-source our characterization framework.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœ¬åœ°è®¾å¤‡å¤„ç†é•¿ä¸Šä¸‹æ–‡è¾“å…¥çš„éœ€æ±‚ï¼Œåˆ†æäº†ä¼ ç»Ÿ Transformer æ¶æ„åœ¨è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜æ¶ˆè€—æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶å¯¹ State Space Models (SSMs) åŠå…¶æ··åˆæ¨¡å‹ï¼ˆhybrid modelsï¼‰åœ¨æ¶ˆè´¹çº§å’ŒåµŒå…¥å¼ GPU ä¸Šçš„é•¿ä¸Šä¸‹æ–‡æ¨ç†æ€§èƒ½è¿›è¡Œäº†ç³»ç»Ÿæ€§åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSSM åœ¨ 24GB æ˜¾å­˜çš„æ¶ˆè´¹çº§ GPU ä¸Šå¯å¤„ç†é«˜è¾¾ 220K tokens çš„åºåˆ—ï¼Œå¤„ç†èƒ½åŠ›çº¦æ˜¯ Transformer çš„ 4 å€ã€‚å°½ç®¡ Transformer åœ¨çŸ­åºåˆ—å¤„ç†ä¸Šå…·æœ‰é€Ÿåº¦ä¼˜åŠ¿ï¼Œä½†åœ¨é•¿ä¸Šä¸‹æ–‡ï¼ˆçº¦ 57K tokensï¼‰ä¸‹ï¼ŒSSM çš„æ¨ç†é€Ÿåº¦å¯è¾¾å…¶ 4 å€ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½åè½¬ã€‚ç®—å­çº§åˆ†ææ­ç¤ºï¼Œç¡¬ä»¶æ„ŸçŸ¥çš„ SSM kernels åœ¨è¾¹ç¼˜å¹³å°ä¸Šå æ®äº†è¶…è¿‡ 55% çš„æ¨ç†å»¶è¿Ÿï¼Œæ˜¯æœªæ¥ç¡¬ä»¶åŠ é€Ÿçš„å…³é”®ä¼˜åŒ–ç›®æ ‡ã€‚è¯¥ç ”ç©¶ä¸ºè¾¹ç¼˜ä¾§ç³»ç»Ÿçš„è½¯ç¡¬ä»¶ååŒè®¾è®¡æä¾›äº†è¯¦ç»†çš„æ€§èƒ½è¡¨å¾ï¼Œå¹¶è®¡åˆ’å¼€æºå…¶åˆ†ææ¡†æ¶ä»¥ä¿ƒè¿›ç›¸å…³é¢†åŸŸçš„åç»­æ¢ç´¢ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.AR",
      "comment": "12 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.12442v2",
      "published_date": "2025-07-16 17:28:40 UTC",
      "updated_date": "2025-07-19 08:24:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:03:10.141629+00:00"
    },
    {
      "arxiv_id": "2507.12440v3",
      "title": "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos",
      "title_zh": "EgoVLAï¼šåŸºäºç¬¬ä¸€äººç§°è§†è§’äººç±»è§†é¢‘çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹å­¦ä¹ ",
      "authors": [
        "Ruihan Yang",
        "Qinxi Yu",
        "Yecheng Wu",
        "Rui Yan",
        "Borui Li",
        "An-Chieh Cheng",
        "Xueyan Zou",
        "Yunhao Fang",
        "Xuxin Cheng",
        "Ri-Zhao Qiu",
        "Hongxu Yin",
        "Sifei Liu",
        "Song Han",
        "Yao Lu",
        "Xiaolong Wang"
      ],
      "abstract": "Real robot data collection for imitation learning has led to significant advancements in robotic manipulation. However, the requirement for robot hardware in the process fundamentally constrains the scale of the data. In this paper, we explore training Vision-Language-Action (VLA) models using egocentric human videos. The benefit of using human videos is not only for their scale but more importantly for the richness of scenes and tasks. With a VLA trained on human video that predicts human wrist and hand actions, we can perform Inverse Kinematics and retargeting to convert the human actions to robot actions. We fine-tune the model using a few robot manipulation demonstrations to obtain the robot policy, namely EgoVLA. We propose a simulation benchmark called Ego Humanoid Manipulation Benchmark, where we design diverse bimanual manipulation tasks with demonstrations. We fine-tune and evaluate EgoVLA with Ego Humanoid Manipulation Benchmark and show significant improvements over baselines and ablate the importance of human data. Videos can be found on our website: https://rchalyang.github.io/EgoVLA",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨ç¬¬ä¸€äººç§°è§†è§’(egocentric)çš„äººç±»è§†é¢‘æ¥è®­ç»ƒè§†è§‰-è¯­è¨€-åŠ¨ä½œ(Vision-Language-Action, VLA)æ¨¡å‹ï¼Œä»¥è§£å†³æœºå™¨äººæ¨¡æ‹Ÿå­¦ä¹ ä¸­å› ç¡¬ä»¶é™åˆ¶å¯¼è‡´çš„è®­ç»ƒæ•°æ®è§„æ¨¡ä¸è¶³é—®é¢˜ã€‚è®ºæ–‡æŒ‡å‡ºäººç±»è§†é¢‘ä¸ä»…è§„æ¨¡åºå¤§ï¼Œä¸”æ¶µç›–äº†æå…¶ä¸°å¯Œçš„åœºæ™¯å’Œä»»åŠ¡ï¼Œå…·æœ‰æé«˜çš„åˆ©ç”¨ä»·å€¼ã€‚ç ”ç©¶è€…é¦–å…ˆè®­ç»ƒäº†ä¸€ä¸ªèƒ½å¤Ÿé¢„æµ‹äººç±»è…•éƒ¨å’Œæ‰‹éƒ¨åŠ¨ä½œçš„VLAæ¨¡å‹ï¼Œéšååˆ©ç”¨é€†è¿åŠ¨å­¦(Inverse Kinematics)å’Œé‡å®šå‘(retargeting)æŠ€æœ¯å°†äººç±»åŠ¨ä½œæ˜ å°„ä¸ºæœºå™¨äººåŠ¨ä½œã€‚é€šè¿‡åœ¨å°‘é‡æœºå™¨äººæ“ä½œæ¼”ç¤ºä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¯¥ç ”ç©¶æˆåŠŸæ„å»ºäº†æœºå™¨äººç­–ç•¥EgoVLAã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æå‡ºäº†åŒ…å«å¤šæ ·åŒ–åŒè‡‚æ“ä½œä»»åŠ¡çš„æ¨¡æ‹ŸåŸºå‡†æµ‹è¯•Ego Humanoid Manipulation Benchmarkã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEgoVLAåœ¨å„é¡¹ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼ŒéªŒè¯äº†å¼•å…¥äººç±»è§†é¢‘æ•°æ®å¯¹äºå¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›å’Œæ“ä½œæ€§èƒ½çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "More videos can be found on our website: https://rchalyang.github.io/EgoVLA",
      "pdf_url": "https://arxiv.org/pdf/2507.12440v3",
      "published_date": "2025-07-16 17:27:44 UTC",
      "updated_date": "2025-07-18 07:18:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:03:03.301739+00:00"
    },
    {
      "arxiv_id": "2507.12428v2",
      "title": "Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models",
      "title_zh": "æˆ‘ä»¬èƒ½å¦åœ¨æ¨¡å‹å®Œæˆæ€è€ƒå‰é¢„æµ‹å…¶å¯¹é½æ€§ï¼Ÿè¿ˆå‘å¯¹æœªå¯¹é½æ¨ç†æ¨¡å‹çš„ç›‘æµ‹",
      "authors": [
        "Yik Siu Chan",
        "Zheng-Xin Yong",
        "Stephen H. Bach"
      ],
      "abstract": "Reasoning language models improve performance on complex tasks by generating long chains of thought (CoTs), but this process can also increase harmful outputs in adversarial settings. In this work, we ask whether the long CoTs can be leveraged for predictive safety monitoring: do the reasoning traces provide early signals of final response alignment that could enable timely intervention? We evaluate a range of monitoring methods using either CoT text or activations, including highly capable large language models, fine-tuned classifiers, and humans. First, we find that a simple linear probe trained on CoT activations significantly outperforms all text-based baselines in predicting whether a final response is safe or unsafe, with an average absolute increase of 13 in F1 scores over the best-performing alternatives. CoT texts are often unfaithful and misleading, while model latents provide a more reliable predictive signal. Second, the probe can be applied to early CoT segments before the response is generated, showing that alignment signals appear before reasoning completes. Error analysis reveals that the performance gap between text classifiers and the linear probe largely stems from a subset of responses we call performative CoTs, where the reasoning consistently contradicts the final response as the CoT progresses. Our findings generalize across model sizes, families, and safety benchmarks, suggesting that lightweight probes could enable real-time safety monitoring and early intervention during generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ˜¯å¦å¯ä»¥åœ¨æ¨ç†è¯­è¨€æ¨¡å‹å®Œæˆæ€è€ƒä¹‹å‰ï¼Œé€šè¿‡å…¶ç”Ÿæˆçš„é•¿é“¾å¼æ€ç»´(Chain-of-Thought, CoT)æ¥é¢„æµ‹æ¨¡å‹æœ€ç»ˆå›ç­”çš„å¯¹é½(Alignment)å®‰å…¨æ€§ã€‚ç ”ç©¶å›¢é˜Ÿè¯„ä¼°äº†åŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ã€å¾®è°ƒåˆ†ç±»å™¨ä»¥åŠåŸºäºæ¨¡å‹å†…éƒ¨æ¿€æ´»(Activations)çš„çº¿æ€§æ¢æµ‹(Linear Probe)åœ¨å†…çš„å¤šç§ç›‘æ§æ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨CoTæ¿€æ´»å±‚è®­ç»ƒçš„ç®€å•çº¿æ€§æ¢æµ‹åœ¨é¢„æµ‹æœ€ç»ˆå›ç­”æ˜¯å¦å®‰å…¨æ–¹é¢è¡¨ç°æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºäºæ–‡æœ¬çš„åŸºå‡†ï¼Œå…¶F1åˆ†æ•°æ¯”æœ€ä½³æ›¿ä»£æ–¹æ¡ˆå¹³å‡æé«˜äº†13%ã€‚ç ”ç©¶å‘ç°CoTæ–‡æœ¬å¾€å¾€å…·æœ‰è¯¯å¯¼æ€§ä¸”ä¸å¿ å®(Unfaithful)ï¼Œè€Œæ¨¡å‹çš„æ½œå±‚çŠ¶æ€(Model Latents)èƒ½æä¾›æ›´å¯é ä¸”æ›´æ—©å‡ºç°çš„é¢„æµ‹ä¿¡å·ã€‚è¯¯å·®åˆ†æè¡¨æ˜ï¼Œæ€§èƒ½å·®è·ä¸»è¦æºäºæ‰€è°“çš„â€œè¡¨æ¼”æ€§CoTsâ€(Performative CoTs)ï¼Œå³æ¨ç†é€»è¾‘ä¸æœ€ç»ˆå›ç­”åœ¨å‘å±•è¿‡ç¨‹ä¸­å­˜åœ¨æŒç»­çŸ›ç›¾ã€‚è¯¥ç ”ç©¶ç»“è®ºè¯æ˜äº†å¯¹é½ä¿¡å·åœ¨æ¨ç†å®Œæˆå‰å³å¯æ•æ‰ï¼Œä¸ºåˆ©ç”¨è½»é‡çº§æ¢æµ‹å™¨å®ç°å®æ—¶å®‰å…¨ç›‘æ§å’Œç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ—©æœŸå¹²é¢„å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12428v2",
      "published_date": "2025-07-16 17:16:03 UTC",
      "updated_date": "2025-10-07 16:30:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:03:05.783101+00:00"
    },
    {
      "arxiv_id": "2507.12427v1",
      "title": "Unit-Based Histopathology Tissue Segmentation via Multi-Level Feature Representation",
      "title_zh": "åŸºäºå¤šçº§ç‰¹å¾è¡¨ç¤ºçš„å•å…ƒçº§ç»„ç»‡ç—…ç†å­¦ç»„ç»‡åˆ†å‰²",
      "authors": [
        "Ashkan Shakarami",
        "Azade Farshad",
        "Yousef Yeganeh",
        "Lorenzo Nicole",
        "Peter SchÃ¼ffler",
        "Stefano Ghidoni",
        "Nassir Navab"
      ],
      "abstract": "We propose UTS, a unit-based tissue segmentation framework for histopathology that classifies each fixed-size 32 * 32 tile, rather than each pixel, as the segmentation unit. This approach reduces annotation effort and improves computational efficiency without compromising accuracy. To implement this approach, we introduce a Multi-Level Vision Transformer (L-ViT), which benefits the multi-level feature representation to capture both fine-grained morphology and global tissue context. Trained to segment breast tissue into three categories (infiltrating tumor, non-neoplastic stroma, and fat), UTS supports clinically relevant tasks such as tumor-stroma quantification and surgical margin assessment. Evaluated on 386,371 tiles from 459 H&E-stained regions, it outperforms U-Net variants and transformer-based baselines. Code and Dataset will be available at GitHub.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†UTSï¼Œä¸€ç§ç”¨äºç»„ç»‡ç—…ç†å­¦å›¾åƒçš„å•å…ƒåŒ–ç»„ç»‡åˆ†å‰²(unit-based tissue segmentation)æ¡†æ¶ï¼Œé€šè¿‡å°†32x32çš„å›ºå®šå°ºå¯¸ç“¦ç‰‡(tile)è€Œéåƒç´ ä½œä¸ºåŸºæœ¬åˆ†å‰²å•å…ƒï¼Œæ—¨åœ¨é™ä½æ ‡æ³¨å·¥ä½œé‡å¹¶æé«˜è®¡ç®—æ•ˆç‡ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒå¼•å…¥äº†å¤šçº§è§†è§‰Transformer (Multi-Level Vision Transformer, L-ViT)ï¼Œåˆ©ç”¨å¤šçº§ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›æ¥åŒæ—¶æ•è·ç»†ç²’åº¦çš„å½¢æ€ç‰¹å¾å’Œå…¨å±€ç»„ç»‡ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚UTSè¢«è®­ç»ƒç”¨äºå°†ä¹³è…ºç»„ç»‡åˆ’åˆ†ä¸ºæµ¸æ¶¦æ€§è‚¿ç˜¤ã€éè‚¿ç˜¤æ€§é—´è´¨å’Œè„‚è‚ªä¸‰ä¸ªç±»åˆ«ï¼Œä»è€Œæ”¯æŒè‚¿ç˜¤é—´è´¨å®šé‡å’Œæ‰‹æœ¯è¾¹ç¼˜è¯„ä¼°ç­‰ä¸´åºŠç›¸å…³ä»»åŠ¡ã€‚åœ¨æ¶µç›–459ä¸ªH&EæŸ“è‰²åŒºåŸŸçš„386,371ä¸ªç“¦ç‰‡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„æ€§èƒ½ä¼˜äºU-Netå˜ä½“å’Œå¤šç§åŸºäºTransformerçš„åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "12 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.12427v1",
      "published_date": "2025-07-16 17:15:18 UTC",
      "updated_date": "2025-07-16 17:15:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:03:11.243899+00:00"
    },
    {
      "arxiv_id": "2507.12425v1",
      "title": "Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data",
      "title_zh": "é¢å‘ç»“æ„åŒ–ä¼ä¸šä¸å†…éƒ¨æ•°æ®çš„æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯è¿›é˜¶",
      "authors": [
        "Chandana Cheerla"
      ],
      "abstract": "Organizations increasingly rely on proprietary enterprise data, including HR records, structured reports, and tabular documents, for critical decision-making. While Large Language Models (LLMs) have strong generative capabilities, they are limited by static pretraining, short context windows, and challenges in processing heterogeneous data formats. Conventional Retrieval-Augmented Generation (RAG) frameworks address some of these gaps but often struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval strategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by metadata-aware filtering with SpaCy NER and cross-encoder reranking. The framework applies semantic chunking to maintain textual coherence and retains tabular data structures to preserve row-column integrity. Quantized indexing optimizes retrieval efficiency, while human-in-the-loop feedback and conversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5 increased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74), and Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative evaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness (4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale. These results demonstrate the framework's effectiveness in delivering accurate, comprehensive, and contextually relevant responses for enterprise tasks. Future work includes extending to multimodal data and integrating agent-based retrieval. The source code will be released at https://github.com/CheerlaChandana/Enterprise-Chatbot",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹ä¼ä¸šå†…éƒ¨ç»“æ„åŒ–å’ŒåŠç»“æ„åŒ–æ•°æ®å¤„ç†çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†åŸºäºå¯†é›†åµŒå…¥(all-mpnet-base-v2)å’ŒBM25çš„æ··åˆæ£€ç´¢ç­–ç•¥ï¼Œå¹¶é€šè¿‡SpaCy NERå…ƒæ•°æ®è¿‡æ»¤å’Œcross-encoderé‡æ’åºè¿›ä¸€æ­¥å¢å¼ºæ£€ç´¢æ•ˆæœã€‚ä¸ºäº†ç¡®ä¿æ–‡æœ¬è¿è´¯æ€§ï¼Œç³»ç»Ÿé‡‡ç”¨äº†è¯­ä¹‰åˆ†å—(semantic chunking)æŠ€æœ¯ï¼Œå¹¶ä¸“é—¨ä¿ç•™äº†è¡¨æ ¼æ•°æ®ç»“æ„ä»¥ç»´æŠ¤è¡Œåˆ—å®Œæ•´æ€§ã€‚æ­¤å¤–ï¼Œæ¡†æ¶åˆ©ç”¨é‡åŒ–ç´¢å¼•(quantized indexing)ä¼˜åŒ–æ£€ç´¢æ•ˆç‡ï¼Œå¹¶å¼•å…¥äººæœºåä½œåé¦ˆä¸å¯¹è¯è®°å¿†æœºåˆ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨Precision@5å’ŒRecall@5ä¸Šåˆ†åˆ«æå‡äº†15%å’Œ13%ï¼Œä¸”åœ¨å¿ å®åº¦(Faithfulness)å’Œå®Œæ•´æ€§(Completeness)ç­‰å®šæ€§æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ¨¡å‹ã€‚ç ”ç©¶è¯æ˜äº†è¯¥æ–¹æ¡ˆèƒ½ä¸ºå¤æ‚ä¼ä¸šä»»åŠ¡æä¾›å‡†ç¡®ä¸”é«˜åº¦ç›¸å…³çš„ä¸Šä¸‹æ–‡å“åº”ï¼Œå±•ç°äº†å…¶åœ¨å¤„ç†å¼‚æ„æ•°æ®æ–¹é¢çš„å“è¶Šæ•ˆèƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12425v1",
      "published_date": "2025-07-16 17:13:06 UTC",
      "updated_date": "2025-07-16 17:13:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:03:24.936892+00:00"
    },
    {
      "arxiv_id": "2507.12419v1",
      "title": "Mixture of Raytraced Experts",
      "title_zh": "å…‰çº¿è¿½è¸ªæ··åˆä¸“å®¶æ¨¡å‹",
      "authors": [
        "Andrea Perin",
        "Giacomo Lagomarsini",
        "Claudio Gallicchio",
        "Giuseppe Nuti"
      ],
      "abstract": "We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts (MoE) architecture which can dynamically select sequences of experts, producing computational graphs of variable width and depth. Existing MoE architectures generally require a fixed amount of computation for a given sample. Our approach, in contrast, yields predictions with increasing accuracy as the computation cycles through the experts' sequence. We train our model by iteratively sampling from a set of candidate experts, unfolding the sequence akin to how Recurrent Neural Networks are trained. Our method does not require load-balancing mechanisms, and preliminary experiments show a reduction in training epochs of 10\\% to 40\\% with a comparable/higher accuracy. These results point to new research directions in the field of MoEs, allowing the design of potentially faster and more expressive models. The code is available at https://github.com/nutig/RayTracing",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† Mixture of Raytraced Expertsï¼Œè¿™æ˜¯ä¸€ç§å †å å¼çš„æ··åˆä¸“å®¶(Mixture of Experts, MoE)æ¶æ„ï¼Œèƒ½å¤ŸåŠ¨æ€é€‰æ‹©ä¸“å®¶åºåˆ—å¹¶ç”Ÿæˆå…·æœ‰å¯å˜å®½åº¦å’Œæ·±åº¦çš„è®¡ç®—å›¾ã€‚ä¸ç°æœ‰éœ€è¦å›ºå®šè®¡ç®—é‡çš„ MoE æ¶æ„ä¸åŒï¼Œè¯¥æ–¹æ³•å…è®¸é¢„æµ‹å‡†ç¡®åº¦éšç€è®¡ç®—åœ¨ä¸“å®¶åºåˆ—ä¸­çš„å¾ªç¯è€Œé€æ­¥æé«˜ã€‚æ¨¡å‹é€šè¿‡ä»å€™é€‰ä¸“å®¶é›†ä¸­è¿­ä»£é‡‡æ ·è¿›è¡Œè®­ç»ƒï¼Œå…¶åºåˆ—å±•å¼€è¿‡ç¨‹ç±»ä¼¼äºå¾ªç¯ç¥ç»ç½‘ç»œ(Recurrent Neural Networks, RNN)çš„è®­ç»ƒæ¨¡å¼ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦å¤æ‚çš„è´Ÿè½½å‡è¡¡(load-balancing)æœºåˆ¶ï¼Œåˆæ­¥å®éªŒç»“æœæ˜¾ç¤ºï¼Œå®ƒåœ¨ä¿æŒç›¸åŒæˆ–æ›´é«˜å‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œèƒ½å¤Ÿå°†è®­ç»ƒå‘¨æœŸ(training epochs)å‡å°‘ 10% è‡³ 40%ã€‚è¿™ä¸€æˆæœä¸º MoE çš„æœªæ¥å‘å±•æä¾›äº†æ–°æ–¹å‘ï¼Œæ—¨åœ¨æ„å»ºæ›´é«˜æ•ˆã€è¡¨è¾¾èƒ½åŠ›(expressive)æ›´å¼ºçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preliminary version (pre-submission)",
      "pdf_url": "https://arxiv.org/pdf/2507.12419v1",
      "published_date": "2025-07-16 17:08:46 UTC",
      "updated_date": "2025-07-16 17:08:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:03:42.000024+00:00"
    },
    {
      "arxiv_id": "2507.12416v1",
      "title": "QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval",
      "title_zh": "QuReï¼šç»„åˆå›¾åƒæ£€ç´¢ä¸­åŸºäºéš¾è´Ÿé‡‡æ ·çš„æŸ¥è¯¢ç›¸å…³æ£€ç´¢",
      "authors": [
        "Jaehyun Kwak",
        "Ramahdani Muhammad Izaaz Inhar",
        "Se-Young Yun",
        "Sung-Ju Lee"
      ],
      "abstract": "Composed Image Retrieval (CIR) retrieves relevant images based on a reference image and accompanying text describing desired modifications. However, existing CIR methods only focus on retrieving the target image and disregard the relevance of other images. This limitation arises because most methods employing contrastive learning-which treats the target image as positive and all other images in the batch as negatives-can inadvertently include false negatives. This may result in retrieving irrelevant images, reducing user satisfaction even when the target image is retrieved. To address this issue, we propose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which optimizes a reward model objective to reduce false negatives. Additionally, we introduce a hard negative sampling strategy that selects images positioned between two steep drops in relevance scores following the target image, to effectively filter false negatives. In order to evaluate CIR models on their alignment with human satisfaction, we create Human-Preference FashionIQ (HP-FashionIQ), a new dataset that explicitly captures user preferences beyond target retrieval. Extensive experiments demonstrate that QuRe achieves state-of-the-art performance on FashionIQ and CIRR datasets while exhibiting the strongest alignment with human preferences on the HP-FashionIQ dataset. The source code is available at https://github.com/jackwaky/QuRe.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç»„åˆå›¾åƒæ£€ç´¢(Composed Image Retrieval, CIR)é¢†åŸŸä¸­ç°æœ‰æ–¹æ³•ä»…å…³æ³¨ç›®æ ‡å›¾åƒè€Œå¿½ç•¥å…¶ä»–å›¾åƒç›¸å…³æ€§ï¼Œä»¥åŠå¯¹æ¯”å­¦ä¹ ä¸­å®¹æ˜“å°†ç›¸å…³å›¾åƒè¯¯æ ‡ä¸ºè´Ÿæ ·æœ¬(false negatives)çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºQuReçš„æ–°å‹æ¡†æ¶ã€‚QuReé€šè¿‡ä¼˜åŒ–å¥–åŠ±æ¨¡å‹(reward model)ç›®æ ‡æ¥å‡å°‘è¯¯æ ‡è´Ÿæ ·æœ¬çš„å½±å“ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§ç¡¬è´Ÿæ ·æœ¬é‡‡æ ·(hard negative sampling)ç­–ç•¥ï¼Œé€šè¿‡è¯†åˆ«ç›¸å…³æ€§å¾—åˆ†æ˜¾è‘—ä¸‹é™ç‚¹æ¥æœ‰æ•ˆè¿‡æ»¤å‡è´Ÿæ ·æœ¬ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜æ¨å‡ºäº†Human-Preference FashionIQ (HP-FashionIQ)æ•°æ®é›†ï¼Œæ—¨åœ¨æ•æ‰è¶…è¶Šå•ä¸€ç›®æ ‡æ£€ç´¢çš„ç”¨æˆ·åå¥½ï¼Œä»è€Œæ›´å…¨é¢åœ°è¡¡é‡æ¨¡å‹ä¸äººç±»æ»¡æ„åº¦çš„å¯¹é½ç¨‹åº¦ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒQuReåœ¨FashionIQå’ŒCIRRæ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†é¢†åŸŸé¡¶å°–(state-of-the-art)çš„æ€§èƒ½ï¼Œå¹¶åœ¨HP-FashionIQæ•°æ®é›†ä¸Šå±•ç°å‡ºæå¼ºçš„äººç±»åå¥½ä¸€è‡´æ€§ï¼Œæœ‰æ•ˆè§£å†³äº†CIRç³»ç»Ÿæ£€ç´¢ç»“æœä¸ç›¸å…³çš„é—®é¢˜ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.12416v1",
      "published_date": "2025-07-16 17:06:33 UTC",
      "updated_date": "2025-07-16 17:06:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:03:31.119982+00:00"
    },
    {
      "arxiv_id": "2507.12414v1",
      "title": "AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models",
      "title_zh": "AutoVDCï¼šåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ–è§†è§‰æ•°æ®æ¸…æ´—",
      "authors": [
        "Santosh Vasa",
        "Aditi Ramadwar",
        "Jnana Rama Krishna Darabattula",
        "Md Zafar Anwar",
        "Stanislaw Antol",
        "Andrei Vatavu",
        "Thomas Monninger",
        "Sihao Ding"
      ],
      "abstract": "Training of autonomous driving systems requires extensive datasets with precise annotations to attain robust performance. Human annotations suffer from imperfections, and multiple iterations are often needed to produce high-quality datasets. However, manually reviewing large datasets is laborious and expensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning) framework and investigate the utilization of Vision-Language Models (VLMs) to automatically identify erroneous annotations in vision datasets, thereby enabling users to eliminate these errors and enhance data quality. We validate our approach using the KITTI and nuImages datasets, which contain object detection benchmarks for autonomous driving. To test the effectiveness of AutoVDC, we create dataset variants with intentionally injected erroneous annotations and observe the error detection rate of our approach. Additionally, we compare the detection rates using different VLMs and explore the impact of VLM fine-tuning on our pipeline. The results demonstrate our method's high performance in error detection and data cleaning experiments, indicating its potential to significantly improve the reliability and accuracy of large-scale production datasets in autonomous driving.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AutoVDC (Automated Vision Data Cleaning) æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨Vision-Language Models (VLMs) è‡ªåŠ¨è¯†åˆ«è§†è§‰æ•°æ®é›†ä¸­çš„é”™è¯¯æ ‡æ³¨ï¼Œä»¥è§£å†³è‡ªåŠ¨é©¾é©¶è®­ç»ƒä¸­äººå·¥æ ‡æ³¨æˆæœ¬é«˜ä¸”æ˜“å‡ºé”™çš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡åœ¨KITTIå’ŒnuImagesè¿™ä¸¤ä¸ªè‡ªåŠ¨é©¾é©¶ç›®æ ‡æ£€æµ‹åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ï¼Œå¹¶æ„å»ºäº†åŒ…å«æ³¨å…¥é”™è¯¯æ ‡æ³¨çš„å˜ä½“æ•°æ®é›†æ¥è¯„ä¼°é”™è¯¯æ£€æµ‹ç‡ã€‚æ–‡ä¸­ä¸ä»…å¯¹æ¯”äº†ä¸åŒVLMsçš„æ€§èƒ½å·®å¼‚ï¼Œè¿˜æ¢è®¨äº†æ¨¡å‹å¾®è°ƒ(fine-tuning) å¯¹æ•´ä½“æµæ°´çº¿çš„å½±å“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAutoVDCåœ¨é”™è¯¯æ£€æµ‹å’Œæ•°æ®æ¸…æ´—ä»»åŠ¡ä¸­è¡¨ç°å‡ºæé«˜æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨æå‡å¤§è§„æ¨¡è‡ªåŠ¨é©¾é©¶ç”Ÿäº§æ•°æ®é›†å¯é æ€§æ–¹é¢çš„æ½œåŠ›ã€‚è¯¥æ¡†æ¶ä¸ºæé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿè®­ç»ƒæ•°æ®çš„ç²¾ç¡®åº¦æä¾›äº†ä¸€ç§é«˜æ•ˆçš„è‡ªåŠ¨åŒ–æ‰‹æ®µã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12414v1",
      "published_date": "2025-07-16 17:04:49 UTC",
      "updated_date": "2025-07-16 17:04:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:03:35.305299+00:00"
    },
    {
      "arxiv_id": "2507.12412v1",
      "title": "NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal Data",
      "title_zh": "NOCTAï¼šé¢å‘çºµå‘æ•°æ®çš„éè´ªå©ªç›®æ ‡æˆæœ¬æƒè¡¡è·å–æ–¹æ³•",
      "authors": [
        "Dzung Dinh",
        "Boqi Chen",
        "Marc Niethammer",
        "Junier Oliva"
      ],
      "abstract": "In many critical applications, resource constraints limit the amount of information that can be gathered to make predictions. For example, in healthcare, patient data often spans diverse features ranging from lab tests to imaging studies. Each feature may carry different information and must be acquired at a respective cost of time, money, or risk to the patient. Moreover, temporal prediction tasks, where both instance features and labels evolve over time, introduce additional complexity in deciding when or what information is important. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff Acquisition method that sequentially acquires the most informative features at inference time while accounting for both temporal dynamics and acquisition cost. We first introduce a cohesive estimation target for our NOCTA setting, and then develop two complementary estimators: 1) a non-parametric method based on nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric method that directly predicts the utility of potential acquisitions (NOCTA-P). Experiments on synthetic and real-world medical datasets demonstrate that both NOCTA variants outperform existing baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—ç­‰å…³é”®é¢†åŸŸä¸­èµ„æºå—é™å¯¼è‡´çš„é¢„æµ‹ä¿¡æ¯é‡‡é›†éš¾é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å¾å’Œæ ‡ç­¾éšæ—¶é—´å˜åŒ–çš„çºµå‘æ•°æ®(Longitudinal Data)åœºæ™¯ä¸‹ï¼Œæ¢è®¨äº†å¦‚ä½•æœ‰æ•ˆæƒè¡¡ä¿¡æ¯ä»·å€¼ä¸é‡‡é›†æˆæœ¬ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†NOCTAï¼Œä¸€ç§éè´ªå©ªç›®æ ‡æˆæœ¬æƒè¡¡é‡‡é›†(Non-Greedy Objective Cost-Tradeoff Acquisition)æ–¹æ³•ï¼Œæ—¨åœ¨æ¨ç†é˜¶æ®µç»“åˆæ—¶é—´åŠ¨æ€å’Œé‡‡é›†æˆæœ¬ï¼Œåºåˆ—åŒ–åœ°è·å–æœ€å…·ä¿¡æ¯é‡çš„ç‰¹å¾ã€‚ç ”ç©¶é¦–å…ˆä¸ºè¯¥è®¾å®šå¼•å…¥äº†ä¸€ä¸ªå†…èšçš„ä¼°è®¡ç›®æ ‡ï¼Œå¹¶å¼€å‘äº†ä¸¤ç§äº’è¡¥çš„è¯„ä¼°å™¨ï¼šä¸€ç§æ˜¯åŸºäºæœ€è¿‘é‚»å¼•å¯¼é‡‡é›†çš„éå‚æ•°åŒ–æ–¹æ³•(NOCTA-NP)ï¼Œå¦ä¸€ç§æ˜¯ç›´æ¥é¢„æµ‹æ½œåœ¨é‡‡é›†æ•ˆç”¨çš„å‚æ•°åŒ–æ–¹æ³•(NOCTA-P)ã€‚åœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®åŒ»ç–—æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒNOCTAçš„ä¸¤ç§å˜ä½“åœ¨æ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰çš„åŸºçº¿æ¨¡å‹ã€‚è¯¥æ¡†æ¶é€šè¿‡æœ‰æ•ˆæ•´åˆæ—¶é—´åŠ¨æ€åˆ†æï¼Œä¸ºå—é™èµ„æºä¸‹çš„åºè´¯ç‰¹å¾é‡‡é›†æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12412v1",
      "published_date": "2025-07-16 17:00:41 UTC",
      "updated_date": "2025-07-16 17:00:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:03:35.833332+00:00"
    },
    {
      "arxiv_id": "2507.12379v1",
      "title": "Probing for Arithmetic Errors in Language Models",
      "title_zh": "æ¢æµ‹è¯­è¨€æ¨¡å‹ä¸­çš„ç®—æœ¯é”™è¯¯",
      "authors": [
        "Yucheng Sun",
        "Alessandro Stolfo",
        "Mrinmaya Sachan"
      ],
      "abstract": "We investigate whether internal activations in language models can be used to detect arithmetic errors. Starting with a controlled setting of 3-digit addition, we show that simple probes can accurately decode both the model's predicted output and the correct answer from hidden states, regardless of whether the model's output is correct. Building on this, we train lightweight error detectors that predict model correctness with over 90% accuracy. We then extend our analysis to structured chain-of-thought traces on addition-only GSM8K problems and find that probes trained on simple arithmetic generalize well to this more complex setting, revealing consistent internal representations. Finally, we demonstrate that these probes can guide selective re-prompting of erroneous reasoning steps, improving task accuracy with minimal disruption to correct outputs. Our findings suggest that arithmetic errors can be anticipated from internal activations alone, and that simple probes offer a viable path toward lightweight model self-correction.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ˜¯å¦å¯ä»¥é€šè¿‡è¯­è¨€æ¨¡å‹çš„å†…éƒ¨æ¿€æ´»(internal activations)æ¥æ£€æµ‹ç®—æœ¯é”™è¯¯ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºè½»é‡çº§æ¢æµ‹å™¨(probes)çš„æ£€æµ‹æ–¹æ³•ã€‚åœ¨3ä½æ•°åŠ æ³•çš„å—æ§å®éªŒä¸­ï¼Œç ”ç©¶è¯æ˜æ¢æµ‹å™¨èƒ½ä»éšè—çŠ¶æ€(hidden states)ä¸­å‡†ç¡®è§£ç æ¨¡å‹çš„é¢„æµ‹è¾“å‡ºå’Œæ­£ç¡®ç­”æ¡ˆï¼Œå…¶é”™è¯¯æ£€æµ‹å‡†ç¡®ç‡è¶…è¿‡90%ã€‚è¯¥åˆ†æè¿›ä¸€æ­¥æ‰©å±•è‡³GSM8Ké—®é¢˜çš„é“¾å¼æ€ç»´(chain-of-thought)æ¨ç†è½¨è¿¹ï¼Œå‘ç°é’ˆå¯¹ç®€å•ç®—æœ¯è®­ç»ƒçš„æ¢æµ‹å™¨åœ¨æ­¤ç±»å¤æ‚åœºæ™¯ä¸‹ä»è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–æ€§ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨æ¢æµ‹å™¨æŒ‡å¯¼å¯¹é”™è¯¯æ¨ç†æ­¥éª¤çš„é€‰æ‹©æ€§é‡æ–°æç¤º(selective re-prompting)ï¼Œåœ¨ä¸å¹²æ‰°æ­£ç¡®è¾“å‡ºçš„å‰æä¸‹æ˜¾è‘—æå‡äº†ä»»åŠ¡å‡†ç¡®æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹å†…éƒ¨æ¿€æ´»è¶³ä»¥é¢„åˆ¤ç®—æœ¯é”™è¯¯ï¼Œç®€å•æ¢æµ‹å™¨çš„åº”ç”¨ä¸ºå®ç°è½»é‡åŒ–æ¨¡å‹è‡ªæˆ‘ä¿®æ­£æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12379v1",
      "published_date": "2025-07-16 16:27:50 UTC",
      "updated_date": "2025-07-16 16:27:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:03:38.008881+00:00"
    },
    {
      "arxiv_id": "2507.12367v2",
      "title": "GitChameleon 2.0: Evaluating AI Code Generation Against Python Library Version Incompatibilities",
      "title_zh": "GitChameleon 2.0ï¼šé’ˆå¯¹ Python åº“ç‰ˆæœ¬ä¸å…¼å®¹æ€§çš„ AI ä»£ç ç”Ÿæˆè¯„ä¼°",
      "authors": [
        "Diganta Misra",
        "Nizar Islah",
        "Victor May",
        "Brice Rauby",
        "Zihan Wang",
        "Justine Gehring",
        "Antonio Orvieto",
        "Muawiz Chaudhary",
        "Eilif B. Muller",
        "Irina Rish",
        "Samira Ebrahimi Kahou",
        "Massimo Caccia"
      ],
      "abstract": "The rapid evolution of software libraries poses a considerable hurdle for code generation, necessitating continuous adaptation to frequent version updates while preserving backward compatibility. While existing code evolution benchmarks provide valuable insights, they typically lack execution-based evaluation for generating code compliant with specific library versions. To address this, we introduce GitChameleon 2.0, a novel, meticulously curated dataset comprising 328 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. GitChameleon 2.0 rigorously evaluates the capacity of contemporary large language models (LLMs), LLM-powered agents, code assistants, and RAG systems to perform version-conditioned code generation that demonstrates functional accuracy through execution. Our extensive evaluations indicate that state-of-the-art systems encounter significant challenges with this task; enterprise models achieving baseline success rates in the 48-51% range, underscoring the intricacy of the problem. By offering an execution-based benchmark emphasizing the dynamic nature of code libraries, GitChameleon 2.0 enables a clearer understanding of this challenge and helps guide the development of more adaptable and dependable AI code generation methods. We make the dataset and evaluation code publicly available at https://github.com/mrcabbage972/GitChameleonBenchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† GitChameleon 2.0ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼° AI ä»£ç ç”Ÿæˆåœ¨é¢å¯¹ Python åº“ç‰ˆæœ¬ä¸å…¼å®¹é—®é¢˜æ—¶è¡¨ç°çš„æ–°å‹æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å« 328 ä¸ª Python ä»£ç è¡¥å…¨é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½ä»¥ç‰¹å®šçš„åº“ç‰ˆæœ¬ä¸ºæ¡ä»¶ï¼Œå¹¶é…å¤‡äº†å¯æ‰§è¡Œçš„å•å…ƒæµ‹è¯• (Unit Tests) ä»¥éªŒè¯å…¶åŠŸèƒ½çš„å‡†ç¡®æ€§ã€‚GitChameleon 2.0 æ—¨åœ¨ä¸¥æ ¼è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs)ã€æ™ºèƒ½ä½“ (Agents)ã€ä»£ç åŠ©æ‰‹å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ç³»ç»Ÿåœ¨ç‰¹å®šç‰ˆæœ¬çº¦æŸä¸‹çš„ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä¾¿æ˜¯ç›®å‰æœ€å…ˆè¿›çš„ç³»ç»Ÿä¹Ÿé¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œé¢†å…ˆçš„ä¼ä¸šçº§æ¨¡å‹æˆåŠŸç‡ä»…åœ¨ 48-51% ä¹‹é—´ï¼Œå‡¸æ˜¾äº†è¯¥é—®é¢˜çš„å¤æ‚æ€§ã€‚é€šè¿‡æä¾›å¼ºè°ƒä»£ç åº“åŠ¨æ€ç‰¹æ€§çš„åŸºäºæ‰§è¡Œçš„åŸºå‡†æµ‹è¯•ï¼Œè¯¥ç ”ç©¶ä¸ä»…æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨å¤„ç†ç‰ˆæœ¬æ¼”åŒ–æ–¹é¢çš„ä¸è¶³ï¼Œä¹Ÿä¸ºå¼€å‘æ›´å…·é€‚åº”æ€§å’Œå¯é æ€§çš„ AI ä»£ç ç”Ÿæˆæ–¹æ³•æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "Version 2 of the dataset from: arXiv:2411.05830",
      "pdf_url": "https://arxiv.org/pdf/2507.12367v2",
      "published_date": "2025-07-16 16:10:42 UTC",
      "updated_date": "2025-07-21 21:56:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:03:48.285646+00:00"
    },
    {
      "arxiv_id": "2507.12366v1",
      "title": "FactorHD: A Hyperdimensional Computing Model for Multi-Object Multi-Class Representation and Factorization",
      "title_zh": "FactorHDï¼šç”¨äºå¤šå¯¹è±¡å¤šç±»åˆ«è¡¨ç¤ºä¸åˆ†è§£çš„è¶…ç»´è®¡ç®—æ¨¡å‹",
      "authors": [
        "Yifei Zhou",
        "Xuchu Huang",
        "Chenyu Ni",
        "Min Zhou",
        "Zheyu Yan",
        "Xunzhao Yin",
        "Cheng Zhuo"
      ],
      "abstract": "Neuro-symbolic artificial intelligence (neuro-symbolic AI) excels in logical analysis and reasoning. Hyperdimensional Computing (HDC), a promising brain-inspired computational model, is integral to neuro-symbolic AI. Various HDC models have been proposed to represent class-instance and class-class relations, but when representing the more complex class-subclass relation, where multiple objects associate different levels of classes and subclasses, they face challenges for factorization, a crucial task for neuro-symbolic AI systems. In this article, we propose FactorHD, a novel HDC model capable of representing and factorizing the complex class-subclass relation efficiently. FactorHD features a symbolic encoding method that embeds an extra memorization clause, preserving more information for multiple objects. In addition, it employs an efficient factorization algorithm that selectively eliminates redundant classes by identifying the memorization clause of the target class. Such model significantly enhances computing efficiency and accuracy in representing and factorizing multiple objects with class-subclass relation, overcoming limitations of existing HDC models such as \"superposition catastrophe\" and \"the problem of 2\". Evaluations show that FactorHD achieves approximately 5667x speedup at a representation size of 10^9 compared to existing HDC models. When integrated with the ResNet-18 neural network, FactorHD achieves 92.48% factorization accuracy on the Cifar-10 dataset.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º FactorHD çš„æ–°å‹è¶…ç»´è®¡ç®— (Hyperdimensional Computing, HDC) æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç¥ç»ç¬¦å·äººå·¥æ™ºèƒ½ (neuro-symbolic AI) ä¸­å¤šå¯¹è±¡å¤šç±»åˆ«è¡¨ç¤ºä¸åˆ†è§£çš„éš¾é¢˜ã€‚é’ˆå¯¹ç°æœ‰ HDC æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„ç±»-å­ç±» (class-subclass) å…³ç³»æ—¶é¢ä¸´çš„åˆ†è§£æŒ‘æˆ˜åŠâ€œå åŠ ç¾éš¾ (superposition catastrophe)â€ç­‰å±€é™æ€§ï¼ŒFactorHD å¼•å…¥äº†ä¸€ç§ç‰¹æ®Šçš„ç¬¦å·ç¼–ç æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åµŒå…¥é¢å¤–çš„è®°å¿†å­å¥ (memorization clause) æ¥ä¸ºå¤šä¸ªå¯¹è±¡ä¿å­˜æ›´å®Œæ•´çš„ä¿¡æ¯ï¼Œå¹¶é…åˆä¸€ç§é«˜æ•ˆçš„åˆ†è§£ç®—æ³•æ¥ç²¾ç¡®è¯†åˆ«å¹¶æ¶ˆé™¤å†—ä½™ç±»åˆ«ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒFactorHD åœ¨è¡¨ç¤ºè§„æ¨¡ä¸º 10^9 æ—¶ç›¸æ¯”ç°æœ‰æ¨¡å‹å®ç°äº†çº¦ 5667 å€çš„åŠ é€Ÿï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡ã€‚æ­¤å¤–ï¼Œåœ¨ä¸ ResNet-18 ç»“åˆçš„åº”ç”¨ä¸­ï¼Œè¯¥æ¨¡å‹åœ¨ Cifar-10 æ•°æ®é›†ä¸Šå–å¾—äº† 92.48% çš„åˆ†è§£å‡†ç¡®ç‡ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚å¤šå¯¹è±¡åœºæ™¯ä¸‹çš„å“è¶Šç²¾åº¦ä¸ç¨³å¥æ€§ã€‚",
      "categories": [
        "cs.SC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.SC",
      "comment": "7 pages, 5 figures, 2 tables, to be published in the 62nd DAC (Design Automation Conference) proceedings",
      "pdf_url": "https://arxiv.org/pdf/2507.12366v1",
      "published_date": "2025-07-16 16:09:51 UTC",
      "updated_date": "2025-07-16 16:09:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:03:47.487386+00:00"
    },
    {
      "arxiv_id": "2507.12359v1",
      "title": "Cluster Contrast for Unsupervised Visual Representation Learning",
      "title_zh": "é¢å‘æ— ç›‘ç£è§†è§‰è¡¨ç¤ºå­¦ä¹ çš„èšç±»å¯¹æ¯”",
      "authors": [
        "Nikolaos Giakoumoglou",
        "Tania Stathaki"
      ],
      "abstract": "We introduce Cluster Contrast (CueCo), a novel approach to unsupervised visual representation learning that effectively combines the strengths of contrastive learning and clustering methods. Inspired by recent advancements, CueCo is designed to simultaneously scatter and align feature representations within the feature space. This method utilizes two neural networks, a query and a key, where the key network is updated through a slow-moving average of the query outputs. CueCo employs a contrastive loss to push dissimilar features apart, enhancing inter-class separation, and a clustering objective to pull together features of the same cluster, promoting intra-class compactness. Our method achieves 91.40% top-1 classification accuracy on CIFAR-10, 68.56% on CIFAR-100, and 78.65% on ImageNet-100 using linear evaluation with a ResNet-18 backbone. By integrating contrastive learning with clustering, CueCo sets a new direction for advancing unsupervised visual representation learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Cluster Contrast (CueCo) çš„æ–°å‹æ— ç›‘ç£è§†è§‰è¡¨ç¤ºå­¦ä¹  (unsupervised visual representation learning) æ–¹æ³•ï¼Œæœ‰æ•ˆç»“åˆäº†å¯¹æ¯”å­¦ä¹  (contrastive learning) å’Œèšç±»æ–¹æ³• (clustering methods) çš„ä¼˜åŠ¿ã€‚CueCo æ—¨åœ¨åŒæ—¶åœ¨ç‰¹å¾ç©ºé—´ä¸­åˆ†æ•£å’Œå¯¹é½ç‰¹å¾è¡¨ç¤ºï¼Œé‡‡ç”¨äº†æŸ¥è¯¢ (query) å’Œé”® (key) ä¸¤ä¸ªç¥ç»ç½‘ç»œç»“æ„ï¼Œå…¶ä¸­é”®ç½‘ç»œé€šè¿‡æŸ¥è¯¢è¾“å‡ºçš„æ…¢ç§»åŠ¨å¹³å‡ (slow-moving average) è¿›è¡Œæ›´æ–°ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¯¹æ¯”æŸå¤± (contrastive loss) æ¥æ¨å¼€ä¸ç›¸ä¼¼çš„ç‰¹å¾ä»¥å¢å¼ºç±»é—´åˆ†ç¦»åº¦ (inter-class separation)ï¼ŒåŒæ—¶åˆ©ç”¨èšç±»ç›®æ ‡ (clustering objective) å°†åŒç°‡ç‰¹å¾èšåˆï¼Œä»è€Œæé«˜ç±»å†…ç´§å‡‘åº¦ (intra-class compactness)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä½¿ç”¨ ResNet-18 éª¨å¹²ç½‘ç»œçš„çº¿æ€§è¯„ä¼°ä¸‹ï¼ŒCueCo åœ¨ CIFAR-10ã€CIFAR-100 å’Œ ImageNet-100 æ•°æ®é›†ä¸Šåˆ†åˆ«è¾¾åˆ°äº† 91.40%ã€68.56% å’Œ 78.65% çš„ Top-1 åˆ†ç±»å‡†ç¡®ç‡ã€‚é€šè¿‡æ•´åˆå¯¹æ¯”å­¦ä¹ ä¸èšç±»ï¼ŒCueCo ä¸ºæ¨åŠ¨æ— ç›‘ç£è§†è§‰è¡¨ç¤ºå­¦ä¹ çš„å‘å±•æä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICIP 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.12359v1",
      "published_date": "2025-07-16 15:59:43 UTC",
      "updated_date": "2025-07-16 15:59:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:03:50.015231+00:00"
    },
    {
      "arxiv_id": "2507.12329v1",
      "title": "Neural Polar Decoders for Deletion Channels",
      "title_zh": "é¢å‘åˆ é™¤ä¿¡é“çš„ç¥ç»æåŒ–è¯‘ç å™¨",
      "authors": [
        "Ziv Aharoni",
        "Henry D. Pfister"
      ],
      "abstract": "This paper introduces a neural polar decoder (NPD) for deletion channels with a constant deletion rate. Existing polar decoders for deletion channels exhibit high computational complexity of $O(N^4)$, where $N$ is the block length. This limits the application of polar codes for deletion channels to short-to-moderate block lengths. In this work, we demonstrate that employing NPDs for deletion channels can reduce the computational complexity. First, we extend the architecture of the NPD to support deletion channels. Specifically, the NPD architecture consists of four neural networks (NNs), each replicating fundamental successive cancellation (SC) decoder operations. To support deletion channels, we change the architecture of only one. The computational complexity of the NPD is $O(AN\\log N)$, where the parameter $A$ represents a computational budget determined by the user and is independent of the channel. We evaluate the new extended NPD for deletion channels with deletion rates $Î´\\in\\{0.01, 0.1\\}$ and we verify the NPD with the ground truth given by the trellis decoder by Tal et al. We further show that due to the reduced complexity of the NPD, we are able to incorporate list decoding and further improve performance. We believe that the extended NPD presented here could have applications in future technologies like DNA storage.",
      "tldr_zh": "æœ¬æ–‡é’ˆå¯¹å…·æœ‰æ’å®šåˆ é™¤ç‡çš„ Deletion Channels æå‡ºäº†ä¸€ç§ Neural Polar Decoder (NPD)ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰æåŒ–ç è¯‘ç å™¨å›  $O(N^4)$ é«˜å¤æ‚åº¦è€Œéš¾ä»¥åº”ç”¨äºé•¿ç é•¿çš„é—®é¢˜ã€‚è¯¥ç ”ç©¶é€šè¿‡æ‰©å±• NPD æ¶æ„ï¼Œä»…éœ€ä¿®æ”¹æ¨¡æ‹Ÿ Successive Cancellation (SC) è¯‘ç æ“ä½œçš„å››ä¸ªç¥ç»ç½‘ç»œç»„ä»¶ä¹‹ä¸€ï¼Œä¾¿æˆåŠŸå°†è®¡ç®—å¤æ‚åº¦é™ä½è‡³ $O(AN \\log N)$ã€‚å®éªŒåœ¨ä¸åŒåˆ é™¤ç‡ä¸‹éªŒè¯äº†è¯¥æ¨¡å‹ä¸ Trellis Decoder åœ°é¢çœŸå€¼çš„é«˜åº¦ä¸€è‡´æ€§ã€‚å¾—ç›Šäºè®¡ç®—å¤æ‚åº¦çš„æ˜¾è‘—é™ä½ï¼Œç ”ç©¶è€…è¿›ä¸€æ­¥å¼•å…¥äº† List Decoding æŠ€æœ¯ï¼Œä»è€Œåœ¨ä¿è¯æ•ˆç‡çš„åŒæ—¶æå‡äº†æ€§èƒ½è¡¨ç°ã€‚è¿™é¡¹å·¥ä½œä¸º DNA storage ç­‰å‰æ²¿å­˜å‚¨æŠ€æœ¯åœ¨å¤æ‚ä¿¡é“ç¯å¢ƒä¸‹çš„åº”ç”¨æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„è¯‘ç æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IT",
      "comment": "arXiv admin note: text overlap with arXiv:2506.17076",
      "pdf_url": "https://arxiv.org/pdf/2507.12329v1",
      "published_date": "2025-07-16 15:22:34 UTC",
      "updated_date": "2025-07-16 15:22:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:03:57.633867+00:00"
    },
    {
      "arxiv_id": "2507.12318v3",
      "title": "Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models",
      "title_zh": "é¢å‘é«˜ä¿çœŸã€é«˜ç”ŸæˆåŠ›æ‰©æ•£æ¨¡å‹çš„ç»„åˆå¼ç¦»æ•£æ½œç ",
      "authors": [
        "Samuel Lavoie",
        "Michael Noukhovitch",
        "Aaron Courville"
      ],
      "abstract": "We argue that diffusion models' success in modeling complex distributions is, for the most part, coming from their input conditioning. This paper investigates the representation used to condition diffusion models from the perspective that ideal representations should improve sample fidelity, be easy to generate, and be compositional to allow out-of-training samples generation. We introduce Discrete Latent Code (DLC), an image representation derived from Simplicial Embeddings trained with a self-supervised learning objective. DLCs are sequences of discrete tokens, as opposed to the standard continuous image embeddings. They are easy to generate and their compositionality enables sampling of novel images beyond the training distribution. Diffusion models trained with DLCs have improved generation fidelity, establishing a new state-of-the-art for unconditional image generation on ImageNet. Additionally, we show that composing DLCs allows the image generator to produce out-of-distribution samples that coherently combine the semantics of images in diverse ways. Finally, we showcase how DLCs can enable text-to-image generation by leveraging large-scale pretrained language models. We efficiently finetune a text diffusion language model to generate DLCs that produce novel samples outside of the image generator training distribution.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ‰©æ•£æ¨¡å‹(Diffusion Models)ä¸­è¾“å…¥è°ƒèŠ‚(Input Conditioning)å¯¹å»ºæ¨¡å¤æ‚åˆ†å¸ƒçš„é‡è¦æ€§ï¼Œæå‡ºç†æƒ³çš„è¡¨ç¤ºåº”å…·å¤‡é«˜ä¿çœŸåº¦ã€æ˜“ç”Ÿæˆæ€§å’Œç»„åˆæ€§(Compositionality)ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†ç¦»æ•£æ½œç (Discrete Latent Code, DLC)ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç®€å•åµŒå…¥(Simplicial Embeddings)å¹¶é€šè¿‡è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒå¾—åˆ°çš„ç¦»æ•£æ ‡è®°(Tokens)åºåˆ—è¡¨ç¤ºã€‚ä¸ä¼ ç»Ÿçš„è¿ç»­å›¾åƒåµŒå…¥ä¸åŒï¼ŒDLC çš„ç»„åˆèƒ½åŠ›å…è®¸æ¨¡å‹é‡‡æ ·å‡ºè¶…å‡ºè®­ç»ƒåˆ†å¸ƒçš„æ–°å›¾åƒï¼Œæå¤§åœ°å¢å¼ºäº†ç”Ÿæˆç”Ÿäº§åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨ DLC çš„æ‰©æ•£æ¨¡å‹åœ¨ ImageNet æ— æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šåˆ›ä¸‹äº†æ–°çš„æœ€å…ˆè¿›(State-of-the-art)è®°å½•ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯æ˜é€šè¿‡ç»„åˆ DLC å¯ä»¥äº§ç”Ÿè¿è´¯ç»“åˆå¤šç§å›¾åƒè¯­ä¹‰çš„åˆ†å¸ƒå¤–(Out-of-distribution)æ ·æœ¬ã€‚æœ€åï¼Œè¯¥å·¥ä½œè¿˜å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹é«˜æ•ˆå¾®è°ƒä»¥ç”Ÿæˆ DLCï¼Œä»è€Œå®ç°å…·æœ‰é«˜åº¦æ–°é¢–æ€§çš„æ–‡æœ¬åˆ°å›¾åƒ(Text-to-image)ç”Ÿæˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Published at NeurIPS, 22 pages, 7 tables, 12 figures, code and models available",
      "pdf_url": "https://arxiv.org/pdf/2507.12318v3",
      "published_date": "2025-07-16 15:12:17 UTC",
      "updated_date": "2026-01-05 21:20:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:04:26.586892+00:00"
    },
    {
      "arxiv_id": "2508.00858v1",
      "title": "Deploying Geospatial Foundation Models in the Real World: Lessons from WorldCereal",
      "title_zh": "åœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œä¸­çš„éƒ¨ç½²ï¼šæ¥è‡ª WorldCereal çš„ç»éªŒä¸å¯ç¤º",
      "authors": [
        "Christina Butsko",
        "Kristof Van Tricht",
        "Gabriel Tseng",
        "Giorgia Milli",
        "David Rolnick",
        "Ruben Cartuyvels",
        "Inbal Becker Reshef",
        "Zoltan Szantoi",
        "Hannah Kerner"
      ],
      "abstract": "The increasing availability of geospatial foundation models has the potential to transform remote sensing applications such as land cover classification, environmental monitoring, and change detection. Despite promising benchmark results, the deployment of these models in operational settings is challenging and rare. Standardized evaluation tasks often fail to capture real-world complexities relevant for end-user adoption such as data heterogeneity, resource constraints, and application-specific requirements. This paper presents a structured approach to integrate geospatial foundation models into operational mapping systems. Our protocol has three key steps: defining application requirements, adapting the model to domain-specific data and conducting rigorous empirical testing. Using the Presto model in a case study for crop mapping, we demonstrate that fine-tuning a pre-trained model significantly improves performance over conventional supervised methods. Our results highlight the model's strong spatial and temporal generalization capabilities. Our protocol provides a replicable blueprint for practitioners and lays the groundwork for future research to operationalize foundation models in diverse remote sensing applications. Application of the protocol to the WorldCereal global crop-mapping system showcases the framework's scalability.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹ (Geospatial Foundation Models) åœ¨å®é™…ä¸šåŠ¡éƒ¨ç½²ä¸­é¢ä¸´çš„æ•°æ®å¼‚è´¨æ€§ã€èµ„æºé™åˆ¶å’Œç‰¹å®šéœ€æ±‚ç­‰æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€å¥—ç»“æ„åŒ–çš„é›†æˆåè®®ã€‚è¯¥åè®®åŒ…å«å®šä¹‰åº”ç”¨éœ€æ±‚ã€é¢†åŸŸæ•°æ®é€‚é…åŠå®è¯æµ‹è¯•ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼Œæ—¨åœ¨å°†åŸºç¡€æ¨¡å‹æœ‰æ•ˆåœ°å¼•å…¥ä¸šåŠ¡åŒ–åˆ¶å›¾ç³»ç»Ÿã€‚é€šè¿‡åœ¨ WorldCereal å…¨çƒä½œç‰©åˆ¶å›¾ç³»ç»Ÿä¸­ä½¿ç”¨ Presto æ¨¡å‹è¿›è¡Œæ¡ˆä¾‹ç ”ç©¶ï¼Œç»“æœè¡¨æ˜å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒ (fine-tuning) çš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚å®éªŒè¿›ä¸€æ­¥éªŒè¯äº†æ¨¡å‹å…·å¤‡å¼ºå¤§çš„ç©ºé—´ä¸æ—¶é—´æ³›åŒ–èƒ½åŠ›ï¼Œä»¥åŠåœ¨å¤„ç†å…¨çƒå°ºåº¦ä»»åŠ¡æ—¶çš„å¯æ‰©å±•æ€§ã€‚è¯¥ç ”ç©¶ä¸ä»…ä¸ºé¥æ„Ÿä»ä¸šè€…æä¾›äº†å¯å¤åˆ¶çš„å®è·µè“å›¾ï¼Œä¹Ÿä¸ºæ¨åŠ¨åŸºç¡€æ¨¡å‹åœ¨å¤šæ ·åŒ–ç°å®ç›‘æµ‹åœºæ™¯ä¸­çš„ä¸šåŠ¡åŒ–è¿è¡Œå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00858v1",
      "published_date": "2025-07-16 15:10:32 UTC",
      "updated_date": "2025-07-16 15:10:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:04:27.684520+00:00"
    },
    {
      "arxiv_id": "2507.12314v2",
      "title": "Thought Purity: A Defense Framework For Chain-of-Thought Attack",
      "title_zh": "Thought Purityï¼šé’ˆå¯¹é“¾å¼æ€ç»´æ”»å‡»çš„é˜²å¾¡æ¡†æ¶",
      "authors": [
        "Zihao Xue",
        "Zhen Bi",
        "Long Ma",
        "Zhenlin Hu",
        "Yan Wang",
        "Zhenfang Liu",
        "Qing Sheng",
        "Jie Xiao",
        "Jungang Lou"
      ],
      "abstract": "While reinforcement learning-trained Large Reasoning Models (LRMs, e.g., Deepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large Language Models (LLMs) domain, their susceptibility to security threats remains a critical vulnerability. This weakness is particularly evident in Chain-of-Thought (CoT) generation processes, where adversarial methods like backdoor prompt attacks can systematically subvert the model's core reasoning mechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this vulnerability through exploiting prompt controllability, simultaneously degrading both CoT safety and task performance with low-cost interventions. To address this compounded security-performance vulnerability, we propose Thought Purity (TP): a defense framework that systematically strengthens resistance to malicious content while preserving operational efficacy. Our solution achieves this through three synergistic components: (1) a safety-optimized data processing pipeline (2) reinforcement learning-enhanced rule constraints (3) adaptive monitoring metrics. Our approach establishes the first comprehensive defense mechanism against CoTA vulnerabilities in reinforcement learning-aligned reasoning systems, significantly advancing the security-functionality equilibrium for next-generation AI architectures.",
      "tldr_zh": "éšç€å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„å¤§è¯­è¨€æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼Œå¦‚Deepseek-R1ï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šçš„æå‡ï¼Œå…¶åœ¨æ€ç»´é“¾ï¼ˆChain-of-Thought, CoTï¼‰ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å®‰å…¨æ¼æ´ä¹Ÿé€æ¸æš´éœ²ï¼Œç‰¹åˆ«æ˜¯æ€ç»´é“¾æ”»å‡»ï¼ˆChain-of-Thought Attack, CoTAï¼‰èƒ½ä»¥ä½æˆæœ¬æ‰‹æ®µç³»ç»Ÿæ€§åœ°ç ´åæ¨ç†æœºåˆ¶å¹¶é™ä½æ¨¡å‹æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶æå‡ºäº†Thought Purity (TP) é˜²å¾¡æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿæ€§åœ°å¢å¼ºæ¨¡å‹å¯¹æ¶æ„å†…å®¹çš„æŠµæŠ—åŠ›ï¼ŒåŒæ—¶ä¿ç•™å…¶è¿è¡Œæ•ˆèƒ½ã€‚è¯¥æ¡†æ¶ç”±å®‰å…¨ä¼˜åŒ–çš„æ•°æ®å¤„ç†æµæ°´çº¿ã€å¼ºåŒ–å­¦ä¹ å¢å¼ºçš„è§„åˆ™çº¦æŸä»¥åŠè‡ªé€‚åº”ç›‘æ§æŒ‡æ ‡ä¸‰ä¸ªååŒéƒ¨åˆ†ç»„æˆã€‚ä½œä¸ºé¦–ä¸ªé’ˆå¯¹å¼ºåŒ–å­¦ä¹ å¯¹é½æ¨ç†ç³»ç»Ÿä¸­ CoTA æ¼æ´çš„å…¨é¢é˜²å¾¡æœºåˆ¶ï¼ŒThought Purity åœ¨ä¿éšœå®‰å…¨æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†ä¸‹ä¸€ä»£äººå·¥æ™ºèƒ½æ¶æ„åœ¨å®‰å…¨æ€§ä¸åŠŸèƒ½æ€§ä¹‹é—´çš„å¹³è¡¡ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºå¯æŠµå¾¡å¤æ‚æç¤ºè¯æ”»å‡»çš„é«˜çº§æ¨ç†ç³»ç»Ÿæä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ä¸å®è·µæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12314v2",
      "published_date": "2025-07-16 15:09:13 UTC",
      "updated_date": "2025-10-04 04:42:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:04:25.790536+00:00"
    },
    {
      "arxiv_id": "2507.12308v1",
      "title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization",
      "title_zh": "Chain-of-Descriptionsï¼šæå‡ VHDL ä»£ç ç”Ÿæˆä¸æ‘˜è¦çš„ä»£ç å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½",
      "authors": [
        "Prashanth Vijayaraghavan",
        "Apoorva Nitsure",
        "Charles Mackin",
        "Luyao Shi",
        "Stefano Ambrogio",
        "Arvind Haran",
        "Viresh Paruthi",
        "Ali Elzein",
        "Dan Coops",
        "David Beymer",
        "Tyler Baldwin",
        "Ehsan Degan"
      ],
      "abstract": "Large Language Models (LLMs) have become widely used across diverse NLP tasks and domains, demonstrating their adaptability and effectiveness. In the realm of Electronic Design Automation (EDA), LLMs show promise for tasks like Register-Transfer Level (RTL) code generation and summarization. However, despite the proliferation of LLMs for general code-related tasks, there's a dearth of research focused on evaluating and refining these models for hardware description languages (HDLs), notably VHDL. In this study, we evaluate the performance of existing code LLMs for VHDL code generation and summarization using various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter, an in-house dataset, aims to gauge LLMs' understanding of functionally equivalent code. Our findings reveal consistent underperformance of these models across different metrics, underscoring a significant gap in their suitability for this domain. To address this challenge, we propose Chain-of-Descriptions (CoDes), a novel approach to enhance the performance of LLMs for VHDL code generation and summarization tasks. CoDes involves generating a series of intermediate descriptive steps based on: (i) the problem statement for code generation, and (ii) the VHDL code for summarization. These steps are then integrated with the original input prompt (problem statement or code) and provided as input to the LLMs to generate the final output. Our experiments demonstrate that the CoDes approach significantly surpasses the standard prompting strategy across various metrics on both datasets. This method not only improves the quality of VHDL code generation and summarization but also serves as a framework for future research aimed at enhancing code LLMs for VHDL.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç”µå­è®¾è®¡è‡ªåŠ¨åŒ–(EDA)é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯ç¡¬ä»¶æè¿°è¯­è¨€VHDLä¸­çš„åº”ç”¨ç°çŠ¶ã€‚é€šè¿‡åœ¨VHDL-Evalå’ŒVHDL-Xformä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼Œç ”ç©¶å‘ç°ç°æœ‰æ¨¡å‹åœ¨VHDLä»£ç ç”Ÿæˆå’Œæ‘˜è¦ä»»åŠ¡ä¸­è¡¨ç°æ™®éä¸ä½³ï¼Œå­˜åœ¨æ˜¾è‘—çš„é¢†åŸŸé€‚é…ç¼ºå£ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Chain-of-Descriptions (CoDes)æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ç”ŸæˆåŸºäºé—®é¢˜é™ˆè¿°æˆ–ä»£ç çš„ä¸­é—´æè¿°æ­¥éª¤æ¥è¾…åŠ©æ¨¡å‹æ¨ç†ã€‚è¿™äº›ç”Ÿæˆçš„ä¸­é—´æ­¥éª¤ä¸åŸå§‹æç¤ºè¯æ•´åˆåè¾“å…¥LLMsï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¼•å¯¼æœ€ç»ˆè¾“å‡ºçš„ç²¾ç¡®ç”Ÿæˆã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCoDesåœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºæ ‡å‡†æç¤ºç­–ç•¥ï¼Œå¤§å¹…æå‡äº†VHDLä»£ç ç”Ÿæˆå’Œæ‘˜è¦çš„è´¨é‡ã€‚è¯¥ç ”ç©¶ä¸ä»…æ”¹å–„äº†é’ˆå¯¹VHDLä»»åŠ¡çš„å¤„ç†æ•ˆæœï¼Œä¹Ÿä¸ºæœªæ¥æå‡ç¡¬ä»¶æè¿°è¯­è¨€é¢†åŸŸä»£ç å¤§æ¨¡å‹æ€§èƒ½çš„ç ”ç©¶æä¾›äº†é‡è¦æ¡†æ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages (6 content pages + 4 supplementary), 5 figures, Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD. 2024 (MLCAD'24)",
      "pdf_url": "https://arxiv.org/pdf/2507.12308v1",
      "published_date": "2025-07-16 15:05:30 UTC",
      "updated_date": "2025-07-16 15:05:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:04:30.288118+00:00"
    },
    {
      "arxiv_id": "2507.12305v1",
      "title": "PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning",
      "title_zh": "PROLï¼šåŸºäºæç¤ºåœ¨çº¿å­¦ä¹ çš„æµå¼æ•°æ®æ— é‡æ¼”æŒç»­å­¦ä¹ ",
      "authors": [
        "M. Anwar Ma'sum",
        "Mahardhika Pratama",
        "Savitha Ramasamy",
        "Lin Liu",
        "Habibullah Habibullah",
        "Ryszard Kowalczyk"
      ],
      "abstract": "The data privacy constraint in online continual learning (OCL), where the data can be seen only once, complicates the catastrophic forgetting problem in streaming data. A common approach applied by the current SOTAs in OCL is with the use of memory saving exemplars or features from previous classes to be replayed in the current task. On the other hand, the prompt-based approach performs excellently in continual learning but with the cost of a growing number of trainable parameters. The first approach may not be applicable in practice due to data openness policy, while the second approach has the issue of throughput associated with the streaming data. In this study, we propose a novel prompt-based method for online continual learning that includes 4 main components: (1) single light-weight prompt generator as a general knowledge, (2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model (PTM) generalization preserving, and (4) hard-soft updates mechanism. Our proposed method achieves significantly higher performance than the current SOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity analysis shows that our method requires a relatively smaller number of parameters and achieves moderate training time, inference time, and throughput. For further study, the source code of our method is available at https://github.com/anwarmaxsum/PROL.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨çº¿æŒç»­å­¦ä¹ (Online Continual Learning)ä¸­ç”±äºéšç§é™åˆ¶å¯¼è‡´æ— æ³•ä½¿ç”¨é‡æ¼”(Rehearsal)æœºåˆ¶ï¼Œä»¥åŠç°æœ‰æç¤ºå­¦ä¹ æ–¹æ³•å‚æ•°å¢é•¿è¿‡å¿«å’Œååé‡ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºPROLçš„æç¤ºåœ¨çº¿å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ–¹æ³•åŒ…å«è½»é‡åŒ–æç¤ºç”Ÿæˆå™¨ã€å¯è®­ç»ƒç¼©æ”¾å¹³ç§»å™¨ã€é¢„è®­ç»ƒæ¨¡å‹(PTM)æ³›åŒ–ä¿æŒæœºåˆ¶ä»¥åŠç¡¬è½¯æ›´æ–°(Hard-soft updates)æœºåˆ¶å››ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚é€šè¿‡è¿™äº›ç»„ä»¶çš„åä½œï¼ŒPROLåœ¨å®ç°æ— é‡æ¼”å­¦ä¹ çš„åŒæ—¶æœ‰æ•ˆæ§åˆ¶äº†æ¨¡å‹å¤æ‚åº¦ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†æµå¼æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPROLåœ¨CIFAR100ã€ImageNet-Rã€ImageNet-Aå’ŒCUBç­‰å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºå½“å‰çš„SOTAæ–¹æ³•ã€‚æ­¤å¤–ï¼Œå¤æ‚åº¦åˆ†æéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å…·å¤‡è¾ƒå°‘å‚æ•°é‡çš„åŒæ—¶ï¼Œèƒ½ä¿æŒè‰¯å¥½çš„è®­ç»ƒæ¨ç†æ•ˆç‡å’Œååé‡æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.12305v1",
      "published_date": "2025-07-16 15:04:46 UTC",
      "updated_date": "2025-07-16 15:04:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:04:31.285508+00:00"
    },
    {
      "arxiv_id": "2507.12295v1",
      "title": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding",
      "title_zh": "Text-ADBenchï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹åµŒå…¥çš„æ–‡æœ¬å¼‚å¸¸æ£€æµ‹åŸºå‡†",
      "authors": [
        "Feng Xiao",
        "Jicong Fan"
      ],
      "abstract": "Text anomaly detection is a critical task in natural language processing (NLP), with applications spanning fraud detection, misinformation identification, spam detection and content moderation, etc. Despite significant advances in large language models (LLMs) and anomaly detection algorithms, the absence of standardized and comprehensive benchmarks for evaluating the existing anomaly detection methods on text data limits rigorous comparison and development of innovative approaches. This work performs a comprehensive empirical study and introduces a benchmark for text anomaly detection, leveraging embeddings from diverse pre-trained language models across a wide array of text datasets. Our work systematically evaluates the effectiveness of embedding-based text anomaly detection by incorporating (1) early language models (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI (small, ada, large)); (3) multi-domain text datasets (news, social media, scientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC). Our experiments reveal a critical empirical insight: embedding quality significantly governs anomaly detection efficacy, and deep learning-based approaches demonstrate no performance advantage over conventional shallow algorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived embeddings.In addition, we observe strongly low-rank characteristics in cross-model performance matrices, which enables an efficient strategy for rapid model evaluation (or embedding evaluation) and selection in practical applications. Furthermore, by open-sourcing our benchmark toolkit that includes all embeddings from different models and code at https://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work provides a foundation for future research in robust and scalable text anomaly detection systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬å¼‚å¸¸æ£€æµ‹ (Text Anomaly Detection) é¢†åŸŸç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLMs) åµŒå…¥çš„ç»¼åˆæ€§è¯„ä¼°åŸºå‡† Text-ADBenchã€‚è¯¥åŸºå‡†ç³»ç»Ÿåœ°è¯„ä¼°äº†åŒ…æ‹¬ GloVeã€BERT ä»¥åŠ LLaMa-2ã€LLaMa-3ã€Mistral å’Œ OpenAI ç³»åˆ—åœ¨å†…çš„å¤šç§é¢„è®­ç»ƒæ¨¡å‹åµŒå…¥ï¼Œæ¶µç›–äº†æ–°é—»ã€ç¤¾äº¤åª’ä½“å’Œå­¦æœ¯å‡ºç‰ˆç­‰å¤šä¸ªé¢†åŸŸçš„æ–‡æœ¬æ•°æ®é›†ã€‚å®éªŒç»“æœæ­ç¤ºäº†åµŒå…¥è´¨é‡æ˜¯å†³å®šå¼‚å¸¸æ£€æµ‹æ•ˆæœçš„æ ¸å¿ƒå› ç´ ï¼Œå¹¶å‘ç°å½“åˆ©ç”¨ LLM æ´¾ç”Ÿçš„åµŒå…¥æ—¶ï¼ŒKNN å’Œ Isolation Forest ç­‰ä¼ ç»Ÿæµ…å±‚ç®—æ³•ç›¸è¾ƒäºæ·±åº¦å­¦ä¹ æ–¹æ³•å¹¶æ²¡æœ‰è¡¨ç°å‡ºæ€§èƒ½åŠ£åŠ¿ã€‚æ­¤å¤–ï¼Œç ”ç©¶è§‚å¯Ÿåˆ°è·¨æ¨¡å‹æ€§èƒ½çŸ©é˜µå…·æœ‰æ˜¾è‘—çš„ä½ç§©ç‰¹æ€§ï¼Œè¿™ä¸ºå®é™…åº”ç”¨ä¸­å¿«é€Ÿè¿›è¡Œæ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©æä¾›äº†é«˜æ•ˆç­–ç•¥ã€‚é€šè¿‡å¼€æºåŒ…å«æ‰€æœ‰æ¨¡å‹åµŒå…¥å’Œä»£ç çš„åŸºå‡†å·¥å…·åŒ…ï¼Œè¯¥å·¥ä½œä¸ºæœªæ¥æ„å»ºç¨³å¥ä¸”å¯æ‰©å±•çš„æ–‡æœ¬å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12295v1",
      "published_date": "2025-07-16 14:47:41 UTC",
      "updated_date": "2025-07-16 14:47:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:04:56.046196+00:00"
    },
    {
      "arxiv_id": "2507.12286v2",
      "title": "SHACL Validation in the Presence of Ontologies: Semantics and Rewriting Techniques",
      "title_zh": "ç»“åˆæœ¬ä½“çš„ SHACL éªŒè¯ï¼šè¯­ä¹‰ä¸é‡å†™æŠ€æœ¯",
      "authors": [
        "Anouk Oudshoorn",
        "Magdalena Ortiz",
        "Mantas Simkus"
      ],
      "abstract": "SHACL and OWL are two prominent W3C standards for managing RDF data. These languages share many features, but they have one fundamental difference: OWL, designed for inferring facts from incomplete data, makes the open-world assumption, whereas SHACL is a constraint language that treats the data as complete and must be validated under the closed-world assumption. The combination of both formalisms is very appealing and has been called for, but their semantic gap is a major challenge, semantically and computationally. In this paper, we advocate a semantics for SHACL validation in the presence of ontologies based on core universal models. We provide a technique for constructing these models for ontologies in the rich data-tractable description logic Horn-ALCHIQ. Furthermore, we use a finite representation of this model to develop a rewriting technique that reduces SHACL validation in the presence of ontologies to standard validation. Finally, we study the complexity of SHACL validation in the presence of ontologies, and show that even very simple ontologies make the problem EXPTIME-complete, and PTIME-complete in data complexity.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æœ¬ä½“(Ontologies)èƒŒæ™¯ä¸‹è¿›è¡ŒSHACLéªŒè¯çš„é—®é¢˜ï¼Œæ—¨åœ¨æ¡¥æ¥OWLçš„å¼€æ”¾ä¸–ç•Œå‡è®¾ä¸SHACLçš„å°é—­ä¸–ç•Œå‡è®¾ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºæ ¸å¿ƒé€šç”¨æ¨¡å‹(core universal models)çš„è¯­ä¹‰æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºå¤„ç†å­˜åœ¨æœ¬ä½“æ—¶çš„SHACLéªŒè¯éœ€æ±‚ã€‚é’ˆå¯¹æè¿°é€»è¾‘Horn-ALCHIQï¼Œç ”ç©¶æä¾›äº†ä¸€ç§æ„å»ºæ­¤ç±»æ¨¡å‹çš„æ–¹æ³•ï¼Œå¹¶å¼€å‘äº†ä¸€ç§é‡å†™æŠ€æœ¯(rewriting technique)ï¼Œå°†å¤æ‚çš„ç»“åˆéªŒè¯é—®é¢˜è½¬åŒ–ä¸ºæ ‡å‡†çš„SHACLéªŒè¯ä»»åŠ¡ã€‚å¤æ‚æ€§åˆ†æè¡¨æ˜ï¼Œè¯¥é—®é¢˜åœ¨ä¸€èˆ¬æƒ…å†µä¸‹æ˜¯EXPTIME-completeï¼Œè€Œåœ¨æ•°æ®å¤æ‚æ€§ä¸Šä¸ºPTIME-completeã€‚è¿™é¡¹å·¥ä½œä¸ºRDFæ•°æ®ç®¡ç†ä¸­ä¸¤ç§é‡è¦W3Cæ ‡å‡†çš„ååŒå·¥ä½œæä¾›äº†ç†è®ºæ”¯æ’‘å’Œæœ‰æ•ˆçš„è®¡ç®—æ‰‹æ®µã€‚",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "Published in AIJ",
      "pdf_url": "https://arxiv.org/pdf/2507.12286v2",
      "published_date": "2025-07-16 14:38:27 UTC",
      "updated_date": "2026-01-20 15:29:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:04:46.997277+00:00"
    },
    {
      "arxiv_id": "2507.12284v3",
      "title": "MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks",
      "title_zh": "MERA Codeï¼šè·¨ä»»åŠ¡ä»£ç ç”Ÿæˆè¯„ä¼°çš„ç»Ÿä¸€æ¡†æ¶",
      "authors": [
        "Artem Chervyakov",
        "Alexander Kharitonov",
        "Pavel Zadorozhny",
        "Adamenko Pavel",
        "Rodion Levichev",
        "Dmitrii Vorobev",
        "Dmitrii Salikhov",
        "Aidar Valeev",
        "Alena Pestova",
        "Maria Dziuba",
        "Ilseyar Alimova",
        "Artem Zavgorodnev",
        "Aleksandr Medvedev",
        "Stanislav Moiseev",
        "Elena Bruches",
        "Daniil Grebenkin",
        "Roman Derunets",
        "Vikulov Vladimir",
        "Anton Emelyanov",
        "Dmitrii Babaev",
        "Vladimir V. Ivanov",
        "Valentin Malykh",
        "Alena Fenogenova"
      ],
      "abstract": "Advancements in LLMs have enhanced task automation in software engineering; however, current evaluations primarily focus on natural language tasks, overlooking code quality. Most benchmarks prioritize high-level reasoning over executable code and real-world performance, leaving gaps in understanding true capabilities and risks associated with these models in production. To address this issue, we propose MERA Code, a new addition to the MERA benchmark family, specifically focused on evaluating code for the latest code generation LLMs in Russian. This benchmark includes 11 evaluation tasks that span 8 programming languages. Our proposed evaluation methodology features a taxonomy that outlines the practical coding skills necessary for models to complete these tasks. The benchmark comprises an open-source codebase for users to conduct MERA assessments, a scoring system compatible with various programming environments, and a platform featuring a leaderboard and submission system. We evaluate open LLMs and frontier API models, analyzing their limitations in terms of practical coding tasks in non-English languages. We are publicly releasing MERA to guide future research, anticipate groundbreaking features in model development, and standardize evaluation procedures.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MERA Codeï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è·¨ä»»åŠ¡ä»£ç ç”Ÿæˆçš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ LLMs è¯„ä¼°ä¸­å¿½è§†ä»£ç è´¨é‡å’Œå®é™…å¯æ‰§è¡Œæ€§çš„é—®é¢˜ã€‚ä½œä¸º MERA åŸºå‡†å®¶æ—çš„æ–°æˆå‘˜ï¼Œè¯¥æ¡†æ¶ä¸“é—¨é’ˆå¯¹ä¿„è¯­èƒŒæ™¯ä¸‹çš„æœ€æ–°ä»£ç ç”Ÿæˆæ¨¡å‹ï¼Œæ¶µç›–äº† 8 ç§ç¼–ç¨‹è¯­è¨€çš„ 11 é¡¹è¯„ä¼°ä»»åŠ¡ã€‚ç ”ç©¶æå‡ºäº†ä¸€å¥—åŒ…å«æŠ€èƒ½åˆ†ç±»ä½“ç³» (taxonomy) çš„è¯„ä¼°æ–¹æ³•è®ºï¼Œä»¥è¯„ä¼°æ¨¡å‹å®Œæˆå®é™…ç¼–ç ä»»åŠ¡çš„èƒ½åŠ›ã€‚MERA Code åŒ…å«å¼€æºä»£ç åº“ã€å…¼å®¹å¤šç§ç¯å¢ƒçš„è¯„åˆ†ç³»ç»Ÿä»¥åŠé›†æˆæ’è¡Œæ¦œå’Œæäº¤ç³»ç»Ÿçš„å¹³å°ã€‚é€šè¿‡å¯¹å¼€æº LLMs å’Œå‰æ²¿ API æ¨¡å‹ (frontier API models) çš„è¯„ä¼°ï¼Œç ”ç©¶æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨éè‹±è¯­ç¼–ç¨‹ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶çš„å…¬å¼€å‘å¸ƒä¸ºæ ‡å‡†åŒ–ä»£ç ç”Ÿæˆè¯„ä¼°æµç¨‹å¹¶å¼•å¯¼æœªæ¥æ¨¡å‹ç ”ç©¶æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12284v3",
      "published_date": "2025-07-16 14:31:33 UTC",
      "updated_date": "2025-12-01 11:19:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:05:03.194191+00:00"
    },
    {
      "arxiv_id": "2507.12269v3",
      "title": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants",
      "title_zh": "åŸºäºæ¸è¿›å¼å±‚å†»ç»“çš„ç«™ç‚¹çº§å¾®è°ƒï¼šå®ç°è¶…æ—©äº§å„¿å‡ºç”Ÿé¦–æ—¥èƒ¸ç‰‡å¯¹æ”¯æ°”ç®¡è‚ºå‘è‚²ä¸è‰¯çš„ç¨³å¥é¢„æµ‹",
      "authors": [
        "Sybelle Goedicke-Fritz",
        "Michelle Bous",
        "Annika Engel",
        "Matthias Flotho",
        "Pascal Hirsch",
        "Hannah Wittig",
        "Dino Milanovic",
        "Dominik Mohr",
        "Mathias Kaspar",
        "Sogand Nemat",
        "Dorothea Kerner",
        "Arno BÃ¼cker",
        "Andreas Keller",
        "Sascha Meyer",
        "Michael Zemlin",
        "Philipp Flotho"
      ],
      "abstract": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of extremely low birth weight infants. Defined by oxygen dependence at 36 weeks postmenstrual age, it causes lifelong respiratory complications. However, preventive interventions carry severe risks, including neurodevelopmental impairment, ventilator-induced lung injury, and systemic complications. Therefore, early BPD prognosis and prediction of BPD outcome is crucial to avoid unnecessary toxicity in low risk infants. Admission radiographs of extremely preterm infants are routinely acquired within 24h of life and could serve as a non-invasive prognostic tool. In this work, we developed and investigated a deep learning approach using chest X-rays from 163 extremely low-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within 24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult chest radiographs, employing progressive layer freezing with discriminative learning rates to prevent overfitting and evaluated a CutMix augmentation and linear probing. For moderate/severe BPD outcome prediction, our best performing model with progressive freezing, linear probing and CutMix achieved an AUROC of 0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67 $\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet initialization (p = 0.031) which confirms domain-specific pretraining to be important for BPD outcome prediction. Routine IRDS grades showed limited prognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned markers. Our approach demonstrates that domain-specific pretraining enables accurate BPD prediction from routine day-1 radiographs. Through progressive freezing and linear probing, the method remains computationally feasible for site-level implementation and future federated learning deployments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨é€šè¿‡æä½å‡ºç”Ÿä½“é‡æ—©äº§å„¿å‡ºç”Ÿ24å°æ—¶å†…çš„å¸¸è§„èƒ¸éƒ¨ X-ray å›¾åƒï¼Œå®ç°æ”¯æ°”ç®¡è‚ºå‘è‚²ä¸è‰¯ (Bronchopulmonary dysplasia, BPD) çš„æ—©æœŸå¥å£®é¢„æµ‹ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäº ResNet-50 çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¨¡å‹é¢„å…ˆåœ¨æˆäººèƒ¸ç‰‡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶é‡‡ç”¨äº†æ¸è¿›å¼å±‚å†»ç»“ (Progressive layer freezing) å’Œåˆ¤åˆ«å­¦ä¹ ç‡ (Discriminative learning rates) æŠ€æœ¯è¿›è¡Œ Site-Level çš„å¾®è°ƒï¼ŒåŒæ—¶ç»“åˆäº† CutMix æ•°æ®å¢å¼ºå’Œçº¿æ€§æ¢æµ‹ (Linear probing)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€ä¼˜æ¨¡å‹åœ¨é¢„æµ‹ä¸­é‡åº¦ BPD æ–¹é¢è¾¾åˆ°äº† 0.78 çš„ AUROC å’Œ 0.69 çš„å¹³è¡¡å‡†ç¡®åº¦ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äº ImageNet åˆå§‹åŒ–å’Œä¼ ç»Ÿçš„ IRDS ä¸´åºŠåˆ†çº§ã€‚ç ”ç©¶è¯å®äº†é¢†åŸŸå†…é¢„è®­ç»ƒ (In-domain pre-training) å¯¹æå‡ BPD é¢„æµ‹å‡†ç¡®æ€§çš„æ ¸å¿ƒä»·å€¼ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ¸è¿›å¼å†»ç»“æŠ€æœ¯ä¿è¯äº†è®¡ç®—çš„å¯è¡Œæ€§ï¼Œä¸ºåŒ»ç–—æœºæ„çš„ç°åœºå®æ–½ä»¥åŠæœªæ¥åœ¨è”é‚¦å­¦ä¹  (Federated learning) ä¸­çš„éƒ¨ç½²æä¾›äº†æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "S.G.-F., M.B., and A.E. contributed equally to this work and share first authorship. M.Z. and P.F. contributed equally to this work and share senior authorship",
      "pdf_url": "https://arxiv.org/pdf/2507.12269v3",
      "published_date": "2025-07-16 14:19:44 UTC",
      "updated_date": "2025-10-10 02:07:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:04:57.950122+00:00"
    },
    {
      "arxiv_id": "2507.12262v1",
      "title": "A Framework for Nonstationary Gaussian Processes with Neural Network Parameters",
      "title_zh": "åŸºäºç¥ç»ç½‘ç»œå‚æ•°çš„éå¹³ç¨³é«˜æ–¯è¿‡ç¨‹æ¡†æ¶",
      "authors": [
        "Zachary James",
        "Joseph Guinness"
      ],
      "abstract": "Gaussian processes have become a popular tool for nonparametric regression because of their flexibility and uncertainty quantification. However, they often use stationary kernels, which limit the expressiveness of the model and may be unsuitable for many datasets. We propose a framework that uses nonstationary kernels whose parameters vary across the feature space, modeling these parameters as the output of a neural network that takes the features as input. The neural network and Gaussian process are trained jointly using the chain rule to calculate derivatives. Our method clearly describes the behavior of the nonstationary parameters and is compatible with approximation methods for scaling to large datasets. It is flexible and easily adapts to different nonstationary kernels without needing to redesign the optimization procedure. Our methods are implemented with the GPyTorch library and can be readily modified. We test a nonstationary variance and noise variant of our method on several machine learning datasets and find that it achieves better accuracy and log-score than both a stationary model and a hierarchical model approximated with variational inference. Similar results are observed for a model with only nonstationary variance. We also demonstrate our approach's ability to recover the nonstationary parameters of a spatial dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹éå¹³ç¨³é«˜æ–¯è¿‡ç¨‹ (Nonstationary Gaussian Processes) çš„é€šç”¨æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå¹³ç¨³æ ¸ (stationary kernels) åœ¨å¤„ç†å¤æ‚æ•°æ®é›†æ—¶è¡¨è¾¾èƒ½åŠ›å—é™çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯å°†æ ¸å‚æ•°å»ºæ¨¡ä¸ºç¥ç»ç½‘ç»œ (neural network) çš„è¾“å‡ºï¼Œä½¿å¾—è¿™äº›å‚æ•°èƒ½å¤Ÿéšç‰¹å¾ç©ºé—´çš„å˜åŒ–è€ŒåŠ¨æ€è°ƒæ•´ã€‚é€šè¿‡åˆ©ç”¨é“¾å¼æ³•åˆ™ (chain rule) å¯¹ç¥ç»ç½‘ç»œå’Œé«˜æ–¯è¿‡ç¨‹è¿›è¡Œè”åˆè®­ç»ƒï¼Œè¯¥æ–¹æ³•ä¸ä»…æè¿°äº†éå¹³ç¨³å‚æ•°çš„å…·ä½“è¡Œä¸ºï¼Œè¿˜èƒ½å¤Ÿé€šè¿‡è¿‘ä¼¼æ–¹æ³•æ‰©å±•è‡³å¤§è§„æ¨¡æ•°æ®é›†ã€‚è¯¥ç ”ç©¶åŸºäº GPyTorch åº“å®ç°ï¼Œå…·æœ‰æé«˜çš„çµæ´»æ€§ï¼Œå¯è½»æ¾é€‚é…ä¸åŒçš„éå¹³ç¨³æ ¸è€Œæ— éœ€é‡æ–°è®¾è®¡ä¼˜åŒ–æµç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæœºå™¨å­¦ä¹ æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡å’Œå¯¹æ•°å¾—åˆ† (log-score) å‡ä¼˜äºå¹³ç¨³æ¨¡å‹ä»¥åŠåŸºäºå˜åˆ†æ¨ç† (variational inference) çš„å±‚çº§æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨ç©ºé—´æ•°æ®é›† (spatial dataset) ä¸­å±•ç°äº†å‡ºè‰²çš„éå¹³ç¨³å‚æ•°æ¢å¤èƒ½åŠ›ï¼Œä¸ºéå¹³ç¨³æ•°æ®çš„å»ºæ¨¡æä¾›äº†å…¼å…·çµæ´»æ€§ä¸å¯æ‰©å±•æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12262v1",
      "published_date": "2025-07-16 14:09:49 UTC",
      "updated_date": "2025-07-16 14:09:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:04:52.399258+00:00"
    },
    {
      "arxiv_id": "2507.12261v1",
      "title": "Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes",
      "title_zh": "Infhernoï¼šåŸºäºæ™ºèƒ½ä½“çš„è‡ªç”±æ ¼å¼ä¸´åºŠç¬”è®°ç«¯åˆ°ç«¯ FHIR èµ„æºåˆæˆ",
      "authors": [
        "Johann Frei",
        "Nils Feldhus",
        "Lisa Raithel",
        "Roland Roller",
        "Alexander Meyer",
        "Frank Kramer"
      ],
      "abstract": "For clinical data integration and healthcare services, the HL7 FHIR standard has established itself as a desirable format for interoperability between complex health data. Previous attempts at automating the translation from free-form clinical notes into structured FHIR resources rely on modular, rule-based systems or LLMs with instruction tuning and constrained decoding. Since they frequently suffer from limited generalizability and structural inconformity, we propose an end-to-end framework powered by LLM agents, code execution, and healthcare terminology database tools to address these issues. Our solution, called Infherno, is designed to adhere to the FHIR document schema and competes well with a human baseline in predicting FHIR resources from unstructured text. The implementation features a front end for custom and synthetic data and both local and proprietary models, supporting clinical data integration processes and interoperability across institutions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Infhernoï¼Œä¸€ä¸ªåŸºäº LLM agents çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œæ—¨åœ¨å°†éç»“æ„åŒ–çš„è‡ªç”±æ–‡æœ¬ä¸´åºŠç¬”è®° (free-form clinical notes) è½¬åŒ–ä¸ºç¬¦åˆ HL7 FHIR æ ‡å‡†çš„ç»“æ„åŒ–èµ„æºã€‚é’ˆå¯¹ç°æœ‰è§„åˆ™ç³»ç»Ÿæˆ–æŒ‡ä»¤å¾®è°ƒæ¨¡å‹åœ¨æ³›åŒ–èƒ½åŠ›å’Œç»“æ„ä¸€è‡´æ€§æ–¹é¢çš„å±€é™ï¼ŒInfherno æ•´åˆäº†ä»£ç æ‰§è¡Œ (code execution) å’ŒåŒ»ç–—æœ¯è¯­æ•°æ®åº“å·¥å…· (healthcare terminology database tools) æ¥ç¡®ä¿æ•°æ®åˆæˆçš„å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶ä¸¥æ ¼éµå¾ª FHIR document schemaï¼Œåœ¨ä»éç»“æ„åŒ–æ–‡æœ¬é¢„æµ‹ FHIR èµ„æºçš„æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…¶å®åŠ›å¯ä¸äººç±»åŸºå‡† (human baseline) ç›¸åª²ç¾ã€‚æ­¤å¤–ï¼ŒInfherno æ”¯æŒæœ¬åœ°å’Œå•†ä¸šæ¨¡å‹ä»¥åŠè‡ªå®šä¹‰æ•°æ®çš„é›†æˆï¼Œä¸ºåŒ»ç–—æœºæ„é—´çš„ä¸´åºŠæ•°æ®é›†æˆä¸äº’æ“ä½œæ€§ (interoperability) æä¾›äº†é«˜æ•ˆçš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Submitted to EMNLP 2025 System Demonstrations | Code: https://github.com/j-frei/Infherno | Video: https://www.youtube.com/watch?v=kyj5C2ivbMw | Demo: https://infherno.misit-augsburg.de | HuggingFace Spaces: https://huggingface.co/spaces/nfel/infherno",
      "pdf_url": "https://arxiv.org/pdf/2507.12261v1",
      "published_date": "2025-07-16 14:06:51 UTC",
      "updated_date": "2025-07-16 14:06:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:05:19.296139+00:00"
    },
    {
      "arxiv_id": "2507.12252v1",
      "title": "Improving Contextual ASR via Multi-grained Fusion with Large Language Models",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹å¤šç²’åº¦èåˆçš„ä¸Šä¸‹æ–‡ ASR æ”¹è¿›",
      "authors": [
        "Shilin Zhou",
        "Zhenghua Li"
      ],
      "abstract": "While end-to-end Automatic Speech Recognition (ASR) models have shown impressive performance in transcribing general speech, they often struggle to accurately recognize contextually relevant keywords, such as proper nouns or user-specific entities.\n  Previous approaches have explored leveraging keyword dictionaries in the textual modality to improve keyword recognition, either through token-level fusion that guides token-by-token generation or phrase-level fusion that enables direct copying of keyword phrases.\n  However, these methods operate at different granularities and have their own limitations.\n  In this paper, we propose a novel multi-grained fusion approach that jointly leverages the strengths of both token-level and phrase-level fusion with Large Language Models (LLMs).\n  Our approach incorporates a late-fusion strategy that elegantly combines ASR's acoustic information with LLM's rich contextual knowledge, balancing fine-grained token precision with holistic phrase-level understanding.\n  Experiments on Chinese and English datasets demonstrate that our approach achieves state-of-the-art performance on keyword-related metrics while preserving high accuracy on non-keyword text.\n  Ablation studies further confirm that the token-level and phrase-level components both contribute significantly to the performance gains, complementing each other in our joint multi-grained framework.\n  The code and models will be publicly available at https://github.com/.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç«¯åˆ°ç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)åœ¨è¯†åˆ«ä¸“æœ‰åè¯ç­‰ç‰¹å®šå…³é”®è¯æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å¤šç²’åº¦èåˆ(multi-grained fusion)æ–¹æ³•ã€‚é’ˆå¯¹ä»¥å¾€token-levelæˆ–phrase-levelèåˆæ–¹å¼çš„å„è‡ªå±€é™ï¼Œè¯¥æ¡†æ¶é€šè¿‡åæœŸèåˆ(late-fusion)ç­–ç•¥å°†ASRçš„å£°å­¦ä¿¡æ¯ä¸LLMä¸°å¯Œçš„ä¸Šä¸‹æ–‡çŸ¥è¯†è¿›è¡Œæ•´åˆã€‚è¯¥æ–¹æ³•æœ‰æ•ˆå¹³è¡¡äº†ç»†ç²’åº¦çš„tokenç²¾ç¡®åº¦ä¸æ•´ä½“çš„phrase-levelç†è§£èƒ½åŠ›ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹ä¸Šä¸‹æ–‡ç›¸å…³å®ä½“çš„è¯†åˆ«ç²¾åº¦ã€‚åœ¨ä¸­æ–‡å’Œè‹±æ–‡æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨å…³é”®è¯ç›¸å…³æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†State-of-the-artæ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†éå…³é”®è¯æ–‡æœ¬çš„é«˜å‡†ç¡®ç‡ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®ï¼Œtoken-levelå’Œphrase-levelç»„ä»¶åœ¨å¤šç²’åº¦æ¡†æ¶ä¸­å…·æœ‰æ˜¾è‘—çš„äº’è¡¥ä½œç”¨ï¼Œå…±åŒæ¨åŠ¨äº†æ•´ä½“æ€§èƒ½çš„æå‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12252v1",
      "published_date": "2025-07-16 13:59:32 UTC",
      "updated_date": "2025-07-16 13:59:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:05:24.088538+00:00"
    },
    {
      "arxiv_id": "2507.12242v1",
      "title": "Looking for Fairness in Recommender Systems",
      "title_zh": "æ¢å¯»æ¨èç³»ç»Ÿä¸­çš„å…¬å¹³æ€§",
      "authors": [
        "CÃ©cile LogÃ©"
      ],
      "abstract": "Recommender systems can be found everywhere today, shaping our everyday experience whenever we're consuming content, ordering food, buying groceries online, or even just reading the news. Let's imagine we're in the process of building a recommender system to make content suggestions to users on social media. When thinking about fairness, it becomes clear there are several perspectives to consider: the users asking for tailored suggestions, the content creators hoping for some limelight, and society at large, navigating the repercussions of algorithmic recommendations. A shared fairness concern across all three is the emergence of filter bubbles, a side-effect that takes place when recommender systems are almost \"too good\", making recommendations so tailored that users become inadvertently confined to a narrow set of opinions/themes and isolated from alternative ideas. From the user's perspective, this is akin to manipulation. From the small content creator's perspective, this is an obstacle preventing them access to a whole range of potential fans. From society's perspective, the potential consequences are far-reaching, influencing collective opinions, social behavior and political decisions. How can our recommender system be fine-tuned to avoid the creation of filter bubbles, and ensure a more inclusive and diverse content landscape? Approaching this problem involves defining one (or more) performance metric to represent diversity, and tweaking our recommender system's performance through the lens of fairness. By incorporating this metric into our evaluation framework, we aim to strike a balance between personalized recommendations and the broader societal goal of fostering rich and varied cultures and points of view.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ¨èç³»ç»Ÿ(Recommender Systems)åœ¨ä¸ªæ€§åŒ–æ¨èè¿‡ç¨‹ä¸­é¢ä¸´çš„å…¬å¹³æ€§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯â€œè¿‡æ»¤æ°”æ³¡â€(Filter Bubbles)å¯¹ç”¨æˆ·ã€å†…å®¹åˆ›ä½œè€…åŠç¤¾ä¼šäº§ç”Ÿçš„æ·±è¿œå½±å“ã€‚æ–‡ä¸­æŒ‡å‡ºï¼Œè™½ç„¶ç®—æ³•èƒ½æä¾›é«˜åº¦å®šåˆ¶åŒ–çš„å»ºè®®ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´ç”¨æˆ·é™·å…¥ç‹­éš˜çš„ä¿¡æ¯èŒ§æˆ¿ï¼Œé˜»ç¢å°å‹åˆ›ä½œè€…è·å¾—æ›å…‰ï¼Œç”šè‡³å½±å“ç¤¾ä¼šçš„é›†ä½“èˆ†è®ºå’Œæ”¿æ²»å†³ç­–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶æå‡ºé€šè¿‡å®šä¹‰è¡¨å¾å¤šæ ·æ€§(Diversity)çš„æ€§èƒ½æŒ‡æ ‡ï¼Œå¹¶ä»å…¬å¹³æ€§çš„è§†è§’å¯¹æ¨èç³»ç»Ÿè¿›è¡Œå¾®è°ƒã€‚é€šè¿‡å°†è¿™äº›æŒ‡æ ‡æ•´åˆè¿›è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨å¹³è¡¡ä¸ªæ€§åŒ–éœ€æ±‚ä¸ä¿ƒè¿›å†…å®¹åŒ…å®¹æ€§åŠæ–‡åŒ–å¤šæ ·æ€§ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œæ„å»ºä¸€ä¸ªæ›´åŠ å¥åº·å’Œå¤šå…ƒçš„ä¿¡æ¯ç”Ÿæ€ç³»ç»Ÿã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12242v1",
      "published_date": "2025-07-16 13:53:02 UTC",
      "updated_date": "2025-07-16 13:53:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:05:23.897654+00:00"
    },
    {
      "arxiv_id": "2507.12215v2",
      "title": "Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning",
      "title_zh": "Xiangqi-R1ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨è±¡æ£‹ä¸­çš„ç©ºé—´æˆ˜ç•¥æ¨ç†èƒ½åŠ›",
      "authors": [
        "Yuhao Chen",
        "Shuochen Liu",
        "Yuanjie Lyu",
        "Chao Zhang",
        "Jiayao Shi",
        "Tong Xu"
      ],
      "abstract": "Game playing has long served as a fundamental benchmark for evaluating Artificial General Intelligence. While Large Language Models (LLMs) have demonstrated impressive capabilities in general reasoning, their effectiveness in spatial strategic reasoning, which is critical for complex and fully observable board games, remains insufficiently explored. In this work, we adopt Chinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate rules and spatial complexity. To advance LLMs' strategic competence in such environments, we propose a training framework tailored to Xiangqi, built upon a large-scale dataset of five million board-move pairs enhanced with expert annotations and engine evaluations. Building on this foundation, we introduce Xiangqi-R1, a 7B-parameter model trained in multi-stage manner. Our Experimental results indicate that, despite their size and power, general-purpose LLMs struggle to achieve satisfactory performance in these tasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an 18% rise in move legality and a 22% boost in analysis accuracy. Our results point to a promising path for creating general strategic intelligence in complex areas.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Xiangqi-R1ï¼Œä¸€ä¸ªå‚æ•°é‡ä¸º7Bçš„æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å¢å¼ºå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è±¡æ£‹(Xiangqi)ä¸­çš„ç©ºé—´ç­–ç•¥æ¨ç†(Spatial Strategic Reasoning)èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨åŒ…å«äº”ç™¾ä¸‡ä¸ªæ£‹ç›˜èµ°æ³•å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶ç»“åˆä¸“å®¶æ ‡æ³¨ä¸å¼•æ“è¯„ä¼°ï¼Œæ„å»ºäº†ä¸“ä¸ºè±¡æ£‹å®šåˆ¶çš„å¤šé˜¶æ®µè®­ç»ƒæ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šç”¨å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ­¤ç±»å…·æœ‰å¤æ‚è§„åˆ™å’Œç©ºé—´å¤æ‚æ€§çš„ä»»åŠ¡ä¸­è¡¨ç°æœ‰é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒXiangqi-R1åœ¨èµ°æ³•åˆæ³•æ€§(Move Legality)æ–¹é¢æå‡äº†18%ï¼Œåˆ†æå‡†ç¡®ç‡(Analysis Accuracy)æå‡äº†22%ã€‚è¯¥é¡¹å·¥ä½œå±•ç¤ºäº†åœ¨å¤æ‚é¢†åŸŸæ„å»ºé€šç”¨ç­–ç•¥æ™ºèƒ½çš„æœ‰æ•ˆè·¯å¾„ï¼Œè¯æ˜äº†ä¸“ç”¨æ•°æ®é›†ä¸é’ˆå¯¹æ€§è®­ç»ƒå¯¹æå‡æ¨¡å‹å¤æ‚å†³ç­–èƒ½åŠ›çš„æ˜¾è‘—ä½œç”¨ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.12215v2",
      "published_date": "2025-07-16 13:19:46 UTC",
      "updated_date": "2025-11-18 03:21:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:05:26.408066+00:00"
    },
    {
      "arxiv_id": "2507.12212v1",
      "title": "Draw an Ugly Person An Exploration of Generative AIs Perceptions of Ugliness",
      "title_zh": "â€œç”»ä¸€ä¸ªä¸‘äººâ€ï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¯¹â€œä¸‘é™‹â€è®¤çŸ¥çš„æ¢ç´¢",
      "authors": [
        "Garyoung Kim",
        "Huisung Kwon",
        "Seoju Yun",
        "Yu-Won Youn"
      ],
      "abstract": "Generative AI does not only replicate human creativity but also reproduces deep-seated cultural biases, making it crucial to critically examine how concepts like ugliness are understood and expressed by these tools. This study investigates how four different generative AI models understand and express ugliness through text and image and explores the biases embedded within these representations. We extracted 13 adjectives associated with ugliness through iterative prompting of a large language model and generated 624 images across four AI models and three prompts. Demographic and socioeconomic attributes within the images were independently coded and thematically analyzed. Our findings show that AI models disproportionately associate ugliness with old white male figures, reflecting entrenched social biases as well as paradoxical biases, where efforts to avoid stereotypical depictions of marginalized groups inadvertently result in the disproportionate projection of negative attributes onto majority groups. Qualitative analysis further reveals that, despite supposed attempts to frame ugliness within social contexts, conventional physical markers such as asymmetry and aging persist as central visual motifs. These findings demonstrate that despite attempts to create more equal representations, generative AI continues to perpetuate inherited and paradoxical biases, underscoring the critical work being done to create ethical AI training paradigms and advance methodologies for more inclusive AI development.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) å¦‚ä½•é€šè¿‡æ–‡æœ¬å’Œå›¾åƒç†è§£å¹¶è¡¨è¾¾â€œä¸‘é™‹â€æ¦‚å¿µï¼Œä»¥åŠå…¶ä¸­åµŒå…¥çš„æ–‡åŒ–åè§ã€‚ç ”ç©¶è€…é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) æå–äº†13ä¸ªç›¸å…³å½¢å®¹è¯ï¼Œå¹¶åˆ©ç”¨å››ç§ AI æ¨¡å‹ç”Ÿæˆäº†624å¼ å›¾åƒï¼Œå¯¹å…¶ä¸­çš„äººå£ç»Ÿè®¡å­¦å’Œç¤¾ä¼šç»æµå±æ€§è¿›è¡Œäº†å®šæ€§ä¸å®šé‡åˆ†æã€‚ç ”ç©¶å‘ç°ï¼ŒAI æ¨¡å‹ä¸æˆæ¯”ä¾‹åœ°å°†ä¸‘é™‹ä¸è€å¹´ç™½äººç”·æ€§è”ç³»åœ¨ä¸€èµ·ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨è¯•å›¾é¿å…è¾¹ç¼˜åŒ–ç¾¤ä½“åˆ»æ¿å°è±¡æ—¶äº§ç”Ÿçš„â€œæ‚–è®ºåè§â€ (paradoxical biases)ï¼Œå³å°†è´Ÿé¢å±æ€§è¿‡åº¦æŠ•å°„åˆ°äº†å¤šæ•°ç¾¤ä½“ã€‚å®šæ€§åˆ†ææ˜¾ç¤ºï¼Œå°½ç®¡ AI è¯•å›¾æä¾›å¤šå…ƒåŒ–è¡¨å¾ï¼Œä½†ä¸å¯¹ç§° (asymmetry) å’Œè¡°è€ (aging) ç­‰ä¼ ç»Ÿç”Ÿç†æ ‡è®°ä»æ˜¯å…¶è¡¨è¾¾ä¸‘é™‹çš„æ ¸å¿ƒè§†è§‰ç‰¹å¾ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†ç”Ÿæˆå¼ AI åœ¨å»¶ç»­é—ä¼ åè§æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶æŒ‡å‡ºäº†å»ºç«‹ä¼¦ç† AI è®­ç»ƒèŒƒå¼å’Œå¼€å‘åŒ…å®¹æ€§æŠ€æœ¯çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "7 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.12212v1",
      "published_date": "2025-07-16 13:16:56 UTC",
      "updated_date": "2025-07-16 13:16:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:05:29.042235+00:00"
    },
    {
      "arxiv_id": "2507.12207v1",
      "title": "BuildEvo: Designing Building Energy Consumption Forecasting Heuristics via LLM-driven Evolution",
      "title_zh": "BuildEvoï¼šé€šè¿‡å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨æ¼”åŒ–è®¾è®¡å»ºç­‘èƒ½è€—é¢„æµ‹å¯å‘å¼æ–¹æ³•",
      "authors": [
        "Subin Lin",
        "Chuanbo Hua"
      ],
      "abstract": "Accurate building energy forecasting is essential, yet traditional heuristics often lack precision, while advanced models can be opaque and struggle with generalization by neglecting physical principles. This paper introduces BuildEvo, a novel framework that uses Large Language Models (LLMs) to automatically design effective and interpretable energy prediction heuristics. Within an evolutionary process, BuildEvo guides LLMs to construct and enhance heuristics by systematically incorporating physical insights from building characteristics and operational data (e.g., from the Building Data Genome Project 2). Evaluations show BuildEvo achieves state-of-the-art performance on benchmarks, offering improved generalization and transparent prediction logic. This work advances the automated design of robust, physically grounded heuristics, promoting trustworthy models for complex energy systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BuildEvoï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) è‡ªåŠ¨è®¾è®¡é«˜æ•ˆä¸”å…·å¯è§£é‡Šæ€§çš„å»ºç­‘èƒ½æºé¢„æµ‹å¯å‘å¼ (heuristics) æ–¹æ³•çš„æ–°å‹æ¡†æ¶ã€‚é’ˆå¯¹ä¼ ç»Ÿå¯å‘å¼æ–¹æ³•ç²¾åº¦ä¸è¶³ä»¥åŠé«˜çº§æ¨¡å‹å› å¿½è§†ç‰©ç†åŸåˆ™è€Œå¯¼è‡´çš„é»‘ç›’åŒ–ä¸æ³›åŒ–å›°éš¾ï¼ŒBuildEvo é€šè¿‡æ¼”åŒ–è¿‡ç¨‹ (evolutionary process) å¼•å¯¼ LLMs ç³»ç»Ÿåœ°æ•´åˆå»ºç­‘ç‰¹å¾ä¸è¿è¡Œæ•°æ®ä¸­çš„ç‰©ç†è§è§£ã€‚ç ”ç©¶åˆ©ç”¨ Building Data Genome Project 2 æ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤º BuildEvo åœ¨åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„ (SOTA) æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å¹¶æä¾›äº†é€æ˜çš„é¢„æµ‹é€»è¾‘ã€‚è¿™é¡¹å·¥ä½œæ¨åŠ¨äº†ç‰©ç†é©±åŠ¨çš„ç¨³å¥å¯å‘å¼æ–¹æ³•è‡ªåŠ¨åŒ–è®¾è®¡ï¼Œä¸ºå¤æ‚èƒ½æºç³»ç»Ÿå¼€å‘å¯ä¿¡é¢„æµ‹æ¨¡å‹æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "ICML 2025 CO-Build Workshop Poster",
      "pdf_url": "https://arxiv.org/pdf/2507.12207v1",
      "published_date": "2025-07-16 13:07:24 UTC",
      "updated_date": "2025-07-16 13:07:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:05:37.616267+00:00"
    },
    {
      "arxiv_id": "2507.12202v1",
      "title": "Sparse Autoencoders for Sequential Recommendation Models: Interpretation and Flexible Control",
      "title_zh": "åºåˆ—æ¨èæ¨¡å‹ä¸­çš„ç¨€ç–è‡ªç¼–ç å™¨ï¼šè§£é‡Šä¸çµæ´»æ§åˆ¶",
      "authors": [
        "Anton Klenitskiy",
        "Konstantin Polev",
        "Daria Denisova",
        "Alexey Vasilev",
        "Dmitry Simakov",
        "Gleb Gusev"
      ],
      "abstract": "Many current state-of-the-art models for sequential recommendations are based on transformer architectures. Interpretation and explanation of such black box models is an important research question, as a better understanding of their internals can help understand, influence, and control their behavior, which is very important in a variety of real-world applications. Recently sparse autoencoders (SAE) have been shown to be a promising unsupervised approach for extracting interpretable features from language models. These autoencoders learn to reconstruct hidden states of the transformer's internal layers from sparse linear combinations of directions in their activation space.\n  This paper is focused on the application of SAE to the sequential recommendation domain. We show that this approach can be successfully applied to the transformer trained on a sequential recommendation task: learned directions turn out to be more interpretable and monosemantic than the original hidden state dimensions. Moreover, we demonstrate that the features learned by SAE can be used to effectively and flexibly control the model's behavior, providing end-users with a straightforward method to adjust their recommendations to different custom scenarios and contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨åºåˆ—æ¨èæ¨¡å‹ï¼ˆSequential Recommendation Modelsï¼‰ä¸­åº”ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSparse Autoencoders, SAEï¼‰çš„å¯è¡Œæ€§ï¼Œæ—¨åœ¨è§£å†³åŸºäºTransformeræ¶æ„çš„é»‘ç›’æ¨¡å‹éš¾ä»¥è§£é‡Šå’Œæ§åˆ¶çš„é—®é¢˜ã€‚ç ”ç©¶äººå‘˜é€šè¿‡SAEé‡æ„Transformerå†…éƒ¨å±‚çš„éšè—çŠ¶æ€ï¼Œä»ä¸­æå–ç¨€ç–çš„çº¿æ€§ç‰¹å¾æ–¹å‘ã€‚å®éªŒè¯æ˜ï¼ŒSAEåœ¨åºåˆ—æ¨èä»»åŠ¡ä¸­å­¦ä¹ åˆ°çš„ç‰¹å¾æ–¹å‘æ¯”åŸå§‹éšè—çŠ¶æ€ç»´åº¦æ›´å…·å¯è§£é‡Šæ€§ï¼ˆinterpretableï¼‰å’Œå•è¯­ä¹‰æ€§ï¼ˆmonosemanticï¼‰ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†åˆ©ç”¨è¿™äº›ç‰¹å¾å¯ä»¥å®ç°å¯¹æ¨¡å‹è¡Œä¸ºçš„çµæ´»æ§åˆ¶ã€‚è¿™ä¸ºæœ€ç»ˆç”¨æˆ·æä¾›äº†ä¸€ç§ç›´æ¥ä¸”æœ‰æ•ˆçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿé’ˆå¯¹ä¸åŒçš„è‡ªå®šä¹‰åœºæ™¯å’Œä¸Šä¸‹æ–‡å®æ—¶è°ƒæ•´æ¨èç»“æœã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12202v1",
      "published_date": "2025-07-16 12:57:43 UTC",
      "updated_date": "2025-07-16 12:57:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:05:37.239524+00:00"
    },
    {
      "arxiv_id": "2507.14223v1",
      "title": "Multi-Granular Discretization for Interpretable Generalization in Precise Cyberattack Identification",
      "title_zh": "é¢å‘ç²¾ç¡®ç½‘ç»œæ”»å‡»è¯†åˆ«ä¸­å¯è§£é‡Šæ€§æ³›åŒ–çš„å¤šç²’åº¦ç¦»æ•£åŒ–",
      "authors": [
        "Wen-Cheng Chung",
        "Shu-Ting Huang",
        "Hao-Ting Pai"
      ],
      "abstract": "Explainable intrusion detection systems (IDS) are now recognized as essential for mission-critical networks, yet most \"XAI\" pipelines still bolt an approximate explainer onto an opaque classifier, leaving analysts with partial and sometimes misleading insights. The Interpretable Generalization (IG) mechanism, published in IEEE Transactions on Information Forensics and Security, eliminates that bottleneck by learning coherent patterns - feature combinations unique to benign or malicious traffic - and turning them into fully auditable rules. IG already delivers outstanding precision, recall, and AUC on NSL-KDD, UNSW-NB15, and UKM-IDS20, even when trained on only 10% of the data. To raise precision further without sacrificing transparency, we introduce Multi-Granular Discretization (IG-MD), which represents every continuous feature at several Gaussian-based resolutions. On UKM-IDS20, IG-MD lifts precision by greater than or equal to 4 percentage points across all nine train-test splits while preserving recall approximately equal to 1.0, demonstrating that a single interpretation-ready model can scale across domains without bespoke tuning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Multi-Granular Discretization (IG-MD)ï¼Œæ—¨åœ¨æå‡å…¥ä¾µæ£€æµ‹ç³»ç»Ÿ(IDS)çš„ç²¾ç¡®åº¦å¹¶ç¡®ä¿å…¶é€æ˜åº¦ï¼Œè§£å†³äº†ä¼ ç»Ÿå¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)ä¸­è§£é‡Šå™¨ä¸åˆ†ç±»å™¨åˆ†ç¦»å¯¼è‡´çš„åˆ†æåå·®é—®é¢˜ã€‚è¯¥æ–¹æ³•åŸºäºå¯è§£é‡Šæ³›åŒ–(Interpretable Generalization, IG)æœºåˆ¶ï¼Œé€šè¿‡å°†ç‹¬ç‰¹çš„æµé‡ç‰¹å¾ç»„åˆè½¬åŒ–ä¸ºå®Œå…¨å¯å®¡è®¡çš„è§„åˆ™æ¥ä¼˜åŒ–è¯†åˆ«è¿‡ç¨‹ã€‚IG-MDå¼•å…¥äº†åŸºäºé«˜æ–¯åˆ†å¸ƒ(Gaussian-based)çš„å¤šåˆ†è¾¨ç‡ç¦»æ•£åŒ–æŠ€æœ¯ï¼Œä»¥å¤šä¸ªç²’åº¦è¡¨ç¤ºè¿ç»­ç‰¹å¾ï¼Œä»è€Œåœ¨ä¸ç‰ºç‰²æ¨¡å‹è§£é‡Šæ€§çš„å‰æä¸‹å¢å¼ºäº†æ¨¡å¼æå–èƒ½åŠ›ã€‚åœ¨UKM-IDS20æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIG-MDåœ¨ä¿æŒå¬å›ç‡(recall)çº¦ä¸º1.0çš„åŒæ—¶ï¼Œå°†ç²¾ç¡®åº¦(precision)æå‡äº†4ä¸ªç™¾åˆ†ç‚¹ä»¥ä¸Šã€‚è¯¥ç ”ç©¶è¯æ˜äº†å•ä¸€çš„å¯è§£é‡Šæ¨¡å‹å…·å¤‡å¼ºå¤§çš„è·¨é¢†åŸŸæ‰©å±•èƒ½åŠ›ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šç¯å¢ƒè¿›è¡Œå®šåˆ¶å¾®è°ƒå³å¯å®ç°ç²¾ç¡®ä¸”å¯ä¿¡çš„ç½‘ç»œæ”»å‡»è¯†åˆ«ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "ACM CCS 2025 (Submitted)",
      "pdf_url": "https://arxiv.org/pdf/2507.14223v1",
      "published_date": "2025-07-16 12:57:38 UTC",
      "updated_date": "2025-07-16 12:57:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:05:46.740763+00:00"
    },
    {
      "arxiv_id": "2507.12197v1",
      "title": "Quantize More, Lose Less: Autoregressive Generation from Residually Quantized Speech Representations",
      "title_zh": "é‡åŒ–è¶Šå¤šï¼ŒæŸè€—è¶Šå°‘ï¼šåŸºäºæ®‹å·®é‡åŒ–è¯­éŸ³è¡¨ç¤ºçš„è‡ªå›å½’ç”Ÿæˆ",
      "authors": [
        "Yichen Han",
        "Xiaoyang Hao",
        "Keming Chen",
        "Weibo Xiong",
        "Jun He",
        "Ruonan Zhang",
        "Junjie Cao",
        "Yue Liu",
        "Bowen Li",
        "Dongrui Zhang",
        "Hui Xia",
        "Huilei Fu",
        "Kai Jia",
        "Kaixuan Guo",
        "Mingli Jin",
        "Qingyun Meng",
        "Ruidong Ma",
        "Ruiqian Fang",
        "Shaotong Guo",
        "Xuhui Li",
        "Yang Xiang",
        "Ying Zhang",
        "Yulong Liu",
        "Yunfeng Li",
        "Yuyi Zhang",
        "Yuze Zhou",
        "Zhen Wang",
        "Zhaowen Chen"
      ],
      "abstract": "Text-to-speech (TTS) synthesis has seen renewed progress under the discrete modeling paradigm. Existing autoregressive approaches often rely on single-codebook representations, which suffer from significant information loss. Even with post-hoc refinement techniques such as flow matching, these methods fail to recover fine-grained details (e.g., prosodic nuances, speaker-specific timbres), especially in challenging scenarios like singing voice or music synthesis. We propose QTTS, a novel TTS framework built upon our new audio codec, QDAC. The core innovation of QDAC lies in its end-to-end training of an ASR-based auto-regressive network with a GAN, which achieves superior semantic feature disentanglement for scalable, near-lossless compression. QTTS models these discrete codes using two innovative strategies: the Hierarchical Parallel architecture, which uses a dual-AR structure to model inter-codebook dependencies for higher-quality synthesis, and the Delay Multihead approach, which employs parallelized prediction with a fixed delay to accelerate inference speed. Our experiments demonstrate that the proposed framework achieves higher synthesis quality and better preserves expressive content compared to baseline. This suggests that scaling up compression via multi-codebook modeling is a promising direction for high-fidelity, general-purpose speech and audio generation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬è½¬è¯­éŸ³(TTS)ä¸­å•ç æœ¬(single-codebook)è¡¨ç¤ºå¯¼è‡´çš„ä¿¡æ¯ä¸¢å¤±ä»¥åŠç»†ç²’åº¦ç»†èŠ‚ï¼ˆå¦‚éŸµå¾‹å’ŒéŸ³è‰²ï¼‰ç¼ºå¤±é—®é¢˜ï¼Œæå‡ºäº†å…¨æ–°çš„è¯­éŸ³åˆæˆæ¡†æ¶ QTTSã€‚è¯¥æ¡†æ¶åŸºäºå…¶å¼€å‘çš„æ–°å‹éŸ³é¢‘ç¼–è§£ç å™¨ QDACï¼Œé€šè¿‡ç«¯åˆ°ç«¯è®­ç»ƒ ASR-based AR ç½‘ç»œä¸ GANï¼Œå®ç°äº†ä¼˜å¼‚çš„è¯­ä¹‰ç‰¹å¾è§£è€¦å’Œè¿‘ä¹æ— æŸçš„éŸ³é¢‘å‹ç¼©ã€‚åœ¨å»ºæ¨¡ç­–ç•¥ä¸Šï¼ŒQTTS é‡‡ç”¨äº†åˆ†å±‚å¹¶è¡Œ(Hierarchical Parallel)æ¶æ„ï¼Œåˆ©ç”¨åŒè‡ªå›å½’(dual-AR)ç»“æ„æ•æ‰ç æœ¬é—´çš„ä¾èµ–å…³ç³»ä»¥æå‡åˆæˆè´¨é‡ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†å»¶è¿Ÿå¤šå¤´(Delay Multihead)æ–¹æ³•ï¼Œé€šè¿‡å¸¦å›ºå®šå»¶è¿Ÿçš„å¹¶è¡Œé¢„æµ‹æ˜¾è‘—åŠ å¿«äº†æ¨ç†é€Ÿåº¦ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨åˆæˆè´¨é‡å’Œè¡¨ç°åŠ›ä¿ç•™æ–¹é¢å‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œä¸ºé«˜ä¿çœŸã€é€šç”¨è¯­éŸ³åŠéŸ³é¢‘ç”Ÿæˆæä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚è¿™è¡¨æ˜é€šè¿‡å¤šç æœ¬å»ºæ¨¡(multi-codebook modeling)æ‰©å±•å‹ç¼©è§„æ¨¡æ˜¯è§£å†³å¤æ‚éŸ³é¢‘åˆæˆä»»åŠ¡çš„æå…·å‰æ™¯çš„æ–¹å‘ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12197v1",
      "published_date": "2025-07-16 12:47:09 UTC",
      "updated_date": "2025-07-16 12:47:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:05:44.922742+00:00"
    },
    {
      "arxiv_id": "2507.12195v1",
      "title": "Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision",
      "title_zh": "é‡ç°å¤éŸµï¼šåŸºäºè®¡ç®—æœºè§†è§‰çš„å¯ºåº™ç –ç“¦æ•°å­—åŒ–é‡å»º",
      "authors": [
        "Arkaprabha Basu"
      ],
      "abstract": "Modern digitised approaches have dramatically changed the preservation and restoration of cultural treasures, integrating computer scientists into multidisciplinary projects with ease. Machine learning, deep learning, and computer vision techniques have revolutionised developing sectors like 3D reconstruction, picture inpainting,IoT-based methods, genetic algorithms, and image processing with the integration of computer scientists into multidisciplinary initiatives. We suggest three cutting-edge techniques in recognition of the special qualities of Indian monuments, which are famous for their architectural skill and aesthetic appeal. First is the Fractal Convolution methodology, a segmentation method based on image processing that successfully reveals subtle architectural patterns within these irreplaceable cultural buildings. The second is a revolutionary Self-Sensitive Tile Filling (SSTF) method created especially for West Bengal's mesmerising Bankura Terracotta Temples with a brand-new data augmentation method called MosaicSlice on the third. Furthermore, we delve deeper into the Super Resolution strategy to upscale the images without losing significant amount of quality. Our methods allow for the development of seamless region-filling and highly detailed tiles while maintaining authenticity using a novel data augmentation strategy within affordable costs introducing automation. By providing effective solutions that preserve the delicate balance between tradition and innovation, this study improves the subject and eventually ensures unrivalled efficiency and aesthetic excellence in cultural heritage protection. The suggested approaches advance the field into an era of unmatched efficiency and aesthetic quality while carefully upholding the delicate equilibrium between tradition and innovation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨è®¡ç®—æœºè§†è§‰æŠ€æœ¯å¯¹å¤ä»£å¯ºåº™ç“·ç –è¿›è¡Œæ•°å­—åŒ–é‡å»ºä¸ä¿æŠ¤çš„ç»¼åˆæ¡†æ¶ã€‚ç ”ç©¶é¦–å…ˆå¼•å…¥äº† Fractal Convolution å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œç”¨äºç²¾ç¡®æå–æ–‡åŒ–å»ºç­‘ä¸­å¤æ‚çš„å»ºç­‘çº¹ç†ã€‚é’ˆå¯¹è¥¿å­ŸåŠ æ‹‰é‚¦ Bankura Terracotta Temples çš„ç‰¹æ®Šéœ€æ±‚ï¼Œè®ºæ–‡å¼€å‘äº† Self-Sensitive Tile Filling (SSTF) åŒºåŸŸå¡«å……æŠ€æœ¯ï¼Œå¹¶é…åˆä½¿ç”¨åä¸º MosaicSlice çš„æ–°å‹æ•°æ®å¢å¼ºç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆè¿˜é›†æˆäº† Super Resolution ç­–ç•¥ä»¥æå‡å›¾åƒåˆ†è¾¨ç‡ï¼Œç¡®ä¿é‡å»ºè¿‡ç¨‹åœ¨ä½æˆæœ¬å’Œè‡ªåŠ¨åŒ–å‰æä¸‹ä¸æŸå¤±ç»†èŠ‚è´¨é‡ã€‚è¿™äº›æŠ€æœ¯çš„ç»“åˆå®ç°äº†ç“·ç –çš„é«˜ç²¾åº¦ã€æ— ç¼å¡«å……ï¼Œåœ¨ä¿æŒæ–‡åŒ–é—äº§çœŸå®æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†æ–‡ç‰©ä¿®å¤ä¸ä¿æŠ¤çš„æ•ˆç‡å’Œç¾å­¦è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12195v1",
      "published_date": "2025-07-16 12:46:04 UTC",
      "updated_date": "2025-07-16 12:46:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:05:53.464738+00:00"
    },
    {
      "arxiv_id": "2507.12196v1",
      "title": "Selective Quantization Tuning for ONNX Models",
      "title_zh": "é’ˆå¯¹ ONNX æ¨¡å‹çš„é€‰æ‹©æ€§é‡åŒ–è°ƒä¼˜",
      "authors": [
        "Nikolaos Louloudakis",
        "Ajitha Rajan"
      ],
      "abstract": "Quantization is a process that reduces the precision of deep neural network models to lower model size and computational demands, often at the cost of accuracy. However, fully quantized models may exhibit sub-optimal performance below acceptable levels and face deployment challenges on low-end hardware accelerators due to practical constraints. To address these issues, quantization can be selectively applied to only a subset of layers, but selecting which layers to exclude is non-trivial. To this direction, we propose TuneQn, a suite enabling selective quantization, deployment and execution of ONNX models across various CPU and GPU devices, combined with profiling and multi-objective optimization. TuneQn generates selectively quantized ONNX models, deploys them on different hardware, measures performance on metrics like accuracy and size, performs Pareto Front minimization to identify the best model candidate and visualizes the results. To demonstrate the effectiveness of TuneQn, we evaluated TuneQn on four ONNX models with two quantization settings across CPU and GPU devices. As a result, we demonstrated that our utility effectively performs selective quantization and tuning, selecting ONNX model candidates with up to a $54.14$% reduction in accuracy loss compared to the fully quantized model, and up to a $72.9$% model size reduction compared to the original model.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å…¨é‡åŒ–è¿‡ç¨‹ä¸­é¢ä¸´çš„ç²¾åº¦ä¸‹é™å’Œç¡¬ä»¶éƒ¨ç½²é™åˆ¶ï¼Œæå‡ºäº†åä¸ºTuneQnçš„è‡ªåŠ¨åŒ–å·¥å…·å¥—ä»¶ï¼Œæ—¨åœ¨å®ç°ONNXæ¨¡å‹çš„é€‰æ‹©æ€§é‡åŒ–(Selective Quantization)ä¸è°ƒä¼˜ã€‚TuneQnèƒ½å¤Ÿè·¨å¤šç§CPUå’ŒGPUè®¾å¤‡è¿›è¡Œæ¨¡å‹éƒ¨ç½²å’Œæ€§èƒ½åˆ†æï¼Œå¹¶ç»“åˆå¤šç›®æ ‡ä¼˜åŒ–(Multi-objective Optimization)ä¸å¸•ç´¯æ‰˜å‰æ²¿(Pareto Front)æœ€å°åŒ–ç®—æ³•ï¼Œè‡ªåŠ¨è¯†åˆ«å‡ºæœ€ä¼˜çš„é‡åŒ–å±‚å­é›†ã€‚é€šè¿‡è¯„ä¼°ç²¾åº¦ã€æ¨¡å‹ä½“ç§¯ç­‰æ ¸å¿ƒæŒ‡æ ‡ï¼Œè¯¥æ¡†æ¶ååŠ©å¼€å‘è€…åœ¨è®¡ç®—èµ„æºå—é™çš„æƒ…å†µä¸‹å¯»æ‰¾æ€§èƒ½å¹³è¡¡ç‚¹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç›¸æ¯”å…¨é‡åŒ–æ¨¡å‹ï¼ŒTuneQné€‰å‡ºçš„æ¨¡å‹å€™é€‰è€…å¯å‡å°‘é«˜è¾¾54.14%çš„ç²¾åº¦æŸå¤±ï¼Œå¹¶è¾ƒåŸå§‹æ¨¡å‹å®ç°äº†72.9%çš„ä½“ç§¯å‹ç¼©ã€‚è¿™ä¸€æˆæœä¸ºåœ¨ä½ç«¯ç¡¬ä»¶åŠ é€Ÿå™¨ä¸Šé«˜æ•ˆã€ç²¾å‡†åœ°éƒ¨ç½²æ·±åº¦å­¦ä¹ æ¨¡å‹æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 3 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2507.12196v1",
      "published_date": "2025-07-16 12:46:04 UTC",
      "updated_date": "2025-07-16 12:46:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:06:12.849163+00:00"
    },
    {
      "arxiv_id": "2507.12189v2",
      "title": "BenchRL-QAS: Benchmarking reinforcement learning algorithms for quantum architecture search",
      "title_zh": "BenchRL-QASï¼šé¢å‘é‡å­æ¶æ„æœç´¢çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•åŸºå‡†æµ‹è¯•",
      "authors": [
        "Azhar Ikhtiarudin",
        "Aditi Das",
        "Param Thakkar",
        "Akash Kundu"
      ],
      "abstract": "We present BenchRL-QAS, a unified benchmarking framework for reinforcement learning (RL) in quantum architecture search (QAS) across a spectrum of variational quantum algorithm tasks on 2- to 8-qubit systems. Our study systematically evaluates 9 different RL agents, including both value-based and policy-gradient methods, on quantum problems such as variational eigensolver, quantum state diagonalization, variational quantum classification (VQC), and state preparation, under both noiseless and noisy execution settings. To ensure fair comparison, we propose a weighted ranking metric that integrates accuracy, circuit depth, gate count, and training time. Results demonstrate that no single RL method dominates universally, the performance dependents on task type, qubit count, and noise conditions providing strong evidence of no free lunch principle in RL-QAS. As a byproduct we observe that a carefully chosen RL algorithm in RL-based VQC outperforms baseline VQCs. BenchRL-QAS establishes the most extensive benchmark for RL-based QAS to date, codes and experimental made publicly available for reproducibility and future advances.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BenchRL-QASï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é‡å­æ¶æ„æœç´¢ (Quantum Architecture Search, QAS) ä¸­å¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL) ç®—æ³•çš„ç»Ÿä¸€åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œæ¶µç›–äº† 2 åˆ° 8 ä¸ªé‡å­æ¯”ç‰¹ç³»ç»Ÿä¸Šçš„å¤šç§å˜åˆ†é‡å­ç®—æ³• (Variational Quantum Algorithm) ä»»åŠ¡ã€‚ç ”ç©¶ç³»ç»Ÿåœ°è¯„ä¼°äº†åŒ…æ‹¬åŸºäºå€¼å’Œç­–ç•¥æ¢¯åº¦æ–¹æ³•åœ¨å†…çš„ 9 ç§å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œå¹¶åœ¨æ— å™ªå£°å’Œæœ‰å™ªå£°ç¯å¢ƒä¸‹å¯¹å˜åˆ†é‡å­æœ¬å¾æ±‚è§£å™¨ã€é‡å­æ€å¯¹è§’åŒ–ã€å˜åˆ†é‡å­åˆ†ç±» (VQC) åŠæ€å‡†å¤‡ç­‰é—®é¢˜è¿›è¡Œäº†æ·±å…¥æµ‹è¯•ã€‚ä¸ºäº†å®ç°å…¬å¹³è¯„ä¼°ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†æ•´åˆå‡†ç¡®ç‡ã€ç”µè·¯æ·±åº¦ã€é—¨æ•°é‡å’Œè®­ç»ƒæ—¶é—´çš„åŠ æƒæ’åæŒ‡æ ‡ã€‚å®éªŒç»“æœæ­ç¤ºäº†æ²¡æœ‰å•ä¸€å¼ºåŒ–å­¦ä¹ ç®—æ³•èƒ½åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­èƒœå‡ºï¼Œå…¶æ€§èƒ½å—ä»»åŠ¡ç±»å‹ã€æ¯”ç‰¹æ•°å’Œå™ªå£°æ¡ä»¶çš„æ˜¾è‘—å½±å“ï¼Œæœ‰åŠ›éªŒè¯äº†è¯¥é¢†åŸŸä¸­çš„â€œæ²¡æœ‰å…è´¹åˆé¤â€ (No Free Lunch) åŸåˆ™ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯æ˜äº†é€šè¿‡ç²¾å¿ƒé€‰æ‹©çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨å˜åˆ†é‡å­åˆ†ç±»ä»»åŠ¡ä¸­èƒ½å¤Ÿè¶…è¶ŠåŸºå‡†æ¨¡å‹ã€‚ä½œä¸ºè¿„ä»Šä¸ºæ­¢æœ€å¹¿æ³›çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„é‡å­æ¶æ„æœç´¢åŸºå‡†ï¼ŒBenchRL-QAS ä¸ºåç»­ç ”ç©¶çš„å¯é‡å¤æ€§å’ŒæŠ€æœ¯æ¼”è¿›å¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "quant-ph",
      "comment": "In AAAI Symposium series. Contributions are welcomed here: https://github.com/azhar-ikhtiarudin/bench-rlqas",
      "pdf_url": "https://arxiv.org/pdf/2507.12189v2",
      "published_date": "2025-07-16 12:43:25 UTC",
      "updated_date": "2025-09-27 08:44:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:06:10.387985+00:00"
    },
    {
      "arxiv_id": "2507.12188v1",
      "title": "Wavelet-based Decoupling Framework for low-light Stereo Image Enhancement",
      "title_zh": "åŸºäºå°æ³¢çš„ä½å…‰ç…§ç«‹ä½“å›¾åƒå¢å¼ºè§£è€¦æ¡†æ¶",
      "authors": [
        "Shuangli Du",
        "Siming Yan",
        "Zhenghao Shi",
        "Zhenzhen You",
        "Lu Sun"
      ],
      "abstract": "Low-light images suffer from complex degradation, and existing enhancement methods often encode all degradation factors within a single latent space. This leads to highly entangled features and strong black-box characteristics, making the model prone to shortcut learning. To mitigate the above issues, this paper proposes a wavelet-based low-light stereo image enhancement method with feature space decoupling. Our insight comes from the following findings: (1) Wavelet transform enables the independent processing of low-frequency and high-frequency information. (2) Illumination adjustment can be achieved by adjusting the low-frequency component of a low-light image, extracted through multi-level wavelet decomposition. Thus, by using wavelet transform the feature space is decomposed into a low-frequency branch for illumination adjustment and multiple high-frequency branches for texture enhancement. Additionally, stereo low-light image enhancement can extract useful cues from another view to improve enhancement. To this end, we propose a novel high-frequency guided cross-view interaction module (HF-CIM) that operates within high-frequency branches rather than across the entire feature space, effectively extracting valuable image details from the other view. Furthermore, to enhance the high-frequency information, a detail and texture enhancement module (DTEM) is proposed based on cross-attention mechanism. The model is trained on a dataset consisting of images with uniform illumination and images with non-uniform illumination. Experimental results on both real and synthetic images indicate that our algorithm offers significant advantages in light adjustment while effectively recovering high-frequency information. The code and dataset are publicly available at: https://github.com/Cherisherr/WDCI-Net.git.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† WDCI-Netï¼Œä¸€ç§åŸºäºå°æ³¢å˜æ¢ (wavelet-based) çš„ä½å…‰ç«‹ä½“å›¾åƒå¢å¼ºè§£è€¦æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å•ä¸€æ½œç©ºé—´ä¸­ç‰¹å¾é«˜åº¦è€¦åˆä¸”å…·æœ‰é»‘ç›’ç‰¹æ€§çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å°æ³¢å˜æ¢å°†ç‰¹å¾ç©ºé—´åˆ†è§£ä¸ºç”¨äºäº®åº¦è°ƒèŠ‚çš„ä½é¢‘åˆ†æ”¯å’Œç”¨äºçº¹ç†å¢å¼ºçš„å¤šä¸ªé«˜é¢‘åˆ†æ”¯ï¼Œå®ç°äº†å¯¹ä¸åŒé€€åŒ–å› å­çš„ç‹¬ç«‹å¤„ç†ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨ç«‹ä½“å›¾åƒçš„è·¨è§†å›¾ä¿¡æ¯ï¼Œç ”ç©¶è®¾è®¡äº†é«˜é¢‘å¼•å¯¼è·¨è§†å›¾äº¤äº’æ¨¡å— (HF-CIM)ï¼Œä¸“é—¨åœ¨é«˜é¢‘åˆ†æ”¯ä¸­æå–å¦ä¸€è§†å›¾çš„å›¾åƒç»†èŠ‚ã€‚æ­¤å¤–ï¼Œæ¨¡å‹å¼•å…¥äº†åŸºäºäº¤å‰æ³¨æ„åŠ› (cross-attention) æœºåˆ¶çš„ç»†èŠ‚ä¸çº¹ç†å¢å¼ºæ¨¡å— (DTEM) ä»¥è¿›ä¸€æ­¥å¼ºåŒ–é«˜é¢‘ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤„ç†å‡åŒ€å’Œéå‡åŒ€å…‰ç…§è°ƒæ•´æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶èƒ½æœ‰æ•ˆæ¢å¤å›¾åƒçš„é«˜é¢‘çº¹ç†ç»†èŠ‚ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12188v1",
      "published_date": "2025-07-16 12:42:27 UTC",
      "updated_date": "2025-07-16 12:42:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:06:12.677719+00:00"
    },
    {
      "arxiv_id": "2507.12186v1",
      "title": "Partially Observable Reference Policy Programming: Solving POMDPs Sans Numerical Optimisation",
      "title_zh": "éƒ¨åˆ†å¯è§‚æµ‹å‚è€ƒç­–ç•¥è§„åˆ’ï¼šæ— éœ€æ•°å€¼ä¼˜åŒ–çš„ POMDP æ±‚è§£æ–¹æ³•",
      "authors": [
        "Edward Kim",
        "Hanna Kurniawati"
      ],
      "abstract": "This paper proposes Partially Observable Reference Policy Programming, a novel anytime online approximate POMDP solver which samples meaningful future histories very deeply while simultaneously forcing a gradual policy update. We provide theoretical guarantees for the algorithm's underlying scheme which say that the performance loss is bounded by the average of the sampling approximation errors rather than the usual maximum, a crucial requirement given the sampling sparsity of online planning. Empirical evaluations on two large-scale problems with dynamically evolving environments -- including a helicopter emergency scenario in the Corsica region requiring approximately 150 planning steps -- corroborate the theoretical results and indicate that our solver considerably outperforms current online benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Partially Observable Reference Policy Programmingï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„éšæ—¶åœ¨çº¿è¿‘ä¼¼(anytime online approximate) POMDPæ±‚è§£å™¨ï¼Œèƒ½å¤Ÿå®ç°åœ¨æ— éœ€æ•°å€¼ä¼˜åŒ–çš„å‰æä¸‹è§£å†³å¤æ‚å†³ç­–é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡æ·±åº¦é‡‡æ ·æœ‰æ„ä¹‰çš„æœªæ¥å†å²è®°å½•ï¼Œå¹¶åŒæ—¶å¼ºåˆ¶æ‰§è¡Œæ¸è¿›å¼çš„ç­–ç•¥æ›´æ–°ï¼Œæå‡äº†åœ¨çº¿è§„åˆ’çš„æ·±åº¦ä¸ç¨³å®šæ€§ã€‚ç ”ç©¶å›¢é˜Ÿä¸ºè¯¥ç®—æ³•æä¾›äº†ç†è®ºä¿è¯ï¼Œè¯æ˜å…¶æ€§èƒ½æŸå¤±å—é™äºé‡‡æ ·è¿‘ä¼¼è¯¯å·®çš„å¹³å‡å€¼è€Œéæœ€å¤§å€¼ï¼Œè¿™æœ‰æ•ˆç¼“è§£äº†åœ¨çº¿è§„åˆ’ä¸­çš„é‡‡æ ·ç¨€ç–(sampling sparsity)æŒ‘æˆ˜ã€‚åœ¨åŒ…æ‹¬ç§‘è¥¿å˜‰åœ°åŒºç›´å‡æœºåº”æ€¥æ•‘æ´åœ¨å†…çš„ä¸¤ä¸ªå¤§è§„æ¨¡åŠ¨æ€ç¯å¢ƒå®éªŒä¸­ï¼Œè¯¥æ±‚è§£å™¨çš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„åœ¨çº¿åŸºå‡†æ¨¡å‹ï¼Œè¯å®äº†å…¶åœ¨é•¿ç¨‹è§„åˆ’ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 2 tables, 3 figures. To be presented at International Joint Conference on Artificial Intelligence 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.12186v1",
      "published_date": "2025-07-16 12:33:32 UTC",
      "updated_date": "2025-07-16 12:33:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:06:19.591124+00:00"
    },
    {
      "arxiv_id": "2507.12145v1",
      "title": "PRISM: Distributed Inference for Foundation Models at Edge",
      "title_zh": "PRISMï¼šé¢å‘è¾¹ç¼˜ç«¯åŸºç¡€æ¨¡å‹çš„åˆ†å¸ƒå¼æ¨ç†",
      "authors": [
        "Muhammad Azlan Qazi",
        "Alexandros Iosifidis",
        "Qi Zhang"
      ],
      "abstract": "Foundation models (FMs) have achieved remarkable success across a wide range of applications, from image classification to natural langurage processing, but pose significant challenges for deployment at edge. This has sparked growing interest in developing practical and efficient strategies for bringing foundation models to edge environments. In this work, we propose PRISM, a communication-efficient and compute-aware strategy for distributed Transformer inference on edge devices. Our method leverages a Segment Means representation to approximate intermediate output features, drastically reducing inter-device communication. Additionally, we restructure the self-attention mechanism to eliminate redundant computations caused by per-device Key/Value calculation in position-wise partitioning and design a partition-aware causal masking scheme tailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2 across diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and CBT. Our results demonstrate substantial reductions in communication overhead (up to 99.2% for BERT at compression rate CR = 128) and per-device computation (51.24% for BERT at the same setting), with only minor accuracy degradation. This method offers a scalable and practical solution for deploying foundation models in distributed resource-constrained environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PRISMï¼Œä¸€ç§é¢å‘è¾¹ç¼˜è®¾å¤‡åˆ†å¸ƒå¼Transformeræ¨ç†çš„é€šä¿¡é«˜æ•ˆä¸”è®¡ç®—æ„ŸçŸ¥çš„ç­–ç•¥ï¼Œæ—¨åœ¨è§£å†³Foundation Modelsåœ¨è¾¹ç¼˜ç«¯éƒ¨ç½²é¢ä¸´çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é‡‡ç”¨Segment Meansè¡¨ç¤ºæ³•æ¥è¿‘ä¼¼ä¸­é—´è¾“å‡ºç‰¹å¾ï¼Œä»è€Œæ˜¾è‘—é™ä½è®¾å¤‡é—´çš„é€šä¿¡æˆæœ¬ã€‚åŒæ—¶ï¼ŒPRISMé€šè¿‡é‡æ„self-attentionæœºåˆ¶æ¶ˆé™¤äº†ä½ç½®åˆ†åŒºä¸­å†—ä½™çš„Key/Valueè®¡ç®—ï¼Œå¹¶ä¸ºè‡ªå›å½’æ¨¡å‹è®¾è®¡äº†ç‰¹å®šçš„partition-awareå› æœæ©ç æ–¹æ¡ˆã€‚å®éªŒåœ¨ViTã€BERTå’ŒGPT-2ç­‰æ¨¡å‹ä»¥åŠImageNet-1kã€GLUEç­‰æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å‹ç¼©ç‡ä¸º128æ—¶ï¼Œè¯¥æ–¹æ³•å¯å°†BERTçš„é€šä¿¡å¼€é”€é™ä½è¾¾99.2%ï¼Œå¹¶å‡å°‘51.24%çš„å•è®¾å¤‡è®¡ç®—é‡ï¼Œè€Œå‡†ç¡®ç‡ä»…è½»å¾®ä¸‹é™ã€‚è¿™é¡¹å·¥ä½œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹éƒ¨ç½²Foundation Modelsæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å…·å¤‡æ‰©å±•æ€§çš„åˆ†å¸ƒå¼è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12145v1",
      "published_date": "2025-07-16 11:25:03 UTC",
      "updated_date": "2025-07-16 11:25:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:06:21.385956+00:00"
    },
    {
      "arxiv_id": "2507.14219v2",
      "title": "Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman",
      "title_zh": "åŸºäºSHAPç»¼åˆæŒ‡æ•°çš„äººå·¥æ™ºèƒ½ç»¿æ°¢äº§é‡é¢„æµ‹ä¸é€‰å€é€‚å®œæ€§è¯„ä¼°ï¼šèšç„¦Oman",
      "authors": [
        "Obumneme Zimuzor Nwafor",
        "Mohammed Abdul Majeed Al Hooti"
      ],
      "abstract": "As nations seek sustainable alternatives to fossil fuels, green hydrogen has emerged as a promising strategic pathway toward decarbonisation, particularly in solar-rich arid regions. However, identifying optimal locations for hydrogen production requires the integration of complex environmental, atmospheric, and infrastructural factors, often compounded by limited availability of direct hydrogen yield data. This study presents a novel Artificial Intelligence (AI) framework for computing green hydrogen yield and site suitability index using mean absolute SHAP (SHapley Additive exPlanations) values. This framework consists of a multi-stage pipeline of unsupervised multi-variable clustering, supervised machine learning classifier and SHAP algorithm. The pipeline trains on an integrated meteorological, topographic and temporal dataset and the results revealed distinct spatial patterns of suitability and relative influence of the variables. With model predictive accuracy of 98%, the result also showed that water proximity, elevation and seasonal variation are the most influential factors determining green hydrogen site suitability in Oman with mean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively. Given limited or absence of ground-truth yield data in many countries that have green hydrogen prospects and ambitions, this study offers an objective and reproducible alternative to subjective expert weightings, thus allowing the data to speak for itself and potentially discover novel latent groupings without pre-imposed assumptions. This study offers industry stakeholders and policymakers a replicable and scalable tool for green hydrogen infrastructure planning and other decision making in data-scarce regions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é˜¿æ›¼ç­‰å…‰ç…§ä¸°å¯Œçš„å¹²æ—±åœ°åŒºï¼Œæå‡ºäº†ä¸€ç§åŸºäºäººå·¥æ™ºèƒ½(AI)çš„åˆ›æ–°æ¡†æ¶ï¼Œç”¨äºé¢„æµ‹ç»¿æ°¢(Green Hydrogen)äº§é‡å¹¶è¯„ä¼°é€‰å€é€‚å®œæ€§ã€‚è¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«æ— ç›‘ç£å¤šå˜é‡èšç±»(Unsupervised Multi-variable Clustering)ã€æœ‰ç›‘ç£æœºå™¨å­¦ä¹ åˆ†ç±»å™¨(Supervised Machine Learning Classifier)å’ŒSHAPç®—æ³•çš„å¤šé˜¶æ®µæµæ°´çº¿ï¼Œé€šè¿‡è®¡ç®—å¹³å‡ç»å¯¹SHAPå€¼ç”Ÿæˆç»¼åˆæŒ‡æ•°ã€‚ç ”ç©¶åˆ©ç”¨é›†æˆçš„æ°”è±¡ã€åœ°å½¢å’Œæ—¶é—´æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®ç‡é«˜è¾¾98%ã€‚åˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼Œæ°´æºæ¥è¿‘åº¦(Water Proximity)ã€æµ·æ‹”(Elevation)å’Œå­£èŠ‚å˜åŒ–(Seasonal Variation)æ˜¯å†³å®šé˜¿æ›¼ç»¿æ°¢åœºå€é€‚å®œæ€§çš„æœ€å…·å½±å“åŠ›çš„å› ç´ ã€‚è¯¥æ–¹æ³•ä¸ºç¼ºä¹å®æµ‹äº§é‡æ•°æ®çš„å›½å®¶æä¾›äº†ä¸€ç§å®¢è§‚ä¸”å¯é‡å¤çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæœ‰æ•ˆé¿å…äº†ä¸»è§‚ä¸“å®¶æƒé‡å¸¦æ¥çš„åå·®ã€‚è¿™é¡¹ç ”ç©¶ä¸ºè¡Œä¸šåˆ©ç›Šç›¸å…³è€…å’Œæ”¿ç­–åˆ¶å®šè€…åœ¨ç»¿æ°¢åŸºç¡€è®¾æ–½è§„åˆ’æ–¹é¢æä¾›äº†å¯æ‰©å±•ä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„å†³ç­–å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14219v2",
      "published_date": "2025-07-16 10:56:24 UTC",
      "updated_date": "2025-07-23 10:00:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:06:27.803620+00:00"
    },
    {
      "arxiv_id": "2507.12117v2",
      "title": "Quantum Machine Learning in Multi-Qubit Phase-Space Part I: Foundations",
      "title_zh": "å¤šé‡å­æ¯”ç‰¹ç›¸ç©ºé—´ä¸­çš„é‡å­æœºå™¨å­¦ä¹  ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€",
      "authors": [
        "Timothy Heightman",
        "Edward Jiang",
        "Ruth Mora-Soto",
        "Maciej Lewenstein",
        "Marcin PÅ‚odzieÅ„"
      ],
      "abstract": "Quantum machine learning (QML) seeks to exploit the intrinsic properties of quantum mechanical systems, including superposition, coherence, and quantum entanglement for classical data processing. However, due to the exponential growth of the Hilbert space, QML faces practical limits in classical simulations with the state-vector representation of quantum system. On the other hand, phase-space methods offer an alternative by encoding quantum states as quasi-probability functions. Building on prior work in qubit phase-space and the Stratonovich-Weyl (SW) correspondence, we construct a closed, composable dynamical formalism for one- and many-qubit systems in phase-space. This formalism replaces the operator algebra of the Pauli group with function dynamics on symplectic manifolds, and recasts the curse of dimensionality in terms of harmonic support on a domain that scales linearly with the number of qubits. It opens a new route for QML based on variational modelling over phase-space.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é‡å­æœºå™¨å­¦ä¹  (Quantum machine learning, QML) åœ¨å¤šæ¯”ç‰¹ç›¸ç©ºé—´ (Multi-Qubit Phase-Space) ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³å¸Œå°”ä¼¯ç‰¹ç©ºé—´ (Hilbert space) ç»´åº¦æŒ‡æ•°çº§å¢é•¿å¯¼è‡´çš„ç»å…¸æ¨¡æ‹Ÿå—é™é—®é¢˜ã€‚åŸºäºå…ˆå‰çš„é‡å­æ¯”ç‰¹ç›¸ç©ºé—´ç ”ç©¶ä¸ Stratonovich-Weyl (SW) å¯¹åº”å…³ç³»ï¼Œæœ¬æ–‡ä¸ºå¤šæ¯”ç‰¹ç³»ç»Ÿæ„å»ºäº†ä¸€ä¸ªå°é—­ä¸”å¯ç»„åˆçš„ç›¸ç©ºé—´åŠ¨åŠ›å­¦å½¢å¼ã€‚è¯¥å½¢å¼é€šè¿‡è¾›æµå½¢ (symplectic manifolds) ä¸Šçš„å‡½æ•°åŠ¨åŠ›å­¦å–ä»£äº† Pauli ç¾¤çš„ç®—å­ä»£æ•°ï¼Œä»è€Œå°†ç»´åº¦ç¾éš¾è½¬åŒ–ä¸ºåœ¨éšé‡å­æ¯”ç‰¹æ•°çº¿æ€§æ‰©å±•çš„åŸŸä¸Šçš„è°ƒå’Œæ”¯æŒ (harmonic support) é—®é¢˜ã€‚è¿™é¡¹åŸºç¡€æ€§å·¥ä½œä¸ºåŸºäºç›¸ç©ºé—´å˜åˆ†å»ºæ¨¡ (variational modelling) çš„ QML ç®—æ³•å¼€å‘å¼€è¾Ÿäº†å…¨æ–°çš„ç†è®ºä¸æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "math-ph"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12117v2",
      "published_date": "2025-07-16 10:37:16 UTC",
      "updated_date": "2025-10-08 10:26:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:06:32.482816+00:00"
    },
    {
      "arxiv_id": "2507.12110v1",
      "title": "Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs",
      "title_zh": "é¢å‘ CAVs å¤šè½¦ååŒå†³ç­–çš„æ‹“æ‰‘å¢å¼ºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Ye Han",
        "Lijun Zhang",
        "Dejian Meng",
        "Zhuang Zhang"
      ],
      "abstract": "The exploration-exploitation trade-off constitutes one of the fundamental challenges in reinforcement learning (RL), which is exacerbated in multi-agent reinforcement learning (MARL) due to the exponential growth of joint state-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL) method for optimizing cooperative decision-making of connected and autonomous vehicles (CAVs) in mixed traffic. This work presents two primary contributions: First, we construct a game topology tensor for dynamic traffic flow, effectively compressing high-dimensional traffic state information and decrease the search space for MARL algorithms. Second, building upon the designed game topology tensor and using QMIX as the backbone RL algorithm, we establish a topology-enhanced MARL framework incorporating visit counts and agent mutual information. Extensive simulations across varying traffic densities and CAV penetration rates demonstrate the effectiveness of TPE-MARL. Evaluations encompassing training dynamics, exploration patterns, macroscopic traffic performance metrics, and microscopic vehicle behaviors reveal that TPE-MARL successfully balances exploration and exploitation. Consequently, it exhibits superior performance in terms of traffic efficiency, safety, decision smoothness, and task completion. Furthermore, the algorithm demonstrates decision-making rationality comparable to or exceeding that of human drivers in both mixed-autonomy and fully autonomous traffic scenarios. Code of our work is available at \\href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (MARL) åœ¨è½¦è”ç½‘è‡ªåŠ¨é©¾é©¶è½¦è¾† (CAVs) ååŒå†³ç­–ä¸­é¢ä¸´çš„æœç´¢ç©ºé—´åºå¤§ä»¥åŠæ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡æŒ‘æˆ˜ï¼Œæå‡ºäº†æ‹“æ‰‘å¢å¼ºçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³• (TPE-MARL)ã€‚ç ”ç©¶é¦–å…ˆæ„å»ºäº†ä¸€ç§ç”¨äºåŠ¨æ€äº¤é€šæµçš„åšå¼ˆæ‹“æ‰‘å¼ é‡ (Game topology tensor)ï¼Œæœ‰æ•ˆå‹ç¼©äº†é«˜ç»´äº¤é€šçŠ¶æ€ä¿¡æ¯å¹¶å‡å°äº†ç®—æ³•çš„æœç´¢ç©ºé—´ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œä»¥ QMIX ä¸ºéª¨å¹²ç®—æ³•ï¼Œè¯¥æ¡†æ¶è¿›ä¸€æ­¥å¼•å…¥äº†è®¿é—®è®¡æ•°å’Œæ™ºèƒ½ä½“äº’ä¿¡æ¯ (Mutual information) ä»¥å¢å¼ºæ¢ç´¢æœºåˆ¶ã€‚åœ¨ä¸åŒäº¤é€šå¯†åº¦å’Œ CAV æ¸—é€ç‡ä¸‹çš„ä»¿çœŸå®éªŒè¡¨æ˜ï¼ŒTPE-MARL èƒ½å¤ŸæˆåŠŸå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº¤é€šæ•ˆç‡ã€å®‰å…¨æ€§ã€å†³ç­–å¹³æ»‘åº¦å’Œä»»åŠ¡å®Œæˆç‡æ–¹é¢å‡è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨æ··åˆè‡ªä¸»å’Œå…¨è‡ªä¸»äº¤é€šåœºæ™¯ä¸­å±•ç°å‡ºä¼˜äºæˆ–ç­‰åŒäºäººç±»é©¾é©¶å‘˜çš„å†³ç­–åˆç†æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 16 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.12110v1",
      "published_date": "2025-07-16 10:27:36 UTC",
      "updated_date": "2025-07-16 10:27:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:06:33.185401+00:00"
    },
    {
      "arxiv_id": "2507.12108v2",
      "title": "Multimodal Coordinated Online Behavior: Trade-offs and Strategies",
      "title_zh": "å¤šæ¨¡æ€åœ¨çº¿ååŒè¡Œä¸ºï¼šæƒè¡¡ä¸ç­–ç•¥",
      "authors": [
        "Lorenzo Mannocci",
        "Stefano Cresci",
        "Matteo Magnani",
        "Anna Monreale",
        "Maurizio Tesconi"
      ],
      "abstract": "Coordinated online behavior, which spans from beneficial collective actions to harmful manipulation such as disinformation campaigns, has become a key focus in digital ecosystem analysis. Traditional methods often rely on monomodal approaches, focusing on single types of interactions like co-retweets or co-hashtags, or consider multiple modalities independently of each other. However, these approaches may overlook the complex dynamics inherent in multimodal coordination. This study compares different ways of operationalizing the detection of multimodal coordinated behavior. It examines the trade-off between weakly and strongly integrated multimodal models, highlighting the balance between capturing broader coordination patterns and identifying tightly coordinated behavior. By comparing monomodal and multimodal approaches, we assess the unique contributions of different data modalities and explore how varying implementations of multimodality impact detection outcomes. Our findings reveal that not all the modalities provide distinct insights, but that with a multimodal approach we can get a more comprehensive understanding of coordination dynamics. This work enhances the ability to detect and analyze coordinated online behavior, offering new perspectives for safeguarding the integrity of digital platforms.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€ååŒåœ¨çº¿è¡Œä¸º(Multimodal Coordinated Online Behavior)ï¼Œåˆ†æäº†ä»æœ‰ç›Šé›†ä½“è¡ŒåŠ¨åˆ°æœ‰å®³è™šå‡ä¿¡æ¯æ“çºµç­‰å¤šç§åœºæ™¯ã€‚é’ˆå¯¹ä¼ ç»Ÿå•æ¨¡æ€(Monomodal)æ–¹æ³•éš¾ä»¥æ•æ‰å¤æ‚åŠ¨æ€çš„é—®é¢˜ï¼Œæœ¬æ–‡å¯¹æ¯”äº†å¤šç§æ£€æµ‹å¤šæ¨¡æ€ååŒè¡Œä¸ºçš„å®ç°æ–¹å¼ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†å¼±é›†æˆæ¨¡å‹ä¸å¼ºé›†æˆæ¨¡å‹ä¹‹é—´çš„æƒè¡¡(Trade-offs)ã€‚é€šè¿‡å¯¹æ¯”å•æ¨¡æ€ä¸å¤šæ¨¡æ€æ–¹æ³•çš„æ£€æµ‹ç»“æœï¼Œç ”ç©¶è¯„ä¼°äº†ä¸åŒæ•°æ®æ¨¡æ€çš„ç‹¬ç‰¹è´¡çŒ®åŠå…¶å®æ–½æ–¹å¼å¯¹æ£€æµ‹ç»“æœçš„å½±å“ã€‚å®éªŒå‘ç°ï¼Œè™½ç„¶å¹¶éæ‰€æœ‰æ¨¡æ€éƒ½èƒ½æä¾›ç‹¬ç«‹è§è§£ï¼Œä½†å¤šæ¨¡æ€æ–¹æ³•èƒ½æ›´å…¨é¢åœ°æ­ç¤ºååŒè¡Œä¸ºçš„åŠ¨æ€ç‰¹å¾ã€‚è¯¥é¡¹å·¥ä½œæ˜¾è‘—å¢å¼ºäº†å¯¹ååŒåœ¨çº¿è¡Œä¸ºçš„è¯†åˆ«ä¸åˆ†æèƒ½åŠ›ï¼Œä¸ºä¿éšœæ•°å­—å¹³å°å®‰å…¨ä¸å®Œæ•´æ€§æä¾›äº†æ–°çš„è§†è§’ä¸ç­–ç•¥ã€‚",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12108v2",
      "published_date": "2025-07-16 10:25:45 UTC",
      "updated_date": "2025-07-22 08:38:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:06:37.386127+00:00"
    },
    {
      "arxiv_id": "2507.12107v1",
      "title": "Non-Adaptive Adversarial Face Generation",
      "title_zh": "éè‡ªé€‚åº”å¯¹æŠ—æ€§äººè„¸ç”Ÿæˆ",
      "authors": [
        "Sunpill Kim",
        "Seunghun Paik",
        "Chanwoo Hwang",
        "Minsu Kim",
        "Jae Hong Seo"
      ],
      "abstract": "Adversarial attacks on face recognition systems (FRSs) pose serious security and privacy threats, especially when these systems are used for identity verification. In this paper, we propose a novel method for generating adversarial faces-synthetic facial images that are visually distinct yet recognized as a target identity by the FRS. Unlike iterative optimization-based approaches (e.g., gradient descent or other iterative solvers), our method leverages the structural characteristics of the FRS feature space. We figure out that individuals sharing the same attribute (e.g., gender or race) form an attributed subsphere. By utilizing such subspheres, our method achieves both non-adaptiveness and a remarkably small number of queries. This eliminates the need for relying on transferability and open-source surrogate models, which have been a typical strategy when repeated adaptive queries to commercial FRSs are impossible. Despite requiring only a single non-adaptive query consisting of 100 face images, our method achieves a high success rate of over 93% against AWS's CompareFaces API at its default threshold. Furthermore, unlike many existing attacks that perturb a given image, our method can deliberately produce adversarial faces that impersonate the target identity while exhibiting high-level attributes chosen by the adversary.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººè„¸è¯†åˆ«ç³»ç»Ÿ(Face Recognition Systems, FRSs)é¢ä¸´çš„å®‰å…¨å¨èƒï¼Œæå‡ºäº†ä¸€ç§ç”Ÿæˆå¯¹æŠ—äººè„¸(adversarial faces)çš„æ–°æ–¹æ³•ã€‚ä¸åŒäºä¼ ç»Ÿçš„åŸºäºè¿­ä»£ä¼˜åŒ–(iterative optimization-based)çš„æ–¹æ³•ï¼Œè¯¥ç ”ç©¶åˆ©ç”¨äº†FRSsç‰¹å¾ç©ºé—´ä¸­å…·æœ‰ç›¸åŒå±æ€§çš„ä¸ªä½“æ‰€å½¢æˆçš„å±æ€§å­çƒ(attributed subsphere)ç»“æ„ç‰¹å¾ã€‚è¿™ç§æ–¹æ³•é€šè¿‡åˆ©ç”¨å­çƒç»“æ„å®ç°äº†éè‡ªé€‚åº”æ€§(non-adaptiveness)å’Œæä½æŸ¥è¯¢æ¬¡æ•°çš„éœ€æ±‚ï¼Œæœ‰æ•ˆè§£å†³äº†åœ¨æ— æ³•å¯¹å•†ä¸šç³»ç»Ÿè¿›è¡Œé‡å¤è‡ªé€‚åº”æŸ¥è¯¢æ—¶å¯¹æ¨¡å‹å¯è¿ç§»æ€§(transferability)çš„ä¾èµ–é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä»…åŒ…å«100å¼ å›¾åƒçš„ä¸€æ¬¡éè‡ªé€‚åº”æŸ¥è¯¢ä¸‹ï¼Œé’ˆå¯¹AWSçš„CompareFaces APIè¾¾åˆ°äº†è¶…è¿‡93%çš„æ”»å‡»æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¸ä»…èƒ½é€šè¿‡æ‰°åŠ¨ç°æœ‰å›¾åƒè¿›è¡Œæ”»å‡»ï¼Œè¿˜èƒ½æ ¹æ®æ”»å‡»è€…çš„é€‰æ‹©ç”Ÿæˆå…·æœ‰ç‰¹å®šé«˜å±‚å±æ€§(high-level attributes)ä¸”èƒ½æˆåŠŸå†’å……ç›®æ ‡èº«ä»½çš„åˆæˆäººè„¸ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12107v1",
      "published_date": "2025-07-16 10:24:54 UTC",
      "updated_date": "2025-07-16 10:24:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:06:36.290367+00:00"
    },
    {
      "arxiv_id": "2507.12104v1",
      "title": "From Static to Intelligent: Evolving SaaS Pricing with LLMs",
      "title_zh": "ä»é™æ€åˆ°æ™ºèƒ½ï¼šåŸºäº LLM çš„ SaaS å®šä»·æ¼”è¿›",
      "authors": [
        "Francisco Javier Cavero",
        "Juan C. Alonso",
        "Antonio Ruiz-CortÃ©s"
      ],
      "abstract": "The SaaS paradigm has revolutionized software distribution by offering flexible pricing options to meet diverse customer needs. However, the rapid expansion of the SaaS market has introduced significant complexity for DevOps teams, who must manually manage and evolve pricing structures, an approach that is both time-consuming and prone to errors. The absence of automated tools for pricing analysis restricts the ability to efficiently evaluate, optimize, and scale these models. This paper proposes leveraging intelligent pricing (iPricing), dynamic, machine-readable pricing models, as a solution to these challenges. Intelligent pricing enables competitive analysis, streamlines operational decision-making, and supports continuous pricing evolution in response to market dynamics, leading to improved efficiency and accuracy. We present an LLM-driven approach that automates the transformation of static HTML pricing into iPricing, significantly improving efficiency and consistency while minimizing human error. Our implementation, AI4Pricing2Yaml, features a basic Information Extractor that uses web scraping and LLMs technologies to extract essential pricing components, plans, features, usage limits, and add-ons, from SaaS websites. Validation against a dataset of 30 distinct commercial SaaS, encompassing over 150 intelligent pricings, demonstrates the system's effectiveness in extracting the desired elements across all steps. However, challenges remain in addressing hallucinations, complex structures, and dynamic content. This work highlights the potential of automating intelligent pricing transformation to streamline SaaS pricing management, offering implications for improved consistency and scalability in an increasingly intricate pricing landscape. Future research will focus on refining extraction capabilities and enhancing the system's adaptability to a wider range of SaaS websites.",
      "tldr_zh": "éšç€ SaaS å¸‚åœºçš„å¿«é€Ÿæ‰©å¼ ï¼ŒDevOps å›¢é˜Ÿæ‰‹åŠ¨ç®¡ç†å®šä»·ç»“æ„å˜å¾—æ„ˆå‘å¤æ‚ä¸”æ˜“å‡ºé”™ï¼Œä¸”ç¼ºä¹æœ‰æ•ˆçš„è‡ªåŠ¨åŒ–å®šä»·åˆ†æå·¥å…·ã€‚è¯¥ç ”ç©¶æå‡ºäº†æ™ºèƒ½å®šä»· (iPricing) çš„æ¦‚å¿µï¼Œå³ä¸€ç§åŠ¨æ€çš„ã€æœºå™¨å¯è¯»çš„å®šä»·æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡è‡ªåŠ¨åŒ–è¯„ä¼°å’Œä¼˜åŒ–æ¥æå‡æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æ–¹æ³•ï¼Œå°†é™æ€çš„ HTML å®šä»·é¡µé¢è‡ªåŠ¨è½¬æ¢ä¸º iPricing æ¨¡å‹ï¼Œå¹¶å¼€å‘äº†åä¸º AI4Pricing2Yaml çš„åŸå‹å·¥å…·ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨ç½‘é¡µæŠ“å–æŠ€æœ¯å’Œ LLMs æå–æ ¸å¿ƒå®šä»·ç»„ä»¶ï¼ŒåŒ…æ‹¬å®šä»·æ–¹æ¡ˆ (plans)ã€åŠŸèƒ½ (features)ã€ä½¿ç”¨é™åˆ¶ (usage limits) å’Œå¢å€¼ç»„ä»¶ (add-ons)ã€‚é€šè¿‡å¯¹ 30 ä¸ªå•†ä¸š SaaS ç½‘ç«™åŠè¶…è¿‡ 150 ä¸ªæ™ºèƒ½å®šä»·æ¡ˆä¾‹çš„éªŒè¯ï¼Œå®éªŒè¯æ˜äº†è¯¥ç³»ç»Ÿåœ¨æå–å…³é”®å…ƒç´ æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å°½ç®¡ç›®å‰ä»é¢ä¸´å¹»è§‰ (hallucinations) å’Œå¤æ‚åŠ¨æ€å†…å®¹ç­‰æŒ‘æˆ˜ï¼Œè¯¥å·¥ä½œå±•ç¤ºäº†è‡ªåŠ¨åŒ–æ™ºèƒ½å®šä»·è½¬æ¢åœ¨æå‡ SaaS å®šä»·ä¸€è‡´æ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "12 pages. Accepted at the SOC4AI Workshop (Service-Oriented Computing for AI Applications), held in conjunction with the 22nd International Conference on Service-Oriented Computing (ICSOC 2024)",
      "pdf_url": "https://arxiv.org/pdf/2507.12104v1",
      "published_date": "2025-07-16 10:20:14 UTC",
      "updated_date": "2025-07-16 10:20:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:06:52.991293+00:00"
    },
    {
      "arxiv_id": "2507.13395v1",
      "title": "Mitigating Stylistic Biases of Machine Translation Systems via Monolingual Corpora Only",
      "title_zh": "ä»…åˆ©ç”¨å•è¯­è¯­æ–™åº“ç¼“è§£æœºå™¨ç¿»è¯‘ç³»ç»Ÿçš„é£æ ¼åå·®",
      "authors": [
        "Xuanqi Gao",
        "Weipeng Jiang",
        "Juan Zhai",
        "Shiqing Ma",
        "Siyi Xie",
        "Xinyang Yin",
        "Chao Shen"
      ],
      "abstract": "The advent of neural machine translation (NMT) has revolutionized cross-lingual communication, yet preserving stylistic nuances remains a significant challenge. While existing approaches often require parallel corpora for style preservation, we introduce Babel, a novel framework that enhances stylistic fidelity in NMT using only monolingual corpora. Babel employs two key components: (1) a style detector based on contextual embeddings that identifies stylistic disparities between source and target texts, and (2) a diffusion-based style applicator that rectifies stylistic inconsistencies while maintaining semantic integrity. Our framework integrates with existing NMT systems as a post-processing module, enabling style-aware translation without requiring architectural modifications or parallel stylistic data. Extensive experiments on five diverse domains (law, literature, scientific writing, medicine, and educational content) demonstrate Babel's effectiveness: it identifies stylistic inconsistencies with 88.21% precision and improves stylistic preservation by 150% while maintaining a high semantic similarity score of 0.92. Human evaluation confirms that translations refined by Babel better preserve source text style while maintaining fluency and adequacy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»æœºå™¨ç¿»è¯‘(NMT)åœ¨é£æ ¼ä¿ç•™æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä»…ä¾èµ–å•è¯­è¯­æ–™åº“(Monolingual Corpora)çš„Babelæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºç¿»è¯‘çš„é£æ ¼ä¿çœŸåº¦ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šåŸºäºä¸Šä¸‹æ–‡åµŒå…¥(Contextual Embeddings)çš„é£æ ¼æ£€æµ‹å™¨ç”¨äºè¯†åˆ«ç¿»è¯‘ä¸­çš„é£æ ¼åå·®ï¼Œä»¥åŠåŸºäºæ‰©æ•£æ¨¡å‹(Diffusion-based)çš„é£æ ¼åº”ç”¨å™¨ç”¨äºåœ¨ä¿æŒè¯­ä¹‰å®Œæ•´æ€§çš„å‰æä¸‹çº æ­£é£æ ¼ä¸ä¸€è‡´ã€‚Babelä½œä¸ºä¸€ç§åå¤„ç†æ¨¡å—(Post-processing Module)å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿä¸­ï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹æ¶æ„æˆ–ä¾èµ–ç¨€ç¼ºçš„å¹³è¡Œé£æ ¼æ•°æ®ã€‚åœ¨æ³•å¾‹ã€æ–‡å­¦å’ŒåŒ»å­¦ç­‰äº”ä¸ªé¢†åŸŸçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½ä»¥88.21%çš„å‡†ç¡®ç‡è¯†åˆ«é£æ ¼åå·®ï¼Œå¹¶å°†é£æ ¼ä¿ç•™åº¦æå‡150%ã€‚äººå·¥è¯„ä¼°ä¹Ÿè¿›ä¸€æ­¥è¯å®ï¼Œç»Babelä¼˜åŒ–çš„è¯‘æ–‡åœ¨ä¿æŒ0.92é«˜è¯­ä¹‰ç›¸ä¼¼åº¦çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†ç¿»è¯‘çš„æµåˆ©åº¦ä¸é£æ ¼ä¸€è‡´æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13395v1",
      "published_date": "2025-07-16 09:45:11 UTC",
      "updated_date": "2025-07-16 09:45:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:07:03.558430+00:00"
    },
    {
      "arxiv_id": "2507.12075v1",
      "title": "BOOKCOREF: Coreference Resolution at Book Scale",
      "title_zh": "BOOKCOREFï¼šå…¨ä¹¦å°ºåº¦çš„æŒ‡ä»£æ¶ˆè§£",
      "authors": [
        "Giuliano Martinelli",
        "Tommaso Bonomo",
        "Pere-LluÃ­s Huguet Cabot",
        "Roberto Navigli"
      ],
      "abstract": "Coreference Resolution systems are typically evaluated on benchmarks containing small- to medium-scale documents. When it comes to evaluating long texts, however, existing benchmarks, such as LitBank, remain limited in length and do not adequately assess system capabilities at the book scale, i.e., when co-referring mentions span hundreds of thousands of tokens. To fill this gap, we first put forward a novel automatic pipeline that produces high-quality Coreference Resolution annotations on full narrative texts. Then, we adopt this pipeline to create the first book-scale coreference benchmark, BOOKCOREF, with an average document length of more than 200,000 tokens. We carry out a series of experiments showing the robustness of our automatic procedure and demonstrating the value of our resource, which enables current long-document coreference systems to gain up to +20 CoNLL-F1 points when evaluated on full books. Moreover, we report on the new challenges introduced by this unprecedented book-scale setting, highlighting that current models fail to deliver the same performance they achieve on smaller documents. We release our data and code to encourage research and development of new book-scale Coreference Resolution systems at https://github.com/sapienzanlp/bookcoref.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æŒ‡ä»£æ¶ˆè§£(Coreference Resolution)åŸºå‡†æµ‹è¯•åœ¨å¤„ç†ä¹¦ç±çº§(Book Scale)é•¿æ–‡æœ¬æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†é¦–ä¸ªå¤§è§„æ¨¡ä¹¦ç±æŒ‡ä»£æ¶ˆè§£åŸºå‡†æ•°æ®é›†BOOKCOREFã€‚ç°æœ‰çš„è¯„ä¼°èµ„æºå¦‚LitBankå¤šå±€é™äºä¸­å°è§„æ¨¡æ–‡æ¡£ï¼Œéš¾ä»¥è¯„ä¼°è·¨è¶Šæ•°åä¸‡ä¸ªtokençš„é•¿ç¨‹æŒ‡ä»£å…³ç³»ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼€å‘äº†ä¸€å¥—é«˜æ•ˆçš„è‡ªåŠ¨åŒ–æµæ°´çº¿(Pipeline)æ¥ç”Ÿæˆé«˜è´¨é‡æ ‡æ³¨ï¼Œå¹¶æ„å»ºäº†å¹³å‡æ–‡æ¡£é•¿åº¦è¶…è¿‡20ä¸‡tokençš„BOOKCOREFæ•°æ®é›†ã€‚å®éªŒè¯æ˜ï¼Œè¯¥èµ„æºèƒ½æ˜¾è‘—æå‡é•¿æ–‡æ¡£æŒ‡ä»£æ¶ˆè§£ç³»ç»Ÿåœ¨å®Œæ•´ä¹¦ç±è¯„ä¼°ä¸­çš„è¡¨ç°ï¼Œä½¿CoNLL-F1åˆ†æ•°æœ€é«˜æå‡20åˆ†ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶ä¹Ÿæ­ç¤ºäº†ä¹¦ç±çº§åœºæ™¯å¸¦æ¥çš„å‰æ‰€æœªæœ‰çš„æ–°æŒ‘æˆ˜ï¼Œè¡¨æ˜å½“å‰ä¸»æµæ¨¡å‹åœ¨è¶…é•¿æ–‡æœ¬ä¸Šçš„è¡¨ç°ä»éš¾ä»¥è¾¾åˆ°åœ¨çŸ­æ–‡æ¡£ä¸Šçš„åŒç­‰æ°´å¹³ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2025 Main Conference. 19 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.12075v1",
      "published_date": "2025-07-16 09:35:38 UTC",
      "updated_date": "2025-07-16 09:35:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:07:11.041894+00:00"
    },
    {
      "arxiv_id": "2507.12064v1",
      "title": "StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features",
      "title_zh": "StylOch at PANï¼šç»“åˆåŸºäºé¢‘ç‡çš„æ–‡ä½“ç‰¹å¾çš„æ¢¯åº¦æå‡æ ‘",
      "authors": [
        "Jeremi K. Ochab",
        "Mateusz Matias",
        "Tymoteusz Boba",
        "Tomasz Walkowiak"
      ],
      "abstract": "This submission to the binary AI detection task is based on a modular stylometric pipeline, where: public spaCy models are used for text preprocessing (including tokenisation, named entity recognition, dependency parsing, part-of-speech tagging, and morphology annotation) and extracting several thousand features (frequencies of n-grams of the above linguistic annotations); light-gradient boosting machines are used as the classifier. We collect a large corpus of more than 500 000 machine-generated texts for the classifier's training. We explore several parameter options to increase the classifier's capacity and take advantage of that training set. Our approach follows the non-neural, computationally inexpensive but explainable approach found effective previously.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹PANäºŒè¿›åˆ¶AIæ£€æµ‹ä»»åŠ¡æå‡ºäº†StylOchæ¨¡å—åŒ–æ–‡ä½“åˆ†æ(stylometric)æµæ°´çº¿ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å…¬å¼€çš„spaCyæ¨¡å‹è¿›è¡Œæ–‡æœ¬é¢„å¤„ç†ï¼Œæ¶µç›–äº†åˆ†è¯(tokenization)ã€å‘½åå®ä½“è¯†åˆ«(NER)ã€ä¾å­˜å¥æ³•åˆ†æ(dependency parsing)ã€è¯æ€§æ ‡æ³¨(POS)åŠå½¢æ€å­¦æ ‡æ³¨(morphology annotation)ç­‰ç¯èŠ‚ã€‚é€šè¿‡ä»ä¸Šè¿°è¯­è¨€å­¦æ ‡æ³¨ä¸­æå–æ•°åƒä¸ªåŸºäºé¢‘ç‡çš„ç‰¹å¾(frequency-based features)ï¼Œè¯¥ç ”ç©¶é‡‡ç”¨è½»é‡çº§æ¢¯åº¦æå‡æœº(Light-Gradient Boosting Machines)ä½œä¸ºæ ¸å¿ƒåˆ†ç±»å™¨ã€‚ä¸ºäº†å……åˆ†å‘æŒ¥æ¨¡å‹èƒ½åŠ›ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å«è¶…è¿‡50ä¸‡ä¸ªæœºå™¨ç”Ÿæˆæ–‡æœ¬çš„å¤§è§„æ¨¡è¯­æ–™åº“è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¼˜åŒ–äº†å‚æ•°é…ç½®ã€‚è¯¥æ–¹æ³•åšæŒéç¥ç»(non-neural)ã€ä½è®¡ç®—æˆæœ¬ä¸”å…·æœ‰å¯è§£é‡Šæ€§(explainable)çš„æŠ€æœ¯è·¯çº¿ï¼Œå»¶ç»­äº†ä»¥å¾€ç ”ç©¶ä¸­è¡Œä¹‹æœ‰æ•ˆçš„æŠ€æœ¯èŒƒå¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12064v1",
      "published_date": "2025-07-16 09:21:20 UTC",
      "updated_date": "2025-07-16 09:21:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:06:59.145316+00:00"
    },
    {
      "arxiv_id": "2507.13392v1",
      "title": "TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction",
      "title_zh": "TopicImpactï¼šåˆ©ç”¨è§‚ç‚¹å•å…ƒæ”¹è¿›ä¸»é¢˜å»ºæ¨¡ä¸æ˜Ÿçº§é¢„æµ‹ä¸­çš„å®¢æˆ·åé¦ˆåˆ†æ",
      "authors": [
        "Emil HÃ¤glund",
        "Johanna BjÃ¶rklund"
      ],
      "abstract": "We improve the extraction of insights from customer reviews by restructuring the topic modelling pipeline to operate on opinion units - distinct statements that include relevant text excerpts and associated sentiment scores. Prior work has demonstrated that such units can be reliably extracted using large language models. The result is a heightened performance of the subsequent topic modeling, leading to coherent and interpretable topics while also capturing the sentiment associated with each topic. By correlating the topics and sentiments with business metrics, such as star ratings, we can gain insights on how specific customer concerns impact business outcomes. We present our system's implementation, use cases, and advantages over other topic modeling and classification solutions. We also evaluate its effectiveness in creating coherent topics and assess methods for integrating topic and sentiment modalities for accurate star-rating prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TopicImpact ç³»ç»Ÿï¼Œé€šè¿‡é‡æ„ä¸»é¢˜å»ºæ¨¡ (topic modeling) æµç¨‹å¹¶å¼•å…¥ opinion unitsï¼ˆå³åŒ…å«æ–‡æœ¬ç‰‡æ®µåŠå…¶æƒ…æ„Ÿè¯„åˆ†çš„ç‹¬ç«‹é™ˆè¿°ï¼‰æ¥æå‡å®¢æˆ·åé¦ˆåˆ†æçš„æ·±åº¦ã€‚ç ”ç©¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¯é åœ°æå–è¿™äº›å•å…ƒï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†ä¸»é¢˜çš„è¿è´¯æ€§ä¸å¯è§£é‡Šæ€§ï¼Œå¹¶èƒ½å¤Ÿç²¾å‡†æ•æ‰æ¯ä¸ªä¸»é¢˜ç›¸å…³çš„æƒ…æ„Ÿå€¾å‘ã€‚é€šè¿‡å°†æå–çš„ä¸»é¢˜å’Œæƒ…æ„Ÿä¸æ˜Ÿçº§è¯„åˆ† (star ratings) ç­‰ä¸šåŠ¡æŒ‡æ ‡æŒ‚é’©ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿæ­ç¤ºå®¢æˆ·å…·ä½“å…³æ³¨ç‚¹å¯¹ä¸šåŠ¡æˆæœçš„å½±å“ã€‚ä½œè€…å±•ç¤ºäº†ç³»ç»Ÿçš„å®ç°è·¯å¾„ä¸åº”ç”¨ä¼˜åŠ¿ï¼Œè¯æ˜å…¶åœ¨å¤„ç†å¤æ‚è¯„è®ºæ•°æ®æ—¶ä¼˜äºä¼ ç»Ÿçš„ä¸»é¢˜å»ºæ¨¡å’Œåˆ†ç±»æ–¹æ¡ˆã€‚å®éªŒè¯„ä¼°ç¡®è®¤äº†è¯¥æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡ä¸»é¢˜ä»¥åŠæ•´åˆå¤šæ¨¡æ€ä¿¡æ¯è¿›è¡Œå‡†ç¡®æ˜Ÿçº§é¢„æµ‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13392v1",
      "published_date": "2025-07-16 09:19:26 UTC",
      "updated_date": "2025-07-16 09:19:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:07:08.998625+00:00"
    },
    {
      "arxiv_id": "2507.12060v2",
      "title": "InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing",
      "title_zh": "InstructFLIPï¼šé¢å‘äººè„¸æ´»ä½“æ£€æµ‹çš„ç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹æ¢ç´¢",
      "authors": [
        "Kun-Hsiang Lin",
        "Yu-Wen Tseng",
        "Kang-Yang Huang",
        "Jhih-Ciang Wu",
        "Wen-Huang Cheng"
      ],
      "abstract": "Face anti-spoofing (FAS) aims to construct a robust system that can withstand diverse attacks. While recent efforts have concentrated mainly on cross-domain generalization, two significant challenges persist: limited semantic understanding of attack types and training redundancy across domains. We address the first by integrating vision-language models (VLMs) to enhance the perception of visual input. For the second challenge, we employ a meta-domain strategy to learn a unified model that generalizes well across multiple domains. Our proposed InstructFLIP is a novel instruction-tuned framework that leverages VLMs to enhance generalization via textual guidance trained solely on a single domain. At its core, InstructFLIP explicitly decouples instructions into content and style components, where content-based instructions focus on the essential semantics of spoofing, and style-based instructions consider variations related to the environment and camera characteristics. Extensive experiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA models in accuracy and substantially reducing training redundancy across diverse domains in FAS. Project website is available at https://kunkunlin1221.github.io/InstructFLIP.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººè„¸é˜²æ¬ºè¯ˆ (Face Anti-spoofing, FAS) é¢†åŸŸä¸­æ”»å‡»ç±»å‹è¯­ä¹‰ç†è§£æœ‰é™ä»¥åŠè·¨é¢†åŸŸè®­ç»ƒå†—ä½™è¿™ä¸¤å¤§æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º InstructFLIP çš„ç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹æ¡†æ¶ã€‚InstructFLIP é‡‡ç”¨æŒ‡ä»¤å¾®è°ƒ (instruction-tuned) æŠ€æœ¯ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) æä¾›çš„æ–‡æœ¬å¼•å¯¼æ¥å¢å¼ºæ¨¡å‹å¯¹æ”»å‡»ç‰¹å¾çš„æ„ŸçŸ¥ä¸æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†æŒ‡ä»¤æ˜¾å¼è§£è€¦ä¸ºå†…å®¹ (content) å’Œé£æ ¼ (style) ä¸¤ä¸ªç»„ä»¶ï¼Œå…¶ä¸­å†…å®¹æŒ‡ä»¤èšç„¦äºæ¬ºè¯ˆçš„æœ¬è´¨è¯­ä¹‰ï¼Œè€Œé£æ ¼æŒ‡ä»¤åˆ™æ¶µç›–ç¯å¢ƒä¸æ‘„åƒå¤´ç‰¹å¾çš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡å¼•å…¥å…ƒé¢†åŸŸç­–ç•¥ (meta-domain strategy)ï¼Œä½¿å¾—æ¨¡å‹åœ¨ä»…ä½¿ç”¨å•ä¸€é¢†åŸŸæ•°æ®è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä»èƒ½åœ¨å¤šä¸ªé¢†åŸŸé—´è¡¨ç°å‡ºæä½³çš„æ³›åŒ–æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒInstructFLIP åœ¨å‡†ç¡®ç‡ä¸Šå…¨é¢è¶…è¶Šäº†å½“å‰çš„ SOTA æ¨¡å‹ï¼Œå¹¶æ˜¾è‘—é™ä½äº† FAS ç³»ç»Ÿåœ¨å¤šé¢†åŸŸåº”ç”¨ä¸­çš„è®­ç»ƒå†—ä½™ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by MM'25",
      "pdf_url": "https://arxiv.org/pdf/2507.12060v2",
      "published_date": "2025-07-16 09:16:51 UTC",
      "updated_date": "2025-07-28 08:51:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:07:10.437488+00:00"
    },
    {
      "arxiv_id": "2507.14218v1",
      "title": "Cognitive Castes: Artificial Intelligence, Epistemic Stratification, and the Dissolution of Democratic Discourse",
      "title_zh": "è®¤çŸ¥ç§å§“ï¼šäººå·¥æ™ºèƒ½ã€è®¤è¯†è®ºåˆ†å±‚ä¸æ°‘ä¸»è¯è¯­çš„ç“¦è§£",
      "authors": [
        "Craig S Wright"
      ],
      "abstract": "Artificial intelligence functions not as an epistemic leveller, but as an accelerant of cognitive stratification, entrenching and formalising informational castes within liberal-democratic societies. Synthesising formal epistemology, political theory, algorithmic architecture, and economic incentive structures, the argument traces how contemporary AI systems selectively amplify the reasoning capacity of individuals equipped with recursive abstraction, symbolic logic, and adversarial interrogation, whilst simultaneously pacifying the cognitively untrained through engagement-optimised interfaces. Fluency replaces rigour, immediacy displaces reflection, and procedural reasoning is eclipsed by reactive suggestion. The result is a technocratic realignment of power: no longer grounded in material capital alone, but in the capacity to navigate, deconstruct, and manipulate systems of epistemic production. Information ceases to be a commons; it becomes the substrate through which consent is manufactured and autonomy subdued. Deliberative democracy collapses not through censorship, but through the erosion of interpretive agency. The proposed response is not technocratic regulation, nor universal access, but the reconstruction of rational autonomy as a civic mandate, codified in education, protected by epistemic rights, and structurally embedded within open cognitive infrastructure.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)å¦‚ä½•ä½œä¸ºè®¤çŸ¥åˆ†å±‚(cognitive stratification)çš„åŠ é€Ÿå™¨ï¼Œè€ŒéçŸ¥è¯†å¹³æƒå·¥å…·ï¼Œä»è€Œåœ¨è‡ªç”±æ°‘ä¸»ç¤¾ä¼šä¸­å·©å›ºå¹¶å½¢å¼åŒ–äº†ä¿¡æ¯é˜¶å±‚ã€‚æ–‡ç« é€šè¿‡ç»¼åˆå½¢å¼è®¤è¯†è®º(formal epistemology)ã€æ”¿æ²»ç†è®ºå’Œç®—æ³•æ¶æ„ï¼Œé˜è¿°äº†å½“ä»£AIç³»ç»Ÿå¦‚ä½•é€‰æ‹©æ€§åœ°å¢å¼ºå…·å¤‡é€’å½’æŠ½è±¡(recursive abstraction)å’Œç¬¦å·é€»è¾‘(symbolic logic)èƒ½åŠ›çš„ä¸ªä½“çš„æ¨ç†èƒ½åŠ›ã€‚ä¸æ­¤åŒæ—¶ï¼ŒAIåˆ©ç”¨ç»è¿‡å‚ä¸åº¦ä¼˜åŒ–(engagement-optimised)çš„ç•Œé¢ä½¿ç¼ºä¹ä¸“ä¸šè®­ç»ƒçš„ç¾¤ä½“è¶‹äºè¢«åŠ¨ï¼Œå¯¼è‡´æµç•…æ€§(fluency)å–ä»£äº†ä¸¥è°¨æ€§ï¼Œååº”æ€§å»ºè®®(reactive suggestion)æ©ç›–äº†ç¨‹åºæ€§æ¨ç†ã€‚è¿™ç§è½¬å˜å¼•å‘äº†æƒåŠ›çš„æŠ€æœ¯å®˜åƒšé‡ç»„ï¼ŒæƒåŠ›åŸºç¡€ä»å•çº¯çš„ç‰©è´¨èµ„æœ¬è½¬å‘äº†å¯¼èˆªã€è§£æ„å’Œæ“çºµçŸ¥è¯†ç”Ÿäº§ç³»ç»Ÿ(epistemic production)çš„èƒ½åŠ›ã€‚ä¿¡æ¯ä¸å†æ˜¯å…¬å…±èµ„æºï¼Œè€Œæ˜¯åˆ¶é€ å…±è¯†å’Œå‰Šå¼±è‡ªä¸»æ€§çš„åŸºè´¨ï¼Œå¯¼è‡´åå•†æ°‘ä¸»(Deliberative democracy)å› è§£é‡Šä»£ç†æƒ(interpretive agency)çš„ä¾µèš€è€Œé¢ä¸´å´©æºƒã€‚ä½œè€…æå‡ºï¼Œåº”å¯¹æªæ–½ä¸åº”ä»…é™äºæŠ€æœ¯å®˜åƒšç›‘ç®¡ï¼Œè€Œåº”å°†é‡å»ºç†æ€§è‡ªä¸»(rational autonomy)ä½œä¸ºå…¬æ°‘ä½¿å‘½ï¼Œå¹¶å°†å…¶ç¼–çº‚è¿›æ•™è‚²ä½“ç³»ï¼Œå—åˆ°è®¤è¯†è®ºæƒåˆ©(epistemic rights)çš„ä¿æŠ¤ã€‚æœ€åï¼Œç ”ç©¶å¼ºè°ƒå¿…é¡»å°†è¿™äº›åŸåˆ™ç»“æ„æ€§åœ°åµŒå…¥åˆ°å¼€æ”¾çš„è®¤çŸ¥åŸºç¡€è®¾æ–½(cognitive infrastructure)ä¸­ï¼Œä»¥ç»´æŠ¤æ°‘ä¸»è¯è¯­çš„å®Œæ•´æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.CY",
      "comment": "42 Pages; Approx. 10,000 words, no figures. Theoretical contribution with interdisciplinary scope",
      "pdf_url": "https://arxiv.org/pdf/2507.14218v1",
      "published_date": "2025-07-16 08:46:45 UTC",
      "updated_date": "2025-07-16 08:46:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:07:11.745969+00:00"
    },
    {
      "arxiv_id": "2507.12029v1",
      "title": "Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery",
      "title_zh": "è§†å›¾å†…ä¸è§†å›¾é—´ç›¸å…³æ€§å¼•å¯¼çš„å¤šè§†å›¾æ–°ç±»å‘ç°",
      "authors": [
        "Xinhang Wan",
        "Jiyuan Liu",
        "Qian Qu",
        "Suyuan Liu",
        "Chuyu Zhang",
        "Fangdi Wang",
        "Xinwang Liu",
        "En Zhu",
        "Kunlun He"
      ],
      "abstract": "In this paper, we address the problem of novel class discovery (NCD), which aims to cluster novel classes by leveraging knowledge from disjoint known classes. While recent advances have made significant progress in this area, existing NCD methods face two major limitations. First, they primarily focus on single-view data (e.g., images), overlooking the increasingly common multi-view data, such as multi-omics datasets used in disease diagnosis. Second, their reliance on pseudo-labels to supervise novel class clustering often results in unstable performance, as pseudo-label quality is highly sensitive to factors such as data noise and feature dimensionality. To address these challenges, we propose a novel framework named Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery (IICMVNCD), which is the first attempt to explore NCD in multi-view setting so far. Specifically, at the intra-view level, leveraging the distributional similarity between known and novel classes, we employ matrix factorization to decompose features into view-specific shared base matrices and factor matrices. The base matrices capture distributional consistency among the two datasets, while the factor matrices model pairwise relationships between samples. At the inter-view level, we utilize view relationships among known classes to guide the clustering of novel classes. This includes generating predicted labels through the weighted fusion of factor matrices and dynamically adjusting view weights of known classes based on the supervision loss, which are then transferred to novel class learning. Experimental results validate the effectiveness of our proposed approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–°ç±»åˆ«å‘ç°(Novel Class Discovery, NCD)é¢†åŸŸä¸­ç°æœ‰æ–¹æ³•å±€é™äºå•è§†å›¾æ•°æ®ä¸”ä¾èµ–ä¼ªæ ‡ç­¾å¯¼è‡´æ€§èƒ½ä¸ç¨³å®šçš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªå¤šè§†å›¾æ–°ç±»åˆ«å‘ç°æ¡†æ¶IICMVNCDã€‚åœ¨è§†å›¾å†…(intra-view)å±‚é¢ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å·²çŸ¥ç±»å’Œæ–°ç±»ä¹‹é—´çš„åˆ†å¸ƒç›¸ä¼¼æ€§ï¼Œé€šè¿‡çŸ©é˜µåˆ†è§£(Matrix Factorization)å°†ç‰¹å¾åˆ†è§£ä¸ºè§†å›¾ç‰¹å®šçš„å…±äº«åŸºçŸ©é˜µå’Œå› å­çŸ©é˜µï¼Œä»¥æ•æ‰åˆ†å¸ƒä¸€è‡´æ€§å’Œæ ·æœ¬é—´çš„æˆå¯¹å…³ç³»ã€‚åœ¨è§†å›¾é—´(inter-view)å±‚é¢ï¼Œç ”ç©¶åˆ©ç”¨å·²çŸ¥ç±»çš„è§†å›¾å…³ç³»æ¥æŒ‡å¯¼æ–°ç±»çš„èšç±»ï¼Œé€šè¿‡å› å­çŸ©é˜µçš„åŠ æƒèåˆç”Ÿæˆé¢„æµ‹æ ‡ç­¾ï¼Œå¹¶æ ¹æ®ç›‘ç£æŸå¤±åŠ¨æ€è°ƒæ•´è§†å›¾æƒé‡å¹¶è¿ç§»è‡³æ–°ç±»å­¦ä¹ ã€‚å®éªŒç»“æœéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤šè§†å›¾åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤„ç†ç–¾ç—…è¯Šæ–­ä¸­çš„å¤šç»„å­¦(multi-omics)ç­‰å¤æ‚æ•°æ®æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12029v1",
      "published_date": "2025-07-16 08:42:52 UTC",
      "updated_date": "2025-07-16 08:42:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:07:19.042779+00:00"
    },
    {
      "arxiv_id": "2507.12017v1",
      "title": "SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection",
      "title_zh": "SS-DCï¼šé¢å‘åŸŸè‡ªé€‚åº”ç›®æ ‡æ£€æµ‹çš„è·¨å¯è§å…‰-çº¢å¤–é¸¿æ²Ÿç©ºè°±è§£è€¦ä¸è€¦åˆ",
      "authors": [
        "Xiwei Zhang",
        "Chunjin Yang",
        "Yiming Xiao",
        "Runtong Zhang",
        "Fanman Meng"
      ],
      "abstract": "Unsupervised domain adaptive object detection (UDAOD) from the visible domain to the infrared (RGB-IR) domain is challenging. Existing methods regard the RGB domain as a unified domain and neglect the multiple subdomains within it, such as daytime, nighttime, and foggy scenes. We argue that decoupling the domain-invariant (DI) and domain-specific (DS) features across these multiple subdomains is beneficial for RGB-IR domain adaptation. To this end, this paper proposes a new SS-DC framework based on a decoupling-coupling strategy. In terms of decoupling, we design a Spectral Adaptive Idempotent Decoupling (SAID) module in the aspect of spectral decomposition. Due to the style and content information being highly embedded in different frequency bands, this module can decouple DI and DS components more accurately and interpretably. A novel filter bank-based spectral processing paradigm and a self-distillation-driven decoupling loss are proposed to improve the spectral domain decoupling. In terms of coupling, a new spatial-spectral coupling method is proposed, which realizes joint coupling through spatial and spectral DI feature pyramids. Meanwhile, this paper introduces DS from decoupling to reduce the domain bias. Extensive experiments demonstrate that our method can significantly improve the baseline performance and outperform existing UDAOD methods on multiple RGB-IR datasets, including a new experimental protocol proposed in this paper based on the FLIR-ADAS dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯è§å…‰åˆ°çº¢å¤–ï¼ˆRGB-IRï¼‰é¢†åŸŸçš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”ç›®æ ‡æ£€æµ‹ï¼ˆUDAODï¼‰æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äº†RGBåŸŸå†…ç™½å¤©ã€é»‘å¤œå’Œé›¾å¤©ç­‰å¤šä¸ªå­åŸŸçš„ç‰¹å¾å·®å¼‚ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†åŸºäºè§£è€¦-è€¦åˆç­–ç•¥çš„SS-DCæ¡†æ¶ï¼Œæ—¨åœ¨æ›´æœ‰æ•ˆåœ°æå–è·¨åŸŸç‰¹å¾ã€‚åœ¨è§£è€¦æ–¹é¢ï¼Œç ”ç©¶è®¾è®¡äº†å…‰è°±è‡ªé€‚åº”å¹‚ç­‰è§£è€¦ï¼ˆSAIDï¼‰æ¨¡å—ï¼Œåˆ©ç”¨æ»¤æ³¢å™¨ç»„ï¼ˆfilter bankï¼‰å¤„ç†èŒƒå¼å’Œè‡ªè’¸é¦é©±åŠ¨çš„è§£è€¦æŸå¤±ï¼Œä»å…‰è°±ç»´åº¦æ›´å‡†ç¡®åœ°åˆ†ç¦»åŸŸä¸å˜ï¼ˆDIï¼‰å’ŒåŸŸç‰¹å®šï¼ˆDSï¼‰ç‰¹å¾ã€‚åœ¨è€¦åˆé˜¶æ®µï¼Œè¯¥æ¡†æ¶é€šè¿‡ç©ºé—´-å…‰è°±DIç‰¹å¾é‡‘å­—å¡”å®ç°è”åˆè€¦åˆï¼Œå¹¶å¼•å…¥DSæˆåˆ†æ¥é™ä½åŸŸåç½®ï¼ˆdomain biasï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSS-DCåœ¨åŒ…æ‹¬FLIR-ADASåœ¨å†…çš„å¤šä¸ªRGB-IRæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„UDAODæ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„ä¼˜è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 main-pages, 3 reference-pages, 5 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2507.12017v1",
      "published_date": "2025-07-16 08:21:41 UTC",
      "updated_date": "2025-07-16 08:21:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:07:25.392457+00:00"
    },
    {
      "arxiv_id": "2507.12012v1",
      "title": "Identifying Signatures of Image Phenotypes to Track Treatment Response in Liver Disease",
      "title_zh": "è¯†åˆ«å½±åƒè¡¨å‹ç‰¹å¾ä»¥è¿½è¸ªè‚è„ç–¾ç—…çš„æ²»ç–—ååº”",
      "authors": [
        "Matthias Perkonigg",
        "Nina Bastati",
        "Ahmed Ba-Ssalamah",
        "Peter Mesenbrink",
        "Alexander Goehler",
        "Miljen Martic",
        "Xiaofei Zhou",
        "Michael Trauner",
        "Georg Langs"
      ],
      "abstract": "Quantifiable image patterns associated with disease progression and treatment response are critical tools for guiding individual treatment, and for developing novel therapies. Here, we show that unsupervised machine learning can identify a pattern vocabulary of liver tissue in magnetic resonance images that quantifies treatment response in diffuse liver disease. Deep clustering networks simultaneously encode and cluster patches of medical images into a low-dimensional latent space to establish a tissue vocabulary. The resulting tissue types capture differential tissue change and its location in the liver associated with treatment response. We demonstrate the utility of the vocabulary on a randomized controlled trial cohort of non-alcoholic steatohepatitis patients. First, we use the vocabulary to compare longitudinal liver change in a placebo and a treatment cohort. Results show that the method identifies specific liver tissue change pathways associated with treatment, and enables a better separation between treatment groups than established non-imaging measures. Moreover, we show that the vocabulary can predict biopsy derived features from non-invasive imaging data. We validate the method on a separate replication cohort to demonstrate the applicability of the proposed method.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ— ç›‘ç£æœºå™¨å­¦ä¹ (Unsupervised machine learning)çš„æ–¹æ³•ï¼Œé€šè¿‡æ·±åº¦èšç±»ç½‘ç»œ(Deep clustering networks)è¯†åˆ«ç£å…±æŒ¯æˆåƒ(MRI)ä¸­çš„è‚è„ç»„ç»‡æ¨¡å¼è¯æ±‡ï¼Œä»è€Œé‡åŒ–å¼¥æ¼«æ€§è‚è„ç–¾ç—…çš„æ²»ç–—ååº”ã€‚è¯¥æ–¹æ³•å°†åŒ»å­¦å›¾åƒåˆ‡ç‰‡ç¼–ç å¹¶èšç±»è‡³ä½ç»´æ½œåœ¨ç©ºé—´(Low-dimensional latent space)ä»¥å»ºç«‹ç»„ç»‡è¯æ±‡è¡¨ï¼Œæœ‰æ•ˆæ•æ‰è‚è„ç»„ç»‡çš„å˜åŒ–ç‰¹å¾åŠå…¶è§£å‰–ä½ç½®ã€‚åœ¨éé…’ç²¾æ€§è„‚è‚ªæ€§è‚ç‚(NASH)æ‚£è€…çš„éšæœºå¯¹ç…§è¯•éªŒä¸­ï¼Œè¯¥æ–¹æ³•æˆåŠŸè¯†åˆ«äº†ä¸æ²»ç–—ç›¸å…³çš„ç‰¹å®šç»„ç»‡å˜åŒ–è·¯å¾„ï¼Œä¸”åœ¨åŒºåˆ†æ²»ç–—ç»„ä¸å®‰æ…°å‰‚ç»„çš„è¡¨ç°ä¸Šä¼˜äºä¼ ç»Ÿçš„éå½±åƒè¯„ä¼°æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯æ˜è¯¥è¯æ±‡è¡¨èƒ½å¤Ÿåˆ©ç”¨éä¾µå…¥æ€§å½±åƒæ•°æ®é¢„æµ‹æ´»æ£€(Biopsy)è¡ç”Ÿçš„ç—…ç†ç‰¹å¾ã€‚é€šè¿‡åœ¨ç‹¬ç«‹å¤åˆ¶é˜Ÿåˆ—ä¸Šçš„éªŒè¯ï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†åˆ©ç”¨å›¾åƒè¡¨å‹ç­¾åè¿½è¸ªè‚ç—…è¿›å±•å’Œè¯„ä¼°ç–—æ•ˆçš„ä¸´åºŠåº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12012v1",
      "published_date": "2025-07-16 08:11:48 UTC",
      "updated_date": "2025-07-16 08:11:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:08:26.994929+00:00"
    },
    {
      "arxiv_id": "2507.12011v1",
      "title": "DUSE: A Data Expansion Framework for Low-resource Automatic Modulation Recognition based on Active Learning",
      "title_zh": "DUSEï¼šåŸºäºä¸»åŠ¨å­¦ä¹ çš„ä½èµ„æºè‡ªåŠ¨è°ƒåˆ¶è¯†åˆ«æ•°æ®æ‰©å……æ¡†æ¶",
      "authors": [
        "Yao Lu",
        "Hongyu Gao",
        "Zhuangzhi Chen",
        "Dongwei Xu",
        "Yun Lin",
        "Qi Xuan",
        "Guan Gui"
      ],
      "abstract": "Although deep neural networks have made remarkable achievements in the field of automatic modulation recognition (AMR), these models often require a large amount of labeled data for training. However, in many practical scenarios, the available target domain data is scarce and difficult to meet the needs of model training. The most direct way is to collect data manually and perform expert annotation, but the high time and labor costs are unbearable. Another common method is data augmentation. Although it can enrich training samples to a certain extent, it does not introduce new data and therefore cannot fundamentally solve the problem of data scarcity. To address these challenges, we introduce a data expansion framework called Dynamic Uncertainty-driven Sample Expansion (DUSE). Specifically, DUSE uses an uncertainty scoring function to filter out useful samples from relevant AMR datasets and employs an active learning strategy to continuously refine the scorer. Extensive experiments demonstrate that DUSE consistently outperforms 8 coreset selection baselines in both class-balance and class-imbalance settings. Besides, DUSE exhibits strong cross-architecture generalization for unseen models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Automatic Modulation Recognition (AMR) æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ç›®æ ‡åŸŸæ•°æ®ç¨€ç¼ºä¸”ä¸“å®¶æ ‡æ³¨æˆæœ¬é«˜æ˜‚æ—¶çš„è®­ç»ƒéš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º DUSE (Dynamic Uncertainty-driven Sample Expansion) çš„æ•°æ®æ‰©å±•æ¡†æ¶ã€‚DUSE çªç ´äº†ä¼ ç»Ÿ Data Augmentation æ— æ³•å¼•å…¥æ–°æ•°æ®çš„å±€é™ï¼Œåˆ©ç”¨ Uncertainty Scoring Function ä»ç›¸å…³æ•°æ®é›†ä¸­ç­›é€‰é«˜ä»·å€¼æ ·æœ¬ï¼Œå¹¶é‡‡ç”¨ Active Learning ç­–ç•¥å®ç°è¯„åˆ†å™¨çš„æŒç»­è¿­ä»£ä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDUSE åœ¨ Class-balance å’Œ Class-imbalance ä¸¤ç§è®¾ç½®ä¸‹çš„æ€§èƒ½å‡æ˜¾è‘—ä¼˜äº 8 ç§ Coreset Selection åŸºå‡†æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å±•ç°å‡ºä¼˜å¼‚çš„ Cross-architecture Generalization èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ³›åŒ–è‡³æœªå‚ä¸è®­ç»ƒçš„æ¨¡å‹æ¶æ„ä¸­ï¼Œä¸ºä½èµ„æºç¯å¢ƒä¸‹çš„é€šä¿¡ä¿¡å·è¯†åˆ«æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12011v1",
      "published_date": "2025-07-16 08:09:41 UTC",
      "updated_date": "2025-07-16 08:09:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:08:40.786841+00:00"
    },
    {
      "arxiv_id": "2507.12008v1",
      "title": "Dual form Complementary Masking for Domain-Adaptive Image Segmentation",
      "title_zh": "é¢å‘é¢†åŸŸè‡ªé€‚åº”å›¾åƒåˆ†å‰²çš„å¯¹å¶å½¢å¼äº’è¡¥æ©ç ",
      "authors": [
        "Jiawen Wang",
        "Yinda Chen",
        "Xiaoyu Liu",
        "Che Liu",
        "Dong Liu",
        "Jianqing Gao",
        "Zhiwei Xiong"
      ],
      "abstract": "Recent works have correlated Masked Image Modeling (MIM) with consistency regularization in Unsupervised Domain Adaptation (UDA). However, they merely treat masking as a special form of deformation on the input images and neglect the theoretical analysis, which leads to a superficial understanding of masked reconstruction and insufficient exploitation of its potential in enhancing feature extraction and representation learning. In this paper, we reframe masked reconstruction as a sparse signal reconstruction problem and theoretically prove that the dual form of complementary masks possesses superior capabilities in extracting domain-agnostic image features. Based on this compelling insight, we propose MaskTwins, a simple yet effective UDA framework that integrates masked reconstruction directly into the main training pipeline. MaskTwins uncovers intrinsic structural patterns that persist across disparate domains by enforcing consistency between predictions of images masked in complementary ways, enabling domain generalization in an end-to-end manner. Extensive experiments verify the superiority of MaskTwins over baseline methods in natural and biological image segmentation. These results demonstrate the significant advantages of MaskTwins in extracting domain-invariant features without the need for separate pre-training, offering a new paradigm for domain-adaptive segmentation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æ©ç å›¾åƒå»ºæ¨¡(Masked Image Modeling, MIM)åœ¨æ— ç›‘ç£åŸŸè‡ªé€‚åº”(Unsupervised Domain Adaptation, UDA)ä¸­ç¼ºä¹ç†è®ºæ·±åº¦çš„é—®é¢˜ï¼Œå°†æ©ç é‡å»ºé‡æ–°å®šä¹‰ä¸ºç¨€ç–ä¿¡å·é‡å»ºé—®é¢˜ã€‚è®ºæ–‡ä»ç†è®ºä¸Šè¯æ˜äº†äº’è¡¥æ©ç (complementary masks)çš„å¯¹å¶å½¢å¼åœ¨æå–è·¨åŸŸé€šç”¨å›¾åƒç‰¹å¾æ–¹é¢å…·æœ‰å“è¶Šèƒ½åŠ›ï¼Œå¹¶æ®æ­¤æå‡ºäº†MaskTwinsæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼ºåˆ¶æ‰§è¡Œäº’è¡¥æ©ç å›¾åƒé¢„æµ‹ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œèƒ½å¤Ÿæ•æ‰è·¨é¢†åŸŸæŒä¹…å­˜åœ¨çš„å†…åœ¨ç»“æ„æ¨¡å¼ï¼Œä»è€Œåœ¨ä¸»è®­ç»ƒæµç¨‹ä¸­å®ç°ç«¯åˆ°ç«¯çš„é¢†åŸŸæ³›åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMaskTwinsåœ¨è‡ªç„¶å’Œç”Ÿç‰©å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰åŸºå‡†æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æ— éœ€å•ç‹¬é¢„è®­ç»ƒå³å¯é«˜æ•ˆæå–åŸŸä¸å˜ç‰¹å¾çš„èƒ½åŠ›ã€‚è¿™ç§æ–°èŒƒå¼ä¸ºåŸŸè‡ªé€‚åº”åˆ†å‰²æä¾›äº†æ›´å¼ºçš„ç‰¹å¾æå–å’Œè¡¨ç¤ºå­¦ä¹ æ‰‹æ®µã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.12008v1",
      "published_date": "2025-07-16 08:05:22 UTC",
      "updated_date": "2025-07-16 08:05:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:08:46.632861+00:00"
    },
    {
      "arxiv_id": "2507.12006v4",
      "title": "Frequency-Dynamic Attention Modulation for Dense Prediction",
      "title_zh": "é¢å‘å¯†é›†é¢„æµ‹çš„é¢‘ç‡åŠ¨æ€æ³¨æ„åŠ›è°ƒåˆ¶",
      "authors": [
        "Linwei Chen",
        "Lin Gu",
        "Ying Fu"
      ],
      "abstract": "Vision Transformers (ViTs) have significantly advanced computer vision, demonstrating strong performance across various tasks. However, the attention mechanism in ViTs makes each layer function as a low-pass filter, and the stacked-layer architecture in existing transformers suffers from frequency vanishing. This leads to the loss of critical details and textures. We propose a novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention Modulation (FDAM), which can be easily plugged into ViTs. FDAM directly modulates the overall frequency response of ViTs and consists of two techniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling (FreqScale). Since circuit theory uses low-pass filters as fundamental elements, we introduce AttInv, a method that generates complementary high-pass filtering by inverting the low-pass filter in the attention matrix, and dynamically combining the two. We further design FreqScale to weight different frequency components for fine-grained adjustments to the target response function. Through feature similarity analysis and effective rank evaluation, we demonstrate that our approach avoids representation collapse, leading to consistent performance improvements across various models, including SegFormer, DeiT, and MaskDINO. These improvements are evident in tasks such as semantic segmentation, object detection, and instance segmentation. Additionally, we apply our method to remote sensing detection, achieving state-of-the-art results in single-scale settings. The code is available at https://github.com/Linwei-Chen/FDAM.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰å˜å‹å™¨(Vision Transformers, ViTs)ä¸­æ³¨æ„åŠ›æœºåˆ¶ä½œä¸ºä½é€šæ»¤æ³¢å™¨å¯¼è‡´â€œé¢‘ç‡æ¶ˆå¤±â€(frequency vanishing)ä»¥åŠç»†èŠ‚çº¹ç†ä¸¢å¤±çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å—ç”µè·¯ç†è®ºå¯å‘çš„é¢‘ç‡åŠ¨æ€æ³¨æ„åŠ›è°ƒåˆ¶(Frequency-Dynamic Attention Modulation, FDAM)ç­–ç•¥ã€‚FDAMåŒ…å«æ³¨æ„åŠ›åè½¬(Attention Inversion, AttInv)å’Œé¢‘ç‡åŠ¨æ€ç¼©æ”¾(Frequency Dynamic Scaling, FreqScale)ä¸¤é¡¹å…³é”®æŠ€æœ¯ï¼Œæ—¨åœ¨ç›´æ¥è°ƒåˆ¶ViTsçš„æ•´ä½“é¢‘ç‡å“åº”ã€‚AttInvé€šè¿‡åè½¬æ³¨æ„åŠ›çŸ©é˜µä¸­çš„ä½é€šæ»¤æ³¢å™¨æ¥äº§ç”Ÿäº’è¡¥çš„é«˜é€šæ»¤æ³¢æ•ˆæœï¼Œè€ŒFreqScaleåˆ™å¯¹ä¸åŒé¢‘ç‡æˆåˆ†è¿›è¡ŒåŠ æƒä»¥å®ç°å¯¹ç›®æ ‡å“åº”å‡½æ•°çš„ç²¾ç»†è°ƒæ•´ã€‚é€šè¿‡ç‰¹å¾ç›¸ä¼¼æ€§åˆ†æå’Œæœ‰æ•ˆç§©è¯„ä¼°ï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆé¿å…è¡¨å¾åç¼©(representation collapse)ï¼Œå¹¶åœ¨SegFormerã€DeiTå’ŒMaskDINOç­‰å¤šç§æ¨¡å‹ä¸Šè¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡ã€‚è¯¥æ–¹æ³•å¹¿æ³›é€‚ç”¨äºè¯­ä¹‰åˆ†å‰²ã€ç›®æ ‡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡ï¼Œå¹¶åœ¨é¥æ„Ÿæ¢æµ‹çš„å•å°ºåº¦è®¾ç½®ä¸­è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›æ°´å¹³(State-of-the-art)ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.12006v4",
      "published_date": "2025-07-16 07:59:54 UTC",
      "updated_date": "2025-10-23 14:06:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:08:55.585102+00:00"
    },
    {
      "arxiv_id": "2507.11997v2",
      "title": "Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection",
      "title_zh": "LLM èƒ½å¦è¯†åˆ«æ¬ºè¯ˆè€…ï¼Ÿå¤šçº§ LLM å¢å¼ºçš„å›¾æ¬ºè¯ˆæ£€æµ‹",
      "authors": [
        "Tairan Huang",
        "Yili Wang",
        "Qiutong Li",
        "Changlong He",
        "Jianliang Gao"
      ],
      "abstract": "Graph fraud detection has garnered significant attention as Graph Neural Networks (GNNs) have proven effective in modeling complex relationships within multimodal data. However, existing graph fraud detection methods typically use preprocessed node embeddings and predefined graph structures to reveal fraudsters, which ignore the rich semantic cues contained in raw textual information. Although Large Language Models (LLMs) exhibit powerful capabilities in processing textual information, it remains a significant challenge to perform multimodal fusion of processed textual embeddings with graph structures. In this paper, we propose a \\textbf{M}ulti-level \\textbf{L}LM \\textbf{E}nhanced Graph Fraud \\textbf{D}etection framework called MLED. In MLED, we utilize LLMs to extract external knowledge from textual information to enhance graph fraud detection methods. To integrate LLMs with graph structure information and enhance the ability to distinguish fraudsters, we design a multi-level LLM enhanced framework including type-level enhancer and relation-level enhancer. One is to enhance the difference between the fraudsters and the benign entities, the other is to enhance the importance of the fraudsters in different relations. The experiments on four real-world datasets show that MLED achieves state-of-the-art performance in graph fraud detection as a generalized framework that can be applied to existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MLEDï¼Œä¸€ç§å¤šå±‚çº§LLMå¢å¼ºçš„å›¾æ¬ºè¯ˆæ£€æµ‹(Graph Fraud Detection)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å›¾ç¥ç»ç½‘ç»œ(GNNs)åœ¨å¤„ç†å¤šæ¨¡æ€æ•°æ®æ—¶å¿½ç•¥åŸå§‹æ–‡æœ¬è¯­ä¹‰çº¿ç´¢çš„é—®é¢˜ã€‚MLEDåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ä»æ–‡æœ¬ä¿¡æ¯ä¸­æå–å¤–éƒ¨çŸ¥è¯†ï¼Œä»¥å¼¥è¡¥é¢„å¤„ç†èŠ‚ç‚¹åµŒå…¥å’Œé¢„å®šä¹‰å›¾ç»“æ„çš„å±€é™æ€§ã€‚ä¸ºäº†å®ç°LLMsä¸å›¾ç»“æ„ä¿¡æ¯çš„æ·±åº¦èåˆï¼Œè¯¥æ¡†æ¶è®¾è®¡äº†ç±»å‹çº§å¢å¼ºå™¨(type-level enhancer)å’Œå…³ç³»çº§å¢å¼ºå™¨(relation-level enhancer)ï¼Œåˆ†åˆ«ç”¨äºæ‹‰å¤§æ¬ºè¯ˆè€…ä¸è‰¯æ€§å®ä½“çš„ç‰¹å¾å·®å¼‚ï¼Œå¹¶å¢å¼ºæ¬ºè¯ˆè€…åœ¨ä¸åŒå…³ç³»ä¸­çš„æƒé‡ã€‚åœ¨å››ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMLEDåœ¨å›¾æ¬ºè¯ˆæ£€æµ‹ä»»åŠ¡ä¸­è¾¾åˆ°äº†SOTAæ€§èƒ½ã€‚ä½œä¸ºä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼ŒMLEDèƒ½å¤Ÿæœ‰æ•ˆæå‡ç°æœ‰æ£€æµ‹æ–¹æ³•çš„è¯†åˆ«èƒ½åŠ›ï¼Œä¸ºç»“åˆå¤§æ¨¡å‹ä¸ç»“æ„åŒ–æ•°æ®çš„åæ¬ºè¯ˆç ”ç©¶æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ACM MM 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.11997v2",
      "published_date": "2025-07-16 07:50:43 UTC",
      "updated_date": "2025-10-02 11:55:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:09:22.405096+00:00"
    },
    {
      "arxiv_id": "2507.11992v3",
      "title": "Understanding visual attention beehind bee-inspired UAV navigation",
      "title_zh": "æ¢ç©¶å—èœœèœ‚å¯å‘çš„æ— äººæœºå¯¼èˆªèƒŒåçš„è§†è§‰æ³¨æ„åŠ›",
      "authors": [
        "Pranav Rajbhandari",
        "Abhi Veda",
        "Matthew Garratt",
        "Mandyam Srinivasan",
        "Sridhar Ravi"
      ],
      "abstract": "Bio-inspired design is often used in autonomous UAV navigation due to the capacity of biological systems for flight and obstacle avoidance despite limited sensory and computational capabilities. In particular, honeybees mainly use the sensory input of optic flow, the apparent motion of objects in their visual field, to navigate cluttered environments. In our work, we train a Reinforcement Learning agent to navigate a tunnel with obstacles using only optic flow as sensory input. We inspect the attention patterns of trained agents to determine the regions of optic flow on which they primarily base their motor decisions. We find that agents trained in this way pay most attention to regions of discontinuity in optic flow, as well as regions with large optic flow magnitude. The trained agents appear to navigate a cluttered tunnel by avoiding the obstacles that produce large optic flow, while maintaining a centered position in their environment, which resembles the behavior seen in flying insects. This pattern persists across independently trained agents, which suggests that this could be a good strategy for developing a simple explicit control law for physical UAVs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å—èœœèœ‚å¯å‘çš„æ— äººæœº(UAV)è‡ªä¸»å¯¼èˆªï¼Œé€šè¿‡è®­ç»ƒä¸€ä¸ªå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ™ºèƒ½ä½“åœ¨ä»…ä½¿ç”¨å…‰æµ(optic flow)ä½œä¸ºæ„Ÿå®˜è¾“å…¥çš„æƒ…å†µä¸‹ç©¿è¶Šå¸¦æœ‰éšœç¢ç‰©çš„éš§é“ã€‚ç ”ç©¶äººå‘˜åˆ†æäº†è®­ç»ƒåæ™ºèƒ½ä½“çš„æ³¨æ„åŠ›æ¨¡å¼(attention patterns)ï¼Œä»¥ç¡®å®šå…¶åšå‡ºè¿åŠ¨å†³ç­–æ—¶ä¸»è¦ä¾èµ–çš„å…‰æµåŒºåŸŸã€‚å®éªŒå‘ç°ï¼Œæ™ºèƒ½ä½“ä¸»è¦å…³æ³¨å…‰æµä¸è¿ç»­çš„åŒºåŸŸä»¥åŠå…‰æµå¹…å€¼(optic flow magnitude)è¾ƒå¤§çš„åŒºåŸŸã€‚è®­ç»ƒåçš„æ™ºèƒ½ä½“é€šè¿‡é¿å¼€äº§ç”Ÿå¤§å…‰æµçš„éšœç¢ç‰©å¹¶ä¿æŒåœ¨ç¯å¢ƒä¸­å¤®è¿›è¡Œå¯¼èˆªï¼Œè¿™ä¸é£è¡Œæ˜†è™«çš„è‡ªç„¶è¡Œä¸ºé«˜åº¦ç›¸ä¼¼ã€‚ç”±äºè¯¥æ¨¡å¼åœ¨ç‹¬ç«‹è®­ç»ƒçš„æ™ºèƒ½ä½“ä¸­å…·æœ‰ä¸€è‡´æ€§ï¼Œç ”ç©¶æŒ‡å‡ºè¿™å¯ä»¥ä½œä¸ºä¸ºç‰©ç†æ— äººæœºå¼€å‘ç®€å•ä¸”æ˜ç¡®çš„æ§åˆ¶å¾‹(control law)çš„æœ‰æ•ˆç­–ç•¥ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.11992v3",
      "published_date": "2025-07-16 07:44:25 UTC",
      "updated_date": "2025-09-10 03:07:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:09:23.289619+00:00"
    },
    {
      "arxiv_id": "2507.11991v1",
      "title": "Robust Planning for Autonomous Vehicles with Diffusion-Based Failure Samplers",
      "title_zh": "åŸºäºæ‰©æ•£å¤±æ•ˆé‡‡æ ·å™¨çš„è‡ªåŠ¨é©¾é©¶æ±½è½¦é²æ£’è§„åˆ’",
      "authors": [
        "Juanran Wang",
        "Marc R. Schlichting",
        "Mykel J. Kochenderfer"
      ],
      "abstract": "High-risk traffic zones such as intersections are a major cause of collisions. This study leverages deep generative models to enhance the safety of autonomous vehicles in an intersection context. We train a 1000-step denoising diffusion probabilistic model to generate collision-causing sensor noise sequences for an autonomous vehicle navigating a four-way intersection based on the current relative position and velocity of an intruder. Using the generative adversarial architecture, the 1000-step model is distilled into a single-step denoising diffusion model which demonstrates fast inference speed while maintaining similar sampling quality. We demonstrate one possible application of the single-step model in building a robust planner for the autonomous vehicle. The planner uses the single-step model to efficiently sample potential failure cases based on the currently measured traffic state to inform its decision-making. Through simulation experiments, the robust planner demonstrates significantly lower failure rate and delay rate compared with the baseline Intelligent Driver Model controller.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨åˆ©ç”¨æ·±åº¦ç”Ÿæˆæ¨¡å‹æå‡è‡ªåŠ¨é©¾é©¶æ±½è½¦åœ¨äº¤å‰è·¯å£ç­‰é«˜é£é™©åŒºåŸŸçš„å®‰å…¨æ€§ã€‚ä½œè€…è®­ç»ƒäº†ä¸€ä¸ª1000æ­¥çš„Denoising Diffusion Probabilistic Modelï¼Œæ ¹æ®å‘¨å›´è½¦è¾†çš„ç›¸å¯¹ä½ç½®å’Œé€Ÿåº¦ç”Ÿæˆå¯èƒ½å¯¼è‡´ç¢°æ’çš„ä¼ æ„Ÿå™¨å™ªå£°åºåˆ—ã€‚ä¸ºäº†æ»¡è¶³å®æ—¶æ€§éœ€æ±‚ï¼Œç ”ç©¶é‡‡ç”¨Generative Adversarialæ¶æ„å°†è¯¥æ¨¡å‹è’¸é¦ä¸ºå•æ­¥æ‰©æ•£æ¨¡å‹ï¼Œåœ¨ä¿æŒé‡‡æ ·è´¨é‡çš„åŒæ—¶å®ç°äº†æå¿«çš„æ¨ç†é€Ÿåº¦ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§Robust Plannerï¼Œåˆ©ç”¨è¯¥å•æ­¥æ¨¡å‹é«˜æ•ˆé‡‡æ ·æ½œåœ¨æ•…éšœæ¡ˆä¾‹ï¼Œä»è€Œä¸ºè½¦è¾†å†³ç­–æä¾›é¢„æµ‹æ€§æ”¯æŒã€‚ä»¿çœŸå®éªŒè¡¨æ˜ï¼Œä¸åŸºçº¿Intelligent Driver Modelæ§åˆ¶å™¨ç›¸æ¯”ï¼Œè¯¥é²æ£’è§„åˆ’å™¨æ˜¾è‘—é™ä½äº†æ•…éšœç‡å’Œå»¶è¿Ÿç‡ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.11991v1",
      "published_date": "2025-07-16 07:43:55 UTC",
      "updated_date": "2025-07-16 07:43:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:09:23.986520+00:00"
    },
    {
      "arxiv_id": "2507.12504v1",
      "title": "Transforming Football Data into Object-centric Event Logs with Spatial Context Information",
      "title_zh": "èåˆç©ºé—´ä¸Šä¸‹æ–‡ä¿¡æ¯å°†è¶³çƒæ•°æ®è½¬åŒ–ä¸ºä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„äº‹ä»¶æ—¥å¿—",
      "authors": [
        "Vito Chan",
        "Lennart Ebert",
        "Paul-Julius Hillmann",
        "Christoffer Rubensson",
        "Stephan A. Fahrenkrog-Petersen",
        "Jan Mendling"
      ],
      "abstract": "Object-centric event logs expand the conventional single-case notion event log by considering multiple objects, allowing for the analysis of more complex and realistic process behavior. However, the number of real-world object-centric event logs remains limited, and further studies are needed to test their usefulness. The increasing availability of data from team sports can facilitate object-centric process mining, leveraging both real-world data and suitable use cases. In this paper, we present a framework for transforming football (soccer) data into an object-centric event log, further enhanced with a spatial dimension. We demonstrate the effectiveness of our framework by generating object-centric event logs based on real-world football data and discuss the results for varying process representations. With our paper, we provide the first example for object-centric event logs in football analytics. Future work should consider variant analysis and filtering techniques to better handle variability",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çœŸå®ä¸–ç•Œä¸­é¢å‘å¯¹è±¡äº‹ä»¶æ—¥å¿—(Object-centric event logs)æ•°é‡æœ‰é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå°†è¶³çƒæ•°æ®è½¬åŒ–ä¸ºåŒ…å«ç©ºé—´ä¸Šä¸‹æ–‡ä¿¡æ¯çš„é¢å‘å¯¹è±¡äº‹ä»¶æ—¥å¿—çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆå¤šä¸ªå¯¹è±¡å’Œç©ºé—´ç»´åº¦ï¼Œæ‰©å±•äº†ä¼ ç»Ÿçš„å•æ¡ˆä¾‹äº‹ä»¶æ—¥å¿—æ¦‚å¿µï¼Œä»è€Œèƒ½å¤Ÿåˆ†ææ›´ä¸ºå¤æ‚å’ŒçœŸå®çš„æµç¨‹è¡Œä¸ºã€‚ç ”ç©¶åˆ©ç”¨çœŸå®ä¸–ç•Œçš„è¶³çƒæ•°æ®éªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯¦ç»†è®¨è®ºäº†ä¸åŒæµç¨‹è¡¨ç¤ºä¸‹çš„åˆ†æç»“æœã€‚ä½œä¸ºé¢å‘å¯¹è±¡äº‹ä»¶æ—¥å¿—åœ¨è¶³çƒåˆ†æé¢†åŸŸçš„é¦–æ¬¡åº”ç”¨ï¼Œè¯¥ç ”ç©¶ä¸ºä½“è‚²èµ›äº‹çš„é¢å‘å¯¹è±¡æµç¨‹æŒ–æ˜(Object-centric process mining)æä¾›äº†æ–°çš„æ–¹æ³•ã€‚æœªæ¥çš„å·¥ä½œå°†è¿›ä¸€æ­¥æ¢ç´¢å˜ä½“åˆ†æå’Œè¿‡æ»¤æŠ€æœ¯ï¼Œä»¥æ›´å¥½åœ°å¤„ç†è¶³çƒè¿åŠ¨ä¸­çš„é«˜åº¦å˜å¼‚æ€§ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "Accepted for the 3rd Workshop on Object-centric processes from A to Z (co-locatedOBJECTS 2025) with BPM 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.12504v1",
      "published_date": "2025-07-16 07:40:29 UTC",
      "updated_date": "2025-07-16 07:40:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:09:00.040104+00:00"
    },
    {
      "arxiv_id": "2507.11988v2",
      "title": "Aime: Towards Fully-Autonomous Multi-Agent Framework",
      "title_zh": "Aimeï¼šè¿ˆå‘å…¨è‡ªä¸»å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Yexuan Shi",
        "Mingyu Wang",
        "Yunxiang Cao",
        "Hongjie Lai",
        "Junjian Lan",
        "Xin Han",
        "Yu Wang",
        "Jie Geng",
        "Zhenan Li",
        "Zihao Xia",
        "Xiang Chen",
        "Chen Li",
        "Jian Xu",
        "Wenbo Duan",
        "Yuanshuo Zhu"
      ],
      "abstract": "Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are emerging as a powerful paradigm for solving complex, multifaceted problems. However, the potential of these systems is often constrained by the prevalent plan-and-execute framework, which suffers from critical limitations: rigid plan execution, static agent capabilities, and inefficient communication. These weaknesses hinder their adaptability and robustness in dynamic environments. This paper introduces Aime, a novel multi-agent framework designed to overcome these challenges through dynamic, reactive planning and execution. Aime replaces the conventional static workflow with a fluid and adaptive architecture. Its core innovations include: (1) a Dynamic Planner that continuously refines the overall strategy based on real-time execution feedback; (2) an Actor Factory that implements Dynamic Actor instantiation, assembling specialized agents on-demand with tailored tools and knowledge; and (3) a centralized Progress Management Module that serves as a single source of truth for coherent, system-wide state awareness. We empirically evaluated Aime on a diverse suite of benchmarks spanning general reasoning (GAIA), software engineering (SWE-bench Verified), and live web navigation (WebVoyager). The results demonstrate that Aime consistently outperforms even highly specialized state-of-the-art agents in their respective domains. Its superior adaptability and task success rate establish Aime as a more resilient and effective foundation for multi-agent collaboration.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Aimeï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å®ç°å…¨è‡ªä¸»åä½œçš„æ–°å‹å¤šæ™ºèƒ½ä½“æ¡†æ¶ (Multi-Agent Systems)ï¼Œé’ˆå¯¹æ€§åœ°è§£å†³äº†ä¼ ç»Ÿâ€œè®¡åˆ’-æ‰§è¡Œâ€ (plan-and-execute) æ¶æ„ä¸­å­˜åœ¨çš„æ‰§è¡ŒåƒµåŒ–ã€æ™ºèƒ½ä½“èƒ½åŠ›é™æ€åŠé€šä¿¡ä½æ•ˆç­‰å±€é™ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬æ ¹æ®å®æ—¶æ‰§è¡Œåé¦ˆæŒç»­ç²¾ç‚¼ç­–ç•¥çš„åŠ¨æ€è§„åˆ’å™¨ (Dynamic Planner)ã€èƒ½å¤ŸæŒ‰éœ€å®ä¾‹åŒ–å…·å¤‡ä¸“é—¨å·¥å…·å’ŒçŸ¥è¯†çš„æ™ºèƒ½ä½“å·¥å‚ (Actor Factory)ï¼Œä»¥åŠä½œä¸ºç³»ç»Ÿå…¨å±€çŠ¶æ€å”¯ä¸€äº‹å®æ¥æºçš„è¿›åº¦ç®¡ç†æ¨¡å— (Progress Management Module)ã€‚é€šè¿‡åœ¨é€šç”¨æ¨ç† (GAIA)ã€è½¯ä»¶å·¥ç¨‹ (SWE-bench Verified) å’Œå®æ—¶ç½‘é¡µå¯¼èˆª (WebVoyager) ç­‰å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ï¼ŒAime çš„è¡¨ç°ä¸€è‡´ä¼˜äºå„é¢†åŸŸçš„å…ˆè¿›ä¸“ç”¨æ™ºèƒ½ä½“ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒAime å‡­å€Ÿå…¶å“è¶Šçš„é€‚åº”æ€§å’Œä»»åŠ¡æˆåŠŸç‡ï¼Œä¸ºæ„å»ºæ›´å…·éŸ§æ€§ä¸”é«˜æ•ˆçš„è‡ªä¸»å¤šæ™ºèƒ½ä½“åä½œå¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 1 figures,",
      "pdf_url": "https://arxiv.org/pdf/2507.11988v2",
      "published_date": "2025-07-16 07:38:28 UTC",
      "updated_date": "2025-07-17 03:34:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:09:06.508954+00:00"
    },
    {
      "arxiv_id": "2507.11987v1",
      "title": "Formal Verification of Neural Certificates Done Dynamically",
      "title_zh": "ç¥ç»è¯æ˜çš„åŠ¨æ€å½¢å¼åŒ–éªŒè¯",
      "authors": [
        "Thomas A. Henzinger",
        "Konstantin Kueffner",
        "Emily Yu"
      ],
      "abstract": "Neural certificates have emerged as a powerful tool in cyber-physical systems control, providing witnesses of correctness. These certificates, such as barrier functions, often learned alongside control policies, once verified, serve as mathematical proofs of system safety. However, traditional formal verification of their defining conditions typically faces scalability challenges due to exhaustive state-space exploration. To address this challenge, we propose a lightweight runtime monitoring framework that integrates real-time verification and does not require access to the underlying control policy. Our monitor observes the system during deployment and performs on-the-fly verification of the certificate over a lookahead region to ensure safety within a finite prediction horizon. We instantiate this framework for ReLU-based control barrier functions and demonstrate its practical effectiveness in a case study. Our approach enables timely detection of safety violations and incorrect certificates with minimal overhead, providing an effective but lightweight alternative to the static verification of the certificates.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç½‘ç»œç‰©ç†ç³»ç»Ÿ(cyber-physical systems)æ§åˆ¶ä¸­ç¥ç»è¯æ˜(Neural certificates)ï¼ˆå¦‚éšœç¢å‡½æ•° Barrier functionsï¼‰åœ¨å½¢å¼åŒ–éªŒè¯æ—¶é¢ä¸´çš„å¯æ‰©å±•æ€§éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§è½»é‡çº§çš„è¿è¡Œæ—¶ç›‘æ§æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆå®æ—¶éªŒè¯åŠŸèƒ½ï¼Œåœ¨æ— éœ€è®¿é—®åº•å±‚æ§åˆ¶ç­–ç•¥çš„å‰æä¸‹ï¼Œå¯¹å‰ç»åŒºåŸŸ(lookahead region)å†…çš„è¯æ˜è¿›è¡Œå³æ—¶éªŒè¯ï¼Œä»¥ç¡®ä¿æœ‰é™é¢„æµ‹æ—¶ç•Œ(finite prediction horizon)å†…çš„ç³»ç»Ÿå®‰å…¨æ€§ã€‚ç ”ç©¶è€…å°†æ­¤æ¡†æ¶å…·ä½“åº”ç”¨äºåŸºäº ReLU çš„æ§åˆ¶éšœç¢å‡½æ•°(Control barrier functions)ï¼Œå¹¶é€šè¿‡æ¡ˆä¾‹ç ”ç©¶è¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æä½å¼€é”€ä¸‹åŠæ—¶æ£€æµ‹å®‰å…¨è¿è§„å’Œé”™è¯¯è¯æ˜ï¼Œä¸ºç¥ç»è¯æ˜çš„é™æ€éªŒè¯æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”è½»é‡åŒ–çš„åŠ¨æ€æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SC",
        "cs.AI"
      ],
      "primary_category": "cs.SC",
      "comment": "Accepted at RV'25",
      "pdf_url": "https://arxiv.org/pdf/2507.11987v1",
      "published_date": "2025-07-16 07:37:23 UTC",
      "updated_date": "2025-07-16 07:37:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:09:12.737009+00:00"
    },
    {
      "arxiv_id": "2507.11975v1",
      "title": "Online Training and Pruning of Deep Reinforcement Learning Networks",
      "title_zh": "æ·±åº¦å¼ºåŒ–å­¦ä¹ ç½‘ç»œçš„åœ¨çº¿è®­ç»ƒä¸å‰ªæ",
      "authors": [
        "Valentin Frank Ingmar Guenter",
        "Athanasios Sideris"
      ],
      "abstract": "Scaling deep neural networks (NN) of reinforcement learning (RL) algorithms has been shown to enhance performance when feature extraction networks are used but the gained performance comes at the significant expense of increased computational and memory complexity. Neural network pruning methods have successfully addressed this challenge in supervised learning. However, their application to RL is underexplored. We propose an approach to integrate simultaneous training and pruning within advanced RL methods, in particular to RL algorithms enhanced by the Online Feature Extractor Network (OFENet). Our networks (XiNet) are trained to solve stochastic optimization problems over the RL networks' weights and the parameters of variational Bernoulli distributions for 0/1 Random Variables $Î¾$ scaling each unit in the networks. The stochastic problem formulation induces regularization terms that promote convergence of the variational parameters to 0 when a unit contributes little to the performance. In this case, the corresponding structure is rendered permanently inactive and pruned from its network. We propose a cost-aware, sparsity-promoting regularization scheme, tailored to the DenseNet architecture of OFENets expressing the parameter complexity of involved networks in terms of the parameters of the RVs in these networks. Then, when matching this cost with the regularization terms, the many hyperparameters associated with them are automatically selected, effectively combining the RL objectives and network compression. We evaluate our method on continuous control benchmarks (MuJoCo) and the Soft Actor-Critic RL agent, demonstrating that OFENets can be pruned considerably with minimal loss in performance. Furthermore, our results confirm that pruning large networks during training produces more efficient and higher performing RL agents rather than training smaller networks from scratch.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¸­æ¨¡å‹è§„æ¨¡æ‰©å¤§å¸¦æ¥çš„è®¡ç®—ä¸å­˜å‚¨å¤æ‚åº¦é—®é¢˜ï¼Œæå‡ºäº†åä¸ºXiNetçš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ç½‘ç»œè®­ç»ƒä¸å‰ªæ(Pruning)çš„åŒæ­¥è¿›è¡Œã€‚XiNeté€šè¿‡åœ¨ç½‘ç»œæƒé‡å’Œå˜åˆ†ä¼¯åŠªåˆ©åˆ†å¸ƒ(Variational Bernoulli distributions)å‚æ•°ä¸Šæ„å»ºéšæœºä¼˜åŒ–é—®é¢˜ï¼Œåˆ©ç”¨æ­£åˆ™åŒ–æ‰‹æ®µä½¿å¯¹æ€§èƒ½è´¡çŒ®è¾ƒå°çš„å•å…ƒå®ç°æ°¸ä¹…å‰ªæã€‚é’ˆå¯¹åœ¨çº¿ç‰¹å¾æå–ç½‘ç»œ(OFENet)çš„DenseNetæ¶æ„ï¼Œç ”ç©¶è€…è®¾è®¡äº†ä¸€ç§æˆæœ¬æ„ŸçŸ¥çš„ç¨€ç–ä¿ƒè¿›æ­£åˆ™åŒ–æ–¹æ¡ˆï¼Œèƒ½å¤Ÿè‡ªåŠ¨é€‰æ‹©è¶…å‚æ•°ä»¥å¹³è¡¡å¼ºåŒ–å­¦ä¹ ç›®æ ‡ä¸ç½‘ç»œå‹ç¼©ã€‚åœ¨MuJoCoè¿ç»­æ§åˆ¶åŸºå‡†å’ŒSoft Actor-Criticç®—æ³•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒOFENetså¯ä»¥åœ¨æå°æ€§èƒ½æŸå¤±çš„æƒ…å†µä¸‹è¢«å¤§å¹…åº¦å‹ç¼©ã€‚ç ”ç©¶æœ€ç»ˆè¯å®ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‰ªæå¤§å‹ç½‘ç»œæ¯”ä»é›¶å¼€å§‹è®­ç»ƒå°å‹ç½‘ç»œèƒ½äº§ç”Ÿæ›´é«˜æ•ˆä¸”æ€§èƒ½æ›´å¼ºçš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 5 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2507.11975v1",
      "published_date": "2025-07-16 07:17:41 UTC",
      "updated_date": "2025-07-16 07:17:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:09:22.806961+00:00"
    },
    {
      "arxiv_id": "2507.11966v1",
      "title": "Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation",
      "title_zh": "é¢å‘ä½èµ„æº Singlish ç¿»è¯‘çš„æ¯’æ€§æ„ŸçŸ¥å°‘æ ·æœ¬æç¤º",
      "authors": [
        "Ziyu Ge",
        "Gabriel Chua",
        "Leanne Tan",
        "Roy Ka-Wei Lee"
      ],
      "abstract": "As online communication increasingly incorporates under-represented languages and colloquial dialects, standard translation systems often fail to preserve local slang, code-mixing, and culturally embedded markers of harmful speech. Translating toxic content between low-resource language pairs poses additional challenges due to scarce parallel data and safety filters that sanitize offensive expressions. In this work, we propose a reproducible, two-stage framework for toxicity-preserving translation, demonstrated on a code-mixed Singlish safety corpus. First, we perform human-verified few-shot prompt engineering: we iteratively curate and rank annotator-selected Singlish-target examples to capture nuanced slang, tone, and toxicity. Second, we optimize model-prompt pairs by benchmarking several large language models using semantic similarity via direct and back-translation. Quantitative human evaluation confirms the effectiveness and efficiency of our pipeline. Beyond improving translation quality, our framework contributes to the safety of multicultural LLMs by supporting culturally sensitive moderation and benchmarking in low-resource contexts. By positioning Singlish as a testbed for inclusive NLP, we underscore the importance of preserving sociolinguistic nuance in real-world applications such as content moderation and regional platform governance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”¨äºä½èµ„æºè¯­ç§ Singlish ç¿»è¯‘çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé‡ç‚¹è§£å†³ä»£ç æ··æ‚ (code-mixed) ç¯å¢ƒä¸‹æ¯’æ€§å†…å®¹ (toxic content) çš„ä¿ç•™é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰ç³»ç»Ÿåœ¨å¤„ç†ä¿šè¯­å’Œæ–‡åŒ–ç‰¹å®šæœ‰å®³è¨€è®ºæ–¹é¢çš„ä¸è¶³ï¼Œè¯¥æ–¹æ³•é€šè¿‡äººå·¥éªŒè¯çš„å°‘æ ·æœ¬æç¤ºè¯å·¥ç¨‹ (few-shot prompt engineering) ç²¾å‡†æ•æ‰ç¤¾ä¼šè¯­è¨€å­¦ç»†å¾®å·®åˆ«ã€‚ç ”ç©¶è¿›ä¸€æ­¥åˆ©ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦å’Œå›è¯‘ (back-translation) æŠ€æœ¯ä¼˜åŒ–äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æç¤ºè¯é…å¯¹ï¼Œå¹¶é€šè¿‡å®šé‡äººå·¥è¯„ä¼°éªŒè¯äº†å…¶æ•ˆç‡ã€‚è¯¥æ¡†æ¶ä¸ä»…æ˜¾è‘—æå‡äº†ç¿»è¯‘è´¨é‡ï¼Œè¿˜ä¸ºå¤šæ–‡åŒ–èƒŒæ™¯ä¸‹çš„å†…å®¹å®¡æ ¸å’Œå®‰å…¨åŸºå‡†æµ‹è¯•æä¾›äº†æ”¯æŒã€‚é€šè¿‡å°† Singlish ä½œä¸ºåŒ…å®¹æ€§ NLP çš„æµ‹è¯•å¹³å°ï¼Œè¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨åŒºåŸŸå¹³å°æ²»ç†ä¸­ä¿ç•™è¯­è¨€åŸæ„çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.11966v1",
      "published_date": "2025-07-16 06:58:02 UTC",
      "updated_date": "2025-07-16 06:58:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:09:38.632217+00:00"
    },
    {
      "arxiv_id": "2507.11959v1",
      "title": "PoTPTQ: A Two-step Power-of-Two Post-training for LLMs",
      "title_zh": "PoTPTQï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„ä¸¤æ­¥å¼2çš„å¹‚è®­ç»ƒåæ–¹æ³•",
      "authors": [
        "Xinyu Wang",
        "Vahid Partovi Nia",
        "Peng Lu",
        "Jerry Huang",
        "Xiao-Wen Chang",
        "Boxing Chen",
        "Yufei Cui"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across various natural language processing (NLP) tasks. However, their deployment is challenging due to the substantial computational resources required. Power-of-two (PoT) quantization is a general tool to counteract this difficulty. Albeit previous works on PoT quantization can be efficiently dequantized on CPUs using fixed-point addition, it showed less effectiveness on GPUs. The reason is entanglement of the sign bit and sequential bit manipulations needed for dequantization. We propose a novel POT quantization framework for LLM weights that (i) outperforms state-of-the-art accuracy in extremely low-precision number formats, and (ii) enables faster inference through more efficient dequantization. To maintain the accuracy of the quantized model, we introduce a two-step post-training algorithm: (i) initialize the quantization scales with a robust starting point, and (ii) refine these scales using a minimal calibration set. The performance of our PoT post-training algorithm surpasses the current state-of-the-art in integer quantization, particularly at low precisions such as 2- and 3-bit formats. Our PoT quantization accelerates the dequantization step required for the floating point inference and leads to $3.67\\times$ speed up on a NVIDIA V100, and $1.63\\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºPoTPTQï¼Œä¸€ç§é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)æƒé‡çš„ä¸¤æ­¥å¹‚æ¬¡(Power-of-Two)è®­ç»ƒåé‡åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸPoTé‡åŒ–åœ¨GPUä¸Šç”±äºç¬¦å·ä½çº ç¼ å’Œåºåˆ—ä½æ“ä½œå¯¼è‡´çš„æ¨ç†æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸¤æ­¥è®­ç»ƒåç®—æ³•å®ç°ï¼Œé¦–å…ˆä¸ºé‡åŒ–ç¼©æ”¾å› å­æä¾›é²æ£’çš„åˆå§‹åŒ–èµ·ç‚¹ï¼Œéšååˆ©ç”¨æå°æ ¡å‡†é›†(Calibration Set)å¯¹ç¼©æ”¾å› å­è¿›è¡Œç²¾ç»†åŒ–è°ƒæ•´ï¼Œä»¥ç¡®ä¿é‡åŒ–åçš„æ¨¡å‹ç²¾åº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒPoTPTQåœ¨2ä½å’Œ3ä½ç­‰æä½ç²¾åº¦ä¸‹çš„è¡¨ç°è¶…è¶Šäº†å½“å‰çš„æ•´æ•°(Integer)é‡åŒ–æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆé€šè¿‡ä¼˜åŒ–åé‡åŒ–(Dequantization)è¿‡ç¨‹æ˜¾è‘—æå‡äº†æ¨ç†é€Ÿåº¦ï¼Œåœ¨NVIDIA V100å’ŒRTX 4090ä¸Šåˆ†åˆ«å®ç°äº†3.67å€å’Œ1.63å€çš„åŠ é€Ÿï¼Œä¸ºä½èµ„æºç¯å¢ƒä¸‹çš„é«˜æ€§èƒ½LLMéƒ¨ç½²æä¾›äº†é«˜æ•ˆæ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ECAI 2025 (European Conference on Artificial Intelligence)",
      "pdf_url": "https://arxiv.org/pdf/2507.11959v1",
      "published_date": "2025-07-16 06:44:14 UTC",
      "updated_date": "2025-07-16 06:44:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:09:48.435032+00:00"
    },
    {
      "arxiv_id": "2507.11948v1",
      "title": "Kevin: Multi-Turn RL for Generating CUDA Kernels",
      "title_zh": "Kevinï¼šåŸºäºå¤šè½®å¼ºåŒ–å­¦ä¹ çš„ CUDA å†…æ ¸ç”Ÿæˆ",
      "authors": [
        "Carlo Baronio",
        "Pietro Marsella",
        "Ben Pan",
        "Simon Guo",
        "Silas Alberti"
      ],
      "abstract": "Writing GPU kernels is a challenging task and critical for AI systems' efficiency. It is also highly iterative: domain experts write code and improve performance through execution feedback. Moreover, it presents verifiable rewards like correctness and speedup, making it a natural environment to apply Reinforcement Learning (RL). To explicitly incorporate the iterative nature of this process into training, we develop a flexible multi-turn RL recipe that addresses unique challenges encountered in real-world settings, such as learning from long trajectories and effective reward attribution across turns. We present Kevin - K(ernel D)evin, the first model trained with multi-turn RL for CUDA kernel generation and optimization. In our evaluation setup, Kevin shows significant gains over its base model (QwQ-32B), improving correctness of generated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to 1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini (0.78x). Finally, we study its behavior across test-time scaling axes: we found scaling serial refinement more beneficial than parallel sampling. In particular, when given more refinement turns, Kevin shows a higher rate of improvement.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº† Kevinï¼ˆKernel Devinï¼‰ï¼Œè¿™æ˜¯é¦–ä¸ªé‡‡ç”¨å¤šè½® Reinforcement Learning (RL) è®­ç»ƒçš„ CUDA kernel ç”Ÿæˆä¸ä¼˜åŒ–æ¨¡å‹ã€‚é’ˆå¯¹ GPU ç®—å­ç¼–å†™ä¸­é«˜åº¦ä¾èµ–è¿­ä»£åé¦ˆã€å­¦ä¹ è½¨è¿¹é•¿ä»¥åŠè·¨è½®æ¬¡å¥–åŠ±å½’å› éš¾ç­‰æŒ‘æˆ˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€å¥—çµæ´»çš„å¤šè½® RL æ–¹æ¡ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒKevin ç›¸æ¯”å…¶åŸºåº§æ¨¡å‹ QwQ-32B è¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼Œå°†çº¯ CUDA ä»£ç ç”Ÿæˆçš„æ­£ç¡®ç‡ä» 56% æé«˜åˆ° 82%ï¼Œå¹³å‡åŠ é€Ÿæ¯”ç”± 0.53x æå‡è‡³ PyTorch Eager åŸºå‡†çš„ 1.10xï¼Œæ€§èƒ½è¶…è¶Šäº† o4-mini ç­‰å‰æ²¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹æµ‹è¯•æ—¶ç¼©æ”¾ (test-time scaling) çš„ç ”ç©¶å‘ç°ï¼Œä¸²è¡Œç»†åŒ– (serial refinement) æ¯”å¹¶è¡Œé‡‡æ · (parallel sampling) æ›´èƒ½æœ‰æ•ˆå¢å¼ºæ¨¡å‹è¡¨ç°ã€‚éšç€è¿­ä»£è½®æ¬¡çš„å¢åŠ ï¼ŒKevin å±•ç¤ºå‡ºæ›´é«˜çš„æ”¹è¿›æ•ˆç‡ï¼Œè¯æ˜äº†å¤šè½®è¿­ä»£æ¨ç†åœ¨é«˜æ€§èƒ½è®¡ç®—ä»£ç ç”Ÿæˆé¢†åŸŸçš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PF",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.11948v1",
      "published_date": "2025-07-16 06:33:07 UTC",
      "updated_date": "2025-07-16 06:33:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:09:53.947333+00:00"
    },
    {
      "arxiv_id": "2507.11947v1",
      "title": "RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation",
      "title_zh": "RaDLï¼šé¢å‘å¤šå®ä¾‹æ–‡æœ¬ç”Ÿæˆå›¾åƒçš„å…³ç³»æ„ŸçŸ¥è§£è€¦å­¦ä¹ ",
      "authors": [
        "Geon Park",
        "Seon Bin Kim",
        "Gunho Jung",
        "Seong-Whan Lee"
      ],
      "abstract": "With recent advancements in text-to-image (T2I) models, effectively generating multiple instances within a single image prompt has become a crucial challenge. Existing methods, while successful in generating positions of individual instances, often struggle to account for relationship discrepancy and multiple attributes leakage. To address these limitations, this paper proposes the relation-aware disentangled learning (RaDL) framework. RaDL enhances instance-specific attributes through learnable parameters and generates relation-aware image features via Relation Attention, utilizing action verbs extracted from the global prompt. Through extensive evaluations on benchmarks such as COCO-Position, COCO-MIG, and DrawBench, we demonstrate that RaDL outperforms existing methods, showing significant improvements in positional accuracy, multiple attributes consideration, and the relationships between instances. Our results present RaDL as the solution for generating images that consider both the relationships and multiple attributes of each instance within the multi-instance image.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Text-to-Image (T2I) æ¨¡å‹åœ¨å¤šå®ä¾‹å›¾åƒç”Ÿæˆä¸­é¢ä¸´çš„å…³ç³»åå·®ï¼ˆrelationship discrepancyï¼‰å’Œå±æ€§æ³„éœ²ï¼ˆmultiple attributes leakageï¼‰é—®é¢˜ï¼Œæå‡ºäº† RaDL (Relation-aware Disentangled Learning) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥å¯å­¦ä¹ å‚æ•°æ¥å¢å¼ºç‰¹å®šå®ä¾‹çš„å±æ€§è¡¨è¾¾ï¼Œå¹¶åˆ©ç”¨ä»å…¨å±€æç¤ºè¯­ä¸­æå–çš„åŠ¨ä½œåŠ¨è¯ï¼Œç»“åˆ Relation Attention æœºåˆ¶ç”Ÿæˆå…·æœ‰å…³ç³»æ„ŸçŸ¥èƒ½åŠ›çš„å›¾åƒç‰¹å¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRaDL åœ¨ COCO-Positionã€COCO-MIG å’Œ DrawBench ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨ä½ç½®å‡†ç¡®æ€§ã€å¤šå±æ€§å¤„ç†ä»¥åŠå®ä¾‹é—´å…³ç³»åˆ»ç”»æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºç”Ÿæˆèƒ½å¤ŸåŒæ—¶å…¼é¡¾æ¯ä¸ªå®ä¾‹å±æ€§ä¸ç›¸äº’å…³ç³»çš„å¤šå®ä¾‹å›¾åƒæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 Pages",
      "pdf_url": "https://arxiv.org/pdf/2507.11947v1",
      "published_date": "2025-07-16 06:28:20 UTC",
      "updated_date": "2025-07-16 06:28:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:09:45.741312+00:00"
    },
    {
      "arxiv_id": "2507.11943v1",
      "title": "Effective Fine-Tuning of Vision Transformers with Low-Rank Adaptation for Privacy-Preserving Image Classification",
      "title_zh": "é¢å‘éšç§ä¿æŠ¤å›¾åƒåˆ†ç±»çš„è§†è§‰ Transformer ä½ç§©è‡ªé€‚åº”é«˜æ•ˆå¾®è°ƒ",
      "authors": [
        "Haiwei Lin",
        "Shoko Imaizumi",
        "Hitoshi Kiya"
      ],
      "abstract": "We propose a low-rank adaptation method for training privacy-preserving vision transformer (ViT) models that efficiently freezes pre-trained ViT model weights. In the proposed method, trainable rank decomposition matrices are injected into each layer of the ViT architecture, and moreover, the patch embedding layer is not frozen, unlike in the case of the conventional low-rank adaptation methods. The proposed method allows us not only to reduce the number of trainable parameters but to also maintain almost the same accuracy as that of full-time tuning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹éšç§ä¿æŠ¤å›¾åƒåˆ†ç±»ä»»åŠ¡çš„ Vision Transformers (ViT) ä½ç§©è‡ªé€‚åº” (Low-Rank Adaptation) å¾®è°ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ ViT æ¶æ„çš„æ¯ä¸€å±‚ä¸­æ³¨å…¥å¯è®­ç»ƒçš„ç§©åˆ†è§£çŸ©é˜µï¼Œå¹¶é«˜æ•ˆå†»ç»“é¢„è®­ç»ƒæ¨¡å‹çš„å¤§éƒ¨åˆ†æƒé‡ï¼Œå®ç°äº†å‚æ•°é«˜æ•ˆçš„å­¦ä¹ ã€‚ä¸ä¼ ç»Ÿçš„ Low-Rank Adaptation æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ¡ˆåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­é€‰æ‹©ä¸å†»ç»“ patch embedding å±‚ã€‚è¿™ç§æ”¹è¿›ä½¿å¾—æ¨¡å‹åœ¨æ˜¾è‘—å‡å°‘å¯è®­ç»ƒå‚æ•°æ•°é‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿç»´æŒä¸å…¨é‡å¾®è°ƒ (full-time tuning) å‡ ä¹ä¸€è‡´çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚è¯¥ç ”ç©¶è¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤„ç†éšç§ä¿æŠ¤æ¨¡å‹è®­ç»ƒæ—¶çš„æœ‰æ•ˆæ€§ï¼Œä¸ºé«˜æ•ˆä¸”å®‰å…¨çš„è§†è§‰æ¨¡å‹å¾®è°ƒæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "comment": "3 pages, 3 figures, conference",
      "pdf_url": "https://arxiv.org/pdf/2507.11943v1",
      "published_date": "2025-07-16 06:18:52 UTC",
      "updated_date": "2025-07-16 06:18:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:09:47.831938+00:00"
    },
    {
      "arxiv_id": "2507.11939v2",
      "title": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering",
      "title_zh": "POLYCHARTQAï¼šåŸºäºå¤šè¯­è¨€å›¾è¡¨é—®ç­”çš„å¤§è§†è§‰è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•",
      "authors": [
        "Yichen Xu",
        "Liangyu Chen",
        "Liang Zhang",
        "Jianzhe Ma",
        "Wenxuan Wang",
        "Qin Jin"
      ],
      "abstract": "Charts are a universally adopted medium for data communication, yet existing chart understanding benchmarks are overwhelmingly English-centric, limiting their accessibility and relevance to global audiences. To address this limitation, we introduce PolyChartQA, the first large-scale multilingual benchmark for chart question answering, comprising 22,606 charts and 26,151 QA pairs across 10 diverse languages. PolyChartQA is constructed through a scalable pipeline that enables efficient multilingual chart generation via data translation and code reuse, supported by LLM-based translation and rigorous quality control. We systematically evaluate multilingual chart understanding with PolyChartQA on state-of-the-art LVLMs and reveal a significant performance gap between English and other languages, particularly low-resource ones. Additionally, we introduce a companion multilingual chart question answering training set, PolyChartQA-Train, on which fine-tuning LVLMs yields substantial gains in multilingual chart understanding across diverse model sizes and architectures. Together, our benchmark provides a foundation for developing globally inclusive vision-language models capable of understanding charts across diverse linguistic contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å›¾è¡¨ç†è§£åŸºå‡†æµ‹è¯•è¿‡åº¦é›†ä¸­äºè‹±è¯­çš„é—®é¢˜ï¼Œæ¨å‡ºäº†é¦–ä¸ªå¤§è§„æ¨¡å¤šè¯­è¨€å›¾è¡¨é—®ç­”åŸºå‡† PolyChartQAã€‚è¯¥åŸºå‡†æ¶µç›– 10 ç§è¯­è¨€çš„ 22,606 ä¸ªå›¾è¡¨å’Œ 26,151 ä¸ª QA å¯¹ï¼Œé€šè¿‡ç»“åˆ LLM ç¿»è¯‘ä¸ä¸¥æ ¼è´¨é‡æ§åˆ¶çš„å¯æ‰©å±•å·¥ä½œæµæ„å»ºè€Œæˆã€‚é€šè¿‡å¯¹å½“å‰å…ˆè¿›çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (LVLMs) è¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶æ­ç¤ºäº†è‹±è¯­ä¸ä½èµ„æºè¯­è¨€ä¹‹é—´æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜æä¾›äº†é…å¥—è®­ç»ƒé›† PolyChartQA-Trainï¼Œå®éªŒè¯æ˜å¾®è°ƒåçš„æ¨¡å‹åœ¨å¤šè¯­è¨€å›¾è¡¨ç†è§£ä¸Šå–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚è¿™ä¸€å·¥ä½œä¸ºå¼€å‘å…·å¤‡å…¨çƒè¯­è¨€åŒ…å®¹æ€§çš„è§†è§‰è¯­è¨€æ¨¡å‹å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in Progress",
      "pdf_url": "https://arxiv.org/pdf/2507.11939v2",
      "published_date": "2025-07-16 06:09:02 UTC",
      "updated_date": "2026-01-08 17:00:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:09:50.320849+00:00"
    },
    {
      "arxiv_id": "2507.11936v5",
      "title": "A Survey of Deep Learning for Geometry Problem Solving",
      "title_zh": "æ·±åº¦å­¦ä¹ åœ¨å‡ ä½•é—®é¢˜æ±‚è§£ä¸­çš„ç ”ç©¶ç»¼è¿°",
      "authors": [
        "Jianzhe Ma",
        "Wenxuan Wang",
        "Qin Jin"
      ],
      "abstract": "Geometry problem solving, a crucial aspect of mathematical reasoning, is vital across various domains, including education, the assessment of AI's mathematical abilities, and multimodal capability evaluation. The recent surge in deep learning technologies, particularly the emergence of multimodal large language models, has significantly accelerated research in this area. This paper provides a survey of the applications of deep learning in geometry problem solving, including (i) a comprehensive summary of the relevant tasks in geometry problem solving; (ii) a thorough review of related deep learning methods; (iii) a detailed analysis of evaluation metrics and methods; and (iv) a critical discussion of the current challenges and future directions that can be explored. Our objective is to offer a comprehensive and practical reference of deep learning for geometry problem solving, thereby fostering further advancements in this field. We create a continuously updated list of papers on GitHub: https://github.com/majianz/dl4gps.",
      "tldr_zh": "è¯¥ç»¼è¿°æ¢è®¨äº†æ·±åº¦å­¦ä¹ (Deep Learning)åœ¨å‡ ä½•é¢˜ç›®æ±‚è§£(Geometry Problem Solving)é¢†åŸŸçš„ç ”ç©¶ç°çŠ¶ï¼Œå¼ºè°ƒäº†å…¶åœ¨æ•°å­¦æ¨ç†ã€äººå·¥æ™ºèƒ½æ•°å­¦èƒ½åŠ›è¯„ä¼°åŠå¤šæ¨¡æ€(Multimodal)èƒ½åŠ›è¯„ä»·ä¸­çš„å…³é”®ä½œç”¨ã€‚æ–‡ç« å…¨é¢æ¢³ç†äº†å‡ ä½•é¢˜ç›®æ±‚è§£çš„ç›¸å…³ä»»åŠ¡ï¼Œå¹¶å¯¹ç°æœ‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•è¿›è¡Œäº†æ·±å…¥ç»¼è¿°ï¼Œç‰¹åˆ«æ˜¯è¿‘æœŸæ¶Œç°çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(Multimodal Large Language Models)å¯¹è¯¥é¢†åŸŸçš„æ˜¾è‘—æ¨åŠ¨ä½œç”¨ã€‚ç ”ç©¶è¯¦ç»†åˆ†æäº†è¯¥é¢†åŸŸçš„è¯„ä¼°æŒ‡æ ‡ä¸æ–¹æ³•ï¼Œå¹¶å¯¹å½“å‰é¢ä¸´çš„æŒ‘æˆ˜åŠæœªæ¥çš„æ½œåœ¨ç ”ç©¶æ–¹å‘è¿›è¡Œäº†æ‰¹åˆ¤æ€§è®¨è®ºã€‚è¯¥è®ºæ–‡æ—¨åœ¨ä¸ºå‡ ä½•é¢˜ç›®æ±‚è§£ç ”ç©¶æä¾›å…¨é¢ä¸”å®ç”¨çš„å‚è€ƒæŒ‡å—ï¼Œä»è€Œä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥æŠ€æœ¯çªç ´ä¸åº”ç”¨ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜åŒæ­¥ç»´æŠ¤äº†ä¸€ä¸ªæŒç»­æ›´æ–°çš„GitHubèµ„æºåº“ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶è€…æä¾›äº†ä¸°å¯Œçš„å­¦æœ¯ç´ æã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in progress",
      "pdf_url": "https://arxiv.org/pdf/2507.11936v5",
      "published_date": "2025-07-16 06:03:08 UTC",
      "updated_date": "2025-08-22 04:19:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:09:55.696625+00:00"
    },
    {
      "arxiv_id": "2507.11935v3",
      "title": "AI-Native Open RAN for Non-Terrestrial Networks: An Overview",
      "title_zh": "é¢å‘éåœ°é¢ç½‘ç»œçš„ AI åŸç”Ÿ Open RANï¼šç»¼è¿°",
      "authors": [
        "Jikang Deng",
        "S. Fizza Hassan",
        "Hui Zhou",
        "Saad Al-Ahmadi",
        "Mohamed-Slim Alouini",
        "Daniel B. Da Costa"
      ],
      "abstract": "Non-terrestrial network (NTN) is envisioned as a critical component of Sixth Generation (6G) networks by enabling ubiquitous services and enhancing network resilience. However, the inherent mobility and high-altitude operation of NTN pose significant challenges throughout the development and operations (DevOps) lifecycle. To address these challenges, integrating NTNs with the Open Radio Access Network (ORAN) is a promising approach, since ORAN can offer disaggregation, openness, virtualization, and embedded intelligence. Despite extensive literature on ORAN and NTN, a holistic view of ORAN-based NTN frameworks is still lacking, particularly regarding how ORAN can effectively address the existing challenges of NTN. Furthermore, although artificial intelligence native (AI-Native) capabilities have the potential to enhance intelligence network control and optimization, their practical realization in NTNs has not yet been sufficiently investigated. Therefore, in this paper, we provide a comprehensive and structured overview of AI-Native ORAN for NTN. This paper commences with an in-depth review of the existing literature and subsequently introduces the necessary background about ORAN, NTN, and AI-Native for communication. After analyzing the DevOps challenges for NTN, we propose the orchestrated AI-Native ORAN-based NTN framework and discuss its key technological enablers. Finally, we present the representative use cases and outline the prospective future research directions of this study.",
      "tldr_zh": "è¯¥ç»¼è¿°æ¢è®¨äº†éåœ°é¢ç½‘ç»œ(NTN)åœ¨è¿ˆå‘6Gè¿‡ç¨‹ä¸­é¢ä¸´çš„å¼€å‘è¿è¥(DevOps)æŒ‘æˆ˜ï¼Œå¹¶æå‡ºå°†å¼€æ”¾æ— çº¿æ¥å…¥ç½‘(ORAN)çš„è§£è€¦ä¸è™šæ‹ŸåŒ–ç‰¹æ€§å¼•å…¥NTNä»¥æå‡ç½‘ç»œå¼¹æ€§ã€‚é’ˆå¯¹AI-Nativeèƒ½åŠ›åœ¨NTNä¸­å®é™…åº”ç”¨ç ”ç©¶ä¸è¶³çš„é—®é¢˜ï¼Œæœ¬æ–‡æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æŠ€æœ¯æ¡†æ¶ï¼Œé‡ç‚¹ä»‹ç»äº†ç¼–æ’å¼AI-Native ORAN-based NTNæ¡†æ¶åŠå…¶æ ¸å¿ƒæŠ€æœ¯èµ‹èƒ½å› ç´ ã€‚é€šè¿‡å¯¹è¯¥æ¡†æ¶çš„ç»“æ„åŒ–åˆ†æï¼Œæ–‡ç« è¯¦ç»†é˜è¿°äº†æ™ºèƒ½ç½‘ç»œæ§åˆ¶ä¸ä¼˜åŒ–çš„å®ç°è·¯å¾„ï¼Œå¹¶å±•ç¤ºäº†ä»£è¡¨æ€§çš„åº”ç”¨åœºæ™¯ã€‚è¯¥ç ”ç©¶ä¸ä»…ç³»ç»Ÿæ¢³ç†äº†ç°æœ‰æ–‡çŒ®ï¼Œè¿˜æ˜ç¡®äº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä¸ºå®ç°é«˜åº¦æ™ºèƒ½åŒ–å’Œæ³›åœ¨è¿æ¥çš„6Géåœ°é¢ç½‘ç»œé€šä¿¡å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.11935v3",
      "published_date": "2025-07-16 05:58:45 UTC",
      "updated_date": "2026-01-18 08:30:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:10:01.864084+00:00"
    },
    {
      "arxiv_id": "2507.11916v2",
      "title": "A Parallel CPU-GPU Framework for Batching Heuristic Operations in Depth-First Heuristic Search",
      "title_zh": "æ·±åº¦ä¼˜å…ˆå¯å‘å¼æœç´¢ä¸­å¯å‘å¼è¿ç®—æ‰¹å¤„ç†çš„ CPU-GPU å¹¶è¡Œæ¡†æ¶",
      "authors": [
        "Ehsan Futuhi",
        "Nathan R. Sturtevant"
      ],
      "abstract": "The rapid advancement of GPU technology has unlocked powerful parallel processing capabilities, creating new opportunities to enhance classic search algorithms. This hardware has been exploited in best-first search algorithms with neural network-based heuristics by creating batched versions of A* and Weighted A* that delay heuristic evaluation until sufficiently many states can be evaluated in parallel on the GPU. But, research has not addressed how depth-first algorithms like IDA* or Budgeted Tree Search (BTS) can have their heuristic computations batched. This is more complicated in a tree search, because progress in the search tree is blocked until heuristic evaluations are complete. In this paper we show that GPU parallelization of heuristics can be effectively performed when the tree search is parallelized on the CPU while heuristic evaluations are parallelized on the GPU. We develop a parallelized cost-bounded depth-first search (CB-DFS) framework that can be applied to both IDA* and BTS, significantly improving their performance. We demonstrate the strength of the approach on the 3x3 Rubik's Cube and the 4x4 sliding tile puzzle (STP) with both classifier-based and regression-based heuristics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ä¼˜å…ˆå¯å‘å¼æœç´¢(Depth-First Heuristic Search)åœ¨ GPU å¹¶è¡ŒåŠ é€Ÿä¸­é¢ä¸´çš„ç“¶é¢ˆï¼Œæå‡ºäº†ä¸€ç§å¹¶è¡Œçš„ CPU-GPU æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¯å‘å¼è®¡ç®—æ‰¹é‡åŒ–å¤„ç†çš„éš¾é¢˜ã€‚ä¼ ç»Ÿçš„ IDA\\* å’Œ Budgeted Tree Search (BTS) ç­‰ç®—æ³•ç”±äºæœç´¢æ ‘è¿›åº¦å—é™äºå¯å‘å¼è¯„ä¼°ï¼Œéš¾ä»¥åƒ A\\* ç®—æ³•é‚£æ ·ç›´æ¥åˆ©ç”¨ GPU çš„æ‰¹å¤„ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œè¯¥æ¡†æ¶åœ¨ CPU ä¸Šå¹¶è¡Œæ‰§è¡Œæœç´¢æ ‘æ‰©å±•ï¼ŒåŒæ—¶å°†å¯å‘å¼è¯„ä¼°(Heuristic Evaluations)ä»»åŠ¡æ‰¹é‡å‘é€è‡³ GPU å¹¶è¡Œå¤„ç†ï¼Œå®ç°äº†æœç´¢ä¸è¯„ä¼°çš„ååŒã€‚è®ºæ–‡å¼€å‘äº†ä¸€ç§é€šç”¨çš„å¹¶è¡ŒåŒ–æˆæœ¬å—é™æ·±åº¦ä¼˜å…ˆæœç´¢(Cost-Bounded Depth-First Search, CB-DFS)æ¡†æ¶ï¼Œå¹¶è¯æ˜å…¶å¯æœ‰æ•ˆåº”ç”¨äºå¤šç§æ·±åº¦ä¼˜å…ˆç®—æ³•ã€‚åœ¨ 3x3 é­”æ–¹å’Œ 4x4 æ»‘åŠ¨æ‹¼å›¾(STP)ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç»“åˆåˆ†ç±»å™¨(Classifier-based)å’Œå›å½’(Regression-based)å¯å‘å¼å‡½æ•°ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æœç´¢æ€§èƒ½ã€‚è¿™ä¸€æˆæœå±•ç¤ºäº†å¦‚ä½•é€šè¿‡è½¯ç¡¬ä»¶ååŒæœ‰æ•ˆåŠ é€Ÿæ·±åº¦ä¼˜å…ˆå¯å‘å¼æœç´¢ï¼Œä¸ºé«˜è®¡ç®—è´Ÿè½½çš„å¯å‘å¼ä»»åŠ¡æä¾›äº†é«˜æ•ˆçš„å¹¶è¡ŒåŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.11916v2",
      "published_date": "2025-07-16 05:07:33 UTC",
      "updated_date": "2025-11-16 04:11:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:10:02.033255+00:00"
    },
    {
      "arxiv_id": "2507.11893v2",
      "title": "Spatial Frequency Modulation for Semantic Segmentation",
      "title_zh": "é¢å‘è¯­ä¹‰åˆ†å‰²çš„ç©ºé—´é¢‘ç‡è°ƒåˆ¶",
      "authors": [
        "Linwei Chen",
        "Ying Fu",
        "Lin Gu",
        "Dezhi Zheng",
        "Jifeng Dai"
      ],
      "abstract": "High spatial frequency information, including fine details like textures, significantly contributes to the accuracy of semantic segmentation. However, according to the Nyquist-Shannon Sampling Theorem, high-frequency components are vulnerable to aliasing or distortion when propagating through downsampling layers such as strided-convolution. Here, we propose a novel Spatial Frequency Modulation (SFM) that modulates high-frequency features to a lower frequency before downsampling and then demodulates them back during upsampling. Specifically, we implement modulation through adaptive resampling (ARS) and design a lightweight add-on that can densely sample the high-frequency areas to scale up the signal, thereby lowering its frequency in accordance with the Frequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling (MSAU) to demodulate the modulated feature and recover high-frequency information through non-uniform upsampling This module further improves segmentation by explicitly exploiting information interaction between densely and sparsely resampled areas at multiple scales. Both modules can seamlessly integrate with various architectures, extending from convolutional neural networks to transformers. Feature visualization and analysis confirm that our method effectively alleviates aliasing while successfully retaining details after demodulation. Finally, we validate the broad applicability and effectiveness of SFM by extending it to image classification, adversarial robustness, instance segmentation, and panoptic segmentation tasks. The code is available at https://github.com/Linwei-Chen/SFM.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­ä¹‰åˆ†å‰²ä¸­é«˜é¢‘ä¿¡æ¯åœ¨ä¸‹é‡‡æ ·å±‚æ˜“å—æ··å  aliasing æˆ–å¤±çœŸå½±å“çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„ç©ºé—´é¢‘ç‡è°ƒåˆ¶ Spatial Frequency Modulation (SFM) æ–¹æ³•ã€‚SFM åœ¨ä¸‹é‡‡æ ·å‰å°†é«˜é¢‘ç‰¹å¾è°ƒåˆ¶ä¸ºä½é¢‘ï¼Œå¹¶åœ¨ä¸Šé‡‡æ ·è¿‡ç¨‹ä¸­å°†å…¶è§£è°ƒè¿˜åŸï¼Œä»¥å…‹æœ Nyquist-Shannon Sampling Theorem çš„é™åˆ¶ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•é€šè¿‡è‡ªé€‚åº”é‡é‡‡æ · Adaptive Resampling (ARS) å¯¹é«˜é¢‘åŒºåŸŸè¿›è¡Œå¯†é›†é‡‡æ ·ï¼Œåˆ©ç”¨é¢‘ç‡ç¼©æ”¾å±æ€§ Frequency Scaling Property é™ä½ä¿¡å·é¢‘ç‡ï¼Œå¹¶é…åˆå¤šå°ºåº¦è‡ªé€‚åº”ä¸Šé‡‡æ · Multi-Scale Adaptive Upsampling (MSAU) é€šè¿‡éå‡åŒ€é‡‡æ ·æ¢å¤ç»†èŠ‚ã€‚å®éªŒåˆ†æè¯å®ï¼Œè¯¥æ¨¡å—èƒ½æœ‰æ•ˆç¼“è§£æ··å ç°è±¡å¹¶æˆåŠŸä¿ç•™å›¾åƒç»†èŠ‚ï¼Œä¸”èƒ½æ— ç¼é›†æˆè‡³å·ç§¯ç¥ç»ç½‘ç»œ CNNs å’Œ Transformer æ¶æ„ä¸­ã€‚æœ€ç»ˆï¼Œç ”ç©¶åœ¨è¯­ä¹‰åˆ†å‰²ã€å›¾åƒåˆ†ç±»ã€å¯¹æŠ—é²æ£’æ€§ã€å®ä¾‹åˆ†å‰²åŠå…¨æ™¯åˆ†å‰²ç­‰å¤šé¡¹ä»»åŠ¡ä¸­éªŒè¯äº† SFM çš„å¹¿æ³›é€‚ç”¨æ€§ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accept by TPAMI 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.11893v2",
      "published_date": "2025-07-16 04:15:53 UTC",
      "updated_date": "2025-07-23 03:04:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:10:07.693042+00:00"
    },
    {
      "arxiv_id": "2507.11892v1",
      "title": "From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition",
      "title_zh": "ä»ç²—ç•¥åˆ°ç»†è‡´ï¼šé¢å‘åŠ¨æ€æƒ…æ„Ÿè¯†åˆ«çš„ç»†ç²’åº¦è¯­è¨€çº¿ç´¢ä¸è§†è§‰æ˜¾è‘—åŒºåŸŸè·¨æ¨¡æ€å¯¹é½",
      "authors": [
        "Yu Liu",
        "Leyuan Qu",
        "Hanlei Shi",
        "Di Gao",
        "Yuhua Zheng",
        "Taihao Li"
      ],
      "abstract": "Dynamic Facial Expression Recognition (DFER) aims to identify human emotions from temporally evolving facial movements and plays a critical role in affective computing. While recent vision-language approaches have introduced semantic textual descriptions to guide expression recognition, existing methods still face two key limitations: they often underutilize the subtle emotional cues embedded in generated text, and they have yet to incorporate sufficiently effective mechanisms for filtering out facial dynamics that are irrelevant to emotional expression. To address these gaps, We propose GRACE, Granular Representation Alignment for Cross-modal Emotion recognition that integrates dynamic motion modeling, semantic text refinement, and token-level cross-modal alignment to facilitate the precise localization of emotionally salient spatiotemporal features. Our method constructs emotion-aware textual descriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and highlights expression-relevant facial motion through a motion-difference weighting mechanism. These refined semantic and visual signals are aligned at the token level using entropy-regularized optimal transport. Experiments on three benchmark datasets demonstrate that our method significantly improves recognition performance, particularly in challenging settings with ambiguous or imbalanced emotion classes, establishing new state-of-the-art (SOTA) results in terms of both UAR and WAR.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GRACEï¼Œä¸€ç§ç”¨äºåŠ¨æ€äººè„¸è¡¨æƒ…è¯†åˆ«ï¼ˆDynamic Facial Expression Recognitionï¼‰çš„ç²’åº¦è¡¨ç¤ºå¯¹é½æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰è¯­è¨€æ–¹æ³•å¯¹ç»†å¾®æƒ…æ„Ÿçº¿ç´¢åˆ©ç”¨ä¸è¶³ä»¥åŠéš¾ä»¥è¿‡æ»¤æ— å…³é¢éƒ¨åŠ¨æ€çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä»ç²—åˆ°ç»†çš„æƒ…æ„Ÿæ–‡æœ¬å¢å¼ºï¼ˆCoarse-to-fine Affective Text Enhancement, CATEï¼‰æ¨¡å—æ¥æ„å»ºæƒ…æ„Ÿæ„ŸçŸ¥çš„æ–‡æœ¬æè¿°ï¼Œå¹¶é€šè¿‡è¿åŠ¨å·®å¼‚æƒé‡æœºåˆ¶è¯†åˆ«ä¸è¡¨æƒ…é«˜åº¦ç›¸å…³çš„å…³é”®è§†è§‰ç‰¹å¾ã€‚éšåï¼Œåˆ©ç”¨ç†µæ­£åˆ™åŒ–æœ€ä¼˜ä¼ è¾“ï¼ˆentropy-regularized optimal transportï¼‰å®ç°äº†è¯­ä¹‰ä¸è§†è§‰ä¿¡å·åœ¨ Token çº§åˆ«çš„ç²¾ç¡®è·¨æ¨¡æ€å¯¹é½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGRACE åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ¨¡ç³Šæˆ–ä¸å¹³è¡¡çš„æƒ…æ„Ÿç±»åˆ«æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨ UAR å’Œ WAR æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°äº†ç›®å‰çš„ state-of-the-artï¼ˆSOTAï¼‰æ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.11892v1",
      "published_date": "2025-07-16 04:15:06 UTC",
      "updated_date": "2025-07-16 04:15:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:10:26.187300+00:00"
    },
    {
      "arxiv_id": "2507.22069v2",
      "title": "A Compute-Matched Re-Evaluation of TroVE on MATH",
      "title_zh": "å¯¹ MATH ä¸Š TroVE çš„è®¡ç®—é‡å¯¹é½é‡æ–°è¯„ä¼°",
      "authors": [
        "Tobias Sesterhenn",
        "Ian Berlot-Attwell",
        "Janis Zenkner",
        "Christian Bartelt"
      ],
      "abstract": "Reusing established theorems and formulas is central to mathematical problem solving, serving as essential building blocks for tackling increasingly complex challenges. Recent work, TroVE, argues that code-generating Large Language Models (LLMs) can benefit similarly on the MATH benchmark by inducing and reusing higher-level toolboxes. By allocating computational budget across an ensemble of three modes -- directly generating code, creating tools, and reusing tools -- TroVE claims to outperform a PRIMITIVE baseline that only performs direct generation. However, recent analysis (Berlot-Attwell et al., 2024) casts doubt on these gains, noting that the tools created are often trivial or rarely reused, suggesting that improvements may stem from self-consistency or self-correction. In this work, we re-evaluate TroVE on MATH, analyze the impact of each of its modes, and show that its benefit does not come from these mechanisms, but simply from a higher computational budget spent for TroVE compared to PRIMITIVE. To this end, we also perform a small correction in the original implementation of TroVE's selection mechanism, boosting TroVE's performance on MATH by 3\\% in accuracy. After matching for compute, the benefit of TroVE reduces to a marginal improvement of 1\\%, suggesting that this toolbox approach does not provide a significant benefit on MATH.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ•°å­¦é—®é¢˜æ±‚è§£ä¸­é‡ç”¨å®šç†å’Œå…¬å¼çš„é‡è¦æ€§ï¼Œå¯¹ TroVE æ¡†æ¶åœ¨ MATH åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¿›è¡Œäº†é‡æ–°è¯„ä¼°ã€‚TroVE å®£ç§°é€šè¿‡åœ¨ç›´æ¥ç”Ÿæˆä»£ç ã€åˆ›å»ºå·¥å…·å’Œé‡ç”¨å·¥å…·è¿™ä¸‰ç§æ¨¡å¼é—´åˆ†é…è®¡ç®—é¢„ç®—ï¼Œèƒ½å¤Ÿè¶…è¶Šä»…ç›´æ¥ç”Ÿæˆçš„ PRIMITIVE åŸºçº¿æ¨¡å‹ã€‚é’ˆå¯¹è¿‘æœŸå…³äºå…¶å¢ç›Šæºäº self-consistency æˆ– self-correction çš„è´¨ç–‘ï¼Œæœ¬æ–‡åˆ†æäº† TroVE å„æ¨¡å¼çš„å½±å“ï¼Œå¹¶åœ¨ä¿®æ­£å…¶åŸå®ç°çš„é€‰æ‹©æœºåˆ¶åï¼Œä½¿ TroVE åœ¨ MATH ä¸Šçš„å‡†ç¡®ç‡æå‡äº† 3%ã€‚ç ”ç©¶å‘ç°ï¼ŒTroVE çš„ä¼˜åŠ¿å¹¶éæ¥è‡ªå…¶ç‰¹å®šçš„å·¥å…·ç®±æœºåˆ¶ï¼Œè€Œä¸»è¦æºäºå…¶ç›¸æ¯”åŸºçº¿æ¨¡å‹æ¶ˆè€—äº†æ›´é«˜çš„è®¡ç®—é¢„ç®—ã€‚åœ¨åŒ¹é…è®¡ç®—èµ„æº(compute-matched)åï¼ŒTroVE çš„é¢†å…ˆä¼˜åŠ¿é™è‡³ä»… 1%ï¼Œè¡¨æ˜è¿™ç§å·¥å…·ç®±æ–¹æ³•åœ¨ MATH ä»»åŠ¡ä¸Šå¹¶æœªå¸¦æ¥æ˜¾è‘—æ”¶ç›Šã€‚",
      "categories": [
        "cs.PL",
        "cs.AI"
      ],
      "primary_category": "cs.PL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22069v2",
      "published_date": "2025-07-16 03:11:43 UTC",
      "updated_date": "2025-07-31 07:33:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:10:25.791068+00:00"
    },
    {
      "arxiv_id": "2507.11848v1",
      "title": "Interactive Hybrid Rice Breeding with Parametric Dual Projection",
      "title_zh": "åŸºäºå‚æ•°åŒ–åŒé‡æŠ•å½±çš„äº¤äº’å¼æ‚äº¤æ°´ç¨»è‚²ç§",
      "authors": [
        "Changjian Chen",
        "Pengcheng Wang",
        "Fei Lyu",
        "Zhuo Tang",
        "Li Yang",
        "Long Wang",
        "Yong Cai",
        "Feng Yu",
        "Kenli Li"
      ],
      "abstract": "Hybrid rice breeding crossbreeds different rice lines and cultivates the resulting hybrids in fields to select those with desirable agronomic traits, such as higher yields. Recently, genomic selection has emerged as an efficient way for hybrid rice breeding. It predicts the traits of hybrids based on their genes, which helps exclude many undesired hybrids, largely reducing the workload of field cultivation. However, due to the limited accuracy of genomic prediction models, breeders still need to combine their experience with the models to identify regulatory genes that control traits and select hybrids, which remains a time-consuming process. To ease this process, in this paper, we proposed a visual analysis method to facilitate interactive hybrid rice breeding. Regulatory gene identification and hybrid selection naturally ensemble a dual-analysis task. Therefore, we developed a parametric dual projection method with theoretical guarantees to facilitate interactive dual analysis. Based on this dual projection method, we further developed a gene visualization and a hybrid visualization to verify the identified regulatory genes and hybrids. The effectiveness of our method is demonstrated through the quantitative evaluation of the parametric dual projection method, identified regulatory genes and desired hybrids in the case study, and positive feedback from breeders.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‚äº¤æ°´ç¨»è‚²ç§ä¸­åŸºå› ç»„é€‰æ‹©(genomic selection)æ¨¡å‹ç²¾åº¦æœ‰é™ä¸”äººå·¥ç­›é€‰ç¹ççš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºäº¤äº’å¼è‚²ç§çš„å¯è§†åˆ†ææ–¹æ³•ã€‚ç ”ç©¶çš„æ ¸å¿ƒåœ¨äºå¼€å‘äº†ä¸€ç§å…·æœ‰ç†è®ºä¿éšœçš„å‚æ•°åŒ–åŒé‡æŠ•å½±(parametric dual projection)æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡åŒé‡åˆ†æä»»åŠ¡ååŒè¯†åˆ«è°ƒèŠ‚åŸºå› å¹¶ç­›é€‰æ‚äº¤ç§ã€‚åŸºäºè¯¥æŠ•å½±æŠ€æœ¯ï¼Œç³»ç»Ÿè¿›ä¸€æ­¥æä¾›äº†åŸºå› å¯è§†åŒ–å’Œæ‚äº¤ç§å¯è§†åŒ–åŠŸèƒ½ï¼Œå¸®åŠ©è‚²ç§è€…é«˜æ•ˆéªŒè¯è¯†åˆ«å‡ºçš„å…³é”®é—ä¼ ç‰¹å¾ã€‚é€šè¿‡å¯¹å‚æ•°åŒ–åŒé‡æŠ•å½±æ–¹æ³•çš„å®šé‡è¯„ä¼°ä»¥åŠåœ¨æ¡ˆä¾‹ç ”ç©¶ä¸­æˆåŠŸè¯†åˆ«å‡ºçš„è°ƒèŠ‚åŸºå› å’Œç†æƒ³æ‚äº¤ç§ï¼Œå¹¶ç»“åˆè‚²ç§ä¸“å®¶çš„ç§¯æåé¦ˆï¼Œè¯å®äº†è¯¥æ–¹æ³•åœ¨æå‡è‚²ç§æ•ˆç‡å’Œè¾…åŠ©å†³ç­–æ–¹é¢çš„æ˜¾è‘—ä½œç”¨ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.11848v1",
      "published_date": "2025-07-16 02:25:31 UTC",
      "updated_date": "2025-07-16 02:25:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:10:29.390297+00:00"
    },
    {
      "arxiv_id": "2507.11821v1",
      "title": "MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory",
      "title_zh": "MNIST-Genï¼šåŸºäºå±‚çº§è¯­ä¹‰ã€å¼ºåŒ–å­¦ä¹ ä¸èŒƒç•´è®ºçš„æ¨¡å—åŒ– MNIST é£æ ¼æ•°æ®é›†ç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Pouya Shaeri",
        "Arash Karimi",
        "Ariane Middel"
      ],
      "abstract": "Neural networks are often benchmarked using standard datasets such as MNIST, FashionMNIST, or other variants of MNIST, which, while accessible, are limited to generic classes such as digits or clothing items. For researchers working on domain-specific tasks, such as classifying trees, food items, or other real-world objects, these data sets are insufficient and irrelevant. Additionally, creating and publishing a custom dataset can be time consuming, legally constrained, or beyond the scope of individual projects. We present MNIST-Gen, an automated, modular, and adaptive framework for generating MNIST-style image datasets tailored to user-specified categories using hierarchical semantic categorization. The system combines CLIP-based semantic understanding with reinforcement learning and human feedback to achieve intelligent categorization with minimal manual intervention. Our hierarchical approach supports complex category structures with semantic characteristics, enabling fine-grained subcategorization and multiple processing modes: individual review for maximum control, smart batch processing for large datasets, and fast batch processing for rapid creation. Inspired by category theory, MNIST-Gen models each data transformation stage as a composable morphism, enhancing clarity, modularity, and extensibility. As proof of concept, we generate and benchmark two novel datasets-\\textit{Tree-MNIST} and \\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing task-specific evaluation data while achieving 85\\% automatic categorization accuracy and 80\\% time savings compared to manual approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MNIST-Genï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–ã€æ¨¡å—åŒ–ä¸”å…·æœ‰è‡ªé€‚åº”èƒ½åŠ›çš„æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºç”Ÿæˆç¬¦åˆç”¨æˆ·ç‰¹å®šç±»åˆ«çš„ MNIST é£æ ¼å›¾åƒæ•°æ®é›†ã€‚è¯¥ç³»ç»Ÿæ—¨åœ¨è§£å†³ç°æœ‰ MNIST å˜ä½“åœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œå¹¶é™ä½æ‰‹åŠ¨åˆ›å»ºæ•°æ®é›†çš„æ—¶é—´å’Œæ³•å¾‹æˆæœ¬ã€‚æŠ€æœ¯ä¸Šï¼Œå®ƒç»“åˆäº†åŸºäº CLIP çš„è¯­ä¹‰ç†è§£ã€å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) ä¸äººç±»åé¦ˆï¼Œå®ç°äº†æä½äººå·¥å¹²é¢„ä¸‹çš„æ™ºèƒ½åˆ†ç±»ã€‚å—èŒƒç•´è®º (Category Theory) å¯å‘ï¼ŒMNIST-Gen å°†æ•°æ®è½¬æ¢é˜¶æ®µå»ºæ¨¡ä¸ºå¯ç»„åˆæ€å°„ (Composable Morphism)ï¼Œæ˜¾è‘—å¢å¼ºäº†ç³»ç»Ÿçš„æ¨¡å—åŒ–ä¸å¯æ‰©å±•æ€§ã€‚è¯¥æ¡†æ¶æ”¯æŒå±‚æ¬¡åŒ–è¯­ä¹‰åˆ†ç±»ï¼Œå¹¶æä¾›ä¸ªäººå®¡æŸ¥ã€æ™ºèƒ½æ‰¹å¤„ç†å’Œå¿«é€Ÿæ‰¹å¤„ç†ä¸‰ç§æ¨¡å¼ï¼Œä»¥å¹³è¡¡æ§åˆ¶ç²¾åº¦ä¸å¤„ç†é€Ÿåº¦ã€‚ä½œä¸ºæ¦‚å¿µéªŒè¯ï¼Œç ”ç©¶ç”Ÿæˆçš„ Tree-MNIST å’Œ Food-MNIST æ•°æ®é›†è¾¾åˆ°äº† 85% çš„è‡ªåŠ¨åˆ†ç±»å‡†ç¡®ç‡ï¼Œå¹¶ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•èŠ‚çœäº† 80% çš„æ—¶é—´ï¼Œä¸ºç‰¹å®šä»»åŠ¡çš„è¯„ä¼°æä¾›äº†é«˜æ•ˆçš„æ•°æ®ç”Ÿæˆå·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to a computer science conference",
      "pdf_url": "https://arxiv.org/pdf/2507.11821v1",
      "published_date": "2025-07-16 00:50:09 UTC",
      "updated_date": "2025-07-16 00:50:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:10:32.592270+00:00"
    },
    {
      "arxiv_id": "2507.11810v1",
      "title": "The Evolving Role of Large Language Models in Scientific Innovation: Evaluator, Collaborator, and Scientist",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦åˆ›æ–°ä¸­ä¸æ–­æ¼”è¿›çš„è§’è‰²ï¼šè¯„ä¼°è€…ã€åä½œè€…ä¸ç§‘å­¦å®¶",
      "authors": [
        "Haoxuan Zhang",
        "Ruochi Li",
        "Yang Zhang",
        "Ting Xiao",
        "Jiangping Chen",
        "Junhua Ding",
        "Haihua Chen"
      ],
      "abstract": "Scientific innovation is undergoing a paradigm shift driven by the rapid advancement of Large Language Models (LLMs). As science faces mounting challenges including information overload, disciplinary silos, and diminishing returns on conventional research methods, LLMs are emerging as powerful agents capable not only of enhancing scientific workflows but also of participating in and potentially leading the innovation process. Existing surveys mainly focus on different perspectives, phrases, and tasks in scientific research and discovery, while they have limitations in understanding the transformative potential and role differentiation of LLM. This survey proposes a comprehensive framework to categorize the evolving roles of LLMs in scientific innovation across three hierarchical levels: Evaluator, Collaborator, and Scientist. We distinguish between LLMs' contributions to structured scientific research processes and open-ended scientific discovery, thereby offering a unified taxonomy that clarifies capability boundaries, evaluation criteria, and human-AI interaction patterns at each level. Through an extensive analysis of current methodologies, benchmarks, systems, and evaluation metrics, this survey delivers an in-depth and systematic synthesis on LLM-driven scientific innovation. We present LLMs not only as tools for automating existing processes, but also as catalysts capable of reshaping the epistemological foundations of science itself. This survey offers conceptual clarity, practical guidance, and theoretical foundations for future research, while also highlighting open challenges and ethical considerations in the pursuit of increasingly autonomous AI-driven science. Resources related to this survey can be accessed on GitHub at: https://github.com/haoxuan-unt2024/llm4innovation.",
      "tldr_zh": "è¯¥ç»¼è¿°æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨ç§‘å­¦åˆ›æ–°ä¸­æ‰€å¼•å‘çš„èŒƒå¼è½¬å˜ã€‚ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŒ…å«è¯„ä¼°è€…(Evaluator)ã€åˆä½œè€…(Collaborator)å’Œç§‘å­¦å®¶(Scientist)ä¸‰ä¸ªå±‚çº§çš„ç»¼åˆæ¡†æ¶ï¼Œæ—¨åœ¨åˆ†ç±»å’Œç•Œå®šLLMsåœ¨ç§‘å­¦é¢†åŸŸä¸æ–­æ¼”å˜çš„è§’è‰²ã€‚é€šè¿‡åŒºåˆ†ç»“æ„åŒ–ç ”ç©¶è¿‡ç¨‹ä¸å¼€æ”¾å¼ç§‘å­¦å‘ç°ï¼Œæœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„åˆ†ç±»æ³•ï¼Œæ˜ç¡®äº†å„å±‚çº§çš„èƒ½åŠ›è¾¹ç•Œã€è¯„ä»·æ ‡å‡†åŠäººæœºäº¤äº’æ¨¡å¼ã€‚æ–‡ç« ç³»ç»Ÿåœ°åˆ†æäº†å½“å‰çš„æ–¹æ³•è®ºã€åŸºå‡†æµ‹è¯•(Benchmarks)ã€ç³»ç»Ÿå’Œè¯„ä¼°æŒ‡æ ‡ï¼Œå±•ç¤ºäº†LLMsä¸ä»…æ˜¯è‡ªåŠ¨åŒ–å·¥å…·ï¼Œæ›´æ˜¯é‡å¡‘ç§‘å­¦è®¤è¯†è®ºåŸºç¡€çš„å‚¬åŒ–å‰‚ã€‚è¯¥ç ”ç©¶ä¸ºæœªæ¥äººå·¥æ™ºèƒ½é©±åŠ¨çš„ç§‘å­¦ç ”ç©¶æä¾›äº†æ¦‚å¿µæ¸…æ™°åº¦ã€å®è·µæŒ‡å—å’Œç†è®ºåŸºç¡€ï¼ŒåŒæ—¶æ·±å…¥æ¢è®¨äº†åœ¨è¿½æ±‚è‡ªä¸»AIé©±åŠ¨ç§‘å­¦è¿‡ç¨‹ä¸­çš„å¼€æ”¾æŒ‘æˆ˜ä¸ä¼¦ç†è€ƒé‡ã€‚",
      "categories": [
        "cs.DL",
        "cs.AI"
      ],
      "primary_category": "cs.DL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.11810v1",
      "published_date": "2025-07-16 00:11:01 UTC",
      "updated_date": "2025-07-16 00:11:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:10:37.292487+00:00"
    },
    {
      "arxiv_id": "2507.11809v1",
      "title": "Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models",
      "title_zh": "è¿½è¸ªäº‹å®è¿˜æ˜¯ä»…ä¸ºå¤åˆ¶ï¼Ÿå¤§è¯­è¨€æ¨¡å‹å†…éƒ¨æœºåˆ¶ç«äº‰çš„æ‰¹åˆ¤æ€§æ¢ç©¶",
      "authors": [
        "Dante Campregher",
        "Yanxu Chen",
        "Sander Hoffman",
        "Maria Heuss"
      ],
      "abstract": "This paper presents a reproducibility study examining how Large Language Models (LLMs) manage competing factual and counterfactual information, focusing on the role of attention heads in this process. We attempt to reproduce and reconcile findings from three recent studies by Ortu et al., Yu, Merullo, and Pavlick and McDougall et al. that investigate the competition between model-learned facts and contradictory context information through Mechanistic Interpretability tools. Our study specifically examines the relationship between attention head strength and factual output ratios, evaluates competing hypotheses about attention heads' suppression mechanisms, and investigates the domain specificity of these attention patterns. Our findings suggest that attention heads promoting factual output do so via general copy suppression rather than selective counterfactual suppression, as strengthening them can also inhibit correct facts. Additionally, we show that attention head behavior is domain-dependent, with larger models exhibiting more specialized and category-sensitive patterns.",
      "tldr_zh": "è¯¥ç ”ç©¶æ˜¯ä¸€é¡¹é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•å¤„ç†äº‹å®ä¸åäº‹å®ä¿¡æ¯ç«äº‰çš„å¤ç°æ€§ç ”ç©¶ï¼Œé‡ç‚¹æ¢è®¨äº†æ³¨æ„åŠ›å¤´ï¼ˆattention headsï¼‰åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‚é€šè¿‡æœºæ¢°å¯è§£é‡Šæ€§ï¼ˆMechanistic Interpretabilityï¼‰å·¥å…·ï¼Œç ”ç©¶è€…å¤ç°å¹¶æ•´åˆäº†è¿‘æœŸå…³äºæ¨¡å‹å›ºæœ‰äº‹å®ä¸çŸ›ç›¾ä¸Šä¸‹æ–‡ç«äº‰çš„å¤šé¡¹ç ”ç©¶å‘ç°ã€‚ç ”ç©¶å‘ç°ï¼Œä¿ƒè¿›äº‹å®è¾“å‡ºçš„æ³¨æ„åŠ›å¤´å¹¶éé€šè¿‡é€‰æ‹©æ€§çš„åäº‹å®æŠ‘åˆ¶èµ·ä½œç”¨ï¼Œè€Œæ˜¯åˆ©ç”¨é€šç”¨çš„å¤åˆ¶æŠ‘åˆ¶ï¼ˆcopy suppressionï¼‰æœºåˆ¶ï¼Œè¿™æ„å‘³ç€å¢å¼ºè¿™äº›æ³¨æ„åŠ›å¤´æœ‰æ—¶åè€Œä¼šæŠ‘åˆ¶æ­£ç¡®äº‹å®çš„è¾“å‡ºã€‚æ­¤å¤–ï¼Œå®éªŒè¡¨æ˜æ³¨æ„åŠ›å¤´çš„è¡Œä¸ºå…·æœ‰æ˜æ˜¾çš„é¢†åŸŸä¾èµ–æ€§ï¼ˆdomain-dependentï¼‰ï¼Œä¸”å‚æ•°é‡æ›´å¤§çš„æ¨¡å‹åœ¨å¤„ç†ä¸åŒç±»åˆ«ä¿¡æ¯æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„ä¸“é—¨åŒ–æ¨¡å¼ã€‚è¯¥é¡¹å·¥ä½œä¸ºæ·±å…¥ç†è§£å¤§è¯­è¨€æ¨¡å‹çš„å†…éƒ¨äº‹å®å¤„ç†æœºåˆ¶æä¾›äº†å…³é”®çš„æ‰¹åˆ¤æ€§è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "18 Pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.11809v1",
      "published_date": "2025-07-16 00:08:48 UTC",
      "updated_date": "2025-07-16 00:08:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:10:41.893577+00:00"
    },
    {
      "arxiv_id": "2507.11807v1",
      "title": "CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for Learning with Noisy Labels",
      "title_zh": "CLID-MUï¼šåŸºäºè·¨å±‚ä¿¡æ¯æ•£åº¦çš„å™ªå£°æ ‡ç­¾å­¦ä¹ å…ƒæ›´æ–°ç­–ç•¥",
      "authors": [
        "Ruofan Hu",
        "Dongyu Zhang",
        "Huayi Zhang",
        "Elke Rundensteiner"
      ],
      "abstract": "Learning with noisy labels (LNL) is essential for training deep neural networks with imperfect data. Meta-learning approaches have achieved success by using a clean unbiased labeled set to train a robust model. However, this approach heavily depends on the availability of a clean labeled meta-dataset, which is difficult to obtain in practice. In this work, we thus tackle the challenge of meta-learning for noisy label scenarios without relying on a clean labeled dataset. Our approach leverages the data itself while bypassing the need for labels. Building on the insight that clean samples effectively preserve the consistency of related data structures across the last hidden and the final layer, whereas noisy samples disrupt this consistency, we design the Cross-layer Information Divergence-based Meta Update Strategy (CLID-MU). CLID-MU leverages the alignment of data structures across these diverse feature spaces to evaluate model performance and use this alignment to guide training. Experiments on benchmark datasets with varying amounts of labels under both synthetic and real-world noise demonstrate that CLID-MU outperforms state-of-the-art methods. The code is released at https://github.com/ruofanhu/CLID-MU.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¸¦å™ªå£°æ ‡ç­¾çš„å­¦ä¹  (Learning with noisy labels, LNL) æŒ‘æˆ˜ï¼Œæå‡ºäº† CLID-MUï¼Œä¸€ç§åŸºäºè·¨å±‚ä¿¡æ¯æ•£åº¦çš„å…ƒæ›´æ–°ç­–ç•¥ (Meta Update Strategy)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ meta-learning æ–¹æ³•è¿‡åº¦ä¾èµ–éš¾ä»¥è·å–çš„å¹²å‡€å…ƒæ•°æ®é›† (clean labeled meta-dataset) çš„é—®é¢˜ã€‚è¯¥æ–¹æ¡ˆåŸºäºä¸€ä¸ªå…³é”®è§‚å¯Ÿï¼šå¹²å‡€æ ·æœ¬åœ¨æœ€åéšè—å±‚ (last hidden layer) ä¸æœ€ç»ˆè¾“å‡ºå±‚ (final layer) ä¹‹é—´èƒ½æœ‰æ•ˆä¿æŒæ•°æ®ç»“æ„çš„ä¸€è‡´æ€§ï¼Œè€Œå™ªå£°æ ·æœ¬åˆ™ä¼šç ´åè¿™ç§ä¸€è‡´æ€§ã€‚CLID-MU æ‘†è„±äº†å¯¹å¹²å‡€æ•°æ®é›†çš„ä¾èµ–ï¼Œç›´æ¥åˆ©ç”¨æ•°æ®æœ¬èº«çš„è·¨å±‚ç‰¹å¾ç©ºé—´å¯¹é½ç¨‹åº¦æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½å¹¶æŒ‡å¯¼è®­ç»ƒè¿‡ç¨‹ã€‚åœ¨åŒ…å«åˆæˆå™ªå£°å’Œç°å®å™ªå£°çš„åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCLID-MU åœ¨ä¸åŒæ ‡ç­¾æ•°é‡ä¸‹å‡ä¼˜äºç°æœ‰çš„ state-of-the-art æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†ä¸å®Œç¾æ•°æ®æ—¶çš„é²æ£’æ€§ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "KDD 2025, 12 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.11807v1",
      "published_date": "2025-07-16 00:03:07 UTC",
      "updated_date": "2025-07-16 00:03:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:11:06.129399+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 107,
  "processed_papers_count": 107,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T05:12:02.217309+00:00"
}
[
  {
    "arxiv_id": "2401.10420v1",
    "title": "Generalized Nested Rollout Policy Adaptation with Limited Repetitions",
    "authors": [
      "Tristan Cazenave"
    ],
    "abstract": "Generalized Nested Rollout Policy Adaptation (GNRPA) is a Monte Carlo search\nalgorithm for optimizing a sequence of choices. We propose to improve on GNRPA\nby avoiding too deterministic policies that find again and again the same\nsequence of choices. We do so by limiting the number of repetitions of the best\nsequence found at a given level. Experiments show that it improves the\nalgorithm for three different combinatorial problems: Inverse RNA Folding, the\nTraveling Salesman Problem with Time Windows and the Weak Schur problem.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10420v1",
    "published_date": "2024-01-18 23:19:47 UTC",
    "updated_date": "2024-01-18 23:19:47 UTC"
  },
  {
    "arxiv_id": "2401.10415v2",
    "title": "Can Large Language Model Summarizers Adapt to Diverse Scientific Communication Goals?",
    "authors": [
      "Marcio Fonseca",
      "Shay B. Cohen"
    ],
    "abstract": "In this work, we investigate the controllability of large language models\n(LLMs) on scientific summarization tasks. We identify key stylistic and content\ncoverage factors that characterize different types of summaries such as paper\nreviews, abstracts, and lay summaries. By controlling stylistic features, we\nfind that non-fine-tuned LLMs outperform humans in the MuP review generation\ntask, both in terms of similarity to reference summaries and human preferences.\nAlso, we show that we can improve the controllability of LLMs with\nkeyword-based classifier-free guidance (CFG) while achieving lexical overlap\ncomparable to strong fine-tuned baselines on arXiv and PubMed. However, our\nresults also indicate that LLMs cannot consistently generate long summaries\nwith more than 8 sentences. Furthermore, these models exhibit limited capacity\nto produce highly abstractive lay summaries. Although LLMs demonstrate strong\ngeneric summarization competency, sophisticated content control without costly\nfine-tuning remains an open problem for domain-specific applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024 camera ready",
    "pdf_url": "http://arxiv.org/pdf/2401.10415v2",
    "published_date": "2024-01-18 23:00:54 UTC",
    "updated_date": "2024-06-27 04:00:19 UTC"
  },
  {
    "arxiv_id": "2401.10394v1",
    "title": "Distribution Consistency based Self-Training for Graph Neural Networks with Sparse Labels",
    "authors": [
      "Fali Wang",
      "Tianxiang Zhao",
      "Suhang Wang"
    ],
    "abstract": "Few-shot node classification poses a significant challenge for Graph Neural\nNetworks (GNNs) due to insufficient supervision and potential distribution\nshifts between labeled and unlabeled nodes. Self-training has emerged as a\nwidely popular framework to leverage the abundance of unlabeled data, which\nexpands the training set by assigning pseudo-labels to selected unlabeled\nnodes. Efforts have been made to develop various selection strategies based on\nconfidence, information gain, etc. However, none of these methods takes into\naccount the distribution shift between the training and testing node sets. The\npseudo-labeling step may amplify this shift and even introduce new ones,\nhindering the effectiveness of self-training. Therefore, in this work, we\nexplore the potential of explicitly bridging the distribution shift between the\nexpanded training set and test set during self-training. To this end, we\npropose a novel Distribution-Consistent Graph Self-Training (DC-GST) framework\nto identify pseudo-labeled nodes that are both informative and capable of\nredeeming the distribution discrepancy and formulate it as a differentiable\noptimization task. A distribution-shift-aware edge predictor is further adopted\nto augment the graph and increase the model's generalizability in assigning\npseudo labels. We evaluate our proposed method on four publicly available\nbenchmark datasets and extensive experiments demonstrate that our framework\nconsistently outperforms state-of-the-art baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "F.2.2; I.2.7"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by WSDM 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.10394v1",
    "published_date": "2024-01-18 22:07:48 UTC",
    "updated_date": "2024-01-18 22:07:48 UTC"
  },
  {
    "arxiv_id": "2401.10393v3",
    "title": "Natural Mitigation of Catastrophic Interference: Continual Learning in Power-Law Learning Environments",
    "authors": [
      "Atith Gandhi",
      "Raj Sanjay Shah",
      "Vijay Marupudi",
      "Sashank Varma"
    ],
    "abstract": "Neural networks often suffer from catastrophic interference (CI): performance\non previously learned tasks drops off significantly when learning a new task.\nThis contrasts strongly with humans, who can continually learn new tasks\nwithout appreciably forgetting previous tasks. Prior work has explored various\ntechniques for mitigating CI and promoting continual learning such as\nregularization, rehearsal, generative replay, and context-specific components.\nThis paper takes a different approach, one guided by cognitive science research\nshowing that in naturalistic environments, the probability of encountering a\ntask decreases as a power-law of the time since it was last performed. We argue\nthat techniques for mitigating CI should be compared against the intrinsic\nmitigation in simulated naturalistic learning environments. Thus, we evaluate\nthe extent of the natural mitigation of CI when training models in power-law\nenvironments, similar to those humans face. Our results show that natural\nrehearsal environments are better at mitigating CI than existing methods,\ncalling for the need for better evaluation processes. The benefits of this\nenvironment include simplicity, rehearsal that is agnostic to both tasks and\nmodels, and the lack of a need for extra neural circuitry. In addition, we\nexplore popular mitigation techniques in power-law environments to create new\nbaselines for continual learning research.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10393v3",
    "published_date": "2024-01-18 22:06:38 UTC",
    "updated_date": "2024-08-26 23:10:59 UTC"
  },
  {
    "arxiv_id": "2401.10379v1",
    "title": "Agricultural Object Detection with You Look Only Once (YOLO) Algorithm: A Bibliometric and Systematic Literature Review",
    "authors": [
      "Chetan M Badgujar",
      "Alwin Poulose",
      "Hao Gan"
    ],
    "abstract": "Vision is a major component in several digital technologies and tools used in\nagriculture. The object detector, You Look Only Once (YOLO), has gained\npopularity in agriculture in a relatively short span due to its\nstate-of-the-art performance. YOLO offers real-time detection with good\naccuracy and is implemented in various agricultural tasks, including\nmonitoring, surveillance, sensing, automation, and robotics. The research and\napplication of YOLO in agriculture are accelerating rapidly but are fragmented\nand multidisciplinary. Moreover, the performance characteristics (i.e.,\naccuracy, speed, computation) of the object detector influence the rate of\ntechnology implementation and adoption in agriculture. Thus, the study aims to\ncollect extensive literature to document and critically evaluate the advances\nand application of YOLO for agricultural object recognition. First, we\nconducted a bibliometric review of 257 articles to understand the scholarly\nlandscape of YOLO in agricultural domain. Secondly, we conducted a systematic\nreview of 30 articles to identify current knowledge, gaps, and modifications in\nYOLO for specific agricultural tasks. The study critically assesses and\nsummarizes the information on YOLO's end-to-end learning approach, including\ndata acquisition, processing, network modification, integration, and\ndeployment. We also discussed task-specific YOLO algorithm modification and\nintegration to meet the agricultural object or environment-specific challenges.\nIn general, YOLO-integrated digital tools and technologies show the potential\nfor real-time, automated monitoring, surveillance, and object handling to\nreduce labor, production cost, and environmental impact while maximizing\nresource efficiency. The study provides detailed documentation and\nsignificantly advances the existing knowledge on applying YOLO in agriculture,\nwhich can greatly benefit the scientific community.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10379v1",
    "published_date": "2024-01-18 21:04:25 UTC",
    "updated_date": "2024-01-18 21:04:25 UTC"
  },
  {
    "arxiv_id": "2401.10372v1",
    "title": "MutaBot: A Mutation Testing Approach for Chatbots",
    "authors": [
      "Michael Ferdinando Urrico",
      "Diego Clerissi",
      "Leonardo Mariani"
    ],
    "abstract": "Mutation testing is a technique aimed at assessing the effectiveness of test\nsuites by seeding artificial faults into programs. Although available for many\nplatforms and languages, no mutation testing tool is currently available for\nconversational chatbots, which represent an increasingly popular solution to\ndesign systems that can interact with users through a natural language\ninterface. Note that since conversations must be explicitly engineered by the\ndevelopers of conversational chatbots, these systems are exposed to specific\ntypes of faults not supported by existing mutation testing tools.\n  In this paper, we present MutaBot, a mutation testing tool for conversational\nchatbots. MutaBot addresses mutations at multiple levels, including\nconversational flows, intents, and contexts. We designed the tool to\npotentially target multiple platforms, while we implemented initial support for\nGoogle Dialogflow chatbots. We assessed the tool with three Dialogflow chatbots\nand test cases generated with Botium, revealing weaknesses in the test suites.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "5 pages, 2 figures, 2024 IEEE/ACM 46th International Conference on\n  Software Engineering: Companion Proceedings (ICSE-Companion '24)",
    "pdf_url": "http://arxiv.org/pdf/2401.10372v1",
    "published_date": "2024-01-18 20:38:27 UTC",
    "updated_date": "2024-01-18 20:38:27 UTC"
  },
  {
    "arxiv_id": "2401.10361v1",
    "title": "Hierarchical Federated Learning in Multi-hop Cluster-Based VANETs",
    "authors": [
      "M. Saeid HaghighiFard",
      "Sinem Coleri"
    ],
    "abstract": "The usage of federated learning (FL) in Vehicular Ad hoc Networks (VANET) has\ngarnered significant interest in research due to the advantages of reducing\ntransmission overhead and protecting user privacy by communicating local\ndataset gradients instead of raw data. However, implementing FL in VANETs faces\nchallenges, including limited communication resources, high vehicle mobility,\nand the statistical diversity of data distributions. In order to tackle these\nissues, this paper introduces a novel framework for hierarchical federated\nlearning (HFL) over multi-hop clustering-based VANET. The proposed method\nutilizes a weighted combination of the average relative speed and cosine\nsimilarity of FL model parameters as a clustering metric to consider both data\ndiversity and high vehicle mobility. This metric ensures convergence with\nminimum changes in cluster heads while tackling the complexities associated\nwith non-independent and identically distributed (non-IID) data scenarios.\nAdditionally, the framework includes a novel mechanism to manage seamless\ntransitions of cluster heads (CHs), followed by transferring the most recent FL\nmodel parameter to the designated CH. Furthermore, the proposed approach\nconsiders the option of merging CHs, aiming to reduce their count and,\nconsequently, mitigate associated overhead. Through extensive simulations, the\nproposed hierarchical federated learning over clustered VANET has been\ndemonstrated to improve accuracy and convergence time significantly while\nmaintaining an acceptable level of packet overhead compared to previously\nproposed clustering algorithms and non-clustered VANET.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.NI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10361v1",
    "published_date": "2024-01-18 20:05:34 UTC",
    "updated_date": "2024-01-18 20:05:34 UTC"
  },
  {
    "arxiv_id": "2401.10359v1",
    "title": "Keeping Deep Learning Models in Check: A History-Based Approach to Mitigate Overfitting",
    "authors": [
      "Hao Li",
      "Gopi Krishnan Rajbahadur",
      "Dayi Lin",
      "Cor-Paul Bezemer",
      "Zhen Ming",
      "Jiang"
    ],
    "abstract": "In software engineering, deep learning models are increasingly deployed for\ncritical tasks such as bug detection and code review. However, overfitting\nremains a challenge that affects the quality, reliability, and trustworthiness\nof software systems that utilize deep learning models. Overfitting can be (1)\nprevented (e.g., using dropout or early stopping) or (2) detected in a trained\nmodel (e.g., using correlation-based approaches). Both overfitting detection\nand prevention approaches that are currently used have constraints (e.g.,\nrequiring modification of the model structure, and high computing resources).\nIn this paper, we propose a simple, yet powerful approach that can both detect\nand prevent overfitting based on the training history (i.e., validation\nlosses). Our approach first trains a time series classifier on training\nhistories of overfit models. This classifier is then used to detect if a\ntrained model is overfit. In addition, our trained classifier can be used to\nprevent overfitting by identifying the optimal point to stop a model's\ntraining. We evaluate our approach on its ability to identify and prevent\noverfitting in real-world samples. We compare our approach against\ncorrelation-based detection approaches and the most commonly used prevention\napproach (i.e., early stopping). Our approach achieves an F1 score of 0.91\nwhich is at least 5% higher than the current best-performing non-intrusive\noverfitting detection approach. Furthermore, our approach can stop training to\navoid overfitting at least 32% of the times earlier than early stopping and has\nthe same or a better rate of returning the best model.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10359v1",
    "published_date": "2024-01-18 19:56:27 UTC",
    "updated_date": "2024-01-18 19:56:27 UTC"
  },
  {
    "arxiv_id": "2401.10348v1",
    "title": "Exploring General Intelligence via Gated Graph Transformer in Functional Connectivity Studies",
    "authors": [
      "Gang Qu",
      "Anton Orlichenko",
      "Junqi Wang",
      "Gemeng Zhang",
      "Li Xiao",
      "Aiying Zhang",
      "Zhengming Ding",
      "Yu-Ping Wang"
    ],
    "abstract": "Functional connectivity (FC) as derived from fMRI has emerged as a pivotal\ntool in elucidating the intricacies of various psychiatric disorders and\ndelineating the neural pathways that underpin cognitive and behavioral dynamics\ninherent to the human brain. While Graph Neural Networks (GNNs) offer a\nstructured approach to represent neuroimaging data, they are limited by their\nneed for a predefined graph structure to depict associations between brain\nregions, a detail not solely provided by FCs. To bridge this gap, we introduce\nthe Gated Graph Transformer (GGT) framework, designed to predict cognitive\nmetrics based on FCs. Empirical validation on the Philadelphia\nNeurodevelopmental Cohort (PNC) underscores the superior predictive prowess of\nour model, further accentuating its potential in identifying pivotal neural\nconnectivities that correlate with human cognitive processes.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10348v1",
    "published_date": "2024-01-18 19:28:26 UTC",
    "updated_date": "2024-01-18 19:28:26 UTC"
  },
  {
    "arxiv_id": "2401.10341v1",
    "title": "ELRT: Efficient Low-Rank Training for Compact Convolutional Neural Networks",
    "authors": [
      "Yang Sui",
      "Miao Yin",
      "Yu Gong",
      "Jinqi Xiao",
      "Huy Phan",
      "Bo Yuan"
    ],
    "abstract": "Low-rank compression, a popular model compression technique that produces\ncompact convolutional neural networks (CNNs) with low rankness, has been\nwell-studied in the literature. On the other hand, low-rank training, as an\nalternative way to train low-rank CNNs from scratch, has been exploited little\nyet. Unlike low-rank compression, low-rank training does not need pre-trained\nfull-rank models, and the entire training phase is always performed on the\nlow-rank structure, bringing attractive benefits for practical applications.\nHowever, the existing low-rank training solutions still face several\nchallenges, such as a considerable accuracy drop and/or still needing to update\nfull-size models during the training. In this paper, we perform a systematic\ninvestigation on low-rank CNN training. By identifying the proper low-rank\nformat and performance-improving strategy, we propose ELRT, an efficient\nlow-rank training solution for high-accuracy, high-compactness, low-rank CNN\nmodels. Our extensive evaluation results for training various CNNs on different\ndatasets demonstrate the effectiveness of ELRT.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10341v1",
    "published_date": "2024-01-18 19:09:47 UTC",
    "updated_date": "2024-01-18 19:09:47 UTC"
  },
  {
    "arxiv_id": "2401.10337v3",
    "title": "Noise Contrastive Estimation-based Matching Framework for Low-Resource Security Attack Pattern Recognition",
    "authors": [
      "Tu Nguyen",
      "Nedim Šrndić",
      "Alexander Neth"
    ],
    "abstract": "Tactics, Techniques and Procedures (TTPs) represent sophisticated attack\npatterns in the cybersecurity domain, described encyclopedically in textual\nknowledge bases. Identifying TTPs in cybersecurity writing, often called TTP\nmapping, is an important and challenging task. Conventional learning approaches\noften target the problem in the classical multi-class or multilabel\nclassification setting. This setting hinders the learning ability of the model\ndue to a large number of classes (i.e., TTPs), the inevitable skewness of the\nlabel distribution and the complex hierarchical structure of the label space.\nWe formulate the problem in a different learning paradigm, where the assignment\nof a text to a TTP label is decided by the direct semantic similarity between\nthe two, thus reducing the complexity of competing solely over the large\nlabeling space. To that end, we propose a neural matching architecture with an\neffective sampling-based learn-to-compare mechanism, facilitating the learning\nprocess of the matching model despite constrained resources.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted at EACL 2024, in ARR October 2023",
    "pdf_url": "http://arxiv.org/pdf/2401.10337v3",
    "published_date": "2024-01-18 19:02:00 UTC",
    "updated_date": "2024-01-30 11:40:07 UTC"
  },
  {
    "arxiv_id": "2401.10316v1",
    "title": "Improving One-class Recommendation with Multi-tasking on Various Preference Intensities",
    "authors": [
      "Chu-Jen Shao",
      "Hao-Ming Fu",
      "Pu-Jen Cheng"
    ],
    "abstract": "In the one-class recommendation problem, it's required to make\nrecommendations basing on users' implicit feedback, which is inferred from\ntheir action and inaction. Existing works obtain representations of users and\nitems by encoding positive and negative interactions observed from training\ndata. However, these efforts assume that all positive signals from implicit\nfeedback reflect a fixed preference intensity, which is not realistic.\nConsequently, representations learned with these methods usually fail to\ncapture informative entity features that reflect various preference\nintensities.\n  In this paper, we propose a multi-tasking framework taking various preference\nintensities of each signal from implicit feedback into consideration.\nRepresentations of entities are required to satisfy the objective of each\nsubtask simultaneously, making them more robust and generalizable. Furthermore,\nwe incorporate attentive graph convolutional layers to explore high-order\nrelationships in the user-item bipartite graph and dynamically capture the\nlatent tendencies of users toward the items they interact with. Experimental\nresults show that our method performs better than state-of-the-art methods by a\nlarge margin on three large-scale real-world benchmark datasets.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "RecSys 2020 (ACM Conference on Recommender Systems 2020)",
    "pdf_url": "http://arxiv.org/pdf/2401.10316v1",
    "published_date": "2024-01-18 18:59:55 UTC",
    "updated_date": "2024-01-18 18:59:55 UTC"
  },
  {
    "arxiv_id": "2401.10225v5",
    "title": "ChatQA: Surpassing GPT-4 on Conversational QA and RAG",
    "authors": [
      "Zihan Liu",
      "Wei Ping",
      "Rajarshi Roy",
      "Peng Xu",
      "Chankyu Lee",
      "Mohammad Shoeybi",
      "Bryan Catanzaro"
    ],
    "abstract": "In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on\nretrieval-augmented generation (RAG) and conversational question answering\n(QA). To enhance generation, we propose a two-stage instruction tuning method\nthat significantly boosts the performance of RAG. For effective retrieval, we\nintroduce a dense retriever optimized for conversational QA, which yields\nresults comparable to the alternative state-of-the-art query rewriting models,\nwhile substantially reducing deployment costs. We also present the ChatRAG\nBench, which encompasses ten datasets covering comprehensive evaluations on\nRAG, table-related QA, arithmetic calculations, and scenarios involving\nunanswerable questions. Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a\nweaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score:\n53.90) and GPT-4-Turbo-2024-04-09 (score: 54.03) on the ChatRAG Bench, without\nrelying on any synthetic data from OpenAI GPT models. Notably, the\nLlama3-ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09,\nachieving a 4.4% improvement. To advance research in this field, we\nopen-sourced the model weights, instruction tuning data, ChatRAG Bench, and\nretriever for the community: https://chatqa-project.github.io/.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.10225v5",
    "published_date": "2024-01-18 18:59:11 UTC",
    "updated_date": "2024-10-30 02:58:14 UTC"
  },
  {
    "arxiv_id": "2401.10222v2",
    "title": "Supervised Fine-tuning in turn Improves Visual Foundation Models",
    "authors": [
      "Xiaohu Jiang",
      "Yixiao Ge",
      "Yuying Ge",
      "Dachuan Shi",
      "Chun Yuan",
      "Ying Shan"
    ],
    "abstract": "Image-text training like CLIP has dominated the pretraining of vision\nfoundation models in recent years. Subsequent efforts have been made to\nintroduce region-level visual learning into CLIP's pretraining but face\nscalability challenges due to the lack of large-scale region-level datasets.\nDrawing inspiration from supervised fine-tuning (SFT) in natural language\nprocessing such as instruction tuning, we explore the potential of fine-grained\nSFT in enhancing the generation of vision foundation models after their\npretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash\nthe fine-grained knowledge of vision foundation models. In ViSFT, the vision\nfoundation model is enhanced by performing visual joint learning on some\nin-domain tasks and then tested on out-of-domain benchmarks. With updating\nusing ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over\n4.4B parameters shows improvements across various out-of-domain benchmarks\nincluding vision and vision-linguistic scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "23 pages, 3 figures, Project page:\n  https://github.com/TencentARC/ViSFT/tree/main",
    "pdf_url": "http://arxiv.org/pdf/2401.10222v2",
    "published_date": "2024-01-18 18:58:54 UTC",
    "updated_date": "2024-04-11 17:59:42 UTC"
  },
  {
    "arxiv_id": "2401.10314v2",
    "title": "LangProp: A code optimization framework using Large Language Models applied to driving",
    "authors": [
      "Shu Ishida",
      "Gianluca Corrado",
      "George Fedoseev",
      "Hudson Yeo",
      "Lloyd Russell",
      "Jamie Shotton",
      "João F. Henriques",
      "Anthony Hu"
    ],
    "abstract": "We propose LangProp, a framework for iteratively optimizing code generated by\nlarge language models (LLMs), in both supervised and reinforcement learning\nsettings. While LLMs can generate sensible coding solutions zero-shot, they are\noften sub-optimal. Especially for code generation tasks, it is likely that the\ninitial code will fail on certain edge cases. LangProp automatically evaluates\nthe code performance on a dataset of input-output pairs, catches any\nexceptions, and feeds the results back to the LLM in the training loop, so that\nthe LLM can iteratively improve the code it generates. By adopting a metric-\nand data-driven training paradigm for this code optimization procedure, one\ncould easily adapt findings from traditional machine learning techniques such\nas imitation learning, DAgger, and reinforcement learning. We show LangProp's\napplicability to general domains such as Sudoku and CartPole, as well as\ndemonstrate the first proof of concept of automated code optimization for\nautonomous driving in CARLA. We show that LangProp can generate interpretable\nand transparent policies that can be verified and improved in a metric- and\ndata-driven way. Our code is available at\nhttps://github.com/shuishida/LangProp.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10314v2",
    "published_date": "2024-01-18 18:52:06 UTC",
    "updated_date": "2024-05-03 16:15:45 UTC"
  },
  {
    "arxiv_id": "2401.10207v1",
    "title": "Eclectic Rule Extraction for Explainability of Deep Neural Network based Intrusion Detection Systems",
    "authors": [
      "Jesse Ables",
      "Nathaniel Childers",
      "William Anderson",
      "Sudip Mittal",
      "Shahram Rahimi",
      "Ioana Banicescu",
      "Maria Seale"
    ],
    "abstract": "This paper addresses trust issues created from the ubiquity of black box\nalgorithms and surrogate explainers in Explainable Intrusion Detection Systems\n(X-IDS). While Explainable Artificial Intelligence (XAI) aims to enhance\ntransparency, black box surrogate explainers, such as Local Interpretable\nModel-Agnostic Explanation (LIME) and SHapley Additive exPlanation (SHAP), are\ndifficult to trust. The black box nature of these surrogate explainers makes\nthe process behind explanation generation opaque and difficult to understand.\nTo avoid this problem, one can use transparent white box algorithms such as\nRule Extraction (RE). There are three types of RE algorithms: pedagogical,\ndecompositional, and eclectic. Pedagogical methods offer fast but untrustworthy\nwhite-box explanations, while decompositional RE provides trustworthy\nexplanations with poor scalability. This work explores eclectic rule\nextraction, which strikes a balance between scalability and trustworthiness. By\ncombining techniques from pedagogical and decompositional approaches, eclectic\nrule extraction leverages the advantages of both, while mitigating some of\ntheir drawbacks. The proposed Hybrid X-IDS architecture features eclectic RE as\na white box surrogate explainer for black box Deep Neural Networks (DNN). The\npresented eclectic RE algorithm extracts human-readable rules from hidden\nlayers, facilitating explainable and trustworthy rulesets. Evaluations on\nUNSW-NB15 and CIC-IDS-2017 datasets demonstrate the algorithm's ability to\ngenerate rulesets with 99.9% accuracy, mimicking DNN outputs. The contributions\nof this work include the hybrid X-IDS architecture, the eclectic rule\nextraction algorithm applicable to intrusion detection datasets, and a thorough\nanalysis of performance and explainability, demonstrating the trade-offs\ninvolved in rule extraction speed and accuracy.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10207v1",
    "published_date": "2024-01-18 18:45:29 UTC",
    "updated_date": "2024-01-18 18:45:29 UTC"
  },
  {
    "arxiv_id": "2401.10189v4",
    "title": "Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction",
    "authors": [
      "Qingyun Wang",
      "Zixuan Zhang",
      "Hongxiang Li",
      "Xuan Liu",
      "Jiawei Han",
      "Huimin Zhao",
      "Heng Ji"
    ],
    "abstract": "Fine-grained few-shot entity extraction in the chemical domain faces two\nunique challenges. First, compared with entity extraction tasks in the general\ndomain, sentences from chemical papers usually contain more entities. Moreover,\nentity extraction models usually have difficulty extracting entities of\nlong-tailed types. In this paper, we propose Chem-FINESE, a novel\nsequence-to-sequence (seq2seq) based few-shot entity extraction approach, to\naddress these two challenges. Our Chem-FINESE has two components: a seq2seq\nentity extractor to extract named entities from the input sentence and a\nseq2seq self-validation module to reconstruct the original input sentence from\nextracted entities. Inspired by the fact that a good entity extraction system\nneeds to extract entities faithfully, our new self-validation module leverages\nentity extraction results to reconstruct the original input sentence. Besides,\nwe design a new contrastive loss to reduce excessive copying during the\nextraction process. Finally, we release ChemNER+, a new fine-grained chemical\nentity extraction dataset that is annotated by domain experts with the ChemNER\nschema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasets\nshow that our newly proposed framework has contributed up to 8.26% and 6.84%\nabsolute F1-score gains respectively.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages. Accepted by Findings of the Association for Computational\n  Linguistics: EACL 2024. Code and resources are available at\n  https://github.com/EagleW/Chem-FINESE",
    "pdf_url": "http://arxiv.org/pdf/2401.10189v4",
    "published_date": "2024-01-18 18:20:15 UTC",
    "updated_date": "2024-05-29 18:24:15 UTC"
  },
  {
    "arxiv_id": "2401.12996v1",
    "title": "A Comparison of Veterans with Problematic Opioid Use Identified through Natural Language Processing of Clinical Notes versus Using Diagnostic Codes",
    "authors": [
      "Terri Elizabeth Workman",
      "Joel Kupersmith",
      "Phillip Ma",
      "Christopher Spevak",
      "Friedhelm Sandbrink",
      "Yan Cheng Qing Zeng-Treitler"
    ],
    "abstract": "Background: Electronic health records (EHRs) are a data source for opioid\nresearch. Opioid use disorder is known to be under-coded as a diagnosis, yet\nproblematic opioid use can be documented in clinical notes.\n  Objectives: Our goals were 1) to identify problematic opioid use from a full\nrange of clinical notes; and 2) to compare the characteristics of patients\nidentified as having problematic opioid use, exclusively documented in clinical\nnotes, to those having documented ICD opioid use disorder diagnostic codes.\n  Materials and Methods: We developed and applied a natural language processing\n(NLP) tool to the clinical notes of a patient cohort (n=222,371) from two\nVeteran Affairs service regions to identify patients with problematic opioid\nuse. We also used a set of ICD diagnostic codes to identify patients with\nopioid use disorder from the same cohort. We compared the demographic and\nclinical characteristics of patients identified only through NLP, to those of\npatients identified through ICD codes.\n  Results: NLP exclusively identified 57,331 patients; 6,997 patients had\npositive ICD code identifications. Patients exclusively identified through NLP\nwere more likely to be women. Those identified through ICD codes were more\nlikely to be male, younger, have concurrent benzodiazepine prescriptions, more\ncomorbidities, more care encounters, and less likely to be married. Patients in\nthe NLP and ICD groups had substantially elevated comorbidity levels compared\nto patients not documented as experiencing problematic opioid use.\n  Conclusions: NLP is a feasible approach for identifying problematic opioid\nuse not otherwise recorded by ICD codes. Clinicians may be reluctant to code\nfor opioid use disorder. It is therefore incumbent on the healthcare team to\nsearch for documentation of opioid concerns within clinical notes.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "J.3"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 4 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.12996v1",
    "published_date": "2024-01-18 18:08:16 UTC",
    "updated_date": "2024-01-18 18:08:16 UTC"
  },
  {
    "arxiv_id": "2401.10178v1",
    "title": "Neural Echos: Depthwise Convolutional Filters Replicate Biological Receptive Fields",
    "authors": [
      "Zahra Babaiee",
      "Peyman M. Kiasari",
      "Daniela Rus",
      "Radu Grosu"
    ],
    "abstract": "In this study, we present evidence suggesting that depthwise convolutional\nkernels are effectively replicating the structural intricacies of the\nbiological receptive fields observed in the mammalian retina. We provide\nanalytics of trained kernels from various state-of-the-art models\nsubstantiating this evidence. Inspired by this intriguing discovery, we propose\nan initialization scheme that draws inspiration from the biological receptive\nfields. Experimental analysis of the ImageNet dataset with multiple CNN\narchitectures featuring depthwise convolutions reveals a marked enhancement in\nthe accuracy of the learned model when initialized with biologically derived\nweights. This underlies the potential for biologically inspired computational\nmodels to further our understanding of vision processing systems and to improve\nthe efficacy of convolutional networks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10178v1",
    "published_date": "2024-01-18 18:06:22 UTC",
    "updated_date": "2024-01-18 18:06:22 UTC"
  },
  {
    "arxiv_id": "2401.10148v2",
    "title": "Explicitly Disentangled Representations in Object-Centric Learning",
    "authors": [
      "Riccardo Majellaro",
      "Jonathan Collu",
      "Aske Plaat",
      "Thomas M. Moerland"
    ],
    "abstract": "Extracting structured representations from raw visual data is an important\nand long-standing challenge in machine learning. Recently, techniques for\nunsupervised learning of object-centric representations have raised growing\ninterest. In this context, enhancing the robustness of the latent features can\nimprove the efficiency and effectiveness of the training of downstream tasks. A\npromising step in this direction is to disentangle the factors that cause\nvariation in the data. Previously, Invariant Slot Attention disentangled\nposition, scale, and orientation from the remaining features. Extending this\napproach, we focus on separating the shape and texture components. In\nparticular, we propose a novel architecture that biases object-centric models\ntoward disentangling shape and texture components into two non-overlapping\nsubsets of the latent space dimensions. These subsets are known a priori, hence\nbefore the training process. Experiments on a range of object-centric\nbenchmarks reveal that our approach achieves the desired disentanglement while\nalso numerically improving baseline performance in most cases. In addition, we\nshow that our method can generate novel textures for a specific object or\ntransfer textures between objects with distinct shapes.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Published in TMLR",
    "pdf_url": "http://arxiv.org/pdf/2401.10148v2",
    "published_date": "2024-01-18 17:22:11 UTC",
    "updated_date": "2025-01-23 10:22:02 UTC"
  },
  {
    "arxiv_id": "2401.10139v1",
    "title": "Model Compression Techniques in Biometrics Applications: A Survey",
    "authors": [
      "Eduarda Caldeira",
      "Pedro C. Neto",
      "Marco Huber",
      "Naser Damer",
      "Ana F. Sequeira"
    ],
    "abstract": "The development of deep learning algorithms has extensively empowered\nhumanity's task automatization capacity. However, the huge improvement in the\nperformance of these models is highly correlated with their increasing level of\ncomplexity, limiting their usefulness in human-oriented applications, which are\nusually deployed in resource-constrained devices. This led to the development\nof compression techniques that drastically reduce the computational and memory\ncosts of deep learning models without significant performance degradation. This\npaper aims to systematize the current literature on this topic by presenting a\ncomprehensive survey of model compression techniques in biometrics\napplications, namely quantization, knowledge distillation and pruning. We\nconduct a critical analysis of the comparative value of these techniques,\nfocusing on their advantages and disadvantages and presenting suggestions for\nfuture work directions that can potentially improve the current methods.\nAdditionally, we discuss and analyze the link between model bias and model\ncompression, highlighting the need to direct compression research toward model\nfairness in future works.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Under review at IEEE Journal",
    "pdf_url": "http://arxiv.org/pdf/2401.10139v1",
    "published_date": "2024-01-18 17:06:21 UTC",
    "updated_date": "2024-01-18 17:06:21 UTC"
  },
  {
    "arxiv_id": "2401.10119v4",
    "title": "Towards Principled Graph Transformers",
    "authors": [
      "Luis Müller",
      "Daniel Kusuma",
      "Blai Bonet",
      "Christopher Morris"
    ],
    "abstract": "Graph learning architectures based on the k-dimensional Weisfeiler-Leman\n(k-WL) hierarchy offer a theoretically well-understood expressive power.\nHowever, such architectures often fail to deliver solid predictive performance\non real-world tasks, limiting their practical impact. In contrast, global\nattention-based models such as graph transformers demonstrate strong\nperformance in practice, but comparing their expressive power with the k-WL\nhierarchy remains challenging, particularly since these architectures rely on\npositional or structural encodings for their expressivity and predictive\nperformance. To address this, we show that the recently proposed Edge\nTransformer, a global attention model operating on node pairs instead of nodes,\nhas at least 3-WL expressive power. Empirically, we demonstrate that the Edge\nTransformer surpasses other theoretically aligned architectures regarding\npredictive performance while not relying on positional or structural encodings.\nOur code is available at https://github.com/luis-mueller/towards-principled-gts",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.10119v4",
    "published_date": "2024-01-18 16:50:55 UTC",
    "updated_date": "2024-11-08 10:06:06 UTC"
  },
  {
    "arxiv_id": "2401.10101v2",
    "title": "Bayesian Networks for Causal Analysis in Socioecological Systems",
    "authors": [
      "Rafael Cabañas",
      "Ana D. Maldonado",
      "María Morales",
      "Pedro A. Aguilera",
      "Antonio Salmerón"
    ],
    "abstract": "Causal and counterfactual reasoning are emerging directions in data science\nthat allow us to reason about hypothetical scenarios. This is particularly\nuseful in fields like environmental and ecological sciences, where\ninterventional data are usually not available. Structural causal models are\nprobabilistic models for causal analysis that simplify this kind of reasoning\ndue to their graphical representation. They can be regarded as extensions of\nthe so-called Bayesian networks, a well known modeling tool commonly used in\nenvironmental and ecological problems. The main contribution of this paper is\nto analyze the relations of necessity and sufficiency between the variables of\na socioecological system using counterfactual reasoning with Bayesian networks.\nIn particular, we consider a case study involving socioeconomic factors and\nland-uses in southern Spain. In addition, this paper aims to be a coherent\noverview of the fundamental concepts for applying counterfactual reasoning, so\nthat environmental researchers with a background in Bayesian networks can\neasily take advantage of the structural causal model formalism.",
    "categories": [
      "cs.AI",
      "math.PR",
      "stat.AP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10101v2",
    "published_date": "2024-01-18 16:10:07 UTC",
    "updated_date": "2024-12-05 10:06:43 UTC"
  },
  {
    "arxiv_id": "2401.10310v1",
    "title": "Mathematical Algorithm Design for Deep Learning under Societal and Judicial Constraints: The Algorithmic Transparency Requirement",
    "authors": [
      "Holger Boche",
      "Adalbert Fono",
      "Gitta Kutyniok"
    ],
    "abstract": "Deep learning still has drawbacks in terms of trustworthiness, which\ndescribes a comprehensible, fair, safe, and reliable method. To mitigate the\npotential risk of AI, clear obligations associated to trustworthiness have been\nproposed via regulatory guidelines, e.g., in the European AI Act. Therefore, a\ncentral question is to what extent trustworthy deep learning can be realized.\nEstablishing the described properties constituting trustworthiness requires\nthat the factors influencing an algorithmic computation can be retraced, i.e.,\nthe algorithmic implementation is transparent. Motivated by the observation\nthat the current evolution of deep learning models necessitates a change in\ncomputing technology, we derive a mathematical framework which enables us to\nanalyze whether a transparent implementation in a computing model is feasible.\nWe exemplarily apply our trustworthiness framework to analyze deep learning\napproaches for inverse problems in digital and analog computing models\nrepresented by Turing and Blum-Shub-Smale Machines, respectively. Based on\nprevious results, we find that Blum-Shub-Smale Machines have the potential to\nestablish trustworthy solvers for inverse problems under fairly general\nconditions, whereas Turing machines cannot guarantee trustworthiness to the\nsame degree.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10310v1",
    "published_date": "2024-01-18 15:32:38 UTC",
    "updated_date": "2024-01-18 15:32:38 UTC"
  },
  {
    "arxiv_id": "2401.10061v1",
    "title": "DiffusionGPT: LLM-Driven Text-to-Image Generation System",
    "authors": [
      "Jie Qin",
      "Jie Wu",
      "Weifeng Chen",
      "Yuxi Ren",
      "Huixia Li",
      "Hefeng Wu",
      "Xuefeng Xiao",
      "Rui Wang",
      "Shilei Wen"
    ],
    "abstract": "Diffusion models have opened up new avenues for the field of image\ngeneration, resulting in the proliferation of high-quality models shared on\nopen-source platforms. However, a major challenge persists in current\ntext-to-image systems are often unable to handle diverse inputs, or are limited\nto single model results. Current unified attempts often fall into two\northogonal aspects: i) parse Diverse Prompts in input stage; ii) activate\nexpert model to output. To combine the best of both worlds, we propose\nDiffusionGPT, which leverages Large Language Models (LLM) to offer a unified\ngeneration system capable of seamlessly accommodating various types of prompts\nand integrating domain-expert models. DiffusionGPT constructs domain-specific\nTrees for various generative models based on prior knowledge. When provided\nwith an input, the LLM parses the prompt and employs the Trees-of-Thought to\nguide the selection of an appropriate model, thereby relaxing input constraints\nand ensuring exceptional performance across diverse domains. Moreover, we\nintroduce Advantage Databases, where the Tree-of-Thought is enriched with human\nfeedback, aligning the model selection process with human preferences. Through\nextensive experiments and comparisons, we demonstrate the effectiveness of\nDiffusionGPT, showcasing its potential for pushing the boundaries of image\nsynthesis in diverse domains.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10061v1",
    "published_date": "2024-01-18 15:30:58 UTC",
    "updated_date": "2024-01-18 15:30:58 UTC"
  },
  {
    "arxiv_id": "2401.10040v1",
    "title": "Large Language Models for Scientific Information Extraction: An Empirical Study for Virology",
    "authors": [
      "Mahsa Shamsabadi",
      "Jennifer D'Souza",
      "Sören Auer"
    ],
    "abstract": "In this paper, we champion the use of structured and semantic content\nrepresentation of discourse-based scholarly communication, inspired by tools\nlike Wikipedia infoboxes or structured Amazon product descriptions. These\nrepresentations provide users with a concise overview, aiding scientists in\nnavigating the dense academic landscape. Our novel automated approach leverages\nthe robust text generation capabilities of LLMs to produce structured scholarly\ncontribution summaries, offering both a practical solution and insights into\nLLMs' emergent abilities.\n  For LLMs, the prime focus is on improving their general intelligence as\nconversational agents. We argue that these models can also be applied\neffectively in information extraction (IE), specifically in complex IE tasks\nwithin terse domains like Science. This paradigm shift replaces the traditional\nmodular, pipelined machine learning approach with a simpler objective expressed\nthrough instructions. Our results show that finetuned FLAN-T5 with 1000x fewer\nparameters than the state-of-the-art GPT-davinci is competitive for the task.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 6 figures, Accepted as Findings of the ACL: EACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.10040v1",
    "published_date": "2024-01-18 15:04:55 UTC",
    "updated_date": "2024-01-18 15:04:55 UTC"
  },
  {
    "arxiv_id": "2401.10036v2",
    "title": "LOCALINTEL: Generating Organizational Threat Intelligence from Global and Local Cyber Knowledge",
    "authors": [
      "Shaswata Mitra",
      "Subash Neupane",
      "Trisha Chakraborty",
      "Sudip Mittal",
      "Aritran Piplai",
      "Manas Gaur",
      "Shahram Rahimi"
    ],
    "abstract": "Security Operations Center (SoC) analysts gather threat reports from openly\naccessible global threat repositories and tailor the information to their\norganization's needs, such as developing threat intelligence and security\npolicies. They also depend on organizational internal repositories, which act\nas private local knowledge database. These local knowledge databases store\ncredible cyber intelligence, critical operational and infrastructure details.\nSoCs undertake a manual labor-intensive task of utilizing these global threat\nrepositories and local knowledge databases to create both organization-specific\nthreat intelligence and mitigation policies. Recently, Large Language Models\n(LLMs) have shown the capability to process diverse knowledge sources\nefficiently. We leverage this ability to automate this organization-specific\nthreat intelligence generation. We present LocalIntel, a novel automated threat\nintelligence contextualization framework that retrieves zero-day vulnerability\nreports from the global threat repositories and uses its local knowledge\ndatabase to determine implications and mitigation strategies to alert and\nassist the SoC analyst. LocalIntel comprises two key phases: knowledge\nretrieval and contextualization. Quantitative and qualitative assessment has\nshown effectiveness in generating up to 93% accurate organizational threat\nintelligence with 64% inter-rater agreement.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.IR",
      "cs.LO"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10036v2",
    "published_date": "2024-01-18 15:00:01 UTC",
    "updated_date": "2025-02-09 20:56:46 UTC"
  },
  {
    "arxiv_id": "2401.10034v3",
    "title": "Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap",
    "authors": [
      "Xingyu Wu",
      "Sheng-hao Wu",
      "Jibin Wu",
      "Liang Feng",
      "Kay Chen Tan"
    ],
    "abstract": "Large language models (LLMs) have not only revolutionized natural language\nprocessing but also extended their prowess to various domains, marking a\nsignificant stride towards artificial general intelligence. The interplay\nbetween LLMs and evolutionary algorithms (EAs), despite differing in objectives\nand methodologies, share a common pursuit of applicability in complex problems.\nMeanwhile, EA can provide an optimization framework for LLM's further\nenhancement under black-box settings, empowering LLM with flexible global\nsearch capacities. On the other hand, the abundant domain knowledge inherent in\nLLMs could enable EA to conduct more intelligent searches. Furthermore, the\ntext processing and generative capabilities of LLMs would aid in deploying EAs\nacross a wide range of tasks. Based on these complementary advantages, this\npaper provides a thorough review and a forward-looking roadmap, categorizing\nthe reciprocal inspiration into two main avenues: LLM-enhanced EA and\nEA-enhanced LLM. Some integrated synergy methods are further introduced to\nexemplify the complementarity between LLMs and EAs in diverse scenarios,\nincluding code generation, software engineering, neural architecture search,\nand various generation tasks. As the first comprehensive review focused on the\nEA research in the era of LLMs, this paper provides a foundational stepping\nstone for understanding the collaborative potential of LLMs and EAs. The\nidentified challenges and future directions offer guidance for researchers and\npractitioners to unlock the full potential of this innovative collaboration in\npropelling advancements in optimization and artificial intelligence. We have\ncreated a GitHub repository to index the relevant papers:\nhttps://github.com/wuxingyu-ai/LLM4EC.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.NE",
    "comment": "evolutionary algorithm (EA), large language model (LLM), optimization\n  problem, prompt engineering, algorithm generation, neural architecture search",
    "pdf_url": "http://arxiv.org/pdf/2401.10034v3",
    "published_date": "2024-01-18 14:58:17 UTC",
    "updated_date": "2024-05-29 09:00:25 UTC"
  },
  {
    "arxiv_id": "2401.10032v1",
    "title": "FreGrad: Lightweight and Fast Frequency-aware Diffusion Vocoder",
    "authors": [
      "Tan Dat Nguyen",
      "Ji-Hoon Kim",
      "Youngjoon Jang",
      "Jaehun Kim",
      "Joon Son Chung"
    ],
    "abstract": "The goal of this paper is to generate realistic audio with a lightweight and\nfast diffusion-based vocoder, named FreGrad. Our framework consists of the\nfollowing three key components: (1) We employ discrete wavelet transform that\ndecomposes a complicated waveform into sub-band wavelets, which helps FreGrad\nto operate on a simple and concise feature space, (2) We design a\nfrequency-aware dilated convolution that elevates frequency awareness,\nresulting in generating speech with accurate frequency information, and (3) We\nintroduce a bag of tricks that boosts the generation quality of the proposed\nmodel. In our experiments, FreGrad achieves 3.7 times faster training time and\n2.2 times faster inference speed compared to our baseline while reducing the\nmodel size by 0.6 times (only 1.78M parameters) without sacrificing the output\nquality. Audio samples are available at:\nhttps://mm.kaist.ac.kr/projects/FreGrad.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted to ICASSP 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.10032v1",
    "published_date": "2024-01-18 14:57:25 UTC",
    "updated_date": "2024-01-18 14:57:25 UTC"
  },
  {
    "arxiv_id": "2401.10020v3",
    "title": "Self-Rewarding Language Models",
    "authors": [
      "Weizhe Yuan",
      "Richard Yuanzhe Pang",
      "Kyunghyun Cho",
      "Xian Li",
      "Sainbayar Sukhbaatar",
      "Jing Xu",
      "Jason Weston"
    ],
    "abstract": "We posit that to achieve superhuman agents, future models require superhuman\nfeedback in order to provide an adequate training signal. Current approaches\ncommonly train reward models from human preferences, which may then be\nbottlenecked by human performance level, and secondly these separate frozen\nreward models cannot then learn to improve during LLM training. In this work,\nwe study Self-Rewarding Language Models, where the language model itself is\nused via LLM-as-a-Judge prompting to provide its own rewards during training.\nWe show that during Iterative DPO training that not only does instruction\nfollowing ability improve, but also the ability to provide high-quality rewards\nto itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a\nmodel that outperforms many existing systems on the AlpacaEval 2.0 leaderboard,\nincluding Claude 2, Gemini Pro, and GPT-4 0613. While there is much left still\nto explore, this work opens the door to the possibility of models that can\ncontinually improve in both axes.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.10020v3",
    "published_date": "2024-01-18 14:43:47 UTC",
    "updated_date": "2025-03-28 00:06:51 UTC"
  },
  {
    "arxiv_id": "2401.10019v3",
    "title": "R-Judge: Benchmarking Safety Risk Awareness for LLM Agents",
    "authors": [
      "Tongxin Yuan",
      "Zhiwei He",
      "Lingzhong Dong",
      "Yiming Wang",
      "Ruijie Zhao",
      "Tian Xia",
      "Lizhen Xu",
      "Binglin Zhou",
      "Fangqi Li",
      "Zhuosheng Zhang",
      "Rui Wang",
      "Gongshen Liu"
    ],
    "abstract": "Large language models (LLMs) have exhibited great potential in autonomously\ncompleting tasks across real-world applications. Despite this, these LLM agents\nintroduce unexpected safety risks when operating in interactive environments.\nInstead of centering on the harmlessness of LLM-generated content in most prior\nstudies, this work addresses the imperative need for benchmarking the\nbehavioral safety of LLM agents within diverse environments. We introduce\nR-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging and\nidentifying safety risks given agent interaction records. R-Judge comprises 569\nrecords of multi-turn agent interaction, encompassing 27 key risk scenarios\namong 5 application categories and 10 risk types. It is of high-quality\ncuration with annotated safety labels and risk descriptions. Evaluation of 11\nLLMs on R-Judge shows considerable room for enhancing the risk awareness of\nLLMs: The best-performing model, GPT-4o, achieves 74.42% while no other models\nsignificantly exceed the random. Moreover, we reveal that risk awareness in\nopen agent scenarios is a multi-dimensional capability involving knowledge and\nreasoning, thus challenging for LLMs. With further experiments, we find that\nfine-tuning on safety judgment significantly improve model performance while\nstraightforward prompting mechanisms fail. R-Judge is publicly available at\nhttps://github.com/Lordog/R-Judge.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP Findings 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.10019v3",
    "published_date": "2024-01-18 14:40:46 UTC",
    "updated_date": "2024-10-05 06:50:15 UTC"
  },
  {
    "arxiv_id": "2401.10016v1",
    "title": "Gender Bias in Machine Translation and The Era of Large Language Models",
    "authors": [
      "Eva Vanmassenhove"
    ],
    "abstract": "This chapter examines the role of Machine Translation in perpetuating gender\nbias, highlighting the challenges posed by cross-linguistic settings and\nstatistical dependencies. A comprehensive overview of relevant existing work\nrelated to gender bias in both conventional Neural Machine Translation\napproaches and Generative Pretrained Transformer models employed as Machine\nTranslation systems is provided. Through an experiment using ChatGPT (based on\nGPT-3.5) in an English-Italian translation context, we further assess ChatGPT's\ncurrent capacity to address gender bias. The findings emphasize the ongoing\nneed for advancements in mitigating bias in Machine Translation systems and\nunderscore the importance of fostering fairness and inclusivity in language\ntechnologies.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.10016v1",
    "published_date": "2024-01-18 14:34:49 UTC",
    "updated_date": "2024-01-18 14:34:49 UTC"
  },
  {
    "arxiv_id": "2401.09987v2",
    "title": "Adaptive Kalman-Informed Transformer",
    "authors": [
      "Nadav Cohen",
      "Itzik Klein"
    ],
    "abstract": "The extended Kalman filter (EKF) is a widely adopted method for sensor fusion\nin navigation applications. A crucial aspect of the EKF is the online\ndetermination of the process noise covariance matrix reflecting the model\nuncertainty. While common EKF implementation assumes a constant process noise,\nin real-world scenarios, the process noise varies, leading to inaccuracies in\nthe estimated state and potentially causing the filter to diverge. Model-based\nadaptive EKF methods were proposed and demonstrated performance improvements to\ncope with such situations, highlighting the need for a robust adaptive\napproach. In this paper, we derive an adaptive Kalman-informed transformer\n(A-KIT) designed to learn the varying process noise covariance online. Built\nupon the foundations of the EKF, A-KIT utilizes the well-known capabilities of\nset transformers, including inherent noise reduction and the ability to capture\nnonlinear behavior in the data. This approach is suitable for any application\ninvolving the EKF. In a case study, we demonstrate the effectiveness of A-KIT\nin nonlinear fusion between a Doppler velocity log and inertial sensors. This\nis accomplished using real data recorded from sensors mounted on an autonomous\nunderwater vehicle operating in the Mediterranean Sea. We show that A-KIT\noutperforms the conventional EKF by more than 49.5% and model-based adaptive\nEKF by an average of 35.4% in terms of position accuracy.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09987v2",
    "published_date": "2024-01-18 14:04:51 UTC",
    "updated_date": "2025-03-07 19:48:55 UTC"
  },
  {
    "arxiv_id": "2401.09986v2",
    "title": "Improving Local Training in Federated Learning via Temperature Scaling",
    "authors": [
      "Kichang Lee",
      "Songkuk Kim",
      "JeongGil Ko"
    ],
    "abstract": "Federated learning is inherently hampered by data heterogeneity: non-i.i.d.\ntraining data over local clients. We propose a novel model training approach\nfor federated learning, FLex&Chill, which exploits the Logit Chilling method.\nThrough extensive evaluations, we demonstrate that, in the presence of\nnon-i.i.d. data characteristics inherent in federated learning systems, this\napproach can expedite model convergence and improve inference accuracy.\nQuantitatively, from our experiments, we observe up to 6X improvement in the\nglobal federated learning model convergence time, and up to 3.37% improvement\nin inference accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68",
      "I.2.11"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.09986v2",
    "published_date": "2024-01-18 14:02:23 UTC",
    "updated_date": "2024-06-26 10:16:46 UTC"
  },
  {
    "arxiv_id": "2401.09983v1",
    "title": "Multiobjective Optimization Analysis for Finding Infrastructure-as-Code Deployment Configurations",
    "authors": [
      "Eneko Osaba",
      "Josu Diaz-de-Arcaya",
      "Juncal Alonso",
      "Jesus L. Lobo",
      "Gorka Benguria",
      "Iñaki Etxaniz"
    ],
    "abstract": "Multiobjective optimization is a hot topic in the artificial intelligence and\noperations research communities. The design and development of multiobjective\nmethods is a frequent task for researchers and practitioners. As a result of\nthis vibrant activity, a myriad of techniques have been proposed in the\nliterature to date, demonstrating a significant effectiveness for dealing with\nsituations coming from a wide range of real-world areas. This paper is focused\non a multiobjective problem related to optimizing Infrastructure-as-Code\ndeployment configurations. The system implemented for solving this problem has\nbeen coined as IaC Optimizer Platform (IOP). Despite the fact that a\nprototypical version of the IOP has been introduced in the literature before, a\ndeeper analysis focused on the resolution of the problem is needed, in order to\ndetermine which is the most appropriate multiobjective method for embedding in\nthe IOP. The main motivation behind the analysis conducted in this work is to\nenhance the IOP performance as much as possible. This is a crucial aspect of\nthis system, deeming that it will be deployed in a real environment, as it is\nbeing developed as part of a H2020 European project. Going deeper, we resort in\nthis paper to nine different evolutionary computation-based multiobjective\nalgorithms. For assessing the quality of the considered solvers, 12 different\nproblem instances have been generated based on real-world settings. Results\nobtained by each method after 10 independent runs have been compared using\nFriedman's non-parametric tests. Findings reached from the tests carried out\nlad to the creation of a multi-algorithm system, capable of applying different\ntechniques according to the user's needs.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "9 pages, 1 figure, 4 tables. Paper presented in the 11th\n  International Conference on Computer and Communications Management (ICCCM\n  2023)",
    "pdf_url": "http://arxiv.org/pdf/2401.09983v1",
    "published_date": "2024-01-18 13:55:32 UTC",
    "updated_date": "2024-01-18 13:55:32 UTC"
  },
  {
    "arxiv_id": "2401.09966v3",
    "title": "Towards Generative Abstract Reasoning: Completing Raven's Progressive Matrix via Rule Abstraction and Selection",
    "authors": [
      "Fan Shi",
      "Bin Li",
      "Xiangyang Xue"
    ],
    "abstract": "Endowing machines with abstract reasoning ability has been a long-term\nresearch topic in artificial intelligence. Raven's Progressive Matrix (RPM) is\nwidely used to probe abstract visual reasoning in machine intelligence, where\nmodels will analyze the underlying rules and select one image from candidates\nto complete the image matrix. Participators of RPM tests can show powerful\nreasoning ability by inferring and combining attribute-changing rules and\nimagining the missing images at arbitrary positions of a matrix. However,\nexisting solvers can hardly manifest such an ability in realistic RPM tests. In\nthis paper, we propose a deep latent variable model for answer generation\nproblems through Rule AbstractIon and SElection (RAISE). RAISE can encode image\nattributes into latent concepts and abstract atomic rules that act on the\nlatent concepts. When generating answers, RAISE selects one atomic rule out of\nthe global knowledge set for each latent concept to constitute the underlying\nrule of an RPM. In the experiments of bottom-right and arbitrary-position\nanswer generation, RAISE outperforms the compared solvers in most\nconfigurations of realistic RPM datasets. In the odd-one-out task and two\nheld-out configurations, RAISE can leverage acquired latent concepts and atomic\nrules to find the rule-breaking image in a matrix and handle problems with\nunseen combinations of rules and attributes.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09966v3",
    "published_date": "2024-01-18 13:28:44 UTC",
    "updated_date": "2024-04-14 10:53:43 UTC"
  },
  {
    "arxiv_id": "2401.09964v1",
    "title": "When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference",
    "authors": [
      "Zhensu Sun",
      "Xiaoning Du",
      "Fu Song",
      "Shangwen Wang",
      "Li Li"
    ],
    "abstract": "Leveraging recent advancements in large language models, modern neural code\ncompletion models have demonstrated the capability to generate highly accurate\ncode suggestions. However, their massive size poses challenges in terms of\ncomputational costs and environmental impact, hindering their widespread\nadoption in practical scenarios. Dynamic inference emerges as a promising\nsolution, as it allocates minimal computation during inference while\nmaintaining the model's performance. In this research, we explore dynamic\ninference within the context of code completion. Initially, we conducted an\nempirical investigation on GPT-2, focusing on the inference capabilities of\nintermediate layers for code completion. We found that 54.4% of tokens can be\naccurately generated using just the first layer, signifying significant\ncomputational savings potential. Moreover, despite using all layers, the model\nstill fails to predict 14.5% of tokens correctly, and the subsequent\ncompletions continued from them are rarely considered helpful, with only a 4.2%\nAcceptance Rate. These findings motivate our exploration of dynamic inference\nin code completion and inspire us to enhance it with a decision-making\nmechanism that stops the generation of incorrect code. We thus propose a novel\ndynamic inference method specifically tailored for code completion models. This\nmethod aims not only to produce correct predictions with largely reduced\ncomputation but also to prevent incorrect predictions proactively. Our\nextensive evaluation shows that it can averagely skip 1.7 layers out of 16\nlayers in the models, leading to an 11.2% speedup with only a marginal 1.1%\nreduction in ROUGE-L.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted to ICSE24",
    "pdf_url": "http://arxiv.org/pdf/2401.09964v1",
    "published_date": "2024-01-18 13:26:53 UTC",
    "updated_date": "2024-01-18 13:26:53 UTC"
  },
  {
    "arxiv_id": "2401.09944v1",
    "title": "WindSeer: Real-time volumetric wind prediction over complex terrain aboard a small UAV",
    "authors": [
      "Florian Achermann",
      "Thomas Stastny",
      "Bogdan Danciu",
      "Andrey Kolobov",
      "Jen Jen Chung",
      "Roland Siegwart",
      "Nicholas Lawrance"
    ],
    "abstract": "Real-time high-resolution wind predictions are beneficial for various\napplications including safe manned and unmanned aviation. Current weather\nmodels require too much compute and lack the necessary predictive capabilities\nas they are valid only at the scale of multiple kilometers and hours - much\nlower spatial and temporal resolutions than these applications require. Our\nwork, for the first time, demonstrates the ability to predict low-altitude wind\nin real-time on limited-compute devices, from only sparse measurement data. We\ntrain a neural network, WindSeer, using only synthetic data from computational\nfluid dynamics simulations and show that it can successfully predict real wind\nfields over terrain with known topography from just a few noisy and spatially\nclustered wind measurements. WindSeer can generate accurate predictions at\ndifferent resolutions and domain sizes on previously unseen topography without\nretraining. We demonstrate that the model successfully predicts historical wind\ndata collected by weather stations and wind measured onboard drones.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09944v1",
    "published_date": "2024-01-18 12:46:26 UTC",
    "updated_date": "2024-01-18 12:46:26 UTC"
  },
  {
    "arxiv_id": "2403.08773v2",
    "title": "Veagle: Advancements in Multimodal Representation Learning",
    "authors": [
      "Rajat Chawla",
      "Arkajit Datta",
      "Tushar Verma",
      "Adarsh Jha",
      "Anmol Gautam",
      "Ayush Vatsal",
      "Sukrit Chaterjee",
      "Mukunda NS",
      "Ishaan Bhola"
    ],
    "abstract": "Lately, researchers in artificial intelligence have been really interested in\nhow language and vision come together, giving rise to the development of\nmultimodal models that aim to seamlessly integrate textual and visual\ninformation. Multimodal models, an extension of Large Language Models (LLMs),\nhave exhibited remarkable capabilities in addressing a diverse array of tasks,\nranging from image captioning and visual question answering (VQA) to visual\ngrounding. While these models have showcased significant advancements,\nchallenges persist in accurately interpreting images and answering the\nquestion, a common occurrence in real-world scenarios. This paper introduces a\nnovel approach to enhance the multimodal capabilities of existing models. In\nresponse to the limitations observed in current Vision Language Models (VLMs)\nand Multimodal Large Language Models (MLLMs), our proposed model Veagle,\nincorporates a unique mechanism inspired by the successes and insights of\nprevious works. Veagle leverages a dynamic mechanism to project encoded visual\ninformation directly into the language model. This dynamic approach allows for\na more nuanced understanding of intricate details present in visual contexts.\nTo validate the effectiveness of Veagle, we conduct comprehensive experiments\non benchmark datasets, emphasizing tasks such as visual question answering and\nimage understanding. Our results indicate a improvement of 5-6 \\% in\nperformance, with Veagle outperforming existing models by a notable margin. The\noutcomes underscore the model's versatility and applicability beyond\ntraditional benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.08773v2",
    "published_date": "2024-01-18 12:45:25 UTC",
    "updated_date": "2024-10-27 06:01:49 UTC"
  },
  {
    "arxiv_id": "2401.09942v1",
    "title": "Multi-task Learning for Joint Re-identification, Team Affiliation, and Role Classification for Sports Visual Tracking",
    "authors": [
      "Amir M. Mansourian",
      "Vladimir Somers",
      "Christophe De Vleeschouwer",
      "Shohreh Kasaei"
    ],
    "abstract": "Effective tracking and re-identification of players is essential for\nanalyzing soccer videos. But, it is a challenging task due to the non-linear\nmotion of players, the similarity in appearance of players from the same team,\nand frequent occlusions. Therefore, the ability to extract meaningful\nembeddings to represent players is crucial in developing an effective tracking\nand re-identification system. In this paper, a multi-purpose part-based person\nrepresentation method, called PRTreID, is proposed that performs three tasks of\nrole classification, team affiliation, and re-identification, simultaneously.\nIn contrast to available literature, a single network is trained with\nmulti-task supervision to solve all three tasks, jointly. The proposed joint\nmethod is computationally efficient due to the shared backbone. Also, the\nmulti-task learning leads to richer and more discriminative representations, as\ndemonstrated by both quantitative and qualitative results. To demonstrate the\neffectiveness of PRTreID, it is integrated with a state-of-the-art tracking\nmethod, using a part-based post-processing module to handle long-term tracking.\nThe proposed tracking method outperforms all existing tracking methods on the\nchallenging SoccerNet tracking dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09942v1",
    "published_date": "2024-01-18 12:45:14 UTC",
    "updated_date": "2024-01-18 12:45:14 UTC"
  },
  {
    "arxiv_id": "2401.10304v2",
    "title": "On the Readiness of Scientific Data for a Fair and Transparent Use in Machine Learning",
    "authors": [
      "Joan Giner-Miguelez",
      "Abel Gómez",
      "Jordi Cabot"
    ],
    "abstract": "To ensure the fairness and trustworthiness of machine learning (ML) systems,\nrecent legislative initiatives and relevant research in the ML community have\npointed out the need to document the data used to train ML models. Besides,\ndata-sharing practices in many scientific domains have evolved in recent years\nfor reproducibility purposes. In this sense, academic institutions' adoption of\nthese practices has encouraged researchers to publish their data and technical\ndocumentation in peer-reviewed publications such as data papers. In this study,\nwe analyze how this broader scientific data documentation meets the needs of\nthe ML community and regulatory bodies for its use in ML technologies. We\nexamine a sample of 4041 data papers of different domains, assessing their\ncompleteness, coverage of the requested dimensions, and trends in recent years.\nWe focus on the most and least documented dimensions and compare the results\nwith those of an ML-focused venue (NeurIPS D&B track) publishing papers\ndescribing datasets. As a result, we propose a set of recommendation guidelines\nfor data creators and scientific data publishers to increase their data's\npreparedness for its transparent and fairer use in ML technologies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10304v2",
    "published_date": "2024-01-18 12:11:27 UTC",
    "updated_date": "2024-12-17 16:34:49 UTC"
  },
  {
    "arxiv_id": "2401.09900v1",
    "title": "XAI-Enhanced Semantic Segmentation Models for Visual Quality Inspection",
    "authors": [
      "Tobias Clement",
      "Truong Thanh Hung Nguyen",
      "Mohamed Abdelaal",
      "Hung Cao"
    ],
    "abstract": "Visual quality inspection systems, crucial in sectors like manufacturing and\nlogistics, employ computer vision and machine learning for precise, rapid\ndefect detection. However, their unexplained nature can hinder trust, error\nidentification, and system improvement. This paper presents a framework to\nbolster visual quality inspection by using CAM-based explanations to refine\nsemantic segmentation models. Our approach consists of 1) Model Training, 2)\nXAI-based Model Explanation, 3) XAI Evaluation, and 4) Annotation Augmentation\nfor Model Enhancement, informed by explanations and expert insights.\nEvaluations show XAI-enhanced models surpass original DeepLabv3-ResNet101\nmodels, especially in intricate object segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "IEEE ICCE 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.09900v1",
    "published_date": "2024-01-18 11:26:20 UTC",
    "updated_date": "2024-01-18 11:26:20 UTC"
  },
  {
    "arxiv_id": "2401.09886v2",
    "title": "Cooperative Edge Caching Based on Elastic Federated and Multi-Agent Deep Reinforcement Learning in Next-Generation Network",
    "authors": [
      "Qiong Wu",
      "Wenhua Wang",
      "Pingyi Fan",
      "Qiang Fan",
      "Huiling Zhu",
      "Khaled B. Letaief"
    ],
    "abstract": "Edge caching is a promising solution for next-generation networks by\nempowering caching units in small-cell base stations (SBSs), which allows user\nequipments (UEs) to fetch users' requested contents that have been pre-cached\nin SBSs. It is crucial for SBSs to predict accurate popular contents through\nlearning while protecting users' personal information. Traditional federated\nlearning (FL) can protect users' privacy but the data discrepancies among UEs\ncan lead to a degradation in model quality. Therefore, it is necessary to train\npersonalized local models for each UE to predict popular contents accurately.\nIn addition, the cached contents can be shared among adjacent SBSs in\nnext-generation networks, thus caching predicted popular contents in different\nSBSs may affect the cost to fetch contents. Hence, it is critical to determine\nwhere the popular contents are cached cooperatively. To address these issues,\nwe propose a cooperative edge caching scheme based on elastic federated and\nmulti-agent deep reinforcement learning (CEFMR) to optimize the cost in the\nnetwork. We first propose an elastic FL algorithm to train the personalized\nmodel for each UE, where adversarial autoencoder (AAE) model is adopted for\ntraining to improve the prediction accuracy, then {a popular} content\nprediction algorithm is proposed to predict the popular contents for each SBS\nbased on the trained AAE model. Finally, we propose a multi-agent deep\nreinforcement learning (MADRL) based algorithm to decide where the predicted\npopular contents are collaboratively cached among SBSs. Our experimental\nresults demonstrate the superiority of our proposed scheme to existing baseline\ncaching schemes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been submitted to IEEE TNSM. The source code has been\n  released at:\n  https://github.com/qiongwu86/Edge-Caching-Based-on-Multi-Agent-Deep-Reinforcement-Learning-and-Federated-Learning",
    "pdf_url": "http://arxiv.org/pdf/2401.09886v2",
    "published_date": "2024-01-18 10:59:18 UTC",
    "updated_date": "2024-06-05 00:35:48 UTC"
  },
  {
    "arxiv_id": "2401.10946v1",
    "title": "Self context-aware emotion perception on human-robot interaction",
    "authors": [
      "Zihan Lin",
      "Francisco Cruz",
      "Eduardo Benitez Sandoval"
    ],
    "abstract": "Emotion recognition plays a crucial role in various domains of human-robot\ninteraction. In long-term interactions with humans, robots need to respond\ncontinuously and accurately, however, the mainstream emotion recognition\nmethods mostly focus on short-term emotion recognition, disregarding the\ncontext in which emotions are perceived. Humans consider that contextual\ninformation and different contexts can lead to completely different emotional\nexpressions. In this paper, we introduce self context-aware model (SCAM) that\nemploys a two-dimensional emotion coordinate system for anchoring and\nre-labeling distinct emotions. Simultaneously, it incorporates its distinctive\ninformation retention structure and contextual loss. This approach has yielded\nsignificant improvements across audio, video, and multimodal. In the auditory\nmodality, there has been a notable enhancement in accuracy, rising from 63.10%\nto 72.46%. Similarly, the visual modality has demonstrated improved accuracy,\nincreasing from 77.03% to 80.82%. In the multimodal, accuracy has experienced\nan elevation from 77.48% to 78.93%. In the future, we will validate the\nreliability and usability of SCAM on robots through psychology experiments.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Australasian Conference on Robotics and Automation (ACRA). 2023",
    "pdf_url": "http://arxiv.org/pdf/2401.10946v1",
    "published_date": "2024-01-18 10:58:27 UTC",
    "updated_date": "2024-01-18 10:58:27 UTC"
  },
  {
    "arxiv_id": "2401.09880v1",
    "title": "Attention-Based Recurrent Neural Network For Automatic Behavior Laying Hen Recognition",
    "authors": [
      "Fréjus A. A. Laleye",
      "Mikaël A. Mousse"
    ],
    "abstract": "One of the interests of modern poultry farming is the vocalization of laying\nhens which contain very useful information on health behavior. This information\nis used as health and well-being indicators that help breeders better monitor\nlaying hens, which involves early detection of problems for rapid and more\neffective intervention. In this work, we focus on the sound analysis for the\nrecognition of the types of calls of the laying hens in order to propose a\nrobust system of characterization of their behavior for a better monitoring. To\ndo this, we first collected and annotated laying hen call signals, then\ndesigned an optimal acoustic characterization based on the combination of time\nand frequency domain features. We then used these features to build the\nmulti-label classification models based on recurrent neural network to assign a\nsemantic class to the vocalization that characterize the laying hen behavior.\nThe results show an overall performance with our model based on the combination\nof time and frequency domain features that obtained the highest F1-score\n(F1=92.75) with a gain of 17% on the models using the frequency domain features\nand of 8% on the compared approaches from the litterature.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09880v1",
    "published_date": "2024-01-18 10:52:46 UTC",
    "updated_date": "2024-01-18 10:52:46 UTC"
  },
  {
    "arxiv_id": "2401.09870v2",
    "title": "Reconciling Spatial and Temporal Abstractions for Goal Representation",
    "authors": [
      "Mehdi Zadem",
      "Sergio Mover",
      "Sao Mai Nguyen"
    ],
    "abstract": "Goal representation affects the performance of Hierarchical Reinforcement\nLearning (HRL) algorithms by decomposing the complex learning problem into\neasier subtasks. Recent studies show that representations that preserve\ntemporally abstract environment dynamics are successful in solving difficult\nproblems and provide theoretical guarantees for optimality. These methods\nhowever cannot scale to tasks where environment dynamics increase in complexity\ni.e. the temporally abstract transition relations depend on larger number of\nvariables. On the other hand, other efforts have tried to use spatial\nabstraction to mitigate the previous issues. Their limitations include\nscalability to high dimensional environments and dependency on prior knowledge.\n  In this paper, we propose a novel three-layer HRL algorithm that introduces,\nat different levels of the hierarchy, both a spatial and a temporal goal\nabstraction. We provide a theoretical study of the regret bounds of the learned\npolicies. We evaluate the approach on complex continuous control tasks,\ndemonstrating the effectiveness of spatial and temporal abstractions learned by\nthis approach. Find open-source code at https://github.com/cosynus-lix/STAR.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09870v2",
    "published_date": "2024-01-18 10:33:30 UTC",
    "updated_date": "2024-06-30 09:02:37 UTC"
  },
  {
    "arxiv_id": "2401.09865v1",
    "title": "Improving fine-grained understanding in image-text pre-training",
    "authors": [
      "Ioana Bica",
      "Anastasija Ilić",
      "Matthias Bauer",
      "Goker Erdogan",
      "Matko Bošnjak",
      "Christos Kaplanis",
      "Alexey A. Gritsenko",
      "Matthias Minderer",
      "Charles Blundell",
      "Razvan Pascanu",
      "Jovana Mitrović"
    ],
    "abstract": "We introduce SPARse Fine-grained Contrastive Alignment (SPARC), a simple\nmethod for pretraining more fine-grained multimodal representations from\nimage-text pairs. Given that multiple image patches often correspond to single\nwords, we propose to learn a grouping of image patches for every token in the\ncaption. To achieve this, we use a sparse similarity metric between image\npatches and language tokens and compute for each token a language-grouped\nvision embedding as the weighted average of patches. The token and\nlanguage-grouped vision embeddings are then contrasted through a fine-grained\nsequence-wise loss that only depends on individual samples and does not require\nother batch samples as negatives. This enables more detailed information to be\nlearned in a computationally inexpensive manner. SPARC combines this\nfine-grained loss with a contrastive loss between global image and text\nembeddings to learn representations that simultaneously encode global and local\ninformation. We thoroughly evaluate our proposed method and show improved\nperformance over competing approaches both on image-level tasks relying on\ncoarse-grained information, e.g. classification, as well as region-level tasks\nrelying on fine-grained information, e.g. retrieval, object detection, and\nsegmentation. Moreover, SPARC improves model faithfulness and captioning in\nfoundational vision-language models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "26 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.09865v1",
    "published_date": "2024-01-18 10:28:45 UTC",
    "updated_date": "2024-01-18 10:28:45 UTC"
  },
  {
    "arxiv_id": "2401.09862v1",
    "title": "Evolutionary Multi-Objective Optimization of Large Language Model Prompts for Balancing Sentiments",
    "authors": [
      "Jill Baumann",
      "Oliver Kramer"
    ],
    "abstract": "The advent of large language models (LLMs) such as ChatGPT has attracted\nconsiderable attention in various domains due to their remarkable performance\nand versatility. As the use of these models continues to grow, the importance\nof effective prompt engineering has come to the fore. Prompt optimization\nemerges as a crucial challenge, as it has a direct impact on model performance\nand the extraction of relevant information. Recently, evolutionary algorithms\n(EAs) have shown promise in addressing this issue, paving the way for novel\noptimization strategies. In this work, we propose a evolutionary\nmulti-objective (EMO) approach specifically tailored for prompt optimization\ncalled EMO-Prompts, using sentiment analysis as a case study. We use sentiment\nanalysis capabilities as our experimental targets. Our results demonstrate that\nEMO-Prompts effectively generates prompts capable of guiding the LLM to produce\ntexts embodying two conflicting emotions simultaneously.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted in EvoApps at EvoStar 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.09862v1",
    "published_date": "2024-01-18 10:21:15 UTC",
    "updated_date": "2024-01-18 10:21:15 UTC"
  },
  {
    "arxiv_id": "2401.09861v1",
    "title": "Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models",
    "authors": [
      "Li Sun",
      "Liuan Wang",
      "Jun Sun",
      "Takayuki Okatani"
    ],
    "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced the comprehension of multimedia content, bringing\ntogether diverse modalities such as text, images, and videos. However, a\ncritical challenge faced by these models, especially when processing video\ninputs, is the occurrence of hallucinations - erroneous perceptions or\ninterpretations, particularly at the event level. This study introduces an\ninnovative method to address event-level hallucinations in MLLMs, focusing on\nspecific temporal understanding in video content. Our approach leverages a\nnovel framework that extracts and utilizes event-specific information from both\nthe event query and the provided video to refine MLLMs' response. We propose a\nunique mechanism that decomposes on-demand event queries into iconic actions.\nSubsequently, we employ models like CLIP and BLIP2 to predict specific\ntimestamps for event occurrences. Our evaluation, conducted using the\nCharades-STA dataset, demonstrates a significant reduction in temporal\nhallucinations and an improvement in the quality of event-related responses.\nThis research not only provides a new perspective in addressing a critical\nlimitation of MLLMs but also contributes a quantitatively measurable method for\nevaluating MLLMs in the context of temporal-related questions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.09861v1",
    "published_date": "2024-01-18 10:18:48 UTC",
    "updated_date": "2024-01-18 10:18:48 UTC"
  },
  {
    "arxiv_id": "2401.09852v1",
    "title": "Enhancing the Fairness and Performance of Edge Cameras with Explainable AI",
    "authors": [
      "Truong Thanh Hung Nguyen",
      "Vo Thanh Khang Nguyen",
      "Quoc Hung Cao",
      "Van Binh Truong",
      "Quoc Khanh Nguyen",
      "Hung Cao"
    ],
    "abstract": "The rising use of Artificial Intelligence (AI) in human detection on Edge\ncamera systems has led to accurate but complex models, challenging to interpret\nand debug. Our research presents a diagnostic method using Explainable AI (XAI)\nfor model debugging, with expert-driven problem identification and solution\ncreation. Validated on the Bytetrack model in a real-world office Edge network,\nwe found the training dataset as the main bias source and suggested model\naugmentation as a solution. Our approach helps identify model biases, essential\nfor achieving fair and trustworthy models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "IEEE ICCE 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.09852v1",
    "published_date": "2024-01-18 10:08:24 UTC",
    "updated_date": "2024-01-18 10:08:24 UTC"
  },
  {
    "arxiv_id": "2401.09851v4",
    "title": "Next-Generation Simulation Illuminates Scientific Problems of Organised Complexity",
    "authors": [
      "Cheng Wang",
      "Chuwen Wang",
      "Wang Zhang",
      "Shirong Zeng",
      "Yu Zhao",
      "Ronghui Ning",
      "Changjun Jiang"
    ],
    "abstract": "As artificial intelligence becomes increasingly prevalent in scientific\nresearch, data-driven methodologies appear to overshadow traditional approaches\nin resolving scientific problems. In this Perspective, we revisit a classic\nclassification of scientific problems and acknowledge that a series of\nunresolved problems remain. Throughout the history of researching scientific\nproblems, scientists have continuously formed new paradigms facilitated by\nadvances in data, algorithms, and computational power. To better tackle\nunresolved problems, especially those of organised complexity, a novel paradigm\nis necessitated. While recognising that the strengths of new paradigms have\nexpanded the scope of resolvable scientific problems, we aware that the\ncontinued advancement of data, algorithms, and computational power alone is\nhardly to bring a new paradigm. We posit that the integration of paradigms,\nwhich capitalises on the strengths of each, represents a promising approach.\nSpecifically, we focus on next-generation simulation (NGS), which can serve as\na platform to integrate methods from different paradigms. We propose a\nmethodology, sophisticated behavioural simulation (SBS), to realise it. SBS\nrepresents a higher level of paradigms integration based on foundational models\nto simulate complex systems, such as social systems involving sophisticated\nhuman strategies and behaviours. NGS extends beyond the capabilities of\ntraditional mathematical modelling simulations and agent-based modelling\nsimulations, and therefore, positions itself as a potential solution to\nproblems of organised complexity in complex systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09851v4",
    "published_date": "2024-01-18 10:05:52 UTC",
    "updated_date": "2024-06-14 11:31:43 UTC"
  },
  {
    "arxiv_id": "2401.09833v1",
    "title": "Slicer Networks",
    "authors": [
      "Hang Zhang",
      "Xiang Chen",
      "Rongguang Wang",
      "Renjiu Hu",
      "Dongdong Liu",
      "Gaolei Li"
    ],
    "abstract": "In medical imaging, scans often reveal objects with varied contrasts but\nconsistent internal intensities or textures. This characteristic enables the\nuse of low-frequency approximations for tasks such as segmentation and\ndeformation field estimation. Yet, integrating this concept into neural network\narchitectures for medical image analysis remains underexplored. In this paper,\nwe propose the Slicer Network, a novel architecture designed to leverage these\ntraits. Comprising an encoder utilizing models like vision transformers for\nfeature extraction and a slicer employing a learnable bilateral grid, the\nSlicer Network strategically refines and upsamples feature maps via a\nsplatting-blurring-slicing process. This introduces an edge-preserving\nlow-frequency approximation for the network outcome, effectively enlarging the\neffective receptive field. The enhancement not only reduces computational\ncomplexity but also boosts overall performance. Experiments across different\nmedical imaging applications, including unsupervised and keypoints-based image\nregistration and lesion segmentation, have verified the Slicer Network's\nimproved accuracy and efficiency.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "8 figures and 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.09833v1",
    "published_date": "2024-01-18 09:50:26 UTC",
    "updated_date": "2024-01-18 09:50:26 UTC"
  },
  {
    "arxiv_id": "2401.09819v2",
    "title": "PPNet: A Two-Stage Neural Network for End-to-end Path Planning",
    "authors": [
      "Qinglong Meng",
      "Chongkun Xia",
      "Xueqian Wang",
      "Songping Mai",
      "Bin Liang"
    ],
    "abstract": "The classical path planners, such as sampling-based path planners, can\nprovide probabilistic completeness guarantees in the sense that the probability\nthat the planner fails to return a solution if one exists, decays to zero as\nthe number of samples approaches infinity. However, finding a near-optimal\nfeasible solution in a given period is challenging in many applications such as\nthe autonomous vehicle. To achieve an end-to-end near-optimal path planner, we\nfirst divide the path planning problem into two subproblems, which are path\nspace segmentation and waypoints generation in the given path's space. We\nfurther propose a two-stage neural network named Path Planning Network (PPNet)\neach stage solves one of the subproblems abovementioned. Moreover, we propose a\nnovel efficient data generation method for path planning named EDaGe-PP.\nEDaGe-PP can generate data with continuous-curvature paths with analytical\nexpression while satisfying the clearance requirement. The results show the\ntotal computation time of generating random 2D path planning data is less than\n1/33 and the success rate of PPNet trained by the dataset that is generated by\nEDaGe-PP is about 2 times compared to other methods. We validate PPNet against\nstate-of-the-art path planning methods. The results show that PPNet can find a\nnear-optimal solution in 15.3ms, which is much shorter than the\nstate-of-the-art path planners.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09819v2",
    "published_date": "2024-01-18 09:20:27 UTC",
    "updated_date": "2024-04-23 09:21:50 UTC"
  },
  {
    "arxiv_id": "2402.01673v1",
    "title": "Legal and ethical implications of applications based on agreement technologies: the case of auction-based road intersections",
    "authors": [
      "José-Antonio Santos",
      "Alberto Fernández",
      "Mar Moreno-Rebato",
      "Holger Billhardt",
      "José-A. Rodríguez-García",
      "Sascha Ossowski"
    ],
    "abstract": "Agreement Technologies refer to a novel paradigm for the construction of\ndistributed intelligent systems, where autonomous software agents negotiate to\nreach agreements on behalf of their human users. Smart Cities are a key\napplication domain for Agreement Technologies. While several proofs of concept\nand prototypes exist, such systems are still far from ready for being deployed\nin the real-world. In this paper we focus on a novel method for managing\nelements of smart road infrastructures of the future, namely the case of\nauction-based road intersections. We show that, even though the key\ntechnological elements for such methods are already available, there are\nmultiple non-technical issues that need to be tackled before they can be\napplied in practice. For this purpose, we analyse legal and ethical\nimplications of auction-based road intersections in the context of\ninternational regulations and from the standpoint of the Spanish legislation.\nFrom this exercise, we extract a set of required modifications, of both\ntechnical and legal nature, which need to be addressed so as to pave the way\nfor the potential real-world deployment of such systems in a future that may\nnot be too far away.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "I.2.1"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.01673v1",
    "published_date": "2024-01-18 09:12:48 UTC",
    "updated_date": "2024-01-18 09:12:48 UTC"
  },
  {
    "arxiv_id": "2402.05115v1",
    "title": "Unsupervised Motion Retargeting for Human-Robot Imitation",
    "authors": [
      "Louis Annabi",
      "Ziqi Ma",
      "Sao Mai Nguyen"
    ],
    "abstract": "This early-stage research work aims to improve online human-robot imitation\nby translating sequences of joint positions from the domain of human motions to\na domain of motions achievable by a given robot, thus constrained by its\nembodiment. Leveraging the generalization capabilities of deep learning\nmethods, we address this problem by proposing an encoder-decoder neural network\nmodel performing domain-to-domain translation. In order to train such a model,\none could use pairs of associated robot and human motions. Though, such paired\ndata is extremely rare in practice, and tedious to collect. Therefore, we turn\ntowards deep learning methods for unpaired domain-to-domain translation, that\nwe adapt in order to perform human-robot imitation.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Companion of the 2024 ACM/IEEE International Conference on\n  Human-Robot Interactio, Mar 2024, Boulder (CO), United States",
    "pdf_url": "http://arxiv.org/pdf/2402.05115v1",
    "published_date": "2024-01-18 09:03:33 UTC",
    "updated_date": "2024-01-18 09:03:33 UTC"
  },
  {
    "arxiv_id": "2402.01672v1",
    "title": "Prerequisite Structure Discovery in Intelligent Tutoring Systems",
    "authors": [
      "Louis Annabi",
      "Sao Mai Nguyen"
    ],
    "abstract": "This paper addresses the importance of Knowledge Structure (KS) and Knowledge\nTracing (KT) in improving the recommendation of educational content in\nintelligent tutoring systems. The KS represents the relations between different\nKnowledge Components (KCs), while KT predicts a learner's success based on her\npast history. The contribution of this research includes proposing a KT model\nthat incorporates the KS as a learnable parameter, enabling the discovery of\nthe underlying KS from learner trajectories. The quality of the uncovered KS is\nassessed by using it to recommend content and evaluating the recommendation\nalgorithm with simulated students.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.01672v1",
    "published_date": "2024-01-18 09:01:49 UTC",
    "updated_date": "2024-01-18 09:01:49 UTC"
  },
  {
    "arxiv_id": "2401.10300v2",
    "title": "A Hierarchical Framework with Spatio-Temporal Consistency Learning for Emergence Detection in Complex Adaptive Systems",
    "authors": [
      "Siyuan Chen",
      "Xin Du",
      "Jiahai Wang"
    ],
    "abstract": "Emergence, a global property of complex adaptive systems (CASs) constituted\nby interactive agents, is prevalent in real-world dynamic systems, e.g.,\nnetwork-level traffic congestions. Detecting its formation and evaporation\nhelps to monitor the state of a system, allowing to issue a warning signal for\nharmful emergent phenomena. Since there is no centralized controller of CAS,\ndetecting emergence based on each agent's local observation is desirable but\nchallenging. Existing works are unable to capture emergence-related spatial\npatterns, and fail to model the nonlinear relationships among agents. This\npaper proposes a hierarchical framework with spatio-temporal consistency\nlearning to solve these two problems by learning the system representation and\nagent representations, respectively. Spatio-temporal encoders composed of\nspatial and temporal transformers are designed to capture agents' nonlinear\nrelationships and the system's complex evolution. Agents' and the system's\nrepresentations are learned to preserve the spatio-temporal consistency by\nminimizing the spatial and temporal dissimilarities in a self-supervised manner\nin the latent space. Our method achieves more accurate detection than\ntraditional methods and deep learning methods on three datasets with well-known\nyet hard-to-detect emergent behaviors. Notably, our hierarchical framework is\ngeneric in incorporating other deep learning methods for agent-level and\nsystem-level detection.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "23 pages, accepted by IEEE TNNLS",
    "pdf_url": "http://arxiv.org/pdf/2401.10300v2",
    "published_date": "2024-01-18 08:55:05 UTC",
    "updated_date": "2024-10-28 03:33:04 UTC"
  },
  {
    "arxiv_id": "2401.09798v3",
    "title": "All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks",
    "authors": [
      "Kazuhiro Takemoto"
    ],
    "abstract": "Large Language Models (LLMs), such as ChatGPT, encounter `jailbreak'\nchallenges, wherein safeguards are circumvented to generate ethically harmful\nprompts. This study introduces a straightforward black-box method for\nefficiently crafting jailbreak prompts, addressing the significant complexity\nand computational costs associated with conventional methods. Our technique\niteratively transforms harmful prompts into benign expressions directly\nutilizing the target LLM, predicated on the hypothesis that LLMs can\nautonomously generate expressions that evade safeguards. Through experiments\nconducted with ChatGPT (GPT-3.5 and GPT-4) and Gemini-Pro, our method\nconsistently achieved an attack success rate exceeding 80% within an average of\nfive iterations for forbidden questions and proved robust against model\nupdates. The jailbreak prompts generated were not only naturally-worded and\nsuccinct but also challenging to defend against. These findings suggest that\nthe creation of effective jailbreak prompts is less complex than previously\nbelieved, underscoring the heightened risk posed by black-box jailbreak\nattacks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 4 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.09798v3",
    "published_date": "2024-01-18 08:36:54 UTC",
    "updated_date": "2024-02-12 02:29:28 UTC"
  },
  {
    "arxiv_id": "2401.09795v1",
    "title": "A Comparative Analysis on Metaheuristic Algorithms Based Vision Transformer Model for Early Detection of Alzheimer's Disease",
    "authors": [
      "Anuvab Sen",
      "Udayon Sen",
      "Subhabrata Roy"
    ],
    "abstract": "A number of life threatening neuro-degenerative disorders had degraded the\nquality of life for the older generation in particular. Dementia is one such\nsymptom which may lead to a severe condition called Alzheimer's disease if not\ndetected at an early stage. It has been reported that the progression of such\ndisease from a normal stage is due to the change in several parameters inside\nthe human brain. In this paper, an innovative metaheuristic algorithms based\nViT model has been proposed for the identification of dementia at different\nstage. A sizeable number of test data have been utilized for the validation of\nthe proposed scheme. It has also been demonstrated that our model exhibits\nsuperior performance in terms of accuracy, precision, recall as well as\nF1-score.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "2023 IEEE 15th International Conference on Computational Intelligence\n  and Communication Networks (CICN). arXiv admin note: text overlap with\n  arXiv:2309.16796",
    "pdf_url": "http://arxiv.org/pdf/2401.09795v1",
    "published_date": "2024-01-18 08:31:38 UTC",
    "updated_date": "2024-01-18 08:31:38 UTC"
  },
  {
    "arxiv_id": "2401.09789v1",
    "title": "A Semantic Approach for Big Data Exploration in Industry 4.0",
    "authors": [
      "Idoia Berges",
      "Víctor Julio Ramírez-Durán",
      "Arantza Illarramendi"
    ],
    "abstract": "The growing trends in automation, Internet of Things, big data and cloud\ncomputing technologies have led to the fourth industrial revolution (Industry\n4.0), where it is possible to visualize and identify patterns and insights,\nwhich results in a better understanding of the data and can improve the\nmanufacturing process. However, many times, the task of data exploration\nresults difficult for manufacturing experts because they might be interested in\nanalyzing also data that does not appear in pre-designed visualizations and\ntherefore they must be assisted by Information Technology experts. In this\npaper, we present a proposal materialized in a semantic-based visual query\nsystem developed for a real Industry 4.0 scenario that allows domain experts to\nexplore and visualize data in a friendly way. The main novelty of the system is\nthe combined use that it makes of captured data that are semantically annotated\nfirst, and a 2D customized digital representation of a machine that is also\nlinked with semantic descriptions. Those descriptions are expressed using terms\nof an ontology, where, among others, the sensors that are used to capture\nindicators about the performance of a machine that belongs to a Industry 4.0\nscenario have been modeled. Moreover, this semantic description allows to:\nformulate queries at a higher level of abstraction, provide customized\ngraphical visualizations of the results based on the format and nature of the\ndata, and download enriched data enabling further types of analysis.",
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "Published version of paper: Idoia Berges, V\\'ictor Julio\n  Ram\\'irez-Dur\\'an, Arantza Illarramendi: A Semantic Approach for Big Data\n  Exploration in Industry 4.0. Big Data Res. 25: 100222 (2021). DOI:\n  10.1016/j.bdr.2021.100222",
    "pdf_url": "http://arxiv.org/pdf/2401.09789v1",
    "published_date": "2024-01-18 08:20:19 UTC",
    "updated_date": "2024-01-18 08:20:19 UTC"
  },
  {
    "arxiv_id": "2401.09787v2",
    "title": "Querying Easily Flip-flopped Samples for Deep Active Learning",
    "authors": [
      "Seong Jin Cho",
      "Gwangsu Kim",
      "Junghyun Lee",
      "Jinwoo Shin",
      "Chang D. Yoo"
    ],
    "abstract": "Active learning is a machine learning paradigm that aims to improve the\nperformance of a model by strategically selecting and querying unlabeled data.\nOne effective selection strategy is to base it on the model's predictive\nuncertainty, which can be interpreted as a measure of how informative a sample\nis. The sample's distance to the decision boundary is a natural measure of\npredictive uncertainty, but it is often intractable to compute, especially for\ncomplex decision boundaries formed in multiclass classification tasks. To\naddress this issue, this paper proposes the {\\it least disagree metric} (LDM),\ndefined as the smallest probability of disagreement of the predicted label, and\nan estimator for LDM proven to be asymptotically consistent under mild\nassumptions. The estimator is computationally efficient and can be easily\nimplemented for deep learning models using parameter perturbation. The\nLDM-based active learning is performed by querying unlabeled data with the\nsmallest LDM. Experimental results show that our LDM-based active learning\nalgorithm obtains state-of-the-art overall performance on all considered\ndatasets and deep architectures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "34 pages, 17 figures, 5 tables. Accepted to the 12th International\n  Conference on Learning Representations (ICLR 2024) (ver2: fixed some typos\n  and improved some parts of the writing)",
    "pdf_url": "http://arxiv.org/pdf/2401.09787v2",
    "published_date": "2024-01-18 08:12:23 UTC",
    "updated_date": "2024-05-16 08:36:30 UTC"
  },
  {
    "arxiv_id": "2401.09786v5",
    "title": "Adaptive Self-training Framework for Fine-grained Scene Graph Generation",
    "authors": [
      "Kibum Kim",
      "Kanghoon Yoon",
      "Yeonjun In",
      "Jinyoung Moon",
      "Donghyun Kim",
      "Chanyoung Park"
    ],
    "abstract": "Scene graph generation (SGG) models have suffered from inherent problems\nregarding the benchmark datasets such as the long-tailed predicate distribution\nand missing annotation problems. In this work, we aim to alleviate the\nlong-tailed problem of SGG by utilizing unannotated triplets. To this end, we\nintroduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels\nfor unannotated triplets based on which the SGG models are trained. While there\nhas been significant progress in self-training for image recognition, designing\na self-training framework for the SGG task is more challenging due to its\ninherent nature such as the semantic ambiguity and the long-tailed distribution\nof predicate classes. Hence, we propose a novel pseudo-labeling technique for\nSGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is\na model-agnostic framework that can be applied to any existing SGG models.\nFurthermore, we devise a graph structure learner (GSL) that is beneficial when\nadopting our proposed self-training framework to the state-of-the-art\nmessage-passing neural network (MPNN)-based SGG models. Our extensive\nexperiments verify the effectiveness of ST-SGG on various SGG models,\nparticularly in enhancing the performance on fine-grained predicate classes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages; ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.09786v5",
    "published_date": "2024-01-18 08:10:34 UTC",
    "updated_date": "2024-08-02 01:22:46 UTC"
  },
  {
    "arxiv_id": "2401.09773v2",
    "title": "SEINE: Structure Encoding and Interaction Network for Nuclei Instance Segmentation",
    "authors": [
      "Ye Zhang",
      "Linghan Cai",
      "Ziyue Wang",
      "Yongbing Zhang"
    ],
    "abstract": "Nuclei instance segmentation in histopathological images is of great\nimportance for biological analysis and cancer diagnosis but remains challenging\nfor two reasons. (1) Similar visual presentation of intranuclear and\nextranuclear regions of chromophobe nuclei often causes under-segmentation, and\n(2) current methods lack the exploration of nuclei structure, resulting in\nfragmented instance predictions. To address these problems, this paper proposes\na structure encoding and interaction network, termed SEINE, which develops the\nstructure modeling scheme of nuclei and exploits the structure similarity\nbetween nuclei to improve the integrality of each segmented instance.\nConcretely, SEINE introduces a contour-based structure encoding (SE) that\nconsiders the correlation between nuclei structure and semantics, realizing a\nreasonable representation of the nuclei structure. Based on the encoding, we\npropose a structure-guided attention (SGA) module that takes the clear nuclei\nas prototypes to enhance the structure learning for the fuzzy nuclei. To\nstrengthen the structural learning ability, a semantic feature fusion (SFF) is\npresented to boost the semantic consistency of semantic and structure branches.\nFurthermore, a position enhancement (PE) method is applied to suppress\nincorrect nuclei boundary predictions. Extensive experiments demonstrate the\nsuperiority of our approaches, and SEINE achieves state-of-the-art (SOTA)\nperformance on four datasets. The code is available at\nhttps://github.com/zhangye-zoe/SEINE.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 12 figures, 6 tables, submitted to TMI",
    "pdf_url": "http://arxiv.org/pdf/2401.09773v2",
    "published_date": "2024-01-18 07:44:04 UTC",
    "updated_date": "2024-02-09 03:14:10 UTC"
  },
  {
    "arxiv_id": "2401.09769v4",
    "title": "A Survey on Learning from Graphs with Heterophily: Recent Advances and Future Directions",
    "authors": [
      "Chenghua Gong",
      "Yao Cheng",
      "Jianxiang Yu",
      "Can Xu",
      "Caihua Shan",
      "Siqiang Luo",
      "Xiang Li"
    ],
    "abstract": "Graphs are structured data that models complex relations between real-world\nentities. Heterophilic graphs, where linked nodes are prone to be with\ndifferent labels or dissimilar features, have recently attracted significant\nattention and found many real-world applications. Meanwhile, increasing efforts\nhave been made to advance learning from graphs with heterophily. Various graph\nheterophily measures, benchmark datasets, and learning paradigms are emerging\nrapidly. In this survey, we comprehensively review existing works on learning\nfrom graphs with heterophily. First, we overview over 500 publications, of\nwhich more than 340 are directly related to heterophilic graphs. After that, we\nsurvey existing metrics of graph heterophily and list recent benchmark\ndatasets. Further, we systematically categorize existing methods based on a\nhierarchical taxonomy including GNN models, learning paradigms and practical\napplications. In addition, broader topics related to graph heterophily are also\nincluded. Finally, we discuss the primary challenges of existing studies and\nhighlight promising avenues for future research.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "64 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.09769v4",
    "published_date": "2024-01-18 07:36:38 UTC",
    "updated_date": "2024-09-30 05:56:58 UTC"
  },
  {
    "arxiv_id": "2401.09763v1",
    "title": "CLIP Model for Images to Textual Prompts Based on Top-k Neighbors",
    "authors": [
      "Xin Zhang",
      "Xin Zhang",
      "YeMing Cai",
      "Tianzhi Jia"
    ],
    "abstract": "Text-to-image synthesis, a subfield of multimodal generation, has gained\nsignificant attention in recent years. We propose a cost-effective approach for\nimage-to-prompt generation that leverages generative models to generate textual\nprompts without the need for large amounts of annotated data. We divide our\nmethod into two stages: online stage and offline stage. We use a combination of\nthe CLIP model and K-nearest neighbors (KNN) algorithm. The proposed system\nconsists of two main parts: an offline task and an online task. Our method owns\nthe highest metric 0.612 among these models, which is 0.013, 0.055, 0.011\nhigher than Clip, Clip + KNN(top 10) respectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CLIP model, KNN, image-to-prompts",
    "pdf_url": "http://arxiv.org/pdf/2401.09763v1",
    "published_date": "2024-01-18 07:28:17 UTC",
    "updated_date": "2024-01-18 07:28:17 UTC"
  },
  {
    "arxiv_id": "2401.09757v1",
    "title": "Cooperative Tri-Point Model-Based Ground-to-Air Coverage Extension in Beyond 5G Networks",
    "authors": [
      "Ziwei Cai",
      "Min Sheng",
      "Junju Liu",
      "Chenxi Zhao",
      "Jiandong Li"
    ],
    "abstract": "The utilization of existing terrestrial infrastructures to provide coverage\nfor aerial users is a potentially low-cost solution. However, the already\ndeployed terrestrial base stations (TBSs) result in weak ground-to-air (G2A)\ncoverage due to the down-tilted antennas. Furthermore, achieving optimal\ncoverage across the entire airspace through antenna adjustment is challenging\ndue to the complex signal coverage requirements in three-dimensional space,\nespecially in the vertical direction. In this paper, we propose a cooperative\ntri-point (CoTP) model-based method that utilizes cooperative beams to enhance\nthe G2A coverage extension. To utilize existing TBSs for establishing effective\ncooperation, we prove that the cooperation among three TBSs can ensure G2A\ncoverage with a minimum coverage overlap, and design the CoTP model to analyze\nthe G2A coverage extension. Using the model, a cooperative coverage structure\nbased on Delaunay triangulation is designed to divide triangular prism-shaped\nsubspaces and corresponding TBS cooperation sets. To enable TBSs in the\ncooperation set to cover different height subspaces while maintaining ground\ncoverage, we design a cooperative beam generation algorithm to maximize the\ncoverage in the triangular prism-shaped airspace. The simulation results and\nfield trials demonstrate that the proposed method can efficiently enhance the\nG2A coverage extension while guaranteeing ground coverage.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09757v1",
    "published_date": "2024-01-18 07:07:44 UTC",
    "updated_date": "2024-01-18 07:07:44 UTC"
  },
  {
    "arxiv_id": "2401.09756v1",
    "title": "Explaining Drift using Shapley Values",
    "authors": [
      "Narayanan U. Edakunni",
      "Utkarsh Tekriwal",
      "Anukriti Jain"
    ],
    "abstract": "Machine learning models often deteriorate in their performance when they are\nused to predict the outcomes over data on which they were not trained. These\nscenarios can often arise in real world when the distribution of data changes\ngradually or abruptly due to major events like a pandemic. There have been many\nattempts in machine learning research to come up with techniques that are\nresilient to such Concept drifts. However, there is no principled framework to\nidentify the drivers behind the drift in model performance. In this paper, we\npropose a novel framework - DBShap that uses Shapley values to identify the\nmain contributors of the drift and quantify their respective contributions. The\nproposed framework not only quantifies the importance of individual features in\ndriving the drift but also includes the change in the underlying relation\nbetween the input and output as a possible driver. The explanation provided by\nDBShap can be used to understand the root cause behind the drift and use it to\nmake the model resilient to the drift.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09756v1",
    "published_date": "2024-01-18 07:07:42 UTC",
    "updated_date": "2024-01-18 07:07:42 UTC"
  },
  {
    "arxiv_id": "2401.10299v1",
    "title": "An attempt to generate new bridge types from latent space of generative flow",
    "authors": [
      "Hongjun Zhang"
    ],
    "abstract": "Through examples of coordinate and probability transformation between\ndifferent distributions, the basic principle of normalizing flow is introduced\nin a simple and concise manner. From the perspective of the distribution of\nrandom variable function, the essence of probability transformation is\nexplained, and the scaling factor Jacobian determinant of probability\ntransformation is introduced. Treating the dataset as a sample from the\npopulation, obtaining normalizing flow is essentially through sampling surveys\nto statistically infer the numerical features of the population, and then the\nloss function is established by using the maximum likelihood estimation method.\nThis article introduces how normalizing flow cleverly solves the two major\napplication challenges of high-dimensional matrix determinant calculation and\nneural network reversible transformation. Using symmetric structured image\ndataset of three-span beam bridge, arch bridge, cable-stayed bridge and\nsuspension bridge, constructing and training normalizing flow based on the Glow\nAPI in the TensorFlow Probability library. The model can smoothly transform the\ncomplex distribution of the bridge dataset into a standard normal distribution,\nand from the obtained latent space sampling, it can generate new bridge types\nthat are different from the training dataset.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.10299v1",
    "published_date": "2024-01-18 06:26:44 UTC",
    "updated_date": "2024-01-18 06:26:44 UTC"
  },
  {
    "arxiv_id": "2401.09748v1",
    "title": "Bootstrapping OTS-Funcimg Pre-training Model (Botfip) -- A Comprehensive Symbolic Regression Framework",
    "authors": [
      "Tianhao Chen",
      "Pengbo Xu",
      "Haibiao Zheng"
    ],
    "abstract": "In the field of scientific computing, many problem-solving approaches tend to\nfocus only on the process and final outcome, even in AI for science, there is a\nlack of deep multimodal information mining behind the data, missing a\nmultimodal framework akin to that in the image-text domain. In this paper, we\ntake Symbolic Regression(SR) as our focal point and, drawing inspiration from\nthe BLIP model in the image-text domain, propose a scientific computing\nmultimodal framework based on Function Images (Funcimg) and Operation Tree\nSequence (OTS), named Bootstrapping OTS-Funcimg Pre-training Model (Botfip). In\nSR experiments, we validate the advantages of Botfip in low-complexity SR\nproblems, showcasing its potential. As a MED framework, Botfip holds promise\nfor future applications in a broader range of scientific computing problems.",
    "categories": [
      "cs.SC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09748v1",
    "published_date": "2024-01-18 06:19:05 UTC",
    "updated_date": "2024-01-18 06:19:05 UTC"
  },
  {
    "arxiv_id": "2401.09717v1",
    "title": "Parameter Selection for Analyzing Conversations with Autism Spectrum Disorder",
    "authors": [
      "Tahiya Chowdhury",
      "Veronica Romero",
      "Amanda Stent"
    ],
    "abstract": "The diagnosis of autism spectrum disorder (ASD) is a complex, challenging\ntask as it depends on the analysis of interactional behaviors by psychologists\nrather than the use of biochemical diagnostics. In this paper, we present a\nmodeling approach to ASD diagnosis by analyzing acoustic/prosodic and\nlinguistic features extracted from diagnostic conversations between a\npsychologist and children who either are typically developing (TD) or have ASD.\nWe compare the contributions of different features across a range of\nconversation tasks. We focus on finding a minimal set of parameters that\ncharacterize conversational behaviors of children with ASD. Because ASD is\ndiagnosed through conversational interaction, in addition to analyzing the\nbehavior of the children, we also investigate whether the psychologist's\nconversational behaviors vary across diagnostic groups. Our results can\nfacilitate fine-grained analysis of conversation data for children with ASD to\nsupport diagnosis and intervention.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "5 pages, 4 tables, Proceedings of INTERSPEECH 2023",
    "pdf_url": "http://arxiv.org/pdf/2401.09717v1",
    "published_date": "2024-01-18 04:28:56 UTC",
    "updated_date": "2024-01-18 04:28:56 UTC"
  },
  {
    "arxiv_id": "2401.09716v1",
    "title": "HCVP: Leveraging Hierarchical Contrastive Visual Prompt for Domain Generalization",
    "authors": [
      "Guanglin Zhou",
      "Zhongyi Han",
      "Shiming Chen",
      "Biwei Huang",
      "Liming Zhu",
      "Tongliang Liu",
      "Lina Yao",
      "Kun Zhang"
    ],
    "abstract": "Domain Generalization (DG) endeavors to create machine learning models that\nexcel in unseen scenarios by learning invariant features. In DG, the prevalent\npractice of constraining models to a fixed structure or uniform\nparameterization to encapsulate invariant features can inadvertently blend\nspecific aspects. Such an approach struggles with nuanced differentiation of\ninter-domain variations and may exhibit bias towards certain domains, hindering\nthe precise learning of domain-invariant features. Recognizing this, we\nintroduce a novel method designed to supplement the model with domain-level and\ntask-specific characteristics. This approach aims to guide the model in more\neffectively separating invariant features from specific characteristics,\nthereby boosting the generalization. Building on the emerging trend of visual\nprompts in the DG paradigm, our work introduces the novel \\textbf{H}ierarchical\n\\textbf{C}ontrastive \\textbf{V}isual \\textbf{P}rompt (HCVP) methodology. This\nrepresents a significant advancement in the field, setting itself apart with a\nunique generative approach to prompts, alongside an explicit model structure\nand specialized loss functions. Differing from traditional visual prompts that\nare often shared across entire datasets, HCVP utilizes a hierarchical prompt\ngeneration network enhanced by prompt contrastive learning. These generative\nprompts are instance-dependent, catering to the unique characteristics inherent\nto different domains and tasks. Additionally, we devise a prompt modulation\nnetwork that serves as a bridge, effectively incorporating the generated visual\nprompts into the vision transformer backbone. Experiments conducted on five DG\ndatasets demonstrate the effectiveness of HCVP, outperforming both established\nDG algorithms and adaptation protocols.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09716v1",
    "published_date": "2024-01-18 04:23:21 UTC",
    "updated_date": "2024-01-18 04:23:21 UTC"
  },
  {
    "arxiv_id": "2401.09699v1",
    "title": "Curriculum Recommendations Using Transformer Base Model with InfoNCE Loss And Language Switching Method",
    "authors": [
      "Xiaonan Xu",
      "Bin Yuan",
      "Yongyao Mo",
      "Tianbo Song",
      "Shulin Li"
    ],
    "abstract": "The Curriculum Recommendations paradigm is dedicated to fostering learning\nequality within the ever-evolving realms of educational technology and\ncurriculum development. In acknowledging the inherent obstacles posed by\nexisting methodologies, such as content conflicts and disruptions from language\ntranslation, this paradigm aims to confront and overcome these challenges.\nNotably, it addresses content conflicts and disruptions introduced by language\ntranslation, hindrances that can impede the creation of an all-encompassing and\npersonalized learning experience. The paradigm's objective is to cultivate an\neducational environment that not only embraces diversity but also customizes\nlearning experiences to suit the distinct needs of each learner. To overcome\nthese challenges, our approach builds upon notable contributions in curriculum\ndevelopment and personalized learning, introducing three key innovations. These\ninclude the integration of Transformer Base Model to enhance computational\nefficiency, the implementation of InfoNCE Loss for accurate content-topic\nmatching, and the adoption of a language switching strategy to alleviate\ntranslation-related ambiguities. Together, these innovations aim to\ncollectively tackle inherent challenges and contribute to forging a more\nequitable and effective learning journey for a diverse range of learners.\nCompetitive cross-validation scores underscore the efficacy of\nsentence-transformers/LaBSE, achieving 0.66314, showcasing our methodology's\neffectiveness in diverse linguistic nuances for content alignment prediction.\nIndex Terms-Curriculum Recommendation, Transformer model with InfoNCE Loss,\nLanguage Switching.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50"
    ],
    "primary_category": "cs.CL",
    "comment": "4pages, 2 figures, ICAICA2023",
    "pdf_url": "http://arxiv.org/pdf/2401.09699v1",
    "published_date": "2024-01-18 03:09:06 UTC",
    "updated_date": "2024-01-18 03:09:06 UTC"
  },
  {
    "arxiv_id": "2401.09695v2",
    "title": "Should ChatGPT Write Your Breakup Text? Exploring the Role of AI in Relationship Dissolution",
    "authors": [
      "Yue Fu",
      "Yixin Chen",
      "Zelia Gomes Da Costa Lai",
      "Alexis Hiniker"
    ],
    "abstract": "Relationships are essential to our happiness and wellbeing, yet their\ndissolution-the final stage of a relationship's lifecycle-is among the most\nstressful events individuals can experience, often leading to profound and\nlasting impacts. With the breakup process increasingly facilitated by\ntechnology, such as computer-mediated communication, and the likely future\ninfluence of generative AI (GenAI) tools, we conducted a semi-structured\ninterview study with 21 participants. We aim to understand: 1) the current role\nof technology in the breakup process, 2) the needs and support individuals seek\nduring this time, and 3) how GenAI might address or undermine these needs. Our\nfindings show that people have distinct needs at various stages of breakups.\nWhile currently technology plays an important role, it falls short in\nsupporting users' unmet needs. Participants envision that GenAI could: 1) aid\nin prompting self-reflection, providing neutral second opinions, and assisting\nwith planning leading up to a breakup; 2) serve as a communication mediator,\nsupporting wording and tone to facilitate emotional expression during breakup\nconversations; and 3) support personal growth and offer companionship after a\nbreakup. However, our findings also reveal participants' concerns about\ninvolving GenAI in this process. Based on our results, we discuss the potential\nopportunities, design considerations, and harms of GenAI tools in facilitating\npeople's relationship dissolution.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09695v2",
    "published_date": "2024-01-18 02:53:36 UTC",
    "updated_date": "2024-10-31 18:59:30 UTC"
  },
  {
    "arxiv_id": "2401.09691v2",
    "title": "Imitation Learning Inputting Image Feature to Each Layer of Neural Network",
    "authors": [
      "Koki Yamane",
      "Sho Sakaino",
      "Toshiaki Tsuji"
    ],
    "abstract": "Imitation learning enables robots to learn and replicate human behavior from\ntraining data. Recent advances in machine learning enable end-to-end learning\napproaches that directly process high-dimensional observation data, such as\nimages. However, these approaches face a critical challenge when processing\ndata from multiple modalities, inadvertently ignoring data with a lower\ncorrelation to the desired output, especially when using short sampling\nperiods. This paper presents a useful method to address this challenge, which\namplifies the influence of data with a relatively low correlation to the output\nby inputting the data into each neural network layer. The proposed approach\neffectively incorporates diverse data sources into the learning process.\nThrough experiments using a simple pick-and-place operation with raw images and\njoint information as input, significant improvements in success rates are\ndemonstrated even when dealing with data from short sampling periods.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 4 figures, Accepted at AMC2024",
    "pdf_url": "http://arxiv.org/pdf/2401.09691v2",
    "published_date": "2024-01-18 02:44:18 UTC",
    "updated_date": "2024-01-19 12:43:36 UTC"
  },
  {
    "arxiv_id": "2401.09680v2",
    "title": "Tiny Multi-Agent DRL for Twins Migration in UAV Metaverses: A Multi-Leader Multi-Follower Stackelberg Game Approach",
    "authors": [
      "Jiawen Kang",
      "Yue Zhong",
      "Minrui Xu",
      "Jiangtian Nie",
      "Jinbo Wen",
      "Hongyang Du",
      "Dongdong Ye",
      "Xumin Huang",
      "Dusit Niyato",
      "Shengli Xie"
    ],
    "abstract": "The synergy between Unmanned Aerial Vehicles (UAVs) and metaverses is giving\nrise to an emerging paradigm named UAV metaverses, which create a unified\necosystem that blends physical and virtual spaces, transforming drone\ninteraction and virtual exploration. UAV Twins (UTs), as the digital twins of\nUAVs that revolutionize UAV applications by making them more immersive,\nrealistic, and informative, are deployed and updated on ground base stations,\ne.g., RoadSide Units (RSUs), to offer metaverse services for UAV Metaverse\nUsers (UMUs). Due to the dynamic mobility of UAVs and limited communication\ncoverages of RSUs, it is essential to perform real-time UT migration to ensure\nseamless immersive experiences for UMUs. However, selecting appropriate RSUs\nand optimizing the required bandwidth is challenging for achieving reliable and\nefficient UT migration. To address the challenges, we propose a tiny machine\nlearning-based Stackelberg game framework based on pruning techniques for\nefficient UT migration in UAV metaverses. Specifically, we formulate a\nmulti-leader multi-follower Stackelberg model considering a new immersion\nmetric of UMUs in the utilities of UAVs. Then, we design a Tiny Multi-Agent\nDeep Reinforcement Learning (Tiny MADRL) algorithm to obtain the tiny networks\nrepresenting the optimal game solution. Specifically, the actor-critic network\nleverages the pruning techniques to reduce the number of network parameters and\nachieve model size and computation reduction, allowing for efficient\nimplementation of Tiny MADRL. Numerical results demonstrate that our proposed\nschemes have better performance than traditional schemes.",
    "categories": [
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09680v2",
    "published_date": "2024-01-18 02:14:13 UTC",
    "updated_date": "2024-04-08 12:31:58 UTC"
  },
  {
    "arxiv_id": "2401.09671v2",
    "title": "Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach",
    "authors": [
      "Sagar Shrestha",
      "Xiao Fu"
    ],
    "abstract": "Unsupervised domain translation (UDT) aims to find functions that convert\nsamples from one domain (e.g., sketches) to another domain (e.g., photos)\nwithout changing the high-level semantic meaning (also referred to as\n``content''). The translation functions are often sought by probability\ndistribution matching of the transformed source domain and target domain.\nCycleGAN stands as arguably the most representative approach among this line of\nwork. However, it was noticed in the literature that CycleGAN and variants\ncould fail to identify the desired translation functions and produce\ncontent-misaligned translations. This limitation arises due to the presence of\nmultiple translation functions -- referred to as ``measure-preserving\nautomorphism\" (MPA) -- in the solution space of the learning criteria. Despite\nawareness of such identifiability issues, solutions have remained elusive. This\nstudy delves into the core identifiability inquiry and introduces an MPA\nelimination theory. Our analysis shows that MPA is unlikely to exist, if\nmultiple pairs of diverse cross-domain conditional distributions are matched by\nthe learning function. Our theory leads to a UDT learner using distribution\nmatching over auxiliary variable-induced subsets of the domains -- other than\nover the entire data domains as in the classical approaches. The proposed\nframework is the first to rigorously establish translation identifiability\nunder reasonable UDT settings, to our best knowledge. Experiments corroborate\nwith our theoretical claims.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09671v2",
    "published_date": "2024-01-18 01:07:00 UTC",
    "updated_date": "2024-01-21 07:27:25 UTC"
  },
  {
    "arxiv_id": "2401.09666v1",
    "title": "Traffic Smoothing Controllers for Autonomous Vehicles Using Deep Reinforcement Learning and Real-World Trajectory Data",
    "authors": [
      "Nathan Lichtlé",
      "Kathy Jang",
      "Adit Shah",
      "Eugene Vinitsky",
      "Jonathan W. Lee",
      "Alexandre M. Bayen"
    ],
    "abstract": "Designing traffic-smoothing cruise controllers that can be deployed onto\nautonomous vehicles is a key step towards improving traffic flow, reducing\ncongestion, and enhancing fuel efficiency in mixed autonomy traffic. We bypass\nthe common issue of having to carefully fine-tune a large traffic\nmicrosimulator by leveraging real-world trajectory data from the I-24 highway\nin Tennessee, replayed in a one-lane simulation. Using standard deep\nreinforcement learning methods, we train energy-reducing wave-smoothing\npolicies. As an input to the agent, we observe the speed and distance of only\nthe vehicle in front, which are local states readily available on most recent\nvehicles, as well as non-local observations about the downstream state of the\ntraffic. We show that at a low 4% autonomous vehicle penetration rate, we\nachieve significant fuel savings of over 15% on trajectories exhibiting many\nstop-and-go waves. Finally, we analyze the smoothing effect of the controllers\nand demonstrate robustness to adding lane-changing into the simulation as well\nas the removal of downstream information.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.MA",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "Accepted to be published as part of the 26th IEEE International\n  Conference on Intelligent Transportation Systems (ITSC) 2023, Bilbao, Spain,\n  September 24-28, 2023",
    "pdf_url": "http://arxiv.org/pdf/2401.09666v1",
    "published_date": "2024-01-18 00:50:41 UTC",
    "updated_date": "2024-01-18 00:50:41 UTC"
  },
  {
    "arxiv_id": "2401.09656v1",
    "title": "Mobility Accelerates Learning: Convergence Analysis on Hierarchical Federated Learning in Vehicular Networks",
    "authors": [
      "Tan Chen",
      "Jintao Yan",
      "Yuxuan Sun",
      "Sheng Zhou",
      "Deniz Gündüz",
      "Zhisheng Niu"
    ],
    "abstract": "Hierarchical federated learning (HFL) enables distributed training of models\nacross multiple devices with the help of several edge servers and a cloud edge\nserver in a privacy-preserving manner. In this paper, we consider HFL with\nhighly mobile devices, mainly targeting at vehicular networks. Through\nconvergence analysis, we show that mobility influences the convergence speed by\nboth fusing the edge data and shuffling the edge models. While mobility is\nusually considered as a challenge from the perspective of communication, we\nprove that it increases the convergence speed of HFL with edge-level\nheterogeneous data, since more diverse data can be incorporated. Furthermore,\nwe demonstrate that a higher speed leads to faster convergence, since it\naccelerates the fusion of data. Simulation results show that mobility increases\nthe model accuracy of HFL by up to 15.1% when training a convolutional neural\nnetwork on the CIFAR-10 dataset.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2401.09656v1",
    "published_date": "2024-01-18 00:09:54 UTC",
    "updated_date": "2024-01-18 00:09:54 UTC"
  }
]
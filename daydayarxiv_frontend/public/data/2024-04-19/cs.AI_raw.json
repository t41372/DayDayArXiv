[
  {
    "arxiv_id": "2404.13218v1",
    "title": "On the Temperature of Machine Learning Systems",
    "authors": [
      "Dong Zhang"
    ],
    "abstract": "We develop a thermodynamic theory for machine learning (ML) systems. Similar\nto physical thermodynamic systems which are characterized by energy and\nentropy, ML systems possess these characteristics as well. This comparison\ninspire us to integrate the concept of temperature into ML systems grounded in\nthe fundamental principles of thermodynamics, and establish a basic\nthermodynamic framework for machine learning systems with non-Boltzmann\ndistributions. We introduce the concept of states within a ML system, identify\ntwo typical types of state, and interpret model training and refresh as a\nprocess of state phase transition. We consider that the initial potential\nenergy of a ML system is described by the model's loss functions, and the\nenergy adheres to the principle of minimum potential energy. For a variety of\nenergy forms and parameter initialization methods, we derive the temperature of\nsystems during the phase transition both analytically and asymptotically,\nhighlighting temperature as a vital indicator of system data distribution and\nML training complexity. Moreover, we perceive deep neural networks as complex\nheat engines with both global temperature and local temperatures in each layer.\nThe concept of work efficiency is introduced within neural networks, which\nmainly depends on the neural activation functions. We then classify neural\nnetworks based on their work efficiency, and describe neural networks as two\ntypes of heat engines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "44 pages, 8 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.13218v1",
    "published_date": "2024-04-19 23:54:32 UTC",
    "updated_date": "2024-04-19 23:54:32 UTC"
  },
  {
    "arxiv_id": "2404.13194v1",
    "title": "Privacy-Preserving Debiasing using Data Augmentation and Machine Unlearning",
    "authors": [
      "Zhixin Pan",
      "Emma Andrews",
      "Laura Chang",
      "Prabhat Mishra"
    ],
    "abstract": "Data augmentation is widely used to mitigate data bias in the training\ndataset. However, data augmentation exposes machine learning models to privacy\nattacks, such as membership inference attacks. In this paper, we propose an\neffective combination of data augmentation and machine unlearning, which can\nreduce data bias while providing a provable defense against known attacks.\nSpecifically, we maintain the fairness of the trained model with\ndiffusion-based data augmentation, and then utilize multi-shard unlearning to\nremove identifying information of original data from the ML model for\nprotection against privacy attacks. Experimental evaluation across diverse\ndatasets demonstrates that our approach can achieve significant improvements in\nbias reduction as well as robustness against state-of-the-art privacy attacks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13194v1",
    "published_date": "2024-04-19 21:54:20 UTC",
    "updated_date": "2024-04-19 21:54:20 UTC"
  },
  {
    "arxiv_id": "2404.15370v1",
    "title": "Self-Supervised Learning for User Localization",
    "authors": [
      "Ankan Dash",
      "Jingyi Gu",
      "Guiling Wang",
      "Nirwan Ansari"
    ],
    "abstract": "Machine learning techniques have shown remarkable accuracy in localization\ntasks, but their dependency on vast amounts of labeled data, particularly\nChannel State Information (CSI) and corresponding coordinates, remains a\nbottleneck. Self-supervised learning techniques alleviate the need for labeled\ndata, a potential that remains largely untapped and underexplored in existing\nresearch. Addressing this gap, we propose a pioneering approach that leverages\nself-supervised pretraining on unlabeled data to boost the performance of\nsupervised learning for user localization based on CSI. We introduce two\npretraining Auto Encoder (AE) models employing Multi Layer Perceptrons (MLPs)\nand Convolutional Neural Networks (CNNs) to glean representations from\nunlabeled data via self-supervised learning. Following this, we utilize the\nencoder portion of the AE models to extract relevant features from labeled\ndata, and finetune an MLP-based Position Estimation Model to accurately deduce\nuser locations. Our experimentation on the CTW-2020 dataset, which features a\nsubstantial volume of unlabeled data but limited labeled samples, demonstrates\nthe viability of our approach. Notably, the dataset covers a vast area spanning\nover 646x943x41 meters, and our approach demonstrates promising results even\nfor such expansive localization tasks.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15370v1",
    "published_date": "2024-04-19 21:49:10 UTC",
    "updated_date": "2024-04-19 21:49:10 UTC"
  },
  {
    "arxiv_id": "2404.13192v1",
    "title": "Heterogeneous Subgraph Transformer for Fake News Detection",
    "authors": [
      "Yuchen Zhang",
      "Xiaoxiao Ma",
      "Jia Wu",
      "Jian Yang",
      "Hao Fan"
    ],
    "abstract": "Fake news is pervasive on social media, inflicting substantial harm on public\ndiscourse and societal well-being. We investigate the explicit structural\ninformation and textual features of news pieces by constructing a heterogeneous\ngraph concerning the relations among news topics, entities, and content.\nThrough our study, we reveal that fake news can be effectively detected in\nterms of the atypical heterogeneous subgraphs centered on them, which\nencapsulate the essential semantics and intricate relations between news\nelements. However, suffering from the heterogeneity, exploring such\nheterogeneous subgraphs remains an open problem. To bridge the gap, this work\nproposes a heterogeneous subgraph transformer (HeteroSGT) to exploit subgraphs\nin our constructed heterogeneous graph. In HeteroSGT, we first employ a\npre-trained language model to derive both word-level and sentence-level\nsemantics. Then the random walk with restart (RWR) is applied to extract\nsubgraphs centered on each news, which are further fed to our proposed subgraph\nTransformer to quantify the authenticity. Extensive experiments on five\nreal-world datasets demonstrate the superior performance of HeteroSGT over five\nbaselines. Further case and ablation studies validate our motivation and\ndemonstrate that performance improvement stems from our specially designed\ncomponents.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13192v1",
    "published_date": "2024-04-19 21:39:37 UTC",
    "updated_date": "2024-04-19 21:39:37 UTC"
  },
  {
    "arxiv_id": "2404.13150v1",
    "title": "Transformer Based Planning in the Observation Space with Applications to Trick Taking Card Games",
    "authors": [
      "Douglas Rebstock",
      "Christopher Solinas",
      "Nathan R. Sturtevant",
      "Michael Buro"
    ],
    "abstract": "Traditional search algorithms have issues when applied to games of imperfect\ninformation where the number of possible underlying states and trajectories are\nvery large. This challenge is particularly evident in trick-taking card games.\nWhile state sampling techniques such as Perfect Information Monte Carlo (PIMC)\nsearch has shown success in these contexts, they still have major limitations.\n  We present Generative Observation Monte Carlo Tree Search (GO-MCTS), which\nutilizes MCTS on observation sequences generated by a game specific model. This\nmethod performs the search within the observation space and advances the search\nusing a model that depends solely on the agent's observations. Additionally, we\ndemonstrate that transformers are well-suited as the generative model in this\ncontext, and we demonstrate a process for iteratively training the transformer\nvia population-based self-play.\n  The efficacy of GO-MCTS is demonstrated in various games of imperfect\ninformation, such as Hearts, Skat, and \"The Crew: The Quest for Planet Nine,\"\nwith promising results.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13150v1",
    "published_date": "2024-04-19 19:41:00 UTC",
    "updated_date": "2024-04-19 19:41:00 UTC"
  },
  {
    "arxiv_id": "2404.13149v1",
    "title": "Beyond Self-Consistency: Ensemble Reasoning Boosts Consistency and Accuracy of LLMs in Cancer Staging",
    "authors": [
      "Chia-Hsuan Chang",
      "Mary M. Lucas",
      "Yeawon Lee",
      "Christopher C. Yang",
      "Grace Lu-Yao"
    ],
    "abstract": "Advances in large language models (LLMs) have encouraged their adoption in\nthe healthcare domain where vital clinical information is often contained in\nunstructured notes. Cancer staging status is available in clinical reports, but\nit requires natural language processing to extract the status from the\nunstructured text. With the advance in clinical-oriented LLMs, it is promising\nto extract such status without extensive efforts in training the algorithms.\nPrompting approaches of the pre-trained LLMs that elicit a model's reasoning\nprocess, such as chain-of-thought, may help to improve the trustworthiness of\nthe generated responses. Using self-consistency further improves model\nperformance, but often results in inconsistent generations across the multiple\nreasoning paths. In this study, we propose an ensemble reasoning approach with\nthe aim of improving the consistency of the model generations. Using an open\naccess clinical large language model to determine the pathologic cancer stage\nfrom real-world pathology reports, we show that the ensemble reasoning approach\nis able to improve both the consistency and performance of the LLM in\ndetermining cancer stage, thereby demonstrating the potential to use these\nmodels in clinical or other domains where reliability and trustworthiness are\ncritical.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "accepted to the 22nd International Conference on Artificial\n  Intelligence in Medicine (AIME'24)",
    "pdf_url": "http://arxiv.org/pdf/2404.13149v1",
    "published_date": "2024-04-19 19:34:35 UTC",
    "updated_date": "2024-04-19 19:34:35 UTC"
  },
  {
    "arxiv_id": "2404.13142v2",
    "title": "Decentralized Coordination of Distributed Energy Resources through Local Energy Markets and Deep Reinforcement Learning",
    "authors": [
      "Daniel May",
      "Matthew Taylor",
      "Petr Musilek"
    ],
    "abstract": "As distributed energy resources (DERs) grow, the electricity grid faces\nincreased net load variability at the grid edge, impacting operability and\nreliability. Transactive energy, facilitated through local energy markets,\noffers a decentralized, indirect demand response solution, with model-free\ncontrol techniques, such as deep reinforcement learning (DRL), enabling\nautomated, decentralized participation. However, existing studies largely\noverlook community-level net load variability, focusing instead on\nsocioeconomic metrics.\n  This study addresses this gap by using DRL agents to automate end-user\nparticipation in a local energy market (ALEX), where agents act independently\nto minimize individual energy bills. Results reveal a strong link between bill\nreduction and decreased net load variability, assessed across metrics such as\nramping rate, load factor, and peak demand over various time horizons. Using a\nno-control baseline, DRL agents are benchmarked against a near-optimal dynamic\nprogramming approach. The dynamic programming benchmark achieves reductions of\n22.05 percent, 83.92 percent, and 24.09 percent in daily import, export, and\npeak demand, respectively, while the DRL agents show comparable or superior\nresults with reductions of 21.93 percent, 84.46 percent, and 27.02 percent.\n  This study demonstrates the effectiveness of DRL in decentralized grid\nmanagement, highlighting its scalability and near-optimal performance in\nreducing net load variability within community-driven energy markets.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "preprint, submitted to Energy and AI",
    "pdf_url": "http://arxiv.org/pdf/2404.13142v2",
    "published_date": "2024-04-19 19:03:33 UTC",
    "updated_date": "2024-11-14 19:36:14 UTC"
  },
  {
    "arxiv_id": "2404.13139v1",
    "title": "Explainable AI for Fair Sepsis Mortality Predictive Model",
    "authors": [
      "Chia-Hsuan Chang",
      "Xiaoyang Wang",
      "Christopher C. Yang"
    ],
    "abstract": "Artificial intelligence supports healthcare professionals with predictive\nmodeling, greatly transforming clinical decision-making. This study addresses\nthe crucial need for fairness and explainability in AI applications within\nhealthcare to ensure equitable outcomes across diverse patient demographics. By\nfocusing on the predictive modeling of sepsis-related mortality, we propose a\nmethod that learns a performance-optimized predictive model and then employs\nthe transfer learning process to produce a model with better fairness. Our\nmethod also introduces a novel permutation-based feature importance algorithm\naiming at elucidating the contribution of each feature in enhancing fairness on\npredictions. Unlike existing explainability methods concentrating on explaining\nfeature contribution to predictive performance, our proposed method uniquely\nbridges the gap in understanding how each feature contributes to fairness. This\nadvancement is pivotal, given sepsis's significant mortality rate and its role\nin one-third of hospital deaths. Our method not only aids in identifying and\nmitigating biases within the predictive model but also fosters trust among\nhealthcare stakeholders by improving the transparency and fairness of model\npredictions, thereby contributing to more equitable and trustworthy healthcare\ndelivery.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the 22nd International Conference on Artificial\n  Intelligence in Medicine (AIME'24)",
    "pdf_url": "http://arxiv.org/pdf/2404.13139v1",
    "published_date": "2024-04-19 18:56:46 UTC",
    "updated_date": "2024-04-19 18:56:46 UTC"
  },
  {
    "arxiv_id": "2404.15369v2",
    "title": "Can a Machine be Conscious? Towards Universal Criteria for Machine Consciousness",
    "authors": [
      "Nur Aizaan Anwar",
      "Cosmin Badea"
    ],
    "abstract": "As artificially intelligent systems become more anthropomorphic and\npervasive, and their potential impact on humanity more urgent, discussions\nabout the possibility of machine consciousness have significantly intensified,\nand it is sometimes seen as 'the holy grail'. Many concerns have been voiced\nabout the ramifications of creating an artificial conscious entity. This is\ncompounded by a marked lack of consensus around what constitutes consciousness\nand by an absence of a universal set of criteria for determining consciousness.\nBy going into depth on the foundations and characteristics of consciousness, we\npropose five criteria for determining whether a machine is conscious, which can\nalso be applied more generally to any entity. This paper aims to serve as a\nprimer and stepping stone for researchers of consciousness, be they in\nphilosophy, computer science, medicine, or any other field, to further pursue\nthis holy grail of philosophy, neuroscience and artificial intelligence.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "q-bio.NC",
    "comment": "This work was supported by the UKRI CDT in AI for Healthcare,\n  http://ai4health.io (Grant No. EP/S023283/1)",
    "pdf_url": "http://arxiv.org/pdf/2404.15369v2",
    "published_date": "2024-04-19 18:38:22 UTC",
    "updated_date": "2024-04-30 17:28:30 UTC"
  },
  {
    "arxiv_id": "2404.13131v1",
    "title": "From Model Performance to Claim: How a Change of Focus in Machine Learning Replicability Can Help Bridge the Responsibility Gap",
    "authors": [
      "Tianqi Kou"
    ],
    "abstract": "Two goals - improving replicability and accountability of Machine Learning\nresearch respectively, have accrued much attention from the AI ethics and the\nMachine Learning community. Despite sharing the measures of improving\ntransparency, the two goals are discussed in different registers -\nreplicability registers with scientific reasoning whereas accountability\nregisters with ethical reasoning. Given the existing challenge of the\nResponsibility Gap - holding Machine Learning scientists accountable for\nMachine Learning harms due to them being far from sites of application, this\npaper posits that reconceptualizing replicability can help bridge the gap.\nThrough a shift from model performance replicability to claim replicability,\nMachine Learning scientists can be held accountable for producing\nnon-replicable claims that are prone to eliciting harm due to misuse and\nmisinterpretation. In this paper, I make the following contributions. First, I\ndefine and distinguish two forms of replicability for ML research that can aid\nconstructive conversations around replicability. Second, I formulate an\nargument for claim-replicability's advantage over model performance\nreplicability in justifying assigning accountability to Machine Learning\nscientists for producing non-replicable claims and show how it enacts a sense\nof responsibility that is actionable. In addition, I characterize the\nimplementation of claim replicability as more of a social project than a\ntechnical one by discussing its competing epistemological principles, practical\nimplications on Circulating Reference, Interpretative Labor, and research\ncommunication.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "Forthcoming in FAccT 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.13131v1",
    "published_date": "2024-04-19 18:36:14 UTC",
    "updated_date": "2024-04-19 18:36:14 UTC"
  },
  {
    "arxiv_id": "2404.13038v1",
    "title": "Mapping Social Choice Theory to RLHF",
    "authors": [
      "Jessica Dai",
      "Eve Fleisig"
    ],
    "abstract": "Recent work on the limitations of using reinforcement learning from human\nfeedback (RLHF) to incorporate human preferences into model behavior often\nraises social choice theory as a reference point. Social choice theory's\nanalysis of settings such as voting mechanisms provides technical\ninfrastructure that can inform how to aggregate human preferences amid\ndisagreement. We analyze the problem settings of social choice and RLHF,\nidentify key differences between them, and discuss how these differences may\naffect the RLHF interpretation of well-known technical results in social\nchoice.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13038v1",
    "published_date": "2024-04-19 17:49:56 UTC",
    "updated_date": "2024-04-19 17:49:56 UTC"
  },
  {
    "arxiv_id": "2404.13028v1",
    "title": "When Life gives you LLMs, make LLM-ADE: Large Language Models with Adaptive Data Engineering",
    "authors": [
      "Stephen Choi",
      "William Gazeley"
    ],
    "abstract": "This paper presents the LLM-ADE framework, a novel methodology for continued\npre-training of large language models (LLMs) that addresses the challenges of\ncatastrophic forgetting and double descent. LLM-ADE employs dynamic\narchitectural adjustments, including selective block freezing and expansion,\ntailored to specific datasets. This strategy enhances model adaptability to new\ndata while preserving previously acquired knowledge. We demonstrate LLM-ADE's\neffectiveness on the TinyLlama model across various general knowledge\nbenchmarks, showing significant performance improvements without the drawbacks\nof traditional continuous training methods. This approach promises a more\nversatile and robust way to keep LLMs current and efficient in real-world\napplications.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "6 pages, 3 tables and 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.13028v1",
    "published_date": "2024-04-19 17:43:26 UTC",
    "updated_date": "2024-04-19 17:43:26 UTC"
  },
  {
    "arxiv_id": "2404.13026v2",
    "title": "PhysDreamer: Physics-Based Interaction with 3D Objects via Video Generation",
    "authors": [
      "Tianyuan Zhang",
      "Hong-Xing Yu",
      "Rundi Wu",
      "Brandon Y. Feng",
      "Changxi Zheng",
      "Noah Snavely",
      "Jiajun Wu",
      "William T. Freeman"
    ],
    "abstract": "Realistic object interactions are crucial for creating immersive virtual\nexperiences, yet synthesizing realistic 3D object dynamics in response to novel\ninteractions remains a significant challenge. Unlike unconditional or\ntext-conditioned dynamics generation, action-conditioned dynamics requires\nperceiving the physical material properties of objects and grounding the 3D\nmotion prediction on these properties, such as object stiffness. However,\nestimating physical material properties is an open problem due to the lack of\nmaterial ground-truth data, as measuring these properties for real objects is\nhighly difficult. We present PhysDreamer, a physics-based approach that endows\nstatic 3D objects with interactive dynamics by leveraging the object dynamics\npriors learned by video generation models. By distilling these priors,\nPhysDreamer enables the synthesis of realistic object responses to novel\ninteractions, such as external forces or agent manipulations. We demonstrate\nour approach on diverse examples of elastic objects and evaluate the realism of\nthe synthesized interactions through a user study. PhysDreamer takes a step\ntowards more engaging and realistic virtual experiences by enabling static 3D\nobjects to dynamically respond to interactive stimuli in a physically plausible\nmanner. See our project page at https://physdreamer.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project website at: https://physdreamer.github.io/ Appear on ECCV\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2404.13026v2",
    "published_date": "2024-04-19 17:41:05 UTC",
    "updated_date": "2024-10-07 06:08:09 UTC"
  },
  {
    "arxiv_id": "2404.13013v1",
    "title": "Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models",
    "authors": [
      "Chuofan Ma",
      "Yi Jiang",
      "Jiannan Wu",
      "Zehuan Yuan",
      "Xiaojuan Qi"
    ],
    "abstract": "We introduce Groma, a Multimodal Large Language Model (MLLM) with grounded\nand fine-grained visual perception ability. Beyond holistic image\nunderstanding, Groma is adept at region-level tasks such as region captioning\nand visual grounding. Such capabilities are built upon a localized visual\ntokenization mechanism, where an image input is decomposed into regions of\ninterest and subsequently encoded into region tokens. By integrating region\ntokens into user instructions and model responses, we seamlessly enable Groma\nto understand user-specified region inputs and ground its textual output to\nimages. Besides, to enhance the grounded chat ability of Groma, we curate a\nvisually grounded instruction dataset by leveraging the powerful GPT-4V and\nvisual prompting techniques. Compared with MLLMs that rely on the language\nmodel or external module for localization, Groma consistently demonstrates\nsuperior performances in standard referring and grounding benchmarks,\nhighlighting the advantages of embedding localization into image tokenization.\nProject page: https://groma-mllm.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13013v1",
    "published_date": "2024-04-19 17:22:51 UTC",
    "updated_date": "2024-04-19 17:22:51 UTC"
  },
  {
    "arxiv_id": "2404.13004v3",
    "title": "A Generative Approach to Credit Prediction with Learnable Prompts for Multi-scale Temporal Representation Learning",
    "authors": [
      "Yu Lei",
      "Zixuan Wang",
      "Yiqing Feng",
      "Junru Zhang",
      "Yahui Li",
      "Chu Liu",
      "Tongyao Wang"
    ],
    "abstract": "Recent industrial credit scoring models remain heavily reliant on manually\ntuned statistical learning methods. While deep learning offers promising\nsolutions, its effectiveness is often limited by the complexity of financial\ndata, particularly in long-horizon scenarios. In this work, we propose\nFinLangNet, which addresses credit scoring by reframing it as the task of\ngenerating multi-scale distributions of a user's future behavior. Within this\nframework, tabular data is transformed into sequential representations,\nenabling the generation of user embeddings across multiple temporal scales.\nInspired by the recent success of prompt-based training in Large Language\nModels (LLMs), FinLangNet also introduces two types of prompts to model and\ncapture user behavior at both the feature-granularity and user-granularity\nlevels. Experimental results demonstrate that FinLangNet outperforms the online\nXGBoost benchmark, achieving a 7.2\\% improvement in KS metric performance and a\n9.9\\% reduction in the relative bad debt rate. Furthermore, FinLangNet exhibits\nsuperior performance on public UEA archives, underscoring its scalability and\nadaptability in time series classification tasks.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13004v3",
    "published_date": "2024-04-19 17:01:46 UTC",
    "updated_date": "2025-02-22 10:23:03 UTC"
  },
  {
    "arxiv_id": "2404.12999v1",
    "title": "Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned Reinforcement Learning",
    "authors": [
      "Lisheng Wu",
      "Ke Chen"
    ],
    "abstract": "Exploration efficiency poses a significant challenge in goal-conditioned\nreinforcement learning (GCRL) tasks, particularly those with long horizons and\nsparse rewards. A primary limitation to exploration efficiency is the agent's\ninability to leverage environmental structural patterns. In this study, we\nintroduce a novel framework, GEASD, designed to capture these patterns through\nan adaptive skill distribution during the learning process. This distribution\noptimizes the local entropy of achieved goals within a contextual horizon,\nenhancing goal-spreading behaviors and facilitating deep exploration in states\ncontaining familiar structural patterns. Our experiments reveal marked\nimprovements in exploration efficiency using the adaptive skill distribution\ncompared to a uniform skill distribution. Additionally, the learned skill\ndistribution demonstrates robust generalization capabilities, achieving\nsubstantial exploration progress in unseen tasks containing similar local\nstructures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12999v1",
    "published_date": "2024-04-19 16:54:55 UTC",
    "updated_date": "2024-04-19 16:54:55 UTC"
  },
  {
    "arxiv_id": "2404.12984v2",
    "title": "Eye-tracking in Mixed Reality for Diagnosis of Neurodegenerative Diseases",
    "authors": [
      "Mateusz Daniol",
      "Daria Hemmerling",
      "Jakub Sikora",
      "Pawel Jemiolo",
      "Marek Wodzinski",
      "Magdalena Wojcik-Pedziwiatr"
    ],
    "abstract": "Parkinson's disease ranks as the second most prevalent neurodegenerative\ndisorder globally. This research aims to develop a system leveraging Mixed\nReality capabilities for tracking and assessing eye movements. In this paper,\nwe present a medical scenario and outline the development of an application\ndesigned to capture eye-tracking signals through Mixed Reality technology for\nthe evaluation of neurodegenerative diseases. Additionally, we introduce a\npipeline for extracting clinically relevant features from eye-gaze analysis,\ndescribing the capabilities of the proposed system from a medical perspective.\nThe study involved a cohort of healthy control individuals and patients\nsuffering from Parkinson's disease, showcasing the feasibility and potential of\nthe proposed technology for non-intrusive monitoring of eye movement patterns\nfor the diagnosis of neurodegenerative diseases.\n  Clinical relevance - Developing a non-invasive biomarker for Parkinson's\ndisease is urgently needed to accurately detect the disease's onset. This would\nallow for the timely introduction of neuroprotective treatment at the earliest\nstage and enable the continuous monitoring of intervention outcomes. The\nability to detect subtle changes in eye movements allows for early diagnosis,\noffering a critical window for intervention before more pronounced symptoms\nemerge. Eye tracking provides objective and quantifiable biomarkers, ensuring\nreliable assessments of disease progression and cognitive function. The eye\ngaze analysis using Mixed Reality glasses is wireless, facilitating convenient\nassessments in both home and hospital settings. The approach offers the\nadvantage of utilizing hardware that requires no additional specialized\nattachments, enabling examinations through personal eyewear.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12984v2",
    "published_date": "2024-04-19 16:34:15 UTC",
    "updated_date": "2024-06-03 10:45:42 UTC"
  },
  {
    "arxiv_id": "2404.12975v1",
    "title": "FineRec:Exploring Fine-grained Sequential Recommendation",
    "authors": [
      "Xiaokun Zhang",
      "Bo Xu",
      "Youlin Wu",
      "Yuan Zhong",
      "Hongfei Lin",
      "Fenglong Ma"
    ],
    "abstract": "Sequential recommendation is dedicated to offering items of interest for\nusers based on their history behaviors. The attribute-opinion pairs, expressed\nby users in their reviews for items, provide the potentials to capture user\npreferences and item characteristics at a fine-grained level. To this end, we\npropose a novel framework FineRec that explores the attribute-opinion pairs of\nreviews to finely handle sequential recommendation. Specifically, we utilize a\nlarge language model to extract attribute-opinion pairs from reviews. For each\nattribute, a unique attribute-specific user-opinion-item graph is created,\nwhere corresponding opinions serve as the edges linking heterogeneous user and\nitem nodes. To tackle the diversity of opinions, we devise a diversity-aware\nconvolution operation to aggregate information within the graphs, enabling\nattribute-specific user and item representation learning. Ultimately, we\npresent an interaction-driven fusion mechanism to integrate attribute-specific\nuser/item representations across all attributes for generating recommendations.\nExtensive experiments conducted on several realworld datasets demonstrate the\nsuperiority of our FineRec over existing state-of-the-art methods. Further\nanalysis also verifies the effectiveness of our fine-grained manner in handling\nthe task.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "This work has been accepted by SIGIR24' as a full paper",
    "pdf_url": "http://arxiv.org/pdf/2404.12975v1",
    "published_date": "2024-04-19 16:04:26 UTC",
    "updated_date": "2024-04-19 16:04:26 UTC"
  },
  {
    "arxiv_id": "2404.12969v1",
    "title": "Disentangling ID and Modality Effects for Session-based Recommendation",
    "authors": [
      "Xiaokun Zhang",
      "Bo Xu",
      "Zhaochun Ren",
      "Xiaochen Wang",
      "Hongfei Lin",
      "Fenglong Ma"
    ],
    "abstract": "Session-based recommendation aims to predict intents of anonymous users based\non their limited behaviors. Modeling user behaviors involves two distinct\nrationales: co-occurrence patterns reflected by item IDs, and fine-grained\npreferences represented by item modalities (e.g., text and images). However,\nexisting methods typically entangle these causes, leading to their failure in\nachieving accurate and explainable recommendations. To this end, we propose a\nnovel framework DIMO to disentangle the effects of ID and modality in the task.\nAt the item level, we introduce a co-occurrence representation schema to\nexplicitly incorporate cooccurrence patterns into ID representations.\nSimultaneously, DIMO aligns different modalities into a unified semantic space\nto represent them uniformly. At the session level, we present a multi-view\nself-supervised disentanglement, including proxy mechanism and counterfactual\ninference, to disentangle ID and modality effects without supervised signals.\nLeveraging these disentangled causes, DIMO provides recommendations via causal\ninference and further creates two templates for generating explanations.\nExtensive experiments on multiple real-world datasets demonstrate the\nconsistent superiority of DIMO over existing methods. Further analysis also\nconfirms DIMO's effectiveness in generating explanations.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "This work has been accepted by SIGIR24' as a full paper",
    "pdf_url": "http://arxiv.org/pdf/2404.12969v1",
    "published_date": "2024-04-19 15:54:46 UTC",
    "updated_date": "2024-04-19 15:54:46 UTC"
  },
  {
    "arxiv_id": "2404.12966v5",
    "title": "Look Before You Decide: Prompting Active Deduction of MLLMs for Assumptive Reasoning",
    "authors": [
      "Yian Li",
      "Wentao Tian",
      "Yang Jiao",
      "Jingjing Chen",
      "Tianwen Qian",
      "Bin Zhu",
      "Na Zhao",
      "Yu-Gang Jiang"
    ],
    "abstract": "Recently, Multimodal Large Language Models (MLLMs) have achieved significant\nsuccess across multiple disciplines due to their exceptional\ninstruction-following capabilities and extensive world knowledge. However,\nwhether these MLLMs possess human-like compositional reasoning abilities\nremains an open problem. To unveil their reasoning behaviors, we first curate a\n\\textbf{M}ultimodal \\textbf{A}ssumptive \\textbf{R}ea\\textbf{s}oning Benchmark\n(MARS-Bench) in this paper. Interestingly, we find that most prevalent MLLMs\ncan be easily fooled by the introduction of a presupposition into the question,\nwhereas such presuppositions appear naive to human reasoning. Besides, we also\npropose a simple yet effective method, Active Deduction (AD), a novel\nreinforcement learning paradigm to encourage the model to actively perform\ncomposite deduction before reaching a final decision. Equipped with the\nproposed AD method, a MLLM demonstrates significant improvements in assumptive\nreasoning abilities without compromising its general-purpose question-answering\nperformance. We also provide extensive evaluations of both open-source and\nprivate MLLMs on MARS-Bench, along with experimental analyses of the AD method.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12966v5",
    "published_date": "2024-04-19 15:53:27 UTC",
    "updated_date": "2025-04-17 08:05:10 UTC"
  },
  {
    "arxiv_id": "2404.12938v2",
    "title": "MAiDE-up: Multilingual Deception Detection of GPT-generated Hotel Reviews",
    "authors": [
      "Oana Ignat",
      "Xiaomeng Xu",
      "Rada Mihalcea"
    ],
    "abstract": "Deceptive reviews are becoming increasingly common, especially given the\nincrease in performance and the prevalence of LLMs. While work to date has\naddressed the development of models to differentiate between truthful and\ndeceptive human reviews, much less is known about the distinction between real\nreviews and AI-authored fake reviews. Moreover, most of the research so far has\nfocused primarily on English, with very little work dedicated to other\nlanguages. In this paper, we compile and make publicly available the MAiDE-up\ndataset, consisting of 10,000 real and 10,000 AI-generated fake hotel reviews,\nbalanced across ten languages. Using this dataset, we conduct extensive\nlinguistic analyses to (1) compare the AI fake hotel reviews to real hotel\nreviews, and (2) identify the factors that influence the deception detection\nmodel performance. We explore the effectiveness of several models for deception\ndetection in hotel reviews across three main dimensions: sentiment, location,\nand language. We find that these dimensions influence how well we can detect\nAI-generated fake reviews.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12938v2",
    "published_date": "2024-04-19 15:08:06 UTC",
    "updated_date": "2024-06-19 03:34:42 UTC"
  },
  {
    "arxiv_id": "2404.12933v2",
    "title": "Cross-cultural Inspiration Detection and Analysis in Real and LLM-generated Social Media Data",
    "authors": [
      "Oana Ignat",
      "Gayathri Ganesh Lakshmy",
      "Rada Mihalcea"
    ],
    "abstract": "Inspiration is linked to various positive outcomes, such as increased\ncreativity, productivity, and happiness. Although inspiration has great\npotential, there has been limited effort toward identifying content that is\ninspiring, as opposed to just engaging or positive. Additionally, most research\nhas concentrated on Western data, with little attention paid to other cultures.\nThis work is the first to study cross-cultural inspiration through machine\nlearning methods. We aim to identify and analyze real and AI-generated\ncross-cultural inspiring posts. To this end, we compile and make publicly\navailable the InspAIred dataset, which consists of 2,000 real inspiring posts,\n2,000 real non-inspiring posts, and 2,000 generated inspiring posts evenly\ndistributed across India and the UK. The real posts are sourced from Reddit,\nwhile the generated posts are created using the GPT-4 model. Using this\ndataset, we conduct extensive computational linguistic analyses to (1) compare\ninspiring content across cultures, (2) compare AI-generated inspiring posts to\nreal inspiring posts, and (3) determine if detection models can accurately\ndistinguish between inspiring content across cultures and data sources.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12933v2",
    "published_date": "2024-04-19 15:04:30 UTC",
    "updated_date": "2024-06-19 03:27:43 UTC"
  },
  {
    "arxiv_id": "2404.12928v1",
    "title": "The Positivity of the Neural Tangent Kernel",
    "authors": [
      "Luís Carvalho",
      "João L. Costa",
      "José Mourão",
      "Gonçalo Oliveira"
    ],
    "abstract": "The Neural Tangent Kernel (NTK) has emerged as a fundamental concept in the\nstudy of wide Neural Networks. In particular, it is known that the positivity\nof the NTK is directly related to the memorization capacity of sufficiently\nwide networks, i.e., to the possibility of reaching zero loss in training, via\ngradient descent. Here we will improve on previous works and obtain a sharp\nresult concerning the positivity of the NTK of feedforward networks of any\ndepth. More precisely, we will show that, for any non-polynomial activation\nfunction, the NTK is strictly positive definite. Our results are based on a\nnovel characterization of polynomial functions which is of independent\ninterest.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.PR",
      "math.SP",
      "68T07, 68R01"
    ],
    "primary_category": "cs.LG",
    "comment": "Comments welcome",
    "pdf_url": "http://arxiv.org/pdf/2404.12928v1",
    "published_date": "2024-04-19 14:55:21 UTC",
    "updated_date": "2024-04-19 14:55:21 UTC"
  },
  {
    "arxiv_id": "2404.12926v2",
    "title": "MM-PhyRLHF: Reinforcement Learning Framework for Multimodal Physics Question-Answering",
    "authors": [
      "Janak Kapuriya",
      "Chhavi Kirtani",
      "Apoorv Singh",
      "Jay Saraf",
      "Naman Lal",
      "Jatin Kumar",
      "Adarsh Raj Shivam",
      "Astha Verma",
      "Avinash Anand",
      "Rajiv Ratn Shah"
    ],
    "abstract": "Recent advancements in LLMs have shown their significant potential in tasks\nlike text summarization and generation. Yet, they often encounter difficulty\nwhile solving complex physics problems that require arithmetic calculation and\na good understanding of concepts. Moreover, many physics problems include\nimages that contain important details required to understand the problem's\ncontext. We propose an LMM-based chatbot to answer multimodal physics MCQs. For\ndomain adaptation, we utilize the MM-PhyQA dataset comprising Indian high\nschool-level multimodal physics problems. To improve the LMM's performance, we\nexperiment with two techniques, RLHF (Reinforcement Learning from Human\nFeedback) and Image Captioning. In image captioning, we add a detailed\nexplanation of the diagram in each image, minimizing hallucinations and image\nprocessing errors. We further explore the integration of Reinforcement Learning\nfrom Human Feedback (RLHF) methodology inspired by the ranking approach in RLHF\nto enhance the human-like problem-solving abilities of the models. The RLHF\napproach incorporates human feedback into the learning process of LLMs,\nimproving the model's problem-solving skills, truthfulness, and reasoning\ncapabilities, minimizing the hallucinations in the answers, and improving the\nquality instead of using vanilla-supervised fine-tuned models. We employ the\nLLaVA open-source model to answer multimodal physics MCQs and compare the\nperformance with and without using RLHF.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12926v2",
    "published_date": "2024-04-19 14:52:57 UTC",
    "updated_date": "2025-01-11 09:40:58 UTC"
  },
  {
    "arxiv_id": "2404.12922v1",
    "title": "Is Retain Set All You Need in Machine Unlearning? Restoring Performance of Unlearned Models with Out-Of-Distribution Images",
    "authors": [
      "Jacopo Bonato",
      "Marco Cotogni",
      "Luigi Sabetta"
    ],
    "abstract": "In this paper, we introduce Selective-distillation for Class and\nArchitecture-agnostic unleaRning (SCAR), a novel approximate unlearning method.\nSCAR efficiently eliminates specific information while preserving the model's\ntest accuracy without using a retain set, which is a key component in\nstate-of-the-art approximate unlearning algorithms. Our approach utilizes a\nmodified Mahalanobis distance to guide the unlearning of the feature vectors of\nthe instances to be forgotten, aligning them to the nearest wrong class\ndistribution. Moreover, we propose a distillation-trick mechanism that distills\nthe knowledge of the original model into the unlearning model with\nout-of-distribution images for retaining the original model's test performance\nwithout using any retain set. Importantly, we propose a self-forget version of\nSCAR that unlearns without having access to the forget set. We experimentally\nverified the effectiveness of our method, on three public datasets, comparing\nit with state-of-the-art methods. Our method obtains performance higher than\nmethods that operate without the retain set and comparable w.r.t the best\nmethods that rely on the retain set.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12922v1",
    "published_date": "2024-04-19 14:45:27 UTC",
    "updated_date": "2024-04-19 14:45:27 UTC"
  },
  {
    "arxiv_id": "2404.12917v3",
    "title": "R3L: Relative Representations for Reinforcement Learning",
    "authors": [
      "Antonio Pio Ricciardi",
      "Valentino Maiorca",
      "Luca Moschella",
      "Riccardo Marin",
      "Emanuele Rodolà"
    ],
    "abstract": "Visual Reinforcement Learning is a popular and powerful framework that takes\nfull advantage of the Deep Learning breakthrough. It is known that variations\nin input domains (e.g., different panorama colors due to seasonal changes) or\ntask domains (e.g., altering the target speed of a car) can disrupt agent\nperformance, necessitating new training for each variation. Recent advancements\nin the field of representation learning have demonstrated the possibility of\ncombining components from different neural networks to create new models in a\nzero-shot fashion. In this paper, we build upon relative representations, a\nframework that maps encoder embeddings to a universal space. We adapt this\nframework to the Visual Reinforcement Learning setting, allowing to combine\nagents components to create new agents capable of effectively handling novel\nvisual-task pairs not encountered during training. Our findings highlight the\npotential for model reuse, significantly reducing the need for retraining and,\nconsequently, the time and computational resources required.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "68T07",
      "I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 5 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.12917v3",
    "published_date": "2024-04-19 14:42:42 UTC",
    "updated_date": "2025-02-18 15:17:38 UTC"
  },
  {
    "arxiv_id": "2404.12901v2",
    "title": "Large Language Models for Networking: Workflow, Advances and Challenges",
    "authors": [
      "Chang Liu",
      "Xiaohui Xie",
      "Xinggong Zhang",
      "Yong Cui"
    ],
    "abstract": "The networking field is characterized by its high complexity and rapid\niteration, requiring extensive expertise to accomplish network tasks, ranging\nfrom network design, configuration, diagnosis and security. The inherent\ncomplexity of these tasks, coupled with the ever-changing landscape of\nnetworking technologies and protocols, poses significant hurdles for\ntraditional machine learning-based methods. These methods often struggle to\ngeneralize and automate complex tasks in networking, as they require extensive\nlabeled data, domain-specific feature engineering, and frequent retraining to\nadapt to new scenarios. However, the recent emergence of large language models\n(LLMs) has sparked a new wave of possibilities in addressing these challenges.\nLLMs have demonstrated remarkable capabilities in natural language\nunderstanding, generation, and reasoning. These models, trained on extensive\ndata, can benefit the networking domain. Some efforts have already explored the\napplication of LLMs in the networking domain and revealed promising results. By\nreviewing recent advances, we present an abstract workflow to describe the\nfundamental process involved in applying LLM for Networking. We introduce the\nhighlights of existing works by category and explain in detail how they operate\nat different stages of the workflow. Furthermore, we delve into the challenges\nencountered, discuss potential solutions, and outline future research\nprospects. We hope that this survey will provide insight for researchers and\npractitioners, promoting the development of this interdisciplinary research\nfield.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12901v2",
    "published_date": "2024-04-19 14:17:02 UTC",
    "updated_date": "2024-04-29 04:46:13 UTC"
  },
  {
    "arxiv_id": "2404.12900v2",
    "title": "Training-and-Prompt-Free General Painterly Harmonization via Zero-Shot Disentenglement on Style and Content References",
    "authors": [
      "Teng-Fang Hsiao",
      "Bo-Kai Ruan",
      "Hong-Han Shuai"
    ],
    "abstract": "Painterly image harmonization aims at seamlessly blending disparate visual\nelements within a single image. However, previous approaches often struggle due\nto limitations in training data or reliance on additional prompts, leading to\ninharmonious and content-disrupted output. To surmount these hurdles, we design\na Training-and-prompt-Free General Painterly Harmonization method (TF-GPH).\nTF-GPH incorporates a novel ``Similarity Disentangle Mask'', which disentangles\nthe foreground content and background image by redirecting their attention to\ncorresponding reference images, enhancing the attention mechanism for\nmulti-image inputs. Additionally, we propose a ``Similarity Reweighting''\nmechanism to balance harmonization between stylization and content\npreservation. This mechanism minimizes content disruption by prioritizing the\ncontent-similar features within the given background style reference. Finally,\nwe address the deficiencies in existing benchmarks by proposing novel\nrange-based evaluation metrics and a new benchmark to better reflect real-world\napplications. Extensive experiments demonstrate the efficacy of our method in\nall benchmarks. More detailed in https://github.com/BlueDyee/TF-GPH.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12900v2",
    "published_date": "2024-04-19 14:13:46 UTC",
    "updated_date": "2024-12-15 14:53:00 UTC"
  },
  {
    "arxiv_id": "2404.17591v2",
    "title": "Large Language Models for Next Point-of-Interest Recommendation",
    "authors": [
      "Peibo Li",
      "Maarten de Rijke",
      "Hao Xue",
      "Shuang Ao",
      "Yang Song",
      "Flora D. Salim"
    ],
    "abstract": "The next Point of Interest (POI) recommendation task is to predict users'\nimmediate next POI visit given their historical data. Location-Based Social\nNetwork (LBSN) data, which is often used for the next POI recommendation task,\ncomes with challenges. One frequently disregarded challenge is how to\neffectively use the abundant contextual information present in LBSN data.\nPrevious methods are limited by their numerical nature and fail to address this\nchallenge. In this paper, we propose a framework that uses pretrained Large\nLanguage Models (LLMs) to tackle this challenge. Our framework allows us to\npreserve heterogeneous LBSN data in its original format, hence avoiding the\nloss of contextual information. Furthermore, our framework is capable of\ncomprehending the inherent meaning of contextual information due to the\ninclusion of commonsense knowledge. In experiments, we test our framework on\nthree real-world LBSN datasets. Our results show that the proposed framework\noutperforms the state-of-the-art models in all three datasets. Our analysis\ndemonstrates the effectiveness of the proposed framework in using contextual\ninformation as well as alleviating the commonly encountered cold-start and\nshort trajectory problems.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17591v2",
    "published_date": "2024-04-19 13:28:36 UTC",
    "updated_date": "2024-08-01 08:54:15 UTC"
  },
  {
    "arxiv_id": "2404.12876v1",
    "title": "A Large-scale Medical Visual Task Adaptation Benchmark",
    "authors": [
      "Shentong Mo",
      "Xufang Luo",
      "Yansen Wang",
      "Dongsheng Li"
    ],
    "abstract": "Visual task adaptation has been demonstrated to be effective in adapting\npre-trained Vision Transformers (ViTs) to general downstream visual tasks using\nspecialized learnable layers or tokens. However, there is yet a large-scale\nbenchmark to fully explore the effect of visual task adaptation on the\nrealistic and important medical domain, particularly across diverse medical\nvisual modalities, such as color images, X-ray, and CT. To close this gap, we\npresent Med-VTAB, a large-scale Medical Visual Task Adaptation Benchmark\nconsisting of 1.68 million medical images for diverse organs, modalities, and\nadaptation approaches. Based on Med-VTAB, we explore the scaling law of medical\nprompt tuning concerning tunable parameters and the generalizability of medical\nvisual adaptation using non-medical/medical pre-train weights. Besides, we\nstudy the impact of patient ID out-of-distribution on medical visual\nadaptation, which is a real and challenging scenario. Furthermore, results from\nMed-VTAB indicate that a single pre-trained model falls short in medical task\nadaptation. Therefore, we introduce GMoE-Adapter, a novel method that combines\nmedical and general pre-training weights through a gated mixture-of-experts\nadapter, achieving state-of-the-art results in medical visual task adaptation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12876v1",
    "published_date": "2024-04-19 13:25:27 UTC",
    "updated_date": "2024-04-19 13:25:27 UTC"
  },
  {
    "arxiv_id": "2404.12866v2",
    "title": "How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?",
    "authors": [
      "Yang Luo",
      "Zangwei Zheng",
      "Zirui Zhu",
      "Yang You"
    ],
    "abstract": "The increase in parameter size of multimodal large language models (MLLMs)\nintroduces significant capabilities, particularly in-context learning, where\nMLLMs enhance task performance without updating pre-trained parameters. This\neffectiveness, however, hinges on the appropriate selection of in-context\nexamples, a process that is currently biased towards visual data, overlooking\ntextual information. Furthermore, the area of supervised retrievers for MLLMs,\ncrucial for optimal in-context example selection, continues to be\nuninvestigated. Our study offers an in-depth evaluation of the impact of\ntextual information on the unsupervised selection of in-context examples in\nmultimodal contexts, uncovering a notable sensitivity of retriever performance\nto the employed modalities. Responding to this, we introduce a novel supervised\nMLLM-retriever MSIER that employs a neural network to select examples that\nenhance multimodal in-context learning efficiency. This approach is validated\nthrough extensive testing across three distinct tasks, demonstrating the\nmethod's effectiveness. Additionally, we investigate the influence of\nmodalities on our supervised retrieval method's training and pinpoint factors\ncontributing to our model's success. This exploration paves the way for future\nadvancements, highlighting the potential for refined in-context learning in\nMLLMs through the strategic use of multimodal data.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12866v2",
    "published_date": "2024-04-19 13:05:37 UTC",
    "updated_date": "2024-11-12 08:59:30 UTC"
  },
  {
    "arxiv_id": "2404.12856v2",
    "title": "Language-Driven Active Learning for Diverse Open-Set 3D Object Detection",
    "authors": [
      "Ross Greer",
      "Bjørk Antoniussen",
      "Andreas Møgelmose",
      "Mohan Trivedi"
    ],
    "abstract": "Object detection is crucial for ensuring safe autonomous driving. However,\ndata-driven approaches face challenges when encountering minority or novel\nobjects in the 3D driving scene. In this paper, we propose VisLED, a\nlanguage-driven active learning framework for diverse open-set 3D Object\nDetection. Our method leverages active learning techniques to query diverse and\ninformative data samples from an unlabeled pool, enhancing the model's ability\nto detect underrepresented or novel objects. Specifically, we introduce the\nVision-Language Embedding Diversity Querying (VisLED-Querying) algorithm, which\noperates in both open-world exploring and closed-world mining settings. In\nopen-world exploring, VisLED-Querying selects data points most novel relative\nto existing data, while in closed-world mining, it mines novel instances of\nknown classes. We evaluate our approach on the nuScenes dataset and demonstrate\nits efficiency compared to random sampling and entropy-querying methods. Our\nresults show that VisLED-Querying consistently outperforms random sampling and\noffers competitive performance compared to entropy-querying despite the\nlatter's model-optimality, highlighting the potential of VisLED for improving\nobject detection in autonomous driving scenarios. We make our code publicly\navailable at https://github.com/Bjork-crypto/VisLED-Querying",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12856v2",
    "published_date": "2024-04-19 12:50:43 UTC",
    "updated_date": "2024-06-18 07:34:33 UTC"
  },
  {
    "arxiv_id": "2404.13104v1",
    "title": "Multi Class Depression Detection Through Tweets using Artificial Intelligence",
    "authors": [
      "Muhammad Osama Nusrat",
      "Waseem Shahzad",
      "Saad Ahmed Jamal"
    ],
    "abstract": "Depression is a significant issue nowadays. As per the World Health\nOrganization (WHO), in 2023, over 280 million individuals are grappling with\ndepression. This is a huge number; if not taken seriously, these numbers will\nincrease rapidly. About 4.89 billion individuals are social media users. People\nexpress their feelings and emotions on platforms like Twitter, Facebook,\nReddit, Instagram, etc. These platforms contain valuable information which can\nbe used for research purposes. Considerable research has been conducted across\nvarious social media platforms. However, certain limitations persist in these\nendeavors. Particularly, previous studies were only focused on detecting\ndepression and the intensity of depression in tweets. Also, there existed\ninaccuracies in dataset labeling. In this research work, five types of\ndepression (Bipolar, major, psychotic, atypical, and postpartum) were predicted\nusing tweets from the Twitter database based on lexicon labeling. Explainable\nAI was used to provide reasoning by highlighting the parts of tweets that\nrepresent type of depression. Bidirectional Encoder Representations from\nTransformers (BERT) was used for feature extraction and training. Machine\nlearning and deep learning methodologies were used to train the model. The BERT\nmodel presented the most promising results, achieving an overall accuracy of\n0.96.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "33 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.13104v1",
    "published_date": "2024-04-19 12:47:56 UTC",
    "updated_date": "2024-04-19 12:47:56 UTC"
  },
  {
    "arxiv_id": "2404.12839v1",
    "title": "ECOR: Explainable CLIP for Object Recognition",
    "authors": [
      "Ali Rasekh",
      "Sepehr Kazemi Ranjbar",
      "Milad Heidari",
      "Wolfgang Nejdl"
    ],
    "abstract": "Large Vision Language Models (VLMs), such as CLIP, have significantly\ncontributed to various computer vision tasks, including object recognition and\nobject detection. Their open vocabulary feature enhances their value. However,\ntheir black-box nature and lack of explainability in predictions make them less\ntrustworthy in critical domains. Recently, some work has been done to force\nVLMs to provide reasonable rationales for object recognition, but this often\ncomes at the expense of classification accuracy. In this paper, we first\npropose a mathematical definition of explainability in the object recognition\ntask based on the joint probability distribution of categories and rationales,\nthen leverage this definition to fine-tune CLIP in an explainable manner.\nThrough evaluations of different datasets, our method demonstrates\nstate-of-the-art performance in explainable classification. Notably, it excels\nin zero-shot settings, showcasing its adaptability. This advancement improves\nexplainable object recognition, enhancing trust across diverse applications.\nThe code will be made available online upon publication.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12839v1",
    "published_date": "2024-04-19 12:20:49 UTC",
    "updated_date": "2024-04-19 12:20:49 UTC"
  },
  {
    "arxiv_id": "2404.12832v2",
    "title": "COIN: Counterfactual inpainting for weakly supervised semantic segmentation for medical images",
    "authors": [
      "Dmytro Shvetsov",
      "Joonas Ariva",
      "Marharyta Domnich",
      "Raul Vicente",
      "Dmytro Fishman"
    ],
    "abstract": "Deep learning is dramatically transforming the field of medical imaging and\nradiology, enabling the identification of pathologies in medical images,\nincluding computed tomography (CT) and X-ray scans. However, the performance of\ndeep learning models, particularly in segmentation tasks, is often limited by\nthe need for extensive annotated datasets. To address this challenge, the\ncapabilities of weakly supervised semantic segmentation are explored through\nthe lens of Explainable AI and the generation of counterfactual explanations.\nThe scope of this research is development of a novel counterfactual inpainting\napproach (COIN) that flips the predicted classification label from abnormal to\nnormal by using a generative model. For instance, if the classifier deems an\ninput medical image X as abnormal, indicating the presence of a pathology, the\ngenerative model aims to inpaint the abnormal region, thus reversing the\nclassifier's original prediction label. The approach enables us to produce\nprecise segmentations for pathologies without depending on pre-existing\nsegmentation masks. Crucially, image-level labels are utilized, which are\nsubstantially easier to acquire than creating detailed segmentation masks. The\neffectiveness of the method is demonstrated by segmenting synthetic targets and\nactual kidney tumors from CT images acquired from Tartu University Hospital in\nEstonia. The findings indicate that COIN greatly surpasses established\nattribution methods, such as RISE, ScoreCAM, and LayerCAM, as well as an\nalternative counterfactual explanation method introduced by Singla et al. This\nevidence suggests that COIN is a promising approach for semantic segmentation\nof tumors in CT images, and presents a step forward in making deep learning\napplications more accessible and effective in healthcare, where annotated data\nis scarce.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "This work has been accepted to be presented to The 2nd World\n  Conference on eXplainable Artificial Intelligence (xAI 2024), July 17-19,\n  2024 - Valletta, Malta",
    "pdf_url": "http://arxiv.org/pdf/2404.12832v2",
    "published_date": "2024-04-19 12:09:49 UTC",
    "updated_date": "2024-07-25 08:09:12 UTC"
  },
  {
    "arxiv_id": "2404.12814v3",
    "title": "Generative Modelling with High-Order Langevin Dynamics",
    "authors": [
      "Ziqiang Shi",
      "Rujie Liu"
    ],
    "abstract": "Diffusion generative modelling (DGM) based on stochastic differential\nequations (SDEs) with score matching has achieved unprecedented results in data\ngeneration. In this paper, we propose a novel fast high-quality generative\nmodelling method based on high-order Langevin dynamics (HOLD) with score\nmatching. This motive is proved by third-order Langevin dynamics. By augmenting\nthe previous SDEs, e.g. variance exploding or variance preserving SDEs for\nsingle-data variable processes, HOLD can simultaneously model position,\nvelocity, and acceleration, thereby improving the quality and speed of the data\ngeneration at the same time. HOLD is composed of one Ornstein-Uhlenbeck process\nand two Hamiltonians, which reduce the mixing time by two orders of magnitude.\nEmpirical experiments for unconditional image generation on the public data set\nCIFAR-10 and CelebA-HQ show that the effect is significant in both Frechet\ninception distance (FID) and negative log-likelihood, and achieves the\nstate-of-the-art FID of 1.85 on CIFAR-10.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Some of the results in this paper have been published at conferences,\n  such as WACV2024, ICASSP2024, and ICME2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12814v3",
    "published_date": "2024-04-19 11:49:01 UTC",
    "updated_date": "2025-01-02 13:06:57 UTC"
  },
  {
    "arxiv_id": "2404.12810v2",
    "title": "Enhancing Counterfactual Explanation Search with Diffusion Distance and Directional Coherence",
    "authors": [
      "Marharyta Domnich",
      "Raul Vicente"
    ],
    "abstract": "A pressing issue in the adoption of AI models is the increasing demand for\nmore human-centric explanations of their predictions. To advance towards more\nhuman-centric explanations, understanding how humans produce and select\nexplanations has been beneficial. In this work, inspired by insights of human\ncognition we propose and test the incorporation of two novel biases to enhance\nthe search for effective counterfactual explanations. Central to our\nmethodology is the application of diffusion distance, which emphasizes data\nconnectivity and actionability in the search for feasible counterfactual\nexplanations. In particular, diffusion distance effectively weights more those\npoints that are more interconnected by numerous short-length paths. This\napproach brings closely connected points nearer to each other, identifying a\nfeasible path between them. We also introduce a directional coherence term that\nallows the expression of a preference for the alignment between the joint and\nmarginal directional changes in feature space to reach a counterfactual. This\nterm enables the generation of counterfactual explanations that align with a\nset of marginal predictions based on expectations of how the outcome of the\nmodel varies by changing one feature at a time. We evaluate our method, named\nCoherent Directional Counterfactual Explainer (CoDiCE), and the impact of the\ntwo novel biases against existing methods such as DiCE, FACE, Prototypes, and\nGrowing Spheres. Through a series of ablation experiments on both synthetic and\nreal datasets with continuous and mixed-type features, we demonstrate the\neffectiveness of our method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This work has been accepted to be presented to The 2nd World\n  Conference on eXplainable Artificial Intelligence (xAI 2024), July 17-19,\n  2024 - Valletta, Malta",
    "pdf_url": "http://arxiv.org/pdf/2404.12810v2",
    "published_date": "2024-04-19 11:47:17 UTC",
    "updated_date": "2024-07-25 08:00:44 UTC"
  },
  {
    "arxiv_id": "2404.12802v1",
    "title": "Enhancing Interval Type-2 Fuzzy Logic Systems: Learning for Precision and Prediction Intervals",
    "authors": [
      "Ata Koklu",
      "Yusuf Guven",
      "Tufan Kumbasar"
    ],
    "abstract": "In this paper, we tackle the task of generating Prediction Intervals (PIs) in\nhigh-risk scenarios by proposing enhancements for learning Interval Type-2\n(IT2) Fuzzy Logic Systems (FLSs) to address their learning challenges. In this\ncontext, we first provide extra design flexibility to the Karnik-Mendel (KM)\nand Nie-Tan (NT) center of sets calculation methods to increase their\nflexibility for generating PIs. These enhancements increase the flexibility of\nKM in the defuzzification stage while the NT in the fuzzification stage. To\naddress the large-scale learning challenge, we transform the IT2-FLS's\nconstraint learning problem into an unconstrained form via parameterization\ntricks, enabling the direct application of deep learning optimizers. To address\nthe curse of dimensionality issue, we expand the High-Dimensional\nTakagi-Sugeno-Kang (HTSK) method proposed for type-1 FLS to IT2-FLSs, resulting\nin the HTSK2 approach. Additionally, we introduce a framework to learn the\nenhanced IT2-FLS with a dual focus, aiming for high precision and PI\ngeneration. Through exhaustive statistical results, we reveal that HTSK2\neffectively addresses the dimensionality challenge, while the enhanced KM and\nNT methods improved learning and enhanced uncertainty quantification\nperformances of IT2-FLSs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "in the IEEE World Congress on Computational Intelligence, 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12802v1",
    "published_date": "2024-04-19 11:37:51 UTC",
    "updated_date": "2024-04-19 11:37:51 UTC"
  },
  {
    "arxiv_id": "2404.12800v1",
    "title": "Zadeh's Type-2 Fuzzy Logic Systems: Precision and High-Quality Prediction Intervals",
    "authors": [
      "Yusuf Guven",
      "Ata Koklu",
      "Tufan Kumbasar"
    ],
    "abstract": "General Type-2 (GT2) Fuzzy Logic Systems (FLSs) are perfect candidates to\nquantify uncertainty, which is crucial for informed decisions in high-risk\ntasks, as they are powerful tools in representing uncertainty. In this paper,\nwe travel back in time to provide a new look at GT2-FLSs by adopting Zadeh's\n(Z) GT2 Fuzzy Set (FS) definition, intending to learn GT2-FLSs that are capable\nof achieving reliable High-Quality Prediction Intervals (HQ-PI) alongside\nprecision. By integrating Z-GT2-FS with the \\(\\alpha\\)-plane representation, we\nshow that the design flexibility of GT2-FLS is increased as it takes away the\ndependency of the secondary membership function from the primary membership\nfunction. After detailing the construction of Z-GT2-FLSs, we provide solutions\nto challenges while learning from high-dimensional data: the curse of\ndimensionality, and integrating Deep Learning (DL) optimizers. We develop a DL\nframework for learning dual-focused Z-GT2-FLSs with high performances. Our\nstudy includes statistical analyses, highlighting that the Z-GT2-FLS not only\nexhibits high-precision performance but also produces HQ-PIs in comparison to\nits GT2 and IT2 fuzzy counterparts which have more learnable parameters. The\nresults show that the Z-GT2-FLS has a huge potential in uncertainty\nquantification.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "in the IEEE World Congress on Computational Intelligence, 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12800v1",
    "published_date": "2024-04-19 11:29:10 UTC",
    "updated_date": "2024-04-19 11:29:10 UTC"
  },
  {
    "arxiv_id": "2404.12792v1",
    "title": "Efficient Learning of Fuzzy Logic Systems for Large-Scale Data Using Deep Learning",
    "authors": [
      "Ata Koklu",
      "Yusuf Guven",
      "Tufan Kumbasar"
    ],
    "abstract": "Type-1 and Interval Type-2 (IT2) Fuzzy Logic Systems (FLS) excel in handling\nuncertainty alongside their parsimonious rule-based structure. Yet, in learning\nlarge-scale data challenges arise, such as the curse of dimensionality and\ntraining complexity of FLSs. The complexity is due mainly to the constraints to\nbe satisfied as the learnable parameters define FSs and the complexity of the\ncenter of the sets calculation method, especially of IT2-FLSs. This paper\nexplicitly focuses on the learning problem of FLSs and presents a\ncomputationally efficient learning method embedded within the realm of Deep\nLearning (DL). The proposed method tackles the learning challenges of FLSs by\npresenting computationally efficient implementations of FLSs, thereby\nminimizing training time while leveraging mini-batched DL optimizers and\nautomatic differentiation provided within the DL frameworks. We illustrate the\nefficiency of the DL framework for FLSs on benchmark datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "in the International Conference on Intelligent and Fuzzy Systems,\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12792v1",
    "published_date": "2024-04-19 11:09:55 UTC",
    "updated_date": "2024-04-19 11:09:55 UTC"
  },
  {
    "arxiv_id": "2404.12782v1",
    "title": "Sentiment-oriented Transformer-based Variational Autoencoder Network for Live Video Commenting",
    "authors": [
      "Fengyi Fu",
      "Shancheng Fang",
      "Weidong Chen",
      "Zhendong Mao"
    ],
    "abstract": "Automatic live video commenting is with increasing attention due to its\nsignificance in narration generation, topic explanation, etc. However, the\ndiverse sentiment consideration of the generated comments is missing from the\ncurrent methods. Sentimental factors are critical in interactive commenting,\nand lack of research so far. Thus, in this paper, we propose a\nSentiment-oriented Transformer-based Variational Autoencoder (So-TVAE) network\nwhich consists of a sentiment-oriented diversity encoder module and a batch\nattention module, to achieve diverse video commenting with multiple sentiments\nand multiple semantics. Specifically, our sentiment-oriented diversity encoder\nelegantly combines VAE and random mask mechanism to achieve semantic diversity\nunder sentiment guidance, which is then fused with cross-modal features to\ngenerate live video comments. Furthermore, a batch attention module is also\nproposed in this paper to alleviate the problem of missing sentimental samples,\ncaused by the data imbalance, which is common in live videos as the popularity\nof videos varies. Extensive experiments on Livebot and VideoIC datasets\ndemonstrate that the proposed So-TVAE outperforms the state-of-the-art methods\nin terms of the quality and diversity of generated comments. Related code is\navailable at https://github.com/fufy1024/So-TVAE.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "27 pages, 10 figures, ACM Transactions on Multimedia Computing,\n  Communications and Applications, 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12782v1",
    "published_date": "2024-04-19 10:43:25 UTC",
    "updated_date": "2024-04-19 10:43:25 UTC"
  },
  {
    "arxiv_id": "2404.12768v1",
    "title": "MixLight: Borrowing the Best of both Spherical Harmonics and Gaussian Models",
    "authors": [
      "Xinlong Ji",
      "Fangneng Zhan",
      "Shijian Lu",
      "Shi-Sheng Huang",
      "Hua Huang"
    ],
    "abstract": "Accurately estimating scene lighting is critical for applications such as\nmixed reality. Existing works estimate illumination by generating illumination\nmaps or regressing illumination parameters. However, the method of generating\nillumination maps has poor generalization performance and parametric models\nsuch as Spherical Harmonic (SH) and Spherical Gaussian (SG) fall short in\ncapturing high-frequency or low-frequency components. This paper presents\nMixLight, a joint model that utilizes the complementary characteristics of SH\nand SG to achieve a more complete illumination representation, which uses SH\nand SG to capture low-frequency ambient and high-frequency light sources\nrespectively. In addition, a special spherical light source sparsemax\n(SLSparsemax) module that refers to the position and brightness relationship\nbetween spherical light sources is designed to improve their sparsity, which is\nsignificant but omitted by prior works. Extensive experiments demonstrate that\nMixLight surpasses state-of-the-art (SOTA) methods on multiple metrics. In\naddition, experiments on Web Dataset also show that MixLight as a parametric\nmethod has better generalization performance than non-parametric methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12768v1",
    "published_date": "2024-04-19 10:17:10 UTC",
    "updated_date": "2024-04-19 10:17:10 UTC"
  },
  {
    "arxiv_id": "2404.12762v2",
    "title": "How should AI decisions be explained? Requirements for Explanations from the Perspective of European Law",
    "authors": [
      "Benjamin Fresz",
      "Elena Dubovitskaya",
      "Danilo Brajovic",
      "Marco Huber",
      "Christian Horz"
    ],
    "abstract": "This paper investigates the relationship between law and eXplainable\nArtificial Intelligence (XAI). While there is much discussion about the AI Act,\nfor which the trilogue of the European Parliament, Council and Commission\nrecently concluded, other areas of law seem underexplored. This paper focuses\non European (and in part German) law, although with international concepts and\nregulations such as fiduciary plausibility checks, the General Data Protection\nRegulation (GDPR), and product safety and liability. Based on XAI-taxonomies,\nrequirements for XAI-methods are derived from each of the legal bases,\nresulting in the conclusion that each legal basis requires different XAI\nproperties and that the current state of the art does not fulfill these to full\nsatisfaction, especially regarding the correctness (sometimes called fidelity)\nand confidence estimates of XAI-methods.\n  Published in the Proceedings of the AAAI/ACM Conference on AI, Ethics, and\nSociety https://doi.org/10.1609/aies.v7i1.31648 .",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12762v2",
    "published_date": "2024-04-19 10:08:28 UTC",
    "updated_date": "2024-11-26 14:13:46 UTC"
  },
  {
    "arxiv_id": "2404.12760v1",
    "title": "Food Development through Co-creation with AI: bread with a \"taste of love\"",
    "authors": [
      "Takuya Sera",
      "Izumi Kuwata",
      "Yuki Taya",
      "Noritaka Shimura",
      "Yosuke Motohashi"
    ],
    "abstract": "This study explores a new method in food development by utilizing AI\nincluding generative AI, aiming to craft products that delight the senses and\nresonate with consumers' emotions. The food ingredient recommendation approach\nused in this study can be considered as a form of multimodal generation in a\nbroad sense, as it takes text as input and outputs food ingredient candidates.\nThis Study focused on producing \"Romance Bread,\" a collection of breads infused\nwith flavors that reflect the nuances of a romantic Japanese television\nprogram. We analyzed conversations from TV programs and lyrics from songs\nfeaturing fruits and sweets to recommend ingredients that express romantic\nfeelings. Based on these recommendations, the bread developers then considered\nthe flavoring of the bread and developed new bread varieties. The research\nincluded a tasting evaluation involving 31 participants and interviews with the\nproduct developers. Findings indicate a notable correlation between tastes\ngenerated by AI and human preferences. This study validates the concept of\nusing AI in food innovation and highlights the broad potential for developing\nunique consumer experiences that focus on emotional engagement through AI and\nhuman collaboration.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to GenAICHI: CHI 2024 Workshop on Generative AI and HCI",
    "pdf_url": "http://arxiv.org/pdf/2404.12760v1",
    "published_date": "2024-04-19 10:03:59 UTC",
    "updated_date": "2024-04-19 10:03:59 UTC"
  },
  {
    "arxiv_id": "2404.12754v1",
    "title": "Adaptive Regularization of Representation Rank as an Implicit Constraint of Bellman Equation",
    "authors": [
      "Qiang He",
      "Tianyi Zhou",
      "Meng Fang",
      "Setareh Maghsudi"
    ],
    "abstract": "Representation rank is an important concept for understanding the role of\nNeural Networks (NNs) in Deep Reinforcement learning (DRL), which measures the\nexpressive capacity of value networks. Existing studies focus on unboundedly\nmaximizing this rank; nevertheless, that approach would introduce overly\ncomplex models in the learning, thus undermining performance. Hence,\nfine-tuning representation rank presents a challenging and crucial optimization\nproblem. To address this issue, we find a guiding principle for adaptive\ncontrol of the representation rank. We employ the Bellman equation as a\ntheoretical foundation and derive an upper bound on the cosine similarity of\nconsecutive state-action pairs representations of value networks. We then\nleverage this upper bound to propose a novel regularizer, namely BEllman\nEquation-based automatic rank Regularizer (BEER). This regularizer adaptively\nregularizes the representation rank, thus improving the DRL agent's\nperformance. We first validate the effectiveness of automatic control of rank\non illustrative experiments. Then, we scale up BEER to complex continuous\ncontrol tasks by combining it with the deterministic policy gradient method.\nAmong 12 challenging DeepMind control tasks, BEER outperforms the baselines by\na large margin. Besides, BEER demonstrates significant advantages in Q-value\napproximation. Our code is available at\nhttps://github.com/sweetice/BEER-ICLR2024.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to CVPR23; Code: https://github.com/sweetice/BEER-ICLR2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12754v1",
    "published_date": "2024-04-19 10:00:34 UTC",
    "updated_date": "2024-04-19 10:00:34 UTC"
  },
  {
    "arxiv_id": "2404.12753v2",
    "title": "AutoScraper: A Progressive Understanding Web Agent for Web Scraper Generation",
    "authors": [
      "Wenhao Huang",
      "Zhouhong Gu",
      "Chenghao Peng",
      "Zhixu Li",
      "Jiaqing Liang",
      "Yanghua Xiao",
      "Liqian Wen",
      "Zulong Chen"
    ],
    "abstract": "Web scraping is a powerful technique that extracts data from websites,\nenabling automated data collection, enhancing data analysis capabilities, and\nminimizing manual data entry efforts. Existing methods, wrappers-based methods\nsuffer from limited adaptability and scalability when faced with a new website,\nwhile language agents, empowered by large language models (LLMs), exhibit poor\nreusability in diverse web environments. In this work, we introduce the\nparadigm of generating web scrapers with LLMs and propose AutoScraper, a\ntwo-stage framework that can handle diverse and changing web environments more\nefficiently. AutoScraper leverages the hierarchical structure of HTML and\nsimilarity across different web pages for generating web scrapers. Besides, we\npropose a new executability metric for better measuring the performance of web\nscraper generation tasks. We conduct comprehensive experiments with multiple\nLLMs and demonstrate the effectiveness of our framework. Resources of this\npaper can be found at \\url{https://github.com/EZ-hwh/AutoScraper}",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 4 figures, 18 tables. Accepted to EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12753v2",
    "published_date": "2024-04-19 09:59:44 UTC",
    "updated_date": "2024-09-26 09:17:10 UTC"
  },
  {
    "arxiv_id": "2404.13101v1",
    "title": "DensePANet: An improved generative adversarial network for photoacoustic tomography image reconstruction from sparse data",
    "authors": [
      "Hesam Hakimnejad",
      "Zohreh Azimifar",
      "Narjes Goshtasbi"
    ],
    "abstract": "Image reconstruction is an essential step of every medical imaging method,\nincluding Photoacoustic Tomography (PAT), which is a promising modality of\nimaging, that unites the benefits of both ultrasound and optical imaging\nmethods. Reconstruction of PAT images using conventional methods results in\nrough artifacts, especially when applied directly to sparse PAT data. In recent\nyears, generative adversarial networks (GANs) have shown a powerful performance\nin image generation as well as translation, rendering them a smart choice to be\napplied to reconstruction tasks. In this study, we proposed an end-to-end\nmethod called DensePANet to solve the problem of PAT image reconstruction from\nsparse data. The proposed model employs a novel modification of UNet in its\ngenerator, called FD-UNet++, which considerably improves the reconstruction\nperformance. We evaluated the method on various in-vivo and simulated datasets.\nQuantitative and qualitative results show the better performance of our model\nover other prevalent deep learning techniques.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13101v1",
    "published_date": "2024-04-19 09:52:32 UTC",
    "updated_date": "2024-04-19 09:52:32 UTC"
  },
  {
    "arxiv_id": "2404.12744v2",
    "title": "Beyond Human Norms: Unveiling Unique Values of Large Language Models through Interdisciplinary Approaches",
    "authors": [
      "Pablo Biedma",
      "Xiaoyuan Yi",
      "Linus Huang",
      "Maosong Sun",
      "Xing Xie"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have revolutionized the\nAI field but also pose potential safety and ethical risks. Deciphering LLMs'\nembedded values becomes crucial for assessing and mitigating their risks.\nDespite extensive investigation into LLMs' values, previous studies heavily\nrely on human-oriented value systems in social sciences. Then, a natural\nquestion arises: Do LLMs possess unique values beyond those of humans? Delving\ninto it, this work proposes a novel framework, ValueLex, to reconstruct LLMs'\nunique value system from scratch, leveraging psychological methodologies from\nhuman personality/value research. Based on Lexical Hypothesis, ValueLex\nintroduces a generative approach to elicit diverse values from 30+ LLMs,\nsynthesizing a taxonomy that culminates in a comprehensive value framework via\nfactor analysis and semantic clustering. We identify three core value\ndimensions, Competence, Character, and Integrity, each with specific\nsubdimensions, revealing that LLMs possess a structured, albeit non-human,\nvalue system. Based on this system, we further develop tailored projective\ntests to evaluate and analyze the value inclinations of LLMs across different\nmodel sizes, training methods, and data sources. Our framework fosters an\ninterdisciplinary paradigm of understanding LLMs, paving the way for future AI\nalignment and regulation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, work in progress",
    "pdf_url": "http://arxiv.org/pdf/2404.12744v2",
    "published_date": "2024-04-19 09:44:51 UTC",
    "updated_date": "2024-05-10 06:09:02 UTC"
  },
  {
    "arxiv_id": "2404.12721v1",
    "title": "Generalized Few-Shot Meets Remote Sensing: Discovering Novel Classes in Land Cover Mapping via Hybrid Semantic Segmentation Framework",
    "authors": [
      "Zhuohong Li",
      "Fangxiao Lu",
      "Jiaqi Zou",
      "Lei Hu",
      "Hongyan Zhang"
    ],
    "abstract": "Land-cover mapping is one of the vital applications in Earth observation,\naiming at classifying each pixel's land-cover type of remote-sensing images. As\nnatural and human activities change the landscape, the land-cover map needs to\nbe rapidly updated. However, discovering newly appeared land-cover types in\nexisting classification systems is still a non-trivial task hindered by various\nscales of complex land objects and insufficient labeled data over a wide-span\ngeographic area. In this paper, we propose a generalized few-shot\nsegmentation-based framework, named SegLand, to update novel classes in\nhigh-resolution land-cover mapping. Specifically, the proposed framework is\ndesigned in three parts: (a) Data pre-processing: the base training set and the\nfew-shot support sets of novel classes are analyzed and augmented; (b) Hybrid\nsegmentation structure; Multiple base learners and a modified Projection onto\nOrthogonal Prototypes (POP) network are combined to enhance the base-class\nrecognition and to dig novel classes from insufficient labels data; (c)\nUltimate fusion: the semantic segmentation results of the base learners and POP\nnetwork are reasonably fused. The proposed framework has won first place in the\nleaderboard of the OpenEarthMap Land Cover Mapping Few-Shot Challenge.\nExperiments demonstrate the superiority of the framework for automatically\nupdating novel land-cover classes with limited labeled data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 11 figures, accepted by CVPR 2024 L3D-IVU Workshop",
    "pdf_url": "http://arxiv.org/pdf/2404.12721v1",
    "published_date": "2024-04-19 09:01:58 UTC",
    "updated_date": "2024-04-19 09:01:58 UTC"
  },
  {
    "arxiv_id": "2404.12717v1",
    "title": "Show and Grasp: Few-shot Semantic Segmentation for Robot Grasping through Zero-shot Foundation Models",
    "authors": [
      "Leonardo Barcellona",
      "Alberto Bacchin",
      "Matteo Terreran",
      "Emanuele Menegatti",
      "Stefano Ghidoni"
    ],
    "abstract": "The ability of a robot to pick an object, known as robot grasping, is crucial\nfor several applications, such as assembly or sorting. In such tasks, selecting\nthe right target to pick is as essential as inferring a correct configuration\nof the gripper. A common solution to this problem relies on semantic\nsegmentation models, which often show poor generalization to unseen objects and\nrequire considerable time and massive data to be trained. To reduce the need\nfor large datasets, some grasping pipelines exploit few-shot semantic\nsegmentation models, which are capable of recognizing new classes given a few\nexamples. However, this often comes at the cost of limited performance and\nfine-tuning is required to be effective in robot grasping scenarios. In this\nwork, we propose to overcome all these limitations by combining the impressive\ngeneralization capability reached by foundation models with a high-performing\nfew-shot classifier, working as a score function to select the segmentation\nthat is closer to the support set. The proposed model is designed to be\nembedded in a grasp synthesis pipeline. The extensive experiments using one or\nfive examples show that our novel approach overcomes existing performance\nlimitations, improving the state of the art both in few-shot semantic\nsegmentation on the Graspnet-1B (+10.5% mIoU) and Ocid-grasp (+1.6% AP)\ndatasets, and real-world few-shot grasp synthesis (+21.7% grasp accuracy). The\nproject page is available at:\nhttps://leobarcellona.github.io/showandgrasp.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12717v1",
    "published_date": "2024-04-19 08:58:52 UTC",
    "updated_date": "2024-04-19 08:58:52 UTC"
  },
  {
    "arxiv_id": "2404.12712v1",
    "title": "uTRAND: Unsupervised Anomaly Detection in Traffic Trajectories",
    "authors": [
      "Giacomo D'Amicantonio",
      "Egor Bondarau",
      "Peter H. N. de With"
    ],
    "abstract": "Deep learning-based approaches have achieved significant improvements on\npublic video anomaly datasets, but often do not perform well in real-world\napplications. This paper addresses two issues: the lack of labeled data and the\ndifficulty of explaining the predictions of a neural network. To this end, we\npresent a framework called uTRAND, that shifts the problem of anomalous\ntrajectory prediction from the pixel space to a semantic-topological domain.\nThe framework detects and tracks all types of traffic agents in bird's-eye-view\nvideos of traffic cameras mounted at an intersection. By conceptualizing the\nintersection as a patch-based graph, it is shown that the framework learns and\nmodels the normal behaviour of traffic agents without costly manual labeling.\nFurthermore, uTRAND allows to formulate simple rules to classify anomalous\ntrajectories in a way suited for human interpretation. We show that uTRAND\noutperforms other state-of-the-art approaches on a dataset of anomalous\ntrajectories collected in a real-world setting, while producing explainable\ndetection results.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12712v1",
    "published_date": "2024-04-19 08:46:33 UTC",
    "updated_date": "2024-04-19 08:46:33 UTC"
  },
  {
    "arxiv_id": "2404.13099v1",
    "title": "Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks",
    "authors": [
      "Avinash Anand",
      "Mohit Gupta",
      "Kritarth Prasad",
      "Navya Singla",
      "Sanjana Sanjeev",
      "Jatin Kumar",
      "Adarsh Raj Shivam",
      "Rajiv Ratn Shah"
    ],
    "abstract": "The rapid progress in the field of natural language processing (NLP) systems\nand the expansion of large language models (LLMs) have opened up numerous\nopportunities in the field of education and instructional methods. These\nadvancements offer the potential for tailored learning experiences and\nimmediate feedback, all delivered through accessible and cost-effective\nservices. One notable application area for this technological advancement is in\nthe realm of solving mathematical problems. Mathematical problem-solving not\nonly requires the ability to decipher complex problem statements but also the\nskill to perform precise arithmetic calculations at each step of the\nproblem-solving process. However, the evaluation of the arithmetic capabilities\nof large language models remains an area that has received relatively little\nattention. In response, we introduce an extensive mathematics dataset called\n\"MathQuest\" sourced from the 11th and 12th standard Mathematics NCERT\ntextbooks. This dataset encompasses mathematical challenges of varying\ncomplexity and covers a wide range of mathematical concepts. Utilizing this\ndataset, we conduct fine-tuning experiments with three prominent LLMs: LLaMA-2,\nWizardMath, and MAmmoTH. These fine-tuned models serve as benchmarks for\nevaluating their performance on our dataset. Our experiments reveal that among\nthe three models, MAmmoTH-13B emerges as the most proficient, achieving the\nhighest level of competence in solving the presented mathematical problems.\nConsequently, MAmmoTH-13B establishes itself as a robust and dependable\nbenchmark for addressing NCERT mathematics problems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 3 figures, NeurIPS 2023 Workshop on Generative AI for\n  Education (GAIED)",
    "pdf_url": "http://arxiv.org/pdf/2404.13099v1",
    "published_date": "2024-04-19 08:45:42 UTC",
    "updated_date": "2024-04-19 08:45:42 UTC"
  },
  {
    "arxiv_id": "2404.17590v1",
    "title": "Leveraging Intra-modal and Inter-modal Interaction for Multi-Modal Entity Alignment",
    "authors": [
      "Zhiwei Hu",
      "Víctor Gutiérrez-Basulto",
      "Zhiliang Xiang",
      "Ru Li",
      "Jeff Z. Pan"
    ],
    "abstract": "Multi-modal entity alignment (MMEA) aims to identify equivalent entity pairs\nacross different multi-modal knowledge graphs (MMKGs). Existing approaches\nfocus on how to better encode and aggregate information from different\nmodalities. However, it is not trivial to leverage multi-modal knowledge in\nentity alignment due to the modal heterogeneity. In this paper, we propose a\nMulti-Grained Interaction framework for Multi-Modal Entity Alignment (MIMEA),\nwhich effectively realizes multi-granular interaction within the same modality\nor between different modalities. MIMEA is composed of four modules: i) a\nMulti-modal Knowledge Embedding module, which extracts modality-specific\nrepresentations with multiple individual encoders; ii) a Probability-guided\nModal Fusion module, which employs a probability guided approach to integrate\nuni-modal representations into joint-modal embeddings, while considering the\ninteraction between uni-modal representations; iii) an Optimal Transport Modal\nAlignment module, which introduces an optimal transport mechanism to encourage\nthe interaction between uni-modal and joint-modal embeddings; iv) a\nModal-adaptive Contrastive Learning module, which distinguishes the embeddings\nof equivalent entities from those of non-equivalent ones, for each modality.\nExtensive experiments conducted on two real-world datasets demonstrate the\nstrong performance of MIMEA compared to the SoTA. Datasets and code have been\nsubmitted as supplementary materials.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17590v1",
    "published_date": "2024-04-19 08:43:11 UTC",
    "updated_date": "2024-04-19 08:43:11 UTC"
  },
  {
    "arxiv_id": "2404.12711v1",
    "title": "Dynamic Temperature Knowledge Distillation",
    "authors": [
      "Yukang Wei",
      "Yu Bai"
    ],
    "abstract": "Temperature plays a pivotal role in moderating label softness in the realm of\nknowledge distillation (KD). Traditional approaches often employ a static\ntemperature throughout the KD process, which fails to address the nuanced\ncomplexities of samples with varying levels of difficulty and overlooks the\ndistinct capabilities of different teacher-student pairings. This leads to a\nless-than-ideal transfer of knowledge. To improve the process of knowledge\npropagation, we proposed Dynamic Temperature Knowledge Distillation (DTKD)\nwhich introduces a dynamic, cooperative temperature control for both teacher\nand student models simultaneously within each training iterafion. In\nparticular, we proposed \"\\textbf{sharpness}\" as a metric to quantify the\nsmoothness of a model's output distribution. By minimizing the sharpness\ndifference between the teacher and the student, we can derive sample-specific\ntemperatures for them respectively. Extensive experiments on CIFAR-100 and\nImageNet-2012 demonstrate that DTKD performs comparably to leading KD\ntechniques, with added robustness in Target Class KD and None-target Class KD\nscenarios.The code is available at https://github.com/JinYu1998/DTKD.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12711v1",
    "published_date": "2024-04-19 08:40:52 UTC",
    "updated_date": "2024-04-19 08:40:52 UTC"
  },
  {
    "arxiv_id": "2404.12704v1",
    "title": "A Clean-graph Backdoor Attack against Graph Convolutional Networks with Poisoned Label Only",
    "authors": [
      "Jiazhu Dai",
      "Haoyu Sun"
    ],
    "abstract": "Graph Convolutional Networks (GCNs) have shown excellent performance in\ndealing with various graph structures such as node classification, graph\nclassification and other tasks. However,recent studies have shown that GCNs are\nvulnerable to a novel threat known as backdoor attacks. However, all existing\nbackdoor attacks in the graph domain require modifying the training samples to\naccomplish the backdoor injection, which may not be practical in many realistic\nscenarios where adversaries have no access to modify the training samples and\nmay leads to the backdoor attack being detected easily. In order to explore the\nbackdoor vulnerability of GCNs and create a more practical and stealthy\nbackdoor attack method, this paper proposes a clean-graph backdoor attack\nagainst GCNs (CBAG) in the node classification task,which only poisons the\ntraining labels without any modification to the training samples, revealing\nthat GCNs have this security vulnerability. Specifically, CBAG designs a new\ntrigger exploration method to find important feature dimensions as the trigger\npatterns to improve the attack performance. By poisoning the training labels, a\nhidden backdoor is injected into the GCNs model. Experimental results show that\nour clean graph backdoor can achieve 99% attack success rate while maintaining\nthe functionality of the GCNs model on benign samples.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12704v1",
    "published_date": "2024-04-19 08:21:54 UTC",
    "updated_date": "2024-04-19 08:21:54 UTC"
  },
  {
    "arxiv_id": "2404.12691v2",
    "title": "Data Authenticity, Consent, & Provenance for AI are all broken: what will it take to fix them?",
    "authors": [
      "Shayne Longpre",
      "Robert Mahari",
      "Naana Obeng-Marnu",
      "William Brannon",
      "Tobin South",
      "Katy Gero",
      "Sandy Pentland",
      "Jad Kabbara"
    ],
    "abstract": "New capabilities in foundation models are owed in large part to massive,\nwidely-sourced, and under-documented training data collections. Existing\npractices in data collection have led to challenges in tracing authenticity,\nverifying consent, preserving privacy, addressing representation and bias,\nrespecting copyright, and overall developing ethical and trustworthy foundation\nmodels. In response, regulation is emphasizing the need for training data\ntransparency to understand foundation models' limitations. Based on a\nlarge-scale analysis of the foundation model training data landscape and\nexisting solutions, we identify the missing infrastructure to facilitate\nresponsible foundation model development practices. We examine the current\nshortcomings of common tools for tracing data authenticity, consent, and\ndocumentation, and outline how policymakers, developers, and data creators can\nfacilitate responsible foundation model development by adopting universal data\nprovenance standards.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "ICML 2024 camera-ready version (Spotlight paper). 9 pages, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.12691v2",
    "published_date": "2024-04-19 07:42:35 UTC",
    "updated_date": "2024-08-30 21:20:12 UTC"
  },
  {
    "arxiv_id": "2404.12689v2",
    "title": "Can LLMs Understand Computer Networks? Towards a Virtual System Administrator",
    "authors": [
      "Denis Donadel",
      "Francesco Marchiori",
      "Luca Pajola",
      "Mauro Conti"
    ],
    "abstract": "Recent advancements in Artificial Intelligence, and particularly Large\nLanguage Models (LLMs), offer promising prospects for aiding system\nadministrators in managing the complexity of modern networks. However, despite\nthis potential, a significant gap exists in the literature regarding the extent\nto which LLMs can understand computer networks. Without empirical evidence,\nsystem administrators might rely on these models without assurance of their\nefficacy in performing network-related tasks accurately.\n  In this paper, we are the first to conduct an exhaustive study on LLMs'\ncomprehension of computer networks. We formulate several research questions to\ndetermine whether LLMs can provide correct answers when supplied with a network\ntopology and questions on it. To assess them, we developed a thorough framework\nfor evaluating LLMs' capabilities in various network-related tasks. We evaluate\nour framework on multiple computer networks employing proprietary (e.g., GPT4)\nand open-source (e.g., Llama2) models. Our findings in general purpose LLMs\nusing a zero-shot scenario demonstrate promising results, with the best model\nachieving an average accuracy of 79.3%. Proprietary LLMs achieve noteworthy\nresults in small and medium networks, while challenges persist in comprehending\ncomplex network topologies, particularly for open-source models. Moreover, we\nprovide insight into how prompt engineering can enhance the accuracy of some\ntasks.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.ET",
      "C.2.1; C.2.5; I.2.1"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12689v2",
    "published_date": "2024-04-19 07:41:54 UTC",
    "updated_date": "2024-07-31 12:02:56 UTC"
  },
  {
    "arxiv_id": "2404.12667v1",
    "title": "Detecting Out-Of-Distribution Earth Observation Images with Diffusion Models",
    "authors": [
      "Georges Le Bellier",
      "Nicolas Audebert"
    ],
    "abstract": "Earth Observation imagery can capture rare and unusual events, such as\ndisasters and major landscape changes, whose visual appearance contrasts with\nthe usual observations. Deep models trained on common remote sensing data will\noutput drastically different features for these out-of-distribution samples,\ncompared to those closer to their training dataset. Detecting them could\ntherefore help anticipate changes in the observations, either geographical or\nenvironmental. In this work, we show that the reconstruction error of diffusion\nmodels can effectively serve as unsupervised out-of-distribution detectors for\nremote sensing images, using them as a plausibility score. Moreover, we\nintroduce ODEED, a novel reconstruction-based scorer using the probability-flow\nODE of diffusion models. We validate it experimentally on SpaceNet 8 with\nvarious scenarios, such as classical OOD detection with geographical shift and\nnear-OOD setups: pre/post-flood and non-flooded/flooded image recognition. We\nshow that our ODEED scorer significantly outperforms other diffusion-based and\ndiscriminative baselines on the more challenging near-OOD scenarios of flood\nimage detection, where OOD images are close to the distribution tail. We aim to\npave the way towards better use of generative models for anomaly detection in\nremote sensing.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "EARTHVISION 2024 IEEE/CVF CVPR Workshop. Large Scale Computer Vision\n  for Remote Sensing Imagery, Jun 2024, Seattle, United States",
    "pdf_url": "http://arxiv.org/pdf/2404.12667v1",
    "published_date": "2024-04-19 07:07:36 UTC",
    "updated_date": "2024-04-19 07:07:36 UTC"
  },
  {
    "arxiv_id": "2404.12653v1",
    "title": "How Real Is Real? A Human Evaluation Framework for Unrestricted Adversarial Examples",
    "authors": [
      "Dren Fazlija",
      "Arkadij Orlov",
      "Johanna Schrader",
      "Monty-Maximilian Zühlke",
      "Michael Rohs",
      "Daniel Kudenko"
    ],
    "abstract": "With an ever-increasing reliance on machine learning (ML) models in the real\nworld, adversarial examples threaten the safety of AI-based systems such as\nautonomous vehicles. In the image domain, they represent maliciously perturbed\ndata points that look benign to humans (i.e., the image modification is not\nnoticeable) but greatly mislead state-of-the-art ML models. Previously,\nresearchers ensured the imperceptibility of their altered data points by\nrestricting perturbations via $\\ell_p$ norms. However, recent publications\nclaim that creating natural-looking adversarial examples without such\nrestrictions is also possible. With much more freedom to instill malicious\ninformation into data, these unrestricted adversarial examples can potentially\novercome traditional defense strategies as they are not constrained by the\nlimitations or patterns these defenses typically recognize and mitigate. This\nallows attackers to operate outside of expected threat models. However,\nsurveying existing image-based methods, we noticed a need for more human\nevaluations of the proposed image modifications. Based on existing\nhuman-assessment frameworks for image generation quality, we propose SCOOTER -\nan evaluation framework for unrestricted image-based attacks. It provides\nresearchers with guidelines for conducting statistically significant human\nexperiments, standardized questions, and a ready-to-use implementation. We\npropose a framework that allows researchers to analyze how imperceptible their\nunrestricted attacks truly are.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "3 pages, 3 figures, AAAI 2024 Spring Symposium on User-Aligned\n  Assessment of Adaptive AI Systems",
    "pdf_url": "http://arxiv.org/pdf/2404.12653v1",
    "published_date": "2024-04-19 06:42:01 UTC",
    "updated_date": "2024-04-19 06:42:01 UTC"
  },
  {
    "arxiv_id": "2404.12652v2",
    "title": "Pre-trained Vision-Language Models Learn Discoverable Visual Concepts",
    "authors": [
      "Yuan Zang",
      "Tian Yun",
      "Hao Tan",
      "Trung Bui",
      "Chen Sun"
    ],
    "abstract": "Do vision-language models (VLMs) pre-trained to caption an image of a\n\"durian\" learn visual concepts such as \"brown\" (color) and \"spiky\" (texture) at\nthe same time? We aim to answer this question as visual concepts learned \"for\nfree\" would enable wide applications such as neuro-symbolic reasoning or\nhuman-interpretable object classification. We assume that the visual concepts,\nif captured by pre-trained VLMs, can be extracted by their vision-language\ninterface with text-based concept prompts. We observe that recent works\nprompting VLMs with concepts often differ in their strategies to define and\nevaluate the visual concepts, leading to conflicting conclusions. We propose a\nnew concept definition strategy based on two observations: First, certain\nconcept prompts include shortcuts that recognize correct concepts for wrong\nreasons; Second, multimodal information (e.g. visual discriminativeness, and\ntextual knowledge) should be leveraged when selecting the concepts. Our\nproposed concept discovery and learning (CDL) framework is thus designed to\nidentify a diverse list of generic visual concepts (e.g. \"spiky\" as opposed to\n\"spiky durian\"), which are ranked and selected based on visual and language\nmutual information. We carefully design quantitative and human evaluations of\nthe discovered concepts on six diverse visual recognition datasets, which\nconfirm that pre-trained VLMs do learn visual concepts that provide accurate\nand thorough descriptions for the recognized objects. All code and models are\npublicly released.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Transactions on Machine Learning Research, 2025",
    "pdf_url": "http://arxiv.org/pdf/2404.12652v2",
    "published_date": "2024-04-19 06:41:32 UTC",
    "updated_date": "2025-01-13 21:59:56 UTC"
  },
  {
    "arxiv_id": "2404.13096v1",
    "title": "Reducing Redundant Computation in Multi-Agent Coordination through Locally Centralized Execution",
    "authors": [
      "Yidong Bai",
      "Toshiharu Sugawara"
    ],
    "abstract": "In multi-agent reinforcement learning, decentralized execution is a common\napproach, yet it suffers from the redundant computation problem. This occurs\nwhen multiple agents redundantly perform the same or similar computation due to\noverlapping observations. To address this issue, this study introduces a novel\nmethod referred to as locally centralized team transformer (LCTT). LCTT\nestablishes a locally centralized execution framework where selected agents\nserve as leaders, issuing instructions, while the rest agents, designated as\nworkers, act as these instructions without activating their policy networks.\nFor LCTT, we proposed the team-transformer (T-Trans) architecture that allows\nleaders to provide specific instructions to each worker, and the leadership\nshift mechanism that allows agents autonomously decide their roles as leaders\nor workers. Our experimental results demonstrate that the proposed method\neffectively reduces redundant computation, does not decrease reward levels, and\nleads to faster learning convergence.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "6 pages, 5 figures, Under review",
    "pdf_url": "http://arxiv.org/pdf/2404.13096v1",
    "published_date": "2024-04-19 06:13:37 UTC",
    "updated_date": "2024-04-19 06:13:37 UTC"
  },
  {
    "arxiv_id": "2404.12638v1",
    "title": "Learning to Cut via Hierarchical Sequence/Set Model for Efficient Mixed-Integer Programming",
    "authors": [
      "Jie Wang",
      "Zhihai Wang",
      "Xijun Li",
      "Yufei Kuang",
      "Zhihao Shi",
      "Fangzhou Zhu",
      "Mingxuan Yuan",
      "Jia Zeng",
      "Yongdong Zhang",
      "Feng Wu"
    ],
    "abstract": "Cutting planes (cuts) play an important role in solving mixed-integer linear\nprograms (MILPs), which formulate many important real-world applications. Cut\nselection heavily depends on (P1) which cuts to prefer and (P2) how many cuts\nto select. Although modern MILP solvers tackle (P1)-(P2) by human-designed\nheuristics, machine learning carries the potential to learn more effective\nheuristics. However, many existing learning-based methods learn which cuts to\nprefer, neglecting the importance of learning how many cuts to select.\nMoreover, we observe that (P3) what order of selected cuts to prefer\nsignificantly impacts the efficiency of MILP solvers as well. To address these\nchallenges, we propose a novel hierarchical sequence/set model (HEM) to learn\ncut selection policies. Specifically, HEM is a bi-level model: (1) a\nhigher-level module that learns how many cuts to select, (2) and a lower-level\nmodule -- that formulates the cut selection as a sequence/set to sequence\nlearning problem -- to learn policies selecting an ordered subset with the\ncardinality determined by the higher-level module. To the best of our\nknowledge, HEM is the first data-driven methodology that well tackles (P1)-(P3)\nsimultaneously. Experiments demonstrate that HEM significantly improves the\nefficiency of solving MILPs on eleven challenging MILP benchmarks, including\ntwo Huawei's real problems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2302.00244",
    "pdf_url": "http://arxiv.org/pdf/2404.12638v1",
    "published_date": "2024-04-19 05:40:25 UTC",
    "updated_date": "2024-04-19 05:40:25 UTC"
  },
  {
    "arxiv_id": "2404.12634v3",
    "title": "Transformer-Based Classification Outcome Prediction for Multimodal Stroke Treatment",
    "authors": [
      "Danqing Ma",
      "Meng Wang",
      "Ao Xiang",
      "Zongqing Qi",
      "Qin Yang"
    ],
    "abstract": "This study proposes a multi-modal fusion framework Multitrans based on the\nTransformer architecture and self-attention mechanism. This architecture\ncombines the study of non-contrast computed tomography (NCCT) images and\ndischarge diagnosis reports of patients undergoing stroke treatment, using a\nvariety of methods based on Transformer architecture approach to predicting\nfunctional outcomes of stroke treatment. The results show that the performance\nof single-modal text classification is significantly better than single-modal\nimage classification, but the effect of multi-modal combination is better than\nany single modality. Although the Transformer model only performs worse on\nimaging data, when combined with clinical meta-diagnostic information, both can\nlearn better complementary information and make good contributions to\naccurately predicting stroke treatment effects..",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12634v3",
    "published_date": "2024-04-19 05:31:37 UTC",
    "updated_date": "2024-11-16 02:36:32 UTC"
  },
  {
    "arxiv_id": "2404.12633v4",
    "title": "FlagVNE: A Flexible and Generalizable Reinforcement Learning Framework for Network Resource Allocation",
    "authors": [
      "Tianfu Wang",
      "Qilin Fan",
      "Chao Wang",
      "Long Yang",
      "Leilei Ding",
      "Nicholas Jing Yuan",
      "Hui Xiong"
    ],
    "abstract": "Virtual network embedding (VNE) is an essential resource allocation task in\nnetwork virtualization, aiming to map virtual network requests (VNRs) onto\nphysical infrastructure. Reinforcement learning (RL) has recently emerged as a\npromising solution to this problem. However, existing RL-based VNE methods are\nlimited by the unidirectional action design and one-size-fits-all training\nstrategy, resulting in restricted searchability and generalizability. In this\npaper, we propose a FLexible And Generalizable RL framework for VNE, named\nFlagVNE. Specifically, we design a bidirectional action-based Markov decision\nprocess model that enables the joint selection of virtual and physical nodes,\nthus improving the exploration flexibility of solution space. To tackle the\nexpansive and dynamic action space, we design a hierarchical decoder to\ngenerate adaptive action probability distributions and ensure high training\nefficiency. Furthermore, to overcome the generalization issue for varying VNR\nsizes, we propose a meta-RL-based training method with a curriculum scheduling\nstrategy, facilitating specialized policy training for each VNR size. Finally,\nextensive experimental results show the effectiveness of FlagVNE across\nmultiple key metrics. Our code is available at GitHub\n(https://github.com/GeminiLight/flag-vne).",
    "categories": [
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12633v4",
    "published_date": "2024-04-19 05:24:24 UTC",
    "updated_date": "2024-05-01 18:58:36 UTC"
  },
  {
    "arxiv_id": "2404.12631v2",
    "title": "Breaching the Bottleneck: Evolutionary Transition from Reward-Driven Learning to Reward-Agnostic Domain-Adapted Learning in Neuromodulated Neural Nets",
    "authors": [
      "Solvi Arnold",
      "Reiji Suzuki",
      "Takaya Arita",
      "Kimitoshi Yamazaki"
    ],
    "abstract": "Advanced biological intelligence learns efficiently from an information-rich\nstream of stimulus information, even when feedback on behaviour quality is\nsparse or absent. Such learning exploits implicit assumptions about task\ndomains. We refer to such learning as Domain-Adapted Learning (DAL). In\ncontrast, AI learning algorithms rely on explicit externally provided measures\nof behaviour quality to acquire fit behaviour. This imposes an information\nbottleneck that precludes learning from diverse non-reward stimulus\ninformation, limiting learning efficiency. We consider the question of how\nbiological evolution circumvents this bottleneck to produce DAL. We propose\nthat species first evolve the ability to learn from reward signals, providing\ninefficient (bottlenecked) but broad adaptivity. From there, integration of\nnon-reward information into the learning process can proceed via gradual\naccumulation of biases induced by such information on specific task domains.\nThis scenario provides a biologically plausible pathway towards\nbottleneck-free, domain-adapted learning. Focusing on the second phase of this\nscenario, we set up a population of NNs with reward-driven learning modelled as\nReinforcement Learning (A2C), and allow evolution to improve learning\nefficiency by integrating non-reward information into the learning process\nusing a neuromodulatory update mechanism. On a navigation task in continuous 2D\nspace, evolved DAL agents show a 300-fold increase in learning speed compared\nto pure RL agents. Evolution is found to eliminate reliance on reward\ninformation altogether, allowing DAL agents to learn from non-reward\ninformation exclusively, using local neuromodulation-based connection weight\nupdates only. Code available at github.com/aislab/dal.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "I.2.6"
    ],
    "primary_category": "cs.NE",
    "comment": "Camera ready version. 9 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.12631v2",
    "published_date": "2024-04-19 05:14:47 UTC",
    "updated_date": "2024-08-02 07:04:42 UTC"
  },
  {
    "arxiv_id": "2404.12627v1",
    "title": "A Soft e-Textile Sensor for Enhanced Deep Learning-based Shape Sensing of Soft Continuum Robots",
    "authors": [
      "Eric Vincent Galeta",
      "Ayman A. Nada",
      "Sabah M. Ahmed",
      "Victor Parque",
      "Haitham El-Hussieny"
    ],
    "abstract": "The safety and accuracy of robotic navigation hold paramount importance,\nespecially in the realm of soft continuum robotics, where the limitations of\ntraditional rigid sensors become evident. Encoders, piezoresistive, and\npotentiometer sensors often fail to integrate well with the flexible nature of\nthese robots, adding unwanted bulk and rigidity. To overcome these hurdles, our\nstudy presents a new approach to shape sensing in soft continuum robots through\nthe use of soft e-textile resistive sensors. This sensor, designed to\nflawlessly integrate with the robot's structure, utilizes a resistive material\nthat adjusts its resistance in response to the robot's movements and\ndeformations. This adjustment facilitates the capture of multidimensional force\nmeasurements across the soft sensor layers. A deep Convolutional Neural Network\n(CNN) is employed to decode the sensor signals, enabling precise estimation of\nthe robot's shape configuration based on the detailed data from the e-textile\nsensor. Our research investigates the efficacy of this e-textile sensor in\ndetermining the curvature parameters of soft continuum robots. The findings are\nencouraging, showing that the soft e-textile sensor not only matches but\npotentially exceeds the capabilities of traditional rigid sensors in terms of\nshape sensing and estimation. This advancement significantly boosts the safety\nand efficiency of robotic navigation systems.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12627v1",
    "published_date": "2024-04-19 05:00:25 UTC",
    "updated_date": "2024-04-19 05:00:25 UTC"
  },
  {
    "arxiv_id": "2404.12626v1",
    "title": "Grasper: A Generalist Pursuer for Pursuit-Evasion Problems",
    "authors": [
      "Pengdeng Li",
      "Shuxin Li",
      "Xinrun Wang",
      "Jakub Cerny",
      "Youzhi Zhang",
      "Stephen McAleer",
      "Hau Chan",
      "Bo An"
    ],
    "abstract": "Pursuit-evasion games (PEGs) model interactions between a team of pursuers\nand an evader in graph-based environments such as urban street networks. Recent\nadvancements have demonstrated the effectiveness of the pre-training and\nfine-tuning paradigm in PSRO to improve scalability in solving large-scale\nPEGs. However, these methods primarily focus on specific PEGs with fixed\ninitial conditions that may vary substantially in real-world scenarios, which\nsignificantly hinders the applicability of the traditional methods. To address\nthis issue, we introduce Grasper, a GeneRAlist purSuer for Pursuit-Evasion\npRoblems, capable of efficiently generating pursuer policies tailored to\nspecific PEGs. Our contributions are threefold: First, we present a novel\narchitecture that offers high-quality solutions for diverse PEGs, comprising\ncritical components such as (i) a graph neural network (GNN) to encode PEGs\ninto hidden vectors, and (ii) a hypernetwork to generate pursuer policies based\non these hidden vectors. As a second contribution, we develop an efficient\nthree-stage training method involving (i) a pre-pretraining stage for learning\nrobust PEG representations through self-supervised graph learning techniques\nlike GraphMAE, (ii) a pre-training stage utilizing heuristic-guided multi-task\npre-training (HMP) where heuristic-derived reference policies (e.g., through\nDijkstra's algorithm) regularize pursuer policies, and (iii) a fine-tuning\nstage that employs PSRO to generate pursuer policies on designated PEGs.\nFinally, we perform extensive experiments on synthetic and real-world maps,\nshowcasing Grasper's significant superiority over baselines in terms of\nsolution quality and generalizability. We demonstrate that Grasper provides a\nversatile approach for solving pursuit-evasion problems across a broad range of\nscenarios, enabling practical deployment in real-world situations.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear in the 23rd International Conference on Autonomous Agents\n  and Multi-Agent Systems (AAMAS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2404.12626v1",
    "published_date": "2024-04-19 04:54:38 UTC",
    "updated_date": "2024-04-19 04:54:38 UTC"
  },
  {
    "arxiv_id": "2404.12618v1",
    "title": "CORI: CJKV Benchmark with Romanization Integration -- A step towards Cross-lingual Transfer Beyond Textual Scripts",
    "authors": [
      "Hoang H. Nguyen",
      "Chenwei Zhang",
      "Ye Liu",
      "Natalie Parde",
      "Eugene Rohrbaugh",
      "Philip S. Yu"
    ],
    "abstract": "Naively assuming English as a source language may hinder cross-lingual\ntransfer for many languages by failing to consider the importance of language\ncontact. Some languages are more well-connected than others, and target\nlanguages can benefit from transferring from closely related languages; for\nmany languages, the set of closely related languages does not include English.\nIn this work, we study the impact of source language for cross-lingual\ntransfer, demonstrating the importance of selecting source languages that have\nhigh contact with the target language. We also construct a novel benchmark\ndataset for close contact Chinese-Japanese-Korean-Vietnamese (CJKV) languages\nto further encourage in-depth studies of language contact. To comprehensively\ncapture contact between these languages, we propose to integrate Romanized\ntranscription beyond textual scripts via Contrastive Learning objectives,\nleading to enhanced cross-lingual representations and effective zero-shot\ncross-lingual transfer.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.12618v1",
    "published_date": "2024-04-19 04:02:50 UTC",
    "updated_date": "2024-04-19 04:02:50 UTC"
  },
  {
    "arxiv_id": "2404.12605v1",
    "title": "GluMarker: A Novel Predictive Modeling of Glycemic Control Through Digital Biomarkers",
    "authors": [
      "Ziyi Zhou",
      "Ming Cheng",
      "Xingjian Diao",
      "Yanjun Cui",
      "Xiangling Li"
    ],
    "abstract": "The escalating prevalence of diabetes globally underscores the need for\ndiabetes management. Recent research highlights the growing focus on digital\nbiomarkers in diabetes management, with innovations in computational frameworks\nand noninvasive monitoring techniques using personalized glucose metrics.\nHowever, they predominantly focus on insulin dosing and specific glucose\nvalues, or with limited attention given to overall glycemic control. This\nleaves a gap in expanding the scope of digital biomarkers for overall glycemic\ncontrol in diabetes management. To address such a research gap, we propose\nGluMarker -- an end-to-end framework for modeling digital biomarkers using\nbroader factors sources to predict glycemic control. Through the assessment and\nrefinement of various machine learning baselines, GluMarker achieves\nstate-of-the-art on Anderson's dataset in predicting next-day glycemic control.\nMoreover, our research identifies key digital biomarkers for the next day's\nglycemic control prediction. These identified biomarkers are instrumental in\nilluminating the daily factors that influence glycemic management, offering\nvital insights for diabetes care.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12605v1",
    "published_date": "2024-04-19 03:15:50 UTC",
    "updated_date": "2024-04-19 03:15:50 UTC"
  },
  {
    "arxiv_id": "2404.12596v1",
    "title": "Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level Knowledge Distillation",
    "authors": [
      "Lasal Jayawardena",
      "Prasan Yapa"
    ],
    "abstract": "Over the past year, the field of Natural Language Generation (NLG) has\nexperienced an exponential surge, largely due to the introduction of Large\nLanguage Models (LLMs). These models have exhibited the most effective\nperformance in a range of domains within the Natural Language Processing and\nGeneration domains. However, their application in domain-specific tasks, such\nas paraphrasing, presents significant challenges. The extensive number of\nparameters makes them difficult to operate on commercial hardware, and they\nrequire substantial time for inference, leading to high costs in a production\nsetting. In this study, we tackle these obstacles by employing LLMs to develop\nthree distinct models for the paraphrasing field, applying a method referred to\nas sequence-level knowledge distillation. These distilled models are capable of\nmaintaining the quality of paraphrases generated by the LLM. They demonstrate\nfaster inference times and the ability to generate diverse paraphrases of\ncomparable quality. A notable characteristic of these models is their ability\nto exhibit syntactic diversity while also preserving lexical diversity,\nfeatures previously uncommon due to existing data quality issues in datasets\nand not typically observed in neural-based approaches. Human evaluation of our\nmodels shows that there is only a 4% drop in performance compared to the LLM\nteacher model used in the distillation process, despite being 1000 times\nsmaller. This research provides a significant contribution to the NLG field,\noffering a more efficient and cost-effective solution for paraphrasing tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Published in: 2024 5th International Conference on Advancements in\n  Computational Sciences (ICACS) with IEEE",
    "pdf_url": "http://arxiv.org/pdf/2404.12596v1",
    "published_date": "2024-04-19 02:59:09 UTC",
    "updated_date": "2024-04-19 02:59:09 UTC"
  },
  {
    "arxiv_id": "2404.12594v1",
    "title": "Random Network Distillation Based Deep Reinforcement Learning for AGV Path Planning",
    "authors": [
      "Huilin Yin",
      "Shengkai Su",
      "Yinjia Lin",
      "Pengju Zhen",
      "Karin Festl",
      "Daniel Watzenig"
    ],
    "abstract": "With the flourishing development of intelligent warehousing systems, the\ntechnology of Automated Guided Vehicle (AGV) has experienced rapid growth.\nWithin intelligent warehousing environments, AGV is required to safely and\nrapidly plan an optimal path in complex and dynamic environments. Most research\nhas studied deep reinforcement learning to address this challenge. However, in\nthe environments with sparse extrinsic rewards, these algorithms often converge\nslowly, learn inefficiently or fail to reach the target. Random Network\nDistillation (RND), as an exploration enhancement, can effectively improve the\nperformance of proximal policy optimization, especially enhancing the\nadditional intrinsic rewards of the AGV agent which is in sparse reward\nenvironments. Moreover, most of the current research continues to use 2D grid\nmazes as experimental environments. These environments have insufficient\ncomplexity and limited action sets. To solve this limitation, we present\nsimulation environments of AGV path planning with continuous actions and\npositions for AGVs, so that it can be close to realistic physical scenarios.\nBased on our experiments and comprehensive analysis of the proposed method, the\nresults demonstrate that our proposed method enables AGV to more rapidly\ncomplete path planning tasks with continuous actions in our environments. A\nvideo of part of our experiments can be found at https://youtu.be/lwrY9YesGmw.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.12594v1",
    "published_date": "2024-04-19 02:52:56 UTC",
    "updated_date": "2024-04-19 02:52:56 UTC"
  },
  {
    "arxiv_id": "2404.12587v1",
    "title": "Reinforcement Learning Approach for Integrating Compressed Contexts into Knowledge Graphs",
    "authors": [
      "Ngoc Quach",
      "Qi Wang",
      "Zijun Gao",
      "Qifeng Sun",
      "Bo Guan",
      "Lillian Floyd"
    ],
    "abstract": "The widespread use of knowledge graphs in various fields has brought about a\nchallenge in effectively integrating and updating information within them. When\nit comes to incorporating contexts, conventional methods often rely on rules or\nbasic machine learning models, which may not fully grasp the complexity and\nfluidity of context information. This research suggests an approach based on\nreinforcement learning (RL), specifically utilizing Deep Q Networks (DQN) to\nenhance the process of integrating contexts into knowledge graphs. By\nconsidering the state of the knowledge graph as environment states defining\nactions as operations for integrating contexts and using a reward function to\ngauge the improvement in knowledge graph quality post-integration, this method\naims to automatically develop strategies for optimal context integration. Our\nDQN model utilizes networks as function approximators, continually updating Q\nvalues to estimate the action value function, thus enabling effective\nintegration of intricate and dynamic context information. Initial experimental\nfindings show that our RL method outperforms techniques in achieving precise\ncontext integration across various standard knowledge graph datasets,\nhighlighting the potential and effectiveness of reinforcement learning in\nenhancing and managing knowledge graphs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted by the 2024 International Conference on\n  Machine Learning and Neural Networks (MLNN 2024)",
    "pdf_url": "http://arxiv.org/pdf/2404.12587v1",
    "published_date": "2024-04-19 02:32:43 UTC",
    "updated_date": "2024-04-19 02:32:43 UTC"
  },
  {
    "arxiv_id": "2404.12580v1",
    "title": "iTBLS: A Dataset of Interactive Conversations Over Tabular Information",
    "authors": [
      "Anirudh Sundar",
      "Christopher Richardson",
      "William Gay",
      "Larry Heck"
    ],
    "abstract": "This paper introduces Interactive Tables (iTBLS), a dataset of interactive\nconversations situated in tables from scientific articles. This dataset is\ndesigned to facilitate human-AI collaborative problem-solving through\nAI-powered multi-task tabular capabilities. In contrast to prior work that\nmodels interactions as factoid QA or procedure synthesis, iTBLS broadens the\nscope of interactions to include mathematical reasoning, natural language\nmanipulation, and expansion of existing tables from natural language\nconversation by delineating interactions into one of three tasks:\ninterpretation, modification, or generation. Additionally, the paper presents a\nsuite of baseline approaches to iTBLS, utilizing zero-shot prompting and\nparameter-efficient fine-tuning for different computing situations. We also\nintroduce a novel multi-step approach and show how it can be leveraged in\nconjunction with parameter-efficient fine-tuning to achieve the\nstate-of-the-art on iTBLS; outperforming standard parameter-efficient\nfine-tuning by up to 15% on interpretation, 18% on modification, and 38% on\ngeneration.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.12580v1",
    "published_date": "2024-04-19 02:11:41 UTC",
    "updated_date": "2024-04-19 02:11:41 UTC"
  },
  {
    "arxiv_id": "2404.12569v1",
    "title": "Multi-View Subgraph Neural Networks: Self-Supervised Learning with Scarce Labeled Data",
    "authors": [
      "Zhenzhong Wang",
      "Qingyuan Zeng",
      "Wanyu Lin",
      "Min Jiang",
      "Kay Chen Tan"
    ],
    "abstract": "While graph neural networks (GNNs) have become the de-facto standard for\ngraph-based node classification, they impose a strong assumption on the\navailability of sufficient labeled samples. This assumption restricts the\nclassification performance of prevailing GNNs on many real-world applications\nsuffering from low-data regimes. Specifically, features extracted from scarce\nlabeled nodes could not provide sufficient supervision for the unlabeled\nsamples, leading to severe over-fitting. In this work, we point out that\nleveraging subgraphs to capture long-range dependencies can augment the\nrepresentation of a node with homophily properties, thus alleviating the\nlow-data regime. However, prior works leveraging subgraphs fail to capture the\nlong-range dependencies among nodes. To this end, we present a novel\nself-supervised learning framework, called multi-view subgraph neural networks\n(Muse), for handling long-range dependencies. In particular, we propose an\ninformation theory-based identification mechanism to identify two types of\nsubgraphs from the views of input space and latent space, respectively. The\nformer is to capture the local structure of the graph, while the latter\ncaptures the long-range dependencies among nodes. By fusing these two views of\nsubgraphs, the learned representations can preserve the topological properties\nof the graph at large, including the local structure and long-range\ndependencies, thus maximizing their expressiveness for downstream node\nclassification tasks. Experimental results show that Muse outperforms the\nalternative methods on node classification tasks with limited labeled data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12569v1",
    "published_date": "2024-04-19 01:36:50 UTC",
    "updated_date": "2024-04-19 01:36:50 UTC"
  },
  {
    "arxiv_id": "2404.12548v2",
    "title": "RetailOpt: Opt-In, Easy-to-Deploy Trajectory Estimation from Smartphone Motion Data and Retail Facility Information",
    "authors": [
      "Ryo Yonetani",
      "Jun Baba",
      "Yasutaka Furukawa"
    ],
    "abstract": "We present RetailOpt, a novel opt-in, easy-to-deploy system for tracking\ncustomer movements offline in indoor retail environments. The system uses\nreadily accessible information from customer smartphones and retail apps,\nincluding motion data, store maps, and purchase records. This eliminates the\nneed for additional hardware installations/maintenance and ensures customers\nfull data control. Specifically, RetailOpt first uses inertial navigation to\nrecover relative trajectories from smartphone motion data. The store map and\npurchase records are cross-referenced to identify a list of visited shelves,\nproviding anchors to localize the relative trajectories in a store through\ncontinuous and discrete optimization. We demonstrate the effectiveness of our\nsystem in five diverse environments. The system, if successful, would produce\naccurate customer movement data, essential for a broad range of retail\napplications including customer behavior analysis and in-store navigation.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12548v2",
    "published_date": "2024-04-19 00:03:49 UTC",
    "updated_date": "2024-07-16 00:58:19 UTC"
  }
]
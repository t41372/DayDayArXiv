{
  "date": "2025-04-03",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间2025-04-03的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文再次被大型语言模型（LLM）相关研究占据主导地位，涵盖了从基础理论（如长度泛化、推理评估、知识图谱应用）到实际应用（如模型编辑、安全对齐、代码生成、机器人控制）的广泛议题。值得关注的工作包括用于图像编辑的 Concept Lancet、评估 LLM 推理能力的 KUMO 框架、大规模数学预训练语料库 MegaMath、以及对多模型合成数据在 DPO 安全对齐中潜在陷阱的深入探讨。视觉和多模态领域也亮点频出，包括利用稀疏自编码器提升 VLM 可解释性、高效的 3D 场景生成、长视频处理以及机器人世界模型等。此外，对模型鲁棒性、可解释性和公平性的关注仍在持续。\n\n以下是今天值得关注的论文：\n\n---\n\n**LLM 与 AI 智能体**\n\n1.  **KUMO：大型语言模型复杂推理能力的生成式评估 (Generative Evaluation of Complex Reasoning in Large Language Models)**\n    *   论文提出 KUMO 框架，通过结合 LLM 和符号引擎动态生成多样化、部分可观察且难度可调的多轮推理任务，旨在解决现有基准易被污染的问题，从而更真实地评估 LLM 的泛化推理能力而非记忆能力。评估了 23 个 SOTA LLM，发现许多模型在简单任务上超越大学生，推理优化模型在复杂任务上达到大学生水平。\n\n2.  **MegaMath：推动开放数学语料库的极限 (MegaMath: Pushing the Limits of Open Math Corpora)**\n    *   针对数学推理 LLM 预训练缺乏大规模、高质量开放语料库的问题，本文提出了 MegaMath 数据集 (371B tokens)。通过优化 Common Crawl 的数学文档提取、筛选数学相关代码、合成 QA/代码/图文混排数据等策略，构建了目前规模最大、质量最高的开放数学预训练数据集。\n\n3.  **知识思维图谱助力经济型 AI 助手 (Affordable AI Assistants with Knowledge Graph of Thoughts)**\n    *   为解决 LLM 驱动的 AI 助手成本高、复杂任务成功率有限的问题，提出了知识思维图谱 (KGoT) 架构。该架构将 LLM 推理与动态构建的知识图谱 (KG) 相结合，利用外部工具（数学求解器、网络爬虫等）迭代增强 KG，使得低成本模型也能有效解决复杂任务。在 GAIA 基准上，KGoT 相比使用 GPT-4o mini 的 Hugging Face Agents 成功率提升 29%，成本降低 36 倍以上。\n\n4.  **GPG：一种简单且强大的用于模型推理的强化学习基线 (GPG: A Simple and Strong Reinforcement Learning Baseline for Model Reasoning)**\n    *   本文提出了一种极简的强化学习方法——组策略梯度 (Group Policy Gradient, GPG)，用于直接增强 LLM 的推理能力。GPG 直接优化原始 RL 目标，无需替代损失函数、评论家模型或 KL 散度约束，显著简化了训练过程。实验表明，GPG 在降低计算成本的同时，在多种单模态和多模态任务上优于 GRPO 等方法。\n\n5.  **多即是少：DPO 安全对齐中多模型合成偏好数据的陷阱 (More is Less: The Pitfalls of Multi-Model Synthetic Preference Data in DPO Safety Alignment)**\n    *   研究发现，虽然使用多个模型生成的合成偏好数据进行 DPO 对齐能提升通用任务性能，但在安全对齐方面存在陷阱。混合使用不同模型（尤其是更强模型）生成的偏好数据，会使模型更容易在训练中发生“奖励黑客攻击 (reward hacking)”，导致面对越狱提示时攻击成功率 (ASR) 显著升高。研究表明，仅使用目标模型自生成的偏好对（单一模型生成）在安全性上优于混合强模型数据的配置，原因是多模型数据具有更高的线性可分性，使模型倾向于利用表面线索而非内化安全约束。\n\n6.  **变压器长度泛化中的方差消失问题 (On Vanishing Variance in Transformer Length Generalization)**\n    *   研究指出 Transformer 在短序列上训练后难以泛化到长序列的问题，并首次从“方差消失”的角度进行分析。实验证明，即使是当前的前沿模型，在处理更长序列时，多头注意力模块输出的方差也会减小。在特定任务上，注意力输出后应用层归一化 (Layer Normalization) 有助于改善长度泛化，部分缓解了方差消失导致的分布偏移。\n\n7.  **两个 AI 科学家会达成一致吗？ (Do Two AI Scientists Agree?)**\n    *   本文探讨了在相同科学任务上训练的两个 AI 模型是否学习到相同理论的问题。通过在物理问题上训练 Hamiltonian-Lagrangian 神经网络作为“AI 科学家”，研究发现随着训练数据中系统数量的增加，AI 科学家学习到的理论趋于收敛，但有时会形成对应不同理论的群体。研究还观察到 AI 科学家在简单设置下学习哈密顿理论，在复杂系统下转向拉格朗日表述，并强调了训练随机性对理论形成的影响。\n\n8.  **大型语言模型在多大程度上内化了科学文献和引用实践？ (How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?)**\n    *   研究发现，LLM（以 GPT-4o 为例）在生成参考文献时系统性地强化了“马太效应”，倾向于引用高被引论文。虽然 LLM 生成的引用在内容上与原文相关性较高，且减少了自引，但它们更偏好更新、标题更短、作者更少的文献，这与传统引用模式存在差异，可能重塑科学知识的传播和发现路径。\n\n9.  **使用日志化的 Bandit 数据进行提示优化 (Prompt Optimization with Logged Bandit Data)**\n    *   研究如何利用用户反馈（如点击）来优化 LLM 生成个性化句子的提示。提出了一种基于核函数的离策略梯度方法，利用生成句子间的相似性来估计策略梯度，旨在减少方差并抑制偏差。在电影推荐描述生成等任务上验证了方法的有效性。\n\n10. **多任务工具基准：通过相关和动态任务评估基于 LLM 的智能体的鲁棒性 (Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents through Related and Dynamic Missions)**\n    *   针对现有基准主要评估单任务场景、无法反映真实世界复杂性的问题，提出了 Multi-Mission Tool Bench。该基准包含多个相互关联的任务，要求智能体动态适应变化的需求，并探索了所有可能的任务切换模式。同时提出了一种基于动态决策树的评估方法。\n\n11. **通过任务局部化稀疏微调实现高效模型编辑 (Efficient Model Editing with Task-Localized Sparse Fine-tuning)**\n    *   为解决任务算术 (Task Arithmetic) 中模型编辑依赖网络线性化带来的计算瓶颈和权重纠缠问题，提出了 TaLoS 方法。该方法发现预训练模型中存在一部分跨任务梯度敏感度低的参数，仅稀疏更新这些参数即可促进权重解耦，无需显式线性化。实验证明 TaLoS 提高了训练和推理效率，并在任务加法和否定方面优于现有方法。\n\n12. **面向程序引导事实核查的 BOOST：引导策略驱动的推理程序 (BOOST: Bootstrapping Strategy-Driven Reasoning Programs for Program-Guided Fact-Checking)**\n    *   针对程序引导的复杂声明事实核查中，现有方法依赖人工设计的少样本示例、缺乏程序多样性的问题，提出了 BOOST 框架。BOOST 显式整合声明分解和信息收集策略作为结构化指导，以策略驱动和数据中心的方式迭代优化自举生成的示例，无需人工干预，实现了从零样本到少样本的策略性程序引导学习。\n\n13. **大型语言模型中的认知记忆 (Cognitive Memory in Large Language Models)**\n    *   这篇综述论文探讨了 LLM 中的记忆机制，将其分为感觉记忆（输入提示）、短期记忆（处理即时上下文）和长期记忆（外部数据库/结构）。详细讨论了基于文本、KV 缓存、模型参数和隐藏状态的记忆实现方式、管理策略和利用方法，强调了记忆对于生成上下文丰富、减少幻觉和提高效率的重要性。\n\n14. **LLM 作为欺骗性智能体：基于角色的提示如何在解谜任务中诱导语义模糊性 (LLMs as Deceptive Agents: How Role-Based Prompting Induces Semantic Ambiguity in Puzzle Tasks)**\n    *   研究 LLM 如何利用语义模糊性生成具有欺骗性的谜题。通过比较零样本、角色注入对抗性提示和人类设计的谜题，发现明确的对抗性智能体行为显著增加了谜题的语义模糊性，从而增加了人类解决者的认知负荷并降低了公平性。\n\n15. **LLM 社会模拟是一种有前景的研究方法 (LLM Social Simulations Are a Promising Research Method)**\n    *   这篇立场文件认为，通过 LLM 模拟人类研究对象有望成为理解人类行为和训练新 AI 系统的有效方法，并指出现有挑战（如提示工程、微调、评估等）是可解决的。作者认为 LLM 模拟已可用于探索性研究，并呼吁开发可随 AI 进展迭代部署和完善的概念模型与评估方法。\n\n---\n\n**计算机视觉与多模态**\n\n16. **Concept Lancet：使用组合表示移植进行图像编辑 (Concept Lancet: Image Editing with Compositional Representation Transplant)**\n    *   针对现有扩散模型图像编辑方法难以平衡编辑强度和视觉一致性的问题，提出 Concept Lancet (CoLan) 框架。CoLan 在推理时将源输入在潜在空间（文本嵌入或扩散分数）分解为视觉概念表示的稀疏线性组合，从而准确估计图像中概念的存在，并据此执行定制化的概念移植（替换/添加/移除）以施加编辑方向。同时构建了包含 15 万视觉术语/短语描述的 CoLan-150K 数据集。实验表明 CoLan 能提升多种基线方法的编辑效果和一致性。 (CVPR 2025)\n\n17. **稀疏自编码器在视觉语言模型中学习单语义特征 (Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models)**\n    *   将稀疏自编码器 (SAEs) 应用于 CLIP 等视觉语言模型 (VLMs)，并提出了评估视觉表示单语义性的框架。实验表明，在 VLM 上训练的 SAEs 显著增强了单个神经元的单语义性，并展现出与专家定义结构（如 iNaturalist 分类）一致的层级表示。更重要的是，通过干预 CLIP 视觉编码器，可以直接引导多模态 LLM（如 LLaVA）的输出，而无需修改底层模型。\n\n18. **GMR-Conv：使用高斯混合环的高效旋转和反射等变卷积核 (GMR-Conv: An Efficient Rotation and Reflection Equivariant Convolution Kernel Using Gaussian Mixture Rings)**\n    *   提出高斯混合环卷积 (GMR-Conv)，一种利用高斯加权环混合来平滑径向对称性的高效卷积核。该设计减轻了圆形核的离散化误差，在不增加计算开销的情况下保持了鲁棒的旋转和反射等变性。通过新颖的参数化和计算策略优化了空间和速度效率。在多个分类和分割数据集上的实验表明，GMR-Conv 性能优越，尤其在无方向性数据应用中，且比现有等变学习方法更鲁棒高效。\n\n19. **系统性评估大型视觉语言模型在手术人工智能中的应用 (Systematic Evaluation of Large Vision-Language Models for Surgical Artificial Intelligence)**\n    *   全面分析了 11 种 SOTA VLMs 在 17 项关键手术 AI 视觉理解任务（从解剖识别到技能评估）中的表现，涵盖腹腔镜、机器人和开放手术等 13 个数据集。实验发现 VLMs 展示出良好的泛化能力，有时甚至优于在特定训练集外部署的有监督模型。上下文学习 (In-context learning) 能显著提升性能。但需要空间或时间推理的任务仍具挑战性。\n\n20. **场景溅射：利用视频扩散模型从单图像进行动量 3D 场景生成 (Scene Splatter: Momentum 3D Scene Generation from Single Image with Video Diffusion Model)**\n    *   提出 Scene Splatter，一种基于动量 (momentum) 的视频扩散范式，用于从单张图像生成通用 3D 场景。为解决现有方法视频长度受限和场景不一致的问题，该方法将原始特征构造的噪声样本作为动量来增强视频细节和保持场景一致性。同时引入像素级动量以更好恢复未见区域。通过级联动量和迭代优化全局高斯表示，实现了高保真和一致的 3D 场景生成，避免了视频长度限制。 (CVPR 2025)\n\n21. **通过分层差分蒸馏将视频语言模型扩展至 10K 帧 (Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation)**\n    *   为解决长视频处理中 VLM 的高计算成本问题，提出差分蒸馏 (differential distillation) 方法，系统性保留任务相关信息并抑制冗余。基于此开发了 ViLaMP，一个分层 VLM，通过差分关键帧选择（最大化查询相关性并保持时间独特性）和差分特征融合（保留非关键帧中的查询显著特征）实现对长视频的“混合精度”处理。ViLaMP 能在单 A100 GPU 上处理高达 10K 帧的超长视频。\n\n22. **ConsDreamer：提升零样本 Text-to-3D 生成的多视图一致性 (ConsDreamer: Advancing Multi-View Consistency for Zero-Shot Text-to-3D Generation)**\n    *   针对现有基于高斯溅射和分数蒸馏的 Text-to-3D 方法因 T2I 先验的视图偏差导致的多面 Janus 问题，提出 ConsDreamer 框架。通过视图解耦模块 (VDM) 消除条件提示中的视点偏差，并引入基于相似性的部分排序损失来强制执行无条件项中的几何一致性，有效缓解了多面问题，提高了生成质量和一致性。\n\n23. **OmniCam：通过相机控制实现统一的多模态视频生成 (OmniCam: Unified Multimodal Video Generation via Camera Control)**\n    *   提出 OmniCam，一个统一的多模态相机控制框架。利用 LLM 和视频扩散模型，支持文本/视频+轨迹或图像/视频+轨迹等多种输入组合，实现对相机运动的精确控制，生成时空一致的视频。同时引入了包含大量高质量长序列轨迹、视频和描述的 OmniTr 数据集。\n\n24. **利用静态关系进行视频问答中的类型内和类型间消息传递 (Leveraging Static Relationships for Intra-Type and Inter-Type Message Passing in Video Question Answering)**\n    *   提出一种基于静态关系的视频问答推理方法。构建双图进行类型内消息传递推理，构建基于静态关系的异构图进行类型间消息传递推理，分别捕获与问题相关的目标和关系的邻域信息，最终结合两种线索推断答案。\n\n25. **EvMic：基于有效时空建模的事件相机非接触声音恢复 (EvMic: Event-based Non-contact sound recovery from effective spatial-temporal modeling)**\n    *   提出一种利用事件相机进行非接触式声音恢复的新流程。设计了一个利用事件稀疏性捕获空间信息、使用 Mamba 建模长期时间信息、并通过空间聚合块提升信号质量的网络。同时设计了激光矩阵成像系统以增强梯度，并在合成和真实数据上验证了方法的有效性。\n\n---\n\n**机器人与强化学习**\n\n26. **统一世界模型：耦合视频和动作扩散以在大型机器人数据集上进行预训练 (Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets)**\n    *   提出统一世界模型 (UWM) 框架，旨在利用大量无动作标注的视频数据和有标注的专家演示数据进行策略学习。UWM 在统一的 Transformer 架构内集成了动作扩散和视频扩散过程，通过控制各自的扩散时间步，可以灵活地表示策略、前向/反向动力学模型和视频生成器。实验表明 UWM 能有效利用异构数据进行预训练，提升策略的泛化性和鲁棒性。\n\n27. **通过操作员模仿实现自主式人机交互 (Autonomous Human-Robot Interaction via Operator Imitation)**\n    *   提出通过模仿专家操作员数据来训练模型，以创建能够进行自主交互的机器人。模型在一个包含人机交互（操作员指令、人与机器人姿态）的数据集上训练，学习预测连续的操作员指令（通过扩散过程）和离散指令（通过分类器）。在模拟和真实系统上的用户研究表明，该方法实现了与专家操作相当的自主交互，并能零样本迁移到不同机器人平台。\n\n28. **使用潜在状态动力学残差调整世界模型 (Adapting World Models with Latent-State Dynamics Residuals)**\n    *   为解决模拟到现实强化学习中的动力学差异问题，提出 ReDRAW 方法。该方法在模拟环境中预训练一个潜在状态自回归世界模型，然后通过学习潜在状态动力学的残差（而非显式观测状态的残差）来校准到目标环境。利用调整后的世界模型，智能体可以在修正后的动力学下进行想象 rollout 优化。在视觉 MuJoCo 和物理机器人视觉循迹任务中验证了有效性。\n\n29. **用于非内聚目标多智能体牧羊控制的分层策略梯度强化学习 (Hierarchical Policy-Gradient Reinforcement Learning for Multi-Agent Shepherding Control of Non-Cohesive Targets)**\n    *   提出一种基于策略梯度方法（PPO）的去中心化强化学习方案，用于多智能体对非内聚目标（如羊群）的牧羊控制。该架构将目标选择与目标驱动相结合，克服了以往 DQN 方法的离散动作限制，实现了更平滑的智能体轨迹。\n\n30. **SymDQN：神经网络强化学习中的符号知识与推理 (SymDQN: Symbolic Knowledge and Reasoning in Neural Network-based Reinforcement Learning)**\n    *   提出 SymDQN 架构，将基于逻辑张量网络 (LTN) 的神经符号模块添加到 Dueling DQN 中，以实现符号控制和引导。这些模块用于指导动作策略学习，使智能体能展现出基于环境推理的行为。在网格世界导航任务中，实验证明该架构显著提高了学习性能和精度。\n\n---\n\n**理论、鲁棒性与可解释性**\n\n31. **稀疏点云人体活动识别的多头自适应图卷积网络 (Multi-Head Adaptive Graph Convolution Network for Sparse Point Cloud-Based Human Activity Recognition)**\n    *   针对毫米波雷达点云稀疏、噪声大且现有图方法多依赖固定核的问题，提出多头自适应核 (MAK) 模块。MAK 能为每个局部邻域生成多个动态卷积核，捕获局部特征空间的不同方面，从而适应点云数据的局部几何变化。在基准数据集上取得了 SOTA 性能。\n\n32. **STOOD-X 方法论：使用统计非参数检验进行 OOD 检测，并结合可解释性增强大规模数据集 (STOOD-X methodology: using statistical nonparametric test for OOD Detection Large-Scale datasets enhanced with explainability)**\n    *   提出 STOOD-X，一种结合统计非参数检验和可解释性的两阶段 OOD 检测方法。第一阶段使用特征空间距离和 Wilcoxon-Mann-Whitney 检验识别 OOD 样本，无需分布假设。第二阶段生成基于概念的视觉解释，揭示决策依据。实验证明其在复杂高维设置下性能优越，并能通过解释性促进人机协作。\n\n33. **通过不确定性量化改进分子性质预测的反事实真实性 (Improving Counterfactual Truthfulness for Molecular Property Prediction through Uncertainty Quantification)**\n    *   在分子性质预测中，反事实解释旨在揭示输入分子结构的微小扰动如何影响预测结果。为确保这些解释反映真实的潜在性质（定义为反事实真实性），本文提出整合不确定性估计技术（如集成、均值方差估计）来过滤掉预测不确定性高的反事实候选项。实验证明该方法能显著降低预测误差，提高反事实真实性，尤其在 OOD 设置下。\n\n34. **用于多集和度量的傅里叶切片-瓦瑟斯坦嵌入 (Fourier Sliced-Wasserstein Embedding for Multisets and Measures)**\n    *   提出傅里叶切片-瓦瑟斯坦 (FSW) 嵌入，一种将 $\\mathbb{R}^d$ 上的多集和度量嵌入到欧氏空间的新方法。该嵌入近似保留了分布上的切片瓦瑟斯坦距离，产生几何上有意义的表示。它在度量上是单射的，在多集上是双李普希茨的，优于基于 sum/max pooling 的方法。 (ICLR 2025)\n\n35. **知识图谱补全的混合几何张量分解 (Knowledge Graph Completion with Mixed Geometry Tensor Factorization)**\n    *   提出一种新的知识图谱补全几何方法，通过低秩张量近似实现。该方法在基于 Tucker 张量分解的成熟欧氏模型基础上，增加了一个新的双曲交互项，以更细致地捕捉真实世界知识图谱的数据分布特性。结合两种几何提高了模型表达能力，以更少参数达到了 SOTA 的链接预测精度。 (AISTATS 2025)\n\n36. **深度学习中的推理不一致性及其缓解方法 (Reasoning Inconsistencies and How to Mitigate Them in Deep Learning)**\n    *   这篇博士论文探讨了深度学习模型中系统性的推理不一致或错误模式。论文贡献了检测和量化自然语言/图像处理模型中不一致性的技术，提出了缓解训练数据偏差的数据高效采样和合成数据生成方法，并优化了模型在复杂推理任务中的性能，旨在提高模型的鲁棒性、公平性和可解释性。\n\n37. **FT-Transformer：具有端到端容错注意力的弹性和可靠 Transformer (FT-Transformer: Resilient and Reliable Transformer with End-to-End Fault Tolerant Attention)**\n    *   提出一种新颖的 Transformer 模型错误弹性框架，集成了端到端容错注意力 (EFTA) 以提高推理对软错误的可靠性。该方法在完全融合的注意力核内实现错误检测和纠正，减少冗余数据访问。结合架构感知的 ABFT、选择性神经元值限制和统一验证等技术，显著降低了容错开销并提高了速度。\n\n38. **ESC：用于知识删除的擦除空间概念 (ESC: Erasing Space Concept for Knowledge Deletion)**\n    *   针对现有模型遗忘方法未能满足用户完全擦除知识的需求，且存在特征泄露风险的问题，提出了知识删除 (Knowledge Deletion, KD) 的新概念和评估指标 KR。提出无训练的擦除方法 ESC，通过消除特征中相关激活来限制遗忘知识的重要子空间。同时提出带训练的 ESC-T，使用可学习掩码平衡遗忘和保留。实验证明方法高效且达到 SOTA。 (CVPR 2025)\n\n---\n\n**其他应用与杂项**\n\n39. **RBR4DNN：神经网络的基于需求的测试 (Requirements-based Testing of Neural Networks)**\n    *   提出一种基于需求的 DNN 测试套件生成方法。使用结构化自然语言需求（定义在语义特征空间中）来提示文本条件潜在扩散模型生成满足前提条件的测试用例，并使用后置条件定义测试预言机。在多个数据集上验证了生成测试的真实性、多样性和揭示故障的能力。\n\n40. **SCMPPI：用于预测蛋白质-蛋白质相互作用的监督对比多模态框架 (SCMPPI: Supervised Contrastive Multimodal Framework for Predicting Protein-Protein Interactions)**\n    *   提出 SCMPPI 框架，整合蛋白质序列特征和 PPI 网络拓扑信息，并结合改进的监督对比学习策略（引入负样本过滤和修改损失函数）来预测 PPI。在多个基准数据集和跨物种预测中表现优于 SOTA 方法。\n\n41. **面向未见视图泛化的时序动作分割 (Towards Generalizing Temporal Action Segmentation to Unseen Views)**\n    *   定义了一个未见视图动作分割协议，并在 Assembly101 等数据集上提出一种方法。该方法利用序列级和段级的共享表示，并引入序列损失和动作损失，以促进跨视图的一致视频和动作表示，显著提高了在未见视图（包括跨中心视角和自我中心视角）上的性能。\n\n42. **用于双峰单光子激光雷达成像的图注意力驱动贝叶斯深度展开 (Graph Attention-Driven Bayesian Deep Unrolling for Dual-Peak Single-Photon Lidar Imaging)**\n    *   提出一种用于处理每个像素有多个目标（双峰）的单光子 Lidar 成像的深度展开算法。引入了多目标的分层贝叶斯模型，并设计了一个展开底层统计方法的神经网络。采用双深度图表示，并利用几何深度学习从点云中提取特征，实现了高精度和不确定性量化。\n\n43. **用于垂直联邦学习的基于树的模型：综述 (Tree-based Models for Vertical Federated Learning: A Survey)**\n    *   这篇综述文章全面梳理了垂直联邦学习 (VFL) 场景下基于树的模型。将模型分为特征聚集型和标签分散型两类，详细讨论了它们的特点、优势、隐私保护机制和应用，并总结了实现原则和进行了实验比较。 (ACM Computing Surveys)\n\n44. **用于 VFSS 分割的基于蒸馏的大型视觉编码器聚合 (Agglomerating Large Vision Encoders via Distillation for VFSS Segmentation)**\n    *   为平衡医学图像分割中基础模型的性能与复杂度，提出一种新框架。通过知识蒸馏，将来自多个大型医学基础模型（如 MedSAM, RAD-DINO, MedCLIP）的知识聚合到一个低复杂度模型中，有效提升了其在 12 个分割任务上的泛化性能。\n\n45. **利用人工智能将胎儿脑部超声图像转换为伪 MRI 图像 (Translation of Fetal Brain Ultrasound Images into Pseudo-MRI Images using Artificial Intelligence)**\n    *   利用基于扩散模型的 AI 方法 (DDIC)，将信息有限的胎儿脑部超声图像转换为类似 MRI 的高质量图像，以改善组织解剖结构的可视化，特别是在脑室和 Sylvian 裂等区域。定量指标和医学专家评估均显示出显著改进。\n\n---\n\n今天的快报就到这里，希望能帮助你快速了解 arXiv 的最新动态！",
  "papers": [
    {
      "arxiv_id": "2504.02828v1",
      "title": "Concept Lancet: Image Editing with Compositional Representation Transplant",
      "title_zh": "概念柳叶刀：基于组合表示移植的图像编辑",
      "authors": [
        "Jinqi Luo",
        "Tianjiao Ding",
        "Kwan Ho Ryan Chan",
        "Hancheng Min",
        "Chris Callison-Burch",
        "René Vidal"
      ],
      "abstract": "Diffusion models are widely used for image editing tasks. Existing editing\nmethods often design a representation manipulation procedure by curating an\nedit direction in the text embedding or score space. However, such a procedure\nfaces a key challenge: overestimating the edit strength harms visual\nconsistency while underestimating it fails the editing task. Notably, each\nsource image may require a different editing strength, and it is costly to\nsearch for an appropriate strength via trial-and-error. To address this\nchallenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play\nframework for principled representation manipulation in diffusion-based image\nediting. At inference time, we decompose the source input in the latent (text\nembedding or diffusion score) space as a sparse linear combination of the\nrepresentations of the collected visual concepts. This allows us to accurately\nestimate the presence of concepts in each image, which informs the edit. Based\non the editing task (replace/add/remove), we perform a customized concept\ntransplant process to impose the corresponding editing direction. To\nsufficiently model the concept space, we curate a conceptual representation\ndataset, CoLan-150K, which contains diverse descriptions and scenarios of\nvisual terms and phrases for the latent dictionary. Experiments on multiple\ndiffusion-based image editing baselines show that methods equipped with CoLan\nachieve state-of-the-art performance in editing effectiveness and consistency\npreservation.",
      "tldr_zh": "本文提出了一种名为Concept Lancet (CoLan) 的零样本即插即用框架，用于扩散模型图像编辑中的表示操作。CoLan通过将潜在空间中的源图像分解为视觉概念表示的稀疏线性组合，从而准确估计每个图像中概念的存在情况，并以此指导编辑强度。根据编辑任务（替换/添加/删除），CoLan执行定制的概念移植过程来施加相应的编辑方向。为了充分建模概念空间，作者构建了一个包含15万个视觉术语和短语描述及场景的概念表示数据集CoLan-150K。实验表明，配备CoLan的方法在编辑效果和一致性保持方面均达到了最先进的性能。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted in CVPR 2025. Project page at\n  https://peterljq.github.io/project/colan",
      "pdf_url": "http://arxiv.org/pdf/2504.02828v1",
      "published_date": "2025-04-03 17:59:58 UTC",
      "updated_date": "2025-04-03 17:59:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:01:21.893138"
    },
    {
      "arxiv_id": "2504.02827v1",
      "title": "On Vanishing Variance in Transformer Length Generalization",
      "title_zh": "关于 Transformer 长度泛化中的方差消失问题\n",
      "authors": [
        "Ruining Li",
        "Gabrijel Boduljak",
        "Jensen",
        "Zhou"
      ],
      "abstract": "It is a widely known issue that Transformers, when trained on shorter\nsequences, fail to generalize robustly to longer ones at test time. This raises\nthe question of whether Transformer models are real reasoning engines, despite\ntheir impressive abilities in mathematical problem solving and code synthesis.\nIn this paper, we offer a vanishing variance perspective on this issue. To the\nbest of our knowledge, we are the first to demonstrate that even for today's\nfrontier models, a longer sequence length results in a decrease in variance in\nthe output of the multi-head attention modules. On the argmax retrieval and\ndictionary lookup tasks, our experiments show that applying layer normalization\nafter the attention outputs leads to significantly better length\ngeneralization. Our analyses attribute this improvement to a reduction-though\nnot a complete elimination-of the distribution shift caused by vanishing\nvariance.",
      "tldr_zh": "该论文研究了Transformer模型在长度泛化上的问题，即在短序列上训练的Transformer在测试时难以泛化到长序列。研究发现，长序列会导致multi-head attention模块输出的方差减小（vanishing variance）。通过argmax检索和字典查找任务的实验表明，在attention输出后应用Layer Normalization可以显著改善长度泛化能力。分析表明，这种改进归因于Layer Normalization减少了由vanishing variance引起的分布偏移，但并未完全消除。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Project page: https://ruiningli.com/vanishing-variance. The first two\n  authors contributed equally to this work",
      "pdf_url": "http://arxiv.org/pdf/2504.02827v1",
      "published_date": "2025-04-03 17:59:56 UTC",
      "updated_date": "2025-04-03 17:59:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:01:33.634681"
    },
    {
      "arxiv_id": "2504.02822v1",
      "title": "Do Two AI Scientists Agree?",
      "title_zh": "两个AI科学家会达成一致吗？\n",
      "authors": [
        "Xinghong Fu",
        "Ziming Liu",
        "Max Tegmark"
      ],
      "abstract": "When two AI models are trained on the same scientific task, do they learn the\nsame theory or two different theories? Throughout history of science, we have\nwitnessed the rise and fall of theories driven by experimental validation or\nfalsification: many theories may co-exist when experimental data is lacking,\nbut the space of survived theories become more constrained with more\nexperimental data becoming available. We show the same story is true for AI\nscientists. With increasingly more systems provided in training data, AI\nscientists tend to converge in the theories they learned, although sometimes\nthey form distinct groups corresponding to different theories. To\nmechanistically interpret what theories AI scientists learn and quantify their\nagreement, we propose MASS, Hamiltonian-Lagrangian neural networks as AI\nScientists, trained on standard problems in physics, aggregating training\nresults across many seeds simulating the different configurations of AI\nscientists. Our findings suggests for AI scientists switch from learning a\nHamiltonian theory in simple setups to a Lagrangian formulation when more\ncomplex systems are introduced. We also observe strong seed dependence of the\ntraining dynamics and final learned weights, controlling the rise and fall of\nrelevant theories. We finally demonstrate that not only can our neural networks\naid interpretability, it can also be applied to higher dimensional problems.",
      "tldr_zh": "该研究探讨了在相同科学任务上训练的两个AI模型是否会学习到相同的理论。研究人员使用 Hamiltonian-Lagrangian 神经网络（MASS）作为AI科学家，在物理学标准问题上进行训练，模拟不同配置的AI科学家。结果表明，随着训练数据中系统数量的增加，AI科学家倾向于收敛到相同的理论，但也可能形成对应于不同理论的不同群体。研究发现，AI科学家从学习哈密顿理论转向学习拉格朗日公式，并且训练动态和最终学习权重对种子具有很强的依赖性，从而控制了相关理论的兴衰。该研究证明了神经网络不仅可以帮助解释性，还可以应用于更高维度的问题。\n",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02822v1",
      "published_date": "2025-04-03 17:58:44 UTC",
      "updated_date": "2025-04-03 17:58:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:01:45.877553"
    },
    {
      "arxiv_id": "2504.02821v1",
      "title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models",
      "title_zh": "稀疏自编码器在视觉-语言模型中学习单义特征\n",
      "authors": [
        "Mateusz Pach",
        "Shyamgopal Karthik",
        "Quentin Bouniot",
        "Serge Belongie",
        "Zeynep Akata"
      ],
      "abstract": "Sparse Autoencoders (SAEs) have recently been shown to enhance\ninterpretability and steerability in Large Language Models (LLMs). In this\nwork, we extend the application of SAEs to Vision-Language Models (VLMs), such\nas CLIP, and introduce a comprehensive framework for evaluating monosemanticity\nin vision representations. Our experimental results reveal that SAEs trained on\nVLMs significantly enhance the monosemanticity of individual neurons while also\nexhibiting hierarchical representations that align well with expert-defined\nstructures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that\napplying SAEs to intervene on a CLIP vision encoder, directly steer output from\nmultimodal LLMs (e.g., LLaVA) without any modifications to the underlying\nmodel. These findings emphasize the practicality and efficacy of SAEs as an\nunsupervised approach for enhancing both the interpretability and control of\nVLMs.",
      "tldr_zh": "该研究探索了稀疏自编码器(Sparse Autoencoders, SAEs)在视觉-语言模型(Vision-Language Models, VLMs)中的应用，并提出了一个评估视觉表征中单义性(monosemanticity)的框架。实验结果表明，在VLMs上训练的SAEs显著增强了神经元的单义性，并展现出与专家定义的结构（如iNaturalist分类）良好对齐的层次化表征。更重要的是，通过应用SAEs干预CLIP视觉编码器，可以直接引导多模态LLMs（如LLaVA）的输出，而无需修改底层模型。这些发现强调了SAEs作为一种无监督方法在增强VLMs的可解释性和可控性方面的实用性和有效性。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint. The code is available at\n  https://github.com/ExplainableML/sae-for-vlm",
      "pdf_url": "http://arxiv.org/pdf/2504.02821v1",
      "published_date": "2025-04-03 17:58:35 UTC",
      "updated_date": "2025-04-03 17:58:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:01:57.897163"
    },
    {
      "arxiv_id": "2504.02819v1",
      "title": "GMR-Conv: An Efficient Rotation and Reflection Equivariant Convolution Kernel Using Gaussian Mixture Rings",
      "title_zh": "GMR-Conv：一种使用高斯混合环的高效旋转和反射等变卷积核\n",
      "authors": [
        "Yuexi Du",
        "Jiazhen Zhang",
        "Nicha C. Dvornek",
        "John A. Onofrey"
      ],
      "abstract": "Symmetry, where certain features remain invariant under geometric\ntransformations, can often serve as a powerful prior in designing convolutional\nneural networks (CNNs). While conventional CNNs inherently support\ntranslational equivariance, extending this property to rotation and reflection\nhas proven challenging, often forcing a compromise between equivariance,\nefficiency, and information loss. In this work, we introduce Gaussian Mixture\nRing Convolution (GMR-Conv), an efficient convolution kernel that smooths\nradial symmetry using a mixture of Gaussian-weighted rings. This design\nmitigates discretization errors of circular kernels, thereby preserving robust\nrotation and reflection equivariance without incurring computational overhead.\nWe further optimize both the space and speed efficiency of GMR-Conv via a novel\nparameterization and computation strategy, allowing larger kernels at an\nacceptable cost. Extensive experiments on eight classification and one\nsegmentation datasets demonstrate that GMR-Conv not only matches conventional\nCNNs' performance but can also surpass it in applications with orientation-less\ndata. GMR-Conv is also proven to be more robust and efficient than the\nstate-of-the-art equivariant learning methods. Our work provides inspiring\nempirical evidence that carefully applied radial symmetry can alleviate the\nchallenges of information loss, marking a promising advance in equivariant\nnetwork architectures. The code is available at\nhttps://github.com/XYPB/GMR-Conv.",
      "tldr_zh": "该论文提出了一种高效的旋转和反射等变卷积核GMR-Conv（Gaussian Mixture Ring Convolution），它利用高斯混合环来平滑径向对称性。GMR-Conv通过高斯加权环的混合来减轻圆形内核的离散化误差，从而在不增加计算开销的情况下保持鲁棒的旋转和反射等变性。此外，论文还通过一种新颖的参数化和计算策略优化了GMR-Conv的空间和速度效率，使其能够在可接受的成本下使用更大的内核。在多个分类和分割数据集上的实验表明，GMR-Conv不仅能达到传统CNN的性能，而且在具有无方向数据的应用中能够超越它，同时比最先进的等变学习方法更鲁棒和高效。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV",
        "eess.SP"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02819v1",
      "published_date": "2025-04-03 17:58:18 UTC",
      "updated_date": "2025-04-03 17:58:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:02:09.949933"
    },
    {
      "arxiv_id": "2504.02810v1",
      "title": "Generative Evaluation of Complex Reasoning in Large Language Models",
      "title_zh": "大型语言模型中复杂推理的生成式评估\n",
      "authors": [
        "Haowei Lin",
        "Xiangyu Wang",
        "Ruilin Yan",
        "Baizhou Huang",
        "Haotian Ye",
        "Jianhua Zhu",
        "Zihao Wang",
        "James Zou",
        "Jianzhu Ma",
        "Yitao Liang"
      ],
      "abstract": "With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities.",
      "tldr_zh": "为了评估大型语言模型(LLMs)是否真正具备推理能力而非仅仅记忆训练数据，该论文提出了一个生成式评估框架KUMO。KUMO结合了LLMs和符号引擎，动态生成多样化的、多轮的、部分可观察且难度可调的推理任务。通过自动化流程，KUMO能够持续生成开放领域的全新任务，迫使模型展示真正的泛化能力。研究者使用KUMO在100个领域生成了5000个任务，评估了23个先进的LLMs，并与大学生进行了对比。结果表明，许多LLMs在简单的推理任务上超过了大学生的水平，而经过推理缩放的LLMs在复杂的推理挑战中达到了大学生的水平。此外，LLM在KUMO任务上的表现与新发布的真实世界推理基准测试结果高度相关，证明了KUMO作为评估LLM真实推理能力的强大工具的价值。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02810v1",
      "published_date": "2025-04-03 17:54:18 UTC",
      "updated_date": "2025-04-03 17:54:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:02:22.173877"
    },
    {
      "arxiv_id": "2504.02807v1",
      "title": "MegaMath: Pushing the Limits of Open Math Corpora",
      "title_zh": "MegaMath：突破开放数学语料库的极限\n",
      "authors": [
        "Fan Zhou",
        "Zengzhi Wang",
        "Nikhil Ranjan",
        "Zhoujun Cheng",
        "Liping Tang",
        "Guowei He",
        "Zhengzhong Liu",
        "Eric P. Xing"
      ],
      "abstract": "Mathematical reasoning is a cornerstone of human intelligence and a key\nbenchmark for advanced capabilities in large language models (LLMs). However,\nthe research community still lacks an open, large-scale, high-quality corpus\ntailored to the demands of math-centric LLM pre-training. We present MegaMath,\nan open dataset curated from diverse, math-focused sources through following\npractices: (1) Revisiting web data: We re-extracted mathematical documents from\nCommon Crawl with math-oriented HTML optimizations, fasttext-based filtering\nand deduplication, all for acquiring higher-quality data on the Internet. (2)\nRecalling Math-related code data: We identified high quality math-related code\nfrom large code training corpus, Stack-V2, further enhancing data diversity.\n(3) Exploring Synthetic data: We synthesized QA-style text, math-related code,\nand interleaved text-code blocks from web data or code data. By integrating\nthese strategies and validating their effectiveness through extensive\nablations, MegaMath delivers 371B tokens with the largest quantity and top\nquality among existing open math pre-training datasets.",
      "tldr_zh": "MegaMath 旨在构建一个大规模、高质量的开放数学语料库，以推动大型语言模型 (LLMs) 在数学推理方面的能力。该数据集通过三种策略收集数据：(1) 重新提取 Common Crawl 中的数学文档，并进行优化和过滤；(2) 从 Stack-V2 中识别高质量的数学相关代码；(3) 合成 QA 风格的文本、数学相关代码以及文本-代码混合块。MegaMath 最终包含 371B tokens，是目前开放数学预训练数据集中规模最大、质量最高的数据集。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "26 pages, 15 figures, 22 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.02807v1",
      "published_date": "2025-04-03 17:52:07 UTC",
      "updated_date": "2025-04-03 17:52:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:02:33.804834"
    },
    {
      "arxiv_id": "2504.02799v1",
      "title": "Systematic Evaluation of Large Vision-Language Models for Surgical Artificial Intelligence",
      "title_zh": "大型视觉语言模型在手术人工智能中的系统性评估\n",
      "authors": [
        "Anita Rau",
        "Mark Endo",
        "Josiah Aklilu",
        "Jaewoo Heo",
        "Khaled Saab",
        "Alberto Paderno",
        "Jeffrey Jopling",
        "F. Christopher Holsinger",
        "Serena Yeung-Levy"
      ],
      "abstract": "Large Vision-Language Models offer a new paradigm for AI-driven image\nunderstanding, enabling models to perform tasks without task-specific training.\nThis flexibility holds particular promise across medicine, where\nexpert-annotated data is scarce. Yet, VLMs' practical utility in\nintervention-focused domains--especially surgery, where decision-making is\nsubjective and clinical scenarios are variable--remains uncertain. Here, we\npresent a comprehensive analysis of 11 state-of-the-art VLMs across 17 key\nvisual understanding tasks in surgical AI--from anatomy recognition to skill\nassessment--using 13 datasets spanning laparoscopic, robotic, and open\nprocedures. In our experiments, VLMs demonstrate promising generalizability, at\ntimes outperforming supervised models when deployed outside their training\nsetting. In-context learning, incorporating examples during testing, boosted\nperformance up to three-fold, suggesting adaptability as a key strength. Still,\ntasks requiring spatial or temporal reasoning remained difficult. Beyond\nsurgery, our findings offer insights into VLMs' potential for tackling complex\nand dynamic scenarios in clinical and broader real-world applications.",
      "tldr_zh": "本文系统评估了11个先进的视觉语言模型(VLMs)在手术人工智能领域的应用潜力。研究涵盖了17项关键的视觉理解任务，使用了13个包含腹腔镜、机器人和开放手术的数据集，任务范围从解剖结构识别到技能评估。实验结果表明，VLMs展现出良好的泛化能力，在非训练环境下甚至优于监督模型。通过上下文学习(in-context learning)，模型性能提升高达三倍，但对于需要空间或时间推理的任务仍然具有挑战性。该研究揭示了VLMs在复杂和动态的临床场景中的应用前景。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02799v1",
      "published_date": "2025-04-03 17:42:56 UTC",
      "updated_date": "2025-04-03 17:42:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:02:45.959847"
    },
    {
      "arxiv_id": "2504.02793v1",
      "title": "A Framework for Situating Innovations, Opportunities, and Challenges in Advancing Vertical Systems with Large AI Models",
      "title_zh": "一个利用大型AI模型推进垂直系统，以定位创新、机遇和挑战的框架\n",
      "authors": [
        "Gaurav Verma",
        "Jiawei Zhou",
        "Mohit Chandra",
        "Srijan Kumar",
        "Munmun De Choudhury"
      ],
      "abstract": "Large artificial intelligence (AI) models have garnered significant attention\nfor their remarkable, often \"superhuman\", performance on standardized\nbenchmarks. However, when these models are deployed in high-stakes verticals\nsuch as healthcare, education, and law, they often reveal notable limitations.\nFor instance, they exhibit brittleness to minor variations in input data,\npresent contextually uninformed decisions in critical settings, and undermine\nuser trust by confidently producing or reproducing inaccuracies. These\nchallenges in applying large models necessitate cross-disciplinary innovations\nto align the models' capabilities with the needs of real-world applications. We\nintroduce a framework that addresses this gap through a layer-wise abstraction\nof innovations aimed at meeting users' requirements with large models. Through\nmultiple case studies, we illustrate how researchers and practitioners across\nvarious fields can operationalize this framework. Beyond modularizing the\npipeline of transforming large models into useful \"vertical systems\", we also\nhighlight the dynamism that exists within different layers of the framework.\nFinally, we discuss how our framework can guide researchers and practitioners\nto (i) optimally situate their innovations (e.g., when vertical-specific\ninsights can empower broadly impactful vertical-agnostic innovations), (ii)\nuncover overlooked opportunities (e.g., spotting recurring problems across\nverticals to develop practically useful foundation models instead of chasing\nbenchmarks), and (iii) facilitate cross-disciplinary communication of critical\nchallenges (e.g., enabling a shared vocabulary for AI developers, domain\nexperts, and human-computer interaction scholars).",
      "tldr_zh": "该论文提出了一个框架，用于解决大型AI模型在医疗、教育和法律等高风险垂直领域应用时面临的挑战，例如对输入数据微小变化的脆弱性、情境理解不足以及产生不准确信息等问题。该框架通过分层抽象的方式，将创新点与用户需求对齐，旨在将大型模型转化为有用的“垂直系统”。通过案例研究，论文展示了研究人员和实践者如何利用该框架，并强调了框架内不同层级的动态性。此外，该框架还旨在帮助研究人员和实践者定位创新点、发现潜在机遇（例如，开发通用的基础模型）并促进跨学科交流，从而解决AI开发者、领域专家和人机交互学者之间的沟通障碍。\n",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "pre-print; 7 pages of main content, 1 figure, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2504.02793v1",
      "published_date": "2025-04-03 17:40:11 UTC",
      "updated_date": "2025-04-03 17:40:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:02:57.983290"
    },
    {
      "arxiv_id": "2504.02792v1",
      "title": "Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets",
      "title_zh": "统一世界模型：耦合视频和动作扩散，用于在大型机器人数据集上进行预训练\n",
      "authors": [
        "Chuning Zhu",
        "Raymond Yu",
        "Siyuan Feng",
        "Benjamin Burchfiel",
        "Paarth Shah",
        "Abhishek Gupta"
      ],
      "abstract": "Imitation learning has emerged as a promising approach towards building\ngeneralist robots. However, scaling imitation learning for large robot\nfoundation models remains challenging due to its reliance on high-quality\nexpert demonstrations. Meanwhile, large amounts of video data depicting a wide\nrange of environments and diverse behaviors are readily available. This data\nprovides a rich source of information about real-world dynamics and\nagent-environment interactions. Leveraging this data directly for imitation\nlearning, however, has proven difficult due to the lack of action annotation\nrequired for most contemporary methods. In this work, we present Unified World\nModels (UWM), a framework that allows for leveraging both video and action data\nfor policy learning. Specifically, a UWM integrates an action diffusion process\nand a video diffusion process within a unified transformer architecture, where\nindependent diffusion timesteps govern each modality. We show that by simply\ncontrolling each diffusion timestep, UWM can flexibly represent a policy, a\nforward dynamics, an inverse dynamics, and a video generator. Through simulated\nand real-world experiments, we show that: (1) UWM enables effective pretraining\non large-scale multitask robot datasets with both dynamics and action\npredictions, resulting in more generalizable and robust policies than imitation\nlearning, (2) UWM naturally facilitates learning from action-free video data\nthrough independent control of modality-specific diffusion timesteps, further\nimproving the performance of finetuned policies. Our results suggest that UWM\noffers a promising step toward harnessing large, heterogeneous datasets for\nscalable robot learning, and provides a simple unification between the often\ndisparate paradigms of imitation learning and world modeling. Videos and code\nare available at https://weirdlabuw.github.io/uwm/.",
      "tldr_zh": "该论文提出了统一世界模型(Unified World Models, UWM)，一个利用视频和动作数据进行机器人策略学习的框架。UWM集成了动作扩散过程和视频扩散过程，通过统一的Transformer架构，使用独立的扩散时间步长控制每个模态。UWM可以灵活地表示策略、正向动力学、逆向动力学和视频生成器。实验表明，UWM可以通过动力学和动作预测，在大规模多任务机器人数据集上进行有效的预训练，从而产生比模仿学习更具泛化性和鲁棒性的策略。此外，UWM可以通过独立控制特定模态的扩散时间步长，自然地从无动作视频数据中学习，进一步提高微调策略的性能。UWM为可扩展的机器人学习提供了一个有希望的步骤，并简化了模仿学习和世界建模这两种通常不同的范例之间的统一。\n",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02792v1",
      "published_date": "2025-04-03 17:38:59 UTC",
      "updated_date": "2025-04-03 17:38:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:03:10.084567"
    },
    {
      "arxiv_id": "2504.02781v1",
      "title": "Towards Green AI-Native Networks: Evaluation of Neural Circuit Policy for Estimating Energy Consumption of Base Stations",
      "title_zh": "迈向绿色 AI 原生网络：用于评估基站能耗的神经回路策略\n",
      "authors": [
        "Selim Ickin",
        "Shruti Bothe",
        "Aman Raparia",
        "Nitin Khanna",
        "Erik Sanders"
      ],
      "abstract": "Optimization of radio hardware and AI-based network management software yield\nsignificant energy savings in radio access networks. The execution of\nunderlying Machine Learning (ML) models, which enable energy savings through\nrecommended actions, may require additional compute and energy, highlighting\nthe opportunity to explore and adopt accurate and energy-efficient ML\ntechnologies. This work evaluates the novel use of sparsely structured Neural\nCircuit Policies (NCPs) in a use case to estimate the energy consumption of\nbase stations. Sparsity in ML models yields reduced memory, computation and\nenergy demand, hence facilitating a low-cost and scalable solution. We also\nevaluate the generalization capability of NCPs in comparison to traditional and\nwidely used ML models such as Long Short Term Memory (LSTM), via quantifying\ntheir sensitivity to varying model hyper-parameters (HPs). NCPs demonstrated a\nclear reduction in computational overhead and energy consumption. Moreover,\nresults indicated that the NCPs are robust to varying HPs such as number of\nepochs and neurons in each layer, making them a suitable option to ease model\nmanagement and to reduce energy consumption in Machine Learning Operations\n(MLOps) in telecommunications.",
      "tldr_zh": "该研究探索了在无线接入网络中利用稀疏结构的神经电路策略(Neural Circuit Policies, NCPs)来评估基站能耗，旨在实现绿色AI原生网络。相比传统的长短期记忆网络(LSTM)，NCPs通过模型稀疏性降低了内存、计算和能源需求，从而实现低成本和可扩展的解决方案。实验结果表明，NCPs在计算开销和能源消耗方面表现出明显的优势，并且对超参数（如epoch数和每层神经元数）的变化具有鲁棒性。这使得NCPs成为电信领域机器学习运维(MLOps)中降低能源消耗和简化模型管理的理想选择。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.02781v1",
      "published_date": "2025-04-03 17:22:39 UTC",
      "updated_date": "2025-04-03 17:22:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:03:21.989031"
    },
    {
      "arxiv_id": "2504.02780v1",
      "title": "From Consumption to Collaboration: Measuring Interaction Patterns to Augment Human Cognition in Open-Ended Tasks",
      "title_zh": "从消费到协作：衡量交互模式以增强开放式任务中的人类认知\n",
      "authors": [
        "Joshua Holstein",
        "Moritz Diener",
        "Philipp Spitzer"
      ],
      "abstract": "The rise of Generative AI, and Large Language Models (LLMs) in particular, is\nfundamentally changing cognitive processes in knowledge work, raising critical\nquestions about their impact on human reasoning and problem-solving\ncapabilities. As these AI systems become increasingly integrated into\nworkflows, they offer unprecedented opportunities for augmenting human thinking\nwhile simultaneously risking cognitive erosion through passive consumption of\ngenerated answers. This tension is particularly pronounced in open-ended tasks,\nwhere effective solutions require deep contextualization and integration of\ndomain knowledge. Unlike structured tasks with established metrics, measuring\nthe quality of human-LLM interaction in such open-ended tasks poses significant\nchallenges due to the absence of ground truth and the iterative nature of\nsolution development. To address this, we present a framework that analyzes\ninteraction patterns along two dimensions: cognitive activity mode (exploration\nvs. exploitation) and cognitive engagement mode (constructive vs. detrimental).\nThis framework provides systematic measurements to evaluate when LLMs are\neffective tools for thought rather than substitutes for human cognition,\nadvancing theoretical understanding and practical guidance for developing AI\nsystems that protect and augment human cognitive capabilities.",
      "tldr_zh": "大型语言模型(LLMs)的兴起对知识工作中的认知过程产生根本性改变。该研究提出一个框架，通过分析认知活动模式（探索 vs. 利用）和认知参与模式（建设性 vs. 有害）这两个维度上的交互模式，来评估LLMs在开放式任务中对人类认知的影响。该框架旨在区分LLMs作为有效的思考工具和作为人类认知的替代品这两种情况，从而为开发能够保护和增强人类认知能力的AI系统提供理论理解和实践指导。该研究着重解决在缺乏标准答案和解决方案迭代的开放式任务中，衡量人与LLM交互质量的挑战。\n",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted at Tools for Thought Workshop (CHI'25)",
      "pdf_url": "http://arxiv.org/pdf/2504.02780v1",
      "published_date": "2025-04-03 17:20:36 UTC",
      "updated_date": "2025-04-03 17:20:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:03:34.007592"
    },
    {
      "arxiv_id": "2504.02778v1",
      "title": "Multi-Head Adaptive Graph Convolution Network for Sparse Point Cloud-Based Human Activity Recognition",
      "title_zh": "用于基于稀疏点云的人体活动识别的多头自适应图卷积网络\n",
      "authors": [
        "Vincent Gbouna Zakka",
        "Luis J. Manso",
        "Zhuangzhuang Dai"
      ],
      "abstract": "Human activity recognition is increasingly vital for supporting independent\nliving, particularly for the elderly and those in need of assistance. Domestic\nservice robots with monitoring capabilities can enhance safety and provide\nessential support. Although image-based methods have advanced considerably in\nthe past decade, their adoption remains limited by concerns over privacy and\nsensitivity to low-light or dark conditions. As an alternative, millimetre-wave\n(mmWave) radar can produce point cloud data which is privacy-preserving.\nHowever, processing the sparse and noisy point clouds remains a long-standing\nchallenge. While graph-based methods and attention mechanisms show promise,\nthey predominantly rely on \"fixed\" kernels; kernels that are applied uniformly\nacross all neighbourhoods, highlighting the need for adaptive approaches that\ncan dynamically adjust their kernels to the specific geometry of each local\nneighbourhood in point cloud data. To overcome this limitation, we introduce an\nadaptive approach within the graph convolutional framework. Instead of a single\nshared weight function, our Multi-Head Adaptive Kernel (MAK) module generates\nmultiple dynamic kernels, each capturing different aspects of the local feature\nspace. By progressively refining local features while maintaining global\nspatial context, our method enables convolution kernels to adapt to varying\nlocal features. Experimental results on benchmark datasets confirm the\neffectiveness of our approach, achieving state-of-the-art performance in human\nactivity recognition. Our source code is made publicly available at:\nhttps://github.com/Gbouna/MAK-GCN",
      "tldr_zh": "该论文提出了一种用于稀疏点云人体行为识别的多头自适应图卷积网络(Multi-Head Adaptive Graph Convolution Network, MAK-GCN)。针对传统图卷积网络中\"固定\"卷积核无法适应点云局部几何特征的问题，MAK-GCN引入了多头自适应核(Multi-Head Adaptive Kernel, MAK)模块，该模块生成多个动态卷积核，捕捉局部特征空间的不同方面。通过在保持全局空间上下文的同时逐步细化局部特征，MAK-GCN使卷积核能够适应不同的局部特征。在基准数据集上的实验结果表明，该方法在人体行为识别方面达到了state-of-the-art的性能。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02778v1",
      "published_date": "2025-04-03 17:19:20 UTC",
      "updated_date": "2025-04-03 17:19:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:03:45.995621"
    },
    {
      "arxiv_id": "2504.02767v1",
      "title": "How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?",
      "title_zh": "大型语言模型在多大程度上内化了科学文献和引用规范？\n",
      "authors": [
        "Andres Algaba",
        "Vincent Holst",
        "Floriano Tori",
        "Melika Mobini",
        "Brecht Verbeken",
        "Sylvia Wenmackers",
        "Vincent Ginis"
      ],
      "abstract": "The spread of scientific knowledge depends on how researchers discover and\ncite previous work. The adoption of large language models (LLMs) in the\nscientific research process introduces a new layer to these citation practices.\nHowever, it remains unclear to what extent LLMs align with human citation\npractices, how they perform across domains, and may influence citation\ndynamics. Here, we show that LLMs systematically reinforce the Matthew effect\nin citations by consistently favoring highly cited papers when generating\nreferences. This pattern persists across scientific domains despite significant\nfield-specific variations in existence rates, which refer to the proportion of\ngenerated references that match existing records in external bibliometric\ndatabases. Analyzing 274,951 references generated by GPT-4o for 10,000 papers,\nwe find that LLM recommendations diverge from traditional citation patterns by\npreferring more recent references with shorter titles and fewer authors.\nEmphasizing their content-level relevance, the generated references are\nsemantically aligned with the content of each paper at levels comparable to the\nground truth references and display similar network effects while reducing\nauthor self-citations. These findings illustrate how LLMs may reshape citation\npractices and influence the trajectory of scientific discovery by reflecting\nand amplifying established trends. As LLMs become more integrated into the\nscientific research process, it is important to understand their role in\nshaping how scientific communities discover and build upon prior work.",
      "tldr_zh": "该研究探讨了大型语言模型(LLMs)在多大程度上内化了科学文献和引用实践。通过分析GPT-4o为10,000篇论文生成的274,951条参考文献，研究发现LLMs系统性地强化了引用的马太效应，倾向于推荐高被引论文。尽管不同科学领域存在显著差异，但这种模式依然存在。LLMs倾向于推荐标题较短、作者较少、更新近的参考文献。尽管如此，LLMs生成的参考文献在语义上与论文内容高度相关，并表现出类似的网络效应，同时减少了作者的自我引用。研究结果表明，LLMs可能会通过反映和放大既定趋势来重塑引用实践，并影响科学发现的轨迹。\n",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.LG",
        "cs.SI"
      ],
      "primary_category": "cs.DL",
      "comment": "32 pages, 17 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.02767v1",
      "published_date": "2025-04-03 17:04:56 UTC",
      "updated_date": "2025-04-03 17:04:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:03:58.106270"
    },
    {
      "arxiv_id": "2504.02764v1",
      "title": "Scene Splatter: Momentum 3D Scene Generation from Single Image with Video Diffusion Model",
      "title_zh": "Scene Splatter：基于视频扩散模型的单张图像动量三维场景生成\n",
      "authors": [
        "Shengjun Zhang",
        "Jinzhao Li",
        "Xin Fei",
        "Hao Liu",
        "Yueqi Duan"
      ],
      "abstract": "In this paper, we propose Scene Splatter, a momentum-based paradigm for video\ndiffusion to generate generic scenes from single image. Existing methods, which\nemploy video generation models to synthesize novel views, suffer from limited\nvideo length and scene inconsistency, leading to artifacts and distortions\nduring further reconstruction. To address this issue, we construct noisy\nsamples from original features as momentum to enhance video details and\nmaintain scene consistency. However, for latent features with the perception\nfield that spans both known and unknown regions, such latent-level momentum\nrestricts the generative ability of video diffusion in unknown regions.\nTherefore, we further introduce the aforementioned consistent video as a\npixel-level momentum to a directly generated video without momentum for better\nrecovery of unseen regions. Our cascaded momentum enables video diffusion\nmodels to generate both high-fidelity and consistent novel views. We further\nfinetune the global Gaussian representations with enhanced frames and render\nnew frames for momentum update in the next step. In this manner, we can\niteratively recover a 3D scene, avoiding the limitation of video length.\nExtensive experiments demonstrate the generalization capability and superior\nperformance of our method in high-fidelity and consistent scene generation.",
      "tldr_zh": "本文提出了一种名为Scene Splatter的基于动量的框架，利用视频扩散模型从单张图像生成通用场景。为了解决现有方法中视频长度限制和场景不一致的问题，该方法构建了基于原始特征的噪声样本作为动量，以增强视频细节并保持场景一致性。为了解决潜在特征感知范围跨越已知和未知区域的问题，该方法进一步引入一致性视频作为像素级动量，以更好地恢复未见区域。这种级联动量使得视频扩散模型能够生成高保真和一致的新视角。通过对增强帧进行微调并渲染新帧，该方法可以迭代地恢复3D场景，避免视频长度的限制。实验结果表明，该方法在高保真和一致的场景生成方面具有优越的性能和泛化能力。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.02764v1",
      "published_date": "2025-04-03 17:00:44 UTC",
      "updated_date": "2025-04-03 17:00:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:04:10.104103"
    },
    {
      "arxiv_id": "2504.02737v1",
      "title": "RBR4DNN: Requirements-based Testing of Neural Networks",
      "title_zh": "RBR4DNN：基于需求的神经网络测试\n",
      "authors": [
        "Nusrat Jahan Mozumder",
        "Felipe Toledo",
        "Swaroopa Dola",
        "Matthew B. Dwyer"
      ],
      "abstract": "Deep neural network (DNN) testing is crucial for the reliability and safety\nof critical systems, where failures can have severe consequences. Although\nvarious techniques have been developed to create robustness test suites,\nrequirements-based testing for DNNs remains largely unexplored -- yet such\ntests are recognized as an essential component of software validation of\ncritical systems. In this work, we propose a requirements-based test suite\ngeneration method that uses structured natural language requirements formulated\nin a semantic feature space to create test suites by prompting text-conditional\nlatent diffusion models with the requirement precondition and then using the\nassociated postcondition to define a test oracle to judge outputs of the DNN\nunder test. We investigate the approach using fine-tuned variants of\npre-trained generative models. Our experiments on the MNIST, CelebA-HQ,\nImageNet, and autonomous car driving datasets demonstrate that the generated\ntest suites are realistic, diverse, consistent with preconditions, and capable\nof revealing faults.",
      "tldr_zh": "本文提出了一种基于需求的神经网络测试方法RBR4DNN，旨在解决DNN在关键系统中的可靠性和安全性问题。该方法利用语义特征空间中结构化的自然语言需求，通过提示文本条件潜在扩散模型生成测试套件，并使用相关的后置条件定义测试预言来判断被测DNN的输出。实验结果表明，在MNIST、CelebA-HQ、ImageNet和自动驾驶数据集上，生成的测试套件具有真实性、多样性，与前提条件一致，并能够揭示故障。\n",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02737v1",
      "published_date": "2025-04-03 16:24:49 UTC",
      "updated_date": "2025-04-03 16:24:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:04:21.807923"
    },
    {
      "arxiv_id": "2504.02724v1",
      "title": "Autonomous Human-Robot Interaction via Operator Imitation",
      "title_zh": "通过操作员模仿实现自主的人机交互\n",
      "authors": [
        "Sammy Christen",
        "David Müller",
        "Agon Serifi",
        "Ruben Grandia",
        "Georg Wiedebach",
        "Michael A. Hopkins",
        "Espen Knoop",
        "Moritz Bächer"
      ],
      "abstract": "Teleoperated robotic characters can perform expressive interactions with\nhumans, relying on the operators' experience and social intuition. In this\nwork, we propose to create autonomous interactive robots, by training a model\nto imitate operator data. Our model is trained on a dataset of human-robot\ninteractions, where an expert operator is asked to vary the interactions and\nmood of the robot, while the operator commands as well as the pose of the human\nand robot are recorded. Our approach learns to predict continuous operator\ncommands through a diffusion process and discrete commands through a\nclassifier, all unified within a single transformer architecture. We evaluate\nthe resulting model in simulation and with a user study on the real system. We\nshow that our method enables simple autonomous human-robot interactions that\nare comparable to the expert-operator baseline, and that users can recognize\nthe different robot moods as generated by our model. Finally, we demonstrate a\nzero-shot transfer of our model onto a different robotic platform with the same\noperator interface.",
      "tldr_zh": "本文提出了一种通过模仿操作员数据实现自主人机交互的方法。该方法通过训练模型来模仿专家操作员在人机交互中的行为，操作员被要求改变机器人的交互方式和情绪，同时记录操作员的指令以及人和机器人的姿势。该模型通过扩散过程预测连续的操作员指令，并通过分类器预测离散指令，所有这些都统一在一个Transformer架构中。实验结果表明，该方法能够实现简单自主的人机交互，其性能与专家操作员的基线相当，并且用户可以识别模型生成的不同机器人情绪。此外，该模型还展示了在具有相同操作界面的不同机器人平台上的零样本迁移能力。\n",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02724v1",
      "published_date": "2025-04-03 16:06:44 UTC",
      "updated_date": "2025-04-03 16:06:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:04:34.086751"
    },
    {
      "arxiv_id": "2504.02701v1",
      "title": "Responsible Development of Offensive AI",
      "title_zh": "进攻性 AI 的责任发展\n",
      "authors": [
        "Ryan Marinelli"
      ],
      "abstract": "As AI advances, broader consensus is needed to determine research priorities.\nThis endeavor discusses offensive AI and provides guidance by leveraging\nSustainable Development Goals (SDGs) and interpretability techniques. The\nobjective is to more effectively establish priorities that balance societal\nbenefits against risks. The two forms of offensive AI evaluated in this study\nare vulnerability detection agents, which solve Capture- The-Flag challenges,\nand AI-powered malware.",
      "tldr_zh": "本文探讨了进攻性人工智能的负责任发展，旨在通过可持续发展目标(SDGs)和可解释性技术，更有效地平衡社会效益与风险。研究重点关注两种进攻性AI：漏洞检测智能体（用于解决Capture-The-Flag挑战）和AI驱动的恶意软件。通过SDGs框架，研究旨在为进攻性AI的研究重点提供指导，促进其在负责任的框架内发展。\n",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02701v1",
      "published_date": "2025-04-03 15:37:38 UTC",
      "updated_date": "2025-04-03 15:37:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:04:45.706565"
    },
    {
      "arxiv_id": "2504.02698v1",
      "title": "SCMPPI: Supervised Contrastive Multimodal Framework for Predicting Protein-Protein Interactions",
      "title_zh": "SCMPPI：用于预测蛋白质-蛋白质相互作用的监督对比多模态框架\n",
      "authors": [
        "Shengrui XU",
        "Tianchi Lu",
        "Zikun Wang",
        "Jixiu Zhai",
        "Jingwan Wang"
      ],
      "abstract": "Protein-Protein Interaction (PPI) prediction is a key task in uncovering\ncellular functional networks and disease mechanisms. However, traditional\nexperimental methods are time-consuming and costly, and existing computational\nmodels face challenges in cross-modal feature fusion, robustness, and\nfalse-negative suppression. In this paper, we propose a novel supervised\ncontrastive multimodal framework, SCMPPI, for PPI prediction. By integrating\nprotein sequence features (AAC, DPC, CKSAAP-ESMC) with PPI network topology\ninformation (Node2Vec graph embedding), and combining an improved supervised\ncontrastive learning strategy, SCMPPI significantly enhances PPI prediction\nperformance. For the PPI task, SCMPPI introduces a negative sample filtering\nmechanism and modifies the contrastive loss function, effectively optimizing\nmultimodal features. Experiments on eight benchmark datasets, including yeast,\nhuman, and H.pylori, show that SCMPPI outperforms existing state-of-the-art\nmethods (such as DF-PPI and TAGPPI) in key metrics such as accuracy ( 98.01%)\nand AUC (99.62%), and demonstrates strong generalization in cross-species\nprediction (AUC > 99% on multi-species datasets). Furthermore, SCMPPI has been\nsuccessfully applied to CD9 networks, the Wnt pathway, and cancer-specific\nnetworks, providing a reliable tool for disease target discovery. This\nframework also offers a new paradigm for multimodal biological information\nfusion and contrastive learning in collaborative optimization for various\ncombined predictions.",
      "tldr_zh": "该论文提出了一种名为SCMPPI的监督对比多模态框架，用于预测蛋白质-蛋白质相互作用(PPI)。SCMPPI集成了蛋白质序列特征（AAC, DPC, CKSAAP-ESMC）与PPI网络拓扑信息（Node2Vec graph embedding），并结合改进的监督对比学习策略，显著提升了PPI预测性能。该框架引入负样本过滤机制并改进了对比损失函数，有效优化了多模态特征。在酵母、人类和幽门螺杆菌等八个基准数据集上的实验表明，SCMPPI在准确率(98.01%)和AUC(99.62%)等关键指标上优于现有方法，并在跨物种预测中表现出强大的泛化能力。SCMPPI已成功应用于CD9网络、Wnt通路和癌症特异性网络，为疾病靶点发现提供了一种可靠的工具。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM",
        "92C40, 68T07",
        "I.2.6; J.3"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages,11 figures,conference",
      "pdf_url": "http://arxiv.org/pdf/2504.02698v1",
      "published_date": "2025-04-03 15:34:02 UTC",
      "updated_date": "2025-04-03 15:34:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:04:58.165732"
    },
    {
      "arxiv_id": "2504.02685v1",
      "title": "STOOD-X methodology: using statistical nonparametric test for OOD Detection Large-Scale datasets enhanced with explainability",
      "title_zh": "STOOD-X 方法论：使用统计非参数检验进行 OOD 检测，并增强对大规模数据集的可解释性\n",
      "authors": [
        "Iván Sevillano-García",
        "Julián Luengo",
        "Francisco Herrera"
      ],
      "abstract": "Out-of-Distribution (OOD) detection is a critical task in machine learning,\nparticularly in safety-sensitive applications where model failures can have\nserious consequences. However, current OOD detection methods often suffer from\nrestrictive distributional assumptions, limited scalability, and a lack of\ninterpretability. To address these challenges, we propose STOOD-X, a two-stage\nmethodology that combines a Statistical nonparametric Test for OOD Detection\nwith eXplainability enhancements. In the first stage, STOOD-X uses\nfeature-space distances and a Wilcoxon-Mann-Whitney test to identify OOD\nsamples without assuming a specific feature distribution. In the second stage,\nit generates user-friendly, concept-based visual explanations that reveal the\nfeatures driving each decision, aligning with the BLUE XAI paradigm. Through\nextensive experiments on benchmark datasets and multiple architectures, STOOD-X\nachieves competitive performance against state-of-the-art post hoc OOD\ndetectors, particularly in high-dimensional and complex settings. In addition,\nits explainability framework enables human oversight, bias detection, and model\ndebugging, fostering trust and collaboration between humans and AI systems. The\nSTOOD-X methodology therefore offers a robust, explainable, and scalable\nsolution for real-world OOD detection tasks.",
      "tldr_zh": "该论文提出了STOOD-X方法，一种两阶段的OOD（Out-of-Distribution）检测方法，结合了统计非参数检验和可解释性增强。第一阶段，STOOD-X使用特征空间距离和Wilcoxon-Mann-Whitney检验来识别OOD样本，无需假设特定的特征分布。第二阶段，它生成基于概念的可视化解释，揭示驱动每个决策的特征，符合BLUE XAI范式。在基准数据集和多种架构上的实验表明，STOOD-X在高性能、高维度和复杂环境中实现了与最先进的post hoc OOD检测器相媲美的性能。此外，其可解释性框架支持人工监督、偏差检测和模型调试，从而促进人与AI系统之间的信任和协作。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 7 Figures",
      "pdf_url": "http://arxiv.org/pdf/2504.02685v1",
      "published_date": "2025-04-03 15:26:03 UTC",
      "updated_date": "2025-04-03 15:26:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:05:10.086653"
    },
    {
      "arxiv_id": "2504.02670v1",
      "title": "Affordable AI Assistants with Knowledge Graph of Thoughts",
      "title_zh": "基于思维知识图谱的经济型 AI 助手\n",
      "authors": [
        "Maciej Besta",
        "Lorenzo Paleari",
        "Jia Hao Andrea Jiang",
        "Robert Gerstenberger",
        "You Wu",
        "Patrick Iff",
        "Ales Kubicek",
        "Piotr Nyczyk",
        "Diana Khimey",
        "Jón Gunnar Hannesson",
        "Grzegorz Kwaśniewski",
        "Marcin Copik",
        "Hubert Niewiadomski",
        "Torsten Hoefler"
      ],
      "abstract": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose the Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively. For example, KGoT achieves a 29%\nimprovement in task success rates on the GAIA benchmark compared to Hugging\nFace Agents with GPT-4o mini, while reducing costs by over 36x compared to\nGPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and\n37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a\nscalable, affordable, and high-performing solution for AI assistants.",
      "tldr_zh": "该论文提出了“知识图谱思维”（Knowledge Graph of Thoughts, KGoT）这一新型AI助手架构，旨在解决现有基于大型语言模型（LLM）的AI助手成本高昂且在复杂任务（如GAIA基准测试）中成功率有限的问题。KGoT通过将LLM推理与动态构建的知识图谱（KG）相结合，提取任务相关的知识并将其结构化为动态KG表示，并通过数学求解器、网络爬虫和Python脚本等外部工具进行迭代增强。实验表明，KGoT在GAIA基准测试中的任务成功率比使用GPT-4o mini的Hugging Face Agents提高了29%，同时成本降低了36倍以上，并且对Qwen2.5-32B和Deepseek-R1-70B等模型也有显著提升。KGoT为AI助手提供了一种可扩展、经济高效且高性能的解决方案。\n",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02670v1",
      "published_date": "2025-04-03 15:11:55 UTC",
      "updated_date": "2025-04-03 15:11:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:05:22.291857"
    },
    {
      "arxiv_id": "2504.02654v1",
      "title": "SymDQN: Symbolic Knowledge and Reasoning in Neural Network-based Reinforcement Learning",
      "title_zh": "SymDQN：基于神经网络的强化学习中的符号知识与推理\n",
      "authors": [
        "Ivo Amador",
        "Nina Gierasimczuk"
      ],
      "abstract": "We propose a learning architecture that allows symbolic control and guidance\nin reinforcement learning with deep neural networks. We introduce SymDQN, a\nnovel modular approach that augments the existing Dueling Deep Q-Networks\n(DuelDQN) architecture with modules based on the neuro-symbolic framework of\nLogic Tensor Networks (LTNs). The modules guide action policy learning and\nallow reinforcement learning agents to display behaviour consistent with\nreasoning about the environment. Our experiment is an ablation study performed\non the modules. It is conducted in a reinforcement learning environment of a\n5x5 grid navigated by an agent that encounters various shapes, each associated\nwith a given reward. The underlying DuelDQN attempts to learn the optimal\nbehaviour of the agent in this environment, while the modules facilitate shape\nrecognition and reward prediction. We show that our architecture significantly\nimproves learning, both in terms of performance and the precision of the agent.\nThe modularity of SymDQN allows reflecting on the intricacies and complexities\nof combining neural and symbolic approaches in reinforcement learning.",
      "tldr_zh": "该论文提出了SymDQN，一种新颖的模块化架构，它使用神经符号框架Logic Tensor Networks (LTNs)增强了Dueling Deep Q-Networks (DuelDQN)，从而在基于神经网络的强化学习中实现符号控制和指导。SymDQN通过模块引导动作策略学习，并使强化学习智能体能够展示与环境推理一致的行为。实验在一个5x5网格环境中进行，智能体遇到各种形状并获得奖励。结果表明，SymDQN显著提高了学习性能和智能体的精度，并为在强化学习中结合神经和符号方法提供了新的思路。\n",
      "categories": [
        "cs.AI",
        "cs.LO",
        "cs.NE",
        "I.2.6"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.02654v1",
      "published_date": "2025-04-03 14:51:11 UTC",
      "updated_date": "2025-04-03 14:51:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:05:33.984331"
    },
    {
      "arxiv_id": "2504.02646v1",
      "title": "Prompt Optimization with Logged Bandit Data",
      "title_zh": "利用已记录的 Bandit 数据进行 Prompt 优化\n",
      "authors": [
        "Haruka Kiyohara",
        "Daniel Yiming Cao",
        "Yuta Saito",
        "Thorsten Joachims"
      ],
      "abstract": "We study how to use naturally available user feedback, such as clicks, to\noptimize large language model (LLM) pipelines for generating personalized\nsentences using prompts. Naive approaches, which estimate the policy gradient\nin the prompt space, suffer either from variance caused by the large action\nspace of prompts or bias caused by inaccurate reward predictions. To circumvent\nthese challenges, we propose a novel kernel-based off-policy gradient method,\nwhich estimates the policy gradient by leveraging similarity among generated\nsentences, substantially reducing variance while suppressing the bias.\nEmpirical results on our newly established suite of benchmarks demonstrate the\neffectiveness of the proposed approach in generating personalized descriptions\nfor movie recommendations, particularly when the number of candidate prompts is\nlarge.",
      "tldr_zh": "本文研究如何利用用户反馈（如点击）优化大型语言模型(LLM) pipeline，以生成个性化句子。针对直接在prompt空间估计策略梯度时面临的方差过大或奖励预测不准确导致的偏差问题，提出了一种基于核函数的off-policy梯度方法。该方法通过利用生成句子之间的相似性来估计策略梯度，从而显著降低方差并抑制偏差。在电影推荐的个性化描述生成任务上，实验结果表明，尤其是在候选prompt数量较大时，该方法能够有效提升生成效果。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2504.02646v1",
      "published_date": "2025-04-03 14:40:40 UTC",
      "updated_date": "2025-04-03 14:40:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:05:45.894759"
    },
    {
      "arxiv_id": "2504.02623v1",
      "title": "Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents through Related and Dynamic Missions",
      "title_zh": "多任务工具平台：通过相关和动态任务评估基于 LLM 的智能体的稳健性\n",
      "authors": [
        "PeiJie Yu",
        "Yifan Yang",
        "Jinjian Li",
        "Zelong Zhang",
        "Haorui Wang",
        "Xiao Feng",
        "Feng Zhang"
      ],
      "abstract": "Large language models (LLMs) demonstrate strong potential as agents for tool\ninvocation due to their advanced comprehension and planning capabilities. Users\nincreasingly rely on LLM-based agents to solve complex missions through\niterative interactions. However, existing benchmarks predominantly access\nagents in single-mission scenarios, failing to capture real-world complexity.\nTo bridge this gap, we propose the Multi-Mission Tool Bench. In the benchmark,\neach test case comprises multiple interrelated missions. This design requires\nagents to dynamically adapt to evolving demands. Moreover, the proposed\nbenchmark explores all possible mission-switching patterns within a fixed\nmission number. Specifically, we propose a multi-agent data generation\nframework to construct the benchmark. We also propose a novel method to\nevaluate the accuracy and efficiency of agent decisions with dynamic decision\ntrees. Experiments on diverse open-source and closed-source LLMs reveal\ncritical factors influencing agent robustness and provide actionable insights\nto the tool invocation society.",
      "tldr_zh": "该论文提出了一个多任务工具平台(Multi-Mission Tool Bench)，用于评估基于大型语言模型(LLM)的智能体在处理相关且动态任务时的鲁棒性。与现有基准测试主要关注单任务场景不同，该平台包含多个相互关联的任务，要求智能体动态适应不断变化的需求。研究者提出了一个多智能体数据生成框架来构建该基准，并使用动态决策树评估智能体决策的准确性和效率。实验结果揭示了影响智能体鲁棒性的关键因素，并为工具调用领域提供了有价值的见解。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02623v1",
      "published_date": "2025-04-03 14:21:33 UTC",
      "updated_date": "2025-04-03 14:21:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:05:57.891206"
    },
    {
      "arxiv_id": "2504.02620v1",
      "title": "Efficient Model Editing with Task-Localized Sparse Fine-tuning",
      "title_zh": "基于任务局部稀疏微调的高效模型编辑\n",
      "authors": [
        "Leonardo Iurada",
        "Marco Ciccone",
        "Tatiana Tommasi"
      ],
      "abstract": "Task arithmetic has emerged as a promising approach for editing models by\nrepresenting task-specific knowledge as composable task vectors. However,\nexisting methods rely on network linearization to derive task vectors, leading\nto computational bottlenecks during training and inference. Moreover,\nlinearization alone does not ensure weight disentanglement, the key property\nthat enables conflict-free composition of task vectors. To address this, we\npropose TaLoS which allows to build sparse task vectors with minimal\ninterference without requiring explicit linearization and sharing information\nacross tasks. We find that pre-trained models contain a subset of parameters\nwith consistently low gradient sensitivity across tasks, and that sparsely\nupdating only these parameters allows for promoting weight disentanglement\nduring fine-tuning. Our experiments prove that TaLoS improves training and\ninference efficiency while outperforming current methods in task addition and\nnegation. By enabling modular parameter editing, our approach fosters practical\ndeployment of adaptable foundation models in real-world applications.",
      "tldr_zh": "本文提出了一种名为TaLoS的高效模型编辑方法，旨在解决现有任务算术方法在模型编辑中存在的计算瓶颈和权重解耦问题。TaLoS通过任务局部稀疏微调构建稀疏任务向量，无需显式线性化，并允许在任务间共享信息。研究发现，预训练模型包含一个参数子集，这些参数在不同任务中具有一致的低梯度敏感性；稀疏地更新这些参数有助于在微调期间促进权重解耦。实验结果表明，TaLoS在任务添加和否定方面优于现有方法，提高了训练和推理效率，为可适应的基础模型在实际应用中的部署提供了可能。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted ICLR 2025 - https://github.com/iurada/talos-task-arithmetic",
      "pdf_url": "http://arxiv.org/pdf/2504.02620v1",
      "published_date": "2025-04-03 14:20:06 UTC",
      "updated_date": "2025-04-03 14:20:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:06:10.090264"
    },
    {
      "arxiv_id": "2504.02607v1",
      "title": "Learning Geometrically-Informed Lyapunov Functions with Deep Diffeomorphic RBF Networks",
      "title_zh": "利用深度微分同胚 RBF 网络学习几何信息 Lyapunov 函数\n",
      "authors": [
        "Samuel Tesfazgi",
        "Leonhard Sprandl",
        "Sandra Hirche"
      ],
      "abstract": "The practical deployment of learning-based autonomous systems would greatly\nbenefit from tools that flexibly obtain safety guarantees in the form of\ncertificate functions from data. While the geometrical properties of such\ncertificate functions are well understood, synthesizing them using machine\nlearning techniques still remains a challenge. To mitigate this issue, we\npropose a diffeomorphic function learning framework where prior structural\nknowledge of the desired output is encoded in the geometry of a simple\nsurrogate function, which is subsequently augmented through an expressive,\ntopology-preserving state-space transformation. Thereby, we achieve an indirect\nfunction approximation framework that is guaranteed to remain in the desired\nhypothesis space. To this end, we introduce a novel approach to construct\ndiffeomorphic maps based on RBF networks, which facilitate precise, local\ntransformations around data. Finally, we demonstrate our approach by learning\ndiffeomorphic Lyapunov functions from real-world data and apply our method to\ndifferent attractor systems.",
      "tldr_zh": "该论文提出了一种基于深度微分同胚RBF网络(Deep Diffeomorphic RBF Networks)的学习框架，用于学习具有几何信息的Lyapunov函数，从而为基于学习的自主系统提供安全保证。该框架通过微分同胚函数学习方法，将期望输出的结构知识编码到简单代理函数的几何结构中，并通过保持拓扑结构的状态空间变换进行增强。该方法保证了函数逼近过程始终在期望的假设空间内。论文还提出了一种构建基于RBF网络的微分同胚映射的新方法，并应用于从真实世界数据中学习Lyapunov函数，以及不同的吸引子系统。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02607v1",
      "published_date": "2025-04-03 14:09:17 UTC",
      "updated_date": "2025-04-03 14:09:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:06:22.114325"
    },
    {
      "arxiv_id": "2504.02606v1",
      "title": "Improving Counterfactual Truthfulness for Molecular Property Prediction through Uncertainty Quantification",
      "title_zh": "通过不确定性量化提高分子性质预测的反事实真实性\n",
      "authors": [
        "Jonas Teufel",
        "Annika Leinweber",
        "Pascal Friederich"
      ],
      "abstract": "Explainable AI (xAI) interventions aim to improve interpretability for\ncomplex black-box models, not only to improve user trust but also as a means to\nextract scientific insights from high-performing predictive systems. In\nmolecular property prediction, counterfactual explanations offer a way to\nunderstand predictive behavior by highlighting which minimal perturbations in\nthe input molecular structure cause the greatest deviation in the predicted\nproperty. However, such explanations only allow for meaningful scientific\ninsights if they reflect the distribution of the true underlying property -- a\nfeature we define as counterfactual truthfulness. To increase this\ntruthfulness, we propose the integration of uncertainty estimation techniques\nto filter counterfactual candidates with high predicted uncertainty. Through\ncomputational experiments with synthetic and real-world datasets, we\ndemonstrate that traditional uncertainty estimation methods, such as ensembles\nand mean-variance estimation, can already substantially reduce the average\nprediction error and increase counterfactual truthfulness, especially for\nout-of-distribution settings. Our results highlight the importance and\npotential impact of incorporating uncertainty estimation into explainability\nmethods, especially considering the relatively high effectiveness of low-effort\ninterventions like model ensembles.",
      "tldr_zh": "该论文针对分子性质预测中反事实解释的真实性问题，提出了一种通过不确定性量化来提高反事实解释真实性的方法。反事实解释旨在通过揭示分子结构的最小扰动来理解预测行为，但只有当它们反映真实性质的分布时才有意义。该方法通过集成不确定性估计技术，过滤掉具有高预测不确定性的反事实候选，从而提高反事实解释的真实性。实验结果表明，诸如集成方法和均值-方差估计等传统不确定性估计方法可以显著降低平均预测误差，并提高反事实解释的真实性，尤其是在分布外设置中。该研究强调了将不确定性估计纳入可解释性方法的重要性，特别是在考虑到模型集成等低成本干预措施的有效性时。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, 5 figures, 4 tabels, accepted at the 3rd xAI World\n  Conference",
      "pdf_url": "http://arxiv.org/pdf/2504.02606v1",
      "published_date": "2025-04-03 14:07:30 UTC",
      "updated_date": "2025-04-03 14:07:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:06:34.238149"
    },
    {
      "arxiv_id": "2504.02605v1",
      "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
      "title_zh": "Multi-SWE-bench：用于问题解决的多语言基准测试",
      "authors": [
        "Daoguang Zan",
        "Zhirong Huang",
        "Wei Liu",
        "Hanwu Chen",
        "Linhao Zhang",
        "Shulin Xin",
        "Lu Chen",
        "Qi Liu",
        "Xiaojian Zhong",
        "Aoyan Li",
        "Siyao Liu",
        "Yongsheng Xiao",
        "Liangqiang Chen",
        "Yuyu Zhang",
        "Jing Su",
        "Tianyu Liu",
        "Rui Long",
        "Kai Shen",
        "Liang Xiang"
      ],
      "abstract": "The task of issue resolving is to modify a codebase to generate a patch that\naddresses a given issue. However, existing benchmarks, such as SWE-bench, focus\nalmost exclusively on Python, making them insufficient for evaluating Large\nLanguage Models (LLMs) across diverse software ecosystems. To address this, we\nintroduce a multilingual issue-resolving benchmark, called Multi-SWE-bench,\ncovering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a\ntotal of 1,632 high-quality instances, which were carefully annotated from\n2,456 candidates by 68 expert annotators, ensuring that the benchmark can\nprovide an accurate and reliable evaluation. Based on Multi-SWE-bench, we\nevaluate a series of state-of-the-art models using three representative methods\n(Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with\nkey empirical insights. In addition, we launch a Multi-SWE-RL open-source\ncommunity, aimed at building large-scale reinforcement learning (RL) training\ndatasets for issue-resolving tasks. As an initial contribution, we release a\nset of 4,723 well-structured instances spanning seven programming languages,\nlaying a solid foundation for RL research in this domain. More importantly, we\nopen-source our entire data production pipeline, along with detailed tutorials,\nencouraging the open-source community to continuously contribute and expand the\ndataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL\ncommunity as catalysts for advancing RL toward its full potential, bringing us\none step closer to the dawn of AGI.",
      "tldr_zh": "该论文提出了一个多语言问题解决基准测试集Multi-SWE-bench，旨在评估大型语言模型(LLMs)在不同软件生态系统中的表现，弥补现有基准测试集（如SWE-bench）主要集中于Python的不足。Multi-SWE-bench覆盖Java、TypeScript、JavaScript、Go、Rust、C和C++七种编程语言，包含1632个高质量实例，这些实例经过68位专家从2456个候选集中精心标注。论文使用Agentless、SWE-agent和OpenHands三种方法评估了一系列先进模型，并进行了全面的分析。此外，论文还推出了Multi-SWE-RL开源社区，发布了包含七种编程语言的4723个结构化实例，为问题解决任务中的强化学习(RL)研究奠定了基础，并开源了整个数据生产流程。\n",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02605v1",
      "published_date": "2025-04-03 14:06:17 UTC",
      "updated_date": "2025-04-03 14:06:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:06:46.338588"
    },
    {
      "arxiv_id": "2504.02589v1",
      "title": "Knowledge Graph Completion with Mixed Geometry Tensor Factorization",
      "title_zh": "基于混合几何张量分解的知识图谱补全\n",
      "authors": [
        "Viacheslav Yusupov",
        "Maxim Rakhuba",
        "Evgeny Frolov"
      ],
      "abstract": "In this paper, we propose a new geometric approach for knowledge graph\ncompletion via low rank tensor approximation. We augment a pretrained and\nwell-established Euclidean model based on a Tucker tensor decomposition with a\nnovel hyperbolic interaction term. This correction enables more nuanced\ncapturing of distributional properties in data better aligned with real-world\nknowledge graphs. By combining two geometries together, our approach improves\nexpressivity of the resulting model achieving new state-of-the-art link\nprediction accuracy with a significantly lower number of parameters compared to\nthe previous Euclidean and hyperbolic models.",
      "tldr_zh": "本文提出了一种新的几何方法，通过低秩张量近似来实现知识图谱补全。该方法使用双曲交互项增强了基于 Tucker 张量分解的预训练的、成熟的欧几里得模型。 这种修正使得能够更细致地捕获与真实世界知识图谱更一致的数据中的分布特性。 通过将两种几何结构结合在一起，我们的方法提高了生成模型的表达能力，与之前的欧几里得和双曲模型相比，以显着更少的参数实现了新的最先进的链接预测精度。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to AISTATS 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.02589v1",
      "published_date": "2025-04-03 13:54:43 UTC",
      "updated_date": "2025-04-03 13:54:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:06:58.125912"
    },
    {
      "arxiv_id": "2504.02586v1",
      "title": "Deep learning for music generation. Four approaches and their comparative evaluation",
      "title_zh": "用于音乐生成的深度学习：四种方法及其比较评估\n",
      "authors": [
        "Razvan Paroiu",
        "Stefan Trausan-Matu"
      ],
      "abstract": "This paper introduces four different artificial intelligence algorithms for\nmusic generation and aims to compare these methods not only based on the\naesthetic quality of the generated music but also on their suitability for\nspecific applications. The first set of melodies is produced by a slightly\nmodified visual transformer neural network that is used as a language model.\nThe second set of melodies is generated by combining chat sonification with a\nclassic transformer neural network (the same method of music generation is\npresented in a previous research), the third set of melodies is generated by\ncombining the Schillinger rhythm theory together with a classic transformer\nneural network, and the fourth set of melodies is generated using GPT3\ntransformer provided by OpenAI. A comparative analysis is performed on the\nmelodies generated by these approaches and the results indicate that\nsignificant differences can be observed between them and regarding the\naesthetic value of them, GPT3 produced the most pleasing melodies, and the\nnewly introduced Schillinger method proved to generate better sounding music\nthan previous sonification methods.",
      "tldr_zh": "本文介绍了四种用于音乐生成的深度学习算法，并从生成音乐的美学质量和对特定应用的适用性两方面对这些方法进行了比较。这四种方法分别是：改进的视觉Transformer神经网络（作为语言模型）、结合聊天音化与经典Transformer神经网络、结合Schillinger节奏理论与经典Transformer神经网络，以及使用OpenAI的GPT3 Transformer。对这些方法生成的旋律进行比较分析表明，它们之间存在显著差异，并且在美学价值方面，GPT3生成的旋律最悦耳，而新引入的Schillinger方法证明比以前的音化方法能生成更好听的音乐。\n",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02586v1",
      "published_date": "2025-04-03 13:51:07 UTC",
      "updated_date": "2025-04-03 13:51:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:07:10.075270"
    },
    {
      "arxiv_id": "2504.02577v1",
      "title": "Reasoning Inconsistencies and How to Mitigate Them in Deep Learning",
      "title_zh": "深度学习中的推理不一致性以及如何缓解这些问题\n",
      "authors": [
        "Erik Arakelyan"
      ],
      "abstract": "The recent advancements in Deep Learning models and techniques have led to\nsignificant strides in performance across diverse tasks and modalities.\nHowever, while the overall capabilities of models show promising growth, our\nunderstanding of their internal reasoning processes remains limited,\nparticularly concerning systematic inconsistencies or errors patterns of\nlogical or inferential flaws. These inconsistencies may manifest as\ncontradictory outputs, failure to generalize across similar tasks, or erroneous\nconclusions in specific contexts. Even detecting and measuring such reasoning\ndiscrepancies is challenging, as they may arise from opaque internal\nprocedures, biases and imbalances in training data, or the inherent complexity\nof the task. Without effective methods to detect, measure, and mitigate these\nerrors, there is a risk of deploying models that are biased, exploitable, or\nlogically unreliable. This thesis aims to address these issues by producing\nnovel methods for deep learning models that reason over knowledge graphs,\nnatural language, and images. The thesis contributes two techniques for\ndetecting and quantifying predictive inconsistencies originating from opaque\ninternal procedures in natural language and image processing models. To\nmitigate inconsistencies from biases in training data, this thesis presents a\ndata efficient sampling method to improve fairness and performance and a\nsynthetic dataset generation approach in low resource scenarios. Finally, the\nthesis offers two techniques to optimize the models for complex reasoning\ntasks. These methods enhance model performance while allowing for more faithful\nand interpretable exploration and exploitation during inference. Critically,\nthis thesis provides a comprehensive framework to improve the robustness,\nfairness, and interpretability of deep learning models across diverse tasks and\nmodalities.",
      "tldr_zh": "这篇论文关注深度学习模型中存在的推理不一致性问题，这些不一致性可能导致模型输出矛盾、泛化能力不足或结论错误。论文提出了多种新方法来检测和量化自然语言和图像处理模型中由不透明内部过程引起的预测不一致性。为了缓解训练数据偏差导致的不一致性，论文提出了一种数据高效的采样方法，以提高公平性和性能，以及一种在低资源场景下生成合成数据集的方法。此外，论文还提供了两种优化模型以进行复杂推理任务的技术，从而提高模型性能，同时允许在推理过程中进行更忠实和可解释的探索和利用。总体而言，该论文提供了一个全面的框架，旨在提高深度学习模型在各种任务和模式中的鲁棒性、公平性和可解释性。\n",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "PhD thesis",
      "pdf_url": "http://arxiv.org/pdf/2504.02577v1",
      "published_date": "2025-04-03 13:40:55 UTC",
      "updated_date": "2025-04-03 13:40:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:07:22.300656"
    },
    {
      "arxiv_id": "2504.02558v1",
      "title": "Rip Current Segmentation: A Novel Benchmark and YOLOv8 Baseline Results",
      "title_zh": "裂流分割：一种新基准和 YOLOv8 基线结果\n",
      "authors": [
        "Andrei Dumitriu",
        "Florin Tatui",
        "Florin Miron",
        "Radu Tudor Ionescu",
        "Radu Timofte"
      ],
      "abstract": "Rip currents are the leading cause of fatal accidents and injuries on many\nbeaches worldwide, emphasizing the importance of automatically detecting these\nhazardous surface water currents. In this paper, we address a novel task: rip\ncurrent instance segmentation. We introduce a comprehensive dataset containing\n$2,466$ images with newly created polygonal annotations for instance\nsegmentation, used for training and validation. Additionally, we present a\nnovel dataset comprising $17$ drone videos (comprising about $24K$ frames)\ncaptured at $30 FPS$, annotated with both polygons for instance segmentation\nand bounding boxes for object detection, employed for testing purposes. We\ntrain various versions of YOLOv8 for instance segmentation on static images and\nassess their performance on the test dataset (videos). The best results were\nachieved by the YOLOv8-nano model (runnable on a portable device), with an\nmAP50 of $88.94%$ on the validation dataset and $81.21%$ macro average on the\ntest dataset. The results provide a baseline for future research in rip current\nsegmentation. Our work contributes to the existing literature by introducing a\ndetailed, annotated dataset, and training a deep learning model for instance\nsegmentation of rip currents. The code, training details and the annotated\ndataset are made publicly available at https://github.com/Irikos/rip_currents.",
      "tldr_zh": "该论文提出了一个用于识别离岸流的实例分割新任务，并构建了一个包含2466张带有离岸流多边形标注图像的综合数据集用于训练和验证。同时，作者还提供了一个包含17个无人机视频（约2.4万帧）的新数据集用于测试，视频数据带有实例分割多边形标注和目标检测边界框标注。研究者使用YOLOv8的不同版本在静态图像上进行训练，并在视频测试集上评估性能。YOLOv8-nano模型取得了最佳结果，在验证集上的mAP50为88.94%，在测试集上的宏平均mAP50为81.21%。该研究为离岸流分割提供了基准，并公开了代码、训练细节和标注数据集。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.4.0; I.4.9"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at CVPR 2023 NTIRE Workshop",
      "pdf_url": "http://arxiv.org/pdf/2504.02558v1",
      "published_date": "2025-04-03 13:14:16 UTC",
      "updated_date": "2025-04-03 13:14:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:07:34.385306"
    },
    {
      "arxiv_id": "2504.02546v1",
      "title": "GPG: A Simple and Strong Reinforcement Learning Baseline for Model Reasoning",
      "title_zh": "GPG：一种简单而强大的模型推理强化学习基线方法\n",
      "authors": [
        "Xiangxiang Chu",
        "Hailang Huang",
        "Xiao Zhang",
        "Fei Wei",
        "Yong Wang"
      ],
      "abstract": "Reinforcement Learning (RL) can directly enhance the reasoning capabilities\nof large language models without extensive reliance on Supervised Fine-Tuning\n(SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism\nand propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike\nconventional methods, GPG directly optimize the original RL objective, thus\nobviating the need for surrogate loss functions. As illustrated in our paper,\nby eliminating both the critic and reference models, and avoiding KL divergence\nconstraints, our approach significantly simplifies the training process when\ncompared to Group Relative Policy Optimization (GRPO). Our approach achieves\nsuperior performance without relying on auxiliary techniques or adjustments.\nExtensive experiments demonstrate that our method not only reduces\ncomputational costs but also consistently outperforms GRPO across various\nunimodal and multimodal tasks. Our code is available at\nhttps://github.com/AMAP-ML/GPG.",
      "tldr_zh": "该论文提出了一种名为Group Policy Gradient (GPG) 的极简强化学习方法，用于提升大型语言模型的推理能力，无需过度依赖监督微调(SFT)。GPG直接优化原始强化学习目标，避免了代理损失函数，并消除了critic模型、参考模型以及KL散度约束，从而简化了训练过程。实验结果表明，GPG在多种单模态和多模态任务中均优于Group Relative Policy Optimization (GRPO)，同时降低了计算成本，为模型推理提供了一个简单而强大的基线方法。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02546v1",
      "published_date": "2025-04-03 12:53:41 UTC",
      "updated_date": "2025-04-03 12:53:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:07:46.027086"
    },
    {
      "arxiv_id": "2504.02544v1",
      "title": "Fourier Sliced-Wasserstein Embedding for Multisets and Measures",
      "title_zh": "用于多重集和测度的傅里叶切片 Wasserstein 嵌入\n",
      "authors": [
        "Tal Amir",
        "Nadav Dym"
      ],
      "abstract": "We present the Fourier Sliced-Wasserstein (FSW) embedding - a novel method to\nembed multisets and measures over $\\mathbb{R}^d$ into Euclidean space.\n  Our proposed embedding approximately preserves the sliced Wasserstein\ndistance on distributions, thereby yielding geometrically meaningful\nrepresentations that better capture the structure of the input. Moreover, it is\ninjective on measures and bi-Lipschitz on multisets - a significant advantage\nover prevalent methods based on sum- or max-pooling, which are provably not\nbi-Lipschitz, and, in many cases, not even injective. The required output\ndimension for these guarantees is near-optimal: roughly $2 N d$, where $N$ is\nthe maximal input multiset size.\n  Furthermore, we prove that it is impossible to embed distributions over\n$\\mathbb{R}^d$ into Euclidean space in a bi-Lipschitz manner. Thus, the metric\nproperties of our embedding are, in a sense, the best possible.\n  Through numerical experiments, we demonstrate that our method yields superior\nmultiset representations that improve performance in practical learning tasks.\nSpecifically, we show that (a) a simple combination of the FSW embedding with\nan MLP achieves state-of-the-art performance in learning the (non-sliced)\nWasserstein distance; and (b) replacing max-pooling with the FSW embedding\nmakes PointNet significantly more robust to parameter reduction, with only\nminor performance degradation even after a 40-fold reduction.",
      "tldr_zh": "该论文提出了一种新的傅里叶切片Wasserstein (FSW)嵌入方法，用于将$\\mathbb{R}^d$上的多重集和测度嵌入到欧几里得空间中。该嵌入近似保留了分布上的切片Wasserstein距离，从而产生在几何上更有意义的表示，能更好地捕捉输入结构。FSW嵌入在测度上是单射的，在多重集上是双Lipschitz连续的，优于基于求和或最大池化的方法。理论分析表明，实现这些性质所需的输出维度接近最优，约为$2Nd$，其中$N$是最大输入多重集的大小。此外，论文证明了将$\\mathbb{R}^d$上的分布以双Lipschitz方式嵌入到欧几里得空间是不可能的，因此FSW嵌入的度量性质在某种意义上是最佳的。实验结果表明，FSW嵌入能够产生更好的多重集表示，提高实际学习任务的性能，例如在学习Wasserstein距离和增强PointNet的鲁棒性方面表现出色。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025 camera-ready. arXiv admin note: substantial text overlap\n  with arXiv:2405.16519",
      "pdf_url": "http://arxiv.org/pdf/2504.02544v1",
      "published_date": "2025-04-03 12:51:40 UTC",
      "updated_date": "2025-04-03 12:51:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:07:58.618283"
    },
    {
      "arxiv_id": "2504.02526v1",
      "title": "Improving User Experience with FAICO: Towards a Framework for AI Communication in Human-AI Co-Creativity",
      "title_zh": "利用 FAICO 改善用户体验：构建人机协同创作中 AI 通信框架\n",
      "authors": [
        "Jeba Rezwana",
        "Corey Ford"
      ],
      "abstract": "How AI communicates with humans is crucial for effective human-AI\nco-creation. However, many existing co-creative AI tools cannot communicate\neffectively, limiting their potential as collaborators. This paper introduces\nour initial design of a Framework for designing AI Communication (FAICO) for\nco-creative AI based on a systematic review of 107 full-length papers. FAICO\npresents key aspects of AI communication and their impacts on user experience\nto guide the design of effective AI communication. We then show actionable ways\nto translate our framework into two practical tools: design cards for designers\nand a configuration tool for users. The design cards enable designers to\nconsider AI communication strategies that cater to a diverse range of users in\nco-creative contexts, while the configuration tool empowers users to customize\nAI communication based on their needs and creative workflows. This paper\ncontributes new insights within the literature on human-AI co-creativity and\nHuman-Computer Interaction, focusing on designing AI communication to enhance\nuser experience.",
      "tldr_zh": "本文提出了一个用于人机协同创作中AI沟通的框架FAICO，旨在提升用户体验。该框架基于对107篇相关论文的系统性回顾，总结了AI沟通的关键方面及其对用户体验的影响，为设计有效的AI沟通提供指导。FAICO被转化为两种实用工具：设计卡片和配置工具。设计卡片帮助设计者考虑针对不同用户的AI沟通策略，而配置工具则允许用户根据需求和创作流程自定义AI沟通。该研究为提升人机协同创作的用户体验，以及人机交互领域做出了贡献，重点在于设计AI沟通。\n",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02526v1",
      "published_date": "2025-04-03 12:29:53 UTC",
      "updated_date": "2025-04-03 12:29:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:08:10.169196"
    },
    {
      "arxiv_id": "2504.02512v1",
      "title": "Towards Generalizing Temporal Action Segmentation to Unseen Views",
      "title_zh": "迈向将时间动作分割推广到未见过的视角\n",
      "authors": [
        "Emad Bahrami",
        "Olga Zatsarynna",
        "Gianpiero Francesca",
        "Juergen Gall"
      ],
      "abstract": "While there has been substantial progress in temporal action segmentation,\nthe challenge to generalize to unseen views remains unaddressed. Hence, we\ndefine a protocol for unseen view action segmentation where camera views for\nevaluating the model are unavailable during training. This includes changing\nfrom top-frontal views to a side view or even more challenging from exocentric\nto egocentric views. Furthermore, we present an approach for temporal action\nsegmentation that tackles this challenge. Our approach leverages a shared\nrepresentation at both the sequence and segment levels to reduce the impact of\nview differences during training. We achieve this by introducing a sequence\nloss and an action loss, which together facilitate consistent video and action\nrepresentations across different views. The evaluation on the Assembly101,\nIkeaASM, and EgoExoLearn datasets demonstrate significant improvements, with a\n12.8% increase in F1@50 for unseen exocentric views and a substantial 54%\nimprovement for unseen egocentric views.",
      "tldr_zh": "该论文提出了一个针对未见视角的时间动作分割协议，旨在解决现有方法泛化性不足的问题。该方法通过引入序列损失和动作损失，学习在序列和片段层面的共享表示，从而减少视角差异带来的影响。实验结果表明，在Assembly101、IkeaASM和EgoExoLearn数据集上，该方法在未见的外部视角和第一人称视角下均取得了显著的性能提升，F1@50指标分别提高了12.8%和54%。这表明该方法在跨视角时间动作分割任务中具有良好的泛化能力。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02512v1",
      "published_date": "2025-04-03 11:53:59 UTC",
      "updated_date": "2025-04-03 11:53:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:08:22.065670"
    },
    {
      "arxiv_id": "2504.02509v1",
      "title": "A Memory-Augmented LLM-Driven Method for Autonomous Merging of 3D Printing Work Orders",
      "title_zh": "一种基于记忆增强的 LLM 驱动方法，用于 3D 打印工单的自主合并\n",
      "authors": [
        "Yuhao Liu",
        "Maolin Yang",
        "Pingyu Jiang"
      ],
      "abstract": "With the rapid development of 3D printing, the demand for personalized and\ncustomized production on the manufacturing line is steadily increasing.\nEfficient merging of printing workpieces can significantly enhance the\nprocessing efficiency of the production line. Addressing the challenge, a Large\nLanguage Model (LLM)-driven method is established in this paper for the\nautonomous merging of 3D printing work orders, integrated with a\nmemory-augmented learning strategy. In industrial scenarios, both device and\norder features are modeled into LLM-readable natural language prompt templates,\nand develop an order-device matching tool along with a merging interference\nchecking module. By incorporating a self-memory learning strategy, an\nintelligent agent for autonomous order merging is constructed, resulting in\nimproved accuracy and precision in order allocation. The proposed method\neffectively leverages the strengths of LLMs in industrial applications while\nreducing hallucination.",
      "tldr_zh": "本文提出了一种基于LLM并结合记忆增强学习策略的3D打印工单自主合并方法，旨在提高个性化定制生产线的效率。该方法将设备和订单特征建模成LLM可读的自然语言提示模板，并开发了订单-设备匹配工具和合并干涉检查模块。通过引入自记忆学习策略，构建了一个用于自主订单合并的智能代理，从而提高了订单分配的准确性和精度。该方法有效利用了LLM在工业应用中的优势，并减少了幻觉。\n",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.02509v1",
      "published_date": "2025-04-03 11:50:29 UTC",
      "updated_date": "2025-04-03 11:50:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:08:33.951481"
    },
    {
      "arxiv_id": "2504.02495v1",
      "title": "Inference-Time Scaling for Generalist Reward Modeling",
      "title_zh": "通用奖励建模的推理时扩展\n",
      "authors": [
        "Zijun Liu",
        "Peiyi Wang",
        "Runxin Xu",
        "Shirong Ma",
        "Chong Ruan",
        "Peng Li",
        "Yang Liu",
        "Yu Wu"
      ],
      "abstract": "Reinforcement learning (RL) has been widely adopted in post-training for\nlarge language models (LLMs) at scale. Recently, the incentivization of\nreasoning capabilities in LLMs from RL indicates that $\\textit{proper learning\nmethods could enable effective inference-time scalability}$. A key challenge of\nRL is to obtain accurate reward signals for LLMs in various domains beyond\nverifiable questions or artificial rules. In this work, we investigate how to\nimprove reward modeling (RM) with more inference compute for general queries,\ni.e. the $\\textbf{inference-time scalability of generalist RM}$, and further,\nhow to improve the effectiveness of performance-compute scaling with proper\nlearning methods. For the RM approach, we adopt pointwise generative reward\nmodeling (GRM) to enable flexibility for different input types and potential\nfor inference-time scaling. For the learning method, we propose Self-Principled\nCritique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs\nthrough online RL, to generate principles adaptively and critiques accurately,\nresulting in $\\textbf{DeepSeek-GRM}$ models. Furthermore, for effective\ninference-time scaling, we use parallel sampling to expand compute usage, and\nintroduce a meta RM to guide voting process for better scaling performance.\nEmpirically, we show that SPCT significantly improves the quality and\nscalability of GRMs, outperforming existing methods and models in various RM\nbenchmarks without severe biases, and could achieve better performance compared\nto training-time scaling. DeepSeek-GRM still meets challenges in some tasks,\nwhich we believe can be addressed by future efforts in generalist reward\nsystems. The models will be released and open-sourced.",
      "tldr_zh": "该研究探索了通用奖励模型(Generalist Reward Modeling, RM)的推理时扩展性(Inference-Time Scaling)，旨在提升大规模语言模型(LLMs)在各种领域中奖励信号的准确性。研究提出了自洽批判调整(Self-Principled Critique Tuning, SPCT)方法，通过在线强化学习(RL)促进生成式奖励模型(Generative Reward Modeling, GRM)的可扩展奖励生成行为，从而构建了DeepSeek-GRM模型。此外，研究还引入了一种元奖励模型(Meta RM)来指导投票过程，以实现更有效的推理时扩展。实验结果表明，SPCT显著提高了GRM的质量和可扩展性，优于现有的方法和模型，并在各种RM基准测试中表现出色，同时没有严重的偏差。该研究为通用奖励系统的未来发展奠定了基础，并将开源相关模型。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint, under review. 42 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.02495v1",
      "published_date": "2025-04-03 11:19:49 UTC",
      "updated_date": "2025-04-03 11:19:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:08:46.521077"
    },
    {
      "arxiv_id": "2504.02492v1",
      "title": "Industrial Internet Robot Collaboration System and Edge Computing Optimization",
      "title_zh": "工业互联网机器人协作系统与边缘计算优化\n",
      "authors": [
        "Qian Zuo",
        "Dajun Tao",
        "Tian Qi",
        "Jieyi Xie",
        "Zijie Zhou",
        "Zhen Tian",
        "Yu Mingyu"
      ],
      "abstract": "In a complex environment, for a mobile robot to safely and collision - free\navoid all obstacles, it poses high requirements for its intelligence level.\nGiven that the information such as the position and geometric characteristics\nof obstacles is random, the control parameters of the robot, such as velocity\nand angular velocity, are also prone to random deviations. To address this\nissue in the framework of the Industrial Internet Robot Collaboration System,\nthis paper proposes a global path control scheme for mobile robots based on\ndeep learning. First of all, the dynamic equation of the mobile robot is\nestablished. According to the linear velocity and angular velocity of the\nmobile robot, its motion behaviors are divided into obstacle - avoidance\nbehavior, target - turning behavior, and target approaching behavior.\nSubsequently, the neural network method in deep learning is used to build a\nglobal path planning model for the robot. On this basis, a fuzzy controller is\ndesigned with the help of a fuzzy control algorithm to correct the deviations\nthat occur during path planning, thereby achieving optimized control of the\nrobot's global path. In addition, considering edge computing optimization, the\nproposed model can process local data at the edge device, reducing the\ncommunication burden between the robot and the central server, and improving\nthe real time performance of path planning. The experimental results show that\nfor the mobile robot controlled by the research method in this paper, the\ndeviation distance of the path angle is within 5 cm, the deviation convergence\ncan be completed within 10 ms, and the planned path is shorter. This indicates\nthat the proposed scheme can effectively improve the global path planning\nability of mobile robots in the industrial Internet environment and promote the\ncollaborative operation of robots through edge computing optimization.",
      "tldr_zh": "本文提出了一种基于深度学习的移动机器人全局路径控制方案，用于工业互联网机器人协作系统，旨在解决复杂环境中机器人避障和路径规划问题。该方案首先建立了移动机器人的动态方程，并将其运动行为分解为避障、目标转向和目标接近三种。然后，利用深度学习构建全局路径规划模型，并设计模糊控制器来校正路径规划中的偏差。此外，结合边缘计算优化，该模型可以在边缘设备处理本地数据，减少通信负担，提高实时性。实验结果表明，该方案能有效提高机器人在工业互联网环境下的全局路径规划能力，并通过边缘计算优化促进机器人的协同操作，路径角度偏差在5cm以内，偏差收敛时间在10ms以内。\n",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02492v1",
      "published_date": "2025-04-03 11:15:10 UTC",
      "updated_date": "2025-04-03 11:15:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:08:58.419715"
    },
    {
      "arxiv_id": "2504.02489v1",
      "title": "The Self-Learning Agent with a Progressive Neural Network Integrated Transformer",
      "title_zh": "集成渐进式神经网络 Transformer 的自学习 Agent\n",
      "authors": [
        "Ajay Sivakumar",
        "Shalini",
        "Vasantha Raj",
        "Sebastian Sylvester"
      ],
      "abstract": "This paper introduces a self-learning agent that integrates LLaMA 3.2 with a\nProgressive Neural Network (PNN) for continual learning in conversational AI\nand code generation. The framework dynamically collects data, fine-tunes tasks\nwith minimal samples, and leverages Meta-Learning for rapid adaptation. LoRA\noptimizes fine-tuning, while Elastic Weight Consolidation (EWC) enhances\nknowledge retention. Experimental results demonstrate improved adaptability and\nmemory stability, positioning this approach as a scalable step toward\nArtificial General Intelligence (AGI).",
      "tldr_zh": "本文提出了一种自学习Agent，它集成了LLaMA 3.2和一个Progressive Neural Network (PNN)，用于会话AI和代码生成中的持续学习。该框架动态收集数据，使用最少的样本进行任务微调，并利用元学习(Meta-Learning)进行快速适应。LoRA优化了微调过程，而Elastic Weight Consolidation (EWC)增强了知识保留。实验结果表明，该方法提高了适应性和记忆稳定性，是迈向通用人工智能(AGI)的可扩展步骤。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 2 figures, focuses on continual learning with PNN and LLaMA.\n  Experiments demonstrate scalability and lifelong learning capabilities",
      "pdf_url": "http://arxiv.org/pdf/2504.02489v1",
      "published_date": "2025-04-03 11:13:31 UTC",
      "updated_date": "2025-04-03 11:13:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:09:10.016372"
    },
    {
      "arxiv_id": "2504.02486v1",
      "title": "We Need Improved Data Curation and Attribution in AI for Scientific Discovery",
      "title_zh": "我们需要改进人工智能在科学发现中的数据管理和归因\n",
      "authors": [
        "Mara Graziani",
        "Antonio Foncubierta",
        "Dimitrios Christofidellis",
        "Irina Espejo-Morales",
        "Malina Molnar",
        "Marvin Alberts",
        "Matteo Manica",
        "Jannis Born"
      ],
      "abstract": "As the interplay between human-generated and synthetic data evolves, new\nchallenges arise in scientific discovery concerning the integrity of the data\nand the stability of the models. In this work, we examine the role of synthetic\ndata as opposed to that of real experimental data for scientific research. Our\nanalyses indicate that nearly three-quarters of experimental datasets available\non open-access platforms have relatively low adoption rates, opening new\nopportunities to enhance their discoverability and usability by automated\nmethods. Additionally, we observe an increasing difficulty in distinguishing\nsynthetic from real experimental data. We propose supplementing ongoing efforts\nin automating synthetic data detection by increasing the focus on watermarking\nreal experimental data, thereby strengthening data traceability and integrity.\nOur estimates suggest that watermarking even less than half of the real world\ndata generated annually could help sustain model robustness, while promoting a\nbalanced integration of synthetic and human-generated content.",
      "tldr_zh": "这篇论文探讨了科学研究中人工智能使用数据时面临的挑战，特别是真实实验数据和合成数据之间的区分问题。研究发现，大量开放获取的实验数据集利用率较低，同时区分合成数据和真实数据的难度越来越大。为了解决这些问题，论文建议在加强合成数据检测的同时，更应注重对真实实验数据进行水印标记，从而提高数据的可追溯性和完整性。研究估计，即使每年只对不到一半的真实世界数据进行水印标记，也能维持模型的鲁棒性，并促进合成数据和人工生成内容的平衡整合。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02486v1",
      "published_date": "2025-04-03 11:07:52 UTC",
      "updated_date": "2025-04-03 11:07:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:09:22.320552"
    },
    {
      "arxiv_id": "2504.02480v1",
      "title": "Graph Attention-Driven Bayesian Deep Unrolling for Dual-Peak Single-Photon Lidar Imaging",
      "title_zh": "基于图注意力驱动的贝叶斯深度展开算法，用于双峰单光子激光雷达成像\n",
      "authors": [
        "Kyungmin Choi",
        "JaKeoung Koo",
        "Stephen McLaughlin",
        "Abderrahim Halimi"
      ],
      "abstract": "Single-photon Lidar imaging offers a significant advantage in 3D imaging due\nto its high resolution and long-range capabilities, however it is challenging\nto apply in noisy environments with multiple targets per pixel. To tackle these\nchallenges, several methods have been proposed. Statistical methods demonstrate\ninterpretability on the inferred parameters, but they are often limited in\ntheir ability to handle complex scenes. Deep learning-based methods have shown\nsuperior performance in terms of accuracy and robustness, but they lack\ninterpretability or they are limited to a single-peak per pixel. In this paper,\nwe propose a deep unrolling algorithm for dual-peak single-photon Lidar\nimaging. We introduce a hierarchical Bayesian model for multiple targets and\npropose a neural network that unrolls the underlying statistical method. To\nsupport multiple targets, we adopt a dual depth maps representation and exploit\ngeometric deep learning to extract features from the point cloud. The proposed\nmethod takes advantages of statistical methods and learning-based methods in\nterms of accuracy and quantifying uncertainty. The experimental results on\nsynthetic and real data demonstrate the competitive performance when compared\nto existing methods, while also providing uncertainty information.",
      "tldr_zh": "该论文提出了一种基于图注意力驱动的贝叶斯深度展开算法，用于解决双峰单光子激光雷达成像问题。该方法结合了统计方法和深度学习方法的优点，构建了一个多目标分层贝叶斯模型，并设计了一个神经网络来展开底层的统计方法。为了支持多目标，采用了双深度图表示，并利用几何深度学习从点云中提取特征。实验结果表明，该方法在合成数据和真实数据上都表现出良好的性能，同时提供了不确定性信息。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02480v1",
      "published_date": "2025-04-03 10:57:26 UTC",
      "updated_date": "2025-04-03 10:57:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:09:34.064542"
    },
    {
      "arxiv_id": "2504.02479v1",
      "title": "Hierarchical Policy-Gradient Reinforcement Learning for Multi-Agent Shepherding Control of Non-Cohesive Targets",
      "title_zh": "用于非凝聚目标多智能体放牧控制的分层策略梯度强化学习\n",
      "authors": [
        "Stefano Covone",
        "Italo Napolitano",
        "Francesco De Lellis",
        "Mario di Bernardo"
      ],
      "abstract": "We propose a decentralized reinforcement learning solution for multi-agent\nshepherding of non-cohesive targets using policy-gradient methods. Our\narchitecture integrates target-selection with target-driving through Proximal\nPolicy Optimization, overcoming discrete-action constraints of previous Deep\nQ-Network approaches and enabling smoother agent trajectories. This model-free\nframework effectively solves the shepherding problem without prior dynamics\nknowledge. Experiments demonstrate our method's effectiveness and scalability\nwith increased target numbers and limited sensing capabilities.",
      "tldr_zh": "本文提出了一种分散式强化学习方案，使用策略梯度方法实现对非凝聚性目标的多智能体放牧控制。该架构通过近端策略优化(Proximal Policy Optimization)将目标选择与目标驱动相结合，克服了以往深度Q网络方法(Deep Q-Network)的离散动作约束，并实现了更平滑的智能体轨迹。这种无模型框架有效地解决了放牧问题，无需先验动力学知识。实验证明了该方法在目标数量增加和感知能力有限的情况下，其有效性和可扩展性。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "cs.SY",
        "eess.SY",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02479v1",
      "published_date": "2025-04-03 10:56:57 UTC",
      "updated_date": "2025-04-03 10:56:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:09:46.017880"
    },
    {
      "arxiv_id": "2504.02467v1",
      "title": "BOOST: Bootstrapping Strategy-Driven Reasoning Programs for Program-Guided Fact-Checking",
      "title_zh": "BOOST：用于程序引导的事实验证的，基于自举策略驱动的推理程序\n",
      "authors": [
        "Qisheng Hu",
        "Quanyu Long",
        "Wenya Wang"
      ],
      "abstract": "Program-guided reasoning has shown promise in complex claim fact-checking by\ndecomposing claims into function calls and executing reasoning programs.\nHowever, prior work primarily relies on few-shot in-context learning (ICL) with\nad-hoc demonstrations, which limit program diversity and require manual design\nwith substantial domain knowledge. Fundamentally, the underlying principles of\neffective reasoning program generation still remain underexplored, making it\nchallenging to construct effective demonstrations. To address this, we propose\nBOOST, a bootstrapping-based framework for few-shot reasoning program\ngeneration. BOOST explicitly integrates claim decomposition and\ninformation-gathering strategies as structural guidance for program generation,\niteratively refining bootstrapped demonstrations in a strategy-driven and\ndata-centric manner without human intervention. This enables a seamless\ntransition from zero-shot to few-shot strategic program-guided learning,\nenhancing interpretability and effectiveness. Experimental results show that\nBOOST outperforms prior few-shot baselines in both zero-shot and few-shot\nsettings for complex claim verification.",
      "tldr_zh": "该论文提出了一个名为BOOST的自举框架，用于生成少样本推理程序，以实现程序引导的事实核查。BOOST通过将声明分解和信息收集策略作为结构指导，迭代地改进自举演示，从而在没有人工干预的情况下，以策略驱动和数据为中心的方式进行学习。这种方法实现了从零样本到少样本策略性程序引导学习的无缝过渡，提高了可解释性和有效性。实验结果表明，在复杂声明验证的零样本和少样本设置中，BOOST优于现有的少样本基线模型。核心在于利用自举法和策略指导来提升推理程序生成的效果。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.02467v1",
      "published_date": "2025-04-03 10:38:45 UTC",
      "updated_date": "2025-04-03 10:38:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:09:58.402499"
    },
    {
      "arxiv_id": "2504.02463v1",
      "title": "Evaluating AI Recruitment Sourcing Tools by Human Preference",
      "title_zh": "通过人类偏好评估 AI 招聘寻源工具\n",
      "authors": [
        "Vladimir Slaykovskiy",
        "Maksim Zvegintsev",
        "Yury Sakhonchyk",
        "Hrachik Ajamian"
      ],
      "abstract": "This study introduces a benchmarking methodology designed to evaluate the\nperformance of AI-driven recruitment sourcing tools. We created and utilized a\ndataset to perform a comparative analysis of search results generated by\nleading AI-based solutions, LinkedIn Recruiter, and our proprietary system,\nPearch.ai. Human experts assessed the relevance of the returned candidates, and\nan Elo rating system was applied to quantitatively measure each tool's\ncomparative performance. Our findings indicate that AI-driven recruitment\nsourcing tools consistently outperform LinkedIn Recruiter in candidate\nrelevance, with Pearch.ai achieving the highest performance scores.\nFurthermore, we found a strong alignment between AI-based evaluations and human\njudgments, highlighting the potential for advanced AI technologies to\nsubstantially enhance talent acquisition effectiveness. Code and supporting\ndata are publicly available at\nhttps://github.com/vslaykovsky/ai-sourcing-benchmark",
      "tldr_zh": "本研究提出了一种评估AI驱动的招聘寻源工具性能的基准测试方法。通过构建数据集，对领先的AI解决方案、LinkedIn Recruiter和Pearch.ai的搜索结果进行了比较分析，并由人工专家评估候选人的相关性。采用Elo评分系统量化各工具的相对性能。结果表明，AI驱动的招聘工具在候选人相关性方面始终优于LinkedIn Recruiter，Pearch.ai表现最佳。AI评估与人工判断高度一致，突显了先进AI技术在提高人才获取效率方面的潜力。代码和数据已公开。\n",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02463v1",
      "published_date": "2025-04-03 10:33:43 UTC",
      "updated_date": "2025-04-03 10:33:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:10:10.113663"
    },
    {
      "arxiv_id": "2504.02464v1",
      "title": "CornerPoint3D: Look at the Nearest Corner Instead of the Center",
      "title_zh": "CornerPoint3D：关注最近的角点而非中心点\n",
      "authors": [
        "Ruixiao Zhang",
        "Runwei Guan",
        "Xiangyu Chen",
        "Adam Prugel-Bennett",
        "Xiaohao Cai"
      ],
      "abstract": "3D object detection aims to predict object centers, dimensions, and rotations\nfrom LiDAR point clouds. Despite its simplicity, LiDAR captures only the near\nside of objects, making center-based detectors prone to poor localization\naccuracy in cross-domain tasks with varying point distributions. Meanwhile,\nexisting evaluation metrics designed for single-domain assessment also suffer\nfrom overfitting due to dataset-specific size variations. A key question\narises: Do we really need models to maintain excellent performance in the\nentire 3D bounding boxes after being applied across domains? Actually, one of\nour main focuses is on preventing collisions between vehicles and other\nobstacles, especially in cross-domain scenarios where correctly predicting the\nsizes is much more difficult. To address these issues, we rethink cross-domain\n3D object detection from a practical perspective. We propose two new metrics\nthat evaluate a model's ability to detect objects' closer-surfaces to the LiDAR\nsensor. Additionally, we introduce EdgeHead, a refinement head that guides\nmodels to focus more on learnable closer surfaces, significantly improving\ncross-domain performance under both our new and traditional BEV/3D metrics.\nFurthermore, we argue that predicting the nearest corner rather than the object\ncenter enhances robustness. We propose a novel 3D object detector, coined as\nCornerPoint3D, which is built upon CenterPoint and uses heatmaps to supervise\nthe learning and detection of the nearest corner of each object. Our proposed\nmethods realize a balanced trade-off between the detection quality of entire\nbounding boxes and the locating accuracy of closer surfaces to the LiDAR\nsensor, outperforming the traditional center-based detector CenterPoint in\nmultiple cross-domain tasks and providing a more practically reasonable and\nrobust cross-domain 3D object detection solution.",
      "tldr_zh": "该论文针对LiDAR点云3D目标检测中，基于中心点的检测器在跨域任务中定位精度差的问题，以及现有评估指标对数据集特定尺寸变化的过拟合问题，提出了新的思路。论文提出两个新的评估指标，用于评估模型检测物体靠近LiDAR传感器表面的能力。同时，引入EdgeHead，引导模型更多关注可学习的近表面，提升跨域性能。此外，论文提出CornerPoint3D，一种基于CenterPoint的新型3D目标检测器，通过热图监督学习和检测每个物体最近的角点，而非中心点，从而增强鲁棒性。实验表明，该方法在多个跨域任务中优于传统的基于中心点的检测器CenterPoint，为跨域3D目标检测提供了一种更实用、更鲁棒的解决方案。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2407.04061",
      "pdf_url": "http://arxiv.org/pdf/2504.02464v1",
      "published_date": "2025-04-03 10:33:43 UTC",
      "updated_date": "2025-04-03 10:33:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:10:22.537125"
    },
    {
      "arxiv_id": "2504.02461v1",
      "title": "Am I Being Treated Fairly? A Conceptual Framework for Individuals to Ascertain Fairness",
      "title_zh": "我是否受到了公平对待？一个供个人确定公平性的概念框架\n",
      "authors": [
        "Juliett Suárez Ferreira",
        "Marija Slavkovik",
        "Jorge Casillas"
      ],
      "abstract": "Current fairness metrics and mitigation techniques provide tools for\npractitioners to asses how non-discriminatory Automatic Decision Making (ADM)\nsystems are. What if I, as an individual facing a decision taken by an ADM\nsystem, would like to know: Am I being treated fairly? We explore how to create\nthe affordance for users to be able to ask this question of ADM. In this paper,\nwe argue for the reification of fairness not only as a property of ADM, but\nalso as an epistemic right of an individual to acquire information about the\ndecisions that affect them and use that information to contest and seek\neffective redress against those decisions, in case they are proven to be\ndiscriminatory. We examine key concepts from existing research not only in\nalgorithmic fairness but also in explainable artificial intelligence,\naccountability, and contestability. Integrating notions from these domains, we\npropose a conceptual framework to ascertain fairness by combining different\ntools that empower the end-users of ADM systems. Our framework shifts the focus\nfrom technical solutions aimed at practitioners to mechanisms that enable\nindividuals to understand, challenge, and verify the fairness of decisions, and\nalso serves as a blueprint for organizations and policymakers, bridging the gap\nbetween technical requirements and practical, user-centered accountability.",
      "tldr_zh": "该论文提出了一个概念框架，旨在帮助个体用户评估自动决策系统（ADM）对其做出的决策是否公平。该框架将公平性视为个体的一种认知权利，即有权获取影响自身决策的信息，并在发现歧视时进行质疑和寻求补救。该框架整合了算法公平性、可解释人工智能、可问责性和可争议性等领域的研究，旨在弥合技术要求和以用户为中心的实践问责制之间的差距，为组织和政策制定者提供蓝图，使用户能够理解、挑战和验证决策的公平性。该框架强调从面向从业者的技术解决方案，转变为使用户能够理解、挑战和验证决策公平性的机制。\n",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.MA",
        "I.2; J.4"
      ],
      "primary_category": "cs.CY",
      "comment": "21 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.02461v1",
      "published_date": "2025-04-03 10:28:19 UTC",
      "updated_date": "2025-04-03 10:28:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:10:34.437757"
    },
    {
      "arxiv_id": "2504.02458v1",
      "title": "Retrieval-Augmented Purifier for Robust LLM-Empowered Recommendation",
      "title_zh": "检索增强净化器：用于增强 LLM 驱动推荐系统的鲁棒性\n",
      "authors": [
        "Liangbo Ning",
        "Wenqi Fan",
        "Qing Li"
      ],
      "abstract": "Recently, Large Language Model (LLM)-empowered recommender systems have\nrevolutionized personalized recommendation frameworks and attracted extensive\nattention. Despite the remarkable success, existing LLM-empowered RecSys have\nbeen demonstrated to be highly vulnerable to minor perturbations. To mitigate\nthe negative impact of such vulnerabilities, one potential solution is to\nemploy collaborative signals based on item-item co-occurrence to purify the\nmalicious collaborative knowledge from the user's historical interactions\ninserted by attackers. On the other hand, due to the capabilities to expand\ninsufficient internal knowledge of LLMs, Retrieval-Augmented Generation (RAG)\ntechniques provide unprecedented opportunities to enhance the robustness of\nLLM-empowered recommender systems by introducing external collaborative\nknowledge. Therefore, in this paper, we propose a novel framework (RETURN) by\nretrieving external collaborative signals to purify the poisoned user profiles\nand enhance the robustness of LLM-empowered RecSys in a plug-and-play manner.\nSpecifically, retrieval-augmented perturbation positioning is proposed to\nidentify potential perturbations within the users' historical sequences by\nretrieving external knowledge from collaborative item graphs. After that, we\nfurther retrieve the collaborative knowledge to cleanse the perturbations by\nusing either deletion or replacement strategies and introduce a robust ensemble\nrecommendation strategy to generate final robust predictions. Extensive\nexperiments on three real-world datasets demonstrate the effectiveness of the\nproposed RETURN.",
      "tldr_zh": "该论文提出了一种名为RETURN的检索增强净化框架，旨在提高基于大型语言模型(LLM)的推荐系统在面对对抗攻击时的鲁棒性。RETURN通过从协同物品图中检索外部协同信号，定位用户历史交互中的潜在扰动。然后，利用检索到的协同知识，通过删除或替换策略来净化这些扰动。此外，还引入了一种鲁棒的集成推荐策略来生成最终的鲁棒预测。在三个真实世界数据集上的大量实验表明，所提出的RETURN框架的有效性。该方法以即插即用的方式工作，增强了LLM推荐系统抵抗恶意协同知识攻击的能力。\n",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02458v1",
      "published_date": "2025-04-03 10:22:30 UTC",
      "updated_date": "2025-04-03 10:22:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:10:46.335511"
    },
    {
      "arxiv_id": "2504.02450v1",
      "title": "CHARMS: Cognitive Hierarchical Agent with Reasoning and Motion Styles",
      "title_zh": "CHARMS：具有推理和运动风格的认知分层智能体\n",
      "authors": [
        "Jingyi Wang",
        "Duanfeng Chu",
        "Zejian Deng",
        "Liping Lu"
      ],
      "abstract": "To address the current challenges of low intelligence and simplistic vehicle\nbehavior modeling in autonomous driving simulation scenarios, this paper\nproposes the Cognitive Hierarchical Agent with Reasoning and Motion Styles\n(CHARMS). The model can reason about the behavior of other vehicles like a\nhuman driver and respond with different decision-making styles, thereby\nimproving the intelligence and diversity of the surrounding vehicles in the\ndriving scenario. By introducing the Level-k behavioral game theory, the paper\nmodels the decision-making process of human drivers and employs deep\nreinforcement learning to train the models with diverse decision styles,\nsimulating different reasoning approaches and behavioral characteristics.\nBuilding on the Poisson cognitive hierarchy theory, this paper also presents a\nnovel driving scenario generation method. The method controls the proportion of\nvehicles with different driving styles in the scenario using Poisson and\nbinomial distributions, thus generating controllable and diverse driving\nenvironments. Experimental results demonstrate that CHARMS not only exhibits\nsuperior decision-making capabilities as ego vehicles, but also generates more\ncomplex and diverse driving scenarios as surrounding vehicles. We will release\ncode for CHARMS at https://github.com/WUTAD-Wjy/CHARMS.",
      "tldr_zh": "该论文提出了一个名为CHARMS的认知分层智能体，它具备推理和运动风格，旨在解决自动驾驶模拟场景中车辆行为建模智能化程度低和过于简单的问题。CHARMS模型能够像人类驾驶员一样推理其他车辆的行为，并以不同的决策风格做出反应，从而提高驾驶场景中周围车辆的智能性和多样性。该模型引入了Level-k行为博弈论来模拟人类驾驶员的决策过程，并采用深度强化学习训练具有不同决策风格的模型，模拟不同的推理方法和行为特征。此外，论文还提出了一种基于泊松认知层次理论的新型驾驶场景生成方法，通过控制场景中不同驾驶风格车辆的比例，生成可控且多样化的驾驶环境。实验结果表明，CHARMS不仅作为自车表现出卓越的决策能力，而且作为周围车辆生成了更复杂和多样化的驾驶场景。\n",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02450v1",
      "published_date": "2025-04-03 10:15:19 UTC",
      "updated_date": "2025-04-03 10:15:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:10:58.510627"
    },
    {
      "arxiv_id": "2504.02441v1",
      "title": "Cognitive Memory in Large Language Models",
      "title_zh": "大型语言模型中的认知记忆",
      "authors": [
        "Lianlei Shan",
        "Shixian Luo",
        "Zezhou Zhu",
        "Yu Yuan",
        "Yong Wu"
      ],
      "abstract": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
      "tldr_zh": "本文深入探讨了大型语言模型(LLMs)中的记忆机制，强调其对于生成上下文丰富、减少幻觉和提高效率的重要性。论文将记忆分为感觉记忆（对应输入提示）、短期记忆（处理即时上下文）和长期记忆（通过外部数据库或结构实现）。文章详细分析了基于文本的记忆（包括获取、管理和利用），KV缓存的记忆（包括选择和压缩技术以及管理策略），基于参数的记忆（如LoRA, TTT, MoE）以及基于隐藏状态的记忆（如chunk机制，循环Transformer，Mamba模型）。总而言之，本文全面分析了LLM的记忆机制，突出了其重要性及未来研究方向。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "37 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.02441v1",
      "published_date": "2025-04-03 09:58:19 UTC",
      "updated_date": "2025-04-03 09:58:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:11:10.467313"
    },
    {
      "arxiv_id": "2504.02438v1",
      "title": "Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation",
      "title_zh": "通过分层差分蒸馏将视频-语言模型扩展到1万帧\n",
      "authors": [
        "Chuanqi Cheng",
        "Jian Guan",
        "Wei Wu",
        "Rui Yan"
      ],
      "abstract": "Long-form video processing fundamentally challenges vision-language models\n(VLMs) due to the high computational costs of handling extended temporal\nsequences. Existing token pruning and feature merging methods often sacrifice\ncritical temporal dependencies or dilute semantic information. We introduce\ndifferential distillation, a principled approach that systematically preserves\ntask-relevant information while suppressing redundancy. Based on this\nprinciple, we develop ViLaMP, a hierarchical video-language model that\nprocesses hour-long videos at ``mixed precision'' through two key mechanisms:\n(1) differential keyframe selection that maximizes query relevance while\nmaintaining temporal distinctiveness at the frame level and (2) differential\nfeature merging that preserves query-salient features in non-keyframes at the\npatch level. Hence, ViLaMP retains full information in keyframes while reducing\nnon-keyframes to their most salient features, resembling mixed-precision\ntraining. Extensive experiments demonstrate ViLaMP's superior performance\nacross four video understanding benchmarks, particularly on long-form content.\nNotably, ViLaMP can process ultra-long videos (up to 10K frames) on a single\nNVIDIA A100 GPU, achieving substantial computational efficiency while\nmaintaining state-of-the-art performance.",
      "tldr_zh": "该论文提出了一种名为ViLaMP的分层视频语言模型，旨在解决VLMs处理长视频时计算成本过高的问题。ViLaMP通过“混合精度”的方式处理长达数小时的视频，包含两个关键机制：一是差异化关键帧选择，最大化查询相关性并保持时间独特性；二是差异化特征融合，在非关键帧中保留查询显著特征。这种方法类似于混合精度训练，ViLaMP在关键帧中保留完整信息，同时将非关键帧减少到最显著的特征。实验结果表明，ViLaMP在四个视频理解基准测试中表现出色，尤其是在长视频内容上，并且可以在单个NVIDIA A100 GPU上处理超长视频（最多10K帧），实现了显著的计算效率，同时保持了最先进的性能。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02438v1",
      "published_date": "2025-04-03 09:55:09 UTC",
      "updated_date": "2025-04-03 09:55:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:11:22.942988"
    },
    {
      "arxiv_id": "2504.02430v1",
      "title": "How Artificial Intelligence Leads to Knowledge Why: An Inquiry Inspired by Aristotle's Posterior Analytics",
      "title_zh": "人工智能如何通向“知其所以然”的知识：一项受亚里士多德《后分析篇》启发的探究\n",
      "authors": [
        "Guus Eelink",
        "Kilian Rückschloß",
        "Felix Weitkämper"
      ],
      "abstract": "Bayesian networks and causal models provide frameworks for handling queries\nabout external interventions and counterfactuals, enabling tasks that go beyond\nwhat probability distributions alone can address. While these formalisms are\noften informally described as capturing causal knowledge, there is a lack of a\nformal theory characterizing the type of knowledge required to predict the\neffects of external interventions. This work introduces the theoretical\nframework of causal systems to clarify Aristotle's distinction between\nknowledge that and knowledge why within artificial intelligence. By\ninterpreting existing artificial intelligence technologies as causal systems,\nit investigates the corresponding types of knowledge. Furthermore, it argues\nthat predicting the effects of external interventions is feasible only with\nknowledge why, providing a more precise understanding of the knowledge\nnecessary for such tasks.",
      "tldr_zh": "该论文受到亚里士多德《后分析篇》的启发，探讨了人工智能如何获得“知其所以然”的知识。论文引入了“因果系统”的理论框架，区分了人工智能中的“知其然”和“知其所以然”两种知识。通过将现有的人工智能技术解释为因果系统，研究了它们所对应的知识类型。论文论证了只有掌握“知其所以然”的知识才能预测外部干预的影响，从而更精确地理解了执行此类任务所需的知识。该研究为理解人工智能中的因果推理提供了理论基础。\n",
      "categories": [
        "cs.AI",
        "cs.LO",
        "I.2.4"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02430v1",
      "published_date": "2025-04-03 09:37:05 UTC",
      "updated_date": "2025-04-03 09:37:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:11:34.297670"
    },
    {
      "arxiv_id": "2504.02426v1",
      "title": "Narrative Studio: Visual narrative exploration using LLMs and Monte Carlo Tree Search",
      "title_zh": "叙事工作室：使用 LLM 和蒙特卡洛树搜索进行视觉叙事探索\n",
      "authors": [
        "Parsa Ghaffari",
        "Chris Hokamp"
      ],
      "abstract": "Interactive storytelling benefits from planning and exploring multiple 'what\nif' scenarios. Modern LLMs are useful tools for ideation and exploration, but\ncurrent chat-based user interfaces restrict users to a single linear flow. To\naddress this limitation, we propose Narrative Studio -- a novel in-browser\nnarrative exploration environment featuring a tree-like interface that allows\nbranching exploration from user-defined points in a story. Each branch is\nextended via iterative LLM inference guided by system and user-defined prompts.\nAdditionally, we employ Monte Carlo Tree Search (MCTS) to automatically expand\npromising narrative paths based on user-specified criteria, enabling more\ndiverse and robust story development. We also allow users to enhance narrative\ncoherence by grounding the generated text in an entity graph that represents\nthe actors and environment of the story.",
      "tldr_zh": "本文提出了Narrative Studio，一个新颖的浏览器内叙事探索环境，旨在通过LLM和蒙特卡洛树搜索(MCTS)支持交互式故事创作。该系统采用树状界面，允许用户从故事中的任意点进行分支探索，并通过迭代的LLM推理扩展每个分支，推理过程由系统和用户定义的提示引导。此外，系统利用MCTS根据用户指定的标准自动扩展有前景的叙事路径，从而实现更丰富和稳健的故事发展。为了增强叙事连贯性，系统还支持将生成的文本与实体图相关联，该实体图表示故事中的角色和环境。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02426v1",
      "published_date": "2025-04-03 09:31:07 UTC",
      "updated_date": "2025-04-03 09:31:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:11:46.366608"
    },
    {
      "arxiv_id": "2504.02417v1",
      "title": "Leveraging Static Relationships for Intra-Type and Inter-Type Message Passing in Video Question Answering",
      "title_zh": "利用静态关系进行视频问答中类型内和类型间消息传递\n",
      "authors": [
        "Lili Liang",
        "Guanglu Sun"
      ],
      "abstract": "Video Question Answering (VideoQA) is an important research direction in the\nfield of artificial intelligence, enabling machines to understand video content\nand perform reasoning and answering based on natural language questions.\nAlthough methods based on static relationship reasoning have made certain\nprogress, there are still deficiencies in the accuracy of static relationship\nrecognition and representation, and they have not fully utilized the static\nrelationship information in videos for in-depth reasoning and analysis.\nTherefore, this paper proposes a reasoning method for intra-type and inter-type\nmessage passing based on static relationships. This method constructs a dual\ngraph for intra-type message passing reasoning and builds a heterogeneous graph\nbased on static relationships for inter-type message passing reasoning. The\nintra-type message passing reasoning model captures the neighborhood\ninformation of targets and relationships related to the question in the dual\ngraph, updating the dual graph to obtain intra-type clues for answering the\nquestion. The inter-type message passing reasoning model captures the\nneighborhood information of targets and relationships from different categories\nrelated to the question in the heterogeneous graph, updating the heterogeneous\ngraph to obtain inter-type clues for answering the question. Finally, the\nanswers are inferred by combining the intra-type and inter-type clues based on\nstatic relationships. Experimental results on the ANetQA and Next-QA datasets\ndemonstrate the effectiveness of this method.",
      "tldr_zh": "该论文提出了一种基于静态关系的内类型和间类型消息传递的视频问答(VideoQA)推理方法。该方法构建了一个双图用于内类型消息传递推理，捕捉目标和关系相关的邻域信息，更新双图以获得内类型线索。同时，构建了一个异构图用于间类型消息传递推理，捕捉来自不同类别的目标和关系的邻域信息，更新异构图以获得间类型线索。最后，结合内类型和间类型线索进行答案推断。在AnetQA和Next-QA数据集上的实验结果表明了该方法的有效性。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02417v1",
      "published_date": "2025-04-03 09:14:41 UTC",
      "updated_date": "2025-04-03 09:14:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:11:58.123655"
    },
    {
      "arxiv_id": "2504.02408v1",
      "title": "Translation of Fetal Brain Ultrasound Images into Pseudo-MRI Images using Artificial Intelligence",
      "title_zh": "利用人工智能将胎儿脑部超声图像转换为伪 MRI 图像\n",
      "authors": [
        "Naomi Silverstein",
        "Efrat Leibowitz",
        "Ron Beloosesky",
        "Haim Azhari"
      ],
      "abstract": "Ultrasound is a widely accessible and cost-effective medical imaging tool\ncommonly used for prenatal evaluation of the fetal brain. However, it has\nlimitations, particularly in the third trimester, where the complexity of the\nfetal brain requires high image quality for extracting quantitative data. In\ncontrast, magnetic resonance imaging (MRI) offers superior image quality and\ntissue differentiation but is less available, expensive, and requires\ntime-consuming acquisition. Thus, transforming ultrasonic images into an\nMRI-mimicking display may be advantageous and allow better tissue anatomy\npresentation. To address this goal, we have examined the use of artificial\nintelligence, implementing a diffusion model renowned for generating\nhigh-quality images. The proposed method, termed \"Dual Diffusion Imposed\nCorrelation\" (DDIC), leverages a diffusion-based translation methodology,\nassuming a shared latent space between ultrasound and MRI domains. Model\ntraining was obtained utilizing the \"HC18\" dataset for ultrasound and the \"CRL\nfetal brain atlas\" along with the \"FeTA \" datasets for MRI. The generated\npseudo-MRI images provide notable improvements in visual discrimination of\nbrain tissue, especially in the lateral ventricles and the Sylvian fissure,\ncharacterized by enhanced contrast clarity. Improvement was demonstrated in\nMutual information, Peak signal-to-noise ratio, Fr\\'echet Inception Distance,\nand Contrast-to-noise ratio. Findings from these evaluations indicate\nstatistically significant superior performance of the DDIC compared to other\ntranslation methodologies. In addition, a Medical Opinion Test was obtained\nfrom 5 gynecologists. The results demonstrated display improvement in 81% of\nthe tested images. In conclusion, the presented pseudo-MRI images hold the\npotential for streamlining diagnosis and enhancing clinical outcomes through\nimproved representation.",
      "tldr_zh": "该研究提出了一种名为“双重扩散施加相关性”（DDIC）的人工智能方法，利用扩散模型将胎儿脑部超声图像转换为伪MRI图像。该方法假设超声和MRI图像域之间存在共享潜在空间，通过训练模型，显著改善了脑组织的可视化区分，尤其是在侧脑室和Sylvian裂隙的对比度清晰度方面。实验结果表明，DDIC在互信息、峰值信噪比、Fréchet初始距离和对比噪声比等方面均优于其他转换方法。妇科医生的医学意见测试也显示，81%的测试图像显示效果得到改善。该伪MRI图像有望通过改进图像呈现，简化诊断并改善临床结果。\n",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "13 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.02408v1",
      "published_date": "2025-04-03 08:59:33 UTC",
      "updated_date": "2025-04-03 08:59:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:12:10.547749"
    },
    {
      "arxiv_id": "2504.02402v1",
      "title": "EvMic: Event-based Non-contact sound recovery from effective spatial-temporal modeling",
      "title_zh": "EvMic：基于有效时空建模的事件相机非接触声音恢复\n",
      "authors": [
        "Hao Yin",
        "Shi Guo",
        "Xu Jia",
        "Xudong XU",
        "Lu Zhang",
        "Si Liu",
        "Dong Wang",
        "Huchuan Lu",
        "Tianfan Xue"
      ],
      "abstract": "When sound waves hit an object, they induce vibrations that produce\nhigh-frequency and subtle visual changes, which can be used for recovering the\nsound. Early studies always encounter trade-offs related to sampling rate,\nbandwidth, field of view, and the simplicity of the optical path. Recent\nadvances in event camera hardware show good potential for its application in\nvisual sound recovery, because of its superior ability in capturing\nhigh-frequency signals. However, existing event-based vibration recovery\nmethods are still sub-optimal for sound recovery. In this work, we propose a\nnovel pipeline for non-contact sound recovery, fully utilizing spatial-temporal\ninformation from the event stream. We first generate a large training set using\na novel simulation pipeline. Then we designed a network that leverages the\nsparsity of events to capture spatial information and uses Mamba to model\nlong-term temporal information. Lastly, we train a spatial aggregation block to\naggregate information from different locations to further improve signal\nquality. To capture event signals caused by sound waves, we also designed an\nimaging system using a laser matrix to enhance the gradient and collected\nmultiple data sequences for testing. Experimental results on synthetic and\nreal-world data demonstrate the effectiveness of our method.",
      "tldr_zh": "该论文提出了一种名为EvMic的非接触式声音恢复新流程，充分利用了事件流中的时空信息。首先，通过新颖的仿真流程生成大型训练数据集。然后，设计了一个网络，利用事件的稀疏性来捕获空间信息，并使用Mamba模型来建模长期时间信息。最后，训练一个空间聚合块来聚合来自不同位置的信息，以进一步提高信号质量。为了捕获由声波引起的事件信号，还设计了一种使用激光矩阵的成像系统来增强梯度，并收集了多个数据序列用于测试。在合成和真实世界数据上的实验结果表明了该方法的有效性。\n",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Our project page: https://yyzq1.github.io/EvMic/",
      "pdf_url": "http://arxiv.org/pdf/2504.02402v1",
      "published_date": "2025-04-03 08:51:17 UTC",
      "updated_date": "2025-04-03 08:51:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:12:22.288372"
    },
    {
      "arxiv_id": "2504.02388v1",
      "title": "Steiner Traveling Salesman Problem with Quantum Annealing",
      "title_zh": "基于量子退火的 Steiner 旅行商问题\n",
      "authors": [
        "Alessia Ciacco",
        "Francesca Guerriero",
        "Eneko Osaba"
      ],
      "abstract": "The Steiner Traveling Salesman Problem (STSP) is a variant of the classical\nTraveling Salesman Problem. The STSP involves incorporating steiner nodes,\nwhich are extra nodes not originally part of the required visit set but that\ncan be added to the route to enhance the overall solution and minimize the\ntotal travel cost. Given the NP-hard nature of the STSP, we propose a quantum\napproach to address it. Specifically, we employ quantum annealing using\nD-Wave's hardware to explore its potential for solving this problem. To enhance\ncomputational feasibility, we develop a preprocessing method that effectively\nreduces the network size. Our experimental results demonstrate that this\nreduction technique significantly decreases the problem complexity, making the\nQuadratic Unconstrained Binary Optimization formulation, the standard input for\nquantum annealers, better suited for existing quantum hardware. Furthermore,\nthe results highlight the potential of quantum annealing as a promising and\ninnovative approach for solving the STSP.",
      "tldr_zh": "本文研究了Steiner旅行商问题(STSP)，该问题是经典旅行商问题的一个变体，允许在路径中加入Steiner节点以优化总旅行成本。由于STSP的NP-hard特性，本文提出了一种基于量子退火的解决方案，利用D-Wave的硬件来探索其潜力。为了提高计算可行性，开发了一种预处理方法来有效减小网络规模。实验结果表明，该方法显著降低了问题复杂度，使得二次无约束二元优化(QUBO)公式更适合现有的量子硬件，并突显了量子退火作为解决STSP的一种有前景的创新方法。\n",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "quant-ph",
      "comment": "7 pages, 1 figure, 6 tables. Paper submitted to The Genetic and\n  Evolutionary Computation Conference (GECCO 2025)",
      "pdf_url": "http://arxiv.org/pdf/2504.02388v1",
      "published_date": "2025-04-03 08:29:57 UTC",
      "updated_date": "2025-04-03 08:29:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:12:34.362139"
    },
    {
      "arxiv_id": "2504.02382v1",
      "title": "Benchmark of Segmentation Techniques for Pelvic Fracture in CT and X-ray: Summary of the PENGWIN 2024 Challenge",
      "title_zh": "CT 和 X 射线骨盆骨折分割技术基准：PENGWIN 2024 挑战赛总结\n",
      "authors": [
        "Yudi Sang",
        "Yanzhen Liu",
        "Sutuke Yibulayimu",
        "Yunning Wang",
        "Benjamin D. Killeen",
        "Mingxu Liu",
        "Ping-Cheng Ku",
        "Ole Johannsen",
        "Karol Gotkowski",
        "Maximilian Zenk",
        "Klaus Maier-Hein",
        "Fabian Isensee",
        "Peiyan Yue",
        "Yi Wang",
        "Haidong Yu",
        "Zhaohong Pan",
        "Yutong He",
        "Xiaokun Liang",
        "Daiqi Liu",
        "Fuxin Fan",
        "Artur Jurgas",
        "Andrzej Skalski",
        "Yuxi Ma",
        "Jing Yang",
        "Szymon Płotka",
        "Rafał Litka",
        "Gang Zhu",
        "Yingchun Song",
        "Mathias Unberath",
        "Mehran Armand",
        "Dan Ruan",
        "S. Kevin Zhou",
        "Qiyong Cao",
        "Chunpeng Zhao",
        "Xinbao Wu",
        "Yu Wang"
      ],
      "abstract": "The segmentation of pelvic fracture fragments in CT and X-ray images is\ncrucial for trauma diagnosis, surgical planning, and intraoperative guidance.\nHowever, accurately and efficiently delineating the bone fragments remains a\nsignificant challenge due to complex anatomy and imaging limitations. The\nPENGWIN challenge, organized as a MICCAI 2024 satellite event, aimed to advance\nautomated fracture segmentation by benchmarking state-of-the-art algorithms on\nthese complex tasks. A diverse dataset of 150 CT scans was collected from\nmultiple clinical centers, and a large set of simulated X-ray images was\ngenerated using the DeepDRR method. Final submissions from 16 teams worldwide\nwere evaluated under a rigorous multi-metric testing scheme. The top-performing\nCT algorithm achieved an average fragment-wise intersection over union (IoU) of\n0.930, demonstrating satisfactory accuracy. However, in the X-ray task, the\nbest algorithm attained an IoU of 0.774, highlighting the greater challenges\nposed by overlapping anatomical structures. Beyond the quantitative evaluation,\nthe challenge revealed methodological diversity in algorithm design. Variations\nin instance representation, such as primary-secondary classification versus\nboundary-core separation, led to differing segmentation strategies. Despite\npromising results, the challenge also exposed inherent uncertainties in\nfragment definition, particularly in cases of incomplete fractures. These\nfindings suggest that interactive segmentation approaches, integrating human\ndecision-making with task-relevant information, may be essential for improving\nmodel reliability and clinical applicability.",
      "tldr_zh": "PENGWIN 2024 挑战赛旨在评估CT和X光图像中骨盆骨折分割算法的性能，以推动创伤诊断、手术计划和术中指导的自动化。该挑战赛使用包含150个CT扫描和大量模拟X光图像的数据集，并对来自全球16个团队的算法进行了评估。在CT图像分割任务中，最佳算法实现了0.930的平均 fragment-wise IoU，而在X光图像分割任务中，最佳算法的IoU为0.774，表明X光图像分割更具挑战性。挑战赛结果表明，不同的 instance representation 方法导致了不同的分割策略，同时也揭示了 fragment 定义中固有的不确定性。该研究建议，集成人类决策和任务相关信息的交互式分割方法可能对提高模型可靠性和临床适用性至关重要。\n",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "PENGWIN 2024 Challenge Report",
      "pdf_url": "http://arxiv.org/pdf/2504.02382v1",
      "published_date": "2025-04-03 08:19:36 UTC",
      "updated_date": "2025-04-03 08:19:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:12:46.699606"
    },
    {
      "arxiv_id": "2504.02351v1",
      "title": "Agglomerating Large Vision Encoders via Distillation for VFSS Segmentation",
      "title_zh": "通过蒸馏聚合大型视觉编码器用于 VFSS 分割\n",
      "authors": [
        "Chengxi Zeng",
        "Yuxuan Jiang",
        "Fan Zhang",
        "Alberto Gambaruto",
        "Tilo Burghardt"
      ],
      "abstract": "The deployment of foundation models for medical imaging has demonstrated\nconsiderable success. However, their training overheads associated with\ndownstream tasks remain substantial due to the size of the image encoders\nemployed, and the inference complexity is also significantly high. Although\nlightweight variants have been obtained for these foundation models, their\nperformance is constrained by their limited model capacity and suboptimal\ntraining strategies. In order to achieve an improved tradeoff between\ncomplexity and performance, we propose a new framework to improve the\nperformance of low complexity models via knowledge distillation from multiple\nlarge medical foundation models (e.g., MedSAM, RAD-DINO, MedCLIP), each\nspecializing in different vision tasks, with the goal to effectively bridge the\nperformance gap for medical image segmentation tasks. The agglomerated model\ndemonstrates superior generalization across 12 segmentation tasks, whereas\nspecialized models require explicit training for each task. Our approach\nachieved an average performance gain of 2\\% in Dice coefficient compared to\nsimple distillation.",
      "tldr_zh": "本文提出了一种通过知识蒸馏聚合大型视觉编码器的方法，用于腹腔镜下血管荧光造影(VFSS)分割。该方法旨在解决医学影像领域应用大型基础模型时，下游任务训练开销大和推理复杂度高的问题。通过从多个大型医学基础模型（如MedSAM、RAD-DINO、MedCLIP）中进行知识蒸馏，将它们在不同视觉任务上的优势整合到一个低复杂度的模型中，从而提升模型在医学图像分割任务上的性能。实验结果表明，该聚合模型在12个分割任务中表现出更强的泛化能力，相比简单蒸馏，Dice系数平均提高了2%。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02351v1",
      "published_date": "2025-04-03 07:38:09 UTC",
      "updated_date": "2025-04-03 07:38:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:12:58.401829"
    },
    {
      "arxiv_id": "2504.02317v1",
      "title": "Temporal Gaussian Copula For Clinical Multivariate Time Series Data Imputation",
      "title_zh": "用于临床多元时间序列数据插补的时间高斯 Copula 模型\n",
      "authors": [
        "Ye Su",
        "Hezhe Qiao",
        "Di Wu",
        "Yuwen Chen",
        "Lin Chen"
      ],
      "abstract": "The imputation of the Multivariate time series (MTS) is particularly\nchallenging since the MTS typically contains irregular patterns of missing\nvalues due to various factors such as instrument failures, interference from\nirrelevant data, and privacy regulations. Existing statistical methods and deep\nlearning methods have shown promising results in time series imputation. In\nthis paper, we propose a Temporal Gaussian Copula Model (TGC) for three-order\nMTS imputation. The key idea is to leverage the Gaussian Copula to explore the\ncross-variable and temporal relationships based on the latent Gaussian\nrepresentation. Subsequently, we employ an Expectation-Maximization (EM)\nalgorithm to improve robustness in managing data with varying missing rates.\nComprehensive experiments were conducted on three real-world MTS datasets. The\nresults demonstrate that our TGC substantially outperforms the state-of-the-art\nimputation methods. Additionally, the TGC model exhibits stronger robustness to\nthe varying missing ratios in the test dataset. Our code is available at\nhttps://github.com/MVL-Lab/TGC-MTS.",
      "tldr_zh": "该论文提出了一种用于临床多变量时间序列(MTS)数据插补的Temporal Gaussian Copula模型(TGC)。TGC利用Gaussian Copula探索基于潜在高斯表示的跨变量和时间关系，并采用期望最大化(EM)算法来提高处理不同缺失率数据的鲁棒性。在三个真实世界MTS数据集上的实验结果表明，TGC显著优于现有先进的插补方法，并且对测试数据集中不同的缺失率表现出更强的鲁棒性。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted in BIBM2024",
      "pdf_url": "http://arxiv.org/pdf/2504.02317v1",
      "published_date": "2025-04-03 06:44:05 UTC",
      "updated_date": "2025-04-03 06:44:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:13:10.199367"
    },
    {
      "arxiv_id": "2504.02316v1",
      "title": "ConsDreamer: Advancing Multi-View Consistency for Zero-Shot Text-to-3D Generation",
      "title_zh": "ConsDreamer：提升零样本文本到3D生成的多视角一致性\n",
      "authors": [
        "Yuan Zhou",
        "Shilong Jin",
        "Litao Hua",
        "Wanjun Lv",
        "Haoran Duan",
        "Jungong Han"
      ],
      "abstract": "Recent advances in zero-shot text-to-3D generation have revolutionized 3D\ncontent creation by enabling direct synthesis from textual descriptions. While\nstate-of-the-art methods leverage 3D Gaussian Splatting with score distillation\nto enhance multi-view rendering through pre-trained text-to-image (T2I) models,\nthey suffer from inherent view biases in T2I priors. These biases lead to\ninconsistent 3D generation, particularly manifesting as the multi-face Janus\nproblem, where objects exhibit conflicting features across views. To address\nthis fundamental challenge, we propose ConsDreamer, a novel framework that\nmitigates view bias by refining both the conditional and unconditional terms in\nthe score distillation process: (1) a View Disentanglement Module (VDM) that\neliminates viewpoint biases in conditional prompts by decoupling irrelevant\nview components and injecting precise camera parameters; and (2) a\nsimilarity-based partial order loss that enforces geometric consistency in the\nunconditional term by aligning cosine similarities with azimuth relationships.\nExtensive experiments demonstrate that ConsDreamer effectively mitigates the\nmulti-face Janus problem in text-to-3D generation, outperforming existing\nmethods in both visual quality and consistency.",
      "tldr_zh": "ConsDreamer 是一种用于零样本文本到3D生成的新框架，旨在解决现有方法中由于预训练文本到图像 (T2I) 模型固有的视角偏差而导致的多视角不一致性问题，特别是多面 Janus 问题。该框架通过改进 score distillation 过程中的条件项和非条件项来减轻视角偏差：(1) 视角解耦模块 (VDM) 通过解耦不相关的视角组件并注入精确的相机参数来消除条件提示中的视角偏差；(2) 基于相似性的偏序损失通过将余弦相似性与方位角关系对齐来加强非条件项中的几何一致性。实验表明，ConsDreamer 有效地缓解了文本到 3D 生成中的多面 Janus 问题，并在视觉质量和一致性方面优于现有方法。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 11 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.02316v1",
      "published_date": "2025-04-03 06:43:23 UTC",
      "updated_date": "2025-04-03 06:43:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:13:22.656608"
    },
    {
      "arxiv_id": "2504.02312v1",
      "title": "OmniCam: Unified Multimodal Video Generation via Camera Control",
      "title_zh": "OmniCam：通过相机控制实现统一的多模态视频生成\n",
      "authors": [
        "Xiaoda Yang",
        "Jiayang Xu",
        "Kaixuan Luan",
        "Xinyu Zhan",
        "Hongshun Qiu",
        "Shijun Shi",
        "Hao Li",
        "Shuai Yang",
        "Li Zhang",
        "Checheng Yu",
        "Cewu Lu",
        "Lixin Yang"
      ],
      "abstract": "Camera control, which achieves diverse visual effects by changing camera\nposition and pose, has attracted widespread attention. However, existing\nmethods face challenges such as complex interaction and limited control\ncapabilities. To address these issues, we present OmniCam, a unified multimodal\ncamera control framework. Leveraging large language models and video diffusion\nmodels, OmniCam generates spatio-temporally consistent videos. It supports\nvarious combinations of input modalities: the user can provide text or video\nwith expected trajectory as camera path guidance, and image or video as content\nreference, enabling precise control over camera motion. To facilitate the\ntraining of OmniCam, we introduce the OmniTr dataset, which contains a large\ncollection of high-quality long-sequence trajectories, videos, and\ncorresponding descriptions. Experimental results demonstrate that our model\nachieves state-of-the-art performance in high-quality camera-controlled video\ngeneration across various metrics.",
      "tldr_zh": "OmniCam是一个统一的多模态相机控制框架，旨在通过相机位置和姿态的控制生成多样化的视觉效果视频。该框架利用大型语言模型和视频扩散模型，生成时空一致的视频，并支持多种输入模态组合，包括文本或视频引导的相机轨迹，以及图像或视频作为内容参考。为了训练OmniCam，作者构建了包含高质量长序列轨迹、视频和描述的OmniTr数据集。实验结果表明，OmniCam在高质量相机控制视频生成方面达到了state-of-the-art的性能。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02312v1",
      "published_date": "2025-04-03 06:38:30 UTC",
      "updated_date": "2025-04-03 06:38:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:13:34.358135"
    },
    {
      "arxiv_id": "2504.02293v1",
      "title": "State-of-the-Art Translation of Text-to-Gloss using mBART : A case study of Bangla",
      "title_zh": "使用 mBART 的最先进的文本到手语翻译：孟加拉语案例研究\n",
      "authors": [
        "Sharif Md. Abdullah",
        "Abhijit Paul",
        "Shebuti Rayana",
        "Ahmedul Kabir",
        "Zarif Masud"
      ],
      "abstract": "Despite a large deaf and dumb population of 1.7 million, Bangla Sign Language\n(BdSL) remains a understudied domain. Specifically, there are no works on\nBangla text-to-gloss translation task. To address this gap, we begin by\naddressing the dataset problem. We take inspiration from grammatical rule based\ngloss generation used in Germany and American sign langauage (ASL) and adapt it\nfor BdSL. We also leverage LLM to generate synthetic data and use\nback-translation, text generation for data augmentation. With dataset prepared,\nwe started experimentation. We fine-tuned pretrained mBART-50 and\nmBERT-multiclass-uncased model on our dataset. We also trained GRU, RNN and a\nnovel seq-to-seq model with multi-head attention. We observe significant high\nperformance (ScareBLEU=79.53) with fine-tuning pretrained mBART-50 multilingual\nmodel from Facebook. We then explored why we observe such high performance with\nmBART. We soon notice an interesting property of mBART -- it was trained on\nshuffled and masked text data. And as we know, gloss form has shuffling\nproperty. So we hypothesize that mBART is inherently good at text-to-gloss\ntasks. To find support against this hypothesis, we trained mBART-50 on\nPHOENIX-14T benchmark and evaluated it with existing literature. Our mBART-50\nfinetune demonstrated State-of-the-Art performance on PHOENIX-14T benchmark,\nfar outperforming existing models in all 6 metrics (ScareBLEU = 63.89, BLEU-1 =\n55.14, BLEU-2 = 38.07, BLEU-3 = 27.13, BLEU-4 = 20.68, COMET = 0.624). Based on\nthe results, this study proposes a new paradigm for text-to-gloss task using\nmBART models. Additionally, our results show that BdSL text-to-gloss task can\ngreatly benefit from rule-based synthetic dataset.",
      "tldr_zh": "该研究针对孟加拉语手语(BdSL)文本到词语的翻译任务，提出了利用mBART模型进行迁移学习的方法。首先，研究人员借鉴德语和美国手语的语法规则，并结合LLM生成合成数据，构建了BdSL的文本到词语数据集。然后，通过在数据集上微调预训练的mBART-50模型，取得了显著的性能提升（ScareBLEU=79.53）。进一步研究发现，mBART模型因其训练方式（shuffle和mask文本数据）而天然适合文本到词语的翻译任务。为了验证这一假设，研究人员在PHOENIX-14T基准测试中微调了mBART-50，并取得了当前最优的结果(SOTA)，证明了mBART模型在文本到词语翻译任务中的优越性，并为该任务提供了一种新的范例。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Initial Version",
      "pdf_url": "http://arxiv.org/pdf/2504.02293v1",
      "published_date": "2025-04-03 05:47:51 UTC",
      "updated_date": "2025-04-03 05:47:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:13:46.902810"
    },
    {
      "arxiv_id": "2504.02285v1",
      "title": "Tree-based Models for Vertical Federated Learning: A Survey",
      "title_zh": "垂直联邦学习中的基于树的模型：一项综述\n",
      "authors": [
        "Bingchen Qian",
        "Yuexiang Xie",
        "Yaliang Li",
        "Bolin Ding",
        "Jingren Zhou"
      ],
      "abstract": "Tree-based models have achieved great success in a wide range of real-world\napplications due to their effectiveness, robustness, and interpretability,\nwhich inspired people to apply them in vertical federated learning (VFL)\nscenarios in recent years. In this paper, we conduct a comprehensive study to\ngive an overall picture of applying tree-based models in VFL, from the\nperspective of their communication and computation protocols. We categorize\ntree-based models in VFL into two types, i.e., feature-gathering models and\nlabel-scattering models, and provide a detailed discussion regarding their\ncharacteristics, advantages, privacy protection mechanisms, and applications.\nThis study also focuses on the implementation of tree-based models in VFL,\nsummarizing several design principles for better satisfying various\nrequirements from both academic research and industrial deployment. We conduct\na series of experiments to provide empirical observations on the differences\nand advances of different types of tree-based models.",
      "tldr_zh": "本文对垂直联邦学习(VFL)中基于树模型的应用进行了全面的综述。文章从通信和计算协议的角度，将VFL中的树模型分为特征聚集模型和标签分散模型两类，并详细讨论了它们的特性、优点、隐私保护机制和应用。此外，该研究还关注VFL中树模型的实现，总结了若干设计原则，以更好地满足学术研究和工业部署的各种需求。通过一系列实验，文章对不同类型树模型的差异和进展进行了实证观察。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ACM Computing Surveys (CSUR)",
      "pdf_url": "http://arxiv.org/pdf/2504.02285v1",
      "published_date": "2025-04-03 05:16:09 UTC",
      "updated_date": "2025-04-03 05:16:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:13:58.190494"
    },
    {
      "arxiv_id": "2504.02277v1",
      "title": "Beyond Conventional Transformers: The Medical X-ray Attention (MXA) Block for Improved Multi-Label Diagnosis Using Knowledge Distillation",
      "title_zh": "超越传统 Transformer：用于改进多标签诊断的医学 X 射线注意力 (MXA) 模块，并采用知识蒸馏\n",
      "authors": [
        "Amit Rand",
        "Hadi Ibrahim"
      ],
      "abstract": "Medical imaging, particularly X-ray analysis, often involves detecting\nmultiple conditions simultaneously within a single scan, making multi-label\nclassification crucial for real-world clinical applications. We present the\nMedical X-ray Attention (MXA) block, a novel attention mechanism tailored\nspecifically to address the unique challenges of X-ray abnormality detection.\nThe MXA block enhances traditional Multi-Head Self Attention (MHSA) by\nintegrating a specialized module that efficiently captures both detailed local\ninformation and broader global context. To the best of our knowledge, this is\nthe first work to propose a task-specific attention mechanism for diagnosing\nchest X-rays, as well as to attempt multi-label classification using an\nEfficient Vision Transformer (EfficientViT). By embedding the MXA block within\nthe EfficientViT architecture and employing knowledge distillation, our\nproposed model significantly improves performance on the CheXpert dataset, a\nwidely used benchmark for multi-label chest X-ray abnormality detection. Our\napproach achieves an area under the curve (AUC) of 0.85, an absolute\nimprovement of 0.19 compared to our baseline model's AUC of 0.66, corresponding\nto a substantial approximate 233% relative improvement over random guessing\n(AUC = 0.5).",
      "tldr_zh": "该论文提出了医学X射线注意力(MXA)模块，一种专门为解决X射线异常检测挑战而设计的注意力机制。MXA模块通过整合一个专用模块来增强传统的多头自注意力(MHSA)，该模块能够有效地捕获详细的局部信息和更广泛的全局上下文。研究将MXA模块嵌入到EfficientViT架构中，并采用知识蒸馏，显著提高了在CheXpert数据集上的多标签胸部X射线异常检测性能。实验结果表明，该方法实现了0.85的曲线下面积(AUC)，相比基线模型0.66的AUC，有显著提升。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 4 figures, 5 tables. For supplementary material and code,\n  see https://github.com/Hadi-M-Ibrahim/Beyond-Conventional-Transformers/",
      "pdf_url": "http://arxiv.org/pdf/2504.02277v1",
      "published_date": "2025-04-03 04:55:42 UTC",
      "updated_date": "2025-04-03 04:55:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:14:10.550531"
    },
    {
      "arxiv_id": "2504.02269v1",
      "title": "Engineering Artificial Intelligence: Framework, Challenges, and Future Direction",
      "title_zh": "人工智能工程化：框架、挑战与未来方向\n",
      "authors": [
        "Jay Lee",
        "Hanqi Su",
        "Dai-Yan Ji",
        "Takanobu Minami"
      ],
      "abstract": "Over the past ten years, the application of artificial intelligence (AI) and\nmachine learning (ML) in engineering domains has gained significant popularity,\nshowcasing their potential in data-driven contexts. However, the complexity and\ndiversity of engineering problems often require the development of\ndomain-specific AI approaches, which are frequently hindered by a lack of\nsystematic methodologies, scalability, and robustness during the development\nprocess. To address this gap, this paper introduces the \"ABCDE\" as the key\nelements of Engineering AI and proposes a unified, systematic engineering AI\necosystem framework, including eight essential layers, along with attributes,\ngoals, and applications, to guide the development and deployment of AI\nsolutions for specific engineering needs. Additionally, key challenges are\nexamined, and nine future research directions are highlighted. By providing a\ncomprehensive perspective, this paper aims to advance the strategic\nimplementation of AI, fostering the development of next-generation engineering\nAI solutions.",
      "tldr_zh": "本文针对工程领域人工智能(AI)应用中缺乏系统方法、可扩展性和鲁棒性的问题，提出了“ABCDE”关键要素，并构建了一个统一的、系统化的工程AI生态系统框架。该框架包含八个必要层级，以及属性、目标和应用，旨在指导针对特定工程需求的AI解决方案的开发和部署。此外，文章还探讨了关键挑战，并强调了九个未来的研究方向。通过提供全面的视角，本文旨在推进AI的战略实施，促进下一代工程AI解决方案的开发。\n",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02269v1",
      "published_date": "2025-04-03 04:30:10 UTC",
      "updated_date": "2025-04-03 04:30:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:14:22.315963"
    },
    {
      "arxiv_id": "2504.02260v1",
      "title": "Implicit Neural Differential Model for Spatiotemporal Dynamics",
      "title_zh": "时空动力学的隐式神经微分模型\n",
      "authors": [
        "Deepak Akhare",
        "Pan Du",
        "Tengfei Luo",
        "Jian-Xun Wang"
      ],
      "abstract": "Hybrid neural-physics modeling frameworks through differentiable programming\nhave emerged as powerful tools in scientific machine learning, enabling the\nintegration of known physics with data-driven learning to improve prediction\naccuracy and generalizability. However, most existing hybrid frameworks rely on\nexplicit recurrent formulations, which suffer from numerical instability and\nerror accumulation during long-horizon forecasting. In this work, we introduce\nIm-PiNDiff, a novel implicit physics-integrated neural differentiable solver\nfor stable and accurate modeling of spatiotemporal dynamics. Inspired by deep\nequilibrium models, Im-PiNDiff advances the state using implicit fixed-point\nlayers, enabling robust long-term simulation while remaining fully end-to-end\ndifferentiable. To enable scalable training, we introduce a hybrid gradient\npropagation strategy that integrates adjoint-state methods with reverse-mode\nautomatic differentiation. This approach eliminates the need to store\nintermediate solver states and decouples memory complexity from the number of\nsolver iterations, significantly reducing training overhead. We further\nincorporate checkpointing techniques to manage memory in long-horizon rollouts.\nNumerical experiments on various spatiotemporal PDE systems, including\nadvection-diffusion processes, Burgers' dynamics, and multi-physics chemical\nvapor infiltration processes, demonstrate that Im-PiNDiff achieves superior\npredictive performance, enhanced numerical stability, and substantial\nreductions in memory and runtime cost relative to explicit and naive implicit\nbaselines. This work provides a principled, efficient, and scalable framework\nfor hybrid neural-physics modeling.",
      "tldr_zh": "该论文提出了一种新的隐式物理集成神经微分求解器Im-PiNDiff，用于时空动力学的稳定和精确建模。Im-PiNDiff采用隐式不动点层推进状态，实现了鲁棒的长期模拟，并保持端到端可微性。为了实现可扩展的训练，论文引入了一种混合梯度传播策略，结合伴随状态法和反向模式自动微分，显著降低了训练开销。实验结果表明，Im-PiNDiff在各种时空偏微分方程系统（包括平流扩散过程、Burgers动力学和多物理场化学气相渗透过程）上，相比显式和朴素隐式基线，实现了卓越的预测性能、增强的数值稳定性和显著降低的内存和运行时间成本。该工作为混合神经物理建模提供了一个原则性、高效且可扩展的框架。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02260v1",
      "published_date": "2025-04-03 04:07:18 UTC",
      "updated_date": "2025-04-03 04:07:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:14:34.701409"
    },
    {
      "arxiv_id": "2504.02254v1",
      "title": "LLMs as Deceptive Agents: How Role-Based Prompting Induces Semantic Ambiguity in Puzzle Tasks",
      "title_zh": "LLM 作为欺骗性智能体：基于角色的提示如何诱导谜题任务中的语义模糊",
      "authors": [
        "Seunghyun Yoo"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have not only showcased\nimpressive creative capabilities but also revealed emerging agentic behaviors\nthat exploit linguistic ambiguity in adversarial settings. In this study, we\ninvestigate how an LLM, acting as an autonomous agent, leverages semantic\nambiguity to generate deceptive puzzles that mislead and challenge human users.\nInspired by the popular puzzle game \"Connections\", we systematically compare\npuzzles produced through zero-shot prompting, role-injected adversarial\nprompts, and human-crafted examples, with an emphasis on understanding the\nunderlying agent decision-making processes. Employing computational analyses\nwith HateBERT to quantify semantic ambiguity, alongside subjective human\nevaluations, we demonstrate that explicit adversarial agent behaviors\nsignificantly heighten semantic ambiguity -- thereby increasing cognitive load\nand reducing fairness in puzzle solving. These findings provide critical\ninsights into the emergent agentic qualities of LLMs and underscore important\nethical considerations for evaluating and safely deploying autonomous language\nsystems in both educational technologies and entertainment.",
      "tldr_zh": "该研究探讨了大型语言模型(LLMs)作为自主智能体时，如何利用语义模糊来生成具有欺骗性的谜题，从而误导人类用户。通过模仿“Connections”谜题游戏，研究对比了零样本提示、角色注入对抗提示和人工设计的谜题。使用HateBERT模型量化语义模糊，并结合主观人工评估，结果表明，对抗性智能体行为显著增加了语义模糊，提高了认知负荷，降低了谜题解决的公平性。该研究揭示了LLMs涌现出的智能体特性，并强调了在教育技术和娱乐领域安全部署自主语言系统的重要伦理考量。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50, 68T05, 68U35"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 5 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2504.02254v1",
      "published_date": "2025-04-03 03:45:58 UTC",
      "updated_date": "2025-04-03 03:45:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:14:46.555710"
    },
    {
      "arxiv_id": "2504.02252v1",
      "title": "Adapting World Models with Latent-State Dynamics Residuals",
      "title_zh": "利用隐状态动态残差调整世界模型\n",
      "authors": [
        "JB Lanier",
        "Kyungmin Kim",
        "Armin Karamzade",
        "Yifei Liu",
        "Ankita Sinha",
        "Kat He",
        "Davide Corsi",
        "Roy Fox"
      ],
      "abstract": "Simulation-to-reality reinforcement learning (RL) faces the critical\nchallenge of reconciling discrepancies between simulated and real-world\ndynamics, which can severely degrade agent performance. A promising approach\ninvolves learning corrections to simulator forward dynamics represented as a\nresidual error function, however this operation is impractical with\nhigh-dimensional states such as images. To overcome this, we propose ReDRAW, a\nlatent-state autoregressive world model pretrained in simulation and calibrated\nto target environments through residual corrections of latent-state dynamics\nrather than of explicit observed states. Using this adapted world model, ReDRAW\nenables RL agents to be optimized with imagined rollouts under corrected\ndynamics and then deployed in the real world. In multiple vision-based MuJoCo\ndomains and a physical robot visual lane-following task, ReDRAW effectively\nmodels changes to dynamics and avoids overfitting in low data regimes where\ntraditional transfer methods fail.",
      "tldr_zh": "该论文提出了一种名为ReDRAW的方法，用于解决模拟到现实(Sim-to-Real)强化学习中的动态差异问题。ReDRAW通过学习潜在状态动态残差来校正模拟器的前向动态，而非直接校正高维状态（如图像），从而避免了传统方法在高维状态下的不适用性。ReDRAW使用在模拟环境中预训练的自回归世界模型，并通过残差校正将其调整到目标环境。利用修正后的世界模型，强化学习智能体可以在想象的rollout中进行优化，并在真实世界中部署。实验表明，ReDRAW在多个基于视觉的MuJoCo领域和一个物理机器人视觉车道跟踪任务中，能够有效地模拟动态变化，并避免在低数据情况下过度拟合。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 11 figures. Project website at https://redraw.jblanier.net/",
      "pdf_url": "http://arxiv.org/pdf/2504.02252v1",
      "published_date": "2025-04-03 03:41:30 UTC",
      "updated_date": "2025-04-03 03:41:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:14:58.818273"
    },
    {
      "arxiv_id": "2504.02234v1",
      "title": "LLM Social Simulations Are a Promising Research Method",
      "title_zh": "LLM 社会模拟是一种很有前景的研究方法\n",
      "authors": [
        "Jacy Reese Anthis",
        "Ryan Liu",
        "Sean M. Richardson",
        "Austin C. Kozlowski",
        "Bernard Koch",
        "James Evans",
        "Erik Brynjolfsson",
        "Michael Bernstein"
      ],
      "abstract": "Accurate and verifiable large language model (LLM) simulations of human\nresearch subjects promise an accessible data source for understanding human\nbehavior and training new AI systems. However, results to date have been\nlimited, and few social scientists have adopted these methods. In this position\npaper, we argue that the promise of LLM social simulations can be achieved by\naddressing five tractable challenges. We ground our argument in a literature\nsurvey of empirical comparisons between LLMs and human research subjects,\ncommentaries on the topic, and related work. We identify promising directions\nwith prompting, fine-tuning, and complementary methods. We believe that LLM\nsocial simulations can already be used for exploratory research, such as pilot\nexperiments for psychology, economics, sociology, and marketing. More\nwidespread use may soon be possible with rapidly advancing LLM capabilities,\nand researchers should prioritize developing conceptual models and evaluations\nthat can be iteratively deployed and refined at pace with ongoing AI advances.",
      "tldr_zh": "本文认为，准确且可验证的基于大型语言模型(LLM)的人类研究对象模拟，为理解人类行为和训练新的AI系统提供了一种有前景的数据来源。尽管目前成果有限，且社会科学家对此方法采用较少，但本文提出，通过解决五个可处理的挑战，LLM社会模拟的潜力可以实现。作者通过对LLM与人类研究对象的实证比较、相关评论和相关工作的文献调查，论证了这一观点，并提出了prompting、微调和补充方法等有希望的方向。文章认为，LLM社会模拟已经可以用于探索性研究，例如心理学、经济学、社会学和市场营销的初步实验。随着LLM能力的快速发展，更广泛的应用可能很快实现，研究人员应优先开发概念模型和评估方法，以便与正在进行的AI进展同步迭代部署和改进。\n",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02234v1",
      "published_date": "2025-04-03 03:01:26 UTC",
      "updated_date": "2025-04-03 03:01:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:15:10.811852"
    },
    {
      "arxiv_id": "2504.02231v1",
      "title": "AC-LoRA: Auto Component LoRA for Personalized Artistic Style Image Generation",
      "title_zh": "AC-LoRA：用于个性化艺术风格图像生成的自适应组件 LoRA\n",
      "authors": [
        "Zhipu Cui",
        "Andong Tian",
        "Zhi Ying",
        "Jialiang Lu"
      ],
      "abstract": "Personalized image generation allows users to preserve styles or subjects of\na provided small set of images for further image generation. With the\nadvancement in large text-to-image models, many techniques have been developed\nto efficiently fine-tune those models for personalization, such as Low Rank\nAdaptation (LoRA). However, LoRA-based methods often face the challenge of\nadjusting the rank parameter to achieve satisfactory results. To address this\nchallenge, AutoComponent-LoRA (AC-LoRA) is proposed, which is able to\nautomatically separate the signal component and noise component of the LoRA\nmatrices for fast and efficient personalized artistic style image generation.\nThis method is based on Singular Value Decomposition (SVD) and dynamic\nheuristics to update the hyperparameters during training. Superior performance\nover existing methods in overcoming model underfitting or overfitting problems\nis demonstrated. The results were validated using FID, CLIP, DINO, and\nImageReward, achieving an average of 9% improvement.",
      "tldr_zh": "该论文提出了AutoComponent-LoRA (AC-LoRA)，一种用于个性化艺术风格图像生成的LoRA改进方法。AC-LoRA旨在解决传统LoRA方法中rank参数调整困难的问题，通过奇异值分解(SVD)自动分离LoRA矩阵中的信号和噪声成分，并使用动态启发式算法更新训练过程中的超参数。实验结果表明，AC-LoRA在克服模型欠拟合和过拟合方面优于现有方法，在FID、CLIP、DINO和ImageReward指标上平均提升了9%。该方法能够实现快速高效的个性化艺术风格图像生成。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T05, 68U10",
        "I.2.6; I.4.0"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 4 figures, ICCGV 2025, SPIE",
      "pdf_url": "http://arxiv.org/pdf/2504.02231v1",
      "published_date": "2025-04-03 02:56:01 UTC",
      "updated_date": "2025-04-03 02:56:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:15:22.442437"
    },
    {
      "arxiv_id": "2504.02227v1",
      "title": "VEGAS: Towards Visually Explainable and Grounded Artificial Social Intelligence",
      "title_zh": "VEGAS：迈向视觉可解释和可溯源的人工社会智能\n",
      "authors": [
        "Hao Li",
        "Hao Fei",
        "Zechao Hu",
        "Zhengwei Yang",
        "Zheng Wang"
      ],
      "abstract": "Social Intelligence Queries (Social-IQ) serve as the primary multimodal\nbenchmark for evaluating a model's social intelligence level. While impressive\nmultiple-choice question(MCQ) accuracy is achieved by current solutions,\nincreasing evidence shows that they are largely, and in some cases entirely,\ndependent on language modality, overlooking visual context. Additionally, the\nclosed-set nature further prevents the exploration of whether and to what\nextent the reasoning path behind selection is correct. To address these\nlimitations, we propose the Visually Explainable and Grounded Artificial Social\nIntelligence (VEGAS) model. As a generative multimodal model, VEGAS leverages\nopen-ended answering to provide explainable responses, which enhances the\nclarity and evaluation of reasoning paths. To enable visually grounded\nanswering, we propose a novel sampling strategy to provide the model with more\nrelevant visual frames. We then enhance the model's interpretation of these\nframes through Generalist Instruction Fine-Tuning (GIFT), which aims to: i)\nlearn multimodal-language transformations for fundamental emotional social\ntraits, and ii) establish multimodal joint reasoning capabilities. Extensive\nexperiments, comprising modality ablation, open-ended assessments, and\nsupervised MCQ evaluations, consistently show that VEGAS effectively utilizes\nvisual information in reasoning to produce correct and also credible answers.\nWe expect this work to of fer a new perspective on Social-IQ and advance the\ndevelopment of human-like social AI.",
      "tldr_zh": "该论文提出了Visually Explainable and Grounded Artificial Social Intelligence (VEGAS)模型，旨在解决现有Social-IQ模型过度依赖语言信息、忽略视觉上下文的问题。VEGAS是一个生成式多模态模型，通过开放式回答提供可解释的推理路径。为了实现视觉信息 grounding，论文提出了一种新的采样策略，为模型提供更相关的视觉帧，并通过Generalist Instruction Fine-Tuning (GIFT)增强模型对这些帧的理解，从而学习多模态-语言转换和建立多模态联合推理能力。实验表明，VEGAS能够有效利用视觉信息进行推理，产生正确且可信的答案，为类人社交AI的发展提供新的视角。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 5 figures, AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.02227v1",
      "published_date": "2025-04-03 02:48:21 UTC",
      "updated_date": "2025-04-03 02:48:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:15:34.622380"
    },
    {
      "arxiv_id": "2504.02221v1",
      "title": "Learning and Improving Backgammon Strategy",
      "title_zh": "学习和改进西洋双陆棋策略\n",
      "authors": [
        "Gregory R. Galperin"
      ],
      "abstract": "A novel approach to learning is presented, combining features of on-line and\noff-line methods to achieve considerable performance in the task of learning a\nbackgammon value function in a process that exploits the processing power of\nparallel supercomputers. The off-line methods comprise a set of techniques for\nparallelizing neural network training and $TD(\\lambda)$ reinforcement learning;\nhere Monte-Carlo ``Rollouts'' are introduced as a massively parallel on-line\npolicy improvement technique which applies resources to the decision points\nencountered during the search of the game tree to further augment the learned\nvalue function estimate. A level of play roughly as good as, or possibly better\nthan, the current champion human and computer backgammon players has been\nachieved in a short period of learning.",
      "tldr_zh": "该论文提出了一种新的学习方法，结合了在线和离线方法的特点，用于学习backgammon的价值函数，并利用并行超级计算机的处理能力。离线方法包括并行化神经网络训练和$TD(\\lambda)$强化学习的技术。引入了蒙特卡洛“Rollouts”作为一种大规模并行的在线策略改进技术，将资源应用于游戏树搜索中遇到的决策点，以进一步增强学习到的价值函数估计。在短时间内，该方法达到了与当前backgammon冠军人类和计算机玩家大致相同或可能更好的水平。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "Accompanied by oral presentation by Gregory Galperin at the CBCL\n  Learning Day 1994",
      "pdf_url": "http://arxiv.org/pdf/2504.02221v1",
      "published_date": "2025-04-03 02:27:22 UTC",
      "updated_date": "2025-04-03 02:27:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:15:46.473354"
    },
    {
      "arxiv_id": "2504.02211v1",
      "title": "FT-Transformer: Resilient and Reliable Transformer with End-to-End Fault Tolerant Attention",
      "title_zh": "FT-Transformer：具有端到端容错注意力的弹性可靠 Transformer\n",
      "authors": [
        "Huangliang Dai",
        "Shixun Wu",
        "Hairui Zhao",
        "Jiajun Huang",
        "Zizhe Jian",
        "Yue Zhu",
        "Haiyang Hu",
        "Zizhong Chen"
      ],
      "abstract": "Transformer models leverage self-attention mechanisms to capture complex\ndependencies, demonstrating exceptional performance in various applications.\nHowever, the long-duration high-load computations required for model inference\nimpose stringent reliability demands on the computing platform, as soft errors\nthat occur during execution can significantly degrade model performance.\nExisting fault tolerance methods protect each operation separately using\ndecoupled kernels, incurring substantial computational and memory overhead. In\nthis paper, we propose a novel error-resilient framework for Transformer\nmodels, integrating end-to-end fault tolerant attention (EFTA) to improve\ninference reliability against soft errors. Our approach enables error detection\nand correction within a fully fused attention kernel, reducing redundant data\naccess and thereby mitigating memory faults. To further enhance error coverage\nand reduce overhead, we design a hybrid fault tolerance scheme tailored for the\nEFTA, introducing for the first time: 1) architecture-aware algorithm-based\nfault tolerance (ABFT) using tensor checksum, which minimizes inter-thread\ncommunication overhead on tensor cores during error detection; 2) selective\nneuron value restriction, which selectively applies adaptive fault tolerance\nconstraints to neuron values, balancing error coverage and overhead; 3) unified\nverification, reusing checksums to streamline multiple computation steps into a\nsingle verification process. Experimental results show that EFTA achieves up to\n7.56x speedup over traditional methods with an average fault tolerance overhead\nof 13.9%.",
      "tldr_zh": "该论文提出了一个名为FT-Transformer的容错Transformer框架，旨在提高Transformer模型在推理过程中抵抗软错误的可靠性。核心是端到端容错注意力机制(EFTA)，它在完全融合的注意力kernel中实现错误检测和纠正，减少冗余数据访问。此外，论文还引入了架构感知的基于算法的容错(ABFT)机制，利用张量校验和减少线程间通信开销，并采用选择性神经元值限制和统一验证来进一步提高容错能力并降低开销。实验结果表明，EFTA相比传统方法实现了高达7.56倍的加速，平均容错开销为13.9%。\n",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02211v1",
      "published_date": "2025-04-03 02:05:08 UTC",
      "updated_date": "2025-04-03 02:05:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:15:58.869600"
    },
    {
      "arxiv_id": "2504.02199v1",
      "title": "ESC: Erasing Space Concept for Knowledge Deletion",
      "title_zh": "ESC：擦除空间概念以实现知识删除\n",
      "authors": [
        "Tae-Young Lee",
        "Sundong Park",
        "Minwoo Jeon",
        "Hyoseok Hwang",
        "Gyeong-Moon Park"
      ],
      "abstract": "As concerns regarding privacy in deep learning continue to grow, individuals\nare increasingly apprehensive about the potential exploitation of their\npersonal knowledge in trained models. Despite several research efforts to\naddress this, they often fail to consider the real-world demand from users for\ncomplete knowledge erasure. Furthermore, our investigation reveals that\nexisting methods have a risk of leaking personal knowledge through embedding\nfeatures. To address these issues, we introduce a novel concept of Knowledge\nDeletion (KD), an advanced task that considers both concerns, and provides an\nappropriate metric, named Knowledge Retention score (KR), for assessing\nknowledge retention in feature space. To achieve this, we propose a novel\ntraining-free erasing approach named Erasing Space Concept (ESC), which\nrestricts the important subspace for the forgetting knowledge by eliminating\nthe relevant activations in the feature. In addition, we suggest ESC with\nTraining (ESC-T), which uses a learnable mask to better balance the trade-off\nbetween forgetting and preserving knowledge in KD. Our extensive experiments on\nvarious datasets and models demonstrate that our proposed methods achieve the\nfastest and state-of-the-art performance. Notably, our methods are applicable\nto diverse forgetting scenarios, such as facial domain setting, demonstrating\nthe generalizability of our methods. The code is available at\nhttp://github.com/KU-VGI/ESC .",
      "tldr_zh": "该论文提出了“知识删除”(Knowledge Deletion, KD) 的概念，旨在解决深度学习模型中个人知识泄露的问题。现有方法存在通过嵌入特征泄露个人知识的风险，且未能充分考虑用户对完全知识删除的实际需求。为此，论文提出了一种名为“擦除空间概念”(Erasing Space Concept, ESC) 的无训练擦除方法，通过消除特征中的相关激活来限制遗忘知识的重要子空间。此外，还提出了ESC-T，利用可学习的掩码来平衡KD中遗忘和保留知识之间的权衡。实验结果表明，所提出的方法在各种数据集和模型上实现了最快和最先进的性能，并具有良好的泛化能力。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "22 pages, 14 figures, 18 tables, CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.02199v1",
      "published_date": "2025-04-03 00:53:09 UTC",
      "updated_date": "2025-04-03 00:53:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:16:10.822515"
    },
    {
      "arxiv_id": "2504.02193v1",
      "title": "More is Less: The Pitfalls of Multi-Model Synthetic Preference Data in DPO Safety Alignment",
      "title_zh": "多多益损：DPO安全对齐中多模型合成偏好数据的陷阱\n",
      "authors": [
        "Yifan Wang",
        "Runjin Chen",
        "Bolian Li",
        "David Cho",
        "Yihe Deng",
        "Ruqi Zhang",
        "Tianlong Chen",
        "Zhangyang Wang",
        "Ananth Grama",
        "Junyuan Hong"
      ],
      "abstract": "Aligning large language models (LLMs) with human values is an increasingly\ncritical step in post-training. Direct Preference Optimization (DPO) has\nemerged as a simple, yet effective alternative to reinforcement learning from\nhuman feedback (RLHF). Synthetic preference data with its low cost and high\nquality enable effective alignment through single- or multi-model generated\npreference data. Our study reveals a striking, safety-specific phenomenon\nassociated with DPO alignment: Although multi-model generated data enhances\nperformance on general tasks (ARC, Hellaswag, MMLU, TruthfulQA, Winogrande) by\nproviding diverse responses, it also tends to facilitate reward hacking during\ntraining. This can lead to a high attack success rate (ASR) when models\nencounter jailbreaking prompts. The issue is particularly pronounced when\nemploying stronger models like GPT-4o or larger models in the same family to\ngenerate chosen responses paired with target model self-generated rejected\nresponses, resulting in dramatically poorer safety outcomes. Furthermore, with\nrespect to safety, using solely self-generated responses (single-model\ngeneration) for both chosen and rejected pairs significantly outperforms\nconfigurations that incorporate responses from stronger models, whether used\ndirectly as chosen data or as part of a multi-model response pool. We\ndemonstrate that multi-model preference data exhibits high linear separability\nbetween chosen and rejected responses, which allows models to exploit\nsuperficial cues rather than internalizing robust safety constraints. Our\nexperiments, conducted on models from the Llama, Mistral, and Qwen families,\nconsistently validate these findings.",
      "tldr_zh": "该研究探讨了使用多模型合成偏好数据进行DPO安全对齐的潜在问题。研究发现，虽然多模型生成数据可以通过提供多样化的响应来提高通用任务的性能，但它也容易导致训练过程中的奖励利用(reward hacking)，从而提高模型在面对越狱提示时的攻击成功率(ASR)。特别是使用更强大的模型（如GPT-4o）或同一系列中更大的模型生成chosen响应，而目标模型生成rejected响应时，安全效果会显著降低。相反，仅使用自生成响应（单模型生成）的DPO训练在安全性方面优于使用多模型数据的配置。研究表明，多模型偏好数据在chosen和rejected响应之间表现出高度的线性可分性，这使得模型能够利用表面线索而不是内化强大的安全约束。Llama、Mistral和Qwen系列模型的实验验证了这些发现。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.02193v1",
      "published_date": "2025-04-03 00:36:40 UTC",
      "updated_date": "2025-04-03 00:36:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-05T02:16:23.158096"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 76,
  "processed_papers_count": 76,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-04-05T02:17:57.916921"
}
[
  {
    "arxiv_id": "2505.03739v1",
    "title": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model",
    "authors": [
      "Zuwei Long",
      "Yunhang Shen",
      "Chaoyou Fu",
      "Heting Gao",
      "Lijiang Li",
      "Peixian Chen",
      "Mengdan Zhang",
      "Hang Shao",
      "Jian Li",
      "Jinlong Peng",
      "Haoyu Cao",
      "Ke Li",
      "Rongrong Ji",
      "Xing Sun"
    ],
    "abstract": "With the growing requirement for natural human-computer interaction,\nspeech-based systems receive increasing attention as speech is one of the most\ncommon forms of daily communication. However, the existing speech models still\nexperience high latency when generating the first audio token during streaming,\nwhich poses a significant bottleneck for deployment. To address this issue, we\npropose VITA-Audio, an end-to-end large speech model with fast audio-text token\ngeneration. Specifically, we introduce a lightweight Multiple Cross-modal Token\nPrediction (MCTP) module that efficiently generates multiple audio tokens\nwithin a single model forward pass, which not only accelerates the inference\nbut also significantly reduces the latency for generating the first audio in\nstreaming scenarios. In addition, a four-stage progressive training strategy is\nexplored to achieve model acceleration with minimal loss of speech quality. To\nour knowledge, VITA-Audio is the first multi-modal large language model capable\nof generating audio output during the first forward pass, enabling real-time\nconversational capabilities with minimal latency. VITA-Audio is fully\nreproducible and is trained on open-source data only. Experimental results\ndemonstrate that our model achieves an inference speedup of 3~5x at the 7B\nparameter scale, but also significantly outperforms open-source models of\nsimilar model size on multiple benchmarks for automatic speech recognition\n(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Training and Inference Codes: https://github.com/VITA-MLLM/VITA-Audio",
    "pdf_url": "http://arxiv.org/pdf/2505.03739v1",
    "published_date": "2025-05-06 17:59:53 UTC",
    "updated_date": "2025-05-06 17:59:53 UTC"
  },
  {
    "arxiv_id": "2505.03738v1",
    "title": "AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control",
    "authors": [
      "Jialong Li",
      "Xuxin Cheng",
      "Tianshu Huang",
      "Shiqi Yang",
      "Ri-Zhao Qiu",
      "Xiaolong Wang"
    ],
    "abstract": "Humanoid robots derive much of their dexterity from hyper-dexterous\nwhole-body movements, enabling tasks that require a large operational\nworkspace: such as picking objects off the ground. However, achieving these\ncapabilities on real humanoids remains challenging due to their high degrees of\nfreedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization\n(AMO), a framework that integrates sim-to-real reinforcement learning (RL) with\ntrajectory optimization for real-time, adaptive whole-body control. To mitigate\ndistribution bias in motion imitation RL, we construct a hybrid AMO dataset and\ntrain a network capable of robust, on-demand adaptation to potentially O.O.D.\ncommands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid\nrobot, demonstrating superior stability and an expanded workspace compared to\nstrong baselines. Finally, we show that AMO's consistent performance supports\nautonomous task execution via imitation learning, underscoring the system's\nversatility and robustness.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "website: https://amo-humanoid.github.io",
    "pdf_url": "http://arxiv.org/pdf/2505.03738v1",
    "published_date": "2025-05-06 17:59:51 UTC",
    "updated_date": "2025-05-06 17:59:51 UTC"
  },
  {
    "arxiv_id": "2505.03730v1",
    "title": "FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios",
    "authors": [
      "Shiyi Zhang",
      "Junhao Zhuang",
      "Zhaoyang Zhang",
      "Ying Shan",
      "Yansong Tang"
    ],
    "abstract": "Action customization involves generating videos where the subject performs\nactions dictated by input control signals. Current methods use pose-guided or\nglobal motion customization but are limited by strict constraints on spatial\nstructure, such as layout, skeleton, and viewpoint consistency, reducing\nadaptability across diverse subjects and scenarios. To overcome these\nlimitations, we propose FlexiAct, which transfers actions from a reference\nvideo to an arbitrary target image. Unlike existing methods, FlexiAct allows\nfor variations in layout, viewpoint, and skeletal structure between the subject\nof the reference video and the target image, while maintaining identity\nconsistency. Achieving this requires precise action control, spatial structure\nadaptation, and consistency preservation. To this end, we introduce RefAdapter,\na lightweight image-conditioned adapter that excels in spatial adaptation and\nconsistency preservation, surpassing existing methods in balancing appearance\nconsistency and structural flexibility. Additionally, based on our\nobservations, the denoising process exhibits varying levels of attention to\nmotion (low frequency) and appearance details (high frequency) at different\ntimesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike\nexisting methods that rely on separate spatial-temporal architectures, directly\nachieves action extraction during the denoising process. Experiments\ndemonstrate that our method effectively transfers actions to subjects with\ndiverse layouts, skeletons, and viewpoints. We release our code and model\nweights to support further research at\nhttps://shiyi-zh0408.github.io/projectpages/FlexiAct/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by Siggraph2025, Project Page:\n  https://shiyi-zh0408.github.io/projectpages/FlexiAct/",
    "pdf_url": "http://arxiv.org/pdf/2505.03730v1",
    "published_date": "2025-05-06 17:58:02 UTC",
    "updated_date": "2025-05-06 17:58:02 UTC"
  },
  {
    "arxiv_id": "2505.03710v1",
    "title": "Actor-Critics Can Achieve Optimal Sample Efficiency",
    "authors": [
      "Kevin Tan",
      "Wei Fan",
      "Yuting Wei"
    ],
    "abstract": "Actor-critic algorithms have become a cornerstone in reinforcement learning\n(RL), leveraging the strengths of both policy-based and value-based methods.\nDespite recent progress in understanding their statistical efficiency, no\nexisting work has successfully learned an $\\epsilon$-optimal policy with a\nsample complexity of $O(1/\\epsilon^2)$ trajectories with general function\napproximation when strategic exploration is necessary.\n  We address this open problem by introducing a novel actor-critic algorithm\nthat attains a sample-complexity of $O(dH^5 \\log|\\mathcal{A}|/\\epsilon^2 + d\nH^4 \\log|\\mathcal{F}|/ \\epsilon^2)$ trajectories, and accompanying $\\sqrt{T}$\nregret when the Bellman eluder dimension $d$ does not increase with $T$ at more\nthan a $\\log T$ rate.\n  Here, $\\mathcal{F}$ is the critic function class, $\\mathcal{A}$ is the action\nspace, and $H$ is the horizon in the finite horizon MDP setting. Our algorithm\nintegrates optimism, off-policy critic estimation targeting the optimal\nQ-function, and rare-switching policy resets.\n  We extend this to the setting of Hybrid RL, showing that initializing the\ncritic with offline data yields sample efficiency gains compared to purely\noffline or online RL. Further, utilizing access to offline data, we provide a\n\\textit{non-optimistic} provably efficient actor-critic algorithm that only\nadditionally requires $N_{\\text{off}} \\geq c_{\\text{off}}^*dH^4/\\epsilon^2$ in\nexchange for omitting optimism, where $c_{\\text{off}}^*$ is the single-policy\nconcentrability coefficient and $N_{\\text{off}}$ is the number of offline\nsamples. This addresses another open problem in the literature. We further\nprovide numerical experiments to support our theoretical findings.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "Accepted to ICML 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.03710v1",
    "published_date": "2025-05-06 17:32:39 UTC",
    "updated_date": "2025-05-06 17:32:39 UTC"
  },
  {
    "arxiv_id": "2505.03694v1",
    "title": "Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and Avoid",
    "authors": [
      "Parv Kapoor",
      "Ian Higgins",
      "Nikhil Keetha",
      "Jay Patrikar",
      "Brady Moon",
      "Zelin Ye",
      "Yao He",
      "Ivan Cisneros",
      "Yaoyu Hu",
      "Changliu Liu",
      "Eunsuk Kang",
      "Sebastian Scherer"
    ],
    "abstract": "Assured safe-separation is essential for achieving seamless high-density\noperation of airborne vehicles in a shared airspace. To equip\nresource-constrained aerial systems with this safety-critical capability, we\npresent ViSafe, a high-speed vision-only airborne collision avoidance system.\nViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by\ntightly integrating a learning-based edge-AI framework with a custom\nmulti-camera hardware prototype designed under SWaP-C constraints. By\nleveraging perceptual input-focused control barrier functions (CBF) to design,\nencode, and enforce safety thresholds, ViSafe can provide provably safe runtime\nguarantees for self-separation in high-speed aerial operations. We evaluate\nViSafe's performance through an extensive test campaign involving both\nsimulated digital twins and real-world flight scenarios. By independently\nvarying agent types, closure rates, interaction geometries, and environmental\nconditions (e.g., weather and lighting), we demonstrate that ViSafe\nconsistently ensures self-separation across diverse scenarios. In\nfirst-of-its-kind real-world high-speed collision avoidance tests with closure\nrates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous\ncollision avoidance, establishing a new standard for safety in high-speed\naerial navigation.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "13 pages, RSS 2025 Demo track",
    "pdf_url": "http://arxiv.org/pdf/2505.03694v1",
    "published_date": "2025-05-06 16:59:54 UTC",
    "updated_date": "2025-05-06 16:59:54 UTC"
  },
  {
    "arxiv_id": "2505.03678v1",
    "title": "Graph Drawing for LLMs: An Empirical Evaluation",
    "authors": [
      "Walter Didimo",
      "Fabrizio Montecchiani",
      "Tommaso Piselli"
    ],
    "abstract": "Our work contributes to the fast-growing literature on the use of Large\nLanguage Models (LLMs) to perform graph-related tasks. In particular, we focus\non usage scenarios that rely on the visual modality, feeding the model with a\ndrawing of the graph under analysis. We investigate how the model's performance\nis affected by the chosen layout paradigm, the aesthetics of the drawing, and\nthe prompting technique used for the queries. We formulate three corresponding\nresearch questions and present the results of a thorough experimental analysis.\nOur findings reveal that choosing the right layout paradigm and optimizing the\nreadability of the input drawing from a human perspective can significantly\nimprove the performance of the model on the given task. Moreover, selecting the\nmost effective prompting technique is a challenging yet crucial task for\nachieving optimal performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03678v1",
    "published_date": "2025-05-06 16:23:42 UTC",
    "updated_date": "2025-05-06 16:23:42 UTC"
  },
  {
    "arxiv_id": "2505.03674v1",
    "title": "Gap the (Theory of) Mind: Sharing Beliefs About Teammates' Goals Boosts Collaboration Perception, Not Performance",
    "authors": [
      "Yotam Amitai",
      "Reuth Mirsky",
      "Ofra Amir"
    ],
    "abstract": "In human-agent teams, openly sharing goals is often assumed to enhance\nplanning, collaboration, and effectiveness. However, direct communication of\nthese goals is not always feasible, requiring teammates to infer their\npartner's intentions through actions. Building on this, we investigate whether\nan AI agent's ability to share its inferred understanding of a human teammate's\ngoals can improve task performance and perceived collaboration. Through an\nexperiment comparing three conditions-no recognition (NR), viable goals (VG),\nand viable goals on-demand (VGod) - we find that while goal-sharing information\ndid not yield significant improvements in task performance or overall\nsatisfaction scores, thematic analysis suggests that it supported strategic\nadaptations and subjective perceptions of collaboration. Cognitive load\nassessments revealed no additional burden across conditions, highlighting the\nchallenge of balancing informativeness and simplicity in human-agent\ninteractions. These findings highlight the nuanced trade-off of goal-sharing:\nwhile it fosters trust and enhances perceived collaboration, it can\noccasionally hinder objective performance gains.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03674v1",
    "published_date": "2025-05-06 16:15:24 UTC",
    "updated_date": "2025-05-06 16:15:24 UTC"
  },
  {
    "arxiv_id": "2505.03668v1",
    "title": "Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time",
    "authors": [
      "Celeste Veronese",
      "Daniele Meli",
      "Alessandro Farinelli"
    ],
    "abstract": "This paper proposes an integration of temporal logical reasoning and\nPartially Observable Markov Decision Processes (POMDPs) to achieve\ninterpretable decision-making under uncertainty with macro-actions. Our method\nleverages a fragment of Linear Temporal Logic (LTL) based on Event Calculus\n(EC) to generate \\emph{persistent} (i.e., constant) macro-actions, which guide\nMonte Carlo Tree Search (MCTS)-based POMDP solvers over a time horizon,\nsignificantly reducing inference time while ensuring robust performance. Such\nmacro-actions are learnt via Inductive Logic Programming (ILP) from a few\ntraces of execution (belief-action pairs), thus eliminating the need for\nmanually designed heuristics and requiring only the specification of the POMDP\ntransition model. In the Pocman and Rocksample benchmark scenarios, our learned\nmacro-actions demonstrate increased expressiveness and generality when compared\nto time-independent heuristics, indeed offering substantial computational\nefficiency improvements.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at 9th Conference on Neurosymbolic Learning and Reasoning",
    "pdf_url": "http://arxiv.org/pdf/2505.03668v1",
    "published_date": "2025-05-06 16:08:55 UTC",
    "updated_date": "2025-05-06 16:08:55 UTC"
  },
  {
    "arxiv_id": "2505.03662v1",
    "title": "Revolutionizing Brain Tumor Imaging: Generating Synthetic 3D FA Maps from T1-Weighted MRI using CycleGAN Models",
    "authors": [
      "Xin Du",
      "Francesca M. Cozzi",
      "Rajesh Jena"
    ],
    "abstract": "Fractional anisotropy (FA) and directionally encoded colour (DEC) maps are\nessential for evaluating white matter integrity and structural connectivity in\nneuroimaging. However, the spatial misalignment between FA maps and\ntractography atlases hinders their effective integration into predictive\nmodels. To address this issue, we propose a CycleGAN based approach for\ngenerating FA maps directly from T1-weighted MRI scans, representing the first\napplication of this technique to both healthy and tumour-affected tissues. Our\nmodel, trained on unpaired data, produces high fidelity maps, which have been\nrigorously evaluated using Structural Similarity Index (SSIM) and Peak\nSignal-to-Noise Ratio (PSNR), demonstrating particularly robust performance in\ntumour regions. Radiological assessments further underscore the model's\npotential to enhance clinical workflows by providing an AI-driven alternative\nthat reduces the necessity for additional scans.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68U10"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.03662v1",
    "published_date": "2025-05-06 16:05:22 UTC",
    "updated_date": "2025-05-06 16:05:22 UTC"
  },
  {
    "arxiv_id": "2505.03655v1",
    "title": "Counterfactual Inference for Eliminating Sentiment Bias in Recommender Systems",
    "authors": [
      "Le Pan",
      "Yuanjiang Cao",
      "Chengkai Huang",
      "Wenjie Zhang",
      "Lina Yao"
    ],
    "abstract": "Recommender Systems (RSs) aim to provide personalized recommendations for\nusers. A newly discovered bias, known as sentiment bias, uncovers a common\nphenomenon within Review-based RSs (RRSs): the recommendation accuracy of users\nor items with negative reviews deteriorates compared with users or items with\npositive reviews. Critical users and niche items are disadvantaged by such\nunfair recommendations. We study this problem from the perspective of\ncounterfactual inference with two stages. At the model training stage, we build\na causal graph and model how sentiment influences the final rating score.\nDuring the inference stage, we decouple the direct and indirect effects to\nmitigate the impact of sentiment bias and remove the indirect effect using\ncounterfactual inference. We have conducted extensive experiments, and the\nresults validate that our model can achieve comparable performance on rating\nprediction for better recommendations and effective mitigation of sentiment\nbias. To the best of our knowledge, this is the first work to employ\ncounterfactual inference on sentiment bias mitigation in RSs.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03655v1",
    "published_date": "2025-05-06 16:00:41 UTC",
    "updated_date": "2025-05-06 16:00:41 UTC"
  },
  {
    "arxiv_id": "2505.03654v1",
    "title": "ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language and Vision Assistant",
    "authors": [
      "Yifan Xiang",
      "Zhenxi Zhang",
      "Bin Li",
      "Yixuan Weng",
      "Shoujun Zhou",
      "Yangfan He",
      "Keqin Li"
    ],
    "abstract": "Recent advances in personalized MLLMs enable effective capture of\nuser-specific concepts, supporting both recognition of personalized concepts\nand contextual captioning. However, humans typically explore and reason over\nrelations among objects and individuals, transcending surface-level information\nto achieve more personalized and contextual understanding. To this end,\nexisting methods may face three main limitations: Their training data lacks\nmulti-object sets in which relations among objects are learnable. Building on\nthe limited training data, their models overlook the relations between\ndifferent personalized concepts and fail to reason over them. Their experiments\nmainly focus on a single personalized concept, where evaluations are limited to\nrecognition and captioning tasks. To address the limitations, we present a new\ndataset named ReGraP, consisting of 120 sets of personalized knowledge. Each\nset includes images, KGs, and CoT QA pairs derived from the KGs, enabling more\nstructured and sophisticated reasoning pathways. We propose ReGraP-LLaVA, an\nMLLM trained with the corresponding KGs and CoT QA pairs, where soft and hard\ngraph prompting methods are designed to align KGs within the model's semantic\nspace. We establish the ReGraP Benchmark, which contains diverse task types:\nmultiple-choice, fill-in-the-blank, True/False, and descriptive questions in\nboth open- and closed-ended settings. The proposed benchmark is designed to\nevaluate the relational reasoning and knowledge-connection capability of\npersonalized MLLMs. We conduct experiments on the proposed ReGraP-LLaVA and\nother competitive MLLMs. Results show that the proposed model not only learns\npersonalized knowledge but also performs relational reasoning in responses,\nachieving the SoTA performance compared with the competitive methods. All the\ncodes and datasets are released at: https://github.com/xyfyyds/ReGraP.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2505.03654v1",
    "published_date": "2025-05-06 16:00:13 UTC",
    "updated_date": "2025-05-06 16:00:13 UTC"
  },
  {
    "arxiv_id": "2505.03648v1",
    "title": "Binding threshold units with artificial oscillatory neurons",
    "authors": [
      "Vladimir Fanaskov",
      "Ivan Oseledets"
    ],
    "abstract": "Artificial Kuramoto oscillatory neurons were recently introduced as an\nalternative to threshold units. Empirical evidence suggests that oscillatory\nunits outperform threshold units in several tasks including unsupervised object\ndiscovery and certain reasoning problems. The proposed coupling mechanism for\nthese oscillatory neurons is heterogeneous, combining a generalized Kuramoto\nequation with standard coupling methods used for threshold units. In this\nresearch note, we present a theoretical framework that clearly distinguishes\noscillatory neurons from threshold units and establishes a coupling mechanism\nbetween them. We argue that, from a biological standpoint, oscillatory and\nthreshold units realise distinct aspects of neural coding: roughly, threshold\nunits model intensity of neuron firing, while oscillatory units facilitate\ninformation exchange by frequency modulation. To derive interaction between\nthese two types of units, we constrain their dynamics by focusing on dynamical\nsystems that admit Lyapunov functions. For threshold units, this leads to\nHopfield associative memory model, and for oscillatory units it yields a\nspecific form of generalized Kuramoto model. The resulting dynamical systems\ncan be naturally coupled to form a Hopfield-Kuramoto associative memory model,\nwhich also admits a Lyapunov function. Various forms of coupling are possible.\nNotably, oscillatory neurons can be employed to implement a low-rank correction\nto the weight matrix of a Hopfield network. This correction can be viewed\neither as a form of Hebbian learning or as a popular LoRA method used for\nfine-tuning of large language models. We demonstrate the practical realization\nof this particular coupling through illustrative toy experiments.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03648v1",
    "published_date": "2025-05-06 15:54:52 UTC",
    "updated_date": "2025-05-06 15:54:52 UTC"
  },
  {
    "arxiv_id": "2505.03646v1",
    "title": "ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders",
    "authors": [
      "Chethan Krishnamurthy Ramanaik",
      "Arjun Roy",
      "Eirini Ntoutsi"
    ],
    "abstract": "Despite the extensive use of deep autoencoders (AEs) in critical\napplications, their adversarial robustness remains relatively underexplored\ncompared to classification models. AE robustness is characterized by the\nLipschitz bounds of its components. Existing robustness evaluation frameworks\nbased on white-box attacks do not fully exploit the vulnerabilities of\nintermediate ill-conditioned layers in AEs. In the context of optimizing\nimperceptible norm-bounded additive perturbations to maximize output damage,\nexisting methods struggle to effectively propagate adversarial loss gradients\nthroughout the network, often converging to less effective perturbations. To\naddress this, we propose a novel layer-conditioning-based adversarial\noptimization objective that effectively guides the adversarial map toward\nregions of local Lipschitz bounds by enhancing loss gradient information\npropagation during attack optimization. We demonstrate through extensive\nexperiments on state-of-the-art AEs that our adversarial objective results in\nstronger attacks, outperforming existing methods in both universal and\nsample-specific scenarios. As a defense method against this attack, we\nintroduce an inference-time adversarially trained defense plugin that mitigates\nthe effects of adversarial examples.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03646v1",
    "published_date": "2025-05-06 15:52:14 UTC",
    "updated_date": "2025-05-06 15:52:14 UTC"
  },
  {
    "arxiv_id": "2505.03643v1",
    "title": "BURNS: Backward Underapproximate Reachability for Neural-Feedback-Loop Systems",
    "authors": [
      "Chelsea Sidrane",
      "Jana Tumova"
    ],
    "abstract": "Learning-enabled planning and control algorithms are increasingly popular,\nbut they often lack rigorous guarantees of performance or safety. We introduce\nan algorithm for computing underapproximate backward reachable sets of\nnonlinear discrete time neural feedback loops. We then use the backward\nreachable sets to check goal-reaching properties. Our algorithm is based on\noverapproximating the system dynamics function to enable computation of\nunderapproximate backward reachable sets through solutions of mixed-integer\nlinear programs. We rigorously analyze the soundness of our algorithm and\ndemonstrate it on a numerical example. Our work expands the class of properties\nthat can be verified for learning-enabled systems.",
    "categories": [
      "cs.AI",
      "cs.LO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03643v1",
    "published_date": "2025-05-06 15:50:43 UTC",
    "updated_date": "2025-05-06 15:50:43 UTC"
  },
  {
    "arxiv_id": "2505.03641v1",
    "title": "Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering and Manipulating Human Perceptual Variability",
    "authors": [
      "Chen Wei",
      "Chi Zhang",
      "Jiachen Zou",
      "Haotian Deng",
      "Dietmar Heinke",
      "Quanying Liu"
    ],
    "abstract": "Human decision-making in cognitive tasks and daily life exhibits considerable\nvariability, shaped by factors such as task difficulty, individual preferences,\nand personal experiences. Understanding this variability across individuals is\nessential for uncovering the perceptual and decision-making mechanisms that\nhumans rely on when faced with uncertainty and ambiguity. We present a\ncomputational framework BAM (Boundary Alignment & Manipulation framework) that\ncombines perceptual boundary sampling in ANNs and human behavioral experiments\nto systematically investigate this phenomenon. Our perceptual boundary sampling\nalgorithm generates stimuli along ANN decision boundaries that intrinsically\ninduce significant perceptual variability. The efficacy of these stimuli is\nempirically validated through large-scale behavioral experiments involving 246\nparticipants across 116,715 trials, culminating in the variMNIST dataset\ncontaining 19,943 systematically annotated images. Through personalized model\nalignment and adversarial generation, we establish a reliable method for\nsimultaneously predicting and manipulating the divergent perceptual decisions\nof pairs of participants. This work bridges the gap between computational\nmodels and human individual difference research, providing new tools for\npersonalized perception analysis.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "accepted at ICML 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.03641v1",
    "published_date": "2025-05-06 15:44:42 UTC",
    "updated_date": "2025-05-06 15:44:42 UTC"
  },
  {
    "arxiv_id": "2505.03586v1",
    "title": "Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation",
    "authors": [
      "Songchen Fu",
      "Siang Chen",
      "Shaojing Zhao",
      "Letian Bai",
      "Ta Li",
      "Yonghong Yan"
    ],
    "abstract": "In real-world multi-agent systems (MASs), observation delays are ubiquitous,\npreventing agents from making decisions based on the environment's true state.\nAn individual agent's local observation often consists of multiple components\nfrom other agents or dynamic entities in the environment. These discrete\nobservation components with varying delay characteristics pose significant\nchallenges for multi-agent reinforcement learning (MARL). In this paper, we\nfirst formulate the decentralized stochastic individual delay partially\nobservable Markov decision process (DSID-POMDP) by extending the standard\nDec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL\ntraining framework for addressing stochastic individual delays, along with\nrecommended implementations for its constituent modules. We implement the\nDSID-POMDP's observation generation pattern using standard MARL benchmarks,\nincluding MPE and SMAC. Experiments demonstrate that baseline MARL methods\nsuffer severe performance degradation under fixed and unfixed delays. The\nRDC-enhanced approach mitigates this issue, remarkably achieving ideal\ndelay-free performance in certain delay scenarios while maintaining\ngeneralization capability. Our work provides a novel perspective on multi-agent\ndelayed observation problems and offers an effective solution framework.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "68T07 (Primary), 68T20, 68T42 (Secondary)",
      "I.2"
    ],
    "primary_category": "cs.MA",
    "comment": "The code will be open-sourced in the RDC-pymarl project under\n  https://github.com/linkjoker1006",
    "pdf_url": "http://arxiv.org/pdf/2505.03586v1",
    "published_date": "2025-05-06 14:47:56 UTC",
    "updated_date": "2025-05-06 14:47:56 UTC"
  },
  {
    "arxiv_id": "2505.03584v1",
    "title": "BCause: Human-AI collaboration to improve hybrid mapping and ideation in argumentation-grounded deliberation",
    "authors": [
      "Lucas Anastasiou",
      "Anna De Liddo"
    ],
    "abstract": "Public deliberation, as in open discussion of issues of public concern, often\nsuffers from scattered and shallow discourse, poor sensemaking, and a\ndisconnect from actionable policy outcomes. This paper introduces BCause, a\ndiscussion system leveraging generative AI and human-machine collaboration to\ntransform unstructured dialogue around public issues (such as urban living,\npolicy changes, and current socio-economic transformations) into structured,\nactionable democratic processes. We present three innovations: (i) importing\nand transforming unstructured transcripts into argumentative discussions, (ii)\ngeo-deliberated problem-sensing via a Telegram bot for local issue reporting,\nand (iii) smart reporting with customizable widgets (e.g., summaries, topic\nmodelling, policy recommendations, clustered arguments). The system's human-AI\npartnership preserves critical human participation to ensure ethical oversight,\ncontextual relevance, and creative synthesis.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "I.2"
    ],
    "primary_category": "cs.HC",
    "comment": "5 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.03584v1",
    "published_date": "2025-05-06 14:43:49 UTC",
    "updated_date": "2025-05-06 14:43:49 UTC"
  },
  {
    "arxiv_id": "2505.03574v1",
    "title": "LlamaFirewall: An open source guardrail system for building secure AI agents",
    "authors": [
      "Sahana Chennabasappa",
      "Cyrus Nikolaidis",
      "Daniel Song",
      "David Molnar",
      "Stephanie Ding",
      "Shengye Wan",
      "Spencer Whitman",
      "Lauren Deason",
      "Nicholas Doucette",
      "Abraham Montilla",
      "Alekhya Gampa",
      "Beto de Paola",
      "Dominik Gabi",
      "James Crnkovich",
      "Jean-Christophe Testud",
      "Kat He",
      "Rashnil Chaturvedi",
      "Wu Zhou",
      "Joshua Saxe"
    ],
    "abstract": "Large language models (LLMs) have evolved from simple chatbots into\nautonomous agents capable of performing complex tasks such as editing\nproduction code, orchestrating workflows, and taking higher-stakes actions\nbased on untrusted inputs like webpages and emails. These capabilities\nintroduce new security risks that existing security measures, such as model\nfine-tuning or chatbot-focused guardrails, do not fully address. Given the\nhigher stakes and the absence of deterministic solutions to mitigate these\nrisks, there is a critical need for a real-time guardrail monitor to serve as a\nfinal layer of defense, and support system level, use case specific safety\npolicy definition and enforcement. We introduce LlamaFirewall, an open-source\nsecurity focused guardrail framework designed to serve as a final layer of\ndefense against security risks associated with AI Agents. Our framework\nmitigates risks such as prompt injection, agent misalignment, and insecure code\nrisks through three powerful guardrails: PromptGuard 2, a universal jailbreak\ndetector that demonstrates clear state of the art performance; Agent Alignment\nChecks, a chain-of-thought auditor that inspects agent reasoning for prompt\ninjection and goal misalignment, which, while still experimental, shows\nstronger efficacy at preventing indirect injections in general scenarios than\npreviously proposed approaches; and CodeShield, an online static analysis\nengine that is both fast and extensible, aimed at preventing the generation of\ninsecure or dangerous code by coding agents. Additionally, we include\neasy-to-use customizable scanners that make it possible for any developer who\ncan write a regular expression or an LLM prompt to quickly update an agent's\nsecurity guardrails.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03574v1",
    "published_date": "2025-05-06 14:34:21 UTC",
    "updated_date": "2025-05-06 14:34:21 UTC"
  },
  {
    "arxiv_id": "2505.03570v1",
    "title": "OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents",
    "authors": [
      "Mariya Davydova",
      "Daniel Jeffries",
      "Patrick Barker",
      "Arturo Márquez Flores",
      "Sinéad Ryan"
    ],
    "abstract": "In this paper, we introduce OSUniverse: a benchmark of complex, multimodal\ndesktop-oriented tasks for advanced GUI-navigation AI agents that focuses on\nease of use, extensibility, comprehensive coverage of test cases, and automated\nvalidation. We divide the tasks in increasing levels of complexity, from basic\nprecision clicking to multistep, multiapplication tests requiring dexterity,\nprecision, and clear thinking from the agent. In version one of the benchmark,\npresented here, we have calibrated the complexity of the benchmark test cases\nto ensure that the SOTA (State of the Art) agents (at the time of publication)\ndo not achieve results higher than 50%, while the average white collar worker\ncan perform all these tasks with perfect accuracy. The benchmark can be scored\nmanually, but we also introduce an automated validation mechanism that has an\naverage error rate less than 2%. Therefore, this benchmark presents solid\nground for fully automated measuring of progress, capabilities and the\neffectiveness of GUI-navigation AI agents over the short and medium-term\nhorizon. The source code of the benchmark is available at\nhttps://github.com/agentsea/osuniverse.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03570v1",
    "published_date": "2025-05-06 14:29:47 UTC",
    "updated_date": "2025-05-06 14:29:47 UTC"
  },
  {
    "arxiv_id": "2505.03562v1",
    "title": "Real-Time Person Image Synthesis Using a Flow Matching Model",
    "authors": [
      "Jiwoo Jeong",
      "Kirok Kim",
      "Wooju Kim",
      "Nam-Joon Kim"
    ],
    "abstract": "Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images\nconditioned on a target pose and a source image. This task plays a key role in\nvarious real-world applications, such as sign language video generation, AR/VR,\ngaming, and live streaming. In these scenarios, real-time PGPIS is critical for\nproviding immediate visual feedback and maintaining user immersion.However,\nachieving real-time performance remains a significant challenge due to the\ncomplexity of synthesizing high-fidelity images from diverse and dynamic human\nposes. Recent diffusion-based methods have shown impressive image quality in\nPGPIS, but their slow sampling speeds hinder deployment in time-sensitive\napplications. This latency is particularly problematic in tasks like generating\nsign language videos during live broadcasts, where rapid image updates are\nrequired. Therefore, developing a fast and reliable PGPIS model is a crucial\nstep toward enabling real-time interactive systems. To address this challenge,\nwe propose a generative model based on flow matching (FM). Our approach enables\nfaster, more stable, and more efficient training and sampling. Furthermore, the\nproposed model supports conditional generation and can operate in latent space,\nmaking it especially suitable for real-time PGPIS applications where both speed\nand quality are critical. We evaluate our proposed method, Real-Time Person\nImage Synthesis Using a Flow Matching Model (RPFM), on the widely used\nDeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves\nnear-real-time sampling speeds while maintaining performance comparable to the\nstate-of-the-art models. Our methodology trades off a slight, acceptable\ndecrease in generated-image accuracy for over a twofold increase in generation\nspeed, thereby ensuring real-time performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03562v1",
    "published_date": "2025-05-06 14:13:44 UTC",
    "updated_date": "2025-05-06 14:13:44 UTC"
  },
  {
    "arxiv_id": "2505.03561v1",
    "title": "Ergodic Generative Flows",
    "authors": [
      "Leo Maxime Brunswic",
      "Mateo Clemente",
      "Rui Heng Yang",
      "Adam Sigal",
      "Amir Rasouli",
      "Yinchuan Li"
    ],
    "abstract": "Generative Flow Networks (GFNs) were initially introduced on directed acyclic\ngraphs to sample from an unnormalized distribution density. Recent works have\nextended the theoretical framework for generative methods allowing more\nflexibility and enhancing application range. However, many challenges remain in\ntraining GFNs in continuous settings and for imitation learning (IL), including\nintractability of flow-matching loss, limited tests of non-acyclic training,\nand the need for a separate reward model in imitation learning. The present\nwork proposes a family of generative flows called Ergodic Generative Flows\n(EGFs) which are used to address the aforementioned issues. First, we leverage\nergodicity to build simple generative flows with finitely many globally defined\ntransformations (diffeomorphisms) with universality guarantees and tractable\nflow-matching loss (FM loss). Second, we introduce a new loss involving\ncross-entropy coupled to weak flow-matching control, coined KL-weakFM loss. It\nis designed for IL training without a separate reward model. We evaluate\nIL-EGFs on toy 2D tasks and real-world datasets from NASA on the sphere, using\nthe KL-weakFM loss. Additionally, we conduct toy 2D reinforcement learning\nexperiments with a target reward, using the FM loss.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.DG",
      "math.DS",
      "37A25, 68T07, 68W20, 68Q87, 68T99"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 5 figures, 1 table, accepted at ICML 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.03561v1",
    "published_date": "2025-05-06 14:13:21 UTC",
    "updated_date": "2025-05-06 14:13:21 UTC"
  },
  {
    "arxiv_id": "2505.03560v1",
    "title": "Rapid AI-based generation of coverage paths for dispensing applications",
    "authors": [
      "Simon Baeuerle",
      "Ian F. Mendonca",
      "Kristof Van Laerhoven",
      "Ralf Mikut",
      "Andreas Steimer"
    ],
    "abstract": "Coverage Path Planning of Thermal Interface Materials (TIM) plays a crucial\nrole in the design of power electronics and electronic control units. Up to\nnow, this is done manually by experts or by using optimization approaches with\na high computational effort. We propose a novel AI-based approach to generate\ndispense paths for TIM and similar dispensing applications. It is a drop-in\nreplacement for optimization-based approaches. An Artificial Neural Network\n(ANN) receives the target cooling area as input and directly outputs the\ndispense path. Our proposed setup does not require labels and we show its\nfeasibility on multiple target areas. The resulting dispense paths can be\ndirectly transferred to automated manufacturing equipment and do not exhibit\nair entrapments. The approach of using an ANN to predict process parameters for\na desired target state in real-time could potentially be transferred to other\nmanufacturing processes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03560v1",
    "published_date": "2025-05-06 14:13:20 UTC",
    "updated_date": "2025-05-06 14:13:20 UTC"
  },
  {
    "arxiv_id": "2505.03557v1",
    "title": "Generating Synthetic Data via Augmentations for Improved Facial Resemblance in DreamBooth and InstantID",
    "authors": [
      "Koray Ulusan",
      "Benjamin Kiefer"
    ],
    "abstract": "The personalization of Stable Diffusion for generating professional portraits\nfrom amateur photographs is a burgeoning area, with applications in various\ndownstream contexts. This paper investigates the impact of augmentations on\nimproving facial resemblance when using two prominent personalization\ntechniques: DreamBooth and InstantID. Through a series of experiments with\ndiverse subject datasets, we assessed the effectiveness of various augmentation\nstrategies on the generated headshots' fidelity to the original subject. We\nintroduce FaceDistance, a wrapper around FaceNet, to rank the generations based\non facial similarity, which aided in our assessment. Ultimately, this research\nprovides insights into the role of augmentations in enhancing facial\nresemblance in SDXL-generated portraits, informing strategies for their\neffective deployment in downstream applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR 2025 Workshop \"Synthetic Data for Computer Vision\n  Workshop\", https://syndata4cv.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2505.03557v1",
    "published_date": "2025-05-06 14:11:02 UTC",
    "updated_date": "2025-05-06 14:11:02 UTC"
  },
  {
    "arxiv_id": "2505.03553v1",
    "title": "A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model Reasoning",
    "authors": [
      "Kolawole E. Ogunsina",
      "Morayo A. Ogunsina"
    ],
    "abstract": "Inconsistent outputs and hallucinations from large language models (LLMs) are\nmajor obstacles to reliable AI systems. When different proprietary reasoning\nmodels (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI,\nare given the same complex request, they often produce divergent results due to\nvariations in training and inference. This paper proposes a novel consensus\nmechanism, inspired by distributed ledger technology, to validate and converge\nthese outputs, treating each RM as a black-box peer. Building on the Hashgraph\nconsensus algorithm, our approach employs gossip-about-gossip communication and\nvirtual voting to achieve agreement among an ensemble of RMs. We present an\narchitectural design for a prototype system in which RMs iteratively exchange\nand update their answers, using information from each round to improve accuracy\nand confidence in subsequent rounds. This approach goes beyond simple majority\nvoting by incorporating the knowledge and cross-verification content of every\nmodel. We justify the feasibility of this Hashgraph-inspired consensus for AI\nensembles and outline its advantages over traditional ensembling techniques in\nreducing nonfactual outputs. Preliminary considerations for implementation,\nevaluation criteria for convergence and accuracy, and potential challenges are\ndiscussed. The proposed mechanism demonstrates a promising direction for\nmulti-agent AI systems to self-validate and deliver high-fidelity responses in\ncomplex tasks.",
    "categories": [
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.03553v1",
    "published_date": "2025-05-06 14:05:12 UTC",
    "updated_date": "2025-05-06 14:05:12 UTC"
  },
  {
    "arxiv_id": "2505.03547v1",
    "title": "STORY2GAME: Generating (Almost) Everything in an Interactive Fiction Game",
    "authors": [
      "Eric Zhou",
      "Shreyas Basavatia",
      "Moontashir Siam",
      "Zexin Chen",
      "Mark O. Riedl"
    ],
    "abstract": "We introduce STORY2GAME, a novel approach to using Large Language Models to\ngenerate text-based interactive fiction games that starts by generating a\nstory, populates the world, and builds the code for actions in a game engine\nthat enables the story to play out interactively. Whereas a given set of\nhard-coded actions can artificially constrain story generation, the ability to\ngenerate actions means the story generation process can be more open-ended but\nstill allow for experiences that are grounded in a game state. The key to\nsuccessful action generation is to use LLM-generated preconditions and effects\nof actions in the stories as guides for what aspects of the game state must be\ntracked and changed by the game engine when a player performs an action. We\nalso introduce a technique for dynamically generating new actions to\naccommodate the player's desire to perform actions that they think of that are\nnot part of the story. Dynamic action generation may require on-the-fly updates\nto the game engine's state representation and revision of previously generated\nactions. We evaluate the success rate of action code generation with respect to\nwhether a player can interactively play through the entire generated story.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03547v1",
    "published_date": "2025-05-06 14:00:41 UTC",
    "updated_date": "2025-05-06 14:00:41 UTC"
  },
  {
    "arxiv_id": "2505.03522v1",
    "title": "Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks",
    "authors": [
      "Haotong Cheng",
      "Zhiqi Zhang",
      "Hao Li",
      "Xinshang Zhang"
    ],
    "abstract": "Deep learning has substantially advanced the Single Image Super-Resolution\n(SISR). However, existing researches have predominantly focused on raw\nperformance gains, with little attention paid to quantifying the\ntransferability of architectural components. In this paper, we introduce the\nconcept of \"Universality\" and its associated definitions which extend the\ntraditional notion of \"Generalization\" to encompass the modules' ease of\ntransferability, thus revealing the relationships between module universality\nand model generalizability. Then we propose the Universality Assessment\nEquation (UAE), a metric for quantifying how readily a given module could be\ntransplanted across models. Guided by the UAE results of standard residual\nblocks and other plug-and-play modules, we further design two optimized\nmodules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB).\nThrough comprehensive experiments on natural-scene benchmarks, remote-sensing\ndatasets, extreme-industrial imagery and on-device deployments, we demonstrate\nthat networks embedded with the proposed plug-and-play modules outperform\nseveral state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or\nenabling a 71.3% reduction in parameters with negligible loss in reconstruction\nfidelity.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03522v1",
    "published_date": "2025-05-06 13:35:59 UTC",
    "updated_date": "2025-05-06 13:35:59 UTC"
  },
  {
    "arxiv_id": "2505.03510v1",
    "title": "From Neurons to Computation: Biological Reservoir Computing for Pattern Recognition",
    "authors": [
      "Ludovico Iannello",
      "Luca Ciampi",
      "Gabriele Lagani",
      "Fabrizio Tonelli",
      "Eleonora Crocco",
      "Lucio Maria Calcagnile",
      "Angelo Di Garbo",
      "Federico Cremisi",
      "Giuseppe Amato"
    ],
    "abstract": "In this paper, we introduce a novel paradigm for reservoir computing (RC)\nthat leverages a pool of cultured biological neurons as the reservoir\nsubstrate, creating a biological reservoir computing (BRC). This system\noperates similarly to an echo state network (ESN), with the key distinction\nthat the neural activity is generated by a network of cultured neurons, rather\nthan being modeled by traditional artificial computational units. The neuronal\nactivity is recorded using a multi-electrode array (MEA), which enables\nhigh-throughput recording of neural signals. In our approach, inputs are\nintroduced into the network through a subset of the MEA electrodes, while the\nremaining electrodes capture the resulting neural activity. This generates a\nnonlinear mapping of the input data to a high-dimensional biological feature\nspace, where distinguishing between data becomes more efficient and\nstraightforward, allowing a simple linear classifier to perform pattern\nrecognition tasks effectively. To evaluate the performance of our proposed\nsystem, we present an experimental study that includes various input patterns,\nsuch as positional codes, bars with different orientations, and a digit\nrecognition task. The results demonstrate the feasibility of using biological\nneural networks to perform tasks traditionally handled by artificial neural\nnetworks, paving the way for further exploration of biologically-inspired\ncomputing systems, with potential applications in neuromorphic engineering and\nbio-hybrid computing.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03510v1",
    "published_date": "2025-05-06 13:20:04 UTC",
    "updated_date": "2025-05-06 13:20:04 UTC"
  },
  {
    "arxiv_id": "2505.03492v1",
    "title": "Augmenting Human Cognition through Everyday AR",
    "authors": [
      "Xiaoan Liu"
    ],
    "abstract": "As spatial computing and multimodal LLMs mature, AR is tending to become an\nintuitive \"thinking tool,\" embedding semantic and context-aware intelligence\ndirectly into everyday environments. This paper explores how always-on AR can\nseamlessly bridge digital cognition and physical affordances, enabling\nproactive, context-sensitive interactions that enhance human task performance\nand understanding.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "3 pages, 4 figures. Position paper accepted to CHI'25 Workshop\n  'Everyday AR through AI-in-the-Loop'",
    "pdf_url": "http://arxiv.org/pdf/2505.03492v1",
    "published_date": "2025-05-06 12:48:38 UTC",
    "updated_date": "2025-05-06 12:48:38 UTC"
  },
  {
    "arxiv_id": "2505.03490v1",
    "title": "A new membership inference attack that spots memorization in generative and predictive models: Loss-Based with Reference Model algorithm (LBRM)",
    "authors": [
      "Faiz Taleb",
      "Ivan Gazeau",
      "Maryline Laurent"
    ],
    "abstract": "Generative models can unintentionally memorize training data, posing\nsignificant privacy risks. This paper addresses the memorization phenomenon in\ntime series imputation models, introducing the Loss-Based with Reference Model\n(LBRM) algorithm. The LBRM method leverages a reference model to enhance the\naccuracy of membership inference attacks, distinguishing between training and\ntest data. Our contributions are twofold: first, we propose an innovative\nmethod to effectively extract and identify memorized training data,\nsignificantly improving detection accuracy. On average, without fine-tuning,\nthe AUROC improved by approximately 40\\%. With fine-tuning, the AUROC increased\nby approximately 60\\%. Second, we validate our approach through membership\ninference attacks on two types of architectures designed for time series\nimputation, demonstrating the robustness and versatility of the LBRM approach\nin different contexts. These results highlight the significant enhancement in\ndetection accuracy provided by the LBRM approach, addressing privacy risks in\ntime series imputation models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03490v1",
    "published_date": "2025-05-06 12:47:24 UTC",
    "updated_date": "2025-05-06 12:47:24 UTC"
  },
  {
    "arxiv_id": "2505.03475v1",
    "title": "am-ELO: A Stable Framework for Arena-based LLM Evaluation",
    "authors": [
      "Zirui Liu",
      "Jiatong Li",
      "Yan Zhuang",
      "Qi Liu",
      "Shuanghong Shen",
      "Jie Ouyang",
      "Mingyue Cheng",
      "Shijin Wang"
    ],
    "abstract": "Arena-based evaluation is a fundamental yet significant evaluation paradigm\nfor modern AI models, especially large language models (LLMs). Existing\nframework based on ELO rating system suffers from the inevitable instability\nproblem due to ranking inconsistency and the lack of attention to the varying\nabilities of annotators. In this paper, we introduce a novel stable arena\nframework to address these issues by enhancing the ELO Rating System.\nSpecifically, we replace the iterative update method with a Maximum Likelihood\nEstimation (MLE) approach, m-ELO, and provide theoretical proof of the\nconsistency and stability of the MLE approach for model ranking. Additionally,\nwe proposed the am-ELO, which modify the Elo Rating's probability function to\nincorporate annotator abilities, enabling the simultaneous estimation of model\nscores and annotator reliability. Experiments demonstrate that this method\nensures stability, proving that this framework offers a more robust, accurate,\nand stable evaluation method for LLMs.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "ICML2025 Accepted",
    "pdf_url": "http://arxiv.org/pdf/2505.03475v1",
    "published_date": "2025-05-06 12:28:50 UTC",
    "updated_date": "2025-05-06 12:28:50 UTC"
  },
  {
    "arxiv_id": "2505.03470v1",
    "title": "Blending 3D Geometry and Machine Learning for Multi-View Stereopsis",
    "authors": [
      "Vibhas Vats",
      "Md. Alimoor Reza",
      "David Crandall",
      "Soon-heung Jung"
    ],
    "abstract": "Traditional multi-view stereo (MVS) methods primarily depend on photometric\nand geometric consistency constraints. In contrast, modern learning-based\nalgorithms often rely on the plane sweep algorithm to infer 3D geometry,\napplying explicit geometric consistency (GC) checks only as a post-processing\nstep, with no impact on the learning process itself. In this work, we introduce\nGC MVSNet plus plus, a novel approach that actively enforces geometric\nconsistency of reference view depth maps across multiple source views (multi\nview) and at various scales (multi scale) during the learning phase (see Fig.\n1). This integrated GC check significantly accelerates the learning process by\ndirectly penalizing geometrically inconsistent pixels, effectively halving the\nnumber of training iterations compared to other MVS methods. Furthermore, we\nintroduce a densely connected cost regularization network with two distinct\nblock designs simple and feature dense optimized to harness dense feature\nconnections for enhanced regularization. Extensive experiments demonstrate that\nour approach achieves a new state of the art on the DTU and BlendedMVS datasets\nand secures second place on the Tanks and Temples benchmark. To our knowledge,\nGC MVSNet plus plus is the first method to enforce multi-view, multi-scale\nsupervised geometric consistency during learning. Our code is available.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CG",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "A pre-print -- paper under-review. arXiv admin note: substantial text\n  overlap with arXiv:2310.19583",
    "pdf_url": "http://arxiv.org/pdf/2505.03470v1",
    "published_date": "2025-05-06 12:22:45 UTC",
    "updated_date": "2025-05-06 12:22:45 UTC"
  },
  {
    "arxiv_id": "2505.03452v1",
    "title": "An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation",
    "authors": [
      "Matan Orbach",
      "Ohad Eytan",
      "Benjamin Sznajder",
      "Ariel Gera",
      "Odellia Boni",
      "Yoav Kantor",
      "Gal Bloch",
      "Omri Levy",
      "Hadas Abraham",
      "Nitzan Barzilay",
      "Eyal Shnarch",
      "Michael E. Factor",
      "Shila Ofek-Koifman",
      "Paula Ta-Shma",
      "Assaf Toledo"
    ],
    "abstract": "Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a\ngiven use case can be complex and expensive. Motivated by this challenge,\nframeworks for RAG hyper-parameter optimization (HPO) have recently emerged,\nyet their effectiveness has not been rigorously benchmarked. To address this\ngap, we present a comprehensive study involving 5 HPO algorithms over 5\ndatasets from diverse domains, including a new one collected for this work on\nreal-world product documentation. Our study explores the largest HPO search\nspace considered to date, with two optimized evaluation metrics. Analysis of\nthe results shows that RAG HPO can be done efficiently, either greedily or with\niterative random search, and that it significantly boosts RAG performance for\nall datasets. For greedy HPO approaches, we show that optimizing models first\nis preferable to the prevalent practice of optimizing sequentially according to\nthe RAG pipeline order.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03452v1",
    "published_date": "2025-05-06 11:47:52 UTC",
    "updated_date": "2025-05-06 11:47:52 UTC"
  },
  {
    "arxiv_id": "2505.03451v1",
    "title": "Detecting Quishing Attacks with Machine Learning Techniques Through QR Code Analysis",
    "authors": [
      "Fouad Trad",
      "Ali Chehab"
    ],
    "abstract": "The rise of QR code based phishing (\"Quishing\") poses a growing cybersecurity\nthreat, as attackers increasingly exploit QR codes to bypass traditional\nphishing defenses. Existing detection methods predominantly focus on URL\nanalysis, which requires the extraction of the QR code payload, and may\ninadvertently expose users to malicious content. Moreover, QR codes can encode\nvarious types of data beyond URLs, such as Wi-Fi credentials and payment\ninformation, making URL-based detection insufficient for broader security\nconcerns. To address these gaps, we propose the first framework for quishing\ndetection that directly analyzes QR code structure and pixel patterns without\nextracting the embedded content. We generated a dataset of phishing and benign\nQR codes and we used it to train and evaluate multiple machine learning models,\nincluding Logistic Regression, Decision Trees, Random Forest, Naive Bayes,\nLightGBM, and XGBoost. Our best-performing model (XGBoost) achieves an AUC of\n0.9106, demonstrating the feasibility of QR-centric detection. Through feature\nimportance analysis, we identify key visual indicators of malicious intent and\nrefine our feature set by removing non-informative pixels, improving\nperformance to an AUC of 0.9133 with a reduced feature space. Our findings\nreveal that the structural features of QR code correlate strongly with phishing\nrisk. This work establishes a foundation for quishing mitigation and highlights\nthe potential of direct QR analysis as a critical layer in modern phishing\ndefenses.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted in 8th International Conference on Optimization and Learning\n  (OLA2025)",
    "pdf_url": "http://arxiv.org/pdf/2505.03451v1",
    "published_date": "2025-05-06 11:47:13 UTC",
    "updated_date": "2025-05-06 11:47:13 UTC"
  },
  {
    "arxiv_id": "2505.03443v1",
    "title": "Elevating Semantic Exploration: A Novel Approach Utilizing Distributed Repositories",
    "authors": [
      "Valerio Bellandi"
    ],
    "abstract": "Centralized and distributed systems are two main approaches to organizing ICT\ninfrastructure, each with its pros and cons. Centralized systems concentrate\nresources in one location, making management easier but creating single points\nof failure. Distributed systems, on the other hand, spread resources across\nmultiple nodes, offering better scalability and fault tolerance, but requiring\nmore complex management. The choice between them depends on factors like\napplication needs, scalability, and data sensitivity. Centralized systems suit\napplications with limited scalability and centralized control, while\ndistributed systems excel in large-scale environments requiring high\navailability and performance. This paper explores a distributed document\nrepository system developed for the Italian Ministry of Justice, using edge\nrepositories to analyze textual data and metadata, enhancing semantic\nexploration capabilities.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.DC",
    "comment": "This paper has been accepted at the 6th International Conference on\n  Recent Trends and Applications in Computer Science. It will appear in the\n  proceedings",
    "pdf_url": "http://arxiv.org/pdf/2505.03443v1",
    "published_date": "2025-05-06 11:30:16 UTC",
    "updated_date": "2025-05-06 11:30:16 UTC"
  },
  {
    "arxiv_id": "2505.03439v1",
    "title": "The Steganographic Potentials of Language Models",
    "authors": [
      "Artem Karpov",
      "Tinuade Adeleke",
      "Seong Hah Cho",
      "Natalia Perez-Campanero"
    ],
    "abstract": "The potential for large language models (LLMs) to hide messages within plain\ntext (steganography) poses a challenge to detection and thwarting of unaligned\nAI agents, and undermines faithfulness of LLMs reasoning. We explore the\nsteganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)\nto: (1) develop covert encoding schemes, (2) engage in steganography when\nprompted, and (3) utilize steganography in realistic scenarios where hidden\nreasoning is likely, but not prompted. In these scenarios, we detect the\nintention of LLMs to hide their reasoning as well as their steganography\nperformance. Our findings in the fine-tuning experiments as well as in\nbehavioral non fine-tuning evaluations reveal that while current models exhibit\nrudimentary steganographic abilities in terms of security and capacity,\nexplicit algorithmic guidance markedly enhances their capacity for information\nconcealment.",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Published at Building Trust Workshop at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.03439v1",
    "published_date": "2025-05-06 11:25:52 UTC",
    "updated_date": "2025-05-06 11:25:52 UTC"
  },
  {
    "arxiv_id": "2505.03434v1",
    "title": "Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in LLM-Based Agents",
    "authors": [
      "Schaun Wheeler",
      "Olivier Jeunen"
    ],
    "abstract": "Large Language Models (LLMs) represent a landmark achievement in Artificial\nIntelligence (AI), demonstrating unprecedented proficiency in procedural tasks\nsuch as text generation, code completion, and conversational coherence. These\ncapabilities stem from their architecture, which mirrors human procedural\nmemory -- the brain's ability to automate repetitive, pattern-driven tasks\nthrough practice. However, as LLMs are increasingly deployed in real-world\napplications, it becomes impossible to ignore their limitations operating in\ncomplex, unpredictable environments. This paper argues that LLMs, while\ntransformative, are fundamentally constrained by their reliance on procedural\nmemory. To create agents capable of navigating ``wicked'' learning environments\n-- where rules shift, feedback is ambiguous, and novelty is the norm -- we must\naugment LLMs with semantic memory and associative learning systems. By adopting\na modular architecture that decouples these cognitive functions, we can bridge\nthe gap between narrow procedural expertise and the adaptive intelligence\nrequired for real-world problem-solving.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to the workshop on Hybrid AI for Human-Centric\n  Personalization (HyPer), co-located with ACM UMAP '25",
    "pdf_url": "http://arxiv.org/pdf/2505.03434v1",
    "published_date": "2025-05-06 11:18:34 UTC",
    "updated_date": "2025-05-06 11:18:34 UTC"
  },
  {
    "arxiv_id": "2505.03427v1",
    "title": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks",
    "authors": [
      "Mouath Abu Daoud",
      "Chaimae Abouzahir",
      "Leen Kharouf",
      "Walid Al-Eisawi",
      "Nizar Habash",
      "Farah E. Shamout"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.03427v1",
    "published_date": "2025-05-06 11:07:26 UTC",
    "updated_date": "2025-05-06 11:07:26 UTC"
  },
  {
    "arxiv_id": "2505.03426v1",
    "title": "Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI Synthesis: Advancing Pretraining and Clinical Applications",
    "authors": [
      "Ziyu Li",
      "Yujian Hu",
      "Zhengyao Ding",
      "Yiheng Mao",
      "Haitao Li",
      "Fan Yi",
      "Hongkun Zhang",
      "Zhengxing Huang"
    ],
    "abstract": "Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for\ndiagnosing heart diseases and evaluating cardiac health. However, the limited\navailability of large-scale, high-quality CMR datasets poses a major challenge\nto the effective application of artificial intelligence (AI) in this domain.\nEven the amount of unlabeled data and the health status it covers are difficult\nto meet the needs of model pretraining, which hinders the performance of AI\nmodels on downstream tasks. In this study, we present Cardiac Phenotype-Guided\nCMR Generation (CPGG), a novel approach for generating diverse CMR data that\ncovers a wide spectrum of cardiac health status. The CPGG framework consists of\ntwo stages: in the first stage, a generative model is trained using cardiac\nphenotypes derived from CMR data; in the second stage, a masked autoregressive\ndiffusion model, conditioned on these phenotypes, generates high-fidelity CMR\ncine sequences that capture both structural and functional features of the\nheart in a fine-grained manner. We synthesized a massive amount of CMR to\nexpand the pretraining data. Experimental results show that CPGG generates\nhigh-quality synthetic CMR data, significantly improving performance on various\ndownstream tasks, including diagnosis and cardiac phenotypes prediction. These\ngains are demonstrated across both public and private datasets, highlighting\nthe effectiveness of our approach. Code is availabel at\nhttps://anonymous.4open.science/r/CPGG.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03426v1",
    "published_date": "2025-05-06 11:06:41 UTC",
    "updated_date": "2025-05-06 11:06:41 UTC"
  },
  {
    "arxiv_id": "2505.03424v1",
    "title": "Framework GNN-AID: Graph Neural Network Analysis Interpretation and Defense",
    "authors": [
      "Kirill Lukyanov",
      "Mikhail Drobyshevskiy",
      "Georgii Sazonov",
      "Mikhail Soloviov",
      "Ilya Makarov"
    ],
    "abstract": "The growing need for Trusted AI (TAI) highlights the importance of\ninterpretability and robustness in machine learning models. However, many\nexisting tools overlook graph data and rarely combine these two aspects into a\nsingle solution. Graph Neural Networks (GNNs) have become a popular approach,\nachieving top results across various tasks. We introduce GNN-AID (Graph Neural\nNetwork Analysis, Interpretation, and Defense), an open-source framework\ndesigned for graph data to address this gap. Built as a Python library, GNN-AID\nsupports advanced trust methods and architectural layers, allowing users to\nanalyze graph datasets and GNN behavior using attacks, defenses, and\ninterpretability methods.\n  GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models,\nand support for any GNNs through customizable interfaces. It also includes a\nweb interface with tools for graph visualization and no-code features like an\ninteractive model builder, simplifying the exploration and analysis of GNNs.\nThe framework also supports MLOps techniques, ensuring reproducibility and\nresult versioning to track and revisit analyses efficiently.\n  GNN-AID is a flexible tool for developers and researchers. It helps\ndevelopers create, analyze, and customize graph models, while also providing\naccess to prebuilt datasets and models for quick experimentation. Researchers\ncan use the framework to explore advanced topics on the relationship between\ninterpretability and robustness, test defense strategies, and combine methods\nto protect against different types of attacks.\n  We also show how defenses against evasion and poisoning attacks can conflict\nwhen applied to graph data, highlighting the complex connections between\ndefense strategies.\n  GNN-AID is available at\n\\href{https://github.com/ispras/GNN-AID}{github.com/ispras/GNN-AID}",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03424v1",
    "published_date": "2025-05-06 11:03:19 UTC",
    "updated_date": "2025-05-06 11:03:19 UTC"
  },
  {
    "arxiv_id": "2505.03406v1",
    "title": "Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation",
    "authors": [
      "Mohammad Shoaib Ansari",
      "Mohd Sohail Ali Khan",
      "Shubham Revankar",
      "Aditya Varma",
      "Anil S. Mokhade"
    ],
    "abstract": "This research paper investigates the application of Large Language Models\n(LLMs) in healthcare, specifically focusing on enhancing medical decision\nsupport through Retrieval-Augmented Generation (RAG) integrated with\nhospital-specific data and fine-tuning using Quantized Low-Rank Adaptation\n(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By\nembedding and retrieving context-relevant healthcare information, the system\nsignificantly improves response accuracy. QLoRA facilitates notable parameter\nefficiency and memory optimization, preserving the integrity of medical\ninformation through specialized quantization techniques. Our research also\nshows that our model performs relatively well on various medical benchmarks,\nindicating that it can be used to make basic medical suggestions. This paper\ndetails the system's technical components, including its architecture,\nquantization methods, and key healthcare applications such as enhanced disease\nprediction from patient symptoms and medical history, treatment suggestions,\nand efficient summarization of complex medical reports. We touch on the ethical\nconsiderations-patient privacy, data security, and the need for rigorous\nclinical validation-as well as the practical challenges of integrating such\nsystems into real-world healthcare workflows. Furthermore, the lightweight\nquantized weights ensure scalability and ease of deployment even in\nlow-resource hospital environments. Finally, the paper concludes with an\nanalysis of the broader impact of LLMs on healthcare and outlines future\ndirections for LLMs in medical settings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.03406v1",
    "published_date": "2025-05-06 10:31:54 UTC",
    "updated_date": "2025-05-06 10:31:54 UTC"
  },
  {
    "arxiv_id": "2505.03401v1",
    "title": "DDaTR: Dynamic Difference-aware Temporal Residual Network for Longitudinal Radiology Report Generation",
    "authors": [
      "Shanshan Song",
      "Hui Tang",
      "Honglong Yang",
      "Xiaomeng Li"
    ],
    "abstract": "Radiology Report Generation (RRG) automates the creation of radiology reports\nfrom medical imaging, enhancing the efficiency of the reporting process.\nLongitudinal Radiology Report Generation (LRRG) extends RRG by incorporating\nthe ability to compare current and prior exams, facilitating the tracking of\ntemporal changes in clinical findings. Existing LRRG approaches only extract\nfeatures from prior and current images using a visual pre-trained encoder,\nwhich are then concatenated to generate the final report. However, these\nmethods struggle to effectively capture both spatial and temporal correlations\nduring the feature extraction process. Consequently, the extracted features\ninadequately capture the information of difference across exams and thus\nunderrepresent the expected progressions, leading to sub-optimal performance in\nLRRG. To address this, we develop a novel dynamic difference-aware temporal\nresidual network (DDaTR). In DDaTR, we introduce two modules at each stage of\nthe visual encoder to capture multi-level spatial correlations. The Dynamic\nFeature Alignment Module (DFAM) is designed to align prior features across\nmodalities for the integrity of prior clinical information. Prompted by the\nenriched prior features, the dynamic difference-aware module (DDAM) captures\nfavorable difference information by identifying relationships across exams.\nFurthermore, our DDaTR employs the dynamic residual network to unidirectionally\ntransmit longitudinal information, effectively modelling temporal correlations.\nExtensive experiments demonstrated superior performance over existing methods\non three benchmarks, proving its efficacy in both RRG and LRRG tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03401v1",
    "published_date": "2025-05-06 10:29:23 UTC",
    "updated_date": "2025-05-06 10:29:23 UTC"
  },
  {
    "arxiv_id": "2505.03392v1",
    "title": "Automatic Calibration for Membership Inference Attack on Large Language Models",
    "authors": [
      "Saleh Zare Zade",
      "Yao Qiang",
      "Xiangyu Zhou",
      "Hui Zhu",
      "Mohammad Amin Roshani",
      "Prashant Khanduri",
      "Dongxiao Zhu"
    ],
    "abstract": "Membership Inference Attacks (MIAs) have recently been employed to determine\nwhether a specific text was part of the pre-training data of Large Language\nModels (LLMs). However, existing methods often misinfer non-members as members,\nleading to a high false positive rate, or depend on additional reference models\nfor probability calibration, which limits their practicality. To overcome these\nchallenges, we introduce a novel framework called Automatic Calibration\nMembership Inference Attack (ACMIA), which utilizes a tunable temperature to\ncalibrate output probabilities effectively. This approach is inspired by our\ntheoretical insights into maximum likelihood estimation during the pre-training\nof LLMs. We introduce ACMIA in three configurations designed to accommodate\ndifferent levels of model access and increase the probability gap between\nmembers and non-members, improving the reliability and robustness of membership\ninference. Extensive experiments on various open-source LLMs demonstrate that\nour proposed attack is highly effective, robust, and generalizable, surpassing\nstate-of-the-art baselines across three widely used benchmarks. Our code is\navailable at:\n\\href{https://github.com/Salehzz/ACMIA}{\\textcolor{blue}{Github}}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03392v1",
    "published_date": "2025-05-06 10:15:05 UTC",
    "updated_date": "2025-05-06 10:15:05 UTC"
  },
  {
    "arxiv_id": "2505.03380v1",
    "title": "Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant",
    "authors": [
      "Haonan Wang",
      "Jiaji Mao",
      "Lehan Wang",
      "Qixiang Zhang",
      "Marawan Elbatel",
      "Yi Qin",
      "Huijun Hu",
      "Baoxun Li",
      "Wenhui Deng",
      "Weifeng Qin",
      "Hongrui Li",
      "Jialin Liang",
      "Jun Shen",
      "Xiaomeng Li"
    ],
    "abstract": "Medical AI assistants support doctors in disease diagnosis, medical image\nanalysis, and report generation. However, they still face significant\nchallenges in clinical use, including limited accuracy with multimodal content\nand insufficient validation in real-world settings. We propose RCMed, a\nfull-stack AI assistant that improves multimodal alignment in both input and\noutput, enabling precise anatomical delineation, accurate localization, and\nreliable diagnosis through hierarchical vision-language grounding. A\nself-reinforcing correlation mechanism allows visual features to inform\nlanguage context, while language semantics guide pixel-wise attention, forming\na closed loop that refines both modalities. This correlation is enhanced by a\ncolor region description strategy, translating anatomical structures into\nsemantically rich text to learn shape-location-text relationships across\nscales. Trained on 20 million image-mask-description triplets, RCMed achieves\nstate-of-the-art precision in contextualizing irregular lesions and subtle\nanatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It\nachieved a 23.5% relative improvement in cell segmentation from microscopy\nimages over prior methods. RCMed's strong vision-language alignment enables\nexceptional generalization, with state-of-the-art performance in external\nvalidation across 20 clinically significant cancer types, including novel\ntasks. This work demonstrates how integrated multimodal models capture\nfine-grained patterns, enabling human-level interpretation in complex scenarios\nand advancing human-centric AI healthcare.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03380v1",
    "published_date": "2025-05-06 10:00:08 UTC",
    "updated_date": "2025-05-06 10:00:08 UTC"
  },
  {
    "arxiv_id": "2505.03373v1",
    "title": "SPAP: Structured Pruning via Alternating Optimization and Penalty Methods",
    "authors": [
      "Hanyu Hu",
      "Xiaoming Yuan"
    ],
    "abstract": "The deployment of large language models (LLMs) is often constrained by their\nsubstantial computational and memory demands. While structured pruning presents\na viable approach by eliminating entire network components, existing methods\nsuffer from performance degradation, reliance on heuristic metrics, or\nexpensive finetuning. To address these challenges, we propose SPAP (Structured\nPruning via Alternating Optimization and Penalty Methods), a novel and\nefficient structured pruning framework for LLMs grounded in optimization\ntheory. SPAP formulates the pruning problem through a mixed-integer\noptimization model, employs a penalty method that effectively makes pruning\ndecisions to minimize pruning errors, and introduces an alternating\nminimization algorithm tailored to the splittable problem structure for\nefficient weight updates and performance recovery. Extensive experiments on\nOPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over\nstate-of-the-art methods, delivering linear inference speedups (1.29$\\times$ at\n30% sparsity) and proportional memory reductions. Our work offers a practical,\noptimization-driven solution for pruning LLMs while preserving model\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03373v1",
    "published_date": "2025-05-06 09:47:53 UTC",
    "updated_date": "2025-05-06 09:47:53 UTC"
  },
  {
    "arxiv_id": "2505.03369v1",
    "title": "Validating the Effectiveness of a Large Language Model-based Approach for Identifying Children's Development across Various Free Play Settings in Kindergarten",
    "authors": [
      "Yuanyuan Yang",
      "Yuan Shen",
      "Tianchen Sun",
      "Yangbin Xie"
    ],
    "abstract": "Free play is a fundamental aspect of early childhood education, supporting\nchildren's cognitive, social, emotional, and motor development. However,\nassessing children's development during free play poses significant challenges\ndue to the unstructured and spontaneous nature of the activity. Traditional\nassessment methods often rely on direct observations by teachers, parents, or\nresearchers, which may fail to capture comprehensive insights from free play\nand provide timely feedback to educators. This study proposes an innovative\napproach combining Large Language Models (LLMs) with learning analytics to\nanalyze children's self-narratives of their play experiences. The LLM\nidentifies developmental abilities, while performance scores across different\nplay settings are calculated using learning analytics techniques. We collected\n2,224 play narratives from 29 children in a kindergarten, covering four\ndistinct play areas over one semester. According to the evaluation results from\neight professionals, the LLM-based approach achieved high accuracy in\nidentifying cognitive, motor, and social abilities, with accuracy exceeding 90%\nin most domains. Moreover, significant differences in developmental outcomes\nwere observed across play settings, highlighting each area's unique\ncontributions to specific abilities. These findings confirm that the proposed\napproach is effective in identifying children's development across various free\nplay settings. This study demonstrates the potential of integrating LLMs and\nlearning analytics to provide child-centered insights into developmental\ntrajectories, offering educators valuable data to support personalized learning\nand enhance early childhood education practices.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.03369v1",
    "published_date": "2025-05-06 09:40:47 UTC",
    "updated_date": "2025-05-06 09:40:47 UTC"
  },
  {
    "arxiv_id": "2505.03359v1",
    "title": "Domain Adversarial Training for Mitigating Gender Bias in Speech-based Mental Health Detection",
    "authors": [
      "June-Woo Kim",
      "Haram Yoon",
      "Wonkyo Oh",
      "Dawoon Jung",
      "Sung-Hoon Yoon",
      "Dae-Jin Kim",
      "Dong-Ho Lee",
      "Sang-Yeol Lee",
      "Chan-Mo Yang"
    ],
    "abstract": "Speech-based AI models are emerging as powerful tools for detecting\ndepression and the presence of Post-traumatic stress disorder (PTSD), offering\na non-invasive and cost-effective way to assess mental health. However, these\nmodels often struggle with gender bias, which can lead to unfair and inaccurate\npredictions. In this study, our study addresses this issue by introducing a\ndomain adversarial training approach that explicitly considers gender\ndifferences in speech-based depression and PTSD detection. Specifically, we\ntreat different genders as distinct domains and integrate this information into\na pretrained speech foundation model. We then validate its effectiveness on the\nE-DAIC dataset to assess its impact on performance. Experimental results show\nthat our method notably improves detection performance, increasing the F1-score\nby up to 13.29 percentage points compared to the baseline. This highlights the\nimportance of addressing demographic disparities in AI-driven mental health\nassessment.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to EMBC 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.03359v1",
    "published_date": "2025-05-06 09:29:14 UTC",
    "updated_date": "2025-05-06 09:29:14 UTC"
  },
  {
    "arxiv_id": "2505.03338v1",
    "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI",
    "authors": [
      "Lena Reissinger",
      "Yuanyuan Li",
      "Anna-Carolina Haensch",
      "Neeraj Sarna"
    ],
    "abstract": "Visual Generative AI models have demonstrated remarkable capability in\ngenerating high-quality images from simple inputs like text prompts. However,\nbecause these models are trained on images from diverse sources, they risk\nmemorizing and reproducing specific content, raising concerns about\nintellectual property (IP) infringement. Recent advances in prompt engineering\noffer a cost-effective way to enhance generative AI performance. In this paper,\nwe evaluate the effectiveness of prompt engineering techniques in mitigating IP\ninfringement risks in image generation. Our findings show that Chain of Thought\nPrompting and Task Instruction Prompting significantly reduce the similarity\nbetween generated images and the training data of diffusion models, thereby\nlowering the risk of IP infringement.",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.NA"
    ],
    "primary_category": "math.NA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03338v1",
    "published_date": "2025-05-06 09:10:12 UTC",
    "updated_date": "2025-05-06 09:10:12 UTC"
  },
  {
    "arxiv_id": "2505.03336v1",
    "title": "Avoid Recommending Out-of-Domain Items: Constrained Generative Recommendation with LLMs",
    "authors": [
      "Hao Liao",
      "Wensheng Lu",
      "Jianxun Lian",
      "Mingqi Wu",
      "Shuo Wang",
      "Yong Zhang",
      "Yitian Huang",
      "Mingyang Zhou",
      "Xing Xie"
    ],
    "abstract": "Large Language Models (LLMs) have shown promise for generative recommender\nsystems due to their transformative capabilities in user interaction. However,\nensuring they do not recommend out-of-domain (OOD) items remains a challenge.\nWe study two distinct methods to address this issue: RecLM-ret, a\nretrieval-based method, and RecLM-cgen, a constrained generation method. Both\nmethods integrate seamlessly with existing LLMs to ensure in-domain\nrecommendations. Comprehensive experiments on three recommendation datasets\ndemonstrate that RecLM-cgen consistently outperforms RecLM-ret and existing\nLLM-based recommender models in accuracy while eliminating OOD recommendations,\nmaking it the preferred method for adoption. Additionally, RecLM-cgen maintains\nstrong generalist capabilities and is a lightweight plug-and-play module for\neasy integration into LLMs, offering valuable practical benefits for the\ncommunity. Source code is available at https://github.com/microsoft/RecAI",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.03336v1",
    "published_date": "2025-05-06 09:08:36 UTC",
    "updated_date": "2025-05-06 09:08:36 UTC"
  },
  {
    "arxiv_id": "2505.03335v2",
    "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
    "authors": [
      "Andrew Zhao",
      "Yiran Wu",
      "Yang Yue",
      "Tong Wu",
      "Quentin Xu",
      "Yang Yue",
      "Matthieu Lin",
      "Shenzhi Wang",
      "Qingyun Wu",
      "Zilong Zheng",
      "Gao Huang"
    ],
    "abstract": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in\nenhancing the reasoning capabilities of large language models by learning\ndirectly from outcome-based rewards. Recent RLVR works that operate under the\nzero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training.\nThe scarcity of high-quality, human-produced examples raises concerns about the\nlong-term scalability of relying on human supervision, a challenge already\nevident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by\nhumans may offer limited learning potential for a superintelligent system. To\naddress these concerns, we propose a new RLVR paradigm called Absolute Zero, in\nwhich a single model learns to propose tasks that maximize its own learning\nprogress and improves reasoning by solving them, without relying on any\nexternal data. Under this paradigm, we introduce the Absolute Zero Reasoner\n(AZR), a system that self-evolves its training curriculum and reasoning ability\nby using a code executor to both validate proposed code reasoning tasks and\nverify answers, serving as an unified source of verifiable reward to guide\nopen-ended yet grounded learning. Despite being trained entirely without\nexternal data, AZR achieves overall SOTA performance on coding and mathematical\nreasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate\nthat AZR can be effectively applied across different model scales and is\ncompatible with various model classes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03335v2",
    "published_date": "2025-05-06 09:08:00 UTC",
    "updated_date": "2025-05-07 13:01:17 UTC"
  },
  {
    "arxiv_id": "2505.03332v1",
    "title": "AI-Driven Scholarly Peer Review via Persistent Workflow Prompting, Meta-Prompting, and Meta-Reasoning",
    "authors": [
      "Evgeny Markhasin"
    ],
    "abstract": "Critical peer review of scientific manuscripts presents a significant\nchallenge for Large Language Models (LLMs), partly due to data limitations and\nthe complexity of expert reasoning. This report introduces Persistent Workflow\nPrompting (PWP), a potentially broadly applicable prompt engineering\nmethodology designed to bridge this gap using standard LLM chat interfaces\n(zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical\nanalysis of experimental chemistry manuscripts, featuring a hierarchical,\nmodular architecture (structured via Markdown) that defines detailed analysis\nworkflows. We develop this PWP prompt through iterative application of\nmeta-prompting techniques and meta-reasoning aimed at systematically codifying\nexpert review workflows, including tacit knowledge. Submitted once at the start\nof a session, this PWP prompt equips the LLM with persistent workflows\ntriggered by subsequent queries, guiding modern reasoning LLMs through\nsystematic, multimodal evaluations. Demonstrations show the PWP-guided LLM\nidentifying major methodological flaws in a test case while mitigating LLM\ninput bias and performing complex tasks, including distinguishing claims from\nevidence, integrating text/photo/figure analysis to infer parameters, executing\nquantitative feasibility checks, comparing estimates against claims, and\nassessing a priori plausibility. To ensure transparency and facilitate\nreplication, we provide full prompts, detailed demonstration analyses, and logs\nof interactive chats as supplementary resources. Beyond the specific\napplication, this work offers insights into the meta-development process\nitself, highlighting the potential of PWP, informed by detailed workflow\nformalization, to enable sophisticated analysis using readily available LLMs\nfor complex scientific tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 36 pages (references and appendixes)",
    "pdf_url": "http://arxiv.org/pdf/2505.03332v1",
    "published_date": "2025-05-06 09:06:18 UTC",
    "updated_date": "2025-05-06 09:06:18 UTC"
  },
  {
    "arxiv_id": "2505.03327v1",
    "title": "Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and Self-Supervised Learning",
    "authors": [
      "José-Luis Bueso-Bello",
      "Benjamin Chauvel",
      "Daniel Carcereri",
      "Philipp Posovszky",
      "Pietro Milillo",
      "Jennifer Ruiz",
      "Juan-Carlos Fernández-Diaz",
      "Carolina González",
      "Michele Martone",
      "Ronny Hänsch",
      "Paola Rizzoli"
    ],
    "abstract": "Deep learning models have shown encouraging capabilities for mapping\naccurately forests at medium resolution with TanDEM-X interferometric SAR data.\nSuch models, as most of current state-of-the-art deep learning techniques in\nremote sensing, are trained in a fully-supervised way, which requires a large\namount of labeled data for training and validation. In this work, our aim is to\nexploit the high-resolution capabilities of the TanDEM-X mission to map forests\nat 6 m. The goal is to overcome the intrinsic limitations posed by\nmidresolution products, which affect, e.g., the detection of narrow roads\nwithin vegetated areas and the precise delineation of forested regions\ncontours. To cope with the lack of extended reliable reference datasets at such\na high resolution, we investigate self-supervised learning techniques for\nextracting highly informative representations from the input features, followed\nby a supervised training step with a significantly smaller number of reliable\nlabels. A 1 m resolution forest/non-forest reference map over Pennsylvania,\nUSA, allows for comparing different training approaches for the development of\nan effective forest mapping framework with limited labeled samples. We select\nthe best-performing approach over this test region and apply it in a real-case\nforest mapping scenario over the Amazon rainforest, where only very few labeled\ndata at high resolution are available. In this challenging scenario, the\nproposed self-supervised framework significantly enhances the classification\naccuracy with respect to fully-supervised methods, trained using the same\namount of labeled data, representing an extremely promising starting point for\nlarge-scale, very high-resolution forest mapping with TanDEM-X data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint submitted to Remote Sensing of Environment",
    "pdf_url": "http://arxiv.org/pdf/2505.03327v1",
    "published_date": "2025-05-06 08:54:28 UTC",
    "updated_date": "2025-05-06 08:54:28 UTC"
  },
  {
    "arxiv_id": "2505.03319v1",
    "title": "SD-VSum: A Method and Dataset for Script-Driven Video Summarization",
    "authors": [
      "Manolis Mylonas",
      "Evlampios Apostolidis",
      "Vasileios Mezaris"
    ],
    "abstract": "In this work, we introduce the task of script-driven video summarization,\nwhich aims to produce a summary of the full-length video by selecting the parts\nthat are most relevant to a user-provided script outlining the visual content\nof the desired summary. Following, we extend a recently-introduced large-scale\ndataset for generic video summarization (VideoXum) by producing natural\nlanguage descriptions of the different human-annotated summaries that are\navailable per video. In this way we make it compatible with the introduced\ntask, since the available triplets of ``video, summary and summary\ndescription'' can be used for training a method that is able to produce\ndifferent summaries for a given video, driven by the provided script about the\ncontent of each summary. Finally, we develop a new network architecture for\nscript-driven video summarization (SD-VSum), that relies on the use of a\ncross-modal attention mechanism for aligning and fusing information from the\nvisual and text modalities. Our experimental evaluations demonstrate the\nadvanced performance of SD-VSum against state-of-the-art approaches for\nquery-driven and generic (unimodal and multimodal) summarization from the\nliterature, and document its capacity to produce video summaries that are\nadapted to each user's needs about their content.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2505.03319v1",
    "published_date": "2025-05-06 08:47:14 UTC",
    "updated_date": "2025-05-06 08:47:14 UTC"
  },
  {
    "arxiv_id": "2505.03315v1",
    "title": "Artificial Behavior Intelligence: Technology, Challenges, and Future Directions",
    "authors": [
      "Kanghyun Jo",
      "Jehwan Choi",
      "Kwanho Kim",
      "Seongmin Kim",
      "Duy-Linh Nguyen",
      "Xuan-Thuy Vo",
      "Adri Priadana",
      "Tien-Dat Tran"
    ],
    "abstract": "Understanding and predicting human behavior has emerged as a core capability\nin various AI application domains such as autonomous driving, smart healthcare,\nsurveillance systems, and social robotics. This paper defines the technical\nframework of Artificial Behavior Intelligence (ABI), which comprehensively\nanalyzes and interprets human posture, facial expressions, emotions, behavioral\nsequences, and contextual cues. It details the essential components of ABI,\nincluding pose estimation, face and emotion recognition, sequential behavior\nanalysis, and context-aware modeling. Furthermore, we highlight the\ntransformative potential of recent advances in large-scale pretrained models,\nsuch as large language models (LLMs), vision foundation models, and multimodal\nintegration models, in significantly improving the accuracy and\ninterpretability of behavior recognition. Our research team has a strong\ninterest in the ABI domain and is actively conducting research, particularly\nfocusing on the development of intelligent lightweight models capable of\nefficiently inferring complex human behaviors. This paper identifies several\ntechnical challenges that must be addressed to deploy ABI in real-world\napplications including learning behavioral intelligence from limited data,\nquantifying uncertainty in complex behavior prediction, and optimizing model\nstructures for low-power, real-time inference. To tackle these challenges, our\nteam is exploring various optimization strategies including lightweight\ntransformers, graph-based recognition architectures, energy-aware loss\nfunctions, and multimodal knowledge distillation, while validating their\napplicability in real-time environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 6 figures, Pre-print for IWIS2025",
    "pdf_url": "http://arxiv.org/pdf/2505.03315v1",
    "published_date": "2025-05-06 08:45:44 UTC",
    "updated_date": "2025-05-06 08:45:44 UTC"
  },
  {
    "arxiv_id": "2505.03314v1",
    "title": "Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic Music Generation",
    "authors": [
      "Jincheng Zhang",
      "György Fazekas",
      "Charalampos Saitis"
    ],
    "abstract": "The recent surge in the popularity of diffusion models for image synthesis\nhas attracted new attention to their potential for generation tasks in other\ndomains. However, their applications to symbolic music generation remain\nlargely under-explored because symbolic music is typically represented as\nsequences of discrete events and standard diffusion models are not well-suited\nfor discrete data. We represent symbolic music as image-like pianorolls,\nfacilitating the use of diffusion models for the generation of symbolic music.\nMoreover, this study introduces a novel diffusion model that incorporates our\nproposed Transformer-Mamba block and learnable wavelet transform.\nClassifier-free guidance is utilised to generate symbolic music with target\nchords. Our evaluation shows that our method achieves compelling results in\nterms of music quality and controllability, outperforming the strong baseline\nin pianoroll generation. Our code is available at\nhttps://github.com/jinchengzhanggg/proffusion.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03314v1",
    "published_date": "2025-05-06 08:44:52 UTC",
    "updated_date": "2025-05-06 08:44:52 UTC"
  },
  {
    "arxiv_id": "2505.03303v1",
    "title": "Comparative Analysis of Lightweight Deep Learning Models for Memory-Constrained Devices",
    "authors": [
      "Tasnim Shahriar"
    ],
    "abstract": "This paper presents a comprehensive evaluation of lightweight deep learning\nmodels for image classification, emphasizing their suitability for deployment\nin resource-constrained environments such as low-memory devices. Five\nstate-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,\nEfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse\ndatasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using\nfour key performance metrics: classification accuracy, inference time,\nfloating-point operations (FLOPs), and model size. Additionally, we investigate\nthe impact of hyperparameter tuning, data augmentation, and training paradigms\nby comparing pretrained models with scratch-trained counterparts, focusing on\nMobileNetV3 Small. Our findings reveal that transfer learning significantly\nenhances model accuracy and computational efficiency, particularly for complex\ndatasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest\naccuracy, while MobileNetV3 offers the best balance between accuracy and\nefficiency, and SqueezeNet excels in inference speed and compactness. This\nstudy highlights critical trade-offs between accuracy and efficiency, offering\nactionable insights for deploying lightweight models in real-world applications\nwhere computational resources are limited. By addressing these challenges, this\nresearch contributes to optimizing deep learning systems for edge computing and\nmobile platforms.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68-XX (Primary) 68Txx, 68T07 (Secondary)"
    ],
    "primary_category": "cs.CV",
    "comment": "22 pages, 10 figures, 4 tables, submitted to Springer - Pattern\n  Recognition and Image Analysis",
    "pdf_url": "http://arxiv.org/pdf/2505.03303v1",
    "published_date": "2025-05-06 08:36:01 UTC",
    "updated_date": "2025-05-06 08:36:01 UTC"
  },
  {
    "arxiv_id": "2505.03299v1",
    "title": "Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach",
    "authors": [
      "Pierre Adorni",
      "Minh-Tan Pham",
      "Stéphane May",
      "Sébastien Lefèvre"
    ],
    "abstract": "Foundation models constitute a significant advancement in computer vision:\nafter a single, albeit costly, training phase, they can address a wide array of\ntasks. In the field of Earth observation, over 75 remote sensing vision\nfoundation models have been developed in the past four years. However, none has\nconsistently outperformed the others across all available downstream tasks. To\nfacilitate their comparison, we propose a cost-effective method for predicting\na model's performance on multiple downstream tasks without the need for\nfine-tuning on each one. This method is based on what we call \"capabilities\nencoding.\" The utility of this novel approach is twofold: we demonstrate its\npotential to simplify the selection of a foundation model for a given new task,\nand we employ it to offer a fresh perspective on the existing literature,\nsuggesting avenues for future research. Codes are available at\nhttps://github.com/pierreadorni/capabilities-encoding.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at the MORSE workshop of CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.03299v1",
    "published_date": "2025-05-06 08:29:18 UTC",
    "updated_date": "2025-05-06 08:29:18 UTC"
  },
  {
    "arxiv_id": "2505.03296v1",
    "title": "The Unreasonable Effectiveness of Discrete-Time Gaussian Process Mixtures for Robot Policy Learning",
    "authors": [
      "Jan Ole von Hartz",
      "Adrian Röfer",
      "Joschka Boedecker",
      "Abhinav Valada"
    ],
    "abstract": "We present Mixture of Discrete-time Gaussian Processes (MiDiGap), a novel\napproach for flexible policy representation and imitation learning in robot\nmanipulation. MiDiGap enables learning from as few as five demonstrations using\nonly camera observations and generalizes across a wide range of challenging\ntasks. It excels at long-horizon behaviors such as making coffee, highly\nconstrained motions such as opening doors, dynamic actions such as scooping\nwith a spatula, and multimodal tasks such as hanging a mug. MiDiGap learns\nthese tasks on a CPU in less than a minute and scales linearly to large\ndatasets. We also develop a rich suite of tools for inference-time steering\nusing evidence such as collision signals and robot kinematic constraints. This\nsteering enables novel generalization capabilities, including obstacle\navoidance and cross-embodiment policy transfer. MiDiGap achieves\nstate-of-the-art performance on diverse few-shot manipulation benchmarks. On\nconstrained RLBench tasks, it improves policy success by 76 percentage points\nand reduces trajectory cost by 67%. On multimodal tasks, it improves policy\nsuccess by 48 percentage points and increases sample efficiency by a factor of\n20. In cross-embodiment transfer, it more than doubles policy success. We make\nthe code publicly available at https://midigap.cs.uni-freiburg.de.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Submitted for publication to IEEE Transaction on Robotics",
    "pdf_url": "http://arxiv.org/pdf/2505.03296v1",
    "published_date": "2025-05-06 08:27:23 UTC",
    "updated_date": "2025-05-06 08:27:23 UTC"
  },
  {
    "arxiv_id": "2505.03295v1",
    "title": "Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for Reusing Existing Libraries and Interfaces",
    "authors": [
      "Luis Miguel Vieira da Silva",
      "Aljosha Köcher",
      "Nicolas König",
      "Felix Gehlhoff",
      "Alexander Fay"
    ],
    "abstract": "Modern automation systems increasingly rely on modular architectures, with\ncapabilities and skills as one solution approach. Capabilities define the\nfunctions of resources in a machine-readable form and skills provide the\nconcrete implementations that realize those capabilities. However, the\ndevelopment of a skill implementation conforming to a corresponding capability\nremains a time-consuming and challenging task. In this paper, we present a\nmethod that treats capabilities as contracts for skill implementations and\nleverages large language models to generate executable code based on natural\nlanguage user input. A key feature of our approach is the integration of\nexisting software libraries and interface technologies, enabling the generation\nof skill implementations across different target languages. We introduce a\nframework that allows users to incorporate their own libraries and resource\ninterfaces into the code generation process through a retrieval-augmented\ngeneration architecture. The proposed method is evaluated using an autonomous\nmobile robot controlled via Python and ROS 2, demonstrating the feasibility and\nflexibility of the approach.",
    "categories": [
      "cs.AI",
      "cs.RO",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03295v1",
    "published_date": "2025-05-06 08:27:04 UTC",
    "updated_date": "2025-05-06 08:27:04 UTC"
  },
  {
    "arxiv_id": "2505.03281v1",
    "title": "Physics-inspired Energy Transition Neural Network for Sequence Learning",
    "authors": [
      "Zhou Wu",
      "Junyi An",
      "Baile Xu",
      "Furao Shen",
      "Jian Zhao"
    ],
    "abstract": "Recently, the superior performance of Transformers has made them a more\nrobust and scalable solution for sequence modeling than traditional recurrent\nneural networks (RNNs). However, the effectiveness of Transformer in capturing\nlong-term dependencies is primarily attributed to their comprehensive\npair-modeling process rather than inherent inductive biases toward sequence\nsemantics. In this study, we explore the capabilities of pure RNNs and reassess\ntheir long-term learning mechanisms. Inspired by the physics energy transition\nmodels that track energy changes over time, we propose a effective recurrent\nstructure called the``Physics-inspired Energy Transition Neural Network\"\n(PETNN). We demonstrate that PETNN's memory mechanism effectively stores\ninformation over long-term dependencies. Experimental results indicate that\nPETNN outperforms transformer-based methods across various sequence tasks.\nFurthermore, owing to its recurrent nature, PETNN exhibits significantly lower\ncomplexity. Our study presents an optimal foundational recurrent architecture\nand highlights the potential for developing effective recurrent neural networks\nin fields currently dominated by Transformer.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03281v1",
    "published_date": "2025-05-06 08:07:15 UTC",
    "updated_date": "2025-05-06 08:07:15 UTC"
  },
  {
    "arxiv_id": "2505.03275v1",
    "title": "RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation",
    "authors": [
      "Tiantian Gan",
      "Qiyao Sun"
    ],
    "abstract": "Large language models (LLMs) struggle to effectively utilize a growing number\nof external tools, such as those defined by the Model Context Protocol\n(MCP)\\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We\nintroduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes\nthis challenge by offloading tool discovery. RAG-MCP uses semantic retrieval to\nidentify the most relevant MCP(s) for a given query from an external index\nbefore engaging the LLM. Only the selected tool descriptions are passed to the\nmodel, drastically reducing prompt size and simplifying decision-making.\nExperiments, including an MCP stress test, demonstrate RAG-MCP significantly\ncuts prompt tokens (e.g., by over 50%) and more than triples tool selection\naccuracy (43.13% vs 13.62% baseline) on benchmark tasks. RAG-MCP enables\nscalable and accurate tool integration for LLMs.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03275v1",
    "published_date": "2025-05-06 08:05:35 UTC",
    "updated_date": "2025-05-06 08:05:35 UTC"
  },
  {
    "arxiv_id": "2505.03265v1",
    "title": "Synthline: A Product Line Approach for Synthetic Requirements Engineering Data Generation using Large Language Models",
    "authors": [
      "Abdelkarim El-Hajjami",
      "Camille Salinesi"
    ],
    "abstract": "While modern Requirements Engineering (RE) heavily relies on natural language\nprocessing and Machine Learning (ML) techniques, their effectiveness is limited\nby the scarcity of high-quality datasets. This paper introduces Synthline, a\nProduct Line (PL) approach that leverages Large Language Models to\nsystematically generate synthetic RE data for classification-based use cases.\nThrough an empirical evaluation conducted in the context of using ML for the\nidentification of requirements specification defects, we investigated both the\ndiversity of the generated data and its utility for training downstream models.\nOur analysis reveals that while synthetic datasets exhibit less diversity than\nreal data, they are good enough to serve as viable training resources.\nMoreover, our evaluation shows that combining synthetic and real data leads to\nsubstantial performance improvements. Specifically, hybrid approaches achieve\nup to 85% improvement in precision and a 2x increase in recall compared to\nmodels trained exclusively on real data. These findings demonstrate the\npotential of PL-based synthetic data generation to address data scarcity in RE.\nWe make both our implementation and generated datasets publicly available to\nsupport reproducibility and advancement in the field.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03265v1",
    "published_date": "2025-05-06 07:57:16 UTC",
    "updated_date": "2025-05-06 07:57:16 UTC"
  },
  {
    "arxiv_id": "2505.03242v1",
    "title": "Seeing the Abstract: Translating the Abstract Language for Vision Language Models",
    "authors": [
      "Davide Talon",
      "Federico Girella",
      "Ziyue Liu",
      "Marco Cristani",
      "Yiming Wang"
    ],
    "abstract": "Natural language goes beyond dryly describing visual content. It contains\nrich abstract concepts to express feeling, creativity and properties that\ncannot be directly perceived. Yet, current research in Vision Language Models\n(VLMs) has not shed light on abstract-oriented language. Our research breaks\nnew ground by uncovering its wide presence and under-estimated value, with\nextensive analysis. Particularly, we focus our investigation on the fashion\ndomain, a highly-representative field with abstract expressions. By analyzing\nrecent large-scale multimodal fashion datasets, we find that abstract terms\nhave a dominant presence, rivaling the concrete ones, providing novel\ninformation, and being useful in the retrieval task. However, a critical\nchallenge emerges: current general-purpose or fashion-specific VLMs are\npre-trained with databases that lack sufficient abstract words in their text\ncorpora, thus hindering their ability to effectively represent\nabstract-oriented language. We propose a training-free and model-agnostic\nmethod, Abstract-to-Concrete Translator (ACT), to shift abstract\nrepresentations towards well-represented concrete ones in the VLM latent space,\nusing pre-trained models and existing multimodal databases. On the\ntext-to-image retrieval task, despite being training-free, ACT outperforms the\nfine-tuned VLMs in both same- and cross-dataset settings, exhibiting its\neffectiveness with a strong generalization capability. Moreover, the\nimprovement introduced by ACT is consistent with various VLMs, making it a\nplug-and-play solution.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR25. Project page:\n  https://davidetalon.github.io/fashionact-page/",
    "pdf_url": "http://arxiv.org/pdf/2505.03242v1",
    "published_date": "2025-05-06 07:14:10 UTC",
    "updated_date": "2025-05-06 07:14:10 UTC"
  },
  {
    "arxiv_id": "2505.03217v1",
    "title": "Accelerating Evolution: Integrating PSO Principles into Real-Coded Genetic Algorithm Crossover",
    "authors": [
      "Xiaobo Jin",
      "JiaShu Tu"
    ],
    "abstract": "This study introduces an innovative crossover operator named Particle Swarm\nOptimization-inspired Crossover (PSOX), which is specifically developed for\nreal-coded genetic algorithms. Departing from conventional crossover approaches\nthat only exchange information between individuals within the same generation,\nPSOX uniquely incorporates guidance from both the current global best solution\nand historical optimal solutions across multiple generations. This novel\nmechanism enables the algorithm to maintain population diversity while\nsimultaneously accelerating convergence toward promising regions of the search\nspace. The effectiveness of PSOX is rigorously evaluated through comprehensive\nexperiments on 15 benchmark test functions with diverse characteristics,\nincluding unimodal, multimodal, and highly complex landscapes. Comparative\nanalysis against five state-of-the-art crossover operators reveals that PSOX\nconsistently delivers superior performance in terms of solution accuracy,\nalgorithmic stability, and convergence speed, especially when combined with an\nappropriate mutation strategy. Furthermore, the study provides an in-depth\ninvestigation of how different mutation rates influence PSOX's performance,\nyielding practical guidelines for parameter tuning when addressing optimization\nproblems with varying landscape properties.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "I.2.8; G.1.6"
    ],
    "primary_category": "cs.NE",
    "comment": "14 pages,2 figures,4 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.03217v1",
    "published_date": "2025-05-06 06:17:57 UTC",
    "updated_date": "2025-05-06 06:17:57 UTC"
  },
  {
    "arxiv_id": "2505.03214v1",
    "title": "DocSpiral: A Platform for Integrated Assistive Document Annotation through Human-in-the-Spiral",
    "authors": [
      "Qiang Sun",
      "Sirui Li",
      "Tingting Bi",
      "Du Huynh",
      "Mark Reynolds",
      "Yuanyi Luo",
      "Wei Liu"
    ],
    "abstract": "Acquiring structured data from domain-specific, image-based documents such as\nscanned reports is crucial for many downstream tasks but remains challenging\ndue to document variability. Many of these documents exist as images rather\nthan as machine-readable text, which requires human annotation to train\nautomated extraction systems. We present DocSpiral, the first\nHuman-in-the-Spiral assistive document annotation platform, designed to address\nthe challenge of extracting structured information from domain-specific,\nimage-based document collections. Our spiral design establishes an iterative\ncycle in which human annotations train models that progressively require less\nmanual intervention. DocSpiral integrates document format normalization,\ncomprehensive annotation interfaces, evaluation metrics dashboard, and API\nendpoints for the development of AI / ML models into a unified workflow.\nExperiments demonstrate that our framework reduces annotation time by at least\n41\\% while showing consistent performance gains across three iterations during\nmodel training. By making this annotation platform freely accessible, we aim to\nlower barriers to AI/ML models development in document processing, facilitating\nthe adoption of large language models in image-based, document-intensive fields\nsuch as geoscience and healthcare. The system is freely available at:\nhttps://app.ai4wa.com. The demonstration video is available:\nhttps://app.ai4wa.com/docs/docspiral/demo.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03214v1",
    "published_date": "2025-05-06 06:02:42 UTC",
    "updated_date": "2025-05-06 06:02:42 UTC"
  },
  {
    "arxiv_id": "2505.03204v2",
    "title": "DCS-ST for Classification of Breast Cancer Histopathology Images with Limited Annotations",
    "authors": [
      "Liu Suxing",
      "Byungwon Min"
    ],
    "abstract": "Deep learning methods have shown promise in classifying breast cancer\nhistopathology images, but their performance often declines with limited\nannotated data, a critical challenge in medical imaging due to the high cost\nand expertise required for annotations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03204v2",
    "published_date": "2025-05-06 05:38:17 UTC",
    "updated_date": "2025-05-07 04:09:12 UTC"
  },
  {
    "arxiv_id": "2505.03196v1",
    "title": "A Trustworthy Multi-LLM Network: Challenges,Solutions, and A Use Case",
    "authors": [
      "Haoxiang Luo",
      "Gang Sun",
      "Yinqiu Liu",
      "Dusit Niyato",
      "Hongfang Yu",
      "Mohammed Atiquzzaman",
      "Schahram Dustdar"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate strong potential across a variety of\ntasks in communications and networking due to their advanced reasoning\ncapabilities. However, because different LLMs have different model structures\nand are trained using distinct corpora and methods, they may offer varying\noptimization strategies for the same network issues. Moreover, the limitations\nof an individual LLM's training data, aggravated by the potential maliciousness\nof its hosting device, can result in responses with low confidence or even\nbias. To address these challenges, we propose a blockchain-enabled\ncollaborative framework that connects multiple LLMs into a Trustworthy\nMulti-LLM Network (MultiLLMN). This architecture enables the cooperative\nevaluation and selection of the most reliable and high-quality responses to\ncomplex network optimization problems. Specifically, we begin by reviewing\nrelated work and highlighting the limitations of existing LLMs in collaboration\nand trust, emphasizing the need for trustworthiness in LLM-based systems. We\nthen introduce the workflow and design of the proposed Trustworthy MultiLLMN\nframework. Given the severity of False Base Station (FBS) attacks in B5G and 6G\ncommunication systems and the difficulty of addressing such threats through\ntraditional modeling techniques, we present FBS defense as a case study to\nempirically validate the effectiveness of our approach. Finally, we outline\npromising future research directions in this emerging area.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03196v1",
    "published_date": "2025-05-06 05:32:46 UTC",
    "updated_date": "2025-05-06 05:32:46 UTC"
  },
  {
    "arxiv_id": "2505.03193v1",
    "title": "A study on audio synchronous steganography detection and distributed guide inference model based on sliding spectral features and intelligent inference drive",
    "authors": [
      "Wei Meng"
    ],
    "abstract": "With the rise of short video platforms in global communication, embedding\nsteganographic data in audio synchronization streams has emerged as a new\ncovert communication method. To address the limitations of traditional\ntechniques in detecting synchronized steganography, this paper proposes a\ndetection and distributed guidance reconstruction model based on short video\n\"Yupan\" samples released by China's South Sea Fleet on TikTok. The method\nintegrates sliding spectrum feature extraction and intelligent inference\nmechanisms. A 25 ms sliding window with short-time Fourier transform (STFT) is\nused to extract the main frequency trajectory and construct the synchronization\nframe detection model (M1), identifying a frame flag \"FFFFFFFFFFFFFFFFFF80\".\nThe subsequent 32-byte payload is decoded by a structured model (M2) to infer\ndistributed guidance commands. Analysis reveals a low-entropy, repetitive byte\nsequence in the 36 to 45 second audio segment with highly concentrated spectral\nenergy, confirming the presence of synchronization frames. Although plaintext\nsemantics are not restored, the consistency in command field layout suggests\nfeatures of military communication protocols. The multi-segment splicing model\nfurther shows cross-video embedding and centralized decoding capabilities. The\nproposed framework validates the effectiveness of sliding spectral features for\nsynchronized steganography detection and builds an extensible inference model\nfor covert communication analysis and tactical guidance simulation on open\nplatforms.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CR",
      "eess.AS",
      "94A12 (Primary), 68T07, 42A38 (Secondary)",
      "H.3.3; I.5.4; I.2.6"
    ],
    "primary_category": "cs.SD",
    "comment": "This paper proposes a novel framework for detecting steganographic\n  content in short video audio streams using sliding spectral features and\n  distributed inference models, combining STFT analysis, entropy-based\n  synchronization, and deep learning-driven decoding strategies",
    "pdf_url": "http://arxiv.org/pdf/2505.03193v1",
    "published_date": "2025-05-06 05:24:11 UTC",
    "updated_date": "2025-05-06 05:24:11 UTC"
  },
  {
    "arxiv_id": "2505.03189v1",
    "title": "Patterns and Mechanisms of Contrastive Activation Engineering",
    "authors": [
      "Yixiong Hao",
      "Ayush Panda",
      "Stepan Shabalin",
      "Sheikh Abdur Raheem Ali"
    ],
    "abstract": "Controlling the behavior of Large Language Models (LLMs) remains a\nsignificant challenge due to their inherent complexity and opacity. While\ntechniques like fine-tuning can modify model behavior, they typically require\nextensive computational resources. Recent work has introduced a class of\ncontrastive activation engineering (CAE) techniques as promising approaches for\nsteering LLM outputs through targeted modifications to their internal\nrepresentations. Applied at inference-time with zero cost, CAE has the\npotential to introduce a new paradigm of flexible, task-specific LLM behavior\ntuning. We analyze the performance of CAE in in-distribution,\nout-of-distribution settings, evaluate drawbacks, and begin to develop\ncomprehensive guidelines for its effective deployment. We find that 1. CAE is\nonly reliably effective when applied to in-distribution contexts. 2. Increasing\nthe number of samples used to generate steering vectors has diminishing returns\nat around 80 samples. 3. Steering vectors are susceptible to adversarial inputs\nthat reverses the behavior that is steered for. 4. Steering vectors harm the\noverall model perplexity. 5. Larger models are more resistant to\nsteering-induced degradation.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "Published at the ICLR 2025 Bi-Align, HAIC, and Building Trust\n  workshops",
    "pdf_url": "http://arxiv.org/pdf/2505.03189v1",
    "published_date": "2025-05-06 05:15:12 UTC",
    "updated_date": "2025-05-06 05:15:12 UTC"
  },
  {
    "arxiv_id": "2505.03176v1",
    "title": "seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models",
    "authors": [
      "Hafez Ghaemi",
      "Eilif Muller",
      "Shahab Bakhtiari"
    ],
    "abstract": "Current self-supervised algorithms mostly rely on transformations such as\ndata augmentation and masking to learn visual representations. This is achieved\nby inducing invariance or equivariance with respect to these transformations\nafter encoding two views of an image. This dominant two-view paradigm can limit\nthe flexibility of learned representations for downstream adaptation by\ncreating performance trade-offs between invariance-related tasks such as image\nclassification and more fine-grained equivariance-related tasks. In this work,\nwe introduce \\emph{seq-JEPA}, a world modeling paradigm based on\njoint-embedding predictive architecture that leverages architectural inductive\nbiases to resolve this trade-off. Without requiring an additional equivariance\npredictor or loss term, seq-JEPA simultaneously learns two architecturally\nsegregated representations: one equivariant to the specified transformations\nand another invariant to them and suited for tasks such as classification. To\ndo so, our model processes a short sequence of different views (observations)\nof an input image. Each encoded view is concatenated with embeddings\ncorresponding to the relative transformation (action) producing the next\nobservation in the sequence. A transformer encoder outputs an aggregate\nrepresentation of this sequence, which is subsequently conditioned on the\naction leading to the next observation to predict its representation.\nEmpirically, seq-JEPA achieves strong performance on equivariant benchmarks and\nimage classification without sacrificing one for the other. Additionally, our\nframework excels at tasks that inherently require aggregating a sequence of\nobservations, such as path integration across actions and predictive learning\nacross eye movements.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03176v1",
    "published_date": "2025-05-06 04:39:11 UTC",
    "updated_date": "2025-05-06 04:39:11 UTC"
  },
  {
    "arxiv_id": "2505.03173v1",
    "title": "RAVU: Retrieval Augmented Video Understanding with Compositional Reasoning over Graph",
    "authors": [
      "Sameer Malik",
      "Moyuru Yamada",
      "Ayush Singh",
      "Dishank Aggarwal"
    ],
    "abstract": "Comprehending long videos remains a significant challenge for Large\nMulti-modal Models (LMMs). Current LMMs struggle to process even minutes to\nhours videos due to their lack of explicit memory and retrieval mechanisms. To\naddress this limitation, we propose RAVU (Retrieval Augmented Video\nUnderstanding), a novel framework for video understanding enhanced by retrieval\nwith compositional reasoning over a spatio-temporal graph. We construct a graph\nrepresentation of the video, capturing both spatial and temporal relationships\nbetween entities. This graph serves as a long-term memory, allowing us to track\nobjects and their actions across time. To answer complex queries, we decompose\nthe queries into a sequence of reasoning steps and execute these steps on the\ngraph, retrieving relevant key information. Our approach enables more accurate\nunderstanding of long videos, particularly for queries that require multi-hop\nreasoning and tracking objects across frames. Our approach demonstrate superior\nperformances with limited retrieved frames (5-10) compared with other SOTA\nmethods and baselines on two major video QA datasets, NExT-QA and EgoSchema.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03173v1",
    "published_date": "2025-05-06 04:38:09 UTC",
    "updated_date": "2025-05-06 04:38:09 UTC"
  },
  {
    "arxiv_id": "2505.03172v1",
    "title": "Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning",
    "authors": [
      "Caleb Chuck",
      "Fan Feng",
      "Carl Qi",
      "Chang Shi",
      "Siddhant Agarwal",
      "Amy Zhang",
      "Scott Niekum"
    ],
    "abstract": "Hindsight relabeling is a powerful tool for overcoming sparsity in\ngoal-conditioned reinforcement learning (GCRL), especially in certain domains\nsuch as navigation and locomotion. However, hindsight relabeling can struggle\nin object-centric domains. For example, suppose that the goal space consists of\na robotic arm pushing a particular target block to a goal location. In this\ncase, hindsight relabeling will give high rewards to any trajectory that does\nnot interact with the block. However, these behaviors are only useful when the\nobject is already at the goal -- an extremely rare case in practice. A dataset\ndominated by these kinds of trajectories can complicate learning and lead to\nfailures. In object-centric domains, one key intuition is that meaningful\ntrajectories are often characterized by object-object interactions such as\npushing the block with the gripper. To leverage this intuition, we introduce\nHindsight Relabeling using Interactions (HInt), which combines interactions\nwith hindsight relabeling to improve the sample efficiency of downstream RL.\nHowever because interactions do not have a consensus statistical definition\ntractable for downstream GCRL, we propose a definition of interactions based on\nthe concept of null counterfactual: a cause object is interacting with a target\nobject if, in a world where the cause object did not exist, the target object\nwould have different transition dynamics. We leverage this definition to infer\ninteractions in Null Counterfactual Interaction Inference (NCII), which uses a\n\"nulling'' operation with a learned model to infer interactions. NCII is able\nto achieve significantly improved interaction inference accuracy in both simple\nlinear dynamics domains and dynamic robotic domains in Robosuite, Robot Air\nHockey, and Franka Kitchen and HInt improves sample efficiency by up to 4x.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.03172v1",
    "published_date": "2025-05-06 04:32:47 UTC",
    "updated_date": "2025-05-06 04:32:47 UTC"
  },
  {
    "arxiv_id": "2505.03171v1",
    "title": "CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics",
    "authors": [
      "Junqi Liu",
      "Xiaohan Lin",
      "Jonas Bayer",
      "Yael Dillies",
      "Weijie Jiang",
      "Xiaodan Liang",
      "Roman Soletskyi",
      "Haiming Wang",
      "Yunzhou Xie",
      "Beibei Xiong",
      "Zhengfeng Yang",
      "Jujian Zhang",
      "Lihong Zhi",
      "Jia Li",
      "Zhengying Liu"
    ],
    "abstract": "Neurosymbolic approaches integrating large language models with formal\nreasoning have recently achieved human-level performance on mathematics\ncompetition problems in algebra, geometry and number theory. In comparison,\ncombinatorics remains a challenging domain, characterized by a lack of\nappropriate benchmarks and theorem libraries. To address this gap, we introduce\nCombiBench, a comprehensive benchmark comprising 100 combinatorial problems,\neach formalized in Lean~4 and paired with its corresponding informal statement.\nThe problem set covers a wide spectrum of difficulty levels, ranging from\nmiddle school to IMO and university level, and span over ten combinatorial\ntopics. CombiBench is suitable for testing IMO solving capabilities since it\nincludes all IMO combinatorial problems since 2000 (except IMO 2004 P3 as its\nstatement contain an images). Furthermore, we provide a comprehensive and\nstandardized evaluation framework, dubbed Fine-Eval (for\n$\\textbf{F}$ill-in-the-blank $\\textbf{in}$ L$\\textbf{e}$an Evaluation), for\nformal mathematics. It accommodates not only proof-based problems but also, for\nthe first time, the evaluation of fill-in-the-blank questions. Using Fine-Eval\nas the evaluation method and Kimina Lean Server as the backend, we benchmark\nseveral LLMs on CombiBench and observe that their capabilities for formally\nsolving combinatorial problems remain limited. Among all models tested (none of\nwhich has been trained for this particular task), Kimina-Prover attains the\nbest results, solving 7 problems (out of 100) under both ``with solution'' and\n``without solution'' scenarios. We open source the benchmark dataset alongside\nwith the code of the proposed evaluation method at\nhttps://github.com/MoonshotAI/CombiBench/.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03171v1",
    "published_date": "2025-05-06 04:32:17 UTC",
    "updated_date": "2025-05-06 04:32:17 UTC"
  },
  {
    "arxiv_id": "2505.03156v1",
    "title": "Soft Best-of-n Sampling for Model Alignment",
    "authors": [
      "Claudio Mayrink Verdun",
      "Alex Oesterling",
      "Himabindu Lakkaraju",
      "Flavio P. Calmon"
    ],
    "abstract": "Best-of-$n$ (BoN) sampling is a practical approach for aligning language\nmodel outputs with human preferences without expensive fine-tuning. BoN\nsampling is performed by generating $n$ responses to a prompt and then\nselecting the sample that maximizes a reward function. BoN yields high reward\nvalues in practice at a distortion cost, as measured by the KL-divergence\nbetween the sampled and original distribution. This distortion is coarsely\ncontrolled by varying the number of samples: larger $n$ yields a higher reward\nat a higher distortion cost. We introduce Soft Best-of-$n$ sampling, a\ngeneralization of BoN that allows for smooth interpolation between the original\ndistribution and reward-maximizing distribution through a temperature parameter\n$\\lambda$. We establish theoretical guarantees showing that Soft Best-of-$n$\nsampling converges sharply to the optimal tilted distribution at a rate of\n$O(1/n)$ in KL and the expected (relative) reward. For sequences of discrete\noutputs, we analyze an additive reward model that reveals the fundamental\nlimitations of blockwise sampling.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "Accepted for presentation at the 2025 IEEE International Symposium on\n  Information Theory (ISIT 2025)",
    "pdf_url": "http://arxiv.org/pdf/2505.03156v1",
    "published_date": "2025-05-06 04:03:11 UTC",
    "updated_date": "2025-05-06 04:03:11 UTC"
  },
  {
    "arxiv_id": "2505.03154v1",
    "title": "StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data",
    "authors": [
      "Yuxuan Mu",
      "Hung Yu Ling",
      "Yi Shi",
      "Ismael Baira Ojeda",
      "Pengcheng Xi",
      "Chang Shu",
      "Fabio Zinno",
      "Xue Bin Peng"
    ],
    "abstract": "Motion capture (mocap) data often exhibits visually jarring artifacts due to\ninaccurate sensors and post-processing. Cleaning this corrupted data can\nrequire substantial manual effort from human experts, which can be a costly and\ntime-consuming process. Previous data-driven motion cleanup methods offer the\npromise of automating this cleanup process, but often require in-domain paired\ncorrupted-to-clean training data. Constructing such paired datasets requires\naccess to high-quality, relatively artifact-free motion clips, which often\nnecessitates laborious manual cleanup. In this work, we present StableMotion, a\nsimple yet effective method for training motion cleanup models directly from\nunpaired corrupted datasets that need cleanup. The core component of our method\nis the introduction of motion quality indicators, which can be easily annotated\nthrough manual labeling or heuristic algorithms and enable training of\nquality-aware motion generation models on raw motion data with mixed quality.\nAt test time, the model can be prompted to generate high-quality motions using\nthe quality indicators. Our method can be implemented through a simple\ndiffusion-based framework, leading to a unified motion generate-discriminate\nmodel, which can be used to both identify and fix corrupted frames. We\ndemonstrate that our proposed method is effective for training motion cleanup\nmodels on raw mocap data in production scenarios by applying StableMotion to\nSoccerMocap, a 245-hour soccer mocap dataset containing real-world motion\nartifacts. The trained model effectively corrects a wide range of motion\nartifacts, reducing motion pops and frozen frames by 68% and 81%, respectively.\nSee https://youtu.be/3Y7MMAH02B4 for more results.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.03154v1",
    "published_date": "2025-05-06 04:02:47 UTC",
    "updated_date": "2025-05-06 04:02:47 UTC"
  },
  {
    "arxiv_id": "2505.03149v1",
    "title": "Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo)",
    "authors": [
      "Joseph William Kettelkamp",
      "Ludovica Romanin",
      "Sarv Priya",
      "Mathews Jacob"
    ],
    "abstract": "We introduce an unsupervised motion-compensated image reconstruction\nalgorithm for free-breathing and ungated 3D cardiac magnetic resonance imaging\n(MRI). We express the image volume corresponding to each specific motion phase\nas the deformation of a single static image template. The main contribution of\nthe work is the low-rank model for the compact joint representation of the\nfamily of diffeomorphisms, parameterized by the motion phases. The\ndiffeomorphism at a specific motion phase is obtained by integrating a\nparametric velocity field along a path connecting the reference template phase\nto the motion phase. The velocity field at different phases is represented\nusing a low-rank model. The static template and the low-rank motion model\nparameters are learned directly from the k-space data in an unsupervised\nfashion. The more constrained motion model is observed to offer improved\nrecovery compared to current motion-resolved and motion-compensated algorithms\nfor free-breathing 3D cine MRI.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03149v1",
    "published_date": "2025-05-06 03:52:17 UTC",
    "updated_date": "2025-05-06 03:52:17 UTC"
  },
  {
    "arxiv_id": "2505.03135v1",
    "title": "Holmes: Automated Fact Check with Large Language Models",
    "authors": [
      "Haoran Ou",
      "Gelei Deng",
      "Xingshuo Han",
      "Jie Zhang",
      "Xinlei He",
      "Han Qiu",
      "Shangwei Guo",
      "Tianwei Zhang"
    ],
    "abstract": "The rise of Internet connectivity has accelerated the spread of\ndisinformation, threatening societal trust, decision-making, and national\nsecurity. Disinformation has evolved from simple text to complex multimodal\nforms combining images and text, challenging existing detection methods.\nTraditional deep learning models struggle to capture the complexity of\nmultimodal disinformation. Inspired by advances in AI, this study explores\nusing Large Language Models (LLMs) for automated disinformation detection. The\nempirical study shows that (1) LLMs alone cannot reliably assess the\ntruthfulness of claims; (2) providing relevant evidence significantly improves\ntheir performance; (3) however, LLMs cannot autonomously search for accurate\nevidence. To address this, we propose Holmes, an end-to-end framework featuring\na novel evidence retrieval method that assists LLMs in collecting high-quality\nevidence. Our approach uses (1) LLM-powered summarization to extract key\ninformation from open sources and (2) a new algorithm and metrics to evaluate\nevidence quality. Holmes enables LLMs to verify claims and generate\njustifications effectively. Experiments show Holmes achieves 88.3% accuracy on\ntwo open-source datasets and 90.2% in real-time verification tasks. Notably,\nour improved evidence retrieval boosts fact-checking accuracy by 30.8% over\nexisting methods",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03135v1",
    "published_date": "2025-05-06 03:19:51 UTC",
    "updated_date": "2025-05-06 03:19:51 UTC"
  },
  {
    "arxiv_id": "2505.03132v1",
    "title": "VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis",
    "authors": [
      "Xinyuan Yan",
      "Xiwei Xuan",
      "Jorge Piazentin Ono",
      "Jiajing Guo",
      "Vikram Mohanty",
      "Shekar Arvind Kumar",
      "Liang Gou",
      "Bei Wang",
      "Liu Ren"
    ],
    "abstract": "Real-world machine learning models require rigorous evaluation before\ndeployment, especially in safety-critical domains like autonomous driving and\nsurveillance. The evaluation of machine learning models often focuses on data\nslices, which are subsets of the data that share a set of characteristics. Data\nslice finding automatically identifies conditions or data subgroups where\nmodels underperform, aiding developers in mitigating performance issues.\nDespite its popularity and effectiveness, data slicing for vision model\nvalidation faces several challenges. First, data slicing often needs additional\nimage metadata or visual concepts, and falls short in certain computer vision\ntasks, such as object detection. Second, understanding data slices is a\nlabor-intensive and mentally demanding process that heavily relies on the\nexpert's domain knowledge. Third, data slicing lacks a human-in-the-loop\nsolution that allows experts to form hypothesis and test them interactively. To\novercome these limitations and better support the machine learning operations\nlifecycle, we introduce VISLIX, a novel visual analytics framework that employs\nstate-of-the-art foundation models to help domain experts analyze slices in\ncomputer vision models. Our approach does not require image metadata or visual\nconcepts, automatically generates natural language insights, and allows users\nto test data slice hypothesis interactively. We evaluate VISLIX with an expert\nstudy and three use cases, that demonstrate the effectiveness of our tool in\nproviding comprehensive insights for validating object detection models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03132v1",
    "published_date": "2025-05-06 03:09:15 UTC",
    "updated_date": "2025-05-06 03:09:15 UTC"
  },
  {
    "arxiv_id": "2505.03108v1",
    "title": "Is AI currently capable of identifying wild oysters? A comparison of human annotators against the AI model, ODYSSEE",
    "authors": [
      "Brendan Campbell",
      "Alan Williams",
      "Kleio Baxevani",
      "Alyssa Campbell",
      "Rushabh Dhoke",
      "Rileigh E. Hudock",
      "Xiaomin Lin",
      "Vivek Mange",
      "Bernhard Neuberger",
      "Arjun Suresh",
      "Alhim Vera",
      "Arthur Trembanis",
      "Herbert G. Tanner",
      "Edward Hale"
    ],
    "abstract": "Oysters are ecologically and commercially important species that require\nfrequent monitoring to track population demographics (e.g. abundance, growth,\nmortality). Current methods of monitoring oyster reefs often require\ndestructive sampling methods and extensive manual effort. Therefore, they are\nsuboptimal for small-scale or sensitive environments. A recent alternative, the\nODYSSEE model, was developed to use deep learning techniques to identify live\noysters using video or images taken in the field of oyster reefs to assess\nabundance. The validity of this model in identifying live oysters on a reef was\ncompared to expert and non-expert annotators. In addition, we identified\npotential sources of prediction error. Although the model can make inferences\nsignificantly faster than expert and non-expert annotators (39.6 s, $2.34 \\pm\n0.61$ h, $4.50 \\pm 1.46$ h, respectively), the model overpredicted the number\nof live oysters, achieving lower accuracy (63\\%) in identifying live oysters\ncompared to experts (74\\%) and non-experts (75\\%) alike. Image quality was an\nimportant factor in determining the accuracy of the model and the annotators.\nBetter quality images improved human accuracy and worsened model accuracy.\nAlthough ODYSSEE was not sufficiently accurate, we anticipate that future\ntraining on higher-quality images, utilizing additional live imagery, and\nincorporating additional annotation training classes will greatly improve the\nmodel's predictive power based on the results of this analysis. Future research\nshould address methods that improve the detection of living vs. dead oysters.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03108v1",
    "published_date": "2025-05-06 02:01:27 UTC",
    "updated_date": "2025-05-06 02:01:27 UTC"
  },
  {
    "arxiv_id": "2505.03105v1",
    "title": "Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI Knowledge Co-Creation",
    "authors": [
      "Xule Lin"
    ],
    "abstract": "Scientific knowledge creation is fundamentally transforming as humans and AI\nsystems evolve beyond tool-user relationships into co-evolutionary epistemic\npartnerships. When AlphaFold revolutionized protein structure prediction,\nresearchers described engaging with an epistemic partner that reshaped how they\nconceptualized fundamental relationships. This article introduces Cognitio\nEmergens (CE), a framework addressing critical limitations in existing models\nthat focus on static roles or narrow metrics while failing to capture how\nscientific understanding emerges through recursive human-AI interaction over\ntime. CE integrates three components addressing these limitations: Agency\nConfigurations describing how authority distributes between humans and AI\n(Directed, Contributory, Partnership), with partnerships dynamically\noscillating between configurations rather than following linear progression;\nEpistemic Dimensions capturing six specific capabilities emerging through\ncollaboration across Discovery, Integration, and Projection axes, creating\ndistinctive \"capability signatures\" that guide development; and Partnership\nDynamics identifying forces shaping how these relationships evolve,\nparticularly the risk of epistemic alienation where researchers lose\ninterpretive control over knowledge they formally endorse. Drawing from\nautopoiesis theory, social systems theory, and organizational modularity, CE\nreveals how knowledge co-creation emerges through continuous negotiation of\nroles, values, and organizational structures. By reconceptualizing human-AI\nscientific collaboration as fundamentally co-evolutionary, CE offers a balanced\nperspective that neither uncritically celebrates nor unnecessarily fears AI's\nevolving role, instead providing conceptual tools for cultivating partnerships\nthat maintain meaningful human participation while enabling transformative\nscientific breakthroughs.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "H.5.3; I.2.11; K.4.3; H.1.2; I.2.4"
    ],
    "primary_category": "cs.HC",
    "comment": "62 pages (31 appendix pages for guidance), 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.03105v1",
    "published_date": "2025-05-06 01:49:44 UTC",
    "updated_date": "2025-05-06 01:49:44 UTC"
  },
  {
    "arxiv_id": "2505.03096v1",
    "title": "Assessing and Enhancing the Robustness of LLM-based Multi-Agent Systems Through Chaos Engineering",
    "authors": [
      "Joshua Owotogbe"
    ],
    "abstract": "This study explores the application of chaos engineering to enhance the\nrobustness of Large Language Model-Based Multi-Agent Systems (LLM-MAS) in\nproduction-like environments under real-world conditions. LLM-MAS can\npotentially improve a wide range of tasks, from answering questions and\ngenerating content to automating customer support and improving decision-making\nprocesses. However, LLM-MAS in production or preproduction environments can be\nvulnerable to emergent errors or disruptions, such as hallucinations, agent\nfailures, and agent communication failures. This study proposes a chaos\nengineering framework to proactively identify such vulnerabilities in LLM-MAS,\nassess and build resilience against them, and ensure reliable performance in\ncritical applications.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03096v1",
    "published_date": "2025-05-06 01:13:14 UTC",
    "updated_date": "2025-05-06 01:13:14 UTC"
  },
  {
    "arxiv_id": "2505.03077v1",
    "title": "Latent Adaptive Planner for Dynamic Manipulation",
    "authors": [
      "Donghun Noh",
      "Deqian Kong",
      "Minglu Zhao",
      "Andrew Lizarraga",
      "Jianwen Xie",
      "Ying Nian Wu",
      "Dennis Hong"
    ],
    "abstract": "This paper presents Latent Adaptive Planner (LAP), a novel approach for\ndynamic nonprehensile manipulation tasks that formulates planning as latent\nspace inference, effectively learned from human demonstration videos. Our\nmethod addresses key challenges in visuomotor policy learning through a\nprincipled variational replanning framework that maintains temporal consistency\nwhile efficiently adapting to environmental changes. LAP employs Bayesian\nupdating in latent space to incrementally refine plans as new observations\nbecome available, striking an optimal balance between computational efficiency\nand real-time adaptability. We bridge the embodiment gap between humans and\nrobots through model-based proportional mapping that regenerates accurate\nkinematic-dynamic joint states and object positions from human demonstrations.\nExperimental evaluations across multiple complex manipulation benchmarks\ndemonstrate that LAP achieves state-of-the-art performance, outperforming\nexisting approaches in success rate, trajectory smoothness, and energy\nefficiency, particularly in dynamic adaptation scenarios. Our approach enables\nrobots to perform complex interactions with human-like adaptability while\nproviding an expandable framework applicable to diverse robotic platforms using\nthe same human demonstration videos.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03077v1",
    "published_date": "2025-05-06 00:09:09 UTC",
    "updated_date": "2025-05-06 00:09:09 UTC"
  }
]
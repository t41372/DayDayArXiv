[
  {
    "arxiv_id": "2502.00020v2",
    "title": "Temporal Reasoning in AI systems",
    "authors": [
      "Abhishek Sharma"
    ],
    "abstract": "Commonsense temporal reasoning at scale is a core problem for cognitive\nsystems. The correct inference of the duration for which fluents hold is\nrequired by many tasks, including natural language understanding and planning.\nMany AI systems have limited deductive closure because they cannot extrapolate\ninformation correctly regarding existing fluents and events. In this study, we\ndiscuss the knowledge representation and reasoning schemes required for robust\ntemporal projection in the Cyc Knowledge Base. We discuss how events can start\nand end risk periods for fluents. We then use discrete survival functions,\nwhich represent knowledge of the persistence of facts, to extrapolate a given\nfluent. The extrapolated intervals can be truncated by temporal constraints and\nother types of commonsense knowledge. Finally, we present the results of\nexperiments to demonstrate that these methods obtain significant improvements\nin terms of Q/A performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00020v2",
    "published_date": "2025-01-15 23:47:50 UTC",
    "updated_date": "2025-02-12 21:39:39 UTC"
  },
  {
    "arxiv_id": "2502.10394v1",
    "title": "A Coordination-based Approach for Focused Learning in Knowledge-Based Systems",
    "authors": [
      "Abhishek Sharma"
    ],
    "abstract": "Recent progress in Learning by Reading and Machine Reading systems has\nsignificantly increased the capacity of knowledge-based systems to learn new\nfacts. In this work, we discuss the problem of selecting a set of learning\nrequests for these knowledge-based systems which would lead to maximum Q/A\nperformance. To understand the dynamics of this problem, we simulate the\nproperties of a learning strategy, which sends learning requests to an external\nknowledge source. We show that choosing an optimal set of facts for these\nlearning systems is similar to a coordination game, and use reinforcement\nlearning to solve this problem. Experiments show that such an approach can\nsignificantly improve Q/A performance.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.10394v1",
    "published_date": "2025-01-15 23:45:02 UTC",
    "updated_date": "2025-01-15 23:45:02 UTC"
  },
  {
    "arxiv_id": "2502.00019v1",
    "title": "Growth Patterns of Inference",
    "authors": [
      "Abhishek Sharma"
    ],
    "abstract": "What properties of a first-order search space support/hinder inference? What\nkinds of facts would be most effective to learn? Answering these questions is\nessential for understanding the dynamics of deductive reasoning and creating\nlarge-scale knowledge-based learning systems that support efficient inference.\nWe address these questions by developing a model of how the distribution of\nground facts affects inference performance in search spaces. Experiments\nsuggest that uniform search spaces are suitable for larger KBs whereas search\nspaces with skewed degree distribution show better performance in smaller KBs.\nA sharp transition in Q/A performance is seen in some cases, suggesting that\nanalysis of the structure of search spaces with existing knowledge should be\nused to guide the acquisition of new ground facts in learning systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00019v1",
    "published_date": "2025-01-15 23:41:04 UTC",
    "updated_date": "2025-01-15 23:41:04 UTC"
  },
  {
    "arxiv_id": "2501.09194v2",
    "title": "Grounding Text-to-Image Diffusion Models for Controlled High-Quality Image Generation",
    "authors": [
      "Ahmad Süleyman",
      "Göksel Biricik"
    ],
    "abstract": "Text-to-image (T2I) generative diffusion models have demonstrated outstanding\nperformance in synthesizing diverse, high-quality visuals from text captions.\nSeveral layout-to-image models have been developed to control the generation\nprocess by utilizing a wide range of layouts, such as segmentation maps, edges,\nand human keypoints. In this work, we propose ObjectDiffusion, a model that\nconditions T2I diffusion models on semantic and spatial grounding information,\nenabling the precise rendering and placement of desired objects in specific\nlocations defined by bounding boxes. To achieve this, we make substantial\nmodifications to the network architecture introduced in ControlNet to integrate\nit with the grounding method proposed in GLIGEN. We fine-tune ObjectDiffusion\non the COCO2017 training dataset and evaluate it on the COCO2017 validation\ndataset. Our model improves the precision and quality of controllable image\ngeneration, achieving an AP$_{\\text{50}}$ of 46.6, an AR of 44.5, and an FID of\n19.8, outperforming the current SOTA model trained on open-source datasets\nacross all three metrics. ObjectDiffusion demonstrates a distinctive capability\nin synthesizing diverse, high-quality, high-fidelity images that seamlessly\nconform to the semantic and spatial control layout. Evaluated in qualitative\nand quantitative tests, ObjectDiffusion exhibits remarkable grounding\ncapabilities in closed-set and open-set vocabulary settings across a wide\nvariety of contexts. The qualitative assessment verifies the ability of\nObjectDiffusion to generate multiple detailed objects in varying sizes, forms,\nand locations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09194v2",
    "published_date": "2025-01-15 22:55:26 UTC",
    "updated_date": "2025-02-10 18:54:23 UTC"
  },
  {
    "arxiv_id": "2501.09187v1",
    "title": "Patch-aware Vector Quantized Codebook Learning for Unsupervised Visual Defect Detection",
    "authors": [
      "Qisen Cheng",
      "Shuhui Qu",
      "Janghwan Lee"
    ],
    "abstract": "Unsupervised visual defect detection is critical in industrial applications,\nrequiring a representation space that captures normal data features while\ndetecting deviations. Achieving a balance between expressiveness and\ncompactness is challenging; an overly expressive space risks inefficiency and\nmode collapse, impairing detection accuracy. We propose a novel approach using\nan enhanced VQ-VAE framework optimized for unsupervised defect detection. Our\nmodel introduces a patch-aware dynamic code assignment scheme, enabling\ncontext-sensitive code allocation to optimize spatial representation. This\nstrategy enhances normal-defect distinction and improves detection accuracy\nduring inference. Experiments on MVTecAD, BTAD, and MTSD datasets show our\nmethod achieves state-of-the-art performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, Accepted to 36th IEEE ICTAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2501.09187v1",
    "published_date": "2025-01-15 22:26:26 UTC",
    "updated_date": "2025-01-15 22:26:26 UTC"
  },
  {
    "arxiv_id": "2501.09186v1",
    "title": "Guiding Retrieval using LLM-based Listwise Rankers",
    "authors": [
      "Mandeep Rathee",
      "Sean MacAvaney",
      "Avishek Anand"
    ],
    "abstract": "Large Language Models (LLMs) have shown strong promise as rerankers,\nespecially in ``listwise'' settings where an LLM is prompted to rerank several\nsearch results at once. However, this ``cascading'' retrieve-and-rerank\napproach is limited by the bounded recall problem: relevant documents not\nretrieved initially are permanently excluded from the final ranking. Adaptive\nretrieval techniques address this problem, but do not work with listwise\nrerankers because they assume a document's score is computed independently from\nother documents. In this paper, we propose an adaptation of an existing\nadaptive retrieval method that supports the listwise setting and helps guide\nthe retrieval process itself (thereby overcoming the bounded recall problem for\nLLM rerankers). Specifically, our proposed algorithm merges results both from\nthe initial ranking and feedback documents provided by the most relevant\ndocuments seen up to that point. Through extensive experiments across diverse\nLLM rerankers, first stage retrievers, and feedback sources, we demonstrate\nthat our method can improve nDCG@10 by up to 13.23% and recall by 28.02%--all\nwhile keeping the total number of LLM inferences constant and overheads due to\nthe adaptive process minimal. The work opens the door to leveraging LLM-based\nsearch in settings where the initial pool of results is limited, e.g., by\nlegacy systems, or by the cost of deploying a semantic first-stage.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "16 pages, 2 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.09186v1",
    "published_date": "2025-01-15 22:23:53 UTC",
    "updated_date": "2025-01-15 22:23:53 UTC"
  },
  {
    "arxiv_id": "2501.09182v1",
    "title": "A Blockchain-Enabled Approach to Cross-Border Compliance and Trust",
    "authors": [
      "Vikram Kulothungan"
    ],
    "abstract": "As artificial intelligence (AI) systems become increasingly integral to\ncritical infrastructure and global operations, the need for a unified,\ntrustworthy governance framework is more urgent that ever. This paper proposes\na novel approach to AI governance, utilizing blockchain and distributed ledger\ntechnologies (DLT) to establish a decentralized, globally recognized framework\nthat ensures security, privacy, and trustworthiness of AI systems across\nborders. The paper presents specific implementation scenarios within the\nfinancial sector, outlines a phased deployment timeline over the next decade,\nand addresses potential challenges with solutions grounded in current research.\nBy synthesizing advancements in blockchain, AI ethics, and cybersecurity, this\npaper offers a comprehensive roadmap for a decentralized AI governance\nframework capable of adapting to the complex and evolving landscape of global\nAI regulation.",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.CY",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "This is a preprint of paper that has been accepted for Publication at\n  2024 IEEE International Conference on Trust, Privacy and Security in\n  Intelligent Systems, and Applications",
    "pdf_url": "http://arxiv.org/pdf/2501.09182v1",
    "published_date": "2025-01-15 22:19:34 UTC",
    "updated_date": "2025-01-15 22:19:34 UTC"
  },
  {
    "arxiv_id": "2501.09166v1",
    "title": "Attention is All You Need Until You Need Retention",
    "authors": [
      "M. Murat Yaslioglu"
    ],
    "abstract": "This work introduces a novel Retention Layer mechanism for Transformer based\narchitectures, addressing their inherent lack of intrinsic retention\ncapabilities. Unlike human cognition, which can encode and dynamically recall\nsymbolic templates, Generative Pretrained Transformers rely solely on fixed\npretrained weights and ephemeral context windows, limiting their adaptability.\nThe proposed Retention Layer incorporates a persistent memory module capable of\nreal time data population, dynamic recall, and guided output generation. This\nenhancement allows models to store, update, and reuse observed patterns across\nsessions, enabling incremental learning and bridging the gap between static\npretraining and dynamic, context sensitive adaptation. The Retention Layer\ndesign parallels social learning processes, encompassing attention, retention,\nreproduction, and motivation stages. Technically, it integrates a memory\nattention mechanism and episodic buffers to manage memory scalability, mitigate\noverfitting, and ensure efficient recall. Applications span adaptive personal\nassistants, real time fraud detection, autonomous robotics, content moderation,\nand healthcare diagnostics. In each domain, the retention mechanism enables\nsystems to learn incrementally, personalize outputs, and respond to evolving\nreal world challenges effectively. By emulating key aspects of human learning,\nthis retention enhanced architecture fosters a more fluid and responsive AI\nparadigm, paving the way for dynamic, session aware models that extend the\ncapabilities of traditional Transformers into domains requiring continual\nadaptation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09166v1",
    "published_date": "2025-01-15 21:33:53 UTC",
    "updated_date": "2025-01-15 21:33:53 UTC"
  },
  {
    "arxiv_id": "2501.09164v1",
    "title": "The Veln(ia)s is in the Details: Evaluating LLM Judgment on Latvian and Lithuanian Short Answer Matching",
    "authors": [
      "Yevhen Kostiuk",
      "Oxana Vitman",
      "Łukasz Gagała",
      "Artur Kiulian"
    ],
    "abstract": "In this work, we address the challenge of evaluating large language models\n(LLMs) on the short answer matching task for Latvian and Lithuanian languages.\nWe introduce novel datasets consisting of 502 Latvian and 690 Lithuanian\nquestion-answer pairs. For each question-answer pair, we generated matched and\nnon-matched answers using a set of alteration rules specifically designed to\nintroduce small but meaningful changes in the text. These generated answers\nserve as test cases to assess the ability of LLMs to detect subtle differences\nin matching of the original answers. A subset of the datasets was manually\nverified for quality and accuracy. Our results show that while larger LLMs,\nsuch as QWEN2.5 72b and LLaMa3.1 70b, demonstrate near-perfect performance in\ndistinguishing matched and non-matched answers, smaller models show more\nvariance. For instance, LLaMa3.1 8b and EuroLLM 9b benefited from few-shot\nexamples, while Mistral Nemo 12b underperformed on detection of subtle text\nalteration, particularly in Lithuanian, even with additional examples. QWEN2.5\n7b and Mistral 7b were able to obtain a strong and comparable performance to\nthe larger 70b models in zero and few shot experiments. Moreover, the\nperformance of Mistral 7b was weaker in few shot experiments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09164v1",
    "published_date": "2025-01-15 21:30:03 UTC",
    "updated_date": "2025-01-15 21:30:03 UTC"
  },
  {
    "arxiv_id": "2501.09163v1",
    "title": "Towards Understanding Extrapolation: a Causal Lens",
    "authors": [
      "Lingjing Kong",
      "Guangyi Chen",
      "Petar Stojanov",
      "Haoxuan Li",
      "Eric P. Xing",
      "Kun Zhang"
    ],
    "abstract": "Canonical work handling distribution shifts typically necessitates an entire\ntarget distribution that lands inside the training distribution. However,\npractical scenarios often involve only a handful of target samples, potentially\nlying outside the training support, which requires the capability of\nextrapolation. In this work, we aim to provide a theoretical understanding of\nwhen extrapolation is possible and offer principled methods to achieve it\nwithout requiring an on-support target distribution. To this end, we formulate\nthe extrapolation problem with a latent-variable model that embodies the\nminimal change principle in causal mechanisms. Under this formulation, we cast\nthe extrapolation problem into a latent-variable identification problem. We\nprovide realistic conditions on shift properties and the estimation objectives\nthat lead to identification even when only one off-support target sample is\navailable, tackling the most challenging scenarios. Our theory reveals the\nintricate interplay between the underlying manifold's smoothness and the shift\nproperties. We showcase how our theoretical results inform the design of\npractical adaptation algorithms. Through experiments on both synthetic and\nreal-world data, we validate our theoretical findings and their practical\nimplications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2501.09163v1",
    "published_date": "2025-01-15 21:29:29 UTC",
    "updated_date": "2025-01-15 21:29:29 UTC"
  },
  {
    "arxiv_id": "2501.09160v1",
    "title": "AutoLoop: Fast Visual SLAM Fine-tuning through Agentic Curriculum Learning",
    "authors": [
      "Assaf Lahiany",
      "Oren Gal"
    ],
    "abstract": "Current visual SLAM systems face significant challenges in balancing\ncomputational efficiency with robust loop closure handling. Traditional\napproaches require careful manual tuning and incur substantial computational\noverhead, while learning-based methods either lack explicit loop closure\ncapabilities or implement them through computationally expensive methods. We\npresent AutoLoop, a novel approach that combines automated curriculum learning\nwith efficient fine-tuning for visual SLAM systems. Our method employs a DDPG\n(Deep Deterministic Policy Gradient) agent to dynamically adjust loop closure\nweights during training, eliminating the need for manual hyperparameter search\nwhile significantly reducing the required training steps. The approach\npre-computes potential loop closure pairs offline and leverages them through an\nagent-guided curriculum, allowing the model to adapt efficiently to new\nscenarios. Experiments conducted on TartanAir for training and validated across\nmultiple benchmarks including KITTI, EuRoC, ICL-NUIM and TUM RGB-D demonstrate\nthat AutoLoop achieves comparable or superior performance while reducing\ntraining time by an order of magnitude compared to traditional approaches.\nAutoLoop provides a practical solution for rapid adaptation of visual SLAM\nsystems, automating the weight tuning process that traditionally requires\nmultiple manual iterations. Our results show that this automated curriculum\nstrategy not only accelerates training but also maintains or improves the\nmodel's performance across diverse environmental conditions.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09160v1",
    "published_date": "2025-01-15 21:22:09 UTC",
    "updated_date": "2025-01-15 21:22:09 UTC"
  },
  {
    "arxiv_id": "2501.09154v1",
    "title": "Towards Multilingual LLM Evaluation for Baltic and Nordic languages: A study on Lithuanian History",
    "authors": [
      "Yevhen Kostiuk",
      "Oxana Vitman",
      "Łukasz Gagała",
      "Artur Kiulian"
    ],
    "abstract": "In this work, we evaluated Lithuanian and general history knowledge of\nmultilingual Large Language Models (LLMs) on a multiple-choice\nquestion-answering task. The models were tested on a dataset of Lithuanian\nnational and general history questions translated into Baltic, Nordic, and\nother languages (English, Ukrainian, Arabic) to assess the knowledge sharing\nfrom culturally and historically connected groups. We evaluated GPT-4o,\nLLaMa3.1 8b and 70b, QWEN2.5 7b and 72b, Mistral Nemo 12b, LLaMa3 8b, Mistral\n7b, LLaMa3.2 3b, and Nordic fine-tuned models (GPT-SW3 and LLaMa3 8b).\n  Our results show that GPT-4o consistently outperformed all other models\nacross language groups, with slightly better results for Baltic and Nordic\nlanguages. Larger open-source models like QWEN2.5 72b and LLaMa3.1 70b\nperformed well but showed weaker alignment with Baltic languages. Smaller\nmodels (Mistral Nemo 12b, LLaMa3.2 3b, QWEN 7B, LLaMa3.1 8B, and LLaMa3 8b)\ndemonstrated gaps with LT-related alignment with Baltic languages while\nperforming better on Nordic and other languages. The Nordic fine-tuned models\ndid not surpass multilingual models, indicating that shared cultural or\nhistorical context alone does not guarantee better performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09154v1",
    "published_date": "2025-01-15 21:14:09 UTC",
    "updated_date": "2025-01-15 21:14:09 UTC"
  },
  {
    "arxiv_id": "2501.09136v3",
    "title": "Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG",
    "authors": [
      "Aditi Singh",
      "Abul Ehtesham",
      "Saket Kumar",
      "Tala Talaei Khoei"
    ],
    "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence (AI)\nby enabling human like text generation and natural language understanding.\nHowever, their reliance on static training data limits their ability to respond\nto dynamic, real time queries, resulting in outdated or inaccurate outputs.\nRetrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs\nby integrating real time data retrieval to provide contextually relevant and\nup-to-date responses. Despite its promise, traditional RAG systems are\nconstrained by static workflows and lack the adaptability required for\nmultistep reasoning and complex task management.\n  Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these\nlimitations by embedding autonomous AI agents into the RAG pipeline. These\nagents leverage agentic design patterns reflection, planning, tool use, and\nmultiagent collaboration to dynamically manage retrieval strategies,\niteratively refine contextual understanding, and adapt workflows to meet\ncomplex task requirements. This integration enables Agentic RAG systems to\ndeliver unparalleled flexibility, scalability, and context awareness across\ndiverse applications.\n  This survey provides a comprehensive exploration of Agentic RAG, beginning\nwith its foundational principles and the evolution of RAG paradigms. It\npresents a detailed taxonomy of Agentic RAG architectures, highlights key\napplications in industries such as healthcare, finance, and education, and\nexamines practical implementation strategies. Additionally, it addresses\nchallenges in scaling these systems, ensuring ethical decision making, and\noptimizing performance for real-world applications, while providing detailed\ninsights into frameworks and tools for implementing Agentic RAG.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09136v3",
    "published_date": "2025-01-15 20:40:25 UTC",
    "updated_date": "2025-02-04 04:48:00 UTC"
  },
  {
    "arxiv_id": "2501.09134v1",
    "title": "Benchmarking Robustness of Contrastive Learning Models for Medical Image-Report Retrieval",
    "authors": [
      "Demetrio Deanda",
      "Yuktha Priya Masupalli",
      "Jeong Yang",
      "Young Lee",
      "Zechun Cao",
      "Gongbo Liang"
    ],
    "abstract": "Medical images and reports offer invaluable insights into patient health. The\nheterogeneity and complexity of these data hinder effective analysis. To bridge\nthis gap, we investigate contrastive learning models for cross-domain\nretrieval, which associates medical images with their corresponding clinical\nreports. This study benchmarks the robustness of four state-of-the-art\ncontrastive learning models: CLIP, CXR-RePaiR, MedCLIP, and CXR-CLIP. We\nintroduce an occlusion retrieval task to evaluate model performance under\nvarying levels of image corruption. Our findings reveal that all evaluated\nmodels are highly sensitive to out-of-distribution data, as evidenced by the\nproportional decrease in performance with increasing occlusion levels. While\nMedCLIP exhibits slightly more robustness, its overall performance remains\nsignificantly behind CXR-CLIP and CXR-RePaiR. CLIP, trained on a\ngeneral-purpose dataset, struggles with medical image-report retrieval,\nhighlighting the importance of domain-specific training data. The evaluation of\nthis work suggests that more effort needs to be spent on improving the\nrobustness of these models. By addressing these limitations, we can develop\nmore reliable cross-domain retrieval models for medical applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "This work is accepted to AAAI 2025 Workshop -- the 9th International\n  Workshop on Health Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2501.09134v1",
    "published_date": "2025-01-15 20:37:04 UTC",
    "updated_date": "2025-01-15 20:37:04 UTC"
  },
  {
    "arxiv_id": "2501.09114v1",
    "title": "Generative Medical Image Anonymization Based on Latent Code Projection and Optimization",
    "authors": [
      "Huiyu Li",
      "Nicholas Ayache",
      "Hervé Delingette"
    ],
    "abstract": "Medical image anonymization aims to protect patient privacy by removing\nidentifying information, while preserving the data utility to solve downstream\ntasks. In this paper, we address the medical image anonymization problem with a\ntwo-stage solution: latent code projection and optimization. In the projection\nstage, we design a streamlined encoder to project input images into a latent\nspace and propose a co-training scheme to enhance the projection process. In\nthe optimization stage, we refine the latent code using two deep loss functions\ndesigned to address the trade-off between identity protection and data utility\ndedicated to medical images. Through a comprehensive set of qualitative and\nquantitative experiments, we showcase the effectiveness of our approach on the\nMIMIC-CXR chest X-ray dataset by generating anonymized synthetic images that\ncan serve as training set for detecting lung pathologies. Source codes are\navailable at https://github.com/Huiyu-Li/GMIA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Conference",
    "pdf_url": "http://arxiv.org/pdf/2501.09114v1",
    "published_date": "2025-01-15 19:50:56 UTC",
    "updated_date": "2025-01-15 19:50:56 UTC"
  },
  {
    "arxiv_id": "2501.09112v1",
    "title": "Mantis Shrimp: Exploring Photometric Band Utilization in Computer Vision Networks for Photometric Redshift Estimation",
    "authors": [
      "Andrew Engel",
      "Nell Byler",
      "Adam Tsou",
      "Gautham Narayan",
      "Emmanuel Bonilla",
      "Ian Smith"
    ],
    "abstract": "We present Mantis Shrimp, a multi-survey deep learning model for photometric\nredshift estimation that fuses ultra-violet (GALEX), optical (PanSTARRS), and\ninfrared (UnWISE) imagery. Machine learning is now an established approach for\nphotometric redshift estimation, with generally acknowledged higher performance\nin areas with a high density of spectroscopically identified galaxies over\ntemplate-based methods. Multiple works have shown that image-based\nconvolutional neural networks can outperform tabular-based color/magnitude\nmodels. In comparison to tabular models, image models have additional design\ncomplexities: it is largely unknown how to fuse inputs from different\ninstruments which have different resolutions or noise properties. The Mantis\nShrimp model estimates the conditional density estimate of redshift using\ncutout images. The density estimates are well calibrated and the point\nestimates perform well in the distribution of available spectroscopically\nconfirmed galaxies with (bias = 1e-2), scatter (NMAD = 2.44e-2) and\ncatastrophic outlier rate ($\\eta$=17.53$\\%$). We find that early fusion\napproaches (e.g., resampling and stacking images from different instruments)\nmatch the performance of late fusion approaches (e.g., concatenating latent\nspace representations), so that the design choice ultimately is left to the\nuser. Finally, we study how the models learn to use information across bands,\nfinding evidence that our models successfully incorporates information from all\nsurveys. The applicability of our model to the analysis of large populations of\ngalaxies is limited by the speed of downloading cutouts from external servers;\nhowever, our model could be useful in smaller studies such as generating priors\nover redshift for stellar population synthesis.",
    "categories": [
      "astro-ph.IM",
      "cs.AI"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09112v1",
    "published_date": "2025-01-15 19:46:23 UTC",
    "updated_date": "2025-01-15 19:46:23 UTC"
  },
  {
    "arxiv_id": "2501.09104v2",
    "title": "A Non-autoregressive Model for Joint STT and TTS",
    "authors": [
      "Vishal Sunder",
      "Brian Kingsbury",
      "George Saon",
      "Samuel Thomas",
      "Slava Shechtman",
      "Hagai Aronowitz",
      "Eric Fosler-Lussier",
      "Luis Lastras"
    ],
    "abstract": "In this paper, we take a step towards jointly modeling automatic speech\nrecognition (STT) and speech synthesis (TTS) in a fully non-autoregressive way.\nWe develop a novel multimodal framework capable of handling the speech and text\nmodalities as input either individually or together. The proposed model can\nalso be trained with unpaired speech or text data owing to its multimodal\nnature. We further propose an iterative refinement strategy to improve the STT\nand TTS performance of our model such that the partial hypothesis at the output\ncan be fed back to the input of our model, thus iteratively improving both STT\nand TTS predictions. We show that our joint model can effectively perform both\nSTT and TTS tasks, outperforming the STT-specific baseline in all tasks and\nperforming competitively with the TTS-specific baseline across a wide range of\nevaluation metrics.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, 3 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.09104v2",
    "published_date": "2025-01-15 19:42:41 UTC",
    "updated_date": "2025-01-20 18:35:40 UTC"
  },
  {
    "arxiv_id": "2501.09102v1",
    "title": "Tracking the Takes and Trajectories of English-Language News Narratives across Trustworthy and Worrisome Websites",
    "authors": [
      "Hans W. A. Hanley",
      "Emily Okabe",
      "Zakir Durumeric"
    ],
    "abstract": "Understanding how misleading and outright false information enters news\necosystems remains a difficult challenge that requires tracking how narratives\nspread across thousands of fringe and mainstream news websites. To do this, we\nintroduce a system that utilizes encoder-based large language models and\nzero-shot stance detection to scalably identify and track news narratives and\ntheir attitudes across over 4,000 factually unreliable, mixed-reliability, and\nfactually reliable English-language news websites. Running our system over an\n18 month period, we track the spread of 146K news stories. Using network-based\ninterference via the NETINF algorithm, we show that the paths of news\nnarratives and the stances of websites toward particular entities can be used\nto uncover slanted propaganda networks (e.g., anti-vaccine and anti-Ukraine)\nand to identify the most influential websites in spreading these attitudes in\nthe broader news ecosystem. We hope that increased visibility into our\ndistributed news ecosystem can help with the reporting and fact-checking of\npropaganda and disinformation.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "To appear at USENIX Security Symposium 2025. Keywords:\n  Misinformation, News, Narratives, LLMs, Stance-Detection",
    "pdf_url": "http://arxiv.org/pdf/2501.09102v1",
    "published_date": "2025-01-15 19:37:44 UTC",
    "updated_date": "2025-01-15 19:37:44 UTC"
  },
  {
    "arxiv_id": "2501.09092v1",
    "title": "SteLLA: A Structured Grading System Using LLMs with RAG",
    "authors": [
      "Hefei Qiu",
      "Brian White",
      "Ashley Ding",
      "Reinaldo Costa",
      "Ali Hachem",
      "Wei Ding",
      "Ping Chen"
    ],
    "abstract": "Large Language Models (LLMs) have shown strong general capabilities in many\napplications. However, how to make them reliable tools for some specific tasks\nsuch as automated short answer grading (ASAG) remains a challenge. We present\nSteLLA (Structured Grading System Using LLMs with RAG) in which a) Retrieval\nAugmented Generation (RAG) approach is used to empower LLMs specifically on the\nASAG task by extracting structured information from the highly relevant and\nreliable external knowledge based on the instructor-provided reference answer\nand rubric, b) an LLM performs a structured and question-answering-based\nevaluation of student answers to provide analytical grades and feedback. A\nreal-world dataset that contains students' answers in an exam was collected\nfrom a college-level Biology course. Experiments show that our proposed system\ncan achieve substantial agreement with the human grader while providing\nbreak-down grades and feedback on all the knowledge points examined in the\nproblem. A qualitative and error analysis of the feedback generated by GPT4\nshows that GPT4 is good at capturing facts while may be prone to inferring too\nmuch implication from the given text in the grading task which provides\ninsights into the usage of LLMs in the ASAG system.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09092v1",
    "published_date": "2025-01-15 19:24:48 UTC",
    "updated_date": "2025-01-15 19:24:48 UTC"
  },
  {
    "arxiv_id": "2501.09081v1",
    "title": "Inferring Transition Dynamics from Value Functions",
    "authors": [
      "Jacob Adamczyk"
    ],
    "abstract": "In reinforcement learning, the value function is typically trained to solve\nthe Bellman equation, which connects the current value to future values. This\ntemporal dependency hints that the value function may contain implicit\ninformation about the environment's transition dynamics. By rearranging the\nBellman equation, we show that a converged value function encodes a model of\nthe underlying dynamics of the environment. We build on this insight to propose\na simple method for inferring dynamics models directly from the value function,\npotentially mitigating the need for explicit model learning. Furthermore, we\nexplore the challenges of next-state identifiability, discussing conditions\nunder which the inferred dynamics model is well-defined. Our work provides a\ntheoretical foundation for leveraging value functions in dynamics modeling and\nopens a new avenue for bridging model-free and model-based reinforcement\nlearning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the AAAI-25 8th Workshop on Generalization in Planning",
    "pdf_url": "http://arxiv.org/pdf/2501.09081v1",
    "published_date": "2025-01-15 19:00:47 UTC",
    "updated_date": "2025-01-15 19:00:47 UTC"
  },
  {
    "arxiv_id": "2501.09080v1",
    "title": "Average-Reward Reinforcement Learning with Entropy Regularization",
    "authors": [
      "Jacob Adamczyk",
      "Volodymyr Makarenko",
      "Stas Tiomkin",
      "Rahul V. Kulkarni"
    ],
    "abstract": "The average-reward formulation of reinforcement learning (RL) has drawn\nincreased interest in recent years due to its ability to solve\ntemporally-extended problems without discounting. Independently, RL algorithms\nhave benefited from entropy-regularization: an approach used to make the\noptimal policy stochastic, thereby more robust to noise. Despite the distinct\nbenefits of the two approaches, the combination of entropy regularization with\nan average-reward objective is not well-studied in the literature and there has\nbeen limited development of algorithms for this setting. To address this gap in\nthe field, we develop algorithms for solving entropy-regularized average-reward\nRL problems with function approximation. We experimentally validate our method,\ncomparing it with existing algorithms on standard benchmarks for RL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the AAAI-25 Eighth Workshop on Bridging the Gap Between\n  AI Planning and Reinforcement Learning (PRL)",
    "pdf_url": "http://arxiv.org/pdf/2501.09080v1",
    "published_date": "2025-01-15 19:00:46 UTC",
    "updated_date": "2025-01-15 19:00:46 UTC"
  },
  {
    "arxiv_id": "2501.09770v1",
    "title": "EVAL: EigenVector-based Average-reward Learning",
    "authors": [
      "Jacob Adamczyk",
      "Volodymyr Makarenko",
      "Stas Tiomkin",
      "Rahul V. Kulkarni"
    ],
    "abstract": "In reinforcement learning, two objective functions have been developed\nextensively in the literature: discounted and averaged rewards. The\ngeneralization to an entropy-regularized setting has led to improved robustness\nand exploration for both of these objectives. Recently, the entropy-regularized\naverage-reward problem was addressed using tools from large deviation theory in\nthe tabular setting. This method has the advantage of linearity, providing\naccess to both the optimal policy and average reward-rate through properties of\na single matrix. In this paper, we extend that framework to more general\nsettings by developing approaches based on function approximation by neural\nnetworks. This formulation reveals new theoretical insights into the\nrelationship between different objectives used in RL. Additionally, we combine\nour algorithm with a posterior policy iteration scheme, showing how our\napproach can also solve the average-reward RL problem without\nentropy-regularization. Using classic control benchmarks, we experimentally\nfind that our method compares favorably with other algorithms in terms of\nstability and rate of convergence.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the AAAI-25 8th Workshop on Generalization in Planning.\n  arXiv admin note: text overlap with arXiv:2501.09080",
    "pdf_url": "http://arxiv.org/pdf/2501.09770v1",
    "published_date": "2025-01-15 19:00:45 UTC",
    "updated_date": "2025-01-15 19:00:45 UTC"
  },
  {
    "arxiv_id": "2501.09014v1",
    "title": "How Do Generative Models Draw a Software Engineer? A Case Study on Stable Diffusion Bias",
    "authors": [
      "Tosin Fadahunsi",
      "Giordano d'Aloisio",
      "Antinisca Di Marco",
      "Federica Sarro"
    ],
    "abstract": "Generative models are nowadays widely used to generate graphical content used\nfor multiple purposes, e.g. web, art, advertisement. However, it has been shown\nthat the images generated by these models could reinforce societal biases\nalready existing in specific contexts. In this paper, we focus on understanding\nif this is the case when one generates images related to various software\nengineering tasks. In fact, the Software Engineering (SE) community is not\nimmune from gender and ethnicity disparities, which could be amplified by the\nuse of these models. Hence, if used without consciousness, artificially\ngenerated images could reinforce these biases in the SE domain. Specifically,\nwe perform an extensive empirical evaluation of the gender and ethnicity bias\nexposed by three versions of the Stable Diffusion (SD) model (a very popular\nopen-source text-to-image model) - SD 2, SD XL, and SD 3 - towards SE tasks. We\nobtain 6,720 images by feeding each model with two sets of prompts describing\ndifferent software-related tasks: one set includes the Software Engineer\nkeyword, and one set does not include any specification of the person\nperforming the task. Next, we evaluate the gender and ethnicity disparities in\nthe generated images. Results show how all models are significantly biased\ntowards male figures when representing software engineers. On the contrary,\nwhile SD 2 and SD XL are strongly biased towards White figures, SD 3 is\nslightly more biased towards Asian figures. Nevertheless, all models\nsignificantly under-represent Black and Arab figures, regardless of the prompt\nstyle used. The results of our analysis highlight severe concerns about\nadopting those models to generate content for SE tasks and open the field for\nfuture research on bias mitigation in this context.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09014v1",
    "published_date": "2025-01-15 18:57:17 UTC",
    "updated_date": "2025-01-15 18:57:17 UTC"
  },
  {
    "arxiv_id": "2501.09012v2",
    "title": "Multimodal LLMs Can Reason about Aesthetics in Zero-Shot",
    "authors": [
      "Ruixiang Jiang",
      "Changwen Chen"
    ],
    "abstract": "The rapid progress of generative art has democratized the creation of\nvisually pleasing imagery. However, achieving genuine artistic impact - the\nkind that resonates with viewers on a deeper, more meaningful level - requires\na sophisticated aesthetic sensibility. This sensibility involves a\nmulti-faceted reasoning process extending beyond mere visual appeal, which is\noften overlooked by current computational models. This paper pioneers an\napproach to capture this complex process by investigating how the reasoning\ncapabilities of Multimodal LLMs (MLLMs) can be effectively elicited for\naesthetic judgment. Our analysis reveals a critical challenge: MLLMs exhibit a\ntendency towards hallucinations during aesthetic reasoning, characterized by\nsubjective opinions and unsubstantiated artistic interpretations. We further\ndemonstrate that these limitations can be overcome by employing an\nevidence-based, objective reasoning process, as substantiated by our proposed\nbaseline, ArtCoT. MLLMs prompted by this principle produce multi-faceted and\nin-depth aesthetic reasoning that aligns significantly better with human\njudgment. These findings have direct applications in areas such as AI art\ntutoring and as reward models for generative art. Ultimately, our work paves\nthe way for AI systems that can truly understand, appreciate, and generate\nartworks that align with the sensible human aesthetic standard.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "WIP, Homepage https://github.com/songrise/MLLM4Art",
    "pdf_url": "http://arxiv.org/pdf/2501.09012v2",
    "published_date": "2025-01-15 18:56:22 UTC",
    "updated_date": "2025-04-17 17:14:09 UTC"
  },
  {
    "arxiv_id": "2501.09007v1",
    "title": "AI-RAN: Transforming RAN with AI-driven Computing Infrastructure",
    "authors": [
      "Lopamudra Kundu",
      "Xingqin Lin",
      "Rajesh Gadiyar",
      "Jean-Francois Lacasse",
      "Shuvo Chowdhury"
    ],
    "abstract": "The radio access network (RAN) landscape is undergoing a transformative shift\nfrom traditional, communication-centric infrastructures towards converged\ncompute-communication platforms. This article introduces AI-RAN which\nintegrates both RAN and artificial intelligence (AI) workloads on the same\ninfrastructure. By doing so, AI-RAN not only meets the performance demands of\nfuture networks but also improves asset utilization. We begin by examining how\nRANs have evolved beyond mobile broadband towards AI-RAN and articulating\nmanifestations of AI-RAN into three forms: AI-for-RAN, AI-on-RAN, and\nAI-and-RAN. Next, we identify the key requirements and enablers for the\nconvergence of communication and computing in AI-RAN. We then provide a\nreference architecture for advancing AI-RAN from concept to practice. To\nillustrate the practical potential of AI-RAN, we present a proof-of-concept\nthat concurrently processes RAN and AI workloads utilizing NVIDIA Grace-Hopper\nGH200 servers. Finally, we conclude the article by outlining future work\ndirections to guide further developments of AI-RAN.",
    "categories": [
      "cs.AI",
      "cs.NI",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.09007v1",
    "published_date": "2025-01-15 18:47:05 UTC",
    "updated_date": "2025-01-15 18:47:05 UTC"
  },
  {
    "arxiv_id": "2501.09056v1",
    "title": "Decompose-ToM: Enhancing Theory of Mind Reasoning in Large Language Models through Simulation and Task Decomposition",
    "authors": [
      "Sneheel Sarangi",
      "Maha Elgarf",
      "Hanan Salam"
    ],
    "abstract": "Theory of Mind (ToM) is the ability to understand and reflect on the mental\nstates of others. Although this capability is crucial for human interaction,\ntesting on Large Language Models (LLMs) reveals that they possess only a\nrudimentary understanding of it. Although the most capable closed-source LLMs\nhave come close to human performance on some ToM tasks, they still perform\npoorly on complex variations of the task that involve more structured\nreasoning. In this work, we utilize the concept of \"pretend-play\", or\n``Simulation Theory'' from cognitive psychology to propose ``Decompose-ToM'':\nan LLM-based inference algorithm that improves model performance on complex ToM\ntasks. We recursively simulate user perspectives and decompose the ToM task\ninto a simpler set of functions: subject identification, question-reframing,\nworld model updation, and knowledge availability. We test the algorithm on\nhigher-order ToM tasks and a task testing for ToM capabilities in a\nconversational setting, demonstrating that our approach shows significant\nimprovement across models compared to baseline methods while requiring minimal\nprompt tuning across tasks and no additional model training.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.09056v1",
    "published_date": "2025-01-15 18:44:01 UTC",
    "updated_date": "2025-01-15 18:44:01 UTC"
  },
  {
    "arxiv_id": "2501.10467v1",
    "title": "Securing the AI Frontier: Urgent Ethical and Regulatory Imperatives for AI-Driven Cybersecurity",
    "authors": [
      "Vikram Kulothungan"
    ],
    "abstract": "This paper critically examines the evolving ethical and regulatory challenges\nposed by the integration of artificial intelligence (AI) in cybersecurity. We\ntrace the historical development of AI regulation, highlighting major\nmilestones from theoretical discussions in the 1940s to the implementation of\nrecent global frameworks such as the European Union AI Act. The current\nregulatory landscape is analyzed, emphasizing risk-based approaches,\nsector-specific regulations, and the tension between fostering innovation and\nmitigating risks. Ethical concerns such as bias, transparency, accountability,\nprivacy, and human oversight are explored in depth, along with their\nimplications for AI-driven cybersecurity systems. Furthermore, we propose\nstrategies for promoting AI literacy and public engagement, essential for\nshaping a future regulatory framework. Our findings underscore the need for a\nunified, globally harmonized regulatory approach that addresses the unique\nrisks of AI in cybersecurity. We conclude by identifying future research\nopportunities and recommending pathways for collaboration between policymakers,\nindustry leaders, and researchers to ensure the responsible deployment of AI\ntechnologies in cybersecurity.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "This is a preprint of a paper that has been accepted at BigCyber at\n  2024 IEEE International Conference on Big Data (IEEE BigData 2024)",
    "pdf_url": "http://arxiv.org/pdf/2501.10467v1",
    "published_date": "2025-01-15 18:17:37 UTC",
    "updated_date": "2025-01-15 18:17:37 UTC"
  },
  {
    "arxiv_id": "2503.15497v1",
    "title": "The Impact of Big Five Personality Traits on AI Agent Decision-Making in Public Spaces: A Social Simulation Study",
    "authors": [
      "Mingjun Ren",
      "Wentao Xu"
    ],
    "abstract": "This study investigates how the Big Five personality traits influence\ndecision-making processes in AI agents within public spaces. Using AgentVerse\nframework and GPT-3.5-turbo, we simulated interactions among 10 AI agents, each\nembodying different dimensions of the Big Five personality traits, in a\nclassroom environment responding to misinformation. The experiment assessed\nboth public expressions ([Speak]) and private thoughts ([Think]) of agents,\nrevealing significant correlations between personality traits and\ndecision-making patterns. Results demonstrate that Openness to Experience had\nthe strongest impact on information acceptance, with curious agents showing\nhigh acceptance rates and cautious agents displaying strong skepticism.\nExtraversion and Conscientiousness also showed notable influence on\ndecision-making, while Neuroticism and Agreeableness exhibited more balanced\nresponses. Additionally, we observed significant discrepancies between public\nexpressions and private thoughts, particularly in agents with friendly and\nextroverted personalities, suggesting that social context influences\ndecision-making behavior. Our findings contribute to understanding how\npersonality traits shape AI agent behavior in social settings and have\nimplications for developing more nuanced and context-aware AI systems.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.15497v1",
    "published_date": "2025-01-15 18:06:18 UTC",
    "updated_date": "2025-01-15 18:06:18 UTC"
  },
  {
    "arxiv_id": "2501.08985v1",
    "title": "Personality Modeling for Persuasion of Misinformation using AI Agent",
    "authors": [
      "Qianmin Lou",
      "Wentao Xu"
    ],
    "abstract": "The proliferation of misinformation on social media platforms has highlighted\nthe need to understand how individual personality traits influence\nsusceptibility to and propagation of misinformation. This study employs an\ninnovative agent-based modeling approach to investigate the relationship\nbetween personality traits and misinformation dynamics. Using six AI agents\nembodying different dimensions of the Big Five personality traits\n(Extraversion, Agreeableness, and Neuroticism), we simulated interactions\nacross six diverse misinformation topics. The experiment, implemented through\nthe AgentScope framework using the GLM-4-Flash model, generated 90 unique\ninteractions, revealing complex patterns in how personality combinations affect\npersuasion and resistance to misinformation. Our findings demonstrate that\nanalytical and critical personality traits enhance effectiveness in\nevidence-based discussions, while non-aggressive persuasion strategies show\nunexpected success in misinformation correction. Notably, agents with critical\ntraits achieved a 59.4% success rate in HIV-related misinformation discussions,\nwhile those employing non-aggressive approaches maintained consistent\npersuasion rates above 40% across different personality combinations. The study\nalso revealed a non-transitive pattern in persuasion effectiveness, challenging\nconventional assumptions about personality-based influence. These results\nprovide crucial insights for developing personality-aware interventions in\ndigital environments and suggest that effective misinformation countermeasures\nshould prioritize emotional connection and trust-building over confrontational\napproaches. The findings contribute to both theoretical understanding of\npersonality-misinformation dynamics and practical strategies for combating\nmisinformation in social media contexts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08985v1",
    "published_date": "2025-01-15 18:04:21 UTC",
    "updated_date": "2025-01-15 18:04:21 UTC"
  },
  {
    "arxiv_id": "2501.08977v2",
    "title": "Development and Validation of the Provider Documentation Summarization Quality Instrument for Large Language Models",
    "authors": [
      "Emma Croxford",
      "Yanjun Gao",
      "Nicholas Pellegrino",
      "Karen K. Wong",
      "Graham Wills",
      "Elliot First",
      "Miranda Schnier",
      "Kyle Burton",
      "Cris G. Ebby",
      "Jillian Gorskic",
      "Matthew Kalscheur",
      "Samy Khalil",
      "Marie Pisani",
      "Tyler Rubeor",
      "Peter Stetson",
      "Frank Liao",
      "Cherodeep Goswami",
      "Brian Patterson",
      "Majid Afshar"
    ],
    "abstract": "As Large Language Models (LLMs) are integrated into electronic health record\n(EHR) workflows, validated instruments are essential to evaluate their\nperformance before implementation. Existing instruments for provider\ndocumentation quality are often unsuitable for the complexities of\nLLM-generated text and lack validation on real-world data. The Provider\nDocumentation Summarization Quality Instrument (PDSQI-9) was developed to\nevaluate LLM-generated clinical summaries. Multi-document summaries were\ngenerated from real-world EHR data across multiple specialties using several\nLLMs (GPT-4o, Mixtral 8x7b, and Llama 3-8b). Validation included Pearson\ncorrelation for substantive validity, factor analysis and Cronbach's alpha for\nstructural validity, inter-rater reliability (ICC and Krippendorff's alpha) for\ngeneralizability, a semi-Delphi process for content validity, and comparisons\nof high-versus low-quality summaries for discriminant validity. Seven physician\nraters evaluated 779 summaries and answered 8,329 questions, achieving over 80%\npower for inter-rater reliability. The PDSQI-9 demonstrated strong internal\nconsistency (Cronbach's alpha = 0.879; 95% CI: 0.867-0.891) and high\ninter-rater reliability (ICC = 0.867; 95% CI: 0.867-0.868), supporting\nstructural validity and generalizability. Factor analysis identified a 4-factor\nmodel explaining 58% of the variance, representing organization, clarity,\naccuracy, and utility. Substantive validity was supported by correlations\nbetween note length and scores for Succinct (rho = -0.200, p = 0.029) and\nOrganized ($\\rho = -0.190$, $p = 0.037$). Discriminant validity distinguished\nhigh- from low-quality summaries ($p < 0.001$). The PDSQI-9 demonstrates robust\nconstruct validity, supporting its use in clinical practice to evaluate\nLLM-generated summaries and facilitate safer integration of LLMs into\nhealthcare workflows.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08977v2",
    "published_date": "2025-01-15 17:47:57 UTC",
    "updated_date": "2025-01-17 19:10:00 UTC"
  },
  {
    "arxiv_id": "2501.08970v1",
    "title": "Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography",
    "authors": [
      "Ilia Shumailov",
      "Daniel Ramage",
      "Sarah Meiklejohn",
      "Peter Kairouz",
      "Florian Hartmann",
      "Borja Balle",
      "Eugene Bagdasarian"
    ],
    "abstract": "We often interact with untrusted parties. Prioritization of privacy can limit\nthe effectiveness of these interactions, as achieving certain goals\nnecessitates sharing private data. Traditionally, addressing this challenge has\ninvolved either seeking trusted intermediaries or constructing cryptographic\nprotocols that restrict how much data is revealed, such as multi-party\ncomputations or zero-knowledge proofs. While significant advances have been\nmade in scaling cryptographic approaches, they remain limited in terms of the\nsize and complexity of applications they can be used for. In this paper, we\nargue that capable machine learning models can fulfill the role of a trusted\nthird party, thus enabling secure computations for applications that were\npreviously infeasible. In particular, we describe Trusted Capable Model\nEnvironments (TCMEs) as an alternative approach for scaling secure computation,\nwhere capable machine learning model(s) interact under input/output\nconstraints, with explicit information flow control and explicit statelessness.\nThis approach aims to achieve a balance between privacy and computational\nefficiency, enabling private inference where classical cryptographic solutions\nare currently infeasible. We describe a number of use cases that are enabled by\nTCME, and show that even some simple classic cryptographic problems can already\nbe solved with TCME. Finally, we outline current limitations and discuss the\npath forward in implementing them.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08970v1",
    "published_date": "2025-01-15 17:28:53 UTC",
    "updated_date": "2025-01-15 17:28:53 UTC"
  },
  {
    "arxiv_id": "2501.08962v2",
    "title": "An analysis of data variation and bias in image-based dermatological datasets for machine learning classification",
    "authors": [
      "Francisco Filho",
      "Emanoel Santos",
      "Rodrigo Mota",
      "Kelvin Cunha",
      "Fabio Papais",
      "Amanda Arruda",
      "Mateus Baltazar",
      "Camila Vieira",
      "José Gabriel Tavares",
      "Rafael Barros",
      "Othon Souza",
      "Thales Bezerra",
      "Natalia Lopes",
      "Érico Moutinho",
      "Jéssica Guido",
      "Shirley Cruz",
      "Paulo Borba",
      "Tsang Ing Ren"
    ],
    "abstract": "AI algorithms have become valuable in aiding professionals in healthcare. The\nincreasing confidence obtained by these models is helpful in critical decision\ndemands. In clinical dermatology, classification models can detect malignant\nlesions on patients' skin using only RGB images as input. However, most\nlearning-based methods employ data acquired from dermoscopic datasets on\ntraining, which are large and validated by a gold standard. Clinical models aim\nto deal with classification on users' smartphone cameras that do not contain\nthe corresponding resolution provided by dermoscopy. Also, clinical\napplications bring new challenges. It can contain captures from uncontrolled\nenvironments, skin tone variations, viewpoint changes, noises in data and\nlabels, and unbalanced classes. A possible alternative would be to use transfer\nlearning to deal with the clinical images. However, as the number of samples is\nlow, it can cause degradations on the model's performance; the source\ndistribution used in training differs from the test set. This work aims to\nevaluate the gap between dermoscopic and clinical samples and understand how\nthe dataset variations impact training. It assesses the main differences\nbetween distributions that disturb the model's prediction. Finally, from\nexperiments on different architectures, we argue how to combine the data from\ndivergent distributions, decreasing the impact on the model's final accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.5.4; J.3"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2501.08962v2",
    "published_date": "2025-01-15 17:18:46 UTC",
    "updated_date": "2025-02-11 13:55:01 UTC"
  },
  {
    "arxiv_id": "2501.08958v2",
    "title": "Kolmogorov-Arnold Networks for Time Series Granger Causality Inference",
    "authors": [
      "Meiliang Liu",
      "Yunfang Xu",
      "Zijin Li",
      "Zhengye Si",
      "Xiaoxiao Yang",
      "Xinyue Yang",
      "Zhiwen Zhao"
    ],
    "abstract": "We propose the Granger causality inference Kolmogorov-Arnold Networks\n(KANGCI), a novel architecture that extends the recently proposed\nKolmogorov-Arnold Networks (KAN) to the domain of causal inference. By\nextracting base weights from KAN layers and incorporating the sparsity-inducing\npenalty and ridge regularization, KANGCI effectively infers the Granger\ncausality from time series. Additionally, we propose an algorithm based on\ntime-reversed Granger causality that automatically selects causal relationships\nwith better inference performance from the original or time-reversed time\nseries or integrates the results to mitigate spurious connectivities.\nComprehensive experiments conducted on Lorenz-96, Gene regulatory networks,\nfMRI BOLD signals, VAR, and real-world EEG datasets demonstrate that the\nproposed model achieves competitive performance to state-of-the-art methods in\ninferring Granger causality from nonlinear, high-dimensional, and\nlimited-sample time series.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08958v2",
    "published_date": "2025-01-15 17:09:07 UTC",
    "updated_date": "2025-02-05 15:26:49 UTC"
  },
  {
    "arxiv_id": "2501.08951v1",
    "title": "Analyzing the Ethical Logic of Six Large Language Models",
    "authors": [
      "W. Russell Neuman",
      "Chad Coleman",
      "Manan Shah"
    ],
    "abstract": "This study examines the ethical reasoning of six prominent generative large\nlanguage models: OpenAI GPT-4o, Meta LLaMA 3.1, Perplexity, Anthropic Claude\n3.5 Sonnet, Google Gemini, and Mistral 7B. The research explores how these\nmodels articulate and apply ethical logic, particularly in response to moral\ndilemmas such as the Trolley Problem, and Heinz Dilemma. Departing from\ntraditional alignment studies, the study adopts an explainability-transparency\nframework, prompting models to explain their ethical reasoning. This approach\nis analyzed through three established ethical typologies: the\nconsequentialist-deontological analytic, Moral Foundations Theory, and the\nKohlberg Stages of Moral Development Model. Findings reveal that LLMs exhibit\nlargely convergent ethical logic, marked by a rationalist, consequentialist\nemphasis, with decisions often prioritizing harm minimization and fairness.\nDespite similarities in pre-training and model architecture, a mixture of\nnuanced and significant differences in ethical reasoning emerge across models,\nreflecting variations in fine-tuning and post-training processes. The models\nconsistently display erudition, caution, and self-awareness, presenting ethical\nreasoning akin to a graduate-level discourse in moral philosophy. In striking\nuniformity these systems all describe their ethical reasoning as more\nsophisticated than what is characteristic of typical human moral logic.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08951v1",
    "published_date": "2025-01-15 16:56:26 UTC",
    "updated_date": "2025-01-15 16:56:26 UTC"
  },
  {
    "arxiv_id": "2501.14809v1",
    "title": "Towards Foundation Models: Evaluation of Geoscience Artificial Intelligence with Uncertainty",
    "authors": [
      "Samuel Myren",
      "Nidhi Parikh",
      "Rosalyn Rael",
      "Garrison Flynn",
      "Dave Higdon",
      "Emily Casleton"
    ],
    "abstract": "Artificial intelligence (AI) has transformed the geoscience community with\ndeep learning models (DLMs) that are trained to complete specific tasks within\nworkflows. This success has led to the development of geoscience foundation\nmodels (FMs), which promise to accomplish multiple tasks within a workflow or\nreplace the workflow altogether. However, lack of robust evaluation frameworks,\neven for traditional DLMs, leaves the geoscience community ill prepared for the\ninevitable adoption of FMs. We address this gap by designing an evaluation\nframework that jointly incorporates three crucial aspects to current DLMs and\nfuture FMs: performance uncertainty, learning efficiency, and overlapping\ntraining-test data splits. To target the three aspects, we meticulously\nconstruct the training, validation, and test splits using clustering methods\ntailored to geoscience data and enact an expansive training design to segregate\nperformance uncertainty arising from stochastic training processes and random\ndata sampling. The framework's ability to guard against misleading declarations\nof model superiority is demonstrated through evaluation of PhaseNet, a popular\nseismic phase picking DLM, under 3 training approaches. Furthermore, we show\nhow the performance gains due to overlapping training-test data can lead to\nbiased FM evaluation. Our framework helps practitioners choose the best model\nfor their problem and set performance expectations by explicitly analyzing\nmodel performance at varying budgets of training data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.geo-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14809v1",
    "published_date": "2025-01-15 16:45:51 UTC",
    "updated_date": "2025-01-15 16:45:51 UTC"
  },
  {
    "arxiv_id": "2501.08931v1",
    "title": "Visual WetlandBirds Dataset: Bird Species Identification and Behavior Recognition in Videos",
    "authors": [
      "Javier Rodriguez-Juan",
      "David Ortiz-Perez",
      "Manuel Benavent-Lledo",
      "David Mulero-Pérez",
      "Pablo Ruiz-Ponce",
      "Adrian Orihuela-Torres",
      "Jose Garcia-Rodriguez",
      "Esther Sebastián-González"
    ],
    "abstract": "The current biodiversity loss crisis makes animal monitoring a relevant field\nof study. In light of this, data collected through monitoring can provide\nessential insights, and information for decision-making aimed at preserving\nglobal biodiversity. Despite the importance of such data, there is a notable\nscarcity of datasets featuring videos of birds, and none of the existing\ndatasets offer detailed annotations of bird behaviors in video format. In\nresponse to this gap, our study introduces the first fine-grained video dataset\nspecifically designed for bird behavior detection and species classification.\nThis dataset addresses the need for comprehensive bird video datasets and\nprovides detailed data on bird actions, facilitating the development of deep\nlearning models to recognize these, similar to the advancements made in human\naction recognition. The proposed dataset comprises 178 videos recorded in\nSpanish wetlands, capturing 13 different bird species performing 7 distinct\nbehavior classes. In addition, we also present baseline results using state of\nthe art models on two tasks: bird behavior recognition and species\nclassification.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08931v1",
    "published_date": "2025-01-15 16:34:20 UTC",
    "updated_date": "2025-01-15 16:34:20 UTC"
  },
  {
    "arxiv_id": "2501.08925v2",
    "title": "Disentangling Exploration of Large Language Models by Optimal Exploitation",
    "authors": [
      "Tim Grams",
      "Patrick Betz",
      "Christian Bartelt"
    ],
    "abstract": "Exploration is a crucial skill for self-improvement and open-ended\nproblem-solving. However, it remains unclear if large language models can\neffectively explore the state-space within an unknown environment. This work\nisolates exploration as the sole objective, tasking the agent with delivering\ninformation that enhances future returns. Within this framework, we argue that\nmeasuring agent returns is not sufficient for a fair evaluation and decompose\nmissing rewards into exploration and exploitation components based on the\noptimal achievable return. Comprehensive experiments with various models reveal\nthat most struggle to sufficiently explore the state-space and weak exploration\nis insufficient. We observe a positive correlation between parameter count and\nexploration performance, with larger models demonstrating superior\ncapabilities. Furthermore, we show that our decomposition provides insights\ninto differences in behaviors driven by prompt engineering, offering a valuable\ntool for refining performance in exploratory tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08925v2",
    "published_date": "2025-01-15 16:30:29 UTC",
    "updated_date": "2025-02-03 15:17:44 UTC"
  },
  {
    "arxiv_id": "2501.08922v2",
    "title": "Discovery of Spatter Constitutive Models in Additive Manufacturing Using Machine Learning",
    "authors": [
      "Olabode T. Ajenifujah",
      "Amir Barati Farimani"
    ],
    "abstract": "Additive manufacturing (AM) is a rapidly evolving technology that has\nattracted applications across a wide range of fields due to its ability to\nfabricate complex geometries. However, one of the key challenges in AM is\nachieving consistent print quality. This inconsistency is often attributed to\nuncontrolled melt pool dynamics, partly caused by spatter which can lead to\ndefects. Therefore, capturing and controlling the evolution of the melt pool is\ncrucial for enhancing process stability and part quality. In this study, we\ndeveloped a framework to support decision-making towards efficient AM process\noperations, capable of facilitating quality control and minimizing defects via\nmachine learning (ML) and polynomial symbolic regression models. We implemented\nexperimentally validated computational tools, specifically for laser powder bed\nfusion (LPBF) processes as a cost-effective approach to collect large datasets.\nFor a dataset consisting of 281 varying process conditions, parameters such as\nmelt pool dimensions (length, width, depth), melt pool geometry (area, volume),\nand volume indicated as spatter were extracted. Using machine learning (ML) and\npolynomial symbolic regression models, a high R2 of over 95 % was achieved in\npredicting the melt pool dimensions and geometry features on both the training\nand testing datasets, with either process conditions (power and velocity) or\nmelt pool dimensions as the model inputs. In the case of volume indicated as\nspatter the value of the R2 improved after logarithmic transforming the model\ninputs, which were either the process conditions or the melt pool dimensions.\nAmong the investigated ML models, the ExtraTree model achieved the highest R2\nvalues of 96.7 % and 87.5 %.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08922v2",
    "published_date": "2025-01-15 16:26:01 UTC",
    "updated_date": "2025-02-04 16:56:24 UTC"
  },
  {
    "arxiv_id": "2501.08907v1",
    "title": "Projection Implicit Q-Learning with Support Constraint for Offline Reinforcement Learning",
    "authors": [
      "Xinchen Han",
      "Hossam Afifi",
      "Michel Marot"
    ],
    "abstract": "Offline Reinforcement Learning (RL) faces a critical challenge of\nextrapolation errors caused by out-of-distribution (OOD) actions. Implicit\nQ-Learning (IQL) algorithm employs expectile regression to achieve in-sample\nlearning, effectively mitigating the risks associated with OOD actions.\nHowever, the fixed hyperparameter in policy evaluation and density-based policy\nimprovement method limit its overall efficiency. In this paper, we propose\nProj-IQL, a projective IQL algorithm enhanced with the support constraint. In\nthe policy evaluation phase, Proj-IQL generalizes the one-step approach to a\nmulti-step approach through vector projection, while maintaining in-sample\nlearning and expectile regression framework. In the policy improvement phase,\nProj-IQL introduces support constraint that is more aligned with the policy\nevaluation approach. Furthermore, we theoretically demonstrate that Proj-IQL\nguarantees monotonic policy improvement and enjoys a progressively more\nrigorous criterion for superior actions. Empirical results demonstrate the\nProj-IQL achieves state-of-the-art performance on D4RL benchmarks, especially\nin challenging navigation domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08907v1",
    "published_date": "2025-01-15 16:17:02 UTC",
    "updated_date": "2025-01-15 16:17:02 UTC"
  },
  {
    "arxiv_id": "2501.08905v2",
    "title": "Computing Game Symmetries and Equilibria That Respect Them",
    "authors": [
      "Emanuel Tewolde",
      "Brian Hu Zhang",
      "Caspar Oesterheld",
      "Tuomas Sandholm",
      "Vincent Conitzer"
    ],
    "abstract": "Strategic interactions can be represented more concisely, and analyzed and\nsolved more efficiently, if we are aware of the symmetries within the\nmultiagent system. Symmetries also have conceptual implications, for example\nfor equilibrium selection. We study the computational complexity of identifying\nand using symmetries. Using the classical framework of normal-form games, we\nconsider game symmetries that can be across some or all players and/or actions.\nWe find a strong connection between game symmetries and graph automorphisms,\nyielding graph automorphism and graph isomorphism completeness results for\ncharacterizing the symmetries present in a game. On the other hand, we also\nshow that the problem becomes polynomial-time solvable when we restrict the\nconsideration of actions in one of two ways.\n  Next, we investigate when exactly game symmetries can be successfully\nleveraged for Nash equilibrium computation. We show that finding a Nash\nequilibrium that respects a given set of symmetries is PPAD- and CLS-complete\nin general-sum and team games respectively -- that is, exactly as hard as\nBrouwer fixed point and gradient descent problems. Finally, we present\npolynomial-time methods for the special cases where we are aware of a vast\nnumber of symmetries, or where the game is two-player zero-sum and we do not\neven know the symmetries.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.CC",
      "cs.MA",
      "91A05, 91A06, 91A10, 91A26, 91A35, 91A68, 68Q17, 68Q25, 68T01",
      "I.2; J.4; F.2"
    ],
    "primary_category": "cs.GT",
    "comment": "Long and updated version to the published paper in the Proceedings of\n  the 39th Annual AAAI Conference on Artificial Intelligence (AAAI 2025). 24\n  pages, 2 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2501.08905v2",
    "published_date": "2025-01-15 16:15:16 UTC",
    "updated_date": "2025-02-27 23:47:39 UTC"
  },
  {
    "arxiv_id": "2501.08897v2",
    "title": "Automated Retrosynthesis Planning of Macromolecules Using Large Language Models and Knowledge Graphs",
    "authors": [
      "Qinyu Ma",
      "Yuhao Zhou",
      "Jianfeng Li"
    ],
    "abstract": "Identifying reliable synthesis pathways in materials chemistry is a complex\ntask, particularly in polymer science, due to the intricate and often\nnon-unique nomenclature of macromolecules. To address this challenge, we\npropose an agent system that integrates large language models (LLMs) and\nknowledge graphs. By leveraging LLMs' powerful capabilities for extracting and\nrecognizing chemical substance names, and storing the extracted data in a\nstructured knowledge graph, our system fully automates the retrieval of\nrelevant literatures, extraction of reaction data, database querying,\nconstruction of retrosynthetic pathway trees, further expansion through the\nretrieval of additional literature and recommendation of optimal reaction\npathways. By considering the complex interdependencies among chemical\nreactants, a novel Multi-branched Reaction Pathway Search Algorithm (MBRPS) is\nproposed to help identify all valid multi-branched reaction pathways, which\narise when a single product decomposes into multiple reaction intermediates. In\ncontrast, previous studies were limited to cases where a product decomposes\ninto at most one reaction intermediate. This work represents the first attempt\nto develop a fully automated retrosynthesis planning agent tailored specially\nfor macromolecules powered by LLMs. Applied to polyimide synthesis, our new\napproach constructs a retrosynthetic pathway tree with hundreds of pathways and\nrecommends optimized routes, including both known and novel pathways. This\ndemonstrates utilizing LLMs for literature consultation to accomplish specific\ntasks is possible and crucial for future materials research, given the vast\namount of materials-related literature.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "The source code of RetroSynthesisAgent is available at\n  https://github.com/QinyuMa316/RetroSynthesisAgent",
    "pdf_url": "http://arxiv.org/pdf/2501.08897v2",
    "published_date": "2025-01-15 16:06:10 UTC",
    "updated_date": "2025-04-15 14:40:07 UTC"
  },
  {
    "arxiv_id": "2501.08889v1",
    "title": "Karatsuba Matrix Multiplication and its Efficient Custom Hardware Implementations",
    "authors": [
      "Trevor E. Pogue",
      "Nicola Nicolici"
    ],
    "abstract": "While the Karatsuba algorithm reduces the complexity of large integer\nmultiplication, the extra additions required minimize its benefits for smaller\nintegers of more commonly-used bitwidths. In this work, we propose the\nextension of the scalar Karatsuba multiplication algorithm to matrix\nmultiplication, showing how this maintains the reduction in multiplication\ncomplexity of the original Karatsuba algorithm while reducing the complexity of\nthe extra additions. Furthermore, we propose new matrix multiplication hardware\narchitectures for efficiently exploiting this extension of the Karatsuba\nalgorithm in custom hardware. We show that the proposed algorithm and hardware\narchitectures can provide real area or execution time improvements for integer\nmatrix multiplication compared to scalar Karatsuba or conventional matrix\nmultiplication algorithms, while also supporting implementation through proven\nsystolic array and conventional multiplier architectures at the core. We\nprovide a complexity analysis of the algorithm and architectures and evaluate\nthe proposed designs both in isolation and in an end-to-end deep learning\naccelerator system compared to baseline designs and prior state-of-the-art\nworks implemented on the same type of compute platform, demonstrating their\nability to increase the performance-per-area of matrix multiplication hardware.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.AR",
    "comment": "Accepted for publication in IEEE Transactions on Computers;\n  Associated source code available on github at\n  https://github.com/trevorpogue/algebraic-nnhw",
    "pdf_url": "http://arxiv.org/pdf/2501.08889v1",
    "published_date": "2025-01-15 16:00:43 UTC",
    "updated_date": "2025-01-15 16:00:43 UTC"
  },
  {
    "arxiv_id": "2501.08878v2",
    "title": "Incrementally Learning Multiple Diverse Data Domains via Multi-Source Dynamic Expansion Model",
    "authors": [
      "Runqing Wu",
      "Fei Ye",
      "Qihe Liu",
      "Guoxi Huang",
      "Jinyu Guo",
      "Rongyao Hu"
    ],
    "abstract": "Continual Learning seeks to develop a model capable of incrementally\nassimilating new information while retaining prior knowledge. However, current\nresearch predominantly addresses a straightforward learning context, wherein\nall data samples originate from a singular data domain. This paper shifts focus\nto a more complex and realistic learning environment, characterized by data\nsamples sourced from multiple distinct domains. We tackle this intricate\nlearning challenge by introducing a novel methodology, termed the Multi-Source\nDynamic Expansion Model (MSDEM), which leverages various pre-trained models as\nbackbones and progressively establishes new experts based on them to adapt to\nemerging tasks. Additionally, we propose an innovative dynamic expandable\nattention mechanism designed to selectively harness knowledge from multiple\nbackbones, thereby accelerating the new task learning. Moreover, we introduce a\ndynamic graph weight router that strategically reuses all previously acquired\nparameters and representations for new task learning, maximizing the positive\nknowledge transfer effect, which further improves generalization performance.\nWe conduct a comprehensive series of experiments, and the empirical findings\nindicate that our proposed approach achieves state-of-the-art performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.08878v2",
    "published_date": "2025-01-15 15:49:46 UTC",
    "updated_date": "2025-04-16 01:21:23 UTC"
  },
  {
    "arxiv_id": "2501.10466v1",
    "title": "Improving the Efficiency of Self-Supervised Adversarial Training through Latent Clustering-Based Selection",
    "authors": [
      "Somrita Ghosh",
      "Yuelin Xu",
      "Xiao Zhang"
    ],
    "abstract": "Compared with standard learning, adversarially robust learning is widely\nrecognized to demand significantly more training examples. Recent works propose\nthe use of self-supervised adversarial training (SSAT) with external or\nsynthetically generated unlabeled data to enhance model robustness. However,\nSSAT requires a substantial amount of extra unlabeled data, significantly\nincreasing memory usage and model training times. To address these challenges,\nwe propose novel methods to strategically select a small subset of unlabeled\ndata essential for SSAT and robustness improvement. Our selection prioritizes\ndata points near the model's decision boundary based on latent clustering-based\ntechniques, efficiently identifying a critical subset of unlabeled data with a\nhigher concentration of boundary-adjacent points. While focusing on\nnear-boundary data, our methods are designed to maintain a balanced ratio\nbetween boundary and non-boundary data points to avoid overfitting. Our\nexperiments on image benchmarks show that integrating our selection strategies\ninto self-supervised adversarial training can largely reduce memory and\ncomputational requirements while achieving high model robustness. In\nparticular, our latent clustering-based selection method with k-means is the\nmost effective, achieving nearly identical test-time robust accuracies with 5\nto 10 times less external or generated unlabeled data when applied to image\nbenchmarks. Additionally, we validate the generalizability of our approach\nacross various application scenarios, including a real-world medical dataset\nfor COVID-19 chest X-ray classification.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Shorter version of this work accepted by NextGenAISafety Workshop at\n  ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2501.10466v1",
    "published_date": "2025-01-15 15:47:49 UTC",
    "updated_date": "2025-01-15 15:47:49 UTC"
  },
  {
    "arxiv_id": "2501.08869v2",
    "title": "Silent Abandonment in Text-Based Contact Centers: Identifying, Quantifying, and Mitigating its Operational Impacts",
    "authors": [
      "Antonio Castellanos",
      "Galit B. Yom-Tov",
      "Yair Goldberg",
      "Jaeyoung Park"
    ],
    "abstract": "In the quest to improve services, companies offer customers the option to\ninteract with agents via texting. Such contact centers face unique challenges\ncompared to traditional call centers, as measuring customer experience proxies\nlike abandonment and patience involves uncertainty. A key source of this\nuncertainty is silent abandonment, where customers leave without notifying the\nsystem, wasting agent time and leaving their status unclear. Silent abandonment\nalso obscures whether a customer was served or left. Our goals are to measure\nthe magnitude of silent abandonment and mitigate its effects. Classification\nmodels show that 3%-70% of customers across 17 companies abandon silently. In\none study, 71.3% of abandoning customers did so silently, reducing agent\nefficiency by 3.2% and system capacity by 15.3%, incurring $5,457 in annual\ncosts per agent. We develop an expectation-maximization (EM) algorithm to\nestimate customer patience under uncertainty and identify influencing\ncovariates. We find that companies should use classification models to estimate\nabandonment scope and our EM algorithm to assess patience. We suggest\nstrategies to operationally mitigate the impact of silent abandonment by\npredicting suspected silent-abandonment behavior or changing service design.\nSpecifically, we show that while allowing customers to write while waiting in\nthe queue creates a missing data challenge, it also significantly increases\npatience and reduces service time, leading to reduced abandonment and lower\nstaffing requirements.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "75% of the paper is an updated version of arXiv:2304.11754",
    "pdf_url": "http://arxiv.org/pdf/2501.08869v2",
    "published_date": "2025-01-15 15:38:56 UTC",
    "updated_date": "2025-01-16 13:30:23 UTC"
  },
  {
    "arxiv_id": "2501.08862v1",
    "title": "ARMOR: Shielding Unlearnable Examples against Data Augmentation",
    "authors": [
      "Xueluan Gong",
      "Yuji Wang",
      "Yanjiao Chen",
      "Haocheng Dong",
      "Yiming Li",
      "Mengyuan Sun",
      "Shuaike Li",
      "Qian Wang",
      "Chen Chen"
    ],
    "abstract": "Private data, when published online, may be collected by unauthorized parties\nto train deep neural networks (DNNs). To protect privacy, defensive noises can\nbe added to original samples to degrade their learnability by DNNs. Recently,\nunlearnable examples are proposed to minimize the training loss such that the\nmodel learns almost nothing. However, raw data are often pre-processed before\nbeing used for training, which may restore the private information of protected\ndata. In this paper, we reveal the data privacy violation induced by data\naugmentation, a commonly used data pre-processing technique to improve model\ngeneralization capability, which is the first of its kind as far as we are\nconcerned. We demonstrate that data augmentation can significantly raise the\naccuracy of the model trained on unlearnable examples from 21.3% to 66.1%. To\naddress this issue, we propose a defense framework, dubbed ARMOR, to protect\ndata privacy from potential breaches of data augmentation. To overcome the\ndifficulty of having no access to the model training process, we design a\nnon-local module-assisted surrogate model that better captures the effect of\ndata augmentation. In addition, we design a surrogate augmentation selection\nstrategy that maximizes distribution alignment between augmented and\nnon-augmented samples, to choose the optimal augmentation strategy for each\nclass. We also use a dynamic step size adjustment algorithm to enhance the\ndefensive noise generation process. Extensive experiments are conducted on 4\ndatasets and 5 data augmentation methods to verify the performance of ARMOR.\nComparisons with 6 state-of-the-art defense methods have demonstrated that\nARMOR can preserve the unlearnability of protected private data under data\naugmentation. ARMOR reduces the test accuracy of the model trained on augmented\nprotected samples by as much as 60% more than baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08862v1",
    "published_date": "2025-01-15 15:22:57 UTC",
    "updated_date": "2025-01-15 15:22:57 UTC"
  },
  {
    "arxiv_id": "2501.08851v1",
    "title": "Digital Phenotyping for Adolescent Mental Health: A Feasibility Study Employing Machine Learning to Predict Mental Health Risk From Active and Passive Smartphone Data",
    "authors": [
      "Balasundaram Kadirvelu",
      "Teresa Bellido Bel",
      "Aglaia Freccero",
      "Martina Di Simplicio",
      "Dasha Nicholls",
      "A Aldo Faisal"
    ],
    "abstract": "Background: Adolescents are particularly vulnerable to mental disorders, with\nover 75% of cases manifesting before the age of 25. Research indicates that\nonly 18 to 34% of young people experiencing high levels of depression or\nanxiety symptoms seek support. Digital tools leveraging smartphones offer\nscalable and early intervention opportunities. Objective: Using a novel machine\nlearning framework, this study evaluated the feasibility of integrating active\nand passive smartphone data to predict mental disorders in non-clinical\nadolescents. Specifically, we investigated the utility of the Mindcraft app in\npredicting risks for internalising and externalising disorders, eating\ndisorders, insomnia and suicidal ideation. Methods: Participants (N=103; mean\nage 16.1 years) were recruited from three London schools. Participants\ncompleted the Strengths and Difficulties Questionnaire, the Eating Disorders-15\nQuestionnaire, Sleep Condition Indicator Questionnaire and indicated the\npresence/absence of suicidal ideation. They used the Mindcraft app for 14 days,\ncontributing active data via self-reports and passive data from smartphone\nsensors. A contrastive pretraining phase was applied to enhance user-specific\nfeature stability, followed by supervised fine-tuning. The model evaluation\nemployed leave-one-subject-out cross-validation using balanced accuracy as the\nprimary metric. Results: The integration of active and passive data achieved\nsuperior performance compared to individual data sources, with mean balanced\naccuracies of 0.71 for SDQ-High risk, 0.67 for insomnia, 0.77 for suicidal\nideation and 0.70 for eating disorders. The contrastive learning framework\nstabilised daily behavioural representations, enhancing predictive robustness.\nThis study demonstrates the potential of integrating active and passive\nsmartphone data with advanced machine-learning techniques for predicting mental\nhealth risks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08851v1",
    "published_date": "2025-01-15 15:05:49 UTC",
    "updated_date": "2025-01-15 15:05:49 UTC"
  },
  {
    "arxiv_id": "2501.08850v1",
    "title": "Graph Counterfactual Explainable AI via Latent Space Traversal",
    "authors": [
      "Andreas Abildtrup Hansen",
      "Paraskevas Pegios",
      "Anna Calissano",
      "Aasa Feragen"
    ],
    "abstract": "Explaining the predictions of a deep neural network is a nontrivial task, yet\nhigh-quality explanations for predictions are often a prerequisite for\npractitioners to trust these models. Counterfactual explanations aim to explain\npredictions by finding the ''nearest'' in-distribution alternative input whose\nprediction changes in a pre-specified way. However, it remains an open question\nhow to define this nearest alternative input, whose solution depends on both\nthe domain (e.g. images, graphs, tabular data, etc.) and the specific\napplication considered. For graphs, this problem is complicated i) by their\ndiscrete nature, as opposed to the continuous nature of state-of-the-art graph\nclassifiers; and ii) by the node permutation group acting on the graphs. We\npropose a method to generate counterfactual explanations for any differentiable\nblack-box graph classifier, utilizing a case-specific permutation equivariant\ngraph variational autoencoder. We generate counterfactual explanations in a\ncontinuous fashion by traversing the latent space of the autoencoder across the\nclassification boundary of the classifier, allowing for seamless integration of\ndiscrete graph structure and continuous graph attributes. We empirically\nvalidate the approach on three graph datasets, showing that our model is\nconsistently high-performing and more robust than the baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at Northern Lights Deep Learning Conference 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.08850v1",
    "published_date": "2025-01-15 15:04:10 UTC",
    "updated_date": "2025-01-15 15:04:10 UTC"
  },
  {
    "arxiv_id": "2501.10465v1",
    "title": "The Mathematics of Artificial Intelligence",
    "authors": [
      "Gabriel Peyré"
    ],
    "abstract": "This overview article highlights the critical role of mathematics in\nartificial intelligence (AI), emphasizing that mathematics provides tools to\nbetter understand and enhance AI systems. Conversely, AI raises new problems\nand drives the development of new mathematics at the intersection of various\nfields. This article focuses on the application of analytical and probabilistic\ntools to model neural network architectures and better understand their\noptimization. Statistical questions (particularly the generalization capacity\nof these networks) are intentionally set aside, though they are of crucial\nimportance. We also shed light on the evolution of ideas that have enabled\nsignificant advances in AI through architectures tailored to specific tasks,\neach echoing distinct mathematical techniques. The goal is to encourage more\nmathematicians to take an interest in and contribute to this exciting field.",
    "categories": [
      "math.OC",
      "cs.AI"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10465v1",
    "published_date": "2025-01-15 15:00:23 UTC",
    "updated_date": "2025-01-15 15:00:23 UTC"
  },
  {
    "arxiv_id": "2501.08848v1",
    "title": "RouteNet-Gauss: Hardware-Enhanced Network Modeling with Machine Learning",
    "authors": [
      "Carlos Güemes-Palau",
      "Miquel Ferriol-Galmés",
      "Jordi Paillisse-Vilanova",
      "Albert López-Brescó",
      "Pere Barlet-Ros",
      "Albert Cabellos-Aparicio"
    ],
    "abstract": "Network simulation is pivotal in network modeling, assisting with tasks\nranging from capacity planning to performance estimation. Traditional\napproaches such as Discrete Event Simulation (DES) face limitations in terms of\ncomputational cost and accuracy. This paper introduces RouteNet-Gauss, a novel\nintegration of a testbed network with a Machine Learning (ML) model to address\nthese challenges. By using the testbed as a hardware accelerator,\nRouteNet-Gauss generates training datasets rapidly and simulates network\nscenarios with high fidelity to real-world conditions. Experimental results\nshow that RouteNet-Gauss significantly reduces prediction errors by up to 95%\nand achieves a 488x speedup in inference time compared to state-of-the-art\nDES-based methods. RouteNet-Gauss's modular architecture is dynamically\nconstructed based on the specific characteristics of the network scenario, such\nas topology and routing. This enables it to understand and generalize to\ndifferent network configurations beyond those seen during training, including\nnetworks up to 10x larger. Additionally, it supports Temporal Aggregated\nPerformance Estimation (TAPE), providing configurable temporal granularity and\nmaintaining high accuracy in flow performance metrics. This approach shows\npromise in improving both simulation efficiency and accuracy, offering a\nvaluable tool for network operators.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "13 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.08848v1",
    "published_date": "2025-01-15 15:00:11 UTC",
    "updated_date": "2025-01-15 15:00:11 UTC"
  },
  {
    "arxiv_id": "2501.08847v1",
    "title": "Automatic tuning of communication protocols for vehicular ad hoc networks using metaheuristics",
    "authors": [
      "José García-Nieto",
      "Jamal Toutouh",
      "Enrique Alba"
    ],
    "abstract": "The emerging field of vehicular ad hoc networks (VANETs) deals with a set of\ncommunicating vehicles which are able to spontaneously interconnect without any\npre-existing infrastructure. In such kind of networks, it is crucial to make an\noptimal configuration of the communication protocols previously to the final\nnetwork deployment. This way, a human designer can obtain an optimal QoS of the\nnetwork beforehand. The problem we consider in this work lies in configuring\nthe File Transfer protocol Configuration (FTC) with the aim of optimizing the\ntransmission time, the number of lost packets, and the amount of data\ntransferred in realistic VANET scenarios. We face the FTC with five\nrepresentative state-of-the-art optimization techniques and compare their\nperformance. These algorithms are: Particle Swarm Optimization (PSO),\nDifferential Evolution (DE), Genetic Algorithm (GA), Evolutionary Strategy\n(ES), and Simulated Annealing (SA). For our tests, two typical environment\ninstances of VANETs for Urban and Highway scenarios have been defined. The\nexperiments using ns- 2 (a well-known realistic VANET simulator) reveal that\nPSO outperforms all the compared algorithms for both studied VANET instances.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08847v1",
    "published_date": "2025-01-15 14:59:00 UTC",
    "updated_date": "2025-01-15 14:59:00 UTC"
  },
  {
    "arxiv_id": "2501.08841v1",
    "title": "Exploring Task-Level Optimal Prompts for Visual In-Context Learning",
    "authors": [
      "Yan Zhu",
      "Huan Ma",
      "Changqing Zhang"
    ],
    "abstract": "With the development of Vision Foundation Models (VFMs) in recent years,\nVisual In-Context Learning (VICL) has become a better choice compared to\nmodifying models in most scenarios. Different from retraining or fine-tuning\nmodel, VICL does not require modifications to the model's weights or\narchitecture, and only needs a prompt with demonstrations to teach VFM how to\nsolve tasks. Currently, significant computational cost for finding optimal\nprompts for every test sample hinders the deployment of VICL, as determining\nwhich demonstrations to use for constructing prompts is very costly. In this\npaper, however, we find a counterintuitive phenomenon that most test samples\nactually achieve optimal performance under the same prompts, and searching for\nsample-level prompts only costs more time but results in completely identical\nprompts. Therefore, we propose task-level prompting to reduce the cost of\nsearching for prompts during the inference stage and introduce two time-saving\nyet effective task-level prompt search strategies. Extensive experimental\nresults show that our proposed method can identify near-optimal prompts and\nreach the best VICL performance with a minimal cost that prior work has never\nachieved.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08841v1",
    "published_date": "2025-01-15 14:52:20 UTC",
    "updated_date": "2025-01-15 14:52:20 UTC"
  },
  {
    "arxiv_id": "2501.08838v1",
    "title": "ToMATO: Verbalizing the Mental States of Role-Playing LLMs for Benchmarking Theory of Mind",
    "authors": [
      "Kazutoshi Shinoda",
      "Nobukatsu Hojo",
      "Kyosuke Nishida",
      "Saki Mizuno",
      "Keita Suzuki",
      "Ryo Masumura",
      "Hiroaki Sugiyama",
      "Kuniko Saito"
    ],
    "abstract": "Existing Theory of Mind (ToM) benchmarks diverge from real-world scenarios in\nthree aspects: 1) they assess a limited range of mental states such as beliefs,\n2) false beliefs are not comprehensively explored, and 3) the diverse\npersonality traits of characters are overlooked. To address these challenges,\nwe introduce ToMATO, a new ToM benchmark formulated as multiple-choice QA over\nconversations. ToMATO is generated via LLM-LLM conversations featuring\ninformation asymmetry. By employing a prompting method that requires\nrole-playing LLMs to verbalize their thoughts before each utterance, we capture\nboth first- and second-order mental states across five categories: belief,\nintention, desire, emotion, and knowledge. These verbalized thoughts serve as\nanswers to questions designed to assess the mental states of characters within\nconversations. Furthermore, the information asymmetry introduced by hiding\nthoughts from others induces the generation of false beliefs about various\nmental states. Assigning distinct personality traits to LLMs further\ndiversifies both utterances and thoughts. ToMATO consists of 5.4k questions,\n753 conversations, and 15 personality trait patterns. Our analysis shows that\nthis dataset construction approach frequently generates false beliefs due to\nthe information asymmetry between role-playing LLMs, and effectively reflects\ndiverse personalities. We evaluate nine LLMs on ToMATO and find that even\nGPT-4o mini lags behind human performance, especially in understanding false\nbeliefs, and lacks robustness to various personality traits.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.08838v1",
    "published_date": "2025-01-15 14:47:02 UTC",
    "updated_date": "2025-01-15 14:47:02 UTC"
  },
  {
    "arxiv_id": "2501.08828v2",
    "title": "MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents",
    "authors": [
      "Kuicai Dong",
      "Yujing Chang",
      "Xin Deik Goh",
      "Dexun Li",
      "Ruiming Tang",
      "Yong Liu"
    ],
    "abstract": "Multimodal document retrieval aims to identify and retrieve various forms of\nmultimodal content, such as figures, tables, charts, and layout information\nfrom extensive documents. Despite its increasing popularity, there is a notable\nlack of a comprehensive and robust benchmark to effectively evaluate the\nperformance of systems in such tasks. To address this gap, this work introduces\na new benchmark, named MMDocIR, that encompasses two distinct tasks: page-level\nand layout-level retrieval. The former evaluates the performance of identifying\nthe most relevant pages within a long document, while the later assesses the\nability of detecting specific layouts, providing a more fine-grained measure\nthan whole-page analysis. A layout refers to a variety of elements, including\ntextual paragraphs, equations, figures, tables, or charts. The MMDocIR\nbenchmark comprises a rich dataset featuring 1,685 questions annotated by\nexperts and 173,843 questions with bootstrapped labels, making it a valuable\nresource in multimodal document retrieval for both training and evaluation.\nThrough rigorous experiments, we demonstrate that (i) visual retrievers\nsignificantly outperform their text counterparts, (ii) MMDocIR training set\neffectively enhances the performance of multimodal document retrieval and (iii)\ntext retrievers leveraging VLM-text significantly outperforms retrievers\nrelying on OCR-text. Our dataset is available at\nhttps://mmdocrag.github.io/MMDocIR/.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.IR",
    "comment": "https://huggingface.co/MMDocIR",
    "pdf_url": "http://arxiv.org/pdf/2501.08828v2",
    "published_date": "2025-01-15 14:30:13 UTC",
    "updated_date": "2025-05-20 14:49:55 UTC"
  },
  {
    "arxiv_id": "2501.08816v2",
    "title": "IDEA: Image Description Enhanced CLIP-Adapter",
    "authors": [
      "Zhipeng Ye",
      "Feng Jiang",
      "Qiufeng Wang",
      "Kaizhu Huang",
      "Jiaqi Huang"
    ],
    "abstract": "CLIP (Contrastive Language-Image Pre-training) has attained great success in\npattern recognition and computer vision. Transferring CLIP to downstream tasks\n(e.g. zero- or few-shot classification) is a hot topic in multimodal learning.\nHowever, current studies primarily focus on either prompt learning for text or\nadapter tuning for vision, without fully exploiting the complementary\ninformation and correlations among image-text pairs. In this paper, we propose\nan Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to\nfew-shot image classification tasks. This method captures fine-grained features\nby leveraging both visual features and textual descriptions of images. IDEA is\na training-free method for CLIP, and it can be comparable to or even exceeds\nstate-of-the-art models on multiple tasks. Furthermore, we introduce\nTrainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable\ncomponents (i.e., a projector and a learnable latent space), further enhancing\nthe model's performance and achieving SOTA results on 11 datasets. As one\nimportant contribution, we employ the Llama model and design a comprehensive\npipeline to generate textual descriptions for images of 11 datasets, resulting\nin a total of 1,637,795 image-text pairs, named \"IMD-11\". Our code and data are\nreleased at https://github.com/FourierAI/IDEA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08816v2",
    "published_date": "2025-01-15 14:12:59 UTC",
    "updated_date": "2025-01-19 02:34:44 UTC"
  },
  {
    "arxiv_id": "2501.08814v2",
    "title": "SAIF: A Comprehensive Framework for Evaluating the Risks of Generative AI in the Public Sector",
    "authors": [
      "Kyeongryul Lee",
      "Heehyeon Kim",
      "Joyce Jiyoung Whang"
    ],
    "abstract": "The rapid adoption of generative AI in the public sector, encompassing\ndiverse applications ranging from automated public assistance to welfare\nservices and immigration processes, highlights its transformative potential\nwhile underscoring the pressing need for thorough risk assessments. Despite its\ngrowing presence, evaluations of risks associated with AI-driven systems in the\npublic sector remain insufficiently explored. Building upon an established\ntaxonomy of AI risks derived from diverse government policies and corporate\nguidelines, we investigate the critical risks posed by generative AI in the\npublic sector while extending the scope to account for its multimodal\ncapabilities. In addition, we propose a Systematic dAta generatIon Framework\nfor evaluating the risks of generative AI (SAIF). SAIF involves four key\nstages: breaking down risks, designing scenarios, applying jailbreak methods,\nand exploring prompt types. It ensures the systematic and consistent generation\nof prompt data, facilitating a comprehensive evaluation while providing a solid\nfoundation for mitigating the risks. Furthermore, SAIF is designed to\naccommodate emerging jailbreak methods and evolving prompt types, thereby\nenabling effective responses to unforeseen risk scenarios. We believe that this\nstudy can play a crucial role in fostering the safe and responsible integration\nof generative AI into the public sector.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 2 figures, 1 tables. AI for Public Missions (AIPM) Workshop\n  at the 39th AAAI Conference on Artificial Intelligence (AAAI 2025)",
    "pdf_url": "http://arxiv.org/pdf/2501.08814v2",
    "published_date": "2025-01-15 14:12:38 UTC",
    "updated_date": "2025-03-28 04:46:04 UTC"
  },
  {
    "arxiv_id": "2501.08809v1",
    "title": "XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework",
    "authors": [
      "Sida Tian",
      "Can Zhang",
      "Wei Yuan",
      "Wei Tan",
      "Wenjie Zhu"
    ],
    "abstract": "In recent years, remarkable advancements in artificial intelligence-generated\ncontent (AIGC) have been achieved in the fields of image synthesis and text\ngeneration, generating content comparable to that produced by humans. However,\nthe quality of AI-generated music has not yet reached this standard, primarily\ndue to the challenge of effectively controlling musical emotions and ensuring\nhigh-quality outputs. This paper presents a generalized symbolic music\ngeneration framework, XMusic, which supports flexible prompts (i.e., images,\nvideos, texts, tags, and humming) to generate emotionally controllable and\nhigh-quality symbolic music. XMusic consists of two core components, XProjector\nand XComposer. XProjector parses the prompts of various modalities into\nsymbolic music elements (i.e., emotions, genres, rhythms and notes) within the\nprojection space to generate matching music. XComposer contains a Generator and\na Selector. The Generator generates emotionally controllable and melodious\nmusic based on our innovative symbolic music representation, whereas the\nSelector identifies high-quality symbolic music by constructing a multi-task\nlearning scheme involving quality assessment, emotion recognition, and genre\nrecognition tasks. In addition, we build XMIDI, a large-scale symbolic music\ndataset that contains 108,023 MIDI files annotated with precise emotion and\ngenre labels. Objective and subjective evaluations show that XMusic\nsignificantly outperforms the current state-of-the-art methods with impressive\nmusic quality. Our XMusic has been awarded as one of the nine Highlights of\nCollectibles at WAIC 2023. The project homepage of XMusic is\nhttps://xmusic-project.github.io.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "accepted by TMM",
    "pdf_url": "http://arxiv.org/pdf/2501.08809v1",
    "published_date": "2025-01-15 14:08:44 UTC",
    "updated_date": "2025-01-15 14:08:44 UTC"
  },
  {
    "arxiv_id": "2501.10464v3",
    "title": "Adapting Beyond the Depth Limit: Counter Strategies in Large Imperfect Information Games",
    "authors": [
      "David Milec",
      "Vojtěch Kovařík",
      "Viliam Lisý"
    ],
    "abstract": "We study the problem of adapting to a known sub-rational opponent during\nonline play while remaining robust to rational opponents. We focus on large\nimperfect-information (zero-sum) games, which makes it impossible to inspect\nthe whole game tree at once and necessitates the use of depth-limited search.\nHowever, all existing methods assume rational play beyond the depth-limit,\nwhich only allows them to adapt a very limited portion of the opponent's\nbehaviour. We propose an algorithm Adapting Beyond Depth-limit (ABD) that uses\na strategy-portfolio approach - which we refer to as matrix-valued states - for\ndepth-limited search. This allows the algorithm to fully utilise all\ninformation about the opponent model, making it the first robust-adaptation\nmethod to be able to do so in large imperfect-information games. As an\nadditional benefit, the use of matrix-valued states makes the algorithm simpler\nthan traditional methods based on optimal value functions. Our experimental\nresults in poker and battleship show that ABD yields more than a twofold\nincrease in utility when facing opponents who make mistakes beyond the depth\nlimit and also delivers significant improvements in utility and safety against\nrandomly generated opponents.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10464v3",
    "published_date": "2025-01-15 14:04:27 UTC",
    "updated_date": "2025-02-09 16:38:27 UTC"
  },
  {
    "arxiv_id": "2501.08778v1",
    "title": "Networked Agents in the Dark: Team Value Learning under Partial Observability",
    "authors": [
      "Guilherme S. Varela",
      "Alberto Sardinha",
      "Francisco S. Melo"
    ],
    "abstract": "We propose a novel cooperative multi-agent reinforcement learning (MARL)\napproach for networked agents. In contrast to previous methods that rely on\ncomplete state information or joint observations, our agents must learn how to\nreach shared objectives under partial observability. During training, they\ncollect individual rewards and approximate a team value function through local\ncommunication, resulting in cooperative behavior. To describe our problem, we\nintroduce the networked dynamic partially observable Markov game framework,\nwhere agents communicate over a switching topology communication network. Our\ndistributed method, DNA-MARL, uses a consensus mechanism for local\ncommunication and gradient descent for local computation. DNA-MARL increases\nthe range of the possible applications of networked agents, being well-suited\nfor real world domains that impose privacy and where the messages may not reach\ntheir recipients. We evaluate DNA-MARL across benchmark MARL scenarios. Our\nresults highlight the superior performance of DNA-MARL over previous methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "I.2.6; I.2.11"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 7 figures, 5 tables. Accepted as supplemental material at\n  Proceedings of the 24th International Conference on Autonomous Agents and\n  Multiagent Systems (AAMAS 2025), Detroit, Michigan, USA, May 19 - 23, 2025,\n  IFAAMAS",
    "pdf_url": "http://arxiv.org/pdf/2501.08778v1",
    "published_date": "2025-01-15 13:01:32 UTC",
    "updated_date": "2025-01-15 13:01:32 UTC"
  },
  {
    "arxiv_id": "2501.08774v2",
    "title": "How Developers Interact with AI: A Taxonomy of Human-AI Collaboration in Software Engineering",
    "authors": [
      "Christoph Treude",
      "Marco A. Gerosa"
    ],
    "abstract": "Artificial intelligence (AI), including large language models and generative\nAI, is emerging as a significant force in software development, offering\ndevelopers powerful tools that span the entire development lifecycle. Although\nsoftware engineering research has extensively studied AI tools in software\ndevelopment, the specific types of interactions between developers and these\nAI-powered tools have only recently begun to receive attention. Understanding\nand improving these interactions has the potential to enhance productivity,\ntrust, and efficiency in AI-driven workflows. In this paper, we propose a\ntaxonomy of interaction types between developers and AI tools, identifying\neleven distinct interaction types, such as auto-complete code suggestions,\ncommand-driven actions, and conversational assistance. Building on this\ntaxonomy, we outline a research agenda focused on optimizing AI interactions,\nimproving developer control, and addressing trust and usability challenges in\nAI-assisted development. By establishing a structured foundation for studying\ndeveloper-AI interactions, this paper aims to stimulate research on creating\nmore effective, adaptive AI tools for software development.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted at 2nd ACM International Conference on AI Foundation Models\n  and Software Engineering (FORGE 2025)",
    "pdf_url": "http://arxiv.org/pdf/2501.08774v2",
    "published_date": "2025-01-15 12:53:49 UTC",
    "updated_date": "2025-02-05 16:11:33 UTC"
  },
  {
    "arxiv_id": "2501.09051v1",
    "title": "Polyp detection in colonoscopy images using YOLOv11",
    "authors": [
      "Alok Ranjan Sahoo",
      "Satya Sangram Sahoo",
      "Pavan Chakraborty"
    ],
    "abstract": "Colorectal cancer (CRC) is one of the most commonly diagnosed cancers all\nover the world. It starts as a polyp in the inner lining of the colon. To\nprevent CRC, early polyp detection is required. Colonosopy is used for the\ninspection of the colon. Generally, the images taken by the camera placed at\nthe tip of the endoscope are analyzed by the experts manually. Various\ntraditional machine learning models have been used with the rise of machine\nlearning. Recently, deep learning models have shown more effectiveness in polyp\ndetection due to their superiority in generalizing and learning small features.\nThese deep learning models for object detection can be segregated into two\ndifferent types: single-stage and two-stage. Generally, two stage models have\nhigher accuracy than single stage ones but the single stage models have low\ninference time. Hence, single stage models are easy to use for quick object\ndetection. YOLO is one of the singlestage models used successfully for polyp\ndetection. It has drawn the attention of researchers because of its lower\ninference time. The researchers have used Different versions of YOLO so far,\nand with each newer version, the accuracy of the model is increasing. This\npaper aims to see the effectiveness of the recently released YOLOv11 to detect\npolyp. We analyzed the performance for all five models of YOLOv11 (YOLO11n,\nYOLO11s, YOLO11m, YOLO11l, YOLO11x) with Kvasir dataset for the training and\ntesting. Two different versions of the dataset were used. The first consisted\nof the original dataset, and the other was created using augmentation\ntechniques. The performance of all the models with these two versions of the\ndataset have been analysed.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09051v1",
    "published_date": "2025-01-15 12:40:13 UTC",
    "updated_date": "2025-01-15 12:40:13 UTC"
  },
  {
    "arxiv_id": "2501.08760v1",
    "title": "Leveraging LLM Agents for Translating Network Configurations",
    "authors": [
      "Yunze Wei",
      "Xiaohui Xie",
      "Yiwei Zuo",
      "Tianshuo Hu",
      "Xinyi Chen",
      "Kaiwen Chi",
      "Yong Cui"
    ],
    "abstract": "Configuration translation is a critical and frequent task in network\noperations. When a network device is damaged or outdated, administrators need\nto replace it to maintain service continuity. The replacement devices may\noriginate from different vendors, necessitating configuration translation to\nensure seamless network operation. However, translating configurations manually\nis a labor-intensive and error-prone process. In this paper, we propose an\nintent-based framework for translating network configuration with Large\nLanguage Model (LLM) Agents. The core of our approach is an Intent-based\nRetrieval Augmented Generation (IRAG) module that systematically splits a\nconfiguration file into fragments, extracts intents, and generates accurate\ntranslations. We also design a two-stage verification method to validate the\nsyntax and semantics correctness of the translated configurations. We implement\nand evaluate the proposed method on real-world network configurations.\nExperimental results show that our method achieves 97.74% syntax correctness,\noutperforming state-of-the-art methods in translation accuracy.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08760v1",
    "published_date": "2025-01-15 12:25:56 UTC",
    "updated_date": "2025-01-15 12:25:56 UTC"
  },
  {
    "arxiv_id": "2501.09050v1",
    "title": "Generating Realistic Synthetic Head Rotation Data for Extended Reality using Deep Learning",
    "authors": [
      "Jakob Struye",
      "Filip Lemic",
      "Jeroen Famaey"
    ],
    "abstract": "Extended Reality is a revolutionary method of delivering multimedia content\nto users. A large contributor to its popularity is the sense of immersion and\ninteractivity enabled by having real-world motion reflected in the virtual\nexperience accurately and immediately. This user motion, mainly caused by head\nrotations, induces several technical challenges. For instance, which content is\ngenerated and transmitted depends heavily on where the user is looking.\nSeamless systems, taking user motion into account proactively, will therefore\nrequire accurate predictions of upcoming rotations. Training and evaluating\nsuch predictors requires vast amounts of orientational input data, which is\nexpensive to gather, as it requires human test subjects. A more feasible\napproach is to gather a modest dataset through test subjects, and then extend\nit to a more sizeable set using synthetic data generation methods. In this\nwork, we present a head rotation time series generator based on TimeGAN, an\nextension of the well-known Generative Adversarial Network, designed\nspecifically for generating time series. This approach is able to extend a\ndataset of head rotations with new samples closely matching the distribution of\nthe measured time series.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Published and presented at International Conference on Multimedia\n  2022 (ACMMM), Workshop on Interactive eXtended Reality (IXR)",
    "pdf_url": "http://arxiv.org/pdf/2501.09050v1",
    "published_date": "2025-01-15 12:14:15 UTC",
    "updated_date": "2025-01-15 12:14:15 UTC"
  },
  {
    "arxiv_id": "2501.09049v1",
    "title": "Dynamic-Aware Spatio-temporal Representation Learning for Dynamic MRI Reconstruction",
    "authors": [
      "Dayoung Baik",
      "Jaejun Yoo"
    ],
    "abstract": "Dynamic MRI reconstruction, one of inverse problems, has seen a surge by the\nuse of deep learning techniques. Especially, the practical difficulty of\nobtaining ground truth data has led to the emergence of unsupervised learning\napproaches. A recent promising method among them is implicit neural\nrepresentation (INR), which defines the data as a continuous function that maps\ncoordinate values to the corresponding signal values. This allows for filling\nin missing information only with incomplete measurements and solving the\ninverse problem effectively. Nevertheless, previous works incorporating this\nmethod have faced drawbacks such as long optimization time and the need for\nextensive hyperparameter tuning. To address these issues, we propose\nDynamic-Aware INR (DA-INR), an INR-based model for dynamic MRI reconstruction\nthat captures the spatial and temporal continuity of dynamic MRI data in the\nimage domain and explicitly incorporates the temporal redundancy of the data\ninto the model structure. As a result, DA-INR outperforms other models in\nreconstruction quality even at extreme undersampling ratios while significantly\nreducing optimization time and requiring minimal hyperparameter tuning.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09049v1",
    "published_date": "2025-01-15 12:11:33 UTC",
    "updated_date": "2025-01-15 12:11:33 UTC"
  },
  {
    "arxiv_id": "2501.10463v1",
    "title": "GLow -- A Novel, Flower-Based Simulated Gossip Learning Strategy",
    "authors": [
      "Aitor Belenguer",
      "Jose A. Pascual",
      "Javier Navaridas"
    ],
    "abstract": "Fully decentralized learning algorithms are still in an early stage of\ndevelopment. Creating modular Gossip Learning strategies is not trivial due to\nconvergence challenges and Byzantine faults intrinsic in systems of\ndecentralized nature. Our contribution provides a novel means to simulate\ncustom Gossip Learning systems by leveraging the state-of-the-art Flower\nFramework. Specifically, we introduce GLow, which will allow researchers to\ntrain and assess scalability and convergence of devices, across custom network\ntopologies, before making a physical deployment. The Flower Framework is\nselected for being a simulation featured library with a very active community\non Federated Learning research. However, Flower exclusively includes vanilla\nFederated Learning strategies and, thus, is not originally designed to perform\nsimulations without a centralized authority. GLow is presented to fill this gap\nand make simulation of Gossip Learning systems possible. Results achieved by\nGLow in the MNIST and CIFAR10 datasets, show accuracies over 0.98 and 0.75\nrespectively. More importantly, GLow performs similarly in terms of accuracy\nand convergence to its analogous Centralized and Federated approaches in all\ndesigned experiments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 7 figures, 2 tables, source code:\n  https://github.com/AitorB16/GLow",
    "pdf_url": "http://arxiv.org/pdf/2501.10463v1",
    "published_date": "2025-01-15 11:35:32 UTC",
    "updated_date": "2025-01-15 11:35:32 UTC"
  },
  {
    "arxiv_id": "2501.10462v1",
    "title": "BloomScene: Lightweight Structured 3D Gaussian Splatting for Crossmodal Scene Generation",
    "authors": [
      "Xiaolu Hou",
      "Mingcheng Li",
      "Dingkang Yang",
      "Jiawei Chen",
      "Ziyun Qian",
      "Xiao Zhao",
      "Yue Jiang",
      "Jinjie Wei",
      "Qingyao Xu",
      "Lihua Zhang"
    ],
    "abstract": "With the widespread use of virtual reality applications, 3D scene generation\nhas become a new challenging research frontier. 3D scenes have highly complex\nstructures and need to ensure that the output is dense, coherent, and contains\nall necessary structures. Many current 3D scene generation methods rely on\npre-trained text-to-image diffusion models and monocular depth estimators.\nHowever, the generated scenes occupy large amounts of storage space and often\nlack effective regularisation methods, leading to geometric distortions. To\nthis end, we propose BloomScene, a lightweight structured 3D Gaussian splatting\nfor crossmodal scene generation, which creates diverse and high-quality 3D\nscenes from text or image inputs. Specifically, a crossmodal progressive scene\ngeneration framework is proposed to generate coherent scenes utilizing\nincremental point cloud reconstruction and 3D Gaussian splatting. Additionally,\nwe propose a hierarchical depth prior-based regularization mechanism that\nutilizes multi-level constraints on depth accuracy and smoothness to enhance\nthe realism and continuity of the generated scenes. Ultimately, we propose a\nstructured context-guided compression mechanism that exploits structured hash\ngrids to model the context of unorganized anchor attributes, which\nsignificantly eliminates structural redundancy and reduces storage overhead.\nComprehensive experiments across multiple scenes demonstrate the significant\npotential and advantages of our framework compared with several baselines.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10462v1",
    "published_date": "2025-01-15 11:33:34 UTC",
    "updated_date": "2025-01-15 11:33:34 UTC"
  },
  {
    "arxiv_id": "2501.09768v3",
    "title": "Can Large Language Models Predict the Outcome of Judicial Decisions?",
    "authors": [
      "Mohamed Bayan Kmainasi",
      "Ali Ezzat Shahroor",
      "Amani Al-Ghraibah"
    ],
    "abstract": "Large Language Models (LLMs) have shown exceptional capabilities in Natural\nLanguage Processing (NLP) across diverse domains. However, their application in\nspecialized tasks such as Legal Judgment Prediction (LJP) for low-resource\nlanguages like Arabic remains underexplored. In this work, we address this gap\nby developing an Arabic LJP dataset, collected and preprocessed from Saudi\ncommercial court judgments. We benchmark state-of-the-art open-source LLMs,\nincluding LLaMA-3.2-3B and LLaMA-3.1-8B, under varying configurations such as\nzero-shot, one-shot, and fine-tuning using LoRA. Additionally, we employed a\ncomprehensive evaluation framework that integrates both quantitative metrics\n(such as BLEU, ROUGE, and BERT) and qualitative assessments (including\nCoherence, Legal Language, Clarity, etc.) using an LLM. Our results demonstrate\nthat fine-tuned smaller models achieve comparable performance to larger models\nin task-specific contexts while offering significant resource efficiency.\nFurthermore, we investigate the impact of fine-tuning the model on a diverse\nset of instructions, offering valuable insights into the development of a more\nhuman-centric and adaptable LLM. We have made the dataset, code, and models\npublicly available to provide a solid foundation for future research in Arabic\nlegal NLP.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09768v3",
    "published_date": "2025-01-15 11:32:35 UTC",
    "updated_date": "2025-02-28 18:27:21 UTC"
  },
  {
    "arxiv_id": "2501.08712v1",
    "title": "Self-supervised Transformation Learning for Equivariant Representations",
    "authors": [
      "Jaemyung Yu",
      "Jaehyun Choi",
      "Dong-Jae Lee",
      "HyeongGwon Hong",
      "Junmo Kim"
    ],
    "abstract": "Unsupervised representation learning has significantly advanced various\nmachine learning tasks. In the computer vision domain, state-of-the-art\napproaches utilize transformations like random crop and color jitter to achieve\ninvariant representations, embedding semantically the same inputs despite\ntransformations. However, this can degrade performance in tasks requiring\nprecise features, such as localization or flower classification. To address\nthis, recent research incorporates equivariant representation learning, which\ncaptures transformation-sensitive information. However, current methods depend\non transformation labels and thus struggle with interdependency and complex\ntransformations. We propose Self-supervised Transformation Learning (STL),\nreplacing transformation labels with transformation representations derived\nfrom image pairs. The proposed method ensures transformation representation is\nimage-invariant and learns corresponding equivariant transformations, enhancing\nperformance without increased batch complexity. We demonstrate the approach's\neffectiveness across diverse classification and detection tasks, outperforming\nexisting methods in 7 out of 11 benchmarks and excelling in detection. By\nintegrating complex transformations like AugMix, unusable by prior equivariant\nmethods, this approach enhances performance across tasks, underscoring its\nadaptability and resilience. Additionally, its compatibility with various base\nmodels highlights its flexibility and broad applicability. The code is\navailable at https://github.com/jaemyung-u/stl.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2501.08712v1",
    "published_date": "2025-01-15 10:54:21 UTC",
    "updated_date": "2025-01-15 10:54:21 UTC"
  },
  {
    "arxiv_id": "2501.10461v1",
    "title": "A Framework for Mining Collectively-Behaving Bots in MMORPGs",
    "authors": [
      "Hyunsoo Kim",
      "Jun Hee Kim",
      "Jaeman Son",
      "Jihoon Song",
      "Eunjo Lee"
    ],
    "abstract": "In MMORPGs (Massively Multiplayer Online Role-Playing Games), abnormal\nplayers (bots) using unauthorized automated programs to carry out pre-defined\nbehaviors systematically and repeatedly are commonly observed. Bots usually\nengage in these activities to gain in-game money, which they eventually trade\nfor real money outside the game. Such abusive activities negatively impact the\nin-game experiences of legitimate users since bots monopolize specific hunting\nareas and obtain valuable items. Thus, detecting abnormal players is a\nsignificant task for game companies. Motivated by the fact that bots tend to\nbehave collectively with similar in-game trajectories due to the auto-programs,\nwe developed BotTRep, a framework that comprises trajectory representation\nlearning followed by clustering using a completely unlabeled in-game trajectory\ndataset. Our model aims to learn representations for in-game trajectory\nsequences so that players with contextually similar trajectories have closer\nembeddings. Then, by applying DBSCAN to these representations and visualizing\nthe corresponding moving patterns, our framework ultimately assists game\nmasters in identifying and banning bots.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10461v1",
    "published_date": "2025-01-15 10:11:26 UTC",
    "updated_date": "2025-01-15 10:11:26 UTC"
  },
  {
    "arxiv_id": "2501.08669v2",
    "title": "SPEQ: Offline Stabilization Phases for Efficient Q-Learning in High Update-To-Data Ratio Reinforcement Learning",
    "authors": [
      "Carlo Romeo",
      "Girolamo Macaluso",
      "Alessandro Sestini",
      "Andrew D. Bagdanov"
    ],
    "abstract": "High update-to-data (UTD) ratio algorithms in reinforcement learning (RL)\nimprove sample efficiency but incur high computational costs, limiting\nreal-world scalability. We propose Offline Stabilization Phases for Efficient\nQ-Learning (SPEQ), an RL algorithm that combines low-UTD online training with\nperiodic offline stabilization phases. During these phases, Q-functions are\nfine-tuned with high UTD ratios on a fixed replay buffer, reducing redundant\nupdates on suboptimal data. This structured training schedule optimally\nbalances computational and sample efficiency, addressing the limitations of\nboth high and low UTD ratio approaches. We empirically demonstrate that SPEQ\nrequires from 40% to 99% fewer gradient updates and 27% to 78% less training\ntime compared to state-of-the-art high UTD ratio methods while maintaining or\nsurpassing their performance on the MuJoCo continuous control benchmark. Our\nfindings highlight the potential of periodic stabilization phases as an\neffective alternative to conventional training schedules, paving the way for\nmore scalable reinforcement learning solutions in real-world applications where\ncomputational resources are constrained.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08669v2",
    "published_date": "2025-01-15 09:04:19 UTC",
    "updated_date": "2025-03-18 12:54:07 UTC"
  },
  {
    "arxiv_id": "2501.09045v2",
    "title": "Spatio-Temporal Foundation Models: Vision, Challenges, and Opportunities",
    "authors": [
      "Adam Goodge",
      "Wee Siong Ng",
      "Bryan Hooi",
      "See Kiong Ng"
    ],
    "abstract": "Foundation models have revolutionized artificial intelligence, setting new\nbenchmarks in performance and enabling transformative capabilities across a\nwide range of vision and language tasks. However, despite the prevalence of\nspatio-temporal data in critical domains such as transportation, public health,\nand environmental monitoring, spatio-temporal foundation models (STFMs) have\nnot yet achieved comparable success. In this paper, we articulate a vision for\nthe future of STFMs, outlining their essential characteristics and the\ngeneralization capabilities necessary for broad applicability. We critically\nassess the current state of research, identifying gaps relative to these ideal\ntraits, and highlight key challenges that impede their progress. Finally, we\nexplore potential opportunities and directions to advance research towards the\naim of effective and broadly applicable STFMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09045v2",
    "published_date": "2025-01-15 08:52:28 UTC",
    "updated_date": "2025-02-07 02:39:32 UTC"
  },
  {
    "arxiv_id": "2501.08655v1",
    "title": "Application of Deep Reinforcement Learning to UAV Swarming for Ground Surveillance",
    "authors": [
      "Raúl Arranz",
      "David Carramiñana",
      "Gonzalo de Miguel",
      "Juan A. Besada",
      "Ana M. Bernardos"
    ],
    "abstract": "This paper summarizes in depth the state of the art of aerial swarms,\ncovering both classical and new reinforcement-learning-based approaches for\ntheir management. Then, it proposes a hybrid AI system, integrating deep\nreinforcement learning in a multi-agent centralized swarm architecture. The\nproposed system is tailored to perform surveillance of a specific area,\nsearching and tracking ground targets, for security and law enforcement\napplications. The swarm is governed by a central swarm controller responsible\nfor distributing different search and tracking tasks among the cooperating\nUAVs. Each UAV agent is then controlled by a collection of cooperative\nsub-agents, whose behaviors have been trained using different deep\nreinforcement learning models, tailored for the different task types proposed\nby the swarm controller. More specifically, proximal policy optimization (PPO)\nalgorithms were used to train the agents' behavior. In addition, several\nmetrics to assess the performance of the swarm in this application were\ndefined. The results obtained through simulation show that our system searches\nthe operation area effectively, acquires the targets in a reasonable time, and\nis capable of tracking them continuously and consistently.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08655v1",
    "published_date": "2025-01-15 08:46:20 UTC",
    "updated_date": "2025-01-15 08:46:20 UTC"
  },
  {
    "arxiv_id": "2501.08653v2",
    "title": "Fine-grained Spatio-temporal Event Prediction with Self-adaptive Anchor Graph",
    "authors": [
      "Wang-Tao Zhou",
      "Zhao Kang",
      "Sicong Liu",
      "Lizong Zhang",
      "Ling Tian"
    ],
    "abstract": "Event prediction tasks often handle spatio-temporal data distributed in a\nlarge spatial area. Different regions in the area exhibit different\ncharacteristics while having latent correlations. This spatial heterogeneity\nand correlations greatly affect the spatio-temporal distributions of event\noccurrences, which has not been addressed by state-of-the-art models. Learning\nspatial dependencies of events in a continuous space is challenging due to its\nfine granularity and a lack of prior knowledge. In this work, we propose a\nnovel Graph Spatio-Temporal Point Process (GSTPP) model for fine-grained event\nprediction. It adopts an encoder-decoder architecture that jointly models the\nstate dynamics of spatially localized regions using neural Ordinary\nDifferential Equations (ODEs). The state evolution is built on the foundation\nof a novel Self-Adaptive Anchor Graph (SAAG) that captures spatial\ndependencies. By adaptively localizing the anchor nodes in the space and\njointly constructing the correlation edges between them, the SAAG enhances the\nmodel's ability of learning complex spatial event patterns. The proposed GSTPP\nmodel greatly improves the accuracy of fine-grained event prediction. Extensive\nexperimental results show that our method greatly improves the prediction\naccuracy over existing spatio-temporal event prediction approaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to SIAM International Conference on Data Mining 2025\n  (SDM'25)",
    "pdf_url": "http://arxiv.org/pdf/2501.08653v2",
    "published_date": "2025-01-15 08:38:07 UTC",
    "updated_date": "2025-01-19 08:45:34 UTC"
  },
  {
    "arxiv_id": "2501.08648v2",
    "title": "MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities",
    "authors": [
      "Savya Khosla",
      "Aditi Tiwari",
      "Kushal Kafle",
      "Simon Jenni",
      "Handong Zhao",
      "John Collomosse",
      "Jing Shi"
    ],
    "abstract": "While originally designed for unidirectional generative modeling,\ndecoder-only large language models (LLMs) are increasingly being adapted for\nbidirectional modeling. However, unidirectional and bidirectional models are\ntypically trained separately with distinct objectives (generation and\nrepresentation learning). This separation overlooks the opportunity for\ndeveloping a more versatile language model and for these objectives to\ncomplement each other. In this work, we propose MAGNET, a method for adapting\ndecoder-only LLMs to generate robust representations and infill missing text\nspans. MAGNET employs three self-supervised training objectives and introduces\nan attention mechanism that combines bidirectional and causal attention,\nenabling unified training across all objectives. Our results demonstrate that\nLLMs adapted with MAGNET (1) surpass strong text encoders on token-level and\nsentence-level representation learning tasks, (2) generate contextually\nappropriate text infills by leveraging past and future contexts, (3) perform\nopen-ended text generation without excessive repetition of words or phrases,\nand (4) preserve the knowledge and reasoning capability gained by the LLM\nduring pretraining.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08648v2",
    "published_date": "2025-01-15 08:24:03 UTC",
    "updated_date": "2025-02-14 00:32:56 UTC"
  },
  {
    "arxiv_id": "2501.08641v1",
    "title": "Reassessing the Role of Chain-of-Thought in Sentiment Analysis: Insights and Limitations",
    "authors": [
      "Kaiyuan Zheng",
      "Qinghua Zhao",
      "Lei Li"
    ],
    "abstract": "The relationship between language and thought remains an unresolved\nphilosophical issue. Existing viewpoints can be broadly categorized into two\nschools: one asserting their independence, and another arguing that language\nconstrains thought. In the context of large language models, this debate raises\na crucial question: Does a language model's grasp of semantic meaning depend on\nthought processes? To explore this issue, we investigate whether reasoning\ntechniques can facilitate semantic understanding. Specifically, we\nconceptualize thought as reasoning, employ chain-of-thought prompting as a\nreasoning technique, and examine its impact on sentiment analysis tasks. The\nexperiments show that chain-of-thought has a minimal impact on sentiment\nanalysis tasks. Both the standard and chain-of-thought prompts focus on aspect\nterms rather than sentiment in the generated content. Furthermore,\ncounterfactual experiments reveal that the model's handling of sentiment tasks\nprimarily depends on information from demonstrations. The experimental results\nsupport the first viewpoint.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08641v1",
    "published_date": "2025-01-15 08:07:22 UTC",
    "updated_date": "2025-01-15 08:07:22 UTC"
  },
  {
    "arxiv_id": "2501.09044v1",
    "title": "TCMM: Token Constraint and Multi-Scale Memory Bank of Contrastive Learning for Unsupervised Person Re-identification",
    "authors": [
      "Zheng-An Zhu",
      "Hsin-Che Chien",
      "Chen-Kuo Chiang"
    ],
    "abstract": "This paper proposes the ViT Token Constraint and Multi-scale Memory bank\n(TCMM) method to address the patch noises and feature inconsistency in\nunsupervised person re-identification works. Many excellent methods use ViT\nfeatures to obtain pseudo labels and clustering prototypes, then train the\nmodel with contrastive learning. However, ViT processes images by performing\npatch embedding, which inevitably introduces noise in patches and may\ncompromise the performance of the re-identification model. On the other hand,\nprevious memory bank based contrastive methods may lead data inconsistency due\nto the limitation of batch size. Furthermore, existing pseudo label methods\noften discard outlier samples that are difficult to cluster. It sacrifices the\npotential value of outlier samples, leading to limited model diversity and\nrobustness. This paper introduces the ViT Token Constraint to mitigate the\ndamage caused by patch noises to the ViT architecture. The proposed Multi-scale\nMemory enhances the exploration of outlier samples and maintains feature\nconsistency. Experimental results demonstrate that our system achieves\nstate-of-the-art performance on common benchmarks. The project is available at\n\\href{https://github.com/andy412510/TCMM}{https://github.com/andy412510/TCMM}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09044v1",
    "published_date": "2025-01-15 07:14:02 UTC",
    "updated_date": "2025-01-15 07:14:02 UTC"
  },
  {
    "arxiv_id": "2501.08621v1",
    "title": "ViBidirectionMT-Eval: Machine Translation for Vietnamese-Chinese and Vietnamese-Lao language pair",
    "authors": [
      "Hong-Viet Tran",
      "Minh-Quy Nguyen",
      "Van-Vinh Nguyen"
    ],
    "abstract": "This paper presents an results of the VLSP 2022-2023 Machine Translation\nShared Tasks, focusing on Vietnamese-Chinese and Vietnamese-Lao machine\ntranslation. The tasks were organized as part of the 9th, 10th annual workshop\non Vietnamese Language and Speech Processing (VLSP 2022, VLSP 2023). The\nobjective of the shared task was to build machine translation systems,\nspecifically targeting Vietnamese-Chinese and Vietnamese-Lao translation\n(corresponding to 4 translation directions). The submission were evaluated on\n1,000 pairs for testing (news and general domains) using established metrics\nlike BLEU [11] and SacreBLEU [12]. Additionally, system outputs also were\nevaluated with human judgment provided by experts in Chinese and Lao languages.\nThese human assessments played a crucial role in ranking the performance of the\nmachine translation models, ensuring a more comprehensive evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08621v1",
    "published_date": "2025-01-15 06:40:26 UTC",
    "updated_date": "2025-01-15 06:40:26 UTC"
  },
  {
    "arxiv_id": "2501.08618v1",
    "title": "Disjoint Processing Mechanisms of Hierarchical and Linear Grammars in Large Language Models",
    "authors": [
      "Aruna Sankaranarayanan",
      "Dylan Hadfield-Menell",
      "Aaron Mueller"
    ],
    "abstract": "All natural languages are structured hierarchically. In humans, this\nstructural restriction is neurologically coded: when two grammars are presented\nwith identical vocabularies, brain areas responsible for language processing\nare only sensitive to hierarchical grammars. Using large language models\n(LLMs), we investigate whether such functionally distinct hierarchical\nprocessing regions can arise solely from exposure to large-scale language\ndistributions. We generate inputs using English, Italian, Japanese, or nonce\nwords, varying the underlying grammars to conform to either hierarchical or\nlinear/positional rules. Using these grammars, we first observe that language\nmodels show distinct behaviors on hierarchical versus linearly structured\ninputs. Then, we find that the components responsible for processing\nhierarchical grammars are distinct from those that process linear grammars; we\ncausally verify this in ablation experiments. Finally, we observe that\nhierarchy-selective components are also active on nonce grammars; this suggests\nthat hierarchy sensitivity is not tied to meaning, nor in-distribution inputs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08618v1",
    "published_date": "2025-01-15 06:34:34 UTC",
    "updated_date": "2025-01-15 06:34:34 UTC"
  },
  {
    "arxiv_id": "2501.08617v2",
    "title": "RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation",
    "authors": [
      "Kaiqu Liang",
      "Haimin Hu",
      "Ryan Liu",
      "Thomas L. Griffiths",
      "Jaime Fernández Fisac"
    ],
    "abstract": "While Reinforcement Learning from Human Feedback (RLHF) has shown promise in\naligning generative AI, we present empirical evidence that it can also cause\nsevere, systematic misalignment. We hypothesize that this stems from evaluator\nfeedback depending on downstream outcome predictions (foresight) that can be\ninfluenced by the AI's output, inducing Goodhart's law dynamics. Conversely,\nour theoretical analysis shows that conditioning evaluator feedback on\ndownstream observations (hindsight) inhibits this effect by decoupling the\nalignment signal from potentially compromised predictions-crucially, the result\nholds even if the observed outcomes are sampled from the AI's own world model.\nBuilding on this insight, we introduce Reinforcement Learning from Hindsight\nSimulation (RLHS), which presents plausible simulated outcomes to evaluators\nbefore eliciting feedback. We demonstrate RLHS on online (PPO) and offline\n(DPO) large language model fine-tuning, obtaining superior alignment over RLHF\nin controlled consultancy-type experiments and user studies. We evaluate\npost-hoc on the TruthfulQA benchmark and find that, even after single-task\nfine-tuning, both RLHF misalignment and RLHS alignment carry over to\nsubstantially different settings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.08617v2",
    "published_date": "2025-01-15 06:33:15 UTC",
    "updated_date": "2025-02-10 21:17:01 UTC"
  },
  {
    "arxiv_id": "2501.16344v2",
    "title": "WhiSPA: Semantically and Psychologically Aligned Whisper with Self-Supervised Contrastive and Student-Teacher Learning",
    "authors": [
      "Rajath Rao",
      "Adithya Ganesan",
      "Oscar Kjell",
      "Jonah Luby",
      "Akshay Raghavan",
      "Scott Feltman",
      "Whitney Ringwald",
      "Ryan L. Boyd",
      "Benjamin Luft",
      "Camilo Ruggero",
      "Neville Ryant",
      "Roman Kotov",
      "H. Andrew Schwartz"
    ],
    "abstract": "Current speech encoding pipelines often rely on an additional text-based LM\nto get robust representations of human communication, even though SotA\nspeech-to-text models often have a LM within. This work proposes an approach to\nimprove the LM within an audio model such that the subsequent text-LM is\nunnecessary. We introduce WhiSPA (Whisper with Semantic and Psychological\nAlignment), which leverages a novel audio training objective: contrastive loss\nwith a language model embedding as a teacher. Using over 500k speech segments\nfrom mental health audio interviews, we evaluate the utility of aligning\nWhisper's latent space with semantic representations from a text autoencoder\n(SBERT) and lexically derived embeddings of basic psychological dimensions:\nemotion and personality. Over self-supervised affective tasks and downstream\npsychological tasks, WhiSPA surpasses current speech encoders, achieving an\naverage error reduction of 73.4% and 83.8%, respectively. WhiSPA demonstrates\nthat it is not always necessary to run a subsequent text LM on speech-to-text\noutput in order to get a rich psychological representation of human\ncommunication.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "15 pages, 8 figures, ACL ARR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.16344v2",
    "published_date": "2025-01-15 06:30:17 UTC",
    "updated_date": "2025-02-16 23:25:21 UTC"
  },
  {
    "arxiv_id": "2501.08603v3",
    "title": "Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based Automatic Heuristic Design",
    "authors": [
      "Zhi Zheng",
      "Zhuoliang Xie",
      "Zhenkun Wang",
      "Bryan Hooi"
    ],
    "abstract": "Handcrafting heuristics for solving complex optimization tasks (e.g., route\nplanning and task allocation) is a common practice but requires extensive\ndomain knowledge. Recently, Large Language Model (LLM)-based automatic\nheuristic design (AHD) methods have shown promise in generating high-quality\nheuristics without manual interventions. Existing LLM-based AHD methods employ\na population to maintain a fixed number of top-performing LLM-generated\nheuristics and introduce evolutionary computation (EC) to iteratively enhance\nthe population. However, these population-based procedures cannot fully develop\nthe potential of each heuristic and are prone to converge into local optima. To\nmore comprehensively explore the space of heuristics, this paper proposes to\nuse Monte Carlo Tree Search (MCTS) for LLM-based heuristic evolution. The\nproposed MCTS-AHD method organizes all LLM-generated heuristics in a tree\nstructure and can better develop the potential of temporarily underperforming\nheuristics. In experiments, MCTS-AHD delivers significantly higher-quality\nheuristics on various complex tasks. Our code is available.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08603v3",
    "published_date": "2025-01-15 06:00:50 UTC",
    "updated_date": "2025-01-31 05:28:15 UTC"
  },
  {
    "arxiv_id": "2501.08600v2",
    "title": "AutoRestTest: A Tool for Automated REST API Testing Using LLMs and MARL",
    "authors": [
      "Tyler Stennett",
      "Myeongsoo Kim",
      "Saurabh Sinha",
      "Alessandro Orso"
    ],
    "abstract": "As REST APIs have become widespread in modern web services, comprehensive\ntesting of these APIs is increasingly crucial. Because of the vast search space\nof operations, parameters, and parameter values, along with their dependencies\nand constraints, current testing tools often achieve low code coverage,\nresulting in suboptimal fault detection. To address this limitation, we present\nAutoRestTest, a novel tool that integrates the Semantic Property Dependency\nGraph (SPDG) with Multi-Agent Reinforcement Learning (MARL) and large language\nmodels (LLMs) for effective REST API testing. AutoRestTest determines\noperation-dependent parameters using the SPDG and employs five specialized\nagents (operation, parameter, value, dependency, and header) to identify\ndependencies of operations and generate operation sequences, parameter\ncombinations, and values. Through an intuitive command-line interface, users\ncan easily configure and monitor tests with successful operation count, unique\nserver errors detected, and time elapsed. Upon completion, AutoRestTest\ngenerates a detailed report highlighting errors detected and operations\nexercised. In this paper, we introduce our tool and present preliminary\nfindings, with a demonstration video available at\nhttps://www.youtube.com/watch?v=VVus2W8rap8.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "To be published in the 47th IEEE/ACM International Conference on\n  Software Engineering - Demonstration Track (ICSE-Demo 2025)",
    "pdf_url": "http://arxiv.org/pdf/2501.08600v2",
    "published_date": "2025-01-15 05:54:33 UTC",
    "updated_date": "2025-03-04 03:19:22 UTC"
  },
  {
    "arxiv_id": "2501.08598v2",
    "title": "LlamaRestTest: Effective REST API Testing with Small Language Models",
    "authors": [
      "Myeongsoo Kim",
      "Saurabh Sinha",
      "Alessandro Orso"
    ],
    "abstract": "Modern web services rely heavily on REST APIs, typically documented using the\nOpenAPI specification. The widespread adoption of this standard has resulted in\nthe development of many black-box testing tools that generate tests based on\nOpenAPI specifications. Although Large Language Models (LLMs) have shown\npromising test-generation abilities, their application to REST API testing\nremains mostly unexplored. We present LlamaRestTest, a novel approach that\nemploys two custom LLMs-created by fine-tuning and quantizing the Llama3-8B\nmodel using mined datasets of REST API example values and inter-parameter\ndependencies-to generate realistic test inputs and uncover inter-parameter\ndependencies during the testing process by analyzing server responses. We\nevaluated LlamaRestTest on 12 real-world services (including popular services\nsuch as Spotify), comparing it against RESTGPT, a GPT-powered\nspecification-enhancement tool, as well as several state-of-the-art REST API\ntesting tools, including RESTler, MoRest, EvoMaster, and ARAT-RL. Our results\ndemonstrate that fine-tuning enables smaller models to outperform much larger\nmodels in detecting actionable parameter-dependency rules and generating valid\ninputs for REST API testing. We also evaluated different tool configurations,\nranging from the base Llama3-8B model to fine-tuned versions, and explored\nmultiple quantization techniques, including 2-bit, 4-bit, and 8-bit integer\nformats. Our study shows that small language models can perform as well as, or\nbetter than, large language models in REST API testing, balancing effectiveness\nand efficiency. Furthermore, LlamaRestTest outperforms state-of-the-art REST\nAPI testing tools in code coverage achieved and internal server errors\nidentified, even when those tools use RESTGPT-enhanced specifications.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "To be published in the ACM International Conference on the\n  Foundations of Software Engineering (FSE 2025)",
    "pdf_url": "http://arxiv.org/pdf/2501.08598v2",
    "published_date": "2025-01-15 05:51:20 UTC",
    "updated_date": "2025-04-03 19:42:32 UTC"
  },
  {
    "arxiv_id": "2501.08591v1",
    "title": "OpenMLDB: A Real-Time Relational Data Feature Computation System for Online ML",
    "authors": [
      "Xuanhe Zhou",
      "Wei Zhou",
      "Liguo Qi",
      "Hao Zhang",
      "Dihao Chen",
      "Bingsheng He",
      "Mian Lu",
      "Guoliang Li",
      "Fan Wu",
      "Yuqiang Chen"
    ],
    "abstract": "Efficient and consistent feature computation is crucial for a wide range of\nonline ML applications. Typically, feature computation is divided into two\ndistinct phases, i.e., offline stage for model training and online stage for\nmodel serving. These phases often rely on execution engines with different\ninterface languages and function implementations, causing significant\ninconsistencies. Moreover, many online ML features involve complex time-series\ncomputations (e.g., functions over varied-length table windows) that differ\nfrom standard streaming and analytical queries. Existing data processing\nsystems (e.g., Spark, Flink, DuckDB) often incur multi-second latencies for\nthese computations, making them unsuitable for real-time online ML applications\nthat demand timely feature updates.\n  This paper presents OpenMLDB, a feature computation system deployed in\n4Paradigm's SageOne platform and over 100 real scenarios. Technically, OpenMLDB\nfirst employs a unified query plan generator for consistent computation results\nacross the offline and online stages, significantly reducing feature deployment\noverhead. Second, OpenMLDB provides an online execution engine that resolves\nperformance bottlenecks caused by long window computations (via\npre-aggregation) and multi-table window unions (via data self-adjusting). It\nalso provides a high-performance offline execution engine with window parallel\noptimization and time-aware data skew resolving. Third, OpenMLDB features a\ncompact data format and stream-focused indexing to maximize memory usage and\naccelerate data access. Evaluations in testing and real workloads reveal\nsignificant performance improvements and resource savings compared to the\nbaseline systems. The open community of OpenMLDB now has over 150 contributors\nand gained 1.6k stars on GitHub.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08591v1",
    "published_date": "2025-01-15 05:20:01 UTC",
    "updated_date": "2025-01-15 05:20:01 UTC"
  },
  {
    "arxiv_id": "2501.09767v1",
    "title": "LeMo: Enabling LEss Token Involvement for MOre Context Fine-tuning",
    "authors": [
      "Tuowei Wang",
      "Xingyu Chen",
      "Kun Li",
      "Ting Cao",
      "Ju Ren",
      "Yaoxue Zhang"
    ],
    "abstract": "The escalating demand for long-context applications has intensified the\nnecessity of extending the LLM context windows. Despite recent fine-tuning\napproaches successfully expanding context lengths, their high memory\nfootprints, especially for activations, present a critical practical\nlimitation. Current parameter-efficient fine-tuning methods prioritize reducing\nparameter update overhead over addressing activation memory constraints.\nSimilarly, existing sparsity mechanisms improve computational efficiency but\noverlook activation memory optimization due to the phenomenon of Shadowy\nActivation.\n  In this paper, we propose LeMo, the first LLM fine-tuning system that\nexplores and exploits a new token-level sparsity mechanism inherent in\nlong-context scenarios, termed Contextual Token Sparsity. LeMo minimizes\nredundant token involvement by assessing the informativeness of token\nembeddings while preserving model accuracy. Specifically, LeMo introduces three\nkey techniques: (1) Token Elimination, dynamically identifying and excluding\nredundant tokens across varying inputs and layers. (2) Pattern Prediction,\nutilizing well-trained predictors to approximate token sparsity patterns with\nminimal overhead. (3) Kernel Optimization, employing permutation-free and\nsegment-based strategies to boost system performance. We implement LeMo as an\nend-to-end fine-tuning system compatible with various LLM architectures and\nother optimization techniques. Comprehensive evaluations demonstrate that LeMo\nreduces memory consumption by up to 1.93x and achieves up to 1.36x speedups,\noutperforming state-of-the-art fine-tuning systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09767v1",
    "published_date": "2025-01-15 05:17:12 UTC",
    "updated_date": "2025-01-15 05:17:12 UTC"
  },
  {
    "arxiv_id": "2501.08587v1",
    "title": "Sound Scene Synthesis at the DCASE 2024 Challenge",
    "authors": [
      "Mathieu Lagrange",
      "Junwon Lee",
      "Modan Tailleur",
      "Laurie M. Heller",
      "Keunwoo Choi",
      "Brian McFee",
      "Keisuke Imoto",
      "Yuki Okamoto"
    ],
    "abstract": "This paper presents Task 7 at the DCASE 2024 Challenge: sound scene\nsynthesis. Recent advances in sound synthesis and generative models have\nenabled the creation of realistic and diverse audio content. We introduce a\nstandardized evaluation framework for comparing different sound scene synthesis\nsystems, incorporating both objective and subjective metrics. The challenge\nattracted four submissions, which are evaluated using the Fr\\'echet Audio\nDistance (FAD) and human perceptual ratings. Our analysis reveals significant\ninsights into the current capabilities and limitations of sound scene synthesis\nsystems, while also highlighting areas for future improvement in this rapidly\nevolving field.",
    "categories": [
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08587v1",
    "published_date": "2025-01-15 05:15:54 UTC",
    "updated_date": "2025-01-15 05:15:54 UTC"
  },
  {
    "arxiv_id": "2501.09766v3",
    "title": "iTool: Boosting Tool Use of Large Language Models via Iterative Reinforced Fine-Tuning",
    "authors": [
      "Yirong Zeng",
      "Xiao Ding",
      "Yuxian Wang",
      "Weiwen Liu",
      "Wu Ning",
      "Yutai Hou",
      "Xu Huang",
      "Bing Qin",
      "Ting Liu"
    ],
    "abstract": "Augmenting large language models (LLMs) with external tools is known as a\npromising approach to enhancing their capabilities, especially for complex\ntasks. Synthesizing tool-use data through real-world simulations is an\neffective way to achieve it. Nevertheless, our investigation reveals that (1)\ntraining gains significantly decay as synthetic data increases. The model\nstruggles to benefit from more synthetic data due to potential data diversity\nissues, resulting in poor performance in complex scenarios. Moreover, we find\nthat (2) this challenge primarily manifests as minor discrepancies between the\nmodel's output and the ground truth response (termed as deficiency), such as\nerrors in parameter values that require complex reasoning from the context to\nresolve. To this end, we propose an iterative reinforced fine-tuning strategy\ndesigned to alleviate these challenges. This strategy involves: (1) enhancing\nthe diversity of synthetic data through path exploration of Monte Carlo Tree\nSearch. (2) iteratively identifying deficiency-related data, constructing\nfine-grained preference pairs to pinpoint deficiencies, and then applying\npreference optimization to optimize these deficiencies. Our experiments show\nthat models trained using our method achieve about 12\\% better performance than\nbaseline models, outperforming larger open-source and closed-source models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "under review ACL",
    "pdf_url": "http://arxiv.org/pdf/2501.09766v3",
    "published_date": "2025-01-15 04:52:34 UTC",
    "updated_date": "2025-03-27 05:05:03 UTC"
  },
  {
    "arxiv_id": "2501.08569v1",
    "title": "Evaluating SAT and SMT Solvers on Large-Scale Sudoku Puzzles",
    "authors": [
      "Liam Davis",
      "Tairan Ji"
    ],
    "abstract": "Modern SMT solvers have revolutionized the approach to constraint\nsatisfaction problems by integrating advanced theory reasoning and encoding\ntechniques. In this work, we evaluate the performance of modern SMT solvers in\nZ3, CVC5 and DPLL(T) against a standard SAT solver in DPLL. By benchmarking\nthese solvers on novel, diverse 25x25 Sudoku puzzles of various difficulty\nlevels created by our improved Sudoku generator, we examine the impact of\nadvanced theory reasoning and encoding techniques. Our findings demonstrate\nthat modern SMT solvers significantly outperform classical SAT solvers. This\nwork highlights the evolution of logical solvers and exemplifies the utility of\nSMT solvers in addressing large-scale constraint satisfaction problems.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08569v1",
    "published_date": "2025-01-15 04:31:56 UTC",
    "updated_date": "2025-01-15 04:31:56 UTC"
  },
  {
    "arxiv_id": "2501.08566v1",
    "title": "Towards Lightweight and Stable Zero-shot TTS with Self-distilled Representation Disentanglement",
    "authors": [
      "Qianniu Chen",
      "Xiaoyang Hao",
      "Bowen Li",
      "Yue Liu",
      "Li Lu"
    ],
    "abstract": "Zero-shot Text-To-Speech (TTS) synthesis shows great promise for personalized\nvoice customization through voice cloning. However, current methods for\nachieving zero-shot TTS heavily rely on large model scales and extensive\ntraining datasets to ensure satisfactory performance and generalizability\nacross various speakers. This raises concerns regarding both deployment costs\nand data security. In this paper, we present a lightweight and stable zero-shot\nTTS system. We introduce a novel TTS architecture designed to effectively model\nlinguistic content and various speaker attributes from source speech and prompt\nspeech, respectively. Furthermore, we present a two-stage self-distillation\nframework that constructs parallel data pairs for effectively disentangling\nlinguistic content and speakers from the perspective of training data.\nExtensive experiments show that our system exhibits excellent performance and\nsuperior stability on the zero-shot TTS tasks. Moreover, it shows markedly\nsuperior computational efficiency, with RTFs of 0.13 and 0.012 on the CPU and\nGPU, respectively.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages,4 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.08566v1",
    "published_date": "2025-01-15 04:17:48 UTC",
    "updated_date": "2025-01-15 04:17:48 UTC"
  },
  {
    "arxiv_id": "2501.08565v1",
    "title": "DualOpt: A Dual Divide-and-Optimize Algorithm for the Large-scale Traveling Salesman Problem",
    "authors": [
      "Shipei Zhou",
      "Yuandong Ding",
      "Chi Zhang",
      "Zhiguang Cao",
      "Yan Jin"
    ],
    "abstract": "This paper proposes a dual divide-and-optimize algorithm (DualOpt) for\nsolving the large-scale traveling salesman problem (TSP). DualOpt combines two\ncomplementary strategies to improve both solution quality and computational\nefficiency. The first strategy is a grid-based divide-and-conquer procedure\nthat partitions the TSP into smaller sub-problems, solving them in parallel and\niteratively refining the solution by merging nodes and partial routes. The\nprocess continues until only one grid remains, yielding a high-quality initial\nsolution. The second strategy involves a path-based divide-and-optimize\nprocedure that further optimizes the solution by dividing it into sub-paths,\noptimizing each using a neural solver, and merging them back to progressively\nimprove the overall solution. Extensive experiments conducted on two groups of\nTSP benchmark instances, including randomly generated instances with up to\n100,000 nodes and real-world datasets from TSPLIB, demonstrate the\neffectiveness of DualOpt. The proposed DualOpt achieves highly competitive\nresults compared to 10 state-of-the-art algorithms in the literature. In\nparticular, DualOpt achieves an improvement gap up to 1.40% for the largest\ninstance TSP100K with a remarkable 104x speed-up over the leading heuristic\nsolver LKH3. Additionally, DualOpt demonstrates strong generalization on TSPLIB\nbenchmarks, confirming its capability to tackle diverse real-world TSP\napplications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by AAAI-25, February 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.08565v1",
    "published_date": "2025-01-15 04:16:28 UTC",
    "updated_date": "2025-01-15 04:16:28 UTC"
  },
  {
    "arxiv_id": "2501.08561v2",
    "title": "ANSR-DT: An Adaptive Neuro-Symbolic Learning and Reasoning Framework for Digital Twins",
    "authors": [
      "Safayat Bin Hakim",
      "Muhammad Adil",
      "Alvaro Velasquez",
      "Houbing Herbert Song"
    ],
    "abstract": "In this paper, we propose an Adaptive Neuro-Symbolic Learning and Reasoning\nFramework for digital twin technology called ``ANSR-DT.\" Digital twins in\nindustrial environments often struggle with interpretability, real-time\nadaptation, and human input integration. Our approach addresses these\nchallenges by combining CNN-LSTM dynamic event detection with reinforcement\nlearning and symbolic reasoning to enable adaptive intelligence with\ninterpretable decision processes. This integration enhances environmental\nunderstanding while promoting continuous learning, leading to more effective\nreal-time decision-making in human-machine collaborative applications. We\nevaluated ANSR-DT on synthetic industrial data, observing significant\nimprovements over traditional approaches, with up to 99.5% accuracy for dynamic\npattern recognition. The framework demonstrated superior adaptability with\nextended reinforcement learning training, improving explained variance from\n0.447 to 0.547. Future work aims at scaling to larger datasets to test rule\nmanagement beyond the current 14 rules. Our open-source implementation promotes\nreproducibility and establishes a foundation for future research in adaptive,\ninterpretable digital twins for industrial applications.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08561v2",
    "published_date": "2025-01-15 04:04:57 UTC",
    "updated_date": "2025-04-11 13:05:47 UTC"
  },
  {
    "arxiv_id": "2501.08558v1",
    "title": "LAMS: LLM-Driven Automatic Mode Switching for Assistive Teleoperation",
    "authors": [
      "Yiran Tao",
      "Jehan Yang",
      "Dan Ding",
      "Zackory Erickson"
    ],
    "abstract": "Teleoperating high degrees-of-freedom (DoF) robotic manipulators via low-DoF\ncontrollers like joysticks often requires frequent switching between control\nmodes, where each mode maps controller movements to specific robot actions.\nManually performing this frequent switching can make teleoperation cumbersome\nand inefficient. On the other hand, existing automatic mode-switching\nsolutions, such as heuristic-based or learning-based methods, are often\ntask-specific and lack generalizability. In this paper, we introduce LLM-Driven\nAutomatic Mode Switching (LAMS), a novel approach that leverages Large Language\nModels (LLMs) to automatically switch control modes based on task context.\nUnlike existing methods, LAMS requires no prior task demonstrations and\nincrementally improves by integrating user-generated mode-switching examples.\nWe validate LAMS through an ablation study and a user study with 10\nparticipants on complex, long-horizon tasks, demonstrating that LAMS\neffectively reduces manual mode switches, is preferred over alternative\nmethods, and improves performance over time. The project website with\nsupplementary materials is at https://lams-assistance.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08558v1",
    "published_date": "2025-01-15 03:49:08 UTC",
    "updated_date": "2025-01-15 03:49:08 UTC"
  },
  {
    "arxiv_id": "2501.08552v2",
    "title": "Reinforcement Learning-Enhanced Procedural Generation for Dynamic Narrative-Driven AR Experiences",
    "authors": [
      "Aniruddha Srinivas Joshi"
    ],
    "abstract": "Procedural Content Generation (PCG) is widely used to create scalable and\ndiverse environments in games. However, existing methods, such as the Wave\nFunction Collapse (WFC) algorithm, are often limited to static scenarios and\nlack the adaptability required for dynamic, narrative-driven applications,\nparticularly in augmented reality (AR) games. This paper presents a\nreinforcement learning-enhanced WFC framework designed for mobile AR\nenvironments. By integrating environment-specific rules and dynamic tile weight\nadjustments informed by reinforcement learning (RL), the proposed method\ngenerates maps that are both contextually coherent and responsive to gameplay\nneeds. Comparative evaluations and user studies demonstrate that the framework\nachieves superior map quality and delivers immersive experiences, making it\nwell-suited for narrative-driven AR games. Additionally, the method holds\npromise for broader applications in education, simulation training, and\nimmersive extended reality (XR) experiences, where dynamic and adaptive\nenvironments are critical.",
    "categories": [
      "cs.AI",
      "cs.GR",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Published in Proceedings of the 20th International Joint Conference\n  on Computer Vision, Imaging and Computer Graphics Theory and Applications -\n  GRAPP 2025\n  https://www.scitepress.org/PublicationsDetail.aspx?ID=LfPv9Lfiya8=&t=1",
    "pdf_url": "http://arxiv.org/pdf/2501.08552v2",
    "published_date": "2025-01-15 03:23:06 UTC",
    "updated_date": "2025-03-13 07:31:10 UTC"
  },
  {
    "arxiv_id": "2501.08549v1",
    "title": "The Devil is in Temporal Token: High Quality Video Reasoning Segmentation",
    "authors": [
      "Sitong Gong",
      "Yunzhi Zhuge",
      "Lu Zhang",
      "Zongxin Yang",
      "Pingping Zhang",
      "Huchuan Lu"
    ],
    "abstract": "Existing methods for Video Reasoning Segmentation rely heavily on a single\nspecial token to represent the object in the keyframe or the entire video,\ninadequately capturing spatial complexity and inter-frame motion. To overcome\nthese challenges, we propose VRS-HQ, an end-to-end video reasoning segmentation\napproach that leverages Multimodal Large Language Models (MLLMs) to inject rich\nspatiotemporal features into hierarchical tokens.Our key innovations include a\nTemporal Dynamic Aggregation (TDA) and a Token-driven Keyframe Selection (TKS).\nSpecifically, we design frame-level <SEG> and temporal-level <TAK> tokens that\nutilize MLLM's autoregressive learning to effectively capture both local and\nglobal information. Subsequently, we apply a similarity-based weighted fusion\nand frame selection strategy, then utilize SAM2 to perform keyframe\nsegmentation and propagation. To enhance keyframe localization accuracy, the\nTKS filters keyframes based on SAM2's occlusion scores during inference. VRS-HQ\nachieves state-of-the-art performance on ReVOS, surpassing VISA by\n5.9%/12.5%/9.1% in J&F scores across the three subsets. These results highlight\nthe strong temporal reasoning and segmentation capabilities of our method. Code\nand model weights will be released at VRS-HQ.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08549v1",
    "published_date": "2025-01-15 03:17:24 UTC",
    "updated_date": "2025-01-15 03:17:24 UTC"
  },
  {
    "arxiv_id": "2501.12405v1",
    "title": "Scopes of Alignment",
    "authors": [
      "Kush R. Varshney",
      "Zahra Ashktorab",
      "Djallel Bouneffouf",
      "Matthew Riemer",
      "Justin D. Weisz"
    ],
    "abstract": "Much of the research focus on AI alignment seeks to align large language\nmodels and other foundation models to the context-less and generic values of\nhelpfulness, harmlessness, and honesty. Frontier model providers also strive to\nalign their models with these values. In this paper, we motivate why we need to\nmove beyond such a limited conception and propose three dimensions for doing\nso. The first scope of alignment is competence: knowledge, skills, or behaviors\nthe model must possess to be useful for its intended purpose. The second scope\nof alignment is transience: either semantic or episodic depending on the\ncontext of use. The third scope of alignment is audience: either mass, public,\nsmall-group, or dyadic. At the end of the paper, we use the proposed framework\nto position some technologies and workflows that go beyond prevailing notions\nof alignment.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "The 2nd International Workshop on AI Governance (AIGOV) held in\n  conjunction with AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.12405v1",
    "published_date": "2025-01-15 03:06:59 UTC",
    "updated_date": "2025-01-15 03:06:59 UTC"
  },
  {
    "arxiv_id": "2501.08540v1",
    "title": "Knowledge prompt chaining for semantic modeling",
    "authors": [
      "Ning Pei Ding",
      "Jingge Du",
      "Zaiwen Feng"
    ],
    "abstract": "The task of building semantics for structured data such as CSV, JSON, and XML\nfiles is highly relevant in the knowledge representation field. Even though we\nhave a vast of structured data on the internet, mapping them to domain\nontologies to build semantics for them is still very challenging as it requires\nthe construction model to understand and learn graph-structured knowledge.\nOtherwise, the task will require human beings' effort and cost. In this paper,\nwe proposed a novel automatic semantic modeling framework: Knowledge Prompt\nChaining. It can serialize the graph-structured knowledge and inject it into\nthe LLMs properly in a Prompt Chaining architecture. Through this knowledge\ninjection and prompting chaining, the model in our framework can learn the\nstructure information and latent space of the graph and generate the semantic\nlabels and semantic graphs following the chains' insturction naturally. Based\non experimental results, our method achieves better performance than existing\nleading techniques, despite using reduced structured input data.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08540v1",
    "published_date": "2025-01-15 03:00:57 UTC",
    "updated_date": "2025-01-15 03:00:57 UTC"
  },
  {
    "arxiv_id": "2501.08528v1",
    "title": "Dynamic Portfolio Optimization via Augmented DDPG with Quantum Price Levels-Based Trading Strategy",
    "authors": [
      "Runsheng Lin",
      "Zihan Xing",
      "Mingze Ma",
      "Raymond S. T. Lee"
    ],
    "abstract": "With the development of deep learning, Dynamic Portfolio Optimization (DPO)\nproblem has received a lot of attention in recent years, not only in the field\nof finance but also in the field of deep learning. Some advanced research in\nrecent years has proposed the application of Deep Reinforcement Learning (DRL)\nto the DPO problem, which demonstrated to be more advantageous than supervised\nlearning in solving the DPO problem. However, there are still certain unsolved\nissues: 1) DRL algorithms usually have the problems of slow learning speed and\nhigh sample complexity, which is especially problematic when dealing with\ncomplex financial data. 2) researchers use DRL simply for the purpose of\nobtaining high returns, but pay little attention to the problem of risk control\nand trading strategy, which will affect the stability of model returns. In\norder to address these issues, in this study we revamped the intrinsic\nstructure of the model based on the Deep Deterministic Policy Gradient (DDPG)\nand proposed the Augmented DDPG model. Besides, we also proposed an innovative\nrisk control strategy based on Quantum Price Levels (QPLs) derived from Quantum\nFinance Theory (QFT). Our experimental results revealed that our model has\nbetter profitability as well as risk control ability with less sample\ncomplexity in the DPO problem compared to the baseline models.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.08528v1",
    "published_date": "2025-01-15 02:37:28 UTC",
    "updated_date": "2025-01-15 02:37:28 UTC"
  },
  {
    "arxiv_id": "2501.08523v1",
    "title": "Doc-Guided Sent2Sent++: A Sent2Sent++ Agent with Doc-Guided memory for Document-level Machine Translation",
    "authors": [
      "Jiaxin Guo",
      "Yuanchang Luo",
      "Daimeng Wei",
      "Ling Zhang",
      "Zongyao Li",
      "Hengchao Shang",
      "Zhiqiang Rao",
      "Shaojun Li",
      "Jinlong Yang",
      "Zhanglin Wu",
      "Hao Yang"
    ],
    "abstract": "The field of artificial intelligence has witnessed significant advancements\nin natural language processing, largely attributed to the capabilities of Large\nLanguage Models (LLMs). These models form the backbone of Agents designed to\naddress long-context dependencies, particularly in Document-level Machine\nTranslation (DocMT). DocMT presents unique challenges, with quality,\nconsistency, and fluency being the key metrics for evaluation. Existing\napproaches, such as Doc2Doc and Doc2Sent, either omit sentences or compromise\nfluency. This paper introduces Doc-Guided Sent2Sent++, an Agent that employs an\nincremental sentence-level forced decoding strategy \\textbf{to ensure every\nsentence is translated while enhancing the fluency of adjacent sentences.} Our\nAgent leverages a Doc-Guided Memory, focusing solely on the summary and its\ntranslation, which we find to be an efficient approach to maintaining\nconsistency. Through extensive testing across multiple languages and domains,\nwe demonstrate that Sent2Sent++ outperforms other methods in terms of quality,\nconsistency, and fluency. The results indicate that, our approach has achieved\nsignificant improvements in metrics such as s-COMET, d-COMET, LTCR-$1_f$, and\ndocument-level perplexity (d-ppl). The contributions of this paper include a\ndetailed analysis of current DocMT research, the introduction of the\nSent2Sent++ decoding method, the Doc-Guided Memory mechanism, and validation of\nits effectiveness across languages and domains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08523v1",
    "published_date": "2025-01-15 02:25:35 UTC",
    "updated_date": "2025-01-15 02:25:35 UTC"
  },
  {
    "arxiv_id": "2501.08521v2",
    "title": "Mitigating Domain Shift in Federated Learning via Intra- and Inter-Domain Prototypes",
    "authors": [
      "Huy Q. Le",
      "Ye Lin Tun",
      "Yu Qiao",
      "Minh N. H. Nguyen",
      "Keon Oh Kim",
      "Choong Seon Hong"
    ],
    "abstract": "Federated Learning (FL) has emerged as a decentralized machine learning\ntechnique, allowing clients to train a global model collaboratively without\nsharing private data. However, most FL studies ignore the crucial challenge of\nheterogeneous domains where each client has a distinct feature distribution,\nwhich is popular in real-world scenarios. Prototype learning, which leverages\nthe mean feature vectors within the same classes, has become a prominent\nsolution for federated learning under domain shift. However, existing federated\nprototype learning methods focus soley on inter-domain prototypes and neglect\nintra-domain perspectives. In this work, we introduce a novel federated\nprototype learning method, namely I$^2$PFL, which incorporates\n$\\textbf{I}$ntra-domain and $\\textbf{I}$nter-domain $\\textbf{P}$rototypes, to\nmitigate domain shift from both perspectives and learn a generalized global\nmodel across multiple domains in federated learning. To construct intra-domain\nprototypes, we propose feature alignment with MixUp-based augmented prototypes\nto capture the diversity within local domains and enhance the generalization of\nlocal features. Additionally, we introduce a reweighting mechanism for\ninter-domain prototypes to generate generalized prototypes that reduce domain\nshift while providing inter-domain knowledge across multiple clients. Extensive\nexperiments on the Digits, Office-10, and PACS datasets illustrate the superior\nperformance of our method compared to other baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 11 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.08521v2",
    "published_date": "2025-01-15 02:17:38 UTC",
    "updated_date": "2025-03-10 02:01:38 UTC"
  },
  {
    "arxiv_id": "2501.08518v1",
    "title": "Easing Seasickness through Attention Redirection with a Mindfulness-Based Brain--Computer Interface",
    "authors": [
      "Xiaoyu Bao",
      "Kailin Xu",
      "Jiawei Zhu",
      "Haiyun Huang",
      "Kangning Li",
      "Qiyun Huang",
      "Yuanqing Li"
    ],
    "abstract": "Seasickness is a prevalent issue that adversely impacts both passenger\nexperiences and the operational efficiency of maritime crews. While techniques\nthat redirect attention have proven effective in alleviating motion sickness\nsymptoms in terrestrial environments, applying similar strategies to manage\nseasickness poses unique challenges due to the prolonged and intense motion\nenvironment associated with maritime travel. In this study, we propose a\nmindfulness brain-computer interface (BCI), specifically designed to redirect\nattention with the aim of mitigating seasickness symptoms in real-world\nsettings. Our system utilizes a single-channel headband to capture prefrontal\nEEG signals, which are then wirelessly transmitted to computing devices for the\nassessment of mindfulness states. The results are transferred into real-time\nfeedback as mindfulness scores and audiovisual stimuli, facilitating a shift in\nattentional focus from physiological discomfort to mindfulness practices. A\ntotal of 43 individuals participated in a real-world maritime experiment\nconsisted of three sessions: a real-feedback mindfulness session, a resting\nsession, and a pseudofeedback mindfulness session. Notably, 81.39% of\nparticipants reported that the mindfulness BCI intervention was effective, and\nthere was a significant reduction in the severity of seasickness, as measured\nby the Misery Scale (MISC). Furthermore, EEG analysis revealed a decrease in\nthe theta/beta ratio, corresponding with the alleviation of seasickness\nsymptoms. A decrease in overall EEG band power during the real-feedback\nmindfulness session suggests that the mindfulness BCI fosters a more tranquil\nand downregulated state of brain activity. Together, this study presents a\nnovel nonpharmacological, portable, and effective approach for seasickness\nintervention, with the potential to enhance the cruising experience for both\npassengers and crews.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "eess.SP",
      "q-bio.QM"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08518v1",
    "published_date": "2025-01-15 02:06:29 UTC",
    "updated_date": "2025-01-15 02:06:29 UTC"
  },
  {
    "arxiv_id": "2501.08506v2",
    "title": "Exploring the Efficacy of Meta-Learning: Unveiling Superior Data Diversity Utilization of MAML Over Pre-training",
    "authors": [
      "Kavita Selva",
      "Satita Vittayaareekul",
      "Brando Miranda"
    ],
    "abstract": "Currently, data and model size dominate the narrative in the training of\nsuper-large, powerful models. However, there has been a lack of exploration on\nthe effect of other attributes of the training dataset on model performance. We\nhypothesize that dataset diversity can impact the performance of vision models.\nOur study shows positive correlations between test set accuracy and data\ndiversity, providing an argument for furthering the research of dataset\nattributes beyond size. We analyzed pre-training and model-agnostic\nmeta-learning methods on twelve popular visual datasets (e.g., Omniglot,\nCIFAR-FS, Aircraft) and five model configurations, including MAML variants with\ndifferent numbers of inner gradient steps and supervised learning. We show\nmoderate to strong positive correlations (R-squared: 0.15-0.42) between\naccuracy and data diversity and weaker but significant correlations (R-squared:\n~0.2) between loss and diversity. These findings support our hypothesis and\ndemonstrate a promising way for a deeper exploration of how formal data\ndiversity influences model performance. This initial study highlights the\npotential of (Task2Vec) data diversity as a valuable measure in the rapidly\nevolving field of large-scale learning and emphasizes that understanding the\ndataset is key to building more powerful and generalizable models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08506v2",
    "published_date": "2025-01-15 00:56:59 UTC",
    "updated_date": "2025-01-21 01:01:20 UTC"
  },
  {
    "arxiv_id": "2501.08502v1",
    "title": "Adapting Whisper for Regional Dialects: Enhancing Public Services for Vulnerable Populations in the United Kingdom",
    "authors": [
      "Melissa Torgbi",
      "Andrew Clayman",
      "Jordan J. Speight",
      "Harish Tayyar Madabushi"
    ],
    "abstract": "We collect novel data in the public service domain to evaluate the capability\nof the state-of-the-art automatic speech recognition (ASR) models in capturing\nregional differences in accents in the United Kingdom (UK), specifically\nfocusing on two accents from Scotland with distinct dialects. This study\naddresses real-world problems where biased ASR models can lead to\nmiscommunication in public services, disadvantaging individuals with regional\naccents particularly those in vulnerable populations. We first examine the\nout-of-the-box performance of the Whisper large-v3 model on a baseline dataset\nand our data. We then explore the impact of fine-tuning Whisper on the\nperformance in the two UK regions and investigate the effectiveness of existing\nmodel evaluation techniques for our real-world application through manual\ninspection of model errors. We observe that the Whisper model has a higher word\nerror rate (WER) on our test datasets compared to the baseline data and\nfine-tuning on a given data improves performance on the test dataset with the\nsame domain and accent. The fine-tuned models also appear to show improved\nperformance when applied to the test data outside of the region it was trained\non suggesting that fine-tuned models may be transferable within parts of the\nUK. Our manual analysis of model outputs reveals the benefits and drawbacks of\nusing WER as an evaluation metric and fine-tuning to adapt to regional\ndialects.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08502v1",
    "published_date": "2025-01-15 00:39:21 UTC",
    "updated_date": "2025-01-15 00:39:21 UTC"
  }
]
[
  {
    "arxiv_id": "2409.04653v1",
    "title": "Solving Stochastic Orienteering Problems with Chance Constraints Using a GNN Powered Monte Carlo Tree Search",
    "authors": [
      "Marcos Abel Zuzu√°rregui",
      "Stefano Carpin"
    ],
    "abstract": "Leveraging the power of a graph neural network (GNN) with message passing, we\npresent a Monte Carlo Tree Search (MCTS) method to solve stochastic\norienteering problems with chance constraints. While adhering to an assigned\ntravel budget the algorithm seeks to maximize collected reward while incurring\nstochastic travel costs. In this context, the acceptable probability of\nexceeding the assigned budget is expressed as a chance constraint. Our MCTS\nsolution is an online and anytime algorithm alternating planning and execution\nthat determines the next vertex to visit by continuously monitoring the\nremaining travel budget. The novelty of our work is that the rollout phase in\nthe MCTS framework is implemented using a message passing GNN, predicting both\nthe utility and failure probability of each available action. This allows to\nenormously expedite the search process. Our experimental evaluation shows that\nwith the proposed method and architecture we manage to efficiently solve\ncomplex problem instances while incurring in moderate losses in terms of\ncollected reward. Moreover, we demonstrate how the approach is capable of\ngeneralizing beyond the characteristics of the training dataset. The paper's\nwebsite, open-source code, and supplementary documentation can be found at\nucmercedrobotics.github.io/gnn-sop.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.04653v1",
    "published_date": "2024-09-06 23:31:01 UTC",
    "updated_date": "2024-09-06 23:31:01 UTC"
  },
  {
    "arxiv_id": "2409.04641v1",
    "title": "Stacked Universal Successor Feature Approximators for Safety in Reinforcement Learning",
    "authors": [
      "Ian Cannon",
      "Washington Garcia",
      "Thomas Gresavage",
      "Joseph Saurine",
      "Ian Leong",
      "Jared Culbertson"
    ],
    "abstract": "Real-world problems often involve complex objective structures that resist\ndistillation into reinforcement learning environments with a single objective.\nOperation costs must be balanced with multi-dimensional task performance and\nend-states' effects on future availability, all while ensuring safety for other\nagents in the environment and the reinforcement learning agent itself. System\nredundancy through secondary backup controllers has proven to be an effective\nmethod to ensure safety in real-world applications where the risk of violating\nconstraints is extremely high. In this work, we investigate the utility of a\nstacked, continuous-control variation of universal successor feature\napproximation (USFA) adapted for soft actor-critic (SAC) and coupled with a\nsuite of secondary safety controllers, which we call stacked USFA for safety\n(SUSFAS). Our method improves performance on secondary objectives compared to\nSAC baselines using an intervening secondary controller such as a runtime\nassurance (RTA) controller.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.04641v1",
    "published_date": "2024-09-06 22:20:07 UTC",
    "updated_date": "2024-09-06 22:20:07 UTC"
  },
  {
    "arxiv_id": "2409.04637v1",
    "title": "Enhancing Quantum Security over Federated Learning via Post-Quantum Cryptography",
    "authors": [
      "Pingzhi Li",
      "Tianlong Chen",
      "Junyu Liu"
    ],
    "abstract": "Federated learning (FL) has become one of the standard approaches for\ndeploying machine learning models on edge devices, where private training data\nare distributed across clients, and a shared model is learned by aggregating\nlocally computed updates from each client. While this paradigm enhances\ncommunication efficiency by only requiring updates at the end of each training\nepoch, the transmitted model updates remain vulnerable to malicious tampering,\nposing risks to the integrity of the global model. Although current digital\nsignature algorithms can protect these communicated model updates, they fail to\nensure quantum security in the era of large-scale quantum computing.\nFortunately, various post-quantum cryptography algorithms have been developed\nto address this vulnerability, especially the three NIST-standardized\nalgorithms - Dilithium, FALCON, and SPHINCS+. In this work, we empirically\ninvestigate the impact of these three NIST-standardized PQC algorithms for\ndigital signatures within the FL procedure, covering a wide range of models,\ntasks, and FL settings. Our results indicate that Dilithium stands out as the\nmost efficient PQC algorithm for digital signature in federated learning.\nAdditionally, we offer an in-depth discussion of the implications of our\nfindings and potential directions for future research.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "Submission for IEEE 2024 IEEE Workshop on Quantum IntelLigence,\n  Learning & Security (QUILLS), https://sites.google.com/pitt.edu/quills/home",
    "pdf_url": "http://arxiv.org/pdf/2409.04637v1",
    "published_date": "2024-09-06 22:02:08 UTC",
    "updated_date": "2024-09-06 22:02:08 UTC"
  },
  {
    "arxiv_id": "2409.04631v2",
    "title": "Zero-Shot Whole Slide Image Retrieval in Histopathology Using Embeddings of Foundation Models",
    "authors": [
      "Saghir Alfasly",
      "Ghazal Alabtah",
      "Sobhan Hemati",
      "Krishna Rani Kalari",
      "H. R. Tizhoosh"
    ],
    "abstract": "We have tested recently published foundation models for histopathology for\nimage retrieval. We report macro average of F1 score for top-1 retrieval,\nmajority of top-3 retrievals, and majority of top-5 retrievals. We perform\nzero-shot retrievals, i.e., we do not alter embeddings and we do not train any\nclassifier. As test data, we used diagnostic slides of TCGA, The Cancer Genome\nAtlas, consisting of 23 organs and 117 cancer subtypes. As a search platform we\nused Yottixel that enabled us to perform WSI search using patches. Achieved F1\nscores show low performance, e.g., for top-5 retrievals, 27% +/- 13%\n(Yottixel-DenseNet), 42% +/- 14% (Yottixel-UNI), 40%+/-13% (Yottixel-Virchow),\n41%+/-13% (Yottixel-GigaPath), and 41%+/-14% (GigaPath WSI).",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "This paper will be updated with more results",
    "pdf_url": "http://arxiv.org/pdf/2409.04631v2",
    "published_date": "2024-09-06 21:43:00 UTC",
    "updated_date": "2024-09-12 15:37:30 UTC"
  },
  {
    "arxiv_id": "2409.04615v1",
    "title": "A Short Survey on Set-Based Aggregation Techniques for Single-Vector WSI Representation in Digital Pathology",
    "authors": [
      "S. Hemati",
      "Krishna R. Kalari",
      "H. R. Tizhoosh"
    ],
    "abstract": "Digital pathology is revolutionizing the field of pathology by enabling the\ndigitization, storage, and analysis of tissue samples as whole slide images\n(WSIs). WSIs are gigapixel files that capture the intricate details of tissue\nsamples, providing a rich source of information for diagnostic and research\npurposes. However, due to their enormous size, representing these images as one\ncompact vector is essential for many computational pathology tasks, such as\nsearch and retrieval, to ensure efficiency and scalability. Most current\nmethods are \"patch-oriented,\" meaning they divide WSIs into smaller patches for\nprocessing, which prevents a holistic analysis of the entire slide.\nAdditionally, the necessity for compact representation is driven by the\nexpensive high-performance storage required for WSIs. Not all hospitals have\naccess to such extensive storage solutions, leading to potential disparities in\nhealthcare quality and accessibility. This paper provides an overview of\nexisting set-based approaches to single-vector WSI representation, highlighting\nthe innovations that allow for more efficient and effective use of these\ncomplex images in digital pathology, thus addressing both computational\nchallenges and storage limitations.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04615v1",
    "published_date": "2024-09-06 20:56:25 UTC",
    "updated_date": "2024-09-06 20:56:25 UTC"
  },
  {
    "arxiv_id": "2409.04613v5",
    "title": "Convergence of Decentralized Actor-Critic Algorithm in General-sum Markov Games",
    "authors": [
      "Chinmay Maheshwari",
      "Manxi Wu",
      "Shankar Sastry"
    ],
    "abstract": "Markov games provide a powerful framework for modeling strategic multi-agent\ninteractions in dynamic environments. Traditionally, convergence properties of\ndecentralized learning algorithms in these settings have been established only\nfor special cases, such as Markov zero-sum and potential games, which do not\nfully capture real-world interactions. In this paper, we address this gap by\nstudying the asymptotic properties of learning algorithms in general-sum Markov\ngames. In particular, we focus on a decentralized algorithm where each agent\nadopts an actor-critic learning dynamic with asynchronous step sizes. This\ndecentralized approach enables agents to operate independently, without\nrequiring knowledge of others' strategies or payoffs. We introduce the concept\nof a Markov Near-Potential Function (MNPF) and demonstrate that it serves as an\napproximate Lyapunov function for the policy updates in the decentralized\nlearning dynamics, which allows us to characterize the convergent set of\nstrategies. We further strengthen our result under specific regularity\nconditions and with finite Nash equilibria.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.GT",
      "cs.SY",
      "eess.SY",
      "math.OC",
      "91A06, 91A10, 91A14, 91A15, 91A20, 91A40, 91A50, 93E03, 37N40"
    ],
    "primary_category": "cs.MA",
    "comment": "18 pages, 3 figure",
    "pdf_url": "http://arxiv.org/pdf/2409.04613v5",
    "published_date": "2024-09-06 20:49:11 UTC",
    "updated_date": "2025-04-01 00:36:00 UTC"
  },
  {
    "arxiv_id": "2409.04609v1",
    "title": "Detection of False Data Injection Attacks (FDIA) on Power Dynamical Systems With a State Prediction Method",
    "authors": [
      "Abhijeet Sahu",
      "Truc Nguyen",
      "Kejun Chen",
      "Xiangyu Zhang",
      "Malik Hassanaly"
    ],
    "abstract": "With the deeper penetration of inverter-based resources in power systems,\nfalse data injection attacks (FDIA) are a growing cyber-security concern. They\nhave the potential to disrupt the system's stability like frequency stability,\nthereby leading to catastrophic failures. Therefore, an FDIA detection method\nwould be valuable to protect power systems. FDIAs typically induce a\ndiscrepancy between the desired and the effective behavior of the power system\ndynamics. A suitable detection method can leverage power dynamics predictions\nto identify whether such a discrepancy was induced by an FDIA. This work\ninvestigates the efficacy of temporal and spatio-temporal state prediction\nmodels, such as Long Short-Term Memory (LSTM) and a combination of Graph Neural\nNetworks (GNN) with LSTM, for predicting frequency dynamics in the absence of\nan FDIA but with noisy measurements, and thereby identify FDIA events. For\ndemonstration purposes, the IEEE 39 New England Kron-reduced model simulated\nwith a swing equation is considered. It is shown that the proposed state\nprediction models can be used as a building block for developing an effective\nFDIA detection method that can maintain high detection accuracy across various\nattack and deployment settings. It is also shown how the FDIA detection should\nbe deployed to limit its exposure to detection inaccuracies and mitigate its\ncomputational burden.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2409.04609v1",
    "published_date": "2024-09-06 20:47:21 UTC",
    "updated_date": "2024-09-06 20:47:21 UTC"
  },
  {
    "arxiv_id": "2409.04602v2",
    "title": "Training quantum machine learning models on cloud without uploading the data",
    "authors": [
      "Guang Ping He"
    ],
    "abstract": "Based on the linearity of quantum unitary operations, we propose a method\nthat runs the parameterized quantum circuits before encoding the input data.\nThis enables a dataset owner to train machine learning models on quantum cloud\ncomputation platforms, without the risk of leaking the information about the\ndata. It is also capable of encoding a vast amount of data effectively at a\nlater time using classical computations, thus saving runtime on quantum\ncomputation devices. The trained quantum machine learning models can be run\ncompletely on classical computers, meaning the dataset owner does not need to\nhave any quantum hardware, nor even quantum simulators. Moreover, our method\nmitigates the encoding bottleneck by reducing the required circuit depth from\n$O(2^{n})$ to $O(n)$, and relax the tolerance on the precision of the quantum\ngates for the encoding. These results demonstrate yet another advantage of\nquantum and quantum-inspired machine learning models over existing classical\nneural networks, and broaden the approaches to data security.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "Added experimental results and the flow chart of our method",
    "pdf_url": "http://arxiv.org/pdf/2409.04602v2",
    "published_date": "2024-09-06 20:14:52 UTC",
    "updated_date": "2024-10-07 20:19:38 UTC"
  },
  {
    "arxiv_id": "2409.04600v1",
    "title": "The emergence of Large Language Models (LLM) as a tool in literature reviews: an LLM automated systematic review",
    "authors": [
      "Dmitry Scherbakov",
      "Nina Hubig",
      "Vinita Jansari",
      "Alexander Bakumenko",
      "Leslie A. Lenert"
    ],
    "abstract": "Objective: This study aims to summarize the usage of Large Language Models\n(LLMs) in the process of creating a scientific review. We look at the range of\nstages in a review that can be automated and assess the current\nstate-of-the-art research projects in the field. Materials and Methods: The\nsearch was conducted in June 2024 in PubMed, Scopus, Dimensions, and Google\nScholar databases by human reviewers. Screening and extraction process took\nplace in Covidence with the help of LLM add-on which uses OpenAI gpt-4o model.\nChatGPT was used to clean extracted data and generate code for figures in this\nmanuscript, ChatGPT and Scite.ai were used in drafting all components of the\nmanuscript, except the methods and discussion sections. Results: 3,788 articles\nwere retrieved, and 172 studies were deemed eligible for the final review.\nChatGPT and GPT-based LLM emerged as the most dominant architecture for review\nautomation (n=126, 73.2%). A significant number of review automation projects\nwere found, but only a limited number of papers (n=26, 15.1%) were actual\nreviews that used LLM during their creation. Most citations focused on\nautomation of a particular stage of review, such as Searching for publications\n(n=60, 34.9%), and Data extraction (n=54, 31.4%). When comparing pooled\nperformance of GPT-based and BERT-based models, the former were better in data\nextraction with mean precision 83.0% (SD=10.4), and recall 86.0% (SD=9.8),\nwhile being slightly less accurate in title and abstract screening stage\n(Maccuracy=77.3%, SD=13.0). Discussion/Conclusion: Our LLM-assisted systematic\nreview revealed a significant number of research projects related to review\nautomation using LLMs. The results looked promising, and we anticipate that\nLLMs will change in the near future the way the scientific reviews are\nconducted.",
    "categories": [
      "cs.DL",
      "cs.AI"
    ],
    "primary_category": "cs.DL",
    "comment": "18 main pages with 5 figures and 1 table, references, followed by\n  supplementary materials",
    "pdf_url": "http://arxiv.org/pdf/2409.04600v1",
    "published_date": "2024-09-06 20:12:57 UTC",
    "updated_date": "2024-09-06 20:12:57 UTC"
  },
  {
    "arxiv_id": "2409.04585v2",
    "title": "CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance",
    "authors": [
      "Wei Wen",
      "Quanyu Zhu",
      "Weiwei Chu",
      "Wen-Yen Chen",
      "Jiyan Yang"
    ],
    "abstract": "Scaling up deep learning models has been proven effective to improve\nintelligence of machine learning (ML) models, especially for industry\nrecommendation models and large language models. The co-design of large\ndistributed ML systems and algorithms (to maximize training performance) plays\na pivotal role for its success. As it scales, the number of co-design\nhyper-parameters grows rapidly which brings challenges to feasibly find the\noptimal setup for system performance maximization. In this paper, we propose\nCubicML which uses ML to automatically optimize training performance of large\ndistributed ML systems. In CubicML, we use an ML model as a proxy to predict\nthe training performance for search efficiency and performance modeling\nflexibility. We proved that CubicML can effectively optimize training speed of\nin-house ads recommendation models with 73 billion parameters and large\nlanguage models up to 405 billion parameters at Meta.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04585v2",
    "published_date": "2024-09-06 19:55:21 UTC",
    "updated_date": "2024-09-21 05:55:30 UTC"
  },
  {
    "arxiv_id": "2409.04576v1",
    "title": "ActionFlow: Equivariant, Accurate, and Efficient Policies with Spatially Symmetric Flow Matching",
    "authors": [
      "Niklas Funk",
      "Julen Urain",
      "Joao Carvalho",
      "Vignesh Prasad",
      "Georgia Chalvatzaki",
      "Jan Peters"
    ],
    "abstract": "Spatial understanding is a critical aspect of most robotic tasks,\nparticularly when generalization is important. Despite the impressive results\nof deep generative models in complex manipulation tasks, the absence of a\nrepresentation that encodes intricate spatial relationships between\nobservations and actions often limits spatial generalization, necessitating\nlarge amounts of demonstrations. To tackle this problem, we introduce a novel\npolicy class, ActionFlow. ActionFlow integrates spatial symmetry inductive\nbiases while generating expressive action sequences. On the representation\nlevel, ActionFlow introduces an SE(3) Invariant Transformer architecture, which\nenables informed spatial reasoning based on the relative SE(3) poses between\nobservations and actions. For action generation, ActionFlow leverages Flow\nMatching, a state-of-the-art deep generative model known for generating\nhigh-quality samples with fast inference - an essential property for feedback\ncontrol. In combination, ActionFlow policies exhibit strong spatial and\nlocality biases and SE(3)-equivariant action generation. Our experiments\ndemonstrate the effectiveness of ActionFlow and its two main components on\nseveral simulated and real-world robotic manipulation tasks and confirm that we\ncan obtain equivariant, accurate, and efficient policies with spatially\nsymmetric flow matching. Project website: https://flowbasedpolicies.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04576v1",
    "published_date": "2024-09-06 19:30:36 UTC",
    "updated_date": "2024-09-06 19:30:36 UTC"
  },
  {
    "arxiv_id": "2409.04572v1",
    "title": "Neurosymbolic Methods for Dynamic Knowledge Graphs",
    "authors": [
      "Mehwish Alam",
      "Genet Asefa Gesese",
      "Pierre-Henri Paris"
    ],
    "abstract": "Knowledge graphs (KGs) have recently been used for many tools and\napplications, making them rich resources in structured format. However, in the\nreal world, KGs grow due to the additions of new knowledge in the form of\nentities and relations, making these KGs dynamic. This chapter formally defines\nseveral types of dynamic KGs and summarizes how these KGs can be represented.\nAdditionally, many neurosymbolic methods have been proposed for learning\nrepresentations over static KGs for several tasks such as KG completion and\nentity alignment. This chapter further focuses on neurosymbolic methods for\ndynamic KGs with or without temporal information. More specifically, it\nprovides an insight into neurosymbolic methods for dynamic (temporal or\nnon-temporal) KG completion and entity alignment tasks. It further discusses\nthe challenges of current approaches and provides some future directions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04572v1",
    "published_date": "2024-09-06 19:24:29 UTC",
    "updated_date": "2024-09-06 19:24:29 UTC"
  },
  {
    "arxiv_id": "2409.16291v1",
    "title": "Beyond Following: Mixing Active Initiative into Computational Creativity",
    "authors": [
      "Zhiyu Lin",
      "Upol Ehsan",
      "Rohan Agarwal",
      "Samihan Dani",
      "Vidushi Vashishth",
      "Mark Riedl"
    ],
    "abstract": "Generative Artificial Intelligence (AI) encounters limitations in efficiency\nand fairness within the realm of Procedural Content Generation (PCG) when human\ncreators solely drive and bear responsibility for the generative process.\nAlternative setups, such as Mixed-Initiative Co-Creative (MI-CC) systems,\nexhibited their promise. Still, the potential of an active mixed initiative,\nwhere AI takes a role beyond following, is understudied. This work investigates\nthe influence of the adaptive ability of an active and learning AI agent on\ncreators' expectancy of creative responsibilities in an MI-CC setting. We built\nand studied a system that employs reinforcement learning (RL) methods to learn\nthe creative responsibility preferences of a human user during online\ninteractions. Situated in story co-creation, we develop a Multi-armed-bandit\nagent that learns from the human creator, updates its collaborative\ndecision-making belief, and switches between its capabilities during an MI-CC\nexperience. With 39 participants joining a human subject study, Our developed\nsystem's learning capabilities are well recognized compared to the non-learning\nablation, corresponding to a significant increase in overall satisfaction with\nthe MI-CC experience. These findings indicate a robust association between\neffective MI-CC collaborative interactions, particularly the implementation of\nproactive AI initiatives, and deepened understanding among all participants.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "11 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.16291v1",
    "published_date": "2024-09-06 18:56:08 UTC",
    "updated_date": "2024-09-06 18:56:08 UTC"
  },
  {
    "arxiv_id": "2409.13711v2",
    "title": "WebQuest: A Benchmark for Multimodal QA on Web Page Sequences",
    "authors": [
      "Maria Wang",
      "Srinivas Sunkara",
      "Gilles Baechler",
      "Jason Lin",
      "Yun Zhu",
      "Fedir Zubach",
      "Lei Shu",
      "Jindong Chen"
    ],
    "abstract": "The rise of powerful multimodal LLMs has enhanced the viability of building\nweb agents which can, with increasing levels of autonomy, assist users to\nretrieve information and complete tasks on various human-computer interfaces.\nIt is hence necessary to build challenging benchmarks that span a wide-variety\nof use cases reflecting real-world usage. In this work, we present WebQuest, a\nmulti-page question-answering dataset that requires reasoning across multiple\nrelated web pages. In contrast to existing UI benchmarks that focus on\nmulti-step web navigation and task completion, our dataset evaluates\ninformation extraction, multimodal retrieval and composition of information\nfrom many web pages. WebQuest includes three question categories: single-screen\nQA, multi-screen QA, and QA based on navigation traces. We evaluate leading\nproprietary multimodal models like GPT-4V, Gemini Flash, Claude 3, and open\nsource models like InstructBLIP, PaliGemma on our dataset, revealing a\nsignificant gap between single-screen and multi-screen reasoning. Finally, we\ninvestigate inference time techniques like Chain-of-Thought prompting to\nimprove model capabilities on multi-screen reasoning.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13711v2",
    "published_date": "2024-09-06 18:44:25 UTC",
    "updated_date": "2024-09-24 18:38:02 UTC"
  },
  {
    "arxiv_id": "2409.04559v2",
    "title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing",
    "authors": [
      "Gemma Canet Tarr√©s",
      "Zhe Lin",
      "Zhifei Zhang",
      "Jianming Zhang",
      "Yizhi Song",
      "Dan Ruta",
      "Andrew Gilbert",
      "John Collomosse",
      "Soo Ye Kim"
    ],
    "abstract": "Compositing an object into an image involves multiple non-trivial sub-tasks\nsuch as object placement and scaling, color/lighting harmonization,\nviewpoint/geometry adjustment, and shadow/reflection generation. Recent\ngenerative image compositing methods leverage diffusion models to handle\nmultiple sub-tasks at once. However, existing models face limitations due to\ntheir reliance on masking the original object during training, which constrains\ntheir generation to the input mask. Furthermore, obtaining an accurate input\nmask specifying the location and scale of the object in a new image can be\nhighly challenging. To overcome such limitations, we define a novel problem of\nunconstrained generative object compositing, i.e., the generation is not\nbounded by the mask, and train a diffusion-based model on a synthesized paired\ndataset. Our first-of-its-kind model is able to generate object effects such as\nshadows and reflections that go beyond the mask, enhancing image realism.\nAdditionally, if an empty mask is provided, our model automatically places the\nobject in diverse natural locations and scales, accelerating the compositing\nworkflow. Our model outperforms existing object placement and compositing\nmodels in various quality metrics and user studies.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04559v2",
    "published_date": "2024-09-06 18:42:30 UTC",
    "updated_date": "2024-09-11 11:05:56 UTC"
  },
  {
    "arxiv_id": "2409.04519v1",
    "title": "The role of data embedding in quantum autoencoders for improved anomaly detection",
    "authors": [
      "Jack Y. Araz",
      "Michael Spannowsky"
    ],
    "abstract": "The performance of Quantum Autoencoders (QAEs) in anomaly detection tasks is\ncritically dependent on the choice of data embedding and ansatz design. This\nstudy explores the effects of three data embedding techniques, data\nre-uploading, parallel embedding, and alternate embedding, on the\nrepresentability and effectiveness of QAEs in detecting anomalies. Our findings\nreveal that even with relatively simple variational circuits, enhanced data\nembedding strategies can substantially improve anomaly detection accuracy and\nthe representability of underlying data across different datasets. Starting\nwith toy examples featuring low-dimensional data, we visually demonstrate the\neffect of different embedding techniques on the representability of the model.\nWe then extend our analysis to complex, higher-dimensional datasets,\nhighlighting the significant impact of embedding methods on QAE performance.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG",
      "physics.data-an"
    ],
    "primary_category": "quant-ph",
    "comment": "8 pages, 5 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.04519v1",
    "published_date": "2024-09-06 18:00:01 UTC",
    "updated_date": "2024-09-06 18:00:01 UTC"
  },
  {
    "arxiv_id": "2409.04434v3",
    "title": "Accelerating Training with Neuron Interaction and Nowcasting Networks",
    "authors": [
      "Boris Knyazev",
      "Abhinav Moudgil",
      "Guillaume Lajoie",
      "Eugene Belilovsky",
      "Simon Lacoste-Julien"
    ],
    "abstract": "Neural network training can be accelerated when a learnable update rule is\nused in lieu of classic adaptive optimizers (e.g. Adam). However, learnable\nupdate rules can be costly and unstable to train and use. Recently, Jang et al.\n(2023) proposed a simpler approach to accelerate training based on weight\nnowcaster networks (WNNs). In their approach, Adam is used for most of the\noptimization steps and periodically, only every few steps, a WNN nowcasts\n(predicts near future) parameters. We improve WNNs by proposing neuron\ninteraction and nowcasting (NiNo) networks. In contrast to WNNs, NiNo leverages\nneuron connectivity and graph neural networks to more accurately nowcast\nparameters. We further show that in some networks, such as Transformers,\nmodeling neuron connectivity accurately is challenging. We address this and\nother limitations, which allows NiNo to accelerate Adam training by up to 50%\nin vision and language tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025, code is https://github.com/SamsungSAILMontreal/nino",
    "pdf_url": "http://arxiv.org/pdf/2409.04434v3",
    "published_date": "2024-09-06 17:55:49 UTC",
    "updated_date": "2025-02-27 19:52:21 UTC"
  },
  {
    "arxiv_id": "2409.04432v2",
    "title": "A Survey on Knowledge Organization Systems of Research Fields: Resources and Challenges",
    "authors": [
      "Angelo Salatino",
      "Tanay Aggarwal",
      "Andrea Mannocci",
      "Francesco Osborne",
      "Enrico Motta"
    ],
    "abstract": "Knowledge Organization Systems (KOSs), such as term lists, thesauri,\ntaxonomies, and ontologies, play a fundamental role in categorising, managing,\nand retrieving information. In the academic domain, KOSs are often adopted for\nrepresenting research areas and their relationships, primarily aiming to\nclassify research articles, academic courses, patents, books, scientific\nvenues, domain experts, grants, software, experiment materials, and several\nother relevant products and agents. These structured representations of\nresearch areas, widely embraced by many academic fields, have proven effective\nin empowering AI-based systems to i) enhance retrievability of relevant\ndocuments, ii) enable advanced analytic solutions to quantify the impact of\nacademic research, and iii) analyse and forecast research dynamics. This paper\naims to present a comprehensive survey of the current KOS for academic\ndisciplines. We analysed and compared 45 KOSs according to five main\ndimensions: scope, structure, curation, usage, and links to other KOSs. Our\nresults reveal a very heterogeneous scenario in terms of scope, scale, quality,\nand usage, highlighting the need for more integrated solutions for representing\nresearch knowledge across academic fields. We conclude by discussing the main\nchallenges and the most promising future directions.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04432v2",
    "published_date": "2024-09-06 17:54:43 UTC",
    "updated_date": "2025-01-27 18:03:08 UTC"
  },
  {
    "arxiv_id": "2409.15326v1",
    "title": "Evaluating the Impact of a Specialized LLM on Physician Experience in Clinical Decision Support: A Comparison of Ask Avo and ChatGPT-4",
    "authors": [
      "Daniel Jung",
      "Alex Butler",
      "Joongheum Park",
      "Yair Saperstein"
    ],
    "abstract": "The use of Large language models (LLMs) to augment clinical decision support\nsystems is a topic with rapidly growing interest, but current shortcomings such\nas hallucinations and lack of clear source citations make them unreliable for\nuse in the clinical environment. This study evaluates Ask Avo, an LLM-derived\nsoftware by AvoMD that incorporates a proprietary Language Model Augmented\nRetrieval (LMAR) system, in-built visual citation cues, and prompt engineering\ndesigned for interactions with physicians, against ChatGPT-4 in end-user\nexperience for physicians in a simulated clinical scenario environment. Eight\nclinical questions derived from medical guideline documents in various\nspecialties were prompted to both models by 62 study participants, with each\nresponse rated on trustworthiness, actionability, relevancy, comprehensiveness,\nand friendly format from 1 to 5. Ask Avo significantly outperformed ChatGPT-4\nin all criteria: trustworthiness (4.52 vs. 3.34, p<0.001), actionability (4.41\nvs. 3.19, p<0.001), relevancy (4.55 vs. 3.49, p<0.001), comprehensiveness (4.50\nvs. 3.37, p<0.001), and friendly format (4.52 vs. 3.60, p<0.001). Our findings\nsuggest that specialized LLMs designed with the needs of clinicians in mind can\noffer substantial improvements in user experience over general-purpose LLMs.\nAsk Avo's evidence-based approach tailored to clinician needs shows promise in\nthe adoption of LLM-augmented clinical decision support software.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "8 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2409.15326v1",
    "published_date": "2024-09-06 17:53:29 UTC",
    "updated_date": "2024-09-06 17:53:29 UTC"
  },
  {
    "arxiv_id": "2409.04428v2",
    "title": "Hybrid Spiking Neural Networks for Low-Power Intra-Cortical Brain-Machine Interfaces",
    "authors": [
      "Alexandru Vasilache",
      "Jann Krausse",
      "Klaus Knobloch",
      "Juergen Becker"
    ],
    "abstract": "Intra-cortical brain-machine interfaces (iBMIs) have the potential to\ndramatically improve the lives of people with paraplegia by restoring their\nability to perform daily activities. However, current iBMIs suffer from\nscalability and mobility limitations due to bulky hardware and wiring. Wireless\niBMIs offer a solution but are constrained by a limited data rate. To overcome\nthis challenge, we are investigating hybrid spiking neural networks for\nembedded neural decoding in wireless iBMIs. The networks consist of a temporal\nconvolution-based compression followed by recurrent processing and a final\ninterpolation back to the original sequence length. As recurrent units, we\nexplore gated recurrent units (GRUs), leaky integrate-and-fire (LIF) neurons,\nand a combination of both - spiking GRUs (sGRUs) and analyze their differences\nin terms of accuracy, footprint, and activation sparsity. To that end, we train\ndecoders on the \"Nonhuman Primate Reaching with Multichannel Sensorimotor\nCortex Electrophysiology\" dataset and evaluate it using the NeuroBench\nframework, targeting both tracks of the IEEE BioCAS Grand Challenge on Neural\nDecoding. Our approach achieves high accuracy in predicting velocities of\nprimate reaching movements from multichannel primary motor cortex recordings\nwhile maintaining a low number of synaptic operations, surpassing the current\nbaseline models in the NeuroBench framework. This work highlights the potential\nof hybrid neural networks to facilitate wireless iBMIs with high decoding\nprecision and a substantial increase in the number of monitored neurons, paving\nthe way toward more advanced neuroprosthetic technologies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.LG",
    "comment": "This work has been accepted at the 2024 IEEE Biomedical Circuits and\n  Systems Conference",
    "pdf_url": "http://arxiv.org/pdf/2409.04428v2",
    "published_date": "2024-09-06 17:48:44 UTC",
    "updated_date": "2024-09-26 07:53:04 UTC"
  },
  {
    "arxiv_id": "2409.04421v2",
    "title": "RLPF: Reinforcement Learning from Prediction Feedback for User Summarization with LLMs",
    "authors": [
      "Jiaxing Wu",
      "Lin Ning",
      "Luyang Liu",
      "Harrison Lee",
      "Neo Wu",
      "Chao Wang",
      "Sushant Prakash",
      "Shawn O'Banion",
      "Bradley Green",
      "Jun Xie"
    ],
    "abstract": "LLM-powered personalization agent systems employ Large Language Models (LLMs)\nto predict users' behavior from their past activities. However, their\neffectiveness often hinges on the ability to effectively leverage extensive,\nlong user historical data due to its inherent noise and length of such data.\nExisting pretrained LLMs may generate summaries that are concise but lack the\nnecessary context for downstream tasks, hindering their utility in\npersonalization systems. To address these challenges, we introduce\nReinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to\ngenerate concise, human-readable user summaries that are optimized for\ndownstream task performance. By maximizing the usefulness of the generated\nsummaries, RLPF effectively distills extensive user history data while\npreserving essential information for downstream tasks. Our empirical evaluation\ndemonstrates significant improvements in both extrinsic downstream task utility\nand intrinsic summary quality, surpassing baseline methods by up to 22% on\ndownstream task performance and achieving an up to 84.59% win rate on\nFactuality, Abstractiveness, and Readability. RLPF also achieves a remarkable\n74% reduction in context length while improving performance on 16 out of 19\nunseen tasks and/or datasets, showcasing its generalizability. This approach\noffers a promising solution for enhancing LLM personalization by effectively\ntransforming long, noisy user histories into informative and human-readable\nrepresentations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.04421v2",
    "published_date": "2024-09-06 17:30:45 UTC",
    "updated_date": "2025-01-17 01:11:16 UTC"
  },
  {
    "arxiv_id": "2409.04415v1",
    "title": "Improved Parallel Algorithm for Non-Monotone Submodular Maximization under Knapsack Constraint",
    "authors": [
      "Tan D. Tran",
      "Canh V. Pham",
      "Dung T. K. Ha",
      "Phuong N. H. Pham"
    ],
    "abstract": "This work proposes an efficient parallel algorithm for non-monotone\nsubmodular maximization under a knapsack constraint problem over the ground set\nof size $n$. Our algorithm improves the best approximation factor of the\nexisting parallel one from $8+\\epsilon$ to $7+\\epsilon$ with $O(\\log n)$\nadaptive complexity.\n  The key idea of our approach is to create a new alternate threshold\nalgorithmic framework. This strategy alternately constructs two disjoint\ncandidate solutions within a constant number of sequence rounds. Then, the\nalgorithm boosts solution quality without sacrificing the adaptive complexity.\nExtensive experimental studies on three applications, Revenue Maximization,\nImage Summarization, and Maximum Weighted Cut, show that our algorithm not only\nsignificantly increases solution quality but also requires comparative\nadaptivity to state-of-the-art algorithms.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "In Proceedings of the Thirty-Third International Joint Conference on\n  Artificial Intelligence (IJCAI), Main Track",
    "pdf_url": "http://arxiv.org/pdf/2409.04415v1",
    "published_date": "2024-09-06 17:17:52 UTC",
    "updated_date": "2024-09-06 17:17:52 UTC"
  },
  {
    "arxiv_id": "2409.04410v3",
    "title": "Open-MAGVIT2: An Open-Source Project Toward Democratizing Auto-regressive Visual Generation",
    "authors": [
      "Zhuoyan Luo",
      "Fengyuan Shi",
      "Yixiao Ge",
      "Yujiu Yang",
      "Limin Wang",
      "Ying Shan"
    ],
    "abstract": "The Open-MAGVIT2 project produces an open-source replication of Google's\nMAGVIT-v2 tokenizer, a tokenizer with a super-large codebook (i.e., $2^{18}$\ncodes), and achieves the state-of-the-art reconstruction performance on\nImageNet and UCF benchmarks. We also provide a tokenizer pre-trained on\nlarge-scale data, significantly outperforming Cosmos on zero-shot benchmarks\n(1.93 vs. 0.78 rFID on ImageNet original resolution). Furthermore, we explore\nits application in plain auto-regressive models to validate scalability\nproperties, producing a family of auto-regressive image generation models\nranging from 300M to 1.5B. To assist auto-regressive models in predicting with\na super-large vocabulary, we factorize it into two sub-vocabulary of different\nsizes by asymmetric token factorization, and further introduce ``next sub-token\nprediction'' to enhance sub-token interaction for better generation quality. We\nrelease all models and codes to foster innovation and creativity in the field\nof auto-regressive visual generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04410v3",
    "published_date": "2024-09-06 17:14:53 UTC",
    "updated_date": "2025-02-09 08:59:19 UTC"
  },
  {
    "arxiv_id": "2409.04398v3",
    "title": "HiSC4D: Human-centered interaction and 4D Scene Capture in Large-scale Space Using Wearable IMUs and LiDAR",
    "authors": [
      "Yudi Dai",
      "Zhiyong Wang",
      "Xiping Lin",
      "Chenglu Wen",
      "Lan Xu",
      "Siqi Shen",
      "Yuexin Ma",
      "Cheng Wang"
    ],
    "abstract": "We introduce HiSC4D, a novel Human-centered interaction and 4D Scene Capture\nmethod, aimed at accurately and efficiently creating a dynamic digital world,\ncontaining large-scale indoor-outdoor scenes, diverse human motions, rich\nhuman-human interactions, and human-environment interactions. By utilizing\nbody-mounted IMUs and a head-mounted LiDAR, HiSC4D can capture egocentric human\nmotions in unconstrained space without the need for external devices and\npre-built maps. This affords great flexibility and accessibility for\nhuman-centered interaction and 4D scene capturing in various environments.\nTaking into account that IMUs can capture human spatially unrestricted poses\nbut are prone to drifting for long-period using, and while LiDAR is stable for\nglobal localization but rough for local positions and orientations, HiSC4D\nemploys a joint optimization method, harmonizing all sensors and utilizing\nenvironment cues, yielding promising results for long-term capture in large\nscenes. To promote research of egocentric human interaction in large scenes and\nfacilitate downstream tasks, we also present a dataset, containing 8 sequences\nin 4 large scenes (200 to 5,000 $m^2$), providing 36k frames of accurate 4D\nhuman motions with SMPL annotations and dynamic scenes, 31k frames of cropped\nhuman point clouds, and scene mesh of the environment. A variety of scenarios,\nsuch as the basketball gym and commercial street, alongside challenging human\nmotions, such as daily greeting, one-on-one basketball playing, and tour\nguiding, demonstrate the effectiveness and the generalization ability of\nHiSC4D. The dataset and code will be publicated on\nwww.lidarhumanmotion.net/hisc4d available for research purposes.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 10 figures, Jornal",
    "pdf_url": "http://arxiv.org/pdf/2409.04398v3",
    "published_date": "2024-09-06 16:43:04 UTC",
    "updated_date": "2024-09-14 15:48:40 UTC"
  },
  {
    "arxiv_id": "2409.04388v5",
    "title": "Question-Answering Dense Video Events",
    "authors": [
      "Hangyu Qin",
      "Junbin Xiao",
      "Angela Yao"
    ],
    "abstract": "This paper presents question-answering on dense video events, a novel task\nthat answers and grounds dense-event questions in long videos, thus challenging\nMLLMs to faithfully comprehend and reason about multiple events over extended\nperiods of time. To facilitate the study, we construct DeVE-QA -- a dataset\nfeaturing 78K questions about 26K events on 10.6K long videos. Our benchmarking\nshows that state-of-the-art MLLMs struggle on DeVE-QA. For improvement, we\npropose DeVi, a novel training-free MLLM approach that highlights a\nhierarchical captioning module, a temporal event memory module, and a\nself-consistency checking module to respectively detect, contextualize and\nmemorize, and ground dense-events in long videos for question answering.\nExtensive experiments show that DeVi is superior at answering dense-event\nquestions and grounding relevant video moments. Compared with existing MLLMs,\nit achieves a notable increase of 4.8% and 2.1% for G(round)QA accuracy on\nDeVE-QA and NExT-GQA, respectively. Data and code are available at\nhttps://github.com/QHUni/DeVE-QA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to SIGIR'25",
    "pdf_url": "http://arxiv.org/pdf/2409.04388v5",
    "published_date": "2024-09-06 16:27:52 UTC",
    "updated_date": "2025-05-16 08:24:31 UTC"
  },
  {
    "arxiv_id": "2410.00350v1",
    "title": "Efficient Training of Large Vision Models via Advanced Automated Progressive Learning",
    "authors": [
      "Changlin Li",
      "Jiawei Zhang",
      "Sihao Lin",
      "Zongxin Yang",
      "Junwei Liang",
      "Xiaodan Liang",
      "Xiaojun Chang"
    ],
    "abstract": "The rapid advancements in Large Vision Models (LVMs), such as Vision\nTransformers (ViTs) and diffusion models, have led to an increasing demand for\ncomputational resources, resulting in substantial financial and environmental\ncosts. This growing challenge highlights the necessity of developing efficient\ntraining methods for LVMs. Progressive learning, a training strategy in which\nmodel capacity gradually increases during training, has shown potential in\naddressing these challenges. In this paper, we present an advanced automated\nprogressive learning (AutoProg) framework for efficient training of LVMs. We\nbegin by focusing on the pre-training of LVMs, using ViTs as a case study, and\npropose AutoProg-One, an AutoProg scheme featuring momentum growth (MoGrow) and\na one-shot growth schedule search. Beyond pre-training, we extend our approach\nto tackle transfer learning and fine-tuning of LVMs. We expand the scope of\nAutoProg to cover a wider range of LVMs, including diffusion models. First, we\nintroduce AutoProg-Zero, by enhancing the AutoProg framework with a novel\nzero-shot unfreezing schedule search, eliminating the need for one-shot\nsupernet training. Second, we introduce a novel Unique Stage Identifier (SID)\nscheme to bridge the gap during network growth. These innovations, integrated\nwith the core principles of AutoProg, offer a comprehensive solution for\nefficient training across various LVM scenarios. Extensive experiments show\nthat AutoProg accelerates ViT pre-training by up to 1.85x on ImageNet and\naccelerates fine-tuning of diffusion models by up to 2.86x, with comparable or\neven higher performance. This work provides a robust and scalable approach to\nefficient training of LVMs, with potential applications in a wide range of\nvision tasks. Code: https://github.com/changlin31/AutoProg-Zero",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Code: https://github.com/changlin31/AutoProg-Zero. arXiv admin note:\n  substantial text overlap with arXiv:2203.14509",
    "pdf_url": "http://arxiv.org/pdf/2410.00350v1",
    "published_date": "2024-09-06 16:24:24 UTC",
    "updated_date": "2024-09-06 16:24:24 UTC"
  },
  {
    "arxiv_id": "2409.09069v1",
    "title": "Temporal Many-valued Conditional Logics: a Preliminary Report",
    "authors": [
      "Mario Alviano",
      "Laura Giordano",
      "Daniele Theseider Dupr√©"
    ],
    "abstract": "In this paper we propose a many-valued temporal conditional logic. We start\nfrom a many-valued logic with typicality, and extend it with the temporal\noperators of the Linear Time Temporal Logic (LTL), thus providing a formalism\nwhich is able to capture the dynamics of a system, trough strict and defeasible\ntemporal properties. We also consider an instantiation of the formalism for\ngradual argumentation.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "68T27",
      "I.2.4"
    ],
    "primary_category": "cs.LO",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.09069v1",
    "published_date": "2024-09-06 16:23:31 UTC",
    "updated_date": "2024-09-06 16:23:31 UTC"
  },
  {
    "arxiv_id": "2409.04368v2",
    "title": "The Impact of Scanner Domain Shift on Deep Learning Performance in Medical Imaging: an Experimental Study",
    "authors": [
      "Brian Guo",
      "Darui Lu",
      "Gregory Szumel",
      "Rongze Gui",
      "Tingyu Wang",
      "Nicholas Konz",
      "Maciej A. Mazurowski"
    ],
    "abstract": "Purpose: Medical images acquired using different scanners and protocols can\ndiffer substantially in their appearance. This phenomenon, scanner domain\nshift, can result in a drop in the performance of deep neural networks which\nare trained on data acquired by one scanner and tested on another. This\nsignificant practical issue is well-acknowledged, however, no systematic study\nof the issue is available across different modalities and diagnostic tasks.\nMaterials and Methods: In this paper, we present a broad experimental study\nevaluating the impact of scanner domain shift on convolutional neural network\nperformance for different automated diagnostic tasks. We evaluate this\nphenomenon in common radiological modalities, including X-ray, CT, and MRI.\nResults: We find that network performance on data from a different scanner is\nalmost always worse than on same-scanner data, and we quantify the degree of\nperformance drop across different datasets. Notably, we find that this drop is\nmost severe for MRI, moderate for X-ray, and quite small for CT, on average,\nwhich we attribute to the standardized nature of CT acquisition systems which\nis not present in MRI or X-ray. We also study how injecting varying amounts of\ntarget domain data into the training set, as well as adding noise to the\ntraining data, helps with generalization. Conclusion: Our results provide\nextensive experimental evidence and quantification of the extent of performance\ndrop caused by scanner domain shift in deep learning across different\nmodalities, with the goal of guiding the future development of robust deep\nlearning models for medical image analysis.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04368v2",
    "published_date": "2024-09-06 15:59:30 UTC",
    "updated_date": "2024-10-02 13:51:37 UTC"
  },
  {
    "arxiv_id": "2409.04367v3",
    "title": "Algorithm Configuration for Structured Pfaffian Settings",
    "authors": [
      "Maria-Florina Balcan",
      "Anh Tuan Nguyen",
      "Dravyansh Sharma"
    ],
    "abstract": "Data-driven algorithm design automatically adapts algorithms to specific\napplication domains, achieving better performance. In the context of\nparameterized algorithms, this approach involves tuning the algorithm's\nhyperparameters using problem instances drawn from the problem distribution of\nthe target application domain. This can be achieved by maximizing empirical\nutilities that measure the algorithms' performance as a function of their\nhyperparameters, using problem instances. While empirical evidence supports the\neffectiveness of data-driven algorithm design, providing theoretical guarantees\nfor several parameterized families remains challenging. This is due to the\nintricate behaviors of their corresponding utility functions, which typically\nadmit piecewise discontinuous structures. In this work, we present refined\nframeworks for providing learning guarantees for parameterized data-driven\nalgorithm design problems in both distributional and online learning settings.\nFor the distributional learning setting, we introduce the \\textit{Pfaffian GJ\nframework}, an extension of the classical \\textit{GJ framework}, that is\ncapable of providing learning guarantees for function classes for which the\ncomputation involves Pfaffian functions. Unlike the GJ framework, which is\nlimited to function classes with computation characterized by rational\nfunctions, our proposed framework can deal with function classes involving\nPfaffian functions, which are much more general and widely applicable. We then\nshow that for many parameterized algorithms of interest, their utility function\npossesses a \\textit{refined piecewise structure}, which automatically\ntranslates to learning guarantees using our proposed framework.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04367v3",
    "published_date": "2024-09-06 15:58:20 UTC",
    "updated_date": "2024-11-12 22:53:09 UTC"
  },
  {
    "arxiv_id": "2409.05907v3",
    "title": "Programming Refusal with Conditional Activation Steering",
    "authors": [
      "Bruce W. Lee",
      "Inkit Padhi",
      "Karthikeyan Natesan Ramamurthy",
      "Erik Miehling",
      "Pierre Dognin",
      "Manish Nagireddy",
      "Amit Dhurandhar"
    ],
    "abstract": "LLMs have shown remarkable capabilities, but precisely controlling their\nresponse behavior remains challenging. Existing activation steering methods\nalter LLM behavior indiscriminately, limiting their practical applicability in\nsettings where selective responses are essential, such as content moderation or\ndomain-specific assistants. In this paper, we propose Conditional Activation\nSteering (CAST), which analyzes LLM activation patterns during inference to\nselectively apply or withhold activation steering based on the input context.\nOur method is based on the observation that different categories of prompts\nactivate distinct patterns in the model's hidden states. Using CAST, one can\nsystematically control LLM behavior with rules like \"if input is about hate\nspeech or adult content, then refuse\" or \"if input is not about legal advice,\nthen refuse.\" This allows for selective modification of responses to specific\ncontent while maintaining normal responses to other content, all without\nrequiring weight optimization. We release an open-source implementation of our\nframework at github.com/IBM/activation-steering .",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025, Spotlight",
    "pdf_url": "http://arxiv.org/pdf/2409.05907v3",
    "published_date": "2024-09-06 15:47:40 UTC",
    "updated_date": "2025-02-17 20:23:19 UTC"
  },
  {
    "arxiv_id": "2409.04360v1",
    "title": "Connectivity-Inspired Network for Context-Aware Recognition",
    "authors": [
      "Gianluca Carloni",
      "Sara Colantonio"
    ],
    "abstract": "The aim of this paper is threefold. We inform the AI practitioner about the\nhuman visual system with an extensive literature review; we propose a novel\nbiologically motivated neural network for image classification; and, finally,\nwe present a new plug-and-play module to model context awareness. We focus on\nthe effect of incorporating circuit motifs found in biological brains to\naddress visual recognition. Our convolutional architecture is inspired by the\nconnectivity of human cortical and subcortical streams, and we implement\nbottom-up and top-down modulations that mimic the extensive afferent and\nefferent connections between visual and cognitive areas. Our Contextual\nAttention Block is simple and effective and can be integrated with any\nfeed-forward neural network. It infers weights that multiply the feature maps\naccording to their causal influence on the scene, modeling the co-occurrence of\ndifferent objects in the image. We place our module at different bottlenecks to\ninfuse a hierarchical context awareness into the model. We validated our\nproposals through image classification experiments on benchmark data and found\na consistent improvement in performance and the robustness of the produced\nexplanations via class activation. Our code is available at\nhttps://github.com/gianlucarloni/CoCoReco.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV",
      "I.2; I.4; I.5; J.3; J.6"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024 - HCV Workshop, Accepted for presentation, Submitted\n  Manuscript Version (adapted to include author names, Acknowledgements, and\n  reference DOIs): the version of the manuscript improved after peer review\n  will appear in the Proceedings later",
    "pdf_url": "http://arxiv.org/pdf/2409.04360v1",
    "published_date": "2024-09-06 15:42:10 UTC",
    "updated_date": "2024-09-06 15:42:10 UTC"
  },
  {
    "arxiv_id": "2409.04341v1",
    "title": "Towards Fine-Grained Webpage Fingerprinting at Scale",
    "authors": [
      "Xiyuan Zhao",
      "Xinhao Deng",
      "Qi Li",
      "Yunpeng Liu",
      "Zhuotao Liu",
      "Kun Sun",
      "Ke Xu"
    ],
    "abstract": "Website Fingerprinting (WF) attacks can effectively identify the websites\nvisited by Tor clients via analyzing encrypted traffic patterns. Existing\nattacks focus on identifying different websites, but their accuracy\ndramatically decreases when applied to identify fine-grained webpages,\nespecially when distinguishing among different subpages of the same website.\nWebPage Fingerprinting (WPF) attacks face the challenges of highly similar\ntraffic patterns and a much larger scale of webpages. Furthermore, clients\noften visit multiple webpages concurrently, increasing the difficulty of\nextracting the traffic patterns of each webpage from the obfuscated traffic. In\nthis paper, we propose Oscar, a WPF attack based on multi-label metric learning\nthat identifies different webpages from obfuscated traffic by transforming the\nfeature space. Oscar can extract the subtle differences among various webpages,\neven those with similar traffic patterns. In particular, Oscar combines\nproxy-based and sample-based metric learning losses to extract webpage features\nfrom obfuscated traffic and identify multiple webpages. We prototype Oscar and\nevaluate its performance using traffic collected from 1,000 monitored webpages\nand over 9,000 unmonitored webpages in the real world. Oscar demonstrates an\n88.6% improvement in the multi-label metric Recall@5 compared to the\nstate-of-the-art attacks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "To appear in the Proceedings of The ACM Conference on Computer and\n  Communications Security (CCS), 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.04341v1",
    "published_date": "2024-09-06 15:21:00 UTC",
    "updated_date": "2024-09-06 15:21:00 UTC"
  },
  {
    "arxiv_id": "2409.04340v1",
    "title": "AGR: Age Group fairness Reward for Bias Mitigation in LLMs",
    "authors": [
      "Shuirong Cao",
      "Ruoxi Cheng",
      "Zhiqiang Wang"
    ],
    "abstract": "LLMs can exhibit age biases, resulting in unequal treatment of individuals\nacross age groups. While much research has addressed racial and gender biases,\nage bias remains little explored. The scarcity of instruction-tuning and\npreference datasets for age bias hampers its detection and measurement, and\nexisting fine-tuning methods seldom address age-related fairness. In this\npaper, we construct age bias preference datasets and instruction-tuning\ndatasets for RLHF. We introduce ARG, an age fairness reward to reduce\ndifferences in the response quality of LLMs across different age groups.\nExtensive experiments demonstrate that this reward significantly improves\nresponse accuracy and reduces performance disparities across age groups. Our\nsource code and datasets are available at the anonymous\n\\href{https://anonymous.4open.science/r/FairRLHF-D445/readme.md}{link}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "The first two authors contributed equally to this work. Corresponding\n  to Zhiqiang Wang. ACKNOWLEDGMENT: we would like to thank the computing\n  resources support from the State Key Laboratory of New Computer Software\n  Technologies at Nanjing University",
    "pdf_url": "http://arxiv.org/pdf/2409.04340v1",
    "published_date": "2024-09-06 15:18:12 UTC",
    "updated_date": "2024-09-06 15:18:12 UTC"
  },
  {
    "arxiv_id": "2409.04495v1",
    "title": "Learning to Solve Combinatorial Optimization under Positive Linear Constraints via Non-Autoregressive Neural Networks",
    "authors": [
      "Runzhong Wang",
      "Yang Li",
      "Junchi Yan",
      "Xiaokang Yang"
    ],
    "abstract": "Combinatorial optimization (CO) is the fundamental problem at the\nintersection of computer science, applied mathematics, etc. The inherent\nhardness in CO problems brings up challenge for solving CO exactly, making\ndeep-neural-network-based solvers a research frontier. In this paper, we design\na family of non-autoregressive neural networks to solve CO problems under\npositive linear constraints with the following merits. First, the positive\nlinear constraint covers a wide range of CO problems, indicating that our\napproach breaks the generality bottleneck of existing non-autoregressive\nnetworks. Second, compared to existing autoregressive neural network solvers,\nour non-autoregressive networks have the advantages of higher efficiency and\npreserving permutation invariance. Third, our offline unsupervised learning has\nlower demand on high-quality labels, getting rid of the demand of optimal\nlabels in supervised learning. Fourth, our online differentiable search method\nsignificantly improves the generalizability of our neural network solver to\nunseen problems. We validate the effectiveness of this framework in solving\nrepresentative CO problems including facility location, max-set covering, and\ntraveling salesman problem. Our non-autoregressive neural solvers are\ncompetitive to and can be even superior to state-of-the-art solvers such as\nSCIP and Gurobi, especially when both efficiency and efficacy are considered.\nCode is available at https://github.com/Thinklab-SJTU/NAR-CO-Solver",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "English version of the same paper published on Scientia Sinica\n  Informationis",
    "pdf_url": "http://arxiv.org/pdf/2409.04495v1",
    "published_date": "2024-09-06 14:58:31 UTC",
    "updated_date": "2024-09-06 14:58:31 UTC"
  },
  {
    "arxiv_id": "2409.13709v1",
    "title": "Column Vocabulary Association (CVA): semantic interpretation of dataless tables",
    "authors": [
      "Margherita Martorana",
      "Xueli Pan",
      "Benno Kruit",
      "Tobias Kuhn",
      "Jacco van Ossenbruggen"
    ],
    "abstract": "Traditional Semantic Table Interpretation (STI) methods rely primarily on the\nunderlying table data to create semantic annotations. This year's SemTab\nchallenge introduced the ``Metadata to KG'' track, which focuses on performing\nSTI by using only metadata information, without access to the underlying data.\nIn response to this new challenge, we introduce a new term: Column Vocabulary\nAssociation (CVA). This term refers to the task of semantic annotation of\ncolumn headers solely based on metadata information. In this study, we evaluate\nthe performance of various methods in executing the CVA task, including a Large\nLanguage Models (LLMs) and Retrieval Augmented Generation (RAG) approach, as\nwell as a more traditional similarity approach with SemanticBERT. Our\nmethodology uses a zero-shot setting, with no pretraining or examples passed to\nthe Large Language Models (LLMs), as we aim to avoid a domain-specific setting.\n  We investigate a total of 7 different LLMs, of which three commercial GPT\nmodels (i.e. gpt-3.5-turbo-0.125, gpt-4o and gpt-4-turbo) and four open source\nmodels (i.e. llama3-80b, llama3-7b, gemma-7b and mixtral-8x7b). We integrate\nthis models with RAG systems, and we explore how variations in temperature\nsettings affect performances. Moreover, we continue our investigation by\nperforming the CVA task utilizing SemanticBERT, analyzing how various metadata\ninformation influence its performance.\n  Initial findings indicate that LLMs generally perform well at temperatures\nbelow 1.0, achieving an accuracy of 100\\% in certain cases. Nevertheless, our\ninvestigation also reveal that the nature of the data significantly influences\nCVA task outcomes. In fact, in cases where the input data and glossary are\nrelated (for example by being created by the same organizations) traditional\nmethods appear to surpass the performance of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13709v1",
    "published_date": "2024-09-06 14:58:30 UTC",
    "updated_date": "2024-09-06 14:58:30 UTC"
  },
  {
    "arxiv_id": "2409.04318v2",
    "title": "Learning vs Retrieval: The Role of In-Context Examples in Regression with Large Language Models",
    "authors": [
      "Aliakbar Nafar",
      "Kristen Brent Venable",
      "Parisa Kordjamshidi"
    ],
    "abstract": "Generative Large Language Models (LLMs) are capable of being in-context\nlearners. However, the underlying mechanism of in-context learning (ICL) is\nstill a major research question, and experimental research results about how\nmodels exploit ICL are not always consistent. In this work, we propose a\nframework for evaluating in-context learning mechanisms, which we claim are a\ncombination of retrieving internal knowledge and learning from in-context\nexamples by focusing on regression tasks. First, we show that LLMs can solve\nreal-world regression problems and then design experiments to measure the\nextent to which the LLM retrieves its internal knowledge versus learning from\nin-context examples. We argue that this process lies on a spectrum between\nthese two extremes. We provide an in-depth analysis of the degrees to which\nthese mechanisms are triggered depending on various factors, such as prior\nknowledge about the tasks and the type and richness of the information provided\nby the in-context examples. We employ three LLMs and utilize multiple datasets\nto corroborate the robustness of our findings. Our results shed light on how to\nengineer prompts to leverage meta-learning from in-context examples and foster\nknowledge retrieval depending on the problem being addressed.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04318v2",
    "published_date": "2024-09-06 14:46:37 UTC",
    "updated_date": "2025-02-09 22:47:07 UTC"
  },
  {
    "arxiv_id": "2409.04306v1",
    "title": "Safe and Efficient Path Planning under Uncertainty via Deep Collision Probability Fields",
    "authors": [
      "Felix Herrmann",
      "Sebastian Zach",
      "Jacopo Banfi",
      "Jan Peters",
      "Georgia Chalvatzaki",
      "Davide Tateo"
    ],
    "abstract": "Estimating collision probabilities between robots and environmental obstacles\nor other moving agents is crucial to ensure safety during path planning. This\nis an important building block of modern planning algorithms in many\napplication scenarios such as autonomous driving, where noisy sensors perceive\nobstacles. While many approaches exist, they either provide too conservative\nestimates of the collision probabilities or are computationally intensive due\nto their sampling-based nature. To deal with these issues, we introduce Deep\nCollision Probability Fields, a neural-based approach for computing collision\nprobabilities of arbitrary objects with arbitrary unimodal uncertainty\ndistributions. Our approach relegates the computationally intensive estimation\nof collision probabilities via sampling at the training step, allowing for fast\nneural network inference of the constraints during planning. In extensive\nexperiments, we show that Deep Collision Probability Fields can produce\nreasonably accurate collision probabilities (up to 10^{-3}) for planning and\nthat our approach can be easily plugged into standard path planning approaches\nto plan safe paths on 2-D maps containing uncertain static and dynamic\nobstacles. Additional material, code, and videos are available at\nhttps://sites.google.com/view/ral-dcpf.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Preprint version of a paper accepted to the IEEE Robotics and\n  Automation Letters",
    "pdf_url": "http://arxiv.org/pdf/2409.04306v1",
    "published_date": "2024-09-06 14:28:41 UTC",
    "updated_date": "2024-09-06 14:28:41 UTC"
  },
  {
    "arxiv_id": "2409.13708v2",
    "title": "Towards Safe Multilingual Frontier AI",
    "authors": [
      "Art≈´rs Kanepajs",
      "Vladimir Ivanov",
      "Richard Moulange"
    ],
    "abstract": "Linguistically inclusive LLMs -- which maintain good performance regardless\nof the language with which they are prompted -- are necessary for the diffusion\nof AI benefits around the world. Multilingual jailbreaks that rely on language\ntranslation to evade safety measures undermine the safe and inclusive\ndeployment of AI systems. We provide policy recommendations to enhance the\nmultilingual capabilities of AI while mitigating the risks of multilingual\njailbreaks. We examine how a language's level of resourcing relates to how\nvulnerable LLMs are to multilingual jailbreaks in that language. We do this by\ntesting five advanced AI models across 24 official languages of the EU.\nBuilding on prior research, we propose policy actions that align with the EU\nlegal landscape and institutional framework to address multilingual jailbreaks,\nwhile promoting linguistic inclusivity. These include mandatory assessments of\nmultilingual capabilities and vulnerabilities, public opinion research, and\nstate support for multilingual AI development. The measures aim to improve AI\nsafety and functionality through EU policy initiatives, guiding the\nimplementation of the EU AI Act and informing regulatory efforts of the\nEuropean AI Office.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages; 1 figure and 10 supplementary figures; Accepted (spotlight\n  presentation) at NeurIPS 2024 SoLaR workshop",
    "pdf_url": "http://arxiv.org/pdf/2409.13708v2",
    "published_date": "2024-09-06 14:26:18 UTC",
    "updated_date": "2024-10-29 11:14:46 UTC"
  },
  {
    "arxiv_id": "2409.04290v1",
    "title": "CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis",
    "authors": [
      "William Knottenbelt",
      "Zeyu Gao",
      "Rebecca Wray",
      "Woody Zhidong Zhang",
      "Jiashuai Liu",
      "Mireia Crispin-Ortuzar"
    ],
    "abstract": "Survival analysis is a branch of statistics used for modeling the time until\na specific event occurs and is widely used in medicine, engineering, finance,\nand many other fields. When choosing survival models, there is typically a\ntrade-off between performance and interpretability, where the highest\nperformance is achieved by black-box models based on deep learning. This is a\nmajor problem in fields such as medicine where practitioners are reluctant to\nblindly trust black-box models to make important patient decisions.\nKolmogorov-Arnold Networks (KANs) were recently proposed as an interpretable\nand accurate alternative to multi-layer perceptrons (MLPs). We introduce\nCoxKAN, a Cox proportional hazards Kolmogorov-Arnold Network for interpretable,\nhigh-performance survival analysis. We evaluate the proposed CoxKAN on 4\nsynthetic datasets and 9 real medical datasets. The synthetic experiments\ndemonstrate that CoxKAN accurately recovers interpretable symbolic formulae for\nthe hazard function, and effectively performs automatic feature selection.\nEvaluation on the 9 real datasets show that CoxKAN consistently outperforms the\nCox proportional hazards model and achieves performance that is superior or\ncomparable to that of tuned MLPs. Furthermore, we find that CoxKAN identifies\ncomplex interactions between predictor variables that would be extremely\ndifficult to recognise using existing survival methods, and automatically finds\nsymbolic formulae which uncover the precise effect of important biomarkers on\npatient risk.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04290v1",
    "published_date": "2024-09-06 13:59:58 UTC",
    "updated_date": "2024-09-06 13:59:58 UTC"
  },
  {
    "arxiv_id": "2409.04286v2",
    "title": "Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets",
    "authors": [
      "Desiree Heim",
      "Christian Jilek",
      "Adrian Ulges",
      "Andreas Dengel"
    ],
    "abstract": "Current publicly available knowledge work data collections lack diversity,\nextensive annotations, and contextual information about the users and their\ndocuments. These issues hinder objective and comparable data-driven evaluations\nand optimizations of knowledge work assistance systems. Due to the considerable\nresources needed to collect such data in real-life settings and the necessity\nof data censorship, collecting such a dataset appears nearly impossible. For\nthis reason, we propose a configurable, multi-agent knowledge work dataset\ngenerator. This system simulates collaborative knowledge work among agents\nproducing Large Language Model-generated documents and accompanying data\ntraces. Additionally, the generator captures all background information, given\nin its configuration or created during the simulation process, in a knowledge\ngraph. Finally, the resulting dataset can be utilized and shared without\nprivacy or confidentiality concerns.\n  This paper introduces our approach's design and vision and focuses on\ngenerating authentic knowledge work documents using Large Language Models. Our\nstudy involving human raters who assessed 53% of the generated and 74% of the\nreal documents as realistic demonstrates the potential of our approach.\nFurthermore, we analyze the authenticity criteria mentioned in the\nparticipants' comments and elaborate on potential improvements for identified\ncommon issues.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted and published (INFORMATIK Festival, Wiesbaden, 2024)",
    "pdf_url": "http://arxiv.org/pdf/2409.04286v2",
    "published_date": "2024-09-06 13:53:28 UTC",
    "updated_date": "2024-10-24 08:32:54 UTC"
  },
  {
    "arxiv_id": "2409.04272v2",
    "title": "Cycle Pixel Difference Network for Crisp Edge Detection",
    "authors": [
      "Changsong Liu",
      "Wei Zhang",
      "Yanyan Liu",
      "Mingyang Li",
      "Wenlin Li",
      "Yimeng Fan",
      "Xiangnan Bai",
      "Liang Zhang"
    ],
    "abstract": "Edge detection, as a fundamental task in computer vision, has garnered\nincreasing attention. The advent of deep learning has significantly advanced\nthis field. However, recent deep learning-based methods generally face two\nsignificant issues: 1) reliance on large-scale pre-trained weights, and 2)\ngeneration of thick edges. We construct a U-shape encoder-decoder model named\nCPD-Net that successfully addresses these two issues simultaneously. In\nresponse to issue 1), we propose a novel cycle pixel difference convolution\n(CPDC), which effectively integrates edge prior knowledge with modern\nconvolution operations, consequently successfully eliminating the dependence on\nlarge-scale pre-trained weights. As for issue 2), we construct a multi-scale\ninformation enhancement module (MSEM) and a dual residual connection-based\n(DRC) decoder to enhance the edge location ability of the model, thereby\ngenerating crisp and clean contour maps. Comprehensive experiments conducted on\nfour standard benchmarks demonstrate that our method achieves competitive\nperformance on the BSDS500 dataset (ODS=0.813 and AC=0.352), NYUD-V2 (ODS=0.760\nand AC=0.223), BIPED dataset (ODS=0.898 and AC=0.426), and CID (ODS=0.59). Our\napproach provides a novel perspective for addressing these challenges in edge\ndetection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04272v2",
    "published_date": "2024-09-06 13:28:05 UTC",
    "updated_date": "2024-12-19 15:02:37 UTC"
  },
  {
    "arxiv_id": "2409.04267v1",
    "title": "An overview of domain-specific foundation model: key technologies, applications and challenges",
    "authors": [
      "Haolong Chen",
      "Hanzhi Chen",
      "Zijian Zhao",
      "Kaifeng Han",
      "Guangxu Zhu",
      "Yichen Zhao",
      "Ying Du",
      "Wei Xu",
      "Qingjiang Shi"
    ],
    "abstract": "The impressive performance of ChatGPT and other foundation-model-based\nproducts in human language understanding has prompted both academia and\nindustry to explore how these models can be tailored for specific industries\nand application scenarios. This process, known as the customization of\ndomain-specific foundation models, addresses the limitations of general-purpose\nmodels, which may not fully capture the unique patterns and requirements of\ndomain-specific data. Despite its importance, there is a notable lack of\ncomprehensive overview papers on building domain-specific foundation models,\nwhile numerous resources exist for general-purpose models. To bridge this gap,\nthis article provides a timely and thorough overview of the methodology for\ncustomizing domain-specific foundation models. It introduces basic concepts,\noutlines the general architecture, and surveys key methods for constructing\ndomain-specific models. Furthermore, the article discusses various domains that\ncan benefit from these specialized models and highlights the challenges ahead.\nThrough this overview, we aim to offer valuable guidance and reference for\nresearchers and practitioners from diverse fields to develop their own\ncustomized foundation models.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04267v1",
    "published_date": "2024-09-06 13:24:22 UTC",
    "updated_date": "2024-09-06 13:24:22 UTC"
  },
  {
    "arxiv_id": "2409.13707v1",
    "title": "Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support",
    "authors": [
      "Paulina Toro Isaza",
      "Michael Nidd",
      "Noah Zheutlin",
      "Jae-wook Ahn",
      "Chidansh Amitkumar Bhatt",
      "Yu Deng",
      "Ruchi Mahindru",
      "Martin Franz",
      "Hans Florian",
      "Salim Roukos"
    ],
    "abstract": "Clients wishing to implement generative AI in the domain of IT Support and\nAIOps face two critical issues: domain coverage and model size constraints due\nto model choice limitations. Clients might choose to not use larger proprietary\nmodels such as GPT-4 due to cost and privacy concerns and so are limited to\nsmaller models with potentially less domain coverage that do not generalize to\nthe client's domain. Retrieval augmented generation is a common solution that\naddresses both of these issues: a retrieval system first retrieves the\nnecessary domain knowledge which a smaller generative model leverages as\ncontext for generation. We present a system developed for a client in the IT\nSupport domain for support case solution recommendation that combines retrieval\naugmented generation (RAG) for answer generation with an encoder-only model for\nclassification and a generative large language model for query generation. We\ncover architecture details, data collection and annotation, development journey\nand preliminary validations, expected final deployment process and evaluation\nplans, and finally lessons learned.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "7 pages, 3 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.13707v1",
    "published_date": "2024-09-06 13:06:29 UTC",
    "updated_date": "2024-09-06 13:06:29 UTC"
  },
  {
    "arxiv_id": "2409.04249v2",
    "title": "Hermes: Memory-Efficient Pipeline Inference for Large Models on Edge Devices",
    "authors": [
      "Xueyuan Han",
      "Zinuo Cai",
      "Yichu Zhang",
      "Chongxin Fan",
      "Junhan Liu",
      "Ruhui Ma",
      "Rajkumar Buyya"
    ],
    "abstract": "The application of Transformer-based large models has achieved numerous\nsuccess in recent years. However, the exponential growth in the parameters of\nlarge models introduces formidable memory challenge for edge deployment. Prior\nworks to address this challenge mainly focus on optimizing the model structure\nand adopting memory swapping methods. However, the former reduces the inference\naccuracy, and the latter raises the inference latency. This paper introduces\nPIPELOAD, a novel memory-efficient pipeline execution mechanism. It reduces\nmemory usage by incorporating dynamic memory management and minimizes inference\nlatency by employing parallel model loading. Based on PIPELOAD mechanism, we\npresent Hermes, a framework optimized for large model inference on edge\ndevices. We evaluate Hermes on Transformer-based models of different sizes. Our\nexperiments illustrate that Hermes achieves up to 4.24 X increase in inference\nspeed and 86.7% lower memory consumption than the state-of-the-art pipeline\nmechanism for BERT and ViT models, 2.58 X increase in inference speed and 90.3%\nlower memory consumption for GPT-style models.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "Accepted by the 42nd IEEE International Conference on Computer Design\n  (ICCD 2024)",
    "pdf_url": "http://arxiv.org/pdf/2409.04249v2",
    "published_date": "2024-09-06 12:55:49 UTC",
    "updated_date": "2024-09-09 18:25:01 UTC"
  },
  {
    "arxiv_id": "2409.04244v1",
    "title": "WarpAdam: A new Adam optimizer based on Meta-Learning approach",
    "authors": [
      "Chengxi Pan",
      "Junshang Chen",
      "Jingrui Ye"
    ],
    "abstract": "Optimal selection of optimization algorithms is crucial for training deep\nlearning models. The Adam optimizer has gained significant attention due to its\nefficiency and wide applicability. However, to enhance the adaptability of\noptimizers across diverse datasets, we propose an innovative optimization\nstrategy by integrating the 'warped gradient descend'concept from Meta Learning\ninto the Adam optimizer. In the conventional Adam optimizer, gradients are\nutilized to compute estimates of gradient mean and variance, subsequently\nupdating model parameters. Our approach introduces a learnable distortion\nmatrix, denoted as P, which is employed for linearly transforming gradients.\nThis transformation slightly adjusts gradients during each iteration, enabling\nthe optimizer to better adapt to distinct dataset characteristics. By learning\nan appropriate distortion matrix P, our method aims to adaptively adjust\ngradient information across different data distributions, thereby enhancing\noptimization performance. Our research showcases the potential of this novel\napproach through theoretical insights and empirical evaluations. Experimental\nresults across various tasks and datasets validate the superiority of our\noptimizer that integrates the 'warped gradient descend' concept in terms of\nadaptability. Furthermore, we explore effective strategies for training the\nadaptation matrix P and identify scenarios where this method can yield optimal\nresults. In summary, this study introduces an innovative approach that merges\nthe 'warped gradient descend' concept from Meta Learning with the Adam\noptimizer. By introducing a learnable distortion matrix P within the optimizer,\nwe aim to enhance the model's generalization capability across diverse data\ndistributions, thus opening up new possibilities in the field of deep learning\noptimization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04244v1",
    "published_date": "2024-09-06 12:51:10 UTC",
    "updated_date": "2024-09-06 12:51:10 UTC"
  },
  {
    "arxiv_id": "2409.15324v1",
    "title": "Cognitive phantoms in LLMs through the lens of latent variables",
    "authors": [
      "Sanne Peereboom",
      "Inga Schwabe",
      "Bennett Kleinberg"
    ],
    "abstract": "Large language models (LLMs) increasingly reach real-world applications,\nnecessitating a better understanding of their behaviour. Their size and\ncomplexity complicate traditional assessment methods, causing the emergence of\nalternative approaches inspired by the field of psychology. Recent studies\nadministering psychometric questionnaires to LLMs report human-like traits in\nLLMs, potentially influencing LLM behaviour. However, this approach suffers\nfrom a validity problem: it presupposes that these traits exist in LLMs and\nthat they are measurable with tools designed for humans. Typical procedures\nrarely acknowledge the validity problem in LLMs, comparing and interpreting\naverage LLM scores. This study investigates this problem by comparing latent\nstructures of personality between humans and three LLMs using two validated\npersonality questionnaires. Findings suggest that questionnaires designed for\nhumans do not validly measure similar constructs in LLMs, and that these\nconstructs may not exist in LLMs at all, highlighting the need for psychometric\nanalyses of LLM responses to avoid chasing cognitive phantoms.\n  Keywords: large language models, psychometrics, machine behaviour, latent\nvariable modeling, validity",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.15324v1",
    "published_date": "2024-09-06 12:42:35 UTC",
    "updated_date": "2024-09-06 12:42:35 UTC"
  },
  {
    "arxiv_id": "2409.04230v1",
    "title": "SPACE: A Python-based Simulator for Evaluating Decentralized Multi-Robot Task Allocation Algorithms",
    "authors": [
      "Inmo Jang"
    ],
    "abstract": "Swarm robotics explores the coordination of multiple robots to achieve\ncollective goals, with collective decision-making being a central focus. This\nprocess involves decentralized robots autonomously making local decisions and\ncommunicating them, which influences the overall emergent behavior. Testing\nsuch decentralized algorithms in real-world scenarios with hundreds or more\nrobots is often impractical, underscoring the need for effective simulation\ntools. We propose SPACE (Swarm Planning and Control Evaluation), a Python-based\nsimulator designed to support the research, evaluation, and comparison of\ndecentralized Multi-Robot Task Allocation (MRTA) algorithms. SPACE streamlines\ncore algorithmic development by allowing users to implement decision-making\nalgorithms as Python plug-ins, easily construct agent behavior trees via an\nintuitive GUI, and leverage built-in support for inter-agent communication and\nlocal task awareness. To demonstrate its practical utility, we implement and\nevaluate CBBA and GRAPE within the simulator, comparing their performance\nacross different metrics, particularly in scenarios with dynamically introduced\ntasks. This evaluation shows the usefulness of SPACE in conducting rigorous and\nstandardized comparisons of MRTA algorithms, helping to support future research\nin the field.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04230v1",
    "published_date": "2024-09-06 12:38:24 UTC",
    "updated_date": "2024-09-06 12:38:24 UTC"
  },
  {
    "arxiv_id": "2409.15323v1",
    "title": "Introducing ELLIPS: An Ethics-Centered Approach to Research on LLM-Based Inference of Psychiatric Conditions",
    "authors": [
      "Roberta Rocca",
      "Giada Pistilli",
      "Kritika Maheshwari",
      "Riccardo Fusaroli"
    ],
    "abstract": "As mental health care systems worldwide struggle to meet demand, there is\nincreasing focus on using language models to infer neuropsychiatric conditions\nor psychopathological traits from language production. Yet, so far, this\nresearch has only delivered solutions with limited clinical applicability, due\nto insufficient consideration of ethical questions crucial to ensuring the\nsynergy between possible applications and model design. To accelerate progress\ntowards clinically applicable models, our paper charts the ethical landscape of\nresearch on language-based inference of psychopathology and provides a\npractical tool for researchers to navigate it. We identify seven core ethical\nprinciples that should guide model development and deployment in this domain,\ntranslate them into ELLIPS, an ethical toolkit operationalizing these\nprinciples into questions that can guide researchers' choices with respect to\ndata selection, architectures, evaluation, and model deployment, and provide a\ncase study exemplifying its use. With this, we aim to facilitate the emergence\nof model technology with concrete potential for real-world applicability.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.15323v1",
    "published_date": "2024-09-06 12:27:38 UTC",
    "updated_date": "2024-09-06 12:27:38 UTC"
  },
  {
    "arxiv_id": "2409.04224v1",
    "title": "Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework",
    "authors": [
      "Daniel J. Tan",
      "Qianyi Xu",
      "Kay Choong See",
      "Dilruk Perera",
      "Mengling Feng"
    ],
    "abstract": "Multi-organ diseases present significant challenges due to their simultaneous\nimpact on multiple organ systems, necessitating complex and adaptive treatment\nstrategies. Despite recent advancements in AI-powered healthcare decision\nsupport systems, existing solutions are limited to individual organ systems.\nThey often ignore the intricate dependencies between organ system and thereby\nfails to provide holistic treatment recommendations that are useful in\npractice. We propose a novel hierarchical multi-agent reinforcement learning\n(HMARL) framework to address these challenges. This framework uses dedicated\nagents for each organ system, and model dynamic through explicit inter-agent\ncommunication channels, enabling coordinated treatment strategies across\norgans. Furthermore, we introduce a dual-layer state representation technique\nto contextualize patient conditions at various hierarchical levels, enhancing\nthe treatment accuracy and relevance. Through extensive qualitative and\nquantitative evaluations in managing sepsis (a complex multi-organ disease),\nour approach demonstrates its ability to learn effective treatment policies\nthat significantly improve patient survival rates. This framework marks a\nsubstantial advancement in clinical decision support systems, pioneering a\ncomprehensive approach for multi-organ treatment recommendations.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04224v1",
    "published_date": "2024-09-06 12:26:47 UTC",
    "updated_date": "2024-09-06 12:26:47 UTC"
  },
  {
    "arxiv_id": "2409.04196v2",
    "title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting Transformers",
    "authors": [
      "Lorenza Prospero",
      "Abdullah Hamdi",
      "Joao F. Henriques",
      "Christian Rupprecht"
    ],
    "abstract": "Reconstructing posed 3D human models from monocular images has important\napplications in the sports industry, including performance tracking, injury\nprevention and virtual training. In this work, we combine 3D human pose and\nshape estimation with 3D Gaussian Splatting (3DGS), a representation of the\nscene composed of a mixture of Gaussians. This allows training or fine-tuning a\nhuman model predictor on multi-view images alone, without 3D ground truth.\nPredicting such mixtures for a human from a single input image is challenging\ndue to self-occlusions and dependence on articulations, while also needing to\nretain enough flexibility to accommodate a variety of clothes and poses. Our\nkey observation is that the vertices of standardized human meshes (such as\nSMPL) can provide an adequate spatial density and approximate initial position\nfor the Gaussians. We can then train a transformer model to jointly predict\ncomparatively small adjustments to these positions, as well as the other 3DGS\nattributes and the SMPL parameters. We show empirically that this combination\n(using only multi-view supervision) can achieve near real-time inference of 3D\nhuman models from a single image without expensive diffusion models or 3D\npoints supervision, thus making it ideal for the sport industry at any level.\nMore importantly, rendering is an effective auxiliary objective to refine 3D\npose estimation by accounting for clothes and other geometric variations. The\ncode is available at https://github.com/prosperolo/GST.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Camera ready for CVSports workshop at CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.04196v2",
    "published_date": "2024-09-06 11:34:24 UTC",
    "updated_date": "2025-04-16 14:37:31 UTC"
  },
  {
    "arxiv_id": "2409.04194v2",
    "title": "Towards Privacy-Preserving Relational Data Synthesis via Probabilistic Relational Models",
    "authors": [
      "Malte Luttermann",
      "Ralf M√∂ller",
      "Mattis Hartwig"
    ],
    "abstract": "Probabilistic relational models provide a well-established formalism to\ncombine first-order logic and probabilistic models, thereby allowing to\nrepresent relationships between objects in a relational domain. At the same\ntime, the field of artificial intelligence requires increasingly large amounts\nof relational training data for various machine learning tasks. Collecting\nreal-world data, however, is often challenging due to privacy concerns, data\nprotection regulations, high costs, and so on. To mitigate these challenges,\nthe generation of synthetic data is a promising approach. In this paper, we\nsolve the problem of generating synthetic relational data via probabilistic\nrelational models. In particular, we propose a fully-fledged pipeline to go\nfrom relational database to probabilistic relational model, which can then be\nused to sample new synthetic relational data points from its underlying\nprobability distribution. As part of our proposed pipeline, we introduce a\nlearning algorithm to construct a probabilistic relational model from a given\nrelational database.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to the Proceedings of the 47th German Conference on\n  Artificial Intelligence (KI 2024)",
    "pdf_url": "http://arxiv.org/pdf/2409.04194v2",
    "published_date": "2024-09-06 11:24:25 UTC",
    "updated_date": "2024-10-02 17:01:58 UTC"
  },
  {
    "arxiv_id": "2409.04183v1",
    "title": "GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding",
    "authors": [
      "Ziyin Zhang",
      "Hang Yu",
      "Shijie Li",
      "Peng Di",
      "Jianguo Li",
      "Rui Wang"
    ],
    "abstract": "Programming languages possess rich semantic information such as data flow\nthat is represented by graphs and not available from the surface form of source\ncode. Recent code language models have scaled to billions of parameters, but\nmodel source code solely as text tokens while ignoring any other structural\ninformation. Conversely, models that do encode structural information of code\nmake modifications to the Transformer architecture, limiting their scale and\ncompatibility with pretrained LLMs. In this work, we take the best of both\nworlds with GALLa - Graph Aligned Large Language Model. GALLa utilizes graph\nneural networks and cross-modal alignment technologies to inject the structural\ninformation of code into LLMs as an auxiliary task during finetuning. This\nframework is both model-agnostic and task-agnostic, as it can be applied to any\ncode LLM for any code downstream task, and requires the structural graph data\nonly at training time from a corpus unrelated to the finetuning data, while\nincurring no cost at inference time over the baseline LLM. Experiments on five\ncode tasks with four different baseline LLMs ranging in size from 350M to 8B\nvalidate the effectiveness of GALLa, demonstrating consistent improvement over\nthe baseline, even for powerful models such as LLaMA3.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04183v1",
    "published_date": "2024-09-06 10:57:34 UTC",
    "updated_date": "2024-09-06 10:57:34 UTC"
  },
  {
    "arxiv_id": "2409.04180v2",
    "title": "The Prevalence of Neural Collapse in Neural Multivariate Regression",
    "authors": [
      "George Andriopoulos",
      "Zixuan Dong",
      "Li Guo",
      "Zifan Zhao",
      "Keith Ross"
    ],
    "abstract": "Recently it has been observed that neural networks exhibit Neural Collapse\n(NC) during the final stage of training for the classification problem. We\nempirically show that multivariate regression, as employed in imitation\nlearning and other applications, exhibits Neural Regression Collapse (NRC), a\nnew form of neural collapse: (NRC1) The last-layer feature vectors collapse to\nthe subspace spanned by the $n$ principal components of the feature vectors,\nwhere $n$ is the dimension of the targets (for univariate regression, $n=1$);\n(NRC2) The last-layer feature vectors also collapse to the subspace spanned by\nthe last-layer weight vectors; (NRC3) The Gram matrix for the weight vectors\nconverges to a specific functional form that depends on the covariance matrix\nof the targets. After empirically establishing the prevalence of (NRC1)-(NRC3)\nfor a variety of datasets and network architectures, we provide an explanation\nof these phenomena by modeling the regression task in the context of the\nUnconstrained Feature Model (UFM), in which the last layer feature vectors are\ntreated as free variables when minimizing the loss function. We show that when\nthe regularization parameters in the UFM model are strictly positive, then\n(NRC1)-(NRC3) also emerge as solutions in the UFM optimization problem. We also\nshow that if the regularization parameters are equal to zero, then there is no\ncollapse. To our knowledge, this is the first empirical and theoretical study\nof neural collapse in the context of regression. This extension is significant\nnot only because it broadens the applicability of neural collapse to a new\ncategory of problems but also because it suggests that the phenomena of neural\ncollapse could be a universal behavior in deep learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.04180v2",
    "published_date": "2024-09-06 10:45:58 UTC",
    "updated_date": "2024-10-30 02:32:21 UTC"
  },
  {
    "arxiv_id": "2409.04168v2",
    "title": "From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning Tasks",
    "authors": [
      "Andreas Stephan",
      "Dawei Zhu",
      "Matthias A√üenmacher",
      "Xiaoyu Shen",
      "Benjamin Roth"
    ],
    "abstract": "To reduce the need for human annotations, large language models (LLMs) have\nbeen proposed as judges of the quality of other candidate models. The\nperformance of LLM judges is typically evaluated by measuring the correlation\nwith human judgments on generative tasks such as summarization or machine\ntranslation. In contrast, we study LLM judges on mathematical reasoning tasks.\nThese tasks require multi-step reasoning, and the correctness of their\nsolutions is verifiable, enabling a more objective evaluation. We perform a\ndetailed performance analysis and find that easy samples are easy to judge, and\ndifficult samples are difficult to judge. Our analysis uncovers a strong\ncorrelation between judgment performance and the candidate model task\nperformance, indicating that judges tend to favor higher-quality models even if\ntheir answer is incorrect. As a consequence, we test whether we can predict the\nbehavior of LLM judges using simple features such as part-of-speech tags and\nfind that we can correctly predict 70%-75% of judgments. We conclude this study\nby analyzing practical use cases, showing that LLM judges consistently detect\nthe on-average better model but largely fail if we use them to improve task\nperformance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04168v2",
    "published_date": "2024-09-06 10:09:41 UTC",
    "updated_date": "2025-05-12 19:41:57 UTC"
  },
  {
    "arxiv_id": "2409.04142v1",
    "title": "Context is the Key: Backdoor Attacks for In-Context Learning with Vision Transformers",
    "authors": [
      "Gorka Abad",
      "Stjepan Picek",
      "Lorenzo Cavallaro",
      "Aitor Urbieta"
    ],
    "abstract": "Due to the high cost of training, large model (LM) practitioners commonly use\npretrained models downloaded from untrusted sources, which could lead to owning\ncompromised models. In-context learning is the ability of LMs to perform\nmultiple tasks depending on the prompt or context. This can enable new attacks,\nsuch as backdoor attacks with dynamic behavior depending on how models are\nprompted.\n  In this paper, we leverage the ability of vision transformers (ViTs) to\nperform different tasks depending on the prompts. Then, through data poisoning,\nwe investigate two new threats: i) task-specific backdoors where the attacker\nchooses a target task to attack, and only the selected task is compromised at\ntest time under the presence of the trigger. At the same time, any other task\nis not affected, even if prompted with the trigger. We succeeded in attacking\nevery tested model, achieving up to 89.90\\% degradation on the target task. ii)\nWe generalize the attack, allowing the backdoor to affect \\emph{any} task, even\ntasks unseen during the training phase. Our attack was successful on every\ntested model, achieving a maximum of $13\\times$ degradation. Finally, we\ninvestigate the robustness of prompts and fine-tuning as techniques for\nremoving the backdoors from the model. We found that these methods fall short\nand, in the best case, reduce the degradation from 89.90\\% to 73.46\\%.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04142v1",
    "published_date": "2024-09-06 09:16:39 UTC",
    "updated_date": "2024-09-06 09:16:39 UTC"
  },
  {
    "arxiv_id": "2409.04117v1",
    "title": "Confidence-Aware Document OCR Error Detection",
    "authors": [
      "Arthur Hemmer",
      "Micka√´l Coustaty",
      "Nicola Bartolo",
      "Jean-Marc Ogier"
    ],
    "abstract": "Optical Character Recognition (OCR) continues to face accuracy challenges\nthat impact subsequent applications. To address these errors, we explore the\nutility of OCR confidence scores for enhancing post-OCR error detection. Our\nstudy involves analyzing the correlation between confidence scores and error\nrates across different OCR systems. We develop ConfBERT, a BERT-based model\nthat incorporates OCR confidence scores into token embeddings and offers an\noptional pre-training phase for noise adjustment. Our experimental results\ndemonstrate that integrating OCR confidence scores can enhance error detection\ncapabilities. This work underscores the importance of OCR confidence scores in\nimproving detection accuracy and reveals substantial disparities in performance\nbetween commercial and open-source OCR technologies.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04117v1",
    "published_date": "2024-09-06 08:35:28 UTC",
    "updated_date": "2024-09-06 08:35:28 UTC"
  },
  {
    "arxiv_id": "2409.04114v1",
    "title": "Multi-Programming Language Ensemble for Code Generation in Large Language Model",
    "authors": [
      "Tengfei Xue",
      "Xuefeng Li",
      "Tahir Azim",
      "Roman Smirnov",
      "Jianhui Yu",
      "Arash Sadrieh",
      "Babak Pahlavan"
    ],
    "abstract": "Large language models (LLMs) have significantly improved code generation,\nparticularly in one-pass code generation. However, most existing approaches\nfocus solely on generating code in a single programming language, overlooking\nthe potential of leveraging the multi-language capabilities of LLMs. LLMs have\nvarying patterns of errors across different languages, suggesting that a more\nrobust approach could be developed by leveraging these multi-language outputs.\nIn this study, we propose Multi-Programming Language Ensemble (MPLE), a novel\nensemble-based method that utilizes code generation across multiple programming\nlanguages to enhance overall performance. By treating each language-specific\ncode generation process as an individual \"weak expert\" and effectively\nintegrating their outputs, our method mitigates language-specific errors and\nbiases. This multi-language ensemble strategy leverages the complementary\nstrengths of different programming languages, enabling the model to produce\nmore accurate and robust code. Our approach can be seamlessly integrated with\ncommonly used techniques such as the reflection algorithm and Monte Carlo tree\nsearch to improve code generation quality further. Experimental results show\nthat our framework consistently enhances baseline performance by up to 17.92%\non existing benchmarks (HumanEval and HumanEval-plus), with a standout result\nof 96.25% accuracy on the HumanEval benchmark, achieving new state-of-the-art\nresults across various LLM models. The code will be released at\nhttps://github.com/NinjaTech-AI/MPLE",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Code available at https://github.com/NinjaTech-AI/MPLE",
    "pdf_url": "http://arxiv.org/pdf/2409.04114v1",
    "published_date": "2024-09-06 08:31:18 UTC",
    "updated_date": "2024-09-06 08:31:18 UTC"
  },
  {
    "arxiv_id": "2409.04109v1",
    "title": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers",
    "authors": [
      "Chenglei Si",
      "Diyi Yang",
      "Tatsunori Hashimoto"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have sparked optimism\nabout their potential to accelerate scientific discovery, with a growing number\nof works proposing research agents that autonomously generate and validate new\nideas. Despite this, no evaluations have shown that LLM systems can take the\nvery first step of producing novel, expert-level ideas, let alone perform the\nentire research process. We address this by establishing an experimental design\nthat evaluates research idea generation while controlling for confounders and\nperforms the first head-to-head comparison between expert NLP researchers and\nan LLM ideation agent. By recruiting over 100 NLP researchers to write novel\nideas and blind reviews of both LLM and human ideas, we obtain the first\nstatistically significant conclusion on current LLM capabilities for research\nideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than\nhuman expert ideas while being judged slightly weaker on feasibility. Studying\nour agent baselines closely, we identify open problems in building and\nevaluating research agents, including failures of LLM self-evaluation and their\nlack of diversity in generation. Finally, we acknowledge that human judgements\nof novelty can be difficult, even by experts, and propose an end-to-end study\ndesign which recruits researchers to execute these ideas into full projects,\nenabling us to study whether these novelty and feasibility judgements result in\nmeaningful differences in research outcome.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "main paper is 20 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.04109v1",
    "published_date": "2024-09-06 08:25:03 UTC",
    "updated_date": "2024-09-06 08:25:03 UTC"
  },
  {
    "arxiv_id": "2409.04104v1",
    "title": "MixNet: Joining Force of Classical and Modern Approaches Toward the Comprehensive Pipeline in Motor Imagery EEG Classification",
    "authors": [
      "Phairot Autthasan",
      "Rattanaphon Chaisaen",
      "Huy Phan",
      "Maarten De Vos",
      "Theerawit Wilaiprasitporn"
    ],
    "abstract": "Recent advances in deep learning (DL) have significantly impacted motor\nimagery (MI)-based brain-computer interface (BCI) systems, enhancing the\ndecoding of electroencephalography (EEG) signals. However, most studies\nstruggle to identify discriminative patterns across subjects during MI tasks,\nlimiting MI classification performance. In this article, we propose MixNet, a\nnovel classification framework designed to overcome this limitation by\nutilizing spectral-spatial signals from MI data, along with a multitask\nlearning architecture named MIN2Net, for classification. Here, the\nspectral-spatial signals are generated using the filter-bank common spatial\npatterns (FBCSPs) method on MI data. Since the multitask learning architecture\nis used for the classification task, the learning in each task may exhibit\ndifferent generalization rates and potential overfitting across tasks. To\naddress this issue, we implement adaptive gradient blending, simultaneously\nregulating multiple loss weights and adjusting the learning pace for each task\nbased on its generalization/overfitting tendencies. Experimental results on six\nbenchmark data sets of different data sizes demonstrate that MixNet\nconsistently outperforms all state-of-the-art algorithms in subject-dependent\nand -independent settings. Finally, the low-density EEG MI classification\nresults show that MixNet outperforms all state-of-the-art algorithms, offering\npromising implications for Internet of Thing (IoT) applications, such as\nlightweight and portable EEG wearable devices based on low-density montages.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "Supplementary materials and source codes are available on-line at\n  https://github.com/Max-Phairot-A/MixNet",
    "pdf_url": "http://arxiv.org/pdf/2409.04104v1",
    "published_date": "2024-09-06 08:14:58 UTC",
    "updated_date": "2024-09-06 08:14:58 UTC"
  },
  {
    "arxiv_id": "2409.04103v1",
    "title": "The Role of Graph Topology in the Performance of Biomedical Knowledge Graph Completion Models",
    "authors": [
      "Alberto Cattaneo",
      "Stephen Bonner",
      "Thomas Martynec",
      "Carlo Luschi",
      "Ian P Barrett",
      "Daniel Justus"
    ],
    "abstract": "Knowledge Graph Completion has been increasingly adopted as a useful method\nfor several tasks in biomedical research, like drug repurposing or drug-target\nidentification. To that end, a variety of datasets and Knowledge Graph\nEmbedding models has been proposed over the years. However, little is known\nabout the properties that render a dataset useful for a given task and, even\nthough theoretical properties of Knowledge Graph Embedding models are well\nunderstood, their practical utility in this field remains controversial. We\nconduct a comprehensive investigation into the topological properties of\npublicly available biomedical Knowledge Graphs and establish links to the\naccuracy observed in real-world applications. By releasing all model\npredictions and a new suite of analysis tools we invite the community to build\nupon our work and continue improving the understanding of these crucial\napplications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04103v1",
    "published_date": "2024-09-06 08:09:15 UTC",
    "updated_date": "2024-09-06 08:09:15 UTC"
  },
  {
    "arxiv_id": "2409.04102v2",
    "title": "Intelligent tutoring systems by Bayesian nets with noisy gates",
    "authors": [
      "Alessandro Antonucci",
      "Francesca Mangili",
      "Claudio Bonesana",
      "Giorgia Adorni"
    ],
    "abstract": "Directed graphical models such as Bayesian nets are often used to implement\nintelligent tutoring systems able to interact in real-time with learners in a\npurely automatic way. When coping with such models, keeping a bound on the\nnumber of parameters might be important for multiple reasons. First, as these\nmodels are typically based on expert knowledge, a huge number of parameters to\nelicit might discourage practitioners from adopting them. Moreover, the number\nof model parameters affects the complexity of the inferences, while a fast\ncomputation of the queries is needed for real-time feedback. We advocate\nlogical gates with uncertainty for a compact parametrization of the conditional\nprobability tables in the underlying Bayesian net used by tutoring systems. We\ndiscuss the semantics of the model parameters to elicit and the assumptions\nrequired to apply such approach in this domain. We also derive a dedicated\ninference scheme to speed up computations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04102v2",
    "published_date": "2024-09-06 08:08:55 UTC",
    "updated_date": "2024-09-09 08:55:22 UTC"
  },
  {
    "arxiv_id": "2409.04082v1",
    "title": "SDformerFlow: Spatiotemporal swin spikeformer for event-based optical flow estimation",
    "authors": [
      "Yi Tian",
      "Juan Andrade-Cetto"
    ],
    "abstract": "Event cameras generate asynchronous and sparse event streams capturing\nchanges in light intensity. They offer significant advantages over conventional\nframe-based cameras, such as a higher dynamic range and an extremely faster\ndata rate, making them particularly useful in scenarios involving fast motion\nor challenging lighting conditions. Spiking neural networks (SNNs) share\nsimilar asynchronous and sparse characteristics and are well-suited for\nprocessing data from event cameras. Inspired by the potential of transformers\nand spike-driven transformers (spikeformers) in other computer vision tasks, we\npropose two solutions for fast and robust optical flow estimation for event\ncameras: STTFlowNet and SDformerFlow. STTFlowNet adopts a U-shaped artificial\nneural network (ANN) architecture with spatiotemporal shifted window\nself-attention (swin) transformer encoders, while SDformerFlow presents its\nfully spiking counterpart, incorporating swin spikeformer encoders.\nFurthermore, we present two variants of the spiking version with different\nneuron models. Our work is the first to make use of spikeformers for dense\noptical flow estimation. We conduct end-to-end training for all models using\nsupervised learning. Our results yield state-of-the-art performance among\nSNN-based event optical flow methods on both the DSEC and MVSEC datasets, and\nshow significant reduction in power consumption compared to the equivalent\nANNs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2409.04082v1",
    "published_date": "2024-09-06 07:48:18 UTC",
    "updated_date": "2024-09-06 07:48:18 UTC"
  },
  {
    "arxiv_id": "2409.04081v3",
    "title": "UI-JEPA: Towards Active Perception of User Intent through Onscreen User Activity",
    "authors": [
      "Yicheng Fu",
      "Raviteja Anantha",
      "Prabal Vashisht",
      "Jianpeng Cheng",
      "Etai Littwin"
    ],
    "abstract": "Generating user intent from a sequence of user interface (UI) actions is a\ncore challenge in comprehensive UI understanding. Recent advancements in\nmultimodal large language models (MLLMs) have led to substantial progress in\nthis area, but their demands for extensive model parameters, computing power,\nand high latency makes them impractical for scenarios requiring lightweight,\non-device solutions with low latency or heightened privacy. Additionally, the\nlack of high-quality datasets has hindered the development of such lightweight\nmodels. To address these challenges, we propose UI-JEPA, a novel framework that\nemploys masking strategies to learn abstract UI embeddings from unlabeled data\nthrough self-supervised learning, combined with an LLM decoder fine-tuned for\nuser intent prediction. We also introduce two new UI-grounded multimodal\ndatasets, \"Intent in the Wild\" (IIW) and \"Intent in the Tame\" (IIT), designed\nfor few-shot and zero-shot UI understanding tasks. IIW consists of 1.7K videos\nacross 219 intent categories, while IIT contains 914 videos across 10\ncategories. We establish the first baselines for these datasets, showing that\nrepresentations learned using a JEPA-style objective, combined with an LLM\ndecoder, can achieve user intent predictions that match the performance of\nstate-of-the-art large MLLMs, but with significantly reduced annotation and\ndeployment resources. Measured by intent similarity scores, UI-JEPA outperforms\nGPT-4 Turbo and Claude 3.5 Sonnet by 10.0% and 7.2% respectively, averaged\nacross two datasets. Notably, UI-JEPA accomplishes the performance with a 50.5x\nreduction in computational cost and a 6.6x improvement in latency in the IIW\ndataset. These results underscore the effectiveness of UI-JEPA, highlighting\nits potential for lightweight, high-performance UI understanding.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04081v3",
    "published_date": "2024-09-06 07:44:44 UTC",
    "updated_date": "2024-10-02 05:00:57 UTC"
  },
  {
    "arxiv_id": "2409.04073v2",
    "title": "AnyMatch -- Efficient Zero-Shot Entity Matching with a Small Language Model",
    "authors": [
      "Zeyu Zhang",
      "Paul Groth",
      "Iacer Calixto",
      "Sebastian Schelter"
    ],
    "abstract": "Entity matching (EM) is the problem of determining whether two records refer\nto same real-world entity, which is crucial in data integration, e.g., for\nproduct catalogs or address databases. A major drawback of many EM approaches\nis their dependence on labelled examples. We thus focus on the challenging\nsetting of zero-shot entity matching where no labelled examples are available\nfor an unseen target dataset. Recently, large language models (LLMs) have shown\npromising results for zero-shot EM, but their low throughput and high\ndeployment cost limit their applicability and scalability.\n  We revisit the zero-shot EM problem with AnyMatch, a small language model\nfine-tuned in a transfer learning setup. We propose several novel data\nselection techniques to generate fine-tuning data for our model, e.g., by\nselecting difficult pairs to match via an AutoML filter, by generating\nadditional attribute-level examples, and by controlling label imbalance in the\ndata.\n  We conduct an extensive evaluation of the prediction quality and deployment\ncost of our model, in a comparison to thirteen baselines on nine benchmark\ndatasets. We find that AnyMatch provides competitive prediction quality despite\nits small parameter size: it achieves the second-highest F1 score overall, and\noutperforms several other approaches that employ models with hundreds of\nbillions of parameters. Furthermore, our approach exhibits major cost benefits:\nthe average prediction quality of AnyMatch is within 4.4% of the\nstate-of-the-art method MatchGPT with the proprietary trillion-parameter model\nGPT-4, yet AnyMatch requires four orders of magnitude less parameters and\nincurs a 3,899 times lower inference cost (in dollars per 1,000 tokens).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages excluding references, 3 figures, and 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.04073v2",
    "published_date": "2024-09-06 07:29:01 UTC",
    "updated_date": "2024-09-09 11:33:00 UTC"
  },
  {
    "arxiv_id": "2409.04065v1",
    "title": "An Argumentative Approach for Explaining Preemption in Soft-Constraint Based Norms",
    "authors": [
      "Wachara Fungwacharakorn",
      "Kanae Tsushima",
      "Hiroshi Hosobe",
      "Hideaki Takeda",
      "Ken Satoh"
    ],
    "abstract": "Although various aspects of soft-constraint based norms have been explored,\nit is still challenging to understand preemption. Preemption is a situation\nwhere higher-level norms override lower-level norms when new information\nemerges. To address this, we propose a derivation state argumentation framework\n(DSA-framework). DSA-framework incorporates derivation states to explain how\npreemption arises based on evolving situational knowledge. Based on\nDSA-framework, we present an argumentative approach for explaining preemption.\nWe formally prove that, under local optimality, DSA-framework can provide\nexplanations why one consequence is obligatory or forbidden by soft-constraint\nbased norms represented as logical constraint hierarchies.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "submitted to VECOMP/AICOM 2024 associated with 27th European\n  Conference on Artificial Intelligence (ECAI2024)",
    "pdf_url": "http://arxiv.org/pdf/2409.04065v1",
    "published_date": "2024-09-06 07:14:32 UTC",
    "updated_date": "2024-09-06 07:14:32 UTC"
  },
  {
    "arxiv_id": "2409.04060v1",
    "title": "D4: Text-guided diffusion model-based domain adaptive data augmentation for vineyard shoot detection",
    "authors": [
      "Kentaro Hirahara",
      "Chikahito Nakane",
      "Hajime Ebisawa",
      "Tsuyoshi Kuroda",
      "Yohei Iwaki",
      "Tomoyoshi Utsumi",
      "Yuichiro Nomura",
      "Makoto Koike",
      "Hiroshi Mineno"
    ],
    "abstract": "In an agricultural field, plant phenotyping using object detection models is\ngaining attention. However, collecting the training data necessary to create\ngeneric and high-precision models is extremely challenging due to the\ndifficulty of annotation and the diversity of domains. Furthermore, it is\ndifficult to transfer training data across different crops, and although\nmachine learning models effective for specific environments, conditions, or\ncrops have been developed, they cannot be widely applied in actual fields. In\nthis study, we propose a generative data augmentation method (D4) for vineyard\nshoot detection. D4 uses a pre-trained text-guided diffusion model based on a\nlarge number of original images culled from video data collected by unmanned\nground vehicles or other means, and a small number of annotated datasets. The\nproposed method generates new annotated images with background information\nadapted to the target domain while retaining annotation information necessary\nfor object detection. In addition, D4 overcomes the lack of training data in\nagriculture, including the difficulty of annotation and diversity of domains.\nWe confirmed that this generative data augmentation method improved the mean\naverage precision by up to 28.65% for the BBox detection task and the average\nprecision by up to 13.73% for the keypoint detection task for vineyard shoot\ndetection. Our generative data augmentation method D4 is expected to\nsimultaneously solve the cost and domain diversity issues of training data\ngeneration in agriculture and improve the generalization performance of\ndetection models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04060v1",
    "published_date": "2024-09-06 07:04:27 UTC",
    "updated_date": "2024-09-06 07:04:27 UTC"
  },
  {
    "arxiv_id": "2409.04056v1",
    "title": "Refining Wikidata Taxonomy using Large Language Models",
    "authors": [
      "Yiwen Peng",
      "Thomas Bonald",
      "Mehwish Alam"
    ],
    "abstract": "Due to its collaborative nature, Wikidata is known to have a complex\ntaxonomy, with recurrent issues like the ambiguity between instances and\nclasses, the inaccuracy of some taxonomic paths, the presence of cycles, and\nthe high level of redundancy across classes. Manual efforts to clean up this\ntaxonomy are time-consuming and prone to errors or subjective decisions. We\npresent WiKC, a new version of Wikidata taxonomy cleaned automatically using a\ncombination of Large Language Models (LLMs) and graph mining techniques.\nOperations on the taxonomy, such as cutting links or merging classes, are\nperformed with the help of zero-shot prompting on an open-source LLM. The\nquality of the refined taxonomy is evaluated from both intrinsic and extrinsic\nperspectives, on a task of entity typing for the latter, showing the practical\ninterest of WiKC.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "ACM International Conference on Information and Knowledge Management,\n  Oct 2024, Boise, Idaho, United States",
    "pdf_url": "http://arxiv.org/pdf/2409.04056v1",
    "published_date": "2024-09-06 06:53:45 UTC",
    "updated_date": "2024-09-06 06:53:45 UTC"
  },
  {
    "arxiv_id": "2409.04053v2",
    "title": "COLUMBUS: Evaluating COgnitive Lateral Understanding through Multiple-choice reBUSes",
    "authors": [
      "Koen Kraaijveld",
      "Yifan Jiang",
      "Kaixin Ma",
      "Filip Ilievski"
    ],
    "abstract": "While visual question-answering (VQA) benchmarks have catalyzed the\ndevelopment of reasoning techniques, they have focused on vertical thinking.\nEffective problem-solving also necessitates lateral thinking, which remains\nunderstudied in AI and has not been used to test visual perception systems. To\nbridge this gap, we formulate visual lateral thinking as a multiple-choice\nquestion-answering task and describe a three-step taxonomy-driven methodology\nfor instantiating task examples. Then, we develop COLUMBUS, a synthetic\nbenchmark that applies the task pipeline to create QA sets with text and icon\nrebus puzzles based on publicly available collections of compounds and common\nphrases. COLUMBUS comprises over 1,000 puzzles, each with four answer\ncandidates. While the SotA vision-language models (VLMs) achieve decent\nperformance, our evaluation demonstrates a substantial gap between humans and\nmodels. VLMs benefit from human-curated descriptions but struggle to\nself-generate such representations at the right level of abstraction.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 10 figures, accepted to AAAI-25",
    "pdf_url": "http://arxiv.org/pdf/2409.04053v2",
    "published_date": "2024-09-06 06:49:55 UTC",
    "updated_date": "2024-12-20 12:14:37 UTC"
  },
  {
    "arxiv_id": "2409.04040v1",
    "title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV Leakage",
    "authors": [
      "Huan Yang",
      "Deyu Zhang",
      "Yudong Zhao",
      "Yuanchun Li",
      "Yunxin Liu"
    ],
    "abstract": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04040v1",
    "published_date": "2024-09-06 06:16:55 UTC",
    "updated_date": "2024-09-06 06:16:55 UTC"
  },
  {
    "arxiv_id": "2409.04025v2",
    "title": "BFA-YOLO: A balanced multiscale object detection network for building fa√ßade attachments detection",
    "authors": [
      "Yangguang Chen",
      "Tong Wang",
      "Guanzhou Chen",
      "Kun Zhu",
      "Xiaoliang Tan",
      "Jiaqi Wang",
      "Wenchao Guo",
      "Qing Wang",
      "Xiaolong Luo",
      "Xiaodong Zhang"
    ],
    "abstract": "The detection of fa\\c{c}ade elements on buildings, such as doors, windows,\nbalconies, air conditioning units, billboards, and glass curtain walls, is a\ncritical step in automating the creation of Building Information Modeling\n(BIM). Yet, this field faces significant challenges, including the uneven\ndistribution of fa\\c{c}ade elements, the presence of small objects, and\nsubstantial background noise, which hamper detection accuracy. To address these\nissues, we develop the BFA-YOLO model and the BFA-3D dataset in this study. The\nBFA-YOLO model is an advanced architecture designed specifically for analyzing\nmulti-view images of fa\\c{c}ade attachments. It integrates three novel\ncomponents: the Feature Balanced Spindle Module (FBSM) that tackles the issue\nof uneven object distribution; the Target Dynamic Alignment Task Detection Head\n(TDATH) that enhances the detection of small objects; and the Position Memory\nEnhanced Self-Attention Mechanism (PMESA), aimed at reducing the impact of\nbackground noise. These elements collectively enable BFA-YOLO to effectively\naddress each challenge, thereby improving model robustness and detection\nprecision. The BFA-3D dataset, offers multi-view images with precise\nannotations across a wide range of fa\\c{c}ade attachment categories. This\ndataset is developed to address the limitations present in existing fa\\c{c}ade\ndetection datasets, which often feature a single perspective and insufficient\ncategory coverage. Through comparative analysis, BFA-YOLO demonstrated\nimprovements of 1.8\\% and 2.9\\% in mAP$_{50}$ on the BFA-3D dataset and the\npublic Fa\\c{c}ade-WHU dataset, respectively, when compared to the baseline\nYOLOv8 model. These results highlight the superior performance of BFA-YOLO in\nfa\\c{c}ade element detection and the advancement of intelligent BIM\ntechnologies.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "21 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.04025v2",
    "published_date": "2024-09-06 04:44:52 UTC",
    "updated_date": "2024-11-11 06:23:21 UTC"
  },
  {
    "arxiv_id": "2409.04007v1",
    "title": "Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition",
    "authors": [
      "Byunggun Kim",
      "Younghun Kwon"
    ],
    "abstract": "Speech emotion recognition (SER) classifies human emotions in speech with a\ncomputer model. Recently, performance in SER has steadily increased as deep\nlearning techniques have adapted. However, unlike many domains that use speech\ndata, data for training in the SER model is insufficient. This causes\noverfitting of training of the neural network, resulting in performance\ndegradation. In fact, successful emotion recognition requires an effective\npreprocessing method and a model structure that efficiently uses the number of\nweight parameters. In this study, we propose using eight dataset versions with\ndifferent frequency-time resolutions to search for an effective emotional\nspeech preprocessing method. We propose a 6-layer convolutional neural network\n(CNN) model with efficient channel attention (ECA) to pursue an efficient model\nstructure. In particular, the well-positioned ECA blocks can improve channel\nfeature representation with only a few parameters. With the interactive\nemotional dyadic motion capture (IEMOCAP) dataset, increasing the frequency\nresolution in preprocessing emotional speech can improve emotion recognition\nperformance. Also, ECA after the deep convolution layer can effectively\nincrease channel feature representation. Consequently, the best result (79.37UA\n79.68WA) can be obtained, exceeding the performance of previous SER models.\nFurthermore, to compensate for the lack of emotional speech data, we experiment\nwith multiple preprocessing data methods that augment trainable data\npreprocessed with all different settings from one sample. In the experiment, we\ncan achieve the highest result (80.28UA 80.46WA).",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04007v1",
    "published_date": "2024-09-06 03:17:25 UTC",
    "updated_date": "2024-09-06 03:17:25 UTC"
  },
  {
    "arxiv_id": "2409.03992v4",
    "title": "Confidential Computing on NVIDIA Hopper GPUs: A Performance Benchmark Study",
    "authors": [
      "Jianwei Zhu",
      "Hang Yin",
      "Peng Deng",
      "Aline Almeida",
      "Shunfan Zhou"
    ],
    "abstract": "This report evaluates the performance impact of enabling Trusted Execution\nEnvironments (TEE) on NVIDIA Hopper GPUs for large language model (LLM)\ninference tasks. We benchmark the overhead introduced by TEE mode across\nvarious LLMs and token lengths, with a particular focus on the bottleneck\ncaused by CPU-GPU data transfers via PCIe. Our results indicate that while\nthere is minimal computational overhead within the GPU, the overall performance\npenalty is primarily attributable to data transfer. For the majority of typical\nLLM queries, the overhead remains below 7%, with larger models and longer\nsequences experiencing nearly zero overhead.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.03992v4",
    "published_date": "2024-09-06 02:44:27 UTC",
    "updated_date": "2024-11-05 16:57:26 UTC"
  },
  {
    "arxiv_id": "2409.04481v1",
    "title": "Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials",
    "authors": [
      "Yizhen Zheng",
      "Huan Yee Koh",
      "Maddie Yang",
      "Li Li",
      "Lauren T. May",
      "Geoffrey I. Webb",
      "Shirui Pan",
      "George Church"
    ],
    "abstract": "The integration of Large Language Models (LLMs) into the drug discovery and\ndevelopment field marks a significant paradigm shift, offering novel\nmethodologies for understanding disease mechanisms, facilitating drug\ndiscovery, and optimizing clinical trial processes. This review highlights the\nexpanding role of LLMs in revolutionizing various stages of the drug\ndevelopment pipeline. We investigate how these advanced computational models\ncan uncover target-disease linkage, interpret complex biomedical data, enhance\ndrug molecule design, predict drug efficacy and safety profiles, and facilitate\nclinical trial processes. Our paper aims to provide a comprehensive overview\nfor researchers and practitioners in computational biology, pharmacology, and\nAI4Science by offering insights into the potential transformative impact of\nLLMs on drug discovery and development.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04481v1",
    "published_date": "2024-09-06 02:03:38 UTC",
    "updated_date": "2024-09-06 02:03:38 UTC"
  },
  {
    "arxiv_id": "2409.03947v1",
    "title": "FODA-PG for Enhanced Medical Imaging Narrative Generation: Adaptive Differentiation of Normal and Abnormal Attributes",
    "authors": [
      "Kai Shu",
      "Yuzhuo Jia",
      "Ziyang Zhang",
      "Jiechao Gao"
    ],
    "abstract": "Automatic Medical Imaging Narrative generation aims to alleviate the workload\nof radiologists by producing accurate clinical descriptions directly from\nradiological images. However, the subtle visual nuances and domain-specific\nterminology in medical images pose significant challenges compared to generic\nimage captioning tasks. Existing approaches often neglect the vital distinction\nbetween normal and abnormal findings, leading to suboptimal performance. In\nthis work, we propose FODA-PG, a novel Fine-grained Organ-Disease Adaptive\nPartitioning Graph framework that addresses these limitations through\ndomain-adaptive learning. FODA-PG constructs a granular graphical\nrepresentation of radiological findings by separating disease-related\nattributes into distinct \"disease-specific\" and \"disease-free\" categories based\non their clinical significance and location. This adaptive partitioning enables\nour model to capture the nuanced differences between normal and pathological\nstates, mitigating the impact of data biases. By integrating this fine-grained\nsemantic knowledge into a powerful transformer-based architecture and providing\nrigorous mathematical justifications for its effectiveness, FODA-PG generates\nprecise and clinically coherent reports with enhanced generalization\ncapabilities. Extensive experiments on the IU-Xray and MIMIC-CXR benchmarks\ndemonstrate the superiority of our approach over state-of-the-art methods,\nhighlighting the importance of domain adaptation in medical report generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.03947v1",
    "published_date": "2024-09-06 00:04:35 UTC",
    "updated_date": "2024-09-06 00:04:35 UTC"
  }
]
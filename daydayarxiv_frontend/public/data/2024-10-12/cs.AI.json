{
  "date": "2024-10-12",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-10-12 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文聚焦于 AI 框架、LLM 优化、生物医学应用和多模态生成等领域，重点包括 OpenR 框架的开源创新、Goodhart's law 的形式化分析，以及 LLM 在科学推理和图像生成的进展；令人印象深刻的文章有 OpenR（受 OpenAI o1 启发）和 ReLU's Revival（NeurIPS 相关），而有名学者如 El-Mahdi El-Mhamdi 和 Lê-Nguyên Hoang 的工作则为决策理论提供新视角。\n\n### 重点论文讨论\n我挑选了今天最重要、话题度和影响大的论文进行详细概述，将相关主题归类讨论。其他次要论文（如文学分析或环境影响研究）将快速掠过，以控制篇幅。\n\n#### LLM 框架与优化（优先讨论，相关论文紧密关联）\n- **OpenR: An Open Source Framework for Advanced Reasoning with Large Language Models**（中文：OpenR：一个用于大型语言模型高级推理的开源框架，英文：OpenR）  \n  这篇论文由 Jun Wang 等作者提出，受 OpenAI o1 模型启发，构建了一个整合数据获取、强化学习训练和非自回归解码的开源平台。主要贡献是首次探索强化学习提升 LLM 推理能力，在 MATH 数据集上验证了显著性能提升（通过测试时计算和过程奖励模型）。这对 LLM 社区影响深远，提供开源代码（https://openreasoner.github.io），是今天最令人印象深刻的文章。\n\n- **SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for LLM Weight Compression**（中文：SLiM：用于 LLM 权重压缩的单次量化与稀疏低秩近似，英文：SLiM）  \n  作者 Mohammad Mozaffari 等提出了一种单次压缩框架，结合量化、稀疏和低秩近似，显著减少 LLM 推理开销（如 LLaMA-2-7B 上提升 5.66% 准确率）。主要发现是无需重新训练即可实现高效压缩，并在 GPU 上加速 3.75 倍，这对资源受限的部署场景有实际意义。\n\n- **ReLU's Revival: On the Entropic Overload in Normalization-Free Large Language Models**（中文：ReLU 的复兴：无归一化大型语言模型中的熵过载，英文：ReLU's Revival）  \n  Nandan Kumar Jha 和 Brandon Reagen 的工作（NeurIPS 2024 Workshop）发现，在无 LayerNorm 的 LLM 中，ReLU 激活函数优于 GELU，能减少熵过载并提升 8.2% 困惑度。主要贡献是揭示激活函数的几何特性（如输入空间特化），为优化 Transformer 架构提供新见解。\n\n- **MTL-LoRA: Low-Rank Adaptation for Multi-Task Learning**（中文：MTL-LoRA：用于多任务学习的低秩自适应，英文：MTL-LoRA）  \n  Yaming Yang 等作者扩展 LoRA 方法，支持多任务 LLM 适应，显著减少参数量（在多任务基准上提升性能）。关键发现是任务自适应参数能缓解任务干扰，这对高效多任务训练有启发。\n\n#### 生物医学与健康应用（高话题度，结合实际影响）\n- **GPTON: Generative Pre-trained Transformers enhanced with Ontology Narration for accurate annotation of biological data**（中文：GPTON：通过本体叙述增强的生成预训练 Transformer 用于生物数据精确注解，英文：GPTON）  \n  作者 Rongbin Li 等开发了 GPTON，使用 GPT-4 生成本体叙述，实现基因集注解准确率超过 68%。主要贡献是融合结构化知识提升生物医学研究，实验证实其鲁棒性。\n\n- **AIMEN: Use of What-if Scenarios to Help Explain Artificial Intelligence Models for Neonatal Health**（中文：AIMEN：使用“如果”场景解释新生儿健康 AI 模型，英文：AIMEN）  \n  Abdullah Mamun 等作者提出 AIMEN 框架，使用深度学习预测分娩风险并提供反事实解释（F1 分数 0.784）。这篇论文强调 AI 在医疗决策中的可解释性，资源可访问（https://github.com/ab9mamun/AIMEN）。\n\n#### 多模态生成与基准（创新性强，相关论文归类）\n- **ControLRM: Fast and Controllable 3D Generation via Large Reconstruction Model**（中文：ControLRM：通过大型重建模型的快速可控 3D 生成，英文：ControLRM）  \n  Hongbin Xu 等作者设计了一个端到端模型，实现快速 3D 生成（在 G-OBJ 等数据集上竞争性能）。主要发现是结合量化实验和硬件优化，提升生成质量。\n\n- **MMAD: A Comprehensive Benchmark for Multimodal Large Language Models in Industrial Anomaly Detection**（中文：MMAD：多模态大型语言模型在工业异常检测的综合基准，英文：MMAD）  \n  Xi Jiang 等作者构建了首个工业异常检测基准，评估 MLLM 在七个子任务上的性能（GPT-4o 准确率 74.9%）。这为 MLLM 在实际应用提供可扩展评估，代码开源（https://github.com/jam-cc/MMAD）。\n\n#### 其他值得一提的论文（快速概述）\n- **On Goodhart's law, with an application to value alignment**（中文：Goodhart 定律及其在价值对齐中的应用，英文：On Goodhart's law）  \n  El-Mahdi El-Mhamdi 和 Lê-Nguyên Hoang 形式化分析了 Goodhart 定律，证明尾部分布影响指标优化效果。主要贡献是为算法决策提供安全评估框架（47 页，11 图）。\n\n- **Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models**（中文：Mamba4Cast：使用状态空间模型的零样本时间序列预测，英文：Mamba4Cast）  \n  Sathya Kamesh Bhethanabhotla 等提出零样本预测模型，基于 Mamba 架构提升效率（在真实数据集上竞争性能）。这简化了时间序列任务的部署。\n\n其他论文，如关于文学分析的“Traversing Emotional Landscapes and Linguistic Patterns in Bernard-Marie Koltès' Plays”（聚焦 NLP 在文学中的应用）或环境影响的“Eco-Aware Graph Neural Networks”（探讨 GNN 能耗），则相对次要，我仅快速提及以节省篇幅。这些工作虽有价值，但影响力较小。\n\n总之，今天的 arXiv 更新突显了 LLM 和多模态 AI 的创新潜力，OpenR 和相关优化方法值得跟踪。更多细节请查阅 arXiv！",
  "papers": [
    {
      "arxiv_id": "2410.09671v1",
      "title": "OpenR: An Open Source Framework for Advanced Reasoning with Large Language Models",
      "title_zh": "OpenR：一个开源框架，用于大语言模型的高级推理",
      "authors": [
        "Jun Wang",
        "Meng Fang",
        "Ziyu Wan",
        "Muning Wen",
        "Jiachen Zhu",
        "Anjie Liu",
        "Ziqin Gong",
        "Yan Song",
        "Lei Chen",
        "Lionel M. Ni",
        "Linyi Yang",
        "Ying Wen",
        "Weinan Zhang"
      ],
      "abstract": "In this technical report, we introduce OpenR, an open-source framework\ndesigned to integrate key components for enhancing the reasoning capabilities\nof large language models (LLMs). OpenR unifies data acquisition, reinforcement\nlearning training (both online and offline), and non-autoregressive decoding\ninto a cohesive software platform. Our goal is to establish an open-source\nplatform and community to accelerate the development of LLM reasoning. Inspired\nby the success of OpenAI's o1 model, which demonstrated improved reasoning\nabilities through step-by-step reasoning and reinforcement learning, OpenR\nintegrates test-time compute, reinforcement learning, and process supervision\nto improve reasoning in LLMs. Our work is the first to provide an open-source\nframework that explores the core techniques of OpenAI's o1 model with\nreinforcement learning, achieving advanced reasoning capabilities beyond\ntraditional autoregressive methods. We demonstrate the efficacy of OpenR by\nevaluating it on the MATH dataset, utilising publicly available data and search\nmethods. Our initial experiments confirm substantial gains, with relative\nimprovements in reasoning and performance driven by test-time computation and\nreinforcement learning through process reward models. The OpenR framework,\nincluding code, models, and datasets, is accessible at\nhttps://openreasoner.github.io.",
      "tldr_zh": "本文介绍了 OpenR，一个开源框架，旨在提升 Large Language Models (LLMs) 的推理能力，通过整合数据获取、reinforcement learning 训练（在线和离线）以及非自回归解码，提供一个统一平台加速 LLM 推理发展。受 OpenAI's o1 模型启发，OpenR 结合测试时计算、强化学习和过程监督，实现超越传统自回归方法的先进推理。该框架在 MATH 数据集上的实验显示了显著性能提升，包括通过过程奖励模型驱动的相对改进。OpenR 的代码、模型和数据集已在 https://openreasoner.github.io 公开可用。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09671v1",
      "published_date": "2024-10-12 23:42:16 UTC",
      "updated_date": "2024-10-12 23:42:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:36:34.480887"
    },
    {
      "arxiv_id": "2410.09656v1",
      "title": "LSTM-Based Proactive Congestion Management for Internet of Vehicle Networks",
      "title_zh": "基于 LSTM 的车辆互联网网络主动拥塞管理",
      "authors": [
        "Aly Sabri Abdalla",
        "Ahmad Al-Kabbany",
        "Ehab F. Badran",
        "Vuk Marojevic"
      ],
      "abstract": "Vehicle-to-everything (V2X) networks support a variety of safety,\nentertainment, and commercial applications. This is realized by applying the\nprinciples of the Internet of Vehicles (IoV) to facilitate connectivity among\nvehicles and between vehicles and roadside units (RSUs). Network congestion\nmanagement is essential for IoVs and it represents a significant concern due to\nits impact on improving the efficiency of transportation systems and providing\nreliable communication among vehicles for the timely delivery of\nsafety-critical packets. This paper introduces a framework for proactive\ncongestion management for IoV networks. We generate congestion scenarios and a\ndata set to predict the congestion using LSTM. We present the framework and the\npacket congestion dataset. Simulation results using SUMO with NS3 demonstrate\nthe effectiveness of the framework for forecasting IoV network congestion and\nclustering/prioritizing packets employing recurrent neural networks.",
      "tldr_zh": "这篇论文针对Internet of Vehicles (IoV)网络中的拥塞问题，提出了一种基于LSTM的主动拥塞管理框架，以提升交通效率和安全通信。框架通过生成拥塞场景和数据集，利用LSTM模型预测潜在拥塞，并采用循环神经网络对数据包进行聚类和优先级排序。模拟实验使用SUMO和NS3证明了框架的有效性，在预测IoV网络拥塞方面显著提高了性能。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.NI",
      "comment": "This article has been accepted for publication in the IEEE VTC Fall\n  2024",
      "pdf_url": "http://arxiv.org/pdf/2410.09656v1",
      "published_date": "2024-10-12 21:21:42 UTC",
      "updated_date": "2024-10-12 21:21:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:36:45.449117"
    },
    {
      "arxiv_id": "2410.09652v1",
      "title": "Survival of the Safest: Towards Secure Prompt Optimization through Interleaved Multi-Objective Evolution",
      "title_zh": "翻译失败",
      "authors": [
        "Ankita Sinha",
        "Wendi Cui",
        "Kamalika Das",
        "Jiaxin Zhang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities;\nhowever, the optimization of their prompts has historically prioritized\nperformance metrics at the expense of crucial safety and security\nconsiderations. To overcome this shortcoming, we introduce \"Survival of the\nSafest\" (SoS), an innovative multi-objective prompt optimization framework that\nenhances both performance and security in LLMs simultaneously. SoS utilizes an\ninterleaved multi-objective evolution strategy, integrating semantic, feedback,\nand crossover mutations to effectively traverse the prompt landscape. Differing\nfrom the computationally demanding Pareto front methods, SoS provides a\nscalable solution that expedites optimization in complex, high-dimensional\ndiscrete search spaces while keeping computational demands low. Our approach\naccommodates flexible weighting of objectives and generates a pool of optimized\ncandidates, empowering users to select prompts that optimally meet their\nspecific performance and security needs. Experimental evaluations across\ndiverse benchmark datasets affirm SoS's efficacy in delivering high performance\nand notably enhancing safety and security compared to single-objective methods.\nThis advancement marks a significant stride towards the deployment of LLM\nsystems that are both high-performing and secure across varied industrial\napplications",
      "tldr_zh": "这篇论文提出“Survival of the Safest”（SoS），一个创新的多目标提示优化框架，旨在同时提升大型语言模型（LLMs）的性能和安全性，以解决传统优化忽略安全问题的不足。SoS 采用交错多目标进化策略，包括语义、反馈和交叉突变，来高效遍历提示空间，并提供可扩展的解决方案，避免了计算密集型 Pareto front 方法的缺点。实验在多种基准数据集上验证了 SoS 的有效性，其优化结果在保持高性能的同时显著提升了安全性和安全性。总体上，这标志着 LLM 系统在工业应用中实现可靠部署的重要进步。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.CR",
      "comment": "EMNLP 2024 Industry Track",
      "pdf_url": "http://arxiv.org/pdf/2410.09652v1",
      "published_date": "2024-10-12 21:16:29 UTC",
      "updated_date": "2024-10-12 21:16:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:36:59.100211"
    },
    {
      "arxiv_id": "2410.09643v1",
      "title": "Multimodal Physical Activity Forecasting in Free-Living Clinical Settings: Hunting Opportunities for Just-in-Time Interventions",
      "title_zh": "翻译失败",
      "authors": [
        "Abdullah Mamun",
        "Krista S. Leonard",
        "Megan E. Petrov",
        "Matthew P. Buman",
        "Hassan Ghasemzadeh"
      ],
      "abstract": "Objective: This research aims to develop a lifestyle intervention system,\ncalled MoveSense, that forecasts a patient's activity behavior to allow for\nearly and personalized interventions in real-world clinical environments.\nMethods: We conducted two clinical studies involving 58 prediabetic veterans\nand 60 patients with obstructive sleep apnea to gather multimodal behavioral\ndata using wearable devices. We develop multimodal long short-term memory\n(LSTM) network models, which are capable of forecasting the number of step\ncounts of a patient up to 24 hours in advance by examining data from activity\nand engagement modalities. Furthermore, we design goal-based forecasting models\nto predict whether a person's next-day steps will be over a certain threshold.\nResults: Multimodal LSTM with early fusion achieves 33% and 37% lower mean\nabsolute errors than linear regression and ARIMA respectively on the\nprediabetes dataset. LSTM also outperforms linear regression and ARIMA with a\nmargin of 13% and 32% on the sleep dataset. Multimodal forecasting models also\nperform with 72% and 79% accuracy on the prediabetes dataset and sleep dataset\nrespectively on goal-based forecasting. Conclusion: Our experiments conclude\nthat multimodal LSTM models with early fusion are better than multimodal LSTM\nwith late fusion and unimodal LSTM models and also than ARIMA and linear\nregression models. Significance: We address an important and challenging task\nof time-series forecasting in uncontrolled environments. Effective forecasting\nof a person's physical activity can aid in designing adaptive behavioral\ninterventions to keep the user engaged and adherent to a prescribed routine.",
      "tldr_zh": "这篇论文开发了名为 MoveSense 的生活干预系统，利用多模态 LSTM 模型预测患者在自由生活临床环境中的身体活动行为，以支持及时干预。研究通过两个临床试验收集了58名糖尿病前期退伍军人和60名阻塞性睡眠呼吸暂停患者的穿戴设备数据，构建了能够提前24小时预测步数的模型，并设计了基于目标的预测来判断次日步数是否超过阈值。结果显示，多模态 LSTM 与 early fusion 相比线性回归和 ARIMA 模型，平均绝对误差降低了33%和37%（在糖尿病前期数据集），并在基于目标的预测中达到72%和79%的准确率。总体而言，该方法证明了多模态 LSTM 的优越性，并为设计适应性行为干预提供重要机会，以提升患者参与度和依从性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.09643v1",
      "published_date": "2024-10-12 20:44:00 UTC",
      "updated_date": "2024-10-12 20:44:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:37:22.293093"
    },
    {
      "arxiv_id": "2410.10899v2",
      "title": "GPTON: Generative Pre-trained Transformers enhanced with Ontology Narration for accurate annotation of biological data",
      "title_zh": "翻译失败",
      "authors": [
        "Rongbin Li",
        "Wenbo Chen",
        "Jinbo Li",
        "Hanwen Xing",
        "Hua Xu",
        "Zhao Li",
        "W. Jim Zheng"
      ],
      "abstract": "By leveraging GPT-4 for ontology narration, we developed GPTON to infuse\nstructured knowledge into LLMs through verbalized ontology terms, achieving\naccurate text and ontology annotations for over 68% of gene sets in the top\nfive predictions. Manual evaluations confirm GPTON's robustness, highlighting\nits potential to harness LLMs and structured knowledge to significantly advance\nbiomedical research beyond gene set annotation.",
      "tldr_zh": "本研究开发了 GPTON，一种通过 GPT-4 进行 Ontology Narration 的框架，将结构化知识注入 LLMs，以提升生物数据的文本和本体注释准确性。\n实验结果显示，GPTON 在前五预测中对超过 68% 的基因集实现了精确注释。\n手动评估确认了其稳健性，并强调 GPTON 有潜力超越基因集注释，推动生物医学研究的整体进展。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "J.3; I.2.7"
      ],
      "primary_category": "q-bio.QM",
      "comment": "25 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.10899v2",
      "published_date": "2024-10-12 20:27:05 UTC",
      "updated_date": "2024-10-17 04:08:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:37:22.156068"
    },
    {
      "arxiv_id": "2410.09638v1",
      "title": "On Goodhart's law, with an application to value alignment",
      "title_zh": "关于 Goodhart 定律及其在价值对齐中的应用",
      "authors": [
        "El-Mahdi El-Mhamdi",
        "Lê-Nguyên Hoang"
      ],
      "abstract": "``When a measure becomes a target, it ceases to be a good measure'', this\nadage is known as {\\it Goodhart's law}. In this paper, we investigate formally\nthis law and prove that it critically depends on the tail distribution of the\ndiscrepancy between the true goal and the measure that is optimized.\nDiscrepancies with long-tail distributions favor a Goodhart's law, that is, the\noptimization of the measure can have a counter-productive effect on the goal.\n  We provide a formal setting to assess Goodhart's law by studying the\nasymptotic behavior of the correlation between the goal and the measure, as the\nmeasure is optimized. Moreover, we introduce a distinction between a {\\it weak}\nGoodhart's law, when over-optimizing the metric is useless for the true goal,\nand a {\\it strong} Goodhart's law, when over-optimizing the metric is harmful\nfor the true goal. A distinction which we prove to depend on the tail\ndistribution.\n  We stress the implications of this result to large-scale decision making and\npolicies that are (and have to be) based on metrics, and propose numerous\nresearch directions to better assess the safety of such policies in general,\nand to the particularly concerning case where these policies are automated with\nalgorithms.",
      "tldr_zh": "本研究正式探讨了 Goodhart's law，即“当一个指标成为目标时，它就不再是一个好的指标”，并证明该定律的关键取决于真实目标与优化指标之间差异的 tail distribution。作者通过分析指标优化时目标与指标相关性的渐近行为，区分了弱 Goodhart's law（过度优化指标对目标无益）和强 Goodhart's law（过度优化指标有害）。结果显示，长尾分布的差异更易导致 Goodhart's law 的发生，并强调了其对大规模决策和基于算法的价值 alignment 政策的安全性影响，提出进一步研究方向以评估此类政策的潜在风险。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.ST",
        "stat.TH"
      ],
      "primary_category": "stat.ML",
      "comment": "47 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.09638v1",
      "published_date": "2024-10-12 20:26:08 UTC",
      "updated_date": "2024-10-12 20:26:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:37:33.983097"
    },
    {
      "arxiv_id": "2410.09637v3",
      "title": "ReLU's Revival: On the Entropic Overload in Normalization-Free Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Nandan Kumar Jha",
        "Brandon Reagen"
      ],
      "abstract": "LayerNorm is a critical component in modern large language models (LLMs) for\nstabilizing training and ensuring smooth optimization. However, it introduces\nsignificant challenges in mechanistic interpretability, outlier feature\nsuppression, faithful signal propagation, and computational and communication\ncomplexity of private inference. This work explores desirable activation\nfunctions in normalization-free decoder-only LLMs. Contrary to the conventional\npreference for the GELU in transformer-based models, our empirical findings\ndemonstrate an {\\em opposite trend} -- ReLU significantly outperforms GELU in\nLayerNorm-free models, leading to an {\\bf 8.2\\%} perplexity improvement. We\ndiscover a key issue with GELU, where early layers experience entropic\noverload, leading to the under-utilization of the representational capacity of\nattention heads. This highlights that smoother activations like GELU are {\\em\nill-suited} for LayerNorm-free architectures, whereas ReLU's geometrical\nproperties -- specialization in input space and intra-class selectivity -- lead\nto improved learning dynamics and better information retention in the absence\nof LayerNorm. This study offers key insights for optimizing transformer\narchitectures where LayerNorm introduces significant challenges. The code and\nimplementation are available at\nhttps://github.com/Nandan91/relu-revival-normfree",
      "tldr_zh": "本研究探讨了在无 LayerNorm 的 decoder-only 大型语言模型 (LLMs) 中，激活函数的选择问题，强调 LayerNorm 带来的机制解释、异常特征抑制和计算复杂性等挑战。实验结果显示，与传统偏好的 GELU 相反，ReLU 在这些模型中显著优于 GELU，导致 perplexity 改善 8.2%。研究发现，GELU 在早期层会引发 entropic overload，导致注意力头的代表能力未被充分利用，而 ReLU 的几何特性（如输入空间的专业化和类内选择性）提升了学习动态和信息保留。总之，此工作为优化无 LayerNorm 的 transformer 架构提供了关键见解，并提供了相关代码实现。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to NeurIPS 2024 Workshop on Attributing Model Behavior at\n  Scale (Camera-ready version)",
      "pdf_url": "http://arxiv.org/pdf/2410.09637v3",
      "published_date": "2024-10-12 20:26:01 UTC",
      "updated_date": "2024-11-16 17:59:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:37:46.516658"
    },
    {
      "arxiv_id": "2410.09636v1",
      "title": "Can We Estimate Purchase Intention Based on Zero-shot Speech Emotion Recognition?",
      "title_zh": "我们能否基于零样本语音情感识别估计购买意图？",
      "authors": [
        "Ryotaro Nagase",
        "Takashi Sumiyoshi",
        "Natsuo Yamashita",
        "Kota Dohi",
        "Yohei Kawaguchi"
      ],
      "abstract": "This paper proposes a zero-shot speech emotion recognition (SER) method that\nestimates emotions not previously defined in the SER model training.\nConventional methods are limited to recognizing emotions defined by a single\nword. Moreover, we have the motivation to recognize unknown bipolar emotions\nsuch as ``I want to buy - I do not want to buy.'' In order to allow the model\nto define classes using sentences freely and to estimate unknown bipolar\nemotions, our proposed method expands upon the contrastive language-audio\npre-training (CLAP) framework by introducing multi-class and multi-task\nsettings. We also focus on purchase intention as a bipolar emotion and\ninvestigate the model's performance to zero-shot estimate it. This study is the\nfirst attempt to estimate purchase intention from speech directly. Experiments\nconfirm that the results of zero-shot estimation by the proposed method are at\nthe same level as those of the model trained by supervised learning.",
      "tldr_zh": "这篇论文提出了一种零-shot Speech Emotion Recognition (SER) 方法，能够估计训练中未定义的双极情感，例如“我想买 - 我不想买”，以克服传统方法仅限于单一词情感识别的局限性。方法基于 Contrastive Language-Audio Pre-training (CLAP) 框架，引入多类和多任务设置，允许模型自由定义类并直接从语音估计购买意图，这是首次尝试此类任务。实验结果显示，该方法的零-shot 估计性能与监督学习模型相当，为未知情感识别提供了新途径。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.AS",
      "comment": "5 pages, 3 figures, accepted for APSIPA 2024 ASC",
      "pdf_url": "http://arxiv.org/pdf/2410.09636v1",
      "published_date": "2024-10-12 20:25:16 UTC",
      "updated_date": "2024-10-12 20:25:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:37:58.155132"
    },
    {
      "arxiv_id": "2410.09635v1",
      "title": "Use of What-if Scenarios to Help Explain Artificial Intelligence Models for Neonatal Health",
      "title_zh": "翻译失败",
      "authors": [
        "Abdullah Mamun",
        "Lawrence D. Devoe",
        "Mark I. Evans",
        "David W. Britt",
        "Judith Klein-Seetharaman",
        "Hassan Ghasemzadeh"
      ],
      "abstract": "Early detection of intrapartum risk enables interventions to potentially\nprevent or mitigate adverse labor outcomes such as cerebral palsy. Currently,\nthere is no accurate automated system to predict such events to assist with\nclinical decision-making. To fill this gap, we propose \"Artificial Intelligence\n(AI) for Modeling and Explaining Neonatal Health\" (AIMEN), a deep learning\nframework that not only predicts adverse labor outcomes from maternal, fetal,\nobstetrical, and intrapartum risk factors but also provides the model's\nreasoning behind the predictions made. The latter can provide insights into\nwhat modifications in the input variables of the model could have changed the\npredicted outcome. We address the challenges of imbalance and small datasets by\nsynthesizing additional training data using Adaptive Synthetic Sampling\n(ADASYN) and Conditional Tabular Generative Adversarial Networks (CTGAN). AIMEN\nuses an ensemble of fully-connected neural networks as the backbone for its\nclassification with the data augmentation supported by either ADASYN or CTGAN.\nAIMEN, supported by CTGAN, outperforms AIMEN supported by ADASYN in\nclassification. AIMEN can predict a high risk for adverse labor outcomes with\nan average F1 score of 0.784. It also provides counterfactual explanations that\ncan be achieved by changing 2 to 3 attributes on average. Resources available:\nhttps://github.com/ab9mamun/AIMEN.",
      "tldr_zh": "该研究提出AIMEN框架，一种深度学习模型，用于预测分娩风险因素导致的不良新生儿健康结果，如脑瘫，并通过what-if场景提供模型推理和counterfactual explanations。AIMEN采用数据增强技术ADASYN和CTGAN来处理数据不平衡和小样本问题，并使用全连接神经网络的集成作为分类骨干。实验结果显示，CTGAN支持的AIMEN在预测不良结果时平均F1 score达0.784，并能通过改变2-3个属性生成可解释的逆事实解释。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.09635v1",
      "published_date": "2024-10-12 20:21:00 UTC",
      "updated_date": "2024-10-12 20:21:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:38:10.394746"
    },
    {
      "arxiv_id": "2410.09629v1",
      "title": "Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models",
      "title_zh": "合成知识摄取：面向知识精炼和注入",
      "authors": [
        "Jiaxin Zhang",
        "Wendi Cui",
        "Yiran Huang",
        "Kamalika Das",
        "Sricharan Kumar"
      ],
      "abstract": "Large language models (LLMs) are proficient in capturing factual knowledge\nacross various domains. However, refining their capabilities on previously seen\nknowledge or integrating new knowledge from external sources remains a\nsignificant challenge. In this work, we propose a novel synthetic knowledge\ningestion method called Ski, which leverages fine-grained synthesis,\ninterleaved generation, and assemble augmentation strategies to construct\nhigh-quality data representations from raw knowledge sources. We then integrate\nSki and its variations with three knowledge injection techniques: Retrieval\nAugmented Generation (RAG), Supervised Fine-tuning (SFT), and Continual\nPre-training (CPT) to inject and refine knowledge in language models. Extensive\nempirical experiments are conducted on various question-answering tasks\nspanning finance, biomedicine, and open-generation domains to demonstrate that\nSki significantly outperforms baseline methods by facilitating effective\nknowledge injection. We believe that our work is an important step towards\nenhancing the factual accuracy of LLM outputs by refining knowledge\nrepresentation and injection capabilities.",
      "tldr_zh": "这篇论文提出了一种名为 Synthetic Knowledge Ingestion (Ski) 的新方法，用于提升 Large Language Models (LLMs) 对既有知识的精炼和外部新知识的注入。Ski 通过细粒度合成、交错生成和组装增强策略，从原始知识来源构建高质量数据表示，并将其与 Retrieval Augmented Generation (RAG)、Supervised Fine-tuning (SFT) 和 Continual Pre-training (CPT) 等技术整合，以优化知识注入过程。在金融、生物医学和开放生成领域的问答任务实验中，Ski 显著优于基线方法，提高了知识注入的有效性，并为提升 LLM 输出的事实准确性提供了重要途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024 main conference long paper",
      "pdf_url": "http://arxiv.org/pdf/2410.09629v1",
      "published_date": "2024-10-12 19:38:09 UTC",
      "updated_date": "2024-10-12 19:38:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:38:23.244807"
    },
    {
      "arxiv_id": "2410.12861v2",
      "title": "Scaled and Inter-token Relation Enhanced Transformer for Sample-restricted Residential NILM",
      "title_zh": "缩放和令牌间关系增强的 Transformer 用于样本受限住宅 NILM",
      "authors": [
        "Minhajur Rahman",
        "Yasir Arafat"
      ],
      "abstract": "Transformers have demonstrated exceptional performance across various domains\ndue to their self-attention mechanism, which captures complex relationships in\ndata. However, training on smaller datasets poses challenges, as standard\nattention mechanisms can over-smooth attention scores and overly prioritize\nintra-token relationships, reducing the capture of meaningful inter-token\ndependencies critical for tasks like Non-Intrusive Load Monitoring (NILM). To\naddress this, we propose a novel transformer architecture with two key\ninnovations: inter-token relation enhancement and dynamic temperature tuning.\nThe inter-token relation enhancement mechanism removes diagonal entries in the\nsimilarity matrix to improve attention focus on inter-token relations. The\ndynamic temperature tuning mechanism, a learnable parameter, adapts attention\nsharpness during training, preventing over-smoothing and enhancing sensitivity\nto token relationships. We validate our method on the REDD dataset and show\nthat it outperforms the original transformer and state-of-the-art models by\n10-15\\% in F1 score across various appliance types, demonstrating its efficacy\nfor training on smaller datasets.",
      "tldr_zh": "本文提出了一种改进的 Transformer 架构，名为 Scaled and Inter-token Relation Enhanced Transformer，针对小数据集训练的 Non-Intrusive Load Monitoring (NILM) 任务问题。创新点包括 inter-token relation enhancement 机制，通过移除相似性矩阵中的对角线条目来增强跨 token 关系的关注，以及 dynamic temperature tuning 机制，该可学习参数动态调整注意力锐度以防止过度平滑。实验在 REDD 数据集上验证，该方法在各种家电类型上比原 Transformer 和最先进模型的 F1 score 提高了 10-15%，证明了其在样本受限场景下的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to 27th IEEE-ICCIT",
      "pdf_url": "http://arxiv.org/pdf/2410.12861v2",
      "published_date": "2024-10-12 18:58:45 UTC",
      "updated_date": "2024-12-06 19:24:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:38:34.624741"
    },
    {
      "arxiv_id": "2410.09615v2",
      "title": "SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for LLM Weight Compression",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Mozaffari",
        "Amir Yazdanbakhsh",
        "Maryam Mehri Dehnavi"
      ],
      "abstract": "Conventional model compression techniques for LLMs address high memory\nconsumption and slow inference challenges but typically require computationally\nexpensive retraining to preserve accuracy. In contrast, one-shot compression\nmethods eliminate retraining cost, but struggle to achieve accuracy comparable\nto dense models. This paper presents SLIM, a new one-shot compression framework\nthat holistically integrates hardware-friendly quantization, sparsity, and\nlow-rank approximation into a unified process. First, we formulate the\nquantization process using a probabilistic approach (SLIM-Quant) that enables\nus to apply uniform quantization. Then, we use an existing one-shot pruning\nmethod to apply semi-structured sparsity on top of the quantized weights.\nFinally, to compensate for the introduced aggregated quantization and sparsity\nerror, we use a novel saliency function with unique invertible and additive\nfeatures that enables us to mathematically compute the value of low-rank\nadapters. SLIM improves model accuracy by up to 5.66% (LLaMA-2-7B) for 2:4\nsparsity with 4-bit weight quantization, outperforming prior methods. Models\ncompressed with SLIM achieve up to 3.78x and 3.75x layer-wise speedup on Nvidia\nRTX3060 and A100 GPUs, respectively. We also propose an optional PEFT recipe\nthat further improves accuracy by up to 1.66% (LLaMA-2-13B) compared to SLIM\nwithout fine-tuning",
      "tldr_zh": "本文提出SLiM框架，一种one-shot压缩方法，用于LLM权重压缩，通过整合hardware-friendly量化(SLIM-Quant)、semi-structured sparsity和low-rank approximation，统一处理高内存消耗和慢速推理问题。具体而言，SLiM采用概率量化方法并结合one-shot修剪和新型显着性函数来补偿误差，从而显著提升模型性能。实验结果显示，SLiM在LLaMA-2-7B模型上实现了2:4 sparsity和4-bit量化下的5.66%准确率提升，并在Nvidia RTX3060和A100 GPU上分别获得3.78x和3.75x层级加速；此外，可选的PEFT配方可进一步提高准确率高达1.66%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09615v2",
      "published_date": "2024-10-12 18:36:07 UTC",
      "updated_date": "2025-02-04 01:30:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:38:49.918092"
    },
    {
      "arxiv_id": "2410.09613v1",
      "title": "Transformer-based Language Models for Reasoning in the Description Logic ALCQ",
      "title_zh": "翻译失败",
      "authors": [
        "Angelos Poulis",
        "Eleni Tsalapati",
        "Manolis Koubarakis"
      ],
      "abstract": "Recent advancements in transformer-based language models have sparked\nresearch into their logical reasoning capabilities. Most of the benchmarks used\nto evaluate these models are simple: generated from short (fragments of)\nfirst-order logic sentences with only a few logical operators and quantifiers.\nWe construct the natural language dataset, DELTA$_D$, using the expressive\ndescription logic language $\\mathcal{ALCQ}$. DELTA$_D$ comprises 384K examples\nand increases in two dimensions: i) reasoning depth, and ii) linguistic\ncomplexity. In this way, we systematically investigate the logical reasoning\ncapabilities of a supervised fine-tuned DeBERTa-based model and two large\nlanguage models (GPT-3.5, GPT-4) with few-shot prompting. We show that the\nDeBERTa-based model fine-tuned on our dataset can master the entailment\nchecking task. Moreover, the performance of GPTs can improve significantly even\nwhen a small number of samples is provided (9 shots). We open-source our code\nand datasets.",
      "tldr_zh": "本研究构建了 DELTA$_D$ 数据集，使用描述逻辑语言 $\\mathcal{ALCQ}$ 生成 384K 个自然语言示例，以评估 transformer-based 语言模型的逻辑推理能力，该数据集在推理深度和语言复杂性两个维度上进行扩展。研究方法包括对 DeBERTa-based 模型进行监督微调，以及对 GPT-3.5 和 GPT-4 模型使用 few-shot prompting（少量样本提示）。结果表明，微调后的 DeBERTa-based 模型能熟练处理 entailment checking（蕴涵检查）任务，而 GPT 模型在仅提供 9 个样本时性能显著提升。论文开源了代码和数据集，为更复杂逻辑推理的基准测试提供了新资源。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)",
      "pdf_url": "http://arxiv.org/pdf/2410.09613v1",
      "published_date": "2024-10-12 18:25:34 UTC",
      "updated_date": "2024-10-12 18:25:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:39:01.066782"
    },
    {
      "arxiv_id": "2410.09609v1",
      "title": "Traversing Emotional Landscapes and Linguistic Patterns in Bernard-Marie Koltès' Plays: An NLP Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Arezou Zahiri Pourzarandi",
        "Farshad Jafari"
      ],
      "abstract": "This study employs Natural Language Processing (NLP) to analyze the intricate\nlinguistic and emotional dimensions within the plays of Bernard-Marie Kolt\\`es,\na central figure in contemporary French theatre. By integrating advanced\ncomputational techniques, we dissect Kolt\\`es' narrative style, revealing the\nsubtle interplay between language and emotion across his dramatic oeuvre. Our\nfindings highlight how Kolt\\`es crafts his narratives, enriching our\nunderstanding of his thematic explorations and contributing to the broader\nfield of digital humanities in literary analysis.",
      "tldr_zh": "本研究利用 Natural Language Processing (NLP) 技术分析 Bernard-Marie Koltès 戏剧中的语言和情感维度，通过整合高级计算方法剖析其叙事风格。研究揭示了语言与情感之间微妙的互动方式，展示了 Koltès 如何通过巧妙构建叙事来探索主题。最终，这为数字人文领域的文学分析提供了重要贡献，丰富了当代法国戏剧的理解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09609v1",
      "published_date": "2024-10-12 18:13:47 UTC",
      "updated_date": "2024-10-12 18:13:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:39:10.211103"
    },
    {
      "arxiv_id": "2410.09604v1",
      "title": "EmbodiedCity: A Benchmark Platform for Embodied Agent in Real-world City Environment",
      "title_zh": "EmbodiedCity: 一种用于真实世界城市环境的具身代理基准平台",
      "authors": [
        "Chen Gao",
        "Baining Zhao",
        "Weichen Zhang",
        "Jinzhu Mao",
        "Jun Zhang",
        "Zhiheng Zheng",
        "Fanhang Man",
        "Jianjie Fang",
        "Zile Zhou",
        "Jinqiang Cui",
        "Xinlei Chen",
        "Yong Li"
      ],
      "abstract": "Embodied artificial intelligence emphasizes the role of an agent's body in\ngenerating human-like behaviors. The recent efforts on EmbodiedAI pay a lot of\nattention to building up machine learning models to possess perceiving,\nplanning, and acting abilities, thereby enabling real-time interaction with the\nworld. However, most works focus on bounded indoor environments, such as\nnavigation in a room or manipulating a device, with limited exploration of\nembodying the agents in open-world scenarios. That is, embodied intelligence in\nthe open and outdoor environment is less explored, for which one potential\nreason is the lack of high-quality simulators, benchmarks, and datasets. To\naddress it, in this paper, we construct a benchmark platform for embodied\nintelligence evaluation in real-world city environments. Specifically, we first\nconstruct a highly realistic 3D simulation environment based on the real\nbuildings, roads, and other elements in a real city. In this environment, we\ncombine historically collected data and simulation algorithms to conduct\nsimulations of pedestrian and vehicle flows with high fidelity. Further, we\ndesigned a set of evaluation tasks covering different EmbodiedAI abilities.\nMoreover, we provide a complete set of input and output interfaces for access,\nenabling embodied agents to easily take task requirements and current\nenvironmental observations as input and then make decisions and obtain\nperformance evaluations. On the one hand, it expands the capability of existing\nembodied intelligence to higher levels. On the other hand, it has a higher\npractical value in the real world and can support more potential applications\nfor artificial general intelligence. Based on this platform, we evaluate some\npopular large language models for embodied intelligence capabilities of\ndifferent dimensions and difficulties.",
      "tldr_zh": "这篇论文引入了EmbodiedCity基准平台，用于评估代理在真实城市环境的Embodied AI能力，填补了现有研究对户外开放场景的探索不足。平台基于真实城市元素构建高度真实的3D模拟环境，并结合历史数据和模拟算法实现高保真度的行人和车辆流动模拟。研究设计了一系列任务来测试代理的感知、规划和行动能力，并提供完整的输入输出接口，便于代理处理任务并获取性能评估。最终，通过评估流行的大语言模型，平台展示了其在提升Embodied AI水平和支持人工智能通用应用方面的实际价值。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "All of the software, Python library, codes, datasets, tutorials, and\n  real-time online service are available on this website:\n  https://embodied-city.fiblab.net",
      "pdf_url": "http://arxiv.org/pdf/2410.09604v1",
      "published_date": "2024-10-12 17:49:26 UTC",
      "updated_date": "2024-10-12 17:49:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:39:22.795137"
    },
    {
      "arxiv_id": "2410.09597v2",
      "title": "A Complete Characterization of Learnability for Stochastic Noisy Bandits",
      "title_zh": "随机噪声多臂老虎机的可学习性完整表征",
      "authors": [
        "Steve Hanneke",
        "Kun Wang"
      ],
      "abstract": "We study the stochastic noisy bandit problem with an unknown reward function\n$f^*$ in a known function class $\\mathcal{F}$. Formally, a model $M$ maps arms\n$\\pi$ to a probability distribution $M(\\pi)$ of reward. A model class\n$\\mathcal{M}$ is a collection of models. For each model $M$, define its mean\nreward function $f^M(\\pi)=\\mathbb{E}_{r \\sim M(\\pi)}[r]$. In the bandit\nlearning problem, we proceed in rounds, pulling one arm $\\pi$ each round and\nobserving a reward sampled from $M(\\pi)$. With knowledge of $\\mathcal{M}$,\nsupposing that the true model $M\\in \\mathcal{M}$, the objective is to identify\nan arm $\\hat{\\pi}$ of near-maximal mean reward $f^M(\\hat{\\pi})$ with high\nprobability in a bounded number of rounds. If this is possible, then the model\nclass is said to be learnable.\n  Importantly, a result of \\cite{hanneke2023bandit} shows there exist model\nclasses for which learnability is undecidable. However, the model class they\nconsider features deterministic rewards, and they raise the question of whether\nlearnability is decidable for classes containing sufficiently noisy models. For\nthe first time, we answer this question in the positive by giving a complete\ncharacterization of learnability for model classes with arbitrary noise. In\naddition to that, we also describe the full spectrum of possible optimal query\ncomplexities. Further, we prove adaptivity is sometimes necessary to achieve\nthe optimal query complexity. Last, we revisit an important complexity measure\nfor interactive decision making, the Decision-Estimation-Coefficient\n\\citep{foster2021statistical,foster2023tight}, and propose a new variant of the\nDEC which also characterizes learnability in this setting.",
      "tldr_zh": "本论文完全表征了随机噪声博弈(stochastic noisy bandits)中模型类的可学习性(learnability)，针对未知奖励函数和函数类$\\mathcal{F}$，通过拉臂博弈过程识别近似最优臂。研究首次证明，对于包含任意噪声的模型类，可学习性是可判定的，并描述了可能的最优查询复杂度(query complexities)，同时证明了在某些情况下自适应性(adaptivity)是必要的。论文还重新审视了Decision-Estimation-Coefficient (DEC)并提出一个新变体，用于精确表征这种交互决策设置下的可学习性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09597v2",
      "published_date": "2024-10-12 17:23:34 UTC",
      "updated_date": "2025-01-17 00:25:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:39:35.153949"
    },
    {
      "arxiv_id": "2410.09592v1",
      "title": "ControLRM: Fast and Controllable 3D Generation via Large Reconstruction Model",
      "title_zh": "翻译失败",
      "authors": [
        "Hongbin Xu",
        "Weitao Chen",
        "Zhipeng Zhou",
        "Feng Xiao",
        "Baigui Sun",
        "Mike Zheng Shou",
        "Wenxiong Kang"
      ],
      "abstract": "Despite recent advancements in 3D generation methods, achieving\ncontrollability still remains a challenging issue. Current approaches utilizing\nscore-distillation sampling are hindered by laborious procedures that consume a\nsignificant amount of time. Furthermore, the process of first generating 2D\nrepresentations and then mapping them to 3D lacks internal alignment between\nthe two forms of representation. To address these challenges, we introduce\nControLRM, an end-to-end feed-forward model designed for rapid and controllable\n3D generation using a large reconstruction model (LRM). ControLRM comprises a\n2D condition generator, a condition encoding transformer, and a triplane\ndecoder transformer. Instead of training our model from scratch, we advocate\nfor a joint training framework. In the condition training branch, we lock the\ntriplane decoder and reuses the deep and robust encoding layers pretrained with\nmillions of 3D data in LRM. In the image training branch, we unlock the\ntriplane decoder to establish an implicit alignment between the 2D and 3D\nrepresentations. To ensure unbiased evaluation, we curate evaluation samples\nfrom three distinct datasets (G-OBJ, GSO, ABO) rather than relying on\ncherry-picking manual generation. The comprehensive experiments conducted on\nquantitative and qualitative comparisons of 3D controllability and generation\nquality demonstrate the strong generalization capacity of our proposed\napproach.",
      "tldr_zh": "该论文提出ControLRM，一种基于Large Reconstruction Model (LRM)的端到端feed-forward模型，用于实现快速和可控的3D生成，解决了现有score-distillation sampling方法耗时长和2D-3D表示缺乏内部对齐的问题。ControLRM包括2D condition generator、condition encoding transformer和triplane decoder transformer，并采用联合训练框架：在condition training branch中锁定triplane decoder并重用LRM的预训练层，在image training branch中解锁以建立隐式对齐。实验在G-OBJ、GSO和ABO数据集上进行定量和定性比较，结果显示ControLRM在3D可控性和生成质量上具有强泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Draft version. This paper is still in submission. For access to our\n  project page and code, please visit:\n  https://toughstonex.github.io/controlrm.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2410.09592v1",
      "published_date": "2024-10-12 16:47:20 UTC",
      "updated_date": "2024-10-12 16:47:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:39:47.567009"
    },
    {
      "arxiv_id": "2410.09584v1",
      "title": "Toward General Instruction-Following Alignment for Retrieval-Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Guanting Dong",
        "Xiaoshuai Song",
        "Yutao Zhu",
        "Runqi Qiao",
        "Zhicheng Dou",
        "Ji-Rong Wen"
      ],
      "abstract": "Following natural instructions is crucial for the effective application of\nRetrieval-Augmented Generation (RAG) systems. Despite recent advancements in\nLarge Language Models (LLMs), research on assessing and improving\ninstruction-following (IF) alignment within the RAG domain remains limited. To\naddress this issue, we propose VIF-RAG, the first automated, scalable, and\nverifiable synthetic pipeline for instruction-following alignment in RAG\nsystems. We start by manually crafting a minimal set of atomic instructions\n(<100) and developing combination rules to synthesize and verify complex\ninstructions for a seed set. We then use supervised models for instruction\nrewriting while simultaneously generating code to automate the verification of\ninstruction quality via a Python executor. Finally, we integrate these\ninstructions with extensive RAG and general data samples, scaling up to a\nhigh-quality VIF-RAG-QA dataset (>100k) through automated processes. To further\nbridge the gap in instruction-following auto-evaluation for RAG systems, we\nintroduce FollowRAG Benchmark, which includes approximately 3K test samples,\ncovering 22 categories of general instruction constraints and four\nknowledge-intensive QA datasets. Due to its robust pipeline design, FollowRAG\ncan seamlessly integrate with different RAG benchmarks. Using FollowRAG and\neight widely-used IF and foundational abilities benchmarks for LLMs, we\ndemonstrate that VIF-RAG markedly enhances LLM performance across a broad range\nof general instruction constraints while effectively leveraging its\ncapabilities in RAG scenarios. Further analysis offers practical insights for\nachieving IF alignment in RAG systems. Our code and datasets are released at\nhttps://FollowRAG.github.io.",
      "tldr_zh": "该论文针对检索增强生成 (RAG) 系统中的指令遵循 (instruction-following) 对齐问题，提出 VIF-RAG 管道，这是首个自动化、可扩展且可验证的合成框架。通过手动创建少量原子指令 (<100) 并结合监督模型重写和 Python 执行器自动化验证，该框架生成了大规模高质量数据集 (>100k)。论文还引入 FollowRAG Benchmark，包括约 3K 测试样本，覆盖 22 类指令约束和四个知识密集型 QA 数据集，用于评估 RAG 系统的性能。实验结果表明，VIF-RAG 显著提升了大型语言模型 (LLMs) 在一般指令约束和 RAG 场景下的表现，并提供了实际优化洞见。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Working in progress",
      "pdf_url": "http://arxiv.org/pdf/2410.09584v1",
      "published_date": "2024-10-12 16:30:51 UTC",
      "updated_date": "2024-10-12 16:30:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:39:59.642107"
    },
    {
      "arxiv_id": "2410.09582v1",
      "title": "Improving 3D Finger Traits Recognition via Generalizable Neural Rendering",
      "title_zh": "通过可泛化的神经渲染改进 3D 手指特征识别",
      "authors": [
        "Hongbin Xu",
        "Junduan Huang",
        "Yuer Ma",
        "Zifeng Li",
        "Wenxiong Kang"
      ],
      "abstract": "3D biometric techniques on finger traits have become a new trend and have\ndemonstrated a powerful ability for recognition and anti-counterfeiting.\nExisting methods follow an explicit 3D pipeline that reconstructs the models\nfirst and then extracts features from 3D models. However, these explicit 3D\nmethods suffer from the following problems: 1) Inevitable information dropping\nduring 3D reconstruction; 2) Tight coupling between specific hardware and\nalgorithm for 3D reconstruction. It leads us to a question: Is it indispensable\nto reconstruct 3D information explicitly in recognition tasks? Hence, we\nconsider this problem in an implicit manner, leaving the nerve-wracking 3D\nreconstruction problem for learnable neural networks with the help of neural\nradiance fields (NeRFs). We propose FingerNeRF, a novel generalizable NeRF for\n3D finger biometrics. To handle the shape-radiance ambiguity problem that may\nresult in incorrect 3D geometry, we aim to involve extra geometric priors based\non the correspondence of binary finger traits like fingerprints or finger\nveins. First, we propose a novel Trait Guided Transformer (TGT) module to\nenhance the feature correspondence with the guidance of finger traits. Second,\nwe involve extra geometric constraints on the volume rendering loss with the\nproposed Depth Distillation Loss and Trait Guided Rendering Loss. To evaluate\nthe performance of the proposed method on different modalities, we collect two\nnew datasets: SCUT-Finger-3D with finger images and SCUT-FingerVein-3D with\nfinger vein images. Moreover, we also utilize the UNSW-3D dataset with\nfingerprint images for evaluation. In experiments, our FingerNeRF can achieve\n4.37% EER on SCUT-Finger-3D dataset, 8.12% EER on SCUT-FingerVein-3D dataset,\nand 2.90% EER on UNSW-3D dataset, showing the superiority of the proposed\nimplicit method in 3D finger biometrics.",
      "tldr_zh": "本文提出了一种基于 Generalizable Neural Rendering 的隐式方法来改善 3D 指纹特征识别，避开了传统显式 3D 重建的弊端，如信息丢失和硬件依赖问题。作者开发了 FingerNeRF，利用 Neural Radiance Fields (NeRFs) 并引入 Trait Guided Transformer (TGT) 模块、Depth Distillation Loss 和 Trait Guided Rendering Loss 等技术，以几何先验增强特征对应和渲染准确性。实验在自收集的 SCUT-Finger-3D (EER 4.37%)、SCUT-FingerVein-3D (EER 8.12%) 和 UNSW-3D (EER 2.90%) 数据集上显示，该方法显著提升了 3D 指纹生物识别的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper is accepted in IJCV. For further information and access to\n  the code, please visit our project page:\n  https://scut-bip-lab.github.io/fingernerf/",
      "pdf_url": "http://arxiv.org/pdf/2410.09582v1",
      "published_date": "2024-10-12 16:27:21 UTC",
      "updated_date": "2024-10-12 16:27:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:40:11.785857"
    },
    {
      "arxiv_id": "2410.19764v2",
      "title": "Unraveling Movie Genres through Cross-Attention Fusion of Bi-Modal Synergy of Poster",
      "title_zh": "翻译失败",
      "authors": [
        "Utsav Kumar Nareti",
        "Chandranath Adak",
        "Soumi Chattopadhyay",
        "Pichao Wang"
      ],
      "abstract": "Movie posters are not just decorative; they are meticulously designed to\ncapture the essence of a movie, such as its genre, storyline, and tone/vibe.\nFor decades, movie posters have graced cinema walls, billboards, and now our\ndigital screens as a form of digital posters. Movie genre classification plays\na pivotal role in film marketing, audience engagement, and recommendation\nsystems. Previous explorations into movie genre classification have been mostly\nexamined in plot summaries, subtitles, trailers and movie scenes. Movie posters\nprovide a pre-release tantalizing glimpse into a film's key aspects, which can\nignite public interest. In this paper, we presented the framework that exploits\nmovie posters from a visual and textual perspective to address the multilabel\nmovie genre classification problem. Firstly, we extracted text from movie\nposters using an OCR and retrieved the relevant embedding. Next, we introduce a\ncross-attention-based fusion module to allocate attention weights to visual and\ntextual embedding. In validating our framework, we utilized 13882 posters\nsourced from the Internet Movie Database (IMDb). The outcomes of the\nexperiments indicate that our model exhibited promising performance and\noutperformed even some prominent contemporary architectures.",
      "tldr_zh": "这篇论文提出了一种框架，利用电影海报的视觉和文本特征来解决多标签电影类型分类问题，强调海报在捕捉电影类型、故事情节和基调方面的作用。方法包括使用 OCR 提取海报文本并获取相关嵌入，随后引入 cross-attention-based fusion module 来融合视觉和文本嵌入，从而分配注意力权重。实验在 13882 个从 IMDb  sourced 的海报上进行，结果显示该模型的表现超过了某些当代架构，为电影营销和推荐系统提供了改进潜力。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.19764v2",
      "published_date": "2024-10-12 16:14:18 UTC",
      "updated_date": "2024-11-30 07:06:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:40:22.478797"
    },
    {
      "arxiv_id": "2410.09579v1",
      "title": "Structure of Artificial Neural Networks -- Empirical Investigations",
      "title_zh": "人工神经网络",
      "authors": [
        "Julian Stier"
      ],
      "abstract": "Within one decade, Deep Learning overtook the dominating solution methods of\ncountless problems of artificial intelligence. ``Deep'' refers to the deep\narchitectures with operations in manifolds of which there are no immediate\nobservations. For these deep architectures some kind of structure is\npre-defined -- but what is this structure? With a formal definition for\nstructures of neural networks, neural architecture search problems and solution\nmethods can be formulated under a common framework. Both practical and\ntheoretical questions arise from closing the gap between applied neural\narchitecture search and learning theory. Does structure make a difference or\ncan it be chosen arbitrarily?\n  This work is concerned with deep structures of artificial neural networks and\nexamines automatic construction methods under empirical principles to shed\nlight on to the so called ``black-box models''.\n  Our contributions include a formulation of graph-induced neural networks that\nis used to pose optimisation problems for neural architecture. We analyse\nstructural properties for different neural network objectives such as\ncorrectness, robustness or energy consumption and discuss how structure affects\nthem. Selected automation methods for neural architecture optimisation problems\nare discussed and empirically analysed. With the insights gained from\nformalising graph-induced neural networks, analysing structural properties and\ncomparing the applicability of neural architecture search methods qualitatively\nand quantitatively we advance these methods in two ways. First, new predictive\nmodels are presented for replacing computationally expensive evaluation\nschemes, and second, new generative models for informed sampling during neural\narchitecture search are analysed and discussed.",
      "tldr_zh": "本研究探讨了人工神经网络的结构，针对深度学习（Deep Learning）架构的预定义特性，通过正式定义和经验调查来揭示这些“黑箱模型”的本质。作者提出图诱导神经网络（Graph-induced Neural Networks）的公式化框架，用于制定神经架构搜索（Neural Architecture Search）优化问题，并分析结构对网络目标（如正确性、鲁棒性或能耗）的具体影响。实验结果显示，结构选择并非任意，而是显著影响性能；此外，该工作引入新的预测模型和生成模型，以优化架构搜索过程，提高效率和适用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "PhD thesis",
      "pdf_url": "http://arxiv.org/pdf/2410.09579v1",
      "published_date": "2024-10-12 16:13:28 UTC",
      "updated_date": "2024-10-12 16:13:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:40:35.126817"
    },
    {
      "arxiv_id": "2410.09576v1",
      "title": "The Future of Learning in the Age of Generative AI: Automated Question Generation and Assessment with Large Language Models",
      "title_zh": "生成式 AI 时代下的学习未来：使用大语言模型的自动问题生成和评估",
      "authors": [
        "Subhankar Maity",
        "Aniket Deroy"
      ],
      "abstract": "In recent years, large language models (LLMs) and generative AI have\nrevolutionized natural language processing (NLP), offering unprecedented\ncapabilities in education. This chapter explores the transformative potential\nof LLMs in automated question generation and answer assessment. It begins by\nexamining the mechanisms behind LLMs, emphasizing their ability to comprehend\nand generate human-like text. The chapter then discusses methodologies for\ncreating diverse, contextually relevant questions, enhancing learning through\ntailored, adaptive strategies. Key prompting techniques, such as zero-shot and\nchain-of-thought prompting, are evaluated for their effectiveness in generating\nhigh-quality questions, including open-ended and multiple-choice formats in\nvarious languages. Advanced NLP methods like fine-tuning and prompt-tuning are\nexplored for their role in generating task-specific questions, despite\nassociated costs. The chapter also covers the human evaluation of generated\nquestions, highlighting quality variations across different methods and areas\nfor improvement. Furthermore, it delves into automated answer assessment,\ndemonstrating how LLMs can accurately evaluate responses, provide constructive\nfeedback, and identify nuanced understanding or misconceptions. Examples\nillustrate both successful assessments and areas needing improvement. The\ndiscussion underscores the potential of LLMs to replace costly, time-consuming\nhuman assessments when appropriately guided, showcasing their advanced\nunderstanding and reasoning capabilities in streamlining educational processes.",
      "tldr_zh": "本研究探讨了大型语言模型 (LLMs) 和生成式 AI 在教育领域的变革潜力，焦点在于自动生成问题和评估答案。论文分析了 LLMs 的机制及其生成人类-like 文本的能力，并介绍了关键方法，如 zero-shot prompting 和 chain-of-thought prompting，用于创建多样化、上下文相关的问题，包括 open-ended 和 multiple-choice 格式。高级 NLP 技术如 fine-tuning 和 prompt-tuning 被评估，以提高问题生成的质量，尽管存在成本挑战。研究还强调 LLMs 在自动评估答案、提供建设性反馈和识别误解方面的效能，可能取代昂贵的人工评估，从而优化教育过程。实验实例展示了其优势，但也指出了改进空间。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Book Chapter (Under Review)",
      "pdf_url": "http://arxiv.org/pdf/2410.09576v1",
      "published_date": "2024-10-12 15:54:53 UTC",
      "updated_date": "2024-10-12 15:54:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:41:40.331734"
    },
    {
      "arxiv_id": "2410.09575v2",
      "title": "Reconstructive Visual Instruction Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Haochen Wang",
        "Anlin Zheng",
        "Yucheng Zhao",
        "Tiancai Wang",
        "Zheng Ge",
        "Xiangyu Zhang",
        "Zhaoxiang Zhang"
      ],
      "abstract": "This paper introduces reconstructive visual instruction tuning (ROSS), a\nfamily of Large Multimodal Models (LMMs) that exploit vision-centric\nsupervision signals. In contrast to conventional visual instruction tuning\napproaches that exclusively supervise text outputs, ROSS prompts LMMs to\nsupervise visual outputs via reconstructing input images. By doing so, it\ncapitalizes on the inherent richness and detail present within input images\nthemselves, which are often lost in pure text supervision. However, producing\nmeaningful feedback from natural images is challenging due to the heavy spatial\nredundancy of visual signals. To address this issue, ROSS employs a denoising\nobjective to reconstruct latent representations of input images, avoiding\ndirectly regressing exact raw RGB values. This intrinsic activation design\ninherently encourages LMMs to maintain image detail, thereby enhancing their\nfine-grained comprehension capabilities and reducing hallucinations.\nEmpirically, ROSS consistently brings significant improvements across different\nvisual encoders and language models. In comparison with extrinsic assistance\nstate-of-the-art alternatives that aggregate multiple visual experts, ROSS\ndelivers competitive performance with a single SigLIP visual encoder,\ndemonstrating the efficacy of our vision-centric supervision tailored for\nvisual outputs.",
      "tldr_zh": "这篇论文提出了 Reconstructive Visual Instruction Tuning (ROSS)，一种基于视觉中心监督信号的 Large Multimodal Models (LMMs)，通过重建输入图像的潜在表示来监督视觉输出，而非仅依赖文本监督，从而利用图像的丰富细节。ROSS 采用 denoising objective 来处理视觉信号的空间冗余，避免直接回归 RGB 值，这有助于提升模型的细粒度理解能力和减少幻觉。实验结果显示，ROSS 在不同视觉编码器和语言模型上实现了显著性能提升，使用单个 SigLIP 视觉编码器即可与整合多视觉专家的 SOTA 方法竞争。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09575v2",
      "published_date": "2024-10-12 15:54:29 UTC",
      "updated_date": "2024-12-31 02:38:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:40:58.855247"
    },
    {
      "arxiv_id": "2410.09569v2",
      "title": "Are You Human? An Adversarial Benchmark to Expose LLMs",
      "title_zh": "你是人类吗？一个对抗性基准来暴露 LLMs",
      "authors": [
        "Gilad Gressel",
        "Rahul Pankajakshan",
        "Yisroel Mirsky"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated an alarming ability to\nimpersonate humans in conversation, raising concerns about their potential\nmisuse in scams and deception. Humans have a right to know if they are\nconversing to an LLM. We evaluate text-based prompts designed as challenges to\nexpose LLM imposters in real-time. To this end we compile and release an\nopen-source benchmark dataset that includes 'implicit challenges' that exploit\nan LLM's instruction-following mechanism to cause role deviation, and 'exlicit\nchallenges' that test an LLM's ability to perform simple tasks typically easy\nfor humans but difficult for LLMs. Our evaluation of 9 leading models from the\nLMSYS leaderboard revealed that explicit challenges successfully detected LLMs\nin 78.4% of cases, while implicit challenges were effective in 22.9% of\ninstances. User studies validate the real-world applicability of our methods,\nwith humans outperforming LLMs on explicit challenges (78% vs 22% success\nrate). Our framework unexpectedly revealed that many study participants were\nusing LLMs to complete tasks, demonstrating its effectiveness in detecting both\nAI impostors and human misuse of AI tools. This work addresses the critical\nneed for reliable, real-time LLM detection methods in high-stakes\nconversations.",
      "tldr_zh": "这篇论文提出了一种对抗性基准（adversarial benchmark），用于实时检测大型语言模型（LLMs）是否在模仿人类对话，以防范潜在的欺诈风险。\n他们编译了一个开源数据集，包括隐式挑战（exploit an LLM's instruction-following mechanism to cause role deviation）和显式挑战（test simple tasks that are easy for humans but difficult for LLMs）。\n在评估9个领先LLM模型时，显式挑战的检测成功率达到78.4%，而隐式挑战的有效率为22.9%。\n用户研究显示，人类在显式挑战上明显优于LLMs（78% vs 22%成功率），并意外发现许多参与者滥用LLMs来完成任务。\n这项工作强调了在高风险对话中开发可靠的LLM检测方法的重要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09569v2",
      "published_date": "2024-10-12 15:33:50 UTC",
      "updated_date": "2024-12-20 12:25:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:41:21.907563"
    },
    {
      "arxiv_id": "2410.09564v1",
      "title": "Extended Japanese Commonsense Morality Dataset with Masked Token and Label Enhancement",
      "title_zh": "翻译失败",
      "authors": [
        "Takumi Ohashi",
        "Tsubasa Nakagawa",
        "Hitoshi Iyatomi"
      ],
      "abstract": "Rapid advancements in artificial intelligence (AI) have made it crucial to\nintegrate moral reasoning into AI systems. However, existing models and\ndatasets often overlook regional and cultural differences. To address this\nshortcoming, we have expanded the JCommonsenseMorality (JCM) dataset, the only\npublicly available dataset focused on Japanese morality. The Extended JCM\n(eJCM) has grown from the original 13,975 sentences to 31,184 sentences using\nour proposed sentence expansion method called Masked Token and Label\nEnhancement (MTLE). MTLE selectively masks important parts of sentences related\nto moral judgment and replaces them with alternative expressions generated by a\nlarge language model (LLM), while re-assigning appropriate labels. The model\ntrained using our eJCM achieved an F1 score of 0.857, higher than the scores\nfor the original JCM (0.837), ChatGPT one-shot classification (0.841), and data\naugmented using AugGPT, a state-of-the-art augmentation method (0.850).\nSpecifically, in complex moral reasoning tasks unique to Japanese culture, the\nmodel trained with eJCM showed a significant improvement in performance\n(increasing from 0.681 to 0.756) and achieved a performance close to that of\nGPT-4 Turbo (0.787). These results demonstrate the validity of the eJCM dataset\nand the importance of developing models and datasets that consider the cultural\ncontext.",
      "tldr_zh": "本研究扩展了 JCommonsenseMorality (JCM) 数据集，创建了 Extended JCM (eJCM)，将句子数量从 13,975 增加到 31,184，以解决 AI 道德推理中区域文化差异的问题。研究提出 Masked Token and Label Enhancement (MTLE) 方法，通过遮盖句子中与道德判断相关的关键部分，并利用 Large Language Model (LLM) 生成替代表达并重新分配标签，从而增强数据集的多样性。实验结果显示，使用 eJCM 训练的模型 F1 score 达到 0.857，高于原 JCM (0.837)、ChatGPT (0.841) 和其他增强方法 (0.850)，尤其在日本文化特有的复杂道德推理任务上，性能从 0.681 提升至 0.756，接近 GPT-4 Turbo (0.787)。这些发现突出了开发文化敏感数据集的重要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09564v1",
      "published_date": "2024-10-12 15:21:40 UTC",
      "updated_date": "2024-10-12 15:21:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:41:24.709131"
    },
    {
      "arxiv_id": "2410.09543v1",
      "title": "Boltzmann-Aligned Inverse Folding Model as a Predictor of Mutational Effects on Protein-Protein Interactions",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoran Jiao",
        "Weian Mao",
        "Wengong Jin",
        "Peiyuan Yang",
        "Hao Chen",
        "Chunhua Shen"
      ],
      "abstract": "Predicting the change in binding free energy ($\\Delta \\Delta G$) is crucial\nfor understanding and modulating protein-protein interactions, which are\ncritical in drug design. Due to the scarcity of experimental $\\Delta \\Delta G$\ndata, existing methods focus on pre-training, while neglecting the importance\nof alignment. In this work, we propose the Boltzmann Alignment technique to\ntransfer knowledge from pre-trained inverse folding models to $\\Delta \\Delta G$\nprediction. We begin by analyzing the thermodynamic definition of $\\Delta\n\\Delta G$ and introducing the Boltzmann distribution to connect energy with\nprotein conformational distribution. However, the protein conformational\ndistribution is intractable; therefore, we employ Bayes' theorem to circumvent\ndirect estimation and instead utilize the log-likelihood provided by protein\ninverse folding models for $\\Delta \\Delta G$ estimation. Compared to previous\ninverse folding-based methods, our method explicitly accounts for the unbound\nstate of protein complex in the $\\Delta \\Delta G$ thermodynamic cycle,\nintroducing a physical inductive bias and achieving both supervised and\nunsupervised state-of-the-art (SoTA) performance. Experimental results on\nSKEMPI v2 indicate that our method achieves Spearman coefficients of 0.3201\n(unsupervised) and 0.5134 (supervised), significantly surpassing the previously\nreported SoTA values of 0.2632 and 0.4324, respectively. Futhermore, we\ndemonstrate the capability of our method on binding energy prediction,\nprotein-protein docking and antibody optimization tasks.",
      "tldr_zh": "这篇论文提出Boltzmann Alignment技术，用于从预训练的inverse folding models中转移知识，预测蛋白质-蛋白质相互作用的结合自由能变化（ΔΔG），以解决现有方法忽略alignment的问题。方法通过Boltzmann分布和Bayes定理，绕过蛋白构象分布的直接估计，并明确考虑蛋白复合物的unbound状态，引入物理归纳偏差，从而提升预测准确性。在SKEMPI v2数据集上，该方法实现了state-of-the-art性能，有监督Spearman系数达0.5134、无监督达0.3201，显著优于先前最佳值；此外，它还证明了在binding energy prediction、protein-protein docking和antibody optimization任务中的有效应用。",
      "categories": [
        "cs.CE",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09543v1",
      "published_date": "2024-10-12 14:13:42 UTC",
      "updated_date": "2024-10-12 14:13:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:41:35.682926"
    },
    {
      "arxiv_id": "2410.09542v2",
      "title": "MIRAGE: Evaluating and Explaining Inductive Reasoning Process in Language Models",
      "title_zh": "MIRAGE：评估和解释语言模型中的归纳推理过程",
      "authors": [
        "Jiachun Li",
        "Pengfei Cao",
        "Zhuoran Jin",
        "Yubo Chen",
        "Kang Liu",
        "Jun Zhao"
      ],
      "abstract": "Inductive reasoning is an essential capability for large language models\n(LLMs) to achieve higher intelligence, which requires the model to generalize\nrules from observed facts and then apply them to unseen examples. We present\nMIRAGE, a synthetic dataset that addresses the limitations of previous work,\nspecifically the lack of comprehensive evaluation and flexible test data. In\nit, we evaluate LLMs' capabilities in both the inductive and deductive stages,\nallowing for flexible variation in input distribution, task scenario, and task\ndifficulty to analyze the factors influencing LLMs' inductive reasoning. Based\non these multi-faceted evaluations, we demonstrate that the LLM is a poor\nrule-based reasoner. In many cases, when conducting inductive reasoning, they\ndo not rely on a correct rule to answer the unseen case. From the perspectives\nof different prompting methods, observation numbers, and task forms, models\ntend to consistently conduct correct deduction without correct inductive rules.\nBesides, we find that LLMs are good neighbor-based reasoners. In the inductive\nreasoning process, the model tends to focus on observed facts that are close to\nthe current test example in feature space. By leveraging these similar\nexamples, the model maintains strong inductive capabilities within a localized\nregion, significantly improving its deductive performance.",
      "tldr_zh": "本研究提出 MIRAGE，这是一个合成数据集，用于评估和解释大语言模型 (LLMs) 在归纳推理 (inductive reasoning) 过程中的能力，解决现有工作的全面性不足和测试数据灵活性问题。MIRAGE 通过灵活调整输入分布、任务场景和难度，评估 LLMs 在归纳和演绎 (deductive) 阶段的表现，包括不同提示方法、观察数量和任务形式的影响。结果显示，LLMs 不是有效的基于规则的推理器，往往在缺乏正确归纳规则的情况下进行正确演绎；相反，它们擅长基于邻居 (neighbor-based) 推理，通过关注特征空间中与测试示例相似的观察事实，在局部区域提升归纳和演绎性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted as ICLR 2025 conference paper (26 pages, 16 tables, 9\n  figures)",
      "pdf_url": "http://arxiv.org/pdf/2410.09542v2",
      "published_date": "2024-10-12 14:12:36 UTC",
      "updated_date": "2025-02-28 08:01:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:42:32.431169"
    },
    {
      "arxiv_id": "2410.09541v1",
      "title": "LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Jiachun Li",
        "Pengfei Cao",
        "Chenhao Wang",
        "Zhuoran Jin",
        "Yubo Chen",
        "Kang Liu",
        "Xiaojian Jiang",
        "Jiexin Xu",
        "Jun Zhao"
      ],
      "abstract": "Large language models (LLMs) sometimes demonstrate poor performance on\nknowledge-intensive tasks, commonsense reasoning is one of them. Researchers\ntypically address these issues by retrieving related knowledge from knowledge\ngraphs or employing self-enhancement methods to elicit knowledge in LLMs.\nHowever, noisy knowledge and invalid reasoning issues hamper their ability to\nanswer questions accurately. To this end, we propose a novel method named\neliciting, filtering and integrating knowledge in large language model\n(LINKED). In it, we design a reward model to filter out the noisy knowledge and\ntake the marginal consistent reasoning module to reduce invalid reasoning. With\nour comprehensive experiments on two complex commonsense reasoning benchmarks,\nour method outperforms SOTA baselines (up to 9.0% improvement of accuracy).\nBesides, to measure the positive and negative impact of the injected knowledge,\nwe propose a new metric called effectiveness-preservation score for the\nknowledge enhancement works. Finally, through extensive experiments, we conduct\nan in-depth analysis and find many meaningful conclusions about LLMs in\ncommonsense reasoning tasks.",
      "tldr_zh": "该研究针对大型语言模型 (LLMs) 在常识推理任务中的表现不足问题，提出了一种名为 LINKED 的新方法，通过知识提取、过滤和整合来提升模型性能。具体而言，LINKED 利用奖励模型过滤噪声知识，并引入边缘一致性推理模块以减少无效推理。在两个复杂常识推理基准上的实验中，该方法比现有最先进基线 (SOTA baselines) 准确率提高了高达 9.0%。此外，研究还提出 effectiveness-preservation score 指标来评估注入知识的积极与消极影响，并通过广泛实验得出多项有意义的结论。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by EMNLP 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2410.09541v1",
      "published_date": "2024-10-12 14:12:22 UTC",
      "updated_date": "2024-10-12 14:12:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:42:06.716708"
    },
    {
      "arxiv_id": "2410.09531v1",
      "title": "PrivQuant: Communication-Efficient Private Inference with Quantized Network/Protocol Co-Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Tianshi Xu",
        "Shuzhang Zhong",
        "Wenxuan Zeng",
        "Runsheng Wang",
        "Meng Li"
      ],
      "abstract": "Private deep neural network (DNN) inference based on secure two-party\ncomputation (2PC) enables secure privacy protection for both the server and the\nclient. However, existing secure 2PC frameworks suffer from a high inference\nlatency due to enormous communication. As the communication of both linear and\nnon-linear DNN layers reduces with the bit widths of weight and activation, in\nthis paper, we propose PrivQuant, a framework that jointly optimizes the\n2PC-based quantized inference protocols and the network quantization algorithm,\nenabling communication-efficient private inference. PrivQuant proposes DNN\narchitecture-aware optimizations for the 2PC protocols for\ncommunication-intensive quantized operators and conducts graph-level operator\nfusion for communication reduction. Moreover, PrivQuant also develops a\ncommunication-aware mixed precision quantization algorithm to improve inference\nefficiency while maintaining high accuracy. The network/protocol\nco-optimization enables PrivQuant to outperform prior-art 2PC frameworks. With\nextensive experiments, we demonstrate PrivQuant reduces communication by\n$11\\times, 2.5\\times \\mathrm{and}~ 2.8\\times$, which results in $8.7\\times,\n1.8\\times ~ \\mathrm{and}~ 2.4\\times$ latency reduction compared with SiRNN,\nCOINN, and CoPriv, respectively.",
      "tldr_zh": "该论文提出PrivQuant框架，用于优化基于安全双向计算(2PC)的私有深度神经网络(DNN)推理，通过量化网络和协议的联合优化来显著减少通信量。PrivQuant引入DNN架构感知优化、针对通信密集型量化操作的2PC协议改进以及图级操作融合，并开发通信感知的混合精度量化算法，以平衡推理效率和准确性。实验结果显示，与SiRNN、COINN和CoPriv相比，PrivQuant分别减少通信11倍、2.5倍和2.8倍，从而实现推理延迟的8.7倍、1.8倍和2.4倍降低。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "ICCAD 2024",
      "pdf_url": "http://arxiv.org/pdf/2410.09531v1",
      "published_date": "2024-10-12 13:28:42 UTC",
      "updated_date": "2024-10-12 13:28:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:42:16.862871"
    },
    {
      "arxiv_id": "2410.09529v1",
      "title": "Preserving Old Memories in Vivid Detail: Human-Interactive Photo Restoration Framework",
      "title_zh": "以生动细节保存旧回忆：人机交互照片修复框架",
      "authors": [
        "Seung-Yeon Back",
        "Geonho Son",
        "Dahye Jeong",
        "Eunil Park",
        "Simon S. Woo"
      ],
      "abstract": "Photo restoration technology enables preserving visual memories in\nphotographs. However, physical prints are vulnerable to various forms of\ndeterioration, ranging from physical damage to loss of image quality, etc.\nWhile restoration by human experts can improve the quality of outcomes, it\noften comes at a high price in terms of cost and time for restoration. In this\nwork, we present the AI-based photo restoration framework composed of multiple\nstages, where each stage is tailored to enhance and restore specific types of\nphoto damage, accelerating and automating the photo restoration process. By\nintegrating these techniques into a unified architecture, our framework aims to\noffer a one-stop solution for restoring old and deteriorated photographs.\nFurthermore, we present a novel old photo restoration dataset because we lack a\npublicly available dataset for our evaluation.",
      "tldr_zh": "这篇论文提出了一种人类交互式照片修复框架（Human-Interactive Photo Restoration Framework），旨在自动化修复老照片的物理损坏和图像质量损失问题，以替代昂贵且耗时的专业人工修复。\n框架由多个阶段组成，每个阶段针对特定类型损坏进行定制化增强和修复，并整合成一个统一架构，提供高效的一站式解决方案。\n作者还创建了一个新的老照片修复数据集（old photo restoration dataset），用于评估框架性能，并推动相关研究的发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09529v1",
      "published_date": "2024-10-12 13:23:08 UTC",
      "updated_date": "2024-10-12 13:23:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:42:27.942781"
    },
    {
      "arxiv_id": "2410.09528v2",
      "title": "Boosting Deductive Reasoning with Step Signals In RLHF",
      "title_zh": "翻译失败",
      "authors": [
        "Jialian Li",
        "Yipin Zhang",
        "Wei Shen",
        "Yuzi Yan",
        "Jian Xie",
        "Dong Yan"
      ],
      "abstract": "Logical reasoning is a crucial task for Large Language Models (LLMs),\nenabling them to tackle complex problems. Among reasoning tasks, multi-step\nreasoning poses a particular challenge. Grounded in the theory of formal logic,\nwe have developed an automated method, Multi-step Deduction (MuseD), for\ndeductive reasoning data. MuseD has allowed us to create training and testing\ndatasets for multi-step reasoning. Our generation method enables control over\nthe complexity of the generated instructions, facilitating training and\nevaluation of models across different difficulty levels. Through RLHF training,\nour training data has demonstrated significant improvements in logical\ncapabilities for both in-domain of out-of-domain reasoning tasks. Additionally,\nwe have conducted tests to assess the multi-step reasoning abilities of various\nmodels.",
      "tldr_zh": "这篇论文提出了一种自动化方法Multi-step Deduction (MuseD)，基于形式逻辑理论生成多步演绎推理数据，以提升Large Language Models (LLMs)的逻辑推理能力。MuseD允许控制指令的复杂性，从而创建不同难度级别的训练和测试数据集，便于模型的训练和评估。通过RLHF (Reinforcement Learning from Human Feedback)训练，这些数据显著改善了模型在领域内和领域外推理任务的表现。研究者还测试了多种模型的多步推理能力，验证了方法的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09528v2",
      "published_date": "2024-10-12 13:19:11 UTC",
      "updated_date": "2024-10-24 09:36:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:42:46.685617"
    },
    {
      "arxiv_id": "2410.10896v2",
      "title": "AT-MoE: Adaptive Task-planning Mixture of Experts via LoRA Approach",
      "title_zh": "AT-Mo",
      "authors": [
        "Xurui Li",
        "Juanjuan Yao"
      ],
      "abstract": "The advent of Large Language Models (LLMs) has ushered in a new era of\nartificial intelligence, with the potential to transform various sectors\nthrough automation and insightful analysis. The Mixture of Experts (MoE)\narchitecture has been proposed as a solution to enhance model performance in\ncomplex tasks. Yet, existing MoE models struggle with task-specific learning\nand interpretability, especially in fields like medicine where precision is\ncritical. This paper introduces the Adaptive Task-planing Mixture of\nExperts(AT-MoE), an innovative architecture designed to address these\nlimitations. We first train task-specific experts via LoRA approach to enhance\nproblem-solving capabilities and interpretability in specialized areas.\nSubsequently, we introduce a layer-wise adaptive grouped routing module that\noptimizes module fusion based on complex task instructions, ensuring optimal\ntask resolution. The grouped routing module first perform overall weight\nallocation from the dimension of the expert group, and then conduct local\nweight normalization adjustments within the group. This design maintains\nmulti-dimensional balance, controllability, and interpretability, while\nfacilitating task-specific fusion in response to complex instructions.",
      "tldr_zh": "该论文提出 AT-MoE，一种创新的混合专家架构（Mixture of Experts），旨在解决现有 MoE 模型在任务特定学习和可解释性上的不足，特别是适用于医学等高精度领域。研究首先通过 LoRA 方法训练任务特定的 experts，以提升问题解决能力和模型可解释性；随后引入层级自适应分组路由模块（layer-wise adaptive grouped routing），该模块从专家组维度进行整体权重分配，并进行组内局部权重归一化调整，实现对复杂任务指令的优化融合。AT-MoE 通过这种设计，确保了多维平衡、可控性和可解释性，从而在复杂任务处理中显著提高了性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.10896v2",
      "published_date": "2024-10-12 13:03:15 UTC",
      "updated_date": "2024-10-19 02:24:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:42:58.279949"
    },
    {
      "arxiv_id": "2410.09519v1",
      "title": "Pic@Point: Cross-Modal Learning by Local and Global Point-Picture Correspondence",
      "title_zh": "翻译失败",
      "authors": [
        "Vencia Herzog",
        "Stefan Suwelack"
      ],
      "abstract": "Self-supervised pre-training has achieved remarkable success in NLP and 2D\nvision. However, these advances have yet to translate to 3D data. Techniques\nlike masked reconstruction face inherent challenges on unstructured point\nclouds, while many contrastive learning tasks lack in complexity and\ninformative value. In this paper, we present Pic@Point, an effective\ncontrastive learning method based on structural 2D-3D correspondences. We\nleverage image cues rich in semantic and contextual knowledge to provide a\nguiding signal for point cloud representations at various abstraction levels.\nOur lightweight approach outperforms state-of-the-art pre-training methods on\nseveral 3D benchmarks.",
      "tldr_zh": "本论文提出了 Pic@Point，一种基于局部和全局点-图片对应关系的跨模态学习方法，旨在解决自监督预训练在 3D 数据上的挑战，如 masked reconstruction 在非结构化点云中的局限性，以及对比学习任务的复杂性不足。  \n该方法利用图像的语义和上下文知识作为指导信号，对点云表示进行多层次的对比学习，从而提升 3D 特征的抽象和泛化能力。  \n实验结果表明，Pic@Point 作为一种轻量级方法，在多个 3D 基准上超过了最先进的技术。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ACML 2024",
      "pdf_url": "http://arxiv.org/pdf/2410.09519v1",
      "published_date": "2024-10-12 12:43:41 UTC",
      "updated_date": "2024-10-12 12:43:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:43:10.871276"
    },
    {
      "arxiv_id": "2410.09514v1",
      "title": "Eco-Aware Graph Neural Networks for Sustainable Recommendations",
      "title_zh": "面向可持续推荐的生态感知图神经网络",
      "authors": [
        "Antonio Purificato",
        "Fabrizio Silvestri"
      ],
      "abstract": "Recommender systems play a crucial role in alleviating information overload\nby providing personalized recommendations tailored to users' preferences and\ninterests. Recently, Graph Neural Networks (GNNs) have emerged as a promising\napproach for recommender systems, leveraging their ability to effectively\ncapture complex relationships and dependencies between users and items by\nrepresenting them as nodes in a graph structure. In this study, we investigate\nthe environmental impact of GNN-based recommender systems, an aspect that has\nbeen largely overlooked in the literature. Specifically, we conduct a\ncomprehensive analysis of the carbon emissions associated with training and\ndeploying GNN models for recommendation tasks. We evaluate the energy\nconsumption and carbon footprint of different GNN architectures and\nconfigurations, considering factors such as model complexity, training\nduration, hardware specifications and embedding size. By addressing the\nenvironmental impact of resource-intensive algorithms in recommender systems,\nthis study contributes to the ongoing efforts towards sustainable and\nresponsible artificial intelligence, promoting the development of eco-friendly\nrecommendation technologies that balance performance and environmental\nconsiderations. Code is available at:\nhttps://github.com/antoniopurificato/gnn_recommendation_and_environment.",
      "tldr_zh": "该研究探讨了Graph Neural Networks (GNNs)在推荐系统中的环境影响，强调了这些系统在处理用户-物品关系时可能产生的碳排放问题。研究者通过全面分析不同GNN架构的能源消耗和碳足迹，考虑了模型复杂度、训练时长、硬件规格以及嵌入大小等因素。结果显示，此类分析有助于平衡推荐系统的性能与环境可持续性，推动开发更环保的AI技术。代码可在GitHub上获取。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "9 pages, 2 tables, 3 figures, RecSoGood Workshop",
      "pdf_url": "http://arxiv.org/pdf/2410.09514v1",
      "published_date": "2024-10-12 12:26:04 UTC",
      "updated_date": "2024-10-12 12:26:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:43:21.727999"
    },
    {
      "arxiv_id": "2410.09506v1",
      "title": "Distribution-Aware Mean Estimation under User-level Local Differential Privacy",
      "title_zh": "在用户级本地差分隐私下的分布感知均值估计",
      "authors": [
        "Corentin Pla",
        "Hugo Richard",
        "Maxime Vono"
      ],
      "abstract": "We consider the problem of mean estimation under user-level local\ndifferential privacy, where $n$ users are contributing through their local pool\nof data samples. Previous work assume that the number of data samples is the\nsame across users. In contrast, we consider a more general and realistic\nscenario where each user $u \\in [n]$ owns $m_u$ data samples drawn from some\ngenerative distribution $\\mu$; $m_u$ being unknown to the statistician but\ndrawn from a known distribution $M$ over $\\mathbb{N}^\\star$. Based on a\ndistribution-aware mean estimation algorithm, we establish an $M$-dependent\nupper bounds on the worst-case risk over $\\mu$ for the task of mean estimation.\nWe then derive a lower bound. The two bounds are asymptotically matching up to\nlogarithmic factors and reduce to known bounds when $m_u = m$ for any user $u$.",
      "tldr_zh": "本文研究了在用户级本地差分隐私（user-level local differential privacy）下估计均值的问题，考虑了更现实的场景：每个用户拥有从分布 μ 中抽样的 m_u 个数据样本，其中 m_u 未知但来自已知分布 M。论文提出了一种分布感知的均值估计算法（distribution-aware mean estimation algorithm），通过建立 M-dependent 的上界和下界来量化风险。这些界限在渐近上是匹配的（asymptotically matching），并在所有用户样本数量相等时退化到现有已知界限，从而提升了算法的鲁棒性和实用性。",
      "categories": [
        "stat.ME",
        "cs.AI",
        "cs.CR",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "stat.ME",
      "comment": "25 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2410.09506v1",
      "published_date": "2024-10-12 11:57:52 UTC",
      "updated_date": "2024-10-12 11:57:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:43:37.903880"
    },
    {
      "arxiv_id": "2410.09491v1",
      "title": "Dying Clusters Is All You Need -- Deep Clustering With an Unknown Number of Clusters",
      "title_zh": "翻译失败",
      "authors": [
        "Collin Leiber",
        "Niklas Strauß",
        "Matthias Schubert",
        "Thomas Seidl"
      ],
      "abstract": "Finding meaningful groups, i.e., clusters, in high-dimensional data such as\nimages or texts without labeled data at hand is an important challenge in data\nmining. In recent years, deep clustering methods have achieved remarkable\nresults in these tasks. However, most of these methods require the user to\nspecify the number of clusters in advance. This is a major limitation since the\nnumber of clusters is typically unknown if labeled data is unavailable. Thus,\nan area of research has emerged that addresses this problem. Most of these\napproaches estimate the number of clusters separated from the clustering\nprocess. This results in a strong dependency of the clustering result on the\nquality of the initial embedding. Other approaches are tailored to specific\nclustering processes, making them hard to adapt to other scenarios. In this\npaper, we propose UNSEEN, a general framework that, starting from a given upper\nbound, is able to estimate the number of clusters. To the best of our\nknowledge, it is the first method that can be easily combined with various deep\nclustering algorithms. We demonstrate the applicability of our approach by\ncombining UNSEEN with the popular deep clustering algorithms DCN, DEC, and DKM\nand verify its effectiveness through an extensive experimental evaluation on\nseveral image and tabular datasets. Moreover, we perform numerous ablations to\nanalyze our approach and show the importance of its components. The code is\navailable at: https://github.com/collinleiber/UNSEEN",
      "tldr_zh": "该论文解决了深层聚类（deep clustering）在高维数据（如图像或文本）中的挑战，即无需标签数据却需预先指定聚类数量的问题。作者提出了一种通用框架 UNSEEN，从一个给定上界开始动态估计聚类数，并将其与流行算法如 DCN、DEC 和 DKM 结合使用。实验结果显示，UNSEEN 在多个图像和表格数据集上表现出色，通过消融实验验证了其组件的重要性，并提供了开源代码以便进一步应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Acceppted at the Sixth ICDM Workshop on Deep Learning and Clustering",
      "pdf_url": "http://arxiv.org/pdf/2410.09491v1",
      "published_date": "2024-10-12 11:04:10 UTC",
      "updated_date": "2024-10-12 11:04:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:43:46.294537"
    },
    {
      "arxiv_id": "2410.09474v3",
      "title": "Distilling Invariant Representations with Dual Augmentation",
      "title_zh": "通过双重增强蒸馏不变",
      "authors": [
        "Nikolaos Giakoumoglou",
        "Tania Stathaki"
      ],
      "abstract": "Knowledge distillation (KD) has been widely used to transfer knowledge from\nlarge, accurate models (teachers) to smaller, efficient ones (students). Recent\nmethods have explored enforcing consistency by incorporating causal\ninterpretations to distill invariant representations. In this work, we extend\nthis line of research by introducing a dual augmentation strategy to promote\ninvariant feature learning in both teacher and student models. Our approach\nleverages different augmentations applied to both models during distillation,\npushing the student to capture robust, transferable features. This dual\naugmentation strategy complements invariant causal distillation by ensuring\nthat the learned representations remain stable across a wider range of data\nvariations and transformations. Extensive experiments on CIFAR-100 demonstrate\nthe effectiveness of this approach, achieving competitive results in\nsame-architecture KD.",
      "tldr_zh": "这篇论文提出了使用双重增强（Dual Augmentation）策略来提升知识蒸馏（KD）过程中的不变表示学习，旨在从大模型（老师）向小模型（学生）转移更鲁棒的特征。该策略通过在老师和学生模型上应用不同的数据增强，确保学到的表示在各种数据变异和变换下保持稳定，从而补充了不变因果蒸馏方法。实验在 CIFAR-100 数据集上证明了其有效性，在同架构 KD 中取得了竞争性的性能表现。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "68T07",
        "I.4; I.2"
      ],
      "primary_category": "cs.CV",
      "comment": "Not completed work",
      "pdf_url": "http://arxiv.org/pdf/2410.09474v3",
      "published_date": "2024-10-12 10:27:23 UTC",
      "updated_date": "2024-12-20 22:10:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:43:59.177872"
    },
    {
      "arxiv_id": "2410.09472v2",
      "title": "DRCap: Decoding CLAP Latents with Retrieval-Augmented Generation for Zero-shot Audio Captioning",
      "title_zh": "翻译失败",
      "authors": [
        "Xiquan Li",
        "Wenxi Chen",
        "Ziyang Ma",
        "Xuenan Xu",
        "Yuzhe Liang",
        "Zhisheng Zheng",
        "Qiuqiang Kong",
        "Xie Chen"
      ],
      "abstract": "While automated audio captioning (AAC) has made notable progress, traditional\nfully supervised AAC models still face two critical challenges: the need for\nexpensive audio-text pair data for training and performance degradation when\ntransferring across domains. To overcome these limitations, we present DRCap, a\ndata-efficient and flexible zero-shot audio captioning system that requires\ntext-only data for training and can quickly adapt to new domains without\nadditional fine-tuning. DRCap integrates a contrastive language-audio\npre-training (CLAP) model and a large-language model (LLM) as its backbone.\nDuring training, the model predicts the ground-truth caption with a fixed text\nencoder from CLAP, whereas, during inference, the text encoder is replaced with\nthe audio encoder to generate captions for audio clips in a zero-shot manner.\nTo mitigate the modality gap of the CLAP model, we use both the projection\nstrategy from the encoder side and the retrieval-augmented generation strategy\nfrom the decoder side. Specifically, audio embeddings are first projected onto\na text embedding support to absorb extensive semantic information within the\njoint multi-modal space of CLAP. At the same time, similar captions retrieved\nfrom a datastore are fed as prompts to instruct the LLM, incorporating external\nknowledge to take full advantage of its strong generative capability.\nConditioned on both the projected CLAP embedding and the retrieved similar\ncaptions, the model is able to produce a more accurate and semantically rich\ntextual description. By tailoring the text embedding support and the caption\ndatastore to the target domain, DRCap acquires a robust ability to adapt to new\ndomains in a training-free manner. Experimental results demonstrate that DRCap\noutperforms all other zero-shot models in in-domain scenarios and achieves\nstate-of-the-art performance in cross-domain scenarios.",
      "tldr_zh": "本文提出 DRCap，一种零样本音频标注（zero-shot audio captioning）系统，仅需文本数据训练即可克服传统模型对昂贵音频-文本配对数据的依赖，并实现跨域快速适应。DRCap 整合 CLAP（对比语言-音频预训练模型）和 LLM（大语言模型），通过音频嵌入投影策略吸收语义信息，以及检索增强生成（RAG）策略从数据存储中检索类似标题作为提示，提升生成准确性和丰富性。实验结果表明，DRCap 在域内场景中优于其他零样本模型，并在跨域场景中达到最先进性能。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09472v2",
      "published_date": "2024-10-12 10:21:00 UTC",
      "updated_date": "2025-01-06 10:52:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:44:11.983636"
    },
    {
      "arxiv_id": "2410.09463v2",
      "title": "From Theory to Practice: Implementing and Evaluating e-Fold Cross-Validation",
      "title_zh": "翻译失败",
      "authors": [
        "Christopher Mahlich",
        "Tobias Vente",
        "Joeran Beel"
      ],
      "abstract": "This paper introduces e-fold cross-validation, an energy-efficient\nalternative to k-fold cross-validation. It dynamically adjusts the number of\nfolds based on a stopping criterion. The criterion checks after each fold\nwhether the standard deviation of the evaluated folds has consistently\ndecreased or remained stable. Once met, the process stops early. We tested\ne-fold cross-validation on 15 datasets and 10 machine-learning algorithms. On\naverage, it required 4 fewer folds than 10-fold cross-validation, reducing\nevaluation time, computational resources, and energy use by about 40%.\nPerformance differences between e-fold and 10-fold cross-validation were less\nthan 2% for larger datasets. More complex models showed even smaller\ndiscrepancies. In 96% of iterations, the results were within the confidence\ninterval, confirming statistical significance. E-fold cross-validation offers a\nreliable and efficient alternative to k-fold, reducing computational costs\nwhile maintaining comparable accuracy.",
      "tldr_zh": "本论文提出 e-fold cross-validation，一种比传统 k-fold cross-validation 更节能的交叉验证方法，通过动态调整折数基于停止准则（即检查标准差是否持续减少或稳定，从而提前结束过程）。研究者在 15 个数据集和 10 个机器学习算法上进行测试，结果显示 e-fold 平均比 10-fold 少 4 个折，减少评估时间、计算资源和能源使用约 40%。此外，对于较大数据集，性能差异小于 2%，复杂模型差异更小，且在 96% 的迭代中结果保持统计显著性，证明 e-fold 是一种可靠的替代方案，能在维持准确性的同时显著降低计算成本。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09463v2",
      "published_date": "2024-10-12 09:56:28 UTC",
      "updated_date": "2024-10-26 18:25:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:44:23.486716"
    },
    {
      "arxiv_id": "2410.09455v1",
      "title": "VERITAS-NLI : Validation and Extraction of Reliable Information Through Automated Scraping and Natural Language Inference",
      "title_zh": "VERITAS-NLI：通过自动化抓取和自然语言推理进行可靠信息的验证和提取",
      "authors": [
        "Arjun Shah",
        "Hetansh Shah",
        "Vedica Bafna",
        "Charmi Khandor",
        "Sindhu Nair"
      ],
      "abstract": "In today's day and age where information is rapidly spread through online\nplatforms, the rise of fake news poses an alarming threat to the integrity of\npublic discourse, societal trust, and reputed news sources. Classical machine\nlearning and Transformer-based models have been extensively studied for the\ntask of fake news detection, however they are hampered by their reliance on\ntraining data and are unable to generalize on unseen headlines. To address\nthese challenges, we propose our novel solution, leveraging web-scraping\ntechniques and Natural Language Inference (NLI) models to retrieve external\nknowledge necessary for verifying the accuracy of a headline. Our system is\nevaluated on a diverse self-curated evaluation dataset spanning over multiple\nnews channels and broad domains. Our best performing pipeline achieves an\naccuracy of 84.3% surpassing the best classical Machine Learning model by 33.3%\nand Bidirectional Encoder Representations from Transformers (BERT) by 31.0% .\nThis highlights the efficacy of combining dynamic web-scraping with Natural\nLanguage Inference to find support for a claimed headline in the corresponding\nexternally retrieved knowledge for the task of fake news detection.",
      "tldr_zh": "该研究提出VERITAS-NLI系统，通过自动化web-scraping和Natural Language Inference (NLI)模型，检索外部知识来验证新闻标题的准确性，以应对假新闻对公共话语和信任的威胁。不同于传统机器学习和Transformer模型（如BERT）依赖训练数据并泛化能力不足，该系统动态获取相关信息进行交叉验证。实验在自制数据集上显示，VERITAS-NLI的最佳管道准确率达84.3%，比最佳经典Machine Learning模型高33.3%，比BERT高31.0%，证明了结合web-scraping和NLI在假新闻检测中的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.1; I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint, 15 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.09455v1",
      "published_date": "2024-10-12 09:25:12 UTC",
      "updated_date": "2024-10-12 09:25:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:44:34.017484"
    },
    {
      "arxiv_id": "2410.09453v3",
      "title": "MMAD: A Comprehensive Benchmark for Multimodal Large Language Models in Industrial Anomaly Detection",
      "title_zh": "MMAD：多模态大语言模型在工业异常检测的全面基准",
      "authors": [
        "Xi Jiang",
        "Jian Li",
        "Hanqiu Deng",
        "Yong Liu",
        "Bin-Bin Gao",
        "Yifeng Zhou",
        "Jialin Li",
        "Chengjie Wang",
        "Feng Zheng"
      ],
      "abstract": "In the field of industrial inspection, Multimodal Large Language Models\n(MLLMs) have a high potential to renew the paradigms in practical applications\ndue to their robust language capabilities and generalization abilities.\nHowever, despite their impressive problem-solving skills in many domains,\nMLLMs' ability in industrial anomaly detection has not been systematically\nstudied. To bridge this gap, we present MMAD, the first-ever full-spectrum\nMLLMs benchmark in industrial Anomaly Detection. We defined seven key subtasks\nof MLLMs in industrial inspection and designed a novel pipeline to generate the\nMMAD dataset with 39,672 questions for 8,366 industrial images. With MMAD, we\nhave conducted a comprehensive, quantitative evaluation of various\nstate-of-the-art MLLMs. The commercial models performed the best, with the\naverage accuracy of GPT-4o models reaching 74.9%. However, this result falls\nfar short of industrial requirements. Our analysis reveals that current MLLMs\nstill have significant room for improvement in answering questions related to\nindustrial anomalies and defects. We further explore two training-free\nperformance enhancement strategies to help models improve in industrial\nscenarios, highlighting their promising potential for future research.",
      "tldr_zh": "本论文提出了 MMAD，这是一个全面基准，用于评估 Multimodal Large Language Models (MLLMs) 在工业异常检测中的性能，以填补这一领域的系统性研究空白。研究团队定义了七个关键子任务，并设计了一个新颖管道生成 MMAD 数据集，包含 8,366 张工业图像和 39,672 个问题，对各种最先进 MLLMs 进行了定量评估，结果显示 GPT-4o 模型的平均准确率达 74.9%，但仍远低于工业要求。分析表明，当前 MLLMs 在处理工业异常相关问题时存在显著改进空间，并探索了两个无训练性能提升策略，以提升其在工业场景中的应用潜力。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ICLR 2025. The code and data are available at\n  https://github.com/jam-cc/MMAD",
      "pdf_url": "http://arxiv.org/pdf/2410.09453v3",
      "published_date": "2024-10-12 09:16:09 UTC",
      "updated_date": "2025-02-21 04:50:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:44:47.357002"
    },
    {
      "arxiv_id": "2410.09437v3",
      "title": "MTL-LoRA: Low-Rank Adaptation for Multi-Task Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yaming Yang",
        "Dilxat Muhtar",
        "Yelong Shen",
        "Yuefeng Zhan",
        "Jianfeng Liu",
        "Yujing Wang",
        "Hao Sun",
        "Denvy Deng",
        "Feng Sun",
        "Qi Zhang",
        "Weizhu Chen",
        "Yunhai Tong"
      ],
      "abstract": "Parameter-efficient fine-tuning (PEFT) has been widely employed for domain\nadaptation, with LoRA being one of the most prominent methods due to its\nsimplicity and effectiveness. However, in multi-task learning (MTL) scenarios,\nLoRA tends to obscure the distinction between tasks by projecting sparse\nhigh-dimensional features from different tasks into the same dense\nlow-dimensional intrinsic space. This leads to task interference and suboptimal\nperformance for LoRA and its variants. To tackle this challenge, we propose\nMTL-LoRA, which retains the advantages of low-rank adaptation while\nsignificantly enhancing MTL capabilities. MTL-LoRA augments LoRA by\nincorporating additional task-adaptive parameters that differentiate\ntask-specific information and capture shared knowledge across various tasks\nwithin low-dimensional spaces. This approach enables pre-trained models to\njointly adapt to different target domains with a limited number of trainable\nparameters. Comprehensive experimental results, including evaluations on public\nacademic benchmarks for natural language understanding, commonsense reasoning,\nand image-text understanding, as well as real-world industrial text Ads\nrelevance datasets, demonstrate that MTL-LoRA outperforms LoRA and its various\nvariants with comparable or even fewer learnable parameters in MTL setting.",
      "tldr_zh": "本文提出MTL-LoRA，一种针对多任务学习(MTL)的低秩适应方法，以解决现有LoRA在MTL场景中因特征投影到同一低维空间而导致的任务干扰问题。MTL-LoRA在LoRA基础上添加任务自适应参数，用于区分任务特定信息并捕获跨任务共享知识，从而实现预训练模型在有限参数下高效适应多个目标领域。实验结果表明，在自然语言理解、常识推理、图像文本理解等公共基准以及实际工业数据集上，MTL-LoRA以相近或更少的可训练参数，显著优于LoRA及其变体。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "12 Pages, 4 Figures",
      "pdf_url": "http://arxiv.org/pdf/2410.09437v3",
      "published_date": "2024-10-12 08:32:26 UTC",
      "updated_date": "2025-04-01 10:18:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:44:59.138849"
    },
    {
      "arxiv_id": "2410.09428v1",
      "title": "Declarative Knowledge Distillation from Large Language Models for Visual Question Answering Datasets",
      "title_zh": "翻译失败",
      "authors": [
        "Thomas Eiter",
        "Jan Hadl",
        "Nelson Higuera",
        "Johannes Oetsch"
      ],
      "abstract": "Visual Question Answering (VQA) is the task of answering a question about an\nimage and requires processing multimodal input and reasoning to obtain the\nanswer. Modular solutions that use declarative representations within the\nreasoning component have a clear advantage over end-to-end trained systems\nregarding interpretability. The downside is that crafting the rules for such a\ncomponent can be an additional burden on the developer. We address this\nchallenge by presenting an approach for declarative knowledge distillation from\nLarge Language Models (LLMs). Our method is to prompt an LLM to extend an\ninitial theory on VQA reasoning, given as an answer-set program, to meet the\nrequirements of the VQA task. Examples from the VQA dataset are used to guide\nthe LLM, validate the results, and mend rules if they are not correct by using\nfeedback from the ASP solver. We demonstrate that our approach works on the\nprominent CLEVR and GQA datasets. Our results confirm that distilling knowledge\nfrom LLMs is in fact a promising direction besides data-driven rule learning\napproaches.",
      "tldr_zh": "这篇论文提出了一种从Large Language Models (LLMs)中提炼声明式知识的方法，用于提升Visual Question Answering (VQA)任务的可解释性，旨在解决模块化系统编写规则的额外负担问题。方法包括提示LLM扩展一个初始的answer-set program推理理论，并利用VQA数据集的例子进行引导、验证和修正。实验在CLEVR和GQA数据集上验证了这一方法的有效性，证明它是一种有前景的替代方案，与数据驱动的规则学习方法相比更注重可解释性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)",
      "pdf_url": "http://arxiv.org/pdf/2410.09428v1",
      "published_date": "2024-10-12 08:17:03 UTC",
      "updated_date": "2024-10-12 08:17:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:45:11.233249"
    },
    {
      "arxiv_id": "2410.09416v1",
      "title": "Can Vision-Language Models Replace Human Annotators: A Case Study with CelebA Dataset",
      "title_zh": "视觉语言模型能否取代人类标注者：以 CelebA 数据集为例研究",
      "authors": [
        "Haoming Lu",
        "Feifei Zhong"
      ],
      "abstract": "This study evaluates the capability of Vision-Language Models (VLMs) in image\ndata annotation by comparing their performance on the CelebA dataset in terms\nof quality and cost-effectiveness against manual annotation. Annotations from\nthe state-of-the-art LLaVA-NeXT model on 1000 CelebA images are in 79.5%\nagreement with the original human annotations. Incorporating re-annotations of\ndisagreed cases into a majority vote boosts AI annotation consistency to 89.1%\nand even higher for more objective labels. Cost assessments demonstrate that AI\nannotation significantly reduces expenditures compared to traditional manual\nmethods -- representing less than 1% of the costs for manual annotation in the\nCelebA dataset. These findings support the potential of VLMs as a viable,\ncost-effective alternative for specific annotation tasks, reducing both\nfinancial burden and ethical concerns associated with large-scale manual data\nannotation. The AI annotations and re-annotations utilized in this study are\navailable on https://github.com/evev2024/EVEV2024_CelebA.",
      "tldr_zh": "本文评估了 Vision-Language Models (VLMs) 是否能取代人类标注者，通过在 CelebA 数据集上比较 LLaVA-NeXT 模型的标注表现，结果显示其与人类标注一致性达 79.5%。通过对不一致样本进行重新标注并采用多数投票方式，该一致性进一步提升至 89.1%，尤其在更客观的标签上表现更佳。成本分析表明，AI 标注的开销仅为手动方法的不到 1%，从而证明 VLMs 可作为特定图像标注任务的成本有效替代方案，减少财务负担和伦理问题。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by NeurIPS 2024 Workshop (EvalEval 2024)",
      "pdf_url": "http://arxiv.org/pdf/2410.09416v1",
      "published_date": "2024-10-12 07:49:08 UTC",
      "updated_date": "2024-10-12 07:49:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:45:23.326158"
    },
    {
      "arxiv_id": "2410.09412v2",
      "title": "FB-Bench: A Fine-Grained Multi-Task Benchmark for Evaluating LLMs' Responsiveness to Human Feedback",
      "title_zh": "FB-Bench：细粒度的多任务基准测试，用于评估大语言模型对",
      "authors": [
        "Youquan Li",
        "Miao Zheng",
        "Fan Yang",
        "Guosheng Dong",
        "Bin Cui",
        "Weipeng Chen",
        "Zenan Zhou",
        "Wentao Zhang"
      ],
      "abstract": "Human feedback is crucial in the interactions between humans and Large\nLanguage Models (LLMs). However, existing research primarily focuses on\nbenchmarking LLMs in single-turn dialogues. Even in benchmarks designed for\nmulti-turn dialogues, the user inputs are often independent, neglecting the\nnuanced and complex nature of human feedback within real-world usage scenarios.\nTo fill this research gap, we introduce FB-Bench, a fine-grained, multi-task\nbenchmark designed to evaluate LLMs' responsiveness to human feedback under\nreal-world usage scenarios in Chinese. Drawing from the two main interaction\nscenarios, FB-Bench comprises 591 meticulously curated samples, encompassing\neight task types, five deficiency types of response, and nine feedback types.\nWe extensively evaluate a broad array of popular LLMs, revealing significant\nvariations in their performance across different interaction scenarios. Further\nanalysis indicates that task, human feedback, and deficiencies of previous\nresponses can also significantly impact LLMs' responsiveness. Our findings\nunderscore both the strengths and limitations of current models, providing\nvaluable insights and directions for future research. Code and datasets are\navailable at https://github.com/PKU-Baichuan-MLSystemLab/FB-Bench.",
      "tldr_zh": "本研究引入 FB-Bench，这是一个细粒度的多任务基准，用于评估大型语言模型（LLMs）在真实中文场景下对人类反馈的响应能力，填补了现有基准忽略多轮对话复杂性的空白。FB-Bench 包含 591 个精心策划的样本，涵盖八个任务类型、五种响应缺陷类型和九种反馈类型，旨在模拟实际交互情景。实验评估了多种流行 LLMs，揭示了它们在不同场景下的性能差异，并分析了任务、反馈和响应缺陷对模型响应的影响，为未来 LLMs 改进提供宝贵洞见。代码和数据集可从 https://github.com/PKU-Baichuan-MLSystemLab/FB-Bench 获取。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09412v2",
      "published_date": "2024-10-12 07:40:01 UTC",
      "updated_date": "2025-02-17 03:45:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:45:35.153558"
    },
    {
      "arxiv_id": "2410.09407v1",
      "title": "CAMPHOR: Collaborative Agents for Multi-input Planning and High-Order Reasoning On Device",
      "title_zh": "翻译失败",
      "authors": [
        "Yicheng Fu",
        "Raviteja Anantha",
        "Jianpeng Cheng"
      ],
      "abstract": "While server-side Large Language Models (LLMs) demonstrate proficiency in\nfunction calling and complex reasoning, deploying Small Language Models (SLMs)\ndirectly on devices brings opportunities to improve latency and privacy but\nalso introduces unique challenges for accuracy and memory. We introduce\nCAMPHOR, an innovative on-device SLM multi-agent framework designed to handle\nmultiple user inputs and reason over personal context locally, ensuring privacy\nis maintained. CAMPHOR employs a hierarchical architecture where a high-order\nreasoning agent decomposes complex tasks and coordinates expert agents\nresponsible for personal context retrieval, tool interaction, and dynamic plan\ngeneration. By implementing parameter sharing across agents and leveraging\nprompt compression, we significantly reduce model size, latency, and memory\nusage. To validate our approach, we present a novel dataset capturing\nmulti-agent task trajectories centered on personalized mobile assistant\nuse-cases. Our experiments reveal that fine-tuned SLM agents not only surpass\nclosed-source LLMs in task completion F1 by~35\\% but also eliminate the need\nfor server-device communication, all while enhancing privacy.",
      "tldr_zh": "本研究提出CAMPHOR，一种创新的on-device SLM (Small Language Models) 多智能体框架，用于处理多个用户输入并在本地进行高级推理，从而提升隐私和延迟性能。框架采用分层架构，由一个高阶推理智能体负责分解复杂任务并协调其他智能体，包括个人上下文检索、工具交互和动态计划生成，同时通过参数共享和提示压缩显著降低模型大小、延迟和内存使用。为验证效果，研究提供了一个新数据集，聚焦于个性化移动助理场景。实验结果显示，微调后的SLM智能体在任务完成F1分数上比闭源LLMs高出约35%，并消除了服务器-设备通信需求。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09407v1",
      "published_date": "2024-10-12 07:28:10 UTC",
      "updated_date": "2024-10-12 07:28:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:45:46.145925"
    },
    {
      "arxiv_id": "2410.09403v2",
      "title": "Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System",
      "title_zh": "翻译失败",
      "authors": [
        "Haoyang Su",
        "Renqi Chen",
        "Shixiang Tang",
        "Zhenfei Yin",
        "Xinzhe Zheng",
        "Jinzhe Li",
        "Biqing Qi",
        "Qi Wu",
        "Hui Li",
        "Wanli Ouyang",
        "Philip Torr",
        "Bowen Zhou",
        "Nanqing Dong"
      ],
      "abstract": "The rapid advancement of scientific progress requires innovative tools that\ncan accelerate knowledge discovery. Although recent AI methods, particularly\nlarge language models (LLMs), have shown promise in tasks such as hypothesis\ngeneration and experimental design, they fall short of replicating the\ncollaborative nature of real-world scientific practices, where diverse experts\nwork together in teams to tackle complex problems. To address the limitations,\nwe propose an LLM-based multi-agent system, i.e., Virtual Scientists (VirSci),\ndesigned to mimic the teamwork inherent in scientific research. VirSci\norganizes a team of agents to collaboratively generate, evaluate, and refine\nresearch ideas. Through comprehensive experiments, we demonstrate that this\nmulti-agent approach outperforms the state-of-the-art method in producing novel\nscientific ideas. We further investigate the collaboration mechanisms that\ncontribute to its tendency to produce ideas with higher novelty, offering\nvaluable insights to guide future research and illuminating pathways toward\nbuilding a robust system for autonomous scientific discovery. The code is\navailable at https://github.com/open-sciencelab/Virtual-Scientists.",
      "tldr_zh": "本论文提出了一种基于大语言模型(LLM)的多智能体系统，名为 Virtual Scientists (VirSci)，旨在模拟真实科学家团队协作，以提升科学想法的生成、评估和完善过程，从而克服现有AI方法在团队互动方面的局限。VirSci 通过组织多个代理进行协同工作，显著提高了研究想法的新颖性，在实验中比最先进方法表现更优。论文进一步分析了驱动更高创新的协作机制，并为未来构建自主科学发现系统提供了宝贵见解，代码已在GitHub开源。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09403v2",
      "published_date": "2024-10-12 07:16:22 UTC",
      "updated_date": "2025-02-19 06:07:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:45:59.128301"
    },
    {
      "arxiv_id": "2410.09401v1",
      "title": "A Novel Approach to Malicious Code Detection Using CNN-BiLSTM and Feature Fusion",
      "title_zh": "使用 CNN-BiLSTM 和特征融合的恶意代码检测新方法",
      "authors": [
        "Lixia Zhang",
        "Tianxu Liu",
        "Kaihui Shen",
        "Cheng Chen"
      ],
      "abstract": "With the rapid advancement of Internet technology, the threat of malware to\ncomputer systems and network security has intensified. Malware affects\nindividual privacy and security and poses risks to critical infrastructures of\nenterprises and nations. The increasing quantity and complexity of malware,\nalong with its concealment and diversity, challenge traditional detection\ntechniques. Static detection methods struggle against variants and packed\nmalware, while dynamic methods face high costs and risks that limit their\napplication. Consequently, there is an urgent need for novel and efficient\nmalware detection techniques to improve accuracy and robustness.\n  This study first employs the minhash algorithm to convert binary files of\nmalware into grayscale images, followed by the extraction of global and local\ntexture features using GIST and LBP algorithms. Additionally, the study\nutilizes IDA Pro to decompile and extract opcode sequences, applying N-gram and\ntf-idf algorithms for feature vectorization. The fusion of these features\nenables the model to comprehensively capture the behavioral characteristics of\nmalware.\n  In terms of model construction, a CNN-BiLSTM fusion model is designed to\nsimultaneously process image features and opcode sequences, enhancing\nclassification performance. Experimental validation on multiple public datasets\ndemonstrates that the proposed method significantly outperforms traditional\ndetection techniques in terms of accuracy, recall, and F1 score, particularly\nin detecting variants and obfuscated malware with greater stability.\n  The research presented in this paper offers new insights into the development\nof malware detection technologies, validating the effectiveness of feature and\nmodel fusion, and holds promising application prospects.",
      "tldr_zh": "本研究针对恶意软件检测的挑战，提出了一种新型方法，使用CNN-BiLSTM融合模型和特征融合技术来提升检测准确性和鲁棒性。具体而言，该方法先通过minhash算法将恶意软件二进制文件转换为灰度图像，并提取GIST和LBP算法的全局及局部纹理特征；同时，利用IDA Pro反编译opcode序列，并应用N-gram和tf-idf算法进行特征向量化，然后融合这些特征以全面捕捉恶意软件行为。实验在多个公共数据集上验证，该方法在准确率、召回率和F1分数上显著优于传统检测技术，尤其在识别变种和混淆恶意软件时表现出更高的稳定性，为恶意软件检测领域的发展提供了新颖见解。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09401v1",
      "published_date": "2024-10-12 07:10:44 UTC",
      "updated_date": "2024-10-12 07:10:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:46:11.701279"
    },
    {
      "arxiv_id": "2410.09397v1",
      "title": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for Backward Passes",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoyu Li",
        "Yingyu Liang",
        "Zhenmei Shi",
        "Zhao Song",
        "Yufa Zhou"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference.",
      "tldr_zh": "本文对 Large Language Models (LLMs) 中注意力机制的 I/O 复杂度进行了细粒度分析，重点针对 backward passes，并将场景分为小缓存和大型缓存。使用 red-blue pebble game framework 建立了紧确的 I/O 复杂度边界，确认 FlashAttention 在大型缓存场景下为最优算法，同时为小缓存场景提出了一种改进算法以达到这些边界。研究还扩展到 sparse attention，导出了细粒度的下界。总体上，这些发现完善了注意力机制的理论基础，为设计高效的 LLM 训练和推理算法提供了关键洞见。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CC",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09397v1",
      "published_date": "2024-10-12 07:01:30 UTC",
      "updated_date": "2024-10-12 07:01:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:46:23.254297"
    },
    {
      "arxiv_id": "2410.09388v3",
      "title": "3-D Magnetotelluric Deep Learning Inversion Guided by Pseudo-Physical Information",
      "title_zh": "翻译失败",
      "authors": [
        "Peifan Jiang",
        "Xuben Wang",
        "Shuang Wang",
        "Fei Deng",
        "Kunpeng Wang",
        "Bin Wang",
        "Yuhan Yang"
      ],
      "abstract": "Magnetotelluric deep learning (DL) inversion methods based on joint\ndata-driven and physics-driven have become a hot topic in recent years. When\nmapping observation data (or forward modeling data) to the resistivity model\nusing neural networks (NNs), incorporating the error (loss) term of the\ninversion resistivity's forward modeling response--which introduces physical\ninformation about electromagnetic field propagation--can significantly enhance\nthe inversion accuracy. To efficiently achieve data-physical dual-driven MT\ndeep learning inversion for large-scale 3-D MT data, we propose using DL\nforward modeling networks to compute this portion of the loss. This approach\nintroduces pseudo-physical information through the forward modeling of NN\nsimulation, further guiding the inversion network fitting. Specifically, we\nfirst pre-train the forward modeling networks as fixed forward modeling\noperators, then transfer and integrate them into the inversion network\ntraining, and finally optimize the inversion network by minimizing the\nmultinomial loss. Theoretical experimental results indicate that despite some\nsimulation errors in DL forward modeling, the introduced pseudo-physical\ninformation still enhances inversion accuracy and significantly mitigates the\noverfitting problem during training. Additionally, we propose a new input mode\nthat involves masking and adding noise to the data, simulating the field data\nenvironment of 3-D MT inversion, thereby making the method more flexible and\neffective for practical applications.",
      "tldr_zh": "本文提出了一种基于伪物理信息的3-D Magnetotelluric (MT)深层学习 (DL)反演方法，通过整合DL前向建模网络来引入电磁场传播的物理信息，指导神经网络 (NNs)从观测数据映射到电阻率模型。该方法包括预训练前向建模网络作为固定运算符，并将其转移到反演网络训练中，以最小化多项损失函数，从而提升反演准确性并缓解过拟合问题。实验结果表明，尽管DL前向建模存在模拟误差，该方法仍显著提高了反演精度，并通过数据掩码和噪声添加的新输入模式，使其更适用于实际3-D MT实地数据环境。",
      "categories": [
        "physics.geo-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.geo-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09388v3",
      "published_date": "2024-10-12 06:39:31 UTC",
      "updated_date": "2025-05-15 19:17:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:46:35.614797"
    },
    {
      "arxiv_id": "2410.09385v1",
      "title": "Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sathya Kamesh Bhethanabhotla",
        "Omar Swelam",
        "Julien Siems",
        "David Salinas",
        "Frank Hutter"
      ],
      "abstract": "This paper introduces Mamba4Cast, a zero-shot foundation model for time\nseries forecasting. Based on the Mamba architecture and inspired by Prior-data\nFitted Networks (PFNs), Mamba4Cast generalizes robustly across diverse time\nseries tasks without the need for dataset specific fine-tuning. Mamba4Cast's\nkey innovation lies in its ability to achieve strong zero-shot performance on\nreal-world datasets while having much lower inference times than time series\nfoundation models based on the transformer architecture. Trained solely on\nsynthetic data, the model generates forecasts for entire horizons in a single\npass, outpacing traditional auto-regressive approaches. Our experiments show\nthat Mamba4Cast performs competitively against other state-of-the-art\nfoundation models in various data sets while scaling significantly better with\nthe prediction length. The source code can be accessed at\nhttps://github.com/automl/Mamba4Cast.",
      "tldr_zh": "这篇论文介绍了 Mamba4Cast，一种基于 Mamba 架构并受 Prior-data Fitted Networks (PFNs) 启发的零样本时间序列预测基础模型，能够在无需特定数据集微调的情况下泛化到多样任务。Mamba4Cast 的关键创新在于其高效推理性能，仅在合成数据上训练即可一次性生成完整预测horizon，显著优于传统的自回归方法和基于 Transformer 的模型。实验结果表明，该模型在各种真实数据集上与最先进模型竞争力相当，同时在预测长度上扩展性能更佳，源代码可从 GitHub 获取。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09385v1",
      "published_date": "2024-10-12 06:35:18 UTC",
      "updated_date": "2024-10-12 06:35:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:46:46.778622"
    },
    {
      "arxiv_id": "2410.09380v1",
      "title": "Prompting Video-Language Foundation Models with Domain-specific Fine-grained Heuristics for Video Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Ting Yu",
        "Kunhao Fu",
        "Shuhui Wang",
        "Qingming Huang",
        "Jun Yu"
      ],
      "abstract": "Video Question Answering (VideoQA) represents a crucial intersection between\nvideo understanding and language processing, requiring both discriminative\nunimodal comprehension and sophisticated cross-modal interaction for accurate\ninference. Despite advancements in multi-modal pre-trained models and\nvideo-language foundation models, these systems often struggle with\ndomain-specific VideoQA due to their generalized pre-training objectives.\nAddressing this gap necessitates bridging the divide between broad cross-modal\nknowledge and the specific inference demands of VideoQA tasks. To this end, we\nintroduce HeurVidQA, a framework that leverages domain-specific entity-action\nheuristics to refine pre-trained video-language foundation models. Our approach\ntreats these models as implicit knowledge engines, employing domain-specific\nentity-action prompters to direct the model's focus toward precise cues that\nenhance reasoning. By delivering fine-grained heuristics, we improve the\nmodel's ability to identify and interpret key entities and actions, thereby\nenhancing its reasoning capabilities. Extensive evaluations across multiple\nVideoQA datasets demonstrate that our method significantly outperforms existing\nmodels, underscoring the importance of integrating domain-specific knowledge\ninto video-language models for more accurate and context-aware VideoQA.",
      "tldr_zh": "该研究针对视频问答（VideoQA）任务，提出 HeurVidQA 框架，以解决视频-语言基础模型（Video-Language Foundation Models）在领域特定任务中因泛化预训练而导致的性能不足问题。该框架通过引入 domain-specific fine-grained heuristics，特别是实体-动作提示器（entity-action prompters），引导模型聚焦于关键实体和动作的精确线索，从而增强其跨模态推理能力。实验在多个 VideoQA 数据集上进行，结果显示该方法显著优于现有模型，突显了整合领域特定知识对提升上下文感知和准确性的重要性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "IEEE Transactions on Circuits and Systems for Video Technology",
      "pdf_url": "http://arxiv.org/pdf/2410.09380v1",
      "published_date": "2024-10-12 06:22:23 UTC",
      "updated_date": "2024-10-12 06:22:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:46:58.493926"
    },
    {
      "arxiv_id": "2410.09379v1",
      "title": "Multi-granularity Contrastive Cross-modal Collaborative Generation for End-to-End Long-term Video Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Ting Yu",
        "Kunhao Fu",
        "Jian Zhang",
        "Qingming Huang",
        "Jun Yu"
      ],
      "abstract": "Long-term Video Question Answering (VideoQA) is a challenging\nvision-and-language bridging task focusing on semantic understanding of\nuntrimmed long-term videos and diverse free-form questions, simultaneously\nemphasizing comprehensive cross-modal reasoning to yield precise answers. The\ncanonical approaches often rely on off-the-shelf feature extractors to detour\nthe expensive computation overhead, but often result in domain-independent\nmodality-unrelated representations. Furthermore, the inherent gradient blocking\nbetween unimodal comprehension and cross-modal interaction hinders reliable\nanswer generation. In contrast, recent emerging successful video-language\npre-training models enable cost-effective end-to-end modeling but fall short in\ndomain-specific ratiocination and exhibit disparities in task formulation.\nToward this end, we present an entirely end-to-end solution for long-term\nVideoQA: Multi-granularity Contrastive cross-modal collaborative Generation\n(MCG) model. To derive discriminative representations possessing high visual\nconcepts, we introduce Joint Unimodal Modeling (JUM) on a clip-bone\narchitecture and leverage Multi-granularity Contrastive Learning (MCL) to\nharness the intrinsically or explicitly exhibited semantic correspondences. To\nalleviate the task formulation discrepancy problem, we propose a Cross-modal\nCollaborative Generation (CCG) module to reformulate VideoQA as a generative\ntask instead of the conventional classification scheme, empowering the model\nwith the capability for cross-modal high-semantic fusion and generation so as\nto rationalize and answer. Extensive experiments conducted on six publicly\navailable VideoQA datasets underscore the superiority of our proposed method.",
      "tldr_zh": "本文提出了一种端到端长视频问答（VideoQA）解决方案：Multi-granularity Contrastive cross-modal collaborative Generation (MCG) 模型，旨在解决传统方法中模态无关表示和任务制定差异的问题。MCG 模型通过 Joint Unimodal Modeling (JUM) 在 clip-bone 架构上联合建模单模态表示，并利用 Multi-granularity Contrastive Learning (MCL) 捕捉多粒度语义对应，以及 Cross-modal Collaborative Generation (CCG) 模块将 VideoQA 转化为生成任务以实现高语义跨模态融合。实验在六个公开数据集上表明，该方法显著优于基线模型，提升了长视频的语义理解和回答准确性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Transactions on Image Processing",
      "pdf_url": "http://arxiv.org/pdf/2410.09379v1",
      "published_date": "2024-10-12 06:21:58 UTC",
      "updated_date": "2024-10-12 06:21:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:47:11.527915"
    },
    {
      "arxiv_id": "2410.09375v2",
      "title": "Looped ReLU MLPs May Be All You Need as Practical Programmable Computers",
      "title_zh": "翻译失败",
      "authors": [
        "Yingyu Liang",
        "Zhizhou Sha",
        "Zhenmei Shi",
        "Zhao Song",
        "Yufa Zhou"
      ],
      "abstract": "Previous work has demonstrated that attention mechanisms are Turing complete.\nMore recently, it has been shown that a looped 9-layer Transformer can function\nas a universal programmable computer. In contrast, the multi-layer perceptrons\nwith $\\mathsf{ReLU}$ activation ($\\mathsf{ReLU}$-$\\mathsf{MLP}$), one of the\nmost fundamental components of neural networks, is known to be expressive;\nspecifically, a two-layer neural network is a universal approximator given an\nexponentially large number of hidden neurons. However, it remains unclear\nwhether a $\\mathsf{ReLU}$-$\\mathsf{MLP}$ can be made into a universal\nprogrammable computer using a practical number of weights. In this work, we\nprovide an affirmative answer that a looped 23-layer\n$\\mathsf{ReLU}$-$\\mathsf{MLP}$ is capable of performing the basic necessary\noperations, more efficiently and effectively functioning as a programmable\ncomputer than a looped Transformer. This indicates simple modules have stronger\nexpressive power than previously expected and have not been fully explored. Our\nwork provides insights into the mechanisms of neural networks and demonstrates\nthat complex tasks, such as functioning as a programmable computer, do not\nnecessarily require advanced architectures like Transformers.",
      "tldr_zh": "本文证明，一个循环的23层 ReLU-MLP 可以作为实用可编程计算机，执行基本操作并比循环Transformer更高效有效，从而解决了ReLU-MLP 是否能用实际数量的权重实现图灵完备性的问题。研究对比了ReLU-MLP 与Transformer的表达能力，发现简单模块如ReLU-MLP 具有比预期更强的表达力，且未被充分探索。最终，该工作表明复杂任务不一定需要高级架构，如Transformer，就能实现通用计算功能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CC"
      ],
      "primary_category": "cs.LG",
      "comment": "AIStats 2025",
      "pdf_url": "http://arxiv.org/pdf/2410.09375v2",
      "published_date": "2024-10-12 05:54:17 UTC",
      "updated_date": "2025-02-20 10:24:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:47:23.196926"
    },
    {
      "arxiv_id": "2410.09368v1",
      "title": "Towards a Domain-Specific Modelling Environment for Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Natalie Sinani",
        "Sahil Salma",
        "Paul Boutot",
        "Sadaf Mustafiz"
      ],
      "abstract": "In recent years, machine learning technologies have gained immense popularity\nand are being used in a wide range of domains. However, due to the complexity\nassociated with machine learning algorithms, it is a challenge to make it\nuser-friendly, easy to understand and apply. Machine learning applications are\nespecially challenging for users who do not have proficiency in this area.\n  In this paper, we use model-driven engineering (MDE) methods and tools for\ndeveloping a domain-specific modelling environment to contribute towards\nproviding a solution for this problem. We targeted reinforcement learning from\nthe machine learning domain, and evaluated the proposed language, reinforcement\nlearning modelling language (RLML), with multiple applications. The tool\nsupports syntax-directed editing, constraint checking, and automatic generation\nof code from RLML models. The environment also provides support for comparing\nresults generated with multiple RL algorithms. With our proposed MDE approach,\nwe were able to help in abstracting reinforcement learning technologies and\nimprove the learning curve for RL users.",
      "tldr_zh": "本论文针对机器学习的复杂性问题，提出使用 Model-Driven Engineering (MDE) 方法开发一个针对 Reinforcement Learning (RL) 的领域特定建模环境，以提高非专业用户的易用性和理解度。论文引入了 Reinforcement Learning Modelling Language (RLML)，该语言支持语法指导编辑、约束检查、自动代码生成，并提供多 RL 算法结果比较的功能。通过多个应用评估，RLML 成功抽象化了 RL 技术，显著改善了用户的学习曲线。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "24 pages",
      "pdf_url": "http://arxiv.org/pdf/2410.09368v1",
      "published_date": "2024-10-12 04:56:01 UTC",
      "updated_date": "2024-10-12 04:56:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:47:36.554199"
    },
    {
      "arxiv_id": "2410.09362v1",
      "title": "SeRA: Self-Reviewing and Alignment of Large Language Models using Implicit Reward Margins",
      "title_zh": "翻译失败",
      "authors": [
        "Jongwoo Ko",
        "Saket Dingliwal",
        "Bhavana Ganesh",
        "Sailik Sengupta",
        "Sravan Bodapati",
        "Aram Galstyan"
      ],
      "abstract": "Direct alignment algorithms (DAAs), such as direct preference optimization\n(DPO), have become popular alternatives for Reinforcement Learning from Human\nFeedback (RLHF) due to their simplicity, efficiency, and stability. However,\nthe preferences used in DAAs are usually collected before the alignment\ntraining begins and remain unchanged (off-policy). This can lead to two\nproblems where the policy model (1) picks up on spurious correlations in the\ndataset (as opposed to learning the intended alignment expressed in the human\npreference labels), and (2) overfits to feedback on off-policy trajectories\nthat have less likelihood of being generated by an updated policy model. To\naddress these issues, we introduce Self-Reviewing and Alignment (SeRA), a\ncost-efficient and effective method that can be readily combined with existing\nDAAs. SeRA comprises of two components: (1) sample selection using implicit\nreward margins, which helps alleviate over-fitting to some undesired features,\nand (2) preference bootstrapping using implicit rewards to augment preference\ndata with updated policy models in a cost-efficient manner. Extensive\nexperimentation, including some on instruction-following tasks, demonstrate the\neffectiveness and generality of SeRA in training LLMs on offline preference\ndatasets with DAAs.",
      "tldr_zh": "该研究针对直接对齐算法（DAAs，如Direct Preference Optimization，DPO）在训练大型语言模型（LLMs）时的局限性提出SeRA方法，以解决偏好数据离线（off-policy）导致的虚假相关（spurious correlations）和过拟合问题。SeRA包括两个关键组件：使用隐式奖励边际（implicit reward margins）进行样本选择，以减少对不期望特征的过拟合；以及通过隐式奖励进行偏好引导（preference bootstrapping），以低成本方式利用更新后的策略模型增强数据。实验结果显示，SeRA在指令跟随任务等场景中，与现有DAAs结合使用时，能有效提升LLMs在离线偏好数据集上的训练性能和泛化能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09362v1",
      "published_date": "2024-10-12 04:17:28 UTC",
      "updated_date": "2024-10-12 04:17:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:47:46.958158"
    },
    {
      "arxiv_id": "2410.09359v2",
      "title": "Green Recommender Systems: Optimizing Dataset Size for Energy-Efficient Algorithm Performance",
      "title_zh": "翻译失败",
      "authors": [
        "Ardalan Arabzadeh",
        "Tobias Vente",
        "Joeran Beel"
      ],
      "abstract": "As recommender systems become increasingly prevalent, the environmental\nimpact and energy efficiency of training large-scale models have come under\nscrutiny. This paper investigates the potential for energy-efficient algorithm\nperformance by optimizing dataset sizes through downsampling techniques in the\ncontext of Green Recommender Systems. We conducted experiments on the MovieLens\n100K, 1M, 10M, and Amazon Toys and Games datasets, analyzing the performance of\nvarious recommender algorithms under different portions of dataset size. Our\nresults indicate that while more training data generally leads to higher\nalgorithm performance, certain algorithms, such as FunkSVD and BiasedMF,\nparticularly with unbalanced and sparse datasets like Amazon Toys and Games,\nmaintain high-quality recommendations with up to a 50% reduction in training\ndata, achieving nDCG@10 scores within approximately 13% of full dataset\nperformance. These findings suggest that strategic dataset reduction can\ndecrease computational and environmental costs without substantially\ncompromising recommendation quality. This study advances sustainable and green\nrecommender systems by providing insights for reducing energy consumption while\nmaintaining effectiveness.",
      "tldr_zh": "本研究探讨了绿色推荐系统（Green Recommender Systems）的能源效率问题，通过数据集下采样技术优化数据集大小，以减少环境影响和计算成本。实验在 MovieLens 100K、1M、10M 和 Amazon Toys and Games 数据集上测试了各种推荐算法的表现，结果显示，虽然更多训练数据通常提升性能，但算法如 FunkSVD 和 BiasedMF 在不平衡且稀疏的数据集上，可在减少50%训练数据的情况下，保持 nDCG@10 得分仅比完整数据集低约13%。这些发现表明，战略性数据集缩减能显著降低能源消耗，同时基本不影响推荐质量，从而推动可持续推荐系统的开发。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09359v2",
      "published_date": "2024-10-12 04:00:55 UTC",
      "updated_date": "2024-11-05 03:45:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:48:00.888372"
    },
    {
      "arxiv_id": "2410.09350v1",
      "title": "Generative Subgraph Retrieval for Knowledge Graph-Grounded Dialog Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Jinyoung Park",
        "Minseok Joo",
        "Joo-Kyung Kim",
        "Hyunwoo J. Kim"
      ],
      "abstract": "Knowledge graph-grounded dialog generation requires retrieving a\ndialog-relevant subgraph from the given knowledge base graph and integrating it\nwith the dialog history. Previous works typically represent the graph using an\nexternal encoder, such as graph neural networks, and retrieve relevant triplets\nbased on the similarity between single-vector representations of triplets and\nthe dialog history. However, these external encoders fail to leverage the rich\nknowledge of pretrained language models, and the retrieval process is also\nsuboptimal due to the information bottleneck caused by the single-vector\nabstraction of the dialog history. In this work, we propose Dialog generation\nwith Generative Subgraph Retrieval (DialogGSR), which retrieves relevant\nknowledge subgraphs by directly generating their token sequences on top of\nlanguage models. For effective generative subgraph retrieval, we introduce two\nkey methods: (i) structure-aware knowledge graph linearization with\nself-supervised graph-specific tokens and (ii) graph-constrained decoding\nutilizing graph structural proximity-based entity informativeness scores for\nvalid and relevant generative retrieval. DialogGSR achieves state-of-the-art\nperformance in knowledge graph-grounded dialog generation, as demonstrated on\nOpenDialKG and KOMODIS datasets.",
      "tldr_zh": "该论文提出了一种名为 DialogGSR 的生成式子图检索方法，用于知识图谱（Knowledge Graph）基础的对话生成任务，以解决传统方法依赖外部编码器（如 Graph Neural Networks）导致的知识利用不足和检索次优问题。DialogGSR 通过在语言模型上直接生成子图的标记序列，实现更有效的检索，包括结构感知的知识图谱线性化（使用自监督的图特定标记）和图约束解码（基于实体邻近度信息分数确保生成子图的准确性）。实验结果显示，该方法在 OpenDialKG 和 KOMODIS 数据集上达到了最先进的对话生成性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP (main)",
      "pdf_url": "http://arxiv.org/pdf/2410.09350v1",
      "published_date": "2024-10-12 03:33:42 UTC",
      "updated_date": "2024-10-12 03:33:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:48:10.955165"
    },
    {
      "arxiv_id": "2410.09349v2",
      "title": "Inference and Verbalization Functions During In-Context Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Junyi Tao",
        "Xiaoyin Chen",
        "Nelson F. Liu"
      ],
      "abstract": "Large language models (LMs) are capable of in-context learning from a few\ndemonstrations (example-label pairs) to solve new tasks during inference.\nDespite the intuitive importance of high-quality demonstrations, previous work\nhas observed that, in some settings, ICL performance is minimally affected by\nirrelevant labels (Min et al., 2022). We hypothesize that LMs perform ICL with\nirrelevant labels via two sequential processes: an inference function that\nsolves the task, followed by a verbalization function that maps the inferred\nanswer to the label space. Importantly, we hypothesize that the inference\nfunction is invariant to remappings of the label space (e.g., \"true\"/\"false\" to\n\"cat\"/\"dog\"), enabling LMs to share the same inference function across settings\nwith different label words. We empirically validate this hypothesis with\ncontrolled layer-wise interchange intervention experiments. Our findings\nconfirm the hypotheses on multiple datasets and tasks (natural language\ninference, sentiment analysis, and topic classification) and further suggest\nthat the two functions can be localized in specific layers across various\nopen-sourced models, including GEMMA-7B, MISTRAL-7B-V0.3, GEMMA-2-27B, and\nLLAMA-3.1-70B.",
      "tldr_zh": "本研究探讨了大语言模型 (LMs) 在 in-context learning (ICL) 过程中的机制，假设 LMs 通过两个顺序函数运作：inference function 用于解决任务，以及 verbalization function 用于将推断答案映射到标签空间。关键发现是，inference function 对标签空间的重新映射（如 \"true\"/\"false\" 到 \"cat\"/\"dog\"）保持不变，从而允许模型在不同设置中共享该函数。研究通过 controlled layer-wise interchange intervention experiments 在多个数据集和任务（如 natural language inference、sentiment analysis 和 topic classification）上验证了这一假设，并在模型如 GEMMA-7B、MISTRAL-7B-V0.3、GEMMA-2-27B 和 LLAMA-3.1-70B 中定位了这些函数的特定层。总体而言，此工作揭示了 ICL 的内部机制，为提升 LMs 的鲁棒性和可解释性提供了新见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "EMNLP 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2410.09349v2",
      "published_date": "2024-10-12 03:31:37 UTC",
      "updated_date": "2025-05-17 05:41:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:48:24.086864"
    },
    {
      "arxiv_id": "2410.09345v1",
      "title": "Contrastive Learning for Implicit Social Factors in Social Media Popularity Prediction",
      "title_zh": "针对社交媒体流行度预测中",
      "authors": [
        "Zhizhen Zhang",
        "Ruihong Qiu",
        "Xiaohui Xie"
      ],
      "abstract": "On social media sharing platforms, some posts are inherently destined for\npopularity. Therefore, understanding the reasons behind this phenomenon and\npredicting popularity before post publication holds significant practical\nvalue. The previous work predominantly focuses on enhancing post content\nextraction for better prediction results. However, certain factors introduced\nby social platforms also impact post popularity, which has not been extensively\nstudied. For instance, users are more likely to engage with posts from\nindividuals they follow, potentially influencing the popularity of these posts.\nWe term these factors, unrelated to the explicit attractiveness of content, as\nimplicit social factors. Through the analysis of users' post browsing behavior\n(also validated in public datasets), we propose three implicit social factors\nrelated to popularity, including content relevance, user influence similarity,\nand user identity. To model the proposed social factors, we introduce three\nsupervised contrastive learning tasks. For different task objectives and data\ntypes, we assign them to different encoders and control their gradient flows to\nachieve joint optimization. We also design corresponding sampling and\naugmentation algorithms to improve the effectiveness of contrastive learning.\nExtensive experiments on the Social Media Popularity Dataset validate the\nsuperiority of our proposed method and also confirm the important role of\nimplicit social factors in popularity prediction. We open source the code at\nhttps://github.com/Daisy-zzz/PPCL.git.",
      "tldr_zh": "该论文探讨了社交媒体帖子受欢迎度的预测问题，强调了隐式社会因素（implicit social factors）如内容相关性（content relevance）、用户影响相似性（user influence similarity）和用户身份（user identity）的作用，这些因素与内容显性吸引力无关。作者引入了三种监督对比学习（supervised contrastive learning）任务，通过不同的编码器分配梯度流控制和联合优化，并设计了相应的采样和增强算法来建模这些因素。在 Social Media Popularity Dataset 上的实验显示，该方法显著优于基线模型，并证实了隐式社会因素在预测中的重要性。论文开源代码可访问 https://github.com/Daisy-zzz/PPCL.git。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09345v1",
      "published_date": "2024-10-12 03:25:11 UTC",
      "updated_date": "2024-10-12 03:25:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:48:35.931144"
    },
    {
      "arxiv_id": "2410.09344v2",
      "title": "DARE the Extreme: Revisiting Delta-Parameter Pruning For Fine-Tuned Models",
      "title_zh": "翻译失败",
      "authors": [
        "Wenlong Deng",
        "Yize Zhao",
        "Vala Vakilian",
        "Minghui Chen",
        "Xiaoxiao Li",
        "Christos Thrampoulidis"
      ],
      "abstract": "Storing open-source fine-tuned models separately introduces redundancy and\nincreases response times in applications utilizing multiple models.\nDelta-parameter pruning (DPP), particularly the random drop and rescale (DARE)\nmethod proposed by Yu et al., addresses this by pruning the majority of delta\nparameters--the differences between fine-tuned and pre-trained model\nweights--while typically maintaining minimal performance loss. However, DARE\nfails when either the pruning rate or the magnitude of the delta parameters is\nlarge. We highlight two key reasons for this failure: (1) an excessively large\nrescaling factor as pruning rates increase, and (2) high mean and variance in\nthe delta parameters. To push DARE's limits, we introduce DAREx (DARE the\neXtreme), which features two algorithmic improvements: (1) DAREx-q, a rescaling\nfactor modification that significantly boosts performance at high pruning rates\n(e.g., >30 % on COLA and SST2 for encoder models, with even greater gains in\ndecoder models), and (2) DAREx-L2, which combines DARE with AdamR, an\nin-training method that applies appropriate delta regularization before DPP. We\nalso demonstrate that DAREx-q can be seamlessly combined with vanilla\nparameter-efficient fine-tuning techniques like LoRA and can facilitate\nstructural DPP. Additionally, we revisit the application of importance-based\npruning techniques within DPP, demonstrating that they outperform random-based\nmethods when delta parameters are large. Through this comprehensive study, we\ndevelop a pipeline for selecting the most appropriate DPP method under various\npractical scenarios.",
      "tldr_zh": "该论文重新审视了Delta-Parameter Pruning (DPP)技术，特别是DARE方法，用于减少微调模型的存储冗余，同时维持性能。DARE在高修剪率或delta parameters幅度大的情况下失效，主要由于过大的rescaling因子和高均值/方差问题。为此，作者提出DAREx改进方案，包括DAREx-q（修改rescaling因子，提升高修剪率下的性能，如在COLA和SST2数据集上超过30%的提升）和DAREx-L2（结合AdamR进行delta正则化）。实验结果显示，DAREx可与LoRA等技术无缝结合，并证明重要性-based修剪在delta parameters较大的场景中优于随机方法，最终提供了一个选择DPP方法的实用管道。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09344v2",
      "published_date": "2024-10-12 03:21:58 UTC",
      "updated_date": "2025-04-20 20:53:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:48:47.558265"
    },
    {
      "arxiv_id": "2410.09339v1",
      "title": "Advanced Gesture Recognition in Autism: Integrating YOLOv7, Video Augmentation and VideoMAE for Video Analysis",
      "title_zh": "自闭症中的高级手势识别：整合 YOLOv7、视频增强和 VideoMAE 用于视频分析",
      "authors": [
        "Amit Kumar Singh",
        "Trapti Shrivastava",
        "Vrijendra Singh"
      ],
      "abstract": "Deep learning and advancements in contactless sensors have significantly\nenhanced our ability to understand complex human activities in healthcare\nsettings. In particular, deep learning models utilizing computer vision have\nbeen developed to enable detailed analysis of human gesture recognition,\nespecially repetitive gestures which are commonly observed behaviors in\nchildren with autism. This research work aims to identify repetitive behaviors\nindicative of autism by analyzing videos captured in natural settings as\nchildren engage in daily activities. The focus is on accurately categorizing\nreal-time repetitive gestures such as spinning, head banging, and arm flapping.\nTo this end, we utilize the publicly accessible Self-Stimulatory Behavior\nDataset (SSBD) to classify these stereotypical movements. A key component of\nthe proposed methodology is the use of \\textbf{VideoMAE}, a model designed to\nimprove both spatial and temporal analysis of video data through a masking and\nreconstruction mechanism. This model significantly outperformed traditional\nmethods, achieving an accuracy of 97.7\\%, a 14.7\\% improvement over the\nprevious state-of-the-art.",
      "tldr_zh": "本研究旨在通过深度学习模型识别自闭症儿童的重复性手势，如旋转、头部撞击和手臂拍打，利用 YOLOv7、视频增强和 VideoMAE 进行视频分析。方法包括采用 VideoMAE 的掩码和重建机制来提升视频数据的空间和时间分析，并在 Self-Stimulatory Behavior Dataset (SSBD) 上进行分类测试。结果显示，该模型准确率达到 97.7%，比先前最佳方法提高了 14.7%，为自闭症行为评估提供了更可靠的工具。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09339v1",
      "published_date": "2024-10-12 02:55:37 UTC",
      "updated_date": "2024-10-12 02:55:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:48:59.166212"
    },
    {
      "arxiv_id": "2410.09335v2",
      "title": "Rethinking Data Selection at Scale: Random Selection is Almost All You Need",
      "title_zh": "重新审视大规模数据选择：随机选择几乎就是你所需要的一切",
      "authors": [
        "Tingyu Xia",
        "Bowen Yu",
        "Kai Dang",
        "An Yang",
        "Yuan Wu",
        "Yuan Tian",
        "Yi Chang",
        "Junyang Lin"
      ],
      "abstract": "Supervised fine-tuning (SFT) is crucial for aligning Large Language Models\n(LLMs) with human instructions. The primary goal during SFT is to select a\nsmall yet representative subset of training data from the larger pool, such\nthat fine-tuning with this subset achieves results comparable to or even\nexceeding those obtained using the entire dataset. However, most existing data\nselection techniques are designed for small-scale data pools, which fail to\nmeet the demands of real-world SFT scenarios. In this paper, we replicated\nseveral self-scoring methods those that do not rely on external model\nassistance on two million scale datasets, and found that nearly all methods\nstruggled to significantly outperform random selection when dealing with such\nlarge-scale data pools. Moreover, our comparisons suggest that, during SFT,\ndiversity in data selection is more critical than simply focusing on high\nquality data. We also analyzed the limitations of several current approaches,\nexplaining why they perform poorly on large-scale datasets and why they are\nunsuitable for such contexts. Finally, we found that filtering data by token\nlength offers a stable and efficient method for improving results. This\napproach, particularly when training on long text data, proves highly\nbeneficial for relatively weaker base models, such as Llama3.",
      "tldr_zh": "这篇论文重新审视了在 Supervised fine-tuning (SFT) 中大规模数据选择策略的核心问题，发现随机选择几乎能与复杂方法匹敌。作者在百万级数据集上复制了多种自评分方法，结果显示这些方法在大规模场景下无法显著优于随机选择，且数据多样性比单纯追求高质量数据更关键。论文分析了现有方法的局限性，并提出通过 token 长度过滤作为一种稳定有效的改进策略，尤其对较弱的基模型如 Llama3 在处理长文本时带来显著益处。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09335v2",
      "published_date": "2024-10-12 02:48:34 UTC",
      "updated_date": "2024-12-09 09:31:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:49:14.380964"
    },
    {
      "arxiv_id": "2410.19762v1",
      "title": "Reliable, Routable, and Reproducible: Collection of Pedestrian Pathways at Statewide Scale",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxiang Zhang",
        "Bill Howe",
        "Anat Caspi"
      ],
      "abstract": "While advances in mobility technology including autonomous vehicles and\nmulti-modal navigation systems can improve mobility equity for people with\ndisabilities, these technologies depend crucially on accurate, standardized,\nand complete pedestrian path networks. Ad hoc collection efforts lead to a data\nrecord that is sparse, unreliable, and non-interoperable.\n  This paper presents a sociotechnical methodology to collect, manage, serve,\nand maintain pedestrian path data at a statewide scale. Combining the\nautomation afforded by computer-vision approaches applied to aerial imagery and\nexisting road network data with the quality control afforded by interactive\ntools, we aim to produce routable pedestrian pathways for the entire State of\nWashington within approximately two years. We extract paths, crossings, and\ncurb ramps at scale from aerial imagery, integrating multi-input segmentation\nmethods with road topology data to ensure connected, routable networks. We then\norganize the predictions into project regions selected for their value to the\npublic interest, where each project region is divided into intersection-scale\ntasks. These tasks are assigned and tracked through an interactive tool that\nmanages concurrency, progress, feedback, and data management.\n  We demonstrate that our automated systems outperform state-of-the-art methods\nin producing routable pathway networks, which then significantly reduces the\ntime required for human vetting. Our results demonstrate the feasibility of\nyielding accurate, robust pedestrian pathway networks at the scale of an entire\nstate.\n  This paper intends to inform procedures for national-scale ADA compliance by\nproviding pedestrian equity, safety, and accessibility, and improving urban\nenvironments for all users.",
      "tldr_zh": "该论文提出了一种社会技术方法（sociotechnical methodology），旨在解决行人路径网络数据稀疏、不可靠和不可互操作的问题，以支持自治车辆和多模式导航系统，提高残疾人群的流动性。该方法结合计算机视觉技术（如从航空图像中提取路径、交叉口和路缘坡道）与道路拓扑数据，确保生成可路由的行人网络，并通过交互工具管理任务分配、进度和反馈。实验结果显示，该系统优于现有方法，能显著减少人工审核时间，并在华盛顿州规模内证明了构建准确、稳健行人路径网络的可行性。该研究为国家级的 ADA 合规提供指导，促进行人公平、安全和可达性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "arXiv admin note: text overlap with arXiv:2303.02323",
      "pdf_url": "http://arxiv.org/pdf/2410.19762v1",
      "published_date": "2024-10-12 02:31:57 UTC",
      "updated_date": "2024-10-12 02:31:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:49:23.125977"
    },
    {
      "arxiv_id": "2410.09329v1",
      "title": "Zero-shot Commonsense Reasoning over Machine Imagination",
      "title_zh": "翻译失败",
      "authors": [
        "Hyuntae Park",
        "Yeachan Kim",
        "Jun-Hyung Park",
        "SangKeun Lee"
      ],
      "abstract": "Recent approaches to zero-shot commonsense reasoning have enabled Pre-trained\nLanguage Models (PLMs) to learn a broad range of commonsense knowledge without\nbeing tailored to specific situations. However, they often suffer from human\nreporting bias inherent in textual commonsense knowledge, leading to\ndiscrepancies in understanding between PLMs and humans. In this work, we aim to\nbridge this gap by introducing an additional information channel to PLMs. We\npropose Imagine (Machine Imagination-based Reasoning), a novel zero-shot\ncommonsense reasoning framework designed to complement textual inputs with\nvisual signals derived from machine-generated images. To achieve this, we\nenhance PLMs with imagination capabilities by incorporating an image generator\ninto the reasoning process. To guide PLMs in effectively leveraging machine\nimagination, we create a synthetic pre-training dataset that simulates visual\nquestion-answering. Our extensive experiments on diverse reasoning benchmarks\nand analysis show that Imagine outperforms existing methods by a large margin,\nhighlighting the strength of machine imagination in mitigating reporting bias\nand enhancing generalization capabilities.",
      "tldr_zh": "本研究针对预训练语言模型 (PLMs) 在零样本常识推理中存在的文本常识知识偏差问题，提出了一种新框架 Imagine（基于机器想象的推理）。该框架通过整合图像生成器，将视觉信号补充到文本输入中，并利用合成预训练数据集模拟视觉问答任务，以增强 PLMs 的想象能力。实验结果显示，Imagine 在多种推理基准上大幅超越现有方法，显著缓解了人类报告偏差并提升了模型的泛化性能。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 9 figures, EMNLP 2024 (Findings)",
      "pdf_url": "http://arxiv.org/pdf/2410.09329v1",
      "published_date": "2024-10-12 02:15:11 UTC",
      "updated_date": "2024-10-12 02:15:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:49:34.566809"
    },
    {
      "arxiv_id": "2410.09324v1",
      "title": "Token Pruning using a Lightweight Background Aware Vision Transformer",
      "title_zh": "基于轻量级背景感知 Vision Transformer 的令牌修剪",
      "authors": [
        "Sudhakar Sah",
        "Ravish Kumar",
        "Honnesh Rohmetra",
        "Ehsan Saboori"
      ],
      "abstract": "High runtime memory and high latency puts significant constraint on Vision\nTransformer training and inference, especially on edge devices. Token pruning\nreduces the number of input tokens to the ViT based on importance criteria of\neach token. We present a Background Aware Vision Transformer (BAViT) model, a\npre-processing block to object detection models like DETR/YOLOS aimed to reduce\nruntime memory and increase throughput by using a novel approach to identify\nbackground tokens in the image. The background tokens can be pruned completely\nor partially before feeding to a ViT based object detector. We use the semantic\ninformation provided by segmentation map and/or bounding box annotation to\ntrain a few layers of ViT to classify tokens to either foreground or\nbackground. Using 2 layers and 10 layers of BAViT, background and foreground\ntokens can be separated with 75% and 88% accuracy on VOC dataset and 71% and\n80% accuracy on COCO dataset respectively. We show a 2 layer BAViT-small model\nas pre-processor to YOLOS can increase the throughput by 30% - 40% with a mAP\ndrop of 3% without any sparse fine-tuning and 2% with sparse fine-tuning. Our\napproach is specifically targeted for Edge AI use cases.",
      "tldr_zh": "本研究提出了一种轻量级 Background Aware Vision Transformer (BAViT) 模型，用于在 Vision Transformer (ViT) 中进行 token pruning，以缓解高内存和延迟问题，尤其适用于边缘设备。BAViT 作为对象检测模型如 DETR 或 YOLOS 的预处理模块，利用分割图和边界框注释训练 ViT 的少量层来分类图像 tokens 为前景或背景，从而实现对背景 tokens 的部分或完全修剪。实验结果显示，在 VOC 数据集上 BAViT 的分类准确率达 75%（2 层）和 88%（10 层），在 COCO 数据集上分别为 71% 和 80%；作为 YOLOS 的预处理器，它能提高 30%-40% 的吞吐量，同时 mAP 只下降 2%-3%。这种方法针对 Edge AI 用例，提供了高效的优化方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 2 tables, 4 figures, FITML workshop@NeuRIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2410.09324v1",
      "published_date": "2024-10-12 01:44:54 UTC",
      "updated_date": "2024-10-12 01:44:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:49:46.621825"
    },
    {
      "arxiv_id": "2410.09319v1",
      "title": "Hey AI Can You Grade My Essay?: Automatic Essay Grading",
      "title_zh": "嘿，AI，你能帮我批改作文吗？：自动作文评分",
      "authors": [
        "Maisha Maliha",
        "Vishal Pramanik"
      ],
      "abstract": "Automatic essay grading (AEG) has attracted the the attention of the NLP\ncommunity because of its applications to several educational applications, such\nas scoring essays, short answers, etc. AEG systems can save significant time\nand money when grading essays. In the existing works, the essays are graded\nwhere a single network is responsible for the whole process, which may be\nineffective because a single network may not be able to learn all the features\nof a human-written essay. In this work, we have introduced a new model that\noutperforms the state-of-the-art models in the field of AEG. We have used the\nconcept of collaborative and transfer learning, where one network will be\nresponsible for checking the grammatical and structural features of the\nsentences of an essay while another network is responsible for scoring the\noverall idea present in the essay. These learnings are transferred to another\nnetwork to score the essay. We also compared the performances of the different\nmodels mentioned in our work, and our proposed model has shown the highest\naccuracy of 85.50%.",
      "tldr_zh": "本研究探讨了自动论文评分（AEG）系统在教育应用中的潜力，指出现有方法依赖单一网络可能无法全面捕捉人类作文的特征。作者提出了一种新模型，利用协作学习（collaborative learning）和迁移学习（transfer learning），其中一个网络负责检查作文的语法和结构特征，另一个网络评估整体想法，然后将这些学习转移到第三个网络进行最终评分。该模型在实验中与现有模型比较，实现了85.50%的最高准确率，展示了其在节省时间和成本方面的显著优势。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in ICAAAIML (4th International Conference on Advances and\n  Applications of Artificial Intelligence and Machine Learning) 2023",
      "pdf_url": "http://arxiv.org/pdf/2410.09319v1",
      "published_date": "2024-10-12 01:17:55 UTC",
      "updated_date": "2024-10-12 01:17:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:49:58.864171"
    },
    {
      "arxiv_id": "2410.09314v1",
      "title": "\\llinstruct: An Instruction-tuned model for English Language Proficiency Assessments",
      "title_zh": "翻译失败",
      "authors": [
        "Debanjan Ghosh",
        "Sophia Chan"
      ],
      "abstract": "We present \\llinstruct: An 8B instruction-tuned model that is designed to\ngenerate content for English Language Proficiency Assessments (ELPA) and\nrelated applications. Our work involves creating a new dataset of 70K\ninstructions and explanations in the ELPA domain and using these to fine-tune\nLlama-3 8B models (SFT) of different sizes (e.g., SFT-17K, SFT-50K and\nSFT-70K). Human evaluations are conducted over unseen instructions to compare\nthese SFT models against SOTA models (e.g., Dolly-2, Mistral, Llama-3 base\nversion, and GPT-3.5). The findings show although all three SFT models perform\ncomparably, the model trained on largest instruction dataset -- SFT-70K - leads\nto the most valid outputs ready for assessments. However, although the SFT\nmodels perform better than larger model, e.g., GPT 3.5 on the aspect of\nexplanations of outputs, many outputs still need human interventions to make\nthem actual ready for real world assessments.",
      "tldr_zh": "本研究介绍了\\llinstruct，一种针对英语语言能力评估（ELPA）的8B指令微调模型，通过创建70K条指令和解释数据集来微调Llama-3 8B模型的不同版本（如SFT-17K、SFT-50K和SFT-70K）。研究团队通过人类评估将这些SFT模型与SOTA模型（如Dolly-2、Mistral、Llama-3基版本和GPT-3.5）进行比较，结果显示SFT-70K模型在输出有效性方面表现最佳。虽然后者在解释输出上优于GPT-3.5，但许多结果仍需人工干预才能适用于实际评估。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09314v1",
      "published_date": "2024-10-12 00:47:45 UTC",
      "updated_date": "2024-10-12 00:47:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:50:10.986187"
    },
    {
      "arxiv_id": "2410.09312v2",
      "title": "Towards Multi-Modal Animal Pose Estimation: A Survey and In-Depth Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Qianyi Deng",
        "Oishi Deb",
        "Amir Patel",
        "Christian Rupprecht",
        "Philip Torr",
        "Niki Trigoni",
        "Andrew Markham"
      ],
      "abstract": "Animal pose estimation (APE) aims to locate the animal body parts using a\ndiverse array of sensor and modality inputs (e.g. RGB cameras, LiDAR, infrared,\nIMU, acoustic and language cues), which is crucial for research across\nneuroscience, biomechanics, and veterinary medicine. By evaluating 176 papers\nsince 2011, APE methods are categorised by their input sensor and modality\ntypes, output forms, learning paradigms, experimental setup, and application\ndomains, presenting detailed analyses of current trends, challenges, and future\ndirections in single- and multi-modality APE systems. The analysis also\nhighlights the transition between human and animal pose estimation, and how\ninnovations in APE can reciprocally enrich human pose estimation and the\nbroader machine learning paradigm. Additionally, 2D and 3D APE datasets and\nevaluation metrics based on different sensors and modalities are provided. A\nregularly updated project page is provided here:\nhttps://github.com/ChennyDeng/MM-APE.",
      "tldr_zh": "这篇论文对多模态动物姿态估计 (Animal Pose Estimation, APE) 进行了全面调查和深入分析，涵盖了使用多种传感器和模态输入（如 RGB 相机、LiDAR、红外、IMU、声学和语言提示）的技术及其在神经科学、生物力学和兽医学中的应用。作者评估了自 2011 年以来的 176 篇论文，将 APE 方法分类为输入类型、输出形式、学习范式、实验设置和应用领域，并分析了当前趋势、挑战以及单模态和多模态系统的未来方向。论文还探讨了 APE 与人类姿态估计之间的相互影响，以及如何推动更广泛的机器学习发展，并提供了基于不同传感器的 2D 和 3D 数据集、评估指标，以及一个定期更新的项目页面（https://github.com/ChennyDeng/MM-APE）。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "A.1"
      ],
      "primary_category": "cs.CV",
      "comment": "35 pages, 5 figures, 8 tables. Qianyi Deng and Oishi Deb are Joint\n  Major Contributors to this work",
      "pdf_url": "http://arxiv.org/pdf/2410.09312v2",
      "published_date": "2024-10-12 00:37:07 UTC",
      "updated_date": "2025-01-04 20:01:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:50:24.618814"
    },
    {
      "arxiv_id": "2410.09307v1",
      "title": "Graph Neural Alchemist: An innovative fully modular architecture for time series-to-graph classification",
      "title_zh": "翻译失败",
      "authors": [
        "Paulo Coelho",
        "Raul Araju",
        "Luís Ramos",
        "Samir Saliba",
        "Renato Vimieiro"
      ],
      "abstract": "This paper introduces a novel Graph Neural Network (GNN) architecture for\ntime series classification, based on visibility graph representations.\nTraditional time series classification methods often struggle with high\ncomputational complexity and inadequate capture of spatio-temporal dynamics. By\nrepresenting time series as visibility graphs, it is possible to encode both\nspatial and temporal dependencies inherent to time series data, while being\ncomputationally efficient. Our architecture is fully modular, enabling flexible\nexperimentation with different models and representations. We employ directed\nvisibility graphs encoded with in-degree and PageRank features to improve the\nrepresentation of time series, ensuring efficient computation while enhancing\nthe model's ability to capture long-range dependencies in the data. We show the\nrobustness and generalization capability of the proposed architecture across a\ndiverse set of classification tasks and against a traditional model. Our work\nrepresents a significant advancement in the application of GNNs for time series\nanalysis, offering a powerful and flexible framework for future research and\npractical implementations.",
      "tldr_zh": "该论文提出了一种创新的Graph Neural Network (GNN)架构，名为Graph Neural Alchemist，用于时间序列分类，通过将时间序列转换为可见性图(visibility graphs)来编码空间和时间依赖性，同时降低计算复杂度。该架构完全模块化，支持灵活实验不同模型和表示方式，并使用有向可见性图结合in-degree和PageRank特征，以高效捕捉数据中的长程依赖。在各种分类任务中，该框架显示出优于传统模型的鲁棒性和泛化能力，为GNN在时间序列分析中的应用提供了强大灵活的框架。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.09307v1",
      "published_date": "2024-10-12 00:03:40 UTC",
      "updated_date": "2024-10-12 00:03:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T10:50:41.075932"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 70,
  "processed_papers_count": 70,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-20T10:50:58.377201"
}